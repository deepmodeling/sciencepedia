{
    "hands_on_practices": [
        {
            "introduction": "The LATER model offers a compellingly simple yet powerful account of reaction time, linking it to the linear rise of a decision signal. A cornerstone of analyzing data within this framework is the reciprobit plot, which linearizes the cumulative distribution of reaction times. This exercise  challenges you to derive the mathematical basis for this linearity and to explore how a key cognitive parameter—the decision threshold—predictably alters the slope of the reciprobit line, providing a direct link between theoretical constructs and measurable data features.",
            "id": "4012834",
            "problem": "Consider the Linear Approach to Threshold with Ergodic Rate (LATER) model of reaction time, in which a decision signal starts at an initial level $S_{0}$ and rises linearly toward a decision threshold $S$ at a stochastic rate $r$. Assume $r$ is drawn independently on each trial from a normal (Gaussian) distribution with mean $\\mu$ and standard deviation $\\sigma$, and that the measured Reaction Time (RT) is given by $T = (S - S_{0})/r$ with no additive non-decision components. The reciprobit representation considers the reciprocal latency $x = 1/T$ on the horizontal axis and the probit (the inverse of the normal cumulative distribution) of the cumulative probability on the vertical axis. In this representation, the cumulative distribution of $x$ is mapped to a straight line.\n\nStarting from these definitions, derive the general expression for the reciprobit slope as a function of $S$, $S_{0}$, and $\\sigma$. Then, suppose $S$ is doubled to $2S$ while $S_{0}$, $\\mu$, and $\\sigma$ remain fixed. Compute the multiplicative change in the reciprobit slope (i.e., the ratio of the new slope to the original slope), and interpret this result in terms of the speed–accuracy tradeoff implied by the LATER model.\n\nProvide your final answer as a single closed-form analytic expression for the slope ratio. The requested ratio is dimensionless, so no units are required. No rounding is necessary.",
            "solution": "The reaction time $T$ is given by $T = (S - S_{0})/r$. The reciprobit plot considers the reciprocal latency, $x = 1/T = r/(S-S_{0})$.\nThe rate $r$ is normally distributed: $r \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\nSince $x$ is a linear scaling of $r$, $x$ is also normally distributed. Its mean $\\mu_x$ and standard deviation $\\sigma_x$ are:\n$$\\mu_x = E[x] = E\\left[\\frac{r}{S - S_{0}}\\right] = \\frac{E[r]}{S - S_{0}} = \\frac{\\mu}{S - S_{0}}$$\n$$\\sigma_x = \\text{StDev}\\left[\\frac{r}{S - S_{0}}\\right] = \\frac{\\text{StDev}[r]}{|S - S_{0}|} = \\frac{\\sigma}{S - S_{0}}$$\n(assuming $S > S_{0}$).\nThe vertical axis of the reciprobit plot is the probit of the cumulative probability of $x$, which we denote as $z = \\Phi^{-1}(P(X \\le x))$. For a normally distributed variable $x$, its cumulative distribution function (CDF) is $P(X \\le x) = \\Phi\\left(\\frac{x - \\mu_x}{\\sigma_x}\\right)$.\nTherefore, the vertical axis is:\n$$z = \\Phi^{-1}\\left(\\Phi\\left(\\frac{x - \\mu_x}{\\sigma_x}\\right)\\right) = \\frac{x - \\mu_x}{\\sigma_x} = \\left(\\frac{1}{\\sigma_x}\\right)x - \\frac{\\mu_x}{\\sigma_x}$$\nThis is the equation of a straight line in the $(x, z)$ plane.\n\n**Part 1: General Expression for the Reciprobit Slope**\nThe slope of this line, $m_{\\text{recip}}$, is the coefficient of $x$, which is $1/\\sigma_x$.\n$$m_{\\text{recip}} = \\frac{1}{\\sigma_x} = \\frac{1}{\\frac{\\sigma}{S - S_{0}}} = \\frac{S - S_{0}}{\\sigma}$$\n\n**Part 2: Multiplicative Change in the Slope**\nThe original slope is $m_{\\text{orig}} = \\frac{S - S_0}{\\sigma}$.\nWhen the threshold is doubled to $S' = 2S$, the new slope is $m_{\\text{new}} = \\frac{2S - S_0}{\\sigma}$.\nThe ratio of the new slope to the original slope is:\n$$\\text{Ratio} = \\frac{m_{\\text{new}}}{m_{\\text{orig}}} = \\frac{\\frac{2S - S_0}{\\sigma}}{\\frac{S - S_0}{\\sigma}} = \\frac{2S - S_0}{S - S_0}$$\n\n**Part 3: Interpretation**\nIncreasing the decision threshold $S$ is the LATER model's mechanism for a speed-accuracy tradeoff. A higher $S$ leads to longer, more accurate responses. This change is predicted to linearly increase the slope of the reciprobit plot by a factor of $(2S - S_0)/(S - S_0)$. Since $S > S_0$, this factor is greater than 1. This provides a direct, testable link between a theoretical cognitive parameter (the threshold) and an observable feature of the reaction time distribution (the reciprobit slope).",
            "answer": "$$\\boxed{\\frac{2S - S_{0}}{S - S_{0}}}$$"
        },
        {
            "introduction": "A theoretical model's utility is proven in its application to real experimental data and its ability to withstand scrutiny of its core assumptions. This practice  situates the race model within the classic stop-signal paradigm, a workhorse for studying inhibitory control. You will be asked to think critically about the independence assumption—a foundational tenet of the model—by designing a formal test and analyzing the systematic biases in key parameter estimates that arise when this assumption is violated.",
            "id": "4012802",
            "problem": "A laboratory studies inhibitory control with the stop-signal task and models reaction times using the Linear Approach to Threshold with Ergodic Rate (LATER) model. In each trial, a \"go\" process accumulates linearly to a threshold with rate $r$ drawn from a normal distribution, producing a go reaction time $T_G$, while on stop-signal trials an independent \"stop\" process finishes at time $T_S = \\mathrm{SSD} + \\mathrm{SSRT}$, where $\\mathrm{SSD}$ is the stop-signal delay and $\\mathrm{SSRT}$ is the stop-signal reaction time. On stop-signal trials, a response is produced if and only if $T_G  T_S$, and the observed signal-respond reaction time is $T_G$ on those trials.\n\nThe independence assumption of the race model states that the finishing time distributions of the go and stop processes are independent, and that the go process is invariant across stop-signal and no-signal trials. The LATER model implies that the reciprocal reaction times $1/T_G$ are approximately normally distributed, which yields straight lines on reciprobit plots.\n\nYou are tasked to select both:\n1) A valid, distribution-level test of the independence assumption by comparing the signal-respond reaction time distribution to the go reaction time distribution across values of $\\mathrm{SSD}$, and\n2) The direction of bias in the estimated $\\mathrm{SSRT}$ obtained via the standard integration method when participants exhibit systematic slowing of go responses on stop-signal trials relative to no-signal trials (i.e., context invariance is violated, but the staircase tracks $\\mathrm{SSD}$ to maintain $\\mathbb{P}(\\text{respond} \\mid \\mathrm{SSD})$ near a fixed level).\n\nWhich option most accurately specifies both the test and the bias?\n\nA. For each $\\mathrm{SSD}$, transform each signal-respond reaction time $t$ to $u = F_G(t)/p_{\\mathrm{resp}}(\\mathrm{SSD})$, where $F_G$ is the empirical cumulative distribution function of go reaction times from no-signal trials and $p_{\\mathrm{resp}}(\\mathrm{SSD}) = \\mathbb{P}(\\text{respond} \\mid \\mathrm{SSD})$. Under independence, $u$ should be uniformly distributed on $[0,1]$. Systematic slowing of go responses on stop-signal trials causes underestimation of $\\mathrm{SSRT}$ by the integration method.\n\nB. Compare the mean signal-respond reaction time to the mean go reaction time; independence holds if the mean signal-respond time is longer. Systematic slowing of go responses on stop-signal trials causes overestimation of $\\mathrm{SSRT}$ by the integration method.\n\nC. Plot cumulative probability versus $1/T$ (reciprobit) for go and signal-respond reaction times; independence predicts that signal-respond lines have steeper slope than go lines. Systematic slowing of go responses on stop-signal trials causes overestimation of $\\mathrm{SSRT}$ by the integration method.\n\nD. Compute and compare the hazard functions of signal-respond and go reaction times; independence requires equality of hazards. Systematic slowing of go responses on stop-signal trials does not bias $\\mathrm{SSRT}$ when using the integration method.",
            "solution": "### Part 1: Test of the Independence Assumption\n\nThe independence assumption implies that the go process on stop-signal trials is identical to the go process on no-signal trials. Let $F_G(t) = \\mathbb{P}(T_G \\leq t)$ be the cumulative distribution function (CDF) of go reaction times, estimated from no-signal trials. The model posits that a response occurs on a stop-signal trial if $T_G  T_S$, where $T_S$ is the stop time. The probability of this is $p_{\\mathrm{resp}} = \\mathbb{P}(T_G  T_S)$.\n\nThe reaction times on these \"signal-respond\" trials, $T_{SR}$, are from the go process, but conditioned on finishing before $T_S$. The CDF of these observed times, $F_{SR}(t)$, is therefore:\n$$ F_{SR}(t) = \\mathbb{P}(T_{SR} \\leq t) = \\mathbb{P}(T_G \\leq t \\mid T_G  T_S) = \\frac{\\mathbb{P}(T_G \\leq t \\text{ and } T_G  T_S)}{\\mathbb{P}(T_G  T_S)} $$\nFor any time $t  T_S$, this simplifies to:\n$$ F_{SR}(t) = \\frac{\\mathbb{P}(T_G \\leq t)}{p_{\\mathrm{resp}}} = \\frac{F_G(t)}{p_{\\mathrm{resp}}} $$\nThis equation is a key prediction of the model. It can be tested using the probability integral transform. For any observed signal-respond RT, $t_{SR}$, the transformed variable $u = F_{SR}(t_{SR})$ should be uniformly distributed on $[0, 1]$. Substituting our expression, the test becomes checking if $u = F_G(t_{SR}) / p_{\\mathrm{resp}}$ is uniform on $[0,1]$. This confirms the test described in option A.\n\n### Part 2: Bias in SSRT Estimation\n\nThe integration method estimates the stop-signal reaction time ($\\mathrm{SSRT}$) using the equation $\\widehat{\\mathrm{SSRT}} = F_{G,NS}^{-1}(p_{\\mathrm{resp}}) - \\overline{\\mathrm{SSD}}$, where $F_{G,NS}$ is the CDF of no-signal go RTs and $p_{\\mathrm{resp}}$ is the observed response probability on stop trials.\n\nThe problem states that go responses are systematically slower on stop-signal trials than on no-signal trials. Let the true (but unobserved) go RT distribution on stop trials be $F_{G,SS}(t)$. \"Slower\" means that $F_{G,SS}(t) \\leq F_{G,NS}(t)$ for all $t$. This implies that the quantile functions are ordered as $F_{G,SS}^{-1}(p) \\geq F_{G,NS}^{-1}(p)$ for any probability $p$.\n\nThe observed response probability, $p_{\\mathrm{resp}}$, is actually determined by the race with the slower go process: $p_{\\mathrm{resp}} = F_{G,SS}(T_S)$, where $T_S$ is the true stop time. Therefore, the true stop time is $T_S = F_{G,SS}^{-1}(p_{\\mathrm{resp}})$.\n\nThe integration method, however, estimates the stop time as $\\hat{T}_S = F_{G,NS}^{-1}(p_{\\mathrm{resp}})$.\nSince $F_{G,SS}^{-1}(p) \\geq F_{G,NS}^{-1}(p)$, it follows that the true stop time is greater than or equal to the estimated stop time:\n$$ T_S \\geq \\hat{T}_S $$\nThe estimated SSRT is $\\widehat{\\mathrm{SSRT}} = \\hat{T}_S - \\overline{\\mathrm{SSD}}$. The true SSRT is $\\mathrm{SSRT}_{\\text{true}} = T_S - \\overline{\\mathrm{SSD}}$.\nTherefore, $\\widehat{\\mathrm{SSRT}} \\leq \\mathrm{SSRT}_{\\text{true}}$. The systematic slowing of go responses leads to an **underestimation** of the SSRT.\n\n### Conclusion\n\nOption A correctly identifies both the distributional test for independence and the direction of the bias (underestimation). The other options are incorrect:\n-   **B:** The mean signal-respond RT is shorter, not longer, than the mean go RT. The bias is underestimation, not overestimation.\n-   **C:** A reciprobit plot of signal-respond RTs (a truncated distribution) is not a straight line. The bias is underestimation.\n-   **D:** The hazard functions are not equal. The bias is significant.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Parallel race models make a strong, testable prediction about reaction times in redundant-signal experiments: the race model inequality, or Miller bound. Violations of this bound are considered strong evidence for neural coactivation. This hands-on exercise  moves from theory to practice, requiring you to implement a sophisticated bootstrap hypothesis test to determine if a given dataset violates this fundamental inequality, thereby providing a powerful tool for distinguishing between race and coactivation models of decision-making.",
            "id": "4012824",
            "problem": "You are given independent samples of reaction times from three conditions in a redundant signals paradigm: stimulus $A$ alone, stimulus $B$ alone, and redundant stimuli $AB$. Assume reaction times are strictly positive and are measured in seconds. The Linear Approach to Threshold with Ergodic Rate (LATER) model posits that decision time $T$ can be represented as $T = S / R$ where $S$ is a fixed decision bound and $R$ is a random rate variable; in practice, empirical distributions for $A$ and $B$ will be used directly.\n\nDefine the cumulative distribution functions $F_A(t)$, $F_B(t)$, and $F_{AB}(t)$ of the reaction times for the three conditions. The classical race inequality (Miller bound) states that, in the absence of coactivation, the redundant condition $AB$ must satisfy the pointwise bound\n$$\nF_{AB}(t) \\le \\min\\big(F_A(t) + F_B(t), \\, 1\\big) \\quad \\text{for all } t \\in \\mathbb{R}_{\\ge 0}.\n$$\nYou must design and implement a statistical procedure to test for violations of this bound based on finite samples, using empirical cumulative distribution functions and a bootstrap calibrated under a least-favorable null model.\n\nFormulate the hypotheses as follows:\n- Null hypothesis $H_0$: for all $t \\in \\mathbb{R}_{\\ge 0}$, $F_{AB}(t) \\le \\min\\big(F_A(t) + F_B(t), \\, 1\\big)$.\n- Alternative hypothesis $H_1$: there exists $t \\in \\mathbb{R}_{\\ge 0}$ such that $F_{AB}(t)  \\min\\big(F_A(t) + F_B(t), \\, 1\\big)$.\n\nYour test must use the following ingredients:\n- Empirical cumulative distribution functions $\\hat F_A(t)$, $\\hat F_B(t)$, and $\\hat F_{AB}(t)$ computed from the samples.\n- A test statistic defined as the supremum of the pointwise excess of the empirical redundant cumulative distribution over the empirical Miller bound:\n$$\nT_n \\equiv \\sup_{t \\in \\mathcal{T}} \\left\\{ \\hat F_{AB}(t) - \\min\\big(\\hat F_A(t) + \\hat F_B(t), \\, 1\\big) \\right\\},\n$$\nwhere $\\mathcal{T}$ is a finite grid consisting of all observed sample times across all three conditions.\n- A bootstrap procedure that approximates the null distribution of $T_n$ by simulating bootstrap datasets under a least-favorable null model constructed from the empirical marginals. Specifically, for each bootstrap replication:\n  1. Resample with replacement $n_A$ draws from the $A$-only sample to form a bootstrap $A^\\ast$, and resample with replacement $n_B$ draws from the $B$-only sample to form a bootstrap $B^\\ast$.\n  2. Compute the bootstrap empirical bound $G^\\ast(t) = \\min\\big(\\hat F_{A^\\ast}(t) + \\hat F_{B^\\ast}(t), \\, 1\\big)$ on the grid of unique times observed in $A^\\ast \\cup B^\\ast$.\n  3. Sample $n_{AB}$ redundant times $AB^\\ast$ from the discrete distribution with cumulative distribution function $G^\\ast(t)$, implemented by inverse transform on that grid.\n  4. Compute the bootstrap statistic $T_n^\\ast$ using $A^\\ast$, $B^\\ast$, and $AB^\\ast$ exactly as for $T_n$.\n- The bootstrap $p$-value is estimated as\n$$\n\\hat p = \\frac{1 + \\sum_{b=1}^B \\mathbf{1}\\{T_{n,b}^\\ast \\ge T_n\\}}{1 + B},\n$$\nand the test rejects $H_0$ at level $\\alpha$ if $\\hat p \\le \\alpha$. Use $\\alpha = 0.05$.\n\nImplement this test and apply it to the following test suite of synthetic datasets, each generated from the LATER model for $A$ and $B$ and a race of two LATER processes for $AB$ with an optional multiplicative coactivation factor $\\gamma$:\n- For each condition $X \\in \\{A, B\\}$, generate rates $R_X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$ truncated to $R_X  0$, set $S = 1$, and compute $T_X = S / R_X$. For the redundant condition, generate independent draws $T_A'$ and $T_B'$ in the same way and set $T_{AB} = \\gamma \\cdot \\min(T_A', T_B')$. All times must be measured in seconds.\n- The test suite consists of the following parameter sets, each with a fixed random seed for reproducibility:\n  1. Case $1$ (race, large sample): $n_A = n_B = n_{AB} = 600$, $\\mu_A = 3.0$, $\\sigma_A = 0.4$, $\\mu_B = 3.0$, $\\sigma_B = 0.4$, $\\gamma = 1.0$.\n  2. Case $2$ (coactivation, violation): $n_A = n_B = n_{AB} = 400$, $\\mu_A = 3.0$, $\\sigma_A = 0.4$, $\\mu_B = 3.0$, $\\sigma_B = 0.4$, $\\gamma = 0.8$.\n  3. Case $3$ (race, small sample): $n_A = n_B = n_{AB} = 120$, $\\mu_A = 3.0$, $\\sigma_A = 0.5$, $\\mu_B = 3.0$, $\\sigma_B = 0.5$, $\\gamma = 1.0$.\n  4. Case $4$ (asymmetric, near-boundary): $n_A = n_B = n_{AB} = 400$, $\\mu_A = 2.5$, $\\sigma_A = 0.5$, $\\mu_B = 4.0$, $\\sigma_B = 0.5$, $\\gamma = 0.95$.\n- Use $B = 1000$ bootstrap replications per case and a single fixed seed across all cases so that results are exactly reproducible.\n\nYour program must:\n- Generate synthetic datasets according to the above specifications in seconds.\n- Compute the test statistic and bootstrap $p$-value for each case.\n- Decide whether to reject $H_0$ at level $\\alpha = 0.05$ for each case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases $1$ through $4$, where each entry is a boolean indicating whether $H_0$ is rejected for that case. For example: \"[True,False,False,True]\".",
            "solution": "This problem requires the implementation of a bootstrap hypothesis test for the race model inequality (Miller bound). The goal is to determine if reaction time data from a redundant signals experiment is consistent with an independent parallel race model or if it shows evidence of coactivation.\n\nThe core of the method is to compare the observed data against a null hypothesis ($H_0$) which states that the race model holds: $F_{AB}(t) \\le \\min\\big(F_A(t) + F_B(t), 1\\big)$. A violation of this inequality provides evidence for the alternative hypothesis ($H_1$) of coactivation.\n\n**Methodology**\n\n1.  **Test Statistic Calculation**: First, we quantify the extent of the violation in the observed data. We use the empirical cumulative distribution functions (ECDFs) $\\hat{F}_A, \\hat{F}_B, \\hat{F}_{AB}$ calculated from the data samples. The test statistic, $T_n$, is the maximum difference between the redundant ECDF and the Miller bound constructed from the single-stimulus ECDFs. This maximum is evaluated over all unique time points observed in the dataset.\n    $$T_n = \\sup_{t} \\left\\{ \\hat{F}_{AB}(t) - \\min\\big(\\hat{F}_A(t) + \\hat{F}_B(t), 1\\big) \\right\\}$$\n    A larger positive value of $T_n$ suggests a stronger violation of the race model.\n\n2.  **Bootstrap Procedure**: To determine if the observed $T_n$ is statistically significant, we need to know its distribution under the null hypothesis. This is approximated using a bootstrap procedure designed to generate data that conforms to $H_0$. For each bootstrap replication:\n    a. Bootstrap samples for the single-stimulus conditions, $A^\\ast$ and $B^\\ast$, are created by resampling with replacement from the original data.\n    b. These are used to construct a bootstrap null distribution, $G^\\ast(t) = \\min\\big(\\hat{F}_{A^\\ast}(t) + \\hat{F}_{B^\\ast}(t), 1\\big)$. This represents the \"least-favorable\" case under the null hypothesis, lying on the boundary of the inequality, which makes the test maximally sensitive to violations.\n    c. A bootstrap sample for the redundant condition, $AB^\\ast$, is generated by sampling from the discrete distribution defined by $G^\\ast(t)$ via inverse transform sampling. By construction, the triplet $(A^\\ast, B^\\ast, AB^\\ast)$ respects the race model inequality.\n    d. The bootstrap test statistic, $T_n^\\ast$, is calculated from this simulated triplet.\n\n3.  **P-value and Decision**: This process is repeated many times (e.g., $B=1000$) to create an empirical null distribution of the test statistic. The $p$-value is the proportion of bootstrap statistics $T_n^\\ast$ that are greater than or equal to the originally observed statistic $T_n$. If this $p$-value is less than or equal to the significance level $\\alpha=0.05$, we reject the null hypothesis and conclude there is evidence for coactivation.\n\nThis procedure will be applied to four synthetic datasets generated according to the problem's specifications, using the LATER model with and without a coactivation factor $\\gamma$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import truncnorm\n\n# Define global parameters as specified in the problem\nB = 1000\nALPHA = 0.05\n# A single fixed seed for reproducibility across all cases\nRANDOM_SEED = 42\n\ndef generate_LATER_RTs(rng, n, mu, sigma, S=1.0):\n    \"\"\"\n    Generates reaction times from the LATER model S/R, where R is from a\n    truncated normal distribution.\n    \"\"\"\n    # The lower bound is 0. In standard-normal units for truncnorm, this is (0 - mu) / sigma.\n    a = -mu / sigma\n    b = np.inf\n    rates = truncnorm.rvs(a, b, loc=mu, scale=sigma, size=n, random_state=rng)\n    # Ensure rates are strictly positive to avoid division by zero\n    rates[rates = 0] = 1e-9  # A small positive floor\n    return S / rates\n\ndef generate_dataset(rng, n_a, n_b, n_ab, mu_a, sigma_a, mu_b, sigma_b, gamma):\n    \"\"\"\n    Generates a full dataset for conditions A, B, and AB based on LATER model parameters.\n    \"\"\"\n    sample_a = generate_LATER_RTs(rng, n_a, mu_a, sigma_a)\n    sample_b = generate_LATER_RTs(rng, n_b, mu_b, sigma_b)\n\n    # For the redundant condition, generate two sets of latent RTs for the race\n    t_a_prime = generate_LATER_RTs(rng, n_ab, mu_a, sigma_a)\n    t_b_prime = generate_LATER_RTs(rng, n_ab, mu_b, sigma_b)\n    \n    # The redundant RT is the coactivated minimum of the two latent RTs\n    sample_ab = gamma * np.minimum(t_a_prime, t_b_prime)\n\n    return sample_a, sample_b, sample_ab\n\ndef ecdf(sample, t_grid):\n    \"\"\"\n    Computes the Empirical Cumulative Distribution Function values for a sample\n    at specified time points.\n    \"\"\"\n    if len(sample) == 0:\n        return np.zeros_like(t_grid, dtype=float)\n    # Using np.searchsorted is efficient for this task\n    sample_sorted = np.sort(sample)\n    return np.searchsorted(sample_sorted, t_grid, side='right') / len(sample)\n\ndef compute_test_statistic(sample_a, sample_b, sample_ab):\n    \"\"\"\n    Computes the test statistic T_n, the supremum of the violation of the Miller bound.\n    The supremum is taken over the grid of all observed time points.\n    \"\"\"\n    # Create the grid of all unique observed time points\n    t_grid = np.unique(np.concatenate((sample_a, sample_b, sample_ab)))\n\n    # Compute ECDFs for all three conditions on the common grid\n    ecdf_a = ecdf(sample_a, t_grid)\n    ecdf_b = ecdf(sample_b, t_grid)\n    ecdf_ab = ecdf(sample_ab, t_grid)\n\n    # Calculate the Miller bound\n    miller_bound = np.minimum(ecdf_a + ecdf_b, 1.0)\n    \n    # The test statistic is the maximum positive deviation from the bound\n    test_statistic = np.max(ecdf_ab - miller_bound)\n    \n    return test_statistic\n\ndef perform_bootstrap_test(rng, sample_a, sample_b, sample_ab):\n    \"\"\"\n    Performs the full bootstrap hypothesis test.\n    \"\"\"\n    n_a, n_b, n_ab = len(sample_a), len(sample_b), len(sample_ab)\n\n    # 1. Compute the observed test statistic\n    T_n_obs = compute_test_statistic(sample_a, sample_b, sample_ab)\n\n    bootstrap_stats = []\n    for _ in range(B):\n        # 2a. Resample A and B\n        A_star = rng.choice(sample_a, size=n_a, replace=True)\n        B_star = rng.choice(sample_b, size=n_b, replace=True)\n        \n        # 2b. Construct the least-favorable null CDF, G*(t)\n        t_grid_star_gen = np.unique(np.concatenate((A_star, B_star)))\n        ecdf_a_star_gen = ecdf(A_star, t_grid_star_gen)\n        ecdf_b_star_gen = ecdf(B_star, t_grid_star_gen)\n        g_star_cdf = np.minimum(ecdf_a_star_gen + ecdf_b_star_gen, 1.0)\n\n        # 2c. Sample AB* from G*(t) via inverse transform sampling\n        # First, get the PMF from the CDF\n        g_star_pmf = np.diff(g_star_cdf, prepend=0)\n        # Normalize to correct for potential floating-point inaccuracies\n        g_star_pmf /= np.sum(g_star_pmf)\n        \n        AB_star = rng.choice(t_grid_star_gen, size=n_ab, replace=True, p=g_star_pmf)\n        \n        # 2d. Compute the bootstrap statistic T_n*\n        T_n_star = compute_test_statistic(A_star, B_star, AB_star)\n        bootstrap_stats.append(T_n_star)\n\n    # 3. Calculate the p-value\n    bootstrap_stats = np.array(bootstrap_stats)\n    num_exceed = np.sum(bootstrap_stats >= T_n_obs)\n    p_value = (1 + num_exceed) / (1 + B)\n\n    # 4. Make a decision based on the significance level\n    return p_value = ALPHA\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test suite and print the results.\n    \"\"\"\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    test_cases = [\n        # Case 1 (race, large sample)\n        {'n_a': 600, 'n_b': 600, 'n_ab': 600, 'mu_a': 3.0, 'sigma_a': 0.4, 'mu_b': 3.0, 'sigma_b': 0.4, 'gamma': 1.0},\n        # Case 2 (coactivation, violation)\n        {'n_a': 400, 'n_b': 400, 'n_ab': 400, 'mu_a': 3.0, 'sigma_a': 0.4, 'mu_b': 3.0, 'sigma_b': 0.4, 'gamma': 0.8},\n        # Case 3 (race, small sample)\n        {'n_a': 120, 'n_b': 120, 'n_ab': 120, 'mu_a': 3.0, 'sigma_a': 0.5, 'mu_b': 3.0, 'sigma_b': 0.5, 'gamma': 1.0},\n        # Case 4 (asymmetric, near-boundary)\n        {'n_a': 400, 'n_b': 400, 'n_ab': 400, 'mu_a': 2.5, 'sigma_a': 0.5, 'mu_b': 4.0, 'sigma_b': 0.5, 'gamma': 0.95},\n    ]\n\n    results = []\n    for params in test_cases:\n        sample_a, sample_b, sample_ab = generate_dataset(rng, **params)\n        reject_h0 = perform_bootstrap_test(rng, sample_a, sample_b, sample_ab)\n        results.append(reject_h0)\n\n    # Print the final result in the exact specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}