{
    "hands_on_practices": [
        {
            "introduction": "At the heart of any race model lies a simple but powerful question: when multiple processes run in parallel, which one finishes first and how long does it take? This exercise explores the fundamental statistics of this \"winning time\" in the foundational case where each process has an exponentially distributed completion time . By deriving the distribution of the minimum of several such variables, you will build the mathematical engine that drives the simplest forms of race models.",
            "id": "4012846",
            "problem": "In a race model of saccadic initiation inspired by the Linear Approach to Threshold with Ergodic Rate (LATER) framework, suppose there are $n$ independent parallel channels, each representing a stochastic decision unit racing to reach a criterion. In this simplified setting, assume that the completion time for channel $i$ is an exponential random variable $T_i \\sim \\mathrm{Exp}(\\lambda)$ with rate parameter $\\lambda$ (in $\\mathrm{s}^{-1}$), and that the $T_i$ are independent and identically distributed across channels. Let $T_{\\min} = \\min\\{T_1,\\dots,T_n\\}$ denote the observed decision time in the race, corresponding to the earliest channel to finish.\n\nUsing only foundational properties of independence, survival functions, and the memoryless property of the exponential distribution (or an equivalent argument grounded in superposition of independent Poisson processes), derive the probability density function of $T_{\\min}$ and then compute its mean $\\mathbb{E}[T_{\\min}]$ and variance $\\operatorname{Var}(T_{\\min})$ as explicit functions of $n$ and $\\lambda$. Express time in seconds, and rates in $\\mathrm{s}^{-1}$, but do not include units in your final boxed answer.\n\nProvide your final answer as a row vector containing $\\mathbb{E}[T_{\\min}]$ and $\\operatorname{Var}(T_{\\min})$ in terms of $n$ and $\\lambda$. No rounding is required.",
            "solution": "The problem statement is first validated for correctness and solvability.\n\n### Step 1: Extract Givens\n- There are $n$ independent parallel channels.\n- The completion time for channel $i$ is a random variable $T_i$.\n- The distribution of each $T_i$ is exponential with rate parameter $\\lambda$: $T_i \\sim \\mathrm{Exp}(\\lambda)$.\n- The rate parameter $\\lambda$ has units of $\\mathrm{s}^{-1}$.\n- The random variables $T_i$ for $i=1, \\dots, n$ are independent and identically distributed (i.i.d.).\n- The observed decision time is $T_{\\min} = \\min\\{T_1,\\dots,T_n\\}$.\n- The task is to derive the probability density function (PDF) of $T_{\\min}$, and then compute its mean $\\mathbb{E}[T_{\\min}]$ and variance $\\operatorname{Var}(T_{\\min})$ as functions of $n$ and $\\lambda$.\n- The derivation must use foundational properties of independence, survival functions, and the memoryless property, or an equivalent argument based on the superposition of Poisson processes.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly grounded in probability theory and its application to computational neuroscience. Race models, particularly using exponential distributions, are a standard and well-established framework (like the LATER model) for studying decision-making processes such as saccadic eye movements. The problem is a classic exercise in the statistics of order statistics for i.i.d. random variables.\n- **Well-Posed:** The problem is well-posed. The objective is to find the distribution, mean, and variance of the minimum of a set of i.i.d. exponential random variables. This is a standard problem in probability theory with a unique, stable, and meaningful solution.\n- **Objective:** The problem is stated using precise, objective mathematical language.\n- **Flaws:** The problem statement exhibits none of the invalidity flaws. It is scientifically sound, formalizable, complete, realistic within a modeling context, well-posed, and non-trivial.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Derivation and Solution\nLet $T_i$ for $i=1, \\dots, n$ be independent and identically distributed random variables representing the completion times of $n$ channels. Each $T_i$ follows an exponential distribution with rate parameter $\\lambda > 0$, denoted as $T_i \\sim \\mathrm{Exp}(\\lambda)$.\n\nThe probability density function (PDF) for each $T_i$ is given by:\n$$f_{T_i}(t) = \\begin{cases} \\lambda \\exp(-\\lambda t) & \\text{if } t \\ge 0 \\\\ 0 & \\text{if } t < 0 \\end{cases}$$\nThe cumulative distribution function (CDF) for each $T_i$ is $F_{T_i}(t) = P(T_i \\le t) = 1 - \\exp(-\\lambda t)$ for $t \\ge 0$.\nThe survival function for each $T_i$ is $S_{T_i}(t) = P(T_i > t) = 1 - F_{T_i}(t) = \\exp(-\\lambda t)$ for $t \\ge 0$.\n\nWe are interested in the distribution of the random variable $T_{\\min} = \\min\\{T_1, \\dots, T_n\\}$. The most direct way to find the distribution of $T_{\\min}$ is by first finding its survival function, $S_{T_{\\min}}(t) = P(T_{\\min} > t)$.\n\nThe event $\\{T_{\\min} > t\\}$ occurs if and only if the completion time of *every* channel is greater than $t$. Mathematically, this is the intersection of the events $\\{T_1 > t\\}, \\{T_2 > t\\}, \\dots, \\{T_n > t\\}$.\n$$P(T_{\\min} > t) = P(T_1 > t \\text{ and } T_2 > t \\text{ and } \\dots \\text{ and } T_n > t)$$\nSince the random variables $T_i$ are independent, the probability of the intersection of these events is the product of their individual probabilities:\n$$P(T_{\\min} > t) = P(T_1 > t) P(T_2 > t) \\cdots P(T_n > t) = \\prod_{i=1}^{n} P(T_i > t)$$\nBecause the $T_i$ are also identically distributed, $P(T_i > t) = S_{T_i}(t) = \\exp(-\\lambda t)$ for all $i$. Substituting this into the product gives:\n$$S_{T_{\\min}}(t) = \\prod_{i=1}^{n} \\exp(-\\lambda t) = (\\exp(-\\lambda t))^n = \\exp(-n\\lambda t)$$\nThis expression, $S_{T_{\\min}}(t) = \\exp(-(n\\lambda)t)$, is the survival function for an exponential distribution with a rate parameter of $n\\lambda$.\n\nTo confirm and find the PDF, we first find the CDF of $T_{\\min}$:\n$$F_{T_{\\min}}(t) = P(T_{\\min} \\le t) = 1 - S_{T_{\\min}}(t) = 1 - \\exp(-n\\lambda t) \\quad \\text{for } t \\ge 0$$\nThe PDF of $T_{\\min}$ is the derivative of its CDF with respect to $t$:\n$$f_{T_{\\min}}(t) = \\frac{d}{dt} F_{T_{\\min}}(t) = \\frac{d}{dt} (1 - \\exp(-n\\lambda t)) = -(-n\\lambda)\\exp(-n\\lambda t) = n\\lambda \\exp(-n\\lambda t) \\quad \\text{for } t \\ge 0$$\nThis is precisely the PDF of an exponential distribution with rate parameter $n\\lambda$. Thus, we have shown that $T_{\\min} \\sim \\mathrm{Exp}(n\\lambda)$.\n\nAlternatively, as suggested by the prompt, one could consider the superposition of $n$ independent Poisson processes, each with rate $\\lambda$. The time to the first event in each process is an exponential random variable $T_i \\sim \\mathrm{Exp}(\\lambda)$. The superposition of these $n$ processes results in a single Poisson process with a combined rate of $\\Lambda = \\sum_{i=1}^n \\lambda = n\\lambda$. The variable $T_{\\min}$ represents the time to the very first event in this combined process. The waiting time for the first event in a Poisson process with rate $\\Lambda$ is exponentially distributed with that same rate. Therefore, $T_{\\min} \\sim \\mathrm{Exp}(n\\lambda)$, which confirms the previous result.\n\nNow, we compute the mean and variance of $T_{\\min}$. For a general random variable $X \\sim \\mathrm{Exp}(\\mu)$, the mean (expected value) and variance are given by:\n$$\\mathbb{E}[X] = \\frac{1}{\\mu}$$\n$$\\operatorname{Var}(X) = \\frac{1}{\\mu^2}$$\nIn our case, the random variable is $T_{\\min}$ and its rate parameter is $\\mu = n\\lambda$. Applying these formulas, we get:\nThe mean of $T_{\\min}$ is:\n$$\\mathbb{E}[T_{\\min}] = \\frac{1}{n\\lambda}$$\nThe variance of $T_{\\min}$ is:\n$$\\operatorname{Var}(T_{\\min}) = \\frac{1}{(n\\lambda)^2} = \\frac{1}{n^2\\lambda^2}$$\nThese are the explicit functions of $n$ and $\\lambda$ required by the problem. The final answer will be presented as a row vector containing these two results.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{n\\lambda} & \\frac{1}{n^{2}\\lambda^{2}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While simple race models provide a foundation, the LATER model offers a more neurophysiologically plausible account of decision-making. A hallmark of powerful scientific models is their ability to make specific, testable predictions about data, and this practice guides you through a key derivation connecting the LATER model's theoretical parameters—like the decision threshold $S$—to a directly observable feature of reaction time data: the slope of a reciprobit plot . Mastering this connection is crucial for understanding how abstract cognitive strategies, such as the speed-accuracy tradeoff, manifest in behavioral measurements.",
            "id": "4012834",
            "problem": "Consider the Linear Approach to Threshold with Ergodic Rate (LATER) model of reaction time, in which a decision signal starts at an initial level $S_{0}$ and rises linearly toward a decision threshold $S$ at a stochastic rate $r$. Assume $r$ is drawn independently on each trial from a normal (Gaussian) distribution with mean $\\mu$ and standard deviation $\\sigma$, and that the measured Reaction Time (RT) is given by $T = (S - S_{0})/r$ with no additive non-decision components. The reciprobit representation considers the reciprocal latency $x = 1/T$ on the horizontal axis and the probit (the inverse of the normal cumulative distribution) of the cumulative probability on the vertical axis. In this representation, the cumulative distribution of $x$ is mapped to a straight line.\n\nStarting from these definitions, derive the general expression for the reciprobit slope as a function of $S$, $S_{0}$, and $\\sigma$. Then, suppose $S$ is doubled to $2S$ while $S_{0}$, $\\mu$, and $\\sigma$ remain fixed. Compute the multiplicative change in the reciprobit slope (i.e., the ratio of the new slope to the original slope), and interpret this result in terms of the speed–accuracy tradeoff implied by the LATER model.\n\nProvide your final answer as a single closed-form analytic expression for the slope ratio. The requested ratio is dimensionless, so no units are required. No rounding is necessary.",
            "solution": "The problem asks for the derivation of the reciprobit slope for the LATER model and the analysis of how this slope changes when the decision threshold is altered.\n\nFirst, let us define the key quantities. The change in the decision signal required to reach the threshold is $\\Delta S = S - S_{0}$. The reaction time $T$ is given by:\n$$T = \\frac{\\Delta S}{r} = \\frac{S - S_{0}}{r}$$\nThe analysis is performed in the reciprobit-space, which uses the reciprocal of the latency, $x$, on the horizontal axis.\n$$x = \\frac{1}{T} = \\frac{r}{S - S_{0}}$$\nThe rate of rise, $r$, is a random variable drawn from a a normal distribution with mean $\\mu$ and standard deviation $\\sigma$:\n$$r \\sim N(\\mu, \\sigma^2)$$\nSince $x$ is a linear transformation of the normally distributed random variable $r$ (i.e., $x = a \\cdot r$ where $a = 1/(S-S_0)$ is a constant for any given trial), $x$ is also a normally distributed random variable. We can find its mean, $\\mu_x$, and standard deviation, $\\sigma_x$, as follows:\nThe mean of $x$ is:\n$$\\mu_x = E[x] = E\\left[\\frac{r}{S - S_{0}}\\right] = \\frac{E[r]}{S - S_{0}} = \\frac{\\mu}{S - S_{0}}$$\nThe variance of $x$ is:\n$$\\sigma_x^2 = \\text{Var}(x) = \\text{Var}\\left(\\frac{r}{S - S_{0}}\\right) = \\frac{\\text{Var}(r)}{(S - S_{0})^2} = \\frac{\\sigma^2}{(S - S_{0})^2}$$\nThe standard deviation of $x$ is therefore:\n$$\\sigma_x = \\sqrt{\\frac{\\sigma^2}{(S - S_{0})^2}} = \\frac{\\sigma}{|S - S_{0}|}$$\nSince the decision signal rises to the threshold, we must have $S > S_{0}$, so $|S - S_{0}| = S - S_{0}$. Thus:\n$$\\sigma_x = \\frac{\\sigma}{S - S_{0}}$$\nSo, the distribution of the reciprocal latency is $x \\sim N(\\mu_x, \\sigma_x^2)$.\n\nThe reciprobit plot displays the probit of the cumulative probability, $z = \\Phi^{-1}(P(X \\le x))$, on the vertical axis against the reciprocal latency, $x$, on the horizontal axis. Here, $\\Phi^{-1}$ is the inverse of the standard normal cumulative distribution function (CDF), also known as the probit function.\n\nThe cumulative probability for a normally distributed variable $x \\sim N(\\mu_x, \\sigma_x^2)$ is given by:\n$$P(X \\le x) = \\Phi\\left(\\frac{x - \\mu_x}{\\sigma_x}\\right)$$\nSubstituting this into the definition of the vertical axis variable $z$:\n$$z = \\Phi^{-1}\\left( \\Phi\\left(\\frac{x - \\mu_x}{\\sigma_x}\\right) \\right) = \\frac{x - \\mu_x}{\\sigma_x}$$\nThis equation can be rearranged into the standard form of a straight line, $z = mx + c$:\n$$z = \\left(\\frac{1}{\\sigma_x}\\right) x - \\frac{\\mu_x}{\\sigma_x}$$\nThis confirms the problem's premise that the reciprobit plot is a straight line. The slope of this line, which we will call $m_{\\text{recip}}$, is the coefficient of $x$.\n\n**Part 1: General Expression for the Reciprobit Slope**\nThe slope is given by:\n$$m_{\\text{recip}} = \\frac{1}{\\sigma_x}$$\nSubstituting the expression we derived for $\\sigma_x$:\n$$m_{\\text{recip}} = \\frac{1}{\\frac{\\sigma}{S - S_{0}}} = \\frac{S - S_{0}}{\\sigma}$$\nThis is the general expression for the reciprobit slope as a function of the threshold $S$, the starting level $S_{0}$, and the standard deviation of the rate, $\\sigma$.\n\n**Part 2: Multiplicative Change in the Slope**\nThe original slope, with decision threshold $S$, is:\n$$m_{orig} = \\frac{S - S_{0}}{\\sigma}$$\nNow, the threshold is doubled to $S' = 2S$, while all other parameters ($S_{0}$, $\\mu$, $\\sigma$) remain fixed. The new slope, $m_{new}$, is:\n$$m_{new} = \\frac{S' - S_{0}}{\\sigma} = \\frac{2S - S_{0}}{\\sigma}$$\nThe problem asks for the multiplicative change, which is the ratio of the new slope to the original slope:\n$$\\text{Ratio} = \\frac{m_{new}}{m_{orig}} = \\frac{\\frac{2S - S_{0}}{\\sigma}}{\\frac{S - S_{0}}{\\sigma}}$$\nThe factor $\\sigma$ cancels out, leaving:\n$$\\text{Ratio} = \\frac{2S - S_{0}}{S - S_{0}}$$\n\n**Part 3: Interpretation**\nIn the context of decision-making models like LATER, the speed-accuracy tradeoff describes the phenomenon where faster decisions are typically more error-prone, while more accurate decisions take longer. Within the LATER model, this tradeoff is implemented by adjusting the decision threshold, $S$. A higher threshold $S$ means that more evidence must be accumulated before a decision is made. This leads to a longer average reaction time ($E[T] \\approx (S - S_0)/\\mu$) but improves accuracy by reducing the chance of a premature decision based on noisy evidence.\n\nThe derivation shows that increasing the threshold from $S$ to $2S$ (a shift towards accuracy) increases the reciprobit slope by a factor of $(2S - S_0)/(S - S_0)$. Since $S > S_0$ for a decision to occur, this ratio is always greater than $1$. For instance, in the common case where the decision process starts from zero ($S_0 = 0$), the ratio is $(2S - 0)/(S - 0) = 2$, meaning the slope doubles.\n\nThis result provides a direct, quantitative prediction of the LATER model: a strategic adjustment by a subject to be more accurate (i.e., increasing their internal threshold $S$) will manifest in experimental data as a specific, predictable increase in the steepness of the reciprobit plot. The slope of this plot is an observable quantity that can be estimated from a set of reaction times. Therefore, the derived ratio connects a theoretical cognitive parameter ($S$) to a measurable feature of behavioral data, providing a powerful way to test the model's assumptions about how speed-accuracy adjustments are neurally implemented.",
            "answer": "$$\\boxed{\\frac{2S - S_{0}}{S - S_{0}}}$$"
        },
        {
            "introduction": "Theoretical derivations provide the predictions, but computational statistics provides the tools to test them against real-world data. All independent race models, regardless of their specific assumptions about rate distributions, must obey a fundamental constraint known as the race model inequality, or Miller bound. This hands-on coding exercise challenges you to implement a sophisticated bootstrap procedure to test whether a given dataset respects this inequality . This practice bridges the gap between abstract theory and empirical science, equipping you with a powerful method for identifying when a simple race is insufficient and more complex mechanisms, like neural coactivation, might be at play.",
            "id": "4012824",
            "problem": "You are given independent samples of reaction times from three conditions in a redundant signals paradigm: stimulus $A$ alone, stimulus $B$ alone, and redundant stimuli $AB$. Assume reaction times are strictly positive and are measured in seconds. The Linear Approach to Threshold with Ergodic Rate (LATER) model posits that decision time $T$ can be represented as $T = S / R$ where $S$ is a fixed decision bound and $R$ is a random rate variable; in practice, empirical distributions for $A$ and $B$ will be used directly.\n\nDefine the cumulative distribution functions $F_A(t)$, $F_B(t)$, and $F_{AB}(t)$ of the reaction times for the three conditions. The classical race inequality (Miller bound) states that, in the absence of coactivation, the redundant condition $AB$ must satisfy the pointwise bound\n$$\nF_{AB}(t) \\le \\min\\big(F_A(t) + F_B(t), \\, 1\\big) \\quad \\text{for all } t \\in \\mathbb{R}_{\\ge 0}.\n$$\nYou must design and implement a statistical procedure to test for violations of this bound based on finite samples, using empirical cumulative distribution functions and a bootstrap calibrated under a least-favorable null model.\n\nFormulate the hypotheses as follows:\n- Null hypothesis $H_0$: for all $t \\in \\mathbb{R}_{\\ge 0}$, $F_{AB}(t) \\le \\min\\big(F_A(t) + F_B(t), \\, 1\\big)$.\n- Alternative hypothesis $H_1$: there exists $t \\in \\mathbb{R}_{\\ge 0}$ such that $F_{AB}(t) > \\min\\big(F_A(t) + F_B(t), \\, 1\\big)$.\n\nYour test must use the following ingredients:\n- Empirical cumulative distribution functions $\\hat F_A(t)$, $\\hat F_B(t)$, and $\\hat F_{AB}(t)$ computed from the samples.\n- A test statistic defined as the supremum of the pointwise excess of the empirical redundant cumulative distribution over the empirical Miller bound:\n$$\nT_n \\equiv \\sup_{t \\in \\mathcal{T}} \\left\\{ \\hat F_{AB}(t) - \\min\\big(\\hat F_A(t) + \\hat F_B(t), \\, 1\\big) \\right\\},\n$$\nwhere $\\mathcal{T}$ is a finite grid consisting of all observed sample times across all three conditions.\n- A bootstrap procedure that approximates the null distribution of $T_n$ by simulating bootstrap datasets under a least-favorable null model constructed from the empirical marginals. Specifically, for each bootstrap replication:\n  1. Resample with replacement $n_A$ draws from the $A$-only sample to form a bootstrap $A^\\ast$, and resample with replacement $n_B$ draws from the $B$-only sample to form a bootstrap $B^\\ast$.\n  2. Compute the bootstrap empirical bound $G^\\ast(t) = \\min\\big(\\hat F_{A^\\ast}(t) + \\hat F_{B^\\ast}(t), \\, 1\\big)$ on the grid of unique times observed in $A^\\ast \\cup B^\\ast$.\n  3. Sample $n_{AB}$ redundant times $AB^\\ast$ from the discrete distribution with cumulative distribution function $G^\\ast(t)$, implemented by inverse transform on that grid.\n  4. Compute the bootstrap statistic $T_n^\\ast$ using $A^\\ast$, $B^\\ast$, and $AB^\\ast$ exactly as for $T_n$.\n- The bootstrap $p$-value is estimated as\n$$\n\\hat p = \\frac{1 + \\sum_{b=1}^B \\mathbf{1}\\{T_{n,b}^\\ast \\ge T_n\\}}{1 + B},\n$$\nand the test rejects $H_0$ at level $\\alpha = 0.05$.\n\nImplement this test and apply it to the following test suite of synthetic datasets, each generated from the LATER model for $A$ and $B$ and a race of two LATER processes for $AB$ with an optional multiplicative coactivation factor $\\gamma$:\n- For each condition $X \\in \\{A, B\\}$, generate rates $R_X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$ truncated to $R_X > 0$, set $S = 1$, and compute $T_X = S / R_X$. For the redundant condition, generate independent draws $T_A'$ and $T_B'$ in the same way and set $T_{AB} = \\gamma \\cdot \\min(T_A', T_B')$. All times must be measured in seconds.\n- The test suite consists of the following parameter sets, each with a fixed random seed for reproducibility:\n  1. Case $1$ (race, large sample): $n_A = n_B = n_{AB} = 600$, $\\mu_A = 3.0$, $\\sigma_A = 0.4$, $\\mu_B = 3.0$, $\\sigma_B = 0.4$, $\\gamma = 1.0$.\n  2. Case $2$ (coactivation, violation): $n_A = n_B = n_{AB} = 400$, $\\mu_A = 3.0$, $\\sigma_A = 0.4$, $\\mu_B = 3.0$, $\\sigma_B = 0.4$, $\\gamma = 0.8$.\n  3. Case $3$ (race, small sample): $n_A = n_B = n_{AB} = 120$, $\\mu_A = 3.0$, $\\sigma_A = 0.5$, $\\mu_B = 3.0$, $\\sigma_B = 0.5$, $\\gamma = 1.0$.\n  4. Case $4$ (asymmetric, near-boundary): $n_A = n_B = n_{AB} = 400$, $\\mu_A = 2.5$, $\\sigma_A = 0.5$, $\\mu_B = 4.0$, $\\sigma_B = 0.5$, $\\gamma = 0.95$.\n- Use $B = 1000$ bootstrap replications per case and a single fixed seed across all cases so that results are exactly reproducible.\n\nYour program must:\n- Generate synthetic datasets according to the above specifications in seconds.\n- Compute the test statistic and bootstrap $p$-value for each case.\n- Decide whether to reject $H_0$ at level $\\alpha = 0.05$ for each case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases $1$ through $4$, where each entry is a boolean indicating whether $H_0$ is rejected for that case. For example: \"[True,False,False,True]\".",
            "solution": "The task is to implement a bootstrap hypothesis test for the race model inequality, also known as the Miller bound, on reaction time data. The inequality is a fundamental prediction of parallel race models, which assume that for redundant stimuli $A$ and $B$, the response is triggered by whichever of the two independent processing channels finishes first.\n\nLet $F_A(t)$, $F_B(t)$, and $F_{AB}(t)$ be the cumulative distribution functions (CDFs) of reaction times for stimuli $A$ alone, $B$ alone, and redundant stimuli $AB$, respectively. The race model inequality states that for all non-negative times $t \\ge 0$,\n$$F_{AB}(t) \\le \\min\\big(F_A(t) + F_B(t), 1\\big)$$\nA violation of this inequality, particularly $F_{AB}(t) > F_A(t) + F_B(t)$, suggests a coactivation mechanism where the presence of two stimuli leads to processing that is faster than what would be predicted by a simple race between independent processes.\n\nThe hypothesis test is formulated as:\n- Null hypothesis $H_0$: $F_{AB}(t) \\le \\min(F_A(t) + F_B(t), 1)$ for all $t \\ge 0$. The observed data are consistent with a race model.\n- Alternative hypothesis $H_1$: There exists a $t \\ge 0$ such that $F_{AB}(t) > \\min(F_A(t) + F_B(t), 1)$. The data are not consistent with a race model, suggesting coactivation.\n\nSince the true CDFs are unknown, we use their empirical counterparts, the empirical cumulative distribution functions (ECDFs), denoted $\\hat{F}_A(t)$, $\\hat{F}_B(t)$, and $\\hat{F}_{AB}(t)$, calculated from finite samples of size $n_A$, $n_B$, and $n_{AB}$.\n\nThe core of the solution involves the following steps:\n\n1.  **Test Statistic Calculation**: The degree of violation of the inequality in the observed data is quantified by the test statistic $T_n$. This is defined as the maximum observed difference between the redundant ECDF and the empirical Miller bound, taken over all unique time points observed in the data. Let $\\mathcal{T}$ be the set of all unique reaction times across the three experimental conditions. The statistic is:\n    $$T_n = \\sup_{t \\in \\mathcal{T}} \\left\\{ \\hat{F}_{AB}(t) - \\min\\big(\\hat{F}_A(t) + \\hat{F}_B(t), 1\\big) \\right\\}$$\n    A larger positive value of $T_n$ provides stronger evidence against the null hypothesis $H_0$.\n\n2.  **Bootstrap Estimation of the Null Distribution**: The sampling distribution of $T_n$ under $H_0$ is generally intractable. We therefore approximate it using a bootstrap procedure. The crucial aspect of this procedure is to generate bootstrap samples that conform to the null hypothesis. We simulate from a \"least-favorable\" null model, which lies on the boundary of the inequality, i.e., where $F_{AB}(t) = \\min(F_A(t) + F_B(t), 1)$. This choice makes the test sensitive to any violations. The procedure for each of the $B=1000$ bootstrap replications is as follows:\n    a. Generate bootstrap samples $A^\\ast$ and $B^\\ast$ by resampling with replacement from the original single-stimulus samples for conditions $A$ and $B$, respectively.\n    b. Construct the ECDF for the bootstrap marginals, $\\hat{F}_{A^\\ast}(t)$ and $\\hat{F}_{B^\\ast}(t)$.\n    c. Define a bootstrap null CDF, $G^\\ast(t) = \\min\\big(\\hat{F}_{A^\\ast}(t) + \\hat{F}_{B^\\ast}(t), 1\\big)$. This defines a discrete probability distribution over the time points observed in $A^\\ast \\cup B^\\ast$. This distribution represents the fastest possible redundant reaction times that are still consistent with a race model based on the bootstrap marginals.\n    d. Generate a bootstrap redundant sample, $AB^\\ast$, of size $n_{AB}$ by sampling from the discrete distribution defined by $G^\\ast(t)$ using the inverse transform sampling method. By construction, the sample triplet $(A^\\ast, B^\\ast, AB^\\ast)$ satisfies the null hypothesis.\n    e. Calculate the bootstrap test statistic $T_n^\\ast$ for this bootstrap triplet in the same manner as the observed statistic $T_n$.\n\n3.  **P-value Calculation and Decision**: After generating $B$ bootstrap statistics $\\{T_{n,1}^\\ast, \\dots, T_{n,B}^\\ast\\}$, we have an empirical approximation of the null distribution of $T_n$. The one-sided $p$-value is estimated as the proportion of bootstrap statistics that are equal to or greater than the originally observed statistic $T_n$:\n    $$\\hat{p} = \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\{T_{n,b}^\\ast \\ge T_n\\}}{1 + B}$$\n    The addition of $1$ to the numerator and denominator is a standard convention to prevent $p$-values of $0$ and ensures conservative behavior. The null hypothesis $H_0$ is rejected at significance level $\\alpha=0.05$ if $\\hat{p} \\le \\alpha$.\n\n4.  **Data Generation**: The synthetic datasets for the test cases are generated using the LATER model. For each single-stimulus condition $X \\in \\{A, B\\}$, reaction times $T_X$ are computed as $T_X=S/R_X$, where the decision bound $S$ is fixed at $1$ and the rate $R_X$ is drawn from a truncated Normal distribution $\\mathcal{N}(\\mu_X, \\sigma_X^2)$ with the constraint $R_X > 0$. For the redundant condition, two independent RTs, $T_A'$ and $T_B'$, are generated, and the redundant RT is taken as $T_{AB} = \\gamma \\cdot \\min(T_A', T_B')$. A coactivation factor $\\gamma < 1$ leads to faster responses than a pure race model ($\\gamma = 1$), which is expected to cause a violation of the Miller bound.\n\nThe implementation will apply this statistical procedure to four distinct test cases, determining for each whether to reject the null hypothesis of a race model.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import truncnorm\n\n# Define global parameters as specified in the problem\nB = 1000\nALPHA = 0.05\n# A single fixed seed for reproducibility across all cases\nRANDOM_SEED = 42\n\ndef generate_LATER_RTs(rng, n, mu, sigma, S=1.0):\n    \"\"\"\n    Generates reaction times from the LATER model S/R, where R is from a\n    truncated normal distribution.\n    \"\"\"\n    # The lower bound is 0. In standard-normal units for truncnorm, this is (0 - mu) / sigma.\n    a = -mu / sigma\n    b = np.inf\n    rates = truncnorm.rvs(a, b, loc=mu, scale=sigma, size=n, random_state=rng)\n    # Ensure rates are strictly positive to avoid division by zero\n    rates[rates = 0] = 1e-9  # A small positive floor\n    return S / rates\n\ndef generate_dataset(rng, n_a, n_b, n_ab, mu_a, sigma_a, mu_b, sigma_b, gamma):\n    \"\"\"\n    Generates a full dataset for conditions A, B, and AB based on LATER model parameters.\n    \"\"\"\n    sample_a = generate_LATER_RTs(rng, n_a, mu_a, sigma_a)\n    sample_b = generate_LATER_RTs(rng, n_b, mu_b, sigma_b)\n\n    # For the redundant condition, generate two sets of latent RTs for the race\n    t_a_prime = generate_LATER_RTs(rng, n_ab, mu_a, sigma_a)\n    t_b_prime = generate_LATER_RTs(rng, n_ab, mu_b, sigma_b)\n    \n    # The redundant RT is the coactivated minimum of the two latent RTs\n    sample_ab = gamma * np.minimum(t_a_prime, t_b_prime)\n\n    return sample_a, sample_b, sample_ab\n\ndef ecdf(sample, t_grid):\n    \"\"\"\n    Computes the Empirical Cumulative Distribution Function values for a sample\n    at specified time points.\n    \"\"\"\n    if len(sample) == 0:\n        return np.zeros_like(t_grid, dtype=float)\n    # Using np.searchsorted is efficient for this task\n    sample_sorted = np.sort(sample)\n    return np.searchsorted(sample_sorted, t_grid, side='right') / len(sample)\n\ndef compute_test_statistic(sample_a, sample_b, sample_ab):\n    \"\"\"\n    Computes the test statistic T_n, the supremum of the violation of the Miller bound.\n    The supremum is taken over the grid of all observed time points.\n    \"\"\"\n    # Create the grid of all unique observed time points\n    t_grid = np.unique(np.concatenate((sample_a, sample_b, sample_ab)))\n\n    # Compute ECDFs for all three conditions on the common grid\n    ecdf_a = ecdf(sample_a, t_grid)\n    ecdf_b = ecdf(sample_b, t_grid)\n    ecdf_ab = ecdf(sample_ab, t_grid)\n\n    # Calculate the Miller bound\n    miller_bound = np.minimum(ecdf_a + ecdf_b, 1.0)\n    \n    # The test statistic is the maximum positive deviation from the bound\n    test_statistic = np.max(ecdf_ab - miller_bound)\n    \n    return test_statistic\n\ndef perform_bootstrap_test(rng, sample_a, sample_b, sample_ab):\n    \"\"\"\n    Performs the full bootstrap hypothesis test.\n    \"\"\"\n    n_a, n_b, n_ab = len(sample_a), len(sample_b), len(sample_ab)\n\n    # 1. Compute the observed test statistic\n    T_n_obs = compute_test_statistic(sample_a, sample_b, sample_ab)\n\n    bootstrap_stats = []\n    for _ in range(B):\n        # 2a. Resample A and B\n        A_star = rng.choice(sample_a, size=n_a, replace=True)\n        B_star = rng.choice(sample_b, size=n_b, replace=True)\n        \n        # 2b. Construct the least-favorable null CDF, G*(t)\n        t_grid_star_gen = np.unique(np.concatenate((A_star, B_star)))\n        ecdf_a_star_gen = ecdf(A_star, t_grid_star_gen)\n        ecdf_b_star_gen = ecdf(B_star, t_grid_star_gen)\n        g_star_cdf = np.minimum(ecdf_a_star_gen + ecdf_b_star_gen, 1.0)\n\n        # 2c. Sample AB* from G*(t) via inverse transform sampling\n        # First, get the PMF from the CDF\n        g_star_pmf = np.diff(g_star_cdf, prepend=0)\n        # Normalize to correct for potential floating-point inaccuracies\n        g_star_pmf /= np.sum(g_star_pmf)\n        \n        AB_star = rng.choice(t_grid_star_gen, size=n_ab, replace=True, p=g_star_pmf)\n        \n        # 2d. Compute the bootstrap statistic T_n*\n        T_n_star = compute_test_statistic(A_star, B_star, AB_star)\n        bootstrap_stats.append(T_n_star)\n\n    # 3. Calculate the p-value\n    bootstrap_stats = np.array(bootstrap_stats)\n    num_exceed = np.sum(bootstrap_stats >= T_n_obs)\n    p_value = (1 + num_exceed) / (1 + B)\n\n    # 4. Make a decision based on the significance level\n    return p_value = ALPHA\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test suite and print the results.\n    \"\"\"\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    test_cases = [\n        # Case 1 (race, large sample)\n        {'n_a': 600, 'n_b': 600, 'n_ab': 600, 'mu_a': 3.0, 'sigma_a': 0.4, 'mu_b': 3.0, 'sigma_b': 0.4, 'gamma': 1.0},\n        # Case 2 (coactivation, violation)\n        {'n_a': 400, 'n_b': 400, 'n_ab': 400, 'mu_a': 3.0, 'sigma_a': 0.4, 'mu_b': 3.0, 'sigma_b': 0.4, 'gamma': 0.8},\n        # Case 3 (race, small sample)\n        {'n_a': 120, 'n_b': 120, 'n_ab': 120, 'mu_a': 3.0, 'sigma_a': 0.5, 'mu_b': 3.0, 'sigma_b': 0.5, 'gamma': 1.0},\n        # Case 4 (asymmetric, near-boundary)\n        {'n_a': 400, 'n_b': 400, 'n_ab': 400, 'mu_a': 2.5, 'sigma_a': 0.5, 'mu_b': 4.0, 'sigma_b': 0.5, 'gamma': 0.95},\n    ]\n\n    results = []\n    for params in test_cases:\n        sample_a, sample_b, sample_ab = generate_dataset(rng, **params)\n        reject_h0 = perform_bootstrap_test(rng, sample_a, sample_b, sample_ab)\n        results.append(reject_h0)\n\n    # Print the final result in the exact specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}