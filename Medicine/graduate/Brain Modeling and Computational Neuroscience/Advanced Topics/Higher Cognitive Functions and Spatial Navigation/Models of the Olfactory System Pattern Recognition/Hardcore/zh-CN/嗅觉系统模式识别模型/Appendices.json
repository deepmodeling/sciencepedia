{
    "hands_on_practices": [
        {
            "introduction": "在嗅觉系统中，来自嗅觉感受器神经元的信号在嗅球的嗅小球层进行汇集和初步处理。由于不同感受器可能对相似的化学特征有反应，这些原始输入信号往往存在相关性。 这项练习将带你通过主成分分析（PCA）和白化变换，亲手实践如何消除这些相关性，将原始的嗅小球响应向量转换为一个更高效、已解耦的表征，为后续的模式识别任务（如稀疏编码）奠定基础。",
            "id": "4000518",
            "problem": "在哺乳动物嗅觉系统的计算模型中，嗅小球层将受体神经元的输入聚合成一个群体响应向量 $g \\in \\mathbb{R}^{n}$。该向量在不同气味呈现下的变异性通常被建模为零均值多元高斯分布，其协方差矩阵为 $\\Sigma_{g} \\in \\mathbb{R}^{n \\times n}$。在模式识别和下游处理（例如，在基于主成分分析(PCA)的预处理中，或在稀疏编码之前的白化中）中，一个标准步骤是通过将 $g$ 映射到一个线性变换后的表示 $m \\in \\mathbb{R}^{n}$ 来对响应变异性进行去相关和归一化，使得变换后表示的协方差为单位矩阵。该变换应从第一性原理推导得出：使用协方差的定义以及对称正定矩阵的正交对角化。\n\n考虑一个大小为 $n = 3$ 的嗅小球群体，其协方差为\n$$\n\\Sigma_{g} \\;=\\;\n\\begin{pmatrix}\n2  1  0 \\\\\n1  2  0 \\\\\n0  0  2\n\\end{pmatrix},\n$$\n以及一次单次试验响应\n$$\ng = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix}.\n$$\n执行以下操作：\n\n1. 从以下事实出发：对于任何确定性矩阵 $A$，协方差的变换规律为 $\\mathrm{Cov}(A x) = A \\,\\mathrm{Cov}(x)\\, A^{\\top}$；$\\Sigma_{g}$ 是对称正定的；任何此类矩阵都容许正交对角化。通过找到 $\\Sigma_{g}$ 的标准正交特征向量和对应的特征值来计算主成分。将特征值按严格降序排列，并相应地排列特征向量。根据约定，即每个特征向量的第一个非零分量为正，来固定其符号。\n\n2. 仅使用上述原理（不引用问题中陈述的任何快捷公式），推导一个线性变换 $W$，使得变换后的变量 $m = W g$ 具有单位协方差，即 $\\mathrm{Cov}(m) = I$。解释为什么您构造的 $W$ 对于协方差为 $\\Sigma_{g}$ 的任意零均值 $g$ 都能实现 $\\mathrm{Cov}(m) = I$。\n\n3. 使用您推导出的 $W$ 计算给定 $g$ 的白化表示 $m$，并以精确的解析形式呈现结果。请勿近似；无需四舍五入。\n\n将您的最终答案 $m$ 表示为单个行向量，并使用 LaTeX 的 `pmatrix` 环境将其括起（例如 $\\begin{pmatrix} a & b & c \\end{pmatrix}$）。无需单位。",
            "solution": "该问题已经过验证，被认为是有效的。它在计算神经科学和线性代数方面具有科学依据，问题提法恰当，有唯一且稳定的解，并以客观、正式的语言表述。计算所需的所有数据和条件都已提供且内部一致。该问题是数据变换（白化）中的一个标准练习，可以使用指定的第一性原理求解。\n\n解答过程按题目要求分为三个部分。\n\n### 第1部分：$\\Sigma_g$ 的主成分分析\n\n响应分布的主成分是协方差矩阵 $\\Sigma_g$ 的特征向量。我们首先找到 $\\Sigma_g$ 的特征值和对应的特征向量。\n\n给定的协方差矩阵是：\n$$\n\\Sigma_g = \\begin{pmatrix} 2  1  0 \\\\ 1  2  0 \\\\ 0  0  2 \\end{pmatrix}\n$$\n特征值 $\\lambda$ 是特征方程 $\\det(\\Sigma_g - \\lambda I) = 0$ 的根，其中 $I$ 是 $3 \\times 3$ 的单位矩阵。\n$$\n\\det \\begin{pmatrix} 2-\\lambda  1  0 \\\\ 1  2-\\lambda  0 \\\\ 0  0  2-\\lambda \\end{pmatrix} = 0\n$$\n沿第三行展开行列式：\n$$\n(2-\\lambda) \\det \\begin{pmatrix} 2-\\lambda  1 \\\\ 1  2-\\lambda \\end{pmatrix} = (2-\\lambda) \\left[ (2-\\lambda)^2 - 1 \\right] = 0\n$$\n该方程产生三个特征值。一个根显然是 $\\lambda = 2$。另外两个根由 $(2-\\lambda)^2 - 1 = 0$ 求得，这意味着 $2-\\lambda = \\pm 1$。如果 $2-\\lambda = 1$，则 $\\lambda = 1$。如果 $2-\\lambda = -1$，则 $\\lambda = 3$。特征值为 $\\{3, 2, 1\\}$。按要求严格降序排列：$\\lambda_1 = 3$，$\\lambda_2 = 2$，$\\lambda_3 = 1$。由于所有特征值均为正，矩阵 $\\Sigma_g$ 是正定的，正如问题所述。\n\n接下来，我们为每个特征值 $\\lambda_i$ 找到对应的标准正交特征向量 $v_i$。\n\n对于 $\\lambda_1 = 3$：\n我们求解 $(\\Sigma_g - 3I)v = 0$：\n$$\n\\begin{pmatrix} -1  1  0 \\\\ 1  -1  0 \\\\ 0  0  -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n该系统给出的方程是 $-x+y=0$（或 $x=y$）和 $-z=0$。一个特征向量的形式为 $\\begin{pmatrix} c \\\\ c \\\\ 0 \\end{pmatrix}$。要使其成为单位向量，需满足 $c^2 + c^2 = 1 \\implies 2c^2=1 \\implies c = \\pm \\frac{1}{\\sqrt{2}}$。根据约定，第一个非零分量必须为正，因此我们选择 $c = \\frac{1}{\\sqrt{2}}$。\n$$\nv_1 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}\n$$\n\n对于 $\\lambda_2 = 2$：\n我们求解 $(\\Sigma_g - 2I)v = 0$：\n$$\n\\begin{pmatrix} 0  1  0 \\\\ 1  0  0 \\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n这给出 $y=0$ 和 $x=0$。分量 $z$ 是自由的。一个特征向量的形式为 $\\begin{pmatrix} 0 \\\\ 0 \\\\ c \\end{pmatrix}$。要使其成为单位向量，需满足 $c^2=1 \\implies c=\\pm 1$。第一个非零分量是 $z$，因此我们选择 $c=1$。\n$$\nv_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n\n对于 $\\lambda_3 = 1$：\n我们求解 $(\\Sigma_g - 1I)v = 0$：\n$$\n\\begin{pmatrix} 1  1  0 \\\\ 1  1  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n这给出 $x+y=0$（或 $y=-x$）和 $z=0$。一个特征向量的形式为 $\\begin{pmatrix} c \\\\ -c \\\\ 0 \\end{pmatrix}$。要使其成为单位向量，需满足 $c^2 + (-c)^2 = 1 \\implies 2c^2=1 \\implies c = \\pm \\frac{1}{\\sqrt{2}}$。第一个非零分量是 $x$，因此我们选择 $c = \\frac{1}{\\sqrt{2}}$。\n$$\nv_3 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}\n$$\n\n主成分（特征向量）及其对应的特征值（方差）是：\n$\\lambda_1=3, v_1 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix}^\\top$\n$\\lambda_2=2, v_2 = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}^\\top$\n$\\lambda_3=1, v_3 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix}^\\top$\n\n### 第2部分：白化变换 $W$ 的推导\n\n我们的任务是找到一个线性变换矩阵 $W$，使得对于变换后的变量 $m = Wg$，其协方差矩阵是单位矩阵，即 $\\mathrm{Cov}(m) = I$。\n已知 $\\mathrm{Cov}(g) = \\Sigma_g$ 以及协方差变换法则 $\\mathrm{Cov}(Wg) = W \\mathrm{Cov}(g) W^\\top$。因此，我们需要：\n$$\nW \\Sigma_g W^\\top = I\n$$\n从第1部分可知，由于 $\\Sigma_g$ 是一个对称矩阵，它可以进行正交对角化 $\\Sigma_g = V \\Lambda V^\\top$，其中 $V$ 是以标准正交特征向量 $v_1, v_2, v_3$ 为列的正交矩阵，$\\Lambda$ 是由相应特征值 $\\lambda_1, \\lambda_2, \\lambda_3$ 构成的对角矩阵。\n$$\nV = \\begin{pmatrix} v_1 & v_2 & v_3 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\\\ 0 & 1 & 0 \\end{pmatrix} \\quad \\text{以及} \\quad \\Lambda = \\begin{pmatrix} 3 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n将对角化代入我们的要求中：\n$$\nW (V \\Lambda V^\\top) W^\\top = I\n$$\n为了求解 $W$，我们可以为 $W$ 设定一个能简化此方程的形式。白化过程包括两个步骤：首先旋转数据使其与主轴对齐，然后缩放每个新坐标。旋转通过 $V^\\top$ 实现。缩放通过一个对角矩阵实现。我们假设 $W = S V^\\top$，其中 $S$ 是一个待定的缩放矩阵。\n$$\n(S V^\\top) (V \\Lambda V^\\top) (S V^\\top)^\\top = I\n$$\n利用属性 $(AB)^\\top = B^\\top A^\\top$ 和 $V$ 的正交性 ($V^\\top V = I$)：\n$$\nS (V^\\top V) \\Lambda (V^\\top V) S^\\top = S \\Lambda S^\\top = I\n$$\n由于 $\\Lambda$ 是一个对角线上为正项 $\\lambda_i$ 的对角矩阵，我们可以选择 $S$ 为一个“反转”$\\Lambda$ 的对角矩阵。令 $S = \\Lambda^{-1/2}$，定义为对角项为 $\\frac{1}{\\sqrt{\\lambda_i}}$ 的对角矩阵。\n$$\n\\Lambda^{-1/2} = \\begin{pmatrix} \\frac{1}{\\sqrt{\\lambda_1}} & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{\\lambda_2}} & 0 \\\\ 0 & 0 & \\frac{1}{\\sqrt{\\lambda_3}} \\end{pmatrix}\n$$\n由于 $\\Lambda^{-1/2}$ 是对角矩阵，所以它是对称的，因此 $S^\\top = S = \\Lambda^{-1/2}$。我们的方程变为：\n$$\n\\Lambda^{-1/2} \\Lambda \\Lambda^{-1/2} = I\n$$\n这是成立的，因为对于每个对角元素 $i$，我们有 $(\\frac{1}{\\sqrt{\\lambda_i}}) (\\lambda_i) (\\frac{1}{\\sqrt{\\lambda_i}}) = 1$。因此，一个有效的白化矩阵是 $W = \\Lambda^{-1/2} V^\\top$。对于任何具有对称正定协方差矩阵 $\\Sigma_g$ 的零均值随机向量 $g$，这种构造都能实现 $\\mathrm{Cov}(m)=I$，因为它仅依赖于 $\\Sigma_g$ 的谱分解的存在性。\n\n### 第3部分：白化表示 $m$ 的计算\n\n现在我们使用推导出的 $W$ 和给定的向量 $g$ 来计算 $m = Wg$。\n各个组成部分是：\n$$\n\\Lambda^{-1/2} = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n$$\nV^\\top = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix}\n$$\n$$\ng = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix}\n$$\n变换为 $m = (\\Lambda^{-1/2} V^\\top) g$。首先，我们计算投影 $V^\\top g$：\n$$\nV^\\top g = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} \\\\ 2 \\\\ \\frac{3}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{\\sqrt{2}} \\\\ 2 \\\\ \\frac{2}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 2\\sqrt{2} \\\\ 2 \\\\ \\sqrt{2} \\end{pmatrix}\n$$\n接下来，我们应用缩放矩阵 $\\Lambda^{-1/2}$：\n$$\nm = \\Lambda^{-1/2} (V^\\top g) = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} & 0 & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 2\\sqrt{2} \\\\ 2 \\\\ \\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{2\\sqrt{2}}{\\sqrt{3}} \\\\ \\frac{2}{\\sqrt{2}} \\\\ \\sqrt{2} \\end{pmatrix}\n$$\n将每个分量简化为其精确的解析形式：\n$$\nm_1 = \\frac{2\\sqrt{2}}{\\sqrt{3}} = \\frac{2\\sqrt{2}\\sqrt{3}}{3} = \\frac{2\\sqrt{6}}{3}\n$$\n$$\nm_2 = \\frac{2}{\\sqrt{2}} = \\frac{2\\sqrt{2}}{2} = \\sqrt{2}\n$$\n$$\nm_3 = \\sqrt{2}\n$$\n得到的白化向量是 $m = \\begin{pmatrix} \\frac{2\\sqrt{6}}{3} \\\\ \\sqrt{2} \\\\ \\sqrt{2} \\end{pmatrix}$。最终答案要求以行向量形式呈现。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2\\sqrt{6}}{3} & \\sqrt{2} & \\sqrt{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "神经系统在对信号进行初步的解耦处理后，还会利用各种非线性计算来进一步塑造和优化神经编码。除法归一化（divisive normalization）是贯穿于多个感觉系统的一种经典神经计算，它能有效增强神经表征对刺激强度的不变性，并使不同模式的表征更易于区分。 在这个实践中，你将通过编写代码来模拟这一过程，并训练线性解码器，从而定量地评估除法归一化对气味分类准确性和可辨别性指数（$d'$）的提升效果。",
            "id": "4000544",
            "problem": "给定两种气味类别，它们被建模为嗅球中僧帽细胞或簇状细胞响应向量 $m \\in \\mathbb{R}^d$ 上的多元高斯分布。对于每个类别 $k \\in \\{1,2\\}$，群体响应被建模为 $m \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$，其均值 $\\mu_k$ 和协方差 $\\Sigma_k$ 已知。考虑一个除法归一化变换，它将每个响应向量 $m$ 映射到一个归一化向量 $y$，定义为\n$$\ny = \\frac{m}{c + \\mathbf{1}^\\top m},\n$$\n其中 $c > 0$ 是一个常数，$\\mathbf{1}$ 是 $\\mathbb{R}^d$ 中的全1向量。使用在线别先验概率相等的条件下训练的线性解码器，分别从原始响应 $m$ 和归一化响应 $y$ 中对气味身份进行分类，并量化可辨别性的变化。\n\n从以下基本事实出发：(i) 在多元正态类条件密度和相等先验概率的条件下，最优线性决策边界将数据投影到单个轴上以分离类别；(ii) 信号检测理论将沿任何一维决策轴的可辨别性定义为类别均值的分离度除以沿该轴的合并标准差。请执行以下操作：\n\n- 通过从指定的多元高斯分布中抽样，为每个类别生成训练和测试数据集。\n- 在原始响应 $m$ 上训练一个线性解码器，并独立地在由上述除法归一化产生的归一化响应 $y$ 上训练一个线性解码器，两者均在相等的先验概率下进行。\n- 在各自的留出测试集上计算两个解码器的分类准确率。准确率必须以0到1之间的小数形式报告。\n- 通过将测试数据投影到学习到的决策轴上，并使用该轴上跨类别的合并标准差，为两个解码器计算可辨别性指数 $d'$。\n- 报告归一化在准确率和 $d'$ 上引起的变化。\n\n使用以下测试套件。对于每个测试用例，$n_{\\text{train}}$ 表示每个类别的训练样本数，$n_{\\text{test}}$ 表示每个类别的测试样本数，$s$ 表示随机种子。协方差矩阵的构建使用对角矩阵或参数为 $\\rho$ 的托普利茨相关性。\n\n- 测试用例1（理想情况）：\n  - 维度：$d = 8$。\n  - 均值：$\\mu_1 = [0, 0, 0, 0, 0, 0, 0, 0]$，$\\mu_2 = [0.8, 0.6, 0.9, 0.0, 0.0, 0.4, 0.0, 0.3]$。\n  - 协方差：$\\Sigma_1 = \\operatorname{diag}([0.5, 0.6, 0.4, 0.5, 0.5, 0.7, 0.6, 0.4])$，$\\Sigma_2 = \\Sigma_1$。\n  - 归一化常数：$c = 1.0$。\n  - 样本数：$n_{\\text{train}} = 2500$，$n_{\\text{test}} = 5000$。\n  - 随机种子：$s = 42$。\n\n- 测试用例2（相关特征，小编一化常数）：\n  - 维度：$d = 10$。\n  - 均值：$\\mu_1 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]$，$\\mu_2 = [0.3, 0.3, 0.3, -0.1, -0.1, -0.1, 0.15, 0.0, -0.05, 0.2]$。\n  - 方差：$\\operatorname{var}_1 = [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]$，$\\operatorname{var}_2 = [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]$。\n  - 相关性：托普利茨相关性，参数 $\\rho = 0.6$，即 $R_{ij} = \\rho^{|i-j|}$ for $i,j \\in \\{1,\\dots,d\\}$。\n  - 协方差：$\\Sigma_k = D_k^{1/2} R D_k^{1/2}$，其中 $D_k = \\operatorname{diag}(\\operatorname{var}_k)$ 且 $R$ 是上面指定的托普利茨相关矩阵。\n  - 归一化常数：$c = 0.2$。\n  - 样本数：$n_{\\text{train}} = 3000$，$n_{\\text{test}} = 6000$。\n  - 随机种子：$s = 123$。\n\n- 测试用例3（边界情况：极大归一化常数）：\n  - 维度：$d = 6$。\n  - 均值：$\\mu_1 = [0, 0, 0, 0, 0, 0]$，$\\mu_2 = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]$。\n  - 协方差：$\\Sigma_1 = \\operatorname{diag}([0.2, 0.2, 0.2, 0.2, 0.2, 0.2])$，$\\Sigma_2 = \\Sigma_1$。\n  - 归一化常数：$c = 1000.0$。\n  - 样本数：$n_{\\text{train}} = 2000$，$n_{\\text{test}} = 4000$。\n  - 随机种子：$s = 7$。\n\n对于每个测试用例，分别针对原始响应和归一化响应，在训练数据上训练解码器，并在测试数据上进行评估。可辨别性指数 $d'$ 必须使用测试集投影沿学习到的决策轴计算，并使用该轴上跨类别的合并方差。所有准确率必须表示为小数（而非百分比），所有浮点输出必须四舍五入到3位小数。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个列表的列表形式的结果，每个内部列表对应一个测试用例，其形式为\n$$\n[\\text{acc}_{\\text{raw}}, \\text{acc}_{\\text{norm}}, \\Delta \\text{acc}, d'_{\\text{raw}}, d'_{\\text{norm}}, \\Delta d'],\n$$\n所有条目均四舍五入到3位小数，并且 $\\Delta \\text{acc} = \\text{acc}_{\\text{norm}} - \\text{acc}_{\\text{raw}}$，$\\Delta d' = d'_{\\text{norm}} - d'_{\\text{raw}}$。例如，程序应打印如下单行内容\n$$\n[[0.950, 0.960, 0.010, 2.100, 2.200, 0.100],[\\dots],[\\dots]].\n$$",
            "solution": "该问题要求分析除法归一化对代表两种不同气味类别的神经响应的线性可解码性的影响。响应向量表示为 $m \\in \\mathbb{R}^d$，被建模为来自多元高斯分布的样本，对于类别 $k \\in \\{1, 2\\}$ 有 $m \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$。除法归一化变换定义为 $y = \\frac{m}{c + \\mathbf{1}^\\top m}$，其中 $c > 0$ 是一个常数，$\\mathbf{1}$ 是全1向量。我们将实现并比较在原始响应 $m$ 和归一化响应 $y$ 上训练的线性解码器。\n\n所选的线性解码器是线性判别分析（LDA），对于具有相等协方差矩阵和相等类别先验的高斯分布数据，它是最优的线性分类器。虽然某些测试用例涉及不相等的协方差，但LDA仍然是线性解码的标准且强大的选择。它确定一个投影轴，该轴最大化了类别均值之间平方距离与类内方差的比率。\n\n首先，我们形式化LDA过程。给定两个类别的训练数据，我们计算样本均值 $\\hat{\\mu}_1$ 和 $\\hat{\\mu}_2$，以及样本协方差矩阵 $\\hat{\\Sigma}_1$ 和 $\\hat{\\Sigma}_2$。由于每个类别的训练集大小相同，均为 $n_{\\text{train}}$，合并协方差矩阵计算如下：\n$$\n\\hat{\\Sigma}_{\\text{pool}} = \\frac{1}{2n_{\\text{train}}-2} \\left( (n_{\\text{train}}-1)\\hat{\\Sigma}_1 + (n_{\\text{train}}-1)\\hat{\\Sigma}_2 \\right) = \\frac{1}{2}(\\hat{\\Sigma}_1 + \\hat{\\Sigma}_2)\n$$\n定义用于线性分离的最优投影轴的权重向量 $w$ 由下式给出：\n$$\nw = \\hat{\\Sigma}_{\\text{pool}}^{-1} (\\hat{\\mu}_2 - \\hat{\\mu}_1)\n$$\n数据点 $x$ 被投影到这个轴上，产生一个标量值 $p = w^\\top x$。决策边界是一个与 $w$ 正交的超平面。决策阈值 $b$ 设置在两个训练类别均值投影的中点：\n$$\nb = w^\\top \\left( \\frac{\\hat{\\mu}_1 + \\hat{\\mu}_2}{2} \\right)\n$$\n然后，如果 $w^\\top x_{\\text{test}} > b$，则测试点 $x_{\\text{test}}$ 被分类为类别2，否则分类为类别1（假设 $w^\\top \\hat{\\mu}_2 > w^\\top \\hat{\\mu}_1$）。这个过程对原始数据集 ($m$) 和归一化数据集 ($y$) 独立执行，产生两个不同的解码器，其权重向量分别为 $w_m$、$w_y$，阈值分别为 $b_m$、$b_y$。\n\n为了评估性能，我们在留出的测试数据上使用两个指标：\n$1$. **分类准确率**：这是预测类别标签与真实类别标签相匹配的测试样本的比例。我们为两个解码器计算 $\\text{acc}_{\\text{raw}}$ 和 $\\text{acc}_{\\text{norm}}$。\n$2$. **可辨别性指数 ($d'$)**：该指标量化了两个类别在学习到的决策轴上的分离程度。对于给定的权重向量为 $w$ 的解码器，我们将所有测试数据点投影到该轴上。设类别1和类别2得到的投影值集合分别为 $\\{p_1\\}$ 和 $\\{p_2\\}$。我们计算它们的样本均值 $\\bar{p}_1, \\bar{p}_2$ 和样本标准差 $\\sigma_{p_1}, \\sigma_{p_2}$。可辨别性指数则为：\n$$\nd' = \\frac{|\\bar{p}_2 - \\bar{p}_1|}{\\sqrt{\\frac{\\sigma_{p_1}^2 + \\sigma_{p_2}^2}{2}}}\n$$\n这为分类提供了信噪比的度量。我们计算 $d'_{\\text{raw}}$ 和 $d'_{\\text{norm}}$。\n\n每个测试用例的总体流程如下：\n$1$. 为保证可复现性，设置随机种子 $s$。\n$2$. 构建指定的均值向量 $\\mu_k$ 和协方差矩阵 $\\Sigma_k$。对于测试用例2，协方差矩阵 $\\Sigma_k = D_k^{1/2} R D_k^{1/2}$ 是由方差向量 $\\text{var}_k$ 和条目为 $R_{ij} = \\rho^{|i-j|}$ 的托普利茨相关矩阵 $R$ 构建的。\n$3$. 从各自的多元正态分布中为每个类别生成 $n_{\\text{train}}$ 个训练样本和 $n_{\\text{test}}$ 个测试样本。\n$4$. 通过对每个原始数据样本 $m$ 应用变换 $y = m / (c + \\mathbf{1}^\\top m)$ 来创建归一化数据集。\n$5$. 在原始训练数据上训练一个LDA分类器以获得 $w_m$ 和 $b_m$。\n$6$. 在归一化训练数据上训练一个独立的LDA分类器以获得 $w_y$ 和 $b_y$。\n$7$. 在原始测试集上评估原始数据解码器，以计算 $\\text{acc}_{\\text{raw}}$ 和 $d'_{\\text{raw}}$。\n$8$. 在归一化测试集上评估归一化数据解码器，以计算 $\\text{acc}_{\\text{norm}}$ 和 $d'_{\\text{norm}}$。\n$9$. 计算性能变化：$\\Delta\\text{acc} = \\text{acc}_{\\text{norm}} - \\text{acc}_{\\text{raw}}$ 和 $\\Delta d' = d'_{\\text{norm}} - d'_{\\text{raw}}$。\n$10$. 报告所有四舍五入到三位小数的量。\n\n这种对既定统计方法的系统性应用，可以严谨地量化除法归一化如何改变神经表征的线性可分性。例如，在归一化常数 $c$ 非常大（测试用例3）的极限情况下，该变换近似于一个简单的缩放，$y \\approx m/c$。由于线性分类器对此类缩放具有不变性，我们预期性能变化 $\\Delta\\text{acc}$ 和 $\\Delta d'$ 将接近于零。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    \n    test_cases = [\n        # Test case 1\n        {\n            \"d\": 8,\n            \"mu1\": np.array([0, 0, 0, 0, 0, 0, 0, 0]),\n            \"mu2\": np.array([0.8, 0.6, 0.9, 0.0, 0.0, 0.4, 0.0, 0.3]),\n            \"Sigma1\": np.diag([0.5, 0.6, 0.4, 0.5, 0.5, 0.7, 0.6, 0.4]),\n            \"Sigma2\": np.diag([0.5, 0.6, 0.4, 0.5, 0.5, 0.7, 0.6, 0.4]),\n            \"c\": 1.0,\n            \"n_train\": 2500,\n            \"n_test\": 5000,\n            \"seed\": 42\n        },\n        # Test case 2\n        {\n            \"d\": 10,\n            \"mu1\": np.zeros(10),\n            \"mu2\": np.array([0.3, 0.3, 0.3, -0.1, -0.1, -0.1, 0.15, 0.0, -0.05, 0.2]),\n            \"var1\": np.full(10, 0.8),\n            \"var2\": np.full(10, 0.6),\n            \"rho\": 0.6,\n            \"c\": 0.2,\n            \"n_train\": 3000,\n            \"n_test\": 6000,\n            \"seed\": 123\n        },\n        # Test case 3\n        {\n            \"d\": 6,\n            \"mu1\": np.zeros(6),\n            \"mu2\": np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"Sigma1\": np.diag(np.full(6, 0.2)),\n            \"Sigma2\": np.diag(np.full(6, 0.2)),\n            \"c\": 1000.0,\n            \"n_train\": 2000,\n            \"n_test\": 4000,\n            \"seed\": 7\n        }\n    ]\n\n    # Pre-process test case 2 to build covariance matrices\n    case2 = test_cases[1]\n    d2 = case2[\"d\"]\n    rho2 = case2[\"rho\"]\n    R = toeplitz(rho2**np.arange(d2))\n    D1 = np.diag(case2[\"var1\"])\n    D2 = np.diag(case2[\"var2\"])\n    D1_sqrt = np.sqrt(D1)\n    D2_sqrt = np.sqrt(D2)\n    case2[\"Sigma1\"] = D1_sqrt @ R @ D1_sqrt\n    case2[\"Sigma2\"] = D2_sqrt @ R @ D2_sqrt\n\n    results = []\n    for params in test_cases:\n        result = _solve_case(params)\n        results.append(result)\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{','.join(f'{x:.3f}' for x in r)}]\" for r in results]) + \"]\"\n    print(output_str)\n\n\ndef _solve_case(params):\n    \"\"\"\n    Solves a single test case for the given parameters.\n    \"\"\"\n    d = params[\"d\"]\n    mu1, mu2 = params[\"mu1\"], params[\"mu2\"]\n    Sigma1, Sigma2 = params[\"Sigma1\"], params[\"Sigma2\"]\n    c = params[\"c\"]\n    n_train, n_test = params[\"n_train\"], params[\"n_test\"]\n    seed = params[\"seed\"]\n\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate data\n    m_train1 = rng.multivariate_normal(mu1, Sigma1, n_train)\n    m_train2 = rng.multivariate_normal(mu2, Sigma2, n_train)\n    m_test1 = rng.multivariate_normal(mu1, Sigma1, n_test)\n    m_test2 = rng.multivariate_normal(mu2, Sigma2, n_test)\n\n    # 2. Apply divisive normalization\n    def normalize(data, const_c):\n        sum_m = np.sum(data, axis=1, keepdims=True)\n        denominator = const_c + sum_m\n        # Add a small epsilon to avoid division by zero in pathological cases\n        denominator[denominator == 0] = 1e-9\n        return data / denominator\n\n    y_train1 = normalize(m_train1, c)\n    y_train2 = normalize(m_train2, c)\n    y_test1 = normalize(m_test1, c)\n    y_test2 = normalize(m_test2, c)\n\n    # 3. Combine data and labels for processing\n    m_train = np.vstack((m_train1, m_train2))\n    y_train = np.vstack((y_train1, y_train2))\n    m_test = np.vstack((m_test1, m_test2))\n    y_test = np.vstack((y_test1, y_test2))\n    \n    # Class labels: 0 for class 1, 1 for class 2\n    labels_test = np.array([0] * n_test + [1] * n_test)\n\n    # 4. Train and evaluate for raw and normalized data\n    acc_raw, d_prime_raw = _train_and_evaluate_decoder(m_train, m_test, labels_test, n_train, n_test)\n    acc_norm, d_prime_norm = _train_and_evaluate_decoder(y_train, y_test, labels_test, n_train, n_test)\n\n    # 5. Compute differences\n    delta_acc = acc_norm - acc_raw\n    delta_d_prime = d_prime_norm - d_raw\n\n    return [acc_raw, acc_norm, delta_acc, d_prime_raw, d_prime_norm, delta_d_prime]\n\n\ndef _train_and_evaluate_decoder(X_train, X_test, L_test, n_train_per_class, n_test_per_class):\n    \"\"\"\n    Trains an LDA classifier and evaluates its accuracy and discriminability.\n    \"\"\"\n    # Separate training data by class\n    X_train1 = X_train[:n_train_per_class]\n    X_train2 = X_train[n_train_per_class:]\n\n    # Estimate parameters for LDA\n    mu_hat1 = np.mean(X_train1, axis=0)\n    mu_hat2 = np.mean(X_train2, axis=0)\n    \n    # `rowvar=False` because observations are in rows\n    Sigma_hat1 = np.cov(X_train1, rowvar=False)\n    Sigma_hat2 = np.cov(X_train2, rowvar=False)\n    \n    # Pooled covariance (equal sample sizes)\n    Sigma_pool = 0.5 * (Sigma_hat1 + Sigma_hat2)\n\n    # LDA weight vector\n    try:\n        Sigma_pool_inv = np.linalg.inv(Sigma_pool)\n    except np.linalg.LinAlgError:\n        # Use pseudo-inverse if matrix is singular\n        Sigma_pool_inv = np.linalg.pinv(Sigma_pool)\n        \n    w = Sigma_pool_inv @ (mu_hat2 - mu_hat1)\n\n    # Decision threshold\n    b = w @ ((mu_hat1 + mu_hat2) / 2)\n\n    # --- Evaluation on test data ---\n    # Project test data onto the learned axis\n    projections = X_test @ w\n    \n    # Classification and accuracy\n    predictions = (projections > b).astype(int)\n    accuracy = np.mean(predictions == L_test)\n\n    # Discriminability index (d')\n    # Separate test projections by their true class\n    proj_c1 = projections[:n_test_per_class]\n    proj_c2 = projections[n_test_per_class:]\n    \n    mean_p1 = np.mean(proj_c1)\n    mean_p2 = np.mean(proj_c2)\n    \n    # Using default ddof=0 for std is fine for large N\n    std_p1 = np.std(proj_c1)\n    std_p2 = np.std(proj_c2)\n\n    if std_p1 == 0 or std_p2 == 0:\n        # This is unlikely with continuous data but a safeguard\n        d_prime = float('inf')\n    else:\n        pooled_std_p = np.sqrt(0.5 * (std_p1**2 + std_p2**2))\n        d_prime = np.abs(mean_p2 - mean_p1) / pooled_std_p\n            \n    return accuracy, d_prime\n\nsolve()\n```"
        },
        {
            "introduction": "经过嗅球的处理，信息被传递到更高级的脑区，如梨状皮层，在这里形成最终用于识别和学习的神经表征。一个核心的理论是，大脑在此处采用了稀疏且节能的编码策略，即在确保分类准确性的前提下，尽可能减少神经元的活动以降低代谢成本。 这项练习将引导你运用凸优化的方法来解决这个计算神经科学中的前沿问题，亲手计算出满足分类边界条件下的最低能量稀疏编码，从而深刻理解皮层在信息表征中如何权衡效率与性能。",
            "id": "4000509",
            "problem": "您正在为嗅觉系统在气味分类过程中的梨状皮层稀疏、高能效活动进行建模。假设来自嗅球的线性前馈驱动产生一个向量 $z \\in \\mathbb{R}^p$，作为梨状皮层神经元的预激活输入，并且使用一个固定的线性读出向量 $c \\in \\mathbb{R}^p$ 来对具有二元标签 $y \\in \\{-1,+1\\}$ 的气味进行分类。分类要求以边界约束的形式强制执行：带符号的读出值必须满足 $y \\, c^\\top a \\ge m$，其中指定边界 $m > 0$，$a \\in \\mathbb{R}^p$ 是待确定的梨状皮层活动向量。编码的能量被视为来自前馈驱动的二次偏差成本与一个稀疏性诱导惩罚项之和：\n$$\nE(a;z,\\lambda) = \\frac{1}{2} \\left\\| a - z \\right\\|_2^2 + \\lambda \\left\\| a \\right\\|_1,\n$$\n其中 $\\lambda > 0$ 是控制稀疏性的系数。任务是：对于每个测试用例，计算在边界约束下的最小能量梨状皮层活动 $a^\\star$，并报告相应的最小能量 $E(a^\\star;z,\\lambda)$。\n\n基本假设和定义：\n- 高效编码和稀疏表示：稀疏编码通过最小化一个能量泛函来实现，该泛函在对输入的保真度与激活的稀疏性之间进行权衡，通常通过 $\\ell_1$ 范数惩罚项来建模。\n- 线性分类边界：将准确性放宽为一个凸约束，要求线性读出具有最小边界，这是分类性能的标准替代指标，并产生一个凸可行域。\n- 所有量都是无单位的标量和向量；不涉及物理单位。所有角度均不相关，且在本问题中不使用。\n\n形式上，对于每个测试用例，求解凸规划问题\n$$\n\\min_{a \\in \\mathbb{R}^p} \\ \\frac{1}{2} \\left\\| a - z \\right\\|_2^2 + \\lambda \\left\\| a \\right\\|_1 \\quad \\text{subject to} \\quad y \\, c^\\top a \\ge m.\n$$\n\n您的程序必须为以下三个测试用例（每个用例的维度均为 $p = 8$）数值求解此优化问题，并输出最小能量值：\n\n- 测试用例 1（一般情况）：\n  - $z = [0.8, 0.1, 0.0, 0.2, -0.3, 0.5, 0.0, -0.1]$\n  - $c = [0.6, -0.2, 0.0, 0.3, 0.1, -0.5, 0.0, 0.2]$\n  - $y = +1$\n  - $m = 0.4$\n  - $\\lambda = 0.15$\n\n- 测试用例 2（边界轴读出与强边界）：\n  - $z = [-0.2, 0.4, 0.0, -0.1, 0.0, 0.3, 0.05, 0.0]$\n  - $c = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$\n  - $y = +1$\n  - $m = 0.5$\n  - $\\lambda = 0.05$\n\n- 测试用例 3（负标签与混合符号读出）：\n  - $z = [0.1, 0.0, 0.4, -0.2, 0.2, 0.0, 0.0, 0.3]$\n  - $c = [-0.2, 0.5, 0.3, 0.0, -0.4, 0.0, 0.1, 0.2]$\n  - $y = -1$\n  - $m = 0.3$\n  - $\\lambda = 0.2$\n\n算法和数学约束：\n- 您必须将此问题视为一个带有 $\\ell_1$ 惩罚项和单个线性不等式约束的凸优化问题。\n- 您不能假设存在封闭形式的解；必须推导并实现一种有原则的数值方法，该方法能正确处理 $\\ell_1$ 范数的不可微性和线性约束。\n- 计算出的解 $a^\\star$ 必须在数值公差范围内满足分类边界约束。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目是对应一个测试用例的最小能量，表示为具有六位小数的浮点数，例如 [0.123456,0.234567,0.345678]。",
            "solution": "我们从高效编码和稀疏表示的原理出发：为了获得高能效的神经活动，需要最小化一个保真度项和稀疏性项的组合。此处的保真度项是二次的，即 $\\frac{1}{2}\\left\\| a - z \\right\\|_2^2$，它捕捉了与前馈驱动的偏差，而稀疏性则通过 $\\ell_1$ 惩罚项 $\\lambda \\left\\| a \\right\\|_1$ 来促进，这被广泛认为是诱导稀疏编码的方法。分类准确性被建模为一个凸边界约束 $y \\, c^\\top a \\ge m$，这是将离散的准确性放宽为线性读出上的几何边界。\n\n该优化问题为\n$$\n\\min_{a \\in \\mathbb{R}^p} \\ \\frac{1}{2} \\left\\| a - z \\right\\|_2^2 + \\lambda \\left\\| a \\right\\|_1 \\quad \\text{subject to} \\quad y \\, c^\\top a \\ge m.\n$$\n此问题是凸的，因为目标函数是一个凸平滑函数与一个凸非平滑函数之和，且由单个线性不等式定义的可行集是凸的。\n\n为解决此问题，我们采用交替方向乘子法 (Alternating Direction Method of Multipliers, ADMM)。ADMM 适用于目标函数可分解为具有简单近端算子 (proximal operator) 的项之和以及包含线性约束的问题。我们引入两个辅助变量来分离非平滑的 $\\ell_1$ 惩罚项和半空间约束的指示函数：\n- $u \\in \\mathbb{R}^p$ 用于处理 $\\ell_1$ 项，\n- $v \\in \\mathbb{R}^p$ 用于处理约束。\n\n我们将问题重写为\n$$\n\\min_{a,u,v} \\ \\frac{1}{2} \\left\\| a - z \\right\\|_2^2 + \\lambda \\left\\| u \\right\\|_1 + I_{\\mathcal{H}}(v) \\quad \\text{subject to} \\quad a = u, \\quad a = v,\n$$\n其中 $I_{\\mathcal{H}}(v)$ 是半空间 $\\mathcal{H} = \\{ v \\in \\mathbb{R}^p \\mid y \\, c^\\top v \\ge m \\}$ 的指示函数，如果 $v \\in \\mathcal{H}$，则其值为 $0$，否则为 $+\\infty$。\n\n带有惩罚参数 $\\rho > 0$ 和对偶变量 $p_u, p_v$ 的缩放形式 ADMM 迭代通过依次最小化增广拉格朗日函数来对每个变量进行更新：\n- $a$-更新：最小化一个严格凸的二次函数，\n- $u$-更新：$\\ell_1$ 范数的近端步骤，\n- $v$-更新：投影到半空间，\n- 对偶更新：对对偶变量进行梯度上升。\n\n我们明确推导每个更新步骤。\n\n对于 $a$-更新，我们最小化\n$$\n\\frac{1}{2}\\left\\| a - z \\right\\|_2^2 + \\frac{\\rho}{2} \\left\\| a - u + p_u \\right\\|_2^2 + \\frac{\\rho}{2} \\left\\| a - v + p_v \\right\\|_2^2.\n$$\n对 $a$ 求梯度，令其为零，求解可得\n$$\n(1 + 2\\rho) \\, a = z + \\rho \\left( u - p_u \\right) + \\rho \\left( v - p_v \\right),\n$$\n因此，封闭形式的更新为\n$$\na \\leftarrow \\frac{z + \\rho (u - p_u) + \\rho (v - p_v)}{1 + 2\\rho}.\n$$\n\n对于 $u$-更新，我们求解\n$$\n\\min_u \\ \\lambda \\left\\| u \\right\\|_1 + \\frac{\\rho}{2} \\left\\| u - (a + p_u) \\right\\|_2^2.\n$$\n这是 $\\ell_1$ 范数的近端算子，由逐元素应用的软阈值操作给出：\n$$\nu \\leftarrow \\operatorname{soft}(a + p_u, \\lambda/\\rho),\n$$\n其中对于每个分量 $w_i$，\n$$\n\\operatorname{soft}(w_i, \\tau) = \\operatorname{sign}(w_i) \\cdot \\max\\left( |w_i| - \\tau, \\, 0 \\right).\n$$\n\n对于 $v$-更新，我们求解\n$$\n\\min_v \\ I_{\\mathcal{H}}(v) + \\frac{\\rho}{2} \\left\\| v - (a + p_v) \\right\\|_2^2,\n$$\n这是将 $a + p_v$ 欧几里得投影到半空间 $\\mathcal{H}$。令 $w = a + p_v$。如果 $y \\, c^\\top w \\ge m$，则 $w \\in \\mathcal{H}$，投影结果为 $v \\leftarrow w$。否则，我们将其正交投影到支撑超平面 $y \\, c^\\top v = m$ 上。在 $\\mathbb{R}^p$ 中，到一个由单个线性不等式定义的半空间上的投影具有封闭形式：\n$$\nv \\leftarrow w + \\frac{m - y \\, c^\\top w}{\\left\\| c \\right\\|_2^2} \\, y \\, c.\n$$\n这是通过使用拉格朗日乘子法最小化 $\\left\\| v - w \\right\\|_2^2$ 并满足约束 $y \\, c^\\top v = m$ 得出的；拉格朗日函数 $\\mathcal{L}(v,\\alpha) = \\left\\| v - w \\right\\|_2^2 + \\alpha (y \\, c^\\top v - m)$ 产生最优性条件 $v - w + \\frac{\\alpha}{2} y c = 0$，消去 $\\alpha$ 即可在满足约束的同时，实现沿 $c$ 方向的最小范数校正。\n\n对偶变量的更新强制了约束的一致性：\n$$\np_u \\leftarrow p_u + a - u, \\quad p_v \\leftarrow p_v + a - v.\n$$\n\n对于凸问题，ADMM 在温和条件下保证收敛；此处二次项的强凸性进一步有助于收敛。我们监控原始残差 $\\left\\| a - u \\right\\|_2$ 和 $\\left\\| a - v \\right\\|_2$（将它们聚合为一个范数），以及基于 $u$ 和 $v$ 变化的对偶残差，当两者都低于指定公差时停止迭代。\n\n收敛后，最小能量直接计算为\n$$\nE(a^\\star; z, \\lambda) = \\frac{1}{2} \\left\\| a^\\star - z \\right\\|_2^2 + \\lambda \\left\\| a^\\star \\right\\|_1.\n$$\n\n我们将此过程应用于三个指定的测试用例：\n- 测试用例 1：$z = [0.8, 0.1, 0.0, 0.2, -0.3, 0.5, 0.0, -0.1]$，$c = [0.6, -0.2, 0.0, 0.3, 0.1, -0.5, 0.0, 0.2]$，$y = +1$，$m = 0.4$，$\\lambda = 0.15$。约束 $c^\\top a \\ge 0.4$ 确保了足够的正边界；稀疏性降低了活动的大小。\n- 测试用例 2：$z = [-0.2, 0.4, 0.0, -0.1, 0.0, 0.3, 0.05, 0.0]$，$c = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$，$y = +1$，$m = 0.5$，$\\lambda = 0.05$。约束简化为 $a_1 \\ge 0.5$，尽管有稀疏性惩罚，仍强制对第一个分量进行大的校正。\n- 测试用例 3：$z = [0.1, 0.0, 0.4, -0.2, 0.2, 0.0, 0.0, 0.3]$，$c = [-0.2, 0.5, 0.3, 0.0, -0.4, 0.0, 0.1, 0.2]$，$y = -1$，$m = 0.3$，$\\lambda = 0.2$。约束变为 $c^\\top a \\le -0.3$，通过调整活动来降低线性读出值，同时保持稀疏性。\n\n最终的程序实现了上述 ADMM 求解器，并为每个测试用例计算最小能量，以指定格式输出单行结果，每个能量值保留六位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef soft_threshold(x, tau):\n    # Elementwise soft-thresholding\n    return np.sign(x) * np.maximum(np.abs(x) - tau, 0.0)\n\ndef project_halfspace(w, c, y, m):\n    # Project w onto the half-space {v | y * c^T v >= m}\n    s = y * np.dot(c, w)\n    if s >= m:\n        return w.copy()\n    # Minimal correction along c direction to reach the margin hyperplane\n    c_norm_sq = np.dot(c, c)\n    if c_norm_sq == 0.0:\n        # Degenerate case: cannot enforce margin with zero classifier; return w unchanged\n        return w.copy()\n    tau = (m - s) / c_norm_sq\n    return w + tau * y * c\n\ndef admm_min_energy(z, c, y, m, lam, rho=1.0, max_iters=1000, tol=1e-8):\n    \"\"\"\n    Solve: minimize 0.5||a - z||_2^2 + lam*||a||_1 subject to y * c^T a >= m\n    via ADMM with splitting a = u, a = v.\n    \"\"\"\n    p = z.shape[0]\n    # Initialize variables\n    a = z.copy()\n    u = a.copy()\n    v = a.copy()\n    pu = np.zeros(p)\n    pv = np.zeros(p)\n\n    # Precompute denominator for a-update\n    denom = 1.0 + 2.0 * rho\n\n    for k in range(max_iters):\n        # Save previous u, v for dual residual\n        u_prev = u.copy()\n        v_prev = v.copy()\n\n        # a-update: closed-form for quadratic minimization\n        a = (z + rho * (u - pu) + rho * (v - pv)) / denom\n\n        # u-update: soft-threshold for L1 proximal\n        u = soft_threshold(a + pu, lam / rho)\n\n        # v-update: projection onto half-space\n        v = project_halfspace(a + pv, c, y, m)\n\n        # Dual updates\n        pu += a - u\n        pv += a - v\n\n        # Primal residuals: a - u, a - v\n        r1 = a - u\n        r2 = a - v\n        r_norm = np.sqrt(np.dot(r1, r1) + np.dot(r2, r2))\n\n        # Dual residuals: rho*(u - u_prev), rho*(v - v_prev)\n        s1 = rho * (u - u_prev)\n        s2 = rho * (v - v_prev)\n        s_norm = np.sqrt(np.dot(s1, s1) + np.dot(s2, s2))\n\n        if r_norm  tol and s_norm  tol:\n            break\n\n    # Compute minimal energy\n    energy = 0.5 * np.dot(a - z, a - z) + lam * np.sum(np.abs(a))\n    # Optional: ensure margin satisfied (not printed)\n    # margin_val = y * np.dot(c, a)\n    return energy\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"z\": np.array([0.8, 0.1, 0.0, 0.2, -0.3, 0.5, 0.0, -0.1]),\n            \"c\": np.array([0.6, -0.2, 0.0, 0.3, 0.1, -0.5, 0.0, 0.2]),\n            \"y\": 1.0,\n            \"m\": 0.4,\n            \"lam\": 0.15,\n        },\n        # Test case 2\n        {\n            \"z\": np.array([-0.2, 0.4, 0.0, -0.1, 0.0, 0.3, 0.05, 0.0]),\n            \"c\": np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"y\": 1.0,\n            \"m\": 0.5,\n            \"lam\": 0.05,\n        },\n        # Test case 3\n        {\n            \"z\": np.array([0.1, 0.0, 0.4, -0.2, 0.2, 0.0, 0.0, 0.3]),\n            \"c\": np.array([-0.2, 0.5, 0.3, 0.0, -0.4, 0.0, 0.1, 0.2]),\n            \"y\": -1.0,\n            \"m\": 0.3,\n            \"lam\": 0.2,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        z = case[\"z\"]\n        c = case[\"c\"]\n        y = case[\"y\"]\n        m = case[\"m\"]\n        lam = case[\"lam\"]\n        energy = admm_min_energy(z, c, y, m, lam, rho=1.0, max_iters=2000, tol=1e-10)\n        results.append(f\"{energy:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}