{
    "hands_on_practices": [
        {
            "introduction": "神经元通讯的基础在于突触处神经递质的概率性释放。本练习将引导你分析经典的突触释放二项模型，并将其与泊松近似进行比较，从而深入理解神经变异性的根本来源。你将计算并对比这两种模型的关键统计量，例如 Fano 因子 $F = \\mathrm{Var}(K)/\\mathbb{E}[K]$，这是衡量神经活动随机性的一个核心指标。",
            "id": "2738674",
            "problem": "一个突触末梢包含 $n=5$ 个独立且功能相同的释放位点。为响应一个短暂的突触前动作电位，每个位点最多释放一个突触囊泡，其概率为 $p=0.2$，且与其他位点和试验独立。令 $K$ 表示单次试验（一次动作电位）中释放的囊泡数量。假设突触后检测是完美的，因此 $K$ 可以作为每次试验的囊泡计数被直接观测到。\n\n从适用于细胞和分子神经科学及概率论的第一性原理出发——即每个位点的释放是独立的伯努利试验、微观状态的组合计数，以及期望、方差和法诺因子（定义为 $\\mathrm{Var}(K)/\\mathbb{E}[K]$）的定义——推导出 $K$ 在 $k=0,1,2,3,4,5$ 时的精确计数分布，并计算其均值、方差和法诺因子。然后，考虑一个具有相同均值 $\\lambda = n p$ 的神经递质释放的泊松过程模型。将二项位点模型的均值、方差和法诺因子与泊松模型的进行比较。\n\n将二项位点模型的法诺因子与均值匹配的泊松模型的法诺因子之间的绝对差作为你的最终答案。将答案四舍五入到4位有效数字，并以无单位的纯数字形式报告。",
            "solution": "问题陈述经过验证。\n\n逐字提取的已知条件如下：\n- 独立、功能相同的释放位点数量：$n=5$。\n- 每个位点每次试验的释放概率：$p=0.2$。\n- $K$ 是单次试验中释放的囊泡数量。\n- 考虑一个均值为 $\\lambda = n p$ 的泊松过程模型。\n- 法诺因子定义为 $\\mathrm{Var}(K)/\\mathbb{E}[K]$。\n- 最终答案是二项位点模型的法诺因子与均值匹配的泊松模型的法诺因子之间的绝对差，四舍五入到4位有效数字。\n\n验证评估：\n- **科学依据充分：** 该问题使用了突触传递的二项模型，这是由 Katz 及其同事建立的细胞神经科学中的一个基本概念。与泊松分布的比较是该领域中一个标准且信息丰富的分析。参数 $n=5$ 和 $p=0.2$ 在某些类型的突触中处于现实范围内。该问题在科学上是合理的。\n- **提法明确：** 提供了所有必需的参数和定义。任务明确，可以得出一个唯一、稳定且有意义的解。\n- **客观性：** 语言正式、精确，没有主观或推测性内容。\n\n结论：该问题是有效的。它代表了定量神经科学和概率论中的一个标准、定义明确的练习。可以进行求解。\n\n该问题描述了一个包含 $n=5$ 次独立伯努利试验的过程，其中每次试验（一个位点的潜在释放）的成功概率为 $p=0.2$。因此，总成功次数（释放的囊泡数量）$K$ 服从二项分布，$K \\sim \\mathrm{Binomial}(n, p)$。\n\n二项分布的概率质量函数 (PMF) 由下式给出：\n$$P(K=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n其中 $k$ 是成功次数，其取值范围为 $0$ 到 $n$。\n\n首先，我们计算在给定 $n=5$ 和 $p=0.2$ 的情况下 $K$ 的精确计数分布。这需要计算 $k \\in \\{0, 1, 2, 3, 4, 5\\}$ 时 $P(K=k)$ 的值。注意 $1-p = 1 - 0.2 = 0.8$。\n\n- 对于 $k=0$：$P(K=0) = \\binom{5}{0} (0.2)^0 (0.8)^5 = 1 \\times 1 \\times 0.32768 = 0.32768$\n- 对于 $k=1$：$P(K=1) = \\binom{5}{1} (0.2)^1 (0.8)^4 = 5 \\times 0.2 \\times 0.4096 = 0.4096$\n- 对于 $k=2$：$P(K=2) = \\binom{5}{2} (0.2)^2 (0.8)^3 = 10 \\times 0.04 \\times 0.512 = 0.2048$\n- 对于 $k=3$：$P(K=3) = \\binom{5}{3} (0.2)^3 (0.8)^2 = 10 \\times 0.008 \\times 0.64 = 0.0512$\n- 对于 $k=4$：$P(K=4) = \\binom{5}{4} (0.2)^4 (0.8)^1 = 5 \\times 0.0016 \\times 0.8 = 0.0064$\n- 对于 $k=5$：$P(K=5) = \\binom{5}{5} (0.2)^5 (0.8)^0 = 1 \\times 0.00032 \\times 1 = 0.00032$\n\n接下来，我们计算这个二项模型的均值、方差和法诺因子。\n二项分布的期望（均值）由下式给出：\n$$\\mathbb{E}[K] = np$$\n代入给定值：\n$$\\mathbb{E}[K] = 5 \\times 0.2 = 1$$\n\n二项分布的方差由下式给出：\n$$\\mathrm{Var}(K) = np(1-p)$$\n代入给定值：\n$$\\mathrm{Var}(K) = 5 \\times 0.2 \\times (1 - 0.2) = 1 \\times 0.8 = 0.8$$\n\n二项模型的法诺因子 $\\mathrm{FF}_{\\text{Binomial}}$ 是方差与均值的比率：\n$$\\mathrm{FF}_{\\text{Binomial}} = \\frac{\\mathrm{Var}(K)}{\\mathbb{E}[K]} = \\frac{np(1-p)}{np} = 1-p$$\n使用给定的 $p$ 值：\n$$\\mathrm{FF}_{\\text{Binomial}} = 1 - 0.2 = 0.8$$\n\n现在，我们考虑一个均值 $\\lambda$ 与二项模型均值相匹配的泊松过程模型。令 $K_{\\text{Poisson}}$ 为该模型的随机变量。\n匹配条件为：\n$$\\lambda = \\mathbb{E}[K] = np = 1$$\n因此，我们考虑一个参数为 $\\lambda=1$ 的泊松分布。\n\n泊松分布的基本性质是其均值和方差均等于其参数 $\\lambda$。\n泊松模型的均值为：\n$$\\mathbb{E}[K_{\\text{Poisson}}] = \\lambda = 1$$\n泊松模型的方差为：\n$$\\mathrm{Var}(K_{\\text{Poisson}}) = \\lambda = 1$$\n\n因此，泊松模型的法诺因子 $\\mathrm{FF}_{\\text{Poisson}}$ 为：\n$$\\mathrm{FF}_{\\text{Poisson}} = \\frac{\\mathrm{Var}(K_{\\text{Poisson}})}{\\mathbb{E}[K_{\\text{Poisson}}]} = \\frac{\\lambda}{\\lambda} = 1$$\n这个结果对于任何泊松过程都是定义性的；其方差总是等于其均值，从而得到为1的法诺因子。\n\n最后的任务是计算二项位点模型的法诺因子与均值匹配的泊松模型的法诺因子之间的绝对差。\n$$\\Delta \\mathrm{FF} = |\\mathrm{FF}_{\\text{Binomial}} - \\mathrm{FF}_{\\text{Poisson}}|$$\n代入计算出的值：\n$$\\Delta \\mathrm{FF} = |0.8 - 1| = |-0.2| = 0.2$$\n\n题目要求答案四舍五入到4位有效数字。\n$$0.2 \\rightarrow 0.2000$$\n\n这个差异凸显了二项释放模型的一个关键特征：因为释放位点数 $n$ 是有限的，所以方差相对于均值被抑制了。释放过程是“亚泊松的”，其法诺因子严格小于1（对于 $p>0$）。这种偏离泊松法诺因子1的幅度就是释放概率 $p$。",
            "answer": "$$\\boxed{0.2000}$$"
        },
        {
            "introduction": "从单个神经元扩展到神经元群体，我们面临一个核心问题：大脑如何编码信息？此练习将指导你应用贝叶斯推断来构建一个线性解码器，从群体的神经活动中对一个外部变量（如运动方向）进行最优估计。这个过程是神经解码的基础，它将概率论与从神经信号中提取信息的工程学挑战联系在一起。",
            "id": "4011627",
            "problem": "给定一个包含 $N$ 个神经元的群体，其发放率在一个用于解码一维伸展方向变量的线性高斯观测框架下建模。时间 $t$ 的瞬时伸展方向表示为 $x_t$，被视为一个以弧度为单位的标量角度。时间 $t$ 的群体发放率向量表示为 $\\mathbf{r}_t \\in \\mathbb{R}^N$，单位为次/秒 (spk/s)。该生成模型由以下部分组成：\n\n1. 一个关于潜伸展方向的高斯先验，\n$ p(x_t) = \\mathcal{N}(x_t \\mid \\mu_0, \\sigma_0^2), $\n其中 $x_t$ 的单位是弧度，$\\mu_0$ 的单位是弧度，$\\sigma_0^2$ 的单位是弧度的平方。\n\n2. 给定潜变量 $x_t$ 时，群体发放率的一个多元高斯似然（观测模型），\n$ p(\\mathbf{r}_t \\mid x_t) = \\mathcal{N}(\\mathbf{r}_t \\mid \\mathbf{C} x_t + \\mathbf{d}, \\boldsymbol{\\Sigma}), $\n其中 $\\mathbf{C} \\in \\mathbb{R}^N$ 编码了每个神经元的线性调谐斜率，单位为 spk/s 每弧度；$\\mathbf{d} \\in \\mathbb{R}^N$ 编码了基线发放率，单位为 spk/s；$\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{N \\times N}$ 是一个正定协方差矩阵，单位为 $(\\text{spk/s})^2$。\n\n任务：\n- 从贝叶斯法则和多元正态分布性质的基本定义出发，推导后验分布 $p(x_t \\mid \\mathbf{r}_t)$，并将其均值和方差明确表示为 $\\mu_0$、$\\sigma_0^2$、$\\mathbf{C}$、$\\mathbf{d}$、$\\boldsymbol{\\Sigma}$ 和观测值 $\\mathbf{r}_t$ 的函数。\n- 构建一个从 $\\mathbf{r}_t$ 解码 $x_t$ 的线性解码器，使其等于所述模型下的后验均值。将此解码器表示为一个仿射映射 $ \\hat{x}_t = b + \\mathbf{w}^\\top \\mathbf{r}_t $，并提供偏置 $b$（单位为弧度）和权重 $\\mathbf{w} \\in \\mathbb{R}^N$（单位为弧度每 spk/s）关于模型参数的明确表达式。\n\n整个过程中角度单位必须是弧度。发放率单位是 spk/s。协方差单位是 $(\\text{spk/s})^2$。\n\n使用以下固定参数值，其中 $N = 4$：\n- 先验参数：$ \\mu_0 = 0.3 $ (弧度)，$ \\sigma_0^2 = 0.04 $ (弧度的平方)。\n- 调谐斜率：$ \\mathbf{C} = [0.7,\\,-0.5,\\,0.3,\\,0.1] $ (spk/s 每弧度)。\n- 基线发放率：$ \\mathbf{d} = [15.0,\\,10.0,\\,12.0,\\,8.0] $ (spk/s)。\n- 观测噪声协方差：\n$$\n\\boldsymbol{\\Sigma} =\n\\begin{bmatrix}\n4.0  0.6  0.2  0.1 \\\\\n0.6  3.0  0.4  0.2 \\\\\n0.2  0.4  2.5  0.3 \\\\\n0.1  0.2  0.3  3.5 \\\\\n\\end{bmatrix}\n\\quad \\text{(spk/s)}^2.\n$$\n\n测试套件（所有发放率单位均为 spk/s）：\n- 案例 1 (基线)：$ \\mathbf{r}_1 = [15.0,\\,10.0,\\,12.0,\\,8.0] $。\n- 案例 2 (沿调谐方向的正偏差)：$ \\mathbf{r}_2 = \\mathbf{d} + 2 \\mathbf{C} = [16.4,\\,9.0,\\,12.6,\\,8.2] $。\n- 案例 3 (沿调谐方向的负偏差)：$ \\mathbf{r}_3 = \\mathbf{d} - 1.5 \\mathbf{C} = [13.95,\\,10.75,\\,11.55,\\,7.85] $。\n\n您的程序必须：\n1. 为每个测试案例计算 $x_t$ 的后验均值和方差。\n2. 计算解码器偏置 $b$ 和权重 $\\mathbf{w}$，使得后验均值等于 $ b + \\mathbf{w}^\\top \\mathbf{r}_t $。\n3. 生成单行输出，其中包含一个逗号分隔的结果列表，并用方括号括起来，顺序完全如下：\n$ [\\mu_{\\text{post},1},\\,\\sigma^2_{\\text{post},1},\\,\\mu_{\\text{post},2},\\,\\sigma^2_{\\text{post},2},\\,\\mu_{\\text{post},3},\\,\\sigma^2_{\\text{post},3},\\,b,\\,w_1,\\,w_2,\\,w_3,\\,w_4] $。\n所有角度（$\\mu_{\\text{post},i}$ 和 $b$）必须以弧度为单位，所有方差（$\\sigma^2_{\\text{post},i}$）必须以弧度的平方为单位。解码器权重 $w_i$ 必须以弧度每 spk/s 为单位。\n\n不允许外部输入；在程序中硬编码给定的参数和测试套件。程序必须是自包含的，并且可以直接运行。",
            "solution": "该问题是有效的。这是一个适定的、具有科学依据的计算神经科学问题，涉及使用线性高斯模型进行贝叶斯推断。所有必要的参数和条件都已提供，没有内部矛盾或歧义。\n\n### 后验分布的推导\n\n我们的目标是确定在给定观测到的神经群体发放率向量 $\\mathbf{r}_t$ 的条件下，潜伸展方向 $x_t$ 的后验分布，表示为 $p(x_t \\mid \\mathbf{r}_t)$。根据贝叶斯法则，后验分布正比于似然与先验的乘积：\n$$ p(x_t \\mid \\mathbf{r}_t) \\propto p(\\mathbf{r}_t \\mid x_t) p(x_t) $$\n问题定义了一个高斯先验和一个高斯似然：\n1.  先验：$ p(x_t) = \\mathcal{N}(x_t \\mid \\mu_0, \\sigma_0^2) $\n2.  似然：$ p(\\mathbf{r}_t \\mid x_t) = \\mathcal{N}(\\mathbf{r}_t \\mid \\mathbf{C} x_t + \\mathbf{d}, \\boldsymbol{\\Sigma}) $\n\n两个高斯分布的乘积是一个未归一化的高斯分布。为了找到所得后验分布的参数，我们处理概率密度函数的指数部分。高斯分布 $\\mathcal{N}(z \\mid \\mu, \\Sigma)$ 的概率密度函数 (PDF) 正比于 $\\exp(-\\frac{1}{2}(z-\\mu)^\\top \\Sigma^{-1}(z-\\mu))$。\n\n先验分布 $p(x_t)$ 的指数部分是：\n$$ \\mathcal{L}_{\\text{prior}} = -\\frac{1}{2\\sigma_0^2}(x_t - \\mu_0)^2 = -\\frac{1}{2}\\left( \\frac{1}{\\sigma_0^2}x_t^2 - \\frac{2\\mu_0}{\\sigma_0^2}x_t \\right) + \\text{const}_1 $$\n似然分布 $p(\\mathbf{r}_t \\mid x_t)$ 的指数部分是：\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} (\\mathbf{r}_t - (\\mathbf{C} x_t + \\mathbf{d}))^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - (\\mathbf{C} x_t + \\mathbf{d})) $$\n让我们展开似然指数中的二次型，重点关注涉及 $x_t$ 的项：\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} ((\\mathbf{r}_t - \\mathbf{d}) - \\mathbf{C}x_t)^\\top \\boldsymbol{\\Sigma}^{-1} ((\\mathbf{r}_t - \\mathbf{d}) - \\mathbf{C}x_t) $$\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} \\left( x_t \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} x_t - x_t \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}) - (\\mathbf{r}_t - \\mathbf{d})^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} x_t \\right) + \\text{const}_2 $$\n由于所有项都是标量，我们可以合并 $x_t$ 的两个线性项：\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} \\left( (\\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C})x_t^2 - 2(\\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}))x_t \\right) + \\text{const}_2 $$\n\n后验分布的指数 $\\mathcal{L}_{\\text{post}}$ 是先验和似然指数的和。我们按 $x_t$ 的幂次收集项：\n$$ \\mathcal{L}_{\\text{post}} = \\mathcal{L}_{\\text{prior}} + \\mathcal{L}_{\\text{like}} $$\n$$ \\mathcal{L}_{\\text{post}} = -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C}\\right)x_t^2 - 2\\left(\\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d})\\right)x_t \\right] + \\text{const}_3 $$\n这是 $x_t$ 的高斯分布的指数，我们将其表示为 $p(x_t \\mid \\mathbf{r}_t) = \\mathcal{N}(x_t \\mid \\mu_{\\text{post}}, \\sigma^2_{\\text{post}})$。该后验分布的指数具有标准形式：\n$$ -\\frac{1}{2\\sigma^2_{\\text{post}}}(x_t - \\mu_{\\text{post}})^2 = -\\frac{1}{2}\\left( \\frac{1}{\\sigma^2_{\\text{post}}}x_t^2 - \\frac{2\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}}x_t \\right) + \\text{const}_4 $$\n通过匹配 $x_t^2$ 和 $x_t$ 项的系数，我们可以确定后验精度 $(\\sigma^2_{\\text{post}})^{-1}$ 和后验均值 $\\mu_{\\text{post}}$。\n\n$x_t^2$ 的系数给出了后验精度：\n$$ \\frac{1}{\\sigma^2_{\\text{post}}} = \\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} $$\n因此，后验方差为：\n$$ \\sigma^2_{\\text{post}} = \\left( \\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} \\right)^{-1} $$\n请注意，后验方差与观测值 $\\mathbf{r}_t$ 无关。\n\n$x_t$ 的系数给出：\n$$ \\frac{2\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}} = 2\\left(\\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d})\\right) $$\n求解后验均值 $\\mu_{\\text{post}}$：\n$$ \\mu_{\\text{post}} = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}) \\right) $$\n\n### 线性解码器的推导\n\n在平方误差损失下，后验均值 $\\mu_{\\text{post}}$ 是 $x_t$ 的最优贝叶斯估计量。我们将线性解码器 $\\hat{x}_t$ 定义为该后验均值，即 $\\hat{x}_t = \\mu_{\\text{post}}$。我们需要将其表示为仿射形式 $\\hat{x}_t = b + \\mathbf{w}^\\top \\mathbf{r}_t$。\n让我们展开 $\\mu_{\\text{post}}$ 的表达式并重新排列：\n$$ \\mu_{\\text{post}} = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{r}_t - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right) $$\n$$ \\mu_{\\text{post}} = \\underbrace{\\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right)}_{b} + \\underbrace{\\left(\\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1}\\right)}_{\\mathbf{w}^\\top} \\mathbf{r}_t $$\n通过这种重新排列，我们可以确定偏置 $b$ 和权重向量 $\\mathbf{w}$。\n\n偏置项 $b$ 是：\n$$ b = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right) $$\n权重的行向量 $\\mathbf{w}^\\top$ 是：\n$$ \\mathbf{w}^\\top = \\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} $$\n取转置，我们得到权重的列向量 $\\mathbf{w}$：\n$$ \\mathbf{w} = (\\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1})^\\top = \\sigma^2_{\\text{post}} (\\boldsymbol{\\Sigma}^{-1})^\\top (\\mathbf{C}^\\top)^\\top = \\sigma^2_{\\text{post}} \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} $$\n最后一步利用了对称矩阵 $\\boldsymbol{\\Sigma}$ 的逆矩阵也是对称的这一事实，即 $(\\boldsymbol{\\Sigma}^{-1})^\\top = \\boldsymbol{\\Sigma}^{-1}$。这些关于 $b$ 和 $\\mathbf{w}$ 的公式允许直接从模型参数进行计算。\n\n### 数值计算\n现在将使用提供的数值来实现推导出的公式，以找到三个测试案例所需量，以及恒定的解码器参数 $b$ 和 $\\mathbf{w}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the posterior distribution parameters and the linear decoder\n    parameters for a linear-Gaussian model of neural population decoding.\n    \"\"\"\n    # Define fixed parameters from the problem statement.\n    # N is the number of neurons.\n    N = 4\n    \n    # Prior parameters\n    mu0 = 0.3  # radians\n    sigma0_sq = 0.04  # radians^2\n\n    # Likelihood parameters\n    C = np.array([0.7, -0.5, 0.3, 0.1]).reshape(N, 1)  # spk/s per radian\n    d = np.array([15.0, 10.0, 12.0, 8.0]).reshape(N, 1)  # spk/s\n    Sigma = np.array([\n        [4.0, 0.6, 0.2, 0.1],\n        [0.6, 3.0, 0.4, 0.2],\n        [0.2, 0.4, 2.5, 0.3],\n        [0.1, 0.2, 0.3, 3.5]\n    ])  # (spk/s)^2\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (baseline): r1 = d\n        np.array([15.0, 10.0, 12.0, 8.0]).reshape(N, 1),\n        # Case 2 (positive deviation): r2 = d + 2*C\n        np.array([16.4, 9.0, 12.6, 8.2]).reshape(N, 1),\n        # Case 3 (negative deviation): r3 = d - 1.5*C\n        np.array([13.95, 10.75, 11.55, 7.85]).reshape(N, 1)\n    ]\n\n    # --- Step 1: Compute posterior variance ---\n    # This value is constant for all observations in a linear-Gaussian model.\n    # Pre-compute the inverse of the covariance matrix and the prior precision.\n    Sigma_inv = np.linalg.inv(Sigma)\n    prior_precision = 1.0 / sigma0_sq\n    \n    # Compute the term C^T * Sigma^-1 * C, which contributes to the likelihood's precision.\n    # .item() is used to extract the scalar value from the 1x1 matrix.\n    likelihood_precision_term = (C.T @ Sigma_inv @ C).item()\n    \n    # The posterior precision is the sum of the prior and likelihood precisions.\n    posterior_precision = prior_precision + likelihood_precision_term\n    \n    # The posterior variance is the reciprocal of the posterior precision.\n    sigma2_post = 1.0 / posterior_precision\n\n    # --- Step 2: Compute decoder bias (b) and weights (w) ---\n    # According to the derived formula: w = sigma^2_post * Sigma^-1 * C\n    w = sigma2_post * (Sigma_inv @ C)\n    \n    # According to the derived formula: b = sigma^2_post * (mu0/sigma0^2 - C^T * Sigma^-1 * d)\n    b = sigma2_post * (prior_precision * mu0 - (C.T @ Sigma_inv @ d).item())\n    \n    # --- Step 3: Compute posterior mean for each test case ---\n    # The posterior mean can be calculated using the decoder: mu_post = b + w^T * r\n    results = []\n    for r_t in test_cases:\n        mu_post = (b + w.T @ r_t).item()\n        results.extend([mu_post, sigma2_post])\n    \n    # --- Step 4: Assemble the final output list ---\n    # The final list includes results for the 3 cases, followed by the decoder parameters.\n    # w.flatten().tolist() converts the weight column vector to a list of scalars.\n    results.extend([b] + w.flatten().tolist())\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.12f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "超越解码已知变量，神经科学的一个前沿是揭示大规模神经活动中隐藏的组织结构。本高级练习将介绍泊松非负矩阵分解（Poisson NMF），一种用于从棘波计数数据中识别潜在“神经元集合”的强大模型。你将推导变分推断算法的更新规则，这是一种让你能够从复杂数据中发现未知协同模式的尖端机器学习方法。",
            "id": "4011649",
            "problem": "记录一个包含 $I$ 个皮层神经元的群体在 $T$ 个不重叠的时间段内的活动，得到脉冲计数数据 $\\mathbf{n} = \\{n_{i t}\\}_{i=1,\\dots,I;\\,t=1,\\dots,T}$，其中每个 $n_{i t} \\in \\{0,1,2,\\dots\\}$。假设这些计数来自 $K$ 个潜藏组合，其条件分布通过泊松叠加建模，\n$$\nn_{i t} \\,\\big|\\, \\{w_{i k}\\}_{k=1}^{K}, \\{h_{k t}\\}_{k=1}^{K} \\sim \\mathrm{Poisson}\\!\\left(\\sum_{k=1}^{K} w_{i k} h_{k t}\\right),\n$$\n其中非负因子 $w_{i k}  0$ 代表神经元到组合的负载，而 $h_{k t}  0$ 代表组合到时间的激活。为强制非负性并引入收缩，采用形状-率参数化的独立伽马先验，\n$$\nw_{i k} \\sim \\mathrm{Gamma}(a_{w}, b_{w}),\\quad h_{k t} \\sim \\mathrm{Gamma}(a_{h}, b_{h}),\n$$\n其中，对于一个泛型变量 $x$，$\\mathrm{Gamma}(a,b)$ 表示密度 $p(x \\mid a,b) = \\frac{b^{a}}{\\Gamma(a)} x^{a-1} \\exp(-b x)$，其支撑集为 $x  0$。引入潜藏计数变量 $\\{z_{i t k}\\}$ 来表示每个组合对 $n_{i t}$ 的贡献，满足 $\\sum_{k=1}^{K} z_{i t k} = n_{i t}$，并且，在完整数据生成模型下，\n$$\nz_{i t k} \\sim \\mathrm{Poisson}(w_{i k} h_{k t}) \\quad \\text{在 } (i,t,k) \\text{上独立},\n$$\n因此根据泊松叠加属性，$n_{i t} = \\sum_{k} z_{i t k}$ 成立。考虑平均场变分族\n$$\nq(\\mathbf{z}, \\mathbf{w}, \\mathbf{h}) = \\left[\\prod_{i=1}^{I} \\prod_{t=1}^{T} q(\\mathbf{z}_{i t \\cdot})\\right] \\left[\\prod_{i=1}^{I} \\prod_{k=1}^{K} q(w_{i k})\\right] \\left[\\prod_{k=1}^{K} \\prod_{t=1}^{T} q(h_{k t})\\right],\n$$\n其中 $q(w_{i k})$ 和 $q(h_{k t})$ 属于伽马族，而 $q(\\mathbf{z}_{i t \\cdot})$ 是一个在 $K$ 个分量上的多项分布，受约束 $\\sum_{k} z_{i t k} = n_{i t}$，并由责任度 $\\boldsymbol{\\phi}_{i t} = (\\phi_{i t 1},\\dots,\\phi_{i t K})$ 参数化，使得 $\\sum_{k} \\phi_{i t k} = 1$ 且 $\\phi_{i t k} \\in (0,1)$。\n\n从基本概率定义（泊松似然、伽马先验和用于最小化Kullback–Leibler散度的证据下界（Evidence Lower Bound, ELBO））出发，执行以下操作：\n\n- 推导完整数据对数联合分布 $\\ln p(\\mathbf{n}, \\mathbf{z}, \\mathbf{w}, \\mathbf{h})$（不含与 $(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})$ 无关的加性常数）。\n- 使用平均场假设，推导使ELBO相对于 $q$ 中每个因子最大化的坐标上升更新。具体来说，获得：\n  1. $q(\\mathbf{z}_{i t \\cdot})$ 的多项责任度更新，用当前 $q(w_{i k})$ 和 $q(h_{k t})$ 下的期望来表示。\n  2. $q(w_{i k})$ 和 $q(h_{k t})$ 的伽马形状-率参数，用当前变分分布下的期望来表示。\n- 使用伽马分布的期望 $\\mathbb{E}_{q}[\\ln w_{i k}]$, $\\mathbb{E}_{q}[w_{i k}]$, $\\mathbb{E}_{q}[\\ln h_{k t}]$, 和 $\\mathbb{E}_{q}[h_{k t}]$ 来表示你的最终更新，回顾对于 $x \\sim \\mathrm{Gamma}(a,b)$（形状-率参数化），有 $\\mathbb{E}[x] = \\frac{a}{b}$ 和 $\\mathbb{E}[\\ln x] = \\psi(a) - \\ln(b)$，其中 $\\psi(\\cdot)$ 是digamma函数。\n\n你的最终答案必须是一个单一的闭式解析表达式，包含所有用于责任度和伽马变分参数的坐标上升更新公式。不需要进行数值计算。不要包含任何单位。如果你引入任何首字母缩略词，请在首次出现时写出全称。最终答案必须严格按照输出结构和格式规则呈现，并且不能是包含超出上述定义的自由参数的不等式或方程。",
            "solution": "用户希望找到此问题的解。\n问题陈述的验证如下。\n\n### 步骤1：提取给定条件\n- **数据**：脉冲计数 $\\mathbf{n} = \\{n_{i t}\\}$，对于 $i=1,\\dots,I$ 个神经元和 $t=1,\\dots,T$ 个时间段。$n_{i t} \\in \\{0,1,2,\\dots\\}$。\n- **模型**：观测到的计数条件分布为一个泊松过程：\n$$n_{i t} \\,\\big|\\, \\{w_{i k}\\}, \\{h_{k t}\\} \\sim \\mathrm{Poisson}\\!\\left(\\sum_{k=1}^{K} w_{i k} h_{k t}\\right)$$\n- **因子**：非负因子 $w_{i k}  0$ 和 $h_{k t}  0$。\n- **先验**：在因子上设置独立的伽马先验：\n$$w_{i k} \\sim \\mathrm{Gamma}(a_w, b_w)$$\n$$h_{k t} \\sim \\mathrm{Gamma}(a_h, b_h)$$\n其中伽马密度在形状-率参数化下由 $p(x \\mid a,b) = \\frac{b^{a}}{\\Gamma(a)} x^{a-1} \\exp(-b x)$ 给出。\n- **潜变量**：引入潜计数 $\\{z_{i t k}\\}$，满足 $\\sum_{k=1}^{K} z_{i t k} = n_{i t}$。\n- **完整数据模型**：潜计数是独立泊松分布的：\n$$z_{i t k} \\sim \\mathrm{Poisson}(w_{i k} h_{k t})$$\n- **变分族**：对后验分布假定平均场近似：\n$$q(\\mathbf{z}, \\mathbf{w}, \\mathbf{h}) = \\left[\\prod_{i=1}^{I} \\prod_{t=1}^{T} q(\\mathbf{z}_{i t \\cdot})\\right] \\left[\\prod_{i=1}^{I} \\prod_{k=1}^{K} q(w_{i k})\\right] \\left[\\prod_{k=1}^{K} \\prod_{t=1}^{T} q(h_{k t})\\right]$$\n- **变分分布**：\n  - $q(\\mathbf{z}_{i t \\cdot})$ 是计数 $(z_{it1}, \\dots, z_{itK})$ 的多项分布，总数为 $n_{it}$，概率为 $\\boldsymbol{\\phi}_{i t} = (\\phi_{i t 1},\\dots,\\phi_{i t K})$。\n  - $q(w_{i k})$ 是一个伽马分布。\n  - $q(h_{k t})$ 是一个伽马分布。\n- **期望公式**：对于 $x \\sim \\mathrm{Gamma}(a,b)$，$\\mathbb{E}[x] = a/b$ 且 $\\mathbb{E}[\\ln x] = \\psi(a) - \\ln(b)$，其中 $\\psi(\\cdot)$ 是digamma函数。\n\n### 步骤2：使用提取的给定条件进行验证\n- **科学基础**：该问题描述了一种用于泊松非负矩阵分解（Poisson Non-negative Matrix Factorization, NMF）的贝叶斯方法，这是机器学习和计算神经科学中一个标准且成熟的模型，用于分析如脉冲序列之类的计数数据。\n- **良态问题**：该任务要求推导坐标上升变分推断（Coordinate Ascent Variational Inference, CAVI）的更新方程。这是一个定义明确的数学过程，基于变分法和概率论的既定原理，可以得出一套唯一的更新规则。\n- **客观性**：该问题使用精确的数学语言和符号陈述，没有歧义或主观内容。\n- **缺陷检查表**：该问题不违反任何无效性标准。它在科学上是合理的、可形式化的、完整的，并且在数学上是良态的。\n\n### 步骤3：结论和行动\n**问题有效**。将提供完整解答。\n\n### 解答推导\n\n按照要求，解答分为两个主要部分：首先，推导完整数据的对数联合分布，其次，推导变分参数的坐标上升更新方程。\n\n**1. 完整数据的对数联合分布**\n\n此模型中的完整数据包括潜计数 $\\mathbf{z}$、神经元到组合的权重 $\\mathbf{w}$，以及组合的激活 $\\mathbf{h}$。观测数据 $\\mathbf{n}$ 由 $\\mathbf{z}$ 通过约束 $n_{it} = \\sum_k z_{itk}$ 确定。所有变量的联合概率分布为 $p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h}) = p(\\mathbf{z}|\\mathbf{w}, \\mathbf{h})p(\\mathbf{w})p(\\mathbf{h})$。\n\n各个分量由模型设定给出：\n-   $p(\\mathbf{z}|\\mathbf{w}, \\mathbf{h}) = \\prod_{i=1}^I \\prod_{t=1}^T \\prod_{k=1}^K p(z_{itk}|w_{ik}, h_{kt}) = \\prod_{i,t,k} \\frac{(w_{ik}h_{kt})^{z_{itk}} \\exp(-w_{ik}h_{kt})}{z_{itk}!}$\n-   $p(\\mathbf{w}) = \\prod_{i=1}^I \\prod_{k=1}^K p(w_{ik}|a_w, b_w) = \\prod_{i,k} \\frac{b_w^{a_w}}{\\Gamma(a_w)} w_{ik}^{a_w-1} \\exp(-b_w w_{ik})$\n-   $p(\\mathbf{h}) = \\prod_{k=1}^K \\prod_{t=1}^T p(h_{kt}|a_h, b_h) = \\prod_{k,t} \\frac{b_h^{a_h}}{\\Gamma(a_h)} h_{kt}^{a_h-1} \\exp(-b_h h_{kt})$\n\n联合分布的对数 $\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})$ 是这些分量对数的总和。我们省略了对于 $\\mathbf{z}$、$\\mathbf{w}$ 和 $\\mathbf{h}$ 为常数的加性项。\n\n$\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h}) = \\sum_{i,t,k} \\ln p(z_{itk}|w_{ik}, h_{kt}) + \\sum_{i,k} \\ln p(w_{ik}) + \\sum_{k,t} \\ln p(h_{kt})$\n\n展开每一项：\n-   $\\ln p(z_{itk}|w_{ik}, h_{kt}) = z_{itk}\\ln(w_{ik}h_{kt}) - w_{ik}h_{kt} - \\ln(z_{itk}!) = z_{itk}\\ln w_{ik} + z_{itk}\\ln h_{kt} - w_{ik}h_{kt} - \\ln(z_{itk}!)$\n-   $\\ln p(w_{ik}) = (a_w-1)\\ln w_{ik} - b_w w_{ik} + a_w \\ln b_w - \\ln\\Gamma(a_w)$\n-   $\\ln p(h_{kt}) = (a_h-1)\\ln h_{kt} - b_h h_{kt} + a_h \\ln b_h - \\ln\\Gamma(a_h)$\n\n求和并舍去常数（仅涉及 $a_w, b_w, a_h, b_h$ 的项），我们得到完整数据的对数联合分布：\n$$\n\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h}) \\stackrel{c}{=} \\sum_{i,t,k} \\left( z_{itk} (\\ln w_{ik} + \\ln h_{kt}) - w_{ik}h_{kt} - \\ln(z_{itk}!) \\right) + \\sum_{i,k} \\left( (a_w-1)\\ln w_{ik} - b_w w_{ik} \\right) + \\sum_{k,t} \\left( (a_h-1)\\ln h_{kt} - b_h h_{kt} \\right)\n$$\n项 $\\ln(z_{itk}!)$ 被保留，因为它依赖于潜变量 $z_{itk}$。\n\n**2. 坐标上升变分推断（CAVI）更新**\n\nCAVI在保持其他因子固定的情况下，迭代优化变分分布q的每个因子。对于变量 $\\theta_j$ 上的因子 $q_j$，其通用更新规则由下式给出：\n$$ \\ln q_j^*(\\theta_j) = \\mathbb{E}_{q_{\\neg j}}[\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})] + \\text{const} $$\n其中 $\\mathbb{E}_{q_{\\neg j}}$ 表示关于q中所有其他因子的期望。\n\n**$q(\\mathbf{z}_{it\\cdot})$ 的更新**\n我们寻求对于一个特定的 $(i,t)$，$q(\\mathbf{z}_{it\\cdot})$ 的最优形式。我们从 $\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})$ 中收集所有涉及任何 $z_{itk}$（对于 $k=1,\\dots,K$）的项。\n$$ \\ln q^*(\\mathbf{z}_{it\\cdot}) \\stackrel{c}{=} \\mathbb{E}_{q(\\mathbf{w},\\mathbf{h})} \\left[ \\sum_{k=1}^K \\left( z_{itk}(\\ln w_{ik} + \\ln h_{kt}) - \\ln(z_{itk}!) \\right) \\right] $$\n由于平均场假设，期望变为：\n$$ \\ln q^*(\\mathbf{z}_{it\\cdot}) \\stackrel{c}{=} \\sum_{k=1}^K \\left( z_{itk}(\\mathbb{E}_q[\\ln w_{ik}] + \\mathbb{E}_q[\\ln h_{kt}]) - \\ln(z_{itk}!) \\right) = \\sum_{k=1}^K z_{itk} \\ln \\rho_{itk} - \\sum_{k=1}^K \\ln(z_{itk}!) $$\n其中我们定义 $\\ln \\rho_{itk} = \\mathbb{E}_q[\\ln w_{ik}] + \\mathbb{E}_q[\\ln h_{kt}]$。这是一个独立泊松分布乘积 $\\prod_k \\mathrm{Poisson}(z_{itk} | \\rho_{itk})$ 的对数核，受约束 $\\sum_k z_{itk} = n_{it}$。给定总和的泊松计数的条件分布是多项分布。\n因此，$q^*(\\mathbf{z}_{it\\cdot})$ 是一个多项分布，其试验次数为 $n_{it}$，概率参数 $\\phi_{itk}$ 由下式给出：\n$$ \\phi_{itk} = \\frac{\\rho_{itk}}{\\sum_{k'=1}^K \\rho_{itk'}} = \\frac{\\exp(\\mathbb{E}_q[\\ln w_{ik}] + \\mathbb{E}_q[\\ln h_{kt}])}{\\sum_{k'=1}^K \\exp(\\mathbb{E}_q[\\ln w_{ik'}] + \\mathbb{E}_q[\\ln h_{k't}])} $$\n在此分布下 $z_{itk}$ 的期望是 $\\mathbb{E}_q[z_{itk}] = n_{it} \\phi_{itk}$。此结果被用于其他更新中。\n\n**$q(w_{ik})$ 的更新**\n对于一个特定的 $(i,k)$，我们从 $\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})$ 中收集涉及 $w_{ik}$ 的项：\n$$ \\ln q^*(w_{ik}) \\stackrel{c}{=} \\mathbb{E}_{q_{\\neg w_{ik}}} \\left[ \\sum_{t=1}^T (z_{itk} \\ln w_{ik} - w_{ik} h_{kt}) + (a_w-1)\\ln w_{ik} - b_w w_{ik} \\right] $$\n$$ \\ln q^*(w_{ik}) \\stackrel{c}{=} \\left( (a_w-1) + \\sum_{t=1}^T \\mathbb{E}_q[z_{itk}] \\right) \\ln w_{ik} - \\left( b_w + \\sum_{t=1}^T \\mathbb{E}_q[h_{kt}] \\right) w_{ik} $$\n这是伽马分布的对数核，$\\ln p(x|a,b) \\stackrel{c}{=} (a-1)\\ln x - bx$。通过匹配项，我们识别出更新后的形状和率参数，我们将其表示为 $\\hat{a}_{w_{ik}}$ 和 $\\hat{b}_{w_{ik}}$：\n$$ \\hat{a}_{w_{ik}} - 1 = a_w - 1 + \\sum_{t=1}^T \\mathbb{E}_q[z_{itk}] \\implies \\hat{a}_{w_{ik}} = a_w + \\sum_{t=1}^T n_{it} \\phi_{itk} $$\n$$ \\hat{b}_{w_{ik}} = b_w + \\sum_{t=1}^T \\mathbb{E}_q[h_{kt}] $$\n所以，$q^*(w_{ik}) = \\mathrm{Gamma}(w_{ik} | \\hat{a}_{w_{ik}}, \\hat{b}_{w_{ik}})$。\n\n**$q(h_{kt})$ 的更新**\n推导过程与 $w_{ik}$ 的推导对称。对于一个特定的 $(k,t)$，我们从 $\\ln p(\\mathbf{z}, \\mathbf{w}, \\mathbf{h})$ 中收集涉及 $h_{kt}$ 的项：\n$$ \\ln q^*(h_{kt}) \\stackrel{c}{=} \\mathbb{E}_{q_{\\neg h_{kt}}} \\left[ \\sum_{i=1}^I (z_{itk} \\ln h_{kt} - w_{ik} h_{kt}) + (a_h-1)\\ln h_{kt} - b_h h_{kt} \\right] $$\n$$ \\ln q^*(h_{kt}) \\stackrel{c}{=} \\left( (a_h-1) + \\sum_{i=1}^I \\mathbb{E}_q[z_{itk}] \\right) \\ln h_{kt} - \\left( b_h + \\sum_{i=1}^I \\mathbb{E}_q[w_{ik}] \\right) h_{kt} $$\n这同样是伽马分布的对数核。更新后的形状和率参数 $\\hat{a}_{h_{kt}}$ 和 $\\hat{b}_{h_{kt}}$ 为：\n$$ \\hat{a}_{h_{kt}} - 1 = a_h - 1 + \\sum_{i=1}^I \\mathbb{E}_q[z_{itk}] \\implies \\hat{a}_{h_{kt}} = a_h + \\sum_{i=1}^I n_{it} \\phi_{itk} $$\n$$ \\hat{b}_{h_{kt}} = b_h + \\sum_{i=1}^I \\mathbb{E}_q[w_{ik}] $$\n所以，$q^*(h_{kt}) = \\mathrm{Gamma}(h_{kt} | \\hat{a}_{h_{kt}}, \\hat{b}_{h_{kt}})$。\n\n这些方程定义了CAVI算法的一个完整周期。在每次迭代中，依次更新每个变分分布的参数，使用其他变量的当前期望。期望 $\\mathbb{E}_q[\\cdot]$ 是使用当前的变分参数计算的，例如，$\\mathbb{E}_q[w_{ik}] = \\hat{a}_{w_{ik}}/\\hat{b}_{w_{ik}}$ 和 $\\mathbb{E}_q[\\ln w_{ik}] = \\psi(\\hat{a}_{w_{ik}}) - \\ln(\\hat{b}_{w_{ik}})$。",
            "answer": "$$\n\\boxed{\n\\begin{aligned}\n\\phi_{itk}  \\leftarrow \\frac{\\exp\\left(\\mathbb{E}_{q}[\\ln w_{ik}] + \\mathbb{E}_{q}[\\ln h_{kt}]\\right)}{\\sum_{k'=1}^{K} \\exp\\left(\\mathbb{E}_{q}[\\ln w_{ik'}] + \\mathbb{E}_{q}[\\ln h_{k't}]\\right)} \\\\\n\\hat{a}_{w_{ik}}  \\leftarrow a_w + \\sum_{t=1}^{T} n_{it} \\phi_{itk} \\\\\n\\hat{b}_{w_{ik}}  \\leftarrow b_w + \\sum_{t=1}^{T} \\mathbb{E}_{q}[h_{kt}] \\\\\n\\hat{a}_{h_{kt}}  \\leftarrow a_h + \\sum_{i=1}^{I} n_{it} \\phi_{itk} \\\\\n\\hat{b}_{h_{kt}}  \\leftarrow b_h + \\sum_{i=1}^{I} \\mathbb{E}_{q}[w_{ik}]\n\\end{aligned}\n}\n$$"
        }
    ]
}