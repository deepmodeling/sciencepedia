{
    "hands_on_practices": [
        {
            "introduction": "Understanding the stochastic nature of synaptic transmission is fundamental to neuroscience. This first practice explores the classic binomial model of neurotransmitter release and contrasts it with the widely used Poisson approximation. By calculating key statistical moments like the mean, variance, and Fano factor, you will gain a deeper intuition for how these models capture—or fail to capture—the biophysical constraints of a synapse.",
            "id": "2738674",
            "problem": "A single synaptic terminal contains $n=5$ independent, functionally identical release sites. In response to a brief presynaptic action potential, each site releases at most one synaptic vesicle with probability $p=0.2$, independently of other sites and trials. Let $K$ denote the number of vesicles released in a single trial (one action potential). Assume that the postsynaptic detection is perfect so that $K$ is directly observed as the vesicle count per trial.\n\nStarting from first principles appropriate to cellular and molecular neuroscience and probability theory—namely, independent Bernoulli trials for release at each site, combinatorial counting of microstates, and definitions of expectation, variance, and the Fano factor defined as $\\mathrm{Var}(K)/\\mathbb{E}[K]$—derive the exact count distribution of $K$ for $k=0,1,2,3,4,5$, and compute its mean, variance, and Fano factor. Then consider a Poisson process model of neurotransmitter release with the same mean $\\lambda = n p$. Compare the mean, variance, and Fano factor of the binomial site model to those of the Poisson model.\n\nReport as your final answer the absolute difference between the Fano factor of the binomial site model and that of the matched-mean Poisson model. Round your answer to $4$ significant figures and report it as a pure number without units.",
            "solution": "The problem statement is subjected to validation.\n\nGivens extracted verbatim are:\n- Number of independent, functionally identical release sites: $n=5$.\n- Probability of release at each site per trial: $p=0.2$.\n- $K$ is the number of vesicles released in a single trial.\n- A Poisson process model is to be considered with a mean $\\lambda = n p$.\n- The Fano factor is defined as $\\mathrm{Var}(K)/\\mathbb{E}[K]$.\n- The final answer is the absolute difference between the Fano factor of the binomial site model and that of the matched-mean Poisson model, rounded to $4$ significant figures.\n\nValidation assessment:\n- **Scientifically Grounded:** The problem uses the binomial model of synaptic transmission, a foundational concept in cellular neuroscience established by Katz and colleagues. The comparison with a Poisson distribution is a standard and informative analysis in this field. The parameters $n=5$ and $p=0.2$ are within a realistic range for certain types of synapses. The problem is scientifically sound.\n- **Well-Posed:** All necessary parameters and definitions are provided. The tasks are explicit and lead to a unique, stable, and meaningful solution.\n- **Objective:** The language is formal, precise, and devoid of subjective or speculative content.\n\nVerdict: The problem is valid. It presents a standard, well-defined exercise in quantitative neuroscience and probability theory. The solution process may proceed.\n\nThe problem describes a process of $n=5$ independent Bernoulli trials, where each trial (the potential release from one site) has a success probability of $p=0.2$. The total number of successes (released vesicles), $K$, therefore follows a binomial distribution, $K \\sim \\mathrm{Binomial}(n, p)$.\n\nThe probability mass function (PMF) for a binomial distribution is given by:\n$$P(K=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\nwhere $k$ is the number of successes, which can range from $0$ to $n$.\n\nFirst, we will compute the exact count distribution for $K$ given $n=5$ and $p=0.2$. This requires calculating $P(K=k)$ for $k \\in \\{0, 1, 2, 3, 4, 5\\}$. Note that $1-p = 1 - 0.2 = 0.8$.\n\n- For $k=0$: $P(K=0) = \\binom{5}{0} (0.2)^0 (0.8)^5 = 1 \\times 1 \\times 0.32768 = 0.32768$\n- For $k=1$: $P(K=1) = \\binom{5}{1} (0.2)^1 (0.8)^4 = 5 \\times 0.2 \\times 0.4096 = 0.4096$\n- For $k=2$: $P(K=2) = \\binom{5}{2} (0.2)^2 (0.8)^3 = 10 \\times 0.04 \\times 0.512 = 0.2048$\n- For $k=3$: $P(K=3) = \\binom{5}{3} (0.2)^3 (0.8)^2 = 10 \\times 0.008 \\times 0.64 = 0.0512$\n- For $k=4$: $P(K=4) = \\binom{5}{4} (0.2)^4 (0.8)^1 = 5 \\times 0.0016 \\times 0.8 = 0.0064$\n- For $k=5$: $P(K=5) = \\binom{5}{5} (0.2)^5 (0.8)^0 = 1 \\times 0.00032 \\times 1 = 0.00032$\n\nNext, we compute the mean, variance, and Fano factor for this binomial model.\nThe expectation (mean) of a binomial distribution is given by:\n$$\\mathbb{E}[K] = np$$\nSubstituting the given values:\n$$\\mathbb{E}[K] = 5 \\times 0.2 = 1$$\n\nThe variance of a binomial distribution is given by:\n$$\\mathrm{Var}(K) = np(1-p)$$\nSubstituting the given values:\n$$\\mathrm{Var}(K) = 5 \\times 0.2 \\times (1 - 0.2) = 1 \\times 0.8 = 0.8$$\n\nThe Fano factor for the binomial model, $\\mathrm{FF}_{\\text{Binomial}}$, is the ratio of the variance to the mean:\n$$\\mathrm{FF}_{\\text{Binomial}} = \\frac{\\mathrm{Var}(K)}{\\mathbb{E}[K]} = \\frac{np(1-p)}{np} = 1-p$$\nUsing the given value of $p$:\n$$\\mathrm{FF}_{\\text{Binomial}} = 1 - 0.2 = 0.8$$\n\nNow, we consider a Poisson process model with a mean $\\lambda$ matched to the mean of the binomial model. Let $K_{\\text{Poisson}}$ be the random variable for this model.\nThe matching condition is:\n$$\\lambda = \\mathbb{E}[K] = np = 1$$\nSo, we consider a Poisson distribution with parameter $\\lambda=1$.\n\nThe fundamental properties of a Poisson distribution are that its mean and variance are both equal to its parameter $\\lambda$.\nThe mean of the Poisson model is:\n$$\\mathbb{E}[K_{\\text{Poisson}}] = \\lambda = 1$$\nThe variance of the Poisson model is:\n$$\\mathrm{Var}(K_{\\text{Poisson}}) = \\lambda = 1$$\n\nThe Fano factor for the Poisson model, $\\mathrm{FF}_{\\text{Poisson}}$, is therefore:\n$$\\mathrm{FF}_{\\text{Poisson}} = \\frac{\\mathrm{Var}(K_{\\text{Poisson}})}{\\mathbb{E}[K_{\\text{Poisson}}]} = \\frac{\\lambda}{\\lambda} = 1$$\nThis result is definitional for any Poisson process; its variance is always equal to its mean, yielding a Fano factor of unity.\n\nThe final task is to compute the absolute difference between the Fano factor of the binomial site model and that of the matched-mean Poisson model.\n$$\\Delta \\mathrm{FF} = |\\mathrm{FF}_{\\text{Binomial}} - \\mathrm{FF}_{\\text{Poisson}}|$$\nSubstituting the calculated values:\n$$\\Delta \\mathrm{FF} = |0.8 - 1| = |-0.2| = 0.2$$\n\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$0.2 \\rightarrow 0.2000$$\n\nThis difference highlights a key feature of the binomial release model: because the number of release sites $n$ is finite, the variance is suppressed relative to the mean. The release process is \"sub-Poissonian\", with a Fano factor strictly less than $1$ (for $p0$). The magnitude of this deviation from the Poissonian Fano factor of $1$ is simply the release probability $p$.",
            "answer": "$$\\boxed{0.2000}$$"
        },
        {
            "introduction": "In analyzing neural data, we often apply nonlinear transformations to stabilize variance or compress dynamic ranges. This exercise uses a simple Bernoulli model of spiking to demonstrate a key mathematical principle: Jensen's inequality. You will analytically derive how a concave function, like the square root, systematically creates a gap between the function of the mean firing rate and the mean of the transformed rate, a crucial concept for correctly interpreting processed data.",
            "id": "4160976",
            "problem": "A single-neuron spike train is discretized into small bins of width $\\Delta t$ so that at most one spike occurs per bin. Let $X$ denote the binary spike indicator in a given bin, where $X \\in \\{0,1\\}$ and $X \\sim \\mathrm{Bernoulli}(p)$ with firing probability $p \\in [0,1]$. In a common preprocessing step for point-process data, a compressive nonlinearity $g$ is applied to stabilize variance. Consider the concave transformation $g(x) = \\sqrt{x}$, which is well-defined on $[0,1]$ and reflects a compressive mapping used in neural data analysis.\n\nUsing only the definitions of expectation and concavity, construct an explicit example showing that the inequality relating $g(\\mathbb{E}[X])$ and $\\mathbb{E}[g(X)]$ reverses relative to the form that holds for convex transformations. Then, for the Bernoulli input described above, compute the exact analytic expression for the gap\n$$\\Delta(p) \\equiv g(\\mathbb{E}[X]) - \\mathbb{E}[g(X)],$$\nas a function of $p \\in [0,1]$. Provide your final answer as a closed-form expression in terms of $p$. No numerical approximation or rounding is required.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective. It is based on standard models and concepts in computational neuroscience and probability theory, specifically the Bernoulli process for spike trains and the application of transformations to random variables. All necessary information is provided, and the problem is free of contradictions or ambiguities. It is a valid problem.\n\nThe problem requires us to analyze the effect of a concave transformation $g(x) = \\sqrt{x}$ on a Bernoulli-distributed random variable $X$. The core of the problem lies in applying the definition of expectation and understanding Jensen's inequality. For a concave function $g$, Jensen's inequality states that $\\mathbb{E}[g(X)] \\le g(\\mathbb{E}[X])$. We are asked to first provide an example of this and then to compute the exact difference $\\Delta(p) = g(\\mathbb{E}[X]) - \\mathbb{E}[g(X)]$.\n\nLet the random variable $X$ represent the spike indicator, such that $X \\sim \\mathrm{Bernoulli}(p)$. The probability mass function (PMF) of $X$ is given by:\n$$\nP(X=k) = \\begin{cases}\np  \\text{if } k=1 \\\\\n1-p  \\text{if } k=0\n\\end{cases}\n$$\nThe support of $X$ is the set $\\{0, 1\\}$.\n\nFirst, we compute the expectation of $X$, denoted by $\\mathbb{E}[X]$. By definition, for a discrete random variable, the expectation is the sum of each possible value multiplied by its probability:\n$$\n\\mathbb{E}[X] = \\sum_{k \\in \\{0,1\\}} k \\cdot P(X=k) = (0 \\cdot P(X=0)) + (1 \\cdot P(X=1))\n$$\nSubstituting the probabilities from the PMF:\n$$\n\\mathbb{E}[X] = (0 \\cdot (1-p)) + (1 \\cdot p) = 0 + p = p\n$$\n\nNext, we evaluate the term $g(\\mathbb{E}[X])$. The function $g$ is defined as $g(x) = \\sqrt{x}$. Applying this function to the expectation of $X$:\n$$\ng(\\mathbb{E}[X]) = g(p) = \\sqrt{p}\n$$\n\nNow, we compute the expectation of the transformed random variable, $\\mathbb{E}[g(X)]$. The random variable $g(X)$ takes the value $g(0) = \\sqrt{0} = 0$ when $X=0$, and the value $g(1) = \\sqrt{1} = 1$ when $X=1$. The probabilities of these outcomes are the same as for $X$: $P(g(X)=0) = P(X=0) = 1-p$ and $P(g(X)=1) = P(X=1) = p$.\n\nUsing the definition of expectation for the random variable $g(X)$:\n$$\n\\mathbb{E}[g(X)] = \\sum_{k \\in \\{0,1\\}} g(k) \\cdot P(X=k) = (g(0) \\cdot P(X=0)) + (g(1) \\cdot P(X=1))\n$$\nSubstituting the values for $g(k)$ and the probabilities:\n$$\n\\mathbb{E}[g(X)] = (\\sqrt{0} \\cdot (1-p)) + (\\sqrt{1} \\cdot p) = (0 \\cdot (1-p)) + (1 \\cdot p) = 0 + p = p\n$$\n\nThe first part of the problem asks for an explicit example that illustrates the inequality for concave functions. We can use our derived expressions. Jensen's inequality for a concave function $g$ is $\\mathbb{E}[g(X)] \\le g(\\mathbb{E}[X])$. Using our results, this translates to $p \\le \\sqrt{p}$. This inequality holds for all $p \\in [0, 1]$. Let's select a specific value for $p$ in the open interval $(0, 1)$ to construct a non-trivial example, for instance $p = \\frac{1}{4}$.\nFor $p = \\frac{1}{4}$:\n$\\mathbb{E}[X] = \\frac{1}{4}$.\n$g(\\mathbb{E}[X]) = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}$.\n$\\mathbb{E}[g(X)] = p = \\frac{1}{4}$.\nThus, we have $\\mathbb{E}[g(X)] = \\frac{1}{4}$ and $g(\\mathbb{E}[X]) = \\frac{1}{2}$, and the inequality $\\frac{1}{4} \\le \\frac{1}{2}$ holds, confirming Jensen's inequality for this specific case. Strict inequality holds because the random variable is not a constant and the function is strictly concave on $(0, \\infty)$.\n\nThe second part is to compute the analytical expression for the gap $\\Delta(p)$, defined as:\n$$\n\\Delta(p) \\equiv g(\\mathbb{E}[X]) - \\mathbb{E}[g(X)]\n$$\nSubstituting the expressions we derived for $g(\\mathbb{E}[X])$ and $\\mathbb{E}[g(X)]$:\n$$\n\\Delta(p) = \\sqrt{p} - p\n$$\nThis expression represents the difference between the transformed mean and the mean of the transformed variable. As noted, for $p \\in (0, 1)$, $\\sqrt{p}  p$, so $\\Delta(p)  0$. The gap is zero only at the boundaries of the interval, i.e., at $p=0$ and $p=1$, where the random variable becomes deterministic.\nThe final closed-form expression for the gap as a function of $p$ is $\\sqrt{p} - p$.",
            "answer": "$$\\boxed{\\sqrt{p} - p}$$"
        },
        {
            "introduction": "A central goal of computational neuroscience is to decode cognitive or behavioral variables from neural population activity. This practice guides you through the construction of a complete Bayesian decoder using a linear-Gaussian model, a cornerstone of the field. By deriving and implementing the posterior distribution for a latent variable, you will translate the abstract rules of probability into a functional tool for reading the neural code.",
            "id": "4011627",
            "problem": "You are given a population of $N$ neurons with firing rates modeled under a linear Gaussian observation framework for decoding a one-dimensional reaching direction variable. The instantaneous reaching direction at time $t$ is denoted by $x_t$ and is treated as a scalar angle in radians. The population firing rate vector at time $t$ is denoted by $\\mathbf{r}_t \\in \\mathbb{R}^N$ and is measured in spikes per second (spk/s). The generative model consists of the following components:\n\n1. A Gaussian prior over the latent reaching direction,\n$ p(x_t) = \\mathcal{N}(x_t \\mid \\mu_0, \\sigma_0^2), $\nwhere $x_t$ is in radians, $\\mu_0$ is in radians, and $\\sigma_0^2$ is in radians squared.\n\n2. A multivariate Gaussian likelihood (observation model) for the population firing rates given the latent $x_t$,\n$ p(\\mathbf{r}_t \\mid x_t) = \\mathcal{N}(\\mathbf{r}_t \\mid \\mathbf{C} x_t + \\mathbf{d}, \\boldsymbol{\\Sigma}), $\nwhere $\\mathbf{C} \\in \\mathbb{R}^N$ encodes the linear tuning slope of each neuron with units of spk/s per radian, $\\mathbf{d} \\in \\mathbb{R}^N$ encodes the baseline firing rates in spk/s, and $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{N \\times N}$ is a positive-definite covariance matrix with units of $(\\text{spk/s})^2$.\n\nTask:\n- Starting from the basic definitions of Bayes' rule and properties of the multivariate normal distribution, derive the posterior distribution $p(x_t \\mid \\mathbf{r}_t)$, specifying its mean and variance as explicit functions of $\\mu_0$, $\\sigma_0^2$, $\\mathbf{C}$, $\\mathbf{d}$, $\\boldsymbol{\\Sigma}$, and the observation $\\mathbf{r}_t$.\n- Construct a linear decoder for $x_t$ from $\\mathbf{r}_t$ that equals the posterior mean under the stated model. Express this decoder as an affine mapping $ \\hat{x}_t = b + \\mathbf{w}^\\top \\mathbf{r}_t $, and provide explicit expressions for the bias $b$ (in radians) and weights $\\mathbf{w} \\in \\mathbb{R}^N$ (in radians per spk/s) in terms of the model parameters.\n\nAngle units must be in radians throughout. Firing rate units are in spk/s. Covariance units are $(\\text{spk/s})^2$.\n\nUse the following fixed parameter values with $N = 4$:\n- Prior parameters: $ \\mu_0 = 0.3 $ (radians), $ \\sigma_0^2 = 0.04 $ (radians squared).\n- Tuning slopes: $ \\mathbf{C} = [0.7,\\,-0.5,\\,0.3,\\,0.1] $ (spk/s per radian).\n- Baseline rates: $ \\mathbf{d} = [15.0,\\,10.0,\\,12.0,\\,8.0] $ (spk/s).\n- Observation noise covariance:\n$$\n\\boldsymbol{\\Sigma} =\n\\begin{bmatrix}\n4.0  0.6  0.2  0.1 \\\\\n0.6  3.0  0.4  0.2 \\\\\n0.2  0.4  2.5  0.3 \\\\\n0.1  0.2  0.3  3.5 \\\\\n\\end{bmatrix}\n\\quad \\text{(spk/s)}^2.\n$$\n\nTest suite (all firing rates in spk/s):\n- Case 1 (baseline): $ \\mathbf{r}_1 = [15.0,\\,10.0,\\,12.0,\\,8.0] $.\n- Case 2 (positive deviation along tuning): $ \\mathbf{r}_2 = \\mathbf{d} + 2 \\mathbf{C} = [16.4,\\,9.0,\\,12.6,\\,8.2] $.\n- Case 3 (negative deviation along tuning): $ \\mathbf{r}_3 = \\mathbf{d} - 1.5 \\mathbf{C} = [13.95,\\,10.75,\\,11.55,\\,7.85] $.\n\nYour program must:\n1. Compute the posterior mean and variance for $x_t$ for each test case.\n2. Compute the decoder bias $b$ and weights $\\mathbf{w}$ such that the posterior mean equals $ b + \\mathbf{w}^\\top \\mathbf{r}_t $.\n3. Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n$ [\\mu_{\\text{post},1},\\,\\sigma^2_{\\text{post},1},\\,\\mu_{\\text{post},2},\\,\\sigma^2_{\\text{post},2},\\,\\mu_{\\text{post},3},\\,\\sigma^2_{\\text{post},3},\\,b,\\,w_1,\\,w_2,\\,w_3,\\,w_4] $.\nAll angles ($\\mu_{\\text{post},i}$ and $b$) must be in radians, and all variances ($\\sigma^2_{\\text{post},i}$) must be in radians squared. Decoder weights $w_i$ must be in radians per spk/s.\n\nNo external input is allowed; hard-code the given parameters and test suite in your program. The program must be self-contained and runnable as is.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in computational neuroscience involving Bayesian inference with a linear-Gaussian model. All necessary parameters and conditions are provided, and there are no internal contradictions or ambiguities.\n\n### Derivation of the Posterior Distribution\n\nOur objective is to determine the posterior distribution of the latent reaching direction $x_t$ given the observed neural population firing rate vector $\\mathbf{r}_t$, denoted as $p(x_t \\mid \\mathbf{r}_t)$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$ p(x_t \\mid \\mathbf{r}_t) \\propto p(\\mathbf{r}_t \\mid x_t) p(x_t) $$\nThe problem defines a Gaussian prior and a Gaussian likelihood:\n1.  Prior: $ p(x_t) = \\mathcal{N}(x_t \\mid \\mu_0, \\sigma_0^2) $\n2.  Likelihood: $ p(\\mathbf{r}_t \\mid x_t) = \\mathcal{N}(\\mathbf{r}_t \\mid \\mathbf{C} x_t + \\mathbf{d}, \\boldsymbol{\\Sigma}) $\n\nThe product of two Gaussian distributions is an unnormalized Gaussian. To find the parameters of the resulting posterior distribution, we work with the exponents of the probability density functions. The probability density function (PDF) of a Gaussian distribution $\\mathcal{N}(z \\mid \\mu, \\Sigma)$ is proportional to $\\exp(-\\frac{1}{2}(z-\\mu)^\\top \\Sigma^{-1}(z-\\mu))$.\n\nThe exponent of the prior distribution $p(x_t)$ is:\n$$ \\mathcal{L}_{\\text{prior}} = -\\frac{1}{2\\sigma_0^2}(x_t - \\mu_0)^2 = -\\frac{1}{2}\\left( \\frac{1}{\\sigma_0^2}x_t^2 - \\frac{2\\mu_0}{\\sigma_0^2}x_t \\right) + \\text{const}_1 $$\nThe exponent of the likelihood distribution $p(\\mathbf{r}_t \\mid x_t)$ is:\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} (\\mathbf{r}_t - (\\mathbf{C} x_t + \\mathbf{d}))^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - (\\mathbf{C} x_t + \\mathbf{d})) $$\nLet's expand the quadratic form in the likelihood's exponent, focusing on terms involving $x_t$:\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} ((\\mathbf{r}_t - \\mathbf{d}) - \\mathbf{C}x_t)^\\top \\boldsymbol{\\Sigma}^{-1} ((\\mathbf{r}_t - \\mathbf{d}) - \\mathbf{C}x_t) $$\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} \\left( x_t \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} x_t - x_t \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}) - (\\mathbf{r}_t - \\mathbf{d})^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} x_t \\right) + \\text{const}_2 $$\nSince all terms are scalars, we can combine the two linear terms in $x_t$:\n$$ \\mathcal{L}_{\\text{like}} = -\\frac{1}{2} \\left( (\\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C})x_t^2 - 2(\\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}))x_t \\right) + \\text{const}_2 $$\n\nThe exponent of the posterior, $\\mathcal{L}_{\\text{post}}$, is the sum of the prior and likelihood exponents. We collect terms in powers of $x_t$:\n$$ \\mathcal{L}_{\\text{post}} = \\mathcal{L}_{\\text{prior}} + \\mathcal{L}_{\\text{like}} $$\n$$ \\mathcal{L}_{\\text{post}} = -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C}\\right)x_t^2 - 2\\left(\\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d})\\right)x_t \\right] + \\text{const}_3 $$\nThis is the exponent of a Gaussian distribution for $x_t$, which we denote $p(x_t \\mid \\mathbf{r}_t) = \\mathcal{N}(x_t \\mid \\mu_{\\text{post}}, \\sigma^2_{\\text{post}})$. The exponent of this posterior distribution has the canonical form:\n$$ -\\frac{1}{2\\sigma^2_{\\text{post}}}(x_t - \\mu_{\\text{post}})^2 = -\\frac{1}{2}\\left( \\frac{1}{\\sigma^2_{\\text{post}}}x_t^2 - \\frac{2\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}}x_t \\right) + \\text{const}_4 $$\nBy matching the coefficients of the $x_t^2$ and $x_t$ terms, we identify the posterior precision $(\\sigma^2_{\\text{post}})^{-1}$ and mean $\\mu_{\\text{post}}$.\n\nThe coefficient of $x_t^2$ gives the posterior precision:\n$$ \\frac{1}{\\sigma^2_{\\text{post}}} = \\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} $$\nThus, the posterior variance is:\n$$ \\sigma^2_{\\text{post}} = \\left( \\frac{1}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} \\right)^{-1} $$\nNotice that the posterior variance is independent of the observation $\\mathbf{r}_t$.\n\nThe coefficient of $x_t$ gives:\n$$ \\frac{2\\mu_{\\text{post}}}{\\sigma^2_{\\text{post}}} = 2\\left(\\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d})\\right) $$\nSolving for the posterior mean $\\mu_{\\text{post}}$:\n$$ \\mu_{\\text{post}} = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}_t - \\mathbf{d}) \\right) $$\n\n### Derivation of the Linear Decoder\n\nThe posterior mean $\\mu_{\\text{post}}$ is the optimal Bayesian estimator for $x_t$ under a squared error loss. We define our linear decoder $\\hat{x}_t$ to be this posterior mean, $\\hat{x}_t = \\mu_{\\text{post}}$. We need to express this in the affine form $\\hat{x}_t = b + \\mathbf{w}^\\top \\mathbf{r}_t$.\nLet's expand the expression for $\\mu_{\\text{post}}$ and rearrange it:\n$$ \\mu_{\\text{post}} = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{r}_t - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right) $$\n$$ \\mu_{\\text{post}} = \\underbrace{\\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right)}_{b} + \\underbrace{\\left(\\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1}\\right)}_{\\mathbf{w}^\\top} \\mathbf{r}_t $$\nFrom this rearrangement, we can identify the bias $b$ and the weight vector $\\mathbf{w}$.\n\nThe bias term $b$ is:\n$$ b = \\sigma^2_{\\text{post}} \\left( \\frac{\\mu_0}{\\sigma_0^2} - \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{d} \\right) $$\nThe row vector of weights $\\mathbf{w}^\\top$ is:\n$$ \\mathbf{w}^\\top = \\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1} $$\nTaking the transpose, we obtain the column vector of weights $\\mathbf{w}$:\n$$ \\mathbf{w} = (\\sigma^2_{\\text{post}} \\mathbf{C}^\\top \\boldsymbol{\\Sigma}^{-1})^\\top = \\sigma^2_{\\text{post}} (\\boldsymbol{\\Sigma}^{-1})^\\top (\\mathbf{C}^\\top)^\\top = \\sigma^2_{\\text{post}} \\boldsymbol{\\Sigma}^{-1} \\mathbf{C} $$\nThe last step uses the fact that the inverse of a symmetric matrix $\\boldsymbol{\\Sigma}$ is also symmetric, i.e., $(\\boldsymbol{\\Sigma}^{-1})^\\top = \\boldsymbol{\\Sigma}^{-1}$. These formulas for $b$ and $\\mathbf{w}$ allow for direct computation from the model parameters.\n\n### Numerical Computation\nThe derived formulas will now be implemented with the provided numerical values to find the required quantities for the three test cases, along with the constant decoder parameters $b$ and $\\mathbf{w}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the posterior distribution parameters and the linear decoder\n    parameters for a linear-Gaussian model of neural population decoding.\n    \"\"\"\n    # Define fixed parameters from the problem statement.\n    # N is the number of neurons.\n    N = 4\n    \n    # Prior parameters\n    mu0 = 0.3  # radians\n    sigma0_sq = 0.04  # radians^2\n\n    # Likelihood parameters\n    C = np.array([0.7, -0.5, 0.3, 0.1]).reshape(N, 1)  # spk/s per radian\n    d = np.array([15.0, 10.0, 12.0, 8.0]).reshape(N, 1)  # spk/s\n    Sigma = np.array([\n        [4.0, 0.6, 0.2, 0.1],\n        [0.6, 3.0, 0.4, 0.2],\n        [0.2, 0.4, 2.5, 0.3],\n        [0.1, 0.2, 0.3, 3.5]\n    ])  # (spk/s)^2\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (baseline): r1 = d\n        np.array([15.0, 10.0, 12.0, 8.0]).reshape(N, 1),\n        # Case 2 (positive deviation): r2 = d + 2*C\n        np.array([16.4, 9.0, 12.6, 8.2]).reshape(N, 1),\n        # Case 3 (negative deviation): r3 = d - 1.5*C\n        np.array([13.95, 10.75, 11.55, 7.85]).reshape(N, 1)\n    ]\n\n    # --- Step 1: Compute posterior variance ---\n    # This value is constant for all observations in a linear-Gaussian model.\n    # Pre-compute the inverse of the covariance matrix and the prior precision.\n    Sigma_inv = np.linalg.inv(Sigma)\n    prior_precision = 1.0 / sigma0_sq\n    \n    # Compute the term C^T * Sigma^-1 * C, which contributes to the likelihood's precision.\n    # .item() is used to extract the scalar value from the 1x1 matrix.\n    likelihood_precision_term = (C.T @ Sigma_inv @ C).item()\n    \n    # The posterior precision is the sum of the prior and likelihood precisions.\n    posterior_precision = prior_precision + likelihood_precision_term\n    \n    # The posterior variance is the reciprocal of the posterior precision.\n    sigma2_post = 1.0 / posterior_precision\n\n    # --- Step 2: Compute decoder bias (b) and weights (w) ---\n    # According to the derived formula: w = sigma^2_post * Sigma^-1 * C\n    w = sigma2_post * (Sigma_inv @ C)\n    \n    # According to the derived formula: b = sigma^2_post * (mu0/sigma0^2 - C^T * Sigma^-1 * d)\n    b = sigma2_post * (prior_precision * mu0 - (C.T @ Sigma_inv @ d).item())\n    \n    # --- Step 3: Compute posterior mean for each test case ---\n    # The posterior mean can be calculated using the decoder: mu_post = b + w^T * r\n    results = []\n    for r_t in test_cases:\n        mu_post = (b + w.T @ r_t).item()\n        results.extend([mu_post, sigma2_post])\n    \n    # --- Step 4: Assemble the final output list ---\n    # The final list includes results for the 3 cases, followed by the decoder parameters.\n    # w.flatten().tolist() converts the weight column vector to a list of scalars.\n    results.extend([b] + w.flatten().tolist())\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.12f}' for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}