{
    "hands_on_practices": [
        {
            "introduction": "The dynamics of a neural network near a fixed point are governed by its linearization matrix, $A$. The system's fundamental modes of activity correspond to the eigenvectors of $A$. However, not all matrices possess a full basis of eigenvectors. This exercise  tackles this more general case by exploring defective matrices, which are crucial for describing systems operating near critical transitions. You will practice constructing a complete basis using generalized eigenvectors to find the Jordan Normal Form, providing a full description of the system's dynamics.",
            "id": "3994764",
            "problem": "Consider a linearized three-population recurrent rate model near a fixed point in brain modeling and computational neuroscience. The linearized dynamics are given by $\\frac{d}{dt}\\mathbf{x}(t)=A\\,\\mathbf{x}(t)$, where $\\mathbf{x}(t)\\in\\mathbb{R}^{3}$ collects small deviations of the excitatory and inhibitory population activities from equilibrium. The Jacobian (connectivity-linearization) matrix is\n$$\nA=\\begin{pmatrix}\n-1  1  0 \\\\\n0  -1  1 \\\\\n0  0  2\n\\end{pmatrix}.\n$$\nThis matrix is defective due to the repeated eigenvalue associated with decay in the first two populations. Using only the foundational definitions of eigenvalues, eigenvectors, generalized eigenvectors, and similarity, construct a similarity transform that brings $A$ to its Jordan normal form. Then verify the Jordan chain relations for the repeated eigenvalue directly from the definitions. To make the similarity transform unique, impose the following normalization conventions for the basis construction:\n- For the repeated eigenvalue, choose an eigenvector $\\mathbf{v}_{1}$ whose first component is $1$ and whose remaining components are $0$.\n- Choose a rank-$2$ generalized eigenvector $\\mathbf{v}_{2}$ for the same eigenvalue whose second component is $1$, and all other components are as small as required by the chain relations.\n- For the distinct eigenvalue, choose an eigenvector $\\mathbf{w}$ with integer components in the smallest positive integer ratio and positive third component.\n\nForm the similarity matrix $P$ by placing $\\mathbf{v}_{1}$, $\\mathbf{v}_{2}$, and $\\mathbf{w}$ as its columns in that order, and verify that $P^{-1}AP$ is in Jordan normal form and that the Jordan chain relations hold. Provide the determinant of $P$ as your final answer. Express your final answer exactly (no rounding).",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- The linearized dynamics of a three-population neuronal model are given by the differential equation $\\frac{d}{dt}\\mathbf{x}(t)=A\\,\\mathbf{x}(t)$, where $\\mathbf{x}(t) \\in \\mathbb{R}^{3}$.\n- The Jacobian matrix is $A=\\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\\\ 0  0  2 \\end{pmatrix}$.\n- The matrix $A$ is stated to be defective.\n- A similarity transform using a matrix $P$ is sought to find the Jordan normal form $J$ of $A$, such that $J = P^{-1}AP$.\n- The columns of $P$ are an ordered basis $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{w}\\}$.\n- The basis vectors must satisfy the following normalization conventions:\n    1. For the repeated eigenvalue, the eigenvector $\\mathbf{v}_{1}$ has a first component of $1$ and remaining components of $0$.\n    2. For the repeated eigenvalue, the rank-$2$ generalized eigenvector $\\mathbf{v}_{2}$ has a second component of $1$, with other components as small as required.\n    3. For the distinct eigenvalue, the eigenvector $\\mathbf{w}$ has integer components in the smallest positive integer ratio and a positive third component.\n- The task is to form $P = \\begin{pmatrix} \\mathbf{v}_1  \\mathbf{v}_2  \\mathbf{w} \\end{pmatrix}$, verify the Jordan chain relations and that $P^{-1}AP$ is in Jordan form, and provide the determinant of $P$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem uses a linearized rate model, a standard tool in computational neuroscience for analyzing the stability of network activity. The matrix $A$ represents effective synaptic connections and population decay rates. The structure is a simplified but plausible feed-forward inhibitory chain with an unstable third population. The problem is a valid application of linear algebra within the specified scientific context.\n- **Well-Posed**: The problem is well-posed. It asks for the construction of a specific similarity transformation matrix $P$ based on a given matrix $A$ and a set of explicit normalization constraints for the basis vectors. These constraints appear sufficient to determine a unique matrix $P$. The objective, calculating $\\det(P)$, is unambiguous.\n- **Objective**: The problem is stated in precise, objective mathematical language.\n- **Verification of Premises**: The matrix $A$ is upper triangular, so its eigenvalues are the diagonal entries: $\\lambda_1 = -1$ (algebraic multiplicity $2$) and $\\lambda_2 = 2$ (algebraic multiplicity $1$). To verify that $A$ is defective, we find the geometric multiplicity of $\\lambda_1 = -1$. The eigenspace is the null space of $A - (-1)I$:\n$$ A - (-1)I = A+I = \\begin{pmatrix} 0  1  0 \\\\ 0  0  1 \\\\ 0  0  3 \\end{pmatrix} $$\nThe null space of this matrix is given by vectors $\\mathbf{v} = (v_x, v_y, v_z)^T$ such that $v_y=0$, $v_z=0$, and $3v_z=0$. This implies the eigenspace is spanned by the vector $(1, 0, 0)^T$. The geometric multiplicity is $1$, which is less than the algebraic multiplicity of $2$. Thus, the matrix $A$ is indeed defective as stated. The problem statement is internally consistent and factually sound.\n\n### Step 3: Verdict and Action\nThe problem is valid. We may proceed with the solution.\n\n### Solution\nThe first step is to find the eigenvalues and a corresponding basis of eigenvectors and generalized eigenvectors for the matrix $A$.\n\nThe characteristic polynomial is $\\det(A-\\lambda I) = (-1-\\lambda)(-1-\\lambda)(2-\\lambda) = ( \\lambda+1 )^2(2-\\lambda)$. The eigenvalues are $\\lambda_1 = -1$ with algebraic multiplicity $m_1=2$, and $\\lambda_2 = 2$ with algebraic multiplicity $m_2=1$.\n\n**Basis for the repeated eigenvalue $\\lambda_1 = -1$**\n\nWe need to find a Jordan chain of length $2$, consisting of an eigenvector $\\mathbf{v}_1$ and a generalized eigenvector $\\mathbf{v}_2$ satisfying:\n$$(A - \\lambda_1 I)\\mathbf{v}_1 = \\mathbf{0}$$\n$$(A - \\lambda_1 I)\\mathbf{v}_2 = \\mathbf{v}_1$$\n\nThe eigenvector $\\mathbf{v}_1$ must satisfy $(A+I)\\mathbf{v}_1 = \\mathbf{0}$. From our validation analysis, the eigenspace is spanned by $(1, 0, 0)^T$. The normalization constraint for $\\mathbf{v}_1$ is that its first component is $1$ and others are $0$. This directly yields:\n$$ \\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n\nNext, we find the rank-$2$ generalized eigenvector $\\mathbf{v}_2$ by solving $(A+I)\\mathbf{v}_2 = \\mathbf{v}_1$. Let $\\mathbf{v}_2 = (x, y, z)^T$.\n$$ \\begin{pmatrix} 0  1  0 \\\\ 0  0  1 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThis yields the system of equations:\n1. $y = 1$\n2. $z = 0$\n3. $3z = 0$\nThe system is consistent. The component $x$ is not constrained by the equations. The normalization for $\\mathbf{v}_2$ requires its second component to be $y=1$, which is satisfied. It also requires other components to be \"as small as required\". Since $z$ must be $0$ and $x$ is a free parameter, the simplest choice that minimizes complexity (and a common convention) is to set $x=0$. Therefore, we choose:\n$$ \\mathbf{v}_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\n\n**Basis for the distinct eigenvalue $\\lambda_2 = 2$**\n\nWe find the eigenvector $\\mathbf{w}=(x, y, z)^T$ corresponding to $\\lambda_2 = 2$ by solving $(A-2I)\\mathbf{w} = \\mathbf{0}$.\n$$ \\begin{pmatrix} -3  1  0 \\\\ 0  -3  1 \\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThis yields the system:\n1. $-3x + y = 0 \\implies y = 3x$\n2. $-3y + z = 0 \\implies z = 3y$\nSubstituting the first equation into the second gives $z = 3(3x) = 9x$. The eigenvector is of the form $\\mathbf{w} = (x, 3x, 9x)^T = x(1, 3, 9)^T$.\nThe normalization requires integer components in the smallest positive integer ratio and a positive third component. Choosing $x=1$ gives the vector $\\mathbf{w} = (1, 3, 9)^T$. The components are integers, their greatest common divisor is $1$, and the third component $9$ is positive. This satisfies the constraints.\n$$ \\mathbf{w} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 9 \\end{pmatrix} $$\n\n**Construct the similarity matrix $P$**\n\nThe similarity matrix $P$ is formed by using $\\mathbf{v}_1$, $\\mathbf{v}_2$, and $\\mathbf{w}$ as its columns in the specified order.\n$$ P = \\begin{pmatrix} \\mathbf{v}_1  \\mathbf{v}_2  \\mathbf{w} \\end{pmatrix} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  3 \\\\ 0  0  9 \\end{pmatrix} $$\n\n**Verification of Jordan Chain Relations and the Similarity Transformation**\n\nFirst, we verify the Jordan chain relations for $\\lambda_1 = -1$ directly:\n- Check $(A+I)\\mathbf{v}_1 = \\mathbf{0}$:\n$$ (A+I)\\mathbf{v}_1 = \\begin{pmatrix} 0  1  0 \\\\ 0  0  1 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\mathbf{0} $$\nThis is correct.\n- Check $(A+I)\\mathbf{v}_2 = \\mathbf{v}_1$:\n$$ (A+I)\\mathbf{v}_2 = \\begin{pmatrix} 0  1  0 \\\\ 0  0  1 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\mathbf{v}_1 $$\nThis is also correct. The chain relations hold.\n\nNext, we verify that $P^{-1}AP$ is the Jordan normal form $J$.\nThe inverse of $P$ can be calculated. For an upper triangular matrix, the inverse is also upper triangular.\n$$ P^{-1} = \\frac{1}{\\det(P)} \\text{adj}(P)^T = \\frac{1}{9} \\begin{pmatrix} 9  0  -1 \\\\ 0  9  -3 \\\\ 0  0  1 \\end{pmatrix}^T = \\frac{1}{9} \\begin{pmatrix} 9  0  0 \\\\ 0  9  0 \\\\ -1  -3  1 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ -1/9  -1/3  1/9 \\end{pmatrix} $$\nNo, this is wrong. Let's compute it with row operations or cofactors correctly.\nCofactor matrix:\n$ C = \\begin{pmatrix} 9  0  0 \\\\ 0  9  0 \\\\ -1  -3  1 \\end{pmatrix} $. Adjugate matrix is $C^T = \\begin{pmatrix} 9  0  -1 \\\\ 0  9  -3 \\\\ 0  0  1 \\end{pmatrix}$.\nThe determinant is $\\det(P) = 1(9)-0+1(0) = 9$.\n$$ P^{-1} = \\frac{1}{9} \\begin{pmatrix} 9  0  -1 \\\\ 0  9  -3 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 1  0  -1/9 \\\\ 0  1  -1/3 \\\\ 0  0  1/9 \\end{pmatrix} $$\nNow, we compute $AP$:\n$$ AP = \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\\\ 0  0  2 \\end{pmatrix} \\begin{pmatrix} 1  0  1 \\\\ 0  1  3 \\\\ 0  0  9 \\end{pmatrix} = \\begin{pmatrix} -1  1  2 \\\\ 0  -1  6 \\\\ 0  0  18 \\end{pmatrix} $$\nFinally, we compute $J = P^{-1}(AP)$:\n$$ J = \\begin{pmatrix} 1  0  -1/9 \\\\ 0  1  -1/3 \\\\ 0  0  1/9 \\end{pmatrix} \\begin{pmatrix} -1  1  2 \\\\ 0  -1  6 \\\\ 0  0  18 \\end{pmatrix} = \\begin{pmatrix} -1  1  2 - 2 \\\\ 0  -1  6 - 6 \\\\ 0  0  2 \\end{pmatrix} = \\begin{pmatrix} -1  1  0 \\\\ 0  -1  0 \\\\ 0  0  2 \\end{pmatrix} $$\nThis is the correct Jordan normal form, consisting of a $2 \\times 2$ Jordan block for $\\lambda_1 = -1$ and a $1 \\times 1$ block for $\\lambda_2 = 2$. The verification is successful.\n\n**Calculate the determinant of $P$**\n\nThe final step is to calculate the determinant of the matrix $P$.\n$$ P = \\begin{pmatrix} 1  0  1 \\\\ 0  1  3 \\\\ 0  0  9 \\end{pmatrix} $$\nSince $P$ is an upper triangular matrix, its determinant is the product of its diagonal elements.\n$$ \\det(P) = 1 \\cdot 1 \\cdot 9 = 9 $$",
            "answer": "$$\n\\boxed{9}\n$$"
        },
        {
            "introduction": "A prominent theory in neuroscience posits that cortical circuits operate in a \"balanced\" state, where strong excitatory and inhibitory currents nearly cancel. This regime can produce complex computations, but it also makes the network's steady-state response extremely sensitive to inputs and parameters. This practice  explores the mathematical footprint of this sensitivity: the condition number $\\kappa(A)$ of the network's effective operator. You will see how a biologically plausible configuration leads to an ill-conditioned matrix, a state where small errors in the model are dramatically amplified in its predicted output.",
            "id": "3994759",
            "problem": "Consider a linear rate model of a two-population cortical microcircuit near a balanced regime, where the steady-state firing rates $\\mathbf{r}$ satisfy $(\\mathbf{I} - \\mathbf{W}) \\mathbf{r} = \\mathbf{u}$ for external input $\\mathbf{u}$. The effective operator $\\mathbf{A} = \\mathbf{I} - \\mathbf{W}$ determines the susceptibility $\\mathbf{r} = \\mathbf{A}^{-1} \\mathbf{u}$. In this regime, small errors in measuring $\\mathbf{W}$ translate into errors in $\\mathbf{A}$ and can produce large errors in $\\mathbf{A}^{-1}$ if $\\mathbf{A}$ is ill-conditioned.\n\nConstruct the following explicit example in which small errors in $\\mathbf{A}$ lead to large errors in $\\mathbf{A}^{-1}$, and quantify the amplification using the condition number $\\kappa(\\mathbf{A})$ under the matrix two-norm (induced by the Euclidean norm):\n\n- Let the inter-population coupling be strong and nearly balanced, with a symmetric weight matrix $\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ given by\n$$\n\\mathbf{W} = \\begin{pmatrix}\n0  1 - \\varepsilon \\\\\n1 - \\varepsilon  0\n\\end{pmatrix},\n$$\nwhere $0  \\varepsilon \\ll 1$. The effective operator is then\n$$\n\\mathbf{A} = \\mathbf{I} - \\mathbf{W} = \\begin{pmatrix}\n1  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1\n\\end{pmatrix}.\n$$\n\n- Suppose experimental measurement errors perturb $\\mathbf{A}$ to $\\mathbf{A} + \\Delta \\mathbf{A}$ with a small symmetric perturbation of the form\n$$\n\\Delta \\mathbf{A} = \\begin{pmatrix}\n0  \\delta \\\\\n\\delta  0\n\\end{pmatrix},\n$$\nwhere $|\\delta| \\ll 1$.\n\nUsing only core definitions and first principles—namely, the definition of a matrix inverse, the product rule applied to $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}$, and the definition of the induced two-norm—perform the following:\n\n1. Derive the first-order relationship between $\\Delta \\mathbf{A}$ and $\\Delta \\mathbf{A}^{-1}$ that explains how small errors in $\\mathbf{A}$ are amplified in $\\mathbf{A}^{-1}$ for small $|\\delta|$.\n\n2. Compute the condition number $\\kappa_{2}(\\mathbf{A})$ of $\\mathbf{A}$ in the two-norm and express it in closed form as a function of $\\varepsilon$.\n\n3. Evaluate $\\kappa_{2}(\\mathbf{A})$ numerically for $\\varepsilon = 0.01$.\n\nReport only the numerical value of $\\kappa_{2}(\\mathbf{A})$ for $\\varepsilon = 0.01$ as your final answer. Round your answer to four significant figures. The quantity $\\kappa_{2}(\\mathbf{A})$ is dimensionless.",
            "solution": "The problem asks for three tasks related to the sensitivity of a matrix inverse to perturbations, using a specific $2 \\times 2$ matrix $\\mathbf{A}$ derived from a simplified model of a neural circuit. The tasks are: $1$) to derive the first-order approximation for the change in the inverse, $\\Delta \\mathbf{A}^{-1}$; $2$) to compute the condition number $\\kappa_2(\\mathbf{A})$ as a function of a parameter $\\varepsilon$; and $3$) to evaluate this condition number for a specific value of $\\varepsilon$.\n\nFirst, we address the derivation of the first-order relationship between a perturbation $\\Delta \\mathbf{A}$ and the resulting perturbation in the inverse, $\\Delta\\mathbf{A}^{-1}$. We start from the definition of the inverse for the perturbed matrix, $(\\mathbf{A} + \\Delta\\mathbf{A})^{-1}$. Let the inverse be denoted as $\\mathbf{A}^{-1} + \\Delta\\mathbf{A}^{-1}$, where $\\Delta\\mathbf{A}^{-1}$ represents the change in the inverse. The product of a matrix and its inverse is the identity matrix $\\mathbf{I}$:\n$$\n(\\mathbf{A} + \\Delta\\mathbf{A}) (\\mathbf{A}^{-1} + \\Delta\\mathbf{A}^{-1}) = \\mathbf{I}\n$$\nExpanding the left-hand side gives:\n$$\n\\mathbf{A} \\mathbf{A}^{-1} + \\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{I}\n$$\nSince $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}$, the equation simplifies to:\n$$\n\\mathbf{I} + \\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{I}\n$$\nSubtracting $\\mathbf{I}$ from both sides yields:\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{0}\n$$\nThe problem considers small perturbations $\\Delta\\mathbf{A}$ (and consequently small $\\Delta\\mathbf{A}^{-1}$). Therefore, the term $\\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1}$ is of second order in smallness and can be neglected in a first-order approximation. This leads to the linear approximation:\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} \\approx \\mathbf{0}\n$$\nTo solve for $\\Delta\\mathbf{A}^{-1}$, we rearrange the terms:\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} \\approx -\\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\nAssuming $\\mathbf{A}$ is invertible, we can left-multiply by $\\mathbf{A}^{-1}$:\n$$\n\\mathbf{A}^{-1} (\\mathbf{A} \\Delta\\mathbf{A}^{-1}) \\approx \\mathbf{A}^{-1} (-\\Delta\\mathbf{A} \\mathbf{A}^{-1})\n$$\n$$\n(\\mathbf{A}^{-1} \\mathbf{A}) \\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\n$$\n\\mathbf{I} \\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\nThis gives the final first-order relationship, which explains how the error in $\\mathbf{A}$ is propagated to its inverse:\n$$\n\\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\nThis relation shows that the error in the inverse is amplified by factors of $\\mathbf{A}^{-1}$. If the norm of $\\mathbf{A}^{-1}$ is large, even small errors $\\Delta\\mathbf{A}$ can lead to large errors $\\Delta\\mathbf{A}^{-1}$.\n\nNext, we compute the condition number $\\kappa_2(\\mathbf{A})$ for the given matrix $\\mathbf{A}$. The condition number with respect to the matrix two-norm is defined as $\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2$. The two-norm of a matrix is its largest singular value, $\\sigma_{\\max}$. For a symmetric matrix, the singular values are the absolute values of its eigenvalues. The matrix $\\mathbf{A}$ is given as:\n$$\n\\mathbf{A} = \\begin{pmatrix}\n1  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1\n\\end{pmatrix}\n$$\nThis is a real symmetric matrix. We find its eigenvalues, $\\lambda$, by solving the characteristic equation $\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0$.\n$$\n\\det\\begin{pmatrix}\n1 - \\lambda  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1 - \\lambda\n\\end{pmatrix} = 0\n$$\n$$\n(1 - \\lambda)^2 - (-1 + \\varepsilon)^2 = 0\n$$\n$$\n(1 - \\lambda)^2 = (1 - \\varepsilon)^2\n$$\nTaking the square root of both sides gives two possibilities:\n$$\n1 - \\lambda = \\pm(1 - \\varepsilon)\n$$\nCase $1$: $1 - \\lambda = 1 - \\varepsilon \\implies \\lambda_1 = \\varepsilon$.\nCase $2$: $1 - \\lambda = -(1 - \\varepsilon) \\implies 1 - \\lambda = -1 + \\varepsilon \\implies \\lambda_2 = 2 - \\varepsilon$.\nThe eigenvalues of $\\mathbf{A}$ are $\\lambda_1 = \\varepsilon$ and $\\lambda_2 = 2 - \\varepsilon$. Given that $0  \\varepsilon \\ll 1$, both eigenvalues are positive. The singular values of $\\mathbf{A}$ are $\\sigma_1 = |\\lambda_1| = \\varepsilon$ and $\\sigma_2 = |\\lambda_2| = 2 - \\varepsilon$.\nThe two-norm of $\\mathbf{A}$ is the largest singular value:\n$$\n\\|\\mathbf{A}\\|_2 = \\sigma_{\\max}(\\mathbf{A}) = \\max(\\varepsilon, 2 - \\varepsilon)\n$$\nSince $0  \\varepsilon  1$, we have $2 - \\varepsilon  1$ and $\\varepsilon  1$, so $2 - \\varepsilon  \\varepsilon$. Thus, $\\|\\mathbf{A}\\|_2 = 2 - \\varepsilon$.\n\nNow we find $\\|\\mathbf{A}^{-1}\\|_2$. The matrix $\\mathbf{A}^{-1}$ is also symmetric, so its two-norm is the maximum of the absolute values of its eigenvalues. The eigenvalues of $\\mathbf{A}^{-1}$ are the reciprocals of the eigenvalues of $\\mathbf{A}$, which are $1/\\lambda_1 = 1/\\varepsilon$ and $1/\\lambda_2 = 1/(2 - \\varepsilon)$.\nThe two-norm of $\\mathbf{A}^{-1}$ is:\n$$\n\\|\\mathbf{A}^{-1}\\|_2 = \\sigma_{\\max}(\\mathbf{A}^{-1}) = \\max\\left(\\left|\\frac{1}{\\varepsilon}\\right|, \\left|\\frac{1}{2 - \\varepsilon}\\right|\\right)\n$$\nSince $0  \\varepsilon \\ll 1$, we have $0  \\varepsilon  2 - \\varepsilon$, which implies $1/\\varepsilon  1/(2 - \\varepsilon)$. Therefore, $\\|\\mathbf{A}^{-1}\\|_2 = 1/\\varepsilon$.\nThe condition number $\\kappa_2(\\mathbf{A})$ is the product of the norms:\n$$\n\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2 = (2 - \\varepsilon) \\left(\\frac{1}{\\varepsilon}\\right) = \\frac{2 - \\varepsilon}{\\varepsilon}\n$$\nThis expression shows that as $\\varepsilon \\to 0$, the condition number $\\kappa_2(\\mathbf{A}) \\to \\infty$. This confirms that for small $\\varepsilon$, the matrix $\\mathbf{A}$ is ill-conditioned.\n\nFinally, we evaluate $\\kappa_2(\\mathbf{A})$ for $\\varepsilon = 0.01$. Substituting this value into the derived formula:\n$$\n\\kappa_2(\\mathbf{A}) = \\frac{2 - 0.01}{0.01} = \\frac{1.99}{0.01} = 199\n$$\nThe problem requires the answer to be rounded to four significant figures. The exact value is $199$. Expressed with four significant figures, this is $199.0$.",
            "answer": "$$\n\\boxed{199.0}\n$$"
        },
        {
            "introduction": "A key challenge in modern neuroscience is to find simple, interpretable structure within complex, high-dimensional neural recordings. Linear algebra provides the essential tools for this task through dimensionality reduction. This practice  provides a hands-on guide to one of the most fundamental techniques: projecting neural population activity onto a lower-dimensional, task-relevant subspace. By calculating the explained variance, you will learn how to quantify how much of the neural activity's structure is captured by a specific set of explanatory variables, a cornerstone of neural data analysis.",
            "id": "3994746",
            "problem": "Consider a neural population activity matrix $X \\in \\mathbb{R}^{N \\times T}$ where $N$ is the number of neurons and $T$ is the number of time points or trials. Let $B \\in \\mathbb{R}^{N \\times K}$ be a set of basis vectors (as columns) that define a task subspace. Using only core definitions from linear algebra, perform the following tasks in a program:\n\n1. Mean-center the activity matrix $X$ across time, producing $X_0 \\in \\mathbb{R}^{N \\times T}$, by subtracting from each row of $X$ its sample mean over the $T$ time points.\n2. Interpret the task subspace as the span of the columns of $B$ in $\\mathbb{R}^N$. Using only the definition of orthogonal projection as the unique element in the subspace minimizing Euclidean distance from a given vector, construct the orthogonal projection of each column of $X_0$ onto that subspace, producing a projected matrix $\\hat{X} \\in \\mathbb{R}^{N \\times T}$.\n3. Define the explained variance fraction as the ratio between the total variance captured by the projection and the total variance in $X_0$. Use the definition of total variance as the sum of squared deviations across all entries. If the total variance in $X_0$ is zero, define the explained variance fraction to be $0$.\n\nYour program must apply the above to each test case below and produce the explained variance fraction for each case, rounded to six decimal places.\n\nTest suite (each $X_i$ and $B_i$ are as specified):\n\n- Case $1$ (general, non-orthonormal basis, nontrivial capture):\n  $$\n  X_1 = \\begin{bmatrix}\n  1  2  0  -1 \\\\\n  0  1  -1  -2 \\\\\n  2  0  1  -1\n  \\end{bmatrix}, \\quad\n  B_1 = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  1  0\n  \\end{bmatrix}.\n  $$\n\n- Case $2$ (data lie entirely in the subspace):\n  $$\n  X_2 = \\begin{bmatrix}\n  2  3  4 \\\\\n  0  0  0\n  \\end{bmatrix}, \\quad\n  B_2 = \\begin{bmatrix}\n  1 \\\\\n  0\n  \\end{bmatrix}.\n  $$\n\n- Case $3$ (data orthogonal to the subspace):\n  $$\n  X_3 = \\begin{bmatrix}\n  0  0  0 \\\\\n  5  6  7\n  \\end{bmatrix}, \\quad\n  B_3 = \\begin{bmatrix}\n  1 \\\\\n  0\n  \\end{bmatrix}.\n  $$\n\n- Case $4$ (non-orthonormal, rank-deficient basis):\n  $$\n  X_4 = \\begin{bmatrix}\n  1  2  -1  0  3 \\\\\n  1  2  -1  0  3 \\\\\n  2  0  1  3  -1\n  \\end{bmatrix}, \\quad\n  B_4 = \\begin{bmatrix}\n  1  2 \\\\\n  1  2 \\\\\n  0  0\n  \\end{bmatrix}.\n  $$\n\n- Case $5$ (zero variance edge case):\n  $$\n  X_5 = \\begin{bmatrix}\n  1  1  1 \\\\\n  2  2  2 \\\\\n  3  3  3\n  \\end{bmatrix}, \\quad\n  B_5 = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  0  0\n  \\end{bmatrix}.\n  $$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the explained variance fraction for case $i$, rounded to six decimal places. No additional text should be printed.",
            "solution": "The problem requires the computation of the fraction of variance in a neural population activity matrix, $X \\in \\mathbb{R}^{N \\times T}$, that is explained by its orthogonal projection onto a task subspace defined by the column space of a matrix $B \\in \\mathbb{R}^{N \\times K}$. The process involves three main steps as defined: mean-centering the data, projecting the mean-centered data onto the subspace, and computing the ratio of variances.\n\nLet $X$ be the activity matrix, where $N$ is the number of neurons and $T$ is the number of time points. Let $B$ be the matrix whose columns span the task subspace.\n\n**Step 1: Mean-Centering the Activity Matrix**\n\nThe first step is to transform the raw activity matrix $X$ into a mean-centered matrix $X_0$. This is achieved by subtracting the temporal mean of each neuron's activity from its time series. For each row $i$ of $X$ (representing the $i$-th neuron), its mean activity $\\mu_i$ across the $T$ time points is calculated:\n$$\n\\mu_i = \\frac{1}{T} \\sum_{j=1}^{T} X_{ij}\n$$\nThe entries of the mean-centered matrix $X_0$ are then given by:\n$$\n(X_0)_{ij} = X_{ij} - \\mu_i\n$$\nIn matrix notation, if we define a column vector of row means $\\boldsymbol{\\mu} \\in \\mathbb{R}^{N}$ where $(\\boldsymbol{\\mu})_i = \\mu_i$, and a row vector of ones $\\mathbf{1}_T^T \\in \\mathbb{R}^{1 \\times T}$, the mean-centered matrix is:\n$$\nX_0 = X - \\boldsymbol{\\mu} \\mathbf{1}_T^T\n$$\nThis operation ensures that each row of $X_0$ has a mean of $0$.\n\n**Step 2: Orthogonal Projection onto the Task Subspace**\n\nThe task subspace, $\\mathcal{S}$, is the column space of the matrix $B$, denoted $\\mathcal{S} = \\text{span}(B)$. We need to find the orthogonal projection of each column vector of $X_0$ onto $\\mathcal{S}$. For an arbitrary vector $\\mathbf{x} \\in \\mathbb{R}^N$ (representing a column of $X_0$), its orthogonal projection $\\hat{\\mathbf{x}}$ is the unique vector in $\\mathcal{S}$ that minimizes the Euclidean distance $\\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|_2$.\n\nSince $\\hat{\\mathbf{x}} \\in \\mathcal{S}$, it can be expressed as a linear combination of the columns of $B$, i.e., $\\hat{\\mathbf{x}} = B\\mathbf{c}$ for some coefficient vector $\\mathbf{c} \\in \\mathbb{R}^K$. The problem is to find the $\\mathbf{c}$ that minimizes $\\|\\mathbf{x} - B\\mathbf{c}\\|_2^2$. This is a classical linear least-squares problem. The solution is found by solving the normal equations:\n$$\n(B^T B) \\mathbf{c} = B^T \\mathbf{x}\n$$\nIf the columns of $B$ are linearly independent, $B^T B$ is invertible, and $\\mathbf{c} = (B^T B)^{-1} B^T \\mathbf{x}$. The projection is then $\\hat{\\mathbf{x}} = B(B^T B)^{-1} B^T \\mathbf{x}$. The matrix $P = B(B^T B)^{-1} B^T$ is the projection matrix.\n\nHowever, the columns of $B$ might be linearly dependent (i.e., $B$ is not full column rank), making $B^T B$ singular. The problem can still be solved using the Moore-Penrose pseudoinverse, denoted by a superscript '$+$'. The projection matrix $P$ onto the column space of $B$ is given by the general formula:\n$$\nP = B B^+\n$$\nThis formula is universally applicable, regardless of the rank of $B$. The projection of the entire mean-centered data matrix $X_0$ onto the subspace $\\mathcal{S}$ is obtained by applying the projection matrix $P$ to each of its columns:\n$$\n\\hat{X} = P X_0 = (B B^+) X_0\n$$\nAn important property is that if $X_0$ is mean-centered (rows sum to $0$), its projection $\\hat{X}$ is also mean-centered. This is because the sum of columns of $X_0$ is the zero vector, and $P(\\mathbf{0}) = \\mathbf{0}$.\n\n**Step 3: Explained Variance Fraction**\n\nThe problem defines total variance for a mean-centered matrix as the sum of squared deviations across all its entries. For a mean-centered matrix like $X_0$, the deviation of any entry from its row mean is the entry itself. Therefore, the total variance is the squared Frobenius norm of the matrix:\n$$\n\\text{Var}(X_0) = \\sum_{i=1}^{N} \\sum_{j=1}^{T} (X_0)_{ij}^2 = \\|X_0\\|_F^2\n$$\nSimilarly, since $\\hat{X}$ is also mean-centered, the variance captured by the projection is:\n$$\n\\text{Var}(\\hat{X}) = \\sum_{i=1}^{N} \\sum_{j=1}^{T} (\\hat{X})_{ij}^2 = \\|\\hat{X}\\|_F^2\n$$\nThe explained variance fraction (EVF) is the ratio of the variance of the projected data to the total variance of the original mean-centered data:\n$$\n\\text{EVF} = \\frac{\\text{Var}(\\hat{X})}{\\text{Var}(X_0)} = \\frac{\\|\\hat{X}\\|_F^2}{\\|X_0\\|_F^2}\n$$\nAs per the problem statement, if the total variance $\\text{Var}(X_0)$ is $0$, the EVF is defined to be $0$. This occurs when all rows of the original matrix $X$ are constant over time.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_explained_variance(X: np.ndarray, B: np.ndarray) - float:\n    \"\"\"\n    Calculates the fraction of variance in X explained by projection onto the\n    subspace spanned by the columns of B.\n\n    Args:\n        X (np.ndarray): The neural activity matrix of shape (N, T).\n        B (np.ndarray): The basis matrix for the task subspace of shape (N, K).\n\n    Returns:\n        float: The explained variance fraction.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    X = np.asarray(X, dtype=float)\n    B = np.asarray(B, dtype=float)\n\n    # Step 1: Mean-center the activity matrix X across time.\n    # The shape of row_means will be (N, 1), which broadcasts correctly.\n    row_means = X.mean(axis=1, keepdims=True)\n    X0 = X - row_means\n\n    # Step 2: Calculate the total variance in the mean-centered data.\n    # Total variance is the squared Frobenius norm of the mean-centered matrix.\n    total_variance = np.sum(X0**2)\n\n    # Handle the edge case where total variance is zero.\n    if np.isclose(total_variance, 0):\n        return 0.0\n\n    # Step 3: Construct the orthogonal projection of X0 onto the subspace.\n    # Compute the projection matrix P = B * B_pseudoinverse.\n    B_pinv = np.linalg.pinv(B)\n    P = B @ B_pinv\n\n    # Project the mean-centered data matrix X0.\n    X_hat = P @ X0\n\n    # Step 4: Calculate the variance captured by the projection.\n    # This is the squared Frobenius norm of the projected matrix.\n    projected_variance = np.sum(X_hat**2)\n\n    # Step 5: Compute the explained variance fraction.\n    explained_variance_fraction = projected_variance / total_variance\n\n    return explained_variance_fraction\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general, non-orthonormal basis, nontrivial capture)\n        {\n            'X': [[1, 2, 0, -1], [0, 1, -1, -2], [2, 0, 1, -1]],\n            'B': [[1, 0], [0, 1], [1, 0]],\n        },\n        # Case 2 (data lie entirely in the subspace)\n        {\n            'X': [[2, 3, 4], [0, 0, 0]],\n            'B': [[1], [0]],\n        },\n        # Case 3 (data orthogonal to the subspace)\n        {\n            'X': [[0, 0, 0], [5, 6, 7]],\n            'B': [[1], [0]],\n        },\n        # Case 4 (non-orthonormal, rank-deficient basis)\n        {\n            'X': [[1, 2, -1, 0, 3], [1, 2, -1, 0, 3], [2, 0, 1, 3, -1]],\n            'B': [[1, 2], [1, 2], [0, 0]],\n        },\n        # Case 5 (zero variance edge case)\n        {\n            'X': [[1, 1, 1], [2, 2, 2], [3, 3, 3]],\n            'B': [[1, 0], [0, 1], [0, 0]],\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case['X']\n        B = case['B']\n        evf = calculate_explained_variance(X, B)\n        # Round the result to six decimal places\n        results.append(f\"{evf:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}