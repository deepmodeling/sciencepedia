{
    "hands_on_practices": [
        {
            "introduction": "在分析神经网络的动态行为时，系统的雅可比矩阵的特征值至关重要，因为它们决定了系统在平衡点附近的稳定性。然而，对于大型网络，直接计算特征值可能非常复杂。盖尔圆盘定理 (Gershgorin Disk Theorem) 提供了一种强大的方法，无需精确计算即可估计特征值在复平面上的位置，这在神经科学模型的分析中尤其有用。这项练习将通过一个简化的皮层微电路模型，向您展示如何应用该定理，并将特定的动态模式与网络中的单个神经元群体关联起来 。",
            "id": "3994745",
            "problem": "考虑一个前馈皮层微环路的线性化四群体速率模型，其中不动点附近的动力学由一个雅可比矩阵 $J \\in \\mathbb{R}^{4 \\times 4}$ 近似。矩阵 $J$ 的设计使得群体间的耦合是弱的，并以前馈方式单向作用，得到\n$$\nJ \\;=\\;\n\\begin{pmatrix}\n-0.5  0.10  0.04  0.02 \\\\\n0  -2.0  0.08  0.03 \\\\\n0  0  -4.0  0.06 \\\\\n0  0  0  -7.0\n\\end{pmatrix}.\n$$\n对于每一行 $i$，定义复平面上的盖尔圆盘 (Gershgorin disk)，其圆心为 $a_{ii}$，半径等于该行非对角线元素的绝对值之和。使用特征值的核心定义和谱的盖尔圆盘特征，完成以下任务：\n1. 计算 $J$ 每一行对应的盖尔圆盘的圆心和半径，并验证这些圆盘两两不相交。\n2. 给出一个严格的论证，说明每个不相交的盖尔圆盘必须恰好包含 $J$ 的一个特征值，并简要讨论这一性质如何帮助微环路的模态分配（即将特征模态与单个群体关联起来）。\n3. 最后，计算 $J$ 的行列式，结果为一个实数值。最终数值答案无需四舍五入。",
            "solution": "首先验证问题的科学合理性、自洽性和清晰度。问题陈述提供了一个来自线性化动力系统的定义明确的雅可比矩阵 $J$，这是计算神经科学中的一种标准方法。任务涉及应用盖尔圆定理 (Gershgorin circle theorem) 和计算行列式，这两者都是线性代数中的标准程序。前提具有科学依据和数学合理性。问题提法得当、无歧义，并提供了所有必要信息。因此，该问题被认为是有效的，并给出完整解答。\n\n雅可比矩阵由下式给出：\n$$\nJ \\;=\\;\n\\begin{pmatrix}\n-0.5  0.10  0.04  0.02 \\\\\n0  -2.0  0.08  0.03 \\\\\n0  0  -4.0  0.06 \\\\\n0  0  0  -7.0\n\\end{pmatrix}\n$$\n\n对于矩阵 $A = (a_{ij})$ 的第 $i$ 行，其盖尔圆盘 $D_i$ 是复平面上的一个闭圆盘，圆心为 $c_i = a_{ii}$，半径为 $r_i = \\sum_{j \\neq i} |a_{ij}|$。\n\n**1. 盖尔圆盘及其不相交性**\n\n我们计算矩阵 $J$ 的每一行对应的四个盖尔圆盘的圆心 $c_i$ 和半径 $r_i$。\n\n对于第 1 行：\n圆心为对角元素 $c_1 = j_{11} = -0.5$。\n半径为非对角线元素的绝对值之和：$r_1 = |0.10| + |0.04| + |0.02| = 0.10 + 0.04 + 0.02 = 0.16$。\n因此，第一个圆盘是 $D_1 = \\{z \\in \\mathbb{C} : |z - (-0.5)| \\le 0.16\\}$。\n\n对于第 2 行：\n圆心为 $c_2 = j_{22} = -2.0$。\n半径为 $r_2 = |0| + |0.08| + |0.03| = 0.08 + 0.03 = 0.11$。\n因此，第二个圆盘是 $D_2 = \\{z \\in \\mathbb{C} : |z - (-2.0)| \\le 0.11\\}$。\n\n对于第 3 行：\n圆心为 $c_3 = j_{33} = -4.0$。\n半径为 $r_3 = |0| + |0| + |0.06| = 0.06$。\n因此，第三个圆盘是 $D_3 = \\{z \\in \\mathbb{C} : |z - (-4.0)| \\le 0.06\\}$。\n\n对于第 4 行：\n圆心为 $c_4 = j_{44} = -7.0$。\n半径为 $r_4 = |0| + |0| + |0| = 0$。\n因此，第四个圆盘是 $D_4 = \\{z \\in \\mathbb{C} : |z - (-7.0)| \\le 0\\}$，即点 $z = -7.0$。\n\n接下来，我们验证这四个圆盘是否两两不相交。如果两个圆盘 $D_i$ 和 $D_j$ 的圆心距 $|c_i - c_j|$ 大于它们的半径之和 $r_i + r_j$，则它们不相交。\n\n-   **$D_1$ 和 $D_2$**：$|c_1 - c_2| = |-0.5 - (-2.0)| = 1.5$。半径之和为 $r_1 + r_2 = 0.16 + 0.11 = 0.27$。因为 $1.5 > 0.27$，所以 $D_1$ 和 $D_2$ 不相交。\n-   **$D_1$ 和 $D_3$**：$|c_1 - c_3| = |-0.5 - (-4.0)| = 3.5$。半径之和为 $r_1 + r_3 = 0.16 + 0.06 = 0.22$。因为 $3.5 > 0.22$，所以 $D_1$ 和 $D_3$ 不相交。\n-   **$D_1$ 和 $D_4$**：$|c_1 - c_4| = |-0.5 - (-7.0)| = 6.5$。半径之和为 $r_1 + r_4 = 0.16 + 0 = 0.16$。因为 $6.5 > 0.16$，所以 $D_1$ 和 $D_4$ 不相交。\n-   **$D_2$ 和 $D_3$**：$|c_2 - c_3| = |-2.0 - (-4.0)| = 2.0$。半径之和为 $r_2 + r_3 = 0.11 + 0.06 = 0.17$。因为 $2.0 > 0.17$，所以 $D_2$ 和 $D_3$ 不相交。\n-   **$D_2$ 和 $D_4$**：$|c_2 - c_4| = |-2.0 - (-7.0)| = 5.0$。半径之和为 $r_2 + r_4 = 0.11 + 0 = 0.11$。因为 $5.0 > 0.11$，所以 $D_2$ 和 $D_4$ 不相交。\n-   **$D_3$ 和 $D_4$**：$|c_3 - c_4| = |-4.0 - (-7.0)| = 3.0$。半径之和为 $r_3 + r_4 = 0.06 + 0 = 0.06$。因为 $3.0 > 0.06$，所以 $D_3$ 和 $D_4$ 不相交。\n\n所有盖尔圆盘对均不相交。\n\n**2. 特征值定位与模态分配**\n\n盖尔圆定理的一个关键推广结果指出，如果 $k$ 个盖尔圆盘的集合与其余 $n-k$ 个圆盘不相交，那么前一个集合的并集恰好包含该矩阵的 $k$ 个特征值。在本例中，每个圆盘 $D_i$ 都与其他三个圆盘的并集不相交。对每个圆盘 $D_i$（$i=1, 2, 3, 4$）应用 $k=1$ 的定理，我们得出结论：每个圆盘必须恰好包含一个特征值。\n\n这一性质的严格论证依赖于连续性。设 $D$ 为 $J$ 的对角部分，$O = J - D$ 为非对角部分。构造一个矩阵族 $J(t) = D + tO$，其中参数 $t \\in [0, 1]$。$J(t)$ 的特征值是 $t$ 的连续函数。当 $t=0$ 时，我们有 $J(0) = D$，这是一个对角矩阵。其特征值就是其对角线元素，即 $\\lambda_i(0) = j_{ii}$。每个特征值 $\\lambda_i(0)$ 都位于其对应的盖尔圆盘 $D_i$ 的中心。随着 $t$ 从 $0$ 增加到 $1$，特征值 $\\lambda_i(t)$ 在复平面上描绘出连续的路径。第一个盖尔圆定理保证了 $J=J(1)$ 的所有特征值都必须位于四个圆盘的并集 $\\bigcup_{i=1}^4 D_i$ 内。由于这些圆盘是不相交的，从 $D_i$ 出发的连续路径 $\\lambda_i(t)$ 无法到达另一个圆盘 $D_k$（$k \\neq i$）而不离开所有圆盘的并集，因为这将与定理相矛盾。因此，每个特征值路径都保持在其原始圆盘内。由于在 $t=0$ 时每个圆盘中有一个特征值，那么在 $t=1$ 时每个圆盘中也必须恰好有一个特征值。\n\n在皮层微环路模型的背景下，这一性质对于模态分配非常重要。系统在不动点附近的动力学由 $\\dot{\\mathbf{x}} = J\\mathbf{x}$ 给出，其中 $\\mathbf{x}$ 是群体活动向量。解是形如 $\\mathbf{v}_k \\exp(\\lambda_k t)$ 的项的线性组合，其中 $(\\lambda_k, \\mathbf{v}_k)$ 是 $J$ 的一个特征对。每个这样的项都是一个动力学模态。每个特征值 $\\lambda_i$ 都被限制在以 $j_{ii}$ 为中心的小圆盘 $D_i$ 内，这一事实意味着 $\\lambda_i \\approx j_{ii}$。对角线元素 $j_{ii}$ 代表群体 $i$ 的固有衰减率。由于非对角线耦合项很小，相应的特征向量 $\\mathbf{v}_i$ 的最大分量将出现在第 $i$ 维。因此，与 $\\lambda_i$ 相关的动力学模态主要由群体 $i$ 的活动主导。这使得我们可以将每个特征模态与一个特定群体直接关联起来，从而将网络复杂动力学的分析简化为一组近似独立的、特定于群体的动力学。\n\n**3. J 的行列式**\n\n矩阵 $J$ 是一个上三角矩阵。三角矩阵（包括上三角和下三角）的一个基本性质是其行列式等于其对角线元素的乘积。因此，$J$ 的行列式计算如下：\n$$\n\\det(J) = j_{11} \\times j_{22} \\times j_{33} \\times j_{44}\n$$\n代入给定值：\n$$\n\\det(J) = (-0.5) \\times (-2.0) \\times (-4.0) \\times (-7.0)\n$$\n$$\n\\det(J) = (1.0) \\times (28.0)\n$$\n$$\n\\det(J) = 28.0\n$$\n另一种确认方法是，回想一下矩阵的行列式也是其所有特征值的乘积。由于 $J$ 是上三角矩阵，其特征值恰好是其对角线元素：$\\lambda_1 = -0.5$，$\\lambda_2 = -2.0$，$\\lambda_3 = -4.0$ 和 $\\lambda_4 = -7.0$。它们的乘积是 $\\lambda_1 \\lambda_2 \\lambda_3 \\lambda_4 = (-0.5)(-2.0)(-4.0)(-7.0) = 28.0$，这证实了结果。\n$J$ 的行列式是 $28$。",
            "answer": "$$\n\\boxed{28}\n$$"
        },
        {
            "introduction": "理解了系统的基本动态模式后，下一步是评估其对扰动的敏感性。在计算神经科学中，尤其是在模拟“平衡状态”的皮层网络时，模型可能表现出“病态”（ill-conditioned）特性，即模型参数（如突触权重）的微小不确定性会导致系统响应的巨大差异。本练习将引导您构建一个这样的例子，并使用矩阵的条件数 $\\kappa(\\mathbf{A})$ 来量化这种敏感性，从而揭示理论模型在面对真实生物噪声时的鲁棒性 。",
            "id": "3994759",
            "problem": "考虑一个接近平衡态的双种群皮层微电路的线性速率模型，其中稳态发放率 $\\mathbf{r}$ 满足 $(\\mathbf{I} - \\mathbf{W}) \\mathbf{r} = \\mathbf{u}$，$\\mathbf{u}$ 为外部输入。有效算子 $\\mathbf{A} = \\mathbf{I} - \\mathbf{W}$ 决定了敏感性 $\\mathbf{r} = \\mathbf{A}^{-1} \\mathbf{u}$。在此状态下，测量 $\\mathbf{W}$ 的微小误差会转化为 $\\mathbf{A}$ 的误差，如果 $\\mathbf{A}$ 是病态的，则可能在 $\\mathbf{A}^{-1}$ 中产生巨大误差。\n\n构建以下具体例子，其中 $\\mathbf{A}$ 的微小误差导致 $\\mathbf{A}^{-1}$ 的巨大误差，并使用矩阵二范数（由欧几里得范数诱导）下的条件数 $\\kappa(\\mathbf{A})$ 来量化这种放大效应：\n\n- 设种群间的耦合是强且近似平衡的，对称权重矩阵 $\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ 由下式给出\n$$\n\\mathbf{W} = \\begin{pmatrix}\n0  1 - \\varepsilon \\\\\n1 - \\varepsilon  0\n\\end{pmatrix},\n$$\n其中 $0  \\varepsilon \\ll 1$。那么有效算子为\n$$\n\\mathbf{A} = \\mathbf{I} - \\mathbf{W} = \\begin{pmatrix}\n1  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1\n\\end{pmatrix}.\n$$\n\n- 假设实验测量误差将 $\\mathbf{A}$ 扰动为 $\\mathbf{A} + \\Delta \\mathbf{A}$，其形式为小的对称扰动\n$$\n\\Delta \\mathbf{A} = \\begin{pmatrix}\n0  \\delta \\\\\n\\delta  0\n\\end{pmatrix},\n$$\n其中 $|\\delta| \\ll 1$。\n\n仅使用核心定义和第一性原理——即矩阵逆的定义、应用于 $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}$ 的乘法法则以及诱导二范数的定义——执行以下操作：\n\n1. 推导 $\\Delta \\mathbf{A}$ 和 $\\Delta \\mathbf{A}^{-1}$ 之间的一阶关系，该关系解释了当 $|\\delta|$ 很小时，$\\mathbf{A}$ 中的微小误差如何在 $\\mathbf{A}^{-1}$ 中被放大。\n\n2. 计算 $\\mathbf{A}$ 在二范数下的条件数 $\\kappa_{2}(\\mathbf{A})$，并将其表示为 $\\varepsilon$ 的闭式函数。\n\n3. 当 $\\varepsilon = 0.01$ 时，数值计算 $\\kappa_{2}(\\mathbf{A})$。\n\n最终答案仅报告 $\\varepsilon = 0.01$ 时 $\\kappa_{2}(\\mathbf{A})$ 的数值。将答案四舍五入至四位有效数字。量 $\\kappa_{2}(\\mathbf{A})$ 是无量纲的。",
            "solution": "问题要求完成三项与矩阵逆对扰动的敏感性相关的任务，使用的是一个从简化神经回路模型中导出的特定 $2 \\times 2$ 矩阵 $\\mathbf{A}$。这些任务是：1) 推导逆矩阵变化量 $\\Delta \\mathbf{A}^{-1}$ 的一阶近似；2) 计算条件数 $\\kappa_2(\\mathbf{A})$ 作为参数 $\\varepsilon$ 的函数；3) 为 $\\varepsilon$ 的一个特定值计算此条件数。\n\n首先，我们推导扰动 $\\Delta \\mathbf{A}$ 与其导致的逆矩阵扰动 $\\Delta\\mathbf{A}^{-1}$ 之间的一阶关系。我们从受扰矩阵 $(\\mathbf{A} + \\Delta\\mathbf{A})^{-1}$ 的逆的定义开始。设其逆表示为 $\\mathbf{A}^{-1} + \\Delta\\mathbf{A}^{-1}$，其中 $\\Delta\\mathbf{A}^{-1}$ 代表逆矩阵的变化量。一个矩阵与其逆的乘积是单位矩阵 $\\mathbf{I}$：\n$$\n(\\mathbf{A} + \\Delta\\mathbf{A}) (\\mathbf{A}^{-1} + \\Delta\\mathbf{A}^{-1}) = \\mathbf{I}\n$$\n展开左侧：\n$$\n\\mathbf{A} \\mathbf{A}^{-1} + \\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{I}\n$$\n由于 $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}$，方程简化为：\n$$\n\\mathbf{I} + \\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{I}\n$$\n两边减去 $\\mathbf{I}$ 得：\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{0}\n$$\n问题考虑的是小扰动 $\\Delta\\mathbf{A}$（因此 $\\Delta\\mathbf{A}^{-1}$ 也很小）。因此，项 $\\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1}$ 是二阶小量，在一阶近似中可以忽略。这导出了线性近似：\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} \\approx \\mathbf{0}\n$$\n为了解出 $\\Delta\\mathbf{A}^{-1}$，我们整理这些项：\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} \\approx -\\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\n假设 $\\mathbf{A}$ 是可逆的，我们可以在左边乘以 $\\mathbf{A}^{-1}$：\n$$\n\\mathbf{A}^{-1} (\\mathbf{A} \\Delta\\mathbf{A}^{-1}) \\approx \\mathbf{A}^{-1} (-\\Delta\\mathbf{A} \\mathbf{A}^{-1})\n$$\n$$\n(\\mathbf{A}^{-1} \\mathbf{A}) \\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\n$$\n\\mathbf{I} \\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\n这就得到了一阶关系的最终形式，它解释了 $\\mathbf{A}$ 中的误差如何传播到其逆矩阵中：\n$$\n\\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\n这个关系表明，逆矩阵中的误差被因子 $\\mathbf{A}^{-1}$ 放大。如果 $\\mathbf{A}^{-1}$ 的范数很大，即使是很小的误差 $\\Delta\\mathbf{A}$ 也会导致很大的误差 $\\Delta\\mathbf{A}^{-1}$。\n\n接下来，我们计算给定矩阵 $\\mathbf{A}$ 的条件数 $\\kappa_2(\\mathbf{A})$。关于矩阵二范数的条件数定义为 $\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2$。矩阵的二范数是其最大的奇异值 $\\sigma_{\\max}$。对于对称矩阵，奇异值是其特征值的绝对值。矩阵 $\\mathbf{A}$ 如下所示：\n$$\n\\mathbf{A} = \\begin{pmatrix}\n1  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1\n\\end{pmatrix}\n$$\n这是一个实对称矩阵。我们通过求解特征方程 $\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0$ 来找到其特征值 $\\lambda$。\n$$\n\\det\\begin{pmatrix}\n1 - \\lambda  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1 - \\lambda\n\\end{pmatrix} = 0\n$$\n$$\n(1 - \\lambda)^2 - (-1 + \\varepsilon)^2 = 0\n$$\n$$\n(1 - \\lambda)^2 = (1 - \\varepsilon)^2\n$$\n对两边取平方根，得到两种可能性：\n$$\n1 - \\lambda = \\pm(1 - \\varepsilon)\n$$\n情况1：$1 - \\lambda = 1 - \\varepsilon \\implies \\lambda_1 = \\varepsilon$。\n情况2：$1 - \\lambda = -(1 - \\varepsilon) \\implies 1 - \\lambda = -1 + \\varepsilon \\implies \\lambda_2 = 2 - \\varepsilon$。\n$\\mathbf{A}$ 的特征值是 $\\lambda_1 = \\varepsilon$ 和 $\\lambda_2 = 2 - \\varepsilon$。考虑到 $0  \\varepsilon \\ll 1$，两个特征值都是正的。$\\mathbf{A}$ 的奇异值是 $\\sigma_1 = |\\lambda_1| = \\varepsilon$ 和 $\\sigma_2 = |\\lambda_2| = 2 - \\varepsilon$。\n$\\mathbf{A}$ 的二范数是最大的奇异值：\n$$\n\\|\\mathbf{A}\\|_2 = \\sigma_{\\max}(\\mathbf{A}) = \\max(\\varepsilon, 2 - \\varepsilon)\n$$\n由于 $0  \\varepsilon  1$，我们有 $2 - \\varepsilon > 1$ 和 $\\varepsilon  1$，所以 $2 - \\varepsilon > \\varepsilon$。因此，$\\|\\mathbf{A}\\|_2 = 2 - \\varepsilon$。\n\n现在我们求 $\\|\\mathbf{A}^{-1}\\|_2$。矩阵 $\\mathbf{A}^{-1}$ 也是对称的，所以它的二范数是其特征值绝对值的最大值。$\\mathbf{A}^{-1}$ 的特征值是 $\\mathbf{A}$ 的特征值的倒数，即 $1/\\lambda_1 = 1/\\varepsilon$ 和 $1/\\lambda_2 = 1/(2 - \\varepsilon)$。\n$\\mathbf{A}^{-1}$ 的二范数是：\n$$\n\\|\\mathbf{A}^{-1}\\|_2 = \\sigma_{\\max}(\\mathbf{A}^{-1}) = \\max\\left(\\left|\\frac{1}{\\varepsilon}\\right|, \\left|\\frac{1}{2 - \\varepsilon}\\right|\\right)\n$$\n由于 $0  \\varepsilon \\ll 1$，我们有 $0  \\varepsilon  2 - \\varepsilon$，这意味着 $1/\\varepsilon > 1/(2 - \\varepsilon)$。因此，$\\|\\mathbf{A}^{-1}\\|_2 = 1/\\varepsilon$。\n条件数 $\\kappa_2(\\mathbf{A})$ 是范数的乘积：\n$$\n\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2 = (2 - \\varepsilon) \\left(\\frac{1}{\\varepsilon}\\right) = \\frac{2 - \\varepsilon}{\\varepsilon}\n$$\n这个表达式表明，当 $\\varepsilon \\to 0$ 时，条件数 $\\kappa_2(\\mathbf{A}) \\to \\infty$。这证实了对于很小的 $\\varepsilon$，矩阵 $\\mathbf{A}$ 是病态的。\n\n最后，我们计算当 $\\varepsilon = 0.01$ 时 $\\kappa_2(\\mathbf{A})$ 的值。将这个值代入推导出的公式：\n$$\n\\kappa_2(\\mathbf{A}) = \\frac{2 - 0.01}{0.01} = \\frac{1.99}{0.01} = 199\n$$\n问题要求答案四舍五入到四位有效数字。精确值是 $199$。表示为四位有效数字，即为 $199.0$。",
            "answer": "$$\n\\boxed{199.0}\n$$"
        },
        {
            "introduction": "理论分析最终需要与实验数据相结合。在神经科学研究中，我们常常假设高维的神经活动可以被投影到一个与特定任务相关的低维“任务子空间”中，从而揭示其潜在的计算结构。这项编程练习将指导您实现这一核心的数据分析技术：将神经活动数据正交投影到由基向量定义的子空间上，并计算该子空间解释的方差比例，这是连接神经编码与行为的关键一步 。",
            "id": "3994746",
            "problem": "考虑一个神经群体活动矩阵 $X \\in \\mathbb{R}^{N \\times T}$，其中 $N$ 是神经元的数量，$T$ 是时间点或试验的次数。设 $B \\in \\mathbb{R}^{N \\times K}$ 是一组基向量（作为列向量），定义了一个任务子空间。仅使用线性代数的核心定义，在程序中执行以下任务：\n\n1. 对活动矩阵 $X$ 按时间进行均值中心化，得到 $X_0 \\in \\mathbb{R}^{N \\times T}$。具体方法是从 $X$ 的每一行中减去其在 $T$ 个时间点上的样本均值。\n2. 将任务子空间解释为 $B$ 在 $\\mathbb{R}^N$ 中的列向量所张成的空间。仅使用正交投影的定义（即子空间中与给定向量欧几里得距离最小的唯一元素），构造 $X_0$ 的每一列到该子空间的正交投影，得到投影矩阵 $\\hat{X} \\in \\mathbb{R}^{N \\times T}$。\n3. 将解释方差分数定义为投影捕获的总方差与 $X_0$ 中总方差的比率。使用总方差的定义，即所有条目的平方偏差之和。如果 $X_0$ 中的总方差为零，则将解释方差分数定义为 $0$。\n\n您的程序必须将上述步骤应用于下面的每个测试用例，并为每个用例生成解释方差分数，结果四舍五入到六位小数。\n\n测试套件（每个 $X_i$ 和 $B_i$ 如下指定）：\n\n- 案例 1（一般情况，非标准正交基，非平凡捕获）：\n  $$\n  X_1 = \\begin{bmatrix}\n  1  2  0  -1 \\\\\n  0  1  -1  -2 \\\\\n  2  0  1  -1\n  \\end{bmatrix}, \\quad\n  B_1 = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  1  0\n  \\end{bmatrix}.\n  $$\n\n- 案例 2（数据完全位于子空间内）：\n  $$\n  X_2 = \\begin{bmatrix}\n  2  3  4 \\\\\n  0  0  0\n  \\end{bmatrix}, \\quad\n  B_2 = \\begin{bmatrix}\n  1 \\\\\n  0\n  \\end{bmatrix}.\n  $$\n\n- 案例 3（数据与子空间正交）：\n  $$\n  X_3 = \\begin{bmatrix}\n  0  0  0 \\\\\n  5  6  7\n  \\end{bmatrix}, \\quad\n  B_3 = \\begin{bmatrix}\n  1 \\\\\n  0\n  \\end{bmatrix}.\n  $$\n\n- 案例 4（非标准正交、秩亏基）：\n  $$\n  X_4 = \\begin{bmatrix}\n  1  2  -1  0  3 \\\\\n  1  2  -1  0  3 \\\\\n  2  0  1  3  -1\n  \\end{bmatrix}, \\quad\n  B_4 = \\begin{bmatrix}\n  1  2 \\\\\n  1  2 \\\\\n  0  0\n  \\end{bmatrix}.\n  $$\n\n- 案例 5（零方差边界情况）：\n  $$\n  X_5 = \\begin{bmatrix}\n  1  1  1 \\\\\n  2  2  2 \\\\\n  3  3  3\n  \\end{bmatrix}, \\quad\n  B_5 = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  0  0\n  \\end{bmatrix}.\n  $$\n\n最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5]$），其中每个 $r_i$ 是案例 $i$ 的解释方差分数，四舍五入到六位小数。不应打印任何其他文本。",
            "solution": "该问题要求计算神经群体活动矩阵 $X \\in \\mathbb{R}^{N \\times T}$ 中，能够被其到由矩阵 $B \\in \\mathbb{R}^{N \\times K}$ 的列空间定义的任务子空间上的正交投影所解释的方差比例。该过程包括三个主要步骤：对数据进行均值中心化、将均值中心化后的数据投影到子空间上，以及计算方差之比。\n\n设 $X$ 为活动矩阵，其中 $N$ 是神经元数量，$T$ 是时间点数量。设 $B$ 是一个矩阵，其列向量张成任务子空间。\n\n**第 1 步：对活动矩阵进行均值中心化**\n\n第一步是将原始活动矩阵 $X$ 转换为均值中心化矩阵 $X_0$。这通过从每个神经元活动的时间序列中减去其时间均值来实现。对于 $X$ 的每一行 $i$（代表第 $i$ 个神经元），其在 $T$ 个时间点上的平均活动 $\\mu_i$ 计算如下：\n$$\n\\mu_i = \\frac{1}{T} \\sum_{j=1}^{T} X_{ij}\n$$\n均值中心化矩阵 $X_0$ 的条目则由下式给出：\n$$\n(X_0)_{ij} = X_{ij} - \\mu_i\n$$\n在矩阵表示法中，如果我们定义一个行均值列向量 $\\boldsymbol{\\mu} \\in \\mathbb{R}^{N}$，其中 $(\\boldsymbol{\\mu})_i = \\mu_i$，以及一个全为 1 的行向量 $\\mathbf{1}_T^T \\in \\mathbb{R}^{1 \\times T}$，则均值中心化矩阵为：\n$$\nX_0 = X - \\boldsymbol{\\mu} \\mathbf{1}_T^T\n$$\n此操作确保 $X_0$ 的每一行均值为 0。\n\n**第 2 步：到任务子空间的正交投影**\n\n任务子空间 $\\mathcal{S}$ 是矩阵 $B$ 的列空间，记为 $\\mathcal{S} = \\text{span}(B)$。我们需要找到 $X_0$ 的每个列向量到 $\\mathcal{S}$ 上的正交投影。对于任意向量 $\\mathbf{x} \\in \\mathbb{R}^N$（代表 $X_0$ 的一列），其正交投影 $\\hat{\\mathbf{x}}$ 是 $\\mathcal{S}$ 中使欧几里得距离 $\\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|_2$ 最小化的唯一向量。\n\n由于 $\\hat{\\mathbf{x}} \\in \\mathcal{S}$，它可以表示为 $B$ 的列向量的线性组合，即 $\\hat{\\mathbf{x}} = B\\mathbf{c}$，其中 $\\mathbf{c} \\in \\mathbb{R}^K$ 是某个系数向量。问题是找到使 $\\|\\mathbf{x} - B\\mathbf{c}\\|_2^2$ 最小化的 $\\mathbf{c}$。这是一个经典的线性最小二乘问题。解可以通过求解正规方程组找到：\n$$\n(B^T B) \\mathbf{c} = B^T \\mathbf{x}\n$$\n如果 $B$ 的列是线性无关的，那么 $B^T B$ 是可逆的，且 $\\mathbf{c} = (B^T B)^{-1} B^T \\mathbf{x}$。投影则为 $\\hat{\\mathbf{x}} = B(B^T B)^{-1} B^T \\mathbf{x}$。矩阵 $P = B(B^T B)^{-1} B^T$ 是投影矩阵。\n\n然而，$B$ 的列可能线性相关（即 $B$ 不是满列秩），这使得 $B^T B$ 成为奇异矩阵。这个问题仍然可以使用 Moore-Penrose 伪逆来解决，用上标“$+$”表示。到 $B$ 的列空间上的投影矩阵 $P$ 由通用公式给出：\n$$\nP = B B^+\n$$\n这个公式普遍适用，无论 $B$ 的秩是多少。整个均值中心化数据矩阵 $X_0$ 到子空间 $\\mathcal{S}$ 的投影是通过将投影矩阵 $P$ 应用于它的每一列来获得的：\n$$\n\\hat{X} = P X_0 = (B B^+) X_0\n$$\n一个重要的性质是，如果 $X_0$ 是均值中心化的（行和为 0），其投影 $\\hat{X}$ 也是均值中心化的。这是因为投影是一种线性运算，它保持了行均值为零的特性。\n\n**第 3 步：解释方差分数**\n\n问题将均值中心化矩阵的总方差定义为其所有条目的平方偏差之和。对于像 $X_0$ 这样的均值中心化矩阵，任何条目与其行均值的偏差就是该条目本身。因此，总方差是矩阵的弗罗贝尼乌斯范数的平方：\n$$\n\\text{Var}(X_0) = \\sum_{i=1}^{N} \\sum_{j=1}^{T} (X_0)_{ij}^2 = \\|X_0\\|_F^2\n$$\n类似地，由于 $\\hat{X}$ 也是均值中心化的，投影捕获的方差为：\n$$\n\\text{Var}(\\hat{X}) = \\sum_{i=1}^{N} \\sum_{j=1}^{T} (\\hat{X})_{ij}^2 = \\|\\hat{X}\\|_F^2\n$$\n解释方差分数（EVF）是投影数据的方差与原始均值中心化数据的总方差之比：\n$$\n\\text{EVF} = \\frac{\\text{Var}(\\hat{X})}{\\text{Var}(X_0)} = \\frac{\\|\\hat{X}\\|_F^2}{\\|X_0\\|_F^2}\n$$\n根据问题陈述，如果总方差 $\\text{Var}(X_0)$ 为 $0$，则 EVF 定义为 $0$。当原始矩阵 $X$ 的所有行随时间保持不变时，会发生这种情况。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_explained_variance(X: np.ndarray, B: np.ndarray) - float:\n    \"\"\"\n    Calculates the fraction of variance in X explained by projection onto the\n    subspace spanned by the columns of B.\n\n    Args:\n        X (np.ndarray): The neural activity matrix of shape (N, T).\n        B (np.ndarray): The basis matrix for the task subspace of shape (N, K).\n\n    Returns:\n        float: The explained variance fraction.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    X = np.asarray(X, dtype=float)\n    B = np.asarray(B, dtype=float)\n\n    # Step 1: Mean-center the activity matrix X across time.\n    # The shape of row_means will be (N, 1), which broadcasts correctly.\n    row_means = X.mean(axis=1, keepdims=True)\n    X0 = X - row_means\n\n    # Step 2: Calculate the total variance in the mean-centered data.\n    # Total variance is the squared Frobenius norm of the mean-centered matrix.\n    total_variance = np.sum(X0**2)\n\n    # Handle the edge case where total variance is zero.\n    if np.isclose(total_variance, 0):\n        return 0.0\n\n    # Step 3: Construct the orthogonal projection of X0 onto the subspace.\n    # Compute the projection matrix P = B * B_pseudoinverse.\n    B_pinv = np.linalg.pinv(B)\n    P = B @ B_pinv\n\n    # Project the mean-centered data matrix X0.\n    X_hat = P @ X0\n\n    # Step 4: Calculate the variance captured by the projection.\n    # This is the squared Frobenius norm of the projected matrix.\n    projected_variance = np.sum(X_hat**2)\n\n    # Step 5: Compute the explained variance fraction.\n    explained_variance_fraction = projected_variance / total_variance\n\n    return explained_variance_fraction\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general, non-orthonormal basis, nontrivial capture)\n        {\n            'X': [[1, 2, 0, -1], [0, 1, -1, -2], [2, 0, 1, -1]],\n            'B': [[1, 0], [0, 1], [1, 0]],\n        },\n        # Case 2 (data lie entirely in the subspace)\n        {\n            'X': [[2, 3, 4], [0, 0, 0]],\n            'B': [[1], [0]],\n        },\n        # Case 3 (data orthogonal to the subspace)\n        {\n            'X': [[0, 0, 0], [5, 6, 7]],\n            'B': [[1], [0]],\n        },\n        # Case 4 (non-orthonormal, rank-deficient basis)\n        {\n            'X': [[1, 2, -1, 0, 3], [1, 2, -1, 0, 3], [2, 0, 1, 3, -1]],\n            'B': [[1, 2], [1, 2], [0, 0]],\n        },\n        # Case 5 (zero variance edge case)\n        {\n            'X': [[1, 1, 1], [2, 2, 2], [3, 3, 3]],\n            'B': [[1, 0], [0, 1], [0, 0]],\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case['X']\n        B = case['B']\n        evf = calculate_explained_variance(X, B)\n        # Round the result to six decimal places\n        results.append(f\"{evf:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}