{
    "hands_on_practices": [
        {
            "introduction": "In analyzing the dynamics of a complex neural network, the eigenvalues of the system's Jacobian matrix are paramount, yet often difficult to compute directly. This exercise introduces a powerful analytical shortcut, the Gershgorin circle theorem, which allows us to localize the eigenvalues within specific regions of the complex plane. By applying this theorem to a model microcircuit, you will learn how to associate distinct dynamical modes with specific neural populations, a crucial step for interpreting the functional role of different parts of a network .",
            "id": "3994745",
            "problem": "Consider a linearized four-population rate model of a feedforward cortical microcircuit, where the dynamics near a fixed point are approximated by a Jacobian matrix $J \\in \\mathbb{R}^{4 \\times 4}$. The matrix $J$ is designed such that inter-population couplings are weak and unidirectional in a feedforward manner, yielding\n$$\nJ \\;=\\;\n\\begin{pmatrix}\n-0.5  0.10  0.04  0.02 \\\\\n0  -2.0  0.08  0.03 \\\\\n0  0  -4.0  0.06 \\\\\n0  0  0  -7.0\n\\end{pmatrix}.\n$$\nFor each row $i$, define the Gershgorin disk in the complex plane centered at $a_{ii}$ with a radius equal to the sum of the absolute values of the off-diagonal entries in that row. Using core definitions of eigenvalues and the Gershgorin disk characterization of the spectrum, do the following:\n1. Compute the center and radius of the Gershgorin disk for each row of $J$ and verify that the disks are pairwise disjoint.\n2. Provide a rigorous argument that each disjoint Gershgorin disk must contain exactly one eigenvalue of $J$, and discuss briefly how this property aids in mode assignment for the microcircuit (i.e., associating eigenmodes to individual populations).\n3. Finally, compute the determinant of $J$ as a single real-valued number. No rounding is required for your final numerical answer.",
            "solution": "The problem is first validated for scientific soundness, self-consistency, and clarity. The problem statement provides a well-defined Jacobian matrix $J$ from a linearized dynamical system, a standard approach in computational neuroscience. The tasks involve applying the Gershgorin circle theorem and calculating a determinant, both of which are standard procedures in linear algebra. The premises are scientifically grounded and mathematically sound. The problem is well-posed, unambiguous, and all necessary information is provided. Therefore, the problem is deemed valid and a full solution is presented.\n\nThe Jacobian matrix is given by:\n$$\nJ \\;=\\;\n\\begin{pmatrix}\n-0.5  0.10  0.04  0.02 \\\\\n0  -2.0  0.08  0.03 \\\\\n0  0  -4.0  0.06 \\\\\n0  0  0  -7.0\n\\end{pmatrix}\n$$\n\nThe Gershgorin disk $D_i$ for row $i$ of a matrix $A = (a_{ij})$ is the closed disk in the complex plane centered at $c_i = a_{ii}$ with radius $r_i = \\sum_{j \\neq i} |a_{ij}|$.\n\n**1. Gershgorin Disks and Their Disjointness**\n\nWe compute the center $c_i$ and radius $r_i$ for each of the four Gershgorin disks corresponding to the rows of the matrix $J$.\n\nFor row $1$:\nThe center is the diagonal element $c_1 = j_{11} = -0.5$.\nThe radius is the sum of the absolute values of the off-diagonal elements: $r_1 = |0.10| + |0.04| + |0.02| = 0.10 + 0.04 + 0.02 = 0.16$.\nSo, the first disk is $D_1 = \\{z \\in \\mathbb{C} : |z - (-0.5)| \\le 0.16\\}$.\n\nFor row $2$:\nThe center is $c_2 = j_{22} = -2.0$.\nThe radius is $r_2 = |0| + |0.08| + |0.03| = 0.08 + 0.03 = 0.11$.\nSo, the second disk is $D_2 = \\{z \\in \\mathbb{C} : |z - (-2.0)| \\le 0.11\\}$.\n\nFor row $3$:\nThe center is $c_3 = j_{33} = -4.0$.\nThe radius is $r_3 = |0| + |0| + |0.06| = 0.06$.\nSo, the third disk is $D_3 = \\{z \\in \\mathbb{C} : |z - (-4.0)| \\le 0.06\\}$.\n\nFor row $4$:\nThe center is $c_4 = j_{44} = -7.0$.\nThe radius is $r_4 = |0| + |0| + |0| = 0$.\nSo, the fourth disk is $D_4 = \\{z \\in \\mathbb{C} : |z - (-7.0)| \\le 0\\}$, which is simply the point $z = -7.0$.\n\nNext, we verify that these four disks are pairwise disjoint. Two disks $D_i$ and $D_j$ are disjoint if the distance between their centers, $|c_i - c_j|$, is greater than the sum of their radii, $r_i + r_j$.\n\n-   **$D_1$ and $D_2$**: $|c_1 - c_2| = |-0.5 - (-2.0)| = 1.5$. The sum of radii is $r_1 + r_2 = 0.16 + 0.11 = 0.27$. Since $1.5  0.27$, $D_1$ and $D_2$ are disjoint.\n-   **$D_1$ and $D_3$**: $|c_1 - c_3| = |-0.5 - (-4.0)| = 3.5$. The sum of radii is $r_1 + r_3 = 0.16 + 0.06 = 0.22$. Since $3.5  0.22$, $D_1$ and $D_3$ are disjoint.\n-   **$D_1$ and $D_4$**: $|c_1 - c_4| = |-0.5 - (-7.0)| = 6.5$. The sum of radii is $r_1 + r_4 = 0.16 + 0 = 0.16$. Since $6.5  0.16$, $D_1$ and $D_4$ are disjoint.\n-   **$D_2$ and $D_3$**: $|c_2 - c_3| = |-2.0 - (-4.0)| = 2.0$. The sum of radii is $r_2 + r_3 = 0.11 + 0.06 = 0.17$. Since $2.0  0.17$, $D_2$ and $D_3$ are disjoint.\n-   **$D_2$ and $D_4$**: $|c_2 - c_4| = |-2.0 - (-7.0)| = 5.0$. The sum of radii is $r_2 + r_4 = 0.11 + 0 = 0.11$. Since $5.0  0.11$, $D_2$ and $D_4$ are disjoint.\n-   **$D_3$ and $D_4$**: $|c_3 - c_4| = |-4.0 - (-7.0)| = 3.0$. The sum of radii is $r_3 + r_4 = 0.06 + 0 = 0.06$. Since $3.0  0.06$, $D_3$ and $D_4$ are disjoint.\n\nAll pairs of Gershgorin disks are disjoint.\n\n**2. Eigenvalue Localization and Mode Assignment**\n\nA key result that extends the Gershgorin circle theorem states that if a set of $k$ Gershgorin disks is disjoint from the remaining $n-k$ disks, then the union of the first set contains exactly $k$ eigenvalues of the matrix. In our case, each disk $D_i$ is disjoint from the union of the other three disks. Applying the theorem with $k=1$ for each disk $D_i$ ($i=1, 2, 3, 4$), we conclude that each disk must contain precisely one eigenvalue.\n\nA rigorous argument for this property relies on continuity. Let $D$ be the diagonal part of $J$ and let $O = J - D$ be the off-diagonal part. Construct a family of matrices $J(t) = D + tO$ for a parameter $t \\in [0, 1]$. The eigenvalues of $J(t)$ are continuous functions of $t$.\nAt $t=0$, we have $J(0) = D$, a diagonal matrix. Its eigenvalues are simply its diagonal entries, $\\lambda_i(0) = j_{ii}$. Each eigenvalue $\\lambda_i(0)$ lies at the center of its corresponding Gershgorin disk $D_i$.\nAs $t$ increases from $0$ to $1$, the eigenvalues $\\lambda_i(t)$ trace continuous paths in the complex plane. The first Gershgorin circle theorem guarantees that all eigenvalues of $J=J(1)$ must lie within the union of the four disks $\\bigcup_{i=1}^4 D_i$. Since the disks are disjoint, a continuous path $\\lambda_i(t)$ that starts in $D_i$ cannot reach another disk $D_k$ (for $k \\neq i$) without leaving the union of all disks, which would contradict the theorem. Therefore, each eigenvalue path remains confined to its original disk. As there is one eigenvalue in each disk at $t=0$, there must be exactly one eigenvalue in each disk at $t=1$.\n\nIn the context of the cortical microcircuit model, this property is highly significant for mode assignment. The dynamics of the system near the fixed point are given by $\\dot{\\mathbf{x}} = J\\mathbf{x}$, where $\\mathbf{x}$ is the vector of population activities. The solution is a linear combination of terms of the form $\\mathbf{v}_k \\exp(\\lambda_k t)$, where $(\\lambda_k, \\mathbf{v}_k)$ is an eigenpair of $J$. Each such term is a dynamical mode.\nThe fact that each eigenvalue $\\lambda_i$ is confined to a small disk $D_i$ centered at $j_{ii}$ implies that $\\lambda_i \\approx j_{ii}$. The diagonal entry $j_{ii}$ represents the intrinsic decay rate of population $i$. The corresponding eigenvector $\\mathbf{v}_i$ will have its largest component in the $i$-th dimension because the off-diagonal coupling terms are small. Consequently, the dynamical mode associated with $\\lambda_i$ is dominated by the activity of population $i$. This allows a direct association of each eigenmode with a specific population, simplifying the analysis of the network's complex dynamics into a set of nearly independent population-specific dynamics.\n\n**3. Determinant of J**\n\nThe matrix $J$ is an upper triangular matrix. A fundamental property of triangular matrices (both upper and lower) is that their determinant is the product of their diagonal entries.\nTherefore, the determinant of $J$ is calculated as:\n$$\n\\det(J) = j_{11} \\times j_{22} \\times j_{33} \\times j_{44}\n$$\nSubstituting the given values:\n$$\n\\det(J) = (-0.5) \\times (-2.0) \\times (-4.0) \\times (-7.0)\n$$\n$$\n\\det(J) = (1.0) \\times (28.0)\n$$\n$$\n\\det(J) = 28.0\n$$\nAn alternative way to confirm this is to recall that the determinant of a matrix is also the product of its eigenvalues. Since $J$ is upper triangular, its eigenvalues are precisely its diagonal entries: $\\lambda_1 = -0.5$, $\\lambda_2 = -2.0$, $\\lambda_3 = -4.0$, and $\\lambda_4 = -7.0$.\nTheir product is $\\lambda_1 \\lambda_2 \\lambda_3 \\lambda_4 = (-0.5)(-2.0)(-4.0)(-7.0) = 28.0$, which confirms the result.\nThe determinant of $J$ is $28$.",
            "answer": "$$\n\\boxed{28}\n$$"
        },
        {
            "introduction": "Many theoretical models of cortical circuits operate in a \"balanced\" regime where strong excitatory and inhibitory inputs nearly cancel. While neurobiologically plausible, this can lead to mathematical challenges, as the system's effective matrix becomes nearly singular, or ill-conditioned. This practice demonstrates how to quantify the resulting model fragility using the condition number $\\kappa(A)$, revealing how small uncertainties in synaptic weights can be amplified into large errors in predicted network activity, a critical insight for assessing model robustness .",
            "id": "3994759",
            "problem": "Consider a linear rate model of a two-population cortical microcircuit near a balanced regime, where the steady-state firing rates $\\mathbf{r}$ satisfy $(\\mathbf{I} - \\mathbf{W}) \\mathbf{r} = \\mathbf{u}$ for external input $\\mathbf{u}$. The effective operator $\\mathbf{A} = \\mathbf{I} - \\mathbf{W}$ determines the susceptibility $\\mathbf{r} = \\mathbf{A}^{-1} \\mathbf{u}$. In this regime, small errors in measuring $\\mathbf{W}$ translate into errors in $\\mathbf{A}$ and can produce large errors in $\\mathbf{A}^{-1}$ if $\\mathbf{A}$ is ill-conditioned.\n\nConstruct the following explicit example in which small errors in $\\mathbf{A}$ lead to large errors in $\\mathbf{A}^{-1}$, and quantify the amplification using the condition number $\\kappa(\\mathbf{A})$ under the matrix two-norm (induced by the Euclidean norm):\n\n- Let the inter-population coupling be strong and nearly balanced, with a symmetric weight matrix $\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ given by\n$$\n\\mathbf{W} = \\begin{pmatrix}\n0  1 - \\varepsilon \\\\\n1 - \\varepsilon  0\n\\end{pmatrix},\n$$\nwhere $0  \\varepsilon \\ll 1$. The effective operator is then\n$$\n\\mathbf{A} = \\mathbf{I} - \\mathbf{W} = \\begin{pmatrix}\n1  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1\n\\end{pmatrix}.\n$$\n\n- Suppose experimental measurement errors perturb $\\mathbf{A}$ to $\\mathbf{A} + \\Delta \\mathbf{A}$ with a small symmetric perturbation of the form\n$$\n\\Delta \\mathbf{A} = \\begin{pmatrix}\n0  \\delta \\\\\n\\delta  0\n\\end{pmatrix},\n$$\nwhere $|\\delta| \\ll 1$.\n\nUsing only core definitions and first principles—namely, the definition of a matrix inverse, the product rule applied to $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}$, and the definition of the induced two-norm—perform the following:\n\n1. Derive the first-order relationship between $\\Delta \\mathbf{A}$ and $\\Delta \\mathbf{A}^{-1}$ that explains how small errors in $\\mathbf{A}$ are amplified in $\\mathbf{A}^{-1}$ for small $|\\delta|$.\n\n2. Compute the condition number $\\kappa_{2}(\\mathbf{A})$ of $\\mathbf{A}$ in the two-norm and express it in closed form as a function of $\\varepsilon$.\n\n3. Evaluate $\\kappa_{2}(\\mathbf{A})$ numerically for $\\varepsilon = 0.01$.\n\nReport only the numerical value of $\\kappa_{2}(\\mathbf{A})$ for $\\varepsilon = 0.01$ as your final answer. Round your answer to four significant figures. The quantity $\\kappa_{2}(\\mathbf{A})$ is dimensionless.",
            "solution": "The problem asks for three tasks related to the sensitivity of a matrix inverse to perturbations, using a specific $2 \\times 2$ matrix $\\mathbf{A}$ derived from a simplified model of a neural circuit. The tasks are: $1$) to derive the first-order approximation for the change in the inverse, $\\Delta \\mathbf{A}^{-1}$; $2$) to compute the condition number $\\kappa_2(\\mathbf{A})$ as a function of a parameter $\\varepsilon$; and $3$) to evaluate this condition number for a specific value of $\\varepsilon$.\n\nFirst, we address the derivation of the first-order relationship between a perturbation $\\Delta \\mathbf{A}$ and the resulting perturbation in the inverse, $\\Delta\\mathbf{A}^{-1}$. We start from the definition of the inverse for the perturbed matrix, $(\\mathbf{A} + \\Delta\\mathbf{A})^{-1}$. Let the inverse be denoted as $\\mathbf{A}^{-1} + \\Delta\\mathbf{A}^{-1}$, where $\\Delta\\mathbf{A}^{-1}$ represents the change in the inverse. The product of a matrix and its inverse is the identity matrix $\\mathbf{I}$:\n$$\n(\\mathbf{A} + \\Delta\\mathbf{A}) (\\mathbf{A}^{-1} + \\Delta\\mathbf{A}^{-1}) = \\mathbf{I}\n$$\nExpanding the left-hand side gives:\n$$\n\\mathbf{A} \\mathbf{A}^{-1} + \\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{I}\n$$\nSince $\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}$, the equation simplifies to:\n$$\n\\mathbf{I} + \\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{I}\n$$\nSubtracting $\\mathbf{I}$ from both sides yields:\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1} = \\mathbf{0}\n$$\nThe problem considers small perturbations $\\Delta\\mathbf{A}$ (and consequently small $\\Delta\\mathbf{A}^{-1}$). Therefore, the term $\\Delta\\mathbf{A} \\Delta\\mathbf{A}^{-1}$ is of second order in smallness and can be neglected in a first-order approximation. This leads to the linear approximation:\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} + \\Delta\\mathbf{A} \\mathbf{A}^{-1} \\approx \\mathbf{0}\n$$\nTo solve for $\\Delta\\mathbf{A}^{-1}$, we rearrange the terms:\n$$\n\\mathbf{A} \\Delta\\mathbf{A}^{-1} \\approx -\\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\nAssuming $\\mathbf{A}$ is invertible, we can left-multiply by $\\mathbf{A}^{-1}$:\n$$\n\\mathbf{A}^{-1} (\\mathbf{A} \\Delta\\mathbf{A}^{-1}) \\approx \\mathbf{A}^{-1} (-\\Delta\\mathbf{A} \\mathbf{A}^{-1})\n$$\n$$\n(\\mathbf{A}^{-1} \\mathbf{A}) \\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\n$$\n\\mathbf{I} \\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\nThis gives the final first-order relationship, which explains how the error in $\\mathbf{A}$ is propagated to its inverse:\n$$\n\\Delta\\mathbf{A}^{-1} \\approx -\\mathbf{A}^{-1} \\Delta\\mathbf{A} \\mathbf{A}^{-1}\n$$\nThis relation shows that the error in the inverse is amplified by factors of $\\mathbf{A}^{-1}$. If the norm of $\\mathbf{A}^{-1}$ is large, even small errors $\\Delta\\mathbf{A}$ can lead to large errors $\\Delta\\mathbf{A}^{-1}$.\n\nNext, we compute the condition number $\\kappa_2(\\mathbf{A})$ for the given matrix $\\mathbf{A}$. The condition number with respect to the matrix two-norm is defined as $\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2$. The two-norm of a matrix is its largest singular value, $\\sigma_{\\max}$. For a symmetric matrix, the singular values are the absolute values of its eigenvalues. The matrix $\\mathbf{A}$ is given as:\n$$\n\\mathbf{A} = \\begin{pmatrix}\n1  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1\n\\end{pmatrix}\n$$\nThis is a real symmetric matrix. We find its eigenvalues, $\\lambda$, by solving the characteristic equation $\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0$.\n$$\n\\det\\begin{pmatrix}\n1 - \\lambda  -1 + \\varepsilon \\\\\n-1 + \\varepsilon  1 - \\lambda\n\\end{pmatrix} = 0\n$$\n$$\n(1 - \\lambda)^2 - (-1 + \\varepsilon)^2 = 0\n$$\n$$\n(1 - \\lambda)^2 = (1 - \\varepsilon)^2\n$$\nTaking the square root of both sides gives two possibilities:\n$$\n1 - \\lambda = \\pm(1 - \\varepsilon)\n$$\nCase $1$: $1 - \\lambda = 1 - \\varepsilon \\implies \\lambda_1 = \\varepsilon$.\nCase $2$: $1 - \\lambda = -(1 - \\varepsilon) \\implies 1 - \\lambda = -1 + \\varepsilon \\implies \\lambda_2 = 2 - \\varepsilon$.\nThe eigenvalues of $\\mathbf{A}$ are $\\lambda_1 = \\varepsilon$ and $\\lambda_2 = 2 - \\varepsilon$. Given that $0  \\varepsilon \\ll 1$, both eigenvalues are positive. The singular values of $\\mathbf{A}$ are $\\sigma_1 = |\\lambda_1| = \\varepsilon$ and $\\sigma_2 = |\\lambda_2| = 2 - \\varepsilon$.\nThe two-norm of $\\mathbf{A}$ is the largest singular value:\n$$\n\\|\\mathbf{A}\\|_2 = \\sigma_{\\max}(\\mathbf{A}) = \\max(\\varepsilon, 2 - \\varepsilon)\n$$\nSince $0  \\varepsilon  1$, we have $2 - \\varepsilon  1$ and $\\varepsilon  1$, so $2 - \\varepsilon  \\varepsilon$. Thus, $\\|\\mathbf{A}\\|_2 = 2 - \\varepsilon$.\n\nNow we find $\\|\\mathbf{A}^{-1}\\|_2$. The matrix $\\mathbf{A}^{-1}$ is also symmetric, so its two-norm is the maximum of the absolute values of its eigenvalues. The eigenvalues of $\\mathbf{A}^{-1}$ are the reciprocals of the eigenvalues of $\\mathbf{A}$, which are $1/\\lambda_1 = 1/\\varepsilon$ and $1/\\lambda_2 = 1/(2 - \\varepsilon)$.\nThe two-norm of $\\mathbf{A}^{-1}$ is:\n$$\n\\|\\mathbf{A}^{-1}\\|_2 = \\sigma_{\\max}(\\mathbf{A}^{-1}) = \\max\\left(\\left|\\frac{1}{\\varepsilon}\\right|, \\left|\\frac{1}{2 - \\varepsilon}\\right|\\right)\n$$\nSince $0  \\varepsilon \\ll 1$, we have $0  \\varepsilon  2 - \\varepsilon$, which implies $1/\\varepsilon  1/(2 - \\varepsilon)$. Therefore, $\\|\\mathbf{A}^{-1}\\|_2 = 1/\\varepsilon$.\nThe condition number $\\kappa_2(\\mathbf{A})$ is the product of the norms:\n$$\n\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2 = (2 - \\varepsilon) \\left(\\frac{1}{\\varepsilon}\\right) = \\frac{2 - \\varepsilon}{\\varepsilon}\n$$\nThis expression shows that as $\\varepsilon \\to 0$, the condition number $\\kappa_2(\\mathbf{A}) \\to \\infty$. This confirms that for small $\\varepsilon$, the matrix $\\mathbf{A}$ is ill-conditioned.\n\nFinally, we evaluate $\\kappa_2(\\mathbf{A})$ for $\\varepsilon = 0.01$. Substituting this value into the derived formula:\n$$\n\\kappa_2(\\mathbf{A}) = \\frac{2 - 0.01}{0.01} = \\frac{1.99}{0.01} = 199\n$$\nThe problem requires the answer to be rounded to four significant figures. The exact value is $199$. Expressed with four significant figures, this is $199.0$.",
            "answer": "$$\n\\boxed{199.0}\n$$"
        },
        {
            "introduction": "A central task in modern systems neuroscience is to uncover low-dimensional structure hidden within high-dimensional neural recordings. This is often achieved by projecting population activity onto a subspace defined by task-relevant variables or internal dynamics. This computational exercise walks you through the foundational steps of this process: projecting a neural activity matrix onto a specified subspace and quantifying the explanatory power of that projection by calculating the fraction of explained variance, a core skill for applying dimensionality reduction techniques to neural data .",
            "id": "3994746",
            "problem": "Consider a neural population activity matrix $X \\in \\mathbb{R}^{N \\times T}$ where $N$ is the number of neurons and $T$ is the number of time points or trials. Let $B \\in \\mathbb{R}^{N \\times K}$ be a set of basis vectors (as columns) that define a task subspace. Using only core definitions from linear algebra, perform the following tasks in a program:\n\n1. Mean-center the activity matrix $X$ across time, producing $X_0 \\in \\mathbb{R}^{N \\times T}$, by subtracting from each row of $X$ its sample mean over the $T$ time points.\n2. Interpret the task subspace as the span of the columns of $B$ in $\\mathbb{R}^N$. Using only the definition of orthogonal projection as the unique element in the subspace minimizing Euclidean distance from a given vector, construct the orthogonal projection of each column of $X_0$ onto that subspace, producing a projected matrix $\\hat{X} \\in \\mathbb{R}^{N \\times T}$.\n3. Define the explained variance fraction as the ratio between the total variance captured by the projection and the total variance in $X_0$. Use the definition of total variance as the sum of squared deviations across all entries. If the total variance in $X_0$ is zero, define the explained variance fraction to be $0$.\n\nYour program must apply the above to each test case below and produce the explained variance fraction for each case, rounded to six decimal places.\n\nTest suite (each $X_i$ and $B_i$ are as specified):\n\n- Case $1$ (general, non-orthonormal basis, nontrivial capture):\n  $$\n  X_1 = \\begin{bmatrix}\n  1  2  0  -1 \\\\\n  0  1  -1  -2 \\\\\n  2  0  1  -1\n  \\end{bmatrix}, \\quad\n  B_1 = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  1  0\n  \\end{bmatrix}.\n  $$\n\n- Case $2$ (data lie entirely in the subspace):\n  $$\n  X_2 = \\begin{bmatrix}\n  2  3  4 \\\\\n  0  0  0\n  \\end{bmatrix}, \\quad\n  B_2 = \\begin{bmatrix}\n  1 \\\\\n  0\n  \\end{bmatrix}.\n  $$\n\n- Case $3$ (data orthogonal to the subspace):\n  $$\n  X_3 = \\begin{bmatrix}\n  0  0  0 \\\\\n  5  6  7\n  \\end{bmatrix}, \\quad\n  B_3 = \\begin{bmatrix}\n  1 \\\\\n  0\n  \\end{bmatrix}.\n  $$\n\n- Case $4$ (non-orthonormal, rank-deficient basis):\n  $$\n  X_4 = \\begin{bmatrix}\n  1  2  -1  0  3 \\\\\n  1  2  -1  0  3 \\\\\n  2  0  1  3  -1\n  \\end{bmatrix}, \\quad\n  B_4 = \\begin{bmatrix}\n  1  2 \\\\\n  1  2 \\\\\n  0  0\n  \\end{bmatrix}.\n  $$\n\n- Case $5$ (zero variance edge case):\n  $$\n  X_5 = \\begin{bmatrix}\n  1  1  1 \\\\\n  2  2  2 \\\\\n  3  3  3\n  \\end{bmatrix}, \\quad\n  B_5 = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  0  0\n  \\end{bmatrix}.\n  $$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the explained variance fraction for case $i$, rounded to six decimal places. No additional text should be printed.",
            "solution": "The problem requires the computation of the fraction of variance in a neural population activity matrix, $X \\in \\mathbb{R}^{N \\times T}$, that is explained by its orthogonal projection onto a task subspace defined by the column space of a matrix $B \\in \\mathbb{R}^{N \\times K}$. The process involves three main steps as defined: mean-centering the data, projecting the mean-centered data onto the subspace, and computing the ratio of variances.\n\nLet $X$ be the activity matrix, where $N$ is the number of neurons and $T$ is the number of time points. Let $B$ be the matrix whose columns span the task subspace.\n\n**Step 1: Mean-Centering the Activity Matrix**\n\nThe first step is to transform the raw activity matrix $X$ into a mean-centered matrix $X_0$. This is achieved by subtracting the temporal mean of each neuron's activity from its time series. For each row $i$ of $X$ (representing the $i$-th neuron), its mean activity $\\mu_i$ across the $T$ time points is calculated:\n$$\n\\mu_i = \\frac{1}{T} \\sum_{j=1}^{T} X_{ij}\n$$\nThe entries of the mean-centered matrix $X_0$ are then given by:\n$$\n(X_0)_{ij} = X_{ij} - \\mu_i\n$$\nIn matrix notation, if we define a column vector of row means $\\boldsymbol{\\mu} \\in \\mathbb{R}^{N}$ where $(\\boldsymbol{\\mu})_i = \\mu_i$, and a row vector of ones $\\mathbf{1}_T^T \\in \\mathbb{R}^{1 \\times T}$, the mean-centered matrix is:\n$$\nX_0 = X - \\boldsymbol{\\mu} \\mathbf{1}_T^T\n$$\nThis operation ensures that each row of $X_0$ has a mean of $0$.\n\n**Step 2: Orthogonal Projection onto the Task Subspace**\n\nThe task subspace, $\\mathcal{S}$, is the column space of the matrix $B$, denoted $\\mathcal{S} = \\text{span}(B)$. We need to find the orthogonal projection of each column vector of $X_0$ onto $\\mathcal{S}$. For an arbitrary vector $\\mathbf{x} \\in \\mathbb{R}^N$ (representing a column of $X_0$), its orthogonal projection $\\hat{\\mathbf{x}}$ is the unique vector in $\\mathcal{S}$ that minimizes the Euclidean distance $\\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|_2$.\n\nSince $\\hat{\\mathbf{x}} \\in \\mathcal{S}$, it can be expressed as a linear combination of the columns of $B$, i.e., $\\hat{\\mathbf{x}} = B\\mathbf{c}$ for some coefficient vector $\\mathbf{c} \\in \\mathbb{R}^K$. The problem is to find the $\\mathbf{c}$ that minimizes $\\|\\mathbf{x} - B\\mathbf{c}\\|_2^2$. This is a classical linear least-squares problem. The solution is found by solving the normal equations:\n$$\n(B^T B) \\mathbf{c} = B^T \\mathbf{x}\n$$\nIf the columns of $B$ are linearly independent, $B^T B$ is invertible, and $\\mathbf{c} = (B^T B)^{-1} B^T \\mathbf{x}$. The projection is then $\\hat{\\mathbf{x}} = B(B^T B)^{-1} B^T \\mathbf{x}$. The matrix $P = B(B^T B)^{-1} B^T$ is the projection matrix.\n\nHowever, the columns of $B$ might be linearly dependent (i.e., $B$ is not full column rank), making $B^T B$ singular. The problem can still be solved using the Moore-Penrose pseudoinverse, denoted by a superscript '$+$'. The projection matrix $P$ onto the column space of $B$ is given by the general formula:\n$$\nP = B B^+\n$$\nThis formula is universally applicable, regardless of the rank of $B$. The projection of the entire mean-centered data matrix $X_0$ onto the subspace $\\mathcal{S}$ is obtained by applying the projection matrix $P$ to each of its columns:\n$$\n\\hat{X} = P X_0 = (B B^+) X_0\n$$\nAn important property is that if $X_0$ is mean-centered (rows sum to $0$), its projection $\\hat{X}$ is also mean-centered. This is because the sum of columns of $X_0$ is the zero vector, and $P(\\mathbf{0}) = \\mathbf{0}$.\n\n**Step 3: Explained Variance Fraction**\n\nThe problem defines total variance for a mean-centered matrix as the sum of squared deviations across all its entries. For a mean-centered matrix like $X_0$, the deviation of any entry from its row mean is the entry itself. Therefore, the total variance is the squared Frobenius norm of the matrix:\n$$\n\\text{Var}(X_0) = \\sum_{i=1}^{N} \\sum_{j=1}^{T} (X_0)_{ij}^2 = \\|X_0\\|_F^2\n$$\nSimilarly, since $\\hat{X}$ is also mean-centered, the variance captured by the projection is:\n$$\n\\text{Var}(\\hat{X}) = \\sum_{i=1}^{N} \\sum_{j=1}^{T} (\\hat{X})_{ij}^2 = \\|\\hat{X}\\|_F^2\n$$\nThe explained variance fraction (EVF) is the ratio of the variance of the projected data to the total variance of the original mean-centered data:\n$$\n\\text{EVF} = \\frac{\\text{Var}(\\hat{X})}{\\text{Var}(X_0)} = \\frac{\\|\\hat{X}\\|_F^2}{\\|X_0\\|_F^2}\n$$\nAs per the problem statement, if the total variance $\\text{Var}(X_0)$ is $0$, the EVF is defined to be $0$. This occurs when all rows of the original matrix $X$ are constant over time.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_explained_variance(X: np.ndarray, B: np.ndarray) - float:\n    \"\"\"\n    Calculates the fraction of variance in X explained by projection onto the\n    subspace spanned by the columns of B.\n\n    Args:\n        X (np.ndarray): The neural activity matrix of shape (N, T).\n        B (np.ndarray): The basis matrix for the task subspace of shape (N, K).\n\n    Returns:\n        float: The explained variance fraction.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    X = np.asarray(X, dtype=float)\n    B = np.asarray(B, dtype=float)\n\n    # Step 1: Mean-center the activity matrix X across time.\n    # The shape of row_means will be (N, 1), which broadcasts correctly.\n    row_means = X.mean(axis=1, keepdims=True)\n    X0 = X - row_means\n\n    # Step 2: Calculate the total variance in the mean-centered data.\n    # Total variance is the squared Frobenius norm of the mean-centered matrix.\n    total_variance = np.sum(X0**2)\n\n    # Handle the edge case where total variance is zero.\n    if np.isclose(total_variance, 0):\n        return 0.0\n\n    # Step 3: Construct the orthogonal projection of X0 onto the subspace.\n    # Compute the projection matrix P = B * B_pseudoinverse.\n    B_pinv = np.linalg.pinv(B)\n    P = B @ B_pinv\n\n    # Project the mean-centered data matrix X0.\n    X_hat = P @ X0\n\n    # Step 4: Calculate the variance captured by the projection.\n    # This is the squared Frobenius norm of the projected matrix.\n    projected_variance = np.sum(X_hat**2)\n\n    # Step 5: Compute the explained variance fraction.\n    explained_variance_fraction = projected_variance / total_variance\n\n    return explained_variance_fraction\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general, non-orthonormal basis, nontrivial capture)\n        {\n            'X': [[1, 2, 0, -1], [0, 1, -1, -2], [2, 0, 1, -1]],\n            'B': [[1, 0], [0, 1], [1, 0]],\n        },\n        # Case 2 (data lie entirely in the subspace)\n        {\n            'X': [[2, 3, 4], [0, 0, 0]],\n            'B': [[1], [0]],\n        },\n        # Case 3 (data orthogonal to the subspace)\n        {\n            'X': [[0, 0, 0], [5, 6, 7]],\n            'B': [[1], [0]],\n        },\n        # Case 4 (non-orthonormal, rank-deficient basis)\n        {\n            'X': [[1, 2, -1, 0, 3], [1, 2, -1, 0, 3], [2, 0, 1, 3, -1]],\n            'B': [[1, 2], [1, 2], [0, 0]],\n        },\n        # Case 5 (zero variance edge case)\n        {\n            'X': [[1, 1, 1], [2, 2, 2], [3, 3, 3]],\n            'B': [[1, 0], [0, 1], [0, 0]],\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case['X']\n        B = case['B']\n        evf = calculate_explained_variance(X, B)\n        # Round the result to six decimal places\n        results.append(f\"{evf:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}