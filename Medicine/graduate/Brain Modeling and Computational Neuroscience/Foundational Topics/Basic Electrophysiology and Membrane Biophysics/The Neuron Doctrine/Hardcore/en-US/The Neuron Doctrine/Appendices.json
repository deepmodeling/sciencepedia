{
    "hands_on_practices": [
        {
            "introduction": "The Neuron Doctrine posits that the neuron is the fundamental computational unit of the nervous system. A crucial aspect of this idea is that a neuron's information processing capabilities are determined by its intrinsic biophysical properties. This first practice explores the most basic of these: the membrane time constant, $\\tau_m$. By modeling a patch of a neuron's membrane as a simple resistor-capacitor (RC) circuit, we can derive and calculate the time scale that governs how a neuron integrates incoming signals, effectively setting its temporal window for computation.",
            "id": "4030376",
            "problem": "A central implication of the neuron doctrine is that neurons are discrete computational units whose subthreshold voltage dynamics are governed locally by their own membranes, except at anatomically specialized junctions. Consider a passive, spatially isopotential membrane patch of a single neuron modeled as a lumped resistor-capacitor (RC) element. Starting from charge conservation, Ohm’s law, and the capacitor constitutive relation, derive the first-order linear ordinary differential equation for the membrane voltage relative to leak reversal under a constant injected current density, and identify the membrane time constant in terms of the specific membrane resistance $R_m$ and the specific membrane capacitance $C_m$. Then, for $R_m = 10^{4}\\,\\Omega\\cdot\\text{cm}^{2}$ and $C_m = 1\\,\\mu\\text{F}/\\text{cm}^{2}$, compute the numerical value of the time constant in seconds. Round your final numerical answer to three significant figures and express it in seconds. Finally, explain, using the neuron doctrine as your conceptual basis, how the membrane time constant constrains the maximal temporal rate at which a neuron can integrate synaptic inputs independently of its neighbors when direct electrical coupling and ephaptic effects are negligible.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of biophysics and computational neuroscience, well-posed with a clear objective, and free of ambiguity or factual error. The provided parameters are realistic for a biological neuron.\n\nWe will proceed with the solution, which consists of three parts:\n1.  Derivation of the governing differential equation for the membrane potential and identification of the time constant.\n2.  Calculation of the numerical value of the time constant.\n3.  Conceptual explanation of the time constant's role in synaptic integration, grounded in the neuron doctrine.\n\n**1. Derivation of the Governing Equation and Identification of the Time Constant**\n\nWe model a patch of a neuronal membrane as a parallel resistor-capacitor (RC) circuit. The problem statement directs us to start from the principle of charge conservation, also known as Kirchhoff's Current Law, applied to the membrane patch. The total current injected into the neuron, $I_{inj}$, must be equal to the sum of the currents flowing out through the membrane's resistive (leak) and capacitive pathways.\n\nLet $V(t)$ be the membrane potential at time $t$, and $V_L$ be the constant leak reversal potential. The total current flowing across the membrane is the sum of the ionic current, $I_L$, passing through the leak channels (modeled as a resistor), and the capacitive current, $I_C$, which charges or discharges the membrane capacitance.\n\n$$ I_{total} = I_L + I_C $$\n\nAccording to Ohm's law, the leak current $I_L$ is proportional to the potential difference across the resistor:\n$$ I_L = \\frac{V(t) - V_L}{R} $$\nwhere $R$ is the total membrane resistance of the patch.\n\nThe capacitive current is given by the constitutive relation for a capacitor:\n$$ I_C = C \\frac{dV(t)}{dt} $$\nwhere $C$ is the total membrane capacitance of the patch.\n\nAn external current $I_{inj}$ is injected into the neuron. By conservation of charge, this injected current must equal the total current flowing across the membrane:\n$$ I_{inj} = I_L + I_C = \\frac{V(t) - V_L}{R} + C \\frac{dV(t)}{dt} $$\n\nThe problem provides the specific membrane resistance $R_m$ (in units of $\\Omega\\cdot\\text{cm}^2$) and specific membrane capacitance $C_m$ (in units of $\\text{F}/\\text{cm}^2$). These are properties per unit area. For a membrane patch of area $A$, the total resistance and capacitance are:\n$$ R = \\frac{R_m}{A} \\quad \\text{and} \\quad C = C_m A $$\nThe injected current is given as a constant current density, $J_{inj}$ (in units of $\\text{A}/\\text{cm}^2$), so the total injected current is $I_{inj} = J_{inj} A$.\n\nSubstituting these expressions into our charge conservation equation:\n$$ J_{inj} A = \\frac{V(t) - V_L}{R_m/A} + (C_m A) \\frac{dV(t)}{dt} $$\n\nDividing the entire equation by the area $A$ eliminates the dependence on the patch's specific geometry, yielding an equation in terms of intensive properties:\n$$ J_{inj} = \\frac{V(t) - V_L}{R_m} + C_m \\frac{dV(t)}{dt} $$\n\nThe problem asks for the equation for the membrane voltage relative to the leak reversal potential. We define a new variable $v(t) = V(t) - V_L$. Its time derivative is $\\frac{dv}{dt} = \\frac{d(V - V_L)}{dt} = \\frac{dV}{dt}$ since $V_L$ is a constant. Substituting $v(t)$ into the equation gives:\n$$ C_m \\frac{dv(t)}{dt} + \\frac{v(t)}{R_m} = J_{inj} $$\n\nTo obtain the standard form of a first-order linear ordinary differential equation, we divide by $C_m$:\n$$ \\frac{dv(t)}{dt} + \\frac{v(t)}{R_m C_m} = \\frac{J_{inj}}{C_m} $$\n\nThis is the required differential equation. The standard form for a first-order system with time constant $\\tau$ is $\\frac{dx}{dt} + \\frac{x}{\\tau} = F(t)$. By comparing our derived equation to this standard form, we can identify the membrane time constant, $\\tau_m$, as:\n$$ \\tau_m = R_m C_m $$\n\n**2. Calculation of the Membrane Time Constant**\n\nWe are given the following values:\n$$ R_m = 10^{4}\\,\\Omega\\cdot\\text{cm}^{2} $$\n$$ C_m = 1\\,\\mu\\text{F}/\\text{cm}^{2} = 1 \\times 10^{-6}\\,\\text{F}/\\text{cm}^{2} $$\n\nWe compute the product to find the time constant $\\tau_m$:\n$$ \\tau_m = R_m C_m = (10^{4}\\,\\Omega\\cdot\\text{cm}^{2}) \\times (1 \\times 10^{-6}\\,\\text{F}/\\text{cm}^{2}) $$\nThe units of area ($\\text{cm}^2$) cancel out, leaving units of $\\Omega \\cdot \\text{F}$. The product of Ohms and Farads is seconds ($\\Omega \\cdot \\text{F} = \\frac{\\text{V}}{\\text{A}} \\cdot \\frac{\\text{C}}{\\text{V}} = \\frac{\\text{C}}{\\text{A}} = \\frac{\\text{C}}{\\text{C}/\\text{s}} = \\text{s}$).\n\n$$ \\tau_m = 10^{4} \\times 10^{-6} \\, \\text{s} = 10^{-2} \\, \\text{s} = 0.01 \\, \\text{s} $$\n\nThe problem requires the answer to be rounded to three significant figures. Therefore, the numerical value is $0.0100$ seconds.\n\n**3. Conceptual Explanation**\n\nThe neuron doctrine establishes that neurons are anatomically and functionally discrete, individual cells that serve as the fundamental computational units of the nervous system. According to this doctrine, and under the stated conditions of negligible direct electrical or ephaptic coupling, a neuron's subthreshold behavior is governed by its own intrinsic biophysical properties.\n\nThe membrane time constant, $\\tau_m = R_m C_m$, is a direct physical consequence of these intrinsic properties. It represents the characteristic time scale over which the neuron's membrane potential, $v(t)$, responds to an injected current. Specifically, following a step change in current, the membrane potential will change exponentially towards its new steady-state value, reaching approximately $1 - 1/e \\approx 63.2\\%$ of the total change in one time constant.\n\nThis property has a critical functional implication for how a neuron integrates synaptic inputs. Synaptic inputs cause brief, transient currents to be injected into the postsynaptic neuron.\n- A neuron with a **large time constant** ($\\tau_m$) has a \"slow\" membrane. The voltage response to a single synaptic input will rise and decay slowly. This allows the neuron to summate inputs that are separated by relatively long time intervals (temporal summation). Such a neuron acts as an integrator or a low-pass filter, effectively smoothing out high-frequency fluctuations in its input.\n- A neuron with a **small time constant** ($\\tau_m$) has a \"fast\" membrane. The voltage response is brief, rising and falling quickly. This allows the neuron to resolve synaptic inputs that arrive in rapid succession as distinct events, making it a coincidence detector.\n\nTherefore, the membrane time constant $\\tau_m$ constrains the maximal temporal rate at which a neuron can process inputs independently. For two successive synaptic inputs to be treated as separate events rather than being summed into a single, larger potential, the second input must arrive after the voltage change from the first has substantially decayed. The time scale for this decay is dictated by $\\tau_m$. The maximal frequency of inputs that the neuron can resolve is thus inversely related to its time constant, roughly on the order of $1/\\tau_m$. This establishes a fundamental limit on the temporal processing capability of the neuron, a limit that arises directly from the physical properties of the discrete cellular unit, as specified by the neuron doctrine.",
            "answer": "$$\\boxed{0.0100}$$"
        },
        {
            "introduction": "While the concept of a single computational unit is powerful, real neurons possess complex and sprawling dendritic trees. This anatomical reality refines the Neuron Doctrine, suggesting that computation can be localized within different compartments of a single cell. This exercise moves from the temporal domain to the spatial domain, using passive cable theory to derive the electrotonic length constant, $\\lambda$. This value quantifies the spatial scale over which synaptic inputs can influence each other, providing a physical basis for understanding how different parts of a neuron can function as semi-independent computational subunits.",
            "id": "4030373",
            "problem": "A central premise of the neuron doctrine is that individual neurons operate as functionally discrete units, with dendritic trees performing local computations that are only partially coupled to the soma and to other dendritic branches. Under passive subthreshold conditions, the spatial scale over which local dendritic signals are electrotonically coupled is governed by the one-dimensional cable description of a uniform cylindrical dendrite segment. Starting only from conservation of current and Ohm’s law applied to a passive, uniform cylinder of radius $a$, specific membrane resistance $R_{m}$ (in $\\Omega \\cdot \\mathrm{m}^{2}$), and axial resistivity $R_{a}$ (in $\\Omega \\cdot \\mathrm{m}$), derive the steady-state cable equation and obtain the electrotonic length constant in terms of $a$, $R_{m}$, and $R_{a}$. Then, using the provided parameters $a = 8.0 \\times 10^{-7}\\,\\mathrm{m}$, $R_{m} = 1.2\\,\\Omega \\cdot \\mathrm{m}^{2}$, and $R_{a} = 0.9\\,\\Omega \\cdot \\mathrm{m}$, compute its numerical value.\n\nTo quantify how the electrotonic length constant sets the spatial scale of independence among dendritic compartments, define the axial amplitude coupling coefficient between two adjacent compartments separated by a center-to-center distance $\\Delta x$ as the ratio $V(\\Delta x)/V(0)$ under the linear steady-state cable solution. Impose the independence threshold $V(\\Delta x)/V(0) \\leq \\varepsilon$ with $\\varepsilon = 0.1$. Solve for the minimum separation $\\Delta x_{\\mathrm{indep}}$ that guarantees this bound holds.\n\nReport the single final result $\\Delta x_{\\mathrm{indep}}$ in micrometers, and round your answer to four significant figures. Express the final separation in $\\mu\\mathrm{m}$.",
            "solution": "The problem requires the derivation of the steady-state cable equation for a uniform cylindrical dendrite, the calculation of the electrotonic length constant, and the determination of a minimum separation distance for functional independence based on an attenuation threshold. The derivation will start from the fundamental principles of conservation of current and Ohm's law.\n\nFirst, we model the dendrite as a one-dimensional uniform cylinder of radius $a$. We consider an infinitesimal segment of the cylinder of length $dx$ at a position $x$ along its axis. Let $V(x)$ be the intracellular voltage relative to a ground reference potential outside the cell. The axial current flowing inside the cylinder is denoted by $i_a(x)$, and the membrane current leaking out of the cylinder per unit length is $j_m(x)$.\n\nThe principle of conservation of current (Kirchhoff's current law) applied to the segment $dx$ states that the change in axial current along the segment must equal the current that leaks out through the membrane. In the steady-state, this is expressed as:\n$$ \\frac{di_a}{dx} = -j_m(x) $$\n\nNext, we apply Ohm's law to relate the currents to the voltage.\nThe axial current $i_a$ is driven by the voltage gradient along the axis of the dendrite. The resistance of the infinitesimal segment $dx$ to axial current flow is given by $dR_{axial} = R_a \\frac{dx}{A}$, where $R_a$ is the axial resistivity and $A = \\pi a^2$ is the cross-sectional area of the cylinder. Ohm's law states that $dV = -i_a dR_{axial}$, which in differential form is:\n$$ \\frac{dV}{dx} = -i_a(x) \\frac{R_a}{\\pi a^2} $$\nRearranging for the axial current, we get:\n$$ i_a(x) = -\\frac{\\pi a^2}{R_a} \\frac{dV}{dx} $$\n\nThe membrane current density per unit length, $j_m$, is the current leaking out through the membrane over a unit length of the cylinder. The resistance of the membrane patch corresponding to the infinitesimal length $dx$ is $dR_{membrane} = \\frac{R_m}{\\text{Area}} = \\frac{R_m}{2\\pi a dx}$, where $R_m$ is the specific membrane resistance. The current leaking through this patch is $dI_m = \\frac{V(x)}{dR_{membrane}} = \\frac{V(x) (2\\pi a dx)}{R_m}$. The membrane current per unit length is thus $j_m = \\frac{dI_m}{dx}$:\n$$ j_m(x) = \\frac{2\\pi a}{R_m} V(x) $$\n\nNow, we substitute the expressions for $i_a(x)$ and $j_m(x)$ into the current conservation equation. First, we differentiate the expression for $i_a(x)$ with respect to $x$, assuming the parameters $a$ and $R_a$ are constant:\n$$ \\frac{di_a}{dx} = -\\frac{\\pi a^2}{R_a} \\frac{d^2V}{dx^2} $$\nEquating this with $-j_m(x)$:\n$$ -\\frac{\\pi a^2}{R_a} \\frac{d^2V}{dx^2} = -\\frac{2\\pi a}{R_m} V(x) $$\nSimplifying this expression yields the steady-state cable equation:\n$$ \\frac{d^2V}{dx^2} = \\left(\\frac{2\\pi a}{R_m}\\right) \\left(\\frac{R_a}{\\pi a^2}\\right) V(x) $$\n$$ \\frac{d^2V}{dx^2} = \\frac{2 R_a}{a R_m} V(x) $$\nThis equation is typically written in the form $\\lambda^2 \\frac{d^2V}{dx^2} - V = 0$ or $\\frac{d^2V}{dx^2} - \\frac{1}{\\lambda^2} V(x) = 0$. By comparing our derived equation to this standard form, we can identify the electrotonic length constant, $\\lambda$, as:\n$$ \\frac{1}{\\lambda^2} = \\frac{2 R_a}{a R_m} \\implies \\lambda = \\sqrt{\\frac{a R_m}{2 R_a}} $$\n\nThe problem provides the following parameters:\nRadius $a = 8.0 \\times 10^{-7}\\,\\mathrm{m}$\nSpecific membrane resistance $R_{m} = 1.2\\,\\Omega \\cdot \\mathrm{m}^{2}$\nAxial resistivity $R_{a} = 0.9\\,\\Omega \\cdot \\mathrm{m}$\n\nSubstituting these values into the expression for $\\lambda$:\n$$ \\lambda = \\sqrt{\\frac{(8.0 \\times 10^{-7}\\,\\mathrm{m}) (1.2\\,\\Omega \\cdot \\mathrm{m}^{2})}{2 (0.9\\,\\Omega \\cdot \\mathrm{m})}} = \\sqrt{\\frac{9.6 \\times 10^{-7}}{1.8}\\,\\mathrm{m}^2} = \\sqrt{5.333... \\times 10^{-7}\\,\\mathrm{m}^2} $$\n$$ \\lambda \\approx 7.302967 \\times 10^{-4}\\,\\mathrm{m} $$\n\nThe general solution to the steady-state cable equation, $\\frac{d^2V}{dx^2} - \\frac{V}{\\lambda^2} = 0$, is:\n$$ V(x) = C_1 \\exp(x/\\lambda) + C_2 \\exp(-x/\\lambda) $$\nFor a local signal source (e.g., current injection at $x=0$) in an effectively infinite cable, the voltage must decay to zero as $|x| \\to \\infty$. This requires that for $x > 0$, $C_1=0$, and for $x0$, $C_2=0$. The solution describes an exponential decay of voltage away from the source:\n$$ V(x) = V(0) \\exp(-|x|/\\lambda) $$\nThe axial amplitude coupling coefficient between two compartments separated by a distance $\\Delta x$ is defined as the ratio $V(\\Delta x)/V(0)$, which for $\\Delta x \\ge 0$ is:\n$$ \\frac{V(\\Delta x)}{V(0)} = \\frac{V(0) \\exp(-\\Delta x/\\lambda)}{V(0)} = \\exp(-\\Delta x/\\lambda) $$\nThe independence threshold is imposed as $V(\\Delta x)/V(0) \\leq \\varepsilon$, where $\\varepsilon = 0.1$.\n$$ \\exp(-\\Delta x/\\lambda) \\leq 0.1 $$\nWe need to find the minimum separation, $\\Delta x_{\\mathrm{indep}}$, that satisfies this condition. Since the exponential function is monotonically decreasing, this minimum distance is found by setting the expression equal to the threshold value:\n$$ \\exp(-\\Delta x_{\\mathrm{indep}}/\\lambda) = 0.1 $$\nTaking the natural logarithm of both sides:\n$$ -\\frac{\\Delta x_{\\mathrm{indep}}}{\\lambda} = \\ln(0.1) = -\\ln(10) $$\n$$ \\Delta x_{\\mathrm{indep}} = \\lambda \\ln(10) $$\nNow, we substitute the calculated value of $\\lambda$ and the value of $\\ln(10) \\approx 2.302585$:\n$$ \\Delta x_{\\mathrm{indep}} \\approx (7.302967 \\times 10^{-4}\\,\\mathrm{m}) \\times 2.302585 $$\n$$ \\Delta x_{\\mathrm{indep}} \\approx 1.681570 \\times 10^{-3}\\,\\mathrm{m} $$\nThe problem asks for the result to be expressed in micrometers ($\\mu\\mathrm{m}$). Since $1\\,\\mathrm{m} = 10^6\\,\\mu\\mathrm{m}$:\n$$ \\Delta x_{\\mathrm{indep}} \\approx (1.681570 \\times 10^{-3}) \\times 10^6\\,\\mu\\mathrm{m} = 1681.570\\,\\mu\\mathrm{m} $$\nRounding the final answer to four significant figures gives:\n$$ \\Delta x_{\\mathrm{indep}} \\approx 1682\\,\\mu\\mathrm{m} $$",
            "answer": "$$\\boxed{1682}$$"
        },
        {
            "introduction": "The Neuron Doctrine provides a conceptual framework, but which level of abstraction is appropriate for modeling a specific neural phenomenon? Should we treat a neuron as a single point, a set of coupled compartments, or even consider interactions with other cell types like glia? This final practice equips you with modern tools from statistical model selection to address this question quantitatively. By applying information-theoretic criteria like AIC and BIC, you will learn how to use experimental data to adjudicate between competing models, turning a conceptual choice into a data-driven decision.",
            "id": "4030322",
            "problem": "Consider the neuron doctrine, which posits that neurons are discrete, individual units of signaling. In computational neuroscience, model selection at different organizational levels tests whether observed dynamics are better captured by a point-neuron view, a compartmental view (multiple coupled subunits), or neuron–glia coupling. You will construct an information-theoretic model selection criterion grounded in likelihood under Gaussian noise to decide between three models given time-series data and a known external input.\n\nYou are given a scalar time series $\\{y_t\\}_{t=0}^{T-1}$ representing a neurally relevant observable and a scalar input $\\{u_t\\}_{t=0}^{T-1}$. Assume additive independent and identically distributed Gaussian noise. Define the following candidate models using discrete-time linear autoregressive structures aligned to different biological levels of description:\n\n- Model $\\mathcal{M}_1$ (neuron-level, point neuron approximation): for $t \\geq 1$, \n$$y_t = \\alpha y_{t-1} + \\beta u_t + \\varepsilon_t,$$\nwhere $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n\n- Model $\\mathcal{M}_2$ (compartment-level, two effective compartments via second-order memory): for $t \\geq 2$,\n$$y_t = \\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + \\beta_1 u_t + \\beta_2 u_{t-1} + \\varepsilon_t,$$\nwhere $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n\n- Model $\\mathcal{M}_3$ (glial-including, slow modulatory drive included as a fixed filtered regressor): define a slow variable $g_t$ by the recursion $g_t = \\rho g_{t-1} + \\eta u_{t-1}$ with $g_0 = 0$ and fixed known $(\\rho,\\eta)$, and for $t \\geq 2$,\n$$y_t = \\alpha y_{t-1} + \\beta u_t + \\gamma g_t + \\varepsilon_t,$$\nwhere $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nAll models are fit by maximizing the Gaussian likelihood, using ordinary least squares for the linear parameters. The variance $\\sigma^2$ is treated as a free parameter and estimated by the maximum likelihood estimator. You must derive the log-likelihood under the Gaussian noise assumption and then derive the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). Construct a single decision score that uses these information-theoretic metrics to select one model index $m \\in \\{1,2,3\\}$ for a given dataset. The final decision rule must be fully specified from first principles and must not rely on any shortcut formulas beyond what you derive from the Gaussian likelihood and standard definitions of these criteria. You must specify the counted number of free parameters $k$ for each model, explicitly including the noise variance parameter $\\sigma^2$.\n\nYour program must:\n\n1. Generate datasets according to the following test suite using a fixed random seed:\n\n   - Test case $1$ (happy path, neuron-level): sample length $T = 300$. Input is generated by $u_t = \\lambda u_{t-1} + w_t$ with $u_0=0$, $\\lambda = 0.9$, and $w_t \\sim \\mathcal{N}(0,1)$. The observable is generated by $\\mathcal{M}_1$ with $\\alpha = 0.8$, $\\beta = 0.6$, and additive noise $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma^2 = 0.04$.\n\n   - Test case $2$ (happy path, compartment-level): sample length $T = 300$. Same input generation as test case $1$. The observable is generated by $\\mathcal{M}_2$ with $\\alpha_1 = 1.4$, $\\alpha_2 = -0.6$, $\\beta_1 = 0.3$, $\\beta_2 = 0.1$, and $\\sigma^2 = 0.04$.\n\n   - Test case $3$ (happy path, glial-including): sample length $T = 300$. Same input generation as test case $1$. The observable is generated by $\\mathcal{M}_3$ with $\\alpha = 0.5$, $\\beta = 0.2$, $\\gamma = 0.8$. The slow variable uses $(\\rho,\\eta) = (0.95, 0.05)$. Noise variance $\\sigma^2 = 0.0225$.\n\n   - Test case $4$ (boundary condition, small sample with compartment-level ground truth and high noise): sample length $T = 40$. Same input generation as test case $1$. The observable is generated by $\\mathcal{M}_2$ with $\\alpha_1 = 1.4$, $\\alpha_2 = -0.6$, $\\beta_1 = 0.3$, $\\beta_2 = 0.1$, and $\\sigma^2 = 0.25$.\n\n   In all cases, set $y_0 = 0$, and for $\\mathcal{M}_2$ and $\\mathcal{M}_3$ also set $y_1 = 0$ initially when needed, and for the slow variable initialization set $g_0 = 0$.\n\n2. Fit each of $\\mathcal{M}_1$, $\\mathcal{M}_2$, and $\\mathcal{M}_3$ to the same aligned sample indices $t = 2,3,\\dots,T-1$ so that all models are evaluated on an identical number of target points. For $\\mathcal{M}_3$, construct $g_t$ using the fixed $(\\rho,\\eta)$ specified above.\n\n3. Compute the maximum likelihood estimates for the linear parameters of each model, estimate $\\sigma^2$ by the maximum likelihood estimator, compute the log-likelihood, and then compute AIC and BIC for each model using your derived expressions. Define a single scalar decision score per model by combining these criteria, and select the model index $m \\in \\{1,2,3\\}$ that minimizes this decision score.\n\n4. For each test case, output the selected model index $m$ as an integer. The final output must be a single line containing the results as a comma-separated list enclosed in square brackets, for example, $\\texttt{[1,2,3,1]}$.\n\nAll quantities in this problem are dimensionless and unitless. Angles are not used. The final outputs are integers. Use a fixed random seed to ensure reproducibility across runs. Your decision score must be explicitly defined from AIC and BIC that you derive, and you must explicitly justify and specify the parameter counts $k$ for each model, including the noise variance parameter $\\sigma^2$.",
            "solution": "The problem requires performing model selection between three linear autoregressive models using information-theoretic criteria. The solution involves deriving the maximum likelihood estimates for the model parameters, calculating the log-likelihood, and then using the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) to select the best model.\n\n### 1. Unified Model Representation and Likelihood Function\n\nAll three candidate models, $\\mathcal{M}_1$, $\\mathcal{M}_2$, and $\\mathcal{M}_3$, are instances of a general linear model. For a given time series $\\{y_t\\}_{t=0}^{T-1}$ and input $\\{u_t\\}_{t=0}^{T-1}$, each model proposes a linear relationship for $y_t$ based on past values of $y_t$, and current or past values of $u_t$. The general form is:\n$$y_t = \\mathbf{x}_t^T \\boldsymbol{\\theta} + \\varepsilon_t$$\nwhere $\\mathbf{x}_t$ is a vector of regressors, $\\boldsymbol{\\theta}$ is a vector of linear parameters, and $\\varepsilon_t$ is a noise term assumed to be independently and identically distributed (i.i.d.) according to a Gaussian distribution, $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nTo ensure comparability, all models are fitted to the same set of $N = T-2$ observations, corresponding to $t = 2, 3, \\dots, T-1$. This defines a system of linear equations in vector-matrix form:\n$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$$\nwhere $\\mathbf{y} = [y_2, y_3, \\dots, y_{T-1}]^T$ is the $N \\times 1$ vector of observations, $\\mathbf{X}$ is the $N \\times p$ design matrix whose rows are $\\mathbf{x}_t^T$, $\\boldsymbol{\\theta}$ is the $p \\times 1$ vector of parameters, and $\\boldsymbol{\\varepsilon}$ is the $N \\times 1$ vector of noise terms. Here, $p$ is the number of linear regression parameters for a given model.\n\nThe probability density of a single noise term $\\varepsilon_t$ is $p(\\varepsilon_t) = (2\\pi\\sigma^2)^{-1/2} \\exp(-\\varepsilon_t^2/(2\\sigma^2))$. The likelihood of observing the data vector $\\mathbf{y}$ given parameters $\\boldsymbol{\\theta}$ and $\\sigma^2$ is the product of the probabilities of each observation:\n$$L(\\boldsymbol{\\theta}, \\sigma^2 | \\mathbf{y}, \\mathbf{X}) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_{i+1} - \\mathbf{x}_{i+1}^T\\boldsymbol{\\theta})^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-N/2} \\exp\\left(-\\frac{1}{2\\sigma^2} ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}||_2^2\\right)$$\nThe corresponding log-likelihood function is:\n$$\\ln L(\\boldsymbol{\\theta}, \\sigma^2) = -\\frac{N}{2}\\ln(2\\pi) - \\frac{N}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\text{RSS}(\\boldsymbol{\\theta})$$\nwhere $\\text{RSS}(\\boldsymbol{\\theta}) = ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}||_2^2$ is the Residual Sum of Squares.\n\n### 2. Maximum Likelihood Estimation (MLE)\n\nTo find the maximum likelihood estimates (MLEs), we first maximize $\\ln L$ with respect to $\\boldsymbol{\\theta}$. This is equivalent to minimizing $\\text{RSS}(\\boldsymbol{\\theta})$, which is the standard ordinary least squares (OLS) problem. The MLE for $\\boldsymbol{\\theta}$ is the OLS estimator:\n$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\nNext, we find the MLE for $\\sigma^2$ by substituting $\\hat{\\boldsymbol{\\theta}}$ into $\\ln L$ and differentiating with respect to $\\sigma^2$. Let $\\text{RSS} = \\text{RSS}(\\hat{\\boldsymbol{\\theta}})$.\n$$\\frac{\\partial \\ln L}{\\partial (\\sigma^2)} = -\\frac{N}{2\\sigma^2} + \\frac{\\text{RSS}}{2(\\sigma^2)^2} = 0$$\nSolving for $\\sigma^2$ yields the MLE for the variance:\n$$\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{N}$$\nSubstituting $\\hat{\\boldsymbol{\\theta}}$ and $\\hat{\\sigma}^2$ back into the log-likelihood function gives the maximized log-likelihood, $\\ln \\hat{L}$:\n$$\\ln \\hat{L} = -\\frac{N}{2}\\ln(2\\pi\\hat{\\sigma}^2) - \\frac{\\text{RSS}}{2\\hat{\\sigma}^2} = -\\frac{N}{2}\\ln(2\\pi\\hat{\\sigma}^2) - \\frac{N\\hat{\\sigma}^2}{2\\hat{\\sigma}^2} = -\\frac{N}{2}\\left(1 + \\ln(2\\pi\\hat{\\sigma}^2)\\right)$$\n\n### 3. Model-Specific Parameters and Information Criteria\n\nThe number of estimated parameters, $k$, is crucial for information criteria. It includes the $p$ linear parameters in $\\boldsymbol{\\theta}$ plus the variance $\\sigma^2$.\n\n- **Model $\\mathcal{M}_1$**: $y_t = \\alpha y_{t-1} + \\beta u_t + \\varepsilon_t$.\n  - Regressors: $\\mathbf{x}_t = [y_{t-1}, u_t]^T$.\n  - Number of linear parameters: $p_1 = 2$.\n  - Total parameters: $k_1 = p_1 + 1 = 3$ (for $\\alpha, \\beta, \\sigma^2$).\n\n- **Model $\\mathcal{M}_2$**: $y_t = \\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + \\beta_1 u_t + \\beta_2 u_{t-1} + \\varepsilon_t$.\n  - Regressors: $\\mathbf{x}_t = [y_{t-1}, y_{t-2}, u_t, u_{t-1}]^T$.\n  - Number of linear parameters: $p_2 = 4$.\n  - Total parameters: $k_2 = p_2 + 1 = 5$ (for $\\alpha_1, \\alpha_2, \\beta_1, \\beta_2, \\sigma^2$).\n\n- **Model $\\mathcal{M}_3$**: $y_t = \\alpha y_{t-1} + \\beta u_t + \\gamma g_t + \\varepsilon_t$, with $g_t=\\rho g_{t-1} + \\eta u_{t-1}$.\n  - The parameters $\\rho$ and $\\eta$ are given fixed values, not estimated. The regressor $g_t$ is pre-calculated.\n  - Regressors: $\\mathbf{x}_t = [y_{t-1}, u_t, g_t]^T$.\n  - Number of linear parameters: $p_3 = 3$.\n  - Total parameters: $k_3 = p_3 + 1 = 4$ (for $\\alpha, \\beta, \\gamma, \\sigma^2$).\n\nThe Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are defined as:\n$$\\text{AIC} = 2k - 2\\ln \\hat{L}$$\n$$\\text{BIC} = k \\ln(N) - 2\\ln \\hat{L}$$\nwhere $N = T-2$ is the number of samples used for fitting. Lower values of AIC and BIC indicate a better-fitting model, after penalizing for complexity.\n\n### 4. Decision Rule\n\nAs required, we define a single scalar decision score by combining AIC and BIC. We use their sum for each model $m$:\n$$S_m = \\text{AIC}_m + \\text{BIC}_m = (2k_m + k_m \\ln(N)) - 4\\ln \\hat{L}_m = k_m(2 + \\ln(N)) - 4\\ln \\hat{L}_m$$\nThe model with the minimum score, $m^* = \\arg\\min_{m \\in \\{1,2,3\\}} S_m$, is selected as the optimal model for the given data.\n\nThe implementation will proceed by generating data for each test case, then for each model, constructing the appropriate design matrix $\\mathbf{X}$ and observation vector $\\mathbf{y}$, computing the MLEs $\\hat{\\boldsymbol{\\theta}}$ and $\\hat{\\sigma}^2$, calculating the decision score $S_m$, and finally selecting the model with the lowest score.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for the four test cases.\n    \"\"\"\n    \n    # Fix the random seed for reproducibility\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"T\": 300, \"description\": \"M1 ground truth\",\n            \"gen_model\": \"M1\", \"gen_params\": {\"alpha\": 0.8, \"beta\": 0.6},\n            \"sigma_sq\": 0.04\n        },\n        {\n            \"T\": 300, \"description\": \"M2 ground truth\",\n            \"gen_model\": \"M2\", \"gen_params\": {\"alpha1\": 1.4, \"alpha2\": -0.6, \"beta1\": 0.3, \"beta2\": 0.1},\n            \"sigma_sq\": 0.04\n        },\n        {\n            \"T\": 300, \"description\": \"M3 ground truth\",\n            \"gen_model\": \"M3\", \"gen_params\": {\"alpha\": 0.5, \"beta\": 0.2, \"gamma\": 0.8},\n            \"g_params\": {\"rho\": 0.95, \"eta\": 0.05},\n            \"sigma_sq\": 0.0225\n        },\n        {\n            \"T\": 40, \"description\": \"M2 ground truth, small T, high noise\",\n            \"gen_model\": \"M2\", \"gen_params\": {\"alpha1\": 1.4, \"alpha2\": -0.6, \"beta1\": 0.3, \"beta2\": 0.1},\n            \"sigma_sq\": 0.25\n        }\n    ]\n\n    # Parameters for input generation and glial regressor\n    u_gen_params = {\"lambda\": 0.9, \"sigma_w_sq\": 1.0}\n    m3_fixed_g_params = {\"rho\": 0.95, \"eta\": 0.05}\n\n    results = []\n\n    for case in test_cases:\n        T = case[\"T\"]\n        \n        # 1. Generate dataset\n        u = np.zeros(T)\n        w = np.random.normal(0, np.sqrt(u_gen_params[\"sigma_w_sq\"]), T)\n        for t in range(1, T):\n            u[t] = u_gen_params[\"lambda\"] * u[t-1] + w[t]\n\n        y = np.zeros(T)\n        noise = np.random.normal(0, np.sqrt(case[\"sigma_sq\"]), T)\n\n        if case[\"gen_model\"] == \"M1\":\n            p = case[\"gen_params\"]\n            for t in range(1, T):\n                y[t] = p[\"alpha\"] * y[t-1] + p[\"beta\"] * u[t] + noise[t]\n        elif case[\"gen_model\"] == \"M2\":\n            p = case[\"gen_params\"]\n            # y[0] and y[1] are zero\n            for t in range(2, T):\n                y[t] = p[\"alpha1\"] * y[t-1] + p[\"alpha2\"] * y[t-2] + \\\n                       p[\"beta1\"] * u[t] + p[\"beta2\"] * u[t-1] + noise[t]\n        elif case[\"gen_model\"] == \"M3\":\n            p_gen = case[\"gen_params\"]\n            p_g = case[\"g_params\"]\n            g = np.zeros(T)\n            for t in range(1, T):\n                g[t] = p_g[\"rho\"] * g[t-1] + p_g[\"eta\"] * u[t-1]\n            # y[0] and y[1] are zero\n            for t in range(2, T):\n                y[t] = p_gen[\"alpha\"] * y[t-1] + p_gen[\"beta\"] * u[t] + \\\n                       p_gen[\"gamma\"] * g[t] + noise[t]\n\n        # 2. Fit models and compute scores\n        # All models use data from t = 2 to T-1 for fair comparison.\n        y_target = y[2:T]\n        N = len(y_target)\n        \n        scores = []\n        \n        # --- Fit Model 1 ---\n        k1 = 3  # alpha, beta, sigma^2\n        X1 = np.vstack([y[1:T-1], u[2:T]]).T\n        score1 = compute_model_score(X1, y_target, k1, N)\n        scores.append(score1)\n\n        # --- Fit Model 2 ---\n        k2 = 5  # alpha1, alpha2, beta1, beta2, sigma^2\n        X2 = np.vstack([y[1:T-1], y[0:T-2], u[2:T], u[1:T-1]]).T\n        score2 = compute_model_score(X2, y_target, k2, N)\n        scores.append(score2)\n        \n        # --- Fit Model 3 ---\n        k3 = 4  # alpha, beta, gamma, sigma^2\n        g_fit = np.zeros(T)\n        for t in range(1, T):\n            g_fit[t] = m3_fixed_g_params[\"rho\"] * g_fit[t-1] + m3_fixed_g_params[\"eta\"] * u[t-1]\n        X3 = np.vstack([y[1:T-1], u[2:T], g_fit[2:T]]).T\n        score3 = compute_model_score(X3, y_target, k3, N)\n        scores.append(score3)\n\n        # 3. Select model with minimum score\n        best_model_idx = np.argmin(scores) + 1\n        results.append(best_model_idx)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_model_score(X, y, k, N):\n    \"\"\"\n    Computes the decision score for a given model.\n    The score is defined as AIC + BIC.\n    \"\"\"\n    #\n    # OLS solution for linear parameters\n    try:\n        theta_hat = np.linalg.solve(X.T @ X, X.T @ y)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudoinverse for singular matrix\n        theta_hat = np.linalg.pinv(X) @ y\n        \n    # Residual Sum of Squares (RSS)\n    residuals = y - X @ theta_hat\n    RSS = np.sum(residuals**2)\n    \n    # Maximum Likelihood Estimate of variance\n    sigma_sq_hat = RSS / N\n    \n    # If RSS is zero or extremely small, log-likelihood is undefined.\n    # Return a very large score to avoid selecting this model.\n    if sigma_sq_hat  1e-16:\n        return np.inf\n\n    # Maximized Log-Likelihood\n    log_likelihood = -N/2 * (1 + np.log(2 * np.pi * sigma_sq_hat))\n    \n    # AIC and BIC\n    AIC = 2 * k - 2 * log_likelihood\n    BIC = k * np.log(N) - 2 * log_likelihood\n\n    # Decision Score\n    score = AIC + BIC\n    \n    return score\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}