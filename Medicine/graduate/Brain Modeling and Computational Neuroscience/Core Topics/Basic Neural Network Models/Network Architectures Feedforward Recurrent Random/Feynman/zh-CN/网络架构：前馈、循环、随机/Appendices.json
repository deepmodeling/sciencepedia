{
    "hands_on_practices": [
        {
            "introduction": "在深入研究复杂的网络动力学之前，牢固掌握其基本工作机制至关重要。本练习  提供了一个在多层前馈网络中进行前向传播的具体、分步实践，旨在加深您对输入信号如何通过线性运算和非线性激活函数层层转换的理解。通过亲手计算网络输出，您将巩固对所有神经网络基本构成模块的直观认识。",
            "id": "4001294",
            "problem": "考虑一个具有 $L=3$ 层（两个隐藏层和一个输出层）的前馈神经网络。其逐层计算由仿射映射和固定非线性函数的复合定义如下：对于 $l \\in \\{1,2,3\\}$，预激活值为 $a^{(l)} = W^{(l)} z^{(l-1)} + b^{(l)}$，后激活值为 $z^{(l)} = \\phi^{(l)}\\!\\left(a^{(l)}\\right)$，其中 $z^{(0)} \\equiv x$。隐藏层使用修正线性单元（ReLU）非线性函数，$\\phi^{(1)}(u) = \\phi^{(2)}(u) = \\max\\{0,u\\}$，该函数逐元素应用；输出层是线性的，$\\phi^{(3)}(u)=u$。网络参数如下\n$$\nW^{(1)}=\\begin{pmatrix}\n1  0\\\\\n0  1\\\\\n1  1\n\\end{pmatrix}, \\quad\nb^{(1)}=\\begin{pmatrix}\n1\\\\\n1\\\\\n-1\n\\end{pmatrix}, \\quad\nW^{(2)}=\\begin{pmatrix}\n1  -1  2\\\\\n0  2  -1\n\\end{pmatrix}, \\quad\nb^{(2)}=\\begin{pmatrix}\n0\\\\\n1\n\\end{pmatrix}, \\quad\nW^{(3)}=\\begin{pmatrix}\n2  -1\n\\end{pmatrix}, \\quad\nb^{(3)}=\\begin{pmatrix}\n4\n\\end{pmatrix}.\n$$\n对于输入 $x=\\begin{pmatrix}1\\\\ 2\\end{pmatrix}$，计算标量网络输出 $f(x)=z^{(3)}$。此外，通过描述相关的计算依赖有向图，并基于第一性原理，解释为什么在这种前馈结构中不会出现有向环，来验证该架构的无环性。\n\n将 $f(x)$ 报告为单个实数。无需四舍五入，不涉及单位。",
            "solution": "该问题被评估为有效。它具有科学依据，问题明确，客观，并包含计算唯一解和验证所要求的网络属性所需的所有必要信息。参数在指定操作的维度上是一致的。\n\n该问题要求两件事：计算给定输入 $x$ 的网络输出 $f(x)$，以及验证前馈架构的无环性。\n\n第一部分：网络输出 $f(x)$ 的计算\n\n该网络有 $L=3$ 层。计算从输入层（$l=0$）顺序进行到输出层（$l=3$）。给定输入为 $x = z^{(0)} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\n\n第 1 层：\n预激活值 $a^{(1)}$ 是使用权重矩阵 $W^{(1)}$、输入 $z^{(0)}$ 和偏置向量 $b^{(1)}$ 计算的。\n$$a^{(1)} = W^{(1)} z^{(0)} + b^{(1)}$$\n代入给定值：\n$$W^{(1)} z^{(0)} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(2) \\\\ (0)(1) + (1)(2) \\\\ (1)(1) + (1)(2) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$$\n$$a^{(1)} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} + b^{(1)} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1+1 \\\\ 2+1 \\\\ 3-1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 2 \\end{pmatrix}$$\n后激活值 $z^{(1)}$ 是通过逐元素应用 ReLU 激活函数 $\\phi^{(1)}(u) = \\max\\{0,u\\}$ 获得的。\n$$z^{(1)} = \\phi^{(1)}(a^{(1)}) = \\begin{pmatrix} \\max\\{0, 2\\} \\\\ \\max\\{0, 3\\} \\\\ \\max\\{0, 2\\} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 2 \\end{pmatrix}$$\n\n第 2 层：\n预激活值 $a^{(2)}$ 是使用权重矩阵 $W^{(2)}$、前一层的输出 $z^{(1)}$ 和偏置向量 $b^{(2)}$ 计算的。\n$$a^{(2)} = W^{(2)} z^{(1)} + b^{(2)}$$\n代入值：\n$$W^{(2)} z^{(1)} = \\begin{pmatrix} 1  -1  2 \\\\ 0  2  -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (-1)(3) + (2)(2) \\\\ (0)(2) + (2)(3) + (-1)(2) \\end{pmatrix} = \\begin{pmatrix} 2 - 3 + 4 \\\\ 0 + 6 - 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$$\n$$a^{(2)} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} + b^{(2)} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3+0 \\\\ 4+1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 5 \\end{pmatrix}$$\n后激活值 $z^{(2)}$ 是通过逐元素应用 ReLU 激活函数 $\\phi^{(2)}(u) = \\max\\{0,u\\}$ 获得的。\n$$z^{(2)} = \\phi^{(2)}(a^{(2)}) = \\begin{pmatrix} \\max\\{0, 3\\} \\\\ \\max\\{0, 5\\} \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 5 \\end{pmatrix}$$\n\n第 3 层（输出层）：\n预激活值 $a^{(3)}$ 是使用权重矩阵 $W^{(3)}$、前一层的输出 $z^{(2)}$ 和偏置向量 $b^{(3)}$ 计算的。\n$$a^{(3)} = W^{(3)} z^{(2)} + b^{(3)}$$\n代入值：\n$$W^{(3)} z^{(2)} = \\begin{pmatrix} 2  -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} (2)(3) + (-1)(5) \\end{pmatrix} = \\begin{pmatrix} 6 - 5 \\end{pmatrix} = \\begin{pmatrix} 1 \\end{pmatrix}$$\n$$a^{(3)} = \\begin{pmatrix} 1 \\end{pmatrix} + b^{(3)} = \\begin{pmatrix} 1 \\end{pmatrix} + \\begin{pmatrix} 4 \\end{pmatrix} = \\begin{pmatrix} 5 \\end{pmatrix}$$\n后激活值 $z^{(3)}$ 是通过应用线性激活函数 $\\phi^{(3)}(u) = u$ 获得的。\n$$z^{(3)} = \\phi^{(3)}(a^{(3)}) = a^{(3)} = \\begin{pmatrix} 5 \\end{pmatrix}$$\n标量网络输出是该向量的单个元素。\n$$f(x) = 5$$\n\n第二部分：无环性验证\n\n前馈神经网络可以表示为一个有向图 $G=(V, E)$，其中 $V$ 是所有计算单元（神经元）的集合， $E$ 是表示信息流动的有向边的集合。这些单元被划分成层。对于此网络，我们有一个输入层（第 $0$ 层）、两个隐藏层（第 $1$ 层和第 $2$ 层）和一个输出层（第 $3$ 层）。\n\n令 $l(v)$ 表示单元 $v \\in V$ 的层索引。根据前馈网络的定义，第 $l$ 层中任何单元的计算仅依赖于前一层 $l-1$ 中单元的输出。这意味着如果存在从单元 $u$ 到单元 $v$ 的有向边 $(u, v) \\in E$，那么必然有 $l(u) = l(v) - 1$。因此，对于任何边 $(u,v)$，我们有 $l(u)  l(v)$。\n\n为了证明不存在有向环，我们采用反证法。假设图中存在一个有向环。有向环是一条由边 $(v_1, v_2), (v_2, v_3), \\dots, (v_{k-1}, v_k), (v_k, v_1)$ 组成的路径，其中所有顶点 $v_1, \\dots, v_k$ 都是不同的。\n\n基于前馈网络中边的属性，该路径的存在意味着层索引满足以下不等式序列：\n$$l(v_1)  l(v_2)$$\n$$l(v_2)  l(v_3)$$\n$$\\dots$$\n$$l(v_{k-1})  l(v_k)$$\n根据严格不等运算符‘$$’的传递性，我们可以从前 $k-1$ 条边得出 $l(v_1)  l(v_2)  \\dots  l(v_k)$，这意味着 $l(v_1)  l(v_k)$。\n\n然而，环路中的最后一条边是 $(v_k, v_1)$。这条边的存在意味着 $l(v_k)  l(v_1)$。\n\n因此，我们面临两个相互矛盾的陈述：$l(v_1)  l(v_k)$ 和 $l(v_k)  l(v_1)$。这是一个逻辑上的不可能。因此，最初关于存在有向环的假设必定是错误的。\n\n这从第一性原理证明了前馈网络架构是一个有向无环图（DAG）。层索引充当了图顶点的拓扑排序，这是 DAG 的一个定义性特征。",
            "answer": "$$\\boxed{5}$$"
        },
        {
            "introduction": "循环网络引入了反馈回路，这使其能够处理序列信息，但同时也给学习过程带来了挑战。本实践  探讨了其中一个最关键的挑战：训练过程中的梯度稳定性，即所谓的“梯度消失与梯度爆炸”问题。通过模拟一个简单的线性循环网络，并比较标准网络与具有正交权重矩阵的网络中的梯度流动，您将亲身体会到为何网络架构的选择对于循环系统的有效学习至关重要。",
            "id": "4001215",
            "problem": "考虑一个离散时间线性循环神经网络，其状态维度为 $n \\in \\mathbb{N}$，状态向量为 $x_t \\in \\mathbb{R}^n$，转移矩阵为 $W \\in \\mathbb{R}^{n \\times n}$，并根据递推关系 $x_{t+1} = W x_t$（$t = 0, 1, \\dots, T-1$）演化。终端损失定义为 $L = \\tfrac{1}{2} \\lVert x_T \\rVert_2^2$。将时间 $t$ 的梯度定义为 $g_t = \\nabla_{x_t} L \\in \\mathbb{R}^n$。您将使用链式法则凭经验计算梯度范数 $\\lVert g_t \\rVert_2$ 随时间的变化，并比较这两种 $W$ 选择下这些范数的稳定性：一种是正交矩阵（实值酉矩阵），另一种是缩放到指定算子范数的随机高斯矩阵。\n\n将使用的基本原理和定义：\n- 向量值函数的链式法则以及梯度通过线性映射传播的定义。\n- 欧几里得范数 $\\lVert \\cdot \\rVert_2$ 以及由欧几里得范数诱导的算子范数。\n- 正交矩阵 $W$ 满足 $W^\\top W = I$，其算子范数等于 $1$。\n- 线性系统的梯度传播遵循链式法则，除了线性之外不假设任何特殊结构。\n\n您的任务：\n1. 实现一个正交矩阵 $W_{\\mathrm{orth}} \\in \\mathbb{R}^{n \\times n}$ 的生成器，方法是对一个具有独立标准正态分布条目的矩阵进行 QR 分解，取其 Q 因子，并调整符号以确保 Q 是正交的。此 $W_{\\mathrm{orth}}$ 应满足 $W_{\\mathrm{orth}}^\\top W_{\\mathrm{orth}} = I$。\n2. 实现一个高斯矩阵 $W_{\\mathrm{gauss}} \\in \\mathbb{R}^{n \\times n}$ 的生成器，该矩阵具有独立标准正态分布条目，然后对其进行缩放，使其算子范数（最大奇异值）等于指定的标量 $\\alpha > 0$。也就是说，如果 $\\sigma_{\\max}$ 是未缩放高斯矩阵的最大奇异值，则设置 $W_{\\mathrm{gauss}} = \\alpha \\, W_{\\mathrm{raw}} / \\sigma_{\\max}$。\n3. 对于固定的时间范围 $T \\in \\mathbb{N}$ 和初始状态 $x_0 \\in \\mathbb{R}^n$（其中 $\\lVert x_0 \\rVert_2 = 1$），计算前向状态 $x_{t+1} = W x_t$（$t = 0, \\dots, T-1$），然后通过逆时递推使用链式法则计算梯度，从 $g_T = \\nabla_{x_T} L = x_T$ 开始，并按 $g_t = W^\\top g_{t+1}$（$t = T-1, \\dots, 0$）进行计算。\n4. 定义稳定性度量\n$$\nS(W) = \\frac{\\max_{0 \\le t \\le T} \\lVert g_t \\rVert_2}{\\min_{0 \\le t \\le T} \\lVert g_t \\rVert_2}.\n$$\n对每个测试用例，计算 $S(W_{\\mathrm{orth}})$ 和 $S(W_{\\mathrm{gauss}})$。\n5. 对每个测试用例，输出布尔值以指示是否 $S(W_{\\mathrm{orth}}) \\le S(W_{\\mathrm{gauss}})$，该值编码了正交转移是否表现出随时间变化至少与缩放高斯转移一样稳定的梯度范数。\n\n随机性与可复现性：\n- 每个测试用例使用固定的伪随机种子。对于测试用例索引 $k \\in \\{0,1,2,3\\}$，在为该测试用例生成任何随机量之前，将伪随机种子设置为 $s_k = 12345 + 97k$。对于每个测试用例，从独立标准正态分布中抽取 $x_0$，然后重新缩放 $x_0$ 使其单位范数为 $\\lVert x_0 \\rVert_2 = 1$。\n\n测试套件：\n- 用例 1：$n = 64$, $T = 100$, $\\alpha = 0.9$。\n- 用例 2：$n = 64$, $T = 100$, $\\alpha = 1.0$。\n- 用例 3：$n = 64$, $T = 60$, $\\alpha = 1.1$。\n- 用例 4：$n = 1$, $T = 100$, $\\alpha = 0.99$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的结果，格式为方括号内以逗号分隔的布尔值列表，例如 $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True}]$。\n- 无需外部输入。此问题不涉及物理单位或角度单位。输出列表中唯一可接受的类型是布尔值。\n\n您的实现必须是一个完整、可运行的程序。",
            "solution": "此问题是有效的。它描述了一个定义明确的计算实验，旨在研究线性循环神经网络（RNN）中的梯度稳定性，这是计算神经科学和机器学习中的一个基础课题。所有参数、定义和程序都已指定，确保了问题是自包含的、科学上合理的，并且允许一个唯一的、可验证的解决方案。\n\n### 基于原理的推导\n\n该问题的核心是线性 RNN 中梯度的动态变化。网络状态根据线性递推关系演化：\n$$\nx_{t+1} = W x_t\n$$\n其中 $x_t \\in \\mathbb{R}^n$ 是时间 $t$ 的状态向量，$W \\in \\mathbb{R}^{n \\times n}$ 是转移矩阵。该过程从初始状态 $x_0$ 开始，运行一个时间范围 $T$（从 $t=0$ 到 $t=T-1$）。目标是分析终端损失函数 $L = \\frac{1}{2} \\lVert x_T \\rVert_2^2$ 相对于每个时间步状态的梯度 $g_t = \\nabla_{x_t} L$。\n\n使用多元链式法则，我们可以建立梯度的递推关系。状态 $x_t$ 的梯度与后续状态 $x_{t+1}$ 的梯度相关：\n$$\ng_t = \\nabla_{x_t} L = \\left(\\frac{\\partial x_{t+1}}{\\partial x_t}\\right)^\\top (\\nabla_{x_{t+1}} L) = \\left(\\frac{\\partial x_{t+1}}{\\partial x_t}\\right)^\\top g_{t+1}\n$$\n由于状态更新 $x_{t+1} = W x_t$ 是一个线性变换，其雅可比矩阵就是 $\\frac{\\partial x_{t+1}}{\\partial x_t} = W$。这得到了梯度的反向递推关系：\n$$\ng_t = W^\\top g_{t+1}\n$$\n递推从最终时间步 $T$ 开始。损失 $L = \\frac{1}{2} \\sum_{i=1}^n (x_{T,i})^2$ 相对于最终状态 $x_T$ 的梯度是：\n$$\ng_T = \\nabla_{x_T} L = x_T\n$$\n将反向递推从 $t=T-1$ 展开到任意时间 $t$，可以得到梯度的闭式表达式：\n$$\ng_t = (W^\\top) g_{t+1} = (W^\\top)^2 g_{t+2} = \\dots = (W^\\top)^{T-t} g_T\n$$\n因此，梯度范数 $\\lVert g_t \\rVert_2$ 随时间的稳定性取决于矩阵幂 $(W^\\top)^k$ 的性质。我们现在分析 $W$ 的两种指定选择。\n\n#### 情况 1：正交矩阵 ($W = W_{\\mathrm{orth}}$)\n正交矩阵 $W_{\\mathrm{orth}}$ 由属性 $W_{\\mathrm{orth}}^\\top W_{\\mathrm{orth}} = I$ 定义，其中 $I$ 是单位矩阵。这也意味着它的转置 $W_{\\mathrm{orth}}^\\top$ 也是正交的。正交矩阵的一个基本性质是它们相对于欧几里得范数是等距映射；它们保持向量的长度。对于任何向量 $v \\in \\mathbb{R}^n$：\n$$\n\\lVert W_{\\mathrm{orth}}^\\top v \\rVert_2 = \\sqrt{(W_{\\mathrm{orth}}^\\top v)^\\top (W_{\\mathrm{orth}}^\\top v)} = \\sqrt{v^\\top W_{\\mathrm{orth}} W_{\\mathrm{orth}}^\\top v} = \\sqrt{v^\\top I v} = \\sqrt{v^\\top v} = \\lVert v \\rVert_2\n$$\n将此性质应用于我们的梯度递推关系：\n$$\n\\lVert g_t \\rVert_2 = \\lVert W_{\\mathrm{orth}}^\\top g_{t+1} \\rVert_2 = \\lVert g_{t+1} \\rVert_2\n$$\n这表明在反向传播的每一步中，梯度的范数都完全保持不变。通过归纳法可知，对于所有 $t \\in \\{0, 1, \\dots, T\\}$，都有 $\\lVert g_t \\rVert_2 = \\lVert g_T \\rVert_2$。因此，最大和最小梯度范数是相同的：\n$$\n\\max_{0 \\le t \\le T} \\lVert g_t \\rVert_2 = \\min_{0 \\le t \\le T} \\lVert g_t \\rVert_2 = \\lVert g_T \\rVert_2\n$$\n因此，正交情况下的稳定性度量为：\n$$\nS(W_{\\mathrm{orth}}) = \\frac{\\max_{t} \\lVert g_t \\rVert_2}{\\min_{t} \\lVert g_t \\rVert_2} = 1\n$$\n（在数值实现中，由于浮点精度限制，这个值会非常接近 $1$）。这代表了理想的梯度稳定性，信息在时间上向后流动而没有放大或衰减。\n\n#### 情况 2：缩放的高斯矩阵 ($W = W_{\\mathrm{gauss}}$)\n该矩阵由一个原始矩阵 $W_{\\mathrm{raw}}$ 生成，其条目从标准正态分布中独立同分布地抽取，然后进行缩放，使其算子范数为指定值 $\\alpha > 0$。算子范数是最大奇异值 $\\sigma_{\\max}$，定义为 $\\lVert W \\rVert_2 = \\max_{v \\neq 0} \\frac{\\lVert Wv \\rVert_2}{\\lVert v \\rVert_2}$。矩阵及其转置的算子范数相等：$\\lVert W_{\\mathrm{gauss}}^\\top \\rVert_2 = \\lVert W_{\\mathrm{gauss}} \\rVert_2 = \\alpha$。\n\n从梯度递推关系中，我们可以对范数进行限定：\n$$\n\\lVert g_t \\rVert_2 = \\lVert W_{\\mathrm{gauss}}^\\top g_{t+1} \\rVert_2 \\le \\lVert W_{\\mathrm{gauss}}^\\top \\rVert_2 \\lVert g_{t+1} \\rVert_2 = \\alpha \\lVert g_{t+1} \\rVert_2\n$$\n与正交矩阵不同，一般矩阵不作为等距映射。它会将其顶部奇异向量方向上的向量拉伸一个因子 $\\alpha$，并按其其他奇异值对应的因子来收缩或拉伸其他方向的向量。由于向量 $g_t$ 在每一步都不太可能与 $W_{\\mathrm{gauss}}^\\top$ 的顶部奇异向量完全对齐，因此它们的范数通常会发生变化。\n*   如果 $\\alpha  1$，范数在每一步都受一个收缩因子的限制，导致**梯度消失**（即，当 $t$ 从 $T$ 减小到 $0$ 时，$\\lVert g_t \\rVert_2$ 呈指数衰减）。\n*   如果 $\\alpha > 1$，范数在每一步最多可以增长一个因子 $\\alpha$，导致**梯度爆炸**（即，$\\lVert g_t \\rVert_2$ 可能呈指数增长）。\n*   如果 $\\alpha = 1$，虽然最大可能的拉伸是 $1$，但大多数向量都会被收缩，因为它们不与顶部奇异方向对齐。范数可能会波动，并平均而言会衰减。\n\n在所有子情况下，梯度范数 $\\lVert g_t \\rVert_2$ 都会随时间变化，这意味着 $\\max_{t} \\lVert g_t \\rVert_2$ 将不等于 $\\min_{t} \\lVert g_t \\rVert_2$。因此，我们应该预期：\n$$\nS(W_{\\mathrm{gauss}}) > 1\n$$\n\n### 分析结论与算法\n理论分析预测 $S(W_{\\mathrm{orth}}) \\approx 1$ 和 $S(W_{\\mathrm{gauss}}) > 1$。这意味着条件 $S(W_{\\mathrm{orth}}) \\le S(W_{\\mathrm{gauss}})$ 对所有提供的测试用例都应为真。实现将通过数值方法验证这一假设。\n\n每个测试用例的算法流程如下：\n1.  设置伪随机种子以保证可复现性。\n2.  通过从标准正态分布中抽样并将其归一化，生成初始状态向量 $x_0 \\in \\mathbb{R}^n$，使其欧几里得范数为单位范数，即 $\\lVert x_0 \\rVert_2 = 1$。\n3.  **正交矩阵路径**：\n    a. 通过对一个 $n \\times n$ 的标准正态随机变量矩阵执行 QR 分解来生成 $W_{\\mathrm{orth}}$。\n    b. 向前模拟系统 $T$ 步：$x_{t+1} = W_{\\mathrm{orth}} x_t$，以获得最终到 $x_T$ 的状态序列。\n    c. 向后传播梯度 $T$ 步：从 $g_T = x_T$ 开始，计算 $g_t = W_{\\mathrm{orth}}^\\top g_{t+1}$，直到 $t=0$。\n    d. 对每个梯度向量 $g_t$（$t \\in \\{0, \\dots, T\\}$）计算欧几里得范数 $\\lVert g_t \\rVert_2$。\n    e. 计算 $S(W_{\\mathrm{orth}})$，即这些范数的最大值与最小值之比。\n4.  **高斯矩阵路径**：\n    a. 通过创建一个 $n \\times n$ 的标准正态随机变量矩阵，找到其最大奇异值 $\\sigma_{\\max}$，并将该矩阵按 $\\alpha / \\sigma_{\\max}$ 进行缩放来生成 $W_{\\mathrm{gauss}}$。\n    b. 使用 $W_{\\mathrm{gauss}}$ 和相同的初始状态 $x_0$ 重复步骤 3b-3d。\n    c. 计算 $S(W_{\\mathrm{gauss}})$。\n5.  比较两个稳定性度量，并记录 $S(W_{\\mathrm{orth}}) \\le S(W_{\\mathrm{gauss}})$ 的布尔结果。\n最终输出将是所有测试用例的这些布尔结果的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import qr, svd\n\ndef generate_orthogonal_matrix(n, rng):\n    \"\"\"\n    Generates an n x n orthogonal matrix from the QR decomposition\n    of a random Gaussian matrix.\n    \"\"\"\n    random_matrix = rng.standard_normal(size=(n, n))\n    q_matrix, _ = qr(random_matrix)\n    return q_matrix\n\ndef generate_scaled_gaussian_matrix(n, alpha, rng):\n    \"\"\"\n    Generates an n x n scaled Gaussian matrix with a specified operator norm.\n    \"\"\"\n    if n == 1:\n        # For n=1, a random scalar's singular value is its absolute value.\n        # Scaling it by alpha/|w| makes the new 'matrix' [alpha] or [-alpha].\n        w_raw = rng.standard_normal(size=(1, 1))\n        # Handle the edge case of w_raw being zero\n        if np.abs(w_raw[0, 0])  1e-15:\n             w_raw[0, 0] = 1.0\n        sigma_max = np.abs(w_raw[0, 0])\n        return alpha * w_raw / sigma_max\n\n    w_raw = rng.standard_normal(size=(n, n))\n    # 'svd' returns singular values in descending order.\n    # We only need the values, not the vectors, so compute_uv=False.\n    singular_values = svd(w_raw, compute_uv=False)\n    sigma_max = singular_values[0]\n    return alpha * w_raw / sigma_max\n    \ndef compute_dynamics_and_gradients(W, x0, T):\n    \"\"\"\n    Computes the forward states and backward gradients for a given system.\n    \"\"\"\n    n = x0.shape[0]\n    # Forward pass to compute states x_0, ..., x_T\n    states = np.zeros((T + 1, n))\n    states[0] = x0\n    for t in range(T):\n        states[t + 1] = W @ states[t]\n\n    # Backward pass to compute gradients g_0, ..., g_T\n    gradients = np.zeros((T + 1, n))\n    gradients[T] = states[T]  # g_T = x_T\n    for t in range(T - 1, -1, -1):\n        gradients[t] = W.T @ gradients[t + 1]\n\n    return gradients\n\ndef calculate_stability_metric(gradients):\n    \"\"\"\n    Calculates the stability metric S(W) from a sequence of gradients.\n    \"\"\"\n    gradient_norms = np.linalg.norm(gradients, axis=1)\n    # To avoid division by zero in case a norm is numerically zero\n    min_norm = np.min(gradient_norms)\n    if min_norm  1e-15:\n        return np.finfo(float).max # Return a large number for an unstable case\n    \n    max_norm = np.max(gradient_norms)\n    return max_norm / min_norm\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases.\n    \"\"\"\n    # Test cases: (n, T, alpha)\n    test_cases = [\n        (64, 100, 0.9),\n        (64, 100, 1.0),\n        (64, 60, 1.1),\n        (1, 100, 0.99),\n    ]\n\n    results = []\n    \n    for k, (n, T, alpha) in enumerate(test_cases):\n        # Set the seed for reproducibility for each test case\n        seed = 12345 + 97 * k\n        rng = np.random.default_rng(seed)\n\n        # Generate initial state x0 with unit norm\n        x0_raw = rng.standard_normal(size=n)\n        x0 = x0_raw / np.linalg.norm(x0_raw)\n\n        # --- Orthogonal Matrix Path ---\n        W_orth = generate_orthogonal_matrix(n, rng)\n        gradients_orth = compute_dynamics_and_gradients(W_orth, x0, T)\n        S_orth = calculate_stability_metric(gradients_orth)\n\n        # --- Scaled Gaussian Matrix Path ---\n        # Note: A new random matrix must be drawn for W_gauss.\n        # The same rng instance ensures the sequence of random numbers is correct.\n        W_gauss = generate_scaled_gaussian_matrix(n, alpha, rng)\n        gradients_gauss = compute_dynamics_and_gradients(W_gauss, x0, T)\n        S_gauss = calculate_stability_metric(gradients_gauss)\n        \n        # Compare stability and store the result\n        results.append(S_orth = S_gauss)\n\n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现在，我们转向一个更具生物学真实性的模型，它是计算神经科学的基石：一个大规模、随机连接的脉冲神经元循环网络。本练习  要求您实现一个兴奋性-抑制性（$E/I$）网络的模拟，并使用在真实神经回路中观察到的关键统计指标来量化其动态。这种实践经验是脑功能建模的基础，让您能够探索网络结构和神经元特性如何共同产生复杂的、平衡的活动模式。",
            "id": "4001260",
            "problem": "考虑一个在脑建模和计算神经科学领域中的网络模型，该模型由两个相互作用的神经元群体组成。第一个群体是兴奋性的 (E)，第二个是抑制性的 (I)。每个神经元都建模为一个漏泄整合发放 (LIF) 单元。该模型由以下基本定义指定。\n\n每个神经元的膜电位 $V(t)$ 根据 LIF 动力学演化\n$$\n\\frac{dV(t)}{dt} = \\frac{-(V(t) - V_{\\mathrm{rest}}) + R_m I_{\\mathrm{syn}}(t)}{\\tau_m},\n$$\n其中 $V_{\\mathrm{rest}}$ 是静息电位，$R_m$ 是膜电阻，$\\tau_m$ 是膜时间常数。当 $V(t)$ 达到阈值 $V_{\\mathrm{th}}$ 时，神经元发放一个脉冲，其电位被重置为 $V_{\\mathrm{reset}}$，并在一个固定的不应期时间 $\\tau_{\\mathrm{ref}}$ 内保持不应期状态，在此期间其电位被钳制在 $V_{\\mathrm{reset}}$。\n\n到达一个突触后神经元的突触输入电流 $I_{\\mathrm{syn}}(t)$ 是三个分量的总和：兴奋性循环输入、抑制性循环输入以及一个外部前馈输入。令 $s_{\\mathrm{E}}(t)$ 和 $s_{\\mathrm{I}}(t)$ 表示突触门控变量，它们分别代表通过兴奋性和抑制性突触到达的突触前脉冲的效果。这些门控变量遵循由突触前脉冲驱动的一阶指数衰减动力学，\n$$\n\\frac{ds_{\\mathrm{E}}(t)}{dt} = -\\frac{s_{\\mathrm{E}}(t)}{\\tau_s} + \\sum_k \\delta(t - t_k^{(\\mathrm{E})}), \\quad\n\\frac{ds_{\\mathrm{I}}(t)}{dt} = -\\frac{s_{\\mathrm{I}}(t)}{\\tau_s} + \\sum_k \\delta(t - t_k^{(\\mathrm{I})}),\n$$\n其中 $\\tau_s$ 是突触时间常数，$t_k^{(\\mathrm{E})}$ 和 $t_k^{(\\mathrm{I})}$ 分别是来自兴奋性和抑制性神经元的突触前脉冲时间。外部前馈输入被建模为每个神经元的独立泊松脉冲序列，其速率为 $r_{\\mathrm{ext}}$ (单位赫兹)，通过 $K_{\\mathrm{ext}}$ 个相同的外部突触到达，由一个门控变量 $s_{\\mathrm{ext}}(t)$ 表示，该变量遵循相同的指数衰减，并在每个时间步长内按外部脉冲的数量增加。总突触电流为\n$$\nI_{\\mathrm{syn}}(t) = w_{\\mathrm{E}} s_{\\mathrm{E}}(t) - w_{\\mathrm{I}} s_{\\mathrm{I}}(t) + w_{\\mathrm{ext}} s_{\\mathrm{ext}}(t),\n$$\n其中 $w_{\\mathrm{E}} > 0$ 是兴奋性突触权重，$w_{\\mathrm{I}} > 0$ 是以负号进入的抑制性突触权重的大小，$w_{\\mathrm{ext}} > 0$ 是外部突触权重。\n\n群体内部和跨群体的连接性是随机和循环的。令 $p_{\\mathrm{EE}}$ 表示从一个兴奋性突触前神经元到一个兴奋性突触后神经元的连接概率，$p_{\\mathrm{EI}}$ 表示兴奋性到抑制性，$p_{\\mathrm{IE}}$ 表示抑制性到兴奋性，$p_{\\mathrm{II}}$ 表示抑制性到抑制性。连接是使用这些概率通过伯努利试验独立生成的，突触增量是通过现有连接到达的突触前脉冲的计数。\n\n您必须使用显式欧拉离散化实现一个模拟器，时间步长为 $dt$ (秒)，总模拟时间为 $T$ (秒)。在每个时间步，通过指数衰减更新门控变量，并根据通过现有连接到达的突Synapse前脉冲数量增加它们。根据 LIF 方程更新非不应期神经元的膜电位。记录脉冲时间。\n\n按如下方式定义定量平衡度量，重点关注兴奋性群体：\n- 到达兴奋性群体的平均兴奋性和抑制性循环输入电流为\n$$\n\\langle I_{\\mathrm{E}}^{\\mathrm{exc}} \\rangle = \\frac{1}{N_{\\mathrm{E}} N_t} \\sum_{j=1}^{N_{\\mathrm{E}}} \\sum_{n=1}^{N_t} w_{\\mathrm{E}} s_{\\mathrm{E},j}[n], \\quad\n\\langle I_{\\mathrm{E}}^{\\mathrm{inh}} \\rangle = \\frac{1}{N_{\\mathrm{E}} N_t} \\sum_{j=1}^{N_{\\mathrm{E}}} \\sum_{n=1}^{N_t} w_{\\mathrm{I}} s_{\\mathrm{I},j}[n],\n$$\n其中 $N_{\\mathrm{E}}$ 是兴奋性神经元的数量，$N_t$ 是时间步的数量，$s_{\\mathrm{E},j}[n]$ 和 $s_{\\mathrm{I},j}[n]$ 分别是兴奋性神经元 $j$ 在时间步 $n$ 的兴奋性和抑制性循环门控变量。兴奋性群体的兴奋性-抑制性平衡比为\n$$\n\\rho_{\\mathrm{E}} = \\frac{\\langle I_{\\mathrm{E}}^{\\mathrm{inh}} \\rangle}{\\max\\{\\langle I_{\\mathrm{E}}^{\\mathrm{exc}} \\rangle, \\varepsilon\\}},\n$$\n其中包含一个小的正常数 $\\varepsilon$ 以避免除以零。取 $\\varepsilon = 10^{-12}$。\n\n- 兴奋性群体的平均发放率为\n$$\n\\bar{r}_{\\mathrm{E}} = \\frac{1}{N_{\\mathrm{E}}} \\sum_{j=1}^{N_{\\mathrm{E}}} \\frac{N_{\\mathrm{spk},j}}{T},\n$$\n其中 $N_{\\mathrm{spk},j}$ 是兴奋性神经元 $j$ 在模拟期间发放的脉冲数。将 $\\bar{r}_{\\mathrm{E}}$ 以赫兹 (Hz) 表示并作为浮点数输出。\n\n- 兴奋性群体脉冲间隔 (ISIs) 的变异系数 (CV) 是通过汇总所有至少有两个脉冲的兴奋性神经元的 ISIs 来计算的，\n$$\n\\mathrm{CV}_{\\mathrm{E}} = \\frac{\\sigma_{\\mathrm{ISI}}}{\\mu_{\\mathrm{ISI}}},\n$$\n其中 $\\sigma_{\\mathrm{ISI}}$ 和 $\\mu_{\\mathrm{ISI}}$ 分别是汇总的 ISIs (单位为秒) 的标准差和均值。如果没有 ISI，则定义 $\\mathrm{CV}_{\\mathrm{E}} = 0.0$。\n\n- 兴奋性群体脉冲计数的法诺因子是在长度为 $W$ (秒) 的不重叠时间窗口上计算的。令 $C_k$ 为窗口 $k$ 内的总兴奋性脉冲数，则\n$$\n\\mathrm{FF}_{\\mathrm{E}} = \\frac{\\mathrm{Var}(C_k)}{\\mathrm{E}[C_k]}.\n$$\n如果 $\\mathrm{E}[C_k] = 0$，则定义 $\\mathrm{FF}_{\\mathrm{E}} = 0.0$。使用 $W = 0.05$ 秒。\n\n将平衡的二元指示器定义为\n$$\nB_{\\mathrm{E}} = \\begin{cases}\n\\mathrm{True},  \\text{如果 } |\\rho_{\\mathrm{E}} - 1|  \\delta, \\\\\n\\mathrm{False},  \\text{否则},\n\\end{cases}\n$$\n容差为 $\\delta = 0.25$。\n\n为以下参数集测试套件实现模拟器并计算度量。对于每种情况，使用指定的随机种子以保证可复现性。所有情况的通用模拟参数为 $dt = 0.0001$ 秒，$T = 1.0$ 秒，$V_{\\mathrm{rest}} = 0.0$，$V_{\\mathrm{th}} = 1.0$，$V_{\\mathrm{reset}} = 0.0$，$R_m = 1.0$，$\\tau_m = 0.02$ 秒，$\\tau_s = 0.005$ 秒，$\\tau_{\\mathrm{ref}} = 0.002$ 秒，以及 $W = 0.05$ 秒。\n\n- 情况 $1$ (标称平衡的循环 E/I 与中等强度的前馈)：$N_{\\mathrm{E}} = 80$, $N_{\\mathrm{I}} = 20$, $p_{\\mathrm{EE}} = 0.10$, $p_{\\mathrm{EI}} = 0.10$, $p_{\\mathrm{IE}} = 0.10$, $p_{\\mathrm{II}} = 0.10$, $w_{\\mathrm{E}} = 0.020$, $w_{\\mathrm{I}} = 0.080$, $w_{\\mathrm{ext}} = 0.015$, $K_{\\mathrm{ext}} = 100$, $r_{\\mathrm{ext}} = 8.0$ Hz, seed $= 12345$。\n\n- 情况 $2$ (强抑制)：$N_{\\mathrm{E}} = 80$, $N_{\\mathrm{I}} = 20$, $p_{\\mathrm{EE}} = 0.10$, $p_{\\mathrm{EI}} = 0.10$, $p_{\\mathrm{IE}} = 0.10$, $p_{\\mathrm{II}} = 0.10$, $w_{\\mathrm{E}} = 0.020$, $w_{\\mathrm{I}} = 0.120$, $w_{\\mathrm{ext}} = 0.015$, $K_{\\mathrm{ext}} = 100$, $r_{\\mathrm{ext}} = 8.0$ Hz, seed $= 23456$。\n\n- 情况 $3$ (无循环连接，仅前馈)：$N_{\\mathrm{E}} = 80$, $N_{\\mathrm{I}} = 20$, $p_{\\mathrm{EE}} = 0.00$, $p_{\\mathrm{EI}} = 0.00$, $p_{\\mathrm{IE}} = 0.00$, $p_{\\mathrm{II}} = 0.00$, $w_{\\mathrm{E}} = 0.020$, $w_{\\mathrm{I}} = 0.080$, $w_{\\mathrm{ext}} = 0.020$, $K_{\\mathrm{ext}} = 150$, $r_{\\mathrm{ext}} = 15.0$ Hz, seed $= 34567$。\n\n- 情况 $4$ (过度兴奋与相对较弱的抑制)：$N_{\\mathrm{E}} = 80$, $N_{\\mathrm{I}} = 20$, $p_{\\mathrm{EE}} = 0.12$, $p_{\\mathrm{EI}} = 0.12$, $p_{\\mathrm{IE}} = 0.08$, $p_{\\mathrm{II}} = 0.08$, $w_{\\mathrm{E}} = 0.040$, $w_{\\mathrm{I}} = 0.060$, $w_{\\mathrm{ext}} = 0.012$, $K_{\\mathrm{ext}} = 80$, $r_{\\mathrm{ext}} = 12.0$ Hz, seed $= 45678$。\n\n您的程序应生成单行输出，其中包含一个结果列表，每个测试用例一个列表，每个用例的度量顺序如下：$[\\rho_{\\mathrm{E}}, \\bar{r}_{\\mathrm{E}}, \\mathrm{CV}_{\\mathrm{E}}, \\mathrm{FF}_{\\mathrm{E}}, B_{\\mathrm{E}}]$。总输出必须是这些逐例列表的单个顶级列表，用方括号括起来并用逗号分隔，例如 $[[\\ldots],[\\ldots],[\\ldots],[\\ldots]]$。所有浮点值都是无单位的数字，除了 $\\bar{r}_{\\mathrm{E}}$，它必须以赫兹 (Hz) 计算并作为浮点数输出。不应打印其他任何文本。",
            "solution": "该问题提出了一个明确定义的任务：模拟一个由兴奋性和抑制性漏泄整合发放 (LIF) 神经元组成的循环网络，并计算一组指定的定量度量。该模型是计算神经科学中皮层微环路动力学的标准表示。\n\n### **问题验证**\n\n**步骤 1: 提取的已知条件**\n- **神经元模型:** 漏泄整合发放 (LIF)，其动力学为 $\\frac{dV(t)}{dt} = \\frac{-(V(t) - V_{\\mathrm{rest}}) + R_m I_{\\mathrm{syn}}(t)}{\\tau_m}$。当 $V(t) \\geq V_{\\mathrm{th}}$ 时发生脉冲发放，随后重置为 $V_{\\mathrm{reset}}$ 并进入不应期 $\\tau_{\\mathrm{ref}}$。\n- **突触模型:** 用于门控变量 $s_{\\mathrm{E}}(t)$、$s_{\\mathrm{I}}(t)$ 和 $s_{\\mathrm{ext}}(t)$ 的一阶动力学模型，时间常数为 $\\tau_s$，由突触前和外部脉冲驱动。总电流为 $I_{\\mathrm{syn}}(t) = w_{\\mathrm{E}} s_{\\mathrm{E}}(t) - w_{\\mathrm{I}} s_{\\mathrm{I}}(t) + w_{\\mathrm{ext}} s_{\\mathrm{ext}}(t)$。\n- **网络结构:** 两个群体，兴奋性 (E) 和抑制性 (I)，具有由概率 $p_{\\mathrm{EE}}$、$p_{\\mathrm{EI}}$、$p_{\\mathrm{IE}}$、$p_{\\mathrm{II}}$ 定义的随机循环连接。\n- **外部输入:** 每个神经元独立的泊松脉冲序列，速率为 $r_{\\mathrm{ext}}$，通过 $K_{\\mathrm{ext}}$ 个突触。\n- **模拟参数:** 显式欧拉方法，时间步长 $dt$，总时间 $T$。\n- **度量:** 平衡比 $\\rho_{\\mathrm{E}}$、平均兴奋性发放率 $\\bar{r}_{\\mathrm{E}}$、脉冲间隔的变异系数 $\\mathrm{CV}_{\\mathrm{E}}$、脉冲计数的法诺因子 $\\mathrm{FF}_{\\mathrm{E}}$ 和一个二元平衡指示器 $B_{\\mathrm{E}}$。\n- **常量:** $V_{\\mathrm{rest}} = 0.0$，$V_{\\mathrm{th}} = 1.0$，$V_{\\mathrm{reset}} = 0.0$，$R_m = 1.0$，$\\tau_m = 0.02$ s，$\\tau_s = 0.005$ s，$\\tau_{\\mathrm{ref}} = 0.002$ s，$dt = 0.0001$ s，$T = 1.0$ s，$W=0.05$ s，$\\varepsilon = 10^{-12}$，$\\delta = 0.25$。\n- **测试用例:** 提供了四个不同的参数集，每个都有一个唯一的随机种子以保证可复现性。\n\n**步骤 2: 使用提取的已知条件进行验证**\n- **具有科学依据:** 该问题牢固地植根于既定的计算神经科学原理。LIF 神经元模型和基于电流的突触相互作用模型是研究网络动力学的经典选择。\n- **良构的:** 该问题通过一组清晰的微分方程、一个数值积分方案（显式欧拉）、特定的参数和初始条件（隐含地，神经元处于静息状态）进行了规定。使用随机种子确保了随机元素（连接性、外部输入）是可复现的，从而为每个案例带来唯一且稳定的模拟轨迹。度量被严格定义。\n- **客观的:** 问题是用精确的数学语言和定量描述来表述的，没有任何主观或模棱两可的术语。\n\n**步骤 3: 结论与行动**\n问题陈述是**有效的**。它是自包含的、科学上合理的并且计算上是可行的。我现在将着手设计和呈现解决方案。\n\n### **基于原则的解决方案设计**\n\n该解决方案需要实现一个 LIF 神经元网络的时间步进模拟。算法的核心是根据所提供的离散化动力学方程，迭代更新每个神经元及其突触的状态。\n\n**1. 连续动力学的离散化**\n\n使用时间步长为 $dt$ 的显式欧拉方法对控制系统的连续时间微分方程进行离散化。\n\n- **膜电位 ($V$):** 神经元 $j$ 的 LIF 方程离散化为：\n$$\nV_j[n+1] = V_j[n] + \\frac{dt}{\\tau_m} \\left( -(V_j[n] - V_{\\mathrm{rest}}) + R_m I_{\\mathrm{syn}, j}[n] \\right)\n$$\n其中 $V_j[n]$ 是时间步 $n$ 的电位，$I_{\\mathrm{syn}, j}[n]$ 是该步骤中到达神经元 $j$ 的总突触电流。鉴于 $V_{\\mathrm{rest}} = 0.0$ 和 $R_m=1.0$，这简化为：\n$$\nV_j[n+1] = V_j[n] \\left(1 - \\frac{dt}{\\tau_m}\\right) + \\frac{dt}{\\tau_m} I_{\\mathrm{syn}, j}[n]\n$$\n此更新仅适用于非不应期神经元。对于处于不应期 $\\tau_{\\mathrm{ref}}$ 的神经元，其电位被钳制在 $V_{\\mathrm{reset}}$。\n\n- **突触门控变量 ($s$):** 突触门控变量的动力学也被离散化。方程 $\\frac{ds(t)}{dt} = -\\frac{s(t)}{\\tau_s} + \\sum_k \\delta(t - t_k)$ 意味着一个衰减过程与脉冲到达时的瞬时增量相结合。在离散时间中，这在每个时间步 $n$ 实现为一个两步过程：首先是衰减，然后是加上在步骤 $n$ 发生的新脉冲。\n$$\ns_j[n+1] = s_j[n] \\left(1 - \\frac{dt}{\\tau_s}\\right) + N_{\\mathrm{spikes}}\n$$\n其中 $N_{\\mathrm{spikes}}$ 是在时间步 $n$ 到达神经元 $j$ 突触的突触前脉冲总数。这适用于每个神经元的 $s_{\\mathrm{E}}$、$s_{\\mathrm{I}}$ 和 $s_{\\mathrm{ext}}$。\n\n**2. 模拟算法**\n\n模拟在总共 $N_t = T/dt$ 个时间步上迭代进行。\n\n- **初始化:**\n    1. 为可复现性设置随机数生成器的种子。\n    2. 为所有神经元 ($N = N_{\\mathrm{E}} + N_{\\mathrm{I}}$) 创建状态数组：膜电位 $V$（初始化为 $V_{\\mathrm{rest}}$）、突触门控变量 $s_{\\mathrm{E}}, s_{\\mathrm{I}}, s_{\\mathrm{ext}}$（初始化为 $0$）和一个不应期计数器（初始化为 $0$）。\n    3. 四个循环连接矩阵（$C_{\\mathrm{EE}}$、$C_{\\mathrm{EI}}$、$C_{\\mathrm{IE}}$、$C_{\\mathrm{II}}$）使用各自的概率通过伯努利试验生成为二元矩阵。\n    4. 初始化用于记录脉冲和突触变量历史的数据结构。\n\n- **主模拟循环 (对于每个时间步 $n$):**\n    1. **突触电流计算:** 使用当前步骤 $n$ 的门控变量为每个神经元 $j$ 计算总突触电流 $I_{\\mathrm{syn}, j}[n]$：\n       $$ I_{\\mathrm{syn}, j}[n] = w_{\\mathrm{E}} s_{\\mathrm{E}, j}[n] - w_{\\mathrm{I}} s_{\\mathrm{I}, j}[n] + w_{\\mathrm{ext}} s_{\\mathrm{ext}, j}[n] $$\n    2. **膜电位更新:** 使用离散化的 LIF 方程更新所有非不应期神经元的电位 $V_j[n+1]$。递减所有神经元的不应期计数器。将仍处于不应期的神经元的电位钳制在 $V_{\\mathrm{reset}}$。\n    3. **脉冲检测和重置:** 识别所有满足 $V_j[n+1] \\geq V_{\\mathrm{th}}$ 的神经元 $j$。对于这些神经元，在时间 $(n+1)dt$ 记录一个脉冲，将其电位 $V_j[n+1]$ 重置为 $V_{\\mathrm{reset}}$，并将其不应期计数器设置为 $\\tau_{\\mathrm{ref}}/dt$。\n    4. **突触输入更新:**\n        - **外部输入:** 对于每个神经元，传入的外部脉冲数从一个均值为 $\\lambda = K_{\\mathrm{ext}} r_{\\mathrm{ext}} dt$ 的泊松分布中抽取。\n        - **循环输入:** 使用连接矩阵和刚刚发生的脉冲向量，计算到达每个神经元的循环兴奋性和抑制性脉冲数。\n        - **门控变量更新:** 使用离生的衰减-增量规则更新所有神经元的突触门控变量 $s_{\\mathrm{E}, j}[n+1]$、$s_{\\mathrm{I}, j}[n+1]$ 和 $s_{\\mathrm{ext}, j}[n+1]$。\n\n**3. 模拟后度量计算**\n\n模拟完成后，使用记录的数据计算兴奋性群体所需的五个度量。\n\n- **$\\rho_{\\mathrm{E}}$ (平衡比):** 通过对所有兴奋性神经元 $j$ 和所有时间步 $n$ 上的 $w_{\\mathrm{E}}s_{\\mathrm{E},j}[n]$ 和 $w_{\\mathrm{I}}s_{\\mathrm{I},j}[n]$ 的存储时间序列进行平均，计算平均兴奋性和抑制性电流 $\\langle I_{\\mathrm{E}}^{\\mathrm{exc}} \\rangle$ 和 $\\langle I_{\\mathrm{E}}^{\\mathrm{inh}} \\rangle$。然后计算比率 $\\rho_{\\mathrm{E}} = \\langle I_{\\mathrm{E}}^{\\mathrm{inh}} \\rangle / \\max(\\langle I_{\\mathrm{E}}^{\\mathrm{exc}} \\rangle, \\varepsilon)$。\n- **$\\bar{r}_{\\mathrm{E}}$ (平均发放率):** 将每个兴奋性神经元的总脉冲数 $N_{\\mathrm{spk},j}$ 除以总模拟时间 $T$ 得到其发放率。然后在整个兴奋性群体中对这些率进行平均。\n- **$\\mathrm{CV}_{\\mathrm{E}}$ (变异系数):** 对于每个至少有两个脉冲的兴奋性神经元，脉冲间隔 (ISIs) 计算为连续脉冲时间之间的差异。所有这些 ISIs 被汇集到一个数据集中。CV 是该数据集的标准差除以其均值。如果总共可用的 ISI 少于两个，则 $\\mathrm{CV}_{\\mathrm{E}}$ 为 $0.0$。\n- **$\\mathrm{FF}_{\\mathrm{E}}$ (法诺因子):** 模拟持续时间 $T$ 被划分为宽度为 $W$ 的不重叠窗口。在每个窗口内计算来自兴奋性群体的总脉冲数，产生一个计数序列 $C_k$。法诺因子是这些计数的方差除以它们的均值。如果均值为 $0$，则 $\\mathrm{FF}_{\\mathrm{E}}$ 为 $0.0$。\n- **$B_{\\mathrm{E}}$ (平衡指示器):** 一个布尔值，指示平衡比 $\\rho_{\\mathrm{E}}$ 是否接近 $1$，即 $|\\rho_{\\mathrm{E}} - 1|  \\delta$。\n\n这种结构化方法确保了问题陈述的所有方面都得到正确和高效地处理，从而为每个测试用例得到可验证和可复现的结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation_case(params):\n    \"\"\"\n    Runs a single simulation case for the E/I network model.\n    \"\"\"\n    # Unpack common and case-specific parameters\n    dt = params['common']['dt']\n    T = params['common']['T']\n    V_rest = params['common']['V_rest']\n    V_th = params['common']['V_th']\n    V_reset = params['common']['V_reset']\n    R_m = params['common']['R_m']\n    tau_m = params['common']['tau_m']\n    tau_s = params['common']['tau_s']\n    tau_ref = params['common']['tau_ref']\n    W_fano = params['common']['W']\n    epsilon = params['common']['epsilon']\n    delta_balance = params['common']['delta_balance']\n\n    N_E = params['case']['N_E']\n    N_I = params['case']['N_I']\n    p_EE = params['case']['p_EE']\n    p_EI = params['case']['p_EI']\n    p_IE = params['case']['p_IE']\n    p_II = params['case']['p_II']\n    w_E = params['case']['w_E']\n    w_I = params['case']['w_I']\n    w_ext = params['case']['w_ext']\n    K_ext = params['case']['K_ext']\n    r_ext = params['case']['r_ext']\n    seed = params['case']['seed']\n\n    # Initialize random number generator\n    rng = np.random.default_rng(seed)\n\n    # Simulation setup\n    N_t = int(T / dt)\n    N = N_E + N_I\n    ref_steps = int(tau_ref / dt)\n\n    # State variables\n    V = np.full(N, V_rest)\n    s_E = np.zeros(N)\n    s_I = np.zeros(N)\n    s_ext = np.zeros(N)\n    ref_countdown = np.zeros(N, dtype=int)\n    \n    # Data recorders\n    spike_times = [[] for _ in range(N)]\n    s_E_history_E_pop = np.zeros((N_E, N_t))\n    s_I_history_E_pop = np.zeros((N_E, N_t))\n\n    # Connectivity matrices\n    C_EE = (rng.random((N_E, N_E))  p_EE).astype(float)\n    C_EI = (rng.random((N_I, N_E))  p_EI).astype(float) # from E to I\n    C_IE = (rng.random((N_E, N_I))  p_IE).astype(float) # from I to E\n    C_II = (rng.random((N_I, N_I))  p_II).astype(float)\n    np.fill_diagonal(C_EE, 0) # No self-connections\n    np.fill_diagonal(C_II, 0)\n\n    # Pre-calculated constants for Euler updates\n    decay_m = 1.0 - dt / tau_m\n    decay_s = 1.0 - dt / tau_s\n    update_V_const_m = dt / tau_m\n    \n    # External input rate per time step\n    lambda_ext = K_ext * r_ext * dt\n    \n    # Main simulation loop\n    for n in range(N_t):\n        s_E_history_E_pop[:, n] = s_E[:N_E]\n        s_I_history_E_pop[:, n] = s_I[:N_E]\n        \n        I_syn = w_E * s_E - w_I * s_I + w_ext * s_ext\n\n        non_ref_mask = (ref_countdown == 0)\n        V[non_ref_mask] = V[non_ref_mask] * decay_m + update_V_const_m * (I_syn[non_ref_mask] * R_m - (V[non_ref_mask] - V_rest))\n        \n        ref_countdown[ref_countdown > 0] -= 1\n        V[ref_countdown > 0] = V_reset\n        \n        spiked_mask = (V >= V_th)\n        spike_indices = np.where(spiked_mask)[0]\n        if spike_indices.size > 0:\n            for idx in spike_indices:\n                spike_times[idx].append(n * dt)\n            V[spiked_mask] = V_reset\n            ref_countdown[spiked_mask] = ref_steps\n        \n        spikes_E_at_n = spiked_mask[:N_E]\n        spikes_I_at_n = spiked_mask[N_E:]\n        \n        rec_E_input = np.zeros(N)\n        rec_I_input = np.zeros(N)\n        \n        if np.any(spikes_E_at_n):\n            rec_E_input[:N_E] = C_EE.T @ spikes_E_at_n\n            rec_E_input[N_E:] = C_EI.T @ spikes_E_at_n\n        \n        if np.any(spikes_I_at_n):\n            rec_I_input[:N_E] = C_IE.T @ spikes_I_at_n\n            rec_I_input[N_E:] = C_II.T @ spikes_I_at_n\n\n        ext_spikes = rng.poisson(lam=lambda_ext, size=N)\n\n        s_E = s_E * decay_s + rec_E_input\n        s_I = s_I * decay_s + rec_I_input\n        s_ext = s_ext * decay_s + ext_spikes\n\n    # Metrics for E-population\n    mean_I_E_exc_val = w_E * np.mean(s_E_history_E_pop)\n    mean_I_E_inh_val = w_I * np.mean(s_I_history_E_pop)\n    rho_E = mean_I_E_inh_val / max(mean_I_E_exc_val, epsilon)\n    \n    E_spike_counts = np.array([len(st) for st in spike_times[:N_E]])\n    r_E_bar = np.mean(E_spike_counts) / T\n\n    all_is_intervals = []\n    for j in range(N_E):\n        if len(spike_times[j]) >= 2:\n            is_intervals = np.diff(spike_times[j])\n            all_is_intervals.extend(is_intervals)\n    \n    if len(all_is_intervals)  2:\n        CV_E = 0.0\n    else:\n        mu_isi = np.mean(all_is_intervals)\n        sigma_isi = np.std(all_is_intervals)\n        CV_E = sigma_isi / mu_isi if mu_isi > 0 else 0.0\n\n    num_windows = int(T / W_fano)\n    if num_windows > 0:\n        all_E_spikes = np.concatenate([spike_times[j] for j in range(N_E)])\n        window_bins = np.arange(0, T + W_fano, W_fano)\n        if len(window_bins) > 1:\n            counts_per_window, _ = np.histogram(all_E_spikes, bins=window_bins)\n            mean_count = np.mean(counts_per_window)\n            var_count = np.var(counts_per_window)\n            FF_E = var_count / mean_count if mean_count > 0 else 0.0\n        else:\n            FF_E = 0.0\n    else:\n        FF_E = 0.0\n\n    B_E = abs(rho_E - 1)  delta_balance\n\n    return [rho_E, r_E_bar, CV_E, FF_E, B_E]\n\ndef solve():\n    common_params = {\n        'dt': 0.0001, 'T': 1.0, 'V_rest': 0.0, 'V_th': 1.0, 'V_reset': 0.0,\n        'R_m': 1.0, 'tau_m': 0.02, 'tau_s': 0.005, 'tau_ref': 0.002,\n        'W': 0.05, 'epsilon': 1e-12, 'delta_balance': 0.25\n    }\n\n    test_cases = [\n        {'N_E': 80, 'N_I': 20, 'p_EE': 0.10, 'p_EI': 0.10, 'p_IE': 0.10, 'p_II': 0.10,\n         'w_E': 0.020, 'w_I': 0.080, 'w_ext': 0.015, 'K_ext': 100, 'r_ext': 8.0, 'seed': 12345},\n        {'N_E': 80, 'N_I': 20, 'p_EE': 0.10, 'p_EI': 0.10, 'p_IE': 0.10, 'p_II': 0.10,\n         'w_E': 0.020, 'w_I': 0.120, 'w_ext': 0.015, 'K_ext': 100, 'r_ext': 8.0, 'seed': 23456},\n        {'N_E': 80, 'N_I': 20, 'p_EE': 0.00, 'p_EI': 0.00, 'p_IE': 0.00, 'p_II': 0.00,\n         'w_E': 0.020, 'w_I': 0.080, 'w_ext': 0.020, 'K_ext': 150, 'r_ext': 15.0, 'seed': 34567},\n        {'N_E': 80, 'N_I': 20, 'p_EE': 0.12, 'p_EI': 0.12, 'p_IE': 0.08, 'p_II': 0.08,\n         'w_E': 0.040, 'w_I': 0.060, 'w_ext': 0.012, 'K_ext': 80, 'r_ext': 12.0, 'seed': 45678}\n    ]\n\n    results = []\n    for case_params in test_cases:\n        params = {'common': common_params, 'case': case_params}\n        result = run_simulation_case(params)\n        results.append(result)\n\n    formatted_results = [f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}