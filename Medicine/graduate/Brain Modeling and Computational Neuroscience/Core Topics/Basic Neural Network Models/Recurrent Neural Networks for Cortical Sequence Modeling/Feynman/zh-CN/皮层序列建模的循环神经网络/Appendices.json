{
    "hands_on_practices": [
        {
            "introduction": "现代循环神经网络（RNN）的核心在于其复杂的门控机制，它能动态地调节信息流。门控循环单元（GRU）通过其更新门，在先前状态和新的候选状态之间进行插值，从而有选择地保留或更新信息。本练习  提供了一个具体的数值计算，让您亲手实践 GRU 的前向传播步骤，从而直观地理解其隐藏状态是如何作为一种凸组合进行更新的，这是 GRU 稳定性和学习能力的基础。",
            "id": "4013919",
            "problem": "一个用于皮层序列生成的局部回路模型被实现为一个三维门控循环单元 (GRU)。在时间步 $t$，前一个隐藏状态是 $h_{t-1} \\in \\mathbb{R}^{3}$，更新门输出是 $z_{t} \\in (0,1)^{3}$，重置门输出是 $r_{t} \\in (0,1)^{3}$，预计算的候选更新（有时称为候选隐藏状态）是 $\\tilde{h}_{t} \\in (-1,1)^{3}$。您可以假设文献中标准的 GRU 前向方程以及逻辑S型函数和双曲正切函数的常规性质。\n\n考虑以下数值指定的信号：\n- 前一个隐藏状态 $h_{t-1} = \\begin{pmatrix}0.35 \\\\ -0.70 \\\\ 0.20\\end{pmatrix}$。\n- 更新门 $z_{t} = \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix}$。\n- 重置门 $r_{t} = \\begin{pmatrix}0.40 \\\\ 0.90 \\\\ 0.30\\end{pmatrix}$。\n- 候选更新 $\\tilde{h}_{t} = \\begin{pmatrix}0.80 \\\\ -0.10 \\\\ -0.50\\end{pmatrix}$。\n\n任务：\n1. 使用 GRU 的前向定义，根据给定的 $h_{t-1}$、$z_{t}$ 和 $\\tilde{h}_{t}$ 计算下一个隐藏状态 $h_{t}$。\n2. 使用 GRU 更新规则的基本原理以及逻辑S型函数和双曲正切函数的值域性质，验证 $h_{t}$ 的每个坐标是 $h_{t-1}$ 和 $\\tilde{h}_{t}$ 相应坐标的凸组合。提供一个简要的论证，该论证不假设超出这些函数和 GRU 方程的标准定义之外的任何性质。\n3. 令 $\\|h_{t}\\|_{2}$ 表示新隐藏状态的欧几里得范数。计算 $\\|h_{t}\\|_{2}$ 并将您的最终答案表示为一个四舍五入到四位有效数字的实数。不需要单位。",
            "solution": "该问题要求完成与门控循环单元 (GRU) 模型相关的三个任务：计算下一个隐藏状态，验证更新规则的一个性质，以及计算新隐藏状态的欧几里得范数。我将按顺序解决这些任务。\n\n首先，提取给定的数据：\n前一个隐藏状态：$h_{t-1} = \\begin{pmatrix}0.35 \\\\ -0.70 \\\\ 0.20\\end{pmatrix}$。\n更新门输出：$z_{t} = \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix}$。\n重置门输出：$r_{t} = \\begin{pmatrix}0.40 \\\\ 0.90 \\\\ 0.30\\end{pmatrix}$。\n候选更新：$\\tilde{h}_{t} = \\begin{pmatrix}0.80 \\\\ -0.10 \\\\ -0.50\\end{pmatrix}$。\n\n该问题被验证为具有科学依据、问题明确且客观。它基于 GRU 的标准数学公式，GRU 是机器学习和计算建模中广泛使用的组件。所提供的数据是一致的，足以进行所需的计算。对于指定的任务，重置门输出 $r_t$ 的值是不需要的，因为候选更新 $\\tilde{h}_t$ 已经给出。因此，该问题是有效的。\n\n**任务1：计算下一个隐藏状态 $h_{t}$。**\n\n在 GRU 中更新隐藏状态的标准前向方程是：\n$$h_{t} = (1 - z_{t}) \\odot h_{t-1} + z_{t} \\odot \\tilde{h}_{t}$$\n其中 $\\odot$ 表示逐元素（哈达玛）乘积。项 $(1 - z_t)$ 是通过将 $z_t$ 的每个元素从 $1$ 中减去来计算的。\n\n首先，我们计算向量 $(1 - z_t)$：\n$$1 - z_{t} = \\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix} = \\begin{pmatrix}1 - 0.25 \\\\ 1 - 0.60 \\\\ 1 - 0.05\\end{pmatrix} = \\begin{pmatrix}0.75 \\\\ 0.40 \\\\ 0.95\\end{pmatrix}$$\n\n接下来，我们计算更新方程的两项。\n第一项是 $(1 - z_{t}) \\odot h_{t-1}$：\n$$(1 - z_{t}) \\odot h_{t-1} = \\begin{pmatrix}0.75 \\\\ 0.40 \\\\ 0.95\\end{pmatrix} \\odot \\begin{pmatrix}0.35 \\\\ -0.70 \\\\ 0.20\\end{pmatrix} = \\begin{pmatrix}0.75 \\times 0.35 \\\\ 0.40 \\times (-0.70) \\\\ 0.95 \\times 0.20\\end{pmatrix} = \\begin{pmatrix}0.2625 \\\\ -0.28 \\\\ 0.19\\end{pmatrix}$$\n\n第二项是 $z_{t} \\odot \\tilde{h}_{t}$：\n$$z_{t} \\odot \\tilde{h}_{t} = \\begin{pmatrix}0.25 \\\\ 0.60 \\\\ 0.05\\end{pmatrix} \\odot \\begin{pmatrix}0.80 \\\\ -0.10 \\\\ -0.50\\end{pmatrix} = \\begin{pmatrix}0.25 \\times 0.80 \\\\ 0.60 \\times (-0.10) \\\\ 0.05 \\times (-0.50)\\end{pmatrix} = \\begin{pmatrix}0.20 \\\\ -0.06 \\\\ -0.025\\end{pmatrix}$$\n\n最后，我们将这两个结果向量相加得到 $h_{t}$：\n$$h_{t} = \\begin{pmatrix}0.2625 \\\\ -0.28 \\\\ 0.19\\end{pmatrix} + \\begin{pmatrix}0.20 \\\\ -0.06 \\\\ -0.025\\end{pmatrix} = \\begin{pmatrix}0.2625 + 0.20 \\\\ -0.28 - 0.06 \\\\ 0.19 - 0.025\\end{pmatrix} = \\begin{pmatrix}0.4625 \\\\ -0.34 \\\\ 0.165\\end{pmatrix}$$\n\n**任务2：验证 $h_{t}$ 的每个坐标是 $h_{t-1}$ 和 $\\tilde{h}_{t}$ 相应坐标的凸组合。**\n\n如果一个标量 $v$ 可以写成 $v = (1-\\alpha)u + \\alpha w$ 的形式，其中系数 $\\alpha \\in [0, 1]$，那么它就是两个标量 $u$ 和 $w$ 的凸组合。\n\nGRU 更新方程是逐元素操作的。对于任何坐标 $i \\in \\{1, 2, 3\\}$，更新规则是：\n$$(h_{t})_i = (1 - (z_{t})_i)(h_{t-1})_i + (z_{t})_i(\\tilde{h}_{t})_i$$\n让我们定义系数 $\\alpha_i = (z_{t})_i$。第 $i$ 个坐标的方程变为：\n$$(h_{t})_i = (1 - \\alpha_i)(h_{t-1})_i + \\alpha_i(\\tilde{h}_{t})_i$$\n这正是标量 $(h_{t-1})_i$ 和 $(\\tilde{h}_{t})_i$ 的凸组合的形式。\n\n必须满足的条件是 $\\alpha_i \\in [0, 1]$。在问题陈述中，更新门输出 $z_t$ 被给定在 $(0,1)^3$ 范围内。这意味着对于每个坐标 $i$，分量 $(z_t)_i$ 严格介于 $0$ 和 $1$ 之间。区间 $(0,1)$ 是区间 $[0,1]$ 的子集。\n因此，系数 $\\alpha_i = (z_t)_i$ 满足凸组合的要求。这证实了新隐藏状态 $h_t$ 的每个坐标都是前一个隐藏状态 $h_{t-1}$ 和候选更新 $\\tilde{h}_t$ 相应坐标的凸组合。这个性质对于 GRU 的稳定性至关重要，因为它确保了新隐藏状态是前一个状态和候选更新之间的插值，从而防止了值的失控。\n\n**任务3：计算欧几里得范数 $\\|h_{t}\\|_{2}$。**\n\n一个向量 $v = \\begin{pmatrix}v_1  v_2  v_3\\end{pmatrix}^T$ 的欧几里得范数由公式 $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2 + v_3^2}$ 给出。\n使用在任务1中计算的向量 $h_t$：\n$$h_{t} = \\begin{pmatrix}0.4625 \\\\ -0.34 \\\\ 0.165\\end{pmatrix}$$\n我们计算其欧几里得范数：\n$$\\|h_{t}\\|_{2} = \\sqrt{(0.4625)^2 + (-0.34)^2 + (0.165)^2}$$\n首先，计算各分量的平方：\n$$(0.4625)^2 = 0.21390625$$\n$$(-0.34)^2 = 0.1156$$\n$$(0.165)^2 = 0.027225$$\n接下来，将这些平方值相加：\n$$\\|h_{t}\\|_{2}^2 = 0.21390625 + 0.1156 + 0.027225 = 0.35673125$$\n最后，取和的平方根：\n$$\\|h_{t}\\|_{2} = \\sqrt{0.35673125} \\approx 0.59726983$$\n问题要求答案四舍五入到四位有效数字。前四位有效数字是 $5$、$9$、$7$、$2$。第五位有效数字是 $6$，所以我们将第四位数字向上取整。\n$$\\|h_{t}\\|_{2} \\approx 0.5973$$\n这是最终的数值答案。",
            "answer": "$$\n\\boxed{0.5973}\n$$"
        },
        {
            "introduction": "在训练用于序列生成的 RNN 时，一种常见的技术是“教师强制”（teacher forcing），即使用真实的先前数据点作为下一步的输入。然而，当网络部署并自主运行时，它必须依赖自身先前生成的输出，这种训练与部署之间的不匹配会导致误差累积。本练习  通过一个简化的自回归模型，引导您从第一性原理出发，推导出自由运行模式下预测偏差随时间演变的解析表达式，从而深刻揭示模型失配和随机扰动是如何共同导致误差级联放大的。",
            "id": "4013892",
            "problem": "考虑一个皮层微电路潜在活动序列的简化线性模型，该模型表示为一个一阶自回归 ($AR(1)$) 过程。设真实潜在状态为 $\\{x_t\\}_{t \\geq 0}$，其动态为\n$$\nx_t = a\\,x_{t-1} + \\epsilon_t,\n$$\n其中 $a \\in \\mathbb{R}$ 是真实的自回归系数，$\\epsilon_t$ 是独立同分布的高斯新息，满足 $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$，并且 $x_0 \\sim \\mathcal{N}(0,s_0)$ 与 $\\{\\epsilon_t\\}_{t \\geq 1}$ 独立。一个使用教师强制（teacher forcing）训练的循环神经网络（RNN）学习了一个单步线性预测器，在训练时，该预测器将真实的先前状态 $x_{t-1}$ 作为输入，并预测 $\\hat{x}_t = \\hat{a}\\,x_{t-1}$，其中 $\\hat{a} \\in \\mathbb{R}$ 是学习到的系数。由于模型失配，假设 $\\hat{a} \\neq a$。在部署用于自主序列生成时，RNN在没有教师强制的自由运行模式下运行，迭代过程为\n$$\n\\hat{x}_t = \\hat{a}\\,\\hat{x}_{t-1}, \\quad \\text{with } \\hat{x}_0 = x_0.\n$$\n仅使用概率论和线性系统的核心定义以及上述的独立性性质，推导在一个固定时间 horizon $T \\in \\mathbb{N}$ 时，自由运行的 RNN 预测与真实潜在状态之间的期望平方偏差的闭式解析表达式，\n$$\n\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big],\n$$\n该表达式应为 $a$、$\\hat{a}$、$\\sigma^2$、$s_0$ 和 $T$ 的函数。将你的最终答案表示为单个解析表达式。无需四舍五入。如果出现角度，应以弧度为单位；此量無需物理单位。",
            "solution": "用户要求我解决这个问题。\n\n### 第1步：提取已知条件\n- 真实潜在状态动态：$x_t = a\\,x_{t-1} + \\epsilon_t$ for $t \\geq 1$。\n- 真实自回归系数：$a \\in \\mathbb{R}$。\n- 学习到的自回归系数：$\\hat{a} \\in \\mathbb{R}$，且 $\\hat{a} \\neq a$。\n- 新息：$\\epsilon_t$ 是独立同分布 (i.i.d.) 的，满足 $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$。\n- 初始状态分布：$x_0 \\sim \\mathcal{N}(0,s_0)$。\n- 独立性：$x_0$ 与新息集合 $\\{\\epsilon_t\\}_{t \\geq 1}$ 独立。\n- 自由运行 RNN 动态：$\\hat{x}_t = \\hat{a}\\,\\hat{x}_{t-1}$ for $t \\geq 1$。\n- RNN 的初始条件：$\\hat{x}_0 = x_0$。\n- 固定时间 horizon：$T \\in \\mathbb{N}$。\n- 目标：推导 $\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big]$ 的闭式表达式。\n\n### 第2步：使用提取的已知条件进行验证\n该问题具有科学依据。它构建了一个真实随机过程（一阶自回归模型，即 $AR(1)$）与一个确定性线性模型之间的比较，这是时间序列分析、控制理论和机器学习中的一个常见基本问题。教师强制和自由运行模式的概念是循环神经网络研究中的标准概念。\n\n该问题是适定 (well-posed) 的。给出的条件充分且一致，可以得到唯一解。参数（$a, \\hat{a}, \\sigma^2, s_0, T$）定义清晰，目标是一个标准的统计量（均方误差）。所有必要的分布和独立性假设都已提供。\n\n该问题是客观的。语言精确且数学化，没有任何主观或推测性陈述。\n\n该问题要求基于概率论和线性系统的第一性原理进行推导，这是一个有效的科学任务。它并非微不足道或同义反复，因为它需要展开递归并应用期望和方差的性质。\n\n### 第3步：结论与行动\n问题是有效的。我将继续进行推导。\n\n为了推导期望平方偏差 $\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big]$ 的表达式，我们首先需要找到在时间 horizon T 时真实状态 $x_T$ 和预测状态 $\\hat{x}_T$ 的闭式表达式。\n\n首先，考虑自由运行的 RNN 预测 $\\hat{x}_t$。其动态由递推关系 $\\hat{x}_t = \\hat{a}\\,\\hat{x}_{t-1}$ 和初始条件 $\\hat{x}_0 = x_0$ 给出。我们可以展开这个递推关系：\n$$\n\\hat{x}_1 = \\hat{a}\\,\\hat{x}_0\n$$\n$$\n\\hat{x}_2 = \\hat{a}\\,\\hat{x}_1 = \\hat{a}(\\hat{a}\\,\\hat{x}_0) = \\hat{a}^2\\,\\hat{x}_0\n$$\n通过归纳法，在时间 T 的状态由下式给出：\n$$\n\\hat{x}_T = \\hat{a}^T\\,\\hat{x}_0 = \\hat{a}^T\\,x_0\n$$\n\n接下来，考虑真实潜在状态 $x_t$。其动态由随机递推关系 $x_t = a\\,x_{t-1} + \\epsilon_t$ 给出。我们从初始状态 $x_0$ 开始展开这个关系：\n$$\nx_1 = a\\,x_0 + \\epsilon_1\n$$\n$$\nx_2 = a\\,x_1 + \\epsilon_2 = a(a\\,x_0 + \\epsilon_1) + \\epsilon_2 = a^2\\,x_0 + a\\,\\epsilon_1 + \\epsilon_2\n$$\n通过归纳法，我们可以将时间 T 的状态表示为初始状态 $x_0$ 和新息序列 $\\{\\epsilon_t\\}_{t=1}^T$ 的函数：\n$$\nx_T = a^T\\,x_0 + \\sum_{k=1}^{T} a^{T-k} \\epsilon_k\n$$\n\n现在我们可以写出偏差项 $D_T = \\hat{x}_T - x_T$：\n$$\nD_T = \\hat{a}^T\\,x_0 - \\left( a^T\\,x_0 + \\sum_{k=1}^{T} a^{T-k} \\epsilon_k \\right)\n$$\n$$\nD_T = (\\hat{a}^T - a^T)\\,x_0 - \\sum_{k=1}^{T} a^{T-k} \\epsilon_k\n$$\n目标是计算这个偏差平方的期望，即 $\\mathbb{E}[D_T^2]$。我们展开平方项：\n$$\nD_T^2 = \\left( (\\hat{a}^T - a^T)\\,x_0 - \\sum_{k=1}^{T} a^{T-k} \\epsilon_k \\right)^2\n$$\n$$\nD_T^2 = ((\\hat{a}^T - a^T)\\,x_0)^2 - 2 (\\hat{a}^T - a^T)\\,x_0 \\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right) + \\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\n$$\n$$\nD_T^2 = (\\hat{a}^T - a^T)^2\\,x_0^2 - 2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} x_0 \\epsilon_k + \\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\n$$\n根据期望的线性性质，我们可以分别计算每一项的期望：\n$$\n\\mathbb{E}[D_T^2] = \\mathbb{E}\\left[(\\hat{a}^T - a^T)^2\\,x_0^2\\right] - \\mathbb{E}\\left[2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} x_0 \\epsilon_k\\right] + \\mathbb{E}\\left[\\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\\right]\n$$\n\n让我们分析这三项中的每一项。\n\n第1项：$\\mathbb{E}\\left[(\\hat{a}^T - a^T)^2\\,x_0^2\\right]$\n因子 $(\\hat{a}^T - a^T)^2$ 是一个常数。我们可以将其从期望中提出：\n$$\n\\mathbb{E}\\left[(\\hat{a}^T - a^T)^2\\,x_0^2\\right] = (\\hat{a}^T - a^T)^2\\,\\mathbb{E}[x_0^2]\n$$\n已知 $x_0 \\sim \\mathcal{N}(0, s_0)$，这意味着 $\\mathbb{E}[x_0] = 0$ 且方差为 $\\text{Var}(x_0) = s_0$。方差定义为 $\\text{Var}(x_0) = \\mathbb{E}[x_0^2] - (\\mathbb{E}[x_0])^2$。由于均值为 0，我们有 $\\mathbb{E}[x_0^2] = \\text{Var}(x_0) = s_0$。因此，第一项是：\n$$\n(\\hat{a}^T - a^T)^2\\,s_0\n$$\n\n第2项：$-\\mathbb{E}\\left[2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} x_0 \\epsilon_k\\right]$\n我们提出常数并使用期望的线性性质：\n$$\n-2(\\hat{a}^T - a^T) \\sum_{k=1}^{T} a^{T-k} \\mathbb{E}[x_0 \\epsilon_k]\n$$\n问题陈述 $x_0$ 与 $\\{\\epsilon_t\\}_{t \\geq 1}$ 独立。对于任何 $k \\in \\{1, \\dots, T\\}$，$x_0$ 和 $\\epsilon_k$ 是独立的。对于独立的随机变量，其乘积的期望等于期望的乘积：$\\mathbb{E}[x_0 \\epsilon_k] = \\mathbb{E}[x_0]\\mathbb{E}[\\epsilon_k]$。\n我们知道对于所有的 $k$，$\\mathbb{E}[x_0] = 0$ 且 $\\mathbb{E}[\\epsilon_k] = 0$。因此，$\\mathbb{E}[x_0 \\epsilon_k] = 0 \\times 0 = 0$。所以整个交叉项的和为零。\n\n第3项：$\\mathbb{E}\\left[\\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right)^2\\right]$\n设 $S_T = \\sum_{k=1}^{T} a^{T-k} \\epsilon_k$。由于对所有 $k$ 都有 $\\mathbb{E}[\\epsilon_k]=0$，所以 $S_T$ 的均值为 $\\mathbb{E}[S_T] = \\sum_{k=1}^{T} a^{T-k} \\mathbb{E}[\\epsilon_k] = 0$。因此，我们想计算的表达式就是 $S_T$ 的方差，即 $\\text{Var}(S_T) = \\mathbb{E}[S_T^2] - (\\mathbb{E}[S_T])^2 = \\mathbb{E}[S_T^2]$。\n新息 $\\{\\epsilon_t\\}$ 是独立同分布的，这意味着它们是不相关的。对于不相关的随机变量之和，其和的方差等于方差的和：\n$$\n\\text{Var}(S_T) = \\text{Var}\\left(\\sum_{k=1}^{T} a^{T-k} \\epsilon_k\\right) = \\sum_{k=1}^{T} \\text{Var}(a^{T-k} \\epsilon_k)\n$$\n使用性质 $\\text{Var}(cZ) = c^2 \\text{Var}(Z)$，我们得到：\n$$\n\\sum_{k=1}^{T} (a^{T-k})^2 \\text{Var}(\\epsilon_k) = \\sum_{k=1}^{T} a^{2(T-k)} \\sigma^2 = \\sigma^2 \\sum_{k=1}^{T} a^{2(T-k)}\n$$\n让我们分析这个和。我们可以通过令 $j = T-k$ 来改变求和指数。当 $k=1$ 时，$j=T-1$。当 $k=T$ 时，$j=0$。和变为：\n$$\n\\sum_{j=0}^{T-1} a^{2j} = \\sum_{j=0}^{T-1} (a^2)^j\n$$\n这是一个有限几何级数，共有 $T$ 项，首项为 1，公比为 $r = a^2$。其和由公式 $\\frac{1-r^n}{1-r}$ 给出，其中 $n=T$。该公式在 $r \\neq 1$ 时有效，即 $a^2 \\neq 1$。\n$$\n\\sum_{j=0}^{T-1} (a^2)^j = \\frac{1 - (a^2)^T}{1 - a^2} = \\frac{1 - a^{2T}}{1 - a^2}\n$$\n如果 $a^2 = 1$，和就是项数，即 $T$。然而，单个解析表达式的形式通常由分数给出，该分数在极限 $a^2 \\to 1$ 时是连续的。因此，第三项是：\n$$\n\\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}\n$$\n\n最后，我们将这三项相加，得到期望平方偏差的最终表达式：\n$$\n\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big] = (\\hat{a}^T - a^T)^2\\,s_0 + 0 + \\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}\n$$\n$$\n\\mathbb{E}\\big[(\\hat{x}_T - x_T)^2\\big] = (\\hat{a}^T - a^T)^2\\,s_0 + \\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}\n$$\n该表达式由两部分组成。第一项 $(\\hat{a}^T - a^T)^2\\,s_0$ 表示由于模型 ($\\hat{a}$) 和真实过程 ($a$) 之间的动态失配而导致的从初始状态传播的误差。第二项 $\\sigma^2 \\frac{1 - a^{2T}}{1 - a^2}$ 表示由于真实过程中未建模的随机新息 $\\{\\epsilon_t\\}$ 而累积的误差。",
            "answer": "$$\n\\boxed{(\\hat{a}^{T} - a^{T})^{2} s_{0} + \\sigma^{2} \\frac{1 - a^{2T}}{1 - a^{2}}}\n$$"
        },
        {
            "introduction": "除了序列生成，RNN 在计算神经科学中的一个强大应用是作为可学习的时间处理器，用以模拟皮层回路的动态响应，如突触整合。本练习  将引导您完成一个完整的设计与验证流程：构建一个连续时间 RNN (CT-RNN) 来逼近一个特定的目标时间核函数。您将通过最小二乘法拟合权重，并解析地验证所学模型的脉冲响应，将抽象的网络理论与具体的信号处理任务联系起来，体验从理论到实践的工程设计过程。",
            "id": "4013906",
            "problem": "考虑在皮层序列建模的背景下，设计一个连续时间循环神经网络 (CT-RNN) 来逼近一个目标时间核。该CT-RNN被建模为一个线性时不变系统，由 $m$ 个具有对角动态的漏积分器单元组成，并由一个标量输入驱动。目标是通过使用衰减指数基函数，将网络输出与指定的目标时间核进行匹配，从而学习一个有效的脉冲响应，然后通过用一个近似的狄拉克脉冲驱动网络来验证所学到的脉冲响应。\n\n推导和设计的基本依据：\n- 线性时不变 (LTI) 系统原理：对于一个因果系统，输出 $y(t)$ 是输入 $u(t)$ 和脉冲响应 $h(t)$ 的卷积，即 $y(t) = (h * u)(t) = \\int_{0}^{\\infty} h(\\tau) u(t-\\tau) \\, d\\tau$。\n- 脉冲响应 $h(t)$ 定义为当系统由狄拉克δ输入 $u(t) = \\delta(t)$ 驱动时的输出。\n- 一个具有线性动态和对角连接的CT-RNN可以实现一个衰减指数和作为其脉冲响应。设状态向量为 $x(t) \\in \\mathbb{R}^{m}$，动态方程为 $\\frac{dx_i(t)}{dt} = -\\frac{1}{\\tau_i} x_i(t) + u(t)$，其中 $\\tau_i  0$，输出为 $y(t) = \\sum_{i=1}^{m} w_i x_i(t)$，其中 $w_i \\in \\mathbb{R}$ 是学习得到的权重。该系统的脉冲响应为 $h(t) = \\sum_{i=1}^{m} w_i e^{-t/\\tau_i}$，对于 $t \\ge 0$。\n\n设计任务：\n1. 给定在 $t \\in [0, T]$（其中 $T$ 已指定）上定义的目标核 $k(t)$。您必须通过学习权重 $w_i$ 来逼近 $k(t)$，使得在一个均匀的时间采样网格上，$h(t) \\approx k(t)$ 在最小二乘意义上成立。基函数为 $g_i(t) = e^{-t/\\tau_i}$，其固定的时间常数 $\\tau_i$ 在指定的最小值和最大值之间呈对数间隔分布。\n2. 通过最小化正则化目标 $\\sum_{j} \\left( \\sum_{i=1}^{m} w_i g_i(t_j) - k(t_j) \\right)^2 + \\lambda \\sum_{i=1}^{m} w_i^2$ 来拟合权重 $w_i$，通过线性回归得到闭式解。\n3. 通过用一个近似的狄拉克δ函数 $u(t)$ 驱动CT-RNN来验证所学到的脉冲响应。该函数被建模为在时间 $t=0$ 时宽度为 $\\delta$、幅度为 $1/\\delta$ 的方波脉冲（因此面积等于1）。解析地计算此输入产生的输出 $y(t)$，并在相同的网格上将其与 $k(t)$ 进行比较。\n4. 为每个测试案例计算相对$\\ell_2$误差 $\\varepsilon = \\sqrt{\\frac{\\sum_{j} \\left( y(t_j) - k(t_j) \\right)^2}{\\sum_{j} k(t_j)^2}}$。\n\n所有时间量必须以秒为单位表示。三角函数内的角度必须以弧度为单位表示。最终的数值输出必须是浮点数。任何输出中都不允许出现百分号。\n\n测试套件：\n对于所有情况，使用一个均匀的时间网格 $t_j = j \\cdot dt$，其中 $j = 0, 1, \\dots, N$，$N = \\lfloor T/dt \\rfloor$。\n\n- 情况1（理想情况）：目标核 $k(t) = \\frac{t}{\\tau} e^{-t/\\tau}$，对于 $t \\ge 0$，其中 $\\tau = 0.05\\,\\text{s}$。使用 $T = 1.0\\,\\text{s}$，$dt = 0.001\\,\\text{s}$，$\\delta = dt$，$m = 12$，$\\tau_i$ 在 $0.005\\,\\text{s}$ 到 $0.2\\,\\text{s}$ 之间呈对数间隔分布，正则化参数 $\\lambda = 10^{-6}$。\n- 情况2（双指数突触）：目标核 $k(t) = e^{-t/\\tau_d} - e^{-t/\\tau_r}$，其中 $\\tau_r = 0.01\\,\\text{s}$，$\\tau_d = 0.10\\,\\text{s}$。使用 $T = 1.0\\,\\text{s}$，$dt = 0.001\\,\\text{s}$，$\\delta = dt$，$m = 12$，$\\tau_i$ 在 $0.003\\,\\text{s}$ 到 $0.3\\,\\text{s}$ 之间呈对数间隔分布，$\\lambda = 10^{-6}$。\n- 情况3（边缘情况，振荡结构）：目标核 $k(t) = e^{-t/\\tau} \\cos(2 \\pi f t)$，其中 $\\tau = 0.10\\,\\text{s}$，$f = 10.0\\,\\text{Hz}$，注意 $2 \\pi f t$ 以弧度为单位。使用 $T = 1.0\\,\\text{s}$，$dt = 0.001\\,\\text{s}$，$\\delta = dt$，$m = 20$，$\\tau_i$ 在 $0.003\\,\\text{s}$ 到 $0.5\\,\\text{s}$ 之间呈对数间隔分布，$\\lambda = 10^{-6}$。\n- 情况4（容量边界）：与情况2函数形式相同，但 $\\tau_r = 0.02\\,\\text{s}$，$\\tau_d = 0.08\\,\\text{s}$，且 $m = 2$，$\\tau_i$ 在 $0.01\\,\\text{s}$ 到 $0.10\\,\\text{s}$ 之间呈对数间隔分布，$T = 1.0\\,\\text{s}$，$dt = 0.001\\,\\text{s}$，$\\delta = dt$，$\\lambda = 10^{-6}$。\n\n程序要求：\n- 为每个测试案例实现CT-RNN设计、最小二乘拟合和脉冲验证。\n- 对于使用宽度为 $\\delta$、幅度为 $1/\\delta$ 的方波脉冲 $u(t)$ 进行的解析验证，状态 $x_i(t)$ 为：\n  - 对于 $0 \\le t \\le \\delta$，$x_i(t) = \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-t/\\tau_i} \\right)$。\n  - 对于 $t  \\delta$，$x_i(t) = \\left( \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-\\delta/\\tau_i} \\right) \\right) e^{-(t-\\delta)/\\tau_i}$。\n  输出为 $y(t) = \\sum_{i=1}^{m} w_i x_i(t)$。\n- 在指定的网格上为每种情况计算相对$\\ell_2$误差 $\\varepsilon$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，\"[result1,result2,result3,result4]\"）。每个结果必须是相应测试用例的相对$\\ell_2$误差，表示为浮点数。",
            "solution": "用户提供了一个问题，要求设计和验证一个连续时间循环神经网络 (CT-RNN) 以逼近一个目标时间核。验证将按照说明中概述的正式步骤进行。\n\n### 步骤1：提取已知条件\n- **系统动态**：第 $i$ 个漏积分器单元的状态 $x_i(t) \\in \\mathbb{R}$ 受 $\\frac{dx_i(t)}{dt} = -\\frac{1}{\\tau_i} x_i(t) + u(t)$ 控制，其中 $\\tau_i  0$ 是时间常数， $u(t)$ 是一个标量输入。系统由 $m$ 个这样的单元组成。\n- **系统输出**：网络输出为 $y(t) = \\sum_{i=1}^{m} w_i x_i(t)$，其中 $w_i \\in \\mathbb{R}$ 是学习得到的权重。\n- **脉冲响应**：对于 $t \\ge 0$，系统的脉冲响应为 $h(t) = \\sum_{i=1}^{m} w_i e^{-t/\\tau_i}$。\n- **基函数**：用于逼近的基函数是 $g_i(t) = e^{-t/\\tau_i}$。时间常数 $\\tau_i$ 是固定的，在指定的最小值和最大值之间呈对数间隔分布。\n- **权重拟合的目标函数**：通过最小化正则化最小二乘目标来找到权重 $w_i$：$\\sum_{j} \\left( \\sum_{i=1}^{m} w_i g_i(t_j) - k(t_j) \\right)^2 + \\lambda \\sum_{i=1}^{m} w_i^2$。\n- **验证输入**：网络由一个近似的狄拉克δ函数 $u(t)$ 驱动，它是在 $t=0$ 时宽度为 $\\delta$、幅度为 $1/\\delta$ 的方波脉冲。\n- **用于验证的解析状态响应**：\n  - 对于 $0 \\le t \\le \\delta$：$x_i(t) = \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-t/\\tau_i} \\right)$。\n  - 对于 $t  \\delta$：$x_i(t) = \\left( \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-\\delta/\\tau_i} \\right) \\right) e^{-(t-\\delta)/\\tau_i}$。\n- **误差度量**：相对$\\ell_2$误差为 $\\varepsilon = \\sqrt{\\frac{\\sum_{j} \\left( y(t_j) - k(t_j) \\right)^2}{\\sum_{j} k(t_j)^2}}$。\n- **时间网格**：一个均匀网格 $t_j = j \\cdot dt$，其中 $j = 0, 1, \\dots, N$，$N = \\lfloor T/dt \\rfloor$。\n- **测试案例**：\n  - **情况1**：$k(t) = \\frac{t}{\\tau} e^{-t/\\tau}$，其中 $\\tau = 0.05\\,\\text{s}$。$T = 1.0\\,\\text{s}$，$dt = 0.001\\,\\text{s}$，$\\delta = dt$，$m = 12$，$\\tau_i \\in [0.005, 0.2]\\,\\text{s}$，$\\lambda = 10^{-6}$。\n  - **情况2**：$k(t) = e^{-t/\\tau_d} - e^{-t/\\tau_r}$，其中 $\\tau_r = 0.01\\,\\text{s}$，$\\tau_d = 0.10\\,\\text{s}$。$T = 1.0\\,\\text{s}$，$dt = 0.001\\,\\text{s}$，$\\delta = dt$，$m = 12$，$\\tau_i \\in [0.003, 0.3]\\,\\text{s}$，$\\lambda = 10^{-6}$。\n  - **情况3**：$k(t) = e^{-t/\\tau} \\cos(2 \\pi f t)$，其中 $\\tau = 0.10\\,\\text{s}$，$f = 10.0\\,\\text{Hz}$。$T = 1.0\\,\\text{s}$，$dt = 0.001\\,\\text{s}$，$\\delta = dt$，$m = 20$，$\\tau_i \\in [0.003, 0.5]\\,\\text{s}$，$\\lambda = 10^{-6}$。\n  - **情况4**：$k(t) = e^{-t/\\tau_d} - e^{-t/\\tau_r}$，其中 $\\tau_r = 0.02\\,\\text{s}$，$\\tau_d = 0.08\\,\\text{s}$。$m = 2$，$\\tau_i \\in [0.01, 0.10]\\,\\text{s}$，$T = 1.0\\,\\text{s}$，$dt = 0.001\\,\\text{s}$，$\\delta = dt$，$\\lambda = 10^{-6}$。\n\n### 步骤2：使用提取的已知条件进行验证\n1.  **科学或事实上的不健全性**：该问题在科学上是健全的。将CT-RNN建模为线性时不变系统是计算神经科学中的一种标准方法。动态由一阶线性常微分方程描述。使用指数基、带有Tikhonov正则化的最小二乘拟合以及卷积原理都是标准且数学上正确的。所提供的对方波脉冲输入的状态响应 $x_i(t)$ 的解析解已经过验证，并且是从控制微分方程正确推导出来的。\n2.  **不可形式化或不相关**：该问题是高度可形式化的，以精确的数学方程和算法步骤呈现。它与皮层序列建模的循环神经网络这一特定主题直接相关。\n3.  **不完整或矛盾的设置**：该问题是自洽的。对于四个测试案例中的每一个，都明确提供了所有必要的参数（$T, dt, \\delta, m, \\lambda$ 和 $\\tau_i$ 的范围）以及目标核 $k(t)$ 的函数形式。没有矛盾之处。\n4.  **不切实际或不可行**：物理参数（时间常数、频率）在神经动力学的生物物理和计算模型的现实范围内。计算任务是可行的。\n5.  **不适定或结构不良**：该问题是适定的。最小二乘目标函数是凸的，确保了权重 $\\mathbf{w}$ 的唯一解。步骤从系统定义到参数拟合再到验证，逻辑结构清晰。\n6.  **伪深刻、琐碎或同义反复**：该问题并非微不足道。它需要实现一个非平凡的数值过程，涉及线性代数（矩阵构造和解线性系统）、函数的数值评估以及动力系统响应的模拟。测试案例经过精心选择，以探讨逼近方法的不同方面（例如，拟合振荡模式，模型容量有限的影响）。\n7.  **超出科学可验证性范围**：结果是可数值计算和可验证的。\n\n### 步骤3：结论与行动\n该问题是有效的。它有科学依据，适定且完整。将提供完整的解决方案。\n\n---\n\n该问题要求我们使用一个特定的连续时间循环神经网络 (CT-RNN) 的脉冲响应来逼近一个目标时间核 $k(t)$。这涉及两个主要阶段：首先，将模型的参数拟合到目标；其次，验证所得模型。\n\n**1. 理论框架**\n\n该问题的基础在于线性时不变 (LTI) 系统和函数逼近的原理。\n\n- **系统动态与脉冲响应**：$m$ 个神经元单元中每个单元的状态 $x_i(t)$ 受线性微分方程控制：\n$$ \\frac{dx_i(t)}{dt} = -\\frac{1}{\\tau_i} x_i(t) + u(t) $$\n其中 $\\tau_i  0$ 是第 $i$ 个单元的时间常数，$u(t)$ 是一个共同的标量输入。假设系统从静止状态开始，即 $x_i(0) = 0$，对于输入 $u(t) = \\delta(t)$（狄拉克δ函数），解为 $x_{i, \\text{impulse}}(t) = e^{-t/\\tau_i}$（对于 $t \\ge 0$）。总网络输出 $y(t)$ 是状态的线性组合，$y(t) = \\sum_{i=1}^{m} w_i x_i(t)$。因此，整个网络的脉冲响应 $h(t)$ 是单个单元脉冲响应的加权和：\n$$ h(t) = \\sum_{i=1}^{m} w_i e^{-t/\\tau_i} $$\n函数 $g_i(t) = e^{-t/\\tau_i}$ 作为我们逼近目标核 $k(t)$ 的基函数。\n\n- **通过正则化最小二乘法进行核逼近**：为了找到使 $h(t)$ 逼近 $k(t)$ 的最优权重 $w_i$，我们将时间离散化为一系列点 $t_j$，并最小化一个正则化最小二乘目标函数：\n$$ L(\\mathbf{w}) = \\sum_j \\left( h(t_j) - k(t_j) \\right)^2 + \\lambda \\sum_{i=1}^{m} w_i^2 = \\sum_j \\left( \\sum_{i=1}^{m} w_i g_i(t_j) - k(t_j) \\right)^2 + \\lambda \\sum_{i=1}^{m} w_i^2 $$\n在矩阵表示法中，这是 $\\| \\mathbf{G}\\mathbf{w} - \\mathbf{k} \\|_2^2 + \\lambda \\| \\mathbf{w} \\|_2^2$，其中 $\\mathbf{G}$ 是设计矩阵，其元素为 $G_{ji} = g_i(t_j)$，$\\mathbf{w}$ 是权重向量，$\\mathbf{k}$ 是在时间点 $t_j$ 的目标核值向量。正则化项 $\\lambda \\| \\mathbf{w} \\|_2^2$ 可以稳定解并防止过拟合。最小化 $L(\\mathbf{w})$ 的闭式解由岭回归的正规方程给出：\n$$ \\mathbf{w} = (\\mathbf{G}^T \\mathbf{G} + \\lambda \\mathbf{I})^{-1} \\mathbf{G}^T \\mathbf{k} $$\n其中 $\\mathbf{I}$ 是一个大小为 $m \\times m$ 的单位矩阵。\n\n**2. 使用近似狄拉克脉冲进行验证**\n\n验证步骤涉及用一个近似狄拉克δ函数的输入脉冲来驱动配备了学习权重 $w_i$ 的CT-RNN。该脉冲的持续时间为 $\\delta$，幅度为 $1/\\delta$。每个单元 $x_i(t)$ 对此脉冲的解析响应是给定的，并且可以通过分段求解状态微分方程得出：\n- 对于 $0 \\le t \\le \\delta$，其中 $u(t) = 1/\\delta$，方程 $\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + \\frac{1}{\\delta}$ 且 $x_i(0)=0$ 的解为：\n$$ x_i(t) = \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-t/\\tau_i} \\right) $$\n- 对于 $t  \\delta$，其中 $u(t) = 0$，系统从其在 $t=\\delta$ 时的状态自由演化。解为：\n$$ x_i(t) = x_i(\\delta) e^{-(t-\\delta)/\\tau_i} = \\left( \\frac{\\tau_i}{\\delta} \\left( 1 - e^{-\\delta/\\tau_i} \\right) \\right) e^{-(t-\\delta)/\\tau_i} $$\n然后，使用这些表达式计算总网络输出 $y(t) = \\sum_{i=1}^m w_i x_i(t)$。\n\n**3. 误差计算**\n\n所得网络的准确性由相对$\\ell_2$误差 $\\varepsilon$ 来量化，它在离散时间网格上比较网络输出 $y(t_j)$ 与目标核 $k(t_j)$：\n$$ \\varepsilon = \\sqrt{\\frac{\\sum_{j} (y(t_j) - k(t_j))^2}{\\sum_{j} k(t_j)^2}} $$\n\n**4. 实现算法**\n\n对于每个指定的测试案例，执行以下过程：\n1.  **参数初始化**：定义所有参数：函数 $k(t)$、时间范围 $T$、时间步长 $dt$、脉冲宽度 $\\delta$、基函数数量 $m$、时间常数范围 $[\\tau_{\\text{min}}, \\tau_{\\text{max}}]$ 以及正则化因子 $\\lambda$。\n2.  **网格和基的构建**：创建一个从 $0$到 $T$ 的均匀时间网格 $t_j$ 和一个对数间隔的时间常数集合 $\\tau_i$。\n3.  **权重计算**：\n    a. 构建设计矩阵 $\\mathbf{G}$，其中每一列对应于在时间网格上求值的基函数 $g_i(t)$。\n    b. 通过在时间网格上求值 $k(t)$ 来构建目标向量 $\\mathbf{k}$。\n    c. 求解线性系统 $(\\mathbf{G}^T \\mathbf{G} + \\lambda \\mathbf{I})\\mathbf{w} = \\mathbf{G}^T \\mathbf{k}$ 以找到最优权重向量 $\\mathbf{w}$。\n4.  **响应模拟和验证**：\n    a. 对于每个基 $i \\in \\{1, \\dots, m\\}$，计算对方波脉冲输入的解析状态响应向量 $\\mathbf{x}_i$。\n    b. 计算总网络输出向量为加权和 $\\mathbf{y} = \\sum_{i=1}^m w_i \\mathbf{x}_i$。\n5.  **误差计算**：计算输出向量 $\\mathbf{y}$ 和目标向量 $\\mathbf{k}$ 之间的相对$\\ell_2$误差 $\\varepsilon$。\n\n此系列步骤将应用于四个测试案例中的每一个，以得出最终的数值结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CT-RNN design and validation problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"k_func\": lambda t: (t / 0.05) * np.exp(-t / 0.05) if 0.05 > 0 else np.zeros_like(t),\n            \"T\": 1.0, \"dt\": 0.001, \"delta\": 0.001, \"m\": 12,\n            \"tau_min\": 0.005, \"tau_max\": 0.2, \"lambda_reg\": 1e-6\n        },\n        # Case 2 (double-exponential synapse)\n        {\n            \"k_func\": lambda t: np.exp(-t / 0.10) - np.exp(-t / 0.01),\n            \"T\": 1.0, \"dt\": 0.001, \"delta\": 0.001, \"m\": 12,\n            \"tau_min\": 0.003, \"tau_max\": 0.3, \"lambda_reg\": 1e-6\n        },\n        # Case 3 (edge case, oscillatory structure)\n        {\n            \"k_func\": lambda t: np.exp(-t / 0.10) * np.cos(2 * np.pi * 10.0 * t),\n            \"T\": 1.0, \"dt\": 0.001, \"delta\": 0.001, \"m\": 20,\n            \"tau_min\": 0.003, \"tau_max\": 0.5, \"lambda_reg\": 1e-6\n        },\n        # Case 4 (capacity boundary)\n        {\n            \"k_func\": lambda t: np.exp(-t / 0.08) - np.exp(-t / 0.02),\n            \"T\": 1.0, \"dt\": 0.001, \"delta\": 0.001, \"m\": 2,\n            \"tau_min\": 0.01, \"tau_max\": 0.10, \"lambda_reg\": 1e-6\n        }\n    ]\n\n    results = []\n    \n    def calculate_x_i(t, tau, delta):\n        \"\"\"\n        Calculates the analytical response x_i(t) of a single unit to a square pulse.\n        \"\"\"\n        res = np.zeros_like(t)\n        \n        # for 0 = t = delta\n        mask1 = (t >= 0)  (t = delta)\n        t_part1 = t[mask1]\n        res[mask1] = (tau / delta) * (1 - np.exp(-t_part1 / tau))\n\n        # for t > delta\n        mask2 = t > delta\n        t_part2 = t[mask2]\n        prefactor = (tau / delta) * (1 - np.exp(-delta / tau))\n        res[mask2] = prefactor * np.exp(-(t_part2 - delta) / tau)\n        \n        return res\n\n    for case in test_cases:\n        # 1. Setup\n        T = case[\"T\"]\n        dt = case[\"dt\"]\n        delta = case[\"delta\"]\n        m = case[\"m\"]\n        tau_min = case[\"tau_min\"]\n        tau_max = case[\"tau_max\"]\n        lambda_reg = case[\"lambda_reg\"]\n        k_func = case[\"k_func\"]\n\n        # 2. Discretization and Basis Construction\n        N = int(T / dt)\n        t_grid = np.linspace(0, T, N + 1)\n        tau_basis = np.logspace(np.log10(tau_min), np.log10(tau_max), m)\n\n        # 3. Weight Fitting\n        # Construct design matrix G\n        G = np.exp(-t_grid[:, np.newaxis] / tau_basis[np.newaxis, :])\n        \n        # Construct target vector k\n        k_target = k_func(t_grid)\n        \n        # Solve for weights w using regularized least-squares\n        # w = (G.T @ G + lambda * I)^-1 @ G.T @ k\n        A = G.T @ G + lambda_reg * np.eye(m)\n        b = G.T @ k_target\n        w = np.linalg.solve(A, b)\n\n        # 4. Validation\n        # Compute analytical response y(t)\n        y_response = np.zeros_like(t_grid)\n        for i in range(m):\n            tau_i = tau_basis[i]\n            x_i_response = calculate_x_i(t_grid, tau_i, delta)\n            y_response += w[i] * x_i_response\n\n        # 5. Error Computation\n        numerator = np.sum((y_response - k_target)**2)\n        denominator = np.sum(k_target**2)\n        \n        if denominator == 0:\n            # Handle the unlikely case of a zero target kernel\n            epsilon = 0.0 if numerator == 0.0 else np.inf\n        else:\n            epsilon = np.sqrt(numerator / denominator)\n            \n        results.append(epsilon)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}