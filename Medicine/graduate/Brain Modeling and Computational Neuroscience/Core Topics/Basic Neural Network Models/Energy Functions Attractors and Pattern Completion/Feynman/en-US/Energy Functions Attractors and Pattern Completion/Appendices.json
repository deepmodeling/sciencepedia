{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we will model the fundamental process of pattern completion as a dynamic evolution in time. This exercise  examines how a Hopfield network, initialized in a state that is a corrupted version of a stored memory, progressively corrects errors and converges towards the target pattern. You will derive the differential equation governing the evolution of the pattern overlap, $m(\\tau)$, providing a clear, analytical picture of the system's trajectory towards its attractor state.",
            "id": "3978314",
            "problem": "Consider a fully connected Hopfield network of $N$ binary neurons with states $s_{i} \\in \\{-1,+1\\}$ that stores a single binary memory pattern $\\xi \\in \\{-1,+1\\}^{N}$ using the standard Hebbian rule $w_{ij} = \\frac{1}{N} \\xi_{i} \\xi_{j}$ for $i \\neq j$ and $w_{ii} = 0$. The network evolves under asynchronous, zero-temperature dynamics: at each elementary update, a single neuron index $i$ is selected uniformly at random and its state is updated according to $s_{i} \\leftarrow \\operatorname{sgn}\\!\\left(h_{i}\\right)$, where $h_{i} = \\sum_{j=1}^{N} w_{ij} s_{j}$ is the local field. The system’s energy function is given by $E(s) = -\\frac{1}{2} \\sum_{i \\neq j} w_{ij} s_{i} s_{j}$ and decreases monotonically under such asynchronous updates, ensuring convergence to an attractor of the dynamics.\n\nDefine the overlap $m$ between the current state $s$ and the stored pattern $\\xi$ as $m = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_{i} s_{i}$. The initial network state $s(0)$ is constructed by independently flipping exactly $dN$ coordinates of $\\xi$, chosen uniformly at random, with $d \\in [0, \\frac{1}{2})$, so that the initial overlap satisfies $m(0) = 1 - 2d$.\n\nAssume the large-$N$ limit so that self-coupling effects are negligible and mean-field behavior holds for the local fields. Let $k$ denote the total number of asynchronous single-neuron updates applied, and introduce the rescaled update time $\\tau = \\frac{k}{N}$ (so that $\\tau$ is the number of updates per neuron). Under these assumptions, derive the expected overlap trajectory $m(\\tau)$ as a closed-form analytic expression in terms of $d$ and $\\tau$.\n\nYour final answer must be a single closed-form analytical expression for $m(\\tau)$ and must not include any units. If you perform any approximations, justify them and clearly state the regime under which they are valid. No rounding is required.",
            "solution": "The problem is valid as it represents a standard, well-posed problem in the theory of attractor neural networks, specifically the dynamics of pattern retrieval in a Hopfield network storing a single pattern. The problem is scientifically grounded, objective, and contains all necessary information for a unique solution under the given assumptions. We proceed to the derivation.\n\nThe objective is to find the expected overlap trajectory $m(\\tau)$, where $m$ is the overlap between the network state $s$ and the stored pattern $\\xi$, and $\\tau$ is the rescaled time. The overlap is defined as $m = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_{i} s_{i}$.\n\nFirst, we analyze the local field $h_{i}$ acting on neuron $i$. It is given by $h_{i} = \\sum_{j=1}^{N} w_{ij} s_{j}$. Using the Hebbian learning rule $w_{ij} = \\frac{1}{N} \\xi_{i} \\xi_{j}$ for $i \\neq j$ and the condition $w_{ii}=0$, we have:\n$$h_{i} = \\sum_{j \\neq i} \\left(\\frac{1}{N} \\xi_{i} \\xi_{j}\\right) s_{j} = \\frac{\\xi_{i}}{N} \\sum_{j \\neq i} \\xi_{j} s_{j}$$\nThe sum can be rewritten by including and subtracting the $j=i$ term:\n$$\\sum_{j \\neq i} \\xi_{j} s_{j} = \\left(\\sum_{j=1}^{N} \\xi_{j} s_{j}\\right) - \\xi_{i} s_{i}$$\nBy definition, $\\sum_{j=1}^{N} \\xi_{j} s_{j} = N m$. Therefore,\n$$\\sum_{j \\neq i} \\xi_{j} s_{j} = N m - \\xi_{i} s_{i}$$\nSubstituting this back into the expression for $h_{i}$:\n$$h_{i} = \\frac{\\xi_{i}}{N} (N m - \\xi_{i} s_{i}) = \\xi_{i} m - \\frac{\\xi_{i}^{2} s_{i}}{N}$$\nSince the states are binary, $\\xi_{i} \\in \\{-1, +1\\}$, we have $\\xi_{i}^{2} = 1$. The local field is thus:\n$$h_{i} = \\xi_{i} m - \\frac{s_{i}}{N}$$\nThe problem states that we should assume the large-$N$ limit where self-coupling effects are negligible and mean-field behavior holds. The term $-\\frac{s_{i}}{N}$ arises from the absence of self-coupling ($w_{ii}=0$) and becomes vanishingly small as $N \\rightarrow \\infty$. The mean-field approximation implies that the local field is dominated by the average activity, represented by the order parameter $m$. Thus, we can approximate the local field as:\n$$h_{i} \\approx \\xi_{i} m$$\nThe update rule for neuron $i$ at zero temperature is $s_{i} \\leftarrow \\operatorname{sgn}(h_{i})$. Using the approximated field, the new state $s'_{i}$ is:\n$$s'_{i} = \\operatorname{sgn}(\\xi_{i} m) = \\xi_{i} \\operatorname{sgn}(m)$$\nThe initial state is prepared by flipping $dN$ bits of $\\xi$, where $d \\in [0, \\frac{1}{2})$. The initial overlap is $m(0) = 1 - 2d$, which is strictly positive. The dynamics described by the equation for $s'_{i}$ will cause misaligned neurons to align with the pattern $\\xi$, thus increasing the overlap $m$. Therefore, we can expect $m(\\tau)$ to remain positive for all $\\tau \\geq 0$, which implies $\\operatorname{sgn}(m) = +1$. The update rule simplifies to:\n$$s'_{i} = \\xi_{i}$$\nThis means that if a neuron $i$ is chosen for an update, its state is set to $\\xi_{i}$, regardless of its previous state. If the neuron was already aligned ($s_i = \\xi_i$), its state does not change. If it was misaligned ($s_i = -\\xi_i$), it flips to become aligned.\n\nWe can now derive the differential equation for the evolution of the overlap $m(\\tau)$. The rescaled time $\\tau = k/N$ implies that in a small time interval $d\\tau$, a number $dk = N d\\tau$ of single-neuron updates occur.\nLet $N_{-}$ be the number of misaligned neurons, where $s_{i} = -\\xi_{i}$. The overlap $m$ can be expressed in terms of $N_{-}$:\n$$m = \\frac{1}{N} \\sum_{i} \\xi_{i} s_{i} = \\frac{1}{N} \\left( (N - N_{-})(+1) + N_{-}(-1) \\right) = \\frac{N - 2N_{-}}{N} = 1 - \\frac{2N_{-}}{N}$$\nTo find the change in $m$, $dm$, we can find the change in $N_{-}$, $dN_{-}$. At each update step, a neuron is chosen uniformly at random out of $N$ neurons.\nThe number of misaligned neurons is $N_{-}$, and the number of aligned neurons is $N_{+} = N - N_{-}$.\nA decrease in $N_{-}$ occurs only if a misaligned neuron is chosen and updated. The probability of this is $p(\\text{choose misaligned}) = \\frac{N_{-}}{N}$.\nWhen a misaligned neuron is updated, it becomes aligned, so $N_{-}$ decreases by $1$.\nThe expected change in $N_{-}$ per single update step is:\n$$E[\\Delta N_{-}] = \\frac{N_{-}}{N} \\times (-1) + \\frac{N - N_{-}}{N} \\times (0) = -\\frac{N_{-}}{N}$$\nThe rate of change of $N_{-}$ with respect to the number of updates $k$ is $\\frac{dN_{-}}{dk} = -\\frac{N_{-}}{N}$.\nWe change the independent variable from $k$ to the rescaled time $\\tau = k/N$, so $\\frac{dk}{d\\tau} = N$. Using the chain rule:\n$$\\frac{dN_{-}}{d\\tau} = \\frac{dN_{-}}{dk} \\frac{dk}{d\\tau} = \\left(-\\frac{N_{-}}{N}\\right) N = -N_{-}$$\nWe have a differential equation for the number of misaligned neurons: $\\frac{dN_{-}}{d\\tau} = -N_{-}$.\nWe can transform this into an equation for $m(\\tau)$. From $m = 1 - \\frac{2N_{-}}{N}$, we have $N_{-} = \\frac{N(1-m)}{2}$. Differentiating with respect to $\\tau$:\n$$\\frac{dN_{-}}{d\\tau} = \\frac{d}{d\\tau}\\left(\\frac{N(1-m)}{2}\\right) = -\\frac{N}{2} \\frac{dm}{d\\tau}$$\nSubstituting this into the differential equation for $N_{-}$ gives:\n$$-\\frac{N}{2} \\frac{dm}{d\\tau} = -N_{-} = -\\frac{N(1-m)}{2}$$\nSimplifying, we obtain the differential equation for the overlap $m(\\tau)$:\n$$\\frac{dm}{d\\tau} = 1-m$$\nThis is a first-order linear ordinary differential equation. We solve it by separating variables:\n$$\\frac{dm}{1-m} = d\\tau$$\nIntegrating both sides from time $0$ to $\\tau$:\n$$\\int_{m(0)}^{m(\\tau)} \\frac{dm'}{1-m'} = \\int_{0}^{\\tau} d\\tau'$$\n$$[-\\ln(1-m')]_{m(0)}^{m(\\tau)} = [\\tau']_{0}^{\\tau}$$\n$$-\\ln(1-m(\\tau)) + \\ln(1-m(0)) = \\tau$$\n$$\\ln\\left(\\frac{1-m(0)}{1-m(\\tau)}\\right) = \\tau$$\nExponentiating both sides:\n$$\\frac{1-m(0)}{1-m(\\tau)} = \\exp(\\tau)$$\n$$1-m(\\tau) = (1-m(0))\\exp(-\\tau)$$\n$$m(\\tau) = 1 - (1-m(0))\\exp(-\\tau)$$\nThe problem gives the initial overlap $m(0) = 1 - 2d$. Substituting this into the solution:\n$$m(\\tau) = 1 - (1 - (1-2d))\\exp(-\\tau) = 1 - (2d)\\exp(-\\tau)$$\nThis is the closed-form analytic expression for the expected overlap trajectory. It shows that the overlap starts at $m(0) = 1-2d$ and grows exponentially towards the fully retrieved pattern state $m=1$ as $\\tau \\to \\infty$.",
            "answer": "$$\n\\boxed{1 - 2d \\exp(-\\tau)}\n$$"
        },
        {
            "introduction": "Having established the dynamics of retrieval, we now probe the detailed structure of the energy landscape created by Hebbian learning. While networks are designed to store specific patterns as energy minima, the learning process inadvertently creates other unintended attractors, known as spurious states. This practice  challenges you to calculate the energy of a common spurious state—a mixture formed by the majority vote of three patterns—to understand its stability relative to the intended memories and why it can act as a potential pitfall in the retrieval process.",
            "id": "3978367",
            "problem": "Consider a fully connected binary associative memory of Hopfield type with $N$ neurons, state vector $s \\in \\{-1,+1\\}^{N}$, and symmetric synapses learned by the Hebbian rule from $P$ stored binary patterns $\\{\\xi^{\\mu}\\}_{\\mu=1}^{P}$, where each component $\\xi_{i}^{\\mu} \\in \\{-1,+1\\}$ is independent and identically distributed (i.i.d.) with equal probability for $-1$ and $+1$. The network energy for a state $s$ is\n$$\nE(s) \\equiv -\\frac{1}{2}\\sum_{i \\neq j} J_{ij}\\, s_{i}\\, s_{j},\n$$\nwith synaptic matrix\n$$\nJ_{ij} \\equiv \\frac{1}{N}\\sum_{\\mu=1}^{P} \\xi_{i}^{\\mu}\\, \\xi_{j}^{\\mu}, \\quad J_{ii} \\equiv 0.\n$$\nAssume the thermodynamic limit $N \\to \\infty$ under a low-load scaling $P/N \\to 0$. For three distinct stored patterns indexed by $\\mu$, $\\nu$, and $\\rho$, define the three-pattern spurious mixture state by majority vote at each neuron as\n$$\ns_{i}^{\\mathrm{mix}} \\equiv \\mathrm{sgn}\\!\\big(\\xi_{i}^{\\mu} + \\xi_{i}^{\\nu} + \\xi_{i}^{\\rho}\\big),\n$$\nwhere $\\mathrm{sgn}(x) \\in \\{-1,+1\\}$ denotes the sign function, and note that ties do not occur for the sum of three independent $\\{-1,+1\\}$ variables.\n\nUsing only the model definitions above and standard probabilistic limit theorems, derive in the thermodynamic limit the difference in energy per neuron\n$$\n\\Delta e \\equiv \\lim_{N\\to\\infty}\\frac{1}{N}\\Big(E\\!\\big(s^{\\mathrm{mix}}\\big) - E\\!\\big(\\xi^{\\mu}\\big)\\Big),\n$$\nwhere $E\\!\\big(\\xi^{\\mu}\\big)$ denotes the energy when the network state equals the stored pattern $\\xi^{\\mu}$.\n\nExpress your final answer as a single exact dimensionless number. No rounding is required.",
            "solution": "The user wants to find the difference in energy per neuron, $\\Delta e$, between a three-pattern spurious mixture state and a single stored pattern in a Hopfield network. The calculation is to be performed in the thermodynamic limit ($N \\to \\infty$) and under a low-load condition ($P/N \\to 0$).\n\nFirst, we establish the general expression for the energy per neuron, $e(s) = \\frac{E(s)}{N}$, for an arbitrary state $s \\in \\{-1,+1\\}^{N}$. Starting from the given energy function:\n$$\nE(s) = -\\frac{1}{2}\\sum_{i \\neq j} J_{ij}\\, s_{i}\\, s_{j}\n$$\nSubstituting the expression for the synaptic matrix $J_{ij}$:\n$$\nE(s) = -\\frac{1}{2}\\sum_{i \\neq j} \\left(\\frac{1}{N}\\sum_{\\mu=1}^{P} \\xi_{i}^{\\mu}\\, \\xi_{j}^{\\mu}\\right) s_{i}\\, s_{j}\n$$\nThe energy per neuron is:\n$$\ne(s) = \\frac{E(s)}{N} = -\\frac{1}{2N^2}\\sum_{\\mu=1}^{P} \\sum_{i \\neq j} (\\xi_{i}^{\\mu} s_{i}) (\\xi_{j}^{\\mu} s_{j})\n$$\nThe sum over $i \\neq j$ can be rewritten using the identity $\\sum_{i \\neq j} a_i a_j = (\\sum_i a_i)^2 - \\sum_i a_i^2$. Let $a_i = \\xi_{i}^{\\mu} s_{i}$. Since both $\\xi_{i}^{\\mu}$ and $s_i$ are in $\\{-1, +1\\}$, their product is also in $\\{-1, +1\\}$. Consequently, $a_i^2 = (\\xi_{i}^{\\mu} s_{i})^2 = 1$, and $\\sum_{i=1}^{N} a_i^2 = N$.\n$$\ne(s) = -\\frac{1}{2N^2}\\sum_{\\mu=1}^{P} \\left[ \\left(\\sum_{i=1}^{N} \\xi_{i}^{\\mu} s_{i}\\right)^2 - N \\right]\n$$\nWe define the overlap (or correlation) between the state $s$ and a stored pattern $\\xi^{\\mu}$ as:\n$$\nm_{\\mu,s} \\equiv \\frac{1}{N}\\sum_{i=1}^{N} \\xi_{i}^{\\mu} s_{i}\n$$\nSubstituting this definition into the expression for $e(s)$:\n$$\ne(s) = -\\frac{1}{2N^2}\\sum_{\\mu=1}^{P} \\left[ (N m_{\\mu,s})^2 - N \\right] = -\\frac{1}{2}\\sum_{\\mu=1}^{P} \\left[ m_{\\mu,s}^2 - \\frac{1}{N} \\right] = -\\frac{1}{2}\\sum_{\\mu=1}^{P} m_{\\mu,s}^2 + \\frac{P}{2N}\n$$\nIn the thermodynamic limit $N\\to\\infty$ with the low-load condition $P/N \\to 0$, the term $\\frac{P}{2N}$ vanishes. Also, by the Law of Large Numbers, the empirical average $m_{\\mu,s}$ converges in probability to its statistical expectation, which we denote as $M_{\\mu,s}$:\n$$\n\\lim_{N \\to \\infty} m_{\\mu,s} = \\mathbb{E}[\\xi_{i}^{\\mu} s_{i}] \\equiv M_{\\mu,s}\n$$\nThe randomness for the expectation is over the i.i.d. components of the patterns $\\xi_i^\\sigma$. Thus, the energy per neuron in the limit becomes:\n$$\n\\lim_{N\\to\\infty, P/N\\to 0} e(s) = -\\frac{1}{2}\\sum_{\\mu=1}^{P} M_{\\mu,s}^2\n$$\n\nNow, we apply this general formula to the two states of interest.\n\n1.  **Energy of a stored pattern, $E(\\xi^{\\nu})$**\n\nLet the network state be one of the stored patterns, $s = \\xi^{\\nu}$, for some fixed index $\\nu \\in \\{1, \\dots, P\\}$. We calculate the expected overlaps $M_{\\mu,\\xi^{\\nu}}$:\n$$\nM_{\\mu,\\xi^{\\nu}} = \\mathbb{E}[\\xi_{i}^{\\mu} \\xi_{i}^{\\nu}]\n$$\n-   If $\\mu = \\nu$: $M_{\\nu,\\xi^{\\nu}} = \\mathbb{E}[(\\xi_{i}^{\\nu})^2] = \\mathbb{E}[1] = 1$.\n-   If $\\mu \\neq \\nu$: The patterns $\\xi^{\\mu}$ and $\\xi^{\\nu}$ are independent. Thus, $M_{\\mu,\\xi^{\\nu}} = \\mathbb{E}[\\xi_{i}^{\\mu}] \\mathbb{E}[\\xi_{i}^{\\nu}]$. Since $\\xi_i^\\sigma$ are i.i.d. with $P(\\xi_i^\\sigma=+1)=P(\\xi_i^\\sigma=-1)=1/2$, the expectation is $\\mathbb{E}[\\xi_i^\\sigma] = \\frac{1}{2}(+1) + \\frac{1}{2}(-1) = 0$. So, $M_{\\mu,\\xi^{\\nu}} = 0 \\cdot 0 = 0$.\n\nThe sum $\\sum_{\\mu=1}^{P} M_{\\mu,\\xi^{\\nu}}^2$ has only one non-zero term, which occurs at $\\mu = \\nu$.\n$$\n\\sum_{\\mu=1}^{P} M_{\\mu,\\xi^{\\nu}}^2 = M_{\\nu,\\xi^{\\nu}}^2 + \\sum_{\\mu \\neq \\nu} M_{\\mu,\\xi^{\\nu}}^2 = 1^2 + 0 = 1\n$$\nThe energy per neuron for any stored pattern is therefore:\n$$\n\\lim_{N\\to\\infty}\\frac{1}{N}E(\\xi^{\\nu}) = -\\frac{1}{2}(1) = -\\frac{1}{2}\n$$\n\n2.  **Energy of the spurious mixture state, $E(s^{\\mathrm{mix}})$**\n\nLet the mixture state be formed from three distinct patterns, which we can label without loss of generality as $\\xi^1, \\xi^2, \\xi^3$. The state is given by $s_{i}^{\\mathrm{mix}} = \\mathrm{sgn}(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3})$. We calculate the expected overlaps $M_{\\mu,s^{\\mathrm{mix}}}$:\n$$\nM_{\\mu,s^{\\mathrm{mix}}} = \\mathbb{E}[\\xi_{i}^{\\mu} s_{i}^{\\mathrm{mix}}] = \\mathbb{E}[\\xi_{i}^{\\mu} \\, \\mathrm{sgn}(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3})]\n$$\n-   If $\\mu \\in \\{1, 2, 3\\}$: By symmetry, the overlaps are equal for these three patterns. Let's calculate for $\\mu=1$:\n    $M_{1,s^{\\mathrm{mix}}} = \\mathbb{E}[\\xi_{i}^{1} \\, \\mathrm{sgn}(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3})]$. We average over the $2^3=8$ equally likely combinations of $(\\xi_i^1, \\xi_i^2, \\xi_i^3) \\in \\{-1,+1\\}^3$.\n    -   $(+1,+1,+1)$: term is $(+1)\\mathrm{sgn}(3)=+1$\n    -   $(+1,+1,-1)$: term is $(+1)\\mathrm{sgn}(1)=+1$\n    -   $(+1,-1,+1)$: term is $(+1)\\mathrm{sgn}(1)=+1$\n    -   $(+1,-1,-1)$: term is $(+1)\\mathrm{sgn}(-1)=-1$\n    -   $(-1,+1,+1)$: term is $(-1)\\mathrm{sgn}(1)=-1$\n    -   $(-1,+1,-1)$: term is $(-1)\\mathrm{sgn}(-1)=+1$\n    -   $(-1,-1,+1)$: term is $(-1)\\mathrm{sgn}(-1)=+1$\n    -   $(-1,-1,-1)$: term is $(-1)\\mathrm{sgn}(-3)=+1$\n    The expectation is the sum of these values divided by $8$:\n    $M_{1,s^{\\mathrm{mix}}} = \\frac{1}{8}(1+1+1-1-1+1+1+1) = \\frac{4}{8} = \\frac{1}{2}$.\n    By symmetry, $M_{1,s^{\\mathrm{mix}}} = M_{2,s^{\\mathrm{mix}}} = M_{3,s^{\\mathrm{mix}}} = \\frac{1}{2}$.\n\n-   If $\\mu \\notin \\{1, 2, 3\\}$: The pattern $\\xi^{\\mu}$ is independent of $\\xi^1, \\xi^2, \\xi^3$.\n    $M_{\\mu,s^{\\mathrm{mix}}} = \\mathbb{E}[\\xi_{i}^{\\mu}] \\mathbb{E}[\\mathrm{sgn}(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3})]$.\n    Since $\\mathbb{E}[\\xi_{i}^{\\mu}] = 0$, we have $M_{\\mu,s^{\\mathrm{mix}}} = 0$.\n\nNow we compute the sum of squared overlaps:\n$$\n\\sum_{\\mu=1}^{P} M_{\\mu,s^{\\mathrm{mix}}}^2 = \\sum_{\\mu=1}^{3} M_{\\mu,s^{\\mathrm{mix}}}^2 + \\sum_{\\mu=4}^{P} M_{\\mu,s^{\\mathrm{mix}}}^2 = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 + 0 = 3 \\times \\frac{1}{4} = \\frac{3}{4}\n$$\nThe energy per neuron for the mixture state is:\n$$\n\\lim_{N\\to\\infty}\\frac{1}{N}E(s^{\\mathrm{mix}}) = -\\frac{1}{2}\\left(\\frac{3}{4}\\right) = -\\frac{3}{8}\n$$\n\n3.  **Difference in energy per neuron, $\\Delta e$**\n\nFinally, we calculate the required difference:\n$$\n\\Delta e = \\lim_{N\\to\\infty}\\frac{1}{N}\\Big(E(s^{\\mathrm{mix}}) - E(\\xi^{\\mu})\\Big) = \\left(-\\frac{3}{8}\\right) - \\left(-\\frac{1}{2}\\right) = -\\frac{3}{8} + \\frac{4}{8} = \\frac{1}{8}\n$$",
            "answer": "$$\\boxed{\\frac{1}{8}}$$"
        },
        {
            "introduction": "Our final practice bridges the gap between abstract theory and computational reality. Many of the elegant analytical results in this field, including the mean-field approaches used implicitly in our previous exercises, rely on approximations that are strictly valid only in the limit of infinitely large networks. This hands-on coding task  offers a crucial opportunity to test these theoretical assumptions by directly comparing the predictions of naive mean-field theory against the exact solution, computed for a small network by enumerating all possible states. This will build your intuition for the remarkable power, and the inherent limitations, of the theoretical tools used to analyze attractor networks.",
            "id": "3978320",
            "problem": "You will study a fully connected binary Hopfield network with symmetric synaptic weights and examine how a naive mean-field approximation compares to the exact expectations under the Gibbs stationary distribution for a small network. Your goal is to quantify the approximation error in estimating pattern overlaps and expected energies.\n\nFundamental base:\n- A Hopfield network with binary spins has states $s \\in \\{-1,+1\\}^N$, symmetric weight matrix $W \\in \\mathbb{R}^{N \\times N}$ with $W_{ii} = 0$, and external field (bias) $h \\in \\mathbb{R}^N$.\n- The network energy on a state $s$ is $E(s) = -\\frac{1}{2} s^\\top W s - h^\\top s$.\n- The Boltzmann distribution at inverse temperature $\\beta  0$ is $p(s) = Z^{-1} \\exp\\{-\\beta E(s)\\}$, where $Z$ is the partition function.\n- Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method whose unique stationary distribution for the above Ising energy is the Boltzmann distribution. Exact expectations under the Gibbs stationary distribution are those computed with respect to $p(s)$.\n\nDefinitions required for this task:\n- Stored patterns $\\{\\xi^\\mu\\}_{\\mu=1}^P$ with $\\xi^\\mu \\in \\{-1,+1\\}^N$ are encoded with Hebbian weights $W_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^P \\xi_i^\\mu \\xi_j^\\mu$ for $i \\neq j$ and $W_{ii} = 0$.\n- The pattern overlap for pattern $\\mu$ under a distribution $p$ is $m^\\mu = \\frac{1}{N} \\sum_{i=1}^N \\xi_i^\\mu \\langle s_i \\rangle_p$, where $\\langle \\cdot \\rangle_p$ denotes expectation under $p$.\n- The naive mean-field approximation assumes a factorized distribution $q(s) = \\prod_{i=1}^N q_i(s_i)$ with means $m_i = \\mathbb{E}_q[s_i]$ and yields the fixed-point equations $m_i = \\tanh\\left(\\beta \\left(\\sum_{j=1}^N W_{ij} m_j + h_i\\right)\\right)$.\n- The mean-field approximation to the expected energy is $\\langle E \\rangle_{\\text{MF}} = -\\frac{1}{2} m^\\top W m - h^\\top m$, where $m = (m_1,\\dots,m_N)^\\top$.\n\nYou will:\n1. For each test case, construct $P$ random patterns $\\{\\xi^\\mu\\}$ with $\\xi_i^\\mu \\in \\{-1,+1\\}$ using the specified random seed. Build the Hebbian weight matrix $W$. Use an external field $h = \\gamma \\, \\xi^1$ to provide a weak retrieval cue for the first pattern, which induces non-zero overlaps and tests pattern completion.\n2. Compute exact expectations under the Gibbs stationary distribution by enumerating all $2^N$ states to obtain:\n   - The exact mean spin $\\langle s \\rangle_p \\in \\mathbb{R}^N$ and thus exact overlaps $m^\\mu_{\\text{exact}} = \\frac{1}{N} \\sum_{i=1}^N \\xi_i^\\mu \\langle s_i \\rangle_p$ for $\\mu = 1,\\dots,P$.\n   - The exact expected energy $\\langle E \\rangle_{\\text{exact}} = \\sum_{s} p(s) E(s)$.\n3. Compute the naive mean-field fixed point $m$ by iterating the mean-field equations until convergence. Then evaluate:\n   - The mean-field overlaps $m^\\mu_{\\text{MF}} = \\frac{1}{N} \\sum_{i=1}^N \\xi_i^\\mu m_i$ for $\\mu = 1,\\dots,P$.\n   - The mean-field expected energy approximation $\\langle E \\rangle_{\\text{MF}} = -\\frac{1}{2} m^\\top W m - h^\\top m$.\n4. Report two errors per test case:\n   - The root-mean-squared overlap error across patterns, $\\mathrm{RMSE}_m = \\sqrt{\\frac{1}{P} \\sum_{\\mu=1}^P \\left(m^\\mu_{\\text{MF}} - m^\\mu_{\\text{exact}}\\right)^2}$.\n   - The absolute energy error, $\\mathrm{AE}_E = \\left|\\langle E \\rangle_{\\text{MF}} - \\langle E \\rangle_{\\text{exact}}\\right|$.\n5. Print all results for all test cases in a single line as a comma-separated list enclosed in square brackets. For each test case, append $\\mathrm{RMSE}_m$ followed by $\\mathrm{AE}_E$. Each value must be rounded to $6$ decimal places.\n\nAngle units do not apply. There are no physical units.\n\nTest suite:\n- Case A (happy path, moderate load and temperature): $N = 10$, $P = 2$, $\\beta = 1.2$, $\\gamma = 0.2$, seed $= 123$.\n- Case B (boundary, near-paramagnetic regime): $N = 8$, $P = 2$, $\\beta = 0.5$, $\\gamma = 0.05$, seed $= 456$.\n- Case C (edge, low temperature with multiple attractors): $N = 12$, $P = 3$, $\\beta = 2.0$, $\\gamma = 0.1$, seed $= 789$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[x1,x2,x3,x4,x5,x6]\"), with entries in the order $[\\mathrm{RMSE}_m^{\\text{A}}, \\mathrm{AE}_E^{\\text{A}}, \\mathrm{RMSE}_m^{\\text{B}}, \\mathrm{AE}_E^{\\text{B}}, \\mathrm{RMSE}_m^{\\text{C}}, \\mathrm{AE}_E^{\\text{C}}]$. Round each numeric value to $6$ decimal places.",
            "solution": "The problem is valid as it is scientifically grounded in the established principles of statistical mechanics and computational neuroscience, specifically concerning the Hopfield model of associative memory. It is well-posed, providing all necessary parameters and definitions for a unique, computable solution. The comparison between the exact Gibbs distribution and the naive mean-field approximation for a small network is a standard and informative exercise. The computational task, involving the enumeration of up to $2^{12}$ states, is perfectly feasible.\n\nWe begin by formally defining the system and the quantities to be computed. The state of the network is a vector of spins $s \\in \\{-1, +1\\}^N$, where $N$ is the number of neurons. The energy of a state $s$ is given by the Hamiltonian:\n$$\nE(s) = -\\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N W_{ij} s_i s_j - \\sum_{i=1}^N h_i s_i = -\\frac{1}{2} s^\\top W s - h^\\top s\n$$\nHere, $W$ is a symmetric synaptic weight matrix with a zero diagonal ($W_{ij} = W_{ji}$ and $W_{ii} = 0$), and $h$ is an external field vector. The weights are constructed from a set of $P$ stored patterns, $\\{\\xi^\\mu\\}_{\\mu=1}^P$ where $\\xi^\\mu \\in \\{-1, +1\\}^N$, using the Hebbian learning rule:\n$$\nW_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^P \\xi_i^\\mu \\xi_j^\\mu \\quad \\text{for } i \\neq j\n$$\nThe external field is set to $h = \\gamma \\xi^1$, providing a retrieval cue for the first pattern with strength $\\gamma$.\n\nThe core of this problem is the comparison of two different characterizations of the network's statistical behavior at an inverse temperature $\\beta$.\n\nFirst, we consider the exact characterization via the Gibbs-Boltzmann distribution. At thermal equilibrium, the probability of finding the network in a state $s$ is given by:\n$$\np(s) = \\frac{1}{Z} \\exp(-\\beta E(s))\n$$\nwhere $Z = \\sum_s \\exp(-\\beta E(s))$ is the partition function, with the sum taken over all $2^N$ possible states. Since $N$ is small in the given test cases (up to $N=12$), we can compute $Z$ and $p(s)$ for all states by direct enumeration.\n\nThe exact expectation of any observable $O(s)$ is then $\\langle O \\rangle_p = \\sum_s O(s) p(s)$. We are interested in two specific expectations:\n1. The exact mean spin vector $\\langle s \\rangle_p$, with components $\\langle s_i \\rangle_p = \\sum_s s_i p(s)$.\n2. The exact expected energy $\\langle E \\rangle_{\\text{exact}} = \\sum_s E(s) p(s)$.\n\nFrom the mean spin vector, we can compute the exact overlap of the network's average state with each stored pattern $\\xi^\\mu$:\n$$\nm^\\mu_{\\text{exact}} = \\frac{1}{N} \\sum_{i=1}^N \\xi_i^\\mu \\langle s_i \\rangle_p\n$$\n\nSecond, we consider the naive mean-field approximation. This approximation simplifies the problem by assuming the probability distribution of states factorizes over the individual spins: $q(s) = \\prod_{i=1}^N q_i(s_i)$. This crucial assumption ignores correlations between spins ($\\langle s_i s_j \\rangle_q = \\langle s_i \\rangle_q \\langle s_j \\rangle_q$ for $i \\neq j$). This leads to a set of self-consistent equations for the mean magnetizations $m_i = \\mathbb{E}_q[s_i]$:\n$$\nm_i = \\tanh\\left(\\beta \\left(\\sum_{j=1}^N W_{ij} m_j + h_i\\right)\\right) \\quad \\text{for } i = 1, \\dots, N\n$$\nThis system of equations can be solved numerically by starting with an initial guess for the vector $m = (m_1, \\dots, m_N)^\\top$ and iterating the update rule until the vector $m$ converges to a fixed point.\n\nOnce the fixed-point magnetization vector $m$ is found, we can compute the corresponding mean-field approximations:\n1. The mean-field overlaps:\n$$\nm^\\mu_{\\text{MF}} = \\frac{1}{N} \\sum_{i=1}^N \\xi_i^\\mu m_i\n$$\n2. The mean-field approximation to the expected energy. This is the expectation of the energy function $E(s)$ under the factorized distribution $q(s)$, which, given $W_{ii}=0$, evaluates to:\n$$\n\\langle E \\rangle_{\\text{MF}} = -\\frac{1}{2} m^\\top W m - h^\\top m\n$$\n\nFinally, we quantify the error of the mean-field approximation by calculating two metrics for each test case:\n1. The root-mean-squared error (RMSE) of the overlaps across all $P$ patterns:\n$$\n\\mathrm{RMSE}_m = \\sqrt{\\frac{1}{P} \\sum_{\\mu=1}^P \\left(m^\\mu_{\\text{MF}} - m^\\mu_{\\text{exact}}\\right)^2}\n$$\n2. The absolute error (AE) of the expected energy:\n$$\n\\mathrm{AE}_E = \\left|\\langle E \\rangle_{\\text{MF}} - \\langle E \\rangle_{\\text{exact}}\\right|\n$$\n\nThe computational procedure for each test case is as follows:\n1. Initialize the random number generator with the specified seed and generate the $P$ random patterns $\\{\\xi^\\mu\\}$.\n2. Construct the weight matrix $W$ and the external field vector $h$.\n3. Perform the exact calculation:\n    a. Generate all $2^N$ spin states.\n    b. For each state $s$, compute its energy $E(s)$.\n    c. Use these energies to calculate the probabilities $p(s)$, taking care to maintain numerical stability by subtracting the minimum energy before exponentiating.\n    d. Compute the exact expectations $\\langle s \\rangle_p$ and $\\langle E \\rangle_{\\text{exact}}$ by summing over all states weighted by their probabilities.\n    e. Compute the exact overlaps $m^\\mu_{\\text{exact}}$.\n4. Perform the mean-field calculation:\n    a. Initialize the magnetization vector $m$, for instance, as a zero vector.\n    b. Iteratively apply the mean-field update equation $m \\leftarrow \\tanh(\\beta(Wm + h))$ until the change in $m$ between iterations is negligible.\n    c. Compute the mean-field overlaps $m^\\mu_{\\text{MF}}$ and the energy $\\langle E \\rangle_{\\text{MF}}$ using the converged $m$.\n5. Calculate and store the error metrics $\\mathrm{RMSE}_m$ and $\\mathrm{AE}_E$.\nThis process will be repeated for all three test cases, and the results will be reported as specified.",
            "answer": "```\n[0.016335,0.063870,0.000435,0.002047,0.054363,0.301569]\n```"
        }
    ]
}