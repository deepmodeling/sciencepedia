{
    "hands_on_practices": [
        {
            "introduction": "The concept of an energy function is central to understanding attractor networks, as it defines a landscape over the state space where memories reside in low-energy valleys. While the Hebbian learning rule successfully makes stored patterns into attractors, it also creates unintended local minima known as spurious states. This exercise provides a quantitative exploration of this phenomenon by guiding you to calculate the energy of a \"mixture\" state, allowing you to directly compare its stability relative to the intended memory patterns.",
            "id": "3978367",
            "problem": "Consider a fully connected binary associative memory of Hopfield type with $N$ neurons, state vector $s \\in \\{-1,+1\\}^{N}$, and symmetric synapses learned by the Hebbian rule from $P$ stored binary patterns $\\{\\xi^{\\mu}\\}_{\\mu=1}^{P}$, where each component $\\xi_{i}^{\\mu} \\in \\{-1,+1\\}$ is independent and identically distributed (i.i.d.) with equal probability for $-1$ and $+1$. The network energy for a state $s$ is\n$$\nE(s) \\equiv -\\frac{1}{2}\\sum_{i \\neq j} J_{ij}\\, s_{i}\\, s_{j},\n$$\nwith synaptic matrix\n$$\nJ_{ij} \\equiv \\frac{1}{N}\\sum_{\\mu=1}^{P} \\xi_{i}^{\\mu}\\, \\xi_{j}^{\\mu}, \\quad J_{ii} \\equiv 0.\n$$\nAssume the thermodynamic limit $N \\to \\infty$ under a low-load scaling $P/N \\to 0$. For three distinct stored patterns indexed by $\\mu$, $\\nu$, and $\\rho$, define the three-pattern spurious mixture state by majority vote at each neuron as\n$$\ns_{i}^{\\mathrm{mix}} \\equiv \\mathrm{sgn}\\!\\big(\\xi_{i}^{\\mu} + \\xi_{i}^{\\nu} + \\xi_{i}^{\\rho}\\big),\n$$\nwhere $\\mathrm{sgn}(x) \\in \\{-1,+1\\}$ denotes the sign function, and note that ties do not occur for the sum of three independent $\\{-1,+1\\}$ variables.\n\nUsing only the model definitions above and standard probabilistic limit theorems, derive in the thermodynamic limit the difference in energy per neuron\n$$\n\\Delta e \\equiv \\lim_{N\\to\\infty}\\frac{1}{N}\\Big(E\\!\\big(s^{\\mathrm{mix}}\\big) - E\\!\\big(\\xi^{\\mu}\\big)\\Big),\n$$\nwhere $E\\!\\big(\\xi^{\\mu}\\big)$ denotes the energy when the network state equals the stored pattern $\\xi^{\\mu}$.\n\nExpress your final answer as a single exact dimensionless number. No rounding is required.",
            "solution": "The user wants to find the difference in energy per neuron, $\\Delta e$, between a three-pattern spurious mixture state and a single stored pattern in a Hopfield network. The calculation is to be performed in the thermodynamic limit ($N \\to \\infty$) and under a low-load condition ($P/N \\to 0$).\n\nFirst, we establish the general expression for the energy per neuron, $e(s) = \\frac{E(s)}{N}$, for an arbitrary state $s \\in \\{-1,+1\\}^{N}$. Starting from the given energy function:\n$$\nE(s) = -\\frac{1}{2}\\sum_{i \\neq j} J_{ij}\\, s_{i}\\, s_{j}\n$$\nSubstituting the expression for the synaptic matrix $J_{ij}$:\n$$\nE(s) = -\\frac{1}{2}\\sum_{i \\neq j} \\left(\\frac{1}{N}\\sum_{\\mu=1}^{P} \\xi_{i}^{\\mu}\\, \\xi_{j}^{\\mu}\\right) s_{i}\\, s_{j}\n$$\nThe energy per neuron is:\n$$\ne(s) = \\frac{E(s)}{N} = -\\frac{1}{2N^2}\\sum_{\\mu=1}^{P} \\sum_{i \\neq j} (\\xi_{i}^{\\mu} s_{i}) (\\xi_{j}^{\\mu} s_{j})\n$$\nThe sum over $i \\neq j$ can be rewritten using the identity $\\sum_{i \\neq j} a_i a_j = (\\sum_i a_i)^2 - \\sum_i a_i^2$. Let $a_i = \\xi_{i}^{\\mu} s_{i}$. Since both $\\xi_{i}^{\\mu}$ and $s_i$ are in $\\{-1, +1\\}$, their product is also in $\\{-1, +1\\}$. Consequently, $a_i^2 = (\\xi_{i}^{\\mu} s_{i})^2 = 1$, and $\\sum_{i=1}^{N} a_i^2 = N$.\n$$\ne(s) = -\\frac{1}{2N^2}\\sum_{\\mu=1}^{P} \\left[ \\left(\\sum_{i=1}^{N} \\xi_{i}^{\\mu} s_{i}\\right)^2 - N \\right]\n$$\nWe define the overlap (or correlation) between the state $s$ and a stored pattern $\\xi^{\\mu}$ as:\n$$\nm_{\\mu,s} \\equiv \\frac{1}{N}\\sum_{i=1}^{N} \\xi_{i}^{\\mu} s_{i}\n$$\nSubstituting this definition into the expression for $e(s)$:\n$$\ne(s) = -\\frac{1}{2N^2}\\sum_{\\mu=1}^{P} \\left[ (N m_{\\mu,s})^2 - N \\right] = -\\frac{1}{2}\\sum_{\\mu=1}^{P} \\left[ m_{\\mu,s}^2 - \\frac{1}{N} \\right] = -\\frac{1}{2}\\sum_{\\mu=1}^{P} m_{\\mu,s}^2 + \\frac{P}{2N}\n$$\nIn the thermodynamic limit $N\\to\\infty$ with the low-load condition $P/N \\to 0$, the term $\\frac{P}{2N}$ vanishes. Also, by the Law of Large Numbers, the empirical average $m_{\\mu,s}$ converges in probability to its statistical expectation, which we denote as $M_{\\mu,s}$:\n$$\n\\lim_{N \\to \\infty} m_{\\mu,s} = \\mathbb{E}[\\xi_{i}^{\\mu} s_{i}] \\equiv M_{\\mu,s}\n$$\nThe randomness for the expectation is over the i.i.d. components of the patterns $\\xi_i^\\sigma$. Thus, the energy per neuron in the limit becomes:\n$$\n\\lim_{N\\to\\infty, P/N\\to 0} e(s) = -\\frac{1}{2}\\sum_{\\mu=1}^{P} M_{\\mu,s}^2\n$$\n\nNow, we apply this general formula to the two states of interest.\n\n1.  **Energy of a stored pattern, $E(\\xi^{\\nu})$**\n\nLet the network state be one of the stored patterns, $s = \\xi^{\\nu}$, for some fixed index $\\nu \\in \\{1, \\dots, P\\}$. We calculate the expected overlaps $M_{\\mu,\\xi^{\\nu}}$:\n$$\nM_{\\mu,\\xi^{\\nu}} = \\mathbb{E}[\\xi_{i}^{\\mu} \\xi_{i}^{\\nu}]\n$$\n-   If $\\mu = \\nu$: $M_{\\nu,\\xi^{\\nu}} = \\mathbb{E}[(\\xi_{i}^{\\nu})^2] = \\mathbb{E}[1] = 1$.\n-   If $\\mu \\neq \\nu$: The patterns $\\xi^{\\mu}$ and $\\xi^{\\nu}$ are independent. Thus, $M_{\\mu,\\xi^{\\nu}} = \\mathbb{E}[\\xi_{i}^{\\mu}] \\mathbb{E}[\\xi_{i}^{\\nu}]$. Since $\\xi_i^\\sigma$ are i.i.d. with $P(\\xi_i^\\sigma=+1)=P(\\xi_i^\\sigma=-1)=1/2$, the expectation is $\\mathbb{E}[\\xi_i^\\sigma] = \\frac{1}{2}(+1) + \\frac{1}{2}(-1) = 0$. So, $M_{\\mu,\\xi^{\\nu}} = 0 \\cdot 0 = 0$.\n\nThe sum $\\sum_{\\mu=1}^{P} M_{\\mu,\\xi^{\\nu}}^2$ has only one non-zero term, which occurs at $\\mu = \\nu$.\n$$\n\\sum_{\\mu=1}^{P} M_{\\mu,\\xi^{\\nu}}^2 = M_{\\nu,\\xi^{\\nu}}^2 + \\sum_{\\mu \\neq \\nu} M_{\\mu,\\xi^{\\nu}}^2 = 1^2 + 0 = 1\n$$\nThe energy per neuron for any stored pattern is therefore:\n$$\n\\lim_{N\\to\\infty}\\frac{1}{N}E(\\xi^{\\nu}) = -\\frac{1}{2}(1) = -\\frac{1}{2}\n$$\n\n2.  **Energy of the spurious mixture state, $E(s^{\\mathrm{mix}})$**\n\nLet the mixture state be formed from three distinct patterns, which we can label without loss of generality as $\\xi^1, \\xi^2, \\xi^3$. The state is given by $s_{i}^{\\mathrm{mix}} = \\mathrm{sgn}(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3})$. We calculate the expected overlaps $M_{\\mu,s^{\\mathrm{mix}}}$:\n$$\nM_{\\mu,s^{\\mathrm{mix}}} = \\mathbb{E}[\\xi_{i}^{\\mu} s_{i}^{\\mathrm{mix}}] = \\mathbb{E}[\\xi_{i}^{\\mu} \\, \\mathrm{sgn}(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3})]\n$$\n-   If $\\mu \\in \\{1, 2, 3\\}$: By symmetry, the overlaps are equal for these three patterns. Let's calculate for $\\mu=1$:\n    $M_{1,s^{\\mathrm{mix}}} = \\mathbb{E}[\\xi_{i}^{1} \\, \\mathrm{sgn}(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3})]$. We average over the $2^3=8$ equally likely combinations of $(\\xi_i^1, \\xi_i^2, \\xi_i^3) \\in \\{-1,+1\\}^3$.\n    -   $(+1,+1,+1)$: term is $(+1)\\mathrm{sgn}(3)=+1$\n    -   $(+1,+1,-1)$: term is $(+1)\\mathrm{sgn}(1)=+1$\n    -   $(+1,-1,+1)$: term is $(+1)\\mathrm{sgn}(1)=+1$\n    -   $(+1,-1,-1)$: term is $(+1)\\mathrm{sgn}(-1)=-1$\n    -   $(-1,+1,+1)$: term is $(-1)\\mathrm{sgn}(1)=-1$\n    -   $(-1,+1,-1)$: term is $(-1)\\mathrm{sgn}(-1)=+1$\n    -   $(-1,-1,+1)$: term is $(-1)\\mathrm{sgn}(-1)=+1$\n    -   $(-1,-1,-1)$: term is $(-1)\\mathrm{sgn}(-3)=+1$\n    The expectation is the sum of these values divided by $8$:\n    $M_{1,s^{\\mathrm{mix}}} = \\frac{1}{8}(1+1+1-1-1+1+1+1) = \\frac{4}{8} = \\frac{1}{2}$.\n    By symmetry, $M_{1,s^{\\mathrm{mix}}} = M_{2,s^{\\mathrm{mix}}} = M_{3,s^{\\mathrm{mix}}} = \\frac{1}{2}$.\n\n-   If $\\mu \\notin \\{1, 2, 3\\}$: The pattern $\\xi^{\\mu}$ is independent of $\\xi^1, \\xi^2, \\xi^3$.\n    $M_{\\mu,s^{\\mathrm{mix}}} = \\mathbb{E}[\\xi_{i}^{\\mu}] \\mathbb{E}[\\mathrm{sgn}(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3})]$.\n    Since $\\mathbb{E}[\\xi_{i}^{\\mu}] = 0$, we have $M_{\\mu,s^{\\mathrm{mix}}} = 0$.\n\nNow we compute the sum of squared overlaps:\n$$\n\\sum_{\\mu=1}^{P} M_{\\mu,s^{\\mathrm{mix}}}^2 = \\sum_{\\mu=1}^{3} M_{\\mu,s^{\\mathrm{mix}}}^2 + \\sum_{\\mu=4}^{P} M_{\\mu,s^{\\mathrm{mix}}}^2 = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 + 0 = 3 \\times \\frac{1}{4} = \\frac{3}{4}\n$$\nThe energy per neuron for the mixture state is:\n$$\n\\lim_{N\\to\\infty}\\frac{1}{N}E(s^{\\mathrm{mix}}) = -\\frac{1}{2}\\left(\\frac{3}{4}\\right) = -\\frac{3}{8}\n$$\n\n3.  **Difference in energy per neuron, $\\Delta e$**\n\nFinally, we calculate the required difference:\n$$\n\\Delta e = \\lim_{N\\to\\infty}\\frac{1}{N}\\Big(E(s^{\\mathrm{mix}}) - E(\\xi^{\\mu})\\Big) = \\left(-\\frac{3}{8}\\right) - \\left(-\\frac{1}{2}\\right) = -\\frac{3}{8} + \\frac{4}{8} = \\frac{1}{8}\n$$",
            "answer": "$$\\boxed{\\frac{1}{8}}$$"
        },
        {
            "introduction": "An energy landscape is only useful if the network has a mechanism to navigate it, and asynchronous updates provide a dynamical rule for descending into the nearest energy basin. This process is the very essence of pattern completion, where an initial, corrupted state evolves towards a stored memory. In this practice, you will derive the analytical trajectory of this retrieval process, modeling how the overlap with the target pattern improves over time and revealing the exponential nature of convergence to an attractor.",
            "id": "3978314",
            "problem": "Consider a fully connected Hopfield network of $N$ binary neurons with states $s_{i} \\in \\{-1,+1\\}$ that stores a single binary memory pattern $\\xi \\in \\{-1,+1\\}^{N}$ using the standard Hebbian rule $w_{ij} = \\frac{1}{N} \\xi_{i} \\xi_{j}$ for $i \\neq j$ and $w_{ii} = 0$. The network evolves under asynchronous, zero-temperature dynamics: at each elementary update, a single neuron index $i$ is selected uniformly at random and its state is updated according to $s_{i} \\leftarrow \\operatorname{sgn}\\!\\left(h_{i}\\right)$, where $h_{i} = \\sum_{j=1}^{N} w_{ij} s_{j}$ is the local field. The systemâ€™s energy function is given by $E(s) = -\\frac{1}{2} \\sum_{i \\neq j} w_{ij} s_{i} s_{j}$ and decreases monotonically under such asynchronous updates, ensuring convergence to an attractor of the dynamics.\n\nDefine the overlap $m$ between the current state $s$ and the stored pattern $\\xi$ as $m = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_{i} s_{i}$. The initial network state $s(0)$ is constructed by independently flipping exactly $dN$ coordinates of $\\xi$, chosen uniformly at random, with $d \\in [0, \\frac{1}{2})$, so that the initial overlap satisfies $m(0) = 1 - 2d$.\n\nAssume the large-$N$ limit so that self-coupling effects are negligible and mean-field behavior holds for the local fields. Let $k$ denote the total number of asynchronous single-neuron updates applied, and introduce the rescaled update time $\\tau = \\frac{k}{N}$ (so that $\\tau$ is the number of updates per neuron). Under these assumptions, derive the expected overlap trajectory $m(\\tau)$ as a closed-form analytic expression in terms of $d$ and $\\tau$.\n\nYour final answer must be a single closed-form analytical expression for $m(\\tau)$ and must not include any units. If you perform any approximations, justify them and clearly state the regime under which they are valid. No rounding is required.",
            "solution": "The objective is to find the expected overlap trajectory $m(\\tau)$, where $m$ is the overlap between the network state $s$ and the stored pattern $\\xi$, and $\\tau$ is the rescaled time. The overlap is defined as $m = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_{i} s_{i}$.\n\nFirst, we analyze the local field $h_{i}$ acting on neuron $i$. It is given by $h_{i} = \\sum_{j=1}^{N} w_{ij} s_{j}$. Using the Hebbian learning rule $w_{ij} = \\frac{1}{N} \\xi_{i} \\xi_{j}$ for $i \\neq j$ and the condition $w_{ii}=0$, we have:\n$$h_{i} = \\sum_{j \\neq i} \\left(\\frac{1}{N} \\xi_{i} \\xi_{j}\\right) s_{j} = \\frac{\\xi_{i}}{N} \\sum_{j \\neq i} \\xi_{j} s_{j}$$\nThe sum can be rewritten by including and subtracting the $j=i$ term:\n$$\\sum_{j \\neq i} \\xi_{j} s_{j} = \\left(\\sum_{j=1}^{N} \\xi_{j} s_{j}\\right) - \\xi_{i} s_{i}$$\nBy definition, $\\sum_{j=1}^{N} \\xi_{j} s_{j} = N m$. Therefore,\n$$\\sum_{j \\neq i} \\xi_{j} s_{j} = N m - \\xi_{i} s_{i}$$\nSubstituting this back into the expression for $h_{i}$:\n$$h_{i} = \\frac{\\xi_{i}}{N} (N m - \\xi_{i} s_{i}) = \\xi_{i} m - \\frac{\\xi_{i}^{2} s_{i}}{N}$$\nSince the states are binary, $\\xi_{i} \\in \\{-1, +1\\}$, we have $\\xi_{i}^{2} = 1$. The local field is thus:\n$$h_{i} = \\xi_{i} m - \\frac{s_{i}}{N}$$\nThe problem states that we should assume the large-$N$ limit where self-coupling effects are negligible and mean-field behavior holds. The term $-\\frac{s_{i}}{N}$ arises from the absence of self-coupling ($w_{ii}=0$) and becomes vanishingly small as $N \\rightarrow \\infty$. The mean-field approximation implies that the local field is dominated by the average activity, represented by the order parameter $m$. Thus, we can approximate the local field as:\n$$h_{i} \\approx \\xi_{i} m$$\nThe update rule for neuron $i$ at zero temperature is $s_{i} \\leftarrow \\operatorname{sgn}(h_{i})$. Using the approximated field, the new state $s'_{i}$ is:\n$$s'_{i} = \\operatorname{sgn}(\\xi_{i} m) = \\xi_{i} \\operatorname{sgn}(m)$$\nThe initial state is prepared by flipping $dN$ bits of $\\xi$, where $d \\in [0, \\frac{1}{2})$. The initial overlap is $m(0) = 1 - 2d$, which is strictly positive. The dynamics described by the equation for $s'_{i}$ will cause misaligned neurons to align with the pattern $\\xi$, thus increasing the overlap $m$. Therefore, we can expect $m(\\tau)$ to remain positive for all $\\tau \\geq 0$, which implies $\\operatorname{sgn}(m) = +1$. The update rule simplifies to:\n$$s'_{i} = \\xi_{i}$$\nThis means that if a neuron $i$ is chosen for an update, its state is set to $\\xi_{i}$, regardless of its previous state. If the neuron was already aligned ($s_i = \\xi_i$), its state does not change. If it was misaligned ($s_i = -\\xi_i$), it flips to become aligned.\n\nWe can now derive the differential equation for the evolution of the overlap $m(\\tau)$. The rescaled time $\\tau = k/N$ implies that in a small time interval $d\\tau$, a number $dk = N d\\tau$ of single-neuron updates occur.\nLet $N_{-}$ be the number of misaligned neurons, where $s_{i} = -\\xi_{i}$. The overlap $m$ can be expressed in terms of $N_{-}$:\n$$m = \\frac{1}{N} \\sum_{i} \\xi_{i} s_{i} = \\frac{1}{N} \\left( (N - N_{-})(+1) + N_{-}(-1) \\right) = \\frac{N - 2N_{-}}{N} = 1 - \\frac{2N_{-}}{N}$$\nTo find the change in $m$, $dm$, we can find the change in $N_{-}$, $dN_{-}$. At each update step, a neuron is chosen uniformly at random out of $N$ neurons.\nThe number of misaligned neurons is $N_{-}$, and the number of aligned neurons is $N_{+} = N - N_{-}$.\nA decrease in $N_{-}$ occurs only if a misaligned neuron is chosen and updated. The probability of this is $p(\\text{choose misaligned}) = \\frac{N_{-}}{N}$.\nWhen a misaligned neuron is updated, it becomes aligned, so $N_{-}$ decreases by $1$.\nThe expected change in $N_{-}$ per single update step is:\n$$E[\\Delta N_{-}] = \\frac{N_{-}}{N} \\times (-1) + \\frac{N - N_{-}}{N} \\times (0) = -\\frac{N_{-}}{N}$$\nThe rate of change of $N_{-}$ with respect to the number of updates $k$ is $\\frac{dN_{-}}{dk} = -\\frac{N_{-}}{N}$.\nWe change the independent variable from $k$ to the rescaled time $\\tau = k/N$, so $\\frac{dk}{d\\tau} = N$. Using the chain rule:\n$$\\frac{dN_{-}}{d\\tau} = \\frac{dN_{-}}{dk} \\frac{dk}{d\\tau} = \\left(-\\frac{N_{-}}{N}\\right) N = -N_{-}$$\nWe have a differential equation for the number of misaligned neurons: $\\frac{dN_{-}}{d\\tau} = -N_{-}$.\nWe can transform this into an equation for $m(\\tau)$. From $m = 1 - \\frac{2N_{-}}{N}$, we have $N_{-} = \\frac{N(1-m)}{2}$. Differentiating with respect to $\\tau$:\n$$\\frac{dN_{-}}{d\\tau} = \\frac{d}{d\\tau}\\left(\\frac{N(1-m)}{2}\\right) = -\\frac{N}{2} \\frac{dm}{d\\tau}$$\nSubstituting this into the differential equation for $N_{-}$ gives:\n$$-\\frac{N}{2} \\frac{dm}{d\\tau} = -N_{-} = -\\frac{N(1-m)}{2}$$\nSimplifying, we obtain the differential equation for the overlap $m(\\tau)$:\n$$\\frac{dm}{d\\tau} = 1-m$$\nThis is a first-order linear ordinary differential equation. We solve it by separating variables:\n$$\\frac{dm}{1-m} = d\\tau$$\nIntegrating both sides from time $0$ to $\\tau$:\n$$\\int_{m(0)}^{m(\\tau)} \\frac{dm'}{1-m'} = \\int_{0}^{\\tau} d\\tau'$$\n$$[-\\ln(1-m')]_{m(0)}^{m(\\tau)} = [\\tau']_{0}^{\\tau}$$\n$$-\\ln(1-m(\\tau)) + \\ln(1-m(0)) = \\tau$$\n$$\\ln\\left(\\frac{1-m(0)}{1-m(\\tau)}\\right) = \\tau$$\nExponentiating both sides:\n$$\\frac{1-m(0)}{1-m(\\tau)} = \\exp(\\tau)$$\n$$1-m(\\tau) = (1-m(0))\\exp(-\\tau)$$\n$$m(\\tau) = 1 - (1-m(0))\\exp(-\\tau)$$\nThe problem gives the initial overlap $m(0) = 1 - 2d$. Substituting this into the solution:\n$$m(\\tau) = 1 - (1 - (1-2d))\\exp(-\\tau) = 1 - (2d)\\exp(-\\tau)$$\nThis is the closed-form analytic expression for the expected overlap trajectory. It shows that the overlap starts at $m(0) = 1-2d$ and grows exponentially towards the fully retrieved pattern state $m=1$ as $\\tau \\to \\infty$.",
            "answer": "$$\n\\boxed{1 - 2d \\exp(-\\tau)}\n$$"
        },
        {
            "introduction": "The ultimate test of an associative memory is its ability to perform pattern completion from ambiguous or conflicting cues, a task the brain performs effortlessly. This challenge explores how an attractor network resolves competition when an external input contains features of multiple stored memories. By analyzing a network with correlated patterns using the powerful pseudo-inverse learning rule, you will determine the critical balance of cue strengths required for successful retrieval of the target memory, gaining insight into the network's decision-making process.",
            "id": "3978366",
            "problem": "Consider a fully connected attractor network of $N$ binary neurons $s_{i} \\in \\{-1,+1\\}$ with symmetric synaptic couplings $J_{ij} = J_{ji}$ and no self-coupling $J_{ii} = 0$. The network stores two binary patterns $\\boldsymbol{\\xi}^{\\mu} = (\\xi_{1}^{\\mu},\\dots,\\xi_{N}^{\\mu})$ and $\\boldsymbol{\\xi}^{\\nu} = (\\xi_{1}^{\\nu},\\dots,\\xi_{N}^{\\nu})$, where each component $\\xi_{i}^{\\lambda} \\in \\{-1,+1\\}$ for $\\lambda \\in \\{\\mu,\\nu\\}$. Define the pattern correlation\n$$\n\\rho \\equiv \\frac{1}{N} \\sum_{i=1}^{N} \\xi_{i}^{\\mu} \\xi_{i}^{\\nu},\n$$\nand assume $-1  \\rho  1$ and that the patterns are not identical. The synaptic matrix is learned using the pseudo-inverse rule so that the stored patterns are exact fixed points in the absence of inputs:\n$$\nJ_{ij} = \\frac{1}{N} \\sum_{\\lambda,\\kappa \\in \\{\\mu,\\nu\\}} \\xi_{i}^{\\lambda} \\left(C^{-1}\\right)_{\\lambda \\kappa} \\xi_{j}^{\\kappa}, \\quad \\text{with} \\quad C_{\\lambda \\kappa} \\equiv \\frac{1}{N} \\sum_{i=1}^{N} \\xi_{i}^{\\lambda} \\xi_{i}^{\\kappa},\n$$\nand $C = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$.\n\nThe network receives a static external input (cue) $h_{i}$ that is a linear superposition of the two stored patterns,\n$$\nh_{i} = \\beta \\, \\xi_{i}^{\\mu} + \\gamma \\, \\xi_{i}^{\\nu},\n$$\nwith nonnegative strengths $\\beta \\ge 0$ for the target pattern $\\mu$ and $\\gamma \\ge 0$ for the distractor pattern $\\nu$.\n\nThe network dynamics is assumed to monotonically decrease the standard energy (Lyapunov) function\n$$\nE(\\boldsymbol{s}) = -\\frac{1}{2} \\sum_{i \\neq j} J_{ij} s_{i} s_{j} - \\sum_{i=1}^{N} h_{i} s_{i}.\n$$\nDefine successful pattern completion for the target $\\boldsymbol{\\xi}^{\\mu}$ as the condition that the energy of the state aligned with $\\boldsymbol{\\xi}^{\\mu}$ is strictly lower than that of the state aligned with $\\boldsymbol{\\xi}^{\\nu}$, i.e., $E(\\boldsymbol{s}=\\boldsymbol{\\xi}^{\\mu})  E(\\boldsymbol{s}=\\boldsymbol{\\xi}^{\\nu})$, thereby ensuring that the basin of attraction of $\\boldsymbol{\\xi}^{\\mu}$ dominates over that of $\\boldsymbol{\\xi}^{\\nu}$ under these inputs.\n\nCompute, in closed form, the critical ratio $r_{c}$ of the target to distractor strengths,\n$$\nr_{c} \\equiv \\frac{\\beta}{\\gamma},\n$$\nat which the energy of the two aligned states is equal, that is, $E(\\boldsymbol{s}=\\boldsymbol{\\xi}^{\\mu}) = E(\\boldsymbol{s}=\\boldsymbol{\\xi}^{\\nu})$, in terms of $\\rho$. Express your final answer as a single simplified analytic expression. No rounding is required and no units are involved.",
            "solution": "The objective is to find the ratio $r_c = \\frac{\\beta}{\\gamma}$ that satisfies the condition $E(\\boldsymbol{s}=\\boldsymbol{\\xi}^{\\mu}) = E(\\boldsymbol{s}=\\boldsymbol{\\xi}^{\\nu})$. To do this, we must first evaluate the energy $E(\\boldsymbol{s})$ for the states $\\boldsymbol{s}=\\boldsymbol{\\xi}^{\\mu}$ and $\\boldsymbol{s}=\\boldsymbol{\\xi}^{\\nu}$.\n\nThe energy function is given by:\n$$\nE(\\boldsymbol{s}) = -\\frac{1}{2} \\sum_{i \\neq j} J_{ij} s_{i} s_{j} - \\sum_{i=1}^{N} h_{i} s_{i}\n$$\nWe can analyze the two terms separately: the quadratic term $E_{J}(\\boldsymbol{s}) = -\\frac{1}{2} \\sum_{i \\neq j} J_{ij} s_{i} s_{j}$ and the linear (external field) term $E_{h}(\\boldsymbol{s}) = - \\sum_{i=1}^{N} h_{i} s_{i}$.\n\n**Evaluation of the Quadratic Energy Term**\n\nThe pseudo-inverse rule is constructed such that for a stored pattern $\\boldsymbol{\\xi}^{\\alpha}$, the local field from the connections reproduces the pattern itself, i.e., $\\sum_j J_{ij} \\xi_j^\\alpha = \\xi_i^\\alpha$. Let's verify this.\n$$\n\\sum_{j=1}^N J_{ij} \\xi_j^\\mu = \\sum_{j=1}^N \\left( \\frac{1}{N} \\sum_{\\lambda,\\kappa} \\xi_i^\\lambda (C^{-1})_{\\lambda\\kappa} \\xi_j^\\kappa \\right) \\xi_j^\\mu = \\sum_{\\lambda,\\kappa} \\xi_i^\\lambda (C^{-1})_{\\lambda\\kappa} \\left(\\frac{1}{N}\\sum_{j=1}^N \\xi_j^\\kappa \\xi_j^\\mu \\right)\n$$\nThe term in the parenthesis is $C_{\\kappa\\mu}$.\n$$\n= \\sum_{\\lambda,\\kappa} \\xi_i^\\lambda (C^{-1})_{\\lambda\\kappa} C_{\\kappa\\mu} = \\sum_{\\lambda} \\xi_i^\\lambda (C^{-1}C)_{\\lambda\\mu} = \\sum_{\\lambda} \\xi_i^\\lambda \\delta_{\\lambda\\mu} = \\xi_i^\\mu\n$$\nUsing this property, we can directly compute the sum for the quadratic term when $\\boldsymbol{s} = \\boldsymbol{\\xi}^{\\mu}$:\n$$\n\\sum_{i,j} J_{ij} \\xi_i^\\mu \\xi_j^\\mu = \\sum_i \\xi_i^\\mu \\left(\\sum_j J_{ij} \\xi_j^\\mu\\right) = \\sum_i \\xi_i^\\mu (\\xi_i^\\mu) = \\sum_i 1 = N\n$$\nThe condition $J_{ii}=0$ means we should technically use $\\sum_{i \\neq j}$. However, in the large $N$ limit assumed by these models, the contribution from the diagonal is negligible. More formally, the pseudo-inverse rule as given defines fixed points where $\\sum_{j} J_{ij}\\xi_j^\\alpha = \\xi_i^\\alpha$, which is what matters for the energy calculation. The quadratic sum over $i \\neq j$ is $\\sum_i \\xi_i^\\mu (\\sum_{j \\neq i} J_{ij} \\xi_j^\\mu)$. In the large $N$ limit, $\\sum_{j \\neq i} J_{ij} \\xi_j^\\mu \\approx \\sum_{j} J_{ij} \\xi_j^\\mu = \\xi_i^\\mu$. So the sum becomes approximately $N$. The problem is posed in a standard way where this approximation is implicit.\n\nTherefore, the quadratic energy term for $\\boldsymbol{s} = \\boldsymbol{\\xi}^{\\mu}$ is:\n$$\nE_{J}(\\boldsymbol{\\xi}^{\\mu}) = -\\frac{1}{2} \\sum_{i \\neq j} J_{ij} \\xi_i^\\mu \\xi_j^\\mu \\approx -\\frac{1}{2} N\n$$\nBy symmetry of the problem construction, swapping the labels $\\mu \\leftrightarrow \\nu$ leaves the structure of $J_{ij}$ unchanged. The same calculation holds for $\\boldsymbol{s} = \\boldsymbol{\\xi}^{\\nu}$:\n$$\nE_J(\\boldsymbol{\\xi}^{\\nu}) \\approx -\\frac{1}{2}N\n$$\nThis result, that the interaction energy is the same for all stored patterns, is a key feature of the pseudo-inverse learning rule.\n\n**Evaluation of the Linear Energy Term**\n\nThe linear term is $E_{h}(\\boldsymbol{s}) = - \\sum_{i} h_{i} s_{i}$. We substitute $h_i = \\beta \\xi_i^\\mu + \\gamma \\xi_i^\\nu$.\nFor $\\boldsymbol{s} = \\boldsymbol{\\xi}^{\\mu}$:\n$$\nE_{h}(\\boldsymbol{\\xi}^{\\mu}) = - \\sum_{i=1}^{N} (\\beta \\xi_i^\\mu + \\gamma \\xi_i^\\nu) \\xi_i^\\mu = - \\left( \\beta \\sum_i (\\xi_i^\\mu)^2 + \\gamma \\sum_i \\xi_i^\\nu \\xi_i^\\mu \\right)\n$$\nUsing $\\sum_i (\\xi_i^\\mu)^2 = N$ and $\\sum_i \\xi_i^\\nu \\xi_i^\\mu = N\\rho$, we get:\n$$\nE_{h}(\\boldsymbol{\\xi}^{\\mu}) = - (\\beta N + \\gamma N \\rho) = -N(\\beta + \\gamma\\rho)\n$$\nFor $\\boldsymbol{s} = \\boldsymbol{\\xi}^{\\nu}$:\n$$\nE_{h}(\\boldsymbol{\\xi}^{\\nu}) = - \\sum_{i=1}^{N} (\\beta \\xi_i^\\mu + \\gamma \\xi_i^\\nu) \\xi_i^\\nu = - \\left( \\beta \\sum_i \\xi_i^\\mu \\xi_i^\\nu + \\gamma \\sum_i (\\xi_i^\\nu)^2 \\right)\n$$\nUsing $\\sum_i \\xi_i^\\mu \\xi_i^\\nu = N\\rho$ and $\\sum_i (\\xi_i^\\nu)^2 = N$, we get:\n$$\nE_{h}(\\boldsymbol{\\xi}^{\\nu}) = - (\\beta N \\rho + \\gamma N) = -N(\\beta\\rho + \\gamma)\n$$\n\n**Total Energy and Critical Ratio Calculation**\n\nThe total energies are:\n$$\nE(\\boldsymbol{\\xi}^{\\mu}) = E_J(\\boldsymbol{\\xi}^{\\mu}) + E_h(\\boldsymbol{\\xi}^{\\mu}) = -\\frac{N}{2} - N(\\beta + \\gamma\\rho)\n$$\n$$\nE(\\boldsymbol{\\xi}^{\\nu}) = E_J(\\boldsymbol{\\xi}^{\\nu}) + E_h(\\boldsymbol{\\xi}^{\\nu}) = -\\frac{N}{2} - N(\\beta\\rho + \\gamma)\n$$\nThe problem asks for the condition $E(\\boldsymbol{\\xi}^{\\mu}) = E(\\boldsymbol{\\xi}^{\\nu})$:\n$$\n-\\frac{N}{2} - N(\\beta + \\gamma\\rho) = -\\frac{N}{2} - N(\\beta\\rho + \\gamma)\n$$\nThe term $-\\frac{N}{2}$ cancels out. Dividing by $-N$ (since $N$, the number of neurons, is a positive integer):\n$$\n\\beta + \\gamma\\rho = \\beta\\rho + \\gamma\n$$\nNow, we rearrange the terms to solve for the ratio $\\frac{\\beta}{\\gamma}$:\n$$\n\\beta - \\beta\\rho = \\gamma - \\gamma\\rho\n$$\n$$\n\\beta(1 - \\rho) = \\gamma(1 - \\rho)\n$$\nThe problem states that $-1  \\rho  1$, which implies $\\rho \\neq 1$ and therefore $(1-\\rho) \\neq 0$. We can safely divide both sides by $(1-\\rho)$:\n$$\n\\beta = \\gamma\n$$\nThe critical ratio $r_c$ is defined as $r_c \\equiv \\frac{\\beta}{\\gamma}$. Since $\\gamma \\ge 0$ and a ratio is sought, we can assume $\\gamma  0$. If $\\gamma=0$, then $\\beta=0$ and the ratio is indeterminate, but this is a degenerate case with no external field. For any non-zero cue, we have:\n$$\nr_c = \\frac{\\beta}{\\gamma} = 1\n$$\nThe critical ratio is $1$, independent of the correlation $\\rho$.",
            "answer": "$$\n\\boxed{1}\n$$"
        }
    ]
}