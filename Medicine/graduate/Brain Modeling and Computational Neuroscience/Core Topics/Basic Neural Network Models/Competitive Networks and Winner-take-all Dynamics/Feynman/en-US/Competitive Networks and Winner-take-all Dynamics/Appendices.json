{
    "hands_on_practices": [
        {
            "introduction": "A fundamental question in computational neuroscience is how a population of neurons transitions from a state of uniform activity to one where a clear \"winner\" emerges. This exercise provides a hands-on analysis of this very process using the classic Wilson-Cowan model of excitatory and inhibitory populations. By applying the powerful techniques of linear stability analysis and modal decomposition, you will derive the precise parameter condition that triggers a symmetry-breaking bifurcation, marking the onset of winner-take-all (WTA) dynamics .",
            "id": "3970003",
            "problem": "Consider a canonical competitive circuit with two identical excitatory (E) populations, indexed by $i \\in \\{1,2\\}$, and one shared inhibitory (I) population, governed by Wilson–Cowan dynamics. Let $E_{i}(t)$ and $I(t)$ denote the population activities. The dynamics are\n$$\n\\tau_{E}\\,\\dot{E}_{i} \\;=\\; -E_{i} \\;+\\; \\phi\\!\\left( b \\;+\\; w_{s}\\,E_{i} \\;+\\; w_{m}\\,E_{j} \\;-\\; w_{EI}\\,I \\right), \\quad j \\neq i,\\; i \\in \\{1,2\\},\n$$\n$$\n\\tau_{I}\\,\\dot{I} \\;=\\; -I \\;+\\; \\psi\\!\\left( w_{IE}\\,(E_{1}+E_{2}) \\right),\n$$\nwhere $\\tau_{E}  0$ and $\\tau_{I}  0$ are time constants, $b \\in \\mathbb{R}$ is a constant bias to the excitatory populations, $w_{s} \\in \\mathbb{R}$ is the excitatory self-coupling, $w_{m} \\in \\mathbb{R}$ is the excitatory mutual coupling, $w_{EI}  0$ is the inhibitory feedback strength from $I$ to each $E_{i}$, $w_{IE}  0$ is the excitatory drive from the $E$ populations to $I$, and $\\phi$ and $\\psi$ are continuously differentiable, monotonically increasing input-output functions (e.g., sigmoids). Suppose there exists a symmetric fixed point with $E_{1}^{*} = E_{2}^{*} = E^{*}$ and $I^{*}$, and define the slopes at the fixed point as $g_{E} := \\phi^{\\prime}(u_{E}^{*})  0$ and $g_{I} := \\psi^{\\prime}(u_{I}^{*})  0$, where $u_{E}^{*} := b + (w_{s}+w_{m})\\,E^{*} - w_{EI}\\,I^{*}$ and $u_{I}^{*} := w_{IE}\\,(E_{1}^{*}+E_{2}^{*}) = 2\\,w_{IE}\\,E^{*}$.\n\nStarting from the above Wilson–Cowan equations and the definitions of $g_{E}$ and $g_{I}$, use linearization about the symmetric fixed point and a modal decomposition into symmetric and antisymmetric perturbations to determine the exact analytical expression for the critical value of the excitatory coupling difference\n$$\n\\Delta w \\;:=\\; w_{s} - w_{m}\n$$\nat which the symmetric fixed point loses stability to an antisymmetric mode, thereby marking the onset of winner-take-all (WTA) dynamics in which one excitatory population suppresses the other. Express your final answer for the critical value $\\Delta w_{c}$ solely in terms of $g_{E}$. Your final answer must be a single closed-form analytic expression. No numerical approximation or rounding is required.",
            "solution": "The problem asks for the critical value of the excitatory coupling difference, $\\Delta w = w_s - w_m$, at which a symmetric fixed point of a Wilson-Cowan circuit loses stability to an antisymmetric mode. We begin by linearizing the given system of differential equations around the symmetric fixed point $(E_1^*, E_2^*, I^*) = (E^*, E^*, I^*)$.\n\nLet the activities be perturbed from the fixed point such that $E_i(t) = E^* + \\delta E_i(t)$ for $i \\in \\{1,2\\}$, and $I(t) = I^* + \\delta I(t)$. The linearized dynamics for the perturbations $\\delta E_1$, $\\delta E_2$, and $\\delta I$ are derived by performing a first-order Taylor expansion of the nonlinear functions $\\phi$ and $\\psi$ around their fixed-point arguments.\n\nThe given dynamical equations are:\n$$\n\\tau_{E}\\,\\dot{E}_{i} \\;=\\; -E_{i} \\;+\\; \\phi\\!\\left( b \\;+\\; w_{s}\\,E_{i} \\;+\\; w_{m}\\,E_{j} \\;-\\; w_{EI}\\,I \\right), \\quad j \\neq i\n$$\n$$\n\\tau_{I}\\,\\dot{I} \\;=\\; -I \\;+\\; \\psi\\!\\left( w_{IE}\\,(E_{1}+E_{2}) \\right)\n$$\nAt the symmetric fixed point, we have $E_1^*=E_2^*=E^*$, so $E^* = \\phi(u_E^*)$ and $I^* = \\psi(u_I^*)$, where $u_E^* = b + (w_s+w_m)E^* - w_{EI}I^*$ and $u_I^* = 2w_{IE}E^*$.\n\nFor the first excitatory population ($i=1, j=2$), we substitute the perturbed variables:\n$$\n\\tau_E \\frac{d}{dt}(E^* + \\delta E_1) = -(E^* + \\delta E_1) + \\phi(b + w_s(E^*+\\delta E_1) + w_m(E^*+\\delta E_2) - w_{EI}(I^*+\\delta I))\n$$\nExpanding the function $\\phi$ around $u_E^*$:\n$$\n\\phi(u_E^* + w_s\\delta E_1 + w_m\\delta E_2 - w_{EI}\\delta I) \\approx \\phi(u_E^*) + \\phi'(u_E^*)(w_s\\delta E_1 + w_m\\delta E_2 - w_{EI}\\delta I)\n$$\nUsing the definitions $E^* = \\phi(u_E^*)$ and $g_E = \\phi'(u_E^*)$, this becomes:\n$$\n\\phi(...) \\approx E^* + g_E (w_s\\delta E_1 + w_m\\delta E_2 - w_{EI}\\delta I)\n$$\nSubstituting this into the dynamic equation for $E_1$ and canceling the fixed-point terms gives the linearized equation for $\\delta E_1$:\n$$\n\\tau_E \\dot{\\delta E_1} = -\\delta E_1 + g_E (w_s\\delta E_1 + w_m\\delta E_2 - w_{EI}\\delta I)\n$$\n$$\n\\tau_E \\dot{\\delta E_1} = (g_E w_s - 1)\\delta E_1 + g_E w_m\\delta E_2 - g_E w_{EI}\\delta I\n$$\nBy symmetry, the equation for $\\delta E_2$ is obtained by swapping the indices $1$ and $2$:\n$$\n\\tau_E \\dot{\\delta E_2} = g_E w_m\\delta E_1 + (g_E w_s - 1)\\delta E_2 - g_E w_{EI}\\delta I\n$$\nFor the inhibitory population, we linearize the $\\psi$ function:\n$$\n\\psi(u_I^* + w_{IE}(\\delta E_1 + \\delta E_2)) \\approx \\psi(u_I^*) + \\psi'(u_I^*)(w_{IE}(\\delta E_1 + \\delta E_2))\n$$\nUsing $I^*=\\psi(u_I^*)$ and $g_I=\\psi'(u_I^*)$, we get:\n$$\n\\psi(...) \\approx I^* + g_I w_{IE}(\\delta E_1 + \\delta E_2)\n$$\nThe linearized equation for $\\delta I$ is:\n$$\n\\tau_I \\dot{\\delta I} = -\\delta I + g_I w_{IE}(\\delta E_1 + \\delta E_2)\n$$\nThe problem specifies using a modal decomposition. We define the symmetric mode $\\delta E_s = \\delta E_1 + \\delta E_2$ and the antisymmetric mode $\\delta E_a = \\delta E_1 - \\delta E_2$.\n\nTo find the dynamics of the antisymmetric mode, we subtract the linearized equation for $\\delta E_2$ from the one for $\\delta E_1$:\n$$\n\\tau_E (\\dot{\\delta E_1} - \\dot{\\delta E_2}) = \\left((g_E w_s - 1)\\delta E_1 + g_E w_m\\delta E_2\\right) - \\left(g_E w_m\\delta E_1 + (g_E w_s - 1)\\delta E_2\\right)\n$$\nThe inhibitory terms $-g_E w_{EI}\\delta I$ cancel out. Grouping terms by $\\delta E_1$ and $\\delta E_2$:\n$$\n\\tau_E \\dot{\\delta E_a} = (g_E w_s - 1 - g_E w_m)\\delta E_1 + (g_E w_m - (g_E w_s - 1))\\delta E_2\n$$\n$$\n\\tau_E \\dot{\\delta E_a} = (g_E(w_s - w_m) - 1)\\delta E_1 - (g_E(w_s - w_m) - 1)\\delta E_2\n$$\n$$\n\\tau_E \\dot{\\delta E_a} = (g_E(w_s - w_m) - 1)(\\delta E_1 - \\delta E_2)\n$$\nSubstituting $\\delta E_a = \\delta E_1 - \\delta E_2$ and $\\Delta w = w_s - w_m$:\n$$\n\\tau_E \\dot{\\delta E_a} = (g_E \\Delta w - 1)\\delta E_a\n$$\nThis is a first-order linear ordinary differential equation for the antisymmetric mode $\\delta E_a$. The dynamics of this mode are decoupled from the symmetric and inhibitory modes. The stability of the fixed point with respect to antisymmetric perturbations is determined by the sign of the coefficient of $\\delta E_a$. The associated eigenvalue is:\n$$\n\\lambda_a = \\frac{g_E \\Delta w - 1}{\\tau_E}\n$$\nThe symmetric fixed point is stable to antisymmetric perturbations if $\\lambda_a  0$. It loses stability when the eigenvalue crosses zero, i.e., at $\\lambda_a = 0$. This bifurcation point marks the onset of winner-take-all dynamics, where any small antisymmetric perturbation will grow, breaking the symmetry of the system.\n\nWe find the critical value of the coupling difference, $\\Delta w_c$, by setting $\\lambda_a=0$:\n$$\n\\frac{g_E \\Delta w_c - 1}{\\tau_E} = 0\n$$\nSince $\\tau_E  0$, this requires the numerator to be zero:\n$$\ng_E \\Delta w_c - 1 = 0\n$$\nSolving for $\\Delta w_c$ gives the desired expression:\n$$\n\\Delta w_c = \\frac{1}{g_E}\n$$\nThis is the exact analytical expression for the critical value of the excitatory coupling difference at which the symmetric fixed point becomes unstable to an antisymmetric mode, expressed solely in terms of $g_E$.",
            "answer": "$$\n\\boxed{\\frac{1}{g_E}}\n$$"
        },
        {
            "introduction": "Once a winner has emerged from the competition, what network properties ensure this state is stable and maintained over time? This practice directly addresses the stability of the outcome, a critical counterpart to understanding the onset of competition . You will derive the Jacobian matrix for a network with a single active neuron and analyze its eigenvalues to reveal the conditions necessary to keep the winner active and all other units suppressed, thus ensuring a robust winner-take-all state .",
            "id": "3970082",
            "problem": "Consider a recurrent competitive rate network of $N$ neurons with threshold-linear activation, governed by the dynamical system\n$$\n\\tau \\,\\dot{\\mathbf{r}} \\;=\\; -\\,\\ell\\,\\mathbf{r} \\;+\\; \\boldsymbol{\\phi}\\!\\left( W\\,\\mathbf{r} \\;+\\; \\mathbf{I} \\;-\\; \\boldsymbol{\\theta} \\right),\n$$\nwhere $\\mathbf{r} \\in \\mathbb{R}^{N}$ is the vector of firing rates, $\\tau  0$ is a membrane time constant, $\\ell  0$ is a leak gain, $W \\in \\mathbb{R}^{N \\times N}$ is the synaptic weight matrix, $\\mathbf{I} \\in \\mathbb{R}^{N}$ is a constant external input, and $\\boldsymbol{\\theta} \\in \\mathbb{R}^{N}$ is a vector of thresholds. The nonlinearity is applied elementwise and is given by the threshold-linear function with gain $g0$,\n$$\n\\phi(u) \\;=\\; g\\,\\max(0,\\,u).\n$$\nAssume that there exists a candidate single-winner fixed point in which exactly one neuron, indexed by $k \\in \\{1,\\dots,N\\}$, is active with $r_{k}^{\\star}  0$, and all others are inactive with $r_{i}^{\\star} = 0$ for all $i \\neq k$. Further assume $W_{kk}  0$, $\\tau  0$, $\\ell  0$, and $g  0$, and that the fixed point feasibility conditions (positivity for the active unit and subthreshold input for inactive units) are satisfied.\n\nStarting from the definitions of a fixed point, the chain rule for the Jacobian of a composition, and the derivative of the threshold-linear function, perform the following:\n\n1) Derive the Jacobian matrix of the system at the candidate single-winner fixed point in terms of $\\tau$, $\\ell$, $g$, $W$, and the active index $k$.\n\n2) From the Jacobian, derive the full eigenvalue spectrum and state the condition on $g$ and $W_{kk}$ that ensures all eigenvalues have strictly negative real parts (local asymptotic stability).\n\n3) Let $g_{\\mathrm{crit}}$ denote the largest value of $g$ such that the linearization at the single-winner fixed point is locally asymptotically stable. Provide $g_{\\mathrm{crit}}$ as a closed-form analytic expression in terms of $\\ell$ and $W_{kk}$.\n\nGive your final answer as the expression for $g_{\\mathrm{crit}}$. No numerical evaluation is required, and no units are to be included in the final answer.",
            "solution": "The problem asks for an analysis of the local stability of a single-winner fixed point in a recurrent competitive rate network. The dynamics of the network are given by\n$$\n\\tau \\,\\dot{\\mathbf{r}} \\;=\\; -\\,\\ell\\,\\mathbf{r} \\;+\\; \\boldsymbol{\\phi}\\!\\left( W\\,\\mathbf{r} \\;+\\; \\mathbf{I} \\;-\\; \\boldsymbol{\\theta} \\right)\n$$\nLet us denote the right-hand side of the unscaled equation as $\\mathbf{F}(\\mathbf{r}) = -\\ell\\,\\mathbf{r} \\;+\\; \\boldsymbol{\\phi}\\!\\left( W\\,\\mathbf{r} \\;+\\; \\mathbf{I} \\;-\\; \\boldsymbol{\\theta} \\right)$. The system can then be written as $\\dot{\\mathbf{r}} = \\frac{1}{\\tau}\\mathbf{F}(\\mathbf{r})$. The stability of a fixed point $\\mathbf{r}^{\\star}$ is determined by the eigenvalues of the Jacobian matrix $J$ of the vector field $\\frac{1}{\\tau}\\mathbf{F}(\\mathbf{r})$, evaluated at $\\mathbf{r}^{\\star}$.\n\n### 1. Derivation of the Jacobian Matrix\n\nThe Jacobian matrix $J$ has elements $J_{ij} = \\frac{\\partial \\dot{r}_i}{\\partial r_j}$. We have\n$$\nJ_{ij} = \\frac{1}{\\tau} \\frac{\\partial F_i}{\\partial r_j} = \\frac{1}{\\tau} \\frac{\\partial}{\\partial r_j} \\left( -\\ell r_i + \\phi(u_i) \\right)\n$$\nwhere $u_i = (W\\mathbf{r} + \\mathbf{I} - \\boldsymbol{\\theta})_i = \\sum_{m=1}^{N} W_{im} r_m + I_i - \\theta_i$.\n\nApplying the chain rule, we get\n$$\n\\frac{\\partial F_i}{\\partial r_j} = -\\ell \\frac{\\partial r_i}{\\partial r_j} + \\frac{d\\phi}{du_i} \\frac{\\partial u_i}{\\partial r_j} = -\\ell \\delta_{ij} + \\phi'(u_i) W_{ij}\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta and $\\phi'(u)$ is the derivative of the activation function $\\phi(u) = g \\max(0,u)$. The derivative is $\\phi'(u) = g$ for $u0$ and $\\phi'(u) = 0$ for $u0$.\n\nWe evaluate the Jacobian at the specified single-winner fixed point $\\mathbf{r}^{\\star}$, where $r_k^{\\star}  0$ for a single index $k$, and $r_i^{\\star} = 0$ for all $i \\neq k$. The feasibility conditions for this fixed point imply that the total input $u_i^{\\star}$ satisfies $u_k^{\\star}  0$ for the active neuron and $u_i^{\\star} \\le 0$ for the inactive neurons ($i \\neq k$). For the purpose of calculating the Jacobian, we assume the generic case where for inactive neurons, the input is strictly sub-threshold, i.e., $u_i^{\\star}  0$ for $i \\neq k$. This is a standard assumption in the stability analysis of such winner-take-all states.\n\nUnder these conditions, the derivative of the activation function at the fixed point is:\n-   For the active neuron ($i=k$): $u_k^{\\star}  0 \\implies \\phi'(u_k^{\\star}) = g$.\n-   For inactive neurons ($i \\neq k$): $u_i^{\\star}  0 \\implies \\phi'(u_i^{\\star}) = 0$.\n\nLet's denote the Jacobian of $\\mathbf{F}$ at $\\mathbf{r}^{\\star}$ as $\\tilde{J}$. Its elements $\\tilde{J}_{ij} = \\frac{\\partial F_i}{\\partial r_j}\\big|_{\\mathbf{r}=\\mathbf{r}^{\\star}}$ are:\n$$\n\\tilde{J}_{ij} = -\\ell \\delta_{ij} + \\phi'(u_i^{\\star}) W_{ij}\n$$\nWe can distinguish between the row corresponding to the active neuron ($i=k$) and the rows for the inactive neurons ($i \\neq k$):\n-   If $i=k$: $\\tilde{J}_{kj} = -\\ell \\delta_{kj} + \\phi'(u_k^{\\star}) W_{kj} = -\\ell \\delta_{kj} + g W_{kj}$.\n-   If $i \\neq k$: $\\tilde{J}_{ij} = -\\ell \\delta_{ij} + \\phi'(u_i^{\\star}) W_{ij} = -\\ell \\delta_{ij} + 0 \\cdot W_{ij} = -\\ell \\delta_{ij}$.\n\nThis structure can be written compactly using vector notation. Let $\\mathbf{e}_k$ be the standard basis vector with a $1$ at position $k$, and let $\\mathbf{w}_k^T$ be the $k$-th row of the weight matrix $W$. The matrix $\\tilde{J}$ can be expressed as:\n$$\n\\tilde{J} = -\\ell I + g \\mathbf{e}_k \\mathbf{w}_k^T\n$$\nwhere $I$ is the $N \\times N$ identity matrix. The term $g \\mathbf{e}_k \\mathbf{w}_k^T$ is a matrix that is zero everywhere except for its $k$-th row, which is equal to $g \\mathbf{w}_k^T$. Adding this to $-\\ell I$ correctly yields the elements derived above.\n\nThe full system Jacobian is $J = \\frac{1}{\\tau}\\tilde{J}$.\n$$\nJ = \\frac{1}{\\tau} \\left( -\\ell I + g \\mathbf{e}_k \\mathbf{w}_k^T \\right)\n$$\nThis completes the derivation of the Jacobian matrix.\n\n### 2. Eigenvalue Spectrum and Stability Condition\n\nTo determine the local stability of the fixed point, we must find the eigenvalues of $J$. The eigenvalues of $J$ are the eigenvalues of $\\tilde{J}$ divided by $\\tau$. Let $\\lambda_{\\tilde{J}}$ be an eigenvalue of $\\tilde{J}$ with corresponding eigenvector $\\mathbf{v}$. The eigenvalue equation is:\n$$\n\\tilde{J} \\mathbf{v} = \\lambda_{\\tilde{J}} \\mathbf{v}\n$$\n$$\n\\left( -\\ell I + g \\mathbf{e}_k \\mathbf{w}_k^T \\right) \\mathbf{v} = \\lambda_{\\tilde{J}} \\mathbf{v}\n$$\n$$\n-\\ell \\mathbf{v} + g (\\mathbf{w}_k^T \\mathbf{v}) \\mathbf{e}_k = \\lambda_{\\tilde{J}} \\mathbf{v}\n$$\nRearranging the terms, we get:\n$$\ng (\\mathbf{w}_k^T \\mathbf{v}) \\mathbf{e}_k = (\\lambda_{\\tilde{J}} + \\ell) \\mathbf{v}\n$$\nWe analyze two cases for the scalar product $\\mathbf{w}_k^T \\mathbf{v}$:\n\nCase 1: $\\mathbf{w}_k^T \\mathbf{v} = 0$.\nThe equation becomes $(\\lambda_{\\tilde{J}} + \\ell) \\mathbf{v} = \\mathbf{0}$. Since $\\mathbf{v}$ is an eigenvector, $\\mathbf{v} \\neq \\mathbf{0}$, which implies $\\lambda_{\\tilde{J}} + \\ell = 0$. Thus, $\\lambda_{\\tilde{J}} = -\\ell$. The space of vectors orthogonal to $\\mathbf{w}_k$ has dimension $N-1$. Therefore, there are $N-1$ linearly independent eigenvectors corresponding to this eigenvalue. This eigenvalue has a multiplicity of $N-1$.\n\nCase 2: $\\mathbf{w}_k^T \\mathbf{v} \\neq 0$.\nThe equation $g (\\mathbf{w}_k^T \\mathbf{v}) \\mathbf{e}_k = (\\lambda_{\\tilde{J}} + \\ell) \\mathbf{v}$ shows that the eigenvector $\\mathbf{v}$ must be parallel to the basis vector $\\mathbf{e}_k$. So we can set $\\mathbf{v} = \\mathbf{e}_k$. Substituting this into the eigenvalue equation:\n$$\n\\tilde{J} \\mathbf{e}_k = \\lambda_{\\tilde{J}} \\mathbf{e}_k\n$$\n$$\n\\left( -\\ell I + g \\mathbf{e}_k \\mathbf{w}_k^T \\right) \\mathbf{e}_k = \\lambda_{\\tilde{J}} \\mathbf{e}_k\n$$\n$$\n-\\ell \\mathbfe_k + g \\mathbf{e}_k (\\mathbf{w}_k^T \\mathbf{e}_k) = \\lambda_{\\tilde{J}} \\mathbf{e}_k\n$$\nThe scalar product $\\mathbf{w}_k^T \\mathbf{e}_k$ is simply the $k$-th element of the vector $\\mathbf{w}_k$, which is $W_{kk}$.\n$$\n-\\ell \\mathbf{e}_k + g W_{kk} \\mathbf{e}_k = \\lambda_{\\tilde{J}} \\mathbf{e}_k\n$$\n$$\n(-\\ell + g W_{kk}) \\mathbf{e}_k = \\lambda_{\\tilde{J}} \\mathbf{e}_k\n$$\nThis gives the remaining eigenvalue: $\\lambda_{\\tilde{J}} = -\\ell + g W_{kk}$.\n\nThe spectrum of $\\tilde{J}$ consists of the eigenvalue $-\\ell$ with multiplicity $N-1$ and the eigenvalue $-\\ell + g W_{kk}$ with multiplicity $1$. The eigenvalues of the full Jacobian $J$ are these values scaled by $1/\\tau$:\n-   $\\lambda_{1, \\dots, N-1} = -\\frac{\\ell}{\\tau}$\n-   $\\lambda_N = \\frac{1}{\\tau}(-\\ell + g W_{kk})$\n\nFor local asymptotic stability, all eigenvalues must have strictly negative real parts. The parameters $\\ell$ and $\\tau$ are given as positive, so $\\lambda_{1, \\dots, N-1} = -\\frac{\\ell}{\\tau}$ is always real and strictly negative.\nThe stability is thus determined by the sign of $\\lambda_N$. Since all parameters are real, we require $\\lambda_N  0$:\n$$\n\\frac{1}{\\tau}(-\\ell + g W_{kk})  0\n$$\nAs $\\tau  0$, this is equivalent to:\n$$\n-\\ell + g W_{kk}  0 \\implies g W_{kk}  \\ell\n$$\nThis is the condition for local asymptotic stability of the single-winner fixed point.\n\n### 3. Critical Gain $g_{\\mathrm{crit}}$\n\nThe critical gain $g_{\\mathrm{crit}}$ is defined as the largest value of $g$ for which the system is locally asymptotically stable. The stability condition is $g W_{kk}  \\ell$. Given that $W_{kk}  0$, we can rearrange the inequality to solve for $g$:\n$$\ng  \\frac{\\ell}{W_{kk}}\n$$\nThe set of values for $g$ that ensure stability is the open interval $(0, \\ell/W_{kk})$, as $g$ must also be positive. The largest value of $g$ for which this strict inequality holds is the supremum of this interval. At this boundary value, the eigenvalue $\\lambda_N$ becomes zero, and the system loses asymptotic stability, typically through a bifurcation.\nTherefore, the critical gain is:\n$$\ng_{\\mathrm{crit}} = \\frac{\\ell}{W_{kk}}\n$$",
            "answer": "$$\n\\boxed{\\frac{\\ell}{W_{kk}}}\n$$"
        },
        {
            "introduction": "Biological neural circuits are inherently noisy, a factor that can significantly impact the reliability of computation. This exercise moves from deterministic models to a more realistic stochastic framework to explore how noise affects competitive decision-making. By applying concepts from signal detection theory, you will derive an effective discrimination index, $d'_{\\mathrm{eff}}$, to quantify how correlations in neural noise can either enhance or impair the ability of a WTA circuit to make a correct choice .",
            "id": "3970014",
            "problem": "Consider a two-unit winner-take-all (WTA) circuit in a competitive network within the brain modeling and computational neuroscience context. Each unit receives an input that consists of a deterministic component and stochastic noise, and the circuit selects the unit with the largest instantaneous activity. Let the activities be $r_{1} = s_{1} + \\eta_{1}$ and $r_{2} = s_{2} + \\eta_{2}$, where $s_{i}$ is the deterministic input to unit $i$ and $\\eta_{i}$ is noise. Assume the noise vector $(\\eta_{1}, \\eta_{2})$ is zero-mean bivariate Gaussian with covariance matrix\n$$\n\\Sigma = \\sigma^{2}\n\\begin{pmatrix}\n1  \\rho \\\\\n\\rho  1\n\\end{pmatrix},\n$$\nwhere $\\sigma^{2}  0$ is the marginal variance and $\\rho \\in (-1,1)$ is the Pearson correlation coefficient, reflecting correlated fluctuations due to shared circuit mechanisms (for example, common inhibitory feedback can produce $\\rho  0$, whereas shared modulatory drive can produce $\\rho  0$). Let the deterministic inputs be separated by a fixed margin so that $s_{1} - s_{2} = \\Delta  0$.\n\nUsing only standard properties of Gaussian random variables and the definition of WTA selection as choosing the larger activity, define an effective discrimination index for the WTA decision between these two units as\n$$\nd'_{\\mathrm{eff}} = \\frac{\\mathbb{E}[r_{1}] - \\mathbb{E}[r_{2}]}{\\sqrt{\\mathrm{Var}(r_{1} - r_{2})}},\n$$\nand derive a closed-form analytic expression for $d'_{\\mathrm{eff}}$ as a function of $\\Delta$, $\\sigma$, and $\\rho$. Briefly explain, in words, how the sign and magnitude of $\\rho$ influence the reliability of WTA in this two-unit setting through their effect on $d'_{\\mathrm{eff}}$. Your final answer must be the single closed-form expression for $d'_{\\mathrm{eff}}$ and should not include any units.",
            "solution": "The problem provides a model for the activities of two units, $r_{1}$ and $r_{2}$, in a winner-take-all (WTA) circuit. The activities are given by $r_{1} = s_{1} + \\eta_{1}$ and $r_{2} = s_{2} + \\eta_{2}$, where $s_{i}$ are deterministic inputs and $\\eta_{i}$ are stochastic noise components. The problem asks for the derivation of an effective discrimination index, defined as\n$$\nd'_{\\mathrm{eff}} = \\frac{\\mathbb{E}[r_{1}] - \\mathbb{E}[r_{2}]}{\\sqrt{\\mathrm{Var}(r_{1} - r_{2})}}\n$$\nWe will calculate the numerator and the denominator separately using the provided information.\n\nFirst, let's analyze the numerator, which is the difference in the expected activities, $\\mathbb{E}[r_{1}] - \\mathbb{E}[r_{2}]$.\nThe expectation operator is linear. The expected value of each activity $r_{i}$ is:\n$$\n\\mathbb{E}[r_{i}] = \\mathbb{E}[s_{i} + \\eta_{i}] = \\mathbb{E}[s_{i}] + \\mathbb{E}[\\eta_{i}]\n$$\nSince $s_{i}$ is a deterministic component, its expected value is simply the value itself: $\\mathbb{E}[s_{i}] = s_{i}$.\nThe problem states that the noise vector $(\\eta_{1}, \\eta_{2})$ is zero-mean, which implies $\\mathbb{E}[\\eta_{1}] = 0$ and $\\mathbb{E}[\\eta_{2}] = 0$.\nTherefore, the expected activities are:\n$$\n\\mathbb{E}[r_{1}] = s_{1} + 0 = s_{1}\n$$\n$$\n\\mathbb{E}[r_{2}] = s_{2} + 0 = s_{2}\n$$\nThe numerator of $d'_{\\mathrm{eff}}$ is then the difference between these expected values:\n$$\n\\mathbb{E}[r_{1}] - \\mathbb{E}[r_{2}] = s_{1} - s_{2}\n$$\nThe problem specifies that the deterministic inputs are separated by a fixed margin $\\Delta = s_{1} - s_{2}$. Thus, the numerator is $\\Delta$.\n\nNext, let's analyze the denominator, which is the standard deviation of the difference in activities, $\\sqrt{\\mathrm{Var}(r_{1} - r_{2})}$. To find this, we first compute the variance, $\\mathrm{Var}(r_{1} - r_{2})$.\nLet the difference in activities be a new random variable $D = r_{1} - r_{2}$.\n$$\nD = (s_{1} + \\eta_{1}) - (s_{2} + \\eta_{2}) = (s_{1} - s_{2}) + (\\eta_{1} - \\eta_{2})\n$$\nThe variance of a random variable shifted by a constant is equal to the variance of the original random variable. Here, $(s_{1} - s_{2})$ is a constant. Therefore:\n$$\n\\mathrm{Var}(r_{1} - r_{2}) = \\mathrm{Var}((s_{1} - s_{2}) + (\\eta_{1} - \\eta_{2})) = \\mathrm{Var}(\\eta_{1} - \\eta_{2})\n$$\nFor any two random variables $X$ and $Y$, the variance of their difference is given by the formula $\\mathrm{Var}(X - Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) - 2\\mathrm{Cov}(X, Y)$. Applying this to our noise variables $\\eta_{1}$ and $\\eta_{2}$:\n$$\n\\mathrm{Var}(\\eta_{1} - \\eta_{2}) = \\mathrm{Var}(\\eta_{1}) + \\mathrm{Var}(\\eta_{2}) - 2\\mathrm{Cov}(\\eta_{1}, \\eta_{2})\n$$\nThe properties of the noise components are given by the covariance matrix:\n$$\n\\Sigma = \\sigma^{2}\n\\begin{pmatrix}\n1  \\rho \\\\\n\\rho  1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sigma^{2}  \\rho\\sigma^{2} \\\\\n\\rho\\sigma^{2}  \\sigma^{2}\n\\end{pmatrix}\n$$\nFrom this matrix, we can identify the individual variances and the covariance:\nThe variance of $\\eta_{1}$ is the first diagonal element: $\\mathrm{Var}(\\eta_{1}) = \\sigma^{2}$.\nThe variance of $\\eta_{2}$ is the second diagonal element: $\\mathrm{Var}(\\eta_{2}) = \\sigma^{2}$.\nThe covariance of $\\eta_{1}$ and $\\eta_{2}$ is the off-diagonal element: $\\mathrm{Cov}(\\eta_{1}, \\eta_{2}) = \\rho\\sigma^{2}$.\n\nSubstituting these values into the expression for the variance of the difference:\n$$\n\\mathrm{Var}(\\eta_{1} - \\eta_{2}) = \\sigma^{2} + \\sigma^{2} - 2(\\rho\\sigma^{2}) = 2\\sigma^{2} - 2\\rho\\sigma^{2}\n$$\nFactoring out the common term $2\\sigma^{2}$, we get:\n$$\n\\mathrm{Var}(r_{1} - r_{2}) = 2\\sigma^{2}(1 - \\rho)\n$$\nThe denominator of $d'_{\\mathrm{eff}}$ is the square root of this variance. Since $\\sigma^{2}  0$ and $\\rho \\in (-1, 1)$, the term $(1 - \\rho)$ is strictly positive, so the square root is well-defined and real.\n$$\n\\sqrt{\\mathrm{Var}(r_{1} - r_{2})} = \\sqrt{2\\sigma^{2}(1 - \\rho)} = \\sigma\\sqrt{2(1 - \\rho)}\n$$\n\nFinally, we combine the numerator and the denominator to obtain the closed-form expression for $d'_{\\mathrm{eff}}$:\n$$\nd'_{\\mathrm{eff}} = \\frac{\\Delta}{\\sigma\\sqrt{2(1 - \\rho)}}\n$$\n\nThe influence of the correlation coefficient $\\rho$ on the reliability of the WTA decision, as measured by $d'_{\\mathrm{eff}}$, can be understood by examining its position in the derived formula. The index $d'_{\\mathrm{eff}}$ is a signal-to-noise ratio for the decision variable $r_1 - r_2$; a higher $d'_{\\mathrm{eff}}$ indicates greater reliability. The denominator, $\\sigma\\sqrt{2(1 - \\rho)}$, represents the effective noise in the decision. The term $(1 - \\rho)$ shows that the effect of noise depends critically on the correlation.\n- If the noise is positively correlated ($\\rho  0$), the term $(1 - \\rho)$ is smaller than $1$. As $\\rho \\to 1$ (highly correlated common noise), $(1 - \\rho) \\to 0$, the denominator approaches $0$, and $d'_{\\mathrm{eff}} \\to \\infty$. This is because common noise affects both units similarly, so it is effectively canceled out when their activities are compared, making the decision extremely reliable.\n- If the noise is negatively correlated ($\\rho  0$), the term $(1 - \\rho)$ is larger than $1$. As $\\rho \\to -1$ (highly anti-correlated noise), $(1 - \\rho) \\to 2$, the denominator approaches its maximum value of $2\\sigma$, and $d'_{\\mathrm{eff}}$ is minimized. This is because anti-correlated noise fluctuations are amplified in the difference $r_{1} - r_{2}$, maximally interfering with the deterministic signal difference $\\Delta$ and making the decision less reliable.\n- If the noise is uncorrelated ($\\rho=0$), we have the baseline case where $d'_{\\mathrm{eff}} = \\Delta / (\\sigma\\sqrt{2})$.\nIn summary, positive noise correlation enhances WTA reliability by suppressing the variance of the difference signal, whereas negative correlation degrades it by amplifying this variance.",
            "answer": "$$\n\\boxed{\\frac{\\Delta}{\\sigma\\sqrt{2(1 - \\rho)}}}\n$$"
        }
    ]
}