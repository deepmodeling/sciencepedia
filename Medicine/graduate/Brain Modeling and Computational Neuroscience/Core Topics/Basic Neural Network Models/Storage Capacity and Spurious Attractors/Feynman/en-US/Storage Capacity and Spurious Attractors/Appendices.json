{
    "hands_on_practices": [
        {
            "introduction": "The energy landscape of a Hopfield network determines its memory retrieval dynamics, with stored patterns corresponding to energy minima. However, these are not the only minima; spurious attractors, or mixtures of patterns, also exist. This exercise  provides a hands-on way to understand the competition between these states by exploring how an external input field can systematically alter the energy landscape. By calculating the energy of both a pure memory and a spurious mixture, you will determine the critical field strength that causes the network to favor the error state, providing a quantitative model for memory interference.",
            "id": "4023654",
            "problem": "Consider a fully connected Hopfield network of $N$ binary neurons $s_{i} \\in \\{-1, +1\\}$ storing $P$ independent, identically distributed random patterns $\\{\\xi_{i}^{\\mu}\\}_{\\mu=1}^{P}$ with $\\xi_{i}^{\\mu} \\in \\{-1, +1\\}$ and $\\mathbb{P}(\\xi_{i}^{\\mu} = +1) = \\mathbb{P}(\\xi_{i}^{\\mu} = -1) = \\tfrac{1}{2}$. Synaptic couplings are constructed by the Hebbian rule\n$$\nJ_{ij} \\;=\\; \\frac{1}{N} \\sum_{\\mu=1}^{P} \\xi_{i}^{\\mu} \\xi_{j}^{\\mu}, \\quad J_{ii} \\;=\\; 0,\n$$\nand the zero-temperature dynamics minimizes the standard Hopfield energy with an additive external field,\n$$\nE(\\boldsymbol{s}; \\boldsymbol{h}^{\\text{ext}}) \\;=\\; -\\frac{1}{2} \\sum_{i,j=1}^{N} J_{ij} s_{i} s_{j} \\;-\\; \\sum_{i=1}^{N} h_{i}^{\\text{ext}} s_{i}.\n$$\nAssume the thermodynamic limit $N \\to \\infty$ with fixed load $\\alpha = P/N$, and let the external field be aligned with pattern $\\mu = 3$, i.e.,\n$$\nh_{i}^{\\text{ext}} \\;=\\; h \\, \\xi_{i}^{3},\n$$\nwith a constant amplitude $h \\ge 0$.\n\nFocus on two candidate attractors:\n- the pure pattern state $s_{i} = \\xi_{i}^{1}$,\n- the odd spurious mixture state $s_{i} = \\operatorname{sign}\\!\\big(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3}\\big)$, which is well-defined because $\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3} \\in \\{-3, -1, +1, +3\\}$ is never zero.\n\nUsing foundational definitions above, the Law of Large Numbers and Central Limit Theorem (CLT) to evaluate pattern overlaps and the crosstalk contribution from non-condensed patterns in the large-$N$ limit, derive the energy per spin of each of these two states and determine the threshold external bias amplitude $h_{c}$ at which, as $h$ increases from zero, the spurious mixture state first becomes energetically favored over the pure state. Express your final answer as an exact fraction. No rounding is required. The answer is dimensionless.",
            "solution": "We start from the Hopfield energy with additive field\n$$\nE(\\boldsymbol{s}; \\boldsymbol{h}^{\\text{ext}}) \\;=\\; -\\frac{1}{2} \\sum_{i,j=1}^{N} J_{ij} s_{i} s_{j} \\;-\\; \\sum_{i=1}^{N} h_{i}^{\\text{ext}} s_{i}.\n$$\nWith the Hebbian $J_{ij}$ and $J_{ii}=0$, the coupling term can be rewritten in terms of pattern overlaps. Define the overlaps\n$$\nM_{\\mu} \\;=\\; \\sum_{i=1}^{N} \\xi_{i}^{\\mu} s_{i}, \\qquad m_{\\mu} \\;=\\; \\frac{M_{\\mu}}{N}.\n$$\nUsing $J_{ij} = \\frac{1}{N} \\sum_{\\mu} \\xi_{i}^{\\mu} \\xi_{j}^{\\mu}$, and separating diagonal terms via $J_{ii}=0$, we have the identity\n$$\n\\sum_{i,j=1}^{N} J_{ij} s_{i} s_{j}\n= \\frac{1}{N} \\sum_{\\mu=1}^{P} \\left( \\sum_{i=1}^{N} \\xi_{i}^{\\mu} s_{i} \\right)^{\\!2} - P\n= N \\sum_{\\mu=1}^{P} m_{\\mu}^{2} - P.\n$$\nTherefore, the energy per spin, $e = E/N$, is\n$$\ne(\\boldsymbol{s}; h) \\;=\\; -\\frac{1}{2} \\sum_{\\mu=1}^{P} m_{\\mu}^{2} \\;+\\; \\frac{1}{2}\\alpha \\;-\\; \\frac{1}{N} \\sum_{i=1}^{N} h_{i}^{\\text{ext}} s_{i}.\n$$\nThe last term evaluates to $-\\frac{1}{N} \\sum_{i} h \\xi_{i}^{3} s_{i} = - h m_{3}$.\n\nWe will compute $e$ for the two candidate states.\n\nPure pattern state $s_{i} = \\xi_{i}^{1}$. In the thermodynamic limit, the overlaps are\n$$\nm_{1} \\;=\\; \\frac{1}{N} \\sum_{i} \\xi_{i}^{1} \\xi_{i}^{1} \\;=\\; 1, \\qquad\nm_{\\mu} \\;=\\; \\frac{1}{N} \\sum_{i} \\xi_{i}^{\\mu} \\xi_{i}^{1} \\;\\to\\; 0 \\;\\text{for}\\; \\mu \\neq 1,\n$$\nby independence and the Law of Large Numbers. Thus,\n$$\n\\sum_{\\mu=1}^{P} m_{\\mu}^{2} \\;\\to\\; 1, \\qquad m_{3} \\;\\to\\; 0.\n$$\nHence the energy per spin is\n$$\ne_{\\text{pure}}(h) \\;=\\; -\\frac{1}{2} \\cdot 1 \\;+\\; \\frac{1}{2} \\alpha \\;-\\; h \\cdot 0\n\\;=\\; -\\frac{1}{2} + \\frac{1}{2} \\alpha.\n$$\n\nSpurious mixture state $s_{i} = \\operatorname{sign}\\!\\big(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3}\\big)$. By symmetry, the overlaps with the three participating patterns are equal, $m_{1} = m_{2} = m_{3}$. We compute $m_{1}$ exactly by enumerating the $2^{3}$ possibilities of $\\big(\\xi^{1},\\xi^{2},\\xi^{3}\\big) \\in \\{-1,+1\\}^{3}$. Define $x_{1} = \\xi^{1}$, $x_{2} = \\xi^{2}$, $x_{3} = \\xi^{3}$, uniformly independent. Then\n$$\nm_{1} \\;=\\; \\mathbb{E}\\left[ \\operatorname{sign}(x_{1}+x_{2}+x_{3}) \\, x_{1} \\right].\n$$\nThe sum $x_{1}+x_{2}+x_{3}$ takes values in $\\{-3,-1,+1,+3\\}$. Enumerating all eight cases:\n\n$$\n\\begin{array}{c|c|c}\n(x_{1},x_{2},x_{3}) & \\operatorname{sign}(x_{1}+x_{2}+x_{3}) & \\operatorname{sign}(x_{1}+x_{2}+x_{3}) \\, x_{1} \\\\\n\\hline\n(+,+,+) & + & + \\\\\n(+,+,-) & + & + \\\\\n(+,-,+) & + & + \\\\\n(+,-,-) & - & - \\\\\n(-,+,+) & + & - \\\\\n(-,+,-) & - & + \\\\\n(-,-,+) & - & + \\\\\n(-,-,-) & - & + \\\\\n\\end{array}\n$$\n\nThe product is $+1$ in six cases and $-1$ in two cases, so\n$$\nm_{1} \\;=\\; \\frac{6 - 2}{8} \\;=\\; \\frac{1}{2}.\n$$\nBy symmetry, $m_{2} = m_{3} = \\tfrac{1}{2}$ as well, and $m_{\\mu} \\to 0$ for $\\mu \\ge 4$ (non-condensed patterns). Therefore\n$$\n\\sum_{\\mu=1}^{P} m_{\\mu}^{2} \\;=\\; 3 \\left(\\frac{1}{2}\\right)^{\\!2} \\;=\\; \\frac{3}{4}, \\qquad m_{3} \\;=\\; \\frac{1}{2}.\n$$\nThe energy per spin is\n$$\ne_{\\text{mix3}}(h) \\;=\\; -\\frac{1}{2} \\cdot \\frac{3}{4} \\;+\\; \\frac{1}{2} \\alpha \\;-\\; h \\cdot \\frac{1}{2}\n\\;=\\; -\\frac{3}{8} + \\frac{1}{2} \\alpha - \\frac{h}{2}.\n$$\n\nThe spurious mixture becomes energetically favored over the pure state when $e_{\\text{mix3}}(h) \\le e_{\\text{pure}}(h)$. The threshold $h_{c}$ is defined by equality:\n$$\n-\\frac{3}{8} + \\frac{1}{2} \\alpha - \\frac{h_{c}}{2} \\;=\\; -\\frac{1}{2} + \\frac{1}{2} \\alpha.\n$$\nThe load term $\\frac{1}{2}\\alpha$ cancels on both sides, reflecting that non-condensed pattern crosstalk contributes equally to both states in this mean-field limit. Solving for $h_{c}$ gives\n$$\n-\\frac{3}{8} - \\frac{h_{c}}{2} \\;=\\; -\\frac{1}{2}\n\\;\\;\\Longrightarrow\\;\\;\n\\frac{1}{8} \\;=\\; \\frac{h_{c}}{2}\n\\;\\;\\Longrightarrow\\;\\;\nh_{c} \\;=\\; \\frac{1}{4}.\n$$\n\nThus, in the thermodynamic limit with zero temperature and an external field aligned to one constituent of the spurious mixture, the basin flips from the pure pattern to the three-pattern spurious mixture precisely when the external bias amplitude reaches $h_{c} = \\tfrac{1}{4}$.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "Neural systems operate in a noisy environment, a factor that can be modeled as a finite temperature in statistical mechanics frameworks. This thermal noise does not just degrade memory; it can fundamentally reshape the attractor landscape. This practice  guides you through a mean-field analysis to discover how spurious mixture states, which may not be stable at zero temperature, can spontaneously emerge and become stable as temperature increases. You will derive the critical temperature for this transition and the resulting overlap, illustrating the creative and destructive roles of noise in neural computation.",
            "id": "4023666",
            "problem": "Consider a fully connected associative memory model of the Hopfield type with $N$ binary neurons $s_{i} \\in \\{-1,+1\\}$, where the synaptic couplings are set by the Hebbian rule $J_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^{p} \\xi_{i}^{\\mu} \\xi_{j}^{\\mu}$ for $i \\neq j$ and $J_{ii} = 0$. The $\\xi_{i}^{\\mu} \\in \\{-1,+1\\}$ are independent, identically distributed random variables with equal probability for $\\pm 1$, representing $p$ stored patterns. The system is in contact with a thermal bath at inverse temperature $\\beta = 1/T$, with Boltzmann constant set to $k_B=1$, and follows the Gibbs-Boltzmann distribution $P(\\mathbf{s}) \\propto \\exp\\!\\left( \\frac{\\beta}{2} \\sum_{i \\neq j} J_{ij} s_{i} s_{j} \\right)$. Define the overlaps $m_{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_{i}^{\\mu} \\langle s_{i} \\rangle$, where $\\langle \\cdot \\rangle$ denotes the thermal average under $P(\\mathbf{s})$.\n\nAssume the low-load limit $p/N \\to 0$ so that crosstalk noise may be neglected to leading order, and consider a candidate spurious mixture state in which three patterns contribute symmetrically: by symmetry $m_{1} = m_{2} = m_{3} = m$ and $m_{\\mu} = 0$ for $\\mu \\geq 4$. Under a mean-field approximation, the single-neuron thermal averages satisfy $\\langle s_{i} \\rangle = \\tanh(\\beta h_{i})$, where $h_{i}$ is the local field induced by the mean overlaps.\n\nStarting only from these definitions and the low-load approximation, and without introducing any additional shortcut formulas, perform the following steps:\n\n1. Derive the mean-field self-consistency relation for the symmetric three-pattern mixture overlap $m$ in the thermodynamic limit $N \\to \\infty$, expressing $m$ as an average over the random pattern components $(\\xi_{i}^{1}, \\xi_{i}^{2}, \\xi_{i}^{3})$.\n\n2. Use this relation to determine the stability of the $m=0$ state as a function of the inverse temperature $\\beta$ by analyzing small perturbations. Identify the critical inverse temperature $\\beta_{c}$ at which $m=0$ loses stability.\n\n3. For $\\beta$ just above $\\beta_{c}$, perform a systematic small-overlap expansion to cubic order and obtain the leading-order nonzero solution $m(\\beta)$ that bifurcates from $m=0$. Express the result as a closed-form analytic expression in terms of $\\beta$.\n\nYour final answer must be the single closed-form analytic expression for the leading-order overlap $m(\\beta)$ obtained in step 3. No rounding is required, and no units apply. Express angles, if any appear, in radians.",
            "solution": "The model is a Hopfield network of $N$ binary neurons $s_{i} \\in \\{-1,+1\\}$ with synaptic couplings defined by the Hebbian rule for $p$ patterns $\\boldsymbol{\\xi}^{\\mu}$:\n$$J_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^{p} \\xi_{i}^{\\mu} \\xi_{j}^{\\mu} \\quad (i \\neq j), \\quad J_{ii} = 0$$\nThe system is at an inverse temperature $\\beta$. Under a mean-field approximation, the thermal average of a single neuron's state is given by:\n$$\\langle s_{i} \\rangle = \\tanh(\\beta h_{i})$$\nwhere $h_{i}$ is the local field at neuron $i$.\n\n### Step 1: Derivation of the Mean-Field Self-Consistency Relation\n\nThe local field $h_i$ is the sum of inputs from all other neurons, weighted by the synaptic strengths:\n$$h_{i} = \\sum_{j \\neq i} J_{ij} \\langle s_{j} \\rangle$$\nIn the thermodynamic limit $N \\to \\infty$, the exclusion of the $j=i$ term is negligible, so we can write:\n$$h_{i} \\approx \\sum_{j=1}^{N} J_{ij} \\langle s_{j} \\rangle = \\sum_{j=1}^{N} \\left( \\frac{1}{N} \\sum_{\\mu=1}^{p} \\xi_{i}^{\\mu} \\xi_{j}^{\\mu} \\right) \\langle s_{j} \\rangle$$\nBy rearranging the order of summation, we get:\n$$h_{i} = \\sum_{\\mu=1}^{p} \\xi_{i}^{\\mu} \\left( \\frac{1}{N} \\sum_{j=1}^{N} \\xi_{j}^{\\mu} \\langle s_{j} \\rangle \\right)$$\nThe term in the parenthesis is the definition of the overlap, $m_{\\mu}$. Thus, the local field can be expressed in terms of the overlaps:\n$$h_{i} = \\sum_{\\mu=1}^{p} \\xi_{i}^{\\mu} m_{\\mu}$$\nThe problem posits a specific spurious mixture state where three patterns contribute symmetrically: $m_{1} = m_{2} = m_{3} = m$, and all other overlaps are zero ($m_{\\mu} = 0$ for $\\mu \\ge 4$). This simplifies the local field to:\n$$h_{i} = m_{1}\\xi_{i}^{1} + m_{2}\\xi_{i}^{2} + m_{3}\\xi_{i}^{3} = m(\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3})$$\nSubstituting this into the mean-field equation for $\\langle s_i \\rangle$:\n$$\\langle s_{i} \\rangle = \\tanh\\left(\\beta m (\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3})\\right)$$\nTo obtain the self-consistency equation for $m$, we use the definition of the overlap $m_1=m$:\n$$m = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_{i}^{1} \\langle s_{i} \\rangle = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_{i}^{1} \\tanh\\left(\\beta m (\\xi_{i}^{1} + \\xi_{i}^{2} + \\xi_{i}^{3})\\right)$$\nIn the thermodynamic limit ($N \\to \\infty$), the law of large numbers allows us to replace the average over the neurons $i$ with an average over the distribution of the random i.i.d. pattern components $(\\xi^{1}, \\xi^{2}, \\xi^{3})$. Each $\\xi^{k}$ takes values $\\pm 1$ with probability $1/2$. We denote this average by $\\langle \\cdot \\rangle_{\\xi}$:\n$$m = \\left\\langle \\xi^{1} \\tanh\\left(\\beta m (\\xi^{1} + \\xi^{2} + \\xi^{3})\\right) \\right\\rangle_{\\xi}$$\nThis is the required mean-field self-consistency relation for the three-pattern mixture overlap $m$.\n\n### Step 2: Stability of the $m=0$ State and Critical Temperature $\\beta_{c}$\n\nThe state $m=0$ is always a solution to the self-consistency equation since $\\tanh(0)=0$. To analyze its stability, we linearize the equation around $m=0$. Let $f(m) = \\left\\langle \\xi^{1} \\tanh\\left(\\beta m (\\xi^{1} + \\xi^{2} + \\xi^{3})\\right) \\right\\rangle_{\\xi}$. The equation is $m = f(m)$. The $m=0$ state becomes unstable when the derivative of the right-hand side with respect to $m$, evaluated at $m=0$, exceeds $1$. The critical condition is $f'(0) = 1$.\n\nFirst, we compute the derivative $f'(m)$:\n$$f'(m) = \\frac{d}{dm} \\left\\langle \\xi^{1} \\tanh\\left(\\beta m (\\xi^{1} + \\xi^{2} + \\xi^{3})\\right) \\right\\rangle_{\\xi}$$\nUsing the chain rule, $\\frac{d}{dx}\\tanh(u) = \\text{sech}^{2}(u) \\frac{du}{dx}$, we get:\n$$f'(m) = \\left\\langle \\xi^{1} \\cdot \\beta(\\xi^{1} + \\xi^{2} + \\xi^{3}) \\cdot \\text{sech}^{2}\\left(\\beta m (\\xi^{1} + \\xi^{2} + \\xi^{3})\\right) \\right\\rangle_{\\xi}$$\nNow, we evaluate this at $m=0$. Since $\\text{sech}(0) = 1$:\n$$f'(0) = \\left\\langle \\xi^{1} \\cdot \\beta(\\xi^{1} + \\xi^{2} + \\xi^{3}) \\cdot \\text{sech}^{2}(0) \\right\\rangle_{\\xi} = \\beta \\left\\langle \\xi^{1} (\\xi^{1} + \\xi^{2} + \\xi^{3}) \\right\\rangle_{\\xi}$$\nExpanding the term inside the average:\n$$f'(0) = \\beta \\left( \\langle (\\xi^{1})^{2} \\rangle_{\\xi} + \\langle \\xi^{1}\\xi^{2} \\rangle_{\\xi} + \\langle \\xi^{1}\\xi^{3} \\rangle_{\\xi} \\right)$$\nThe pattern components $\\xi^{\\mu}$ are independent and identically distributed with $P(\\xi^{\\mu}=\\pm 1) = 1/2$. Therefore, their mean is $\\langle \\xi^{\\mu} \\rangle_{\\xi} = 0$ and their variance is $\\langle (\\xi^{\\mu})^{2} \\rangle_{\\xi} = 1$. For $\\mu \\neq \\nu$, their covariance is $\\langle \\xi^{\\mu}\\xi^{\\nu} \\rangle_{\\xi} = \\langle \\xi^{\\mu} \\rangle_{\\xi} \\langle \\xi^{\\nu} \\rangle_{\\xi} = 0$.\nSubstituting these values:\n$$f'(0) = \\beta(1 + 0 + 0) = \\beta$$\nThe critical condition $f'(0)=1$ immediately yields the critical inverse temperature:\n$$\\beta_{c} = 1$$\nFor $\\beta > 1$, the trivial state $m=0$ is unstable, and a non-zero solution for $m$ can appear.\n\n### Step 3: Leading-Order Solution for $m(\\beta)$ for $\\beta > \\beta_{c}$\n\nTo find the form of the bifurcating solution for $m$ just above $\\beta_{c}$, we perform a small-overlap expansion of the self-consistency equation up to the third order in $m$. The Taylor series for $\\tanh(x)$ is $x - x^3/3 + O(x^5)$.\n$$m = \\left\\langle \\xi^{1} \\left[ \\beta m (\\xi^{1}+\\xi^{2}+\\xi^{3}) - \\frac{1}{3} \\left(\\beta m (\\xi^{1}+\\xi^{2}+\\xi^{3})\\right)^{3} + O(m^{5}) \\right] \\right\\rangle_{\\xi}$$\n$$m = \\beta m \\left\\langle \\xi^{1} (\\xi^{1}+\\xi^{2}+\\xi^{3}) \\right\\rangle_{\\xi} - \\frac{\\beta^{3} m^{3}}{3} \\left\\langle \\xi^{1} (\\xi^{1}+\\xi^{2}+\\xi^{3})^{3} \\right\\rangle_{\\xi} + O(m^{5})$$\nFor a non-zero solution, we can divide by $m$:\n$$1 = \\beta \\left\\langle \\xi^{1}(\\xi^{1}+\\xi^{2}+\\xi^{3}) \\right\\rangle_{\\xi} - \\frac{\\beta^{3} m^{2}}{3} \\left\\langle \\xi^{1} (\\xi^{1}+\\xi^{2}+\\xi^{3})^{3} \\right\\rangle_{\\xi} + O(m^{4})$$\nWe have already calculated the first average: $\\langle \\xi^{1}(\\xi^{1}+\\xi^{2}+\\xi^{3}) \\rangle_{\\xi} = 1$.\nNow we must compute the second average, $\\langle \\xi^{1} (\\xi^{1}+\\xi^{2}+\\xi^{3})^{3} \\rangle_{\\xi}$.\nUsing $(\\xi^1+\\xi^2+\\xi^3)^3 = 7\\xi^1 + 7\\xi^2 + 7\\xi^3 + 6\\xi^1\\xi^2\\xi^3$, we compute the expectation of $\\xi^1$ multiplied by this expression:\n$$\\left\\langle \\xi^{1} \\left( 7\\xi^{1} + 7\\xi^{2} + 7\\xi^{3} + 6\\xi^{1}\\xi^{2}\\xi^{3} \\right) \\right\\rangle_{\\xi} = \\left\\langle 7(\\xi^{1})^{2} + 7\\xi^{1}\\xi^{2} + 7\\xi^{1}\\xi^{3} + 6(\\xi^{1})^{2}\\xi^{2}\\xi^{3} \\right\\rangle_{\\xi}$$\nUsing the statistical properties of $\\xi^{k}$:\n$$= 7\\langle (\\xi^{1})^{2} \\rangle_{\\xi} + 7\\langle \\xi^{1}\\xi^{2} \\rangle_{\\xi} + 7\\langle \\xi^{1}\\xi^{3} \\rangle_{\\xi} + 6\\langle (\\xi^{1})^{2} \\rangle_{\\xi}\\langle \\xi^{2} \\rangle_{\\xi}\\langle \\xi^{3} \\rangle_{\\xi}$$\n$$= 7(1) + 7(0) + 7(0) + 6(1)(0)(0) = 7$$\nSo, $\\langle \\xi^{1} (\\xi^{1}+\\xi^{2}+\\xi^{3})^{3} \\rangle_{\\xi} = 7$.\nSubstituting the values of the averages back into the expanded self-consistency equation:\n$$1 = \\beta(1) - \\frac{\\beta^{3} m^{2}}{3}(7) + O(m^{4})$$\n$$1 - \\beta = -\\frac{7\\beta^{3}}{3} m^{2}$$\nSolving for $m^{2}$:\n$$m^{2} = \\frac{3(1-\\beta)}{-7\\beta^{3}} = \\frac{3(\\beta-1)}{7\\beta^{3}}$$\nFor $\\beta > \\beta_{c}=1$, the right-hand side is positive, admitting a real, non-zero solution for $m$. The leading-order solution for the overlap amplitude is obtained by taking the square root. By convention, we take the positive root:\n$$m(\\beta) = \\sqrt{\\frac{3(\\beta-1)}{7\\beta^{3}}}$$\nThis is the closed-form analytic expression for the leading-order nonzero overlap that bifurcates from the $m=0$ state at $\\beta = 1$.",
            "answer": "$$\\boxed{\\sqrt{\\frac{3(\\beta-1)}{7\\beta^{3}}}}$$"
        },
        {
            "introduction": "Analytical models often assume infinite precision in synaptic weights, a condition not met in biological or hardware implementations. This final practice  bridges theory and application by asking you to computationally investigate a network with clipped, or ternary, synapses. You will implement the network and directly measure the stability margins and energy densities for both a desired memory and a spurious attractor. This provides a tangible understanding of how theoretical concepts like stability hold up under practical constraints and in finite-sized systems.",
            "id": "4023655",
            "problem": "Consider a fully connected binary associative memory network with $N$ neurons, modeled in the style of a Hopfield network. Each neuron state is $s_i \\in \\{-1,+1\\}$ for $i \\in \\{1,\\dots,N\\}$. A set of $P$ independent and identically distributed random patterns $\\{\\boldsymbol{\\xi}^\\mu\\}_{\\mu=1}^P$ is stored, where $\\xi_i^\\mu \\in \\{-1,+1\\}$ are independent and equiprobable. Synaptic couplings are constructed by the standard Hebbian rule, then quantized (clipped) to ternary values. Your task is to analyze the existence and stability of spurious mixture states under ternary clipping.\n\nFundamental base and definitions:\n\n- Hebbian couplings:\n$$\nJ_{ij} \\;=\\; \\begin{cases}\n\\dfrac{1}{N}\\sum_{\\mu=1}^{P} \\xi_i^\\mu \\,\\xi_j^\\mu, & i \\neq j,\\\\\n0, & i=j,\n\\end{cases}\n$$\nso $\\boldsymbol{J}$ is symmetric with zero diagonal.\n\n- Ternary clipping to the synaptic matrix $\\boldsymbol{W}$:\n$$\nw_{ij} \\;=\\; \\begin{cases}\nw_0 \\,\\mathrm{sign}(J_{ij}), & \\text{if } |J_{ij}| \\ge \\lambda \\text{ and } i \\neq j,\\\\\n0, & \\text{otherwise},\n\\end{cases}\n$$\nwith $w_{ii}=0$. Here $w_0 > 0$ is a fixed synaptic magnitude and $\\lambda \\ge 0$ is a clipping threshold.\n\n- Zero-temperature deterministic dynamics is defined by the local field:\n$$\nh_i(\\boldsymbol{s}) \\;=\\; \\sum_{j\\neq i} w_{ij} \\, s_j,\n$$\nand the stability condition for a fixed point $\\boldsymbol{s}$ is that every neuron is strictly aligned with its local field:\n$$\ns_i \\, h_i(\\boldsymbol{s}) \\;>\\; 0 \\quad \\text{for all } i \\in \\{1,\\dots,N\\}.\n$$\nDefine the stability margin of $\\boldsymbol{s}$ as\n$$\n\\gamma(\\boldsymbol{s}) \\;=\\; \\min_{1\\le i \\le N} \\left[ s_i \\, h_i(\\boldsymbol{s}) \\right].\n$$\n\n- The Lyapunov energy function under symmetric $\\boldsymbol{W}$ is\n$$\nE(\\boldsymbol{s}) \\;=\\; -\\dfrac{1}{2} \\sum_{i\\neq j} w_{ij} \\, s_i \\, s_j,\n$$\nand the energy density is $e(\\boldsymbol{s}) \\;=\\; E(\\boldsymbol{s})/N^2$.\n\n- Define the $k$-pattern spurious mixture state using equal weights as\n$$\ns_i^{(\\mathrm{mix},k)} \\;=\\; \\mathrm{sign}\\!\\left( \\sum_{\\mu=1}^k \\xi_i^\\mu \\right),\n$$\nwith the tie-breaking convention $\\mathrm{sign}(0) = +1$. The pure retrieval state for pattern $\\mu=1$ is\n$$\ns_i^{(\\mathrm{pure})} \\;=\\; \\xi_i^1.\n$$\n\nYour program must, for each specified test case, do the following:\n\n1. Generate $P$ independent patterns $\\{\\boldsymbol{\\xi}^\\mu\\}_{\\mu=1}^P$ with entries in $\\{-1,+1\\}$, using the provided random seed for reproducibility.\n2. Construct $\\boldsymbol{J}$ by the Hebb rule and then clip to ternary $\\boldsymbol{W}$ using $w_0$ and $\\lambda$ as defined above.\n3. Construct the $k$-mixture spurious state $\\boldsymbol{s}^{(\\mathrm{mix},k)}$ from the first $k$ stored patterns.\n4. Compute, for both $\\boldsymbol{s}^{(\\mathrm{mix},k)}$ and $\\boldsymbol{s}^{(\\mathrm{pure})}$:\n   - The fraction of violated stability inequalities,\n     $$\n     \\phi(\\boldsymbol{s}) \\;=\\; \\dfrac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\left\\{ s_i \\, h_i(\\boldsymbol{s}) \\le 0 \\right\\}.\n     $$\n   - The minimum stability margin $\\gamma(\\boldsymbol{s})$.\n   - The energy density $e(\\boldsymbol{s})$.\n   - A boolean indicator of strict stability, defined as $\\mathrm{Stable}(\\boldsymbol{s}) = \\mathbf{1}\\{\\gamma(\\boldsymbol{s})>0\\}$.\n\nOutput specification:\n\n- For each test case, your program must output a list of eight entries in the following order:\n  1. $\\mathrm{Stable}(\\boldsymbol{s}^{(\\mathrm{mix},k)})$ (boolean),\n  2. $\\phi(\\boldsymbol{s}^{(\\mathrm{mix},k)})$ (float),\n  3. $\\gamma(\\boldsymbol{s}^{(\\mathrm{mix},k)})$ (float),\n  4. $e(\\boldsymbol{s}^{(\\mathrm{mix},k)})$ (float),\n  5. $\\mathrm{Stable}(\\boldsymbol{s}^{(\\mathrm{pure})})$ (boolean),\n  6. $\\phi(\\boldsymbol{s}^{(\\mathrm{pure})})$ (float),\n  7. $\\gamma(\\boldsymbol{s}^{(\\mathrm{pure})})$ (float),\n  8. $e(\\boldsymbol{s}^{(\\mathrm{pure})})$ (float).\n- All floating-point outputs must be rounded to six decimal places.\n- Aggregate the results for all test cases into a single line printed as a comma-separated list of these per-test-case lists enclosed in square brackets, e.g., $[\\,[\\dots],\\,[\\dots],\\dots\\,]$.\n- There are no physical units involved in this problem. Angles are not used.\n\nTest suite (each tuple is $(N,P,k,w_0,\\lambda,\\mathrm{seed})$):\n\n- Case $1$: $(200,\\,20,\\,3,\\,1.0,\\,0.0,\\,1)$\n- Case $2$: $(200,\\,20,\\,3,\\,1.0,\\,0.01,\\,1)$\n- Case $3$: $(200,\\,40,\\,3,\\,1.0,\\,0.0,\\,2)$\n- Case $4$: $(200,\\,5,\\,3,\\,1.0,\\,0.0,\\,3)$\n- Case $5$: $(200,\\,20,\\,5,\\,1.0,\\,0.0,\\,4)$\n- Case $6$: $(200,\\,20,\\,2,\\,1.0,\\,0.0,\\,5)$\n- Case $7$: $(200,\\,20,\\,3,\\,1.0,\\,0.05,\\,6)$\n\nScientific requirements:\n\n- Base your reasoning on the definitions above, the law of large numbers, and the Central Limit Theorem (CLT) for approximations if needed, but your implementation must compute the exact quantities for the finite $N$ specified.\n- Ensure that your implementation respects the strict stability criterion $s_i h_i(\\boldsymbol{s}) > 0$ for all $i$ and the tie-breaking convention $\\mathrm{sign}(0)=+1$ when constructing the mixture.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order of the test cases listed above.",
            "solution": "The solution proceeds by first generating the necessary components of the network model for each test case, and then evaluating the specified quantities.\n\n1.  **Generation of Patterns and States:**\n    For each test case, defined by the parameters $(N, P, k, w_0, \\lambda, \\mathrm{seed})$, the simulation begins by initializing a pseudo-random number generator with the given seed for reproducibility.\n    A set of $P$ memory patterns, $\\{\\boldsymbol{\\xi}^\\mu\\}_{\\mu=1}^P$, is generated. Each pattern $\\boldsymbol{\\xi}^\\mu$ is a vector of size $N$, whose components $\\xi_i^\\mu$ are independent and identically distributed random variables taking values in $\\{-1, +1\\}$ with equal probability, i.e., $P(\\xi_i^\\mu = +1) = P(\\xi_i^\\mu = -1) = 1/2$.\n    From these patterns, two specific network states are constructed for analysis:\n    -   The pure retrieval state, $\\boldsymbol{s}^{(\\mathrm{pure})}$, is set to be the first stored pattern: $\\boldsymbol{s}^{(\\mathrm{pure})} = \\boldsymbol{\\xi}^1$.\n    -   The $k$-pattern spurious mixture state, $\\boldsymbol{s}^{(\\mathrm{mix},k)}$, is constructed from the first $k$ patterns. Its components are determined by the rule $s_i^{(\\mathrm{mix},k)} = \\mathrm{sign}(\\sum_{\\mu=1}^k \\xi_i^\\mu)$. The summation $\\sum_{\\mu=1}^k \\xi_i^\\mu$ results in an integer. If this sum is zero (which can only occur for even $k$), the tie-breaking convention $\\mathrm{sign}(0) = +1$ is applied.\n\n2.  **Synaptic Matrix Construction:**\n    The synaptic weights are determined in a two-step process. First, an intermediate Hebbian coupling matrix $\\boldsymbol{J}$ is computed. Following the Hebbian learning principle, the strength of the connection between neuron $i$ and neuron $j$ is proportional to the correlation of their activities across the stored patterns. The elements $J_{ij}$ are given by:\n    $$\n    J_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^{P} \\xi_i^\\mu \\xi_j^\\mu\n    $$\n    for $i \\neq j$, and $J_{ii} = 0$ to prevent self-interaction.\n    Second, this matrix $\\boldsymbol{J}$ is transformed into a ternary weight matrix $\\boldsymbol{W}$ through a clipping procedure. This is a model of synaptic simplification or quantization. For a given clipping threshold $\\lambda \\ge 0$ and synaptic magnitude $w_0 > 0$, the elements $w_{ij}$ are defined as:\n    $$\n    w_{ij} = \\begin{cases}\n    w_0 \\, \\mathrm{sign}(J_{ij}), & \\text{if } |J_{ij}| \\ge \\lambda \\text{ and } i \\neq j,\\\\\n    0, & \\text{otherwise}.\n    \\end{cases}\n    $$\n    The diagonal elements remain zero, $w_{ii}=0$. This clipping sets weak synapses to zero and assigns a uniform magnitude $\\pm w_0$ to all remaining synapses, preserving only the sign of the original Hebbian coupling.\n\n3.  **Stability and Energy Analysis:**\n    For a given network state $\\boldsymbol{s} \\in \\{-1, +1\\}^N$, its stability and energy are evaluated. The analysis is performed for both $\\boldsymbol{s}^{(\\mathrm{pure})}$ and $\\boldsymbol{s}^{(\\mathrm{mix},k)}$.\n    -   **Local Field and Stability:** The local field, or input, to neuron $i$ is the weighted sum of the states of all other neurons: $h_i(\\boldsymbol{s}) = \\sum_{j \\neq i} w_{ij} s_j$. This can be efficiently computed as a matrix-vector product, $\\boldsymbol{h}(\\boldsymbol{s}) = \\boldsymbol{W}\\boldsymbol{s}$.\n        A state $\\boldsymbol{s}$ is a stable fixed point of the zero-temperature dynamics if each neuron's state is aligned with its local field. The strict stability condition is $s_i h_i(\\boldsymbol{s}) > 0$ for all $i=1,\\dots,N$.\n        From this, we compute:\n        a. The minimum stability margin, $\\gamma(\\boldsymbol{s}) = \\min_{i} [s_i h_i(\\boldsymbol{s})]$.\n        b. The boolean indicator of stability, $\\mathrm{Stable}(\\boldsymbol{s}) = \\mathbf{1}\\{\\gamma(\\boldsymbol{s}) > 0\\}$, which is true if and only if all neurons satisfy the strict stability condition.\n        c. The fraction of neurons violating the stability condition, $\\phi(\\boldsymbol{s}) = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{s_i h_i(\\boldsymbol{s}) \\le 0\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Note that a neuron with $s_i h_i(\\boldsymbol{s}) = 0$ is considered unstable.\n    -   **Energy Function:** The network's Lyapunov energy function for a symmetric weight matrix $\\boldsymbol{W}$ is $E(\\boldsymbol{s}) = -\\frac{1}{2} \\sum_{i \\neq j} w_{ij} s_i s_j$. This can be computed more efficiently using the local fields: $E(\\boldsymbol{s}) = -\\frac{1}{2} \\sum_{i=1}^N s_i h_i(\\boldsymbol{s})$.\n        The energy density is then calculated as $e(\\boldsymbol{s}) = E(\\boldsymbol{s})/N^2$.\n\nThe implementation will loop through each test case, execute these steps to compute the eight required numerical values (four for the mixture state, four for the pure state), round the floating-point results to six decimal places, and format them into the specified list-of-lists structure for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, P, k, w_0, lambda, seed)\n        (200, 20, 3, 1.0, 0.0, 1),\n        (200, 20, 3, 1.0, 0.01, 1),\n        (200, 40, 3, 1.0, 0.0, 2),\n        (200, 5, 3, 1.0, 0.0, 3),\n        (200, 20, 5, 1.0, 0.0, 4),\n        (200, 20, 2, 1.0, 0.0, 5),\n        (200, 20, 3, 1.0, 0.05, 6),\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N, P, k, w0, lambda_val, seed = case\n        \n        # 1. Generate patterns and states\n        rng = np.random.default_rng(seed)\n        patterns = rng.choice([-1, 1], size=(P, N))\n        \n        # Pure retrieval state\n        s_pure = patterns[0, :].astype(np.float64)\n        \n        # K-mixture spurious state\n        mix_sum = np.sum(patterns[:k, :], axis=0)\n        s_mix = np.sign(mix_sum).astype(np.float64)\n        s_mix[s_mix == 0] = 1.0 # Tie-breaking rule sign(0) = +1\n        \n        # 2. Construct synaptic matrix\n        # Hebbian couplings J\n        J = (1.0 / N) * (patterns.T @ patterns)\n        np.fill_diagonal(J, 0)\n        \n        # Ternary clipped couplings W\n        W = np.zeros_like(J)\n        mask = np.abs(J) >= lambda_val\n        W[mask] = w0 * np.sign(J[mask])\n        np.fill_diagonal(W, 0) # Ensure no self-connections\n\n        # 3. Analyze states\n        mix_analysis = analyze_state(s_mix, W, N)\n        pure_analysis = analyze_state(s_pure, W, N)\n        \n        # 4. Collate results for the current case\n        case_results = list(mix_analysis) + list(pure_analysis)\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # We manually construct the string to match the required format: [[...],[...],...]\n    # without any spaces and with floating point numbers formatted to 6 decimal places.\n    outer_parts = []\n    for inner_list in all_results:\n        inner_parts = []\n        for item in inner_list:\n            if isinstance(item, (bool, np.bool_)):\n                inner_parts.append(str(item).lower())\n            elif isinstance(item, (float, np.floating)):\n                inner_parts.append(f\"{item:.6f}\")\n            else:\n                inner_parts.append(str(item))\n        outer_parts.append(f\"[{','.join(inner_parts)}]\")\n    \n    final_string = f\"[{','.join(outer_parts)}]\"\n    print(final_string)\n\ndef analyze_state(s, W, N):\n    \"\"\"\n    Computes stability metrics and energy for a given state s.\n    \n    Args:\n        s (np.ndarray): The state vector of size N.\n        W (np.ndarray): The synaptic weight matrix of size (N, N).\n        N (int): The number of neurons.\n    \n    Returns:\n        tuple: A tuple containing (Stable, phi, gamma, energy_density).\n    \"\"\"\n    # Local fields h_i(s)\n    h = W @ s\n    \n    # Alignments s_i * h_i(s)\n    alignments = s * h\n    \n    # Minimum stability margin gamma(s)\n    gamma = np.min(alignments)\n    \n    # Boolean indicator of strict stability\n    is_stable = gamma > 0\n    \n    # Fraction of violated stability inequalities phi(s)\n    violated_count = np.sum(alignments = 0)\n    phi = violated_count / N\n    \n    # Energy E(s) and energy density e(s)\n    energy = -0.5 * np.sum(alignments)\n    energy_density = energy / (N**2)\n\n    return is_stable, phi, gamma, energy_density\n\n# The expected output format requires 'true'/'false' for booleans.\n# The following call to solve() will be wrapped to ensure this formatting.\nimport io\nimport sys\n\noriginal_stdout = sys.stdout\nsys.stdout = captured_output = io.StringIO()\n\nsolve()\n\nsys.stdout = original_stdout\noutput_str = captured_output.getvalue().strip()\noutput_str = output_str.replace(\"True\", \"true\").replace(\"False\", \"false\")\nprint(output_str)\n\n```"
        }
    ]
}