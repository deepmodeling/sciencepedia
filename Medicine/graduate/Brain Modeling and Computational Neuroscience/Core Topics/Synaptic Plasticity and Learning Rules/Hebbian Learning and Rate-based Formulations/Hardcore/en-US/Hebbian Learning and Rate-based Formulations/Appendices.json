{
    "hands_on_practices": [
        {
            "introduction": "A fundamental challenge in Hebbian learning is its inherent instability; synaptic weights can grow without bound. A common conceptual solution is to enforce a constraint, such as keeping the total synaptic strength constant. This exercise provides a crucial link between the intuitive procedure of a discrete Hebbian update followed by explicit weight vector normalization and the classic continuous-time formulation of Oja's rule . By using a first-order Taylor approximation, you will derive how the subtractive, activity-dependent term in Oja's rule naturally emerges, providing a foundational insight into how synaptic competition can be mathematically modeled.",
            "id": "3987622",
            "problem": "Consider a single linear rate neuron receiving an input vector $x \\in \\mathbb{R}^{n}$ with synaptic weight vector $w \\in \\mathbb{R}^{n}$. The neuron's firing rate is $y = w^{\\top} x$. A standard Hebbian increment with step size $\\eta  0$ is applied in discrete time, followed immediately by multiplicative weight normalization to unit norm. That is, in one learning step:\n- First, compute the Hebbian pre-update $w' = w + \\eta\\,y\\,x$.\n- Then, enforce unit norm $w^{+} = \\dfrac{w'}{\\|w'\\|}$.\nAssume that at the start of the step, the weight vector is already normalized, i.e., $\\|w\\| = 1$. You may use first-order Taylor expansions about $0$ for smooth functions of a scalar variable, including $\\sqrt{1+\\epsilon}$ and $(1+\\epsilon)^{-1}$.\n\nStarting from the fundamental definitions of the Euclidean norm and first-order Taylor expansion, derive the leading-order small-step approximation (to first order in $\\eta$) of the weight change induced by the normalize-after-Hebbian update, $w^{+} - w$, in terms of $\\eta$, $y$, $x$, and $w$, given $y = w^{\\top} x$. This leading-order expression is the continuous-time limit drift that is commonly associated with Oja’s rule in rate-based formulations.\n\nAnswer form requirement:\n- Provide only the first-order term as a single closed-form analytic expression in $\\eta$, $y$, $x$, and $w$.\n- Do not include an equality sign in your final answer.\n- No rounding is required, and no units are involved.",
            "solution": "The task is to derive the leading-order approximation of the weight change, $w^{+} - w$, for a small learning step size $\\eta$. The update rule is a Hebbian increment followed by normalization.\n\nThe weight change is given by $\\Delta w = w^{+} - w$. The updated weight vector, $w^{+}$, is defined as:\n$$ w^{+} = \\frac{w'}{\\|w'\\|} $$\nwhere $w'$ is the result of the Hebbian pre-update:\n$$ w' = w + \\eta y x $$\nHere, $y = w^{\\top} x$ is the neuron's firing rate, $x \\in \\mathbb{R}^{n}$ is the input vector, $w \\in \\mathbb{R}^{n}$ is the weight vector, and $\\eta  0$ is the learning rate. We are given the initial condition that $\\|w\\| = 1$.\n\nSubstituting the expression for $w'$ into the equation for $w^{+}$ yields:\n$$ w^{+} = \\frac{w + \\eta y x}{\\|w + \\eta y x\\|} $$\nTo derive a first-order approximation in $\\eta$, we must analyze the term in the denominator, $\\|w + \\eta y x\\|$. We begin by expanding its square:\n$$ \\|w + \\eta y x\\|^2 = (w + \\eta y x)^{\\top}(w + \\eta y x) $$\nUsing the distributive property of the transpose and matrix multiplication:\n$$ \\|w + \\eta y x\\|^2 = (w^{\\top} + \\eta y x^{\\top})(w + \\eta y x) = w^{\\top}w + w^{\\top}(\\eta y x) + (\\eta y x^{\\top})w + (\\eta y x^{\\top})(\\eta y x) $$\n$$ \\|w + \\eta y x\\|^2 = \\|w\\|^2 + \\eta y (w^{\\top}x) + \\eta y (x^{\\top}w) + \\eta^2 y^2 (x^{\\top}x) $$\nSince vector dot products are symmetric, $x^{\\top}w = w^{\\top}x$. We are given that $\\|w\\| = 1$ and $y = w^{\\top}x$. Substituting these into the equation gives:\n$$ \\|w + \\eta y x\\|^2 = 1 + \\eta y (y) + \\eta y (y) + \\eta^2 y^2 \\|x\\|^2 $$\n$$ \\|w + \\eta y x\\|^2 = 1 + 2\\eta y^2 + \\eta^2 y^2 \\|x\\|^2 $$\nNow we take the square root to find $\\|w + \\eta y x\\|$:\n$$ \\|w + \\eta y x\\| = \\sqrt{1 + 2\\eta y^2 + \\eta^2 y^2 \\|x\\|^2} $$\nThe problem permits the use of first-order Taylor expansions for small $\\eta$. We use the expansion $\\sqrt{1+\\epsilon} \\approx 1 + \\frac{1}{2}\\epsilon$ for small $\\epsilon$. In our case, the term $\\epsilon = 2\\eta y^2 + \\eta^2 y^2 \\|x\\|^2$. To first order in $\\eta$, we only need to consider the term proportional to $\\eta$, so we can approximate $\\epsilon \\approx 2\\eta y^2$.\n$$ \\|w + \\eta y x\\| \\approx \\sqrt{1 + 2\\eta y^2} \\approx 1 + \\frac{1}{2}(2\\eta y^2) = 1 + \\eta y^2 $$\nWe have neglected terms of order $\\eta^2$ and higher.\n\nNext, we need to find the inverse of this norm, which appears as a multiplicative factor in the expression for $w^{+}$.\n$$ \\frac{1}{\\|w + \\eta y x\\|} \\approx \\frac{1}{1 + \\eta y^2} $$\nUsing the Taylor expansion $(1+\\delta)^{-1} \\approx 1 - \\delta$ for small $\\delta$, with $\\delta = \\eta y^2$, we get:\n$$ \\frac{1}{\\|w + \\eta y x\\|} \\approx 1 - \\eta y^2 $$\nNow we substitute this approximation back into the equation for $w^{+}$:\n$$ w^{+} \\approx (w + \\eta y x)(1 - \\eta y^2) $$\nExpanding this product:\n$$ w^{+} \\approx w(1 - \\eta y^2) + \\eta y x(1 - \\eta y^2) $$\n$$ w^{+} \\approx w - \\eta y^2 w + \\eta y x - \\eta^2 y^3 x $$\nWe are looking for the leading-order approximation, which means we keep terms up to the first order in $\\eta$ and discard all higher-order terms (i.e., terms with $\\eta^2, \\eta^3, \\dots$).\n$$ w^{+} \\approx w + \\eta y x - \\eta y^2 w $$\nThe weight change is $\\Delta w = w^{+} - w$. Using our approximation for $w^{+}$:\n$$ \\Delta w \\approx (w + \\eta y x - \\eta y^2 w) - w $$\n$$ \\Delta w \\approx \\eta y x - \\eta y^2 w $$\nThis expression can be factored to give the final form of the first-order weight change:\n$$ \\Delta w \\approx \\eta(y x - y^2 w) $$\nThis result is also commonly written as:\n$$ \\Delta w \\approx \\eta y(x - y w) $$\nwhere $y = w^{\\top}x$. This is the mathematical expression for Oja's rule. The problem asks for the first-order term of the weight change $w^{+} - w$, which is this expression.",
            "answer": "$$\n\\boxed{\\eta y (x - y w)}\n$$"
        },
        {
            "introduction": "With a stable learning mechanism in hand, we can ask what computational function it performs. This practice explores how a neuron's receptive field develops when shaped by Hebbian plasticity under different forms of homeostatic control . You will demonstrate that whether the stability is achieved through a multiplicative (Oja-type) or an additive mechanism, the learning dynamics guide the synaptic weight vector to align with the direction of maximum variance in the input data. This exercise solidifies the powerful idea that Hebbian learning is a mechanism for principal component analysis (PCA), enabling neurons to discover and represent the most significant features in their sensory environment.",
            "id": "3987601",
            "problem": "Consider a single linear rate neuron receiving a two-dimensional, zero-mean, stationary Gaussian input $\\mathbf{x} \\in \\mathbb{R}^{2}$ with covariance\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n\\sigma_{1}^{2}  \\rho \\,\\sigma_{1}\\sigma_{2} \\\\\n\\rho \\,\\sigma_{1}\\sigma_{2}  \\sigma_{2}^{2}\n\\end{pmatrix},\n$$\nwhere $\\sigma_{1}  0$, $\\sigma_{2}  0$, and $\\rho \\in (-1,1)$ with $\\rho \\neq 0$. The neuron's firing rate is $y = \\mathbf{w}^{\\top}\\mathbf{x}$, where $\\mathbf{w} \\in \\mathbb{R}^{2}$ are synaptic weights. Learning follows a Hebbian term proportional to the input–output correlation, combined with one of two distinct homeostatic mechanisms:\n\n- Multiplicative homeostasis (Oja-type): \n$$\n\\frac{d\\mathbf{w}}{dt} \\;=\\; \\eta\\big(\\mathbb{E}[y\\,\\mathbf{x}] \\;-\\; \\mathbb{E}[y^{2}]\\,\\mathbf{w}\\big),\n$$\nwith learning rate $\\eta  0$.\n\n- Additive homeostasis with variance control:\n$$\n\\frac{d\\mathbf{w}}{dt} \\;=\\; \\eta\\big(\\mathbb{E}[y\\,\\mathbf{x}] \\;-\\; \\lambda\\,\\mathbf{w}\\big), \\qquad \\frac{d\\lambda}{dt} \\;=\\; \\epsilon\\big(\\mathbb{E}[y^{2}] \\;-\\; q\\big),\n$$\nwith $\\epsilon  0$ and fixed target output variance $q  0$.\n\nStarting from the definitions of covariance and linear response, and without assuming any special structure beyond the given $\\Sigma$, analyze the fixed points of both dynamics in the regime where they converge to a stable, nontrivial receptive field. Show that the learned receptive field direction is identical under both homeostatic mechanisms and corresponds to the principal eigenvector of $\\Sigma$. Then, let the stable learned weight vector be oriented so that $w_{2}  0$. Compute the exact closed-form analytic expression for the ratio $r = w_{1}/w_{2}$ of the principal eigenvector of $\\Sigma$ in terms of $\\sigma_{1}$, $\\sigma_{2}$, and $\\rho$. Your final answer must be a single closed-form expression for $r$. Do not approximate or round.",
            "solution": "The problem asks for the ratio of the components of the stable receptive field vector $\\mathbf{w}$ for a linear neuron under two different Hebbian learning rules. We must first show that the direction of this vector is the same in both cases and corresponds to the principal eigenvector of the input covariance matrix $\\Sigma$.\n\nFirst, we compute the necessary expectation values. The input $\\mathbf{x}$ is a zero-mean Gaussian variable, $\\mathbb{E}[\\mathbf{x}] = \\mathbf{0}$. The output is $y = \\mathbf{w}^{\\top}\\mathbf{x}$. The covariance matrix of the input is given by $\\Sigma = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$.\n\nThe input-output correlation is:\n$$\n\\mathbb{E}[y\\,\\mathbf{x}] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})\\mathbf{x}] = \\mathbb{E}[\\mathbf{x}(\\mathbf{x}^{\\top}\\mathbf{w})] = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w} = \\Sigma\\mathbf{w}\n$$\nThe mean squared output, or output variance, is:\n$$\n\\mathbb{E}[y^2] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})^2] = \\mathbb{E}[(\\mathbf{w}^{\\top}\\mathbf{x})(\\mathbf{x}^{\\top}\\mathbf{w})] = \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w} = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w}\n$$\n\nNow, we analyze the fixed points of the two learning rules. A fixed point $\\mathbf{w}^*$ is reached when $\\frac{d\\mathbf{w}}{dt} = \\mathbf{0}$.\n\nCase 1: Multiplicative homeostasis (Oja-type rule)\nThe learning rule is $\\frac{d\\mathbf{w}}{dt} = \\eta\\big(\\mathbb{E}[y\\,\\mathbf{x}] - \\mathbb{E}[y^{2}]\\,\\mathbf{w}\\big)$.\nSubstituting the expectations, we get:\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta\\big(\\Sigma\\mathbf{w} - (\\mathbf{w}^{\\top}\\Sigma\\mathbf{w})\\mathbf{w}\\big)\n$$\nAt a fixed point, for a non-trivial solution $\\mathbf{w} \\neq \\mathbf{0}$:\n$$\n\\Sigma\\mathbf{w} - (\\mathbf{w}^{\\top}\\Sigma\\mathbf{w})\\mathbf{w} = \\mathbf{0} \\implies \\Sigma\\mathbf{w} = (\\mathbf{w}^{\\top}\\Sigma\\mathbf{w})\\mathbf{w}\n$$\nThis is an eigenvector equation $\\Sigma\\mathbf{w} = \\mu\\mathbf{w}$, where the eigenvalue is $\\mu = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w}$. Thus, any fixed point must be an eigenvector of $\\Sigma$. Stability analysis of Oja's rule shows that the dynamics converge to a stable fixed point direction which is the principal eigenvector of $\\Sigma$ (the eigenvector corresponding to the largest eigenvalue).\n\nCase 2: Additive homeostasis with variance control\nThe dynamics are given by a coupled system:\n$$\n\\text{(i)} \\quad \\frac{d\\mathbf{w}}{dt} = \\eta\\big(\\mathbb{E}[y\\,\\mathbf{x}] - \\lambda\\,\\mathbf{w}\\big) = \\eta(\\Sigma\\mathbf{w} - \\lambda\\mathbf{w})\n$$\n$$\n\\text{(ii)} \\quad \\frac{d\\lambda}{dt} = \\epsilon\\big(\\mathbb{E}[y^{2}] - q\\big) = \\epsilon(\\mathbf{w}^{\\top}\\Sigma\\mathbf{w} - q)\n$$\nAt a fixed point $(\\mathbf{w}^*, \\lambda^*)$, both derivatives are zero. From (i), for a non-trivial solution $\\mathbf{w}^* \\neq \\mathbf{0}$:\n$$\n\\Sigma\\mathbf{w}^* - \\lambda^*\\mathbf{w}^* = \\mathbf{0} \\implies \\Sigma\\mathbf{w}^* = \\lambda^*\\mathbf{w}^*\n$$\nThis again shows that the fixed-point weight vector $\\mathbf{w}^*$ must be an eigenvector of $\\Sigma$, and the homeostatic variable $\\lambda^*$ converges to the corresponding eigenvalue. From (ii):\n$$\n\\mathbf{w}^{*\\top}\\Sigma\\mathbf{w}^* - q = 0 \\implies \\mathbf{w}^{*\\top}\\Sigma\\mathbf{w}^* = q\n$$\nAs with the first rule, stability analysis confirms that the system converges to a state where $\\mathbf{w}^*$ is the principal eigenvector of $\\Sigma$, and $\\lambda^*$ is the largest eigenvalue, $\\mu_{max}$.\n\nIn both cases, the direction of the learned receptive field $\\mathbf{w}$ is that of the principal eigenvector of the input covariance matrix $\\Sigma$. The problem is now reduced to finding this eigenvector.\n\nThe covariance matrix is:\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_{1}^{2}  \\rho \\,\\sigma_{1}\\sigma_{2} \\\\\n\\rho \\,\\sigma_{1}\\sigma_{2}  \\sigma_{2}^{2}\n\\end{pmatrix}\n$$\nThe eigenvalues $\\mu$ are the roots of the characteristic equation $\\det(\\Sigma - \\mu I) = 0$:\n$$\n(\\sigma_1^2 - \\mu)(\\sigma_2^2 - \\mu) - (\\rho \\sigma_1 \\sigma_2)^2 = 0\n$$\n$$\n\\mu^2 - (\\sigma_1^2 + \\sigma_2^2)\\mu + \\sigma_1^2 \\sigma_2^2 (1 - \\rho^2) = 0\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\mu = \\frac{(\\sigma_1^2 + \\sigma_2^2) \\pm \\sqrt{(\\sigma_1^2 + \\sigma_2^2)^2 - 4\\sigma_1^2\\sigma_2^2(1 - \\rho^2)}}{2}\n$$\nThe term under the square root simplifies to $(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2$. The principal eigenvector corresponds to the largest eigenvalue, $\\mu_{max}$, which is obtained by taking the positive sign:\n$$\n\\mu_{max} = \\frac{1}{2}\\left( (\\sigma_1^2 + \\sigma_2^2) + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2} \\right)\n$$\nLet the principal eigenvector be $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$. It must satisfy the equation $(\\Sigma - \\mu_{max}I)\\mathbf{w} = \\mathbf{0}$. We can use the second row of this matrix-vector equation:\n$$\n(\\rho \\sigma_1 \\sigma_2) w_1 + (\\sigma_2^2 - \\mu_{max}) w_2 = 0\n$$\nWe want to find the ratio $r = w_1/w_2$:\n$$\nr = \\frac{w_1}{w_2} = -\\frac{\\sigma_2^2 - \\mu_{max}}{\\rho \\sigma_1 \\sigma_2} = \\frac{\\mu_{max} - \\sigma_2^2}{\\rho \\sigma_1 \\sigma_2}\n$$\nNow we compute the numerator, $\\mu_{max} - \\sigma_2^2$:\n$$\n\\mu_{max} - \\sigma_2^2 = \\frac{1}{2}\\left( (\\sigma_1^2 + \\sigma_2^2) + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2} \\right) - \\sigma_2^2\n$$\n$$\n= \\frac{1}{2}\\left( \\sigma_1^2 + \\sigma_2^2 - 2\\sigma_2^2 + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2} \\right)\n$$\n$$\n= \\frac{1}{2}\\left( \\sigma_1^2 - \\sigma_2^2 + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2} \\right)\n$$\nSubstituting this back into the expression for $r$:\n$$\nr = \\frac{\\frac{1}{2}\\left( \\sigma_1^2 - \\sigma_2^2 + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2} \\right)}{\\rho \\sigma_1 \\sigma_2}\n$$\nThis simplifies to the final closed-form expression for the ratio $r = w_1 / w_2$. The condition $w_2 > 0$ simply orients the eigenvector but does not change this ratio.\n$$\nr = \\frac{\\sigma_1^2 - \\sigma_2^2 + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2}}{2 \\rho \\sigma_1 \\sigma_2}\n$$",
            "answer": "$$\n\\boxed{\\frac{\\sigma_1^2 - \\sigma_2^2 + \\sqrt{(\\sigma_1^2 - \\sigma_2^2)^2 + 4\\rho^2\\sigma_1^2\\sigma_2^2}}{2 \\rho \\sigma_1 \\sigma_2}}\n$$"
        },
        {
            "introduction": "Moving toward more biologically plausible models, we must account for the fact that neuronal firing rates do not grow infinitely but saturate. This saturation introduces a powerful nonlinearity that provides an intrinsic and activity-dependent form of stabilization. In this advanced problem, you will analyze a learning rule that combines a linear Hebbian term with both linear weight decay and a cubic saturating nonlinearity . By deriving the mean-field dynamics and solving for the fixed point, you will see how the final synaptic strength emerges from a precise balance of three distinct forces: Hebbian potentiation, homeostatic decay, and nonlinear saturation, a core principle in the self-organization of neural circuits.",
            "id": "3987647",
            "problem": "Consider a single postsynaptic rate-based neuron with activity modeled as $y=\\phi(w^{\\top} x)$, where $w \\in \\mathbb{R}^{n}$ is the synaptic weight vector, $x \\in \\mathbb{R}^{n}$ is a zero-mean random input, and $\\phi(\\cdot)$ is a smooth odd saturating nonlinearity. The synaptic dynamics follow a combined Hebbian learning and weight decay rule\n$$\n\\dot{w}=\\eta\\,y\\,x-\\gamma\\,w,\n$$\nwhere $\\eta0$ is the learning rate and $\\gamma0$ is the decay coefficient. Assume the inputs are stationary and Gaussian with covariance $\\Sigma\\in\\mathbb{R}^{n\\times n}$ that is symmetric positive definite (SPD), i.e., $x\\sim\\mathcal{N}(0,\\Sigma)$, and that the saturating nonlinearity is well approximated in the operating regime by the truncated odd cubic\n$$\n\\phi(u)=u-\\alpha u^{3},\n$$\nwith $\\alpha0$ characterizing the saturation strength. Work in the mean-field limit where averaging over the input distribution is justified for the dynamics of $w$.\n\nStarting from fundamental definitions of Hebbian learning and rate-based formulations, derive the mean-field dynamical equation for the expected synaptic update in terms of $w$ and the input covariance $\\Sigma$. Then, under the assumption that $w$ aligns with the principal eigenvector $v_{1}$ of $\\Sigma$ (associated with the largest eigenvalue $\\lambda_{1}0$), compute the nontrivial fixed point amplitude $a^{\\ast}0$ in $w^{\\ast}=a^{\\ast}v_{1}$. Show explicitly how the decay coefficient $\\gamma$ sets the steady-state norm through input statistics (specifically $\\lambda_{1}$) and the saturation parameter $\\alpha$. Your final answer should be the closed-form analytic expression for $a^{\\ast}$ in terms of $\\eta$, $\\gamma$, $\\alpha$, and $\\lambda_{1}$. No numerical evaluation is required, and no units should be included in the final expression.",
            "solution": "The problem requires us to find the fixed point of a Hebbian learning rule with a nonlinear activation function and weight decay. We will first derive the mean-field dynamical equation and then solve for the fixed point amplitude.\n\n**1. Derivation of the Mean-Field Dynamical Equation**\n\nThe synaptic dynamics are given by $\\dot{w} = \\eta\\,y\\,x - \\gamma\\,w$. We substitute $y = \\phi(w^\\top x)$ and average over the input distribution of $x$, treating $w$ as quasi-static. This gives the mean-field equation:\n$$\n\\dot{w} = \\eta \\langle \\phi(w^\\top x)\\,x \\rangle_x - \\gamma w\n$$\nWe use the cubic approximation for the nonlinearity, $\\phi(u) = u - \\alpha u^3$. Let $u = w^\\top x$. The expectation term becomes:\n$$\n\\langle \\phi(u)\\,x \\rangle_x = \\langle (u - \\alpha u^3)x \\rangle_x = \\langle ux \\rangle_x - \\alpha \\langle u^3 x \\rangle_x\n$$\nWe compute each term using the properties of the zero-mean Gaussian input $x \\sim \\mathcal{N}(0, \\Sigma)$.\n- The first term is the standard input-output correlation for a linear neuron:\n$$\n\\langle ux \\rangle_x = \\langle (w^\\top x)x \\rangle_x = \\langle x x^\\top \\rangle_x w = \\Sigma w\n$$\n- For the second term, we use Isserlis' theorem for the product of four zero-mean Gaussian variables. The $j$-th component of the vector $\\langle u^3 x \\rangle_x$ is $\\langle u^3 x_j \\rangle = 3 \\langle u^2 \\rangle \\langle u x_j \\rangle$.\nThe required expectations are $\\langle u^2 \\rangle = \\langle (w^\\top x)^2 \\rangle = w^\\top \\Sigma w$ and $\\langle u x \\rangle = \\Sigma w$.\nThus, the vector expectation is:\n$$\n\\langle u^3 x \\rangle_x = 3 \\langle u^2 \\rangle \\langle u x \\rangle_x = 3 (w^\\top \\Sigma w) (\\Sigma w)\n$$\nSubstituting these back, the full expectation is:\n$$\n\\langle \\phi(u)\\,x \\rangle_x = \\Sigma w - 3\\alpha (w^\\top \\Sigma w) (\\Sigma w)\n$$\nThe mean-field dynamical equation is therefore:\n$$\n\\dot{w} = \\eta \\left[ \\Sigma w - 3\\alpha(w^\\top \\Sigma w)\\Sigma w \\right] - \\gamma w\n$$\n\n**2. Computation of the Nontrivial Fixed Point Amplitude**\n\nWe assume the weight vector aligns with the principal eigenvector $v_1$ of $\\Sigma$, such that $w(t) = a(t)v_1$. The eigenvector is normalized ($v_1^\\top v_1 = 1$) and corresponds to the largest eigenvalue $\\lambda_1$, so $\\Sigma v_1 = \\lambda_1 v_1$. Substituting $w = a v_1$ into the dynamical equation:\n$$\n\\frac{d}{dt}(a v_1) = \\eta \\left[ \\Sigma (a v_1) - 3\\alpha((a v_1)^\\top \\Sigma (a v_1))\\Sigma (a v_1) \\right] - \\gamma (a v_1)\n$$\nThe left side becomes $\\dot{a} v_1$. We simplify the terms on the right:\n- $\\Sigma(a v_1) = a \\lambda_1 v_1$\n- $(a v_1)^\\top \\Sigma (a v_1) = a^2 v_1^\\top (\\Sigma v_1) = a^2 v_1^\\top (\\lambda_1 v_1) = a^2 \\lambda_1$\n- The cubic term becomes $3\\alpha(a^2 \\lambda_1)(a \\lambda_1 v_1) = 3\\alpha a^3 \\lambda_1^2 v_1$\n\nSubstituting these into the equation and factoring out $v_1$:\n$$\n\\dot{a}v_1 = \\left[ \\eta(a \\lambda_1 - 3\\alpha a^3 \\lambda_1^2) - \\gamma a \\right] v_1\n$$\nThis vector equation reduces to a scalar ODE for the amplitude $a$:\n$$\n\\dot{a} = (\\eta \\lambda_1 - \\gamma)a - 3\\eta \\alpha \\lambda_1^2 a^3\n$$\nTo find the fixed point amplitude $a^*$, we set $\\dot{a} = 0$:\n$$\n0 = (\\eta \\lambda_1 - \\gamma)a^* - 3\\eta \\alpha \\lambda_1^2 (a^*)^3\n$$\nFactoring out $a^*$:\n$$\n0 = a^* \\left[ (\\eta \\lambda_1 - \\gamma) - 3\\eta \\alpha \\lambda_1^2 (a^*)^2 \\right]\n$$\nThis equation yields the trivial fixed point $a^*=0$ and a nontrivial solution when the term in the brackets is zero. The problem asks for the nontrivial amplitude $a^* > 0$.\n$$\n(\\eta \\lambda_1 - \\gamma) - 3\\eta \\alpha \\lambda_1^2 (a^*)^2 = 0\n$$\nSolving for $(a^*)^2$:\n$$\n(a^*)^2 = \\frac{\\eta \\lambda_1 - \\gamma}{3\\eta \\alpha \\lambda_1^2}\n$$\nFor a real, positive solution to exist, we must have $\\eta \\lambda_1 > \\gamma$. Taking the positive square root gives the final answer:\n$$\na^* = \\sqrt{\\frac{\\eta \\lambda_1 - \\gamma}{3\\eta \\alpha \\lambda_1^2}}\n$$\nThis is the nontrivial fixed point amplitude for the synaptic weight vector.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{\\eta \\lambda_{1} - \\gamma}{3\\eta \\alpha \\lambda_{1}^{2}}}}\n$$"
        }
    ]
}