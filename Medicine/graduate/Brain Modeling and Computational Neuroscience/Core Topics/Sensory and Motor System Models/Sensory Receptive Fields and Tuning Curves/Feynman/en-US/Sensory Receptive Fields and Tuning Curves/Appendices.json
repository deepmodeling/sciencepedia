{
    "hands_on_practices": [
        {
            "introduction": "The Difference-of-Gaussians (DoG) model is a classic and powerful description of the center-surround receptive fields found in the early visual system. This exercise provides fundamental practice in applying the linear filtering model, requiring you to calculate a neuron's response to a patterned stimulus from first principles. By deriving the impact of the surround on the center response , you will gain a concrete understanding of how these receptive fields detect spatial contrast.",
            "id": "4017975",
            "problem": "A relay neuron in the early visual pathway is modeled with a circularly symmetric receptive field given by a Difference of Gaussians (DoG), where the center is excitatory and the surround is inhibitory. The spatial receptive field kernel is defined as\n$$\nh(\\mathbf{x}) \\equiv h(r) = k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) \\;-\\; k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right),\n$$\nwith $r = \\|\\mathbf{x}\\|$, $k_{c} > 0$, $k_{s} > 0$, $\\sigma_{c} > 0$, and $\\sigma_{s} > 0$. The linear generator signal is defined by the standard linear filtering operation\n$$\nG \\;=\\; \\int_{\\mathbb{R}^{2}} h(\\mathbf{x})\\,s(\\mathbf{x})\\,\\mathrm{d}^{2}\\mathbf{x},\n$$\nwhere $s(\\mathbf{x})$ is the stimulus contrast field (dimensionless). Consider a stimulus that consists of a center disk of radius $R_{c}$ at uniform contrast $C_{c}$, surrounded by an annulus extending from radius $R_{c}$ to $R_{s}$ at uniform contrast $C_{s}$, and zero contrast outside radius $R_{s}$. That is,\n$$\ns(r) \\;=\\; \\begin{cases}\nC_{c}, & 0 \\le r \\le R_{c},\\\\\nC_{s}, & R_{c} < r \\le R_{s},\\\\\n0, & r > R_{s}.\n\\end{cases}\n$$\nLet $G_{\\mathrm{full}}$ denote the generator signal for the full stimulus described above, and let $G_{\\mathrm{center}}$ denote the generator signal for the center disk alone (that is, the same $C_{c}$ on $0 \\le r \\le R_{c}$ but with $C_{s} = 0$ for $r > R_{c}$). Using only the definitions given and standard calculus, derive an exact, closed-form analytical expression for the surround impact\n$$\n\\Delta G \\equiv G_{\\mathrm{full}} - G_{\\mathrm{center}}\n$$\nin terms of $C_{c}$, $C_{s}$, $k_{c}$, $k_{s}$, $\\sigma_{c}$, $\\sigma_{s}$, $R_{c}$, and $R_{s}$.\n\nExpress your final answer as a single simplified symbolic expression. Report the generator signal in arbitrary units (a.u.).",
            "solution": "We begin from the definition of the linear generator signal as the spatial inner product of the receptive field with the stimulus,\n$$\nG \\;=\\; \\int_{\\mathbb{R}^{2}} h(\\mathbf{x})\\,s(\\mathbf{x})\\,\\mathrm{d}^{2}\\mathbf{x}.\n$$\nThe receptive field is radially symmetric and is given by a Difference of Gaussians (DoG),\n$$\nh(r) \\;=\\; k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) \\;-\\; k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right),\n$$\nwhere each Gaussian component is normalized to unit integral over the plane:\n$$\n\\int_{\\mathbb{R}^{2}} \\frac{1}{2\\pi \\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\mathrm{d}^{2}\\mathbf{x} \\;=\\; 1.\n$$\nThe stimulus is also radially symmetric and piecewise constant in radius. Because both $h$ and $s$ depend only on $r = \\|\\mathbf{x}\\|$, the two-dimensional integral reduces, in polar coordinates, to\n$$\nG \\;=\\; \\int_{0}^{\\infty} h(r)\\,s(r)\\,2\\pi r\\,\\mathrm{d}r.\n$$\nLet us define the cumulative integral of a normalized Gaussian over a disk of radius $R$:\n$$\nI(\\sigma; R) \\;\\equiv\\; \\int_{0}^{R} \\frac{1}{2\\pi \\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\,2\\pi r\\,\\mathrm{d}r \\;=\\; \\int_{0}^{R} \\frac{1}{\\sigma^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right)\\,r\\,\\mathrm{d}r.\n$$\nMake the substitution $u = \\frac{r^{2}}{2\\sigma^{2}}$, so that $\\mathrm{d}u = \\frac{r}{\\sigma^{2}}\\mathrm{d}r$ and $r\\,\\mathrm{d}r = \\sigma^{2}\\mathrm{d}u$. The integral becomes\n$$\nI(\\sigma; R) \\;=\\; \\int_{u=0}^{u=\\frac{R^{2}}{2\\sigma^{2}}} \\exp(-u)\\,\\mathrm{d}u \\;=\\; 1 - \\exp\\!\\left(-\\frac{R^{2}}{2\\sigma^{2}}\\right).\n$$\nTherefore, the integral of a normalized Gaussian over an annulus from $R_{1}$ to $R_{2}$, with $0 \\le R_{1} < R_{2}$, is\n$$\nI(\\sigma; R_{2}) - I(\\sigma; R_{1}) \\;=\\; \\left[1 - \\exp\\!\\left(-\\frac{R_{2}^{2}}{2\\sigma^{2}}\\right)\\right] - \\left[1 - \\exp\\!\\left(-\\frac{R_{1}^{2}}{2\\sigma^{2}}\\right)\\right] \\;=\\; \\exp\\!\\left(-\\frac{R_{1}^{2}}{2\\sigma^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{2}^{2}}{2\\sigma^{2}}\\right).\n$$\nUsing linearity of the integral and the DoG decomposition of $h(r)$, we express the generator signal as the sum of contributions from the center disk and the surround annulus:\n$$\nG_{\\mathrm{full}} \\;=\\; \\int_{0}^{R_{c}} \\!\\!\\left[k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) - k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right)\\right] C_{c}\\,2\\pi r\\,\\mathrm{d}r\n$$\n$$\n\\quad + \\int_{R_{c}}^{R_{s}} \\!\\!\\left[k_{c}\\,\\frac{1}{2\\pi \\sigma_{c}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{c}^{2}}\\right) - k_{s}\\,\\frac{1}{2\\pi \\sigma_{s}^{2}}\\exp\\!\\left(-\\frac{r^{2}}{2\\sigma_{s}^{2}}\\right)\\right] C_{s}\\,2\\pi r\\,\\mathrm{d}r.\n$$\nBy the definitions above, these evaluate to\n$$\nG_{\\mathrm{full}} \\;=\\; C_{c}\\left[k_{c}\\,I(\\sigma_{c}; R_{c}) - k_{s}\\,I(\\sigma_{s}; R_{c})\\right] \\;+\\; C_{s}\\left\\{k_{c}\\left[I(\\sigma_{c}; R_{s}) - I(\\sigma_{c}; R_{c})\\right] - k_{s}\\left[I(\\sigma_{s}; R_{s}) - I(\\sigma_{s}; R_{c})\\right]\\right\\}.\n$$\nSimilarly, the generator signal for the center-only stimulus ($C_{s}=0$) is\n$$\nG_{\\mathrm{center}} \\;=\\; C_{c}\\left[k_{c}\\,I(\\sigma_{c}; R_{c}) - k_{s}\\,I(\\sigma_{s}; R_{c})\\right].\n$$\nTherefore, the surround impact is\n$$\n\\Delta G \\;\\equiv\\; G_{\\mathrm{full}} - G_{\\mathrm{center}} \\;=\\; C_{s}\\left\\{k_{c}\\left[I(\\sigma_{c}; R_{s}) - I(\\sigma_{c}; R_{c})\\right] - k_{s}\\left[I(\\sigma_{s}; R_{s}) - I(\\sigma_{s}; R_{c})\\right]\\right\\}.\n$$\nSubstituting the closed-form expression for the annular integrals yields\n$$\nI(\\sigma; R_{s}) - I(\\sigma; R_{c}) \\;=\\; \\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma^{2}}\\right),\n$$\nso that\n$$\n\\Delta G \\;=\\; C_{s}\\left\\{k_{c}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{c}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{c}^{2}}\\right)\\right] \\;-\\; k_{s}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{s}^{2}}\\right) - \\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{s}^{2}}\\right)\\right]\\right\\}.\n$$\nThis is the exact, closed-form analytical expression for the surround impact on the generator signal in arbitrary units (a.u.).",
            "answer": "$$\\boxed{C_{s}\\left\\{k_{c}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{c}^{2}}\\right)-\\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{c}^{2}}\\right)\\right]-k_{s}\\left[\\exp\\!\\left(-\\frac{R_{c}^{2}}{2\\sigma_{s}^{2}}\\right)-\\exp\\!\\left(-\\frac{R_{s}^{2}}{2\\sigma_{s}^{2}}\\right)\\right]\\right\\}}$$"
        },
        {
            "introduction": "While forward models describe how a neuron should respond, a central task in neuroscience is the inverse problem: estimating a receptive field from noisy experimental data. This practice moves beyond simple least-squares to the more robust framework of Maximum A Posteriori (MAP) estimation, which is essential for dealing with ill-posed problems. By deriving the ridge regression estimator, which corresponds to an $\\ell_{2}$ penalty, from a Bayesian perspective , you will explore the fundamental bias-variance tradeoff and understand how regularization leads to more stable and plausible filter estimates.",
            "id": "4017986",
            "problem": "Consider a linear receptive field model for a sensory neuron in which the measured response vector $\\mathbf{r} \\in \\mathbb{R}^{T}$ to a sequence of stimuli is modeled as $\\mathbf{r} = \\mathbf{X}\\mathbf{k} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{X} \\in \\mathbb{R}^{T \\times d}$ is the stimulus design matrix (each row a stimulus and columns corresponding to $d$ features), $\\mathbf{k} \\in \\mathbb{R}^{d}$ is the unknown receptive field (filter) to be estimated, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{T}$ is additive measurement noise. Assume the noise is independent and identically distributed Gaussian with zero mean and covariance $\\sigma^{2}\\mathbf{I}_{T}$, and that the prior over the receptive field is Gaussian with isotropic precision, specified by $p(\\mathbf{k}) \\propto \\exp\\!\\big(-\\lambda \\|\\mathbf{k}\\|_{2}^{2}\\big)$ for a known constant $\\lambda > 0$. Using Bayesâ€™ theorem as the foundational principle and assuming the linear-Gaussian observation model, derive the Maximum A Posteriori (MAP) estimator $\\hat{\\mathbf{k}}_{\\text{MAP}}$ that maximizes the posterior distribution $p(\\mathbf{k}\\mid \\mathbf{r}, \\mathbf{X})$. Express your final result in closed form in terms of $\\mathbf{X}$, $\\mathbf{r}$, $\\sigma^{2}$, and $\\lambda$. Then, starting from the definitions of estimator bias and variance for linear estimators under Gaussian noise, explain how the $\\ell_{2}$ penalty implied by the Gaussian prior implements ridge regularization that trades bias for variance in the filter estimates as $\\lambda$ varies, without invoking any shortcut formulas. Your final answer must be a single closed-form analytic expression for $\\hat{\\mathbf{k}}_{\\text{MAP}}$. No numerical approximation is required.",
            "solution": "The derivation of the MAP estimator $\\hat{\\mathbf{k}}_{\\text{MAP}}$ begins with Bayes' theorem, which states that the posterior distribution of the receptive field $\\mathbf{k}$ given the measured response $\\mathbf{r}$ and stimulus matrix $\\mathbf{X}$ is proportional to the product of the likelihood of the data and the prior probability of the receptive field:\n$$\np(\\mathbf{k} \\mid \\mathbf{r}, \\mathbf{X}) \\propto p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) p(\\mathbf{k})\n$$\nThe MAP estimate is the value of $\\mathbf{k}$ that maximizes this posterior probability. It is computationally more convenient to maximize the logarithm of the posterior, as the logarithm is a monotonic function. The log-posterior is given by:\n$$\n\\ln p(\\mathbf{k} \\mid \\mathbf{r}, \\mathbf{X}) = \\ln p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) + \\ln p(\\mathbf{k}) + C\n$$\nwhere $C$ is a constant that does not depend on $\\mathbf{k}$.\n\nFirst, we define the likelihood term, $p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X})$. The model is $\\mathbf{r} = \\mathbf{X}\\mathbf{k} + \\boldsymbol{\\varepsilon}$, where the noise $\\boldsymbol{\\varepsilon}$ is drawn from a zero-mean Gaussian distribution with covariance $\\sigma^{2}\\mathbf{I}_{T}$. This implies that the response $\\mathbf{r}$ is also Gaussian, with mean $\\mathbf{X}\\mathbf{k}$ and covariance $\\sigma^{2}\\mathbf{I}_{T}$. The probability density function is:\n$$\np(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) = \\frac{1}{(2\\pi\\sigma^{2})^{T/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} (\\mathbf{r} - \\mathbf{X}\\mathbf{k})^{T}(\\mathbf{r} - \\mathbf{X}\\mathbf{k})\\right) = \\frac{1}{(2\\pi\\sigma^{2})^{T/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|\\mathbf{r} - \\mathbf{X}\\mathbf{k}\\|_{2}^{2}\\right)\n$$\nThe log-likelihood is therefore:\n$$\n\\ln p(\\mathbf{r} \\mid \\mathbf{k}, \\mathbf{X}) = -\\frac{T}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}} \\|\\mathbf{r} - \\mathbf{X}\\mathbf{k}\\|_{2}^{2}\n$$\n\nNext, we consider the prior distribution over the receptive field, $p(\\mathbf{k})$, which is given as $p(\\mathbf{k}) \\propto \\exp(-\\lambda \\|\\mathbf{k}\\|_{2}^{2})$. The log-prior is:\n$$\n\\ln p(\\mathbf{k}) = -\\lambda \\|\\mathbf{k}\\|_{2}^{2} + C'\n$$\nwhere $C'$ is another constant independent of $\\mathbf{k}$.\n\nCombining these, the log-posterior objective function $\\mathcal{L}(\\mathbf{k})$ to be maximized with respect to $\\mathbf{k}$ is:\n$$\n\\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} \\|\\mathbf{r} - \\mathbf{X}\\mathbf{k}\\|_{2}^{2} - \\lambda \\|\\mathbf{k}\\|_{2}^{2} + \\text{constants}\n$$\nTo find the maximum, we compute the gradient of $\\mathcal{L}(\\mathbf{k})$ with respect to $\\mathbf{k}$ and set it to the zero vector. We first expand the squared norms:\n$$\n\\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} (\\mathbf{r}^{T}\\mathbf{r} - 2\\mathbf{r}^{T}\\mathbf{X}\\mathbf{k} + \\mathbf{k}^{T}\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - \\lambda \\mathbf{k}^{T}\\mathbf{k} + \\text{constants}\n$$\nThe gradient is:\n$$\n\\nabla_{\\mathbf{k}} \\mathcal{L}(\\mathbf{k}) = -\\frac{1}{2\\sigma^{2}} (-2\\mathbf{X}^{T}\\mathbf{r} + 2\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - 2\\lambda\\mathbf{k}\n$$\nSetting the gradient to $\\mathbf{0}$:\n$$\n\\frac{1}{\\sigma^{2}}(\\mathbf{X}^{T}\\mathbf{r} - \\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}) - 2\\lambda\\mathbf{k} = \\mathbf{0}\n$$\nMultiplying by $\\sigma^{2}$ and rearranging terms to solve for $\\mathbf{k}$:\n$$\n\\mathbf{X}^{T}\\mathbf{r} - \\mathbf{X}^{T}\\mathbf{X}\\mathbf{k} = 2\\sigma^{2}\\lambda\\mathbf{k} \\\\\n\\mathbf{X}^{T}\\mathbf{r} = \\mathbf{X}^{T}\\mathbf{X}\\mathbf{k} + 2\\sigma^{2}\\lambda\\mathbf{k} \\\\\n\\mathbf{X}^{T}\\mathbf{r} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})\\mathbf{k}\n$$\nwhere $\\mathbf{I}_{d}$ is the $d \\times d$ identity matrix. The MAP estimator $\\hat{\\mathbf{k}}_{\\text{MAP}}$ is found by pre-multiplying by the inverse of the matrix term. Since $\\mathbf{X}^{T}\\mathbf{X}$ is positive semi-definite and $\\lambda > 0$, $\\sigma^2 > 0$, the matrix $(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})$ is positive definite and thus invertible.\n$$\n\\hat{\\mathbf{k}}_{\\text{MAP}} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{r}\n$$\nThis is the closed-form expression for the MAP estimator, which is equivalent to the solution for ridge regression.\n\nThe second part of the question asks for an explanation of the bias-variance tradeoff. Let the true receptive field be $\\mathbf{k}_{\\text{true}}$. The data is generated as $\\mathbf{r} = \\mathbf{X}\\mathbf{k}_{\\text{true}} + \\boldsymbol{\\varepsilon}$.\nThe bias is $\\text{Bias}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] - \\mathbf{k}_{\\text{true}}$. The expectation of the estimator is:\n$$\n\\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbb{E}[\\mathbf{r}] = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X}\\mathbf{k}_{\\text{true}}\n$$\nFor $\\lambda > 0$, $\\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] \\neq \\mathbf{k}_{\\text{true}}$, so the estimator is biased. This bias increases with $\\lambda$.\nThe variance is $\\text{Var}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\mathbb{E}[(\\hat{\\mathbf{k}}_{\\text{MAP}} - \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}])(\\hat{\\mathbf{k}}_{\\text{MAP}} - \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}])^{T}]$. Let $\\mathbf{M} = (\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}$. Then $\\hat{\\mathbf{k}}_{\\text{MAP}} - \\mathbb{E}[\\hat{\\mathbf{k}}_{\\text{MAP}}] = \\mathbf{M}(\\mathbf{r} - \\mathbb{E}[\\mathbf{r}]) = \\mathbf{M}\\boldsymbol{\\varepsilon}$.\nThe variance is:\n$$\n\\text{Var}(\\hat{\\mathbf{k}}_{\\text{MAP}}) = \\mathbf{M} \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{T}] \\mathbf{M}^{T} = \\sigma^{2}\\mathbf{M}\\mathbf{M}^{T} = \\sigma^{2}(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\n$$\nCompared to the unbiased MLE case ($\\lambda=0$), where variance is $\\sigma^{2}(\\mathbf{X}^{T}\\mathbf{X})^{-1}$, the ridge term $2\\sigma^{2}\\lambda\\mathbf{I}_{d}$ regularizes the matrix inversion, especially when $\\mathbf{X}^{T}\\mathbf{X}$ is ill-conditioned. This reduces the variance of the estimate. Thus, increasing $\\lambda$ increases bias but decreases variance.",
            "answer": "$$\n\\boxed{(\\mathbf{X}^{T}\\mathbf{X} + 2\\sigma^{2}\\lambda\\mathbf{I}_{d})^{-1}\\mathbf{X}^{T}\\mathbf{r}}\n$$"
        },
        {
            "introduction": "A neuron's response is fundamentally stochastic, and the choice of statistical model has profound implications for how we interpret neural coding. This exercise challenges you to look beyond the standard Poisson model and analyze the consequences of under- and over-dispersion in spike counts. By deriving the Fano factor and Fisher information for different generative models , you will learn to quantify neural variability and understand its direct impact on the precision of sensory encoding.",
            "id": "4017987",
            "problem": "A single sensory neuron is probed with a scalar stimulus $s$ and recorded over a fixed counting window of duration $T$. Let $N$ denote the spike count in the window. The mean spike count as a function of stimulus is denoted $\\mu(s) = \\mathbb{E}[N \\mid s]$. Consider three alternative generative models for the trial-to-trial variability of $N$ conditioned on $s$:\n\n- Poisson spiking: Conditional on $s$, $N$ has a Poisson distribution with mean $\\mu(s)$.\n\n- Bernoulli-binomial spiking: The window is divided into $m$ equal sub-bins, with $m \\in \\mathbb{N}$ large enough that at most one spike can occur in any sub-bin. Conditional on $s$, each sub-bin contains a spike with probability $p(s) \\in [0,1]$, independently across sub-bins. The total count $N$ is the sum over sub-bins.\n\n- Negative binomial spiking via a Poisson-Gamma mixture: Conditional on $s$, the instantaneous rate $\\lambda$ fluctuates across trials due to slow gain modulations and has a Gamma distribution with shape parameter $k > 0$ and scale parameter $\\theta(s)$ chosen so that $\\mathbb{E}[\\lambda T \\mid s] = \\mu(s)$. Conditional on $\\lambda$ and $s$, the count $N$ is Poisson with mean $\\lambda T$. Equivalently, the marginal distribution of $N$ given $s$ is negative binomial with shape $k$ and mean $\\mu(s)$, having probability mass function\n$$\np(N \\mid \\mu(s), k) = \\frac{\\Gamma(N + k)}{\\Gamma(k)\\,N!} \\left(\\frac{k}{k + \\mu(s)}\\right)^{k} \\left(\\frac{\\mu(s)}{k + \\mu(s)}\\right)^{N}.\n$$\n\nPart $1$: Starting from the definitions above for each model, derive the Fano factor $F(s) = \\mathrm{Var}[N \\mid s] / \\mathbb{E}[N \\mid s]$ in terms of $\\mu(s)$, $p(s)$, and $k$. Express your answers in simplest symbolic form.\n\nPart $2$: Suppose the tuning curve is parameterized by a positive scalar gain $g > 0$ via $\\mu(s; g) = g\\, f(s)$, where $f(s) > 0$ is a known deterministic function. Treat $k$ as known and fixed. For a single observation at stimulus $s$, derive the expected Fisher information (FI) for $g$ under:\n\n- the Poisson model, and\n\n- the negative binomial model with probability mass function as given above.\n\nThen derive the ratio $R(s; g) = I_{\\mathrm{NB}}(s; g) / I_{\\mathrm{Pois}}(s; g)$ as a function of $\\mu(s; g)$ and $k$.\n\nYour final answer must be a single row matrix containing the three Fano factors from Part $1$ and the information ratio $R(s; g)$ from Part $2$ in this order:\n$$\n\\left[F_{\\mathrm{Pois}}(s) ,\\, F_{\\mathrm{Bern}}(s) ,\\, F_{\\mathrm{NB}}(s) ,\\, R(s; g)\\right].\n$$\nExpress everything symbolically. Do not include any units. No numerical rounding is required.",
            "solution": "This problem has two parts. First, we derive the Fano factor for three different spike count models. Second, we derive the Fisher information for a gain parameter $g$ under two of these models and compute their ratio.\n\n**Part 1: Fano Factor Derivations**\n\nThe Fano factor is defined as $F(s) = \\frac{\\mathrm{Var}[N \\mid s]}{\\mathbb{E}[N \\mid s]}$.\n\n*   **Poisson Model:** For a Poisson-distributed random variable $N$ with mean $\\mu(s)$, the variance is also equal to the mean.\n    -   Mean: $\\mathbb{E}[N \\mid s] = \\mu(s)$\n    -   Variance: $\\mathrm{Var}[N \\mid s] = \\mu(s)$\n    -   Thus, the Fano factor is $F_{\\mathrm{Pois}}(s) = \\frac{\\mu(s)}{\\mu(s)} = 1$. This indicates that the variance of the spike count equals its mean, a key property of Poisson processes.\n\n*   **Bernoulli-Binomial Model:** The spike count $N$ is the sum of $m$ independent Bernoulli trials with success probability $p(s)$, so $N$ follows a binomial distribution $N \\sim \\text{Binomial}(m, p(s))$.\n    -   Mean: $\\mathbb{E}[N \\mid s] = m p(s)$. We equate this with $\\mu(s)$, so $\\mu(s) = m p(s)$.\n    -   Variance: $\\mathrm{Var}[N \\mid s] = m p(s) (1 - p(s)) = \\mu(s)(1 - p(s))$.\n    -   The Fano factor is $F_{\\mathrm{Bern}}(s) = \\frac{\\mu(s)(1 - p(s))}{\\mu(s)} = 1 - p(s)$. Since $p(s) \\in [0,1]$, this model is under-dispersed ($F \\le 1$).\n\n*   **Negative Binomial Model:** The spike count $N$ follows a negative binomial distribution with mean $\\mu(s)$ and shape parameter $k$. For this parameterization, the variance is a known property:\n    -   Mean: $\\mathbb{E}[N \\mid s] = \\mu(s)$\n    -   Variance: $\\mathrm{Var}[N \\mid s] = \\mu(s) \\left(1 + \\frac{\\mu(s)}{k}\\right)$\n    -   The Fano factor is $F_{\\mathrm{NB}}(s) = \\frac{\\mu(s) \\left(1 + \\frac{\\mu(s)}{k}\\right)}{\\mu(s)} = 1 + \\frac{\\mu(s)}{k}$. Since $\\mu(s) > 0$ and $k > 0$, this model is over-dispersed ($F > 1$), which is often observed in real neural data.\n\n**Part 2: Fisher Information and Ratio**\n\nThe Fisher information (FI) for a parameter $\\theta$ in a model with probability mass function $p(N \\mid \\theta)$ is $I(\\theta) = \\mathbb{E}\\left[ \\left(\\frac{\\partial \\ln p(N \\mid \\theta)}{\\partial \\theta}\\right)^2 \\right]$. The parameter of interest is the gain $g$, which affects the mean count via $\\mu(s; g) = g f(s)$.\n\n*   **Fisher Information for Poisson Model ($I_{\\mathrm{Pois}}$):**\n    The log-likelihood for the Poisson model is $\\ln p(N \\mid \\mu) = N \\ln(\\mu) - \\mu - \\ln(N!)$. The derivative with respect to $\\mu$ is $\\frac{\\partial \\ln p}{\\partial \\mu} = \\frac{N - \\mu}{\\mu}$. The FI for the mean parameter $\\mu$ is $I(\\mu) = \\mathbb{E}\\left[\\left(\\frac{N - \\mu}{\\mu}\\right)^2\\right] = \\frac{\\mathrm{Var}[N]}{\\mu^2} = \\frac{\\mu}{\\mu^2} = \\frac{1}{\\mu}$. Using the chain rule for parameters, $I(g) = \\left(\\frac{\\partial \\mu}{\\partial g}\\right)^2 I(\\mu)$. With $\\frac{\\partial \\mu}{\\partial g} = f(s)$, we have:\n    $$\n    I_{\\mathrm{Pois}}(s; g) = (f(s))^2 \\cdot \\frac{1}{\\mu(s;g)} = \\frac{(f(s))^2}{g f(s)} = \\frac{f(s)}{g}.\n    $$\n\n*   **Fisher Information for Negative Binomial Model ($I_{\\mathrm{NB}}$):**\n    The log-likelihood is $\\ln p(N|\\mu,k) = \\ln(\\Gamma(N+k)) - \\ln(\\Gamma(k)) - \\ln(N!) + k\\ln(k) - (N+k)\\ln(k+\\mu) + N\\ln(\\mu)$. The derivative with respect to $\\mu$ is:\n    $$\n    \\frac{\\partial \\ln p}{\\partial \\mu} = -\\frac{N+k}{k+\\mu} + \\frac{N}{\\mu} = \\frac{k(N-\\mu)}{\\mu(k+\\mu)}.\n    $$\n    The FI for $\\mu$ is $I_{\\mathrm{NB}}(\\mu) = \\mathbb{E}\\left[\\left(\\frac{k(N-\\mu)}{\\mu(k+\\mu)}\\right)^2\\right] = \\frac{k^2}{(\\mu(k+\\mu))^2} \\mathrm{Var}[N]$. Substituting the variance $\\mathrm{Var}[N] = \\mu(1+\\frac{\\mu}{k}) = \\mu\\frac{k+\\mu}{k}$:\n    $$\n    I_{\\mathrm{NB}}(\\mu) = \\frac{k^2}{(\\mu(k+\\mu))^2} \\left(\\mu\\frac{k+\\mu}{k}\\right) = \\frac{k}{\\mu(k+\\mu)}.\n    $$\n    The FI for $g$ is then:\n    $$\n    I_{\\mathrm{NB}}(s; g) = (f(s))^2 \\cdot \\frac{k}{\\mu(s;g)(k+\\mu(s;g))} = \\frac{k (f(s))^2}{g f(s) (k+g f(s))} = \\frac{k f(s)}{g(k+g f(s))}.\n    $$\n\n*   **Ratio $R(s; g)$:**\n    Finally, we compute the ratio $R(s; g) = \\frac{I_{\\mathrm{NB}}(s; g)}{I_{\\mathrm{Pois}}(s; g)}$:\n    $$\n    R(s; g) = \\frac{\\frac{k f(s)}{g(k+g f(s))}}{\\frac{f(s)}{g}} = \\frac{k f(s)}{g(k+g f(s))} \\cdot \\frac{g}{f(s)} = \\frac{k}{k+g f(s)} = \\frac{k}{k + \\mu(s; g)}.\n    $$\nThis ratio shows that the information carried by a negative binomial neuron is reduced relative to a Poisson neuron with the same mean firing rate. The reduction factor is directly related to the Fano factor, as $R = 1/(F_{\\mathrm{NB}})$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 1 - p(s) & 1 + \\frac{\\mu(s)}{k} & \\frac{k}{k + \\mu(s; g)} \\end{pmatrix}}\n$$"
        }
    ]
}