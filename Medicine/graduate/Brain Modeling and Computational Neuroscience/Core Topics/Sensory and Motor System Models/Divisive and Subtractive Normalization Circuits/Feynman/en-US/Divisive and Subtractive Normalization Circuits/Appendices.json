{
    "hands_on_practices": [
        {
            "introduction": "Divisive normalization is a fundamental computation thought to be widespread across the nervous system, from sensory processing to decision-making. To truly understand this mechanism, it is essential to move from the abstract concept to concrete calculation. This first exercise provides a direct, hands-on opportunity to apply the canonical divisive normalization equation to a simple, hypothetical three-neuron circuit, building an intuition for how the response of one channel is dynamically scaled by the activity of the entire local population .",
            "id": "3975598",
            "problem": "Consider a canonical divisive normalization circuit used to model cortical gain control. In this circuit, there are $N$ parallel sensory channels indexed by $i \\in \\{1, \\dots, N\\}$. Each channel receives an external nonnegative drive $x_i$ and passes it through a static exponentiation nonlinearity with exponent $n \\geq 1$, producing a drive $x_i^n$. A common inhibitory pool integrates the exponentiated drives from all channels through nonnegative weights $w_{ij}$, yielding a pooled activity. A semi-saturation constant $\\sigma > 0$ sets a baseline inhibitory conductance in the absence of input. At steady state, the output firing rate $r_i$ of each channel is determined by shunting inhibition, which implements gain control such that the effective conductance scales with the pool activity and the semi-saturation constant. In contrast, subtractive normalization circuits implement gain control through additive inhibition at the level of currents rather than conductances.\n\nStarting from these modeling assumptions—which are consistent with conductance-based neurons and widely used to characterize divisive versus subtractive normalization in computational neuroscience—derive the steady-state expression for the divisively normalized output $r_i$ as a function of $x_i$, $n$, $\\sigma$, and the weights $w_{ij}$, assuming a unity proportionality constant between voltage and firing rate. Then, evaluate $r_i$ for the concrete case with $N=3$, input vector $x=(2, 1, 0)$, exponent $n=2$, semi-saturation constant $\\sigma=1$, and uniform weights $w_{ij}=1$ for all $i,j$. Provide the final answer as the three components of $r_i$ in a single row matrix. No rounding is required. Briefly interpret how the normalization pool suppresses the strongest channel relative to its unnormalized exponentiated drive.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- A canonical divisive normalization circuit.\n- Number of channels: $N$, indexed by $i \\in \\{1, \\dots, N\\}$.\n- External drive for channel $i$: $x_i$, where $x_i \\geq 0$.\n- Static exponentiation nonlinearity: exponent $n \\geq 1$.\n- Exponentiated drive: $x_i^n$.\n- A common inhibitory pool integrates exponentiated drives from all channels through nonnegative weights $w_{ij}$.\n- Semi-saturation constant: $\\sigma > 0$.\n- The output firing rate $r_i$ is determined by shunting inhibition.\n- The effective conductance scales with the pool activity and the semi-saturation constant.\n- The proportionality constant between voltage and firing rate is unity.\n- Concrete case for evaluation: $N=3$, input vector $x=(2, 1, 0)$ (i.e., $x_1=2$, $x_2=1$, $x_3=0$), exponent $n=2$, semi-saturation constant $\\sigma=1$, and uniform weights $w_{ij}=1$ for all $i,j$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem describes the canonical model of divisive normalization, a well-established and fundamental concept in computational neuroscience used to explain a wide range of neural phenomena. The description is consistent with foundational papers and reviews on the topic (e.g., Heeger, 1992; Carandini & Heeger, 2012). It is scientifically sound.\n- **Well-Posed:** The problem provides a clear and complete description of the model structure and all necessary parameters to both derive a general expression and evaluate a specific instance. A unique, stable solution exists and is directly computable from the givens.\n- **Objective:** The problem is stated using precise, standard terminology from the field of computational neuroscience, free of ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, and objective. It contains no contradictions, missing information, or other flaws. The solution process will now proceed.\n\n### Derivation of the Steady-State Output\nThe problem describes a divisive normalization circuit based on shunting inhibition. In such a model, the response of a neuron or channel is given by its excitatory drive divided by a term representing the total effective conductance.\n\n1.  The excitatory drive for channel $i$ is the external input $x_i$ processed by a power-law nonlinearity with exponent $n$. Thus, the numerator of the expression for the response $r_i$ is $x_i^n$.\n\n2.  The denominator represents the shunting (divisive) suppression. It consists of two components:\n    a. A baseline inhibitory conductance, represented by the semi-saturation constant $\\sigma$. This term prevents division by zero when all inputs are zero and sets the scale of the normalization.\n    b. A pooled activity term that sums the drives from all channels. The problem specifies that a common inhibitory pool integrates the exponentiated drives $x_j^n$ from all channels $j \\in \\{1, \\dots, N\\}$ using weights $w_{ij}$. The total pooled inhibitory drive for channel $i$ is therefore the weighted sum $\\sum_{j=1}^{N} w_{ij} x_j^n$.\n\n3.  Combining these components, and given a unity proportionality constant, the steady-state output firing rate $r_i$ for channel $i$ is the ratio of its exponentiated drive to the sum of the baseline and pooled inhibitory terms. This yields the general expression for divisive normalization:\n    $$r_i = \\frac{x_i^n}{\\sigma + \\sum_{j=1}^{N} w_{ij} x_j^n}$$\n\n### Evaluation for the Concrete Case\nWe are given the following specific parameters:\n- Number of channels: $N=3$.\n- Input vector: $x_1=2$, $x_2=1$, $x_3=0$.\n- Exponent: $n=2$.\n- Semi-saturation constant: $\\sigma=1$.\n- Uniform weights: $w_{ij}=1$ for all $i, j \\in \\{1, 2, 3\\}$.\n\nFirst, we compute the exponentiated drives for each channel:\n- $x_1^n = 2^2 = 4$\n- $x_2^n = 1^2 = 1$\n- $x_3^n = 0^2 = 0$\n\nNext, we compute the normalization term in the denominator. Since the weights $w_{ij}$ are uniform and equal to $1$, the pooled activity term is the same for all output channels $i$ and is simply the sum of all exponentiated drives:\n$$ \\sum_{j=1}^{3} w_{ij} x_j^n = \\sum_{j=1}^{3} (1) \\cdot x_j^n = x_1^n + x_2^n + x_3^n $$\nSubstituting the values:\n$$ \\text{Pooled Activity} = 4 + 1 + 0 = 5 $$\n\nThe complete denominator is the sum of the semi-saturation constant and this pooled activity:\n$$ \\text{Denominator} = \\sigma + \\sum_{j=1}^{3} x_j^n = 1 + 5 = 6 $$\n\nFinally, we calculate the output firing rate $r_i$ for each channel using the general formula derived earlier:\n- For channel $i=1$:\n  $$ r_1 = \\frac{x_1^2}{\\text{Denominator}} = \\frac{4}{6} = \\frac{2}{3} $$\n- For channel $i=2$:\n  $$ r_2 = \\frac{x_2^2}{\\text{Denominator}} = \\frac{1}{6} $$\n- For channel $i=3$:\n  $$ r_3 = \\frac{x_3^2}{\\text{Denominator}} = \\frac{0}{6} = 0 $$\n\nThe resulting output vector is $r = (\\frac{2}{3}, \\frac{1}{6}, 0)$.\n\n### Interpretation of Suppression\nThe unnormalized exponentiated drive for the strongest channel (channel $1$) is $x_1^2 = 4$. The normalization process divides this drive by a factor of $6$, resulting in a final output of $r_1 = \\frac{2}{3}$. This denominator of $6$ arises from the semi-saturation constant ($\\sigma=1$) plus the total activity pooled from all channels ($x_1^2+x_2^2+x_3^2 = 4+1+0=5$). This demonstrates the core principle of divisive normalization: the response of a single channel is gated not by its own input alone, but by the collective activity across the entire population. The stronger the overall stimulus context, the more each individual channel's response is suppressed, implementing a form of automatic gain control. The strongest channel contributes the most to its own suppression, but it is also suppressed by the activity of weaker channels, making its response relative and context-dependent.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3} & \\frac{1}{6} & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While divisive normalization explains many features of neural responses, it is not the only mechanism for gain control. Subtractive normalization presents an alternative model where inhibition acts additively on the input drive. This practice explores a specific and insightful case of subtractive normalization, revealing its connection to a classic statistical operation: mean-centering. By working through this problem, you will see how a simple, uniformly connected inhibitory pool can implement a powerful computation that removes the average activity level from a neural population, thereby highlighting relative activity patterns .",
            "id": "3975645",
            "problem": "Consider a canonical cortical normalization microcircuit modeling a population of $N$ channels with activities $x_{1},\\dots,x_{N}$. A pooled drive variable $r$ is computed from the population by a linear pooling operation over channels with weights $w_{ij}$ that are identical across all targets $i$ (weight sharing). The subtractive normalization output for each channel is formed by removing the pooled drive from the channel’s activity. Assume the weights implement uniform pooling in the sense that $w_{ij} = \\frac{1}{N}$ for all $i$ and $j$, and that the pooling operation is linear and shift-invariant across channels.\n\nFor $N=4$ and $x=(1,2,3,4)$, compute the pooled drive $r$ exactly. Then, using only linearity and the uniformity of the weights, argue why this choice of pooling guarantees that subtractive normalization removes the across-channel mean from any input vector $x \\in \\mathbb{R}^{N}$ by showing that the sum of the subtractively normalized outputs equals zero. Provide the final answer as the exact value of $r$ with no units and no rounding.",
            "solution": "The problem describes a subtractive normalization model for a population of $N$ neural channels. The activity of these channels is given by a vector of real numbers $x = (x_{1}, x_{2}, \\dots, x_{N})$.\n\nA single pooled drive variable, denoted by $r$, is computed from the population activities. The problem specifies that this is a linear pooling operation with weights $w_{ij}$ connecting source channel $j$ to target channel $i$. The property of \"weight sharing\" means the weights are identical across all targets, i.e., $w_{ij}$ is independent of the target index $i$. Let us denote this as $w_j$. The pooling operation for each target $i$ is $r_i = \\sum_{j=1}^{N} w_{ij} x_j$. Due to weight sharing, $r_i = \\sum_{j=1}^{N} w_{j} x_j$, which is independent of $i$. Thus, a single pooled drive $r$ is computed for the entire population.\n\nThe problem further specifies that the weights implement uniform pooling, such that $w_{ij} = \\frac{1}{N}$ for all $i$ and $j$. This uniquely defines the weights $w_j$ in our formulation as $w_j = \\frac{1}{N}$ for all $j$.\n\nTherefore, the pooled drive $r$ is given by the linear summation:\n$$r = \\sum_{j=1}^{N} w_j x_j = \\sum_{j=1}^{N} \\frac{1}{N} x_j$$\nThis can be rewritten as:\n$$r = \\frac{1}{N} \\sum_{j=1}^{N} x_j$$\nThis expression demonstrates that the pooled drive $r$ is the arithmetic mean of the channel activities.\n\nThe subtractively normalized output for a channel $k$, denoted by $y_k$, is formed by subtracting this pooled drive from the channel's initial activity:\n$$y_k = x_k - r$$\n\nThe first part of the problem requires the computation of the value of the pooled drive $r$ for a specific case: a population of $N=4$ channels with activities given by the vector $x = (1, 2, 3, 4)$. Using the derived formula for $r$:\n$$r = \\frac{1}{4} \\sum_{j=1}^{4} x_j$$\nSubstituting the given values $x_1=1$, $x_2=2$, $x_3=3$, and $x_4=4$:\n$$r = \\frac{1}{4} (1 + 2 + 3 + 4) = \\frac{1}{4} (10) = \\frac{10}{4} = \\frac{5}{2} = 2.5$$\n\nThe second part of the problem asks for an argument, using the properties of linearity and uniform weights, that this normalization scheme removes the across-channel mean. This is to be demonstrated by showing that the sum of the normalized outputs equals zero for any input vector $x \\in \\mathbb{R}^{N}$.\n\nLet us compute the sum of the subtractively normalized outputs, $\\sum_{k=1}^{N} y_k$.\nUsing the definition of $y_k$, we have:\n$$\\sum_{k=1}^{N} y_k = \\sum_{k=1}^{N} (x_k - r)$$\nBy the properties of summations, we can separate the terms:\n$$\\sum_{k=1}^{N} y_k = \\left(\\sum_{k=1}^{N} x_k\\right) - \\left(\\sum_{k=1}^{N} r\\right)$$\nSince $r$ is a single value computed for the entire population, it is constant with respect to the summation index $k$. Thus, the second term evaluates to $\\sum_{k=1}^{N} r = N \\cdot r$.\n$$\\sum_{k=1}^{N} y_k = \\left(\\sum_{k=1}^{N} x_k\\right) - N r$$\nNow, we substitute the expression for the pooled drive $r$. The derivation of $r$ relies on the two key properties provided:\n$1$. The linearity of the pooling operation, captured by the weighted sum form.\n$2$. The uniformity of the weights, given as $w_{ij} = \\frac{1}{N}$, which leads to the formula $r = \\frac{1}{N} \\sum_{j=1}^{N} x_j$.\n\nSubstituting this expression for $r$ into our equation for the sum of outputs:\n$$\\sum_{k=1}^{N} y_k = \\left(\\sum_{k=1}^{N} x_k\\right) - N \\left(\\frac{1}{N} \\sum_{j=1}^{N} x_j\\right)$$\nThe summation index is a dummy variable, so the sum over $j$ is identical to the sum over $k$: $\\sum_{j=1}^{N} x_j = \\sum_{k=1}^{N} x_k$. Therefore, the expression simplifies to:\n$$\\sum_{k=1}^{N} y_k = \\left(\\sum_{k=1}^{N} x_k\\right) - \\left(\\sum_{k=1}^{N} x_k\\right) = 0$$\nThis proves that the sum of the subtractively normalized outputs is always zero for any arbitrary input vector $x \\in \\mathbb{R}^{N}$.\n\nThe across-channel mean of the normalized outputs, $\\bar{y}$, is defined as $\\bar{y} = \\frac{1}{N} \\sum_{k=1}^{N} y_k$. Since we have shown that $\\sum_{k=1}^{N} y_k = 0$, it follows directly that $\\bar{y} = \\frac{1}{N} \\cdot 0 = 0$. This confirms that the subtractive normalization process, with uniform pooling weights, centers the channel activities around zero, which is what is meant by \"removes the across-channel mean\".",
            "answer": "$$\\boxed{2.5}$$"
        },
        {
            "introduction": "A central goal in computational neuroscience is to build models that not only capture baseline neural activity but also explain how that activity is modulated by cognitive states like attention. This advanced problem delves into that challenge by asking you to become a theoretician exploring how a top-down modulatory signal might interact with normalization circuits. By systematically analyzing the effects of modulation at different stages of the circuit—the input, the baseline, and the pooling weights—you will derive distinct, experimentally testable predictions for how attention alters neural gain under both divisive and subtractive normalization schemes .",
            "id": "3975619",
            "problem": "Consider a two-channel sensory circuit modeled at steady state by widely used normalization operations in computational neuroscience. Let the feedforward excitatory drives be $x_1 \\ge 0$ and $x_2 \\ge 0$. A common pooled interaction term is the weighted sum $S = w_1 x_1 + w_2 x_2$ with weights $w_1 \\ge 0$ and $w_2 \\ge 0$, and a nonzero baseline term $\\sigma > 0$ representing spontaneous or tonic activity. In a divisive normalization circuit, the channel-$1$ output is defined by $y_1^{\\mathrm{div}} = \\dfrac{x_1}{\\sigma + S}$. In a subtractive normalization circuit, the channel-$1$ output is defined by $y_1^{\\mathrm{sub}} = x_1 - \\beta \\left(\\sigma + S\\right)$, where $\\beta > 0$ is the effective strength of the pooled subtraction. Assume linear summation in the pool and ignore output rectification.\n\nA top-down modulatory signal is implemented as a multiplicative factor $m > 0$ applied to exactly one of the following loci:\n\n(i) the inputs prior to normalization, $x_j \\mapsto m x_j$ for $j \\in \\{1,2\\}$,\n\n(ii) the baseline, $\\sigma \\mapsto m \\sigma$,\n\n(iii) the pool weights, $w_j \\mapsto m w_j$ for $j \\in \\{1,2\\}$.\n\nTreat $m$ as a small perturbation around $m = 1$ and define the modulation sensitivity as the first derivative of the channel-$1$ output with respect to $m$ evaluated at $m = 1$, that is, $\\left.\\dfrac{\\partial y_1}{\\partial m}\\right|_{m=1}$. Starting from the stated normalization definitions and the standard rules of differentiation, derive closed-form expressions for the six sensitivities corresponding to the three loci for each of the two circuits, all expressed in terms of $x_1$, $x_2$, $\\sigma$, $w_1$, $w_2$, and $\\beta$. Provide your final answer as a single row matrix in the order: divisive-input, divisive-$\\sigma$, divisive-weight, subtractive-input, subtractive-$\\sigma$, subtractive-weight. The quantities are dimensionless, and no rounding is required. Your final answer must be a single closed-form analytic expression.",
            "solution": "The problem requires the calculation of modulation sensitivities for two types of neural normalization circuits: divisive and subtractive. The sensitivity is defined as the first derivative of the channel-$1$ output with respect to a modulatory parameter $m$, evaluated at $m=1$. The modulation can be applied to three different loci: the inputs, the baseline, or the pool weights. We will systematically derive the six requested sensitivities.\n\nLet the feedforward drives be $x_1 \\ge 0$ and $x_2 \\ge 0$. The pooled interaction term is $S = w_1 x_1 + w_2 x_2$, with weights $w_1 \\ge 0$ and $w_2 \\ge 0$. The baseline activity term is $\\sigma > 0$. The modulatory factor is $m > 0$. The effective strength of subtraction is $\\beta > 0$.\n\nFirst, we analyze the divisive normalization circuit.\nThe channel-$1$ output for the divisive normalization circuit is given by:\n$$\ny_1^{\\mathrm{div}} = \\frac{x_1}{\\sigma + S} = \\frac{x_1}{\\sigma + w_1 x_1 + w_2 x_2}\n$$\nWe calculate the sensitivity $\\left.\\frac{\\partial y_1^{\\mathrm{div}}}{\\partial m}\\right|_{m=1}$ for the three modulation loci.\n\n1.  **Divisive Normalization, Input Modulation**:\n    The inputs are modulated as $x_j \\mapsto m x_j$. This affects both the numerator drive $x_1$ and the pooled sum $S$.\n    The modulated output $y_1(m)$ is:\n    $$\n    y_1(m) = \\frac{m x_1}{\\sigma + (w_1 (m x_1) + w_2 (m x_2))} = \\frac{m x_1}{\\sigma + m S}\n    $$\n    To find the sensitivity, we differentiate $y_1(m)$ with respect to $m$ using the quotient rule, $\\frac{d}{dx}\\left(\\frac{u}{v}\\right) = \\frac{u'v - uv'}{v^2}$:\n    $$\n    \\frac{\\partial y_1}{\\partial m} = \\frac{(x_1)(\\sigma + m S) - (m x_1)(S)}{(\\sigma + m S)^2} = \\frac{x_1 \\sigma + m x_1 S - m x_1 S}{(\\sigma + m S)^2} = \\frac{x_1 \\sigma}{(\\sigma + m S)^2}\n    $$\n    Evaluating at $m=1$:\n    $$\n    \\left.\\frac{\\partial y_1^{\\mathrm{div}}}{\\partial m}\\right|_{m=1} = \\frac{x_1 \\sigma}{(\\sigma + S)^2} = \\frac{x_1 \\sigma}{(\\sigma + w_1 x_1 + w_2 x_2)^2}\n    $$\n\n2.  **Divisive Normalization, Baseline Modulation**:\n    The baseline is modulated as $\\sigma \\mapsto m \\sigma$.\n    The modulated output $y_1(m)$ is:\n    $$\n    y_1(m) = \\frac{x_1}{m \\sigma + S} = x_1 (m \\sigma + S)^{-1}\n    $$\n    Differentiating with respect to $m$ using the chain rule:\n    $$\n    \\frac{\\partial y_1}{\\partial m} = x_1 \\cdot (-1) (m \\sigma + S)^{-2} \\cdot \\frac{\\partial}{\\partial m}(m \\sigma + S) = -x_1 (m \\sigma + S)^{-2} (\\sigma) = \\frac{-x_1 \\sigma}{(m \\sigma + S)^2}\n    $$\n    Evaluating at $m=1$:\n    $$\n    \\left.\\frac{\\partial y_1^{\\mathrm{div}}}{\\partial m}\\right|_{m=1} = \\frac{-x_1 \\sigma}{(\\sigma + S)^2} = \\frac{-x_1 \\sigma}{(\\sigma + w_1 x_1 + w_2 x_2)^2}\n    $$\n\n3.  **Divisive Normalization, Weight Modulation**:\n    The pool weights are modulated as $w_j \\mapsto m w_j$. This affects only the pooled sum $S$.\n    The modulated output $y_1(m)$ is:\n    $$\n    y_1(m) = \\frac{x_1}{\\sigma + ( (m w_1) x_1 + (m w_2) x_2 )} = \\frac{x_1}{\\sigma + m S}\n    $$\n    Differentiating with respect to $m$:\n    $$\n    \\frac{\\partial y_1}{\\partial m} = x_1 \\cdot (-1) (\\sigma + m S)^{-2} \\cdot \\frac{\\partial}{\\partial m}(\\sigma + m S) = -x_1 (\\sigma + m S)^{-2} (S) = \\frac{-x_1 S}{(\\sigma + m S)^2}\n    $$\n    Evaluating at $m=1$:\n    $$\n    \\left.\\frac{\\partial y_1^{\\mathrm{div}}}{\\partial m}\\right|_{m=1} = \\frac{-x_1 S}{(\\sigma + S)^2} = \\frac{-x_1 (w_1 x_1 + w_2 x_2)}{(\\sigma + w_1 x_1 + w_2 x_2)^2}\n    $$\n\nNext, we analyze the subtractive normalization circuit.\nThe channel-$1$ output for the subtractive normalization circuit is given by:\n$$\ny_1^{\\mathrm{sub}} = x_1 - \\beta (\\sigma + S) = x_1 - \\beta (\\sigma + w_1 x_1 + w_2 x_2)\n$$\nWe calculate the sensitivity $\\left.\\frac{\\partial y_1^{\\mathrm{sub}}}{\\partial m}\\right|_{m=1}$ for the three modulation loci.\n\n4.  **Subtractive Normalization, Input Modulation**:\n    The inputs are modulated as $x_j \\mapsto m x_j$.\n    The modulated output $y_1(m)$ is:\n    $$\n    y_1(m) = m x_1 - \\beta (\\sigma + (w_1(m x_1) + w_2(m x_2))) = m x_1 - \\beta (\\sigma + m S)\n    $$\n    Differentiating with respect to $m$:\n    $$\n    \\frac{\\partial y_1}{\\partial m} = x_1 - \\beta S\n    $$\n    This derivative is independent of $m$, so evaluating at $m=1$ yields the same expression:\n    $$\n    \\left.\\frac{\\partial y_1^{\\mathrm{sub}}}{\\partial m}\\right|_{m=1} = x_1 - \\beta S = x_1 - \\beta(w_1 x_1 + w_2 x_2)\n    $$\n\n5.  **Subtractive Normalization, Baseline Modulation**:\n    The baseline is modulated as $\\sigma \\mapsto m \\sigma$.\n    The modulated output $y_1(m)$ is:\n    $$\n    y_1(m) = x_1 - \\beta (m \\sigma + S)\n    $$\n    Differentiating with respect to $m$:\n    $$\n    \\frac{\\partial y_1}{\\partial m} = -\\beta \\sigma\n    $$\n    This derivative is independent of $m$. evaluating at $m=1$ yields:\n    $$\n    \\left.\\frac{\\partial y_1^{\\mathrm{sub}}}{\\partial m}\\right|_{m=1} = -\\beta \\sigma\n    $$\n\n6.  **Subtractive Normalization, Weight Modulation**:\n    The pool weights are modulated as $w_j \\mapsto m w_j$.\n    The modulated output $y_1(m)$ is:\n    $$\n    y_1(m) = x_1 - \\beta (\\sigma + (m w_1 x_1 + m w_2 x_2)) = x_1 - \\beta (\\sigma + m S)\n    $$\n    Differentiating with respect to $m$:\n    $$\n    \\frac{\\partial y_1}{\\partial m} = -\\beta S\n    $$\n    This derivative is independent of $m$. evaluating at $m=1$ yields:\n    $$\n    \\left.\\frac{\\partial y_1^{\\mathrm{sub}}}{\\partial m}\\right|_{m=1} = -\\beta S = -\\beta(w_1 x_1 + w_2 x_2)\n    $$\n\nThe six sensitivities are:\n\\begin{enumerate}\n    \\item Divisive-input: $\\dfrac{x_1 \\sigma}{(\\sigma + w_1 x_1 + w_2 x_2)^2}$\n    \\item Divisive-$\\sigma$: $\\dfrac{-x_1 \\sigma}{(\\sigma + w_1 x_1 + w_2 x_2)^2}$\n    \\item Divisive-weight: $\\dfrac{-x_1 (w_1 x_1 + w_2 x_2)}{(\\sigma + w_1 x_1 + w_2 x_2)^2}$\n    \\item Subtractive-input: $x_1 - \\beta (w_1 x_1 + w_2 x_2)$\n    \\item Subtractive-$\\sigma$: $-\\beta \\sigma$\n    \\item Subtractive-weight: $-\\beta (w_1 x_1 + w_2 x_2)$\n\\end{enumerate}\nThese results are now combined into a single row matrix as requested.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{x_1 \\sigma}{(\\sigma + w_1 x_1 + w_2 x_2)^{2}} & \\frac{-x_1 \\sigma}{(\\sigma + w_1 x_1 + w_2 x_2)^{2}} & \\frac{-x_1 (w_1 x_1 + w_2 x_2)}{(\\sigma + w_1 x_1 + w_2 x_2)^{2}} & x_1 - \\beta (w_1 x_1 + w_2 x_2) & -\\beta \\sigma & -\\beta(w_1 x_1 + w_2 x_2) \\end{pmatrix}}\n$$"
        }
    ]
}