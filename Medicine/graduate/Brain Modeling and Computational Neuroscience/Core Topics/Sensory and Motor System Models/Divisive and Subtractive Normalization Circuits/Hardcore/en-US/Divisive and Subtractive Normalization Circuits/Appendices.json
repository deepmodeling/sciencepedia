{
    "hands_on_practices": [
        {
            "introduction": "We begin with the foundational model of divisive normalization, a cornerstone of computational neuroscience for explaining how neurons adjust their sensitivity. This exercise grounds the abstract theory in a concrete calculation, demonstrating how the response of a single channel is shaped not in isolation, but by the collective activity of its neighbors. By working through this simple example , you will develop a direct intuition for the 'shunting' or 'gain control' effect that is central to normalization.",
            "id": "3975598",
            "problem": "Consider a canonical divisive normalization circuit used to model cortical gain control. In this circuit, there are $N$ parallel sensory channels indexed by $i \\in \\{1, \\dots, N\\}$. Each channel receives an external nonnegative drive $x_{i}$ and passes it through a static exponentiation nonlinearity with exponent $n \\geq 1$, producing a drive $x_{i}^{n}$. A common inhibitory pool integrates the exponentiated drives from all channels through nonnegative weights $w_{ij}$, yielding a pooled activity. A semi-saturation constant $\\sigma  0$ sets a baseline inhibitory conductance in the absence of input. At steady state, the output firing rate $r_{i}$ of each channel is determined by shunting inhibition, which implements gain control such that the effective conductance scales with the pool activity and the semi-saturation constant. In contrast, subtractive normalization circuits implement gain control through additive inhibition at the level of currents rather than conductances.\n\nStarting from these modeling assumptions—which are consistent with conductance-based neurons and widely used to characterize divisive versus subtractive normalization in computational neuroscience—derive the steady-state expression for the divisively normalized output $r_{i}$ as a function of $x_{i}$, $n$, $\\sigma$, and the weights $w_{ij}$, assuming a unity proportionality constant between voltage and firing rate. Then, evaluate $r_{i}$ for the concrete case with $N=3$, input vector $x=(2, 1, 0)$, exponent $n=2$, semi-saturation constant $\\sigma=1$, and uniform weights $w_{ij}=1$ for all $i,j$. Provide the final answer as the three components of $r_{i}$ in a single row matrix. No rounding is required. Briefly interpret how the normalization pool suppresses the strongest channel relative to its unnormalized exponentiated drive.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- A canonical divisive normalization circuit.\n- Number of channels: $N$, indexed by $i \\in \\{1, \\dots, N\\}$.\n- External drive for channel $i$: $x_{i}$, where $x_{i} \\geq 0$.\n- Static exponentiation nonlinearity: exponent $n \\geq 1$.\n- Exponentiated drive: $x_{i}^{n}$.\n- A common inhibitory pool integrates exponentiated drives from all channels through nonnegative weights $w_{ij}$.\n- Semi-saturation constant: $\\sigma  0$.\n- The output firing rate $r_{i}$ is determined by shunting inhibition.\n- The effective conductance scales with the pool activity and the semi-saturation constant.\n- The proportionality constant between voltage and firing rate is unity.\n- Concrete case for evaluation: $N=3$, input vector $x=(2, 1, 0)$ (i.e., $x_1=2$, $x_2=1$, $x_3=0$), exponent $n=2$, semi-saturation constant $\\sigma=1$, and uniform weights $w_{ij}=1$ for all $i,j$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem describes the canonical model of divisive normalization, a well-established and fundamental concept in computational neuroscience used to explain a wide range of neural phenomena. The description is consistent with foundational papers and reviews on the topic (e.g., Heeger, 1992; Carandini  Heeger, 2012). It is scientifically sound.\n- **Well-Posed:** The problem provides a clear and complete description of the model structure and all necessary parameters to both derive a general expression and evaluate a specific instance. A unique, stable solution exists and is directly computable from the givens.\n- **Objective:** The problem is stated using precise, standard terminology from the field of computational neuroscience, free of ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, and objective. It contains no contradictions, missing information, or other flaws. The solution process will now proceed.\n\n### Derivation of the Steady-State Output\nThe problem describes a divisive normalization circuit based on shunting inhibition. In such a model, the response of a neuron or channel is given by its excitatory drive divided by a term representing the total effective conductance.\n\n1.  The excitatory drive for channel $i$ is the external input $x_i$ processed by a power-law nonlinearity with exponent $n$. Thus, the numerator of the expression for the response $r_i$ is $x_i^n$.\n\n2.  The denominator represents the shunting (divisive) suppression. It consists of two components:\n    a. A baseline inhibitory conductance, represented by the semi-saturation constant $\\sigma$. This term prevents division by zero when all inputs are zero and sets the scale of the normalization.\n    b. A pooled activity term that sums the drives from all channels. The problem specifies that a common inhibitory pool integrates the exponentiated drives $x_j^n$ from all channels $j \\in \\{1, \\dots, N\\}$ using weights $w_{ij}$. The total pooled inhibitory drive for channel $i$ is therefore the weighted sum $\\sum_{j=1}^{N} w_{ij} x_j^n$.\n\n3.  Combining these components, and given a unity proportionality constant, the steady-state output firing rate $r_i$ for channel $i$ is the ratio of its exponentiated drive to the sum of the baseline and pooled inhibitory terms. This yields the general expression for divisive normalization:\n    $$r_i = \\frac{x_i^n}{\\sigma + \\sum_{j=1}^{N} w_{ij} x_j^n}$$\n\n### Evaluation for the Concrete Case\nWe are given the following specific parameters:\n- Number of channels: $N=3$.\n- Input vector: $x_1=2$, $x_2=1$, $x_3=0$.\n- Exponent: $n=2$.\n- Semi-saturation constant: $\\sigma=1$.\n- Uniform weights: $w_{ij}=1$ for all $i, j \\in \\{1, 2, 3\\}$.\n\nFirst, we compute the exponentiated drives for each channel:\n- $x_1^n = 2^2 = 4$\n- $x_2^n = 1^2 = 1$\n- $x_3^n = 0^2 = 0$\n\nNext, we compute the normalization term in the denominator. Since the weights $w_{ij}$ are uniform and equal to $1$, the pooled activity term is the same for all output channels $i$ and is simply the sum of all exponentiated drives:\n$$ \\sum_{j=1}^{3} w_{ij} x_j^n = \\sum_{j=1}^{3} (1) \\cdot x_j^n = x_1^n + x_2^n + x_3^n $$\nSubstituting the values:\n$$ \\text{Pooled Activity} = 4 + 1 + 0 = 5 $$\n\nThe complete denominator is the sum of the semi-saturation constant and this pooled activity:\n$$ \\text{Denominator} = \\sigma + \\sum_{j=1}^{3} x_j^n = 1 + 5 = 6 $$\n\nFinally, we calculate the output firing rate $r_i$ for each channel using the general formula derived earlier:\n- For channel $i=1$:\n  $$ r_1 = \\frac{x_1^2}{\\text{Denominator}} = \\frac{4}{6} = \\frac{2}{3} $$\n- For channel $i=2$:\n  $$ r_2 = \\frac{x_2^2}{\\text{Denominator}} = \\frac{1}{6} $$\n- For channel $i=3$:\n  $$ r_3 = \\frac{x_3^2}{\\text{Denominator}} = \\frac{0}{6} = 0 $$\n\nThe resulting output vector is $r = (\\frac{2}{3}, \\frac{1}{6}, 0)$.\n\n### Interpretation of Suppression\nThe unnormalized exponentiated drive for the strongest channel (channel $1$) is $x_1^2 = 4$. The normalization process divides this drive by a factor of $6$, resulting in a final output of $r_1 = \\frac{2}{3}$. This denominator of $6$ arises from the semi-saturation constant ($\\sigma=1$) plus the total activity pooled from all channels ($x_1^2+x_2^2+x_3^2 = 4+1+0=5$). This demonstrates the core principle of divisive normalization: the response of a single channel is gated not by its own input alone, but by the collective activity across the entire population. The stronger the overall stimulus context, the more each individual channel's response is suppressed, implementing a form of automatic gain control. The strongest channel contributes the most to its own suppression, but it is also suppressed by the activity of weaker channels, making its response relative and context-dependent.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3}  \\frac{1}{6}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While divisive normalization is a powerful model, it is not the only proposed mechanism for gain control; subtractive normalization offers a distinct alternative. A key task for a computational neuroscientist is to devise experiments that can distinguish between such competing models. This practice challenges you to think like an experimentalist by deriving a quantitative prediction that cleanly separates the two circuits' behavior under a classic stimulus manipulation—contrast scaling .",
            "id": "3975620",
            "problem": "A cortical neuron in primary visual cortex receives a stimulus drive that is proportional to stimulus contrast, and its output is shaped by a population-level normalization pool. To distinguish subtractive normalization from divisive normalization in neural recordings, consider the following contrast-scaling experiment. A drifting grating is presented at fixed orientation and spatial frequency while contrast is varied. Let the contrast be denoted by $c0$, and let the contrast be scaled by a factor $\\alpha0$ to $\\alpha c$ in a second condition. Assume the stimulus drive is $D(c)=a\\,c$ for some $a0$ (encoding the sensitivity of the neuron at the chosen stimulus configuration), and the normalization pool activity is $P(c)=b\\,c$ for some $b0$ (encoding the pooled activity contributed by nearby neurons at the same stimulus configuration). Assume that over the contrast range tested the neuron operates in a regime where output is monotonic in its net drive and remains strictly positive.\n\nTwo canonical normalization circuits are considered, both well-supported in computational neuroscience:\n- Subtractive normalization: the output firing rate is reduced by subtracting a fraction of the pooled activity from the drive.\n- Divisive normalization: the output firing rate is reduced by dividing by the pooled activity plus a semi-saturation constant.\n\nDefine the measured amplitude change under contrast scaling as $\\Delta(\\alpha)=\\dfrac{R(\\alpha c)}{R(c)}$, where $R(\\cdot)$ denotes the neuron’s firing rate (in spikes per second). A scale-invariance test statistic is defined by $\\mathcal{T}(\\alpha)=\\dfrac{\\Delta(\\alpha)}{\\alpha}$; note that exact scale invariance under contrast scaling corresponds to $\\mathcal{T}(\\alpha)=1$.\n\nUsing only the assumptions above and without introducing any empirical fitting shortcuts, derive a closed-form analytic expression for the discriminant $\\delta(\\alpha,c,\\sigma,b)$ defined as the difference between the divisive prediction and the subtractive prediction for the scale-invariance statistic:\n$$\n\\delta(\\alpha,c,\\sigma,b) \\equiv \\mathcal{T}_{\\mathrm{div}}(\\alpha)-\\mathcal{T}_{\\mathrm{sub}}(\\alpha),\n$$\nwhere $\\sigma0$ is the semi-saturation constant of the divisive normalization circuit. Express your final answer as a single simplified expression in terms of $\\alpha$, $c$, $\\sigma$, and $b$. No numerical evaluation is required. The result is dimensionless by construction, so no units are to be attached.",
            "solution": "The problem requires the derivation of a discriminant, $\\delta(\\alpha,c,\\sigma,b)$, which is defined as the difference between the scale-invariance statistic $\\mathcal{T}$ predicted by a divisive normalization model and that predicted by a subtractive normalization model. The scale-invariance statistic is defined as $\\mathcal{T}(\\alpha) = \\frac{\\Delta(\\alpha)}{\\alpha}$, where $\\Delta(\\alpha) = \\frac{R(\\alpha c)}{R(c)}$ is the measured amplitude change of a neuron's response $R$ when the stimulus contrast $c$ is scaled by a factor $\\alpha$.\n\nFirst, we must formalize the response functions, $R(c)$, for both the subtractive and divisive normalization models based on the provided definitions.\n\nThe problem states that the stimulus drive is $D(c) = a\\,c$ and the normalization pool activity is $P(c) = b\\,c$, where $a0$ and $b0$.\n\n**1. Subtractive Normalization Model ($R_{\\mathrm{sub}}(c)$)**\n\nThe problem describes subtractive normalization as a process where \"the output firing rate is reduced by subtracting a fraction of the pooled activity from the drive.\" This implies that the net drive to the neuron is of the form $D(c) - k_s P(c)$ for some constant fraction $k_s0$. The problem further states that the neuron's output is \"monotonic in its net drive.\" The simplest monotonic relationship is a linear one, where the output firing rate is directly proportional to this net drive. We can write the response as:\n$$\nR_{\\mathrm{sub}}(c) = C (D(c) - k_s P(c)) = C (a\\,c - k_s b\\,c) = C(a - k_s b)c\n$$\nwhere $C$ is a constant of proportionality. The problem requires the output to be strictly positive, which means $C(a - k_s b)  0$. Let the combined constant $C(a - k_s b)$ be denoted by $K_{sub}$, which must be positive. Then, the response function for the subtractive model is of the general linear form:\n$$\nR_{\\mathrm{sub}}(c) = K_{sub}\\,c\n$$\nNow, we calculate the scale-invariance statistic, $\\mathcal{T}_{\\mathrm{sub}}(\\alpha)$, for this model.\nThe amplitude change $\\Delta_{\\mathrm{sub}}(\\alpha)$ is the ratio of the response at the scaled contrast $\\alpha c$ to the response at the original contrast $c$:\n$$\n\\Delta_{\\mathrm{sub}}(\\alpha) = \\frac{R_{\\mathrm{sub}}(\\alpha c)}{R_{\\mathrm{sub}}(c)} = \\frac{K_{sub}(\\alpha c)}{K_{sub} c} = \\alpha\n$$\nThe scale-invariance statistic $\\mathcal{T}_{\\mathrm{sub}}(\\alpha)$ is then:\n$$\n\\mathcal{T}_{\\mathrm{sub}}(\\alpha) = \\frac{\\Delta_{\\mathrm{sub}}(\\alpha)}{\\alpha} = \\frac{\\alpha}{\\alpha} = 1\n$$\nThis result demonstrates that any model where the response is a linear function of contrast exhibits perfect scale invariance under the given definition.\n\n**2. Divisive Normalization Model ($R_{\\mathrm{div}}(c)$)**\n\nThe problem describes divisive normalization as a process where the response is reduced \"by dividing by the pooled activity plus a semi-saturation constant,\" which is given as $\\sigma  0$. This corresponds to the canonical form of the divisive normalization model:\n$$\nR_{\\mathrm{div}}(c) = \\frac{D(c)}{\\sigma + P(c)} = \\frac{a\\,c}{\\sigma + b\\,c}\n$$\nThe condition that the response is strictly positive is met since all parameters ($a, c, \\sigma, b$) are positive.\n\nNext, we calculate the scale-invariance statistic, $\\mathcal{T}_{\\mathrm{div}}(\\alpha)$, for this model.\nThe response at the scaled contrast $\\alpha c$ is:\n$$\nR_{\\mathrm{div}}(\\alpha c) = \\frac{a(\\alpha c)}{\\sigma + b(\\alpha c)} = \\frac{a \\alpha c}{\\sigma + b \\alpha c}\n$$\nThe amplitude change $\\Delta_{\\mathrm{div}}(\\alpha)$ is:\n$$\n\\Delta_{\\mathrm{div}}(\\alpha) = \\frac{R_{\\mathrm{div}}(\\alpha c)}{R_{\\mathrm{div}}(c)} = \\frac{\\frac{a \\alpha c}{\\sigma + b \\alpha c}}{\\frac{a\\,c}{\\sigma + b\\,c}} = \\frac{a \\alpha c}{\\sigma + b \\alpha c} \\cdot \\frac{\\sigma + b\\,c}{a\\,c} = \\frac{\\alpha(\\sigma + b\\,c)}{\\sigma + b \\alpha c}\n$$\nThe scale-invariance statistic $\\mathcal{T}_{\\mathrm{div}}(\\alpha)$ is then:\n$$\n\\mathcal{T}_{\\mathrm{div}}(\\alpha) = \\frac{\\Delta_{\\mathrm{div}}(\\alpha)}{\\alpha} = \\frac{1}{\\alpha} \\cdot \\frac{\\alpha(\\sigma + b\\,c)}{\\sigma + b \\alpha c} = \\frac{\\sigma + b\\,c}{\\sigma + b \\alpha c}\n$$\nNote that the neuron-specific sensitivity parameter $a$ has cancelled out, leaving a statistic that depends on the pooled activity parameter $b$, the semi-saturation constant $\\sigma$, the contrast $c$, and the scaling factor $\\alpha$.\n\n**3. The Discriminant $\\delta(\\alpha,c,\\sigma,b)$**\n\nFinally, we compute the discriminant $\\delta$ as the difference between the two statistics:\n$$\n\\delta(\\alpha,c,\\sigma,b) = \\mathcal{T}_{\\mathrm{div}}(\\alpha) - \\mathcal{T}_{\\mathrm{sub}}(\\alpha)\n$$\nSubstituting the derived expressions for $\\mathcal{T}_{\\mathrm{div}}(\\alpha)$ and $\\mathcal{T}_{\\mathrm{sub}}(\\alpha)$:\n$$\n\\delta(\\alpha,c,\\sigma,b) = \\frac{\\sigma + b\\,c}{\\sigma + b \\alpha c} - 1\n$$\nTo simplify this into a single expression, we find a common denominator:\n$$\n\\delta(\\alpha,c,\\sigma,b) = \\frac{\\sigma + b\\,c}{\\sigma + b \\alpha c} - \\frac{\\sigma + b \\alpha c}{\\sigma + b \\alpha c} = \\frac{(\\sigma + b\\,c) - (\\sigma + b \\alpha c)}{\\sigma + b \\alpha c}\n$$\nSimplifying the numerator:\n$$\n\\delta(\\alpha,c,\\sigma,b) = \\frac{\\sigma + b\\,c - \\sigma - b \\alpha c}{\\sigma + b \\alpha c} = \\frac{b\\,c - b \\alpha c}{\\sigma + b \\alpha c}\n$$\nFactoring out the common term $b\\,c$ in the numerator yields the final closed-form expression:\n$$\n\\delta(\\alpha,c,\\sigma,b) = \\frac{b\\,c(1 - \\alpha)}{\\sigma + b \\alpha c}\n$$\nThis expression provides a quantitative prediction for the difference in the scale-invariance properties of the two normalization models, depending on the experimental parameters $\\alpha$ and $c$, and the circuit parameters $\\sigma$ and $b$.",
            "answer": "$$\n\\boxed{\\frac{b\\,c(1 - \\alpha)}{\\sigma + b \\alpha c}}\n$$"
        },
        {
            "introduction": "Theoretical models are only as useful as our ability to connect them to real-world data, which is invariably noisy. This final practice bridges the gap between theory and application by simulating a core activity in brain modeling: fitting a normalization model to synthetic experimental data. You will implement a full parameter estimation pipeline, including the crucial step of calculating confidence intervals to quantify the uncertainty of your findings, providing a realistic, hands-on experience in data-driven neuroscience .",
            "id": "3975629",
            "problem": "You are modeling contrast response functions produced by divisive normalization circuits in brain modeling and computational neuroscience. A canonical two-parameter saturating contrast-response model is assumed, with a semi-saturation parameter and a steepness parameter. Measurements are corrupted by independent Gaussian noise. Your task is to estimate the parameters from noisy measurements and assess their uncertainty using confidence intervals derived from first principles of maximum likelihood estimation.\n\nStarting point and assumptions:\n- The measurement model is additive Gaussian noise: for each stimulus contrast $x_i$ in the interval $[0,1]$, the measured response $y_i$ satisfies $y_i = f(x_i; \\theta) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_{\\text{noise}}^2)$ are independent and identically distributed random variables and $f$ is a two-parameter saturating function with parameter vector $\\theta$.\n- The two parameters of interest are strictly positive and represent a semi-saturation contrast and an exponent controlling steepness.\n- Under the Gaussian noise assumption, the maximum likelihood estimate coincides with nonlinear least squares.\n\nRequired derivations and algorithmic design:\n1. From the divisive normalization circuit principle, derive a two-parameter saturating response function $f$ appropriate for contrast-response modeling, and formulate the nonlinear least squares objective consistent with additive Gaussian noise, without using any shortcut formulas external to the derivation. Clearly specify the positivity constraints on the parameters.\n2. Design an estimation procedure that enforces positivity by reparameterizing the parameters using a logarithmic transformation. Explain how to recover estimates in the original parameter space.\n3. Derive the asymptotic covariance of the parameter estimates using the Jacobian of residuals evaluated at the optimum. Use the delta method (first-order Taylor expansion) to transform covariances back to the original parameterization. Construct two-sided $95$ percent confidence intervals for each parameter using a Wald-type approximation.\n\nImplementation details:\n- Use a fixed grid of stimulus contrasts $x_i$ equally spaced in $[0.01,1.0]$, with the number of points specified for each test case.\n- For each test case, generate synthetic data by drawing independent Gaussian noise with the specified standard deviation and adding it to the noiseless response $f(x_i; \\theta_{\\text{true}})$.\n- Fit the model to the synthetic data via nonlinear least squares using the log-parameterization, compute the residual variance estimate, the Jacobian-based covariance of the log-parameters, transform to the original parameters, and form $95$ percent confidence intervals.\n\nTest suite:\nUse the following four test cases, each defined by the ground-truth semi-saturation parameter $\\sigma_{\\text{true}}$, ground-truth exponent $n_{\\text{true}}$, noise standard deviation $\\sigma_{\\text{noise}}$, and number of contrast samples $N$:\n- Test case $1$: $\\sigma_{\\text{true}} = 0.2$, $n_{\\text{true}} = 2.5$, $\\sigma_{\\text{noise}} = 0.02$, $N = 15$.\n- Test case $2$: $\\sigma_{\\text{true}} = 0.8$, $n_{\\text{true}} = 1.0$, $\\sigma_{\\text{noise}} = 0.03$, $N = 12$.\n- Test case $3$: $\\sigma_{\\text{true}} = 0.1$, $n_{\\text{true}} = 6.0$, $\\sigma_{\\text{noise}} = 0.01$, $N = 20$.\n- Test case $4$: $\\sigma_{\\text{true}} = 0.3$, $n_{\\text{true}} = 3.0$, $\\sigma_{\\text{noise}} = 0.08$, $N = 15$.\n\nOutput specification:\n- For each test case, form two two-sided $95$ percent confidence intervals: one for the semi-saturation parameter and one for the exponent.\n- For each test case, produce a boolean indicating whether both ground-truth parameters lie within their corresponding confidence intervals. The final output must be a single line containing the booleans for all four test cases as a comma-separated list enclosed in square brackets, with no spaces; for example, $[b_1,b_2,b_3,b_4]$ where each $b_i$ is either $True$ or $False$.\n- There are no physical units involved; all quantities are dimensionless. Your program must be self-contained, deterministic, and must not require any input. Use a fixed random seed to ensure reproducibility. The program must implement the required estimation and confidence interval calculation and then print the final output line in the specified format.",
            "solution": "The problem requires the estimation of parameters for a divisive normalization model of neural contrast response, along with an assessment of their statistical uncertainty. This will be addressed by deriving the model and the estimation framework from first principles, specifically maximum likelihood estimation under an additive Gaussian noise model, and then using the asymptotic properties of the estimator to construct confidence intervals.\n\n### Step 1: Model Derivation and Nonlinear Least Squares Objective\n\nA canonical model for a saturating response function, derived from the principle of divisive normalization, is the Hill function, also known as the Naka-Rushton equation in vision science. In a divisive normalization circuit, the response $R$ is proportional to the excitatory input drive $E$ divided by a pooled suppressive signal $S$ plus a constant. For modeling contrast responses, the drive $E$ and the suppressive pool $S$ are often taken to be power-law functions of the stimulus contrast $x$. A general form is $R \\propto \\frac{E(x)}{k + S(x)}$.\n\nLet the excitatory drive be $E(x) = x^n$ and the suppressive signal be proportional to this drive, leading to a response function of the form:\n$$\nf(x; \\sigma, n) = \\frac{x^n}{\\sigma^n + x^n}\n$$\nThis function is normalized to have a maximum response of $1$. The parameter vector is $\\theta = (\\sigma, n)$.\n- $\\sigma$ is the semi-saturation contrast, representing the contrast $x$ at which the response $f(x; \\theta)$ reaches half of its maximum ($0.5$).\n- $n$ is a dimensionless exponent that controls the steepness or slope of the response function.\nThe problem states that these parameters are strictly positive, so the constraints are $\\sigma  0$ and $n  0$.\n\nThe measurement model is given as $y_i = f(x_i; \\theta) + \\varepsilon_i$, where the noise terms $\\varepsilon_i$ are independent and identically distributed (i.i.d.) Gaussian random variables with mean $0$ and variance $\\sigma_{\\text{noise}}^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2)$.\n\nThe probability density function for a single measurement $y_i$ given the parameters $\\theta$ is:\n$$\np(y_i | x_i, \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_{\\text{noise}}^2}} \\exp\\left(-\\frac{(y_i - f(x_i; \\theta))^2}{2\\sigma_{\\text{noise}}^2}\\right)\n$$\nSince the measurements are independent, the likelihood of observing the entire dataset $D = \\{(x_i, y_i)\\}_{i=1}^N$ is the product of the individual probabilities:\n$$\nL(\\theta; D) = \\prod_{i=1}^N p(y_i | x_i, \\theta)\n$$\nThe log-likelihood is:\n$$\n\\ln L(\\theta; D) = \\sum_{i=1}^N \\ln p(y_i | x_i, \\theta) = \\sum_{i=1}^N \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma_{\\text{noise}}^2) - \\frac{(y_i - f(x_i; \\theta))^2}{2\\sigma_{\\text{noise}}^2} \\right]\n$$\nTo find the maximum likelihood estimate (MLE) of $\\theta$, we maximize $\\ln L(\\theta; D)$. This is equivalent to minimizing the negative log-likelihood. Ignoring terms that do not depend on $\\theta$, the problem reduces to minimizing the sum of squared residuals (SSR):\n$$\nS(\\theta) = \\sum_{i=1}^N (y_i - f(x_i; \\theta))^2\n$$\nThis is the objective function for nonlinear least squares (NLS) estimation.\n\n### Step 2: Reparameterization for Positivity Constraints\n\nThe NLS minimization must be performed subject to the constraints $\\sigma  0$ and $n  0$. A standard technique to handle such positivity constraints is to reparameterize the model. We introduce a new, unconstrained parameter vector $\\phi = (\\alpha, \\beta)$, where $\\alpha, \\beta \\in \\mathbb{R}$, and define the original parameters as:\n$$\n\\sigma = e^\\alpha\n$$\n$$\nn = e^\\beta\n$$\nThis transformation ensures that $\\sigma$ and $n$ are always positive. The optimization is then performed over the unconstrained parameters $\\alpha$ and $\\beta$. The objective function becomes:\n$$\nS(\\phi) = \\sum_{i=1}^N \\left(y_i - f(x_i; e^\\alpha, e^\\beta)\\right)^2\n$$\nOnce the optimal log-parameters $\\hat{\\phi} = (\\hat{\\alpha}, \\hat{\\beta})$ are found by minimizing $S(\\phi)$, the estimates for the original parameters are recovered via the inverse transformation:\n$$\n\\hat{\\sigma} = e^{\\hat{\\alpha}}\n$$\n$$\n\\hat{n} = e^{\\hat{\\beta}}\n$$\n\n### Step 3: Asymptotic Covariance and Confidence Intervals\n\nThe asymptotic covariance matrix of the NLS estimator provides a way to quantify the uncertainty in the parameter estimates. We first compute the covariance for the log-parameters $\\hat{\\phi}$ and then transform it to the original parameter space $\\theta$.\n\nFor a large number of samples $N$, the estimator $\\hat{\\phi}$ is approximately normally distributed with a covariance matrix given by:\n$$\n\\text{Cov}(\\hat{\\phi}) \\approx \\hat{\\sigma}_{\\text{res}}^2 (J_\\phi^T J_\\phi)^{-1}\n$$\n- $\\hat{\\sigma}_{\\text{res}}^2$ is the estimated residual variance, calculated as $\\hat{\\sigma}_{\\text{res}}^2 = \\frac{1}{N-p} S(\\hat{\\phi})$, where $p=2$ is the number of parameters.\n- $J_\\phi$ is the Jacobian matrix of the model residuals with respect to the log-parameters $\\phi$, evaluated at the optimum $\\hat{\\phi}$. The elements of this $N \\times 2$ matrix are $J_{\\phi, ij} = \\frac{\\partial r_i}{\\partial \\phi_j} = -\\frac{\\partial f(x_i; \\phi)}{\\partial \\phi_j}$.\n\nThe partial derivatives of $f$ with respect to $\\alpha$ and $\\beta$ are found using the chain rule:\n$$\n\\frac{\\partial f}{\\partial \\alpha} = \\frac{\\partial f}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial \\alpha} = \\left( -\\frac{n \\sigma^{n-1} x^n}{(\\sigma^n + x^n)^2} \\right) (e^\\alpha) = -\\frac{n \\sigma^n x^n}{(\\sigma^n + x^n)^2}\n$$\n$$\n\\frac{\\partial f}{\\partial \\beta} = \\frac{\\partial f}{\\partial n} \\frac{\\partial n}{\\partial \\beta} = \\left( \\frac{\\ln(x/\\sigma) x^n \\sigma^n}{(\\sigma^n + x^n)^2} \\right) (e^\\beta) = \\frac{n \\ln(x/\\sigma) x^n \\sigma^n}{(\\sigma^n + x^n)^2}\n$$\nThe Jacobian $J_\\phi$ is constructed by evaluating these expressions for each data point $x_i$ at the estimated parameters $\\hat{\\phi}$ (and corresponding $\\hat{\\sigma}, \\hat{n}$) and taking the negative. Let $C_\\phi = \\hat{\\sigma}_{\\text{res}}^2 (J_\\phi^T J_\\phi)^{-1}$ be the estimated covariance matrix for $\\hat{\\phi}$.\n\nTo find the covariance matrix for the original parameters $\\hat{\\theta} = (\\hat{\\sigma}, \\hat{n})$, we use the delta method. This method approximates the covariance of a transformed variable using a first-order Taylor expansion. The transformation is $g: \\phi \\mapsto \\theta$, with $g(\\alpha, \\beta) = (e^\\alpha, e^\\beta)$. The Jacobian of this transformation, $G$, is:\n$$\nG(\\phi) = \\begin{pmatrix} \\frac{\\partial \\sigma}{\\partial \\alpha}  \\frac{\\partial \\sigma}{\\partial \\beta} \\\\ \\frac{\\partial n}{\\partial \\alpha}  \\frac{\\partial n}{\\partial \\beta} \\end{pmatrix} = \\begin{pmatrix} e^\\alpha  0 \\\\ 0  e^\\beta \\end{pmatrix} = \\begin{pmatrix} \\sigma  0 \\\\ 0  n \\end{pmatrix}\n$$\nThe covariance matrix for $\\hat{\\theta}$ is then approximated by:\n$$\nC_\\theta = \\text{Cov}(\\hat{\\theta}) \\approx G(\\hat{\\phi}) C_\\phi G(\\hat{\\phi})^T\n$$\nSubstituting the matrices, we get:\n$$\nC_\\theta \\approx \\begin{pmatrix} \\hat{\\sigma}  0 \\\\ 0  \\hat{n} \\end{pmatrix} C_\\phi \\begin{pmatrix} \\hat{\\sigma}  0 \\\\ 0  \\hat{n} \\end{pmatrix} = \\begin{pmatrix} \\hat{\\sigma}^2 C_{\\phi,11}  \\hat{\\sigma}\\hat{n} C_{\\phi,12} \\\\ \\hat{\\sigma}\\hat{n} C_{\\phi,21}  \\hat{n}^2 C_{\\phi,22} \\end{pmatrix}\n$$\nThe variances of the estimators $\\hat{\\sigma}$ and $\\hat{n}$ are the diagonal elements of $C_\\theta$: $\\text{Var}(\\hat{\\sigma}) \\approx C_{\\theta,11}$ and $\\text{Var}(\\hat{n}) \\approx C_{\\theta,22}$. The standard errors (SE) are the square roots of these variances.\n\nFinally, a two-sided $95\\%$ Wald-type confidence interval for a parameter $\\theta_j$ is constructed as:\n$$\nCI(\\theta_j) = \\left[ \\hat{\\theta}_j - z_{0.025} \\cdot SE(\\hat{\\theta}_j), \\hat{\\theta}_j + z_{0.025} \\cdot SE(\\hat{\\theta}_j) \\right]\n$$\nwhere $z_{0.025}$ is the critical value from the standard normal distribution corresponding to a cumulative probability of $0.975$, which is approximately $1.96$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the parameter estimation and confidence interval problem for four test cases.\n    \"\"\"\n    \n    # Use a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # Critical value for 95% confidence interval\n    z_val = norm.ppf(1 - 0.05 / 2)\n\n    test_cases = [\n        # (sigma_true, n_true, sigma_noise, N)\n        (0.2, 2.5, 0.02, 15),\n        (0.8, 1.0, 0.03, 12),\n        (0.1, 6.0, 0.01, 20),\n        (0.3, 3.0, 0.08, 15),\n    ]\n\n    results = []\n\n    def response_function(x, params):\n        \"\"\"\n        Naka-Rushton / Hill function.\n        params can be [sigma, n] or log-transformed [alpha, beta].\n        \"\"\"\n        sigma, n = params\n        # Add a small epsilon to avoid division by zero or log(0) if x is 0\n        # although the problem specifies x in [0.01, 1].\n        x_safe = np.maximum(x, 1e-9)\n        # To prevent overflow with large n\n        try:\n            val = x_safe**n / (sigma**n + x_safe**n)\n        except OverflowError:\n            # For large n, function behaves like a step function at x=sigma\n            val = np.where(x_safe > sigma, 1.0, 0.0)\n        return val\n\n    def objective_function(log_params, x, y):\n        \"\"\"\n        Sum of squared residuals for log-transformed parameters.\n        \"\"\"\n        alpha, beta = log_params\n        sigma = np.exp(alpha)\n        n = np.exp(beta)\n        y_pred = response_function(x, (sigma, n))\n        return np.sum((y - y_pred)**2)\n\n    for case in test_cases:\n        sigma_true, n_true, sigma_noise, N = case\n        \n        # 1. Generate synthetic data\n        x_grid = np.linspace(0.01, 1.0, N)\n        y_true = response_function(x_grid, (sigma_true, n_true))\n        noise = np.random.normal(0, sigma_noise, N)\n        y_obs = y_true + noise\n\n        # 2. Fit the model using nonlinear least squares on log-parameters\n        log_params_true = np.array([np.log(sigma_true), np.log(n_true)])\n        # Use true parameters as initial guess to ensure robust convergence\n        # The focus is on the statistical procedure, not the optimizer robustness\n        opt_result = minimize(\n            objective_function, \n            x0=log_params_true, \n            args=(x_grid, y_obs),\n            method='BFGS'\n        )\n        \n        alpha_hat, beta_hat = opt_result.x\n        sigma_hat = np.exp(alpha_hat)\n        n_hat = np.exp(beta_hat)\n        \n        # 3. Calculate residual variance\n        y_pred = response_function(x_grid, (sigma_hat, n_hat))\n        residuals = y_obs - y_pred\n        sum_sq_res = np.sum(residuals**2)\n        p = 2 # number of parameters\n        residual_variance_hat = sum_sq_res / (N - p)\n\n        # 4. Calculate Jacobian of residuals w.r.t. log-parameters\n        # J_ij = -df_i/d_phi_j\n        # df/d_alpha = -n * sigma^n * x^n / (sigma^n + x^n)^2\n        # df/d_beta = n * ln(x/sigma) * x^n * sigma^n / (sigma^n + x^n)^2\n        \n        x_n = x_grid**n_hat\n        _sigma_n = sigma_hat**n_hat\n        denominator = (_sigma_n + x_n)**2\n        \n        # Add epsilon to x_grid inside log to avoid -inf if x is very small\n        ln_x_div_sigma = np.log(np.maximum(x_grid, 1e-9) / sigma_hat)\n        \n        # Jacobian of function f\n        df_dalpha = -n_hat * _sigma_n * x_n / denominator\n        df_dbeta = n_hat * ln_x_div_sigma * x_n * _sigma_n / denominator\n        \n        # Jacobian of residuals is the negative of the jacobian of f\n        J_phi = -np.vstack([df_dalpha, df_dbeta]).T\n\n        # 5. Covariance of log-parameters\n        try:\n            cov_phi = residual_variance_hat * np.linalg.inv(J_phi.T @ J_phi)\n        except np.linalg.LinAlgError:\n            # If matrix is singular, mark as not in CI and continue\n            results.append(False)\n            continue\n            \n        # 6. Transform covariance to original parameter space via Delta method\n        G = np.array([[sigma_hat, 0], [0, n_hat]])\n        cov_theta = G @ cov_phi @ G.T\n\n        # 7. Calculate standard errors and confidence intervals\n        se_sigma = np.sqrt(cov_theta[0, 0])\n        se_n = np.sqrt(cov_theta[1, 1])\n\n        ci_sigma = [sigma_hat - z_val * se_sigma, sigma_hat + z_val * se_sigma]\n        ci_n = [n_hat - z_val * se_n, n_hat + z_val * se_n]\n\n        # 8. Check if true parameters are within the CIs\n        is_sigma_in_ci = (ci_sigma[0] = sigma_true = ci_sigma[1])\n        is_n_in_ci = (ci_n[0] = n_true = ci_n[1])\n        \n        results.append(is_sigma_in_ci and is_n_in_ci)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}