## 引言
神经科学研究面临着如何从高维、嘈杂的神经[时间序列数据](@entry_id:262935)中提取有意义的计算原理的挑战。状态空间模型（State-Space Models, SSMs）为此提供了一个强大的框架，它假设观测到的复杂神经活动是由一个更简单、低维的潜在动态过程所驱动的。理解这一框架不仅需要掌握其数学基础，还需要了解如何将其灵活应用于多样的实验场景并解释其结果。本文旨在填补理论与实践之间的鸿沟，为研究者提供一个关于SSMs的全面指南。

在接下来的内容中，我们将分三步深入探索状态空间模型。第一章，“原理与机制”，将奠定理论基础，详细介绍模型的核心数学定义、推断算法（如卡尔曼滤波）以及[参数估计](@entry_id:139349)方法。第二章，“应用与跨学科联系”，将展示这些模型如何应用于真实的神经数据（如[脉冲序列](@entry_id:1132157)和[钙成像](@entry_id:172171)），并如何用于检验科学假说，同时揭示其与机器学习和控制论等领域的深刻联系。最后，第三章，“动手实践”，将通过一系列精选的编程练习，帮助您将理论知识转化为实际的分析技能。

## 原理与机制

在神经科学中，我们经常面对高维、嘈杂且动态的时间序列数据，例如多电极阵列记录的神经元集群的脉冲发放或局部场电位（LFP）。理解这些神经活动背后潜在的计算原理，需要我们开发能够捕捉其内在动力学和降维结构的模型。[状态空间模型](@entry_id:137993)（State-Space Models, SSMs）为此提供了一个强大而灵活的框架，它假设观测到的高维神经活动是由一个低维、未被观测的（即“潜在”）动态状态所驱动的。本章旨在阐述状态空间模型的核心原理、推断机制及其在[神经数据分析](@entry_id:1128577)中的关键考量。

### 状态空间模型的数学定义

[状态空间模型](@entry_id:137993)的核心思想是将一个复杂的时序过程分解为两个基本部分：一个描述系统潜在状态如何随时间演变的**[状态方程](@entry_id:274378)（state equation）**，以及一个描述在任意时刻如何从潜在状态生成观测数据的**观测方程（observation equation）**。

#### 线性高斯[状态空间模型](@entry_id:137993) (LGSSM)

最基本且分析上最易于处理的状态空间模型是**线性高斯状态空间模型（Linear Gaussian State-Space Model, LGSSM）**，有时也被称为动态线性模型（DLM）。该模型假设状态演化和观测过程都是线性的，并且受到[高斯噪声](@entry_id:260752)的干扰。

形式上，一个离散时间的LGSSM由以下方程定义：

1.  **[状态方程](@entry_id:274378)**：描述潜在状态 $x_t \in \mathbb{R}^{n_x}$ 在时间步 $t$ 的[演化过程](@entry_id:175749)。
    $$
    x_{t} = A x_{t-1} + B u_{t} + w_t, \quad w_t \sim \mathcal{N}(0, Q)
    $$
    这里，$x_{t-1}$ 是前一时刻的潜在状态。矩阵 $A \in \mathbb{R}^{n_x \times n_x}$ 是**[状态转移矩阵](@entry_id:269075)**，描述了系统内在的动力学。向量 $u_t \in \mathbb{R}^{n_u}$ 是已知的**外生输入**或[控制信号](@entry_id:747841)（例如，呈现给被试的感觉刺激或[光遗传学](@entry_id:175696)刺激），它通过**输入矩阵** $B \in \mathbb{R}^{n_x \times n_u}$ 影响系统状态。$w_t$ 是**[过程噪声](@entry_id:270644)**，一个均值为零、协方差为 $Q$ 的[高斯白噪声](@entry_id:749762)过程，代表了模型未捕捉到的状态随机扰动。

2.  **观测方程**：描述观测数据 $y_t \in \mathbb{R}^{n_y}$ 是如何从当前潜在状态 $x_t$ 生成的。
    $$
    y_t = C x_t + D u_t + v_t, \quad v_t \sim \mathcal{N}(0, R)
    $$
    矩阵 $C \in \mathbb{R}^{n_y \times n_x}$ 是**观测矩阵**或**加载矩阵**（loading matrix），它将低维的潜在状态[线性映射](@entry_id:185132)到高维的观测空间。矩阵 $D \in \mathbb{R}^{n_y \times n_u}$ 允许输入直接影响观测。$v_t$ 是**观测噪声**，一个均值为零、协方差为 $R$ 的[高斯白噪声](@entry_id:749762)过程，代表了测量误差或与潜在状态无关的神经活动。

模型的初始状态 $x_0$ 通常也被假定为服从高斯分布 $x_0 \sim \mathcal{N}(\mu_0, \Sigma_0)$。所有噪声项 $\{w_t\}$ 和 $\{v_t\}$ 以及初始状态 $x_0$ 被假定为[相互独立](@entry_id:273670)。

#### 模型的概率图结构与[联合分布](@entry_id:263960)

LGSSM的结构蕴含了一组关键的**[条件独立性](@entry_id:262650)（conditional independence）**假设。这些假设可以用一个[动态贝叶斯网络](@entry_id:276817)（Dynamic Bayesian Network, DBN）来表示。该网络的核心结构是：$x_{t-1}$ 是 $x_t$ 的父节点，$x_t$ 是 $y_t$ 的父节点。外生输入 $u_t$ 也可以是 $x_t$ 和 $y_t$ 的父节点。

基于这些[条件独立性](@entry_id:262650)，我们可以推导出所有潜在状态和观测数据的联合概率分布。根据[概率的链式法则](@entry_id:268139)，联合密度 $p(x_{0:T}, y_{1:T})$ 可以被分解。利用模型定义的两个核心[条件独立性](@entry_id:262650)假设：

1.  **一阶马尔可夫性**：给定当前状态 $x_{t-1}$，未来状态 $x_t$ 与所有过去的状态 $\{x_0, \dots, x_{t-2}\}$ 和观测 $\{y_1, \dots, y_{t-1}\}$ 是条件独立的。即 $p(x_t | x_{0:t-1}, y_{1:t-1}) = p(x_t | x_{t-1})$。
2.  **观测的[条件独立性](@entry_id:262650)**：给定当前状态 $x_t$，当前观测 $y_t$ 与所有其他状态 $\{x_k\}_{k \neq t}$ 和观测 $\{y_k\}_{k \neq t}$ 是条件独立的。即 $p(y_t | x_{0:T}, y_{1:t-1}) = p(y_t | x_t)$。

将这些假设应用于[链式法则](@entry_id:190743)，我们得到[联合概率](@entry_id:266356)密度的[因子分解](@entry_id:150389)形式 ：
$$
p(x_{0:T}, y_{1:T} | \theta) = p(x_0) \prod_{t=1}^{T} p(x_t | x_{t-1}) p(y_t | x_t)
$$
其中 $\theta = \{A, B, C, D, Q, R, \mu_0, \Sigma_0\}$ 是模型的全套参数。将LGSSM的具体高斯形式代入，我们得到：
$$
p(x_{0:T}, y_{1:T} | \theta) = \mathcal{N}(x_{0} | \mu_{0}, \Sigma_{0}) \prod_{t=1}^{T} \mathcal{N}(x_{t} | A x_{t-1} + B u_{t}, Q) \mathcal{N}(y_{t} | C x_{t} + D u_{t}, R)
$$
这个因子分解是后续所有推断和学习算法的理论基础。

#### 与其他时间序列模型的比较

理解LGSSM的独特之处，有助于我们将其与神经科学中其他常用的时间序列模型进行区分 。

*   **[自回归模型](@entry_id:140558) (Autoregressive model, AR($p$))**：一个AR($p$)模型直接对观测序列 $\{y_t\}$ 的依赖关系进行建模，即 $y_t$ 是其过去 $p$ 个值的[线性组合](@entry_id:154743)加上噪声。其[条件独立性](@entry_id:262650)为 $y_t \perp \{y_{1:t-p-1}\} | \{y_{t-1}, \dots, y_{t-p}\}$。与SSM不同，[AR模型](@entry_id:189434)没有一个独立的潜在状态层；它假设观测过程本身是马尔可夫的。虽然任何AR($p$)模型都可以通过[状态增广](@entry_id:140869)技术转化为一个LGSSM，但其原始的概念框架中缺少了“潜在动力学驱动观测”的核心思想。

*   **[隐马尔可夫模型](@entry_id:275059) (Hidden Markov Model, HMM)**：HMM与SSM共享相同的[基本图](@entry_id:160617)结构：一个潜在的马尔可夫过程驱动着条件独立的观测。然而，HMM的关键区别在于其潜在状态 $s_t$ 是**离散的**，从一个[有限集](@entry_id:145527)合中取值。而我们在此讨论的SSM具有**连续的**潜在状态。这种差异导致了推断算法的根本不同（HMM使用[前向-后向算法](@entry_id:194772)，LGSSM使用卡尔曼滤波）。

### 状态空间模型中的推断

在[神经数据分析](@entry_id:1128577)中，我们通常拥有的是观测序列 $\{y_{1:T}\}$ 和输入序列 $\{u_{1:T}\}$，而驱动这些观测的潜在状态序列 $\{x_{0:T}\}$ 是未知的。**推断（inference）**的任务就是根据观测数据来估计这些潜在状态。主要有三类推断问题：

1.  **滤波（Filtering）**：估计当前状态的后验分布 $p(x_t | y_{1:t})$。这在需要实时解码神经信号的[脑机接口](@entry_id:185810)（BCI）等应用中至关重要。
2.  **平滑（Smoothing）**：利用所有观测数据（包括未来的观测）来估计某一时刻的状态[后验分布](@entry_id:145605) $p(x_t | y_{1:T})$。这通常能提供比滤波更准确的状态估计，是大多数离线[神经数据分析](@entry_id:1128577)的首选。
3.  **预测（Prediction）**：估计未来状态的分布 $p(x_{t+k} | y_{1:t})$（其中 $k > 0$）。

#### [线性高斯模型](@entry_id:268963)中的精确推断：卡尔曼[滤波与平滑](@entry_id:188825)

LGSSM的一个美妙特性是，上述所有推断问题都有精确的[闭式](@entry_id:271343)解。这是因为高斯分布在与线性变换和加法运算结合时具有**[闭合性](@entry_id:1122501)**——高斯分布的[线性变换](@entry_id:149133)结果仍然是高斯分布，两个高斯概率密度函数的乘积也与一个高斯概率密度函数成正比。

*   **卡尔曼滤波器 (Kalman Filter)**：这是一个[递归算法](@entry_id:636816)，它精确地计算滤波分布 $p(x_t | y_{1:t})$。这个分布始终是高斯的，因此算法只需递归地更新其均值和协方差。滤波过程包括两个步骤：
    1.  **预测步**：根据 $t-1$ 时刻的滤波分布 $p(x_{t-1} | y_{1:t-1})$ 和[状态方程](@entry_id:274378)，计算出 $t$ 时刻状态的**先验分布** $p(x_t | y_{1:t-1})$。
    2.  **更新步**：利用贝叶斯定理，将这个[先验分布](@entry_id:141376)与来自新观测 $y_t$ 的[似然](@entry_id:167119)信息 $p(y_t | x_t)$ 相结合，得到 $t$ 时刻的**后验（滤波）分布** $p(x_t | y_{1:t})$。

*   **[劳赫-董-斯特里贝尔平滑器](@entry_id:181982) (Rauch-Tung-Striebel, RTS Smoother)**：在卡尔曼滤波器完成对所有数据的“前向”遍历之后，[RTS平滑器](@entry_id:142379)执行一个“后向”遍历（从 $t=T$ 回到 $t=1$）。它将每个时刻的滤波估计与来自未来数据的平滑估计相结合，从而计算出精确的平滑分布 $p(x_t | y_{1:T})$。同样，这些平滑分布也都是高斯分布 。

#### [非线性](@entry_id:637147)与非高斯模型中的[近似推断](@entry_id:746496)

在神经科学中，线性和[高斯假设](@entry_id:170316)往往只是一个近似。例如，神经元的脉冲发放更自然地被描述为泊松过程，而神经元群体的动力学可能是高度[非线性](@entry_id:637147)的。在这些情况下，[状态空间模型](@entry_id:137993)就变成了[非线性](@entry_id:637147)或非高斯的形式，精确推断通常变得不可行。

一个重要的例子是用于分析脉冲计数的**泊松线性动态系统 (Poisson Linear Dynamical System, PLDS)** 。其[状态方程](@entry_id:274378)保持线性高斯形式，但观测方程变为：
$$
y_{t,i} \sim \text{Poisson}(\lambda_{t,i}), \quad \text{其中 } \lambda_{t,i} = \exp(c_i^\top x_t + d_i)
$$
这里 $y_{t,i}$ 是神经元 $i$ 在时刻 $t$ 的脉冲计数。指数函数确保了发放率 $\lambda_{t,i}$ 始终为正。

在这个模型中，观测[似然](@entry_id:167119) $p(y_t | x_t)$ 是[泊松分布](@entry_id:147769)，它与[高斯先验](@entry_id:749752) $p(x_t | y_{1:t-1})$ **不是共轭的**。这意味着[后验分布](@entry_id:145605) $p(x_t | y_{1:t}) \propto p(y_t | x_t) p(x_t | y_{1:t-1})$ 不再是高斯分布，也没有简单的[闭式](@entry_id:271343)形式。因此，我们必须依赖[近似推断](@entry_id:746496)方法。

*   **[拉普拉斯近似](@entry_id:636859) (Laplace Approximation)**：这种方法通过在[后验分布](@entry_id:145605)的众数（mode）处进行二阶泰勒展开，来用一个高斯分布逼近真实的[后验分布](@entry_id:145605)。对于PLDS，由于其[对数似然函数](@entry_id:168593)是潜在状态 $x_t$ 的[凹函数](@entry_id:274100)，后验分布是单峰的，这使得[拉普拉斯近似](@entry_id:636859)成为一个有效且稳定的选择 。

*   **[无迹卡尔曼滤波器](@entry_id:166733) (Unscented Kalman Filter, UKF)**：对于具有[非线性](@entry_id:637147)状态或观测函数的模型，如 $y_t = g(x_t) + v_t$，UKF提供了一种比[扩展卡尔曼滤波器](@entry_id:199333)（EKF）更精确的近似方法。UKF的核心思想是，用一组精心选择的确定性样本点（称为**[sigma点](@entry_id:171701)**）来捕捉状态分布的均值和协方差，而不是对[非线性](@entry_id:637147)函数本身进行线性化。这些[sigma点](@entry_id:171701)随后通过[非线性](@entry_id:637147)函数进行传播，然后通过加权平均来重构变换后的分布的均值和协方差。这种方法避免了计算[雅可比矩阵](@entry_id:178326)，并且对于高度[非线性](@entry_id:637147)的函数通常能提供更好的性能 。

*   **期望传播 (Expectation Propagation, EP)**：EP是另一种强大的[近似推断](@entry_id:746496)技术，它通过用[指数族](@entry_id:263444)（如高斯）中的“简单”因子来近似[似然函数](@entry_id:921601)中的“复杂”因子，从而使得后验可解。在PLDS的背景下，EP用一个高斯项来近似每个泊松[似然](@entry_id:167119)项，并通过匹配关键矩来确保近似的质量。由于泊松[似然](@entry_id:167119)的对数[凹性](@entry_id:139843)，EP在这个模型中表现得尤为稳定和准确 。

### 参数估计与[模型可辨识性](@entry_id:186414)

一个状态空间模型的完整描述不仅包括状态推断，还包括从数据中学习模型的未知参数 $\theta = \{A, B, C, \dots\}$。

#### 基于[最大似然](@entry_id:146147)的参数估计

参数估计的一个标准方法是**最大似然估计（Maximum Likelihood Estimation, MLE）**，即寻找能使观测数据似然 $p(y_{1:T} | \theta)$ 最大化的参数 $\theta$。

对于LGSSM，我们可以利用卡尔曼滤波器来高效地计算这个似然值。通过[概率的链式法则](@entry_id:268139)，对数似然可以被分解为一系列单步预测概率的对数和，这被称为**创新分解（innovations decomposition）** ：
$$
\ln p(y_{1:T} | \theta) = \sum_{t=1}^{T} \ln p(y_t | y_{1:t-1}, \theta)
$$
卡尔曼滤波器在每一步都会计算出[预测分布](@entry_id:165741) $p(y_t | y_{1:t-1}, \theta)$，它是一个均值为 $\hat{y}_{t|t-1} = C \hat{x}_{t|t-1}$、协方差为 $S_t = C P_{t|t-1} C^\top + R$ 的高斯分布。这里的 $\hat{x}_{t|t-1}$ 和 $P_{t|t-1}$ 是状态的预测均值和协方差。因此，总的[对数似然](@entry_id:273783)可以表示为：
$$
\ln p(y_{1:T} | \theta) = -\frac{Tm}{2}\ln(2\pi) - \frac{1}{2}\sum_{t=1}^{T} \left( \ln(\det(S_t)) + e_t^\top S_t^{-1} e_t \right)
$$
其中 $e_t = y_t - \hat{y}_{t|t-1}$ 是**创新（innovation）**或[预测误差](@entry_id:753692)。这个表达式可以被[数值优化](@entry_id:138060)算法（如梯度上升）最大化。

更常见的是，我们使用**期望-最大化（Expectation-Maximization, EM）**算法来寻找最大似然解。[EM算法](@entry_id:274778)在潜在变量模型中特别有用，它通过迭代执行以下两个步骤来间接最大化数据[似然](@entry_id:167119)：
*   **E步 (Expectation)**：给定当前的参数估计 $\theta^{(k)}$，运行[卡尔曼平滑器](@entry_id:143392)来计算潜在状态的后验分布 $p(x_{0:T} | y_{1:T}, \theta^{(k)})$。然后，计算完整数据[对数似然](@entry_id:273783) $p(x_{0:T}, y_{1:T} | \theta)$ 在这个后验分布下的期望。
*   **[M步](@entry_id:178892) (Maximization)**：最大化E步中计算出的期望[对数似然](@entry_id:273783)，以更新[参数估计](@entry_id:139349)至 $\theta^{(k+1)}$。对于LGSSM，这个最大化步骤有[闭式](@entry_id:271343)解。

#### 模型的[可辨识性](@entry_id:194150)问题

在从数据中学习模型时，一个至关重要的问题是**[可辨识性](@entry_id:194150)（identifiability）**：观测数据能否唯一地确定模型的参数？如果存在两组不同的参数 $\theta_1 \neq \theta_2$ 却生成了完全相同的观测数据分布 $p(y_{1:T}|\theta_1) = p(y_{1:T}|\theta_2)$，那么模型就是不可辨识的。[状态空间模型](@entry_id:137993)存在几种典型的[不可辨识性](@entry_id:1128800)。

*   **能控性与能观性 (Controllability and Observability)**
    在深入探讨[参数辨识](@entry_id:275549)问题之前，我们必须引入两个来自控制理论的核心概念 ：
    - **能控性**：一个系统是能控的，如果可以通过施加一个合适的输入序列 $\{u_t\}$，在有限时间内将系统从任意初始状态驱动到任意期望的最终状态。对于[LTI系统](@entry_id:271946)，这等价于**能控性矩阵** $\mathcal{C}_n = [B, AB, \dots, A^{n-1}B]$ 的秩为 $n_x$。
    - **能观性**：一个系统是能观的，如果仅通过观测有限时间内的输出序列 $\{y_t\}$ 和输入序列 $\{u_t\}$，就能唯一地确定系统的初始状态 $x_0$。对于[LTI系统](@entry_id:271946)，这等价于**能观性矩阵** $\mathcal{O}_n = [C^\top, (CA)^\top, \dots, (CA^{n-1})^\top]^\top$ 的秩为 $n_x$。

    这两个概念对于状态估计至关重要。例如，在一个有噪声的系统中，卡尔曼滤波器的估计[误差协方差](@entry_id:194780)能否收敛到一个稳定的值，取决于系统的**能检测性（detectability）**（一个比能观性更弱的条件）。

    **[卡尔曼分解](@entry_id:144281)（Kalman Decomposition）**提供了一个深刻的视角，它指出任何[线性系统](@entry_id:147850)的[状态空间](@entry_id:160914)都可以分解为四个[互斥](@entry_id:752349)的子空间：(1) 能控且能观的，(2) 能控但不能观的，(3) 不能控但能观的，以及 (4) 既不能控也不能观的 。一个系统的输入-输出行为完全由其“能控且能观”的部分决定。任何处于“不能控”或“不能观”子空间中的动力学都无法从输入-输出数据中被辨识出来。例如，如果一个状态是不能观的，那么它的动力学（如相关的 $A$ [矩阵特征值](@entry_id:156365)）对输出没有影响，因此无法从数据中学习。

*   **相似性变换[不可辨识性](@entry_id:1128800)**
    由于潜在状态 $x_t$ 是未被观测的，其[坐标基](@entry_id:270149)底的选择是任意的。对于任意[可逆矩阵](@entry_id:171829) $T \in \mathbb{R}^{n_x \times n_x}$，我们可以定义一个新的潜在状态 $x'_t = T x_t$。原始模型可以被等价地重写为关于 $x'_t$ 的新模型，其参数为 $\theta' = \{A' = TAT^{-1}, B' = TB, C' = CT^{-1}, \dots\}$。这个新模型 $\theta'$ 与原始模型 $\theta$ 在观测上是无法区分的 。这意味着我们最多只能将模型[参数辨识](@entry_id:275549)到这个[等价类](@entry_id:156032)。唯一的例外是，如果我们有关于 $C$ 的先验知识，例如，如果 $C$ 是已知的并且具有[满列秩](@entry_id:749628)，那么 $C = C' = CT^{-1}$ 意味着 $T$ 必须是[单位矩阵](@entry_id:156724)，从而消除了这种模糊性。

*   **偏移量[不可辨识性](@entry_id:1128800) (Offset Ambiguity)**
    考虑一个带有偏移项的模型：$x_{t+1} = A x_t + b + w_t$ 和 $y_t = C x_t + d + v_t$。我们可以向潜在状态轨迹添加任意常数偏移 $s$，$x'_t = x_t + s$，并通过调整偏移参数 $b$ 和 $d$ 来完全补偿这一变化：$b' = b - (I-A)s$，$d' = d - Cs$。这个变换不会改变观测数据的分布 。为了解决这个问题，通常会施加一个约束，例如要求潜在状态的期望均值为零，$\mathbb{E}[x_t]=0$。

*   **标签交换[不可辨识性](@entry_id:1128800) (Label Switching)**
    对于更复杂的模型，如**切换线性动态系统（Switching Linear Dynamical System, SLDS）**，其中系统的动力学矩阵 $(A_i, Q_i)$ 会根据一个离散的隐状态 $s_t$ 在多个模式之间切换，会产生额外的[不可辨识性](@entry_id:1128800)。最明显的是，这些离散模式的标签是任意的。我们可以任意置换模式的标签 $\{1, \dots, K\}$，并相应地置换所有与模式相关的参数（如 $A_i, Q_i, \Pi_{ij}$），而模型的输入-输出行为保持不变 [@problem_id:4s4022540]。

### 与其他降维方法的关系

状态空间模型，特别是LGSSM，与[神经数据分析](@entry_id:1128577)中其他常用的[降维](@entry_id:142982)方法（如[主成分分析PCA](@entry_id:173144)和[因子分析](@entry_id:165399)FA）有着深刻的联系 。

*   **概率[主成分分析](@entry_id:145395) (Probabilistic PCA, PPCA)** 和 **[因子分析](@entry_id:165399) (Factor Analysis, FA)** 可以被看作是“静态”的LGSSM。它们都假设[高维数据](@entry_id:138874) $y_t$ 是由一个低维潜在变量 $z_t$ 通过[线性映射](@entry_id:185132) $C$ 加噪声生成的：$y_t = \mu + C z_t + \epsilon_t$。关键的区别在于，它们假设每个时刻的观测是[独立同分布](@entry_id:169067)的（i.i.d.），即潜在变量之间没有时间依赖性。这相当于一个LGSSM，其中[状态转移矩阵](@entry_id:269075) $A=0$ 且过程噪声 $Q=I$。PPCA进一步假设观测噪声是各向同性的（$R = \sigma^2 I$），而FA允许各向异性的对角噪声（$R = \Psi$，一个对角矩阵）。

*   **子空间等价性**：在何时这些模型会识别出相同的[神经子空间](@entry_id:1128624)（即由 $C$ 的列[向量张成](@entry_id:152883)的空间）？
    - 如果真实的观测噪声是**各向同性的** ($R=\sigma^2 I$)，那么在总体数据（population data）的极限下，PCA、FA和（[稳态](@entry_id:139253)）LDS识别出的潜在子空间是相同的。这是因为观测协方差矩阵的形式为 $\Sigma_y = C \Sigma_x C^\top + \sigma^2 I$。它的主特征向量（由PCA找到）与信号协方差 $C \Sigma_x C^\top$ 的[特征向量](@entry_id:151813)相同，而后者张成的空间正是 $C$ 的[列空间](@entry_id:156444)。
    - 然而，如果观测噪声是**非各向同性的**（例如，在FA中，$\Psi$ 的对角元素不同），情况就变了。PCA旨在解释观测数据的最大方差，它会将信号方差和不均匀的噪声方差混为一谈，从而导致其识别出的子空间发生旋转，偏离真实的[信号子空间](@entry_id:185227)。相比之下，FA和LDS通过显式地对非各向同性的噪声建模，原则上能够更准确地恢复出由 $C$ 定义的真实潜在子空间。

总之，状态空间模型为分析神经动力学提供了一个原则性且可解释的框架。它不仅能实现降维，还能捕捉数据中丰富的時間结构。然而，成功应用这些模型需要仔细考虑推断的计算成本、[非线性](@entry_id:637147)/非高斯扩展的必要性，以及对模型参数[不可辨识性](@entry_id:1128800)的深刻理解。