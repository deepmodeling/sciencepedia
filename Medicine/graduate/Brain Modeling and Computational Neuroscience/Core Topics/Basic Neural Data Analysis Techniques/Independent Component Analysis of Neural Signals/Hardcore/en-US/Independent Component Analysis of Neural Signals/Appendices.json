{
    "hands_on_practices": [
        {
            "introduction": "Before applying Independent Component Analysis to find statistically independent sources, a crucial preprocessing step is whitening. This linear transformation decorrelates the observed signals and normalizes their variances, simplifying the subsequent search for the unmixing matrix into a problem of finding an orthogonal rotation. This exercise provides fundamental practice by asking you to derive a whitening transform from first principles and apply it to a specific covariance matrix, solidifying your understanding of this essential gateway to ICA .",
            "id": "3990037",
            "problem": "Suppose a neural data analyst is preparing two-channel electroencephalography (EEG) observations for Independent Component Analysis (ICA). Let the zero-mean observation vector be $x \\in \\mathbb{R}^{2}$ with covariance $\\Sigma_{x} = \\mathbb{E}[x x^{\\top}]$, where $\\Sigma_{x}$ is symmetric and positive-definite. A linear transform $V \\in \\mathbb{R}^{2 \\times 2}$ is called a whitening transform if the transformed signal $z = V x$ has identity covariance, that is, $\\mathbb{E}[z z^{\\top}] = I$.\n\nStarting only from the definitions of covariance and the spectral theorem for real symmetric matrices, first derive an explicit expression for a whitening transform $V$ in terms of the eigen-decomposition of $\\Sigma_{x}$. Then, apply your derivation to the specific covariance matrix\n$$\n\\Sigma_{x} = \\begin{pmatrix}\n5 & 4 \\\\\n4 & 5\n\\end{pmatrix}.\n$$\nConstruct $V$ by choosing the orthonormal eigenbasis so that eigenvalues are ordered in nonincreasing order and, for each eigenvector, the first nonzero entry is positive. Provide your final $V$ exactly as a $2 \\times 2$ matrix with no rounding and no units.",
            "solution": "We begin from the definition of covariance. For a zero-mean random vector $x \\in \\mathbb{R}^{n}$ with covariance $\\Sigma_{x} = \\mathbb{E}[x x^{\\top}]$, and a linear transform $V \\in \\mathbb{R}^{n \\times n}$, the transformed variable $z = V x$ has covariance\n$$\n\\Sigma_{z} = \\mathbb{E}[z z^{\\top}] = \\mathbb{E}[(V x)(V x)^{\\top}] = V \\,\\mathbb{E}[x x^{\\top}]\\, V^{\\top} = V \\Sigma_{x} V^{\\top}.\n$$\nA whitening transform is any $V$ such that $\\Sigma_{z} = I$, which requires\n$$\nV \\Sigma_{x} V^{\\top} = I.\n$$\n\nTo derive an explicit construction for $V$, we invoke the spectral theorem for real symmetric matrices. Since $\\Sigma_{x}$ is symmetric and positive-definite, there exists an orthonormal matrix $E$ and a diagonal matrix $D$ with strictly positive diagonal entries (the eigenvalues) such that\n$$\n\\Sigma_{x} = E D E^{\\top},\n$$\nwhere $E^{\\top} E = I$ and $D = \\operatorname{diag}(\\lambda_{1}, \\ldots, \\lambda_{n})$ with $\\lambda_{i} > 0$ for all $i \\in \\{1,\\ldots,n\\}$.\n\nDefine the diagonal matrix $D^{-1/2} = \\operatorname{diag}(\\lambda_{1}^{-1/2}, \\ldots, \\lambda_{n}^{-1/2})$. Consider the transform\n$$\nV = D^{-1/2} E^{\\top}.\n$$\nWe verify that this choice whitens $x$:\n$$\nV \\Sigma_{x} V^{\\top} = \\left(D^{-1/2} E^{\\top}\\right) \\left(E D E^{\\top}\\right) \\left(E D^{-1/2}\\right) = D^{-1/2} \\underbrace{E^{\\top} E}_{I} D \\underbrace{E^{\\top} E}_{I} D^{-1/2} = D^{-1/2} D D^{-1/2} = I.\n$$\nThus, $V = D^{-1/2} E^{\\top}$ is a valid whitening transform. This is sometimes called the principal component whitening transform.\n\nWe now apply this construction to the specific case $n = 2$ with\n$$\n\\Sigma_{x} = \\begin{pmatrix}\n5 & 4 \\\\\n4 & 5\n\\end{pmatrix}.\n$$\nFirst, compute the eigenvalues by solving the characteristic equation $\\det(\\Sigma_{x} - \\lambda I) = 0$:\n\n$$\n\\det\\!\\begin{pmatrix}\n5 - \\lambda & 4 \\\\\n4 & 5 - \\lambda\n\\end{pmatrix}\n= (5 - \\lambda)^{2} - 16\n= \\lambda^{2} - 10 \\lambda + 9\n= 0.\n$$\n\nThe roots of $\\lambda^{2} - 10 \\lambda + 9 = 0$ are\n\n$$\n\\lambda = \\frac{10 \\pm \\sqrt{100 - 36}}{2} = \\frac{10 \\pm 8}{2} \\in \\{9, 1\\}.\n$$\n\nOrder the eigenvalues in nonincreasing order as $\\lambda_{1} = 9$ and $\\lambda_{2} = 1$.\n\nNext, compute the corresponding orthonormal eigenvectors. For $\\lambda_{1} = 9$, solve $(\\Sigma_{x} - 9 I) v = 0$:\n\n$$\n\\begin{pmatrix}\n-4 & 4 \\\\\n4 & -4\n\\end{pmatrix}\n\\begin{pmatrix}\nv_{1} \\\\\nv_{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\n\\quad \\Rightarrow \\quad v_{2} = v_{1}.\n$$\n\nAn eigenvector is proportional to $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$; normalize to unit length and enforce that the first nonzero entry is positive to obtain\n\n$$\ne_{1} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\n\nFor $\\lambda_{2} = 1$, solve $(\\Sigma_{x} - I) v = 0$:\n\n$$\n\\begin{pmatrix}\n4 & 4 \\\\\n4 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\nv_{1} \\\\\nv_{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\n\\quad \\Rightarrow \\quad v_{2} = - v_{1}.\n$$\n\nAn eigenvector is proportional to $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$; normalize and enforce that the first nonzero entry is positive to obtain\n\n$$\ne_{2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\n\nForm the orthonormal eigenvector matrix\n\n$$\nE = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n$$\n\nwith columns $e_{1}$ and $e_{2}$. Construct\n\n$$\nD = \\begin{pmatrix}\n9 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\quad \\text{and} \\quad\nD^{-1/2} = \\begin{pmatrix}\n\\frac{1}{3} & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\n\nTherefore, the whitening transform is\n\n$$\nV = D^{-1/2} E^{\\top} = D^{-1/2} E\n= \\begin{pmatrix}\n\\frac{1}{3} & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{1}{3\\sqrt{2}} & \\frac{1}{3\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}.\n$$\n\nBy the general verification above, this $V$ satisfies $V \\Sigma_{x} V^{\\top} = I$, completing the construction and the specific computation.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{3\\sqrt{2}} & \\frac{1}{3\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Once data is whitened, the core task of ICA is to find the rotation that unmixes the sources by maximizing their non-Gaussianity. The FastICA algorithm provides an efficient fixed-point iteration scheme to achieve this. This hands-on programming exercise challenges you to implement a single, complete step of the symmetric FastICA algorithm, from the core update rule to the orthonormalization procedure, providing a concrete understanding of how the abstract theory translates into a working algorithm .",
            "id": "3990008",
            "problem": "You are given a synthetic multichannel Electroencephalography (EEG) dataset intended for Independent Component Analysis (ICA). Let the observed data matrix be denoted by $X \\in \\mathbb{R}^{m \\times n}$ with $m=3$ channels and $n$ samples. The generative model is the standard instantaneous linear mixture model $x(t) = A s(t)$, where $x(t) \\in \\mathbb{R}^{m}$ are observed mixtures, $s(t) \\in \\mathbb{R}^{m}$ are statistically independent latent sources, and $A \\in \\mathbb{R}^{m \\times m}$ is an invertible mixing matrix. In Independent Component Analysis (ICA) we seek an unmixing matrix $W \\in \\mathbb{R}^{m \\times m}$ such that $y(t) = W z(t)$ recovers the sources up to scale and permutation from whitened data $z(t)$. Fast Independent Component Analysis (FastICA) is a fixed-point algorithm that iteratively updates $W$ to maximize a contrast linked to non-Gaussianity while constraining the rows of $W$ to be orthonormal in the whitened space.\n\nYour task is to implement a single iteration of the symmetric Fast Independent Component Analysis (FastICA) update on a given dataset and initial $W$, using a specified nonlinearity $g(u)$ and its derivative $g'(u)$, after prewhitening the data. The prewhitening step centers $X$ by subtracting the empirical mean and applies a linear transform that makes the covariance of $Z$ equal to the identity. Use a numerically stable whitening that adds a diagonal regularization term $\\epsilon I$ to the empirical covariance before inverting its square root. After computing the single FastICA update, enforce the symmetric orthonormality constraint via symmetric decorrelation of the updated $W$. You must derive the iteration rule from first principles starting from an ICA contrast based on approximate negentropy and its score function, without relying on pre-stated algorithmic formulas in the problem statement.\n\nDefinitions and conventions:\n- Electroencephalography (EEG) is defined here as a multichannel time series matrix $X$ with $m$ channels and $n$ samples.\n- Fast Independent Component Analysis (FastICA) refers to the fixed-point ICA algorithm that maximizes a non-Gaussianity-based contrast under orthonormality constraints in the whitened space.\n- Whitening: given centered $X$, compute $S = \\frac{1}{n} X X^{\\top}$ and form $Z = P X$ with $P = U \\operatorname{diag}(\\lambda)^{-1/2} U^{\\top}$, where $S \\approx U \\operatorname{diag}(\\lambda) U^{\\top}$ is an eigenvalue decomposition, with a numerically stable variant using $S + \\epsilon I$.\n- Symmetric decorrelation: given an updated $W$, replace it by $W \\leftarrow (W W^{\\top})^{-1/2} W$ so that $W W^{\\top} = I$.\n\nNonlinearity choices for the contrast:\n- Case $\\mathrm{tanh}$: $g(u) = \\tanh(\\alpha u)$ and $g'(u) = \\alpha \\left(1 - \\tanh(\\alpha u)^{2}\\right)$ with slope parameter $\\alpha > 0$.\n- Case $\\mathrm{cubic}$: $g(u) = u^{3}$ and $g'(u) = 3 u^{2}$.\n\nCompute exactly one FastICA iteration (update followed by symmetric decorrelation) for each of the following test cases. In all cases use $m=3$.\n\nData matrices $X$:\n- Test Case $1$ and Test Case $2$ use the same $X \\in \\mathbb{R}^{3 \\times 10}$:\n  $$\n  X = \\begin{bmatrix}\n  0.5 & 1.2 & -0.7 & 0.3 & -1.1 & 0.0 & 0.8 & -0.6 & 1.0 & -0.9 \\\\\n  1.0 & -0.4 & 0.6 & -1.3 & 0.2 & 1.1 & -0.5 & 0.7 & -0.8 & 0.3 \\\\\n  -0.3 & 0.9 & 1.4 & -0.2 & 0.5 & -1.2 & 0.6 & 0.4 & -1.0 & 1.3\n  \\end{bmatrix}.\n  $$\n- Test Case $3$ uses $X \\in \\mathbb{R}^{3 \\times 4}$:\n  $$\n  X = \\begin{bmatrix}\n  0.2 & 0.2 & 0.2 & -0.2 \\\\\n  0.1 & 0.1 & 0.1 & -0.1 \\\\\n  -0.3 & -0.3 & -0.3 & 0.3\n  \\end{bmatrix}.\n  $$\n\nInitial unmixing matrices $W_{0}$:\n- Test Case $1$:\n  $$\n  W_{0} = \\begin{bmatrix}\n  0.9 & -0.2 & 0.1 \\\\\n  -0.1 & 0.8 & 0.3 \\\\\n  0.05 & -0.4 & 0.7\n  \\end{bmatrix}.\n  $$\n- Test Case $2$:\n  $$\n  W_{0} = \\begin{bmatrix}\n  1.0 & 0.0 & 0.0 \\\\\n  0.0 & 1.0 & 0.0 \\\\\n  0.0 & 0.0 & 1.0\n  \\end{bmatrix}.\n  $$\n- Test Case $3$:\n  $$\n  W_{0} = \\begin{bmatrix}\n  0.99 & 0.01 & 0.00 \\\\\n  0.98 & 0.02 & 0.00 \\\\\n  0.00 & 0.00 & 1.00\n  \\end{bmatrix}.\n  $$\n\nNonlinearity and whitening regularization parameters:\n- Test Case $1$: use $\\mathrm{tanh}$ with $\\alpha = 1.2$ and whitening regularization $\\epsilon = 10^{-6}$.\n- Test Case $2$: use $\\mathrm{cubic}$ with no slope parameter and whitening regularization $\\epsilon = 10^{-6}$.\n- Test Case $3$: use $\\mathrm{tanh}$ with $\\alpha = 2.0$ and whitening regularization $\\epsilon = 10^{-3}$.\n\nAlgorithmic requirements:\n- Center $X$ by subtracting its column-wise empirical mean.\n- Whiten $X$ using the regularized covariance $S + \\epsilon I$ and an eigen-decomposition to obtain $Z$ with empirical covariance close to the identity.\n- Perform one symmetric FastICA update for all rows of $W$ simultaneously using the chosen $g(u)$ and $g'(u)$, then apply symmetric decorrelation to the updated $W$.\n- No iteration beyond this single update is required.\n\nYour program must output, for each test case, the updated unmixing matrix $W_{\\text{new}}$ as a flat list of $9$ entries in row-major order, rounded to $6$ decimal places. Aggregate all test cases into a single line. Specifically, your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a flat list of $9$ floating-point numbers. For example, the outer format must be of the form $[\\,[\\cdots],\\,[\\cdots],\\,[\\cdots]\\,]$.\n\nThere are no physical units, angles, or percentages involved. All outputs are unitless floating-point numbers.\n\nTest suite coverage:\n- Test Case $1$ is the general case using a smooth $\\mathrm{tanh}$ nonlinearity with moderate slope.\n- Test Case $2$ probes the behavior under a polynomial nonlinearity $\\mathrm{cubic}$.\n- Test Case $3$ explores a boundary condition with highly collinear samples and stronger whitening regularization.\n\nYour program should produce a single line of output containing the three flat lists as described above, in the exact format $[[\\text{case1\\_list}],[\\text{case2\\_list}],[\\text{case3\\_list}]]$.",
            "solution": "The user-provided problem has been analyzed and validated.\n\n### Step 1: Extract Givens\n- **Observed Data Model**: $x(t) = A s(t)$, where $X \\in \\mathbb{R}^{m \\times n}$ is the data matrix with $m=3$ channels.\n- **ICA Model**: $y(t) = W z(t)$, where $W \\in \\mathbb{R}^{m \\times m}$ is the unmixing matrix and $z(t)$ is whitened data.\n- **Whitening Definition**: Data $X$ is first centered. The covariance is $S = \\frac{1}{n} X X^{\\top}$. The whitening transform is $P = U \\operatorname{diag}(\\lambda)^{-1/2} U^{\\top}$, derived from the eigenvalue decomposition of the regularized covariance $S + \\epsilon I \\approx U \\operatorname{diag}(\\lambda) U^{\\top}$. The whitened data is $Z = P X$.\n- **Symmetric Decorrelation**: After an update, $W$ is orthonormalized via $W \\leftarrow (W W^{\\top})^{-1/2} W$.\n- **Nonlinearities**:\n    - $\\mathrm{tanh}$: $g(u) = \\tanh(\\alpha u)$, $g'(u) = \\alpha (1 - \\tanh(\\alpha u)^{2})$.\n    - $\\mathrm{cubic}$: $g(u) = u^{3}$, $g'(u) = 3 u^{2}$.\n- **Test Case 1**:\n    - $X = \\begin{bmatrix} 0.5 & 1.2 & -0.7 & 0.3 & -1.1 & 0.0 & 0.8 & -0.6 & 1.0 & -0.9 \\\\ 1.0 & -0.4 & 0.6 & -1.3 & 0.2 & 1.1 & -0.5 & 0.7 & -0.8 & 0.3 \\\\ -0.3 & 0.9 & 1.4 & -0.2 & 0.5 & -1.2 & 0.6 & 0.4 & -1.0 & 1.3 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 10}$.\n    - $W_{0} = \\begin{bmatrix} 0.9 & -0.2 & 0.1 \\\\ -0.1 & 0.8 & 0.3 \\\\ 0.05 & -0.4 & 0.7 \\end{bmatrix}$.\n    - Nonlinearity: $\\mathrm{tanh}$ with $\\alpha = 1.2$.\n    - Whitening regularization: $\\epsilon = 10^{-6}$.\n- **Test Case 2**:\n    - $X$: Same as Test Case 1.\n    - $W_{0} = I = \\begin{bmatrix} 1.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 \\end{bmatrix}$.\n    - Nonlinearity: $\\mathrm{cubic}$.\n    - Whitening regularization: $\\epsilon = 10^{-6}$.\n- **Test Case 3**:\n    - $X = \\begin{bmatrix} 0.2 & 0.2 & 0.2 & -0.2 \\\\ 0.1 & 0.1 & 0.1 & -0.1 \\\\ -0.3 & -0.3 & -0.3 & 0.3 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4}$.\n    - $W_{0} = \\begin{bmatrix} 0.99 & 0.01 & 0.00 \\\\ 0.98 & 0.02 & 0.00 \\\\ 0.00 & 0.00 & 1.00 \\end{bmatrix}$.\n    - Nonlinearity: $\\mathrm{tanh}$ with $\\alpha = 2.0$.\n    - Whitening regularization: $\\epsilon = 10^{-3}$.\n- **Task**: Compute one symmetric FastICA iteration (update + decorrelation) for each case.\n- **Output**: A single line containing three flat lists (row-major order) of the $9$ entries of the final matrix $W_{\\text{new}}$, rounded to $6$ decimal places, in the format $[[\\dots],[\\dots],[\\dots]]$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on Independent Component Analysis (ICA), a standard and widely used method in signal processing and computational neuroscience. The FastICA algorithm, the concepts of whitening, negentropy, and symmetric decorrelation are all established and mathematically sound principles. The chosen nonlinearities are canonical examples for this algorithm. The problem is a direct application of these principles.\n- **Well-Posed**: All necessary inputs—data matrices, initial unmixing matrices, algorithmic parameters ($\\alpha$, $\\epsilon$), and choice of nonlinearity—are explicitly provided for each test case. The steps of the algorithm are clearly defined. This structure ensures that a unique and deterministic numerical solution exists for a single iteration.\n- **Objective**: The problem is stated in precise mathematical terms and algorithmic steps, devoid of any subjectivity or ambiguity.\n- **Completeness and Consistency**: The problem is self-contained. All dimensions, data, and parameters are consistent and sufficient to perform the required calculations. The test cases are designed to cover different aspects of the algorithm's behavior (e.g., different nonlinearities, nearly-singular data).\n\n### Step 3: Verdict and Action\nThe problem is **valid** as it is scientifically sound, well-posed, objective, and complete. A solution will be provided.\n\n### Principle-Based Solution Design\n\nThe objective of Independent Component Analysis (ICA) is to recover a set of statistically independent source signals $s(t)$ from their observed linear mixtures $x(t) = A s(t)$. This is achieved by finding an unmixing matrix $W$ such that the components of $y(t) = W x(t)$ are as independent as possible. The FastICA algorithm accomplishes this by optimizing a measure of non-Gaussianity, as the Central Limit Theorem implies that mixtures of independent sources tend to be more Gaussian than the sources themselves.\n\n**1. Contrast Function and its Optimization**\n\nThe core of FastICA is the maximization of a contrast function that approximates negentropy. Negentropy $J(y)$ measures the difference in entropy between a random variable $y$ and a Gaussian variable of the same variance. A common approximation for negentropy is of the form $J(y) \\propto [\\mathbb{E}\\{G(y)\\} - \\mathbb{E}\\{G(\\nu)\\}]^2$, where $y$ is assumed to have zero mean and unit variance, $\\nu$ is a standard normal variable, and $G$ is a suitable non-quadratic function. Maximizing this contrast is equivalent to finding an extremum for $\\mathbb{E}\\{G(y)\\}$.\n\nThe FastICA algorithm operates on whitened data $z(t)$, for which the covariance matrix $\\mathbb{E}\\{z z^{\\top}\\}$ is the identity matrix $I$. An estimated source is given by a projection $y = w^{\\top}z$. Since the data is whitened, $\\mathbb{E}\\{y^2\\} = w^{\\top}\\mathbb{E}\\{zz^{\\top}\\}w = w^{\\top}Iw = \\|w\\|^2$. We constrain $w$ to be a unit vector, so $\\|w\\|^2 = 1$, which makes the variance of the projection unity.\n\nThe optimization problem is to maximize $\\mathbb{E}\\{G(w^{\\top}z)\\}$ subject to the constraint $\\|w\\|^2=1$. Using a Lagrange multiplier $\\lambda$, we optimize:\n$$\n\\mathcal{L}(w) = \\mathbb{E}\\{G(w^{\\top}z)\\} - \\frac{\\lambda}{2} (w^{\\top}w - 1)\n$$\nTaking the gradient with respect to $w$ and setting it to zero yields the optimality condition:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\mathbb{E}\\{z \\cdot g(w^{\\top}z)\\} - \\lambda w = 0\n$$\nwhere $g(u) = G'(u)$ is the nonlinearity. This equation forms the basis for a fixed-point iteration. A principled derivation using an approximate Newton's method leads to the following update rule for a single component (one row of the unmixing matrix):\n$$\nw^{+} \\leftarrow \\mathbb{E}\\{z g(w^{\\top}z)\\} - \\mathbb{E}\\{g'(w^{\\top}z)\\}w\n$$\nfollowed by normalization, $w^{+} \\leftarrow w^{+} / \\|w^{+}\\|$.\n\n**2. Symmetric FastICA and Decorrelation**\n\nInstead of estimating components one by one (deflationary approach), the symmetric approach updates the entire unmixing matrix $W$ in parallel. The update rule for $W$ is a direct matrix generalization of the single-unit rule:\n$$\nW^{+} \\leftarrow \\mathbb{E}\\{g(WZ)Z^{\\top}\\} - \\mathrm{diag}(\\mathbb{E}_{\\text{rows}}\\{g'(WZ)\\})W\n$$\nHere, $Z$ is the $m \\times n$ matrix of whitened data, $g$ and $g'$ are applied element-wise to the $m \\times n$ matrix of projections $WZ$. The expectation $\\mathbb{E}\\{\\cdot\\}$ is implemented as an average over the $n$ samples. The first term $\\mathbb{E}\\{g(WZ)Z^{\\top}\\}$ is computed as $\\frac{1}{n} g(WZ)Z^{\\top}$. The second term involves a diagonal matrix whose entries are the row-wise means of $g'(WZ)$, which corresponds to $\\mathbb{E}\\{g'(w_i^{\\top}z)\\}$ for each row $w_i$ of $W$.\n\nAfter this update, the rows of $W^{+}$ are no longer guaranteed to be orthogonal. To enforce the orthonormality constraint required in the whitened space (i.e., $WW^{\\top}=I$), a symmetric decorrelation step is applied:\n$$\nW_{\\text{new}} \\leftarrow (W^{+} (W^{+})^{\\top})^{-1/2} W^{+}\n$$\nThe inverse square root of the matrix $C = W^{+} (W^{+})^{\\top}$ can be computed robustly via its eigenvalue decomposition. If $C = E D E^{\\top}$, where $E$ is the matrix of eigenvectors and $D$ is the diagonal matrix of eigenvalues, then $C^{-1/2} = E D^{-1/2} E^{\\top}$.\n\n**3. Data Pre-processing: Centering and Whitening**\n\nBefore applying the FastICA updates, the data must be pre-processed.\n- **Centering**: The mean of each channel is subtracted from the data to ensure it is zero-mean. Given the data matrix $X \\in \\mathbb{R}^{m \\times n}$, the centered data is $X_c = X - \\mathbb{E}[X]$, where the expectation is the empirical mean across samples (columns of $X$).\n- **Whitening**: This step transforms the data so that its components are uncorrelated and have unit variance, i.e., its covariance matrix is the identity matrix. The empirical covariance matrix of the centered data is $S = \\frac{1}{n} X_c X_c^{\\top}$. For numerical stability, a small regularization term $\\epsilon I$ is added: $S_{\\text{reg}} = S + \\epsilon I$. The eigenvalue decomposition of this symmetric matrix is $S_{\\text{reg}} = U \\Lambda U^{\\top}$. The whitening matrix is $P = U \\Lambda^{-1/2} U^{\\top}$. The final whitened data is $Z = P X_c$.\n\n**4. Algorithm for One Iteration**\n\nThe complete algorithm for a single iteration is as follows:\n1.  **Center Data**: Given $X$, compute $X_c = X - \\frac{1}{n}\\sum_{t=1}^n x(t)$.\n2.  **Whiten Data**:\n    a. Compute the regularized covariance matrix $S_{\\text{reg}} = \\frac{1}{n} X_c X_c^{\\top} + \\epsilon I$.\n    b. Find its eigenvalue decomposition $S_{\\text{reg}} = U \\Lambda U^{\\top}$.\n    c. Construct the whitening matrix $P = U \\Lambda^{-1/2} U^{\\top}$.\n    d. Obtain the whitened data $Z = P X_c$.\n3.  **FastICA Update**: Given the initial unmixing matrix $W_0$, compute the updated (but not yet orthonormal) matrix $W^{+}$:\n    $$\n    W^{+} = \\frac{1}{n} g(W_0 Z) Z^{\\top} - \\mathrm{diag}\\left(\\text{mean}_{\\text{cols}}[g'(W_0 Z)^{\\top}]\\right) W_0\n    $$\n    where the provided nonlinearities $g$ and $g'$ are used.\n4.  **Symmetric Decorrelation**: Orthonormalize the rows of $W^{+}$ to obtain the final matrix for this iteration, $W_{\\text{new}}$:\n    a. Compute $C = W^{+} (W^{+})^{\\top}$.\n    b. Find its eigenvalue decomposition $C = E D E^{\\top}$.\n    c. Apply the transformation $W_{\\text{new}} = (E D^{-1/2} E^{\\top}) W^{+}$.\nThis procedure will be applied to each test case with its specific data and parameters.",
            "answer": "```python\nimport numpy as np\n\ndef run_fastica_iteration(X, W0, g_type, alpha, epsilon):\n    \"\"\"\n    Performs a single symmetric FastICA iteration.\n    \n    Args:\n        X (np.ndarray): Data matrix (m channels x n samples).\n        W0 (np.ndarray): Initial unmixing matrix (m x m).\n        g_type (str): Type of nonlinearity ('tanh' or 'cubic').\n        alpha (float or None): Parameter for the 'tanh' nonlinearity.\n        epsilon (float): Regularization parameter for whitening.\n        \n    Returns:\n        np.ndarray: The updated unmixing matrix W_new (m x m).\n    \"\"\"\n    m, n = X.shape\n\n    # 1. Center the data\n    mean_X = np.mean(X, axis=1, keepdims=True)\n    X_c = X - mean_X\n\n    # 2. Whiten the data\n    # Compute regularized covariance matrix\n    S = (1/n) * (X_c @ X_c.T)\n    S_reg = S + epsilon * np.eye(m)\n    \n    # Eigenvalue decomposition of the covariance matrix\n    # np.linalg.eigh is used for symmetric matrices\n    eigvals, eigvecs = np.linalg.eigh(S_reg)\n    \n    # Whitening matrix P = U * D^(-1/2) * U^T\n    P = eigvecs @ np.diag(eigvals**(-0.5)) @ eigvecs.T\n    \n    # Whitened data Z = P * X_c\n    Z = P @ X_c\n    \n    # 3. Define nonlinearity g and its derivative g'\n    if g_type == 'tanh':\n        if alpha is None:\n            raise ValueError(\"alpha must be provided for tanh nonlinearity\")\n        g = lambda u: np.tanh(alpha * u)\n        g_prime = lambda u: alpha * (1 - np.tanh(alpha * u)**2)\n    elif g_type == 'cubic':\n        g = lambda u: u**3\n        g_prime = lambda u: 3 * u**2\n    else:\n        raise ValueError(f\"Unknown g_type: {g_type}\")\n\n    # 4. Perform one symmetric FastICA update step\n    # W_updated = E{g(W0*Z)*Z'} - diag(E{g'(W0*Z)})*W0\n    W = W0\n    projections = W @ Z\n    \n    g_projections = g(projections)\n    g_prime_projections = g_prime(projections)\n    \n    term1 = (1/n) * (g_projections @ Z.T)\n    term2 = np.diag(np.mean(g_prime_projections, axis=1)) @ W\n    \n    W_updated = term1 - term2\n    \n    # 5. Symmetric decorrelation (orthonormalization)\n    # W_new = (W_updated * W_updated^T)^(-1/2) * W_updated\n    C = W_updated @ W_updated.T\n    \n    # Eigenvalue decomposition of C\n    d, E = np.linalg.eigh(C)\n    \n    # Inverse square root of C\n    C_inv_sqrt = E @ np.diag(d**(-0.5)) @ E.T\n    \n    # Final updated and orthonormalized unmixing matrix\n    W_new = C_inv_sqrt @ W_updated\n    \n    return W_new\n\ndef solve():\n    \"\"\"\n    Sets up test cases, runs the FastICA iteration, and prints the formatted results.\n    \"\"\"\n    # Define the data matrices from the problem statement\n    X12 = np.array([\n        [0.5, 1.2, -0.7, 0.3, -1.1, 0.0, 0.8, -0.6, 1.0, -0.9],\n        [1.0, -0.4, 0.6, -1.3, 0.2, 1.1, -0.5, 0.7, -0.8, 0.3],\n        [-0.3, 0.9, 1.4, -0.2, 0.5, -1.2, 0.6, 0.4, -1.0, 1.3]\n    ])\n\n    X3 = np.array([\n        [0.2, 0.2, 0.2, -0.2],\n        [0.1, 0.1, 0.1, -0.1],\n        [-0.3, -0.3, -0.3, 0.3]\n    ])\n\n    # Define the initial unmixing matrices\n    W0_1 = np.array([\n        [0.9, -0.2, 0.1],\n        [-0.1, 0.8, 0.3],\n        [0.05, -0.4, 0.7]\n    ])\n\n    W0_2 = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n\n    W0_3 = np.array([\n        [0.99, 0.01, 0.00],\n        [0.98, 0.02, 0.00],\n        [0.00, 0.00, 1.00]\n    ])\n\n    # Define the parameter sets for each test case\n    test_cases = [\n        {'X': X12, 'W0': W0_1, 'g_type': 'tanh', 'alpha': 1.2, 'epsilon': 1e-6},\n        {'X': X12, 'W0': W0_2, 'g_type': 'cubic', 'alpha': None, 'epsilon': 1e-6},\n        {'X': X3,  'W0': W0_3, 'g_type': 'tanh', 'alpha': 2.0, 'epsilon': 1e-3},\n    ]\n\n    results_as_strings = []\n    for case in test_cases:\n        # Run the single FastICA iteration\n        W_new = run_fastica_iteration(\n            case['X'], case['W0'], case['g_type'], case['alpha'], case['epsilon']\n        )\n        \n        # Flatten the resulting matrix to a list in row-major order\n        flat_list = W_new.flatten()\n        \n        # Format the list of numbers into a string \"[num1,num2,...]\"\n        # with each number rounded to 6 decimal places.\n        case_str = f\"[{','.join(f'{x:.6f}' for x in flat_list)}]\"\n        results_as_strings.append(case_str)\n\n    # Final print statement in the exact required format: [[...],[...],[...]]\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Applying ICA to real neural recordings involves more than just running an algorithm; it requires critical judgment, especially when parameters are unknown. A pivotal choice is the model order—the number of independent components to estimate—as the true number of underlying neural sources is never known with certainty. This problem explores the consequences of this decision, helping you build the necessary intuition to interpret results correctly and avoid common pitfalls such as merging distinct sources or splitting noise into spurious components .",
            "id": "3990038",
            "problem": "Consider a neural recording scenario with $m$ sensors and latent neural sources modeled by the linear instantaneous mixing model $x(t) = A s(t) + \\epsilon(t)$, where $x(t) \\in \\mathbb{R}^{m}$ is the observed sensor vector at time $t$, $A \\in \\mathbb{R}^{m \\times k}$ is the mixing matrix, $s(t) \\in \\mathbb{R}^{k}$ is the latent source vector whose components are statistically independent and non-Gaussian, and $\\epsilon(t)$ is sensor noise that is approximately Gaussian. You apply Principal Component Analysis (PCA) for dimensionality reduction, retaining $n$ principal components, and then perform Independent Component Analysis (ICA) on the whitened $n$-dimensional data to estimate $n$ independent components. This workflow is common in Electroencephalography (EEG), Magnetoencephalography (MEG), and functional Magnetic Resonance Imaging (fMRI) analysis.\n\nAssume the following foundational facts: the identifiability of Independent Component Analysis (ICA) requires non-Gaussian, statistically independent sources; whitening by Principal Component Analysis (PCA) yields isotropic noise in the retained subspace; and the number of recoverable independent components cannot exceed the retained dimension $n$. Consider the consequences of choosing $n$ smaller than the true number of neural sources $k$ (underestimating $n$) versus choosing $n$ larger than $k$ but not exceeding $m$ (overestimating $n$), specifically in terms of interpretability of the resulting components (ability to assign components to distinct physiological processes).\n\nWhich of the following statements are correct?\n\nA. Underestimating $n$ causes multiple true sources to be merged into single estimated components, reducing statistical independence between estimated component time courses and producing spatial maps and time courses that reflect mixtures of distinct physiological processes, thereby decreasing interpretability.\n\nB. Overestimating $n$ in the presence of additive Gaussian noise tends to split noise across multiple estimated components. The true non-Gaussian sources are still largely recoverable, but the surplus components are unstable across runs and exhibit low reproducibility, thereby decreasing interpretability by inflating the number of spurious components.\n\nC. Underestimating $n$ improves interpretability because fewer components need to be considered, and optimizing independence ensures that merged sources become more independent than the original sources.\n\nD. Overestimating $n$ yields multiple duplicates of the same true source that differ only by sign due to ICA ambiguities. Because duplicates carry the same information, interpretability is essentially unaffected.\n\nE. Underestimating $n$ biases the estimated mixing matrix columns toward dominant-variance sources retained by Principal Component Analysis (PCA), suppressing weaker sources and increasing explained variance of the retained subspace while decreasing the ability to attribute components to distinct biological processes.\n\nF. Overestimating $n$ increases the risk of overfitting the noise subspace, yielding components with near-zero excess kurtosis and low mutual information with behavioral or task regressors. If such components are accepted into analysis, they inflate false discoveries and reduce interpretability; if they are rejected by objective criteria, interpretability can be partially preserved.\n\nSelect all that apply.",
            "solution": "The problem statement is valid. It describes a standard and well-understood scenario in the application of Independent Component Analysis (ICA) to multichannel time-series data, particularly in neuroimaging. The model, assumptions, and the core questions about the consequences of dimensionality selection are scientifically sound, well-posed, and objective.\n\nThe analysis hinges on the sequential application of Principal Component Analysis (PCA) for dimensionality reduction and whitening, followed by ICA for source separation. The choice of the number of components to retain, $n$, relative to the true number of latent sources, $k$, is critical.\n\nThe linear mixing model is given by:\n$$x(t) = A s(t) + \\epsilon(t)$$\nwhere $x(t) \\in \\mathbb{R}^{m}$ are the sensor signals, $s(t) \\in \\mathbb{R}^{k}$ are the latent independent non-Gaussian sources, $A \\in \\mathbb{R}^{m \\times k}$ is the mixing matrix, and $\\epsilon(t)$ is Gaussian noise.\n\nThe process involves two steps:\n1.  **PCA and Whitening**: The data $x(t)$ is projected onto the $n$-dimensional space spanned by the top $n$ principal components. Let the whitened data be $y(t) \\in \\mathbb{R}^{n}$. This step discards information contained in the remaining $m-n$ dimensions.\n2.  **ICA**: An unmixing matrix $W \\in \\mathbb{R}^{n \\times n}$ is found to estimate $n$ source signals $\\hat{s}(t) = W y(t)$, such that the components of $\\hat{s}(t)$ are maximally statistically independent.\n\nWe analyze the two specified cases.\n\n**Case 1: Underestimation ($n < k$)**\n\nWhen the chosen dimension $n$ is less than the true number of sources $k$, the PCA step represents a bottleneck. The $k$ independent sources $s(t)$, which span a $k$-dimensional subspace (the column space of $A$), are projected onto a lower-dimensional subspace of dimension $n$. This projection is inherently lossy. The transformation from the original sources $s(t)$ to the whitened data $y(t)$ can be described by an effective mixing matrix $A' \\in \\mathbb{R}^{n \\times k}$. The ICA algorithm then attempts to solve the problem $y(t) = A' s(t) + \\text{noise}'$ for $n$ sources, when in fact there are $k$ sources present in the data.\n\nBecause $n < k$, it is mathematically impossible to recover the original $k$ independent sources. The ICA algorithm, in its attempt to find $n$ maximally independent components from data generated by $k$ sources, will yield components that are linear mixtures of the original sources. This violates the goal of source separation. According to the Central Limit Theorem, mixtures of independent random variables tend toward a Gaussian distribution. Thus, the pre-mixing in the PCA stage makes the subsequent ICA task more difficult and its solution less meaningful.\n\nFurthermore, PCA selects dimensions based on variance. If some of the $k$ true sources have low variance (\"weaker\" sources), their corresponding directions in the signal space are likely to be discarded during the reduction to $n$ dimensions. The retained subspace is therefore biased towards representing the dominant, high-variance sources.\n\n**Consequence**: The estimated components are not physiologically distinct sources but are instead composites of multiple true sources. Their temporal dynamics and spatial maps are hybrids, making them difficult or impossible to interpret as representing single, well-defined neurophysiological processes.\n\n**Case 2: Overestimation ($k < n \\le m$)**\n\nWhen the chosen dimension $n$ is greater than the true number of non-Gaussian sources $k$, the PCA step retains the entire $k$-dimensional signal subspace, plus an additional $n-k$ dimensions. In the presence of noise, these additional dimensions are primarily determined by the structure of the Gaussian noise $\\epsilon(t)$.\n\nAfter PCA and whitening, the $n$-dimensional data $y(t)$ contains approximately $k$ dimensions carrying the non-Gaussian source information and $n-k$ dimensions containing primarily isotropic (sphericized) Gaussian noise.\n\nThe ICA algorithm is then tasked with finding $n$ independent components.\n-   It will typically succeed in identifying the $k$ strong, non-Gaussian components corresponding to the true sources $s(t)$. These components will be stable and interpretable.\n-   For the remaining $n-k$ components, ICA is forced to find \"maximally independent\" or \"maximally non-Gaussian\" projections within a subspace of sphericized Gaussian noise. Any rotation of sphericized Gaussian data produces components that are independent and Gaussian. There is no unique, non-Gaussian basis to be found. Therefore, the algorithm will likely latch onto spurious, sample-specific deviations from perfect Gaussianity.\n\n**Consequence**: The algorithm outputs the $k$ true sources plus $n-k$ spurious, noise-driven components. These noise components are unstable (i.e., they will vary significantly across different runs of the algorithm or with slight changes in the data) and lack clear physiological meaning. This increases the burden on the researcher to distinguish true sources from noise, inflating the risk of false discoveries (interpreting a noise component as a real signal) and reducing overall interpretability.\n\nNow we evaluate each option based on this analysis.\n\n**A. Underestimating $n$ causes multiple true sources to be merged into single estimated components, reducing statistical independence between estimated component time courses and producing spatial maps and time courses that reflect mixtures of distinct physiological processes, thereby decreasing interpretability.**\nThis statement is a direct consequence of our analysis for the $n < k$ case. The dimensionality reduction step forces a mixing of the $k$ true sources into an $n$-dimensional representation. ICA cannot subsequently separate these pre-mixed sources, resulting in estimated components that are hybrids of the original sources. This fundamentally compromises interpretability. The statement regarding \"reducing statistical independence between estimated component time courses\" is slightly ambiguous; ICA *maximizes* the independence of the *estimated* components. However, these estimated components are themselves mixtures, and thus do not represent the original, truly independent sources. The core assertion is correct.\nVerdict: **Correct**.\n\n**B. Overestimating $n$ in the presence of additive Gaussian noise tends to split noise across multiple estimated components. The true non-Gaussian sources are still largely recoverable, but the surplus components are unstable across runs and exhibit low reproducibility, thereby decreasing interpretability by inflating the number of spurious components.**\nThis statement accurately describes the $k < n$ case. The $k$ true sources are generally found, but the algorithm is forced to model the remaining $n-k$ dimensions of noise. This results in $n-k$ spurious, unstable, and non-reproducible components. The presence of these extra \"noise\" components complicates the analysis and reduces interpretability.\nVerdict: **Correct**.\n\n**C. Underestimating $n$ improves interpretability because fewer components need to be considered, and optimizing independence ensures that merged sources become more independent than the original sources.**\nThis statement is flawed. While there are fewer components, their quality is severely degraded, as they are mixtures of true sources. The claim that merged sources become \"more independent than the original sources\" is mathematically incorrect. The original sources are assumed to be independent; you cannot improve upon this. Mixing them, as happens in the $n < k$ case, reduces their separability.\nVerdict: **Incorrect**.\n\n**D. Overestimating $n$ yields multiple duplicates of the same true source that differ only by sign due to ICA ambiguities. Because duplicates carry the same information, interpretability is essentially unaffected.**\nThis is an incorrect description of the outcome of overestimation. The typical result is not simple duplication of true sources but the generation of spurious components from the noise subspace, as explained in the analysis for B. While a very strong source might occasionally be split into two highly correlated components by some algorithms, the primary issue is the creation of new, uninterpretable noise components. The claim that interpretability is unaffected is false.\nVerdict: **Incorrect**.\n\n**E. Underestimating $n$ biases the estimated mixing matrix columns toward dominant-variance sources retained by Principal Component Analysis (PCA), suppressing weaker sources and increasing explained variance of the retained subspace while decreasing the ability to attribute components to distinct biological processes.**\nThis statement correctly identifies a key mechanism in the underestimation scenario. PCA is a variance-based method. By retaining only the top $n$ components, it necessarily discards the dimensions with the least variance. If some true physiological sources contribute less variance to the sensor signals, they are the ones most likely to be filtered out. This biases the analysis towards only the strongest sources and leads to a loss of information about weaker, but potentially important, biological processes. The decrease in the ability to attribute components to distinct processes is a direct result.\nVerdict: **Correct**.\n\n**F. Overestimating $n$ increases the risk of overfitting the noise subspace, yielding components with near-zero excess kurtosis and low mutual information with behavioral or task regressors. If such components are accepted into analysis, they inflate false discoveries and reduce interpretability; if they are rejected by objective criteria, interpretability can be partially preserved.**\nThis statement provides a detailed and accurate technical description of the consequences of overestimation. \"Overfitting the noise subspace\" is an excellent characterization. Since the surplus components are derived from approximately Gaussian noise, they will have properties close to a Gaussian distribution, such as near-zero excess kurtosis. Being noise, they should not correlate with meaningful external variables. Accepting them leads to false discoveries. The final point—that using objective criteria (like kurtosis, spatial map plausibility, or explained variance) to identify and reject these noise components is a valid strategy to mitigate the problem—is also correct and reflects best practices.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ABEF}$$"
        }
    ]
}