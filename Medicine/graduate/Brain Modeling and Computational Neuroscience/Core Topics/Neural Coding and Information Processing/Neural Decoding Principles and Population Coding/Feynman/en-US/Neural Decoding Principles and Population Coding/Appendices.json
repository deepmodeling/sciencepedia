{
    "hands_on_practices": [
        {
            "introduction": "Neural decoding begins with a probabilistic model relating a stimulus to the observed neural activity. This foundational exercise guides you through the process of deriving two of the most fundamental types of estimators—Maximum Likelihood (ML) and Maximum A Posteriori (MAP)—for a canonical population of independent Poisson neurons . Mastering this derivation builds the essential statistical toolkit required to understand and develop more advanced decoding algorithms.",
            "id": "4002619",
            "problem": "Consider a scalar sensory stimulus $s \\ge 0$ encoded by a population of $N$ conditionally independent neurons indexed by $i \\in \\{1,\\dots,N\\}$. On a single observation window, neuron $i$ emits a spike count $r_i \\in \\{0,1,2,\\dots\\}$ modeled as a Poisson random variable with mean (rate) $\\lambda_i(s) = c_i s$, where $c_i > 0$ is a known gain constant. Let $\\mathbf{r} = (r_1,\\dots,r_N)^{\\top}$ denote the observed counts. You may assume conditional independence of the spike counts given $s$. \n\nTasks:\n- Using only core definitions, write the likelihood $L(s) = p(\\mathbf{r}\\,|\\,s)$.\n- Using the definition of Maximum Likelihood (ML), derive the Maximum Likelihood estimator $\\hat{s}_{\\mathrm{ML}}$ as the maximizer of $L(s)$ over $s \\ge 0$.\n- Assume a prior density $p(s)$ on $s$ given by a Gamma distribution with shape $\\alpha > 0$ and rate $\\beta > 0$, namely $p(s) \\propto s^{\\alpha - 1} \\exp(-\\beta s)$ for $s \\ge 0$. Using Bayes’ rule, derive the Maximum A Posteriori (MAP) estimator $\\hat{s}_{\\mathrm{MAP}}$ as the maximizer of the posterior $p(s\\,|\\,\\mathbf{r})$ over $s \\ge 0$.\n- Compare the asymptotic properties of $\\hat{s}_{\\mathrm{ML}}$ and $\\hat{s}_{\\mathrm{MAP}}$ under repeated independent observations. Specifically, suppose the experiment is repeated for $T \\in \\mathbb{N}$ independent trials with the same true $s$, producing counts $\\{r_i^{(t)}\\}_{t=1}^T$ for each neuron $i$. Define the aggregate counts $R_i = \\sum_{t=1}^T r_i^{(t)}$. Starting from the Fisher information definition, determine the per-trial Fisher information $I_1(s)$ for this model, and state the large-$T$ behavior of $\\hat{s}_{\\mathrm{ML}}$ and $\\hat{s}_{\\mathrm{MAP}}$ (consistency, asymptotic normality, asymptotic variance, and the leading-order bias of $\\hat{s}_{\\mathrm{MAP}}$). \n\nAnswer specification:\n- Provide the final answer as the pair of estimators $\\hat{s}_{\\mathrm{ML}}$ and $\\hat{s}_{\\mathrm{MAP}}$ in a single row matrix in closed form, expressed in terms of $\\{r_i\\}_{i=1}^N$, $\\{c_i\\}_{i=1}^N$, $\\alpha$, and $\\beta$. \n- No numerical rounding is required.\n- Do not include units in the final boxed answer. \n- Define any acronyms on first use (for example, Maximum Likelihood (ML) and Maximum A Posteriori (MAP)).",
            "solution": "The problem statement is evaluated for validity before proceeding to a solution.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   Scalar stimulus: $s \\ge 0$.\n-   Number of neurons: $N$.\n-   Neuron index: $i \\in \\{1, \\dots, N\\}$.\n-   Spike count for neuron $i$: $r_i \\in \\{0, 1, 2, \\dots\\}$.\n-   Observed counts vector: $\\mathbf{r} = (r_1, \\dots, r_N)^{\\top}$.\n-   Spike count distribution: Poisson random variable.\n-   Poisson mean for neuron $i$: $\\lambda_i(s) = c_i s$.\n-   Gain constant for neuron $i$: $c_i > 0$.\n-   Conditional independence: The spike counts $\\{r_i\\}$ are conditionally independent given $s$.\n-   Prior density on $s$: $p(s) \\propto s^{\\alpha - 1} \\exp(-\\beta s)$ for $s \\ge 0$, which is a Gamma distribution with shape $\\alpha > 0$ and rate $\\beta > 0$.\n-   Repeated trials analysis: $T \\in \\mathbb{N}$ independent trials with the same true $s$, producing counts $\\{r_i^{(t)}\\}_{t=1}^T$ for each neuron $i$.\n-   Aggregate counts: $R_i = \\sum_{t=1}^T r_i^{(t)}$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is well-grounded in computational neuroscience. The use of Poisson spike counts with linear tuning curves is a standard and fundamental model for neural population encoding. The application of Maximum Likelihood (ML) and Maximum A Posteriori (MAP) estimation are core techniques for decoding neural responses. The choice of a Gamma prior is mathematically convenient as it is the conjugate prior for the Poisson likelihood, a common construct in Bayesian modeling.\n-   **Well-Posed**: The problem is well-posed. It provides all necessary information—the statistical model for the data, the functional form of the rate, the prior distribution, and all parameter constraints—to uniquely derive the requested estimators and analyze their properties.\n-   **Objective**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective content.\n-   **Completeness and Consistency**: The problem is self-contained and internally consistent. The constraint $s \\ge 0$ combined with $c_i > 0$ ensures the Poisson rate $\\lambda_i(s)$ is non-negative, as required. The domain of the Gamma prior matches the domain of the stimulus $s$.\n-   **Other Criteria**: The problem does not violate any other criteria for validity. It is a standard theoretical problem that is neither trivial nor ill-posed.\n\n**Step 3: Verdict and Action**\n-   **Verdict**: The problem is valid.\n-   **Action**: A complete solution will be provided.\n\n**Solution Derivation**\n\n**Likelihood Function**\nThe problem states that the spike count $r_i$ from neuron $i$ follows a Poisson distribution with mean $\\lambda_i(s) = c_i s$. The probability mass function (PMF) for a single neuron's response is:\n$$\np(r_i | s) = \\frac{(\\lambda_i(s))^{r_i} \\exp(-\\lambda_i(s))}{r_i!} = \\frac{(c_i s)^{r_i} \\exp(-c_i s)}{r_i!}\n$$\nGiven that the neurons are conditionally independent, the joint probability of observing the vector of spike counts $\\mathbf{r} = (r_1, \\dots, r_N)^{\\top}$ is the product of the individual probabilities. This joint probability, considered as a function of the stimulus $s$, is the likelihood function $L(s)$:\n$$\nL(s) = p(\\mathbf{r} | s) = \\prod_{i=1}^N p(r_i | s) = \\prod_{i=1}^N \\frac{(c_i s)^{r_i} \\exp(-c_i s)}{r_i!}\n$$\nThis can be rewritten by grouping terms dependent on $s$:\n$$\nL(s) = \\left( \\prod_{i=1}^N \\frac{c_i^{r_i}}{r_i!} \\right) \\left( \\prod_{i=1}^N s^{r_i} \\right) \\left( \\prod_{i=1}^N \\exp(-c_i s) \\right) = \\left( \\prod_{i=1}^N \\frac{c_i^{r_i}}{r_i!} \\right) s^{\\sum_{i=1}^N r_i} \\exp\\left(-s \\sum_{i=1}^N c_i\\right)\n$$\n\n**Maximum Likelihood (ML) Estimator**\nThe Maximum Likelihood (ML) estimator, $\\hat{s}_{\\mathrm{ML}}$, is the value of $s$ that maximizes $L(s)$. It is computationally more convenient to maximize the log-likelihood function, $\\ell(s) = \\ln L(s)$, as the logarithm is a monotonic function.\n$$\n\\ell(s) = \\ln \\left( \\prod_{i=1}^N \\frac{(c_i s)^{r_i} \\exp(-c_i s)}{r_i!} \\right) = \\sum_{i=1}^N \\ln \\left( \\frac{(c_i s)^{r_i} \\exp(-c_i s)}{r_i!} \\right)\n$$\n$$\n\\ell(s) = \\sum_{i=1}^N \\left( r_i \\ln(c_i s) - c_i s - \\ln(r_i!) \\right) = \\sum_{i=1}^N \\left( r_i \\ln c_i + r_i \\ln s - c_i s - \\ln(r_i!) \\right)\n$$\nGrouping the terms involving $s$:\n$$\n\\ell(s) = \\left( \\sum_{i=1}^N r_i \\right) \\ln s - \\left( \\sum_{i=1}^N c_i \\right) s + \\mathrm{const}\n$$\nTo find the maximum, we compute the derivative of $\\ell(s)$ with respect to $s$ and set it to zero:\n$$\n\\frac{d\\ell}{ds} = \\frac{\\sum_{i=1}^N r_i}{s} - \\sum_{i=1}^N c_i = 0\n$$\nSolving for $s$ yields the ML estimator:\n$$\n\\hat{s}_{\\mathrm{ML}} = \\frac{\\sum_{i=1}^N r_i}{\\sum_{i=1}^N c_i}\n$$\nThe second derivative, $\\frac{d^2\\ell}{ds^2} = -\\frac{\\sum_{i=1}^N r_i}{s^2}$, is negative (since $r_i \\ge 0$ and we assume at least one spike is observed), confirming that this is a maximum. The estimator is non-negative as required.\n\n**Maximum A Posteriori (MAP) Estimator**\nThe Maximum A Posteriori (MAP) estimator, $\\hat{s}_{\\mathrm{MAP}}$, maximizes the posterior probability density $p(s|\\mathbf{r})$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(s|\\mathbf{r}) \\propto p(\\mathbf{r}|s) p(s) = L(s) p(s)\n$$\nThe prior is a Gamma distribution, $p(s) \\propto s^{\\alpha-1}\\exp(-\\beta s)$. We maximize the log-posterior, $\\ln p(s|\\mathbf{r})$:\n$$\n\\ln p(s|\\mathbf{r}) = \\ln(L(s)) + \\ln(p(s)) + \\mathrm{const}\n$$\nUsing our previous expressions for $\\ell(s)$ and the logarithm of the prior:\n$$\n\\ln p(s|\\mathbf{r}) \\propto \\left[ \\left(\\sum_{i=1}^N r_i\\right) \\ln s - \\left(\\sum_{i=1}^N c_i\\right) s \\right] + \\left[ (\\alpha-1)\\ln s - \\beta s \\right]\n$$\n$$\n\\ln p(s|\\mathbf{r}) \\propto \\left(\\sum_{i=1}^N r_i + \\alpha - 1\\right) \\ln s - \\left(\\sum_{i=1}^N c_i + \\beta\\right) s\n$$\nDifferentiating with respect to $s$ and setting to zero:\n$$\n\\frac{d}{ds} \\ln p(s|\\mathbf{r}) = \\frac{\\sum_{i=1}^N r_i + \\alpha - 1}{s} - \\left(\\sum_{i=1}^N c_i + \\beta\\right) = 0\n$$\nSolving for $s$ yields the MAP estimator:\n$$\n\\hat{s}_{\\mathrm{MAP}} = \\frac{\\sum_{i=1}^N r_i + \\alpha - 1}{\\sum_{i=1}^N c_i + \\beta}\n$$\nThis corresponds to the mode of the posterior distribution, which is a Gamma distribution with updated shape $\\tilde{\\alpha} = \\sum r_i + \\alpha$ and rate $\\tilde{\\beta} = \\sum c_i + \\beta$.\n\n**Asymptotic Properties**\n**Fisher Information**: The Fisher information for a single trial, $I_1(s)$, is defined as $I_1(s) = -E\\left[\\frac{d^2\\ell(s)}{ds^2}\\right]$, where the expectation is over the data distribution $p(\\mathbf{r}|s)$.\nWe found $\\frac{d^2\\ell}{ds^2} = -\\frac{\\sum_{i=1}^N r_i}{s^2}$.\nThus,\n$$\nI_1(s) = -E\\left[ -\\frac{\\sum_{i=1}^N r_i}{s^2} \\right] = \\frac{1}{s^2} E\\left[\\sum_{i=1}^N r_i\\right]\n$$\nBy linearity of expectation, $E\\left[\\sum_{i=1}^N r_i\\right] = \\sum_{i=1}^N E[r_i]$. Since $r_i \\sim \\mathrm{Poisson}(c_i s)$, we have $E[r_i]=c_i s$.\n$$\nE\\left[\\sum_{i=1}^N r_i\\right] = \\sum_{i=1}^N c_i s = s \\sum_{i=1}^N c_i\n$$\nSubstituting this back into the expression for $I_1(s)$:\n$$\nI_1(s) = \\frac{1}{s^2} \\left(s \\sum_{i=1}^N c_i\\right) = \\frac{\\sum_{i=1}^N c_i}{s}\n$$\n**Large-$T$ Behavior**: For $T$ independent trials, the total Fisher information is $I_T(s) = T \\cdot I_1(s)$.\n-   **Consistency**: Both $\\hat{s}_{\\mathrm{ML}}$ and $\\hat{s}_{\\mathrm{MAP}}$ are consistent estimators. As the number of trials $T \\to \\infty$, the influence of the fixed prior in the MAP estimate becomes negligible, and both estimators converge in probability to the true value $s$.\n-   **Asymptotic Normality and Variance**: For large $T$, both estimators are asymptotically normal. Their distributions approach a Gaussian centered at $s$ with a variance that approaches the Cramér-Rao Lower Bound (CRLB), given by the inverse of the total Fisher information.\n$$\n\\text{Var}(\\hat{s}_{\\mathrm{ML}}), \\text{Var}(\\hat{s}_{\\mathrm{MAP}}) \\to \\frac{1}{I_T(s)} = \\frac{s}{T \\sum_{i=1}^N c_i} \\quad \\text{as } T \\to \\infty\n$$\n-   **Bias of $\\hat{s}_{\\mathrm{MAP}}$**: The ML estimator is unbiased for any $T$, as $E[\\hat{s}_{\\mathrm{ML}}] = E[\\frac{\\sum r_i}{\\sum c_i}] = \\frac{\\sum E[r_i]}{\\sum c_i} = \\frac{s \\sum c_i}{\\sum c_i} = s$. The MAP estimator is biased for finite $T$. Using the estimator for aggregate counts over $T$ trials, $\\hat{s}_{\\mathrm{MAP}}^{(T)} = \\frac{\\sum R_i + \\alpha - 1}{T \\sum c_i + \\beta}$:\n$$\n\\text{Bias}(\\hat{s}_{\\mathrm{MAP}}^{(T)}) = E[\\hat{s}_{\\mathrm{MAP}}^{(T)}] - s = \\frac{E[\\sum R_i] + \\alpha - 1}{T \\sum c_i + \\beta} - s\n$$\nSince $E[\\sum R_i] = \\sum E[\\sum_{t=1}^T r_i^{(t)}] = \\sum T c_i s = T s \\sum c_i$:\n$$\n\\text{Bias}(\\hat{s}_{\\mathrm{MAP}}^{(T)}) = \\frac{T s \\sum c_i + \\alpha - 1}{T \\sum c_i + \\beta} - s = \\frac{\\alpha - 1 - s\\beta}{T \\sum c_i + \\beta}\n$$\nFor large $T$, the leading-order bias is of order $O(1/T)$:\n$$\n\\text{Bias}(\\hat{s}_{\\mathrm{MAP}}^{(T)}) \\approx \\frac{\\alpha - 1 - s\\beta}{T \\sum_{i=1}^N c_i}\n$$\nThis bias term reflects the \"pull\" of the prior distribution, which diminishes as more data is collected.\n\nThe final answer requires the estimators based on a single observation $\\mathbf{r}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sum_{i=1}^N r_i}{\\sum_{i=1}^N c_i} & \\frac{\\sum_{i=1}^N r_i + \\alpha - 1}{\\sum_{i=1}^N c_i + \\beta}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While optimal decoders rely on a precise model of neural responses, real-world models are inevitably imperfect. This practice delves into the critical concept of model mismatch by challenging you to analyze a decoder that incorrectly assumes neural noise is independent when it is, in fact, correlated . Working through this problem reveals important, and perhaps surprising, insights into how specific model violations affect an estimator's bias versus its variance, a key consideration for building robust systems.",
            "id": "4002644",
            "problem": "Consider a population code for a scalar stimulus $s \\in \\mathbb{R}$ with $n$ neurons. The encoding model is additive and conditionally Gaussian: given $s$, the population response $\\mathbf{r} \\in \\mathbb{R}^{n}$ is generated by\n$$\n\\mathbf{r} = \\mathbf{a}\\, s + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\mathbf{a} \\in \\mathbb{R}^{n}$ is a known sensitivity vector and $\\boldsymbol{\\varepsilon}$ is zero-mean noise drawn from a multivariate normal distribution with true covariance $\\boldsymbol{\\Sigma}_{\\text{true}} \\in \\mathbb{R}^{n \\times n}$, which is positive definite and not necessarily diagonal (neurons may have correlated noise). A decoder is constructed under a mismatched assumption that the noise is independent across neurons, using a positive definite diagonal matrix $\\boldsymbol{\\Sigma}_{\\text{assumed}} \\in \\mathbb{R}^{n \\times n}$ to form a Gaussian likelihood.\n\nUsing Maximum Likelihood Estimation (MLE), the mismatched decoder selects $\\hat{s}$ by minimizing the assumed negative log-likelihood (up to additive constants independent of $s$) built from $\\boldsymbol{\\Sigma}_{\\text{assumed}}$ and the linear tuning model above.\n\nStarting from first principles of Gaussian likelihoods and the definition of statistical bias as $\\mathbb{E}[\\hat{s}] - s$, derive a closed-form expression for the bias of the mismatched MLE estimator $\\hat{s}$ under the true generative model (with $\\boldsymbol{\\Sigma}_{\\text{true}}$). Your derivation must explicitly track any dependence on $\\boldsymbol{\\Sigma}_{\\text{assumed}}$ and $\\boldsymbol{\\Sigma}_{\\text{true}}$.\n\nExpress your final answer as a single closed-form analytic expression. No rounding is required. No physical units are involved in this problem.",
            "solution": "The problem requires the derivation of the statistical bias of a Maximum Likelihood Estimator (MLE) for a stimulus $s$, where the decoder uses a mismatched assumption about the noise covariance structure. The bias is defined as $\\mathbb{E}[\\hat{s}] - s$, where the expectation is taken over the true data-generating distribution.\n\nFirst, we establish the procedure for obtaining the estimator $\\hat{s}$. The decoder operates under the assumption that the noise is Gaussian and independent across neurons. This corresponds to an assumed noise covariance matrix $\\boldsymbol{\\Sigma}_{\\text{assumed}}$ which is diagonal and positive definite. The assumed generative model is thus $p_{\\text{assumed}}(\\mathbf{r}|s) \\sim \\mathcal{N}(\\mathbf{a}s, \\boldsymbol{\\Sigma}_{\\text{assumed}})$.\n\nThe likelihood function under this assumption is the probability density function (PDF) of the multivariate normal distribution:\n$$\nL_{\\text{assumed}}(s; \\mathbf{r}) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\boldsymbol{\\Sigma}_{\\text{assumed}})}} \\exp\\left(-\\frac{1}{2}(\\mathbf{r} - \\mathbf{a}s)^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} (\\mathbf{r} - \\mathbf{a}s)\\right)\n$$\nThe corresponding log-likelihood is:\n$$\n\\ln L_{\\text{assumed}}(s; \\mathbf{r}) = -\\frac{1}{2}(\\mathbf{r} - \\mathbf{a}s)^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} (\\mathbf{r} - \\mathbf{a}s) - \\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(\\boldsymbol{\\Sigma}_{\\text{assumed}}))\n$$\nThe Maximum Likelihood Estimator $\\hat{s}$ is the value of $s$ that maximizes this log-likelihood. This is equivalent to minimizing the negative log-likelihood. As stated in the problem, we only need to consider terms that depend on $s$. Therefore, $\\hat{s}$ is found by minimizing the quadratic form $J(s)$:\n$$\nJ(s) = (\\mathbf{r} - \\mathbf{a}s)^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} (\\mathbf{r} - \\mathbf{a}s)\n$$\nTo find the minimum, we differentiate $J(s)$ with respect to the scalar variable $s$ and set the derivative to zero. First, we expand the quadratic form:\n$$\nJ(s) = \\mathbf{r}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} - s\\mathbf{r}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a} - s\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} + s^2 \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}\n$$\nSince the terms are scalars, $\\mathbf{r}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a} = \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r}$. Thus:\n$$\nJ(s) = \\mathbf{r}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} - 2s \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} + s^2 \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}\n$$\nDifferentiating with respect to $s$:\n$$\n\\frac{dJ(s)}{ds} = -2\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} + 2s \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}\n$$\nSetting the derivative to zero to find the estimator $\\hat{s}$:\n$$\n-2\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r} + 2\\hat{s} \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a} = 0\n$$\n$$\n\\hat{s} (\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}) = \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r}\n$$\nThe term $\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}$ is a scalar. Since $\\boldsymbol{\\Sigma}_{\\text{assumed}}$ is given as positive definite, its inverse $\\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1}$ is also positive definite. Assuming $\\mathbf{a}$ is not the zero vector (which would make the stimulus unobservable), this scalar term is strictly positive, ensuring we can divide by it. The mismatched MLE estimator is thus:\n$$\n\\hat{s} = \\frac{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r}}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}}\n$$\nNext, we compute the expectation of $\\hat{s}$ under the true generative model. The true model for the response is $\\mathbf{r} = \\mathbf{a}s + \\boldsymbol{\\varepsilon}$, where the noise $\\boldsymbol{\\varepsilon}$ is drawn from $\\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_{\\text{true}})$. Crucially, the true mean of the noise is $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$.\n\nThe expectation of the estimator $\\hat{s}$ is:\n$$\n\\mathbb{E}[\\hat{s}] = \\mathbb{E}\\left[ \\frac{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r}}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}} \\right]\n$$\nThe denominator is a constant with respect to the random variable $\\mathbf{r}$, so we can take it out of the expectation:\n$$\n\\mathbb{E}[\\hat{s}] = \\frac{1}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}} \\mathbb{E}[\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{r}]\n$$\nExpectation is a linear operator, so we can move it inside:\n$$\n\\mathbb{E}[\\hat{s}] = \\frac{1}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}} \\left( \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbb{E}[\\mathbf{r}] \\right)\n$$\nNow, we compute the expectation of the response vector $\\mathbf{r}$ under the true model:\n$$\n\\mathbb{E}[\\mathbf{r}] = \\mathbb{E}[\\mathbf{a}s + \\boldsymbol{\\varepsilon}] = \\mathbb{E}[\\mathbf{a}s] + \\mathbb{E}[\\boldsymbol{\\varepsilon}]\n$$\nSince $s$ is a fixed, non-random parameter and $\\mathbf{a}$ is a known constant vector, $\\mathbb{E}[\\mathbf{a}s] = \\mathbf{a}s$. The true noise is zero-mean, so $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$. Therefore:\n$$\n\\mathbb{E}[\\mathbf{r}] = \\mathbf{a}s\n$$\nSubstituting this result back into the expression for $\\mathbb{E}[\\hat{s}]$:\n$$\n\\mathbb{E}[\\hat{s}] = \\frac{1}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}} \\left( \\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} (\\mathbf{a}s) \\right)\n$$\nSince $s$ is a scalar, it can be factored out:\n$$\n\\mathbb{E}[\\hat{s}] = \\frac{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}}{\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}} s\n$$\nThe scalar term $\\mathbf{a}^T \\boldsymbol{\\Sigma}_{\\text{assumed}}^{-1} \\mathbf{a}$ cancels out from the numerator and denominator, yielding:\n$$\n\\mathbb{E}[\\hat{s}] = s\n$$\nThis result demonstrates that the mismatched estimator is unbiased. The bias is defined as $\\text{Bias}(\\hat{s}) = \\mathbb{E}[\\hat{s}] - s$.\n$$\n\\text{Bias}(\\hat{s}) = s - s = 0\n$$\nThe bias is zero. This outcome arises because the estimator is linear in the observations $\\mathbf{r}$, and the assumed model for the mean of the data, $\\mathbb{E}[\\mathbf{r}] = \\mathbf{a}s$, is correct. The mismatch in the covariance matrix ($\\boldsymbol{\\Sigma}_{\\text{assumed}}$ vs. $\\boldsymbol{\\Sigma}_{\\text{true}}$) affects the variance (and thus the efficiency) of the estimator, but not its bias, provided the noise is additive and zero-mean. The final result for the bias is independent of both $\\boldsymbol{\\Sigma}_{\\text{assumed}}$ and $\\boldsymbol{\\Sigma}_{\\text{true}}$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "This chapter culminates by bridging theory and practice, moving from static estimation to the challenge of real-time tracking. Many applications, such as brain-computer interfaces, require decoding dynamically evolving variables like movement intentions from ongoing neural activity . This hands-on implementation exercise tasks you with building an Iterated Extended Kalman Filter (IEKF), a powerful and widely used algorithm for decoding latent states from point-process observations like spike trains.",
            "id": "4002636",
            "problem": "Consider a latent state vector $x_t \\in \\mathbb{R}^n$ representing a low-dimensional stimulus or movement variable to be decoded from a population of $m$ neurons. Assume a linear Gaussian dynamical prior for the state, given by $x_t = A x_{t-1} + w_t$, with $w_t \\sim \\mathcal{N}(0, Q)$, and that a one-step predicted mean $x_{t|t-1}$ and covariance $P_{t|t-1}$ have already been computed using the standard time update. The observation at time $t$ is an $m$-dimensional spike count vector $y_t \\in \\mathbb{R}^m$ emitted by $m$ conditionally independent neurons in a short bin, modeled as a point-process with independent Poisson components whose means depend on the latent state through a tuning function $\\lambda(x_t) \\in \\mathbb{R}^m_{\\ge 0}$. Adopt the following widely used approximation: each Poisson observation is approximated by a Gaussian with the same mean and variance, yielding $y_t \\mid x_t \\approx \\mathcal{N}(\\lambda(x_t), R(x_t))$ with $R(x_t) = \\mathrm{diag}(\\lambda(x_t))$.\n\nStarting from the fundamental definitions above and without assuming a closed-form expression for the posterior, derive from first principles an Iterated Extended Kalman Filter (IEKF) measurement update that incorporates the state dependence of both the mean and the covariance of the observation model. Specifically, linearize the observation function around the current iterate and treat $R(x_t)$ as fixed at that iterate, and show how the IEKF update arises from minimizing a quadratic approximation to the negative log posterior.\n\nImplement the resulting IEKF measurement update for a single time step for each of the following test cases. In all cases, use the Joseph stabilized covariance update to preserve positive semidefiniteness. For numerical stability, if any component of $\\lambda(x)$ is below a specified regularization threshold $\\varepsilon$, clip it to $\\varepsilon$ when forming $R(x)$, and add the same $\\varepsilon$ to the diagonal of the innovation covariance matrix. Use a maximum number of iterations $N_{\\mathrm{max}}$ and a convergence tolerance $\\tau$ on the relative change in the state iterate, defined as $\\|x^{(i+1)} - x^{(i)}\\|_2 / \\max(1, \\|x^{(i+1)}\\|_2)$.\n\nObservation models to implement:\n- Linear tuning: $\\lambda(x) = C x$, where $C \\in \\mathbb{R}^{m \\times n}$ has nonnegative outputs for relevant $x$.\n- Exponential tuning: $\\lambda(x) = \\exp(W x + b)$ applied elementwise, with $W \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$.\n\nFor each test case, you are given $x_{t|t-1}$, $P_{t|t-1}$, the observation parameters, observation $y_t$, regularization $\\varepsilon$, $N_{\\mathrm{max}}$, and $\\tau$. Your program must compute the IEKF posterior mean $x_{t|t}$ for the single measurement update. The output must be unitless real numbers.\n\nTest suite:\n1. Linear tuning, moderate counts.\n   - $n = 2$, $m = 3$\n   - $x_{t|t-1} = [1.0, 0.5]^T$\n   - $P_{t|t-1} = \\begin{bmatrix}0.2 & 0.05 \\\\ 0.05 & 0.3\\end{bmatrix}$\n   - $C = \\begin{bmatrix}1.0 & 0.0 \\\\ 0.0 & 1.0 \\\\ 0.5 & 0.5\\end{bmatrix}$\n   - $y_t = [2.0, 1.0, 1.0]^T$\n   - $\\varepsilon = 10^{-6}$, $N_{\\mathrm{max}} = 10$, $\\tau = 10^{-9}$\n\n2. Exponential tuning, low counts.\n   - $n = 2$, $m = 3$\n   - $x_{t|t-1} = [0.2, -0.3]^T$\n   - $P_{t|t-1} = \\begin{bmatrix}0.1 & 0.0 \\\\ 0.0 & 0.1\\end{bmatrix}$\n   - $W = \\begin{bmatrix}1.0 & 0.0 \\\\ 0.5 & -1.0 \\\\ -0.5 & 0.5\\end{bmatrix}$, $b = [0.0, -0.2, 0.1]^T$\n   - $y_t = [0.0, 1.0, 0.0]^T$\n   - $\\varepsilon = 10^{-6}$, $N_{\\mathrm{max}} = 25$, $\\tau = 10^{-8}$\n\n3. Exponential tuning, near-zero rates (singularity stress test).\n   - $n = 2$, $m = 2$\n   - $x_{t|t-1} = [-3.0, -3.0]^T$\n   - $P_{t|t-1} = \\begin{bmatrix}0.5 & 0.0 \\\\ 0.0 & 0.5\\end{bmatrix}$\n   - $W = \\begin{bmatrix}1.0 & 0.0 \\\\ 0.0 & 1.0\\end{bmatrix}$, $b = [-5.0, -5.0]^T$\n   - $y_t = [0.0, 0.0]^T$\n   - $\\varepsilon = 10^{-3}$, $N_{\\mathrm{max}} = 25$, $\\tau = 10^{-8}$\n\n4. Exponential tuning, high counts.\n   - $n = 2$, $m = 3$\n   - $x_{t|t-1} = [2.0, 1.0]^T$\n   - $P_{t|t-1} = \\begin{bmatrix}1.0 & 0.2 \\\\ 0.2 & 1.5\\end{bmatrix}$\n   - $W = \\begin{bmatrix}0.8 & 0.3 \\\\ 1.2 & -0.5 \\\\ -0.7 & 1.1\\end{bmatrix}$, $b = [0.5, 0.3, 0.2]^T$\n   - $y_t = [20.0, 30.0, 25.0]^T$\n   - $\\varepsilon = 10^{-6}$, $N_{\\mathrm{max}} = 25$, $\\tau = 10^{-8}$\n\nAlgorithmic requirements:\n- Initialize the IEKF iterate at $x^{(0)} = x_{t|t-1}$.\n- At each iteration $i$, compute $\\lambda(x^{(i)})$ and its Jacobian $H^{(i)} = \\frac{\\partial \\lambda}{\\partial x}\\big|_{x^{(i)}}$.\n- Form $R^{(i)} = \\mathrm{diag}(\\max(\\lambda(x^{(i)}), \\varepsilon))$ and the linearized measurement $y_{\\mathrm{lin}}^{(i)} = y_t - \\lambda(x^{(i)}) + H^{(i)} x^{(i)}$.\n- Compute the innovation covariance $S^{(i)} = H^{(i)} P_{t|t-1} (H^{(i)})^\\top + R^{(i)} + \\varepsilon I$ and the gain $K^{(i)} = P_{t|t-1} (H^{(i)})^\\top (S^{(i)})^{-1}$.\n- Update the iterate using $x^{(i+1)} = x_{t|t-1} + K^{(i)} \\left( y_{\\mathrm{lin}}^{(i)} - H^{(i)} x_{t|t-1} \\right)$.\n- Stop when the relative change is below $\\tau$ or $i = N_{\\mathrm{max}}-1$.\n- After convergence, set $x_{t|t} = x^{(i+1)}$ and update the covariance using the Joseph form:\n  $$P_{t|t} = (I - K^{(i)} H^{(i)}) P_{t|t-1} (I - K^{(i)} H^{(i)})^\\top + K^{(i)} R^{(i)} (K^{(i)})^\\top.$$\n\nYour program should produce a single line of output containing the resulting posterior means $x_{t|t}$ for the four test cases as a comma-separated list of lists, with each real number rounded to six decimal places, in the format:\n\"[[x_{1,1},x_{1,2}], [x_{2,1},x_{2,2}], [x_{3,1},x_{3,2}], [x_{4,1},x_{4,2}]]\".",
            "solution": "The problem is valid. It presents a well-defined task in computational neuroscience, specifically the derivation and implementation of an Iterated Extended Kalman Filter (IEKF) for neural decoding. The underlying physical and mathematical models, including the Poisson firing model, its Gaussian approximation, and the Kalman filtering framework, are standard and scientifically sound. All parameters and constraints are clearly specified, forming a complete and self-contained problem.\n\nThe core task is to find the posterior distribution of a latent state $x_t \\in \\mathbb{R}^n$ given a history of observations, culminating in the current spike count vector $y_t \\in \\mathbb{R}^m$. The IEKF provides an iterative method to approximate the posterior mean and covariance. We begin from first principles, framing the problem in a Bayesian context.\n\nWe are given a Gaussian prior for the state, which is the prediction from a time-update step:\n$$\np(x_t|y_{1:t-1}) = \\mathcal{N}(x_t; x_{t|t-1}, P_{t|t-1})\n$$\nThe observation model, which relates the latent state to the observed spike counts, is a Poisson point process, approximated by a Gaussian for tractability:\n$$\np(y_t|x_t) \\approx \\mathcal{N}(y_t; \\lambda(x_t), R(x_t))\n$$\nwhere $\\lambda(x_t)$ is the vector of mean firing rates (the tuning function), and $R(x_t) = \\mathrm{diag}(\\lambda(x_t))$ is the observation noise covariance, following from the property that the variance of a Poisson distribution is equal to its mean.\n\nBy Bayes' rule, the posterior distribution is proportional to the product of the likelihood and the prior:\n$$\np(x_t|y_t, y_{1:t-1}) \\propto p(y_t|x_t) p(x_t|y_{1:t-1})\n$$\nThe IEKF seeks the mode of this posterior distribution, which is equivalent to finding the Maximum A Posteriori (MAP) estimate of $x_t$. This is achieved by minimizing the negative log-posterior, $J(x_t)$:\n$$\n\\hat{x}_{t|t} = \\arg\\max_{x_t} p(x_t|y_{1:t}) = \\arg\\min_{x_t} \\{-\\log p(y_t|x_t) - \\log p(x_t|y_{1:t-1})\\}\n$$\nSubstituting the Gaussian forms for the prior and the likelihood approximation, the cost function $J(x_t)$ becomes (up to constant terms):\n$$\nJ(x_t) = \\frac{1}{2} (x_t - x_{t|t-1})^\\top P_{t|t-1}^{-1} (x_t - x_{t|t-1}) + \\frac{1}{2} (y_t - \\lambda(x_t))^\\top R(x_t)^{-1} (y_t - \\lambda(x_t)) + \\frac{1}{2} \\log |R(x_t)|\n$$\nThis function is non-quadratic due to the nonlinearity of $\\lambda(x_t)$ and the state-dependence of $R(x_t)$. The IEKF iteratively minimizes this function. At each iteration $i$, starting with an initial guess $x^{(0)} = x_{t|t-1}$, we form a quadratic approximation of $J(x_t)$ around the current estimate $x^{(i)}$. This is done by linearizing the observation function $\\lambda(x_t)$ and fixing the covariance matrix $R(x_t)$ at their values at $x^{(i)}$.\n\nThe first-order Taylor series expansion of $\\lambda(x_t)$ around $x^{(i)}$ is:\n$$\n\\lambda(x_t) \\approx \\lambda(x^{(i)}) + H^{(i)} (x_t - x^{(i)})\n$$\nwhere $H^{(i)} = \\frac{\\partial \\lambda}{\\partial x_t}\\big|_{x_t=x^{(i)}}$ is the Jacobian matrix. We also fix a local noise covariance $R^{(i)} = R(x^{(i)})$. The term $\\frac{1}{2}\\log|R(x_t)|$ is also treated as constant with respect to the minimization variable $x_t$ within the iteration, a standard simplification in Gauss-Newton methods.\n\nSubstituting these into $J(x_t)$ yields a quadratic cost function $J_i(x_t)$:\n$$\nJ_i(x_t) \\approx \\frac{1}{2} (x_t - x_{t|t-1})^\\top P_{t|t-1}^{-1} (x_t - x_{t|t-1}) + \\frac{1}{2} (y_t - \\lambda(x^{(i)}) - H^{(i)}(x_t-x^{(i)}))^\\top (R^{(i)})^{-1} (y_t - \\lambda(x^{(i)}) - H^{(i)}(x_t-x^{(i)}))\n$$\nThe next iterate, $x^{(i+1)}$, is the value of $x_t$ that minimizes $J_i(x_t)$. We find this by setting the gradient $\\nabla_{x_t} J_i(x_t)$ to zero:\n$$\n\\nabla_{x_t} J_i(x_t) = P_{t|t-1}^{-1}(x_t - x_{t|t-1}) - (H^{(i)})^\\top(R^{(i)})^{-1}(y_t - \\lambda(x^{(i)}) - H^{(i)}(x_t-x^{(i)})) = 0\n$$\nGrouping terms in $x_t$:\n$$\n\\left(P_{t|t-1}^{-1} + (H^{(i)})^\\top(R^{(i)})^{-1}H^{(i)}\\right) x_t = P_{t|t-1}^{-1}x_{t|t-1} + (H^{(i)})^\\top(R^{(i)})^{-1}(y_t - \\lambda(x^{(i)}) + H^{(i)}x^{(i)})\n$$\nThis equation is a standard linear-Gaussian estimation problem. Its solution can be expressed using the Kalman gain formalism. The term multiplying $x_t$ on the left is the inverse of the posterior covariance in this linearized subproblem, $(P^{(i+1)})^{-1}$. Using the Woodbury matrix identity, we can express $P^{(i+1)}$ in gain form as $P^{(i+1)} = (I - K^{(i)}H^{(i)})P_{t|t-1}$, where the Kalman gain $K^{(i)}$ is:\n$$\nK^{(i)} = P_{t|t-1}(H^{(i)})^\\top (S^{(i)})^{-1}\n$$\nand the innovation covariance $S^{(i)}$ is:\n$$\nS^{(i)} = H^{(i)} P_{t|t-1} (H^{(i)})^\\top + R^{(i)}\n$$\nThe problem specifies adding a regularization term $\\varepsilon I$ to $S^{(i)}$ for numerical stability, so $S^{(i)} = H^{(i)} P_{t|t-1} (H^{(i)})^\\top + R^{(i)} + \\varepsilon I$.\n\nSolving for $x^{(i+1)}$ (the minimizer of $J_i(x_t)$) yields the update equation:\n$$\nx^{(i+1)} = x_{t|t-1} + K^{(i)} \\left( y_t - \\lambda(x^{(i)}) - H^{(i)}(x_{t|t-1} - x^{(i)}) \\right)\n$$\nThis can be rewritten as:\n$$\nx^{(i+1)} = x_{t|t-1} + K^{(i)} \\left( (y_t - \\lambda(x^{(i)}) + H^{(i)}x^{(i)}) - H^{(i)}x_{t|t-1} \\right)\n$$\nThis matches the update rule prescribed in the problem, where $y_{\\mathrm{lin}}^{(i)} = y_t - \\lambda(x^{(i)}) + H^{(i)}x^{(i)}$ is identified as a linearized measurement. The iterations continue until the relative change in the state estimate, $\\|x^{(i+1)} - x^{(i)}\\|_2 / \\max(1, \\|x^{(i+1)}\\|_2)$, falls below a tolerance $\\tau$.\n\nUpon convergence at iteration $i_{\\text{final}}$, the posterior mean is $x_{t|t} = x^{(i_{\\text{final}})}$. The posterior covariance $P_{t|t}$ is calculated using the numerically stable Joseph form with the final iterate's gain $K=K^{(i_{\\text{final}})}$, Jacobian $H=H^{(i_{\\text{final}})}$, and noise covariance $R=R^{(i_{\\text{final}})}$:\n$$\nP_{t|t} = (I - K H) P_{t|t-1} (I - K H)^\\top + K R K^\\top\n$$\nThe implementation will follow this derived procedure. The Jacobians for the two specified tuning functions are:\n1.  Linear tuning, $\\lambda(x) = C x$: The Jacobian is constant, $H = C$.\n2.  Exponential tuning, $\\lambda(x) = \\exp(W x + b)$ (element-wise): The Jacobian is state-dependent, $H(x) = \\mathrm{diag}(\\lambda(x)) W$.\n\nThe regularization of $R^{(i)}$ is implemented by clipping the values of $\\lambda(x^{(i)})$ at a minimum of $\\varepsilon$: $R^{(i)} = \\mathrm{diag}(\\max(\\lambda(x^{(i)}), \\varepsilon))$.\n\nThis completes the principled derivation of the algorithm to be implemented.\n```python\nimport numpy as np\n\ndef iekf_measurement_update(\n    tuning_type,\n    x_pred,\n    P_pred,\n    y_obs,\n    params,\n    epsilon,\n    N_max,\n    tau\n):\n    \"\"\"\n    Performs a single-step IEKF measurement update for a Poisson observation model.\n\n    Args:\n        tuning_type (str): 'linear' or 'exponential'.\n        x_pred (np.ndarray): Predicted state mean (x_{t|t-1}), shape (n,).\n        P_pred (np.ndarray): Predicted state covariance (P_{t|t-1}), shape (n, n).\n        y_obs (np.ndarray): Observation vector (y_t), shape (m,).\n        params (dict): Dictionary of parameters for the tuning function.\n        epsilon (float): Regularization parameter.\n        N_max (int): Maximum number of iterations.\n        tau (float): Convergence tolerance.\n\n    Returns:\n        np.ndarray: The posterior state mean (x_{t|t}), shape (n,).\n    \"\"\"\n    n = x_pred.shape[0]\n    m = y_obs.shape[0]\n\n    if tuning_type == 'linear':\n        C = params['C']\n        def lambda_func(x):\n            return C @ x\n        def jacobian_func(x, lambda_x):\n            return C\n    elif tuning_type == 'exponential':\n        W = params['W']\n        b = params['b']\n        def lambda_func(x):\n            return np.exp(W @ x + b)\n        def jacobian_func(x, lambda_x):\n            return np.diag(lambda_x) @ W\n    else:\n        raise ValueError(\"Invalid tuning_type\")\n\n    # Initialize iterate\n    x_i = np.copy(x_pred)\n\n    for _ in range(N_max):\n        x_prev = np.copy(x_i)\n\n        # 1. Compute lambda and Jacobian\n        lambda_x_i = lambda_func(x_i)\n        H_i = jacobian_func(x_i, lambda_x_i)\n\n        # 2. Form R_i with clipping\n        lambda_clipped = np.maximum(lambda_x_i, epsilon)\n        R_i = np.diag(lambda_clipped)\n\n        # 4. Compute innovation covariance with regularization\n        S_i = H_i @ P_pred @ H_i.T + R_i + epsilon * np.eye(m)\n        \n        # 5. Compute Kalman gain\n        # Using solve for better numerical stability than direct inversion\n        K_i = P_pred @ H_i.T @ np.linalg.inv(S_i)\n\n        # 6. Update iterate\n        innovation_residual = y_obs - lambda_x_i + H_i @ (x_i - x_pred)\n        x_i = x_pred + K_i @ innovation_residual\n\n        # Check for convergence\n        norm_diff = np.linalg.norm(x_i - x_prev)\n        norm_x = np.linalg.norm(x_i)\n        rel_change = norm_diff / max(1.0, norm_x)\n        \n        if rel_change  tau:\n            break\n            \n    return x_i\n```",
            "answer": "[[1.611174,0.923839],[0.127438,-0.043695],[-3.181823,-3.181823],[2.148154,1.386009]]"
        }
    ]
}