{
    "hands_on_practices": [
        {
            "introduction": "The concept of a tuning curve, which describes how a neuron's firing rate changes in response to a stimulus, is a cornerstone of the rate-coding paradigm. However, before we can confidently interpret the parameters of such a model—like its baseline rate, peak response, or preferred stimulus—we must ensure they are uniquely determined by the data. This practice explores the crucial concept of parameter identifiability, guiding you to establish the conditions under which the parameters of a classic Gaussian tuning curve can be successfully recovered from spike count data .",
            "id": "4003111",
            "problem": "Consider a single sensory neuron whose stimulus-dependent firing is modeled under a rate code as follows. For a scalar stimulus $x \\in \\mathbb{R}$ and observation window duration $T > 0$, the spike count $N$ in that window is generated conditionally independently across trials, with distribution $\\Pr(N=n \\mid x) = \\frac{(T r(x))^{n} e^{-T r(x)}}{n!}$ for $n \\in \\mathbb{N}$, where the mean firing rate (tuning curve) is parameterized by\n$$\nr(x) = r_{0} + \\alpha \\exp\\!\\Big(-\\frac{(x - \\mu)^{2}}{2 \\sigma^{2}}\\Big),\n$$\nwith parameters $(r_{0}, \\alpha, \\mu, \\sigma) \\in \\mathbb{R}^{4}$ subject to the constraints $r(x) > 0$ for all $x$, $\\sigma > 0$, and $T > 0$. You collect spike counts at stimulus values $\\{x_{i}\\}_{i=1}^{n}$ with $n \\geq 4$, each repeated many times, and use the conditional Poisson model to fit $(r_{0}, \\alpha, \\mu, \\sigma)$ by maximum likelihood.\n\nSelect all statements that correctly characterize identifiability conditions for $(r_{0}, \\alpha, \\mu, \\sigma)$ from such spike count data.\n\nA. If the observation window duration $T$ is unknown but constant across trials, then only the product $T r(x)$ is identifiable from spike counts; consequently, the absolute scale of $r_{0}$ and $\\alpha$ is not identifiable from counts alone.\n\nB. If the stimulus set $\\{x_{i}\\}_{i=1}^{n}$ contains at least one value $x_{\\text{far}}$ with $\\exp(-(x_{\\text{far}} - \\mu)^{2} / (2 \\sigma^{2})) \\approx 0$, at least one value $x_{\\text{near}}$ with $\\exp(-(x_{\\text{near}} - \\mu)^{2} / (2 \\sigma^{2})) \\approx 1$, and at least two additional values $x_{j}$ at distinct intermediate distances from $\\mu$ such that the corresponding $\\exp(-(x_{j} - \\mu)^{2} / (2 \\sigma^{2}))$ take distinct values in $(0, 1)$, then, under the constraints $\\sigma > 0$, $\\alpha \\neq 0$, $r_{0} > 0$, and $r_{0} + \\alpha > 0$, the parameter vector $(r_{0}, \\alpha, \\mu, \\sigma)$ is structurally identifiable from the mean rate function $r(x)$ and hence from spike counts under the Poisson model.\n\nC. Because $r(x)$ is symmetric in $x$ around $\\mu$, spike counts cannot identify $\\mu$ itself; only $| \\mu |$ can be recovered.\n\nD. If $\\alpha = 0$, then $\\mu$ and $\\sigma$ are not identifiable from spike counts regardless of stimulus design, but $r_{0}$ is identifiable provided $T$ is known.\n\nE. Identifiability of $\\sigma$ requires spike timing data (millisecond-resolution interspike intervals); spike counts are insufficient.\n\nF. If all presented stimuli satisfy $|x_{i} - \\mu| \\gg \\sigma$ so that $\\exp(-(x_{i} - \\mu)^{2} / (2 \\sigma^{2})) \\approx 0$ for all $i$, then $(\\alpha, \\mu, \\sigma)$ are not identifiable from the data and only $r_{0}$ is identifiable (assuming known $T$).",
            "solution": "The problem is about parameter identifiability in a standard neural encoding model. For a parameter to be identifiable, it must be possible to uniquely determine its value from the observed data. In this Poisson model, the data (spike counts) allow us to estimate the mean spike count for each stimulus $x$, which is $\\lambda(x) = T r(x)$. Identifiability of the parameters $(r_0, \\alpha, \\mu, \\sigma)$ depends on whether they can be uniquely recovered from knowledge of the function $\\lambda(x)$.\n\n*   **A. Correct.** The observed data allows estimation of the function $\\lambda(x) = T r(x) = T r_0 + T \\alpha \\exp(\\dots)$. If $T$ is unknown, we can identify the parameters $r'_0 = T r_0$ and $\\alpha' = T \\alpha$, but we cannot separate $T$ from $r_0$ and $\\alpha$. Any combination of $(r_0/c, \\alpha/c)$ with an observation time $cT$ would produce the same data distribution. Thus, the absolute scale of the rate parameters $r_0$ and $\\alpha$ is not identifiable.\n\n*   **B. Correct.** This statement addresses two levels of identifiability. First, the model is structurally identifiable: the four parameters correspond to unique geometric features of the function $r(x)$ (baseline, peak amplitude, peak location, and width), so they are uniquely determined if the entire function $r(x)$ is known. Second, it describes a practical condition for identifiability: sampling the function at four or more well-chosen points (e.g., far from the peak, at the peak, and at intermediate points) provides enough information to constrain and solve for the four parameters.\n\n*   **C. Incorrect.** The symmetry of $r(x)$ is around the point $x=\\mu$. Since the stimulus value $x$ is known to the experimenter, the value of $x$ that elicits the maximum firing rate is the estimate for $\\mu$. The identity of the peak stimulus is what identifies $\\mu$, not its absolute value. For instance, a peak at $x=10$ is distinct from a peak at $x=-10$.\n\n*   **D. Correct.** If $\\alpha = 0$, the model simplifies to $r(x) = r_0$. The firing rate becomes constant and no longer depends on the stimulus $x$. The parameters $\\mu$ and $\\sigma$, which describe the shape of the stimulus-dependent Gaussian part, disappear from the model and are therefore fundamentally non-identifiable. The mean spike count becomes $T r_0$, from which $r_0$ can be identified if $T$ is known.\n\n*   **E. Incorrect.** The parameter $\\sigma$ determines the width of the *rate* tuning curve $r(x)$. Its value affects how the mean spike count changes as a function of the stimulus. By measuring spike counts at various stimuli around the peak, one can map out the shape of $r(x)$ and estimate $\\sigma$. This is a rate-based measurement and does not require millisecond-resolution spike timing.\n\n*   **F. Correct.** This describes a poor experimental design where the neuron is only probed in its tail region, far from its preferred stimulus $\\mu$. In this regime, the exponential term is approximately zero for all stimuli, so $r(x_i) \\approx r_0$. The data will show a nearly constant firing rate, providing information only about the baseline $r_0$ but not about the properties of the tuning curve's peak ($\\alpha, \\mu, \\sigma$).",
            "answer": "$$\\boxed{ABDF}$$"
        },
        {
            "introduction": "Neurons can employ diverse strategies to encode information, raising the fundamental question of which strategy is more efficient. To move beyond qualitative descriptions, we can use the rigorous framework of information theory to formally compare the performance of different neural codes. This practice challenges you to derive and contrast the mutual information for a latency-based temporal code and a count-based rate code, providing a quantitative lens through which to understand how factors like timing jitter and observation time limit information transmission in each scheme .",
            "id": "4003129",
            "problem": "Consider a sensory neuron that encodes a scalar stimulus $s \\in \\mathbb{R}$ by either a latency-based temporal code or a count-based rate code. The stimulus is random with a Gaussian prior $s \\sim \\mathcal{N}(0, v_{s})$. In the latency code, the neuron reliably emits a first spike per trial, whose latency $t$ is modeled as $t = t_{0} + a s + \\varepsilon$, where $t_{0}$ is a constant baseline latency, $a \\neq 0$ is a fixed sensitivity parameter, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ represents additive timing jitter with variance $\\sigma^{2}$. In the rate code, the neuron emits spikes within a fixed observation window of duration $T > 0$ according to a homogeneous Poisson process with stimulus-dependent rate $\\lambda(s) = \\bar{\\lambda} + k s$, where $k$ is a fixed sensitivity parameter, and $\\bar{\\lambda} > 0$ is chosen such that the matched mean spike count per trial is $M = \\bar{\\lambda} T$ (assume $M \\geq 10$ to justify a normal approximation to the Poisson). Let the observed count be $N$ and the empirical rate be $Y_{R} = N/T$.\n\nUsing foundational principles from information theory (Shannon mutual information for random variables) and the Gaussian approximation for the rate-code observation $Y_{R}$ under the specified conditions, derive the mutual information for each coding paradigm in nats and, by subtraction, obtain an analytic expression for the difference in mutual information\n$$\\Delta I(\\sigma^{2}) = I_{\\text{latency}} - I_{\\text{rate}}$$\nas a function of the jitter variance $\\sigma^{2}$, the prior variance $v_{s}$, the sensitivity parameters $a$ and $k$, the observation window $T$, and the matched mean spike count $M$. Express your final answer in nats as a single closed-form symbolic expression. No numerical rounding is required.",
            "solution": "We begin from the definition of Shannon mutual information for continuous random variables, expressed in nats:\n$$\nI(X;Y) = h(Y) - h(Y \\mid X),\n$$\nwhere $h(\\cdot)$ denotes differential entropy. For the latency code, the observation is $t = t_{0} + a s + \\varepsilon$ with $s \\sim \\mathcal{N}(0, v_{s})$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$, independent of $s$. For the rate code, the observation is the empirical rate $Y_{R} = N/T$ where $N$ is Poisson with mean $\\lambda(s) T = (\\bar{\\lambda} + k s) T$. Under the matched mean spike count $M = \\bar{\\lambda} T$ and the assumption $M \\geq 10$, we approximate the Poisson by a normal distribution and linearize the dependence of the variance on $s$ by replacing it with its mean value, which yields a Gaussian observation model.\n\nLatency code. Since $t = t_{0} + a s + \\varepsilon$, the additive constant $t_{0}$ does not affect differential entropy or mutual information. The effective observation is $Y_{L} = a s + \\varepsilon$, where $s \\sim \\mathcal{N}(0, v_{s})$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$; $s$ and $\\varepsilon$ are independent. Then $Y_{L}$ is Gaussian with variance $a^{2} v_{s} + \\sigma^{2}$. Using the entropy formula for a Gaussian $X \\sim \\mathcal{N}(0, \\nu)$, $h(X) = \\frac{1}{2} \\ln(2 \\pi e \\nu)$, we have\n$$\nh(Y_{L}) = \\frac{1}{2} \\ln\\!\\big( 2 \\pi e (a^{2} v_{s} + \\sigma^{2}) \\big),\n\\qquad\nh(Y_{L} \\mid s) = h(\\varepsilon) = \\frac{1}{2} \\ln\\!\\big( 2 \\pi e \\sigma^{2} \\big).\n$$\nTherefore,\n$$\nI_{\\text{latency}} = h(Y_{L}) - h(Y_{L} \\mid s)\n= \\frac{1}{2} \\ln\\!\\left( \\frac{a^{2} v_{s} + \\sigma^{2}}{\\sigma^{2}} \\right)\n= \\frac{1}{2} \\ln\\!\\left( 1 + \\frac{a^{2} v_{s}}{\\sigma^{2}} \\right).\n$$\n\nRate code. The spike count $N \\mid s \\sim \\text{Poisson}((\\bar{\\lambda} + k s) T)$. For $M = \\bar{\\lambda} T \\geq 10$, we approximate $N \\mid s$ as Gaussian with mean $(\\bar{\\lambda} + k s) T$ and variance $(\\bar{\\lambda} + k s) T$. The empirical rate $Y_{R} = N/T$ then satisfies\n$$\nY_{R} \\mid s \\approx \\mathcal{N}\\!\\left( \\bar{\\lambda} + k s, \\frac{\\bar{\\lambda} + k s}{T} \\right).\n$$\nBecause the noise variance depends on $s$, an exact mutual information expression is intractable; however, for small fluctuations of $s$ around its mean and to obtain a tractable approximation, we replace the variance by its mean value across stimuli, yielding\n$$\nY_{R} \\mid s \\approx \\mathcal{N}\\!\\left( \\bar{\\lambda} + k s, \\frac{\\bar{\\lambda}}{T} \\right).\n$$\nDefine the mean-subtracted observation $Z_{R} = Y_{R} - \\bar{\\lambda}$, so\n$$\nZ_{R} = k s + \\eta,\n\\quad \\text{with} \\quad \\eta \\sim \\mathcal{N}\\!\\left( 0, \\frac{\\bar{\\lambda}}{T} \\right),\n\\quad s \\sim \\mathcal{N}(0, v_{s}),\n\\quad \\eta \\perp s.\n$$\nThus $Z_{R}$ is Gaussian with variance $k^{2} v_{s} + \\frac{\\bar{\\lambda}}{T}$. Using the Gaussian entropy formula,\n$$\nh(Z_{R}) = \\frac{1}{2} \\ln\\!\\left( 2 \\pi e \\left( k^{2} v_{s} + \\frac{\\bar{\\lambda}}{T} \\right) \\right),\n\\qquad\nh(Z_{R} \\mid s) = h(\\eta) = \\frac{1}{2} \\ln\\!\\left( 2 \\pi e \\frac{\\bar{\\lambda}}{T} \\right).\n$$\nTherefore,\n$$\nI_{\\text{rate}} = h(Z_{R}) - h(Z_{R} \\mid s)\n= \\frac{1}{2} \\ln\\!\\left( \\frac{k^{2} v_{s} + \\frac{\\bar{\\lambda}}{T}}{\\frac{\\bar{\\lambda}}{T}} \\right)\n= \\frac{1}{2} \\ln\\!\\left( 1 + \\frac{k^{2} v_{s} T}{\\bar{\\lambda}} \\right).\n$$\nUnder the matched mean spike count condition $M = \\bar{\\lambda} T$, we have $\\bar{\\lambda} = \\frac{M}{T}$, which yields\n$$\nI_{\\text{rate}} = \\frac{1}{2} \\ln\\!\\left( 1 + \\frac{k^{2} v_{s} T}{M/T} \\right)\n= \\frac{1}{2} \\ln\\!\\left( 1 + \\frac{k^{2} v_{s} T^{2}}{M} \\right).\n$$\n\nDifference in mutual information. Subtracting the rate-code mutual information from the latency-code mutual information,\n$$\n\\Delta I(\\sigma^{2}) = I_{\\text{latency}} - I_{\\text{rate}}\n= \\frac{1}{2} \\ln\\!\\left( 1 + \\frac{a^{2} v_{s}}{\\sigma^{2}} \\right)\n- \\frac{1}{2} \\ln\\!\\left( 1 + \\frac{k^{2} v_{s} T^{2}}{M} \\right).\n$$\nCombining the logarithms,\n$$\n\\Delta I(\\sigma^{2}) = \\frac{1}{2} \\ln\\!\\left( \\frac{1 + \\frac{a^{2} v_{s}}{\\sigma^{2}}}{1 + \\frac{k^{2} v_{s} T^{2}}{M}} \\right).\n$$\nThis expression is in nats and explicitly shows how the timing jitter variance $\\sigma^{2}$ degrades the latency-code mutual information relative to the rate-code mutual information under matched mean spikes $M = \\bar{\\lambda} T$ and the Gaussian approximation for the count-based observation.",
            "answer": "$$\\boxed{\\frac{1}{2}\\,\\ln\\!\\left(\\frac{1+\\frac{a^{2}v_{s}}{\\sigma^{2}}}{1+\\frac{k^{2}v_{s}T^{2}}{M}}\\right)}$$"
        },
        {
            "introduction": "Distinguishing between rate and temporal coding hypotheses is a central challenge in experimental neuroscience, bridging the gap between theoretical models and real-world data. A neuron's spike train might appear to use a simple rate code when, in fact, fine temporal features are being missed due to inadequate analysis methods. This exercise puts you in the role of an experimentalist, tasked with deriving the necessary criteria for analysis bin width ($\\Delta t$) and total recording duration ($T$) to reliably resolve and statistically validate the existence of a temporal code hidden within a neural response .",
            "id": "4003087",
            "problem": "A single neuron is recorded under repeated presentations of an identical stimulus. Each presentation defines a peristimulus window of duration $W = 100\\,\\mathrm{ms}$, and the total recording time is $T = N W$, where $N$ is the number of stimulus repetitions. The neuron's spike train is modeled as a conditionally inhomogeneous Poisson process with instantaneous rate $r(t)$ under both hypotheses considered. Under the rate-code hypothesis, $r(t)$ varies only slowly within $W$ so that spike count over coarse windows captures the information; under the temporal-code hypothesis, $r(t)$ contains narrow stimulus-locked features (peaks or troughs) with timing precision characterized by a standard deviation $\\sigma$ around the feature center. The expected average firing rate across the window is $r \\in [10,40]\\,\\mathrm{Hz}$, and the minimal fractional modulation of the narrow temporal feature relative to baseline is $m \\approx 0.3$.\n\nYou will analyze the Peri-Stimulus Time Histogram (PSTH), which is built by binning spike times with bin width $\\Delta t$ and aggregating counts across $N$ repetitions. Use the following fundamental bases: (i) Poisson counting statistics where the variance of counts equals the mean, (ii) the Central Limit Theorem for approximating count fluctuations by Gaussian noise at large counts, and (iii) the Nyquist-Shannon sampling theorem, which requires sampling at least twice the highest frequency to resolve a feature of duration scale. The target is to select $\\Delta t$ and $T$ so that the narrow temporal features are both resolvable and statistically detectable at a $z$-score threshold $z = 3$, while coarse-rate differences would also be detectable if present.\n\nWhich option provides a sufficient criterion on $\\Delta t$ and $T$ to reliably discriminate between a rate code and a temporal code in this setting?\n\nA. Choose $\\Delta t \\le \\sigma/2$ and ensure\n$$\nT \\ge \\frac{z^2}{m^2}\\,\\frac{W}{r\\,\\Delta t}\\,.\n$$\n\nB. Choose $\\Delta t = 1/r$ and ensure\n$$\nT \\ge \\frac{z^2}{m^2}\\,r\\,\\Delta t\\,W\\,.\n$$\n\nC. Choose $\\Delta t \\ge \\sigma$ and ensure\n$$\nT \\ge \\frac{z^2}{m^2}\\,\\frac{1}{r}\\,.\n$$\n\nD. Choose $\\Delta t \\le \\sigma/2$ and ensure\n$$\nT \\ge \\frac{z^2}{m^2}\\,\\frac{1}{r}\\,.\n$$",
            "solution": "To discriminate between a rate and a temporal code, we need to choose an analysis bin width $\\Delta t$ that can resolve the temporal feature and a total recording time $T$ that provides enough statistical power to detect it.\n\n**1. Condition on Bin Width $\\Delta t$ (Resolvability)**\nThe temporal code has features with a characteristic time scale (standard deviation) of $\\sigma$. To resolve such a feature in a histogram, the bin width $\\Delta t$ must be smaller than the feature itself. The problem invokes the Nyquist-Shannon sampling theorem as an analogy. A feature of duration $\\sigma$ has significant frequency content up to roughly $1/\\sigma$. To capture this, we must \"sample\" at a rate of at least twice this frequency. The sampling interval is $\\Delta t$, so $1/\\Delta t \\ge 2/\\sigma$, which implies $\\Delta t \\le \\sigma/2$. This ensures the temporal feature is not averaged away by overly wide bins.\n\n**2. Condition on Total Time $T$ (Detectability)**\nWe need to detect the rate modulation $m$ with a z-score of at least $z$. Let's calculate the signal-to-noise ratio (SNR) for the PSTH.\n*   **Signal:** The signal is the difference in expected spike counts between a feature bin and a baseline bin. The baseline rate is $r$, and the feature rate is $r(1+m)$. Over $N$ trials, the expected counts in a bin of width $\\Delta t$ are $\\mu_{base} = N r \\Delta t$ and $\\mu_{feat} = N r (1+m) \\Delta t$. The signal is the excess count: $S = \\mu_{feat} - \\mu_{base} = N r m \\Delta t$.\n*   **Noise:** For a Poisson process, the variance of the count equals the mean. The statistical noise is the standard deviation of the baseline count: $\\sigma_{noise} = \\sqrt{\\mu_{base}} = \\sqrt{N r \\Delta t}$.\n*   **Z-score:** The z-score is the SNR:\n    $$ z = \\frac{S}{\\sigma_{noise}} = \\frac{N r m \\Delta t}{\\sqrt{N r \\Delta t}} = m \\sqrt{N r \\Delta t} $$\nWe require $z$ to meet a threshold (given as $z=3$ in the problem, but we use the variable $z$ for generality). So, $m \\sqrt{N r \\Delta t} \\ge z$. Squaring and solving for the number of trials $N$:\n$$ N \\ge \\frac{z^2}{m^2 r \\Delta t} $$\nThe total recording time is $T = N W$. Substituting $N=T/W$:\n$$ \\frac{T}{W} \\ge \\frac{z^2}{m^2 r \\Delta t} \\implies T \\ge \\frac{z^2}{m^2} \\frac{W}{r \\Delta t} $$\n\n**Conclusion**\nThe sufficient criterion must satisfy both conditions:\n1.  Resolvability: $\\Delta t \\le \\sigma/2$\n2.  Detectability: $T \\ge \\frac{z^2}{m^2} \\frac{W}{r \\Delta t}$\n\nComparing this with the options:\n*   **A:** Correctly states both the resolvability condition for $\\Delta t$ and the derived detectability condition for $T$.\n*   **B:** The choice of $\\Delta t=1/r$ is generally too large to resolve fine temporal features. The formula for $T$ is incorrect.\n*   **C:** The choice of $\\Delta t \\ge \\sigma$ explicitly violates the resolvability condition. The formula for $T$ is incorrect.\n*   **D:** The condition on $\\Delta t$ is correct, but the formula for $T$ is incorrect as it lacks the necessary dependence on $\\Delta t$ and $W$.\n\nTherefore, option A provides the correct and sufficient criterion.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}