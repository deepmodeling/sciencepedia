## 引言
在[计算神经科学](@entry_id:274500)领域，理解神经元如何将复杂的感觉输入转化为一系列离散的脉冲，是解码大脑信息处理机制的核心挑战。线性-[非线性](@entry_id:637147)（LN）级联模型为此提供了一个强大而简洁的框架，它将这一看似神秘的过程分解为一系列在数学上易于处理且在生物学上具有解释力的步骤。该模型不仅是描述神经元反应的基础工具，也构成了更复杂、更具生物真实性的模型（如[广义线性模型](@entry_id:900434)）的基石。本文旨在为读者提供一个关于LN模型的全面而深入的概述，从其基本原理到前沿应用。

在接下来的内容中，我们将分三部分展开：首先，在“原理与机制”一章中，我们将详细剖析LN模型的三个核心组成部分——线性滤波、[非线性变换](@entry_id:636115)和[脉冲生成](@entry_id:1132149)，并探讨其数学基础、参数估计方法以及理论上的考量。接着，在“应用与跨学科连接”一章中，我们将展示LN模型如何在[感觉神经科学](@entry_id:165847)、生物物理学和[统计建模](@entry_id:272466)中发挥关键作用，并探讨其与[高效编码](@entry_id:1124203)等宏观理论及现代机器学习方法的联系。最后，在“动手实践”部分，我们将通过一系列精选的编程练习，引导读者将理论知识付诸实践，学习如何实现、拟合和验证这些模型。通过本文的学习，您将掌握一个分析神经数据的核心计算工具，并对[神经编码](@entry_id:263658)的定量研究建立起深刻的理解。

## 原理与机制

在线性-[非线性](@entry_id:637147)（Linear-Nonlinear, LN）级联模型中，我们将神经元对刺激的编码过程分解为一个概念上清晰且数学上易于处理的三级串联结构。这个框架假设神经元执行的计算可以近似为：首先对输入刺激进行线性滤波，然后对滤波结果进行静态[非线性变换](@entry_id:636115)，最后根据这个变换结果生成一个随机的[脉冲序列](@entry_id:1132157)。本章将深入探讨构成这一模型的各个组件的原理、它们的数学表述，以及如何将这些组件组合起来构建一个完整且可估计的[神经编码](@entry_id:263658)模型。

### 线性-[非线性](@entry_id:637147)（LN）模型的核心架构

一个标准的LN级联模型包含三个核心阶段：线性滤波、静态[非线性变换](@entry_id:636115)和随机[脉冲生成](@entry_id:1132149)。我们将依次详细阐述每个阶段。

#### 线性阶段（L）：作为线性滤波器的[感受野](@entry_id:636171)

模型的第一阶段假设神经元通过一个[线性滤波器](@entry_id:1127279) $k(t)$ 来整合感觉输入 $s(t)$。这个滤波器在功能上扮演了神经元**感受野（receptive field）**的角色，它定义了神经元对刺激的哪些时空特征敏感。滤波器的输出，我们称之为**生成信号（generator signal）** $u(t)$，代表了在时间 $t$ 神经元所“看到”的有效刺激强度。

从数学上讲，如果一个系统是**[线性时不变](@entry_id:276287)（Linear Time-Invariant, LTI）**的，那么其输出可以表示为输入与系统**[脉冲响应函数](@entry_id:1126431)（impulse response function）**的卷积。这为使用卷积来描述神经元的线性整合阶段提供了坚实的理论基础。我们可以从两个角度来理解这一点 ：

1.  **时域视角**：任何输入信号 $s(t)$ 都可以被看作是无穷多个加权和[移位](@entry_id:145848)的狄拉克 $\delta$ 函数的叠加，即 $s(t) = \int_{-\infty}^{\infty} s(\tau)\delta(t-\tau) d\tau$。由于系统是线性的，对 $s(t)$ 的响应等于对每个单独的 $\delta(t-\tau)$ 函数响应的叠加。系统的**脉冲响应** $k(t)$ 定义为系统对在时间 $0$ 施加的[单位脉冲](@entry_id:272155) $\delta(t)$ 的响应。由于系统是时不变的，对[移位脉冲](@entry_id:265965) $\delta(t-\tau)$ 的响应就是移位后的脉冲响应 $k(t-\tau)$。因此，总输出 $u(t)$ 就是这些响应的加权积分：
    $$
    u(t) = \int_{-\infty}^{\infty} s(\tau) k(t-\tau) d\tau = (k * s)(t)
    $$
    这正是卷积的定义。

2.  **频域视角**：对于[LTI系统](@entry_id:271946)，[复指数函数](@entry_id:169796) $e^{i\omega t}$ 是一类特殊的输入，称为**[特征函数](@entry_id:186820)（eigenfunctions）**。当输入为 $e^{i\omega t}$ 时，输出必然是同一频率的[复指数函数](@entry_id:169796)乘以一个复数因子 $H(\omega)$，即 $H(\omega)e^{i\omega t}$。这个因子 $H(\omega)$ 被称为系统的**传递函数（transfer function）**。任何信号 $s(t)$ 都可以通过傅里叶变换分解为这些[复指数](@entry_id:162635)[特征函数](@entry_id:186820)的[线性组合](@entry_id:154743)。利用系统的线性特性，我们可以得到输出信号的傅里叶变换 $U(\omega)$ 等于输入信号的傅里叶变换 $S(\omega)$ 与传递函数 $H(\omega)$ 的乘积：$U(\omega) = H(\omega)S(\omega)$。根据[卷积定理](@entry_id:264711)，频域中的乘积对应于时域中的卷积。因此，$u(t)$ 是 $s(t)$ 与 $H(\omega)$ 的[逆傅里叶变换](@entry_id:178300)（即脉冲响应 $k(t)$）的卷积。

对于一个描述[神经编码](@entry_id:263658)的模型，一个关键的约束是**因果性（causality）**：神经元的响应不能发生在刺激出现之前。这要求[脉冲响应函数](@entry_id:1126431) $k(t)$ 对于所有负时间值都必须为零，即 $k(t) = 0$ for $t \lt 0$。因此，[卷积积分](@entry_id:155865)的下限可以从 $-\infty$ 变为 $0$（如果以 $\tau$ 为积分变量，则上限为 $t$），确保输出 $u(t)$ 只依赖于过去和现在的刺激值 。

#### [非线性](@entry_id:637147)阶段（N）：从特征到发放率的转换

[线性滤波器](@entry_id:1127279)的输出 $u(t)$ 是一个实数值，可正可负。然而，神经元的瞬时发放率（即单位时间内的脉冲期望数）必须是一个非负数。因此，模型的第二个阶段引入一个**静态[非线性](@entry_id:637147)函数（static nonlinearity）** $f(\cdot)$，它将生成信号 $u(t)$ 映射到一个非负的瞬时条件强度或发放率 $\lambda(t)$：
$$
\lambda(t) = f(u(t))
$$
"静态"意味着这个函数是瞬时的，没有记忆；$\lambda(t)$ 的值仅依赖于同一时刻 $u(t)$ 的值。这个[非线性](@entry_id:637147)函数捕捉了神经元将整合后的输入转化为脉冲输出的内在计算特性，例如阈值效应、饱和效应和增益控制。

在实践中，有几种常用的[非线性](@entry_id:637147)函数，它们各自具有不同的数学特性和生物学解释 ：

*   **整流线性函数（Rectified Linear Unit, ReLU）**：$f(u) = \max(0, u)$。这种形式简单地将所有负的输入置零，实现了一个硬阈值。它能有效确保发放率的非负性，但在 $u=0$ 处不可导，这给基于梯度的[模型优化](@entry_id:637432)方法带来了挑战。

*   **逻辑函数（Logistic/Sigmoid Function）**：例如 $f(u) = \frac{L}{1 + \exp(-k(u-u_0))}$。这是一个平滑的S形函数，其输出被限制在一个有限范围内（例如，[标准逻辑](@entry_id:178384)函数的范围是 $(0, 1)$）。虽然平滑性在优化中是有利的，但其有界的特性可能对发放率模型构成一个不必要的强假设，因为它预设了一个神经元的最大发放率。逻辑函数更常用于描述二元事件（如在小时间窗内是否有脉冲）的伯努利模型。

*   **指数函数（Exponential Function）**：$f(u) = \exp(u)$。这个函数是平滑的，并且将整个[实数域](@entry_id:151347)映射到正[实数域](@entry_id:151347) $(0, \infty)$，自然地满足了发放率的非负性，且没有上界。指数函数在**[广义线性模型](@entry_id:900434)（Generalized Linear Models, GLM）**框架中扮演着核心角色。对于泊松（Poisson）分布的观测数据（如脉冲计数），[指数函数](@entry_id:161417)是其**规范链接函数（canonical link function）**的逆。使用规范链接函数的一个巨大优势是，它能保证模型的[对数似然函数](@entry_id:168593)是[凹函数](@entry_id:274100)，这意味着存在唯一的全局最优解，从而极大地简化了模型的参数估计过程  。

#### [脉冲生成](@entry_id:1132149)阶段（P）：随机输出

模型的最后一个阶段是根据瞬时发放率 $\lambda(t)$ 生成随机的[脉冲序列](@entry_id:1132157)。最简单和最常见的假设是，脉冲的产生遵循一个**[非齐次泊松过程](@entry_id:1128851)（inhomogeneous Poisson process）**。

在这个过程中，$\lambda(t)$ 被解释为**[条件强度函数](@entry_id:1122850)（conditional intensity function）**或**风险函数（hazard function）**。它的直观含义是，在给定历史信息（在纯LN模型中，即给定刺激历史）的条件下，在一个极小的时间间隔 $[t, t+\Delta t)$ 内观察到一个脉冲的概率近似等于 $\lambda(t)\Delta t$。
$$
\mathbb{P}\{\text{在 } [t, t+\Delta t) \text{ 内有脉冲} \mid \text{历史}\} \approx \lambda(t)\Delta t
$$
这个泊松假设意味着，在任何给定时刻，脉冲的产生是无记忆的（仅依赖于当前的 $\lambda(t)$），并且脉冲之间是条件独立的。因此，这个完整的模型常被称为**线性-[非线性](@entry_id:637147)-泊松（Linear-Nonlinear-Poisson, LNP）**模型。它提供了一个基准，用于捕捉神经元反应中围绕由刺激驱动的平均发放率的变异性 。

### 数学基础：从连续到离散

尽管LN模型在理论上通常用连续时间函数来表述，但在应用于真实神经数据时，我们处理的几乎总是离散时间序列。将[连续模](@entry_id:158807)型转化为离散形式需要仔细考虑，以避免引入系统性误差 。

假设我们在时间间隔为 $\Delta$ 的离散时间点上对连续刺激 $s(t)$ 进行采样，得到序列 $x_t = s(t\Delta)$。[连续卷积](@entry_id:173896) $u(t) = (k * s)(t)$ 相应地由[离散卷积](@entry_id:160939) $(\mathbf{k} * \mathbf{x})_t = \sum_{\tau} k_\tau x_{t-\tau}$ 来近似。这种近似的有效性取决于几个关键因素：

*   **[混叠](@entry_id:146322)（Aliasing）**：根据**[奈奎斯特-香农采样定理](@entry_id:262499)**，如果一个连续信号的频率成分被限制在某个最大频率 $B$ 以下（即信号是**带限的**），并且[采样频率](@entry_id:264884)高于 $2B$（即采样间隔 $\Delta  1/(2B)$），那么原始信号可以从其采样点中完美重建。然而，如果原始刺激 $s(t)$ 包含高于[奈奎斯特频率](@entry_id:276417) $1/(2\Delta)$ 的频率成分，并且在采样前没有进行适当的**[抗混叠](@entry_id:636139)滤波**（即低通滤波），那么这些高频成分将在采样过程中“折叠”到低频范围，导致**[混叠](@entry_id:146322)**现象。此时，离散序列 $\mathbf{x}$ 将是对原始信号的失真表示，基于它计算的[离散卷积](@entry_id:160939)结果也将是错误的 。

*   **[量化误差](@entry_id:196306)（Quantization Error）**：当输入本身是[点过程](@entry_id:1129862)时，例如来自另一神经元的[脉冲序列](@entry_id:1132157) $x_{\text{spike}}(t) = \sum_i \delta(t - t_i)$，离散化会引入另一种误差。在这种情况下，[连续卷积](@entry_id:173896)的结果是滤波器在每个脉冲时刻的叠加：$u(t) = \sum_i k(t - t_i)$。在离散时间中，我们通常将脉冲时间“分箱”，即将在时间窗 $[t\Delta, (t+1)\Delta)$ 内发生的所有脉冲计为在离散时间点 $t$ 发生。这个过程丢失了脉冲在时间窗内的精确时间信息，从而导致**量化误差**。[离散卷积](@entry_id:160939)将使用[分箱](@entry_id:264748)后的时间，而不是真实的脉冲时间，其结果是对真实[连续卷积](@entry_id:173896)的近似。减小时间窗宽度 $\Delta$ 可以降低这种误差，但只要 $\Delta \gt 0$，误差就无法完全消除 。

因此，虽然离散时间模型在计算上是必要的，但研究者必须清楚地认识到其作为连续过程近似的局限性，并根据刺激和数据的特性选择合适的采样率和离散化方案。

### 模型拟合：[最大似然估计](@entry_id:142509)

确定了LN模型的结构后，接下来的任务是从实验记录的刺激 $(\mathbf{x}_t)$ 和脉冲响应 $(y_t)$ 数据中估计模型的参数，主要是线性滤波器 $\mathbf{k}$（在离散形式下是一个向量）。最常用和最强大的方法是**最大似然估计（Maximum Likelihood Estimation, MLE）**。

#### 对数似然函数

最大似然法的目标是找到一组参数 $\mathbf{k}$，使得在该参数下观测到已知数据集的概率最大。我们首先构建**[似然函数](@entry_id:921601)（likelihood function）** $L(\mathbf{k}) = P(\{y_t\} | \{\mathbf{x}_t\}; \mathbf{k})$。

假设在离散时间窗 $\Delta$ 内的脉冲计数 $y_t$ 遵循泊松分布，其均值 $\mu_t$ 等于瞬时发放率 $\lambda_t$ 乘以时间窗宽度 $\Delta$。而 $\lambda_t$ 由LN模型给出：$\lambda_t = f(\mathbf{k}^\top \mathbf{x}_t)$。因此，$\mu_t = \Delta f(\mathbf{k}^\top \mathbf{x}_t)$。单个时间点 $t$ 的泊松[概率质量函数](@entry_id:265484)为：
$$
P(y_t | \mathbf{x}_t; \mathbf{k}) = \frac{(\mu_t)^{y_t} \exp(-\mu_t)}{y_t!} = \frac{(\Delta f(\mathbf{k}^\top \mathbf{x}_t))^{y_t} \exp(-\Delta f(\mathbf{k}^\top \mathbf{x}_t))}{y_t!}
$$
假设不同时间仓的脉冲计数在给定刺激下是相互独立的，那么整个数据集的联合概率（即[似然函数](@entry_id:921601)）就是每个时间点概率的乘积。为了计算方便，我们通常最大化**对数似然函数（log-likelihood function）** $\mathcal{L}(\mathbf{k}) = \ln L(\mathbf{k})$，因为对数将乘积转化为求和。经过化简并忽略与 $\mathbf{k}$ 无关的常数项（如 $\ln(y_t!)$），我们得到对数似然函数的核心部分 ：
$$
\mathcal{L}(\mathbf{k}) \propto \sum_{t=1}^{T} \left[ y_t \ln(f(\mathbf{k}^\top \mathbf{x}_t)) - \Delta f(\mathbf{k}^\top \mathbf{x}_t) \right]
$$

#### 基于梯度的优化

对数似然函数通常没有解析形式的最大值解，需要使用[数值优化](@entry_id:138060)算法（如梯度上升法）来寻找最优的 $\mathbf{k}$。这些算法的核心是计算[对数似然函数](@entry_id:168593)相对于参数的**梯度（gradient）**和（对于二阶方法）**[海森矩阵](@entry_id:139140)（Hessian matrix）**。

利用[链式法则](@entry_id:190743)，我们可以推导出对数似然函数的梯度 ：
$$
\nabla_{\mathbf{k}}\mathcal{L}(\mathbf{k}) = \sum_{t=1}^{T} \left( \frac{y_t - \lambda_t}{\lambda_t} \right) \frac{f'(u_t)}{f(u_t)} \mathbf{x}_t
$$
其中 $u_t = \mathbf{k}^\top \mathbf{x}_t$, $\lambda_t = f(u_t)$, $f'$ 是[非线性](@entry_id:637147)函数的导数。这个梯度表达式具有直观的解释：它是所有刺激向量 $\mathbf{x}_t$ 的加权和。每个 $\mathbf{x}_t$ 的权重取决于**预测误差** $(y_t - \lambda_t)$（实际观测值与模型预测值之差）和[非线性](@entry_id:637147)函数的敏感度。

同样，我们可以计算[海森矩阵](@entry_id:139140) $\nabla^2 \mathcal{L}(\mathbf{k})$，它描述了[对数似然函数](@entry_id:168593)的局部曲率 。[海森矩阵](@entry_id:139140)在牛顿法等[二阶优化](@entry_id:175310)算法中至关重要，它还可以用来估计参数估计的[置信区间](@entry_id:142297)。

如前所述，当[非线性](@entry_id:637147)函数 $f$ 是[指数函数](@entry_id:161417)（[泊松模型](@entry_id:1129884)的规范链接）时，[对数似然函数](@entry_id:168593) $\mathcal{L}(\mathbf{k})$ 是一个关于 $\mathbf{k}$ 的[凹函数](@entry_id:274100)。这意味着它只有一个[全局最大值](@entry_id:174153)，没有[局部极值](@entry_id:144991)。这个优良的性质保证了梯度上升法能够收敛到唯一的最优解，使得模型拟合过程既稳定又可靠。

### 理论考量与模型扩展

尽管LN模型在概念上简单且应用广泛，但我们需要理解其理论上的局限性，并了解如何扩展它以构建更强大的模型。

#### 模型的[可辨识性](@entry_id:194150)

一个重要的问题是**[可辨识性](@entry_id:194150)（identifiability）**：在给定无限数据的情况下，我们能否唯一地确定滤波核 $\mathbf{k}$ 和[非线性](@entry_id:637147)函数 $f$？答案是否定的，除非施加额外的约束 。LN模型存在固有的模糊性：

1.  **尺度和符号模糊性**：对于任意非零常数 $c$，模型 $(\mathbf{k}, f)$ 与模型 $(c\mathbf{k}, f_c)$（其中 $f_c(u) = f(u/c)$）在观测上是等价的，因为它们对任何刺激 $\mathbf{x}$ 都产生相同的输出发放率。这意味着我们可以任意缩放滤波器 $\mathbf{k}$ 的范数，并通过对[非线性](@entry_id:637147)函数 $f$ 进行反向缩放来补偿。同样，我们可以同时反转 $\mathbf{k}$ 的符号和 $f$ 的坐标轴。为了解决这种模糊性，必须对 $\mathbf{k}$ 施加约束，例如，固定其范数（如 $\|\mathbf{k}\|_2=1$）并采用一个符号约定（如要求其第一个非零元素为正）。

2.  **源于刺激分布的模糊性**：如果刺激数据本身是受限的（例如，所有刺激向量 $\mathbf{x}_t$ 都位于 $\mathbb{R}^d$ 的一个低维子空间中），那么与该子空间正交的任何滤波器分量都无法被确定，因为它对模型的输出没有影响。因此，一个信息丰富的、能够探索所有维度的刺激分布（例如，具有满秩[协方差矩阵](@entry_id:139155)的刺激）对于唯一确定 $\mathbf{k}$ 的方向至关重要。

3.  **源于[非线性](@entry_id:637147)函数的模糊性**：即使在固定了 $\mathbf{k}$ 之后，如果对 $f$ 不加约束，也可能无法唯一确定它。例如，如果 $f$ 不是[单射](@entry_id:183792)的（非一对一），就可能产生新的模糊性。因此，通常需要将 $f$ 限制在一个函数类别内，例如，要求它是**严格单调的**。

综上所述，只有在对滤波器范数和符号、刺激分布的丰富性以及[非线性](@entry_id:637147)函数的类别都施加了适当的约束之后，LN模型的参数 $(\mathbf{k}, f)$ 才能被唯一地辨识。

#### 与[Volterra级数](@entry_id:167552)的关系

LN模型可以被看作是一个更普适的[非线性系统](@entry_id:168347)描述框架——**[Volterra级数](@entry_id:167552)（Volterra series）**——的一个特例 。[Volterra级数](@entry_id:167552)将系统的输出表示为输入的多项式泛函之和。如果LN模型中的[非线性](@entry_id:637147)函数 $f$ 是光滑的，我们可以对其进行泰勒展开。将线性滤波后的信号 $u(t)$ 代入这个展开式，可以证明LN模型的输出等价于一个[Volterra级数](@entry_id:167552)。

这个等价的[Volterra级数](@entry_id:167552)有一个显著特征：它的所有高阶核都是**可分离的**，即第 $n$ 阶核可以写成 $k_n(\tau_1, \dots, \tau_n) = a_n k(\tau_1)\cdots k(\tau_n)$，其中 $a_n$ 是 $f$ 的泰勒系数。这表明LN模型捕捉的是一类特定的、结构化的[非线性](@entry_id:637147)。相比之下，一个一般的[非线性系统](@entry_id:168347)可能具有不可分离的Volterra核。因此，LN模型可以被理解为对[神经计算](@entry_id:154058)的一种[降维](@entry_id:142982)近似。仅当输入到[非线性](@entry_id:637147)阶段的信号 $u(t)$ 非常小，以至于 $f$ 的泰勒展开中的高阶项可以忽略不计时，一个纯线性模型（即一阶[Volterra级数](@entry_id:167552)）才是LN模型的良好近似。

#### 超越LN模型：[广义线性模型](@entry_id:900434)（GLM）

LN模型的一个核心假设是其前馈结构：发放率完全由外部刺激决定。然而，真实神经元的发放行为通常还受到其自身近期发放历史的影响，例如**不应期（refractoriness）**和**发放后易化/抑制（post-spike facilitation/suppression）**。

为了捕捉这些内在动态，我们可以将LN模型扩展为**[广义线性模型](@entry_id:900434)（Generalized Linear Model, GLM）**，通过在模型的线性部分加入一个**[脉冲历史滤波器](@entry_id:1132150)（spike-history filter）** $h(t)$ 。此时，[条件强度函数](@entry_id:1122850)变为：
$$
\lambda(t) = g\big( (k * s)(t) + (h * y)(t) + b \big)
$$
其中 $y(t)$ 是神经元自身的[脉冲序列](@entry_id:1132157)。

这个扩展带来了深刻的改变：

*   **从前馈到反馈**：模型不再是纯粹的前馈系统。由于输出 $y(t)$ 通过 $h*y$ 项影响其自身的生成过程，系统引入了**反馈回路**。这使得脉冲过程成为一个**自激励（self-exciting）**或[自抑制](@entry_id:169700)的点过程，更符合生物物理现实。

*   **[脉冲历史滤波器](@entry_id:1132150)的解释**：历史滤波器 $h(t)$ 的形状捕捉了神经元内在的动力学特性。一个在 $t \approx 0$ 处有强负值的 $h(t)$ 可以有效地在脉冲发放后短暂地将发放率降至零，从而模拟[不应期](@entry_id:152190)。而在更长的时间尺度上的正值或负值则可以分别模拟发放后易化（有助于形成簇状发放）或抑制。

*   **参数解释的改变**：在GLM中，刺激滤波器 $k(t)$ 的解释也发生了变化。它不再代表对刺激的“绝对”响应，而是代表在**考虑了内在发放动态之后**，刺激所贡献的驱动部分。如果在[数据拟合](@entry_id:149007)时忽略了一个真实存在的历史依赖项（即使用LN模型代替GLM），那么由于**[遗漏变量偏差](@entry_id:169961)**，估计出的刺激滤波器 $k(t)$ 将会产生系统性误差。它可能会错误地将本应由[不应期](@entry_id:152190)等内在机制解释的发放率变化归因于刺激的某些特征。因此，从LN模型和GLM中得到的滤波器 $k(t)$ 不可直接比较 。

*   **增益控制的视角**：当使用指数[非线性](@entry_id:637147)时，GLM的[强度函数](@entry_id:755508)可以分解为乘积形式：$\lambda(t) = \exp(b) \cdot \exp((k * s)(t)) \cdot \exp((h * y)(t))$。这揭示了一种**动态增益控制**机制：神经元的内在状态（由 $\exp((h*y)(t))$ 反映）可以乘性地调节它对外部刺激的响应强度。这为理解神经元如何根据其自身活动状态调整计算提供了一个强大的框架 。

总之，LN模型为理解[神经编码](@entry_id:263658)提供了一个简洁而强大的起点，而通过引入脉冲历史依赖项将其扩展到GLM框架，则能够构建出更精确、更具生物学解释力的模型，从而更深入地揭示[神经计算](@entry_id:154058)的复杂动态。