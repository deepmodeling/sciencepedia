{
    "hands_on_practices": [
        {
            "introduction": "The brain operates under strict metabolic constraints, making the efficient use of energy a critical principle of neural design. This first practice explores the foundational \"water-filling\" solution from information theory, which addresses how to optimally allocate a limited resource, such as total firing rate or metabolic energy, across multiple parallel channels of information. By working through this problem (), you will derive from first principles how to invest more coding resources in channels with a high signal-to-noise ratio ($SNR$), providing a powerful intuition for the logic of efficient coding.",
            "id": "3977210",
            "problem": "Consider an $n$-dimensional stimulus $S \\in \\mathbb{R}^{n}$ modeled as a zero-mean multivariate Gaussian random vector with covariance $\\Sigma_{S}$, and a linear encoder producing a neural response $R \\in \\mathbb{R}^{n}$ according to $R = W S + N$, where $W$ is a diagonal encoding matrix, $N$ is zero-mean multivariate Gaussian noise with diagonal covariance $\\Sigma_{N}$, and $S$ and $N$ are independent. Assume the energy constraint $\\mathbb{E}[\\|R\\|_{2}^{2}] \\leq C$. At the level of advanced graduate study in brain modeling and computational neuroscience, start from the fundamental definitions of mutual information $I(S;R) = h(R) - h(R|S)$ and the well-tested formula for the differential entropy of a multivariate Gaussian $h(X) = \\frac{1}{2}\\ln\\!\\left((2\\pi e)^{n}\\det\\Sigma_{X}\\right)$, and derive an expression for $I(S;R)$ as a function of the encoder gains and covariances. Then, pose and solve the constrained optimization that maximizes $I(S;R)$ subject to the energy constraint by using the method of Lagrange multipliers and Karush–Kuhn–Tucker (KKT) conditions, showing that the optimal allocation follows a water-filling principle over modes.\n\nConcretely, let $n=4$, let $\\Sigma_{S} = \\mathrm{diag}(s_{1}^{2}, s_{2}^{2}, s_{3}^{2}, s_{4}^{2})$ with $(s_{1}^{2}, s_{2}^{2}, s_{3}^{2}, s_{4}^{2}) = (2, 1, 0.5, 0.25)$, let $\\Sigma_{N} = \\mathrm{diag}(\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2}, \\sigma_{4}^{2})$ with $(\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2}, \\sigma_{4}^{2}) = (0.5, 1, 2, 3)$, and let $W = \\mathrm{diag}(g_{1}, g_{2}, g_{3}, g_{4})$ with unknown nonnegative gains. The energy budget is $C = 9$, and the logarithm throughout should be taken as the natural logarithm so that information is measured in nats.\n\nYour tasks are:\n- Derive $I(S;R)$ in terms of $(g_{i}, s_{i}^{2}, \\sigma_{i}^{2})$ using the stated base principles.\n- Formulate and solve the optimization problem that maximizes $I(S;R)$ subject to $\\mathbb{E}[\\|R\\|_{2}^{2}] \\leq C$ using Lagrange multipliers and Karush–Kuhn–Tucker (KKT) conditions, and write the water-filling solution for the optimal allocation across modes in terms of a water level parameter.\n- Using the given numerical values, compute the optimal mutual information $I^{\\star}$ for the water-filling allocation.\n\nRound your final reported value of $I^{\\star}$ to four significant figures and express it in nats. The final answer must be a single real number.",
            "solution": "The user has provided a problem statement to be validated and solved.\n\n### Step 1: Extract Givens\n- Stimulus: $S \\in \\mathbb{R}^{n}$, a zero-mean multivariate Gaussian random vector with covariance $\\Sigma_{S}$.\n- Encoder: $R = W S + N$, where $R \\in \\mathbb{R}^{n}$ is the neural response.\n- Encoding matrix: $W$ is a diagonal matrix, $W = \\mathrm{diag}(g_1, \\dots, g_n)$ with $g_i \\ge 0$.\n- Noise: $N \\in \\mathbb{R}^{n}$, a zero-mean multivariate Gaussian random vector with diagonal covariance $\\Sigma_{N}$.\n- Independence: $S$ and $N$ are independent.\n- Constraint: Energy of the response is bounded, $\\mathbb{E}[\\|R\\|_{2}^{2}] \\leq C$.\n- Mutual Information definition: $I(S;R) = h(R) - h(R|S)$.\n- Differential Entropy for a multivariate Gaussian $X$: $h(X) = \\frac{1}{2}\\ln\\!\\left((2\\pi e)^{n}\\det\\Sigma_{X}\\right)$.\n- Numerical values for $n=4$:\n    - $\\Sigma_{S} = \\mathrm{diag}(s_{1}^{2}, s_{2}^{2}, s_{3}^{2}, s_{4}^{2})$ with $(s_{1}^{2}, s_{2}^{2}, s_{3}^{2}, s_{4}^{2}) = (2, 1, 0.5, 0.25)$.\n    - $\\Sigma_{N} = \\mathrm{diag}(\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2}, \\sigma_{4}^{2})$ with $(\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2}, \\sigma_{4}^{2}) = (0.5, 1, 2, 3)$.\n    - Energy budget: $C = 9$.\n- Logarithm: Natural logarithm ($\\ln$), information measured in nats.\n- Tasks:\n    1. Derive $I(S;R)$ in terms of $(g_i, s_i^2, \\sigma_i^2)$.\n    2. Formulate and solve the optimization problem for maximizing $I(S;R)$ subject to the energy constraint using Lagrange multipliers and KKT conditions to find the water-filling solution.\n    3. Compute the optimal mutual information $I^{\\star}$ for the given numerical values, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a classic and fundamental problem in theoretical neuroscience and information theory, investigating the efficient coding hypothesis. The model of a linear-Gaussian channel and the optimization of mutual information under a power constraint are standard and well-established. It is scientifically sound.\n2.  **Well-Posed**: The problem is well-posed. It involves maximizing a strictly concave function over a convex set, which guarantees a unique solution. All necessary information and constraints are provided.\n3.  **Objective**: The language is precise, mathematical, and free of subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full solution.\n\n### Derivation of Mutual Information\nThe mutual information between the stimulus $S$ and the response $R$ is given by $I(S;R) = h(R) - h(R|S)$. We will compute each term separately.\n\nFirst, we calculate the conditional entropy $h(R|S)$. Given a specific stimulus realization $S=s$, the response is $R = Ws + N$. Since $Ws$ is a constant vector and $N$ is a zero-mean Gaussian random vector with covariance $\\Sigma_N$, the conditional distribution $p(r|s)$ is Gaussian with mean $Ws$ and covariance $\\Sigma_N$. The differential entropy of this conditional distribution is given by the formula for Gaussian entropy:\n$$h(R|S=s) = \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det \\Sigma_N\\right)$$\nSince this expression does not depend on the specific value of $s$, the conditional entropy $h(R|S)$ is the expectation of $h(R|S=s)$ over all $s$, which is simply the expression itself:\n$$h(R|S) = \\mathbb{E}_{S}[h(R|S=s)] = \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det \\Sigma_N\\right)$$\n\nNext, we calculate the entropy of the response, $h(R)$. The response is $R = WS + N$. Since $S$ and $N$ are independent, zero-mean Gaussian vectors, their linear combination $R$ is also a zero-mean Gaussian vector. We need to find its covariance matrix, $\\Sigma_R$.\n$$\\Sigma_R = \\mathbb{E}[RR^T] = \\mathbb{E}[(WS+N)(WS+N)^T] = \\mathbb{E}[WSS^TW^T + WSN^T + NS^TW^T + NN^T]$$\nDue to the independence of $S$ and $N$ and their zero-mean property, the cross-terms have zero expectation: $\\mathbb{E}[WSN^T] = W\\mathbb{E}[S]\\mathbb{E}[N^T] = 0$ and $\\mathbb{E}[NS^TW^T] = \\mathbb{E}[N]\\mathbb{E}[S^T]W^T = 0$.\nThus, the covariance of $R$ is the sum of the transformed stimulus covariance and the noise covariance:\n$$\\Sigma_R = W\\mathbb{E}[SS^T]W^T + \\mathbb{E}[NN^T] = W\\Sigma_S W^T + \\Sigma_N$$\nThe entropy of $R$ is therefore:\n$$h(R) = \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det \\Sigma_R\\right) = \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det(W\\Sigma_S W^T + \\Sigma_N)\\right)$$\n\nNow, we combine the terms to find the mutual information:\n$$I(S;R) = h(R) - h(R|S) = \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det(W\\Sigma_S W^T + \\Sigma_N)\\right) - \\frac{1}{2}\\ln\\left((2\\pi e)^n \\det \\Sigma_N\\right)$$\n$$I(S;R) = \\frac{1}{2}\\left[\\ln(\\det(W\\Sigma_S W^T + \\Sigma_N)) - \\ln(\\det \\Sigma_N)\\right] = \\frac{1}{2}\\ln\\left(\\frac{\\det(W\\Sigma_S W^T + \\Sigma_N)}{\\det \\Sigma_N}\\right)$$\nSince all matrices $W$, $\\Sigma_S$, and $\\Sigma_N$ are diagonal with elements $g_i$, $s_i^2$, and $\\sigma_i^2$ respectively, their products and sums are also diagonal. The determinant of a diagonal matrix is the product of its diagonal elements.\n$$W\\Sigma_S W^T + \\Sigma_N = \\mathrm{diag}(g_1^2s_1^2 + \\sigma_1^2, \\dots, g_n^2s_n^2 + \\sigma_n^2)$$\n$$\\det(W\\Sigma_S W^T + \\Sigma_N) = \\prod_{i=1}^n (g_i^2s_i^2 + \\sigma_i^2)$$\n$$\\det \\Sigma_N = \\prod_{i=1}^n \\sigma_i^2$$\nSubstituting these into the expression for $I(S;R)$:\n$$I(S;R) = \\frac{1}{2}\\ln\\left(\\frac{\\prod_{i=1}^n (g_i^2s_i^2 + \\sigma_i^2)}{\\prod_{i=1}^n \\sigma_i^2}\\right) = \\frac{1}{2}\\sum_{i=1}^n \\ln\\left(\\frac{g_i^2s_i^2 + \\sigma_i^2}{\\sigma_i^2}\\right) = \\frac{1}{2}\\sum_{i=1}^n \\ln\\left(1 + \\frac{g_i^2s_i^2}{\\sigma_i^2}\\right)$$\nThis is the desired expression for mutual information.\n\n### Optimization Problem and Water-Filling Solution\nWe aim to maximize $I(S;R)$ subject to the energy constraint $\\mathbb{E}[\\|R\\|_{2}^{2}] \\leq C$. First, we express the constraint in terms of the model parameters.\n$$\\mathbb{E}[\\|R\\|_2^2] = \\mathbb{E}\\left[\\sum_{i=1}^n R_i^2\\right] = \\sum_{i=1}^n \\mathbb{E}[R_i^2]$$\nFor each component, $R_i = g_i S_i + N_i$. The variance of $R_i$ is:\n$$\\mathbb{E}[R_i^2] = \\mathbb{E}[(g_i S_i + N_i)^2] = g_i^2 \\mathbb{E}[S_i^2] + 2g_i\\mathbb{E}[S_iN_i] + \\mathbb{E}[N_i^2]$$\nSince $S_i$ and $N_i$ are independent and zero-mean, $\\mathbb{E}[S_iN_i] = \\mathbb{E}[S_i]\\mathbb{E}[N_i]=0$.\n$$\\mathbb{E}[R_i^2] = g_i^2s_i^2 + \\sigma_i^2$$\nThe total energy constraint is thus $\\sum_{i=1}^n (g_i^2s_i^2 + \\sigma_i^2) \\leq C$.\n\nLet $E_i = g_i^2s_i^2 + \\sigma_i^2$ be the average power of the $i$-th response component. The constraint is $\\sum_{i=1}^n E_i \\leq C$. The term $g_i^2s_i^2$ represents the signal power in the response, which must be non-negative, so $E_i \\ge \\sigma_i^2$.\nThe mutual information can be rewritten in terms of $E_i$:\n$$I(S;R) = \\frac{1}{2}\\sum_{i=1}^n \\ln\\left(1 + \\frac{E_i - \\sigma_i^2}{\\sigma_i^2}\\right) = \\frac{1}{2}\\sum_{i=1}^n \\ln\\left(\\frac{E_i}{\\sigma_i^2}\\right) = \\frac{1}{2}\\left(\\sum_i \\ln(E_i) - \\sum_i \\ln(\\sigma_i^2)\\right)$$\nTo maximize $I(S;R)$, we can equivalently maximize $\\sum_i \\ln(E_i)$, since the second term is a constant. The optimization problem is:\n$$ \\text{maximize} \\quad \\sum_{i=1}^n \\ln(E_i) $$\n$$ \\text{subject to} \\quad \\sum_{i=1}^n E_i \\leq C \\quad \\text{and} \\quad E_i \\geq \\sigma_i^2 \\text{ for all } i=1, \\dots, n. $$\nThis is a convex optimization problem. We form the Lagrangian by minimizing the negative objective:\n$$\\mathcal{L}(\\{E_i\\}, \\lambda, \\{\\mu_i\\}) = -\\sum_{i=1}^n \\ln(E_i) + \\lambda\\left(\\sum_{i=1}^n E_i - C\\right) + \\sum_{i=1}^n \\mu_i(\\sigma_i^2 - E_i)$$\nThe Karush-Kuhn-Tucker (KKT) conditions are:\n1.  Stationarity: $\\frac{\\partial \\mathcal{L}}{\\partial E_i} = -\\frac{1}{E_i} + \\lambda - \\mu_i = 0$ for all $i$.\n2.  Primal Feasibility: $\\sum_i E_i \\leq C$ and $E_i \\ge \\sigma_i^2$.\n3.  Dual Feasibility: $\\lambda \\ge 0$ and $\\mu_i \\ge 0$.\n4.  Complementary Slackness: $\\lambda(\\sum_i E_i - C) = 0$ and $\\mu_i(\\sigma_i^2 - E_i) = 0$.\n\nFrom complementary slackness on $\\mu_i$, we have two cases for each channel $i$:\n- Case 1: $E_i  \\sigma_i^2$. This implies $\\mu_i = 0$. The stationarity condition becomes $-\\frac{1}{E_i} + \\lambda = 0$, which yields $E_i = \\frac{1}{\\lambda}$.\n- Case 2: $E_i = \\sigma_i^2$. This implies $\\mu_i \\ge 0$. The stationarity condition becomes $-\\frac{1}{\\sigma_i^2} + \\lambda = \\mu_i \\ge 0$, which implies $\\lambda \\ge \\frac{1}{\\sigma_i^2}$, or $\\frac{1}{\\lambda} \\le \\sigma_i^2$.\n\nLet's define a \"water level\" $L = \\frac{1}{\\lambda}$. The two cases combine to give the optimal power allocation for each response component:\n$$E_i^\\star = \\max\\left(\\sigma_i^2, L\\right)$$\nThe budget constraint $\\sum_i E_i = C$ is active (since $\\ln(E_i)$ is monotonically increasing), so $\\lambda  0$ and $L$ is finite. The water level $L$ is determined by the total energy constraint:\n$$\\sum_{i=1}^n \\max(\\sigma_i^2, L) = C$$\nThis is the water-filling solution. The total power $E_i$ for each channel is either its noise floor $\\sigma_i^2$ (if the noise is above the water level) or it is filled up to the water level $L$.\n\n### Numerical Calculation\nWe are given $n=4$, $C=9$, and noise variances $(\\sigma_1^2, \\sigma_2^2, \\sigma_3^2, \\sigma_4^2) = (0.5, 1, 2, 3)$. We solve for the water level $L$ using the equation $\\sum_{i=1}^4 \\max(\\sigma_i^2, L) = 9$. We proceed by guessing the number of active channels, i.e., those for which $L  \\sigma_i^2$.\nLet's order the noise variances: $\\sigma_1^2=0.5  \\sigma_2^2=1  \\sigma_3^2=2  \\sigma_4^2=3$.\n\n1.  Assume all 4 channels are active ($E_i=L$ for all $i$). Then $4L = 9 \\implies L=2.25$. This assumption requires $L  \\sigma_i^2$ for all $i$. But $L=2.25$ is not greater than $\\sigma_4^2=3$. The assumption is false. Channel 4 is inactive.\n\n2.  Assume 3 channels are active (1, 2, 3) and channel 4 is inactive ($E_4 = \\sigma_4^2$). The constraint equation becomes $L+L+L+\\sigma_4^2 = 9$.\n    $$3L + 3 = 9 \\implies 3L = 6 \\implies L=2$$\n    We check for consistency. For active channels $i=1,2,3$, we need $L \\ge \\sigma_i^2$.\n    - $i=1: L=2 \\ge \\sigma_1^2=0.5$ (True)\n    - $i=2: L=2 \\ge \\sigma_2^2=1$ (True)\n    - $i=3: L=2 \\ge \\sigma_3^2=2$ (True)\n    For the inactive channel $i=4$, we need $L \\le \\sigma_4^2$.\n    - $i=4: L=2 \\le \\sigma_4^2=3$ (True)\n    The solution is consistent. The water level is $L=2$.\n\nThe optimal response powers $E_i^\\star$ are:\n$E_1^\\star = \\max(0.5, 2) = 2$\n$E_2^\\star = \\max(1, 2) = 2$\n$E_3^\\star = \\max(2, 2) = 2$\n$E_4^\\star = \\max(3, 2) = 3$\nThe sum is $2+2+2+3=9$, satisfying the constraint.\n\nThe optimal signal powers $P_i^\\star = g_i^2s_i^2 = E_i^\\star - \\sigma_i^2$ are:\n$P_1^\\star = 2 - 0.5 = 1.5$\n$P_2^\\star = 2 - 1 = 1$\n$P_3^\\star = 2 - 2 = 0$\n$P_4^\\star = 3 - 3 = 0$\n\nFinally, we compute the maximum mutual information $I^\\star$:\n$$I^\\star = \\frac{1}{2}\\sum_{i=1}^4 \\ln\\left(1 + \\frac{P_i^\\star}{\\sigma_i^2}\\right)$$\n$$I^\\star = \\frac{1}{2}\\left[ \\ln\\left(1 + \\frac{1.5}{0.5}\\right) + \\ln\\left(1 + \\frac{1}{1}\\right) + \\ln\\left(1 + \\frac{0}{2}\\right) + \\ln\\left(1 + \\frac{0}{3}\\right) \\right]$$\n$$I^\\star = \\frac{1}{2}\\left[ \\ln(1 + 3) + \\ln(1 + 1) + \\ln(1) + \\ln(1) \\right]$$\n$$I^\\star = \\frac{1}{2}\\left[ \\ln(4) + \\ln(2) + 0 + 0 \\right]$$\nUsing the logarithm property $\\ln(a) + \\ln(b) = \\ln(ab)$:\n$$I^\\star = \\frac{1}{2}\\ln(4 \\times 2) = \\frac{1}{2}\\ln(8) = \\frac{1}{2}\\ln(2^3) = \\frac{3}{2}\\ln(2)$$\nNow we compute the numerical value:\n$$I^\\star = 1.5 \\times \\ln(2) \\approx 1.5 \\times 0.69314718 \\approx 1.03972077$$\nRounding to four significant figures, we get $1.040$. The result is in nats.",
            "answer": "$$\\boxed{1.040}$$"
        },
        {
            "introduction": "Having established the principle of optimal resource allocation, we now apply it to a key biological system: the primary visual cortex (V1). Natural images are not random noise; they possess statistical regularities, such as a power spectrum that typically falls off with frequency as $S(k) \\propto k^{-\\alpha}$. This exercise () challenges you to derive the optimal linear filters for encoding such images in the presence of noise, demonstrating how the efficient coding hypothesis can predict the emergence of orientation and spatial frequency tuning characteristic of V1 simple cells.",
            "id": "3977250",
            "problem": "Consider a two-dimensional, zero-mean, stationary Gaussian random field representing natural images. Its isotropic power spectral density (PSD) in spatial frequency polar coordinates is given by $S(k) = A k^{-\\alpha}$ for $k \\geq 0$, with constants $A  0$ and $\\alpha  0$. The encoding is modeled by a linear, separable, spatial filter whose transfer function factors into a radial (spatial frequency) envelope and an orientation envelope, $H(k,\\theta) = h(k)\\,g(\\theta)$, where $h(k) \\geq 0$ and $g(\\theta)$ is $2\\pi$-periodic. The encoding is corrupted by additive, orientation-independent, white Gaussian noise with PSD $N_{0}  0$ at the output of the filter.\n\nAssume the output power budget is constrained by\n$$\n\\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\,|H(k,\\theta)|^{2} \\;=\\; P,\n$$\nwith $P  0$ fixed. Impose the normalization on the orientation envelope\n$$\n\\int_{0}^{2\\pi} g(\\theta)^{2}\\,\\mathrm{d}\\theta \\;=\\; 1.\n$$\n\nUsing the mutual information of a linear Gaussian channel as the objective to be maximized over admissible $H(k,\\theta)$, derive from first principles the optimal choice of the orientation envelope $g^{\\star}(\\theta)$ and the optimal spatial frequency tuning defined by the cutoff spatial frequency $k_{c}$ at which the optimal radial envelope $h^{\\star}(k)$ transitions to zero. Express $g^{\\star}(\\theta)$ and $k_{c}$ in closed form as functions of $\\alpha$, $A$, $N_{0}$, and $P$. Your final answer should be provided as a two-entry row matrix containing $g^{\\star}(\\theta)$ and $k_{c}$. No rounding is required, and no units are necessary.",
            "solution": "The problem requires the derivation of the optimal filter transfer function $H(k,\\theta)$ that maximizes the mutual information between the input signal and the filter's output, subject to a constraint on the filter's power. The transfer function is given as separable, $H(k,\\theta) = h(k)g(\\theta)$.\n\nThe input is a zero-mean stationary Gaussian random field with an isotropic power spectral density (PSD) $S(k) = A k^{-\\alpha}$. The encoding is corrupted by additive white Gaussian noise with a constant PSD $N_{0}$ at the output of the filter. The signal-to-noise ratio (SNR) at a given spatial frequency $(k, \\theta)$ is therefore:\n$$\n\\text{SNR}(k, \\theta) = \\frac{|H(k,\\theta)|^{2} S(k)}{N_{0}} = \\frac{h(k)^{2} g(\\theta)^{2} A k^{-\\alpha}}{N_{0}}\n$$\nHere we have used the fact that $|H(k,\\theta)|^2 = (h(k)g(\\theta))^2 = h(k)^2 g(\\theta)^2$ since $h(k)$ and $g(\\theta)$ can be chosen to be real and non-negative without loss of generality. The problem states $h(k) \\ge 0$. We can also choose $g(\\theta)$ to be real.\n\nThe total mutual information $I$ for a linear Gaussian channel is the integral of the information rate density over all independent frequency channels. In two-dimensional polar coordinates, this is given by:\n$$\nI[h, g] = \\frac{1}{2} \\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\, \\ln \\left(1 + \\text{SNR}(k, \\theta) \\right)\n$$\nSubstituting the expression for the SNR, we get:\n$$\nI[h, g] = \\frac{1}{2} \\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\, \\ln \\left(1 + \\frac{h(k)^{2} g(\\theta)^{2} S(k)}{N_{0}} \\right)\n$$\nThe factor of $\\frac{1}{2}$ and the use of the natural logarithm $\\ln$ imply the information is measured in nats. Maximizing this expression is equivalent to maximizing it without the $\\frac{1}{2}$ pre-factor.\n\nThis optimization is subject to two constraints:\n1.  An output power budget (more accurately, a constraint on the squared L2-norm of the filter):\n    $$\n    \\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\,|H(k,\\theta)|^{2} = \\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\, h(k)^{2} g(\\theta)^{2} = P\n    $$\n2.  Normalization of the orientation envelope:\n    $$\n    \\int_{0}^{2\\pi} g(\\theta)^{2}\\,\\mathrm{d}\\theta = 1\n    $$\n\nWe can simplify the power constraint by using the normalization of $g(\\theta)$:\n$$\nP = \\int_{0}^{\\infty} k h(k)^{2} \\left( \\int_{0}^{2\\pi} g(\\theta)^{2} \\,\\mathrm{d}\\theta \\right) \\mathrm{d}k = \\int_{0}^{\\infty} k h(k)^{2} \\cdot 1 \\, \\mathrm{d}k = \\int_{0}^{\\infty} k h(k)^{2} \\,\\mathrm{d}k\n$$\nThis simplified constraint only involves the radial envelope $h(k)$.\n\nThe optimization problem can be solved in two steps. First, we optimize for $g(\\theta)$ for any fixed $h(k)$, and then we optimize for $h(k)$.\n\nStep 1: Optimization of the orientation envelope $g(\\theta)$.\nLet's examine the structure of the mutual information integral. We can separate the integrals:\n$$\nI[h, g] = \\frac{1}{2} \\int_{0}^{\\infty} k \\left( \\int_{0}^{2\\pi} \\ln \\left(1 + \\frac{h(k)^{2} S(k)}{N_{0}} g(\\theta)^{2} \\right) \\mathrm{d}\\theta \\right) \\mathrm{d}k\n$$\nTo maximize $I$, for each value of $k$, we must maximize the inner integral over $\\theta$:\n$$\nJ[g] = \\int_{0}^{2\\pi} \\ln \\left(1 + C(k) g(\\theta)^{2} \\right) \\mathrm{d}\\theta\n$$\nsubject to the constraint $\\int_{0}^{2\\pi} g(\\theta)^{2}\\,\\mathrm{d}\\theta = 1$. Here, $C(k) = \\frac{h(k)^{2} S(k)}{N_{0}}$ is a positive constant with respect to $\\theta$.\n\nWe use the calculus of variations. We form the Lagrangian functional:\n$$\n\\mathcal{L}[g] = \\int_{0}^{2\\pi} \\left[ \\ln \\left(1 + C(k) g(\\theta)^{2} \\right) - \\mu g(\\theta)^{2} \\right] \\mathrm{d}\\theta\n$$\nwhere $\\mu$ is a Lagrange multiplier. The Euler-Lagrange equation $\\frac{\\partial \\mathcal{L}}{\\partial g} = 0$ yields:\n$$\n\\frac{2 C(k) g(\\theta)}{1 + C(k) g(\\theta)^{2}} - 2 \\mu g(\\theta) = 0\n$$\n$$\ng(\\theta) \\left( \\frac{C(k)}{1 + C(k) g(\\theta)^{2}} - \\mu \\right) = 0\n$$\nThis implies either $g(\\theta) = 0$ (a trivial solution that violates the constraint) or $\\frac{C(k)}{1 + C(k) g(\\theta)^{2}} = \\mu$.\nSolving for $g(\\theta)^{2}$:\n$$\nC(k) = \\mu (1 + C(k) g(\\theta)^{2}) \\implies g(\\theta)^{2} = \\frac{1}{C(k)} \\left(\\frac{C(k)}{\\mu} - 1\\right) = \\frac{1}{\\mu} - \\frac{1}{C(k)}\n$$\nSince this must hold for all $\\theta$, $g(\\theta)^{2}$ must be a constant. This is an intuitive result, as the signal PSD $S(k)$ and the noise PSD $N_{0}$ are both isotropic (independent of $\\theta$), so there is no reason to prefer any particular orientation.\nLet $g(\\theta)^{2} = G^{2}$, where $G$ is a constant. Applying the normalization constraint:\n$$\n\\int_{0}^{2\\pi} G^{2}\\,\\mathrm{d}\\theta = 1 \\implies 2\\pi G^{2} = 1 \\implies G^{2} = \\frac{1}{2\\pi}\n$$\nThus, the optimal orientation envelope $g^{\\star}(\\theta)$ is a constant. Choosing the positive real root, we have:\n$$\ng^{\\star}(\\theta) = \\frac{1}{\\sqrt{2\\pi}}\n$$\n\nStep 2: Optimization of the radial envelope $h(k)$.\nSubstituting $g^{\\star}(\\theta)$ into the mutual information expression:\n$$\nI[h] = \\frac{1}{2} \\int_{0}^{2\\pi} \\int_{0}^{\\infty} k\\,\\mathrm{d}k\\,\\mathrm{d}\\theta\\, \\ln \\left(1 + \\frac{h(k)^{2} S(k)}{2\\pi N_{0}} \\right)\n$$\nThe integrand is now independent of $\\theta$, so we can evaluate the inner integral:\n$$\nI[h] = \\pi \\int_{0}^{\\infty} k \\ln \\left(1 + \\frac{h(k)^{2} S(k)}{2\\pi N_{0}} \\right) \\mathrm{d}k\n$$\nWe want to maximize this functional subject to the constraint $P = \\int_{0}^{\\infty} k h(k)^{2} \\,\\mathrm{d}k$.\nLet us define $f(k) = h(k)^{2}$, so $f(k) \\geq 0$. The problem is to maximize $\\pi \\int_0^\\infty k \\ln\\left(1 + \\frac{f(k) S(k)}{2\\pi N_0}\\right) dk$ subject to $\\int_0^\\infty k f(k) dk = P$. We form the Lagrangian:\n$$\n\\mathcal{L}[f] = \\int_{0}^{\\infty} \\left[ \\pi k \\ln \\left(1 + \\frac{f(k) S(k)}{2\\pi N_{0}} \\right) - \\lambda k f(k) \\right] \\mathrm{d}k + \\lambda P\n$$\nwhere $\\lambda$ is a Lagrange multiplier. Taking the functional derivative with respect to $f(k)$ and setting it to zero gives:\n$$\n\\frac{\\delta \\mathcal{L}}{\\delta f(k)} = \\pi k \\left( \\frac{1}{1 + \\frac{f(k) S(k)}{2\\pi N_{0}}} \\cdot \\frac{S(k)}{2\\pi N_{0}} \\right) - \\lambda k = 0\n$$\nFor $k  0$, we can divide by $k$:\n$$\n\\frac{S(k)}{2N_{0}} \\frac{1}{1 + \\frac{f(k) S(k)}{2\\pi N_{0}}} = \\lambda\n$$\nSolving for $f(k) = h(k)^{2}$:\n$$\n\\frac{S(k)}{2N_{0}} = \\lambda \\left(1 + \\frac{f(k) S(k)}{2\\pi N_{0}}\\right) \\implies \\frac{f(k) S(k)}{2\\pi N_{0}} = \\frac{S(k)}{2\\lambda N_{0}} - 1\n$$\n$$\nf(k) = h(k)^{2} = \\frac{2\\pi N_{0}}{S(k)} \\left( \\frac{S(k)}{2\\lambda N_{0}} - 1 \\right) = \\frac{\\pi}{\\lambda} - \\frac{2\\pi N_{0}}{S(k)}\n$$\nSince $h(k)^{2}$ must be non-negative, the optimal filter gain is given by a \"water-filling\" solution:\n$$\nh^{\\star}(k)^{2} = \\max \\left(0, \\frac{\\pi}{\\lambda} - \\frac{2\\pi N_{0}}{S(k)} \\right)\n$$\nThe filter gain is non-zero only for frequencies $k$ where $\\frac{\\pi}{\\lambda} - \\frac{2\\pi N_{0}}{S(k)}  0$, which is equivalent to $S(k)  2 \\lambda N_{0}$.\nLet $k_{c}$ be the cutoff spatial frequency where the gain transitions to zero. At this frequency, $S(k_{c}) = 2 \\lambda N_{0}$.\nSubstituting $S(k) = A k^{-\\alpha}$, we get $A k_{c}^{-\\alpha} = 2 \\lambda N_{0}$.\nSince $S(k)$ is a decreasing function of $k$ (for $\\alpha  0$), the condition $S(k)  S(k_c)$ is satisfied for $k  k_{c}$. Therefore, the optimal filter is non-zero only for $k \\in [0, k_{c}]$.\nWe can express $h^{\\star}(k)^{2}$ for $k \\leq k_{c}$ as:\n$$\nh^{\\star}(k)^{2} = \\frac{2\\pi N_{0}}{S(k_c)} - \\frac{2\\pi N_{0}}{S(k)} = 2\\pi N_{0} \\left( \\frac{1}{A k_{c}^{-\\alpha}} - \\frac{1}{A k^{-\\alpha}} \\right) = \\frac{2\\pi N_{0}}{A} \\left(k_{c}^{\\alpha} - k^{\\alpha}\\right)\n$$\nTo find $k_{c}$, we use the power constraint:\n$$\nP = \\int_{0}^{\\infty} k h^{\\star}(k)^{2} \\mathrm{d}k = \\int_{0}^{k_{c}} k \\left[ \\frac{2\\pi N_{0}}{A} (k_{c}^{\\alpha} - k^{\\alpha}) \\right] \\mathrm{d}k\n$$\n$$\nP = \\frac{2\\pi N_{0}}{A} \\int_{0}^{k_{c}} (k_{c}^{\\alpha} k - k^{1+\\alpha}) \\mathrm{d}k\n$$\nEvaluating the integral:\n$$\nP = \\frac{2\\pi N_{0}}{A} \\left[ k_{c}^{\\alpha} \\frac{k^{2}}{2} - \\frac{k^{2+\\alpha}}{2+\\alpha} \\right]_{0}^{k_{c}} = \\frac{2\\pi N_{0}}{A} \\left( \\frac{k_{c}^{\\alpha+2}}{2} - \\frac{k_{c}^{\\alpha+2}}{2+\\alpha} \\right)\n$$\n$$\nP = \\frac{2\\pi N_{0}}{A} k_{c}^{2+\\alpha} \\left( \\frac{1}{2} - \\frac{1}{2+\\alpha} \\right) = \\frac{2\\pi N_{0}}{A} k_{c}^{2+\\alpha} \\left( \\frac{2+\\alpha-2}{2(2+\\alpha)} \\right)\n$$\n$$\nP = \\frac{2\\pi N_{0}}{A} k_{c}^{2+\\alpha} \\frac{\\alpha}{2(2+\\alpha)} = \\frac{\\pi N_{0} \\alpha}{A(2+\\alpha)} k_{c}^{2+\\alpha}\n$$\nFinally, we solve for the cutoff frequency $k_{c}$:\n$$\nk_{c}^{2+\\alpha} = \\frac{A P (2+\\alpha)}{\\pi N_{0} \\alpha}\n$$\n$$\nk_{c} = \\left( \\frac{A P (2+\\alpha)}{\\pi N_{0} \\alpha} \\right)^{\\frac{1}{2+\\alpha}}\n$$\nThe two required quantities are the optimal orientation envelope $g^{\\star}(\\theta)$ and the optimal cutoff spatial frequency $k_{c}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\sqrt{2\\pi}}  \\left( \\frac{A P (2+\\alpha)}{\\pi N_{0} \\alpha} \\right)^{\\frac{1}{2+\\alpha}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The efficient coding hypothesis suggests that sensory systems should remove predictable, redundant information from the signals they transmit. A powerful mechanism for this is predictive coding, where feedback pathways convey predictions about incoming sensory data, and only the residual \"prediction error\" is sent forward. In this final practice (), you will design and analyze a linear-Gaussian neural circuit that implements this strategy, computing the information throughput of the error signal and thus exploring how neural architectures can embody principles of efficient, inferential processing.",
            "id": "3977263",
            "problem": "You must design a mathematically specified neural architecture with feedforward and feedback pathways that implements prediction error coding under the Efficient Coding Hypothesis. The architecture must be linear and Gaussian, and the information throughput must be computed using first principles. Your program must compute the information throughput for a provided test suite and output the results exactly as specified.\n\nThe architecture operates on a latent stimulus vector $s \\in \\mathbb{R}^{n_s}$ and an observed sensory input vector $x \\in \\mathbb{R}^{n_x}$ with $n_x \\ge 1$ and $n_s \\ge 1$. The environment generates stimuli according to a multivariate normal distribution and a linear generative mapping with Gaussian sensory noise. The network uses prediction error units in a feedforward pathway and prediction units in a feedback pathway to implement Maximum A Posteriori (MAP) inference of the latent stimulus. The representation transmitted forward is the prediction error under additive neural readout noise.\n\nYou must use the following core assumptions and design elements:\n\n- The latent stimulus $s$ is drawn from a multivariate normal distribution with zero mean and covariance $C_s \\in \\mathbb{R}^{n_s \\times n_s}$ that is symmetric positive definite.\n- The sensory input is generated as $x = W s + n_x$, where $W \\in \\mathbb{R}^{n_x \\times n_s}$ is a fixed generative matrix and $n_x \\sim \\mathcal{N}(0, \\sigma_x^2 I_{n_x})$ with $\\sigma_x^2  0$ and $I_{n_x}$ the identity matrix of dimension $n_x$.\n- The network computes a prediction $\\hat{x} = W \\hat{s}$ from the internal representation $\\hat{s}$ and forms the prediction error $e = x - \\hat{x}$ in the feedforward pathway. Feedback conveys predictions $W \\hat{s}$ to subtract from $x$ to produce $e$.\n- The internal representation $\\hat{s}$ is the minimizer of a quadratic energy consistent with Gaussian prior and Gaussian likelihood and is computed at steady state.\n- The code transmitted forward is the error $r = e + n_e$, where $n_e \\sim \\mathcal{N}(0, \\sigma_e^2 I_{n_x})$ with $\\sigma_e^2  0$ independent of all else.\n\nYour task is to:\n\n1. Derive the steady-state estimator $\\hat{s}$ from the assumptions above using only foundational probabilistic principles and linear algebra, without introducing any additional assumptions.\n2. Identify the linear mapping from $x$ to $r$ and express it in terms of $W$, $C_s$, and $\\sigma_x^2$.\n3. Compute the information throughput defined as the Mutual Information (MI) in nats between $x$ and $r$ for a linear Gaussian channel, using the true distribution of $x$ induced by the environment, and independent Gaussian readout noise in the error code.\n4. Implement a program that, for each test case, computes the throughput as a single floating-point number in nats, rounded to six decimal places.\n\nYour program must implement the following steps in a numerically stable manner for general $n_x$ and $n_s$:\n\n- Construct the steady-state estimator for $\\hat{s}$.\n- Compute the linear response mapping from $x$ to $r$.\n- Compute the covariance of $x$ induced by the environment.\n- Compute the Mutual Information between $x$ and $r$ for the specified channel.\n- Aggregate all results into the required single-line output format.\n\nTest Suite:\n- Case 1 (happy path, correlated latents and non-diagonal $W$):\n  - $n_x = 2$, $n_s = 2$\n  - $W = \\begin{bmatrix} 1.0  0.4 \\\\ 0.6  1.2 \\end{bmatrix}$\n  - $C_s = \\begin{bmatrix} 1.0  0.2 \\\\ 0.2  0.8 \\end{bmatrix}$\n  - $\\sigma_x^2 = 0.1$\n  - $\\sigma_e^2 = 0.05$\n\n- Case 2 (boundary, identity generative model):\n  - $n_x = 2$, $n_s = 2$\n  - $W = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$\n  - $C_s = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$\n  - $\\sigma_x^2 = 0.01$\n  - $\\sigma_e^2 = 0.1$\n\n- Case 3 (edge, very high readout noise):\n  - $n_x = 2$, $n_s = 2$\n  - $W = \\begin{bmatrix} 1.0  0.4 \\\\ 0.6  1.2 \\end{bmatrix}$\n  - $C_s = \\begin{bmatrix} 1.0  0.2 \\\\ 0.2  0.8 \\end{bmatrix}$\n  - $\\sigma_x^2 = 0.1$\n  - $\\sigma_e^2 = 100.0$\n\n- Case 4 (coverage, overcomplete latent dimension):\n  - $n_x = 2$, $n_s = 3$\n  - $W = \\begin{bmatrix} 1.0  0.0  0.5 \\\\ 0.0  1.0  0.5 \\end{bmatrix}$\n  - $C_s = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.0  1.0  0.0 \\\\ 0.0  0.0  10.0 \\end{bmatrix}$\n  - $\\sigma_x^2 = 0.5$\n  - $\\sigma_e^2 = 0.2$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\").\n- Each result must be the Mutual Information in nats for the corresponding test case, rounded to six decimal places as a float.",
            "solution": "The user wants me to solve a problem based on the Efficient Coding Hypothesis. I need to:\n1.  Critically validate the problem statement.\n2.  If valid, derive a mathematical solution.\n3.  Implement the solution in Python to solve a suite of test cases.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n- **Latent Stimulus**: $s \\in \\mathbb{R}^{n_s}$, with $n_s \\ge 1$. Distribution is $s \\sim \\mathcal{N}(0, C_s)$, where $C_s \\in \\mathbb{R}^{n_s \\times n_s}$ is a symmetric positive definite covariance matrix.\n- **Sensory Input**: $x \\in \\mathbb{R}^{n_x}$, with $n_x \\ge 1$.\n- **Generative Model**: $x = W s + n_x$, where $W \\in \\mathbb{R}^{n_x \\times n_s}$ is a fixed generative matrix.\n- **Sensory Noise**: $n_x \\sim \\mathcal{N}(0, \\sigma_x^2 I_{n_x})$, where $\\sigma_x^2  0$ and $I_{n_x}$ is the $n_x \\times n_x$ identity matrix.\n- **Prediction**: $\\hat{x} = W \\hat{s}$.\n- **Prediction Error**: $e = x - \\hat{x}$.\n- **Internal Representation**: $\\hat{s}$ is the Maximum A Posteriori (MAP) estimate of $s$ given $x$. This is equivalent to minimizing a quadratic energy function consistent with the specified Gaussian prior and likelihood. Computation is at steady state.\n- **Transmitted Code**: $r = e + n_e$.\n- **Readout Noise**: $n_e \\sim \\mathcal{N}(0, \\sigma_e^2 I_{n_x})$, where $\\sigma_e^2  0$ and $n_e$ is independent of all other variables.\n- **Task**: Compute the information throughput, defined as the Mutual Information $I(x; r)$, in nats.\n- **Test Suite**: Four specific sets of parameters ($n_x, n_s, W, C_s, \\sigma_x^2, \\sigma_e^2$) are provided.\n\n#### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is firmly rooted in computational neuroscience, specifically the theory of predictive coding and the efficient coding hypothesis. It uses standard principles of Bayesian inference (MAP estimation) and information theory (mutual information) for linear-Gaussian systems. The model is a well-established formalization (e.g., the Kalman filter family of models, sparse coding models). The framework is scientifically sound.\n- **Well-Posed**: The problem is well-posed. The use of a Gaussian prior and likelihood leads to a unique MAP estimate. The conditions on the covariance matrices ($C_s$ is positive definite) and noise variances ($\\sigma_x^2  0, \\sigma_e^2  0$) ensure that all required matrix inverses and determinants are well-defined and positive, leading to a unique, stable, and meaningful solution for the mutual information.\n- **Objective**: The problem is stated in precise mathematical language. All terms are formally defined, and the objectives are quantitative. There is no subjective or ambiguous language.\n- **Completeness and Consistency**: The problem is self-contained. All necessary variables, distributions, and relationships are defined. There are no contradictions in the provided information.\n\n#### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined theoretical exercise in statistical signal processing applied to a model of neural coding. I will proceed with the detailed derivation and solution.\n\n### Solution Derivation\n\nThe solution requires a step-by-step derivation of the information throughput, $I(x; r)$. This is accomplished by first finding the optimal internal representation $\\hat{s}$, then characterizing the processing channel from the sensory input $x$ to the transmitted code $r$, and finally applying the formula for mutual information in a linear-Gaussian channel.\n\n#### 1. Derivation of the MAP Estimator $\\hat{s}$\nThe internal representation $\\hat{s}$ is the Maximum A Posteriori (MAP) estimate of the latent stimulus $s$ given the sensory input $x$. Using Bayes' theorem, the posterior probability is:\n$$p(s|x) \\propto p(x|s) p(s)$$\nThe problem specifies the distributions for the likelihood $p(x|s)$ and the prior $p(s)$:\n- The prior is $s \\sim \\mathcal{N}(0, C_s)$, so its probability density function is $p(s) = \\frac{1}{\\sqrt{(2\\pi)^{n_s}\\det(C_s)}} \\exp\\left(-\\frac{1}{2}s^T C_s^{-1} s\\right)$.\n- The generative model $x = Ws + n_x$ with $n_x \\sim \\mathcal{N}(0, \\sigma_x^2 I_{n_x})$ means the likelihood of $x$ given $s$ is $x|s \\sim \\mathcal{N}(Ws, \\sigma_x^2 I_{n_x})$. Its density is $p(x|s) = \\frac{1}{\\sqrt{(2\\pi\\sigma_x^2)^{n_x}}} \\exp\\left(-\\frac{1}{2\\sigma_x^2}(x-Ws)^T(x-Ws)\\right)$.\n\nTo find the MAP estimate, we maximize the log-posterior, which is equivalent to minimizing its negative (an energy function), ignoring constant terms:\n$$E(s) = -\\log p(s|x) = \\frac{1}{2\\sigma_x^2}(x-Ws)^T(x-Ws) + \\frac{1}{2}s^T C_s^{-1} s + \\text{const.}$$\nWe find the minimum by setting the gradient of $E(s)$ with respect to $s$ to zero:\n$$\\nabla_s E(s) = \\frac{1}{2\\sigma_x^2} \\nabla_s(s^T W^T W s - 2x^T W s) + \\frac{1}{2} \\nabla_s(s^T C_s^{-1} s) = 0$$\nUsing standard matrix calculus identities, we get:\n$$\\nabla_s E(s) = \\frac{1}{\\sigma_x^2}(W^T W s - W^T x) + C_s^{-1} s = 0$$\nSolving for $s$ (which is our estimator $\\hat{s}$):\n$$\\left(\\frac{1}{\\sigma_x^2}W^T W + C_s^{-1}\\right) \\hat{s} = \\frac{1}{\\sigma_x^2} W^T x$$\n$$\\hat{s} = \\left(W^T W + \\sigma_x^2 C_s^{-1}\\right)^{-1} W^T x$$\nThis shows that $\\hat{s}$ is a linear function of $x$.\n\n#### 2. Linear Mapping from Sensory Input $x$ to Representation $r$\nThe transmitted code $r$ is defined as $r = e + n_e$, where $e = x - \\hat{x} = x - W\\hat{s}$. We can express $e$ as a linear transformation of $x$ by substituting the expression for $\\hat{s}$:\n$$e = x - W \\left( (W^T W + \\sigma_x^2 C_s^{-1})^{-1} W^T \\right) x$$\n$$e = \\left( I_{n_x} - W (W^T W + \\sigma_x^2 C_s^{-1})^{-1} W^T \\right) x$$\nLet us define the linear operator $L_e \\in \\mathbb{R}^{n_x \\times n_x}$ as:\n$$L_e = I_{n_x} - W (W^T W + \\sigma_x^2 C_s^{-1})^{-1} W^T$$\nThe full channel from $x$ to $r$ is then given by:\n$$r = L_e x + n_e$$\nSince $x$ is a Gaussian random variable (as a linear transform of Gaussian variables $s$ and $n_x$) and $n_e$ is an independent Gaussian noise, this describes a linear-Gaussian communication channel.\n\n#### 3. Information Throughput Calculation\nThe information throughput is the mutual information $I(x; r)$. For continuous variables, this is given by the difference in entropies:\n$$I(x; r) = H(r) - H(r|x)$$\nThe entropy of a multivariate Gaussian variable $Z \\sim \\mathcal{N}(\\mu, \\Sigma)$ is $H(Z) = \\frac{1}{2}\\log\\left(\\det(2\\pi e \\Sigma)\\right)$.\n\nFirst, we find the distribution of $r$ conditioned on $x$. From $r = L_e x + n_e$ and $n_e \\sim \\mathcal{N}(0, \\sigma_e^2 I_{n_x})$, we have:\n$$r|x \\sim \\mathcal{N}(L_e x, \\sigma_e^2 I_{n_x})$$\nThe covariance of this conditional distribution, $\\Sigma_{r|x}$, is $\\sigma_e^2 I_{n_x}$. The conditional entropy $H(r|x)$ is constant for all $x$:\n$$H(r|x) = \\frac{1}{2}\\log\\left(\\det(2\\pi e \\sigma_e^2 I_{n_x})\\right)$$\n\nNext, we find the marginal distribution of $r$. Since $E[s]=0$ and $E[n_x]=0$, we have $E[x] = E[Ws+n_x] = 0$. This implies $E[r] = E[L_e x + n_e] = 0$. The covariance of $r$, $\\Sigma_r$, is:\n$$\\Sigma_r = \\text{Cov}(r) = E[rr^T] = E[(L_e x + n_e)(L_e x + n_e)^T]$$\nSince $x$ (which depends on $s$ and $n_x$) and $n_e$ are independent, the cross-terms vanish:\n$$\\Sigma_r = L_e E[xx^T] L_e^T + E[n_e n_e^T] = L_e \\Sigma_x L_e^T + \\Sigma_{n_e}$$\nHere, $\\Sigma_x$ is the covariance of the input $x$ and $\\Sigma_{n_e} = \\sigma_e^2 I_{n_x}$. We find $\\Sigma_x$:\n$$\\Sigma_x = E[xx^T] = E[(Ws + n_x)(Ws + n_x)^T]$$\nSince $s$ and $n_x$ are independent:\n$$\\Sigma_x = W E[ss^T] W^T + E[n_x n_x^T] = W C_s W^T + \\sigma_x^2 I_{n_x}$$\nThus, the covariance of $r$ is:\n$$\\Sigma_r = L_e (W C_s W^T + \\sigma_x^2 I_{n_x}) L_e^T + \\sigma_e^2 I_{n_x}$$\nThe entropy of $r$ is $H(r) = \\frac{1}{2}\\log\\left(\\det(2\\pi e \\Sigma_r)\\right)$.\n\nNow, we compute the mutual information:\n$$I(x; r) = H(r) - H(r|x) = \\frac{1}{2}\\log\\left(\\det(2\\pi e \\Sigma_r)\\right) - \\frac{1}{2}\\log\\left(\\det(2\\pi e \\sigma_e^2 I_{n_x})\\right)$$\n$$I(x; r) = \\frac{1}{2} \\left( \\log(\\det(\\Sigma_r)) - \\log(\\det(\\sigma_e^2 I_{n_x})) \\right) = \\frac{1}{2} \\log\\left(\\frac{\\det(\\Sigma_r)}{\\det(\\sigma_e^2 I_{n_x})}\\right)$$\n$$I(x; r) = \\frac{1}{2} \\log\\left(\\det\\left(\\Sigma_r (\\sigma_e^2 I_{n_x})^{-1}\\right)\\right) = \\frac{1}{2} \\log\\left(\\det\\left(\\frac{1}{\\sigma_e^2}\\Sigma_r\\right)\\right)$$\nSubstituting the expression for $\\Sigma_r$:\n$$I(x; r) = \\frac{1}{2} \\log\\left(\\det\\left(\\frac{1}{\\sigma_e^2} (L_e \\Sigma_x L_e^T + \\sigma_e^2 I_{n_x}) \\right)\\right)$$\n$$I(x; r) = \\frac{1}{2} \\log\\left(\\det\\left(I_{n_x} + \\frac{1}{\\sigma_e^2} L_e \\Sigma_x L_e^T \\right)\\right)$$\nThis is the final expression for the information throughput. The matrix $I_{n_x} + \\frac{1}{\\sigma_e^2} L_e \\Sigma_x L_e^T$ is positive definite, so its determinant is positive. Furthermore, the determinant is greater than or equal to $1$, ensuring that the mutual information is non-negative.\n\n#### 4. Summary of Computational Steps\n1.  Given $W, C_s, \\sigma_x^2, \\sigma_e^2, n_x, n_s$.\n2.  Compute $C_s^{-1}$.\n3.  Compute $L_e = I_{n_x} - W (W^T W + \\sigma_x^2 C_s^{-1})^{-1} W^T$.\n4.  Compute $\\Sigma_x = W C_s W^T + \\sigma_x^2 I_{n_x}$.\n5.  Form the matrix $B = I_{n_x} + \\frac{1}{\\sigma_e^2} L_e \\Sigma_x L_e^T$.\n6.  Calculate the mutual information as $I(x; r) = \\frac{1}{2} \\log(\\det(B))$. For numerical stability, this is computed using `0.5 * numpy.linalg.slogdet(B)[1]`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    test_cases = [\n        {\n            \"nx\": 2, \"ns\": 2,\n            \"W\": [[1.0, 0.4], [0.6, 1.2]],\n            \"Cs\": [[1.0, 0.2], [0.2, 0.8]],\n            \"sigma_x_sq\": 0.1,\n            \"sigma_e_sq\": 0.05\n        },\n        {\n            \"nx\": 2, \"ns\": 2,\n            \"W\": [[1.0, 0.0], [0.0, 1.0]],\n            \"Cs\": [[1.0, 0.0], [0.0, 1.0]],\n            \"sigma_x_sq\": 0.01,\n            \"sigma_e_sq\": 0.1\n        },\n        {\n            \"nx\": 2, \"ns\": 2,\n            \"W\": [[1.0, 0.4], [0.6, 1.2]],\n            \"Cs\": [[1.0, 0.2], [0.2, 0.8]],\n            \"sigma_x_sq\": 0.1,\n            \"sigma_e_sq\": 100.0\n        },\n        {\n            \"nx\": 2, \"ns\": 3,\n            \"W\": [[1.0, 0.0, 0.5], [0.0, 1.0, 0.5]],\n            \"Cs\": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 10.0]],\n            \"sigma_x_sq\": 0.5,\n            \"sigma_e_sq\": 0.2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_throughput(\n            case[\"nx\"], case[\"ns\"], case[\"W\"], case[\"Cs\"],\n            case[\"sigma_x_sq\"], case[\"sigma_e_sq\"]\n        )\n        results.append(result)\n\n    # Format the final output string.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef compute_throughput(nx, ns, W_list, Cs_list, sigma_x_sq, sigma_e_sq):\n    \"\"\"\n    Computes the information throughput for a single test case.\n\n    Args:\n        nx (int): Dimension of sensory input x.\n        ns (int): Dimension of latent stimulus s.\n        W_list (list): The generative matrix W.\n        Cs_list (list): The stimulus covariance matrix Cs.\n        sigma_x_sq (float): Variance of sensory noise.\n        sigma_e_sq (float): Variance of readout noise.\n\n    Returns:\n        float: The mutual information I(x; r) in nats.\n    \"\"\"\n    # Convert inputs to NumPy arrays for matrix operations.\n    W = np.array(W_list, dtype=np.float64)\n    Cs = np.array(Cs_list, dtype=np.float64)\n    I_nx = np.identity(nx, dtype=np.float64)\n    \n    # 1. Compute the inverse of the stimulus covariance matrix Cs.\n    # Cs is guaranteed to be symmetric positive definite.\n    try:\n        Cs_inv = np.linalg.inv(Cs)\n    except np.linalg.LinAlgError:\n        # Should not be reached given problem constraints.\n        return np.nan\n\n    # 2. Compute the linear operator Le that maps x to the prediction error e.\n    # Le = I_nx - W * (W.T * W + sigma_x_sq * Cs_inv)^-1 * W.T\n    # The term to be inverted is guaranteed to be positive definite.\n    precision_matrix_s = W.T @ W + sigma_x_sq * Cs_inv\n    try:\n        s_hat_mapper_from_Wtx = np.linalg.inv(precision_matrix_s)\n    except np.linalg.LinAlgError:\n        # Should not be reached.\n        return np.nan\n        \n    Le = I_nx - W @ s_hat_mapper_from_Wtx @ W.T\n    \n    # 3. Compute the covariance matrix of the sensory input x.\n    # Sigma_x = W * Cs * W.T + sigma_x_sq * I_nx\n    Sigma_x = W @ Cs @ W.T + sigma_x_sq * I_nx\n    \n    # 4. Compute the final mutual information I(x; r).\n    # I(x; r) = 0.5 * log(det(I_nx + (1/sigma_e_sq) * Le * Sigma_x * Le.T))\n    \n    # Form the matrix B for which we need the log-determinant.\n    matrix_B = I_nx + (1.0 / sigma_e_sq) * (Le @ Sigma_x @ Le.T)\n    \n    # Use slogdet for numerical stability. It returns (sign, log(abs(det))).\n    # For a positive definite matrix, sign must be 1.\n    sign, log_det_B = np.linalg.slogdet(matrix_B)\n    \n    # The theory guarantees det(B) = 1, so log_det_B = 0.\n    # Any negative result is due to numerical precision errors and should be clamped to 0.\n    if sign = 0 or log_det_B  0:\n        mi = 0.0\n    else:\n        mi = 0.5 * log_det_B\n        \n    return mi\n\n# Run the solver.\nsolve()\n```"
        }
    ]
}