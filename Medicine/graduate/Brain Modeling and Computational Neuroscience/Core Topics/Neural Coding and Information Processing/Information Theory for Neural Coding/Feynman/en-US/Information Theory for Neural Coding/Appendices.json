{
    "hands_on_practices": [
        {
            "introduction": "This first exercise explores one of the most fundamental models in information theory and its application to neural coding. By modeling a neuron's response as a linear function of a stimulus corrupted by additive Gaussian noise, we can derive a closed-form expression for the mutual information between stimulus and response. This practice  provides a foundational understanding of how information transmission is limited by the signal-to-noise ratio ($SNR$) in a simple, continuous system.",
            "id": "5037464",
            "problem": "In a simplified neural coding model for a single sensory neuron, a scalar stimulus $S$ represents a deviation around baseline and is modeled as a zero-mean Gaussian random variable with variance $\\sigma_{S}^{2}$, that is $S \\sim \\mathcal{N}(0,\\sigma_{S}^{2})$. The neuronâ€™s response $R$ is assumed to be linear and corrupted by independent additive noise $N \\sim \\mathcal{N}(0,\\sigma_{N}^{2})$, with $S$ and $N$ independent, so that $R = S + N$. Using as foundational starting points the definition of mutual information in terms of differential entropies and well-tested properties of Gaussian random variables, derive a closed-form expression for the mutual information $I(S;R)$ between the stimulus and the response in this model. Use natural logarithms so that the information is measured in nats. Express your final answer symbolically in terms of $\\sigma_{S}^{2}$ and $\\sigma_{N}^{2}$, and do not approximate. The final answer must be a single closed-form analytic expression.",
            "solution": "The problem is valid as it is scientifically grounded in information theory and statistics, well-posed with a unique and meaningful solution, and stated using objective, formal language. All necessary information is provided, and there are no contradictions or ambiguities.\n\nThe objective is to derive a closed-form expression for the mutual information $I(S;R)$ between a stimulus $S$ and a neural response $R$. The model is defined by the following givens:\n1.  The stimulus $S$ is a Gaussian random variable with zero mean and variance $\\sigma_{S}^{2}$, denoted as $S \\sim \\mathcal{N}(0, \\sigma_{S}^{2})$.\n2.  The response $R$ is given by a linear relationship $R = S + N$.\n3.  The noise $N$ is an independent Gaussian random variable with zero mean and variance $\\sigma_{N}^{2}$, denoted as $N \\sim \\mathcal{N}(0, \\sigma_{N}^{2})$.\n4.  The stimulus $S$ and the noise $N$ are statistically independent.\n\nWe begin with the definition of mutual information in terms of differential entropies. For continuous random variables $S$ and $R$, the mutual information is given by:\n$$\nI(S;R) = H(R) - H(R|S)\n$$\nwhere $H(R)$ is the differential entropy of the response $R$, and $H(R|S)$ is the conditional differential entropy of $R$ given $S$. We will calculate each of these terms separately.\n\nFirst, we determine the distribution of the response $R$. Since $R$ is the sum of two independent Gaussian random variables, $S$ and $N$, $R$ itself is a Gaussian random variable.\nThe mean of $R$ is the sum of the means of $S$ and $N$:\n$$\nE[R] = E[S+N] = E[S] + E[N] = 0 + 0 = 0\n$$\nThe variance of $R$ is the sum of the variances of $S$ and $N$, due to their independence:\n$$\n\\text{Var}(R) = \\text{Var}(S+N) = \\text{Var}(S) + \\text{Var}(N) = \\sigma_{S}^{2} + \\sigma_{N}^{2}\n$$\nThus, the distribution of the response is $R \\sim \\mathcal{N}(0, \\sigma_{S}^{2} + \\sigma_{N}^{2})$.\n\nThe differential entropy $H(X)$ of a Gaussian random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, when using the natural logarithm (measured in nats), is given by the formula:\n$$\nH(X) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)\n$$\nApplying this formula to the response variable $R$, we find its entropy:\n$$\nH(R) = \\frac{1}{2} \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right)\n$$\n\nNext, we calculate the conditional entropy $H(R|S)$. This term represents the uncertainty remaining in $R$ when the value of $S$ is known. If we condition on a specific stimulus value $S=s$, the response equation becomes $R = s + N$. Since $s$ is a fixed value in this context, the randomness in $R$ is due solely to the noise $N$. The distribution of $R$ given $S=s$ is a Gaussian distribution with mean $E[R|S=s] = E[s+N] = s + E[N] = s$ and variance $\\text{Var}(R|S=s) = \\text{Var}(s+N) = \\text{Var}(N) = \\sigma_{N}^{2}$. So, $R|S=s \\sim \\mathcal{N}(s, \\sigma_{N}^{2})$.\n\nThe entropy of this conditional distribution is:\n$$\nH(R|S=s) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\nNote that this expression is a constant and does not depend on the specific value of $s$. The conditional entropy $H(R|S)$ is the expectation of $H(R|S=s)$ over all possible values of $S$:\n$$\nH(R|S) = E_S[H(R|S=s)] = E_S\\left[\\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\\right]\n$$\nSince the expression inside the expectation is constant with respect to $S$, the expectation is simply the constant itself:\n$$\nH(R|S) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\n\nFinally, we substitute the expressions for $H(R)$ and $H(R|S)$ back into the definition of mutual information:\n$$\nI(S;R) = H(R) - H(R|S) = \\frac{1}{2} \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right) - \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\nUsing the logarithmic property $\\ln(a) - \\ln(b) = \\ln(a/b)$, we can simplify the expression:\n$$\nI(S;R) = \\frac{1}{2} \\left[ \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right) - \\ln(2\\pi e \\sigma_{N}^{2}) \\right] = \\frac{1}{2} \\ln\\left(\\frac{2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})}{2\\pi e \\sigma_{N}^{2}}\\right)\n$$\nThe term $2\\pi e$ cancels from the numerator and denominator:\n$$\nI(S;R) = \\frac{1}{2} \\ln\\left(\\frac{\\sigma_{S}^{2} + \\sigma_{N}^{2}}{\\sigma_{N}^{2}}\\right)\n$$\nThis can be written in a more standard form by separating the fraction:\n$$\nI(S;R) = \\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}\\right)\n$$\nThe term $\\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}$ is the signal-to-noise ratio (SNR) of the system. This result is a fundamental formula in information theory for a Gaussian channel.",
            "answer": "$$\n\\boxed{\\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}\\right)}\n$$"
        },
        {
            "introduction": "Building on foundational models, we now examine how intrinsic properties of neural firing affect information content. Real neurons exhibit trial-to-trial variability, a feature often quantified by the Fano factor, which compares the variance of the spike count to its mean. This exercise  guides you through an asymptotic analysis to quantify how this variability impacts the conditional entropy of the neural response, revealing that more reliable, less variable (sub-Poisson) firing leads to lower entropy and thus a higher capacity for encoding information.",
            "id": "3990303",
            "problem": "Consider a neuron observed in a fixed counting window of duration $T$ under a single stimulus condition $S$. Let the spike count be a discrete random variable $R \\in \\{0, 1, 2, \\dots\\}$ with conditional mean $\\mathbb{E}[R \\mid S] = m$, where $m$ is large. Define the conditional entropy $H(R \\mid S)$ by the standard information-theoretic definition using the natural logarithm.\n\nAssume two spike-count models at fixed mean $m$:\n- A Poisson model, in which trial-to-trial variability satisfies the property that the Fano factor (variance-to-mean ratio) equals $1$.\n- A sub-Poisson model, in which the Fano factor $F$ satisfies $0 < F < 1$ and the conditional variance scales linearly with the mean, $\\operatorname{Var}(R \\mid S) = F m$.\n\nStarting from core definitions of entropy and well-tested asymptotic reasoning appropriate for large $m$ (for example, invoking the Central Limit Theorem (CLT) to justify a smooth unimodal approximation of the spike-count distribution and using continuous approximations where appropriate), derive the leading-order analytical expression for the change in conditional entropy induced by increasing trial-to-trial variability away from the sub-Poisson case to the Poisson case, at fixed mean $m$. Concretely, derive an explicit closed-form expression for\n$$\n\\Delta H \\equiv H_{\\text{Poisson}}(R \\mid S) - H_{\\text{sub-Poisson},F}(R \\mid S)\n$$\nas a function of the Fano factor $F$, under the large-$m$ asymptotic described above.\n\nUse the natural logarithm throughout and report $\\Delta H$ in nats. Your final answer must be a single closed-form analytic expression in terms of $F$ only (no dependence on $m$ should remain in the leading-order answer), with no rounding.",
            "solution": "The problem asks for the change in conditional entropy, $\\Delta H \\equiv H_{\\text{Poisson}}(R \\mid S) - H_{\\text{sub-Poisson},F}(R \\mid S)$, for a neuron's spike count $R$ under a stimulus $S$. The mean spike count is $\\mathbb{E}[R \\mid S] = m$, which is assumed to be large.\n\nThe conditional entropy of a discrete random variable $R$ with probability mass function $p(k) = P(R=k \\mid S)$ is defined as:\n$$\nH(R \\mid S) = -\\sum_{k=0}^{\\infty} p(k) \\ln(p(k))\n$$\nThe problem states that for a large mean count $m$, we can use an asymptotic approximation. Since the spike count $R$ can be thought of as a sum of many smaller events, the Central Limit Theorem (CLT) justifies approximating its discrete probability mass function $p(k)$ with a continuous, unimodal distribution, specifically a Gaussian (Normal) distribution.\n\nLet $X$ be a continuous random variable that approximates the discrete random variable $R$. The parameters of this Gaussian distribution will be matched to the moments of $R$. The mean is $\\mu = \\mathbb{E}[R \\mid S] = m$, and the variance is $\\sigma^2 = \\operatorname{Var}(R \\mid S)$. The probability density function (PDF) of $X$ is given by:\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-m)^2}{2\\sigma^2}\\right)\n$$\nFor a discrete integer-valued random variable with a large mean that is well-approximated by a continuous PDF $f(x)$, its entropy can be approximated by the differential entropy of the continuous variable. This is because the sum in the discrete entropy definition can be approximated by an integral:\n$$\nH(R \\mid S) = -\\sum_{k=0}^{\\infty} p(k) \\ln(p(k)) \\approx -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) dx\n$$\nThe integral on the right is the definition of the differential entropy, $H(X)$, of the continuous random variable $X$. This approximation provides the leading-order term for the entropy in the large-$m$ limit.\n\nThe differential entropy of a Gaussian distribution with variance $\\sigma^2$ is a standard result in information theory, given in nats by:\n$$\nH_{\\text{Gaussian}}(X) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)\n$$\nWe apply this formula to the two cases described in the problem.\n\nCase 1: The Poisson Model\nIn the Poisson model, the variance is equal to the mean. The Fano factor is $1$.\n$$\n\\operatorname{Var}_{\\text{Poisson}}(R \\mid S) = m\n$$\nThus, we approximate the spike count distribution with a Gaussian distribution having mean $m$ and variance $\\sigma_{\\text{Poisson}}^2 = m$. The entropy is:\n$$\nH_{\\text{Poisson}}(R \\mid S) \\approx \\frac{1}{2}\\ln(2\\pi e m)\n$$\n\nCase 2: The Sub-Poisson Model\nIn this model, the Fano factor is $F$, where $0 < F < 1$. The problem specifies that the variance is:\n$$\n\\operatorname{Var}_{\\text{sub-Poisson},F}(R \\mid S) = F m\n$$\nWe approximate this distribution with a Gaussian having mean $m$ and variance $\\sigma_{\\text{sub-Poisson}}^2 = F m$. The entropy is:\n$$\nH_{\\text{sub-Poisson},F}(R \\mid S) \\approx \\frac{1}{2}\\ln(2\\pi e F m)\n$$\n\nNow, we compute the change in conditional entropy, $\\Delta H$, by taking the difference between these two asymptotic expressions.\n$$\n\\Delta H = H_{\\text{Poisson}}(R \\mid S) - H_{\\text{sub-Poisson},F}(R \\mid S)\n$$\nSubstituting the derived expressions:\n$$\n\\Delta H \\approx \\frac{1}{2}\\ln(2\\pi e m) - \\frac{1}{2}\\ln(2\\pi e F m)\n$$\nUsing the property of logarithms, $\\ln(a) - \\ln(b) = \\ln(a/b)$, we can simplify the expression:\n$$\n\\Delta H \\approx \\frac{1}{2} \\left[ \\ln(2\\pi e m) - \\ln(2\\pi e F m) \\right] = \\frac{1}{2} \\ln\\left(\\frac{2\\pi e m}{2\\pi e F m}\\right)\n$$\nThe terms $2$, $\\pi$, $e$, and $m$ cancel out in the fraction:\n$$\n\\Delta H \\approx \\frac{1}{2} \\ln\\left(\\frac{1}{F}\\right)\n$$\nUsing another property of logarithms, $\\ln(1/F) = -\\ln(F)$, we arrive at the final expression for the leading-order change in entropy:\n$$\n\\Delta H = -\\frac{1}{2}\\ln(F)\n$$\nThis expression depends only on the Fano factor $F$, as required. Since $0 < F < 1$, $\\ln(F)$ is negative, and thus $\\Delta H$ is positive. This is scientifically consistent, as increasing variability from a sub-Poisson level ($F < 1$) to a Poisson level ($F=1$) should increase the uncertainty (entropy) of the spike count.",
            "answer": "$$\n\\boxed{-\\frac{1}{2}\\ln(F)}\n$$"
        },
        {
            "introduction": "The final practice bridges the gap between theoretical understanding and experimental application, where we must work with finite and noisy data. This computational exercise  challenges you to implement a 'plug-in' estimator to calculate mutual information from empirical spike counts. More importantly, you will use the nonparametric bootstrap, a cornerstone statistical technique in neuroscience, to compute confidence intervals and rigorously quantify the uncertainty in your information estimates.",
            "id": "5037495",
            "problem": "You are given discretized neural responses for a set of stimuli, where responses have been binned into integer categories. Let the stimulus be a discrete random variable $S$ taking $K$ values, and the binned neural response be a discrete random variable $R$ taking $B$ values. Each stimulus $s$ is presented for $T$ trials, and for each trial you observe one binned response value $r \\in \\{0,1,\\dots,B-1\\}$. Starting from the core definitions of empirical joint probability distribution, empirical marginal distributions, Shannon entropy, and conditional entropy, derive a principled plug-in estimator for the mutual information $I(S;R)$ in bits from these empirical frequencies. Implement a program that:\n- Computes the point estimate of $I(S;R)$ in bits using the derived plug-in estimator from the empirical data.\n- Applies a nonparametric bootstrap procedure that resamples trials with replacement independently within each stimulus $s$ (preserving $T$ trials per stimulus) to approximate the sampling distribution of the plug-in estimator of $I(S;R)$.\n- Uses the bootstrap distribution to compute a two-sided confidence interval with nominal coverage $95$ percent based on percentile cutoffs.\n\nYour program must use logarithms base $2$ to ensure the information is expressed in bits. The bootstrap must use exactly $B_{\\text{boot}}=400$ resamples and must initialize a deterministic pseudorandom number generator with seed $12345$ to make the results reproducible. For numerical stability, when evaluating any expression of the form $p \\log_{2}\\!\\big(p/q\\big)$, define the contribution to be $0$ whenever $p=0$.\n\nYour program must solve the following test suite. In every case, the response takes values in $\\{0,1,2,3\\}$, that is $B=4$, and each stimulus has the same number of trials $T$ within a case. For each case, you are given the per-stimulus lists of binned responses:\n\n- Case A (general, informative responses): $K=3$, $T=20$,\n  - Stimulus $s=0$: $[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2,2]$.\n  - Stimulus $s=1$: $[2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,1,1]$.\n  - Stimulus $s=2$: $[0,0,0,0,0,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3]$.\n- Case B (near-indistinguishable responses): $K=2$, $T=30$,\n  - Stimulus $s=0$: $[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3]$.\n  - Stimulus $s=1$: $[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3]$.\n- Case C (small-sample and edge counts): $K=4$, $T=10$,\n  - Stimulus $s=0$: $[0,0,0,0,0,0,0,0,0,0]$.\n  - Stimulus $s=1$: $[1,1,1,1,1,1,1,1,1,1]$.\n  - Stimulus $s=2$: $[0,0,0,0,0,1,1,1,1,1]$.\n  - Stimulus $s=3$: $[2,2,2,2,2,3,3,3,3,3]$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, report three floating-point numbers: the lower bootstrap percentile bound, the point estimate, and the upper bootstrap percentile bound for $I(S;R)$, in bits, in this order. All numbers must be rounded to six decimal places. Therefore the output must be of the form `[L_{A},\\hat{I}_{A},U_{A},L_{B},\\hat{I}_{B},U_{B},L_{C},\\hat{I}_{C},U_{C}]`, where $L_{\\cdot}$ and $U_{\\cdot}$ denote the lower and upper bounds of the $95$ percent confidence interval and $\\hat{I}_{\\cdot}$ denotes the plug-in point estimate, all in bits.",
            "solution": "The problem statement has been critically validated and is deemed to be **valid**. It is scientifically grounded in information theory and statistics, well-posed with all necessary data and constraints provided, and formulated in objective, precise language. It presents a standard and meaningful task in computational neuroscience. We may therefore proceed with a solution.\n\nThe solution involves three main parts:\n1.  Derivation of the plug-in estimator for mutual information, $\\hat{I}(S;R)$.\n2.  Description of the nonparametric bootstrap procedure to estimate the sampling distribution of $\\hat{I}(S;R)$.\n3.  Construction of a percentile-based confidence interval from the bootstrap distribution.\n\n### 1. Derivation of the Plug-in Estimator for Mutual Information\n\nMutual information, $I(S;R)$, quantifies the statistical dependency between two random variables, the stimulus $S$ and the response $R$. It measures the reduction in uncertainty about one variable from observing the other. It is defined in terms of Shannon entropy:\n\n$$\nI(S;R) = H(R) - H(R|S)\n$$\n\nwhere all information-theoretic quantities are measured in bits, implying the use of base-$2$ logarithms.\n\n-   **Shannon Entropy of the Response**: $H(R)$ is the entropy of the marginal distribution of the responses, which measures the total uncertainty about the neural response, regardless of the stimulus. For a discrete variable $R$ that can take $B$ values $\\{r_0, ..., r_{B-1}\\}$, it is defined as:\n    $$\n    H(R) = - \\sum_{j=0}^{B-1} p(r_j) \\log_2 p(r_j)\n    $$\n-   **Conditional Entropy of the Response given the Stimulus**: $H(R|S)$ is the entropy of the response, averaged over all stimuli. It measures the remaining uncertainty about the response when the stimulus is known.\n    $$\n    H(R|S) = \\sum_{i=0}^{K-1} p(s_i) H(R|S=s_i)\n    $$\n    where $p(s_i)$ is the probability of stimulus $s_i$ and $H(R|S=s_i)$ is the entropy of the response distribution for that specific stimulus:\n    $$\n    H(R|S=s_i) = - \\sum_{j=0}^{B-1} p(r_j|s_i) \\log_2 p(r_j|s_i)\n    $$\n\nTo compute $I(S;R)$ from empirical data, we use the **plug-in method**, which replaces the true probabilities with their frequency-based estimates.\n\nLet the experimental data consist of $K$ stimuli, with each stimulus $s_i$ ($i \\in \\{0, \\dots, K-1\\}$) presented for $T$ trials. The neural response is binned into one of $B$ categories, $r_j$ ($j \\in \\{0, \\dots, B-1\\}$). The total number of trials is $N = K \\times T$.\n\nLet $N_{ij}$ be the empirical count of observing response $r_j$ when stimulus $s_i$ was presented.\nThe empirical probability estimates are:\n-   **Stimulus Probability**: The problem states each stimulus is presented for an equal number of trials. Thus, the probability of any given stimulus $s_i$ is uniform:\n    $$\n    \\hat{p}(s_i) = \\frac{T}{N} = \\frac{T}{KT} = \\frac{1}{K}\n    $$\n-   **Conditional Response Probability**: The probability of response $r_j$ given stimulus $s_i$ is the frequency of that response for that stimulus:\n    $$\n    \\hat{p}(r_j|s_i) = \\frac{N_{ij}}{T}\n    $$\n-   **Marginal Response Probability**: Using the law of total probability, the marginal probability of response $r_j$ is:\n    $$\n    \\hat{p}(r_j) = \\sum_{i=0}^{K-1} \\hat{p}(s_i) \\hat{p}(r_j|s_i) = \\sum_{i=0}^{K-1} \\frac{1}{K} \\frac{N_{ij}}{T} = \\frac{\\sum_{i=0}^{K-1} N_{ij}}{KT}\n    $$\n\nSubstituting these estimates into the entropy formulas yields the plug-in estimators. We adopt the convention that $p \\log_2 p = 0$ if $p=0$.\n\n-   **Estimator for $H(R)$**:\n    $$\n    \\hat{H}(R) = - \\sum_{j=0}^{B-1} \\hat{p}(r_j) \\log_2 \\hat{p}(r_j) = - \\sum_{j=0}^{B-1} \\left( \\frac{\\sum_{i=0}^{K-1} N_{ij}}{KT} \\right) \\log_2 \\left( \\frac{\\sum_{i=0}^{K-1} N_{ij}}{KT} \\right)\n    $$\n-   **Estimator for $H(R|S)$**:\n    First, we estimate the conditional entropy for each stimulus $s_i$:\n    $$\n    \\hat{H}(R|S=s_i) = - \\sum_{j=0}^{B-1} \\hat{p}(r_j|s_i) \\log_2 \\hat{p}(r_j|s_i) = - \\sum_{j=0}^{B-1} \\left( \\frac{N_{ij}}{T} \\right) \\log_2 \\left( \\frac{N_{ij}}{T} \\right)\n    $$\n    Then, we average these over all stimuli, weighted by $\\hat{p}(s_i) = 1/K$:\n    $$\n    \\hat{H}(R|S) = \\sum_{i=0}^{K-1} \\frac{1}{K} \\hat{H}(R|S=s_i) = \\frac{1}{K} \\sum_{i=0}^{K-1} \\left[ - \\sum_{j=0}^{B-1} \\frac{N_{ij}}{T} \\log_2 \\left( \\frac{N_{ij}}{T} \\right) \\right]\n    $$\nCombining these, the plug-in estimator for mutual information is:\n$$\n\\hat{I}(S;R) = \\hat{H}(R) - \\hat{H}(R|S)\n$$\n\n### 2. Bootstrap Procedure for Sampling Distribution\n\nThe plug-in estimate $\\hat{I}(S;R)$ is a single value computed from our specific dataset. As a statistic, it is subject to sampling variability. To quantify the uncertainty in this estimate, we use a nonparametric bootstrap procedure. The procedure approximates the sampling distribution of the estimator $\\hat{I}(S;R)$ by repeatedly resampling the observed data.\n\nThe procedure is as follows:\n1.  Set the number of bootstrap resamples, $B_{\\text{boot}} = 400$.\n2.  Initialize a pseudorandom number generator with a fixed seed ($12345$) for reproducibility.\n3.  For each bootstrap iteration $b = 1, \\dots, B_{\\text{boot}}$:\n    a.  Create a new, resampled dataset. The problem specifies resampling trials *independently within each stimulus*. This means for each stimulus $s_i$, we generate a new set of $T$ trials by drawing with replacement from the original $T$ observed responses for that stimulus.\n    b.  Using this resampled dataset, compute the count matrix $N_{ij}^*$ and then the bootstrap replicate of the mutual information, $\\hat{I}^*(S;R)_b$, using the same plug-in estimator formula derived above.\n4.  The collection of $B_{\\text{boot}}$ values, $\\{\\hat{I}^*(S;R)_1, \\dots, \\hat{I}^*(S;R)_{B_{\\text{boot}}}\\}$, forms an empirical approximation of the sampling distribution of $\\hat{I}(S;R)$.\n\n### 3. Confidence Interval Construction\n\nUsing the bootstrap distribution, we can construct a confidence interval for the true mutual information. The problem specifies the percentile method. For a two-sided confidence interval with $95\\%$ nominal coverage, we find the values that encompass the central $95\\%$ of the bootstrap distribution.\n\n1.  Sort the bootstrap replicates in ascending order: $\\hat{I}^*_{(1)} \\leq \\hat{I}^*_{(2)} \\leq \\dots \\leq \\hat{I}^*_{(B_{\\text{boot}})}$.\n2.  The lower bound of the confidence interval, $L$, is the $2.5$-th percentile of this distribution.\n3.  The upper bound of the confidence interval, $U$, is the $97.5$-th percentile of this distribution.\n4.  The resulting $95\\%$ confidence interval is $[L, U]$.\n\nThe final program will implement these steps for each test case, reporting the lower bound $L$, the point estimate $\\hat{I}(S;R)$ from the original data, and the upper bound $U$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_mi_plugin(data, B):\n    \"\"\"\n    Computes the plug-in estimate of mutual information I(S;R) in bits.\n\n    Args:\n        data (list of lists): A list where each inner list contains the\n                              binned responses for a single stimulus.\n        B (int): The number of response bins.\n\n    Returns:\n        float: The mutual information estimate in bits.\n    \"\"\"\n    K = len(data)\n    if K == 0:\n        return 0.0\n    T = len(data[0])\n    N = K * T\n    if N == 0:\n        return 0.0\n\n    # 1. Compute the count matrix N_sr of size (K, B)\n    # N_sr[s, r] = count of response r for stimulus s\n    N_sr = np.zeros((K, B), dtype=int)\n    for s_idx, stimulus_responses in enumerate(data):\n        for r_val in stimulus_responses:\n            N_sr[s_idx, r_val] += 1\n\n    # 2. Compute probability distributions\n    # p(r|s) = N_sr / T\n    # Note: Division by zero is not an issue here as T > 0 by problem structure.\n    p_r_given_s = N_sr / T\n\n    # p(r) = sum_s(N_sr) / N\n    N_r = np.sum(N_sr, axis=0)\n    p_r = N_r / N\n\n    # 3. Compute entropy H(R)\n    # H(R) = - sum_r p(r) log2(p(r))\n    # Using np.log2 with `where` handles the case p(r) = 0, where p*log(p) = 0.\n    H_R = -np.sum(p_r * np.log2(p_r, where=p_r > 0, out=np.zeros_like(p_r)))\n\n    # 4. Compute conditional entropy H(R|S)\n    # H(R|S) = sum_s p(s) H(R|S=s)\n    # Since p(s) = 1/K is uniform, H(R|S) is the mean of H(R|S=s).\n    # H(R|S=s) = - sum_r p(r|s) log2(p(r|s))\n    H_R_given_S_per_stimulus = -np.sum(p_r_given_s * np.log2(p_r_given_s, where=p_r_given_s > 0, out=np.zeros_like(p_r_given_s)), axis=1)\n    H_R_given_S = np.mean(H_R_given_S_per_stimulus)\n\n    # 5. Compute Mutual Information I(S;R) = H(R) - H(R|S)\n    mi = H_R - H_R_given_S\n    \n    # Due to floating point precision, very small negative numbers can occur\n    # when true MI is zero. By definition, MI is non-negative.\n    return max(0.0, mi)\n\ndef calculate_mi_and_ci(data, B, B_boot, seed):\n    \"\"\"\n    Calculates the point estimate and bootstrap confidence interval for mutual information.\n\n    Args:\n        data (list of lists): The raw response data for each stimulus.\n        B (int): The number of response bins.\n        B_boot (int): The number of bootstrap samples.\n        seed (int): The seed for the pseudorandom number generator.\n\n    Returns:\n        tuple: A tuple containing (lower_bound, point_estimate, upper_bound).\n    \"\"\"\n    # 1. Calculate the point estimate from the original data\n    point_estimate = compute_mi_plugin(data, B)\n\n    # 2. Perform nonparametric bootstrap\n    K = len(data)\n    T = len(data[0])\n    \n    rng = np.random.default_rng(seed)\n    bootstrap_mi_values = np.zeros(B_boot)\n\n    original_data_arrays = [np.array(d) for d in data]\n\n    for i in range(B_boot):\n        resampled_data = []\n        for s_idx in range(K):\n            # Resample trials with replacement, within each stimulus\n            resampled_indices = rng.integers(0, T, size=T)\n            resampled_trials = original_data_arrays[s_idx][resampled_indices].tolist()\n            resampled_data.append(resampled_trials)\n        \n        bootstrap_mi_values[i] = compute_mi_plugin(resampled_data, B)\n\n    # 3. Compute the 95% confidence interval using the percentile method\n    alpha = 0.05\n    lower_bound = np.percentile(bootstrap_mi_values, 100 * alpha / 2.0)\n    upper_bound = np.percentile(bootstrap_mi_values, 100 * (1 - alpha / 2.0))\n\n    return lower_bound, point_estimate, upper_bound\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A (general, informative responses)\n        {\n            \"data\": [\n                [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2,2],\n                [2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,1,1],\n                [0,0,0,0,0,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3],\n            ],\n            \"B\": 4,\n        },\n        # Case B (near-indistinguishable responses)\n        {\n            \"data\": [\n                [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3],\n                [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3],\n            ],\n            \"B\": 4,\n        },\n        # Case C (small-sample and edge counts)\n        {\n            \"data\": [\n                [0,0,0,0,0,0,0,0,0,0],\n                [1,1,1,1,1,1,1,1,1,1],\n                [0,0,0,0,0,1,1,1,1,1],\n                [2,2,2,2,2,3,3,3,3,3],\n            ],\n            \"B\": 4,\n        },\n    ]\n\n    B_boot = 400\n    seed = 12345\n    \n    all_results = []\n    for case in test_cases:\n        data = case[\"data\"]\n        B = case[\"B\"]\n        \n        lower, point, upper = calculate_mi_and_ci(data, B, B_boot, seed)\n        \n        all_results.append(f\"{lower:.6f}\")\n        all_results.append(f\"{point:.6f}\")\n        all_results.append(f\"{upper:.6f}\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}