{
    "hands_on_practices": [
        {
            "introduction": "理论的生命力在于实践。我们从一个基础的分析练习开始，探索最简单的连续神经编码模型之一。在这个练习中，我们将一个神经元的响应建模为刺激信号与高斯噪声的线性叠加，并推导出刺激和响应之间的互信息。这个过程不仅能加深对互信息定义的理解，还将揭示信息传输与信噪比（$SNR$）之间的深刻联系。",
            "id": "5037464",
            "problem": "在一个简化的单感觉神经元神经编码模型中，标量刺激 $S$ 表示围绕基线的偏差，并被建模为零均值、方差为 $\\sigma_{S}^{2}$ 的高斯随机变量，即 $S \\sim \\mathcal{N}(0,\\sigma_{S}^{2})$。神经元的响应 $R$ 被假设为线性的，并受到独立加性噪声 $N \\sim \\mathcal{N}(0,\\sigma_{N}^{2})$ 的干扰，其中 $S$ 和 $N$ 相互独立，因此 $R = S + N$。以互信息关于微分熵的定义和高斯随机变量的成熟性质为基础出发点，推导该模型中刺激与响应之间的互信息 $I(S;R)$ 的一个闭式表达式。使用自然对数，以便信息以奈特（nats）为单位进行度量。用 $\\sigma_{S}^{2}$ 和 $\\sigma_{N}^{2}$ 符号化地表示您的最终答案，并且不要进行近似。最终答案必须是一个单一的闭式解析表达式。",
            "solution": "该问题是有效的，因为它在信息论和统计学方面有科学依据，问题设定良好，具有唯一且有意义的解，并使用客观、正式的语言进行陈述。所有必要信息都已提供，没有矛盾或含糊之处。\n\n目标是推导刺激 $S$ 与神经响应 $R$ 之间互信息 $I(S;R)$ 的闭式表达式。该模型由以下给定条件定义：\n1.  刺激 $S$ 是一个零均值、方差为 $\\sigma_{S}^{2}$ 的高斯随机变量，记作 $S \\sim \\mathcal{N}(0, \\sigma_{S}^{2})$。\n2.  响应 $R$ 由线性关系 $R = S + N$ 给出。\n3.  噪声 $N$ 是一个独立的零均值、方差为 $\\sigma_{N}^{2}$ 的高斯随机变量，记作 $N \\sim \\mathcal{N}(0, \\sigma_{N}^{2})$。\n4.  刺激 $S$ 和噪声 $N$ 在统计上是独立的。\n\n我们从互信息关于微分熵的定义开始。对于连续随机变量 $S$ 和 $R$，互信息由下式给出：\n$$\nI(S;R) = H(R) - H(R|S)\n$$\n其中 $H(R)$ 是响应 $R$ 的微分熵，$H(R|S)$ 是在给定 $S$ 的条件下 $R$ 的条件微分熵。我们将分别计算这两项。\n\n首先，我们确定响应 $R$ 的分布。由于 $R$ 是两个独立高斯随机变量 $S$ 和 $N$ 的和，因此 $R$ 本身也是一个高斯随机变量。\n$R$ 的均值是 $S$ 和 $N$ 的均值之和：\n$$\nE[R] = E[S+N] = E[S] + E[N] = 0 + 0 = 0\n$$\n$R$ 的方差是 $S$ 和 $N$ 的方差之和，这是由于它们的独立性：\n$$\n\\text{Var}(R) = \\text{Var}(S+N) = \\text{Var}(S) + \\text{Var}(N) = \\sigma_{S}^{2} + \\sigma_{N}^{2}\n$$\n因此，响应的分布为 $R \\sim \\mathcal{N}(0, \\sigma_{S}^{2} + \\sigma_{N}^{2})$。\n\n当使用自然对数（以奈特为单位）时，一个高斯随机变量 $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ 的微分熵 $H(X)$ 由以下公式给出：\n$$\nH(X) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)\n$$\n将此公式应用于响应变量 $R$，我们得到其熵：\n$$\nH(R) = \\frac{1}{2} \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right)\n$$\n\n接下来，我们计算条件熵 $H(R|S)$。该项表示在已知 $S$ 的值时，$R$ 中剩余的不确定性。如果我们以特定的刺激值 $S=s$ 为条件，响应方程变为 $R = s + N$。由于在这种情况下 $s$ 是一个固定值，因此 $R$ 的随机性完全来自于噪声 $N$。给定 $S=s$ 时 $R$ 的分布是一个高斯分布，其均值为 $E[R|S=s] = E[s+N] = s + E[N] = s$，方差为 $\\text{Var}(R|S=s) = \\text{Var}(s+N) = \\text{Var}(N) = \\sigma_{N}^{2}$。所以，$R|S=s \\sim \\mathcal{N}(s, \\sigma_{N}^{2})$。\n\n这个条件分布的熵是：\n$$\nH(R|S=s) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\n请注意，该表达式是一个常数，不依赖于 $s$ 的具体值。条件熵 $H(R|S)$ 是 $H(R|S=s)$ 对所有可能的 $S$ 值的期望：\n$$\nH(R|S) = E_S[H(R|S=s)] = E_S\\left[\\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\\right]\n$$\n由于期望内的表达式相对于 $S$ 是一个常数，因此期望值就是该常数本身：\n$$\nH(R|S) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\n\n最后，我们将 $H(R)$ 和 $H(R|S)$ 的表达式代入互信息的定义中：\n$$\nI(S;R) = H(R) - H(R|S) = \\frac{1}{2} \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right) - \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})\n$$\n使用对数性质 $\\ln(a) - \\ln(b) = \\ln(a/b)$，我们可以简化表达式：\n$$\nI(S;R) = \\frac{1}{2} \\left[ \\ln\\left(2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})\\right) - \\ln(2\\pi e \\sigma_{N}^{2}) \\right] = \\frac{1}{2} \\ln\\left(\\frac{2\\pi e (\\sigma_{S}^{2} + \\sigma_{N}^{2})}{2\\pi e \\sigma_{N}^{2}}\\right)\n$$\n项 $2\\pi e$ 从分子和分母中消去：\n$$\nI(S;R) = \\frac{1}{2} \\ln\\left(\\frac{\\sigma_{S}^{2} + \\sigma_{N}^{2}}{\\sigma_{N}^{2}}\\right)\n$$\n这可以通过分离分数写成更标准的形式：\n$$\nI(S;R) = \\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}\\right)\n$$\n项 $\\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}$ 是系统的信噪比（SNR）。这个结果是信息论中高斯信道的一个基本公式。",
            "answer": "$$\n\\boxed{\\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{S}^{2}}{\\sigma_{N}^{2}}\\right)}\n$$"
        },
        {
            "introduction": "在单个神经元的基础上，我们现在转向一个更复杂但极为重要的场景：神经元群体编码。这个练习构建了一个包含两个神经元的微型“群体”，旨在揭示一个反直觉的现象——噪声相关性有时非但无害，反而能通过“协同作用”（synergy）增加群体编码的信息量。通过对这个具体案例的逐步计算，你将亲身体验协同信息如何从共享的潜在变量中涌现，并理解为何在神经编码中，“整体”有时会大于“部分之和”。",
            "id": "3990337",
            "problem": "考虑两个神经元，它们在短时间窗内的响应是二元随机变量 $R_1 \\in \\{0,1\\}$ 和 $R_2 \\in \\{0,1\\}$。呈现一个二元刺激 $S \\in \\{0,1\\}$，其先验概率为 $P(S=0) = P(S=1) = \\frac{1}{2}$。这些响应共享一个调节共同增益的潜变量 $Z \\in \\{a,b\\}$；在给定 $(S,Z)$ 的条件下，神经元之间的响应是相互独立的。形式上，对于每个 $s \\in \\{0,1\\}$ 和 $z \\in \\{a,b\\}$，\n$$P(R_1=r_1,R_2=r_2 \\mid S=s, Z=z) = P(R_1=r_1 \\mid S=s, Z=z)\\, P(R_2=r_2 \\mid S=s, Z=z)。$$\n对于 $s=0$ 和 $s=1$ 这两种情况，潜变量均满足 $P(Z=a \\mid S=s) = P(Z=b \\mid S=s) = \\frac{1}{2}$。条件响应概率如下：\n- 对于 $S=0$，$P(R_i=1 \\mid S=0, Z=a) = \\frac{1}{2}$ 且 $P(R_i=1 \\mid S=0, Z=b) = \\frac{1}{2}$，其中 $i \\in \\{1,2\\}$。\n- 对于 $S=1$，$P(R_i=1 \\mid S=1, Z=a) = 0.9$ 且 $P(R_i=1 \\mid S=1, Z=b) = 0.1$，其中 $i \\in \\{1,2\\}$。\n\n这种构造通过共享的潜增益 $Z$ 在 $S=1$ 时引入了正向噪声相关性，同时保持了单个神经元在不同刺激下的边缘概率分布不变。令 $R=(R_1,R_2)$ 表示联合响应向量。使用以自然对数为底的 Shannon 互信息定义，计算刺激与联合响应之间的互信息 $I(S;R)$。答案以奈特（nats）为单位，并将最终数值四舍五入至四位有效数字。此外，您可以假设所有未指定的概率均为零，并且在该模型下所有期望和概率都是良定义的。",
            "solution": "在尝试解答之前，对问题进行验证。\n\n### 步骤1：提取已知条件\n-   **随机变量**：一个二元刺激 $S \\in \\{0, 1\\}$，一个潜变量 $Z \\in \\{a, b\\}$，以及两个二元神经响应 $R_1 \\in \\{0, 1\\}$ 和 $R_2 \\in \\{0, 1\\}$。联合响应由向量 $R = (R_1, R_2)$ 表示。\n-   **先验概率**：\n    -   $P(S=0) = P(S=1) = \\frac{1}{2}$。\n    -   对于 $s \\in \\{0, 1\\}$，$P(Z=a \\mid S=s) = P(Z=b \\mid S=s) = \\frac{1}{2}$。\n-   **条件独立性**：给定刺激 $S$ 和潜变量 $Z$，响应 $R_1$ 和 $R_2$ 是条件独立的。\n    $$P(R_1=r_1, R_2=r_2 \\mid S=s, Z=z) = P(R_1=r_1 \\mid S=s, Z=z) P(R_2=r_2 \\mid S=s, Z=z)$$\n-   **条件响应概率**：对于 $i \\in \\{1, 2\\}$，发放（$R_i=1$）的概率给出如下：\n    -   $P(R_i=1 \\mid S=0, Z=a) = \\frac{1}{2}$。\n    -   $P(R_i=1 \\mid S=0, Z=b) = \\frac{1}{2}$。\n    -   $P(R_i=1 \\mid S=1, Z=a) = 0.9$。\n    -   $P(R_i=1 \\mid S=1, Z=b) = 0.1$。\n-   **目标**：使用自然对数计算互信息 $I(S;R)$，以奈特（nats）为单位表示，并四舍五入到四位有效数字。\n\n### 步骤2：使用提取的已知条件进行验证\n检查问题的有效性：\n-   **科学依据**：该问题描述了一个潜变量模型，这是计算神经科学中用于模拟神经群体中噪声相关性的标准理论结构。该框架基于已建立的概率论和信息论。它在科学上是合理的。\n-   **良定义的**：提供了所有必要的概率和条件以完全确定联合分布 $P(S, Z, R_1, R_2)$。待计算的量，即互信息，是一个良定义的度量。存在唯一解。\n-   **客观性**：问题以精确、形式化的数学语言陈述，没有歧义或主观内容。\n\n### 步骤3：结论与行动\n该问题被判定为**有效**。将提供完整解答。\n\n刺激 $S$ 和联合响应向量 $R=(R_1, R_2)$ 之间的互信息由以下公式给出：\n$$I(S;R) = H(R) - H(R|S)$$\n其中 $H(R)$ 是联合响应分布的熵，$H(R|S)$ 是给定刺激下响应的条件熵。我们将分别计算每一项。\n\n首先，我们计算条件熵 $H(R|S)$，其定义为：\n$$H(R|S) = \\sum_{s \\in \\{0,1\\}} P(S=s) H(R|S=s) = \\frac{1}{2} H(R|S=0) + \\frac{1}{2} H(R|S=1)$$\n这需要计算条件概率分布 $P(R|S=0)$ 和 $P(R|S=1)$。我们通过对潜变量 $Z$ 进行边缘化来获得这些分布：\n$$P(R=r \\mid S=s) = \\sum_{z \\in \\{a,b\\}} P(R=r \\mid S=s, Z=z) P(Z=z \\mid S=s)$$\n由于给定 $P(Z=a|S=s) = P(Z=b|S=s) = \\frac{1}{2}$，上式变为：\n$$P(R=r \\mid S=s) = \\frac{1}{2} P(R=r \\mid S=s, Z=a) + \\frac{1}{2} P(R=r \\mid S=s, Z=b)$$\n令 $p_{s,z} = P(R_i=1 \\mid S=s, Z=z)$。由于条件独立性，$P(R=(r_1,r_2) \\mid S=s, Z=z) = p_{s,z}^{r_1+r_2}(1-p_{s,z})^{2-(r_1+r_2)}$。\n\n**情况1：刺激 $S=0$**\n给定的概率是 $p_{0,a} = \\frac{1}{2}$ 和 $p_{0,b} = \\frac{1}{2}$。由于 $p_{0,a} = p_{0,b}$，当 $S=0$ 时，潜变量对响应概率没有影响。令 $p_0 = \\frac{1}{2}$。\n$$P(R=r \\mid S=0) = P(R=r \\mid S=0, Z=a)$$\n响应 $R_1$ 和 $R_2$ 是成功概率为 $p_0 = \\frac{1}{2}$ 的独立伯努利试验。\n$P(R=(0,0) \\mid S=0) = (1-p_0)^2 = (\\frac{1}{2})^2 = \\frac{1}{4}$。\n$P(R=(0,1) \\mid S=0) = (1-p_0)p_0 = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$。\n$P(R=(1,0) \\mid S=0) = p_0(1-p_0) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$。\n$P(R=(1,1) \\mid S=0) = p_0^2 = (\\frac{1}{2})^2 = \\frac{1}{4}$。\n分布 $P(R|S=0)$ 在四种可能的结果上是均匀的。熵为：\n$$H(R|S=0) = -\\sum_{r} P(r|S=0) \\ln P(r|S=0) = -4 \\left(\\frac{1}{4} \\ln \\frac{1}{4}\\right) = -\\ln\\left(\\frac{1}{4}\\right) = \\ln(4) = 2\\ln(2)$$\n\n**情况2：刺激 $S=1$**\n给定的概率是 $p_{1,a} = 0.9$ 和 $p_{1,b} = 0.1$。\n$P(R=(0,0) \\mid S=1) = \\frac{1}{2}((1-p_{1,a})^2 + (1-p_{1,b})^2) = \\frac{1}{2}((0.1)^2 + (0.9)^2) = \\frac{1}{2}(0.01 + 0.81) = 0.41$。\n$P(R=(0,1) \\mid S=1) = \\frac{1}{2}((1-p_{1,a})p_{1,a} + (1-p_{1,b})p_{1,b}) = \\frac{1}{2}((0.1)(0.9) + (0.9)(0.1)) = \\frac{1}{2}(0.09 + 0.09) = 0.09$。\n根据对称性，$P(R=(1,0) \\mid S=1) = P(R=(0,1) \\mid S=1) = 0.09$。\n$P(R=(1,1) \\mid S=1) = \\frac{1}{2}(p_{1,a}^2 + p_{1,b}^2) = \\frac{1}{2}((0.9)^2 + (0.1)^2) = \\frac{1}{2}(0.81 + 0.01) = 0.41$。\n熵为：\n$$H(R|S=1) = -[P((0,0)|1)\\ln P((0,0)|1) + P((0,1)|1)\\ln P((0,1)|1) + P((1,0)|1)\\ln P((1,0)|1) + P((1,1)|1)\\ln P((1,1)|1)]$$\n$$H(R|S=1) = -[2 \\times 0.41 \\ln(0.41) + 2 \\times 0.09 \\ln(0.09)]$$\n现在我们计算总条件熵 $H(R|S)$：\n$$H(R|S) = \\frac{1}{2} H(R|S=0) + \\frac{1}{2} H(R|S=1) = \\frac{1}{2}(2\\ln 2) - \\frac{1}{2}[2 \\times 0.41 \\ln(0.41) + 2 \\times 0.09 \\ln(0.09)]$$\n$$H(R|S) = \\ln(2) - (0.41 \\ln(0.41) + 0.09 \\ln(0.09))$$\n\n接下来，我们计算边缘熵 $H(R)$。这需要边缘响应分布 $P(R)$。\n$$P(R=r) = P(R=r|S=0)P(S=0) + P(R=r|S=1)P(S=1) = \\frac{1}{2}(P(R=r|S=0) + P(R=r|S=1))$$\n$P(R=(0,0)) = \\frac{1}{2}(\\frac{1}{4} + 0.41) = \\frac{1}{2}(0.25 + 0.41) = \\frac{1}{2}(0.66) = 0.33$。\n$P(R=(0,1)) = \\frac{1}{2}(\\frac{1}{4} + 0.09) = \\frac{1}{2}(0.25 + 0.09) = \\frac{1}{2}(0.34) = 0.17$。\n$P(R=(1,0)) = P(R=(0,1)) = 0.17$。\n$P(R=(1,1)) = P(R=(0,0)) = 0.33$。\n边缘熵 $H(R)$ 为：\n$$H(R) = -[2 \\times 0.33 \\ln(0.33) + 2 \\times 0.17 \\ln(0.17)]$$\n\n最后，我们计算互信息 $I(S;R) = H(R) - H(R|S)$：\n$$I(S;R) = - (2 \\times 0.33 \\ln 0.33 + 2 \\times 0.17 \\ln 0.17) - [\\ln(2) - (0.41 \\ln 0.41 + 0.09 \\ln 0.09)]$$\n我们现在代入对数的数值：\n$\\ln(2) \\approx 0.693147$\n$\\ln(0.41) \\approx -0.891598$\n$\\ln(0.09) \\approx -2.407946$\n$\\ln(0.33) \\approx -1.108663$\n$\\ln(0.17) \\approx -1.771957$\n\n首先，计算 $H(R)$：\n$$H(R) \\approx -[2 \\times 0.33 \\times (-1.108663) + 2 \\times 0.17 \\times (-1.771957)]$$\n$$H(R) \\approx -[-0.731718 - 0.602465] = 1.334183$$\n接下来，计算 $H(R|S)$：\n$$H(R|S) \\approx 0.693147 - [0.41 \\times (-0.891598) + 0.09 \\times (-2.407946)]$$\n$$H(R|S) \\approx 0.693147 - [-0.365555 - 0.216715] = 0.693147 + 0.582270 = 1.275417$$\n现在，计算差值：\n$$I(S;R) = H(R) - H(R|S) \\approx 1.334183 - 1.275417 = 0.058766$$\n将结果四舍五入到四位有效数字，得到 $0.05877$。\n信息量约为 $0.05877$ 奈特。",
            "answer": "$$\n\\boxed{0.05877}\n$$"
        },
        {
            "introduction": "理论推导和模型计算是理解神经编码的基础，但最终我们需要面向真实的实验数据。这项计算实践将带领你从理论走向应用，解决如何从有限、嘈杂的神经响应数据中估计互信息的核心问题。你将实现一个“插件”（plug-in）估计器，并使用非参数自举（bootstrap）方法来量化该估计的统计不确定性，最终生成置信区间。掌握这项技能，意味着你不仅能计算出信息值，更能科学地评估其可靠性，这是计算神经科学研究中不可或缺的一环。",
            "id": "5037495",
            "problem": "给定一组离散化的神经响应，这些响应是针对一组刺激的，并且已经被分箱到整数类别中。设刺激为一个离散随机变量 $S$，取 $K$ 个值；分箱后的神经响应为一个离散随机变量 $R$，取 $B$ 个值。每个刺激 $s$ 呈现 $T$ 次试验，每次试验观察到一个分箱后的响应值 $r \\in \\{0,1,\\dots,B-1\\}$。从经验联合概率分布、经验边缘分布、香农熵和条件熵的核心定义出发，根据这些经验频率推导出一个有原则的插件估计量（plug-in estimator），用于计算互信息 $I(S;R)$（以比特为单位）。实现一个程序，该程序：\n- 使用从经验数据中推导出的插件估计量，计算 $I(S;R)$ 的点估计值（以比特为单位）。\n- 应用非参数自举（bootstrap）程序，该程序在每个刺激 $s$ 内独立地进行有放回的试验重采样（每个刺激保持 $T$ 次试验），以近似 $I(S;R)$ 插件估计量的抽样分布。\n- 使用自举分布，基于百分位数截断，计算标称覆盖率为 $95$ 百分比的双侧置信区间。\n\n你的程序必须使用以 2 为底的对数，以确保信息量以比特表示。自举必须使用恰好 $B_{\\text{boot}}=400$ 次重采样，并且必须使用种子 $12345$ 初始化一个确定性伪随机数生成器，以使结果可复现。为保证数值稳定性，在计算任何形式为 $p \\log_{2}\\!\\big(p/q\\big)$ 的表达式时，当 $p=0$ 时，其贡献定义为 $0$。\n\n你的程序必须解决以下测试套件。在每个案例中，响应取值于 $\\{0,1,2,3\\}$，即 $B=4$，并且在同一案例中，每个刺激的试验次数 $T$ 相同。对于每个案例，你都将获得每个刺激的分箱响应列表：\n\n- 案例 A（一般情况，信息丰富的响应）：$K=3$， $T=20$，\n  - 刺激 $s=0$：$[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2,2]$。\n  - 刺激 $s=1$：$[2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,1,1]$。\n  - 刺激 $s=2$：$[0,0,0,0,0,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3]$。\n- 案例 B（几乎无法区分的响应）：$K=2$，$T=30$，\n  - 刺激 $s=0$：$[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3]$。\n  - 刺激 $s=1$：$[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3]$。\n- 案例 C（小样本和边界计数）：$K=4$，$T=10$，\n  - 刺激 $s=0$：$[0,0,0,0,0,0,0,0,0,0]$。\n  - 刺激 $s=1$：$[1,1,1,1,1,1,1,1,1,1]$。\n  - 刺激 $s=2$：$[0,0,0,0,0,1,1,1,1,1]$。\n  - 刺激 $s=3$：$[2,2,2,2,2,3,3,3,3,3]$。\n\n你的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。对于每个案例，按此顺序报告三个浮点数：$I(S;R)$ 的自举百分位数下界、点估计值和自举百分位数上界（均以比特为单位）。所有数字必须四舍五入到六位小数。因此，输出必须采用 $[L_{A},\\hat{I}_{A},U_{A},L_{B},\\hat{I}_{B},U_{B},L_{C},\\hat{I}_{C},U_{C}]$ 的形式，其中 $L_{\\cdot}$ 和 $U_{\\cdot}$ 表示 $95$ 百分位置信区间的下界和上界，$\\hat{I}_{\\cdot}$ 表示插件点估计值，所有单位均为比特。",
            "solution": "问题陈述已经过严格验证，被认为是**有效的**。它在信息论和统计学方面有科学依据，问题提出得很好，提供了所有必要的数据和约束条件，并且用客观、精确的语言表述。它提出了一个计算神经科学领域的标准且有意义的任务。因此，我们可以着手解决。\n\n解决方案包括三个主要部分：\n1.  互信息插件估计量 $\\hat{I}(S;R)$ 的推导。\n2.  用于估计 $\\hat{I}(S;R)$ 抽样分布的非参数自举程序的描述。\n3.  从自举分布构建基于百分位数的置信区间。\n\n### 1. 互信息插件估计量的推导\n\n互信息 $I(S;R)$ 量化了两个随机变量——刺激 $S$ 和响应 $R$ 之间的统计依赖性。它衡量了在观察一个变量后，关于另一个变量不确定性的减少量。它是根据香农熵定义的：\n\n$$\nI(S;R) = H(R) - H(R|S)\n$$\n\n其中所有信息论量都以比特为单位度量，这意味着使用以 2 为底的对数。\n\n-   **响应的香农熵**：$H(R)$ 是响应的边缘分布的熵，它衡量了关于神经响应的总不确定性，而不管刺激是什么。对于一个可以取 $B$ 个值 $\\{r_0, ..., r_{B-1}\\}$ 的离散变量 $R$，其定义为：\n    $$\n    H(R) = - \\sum_{j=0}^{B-1} p(r_j) \\log_2 p(r_j)\n    $$\n-   **给定刺激下响应的条件熵**：$H(R|S)$ 是响应的熵在所有刺激上的平均值。它衡量了在已知刺激的情况下，关于响应的剩余不确定性。\n    $$\n    H(R|S) = \\sum_{i=0}^{K-1} p(s_i) H(R|S=s_i)\n    $$\n    其中 $p(s_i)$ 是刺激 $s_i$ 的概率，而 $H(R|S=s_i)$ 是该特定刺激的响应分布的熵：\n    $$\n    H(R|S=s_i) = - \\sum_{j=0}^{B-1} p(r_j|s_i) \\log_2 p(r_j|s_i)\n    $$\n\n为了从经验数据中计算 $I(S;R)$，我们使用**插件法（plug-in method）**，该方法用基于频率的估计值替换真实的概率。\n\n设实验数据包含 $K$ 个刺激，每个刺激 $s_i$ ($i \\in \\{0, \\dots, K-1\\}$) 呈现 $T$ 次试验。神经响应被分箱到 $B$ 个类别中的一个，$r_j$ ($j \\in \\{0, \\dots, B-1\\}$)。总试验次数为 $N = K \\times T$。\n\n设 $N_{ij}$ 为呈现刺激 $s_i$ 时观察到响应 $r_j$ 的经验计数。\n经验概率估计如下：\n-   **刺激概率**：问题陈述每个刺激都呈现相同次数的试验。因此，任何给定刺激 $s_i$ 的概率是均匀的：\n    $$\n    \\hat{p}(s_i) = \\frac{T}{N} = \\frac{T}{KT} = \\frac{1}{K}\n    $$\n-   **条件响应概率**：给定刺激 $s_i$ 时响应 $r_j$ 的概率是该刺激下该响应的频率：\n    $$\n    \\hat{p}(r_j|s_i) = \\frac{N_{ij}}{T}\n    $$\n-   **边缘响应概率**：使用全概率定律，响应 $r_j$ 的边缘概率为：\n    $$\n    \\hat{p}(r_j) = \\sum_{i=0}^{K-1} \\hat{p}(s_i) \\hat{p}(r_j|s_i) = \\sum_{i=0}^{K-1} \\frac{1}{K} \\frac{N_{ij}}{T} = \\frac{\\sum_{i=0}^{K-1} N_{ij}}{KT}\n    $$\n\n将这些估计值代入熵公式，得到插件估计量。我们采用约定，如果 $p=0$，则 $p \\log_2 p = 0$。\n\n-   **$H(R)$ 的估计量**：\n    $$\n    \\hat{H}(R) = - \\sum_{j=0}^{B-1} \\hat{p}(r_j) \\log_2 \\hat{p}(r_j) = - \\sum_{j=0}^{B-1} \\left( \\frac{\\sum_{i=0}^{K-1} N_{ij}}{KT} \\right) \\log_2 \\left( \\frac{\\sum_{i=0}^{K-1} N_{ij}}{KT} \\right)\n    $$\n-   **$H(R|S)$ 的估计量**：\n    首先，我们估计每个刺激 $s_i$ 的条件熵：\n    $$\n    \\hat{H}(R|S=s_i) = - \\sum_{j=0}^{B-1} \\hat{p}(r_j|s_i) \\log_2 \\hat{p}(r_j|s_i) = - \\sum_{j=0}^{B-1} \\left( \\frac{N_{ij}}{T} \\right) \\log_2 \\left( \\frac{N_{ij}}{T} \\right)\n    $$\n    然后，我们对所有刺激的这些值进行平均，权重为 $\\hat{p}(s_i) = 1/K$：\n    $$\n    \\hat{H}(R|S) = \\sum_{i=0}^{K-1} \\frac{1}{K} \\hat{H}(R|S=s_i) = \\frac{1}{K} \\sum_{i=0}^{K-1} \\left[ - \\sum_{j=0}^{B-1} \\frac{N_{ij}}{T} \\log_2 \\left( \\frac{N_{ij}}{T} \\right) \\right]\n    $$\n结合这些，互信息的插件估计量为：\n$$\n\\hat{I}(S;R) = \\hat{H}(R) - \\hat{H}(R|S)\n$$\n\n### 2. 用于抽样分布的自举程序\n\n插件估计 $\\hat{I}(S;R)$ 是从我们的特定数据集中计算出的单个值。作为一个统计量，它会受到抽样变异性的影响。为了量化此估计中的不确定性，我们使用非参数自举程序。该程序通过重复重采样观测数据来近似估计量 $\\hat{I}(S;R)$ 的抽样分布。\n\n该程序如下：\n1.  设置自举重采样的次数，$B_{\\text{boot}} = 400$。\n2.  使用固定种子（$12345$）初始化一个伪随机数生成器，以保证可复现性。\n3.  对于每次自举迭代 $b = 1, \\dots, B_{\\text{boot}}$：\n    a.  创建一个新的、重采样的数据集。问题指定在*每个刺激内部独立地*重采样试验。这意味着对于每个刺激 $s_i$，我们通过从该刺激的原始 $T$ 个观测响应中有放回地抽取，生成一组新的 $T$ 次试验。\n    b.  使用这个重采样的数据集，计算计数矩阵 $N_{ij}^*$，然后使用上面推导的相同插件估计量公式，计算互信息的自举复制值 $\\hat{I}^*(S;R)_b$。\n4.  这 $B_{\\text{boot}}$ 个值的集合，$\\{\\hat{I}^*(S;R)_1, \\dots, \\hat{I}^*(S;R)_{B_{\\text{boot}}}\\}$，构成了 $\\hat{I}(S;R)$ 抽样分布的一个经验近似。\n\n### 3. 置信区间的构建\n\n使用自举分布，我们可以为真实的互信息构建一个置信区间。问题指定了百分位数法。对于一个标称覆盖率为 $95\\%$ 的双侧置信区间，我们找到包含自举分布中心 $95\\%$ 的值。\n\n1.  将自举复制值按升序排序：$\\hat{I}^*_{(1)} \\leq \\hat{I}^*_{(2)} \\leq \\dots \\leq \\hat{I}^*_{(B_{\\text{boot}})}$。\n2.  置信区间的下界 $L$ 是该分布的第 $2.5$ 百分位数。\n3.  置信区间的上界 $U$ 是该分布的第 $97.5$ 百分位数。\n4.  最终得到的 $95\\%$ 置信区间为 $[L, U]$。\n\n最终的程序将为每个测试案例实现这些步骤，报告下界 $L$、来自原始数据的点估计值 $\\hat{I}(S;R)$ 以及上界 $U$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_mi_plugin(data, B):\n    \"\"\"\n    Computes the plug-in estimate of mutual information I(S;R) in bits.\n\n    Args:\n        data (list of lists): A list where each inner list contains the\n                              binned responses for a single stimulus.\n        B (int): The number of response bins.\n\n    Returns:\n        float: The mutual information estimate in bits.\n    \"\"\"\n    K = len(data)\n    if K == 0:\n        return 0.0\n    T = len(data[0])\n    N = K * T\n    if N == 0:\n        return 0.0\n\n    # 1. Compute the count matrix N_sr of size (K, B)\n    # N_sr[s, r] = count of response r for stimulus s\n    N_sr = np.zeros((K, B), dtype=int)\n    for s_idx, stimulus_responses in enumerate(data):\n        for r_val in stimulus_responses:\n            N_sr[s_idx, r_val] += 1\n\n    # 2. Compute probability distributions\n    # p(r|s) = N_sr / T\n    # Note: Division by zero is not an issue here as T > 0 by problem structure.\n    p_r_given_s = N_sr / T\n\n    # p(r) = sum_s(N_sr) / N\n    N_r = np.sum(N_sr, axis=0)\n    p_r = N_r / N\n\n    # 3. Compute entropy H(R)\n    # H(R) = - sum_r p(r) log2(p(r))\n    # Using np.log2 with `where` handles the case p(r) = 0, where p*log(p) = 0.\n    H_R = -np.sum(p_r * np.log2(p_r, where=p_r > 0, out=np.zeros_like(p_r)))\n\n    # 4. Compute conditional entropy H(R|S)\n    # H(R|S) = sum_s p(s) H(R|S=s)\n    # Since p(s) = 1/K is uniform, H(R|S) is the mean of H(R|S=s).\n    # H(R|S=s) = - sum_r p(r|s) log2(p(r|s))\n    H_R_given_S_per_stimulus = -np.sum(p_r_given_s * np.log2(p_r_given_s, where=p_r_given_s > 0, out=np.zeros_like(p_r_given_s)), axis=1)\n    H_R_given_S = np.mean(H_R_given_S_per_stimulus)\n\n    # 5. Compute Mutual Information I(S;R) = H(R) - H(R|S)\n    mi = H_R - H_R_given_S\n    \n    # Due to floating point precision, very small negative numbers can occur\n    # when true MI is zero. By definition, MI is non-negative.\n    return max(0.0, mi)\n\ndef calculate_mi_and_ci(data, B, B_boot, seed):\n    \"\"\"\n    Calculates the point estimate and bootstrap confidence interval for mutual information.\n\n    Args:\n        data (list of lists): The raw response data for each stimulus.\n        B (int): The number of response bins.\n        B_boot (int): The number of bootstrap samples.\n        seed (int): The seed for the pseudorandom number generator.\n\n    Returns:\n        tuple: A tuple containing (lower_bound, point_estimate, upper_bound).\n    \"\"\"\n    # 1. Calculate the point estimate from the original data\n    point_estimate = compute_mi_plugin(data, B)\n\n    # 2. Perform nonparametric bootstrap\n    K = len(data)\n    T = len(data[0])\n    \n    rng = np.random.default_rng(seed)\n    bootstrap_mi_values = np.zeros(B_boot)\n\n    original_data_arrays = [np.array(d) for d in data]\n\n    for i in range(B_boot):\n        resampled_data = []\n        for s_idx in range(K):\n            # Resample trials with replacement, within each stimulus\n            resampled_indices = rng.integers(0, T, size=T)\n            resampled_trials = original_data_arrays[s_idx][resampled_indices].tolist()\n            resampled_data.append(resampled_trials)\n        \n        bootstrap_mi_values[i] = compute_mi_plugin(resampled_data, B)\n\n    # 3. Compute the 95% confidence interval using the percentile method\n    alpha = 0.05\n    lower_bound = np.percentile(bootstrap_mi_values, 100 * alpha / 2.0)\n    upper_bound = np.percentile(bootstrap_mi_values, 100 * (1 - alpha / 2.0))\n\n    return lower_bound, point_estimate, upper_bound\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A (general, informative responses)\n        {\n            \"data\": [\n                [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2,2],\n                [2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,1,1],\n                [0,0,0,0,0,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3],\n            ],\n            \"B\": 4,\n        },\n        # Case B (near-indistinguishable responses)\n        {\n            \"data\": [\n                [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3],\n                [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3],\n            ],\n            \"B\": 4,\n        },\n        # Case C (small-sample and edge counts)\n        {\n            \"data\": [\n                [0,0,0,0,0,0,0,0,0,0],\n                [1,1,1,1,1,1,1,1,1,1],\n                [0,0,0,0,0,1,1,1,1,1],\n                [2,2,2,2,2,3,3,3,3,3],\n            ],\n            \"B\": 4,\n        },\n    ]\n\n    B_boot = 400\n    seed = 12345\n    \n    all_results = []\n    for case in test_cases:\n        data = case[\"data\"]\n        B = case[\"B\"]\n        \n        lower, point, upper = calculate_mi_and_ci(data, B, B_boot, seed)\n        \n        all_results.append(f\"{lower:.6f}\")\n        all_results.append(f\"{point:.6f}\")\n        all_results.append(f\"{upper:.6f}\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}