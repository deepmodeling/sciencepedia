{
    "hands_on_practices": [
        {
            "introduction": "Understanding a theory is one thing; applying it is another. This first practice exercise guides you through the complete, end-to-end pipeline of building a neural decoder from scratch. You will begin by simulating a realistic dataset of cosine-tuned neurons with Poisson spiking, then use linear regression to estimate the unknown tuning parameters from this 'recorded' data, and finally implement and evaluate a vector averaging decoder on new test data . This comprehensive coding task solidifies the connection between the theoretical model of population coding and its practical application.",
            "id": "4010760",
            "problem": "You are given a computational neuroscience task concerning neural population coding of a planar direction stimulus. Assume a population of neurons that each exhibits cosine tuning with respect to a stimulus direction, and that spiking is modeled as a Poisson point process. The objective is to estimate each neuron's tuning parameters and preferred direction from training data and then decode new stimuli using a vector averaging decoder. The entire task must be implemented as a single runnable program.\n\nFundamental basis and assumptions:\n- Each neuron $i$ has an unknown preferred direction $\\theta_i$ and produces spikes according to a cosine tuning law with baseline. The expected firing rate is a function of the stimulus direction. The neural response in trial $t$ is modeled as a Poisson random variable with mean equal to its rate under the stimulus shown in that trial.\n- The stimulus direction is represented on the circle, and angles must be handled in radians. A decoder based on population vectors and vector averaging produces a single direction estimate per trial.\n\nYour program must perform the following steps for each test case in the suite:\n1. Generate training data:\n   - Construct a population of $N$ neurons with specified preferred directions $\\{\\theta_i\\}_{i=1}^N$, baselines $\\{b_i\\}_{i=1}^N$ (in spikes per second), and gains $\\{g_i\\}_{i=1}^N$ (in spikes per second), where the expected rate of neuron $i$ under stimulus angle $\\phi$ is $b_i + g_i \\cos(\\phi - \\theta_i)$.\n   - Simulate $T$ training trials. In each trial $t$, draw a stimulus angle $\\phi_t$ uniformly from the interval $[-\\pi, \\pi)$, and for each neuron $i$, draw a spike count $k_{i,t}$ according to the Poisson distribution with mean $b_i + g_i \\cos(\\phi_t - \\theta_i)$.\n2. Estimate tuning parameters:\n   - Using the training data $\\{(\\phi_t, k_{i,t})\\}$, estimate for each neuron $i$ its baseline $\\hat{b}_i$, gain $\\hat{g}_i$, and preferred direction $\\hat{\\theta}_i$, relying solely on cosine-tuning structure and linear regression principles grounded in the trigonometric identity that converts a phase-shifted cosine into a linear combination of $\\cos(\\phi)$ and $\\sin(\\phi)$ terms.\n   - For numerical stability, ignore any neuron whose estimated gain falls below a small threshold $\\varepsilon$; use $\\varepsilon = 10^{-3}$ spikes per second.\n3. Generate test data and decode:\n   - Simulate $K$ test trials with new stimulus angles $\\phi'_u$ drawn uniformly from $[-\\pi, \\pi)$, and Poisson spike counts $k'_{i,u}$ generated under the true parameters $(b_i, g_i, \\theta_i)$.\n   - Decode each test stimulus angle using a vector averaging decoder derived from the population vector concept: treat each neuron's contribution as a unit vector oriented at its estimated preferred direction, weighted by a nonnegative activity proportional to its baseline-subtracted spike count. Specifically, for neuron $i$, use the weight $\\max(0, k'_{i,u} - \\hat{b}_i)$, and combine only neurons with $\\hat{g}_i \\ge \\varepsilon$.\n   - The decoder must produce a single angle estimate $\\hat{\\phi}'_u$ in radians for each test trial $u$.\n4. Evaluate performance:\n   - Compute the mean absolute circular error over test trials, where the absolute circular error between a decoded angle $\\hat{\\phi}$ and the true angle $\\phi$ is $d(\\hat{\\phi}, \\phi) = \\min_{k \\in \\mathbb{Z}} |\\hat{\\phi} - \\phi + 2\\pi k|$. Express this in radians.\n\nAngle unit requirement: All angles must be handled and reported in radians.\n\nPhysical units requirement: All firing rates must be treated in spikes per second.\n\nFinal output format: Your program should produce a single line of output containing the mean absolute circular error for each test case as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3]$), where each $result_j$ is a floating-point number in radians.\n\nTest suite:\n- Case $1$ (happy path, diverse gains, ample training):\n  - $N = 8$, $T = 200$, $K = 50$.\n  - Preferred directions: $\\theta_i$ evenly spaced on $[-\\pi, \\pi)$, i.e., $\\theta_i = -\\pi + \\frac{2\\pi (i-1)}{N}$ for $i = 1, \\dots, N$.\n  - Baselines: $b_i = 20$ (spikes per second) for all $i$.\n  - Gains: $(g_1, g_2, \\dots, g_8) = (14, 12, 16, 10, 18, 11, 15, 13)$ (spikes per second).\n  - Pseudo-random number generator seeds: training seed $= 101$, test seed $= 202$.\n- Case $2$ (boundary condition with zero-gain neurons):\n  - $N = 8$, $T = 60$, $K = 40$.\n  - Preferred directions: $\\theta_i$ evenly spaced on $[-\\pi, \\pi)$.\n  - Baselines: $b_i = 20$ (spikes per second) for all $i$.\n  - Gains: $(g_1, g_2, \\dots, g_8) = (0, 0, 12, 10, 8, 0, 6, 0)$ (spikes per second).\n  - Pseudo-random number generator seeds: training seed $= 303$, test seed $= 404$.\n- Case $3$ (small-sample, low-gain, noisier setting):\n  - $N = 6$, $T = 30$, $K = 30$.\n  - Preferred directions: $\\theta_i$ evenly spaced on $[-\\pi, \\pi)$.\n  - Baselines: $b_i = 15$ (spikes per second) for all $i$.\n  - Gains: $(g_1, g_2, \\dots, g_6) = (5, 4, 3, 2, 5, 4)$ (spikes per second).\n  - Pseudo-random number generator seeds: training seed $= 505$, test seed $= 606$.\n\nScientific realism constraints:\n- Use the Poisson distribution for spike counts with the specified mean rates to ensure nonnegative integer counts.\n- Ensure that all computations and decoding are performed in radians.\n- Avoid using any external data or user input; use the specified seeds for reproducible randomness.\n\nYour program must produce the single-line output summarizing the three mean absolute circular errors in radians as $[e_1,e_2,e_3]$.",
            "solution": "### Step 1: Neural Response Model\nThe activity of each neuron in the population is governed by a cosine tuning model. The expected firing rate $r_i$ (in spikes per second, interpreted as spikes per trial) of neuron $i$ in response to a stimulus direction $\\phi$ (in radians) is given by:\n$$\nr_i(\\phi) = b_i + g_i \\cos(\\phi - \\theta_i)\n$$\nwhere $b_i$ is the neuron's baseline firing rate, $g_i$ is its gain (modulation depth), and $\\theta_i$ is its preferred direction. The number of spikes $k_{i,t}$ emitted by neuron $i$ in a given trial $t$ with stimulus $\\phi_t$ is modeled as a random variable drawn from a Poisson distribution with a mean equal to the expected rate:\n$$\nk_{i,t} \\sim \\text{Poisson}(r_i(\\phi_t))\n$$\nThe problem parameters for all test cases ensure that $b_i \\ge g_i$, guaranteeing a non-negative rate $r_i(\\phi) \\ge 0$.\n\n### Step 2: Training Data Generation\nFor each test case, we first synthesize a training dataset. This involves simulating $T$ trials. In each trial $t \\in \\{1, \\dots, T\\}$, a stimulus angle $\\phi_t$ is drawn from a uniform distribution over $[-\\pi, \\pi)$. Subsequently, for each of the $N$ neurons, a spike count $k_{i,t}$ is generated by drawing from the Poisson distribution $\\text{Poisson}(b_i + g_i \\cos(\\phi_t - \\theta_i))$, using the true, pre-specified parameters.\n\n### Step 3: Tuning Parameter Estimation\nThe central task in this step is to estimate the tuning parameters $(\\hat{b}_i, \\hat{g}_i, \\hat{\\theta}_i)$ for each neuron from the generated training data $\\{(\\phi_t, k_{i,t})\\}_{t=1}^T$. This is accomplished through linear regression, based on a trigonometric expansion of the tuning curve.\n\nThe cosine term is expanded as $\\cos(\\phi - \\theta_i) = \\cos\\phi \\cos\\theta_i + \\sin\\phi \\sin\\theta_i$. Substituting this into the rate equation gives:\n$$\nr_i(\\phi) = b_i + (g_i \\cos\\theta_i) \\cos\\phi + (g_i \\sin\\theta_i) \\sin\\phi\n$$\nThis equation has the form of a linear model, $r_i(\\phi) = \\beta_{i,0} \\cdot 1 + \\beta_{i,1} \\cos\\phi + \\beta_{i,2} \\sin\\phi$, with coefficients defined as:\n- $\\beta_{i,0} = b_i$\n- $\\beta_{i,1} = g_i \\cos\\theta_i$\n- $\\beta_{i,2} = g_i \\sin\\theta_i$\n\nWe estimate these coefficients by minimizing the sum of squared errors between the observed spike counts $k_{i,t}$ and the model's prediction. This is a multiple linear regression problem for each neuron, which can be expressed in matrix form for all trials:\n$$\n\\mathbf{k}_i \\approx \\mathbf{X} \\boldsymbol{\\beta}_i\n$$\nHere, $\\mathbf{k}_i = [k_{i,1}, \\dots, k_{i,T}]^T$ is the vector of observed spike counts for neuron $i$, $\\boldsymbol{\\beta}_i = [\\beta_{i,0}, \\beta_{i,1}, \\beta_{i,2}]^T$ is the vector of coefficients to be estimated, and $\\mathbf{X}$ is the $T \\times 3$ design matrix containing the predictor variables for all trials:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & \\cos(\\phi_1) & \\sin(\\phi_1) \\\\\n1 & \\cos(\\phi_2) & \\sin(\\phi_2) \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & \\cos(\\phi_T) & \\sin(\\phi_T)\n\\end{pmatrix}\n$$\nThe standard least-squares solution for the coefficient vector is $\\hat{\\boldsymbol{\\beta}}_i = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{k}_i$. We solve this system for all $N$ neurons to obtain the estimated coefficients $(\\hat{\\beta}_{i,0}, \\hat{\\beta}_{i,1}, \\hat{\\beta}_{i,2})$ for each neuron $i$.\n\nFrom these estimated coefficients, we recover the estimates for the original tuning parameters:\n- Estimated Baseline: $\\hat{b}_i = \\hat{\\beta}_{i,0}$\n- Estimated Gain: $\\hat{g}_i = \\sqrt{\\hat{\\beta}_{i,1}^2 + \\hat{\\beta}_{i,2}^2}$\n- Estimated Preferred Direction: $\\hat{\\theta}_i = \\text{atan2}(\\hat{\\beta}_{i,2}, \\hat{\\beta}_{i,1})$\n\n### Step 4: Decoding Test Stimuli\nAfter parameter estimation, we generate a new set of $K$ test trials. For each trial $u \\in \\{1, \\dots, K\\}$, a stimulus angle $\\phi'_u$ is drawn uniformly from $[-\\pi, \\pi)$, and spike counts $k'_{i,u}$ are generated using the true parameters.\n\nThe stimulus angle for each test trial is then decoded using a vector averaging decoder. This method computes a population vector, $\\vec{V}_u$, for each trial $u$ by summing vectorial contributions from all neurons deemed sufficiently tuned:\n$$\n\\vec{V}_u = \\sum_{i=1}^N w_{i,u} \\cdot \\vec{d}_i\n$$\n- A neuron $i$ contributes to the sum only if its estimated gain $\\hat{g}_i$ is above a threshold, $\\hat{g}_i \\ge \\varepsilon$, where $\\varepsilon=10^{-3}$.\n- The contribution of each valid neuron is a vector $\\vec{d}_i = [\\cos(\\hat{\\theta}_i), \\sin(\\hat{\\theta}_i)]^T$, which points in the direction of its estimated preferred angle $\\hat{\\theta}_i$.\n- This vector is weighted by the neuron's rectified, baseline-subtracted activity: $w_{i,u} = \\max(0, k'_{i,u} - \\hat{b}_i)$.\n\nThe decoded angle for the trial, $\\hat{\\phi}'_u$, is the angle of the resultant population vector $\\vec{V}_u = [V_{u,x}, V_{u,y}]^T$:\n$$\n\\hat{\\phi}'_u = \\text{atan2}(V_{u,y}, V_{u,x})\n$$\n\n### Step 5: Performance Evaluation\nThe decoder's accuracy is evaluated by computing the mean absolute circular error over all $K$ test trials. The absolute circular error for a single trial is the shortest angular distance between the true stimulus angle $\\phi'_u$ and the decoded angle $\\hat{\\phi}'_u$:\n$$\nd(\\hat{\\phi}'_u, \\phi'_u) = \\min_{k \\in \\mathbb{Z}} |\\hat{\\phi}'_u - \\phi'_u + 2\\pi k|\n$$\nThis is equivalent to wrapping the angle difference $\\hat{\\phi}'_u - \\phi'_u$ into the interval $[-\\pi, \\pi]$ and taking its absolute value. The final reported value for each test case is the average of these errors over the $K$ test trials. The entire procedure is encapsulated in a program that executes all three test cases and reports the results according to the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(N, T, K, thetas, bs, gs, train_seed, test_seed, epsilon):\n    \"\"\"\n    Solves a single test case for the neural decoding problem.\n\n    This function performs training data generation, parameter estimation,\n    test data generation, decoding, and performance evaluation.\n    \"\"\"\n    # === Step 1: Generate training data ===\n    rng_train = np.random.default_rng(train_seed)\n    train_stimuli = rng_train.uniform(-np.pi, np.pi, size=T)\n    \n    # train_spikes will have shape (N, T)\n    train_spikes = np.zeros((N, T))\n    for i in range(N):\n        # The problem statement's parameters ensure b_i >= g_i, so rates are non-negative.\n        rates = bs[i] + gs[i] * np.cos(train_stimuli - thetas[i])\n        train_spikes[i, :] = rng_train.poisson(rates)\n\n    # === Step 2: Estimate tuning parameters ===\n    # Design matrix X, shape (T, 3) for linear regression\n    X = np.vstack([np.ones(T), np.cos(train_stimuli), np.sin(train_stimuli)]).T\n    \n    # Solve the least squares problem for all neurons simultaneously.\n    # The model is k_counts.T (shape T,N) = X (shape T,3) @ beta_matrix.T (shape 3,N)\n    # np.linalg.lstsq solves ax=b for x. Here a=X, b=train_spikes.T, x=beta_matrix.\n    beta_matrix, _, _, _ = np.linalg.lstsq(X, train_spikes.T, rcond=None)\n    \n    # beta_matrix has shape (3, N). Rows correspond to b_hat, c_hat, s_hat.\n    b_hat = beta_matrix[0, :]\n    c_hat = beta_matrix[1, :]\n    s_hat = beta_matrix[2, :]\n    \n    g_hat = np.sqrt(c_hat**2 + s_hat**2)\n    theta_hat = np.arctan2(s_hat, c_hat)\n\n    # === Step 3: Generate test data and decode ===\n    rng_test = np.random.default_rng(test_seed)\n    test_stimuli = rng_test.uniform(-np.pi, np.pi, size=K)\n    \n    # test_spikes will have shape (N, K)\n    test_spikes = np.zeros((N, K))\n    for i in range(N):\n        rates = bs[i] + gs[i] * np.cos(test_stimuli - thetas[i])\n        test_spikes[i, :] = rng_test.poisson(rates)\n        \n    # Decoding using vector averaging\n    valid_neurons_mask = g_hat >= epsilon\n    \n    # Weights for population vector, shape (N, K)\n    weights = test_spikes - b_hat[:, np.newaxis]\n    weights[weights < 0] = 0.0\n    weights[~valid_neurons_mask, :] = 0.0  # Zero out weights for invalid neurons\n    \n    # Preferred direction vectors, one x and one y component per neuron\n    pref_dir_vectors_x = np.cos(theta_hat)\n    pref_dir_vectors_y = np.sin(theta_hat)\n    \n    # Population vector components, summed over neurons for each trial, shape (K,)\n    pop_vector_x = pref_dir_vectors_x @ weights\n    pop_vector_y = pref_dir_vectors_y @ weights\n    \n    decoded_stimuli = np.arctan2(pop_vector_y, pop_vector_x)\n\n    # === Step 4: Evaluate performance ===\n    # Calculate absolute circular error\n    error_diff = decoded_stimuli - test_stimuli\n    circular_errors = np.abs((error_diff + np.pi) % (2 * np.pi) - np.pi)\n    \n    mean_abs_error = np.mean(circular_errors)\n    \n    return mean_abs_error\n\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: happy path, diverse gains, ample training\n        {\n            \"N\": 8, \"T\": 200, \"K\": 50,\n            \"b_val\": 20.0, \"gs\": [14.0, 12.0, 16.0, 10.0, 18.0, 11.0, 15.0, 13.0],\n            \"train_seed\": 101, \"test_seed\": 202\n        },\n        # Case 2: boundary condition with zero-gain neurons\n        {\n            \"N\": 8, \"T\": 60, \"K\": 40,\n            \"b_val\": 20.0, \"gs\": [0.0, 0.0, 12.0, 10.0, 8.0, 0.0, 6.0, 0.0],\n            \"train_seed\": 303, \"test_seed\": 404\n        },\n        # Case 3: small-sample, low-gain, noisier setting\n        {\n            \"N\": 6, \"T\": 30, \"K\": 30,\n            \"b_val\": 15.0, \"gs\": [5.0, 4.0, 3.0, 2.0, 5.0, 4.0],\n            \"train_seed\": 505, \"test_seed\": 606\n        }\n    ]\n\n    results = []\n    epsilon = 1e-3\n\n    for case in test_cases:\n        N = case[\"N\"]\n        # True preferred directions are evenly spaced on [-pi, pi)\n        thetas = -np.pi + (2 * np.pi * np.arange(N)) / N\n        bs = np.full(N, case[\"b_val\"])\n        gs = np.array(case[\"gs\"])\n        \n        result = solve_case(\n            N=N, T=case[\"T\"], K=case[\"K\"],\n            thetas=thetas, bs=bs, gs=gs,\n            train_seed=case[\"train_seed\"], test_seed=case[\"test_seed\"],\n            epsilon=epsilon\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After building a decoder, a natural and critical question is: how good is it? This exercise shifts our focus from implementation to a rigorous theoretical analysis of decoder performance. You will analytically derive the mean squared error for the classic population vector decoder and compare it to the error of the optimal linear estimator, providing a crucial benchmark for performance . This practice will deepen your understanding of key statistical concepts like bias and variance in the context of neural decoding and reveal the mathematical trade-offs inherent in decoder design.",
            "id": "4010728",
            "problem": "Consider a two-dimensional stimulus vector $s \\in \\mathbb{R}^{2}$ with fixed magnitude $m = \\|s\\|$. A population of $N$ neurons encodes $s$ via linear tuning with additive Gaussian noise: for neuron $i \\in \\{1,\\dots,N\\}$, the response is $r_{i} = \\beta \\, a_{i}^{\\top} s + \\epsilon_{i}$, where $\\beta > 0$ is a gain, $a_{i} \\in \\mathbb{R}^{2}$ is the unit preferred-direction vector of neuron $i$, and $\\epsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$ are independent and identically distributed across neurons. Assume the preferred directions $\\{a_{i}\\}_{i=1}^{N}$ are equally spaced around the unit circle so that $\\sum_{i=1}^{N} a_{i} a_{i}^{\\top} = \\frac{N}{2} I_{2}$, where $I_{2}$ is the $2 \\times 2$ identity matrix.\n\nDefine the Population Vector (PV) decoder by $\\hat{s}_{\\mathrm{PV}} = \\frac{1}{\\beta} \\sum_{i=1}^{N} r_{i} a_{i}$, and define the Ordinary Least Squares Estimator (OLE) as the linear estimator $\\hat{s}_{\\mathrm{OLE}}$ that minimizes the expected squared Euclidean decoding error $E\\left[\\|\\hat{s} - s\\|^{2}\\right]$ over all linear maps from $\\mathbb{R}^{N}$ to $\\mathbb{R}^{2}$ under the given generative model.\n\nDerive, from first principles, the mean squared error of each decoder, $E\\left[\\|\\hat{s}_{\\mathrm{PV}} - s\\|^{2}\\right]$ and $E\\left[\\|\\hat{s}_{\\mathrm{OLE}} - s\\|^{2}\\right]$, averaged over the noise distribution for the fixed stimulus $s$. Then, compute the ratio\n$$\nR = \\frac{E\\left[\\|\\hat{s}_{\\mathrm{PV}} - s\\|^{2}\\right]}{E\\left[\\|\\hat{s}_{\\mathrm{OLE}} - s\\|^{2}\\right]}\n$$\nas a closed-form analytic expression in terms of $N$, $\\beta$, $\\sigma^{2}$, and $m$. Express your final answer as a single analytic expression. No rounding is required.",
            "solution": "### Derivation of Mean Squared Errors and their Ratio\n\nThe objective is to compute the mean squared error (MSE) for the Population Vector (PV) decoder and the Ordinary Least Squares Estimator (OLE), and then find their ratio. The expectation, denoted by $E[\\cdot]$, is taken over the noise distribution.\n\n**1. Mean Squared Error of the Population Vector (PV) Decoder**\n\nThe PV decoder is defined as:\n$$\n\\hat{s}_{\\mathrm{PV}} = \\frac{1}{\\beta} \\sum_{i=1}^{N} r_{i} a_{i}\n$$\nWe substitute the neural response model, $r_{i} = \\beta a_{i}^{\\top} s + \\epsilon_{i}$, into this equation:\n$$\n\\hat{s}_{\\mathrm{PV}} = \\frac{1}{\\beta} \\sum_{i=1}^{N} (\\beta a_{i}^{\\top} s + \\epsilon_{i}) a_{i} = \\sum_{i=1}^{N} (a_{i}^{\\top} s) a_{i} + \\frac{1}{\\beta} \\sum_{i=1}^{N} \\epsilon_{i} a_{i}\n$$\nThe first term can be rewritten using matrix notation. Recognizing that $\\sum_{i=1}^{N} (a_{i}^{\\top} s) a_{i} = \\left(\\sum_{i=1}^{N} a_{i} a_{i}^{\\top}\\right)s$. Using the given identity $\\sum_{i=1}^{N} a_{i} a_{i}^{\\top} = \\frac{N}{2} I_{2}$, we get:\n$$\n\\hat{s}_{\\mathrm{PV}} = \\frac{N}{2} s + \\frac{1}{\\beta} \\sum_{i=1}^{N} \\epsilon_{i} a_{i}\n$$\nThe decoding error is the difference between the estimate and the true stimulus, $\\mathbf{e}_{\\mathrm{PV}} = \\hat{s}_{\\mathrm{PV}} - s$:\n$$\n\\mathbf{e}_{\\mathrm{PV}} = \\left(\\frac{N}{2} - 1\\right)s + \\frac{1}{\\beta} \\sum_{i=1}^{N} \\epsilon_{i} a_{i}\n$$\nThe MSE is the expected squared magnitude of this error vector, $E[\\|\\mathbf{e}_{\\mathrm{PV}}\\|^{2}]$. The error vector is a sum of a constant term (the bias) and a zero-mean noise term. The expectation of the cross-term is zero because $E[\\epsilon_i]=0$. Thus, the MSE is the sum of the squared magnitude of the bias and the variance.\n\nThe bias is $b_{\\mathrm{PV}} = E[\\mathbf{e}_{\\mathrm{PV}}] = E\\left[\\left(\\frac{N}{2} - 1\\right)s + \\frac{1}{\\beta} \\sum_{i=1}^{N} \\epsilon_{i} a_{i}\\right] = \\left(\\frac{N}{2} - 1\\right)s$.\nThe squared magnitude of the bias is:\n$$\n\\|b_{\\mathrm{PV}}\\|^{2} = \\left\\|\\left(\\frac{N-2}{2}\\right)s\\right\\|^{2} = \\left(\\frac{N-2}{2}\\right)^{2} \\|s\\|^{2} = \\frac{(N-2)^{2}}{4} m^{2}\n$$\nThe variance is the expected squared magnitude of the zero-mean part of the error:\n$$\n\\text{Var}(\\hat{s}_{\\mathrm{PV}}) = E\\left[\\left\\| \\frac{1}{\\beta} \\sum_{i=1}^{N} \\epsilon_{i} a_{i} \\right\\|^{2}\\right] = \\frac{1}{\\beta^{2}} E\\left[ \\left(\\sum_{i=1}^{N} \\epsilon_{i} a_{i}\\right)^{\\top} \\left(\\sum_{j=1}^{N} \\epsilon_{j} a_{j}\\right) \\right]\n$$\n$$\n= \\frac{1}{\\beta^{2}} E\\left[ \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\epsilon_{i} \\epsilon_{j} a_{i}^{\\top} a_{j} \\right] = \\frac{1}{\\beta^{2}} \\sum_{i=1}^{N}\\sum_{j=1}^{N} (a_{i}^{\\top} a_{j}) E[\\epsilon_{i} \\epsilon_{j}]\n$$\nSince the noise terms are i.i.d., $E[\\epsilon_{i} \\epsilon_{j}] = \\sigma^{2} \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n$$\n\\text{Var}(\\hat{s}_{\\mathrm{PV}}) = \\frac{1}{\\beta^{2}} \\sum_{i=1}^{N} (a_{i}^{\\top} a_{i}) \\sigma^{2} = \\frac{\\sigma^{2}}{\\beta^{2}} \\sum_{i=1}^{N} \\|a_{i}\\|^{2}\n$$\nGiven $\\|a_{i}\\|=1$, this simplifies to:\n$$\n\\text{Var}(\\hat{s}_{\\mathrm{PV}}) = \\frac{N \\sigma^{2}}{\\beta^{2}}\n$$\nThe total MSE for the PV decoder is the sum of the squared bias and the variance:\n$$\nE\\left[\\|\\hat{s}_{\\mathrm{PV}} - s\\|^{2}\\right] = \\frac{(N-2)^{2}}{4} m^{2} + \\frac{N \\sigma^{2}}{\\beta^{2}}\n$$\n\n**2. Mean Squared Error of the Ordinary Least Squares Estimator (OLE)**\n\nThe OLE is the linear estimator $\\hat{s} = W \\mathbf{r}$ that minimizes $E[\\|W \\mathbf{r} - s\\|^{2}]$, where $\\mathbf{r} = [r_1, \\dots, r_N]^\\top$. In matrix form, the system is $\\mathbf{r} = \\beta A s + \\boldsymbol{\\epsilon}$, where $A$ is the $N \\times 2$ matrix with rows $a_i^\\top$. The OLE for a model $y = X\\theta + \\epsilon$ is given by $\\hat{\\theta} = (X^\\top X)^{-1} X^\\top y$. Here, $X = \\beta A$ and $\\theta=s$.\n$$\n\\hat{s}_{\\mathrm{OLE}} = ((\\beta A)^{\\top} (\\beta A))^{-1} (\\beta A)^{\\top} \\mathbf{r} = (\\beta^2 A^\\top A)^{-1} \\beta A^\\top \\mathbf{r} = \\frac{1}{\\beta} (A^{\\top}A)^{-1} A^{\\top} \\mathbf{r}\n$$\nThe matrix $A^{\\top}A$ is $\\sum_{i=1}^{N} a_{i} a_{i}^{\\top}$, which is given as $\\frac{N}{2} I_{2}$.\n$$\n\\hat{s}_{\\mathrm{OLE}} = \\frac{1}{\\beta} \\left(\\frac{N}{2} I_{2}\\right)^{-1} A^{\\top} \\mathbf{r} = \\frac{1}{\\beta} \\frac{2}{N} I_{2} A^{\\top} \\mathbf{r} = \\frac{2}{\\beta N} A^{\\top} \\mathbf{r}\n$$\nNow we find the error by substituting $\\mathbf{r} = \\beta A s + \\boldsymbol{\\epsilon}$:\n$$\n\\hat{s}_{\\mathrm{OLE}} = \\frac{2}{\\beta N} A^{\\top} (\\beta A s + \\boldsymbol{\\epsilon}) = \\frac{2}{N} (A^{\\top}A) s + \\frac{2}{\\beta N} A^{\\top} \\boldsymbol{\\epsilon}\n$$\nSubstituting $A^\\top A = \\frac{N}{2} I_2$:\n$$\n\\hat{s}_{\\mathrm{OLE}} = \\frac{2}{N} \\left(\\frac{N}{2} I_{2}\\right) s + \\frac{2}{\\beta N} A^{\\top} \\boldsymbol{\\epsilon} = s + \\frac{2}{\\beta N} A^{\\top} \\boldsymbol{\\epsilon}\n$$\nThe decoding error is $\\mathbf{e}_{\\mathrm{OLE}} = \\hat{s}_{\\mathrm{OLE}} - s = \\frac{2}{\\beta N} A^{\\top} \\boldsymbol{\\epsilon}$.\nThe bias is $E[\\mathbf{e}_{\\mathrm{OLE}}] = \\frac{2}{\\beta N} A^{\\top} E[\\boldsymbol{\\epsilon}] = 0$. The OLE is unbiased.\nThe MSE is therefore equal to the variance of the estimator. We compute $E[\\|\\mathbf{e}_{\\mathrm{OLE}}\\|^2]$:\n$$\nE[\\|\\mathbf{e}_{\\mathrm{OLE}}\\|^2] = E\\left[\\left\\| \\frac{2}{\\beta N} A^{\\top} \\boldsymbol{\\epsilon} \\right\\|^{2}\\right] = \\left(\\frac{2}{\\beta N}\\right)^{2} E[\\|A^{\\top} \\boldsymbol{\\epsilon}\\|^{2}]\n$$\nThe term $E[\\|A^{\\top} \\boldsymbol{\\epsilon}\\|^{2}]$ can be calculated as the trace of the covariance matrix of the vector $A^\\top \\boldsymbol{\\epsilon}$, which is $A^{\\top} \\text{Cov}(\\boldsymbol{\\epsilon}) A = A^{\\top} (\\sigma^2 I_N) A = \\sigma^2 A^\\top A$.\n$$\nE[\\|A^{\\top} \\boldsymbol{\\epsilon}\\|^{2}] = \\text{tr}(\\sigma^{2} A^{\\top}A) = \\sigma^{2} \\text{tr}\\left(\\frac{N}{2}I_{2}\\right) = \\sigma^{2} \\left(\\frac{N}{2} + \\frac{N}{2}\\right) = N \\sigma^{2}\n$$\nSubstituting this back into the MSE expression for the OLE:\n$$\nE\\left[\\|\\hat{s}_{\\mathrm{OLE}} - s\\|^{2}\\right] = \\left(\\frac{2}{\\beta N}\\right)^{2} (N \\sigma^{2}) = \\frac{4}{\\beta^{2} N^{2}} N \\sigma^{2} = \\frac{4 \\sigma^{2}}{\\beta^{2} N}\n$$\n\n**3. Ratio of the Mean Squared Errors**\n\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{E\\left[\\|\\hat{s}_{\\mathrm{PV}} - s\\|^{2}\\right]}{E\\left[\\|\\hat{s}_{\\mathrm{OLE}} - s\\|^{2}\\right]} = \\frac{\\frac{(N-2)^{2}}{4} m^{2} + \\frac{N \\sigma^{2}}{\\beta^{2}}}{\\frac{4 \\sigma^{2}}{\\beta^{2} N}}\n$$\nTo simplify, we multiply the numerator and denominator by $\\frac{\\beta^{2} N}{4 \\sigma^{2}}$:\n$$\nR = \\left(\\frac{(N-2)^{2}}{4} m^{2} + \\frac{N \\sigma^{2}}{\\beta^{2}}\\right) \\left(\\frac{\\beta^{2} N}{4 \\sigma^{2}}\\right)\n$$\n$$\nR = \\left(\\frac{(N-2)^{2}}{4} m^{2}\\right)\\left(\\frac{\\beta^{2} N}{4 \\sigma^{2}}\\right) + \\left(\\frac{N \\sigma^{2}}{\\beta^{2}}\\right)\\left(\\frac{\\beta^{2} N}{4 \\sigma^{2}}\\right)\n$$\n$$\nR = \\frac{N(N-2)^{2} \\beta^{2} m^{2}}{16 \\sigma^{2}} + \\frac{N^{2}}{4}\n$$\nThis is the final closed-form analytic expression for the ratio.",
            "answer": "$$\n\\boxed{\\frac{N(N-2)^{2} \\beta^{2} m^{2}}{16 \\sigma^{2}} + \\frac{N^{2}}{4}}\n$$"
        },
        {
            "introduction": "Our final practice extends the concept of decoding into the powerful framework of Bayesian inference, a leading theory of brain function. This exercise demonstrates how the population vector, representing sensory evidence from neural activity, can be mathematically combined with prior knowledge or expectations to form a more robust posterior belief about a stimulus . By calculating the Maximum A Posteriori (MAP) estimate, you will see how the intuitive vector-averaging mechanism can be elegantly interpreted as a likelihood computation within a sophisticated and generalizable Bayesian framework.",
            "id": "4010755",
            "problem": "A population of $N=8$ orientation-tuned neurons in primary visual cortex encodes a circular variable (orientation) $\\theta \\in [0,2\\pi)$. The neurons have evenly spaced preferred directions $\\phi_{k} \\in \\{0,\\frac{\\pi}{4},\\frac{\\pi}{2},\\frac{3\\pi}{4},\\pi,\\frac{5\\pi}{4},\\frac{3\\pi}{2},\\frac{7\\pi}{4}\\}$ for $k=1,\\dots,8$. Assume independent Poisson spiking with identical baseline rates and cosine-like tuning of identical shape and gain, and a dense, approximately uniform distribution of preferred directions. Under these conditions, the $\\theta$-dependent part of the log-likelihood is well-approximated by its first harmonic in the circular Fourier series and is compatible with a vector-averaging decoder that uses spike counts as weights over preferred directions. The recorded spike counts for a single observation window are\n$$\nn_{1}=14,\\quad n_{2}=23,\\quad n_{3}=21,\\quad n_{4}=10,\\quad n_{5}=7,\\quad n_{6}=5,\\quad n_{7}=8,\\quad n_{8}=12.\n$$\nIn addition, the observer has a non-uniform prior over $\\theta$ that is von Mises with mean direction $\\theta_{0}=\\frac{\\pi}{6}$ and concentration parameter $\\kappa_{0}=10$, i.e., $\\pi(\\theta)\\propto \\exp\\big(\\kappa_{0}\\cos(\\theta-\\theta_{0})\\big)$. Using Bayesâ€™ rule and the modeling assumptions stated, determine the Maximum A Posteriori (MAP) estimate of $\\theta$ produced by a vector-averaging decoder that incorporates this prior. Express your final answer in radians and round your answer to four significant figures.",
            "solution": "The problem asks for the Maximum A Posteriori (MAP) estimate of a stimulus orientation $\\theta$, given a set of spike counts from a population of neurons and a prior distribution over $\\theta$. This is a canonical problem in Bayesian inference.\n\nAccording to Bayes' rule, the posterior probability of the stimulus $\\theta$ given the observed spike counts $\\mathbf{n} = (n_1, \\dots, n_N)$ is given by:\n$$\nP(\\theta|\\mathbf{n}) = \\frac{P(\\mathbf{n}|\\theta)\\pi(\\theta)}{P(\\mathbf{n})}\n$$\nwhere $P(\\mathbf{n}|\\theta)$ is the likelihood of observing the spike counts given the stimulus, $\\pi(\\theta)$ is the prior probability of the stimulus, and $P(\\mathbfn)$ is the marginal probability of the data, which serves as a normalization constant.\n\nThe MAP estimate, $\\hat{\\theta}_{MAP}$, is the value of $\\theta$ that maximizes the posterior probability $P(\\theta|\\mathbf{n})$. Since the denominator $P(\\mathbf{n})$ is independent of $\\theta$, this is equivalent to maximizing the product of the likelihood and the prior:\n$$\n\\hat{\\theta}_{MAP} = \\underset{\\theta}{\\arg\\max} \\, [P(\\mathbf{n}|\\theta)\\pi(\\theta)]\n$$\nThis is also equivalent to maximizing the log-posterior, $\\ln P(\\theta|\\mathbf{n}) = \\ln P(\\mathbf{n}|\\theta) + \\ln \\pi(\\theta) - \\ln P(\\mathbf{n})$.\n\nThe problem states that the decoding process is compatible with a vector-averaging decoder and that the $\\theta$-dependent part of the log-likelihood is well-approximated by its first harmonic. For independent Poisson neurons with cosine tuning, the log-likelihood can be shown to be approximately proportional to $\\sum_{k=1}^{N} n_k \\cos(\\theta - \\phi_k)$, where $n_k$ are the spike counts and $\\phi_k$ are the preferred directions. This expression can be rewritten as:\n$$\n\\sum_{k=1}^{N} n_k \\cos(\\theta - \\phi_k) = \\cos\\theta \\left(\\sum_{k=1}^{N} n_k \\cos\\phi_k\\right) + \\sin\\theta \\left(\\sum_{k=1}^{N} n_k \\sin\\phi_k\\right)\n$$\nThe terms in the parentheses are the components of the population vector, $V_{pop} = (V_{pop,x}, V_{pop,y})$, where $V_{pop,x} = \\sum_k n_k \\cos\\phi_k$ and $V_{pop,y} = \\sum_k n_k \\sin\\phi_k$. The expression is thus equivalent to $|V_{pop}|\\cos(\\theta - \\hat{\\theta}_{ML})$, where $|V_{pop}|$ is the magnitude of the population vector and $\\hat{\\theta}_{ML} = \\text{atan2}(V_{pop,y}, V_{pop,x})$ is its angle, which corresponds to the Maximum Likelihood (ML) estimate of $\\theta$.\n\nTherefore, the likelihood function can be modeled as a von Mises distribution, centered at $\\hat{\\theta}_{ML}$ with a concentration parameter proportional to $|V_{pop}|$:\n$$\nP(\\mathbf{n}|\\theta) \\propto \\exp(|V_{pop}|\\cos(\\theta - \\hat{\\theta}_{ML}))\n$$\nThis is a mathematical formalization of using a vector-averaging decoder. The \"evidence\" from the neural data is encapsulated by the population vector $V_{pop}$.\n\nThe prior distribution is given as a von Mises distribution with mean direction $\\theta_0 = \\frac{\\pi}{6}$ and concentration parameter $\\kappa_0=10$:\n$$\n\\pi(\\theta) \\propto \\exp(\\kappa_0 \\cos(\\theta - \\theta_0))\n$$\n\nThe log-posterior is therefore:\n$$\n\\ln P(\\theta|\\mathbf{n}) \\propto |V_{pop}|\\cos(\\theta - \\hat{\\theta}_{ML}) + \\kappa_0\\cos(\\theta - \\theta_0)\n$$\nMaximizing this expression is equivalent to finding the angle of the sum of two vectors. The first vector is the population vector $V_{pop}$ (representing the likelihood), and the second vector, $V_{prior}$, has a magnitude equal to the prior's concentration $\\kappa_0$ and a direction equal to the prior's mean $\\theta_0$.\n\nLet $V_{MAP} = V_{pop} + V_{prior}$. The MAP estimate $\\hat{\\theta}_{MAP}$ is the angle of this resultant vector.\n\nFirst, we compute the population vector $V_{pop} = (V_{pop,x}, V_{pop,y})$ using the given spike counts $n_k$ and preferred directions $\\phi_k$.\nThe preferred directions are $\\phi_k = (k-1)\\frac{\\pi}{4}$ for $k=1, \\dots, 8$:\n$\\{0, \\frac{\\pi}{4}, \\frac{\\pi}{2}, \\frac{3\\pi}{4}, \\pi, \\frac{5\\pi}{4}, \\frac{3\\pi}{2}, \\frac{7\\pi}{4}\\}$.\nThe spike counts are $n = \\{14, 23, 21, 10, 7, 5, 8, 12\\}$.\n\nThe x-component of the population vector is:\n$$\nV_{pop,x} = \\sum_{k=1}^{8} n_k \\cos(\\phi_k) = 14\\cos(0) + 23\\cos(\\frac{\\pi}{4}) + 21\\cos(\\frac{\\pi}{2}) + 10\\cos(\\frac{3\\pi}{4}) + 7\\cos(\\pi) + 5\\cos(\\frac{5\\pi}{4}) + 8\\cos(\\frac{3\\pi}{2}) + 12\\cos(\\frac{7\\pi}{4})\n$$\n$$\nV_{pop,x} = 14(1) + 23(\\frac{\\sqrt{2}}{2}) + 21(0) + 10(-\\frac{\\sqrt{2}}{2}) + 7(-1) + 5(-\\frac{\\sqrt{2}}{2}) + 8(0) + 12(\\frac{\\sqrt{2}}{2})\n$$\n$$\nV_{pop,x} = (14-7) + (23-10-5+12)\\frac{\\sqrt{2}}{2} = 7 + 20\\frac{\\sqrt{2}}{2} = 7 + 10\\sqrt{2}\n$$\n\nThe y-component of the population vector is:\n$$\nV_{pop,y} = \\sum_{k=1}^{8} n_k \\sin(\\phi_k) = 14\\sin(0) + 23\\sin(\\frac{\\pi}{4}) + 21\\sin(\\frac{\\pi}{2}) + 10\\sin(\\frac{3\\pi}{4}) + 7\\sin(\\pi) + 5\\sin(\\frac{5\\pi}{4}) + 8\\sin(\\frac{3\\pi}{2}) + 12\\sin(\\frac{7\\pi}{4})\n$$\n$$\nV_{pop,y} = 14(0) + 23(\\frac{\\sqrt{2}}{2}) + 21(1) + 10(\\frac{\\sqrt{2}}{2}) + 7(0) + 5(-\\frac{\\sqrt{2}}{2}) + 8(-1) + 12(-\\frac{\\sqrt{2}}{2})\n$$\n$$\nV_{pop,y} = (21-8) + (23+10-5-12)\\frac{\\sqrt{2}}{2} = 13 + 16\\frac{\\sqrt{2}}{2} = 13 + 8\\sqrt{2}\n$$\n\nNext, we define the prior vector $V_{prior} = (V_{prior,x}, V_{prior,y})$ with magnitude $\\kappa_0=10$ and angle $\\theta_0=\\frac{\\pi}{6}$.\n$$\nV_{prior,x} = \\kappa_0 \\cos(\\theta_0) = 10\\cos(\\frac{\\pi}{6}) = 10(\\frac{\\sqrt{3}}{2}) = 5\\sqrt{3}\n$$\n$$\nV_{prior,y} = \\kappa_0 \\sin(\\theta_0) = 10\\sin(\\frac{\\pi}{6}) = 10(\\frac{1}{2}) = 5\n$$\n\nThe resultant vector for the MAP estimate is $V_{MAP} = V_{pop} + V_{prior}$.\nIts components are:\n$$\nV_{MAP,x} = V_{pop,x} + V_{prior,x} = (7 + 10\\sqrt{2}) + 5\\sqrt{3} = 7 + 10\\sqrt{2} + 5\\sqrt{3}\n$$\n$$\nV_{MAP,y} = V_{pop,y} + V_{prior,y} = (13 + 8\\sqrt{2}) + 5 = 18 + 8\\sqrt{2}\n$$\n\nNow, we compute the numerical values of the components:\n$$\nV_{MAP,x} \\approx 7 + 10(1.41421) + 5(1.73205) = 7 + 14.1421 + 8.66025 = 29.80235\n$$\n$$\nV_{MAP,y} \\approx 18 + 8(1.41421) = 18 + 11.31368 = 29.31368\n$$\n\nThe MAP estimate is the angle of this resultant vector:\n$$\n\\hat{\\theta}_{MAP} = \\text{atan2}(V_{MAP,y}, V_{MAP,x}) = \\text{atan2}(29.31368, 29.80235)\n$$\nSince both components are positive, the angle is in the first quadrant.\n$$\n\\hat{\\theta}_{MAP} = \\arctan\\left(\\frac{29.31368}{29.80235}\\right) \\approx \\arctan(0.983603) \\approx 0.777095 \\text{ radians}\n$$\n\nRounding the result to four significant figures, we get $0.7771$.",
            "answer": "$$\\boxed{0.7771}$$"
        }
    ]
}