{
    "hands_on_practices": [
        {
            "introduction": "Metropolis-Hastings 算法的核心在于其接受概率 $α$ 的计算。在编写任何代码之前，至关重要的是要理解如何从第一性原理出发，为给定的目标分布和提议分布推导出这个比率。本练习  将指导您完成这一关键的理论步骤，目标是在一个定义于单纯形上的狄利克雷分布上进行采样，这是一个在建模概率或成分数据时常见的场景。通过这个推导，您将巩固对 Metropolis-Hastings 公式的理解，并学会处理非对称提议分布所需的哈斯廷斯校正项。",
            "id": "791624",
            "problem": "考虑使用Metropolis-Hastings算法从2-单纯形 $\\mathcal{S}_2$ 上的一个目标概率分布中采样。2-单纯形是三维概率向量的集合，定义为 $\\mathcal{S}_2 = \\{ p = (p_1, p_2, p_3) \\in \\mathbb{R}^3 \\mid p_i \\ge 0 \\text{ for } i=1,2,3, \\text{ and } \\sum_{i=1}^3 p_i = 1 \\}$。\n\n目标分布 $\\pi(p)$ 是一个以正实数向量 $\\beta = (\\beta_1, \\beta_2, \\beta_3)$ 为参数的狄利克雷分布。其概率密度函数由下式给出：\n$$\n\\pi(p) \\propto \\prod_{i=1}^3 p_i^{\\beta_i - 1}\n$$\n\n从当前状态 $p$ 移动到建议状态 $p'$ 的提议机制也基于狄利克雷分布。新状态 $p'$ 从一个狄利克雷分布中抽取，该分布的参数由当前状态 $p$ 和一个正集中参数 $\\lambda$ 决定。具体而言，提议分布 $q(p'|p)$ 由下式给出：\n$$\np' \\sim \\text{Dir}(\\lambda p_1, \\lambda p_2, \\lambda p_3)\n$$\n狄利克雷分布 $\\text{Dir}(x | \\alpha)$ 的概率密度函数由下式给出：\n$$\nf(x_1, \\dots, x_K; \\alpha_1, \\dots, \\alpha_K) = \\frac{\\Gamma(\\sum_{i=1}^K \\alpha_i)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)} \\prod_{i=1}^K x_i^{\\alpha_i - 1}\n$$\n其中 $\\Gamma(\\cdot)$ 是伽马函数。\n\n你的任务是推导从状态 $p = (p_1, p_2, p_3)$ 转移到建议新状态 $p' = (p'_1, p'_2, p'_3)$ 的 Metropolis-Hastings 接受概率 $\\alpha(p' | p)$。请用 $p$、$p'$ 的分量以及参数 $\\beta_i$ 和 $\\lambda$ 表示最终答案。",
            "solution": "1. Metropolis–Hastings 接受概率为\n$$\n\\alpha(p'|p)=\\min\\!\\Bigl(1,\\frac{\\pi(p')\\,q(p\\mid p')}{\\pi(p)\\,q(p'\\mid p)}\\Bigr).\n$$\n2. 使用 \n$$\\pi(p)\\propto\\prod_{i=1}^3p_i^{\\beta_i-1},\\quad\nq(p'\\mid p)\n=\\frac{\\Gamma(\\lambda)}{\\prod_i\\Gamma(\\lambda p_i)}\\prod_{i=1}^3p'_i{}^{\\,\\lambda p_i-1},\n$$\n$q(p\\mid p')$ 的形式也类似，可得\n$$\n\\frac{\\pi(p')}{\\pi(p)}\n=\\prod_{i=1}^3\\frac{p'_i{}^{\\beta_i-1}}{p_i^{\\beta_i-1}},\\quad\n\\frac{q(p\\mid p')}{q(p'\\mid p)}\n=\\frac{\\prod_{i=1}^3\\Gamma(\\lambda p_i)}{\\prod_{i=1}^3\\Gamma(\\lambda p'_i)}\n\\prod_{i=1}^3\\frac{p_i^{\\lambda p'_i-1}}{p'_i^{\\lambda p_i-1}}.\n$$\n3. 因此\n$$\n\\alpha(p'|p)\n=\\min\\!\\Bigl(1,\\;\\prod_{i=1}^3\\frac{p'_i^{\\beta_i-1}}{p_i^{\\beta_i-1}}\n\\frac{\\prod_{i=1}^3\\Gamma(\\lambda p_i)}{\\prod_{i=1}^3\\Gamma(\\lambda p'_i)}\n\\prod_{i=1}^3\\frac{p_i^{\\lambda p'_i-1}}{p'_i^{\\lambda p_i-1}}\n\\Bigr).\n$$",
            "answer": "$$\\boxed{\\min\\Bigl(1,\\;\\prod_{i=1}^3\\frac{p'_i^{\\beta_i-1}}{p_i^{\\beta_i-1}}\\frac{\\prod_{i=1}^3\\Gamma(\\lambda p_i)}{\\prod_{i=1}^3\\Gamma(\\lambda p'_i)}\\prod_{i=1}^3\\frac{p_i^{\\lambda p'_i-1}}{p'_i^{\\lambda p_i-1}}\\Bigr)}$$"
        },
        {
            "introduction": "掌握了理论公式之后，下一步就是将其转化为实际的代码。这个练习  将理论付诸实践，要求您为一个常见的正态-正态贝叶斯模型实现 Metropolis-Hastings 算法的关键步骤。您将学习如何以数值方式稳定地计算接受概率，并探索两种不同的提议机制——对称的随机游走提议和非对称的独立性提议。这项实践将帮助您掌握 MCMC 的基本编程技能，这对于构建更复杂的采样器至关重要。",
            "id": "4971664",
            "problem": "考虑一个常用于连续临床结果的单参数贝叶斯模型，其中观察到的响应是条件独立的，并围绕患者水平的治疗效应呈正态分布。假设数据由样本均值 $\\bar{y}$ 和样本量 $n$ 概括，观测方差 $\\sigma^2$ 已知。假设治疗效应参数 $\\theta$ 服从正态先验。具体而言：\n- 似然为 $y_i \\mid \\theta \\sim \\mathcal{N}(\\theta,\\sigma^2)$，对于 $i=1,\\dots,n$ 独立。\n- 先验为 $\\theta \\sim \\mathcal{N}(\\mu_0,\\tau_0^2)$。\n\n您将为此正态-正态模型在马尔可夫链蒙特卡洛 (MCMC) 中实现一个 Metropolis–Hastings (MH) 步骤。该算法必须在两种提议族下，为给定的当前状态 $\\theta$ 和提议状态 $\\theta'$ 计算 MH 接受概率：\n- 随机游走正态提议：$q(\\theta' \\mid \\theta)=\\mathcal{N}(\\theta, s^2)$。\n- 独立正态提议：$q(\\theta' \\mid \\theta)=\\mathcal{N}(\\mu_q,\\sigma_q^2)$。\n\n您的程序必须：\n1. 从贝叶斯后验与似然乘以先验成正比的基本定义出发，并从 Metropolis–Hastings 接受概率的定义出发，仅使用似然的充分统计量 $\\bar{y}$ 和 $n$ 来实现接受概率的数值计算。\n2. 为保持数值稳定性，计算对数接受率，并仔细考虑适用于指定提议族的 Hastings 修正 $q(\\theta \\mid \\theta')/q(\\theta' \\mid \\theta)$。\n3. 对于每个测试用例，将接受概率 $\\alpha=\\min\\{1,\\exp(\\log \\alpha)\\}$ 作为浮点数输出。\n\n使用以下参数集测试套件，每个参数集都完整指定了 $\\bar{y}$、$n$、$\\sigma^2$、$\\mu_0$、$\\tau_0^2$、当前状态 $\\theta$、提议状态 $\\theta'$ 以及提议的详细信息：\n- 情况 $1$ (随机游走正态提议)：$\\bar{y}=1.2$，$n=50$，$\\sigma^2=16$，$\\mu_0=0$，$\\tau_0^2=100$，$\\theta=1.5$，$\\theta'=1.8$，$s^2=0.25$。\n- 情况 $2$ (独立正态提议)：$\\bar{y}=8.4$，$n=5$，$\\sigma^2=4$，$\\mu_0=10$，$\\tau_0^2=1$，$\\theta=8$，$\\theta'=6$，$\\mu_q=9$，$\\sigma_q^2=9$。\n- 情况 $3$ (独立正态提议，状态相同)：$\\bar{y}=0.3$，$n=10$，$\\sigma^2=1$，$\\mu_0=0$，$\\tau_0^2=4$，$\\theta=0.7$，$\\theta'=0.7$，$\\mu_q=0$，$\\sigma_q^2=9$。\n- 情况 $4$ (随机游走正态提议，远离中心的提议)：$\\bar{y}=0.2$，$n=200$，$\\sigma^2=9$，$\\mu_0=0$，$\\tau_0^2=1000$，$\\theta=0.1$，$\\theta'=5.0$，$s^2=1.0$。\n- 情况 $5$ (具有非平凡 Hastings 修正的独立正态提议)：$\\bar{y}=0.6$，$n=5$，$\\sigma^2=1$，$\\mu_0=0$，$\\tau_0^2=10$，$\\theta=0.0$，$\\theta'=0.8$，$\\mu_q=1.0$，$\\sigma_q^2=0.25$。\n\n程序要求：\n- 完全基于 $\\bar{y}$、$n$、$\\sigma^2$、$\\mu_0$、$\\tau_0^2$、$\\theta$、$\\theta'$ 和提议参数实现计算。除了基本定义外，不要使用任何闭式后验。\n- 使用对数来稳定地计算接受率，然后转换为接受概率 $\\alpha$。\n- 对于随机游走正态提议，正确地认识到由于对称性，Hastings 修正会简化；对于独立正态提议，根据指定的 $\\mu_q$ 和 $\\sigma_q^2$ 计算修正项。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含五个案例的接受概率，格式为一个左方括号，后跟五个格式化为小数点后六位并用逗号分隔的值，最后是一个右方括号。\n- 输出是 $[0,1]$ 范围内的无单位浮点数。",
            "solution": "该问题已经过验证，被认为是计算统计学中一个定义明确、科学合理且客观的问题。它要求为正态-正态贝叶斯模型实现一个 Metropolis-Hastings (MH) 步骤。\n\n从当前状态 $\\theta$ 转换到提议状态 $\\theta'$ 的 Metropolis-Hastings 接受概率 $\\alpha$ 由下式给出：\n$$ \\alpha(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi(\\theta')}{\\pi(\\theta)} \\frac{q(\\theta \\mid \\theta')}{q(\\theta' \\mid \\theta)}\\right) $$\n其中 $\\pi(\\cdot)$ 是目标后验概率密度函数，$q(\\cdot \\mid \\cdot)$ 是提议概率密度函数。为了数值稳定性，计算在对数尺度上进行。对数接受率 $\\log R$ 为：\n$$ \\log R = \\log(\\pi(\\theta')) - \\log(\\pi(\\theta)) + \\log(q(\\theta \\mid \\theta')) - \\log(q(\\theta' \\mid \\theta)) $$\n接受概率则为 $\\alpha = \\min(1, \\exp(\\log R))$。\n\n目标后验密度 $\\pi(\\theta)$ 与似然 $L(\\theta \\mid \\mathbf{y})$ 和先验 $p(\\theta)$ 的乘积成正比。\n$$ \\pi(\\theta) \\propto L(\\theta \\mid \\mathbf{y}) \\times p(\\theta) $$\n\n**1. 似然和先验设定**\n\n对于 $n$ 个独立观测值 $y_i \\sim \\mathcal{N}(\\theta, \\sigma^2)$ 的似然为：\n$$ L(\\theta \\mid \\mathbf{y}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta)^2}{2\\sigma^2}\\right) $$\n按照要求，我们使用充分统计量 $\\bar{y}$ 和 $n$。指数中的平方和可以展开为 $\\sum_{i=1}^{n} (y_i - \\theta)^2 = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + n(\\bar{y} - \\theta)^2$。由于项 $\\sum (y_i - \\bar{y})^2$ 不依赖于 $\\theta$，它是一个比例常数。因此，作为 $\\theta$ 的函数，似然与以下表达式成正比：\n$$ L(\\theta \\mid \\bar{y}, n) \\propto \\exp\\left(-\\frac{n(\\bar{y} - \\theta)^2}{2\\sigma^2}\\right) $$\n$\\theta$ 的先验是 $\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$，因此先验密度与以下表达式成正比：\n$$ p(\\theta) \\propto \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right) $$\n未归一化的后验 $\\pi(\\theta)$ 与这两个表达式的乘积成正比。对数后验（不考虑加法常数）为：\n$$ \\log \\pi(\\theta) \\stackrel{c}{=} -\\frac{n(\\bar{y} - \\theta)^2}{2\\sigma^2} - \\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} $$\n我们将此函数表示为 $\\text{log_target}(\\theta)$。对数接受率中的项 $\\log(\\pi(\\theta')) - \\log(\\pi(\\theta))$ 变为 $\\text{log_target}(\\theta') - \\text{log_target}(\\theta)$。\n\n**2. 提议密度与 Hastings 修正**\n\n我们分析指定的两个提议族，以确定 Hastings 修正项 $\\log(q(\\theta \\mid \\theta')) - \\log(q(\\theta' \\mid \\theta))$。\n\n**a. 随机游走正态提议**\n提议为 $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, s^2)$。忽略常数，对数提议密度为：\n$$ \\log q(\\theta' \\mid \\theta) \\stackrel{c}{=} -\\frac{(\\theta' - \\theta)^2}{2s^2} $$\n从 $\\theta'$ 到 $\\theta$ 的逆向移动的密度为 $q(\\theta \\mid \\theta') = \\mathcal{N}(\\theta', s^2)$。其对数密度为：\n$$ \\log q(\\theta \\mid \\theta') \\stackrel{c}{=} -\\frac{(\\theta - \\theta')^2}{2s^2} $$\n由于 $(\\theta' - \\theta)^2 = (\\theta - \\theta')^2$，提议分布是对称的，即 $q(\\theta' \\mid \\theta) = q(\\theta \\mid \\theta')$。Hastings 修正项为 $\\log(1) = 0$。\n对数接受率简化为：\n$$ \\log R_{\\text{RW}} = \\text{log_target}(\\theta') - \\text{log_target}(\\theta) $$\n\n**b. 独立正态提议**\n提议为 $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\mu_q, \\sigma_q^2)$，它不依赖于当前状态 $\\theta$。我们可以写作 $q(\\theta' \\mid \\theta) = q(\\theta')$。对数提议密度为：\n$$ \\log q(\\theta' \\mid \\theta) = \\log q(\\theta') \\stackrel{c}{=} -\\frac{(\\theta' - \\mu_q)^2}{2\\sigma_q^2} $$\n逆向提议密度为 $q(\\theta \\mid \\theta') = q(\\theta) = \\mathcal{N}(\\mu_q, \\sigma_q^2)$：\n$$ \\log q(\\theta \\mid \\theta') = \\log q(\\theta) \\stackrel{c}{=} -\\frac{(\\theta - \\mu_q)^2}{2\\sigma_q^2} $$\n对数 Hastings 修正为非零：\n$$ \\log\\left(\\frac{q(\\theta \\mid \\theta')}{q(\\theta' \\mid \\theta)}\\right) = \\log q(\\theta) - \\log q(\\theta') = \\left(-\\frac{(\\theta - \\mu_q)^2}{2\\sigma_q^2}\\right) - \\left(-\\frac{(\\theta' - \\mu_q)^2}{2\\sigma_q^2}\\right) = \\frac{(\\theta' - \\mu_q)^2 - (\\theta - \\mu_q)^2}{2\\sigma_q^2} $$\n完整的对数接受率为：\n$$ \\log R_{\\text{IND}} = (\\text{log_target}(\\theta') - \\text{log_target}(\\theta)) + \\frac{(\\theta' - \\mu_q)^2 - (\\theta - \\mu_q)^2}{2\\sigma_q^2} $$\n\n**3. 实现策略**\n\nPython 实现将定义一个函数来计算给定模型参数的 $\\text{log_target}(\\theta)$。然后它将遍历测试用例。对于每个用例，它将识别提议类型，并使用上面推导出的适当公式计算对数接受率 $\\log R$。最后，它将计算 $\\alpha = \\min(1, \\exp(\\log R))$ 并存储结果。\n\n特殊情况：对于情况 $3$，$\\theta = \\theta' = 0.7$。在这种情况下，$\\text{log_target}(\\theta') = \\text{log_target}(\\theta)$，并且无论提议族如何，Hastings 修正项也为 $0$。因此，$\\log R = 0$，且 $\\alpha = \\min(1, \\exp(0)) = 1$。程序应能正确处理这种情况。\n对于情况 $4$，提议状态 $\\theta'=5.0$ 远离后验分布的中心（该后验分布集中在 $\\bar{y}=0.2$ 附近）。这将导致一个很大的负数 $\\log R$ 和一个计算上为零的接受概率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Metropolis-Hastings acceptance probability for a Normal-Normal model\n    under two different proposal families for a series of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'case': 1, 'proposal_type': 'random_walk',\n            'params': {'y_bar': 1.2, 'n': 50, 'sigma2': 16, 'mu0': 0, 'tau02': 100},\n            'states': {'theta': 1.5, 'theta_prime': 1.8},\n            'proposal_params': {'s2': 0.25}\n        },\n        {\n            'case': 2, 'proposal_type': 'independence',\n            'params': {'y_bar': 8.4, 'n': 5, 'sigma2': 4, 'mu0': 10, 'tau02': 1},\n            'states': {'theta': 8, 'theta_prime': 6},\n            'proposal_params': {'mu_q': 9, 'sigma_q2': 9}\n        },\n        {\n            'case': 3, 'proposal_type': 'independence',\n            'params': {'y_bar': 0.3, 'n': 10, 'sigma2': 1, 'mu0': 0, 'tau02': 4},\n            'states': {'theta': 0.7, 'theta_prime': 0.7},\n            'proposal_params': {'mu_q': 0, 'sigma_q2': 9}\n        },\n        {\n            'case': 4, 'proposal_type': 'random_walk',\n            'params': {'y_bar': 0.2, 'n': 200, 'sigma2': 9, 'mu0': 0, 'tau02': 1000},\n            'states': {'theta': 0.1, 'theta_prime': 5.0},\n            'proposal_params': {'s2': 1.0}\n        },\n        {\n            'case': 5, 'proposal_type': 'independence',\n            'params': {'y_bar': 0.6, 'n': 5, 'sigma2': 1, 'mu0': 0, 'tau02': 10},\n            'states': {'theta': 0.0, 'theta_prime': 0.8},\n            'proposal_params': {'mu_q': 1.0, 'sigma_q2': 0.25}\n        }\n    ]\n\n    results = []\n\n    def log_target_density(theta, y_bar, n, sigma2, mu0, tau02):\n        \"\"\"\n        Computes the log of the unnormalized posterior density (likelihood * prior).\n        This is proportional to log(pi(theta)).\n        \"\"\"\n        log_likelihood = -n * (y_bar - theta)**2 / (2 * sigma2)\n        log_prior = -(theta - mu0)**2 / (2 * tau02)\n        return log_likelihood + log_prior\n\n    for case in test_cases:\n        params = case['params']\n        states = case['states']\n        proposal_params = case['proposal_params']\n        \n        theta = states['theta']\n        theta_prime = states['theta_prime']\n\n        # If current and proposed states are identical, acceptance probability is 1.\n        if theta == theta_prime:\n            results.append(1.0)\n            continue\n\n        # Calculate the log of the posterior ratio\n        log_pi_ratio = log_target_density(theta_prime, **params) - log_target_density(theta, **params)\n\n        log_hastings_correction = 0.0\n\n        if case['proposal_type'] == 'random_walk':\n            # For a symmetric random-walk proposal, q(theta|theta') = q(theta'|theta),\n            # so the Hastings correction is 1, and its log is 0.\n            pass  # log_hastings_correction is already 0.0\n\n        elif case['proposal_type'] == 'independence':\n            # For an independence sampler, q(theta'|theta) = q(theta'),\n            # so the Hastings correction is q(theta)/q(theta').\n            mu_q = proposal_params['mu_q']\n            sigma_q2 = proposal_params['sigma_q2']\n            \n            # log(q(theta)) - log(q(theta'))\n            # The constant terms cancel.\n            log_q_theta = -(theta - mu_q)**2 / (2 * sigma_q2)\n            log_q_theta_prime = -(theta_prime - mu_q)**2 / (2 * sigma_q2)\n\n            log_hastings_correction = log_q_theta - log_q_theta_prime\n\n        # Total log acceptance ratio\n        log_acceptance_ratio = log_pi_ratio + log_hastings_correction\n        \n        # Acceptance probability\n        alpha = min(1.0, np.exp(log_acceptance_ratio))\n        results.append(alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在现实世界的应用中，尤其是在处理如医学统计中常见的层级模型时，简单的 MCMC 采样器可能会非常低效。本练习  探讨了一种强大的高级技术——重参数化（reparameterization），以显著提高采样器的性能。您将为中心化和非中心化两种参数设置实现吉布斯采样器（Gibbs sampler），并通过比较参数链的自相关性来量化它们之间的效率差异。这项高级实践展示了模型参数化的一个简单改变如何能够打破参数间的依赖性，从而极大地提升 MCMC 算法的混合速度与收敛性，这是任何严肃的实践者都应掌握的重要技巧。",
            "id": "3144797",
            "problem": "考虑一个分层正态模型，其中观测到的组级数据使用潜在的组均值和总体均值进行建模。目标是通过比较中心化参数化与非中心化参数化，分析重参数化对马尔可夫链混合的影响，并量化总体均值参数的自相关差异。此比较必须在一个观测方差和组水平方差已知的情景下，使用马尔可夫链蒙特卡洛（MCMC），特别是吉布斯采样来执行。\n\n基本原理：\n- 使用贝叶斯定理和正态-正态共轭性推导条件分布。\n- 使用马尔可夫链的定义和马尔可夫链蒙特卡洛（MCMC）的概念，其中构建链的转移使其具有作为平稳分布的目标后验分布。\n- 吉布斯采样是MCMC的一个特例，其中按顺序从每个变量的全条件分布中采样。\n- 滞后1的自相关定义为平稳时间序列连续样本之间的相关性。\n\n模型设定：\n- 对于每个组索引 $j \\in \\{1, \\dots, J\\}$，观测数据 $y_j$ 被建模为 $y_j \\mid \\mu_j \\sim \\mathcal{N}(\\mu_j, \\sigma^2)$，其中 $\\sigma^2$ 已知。\n- 潜在的组均值具有分层先验 $\\mu_j \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2)$，其中 $\\tau^2$ 已知。\n- 总体均值具有先验 $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$，其中 $m_0$ 和 $s_0^2$ 已知。\n\n两种参数化：\n- 中心化参数化：直接使用 $\\mu$ 和 $\\{\\mu_j\\}_{j=1}^J$ 进行参数化。\n- 非中心化参数化：通过引入 $\\eta_j \\sim \\mathcal{N}(0,1)$ 进行重参数化，并设置 $\\mu_j = \\mu + \\tau \\eta_j$，从而使用 $\\mu$ 和 $\\{\\eta_j\\}_{j=1}^J$ 进行参数化。\n\n任务：\n- 从贝叶斯定理和正态分布的性质出发，推导吉布斯采样所需的两种参数化下的全条件分布。不要使用快捷公式；从似然和先验的基本定义开始。\n- 实现对应于中心化和非中心化参数化的两个吉布斯采样器。\n- 对于每个采样器，收集总体均值 $\\mu$ 的样本，并计算预烧期后 $\\mu$ 链的滞后1自相关系数。对于一个时间序列 $\\{x_t\\}_{t=1}^N$，滞后1自相关定义为\n$$\n\\rho_1 = \\frac{\\sum_{t=1}^{N-1} (x_t - \\bar{x})(x_{t+1} - \\bar{x})}{\\sum_{t=1}^{N} (x_t - \\bar{x})^2},\n$$\n其中 $\\bar{x}$ 是 $\\{x_t\\}_{t=1}^N$ 的样本均值。\n- 通过计算 $\\Delta = \\rho_{1,\\text{centered}} - \\rho_{1,\\text{non-centered}}$ 来量化自相关的差异。一个正的 $\\Delta$ 表示中心化参数化的自相关性高于非中心化参数化，而负的 $\\Delta$ 则表示相反情况。\n\n测试套件：\n使用以下案例，每个案例由 $(J, \\{y_j\\}_{j=1}^J, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed})$ 指定，所有量都明确以数字形式给出。\n\n- 案例1（弱数据，预计非中心化混合效果更好）：$J = 8$, $\\{y_j\\} = [5, 7, 3, -2, 0, 4, 6, 8]$, $\\sigma = 5$, $\\tau = 1$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 123$。\n- 案例2（强数据，预计中心化混合效果更好）：$J = 8$, $\\{y_j\\} = [5, 7, 3, -2, 0, 4, 6, 8]$, $\\sigma = 1$, $\\tau = 5$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 456$。\n- 案例3（只有一个组的边界情况）：$J = 1$, $\\{y_j\\} = [3]$, $\\sigma = 5$, $\\tau = 2$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 789$。\n- 案例4（多组，中等信息量）：$J = 50$, $\\{y_j\\}$ 由 $y_j = 0.3 \\times (j - 25.5)$ 确定性地定义（对于 $j = 1, \\dots, 50$），$\\sigma = 2$, $\\tau = 2$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 987$。\n\n要求：\n- 实现两个吉布斯采样器并为每个案例计算 $\\Delta$。\n- 使用提供的种子以确保确定性输出。\n- 此问题不涉及物理单位。\n- 您的程序应生成单行输出，其中包含按四个案例顺序排列的结果，格式为逗号分隔的列表并用方括号括起，例如 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$。",
            "solution": "用户要求使用吉布斯采样比较分层正态模型的中心化和非中心化参数化。主要目标是通过计算总体均值参数 $\\mu$ 链的滞后1自相关来分析马尔可夫链蒙特卡洛（MCMC）混合性质的差异。\n\n### 问题验证\n\n**步骤1：提取已知信息**\n-   **模型:**\n    -   数据似然：$y_j \\mid \\mu_j \\sim \\mathcal{N}(\\mu_j, \\sigma^2)$，对于 $j \\in \\{1, \\dots, J\\}$，其中 $\\sigma^2$ 已知。\n    -   组均值的分层先验：$\\mu_j \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2)$，其中 $\\tau^2$ 已知。\n    -   总体均值的先验：$\\mu \\sim \\mathcal{N}(m_0, s_0^2)$，其中 $m_0$ 和 $s_0^2$ 已知。\n-   **参数化:**\n    -   中心化：参数为 $\\mu$ 和 $\\{\\mu_j\\}_{j=1}^J$。\n    -   非中心化：通过 $\\eta_j \\sim \\mathcal{N}(0,1)$ 重参数化，使得 $\\mu_j = \\mu + \\tau \\eta_j$。参数为 $\\mu$ 和 $\\{\\eta_j\\}_{j=1}^J$。\n-   **任务:**\n    1.  在两种参数化下推导吉布斯采样的全条件分布。\n    2.  实现两个吉布斯采样器。\n    3.  为每个采样器计算 $\\mu$ 的预烧期后链的滞后1自相关 $\\rho_1$。公式为 $\\rho_1 = \\frac{\\sum_{t=1}^{N-1} (x_t - \\bar{x})(x_{t+1} - \\bar{x})}{\\sum_{t=1}^{N} (x_t - \\bar{x})^2}$。\n    4.  为每个测试案例计算差异 $\\Delta = \\rho_{1,\\text{centered}} - \\rho_{1,\\text{non-centered}}$。\n-   **测试套件:**\n    -   案例1：$(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (8, [5, 7, 3, -2, 0, 4, 6, 8], 5, 1, 0, 10, 10000, 2000, 123)$。\n    -   案例2：$(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (8, [5, 7, 3, -2, 0, 4, 6, 8], 1, 5, 0, 10, 10000, 2000, 456)$。\n    -   案例3：$(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (1, [3], 5, 2, 0, 10, 10000, 2000, 789)$。\n    -   案例4：$(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (50, \\{0.3 \\times (j - 25.5)\\}_{j=1}^{50}, 2, 2, 0, 10, 10000, 2000, 987)$。\n\n**步骤2：使用提取的已知信息进行验证**\n该问题具有科学依据、定义明确且客观。它是计算贝叶斯统计中的一个标准练习，专注于MCMC效率。\n-   **科学依据**：分层正态模型、吉布斯采样以及用于改善混合的重参数化概念是贝叶斯统计和统计学习中的基本主题。所有模型和公式都基于已建立的概率论和统计学。\n-   **定义明确性**：问题提供了所有必要的数据、参数和一个确定性程序（包括随机种子），以便为每个测试案例得出唯一且有意义的数值结果。\n-   **客观性**：语言精确且技术性强。没有主观因素。\n\n**步骤3：结论与行动**\n问题有效。将提供完整解决方案。\n\n### 全条件分布的推导\n\n吉布斯采样的核心是为每个参数从其全条件分布中迭代抽取样本，全条件分布是指该参数在所有其他参数和数据条件下的分布。我们使用贝叶斯定理为两种参数化推导这些分布，该定理指出后验与似然乘以先验成正比，即 $p(\\theta|D) \\propto p(D|\\theta)p(\\theta)$。\n\n#### 1. 中心化参数化\n\n参数为 $\\theta_C = (\\mu, \\mu_1, \\dots, \\mu_J)$。联合后验分布为：\n$$p(\\mu, \\{\\mu_j\\}_{j=1}^J | \\{y_j\\}_{j=1}^J) \\propto p(\\{y_j\\} | \\{\\mu_j\\}) p(\\{\\mu_j\\} | \\mu) p(\\mu)$$\n$$p(\\mu, \\{\\mu_j\\} | \\{y_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(y_j | \\mu_j, \\sigma^2) \\right) \\left( \\prod_{j=1}^J \\mathcal{N}(\\mu_j | \\mu, \\tau^2) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n\n**$\\mu$ 的全条件分布：**\n$\\mu$ 在所有其他参数条件下的分布仅依赖于在图形模型中作为其父节点的那些参数，即 $\\{\\mu_j\\}$。\n$$p(\\mu | \\{\\mu_j\\}, \\{y_j\\}) \\propto p(\\mu | \\{\\mu_j\\}) \\propto p(\\{\\mu_j\\} | \\mu) p(\\mu)$$\n$$p(\\mu | \\{\\mu_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(\\mu_j | \\mu, \\tau^2) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n$$p(\\mu | \\{\\mu_j\\}) \\propto \\exp\\left( -\\frac{1}{2\\tau^2} \\sum_{j=1}^J (\\mu_j - \\mu)^2 \\right) \\exp\\left( -\\frac{1}{2s_0^2} (\\mu - m_0)^2 \\right)$$\n展开指数中 $\\mu$ 的二次项：\n$$-\\frac{1}{2}\\left[ \\mu^2 \\left(\\frac{J}{\\tau^2} + \\frac{1}{s_0^2}\\right) - 2\\mu \\left(\\frac{1}{\\tau^2}\\sum_{j=1}^J \\mu_j + \\frac{m_0}{s_0^2}\\right) + \\text{const} \\right]$$\n这是一个正态分布 $\\mathcal{N}(\\mu | M_{\\mu}, V_{\\mu})$ 的核。通过配方法或匹配项，我们找到后验方差 $V_{\\mu}$ 和均值 $M_{\\mu}$：\n$$V_{\\mu} = \\left(\\frac{J}{\\tau^2} + \\frac{1}{s_0^2}\\right)^{-1}$$\n$$M_{\\mu} = V_{\\mu} \\left(\\frac{\\sum_{j=1}^J \\mu_j}{\\tau^2} + \\frac{m_0}{s_0^2}\\right)$$\n因此，全条件分布为 $\\mu | \\{\\mu_j\\} \\sim \\mathcal{N}(M_{\\mu}, V_{\\mu})$。\n\n**$\\mu_j$ 的全条件分布：**\n由于模型中的条件独立性，$\\mu_j$ 的全条件分布仅依赖于 $y_j$ 和 $\\mu$。\n$$p(\\mu_j | \\mu, \\{\\mu_{k \\neq j}\\}, \\{y_k\\}) \\propto p(y_j | \\mu_j) p(\\mu_j | \\mu)$$\n$$p(\\mu_j | \\mu, y_j) \\propto \\mathcal{N}(y_j|\\mu_j, \\sigma^2) \\mathcal{N}(\\mu_j|\\mu, \\tau^2)$$\n$$p(\\mu_j | \\mu, y_j) \\propto \\exp\\left( -\\frac{(y_j - \\mu_j)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{(\\mu_j - \\mu)^2}{2\\tau^2} \\right)$$\n展开指数中 $\\mu_j$ 的二次项：\n$$-\\frac{1}{2}\\left[ \\mu_j^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}\\right) - 2\\mu_j \\left(\\frac{y_j}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right) + \\text{const} \\right]$$\n这是一个正态分布 $\\mathcal{N}(\\mu_j | M_j, V_j)$ 的核，其方差为 $V_j$，均值为 $M_j$：\n$$V_j = \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1}$$\n$$M_j = V_j \\left(\\frac{y_j}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right)$$\n对于每个 $j \\in \\{1, \\dots, J\\}$，全条件分布为 $\\mu_j | \\mu, y_j \\sim \\mathcal{N}(M_j, V_j)$。\n\n#### 2. 非中心化参数化\n\n参数为 $\\theta_{NC} = (\\mu, \\eta_1, \\dots, \\eta_J)$，映射关系为 $\\mu_j = \\mu + \\tau \\eta_j$。现在似然为 $y_j | \\mu, \\eta_j \\sim \\mathcal{N}(\\mu + \\tau\\eta_j, \\sigma^2)$。先验为 $\\eta_j \\sim \\mathcal{N}(0, 1)$ 和 $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$。联合后验为：\n$$p(\\mu, \\{\\eta_j\\} | \\{y_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(y_j | \\mu + \\tau\\eta_j, \\sigma^2) \\right) \\left( \\prod_{j=1}^J \\mathcal{N}(\\eta_j | 0, 1) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n\n**$\\mu$ 的全条件分布：**\n$$p(\\mu | \\{\\eta_j\\}, \\{y_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(y_j | \\mu + \\tau\\eta_j, \\sigma^2) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n$$p(\\mu | \\{\\eta_j\\}, \\{y_j\\}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{j=1}^J (y_j - \\tau\\eta_j - \\mu)^2 \\right) \\exp\\left( -\\frac{(\\mu - m_0)^2}{2s_0^2} \\right)$$\n这是一个标准的正态-正态共轭更新，其中我们有 $J$ 个观测值 $y'_j = y_j - \\tau\\eta_j$，每个观测值的均值为 $\\mu$，已知方差为 $\\sigma^2$。$\\mu$ 的全条件分布是 $\\mathcal{N}(\\mu | M_{\\mu, nc}, V_{\\mu, nc})$，其中：\n$$V_{\\mu, nc} = \\left(\\frac{J}{\\sigma^2} + \\frac{1}{s_0^2}\\right)^{-1}$$\n$$M_{\\mu, nc} = V_{\\mu, nc} \\left(\\frac{\\sum_{j=1}^J (y_j - \\tau\\eta_j)}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right)$$\n\n**$\\eta_j$ 的全条件分布：**\n$$p(\\eta_j | \\mu, \\{\\eta_{k \\neq j}\\}, \\{y_k\\}) \\propto p(y_j | \\mu, \\eta_j) p(\\eta_j)$$\n$$p(\\eta_j | \\mu, y_j) \\propto \\mathcal{N}(y_j|\\mu + \\tau\\eta_j, \\sigma^2) \\mathcal{N}(\\eta_j|0, 1)$$\n$$p(\\eta_j | \\mu, y_j) \\propto \\exp\\left( -\\frac{(y_j - \\mu - \\tau\\eta_j)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{\\eta_j^2}{2} \\right)$$\n展开指数中 $\\eta_j$ 的二次项：\n$$-\\frac{1}{2}\\left[ \\eta_j^2 \\left(\\frac{\\tau^2}{\\sigma^2} + 1\\right) - 2\\eta_j \\frac{\\tau(y_j - \\mu)}{\\sigma^2} + \\text{const} \\right]$$\n这是一个正态分布 $\\mathcal{N}(\\eta_j | M_{\\eta_j}, V_{\\eta_j})$ 的核，其参数为：\n$$V_{\\eta_j} = \\left(\\frac{\\tau^2}{\\sigma^2} + 1\\right)^{-1}$$\n$$M_{\\eta_j} = V_{\\eta_j} \\left(\\frac{\\tau(y_j - \\mu)}{\\sigma^2}\\right)$$\n对于每个 $j \\in \\{1, \\dots, J\\}$，全条件分布为 $\\eta_j | \\mu, y_j \\sim \\mathcal{N}(M_{\\eta_j}, V_{\\eta_j})$。\n\n### MCMC 性能与实现\n\n这两种参数化在数学上是等价的，但可能具有截然不同的计算特性。\n-   **中心化参数化：** 参数 $\\mu$ 和 $\\{\\mu_j\\}$ 在后验中通常高度相关，尤其是在数据稀疏（$\\sigma^2$ 较大）且分层方差较小（$\\tau^2$ 较小）时。这种依赖性意味着对 $\\mu$ 的一次抽样会强烈约束随后的对 $\\{\\mu_j\\}$ 的抽样，反之亦然。这种“粘性”导致吉布斯采样器在参数空间中移动缓慢，从而导致连续样本之间的高度自相关。\n-   **非中心化参数化：** 这种方法旨在通过采样 $\\mu$ 和一组独立的标准正态参数 $\\{\\eta_j\\}$ 来打破后验依赖性。依赖关系从先验转移到了似然中。这通常会减少被采样参数块之间的后验相关性，从而导致更快的混合和更低的自相关，尤其是在上述稀疏数据情景中。\n-   相反，当数据信息量大（$\\sigma^2$ 较小）时，中心化参数化可能表现良好，因为每个 $y_j$ 的似然精确地确定了 $\\mu_j$，从而减少了其对 $\\mu$ 的依赖。在这种情况下，非中心化参数化可能会遇到困难，因为一个强约束 $y_j \\approx \\mu + \\tau\\eta_j$ 可能会在 $\\mu$ 和 $\\eta_j$ 之间引起高的后验相关性。\n\n实现将包括两个吉布斯采样器函数，分别对应一种参数化。每个函数将从推导出的全条件分布中迭代采样 $N_{\\text{iter}}$ 步。在丢弃前 $N_{\\text{burn}}$ 个样本（预烧期）后，将计算 $\\mu$ 链的滞后1自相关。每个测试案例的最终结果是来自两个采样器的自相关之间的差异 $\\Delta$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_autocorr_lag1(chain):\n    \"\"\"\n    Computes the lag-1 autocorrelation for a given MCMC chain.\n    \"\"\"\n    if len(chain)  2:\n        return np.nan\n    \n    x_bar = np.mean(chain)\n    dev = chain - x_bar\n    denominator = np.dot(dev, dev)\n    \n    if denominator == 0:\n        return 0.0\n        \n    numerator = np.dot(dev[:-1], dev[1:])\n    return numerator / denominator\n\ndef gibbs_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed):\n    \"\"\"\n    Gibbs sampler for the centered parameterization.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    y = np.array(y)\n    \n    sigma2 = sigma**2\n    tau2 = tau**2\n    s02 = s0**2\n    \n    # Initialize parameters\n    mu = float(m0)\n    mu_j = np.copy(y)\n    \n    mu_samples = np.zeros(N_iter)\n    \n    # Pre-compute conditional posterior variances (constant throughout sampling)\n    V_j = 1.0 / (1.0/sigma2 + 1.0/tau2)\n    V_mu = 1.0 / (J/tau2 + 1.0/s02)\n    \n    std_j = np.sqrt(V_j)\n    std_mu = np.sqrt(V_mu)\n\n    # Gibbs sampling loop\n    for i in range(N_iter):\n        # Update mu (population mean)\n        M_mu = V_mu * (np.sum(mu_j)/tau2 + m0/s02)\n        mu = rng.normal(loc=M_mu, scale=std_mu)\n        mu_samples[i] = mu\n\n        # Update mu_j (group means)\n        M_j = V_j * (y/sigma2 + mu/tau2)\n        mu_j = rng.normal(loc=M_j, scale=std_j)\n        \n    post_burn_in_samples = mu_samples[N_burn:]\n    return calculate_autocorr_lag1(post_burn_in_samples)\n\ndef gibbs_non_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed):\n    \"\"\"\n    Gibbs sampler for the non-centered parameterization.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    y = np.array(y)\n    \n    sigma2 = sigma**2\n    tau2 = tau**2\n    s02 = s0**2\n    \n    # Initialize parameters\n    mu = float(m0)\n    eta_j = np.zeros(J)\n    \n    mu_samples = np.zeros(N_iter)\n\n    # Pre-compute conditional posterior variances (constant throughout sampling)\n    V_mu_nc = 1.0 / (J/sigma2 + 1.0/s02)\n    V_eta_j = 1.0 / (tau2/sigma2 + 1.0)\n    \n    std_mu_nc = np.sqrt(V_mu_nc)\n    std_eta_j = np.sqrt(V_eta_j)\n\n    # Gibbs sampling loop\n    for i in range(N_iter):\n        # Update mu\n        M_mu_nc = V_mu_nc * (np.sum(y - tau * eta_j)/sigma2 + m0/s02)\n        mu = rng.normal(loc=M_mu_nc, scale=std_mu_nc)\n        mu_samples[i] = mu\n\n        # Update eta_j\n        M_eta_j = V_eta_j * (tau * (y - mu) / sigma2)\n        eta_j = rng.normal(loc=M_eta_j, scale=std_eta_j)\n        \n    post_burn_in_samples = mu_samples[N_burn:]\n    return calculate_autocorr_lag1(post_burn_in_samples)\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute the difference in autocorrelation.\n    \"\"\"\n    y_case4 = [0.3 * (j - 25.5) for j in range(1, 51)]\n\n    test_cases = [\n        # (J, {y_j}, sigma, tau, m0, s0, N_iter, N_burn, seed)\n        (8, [5, 7, 3, -2, 0, 4, 6, 8], 5, 1, 0, 10, 10000, 2000, 123),\n        (8, [5, 7, 3, -2, 0, 4, 6, 8], 1, 5, 0, 10, 10000, 2000, 456),\n        (1, [3], 5, 2, 0, 10, 10000, 2000, 789),\n        (50, y_case4, 2, 2, 0, 10, 10000, 2000, 987),\n    ]\n\n    results = []\n    for case in test_cases:\n        J, y, sigma, tau, m0, s0, N_iter, N_burn, seed = case\n        \n        # Run centered sampler\n        rho_centered = gibbs_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed)\n        \n        # Run non-centered sampler\n        rho_non_centered = gibbs_non_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed)\n        \n        # Compute the difference\n        delta_rho = rho_centered - rho_non_centered\n        results.append(delta_rho)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}