{
    "hands_on_practices": [
        {
            "introduction": "A core strength of hierarchical models is their ability to parse variation into different sources, a concept central to understanding multi-center clinical studies. This first practice explores this foundational idea within a simple random-intercept model. By deriving the Intraclass Correlation Coefficient (ICC), you will directly connect the model's variance parameters, $\\tau^2$ and $\\sigma^2$, to a tangible measure of how much of the total outcome variability is due to systematic differences between groups, such as hospitals. ",
            "id": "4953506",
            "problem": "Consider a multi-center clinical study of a continuous biomarker, where patients are nested within centers (e.g., hospitals). Suppose the response for patient $i$ in center $j$ is modeled as a Bayesian hierarchical random-intercept model\n$$\ny_{ij} \\,=\\, \\mu \\,+\\, u_{j} \\,+\\, \\epsilon_{ij},\n$$\nwith center-specific random intercepts $u_{j} \\sim \\mathcal{N}(0,\\tau^{2})$ and patient-level errors $\\epsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^{2})$. Assume that $u_{j}$ and $\\epsilon_{ij}$ are mutually independent across all indices and independent of $\\mu$ (treated as a constant for the purpose of variance calculations). Using only fundamental definitions of variance, covariance, and correlation, derive the intraclass correlation coefficient (ICC) defined as\n$$\n\\operatorname{Corr}\\!\\big(y_{ij},\\,y_{i' j}\\big)\n$$\nfor $i \\neq i'$ within the same center $j$, under the marginal distribution that integrates over $u_{j}$. Then, briefly interpret the ICC in clinical terms, focusing on what it quantifies about within-center similarity and between-center heterogeneity in this medical context. Express your final answer as a simplified symbolic expression in terms of $\\tau^{2}$ and $\\sigma^{2}$. No rounding is required.",
            "solution": "The problem requires the derivation of the intraclass correlation coefficient (ICC), defined as $\\operatorname{Corr}(y_{ij}, y_{i'j})$ for two distinct patients ($i \\neq i'$) within the same center $j$. The model for the response variable $y_{ij}$ is given by:\n$$\ny_{ij} = \\mu + u_j + \\epsilon_{ij}\n$$\nwhere $\\mu$ is a constant overall mean, $u_j \\sim \\mathcal{N}(0, \\tau^2)$ is the random intercept for center $j$, and $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$ is the random error for patient $i$ in center $j$. The random variables $u_j$ and $\\epsilon_{ij}$ are mutually independent.\n\nThe definition of correlation is $\\operatorname{Corr}(X, Y) = \\frac{\\operatorname{Cov}(X, Y)}{\\sqrt{\\operatorname{Var}(X) \\operatorname{Var}(Y)}}$. We must compute the variance of an observation, $\\operatorname{Var}(y_{ij})$, and the covariance between two observations from the same center, $\\operatorname{Cov}(y_{ij}, y_{i'j})$.\n\nFirst, we compute the variance of $y_{ij}$. Since $\\mu$ is a constant and $u_j$ and $\\epsilon_{ij}$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}(y_{ij}) = \\operatorname{Var}(\\mu + u_j + \\epsilon_{ij}) = \\operatorname{Var}(u_j) + \\operatorname{Var}(\\epsilon_{ij}) = \\tau^2 + \\sigma^2\n$$\nBy symmetry, $\\operatorname{Var}(y_{i'j}) = \\tau^2 + \\sigma^2$.\n\nNext, we compute the covariance between $y_{ij}$ and $y_{i'j}$ for $i \\neq i'$.\n$$\n\\operatorname{Cov}(y_{ij}, y_{i'j}) = \\operatorname{Cov}(\\mu + u_j + \\epsilon_{ij}, \\mu + u_j + \\epsilon_{i'j})\n$$\nUsing the bilinearity of covariance and the independence assumptions ($\\operatorname{Cov}(u_j, \\epsilon_{i'j}) = 0$, $\\operatorname{Cov}(\\epsilon_{ij}, u_j) = 0$, and $\\operatorname{Cov}(\\epsilon_{ij}, \\epsilon_{i'j}) = 0$ for $i \\neq i'$):\n$$\n\\operatorname{Cov}(y_{ij}, y_{i'j}) = \\operatorname{Cov}(u_j, u_j) + \\operatorname{Cov}(u_j, \\epsilon_{i'j}) + \\operatorname{Cov}(\\epsilon_{ij}, u_j) + \\operatorname{Cov}(\\epsilon_{ij}, \\epsilon_{i'j})\n$$\n$$\n\\operatorname{Cov}(y_{ij}, y_{i'j}) = \\operatorname{Var}(u_j) + 0 + 0 + 0 = \\tau^2\n$$\nThe shared random effect $u_j$ is the sole source of covariance between outcomes in the same center.\n\nFinally, we assemble the ICC:\n$$\n\\operatorname{ICC} = \\operatorname{Corr}(y_{ij}, y_{i'j}) = \\frac{\\operatorname{Cov}(y_{ij}, y_{i'j})}{\\sqrt{\\operatorname{Var}(y_{ij}) \\operatorname{Var}(y_{i'j})}} = \\frac{\\tau^2}{\\sqrt{(\\tau^2 + \\sigma^2)(\\tau^2 + \\sigma^2)}} = \\frac{\\tau^2}{\\tau^2 + \\sigma^2}\n$$\n\nThe interpretation of the ICC in this clinical context follows directly from its formula. The total variance of the biomarker measurements, $\\operatorname{Var}(y_{ij}) = \\tau^2 + \\sigma^2$, is partitioned into two components:\n- $\\tau^2$: The variance of the center-specific effects, representing the heterogeneity *between* centers.\n- $\\sigma^2$: The variance of patient-specific errors, representing the heterogeneity *within* centers.\n\nThe ICC, $\\frac{\\tau^2}{\\tau^2 + \\sigma^2}$, is thus the proportion of the total variance in the biomarker that is attributable to systematic differences between the clinical centers.\n- A **high ICC** (close to $1$) indicates that $\\tau^2$ is large relative to $\\sigma^2$. This implies that patients within the same center are highly similar to one another, and there are large, systematic differences between centers. In clinical terms, the choice of hospital significantly influences the patient's biomarker level.\n- A **low ICC** (close to $0$) indicates that $\\tau^2$ is small relative to $\\sigma^2$. This implies that most of the variation is due to individual patient differences, not the center they belong to. The outcomes of patients within a center are not strongly correlated, and the centers themselves are relatively homogeneous.\nIn summary, the ICC quantifies the degree of clustering or non-independence of observations within the same center, providing a measure of how much the between-center heterogeneity contributes to the total observed variation in the clinical study.",
            "answer": "$$\n\\boxed{\\frac{\\tau^{2}}{\\tau^{2} + \\sigma^{2}}}\n$$"
        },
        {
            "introduction": "Having established how hierarchical models partition variance, we now examine a direct and powerful consequence of this structure: shrinkage. This phenomenon, also known as \"borrowing strength,\" produces more stable estimates for individual groups by pulling them toward a central, pooled estimate, which is especially useful when some groups have small sample sizes. This exercise challenges you to derive the shrinkage factor in a Beta-Binomial model for complication rates, revealing mathematically how the model adaptively weighs information from a single hospital against the collective trend across all hospitals. ",
            "id": "4800178",
            "problem": "Consider a multicenter surgical cohort where $J$ hospitals independently report counts of post-operative complications. For hospital $j \\in \\{1,\\dots,J\\}$, let $y_j$ denote the number of patients with a complication out of $n_j$ operated patients, modeled as $y_j \\sim \\text{Binomial}(n_j,\\theta_j)$ with a hospital-specific complication probability $0 < \\theta_j < 1$. Assume an exchangeable hierarchical Bayesian model with the prior $\\theta_j \\mid \\alpha,\\beta \\sim \\text{Beta}(\\alpha,\\beta)$, where $(\\alpha,\\beta)$ have proper hyperpriors that encode uncertainty about the across-hospital complication profile. Let the pooled mean under the exchangeable prior be $\\mu_0 = \\alpha/(\\alpha+\\beta)$ and the empirical hospital complication rate be $\\hat{p}_j = y_j/n_j$.\n\nDefine the posterior mean shrinkage of $\\theta_j$ toward the pooled mean as the unique scalar $\\kappa_j \\in (0,1)$ such that the posterior mean of $\\theta_j$ given $(y_j,\\alpha,\\beta)$ can be written as the convex combination $E[\\theta_j \\mid y_j,\\alpha,\\beta] = \\kappa_j \\mu_0 + (1-\\kappa_j) \\hat{p}_j$. Starting from Bayes’ theorem and the Binomial likelihood with a Beta prior, derive a closed-form expression for $\\kappa_j$ as a function of $n_j$, $\\alpha$, and $\\beta$. Express your final answer as a single analytic expression. No numerical rounding is required.",
            "solution": "To find the expression for the shrinkage factor $\\kappa_j$, we first derive the posterior mean $E[\\theta_j \\mid y_j, \\alpha, \\beta]$ and then rearrange the expression to match the specified convex combination form, which allows us to identify $\\kappa_j$.\n\n1.  **Determine the posterior distribution of $\\theta_j$.**\n    The model involves a Binomial likelihood and a Beta prior, which is a conjugate pair.\n    -   Likelihood: $p(y_j \\mid \\theta_j) \\propto \\theta_j^{y_j} (1 - \\theta_j)^{n_j - y_j}$\n    -   Prior: $p(\\theta_j \\mid \\alpha, \\beta) \\propto \\theta_j^{\\alpha-1} (1 - \\theta_j)^{\\beta-1}$\n    \n    By Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n    $$\n    p(\\theta_j \\mid y_j, \\alpha, \\beta) \\propto \\left( \\theta_j^{y_j} (1 - \\theta_j)^{n_j - y_j} \\right) \\cdot \\left( \\theta_j^{\\alpha-1} (1 - \\theta_j)^{\\beta-1} \\right)\n    $$\n    $$\n    p(\\theta_j \\mid y_j, \\alpha, \\beta) \\propto \\theta_j^{y_j + \\alpha - 1} (1 - \\theta_j)^{n_j - y_j + \\beta - 1}\n    $$\n    This is the kernel of a Beta distribution, so the posterior distribution is $\\theta_j \\mid y_j, \\alpha, \\beta \\sim \\text{Beta}(\\alpha', \\beta')$, where the posterior parameters are $\\alpha' = y_j + \\alpha$ and $\\beta' = n_j - y_j + \\beta$.\n\n2.  **Calculate the posterior mean.**\n    The mean of a $\\text{Beta}(a, b)$ distribution is $\\frac{a}{a+b}$. Using the posterior parameters, the posterior mean of $\\theta_j$ is:\n    $$\n    E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\frac{\\alpha'}{\\alpha' + \\beta'} = \\frac{y_j + \\alpha}{(y_j + \\alpha) + (n_j - y_j + \\beta)} = \\frac{y_j + \\alpha}{n_j + \\alpha + \\beta}\n    $$\n\n3.  **Derive the shrinkage factor $\\kappa_j$.**\n    We need to rewrite the posterior mean to match the form $E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\kappa_j \\mu_0 + (1-\\kappa_j) \\hat{p}_j$, where $\\mu_0 = \\frac{\\alpha}{\\alpha+\\beta}$ and $\\hat{p}_j = \\frac{y_j}{n_j}$.\n    \n    We can rewrite the posterior mean as a weighted average:\n    $$\n    E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\frac{n_j (\\frac{y_j}{n_j}) + \\alpha}{n_j + \\alpha + \\beta} = \\frac{n_j \\hat{p}_j + \\alpha}{n_j + \\alpha + \\beta}\n    $$\n    Separating the terms:\n    $$\n    E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\left( \\frac{n_j}{n_j + \\alpha + \\beta} \\right) \\hat{p}_j + \\left( \\frac{\\alpha}{n_j + \\alpha + \\beta} \\right)\n    $$\n    Now, we manipulate the second term to introduce $\\mu_0$:\n    $$\n    \\frac{\\alpha}{n_j + \\alpha + \\beta} = \\left( \\frac{\\alpha + \\beta}{n_j + \\alpha + \\beta} \\right) \\frac{\\alpha}{\\alpha+\\beta} = \\left( \\frac{\\alpha + \\beta}{n_j + \\alpha + \\beta} \\right) \\mu_0\n    $$\n    Substituting this back, we get the posterior mean in the desired convex combination form:\n    $$\n    E[\\theta_j \\mid y_j, \\alpha, \\beta] = \\left( \\frac{n_j}{n_j + \\alpha + \\beta} \\right) \\hat{p}_j + \\left( \\frac{\\alpha + \\beta}{n_j + \\alpha + \\beta} \\right) \\mu_0\n    $$\n    By comparing this expression to $(1 - \\kappa_j) \\hat{p}_j + \\kappa_j \\mu_0$, we can directly identify the shrinkage factor $\\kappa_j$ as the coefficient of the prior mean $\\mu_0$:\n    $$\n    \\kappa_j = \\frac{\\alpha + \\beta}{n_j + \\alpha + \\beta}\n    $$\nThis factor represents the weight given to the prior mean $\\mu_0$ (the pooled estimate), which is the ratio of the prior's effective sample size ($\\alpha + \\beta$) to the total effective sample size (prior + data, $n_j + \\alpha + \\beta$).",
            "answer": "$$\n\\boxed{\\frac{\\alpha + \\beta}{n_j + \\alpha + \\beta}}\n$$"
        },
        {
            "introduction": "The ultimate test of a statistical model is often its ability to make accurate predictions. In a hierarchical context, predictive uncertainty is not uniform; it depends critically on whether we are forecasting for a unit within an existing, observed group or for a unit in an entirely new group. This final practice solidifies this crucial concept by having you formalize the posterior predictive distributions for these two distinct scenarios, highlighting how the model correctly accounts for the additional uncertainty associated with a previously unseen hospital. ",
            "id": "4953491",
            "problem": "Consider a continuous clinical outcome modeled with a Bayesian hierarchical random-intercept structure across hospitals. For hospital index $j \\in \\{1,\\dots,J\\}$ and patient index $i \\in \\{1,\\dots,n_j\\}$, the model is\n$$\ny_{ij} \\mid \\boldsymbol{\\beta}, b_j, \\sigma^2 \\sim \\mathcal{N}\\!\\big(x_{ij}^\\top \\boldsymbol{\\beta} + b_j,\\ \\sigma^2\\big),\n$$\nwith hospital-specific random intercepts\n$$\nb_j \\mid \\tau^2 \\sim \\mathcal{N}\\!\\big(0,\\ \\tau^2\\big),\n$$\nand independent priors\n$$\n\\boldsymbol{\\beta} \\sim \\mathcal{N}\\!\\big(\\boldsymbol{m}_0,\\ \\mathbf{V}_0\\big),\\quad \\sigma^2 \\sim \\text{Inverse-Gamma}\\!\\big(a_0,\\ b_0\\big),\\quad \\tau^2 \\sim \\text{Inverse-Gamma}\\!\\big(c_0,\\ d_0\\big).\n$$\nAssume data $\\mathcal{D}=\\{(y_{ij}, x_{ij}, j): j=1,\\dots,J;\\ i=1,\\dots,n_j\\}$ have been observed from $J$ existing hospitals. Let $x_\\star$ denote the covariate vector for a single new patient. Using the definition of posterior predictive distributions from Bayes’ rule and conditional independence in hierarchical models, select the option that correctly specifies:\n\n(i) The posterior predictive distribution $p(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)$ for a new patient in an existing hospital $j_\\star \\in \\{1,\\dots,J\\}$; and\n\n(ii) The posterior predictive distribution $p(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})$ for a new patient in a previously unseen hospital, together with a correct characterization of how the uncertainty (variance) differs across these two cases, using the law of total variance as the fundamental basis.\n\nOptions:\n\nA. \n(i) \n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)\\;=\\;\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,db_{j_\\star}\\,d\\sigma^2.\n$$\n(ii) \n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})\\;=\\;\\int\\!\\!\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{\\text{new}},\\ \\sigma^2\\big)\\,\\mathcal{N}\\!\\big(b_{\\text{new}} \\mid 0,\\ \\tau^2\\big)\\,p(\\boldsymbol{\\beta}, \\sigma^2, \\tau^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,d\\sigma^2\\,d\\tau^2\\,db_{\\text{new}}.\n$$\nUnder the law of total variance,\n$$\n\\operatorname{Var}\\!\\big(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star\\big)\\;=\\;\\mathbb{E}\\!\\big[\\sigma^2 \\mid \\mathcal{D}\\big]\\;+\\;\\operatorname{Var}\\!\\big(x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star} \\mid \\mathcal{D}\\big),\n$$\nwhereas\n$$\n\\operatorname{Var}\\!\\big(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new}\\big)\\;=\\;\\mathbb{E}\\!\\big[\\sigma^2 \\mid \\mathcal{D}\\big]\\;+\\;\\operatorname{Var}\\!\\big(x_\\star^\\top \\boldsymbol{\\beta} \\mid \\mathcal{D}\\big)\\;+\\;\\mathbb{E}\\!\\big[\\tau^2 \\mid \\mathcal{D}\\big].\n$$\nBecause $b_{j_\\star}$ is informed by hospital-$j_\\star$ data, $\\operatorname{Var}\\!\\big(b_{j_\\star} \\mid \\mathcal{D}\\big)$ is typically less than $\\mathbb{E}\\!\\big[\\tau^2 \\mid \\mathcal{D}\\big]$, so the new-hospital predictive variance is generally larger.\n\nB.\n(i)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)\\;=\\;\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,d\\sigma^2,\n$$\nand\n(ii)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})\\;=\\;\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,d\\sigma^2,\n$$\nwith the uncertainty being identical across the two cases because the random intercepts integrate out automatically.\n\nC.\n(i)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)\\;=\\;\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,db_{j_\\star}\\,d\\sigma^2,\n$$\nand\n(ii)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})\\;=\\;\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{\\text{new}},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, b_{\\text{new}}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,db_{\\text{new}}\\,d\\sigma^2,\n$$\nwith uncertainty smaller for the new hospital because $b_{\\text{new}}$ has mean zero, so it does not contribute variability.\n\nD.\n(i)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)\\;=\\;\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,db_{j_\\star}\\,d\\sigma^2,\n$$\nand\n(ii)\n$$\np(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})\\;=\\;\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,d\\sigma^2,\n$$\nwith uncertainties equal provided that $\\mathbb{E}\\!\\big[\\tau^2 \\mid \\mathcal{D}\\big]=\\operatorname{Var}\\!\\big(b_{j_\\star} \\mid \\mathcal{D}\\big)$, which holds under exchangeability of hospitals.",
            "solution": "The solution relies on the formal definition of the posterior predictive distribution, which involves integrating the likelihood of a new observation over the posterior distribution of the model parameters. The structure of this integral differs depending on whether the prediction is for an existing group or a new group.\n\n1.  **Predictive Distribution for an Existing Hospital ($j_\\star$)**:\n    For a new patient in an existing hospital $j_\\star$, the outcome $y_\\star$ depends on the global parameters $(\\boldsymbol{\\beta}, \\sigma^2)$ and the specific random intercept $b_{j_\\star}$ for that hospital. The posterior predictive distribution is formed by averaging the data-generating distribution $\\mathcal{N}(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star}, \\sigma^2)$ over the joint posterior distribution of the relevant parameters, $p(\\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2 \\mid \\mathcal{D})$. This gives the integral:\n    $$\n    p(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star)\\;=\\;\\int\\!\\!\\int\\!\\!\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star},\\ \\sigma^2\\big)\\,p(\\boldsymbol{\\beta}, b_{j_\\star}, \\sigma^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,db_{j_\\star}\\,d\\sigma^2.\n    $$\n\n2.  **Predictive Distribution for a New Hospital**:\n    For a new patient in a previously unseen hospital, we must account for a new, unknown random intercept, $b_{\\text{new}}$. This new intercept is a draw from the population distribution, $b_{\\text{new}} \\sim \\mathcal{N}(0, \\tau^2)$. The posterior predictive distribution must average over the uncertainty in the global parameters $(\\boldsymbol{\\beta}, \\sigma^2, \\tau^2)$ as well as the uncertainty from this new random intercept. This requires an additional integration:\n    $$\n    p(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new})\\;=\\;\\int \\mathcal{N}\\!\\big(y_\\star \\mid x_\\star^\\top \\boldsymbol{\\beta} + b_{\\text{new}},\\ \\sigma^2\\big)\\,p(b_{\\text{new}} \\mid \\tau^2)\\,p(\\boldsymbol{\\beta}, \\sigma^2, \\tau^2 \\mid \\mathcal{D})\\;d\\boldsymbol{\\beta}\\,d\\sigma^2\\,d\\tau^2\\,db_{\\text{new}}.\n    $$\n    This matches the integral in option A(ii).\n\n3.  **Comparison of Uncertainty (Variance)**:\n    Using the law of total variance, $\\operatorname{Var}(Y) = \\mathbb{E}[\\operatorname{Var}(Y \\mid \\theta)] + \\operatorname{Var}(\\mathbb{E}[Y \\mid \\theta])$, we can decompose the predictive variance in each case.\n    -   **Existing Hospital**: The total variance is the sum of the expected sampling variance and the variance in the predicted mean:\n        $$\n        \\operatorname{Var}\\!\\big(y_\\star \\mid x_\\star, \\mathcal{D}, j_\\star\\big)\\;=\\;\\mathbb{E}\\!\\big[\\sigma^2 \\mid \\mathcal{D}\\big]\\;+\\;\\operatorname{Var}\\!\\big(x_\\star^\\top \\boldsymbol{\\beta} + b_{j_\\star} \\mid \\mathcal{D}\\big).\n        $$\n        The second term captures the posterior uncertainty in both the global coefficients $\\boldsymbol{\\beta}$ and the hospital-specific intercept $b_{j_\\star}$.\n    -   **New Hospital**: For a new hospital, the total variance must also include the uncertainty from the new random intercept, which is governed by $\\tau^2$. The variance decomposition becomes:\n        $$\n        \\operatorname{Var}\\!\\big(y_\\star \\mid x_\\star, \\mathcal{D}, \\text{new}\\big)\\;=\\;\\mathbb{E}\\!\\big[\\sigma^2 \\mid \\mathcal{D}\\big]\\;+\\;\\mathbb{E}\\!\\big[\\tau^2 \\mid \\mathcal{D}\\big] \\;+\\;\\operatorname{Var}\\!\\big(x_\\star^\\top \\boldsymbol{\\beta} \\mid \\mathcal{D}\\big).\n        $$\n    Comparing the two, the predictive variance for the new hospital includes the term $\\mathbb{E}[\\tau^2 \\mid \\mathcal{D}]$, which represents the full between-hospital variability. In contrast, the term $\\operatorname{Var}(b_{j_\\star} \\mid \\mathcal{D})$ within the existing-hospital variance expression is the posterior variance of a *specific* random effect. Because the data $\\mathcal{D}$ (specifically, the data from hospital $j_\\star$) informs our estimate of $b_{j_\\star}$, its posterior variance is \"shrunk\" and is typically smaller than the overall population variance $\\tau^2$. Therefore, the predictive variance for a new hospital is generally larger, correctly reflecting the additional uncertainty of predicting for a completely new entity.\n\n    Option A provides the correct integral forms for both predictive distributions and the correct characterization of their respective variances and the reason for their difference. Options B, C, and D contain incorrect integral forms or flawed reasoning about the sources of uncertainty.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}