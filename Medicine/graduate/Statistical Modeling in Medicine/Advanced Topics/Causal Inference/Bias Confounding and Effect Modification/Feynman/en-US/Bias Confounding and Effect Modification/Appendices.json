{
    "hands_on_practices": [
        {
            "introduction": "Understanding confounding begins with a clear mathematical grasp of how an unobserved common cause can distort the relationship between an exposure and an outcome. This exercise provides a foundational look at omitted variable bias by using linear structural equations to derive the exact form of the asymptotic bias. By working through this derivation , you will see precisely how confounding arises from the association of the confounder with both the exposure and the outcome, and how statistical adjustment can, in principle, remove this bias.",
            "id": "4954330",
            "problem": "Consider an observational cohort study in which a continuous exposure $A$ (e.g., a biomarker level) and a continuous outcome $Y$ (e.g., a clinical severity index) are measured for $n$ patients. Let $U$ denote a patient-level latent factor (e.g., a baseline frailty score) that is a common cause of both $A$ and $Y$. Suppose the data-generating process is given by the linear structural equations\n$$\nA \\;=\\; \\alpha_U\\,U \\;+\\; \\epsilon_A,\\qquad\nY \\;=\\; \\beta_A\\,A \\;+\\; \\beta_U\\,U \\;+\\; \\epsilon_Y,\n$$\nwhere $U$, $\\epsilon_A$, and $\\epsilon_Y$ are independent and identically distributed (i.i.d.) across patients, with $E[U]=E[\\epsilon_A]=E[\\epsilon_Y]=0$, $\\operatorname{Var}(U)=\\sigma_U^2$, $\\operatorname{Var}(\\epsilon_A)=\\sigma_A^2$, and $\\operatorname{Var}(\\epsilon_Y)=\\sigma_Y^2$. Assume further that $U$, $\\epsilon_A$, and $\\epsilon_Y$ are mutually independent for each patient, and that observations are i.i.d. across patients. No other causes of $A$ or $Y$ are present.\n\nDefine the unadjusted estimator of $\\beta_A$ as the ordinary least squares (OLS) slope from the simple linear regression of $Y$ on $A$ (with an intercept), ignoring $U$. Under the stated assumptions and as $n \\to \\infty$, derive from first principles the probability limit of the unadjusted estimator and hence its asymptotic bias, defined as the large-sample limit of the estimator minus the true $\\beta_A$. Then, assuming $U$ were observed, use the same principles to argue whether the OLS coefficient on $A$ from the multiple linear regression of $Y$ on $(A,U)$ is asymptotically biased for $\\beta_A$ under the data-generating process above.\n\nProvide your final answer as a single closed-form analytic expression for the asymptotic bias of the unadjusted estimator in terms of $\\alpha_U$, $\\beta_U$, $\\sigma_U^2$, and $\\sigma_A^2$. Do not include any other symbols in your final expression. No numerical approximation is required; do not round.",
            "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, and objective. It presents a standard, canonical problem in statistical modeling concerning omitted variable bias (a form of confounding) and is free of any invalidating flaws. The solution process may therefore proceed.\n\nThe problem asks for the asymptotic bias of an unadjusted estimator for a regression coefficient and an argument concerning the bias of an adjusted estimator. We will address these two parts in sequence.\n\nFirst, consider the unadjusted estimator of $\\beta_A$, which is the ordinary least squares (OLS) slope from the simple linear regression of $Y$ on $A$. Let this unadjusted estimator be denoted by $\\hat{\\beta}_{A, \\text{unadj}}$. For a sample of size $n$, the formula for this estimator is:\n$$\n\\hat{\\beta}_{A, \\text{unadj}} = \\frac{\\sum_{i=1}^n (A_i - \\bar{A})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (A_i - \\bar{A})^2} \\equiv \\frac{\\widehat{\\operatorname{Cov}}(A,Y)}{\\widehat{\\operatorname{Var}}(A)}\n$$\nwhere $\\bar{A}$ and $\\bar{Y}$ are the sample means of $A$ and $Y$, respectively, and $\\widehat{\\operatorname{Cov}}$ and $\\widehat{\\operatorname{Var}}$ denote the sample covariance and variance.\n\nTo find the asymptotic bias, we first need to determine the probability limit ($\\text{plim}$) of this estimator as the sample size $n \\to \\infty$. Since the observations $(A_i, Y_i)$ are i.i.d., by the Law of Large Numbers, the sample moments converge in probability to the corresponding population moments. By the continuous mapping theorem (invoking Slutsky's theorem), the ratio of these sample moments converges in probability to the ratio of the population moments:\n$$\n\\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{A, \\text{unadj}} = \\frac{\\operatorname{Cov}(A,Y)}{\\operatorname{Var}(A)}\n$$\nWe must now derive expressions for $\\operatorname{Var}(A)$ and $\\operatorname{Cov}(A,Y)$ from first principles, using the given structural equations and statistical assumptions.\n\nThe structural equation for $A$ is $A = \\alpha_U U + \\epsilon_A$. The variables $U$ and $\\epsilon_A$ are assumed to be independent, with variances $\\operatorname{Var}(U) = \\sigma_U^2$ and $\\operatorname{Var}(\\epsilon_A) = \\sigma_A^2$. The variance of $A$ is therefore:\n$$\n\\operatorname{Var}(A) = \\operatorname{Var}(\\alpha_U U + \\epsilon_A)\n$$\nDue to the independence of $U$ and $\\epsilon_A$, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}(A) = \\operatorname{Var}(\\alpha_U U) + \\operatorname{Var}(\\epsilon_A) = \\alpha_U^2 \\operatorname{Var}(U) + \\operatorname{Var}(\\epsilon_A) = \\alpha_U^2 \\sigma_U^2 + \\sigma_A^2\n$$\n\nNext, we derive the covariance between $A$ and $Y$. We start with the structural equation for $Y$, $Y = \\beta_A A + \\beta_U U + \\epsilon_Y$. Using the property of covariance, we have:\n$$\n\\operatorname{Cov}(A,Y) = \\operatorname{Cov}(A, \\beta_A A + \\beta_U U + \\epsilon_Y) = \\beta_A \\operatorname{Cov}(A,A) + \\beta_U \\operatorname{Cov}(A,U) + \\operatorname{Cov}(A,\\epsilon_Y)\n$$\nThis simplifies to:\n$$\n\\operatorname{Cov}(A,Y) = \\beta_A \\operatorname{Var}(A) + \\beta_U \\operatorname{Cov}(A,U) + \\operatorname{Cov}(A,\\epsilon_Y)\n$$\nWe need to evaluate the two covariance terms. For $\\operatorname{Cov}(A,U)$, we substitute the expression for $A$:\n$$\n\\operatorname{Cov}(A,U) = \\operatorname{Cov}(\\alpha_U U + \\epsilon_A, U) = \\alpha_U \\operatorname{Cov}(U,U) + \\operatorname{Cov}(\\epsilon_A, U)\n$$\nSince $U$ and $\\epsilon_A$ are independent, $\\operatorname{Cov}(\\epsilon_A, U) = 0$. Thus:\n$$\n\\operatorname{Cov}(A,U) = \\alpha_U \\operatorname{Var}(U) = \\alpha_U \\sigma_U^2\n$$\nFor $\\operatorname{Cov}(A,\\epsilon_Y)$, we again substitute the expression for $A$:\n$$\n\\operatorname{Cov}(A,\\epsilon_Y) = \\operatorname{Cov}(\\alpha_U U + \\epsilon_A, \\epsilon_Y) = \\alpha_U \\operatorname{Cov}(U, \\epsilon_Y) + \\operatorname{Cov}(\\epsilon_A, \\epsilon_Y)\n$$\nBy assumption, $U$, $\\epsilon_A$, and $\\epsilon_Y$ are mutually independent. This implies $\\operatorname{Cov}(U, \\epsilon_Y) = 0$ and $\\operatorname{Cov}(\\epsilon_A, \\epsilon_Y) = 0$. Therefore:\n$$\n\\operatorname{Cov}(A,\\epsilon_Y) = 0\n$$\nSubstituting these covariance results back into the expression for $\\operatorname{Cov}(A,Y)$:\n$$\n\\operatorname{Cov}(A,Y) = \\beta_A \\operatorname{Var}(A) + \\beta_U (\\alpha_U \\sigma_U^2) + 0 = \\beta_A (\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2) + \\alpha_U \\beta_U \\sigma_U^2\n$$\n\nNow we can find the probability limit of the unadjusted estimator:\n$$\n\\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{A, \\text{unadj}} = \\frac{\\operatorname{Cov}(A,Y)}{\\operatorname{Var}(A)} = \\frac{\\beta_A (\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2) + \\alpha_U \\beta_U \\sigma_U^2}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2}\n$$\nSeparating the terms gives:\n$$\n\\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{A, \\text{unadj}} = \\frac{\\beta_A (\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2)}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2} + \\frac{\\alpha_U \\beta_U \\sigma_U^2}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2} = \\beta_A + \\frac{\\alpha_U \\beta_U \\sigma_U^2}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2}\n$$\nThe asymptotic bias is defined as the probability limit of the estimator minus the true parameter value, $\\beta_A$.\n$$\n\\text{Asymptotic Bias} = \\left( \\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{A, \\text{unadj}} \\right) - \\beta_A = \\frac{\\alpha_U \\beta_U \\sigma_U^2}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2}\n$$\nThis bias arises because the unobserved confounder $U$ induces a correlation between the regressor $A$ and the error term of the simple regression model. The error term in the naive model $Y = \\beta_0 + \\beta_A A + \\text{error}$ is $\\beta_U U + \\epsilon_Y$. The regressor $A = \\alpha_U U + \\epsilon_A$ is correlated with this error term through the common factor $U$, violating the fundamental OLS assumption of exogeneity. The bias is non-zero if and only if $\\alpha_U \\neq 0$ (the confounder affects the exposure) and $\\beta_U \\neq 0$ (the confounder affects the outcome).\n\nSecond, consider the multiple linear regression of $Y$ on both $A$ and $U$, assuming $U$ were observed. The model being fitted is:\n$$\nY = \\gamma_0 + \\gamma_A A + \\gamma_U U + \\nu\n$$\nThe OLS estimators for $\\gamma_A$ and $\\gamma_U$ are consistent for the true parameters if the regressors are uncorrelated with the error term $\\nu$ in the population model. The true data-generating process is given by the structural equation:\n$$\nY = \\beta_A A + \\beta_U U + \\epsilon_Y\n$$\nWhen we fit the multiple regression model, we are attempting to estimate the parameters of this true structural relationship. By comparing the regression model to the true data-generating process, we see that the true coefficients are $\\beta_A$ and $\\beta_U$ (with a true intercept of $0$), and the error term $\\nu$ corresponds to $\\epsilon_Y$.\n\nFor the OLS estimator of the coefficient on $A$, let us call it $\\hat{\\beta}_{A, \\text{adj}}$, to be consistent for the true parameter $\\beta_A$, the regressors in the model ($A$ and $U$) must be uncorrelated with the error term ($\\epsilon_Y$). We verify these conditions:\n$1.$ $\\operatorname{Cov}(A, \\epsilon_Y)$: As derived earlier, $\\operatorname{Cov}(A, \\epsilon_Y) = \\operatorname{Cov}(\\alpha_U U + \\epsilon_A, \\epsilon_Y) = \\alpha_U \\operatorname{Cov}(U, \\epsilon_Y) + \\operatorname{Cov}(\\epsilon_A, \\epsilon_Y) = 0$, due to the mutual independence of $U$, $\\epsilon_A$, and $\\epsilon_Y$.\n$2.$ $\\operatorname{Cov}(U, \\epsilon_Y)$: This covariance is $0$ directly from the assumption that $U$ and $\\epsilon_Y$ are independent.\n\nSince all regressors in the multiple regression model are uncorrelated with the error term, the fundamental condition for OLS consistency is satisfied. Therefore, the probability limit of the OLS estimator for the coefficient on $A$ from the multiple regression of $Y$ on $(A, U)$ is the true parameter $\\beta_A$.\n$$\n\\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{A, \\text{adj}} = \\beta_A\n$$\nThe asymptotic bias of this adjusted estimator is $\\beta_A - \\beta_A = 0$. Thus, a correctly specified multiple regression model that includes the confounder $U$ provides an asymptotically unbiased estimate of the causal effect $\\beta_A$.\n\nThe final answer required is the asymptotic bias of the unadjusted estimator.",
            "answer": "$$\n\\boxed{\\frac{\\alpha_U \\beta_U \\sigma_U^2}{\\alpha_U^2 \\sigma_U^2 + \\sigma_A^2}}\n$$"
        },
        {
            "introduction": "Bias in medical studies does not only arise from common causes (confounding) but can also be introduced by the process of selecting subjects into a study, a phenomenon known as selection bias. This exercise  presents a classic numerical example of Berkson's paradox, where conditioning on a 'collider' variable—in this case, hospitalization—induces a spurious association between two otherwise independent diseases. This practice is crucial for developing an intuition for how study design and analysis choices can create misleading results.",
            "id": "4954402",
            "problem": "In a hospital-based case series study, consider two binary diseases, $D_1$ and $D_2$, in a target population. Suppose $D_1$ and $D_2$ are marginally independent in the population with $P(D_1{=}1){=}0.20$ and $P(D_2{=}1){=}0.30$, and let $H$ denote hospitalization during a specified window. Hospitalization is determined probabilistically by disease status as follows:\n- $P(H{=}1 \\mid D_1{=}0, D_2{=}0){=}0.01$,\n- $P(H{=}1 \\mid D_1{=}1, D_2{=}0){=}0.60$,\n- $P(H{=}1 \\mid D_1{=}0, D_2{=}1){=}0.50$,\n- $P(H{=}1 \\mid D_1{=}1, D_2{=}1){=}0.70$.\n\nUsing only the definitions of joint probability, independence, conditional probability, and Bayes' rule, and taking the odds ratio ($OR$) to mean the odds ratio comparing $D_1$ and $D_2$ within a stratum, determine the induced association between $D_1$ and $D_2$ among hospitalized patients. Which option below correctly quantifies and interprets this association, thereby providing a concrete numerical example of Berkson’s paradox?\n\nA. Among hospitalized patients, $D_1$ and $D_2$ are negatively associated, with conditional odds ratio $\\approx 0.023$.\n\nB. Among hospitalized patients, $D_1$ and $D_2$ remain independent, with conditional odds ratio $=1$.\n\nC. Among hospitalized patients, $D_1$ and $D_2$ are positively associated, with conditional odds ratio $\\approx 2.6$.\n\nD. The direction of association among hospitalized patients cannot be determined without $P(H{=}1)$.",
            "solution": "The user-provided problem statement has been critically validated and is determined to be sound.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   Two binary diseases are denoted by $D_1$ and $D_2$.\n-   In the target population, $D_1$ and $D_2$ are marginally independent.\n-   The prevalence of $D_1$ is $P(D_1=1) = 0.20$.\n-   The prevalence of $D_2$ is $P(D_2=1) = 0.30$.\n-   $H$ denotes hospitalization during a specified window, a binary variable.\n-   The conditional probabilities of hospitalization given disease status are:\n    -   $P(H=1 \\mid D_1=0, D_2=0) = 0.01$\n    -   $P(H=1 \\mid D_1=1, D_2=0) = 0.60$\n    -   $P(H=1 \\mid D_1=0, D_2=1) = 0.50$\n    -   $P(H=1 \\mid D_1=1, D_2=1) = 0.70$\n-   The task is to determine the induced association between $D_1$ and $D_2$ among hospitalized patients (the stratum where $H=1$) and quantify it using the odds ratio (OR).\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding:** The problem is a well-formulated exercise in probability theory, specifically illustrating the statistical phenomenon known as Berkson's paradox or collider bias. All principles involved (joint probability, conditional probability, Bayes' rule, odds ratio) are fundamental to statistics and epidemiology. The scenario is scientifically plausible.\n-   **Well-Posed:** The problem provides all necessary information to calculate the required conditional odds ratio. The question is precise, and a unique, stable solution exists.\n-   **Objective:** The problem is stated in precise, objective language using standard mathematical and statistical notation. There are no subjective or ambiguous terms.\n-   **Conclusion:** The problem statement is valid. It is scientifically sound, well-posed, and objective. It contains no internal contradictions or missing information.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will now be derived.\n\n**Derivation of the Solution**\n\nThe objective is to calculate the odds ratio ($OR$) between the two diseases, $D_1$ and $D_2$, conditional on being hospitalized ($H=1$). The conditional odds ratio is defined as:\n$$ OR_{D_1, D_2 \\mid H=1} = \\frac{P(D_1=1, D_2=1 \\mid H=1) \\cdot P(D_1=0, D_2=0 \\mid H=1)}{P(D_1=1, D_2=0 \\mid H=1) \\cdot P(D_1=0, D_2=1 \\mid H=1)} $$\nEach term in this expression, $P(D_1=i, D_2=j \\mid H=1)$, can be found using Bayes' rule:\n$$ P(D_1=i, D_2=j \\mid H=1) = \\frac{P(H=1 \\mid D_1=i, D_2=j) \\cdot P(D_1=i, D_2=j)}{P(H=1)} $$\nWhen we substitute these expressions into the odds ratio formula, the common denominator $P(H=1)^2$ cancels from the numerator and denominator. Therefore, we can write the odds ratio in terms of the given conditional probabilities and the joint probabilities in the general population:\n$$ OR_{D_1, D_2 \\mid H=1} = \\frac{\\left[ P(H=1 \\mid D_1=1, D_2=1) P(D_1=1, D_2=1) \\right] \\cdot \\left[ P(H=1 \\mid D_1=0, D_2=0) P(D_1=0, D_2=0) \\right]}{\\left[ P(H=1 \\mid D_1=1, D_2=0) P(D_1=1, D_2=0) \\right] \\cdot \\left[ P(H=1 \\mid D_1=0, D_2=1) P(D_1=0, D_2=1) \\right]} $$\nFirst, we calculate the joint probabilities $P(D_1=i, D_2=j)$ in the general population. We are given that $D_1$ and $D_2$ are marginally independent.\n-   $P(D_1=1) = 0.20 \\implies P(D_1=0) = 1 - 0.20 = 0.80$.\n-   $P(D_2=1) = 0.30 \\implies P(D_2=0) = 1 - 0.30 = 0.70$.\n\nUsing independence, $P(A, B) = P(A)P(B)$:\n-   $P(D_1=1, D_2=1) = P(D_1=1) \\cdot P(D_2=1) = 0.20 \\cdot 0.30 = 0.06$.\n-   $P(D_1=1, D_2=0) = P(D_1=1) \\cdot P(D_2=0) = 0.20 \\cdot 0.70 = 0.14$.\n-   $P(D_1=0, D_2=1) = P(D_1=0) \\cdot P(D_2=1) = 0.80 \\cdot 0.30 = 0.24$.\n-   $P(D_1=0, D_2=0) = P(D_1=0) \\cdot P(D_2=0) = 0.80 \\cdot 0.70 = 0.56$.\n\nNow, we calculate each component of the $OR$ formula's numerator and denominator. These are proportional to the joint probabilities within the hospitalized stratum.\n\n-   Numerator Part 1: $P(H=1 \\mid D_1=1, D_2=1) \\cdot P(D_1=1, D_2=1) = 0.70 \\cdot 0.06 = 0.042$.\n-   Numerator Part 2: $P(H=1 \\mid D_1=0, D_2=0) \\cdot P(D_1=0, D_2=0) = 0.01 \\cdot 0.56 = 0.0056$.\n\n-   Denominator Part 1: $P(H=1 \\mid D_1=1, D_2=0) \\cdot P(D_1=1, D_2=0) = 0.60 \\cdot 0.14 = 0.084$.\n-   Denominator Part 2: $P(H=1 \\mid D_1=0, D_2=1) \\cdot P(D_1=0, D_2=1) = 0.50 \\cdot 0.24 = 0.120$.\n\nFinally, we compute the conditional odds ratio:\n$$ OR_{D_1, D_2 \\mid H=1} = \\frac{0.042 \\cdot 0.0056}{0.084 \\cdot 0.120} = \\frac{0.0002352}{0.01008} $$\n$$ OR_{D_1, D_2 \\mid H=1} \\approx 0.02333... $$\nAn odds ratio significantly less than $1$ indicates a negative association. In the general population, the diseases are independent ($OR=1$). However, among the subpopulation of hospitalized patients, the presence of one disease is associated with a lower odds of having the other disease. This spurious negative association induced by conditioning on a common effect (hospitalization) is a classic example of Berkson's paradox, a form of collider bias.\n\n**Option-by-Option Analysis**\n\nA. **Among hospitalized patients, $D_1$ and $D_2$ are negatively associated, with conditional odds ratio $\\approx 0.023$.**\nOur calculation yields an odds ratio of approximately $0.02333...$. This value is substantially less than $1$, confirming a negative association. This option accurately describes both the direction of the association and its magnitude.\n**Verdict: Correct.**\n\nB. **Among hospitalized patients, $D_1$ and $D_2$ remain independent, with conditional odds ratio $=1$.**\nThis statement is false. Conditioning on the common effect $H$ induces a dependency. Our calculated odds ratio is approximately $0.023$, not $1$.\n**Verdict: Incorrect.**\n\nC. **Among hospitalized patients, $D_1$ and $D_2$ are positively associated, with conditional odds ratio $\\approx 2.6$.**\nThis is incorrect. The induced association is negative ($OR < 1$), not positive ($OR > 1$). The calculated value is also incorrect.\n**Verdict: Incorrect.**\n\nD. **The direction of association among hospitalized patients cannot be determined without $P(H{=}1)$.**\nThis is false. As demonstrated in the derivation, the overall probability of hospitalization, $P(H=1)$, appears as a common factor in the denominator of each conditional probability $P(D_1=i, D_2=j \\mid H=1)$ and thus cancels out of the odds ratio calculation. All the information required to determine the direction and magnitude of the association was provided in the problem statement.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "After identifying a statistically significant association, a critical question remains: could this result be due to unmeasured confounding? This practice introduces the E-value, a modern sensitivity analysis tool used to quantify the robustness of an observed risk ratio to potential unmeasured confounders. By deriving and calculating the E-value , you will learn a practical method to assess how strong the associations of a confounder with the exposure and outcome would need to be to explain away an observed effect.",
            "id": "4954364",
            "problem": "An observational cohort study evaluates whether preoperative elevation of a novel inflammatory biomarker is associated with increased $30$-day all-cause mortality among adult surgical patients. After adjustment for measured confounders, the fitted log-binomial model yields a risk ratio (RR) for death comparing high versus low biomarker of $RR_{\\text{obs}}=2.35$. You are concerned about a single, unmeasured binary confounder $U$ that may be associated with both the exposure $E$ (high biomarker) and the outcome $D$ (mortality), conditional on the measured covariates.\n\nBy definition, the E-value for a risk ratio is the minimum strength of association, on the risk ratio scale, that an unmeasured confounder would need to have with both the exposure and the outcome, conditional on the measured covariates, to reduce the true causal risk ratio to $1$ (i.e., to fully explain away the observed association).\n\nAdopt the following well-tested bias-bounding result: under standard conditions for multiplicative confounding with a single unmeasured binary confounder, the maximum multiplicative bias factor $B$ that such a confounder can induce satisfies\n\n$$\nB \\le \\frac{RR_{EU}\\times RR_{UD}}{RR_{EU} + RR_{UD} - 1},\n$$\n\nwhere $RR_{EU}$ is the risk ratio association of $U$ with $E$ conditional on measured covariates, and $RR_{UD}$ is the risk ratio association of $U$ with $D$ conditional on measured covariates.\n\nStarting only from the definition of the E-value and the bias-bounding result above, derive a closed-form expression for the E-value as a function of $RR_{\\text{obs}}$ when $RR_{\\text{obs}}>1$. Then evaluate your expression at $RR_{\\text{obs}}=2.35$. Round your numerical answer to four significant figures and report it as a pure number (no units).",
            "solution": "The relationship between the observed risk ratio ($RR_{\\text{obs}}$), the true causal risk ratio ($RR_{\\text{true}}$), and the bias factor due to unmeasured confounding ($B$) is given by $RR_{\\text{obs}} = RR_{\\text{true}} \\times B$. To \"fully explain away the observed association\" means to show that the observed association could arise from bias alone when the true causal risk ratio is null, i.e., $RR_{\\text{true}} = 1$. In this scenario, the bias factor must be equal to the observed risk ratio:\n$$B = RR_{\\text{obs}}$$\nThe problem provides an upper bound for the bias factor $B$ that can be produced by a single unmeasured binary confounder $U$:\n$$B \\le \\frac{RR_{EU} \\times RR_{UD}}{RR_{EU} + RR_{UD} - 1}$$\nwhere $RR_{EU}$ and $RR_{UD}$ are the risk ratio associations of the confounder with the exposure and outcome, respectively, with both values assumed to be $\\ge 1$.\n\nFor an unmeasured confounder to explain away the observed effect, the maximum possible bias it can generate must be at least as large as $RR_{\\text{obs}}$. This establishes the following necessary condition:\n$$RR_{\\text{obs}} \\le \\frac{RR_{EU} \\times RR_{UD}}{RR_{EU} + RR_{UD} - 1}$$\nThe E-value is defined as the *minimum* strength of association, let's call it $E$, that the confounder must have with *both* the exposure and the outcome to satisfy this condition. The expression on the right-hand side is an increasing function of both $RR_{EU}$ and $RR_{UD}$. Therefore, to find the minimum value $E$ that meets the condition, we should set $RR_{EU} = E$ and $RR_{UD} = E$. The inequality then becomes:\n$$RR_{\\text{obs}} \\le \\frac{E^2}{2E - 1}$$\nThe E-value is the specific value of $E$ for which this inequality becomes an equality, representing the minimum threshold for the confounder-associations to potentially explain away the finding. We solve the equation for $E$:\n$$RR_{\\text{obs}} = \\frac{E^2}{2E - 1}$$\nRearranging this into a standard quadratic form ($aE^2 + bE + c = 0$):\n$$E^2 - (2 \\cdot RR_{\\text{obs}})E + RR_{\\text{obs}} = 0$$\nUsing the quadratic formula $E = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, we get:\n$$E = \\frac{2 \\cdot RR_{\\text{obs}} \\pm \\sqrt{(-2 \\cdot RR_{\\text{obs}})^2 - 4(1)(RR_{\\text{obs}})}}{2}$$\n$$E = \\frac{2 \\cdot RR_{\\text{obs}} \\pm \\sqrt{4 \\cdot RR_{\\text{obs}}^2 - 4 \\cdot RR_{\\text{obs}}}}{2}$$\n$$E = RR_{\\text{obs}} \\pm \\sqrt{RR_{\\text{obs}}^2 - RR_{\\text{obs}}}$$\nThis yields two solutions for $E$. Since the E-value represents a risk ratio, it must be $\\ge 1$. For $RR_{\\text{obs}} > 1$, the larger root, $RR_{\\text{obs}} + \\sqrt{RR_{\\text{obs}}^2 - RR_{\\text{obs}}}$, is always greater than 1. The smaller root, $RR_{\\text{obs}} - \\sqrt{RR_{\\text{obs}}^2 - RR_{\\text{obs}}}$, can be shown to be less than or equal to 1. Thus, the only meaningful solution is the larger root.\n\nThe closed-form expression for the E-value is:\n$$E = RR_{\\text{obs}} + \\sqrt{RR_{\\text{obs}}(RR_{\\text{obs}} - 1)}$$\nNow, we evaluate this expression for the given observed risk ratio, $RR_{\\text{obs}} = 2.35$:\n$$E = 2.35 + \\sqrt{2.35 \\times (2.35 - 1)}$$\n$$E = 2.35 + \\sqrt{2.35 \\times 1.35}$$\n$$E = 2.35 + \\sqrt{3.1725}$$\n$$E \\approx 2.35 + 1.78115131 \\approx 4.13115131$$\nRounding the result to four significant figures gives $4.131$. This means that to explain away the observed risk ratio of $2.35$, an unmeasured confounder would need to have a risk ratio association of at least $4.131$ with both the biomarker exposure and the mortality outcome, conditional on the covariates already in the model.",
            "answer": "$$ \\boxed{4.131} $$"
        }
    ]
}