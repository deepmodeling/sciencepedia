{
    "hands_on_practices": [
        {
            "introduction": "管理多重共线性的第一步是准确诊断其存在和严重程度。方差膨胀因子（VIF）是其中一个关键的诊断工具，它量化了由于共线性，回归系数估计值的方差被放大了多少。本练习将引导你从第一性原理出发计算VIF，从而将估计量方差这一抽象概念与预测变量相关矩阵的具体结构直接联系起来。",
            "id": "4952398",
            "problem": "一个生物医学研究团队正在建立一个多元线性回归模型，以研究一组慢性肾病患者队列中的连续肾脏风险评分 $Y$。模型中包含了三个临床相关的预测变量：血清肌酐 $X_{1}$、估计肾小球滤过率 $X_{2}$ 和年龄 $X_{3}$。每个预测变量都经过标准化处理，使其均值为零，方差为一，从而得到设计矩阵，其列为 $(Z_{1}, Z_{2}, Z_{3})$。$(Z_{1}, Z_{2}, Z_{3})$ 的样本相关系数矩阵为\n$$\n\\mathbf{R} \\;=\\;\n\\begin{pmatrix}\n1  -0.82  0.45 \\\\\n-0.82  1  -0.60 \\\\\n0.45  -0.60  1\n\\end{pmatrix}.\n$$\n从普通最小二乘 (OLS) 估计量和方差膨胀因子 (VIF) 的定义出发，从第一性原理推导每个系数的方差膨胀是如何由预测变量相关系数矩阵的逆矩阵产生的。然后，显式地计算矩阵 $\\mathbf{R}$ 的逆矩阵，并计算 $j \\in \\{1,2,3\\}$ 的 $\\operatorname{VIF}_{j}$。最后，基于将预测变量 $Z_{j}$ 对其余预测变量进行回归得到的决定系数 $R^{2}_{j}$，解释每个 $\\operatorname{VIF}_{j}$ 值在预测变量 $Z_{j}$ 相对于另外两个预测变量的冗余性方面的意义。\n\n将您最终的 $(\\operatorname{VIF}_{1}, \\operatorname{VIF}_{2}, \\operatorname{VIF}_{3})$ 数值答案四舍五入到四位有效数字。将最终的 VIF 向量表示为行矩阵。",
            "solution": "该问题要求从第一性原理推导方差膨胀因子 (VIF)，然后对一组给定的标准化预测变量计算并解释 VIF。\n\n### 第1部分：从第一性原理推导VIF\n\n标准的多元线性回归模型用矩阵形式表示为：\n$$ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} $$\n其中 $\\mathbf{y}$ 是响应变量观测值的 $n \\times 1$ 向量，$\\mathbf{X}$ 是一个 $n \\times (p+1)$ 的设计矩阵（包含一个截距列），$\\boldsymbol{\\beta}$ 是一个 $(p+1) \\times 1$ 的系数向量，$\\boldsymbol{\\epsilon}$ 是一个 $n \\times 1$ 的误差向量，满足 $\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}$ 和 $\\text{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 \\mathbf{I}_n$。\n\n$\\boldsymbol{\\beta}$ 的普通最小二乘 (OLS) 估计量是：\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n该估计量的协方差矩阵是：\n$$ \\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\text{Var}((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}) = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\text{Var}(\\mathbf{y}) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} $$\n由于 $\\text{Var}(\\mathbf{y}) = \\text{Var}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) = \\text{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 \\mathbf{I}_n$，上式可简化为：\n$$ \\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} $$\n单个系数估计量 $\\hat{\\beta}_j$ 的方差是该矩阵的第 $j$ 个对角元素：\n$$ \\text{Var}(\\hat{\\beta}_j) = \\sigma^2 [(\\mathbf{X}^T\\mathbf{X})^{-1}]_{jj} $$\n问题陈述中提到，预测变量已被标准化，均值为零，方差为一。设这些标准化预测变量的 $n \\times p$ 矩阵为 $\\mathbf{Z} = [Z_1, Z_2, \\dots, Z_p]$。由于预测变量是中心化的，截距项与它们正交。因此，我们可以只关注预测变量的系数，相关的矩阵是 $\\mathbf{Z}^T\\mathbf{Z}$。\n\n$\\mathbf{Z}^T\\mathbf{Z}$ 矩阵的第 $(i,j)$ 个元素是 $\\sum_{k=1}^{n} Z_{ki}Z_{kj}$。根据中心化变量的样本相关系数定义，$Z_i$ 和 $Z_j$ 之间的相关系数是 $r_{ij} = \\frac{\\sum_k Z_{ki}Z_{kj}}{\\sqrt{\\sum_k Z_{ki}^2 \\sum_k Z_{kj}^2}}$。由于变量是标准化的，它们的方差为 1，且 $\\sum_k Z_{ki}^2 = \\sum_k Z_{kj}^2 = n-1$。因此，$r_{ij} = \\frac{\\sum_k Z_{ki}Z_{kj}}{n-1}$。\n这意味着预测变量相关系数矩阵 $\\mathbf{R}$ 与 $\\mathbf{Z}^T\\mathbf{Z}$ 的关系如下：\n$$ \\mathbf{Z}^T\\mathbf{Z} = (n-1)\\mathbf{R} $$\n将此代入标准化模型的系数 $\\hat{\\boldsymbol{\\beta}}_\\text{std}$ 的方差公式中：\n$$ \\text{Var}(\\hat{\\boldsymbol{\\beta}}_\\text{std}) = \\sigma^2 (\\mathbf{Z}^T\\mathbf{Z})^{-1} = \\sigma^2 ((n-1)\\mathbf{R})^{-1} = \\frac{\\sigma^2}{n-1}\\mathbf{R}^{-1} $$\n第 $j$ 个系数估计量的方差是第 $j$ 个对角元素：\n$$ \\text{Var}(\\hat{\\beta}_j) = \\frac{\\sigma^2}{n-1} [\\mathbf{R}^{-1}]_{jj} $$\n方差膨胀因子 ($\\text{VIF}_j$) 定义为，与预测变量 $Z_j$ 和所有其他预测变量均正交的基准情景相比，由于共线性导致 $\\text{Var}(\\hat{\\beta}_j)$ 增加的倍数。在这种正交情况下，相关系数矩阵 $\\mathbf{R}$ 将是单位矩阵 $\\mathbf{I}$，其逆矩阵 $\\mathbf{R}^{-1}$ 也将是 $\\mathbf{I}$。第 $j$ 个对角元素将是 $[\\mathbf{I}]_{jj}=1$。在这种理想情况下的方差为：\n$$ \\text{Var}(\\hat{\\beta}_j)_{\\text{ortho}} = \\frac{\\sigma^2}{n-1}(1) $$\nVIF 是实际方差与这种正交情况下的方差之比：\n$$ \\operatorname{VIF}_j = \\frac{\\text{Var}(\\hat{\\beta}_j)}{\\text{Var}(\\hat{\\beta}_j)_{\\text{ortho}}} = \\frac{\\frac{\\sigma^2}{n-1} [\\mathbf{R}^{-1}]_{jj}}{\\frac{\\sigma^2}{n-1}} = [\\mathbf{R}^{-1}]_{jj} $$\n此推导表明，第 $j$ 个预测变量的 VIF 正是预测变量相关系数矩阵 $\\mathbf{R}$ 的逆矩阵的第 $j$ 个对角元素。\n\n此外，相关系数矩阵的逆矩阵有一个已知的性质，即其对角元素与将第 $j$ 个预测变量 $Z_j$ 对其余 $p-1$ 个预测变量进行回归所得到的决定系数 $R^2_j$ 相关。该关系是：\n$$ \\operatorname{VIF}_j = [\\mathbf{R}^{-1}]_{jj} = \\frac{1}{1 - R^2_j} $$\n\n### 第2部分：计算VIF\n\n给定的相关系数矩阵是：\n$$ \\mathbf{R} = \\begin{pmatrix} 1  -0.82  0.45 \\\\ -0.82  1  -0.60 \\\\ 0.45  -0.60  1 \\end{pmatrix} $$\n为了找到 VIF，我们必须计算 $\\mathbf{R}^{-1}$。我们首先计算 $\\mathbf{R}$ 的行列式：\n$$ \\det(\\mathbf{R}) = 1(1 \\cdot 1 - (-0.60)^2) - (-0.82)((-0.82) \\cdot 1 - (-0.60) \\cdot 0.45) + 0.45((-0.82)(-0.60) - 1 \\cdot 0.45) $$\n$$ \\det(\\mathbf{R}) = 1(1 - 0.36) + 0.82(-0.82 + 0.27) + 0.45(0.492 - 0.45) $$\n$$ \\det(\\mathbf{R}) = 0.64 + 0.82(-0.55) + 0.45(0.042) $$\n$$ \\det(\\mathbf{R}) = 0.64 - 0.451 + 0.0189 = 0.2079 $$\n接下来，我们求 $\\mathbf{R}$ 的伴随矩阵，也就是其代数余子式矩阵 $\\mathbf{C}$ 的转置。\n$$ C_{11} = 1 - (-0.60)^2 = 1 - 0.36 = 0.64 $$\n$$ C_{22} = 1 - (0.45)^2 = 1 - 0.2025 = 0.7975 $$\n$$ C_{33} = 1 - (-0.82)^2 = 1 - 0.6724 = 0.3276 $$\n由于 $\\mathbf{R}$ 是对称的，其代数余子式矩阵也是对称的，我们只需要对角元素来计算 VIF。\n逆矩阵为 $\\mathbf{R}^{-1} = \\frac{1}{\\det(\\mathbf{R})} \\text{adj}(\\mathbf{R})$。对角元素是：\n$$ [\\mathbf{R}^{-1}]_{11} = \\frac{C_{11}}{\\det(\\mathbf{R})} = \\frac{0.64}{0.2079} $$\n$$ [\\mathbf{R}^{-1}]_{22} = \\frac{C_{22}}{\\det(\\mathbf{R})} = \\frac{0.7975}{0.2079} $$\n$$ [\\mathbf{R}^{-1}]_{33} = \\frac{C_{33}}{\\det(\\mathbf{R})} = \\frac{0.3276}{0.2079} $$\n现在我们计算 VIF：\n$$ \\operatorname{VIF}_1 = [\\mathbf{R}^{-1}]_{11} \\approx 3.07840307... $$\n$$ \\operatorname{VIF}_2 = [\\mathbf{R}^{-1}]_{22} \\approx 3.83645983... $$\n$$ \\operatorname{VIF}_3 = [\\mathbf{R}^{-1}]_{33} \\approx 1.57575757... $$\n四舍五入到四位有效数字，我们得到：\n$$ \\operatorname{VIF}_1 \\approx 3.078 $$\n$$ \\operatorname{VIF}_2 \\approx 3.836 $$\n$$ \\operatorname{VIF}_3 \\approx 1.576 $$\n\n### 第3部分：解释\n\n我们使用关系式 $\\operatorname{VIF}_j = 1/(1 - R^2_j)$ 来解释这些值，这意味着 $R^2_j = 1 - 1/\\operatorname{VIF}_j$。\n\n对于预测变量 $Z_1$ (血清肌酐)：\n- $\\operatorname{VIF}_1 = 3.078$。这意味着血清肌酐系数的方差是其与 eGFR 和年龄不相关情况下的 3.078 倍。\n- $R^2_1 = 1 - 1/3.0784... = 1 - 0.3248... = 0.6751...$。这表明标准化血清肌酐 ($Z_1$) 中约 $67.5\\%$ 的方差可由标准化的 eGFR ($Z_2$) 和年龄 ($Z_3$) 的线性组合来解释。这种共线性程度为中等。\n\n对于预测变量 $Z_2$ (eGFR)：\n- $\\operatorname{VIF}_2 = 3.836$。eGFR 系数的方差是在没有共线性情况下的 3.836 倍。\n- $R^2_2 = 1 - 1/3.8364... = 1 - 0.2606... = 0.7393...$。这表明标准化 eGFR ($Z_2$) 中约 $73.9\\%$ 的方差可由标准化的血清肌酐 ($Z_1$) 和年龄 ($Z_3$) 来解释。这表示中到高水平的冗余，这在临床上是预期的，因为 eGFR 通常是根据血清肌酐计算出来的。\n\n对于预测变量 $Z_3$ (年龄)：\n- $\\operatorname{VIF}_3 = 1.576$。年龄系数的方差是在没有共线性情况下的 1.576 倍。\n- $R^2_3 = 1 - 1/1.5757... = 1 - 0.6346... = 0.3653...$。这表明标准化年龄 ($Z_3$) 中约 $36.5\\%$ 的方差可由标准化的血清肌酐 ($Z_1$) 和 eGFR ($Z_2$) 来解释。这是一个低水平的共线性，表明与另外两个预测变量相比，年龄提供了相对独特的信息。\n\n总之，VIF 值量化了多重共线性的程度。$Z_1$ 以及特别是 $Z_2$ 显示出中等共线性，其中 $Z_2$ (eGFR) 是最冗余的预测变量，这主要是因为它与 $Z_1$ (血清肌酐) 存在函数关系。$Z_3$ (年龄) 是最不冗余的。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3.078  3.836  1.576\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "VIF告诉我们多重共线性的*程度*，但更深入的理解需要探究其在线性代数中的数学根源。预测变量协方差矩阵的微小特征值是近似线性相关的信号，也是估计不稳定的根本原因。本练习通过构建一个简单的模型来奠定基础，你可以在该模型中分析性地追踪从近乎完美的线性相关到近乎为零的特征值的演变路径，从而揭示从稳定估计问题到不稳定估计问题的转变过程。",
            "id": "4952375",
            "problem": "收集了一个临床数据集，用于使用两个连续预测变量对一个连续结果变量 $y$ 进行建模。这两个预测变量代表了两种实验室检测方法，它们测量同一潜在的生物标志物，但具有不同的噪声特性。对每个受试者 $i \\in \\{1,\\dots,n\\}$，假设预测变量按如下方式生成：$x_{1i} \\sim \\mathcal{N}(0,\\sigma_{x}^{2})$，$\\epsilon_{i} \\sim \\mathcal{N}(0,\\tau^{2})$，其中 $x_{1i}$ 和 $\\epsilon_{i}$ 相互独立，并定义 $x_{2i} = x_{1i} + \\epsilon_{i}$。假设 $\\{(x_{1i},x_{2i})\\}_{i=1}^{n}$ 在 $i$ 上是独立同分布的，并且预测变量与线性模型 $y_{i} = \\beta_{0} + \\beta_{1} x_{1i} + \\beta_{2} x_{2i} + u_{i}$ 中的回归误差 $u_i$ 相互独立。考虑预测变量的 $2 \\times 2$ 总体协方差矩阵，\n$$\n\\Sigma = \\mathbb{E}\\left[\\begin{pmatrix} x_{1i} \\\\ x_{2i} \\end{pmatrix} \\begin{pmatrix} x_{1i}  x_{2i} \\end{pmatrix}\\right].\n$$\n从方差、协方差和特征值的定义出发，推导 $\\Sigma$ 的最小特征值的精确闭式表达式，将其表示为 $\\sigma_{x}^{2}$ 和 $\\tau^{2}$ 的函数，并解释极限 $\\tau \\to 0$ 如何通过预测变量矩阵 $X$ 的秩将近似多重共线性与精确多重共线性联系起来。你最终报告的答案必须是 $\\Sigma$ 的最小特征值的精确解析表达式。不需要进行数值近似。",
            "solution": "问题陈述经评估是有效的。它在科学上基于统计学和线性代数的原理，问题提法得当，为获得唯一解提供了所有必要信息，并且其表述是客观的。该问题为研究多重共线性提供了一个标准的、可形式化的场景。\n\n主要目标是推导预测变量 $x_{1i}$ 和 $x_{2i}$ 的总体协方差矩阵 $\\Sigma$ 的最小特征值。首先，我们必须构建矩阵 $\\Sigma$。预测变量定义为 $x_{1i} \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$，$\\epsilon_{i} \\sim \\mathcal{N}(0, \\tau^{2})$，其中 $x_{1i}$ 和 $\\epsilon_{i}$ 相互独立，且 $x_{2i} = x_{1i} + \\epsilon_{i}$。\n\n总体协方差矩阵由下式给出：\n$$\n\\Sigma = \\begin{pmatrix} \\text{Var}(x_{1i})  \\text{Cov}(x_{1i}, x_{2i}) \\\\ \\text{Cov}(x_{2i}, x_{1i})  \\text{Var}(x_{2i}) \\end{pmatrix}\n$$\n由于问题陈述生成正态分布的均值为 $0$，我们有 $\\mathbb{E}[x_{1i}] = 0$ 以及 $\\mathbb{E}[x_{2i}] = \\mathbb{E}[x_{1i} + \\epsilon_{i}] = \\mathbb{E}[x_{1i}] + \\mathbb{E}[\\epsilon_{i}] = 0 + 0 = 0$。\n\n让我们计算 $\\Sigma$ 的各个分量：\n1.  $x_{1i}$ 的方差由其定义直接给出：\n    $$\n    \\text{Var}(x_{1i}) = \\sigma_{x}^{2}\n    $$\n2.  $x_{2i}$ 的方差使用独立随机变量和的方差性质计算。由于 $x_{1i}$ 和 $\\epsilon_i$ 相互独立：\n    $$\n    \\text{Var}(x_{2i}) = \\text{Var}(x_{1i} + \\epsilon_{i}) = \\text{Var}(x_{1i}) + \\text{Var}(\\epsilon_{i}) = \\sigma_{x}^{2} + \\tau^{2}\n    $$\n3.  $x_{1i}$ 和 $x_{2i}$ 之间的协方差计算如下：\n    $$\n    \\text{Cov}(x_{1i}, x_{2i}) = \\text{Cov}(x_{1i}, x_{1i} + \\epsilon_{i})\n    $$\n    使用协方差的双线性性质：\n    $$\n    \\text{Cov}(x_{1i}, x_{1i} + \\epsilon_{i}) = \\text{Cov}(x_{1i}, x_{1i}) + \\text{Cov}(x_{1i}, \\epsilon_{i})\n    $$\n    一个变量与自身的协方差是其方差，所以 $\\text{Cov}(x_{1i}, x_{1i}) = \\text{Var}(x_{1i}) = \\sigma_{x}^{2}$。由于 $x_{1i}$ 和 $\\epsilon_{i}$ 相互独立且均值为零，它们的协方差为零：$\\text{Cov}(x_{1i}, \\epsilon_{i}) = \\mathbb{E}[x_{1i} \\epsilon_{i}] - \\mathbb{E}[x_{1i}]\\mathbb{E}[\\epsilon_{i}] = 0 - 0 \\cdot 0 = 0$。\n    因此：\n    $$\n    \\text{Cov}(x_{1i}, x_{2i}) = \\sigma_{x}^{2}\n    $$\n\n将这些分量组合起来，总体协方差矩阵为：\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_{x}^{2}  \\sigma_{x}^{2} \\\\ \\sigma_{x}^{2}  \\sigma_{x}^{2} + \\tau^{2} \\end{pmatrix}\n$$\n\n接下来，我们通过求解特征方程 $\\det(\\Sigma - \\lambda I) = 0$ 来求 $\\Sigma$ 的特征值 $\\lambda$，其中 $I$ 是 $2 \\times 2$ 单位矩阵。\n$$\n\\det\\left( \\begin{pmatrix} \\sigma_{x}^{2} - \\lambda  \\sigma_{x}^{2} \\\\ \\sigma_{x}^{2}  \\sigma_{x}^{2} + \\tau^{2} - \\lambda \\end{pmatrix} \\right) = 0\n$$\n$$\n(\\sigma_{x}^{2} - \\lambda)(\\sigma_{x}^{2} + \\tau^{2} - \\lambda) - (\\sigma_{x}^{2})^{2} = 0\n$$\n这导出了关于 $\\lambda$ 的二次方程：\n$$\n\\lambda^{2} - (\\text{tr}(\\Sigma))\\lambda + \\det(\\Sigma) = 0\n$$\n$\\Sigma$ 的迹为 $\\text{tr}(\\Sigma) = \\sigma_{x}^{2} + (\\sigma_{x}^{2} + \\tau^{2}) = 2\\sigma_{x}^{2} + \\tau^{2}$。\n$\\Sigma$ 的行列式为 $\\det(\\Sigma) = \\sigma_{x}^{2}(\\sigma_{x}^{2} + \\tau^{2}) - (\\sigma_{x}^{2})^{2} = \\sigma_{x}^{4} + \\sigma_{x}^{2}\\tau^{2} - \\sigma_{x}^{4} = \\sigma_{x}^{2}\\tau^{2}$。\n特征方程为：\n$$\n\\lambda^{2} - (2\\sigma_{x}^{2} + \\tau^{2})\\lambda + \\sigma_{x}^{2}\\tau^{2} = 0\n$$\n我们使用二次公式 $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ 求解 $\\lambda$：\n$$\n\\lambda = \\frac{(2\\sigma_{x}^{2} + \\tau^{2}) \\pm \\sqrt{(-(2\\sigma_{x}^{2} + \\tau^{2}))^{2} - 4(1)(\\sigma_{x}^{2}\\tau^{2})}}{2}\n$$\n$$\n\\lambda = \\frac{1}{2}\\left( 2\\sigma_{x}^{2} + \\tau^{2} \\pm \\sqrt{(2\\sigma_{x}^{2} + \\tau^{2})^{2} - 4\\sigma_{x}^{2}\\tau^{2}} \\right)\n$$\n让我们简化判别式：\n$$\n(2\\sigma_{x}^{2} + \\tau^{2})^{2} - 4\\sigma_{x}^{2}\\tau^{2} = (4\\sigma_{x}^{4} + 4\\sigma_{x}^{2}\\tau^{2} + \\tau^{4}) - 4\\sigma_{x}^{2}\\tau^{2} = 4\\sigma_{x}^{4} + \\tau^{4}\n$$\n将此代回 $\\lambda$ 的解中：\n$$\n\\lambda = \\frac{1}{2}\\left( 2\\sigma_{x}^{2} + \\tau^{2} \\pm \\sqrt{4\\sigma_{x}^{4} + \\tau^{4}} \\right)\n$$\n两个特征值为：\n$$\n\\lambda_{1} = \\frac{1}{2}\\left( 2\\sigma_{x}^{2} + \\tau^{2} + \\sqrt{4\\sigma_{x}^{4} + \\tau^{4}} \\right) \\quad \\text{和} \\quad \\lambda_{2} = \\frac{1}{2}\\left( 2\\sigma_{x}^{2} + \\tau^{2} - \\sqrt{4\\sigma_{x}^{4} + \\tau^{4}} \\right)\n$$\n由于 $\\sigma_{x}^{2}$ 和 $\\tau^{2}$ 是方差，它们是非负的。对于任何预测有意义的非平凡情况，$\\sigma_{x}^{2} > 0$。因此，项 $\\sqrt{4\\sigma_{x}^{4} + \\tau^{4}}$ 是实数且为正。最小的特征值是 $\\lambda_{2}$，我们将其记为 $\\lambda_{\\text{min}}$。\n\n最小特征值的精确闭式表达式为：\n$$\n\\lambda_{\\text{min}} = \\frac{1}{2}\\left( 2\\sigma_{x}^{2} + \\tau^{2} - \\sqrt{4\\sigma_{x}^{4} + \\tau^{4}} \\right)\n$$\n\n问题的第二部分要求使用极限 $\\tau \\to 0$ 解释近似多重共线性与精确多重共线性之间的联系。多重共线性是通过预测变量协方差矩阵的较小特征值来诊断的。\n当 $\\tau^{2} > 0$ 时，预测变量 $x_{1i}$ 和 $x_{2i}$ 不是完全线性相关的。相关系数为 $\\rho = \\frac{\\sigma_{x}^{2}}{\\sqrt{\\sigma_{x}^{2}(\\sigma_{x}^{2}+\\tau^{2})}} = \\frac{\\sigma_{x}}{\\sqrt{\\sigma_{x}^{2}+\\tau^{2}}}  1$。因为对于 $\\sigma_x^20, \\tau^20$，$\\sqrt{4\\sigma_{x}^{4} + \\tau^{4}}  \\sqrt{(2\\sigma_{x}^{2} + \\tau^{2})^{2}} = 2\\sigma_{x}^{2} + \\tau^{2}$，所以最小特征值 $\\lambda_{\\text{min}}$ 是严格为正的。$\\lambda_{\\text{min}}$ 的一个小的正值（当 $\\tau^{2}$ 相对于 $\\sigma_{x}^{2}$ 很小时发生）表明存在近似多重共线性。在这种状态下，$n \\times 2$ 的预测变量矩阵 $X$ 具有线性无关的列，并且是满秩的，即 $\\text{rank}(X)=2$。\n\n现在，考虑噪声方差 $\\tau^{2}$ 趋近于零时的极限。\n$$\n\\lim_{\\tau \\to 0} \\lambda_{\\text{min}} = \\lim_{\\tau \\to 0} \\frac{1}{2}\\left( 2\\sigma_{x}^{2} + \\tau^{2} - \\sqrt{4\\sigma_{x}^{4} + \\tau^{4}} \\right)\n$$\n$$\n= \\frac{1}{2}\\left( 2\\sigma_{x}^{2} + 0 - \\sqrt{4\\sigma_{x}^{4} + 0} \\right) = \\frac{1}{2}\\left( 2\\sigma_{x}^{2} - \\sqrt{4\\sigma_{x}^{4}} \\right) = \\frac{1}{2}\\left( 2\\sigma_{x}^{2} - 2\\sigma_{x}^{2} \\right) = 0\n$$\n特征值为零表示协方差矩阵 $\\Sigma$ 是奇异的。在这个极限下，$\\Sigma$ 变为：\n$$\n\\lim_{\\tau \\to 0} \\Sigma = \\begin{pmatrix} \\sigma_{x}^{2}  \\sigma_{x}^{2} \\\\ \\sigma_{x}^{2}  \\sigma_{x}^{2} \\end{pmatrix}\n$$\n该矩阵的列是相同的，这是线性相关的明确标志。这对应于总体中的精确多重共线性，因为 $x_{2i} \\to x_{1i}$。\n这种总体水平的奇异性反映在样本数据矩阵 $X$ 中。当 $\\tau \\to 0$ 时，$X$ 的第二列向量 $(x_{11}+\\epsilon_{1}, \\dots, x_{1n}+\\epsilon_{n})^T$ 收敛于第一列向量 $(x_{11}, \\dots, x_{1n})^T$。这两列变得相同，意味着它们是完全线性相关的。因此，矩阵 $X$ 的秩从 $2$ 降到 $1$。秩亏的预测变量矩阵是精确多重共线性的定义。因此，极限 $\\tau \\to 0$ 展示了从近似多重共线性（$\\lambda_{\\text{min}}  0$，$\\text{rank}(X)=2$）到精确多重共线性（$\\lambda_{\\text{min}} = 0$，$\\text{rank}(X)=1$）的连续过渡。",
            "answer": "$$\n\\boxed{\\frac{1}{2}\\left(2\\sigma_{x}^{2} + \\tau^{2} - \\sqrt{4\\sigma_{x}^{4} + \\tau^{4}}\\right)}\n$$"
        },
        {
            "introduction": "在诊断和理解了多重共线性之后，下一个自然的问题是如何以稳健且可复现的方式处理它。这项高级实践将理论付诸实践，要求你基于奇异值分解（SVD）构建一个算法，以系统地检测和解决共线性问题。你将学习如何定义数值秩，并实现两种强大的策略——基于原则的预测变量移除和主成分投影，从而为构建稳定的医学预测模型提供一个实用的工具集。",
            "id": "4952418",
            "problem": "您的任务是构建一个可复现的算法，使用奇异值分解（SVD）的容差阈值来检测和处理医学设计矩阵中的多重共线性。考虑一个线性建模情景，其中设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，$n$ 是观测数量，$p$ 是预测变量的数量。当 $X$ 的列线性相关或近似线性相关时，就会出现多重共线性，导致参数估计不稳定。\n\n从基本原理出发：\n- 矩阵 $X$ 的数值秩是 $X$ 的奇异值中大于某个容差的数量。\n- 奇异值分解（SVD）指出，任何实数矩阵 $X$ 都允许分解为 $X = U \\Sigma V^\\top$，其中 $U$ 和 $V$ 是正交矩阵，$\\Sigma$ 是对角矩阵，其非负对角元素 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{\\min(n,p)} \\ge 0$ 为奇异值。\n- 浮点运算具有机器精度 $\\epsilon$（机器 epsilon），它为数值上可忽略的量设定了一个尺度。\n\n您的程序必须从第一性原理出发，实现以下功能：\n1. 为了在不同单位度量的医学预测变量之间获得可复现性，进行列中心化：将每列 $x_j$ 替换为 $x_j - \\bar{x}_j$，其中 $\\bar{x}_j$ 是第 $j$ 列的均值。中心化后完全变为零的列是常数列，必须被确定性地移除。\n2. 计算中心化矩阵的 SVD，并通过计算大于容差的奇异值数量来确定其数值秩 $r$。容差必须计算为机器精度、问题规模和最大奇异值的函数。设 $\\tau$ 为用户指定的容差乘数；您必须将阈值设置为 $\\tau$、一个与维度相关的因子、机器 epsilon 和最大奇异值的乘积。\n3. 如果 $r  p$，则通过以下两种可复现策略之一来处理多重共线性：\n   - “移除”策略（drop strategy）：从 SVD 中迭代识别一个零空间方向（对应于低于容差的奇异值的右奇异向量），并移除在该向量中绝对载荷最大的预测变量。通过在并列的预测变量中选择索引最大的那个来进行确定性的平局决胜。重新计算，直到剩余的列达到满秩。\n   - “组合”策略（combine strategy）：通过投影到中心化矩阵的前 $r$ 个右奇异向量（主成分）上，构建正交的代理预测变量。报告前 $r$ 个分量所保留的总奇异值平方和的分数，以小数形式表示。\n\n贯穿始终应用的定义和约束：\n- 数值秩 $r$ 定义为严格大于容差的奇异值的数量。\n- 零空间基向量是与小于或等于容差的奇异值相关联的右奇异向量。\n- 除指定的平局决胜规则外，所有决策都必须是确定性的，并且对列的排列保持不变。\n\n测试套件：\n对于每个测试用例，您将获得 $(X, \\tau, \\text{strategy})$，并且必须返回一个结构化的结果。使用以下参数值构建 $X$；您的程序必须精确地复现这些值。\n\n- 测试用例 A（理想情况，在“移除”策略下为满秩）：\n  - $n = 12$\n  - $t_i = \\frac{i}{11}$，对于 $i = 0, 1, \\dots, 11$\n  - 列：$x_1 = t$, $x_2 = t^2$, $x_3 = \\sin(2\\pi t)$\n  - $\\tau = 1.0$\n  - 策略：移除（drop）\n\n- 测试用例 B（三个预测变量间存在精确相关性，使用“移除”策略）：\n  - $n = 50$\n  - $t_i = \\frac{i}{49}$，对于 $i = 0, 1, \\dots, 49$\n  - 列：$x_1 = t$, $x_2 = \\sin(3\\pi t)$, $x_3 = x_1 + 2 x_2$\n  - $\\tau = 1.0$\n  - 策略：移除（drop）\n\n- 测试用例 C（由于容差较大，近似共线性被视为秩亏，使用“移除”策略）：\n  - $n = 50$\n  - $t_i = \\frac{i}{49}$，对于 $i = 0, 1, \\dots, 49$\n  - 列：$x_1 = t$, $x_2 = x_1 + 10^{-8}\\cos(5\\pi t)$, $x_3 = \\sqrt{t + 10^{-6}}$\n  - $\\tau = 10.0$\n  - 策略：移除（drop）\n\n- 测试用例 D（与测试用例 C 相同，但容差较小，因此在“移除”策略下为满秩）：\n  - $n = 50$\n  - $t_i = \\frac{i}{49}$，对于 $i = 0, 1, \\dots, 49$\n  - 列：$x_1 = t$, $x_2 = x_1 + 10^{-8}\\cos(5\\pi t)$, $x_3 = \\sqrt{t + 10^{-6}}$\n  - $\\tau = 0.01$\n  - 策略：移除（drop）\n\n- 测试用例 E（存在多重精确相关性，使用“组合”策略；报告保留的方差分数）：\n  - $n = 100$\n  - $t_i = \\frac{i}{99}$，对于 $i = 0, 1, \\dots, 99$\n  - 列：$x_1 = t$, $x_2 = \\sin(2\\pi t)$, $x_3 = \\log(1 + 2t)$, $x_4 = x_1 + x_2$, $x_5 = 3 x_2 - x_3$\n  - $\\tau = 1.0$\n  - 策略：组合（combine）\n\n最终输出规范：\n- 对于“移除”策略的测试用例，输出一个双元素列表 $[r, [k_1, k_2, \\dots]]$，其中 $r$ 是中心化后的数值秩，$[k_1, k_2, \\dots]$ 是从原始 $X$ 中保留下来的列索引的升序列表（使用基于 0 的索引）。\n- 对于“组合”策略的测试用例，输出一个双元素列表 $[r, f]$，其中 $r$ 是中心化后的数值秩，$f$ 是前 $r$ 个奇异值所保留的总奇异值平方和的分数，以小数形式表示（非百分比），并四舍五入到六位小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表（例如，$[result1,result2,result3,result4,result5]$）。每个 $resulti$ 必须遵循其对应策略的上述格式。\n\n不允许外部输入；程序必须在内部构建测试矩阵，并且必须是可复现和确定性的。",
            "solution": "该问题陈述是数值线性代数及其在统计建模中应用的一个明确定义的任务，专门解决设计矩阵中的多重共线性问题。\n\n### 步骤 1：问题陈述验证 ###\n\n**1.1. 提取给定条件：**\n- **背景**：使用设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$（$n$ 个观测值，$p$ 个预测变量）的线性建模。\n- **问题**：检测和处理多重共线性。\n- **方法**：奇异值分解（SVD），$X = U \\Sigma V^\\top$，其中 $\\Sigma$ 包含奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$。\n- **预处理**：$X$ 的所有列必须通过减去各自的均值进行中心化。中心化后变为全零的常数列必须被移除。\n- **数值秩（$r$）**：中心化矩阵的奇异值中严格大于容差 $\\text{tol}$ 的数量。\n- **容差计算**：容差定义为 $\\text{tol} = \\tau \\cdot (\\text{与维度相关的因子}) \\cdot \\epsilon \\cdot \\sigma_1$，其中 $\\tau$ 是用户指定的乘数，$\\epsilon$ 是机器精度，$\\sigma_1$ 是最大奇异值。与维度相关的因子将使用 $\\max(n, p)$ 的标准选择，因为它符合数值分析中的常见做法。\n- **处理策略（如果 $r  p_{initial}$）**：\n    - **`移除`策略**：迭代移除预测变量。在每一步中，对当前矩阵执行 SVD。识别出与最小奇异值对应的右奇异向量。移除在该向量中具有最大绝对载荷的预测变量。通过移除具有最大原始列索引的预测变量来打破平局。重复此过程，直到剩余矩阵达到满数值秩。\n    - **`组合`策略**：将数据投影到 $r$ 个主成分上（即与 $r$ 个最大奇异值对应的右奇异向量）。输出是前 $r$ 个值所占的总奇异值平方和的分数：$f = (\\sum_{i=1}^r \\sigma_i^2) / (\\sum_{j=1}^{\\min(n,p)} \\sigma_j^2)$。\n- **测试用例**：提供了五个具体的测试用例（A, B, C, D, E），定义了矩阵 $X$ 的构造、参数 $\\tau$ 以及每个用例要使用的策略。\n- **输出格式**：对于 `移除` 策略，结果为 $[r, [k_1, \\dots]]$，其中 $r$ 是初始数值秩，$[k_1, \\dots]$ 是保留列的排序后的 0-based 索引。对于 `组合` 策略，结果为 $[r, f]$，其中 $r$ 是数值秩，$f$ 是保留的方差分数，四舍五入到六位小数。最终输出必须是表示这些结果列表的单行字符串。\n\n**1.2. 验证结论：**\n- **科学依据充分**：该问题牢固地植根于数值线性代数（SVD、数值秩）和统计学（多重共线性、主成分分析）的既定原则。所描述的方法是标准且可靠的。\n- **定义明确**：问题被精确地指定。所有步骤都是确定性的，包括平局决胜规则，确保每个测试用例都有唯一的解决方案。输入定义清晰，输出格式明确。\n- **客观性**：问题以客观的数学语言陈述，没有任何主观性或意见。\n- **完整性**：问题是自洽的。容差计算中“与维度相关的因子”的轻微模糊性通过采用涉及 $\\max(n, p)$ 的标准且在上下文中适当的公式得以解决，这不构成缺陷，而是该领域的标准解释。\n\n**结论**：该问题是**有效的**。它是一个基于可靠科学原理的、严谨且定义明确的计算任务。\n\n### 步骤 2：解决方案推导 ###\n\n解决方案涉及实现一个函数，根据指定的策略（`移除`或`组合`）处理每个测试用例。这个过程可以分解为模块化的步骤。\n\n**2.1. 预处理：矩阵中心化和常数列移除**\n给定一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$，第一步是将其中心化。对于每列 $x_j$（$j = 1, \\dots, p$），我们计算其均值 $\\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij}$。中心化矩阵 $X_c$ 的条目为 $(X_c)_{ij} = X_{ij} - \\bar{x}_j$。此操作可表示为 $X_c = X - \\mathbf{1}\\bar{x}^\\top$，其中 $\\mathbf{1}$ 是一个 $n \\times 1$ 的全一向量，$\\bar{x}$ 是一个 $1 \\times p$ 的列均值向量。\n\n中心化之后，我们必须识别并移除任何完全变为零的列。当且仅当一列 $x_j$ 的所有元素原本都相同时（即它是一个常数预测变量），该列才会变为零。这类预测变量的方差为零，在中心化的其他预测变量的上下文中不提供任何信息。必须将它们移除，以避免人为地引入秩亏。我们必须跟踪剩余列的原始索引。\n\n**2.2. 数值秩计算**\n对于预处理后的矩阵 $X_p$（移除了常数列的中心化矩阵），我们计算其 SVD：$X_p = U \\Sigma V^\\top$。设 $X_p$ 的维度为 $n \\times p'$。奇异值为 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{p'} \\ge 0$。\n\n用于确定数值秩的容差计算如下：\n$$ \\text{tol} = \\tau \\cdot \\max(n, p') \\cdot \\epsilon \\cdot \\sigma_1 $$\n其中 $\\epsilon$ 是所用浮点运算的机器 epsilon，$\\tau$ 是给定的乘数，$\\sigma_1$ 是最大奇异值。数值秩 $r$ 则是严格大于此容差的奇异值的数量：\n$$ r = |\\{ i \\mid \\sigma_i  \\text{tol} \\}| $$\n这个初始秩 $r$ 是两种策略输出的必需部分。\n\n**2.3. `组合`策略**\n这个策略是两者中较为直接的一个。在计算出 $n \\times p'$ 矩阵 $X_p$ 的初始数值秩 $r$ 后，我们计算前 $r$ 个分量保留的方差分数。总方差与所有奇异值的平方和 $\\sum_{j=1}^{p'} \\sigma_j^2$ 成正比。前 $r$ 个分量捕获的方差是 $\\sum_{j=1}^{r} \\sigma_j^2$。所需的分数 $f$ 为：\n$$ f = \\frac{\\sum_{j=1}^{r} \\sigma_j^2}{\\sum_{j=1}^{p'} \\sigma_j^2} $$\n此策略的最终结果是列表 $[r, f]$，$f$ 四舍五入到六位小数。\n\n**2.4. `移除`策略**\n这是一个迭代策略。我们从所有非常数预测变量的集合开始。\n设 $C$ 为当前考虑的矩阵中列的原始索引集合。最初，这是 $X$ 中所有非常数列的索引集合。\n\n迭代过程如下：\n1.  使用与 $C$ 中索引对应的原始中心化矩阵 $X_c$ 的列，构建当前数据矩阵 $X_{current}$。设其维度为 $n \\times p_{curr}$。\n2.  计算 $X_{current}$ 的 SVD：$X_{current} = U_{curr} \\Sigma_{curr} V_{curr}^\\top$。\n3.  计算当前容差 $\\text{tol}_{curr} = \\tau \\cdot \\max(n, p_{curr}) \\cdot \\epsilon \\cdot \\sigma_{1,curr}$ 和当前数值秩 $r_{curr}$。\n4.  如果 $r_{curr} = p_{curr}$，则矩阵具有满数值秩。迭代停止。最终保留的索引集为 $C$。\n5.  如果 $r_{curr}  p_{curr}$，则矩阵是秩亏的。我们必须移除一列。\n    a. 识别与小于或等于 $\\text{tol}_{curr}$ 的奇异值对应的右奇异向量。这些向量构成了数值零空间的一个基。问题暗示我们应关注“最严重”共线性的方向，这与最小的奇异值相关。因此，我们选择与最小奇异值 $\\sigma_{p_{curr}}$ 对应的右奇异向量 $v_{p_{curr}}$。\n    b. 在此向量中找到具有最大绝对值的索引 $j^*$：$j^* = \\arg\\max_j |(v_{p_{curr}})_j|$。这标识了对这个特定共线性关系贡献最大的预测变量。\n    c. 处理平局：如果多个索引共享相同的最大绝对值，找到它们在集合 $C$ 中对应的原始索引。根据问题规范，选择具有最大原始索引的那个进行移除。\n    d. 从集合 $C$ 中移除已识别的原始索引。\n    e. 返回步骤 1。\n\n当列集合表示一个满秩矩阵时，循环终止。输出是一个列表，包含*初始*数值秩 $r$（在完整的非常数矩阵上计算一次）和最终保留的原始索引的排序列表。\n\n这种详细的、分步的逻辑为解决所述问题提供了一个确定性和可复现的算法。",
            "answer": "```python\nimport numpy as np\n\ndef format_result(item):\n    \"\"\"\n    Custom formatter to produce string representation of lists without spaces.\n    This ensures the output strictly matches the required format.\n    \"\"\"\n    if isinstance(item, list):\n        return f\"[{','.join(format_result(x) for x in item)}]\"\n    elif isinstance(item, float):\n        # The problem specified rounding to 6 decimal places for the combine strategy.\n        # This formatting prints it correctly.\n        return f\"{item:.6f}\"\n    else:\n        return str(item)\n\ndef solve_case(X, tau, strategy, n, p_orig):\n    \"\"\"\n    Solves a single test case for multicollinearity analysis.\n    \"\"\"\n    # 1. Column centering and constant column identification\n    X_mean = X.mean(axis=0)\n    X_c = X - X_mean\n\n    # Identify non-constant columns\n    # A column is constant if all its centered values are close to zero.\n    # The standard deviation is a robust way to check this.\n    col_std = np.std(X_c, axis=0)\n    # Using a small threshold related to machine precision for robustness\n    non_constant_mask = col_std  1e-12 \n    \n    # Store original indices of non-constant columns\n    kept_indices = [i for i, is_kept in enumerate(non_constant_mask) if is_kept]\n    \n    if not kept_indices: # All columns were constant\n        if strategy == 'drop':\n            return [0, []]\n        else: # combine\n            return [0, 0.0]\n\n    X_p = X_c[:, non_constant_mask]\n    n_p, p_p = X_p.shape\n    \n    # 2. Compute SVD and initial numerical rank\n    try:\n        _, s, _ = np.linalg.svd(X_p, full_matrices=False)\n    except np.linalg.LinAlgError:\n         # Handle cases where SVD fails, though unlikely with our test data.\n        if strategy == 'drop':\n            return [0, []]\n        else: # combine\n            return [0, 0.0]\n            \n    eps = np.finfo(float).eps\n    \n    if s.size == 0: # If no columns were left\n        initial_rank = 0\n    else:\n        # Standard tolerance formula based on problem description\n        tol = tau * max(n_p, p_p) * eps * s[0]\n        initial_rank = np.sum(s  tol)\n\n    # 3. Apply specified strategy\n    if strategy == 'combine':\n        if s.size == 0:\n            return [0, 0.0]\n        sum_sq_s = np.sum(s**2)\n        if sum_sq_s == 0:\n             # This can happen if the matrix is all zeros\n             return [initial_rank, 0.0]\n        \n        preserved_variance = np.sum(s[:initial_rank]**2) / sum_sq_s\n        return [initial_rank, round(preserved_variance, 6)]\n\n    elif strategy == 'drop':\n        # Iteratively remove columns until full rank\n        current_indices_map = list(range(len(kept_indices))) # local indices 0, 1, ...\n        \n        while True:\n            X_current = X_p[:, current_indices_map]\n            n_curr, p_curr = X_current.shape\n\n            if p_curr == 0:\n                break\n            \n            try:\n                _, s_curr, vh_curr = np.linalg.svd(X_current, full_matrices=False)\n            except np.linalg.LinAlgError:\n                # If SVD fails on a submatrix, stop\n                kept_indices = []\n                break\n\n            tol_curr = tau * max(n_curr, p_curr) * eps * s_curr[0]\n            rank_curr = np.sum(s_curr  tol_curr)\n\n            if rank_curr == p_curr:\n                # Matrix is full rank, stop.\n                break\n\n            # Identify predictor to drop\n            # Right singular vector for smallest singular value\n            null_space_vector = vh_curr[-1, :]\n            \n            # Find local indices with max absolute loading\n            max_loading = np.max(np.abs(null_space_vector))\n            # Use a small tolerance for comparing floats\n            tie_indices_local = np.where(np.abs(null_space_vector) = max_loading - 1e-12)[0]\n            \n            # Map local indices to original indices to apply tie-breaking rule\n            original_indices_of_ties = [kept_indices[current_indices_map[i]] for i in tie_indices_local]\n\n            # Drop the one with the largest original index\n            original_index_to_drop = max(original_indices_of_ties)\n            \n            # Find the corresponding original index in the kept_indices list and its local map\n            idx_in_kept_to_remove = kept_indices.index(original_index_to_drop)\n            local_idx_to_remove = current_indices_map.index(idx_in_kept_to_remove)\n            \n            # Remove from local map\n            del current_indices_map[local_idx_to_remove]\n        \n        # Final set of kept original indices\n        final_kept_indices = sorted([kept_indices[i] for i in current_indices_map])\n        return [initial_rank, final_kept_indices]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the solver, and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 12, \"tau\": 1.0, \"strategy\": \"drop\",\n            \"cols_func\": lambda t: np.vstack([t, t**2, np.sin(2 * np.pi * t)]).T\n        },\n        {\n            \"n\": 50, \"tau\": 1.0, \"strategy\": \"drop\",\n            \"cols_func\": lambda t: np.vstack([t, np.sin(3 * np.pi * t), t + 2 * np.sin(3 * np.pi * t)]).T\n        },\n        {\n            \"n\": 50, \"tau\": 10.0, \"strategy\": \"drop\",\n            \"cols_func\": lambda t: np.vstack([t, t + 1e-8 * np.cos(5 * np.pi * t), np.sqrt(t + 1e-6)]).T\n        },\n        {\n            \"n\": 50, \"tau\": 0.01, \"strategy\": \"drop\",\n            \"cols_func\": lambda t: np.vstack([t, t + 1e-8 * np.cos(5 * np.pi * t), np.sqrt(t + 1e-6)]).T\n        },\n        {\n            \"n\": 100, \"tau\": 1.0, \"strategy\": \"combine\",\n            \"cols_func\": lambda t: (lambda x1, x2, x3: np.vstack([x1, x2, x3, x1 + x2, 3 * x2 - x3]).T)(\n                t, np.sin(2 * np.pi * t), np.log(1 + 2 * t)\n            )\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        t = np.linspace(0, 1, n)\n        X = case[\"cols_func\"](t)\n        p = X.shape[1]\n        \n        result = solve_case(X, case[\"tau\"], case[\"strategy\"], n, p)\n        results.append(result)\n\n    # Note: The problem asks for a single-line string representation of the list of results.\n    # The format_result function is designed to build this string correctly.\n    # The final output to stdout should be exactly `[result_A,result_B,...]` without newlines.\n    # The Python print function automatically adds a newline, which is standard. The content of the print is what matters.\n    # Here, we print the string representation as required.\n    print(f\"[{','.join(format_result(r) for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}