{
    "hands_on_practices": [
        {
            "introduction": "泊松模型是医学领域分析计数数据（如术后感染次数）的基石。从第一性原理推导对数似然函数、得分向量和费雪信息矩阵，是理解广义线性模型如何工作的基本功。这项练习  将指导你构建模型拟合与推断所需的核心数学“引擎”。",
            "id": "4988428",
            "problem": "一个医院系统正在对术后感染计数进行建模，患者索引为 $i=1,\\dots,n$。令 $y_i \\in \\{0,1,2,\\dots\\}$ 表示在固定监测窗口内患者 $i$ 的观测计数，令 $x_i \\in \\mathbb{R}^{p}$ 为患者级别的协变量向量（例如年龄、免疫抑制状态、手术时间和预防性抗生素使用指标）。假设一个广义线性模型（GLM; generalized linear model），其条件分布为泊松分布，并采用典则对数连接函数，即在给定 $x_i$ 和回归系数向量 $\\beta \\in \\mathbb{R}^{p}$ 的条件下，$y_i$ 相互独立，其均值 $\\mu_i$ 满足 $\\log(\\mu_i)=x_i^{\\top}\\beta$。\n\n从泊松概率质量函数的定义以及给定 $\\beta$ 时观测值独立的假设出发，推导 $\\beta$ 的对数似然函数。然后，从第一性原理出发，计算 $\\beta$ 的得分向量和期望费雪信息矩阵。\n\n您的最终答案必须是一个单一的闭式解析表达式，包含三个部分，按以下顺序排列成一个行矩阵：对数似然函数、得分向量和期望费雪信息矩阵。不需要进行数值近似。",
            "solution": "问题陈述需经过验证。\n\n### 步骤1：提取已知条件\n-   数据包含 $n$ 个观测值，索引为 $i=1, \\dots, n$。\n-   对于每个观测值 $i$，有一个观测计数 $y_i \\in \\{0,1,2,\\dots\\}$ 和一个协变量向量 $x_i \\in \\mathbb{R}^{p}$。\n-   模型是一个广义线性模型（GLM）。\n-   在给定 $x_i$ 和回归系数向量 $\\beta \\in \\mathbb{R}^{p}$ 的条件下，$y_i$ 的条件分布是均值为 $\\mu_i$ 的泊松分布。记为 $y_i | x_i, \\beta \\sim \\text{Poisson}(\\mu_i)$。\n-   观测值 $y_i$ 是条件独立的。\n-   连接函数是典则对数连接，由关系式 $\\log(\\mu_i) = x_i^{\\top}\\beta$ 定义。\n\n### 步骤2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n-   **科学依据**：该问题描述了一个泊松回归模型，这是广义线性模型框架的一个标准和基本组成部分。该模型广泛应用于许多科学领域，包括医学、流行病学和生态学，用于分析计数数据。其设置在理论上是合理的，并牢固地植根于已建立的统计学原理。\n-   **适定性**：该问题是适定的。它要求推导三个关键的统计量：对数似然函数、得分向量和期望费雪信息矩阵。给定模型设定（泊松分布、对数连接），这些量是唯一定义的，并且可以从第一性原理推导出来。\n-   **客观性**：语言精确、正式，没有任何主观或模棱两可的术语。\n-   **完整性和一致性**：该问题提供了执行所需推导的所有必要信息——概率分布、连接函数和线性预测器的结构。没有缺失或矛盾的元素。\n-   **现实性和可行性**：使用患者级别的协变量对术后感染计数进行建模的情景是泊松回归在医学统计学中的一个现实且常见的应用。\n\n### 步骤3：结论与行动\n该问题被判定为**有效**。这是统计理论中一个标准的、定义明确的问题。将按要求推导解答。\n\n### 推导过程\n\n解答需要推导对数似然函数 $l(\\beta)$、得分向量 $U(\\beta)$ 和期望费雪信息矩阵 $I(\\beta)$。\n\n**1. 对数似然函数 $l(\\beta)$**\n\n来自均值为 $\\mu_i$ 的泊松分布的单个观测值 $y_i$ 的概率质量函数（PMF）由下式给出：\n$$ P(Y_i = y_i | \\mu_i) = \\frac{\\exp(-\\mu_i)\\mu_i^{y_i}}{y_i!} $$\n由于观测值的条件独立性，整个数据集 $\\{y_1, \\dots, y_n\\}$ 的似然函数 $L(\\beta)$ 是各个概率的乘积：\n$$ L(\\beta) = \\prod_{i=1}^{n} P(Y_i = y_i | x_i, \\beta) = \\prod_{i=1}^{n} \\frac{\\exp(-\\mu_i)\\mu_i^{y_i}}{y_i!} $$\n对数似然函数 $l(\\beta)$ 是似然函数的自然对数：\n$$ l(\\beta) = \\ln(L(\\beta)) = \\ln\\left(\\prod_{i=1}^{n} \\frac{\\exp(-\\mu_i)\\mu_i^{y_i}}{y_i!}\\right) $$\n利用对数的性质，乘积变为求和：\n$$ l(\\beta) = \\sum_{i=1}^{n} \\ln\\left(\\frac{\\exp(-\\mu_i)\\mu_i^{y_i}}{y_i!}\\right) = \\sum_{i=1}^{n} \\left( \\ln(\\exp(-\\mu_i)) + \\ln(\\mu_i^{y_i}) - \\ln(y_i!) \\right) $$\n$$ l(\\beta) = \\sum_{i=1}^{n} \\left( -\\mu_i + y_i\\ln(\\mu_i) - \\ln(y_i!) \\right) $$\n模型指定了连接函数 $\\ln(\\mu_i) = x_i^{\\top}\\beta$，这意味着 $\\mu_i = \\exp(x_i^{\\top}\\beta)$。将 $\\mu_i$ 和 $\\ln(\\mu_i)$ 的这些表达式代入对数似然方程，得到：\n$$ l(\\beta) = \\sum_{i=1}^{n} \\left( -\\exp(x_i^{\\top}\\beta) + y_i(x_i^{\\top}\\beta) - \\ln(y_i!) \\right) $$\n这个表达式可以重新排列，以对包含 $\\beta$ 的项进行分组：\n$$ l(\\beta) = \\sum_{i=1}^{n} \\left( y_i x_i^{\\top}\\beta - \\exp(x_i^{\\top}\\beta) \\right) - \\sum_{i=1}^{n} \\ln(y_i!) $$\n这是 $\\beta$ 的完整对数似然函数。\n\n**2. 得分向量 $U(\\beta)$**\n\n得分向量是对数似然函数关于参数向量 $\\beta$ 的梯度。它是一个 $p \\times 1$ 的向量，其元素是偏导数 $\\frac{\\partial l(\\beta)}{\\partial \\beta_j}$，其中 $j=1, \\dots, p$。\n$$ U(\\beta) = \\frac{\\partial l(\\beta)}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left( \\sum_{i=1}^{n} \\left( y_i x_i^{\\top}\\beta - \\exp(x_i^{\\top}\\beta) - \\ln(y_i!) \\right) \\right) $$\n我们可以逐项求导。项 $\\ln(y_i!)$ 不依赖于 $\\beta$，所以它的导数为零。\n$$ U(\\beta) = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta} \\left( y_i x_i^{\\top}\\beta - \\exp(x_i^{\\top}\\beta) \\right) $$\n使用标准向量微积分法则：\n- $\\frac{\\partial}{\\partial \\beta} (x_i^{\\top}\\beta) = x_i$\n- $\\frac{\\partial}{\\partial \\beta} \\exp(x_i^{\\top}\\beta) = \\exp(x_i^{\\top}\\beta) \\cdot \\frac{\\partial}{\\partial \\beta} (x_i^{\\top}\\beta) = \\exp(x_i^{\\top}\\beta) x_i$\n将这些导数代回 $U(\\beta)$ 的表达式中：\n$$ U(\\beta) = \\sum_{i=1}^{n} \\left( y_i x_i - \\exp(x_i^{\\top}\\beta)x_i \\right) $$\n提出公因子 $x_i$：\n$$ U(\\beta) = \\sum_{i=1}^{n} \\left( y_i - \\exp(x_i^{\\top}\\beta) \\right) x_i $$\n这就是得分向量。它也可以写成 $\\sum_{i=1}^{n} (y_i - \\mu_i)x_i$。\n\n**3. 期望费雪信息矩阵 $I(\\beta)$**\n\n期望费雪信息矩阵定义为 $I(\\beta) = E\\left[-\\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^{\\top}}\\right]$。二阶导数矩阵 $\\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^{\\top}}$ 被称为黑塞矩阵，记为 $H(\\beta)$。我们首先通过对得分向量 $U(\\beta)$ 关于 $\\beta^{\\top}$ 求导来计算黑塞矩阵。\n$$ H(\\beta) = \\frac{\\partial U(\\beta)}{\\partial \\beta^{\\top}} = \\frac{\\partial}{\\partial \\beta^{\\top}} \\left( \\sum_{i=1}^{n} \\left( y_i x_i - \\exp(x_i^{\\top}\\beta)x_i \\right) \\right) $$\n项 $y_i x_i$ 不依赖于 $\\beta$，所以它的导数为零。我们剩下：\n$$ H(\\beta) = -\\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta^{\\top}} \\left( \\exp(x_i^{\\top}\\beta)x_i \\right) $$\n使用向量微积分的乘法法则，$\\frac{\\partial(f A)}{\\partial v^{\\top}} = A \\frac{\\partial f}{\\partial v^{\\top}}$，其中 $A$ 是一个不依赖于 $v$ 的向量。这里，$A = x_i$ 且 $f = \\exp(x_i^{\\top}\\beta)$。我们需要求 $\\frac{\\partial \\exp(x_i^{\\top}\\beta)}{\\partial \\beta^{\\top}}$。梯度是 $\\nabla_{\\beta} \\exp(x_i^{\\top}\\beta) = \\exp(x_i^{\\top}\\beta) x_i$。关于 $\\beta^{\\top}$ 的导数是梯度的转置，所以 $\\frac{\\partial \\exp(x_i^{\\top}\\beta)}{\\partial \\beta^{\\top}} = (\\exp(x_i^{\\top}\\beta) x_i)^{\\top} = \\exp(x_i^{\\top}\\beta) x_i^{\\top}$。\n因此，该项的导数为：\n$$ \\frac{\\partial}{\\partial \\beta^{\\top}} \\left( \\exp(x_i^{\\top}\\beta)x_i \\right) = x_i \\left( \\exp(x_i^{\\top}\\beta) x_i^{\\top} \\right) = \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} $$\n其中 $x_i x_i^{\\top}$ 是一个 $p \\times p$ 的外积矩阵。\n黑塞矩阵是：\n$$ H(\\beta) = - \\sum_{i=1}^{n} \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} $$\n费雪信息是 $I(\\beta) = E[-H(\\beta)]$。\n$$ I(\\beta) = E\\left[ - \\left( - \\sum_{i=1}^{n} \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} \\right) \\right] = E\\left[ \\sum_{i=1}^{n} \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} \\right] $$\n期望是关于 $Y_i$ 的分布计算的。黑塞矩阵的表达式不依赖于 $y_i$；它是参数 $\\beta$ 和固定协变量 $x_i$ 的函数。因此，该表达式的期望就是其本身。\n$$ I(\\beta) = \\sum_{i=1}^{n} \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} $$\n这就是期望费雪信息矩阵。它也可以写成 $\\sum_{i=1}^{n} \\mu_i x_i x_i^{\\top}$。\n\n所要求的三个组成部分都已从第一性原理推导出来。我们现在可以组合出最终答案。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\sum_{i=1}^{n} \\left( y_i x_i^{\\top}\\beta - \\exp(x_i^{\\top}\\beta) - \\ln(y_i!) \\right)  \\sum_{i=1}^{n} (y_i - \\exp(x_i^{\\top}\\beta))x_i  \\sum_{i=1}^{n} \\exp(x_i^{\\top}\\beta) x_i x_i^{\\top} \\end{pmatrix} } $$"
        },
        {
            "introduction": "理论模型在应用于真实数据时常常会遇到挑战，一个典型问题就是多重共线性。这项练习  通过一个直观的医学场景，演示了设计矩阵中的完美多重共线性如何导致模型参数不可识别。你将通过数学推导，亲眼见证这一问题如何体现为费雪信息矩阵的奇异性（不可逆），从而理解模型失效的根本原因。",
            "id": "4988423",
            "problem": "考虑一项医学研究，该研究使用具有二项分布和典则 logit 连接的广义线性模型 (GLM) 框架来建模院内急性肾损伤的概率。对于每位患者 $i \\in \\{1,\\dots,n\\}$，令 $Y_{i} \\in \\{0,1\\}$ 表示二元结局，线性预测变量为\n$$\n\\eta_{i} \\equiv \\beta_{0} + \\beta_{\\mathrm{M}} M_{i} + \\beta_{\\mathrm{F}} F_{i},\n$$\n其中 $M_{i} \\in \\{0,1\\}$ 是男性性别的指示变量，$F_{i} \\in \\{0,1\\}$ 是女性性别的指示变量。协变量满足医学上正确的编码规则 $M_{i} + F_{i} = 1$ 对所有患者都成立（即每位患者要么是男性，要么是女性，且没有患者同时是两者）。平均响应为 $\\mu_{i} \\equiv \\mathbb{E}[Y_{i} \\mid M_{i},F_{i}]$，并通过 logit 连接与线性预测变量相关联：$\\mathrm{logit}(\\mu_{i}) = \\eta_{i}$。假设观测是独立的。\n\n给定以下 $n=6$ 位患者的设计：\n$$\n(M_{1}, F_{1}) = (1,0),\\quad (M_{2}, F_{2}) = (1,0),\\quad (M_{3}, F_{3}) = (0,1),\\quad (M_{4}, F_{4}) = (0,1),\\quad (M_{5}, F_{5}) = (1,0),\\quad (M_{6}, F_{6}) = (0,1).\n$$\n从具有典则连接的二项 GLM 的第一性原理出发，根据设计矩阵和二项分布族的均值-方差关系，推导参数向量 $(\\beta_{0}, \\beta_{\\mathrm{M}}, \\beta_{\\mathrm{F}})$ 的观测费雪信息矩阵，并用它来解释协变量之间的完全多重共线性对系数可识别性和方差估计的影响。然后，计算该设计的观测费雪信息矩阵的行列式，并将其精确地表示为典则权重 $w_{i} \\equiv \\mu_{i}(1-\\mu_{i})$ 的函数。\n\n将行列式的精确值作为你的最终答案。无需四舍五入。",
            "solution": "该问题是有效的。它提出了一个统计建模中明确定义的场景，特别是在广义线性模型 (GLM) 框架内。该设置旨在阐明完全多重共线性、参数可识别性和费雪信息矩阵奇异性等基本统计概念。该问题具有科学依据，是客观的，并包含了得出完整解所需的所有必要信息。\n\n该问题要求完成三个主要任务：首先，推导具有典则连接的二项 GLM 的观测费雪信息矩阵。其次，使用此框架解释完全多重共线性的后果。第三，计算给定设计的该矩阵的行列式。\n\n对于指数族中具有典则连接函数的 GLM，单个观测值 $Y_i$ 的一般对数似然由下式给出\n$$l_{i}(\\theta_i, \\phi; y_i) = \\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi)$$\n其中 $\\theta_i$ 是典则参数。对于二项分布，典则参数是对数优势比，$\\theta_i = \\mathrm{logit}(\\mu_i) = \\ln(\\frac{\\mu_i}{1-\\mu_i})$。问题指定了典则 logit 连接，这意味着线性预测变量 $\\eta_i$ 等于典则参数，即 $\\eta_i = \\theta_i$。函数 $b(\\theta_i)$ 由 $b(\\theta_i) = \\ln(1 + \\exp(\\theta_i))$ 给出。均值为 $\\mathbb{E}[Y_i] = \\mu_i = b'(\\theta_i)$，方差为 $\\mathrm{Var}(Y_i) = a_i(\\phi)b''(\\theta_i)$。对于试验次数为1的二项分布（伯努利分布），离散参数为 $\\phi=1$，一个常见的先验权重是 $a_i(\\phi)=1$，方差为 $\\mathrm{Var}(Y_i) = b''(\\theta_i) = \\mu_i(1-\\mu_i)$。\n\n对于 $n$ 个独立观测的整个样本，对数似然为 $L(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} l_i$，其中 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_M, \\beta_F)^T$ 是回归系数向量。得分向量是 $L(\\boldsymbol{\\beta})$ 关于 $\\boldsymbol{\\beta}$ 的一阶导数向量：\n$$\n\\frac{\\partial L}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\n从对数似然的典则形式可知，$\\frac{\\partial l_i}{\\partial \\theta_i} = y_i - b'(\\theta_i) = y_i - \\mu_i$。使用典则连接时，$\\theta_i = \\eta_i$，所以 $\\frac{\\partial \\theta_i}{\\partial \\eta_i} = 1$。线性预测变量为 $\\eta_i = \\beta_0 + \\beta_M M_i + \\beta_F F_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$，其中 $\\mathbf{x}_i^T = (1, M_i, F_i)$。因此，$\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij}$。第 $j$ 个参数的得分为：\n$$\nU_j = \\frac{\\partial L}{\\partial \\beta_j} = \\sum_{i=1}^{n} (y_i - \\mu_i) x_{ij} \\implies \\mathbf{U}(\\boldsymbol{\\beta}) = \\mathbf{X}^T (\\mathbf{y} - \\boldsymbol{\\mu})\n$$\n黑塞矩阵 $\\mathbf{H}(\\boldsymbol{\\beta})$ 由二阶偏导数组成，$H_{jk} = \\frac{\\partial^2 L}{\\partial \\beta_j \\partial \\beta_k}$。\n$$\n\\frac{\\partial^2 L}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\sum_{i=1}^{n} (y_i - \\mu_i) x_{ij} = \\sum_{i=1}^{n} -x_{ij} \\frac{\\partial \\mu_i}{\\partial \\beta_k}\n$$\n再次使用链式法则，$\\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k}$。对于 GLM，$\\frac{\\partial \\mu_i}{\\partial \\eta_i}$ 是反连接函数的导数。对于 logit 连接，$\\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}$，且 $\\frac{d\\mu_i}{d\\eta_i} = \\mu_i(1-\\mu_i)$。这也是方差函数 $V(\\mu_i)$。\n所以，$\\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\mu_i(1-\\mu_i) x_{ik}$。将其代回，我们得到：\n$$\nH_{jk} = \\sum_{i=1}^{n} -x_{ij} (\\mu_i(1-\\mu_i)) x_{ik}\n$$\n以矩阵形式表示，$\\mathbf{H}(\\boldsymbol{\\beta}) = -\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$，其中 $\\mathbf{W}$ 是一个对角矩阵，其对角元素为 $w_i = \\mu_i(1-\\mu_i)$。观测费雪信息矩阵 $\\mathbf{I}(\\boldsymbol{\\beta})$ 定义为 $\\mathbf{I}(\\boldsymbol{\\beta}) = -\\mathbb{E}[\\mathbf{H}(\\boldsymbol{\\beta})]$。对于典则连接，黑塞矩阵不依赖于数据 $\\mathbf{y}$，因此观测信息等于期望费雪信息：\n$$\n\\mathbf{I}(\\boldsymbol{\\beta}) = -\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}\n$$\n这就完成了问题的第一部分。\n\n对于第二部分，我们分析多重共线性的影响。$n=6$ 位患者的设计矩阵 $\\mathbf{X}$ 由给定的协变量构建：\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1  M_1  F_1 \\\\\n1  M_2  F_2 \\\\\n1  M_3  F_3 \\\\\n1  M_4  F_4 \\\\\n1  M_5  F_5 \\\\\n1  M_6  F_6\n\\end{pmatrix} = \\begin{pmatrix}\n1  1  0 \\\\\n1  1  0 \\\\\n1  0  1 \\\\\n1  0  1 \\\\\n1  1  0 \\\\\n1  0  1\n\\end{pmatrix}\n$$\n该矩阵的列是 $\\mathbf{c}_0 = (1,1,1,1,1,1)^T$，$\\mathbf{c}_M = (1,1,0,0,1,0)^T$ 和 $\\mathbf{c}_F = (0,0,1,1,0,1)^T$。问题陈述了所有患者都满足约束条件 $M_i + F_i = 1$。用设计矩阵的列来表示，这意味着 $\\mathbf{c}_M + \\mathbf{c}_F = \\mathbf{c}_0$，或者 $\\mathbf{c}_0 - \\mathbf{c}_M - \\mathbf{c}_F = \\mathbf{0}$。这是 $\\mathbf{X}$ 的列之间的完全线性相关性，称为完全多重共线性。\n\n这种相关性对参数的可识别性有深远的影响。让我们考虑另一组参数 $\\boldsymbol{\\beta}^* = (\\beta_0^*, \\beta_M^*, \\beta_F^*)^T$，定义为 $\\beta_0^* = \\beta_0 + c$，$\\beta_M^* = \\beta_M - c$ 和 $\\beta_F^* = \\beta_F - c$，其中 $c \\in \\mathbb{R}$ 是任意常数。这组新参数的线性预测变量是：\n$$\n\\eta_i^* = \\beta_0^* + \\beta_M^* M_i + \\beta_F^* F_i = (\\beta_0 + c) + (\\beta_M - c)M_i + (\\beta_F - c)F_i = (\\beta_0 + \\beta_M M_i + \\beta_F F_i) + c(1 - M_i - F_i)\n$$\n给定约束条件 $M_i + F_i = 1$，对于所有 $i$，$1 - M_i - F_i = 0$。因此，$\\eta_i^* = \\eta_i$。这意味着无数个参数向量 $\\boldsymbol{\\beta}^*$ 会产生完全相同的预测概率 $\\mu_i$，从而产生相同的似然函数值。参数是不可识别的，并且不存在唯一的 $\\boldsymbol{\\beta}$ 最大似然估计。\n\n对方法差估计的影响也至关重要。最大似然估计 $\\hat{\\boldsymbol{\\beta}}$ 的渐近协方差矩阵由费雪信息矩阵的逆给出，即 $\\mathrm{Cov}(\\hat{\\boldsymbol{\\beta}}) \\approx \\mathbf{I}(\\hat{\\boldsymbol{\\beta}})^{-1} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}$。$\\mathbf{X}$ 列中的线性相关性意味着矩阵 $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$ 是奇异的。要证明这一点，令 $\\mathbf{v} = (1, -1, -1)^T$。线性相关性为 $\\mathbf{Xv} = \\mathbf{0}$。那么\n$$\n(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})\\mathbf{v} = \\mathbf{X}^T \\mathbf{W} (\\mathbf{Xv}) = \\mathbf{X}^T \\mathbf{W} \\mathbf{0} = \\mathbf{0}\n$$\n因为存在一个非零向量 $\\mathbf{v}$ 使得 $(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})\\mathbf{v} = \\mathbf{0}$，所以根据定义，矩阵 $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$ 是奇异的。奇异矩阵不可逆，这意味着估计量的协方差矩阵是未定义的。这对应于参数估计的方差为无穷大，使其变得无用。\n\n对于第三部分，我们计算 $\\mathbf{I}(\\boldsymbol{\\beta})$ 的行列式。权重 $w_i = \\mu_i(1-\\mu_i)$ 对于所有具有相同协变量值的患者都是恒定的。有 $n_M=3$ 位男性患者（$i=1,2,5$）和 $n_F=3$ 位女性患者（$i=3,4,6$）。\n对于男性患者，$\\eta_i = \\beta_0 + \\beta_M$。令 $w_M = \\mu_M(1-\\mu_M)$，其中 $\\mu_M = (1+\\exp(-(\\beta_0+\\beta_M)))^{-1}$。\n对于女性患者，$\\eta_i = \\beta_0 + \\beta_F$。令 $w_F = \\mu_F(1-\\mu_F)$，其中 $\\mu_F = (1+\\exp(-(\\beta_0+\\beta_F)))^{-1}$。\n权重对角矩阵为 $\\mathbf{W} = \\mathrm{diag}(w_M, w_M, w_F, w_F, w_M, w_F)$。\n费雪信息矩阵为 $\\mathbf{I} = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}$。其元素 $I_{jk} = \\sum_{i=1}^6 x_{ij} w_i x_{ik}$ 为：\n$I_{00} = \\sum w_i = 3w_M + 3w_F$\n$I_{0M} = \\sum w_i M_i = 3w_M$\n$I_{0F} = \\sum w_i F_i = 3w_F$\n$I_{MM} = \\sum w_i M_i^2 = 3w_M$\n$I_{FF} = \\sum w_i F_i^2 = 3w_F$\n$I_{MF} = \\sum w_i M_i F_i = 0$\n完整的矩阵是：\n$$\n\\mathbf{I} = \\begin{pmatrix}\n3w_M + 3w_F  3w_M  3w_F \\\\\n3w_M  3w_M  0 \\\\\n3w_F  0  3w_F\n\\end{pmatrix}\n$$\n为了求行列式，我们可以沿第一行进行余子式展开：\n\\begin{align*}\n\\det(\\mathbf{I}) = (3w_M + 3w_F) \\begin{vmatrix} 3w_M  0 \\\\ 0  3w_F \\end{vmatrix} - (3w_M) \\begin{vmatrix} 3w_M  0 \\\\ 3w_F  3w_F \\end{vmatrix} + (3w_F) \\begin{vmatrix} 3w_M  3w_M \\\\ 3w_F  0 \\end{vmatrix} \\\\\n= (3w_M + 3w_F)(9w_M w_F) - (3w_M)(9w_M w_F - 0) + (3w_F)(0 - 9w_M w_F) \\\\\n= (27w_M^2 w_F + 27w_M w_F^2) - 27w_M^2 w_F - 27w_M w_F^2 \\\\\n= 0\n\\end{align*}\n另外，我们观察到 $\\mathbf{I}$ 的第一列是第二列和第三列的和。一个具有线性相关的列（或行）的矩阵的行列式为零。这证实了我们的理论预期。行列式为 $0$，与权重 $w_i$ 的值无关。",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "现在，我们来处理一个更微妙但普遍存在的问题：模型设定谬误，特别是当模型的方差假设（如泊松模型的方差等于均值）不成立时，即所谓的“过度离散”。这项高级编程练习  旨在连接理论与实践。你将从零开始实现一个泊松GLM，并计算和比较三种不同的标准误——基于模型的、基于拟似然的以及稳健的“三明治”标准误，从而直观地看到在放宽假设后，统计推断会发生怎样的改变。",
            "id": "4988393",
            "problem": "考虑在一个医学研究中，使用广义线性模型 (GLM) 框架对计数结果进行建模。令 $y_i \\in \\{0,1,2,\\dots\\}$ 表示受试者 $i$ 的计数结果，其协变量为 $x_i \\in \\mathbb{R}^p$，线性预测器为 $\\eta_i = x_i^\\top \\beta$。假设使用标准对数连接函数 $g(\\mu_i) = \\log(\\mu_i)$，其中 $\\mu_i = \\mathbb{E}[y_i \\mid x_i] = \\exp(\\eta_i)$。采用拟似然方法，方差函数为 $V(\\mu) = \\mu$，因此 $\\operatorname{Var}(y_i \\mid x_i)$ 与 $\\mu_i$ 成正比。\n\n从泊松模型的指数族表示和拟似然的定义出发，推导并实现一个牛顿-拉弗森算法，以计算泊松GLM下的最大似然估计 $\\widehat{\\beta}$。然后，从得分函数及其期望曲率出发，构建三种类型的处理效应系数的标准误：\n- 在泊松方差假设下的基于模型的标准误，\n- 使用估计的离散度 $\\widehat{\\phi}$ 和 $V(\\mu) = \\mu$ 的拟似然标准误，以及\n- 不依赖于正确方差模型的稳健（Huber–White三明治）标准误。\n\n使用这些方法，比较基于模型与基于稳健标准误对处理效应的推断（在0.05水平下的双边检验）。\n\n数据生成机制：对于每个测试案例，生成独立的受试者 $i=1,\\dots,n$，包含一个用于处理的二元协变量 $t_i \\in \\{0,1\\}$ 和一个用于年龄的连续协变量 $a_i$。令 $t_i \\sim \\text{Bernoulli}(0.5)$ 且 $a_i \\sim \\text{Uniform}(20,80)$。定义一个标准化的年龄协变量 $z_i = (a_i - 50)/10$。平均计数 $\\mu_i$ 服从\n$$\n\\log(\\mu_i) = \\beta_0 + \\beta_1 t_i + \\beta_2 z_i,\n$$\n其中固定 $\\beta_0 = \\log(2.0)$ 和 $\\beta_2 = 0.02$。为了引入超出泊松模型的过离散，独立地从 $g_i \\sim \\text{Gamma}(k, \\text{scale}=1/k)$ 中抽样，其中形状参数 $k > 0$，并设置\n$$\ny_i \\mid x_i, g_i \\sim \\text{Poisson}(\\mu_i g_i),\n$$\n这得到 $\\mathbb{E}[y_i \\mid x_i] = \\mu_i$ 和 $\\operatorname{Var}(y_i \\mid x_i) = \\mu_i \\left(1 + \\frac{\\mu_i}{k}\\right)$，产生的过离散通常违反纯泊松方差模型，但适用于方差函数为 $V(\\mu)=\\mu$ 的拟似然方法。\n\n基于第一性原理的算法要求：\n- 使用泊松模型的对数似然函数推导得分和Hessian矩阵，并实现牛顿-拉弗森更新以获得 $\\widehat{\\beta}$。\n- 从泊松模型下的期望Fisher信息中，获得 $\\widehat{\\beta}$ 的基于模型的协方差。\n- 在 $V(\\mu)=\\mu$ 的假设下，通过Pearson方法估计离散度参数 $\\widehat{\\phi}$，并用它来缩放基于模型的协方差以获得拟似然标准误。\n- 通过将观测到的曲率与得分贡献的经验协方差相结合，构建稳健（Huber–White三明治）协方差，并提取稳健标准误。\n\n假设检验：对于处理效应系数 $\\beta_1$，计算基于模型和稳健标准误的双边$p$值，并在0.05水平下宣告是否拒绝 $H_0\\!:\\,\\beta_1 = 0$。\n\n测试套件：\n- 案例1（近似泊松）：$n=400$, $k=100$, $\\beta_1=0.25$, 随机种子 $12345$。\n- 案例2（中度过离散）：$n=400$, $k=5$, $\\beta_1=0.25$, 随机种子 $12345$。\n- 案例3（小样本，严重过离散）：$n=80$, $k=2$, $\\beta_1=0.15$, 随机种子 $24680$。\n\n实现约束：\n- 拟合一个具有标准对数连接函数且无偏移量的泊松GLM。\n- 使用牛顿-拉弗森算法，并设置合理的收敛容差和步长减半，以确保对数似然函数单调上升。\n- 计算：\n  1. $\\widehat{\\beta}_1$ (处理效应估计值)，\n  2. $\\widehat{\\beta}_1$ 的基于模型的标准误，\n  3. 使用Pearson方法在 $V(\\mu)=\\mu$ 下估计的 $\\widehat{\\phi}$ 计算 $\\widehat{\\beta}_1$ 的拟似然标准误，\n  4. $\\widehat{\\beta}_1$ 的稳健标准误，\n  5. 用于检验 $H_0\\!:\\,\\beta_1=0$ 的双边基于模型的 $p$ 值，\n  6. 用于检验 $H_0\\!:\\,\\beta_1=0$ 的双边稳健 $p$ 值，以及\n  7. 一个布尔值，指示在0.05水平下，基于模型的推断和稳健推断的拒绝决策是否不同。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表。每个测试案例应贡献一个形如\n$$\n[\\widehat{\\beta}_1, \\text{SE}_{\\text{model}}, \\text{SE}_{\\text{quasi}}, \\text{SE}_{\\text{robust}}, p_{\\text{model}}, p_{\\text{robust}}, \\text{changed}],\n$$\n的嵌套列表，其中 $\\text{changed}$ 是一个布尔值，指示拒绝决策是否不同。因此，最终输出必须是\n$$\n[[\\dots], [\\dots], [\\dots]]\n$$\n的形式，且在单行上。不涉及物理单位，所有实值输出都应打印为机器可读的小数。",
            "solution": "此问题已通过验证。\n\n### 步骤1：提取已知信息\n- **模型框架**：针对计数结果 $y_i \\in \\{0, 1, 2, \\dots\\}$ 的广义线性模型 (GLM)。\n- **协变量**：$x_i \\in \\mathbb{R}^p$，用于受试者 $i=1, \\dots, n$。\n- **线性预测器**：$\\eta_i = x_i^\\top \\beta$。\n- **连接函数**：标准对数连接函数，$g(\\mu_i) = \\log(\\mu_i)$，其中 $\\mu_i = \\mathbb{E}[y_i \\mid x_i]$。因此，$\\mu_i = \\exp(\\eta_i)$。\n- **方差函数**：采用拟似然方法，方差函数为 $V(\\mu) = \\mu$。\n- **任务**：\n    1. 推导并实现用于泊松GLM的牛顿-拉弗森算法，以找到最大似然估计 $\\widehat{\\beta}$。\n    2. 为 $\\widehat{\\beta}_1$ 构建三种类型的标准误：基于模型的、拟似然的、以及稳健的（Huber–White三明治）。\n    3. 使用0.05水平下的双边检验，比较对处理效应系数 $\\beta_1$ 的推断。\n- **数据生成机制**：\n    - 处理：$t_i \\sim \\text{Bernoulli}(0.5)$。\n    - 年龄：$a_i \\sim \\text{Uniform}(20, 80)$。\n    - 标准化年龄：$z_i = (a_i - 50)/10$。\n    - 协变量向量：$x_i = [1, t_i, z_i]^\\top$。因此 $p=3$。\n    - 均值模型：$\\log(\\mu_i) = \\beta_0 + \\beta_1 t_i + \\beta_2 z_i$。\n    - 真实参数：$\\beta_0 = \\log(2.0)$, $\\beta_2 = 0.02$。$\\beta_1$ 按测试案例指定。\n    - 过离散机制：$g_i \\sim \\text{Gamma}(k, \\text{scale}=1/k)$，并且 $y_i \\mid x_i, g_i \\sim \\text{Poisson}(\\mu_i g_i)$。得到的边际方差为 $\\operatorname{Var}(y_i \\mid x_i) = \\mu_i + \\mu_i^2/k$。\n- **算法要求**：\n    - 基于泊松对数似然的推导（得分函数，黑塞矩阵）。\n    - 采用步长减半法的牛顿-拉弗森实现，以保证对数似然单调上升。\n    - 使用Pearson方法在 $V(\\mu)=\\mu$ 下估计离散度参数 $\\widehat{\\phi}$。\n- **假设检验**：在 $\\alpha = 0.05$ 水平下对 $H_0: \\beta_1 = 0$ 进行双边检验。\n- **测试用例**：\n    1. $n=400, k=100, \\beta_1=0.25$, 随机种子 $12345$。\n    2. $n=400, k=5, \\beta_1=0.25$, 随机种子 $12345$。\n    3. $n=80, k=2, \\beta_1=0.15$, 随机种子 $24680$。\n- **所需输出（每个案例）**：$[\\widehat{\\beta}_1, \\text{SE}_{\\text{model}}, \\text{SE}_{\\text{quasi}}, \\text{SE}_{\\text{robust}}, p_{\\text{model}}, p_{\\text{robust}}, \\text{changed}]$。`changed` 是一个布尔值，指示在 $\\alpha=0.05$ 水平下拒绝决策是否不同。\n\n### 步骤2：使用提取的已知信息进行验证\n- **科学基础**：该问题位于统计理论的核心领域，特别涉及广义线性模型、最大似然估计和稳健推断。过离散、拟似然和三明治估计量等概念是现代应用统计学中公认的基础。\n- **定义明确**：问题指定了清晰的数据生成过程、一个明确要拟合的模型，以及用于估计和推断的具体数值方法。目标是可量化的，并且对每个测试用例都能导出一组唯一的数值结果。\n- **客观性**：问题以精确的数学和统计语言陈述，没有歧义或主观论断。\n\n该问题未违反任何无效标准。这是一个标准的、尽管全面的统计计算和理论练习。\n\n### 步骤3：结论与行动\n问题是**有效的**。将提供一个完整的、合理的解决方案。\n\n### 基于第一性原理的解决方案\n\n目标是估计泊松广义线性模型 (GLM) 的参数，并使用三种不同的标准误计算方法来比较特定系数的推断。\n\n**1. 模型设定与似然函数**\n\n为了构建似然函数，响应变量 $y_i$ 被假定服从泊松分布。其概率质量函数为 $P(Y_i=y_i) = \\frac{e^{-\\mu_i}\\mu_i^{y_i}}{y_i!}$。单个观测 $i$ 的对数似然函数为：\n$$\n\\ell_i(\\beta) = \\log(P(Y_i=y_i)) = y_i \\log(\\mu_i) - \\mu_i - \\log(y_i!)\n$$\n模型使用对数连接函数，因此均值 $\\mu_i$ 通过 $\\mu_i = \\exp(\\eta_i)$ 与线性预测器 $\\eta_i = x_i^\\top \\beta$ 相关联。将此代入对数似然函数，得到：\n$$\n\\ell_i(\\beta) = y_i (x_i^\\top \\beta) - \\exp(x_i^\\top \\beta) - \\log(y_i!)\n$$\n对于 $n$ 个独立观测，总对数似然函数为 $L(\\beta) = \\sum_{i=1}^n \\ell_i(\\beta)$。\n\n**2. 通过牛顿-拉弗森法进行最大似然估计**\n\n为了找到最大似然估计 (MLE) $\\widehat{\\beta}$，我们求解 $\\frac{\\partial L(\\beta)}{\\partial \\beta} = 0$。牛顿-拉弗森算法为此提供了一个迭代方法，其更新规则为：\n$$\n\\beta^{(t+1)} = \\beta^{(t)} - [H(\\beta^{(t)})]^{-1} U(\\beta^{(t)})\n$$\n其中 $U(\\beta)$ 是得分向量（$L(\\beta)$ 的梯度），$H(\\beta)$ 是黑塞矩阵（$L(\\beta)$ 的二阶导数）。\n\n来自观测 $i$ 的得分贡献为：\n$$\nU_i(\\beta) = \\frac{\\partial \\ell_i}{\\partial \\beta} = \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta} = (y_i - e^{\\eta_i}) x_i = (y_i - \\mu_i) x_i\n$$\n总得分向量为 $U(\\beta) = \\sum_{i=1}^n (y_i - \\mu_i) x_i = \\mathbf{X}^\\top (y - \\mu)$，其中 $\\mathbf{X}$ 是 $n \\times p$ 的设计矩阵，$y$ 是 $n \\times 1$ 的结果向量，$\\mu$ 是 $n \\times 1$ 的均值向量。\n\n来自观测 $i$ 的黑塞矩阵贡献为：\n$$\nH_i(\\beta) = \\frac{\\partial^2 \\ell_i}{\\partial \\beta \\partial \\beta^\\top} = \\frac{\\partial}{\\partial \\beta^\\top} [(y_i - e^{x_i^\\top \\beta}) x_i] = -e^{x_i^\\top \\beta} x_i x_i^\\top = -\\mu_i x_i x_i^\\top\n$$\n总黑塞矩阵为 $H(\\beta) = \\sum_{i=1}^n H_i(\\beta) = -\\sum_{i=1}^n \\mu_i x_i x_i^\\top = -\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$，其中 $\\mathbf{W}$ 是对角矩阵，其元素为 $W_{ii} = \\mu_i$。\n\n期望Fisher信息矩阵 $I(\\beta)$ 定义为 $I(\\beta) = -\\mathbb{E}[H(\\beta)]$。对于标准连接的泊松模型，观测信息 $-H(\\beta)$ 和期望信息 $I(\\beta)$ 是相同的：\n$$\nI(\\beta) = -H(\\beta) = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\n$$\n牛顿-拉弗森更新变为：\n$$\n\\beta^{(t+1)} = \\beta^{(t)} + (\\mathbf{X}^\\top \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^\\top (y - \\mu^{(t)})\n$$\n这是迭代重加权最小二乘 (IRLS) 算法的核心迭代步骤。为确保收敛，使用步长减半法以保证对数似然在每一步都增加。\n\n**3. 标准误估计**\n\n一旦找到MLE $\\widehat{\\beta}$，我们就可以估计其协方差矩阵。\n\n**a. 基于模型的标准误**\n在泊松模型被正确设定的假设下（即 $\\operatorname{Var}(y_i) = \\mu_i$），$\\widehat{\\beta}$ 的渐近协方差矩阵是Fisher信息矩阵在MLE处的逆：\n$$\n\\text{Cov}_{\\text{model}}(\\widehat{\\beta}) = [I(\\widehat{\\beta})]^{-1} = (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1}\n$$\n其中 $\\widehat{W}_{ii} = \\widehat{\\mu}_i = \\exp(x_i^\\top \\widehat{\\beta})$。系数 $\\widehat{\\beta}_j$ 的基于模型的标准误是该矩阵第 $j$ 个对角元素的平方根。\n\n**b. 拟似然标准误**\n拟似然方法承认方差函数可能未被正确设定。它假设 $\\operatorname{Var}(y_i) = \\phi V(\\mu_i)$，其中 $\\phi$ 是一个常数离散度参数。对于本问题，我们使用泊松方差函数 $V(\\mu_i) = \\mu_i$。参数 $\\phi$ 使用Pearson卡方统计量进行估计：\n$$\n\\widehat{\\phi} = \\frac{1}{n-p} \\sum_{i=1}^n \\frac{(y_i - \\widehat{\\mu}_i)^2}{V(\\widehat{\\mu}_i)} = \\frac{1}{n-p} \\sum_{i=1}^n \\frac{(y_i - \\widehat{\\mu}_i)^2}{\\widehat{\\mu}_i}\n$$\n拟似然协方差矩阵是基于模型的协方差矩阵的缩放版本：\n$$\n\\text{Cov}_{\\text{quasi}}(\\widehat{\\beta}) = \\widehat{\\phi} \\cdot \\text{Cov}_{\\text{model}}(\\widehat{\\beta}) = \\widehat{\\phi} (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1}\n$$\n如果数据是过离散的（$\\widehat{\\phi} > 1$），这些标准误将大于基于模型的标准误。\n\n**c. 稳健（三明治）标准误**\nHuber–White稳健估计量即使在方差函数 $V(\\mu)$ 被错误设定的情况下，也能提供协方差矩阵的一致估计。其“三明治”形式由以下公式给出：\n$$\n\\text{Cov}_{\\text{robust}}(\\widehat{\\beta}) = \\mathcal{I}^{-1} \\mathcal{J} \\mathcal{I}^{-1}\n$$\n其中 $\\mathcal{I}$ 是Fisher信息（“面包”），$\\mathcal{J}$ 是得分函数的方差（“肉”）。它们的估计值为：\n- $\\widehat{\\mathcal{I}} = \\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X}$\n- $\\widehat{\\mathcal{J}} = \\sum_{i=1}^n U_i(\\widehat{\\beta}) U_i(\\widehat{\\beta})^\\top = \\sum_{i=1}^n (y_i - \\widehat{\\mu}_i)^2 x_i x_i^\\top$\n因此，稳健协方差估计量为：\n$$\n\\text{Cov}_{\\text{robust}}(\\widehat{\\beta}) = (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1} \\left( \\sum_{i=1}^n (y_i - \\widehat{\\mu}_i)^2 x_i x_i^\\top \\right) (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1}\n$$\n该估计量是稳健的，因为它使用经验残差 $(y_i - \\widehat{\\mu}_i)$ 来估计得分贡献的真实方差，而不是依赖于模型假设 $\\operatorname{Var}(y_i) = \\mu_i$。\n\n**4. 假设检验**\n\n对于每种类型的标准误（SE），都对原假设 $H_0: \\beta_1 = 0$ 进行双边Wald检验。检验统计量为 $Z = \\frac{\\widehat{\\beta}_1}{\\text{SE}(\\widehat{\\beta}_1)}$。在原假设下，$Z$ 渐近服从标准正态分布 $\\mathcal{N}(0,1)$。$p$值计算为 $p = 2 \\cdot (1 - \\Phi(|Z|))$，其中 $\\Phi$ 是标准正态分布的累积分布函数。如果在显著性水平 $\\alpha=0.05$ 下 $p  0.05$，则拒绝原假设。实现将比较基于模型与基于稳健标准误的拒绝决策。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (near-Poisson)\n        {\"n\": 400, \"k\": 100, \"beta1\": 0.25, \"seed\": 12345},\n        # Case 2 (moderate overdispersion)\n        {\"n\": 400, \"k\": 5, \"beta1\": 0.25, \"seed\": 12345},\n        # Case 3 (small sample, severe overdispersion)\n        {\"n\": 80, \"k\": 2, \"beta1\": 0.15, \"seed\": 24680},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = _solve_single_case(**case)\n        all_results.append(result)\n        \n    # Format and print the final output as a single-line string\n    # that represents a list of lists.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef _generate_data(n, k, beta1, seed):\n    \"\"\"\n    Generates data according to the problem specification.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # True coefficients\n    beta_true = np.array([np.log(2.0), beta1, 0.02])\n    \n    # Generate covariates\n    t = rng.binomial(1, 0.5, size=n)\n    a = rng.uniform(20, 80, size=n)\n    z = (a - 50) / 10\n    \n    # Design matrix (intercept, treatment, scaled age)\n    X = np.c_[np.ones(n), t, z]\n    \n    # Generate outcome y\n    eta = X @ beta_true\n    mu = np.exp(eta)\n    \n    # Induce overdispersion via Gamma-Poisson mixture\n    g = rng.gamma(k, scale=1.0/k, size=n)\n    y = rng.poisson(mu * g)\n    \n    return X, y\n\n\ndef _fit_poisson_glm(X, y, tol=1e-8, max_iter=100, step_halving_max=10):\n    \"\"\"\n    Fits a Poisson GLM with log link using Newton-Raphson (IRLS).\n    \"\"\"\n    n_obs, n_params = X.shape\n    beta = np.zeros(n_params)\n    \n    # Initial log-likelihood (ignoring constant term)\n    eta = X @ beta\n    mu = np.exp(eta)\n    ll_old = np.sum(y * eta - mu)\n\n    for i in range(max_iter):\n        eta = X @ beta\n        mu = np.exp(eta)\n\n        # Score vector and Fisher information matrix\n        score = X.T @ (y - mu)\n        W = np.diag(mu)\n        fisher_info = X.T @ W @ X\n\n        try:\n            # Fisher info should be invertible if X is full rank\n            inv_fisher_info = np.linalg.inv(fisher_info)\n            delta_beta = inv_fisher_info @ score\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if singular\n            inv_fisher_info = np.linalg.pinv(fisher_info)\n            delta_beta = inv_fisher_info @ score\n\n        # Step halving to ensure monotone increase in log-likelihood\n        alpha = 1.0\n        beta_new = beta\n        ll_new = -np.inf\n        for _ in range(step_halving_max):\n            beta_try = beta + alpha * delta_beta\n            eta_new = X @ beta_try\n            \n            # Check for numerical overflow before exponentiating\n            if np.any(eta_new  50):\n                alpha /= 2\n                continue\n                \n            mu_new = np.exp(eta_new)\n            ll_try = np.sum(y * eta_new - mu_new)\n\n            if ll_try  ll_old:\n                ll_new = ll_try\n                beta_new = beta_try\n                break\n            alpha /= 2\n        \n        if ll_new == -np.inf: # No improvement found\n            break\n\n        # Check for convergence\n        if np.linalg.norm(beta_new - beta)  tol:\n            beta = beta_new\n            break\n        \n        beta = beta_new\n        ll_old = ll_new\n    \n    # Recalculate final quantities at converged beta\n    eta_hat = X @ beta\n    mu_hat = np.exp(eta_hat)\n    W_hat = np.diag(mu_hat)\n    final_fisher_info = X.T @ W_hat @ X\n    \n    return beta, mu_hat, final_fisher_info\n\n\ndef _solve_single_case(n, k, beta1, seed):\n    \"\"\"\n    Generates data, fits model, computes results for one test case.\n    \"\"\"\n    # 1. Generate data\n    X, y = _generate_data(n, k, beta1, seed)\n    n_obs, n_params = X.shape\n\n    # 2. Fit Poisson GLM to get beta_hat and ancillary quantities\n    beta_hat, mu_hat, fisher_info = _fit_poisson_glm(X, y)\n\n    # 3. Calculate all three types of standard errors for beta_1\n    \n    # 3a. Model-based SE\n    try:\n        cov_model = np.linalg.inv(fisher_info)\n        se_model_vec = np.sqrt(np.diag(cov_model))\n        se_model = se_model_vec[1]\n    except np.linalg.LinAlgError:\n        # Should not happen with well-behaved data, but as a fallback\n        cov_model = np.linalg.pinv(fisher_info)\n        se_model_vec = np.sqrt(np.diag(cov_model))\n        se_model = se_model_vec[1]\n\n    # 3b. Quasi-likelihood SE\n    pearson_chi2 = np.sum((y - mu_hat)**2 / mu_hat)\n    phi_hat = pearson_chi2 / (n_obs - n_params)\n    cov_quasi = phi_hat * cov_model\n    se_quasi = np.sqrt(cov_quasi[1, 1])\n\n    # 3c. Robust (Sandwich) SE\n    # Bread\n    bread = cov_model\n    # Meat\n    meat = np.zeros((n_params, n_params))\n    residuals = y - mu_hat\n    for i in range(n_obs):\n        meat += np.outer(X[i, :], X[i, :]) * residuals[i]**2\n    \n    cov_robust = bread @ meat @ bread\n    se_robust = np.sqrt(cov_robust[1, 1])\n\n    # 4. Perform hypothesis tests\n    beta1_hat = beta_hat[1]\n    \n    # Model-based test\n    z_model = beta1_hat / se_model\n    p_model = 2 * norm.sf(np.abs(z_model))\n    \n    # Robust test\n    z_robust = beta1_hat / se_robust\n    p_robust = 2 * norm.sf(np.abs(z_robust))\n\n    # 5. Compare rejection decisions\n    reject_model = p_model  0.05\n    reject_robust = p_robust  0.05\n    changed = reject_model != reject_robust\n\n    # 6. Format results for this case\n    return [\n        beta1_hat,\n        se_model,\n        se_quasi,\n        se_robust,\n        p_model,\n        p_robust,\n        bool(changed) # Ensure it's a Python boolean for proper str() conversion\n    ]\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}