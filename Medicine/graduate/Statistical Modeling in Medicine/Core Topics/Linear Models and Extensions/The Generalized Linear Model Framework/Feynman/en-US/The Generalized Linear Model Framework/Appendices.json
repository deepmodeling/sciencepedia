{
    "hands_on_practices": [
        {
            "introduction": "Logistic regression is a cornerstone of modern medical statistics, providing a powerful framework for modeling binary outcomes. This exercise takes you back to first principles, asking you to derive the key components of a logistic regression model from its fundamental building blocks: the Bernoulli probability mass function and the canonical logit link. By deriving the log-likelihood, gradient vector, and Hessian matrix, you will build a deep understanding of the mathematical engine that drives model fitting and inference in this essential GLM. ",
            "id": "4988404",
            "problem": "A clinical data science team is developing a Generalized Linear Model (GLM) to estimate the probability that a postoperative patient develops sepsis within $48$ hours based on covariates that include age, surgical duration, baseline white blood cell count, and a comorbidity score. For patient $i$, let the binary outcome be $y_{i} \\in \\{0,1\\}$, where $y_{i}=1$ indicates sepsis and $y_{i}=0$ indicates no sepsis. Let the covariate vector be $x_{i} \\in \\mathbb{R}^{p}$ and the regression parameter be $\\beta \\in \\mathbb{R}^{p}$, with the linear predictor defined by $\\eta_{i} = x_{i}^{\\top}\\beta$. Assume conditional independence of outcomes given covariates and parameters and that, for each $i$, $Y_{i}$ follows a Bernoulli distribution with mean $\\mu_{i} = \\mathbb{E}[Y_{i} \\mid x_{i}]$.\n\nStarting from the following fundamental bases:\n- The Bernoulli probability mass function (pmf) for an independent observation is $p(y_{i} \\mid \\mu_{i}) = \\mu_{i}^{y_{i}} (1-\\mu_{i})^{1-y_{i}}$ for $y_{i} \\in \\{0,1\\}$.\n- The canonical link for the Bernoulli distribution in a Generalized Linear Model (GLM) is the logit link, defined by $\\ln\\!\\left(\\frac{\\mu_{i}}{1-\\mu_{i}}\\right) = \\eta_{i} = x_{i}^{\\top}\\beta$.\n\nUsing only these bases and the definition of the exponential family, derive the log-likelihood function $\\ell(\\beta)$ for the parameter vector $\\beta$ under the logit link. Then compute the gradient (score) vector and Hessian (observed information) matrix of $\\ell(\\beta)$ with respect to $\\beta$, expressing them in terms of $\\{(y_{i}, x_{i})\\}_{i=1}^{n}$ and $\\beta$. Provide simplified closed-form expressions; no numerical approximation is required. Express any logarithms using $\\ln$ and exponentials using $\\exp$. Your final answer must be the three derived expressions.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains a complete and consistent setup for a standard derivation in statistical learning theory.\n\nWe are tasked with deriving the log-likelihood function $\\ell(\\beta)$, its gradient vector $\\nabla_{\\beta} \\ell(\\beta)$, and its Hessian matrix $\\nabla_{\\beta}^2 \\ell(\\beta)$ for a logistic regression model, which is a specific type of Generalized Linear Model (GLM).\n\nFirst, we derive the log-likelihood function $\\ell(\\beta)$.\nThe probability mass function (pmf) for a single Bernoulli observation $y_i$ is given as $p(y_i \\mid \\mu_i) = \\mu_i^{y_i} (1-\\mu_i)^{1-y_i}$. The likelihood for a single observation is identical to this pmf. The log-likelihood for a single observation, denoted $\\ell_i$, is the natural logarithm of the likelihood:\n$$\n\\ell_i(\\mu_i) = \\ln(p(y_i \\mid \\mu_i)) = y_i \\ln(\\mu_i) + (1-y_i) \\ln(1-\\mu_i)\n$$\nWe need to express $\\ell_i$ as a function of the parameter vector $\\beta$. The model links the mean $\\mu_i$ to the linear predictor $\\eta_i = x_i^\\top\\beta$ via the logit link function:\n$$\n\\eta_i = \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)\n$$\nTo express $\\mu_i$ in terms of $\\eta_i$, we invert the link function:\n$$\n\\exp(\\eta_i) = \\frac{\\mu_i}{1-\\mu_i} \\implies \\exp(\\eta_i)(1-\\mu_i) = \\mu_i \\implies \\exp(\\eta_i) - \\mu_i\\exp(\\eta_i) = \\mu_i\n$$\n$$\n\\exp(\\eta_i) = \\mu_i(1 + \\exp(\\eta_i)) \\implies \\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(-\\eta_i)}\n$$\nThis is the logistic sigmoid function. We also find an expression for $1-\\mu_i$:\n$$\n1-\\mu_i = 1 - \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1+\\exp(\\eta_i)-\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(\\eta_i)}\n$$\nSubstituting these into the single-observation log-likelihood $\\ell_i$:\n$$\n\\ell_i(\\eta_i) = y_i \\ln\\left(\\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}\\right) + (1-y_i) \\ln\\left(\\frac{1}{1+\\exp(\\eta_i)}\\right)\n$$\nUsing the properties of logarithms, $\\ln(a/b) = \\ln(a) - \\ln(b)$ and $\\ln(1/b) = -\\ln(b)$:\n$$\n\\ell_i(\\eta_i) = y_i (\\ln(\\exp(\\eta_i)) - \\ln(1+\\exp(\\eta_i))) + (1-y_i) (-\\ln(1+\\exp(\\eta_i)))\n$$\n$$\n\\ell_i(\\eta_i) = y_i \\eta_i - y_i \\ln(1+\\exp(\\eta_i)) - \\ln(1+\\exp(\\eta_i)) + y_i \\ln(1+\\exp(\\eta_i))\n$$\n$$\n\\ell_i(\\eta_i) = y_i \\eta_i - \\ln(1+\\exp(\\eta_i))\n$$\nSubstituting $\\eta_i = x_i^\\top\\beta$:\n$$\n\\ell_i(\\beta) = y_i x_i^\\top\\beta - \\ln(1+\\exp(x_i^\\top\\beta))\n$$\nAssuming the $n$ observations are conditionally independent, the total log-likelihood $\\ell(\\beta)$ is the sum of the individual log-likelihoods:\n$$\n\\ell(\\beta) = \\sum_{i=1}^{n} \\ell_i(\\beta) = \\sum_{i=1}^{n} \\left( y_i x_i^\\top\\beta - \\ln(1+\\exp(x_i^\\top\\beta)) \\right)\n$$\n\nSecond, we compute the gradient (score) vector, $\\nabla_{\\beta} \\ell(\\beta)$.\nThe gradient of the total log-likelihood is the sum of the gradients of the individual log-likelihoods: $\\nabla_{\\beta} \\ell(\\beta) = \\sum_{i=1}^{n} \\nabla_{\\beta} \\ell_i(\\beta)$. We use the chain rule for $\\nabla_{\\beta} \\ell_i(\\beta)$:\n$$\n\\nabla_{\\beta} \\ell_i(\\beta) = \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\nabla_{\\beta} \\eta_i\n$$\nThe derivative of the linear predictor $\\eta_i = x_i^\\top\\beta$ with respect to the vector $\\beta$ is $\\nabla_{\\beta} \\eta_i = x_i$.\nThe derivative of $\\ell_i$ with respect to the scalar $\\eta_i$ is:\n$$\n\\frac{\\partial \\ell_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( y_i \\eta_i - \\ln(1+\\exp(\\eta_i)) \\right) = y_i - \\frac{1}{1+\\exp(\\eta_i)} \\cdot \\exp(\\eta_i) = y_i - \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}\n$$\nRecognizing that $\\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}$, we have $\\frac{\\partial \\ell_i}{\\partial \\eta_i} = y_i - \\mu_i$.\nCombining these results:\n$$\n\\nabla_{\\beta} \\ell_i(\\beta) = (y_i - \\mu_i)x_i\n$$\nThe total gradient is the sum over all observations:\n$$\n\\nabla_{\\beta} \\ell(\\beta) = \\sum_{i=1}^{n} (y_i - \\mu_i)x_i = \\sum_{i=1}^{n} \\left( y_i - \\frac{\\exp(x_i^\\top\\beta)}{1+\\exp(x_i^\\top\\beta)} \\right) x_i\n$$\n\nThird, we compute the Hessian (observed information) matrix, $\\nabla_{\\beta}^2 \\ell(\\beta)$.\nThe Hessian is the matrix of second partial derivatives, $\\nabla_{\\beta}^2 \\ell(\\beta) = \\nabla_{\\beta} (\\nabla_{\\beta} \\ell(\\beta))^\\top$. It is also the sum of the individual Hessians: $\\nabla_{\\beta}^2 \\ell(\\beta) = \\sum_{i=1}^{n} \\nabla_{\\beta}^2 \\ell_i(\\beta)$.\nWe compute $\\nabla_{\\beta}^2 \\ell_i(\\beta)$ by differentiating $\\nabla_{\\beta} \\ell_i(\\beta) = (y_i - \\mu_i)x_i$ with respect to $\\beta^\\top$:\n$$\n\\nabla_{\\beta}^2 \\ell_i(\\beta) = \\nabla_{\\beta} \\left[ (y_i - \\mu_i(\\beta))x_i \\right]^\\top = \\nabla_{\\beta} (y_i - \\mu_i) x_i^\\top\n$$\nSince $y_i$ and $x_i$ are constant with respect to $\\beta$:\n$$\n\\nabla_{\\beta}^2 \\ell_i(\\beta) = - (\\nabla_{\\beta} \\mu_i) x_i^\\top\n$$\nAgain, we apply the chain rule to find $\\nabla_{\\beta} \\mu_i$:\n$$\n\\nabla_{\\beta} \\mu_i = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\nabla_{\\beta} \\eta_i = \\frac{\\partial \\mu_i}{\\partial \\eta_i} x_i\n$$\nWe need the derivative of $\\mu_i$ with respect to $\\eta_i$:\n$$\n\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} \\right)\n$$\nUsing the quotient rule:\n$$\n\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\exp(\\eta_i)(1+\\exp(\\eta_i)) - \\exp(\\eta_i)\\exp(\\eta_i)}{(1+\\exp(\\eta_i))^2} = \\frac{\\exp(\\eta_i)}{(1+\\exp(\\eta_i))^2}\n$$\nThis can be written as:\n$$\n\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} \\cdot \\frac{1}{1+\\exp(\\eta_i)} = \\mu_i(1-\\mu_i)\n$$\nThis quantity, $\\mu_i(1-\\mu_i)$, is the variance of a Bernoulli trial, $V(\\mu_i)$. For a canonical link function, it is generally true that $\\frac{\\partial\\mu_i}{\\partial\\eta_i} = V(\\mu_i)$.\nSubstituting back:\n$$\n\\nabla_{\\beta} \\mu_i = \\mu_i(1-\\mu_i)x_i\n$$\nNow we can write the Hessian for a single observation:\n$$\n\\nabla_{\\beta}^2 \\ell_i(\\beta) = - (\\mu_i(1-\\mu_i)x_i)x_i^\\top = -\\mu_i(1-\\mu_i)x_i x_i^\\top\n$$\nThe total Hessian is the sum over all observations:\n$$\n\\nabla_{\\beta}^2 \\ell(\\beta) = \\sum_{i=1}^{n} \\left( -\\mu_i(1-\\mu_i)x_i x_i^\\top \\right) = - \\sum_{i=1}^{n} \\mu_i(1-\\mu_i)x_i x_i^\\top\n$$\nSubstituting the expressions for $\\mu_i$ and $1-\\mu_i$:\n$$\n\\nabla_{\\beta}^2 \\ell(\\beta) = - \\sum_{i=1}^{n} \\frac{\\exp(x_i^\\top\\beta)}{(1+\\exp(x_i^\\top\\beta))^2} x_i x_i^\\top\n$$\nThe three required expressions have now been derived.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sum_{i=1}^{n} \\left( y_i x_i^\\top \\beta - \\ln(1 + \\exp(x_i^\\top \\beta)) \\right)\n&\n\\sum_{i=1}^{n} \\left( y_i - \\frac{\\exp(x_i^\\top \\beta)}{1 + \\exp(x_i^\\top \\beta)} \\right) x_i\n&\n- \\sum_{i=1}^{n} \\frac{\\exp(x_i^\\top \\beta)}{(1 + \\exp(x_i^\\top \\beta))^2} x_i x_i^\\top\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While theoretical models provide an essential foundation, real-world data often challenges their assumptions, with overdispersion in count data being a classic example. This advanced computational exercise bridges the gap between theory and application by having you implement a Poisson GLM from the ground up and then equip it with modern statistical tools to handle model misspecification. By computing and comparing model-based, quasi-likelihood, and robust 'sandwich' standard errors, you will gain practical skills in diagnosing and correcting for overdispersion, leading to more reliable scientific conclusions. ",
            "id": "4988423",
            "problem": "Consider a medical study modeling the probability of in-hospital acute kidney injury using the Generalized Linear Model (GLM) framework with a binomial distribution and the canonical logit link. For each patient $i \\in \\{1,\\dots,n\\}$, let $Y_{i} \\in \\{0,1\\}$ denote the binary outcome and let the linear predictor be\n$$\n\\eta_{i} \\equiv \\beta_{0} + \\beta_{\\mathrm{M}} M_{i} + \\beta_{\\mathrm{F}} F_{i},\n$$\nwhere $M_{i} \\in \\{0,1\\}$ is an indicator for male sex and $F_{i} \\in \\{0,1\\}$ is an indicator for female sex. The covariates satisfy the medically correct coding rule $M_{i} + F_{i} = 1$ for all patients (every patient is either male or female, and no patient is both). The mean response is $\\mu_{i} \\equiv \\mathbb{E}[Y_{i} \\mid M_{i},F_{i}]$ and is related to the linear predictor through the logit link $\\mathrm{logit}(\\mu_{i}) = \\eta_{i}$. Assume independent observations.\n\nYou are given the following $n=6$-patient design:\n$$\n(M_{1}, F_{1}) = (1,0),\\quad (M_{2}, F_{2}) = (1,0),\\quad (M_{3}, F_{3}) = (0,1),\\quad (M_{4}, F_{4}) = (0,1),\\quad (M_{5}, F_{5}) = (1,0),\\quad (M_{6}, F_{6}) = (0,1).\n$$\nStarting from first principles for a binomial GLM with canonical link, derive the observed Fisher information matrix for the parameter vector $(\\beta_{0}, \\beta_{\\mathrm{M}}, \\beta_{\\mathrm{F}})$ in terms of the design matrix and the mean-variance relationship of the binomial family, and use it to explain the effect of perfect multicollinearity among the covariates on coefficient identifiability and variance estimates. Then, compute the determinant of the observed Fisher information matrix for this design, expressed exactly as a function of the canonical weights $w_{i} \\equiv \\mu_{i}(1-\\mu_{i})$.\n\nState your final answer as the exact value of the determinant. No rounding is required.",
            "solution": "The problem is valid. It presents a well-defined scenario in statistical modeling, specifically within the Generalized Linear Model (GLM) framework. The setup is designed to illustrate the fundamental statistical concepts of perfect multicollinearity, parameter identifiability, and singularity of the Fisher information matrix. The problem is scientifically grounded, objective, and contains all necessary information to proceed to a full solution.\n\nThe problem asks for three main tasks: First, to derive the observed Fisher information matrix for a binomial GLM with a canonical link. Second, to use this framework to explain the consequences of perfect multicollinearity. Third, to compute the determinant of this matrix for the given design.\n\nThe general log-likelihood for a single observation $Y_i$ in a GLM from the exponential family with canonical link function is given by\n$$l_{i}(\\theta_i, \\phi; y_i) = \\frac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c(y_i, \\phi)$$\nwhere $\\theta_i$ is the canonical parameter. For the binomial distribution, the canonical parameter is the log-odds, $\\theta_i = \\mathrm{logit}(\\mu_i) = \\ln(\\frac{\\mu_i}{1-\\mu_i})$. The problem specifies a canonical logit link, which means the linear predictor $\\eta_i$ is equal to the canonical parameter, $\\eta_i = \\theta_i$. The function $b(\\theta_i)$ is given by $b(\\theta_i) = \\ln(1 + \\exp(\\theta_i))$. The mean is $\\mathbb{E}[Y_i] = \\mu_i = b'(\\theta_i)$, and the variance is $\\mathrm{Var}(Y_i) = a_i(\\phi)b''(\\theta_i)$. For the binomial distribution with $1$ trial (Bernoulli), the dispersion parameter is $\\phi=1$, a common prior weight is $a_i(\\phi)=1$, and the variance is $\\mathrm{Var}(Y_i) = b''(\\theta_i) = \\mu_i(1-\\mu_i)$.\n\nThe log-likelihood for the entire sample of $n$ independent observations is $L(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} l_i$, whereboldsymbol{\\beta} = (\\beta_0, \\beta_M, \\beta_F)^T$ is the vector of regression coefficients. The score vector is the vector of first derivatives of $L(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$:\n$$\n\\frac{\\partial L}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nFrom the canonical form of the log-likelihood, $\\frac{\\partial l_i}{\\partial \\theta_i} = y_i - b'(\\theta_i) = y_i - \\mu_i$. With the canonical link, $\\theta_i = \\eta_i$, so $\\frac{\\partial \\theta_i}{\\partial \\eta_i} = 1$. The linear predictor is $\\eta_i = \\beta_0 + \\beta_M M_i + \\beta_F F_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$, where $\\mathbf{x}_i^T = (1, M_i, F_i)$. Thus, $\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij}$. The score for the $j$-th parameter is:\n$$\nU_j = \\frac{\\partial L}{\\partial \\beta_j} = \\sum_{i=1}^{n} (y_i - \\mu_i) x_{ij} \\implies \\mathbf{U}(\\boldsymbol{\\beta}) = \\mathbf{X}^T (\\mathbf{y} - \\boldsymbol{\\mu})\n$$\nThe Hessian matrix $\\mathbf{H}(\\boldsymbol{\\beta})$ consists of the second partial derivatives, $H_{jk} = \\frac{\\partial^2 L}{\\partial \\beta_j \\partial \\beta_k}$.\n$$\n\\frac{\\partial^2 L}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\sum_{i=1}^{n} (y_i - \\mu_i) x_{ij} = \\sum_{i=1}^{n} -x_{ij} \\frac{\\partial \\mu_i}{\\partial \\beta_k}\n$$\nUsing the chain rule again, $\\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k}$. For a GLM, $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$ is the derivative of the inverse link function. For the logit link, $\\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}$, and $\\frac{d\\mu_i}{d\\eta_i} = \\mu_i(1-\\mu_i)$. This is also the variance function $V(\\mu_i)$.\nSo, $\\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\mu_i(1-\\mu_i) x_{ik}$.\nSubstituting this back, we get:\n$$\nH_{jk} = \\sum_{i=1}^{n} -x_{ij} (\\mu_i(1-\\mu_i)) x_{ik}\n$$\nIn matrix form, $\\mathbf{H}(\\boldsymbol{\\beta}) = -\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$, where $\\mathbf{W}$ is a diagonal matrix with diagonal elements $w_i = \\mu_i(1-\\mu_i)$. The observed Fisher information matrix $\\mathbf{I}(\\boldsymbol{\\beta})$ is defined as $\\mathbf{I}(\\boldsymbol{\\beta}) = -\\mathbb{E}[\\mathbf{H}(\\boldsymbol{\\beta})]$. For a canonical link, the Hessian does not depend on the data $\\mathbf{y}$, so the observed information equals the expected Fisher information:\n$$\n\\mathbf{I}(\\boldsymbol{\\beta}) = -\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}\n$$\nThis completes the first part of the problem.\n\nFor the second part, we analyze the effect of multicollinearity. The design matrix $\\mathbf{X}$ for the $n=6$ patients is constructed from the given covariates:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & M_1 & F_1 \\\\\n1 & M_2 & F_2 \\\\\n1 & M_3 & F_3 \\\\\n1 & M_4 & F_4 \\\\\n1 & M_5 & F_5 \\\\\n1 & M_6 & F_6\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1\n\\end{pmatrix}\n$$\nThe columns of this matrix are $\\mathbf{c}_0 = (1,1,1,1,1,1)^T$, $\\mathbf{c}_M = (1,1,0,0,1,0)^T$, and $\\mathbf{c}_F = (0,0,1,1,0,1)^T$. The problem states the constraint $M_i + F_i = 1$ for all patients. In terms of the columns of the design matrix, this means $\\mathbf{c}_M + \\mathbf{c}_F = \\mathbf{c}_0$, or $\\mathbf{c}_0 - \\mathbf{c}_M - \\mathbf{c}_F = \\mathbf{0}$. This is a perfect linear dependency among the columns of $\\mathbf{X}$, which is known as perfect multicollinearity.\n\nThis dependency has a profound effect on parameter identifiability. Let's consider an alternative set of parameters $\\boldsymbol{\\beta}^* = (\\beta_0^*, \\beta_M^*, \\beta_F^*)^T$ defined by $\\beta_0^* = \\beta_0 + c$, $\\beta_M^* = \\beta_M - c$, and $\\beta_F^* = \\beta_F - c$ for any arbitrary constant $c \\in \\mathbb{R}$. The linear predictor for this new set of parameters is:\n$$\n\\eta_i^* = \\beta_0^* + \\beta_M^* M_i + \\beta_F^* F_i = (\\beta_0 + c) + (\\beta_M - c)M_i + (\\beta_F - c)F_i = (\\beta_0 + \\beta_M M_i + \\beta_F F_i) + c(1 - M_i - F_i)\n$$\nGiven the constraint $M_i + F_i = 1$, the term $1 - M_i - F_i = 0$ for all $i$. Therefore, $\\eta_i^* = \\eta_i$. This implies that an infinite number of parameter vectors $\\boldsymbol{\\beta}^*$ produce the exact same predicted probabilities $\\mu_i$ and thus the same value of the likelihood function. The parameters are not identifiable, and a unique maximum likelihood estimate for $\\boldsymbol{\\beta}$ does not exist.\n\nThe effect on variance estimates is also critical. The asymptotic covariance matrix of the MLE $\\hat{\\boldsymbol{\\beta}}$ is given by the inverse of the Fisher information matrix, $\\mathrm{Cov}(\\hat{\\boldsymbol{\\beta}}) \\approx \\mathbf{I}(\\hat{\\boldsymbol{\\beta}})^{-1} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}$. The linear dependency in the columns of $\\mathbf{X}$ implies that the matrix $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$ is singular. To see this, let $\\mathbf{v} = (1, -1, -1)^T$. The linear dependency is $\\mathbf{Xv} = \\mathbf{0}$. Then,\n$$\n(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})\\mathbf{v} = \\mathbf{X}^T \\mathbf{W} (\\mathbf{Xv}) = \\mathbf{X}^T \\mathbf{W} \\mathbf{0} = \\mathbf{0}\n$$\nSince there exists a non-zero vector $\\mathbf{v}$ for which $(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})\\mathbf{v} = \\mathbf{0}$, the matrix $\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$ is singular by definition. A singular matrix is not invertible, which means the covariance matrix of the estimators is undefined. This corresponds to infinite variances for the parameter estimates, rendering them useless.\n\nFor the third part, we compute the determinant of $\\mathbf{I}(\\boldsymbol{\\beta})$. The weights $w_i = \\mu_i(1-\\mu_i)$ are constant for all patients with the same covariate values. There are $n_M=3$ male patients ($i=1,2,5$) and $n_F=3$ female patients ($i=3,4,6$).\nFor male patients, $\\eta_i = \\beta_0 + \\beta_M$. Let $w_M = \\mu_M(1-\\mu_M)$ where $\\mu_M = (1+\\exp(-(\\beta_0+\\beta_M)))^{-1}$.\nFor female patients, $\\eta_i = \\beta_0 + \\beta_F$. Let $w_F = \\mu_F(1-\\mu_F)$ where $\\mu_F = (1+\\exp(-(\\beta_0+\\beta_F)))^{-1}$.\nThe diagonal matrix of weights is $\\mathbf{W} = \\mathrm{diag}(w_M, w_M, w_F, w_F, w_M, w_F)$.\nThe Fisher information matrix is $\\mathbf{I} = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}$. Its elements $I_{jk} = \\sum_{i=1}^6 x_{ij} w_i x_{ik}$ are:\n$I_{00} = \\sum w_i = 3w_M + 3w_F$\n$I_{0M} = \\sum w_i M_i = 3w_M$\n$I_{0F} = \\sum w_i F_i = 3w_F$\n$I_{MM} = \\sum w_i M_i^2 = 3w_M$\n$I_{FF} = \\sum w_i F_i^2 = 3w_F$\n$I_{MF} = \\sum w_i M_i F_i = 0$\nThe full matrix is:\n$$\n\\mathbf{I} = \\begin{pmatrix}\n3w_M + 3w_F & 3w_M & 3w_F \\\\\n3w_M & 3w_M & 0 \\\\\n3w_F & 0 & 3w_F\n\\end{pmatrix}\n$$\nTo find the determinant, we can use the cofactor expansion along the first row:\n\\begin{align*}\n\\det(\\mathbf{I}) &= (3w_M + 3w_F) \\begin{vmatrix} 3w_M & 0 \\\\ 0 & 3w_F \\end{vmatrix} - (3w_M) \\begin{vmatrix} 3w_M & 0 \\\\ 3w_F & 3w_F \\end{vmatrix} + (3w_F) \\begin{vmatrix} 3w_M & 3w_M \\\\ 3w_F & 0 \\end{vmatrix} \\\\\n&= (3w_M + 3w_F)(9w_M w_F) - (3w_M)(9w_M w_F - 0) + (3w_F)(0 - 9w_M w_F) \\\\\n&= (27w_M^2 w_F + 27w_M w_F^2) - 27w_M^2 w_F - 27w_M w_F^2 \\\\\n&= 0\n\\end{align*}\nAlternatively, we observe that the first column of $\\mathbf{I}$ is the sum of the second and third columns. A matrix with linearly dependent columns (or rows) has a determinant of zero. This confirms our theoretical expectation. The determinant is $0$, regardless of the values of the weights $w_i$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "While theoretical models provide an essential foundation, real-world data often challenges their assumptions, with overdispersion in count data being a classic example. This advanced computational exercise bridges the gap between theory and application by having you implement a Poisson GLM from the ground up and then equip it with modern statistical tools to handle model misspecification. By computing and comparing model-based, quasi-likelihood, and robust 'sandwich' standard errors, you will gain practical skills in diagnosing and correcting for overdispersion, leading to more reliable scientific conclusions. ",
            "id": "4988393",
            "problem": "Consider count outcomes in a medical study modeled under the generalized linear model (GLM) framework. Let $y_i \\in \\{0,1,2,\\dots\\}$ denote the count for subject $i$, with covariates $x_i \\in \\mathbb{R}^p$ and linear predictor $\\eta_i = x_i^\\top \\beta$. Assume the canonical log link $g(\\mu_i) = \\log(\\mu_i)$ with $\\mu_i = \\mathbb{E}[y_i \\mid x_i] = \\exp(\\eta_i)$. Adopt the quasi-likelihood approach with variance function $V(\\mu) = \\mu$, so that $\\operatorname{Var}(y_i \\mid x_i)$ is proportional to $\\mu_i$. \n\nStarting from the exponential family representation of the Poisson model and the definition of quasi-likelihood, derive and implement a Newton–Raphson algorithm to compute the maximum likelihood estimate $\\widehat{\\beta}$ under a Poisson GLM. Then, starting from the score function and its expected curvature, construct three types of standard errors for the treatment effect coefficient: \n- model-based standard errors under the Poisson variance assumption,\n- quasi-likelihood standard errors using an estimated dispersion $\\widehat{\\phi}$ with $V(\\mu) = \\mu$, and \n- robust (Huber–White sandwich) standard errors that do not rely on a correct variance model.\n\nUse these to compare inference on the treatment effect (two-sided test at level $0.05$) under model-based versus robust standard errors.\n\nData-generating mechanism: For each test case, generate independent subjects $i=1,\\dots,n$ with one binary covariate for treatment $t_i \\in \\{0,1\\}$ and one continuous covariate for age $a_i$. Let $t_i \\sim \\text{Bernoulli}(0.5)$ and $a_i \\sim \\text{Uniform}(20,80)$. Define a scaled age covariate $z_i = (a_i - 50)/10$. The mean count $\\mu_i$ follows\n$$\n\\log(\\mu_i) = \\beta_0 + \\beta_1 t_i + \\beta_2 z_i,\n$$\nwith fixed $\\beta_0 = \\log(2.0)$ and $\\beta_2 = 0.02$. To induce overdispersion beyond the Poisson model, draw $g_i \\sim \\text{Gamma}(k, \\text{scale}=1/k)$ independently with shape parameter $k > 0$ and set\n$$\ny_i \\mid x_i, g_i \\sim \\text{Poisson}(\\mu_i g_i),\n$$\nwhich yields $\\mathbb{E}[y_i \\mid x_i] = \\mu_i$ and $\\operatorname{Var}(y_i \\mid x_i) = \\mu_i \\left(1 + \\frac{\\mu_i}{k}\\right)$, producing overdispersion that generally violates the pure Poisson variance model but is amenable to quasi-likelihood with $V(\\mu)=\\mu$.\n\nAlgorithmic requirements grounded in first principles:\n- Use the log-likelihood for the Poisson model to derive the score and Hessian, and implement a Newton–Raphson update to obtain $\\widehat{\\beta}$.\n- From the expected Fisher information under the Poisson model, obtain the model-based covariance of $\\widehat{\\beta}$.\n- Estimate the dispersion parameter $\\widehat{\\phi}$ by the Pearson method under $V(\\mu)=\\mu$, and use it to scale the model-based covariance to obtain quasi-likelihood standard errors.\n- Construct robust (Huber–White sandwich) covariance by combining the observed curvature with the empirical covariance of the score contributions, and extract robust standard errors.\n\nHypothesis testing: For the treatment effect coefficient $\\beta_1$, compute two-sided $p$-values under the model-based and robust standard errors and declare rejection of $H_0\\!:\\,\\beta_1 = 0$ at level $0.05$.\n\nTest suite:\n- Case 1 (near-Poisson): $n=400$, $k=100$, $\\beta_1=0.25$, random seed $12345$.\n- Case 2 (moderate overdispersion): $n=400$, $k=5$, $\\beta_1=0.25$, random seed $12345$.\n- Case 3 (small sample, severe overdispersion): $n=80$, $k=2$, $\\beta_1=0.15$, random seed $24680$.\n\nImplementation constraints:\n- Fit a Poisson GLM with the canonical log link and no offset.\n- Use Newton–Raphson with a sensible convergence tolerance and step halving to ensure monotone ascent in the log-likelihood.\n- Compute:\n  1. $\\widehat{\\beta}_1$ (the treatment effect estimate),\n  2. the model-based standard error for $\\widehat{\\beta}_1$,\n  3. the quasi-likelihood standard error for $\\widehat{\\beta}_1$ using $\\widehat{\\phi}$ estimated by the Pearson method under $V(\\mu)=\\mu$,\n  4. the robust standard error for $\\widehat{\\beta}_1$,\n  5. the two-sided model-based $p$-value for testing $H_0\\!:\\,\\beta_1=0$,\n  6. the two-sided robust $p$-value for testing $H_0\\!:\\,\\beta_1=0$, and\n  7. a boolean indicating whether the rejection decision differs between model-based and robust inference at level $0.05$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case should contribute a nested list of the form\n$$\n[\\widehat{\\beta}_1, \\text{SE}_{\\text{model}}, \\text{SE}_{\\text{quasi}}, \\text{SE}_{\\text{robust}}, p_{\\text{model}}, p_{\\text{robust}}, \\text{changed}],\n$$\nwhere $\\text{changed}$ is a boolean indicating whether the rejection decisions differ. Thus the final output must look like\n$$\n[[\\dots], [\\dots], [\\dots]]\n$$\non a single line. No physical units are involved, and all real-valued outputs should be printed as machine-readable decimal numbers.",
            "solution": "### Step 1: Extract Givens\n- **Model Framework**: Generalized Linear Model (GLM) for count outcomes $y_i \\in \\{0, 1, 2, \\dots\\}$.\n- **Covariates**: $x_i \\in \\mathbb{R}^p$ for subject $i=1, \\dots, n$.\n- **Linear Predictor**: $\\eta_i = x_i^\\top \\beta$.\n- **Link Function**: Canonical log link, $g(\\mu_i) = \\log(\\mu_i)$, where $\\mu_i = \\mathbb{E}[y_i \\mid x_i]$. Thus, $\\mu_i = \\exp(\\eta_i)$.\n- **Variance Function**: Quasi-likelihood approach with $V(\\mu) = \\mu$.\n- **Task**:\n    1. Derive and implement a Newton–Raphson algorithm for the Poisson GLM to find the maximum likelihood estimate $\\widehat{\\beta}$.\n    2. Construct three types of standard errors for $\\widehat{\\beta}_1$: model-based, quasi-likelihood, and robust (Huber–White sandwich).\n    3. Compare inference on the treatment effect coefficient $\\beta_1$ using a two-sided test at level $\\alpha=0.05$.\n- **Data-Generating Mechanism**:\n    - Treatment: $t_i \\sim \\text{Bernoulli}(0.5)$.\n    - Age: $a_i \\sim \\text{Uniform}(20, 80)$.\n    - Scaled age: $z_i = (a_i - 50)/10$.\n    - Covariate vector: $x_i = [1, t_i, z_i]^\\top$. So $p=3$.\n    - Mean model: $\\log(\\mu_i) = \\beta_0 + \\beta_1 t_i + \\beta_2 z_i$.\n    - True parameters: $\\beta_0 = \\log(2.0)$, $\\beta_2 = 0.02$. $\\beta_1$ is specified per test case.\n    - Overdispersion mechanism: $g_i \\sim \\text{Gamma}(k, \\text{scale}=1/k)$, and $y_i \\mid x_i, g_i \\sim \\text{Poisson}(\\mu_i g_i)$. The resulting marginal variance is $\\operatorname{Var}(y_i \\mid x_i) = \\mu_i + \\mu_i^2/k$.\n- **Algorithmic Requirements**:\n    - Derivations from the Poisson log-likelihood (score, Hessian).\n    - Newton–Raphson implementation with step halving for monotone log-likelihood ascent.\n    - Dispersion parameter $\\widehat{\\phi}$ estimated using the Pearson method with $V(\\mu)=\\mu$.\n- **Hypothesis Test**: Two-sided test for $H_0: \\beta_1 = 0$ at $\\alpha = 0.05$.\n- **Test Cases**:\n    1. $n=400, k=100, \\beta_1=0.25$, seed $12345$.\n    2. $n=400, k=5, \\beta_1=0.25$, seed $12345$.\n    3. $n=80, k=2, \\beta_1=0.15$, seed $24680$.\n- **Required Outputs (per case)**: $[\\widehat{\\beta}_1, \\text{SE}_{\\text{model}}, \\text{SE}_{\\text{quasi}}, \\text{SE}_{\\text{robust}}, p_{\\text{model}}, p_{\\text{robust}}, \\text{changed}]$. `changed` is a boolean indicating if rejection decisions differ at $\\alpha=0.05$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is located in the core of statistical theory, specifically concerning Generalized Linear Models, maximum likelihood estimation, and robust inference. The concepts of overdispersion, quasi-likelihood, and sandwich estimators are well-established and fundamental to modern applied statistics.\n- **Well-Posed**: The problem specifies a clear data-generating process, a well-defined model to be fitted, and concrete numerical methods for estimation and inference. The objectives are quantifiable and lead to a unique set of numerical results for each test case.\n- **Objective**: The problem is stated in precise mathematical and statistical language, free of ambiguity or subjective claims.\n\nThe problem does not violate any of the invalidity criteria. It is a standard, albeit comprehensive, exercise in statistical computation and theory.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\n### Principle-Based Solution\n\nThe objective is to estimate the parameters of a Poisson generalized linear model (GLM) and compare inference for a specific coefficient using three different methods for calculating standard errors.\n\n**1. Model Specification and Likelihood**\n\nThe response variable $y_i$ is assumed to follow a Poisson distribution for the purpose of constructing the likelihood function. The probability mass function is $P(Y_i=y_i) = \\frac{e^{-\\mu_i}\\mu_i^{y_i}}{y_i!}$. The log-likelihood for a single observation $i$ is:\n$$\n\\ell_i(\\beta) = \\log(P(Y_i=y_i)) = y_i \\log(\\mu_i) - \\mu_i - \\log(y_i!)\n$$\nThe model uses a log link function, so the mean $\\mu_i$ is related to the linear predictor $\\eta_i = x_i^\\top \\beta$ by $\\mu_i = \\exp(\\eta_i)$. Substituting this into the log-likelihood gives:\n$$\n\\ell_i(\\beta) = y_i (x_i^\\top \\beta) - \\exp(x_i^\\top \\beta) - \\log(y_i!)\n$$\nThe total log-likelihood for $n$ independent observations is $L(\\beta) = \\sum_{i=1}^n \\ell_i(\\beta)$.\n\n**2. Maximum Likelihood Estimation via Newton–Raphson**\n\nTo find the maximum likelihood estimate (MLE) $\\widehat{\\beta}$, we solve $\\frac{\\partial L(\\beta)}{\\partial \\beta} = 0$. The Newton–Raphson algorithm provides an iterative method for this, with the update rule:\n$$\n\\beta^{(t+1)} = \\beta^{(t)} - [H(\\beta^{(t)})]^{-1} U(\\beta^{(t)})\n$$\nwhere $U(\\beta)$ is the score vector (gradient of $L(\\beta)$) and $H(\\beta)$ is the Hessian matrix (second derivative of $L(\\beta)$).\n\nThe score contribution from observation $i$ is:\n$$\nU_i(\\beta) = \\frac{\\partial \\ell_i}{\\partial \\beta} = \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta} = (y_i - e^{\\eta_i}) x_i = (y_i - \\mu_i) x_i\n$$\nThe total score vector is $U(\\beta) = \\sum_{i=1}^n (y_i - \\mu_i) x_i = \\mathbf{X}^\\top (y - \\mu)$, where $\\mathbf{X}$ is the $n \\times p$ design matrix, $y$ is the $n \\times 1$ vector of outcomes, and $\\mu$ is the $n \\times 1$ vector of means.\n\nThe Hessian contribution from observation $i$ is:\n$$\nH_i(\\beta) = \\frac{\\partial^2 \\ell_i}{\\partial \\beta \\partial \\beta^\\top} = \\frac{\\partial}{\\partial \\beta^\\top} [(y_i - e^{x_i^\\top \\beta}) x_i] = -e^{x_i^\\top \\beta} x_i x_i^\\top = -\\mu_i x_i x_i^\\top\n$$\nThe total Hessian matrix is $H(\\beta) = \\sum_{i=1}^n H_i(\\beta) = -\\sum_{i=1}^n \\mu_i x_i x_i^\\top = -\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$, where $\\mathbf{W}$ is a diagonal matrix with elements $W_{ii} = \\mu_i$.\n\nThe expected Fisher information matrix, $I(\\beta)$, is defined as $I(\\beta) = -\\mathbb{E}[H(\\beta)]$. For the canonical link Poisson model, the observed information $-H(\\beta)$ and the expected information $I(\\beta)$ are identical:\n$$\nI(\\beta) = -H(\\beta) = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\n$$\nThe Newton–Raphson update becomes:\n$$\n\\beta^{(t+1)} = \\beta^{(t)} + (\\mathbf{X}^\\top \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^\\top (y - \\mu^{(t)})\n$$\nThis is the core iteration of the Iteratively Reweighted Least Squares (IRLS) algorithm. To ensure convergence, step halving is used to guarantee that the log-likelihood increases at each step.\n\n**3. Standard Error Estimation**\n\nOnce the MLE $\\widehat{\\beta}$ is found, we estimate its covariance matrix.\n\n**a. Model-Based Standard Errors**\nUnder the assumption that the Poisson model is correctly specified (i.e., $\\operatorname{Var}(y_i) = \\mu_i$), the asymptotic covariance matrix of $\\widehat{\\beta}$ is the inverse of the Fisher information matrix evaluated at the MLE:\n$$\n\\text{Cov}_{\\text{model}}(\\widehat{\\beta}) = [I(\\widehat{\\beta})]^{-1} = (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1}\n$$\nwhere $\\widehat{W}_{ii} = \\widehat{\\mu}_i = \\exp(x_i^\\top \\widehat{\\beta})$. The model-based standard error for a coefficient $\\widehat{\\beta}_j$ is the square root of the $j$-th diagonal element of this matrix.\n\n**b. Quasi-Likelihood Standard Errors**\nThe quasi-likelihood approach acknowledges that the variance function may not be correctly specified. It assumes $\\operatorname{Var}(y_i) = \\phi V(\\mu_i)$, where $\\phi$ is a constant dispersion parameter. For this problem, we use the Poisson variance function $V(\\mu_i) = \\mu_i$. The parameter $\\phi$ is estimated using the Pearson chi-squared statistic:\n$$\n\\widehat{\\phi} = \\frac{1}{n-p} \\sum_{i=1}^n \\frac{(y_i - \\widehat{\\mu}_i)^2}{V(\\widehat{\\mu}_i)} = \\frac{1}{n-p} \\sum_{i=1}^n \\frac{(y_i - \\widehat{\\mu}_i)^2}{\\widehat{\\mu}_i}\n$$\nThe quasi-likelihood covariance matrix is then a scaled version of the model-based one:\n$$\n\\text{Cov}_{\\text{quasi}}(\\widehat{\\beta}) = \\widehat{\\phi} \\cdot \\text{Cov}_{\\text{model}}(\\widehat{\\beta}) = \\widehat{\\phi} (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1}\n$$\nIf the data are overdispersed ($\\widehat{\\phi} > 1$), these standard errors will be larger than the model-based ones.\n\n**c. Robust (Sandwich) Standard Errors**\nThe Huber–White robust estimator provides a consistent estimate of the covariance matrix even if the variance function $V(\\mu)$ is misspecified. Its \"sandwich\" form is given by:\n$$\n\\text{Cov}_{\\text{robust}}(\\widehat{\\beta}) = \\mathcal{I}^{-1} \\mathcal{J} \\mathcal{I}^{-1}\n$$\nwhere $\\mathcal{I}$ is the Fisher information (the \"bread\") and $\\mathcal{J}$ is the variance of the score function (the \"meat\"). These are estimated as:\n- $\\widehat{\\mathcal{I}} = \\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X}$\n- $\\widehat{\\mathcal{J}} = \\sum_{i=1}^n U_i(\\widehat{\\beta}) U_i(\\widehat{\\beta})^\\top = \\sum_{i=1}^n (y_i - \\widehat{\\mu}_i)^2 x_i x_i^\\top$\nThe robust covariance estimator is therefore:\n$$\n\\text{Cov}_{\\text{robust}}(\\widehat{\\beta}) = (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1} \\left( \\sum_{i=1}^n (y_i - \\widehat{\\mu}_i)^2 x_i x_i^\\top \\right) (\\mathbf{X}^\\top \\widehat{\\mathbf{W}} \\mathbf{X})^{-1}\n$$\nThis estimator is robust because it uses the empirical residuals $(y_i - \\widehat{\\mu}_i)$ to estimate the true variance of the score contributions, rather than relying on the model assumption $\\operatorname{Var}(y_i) = \\mu_i$.\n\n**4. Hypothesis Testing**\n\nFor each type of standard error (SE), a two-sided Wald test is performed for the null hypothesis $H_0: \\beta_1 = 0$. The test statistic is $Z = \\frac{\\widehat{\\beta}_1}{\\text{SE}(\\widehat{\\beta}_1)}$. Under the null hypothesis, $Z$ asymptotically follows a standard normal distribution, $\\mathcal{N}(0,1)$. The $p$-value is calculated as $p = 2 \\cdot (1 - \\Phi(|Z|))$, where $\\Phi$ is the cumulative distribution function of the standard normal distribution. The null hypothesis is rejected at significance level $\\alpha=0.05$ if $p < 0.05$. The implementation will compare the rejection decisions based on model-based vs. robust standard errors.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (near-Poisson)\n        {\"n\": 400, \"k\": 100, \"beta1\": 0.25, \"seed\": 12345},\n        # Case 2 (moderate overdispersion)\n        {\"n\": 400, \"k\": 5, \"beta1\": 0.25, \"seed\": 12345},\n        # Case 3 (small sample, severe overdispersion)\n        {\"n\": 80, \"k\": 2, \"beta1\": 0.15, \"seed\": 24680},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = _solve_single_case(**case)\n        all_results.append(result)\n        \n    # Format and print the final output as a single-line string\n    # that represents a list of lists.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef _generate_data(n, k, beta1, seed):\n    \"\"\"\n    Generates data according to the problem specification.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # True coefficients\n    beta_true = np.array([np.log(2.0), beta1, 0.02])\n    \n    # Generate covariates\n    t = rng.binomial(1, 0.5, size=n)\n    a = rng.uniform(20, 80, size=n)\n    z = (a - 50) / 10\n    \n    # Design matrix (intercept, treatment, scaled age)\n    X = np.c_[np.ones(n), t, z]\n    \n    # Generate outcome y\n    eta = X @ beta_true\n    mu = np.exp(eta)\n    \n    # Induce overdispersion via Gamma-Poisson mixture\n    g = rng.gamma(k, scale=1.0/k, size=n)\n    y = rng.poisson(mu * g)\n    \n    return X, y\n\n\ndef _fit_poisson_glm(X, y, tol=1e-8, max_iter=100, step_halving_max=10):\n    \"\"\"\n    Fits a Poisson GLM with log link using Newton-Raphson (IRLS).\n    \"\"\"\n    n_obs, n_params = X.shape\n    beta = np.zeros(n_params)\n    \n    # Initial log-likelihood (ignoring constant term)\n    eta = X @ beta\n    mu = np.exp(eta)\n    ll_old = np.sum(y * eta - mu)\n\n    for i in range(max_iter):\n        eta = X @ beta\n        mu = np.exp(eta)\n\n        # Score vector and Fisher information matrix\n        score = X.T @ (y - mu)\n        W = np.diag(mu)\n        fisher_info = X.T @ W @ X\n\n        try:\n            # Fisher info should be invertible if X is full rank\n            inv_fisher_info = np.linalg.inv(fisher_info)\n            delta_beta = inv_fisher_info @ score\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if singular\n            inv_fisher_info = np.linalg.pinv(fisher_info)\n            delta_beta = inv_fisher_info @ score\n\n        # Step halving to ensure monotone increase in log-likelihood\n        alpha = 1.0\n        beta_new = beta\n        ll_new = -np.inf\n        for _ in range(step_halving_max):\n            beta_try = beta + alpha * delta_beta\n            eta_new = X @ beta_try\n            \n            # Check for numerical overflow before exponentiating\n            if np.any(eta_new > 50):\n                alpha /= 2\n                continue\n                \n            mu_new = np.exp(eta_new)\n            ll_try = np.sum(y * eta_new - mu_new)\n\n            if ll_try > ll_old:\n                ll_new = ll_try\n                beta_new = beta_try\n                break\n            alpha /= 2\n        \n        if ll_new == -np.inf: # No improvement found\n            break\n\n        # Check for convergence\n        if np.linalg.norm(beta_new - beta)  tol:\n            beta = beta_new\n            break\n        \n        beta = beta_new\n        ll_old = ll_new\n    \n    # Recalculate final quantities at converged beta\n    eta_hat = X @ beta\n    mu_hat = np.exp(eta_hat)\n    W_hat = np.diag(mu_hat)\n    final_fisher_info = X.T @ W_hat @ X\n    \n    return beta, mu_hat, final_fisher_info\n\n\ndef _solve_single_case(n, k, beta1, seed):\n    \"\"\"\n    Generates data, fits model, computes results for one test case.\n    \"\"\"\n    # 1. Generate data\n    X, y = _generate_data(n, k, beta1, seed)\n    n_obs, n_params = X.shape\n\n    # 2. Fit Poisson GLM to get beta_hat and ancillary quantities\n    beta_hat, mu_hat, fisher_info = _fit_poisson_glm(X, y)\n\n    # 3. Calculate all three types of standard errors for beta_1\n    \n    # 3a. Model-based SE\n    try:\n        cov_model = np.linalg.inv(fisher_info)\n        se_model_vec = np.sqrt(np.diag(cov_model))\n        se_model = se_model_vec[1]\n    except np.linalg.LinAlgError:\n        # Should not happen with well-behaved data, but as a fallback\n        cov_model = np.linalg.pinv(fisher_info)\n        se_model_vec = np.sqrt(np.diag(cov_model))\n        se_model = se_model_vec[1]\n\n    # 3b. Quasi-likelihood SE\n    pearson_chi2 = np.sum((y - mu_hat)**2 / mu_hat)\n    phi_hat = pearson_chi2 / (n_obs - n_params)\n    cov_quasi = phi_hat * cov_model\n    se_quasi = np.sqrt(cov_quasi[1, 1])\n\n    # 3c. Robust (Sandwich) SE\n    # Bread\n    bread = cov_model\n    # Meat\n    meat = np.zeros((n_params, n_params))\n    residuals = y - mu_hat\n    for i in range(n_obs):\n        meat += np.outer(X[i, :], X[i, :]) * residuals[i]**2\n    \n    cov_robust = bread @ meat @ bread\n    se_robust = np.sqrt(cov_robust[1, 1])\n\n    # 4. Perform hypothesis tests\n    beta1_hat = beta_hat[1]\n    \n    # Model-based test\n    z_model = beta1_hat / se_model\n    p_model = 2 * norm.sf(np.abs(z_model))\n    \n    # Robust test\n    z_robust = beta1_hat / se_robust\n    p_robust = 2 * norm.sf(np.abs(z_robust))\n\n    # 5. Compare rejection decisions\n    reject_model = p_model  0.05\n    reject_robust = p_robust  0.05\n    changed = reject_model != reject_robust\n\n    # 6. Format results for this case\n    return [\n        beta1_hat,\n        se_model,\n        se_quasi,\n        se_robust,\n        p_model,\n        p_robust,\n        bool(changed) # Ensure it's a Python boolean for proper str() conversion\n    ]\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}