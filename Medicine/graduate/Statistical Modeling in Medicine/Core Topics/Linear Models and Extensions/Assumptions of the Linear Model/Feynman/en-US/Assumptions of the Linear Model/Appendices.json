{
    "hands_on_practices": [
        {
            "introduction": "In medical research, many variables of interest, such as long-term dietary intake or environmental exposure, are impossible to measure with perfect accuracy. This exercise explores the consequences of this reality by examining the classical measurement error model, where a predictor is observed with noise. By deriving the attenuation bias, you will gain a fundamental understanding of how this common violation of linear model assumptions can systematically weaken the estimated relationship between an exposure and an outcome, a critical concept for interpreting the results of clinical and epidemiological studies .",
            "id": "4952742",
            "problem": "A clinical cohort study aims to relate a continuous clinical outcome, such as systolic blood pressure, denoted by $Y$, to a continuous true exposure, such as average daily sodium intake, denoted by $X$. The scientific model posits a linear relationship $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$, where $\\varepsilon$ is a mean-zero random error with $\\operatorname{E}[\\varepsilon \\mid X] = 0$ and finite variance. However, the exposure $X$ is not directly observed; instead, a noisy biomarker $W$ is recorded that follows a classical measurement error model $W = X + U$, where $U$ is a mean-zero measurement error independent of $X$ and $\\varepsilon$, with finite variance. Assume an independent and identically distributed (i.i.d.) sample $\\{(Y_{i}, W_{i})\\}_{i=1}^{n}$ is used to fit the working linear regression model $Y$ on $W$ by ordinary least squares (OLS).\n\nStarting from core definitions of covariance and variance, and the definition of the OLS slope estimator in a simple linear regression, derive the expected OLS slope when regressing $Y$ on $W$ under the stated assumptions, and from it derive the attenuation bias of this slope relative to the true $\\beta_{1}$. Define the reliability ratio $R$ by $R = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(W)}$. Provide the final closed-form analytic expression for the attenuation bias, expressed solely in terms of $R$ and $\\beta_{1}$.\n\nYour final answer must be a single closed-form analytic expression. Do not provide numerical approximations.",
            "solution": "The objective is to derive the attenuation bias of the ordinary least squares (OLS) slope estimator when regressing a continuous outcome $Y$ on a noisy predictor $W$, where $W$ is a measurement of the true predictor $X$. The bias is the difference between the expected value of the estimator and the true parameter $\\beta_{1}$.\n\nThe true underlying scientific model is specified as:\n$$Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$$\nwhere $\\operatorname{E}[\\varepsilon \\mid X] = 0$. This condition implies that $\\varepsilon$ is uncorrelated with $X$, as $\\operatorname{Cov}(X, \\varepsilon) = \\operatorname{E}[X\\varepsilon] - \\operatorname{E}[X]\\operatorname{E}[\\varepsilon]$. From the law of total expectation, $\\operatorname{E}[\\varepsilon] = \\operatorname{E}[\\operatorname{E}[\\varepsilon \\mid X]] = \\operatorname{E}[0] = 0$. Also, $\\operatorname{E}[X\\varepsilon] = \\operatorname{E}[\\operatorname{E}[X\\varepsilon \\mid X]] = \\operatorname{E}[X\\operatorname{E}[\\varepsilon \\mid X]] = \\operatorname{E}[X \\cdot 0] = 0$. Therefore, $\\operatorname{Cov}(X, \\varepsilon) = 0$.\n\nThe true predictor $X$ is not observed. Instead, we observe $W$ from a classical measurement error model:\n$$W = X + U$$\nwhere the measurement error $U$ has a mean of zero, $\\operatorname{E}[U]=0$, and is independent of both $X$ and $\\varepsilon$.\n\nWe are fitting a simple linear regression model of $Y$ on the observed data $W$ using OLS. The OLS slope estimator, which we denote as $\\hat{\\beta}_{1,W}$, is given by $\\hat{\\beta}_{1,W} = \\frac{\\sum_{i=1}^{n} (W_i - \\bar{W})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (W_i - \\bar{W})^2}$. Under the assumption of an independent and identically distributed (i.i.d.) sample, by the Law of Large Numbers, this estimator converges in probability to the ratio of the population covariance to the population variance:\n$$ \\text{plim}_{n \\to \\infty} \\hat{\\beta}_{1,W} = \\frac{\\operatorname{Cov}(Y, W)}{\\operatorname{Var}(W)} $$\nThis probability limit is the quantity to which our estimator is consistent and represents the expected value of the estimator in large samples. We will compute this quantity to determine the asymptotic bias.\n\nFirst, we derive the covariance term, $\\operatorname{Cov}(Y, W)$. We substitute the expressions for $Y$ and $W$:\n$$ \\operatorname{Cov}(Y, W) = \\operatorname{Cov}(\\beta_{0} + \\beta_{1} X + \\varepsilon, X + U) $$\nUsing the bilinearity property of covariance, we can expand this expression:\n$$ \\operatorname{Cov}(Y, W) = \\operatorname{Cov}(\\beta_{0}, X) + \\operatorname{Cov}(\\beta_{0}, U) + \\operatorname{Cov}(\\beta_{1} X, X) + \\operatorname{Cov}(\\beta_{1} X, U) + \\operatorname{Cov}(\\varepsilon, X) + \\operatorname{Cov}(\\varepsilon, U) $$\nWe evaluate each term based on the problem's assumptions:\n1.  $\\operatorname{Cov}(\\beta_{0}, X) = 0$ and $\\operatorname{Cov}(\\beta_{0}, U) = 0$, as the covariance of a constant with any random variable is zero.\n2.  $\\operatorname{Cov}(\\beta_{1} X, X) = \\beta_{1} \\operatorname{Cov}(X, X) = \\beta_{1} \\operatorname{Var}(X)$.\n3.  $\\operatorname{Cov}(\\beta_{1} X, U) = \\beta_{1} \\operatorname{Cov}(X, U)$. Since $X$ and $U$ are stated to be independent, their covariance is zero: $\\operatorname{Cov}(X, U) = 0$.\n4.  $\\operatorname{Cov}(\\varepsilon, X) = 0$, which follows from the assumption $\\operatorname{E}[\\varepsilon \\mid X] = 0$, as shown earlier.\n5.  $\\operatorname{Cov}(\\varepsilon, U) = 0$, as $\\varepsilon$ and $U$ are stated to be independent.\n\nSumming the non-zero terms, we find the covariance:\n$$ \\operatorname{Cov}(Y, W) = \\beta_{1} \\operatorname{Var}(X) $$\n\nNext, we derive the variance term, $\\operatorname{Var}(W)$. Using the model $W = X + U$:\n$$ \\operatorname{Var}(W) = \\operatorname{Var}(X + U) $$\nSince $X$ and $U$ are independent, the variance of their sum is the sum of their variances:\n$$ \\operatorname{Var}(W) = \\operatorname{Var}(X) + \\operatorname{Var}(U) $$\n\nNow, we can write the probability limit of the OLS slope estimator:\n$$ \\text{plim}_{n \\to \\infty} \\hat{\\beta}_{1,W} = \\frac{\\beta_{1} \\operatorname{Var}(X)}{\\operatorname{Var}(X) + \\operatorname{Var}(U)} $$\nThis expression represents the value the OLS slope converges to. We can see that unless $\\operatorname{Var}(U) = 0$ (i.e., no measurement error), this value is not equal to the true parameter $\\beta_{1}$.\n\nThe asymptotic bias of the estimator is the difference between its probability limit and the true parameter value $\\beta_{1}$:\n$$ \\text{Bias}(\\hat{\\beta}_{1,W}) = (\\text{plim}_{n \\to \\infty} \\hat{\\beta}_{1,W}) - \\beta_{1} $$\n$$ \\text{Bias}(\\hat{\\beta}_{1,W}) = \\frac{\\beta_{1} \\operatorname{Var}(X)}{\\operatorname{Var}(X) + \\operatorname{Var}(U)} - \\beta_{1} $$\nFactoring out $\\beta_{1}$:\n$$ \\text{Bias}(\\hat{\\beta}_{1,W}) = \\beta_{1} \\left( \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(X) + \\operatorname{Var}(U)} - 1 \\right) $$\n$$ \\text{Bias}(\\hat{\\beta}_{1,W}) = \\beta_{1} \\left( \\frac{\\operatorname{Var}(X) - (\\operatorname{Var}(X) + \\operatorname{Var}(U))}{\\operatorname{Var}(X) + \\operatorname{Var}(U)} \\right) $$\n$$ \\text{Bias}(\\hat{\\beta}_{1,W}) = \\beta_{1} \\left( \\frac{-\\operatorname{Var}(U)}{\\operatorname{Var}(X) + \\operatorname{Var}(U)} \\right) $$\nThis expression is the attenuation bias. Because variances are non-negative, the factor multiplying $\\beta_{1}$ is between $-1$ and $0$ (assuming $\\operatorname{Var}(U) > 0$). This biases the estimate towards zero, a phenomenon known as attenuation or regression dilution.\n\nFinally, we express this bias in terms of the reliability ratio $R$, which is defined as $R = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(W)}$. Substituting $\\operatorname{Var}(W) = \\operatorname{Var}(X) + \\operatorname{Var}(U)$, we have:\n$$ R = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(X) + \\operatorname{Var}(U)} $$\nNotice that the probability limit of the estimator can be written as:\n$$ \\text{plim}_{n \\to \\infty} \\hat{\\beta}_{1,W} = \\beta_{1} \\left( \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(X) + \\operatorname{Var}(U)} \\right) = \\beta_{1} R $$\nThe attenuation bias is therefore:\n$$ \\text{Bias}(\\hat{\\beta}_{1,W}) = \\beta_{1} R - \\beta_{1} $$\nFactoring out $\\beta_{1}$ yields the final expression for the attenuation bias solely in terms of $R$ and $\\beta_{1}$:\n$$ \\text{Bias}(\\hat{\\beta}_{1,W}) = \\beta_{1}(R - 1) $$",
            "answer": "$$\\boxed{\\beta_{1}(R-1)}$$"
        },
        {
            "introduction": "When building multivariable models in medicine, it is common for predictors to be correlated; for example, several biomarkers may reflect overlapping physiological processes. This practice delves into the issue of multicollinearity, which pushes the design matrix $X$ towards a state of linear dependence. Through a formal derivation connecting the variance of parameter estimates to the geometric properties of the design matrix, you will discover precisely how collinearity can inflate the uncertainty of your model's coefficients, making it difficult to disentangle the independent effect of any single predictor .",
            "id": "4952774",
            "problem": "Consider a cohort study in hypertension with $n$ adult patients, where the outcome vector $y \\in \\mathbb{R}^{n}$ is modeled linearly by $p$ clinically relevant covariates collected in the design matrix $X \\in \\mathbb{R}^{n \\times p}$. Assume the Gauss–Markov conditions: the true model is $y = X \\beta + \\varepsilon$ with $\\mathbb{E}(\\varepsilon) = 0$ and $\\operatorname{Var}(\\varepsilon) = \\sigma^{2} I_{n}$, and the columns of $X$ are linearly independent. Let $\\hat{\\beta}$ denote the Ordinary Least Squares (OLS) estimator, defined as the minimizer of the sum of squared residuals.\n\nIn medical datasets, near collinearity among predictors (for example, among laboratory biomarkers that measure overlapping physiological processes) can inflate the uncertainty in estimated effects. To formalize this, let $s_{1} \\geq s_{2} \\geq \\dots \\geq s_{p} > 0$ be the singular values of $X$ from its Singular Value Decomposition (SVD), and define the Euclidean-norm condition number of $X$ as $\\kappa(X) = \\frac{s_{1}}{s_{p}}$. For any contrast vector $c \\in \\mathbb{R}^{p}$ with $\\|c\\|_{2} = 1$, the sampling variance of the contrast $c^{\\top}\\hat{\\beta}$ depends on the geometry of $X$.\n\nStarting strictly from the stated linear model assumptions and basic definitions (without invoking any pre-stated estimator variance formulas), derive the expression for the maximum-to-minimum ratio, over all unit-norm contrasts $c$, of the sampling variance $\\operatorname{Var}(c^{\\top}\\hat{\\beta})$. Express your final answer solely as a closed-form analytic expression in terms of $\\kappa(X)$, and provide no numerical approximation. No physical units are required for the final expression.",
            "solution": "The objective is to find the ratio $\\frac{\\max_{\\|c\\|_{2}=1} \\operatorname{Var}(c^{\\top} \\hat{\\beta})}{\\min_{\\|c\\|_{2}=1} \\operatorname{Var}(c^{\\top} \\hat{\\beta})}$. We will derive this from first principles as requested.\n\nFirst, we determine the expression for the OLS estimator $\\hat{\\beta}$. The estimator $\\hat{\\beta}$ is defined as the value of $\\beta$ that minimizes the sum of squared residuals, $S(\\beta)$:\n$$ S(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta) = y^{\\top}y - 2y^{\\top}X\\beta + \\beta^{\\top}X^{\\top}X\\beta $$\nTo find the minimum, we compute the gradient of $S(\\beta)$ with respect to $\\beta$ and set it to the zero vector:\n$$ \\nabla_{\\beta} S(\\beta) = -2X^{\\top}y + 2X^{\\top}X\\beta = 0 $$\nThis yields the normal equations:\n$$ X^{\\top}X\\beta = X^{\\top}y $$\nSince the columns of $X$ are linearly independent, the matrix $X^{\\top}X$ is a $p \\times p$ positive definite matrix and is therefore invertible. Solving for $\\beta$ gives the unique OLS estimator:\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y $$\n\nNext, we derive the sampling variance of $\\hat{\\beta}$. Using the properties of the variance operator and the fact that $\\beta$ is a fixed (non-random) vector:\n$$ \\operatorname{Var}(\\hat{\\beta}) = \\operatorname{Var}((X^{\\top}X)^{-1}X^{\\top}y) = (X^{\\top}X)^{-1}X^{\\top} \\operatorname{Var}(y) (X^{\\top}(X^{\\top}X)^{-1}) $$\nThe variance of $y$ is $\\operatorname{Var}(y) = \\operatorname{Var}(X\\beta + \\varepsilon) = \\operatorname{Var}(\\varepsilon)$. From the problem statement, $\\operatorname{Var}(\\varepsilon) = \\sigma^{2}I_{n}$. So, $\\operatorname{Var}(y) = \\sigma^{2}I_{n}$. Substituting this into the expression for $\\operatorname{Var}(\\hat{\\beta})$:\n$$ \\operatorname{Var}(\\hat{\\beta}) = (X^{\\top}X)^{-1}X^{\\top} (\\sigma^{2}I_{n}) X((X^{\\top}X)^{-1})^{\\top} $$\nSince $X^{\\top}X$ is symmetric, its inverse is also symmetric. Thus, $((X^{\\top}X)^{-1})^{\\top} = (X^{\\top}X)^{-1}$.\n$$ \\operatorname{Var}(\\hat{\\beta}) = \\sigma^{2} (X^{\\top}X)^{-1} X^{\\top}X (X^{\\top}X)^{-1} = \\sigma^{2} (X^{\\top}X)^{-1} $$\n\nNow, we find the variance of the linear combination (contrast) $c^{\\top}\\hat{\\beta}$. For a constant vector $c$ and a random vector $\\hat{\\beta}$, the variance is given by:\n$$ \\operatorname{Var}(c^{\\top}\\hat{\\beta}) = c^{\\top} \\operatorname{Var}(\\hat{\\beta}) c $$\nSubstituting the expression for $\\operatorname{Var}(\\hat{\\beta})$:\n$$ \\operatorname{Var}(c^{\\top}\\hat{\\beta}) = c^{\\top} (\\sigma^{2}(X^{\\top}X)^{-1}) c = \\sigma^{2} c^{\\top}(X^{\\top}X)^{-1}c $$\n\nThe problem reduces to finding the maximum and minimum values of the quadratic form $c^{\\top}(X^{\\top}X)^{-1}c$ subject to the constraint $\\|c\\|_{2} = 1$, which is equivalent to $c^{\\top}c=1$. This expression is a Rayleigh quotient for the matrix $(X^{\\top}X)^{-1}$. The extreme values of a Rayleigh quotient are the maximum and minimum eigenvalues of the matrix.\n\nLet the Singular Value Decomposition (SVD) of $X$ be $X = U S V^{\\top}$, where $U$ and $V$ are orthogonal matrices and $S$ is an $n \\times p$ rectangular diagonal matrix with the singular values $s_{1}, s_{2}, \\dots, s_{p}$ on its diagonal.\nThe matrix $X^{\\top}X$ can be expressed in terms of the SVD components:\n$$ X^{\\top}X = (U S V^{\\top})^{\\top}(U S V^{\\top}) = V S^{\\top} U^{\\top}U S V^{\\top} $$\nSince $U^{\\top}U = I_{n}$, this simplifies to:\n$$ X^{\\top}X = V S^{\\top}S V^{\\top} $$\nThe matrix $S^{\\top}S$ is a $p \\times p$ diagonal matrix with entries $s_{1}^{2}, s_{2}^{2}, \\dots, s_{p}^{2}$. So, $X^{\\top}X = V \\operatorname{diag}(s_{1}^{2}, s_{2}^{2}, \\dots, s_{p}^{2}) V^{\\top}$. This is the eigenvalue decomposition of $X^{\\top}X$. The eigenvalues of $X^{\\top}X$ are $\\{s_{i}^{2}\\}_{i=1}^{p}$.\n\nThe matrix of interest is $(X^{\\top}X)^{-1}$. If a matrix has eigenvalues $\\lambda_i$, its inverse has eigenvalues $1/\\lambda_i$. Therefore, the eigenvalues of $(X^{\\top}X)^{-1}$ are $\\{\\frac{1}{s_{i}^{2}}\\}_{i=1}^{p}$.\n\nGiven the ordering of singular values $s_{1} \\geq s_{2} \\geq \\dots \\geq s_{p} > 0$, the ordering of their squares is $s_{1}^{2} \\geq s_{2}^{2} \\geq \\dots \\geq s_{p}^{2} > 0$. The ordering of their reciprocals is reversed:\n$$ \\frac{1}{s_{p}^{2}} \\geq \\frac{1}{s_{p-1}^{2}} \\geq \\dots \\geq \\frac{1}{s_{1}^{2}} > 0 $$\nThe maximum and minimum eigenvalues of $(X^{\\top}X)^{-1}$ are:\n$$ \\lambda_{\\max}((X^{\\top}X)^{-1}) = \\frac{1}{s_{p}^{2}} $$\n$$ \\lambda_{\\min}((X^{\\top}X)^{-1}) = \\frac{1}{s_{1}^{2}} $$\n\nThe maximum and minimum values of the variance of the contrast are:\n$$ \\max_{\\|c\\|_{2}=1} \\operatorname{Var}(c^{\\top} \\hat{\\beta}) = \\sigma^{2} \\left( \\max_{\\|c\\|_{2}=1} c^{\\top}(X^{\\top}X)^{-1}c \\right) = \\sigma^{2} \\lambda_{\\max}((X^{\\top}X)^{-1}) = \\frac{\\sigma^{2}}{s_{p}^{2}} $$\n$$ \\min_{\\|c\\|_{2}=1} \\operatorname{Var}(c^{\\top} \\hat{\\beta}) = \\sigma^{2} \\left( \\min_{\\|c\\|_{2}=1} c^{\\top}(X^{\\top}X)^{-1}c \\right) = \\sigma^{2} \\lambda_{\\min}((X^{\\top}X)^{-1}) = \\frac{\\sigma^{2}}{s_{1}^{2}} $$\n\nFinally, the ratio of the maximum to the minimum variance is:\n$$ \\frac{\\max_{\\|c\\|_{2}=1} \\operatorname{Var}(c^{\\top} \\hat{\\beta})}{\\min_{\\|c\\|_{2}=1} \\operatorname{Var}(c^{\\top} \\hat{\\beta})} = \\frac{\\sigma^{2}/s_{p}^{2}}{\\sigma^{2}/s_{1}^{2}} = \\frac{s_{1}^{2}}{s_{p}^{2}} = \\left(\\frac{s_{1}}{s_{p}}\\right)^{2} $$\nBy definition, the condition number is $\\kappa(X) = \\frac{s_{1}}{s_{p}}$. Therefore, the ratio is equal to the square of the condition number.\n$$ \\left(\\frac{s_{1}}{s_{p}}\\right)^{2} = (\\kappa(X))^{2} $$\nThis result quantifies how collinearity, as measured by the condition number of the design matrix, creates anisotropy in the sampling uncertainty of the estimated parameters. The variance of a standardized linear combination of regression coefficients can vary by a factor of $\\kappa(X)^{2}$ depending on the direction of the contrast in parameter space.",
            "answer": "$$\n\\boxed{\\kappa(X)^{2}}\n$$"
        },
        {
            "introduction": "The overall conclusions drawn from a linear model can sometimes be disproportionately swayed by a single or a small number of unusual observations. This exercise introduces the concept of influence, a key diagnostic for assessing the stability and reliability of a regression model fit. By deriving the exact mathematical change in the coefficient estimates caused by deleting one observation, you will formalize the crucial interplay between a data point's leverage (its extremeness in predictor space) and its residual (how poorly it is fit by the model), providing a powerful tool for practical model validation .",
            "id": "4952702",
            "problem": "Consider a multicenter randomized controlled trial (RCT) in which systolic blood pressure is modeled with a linear predictor under the classical Gauss–Markov conditions: linearity of the conditional mean, full column rank of the design matrix, errors $\\varepsilon$ that are independent and identically distributed with mean $0$ and constant variance $\\sigma^{2}$, and covariates measured without error. Let the design matrix be $X \\in \\mathbb{R}^{n \\times p}$ with $p=3$ columns corresponding to an intercept, centered age, and a binary treatment indicator. The ordinary least squares estimator is defined by $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$.\n\nYou delete a single patient $j$ with row vector $x_{j} \\in \\mathbb{R}^{3}$ and observe their residual $r_{j} = y_{j} - x_{j}^{\\top}\\hat{\\beta}$. Starting from the Gauss–Markov framework and the ordinary least squares normal equations, and using only the rank-one Sherman–Morrison identity to update matrix inverses, derive the exact analytic expression for the change in coefficients due to deleting patient $j$, namely $\\hat{\\beta}_{(j)} - \\hat{\\beta}$, in terms of $x_{j}$, $(X^{\\top}X)^{-1}$, $r_{j}$, and the leverage $h_{jj} = x_{j}^{\\top}(X^{\\top}X)^{-1}x_{j}$. Explain how this expression reveals the roles of leverage and residual in standard influence measures used in medical regression diagnostics.\n\nThen, for a specific site, suppose the following quantities have been estimated on the full sample:\n$$\n(X^{\\top}X)^{-1} \\;=\\;\n\\begin{pmatrix}\n0.052 & 0 & 0 \\\\\n0 & 0.00225 & 0 \\\\\n0 & 0 & 0.008\n\\end{pmatrix},\n\\qquad\nx_{j} \\;=\\; \\begin{pmatrix} 1 \\\\ -0.3 \\\\ 1 \\end{pmatrix},\n\\qquad\nr_{j} \\;=\\; 2.1.\n$$\nCompute the numerical value of the change in the treatment-effect coefficient (the third component of $\\hat{\\beta}$) caused by deleting patient $j$. The treatment-effect coefficient is measured in millimeters of mercury (mmHg). Round your final numerical answer to four significant figures. Express the final numerical value; do not include units in your boxed answer.",
            "solution": "Let $X$ and $y$ be the full data matrices. The OLS estimator is the solution to the normal equations: $X^{\\top}X \\hat{\\beta} = X^{\\top}y$.\nLet $X_{(j)}$ and $y_{(j)}$ denote the data with the $j$-th observation removed. The new estimator, $\\hat{\\beta}_{(j)}$, is given by:\n$$ \\hat{\\beta}_{(j)} = (X_{(j)}^{\\top}X_{(j)})^{-1} X_{(j)}^{\\top}y_{(j)} $$\nThe matrices for the reduced dataset can be expressed in terms of the full-dataset matrices and the deleted observation's data, $x_j$ and $y_j$:\n$$ X^{\\top}X = \\sum_{i=1}^{n} x_i x_i^{\\top} = X_{(j)}^{\\top}X_{(j)} + x_j x_j^{\\top} \\implies X_{(j)}^{\\top}X_{(j)} = X^{\\top}X - x_j x_j^{\\top} $$\n$$ X^{\\top}y = \\sum_{i=1}^{n} x_i y_i = X_{(j)}^{\\top}y_{(j)} + x_j y_j \\implies X_{(j)}^{\\top}y_{(j)} = X^{\\top}y - x_j y_j $$\nTo find the inverse of $X_{(j)}^{\\top}X_{(j)}$, we use the Sherman–Morrison identity for a rank-$1$ update: $(A - uv^{\\top})^{-1} = A^{-1} + \\frac{A^{-1}uv^{\\top}A^{-1}}{1 - v^{\\top}A^{-1}u}$.\nLet $A = X^{\\top}X$, $u = x_j$, and $v = x_j$. The identity becomes:\n$$ (X^{\\top}X - x_j x_j^{\\top})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_j x_j^{\\top}(X^{\\top}X)^{-1}}{1 - x_j^{\\top}(X^{\\top}X)^{-1}x_j} $$\nRecognizing the leverage $h_{jj} = x_j^{\\top}(X^{\\top}X)^{-1}x_j$, we have:\n$$ (X_{(j)}^{\\top}X_{(j)})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_j x_j^{\\top}(X^{\\top}X)^{-1}}{1 - h_{jj}} $$\nNow, we substitute these expressions into the formula for $\\hat{\\beta}_{(j)}$:\n$$ \\hat{\\beta}_{(j)} = \\left[ (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_j x_j^{\\top}(X^{\\top}X)^{-1}}{1 - h_{jj}} \\right] (X^{\\top}y - x_j y_j) $$\nExpanding the product gives four terms:\n$$ \\hat{\\beta}_{(j)} = (X^{\\top}X)^{-1}(X^{\\top}y) - (X^{\\top}X)^{-1}x_j y_j + \\frac{(X^{\\top}X)^{-1}x_j x_j^{\\top}(X^{\\top}X)^{-1}(X^{\\top}y)}{1 - h_{jj}} - \\frac{(X^{\\top}X)^{-1}x_j x_j^{\\top}(X^{\\top}X)^{-1}x_j y_j}{1 - h_{jj}} $$\nWe simplify each term:\n1. $(X^{\\top}X)^{-1}(X^{\\top}y) = \\hat{\\beta}$\n2. $x_j^{\\top}(X^{\\top}X)^{-1}(X^{\\top}y) = x_j^{\\top}\\hat{\\beta} = \\hat{y}_j$ (the fitted value for observation $j$)\n3. $x_j^{\\top}(X^{\\top}X)^{-1}x_j = h_{jj}$ (the leverage of observation $j$)\n\nSubstituting these back:\n$$ \\hat{\\beta}_{(j)} = \\hat{\\beta} - (X^{\\top}X)^{-1}x_j y_j + \\frac{(X^{\\top}X)^{-1}x_j \\hat{y}_j}{1 - h_{jj}} - \\frac{(X^{\\top}X)^{-1}x_j h_{jj} y_j}{1 - h_{jj}} $$\nWe are interested in the change $\\hat{\\beta}_{(j)} - \\hat{\\beta}$:\n$$ \\hat{\\beta}_{(j)} - \\hat{\\beta} = - (X^{\\top}X)^{-1}x_j y_j + \\frac{(X^{\\top}X)^{-1}x_j (\\hat{y}_j - h_{jj} y_j)}{1 - h_{jj}} $$\nFactoring out the common vector $(X^{\\top}X)^{-1}x_j$:\n$$ \\hat{\\beta}_{(j)} - \\hat{\\beta} = (X^{\\top}X)^{-1}x_j \\left[ -y_j + \\frac{\\hat{y}_j - h_{jj} y_j}{1 - h_{jj}} \\right] $$\nCombining the terms inside the brackets over a common denominator:\n$$ \\left[ \\frac{-y_j(1 - h_{jj}) + \\hat{y}_j - h_{jj} y_j}{1 - h_{jj}} \\right] = \\left[ \\frac{-y_j + y_j h_{jj} + \\hat{y}_j - h_{jj} y_j}{1 - h_{jj}} \\right] = \\frac{\\hat{y}_j - y_j}{1 - h_{jj}} $$\nThe residual is defined as $r_j = y_j - \\hat{y}_j$, so $\\hat{y}_j - y_j = -r_j$. Thus, the expression in brackets simplifies to $\\frac{-r_j}{1 - h_{jj}}$.\nSubstituting this back yields the final expression for the change in coefficients:\n$$ \\hat{\\beta}_{(j)} - \\hat{\\beta} = -\\frac{(X^{\\top}X)^{-1}x_j r_j}{1 - h_{jj}} $$\n\n### Explanation of Influence\nThis exact expression, sometimes denoted DFBETA$_j$, quantifies the influence of observation $j$ on the estimated coefficient vector $\\hat{\\beta}$. It reveals that the influence of an observation is a function of two key quantities:\n1.  **The residual ($r_j$)**: The change is directly proportional to the residual $r_j$. An observation is influential only if it is poorly predicted by the model fitted to the full data. A point that lies far from the general trend of the data (large $|r_j|$) has the potential to be influential. If $r_j=0$, the observation has no influence on the coefficients, regardless of its other properties.\n2.  **The leverage ($h_{jj}$)**: The change is amplified by the factor $1/(1 - h_{jj})$. Leverage $h_{jj}$ measures how unusual or extreme an observation's covariate values ($x_j$) are. Since $0 \\le h_{jj} \\le 1$, the denominator $1 - h_{jj}$ is always non-negative. As $h_{jj} \\to 1$, the denominator approaches $0$, and the influence on the coefficients can become arbitrarily large. Therefore, points with high leverage (covariate outliers) have a much greater potential to be influential.\n\nIn summary, an observation is influential if it has a large residual (it is an outlier in the response dimension) and/or high leverage (it is an outlier in the predictor space). The derived formula shows that the greatest influence occurs when an observation has both high leverage and a large residual.\n\n### Numerical Calculation\nWe need to compute the change in the third coefficient (treatment effect), which we denote as $(\\hat{\\beta}_{(j)} - \\hat{\\beta})_3$. The full vector of changes is:\n$$ \\hat{\\beta}_{(j)} - \\hat{\\beta} = -\\frac{r_j}{1 - h_{jj}} (X^{\\top}X)^{-1}x_j $$\nFirst, we compute the leverage $h_{jj}$:\n$$ h_{jj} = x_{j}^{\\top}(X^{\\top}X)^{-1}x_{j} $$\n$$ h_{jj} = \\begin{pmatrix} 1 & -0.3 & 1 \\end{pmatrix} \\begin{pmatrix} 0.052 & 0 & 0 \\\\ 0 & 0.00225 & 0 \\\\ 0 & 0 & 0.008 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -0.3 \\\\ 1 \\end{pmatrix} $$\nSince $(X^{\\top}X)^{-1}$ is diagonal, the calculation simplifies:\n$$ h_{jj} = (1)^2(0.052) + (-0.3)^2(0.00225) + (1)^2(0.008) $$\n$$ h_{jj} = 0.052 + (0.09)(0.00225) + 0.008 $$\n$$ h_{jj} = 0.052 + 0.0002025 + 0.008 = 0.0602025 $$\nNext, we compute the vector $(X^{\\top}X)^{-1}x_j$:\n$$ (X^{\\top}X)^{-1}x_j = \\begin{pmatrix} 0.052 & 0 & 0 \\\\ 0 & 0.00225 & 0 \\\\ 0 & 0 & 0.008 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -0.3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.052 \\times 1 \\\\ 0.00225 \\times (-0.3) \\\\ 0.008 \\times 1 \\end{pmatrix} = \\begin{pmatrix} 0.052 \\\\ -0.000675 \\\\ 0.008 \\end{pmatrix} $$\nThe third component of this vector is $0.008$.\nNow, we can compute the change in the third coefficient using the given $r_j = 2.1$:\n$$ (\\hat{\\beta}_{(j)} - \\hat{\\beta})_3 = -\\frac{r_j}{1 - h_{jj}} \\times \\left( (X^{\\top}X)^{-1}x_j \\right)_3 $$\n$$ (\\hat{\\beta}_{(j)} - \\hat{\\beta})_3 = -\\frac{2.1}{1 - 0.0602025} \\times 0.008 $$\n$$ (\\hat{\\beta}_{(j)} - \\hat{\\beta})_3 = -\\frac{2.1 \\times 0.008}{0.9397975} = -\\frac{0.0168}{0.9397975} $$\n$$ (\\hat{\\beta}_{(j)} - \\hat{\\beta})_3 \\approx -0.01787618... $$\nRounding to four significant figures, we get $-0.01788$.",
            "answer": "$$\\boxed{-0.01788}$$"
        }
    ]
}