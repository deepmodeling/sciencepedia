{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds our understanding of goodness-of-fit in a classic and intuitive method: the Pearson chi-squared statistic. You will apply this test to grouped binomial data, a common scenario in multi-center studies, by calculating the standardized difference between observed and model-expected event counts. This exercise builds foundational skills in evaluating how well a model's predictions align with aggregated outcomes .",
            "id": "4965716",
            "problem": "A multi-center surgical outcomes study evaluates the probability of a postoperative infection within $30$ days. At each center $j$, investigators record $m_j$ patients and $y_j$ infections. A Generalized Linear Model (GLM) with binomial variance function and a logistic link is fitted to the grouped-binomial data, using a standardized comorbidity index $x_j$ as the single covariate. The fitted linear predictor is $g(\\pi_j) = \\log\\!\\left(\\frac{\\pi_j}{1-\\pi_j}\\right) = \\beta_0 + \\beta_1 x_j$, where $\\pi_j$ is the probability of infection for center $j$. The maximum likelihood estimates are $\\hat{\\beta}_0 = \\ln\\!\\left(\\frac{0.2}{0.8}\\right)$ and $\\hat{\\beta}_1 = \\ln(2)$. The five centers have covariates, sample sizes, and observed infections:\n- $j=1$: $x_1 = 0$, $m_1 = 50$, $y_1 = 12$.\n- $j=2$: $x_2 = 1$, $m_2 = 60$, $y_2 = 18$.\n- $j=3$: $x_3 = 2$, $m_3 = 80$, $y_3 = 42$.\n- $j=4$: $x_4 = 3$, $m_4 = 90$, $y_4 = 55$.\n- $j=5$: $x_5 = 4$, $m_5 = 100$, $y_5 = 78$.\n\nStarting from the binomial model for grouped outcomes, derive the appropriate Pearson residual for center $j$ based on the mean and variance of the binomial distribution under the fitted model, and use it to construct the Pearson goodness-of-fit statistic by summing the squared residuals over centers. Compute this goodness-of-fit statistic using the data above. Express the final statistic as a real number. If any intermediate numerical quantities require approximation, carry them symbolically until the final step; no rounding of the final statistic is required in this problem.",
            "solution": "The problem is valid as it is scientifically grounded in statistical theory, well-posed with sufficient data, and objectively stated.\n\nThe problem asks for the computation of the Pearson goodness-of-fit statistic for a logistic regression model fitted to grouped binomial data from five surgical centers.\n\nThe model assumes that for each center $j$, the number of observed infections, $y_j$, follows a binomial distribution, $Y_j \\sim \\text{Binomial}(m_j, \\pi_j)$, where $m_j$ is the total number of patients and $\\pi_j$ is the probability of infection. The mean and variance of $Y_j$ are given by:\n$$\nE[Y_j] = m_j \\pi_j\n$$\n$$\n\\text{Var}(Y_j) = m_j \\pi_j (1 - \\pi_j)\n$$\nThe logistic regression model connects the probability $\\pi_j$ to a linear predictor $\\eta_j = \\beta_0 + \\beta_1 x_j$ via the logit link function:\n$$\n\\eta_j = \\text{logit}(\\pi_j) = \\ln\\left(\\frac{\\pi_j}{1-\\pi_j}\\right)\n$$\nThe Pearson residual for the $j$-th group is defined as the difference between the observed count $y_j$ and the fitted count $\\hat{y}_j$, standardized by the estimated standard deviation of the observed count under the model. The fitted count is $\\hat{y}_j = m_j \\hat{\\pi}_j$, where $\\hat{\\pi}_j$ is the fitted probability for center $j$. The estimated variance is $\\widehat{\\text{Var}}(Y_j) = m_j \\hat{\\pi}_j (1 - \\hat{\\pi}_j)$.\n\nThus, the Pearson residual for center $j$, denoted $r_{P,j}$, is:\n$$\nr_{P,j} = \\frac{y_j - E[Y_j]}{\\sqrt{\\widehat{\\text{Var}}(Y_j)}} = \\frac{y_j - m_j \\hat{\\pi}_j}{\\sqrt{m_j \\hat{\\pi}_j (1 - \\hat{\\pi}_j)}}\n$$\nThe Pearson goodness-of-fit statistic, often denoted as $X^2$, is the sum of the squared Pearson residuals over all $J$ centers:\n$$\nX^2 = \\sum_{j=1}^{J} r_{P,j}^2 = \\sum_{j=1}^{J} \\frac{(y_j - m_j \\hat{\\pi}_j)^2}{m_j \\hat{\\pi}_j (1 - \\hat{\\pi}_j)}\n$$\nIn this problem, we have $J=5$ centers. The first step is to calculate the fitted probabilities $\\hat{\\pi}_j$ for each center using the provided maximum likelihood estimates for the model parameters: $\\hat{\\beta}_0 = \\ln(0.2/0.8) = \\ln(0.25)$ and $\\hat{\\beta}_1 = \\ln(2)$.\n\nThe fitted linear predictor for center $j$ is $\\hat{\\eta}_j = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_j$.\n$$\n\\hat{\\eta}_j = \\ln(0.25) + x_j \\ln(2) = \\ln(0.25 \\cdot 2^{x_j}) = \\ln(2^{-2} \\cdot 2^{x_j}) = \\ln(2^{x_j-2})\n$$\nTo find the fitted probability $\\hat{\\pi}_j$, we apply the inverse logit function:\n$$\n\\hat{\\pi}_j = \\text{logit}^{-1}(\\hat{\\eta}_j) = \\frac{\\exp(\\hat{\\eta}_j)}{1 + \\exp(\\hat{\\eta}_j)} = \\frac{\\exp(\\ln(2^{x_j-2}))}{1 + \\exp(\\ln(2^{x_j-2}))} = \\frac{2^{x_j-2}}{1 + 2^{x_j-2}}\n$$\nMultiplying the numerator and denominator by $2^2=4$ gives a more convenient form for calculation:\n$$\n\\hat{\\pi}_j = \\frac{4 \\cdot 2^{x_j-2}}{4 \\cdot (1 + 2^{x_j-2})} = \\frac{2^2 \\cdot 2^{x_j-2}}{4 + 4 \\cdot 2^{x_j-2}} = \\frac{2^{x_j}}{4 + 2^{x_j}}\n$$\nWe now compute the components for the $X^2$ statistic for each of the five centers.\n\nFor center $j=1$:\n$x_1=0$, $m_1=50$, $y_1=12$.\n$\\hat{\\pi}_1 = \\frac{2^0}{4+2^0} = \\frac{1}{5} = 0.2$.\nFitted count: $\\hat{y}_1 = m_1 \\hat{\\pi}_1 = 50 \\times 0.2 = 10$.\nEstimated variance: $m_1 \\hat{\\pi}_1 (1-\\hat{\\pi}_1) = 50 \\times 0.2 \\times (1-0.2) = 10 \\times 0.8 = 8$.\nSquared residual: $\\frac{(y_1 - \\hat{y}_1)^2}{m_1 \\hat{\\pi}_1 (1-\\hat{\\pi}_1)} = \\frac{(12-10)^2}{8} = \\frac{2^2}{8} = \\frac{4}{8} = 0.5$.\n\nFor center $j=2$:\n$x_2=1$, $m_2=60$, $y_2=18$.\n$\\hat{\\pi}_2 = \\frac{2^1}{4+2^1} = \\frac{2}{6} = \\frac{1}{3}$.\nFitted count: $\\hat{y}_2 = m_2 \\hat{\\pi}_2 = 60 \\times \\frac{1}{3} = 20$.\nEstimated variance: $m_2 \\hat{\\pi}_2 (1-\\hat{\\pi}_2) = 60 \\times \\frac{1}{3} \\times (1-\\frac{1}{3}) = 20 \\times \\frac{2}{3} = \\frac{40}{3}$.\nSquared residual: $\\frac{(y_2 - \\hat{y}_2)^2}{m_2 \\hat{\\pi}_2 (1-\\hat{\\pi}_2)} = \\frac{(18-20)^2}{40/3} = \\frac{(-2)^2}{40/3} = \\frac{4 \\times 3}{40} = \\frac{12}{40} = 0.3$.\n\nFor center $j=3$:\n$x_3=2$, $m_3=80$, $y_3=42$.\n$\\hat{\\pi}_3 = \\frac{2^2}{4+2^2} = \\frac{4}{8} = \\frac{1}{2} = 0.5$.\nFitted count: $\\hat{y}_3 = m_3 \\hat{\\pi}_3 = 80 \\times 0.5 = 40$.\nEstimated variance: $m_3 \\hat{\\pi}_3 (1-\\hat{\\pi}_3) = 80 \\times 0.5 \\times (1-0.5) = 40 \\times 0.5 = 20$.\nSquared residual: $\\frac{(y_3 - \\hat{y}_3)^2}{m_3 \\hat{\\pi}_3 (1-\\hat{\\pi}_3)} = \\frac{(42-40)^2}{20} = \\frac{2^2}{20} = \\frac{4}{20} = 0.2$.\n\nFor center $j=4$:\n$x_4=3$, $m_4=90$, $y_4=55$.\n$\\hat{\\pi}_4 = \\frac{2^3}{4+2^3} = \\frac{8}{12} = \\frac{2}{3}$.\nFitted count: $\\hat{y}_4 = m_4 \\hat{\\pi}_4 = 90 \\times \\frac{2}{3} = 60$.\nEstimated variance: $m_4 \\hat{\\pi}_4 (1-\\hat{\\pi}_4) = 90 \\times \\frac{2}{3} \\times (1-\\frac{2}{3}) = 60 \\times \\frac{1}{3} = 20$.\nSquared residual: $\\frac{(y_4 - \\hat{y}_4)^2}{m_4 \\hat{\\pi}_4 (1-\\hat{\\pi}_4)} = \\frac{(55-60)^2}{20} = \\frac{(-5)^2}{20} = \\frac{25}{20} = 1.25$.\n\nFor center $j=5$:\n$x_5=4$, $m_5=100$, $y_5=78$.\n$\\hat{\\pi}_5 = \\frac{2^4}{4+2^4} = \\frac{16}{20} = \\frac{4}{5} = 0.8$.\nFitted count: $\\hat{y}_5 = m_5 \\hat{\\pi}_5 = 100 \\times 0.8 = 80$.\nEstimated variance: $m_5 \\hat{\\pi}_5 (1-\\hat{\\pi}_5) = 100 \\times 0.8 \\times (1-0.8) = 80 \\times 0.2 = 16$.\nSquared residual: $\\frac{(y_5 - \\hat{y}_5)^2}{m_5 \\hat{\\pi}_5 (1-\\hat{\\pi}_5)} = \\frac{(78-80)^2}{16} = \\frac{(-2)^2}{16} = \\frac{4}{16} = 0.25$.\n\nFinally, the Pearson goodness-of-fit statistic $X^2$ is the sum of these squared residuals:\n$$\nX^2 = 0.5 + 0.3 + 0.2 + 1.25 + 0.25\n$$\n$$\nX^2 = (0.5 + 0.3 + 0.2) + (1.25 + 0.25) = 1.0 + 1.5 = 2.5\n$$\nThe value of the Pearson goodness-of-fit statistic is $2.5$.",
            "answer": "$$\n\\boxed{2.5}\n$$"
        },
        {
            "introduction": "A high Area Under the ROC Curve (AUC) is often praised, but does it guarantee a model is \"good\"? This practice explores the critical distinction between a model's discriminatory ability (ranking risks) and its calibration (the accuracy of its probability estimates). By analyzing a specific transformation, you will demonstrate how a model's AUC can remain unchanged while its calibration is severely distorted, a crucial insight for any practitioner building risk prediction models .",
            "id": "4965808",
            "problem": "A hospital develops a clinical decision support system to predict $30$-day mortality in patients admitted with acute decompensated heart failure using a logistic regression model. Let $Y \\in \\{0,1\\}$ denote mortality by $30$ days and let $S$ denote the model’s linear predictor so that the true conditional probability of death given $S$ is $P(Y=1 \\mid S=s) = \\sigma(s)$, where $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ is the logistic function and $\\logit(p) = \\ln\\!\\left(\\frac{p}{1-p}\\right)$ is its inverse. The model’s predicted probability is $\\hat{\\pi} = \\sigma(S)$, which is therefore perfectly calibrated by construction.\n\nAn investigator wishes to “sharpen” risk stratification without changing ranking. They report a transformed prediction $\\tilde{\\pi} = g(\\hat{\\pi})$ defined by\n$$\n\\tilde{\\pi} \\equiv \\sigma\\!\\big(a + b\\,\\logit(\\hat{\\pi})\\big),\n$$\nwith constants $a \\in \\mathbb{R}$ and $b>0$. Consider the specific choice $a = 0.4$ and $b = 2$.\n\nUsing only fundamental definitions, do the following:\n- Show that $g$ is strictly increasing on $(0,1)$ and therefore preserves the ranking of patients induced by $\\hat{\\pi}$.\n- Starting from the definition of the Area Under the Receiver Operating Characteristic curve (AUC) as $\\,\\mathrm{AUC} = P\\big(S_{1} > S_{0}\\big)$, where $S_{1}$ is the score for a randomly chosen case ($Y=1$) and $S_{0}$ is the score for a randomly chosen control ($Y=0$), argue that any strictly increasing transformation of the score preserves the AUC.\n- Derive, from first principles, the population calibration relationship between $Y$ and the transformed linear predictor $S' \\equiv \\logit(\\tilde{\\pi}) = a + b S$, that is, derive $\\logit\\!\\big(P(Y=1 \\mid S'=s')\\big)$ as a linear function of $s'$. Identify the calibration intercept and calibration slope.\n- Finally, compute the calibration slope numerically for the given $(a,b) = (0.4, 2)$.\n\nProvide the final answer as the single numerical value of the calibration slope. No rounding is required; give the exact value.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a complete derivation.\n\nThe problem requires a four-part analysis of a transformed logistic regression predictor. Let us address each part systematically. The initial, perfectly calibrated model has a linear predictor $S$, such that the true probability of the event $Y=1$ is given by $P(Y=1 \\mid S=s) = \\sigma(s)$, where $\\sigma(z) = (1+\\exp(-z))^{-1}$ is the logistic function. The model's predicted probability is $\\hat{\\pi} = \\sigma(S)$.\n\nThe investigator proposes a transformed prediction $\\tilde{\\pi} = g(\\hat{\\pi})$, where the transformation function is $g(p) = \\sigma(a + b\\,\\logit(p))$ for $p \\in (0,1)$, with constants $a \\in \\mathbb{R}$ and $b>0$.\n\n**Part 1: Show that $g$ is strictly increasing on $(0,1)$**\n\nTo demonstrate that the function $g(p)$ is strictly increasing for $p \\in (0,1)$, we must show that its first derivative, $g'(p)$, is strictly positive on this interval. We use the chain rule for differentiation.\nLet $u(p) = a + b\\,\\logit(p)$. Then $g(p) = \\sigma(u(p))$.\nThe derivative is $g'(p) = \\sigma'(u(p)) \\cdot u'(p)$.\n\nFirst, let's find the derivatives of the component functions. The derivative of the logit function, $\\logit(p) = \\ln(p) - \\ln(1-p)$, is:\n$$\n\\frac{d}{dp}\\logit(p) = \\frac{1}{p} - \\frac{-1}{1-p} = \\frac{1}{p} + \\frac{1}{1-p} = \\frac{1-p+p}{p(1-p)} = \\frac{1}{p(1-p)}\n$$\nFor any $p \\in (0,1)$, both $p$ and $1-p$ are positive, so their product $p(1-p)$ is positive. Thus, $\\frac{d}{dp}\\logit(p) > 0$ for $p \\in (0,1)$.\n\nThe derivative of the logistic function $\\sigma(z)$ is:\n$$\n\\sigma'(z) = \\frac{d}{dz}\\left(\\frac{1}{1+\\exp(-z)}\\right) = -\\frac{-\\exp(-z)}{(1+\\exp(-z))^2} = \\frac{\\exp(-z)}{(1+\\exp(-z))^2}\n$$\nThis can also be written as $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$. Since $\\sigma(z)$ maps $\\mathbb{R}$ to $(0,1)$, both $\\sigma(z)$ and $1-\\sigma(z)$ are strictly positive for any finite $z$. Therefore, $\\sigma'(z) > 0$ for all $z \\in \\mathbb{R}$.\n\nNow, we compute $u'(p)$:\n$$\nu'(p) = \\frac{d}{dp}\\big(a + b\\,\\logit(p)\\big) = b \\cdot \\frac{d}{dp}\\logit(p) = \\frac{b}{p(1-p)}\n$$\nGiven that $b>0$ and $p \\in (0,1)$, it follows that $u'(p) > 0$.\n\nFinally, assembling the derivative $g'(p)$:\n$$\ng'(p) = \\sigma'(u(p)) \\cdot u'(p)\n$$\nAs established, $\\sigma'(u(p))$ is positive because the range of $u(p)$ is a subset of $\\mathbb{R}$, and $u'(p)$ is positive. The product of two positive numbers is positive.\n$$\ng'(p) > 0 \\quad \\text{for all } p \\in (0,1)\n$$\nThis confirms that $g(p)$ is a strictly increasing function on its domain. Consequently, this transformation preserves the ranking of patients' risks as determined by $\\hat{\\pi}$.\n\n**Part 2: Argue that the AUC is preserved**\n\nThe Area Under the Receiver Operating Characteristic curve (AUC) is defined as the probability that the score assigned to a randomly chosen positive case ($Y=1$) is greater than the score assigned to a randomly chosen negative case ($Y=0$). Let $\\hat{\\pi}_1$ be the score for a random case and $\\hat{\\pi}_0$ be the score for a random control. The AUC of the original model is $\\mathrm{AUC}_{\\hat{\\pi}} = P(\\hat{\\pi}_1 > \\hat{\\pi}_0)$. The problem statement uses the linear predictor $S$ for the definition, $\\mathrm{AUC} = P(S_1 > S_0)$; since $\\hat{\\pi}=\\sigma(S)$ and $\\sigma$ is strictly increasing, the ranking is identical, so $P(S_1 > S_0) = P(\\hat{\\pi}_1 > \\hat{\\pi}_0)$.\n\nThe new scores are given by the transformation $\\tilde{\\pi} = g(\\hat{\\pi})$. The AUC for the new scores is $\\mathrm{AUC}_{\\tilde{\\pi}} = P(\\tilde{\\pi}_1 > \\tilde{\\pi}_0)$.\nWe have shown that the function $g$ is strictly increasing. A strictly increasing function preserves order, meaning that for any two values $x_1, x_2$ in its domain, $x_1 > x_2 \\iff g(x_1) > g(x_2)$.\nApplying this to our scores, the inequality $\\hat{\\pi}_1 > \\hat{\\pi}_0$ holds if and only if the inequality $g(\\hat{\\pi}_1) > g(\\hat{\\pi}_0)$ holds. This is equivalent to $\\tilde{\\pi}_1 > \\tilde{\\pi}_0$.\nThe events $\\{\\hat{\\pi}_1 > \\hat{\\pi}_0\\}$ and $\\{\\tilde{\\pi}_1 > \\tilde{\\pi}_0\\}$ are therefore identical. Because the events are identical, their probabilities must be equal:\n$$\nP(\\hat{\\pi}_1 > \\hat{\\pi}_0) = P(\\tilde{\\pi}_1 > \\tilde{\\pi}_0)\n$$\nThus, $\\mathrm{AUC}_{\\hat{\\pi}} = \\mathrm{AUC}_{\\tilde{\\pi}}$. Any strictly increasing transformation of a model's score preserves the AUC because the AUC depends only on the rank-ordering of scores, which is invariant under such transformations.\n\n**Part 3: Derive the population calibration relationship for $S'$**\n\nThe new linear predictor is defined as $S' \\equiv \\logit(\\tilde{\\pi})$. We first express $S'$ in terms of the original linear predictor $S$.\nWe are given $\\tilde{\\pi} = \\sigma(a + b\\,\\logit(\\hat{\\pi}))$.\nWe also know that $\\hat{\\pi} = \\sigma(S)$, and because $\\logit$ is the inverse of $\\sigma$, we have $\\logit(\\hat{\\pi}) = \\logit(\\sigma(S)) = S$.\nSubstituting this into the expression for $\\tilde{\\pi}$:\n$$\n\\tilde{\\pi} = \\sigma(a + bS)\n$$\nNow, we can find $S'$:\n$$\nS' = \\logit(\\tilde{\\pi}) = \\logit(\\sigma(a + bS)) = a + bS\n$$\nWe need to derive the calibration relationship, which is the functional form of $\\logit\\!\\big(P(Y=1 \\mid S'=s')\\big)$ in terms of $s'$.\nThe condition $S' = s'$ is equivalent to $a + bS = s'$, which can be solved for $S$:\n$$\nS = \\frac{s' - a}{b}\n$$\nTherefore, conditioning on $S'=s'$ is equivalent to conditioning on $S = \\frac{s' - a}{b}$.\nWe can write:\n$$\nP(Y=1 \\mid S'=s') = P\\left(Y=1 \\mid S = \\frac{s' - a}{b}\\right)\n$$\nThe problem states that the original model is perfectly calibrated, meaning $P(Y=1 \\mid S=s) = \\sigma(s)$ for any value $s$. We apply this fundamental relationship:\n$$\nP(Y=1 \\mid S'=s') = \\sigma\\left(\\frac{s' - a}{b}\\right)\n$$\nTo find the calibration relationship on the logit scale, we apply the $\\logit$ function to both sides:\n$$\n\\logit\\!\\big(P(Y=1 \\mid S'=s')\\big) = \\logit\\left(\\sigma\\left(\\frac{s' - a}{b}\\right)\\right)\n$$\nSince $\\logit$ and $\\sigma$ are inverse functions, $\\logit(\\sigma(z))=z$. Thus:\n$$\n\\logit\\!\\big(P(Y=1 \\mid S'=s')\\big) = \\frac{s' - a}{b}\n$$\nRewriting this in the standard linear form (intercept + slope $\\times$ predictor):\n$$\n\\logit\\!\\big(P(Y=1 \\mid S'=s')\\big) = -\\frac{a}{b} + \\frac{1}{b}s'\n$$\nThis is the desired calibration relationship. The relationship between the true log-odds and the new predictor $s'$ is linear.\n\nThe calibration intercept is the constant term, $-\\frac{a}{b}$.\nThe calibration slope is the coefficient of the predictor $s'$, which is $\\frac{1}{b}$.\n\n**Part 4: Compute the calibration slope**\n\nThe problem provides the specific parameter values $a=0.4$ and $b=2$.\nUsing the formula for the calibration slope derived above:\n$$\n\\text{Calibration Slope} = \\frac{1}{b}\n$$\nSubstituting $b=2$:\n$$\n\\text{Calibration Slope} = \\frac{1}{2}\n$$\nThe numerical value of the calibration slope is $0.5$.",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "The Hosmer-Lemeshow test is a widely used tool, but its reliance on asymptotic theory can be problematic in finite samples. This advanced practice challenges you to move beyond canned statistical routines by implementing a parametric bootstrap for the HL statistic from first principles. This computational exercise will not only solidify your understanding of the test's mechanics but also equip you with a powerful simulation-based method for obtaining more reliable p-values in practice .",
            "id": "4965784",
            "problem": "You are tasked with implementing a parametric bootstrap procedure to obtain finite-sample p-values for the Hosmer-Lemeshow (HL) goodness-of-fit statistic in a logistic regression model for a binary outcome commonly encountered in medical studies. Use the following foundational base: binary outcomes are modeled as independent Bernoulli trials with probability $p_i$ for subject $i$, and a logistic regression uses the logit link so that the probability satisfies $\\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$, where $\\mathbf{x}_i \\in \\mathbb{R}^{p+1}$ includes an intercept and $p$ predictors, and $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}$ is estimated by maximum likelihood. The Hosmer-Lemeshow (HL) statistic is computed by grouping individuals into $G$ quantile bins based on fitted probabilities and aggregating the discrepancy between observed and expected event counts per group in a Pearson-like manner.\n\nYour program must implement the following functionalities using first principles:\n- Fit a logistic regression via maximum likelihood for a given design matrix and binary outcomes, beginning from the Bernoulli likelihood and the logit link, without using any black-box model-fitting routines.\n- Compute the Hosmer-Lemeshow statistic by partitioning the sample into $G$ approximately equal-sized groups formed by sorting fitted probabilities and splitting contiguously. Within each group, compute observed events and the expected number of events given the fitted probabilities, then sum the standardized discrepancy across groups.\n- Implement a parametric bootstrap of the HL statistic under the fitted logistic model by simulating outcomes from the fitted probabilities, refitting the model on each bootstrap sample, recomputing the HL statistic, and using the empirical distribution to estimate the p-value as the fraction of bootstrap statistics that are at least as large as the observed statistic.\n\nYour program must be fully deterministic and must not require any inputs at runtime. It must generate synthetic datasets according to the parameter values specified in the test suite below, fit the logistic model, compute the observed HL statistic, perform the parametric bootstrap, and output the resulting p-values.\n\nDataset generation rules (to be implemented within the program):\n- For each test case, generate $N$ observations, each predictor independently drawn from a standard normal distribution $\\mathcal{N}(0, 1)$.\n- Form the design matrix with an intercept term: $\\mathbf{X} = [\\mathbf{1}_N, \\mathbf{Z}]$, where $\\mathbf{Z}$ contains the generated predictors.\n- Given a specified coefficient vector $\\boldsymbol{\\beta}$, compute $p_i = \\operatorname{logit}^{-1}(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})$ and simulate outcomes $Y_i \\sim \\operatorname{Bernoulli}(p_i)$.\n- Fit the logistic regression to $(\\mathbf{X}, \\mathbf{Y})$, compute the fitted probabilities $\\hat{p}_i$, compute the observed HL statistic, then perform parametric bootstrap with $B$ replicates by simulating $Y_i^{\\ast} \\sim \\operatorname{Bernoulli}(\\hat{p}_i)$, refitting the model, recomputing the HL statistic for each bootstrap sample, and estimating the p-value as the proportion of bootstrap HL statistics exceeding or equaling the observed HL statistic.\n\nImportant implementation constraints:\n- The logistic regression solver must be implemented using a principled algorithm grounded in the Bernoulli likelihood and the logit link, such as iteratively reweighted least squares (IRLS) or Newton–Raphson derived from the score function and Fisher information. No external model-fitting library calls beyond basic linear algebra are permitted.\n- The grouping for the HL statistic must be recomputed within each bootstrap replicate using the fitted probabilities for that replicate.\n- To ensure numerical stability, when standardizing discrepancies in HL calculations, denominators that would be zero must be handled with a vanishingly small positive constant.\n\nTest suite:\nImplement the program to run the following three test cases and report their p-values:\n- Case $1$ (general well-specified scenario):\n  - Sample size $N = 200$\n  - Number of predictors $p = 3$\n  - Coefficient vector $\\boldsymbol{\\beta} = (-0.5, 0.8, -0.3, 0.5)$ (intercept first)\n  - Number of HL groups $G = 10$\n  - Bootstrap replicates $B = 300$\n  - Random seed $= 20231$\n- Case $2$ (small-sample boundary condition):\n  - Sample size $N = 60$\n  - Number of predictors $p = 2$\n  - Coefficient vector $\\boldsymbol{\\beta} = (0.2, 1.0, -1.2)$ (intercept first)\n  - Number of HL groups $G = 6$\n  - Bootstrap replicates $B = 250$\n  - Random seed $= 13579$\n- Case $3$ (near-separation stress scenario with strong effects):\n  - Sample size $N = 180$\n  - Number of predictors $p = 2$\n  - Coefficient vector $\\boldsymbol{\\beta} = (-3.0, 2.5, 2.5)$ (intercept first)\n  - Number of HL groups $G = 10$\n  - Bootstrap replicates $B = 300$\n  - Random seed $= 24680$\n\nFinal output specification:\n- Your program should produce a single line of output containing the three p-values for Cases $1$, $2$, and $3$ respectively, as a comma-separated list enclosed in square brackets (for example, $[0.123,0.456,0.789]$). Each p-value must be a floating-point number.",
            "solution": "The solution implements a parametric bootstrap to compute the p-value for the Hosmer-Lemeshow (HL) goodness-of-fit test in logistic regression. The procedure is built from first principles as required, encompassing a custom logistic regression solver, the HL statistic calculation, and the bootstrap resampling framework.\n\n### 1. Logistic Regression via Newton-Raphson (IRLS)\n\nThe core of the task is to fit a logistic regression model by maximizing the likelihood function without relying on pre-built model-fitting packages. The model assumes a binary outcome $Y_i$ for subject $i$ follows a Bernoulli distribution with success probability $p_i$, $Y_i \\sim \\operatorname{Bernoulli}(p_i)$. The relationship between $p_i$ and a vector of predictors $\\mathbf{x}_i \\in \\mathbb{R}^{d}$ (where $d=p+1$ includes an intercept) is defined by the logit link function:\n$$ \\eta_i = \\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\mathbf{x}_i^\\top \\boldsymbol{\\beta} $$\nwhere $\\boldsymbol{\\beta}$ is the vector of coefficients. The probability $p_i$ is given by the inverse link, the sigmoid function: $p_i = \\sigma(\\eta_i) = (1 + e^{-\\eta_i})^{-1}$.\n\nThe log-likelihood function for $N$ independent observations $(\\mathbf{X}, \\mathbf{y})$ is:\n$$ l(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] $$\nTo find the maximum likelihood estimate (MLE) $\\hat{\\boldsymbol{\\beta}}$, we solve for the root of the gradient of the log-likelihood. This is achieved using the Newton-Raphson method. The gradient (score vector) $\\mathbf{g}(\\boldsymbol{\\beta})$ and the Hessian matrix $\\mathbf{H}(\\boldsymbol{\\beta})$ are:\n$$ \\mathbf{g}(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} l(\\boldsymbol{\\beta}) = \\sum_{i=1}^N (y_i - p_i) \\mathbf{x}_i = \\mathbf{X}^\\top (\\mathbf{y} - \\mathbf{p}) $$\n$$ \\mathbf{H}(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}}^2 l(\\boldsymbol{\\beta}) = - \\sum_{i=1}^N p_i(1 - p_i) \\mathbf{x}_i \\mathbf{x}_i^\\top = - \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X} $$\nHere, $\\mathbf{p}$ is the vector of probabilities $p_i$, and $\\mathbf{W}$ is a diagonal matrix with entries $W_{ii} = p_i(1 - p_i)$. The Newton-Raphson update at iteration $k$ is:\n$$ \\boldsymbol{\\beta}_{k+1} = \\boldsymbol{\\beta}_k - [\\mathbf{H}(\\boldsymbol{\\beta}_k)]^{-1} \\mathbf{g}(\\boldsymbol{\\beta}_k) = \\boldsymbol{\\beta}_k + (\\mathbf{X}^\\top \\mathbf{W}_k \\mathbf{X})^{-1} \\mathbf{X}^\\top (\\mathbf{y} - \\mathbf{p}_k) $$\nThis is implemented as an iterative loop, known as Iteratively Reweighted Least Squares (IRLS). To ensure numerical stability, particularly against issues like quasi-complete separation which can make the Hessian singular, the implementation takes two precautions:\n1.  A numerically stable sigmoid function is used to prevent overflow with large inputs.\n2.  A small regularization term, $\\lambda\\mathbf{I}$ with $\\lambda=10^{-8}$, is added to the matrix $\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$ before solving the linear system for the update step. This ensures the matrix is always invertible.\n\n### 2. Hosmer-Lemeshow (HL) Statistic Calculation\n\nThe HL statistic evaluates goodness-of-fit by categorizing subjects into $G$ groups based on their predicted probabilities and comparing the observed event counts to the expected counts within each group.\nThe procedure is as follows:\n1. Sort subjects in ascending order based on their fitted probabilities $\\hat{p}_i$.\n2. Partition the sorted subjects into $G$ groups of approximately equal size.\n3. For each group $g \\in \\{1, \\dots, G\\}$, calculate:\n    - The total number of subjects, $N_g$.\n    - The observed number of events (where $y_i=1$), $O_g = \\sum_{i \\in \\text{group } g} y_i$.\n    - The expected number of events, $E_g = \\sum_{i \\in \\text{group } g} \\hat{p}_i$.\n4. The HL statistic sums the Pearson-like chi-squared statistics from each group:\n$$ HL = \\sum_{g=1}^{G} \\frac{(O_g - E_g)^2}{E_g(1 - E_g/N_g)} $$\nTo prevent division by zero when the denominator $E_g(1 - E_g/N_g)$ is zero (i.e., when all $\\hat{p}_i$ in a group are $0$ or $1$), a vanishingly small constant $\\epsilon=10^{-10}$ is added to it.\n\n### 3. Parametric Bootstrap Procedure\n\nThe classical use of the HL statistic relies on an asymptotic $\\chi^2_{G-2}$ distribution, which can be inaccurate for small samples. A parametric bootstrap provides a more reliable, finite-sample p-value.\n1.  **Initial Analysis**: An initial dataset is generated according to the problem's rules. The logistic regression model is fitted to these observed data $(\\mathbf{X}, \\mathbf{Y})$ to obtain the MLE $\\hat{\\boldsymbol{\\beta}}_{obs}$ and corresponding fitted probabilities $\\hat{\\mathbf{p}}_{obs}$. The observed statistic, $HL_{obs}$, is then computed.\n2.  **Bootstrap Resampling**: This process is repeated $B$ times:\n    a. A new bootstrap outcome vector $\\mathbf{Y}^*$ is simulated, where each element $Y^*_i \\sim \\operatorname{Bernoulli}(\\hat{p}_{i, obs})$. The predictor matrix $\\mathbf{X}$ is held constant.\n    b. The logistic model is refitted to the bootstrap data $(\\mathbf{X}, \\mathbf{Y}^*)$ to get a new estimate $\\hat{\\boldsymbol{\\beta}}^*$ and new fitted probabilities $\\hat{\\mathbf{p}}^*$.\n    c. A new HL statistic, $HL^*$, is computed using the bootstrap data $(\\mathbf{Y}^*, \\hat{\\mathbf{p}}^*)$. Crucially, the grouping for the HL statistic is recomputed for each replicate based on the new probabilities $\\hat{\\mathbf{p}}^*$.\n3.  **P-value Estimation**: The p-value is estimated as the fraction of bootstrap statistics that are at least as large as the observed statistic:\n$$ p\\text{-value} = \\frac{\\#\\{HL^* \\geq HL_{obs}\\}}{B} = \\frac{1}{B} \\sum_{j=1}^{B} I(HL^*_j \\geq HL_{obs}) $$\nwhere $I(\\cdot)$ is the indicator function.\n\nThe provided Python code encapsulates this entire workflow. It is deterministic, using the specified random seeds for each test case to ensure reproducibility.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the parametric bootstrap of the Hosmer-Lemeshow statistic.\n    \"\"\"\n\n    def stable_sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        z = np.asarray(z, dtype=np.float64)\n        out = np.empty_like(z)\n        \n        # Positive values: 1 / (1 + exp(-z))\n        pos_mask = z >= 0\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-z[pos_mask]))\n        \n        # Negative values: exp(z) / (1 + exp(z))\n        neg_mask = ~pos_mask\n        exp_z = np.exp(z[neg_mask])\n        out[neg_mask] = exp_z / (1.0 + exp_z)\n        \n        return out\n\n    def fit_logistic_regression(X, y, max_iter=50, tol=1e-7):\n        \"\"\"\n        Fits a logistic regression model using Iteratively Reweighted Least Squares (IRLS).\n        \"\"\"\n        n_samples, n_features = X.shape\n        beta = np.zeros(n_features, dtype=np.float64)\n\n        for _ in range(max_iter):\n            eta = X @ beta\n            p = stable_sigmoid(eta)\n            \n            # Clip probabilities to avoid weights of exactly zero, enhancing stability\n            p_clipped = np.clip(p, 1e-10, 1 - 1e-10)\n            weights = p_clipped * (1 - p_clipped)\n            \n            gradient = X.T @ (y - p)\n            \n            # Hessian H = -X.T W X. We solve (X.T W X) delta = gradient\n            # The calculation (X.T * weights) @ X is an efficient way to compute X.T @ diag(weights) @ X\n            hessian_term = (X.T * weights) @ X\n            \n            # Add a small ridge penalty for numerical stability (against separation)\n            hessian_term += 1e-8 * np.eye(n_features)\n            \n            try:\n                update = np.linalg.solve(hessian_term, gradient)\n            except np.linalg.LinAlgError:\n                # Fallback to pseudo-inverse if solver fails, which is highly unlikely with regularization\n                pinv_hessian = np.linalg.pinv(hessian_term)\n                update = pinv_hessian @ gradient\n\n            beta_new = beta + update\n            \n            if np.linalg.norm(beta_new - beta) < tol:\n                beta = beta_new\n                break\n                \n            beta = beta_new\n            \n        return beta\n\n    def compute_hl_statistic(y, p_hat, G):\n        \"\"\"\n        Computes the Hosmer-Lemeshow goodness-of-fit statistic.\n        \"\"\"\n        n = len(y)\n        # Sort by predicted probability\n        sorted_indices = np.argsort(p_hat)\n        sorted_y = y[sorted_indices]\n        sorted_p_hat = p_hat[sorted_indices]\n\n        # Partition indices into G groups\n        group_indices = np.array_split(np.arange(n), G)\n\n        hl_stat = 0.0\n        \n        for indices in group_indices:\n            if len(indices) == 0:\n                continue\n\n            y_g = sorted_y[indices]\n            p_hat_g = sorted_p_hat[indices]\n            \n            N_g = len(y_g)\n            O_g = np.sum(y_g)\n            E_g = np.sum(p_hat_g)\n\n            # Add a small epsilon to the denominator to prevent division by zero\n            denominator = E_g * (1.0 - E_g / N_g) + 1e-10\n            \n            term = ((O_g - E_g)**2) / denominator\n            hl_stat += term\n            \n        return hl_stat\n\n    def run_bootstrap_procedure(N, p_dim, beta_true, G, B, seed):\n        \"\"\"\n        Executes the full data generation and parametric bootstrap for a single test case.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate original dataset\n        Z = rng.normal(size=(N, p_dim))\n        X = np.c_[np.ones(N), Z]\n        \n        true_p = stable_sigmoid(X @ beta_true)\n        Y = rng.binomial(1, true_p)\n\n        # 2. Fit model on original data and compute observed HL statistic\n        beta_obs = fit_logistic_regression(X, Y)\n        p_hat_obs = stable_sigmoid(X @ beta_obs)\n        hl_obs = compute_hl_statistic(Y, p_hat_obs, G)\n        \n        # 3. Perform parametric bootstrap\n        hl_bootstrap_stats = np.zeros(B, dtype=np.float64)\n        for j in range(B):\n            # Simulate a bootstrap sample\n            Y_star = rng.binomial(1, p_hat_obs)\n            \n            # Refit model on bootstrap sample\n            beta_star = fit_logistic_regression(X, Y_star)\n            p_hat_star = stable_sigmoid(X @ beta_star)\n            \n            # Compute and store bootstrap HL statistic\n            hl_star = compute_hl_statistic(Y_star, p_hat_star, G)\n            hl_bootstrap_stats[j] = hl_star\n\n        # 4. Calculate p-value\n        p_value = np.mean(hl_bootstrap_stats >= hl_obs)\n        \n        return p_value\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: (N, p, beta, G, B, seed)\n        (200, 3, np.array([-0.5, 0.8, -0.3, 0.5]), 10, 300, 20231),\n        # Case 2:\n        (60, 2, np.array([0.2, 1.0, -1.2]), 6, 250, 13579),\n        # Case 3:\n        (180, 2, np.array([-3.0, 2.5, 2.5]), 10, 300, 24680),\n    ]\n\n    results = []\n    for case in test_cases:\n        p_value = run_bootstrap_procedure(*case)\n        results.append(p_value)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}