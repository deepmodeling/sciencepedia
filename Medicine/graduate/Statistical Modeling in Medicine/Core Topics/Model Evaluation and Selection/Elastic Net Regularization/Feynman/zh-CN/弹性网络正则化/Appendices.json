{
    "hands_on_practices": [
        {
            "introduction": "在构建任何统计模型时，理解每个参数的作用至关重要。本练习聚焦于弹性网络模型中常被忽视的截距项 $\\beta_0$。通过从第一性原理出发，你将推导出其闭式解，并从模型的校准和不变性等基本原则出发，论证为何对截距项进行惩罚在统计上是不合适的。这个实践有助于巩固你对正则化模型基本设定的理解。",
            "id": "4961465",
            "problem": "一个临床研究团队正在一个大型观察性患者队列中，为一个连续的生物标志物结局开发一个线性预测器。对于患者 $i \\in \\{1,\\dots,n\\}$，令 $y_{i} \\in \\mathbb{R}$ 表示测量的结局（例如，收缩压），并令 $\\boldsymbol{x}_{i} \\in \\mathbb{R}^{p}$ 表示一个包含 $p$ 个临床协变量（例如，实验室值和人口统计学信息）的向量。该团队提出了带截距的弹性网络正则化线性模型，该模型通过最小化经验风险来估计参数 $(\\beta_{0},\\boldsymbol{\\beta}) \\in \\mathbb{R} \\times \\mathbb{R}^{p}$\n$$\n\\frac{1}{2n}\\sum_{i=1}^{n}\\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big)^{2}+\\lambda\\left(\\frac{1-\\alpha}{2}\\|\\boldsymbol{\\beta}\\|_{2}^{2}+\\alpha\\|\\boldsymbol{\\beta}\\|_{1}\\right),\n$$\n其中 $\\lambda \\ge 0$ 和 $\\alpha \\in [0,1]$ 是正则化超参数。假设每个协变量都已在患者间进行了中心化，因此设计矩阵的列均值为零；等价地，$\\overline{\\boldsymbol{x}}=\\frac{1}{n}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}=\\boldsymbol{0}$。令 $\\overline{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}$ 表示结局的样本均值。\n\n从凸优化的第一性原理和均方误差的基本求导法则出发，并且不借助任何预先指定的回归恒等式，执行以下操作：\n\n- 推导关于 $\\beta_{0}$ 的唯一最小化子，条件是 $\\boldsymbol{\\beta}$ 为任意固定值，并仅使用 $\\overline{y}$、$\\overline{\\boldsymbol{x}}$ 和 $\\boldsymbol{\\beta}$ 将其表示为闭式形式。\n- 在所述的协变量中心化假设下，将此条件最小化子简化为一个不依赖于 $\\boldsymbol{\\beta}$ 或正则化超参数的闭式表达式。\n- 提供一个严谨的论证，基于从目标函数导出的不变性和校准考虑，解释为什么在临床统计建模中，对 $\\beta_{0}$ 应用任何显式惩罚项（例如，添加 $\\lambda_{0}|\\beta_{0}|$ 或 $\\lambda_{0}\\beta_{0}^{2}$，其中 $\\lambda_{0}0$）是不恰当的。\n\n作为你的最终答案，仅报告在中心化协变量下最优 $\\beta_{0}$ 的简化闭式表达式。不需要数值近似，也不适用任何物理单位。",
            "solution": "所述问题是有效的。它在科学上基于正则化线性模型（特别是弹性网络回归）的既定理论。该问题是适定的，提供了一个凸目标函数和一组明确的任务，这些任务导向一个唯一的、可推导的解。所有术语都已定义，并且假设（例如，中心化协变量）已明确说明。该问题是客观的，没有任何事实或逻辑上的不一致之处。\n\n解决方案按照问题陈述的要求分三部分进行。令目标函数表示为 $L(\\beta_{0}, \\boldsymbol{\\beta})$。\n$$\nL(\\beta_{0}, \\boldsymbol{\\beta}) = \\frac{1}{2n}\\sum_{i=1}^{n}\\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big)^{2}+\\lambda\\left(\\frac{1-\\alpha}{2}\\|\\boldsymbol{\\beta}\\|_{2}^{2}+\\alpha\\|\\boldsymbol{\\beta}\\|_{1}\\right)\n$$\n\n首先，我们针对任意但固定的 $\\boldsymbol{\\beta}$ 值，推导 $L$ 关于 $\\beta_{0}$ 的唯一最小化子。惩罚项 $\\lambda\\left(\\frac{1-\\alpha}{2}\\|\\boldsymbol{\\beta}\\|_{2}^{2}+\\alpha\\|\\boldsymbol{\\beta}\\|_{1}\\right)$ 不依赖于 $\\beta_{0}$。因此，最小化 $L(\\beta_{0}, \\boldsymbol{\\beta})$ 关于 $\\beta_{0}$ 的问题等价于最小化均方误差（MSE）项：\n$$\nf(\\beta_{0}) = \\frac{1}{2n}\\sum_{i=1}^{n}\\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big)^{2}\n$$\n这个函数 $f(\\beta_{0})$ 是关于 $\\beta_{0}$ 的二次函数，因此是凸函数。唯一的最小值可以通过将其关于 $\\beta_{0}$ 的一阶导数设为零来找到。使用链式求导法则：\n$$\n\\frac{\\partial f}{\\partial \\beta_{0}} = \\frac{1}{2n}\\sum_{i=1}^{n} 2 \\cdot \\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big) \\cdot \\frac{\\partial}{\\partial \\beta_{0}}\\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big)\n$$\n$$\n\\frac{\\partial f}{\\partial \\beta_{0}} = \\frac{1}{n}\\sum_{i=1}^{n} \\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big) \\cdot (-1) = -\\frac{1}{n}\\sum_{i=1}^{n} \\big(y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big)\n$$\n为了找到最优值，我们将其记为 $\\hat{\\beta}_{0}$，我们将此导数设为零：\n$$\n-\\frac{1}{n}\\sum_{i=1}^{n} \\big(y_{i}-\\hat{\\beta}_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big) = 0\n$$\n乘以 $-n$ 得到：\n$$\n\\sum_{i=1}^{n} \\big(y_{i}-\\hat{\\beta}_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}\\big) = 0\n$$\n我们可以分配求和：\n$$\n\\sum_{i=1}^{n}y_{i} - \\sum_{i=1}^{n}\\hat{\\beta}_{0} - \\sum_{i=1}^{n}\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta} = 0\n$$\n由于 $\\hat{\\beta}_{0}$ 相对于求和索引 $i$ 是一个常数，所以 $\\sum_{i=1}^{n}\\hat{\\beta}_{0} = n\\hat{\\beta}_{0}$。方程变为：\n$$\n\\sum_{i=1}^{n}y_{i} - n\\hat{\\beta}_{0} - \\left(\\sum_{i=1}^{n}\\boldsymbol{x}_{i}^{\\top}\\right)\\boldsymbol{\\beta} = 0\n$$\n求解 $\\hat{\\beta}_{0}$：\n$$\nn\\hat{\\beta}_{0} = \\sum_{i=1}^{n}y_{i} - \\left(\\sum_{i=1}^{n}\\boldsymbol{x}_{i}^{\\top}\\right)\\boldsymbol{\\beta}\n$$\n除以 $n$ 得到 $\\hat{\\beta}_{0}$ 的表达式：\n$$\n\\hat{\\beta}_{0} = \\frac{1}{n}\\sum_{i=1}^{n}y_{i} - \\left(\\frac{1}{n}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}^{\\top}\\right)\\boldsymbol{\\beta}\n$$\n使用给定的定义 $\\overline{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}$ 和 $\\overline{\\boldsymbol{x}}=\\frac{1}{n}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}$，表达式变为：\n$$\n\\hat{\\beta}_{0} = \\overline{y} - \\overline{\\boldsymbol{x}}^{\\top}\\boldsymbol{\\beta}\n$$\n这就是条件最小化子的闭式表达式。为了确认它是一个最小值，我们检查二阶导数：$\\frac{\\partial^{2} f}{\\partial \\beta_{0}^{2}} = \\frac{\\partial}{\\partial \\beta_{0}} \\left[ -\\frac{1}{n}\\sum_{i=1}^{n} (y_{i}-\\beta_{0}-\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}) \\right] = -\\frac{1}{n} \\sum_{i=1}^{n} (-1) = \\frac{1}{n} \\sum_{i=1}^{n} 1 = \\frac{n}{n} = 1$。由于二阶导数为正（$1  0$），该函数在 $\\beta_{0}$ 上是严格凸的，因此 $\\hat{\\beta}_{0}$ 是唯一的最小值。\n\n第二，我们在协变量已中心化的既定假设下简化此表达式，即 $\\overline{\\boldsymbol{x}} = \\boldsymbol{0}$。将 $\\overline{\\boldsymbol{x}} = \\boldsymbol{0}$ 代入推导出的 $\\hat{\\beta}_{0}$ 表达式中：\n$$\n\\hat{\\beta}_{0} = \\overline{y} - \\boldsymbol{0}^{\\top}\\boldsymbol{\\beta}\n$$\n零向量与任何向量 $\\boldsymbol{\\beta}$ 的乘积是标量 $0$。因此，表达式简化为：\n$$\n\\hat{\\beta}_{0} = \\overline{y}\n$$\n这个结果表明，当协变量中心化时，最优的截距项就是结果变量的样本均值 $\\overline{y}$。重要的是，$\\beta_{0}$ 的这个最优值与协变量效应 $\\boldsymbol{\\beta}$ 以及正则化超参数 $\\lambda$ 和 $\\alpha$ 无关。在实践中，这允许进行计算简化：可以（在中心化协变量 $\\boldsymbol{x}$ 的基础上）再中心化结果变量 $y$，然后拟合一个没有截距的模型。原始未中心化问题的截距则恢复为 $\\overline{y}$。\n\n第三，我们提供一个严谨的论证，解释为什么在统计建模中为截距 $\\beta_{0}$ 添加惩罚项是不恰当的。\n1.  **对位置平移的不变性**：统计模型的一个基本要求是，其实质性结论（即协变量的估计效应 $\\boldsymbol{\\beta}$）应对于结果变量原点的任意选择保持不变。例如，如果结果 $y_i$ 是温度，那么温度与一组预测变量之间的估计关系不应取决于温度是以摄氏度还是开尔文（原点的平移）来测量。让我们考虑结果的一个平移 $y_i \\to y_i' = y_i + c$，其中 $c$ 为某个常数。一个直观的模型应该通过平移截距 $\\beta_0 \\to \\beta_0' = \\beta_0 + c$ 来适应这种情况，而保持 $\\boldsymbol{\\beta}$ 不变。在没有对 $\\beta_0$ 进行惩罚的情况下，最优截距是 $\\hat{\\beta}_0 = \\overline{y} - \\overline{\\boldsymbol{x}}^\\top\\boldsymbol{\\beta}$。对于平移后的结果 $y'$，新的均值是 $\\overline{y'} = \\overline{y} + c$，因此新的截距是 $\\hat{\\beta}_0' = (\\overline{y}+c) - \\overline{\\boldsymbol{x}}^\\top\\boldsymbol{\\beta} = \\hat{\\beta}_0 + c$。这种所期望的不变性是成立的。现在，假设我们在目标函数中添加一个像 $\\lambda_0 \\beta_0^2$（其中 $\\lambda_0  0$）这样的惩罚项。关于 $\\beta_0$ 的导数变为 $-\\frac{1}{n}\\sum (y_i - \\beta_0 - \\boldsymbol{x}_i^\\top\\boldsymbol{\\beta}) + 2\\lambda_0 \\beta_0$。新的最优值 $\\hat{\\beta}_0$ 将是一个有偏估计，被拉向 $0$。模型将不再对 $y$ 的平移保持不变，因为 $\\boldsymbol{\\beta}$ 的解会与 $y$ 的原点选择纠缠在一起。这将使得协变量效应的解释依赖于一个任意的基线，这在科学上是不合理的。\n2.  **模型校准与偏差**：截距 $\\beta_0$ 确保模型是经过校准的，即残差的均值为零。正如我们最初的推导所示，将 $\\frac{\\partial L}{\\partial \\beta_0} = 0$ 设为零等价于确保 $\\frac{1}{n}\\sum(y_i - \\hat{y}_i) = 0$，其中 $\\hat{y}_i = \\hat{\\beta}_0 + \\boldsymbol{x}_i^\\top\\boldsymbol{\\beta}$。这意味着模型没有系统性偏差（即，它不会持续地高估或低估）。如果我们对 $\\beta_0$ 进行惩罚，导数条件将会改变，我们将不再有 $\\frac{1}{n}\\sum(y_i - \\hat{y}_i) = 0$。惩罚项会在模型的预测中引入系统性偏差，这在任何应用中都是非常不希望看到的，尤其是在无偏预测至关重要的临床环境中。\n3.  **正则化的概念作用**：正则化惩罚的目的是通过收缩协变量的估计效应 $\\boldsymbol{\\beta}$ 来控制模型复杂度和防止过拟合。系数 $\\beta_j$ 的大小反映了协变量 $x_j$ 与结果 $y$ 之间关联的强度。将这些系数向零收缩对应于假设某些协变量可能几乎没有或完全没有真实效应。然而，截距 $\\beta_0$ 并不衡量协变量的效应。它代表当所有协变量为零时（或者如果已中心化，则在其均值处）结果的基线值。正如我们所展示的，对于中心化的协变量，$\\beta_0$ 只是结果的均值 $\\overline{y}$。惩罚 $\\beta_0$ 意味着将数据的估计均值向零收缩。这样做没有任何科学或统计上的理由；样本均值 $\\overline{y}$ 是总体均值的最佳无偏估计。因此，惩罚截距不符合正则化的原则性目标，即正则化预测变量的效应，而不是结果的整体水平。\n\n总之，惩罚截距会破坏基本的不变性属性，引入系统性偏差，并误用了正则化原理。正是由于这些原因，在正则化回归模型中，截距项通常不被惩罚。",
            "answer": "$$\n\\boxed{\\overline{y}}\n$$"
        },
        {
            "introduction": "弹性网络相较于LASSO的一个核心优势在于其处理高度相关预测变量的能力。本练习设置了一个经典的临床场景，其中两种生物标志物高度相关。通过应用KKT条件，你将亲手计算并对比LASSO和弹性网络的系数估计，直观地看到LASSO如何随意选择其中一个变量，而弹性网络则倾向于将权重分配给相关的变量组，从而揭示其关键的“分组效应”。",
            "id": "4961425",
            "problem": "一个临床研究团队正在使用一个线性模型对一个连续的心血管代谢结局进行建模，该模型包含两个已知高度相关的标准化预测变量：低密度脂蛋白胆固醇和载脂蛋白B。设设计矩阵的各列已标准化为零均值和单位方差，并假设经验格拉姆矩阵和预测变量-结局内积（按样本量 $n$ 缩放）由以下公式给出\n$$\nG \\equiv \\frac{X^{\\top}X}{n} = \\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix}, \\qquad c \\equiv \\frac{X^{\\top}y}{n} = \\begin{pmatrix} 1.0 \\\\ 0.95 \\end{pmatrix}.\n$$\n考虑最小化以下凸目标函数的惩罚最小二乘估计量\n$$\n\\frac{1}{2}\\,\\beta^{\\top} G \\beta - c^{\\top} \\beta \\;+\\; \\lambda_{1}\\,\\|\\beta\\|_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\|\\beta\\|_{2}^{2},\n$$\n其中 $\\beta \\in \\mathbb{R}^{2}$，$\\|\\cdot\\|_{1}$ 是 $\\ell_{1}$ 范数，$\\|\\cdot\\|_{2}$ 是欧几里得范数。最小绝对收缩和选择算子 (LASSO) 对应于 $\\lambda_{2}=0$ 和 $\\lambda_{1}0$ 的情况，弹性网络对应于 $\\lambda_{1}0$ 和 $\\lambda_{2}0$ 的情况。\n\n仅使用凸最优性的基本定义和 Karush–Kuhn–Tucker (KKT) 条件，完成以下任务：\n- 首先，考虑 LASSO，其中 $\\lambda_{1}=0.6$ 且 $\\lambda_{2}=0$，确定其最优解是否将一个系数设为零。指出哪个预测变量被保留，并计算其系数。\n- 其次，考虑弹性网络，其中 $\\lambda_{1}=0.3$ 且 $\\lambda_{2}=0.3$。在与正相关生物标志物一致的符号模式下，求解精确的弹性网络系数，并验证两个系数都严格为正。\n\n作为你的最终答案，报告第二个预测变量（载脂蛋白B的代理变量）的弹性网络系数的精确值，并化简为单个有理数。不要包含单位，也不要进行四舍五入。",
            "solution": "我们从 $\\beta \\in \\mathbb{R}^{2}$ 的凸惩罚最小二乘目标函数开始，\n$$\nQ(\\beta) \\equiv \\frac{1}{2}\\,\\beta^{\\top} G \\beta - c^{\\top} \\beta \\;+\\; \\lambda_{1}\\,\\|\\beta\\|_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\|\\beta\\|_{2}^{2},\n$$\n其中\n$$\nG=\\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix}, \\qquad c=\\begin{pmatrix} 1.0 \\\\ 0.95 \\end{pmatrix}.\n$$\n因为 $G$ 是半正定的且 $\\lambda_{2}\\ge 0$，所以 $Q$ 是凸函数。Karush–Kuhn–Tucker (KKT) 条件刻画了最优性。将 $\\|\\beta\\|_{1}$ 的次梯度写作 $s \\in \\partial \\|\\beta\\|_{1}$，其中当 $\\beta_{j}\\ne 0$ 时 $s_{j}=\\operatorname{sign}(\\beta_{j})$，当 $\\beta_{j}=0$ 时 $s_{j}\\in[-1,1]$，则 KKT 平稳性条件为\n$$\nG\\beta - c + \\lambda_{1}\\,s + \\lambda_{2}\\,\\beta = 0.\n$$\n我们将此应用于两种情况。\n\nLASSO 情况（$\\lambda_{1}=0.6$, $\\lambda_{2}=0$）。我们检验仅第一个预测变量活跃的稀疏解是否是 KKT 可行的。假设 $\\beta_{1}0$ 且 $\\beta_{2}=0$。那么 $s_{1}=+1$, $s_{2}\\in[-1,1]$。平稳性方程按分量展开为\n$$\n\\begin{aligned}\n\\text{对于 } j=1:\\quad (G\\beta)_{1} - c_{1} + \\lambda_{1} s_{1} = 0 \\;\\Rightarrow\\; 1\\cdot \\beta_{1} + 0.9\\cdot 0 - 1.0 + 0.6\\cdot 1 = 0 \\;\\Rightarrow\\; \\beta_{1} = 0.4,\\\\\n\\text{对于 } j=2:\\quad (G\\beta)_{2} - c_{2} + \\lambda_{1} s_{2} = 0 \\;\\Rightarrow\\; 0.9\\cdot \\beta_{1} + 1\\cdot 0 - 0.95 + 0.6\\,s_{2} = 0.\n\\end{aligned}\n$$\n为使 $\\beta_{2}=0$ 是最优的，我们需要存在 $s_{2}\\in[-1,1]$ 使得第二个方程成立，这等价于次梯度不等式\n$$\n\\left| (G\\beta)_{2} - c_{2} \\right| \\le \\lambda_{1} \\;\\;\\Longleftrightarrow\\;\\; \\left| 0.9\\cdot 0.4 - 0.95 \\right| \\le 0.6 \\;\\;\\Longleftrightarrow\\;\\; | -0.59 | \\le 0.6.\n$$\n此不等式成立，因为 $0.59 \\le 0.6$。因此，KKT 条件在 $\\beta_{1}=0.4$ 和 $\\beta_{2}=0$ 时得到满足。因此，LASSO 只选择第一个预测变量，并将第二个预测变量的系数设为零。\n\n弹性网络情况（$\\lambda_{1}=0.3$, $\\lambda_{2}=0.3$）。我们现在求解弹性网络，并假定两个预测变量都为正相关，即 $\\beta_{1}0, \\beta_{2}0$，所以 $s=(1,1)^{\\top}$。KKT 平稳性条件变成一个线性系统：\n$$\n\\left( G + \\lambda_{2} I \\right) \\beta - c + \\lambda_{1} \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = 0\n\\;\\;\\Longleftrightarrow\\;\\;\n\\left( G + \\lambda_{2} I \\right) \\beta = c - \\lambda_{1} \\begin{pmatrix}1\\\\ 1\\end{pmatrix}.\n$$\n代入给定值，\n$$\nA \\equiv G + \\lambda_{2} I = \\begin{pmatrix} 1+0.3  0.9 \\\\ 0.9  1+0.3 \\end{pmatrix} = \\begin{pmatrix} 1.3  0.9 \\\\ 0.9  1.3 \\end{pmatrix}, \\qquad\nb \\equiv c - \\lambda_{1} \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = \\begin{pmatrix} 1.0-0.3 \\\\ 0.95-0.3 \\end{pmatrix} = \\begin{pmatrix} 0.7 \\\\ 0.65 \\end{pmatrix}.\n$$\n我们计算 $A^{-1}$。其行列式为\n$$\n\\det(A) = (1.3)^{2} - (0.9)^{2} = 1.69 - 0.81 = 0.88,\n$$\n且\n$$\nA^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} 1.3  -0.9 \\\\ -0.9  1.3 \\end{pmatrix} = \\frac{1}{0.88} \\begin{pmatrix} 1.3  -0.9 \\\\ -0.9  1.3 \\end{pmatrix}.\n$$\n因此，\n$$\n\\beta = A^{-1} b = \\frac{1}{0.88} \\begin{pmatrix} 1.3  -0.9 \\\\ -0.9  1.3 \\end{pmatrix} \\begin{pmatrix} 0.7 \\\\ 0.65 \\end{pmatrix}.\n$$\n将其展开计算，\n$$\n\\beta_{1} = \\frac{1}{0.88}\\left( 1.3\\cdot 0.7 - 0.9\\cdot 0.65 \\right) = \\frac{1}{0.88}\\left( 0.91 - 0.585 \\right) = \\frac{0.325}{0.88} = \\frac{65}{176},\n$$\n$$\n\\beta_{2} = \\frac{1}{0.88}\\left( -0.9\\cdot 0.7 + 1.3\\cdot 0.65 \\right) = \\frac{1}{0.88}\\left( -0.63 + 0.845 \\right) = \\frac{0.215}{0.88} = \\frac{43}{176}.\n$$\n两个系数都严格为正，这证实了我们假定的符号模式，从而也证实了 KKT 的可行性。因此，在这种高度相关的设定下，LASSO 只选择了第一个预测变量（$\\beta_{1}=0.4$, $\\beta_{2}=0$），而弹性网络则在两个预测变量之间分配了权重（$\\beta_{1}=\\frac{65}{176}$ 且 $\\beta_{2}=\\frac{43}{176}$）。\n\n所求的量是第二个预测变量的精确弹性网络系数，即 $\\beta_{2}=\\frac{43}{176}$。",
            "answer": "$$\\boxed{\\frac{43}{176}}$$"
        },
        {
            "introduction": "在前一个具体数值例子的基础上，本练习将分析提升到更具普遍性的群体层面。你将推导一个闭式表达式，它量化了相关预测变量的系数差异如何依赖于相关性 $\\rho$ 以及正则化参数 $\\lambda$ 和 $\\alpha$。这个推导将让你从分析层面深刻理解弹性网络的分组效应是如何被模型的超参数精确调控的。",
            "id": "4961464",
            "problem": "一个生物统计学团队正在一个心血管队列研究中，使用两种标准化的血清生物标志物（记为 $x_1$ 和 $x_2$）来开发一个关于动脉硬化指数的简约预测模型。这两种生物标志物服从联合高斯分布，其均值为零，方差为单位值，相关系数为 $\\rho \\in (-1,1)$。在总体极限（无抽样噪声）的情况下，临床结局由 $y=x_1$ 生成，不含额外噪声。该团队拟合了一个弹性网络回归模型，以最小化期望平方损失加上一个混合惩罚项：\n$$\n\\frac{1}{2}\\,\\mathbb{E}\\big[(y - x_1 \\beta_1 - x_2 \\beta_2)^2\\big] \\;+\\; \\lambda\\left(\\frac{1-\\alpha}{2}\\,(\\beta_1^2+\\beta_2^2) \\;+\\; \\alpha\\,(|\\beta_1|+|\\beta_2|)\\right),\n$$\n其中 $\\lambda0$ 是正则化强度，$\\alpha \\in (0,1)$ 是岭回归部分与最小绝对收缩和选择算子 (LASSO) 部分之间的混合比例。\n\n在以下科学上符合现实的假设下，于总体设定中进行求解：\n- 预测变量 $x_1$ 和 $x_2$ 服从联合高斯分布，是标准化的，相关系数为 $\\rho$，因此二阶矩的 $2 \\times 2$ 格拉姆矩阵为\n$$\nG \\;=\\; \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}.\n$$\n- 响应变量由 $y=x_1$ 生成，因此预测变量-结局交叉矩的向量为\n$$\nc \\;=\\; \\mathbb{E}\\big[x\\,y\\big] \\;=\\; \\begin{pmatrix} 1 \\\\ \\rho \\end{pmatrix}.\n$$\n- 调整参数满足 $0  \\lambda\\alpha  \\rho  1$，这确保了在最优解中两个估计系数都严格为正且非零。\n\n基于以上基础，推导弹性网络估计值之间的绝对差 $|\\hat{\\beta}_1 - \\hat{\\beta}_2|$ 作为 $\\rho$、$\\lambda$ 和 $\\alpha$ 的函数的精确闭式表达式。您的最终表达式必须简化到只依赖于 $\\rho$、$\\lambda$ 和 $\\alpha$。不要引入或假设任何额外的常数。最终答案必须是单个解析表达式。无需四舍五入。最终答案以无单位形式表示。",
            "solution": "用户希望在特定的总体设定下，求出弹性网络系数估计值之间的绝对差 $|\\hat{\\beta}_1 - \\hat{\\beta}_2|$。\n\n### 第 1 步：提取已知条件\n-   预测变量：$x_1, x_2$ 服从联合高斯分布，满足 $\\mathbb{E}[x_1] = \\mathbb{E}[x_2] = 0$，$\\mathbb{E}[x_1^2] = \\mathbb{E}[x_2^2] = 1$，以及 $\\mathbb{E}[x_1 x_2] = \\rho$，其中 $\\rho \\in (-1, 1)$。\n-   响应变量：$y = x_1$。\n-   待最小化的目标函数：\n    $$L(\\beta_1, \\beta_2) = \\frac{1}{2}\\,\\mathbb{E}\\big[(y - x_1 \\beta_1 - x_2 \\beta_2)^2\\big] \\;+\\; \\lambda\\left(\\frac{1-\\alpha}{2}\\,(\\beta_1^2+\\beta_2^2) \\;+\\; \\alpha\\,(|\\beta_1|+|\\beta_2|)\\right)$$\n-   正则化参数：$\\lambda  0$ 且 $\\alpha \\in (0, 1)$。\n-   格拉姆矩阵：$G = \\mathbb{E}[x x^T] = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$，其中 $x = (x_1, x_2)^T$。\n-   预测变量-结局交叉矩向量：$c = \\mathbb{E}[x y] = \\begin{pmatrix} \\mathbb{E}[x_1 y] \\\\ \\mathbb{E}[x_2 y] \\end{pmatrix} = \\begin{pmatrix} \\mathbb{E}[x_1^2] \\\\ \\mathbb{E}[x_2 x_1] \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\rho \\end{pmatrix}$。\n-   参数约束：$0  \\lambda\\alpha  \\rho  1$，这确保了最优系数 $\\hat{\\beta}_1$ 和 $\\hat{\\beta}_2$ 严格为正。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题在理论统计学中定义明确且有科学依据。目标函数是标准的弹性网络成本函数。关于数据生成过程（联合高斯预测变量，线性响应）的假设是在总体设定中推导闭式解的常用方法。给定的格拉姆矩阵和交叉矩向量与分布假设一致。目标函数是严格凸的，保证了唯一解的存在。约束条件 $0  \\lambda\\alpha  \\rho  1$ 至关重要，并明确指出以确保系数为正，从而简化了推导过程。该问题是自洽的、数学上合理的，并且没有任何无效标志。\n\n### 第 3 步：结论与行动\n问题有效。将提供一个完整的、有理有据的解答。\n\n### 解答推导\n目标是找到最小化目标函数 $L(\\beta_1, \\beta_2)$ 的向量 $\\hat{\\beta} = (\\hat{\\beta}_1, \\hat{\\beta}_2)^T$。我们可以用矩阵表示法来表达损失项。设 $\\beta = (\\beta_1, \\beta_2)^T$。\n期望平方损失项为：\n$$ \\frac{1}{2}\\mathbb{E}[(y - x^T\\beta)^2] = \\frac{1}{2}\\mathbb{E}[y^2 - 2y x^T\\beta + (x^T\\beta)^2] = \\frac{1}{2}(\\mathbb{E}[y^2] - 2\\mathbb{E}[y x^T]\\beta + \\mathbb{E}[\\beta^T x x^T \\beta]) $$\n$$ = \\frac{1}{2}(\\mathbb{E}[y^2] - 2c^T\\beta + \\beta^T G \\beta) $$\n项 $\\frac{1}{2}\\mathbb{E}[y^2]$ 是一个关于 $\\beta$ 的常数，在最小化过程中可以忽略。惩罚项可以写成 $\\lambda\\left(\\frac{1-\\alpha}{2} \\|\\beta\\|_2^2 + \\alpha\\|\\beta\\|_1\\right)$，其中 $\\|\\beta\\|_2^2 = \\beta_1^2 + \\beta_2^2$ 且 $\\|\\beta\\|_1 = |\\beta_1| + |\\beta_2|$。\n因此，需要最小化的目标函数等价于：\n$$ J(\\beta) = \\frac{1}{2} \\beta^T G \\beta - c^T \\beta + \\frac{\\lambda(1-\\alpha)}{2} \\beta^T I \\beta + \\lambda\\alpha \\|\\beta\\|_1 $$\n其中 $I$ 是 $2 \\times 2$ 单位矩阵。\n\n为了找到最小值 $\\hat{\\beta}$，我们求 $J(\\beta)$ 关于 $\\beta$ 的次梯度，并令其为 $0$。次梯度为：\n$$ \\partial_{\\beta}J(\\beta) = G\\beta - c + \\lambda(1-\\alpha)I\\beta + \\lambda\\alpha \\cdot \\text{sgn}(\\beta) $$\n其中 $\\text{sgn}(\\beta) = (\\text{sgn}(\\beta_1), \\text{sgn}(\\beta_2))^T$。\n在最优解 $\\hat{\\beta}$ 处将次梯度设为 $0$，得到平稳性条件：\n$$ (G + \\lambda(1-\\alpha)I)\\hat{\\beta} - c + \\lambda\\alpha \\cdot \\text{sgn}(\\hat{\\beta}) = 0 $$\n$$ (G + \\lambda(1-\\alpha)I)\\hat{\\beta} = c - \\lambda\\alpha \\cdot \\text{sgn}(\\hat{\\beta}) $$\n问题陈述，调整参数的选择使得 $\\hat{\\beta}_1  0$ 和 $\\hat{\\beta}_2  0$。这极大地简化了问题，因为我们可以设 $\\text{sgn}(\\hat{\\beta}) = (1, 1)^T = \\mathbf{1}$。\n该方程变成一个标准线性系统：\n$$ (G + \\lambda(1-\\alpha)I)\\hat{\\beta} = c - \\lambda\\alpha \\mathbf{1} $$\n让我们代入给定的矩阵和向量：\n$$ G = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}, \\quad c = \\begin{pmatrix} 1 \\\\ \\rho \\end{pmatrix}, \\quad I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad \\mathbf{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n左侧的矩阵是：\n$$ G + \\lambda(1-\\alpha)I = \\begin{pmatrix} 1+\\lambda(1-\\alpha)  \\rho \\\\ \\rho  1+\\lambda(1-\\alpha) \\end{pmatrix} $$\n为简化起见，我们定义一个常数 $A = 1+\\lambda(1-\\alpha)$。该矩阵为 $\\begin{pmatrix} A  \\rho \\\\ \\rho  A \\end{pmatrix}$。\n右侧的向量是：\n$$ c - \\lambda\\alpha \\mathbf{1} = \\begin{pmatrix} 1 \\\\ \\rho \\end{pmatrix} - \\lambda\\alpha \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 - \\lambda\\alpha \\\\ \\rho - \\lambda\\alpha \\end{pmatrix} $$\n我们必须解这个线性系统：\n$$ \\begin{pmatrix} A  \\rho \\\\ \\rho  A \\end{pmatrix} \\begin{pmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\lambda\\alpha \\\\ \\rho - \\lambda\\alpha \\end{pmatrix} $$\n为了求解 $\\hat{\\beta}$，我们求矩阵 $\\begin{pmatrix} A  \\rho \\\\ \\rho  A \\end{pmatrix}$ 的逆。其行列式为 $\\det = A^2-\\rho^2$。其逆矩阵为：\n$$ \\begin{pmatrix} A  \\rho \\\\ \\rho  A \\end{pmatrix}^{-1} = \\frac{1}{A^2-\\rho^2} \\begin{pmatrix} A  -\\rho \\\\ -\\rho  A \\end{pmatrix} $$\n现在我们可以求解 $\\hat{\\beta}$：\n$$ \\begin{pmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\frac{1}{A^2-\\rho^2} \\begin{pmatrix} A  -\\rho \\\\ -\\rho  A \\end{pmatrix} \\begin{pmatrix} 1 - \\lambda\\alpha \\\\ \\rho - \\lambda\\alpha \\end{pmatrix} $$\n我们可以写出 $\\hat{\\beta}_1$ 和 $\\hat{\\beta}_2$ 的解：\n$$ \\hat{\\beta}_1 = \\frac{A(1 - \\lambda\\alpha) - \\rho(\\rho - \\lambda\\alpha)}{A^2-\\rho^2} $$\n$$ \\hat{\\beta}_2 = \\frac{-\\rho(1 - \\lambda\\alpha) + A(\\rho - \\lambda\\alpha)}{A^2-\\rho^2} $$\n我们需要求 $|\\hat{\\beta}_1 - \\hat{\\beta}_2|$。让我们计算差值 $\\hat{\\beta}_1 - \\hat{\\beta}_2$：\n$$ \\hat{\\beta}_1 - \\hat{\\beta}_2 = \\frac{1}{A^2-\\rho^2} \\left[ \\left( A(1 - \\lambda\\alpha) - \\rho(\\rho - \\lambda\\alpha) \\right) - \\left( -\\rho(1 - \\lambda\\alpha) + A(\\rho - \\lambda\\alpha) \\right) \\right] $$\n让我们化简分子：\n$$ \\text{Numerator} = A(1 - \\lambda\\alpha) - \\rho(\\rho - \\lambda\\alpha) + \\rho(1 - \\lambda\\alpha) - A(\\rho - \\lambda\\alpha) $$\n$$ = A(1 - \\lambda\\alpha - \\rho + \\lambda\\alpha) + \\rho(1 - \\lambda\\alpha - \\rho + \\lambda\\alpha) $$\n$$ = A(1-\\rho) + \\rho(1-\\rho) $$\n$$ = (A+\\rho)(1-\\rho) $$\n所以，差值为：\n$$ \\hat{\\beta}_1 - \\hat{\\beta}_2 = \\frac{(A+\\rho)(1-\\rho)}{A^2-\\rho^2} $$\n分母可以因式分解为平方差：$A^2 - \\rho^2 = (A-\\rho)(A+\\rho)$。\n$$ \\hat{\\beta}_1 - \\hat{\\beta}_2 = \\frac{(A+\\rho)(1-\\rho)}{(A-\\rho)(A+\\rho)} $$\n因为 $\\lambda  0$ 且 $\\alpha \\in (0, 1)$，我们有 $\\lambda(1-\\alpha)  0$，所以 $A = 1+\\lambda(1-\\alpha)  1$。给定 $\\rho \\in (-1, 1)$，由此可知 $A+\\rho  1-1=0$。因此，我们可以安全地消去 $(A+\\rho)$ 项：\n$$ \\hat{\\beta}_1 - \\hat{\\beta}_2 = \\frac{1-\\rho}{A-\\rho} $$\n现在我们代回 $A = 1+\\lambda(1-\\alpha)$：\n$$ \\hat{\\beta}_1 - \\hat{\\beta}_2 = \\frac{1-\\rho}{1+\\lambda(1-\\alpha) - \\rho} $$\n我们需要求这个差的绝对值。让我们检查它的符号。\n分子是 $1-\\rho$。因为给定 $\\rho  1$，所以分子严格为正。\n分母是 $1-\\rho+\\lambda(1-\\alpha)$。因为 $1-\\rho > 0$ 且 $\\lambda(1-\\alpha) > 0$，所以分母也严格为正。\n因此，差值 $\\hat{\\beta}_1 - \\hat{\\beta}_2$ 是正的，这意味着 $\\hat{\\beta}_1 > \\hat{\\beta}_2$。\n这意味着绝对差就是差值本身：\n$$ |\\hat{\\beta}_1 - \\hat{\\beta}_2| = \\frac{1-\\rho}{1-\\rho+\\lambda(1-\\alpha)} $$\n这是作为 $\\rho$、$\\lambda$ 和 $\\alpha$ 函数的最终简化表达式。",
            "answer": "$$\n\\boxed{\\frac{1-\\rho}{1-\\rho+\\lambda(1-\\alpha)}}\n$$"
        }
    ]
}