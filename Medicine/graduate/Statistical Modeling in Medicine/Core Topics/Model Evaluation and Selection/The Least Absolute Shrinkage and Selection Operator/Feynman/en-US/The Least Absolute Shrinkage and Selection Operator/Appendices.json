{
    "hands_on_practices": [
        {
            "introduction": "While modern software makes fitting a LASSO model instantaneous, a deep understanding comes from tracing the algorithm's steps. This first exercise demystifies the optimization process by guiding you through a single pass of cyclic coordinate descent, the workhorse algorithm for LASSO. By manually applying the soft-thresholding update rule, you will gain a concrete intuition for how LASSO iteratively shrinks coefficients and, crucially, enforces sparsity by setting some to exactly zero .",
            "id": "4990069",
            "problem": "A clinical researcher is studying a continuous outcome representing standardized systolic blood pressure residuals in a cohort of $n=6$ patients. Three candidate predictors $p=3$ (standardized laboratory biomarkers) have been collected. Let the design matrix $X \\in \\mathbb{R}^{6 \\times 3}$ have columns standardized so that each column has mean $0$ and satisfies $(1/n)\\sum_{i=1}^{n} x_{ij}^{2} = 1$ for $j \\in \\{1,2,3\\}$, and let the response vector $y \\in \\mathbb{R}^{6}$ be centered to mean $0$. Consider the Least Absolute Shrinkage and Selection Operator (LASSO) regression, which estimates coefficients $\\beta \\in \\mathbb{R}^{3}$ by minimizing the penalized squared-error objective with squared-loss and an $\\ell_{1}$ penalty.\n\nYou are given\n$$\nX = \n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n-1 & -1 & -1 \\\\\n-1 & 1 & 1 \\\\\n-1 & -1 & -1\n\\end{pmatrix}, \n\\qquad\ny = \n\\begin{pmatrix}\n4 \\\\ 2 \\\\ 4 \\\\ -2 \\\\ -4 \\\\ -4\n\\end{pmatrix},\n\\qquad\n\\lambda = \\frac{3}{2}.\n$$\nVerify that each column of $X$ has mean $0$ and $(1/n)\\sum_{i=1}^{n} x_{ij}^{2} = 1$, and that $y$ has mean $0$. Starting from the core definition of the LASSO objective for linear regression with centered response and standardized predictors, perform one full cyclic coordinate descent pass over $j=1,2,3$ beginning at the initial coefficient vector $\\beta^{(0)} = (0,0,0)$. At each coordinate, derive the coordinate-wise update rule from first principles and then compute the updated coefficient exactly by hand.\n\nExpress the final updated coefficient vector after this single pass as exact rational numbers, with no rounding, in the form of a $1 \\times 3$ row vector. No units are required.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to proceed with a solution. First, we verify the preliminary conditions stated in the problem.\n\nThe number of patients is $n=6$. The number of predictors is $p=3$.\nThe design matrix $X$ and response vector $y$ are given as:\n$$\nX = \n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n-1 & -1 & -1 \\\\\n-1 & 1 & 1 \\\\\n-1 & -1 & -1\n\\end{pmatrix}, \n\\qquad\ny = \n\\begin{pmatrix}\n4 \\\\ 2 \\\\ 4 \\\\ -2 \\\\ -4 \\\\ -4\n\\end{pmatrix}\n$$\n\nVerification of column means of $X$:\nFor $j=1$: $\\sum_{i=1}^{6} x_{i1} = 1 + 1 + 1 - 1 - 1 - 1 = 0$. Mean is $\\frac{0}{6} = 0$.\nFor $j=2$: $\\sum_{i=1}^{6} x_{i2} = 1 - 1 + 1 - 1 + 1 - 1 = 0$. Mean is $\\frac{0}{6} = 0$.\nFor $j=3$: $\\sum_{i=1}^{6} x_{i3} = 1 + 1 - 1 - 1 + 1 - 1 = 0$. Mean is $\\frac{0}{6} = 0$.\n\nVerification of column standardization of $X$:\nFor $j=1$: $\\frac{1}{n}\\sum_{i=1}^{6} x_{i1}^{2} = \\frac{1}{6}(1^2 + 1^2 + 1^2 + (-1)^2 + (-1)^2 + (-1)^2) = \\frac{6}{6} = 1$.\nFor $j=2$: $\\frac{1}{n}\\sum_{i=1}^{6} x_{i2}^{2} = \\frac{1}{6}(1^2 + (-1)^2 + 1^2 + (-1)^2 + 1^2 + (-1)^2) = \\frac{6}{6} = 1$.\nFor $j=3$: $\\frac{1}{n}\\sum_{i=1}^{6} x_{i3}^{2} = \\frac{1}{6}(1^2 + 1^2 + (-1)^2 + (-1)^2 + 1^2 + (-1)^2) = \\frac{6}{6} = 1$.\n\nVerification of mean of $y$:\n$\\sum_{i=1}^{6} y_i = 4 + 2 + 4 - 2 - 4 - 4 = 0$. Mean is $\\frac{0}{6} = 0$.\nAll initial conditions are verified.\n\nThe LASSO objective function for standardized predictors and a centered response is:\n$$\nL(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left(y_i - \\sum_{k=1}^{p} x_{ik}\\beta_k\\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n$$\nCoordinate descent optimizes the objective function with respect to a single coefficient $\\beta_j$ at a time, holding all other coefficients $\\beta_k$ (for $k \\neq j$) fixed.\nLet's isolate the terms in $L(\\beta)$ that depend on $\\beta_j$:\n$$\nL(\\beta_j) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left(y_i - \\sum_{k \\neq j} x_{ik}\\beta_k - x_{ij}\\beta_j\\right)^2 + \\lambda |\\beta_j| + \\text{const}\n$$\nDefine the partial residual $r_{i,(-j)} = y_i - \\sum_{k \\neq j} x_{ik}\\beta_k$. The expression to minimize with respect to $\\beta_j$ becomes:\n$$\nf(\\beta_j) = \\frac{1}{2n} \\sum_{i=1}^{n} (r_{i,(-j)} - x_{ij}\\beta_j)^2 + \\lambda |\\beta_j|\n$$\nExpanding the squared term:\n$$\nf(\\beta_j) = \\frac{1}{2n} \\left( \\sum_{i=1}^{n} r_{i,(-j)}^2 - 2\\beta_j \\sum_{i=1}^{n} r_{i,(-j)}x_{ij} + \\beta_j^2 \\sum_{i=1}^{n} x_{ij}^2 \\right) + \\lambda |\\beta_j|\n$$\nUsing the standardization property $\\sum_{i=1}^{n} x_{ij}^2 = n$:\n$$\nf(\\beta_j) = \\frac{1}{2n} \\left( C - 2\\beta_j \\sum_{i=1}^{n} r_{i,(-j)}x_{ij} + n\\beta_j^2 \\right) + \\lambda |\\beta_j|\n$$\nwhere $C$ is a constant with respect to $\\beta_j$. Ignoring constant terms, we want to minimize:\n$$\ng(\\beta_j) = \\frac{1}{2}\\beta_j^2 - \\left(\\frac{1}{n} \\sum_{i=1}^{n} r_{i,(-j)}x_{ij}\\right) \\beta_j + \\lambda |\\beta_j|\n$$\nLet $S_j = \\frac{1}{n} \\sum_{i=1}^{n} r_{i,(-j)}x_{ij} = \\frac{1}{n}X_j^T r_{(-j)}$, where $X_j$ is the $j$-th column of $X$. We minimize $g(\\beta_j) = \\frac{1}{2}\\beta_j^2 - S_j \\beta_j + \\lambda |\\beta_j|$.\nThis is a convex function. We find the minimum by setting its subgradient to zero. The subgradient $\\partial g(\\beta_j)$ is:\n$$\n\\partial g(\\beta_j) = \\beta_j - S_j + \\lambda \\cdot \\partial|\\beta_j|\n$$\nwhere $\\partial|\\beta_j|$ is $\\text{sgn}(\\beta_j)$ if $\\beta_j \\neq 0$ and the interval $[-1, 1]$ if $\\beta_j = 0$.\nSetting the subgradient to $0$:\n1.  If $\\beta_j > 0$: $\\beta_j - S_j + \\lambda = 0 \\implies \\hat{\\beta}_j = S_j - \\lambda$. This is valid only if $S_j - \\lambda > 0$, i.e., $S_j > \\lambda$.\n2.  If $\\beta_j  0$: $\\beta_j - S_j - \\lambda = 0 \\implies \\hat{\\beta}_j = S_j + \\lambda$. This is valid only if $S_j + \\lambda  0$, i.e., $S_j  -\\lambda$.\n3.  If $\\hat{\\beta}_j = 0$: $0 - S_j + \\lambda \\cdot [-1, 1] = 0 \\implies S_j \\in [-\\lambda, \\lambda]$, i.e., $|S_j| \\le \\lambda$.\n\nThese three cases define the soft-thresholding function:\n$$\n\\hat{\\beta}_j = \\text{ST}(S_j, \\lambda) = \\text{sgn}(S_j) \\max(0, |S_j| - \\lambda)\n$$\nWe are given $\\lambda = \\frac{3}{2}$ and we start with $\\beta^{(0)} = (0, 0, 0)^T$.\n\n**Coordinate 1: Update $\\beta_1$**\nWe update $\\beta_1$ with $\\beta_2=0$ and $\\beta_3=0$. The partial residual is $r_{(-1)} = y - X_2\\beta_2^{(0)} - X_3\\beta_3^{(0)} = y$.\nWe compute $S_1 = \\frac{1}{n} \\sum_{i=1}^n x_{i1}y_i = \\frac{1}{n}X_1^T y$.\n$$\nX_1^T y = 1(4) + 1(2) + 1(4) + (-1)(-2) + (-1)(-4) + (-1)(-4) = 4 + 2 + 4 + 2 + 4 + 4 = 20\n$$\n$$\nS_1 = \\frac{20}{6} = \\frac{10}{3}\n$$\nWe have $\\lambda = \\frac{3}{2}$. Since $S_1 = \\frac{10}{3} > \\frac{3}{2}$ (as $\\frac{20}{6} > \\frac{9}{6}$), we use the update rule $\\hat{\\beta}_1 = S_1 - \\lambda$.\n$$\n\\beta_1^{(1)} = \\frac{10}{3} - \\frac{3}{2} = \\frac{20 - 9}{6} = \\frac{11}{6}\n$$\nThe current coefficient vector is $\\beta = (\\frac{11}{6}, 0, 0)^T$.\n\n**Coordinate 2: Update $\\beta_2$**\nWe update $\\beta_2$ with $\\beta_1 = \\beta_1^{(1)} = \\frac{11}{6}$ and $\\beta_3 = \\beta_3^{(0)} = 0$.\nThe partial residual is $r_{(-2)} = y - X_1\\beta_1^{(1)} - X_3\\beta_3^{(0)} = y - X_1\\frac{11}{6}$.\nWe compute $S_2 = \\frac{1}{n}X_2^T r_{(-2)} = \\frac{1}{n}X_2^T(y - X_1\\frac{11}{6}) = \\frac{1}{n}X_2^T y - \\frac{1}{n}(X_2^T X_1)\\beta_1^{(1)}$.\nFirst, we compute the necessary inner products:\n$$\nX_2^T y = 1(4) + (-1)(2) + 1(4) + (-1)(-2) + 1(-4) + (-1)(-4) = 4 - 2 + 4 + 2 - 4 + 4 = 8\n$$\n$$\nX_2^T X_1 = 1(1) + (-1)(1) + 1(1) + (-1)(-1) + 1(-1) + (-1)(-1) = 1 - 1 + 1 + 1 - 1 + 1 = 2\n$$\nNow, we compute $S_2$:\n$$\nS_2 = \\frac{8}{6} - \\frac{2}{6} \\cdot \\frac{11}{6} = \\frac{4}{3} - \\frac{1}{3} \\cdot \\frac{11}{6} = \\frac{4}{3} - \\frac{11}{18} = \\frac{24 - 11}{18} = \\frac{13}{18}\n$$\nWe compare $|S_2| = \\frac{13}{18}$ with $\\lambda = \\frac{3}{2} = \\frac{27}{18}$. Since $|S_2| \\le \\lambda$, the update is $\\hat{\\beta}_2 = 0$.\n$$\n\\beta_2^{(1)} = 0\n$$\nThe current coefficient vector is $\\beta = (\\frac{11}{6}, 0, 0)^T$.\n\n**Coordinate 3: Update $\\beta_3$**\nWe update $\\beta_3$ with $\\beta_1 = \\beta_1^{(1)} = \\frac{11}{6}$ and $\\beta_2 = \\beta_2^{(1)} = 0$.\nThe partial residual is $r_{(-3)} = y - X_1\\beta_1^{(1)} - X_2\\beta_2^{(1)} = y - X_1\\frac{11}{6}$.\nWe compute $S_3 = \\frac{1}{n}X_3^T r_{(-3)} = \\frac{1}{n}X_3^T y - \\frac{1}{n}(X_3^T X_1)\\beta_1^{(1)} - \\frac{1}{n}(X_3^T X_2)\\beta_2^{(1)}$.\nSince $\\beta_2^{(1)}=0$, this simplifies to $S_3 = \\frac{1}{n}X_3^T y - \\frac{1}{n}(X_3^T X_1)\\beta_1^{(1)}$.\nFirst, we compute the necessary inner products:\n$$\nX_3^T y = 1(4) + 1(2) + (-1)(4) + (-1)(-2) + 1(-4) + (-1)(-4) = 4 + 2 - 4 + 2 - 4 + 4 = 4\n$$\n$$\nX_3^T X_1 = 1(1) + 1(1) + (-1)(1) + (-1)(-1) + 1(-1) + (-1)(-1) = 1 + 1 - 1 + 1 - 1 + 1 = 2\n$$\nNow, we compute $S_3$:\n$$\nS_3 = \\frac{4}{6} - \\frac{2}{6} \\cdot \\frac{11}{6} = \\frac{2}{3} - \\frac{1}{3} \\cdot \\frac{11}{6} = \\frac{2}{3} - \\frac{11}{18} = \\frac{12 - 11}{18} = \\frac{1}{18}\n$$\nWe compare $|S_3| = \\frac{1}{18}$ with $\\lambda = \\frac{3}{2} = \\frac{27}{18}$. Since $|S_3| \\le \\lambda$, the update is $\\hat{\\beta}_3 = 0$.\n$$\n\\beta_3^{(1)} = 0\n$$\nAfter one full cyclic pass, the updated coefficient vector is $\\beta^{(1)} = (\\beta_1^{(1)}, \\beta_2^{(1)}, \\beta_3^{(1)}) = (\\frac{11}{6}, 0, 0)$.\n\nAs a $1 \\times 3$ row vector, the final answer is $(\\frac{11}{6} \\quad 0 \\quad 0)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{11}{6}  0  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Having seen how the LASSO solution is computed for a single given $\\lambda$, we now address a practical question: which values of $\\lambda$ should we investigate? This exercise connects the Karush-Kuhn-Tucker (KKT) optimality conditions to the concept of the regularization path, starting with the calculation of $\\lambda_{\\max}$—the critical point above which all coefficients are zero . Understanding how to define this path is fundamental to exploring the trade-off between model complexity and fit, which is the essence of regularization.",
            "id": "4990123",
            "problem": "A clinical research team is building a sparse linear risk model for a continuous biomarker using the least absolute shrinkage and selection operator (LASSO) in a cohort of adult patients. Let the data be represented by a response vector $y \\in \\mathbb{R}^{n}$ and a design matrix $X \\in \\mathbb{R}^{n \\times p}$ with $n = 800$ and $p = 6$. The columns of $X$ are centered and standardized to have Euclidean norm $\\sqrt{n}$, and the response $y$ is centered. The team fits the LASSO by minimizing the objective\n$$\n\\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nwhere $\\beta \\in \\mathbb{R}^{p}$ and $\\lambda \\ge 0$.\n\nFrom the electronic health record extract, the team has computed the empirical inner products\n$$\n\\frac{1}{n}X^{\\top}y = \\begin{pmatrix}0.238 \\\\ -0.411 \\\\ 0.527 \\\\ 0.105 \\\\ -0.367 \\\\ 0.492\\end{pmatrix}.\n$$\n\nStarting from the first-order optimality conditions (Karush–Kuhn–Tucker (KKT) conditions) for the LASSO, derive an expression for the smallest regularization value $\\lambda_{\\max}$ such that the zero vector $\\hat{\\beta}(\\lambda_{\\max}) = 0$ is an optimal solution. Then, using the provided empirical inner products, compute the numerical value of $\\lambda_{\\max}$.\n\nNext, construct a grid of $K = 100$ regularization values $\\{\\lambda_{k}\\}_{k=1}^{K}$ spanning from $\\lambda_{1} = \\lambda_{\\max}$ down to $\\lambda_{K} = \\lambda_{\\min}$ with $\\lambda_{\\min} = 10^{-3}\\lambda_{\\max}$, using a geometric progression $\\lambda_{k} = \\lambda_{\\max} \\, r^{\\,k-1}$ with common ratio $r \\in (0,1)$. Explain, based on the structure of the LASSO solution path and variable entry events, the rationale for concentrating grid density near the “active” region where predictors begin to enter the model.\n\nFinally, report the common ratio $r$ for this $K=100$ geometric grid, rounded to four significant figures. Your final answer must be this single real number (no units).",
            "solution": "The user-provided problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   **Data representation**: Response vector $y \\in \\mathbb{R}^{n}$, design matrix $X \\in \\mathbb{R}^{n \\times p}$.\n-   **Dimensions**: Number of samples $n = 800$, number of predictors $p = 6$.\n-   **Data preprocessing**: Columns of $X$ are centered and standardized to have Euclidean norm $\\sqrt{n}$. The response $y$ is centered.\n-   **LASSO objective function**: Minimize $\\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$ with respect to $\\beta \\in \\mathbb{R}^{p}$, for a regularization parameter $\\lambda \\ge 0$.\n-   **Empirical inner products**: $\\frac{1}{n}X^{\\top}y = \\begin{pmatrix}0.238 \\\\ -0.411 \\\\ 0.527 \\\\ 0.105 \\\\ -0.367 \\\\ 0.492\\end{pmatrix}$.\n-   **Task 1**: Find the smallest regularization value $\\lambda_{\\max}$ for which $\\hat{\\beta}(\\lambda_{\\max}) = 0$ is an optimal solution.\n-   **Task 2**: Construct a geometric grid of $K = 100$ regularization values from $\\lambda_{1} = \\lambda_{\\max}$ to $\\lambda_{K} = \\lambda_{\\min} = 10^{-3}\\lambda_{\\max}$, with $\\lambda_{k} = \\lambda_{\\max} \\, r^{\\,k-1}$.\n-   **Task 3**: Explain the rationale for grid density in LASSO.\n-   **Task 4**: Report the common ratio $r$ rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in statistical learning, specifically concerning the properties of the LASSO estimator.\n-   **Scientifically Grounded**: The problem is based on the established mathematical theory of LASSO regression and convex optimization. The objective function, KKT conditions, and the concept of a regularization path are fundamental to the field.\n-   **Well-Posed**: All necessary data and definitions are provided to derive $\\lambda_{\\max}$ and the common ratio $r$. The questions are specific and have unique, verifiable answers.\n-   **Objective**: The problem is stated in precise mathematical language, free from subjectivity or ambiguity. The given numerical values are plausible.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Derivation and Calculation of $\\lambda_{\\max}$\nThe LASSO objective function is $L(\\beta) = f(\\beta) + g(\\beta)$, where $f(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2}$ is the differentiable loss term and $g(\\beta) = \\lambda \\|\\beta\\|_{1}$ is the non-differentiable penalty term.\n\nThe first-order optimality conditions, also known as the Karush-Kuhn-Tucker (KKT) conditions, for a solution $\\hat{\\beta}$ state that the zero vector must be in the sum of the gradient of the smooth part and the subdifferential of the non-smooth part, evaluated at $\\hat{\\beta}$:\n$$\n0 \\in \\nabla f(\\hat{\\beta}) + \\partial g(\\hat{\\beta})\n$$\nThe gradient of $f(\\beta)$ is:\n$$\n\\nabla f(\\beta) = \\frac{1}{n} (X^{\\top}X\\beta - X^{\\top}y)\n$$\nThe subdifferential of $g(\\beta) = \\lambda \\sum_{j=1}^{p} |\\beta_j|$ is the set of vectors $s \\in \\mathbb{R}^p$ such that for each component $j$:\n$$\ns_j = \\begin{cases} \\lambda \\, \\text{sgn}(\\beta_j)  \\text{if } \\beta_j \\neq 0 \\\\ \\in [-\\lambda, \\lambda]  \\text{if } \\beta_j = 0 \\end{cases}\n$$\nThe KKT conditions are therefore equivalent to finding a subgradient vector $s \\in \\partial g(\\hat{\\beta})$ such that $\\nabla f(\\hat{\\beta}) + s = 0$, which implies:\n$$\n\\frac{1}{n}(X^{\\top}y - X^{\\top}X\\hat{\\beta}) = s\n$$\nWe are looking for the value $\\lambda_{\\max}$ such that the zero vector $\\hat{\\beta} = 0$ is an optimal solution. Substituting $\\hat{\\beta} = 0$ into the KKT conditions gives:\n$$\n\\frac{1}{n}X^{\\top}y = s\n$$\nFor $\\hat{\\beta} = 0$, the condition on the subgradient vector $s$ is that $s_j \\in [-\\lambda_{\\max}, \\lambda_{\\max}]$ for all components $j=1, \\dots, p$. This translates to the component-wise condition:\n$$\n\\left| \\left(\\frac{1}{n}X^{\\top}y\\right)_j \\right| \\le \\lambda_{\\max} \\quad \\text{for all } j=1, \\dots, p\n$$\nFor this system of inequalities to hold, $\\lambda_{\\max}$ must be greater than or equal to the largest absolute value among the components of the vector $\\frac{1}{n}X^{\\top}y$. The smallest value of $\\lambda$ for which $\\hat{\\beta}=0$ is a solution is therefore the one that makes this inequality tight. This yields the expression for $\\lambda_{\\max}$:\n$$\n\\lambda_{\\max} = \\max_{j=1,\\dots,p} \\left| \\left(\\frac{1}{n}X^{\\top}y\\right)_j \\right| = \\left\\|\\frac{1}{n}X^{\\top}y\\right\\|_{\\infty}\n$$\nUsing the provided numerical data:\n$$\n\\frac{1}{n}X^{\\top}y = \\begin{pmatrix}0.238 \\\\ -0.411 \\\\ 0.527 \\\\ 0.105 \\\\ -0.367 \\\\ 0.492\\end{pmatrix}\n$$\nWe compute the absolute value of each component:\n$$\n\\begin{pmatrix}|0.238| \\\\ |-0.411| \\\\ |0.527| \\\\ |0.105| \\\\ |-0.367| \\\\ |0.492|\\end{pmatrix} = \\begin{pmatrix}0.238 \\\\ 0.411 \\\\ 0.527 \\\\ 0.105 \\\\ 0.367 \\\\ 0.492\\end{pmatrix}\n$$\nThe maximum value in this vector is $0.527$. Thus,\n$$\n\\lambda_{\\max} = 0.527\n$$\n\n### Rationale for Regularization Grid Density\nThe LASSO solution path, which traces the coefficient vector $\\hat{\\beta}(\\lambda)$ as $\\lambda$ varies, is central to understanding the model selection process. For any $\\lambda \\ge \\lambda_{\\max}$, all coefficients are zero. At $\\lambda = \\lambda_{\\max}$, the first predictor enters the model. As $\\lambda$ is further decreased, more predictors become non-zero at specific threshold values of $\\lambda$. These events, where variables enter the \"active set\" of non-zero coefficients, are the defining feature of LASSO's variable selection capability.\n\nThe most significant changes in model structure—namely, the selection of predictors—occur in the region of high regularization, for $\\lambda$ values near $\\lambda_{\\max}$. It is in this \"active\" region that the model transitions from a null model to a sparse model containing the most influential predictors. Concentrating the grid density in this region by using a fine-grained set of $\\lambda$ values allows for a precise characterization of the sequence in which variables enter the model and how their coefficient paths begin to evolve. This detailed view is crucial for understanding relative predictor importance and for selecting a parsimonious model via techniques like cross-validation. In contrast, as $\\lambda \\to 0$, the set of active predictors tends to stabilize, and the coefficient values converge towards their unpenalized ordinary least squares estimates; changes in this region are less about model selection and more about coefficient shrinkage, making a coarser grid adequate.\n\n### Calculation of the Common Ratio $r$\nThe problem specifies a grid of $K=100$ regularization values, $\\{\\lambda_k\\}_{k=1}^{100}$, defined by a geometric progression:\n$$\n\\lambda_k = \\lambda_{\\max} \\, r^{k-1}\n$$\nThe grid starts at $\\lambda_1 = \\lambda_{\\max}$ (since $r^{1-1} = r^0 = 1$) and ends at $\\lambda_{100} = \\lambda_{\\min}$. We are given the condition that $\\lambda_{\\min} = 10^{-3}\\lambda_{\\max}$.\nWe can set up an equation for the last point in the grid, $k=K=100$:\n$$\n\\lambda_{100} = \\lambda_{\\max} \\, r^{100-1} = \\lambda_{\\max} \\, r^{99}\n$$\nSubstituting the given relation for $\\lambda_{100}$:\n$$\n10^{-3}\\lambda_{\\max} = \\lambda_{\\max} \\, r^{99}\n$$\nAssuming $\\lambda_{\\max} > 0$, which we have shown to be true, we can divide both sides by $\\lambda_{\\max}$:\n$$\nr^{99} = 10^{-3}\n$$\nTo solve for the common ratio $r$, we take the $99$-th root of both sides:\n$$\nr = (10^{-3})^{1/99} = 10^{-3/99} = 10^{-1/33}\n$$\nNow, we compute the numerical value:\n$$\nr = 10^{-1/33} \\approx 0.93259101\n$$\nRounding to four significant figures, we get:\n$$\nr \\approx 0.9326\n$$\nThis is the common ratio required for the geometric grid.",
            "answer": "$$\\boxed{0.9326}$$"
        },
        {
            "introduction": "The central promise of LASSO in fields like genomics and clinical prediction is its ability to perform variable selection. However, this ability is not unconditional. This final exercise delves into the theoretical guarantees of LASSO by asking you to compute the 'irrepresentable quantity' for a hypothetical model, a value that determines if the famed Irrepresentable Condition is met . Successfully navigating this problem will provide insight into a critical concept: LASSO's selection consistency depends fundamentally on the correlation structure of the predictors themselves.",
            "id": "4990086",
            "problem": "A clinical research team is constructing a linear risk score for a continuous cardiovascular outcome using $p=4$ standardized biomarker predictors across $n=6$ patients. The predictors have been centered to zero mean and scaled to unit variance so that the empirical Gram matrix equals the sample covariance matrix $\\Sigma = \\frac{1}{n} X^{\\top} X$. The design matrix $X \\in \\mathbb{R}^{6 \\times 4}$ is given by\n$$\nX \\;=\\;\n\\begin{bmatrix}\n1  1  1  1 \\\\\n1  1  -1  -1 \\\\\n1  -1  1  -1 \\\\\n-1  -1  -1  1 \\\\\n-1  1  1  1 \\\\\n-1  -1  -1  -1\n\\end{bmatrix}.\n$$\nAssume the data follow the fixed-design linear model $y = X \\beta^{0} + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the outcome, $\\beta^{0} \\in \\mathbb{R}^{p}$ is the true coefficient vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ is mean-zero noise. The scientific prior indicates the true support (set of nonzero coefficients) is $S = \\{1, 3\\}$, with the signs $\\operatorname{sign}(\\beta^{0}_{1}) = +1$ and $\\operatorname{sign}(\\beta^{0}_{3}) = -1$. The team intends to use the Least Absolute Shrinkage and Selection Operator (LASSO) to perform variable selection by solving\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2n} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nfor a positive tuning parameter $\\lambda$. Using first principles of convex optimization and the Karush–Kuhn–Tucker (KKT) conditions, derive from the fixed-design model and the covariance structure $\\Sigma$ the inactive-set feasibility quantity that governs exact selection recovery in the asymptotic, fixed-design regime, and compute its value for the given $X$ and the stated $S$ and sign pattern. Based on this value, assess whether exact selection consistency is achievable under the usual regularity assumptions (e.g., signals bounded away from zero and vanishing noise correlations). Express the computed quantity as a single exact real number. No rounding is required.",
            "solution": "The problem requires the derivation and computation of the inactive-set feasibility quantity that governs exact support recovery for the Least Absolute Shrinkage and Selection Operator (LASSO). This quantity arises from the Karush–Kuhn–Tucker (KKT) conditions for the LASSO optimization problem.\n\nThe LASSO estimator $\\hat{\\beta}$ is the solution to the convex optimization problem:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; L(\\beta) = \\frac{1}{2n} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}\n$$\nThe optimality condition is that the zero vector must be in the subgradient of the objective function $L(\\beta)$ at the solution $\\hat{\\beta}$, i.e., $0 \\in \\partial L(\\hat{\\beta})$. The subgradient is given by:\n$$\n\\partial L(\\beta) = -\\frac{1}{n} X^{\\top}(y - X\\beta) + \\lambda \\partial \\|\\beta\\|_{1}\n$$\nwhere $\\partial \\|\\beta\\|_{1}$ is the subgradient of the $\\ell_1$-norm. Its $j$-th component, denoted $(\\partial \\|\\beta\\|_{1})_j$, is:\n$$\n(\\partial \\|\\beta\\|_{1})_j = \\begin{cases} \\operatorname{sign}(\\beta_j)  \\text{if } \\beta_j \\neq 0 \\\\ [-1, 1]  \\text{if } \\beta_j = 0 \\end{cases}\n$$\nThe KKT conditions at the solution $\\hat{\\beta}$ can be stated as:\n1. For any active index $j \\in \\hat{S} = \\{k : \\hat{\\beta}_k \\neq 0\\}$, the subgradient is uniquely defined, and we have the stationarity condition:\n   $$ \\frac{1}{n} X_j^{\\top}(y - X\\hat{\\beta}) = \\lambda \\operatorname{sign}(\\hat{\\beta}_j) $$\n2. For any inactive index $j \\in \\hat{S}^c = \\{k : \\hat{\\beta}_k = 0\\}$, the subgradient must contain zero, which implies the feasibility condition:\n   $$ \\left| \\frac{1}{n} X_j^{\\top}(y - X\\hat{\\beta}) \\right| \\leq \\lambda $$\n\nWe are interested in the conditions for exact support recovery, meaning the estimated support $\\hat{S}$ is identical to the true support $S = \\{1, 3\\}$. Let's assume $\\hat{S} = S$ and investigate the implications. In this case, $\\hat{\\beta}_j \\neq 0$ for $j \\in S$ and $\\hat{\\beta}_j = 0$ for $j \\in S^c = \\{2, 4\\}$.\nWe partition the problem according to $S$ and $S^c$: $X = [X_S, X_{S^c}]$, $\\beta = (\\beta_S, \\beta_{S^c})$.\nSince $\\hat{\\beta}_{S^c} = 0$, the KKT conditions become:\n1. For the active set $S$: $\\frac{1}{n} X_S^{\\top}(y - X_S \\hat{\\beta}_S) = \\lambda z_S$, where $z_S = \\operatorname{sign}(\\hat{\\beta}_S)$.\n2. For the inactive set $S^c$: $|\\frac{1}{n} X_{S^c}^{\\top}(y - X_S \\hat{\\beta}_S)| \\leq \\lambda$ (element-wise).\n\nFrom the first condition, we can solve for $\\hat{\\beta}_S$:\n$$\n\\frac{1}{n} X_S^{\\top}y - \\left(\\frac{1}{n} X_S^{\\top}X_S\\right) \\hat{\\beta}_S = \\lambda z_S\n$$\n$$\n\\Sigma_{SS} \\hat{\\beta}_S = \\frac{1}{n} X_S^{\\top}y - \\lambda z_S \\implies \\hat{\\beta}_S = \\Sigma_{SS}^{-1}\\left(\\frac{1}{n} X_S^{\\top}y - \\lambda z_S\\right)\n$$\nwhere $\\Sigma = \\frac{1}{n}X^{\\top}X$ is the sample covariance matrix.\nSubstituting the true model $y = X\\beta^0 + \\varepsilon = X_S \\beta^0_S + \\varepsilon$ (since $\\beta^0_{S^c}=0$):\n$$\n\\hat{\\beta}_S = \\Sigma_{SS}^{-1}\\left(\\frac{1}{n} X_S^{\\top}(X_S \\beta^0_S + \\varepsilon) - \\lambda z_S\\right) = \\beta^0_S + \\Sigma_{SS}^{-1}\\left(\\frac{1}{n} X_S^{\\top}\\varepsilon - \\lambda z_S\\right)\n$$\nFor exact recovery to hold for a range of $\\lambda$ and under typical noise assumptions, we require $\\operatorname{sign}(\\hat{\\beta}_S) = \\operatorname{sign}(\\beta^0_S)$. Let $z_S^0 = \\operatorname{sign}(\\beta^0_S)$. Assuming this consistency holds, we have $z_S = z_S^0$.\n\nNow we analyze the second (feasibility) condition. The argument of the absolute value is:\n$$\n\\frac{1}{n} X_{S^c}^{\\top}(y - X_S \\hat{\\beta}_S) = \\frac{1}{n} X_{S^c}^{\\top}(X_S \\beta^0_S + \\varepsilon - X_S \\hat{\\beta}_S)\n$$\nSubstituting $\\hat{\\beta}_S = \\beta^0_S + \\Sigma_{SS}^{-1}(\\frac{1}{n} X_S^{\\top}\\varepsilon - \\lambda z_S^0)$:\n$$\ny - X_S \\hat{\\beta}_S = X_S \\beta^0_S + \\varepsilon - X_S\\left(\\beta^0_S + \\Sigma_{SS}^{-1}\\left(\\frac{1}{n} X_S^{\\top}\\varepsilon - \\lambda z_S^0\\right)\\right) = \\varepsilon - X_S\\Sigma_{SS}^{-1}\\frac{1}{n} X_S^{\\top}\\varepsilon + \\lambda X_S\\Sigma_{SS}^{-1}z_S^0\n$$\nPremultiplying by $\\frac{1}{n}X_{S^c}^{\\top}$:\n$$\n\\frac{1}{n} X_{S^c}^{\\top}(y - X_S \\hat{\\beta}_S) = \\frac{1}{n}X_{S^c}^{\\top}\\left(I - X_S\\Sigma_{SS}^{-1}\\frac{1}{n} X_S^{\\top}\\right)\\varepsilon + \\lambda \\left(\\frac{1}{n}X_{S^c}^{\\top}X_S\\right)\\Sigma_{SS}^{-1}z_S^0\n$$\n$$\n\\frac{1}{n} X_{S^c}^{\\top}(y - X_S \\hat{\\beta}_S) = \\text{noise term} + \\lambda \\Sigma_{S^c S} \\Sigma_{SS}^{-1}z_S^0\n$$\nThe feasibility condition $|\\frac{1}{n} X_{S^c}^{\\top}(y - X_S \\hat{\\beta}_S)| \\leq \\lambda$ becomes:\n$$\n|\\Sigma_{S^c S} \\Sigma_{SS}^{-1}z_S^0 + \\frac{1}{\\lambda}(\\text{noise term})| \\leq 1\n$$\nIn the asymptotic fixed-design regime, the influence of the noise term is assumed to vanish. For the condition to hold robustly, the deterministic part must satisfy a strict inequality. This leads to the \"Irrepresentable Condition\":\n$$\n\\| \\Sigma_{S^c S} \\Sigma_{SS}^{-1}z_S^0 \\|_{\\infty}  1\n$$\nThe quantity to be computed is this $\\ell_{\\infty}$-norm, which we denote by $\\eta$.\n$\\eta = \\| \\Sigma_{S^c S} \\Sigma_{SS}^{-1}z_S^0 \\|_{\\infty}$.\n\nWe are given $n=6$, $S=\\{1, 3\\}$, and $S^c=\\{2, 4\\}$. The sign vector for $S$ is $z_S^0 = \\begin{pmatrix} \\operatorname{sign}(\\beta_1^0) \\\\ \\operatorname{sign}(\\beta_3^0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nThe design matrix is $X = \\begin{bmatrix} X_1  X_2  X_3  X_4 \\end{bmatrix}$. The columns of $X$ corresponding to $S$ and $S^c$ are:\n$$\nX_S = \\begin{bmatrix} 1  1 \\\\ 1  -1 \\\\ 1  1 \\\\ -1  -1 \\\\ -1  1 \\\\ -1  -1 \\end{bmatrix}, \\quad X_{S^c} = \\begin{bmatrix} 1  1 \\\\ 1  -1 \\\\ -1  -1 \\\\ -1  1 \\\\ 1  1 \\\\ -1  -1 \\end{bmatrix}\n$$\nFirst, we compute $\\Sigma_{SS} = \\frac{1}{n}X_S^{\\top}X_S$. Since the columns of $X$ are standardized, the diagonal entries of $\\Sigma$ are $1$.\n$$\nX_1^{\\top}X_3 = 1(1) + 1(-1) + 1(1) + (-1)(-1) + (-1)(1) + (-1)(-1) = 1-1+1+1-1+1 = 2\n$$\n$$\n\\Sigma_{SS} = \\frac{1}{6} \\begin{bmatrix} 6  2 \\\\ 2  6 \\end{bmatrix} = \\begin{bmatrix} 1  1/3 \\\\ 1/3  1 \\end{bmatrix}\n$$\nNext, we compute the inverse $\\Sigma_{SS}^{-1}$:\n$$\n\\det(\\Sigma_{SS}) = 1 \\cdot 1 - (\\frac{1}{3})^2 = 1 - \\frac{1}{9} = \\frac{8}{9}\n$$\n$$\n\\Sigma_{SS}^{-1} = \\frac{1}{8/9} \\begin{bmatrix} 1  -1/3 \\\\ -1/3  1 \\end{bmatrix} = \\frac{9}{8} \\begin{bmatrix} 1  -1/3 \\\\ -1/3  1 \\end{bmatrix} = \\begin{bmatrix} 9/8  -3/8 \\\\ -3/8  9/8 \\end{bmatrix}\n$$\nNow, we compute $\\Sigma_{S^c S} = \\frac{1}{n}X_{S^c}^{\\top}X_S$:\n$$\nX_2^{\\top}X_1 = 1(1) + 1(1) + (-1)(1) + (-1)(-1) + 1(-1) + (-1)(-1) = 1+1-1+1-1+1 = 2\n$$\n$$\nX_2^{\\top}X_3 = 1(1) + 1(-1) + (-1)(1) + (-1)(-1) + 1(1) + (-1)(-1) = 1-1-1+1+1+1 = 2\n$$\n$$\nX_4^{\\top}X_1 = 1(1) + (-1)(1) + (-1)(1) + 1(-1) + 1(-1) + (-1)(-1) = 1-1-1-1-1+1 = -2\n$$\n$$\nX_4^{\\top}X_3 = 1(1) + (-1)(-1) + (-1)(1) + 1(-1) + 1(1) + (-1)(-1) = 1+1-1-1+1+1 = 2\n$$\n$$\n\\Sigma_{S^c S} = \\frac{1}{6} \\begin{bmatrix} 2  2 \\\\ -2  2 \\end{bmatrix} = \\begin{bmatrix} 1/3  1/3 \\\\ -1/3  1/3 \\end{bmatrix}\n$$\nNow we assemble the terms to find $\\eta$:\n$$\n\\Sigma_{SS}^{-1}z_S^0 = \\begin{bmatrix} 9/8  -3/8 \\\\ -3/8  9/8 \\end{bmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 9/8 - (-3/8) \\\\ -3/8 - 9/8 \\end{pmatrix} = \\begin{pmatrix} 12/8 \\\\ -12/8 \\end{pmatrix} = \\begin{pmatrix} 3/2 \\\\ -3/2 \\end{pmatrix}\n$$\n$$\n\\Sigma_{S^c S} (\\Sigma_{SS}^{-1}z_S^0) = \\begin{bmatrix} 1/3  1/3 \\\\ -1/3  1/3 \\end{bmatrix} \\begin{pmatrix} 3/2 \\\\ -3/2 \\end{pmatrix} = \\begin{pmatrix} (1/3)(3/2) + (1/3)(-3/2) \\\\ (-1/3)(3/2) + (1/3)(-3/2) \\end{pmatrix} = \\begin{pmatrix} 1/2 - 1/2 \\\\ -1/2 - 1/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\n$$\nThe inactive-set feasibility quantity $\\eta$ is the $\\ell_{\\infty}$-norm of this vector:\n$$\n\\eta = \\left\\| \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} \\right\\|_{\\infty} = \\max(|0|, |-1|) = 1\n$$\nThe computed value is exactly $1$. Since this value does not satisfy the strict inequality $\\eta  1$, the irrepresentable condition is violated. This implies that exact selection consistency is not guaranteed for this problem. The system is on the boundary of recoverability, and any perturbation from noise is likely to cause LASSO to fail to select the correct set of predictors. Specifically, the value for predictor $j=4$ is $|-1|=1$, indicating it is as correlated with the model residual (in the noise-free limit) as the active predictors, and is thus prone to be incorrectly included in the model.\nThe problem asks for the value of this quantity.\nThe value is $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        }
    ]
}