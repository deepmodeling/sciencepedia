{
    "hands_on_practices": [
        {
            "introduction": "At the heart of ridge regression is a direct and elegant modification of the ordinary least squares solution. By introducing a penalty term, we ensure the matrix $(X^\\top X + \\lambda I)$ is always invertible, providing a unique solution even when predictors are highly correlated. This first exercise  provides a direct application of the core ridge regression formula, allowing you to compute the coefficient vector by hand and build a concrete understanding of the mechanics of regularization.",
            "id": "1951893",
            "problem": "In the field of machine learning, ridge regression is a common technique used to regularize linear regression models. This is particularly useful for preventing overfitting and handling multicollinearity among predictor variables. The ridge regression estimator for the coefficient vector, $\\hat{\\beta}_{\\lambda}$, is given by the formula:\n$$ \\hat{\\beta}_{\\lambda} = (X^\\top X + \\lambda I)^{-1} X^\\top y $$\nHere, $X$ is the design matrix, $y$ is the vector of observed outcomes, $I$ is the identity matrix of appropriate dimensions, and $\\lambda$ is a non-negative regularization parameter.\n\nSuppose that for a particular dataset with two predictor variables, the following quantities have been pre-computed:\n$$ X^\\top X = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} \\quad \\text{and} \\quad X^\\top y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} $$\nUsing a regularization parameter of $\\lambda = 5$, determine the ridge regression coefficient vector $\\hat{\\beta}_5$.",
            "solution": "The ridge regression estimator is defined by\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top} y.\n$$\nWith the given data,\n$$\nX^{\\top}X = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix}, \\quad X^{\\top} y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 5.\n$$\nCompute the regularized matrix:\n$$\nX^{\\top}X + \\lambda I = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} + 5 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 15 & 5 \\\\ 5 & 15 \\end{pmatrix}.\n$$\nFor a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is given by\n$$\n\\left(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\right)^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\n$$\nApplying this,\n$$\n\\det(X^{\\top}X + \\lambda I) = 15 \\cdot 15 - 5 \\cdot 5 = 225 - 25 = 200,\n$$\nso\n$$\n(X^{\\top}X + \\lambda I)^{-1} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix}.\n$$\nThen\n$$\n\\hat{\\beta}_{5} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 15 \\cdot 3 - 5 \\cdot 1 \\\\ -5 \\cdot 3 + 15 \\cdot 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 40 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ 0 \\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{5}\\\\0\\end{pmatrix}}$$"
        },
        {
            "introduction": "While the matrix formula provides a closed-form solution, inverting the $(X^\\top X + \\lambda I)$ matrix becomes computationally prohibitive for the high-dimensional models common in medical research. This practice introduces coordinate descent, a widely used iterative algorithm, by tasking you with deriving the update rule for a single coefficient . Mastering this concept is key to understanding how many modern statistical software packages efficiently fit regularized models on large-scale data.",
            "id": "1951864",
            "problem": "A data scientist is constructing a linear model to predict a target variable $y$ from a set of $p$ features. The dataset consists of $n$ observations. For the $i$-th observation, the features are given by the vector $\\mathbf{x}_i \\in \\mathbb{R}^p$, and the observed outcome is $y_i$. The linear model's prediction is given by $f(\\mathbf{x}_i; \\beta) = \\sum_{j=1}^{p} x_{ij} \\beta_j$, where $\\beta = (\\beta_1, \\dots, \\beta_p)$ is the vector of model coefficients and $x_{ij}$ is the value of the $j$-th feature for the $i$-th observation.\n\nTo mitigate overfitting and handle multicollinearity among features, the data scientist chooses to use ridge regression. The associated objective function, $L(\\beta)$, which must be minimized, is composed of a sum-of-squared-errors term and an L2-regularization term:\n$$L(\\beta) = \\frac{1}{2} \\sum_{i=1}^{n} \\left(y_i - \\sum_{j=1}^{p} x_{ij} \\beta_j\\right)^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2$$\nHere, $\\lambda > 0$ is a pre-determined regularization parameter.\n\nThe data scientist decides to implement an optimization algorithm that updates one coefficient at a time. In a single step of this iterative procedure, one coefficient, say $\\beta_j$, is updated to its optimal value while all other coefficients $\\beta_k$ (for all $k \\neq j$) are held constant.\n\nYour task is to derive the closed-form expression for the value of $\\beta_j$ that minimizes the objective function $L(\\beta)$, under the assumption that all other coefficients $\\beta_k$ for $k \\neq j$ are fixed. Your final answer should be an expression for $\\beta_j$ in terms of $y_i$, $x_{ik}$, $\\lambda$, and the fixed coefficients $\\beta_k$ for $k \\neq j$.",
            "solution": "We are given the objective function\n$$L(\\beta) = \\frac{1}{2} \\sum_{i=1}^{n} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right)^{2} + \\frac{\\lambda}{2} \\sum_{k=1}^{p} \\beta_k^{2},$$\nwith $\\lambda>0$. In a coordinate-wise update, all $\\beta_k$ with $k \\neq j$ are treated as constants and we minimize $L$ with respect to $\\beta_j$.\n\nDifferentiate $L$ with respect to $\\beta_j$ using the chain rule for the squared-error term and the derivative of the quadratic penalty:\n- For the error term, write $r_i(\\beta) = y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k$. Then\n$$\\frac{\\partial}{\\partial \\beta_j} \\left(\\frac{1}{2} \\sum_{i=1}^{n} r_i(\\beta)^{2}\\right) = \\sum_{i=1}^{n} r_i(\\beta) \\frac{\\partial r_i(\\beta)}{\\partial \\beta_j} = \\sum_{i=1}^{n} r_i(\\beta)\\,(-x_{ij}) = - \\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right).$$\n- For the regularization term, \n$$\\frac{\\partial}{\\partial \\beta_j} \\left(\\frac{\\lambda}{2} \\sum_{k=1}^{p} \\beta_k^{2}\\right) = \\lambda \\beta_j.$$\n\nHence the partial derivative is\n$$\\frac{\\partial L}{\\partial \\beta_j} = - \\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right) + \\lambda \\beta_j.$$\n\nSet this derivative to zero to obtain the first-order optimality condition:\n$$- \\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right) + \\lambda \\beta_j = 0.$$\nEquivalently,\n$$\\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k=1}^{p} x_{ik} \\beta_k\\right) = \\lambda \\beta_j.$$\n\nExpand the inner sum over $k$ and separate the $k=j$ term from the $k \\neq j$ terms:\n$$\\sum_{i=1}^{n} x_{ij} y_i - \\sum_{i=1}^{n} \\sum_{k \\neq j} x_{ij} x_{ik} \\beta_k - \\sum_{i=1}^{n} x_{ij}^{2} \\beta_j = \\lambda \\beta_j.$$\n\nCollect the terms involving $\\beta_j$ on the right-hand side:\n$$\\sum_{i=1}^{n} x_{ij} y_i - \\sum_{k \\neq j} \\beta_k \\sum_{i=1}^{n} x_{ij} x_{ik} = \\beta_j \\left(\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}\\right).$$\n\nSolve for $\\beta_j$:\n$$\\beta_j = \\frac{\\sum_{i=1}^{n} x_{ij} y_i - \\sum_{k \\neq j} \\beta_k \\sum_{i=1}^{n} x_{ij} x_{ik}}{\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}}.$$\n\nEquivalently, in a form that makes the dependence on the fixed coefficients explicit through a partial residual,\n$$\\beta_j = \\frac{\\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k \\neq j} x_{ik} \\beta_k\\right)}{\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}}.$$\nThis is the closed-form coordinate-wise minimizer for $\\beta_j$ in ridge regression with all other coefficients held fixed.",
            "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n} x_{ij} \\left(y_i - \\sum_{k \\neq j} x_{ik} \\beta_k\\right)}{\\lambda + \\sum_{i=1}^{n} x_{ij}^{2}}}$$"
        },
        {
            "introduction": "It is a common misconception that ridge regression uniformly shrinks all coefficients towards zero as the penalty $\\lambda$ increases. This final practice  uses a carefully constructed thought experiment to explore a fascinating and counter-intuitive phenomenon that can occur in the presence of strong multicollinearity. By analyzing this scenario, you will see how a coefficient for a truly irrelevant predictor can initially move *away* from zero, providing deeper insight into the complex dynamics of ridge coefficient paths.",
            "id": "1951905",
            "problem": "Consider a noiseless linear model based on a dataset of size $n$. The model includes two predictors, $x_1$ and $x_2$, which have been centered and scaled such that for each predictor $j \\in \\{1, 2\\}$, we have $\\sum_{i=1}^n x_{ij} = 0$ and $\\sum_{i=1}^n x_{ij}^2 = n$. The sample correlation between the two predictors is given as $\\rho = \\frac{4}{5}$. The true relationship for the response variable $y$ is given by the equation $y = \\beta_1 x_1$, where $\\beta_1$ is a positive constant. Note that the true coefficient for the second predictor, $\\beta_2$, is zero.\n\nAn analyst fits a ridge regression model of the form $y = \\hat{\\beta}_{1, \\lambda} x_1 + \\hat{\\beta}_{2, \\lambda} x_2$. The ridge regression coefficients $\\hat{\\beta}_{\\lambda} = (\\hat{\\beta}_{1, \\lambda}, \\hat{\\beta}_{2, \\lambda})^\\top$ are the solution to the minimization of the penalized sum of squares for a given penalty parameter $\\lambda > 0$:\n$$ L(\\beta_1, \\beta_2; \\lambda) = \\sum_{i=1}^n (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2 + \\lambda (\\beta_1^2 + \\beta_2^2) $$\nDue to the correlation between the predictors, the estimate $\\hat{\\beta}_{2, \\lambda}$ will be non-zero for $\\lambda > 0$. As $\\lambda$ increases from 0, the value of $\\hat{\\beta}_{2, \\lambda}$ will first increase, reach a peak, and then decay back towards its true value of 0.\n\nDetermine the maximum value that the coefficient estimate $\\hat{\\beta}_{2, \\lambda}$ attains over all possible values of the penalty parameter $\\lambda > 0$. Express your answer as a function of $\\beta_1$.",
            "solution": "Let $X$ be the $n \\times 2$ design matrix with columns $x_{1}$ and $x_{2}$. The standardization implies\n$$\nx_{1}^{\\top}x_{1}=n,\\quad x_{2}^{\\top}x_{2}=n,\\quad x_{1}^{\\top}x_{2}=x_{2}^{\\top}x_{1}=n\\rho,\n$$\nand the true response satisfies $y=\\beta_{1}x_{1}$, hence\n$$\nX^{\\top}y=\\begin{pmatrix}x_{1}^{\\top}y\\\\ x_{2}^{\\top}y\\end{pmatrix}\n=\\begin{pmatrix}\\beta_{1}x_{1}^{\\top}x_{1}\\\\ \\beta_{1}x_{2}^{\\top}x_{1}\\end{pmatrix}\n=\\begin{pmatrix}n\\beta_{1}\\\\ n\\rho\\,\\beta_{1}\\end{pmatrix}.\n$$\nThe ridge estimator solves $(X^{\\top}X+\\lambda I)\\hat{\\beta}_{\\lambda}=X^{\\top}y$. Using\n$$\nX^{\\top}X=\\begin{pmatrix}n & n\\rho\\\\ n\\rho & n\\end{pmatrix},\\quad\nX^{\\top}X+\\lambda I=\\begin{pmatrix}n+\\lambda & n\\rho\\\\ n\\rho & n+\\lambda\\end{pmatrix},\n$$\nits inverse is\n$$\n\\left(X^{\\top}X+\\lambda I\\right)^{-1}\n=\\frac{1}{(n+\\lambda)^{2}-n^{2}\\rho^{2}}\n\\begin{pmatrix}n+\\lambda & -n\\rho\\\\ -n\\rho & n+\\lambda\\end{pmatrix}.\n$$\nTherefore,\n$$\n\\hat{\\beta}_{\\lambda}\n=\\left(X^{\\top}X+\\lambda I\\right)^{-1}X^{\\top}y\n=\\frac{1}{(n+\\lambda)^{2}-n^{2}\\rho^{2}}\n\\begin{pmatrix}n+\\lambda & -n\\rho\\\\ -n\\rho & n+\\lambda\\end{pmatrix}\n\\begin{pmatrix}n\\beta_{1}\\\\ n\\rho\\,\\beta_{1}\\end{pmatrix}.\n$$\nExtracting the second component yields\n$$\n\\hat{\\beta}_{2,\\lambda}\n=\\frac{-n\\rho\\cdot n\\beta_{1}+(n+\\lambda)\\cdot n\\rho\\,\\beta_{1}}\n{(n+\\lambda)^{2}-n^{2}\\rho^{2}}\n=\\frac{n\\rho\\,\\beta_{1}\\lambda}{(n+\\lambda)^{2}-n^{2}\\rho^{2}}.\n$$\nLet $t=\\lambda/n>0$. Then\n$$\n\\hat{\\beta}_{2,\\lambda}\n=\\beta_{1}\\,\\rho\\,\\frac{t}{(1+t)^{2}-\\rho^{2}}.\n$$\nDefine $h(t)=\\dfrac{t}{(1+t)^{2}-\\rho^{2}}$ for $t>0$. Differentiate using the quotient rule. With $d(t)=(1+t)^{2}-\\rho^{2}$ and $d'(t)=2(1+t)$,\n$$\nh'(t)=\\frac{d(t)-t\\,d'(t)}{d(t)^{2}}\n=\\frac{(1+t)^{2}-\\rho^{2}-t\\cdot 2(1+t)}{d(t)^{2}}\n=\\frac{1-\\rho^{2}-t^{2}}{d(t)^{2}}.\n$$\nSetting $h'(t)=0$ gives $t^{2}=1-\\rho^{2}$, and since $t>0$ we get the maximizer\n$$\nt^{*}=\\sqrt{1-\\rho^{2}}.\n$$\nEvaluate $h$ at $t^{*}$. Write $s=\\sqrt{1-\\rho^{2}}$, so\n$$\nh(t^{*})=\\frac{s}{(1+s)^{2}-\\rho^{2}}\n=\\frac{s}{1+2s+s^{2}-\\rho^{2}}\n=\\frac{s}{2s+2s^{2}}\n=\\frac{1}{2(1+s)}.\n$$\nThus the maximum of $\\hat{\\beta}_{2,\\lambda}$ over $\\lambda>0$ is\n$$\n\\max_{\\lambda>0}\\hat{\\beta}_{2,\\lambda}\n=\\beta_{1}\\,\\rho\\,\\frac{1}{2\\left(1+\\sqrt{1-\\rho^{2}}\\right)}.\n$$\nWith the given $\\rho=\\frac{4}{5}$, we have $\\sqrt{1-\\rho^{2}}=\\sqrt{\\frac{9}{25}}=\\frac{3}{5}$, hence\n$$\n\\max_{\\lambda>0}\\hat{\\beta}_{2,\\lambda}\n=\\beta_{1}\\cdot\\frac{4}{5}\\cdot\\frac{1}{2\\left(1+\\frac{3}{5}\\right)}\n=\\beta_{1}\\cdot\\frac{4}{5}\\cdot\\frac{1}{2\\cdot\\frac{8}{5}}\n=\\beta_{1}\\cdot\\frac{4}{16}\n=\\frac{\\beta_{1}}{4}.\n$$",
            "answer": "$$\\boxed{\\frac{\\beta_{1}}{4}}$$"
        }
    ]
}