{
    "hands_on_practices": [
        {
            "introduction": "Point estimates of association like the Risk Ratio ($RR$), Odds Ratio ($OR$), and Rate Ratio ($IRR$) are central to epidemiology, but statistical inference requires us to also quantify their uncertainty. This practice guides you through the derivation of the large-sample variance for the logarithm of these measures using the delta method, a cornerstone of asymptotic theory. By completing this exercise , you will gain a first-principles understanding of the standard error formulas that are fundamental for constructing confidence intervals and conducting hypothesis tests in practice.",
            "id": "4972058",
            "problem": "A prospective cohort study compares an exposed group and an unexposed group on a binary health outcome and, in a parallel surveillance system, compares incidence rates based on person-time. Let $a$ denote the number of outcomes in the exposed group and $b$ denote the number of non-outcomes in the exposed group, so that the exposed group size is $n_{1} = a + b$. Let $c$ denote the number of outcomes in the unexposed group and $d$ denote the number of non-outcomes in the unexposed group, so that the unexposed group size is $n_{0} = c + d$. Define the empirical risk ratio (RR), odds ratio (OR), and incidence rate ratio (IRR) by\n$$\\widehat{\\text{RR}} = \\frac{a / n_{1}}{c / n_{0}}, \\quad \\widehat{\\text{OR}} = \\frac{a / b}{c / d} = \\frac{a d}{b c}, \\quad \\widehat{\\text{IRR}} = \\frac{X_{1} / T_{1}}{X_{0} / T_{0}},$$\nwhere $X_{1}$ and $X_{0}$ denote independent Poisson event counts in the exposed and unexposed groups, respectively, and $T_{1}$ and $T_{0}$ denote the corresponding fixed person-time totals.\n\nAssume the following data-generating mechanisms:\n- In the cohort component, $a \\sim \\text{Binomial}(n_{1}, p_{1})$ and $c \\sim \\text{Binomial}(n_{0}, p_{0})$, independently, where $p_{1}$ and $p_{0}$ are the true risks in the exposed and unexposed groups.\n- In the rate component, $X_{1} \\sim \\text{Poisson}(\\lambda_{1} T_{1})$ and $X_{0} \\sim \\text{Poisson}(\\lambda_{0} T_{0})$, independently, where $\\lambda_{1}$ and $\\lambda_{0}$ are the true incidence rates.\n\nUsing only first principles of large-sample theory for Binomial and Poisson models and the delta method, derive first-order large-sample estimators for $\\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{RR}}\\right)\\right]$, $\\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{OR}}\\right)\\right]$, and $\\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{IRR}}\\right)\\right]$ expressed purely in terms of $a, b, c, d, X_{1}, X_{0}, T_{1}, T_{0}$.\n\nYour final answer must be a single closed-form analytic expression, presented as a row matrix with the three entries in the order $\\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{RR}}\\right)\\right]$, $\\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{OR}}\\right)\\right]$, $\\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{IRR}}\\right)\\right]$. Use the natural logarithm $\\ln$. No numerical evaluation, rounding, or units are required.",
            "solution": "The problem requires the derivation of large-sample estimators for the variance of the natural logarithm of the risk ratio ($\\widehat{\\text{RR}}$), odds ratio ($\\widehat{\\text{OR}}$), and incidence rate ratio ($\\widehat{\\text{IRR}}$). The core theoretical tool for this derivation is the delta method.\n\nThe delta method provides a first-order Taylor series approximation for the variance of a function of one or more random variables. For a single random variable $Y$ and a differentiable function $g$, the variance of $g(Y)$ is approximated as:\n$$ \\operatorname{Var}[g(Y)] \\approx \\left( g'(E[Y]) \\right)^2 \\operatorname{Var}[Y] $$\nFor a function of two independent random variables, $Y_1$ and $Y_2$, the variance of $g(Y_1, Y_2)$ is approximated as:\n$$ \\operatorname{Var}[g(Y_1, Y_2)] \\approx \\left( \\frac{\\partial g}{\\partial y_1} \\right)^2 \\operatorname{Var}[Y_1] + \\left( \\frac{\\partial g}{\\partial y_2} \\right)^2 \\operatorname{Var}[Y_2] $$\nwhere the partial derivatives are evaluated at the expected values of $Y_1$ and $Y_2$. A large-sample estimator for this variance, denoted $\\widehat{\\operatorname{Var}}$, is obtained by substituting the population parameters (including expected values and variances) with their sample-based estimates.\n\nThe three requested variance estimators are derived separately.\n\n1.  **Derivation of $\\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{RR}}\\right)\\right]$**\n\nThe empirical risk ratio is defined as $\\widehat{\\text{RR}} = \\frac{\\hat{p}_{1}}{\\hat{p}_{0}}$, where $\\hat{p}_{1} = a/n_{1}$ and $\\hat{p}_{0} = c/n_{0}$ are the estimated risks in the exposed and unexposed groups, respectively. The natural logarithm is $\\ln(\\widehat{\\text{RR}}) = \\ln(\\hat{p}_{1}) - \\ln(\\hat{p}_{0})$.\nThe random variables are $a \\sim \\text{Binomial}(n_{1}, p_{1})$ and $c \\sim \\text{Binomial}(n_{0}, p_{0})$, which are independent. Consequently, the estimators $\\hat{p}_{1} = a/n_{1}$ and $\\hat{p}_{0} = c/n_{0}$ are also independent.\nDue to independence, the variance of the difference is the sum of the variances:\n$$ \\operatorname{Var}[\\ln(\\widehat{\\text{RR}})] = \\operatorname{Var}[\\ln(\\hat{p}_{1})] + \\operatorname{Var}[\\ln(\\hat{p}_{0})] $$\nWe apply the delta method to each term. Let $g(p) = \\ln(p)$, so $g'(p) = 1/p$.\nThe variance of $\\hat{p}_{1}$ is $\\operatorname{Var}[\\hat{p}_{1}] = \\operatorname{Var}[a/n_{1}] = \\frac{1}{n_{1}^2}\\operatorname{Var}[a] = \\frac{n_{1} p_{1}(1-p_{1})}{n_{1}^2} = \\frac{p_{1}(1-p_{1})}{n_{1}}$.\nApplying the delta method for $\\ln(\\hat{p}_{1})$:\n$$ \\operatorname{Var}[\\ln(\\hat{p}_{1})] \\approx \\left( g'(p_1) \\right)^2 \\operatorname{Var}[\\hat{p}_{1}] = \\left(\\frac{1}{p_{1}}\\right)^2 \\frac{p_{1}(1-p_{1})}{n_{1}} = \\frac{1-p_{1}}{n_{1} p_{1}} $$\nSimilarly, for the unexposed group:\n$$ \\operatorname{Var}[\\ln(\\hat{p}_{0})] \\approx \\frac{1-p_{0}}{n_{0} p_{0}} $$\nCombining these gives the approximate variance of $\\ln(\\widehat{\\text{RR}})$:\n$$ \\operatorname{Var}[\\ln(\\widehat{\\text{RR}})] \\approx \\frac{1-p_{1}}{n_{1} p_{1}} + \\frac{1-p_{0}}{n_{0} p_{0}} $$\nTo obtain the estimator $\\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{RR}}\\right)\\right]$, we substitute the true probabilities $p_{1}$ and $p_{0}$ with their sample estimates $\\hat{p}_{1} = a/n_{1}$ and $\\hat{p}_{0} = c/n_{0}$.\n$$ \\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{RR}}\\right)\\right] = \\frac{1-\\hat{p}_{1}}{n_{1}\\hat{p}_{1}} + \\frac{1-\\hat{p}_{0}}{n_{0}\\hat{p}_{0}} = \\frac{1 - a/n_{1}}{n_{1}(a/n_{1})} + \\frac{1 - c/n_{0}}{n_{0}(c/n_{0})} $$\n$$ = \\frac{(n_{1}-a)/n_{1}}{a} + \\frac{(n_{0}-c)/n_{0}}{c} = \\frac{b/n_{1}}{a} + \\frac{d/n_{0}}{c} = \\frac{b}{an_{1}} + \\frac{d}{cn_{0}} $$\nSince $n_{1} = a+b$ and $n_{0} = c+d$, this can be written as:\n$$ \\frac{b}{a(a+b)} + \\frac{d}{c(c+d)} = \\left(\\frac{1}{a} - \\frac{1}{a+b}\\right) + \\left(\\frac{1}{c} - \\frac{1}{c+d}\\right) $$\nThus, the final estimator is $\\frac{1}{a} - \\frac{1}{a+b} + \\frac{1}{c} - \\frac{1}{c+d}$.\n\n2.  **Derivation of $\\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{OR}}\\right)\\right]$**\n\nThe empirical odds ratio is $\\widehat{\\text{OR}} = \\frac{a/b}{c/d}$. It is more convenient to express this in terms of proportions: $\\widehat{\\text{OR}} = \\frac{\\hat{p}_{1}/(1-\\hat{p}_{1})}{\\hat{p}_{0}/(1-\\hat{p}_{0})}$.\nThe natural logarithm of the odds ratio, known as the log-odds ratio, is:\n$$ \\ln(\\widehat{\\text{OR}}) = \\ln\\left(\\frac{\\hat{p}_{1}}{1-\\hat{p}_{1}}\\right) - \\ln\\left(\\frac{\\hat{p}_{0}}{1-\\hat{p}_{0}}\\right) = \\text{logit}(\\hat{p}_{1}) - \\text{logit}(\\hat{p}_{0}) $$\nAs before, $\\hat{p}_{1}$ and $\\hat{p}_{0}$ are independent. We apply the delta method to the logit function, $g(p) = \\text{logit}(p) = \\ln(p) - \\ln(1-p)$.\nThe derivative is $g'(p) = \\frac{1}{p} + \\frac{1}{1-p} = \\frac{1}{p(1-p)}$.\nThe variance of $\\text{logit}(\\hat{p}_{1})$ is:\n$$ \\operatorname{Var}[\\text{logit}(\\hat{p}_{1})] \\approx \\left( g'(p_1) \\right)^2 \\operatorname{Var}[\\hat{p}_{1}] = \\left(\\frac{1}{p_{1}(1-p_{1})}\\right)^2 \\frac{p_{1}(1-p_{1})}{n_{1}} = \\frac{1}{n_{1}p_{1}(1-p_{1})} $$\nBy independence, the variance of $\\ln(\\widehat{\\text{OR}})$ is:\n$$ \\operatorname{Var}[\\ln(\\widehat{\\text{OR}})] \\approx \\frac{1}{n_{1}p_{1}(1-p_{1})} + \\frac{1}{n_{0}p_{0}(1-p_{0})} $$\nThe estimator is found by substituting sample estimates for the parameters:\n$$ \\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{OR}}\\right)\\right] = \\frac{1}{n_{1}\\hat{p}_{1}(1-\\hat{p}_{1})} + \\frac{1}{n_{0}\\hat{p}_{0}(1-\\hat{p}_{0})} $$\nNow, substitute $\\hat{p}_{1} = a/n_{1}$, $1-\\hat{p}_{1} = b/n_{1}$, $\\hat{p}_{0} = c/n_{0}$, and $1-\\hat{p}_{0} = d/n_{0}$:\n$$ \\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{OR}}\\right)\\right] = \\frac{1}{n_{1}\\frac{a}{n_{1}}\\frac{b}{n_{1}}} + \\frac{1}{n_{0}\\frac{c}{n_{0}}\\frac{d}{n_{0}}} = \\frac{n_{1}}{ab} + \\frac{n_{0}}{cd} $$\nSubstituting $n_{1} = a+b$ and $n_{0} = c+d$:\n$$ \\frac{a+b}{ab} + \\frac{c+d}{cd} = \\left(\\frac{a}{ab} + \\frac{b}{ab}\\right) + \\left(\\frac{c}{cd} + \\frac{d}{cd}\\right) = \\frac{1}{b} + \\frac{1}{a} + \\frac{1}{d} + \\frac{1}{c} $$\nThus, the final estimator is $\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}$.\n\n3.  **Derivation of $\\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{IRR}}\\right)\\right]$**\n\nThe empirical incidence rate ratio is $\\widehat{\\text{IRR}} = \\frac{X_{1}/T_{1}}{X_{0}/T_{0}}$. Its natural logarithm is:\n$$ \\ln(\\widehat{\\text{IRR}}) = \\ln(X_{1}) - \\ln(X_{0}) - \\ln(T_{1}) + \\ln(T_{0}) $$\nThe random variables are the event counts $X_{1} \\sim \\text{Poisson}(\\lambda_{1} T_{1})$ and $X_{0} \\sim \\text{Poisson}(\\lambda_{0} T_{0})$, which are independent. The person-time totals $T_{1}$ and $T_{0}$ are fixed constants.\nThe variance of the sum is the sum of the variances (constants have zero variance):\n$$ \\operatorname{Var}[\\ln(\\widehat{\\text{IRR}})] = \\operatorname{Var}[\\ln(X_{1})] + \\operatorname{Var}[\\ln(X_{0})] $$\nWe apply the delta method to $g(X) = \\ln(X)$, for which $g'(X) = 1/X$. For the Poisson distribution, the variance is equal to the mean, so $\\operatorname{Var}[X_{1}] = E[X_{1}] = \\lambda_{1} T_{1}$.\nApplying the delta method for $\\ln(X_{1})$:\n$$ \\operatorname{Var}[\\ln(X_{1})] \\approx \\left( g'(E[X_{1}]) \\right)^2 \\operatorname{Var}[X_{1}] = \\left(\\frac{1}{E[X_{1}]}\\right)^2 \\operatorname{Var}[X_{1}] = \\frac{\\operatorname{Var}[X_{1}]}{(E[X_{1}])^2} = \\frac{\\lambda_{1} T_{1}}{(\\lambda_{1} T_{1})^2} = \\frac{1}{\\lambda_{1} T_{1}} $$\nSimilarly, for the unexposed group, $\\operatorname{Var}[\\ln(X_{0})] \\approx \\frac{1}{\\lambda_{0} T_{0}}$.\nThe approximate variance of $\\ln(\\widehat{\\text{IRR}})$ is:\n$$ \\operatorname{Var}[\\ln(\\widehat{\\text{IRR}})] \\approx \\frac{1}{\\lambda_{1} T_{1}} + \\frac{1}{\\lambda_{0} T_{0}} $$\nTo form the estimator $\\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{IRR}}\\right)\\right]$, we must estimate the parameters $\\mu_{1}=\\lambda_{1}T_{1}$ and $\\mu_{0}=\\lambda_{0}T_{0}$. For a Poisson distribution, the maximum likelihood estimate of the mean parameter $\\mu$ is the observed count. Therefore, we substitute $\\lambda_{1}T_{1}$ with $X_{1}$ and $\\lambda_{0}T_{0}$ with $X_{0}$.\n$$ \\widehat{\\operatorname{Var}}\\left[\\ln\\left(\\widehat{\\text{IRR}}\\right)\\right] = \\frac{1}{X_{1}} + \\frac{1}{X_{0}} $$\nThis is the final estimator for the variance of the log-incidence rate ratio.\n\nThe three derived large-sample estimators are assembled into a row matrix as requested.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{1}{a} - \\frac{1}{a+b} + \\frac{1}{c} - \\frac{1}{c+d}  \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}  \\frac{1}{X_{1}} + \\frac{1}{X_{0}} \\end{pmatrix} } $$"
        },
        {
            "introduction": "Matching is a powerful study design strategy used to control for confounding, but it requires specialized analytical methods that respect the correlated data structure. This exercise explores the elegant framework of conditional logistic regression for 1:1 matched case-control studies. You will derive how conditioning on the number of cases per matched set eliminates stratum-specific nuisance parameters, leading to the classic matched odds ratio estimator , thereby deepening your understanding of how statistical methods can account for complex study designs.",
            "id": "4972042",
            "problem": "A 1:1 matched case-control study evaluates the association between a binary exposure $X \\in \\{0,1\\}$ and a binary outcome $Y \\in \\{0,1\\}$, where $Y=1$ indicates a case and $Y=0$ indicates a control. Each matched pair $i$ consists of two individuals who are comparable on measured matching factors (for example, age, clinic, and calendar time), and within-pair case-control status is determined by $Y$. Assume the following logistic regression model with a stratum-specific intercept for each pair:\n$$\n\\operatorname{logit}\\big(\\Pr(Y=1 \\mid X=x, \\text{pair } i)\\big) \\;=\\; \\alpha_i + \\beta x,\n$$\nwhere $\\alpha_i$ is an unknown nuisance parameter specific to pair $i$, and $\\beta$ is the log-odds ratio parameter of interest associated with $X$.\n\nUsing only the definitions of the logistic function, odds, and conditional probability, and without appealing to any pre-derived conditional likelihood shortcuts:\n\n- Derive the conditional likelihood contribution for a single matched pair $i$ by conditioning on the event that exactly one subject in the pair is a case (that is, $\\sum_{j=1}^{2} Y_{ij} = 1$), and show explicitly how the nuisance parameter $\\alpha_i$ is eliminated by this conditioning.\n- Combine the pairwise conditional contributions across independent pairs to express the full conditional likelihood in terms of the counts of discordant pairs, where a discordant pair is one in which the case and the control have different exposure values. Let $m_{10}$ denote the number of pairs where the case is exposed ($X=1$) and the control is unexposed ($X=0$), and let $m_{01}$ denote the number of pairs where the case is unexposed and the control is exposed.\n- Using your derived conditional likelihood, obtain the maximum likelihood estimator for $\\exp(\\beta)$ in terms of $m_{10}$ and $m_{01}$. Interpret $\\exp(\\beta)$ as a matched Odds Ratio (OR), defining Odds Ratio (OR) on its first use, and explain how conditioning controls for the matching factors.\n- For a dataset with $m_{10} = 18$, $m_{01} = 12$, and the remaining pairs concordant, compute the numerical value of $\\exp(\\hat{\\beta})$. Provide your final answer as a single number with no units; no rounding is required.",
            "solution": "The problem requires a four-part solution: deriving the conditional likelihood for a single pair, extending it to the full likelihood, finding the maximum likelihood estimator (MLE), and calculating its value for the given data.\n\n**Part 1: Conditional Likelihood for a Single Pair**\n\nLet the two individuals in matched pair $i$ be indexed by $j=1$ and $j=2$. Their respective exposures are $X_{i1}$ and $X_{i2}$. The logistic model is given by $\\operatorname{logit}\\big(\\Pr(Y_{ij}=1 \\mid X_{ij}, i)\\big) = \\alpha_i + \\beta X_{ij}$. The logistic function is $p = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}$, where $\\eta$ is the linear predictor. Thus, the probability of an individual $j$ in pair $i$ being a case ($Y_{ij}=1$) is:\n$$\nP_{ij} = \\Pr(Y_{ij}=1 \\mid X_{ij}, i) \\;=\\; \\frac{\\exp(\\alpha_i + \\beta X_{ij})}{1 + \\exp(\\alpha_i + \\beta X_{ij})}\n$$\nAnd the probability of being a control ($Y_{ij}=0$) is:\n$$\n1 - P_{ij} = \\Pr(Y_{ij}=0 \\mid X_{ij}, i) \\;=\\; \\frac{1}{1 + \\exp(\\alpha_i + \\beta X_{ij})}\n$$\nBy design, each pair has exactly one case and one control, so we condition on the event $\\sum_{j=1}^{2} Y_{ij} = 1$. Let's denote the observed exposures for the two individuals as $(x_{i1}, x_{i2})$ and their outcomes as $(y_{i1}, y_{i2})$. Without loss of generality, let's say individual $1$ is the case ($y_{i1}=1$) and individual $2$ is the control ($y_{i2}=0$). The contribution of this pair to the conditional likelihood is the probability of this specific outcome configuration, given that one of them is a case and the other is a control.\n$$\nL_i(\\beta) = \\Pr(Y_{i1}=1, Y_{i2}=0 \\mid \\sum_{j=1}^{2} Y_{ij} = 1, X_{i1}=x_{i1}, X_{i2}=x_{i2})\n$$\nUsing the definition of conditional probability, $P(A \\mid B) = P(A \\cap B) / P(B)$:\n$$\nL_i(\\beta) = \\frac{\\Pr(Y_{i1}=1, Y_{i2}=0 \\mid x_{i1}, x_{i2})}{\\Pr(\\sum_{j=1}^{2} Y_{ij} = 1 \\mid x_{i1}, x_{i2})}\n$$\nAssuming the outcomes of the two individuals are independent conditional on their covariates (which includes the stratum $\\alpha_i$), the numerator is:\n$$\n\\Pr(Y_{i1}=1, Y_{i2}=0 \\mid x_{i1}, x_{i2}) = P_{i1}(1-P_{i2}) = \\frac{\\exp(\\alpha_i + \\beta x_{i1})}{1 + \\exp(\\alpha_i + \\beta x_{i1})} \\cdot \\frac{1}{1 + \\exp(\\alpha_i + \\beta x_{i2})}\n$$\nThe denominator is the total probability of the conditioning event, which has two possibilities: individual $1$ is the case and $2$ is the control, or individual $1$ is the control and $2$ is the case.\n$$\n\\Pr(\\sum_{j=1}^{2} Y_{ij} = 1 \\mid x_{i1}, x_{i2}) = P_{i1}(1-P_{i2}) + (1-P_{i1})P_{i2}\n$$\n$$\n= \\frac{\\exp(\\alpha_i + \\beta x_{i1})}{(1+\\exp(\\alpha_i + \\beta x_{i1}))(1+\\exp(\\alpha_i + \\beta x_{i2}))} + \\frac{\\exp(\\alpha_i + \\beta x_{i2})}{(1+\\exp(\\alpha_i + \\beta x_{i1}))(1+\\exp(\\alpha_i + \\beta x_{i2}))}\n$$\n$$\n= \\frac{\\exp(\\alpha_i + \\beta x_{i1}) + \\exp(\\alpha_i + \\beta x_{i2})}{(1+\\exp(\\alpha_i + \\beta x_{i1}))(1+\\exp(\\alpha_i + \\beta x_{i2}))}\n$$\nDividing the numerator by the denominator, the common factor in the denominators cancels out:\n$$\nL_i(\\beta) = \\frac{\\exp(\\alpha_i + \\beta x_{i1})}{\\exp(\\alpha_i + \\beta x_{i1}) + \\exp(\\alpha_i + \\beta x_{i2})}\n$$\nTo show the elimination of the nuisance parameter $\\alpha_i$, we factor $\\exp(\\alpha_i)$ from the numerator and denominator:\n$$\nL_i(\\beta) = \\frac{\\exp(\\alpha_i) \\exp(\\beta x_{i1})}{\\exp(\\alpha_i) \\exp(\\beta x_{i1}) + \\exp(\\alpha_i) \\exp(\\beta x_{i2})} = \\frac{\\exp(\\alpha_i) \\exp(\\beta x_{i1})}{\\exp(\\alpha_i) (\\exp(\\beta x_{i1}) + \\exp(\\beta x_{i2}))}\n$$\nCanceling $\\exp(\\alpha_i)$, we get the conditional likelihood contribution for pair $i$, where individual $1$ is the case and individual $2$ is the control:\n$$\nL_i(\\beta) = \\frac{\\exp(\\beta x_{i1})}{\\exp(\\beta x_{i1}) + \\exp(\\beta x_{i2})}\n$$\nThis expression depends only on the exposures of the two individuals in the pair and the parameter of interest $\\beta$, thus successfully eliminating the nuisance parameter $\\alpha_i$.\n\n**Part 2: Full Conditional Likelihood**\n\nLet's denote the exposure of the case in pair $i$ as $x_{ic}$ and the exposure of the control as $x_{ik}$. The conditional likelihood for pair $i$ is the probability that the individual with exposure $x_{ic}$ is the case, given the set of exposures $\\{x_{ic}, x_{ik}\\}$:\n$$\nL_i(\\beta) = \\frac{\\exp(\\beta x_{ic})}{\\exp(\\beta x_{ic}) + \\exp(\\beta x_{ik})}\n$$\nThe full conditional likelihood, $L_C(\\beta)$, is the product of the contributions from all $N$ independent pairs:\n$$\nL_C(\\beta) = \\prod_{i=1}^{N} \\frac{\\exp(\\beta x_{ic})}{\\exp(\\beta x_{ic}) + \\exp(\\beta x_{ik})}\n$$\nWe can group the pairs into four types:\n1.  Concordant exposed pairs ($m_{11}$ pairs): $x_{ic}=1, x_{ik}=1$. Contribution is $\\frac{\\exp(\\beta \\cdot 1)}{\\exp(\\beta \\cdot 1) + \\exp(\\beta \\cdot 1)} = \\frac{1}{2}$.\n2.  Concordant unexposed pairs ($m_{00}$ pairs): $x_{ic}=0, x_{ik}=0$. Contribution is $\\frac{\\exp(\\beta \\cdot 0)}{\\exp(\\beta \\cdot 0) + \\exp(\\beta \\cdot 0)} = \\frac{1}{2}$.\n3.  Discordant pairs ($m_{10}$ pairs): $x_{ic}=1, x_{ik}=0$. Contribution is $\\frac{\\exp(\\beta \\cdot 1)}{\\exp(\\beta \\cdot 1) + \\exp(\\beta \\cdot 0)} = \\frac{\\exp(\\beta)}{1+\\exp(\\beta)}$.\n4.  Discordant pairs ($m_{01}$ pairs): $x_{ic}=0, x_{ik}=1$. Contribution is $\\frac{\\exp(\\beta \\cdot 0)}{\\exp(\\beta \\cdot 0) + \\exp(\\beta \\cdot 1)} = \\frac{1}{1+\\exp(\\beta)}$.\n\nThe full conditional likelihood is the product of these terms raised to the power of their respective counts:\n$$\nL_C(\\beta) = \\left(\\frac{1}{2}\\right)^{m_{11}} \\left(\\frac{1}{2}\\right)^{m_{00}} \\left(\\frac{\\exp(\\beta)}{1+\\exp(\\beta)}\\right)^{m_{10}} \\left(\\frac{1}{1+\\exp(\\beta)}\\right)^{m_{01}}\n$$\n$$\nL_C(\\beta) = \\left(\\frac{1}{2}\\right)^{m_{11}+m_{00}} \\frac{[\\exp(\\beta)]^{m_{10}}}{[1+\\exp(\\beta)]^{m_{10}}[1+\\exp(\\beta)]^{m_{01}}}\n$$\n$$\nL_C(\\beta) = \\left(\\frac{1}{2}\\right)^{m_{11}+m_{00}} \\frac{\\exp(\\beta m_{10})}{(1+\\exp(\\beta))^{m_{10}+m_{01}}}\n$$\nAs shown, only the discordant pairs ($m_{10}$ and $m_{01}$) contribute information for the estimation of $\\beta$.\n\n**Part 3: Maximum Likelihood Estimator and Interpretation**\n\nTo find the MLE for $\\beta$, we maximize the log-likelihood $\\ell_C(\\beta) = \\ln(L_C(\\beta))$:\n$$\n\\ell_C(\\beta) = (m_{11}+m_{00})\\ln(1/2) + m_{10}\\beta - (m_{10}+m_{01})\\ln(1+\\exp(\\beta))\n$$\nWe take the derivative with respect to $\\beta$ and set it to zero:\n$$\n\\frac{d\\ell_C(\\beta)}{d\\beta} = m_{10} - (m_{10}+m_{01}) \\frac{\\exp(\\beta)}{1+\\exp(\\beta)}\n$$\nSetting the derivative to zero to find the MLE $\\hat{\\beta}$:\n$$\nm_{10} - (m_{10}+m_{01}) \\frac{\\exp(\\hat{\\beta})}{1+\\exp(\\hat{\\beta})} = 0\n$$\n$$\nm_{10} = (m_{10}+m_{01}) \\frac{\\exp(\\hat{\\beta})}{1+\\exp(\\hat{\\beta})}\n$$\n$$\nm_{10}(1+\\exp(\\hat{\\beta})) = (m_{10}+m_{01})\\exp(\\hat{\\beta})\n$$\n$$\nm_{10} + m_{10}\\exp(\\hat{\\beta}) = m_{10}\\exp(\\hat{\\beta}) + m_{01}\\exp(\\hat{\\beta})\n$$\n$$\nm_{10} = m_{01}\\exp(\\hat{\\beta})\n$$\nSolving for $\\exp(\\hat{\\beta})$, the MLE of the odds ratio, yields:\n$$\n\\exp(\\hat{\\beta}) = \\frac{m_{10}}{m_{01}}\n$$\nInterpretation: An Odds Ratio (OR) is a measure of association between an exposure and an outcome. It represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence of that exposure. From our model, $\\ln(\\text{Odds}(Y=1)) = \\alpha_i + \\beta x$. The odds ratio for exposure ($X=1$) versus non-exposure ($X=0$) within a stratum $i$ is:\n$$\nOR = \\frac{\\text{Odds}(Y=1 \\mid X=1, i)}{\\text{Odds}(Y=1 \\mid X=0, i)} = \\frac{\\exp(\\alpha_i + \\beta \\cdot 1)}{\\exp(\\alpha_i + \\beta \\cdot 0)} = \\frac{\\exp(\\alpha_i)\\exp(\\beta)}{\\exp(\\alpha_i)} = \\exp(\\beta)\n$$\nThus, $\\exp(\\beta)$ is the odds ratio, constant across all pairs. Its estimator, $\\exp(\\hat{\\beta}) = m_{10}/m_{01}$, is known as the matched Odds Ratio. It is the ratio of discordant pairs where the case was exposed to discordant pairs where the control was exposed.\n\nThe conditioning approach controls for matching factors such as age, sex, or clinical center. These factors are assumed to have a common effect on the baseline risk of the outcome for both individuals within a matched pair. This common effect is captured by the stratum-specific intercept $\\alpha_i$. By conditioning on the fact that each pair contains exactly one case and one control, we derive a likelihood that is independent of $\\alpha_i$. Therefore, the resulting estimate for $\\beta$ is not confounded by the factors that were used for matching, as their influence has been mathematically eliminated from the estimation process.\n\n**Part 4: Numerical Calculation**\n\nGiven the data: $m_{10} = 18$ and $m_{01} = 12$. The number of concordant pairs is not needed for the point estimate.\nThe numerical value of the MLE for the odds ratio $\\exp(\\beta)$ is:\n$$\n\\exp(\\hat{\\beta}) = \\frac{m_{10}}{m_{01}} = \\frac{18}{12} = \\frac{3}{2} = 1.5\n$$",
            "answer": "$$\n\\boxed{1.5}\n$$"
        },
        {
            "introduction": "In applied data analysis, theoretical formulas sometimes break down. The simple odds ratio estimator, $\\frac{ad}{bc}$, fails when sparse data results in a zero count in one of the cells, often leading to an infinite maximum likelihood estimate. This exercise  confronts this practical challenge, contrasting the failure of standard estimation with the robustness of exact conditional methods like Fisher’s exact test. Working through this problem will build your intuition for diagnosing data separation and employing appropriate inferential tools for sparse tables.",
            "id": "4972025",
            "problem": "A case-control study of a rare adverse event in a hospital system records the following exposure cross-tabulation: among cases, $12$ were exposed and $3$ were unexposed; among controls, $0$ were exposed and $25$ were unexposed. Let $Y \\in \\{0,1\\}$ denote case status ($Y=1$ for case, $Y=0$ for control) and let $X \\in \\{0,1\\}$ denote exposure. The measure of association of interest is the odds ratio $\\theta$, defined as the ratio of the odds of exposure among cases to the odds of exposure among controls, where odds is defined from the probability $p$ of exposure within a stratum as $p/(1-p)$. Assume independent Bernoulli sampling within strata, and consider two likelihoods: the unconditional likelihood in which $p_1=\\mathbb{P}(X=1\\mid Y=1)$ and $p_0=\\mathbb{P}(X=1\\mid Y=0)$ are estimated directly from the data, and the conditional likelihood obtained by conditioning on the fixed margins of the $2\\times 2$ table.\n\nUsing only the core definitions of odds and odds ratio, and the fact that the maximum likelihood estimate for a Bernoulli probability in a homogeneous stratum equals the observed proportion, reason through the behavior of the maximum likelihood estimate of $\\theta$ in this zero-cell table. Then, consider exact conditional inference for $\\theta$ based on the conditional distribution of the exposed-case count given the margins, and the construction of confidence intervals by inverting two-sided Fisher’s exact tests at nominal level $1-\\alpha$ with $\\alpha=0.05$.\n\nWhich option most accurately characterizes, in this setting, why the maximum likelihood estimate of $\\theta$ is unbounded, what the shape of the exact conditional confidence set obtained by Fisher’s inversion is, and whether a small-sample adjustment can yield finite endpoints?\n\nA. In this table, the unconditional maximum likelihood estimate of $\\theta$ is infinite because the estimated exposure probability among controls is $0$, making the control odds $0$ and driving the ratio to $+\\infty$. Under fixed margins, the conditional likelihood places all mass at the upper boundary $a=n_1$ as $\\theta\\to\\infty$, so the conditional maximum likelihood estimate also diverges. The exact two-sided $95\\%$ confidence set obtained by inverting Fisher’s test is of the form $[L,\\infty)$ with $L1$, reflecting that the observed exposed-case count is at the upper support boundary; adding Haldane–Anscombe $+1/2$ pseudo-counts to each cell can produce a finite but biased interval.\n\nB. Fisher’s exact conditional $95\\%$ confidence interval for $\\theta$ is always finite even when a zero cell occurs, because the hypergeometric likelihood decays symmetrically in $\\theta$, so neither bound can be infinite.\n\nC. Firth’s penalized likelihood (Jeffreys-prior penalty) always matches the interval obtained by Fisher’s exact conditional inversion, so penalization does not change the endpoints and cannot resolve divergence from zero cells.\n\nD. Under fixed margins, the conditional maximum likelihood estimate of $\\theta$ is $\\theta=1$ in any zero-cell table, so there is no divergence in the conditional analysis; any infinite estimate arises only from an inappropriate unconditional model.\n\nE. In a cohort analysis with rate ratio based on Poisson counts, any zero events in one group produce undefined point estimates, but exact Poisson methods always produce finite $95\\%$ confidence intervals for the rate ratio regardless of zero cells.",
            "solution": "First, we structure the provided data into a standard $2 \\times 2$ table format, with cells denoted by $a, b, c, d$.\n-   $a$: exposed cases = $12$\n-   $b$: exposed controls = $0$\n-   $c$: unexposed cases = $3$\n-   $d$: unexposed controls = $25$\n\nThe table is:\n$$\n\\begin{array}{c|cc|c}\n \\text{Case ($Y=1$)}  \\text{Control ($Y=0$)}  \\text{Total} \\\\\n\\hline\n\\text{Exposed ($X=1$)}  a=12  b=0  m_1=12 \\\\\n\\text{Unexposed ($X=0$)}  c=3  d=25  m_0=28 \\\\\n\\hline\n\\text{Total}  n_1=15  n_0=25  N=40\n\\end{array}\n$$\n\n**1. Unconditional Maximum Likelihood Estimate of $\\theta$**\n\nThe problem asks for the estimate of $\\theta = \\frac{p_1/(1-p_1)}{p_0/(1-p_0)}$.\nUsing the principle that the MLE of a Bernoulli probability is the sample proportion:\n-   The MLE of $p_1 = \\mathbb{P}(X=1 \\mid Y=1)$ is $\\hat{p}_1 = \\frac{a}{a+c} = \\frac{12}{12+3} = \\frac{12}{15} = 0.8$.\n-   The MLE of $p_0 = \\mathbb{P}(X=1 \\mid Y=0)$ is $\\hat{p}_0 = \\frac{b}{b+d} = \\frac{0}{0+25} = 0$.\n\nNow, we compute the MLE of the odds for each group:\n-   MLE of odds of exposure in cases: $\\frac{\\hat{p}_1}{1-\\hat{p}_1} = \\frac{0.8}{1-0.8} = \\frac{0.8}{0.2} = 4$. This is also $\\frac{a}{c} = \\frac{12}{3} = 4$.\n-   MLE of odds of exposure in controls: $\\frac{\\hat{p}_0}{1-\\hat{p}_0} = \\frac{0}{1-0} = 0$. This is also $\\frac{b}{d} = \\frac{0}{25} = 0$.\n\nThe unconditional MLE of the odds ratio, $\\hat{\\theta}_{MLE}$, is the ratio of these estimated odds:\n$$ \\hat{\\theta}_{MLE} = \\frac{a/c}{b/d} = \\frac{ad}{bc} = \\frac{12 \\times 25}{0 \\times 3} = \\frac{300}{0} $$\nThis expression involves division by zero and is undefined. The limit as the denominator approaches zero from the positive side is $+\\infty$. Therefore, the unconditional MLE of $\\theta$ is unbounded (infinite). This occurs because the estimated probability of exposure in the control group is $0$, leading to an estimated odds of $0$ for that group.\n\n**2. Conditional Maximum Likelihood Estimate of $\\theta$**\n\nIn the conditional analysis, we condition on the marginal totals of the table: $n_1=15$, $n_0=25$, $m_1=12$, $m_0=28$. The distribution of the count $a$ (exposed cases) follows the non-central hypergeometric distribution, with probability mass function depending only on $\\theta$:\n$$ P(A=k \\mid n_1, m_1, N; \\theta) = \\frac{\\binom{n_1}{k}\\binom{N-n_1}{m_1-k}\\theta^k}{\\sum_{j} \\binom{n_1}{j}\\binom{N-n_1}{m_1-j}\\theta^j} $$\nThe range of possible values for $A$ is $k \\in [\\max(0, n_1+m_1-N), \\min(n_1, m_1)]$.\nFor our data, the support is $k \\in [\\max(0, 15+12-40), \\min(15, 12)] = [\\max(0, -13), 12] = [0, 12]$.\nThe observed value is $a=12$, which is the maximum possible value in the support.\n\nThe conditional log-likelihood function is $l(\\theta; a) = a \\log(\\theta) + \\text{const} - \\log\\left(\\sum_{j} \\binom{n_1}{j}\\binom{N-n_1}{m_1-j}\\theta^j\\right)$. Its derivative with respect to $\\log(\\theta)$ is $a - E[A; \\theta]$. The conditional MLE is found by solving $E[A; \\theta] = a_{obs}$.\nThe expected value $E[A; \\theta]$ is a strictly increasing function of $\\theta$ that is bounded by the support of $A$, i.e., $0 \\le E[A; \\theta] \\le 12$. Since our observed value $a_{obs}=12$ is the upper boundary of the support, the equation $E[A; \\theta] = 12$ can only be satisfied in the limit as $\\theta \\to \\infty$. This situation is known as monotone likelihood or separation, and it implies that the conditional MLE of $\\theta$ also diverges to $+\\infty$.\n\n**3. Exact Conditional Confidence Set by Fisher's Inversion**\n\nA $100(1-\\alpha)\\%$ confidence set for $\\theta$ is the set of all values $\\theta_0$ such that a two-sided test of $H_0: \\theta = \\theta_0$ would not be rejected at level $\\alpha$. For $\\alpha=0.05$, we use $\\alpha/2 = 0.025$ for each tail.\nThe interval is $[L, U]$, where:\n-   $L$ is the value of $\\theta$ for which the upper tail probability equals $\\alpha/2$.\n    $ P(A \\ge a_{obs} \\mid \\theta=L) = \\alpha/2 $.\n    In our case, $a_{obs}=12$. Since $12$ is the maximum value, this simplifies to $P(A=12 \\mid \\theta=L) = 0.025$.\n    The probability $P(A=12 \\mid \\theta)$ is a strictly increasing function of $\\theta$. At $\\theta=0$, this probability is $0$. As $\\theta \\to \\infty$, it approaches $1$. Thus, there must be a unique, finite, positive value $L$ that solves this equation. For $\\theta=1$, $P(A=12) = \\frac{\\binom{15}{12}\\binom{25}{0}}{\\binom{40}{12}}$, which is extremely small ($\\approx 8 \\times 10^{-8}$). To make this probability $0.025$, we must have $L > 1$.\n\n-   $U$ is the value of $\\theta$ for which the lower tail probability equals $\\alpha/2$.\n    $ P(A \\le a_{obs} \\mid \\theta=U) = \\alpha/2 $.\n    In our case, $a_{obs}=12$. Since the support for $A$ is $[0, 12]$, the event $A \\le 12$ comprises the entire sample space.\n    Therefore, $P(A \\le 12 \\mid \\theta) = 1$ for any value of $\\theta$.\n    The equation to solve for $U$ becomes $1 = 0.025$, which is impossible. This means no value of $\\theta$, no matter how large, can be excluded from the confidence set based on this criterion. The upper bound is infinite.\n\nThe resulting $95\\%$ confidence set is a one-sided interval of the form $[L, \\infty)$, where $L>1$.\n\n**4. Small-Sample Adjustment**\n\nThe Haldane-Anscombe adjustment adds a pseudo-count of $0.5$ to each cell to mitigate issues with zero counts.\nThe adjusted table is: $a'=12.5$, $b'=0.5$, $c'=3.5$, $d'=25.5$.\nThe adjusted odds ratio estimate is:\n$$ \\hat{\\theta}_{adj} = \\frac{a'd'}{b'c'} = \\frac{12.5 \\times 25.5}{0.5 \\times 3.5} = \\frac{318.75}{1.75} \\approx 182.14 $$\nThis estimate is finite. Confidence intervals based on this adjusted table (e.g., Woolf's logit interval) will also have finite endpoints. This adjustment introduces a known bias in exchange for obtaining a finite estimate and interval.\n\nAnalyzing the provided options, option A correctly summarizes all these points: the divergence of both unconditional and conditional MLEs, the one-sided nature of the exact confidence interval, and the role of pseudo-counts in yielding a finite (but biased) result. The other options make incorrect claims about the behavior of these statistical methods.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}