{
    "hands_on_practices": [
        {
            "introduction": "Understanding the Central Limit Theorem begins with a firm grasp of the sample mean's properties. This foundational exercise requires deriving the variance of the sample mean, $\\operatorname{Var}(\\bar{X}_n)$, from first principles for independent and identically distributed data. Mastering this derivation illuminates precisely why statistical precision improves with the square root of the sample size, a key insight that underpins the construction of all large-sample confidence intervals.",
            "id": "4986781",
            "problem": "A clinical laboratory is validating a high-throughput assay for measuring a biomarker of systemic inflammation in hospitalized patients. Let $X_1, X_2, \\ldots, X_n$ denote the biomarker measurements on $n$ distinct patients, assumed to be independent and identically distributed (i.i.d.) with finite mean $E[X_i] = \\mu$ and finite variance $\\operatorname{Var}(X_i) = \\sigma^2$. The study team will report the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ and a large-sample confidence interval for $\\mu$ based on the Central Limit Theorem (CLT).\n\nStarting only from core definitions of expectation and variance, the property that independence implies factorization of expectations, and the well-tested fact that the Central Limit Theorem (CLT) provides a normal approximation for appropriately standardized averages when the second moment is finite, derive the variance of the sample mean $\\bar{X}_n$ from first principles without invoking any pre-memorized variance formulas. Then connect this derivation to the standard error that underlies large-sample confidence intervals for the mean in this medical context, explaining why the standard error scales with $n$ as it does under the CLT.\n\nProvide your final answer as a closed-form analytic expression for $\\operatorname{Var}(\\bar{X}_n)$ in terms of $\\sigma$ and $n$. Do not include any units in your final boxed expression. No numerical rounding is required.",
            "solution": "The problem requires the derivation of the variance of the sample mean, $\\operatorname{Var}(\\bar{X}_n)$, from first principles for a set of independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\ldots, X_n$. We are given that $E[X_i] = \\mu$ and $\\operatorname{Var}(X_i) = \\sigma^2$ are finite for all $i \\in \\{1, \\ldots, n\\}$. The derivation must rely only on the core definitions of expectation and variance, and the property of independence.\n\nFirst, let us state the definition of the sample mean, $\\bar{X}_n$:\n$$\n\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i\n$$\nThe core definition of the variance of any random variable $Y$ is given by $\\operatorname{Var}(Y) = E[(Y - E[Y])^2]$. To apply this definition to $\\bar{X}_n$, we must first determine its expectation, $E[\\bar{X}_n]$.\n\nUsing the linearity property of the expectation operator, which states that $E[aY + bZ] = aE[Y] + bE[Z]$ for constants $a, b$ and random variables $Y, Z$, we can compute $E[\\bar{X}_n]$ as follows:\n$$\nE[\\bar{X}_n] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right]\n$$\nThe term $\\frac{1}{n}$ is a constant and can be factored out of the expectation:\n$$\nE[\\bar{X}_n] = \\frac{1}{n} E\\left[\\sum_{i=1}^{n} X_i\\right]\n$$\nThe expectation of a sum is the sum of the expectations:\n$$\nE[\\bar{X}_n] = \\frac{1}{n} \\sum_{i=1}^{n} E[X_i]\n$$\nSince the random variables are identically distributed with mean $\\mu$, we have $E[X_i] = \\mu$ for all $i$. Substituting this into the expression gives:\n$$\nE[\\bar{X}_n] = \\frac{1}{n} \\sum_{i=1}^{n} \\mu = \\frac{1}{n} (n\\mu) = \\mu\n$$\nThus, the sample mean $\\bar{X}_n$ is an unbiased estimator of the population mean $\\mu$.\n\nNow we can proceed to derive the variance, $\\operatorname{Var}(\\bar{X}_n)$. Using the definition of variance with $Y = \\bar{X}_n$ and $E[Y] = \\mu$:\n$$\n\\operatorname{Var}(\\bar{X}_n) = E\\left[ (\\bar{X}_n - E[\\bar{X}_n])^2 \\right] = E\\left[ (\\bar{X}_n - \\mu)^2 \\right]\n$$\nSubstitute the definition of $\\bar{X}_n$:\n$$\n\\bar{X}_n - \\mu = \\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right) - \\mu = \\frac{1}{n}\\left(\\sum_{i=1}^{n} X_i - n\\mu\\right) = \\frac{1}{n}\\sum_{i=1}^{n} (X_i - \\mu)\n$$\nSquaring this expression, we obtain:\n$$\n(\\bar{X}_n - \\mu)^2 = \\left( \\frac{1}{n}\\sum_{i=1}^{n} (X_i - \\mu) \\right)^2 = \\frac{1}{n^2} \\left( \\sum_{i=1}^{n} (X_i - \\mu) \\right)^2\n$$\nThe squared sum can be expanded into a double summation, separating the terms where indices are equal from those where they are not:\n$$\n\\left( \\sum_{i=1}^{n} (X_i - \\mu) \\right)^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_i - \\mu)(X_j - \\mu) = \\sum_{i=1}^{n} (X_i - \\mu)^2 + \\sum_{i \\neq j} (X_i - \\mu)(X_j - \\mu)\n$$\nNow, we substitute this back into the expression for variance and apply the expectation operator:\n$$\n\\operatorname{Var}(\\bar{X}_n) = E\\left[ \\frac{1}{n^2} \\left( \\sum_{i=1}^{n} (X_i - \\mu)^2 + \\sum_{i \\neq j} (X_i - \\mu)(X_j - \\mu) \\right) \\right]\n$$\nUsing the linearity of expectation, we get:\n$$\n\\operatorname{Var}(\\bar{X}_n) = \\frac{1}{n^2} \\left( E\\left[\\sum_{i=1}^{n} (X_i - \\mu)^2\\right] + E\\left[\\sum_{i \\neq j} (X_i - \\mu)(X_j - \\mu)\\right] \\right)\n$$\n$$\n\\operatorname{Var}(\\bar{X}_n) = \\frac{1}{n^2} \\left( \\sum_{i=1}^{n} E[(X_i - \\mu)^2] + \\sum_{i \\neq j} E[(X_i - \\mu)(X_j - \\mu)] \\right)\n$$\nLet's evaluate the two expectation terms separately.\nFor the first term, by the definition of variance, $E[(X_i - \\mu)^2] = \\operatorname{Var}(X_i) = \\sigma^2$. Since this holds for all $i$:\n$$\n\\sum_{i=1}^{n} E[(X_i - \\mu)^2] = \\sum_{i=1}^{n} \\sigma^2 = n\\sigma^2\n$$\nFor the second term, which contains the cross-products where $i \\neq j$, we use the given property that the random variables $X_i$ and $X_j$ are independent. This implies that the random variables $(X_i - \\mu)$ and $(X_j - \\mu)$ are also independent. For independent random variables, the expectation of their product factorizes into the product of their individual expectations:\n$$\nE[(X_i - \\mu)(X_j - \\mu)] = E[X_i - \\mu] \\cdot E[X_j - \\mu] \\quad \\text{for } i \\neq j\n$$\nWe know that for any $k$, $E[X_k - \\mu] = E[X_k] - E[\\mu] = \\mu - \\mu = 0$.\nTherefore, for any pair with $i \\neq j$:\n$$\nE[(X_i - \\mu)(X_j - \\mu)] = 0 \\cdot 0 = 0\n$$\nSince every cross-product term in the sum $\\sum_{i \\neq j}$ has an expectation of zero, the entire sum is zero.\n\nSubstituting these results back into the equation for $\\operatorname{Var}(\\bar{X}_n)$:\n$$\n\\operatorname{Var}(\\bar{X}_n) = \\frac{1}{n^2} (n\\sigma^2 + 0) = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}\n$$\nThis completes the derivation of the variance of the sample mean from first principles.\n\nThe connection to the Central Limit Theorem (CLT) and the standard error is now direct. The CLT states that for a sufficiently large sample size $n$, the distribution of the standardized sample mean is approximately standard normal:\n$$\n\\frac{\\bar{X}_n - \\mu}{\\sqrt{\\operatorname{Var}(\\bar{X}_n)}} \\xrightarrow{d} N(0, 1)\n$$\nThe denominator, $\\sqrt{\\operatorname{Var}(\\bar{X}_n)}$, is the standard deviation of the sampling distribution of $\\bar{X}_n$, which is defined as the **standard error** of the mean, denoted $\\text{SE}(\\bar{X}_n)$. From our derivation, we have $\\operatorname{Var}(\\bar{X}_n) = \\frac{\\sigma^2}{n}$, so the standard error is:\n$$\n\\text{SE}(\\bar{X}_n) = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\n$$\nThis quantity is fundamental for constructing large-sample confidence intervals for $\\mu$. For example, a $95\\%$ confidence interval is approximately $\\bar{X}_n \\pm 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}}$ (if $\\sigma$ is known) or $\\bar{X}_n \\pm 1.96 \\cdot \\frac{s}{\\sqrt{n}}$ (using the sample standard deviation $s$ as an estimate for $\\sigma$).\n\nThe derivation explains the scaling of the standard error with the sample size $n$. The variance $\\operatorname{Var}(\\bar{X}_n)$ is inversely proportional to $n$. This occurs because averaging over $n$ variables introduces a scaling factor of $\\frac{1}{n}$, which becomes $\\frac{1}{n^2}$ in the variance calculation. Meanwhile, the sum of the variances of the individual, independent measurements contributes a term proportional to $n$ (specifically, $n\\sigma^2$). The ratio of these effects, $\\frac{n\\sigma^2}{n^2}$, results in the $\\frac{1}{n}$ dependence. Consequently, the standard error, being the square root of the variance, scales as $\\frac{1}{\\sqrt{n}}$. This scaling reflects the fact that as more independent measurements are averaged, random errors tend to cancel out, leading to a more precise estimate of the true mean $\\mu$. To double the precision of the estimate (i.e., to halve the standard error), the sample size $n$ must be quadrupled, a critical consideration in designing clinical studies and other experiments.",
            "answer": "$$\\boxed{\\frac{\\sigma^2}{n}}$$"
        },
        {
            "introduction": "Real-world data analysis rarely involves known population variances, a gap this exercise bridges by moving from pure theory to a practical application. Set within a common clinical study design, this practice involves applying the Central Limit Theorem to paired differences to assess an intervention's effect. The core task is to justify \"studentization\"—the substitution of the unknown population variance with a sample estimate—by rigorously applying Slutsky's theorem, thereby providing the theoretical foundation for common procedures like the paired t-test.",
            "id": "4986800",
            "problem": "A longitudinal clinical study measures a continuous biomarker before and after an intervention in a cohort of $n$ patients. For patient $i$, denote the pre-intervention measurement by $X_{i}$ and the post-intervention measurement by $Y_{i}$. Assume the pairs $\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) across patients, with arbitrary within-pair dependence, and that there exists $\\delta>0$ such that $\\mathbb{E}(|X_{1}|^{2+\\delta})<\\infty$ and $\\mathbb{E}(|Y_{1}|^{2+\\delta})<\\infty$. Define the paired difference $D_{i}=X_{i}-Y_{i}$ and let $\\bar{D}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}D_{i}$ denote the sample mean of differences. Let $\\mu_{D}=\\mathbb{E}(D_{1})$ and let $S_{D}^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(D_{i}-\\bar{D}_{n})^{2}$ denote the sample variance of the differences.\n\nStarting from core definitions of independence, expectation, and variance, and using only well-tested probabilistic facts such as the Lindeberg–Feller Central Limit Theorem (CLT) and Slutsky’s theorem, do the following:\n\n1. Establish the asymptotic normality of $\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})$ under the stated moment assumptions, making explicit the role of the moment condition in verifying the requisite CLT condition for $\\{D_{i}\\}$.\n2. Justify studentization by proving that $T_{n}=\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})/S_{D}$ converges in distribution to a standard normal random variable under the same assumptions, and explain why the consistency of $S_{D}^{2}$ is sufficient for this conclusion.\n3. Suppose additionally that $\\operatorname{Var}(X_{1})=\\sigma_{X}^{2}$, $\\operatorname{Var}(Y_{1})=\\sigma_{Y}^{2}$, and $\\operatorname{Cov}(X_{1},Y_{1})=\\sigma_{XY}$. Derive the explicit expression, in closed form, for the asymptotic variance of $\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})$ in terms of $\\sigma_{X}^{2}$, $\\sigma_{Y}^{2}$, and $\\sigma_{XY}$.\n\nProvide your final answer as the single closed-form analytic expression for the asymptotic variance requested in item 3. No numerical approximation is needed; do not round. No units are required.",
            "solution": "The problem addresses the asymptotic properties of the sample mean of paired differences, a cornerstone of statistical inference for two-sample paired data. We will address the three parts sequentially.\n\nFirst, we define properties of the random variables $D_{i}$. Since the pairs $(X_{i}, Y_{i})$ are i.i.d. for $i=1, \\dots, n$, the differences $D_{i}=X_{i}-Y_{i}$ also form an i.i.d. sequence of random variables.\n\nThe mean of $D_i$ is $\\mathbb{E}(D_{i}) = \\mathbb{E}(X_{i} - Y_{i})$. By linearity of expectation, $\\mathbb{E}(D_{i}) = \\mathbb{E}(X_{i}) - \\mathbb{E}(Y_{i})$. Since the pairs are identically distributed, this is constant for all $i$, and we denote it $\\mu_D = \\mathbb{E}(D_{1})$. The existence of moments of order $2+\\delta$ implies that all lower-order moments, including the first, are finite.\n\nThe variance of $D_i$ is $\\operatorname{Var}(D_i) = \\operatorname{Var}(X_i - Y_i)$. Let's denote $\\sigma_{D}^{2} = \\operatorname{Var}(D_1)$. We must verify that this variance is finite. The condition $\\mathbb{E}(|X_{1}|^{2+\\delta}) < \\infty$ implies $\\mathbb{E}(X_{1}^{2}) < \\infty$, and similarly $\\mathbb{E}(Y_{1}^{2}) < \\infty$. Using the triangle inequality and the $c_r$-inequality ($|a+b|^p \\le 2^{p-1}(|a|^p+|b|^p)$), we have $\\mathbb{E}(D_1^2) = \\mathbb{E}((X_1-Y_1)^2) \\le \\mathbb{E}((|X_1|+|Y_1|)^2) = \\mathbb{E}(X_1^2 + Y_1^2 + 2|X_1 Y_1|)$. By the Cauchy-Schwarz inequality, $\\mathbb{E}(|X_1 Y_1|) \\le \\sqrt{\\mathbb{E}(X_1^2)\\mathbb{E}(Y_1^2)}$, which is finite. Therefore, $\\mathbb{E}(D_1^2)$ is finite, and so is the variance $\\sigma_{D}^{2} = \\mathbb{E}(D_1^2) - \\mu_D^2$.\n\n**1. Asymptotic Normality of $\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})$**\n\nWe aim to apply the Central Limit Theorem to the i.i.d. sequence $\\{D_i\\}_{i=1}^n$. The classical (Lindeberg-Lévy) CLT requires the variables to be i.i.d. with finite mean and variance. We have already established these properties for $\\{D_i\\}$. Thus, the CLT directly applies.\nThe theorem states that $\\frac{\\sum_{i=1}^{n}(D_i - \\mu_D)}{\\sqrt{n\\sigma_D^2}} \\xrightarrow{d} N(0,1)$, where $\\xrightarrow{d}$ denotes convergence in distribution.\nRearranging the term, we get $\\frac{\\sqrt{n}(\\frac{1}{n}\\sum_{i=1}^{n}D_i - \\mu_D)}{\\sigma_D} = \\frac{\\sqrt{n}(\\bar{D}_n - \\mu_D)}{\\sigma_D} \\xrightarrow{d} N(0,1)$.\nThis is equivalent to stating that $\\sqrt{n}(\\bar{D}_n - \\mu_D) \\xrightarrow{d} N(0, \\sigma_D^2)$. Thus, the sequence $\\sqrt{n}(\\bar{D}_n - \\mu_D)$ is asymptotically normal with mean $0$ and asymptotic variance $\\sigma_D^2$.\n\nThe problem specifically requests an explanation using the Lindeberg-Feller CLT and the role of the moment condition $\\mathbb{E}(|D_1|^{2+\\delta}) < \\infty$. The Lindeberg condition for a sequence of independent random variables $\\{Z_i\\}$ with $\\mathbb{E}(Z_i)=0$ and finite variances $\\sigma_i^2$ is:\nFor any $\\epsilon > 0$, $\\lim_{n \\to \\infty} \\frac{1}{s_n^2} \\sum_{i=1}^n \\mathbb{E} \\left[ Z_i^2 \\cdot \\mathbb{I}(|Z_i| > \\epsilon s_n) \\right] = 0$, where $s_n^2 = \\sum_{i=1}^n \\sigma_i^2$.\n\nIn our case, let $Z_i = D_i - \\mu_D$. These are i.i.d. with $\\mathbb{E}(Z_i)=0$ and $\\operatorname{Var}(Z_i) = \\sigma_D^2$. So, $s_n^2 = n\\sigma_D^2$. The Lindeberg condition becomes:\n$$ \\lim_{n \\to \\infty} \\frac{1}{n\\sigma_D^2} \\sum_{i=1}^n \\mathbb{E} \\left[ (D_i - \\mu_D)^2 \\cdot \\mathbb{I}(|D_i - \\mu_D| > \\epsilon \\sqrt{n}\\sigma_D) \\right] = 0 $$\nSince the terms are identically distributed, this simplifies to:\n$$ \\lim_{n \\to \\infty} \\frac{1}{\\sigma_D^2} \\mathbb{E} \\left[ (D_1 - \\mu_D)^2 \\cdot \\mathbb{I}(|D_1 - \\mu_D| > \\epsilon \\sqrt{n}\\sigma_D) \\right] = 0 $$\nThe stronger moment condition, $\\mathbb{E}(|X_{1}|^{2+\\delta}) < \\infty$ and $\\mathbb{E}(|Y_{1}|^{2+\\delta}) < \\infty$, implies $\\mathbb{E}(|D_1|^{2+\\delta}) < \\infty$ by the $c_r$-inequality. Let $W = D_1 - \\mu_D$. Then $\\mathbb{E}(|W|^{2+\\delta})$ is also finite. We can use this to bound the expectation:\n$$ \\mathbb{E} \\left[ W^2 \\cdot \\mathbb{I}(|W| > C_n) \\right] \\text{ where } C_n = \\epsilon \\sqrt{n}\\sigma_D \\to \\infty $$\n$$ \\mathbb{E} \\left[ W^2 \\cdot \\frac{|W|^\\delta}{|W|^\\delta} \\cdot \\mathbb{I}(|W| > C_n) \\right] \\le \\mathbb{E} \\left[ W^2 \\cdot \\frac{|W|^\\delta}{C_n^\\delta} \\cdot \\mathbb{I}(|W| > C_n) \\right] = \\frac{1}{C_n^\\delta} \\mathbb{E} \\left[ |W|^{2+\\delta} \\cdot \\mathbb{I}(|W| > C_n) \\right] $$\nThis can be further bounded by $\\frac{1}{C_n^\\delta} \\mathbb{E} \\left[ |W|^{2+\\delta} \\right]$.\nSince $\\mathbb{E} \\left[ |W|^{2+\\delta} \\right] < \\infty$ and $C_n^\\delta = (\\epsilon\\sigma_D)^\\delta n^{\\delta/2} \\to \\infty$ as $n \\to \\infty$, the entire expression converges to $0$. This verifies the Lindeberg condition. The existence of a moment of order $2+\\delta$ (for $\\delta>0$) provides a mechanism to prove that the tail integral vanishes, satisfying the CLT's requirements.\n\n**2. Justification of Studentization**\n\nWe need to show that $T_n = \\frac{\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})}{S_{D}}$ converges in distribution to a standard normal variable, $N(0,1)$. This process, called studentization, involves replacing the unknown population standard deviation $\\sigma_D$ in the normalizing factor with its sample estimate $S_D$. The validity of this replacement hinges on Slutsky's theorem.\n\nSlutsky's theorem states that if $A_n \\xrightarrow{d} A$ and $B_n \\xrightarrow{p} c$ (where $c$ is a constant and $\\xrightarrow{p}$ denotes convergence in probability), then $A_n / B_n \\xrightarrow{d} A / c$.\n\nFrom part 1, we identify $A_n = \\sqrt{n}(\\bar{D}_n - \\mu_D)$, and we know that $A_n \\xrightarrow{d} A \\sim N(0, \\sigma_D^2)$.\nWe identify $B_n = S_D$. To apply Slutsky's theorem, we must show that $S_D$ converges in probability to the constant $\\sigma_D$. This is equivalent to showing that its square, $S_D^2$, converges in probability to $\\sigma_D^2$. That is, we must prove that $S_D^2$ is a consistent estimator of $\\sigma_D^2$.\n\nThe sample variance is $S_D^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(D_{i}-\\bar{D}_{n})^{2}$. This can be rewritten as:\n$$ S_D^2 = \\frac{n}{n-1} \\left( \\frac{1}{n}\\sum_{i=1}^n D_i^2 - \\bar{D}_n^2 \\right) $$\nAs $n \\to \\infty$, the factor $\\frac{n}{n-1} \\to 1$.\nThe variables $D_i$ are i.i.d. with finite mean $\\mu_D$. Thus, by the Weak Law of Large Numbers (WLLN), the sample mean converges in probability to the population mean: $\\bar{D}_n \\xrightarrow{p} \\mu_D$.\nThe variables $D_i^2$ are also i.i.d. and we have shown that $\\mathbb{E}(D_1^2) < \\infty$. Therefore, by the WLLN, their sample mean also converges in probability to the population mean: $\\frac{1}{n}\\sum_{i=1}^n D_i^2 \\xrightarrow{p} \\mathbb{E}(D_1^2)$.\n\nBy the continuous mapping theorem, if $g$ is a continuous function, $Z_n \\xrightarrow{p} c \\implies g(Z_n) \\xrightarrow{p} g(c)$. Applying this, $\\bar{D}_n^2 \\xrightarrow{p} \\mu_D^2$.\nCombining these results using properties of convergence in probability:\n$$ S_D^2 = \\left(\\frac{n}{n-1}\\right) \\left( \\frac{1}{n}\\sum_{i=1}^n D_i^2 - \\bar{D}_n^2 \\right) \\xrightarrow{p} (1) \\cdot (\\mathbb{E}(D_1^2) - \\mu_D^2) = \\sigma_D^2 $$\nThus, $S_D^2$ is a consistent estimator of $\\sigma_D^2$. This is precisely what was needed. The consistency of $S_D^2$ is sufficient.\nAgain, by the continuous mapping theorem (with the square root function, which is continuous for non-negative values), $S_D = \\sqrt{S_D^2} \\xrightarrow{p} \\sqrt{\\sigma_D^2} = \\sigma_D$ (assuming $\\sigma_D^2 > 0$).\n\nNow we apply Slutsky's theorem:\n$$ T_n = \\frac{A_n}{B_n} = \\frac{\\sqrt{n}(\\bar{D}_{n}-\\mu_{D})}{S_{D}} \\xrightarrow{d} \\frac{N(0, \\sigma_D^2)}{\\sigma_D} \\sim N\\left(\\frac{0}{\\sigma_D}, \\frac{\\sigma_D^2}{\\sigma_D^2}\\right) \\sim N(0,1) $$\nThis completes the justification. The consistency of $S_D^2$ (and thus $S_D$) allows it to be treated as a constant in the limit, ensuring the resulting statistic has a parameter-free standard normal distribution.\n\n**3. Explicit Expression for Asymptotic Variance**\n\nThe asymptotic variance of $\\sqrt{n}(\\bar{D}_n - \\mu_D)$ is, by definition, the variance of the limiting normal distribution established in part 1. This variance is $\\sigma_D^2 = \\operatorname{Var}(D_1)$. We need to express this in terms of $\\sigma_{X}^{2} = \\operatorname{Var}(X_{1})$, $\\sigma_{Y}^{2} = \\operatorname{Var}(Y_{1})$, and $\\sigma_{XY} = \\operatorname{Cov}(X_{1},Y_{1})$.\n\nUsing the properties of variance, we have:\n$$ \\sigma_D^2 = \\operatorname{Var}(D_1) = \\operatorname{Var}(X_1 - Y_1) $$\nThe formula for the variance of a difference of two random variables is $\\operatorname{Var}(A-B) = \\operatorname{Var}(A) + \\operatorname{Var}(B) - 2\\operatorname{Cov}(A,B)$. Applying this formula with $A=X_1$ and $B=Y_1$:\n$$ \\operatorname{Var}(X_1 - Y_1) = \\operatorname{Var}(X_1) + \\operatorname{Var}(Y_1) - 2\\operatorname{Cov}(X_1, Y_1) $$\nSubstituting the given notation:\n$$ \\sigma_D^2 = \\sigma_{X}^{2} + \\sigma_{Y}^{2} - 2\\sigma_{XY} $$\nThis expression represents the explicit closed form for the asymptotic variance of $\\sqrt{n}(\\bar{D}_n - \\mu_D)$.",
            "answer": "$$\\boxed{\\sigma_{X}^{2} + \\sigma_{Y}^{2} - 2\\sigma_{XY}}$$"
        },
        {
            "introduction": "The power of the Central Limit Theorem extends far beyond simple averages to functions of estimators. In medical statistics, we are often interested in quantities like probabilities, odds, or risks, which are transformations of underlying model parameters. This practice introduces the Delta Method, a pivotal technique that leverages the CLT and Taylor's theorem to derive the asymptotic distribution of these transformed parameters, enabling the construction of confidence intervals for a vast range of clinically relevant metrics.",
            "id": "852589",
            "problem": "Consider a sequence of $n$ independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\dots, X_n$, each following a Poisson distribution with an unknown parameter $\\lambda > 0$. The probability mass function (PMF) is given by $P(X=k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}$ for $k = 0, 1, 2, \\dots$.\n\nWe are interested in estimating the probability that a new observation from this distribution is zero. This probability is a function of the parameter $\\lambda$, given by $g(\\lambda) = P(X=0) = e^{-\\lambda}$.\n\nLet $\\hat{\\theta}_n$ be the maximum likelihood estimator (MLE) for the parameter of interest, $\\theta = g(\\lambda)$. According to the theory of maximum likelihood estimation and the Central Limit Theorem, the estimator $\\hat{\\theta}_n$ is asymptotically normal. Specifically, the distribution of $\\sqrt{n}(\\hat{\\theta}_n - \\theta)$ converges to a normal distribution with a mean of 0 and some variance $V$. This variance $V$ is often referred to as the asymptotic variance of the estimator.\n\nDerive a closed-form expression for this asymptotic variance $V$ in terms of the parameter $\\lambda$.",
            "solution": "1. Relevant functions and estimator  \nWe have $g(\\lambda)=P(X=0)=e^{-\\lambda}$ and the MLE $\\hat\\lambda=\\overline X$.  By the delta method,  \n$$\\sqrt{n}\\bigl(\\hat\\theta-\\theta\\bigr)\\xrightarrow{d}N\\!\\Bigl(0,\\;\\frac{[g'(\\lambda)]^2}{I(\\lambda)}\\Bigr),$$  \nwhere $\\theta=g(\\lambda)$.\n\n2. Compute derivative $g'(\\lambda)$  \n$$g'(\\lambda)=\\frac{d}{d\\lambda}e^{-\\lambda}=-\\,e^{-\\lambda},\\qquad [g'(\\lambda)]^2=e^{-2\\lambda}.$$\n\n3. Fisher information for Poisson($\\lambda$)  \nThe log‐likelihood for one observation is $\\ell(\\lambda)= -\\lambda+X\\ln\\lambda-\\ln(X!)$.  Hence  \n$$\\frac{\\partial^2\\ell}{\\partial\\lambda^2}=-\\frac{X}{\\lambda^2},\\qquad I(\\lambda)=-E\\Bigl[\\frac{\\partial^2\\ell}{\\partial\\lambda^2}\\Bigr]\n=E\\bigl[X\\bigr]/\\lambda^2=\\lambda/\\lambda^2=\\frac1\\lambda.$$\n\n4. Asymptotic variance $V$  \nSubstitute into the delta‐method variance:  \n$$V=\\frac{[g'(\\lambda)]^2}{I(\\lambda)}=\\frac{e^{-2\\lambda}}{1/\\lambda}=\\lambda\\,e^{-2\\lambda}.$$",
            "answer": "$$\\boxed{\\lambda\\,e^{-2\\lambda}}$$"
        }
    ]
}