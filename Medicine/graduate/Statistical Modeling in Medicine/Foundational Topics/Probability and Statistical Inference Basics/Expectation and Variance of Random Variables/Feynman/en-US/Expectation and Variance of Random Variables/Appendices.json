{
    "hands_on_practices": [
        {
            "introduction": "Understanding the properties of complex random variables often begins by decomposing them into simpler, more manageable components. This first-principles exercise asks you to derive the expectation and variance of a Negative Binomial random variable, which models the number of failures before a fixed number of successes. By treating the total count as a sum of independent Geometric random variables , you will apply the fundamental laws of expectation and variance to build the solution from the ground up, reinforcing a core problem-solving technique in probability theory.",
            "id": "4962576",
            "problem": "A clinical microbiology laboratory monitors whether a patient’s blood culture draws are contaminated. Each draw either results in a contamination event (failure) with probability $p \\in (0,1)$ or a clean result (success) with probability $1-p$. Assume draws are conducted sequentially and independently under stable conditions. The protocol continues until $r \\in \\mathbb{N}$ clean results (successes) have been observed, at which point the sampling stops. Define the random variable $X$ to be the total number of contamination events (failures) that occur before the $r$-th clean result is observed.\n\nStarting from the definitions of independent Bernoulli trials and the expectation and variance of random variables, and without appealing to any pre-derived formulas for the negative binomial distribution, derive closed-form analytic expressions for the expectation $\\mathbb{E}[X]$ and the variance $\\mathrm{Var}(X)$ in terms of $r$ and $p$ under this failures-until-$r$ stopping rule. Express your final answer as exact symbolic expressions; no numerical rounding is required.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- A sequence of independent Bernoulli trials is conducted.\n- Each trial results in a \"contamination event\" (failure) with probability $p$ or a \"clean result\" (success) with probability $1-p$.\n- The probability of failure is specified as $p \\in (0,1)$.\n- The process terminates upon the observation of the $r$-th success, where $r \\in \\mathbb{N}$.\n- The random variable $X$ is defined as the total number of failures that occur before the $r$-th success.\n- The objective is to derive the expectation $\\mathbb{E}[X]$ and variance $\\mathrm{Var}(X)$ in terms of $r$ and $p$.\n- The derivation must not rely on pre-derived formulas for the negative binomial distribution but must start from the definitions of independent Bernoulli trials and the properties of expectation and variance.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It describes a classic scenario in probability theory modeled by the negative binomial distribution. The setup of sequential, independent Bernoulli trials is a fundamental concept in statistics. The parameters $p$ and $r$ are clearly defined with appropriate domains, ensuring the problem is self-contained and mathematically consistent. The task is to perform a derivation from first principles, which is a standard and non-trivial exercise in probability theory. The problem does not violate any scientific principles, is not ambiguous, and contains all necessary information for a unique solution.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full, reasoned solution will be provided.\n\n### Derivation\nThe problem describes a sequence of independent Bernoulli trials where the probability of success is $1-p$ and the probability of failure is $p$. The random variable $X$ counts the total number of failures until $r$ successes are accumulated.\n\nTo derive the expectation and variance of $X$ from first principles, we can decompose $X$ into a sum of simpler random variables. Let $Y_i$ be the number of failures between the $(i-1)$-th success and the $i$-th success, for $i=1, 2, \\dots, r$. We define the $0$-th success as the start of the experiment.\n\nThe total number of failures $X$ is the sum of these intermediate failure counts:\n$$X = Y_1 + Y_2 + \\dots + Y_r$$\nBecause the trials are independent and the process conditions are stable, the number of failures after one success does not influence the number of failures until the next. Therefore, the random variables $Y_1, Y_2, \\dots, Y_r$ are independent and identically distributed (i.i.d.).\n\nLet us determine the probability distribution of any one of these variables, say $Y_i$. The event $Y_i=k$ (where $k \\in \\{0, 1, 2, \\dots\\}$) signifies that, after the $(i-1)$-th success, there are exactly $k$ failures followed by one success. The probability of this specific sequence of $k+1$ trials is $p^k(1-p)$. Thus, the probability mass function (PMF) of $Y_i$ is:\n$$P(Y_i=k) = p^k(1-p) \\quad \\text{for } k = 0, 1, 2, \\dots$$\nThis defines a geometric distribution on the non-negative integers.\n\n**Derivation of Expectation $\\mathbb{E}[X]$**\n\nBy the linearity of expectation, the expectation of a sum of random variables is the sum of their expectations:\n$$\\mathbb{E}[X] = \\mathbb{E}\\left[\\sum_{i=1}^{r} Y_i\\right] = \\sum_{i=1}^{r} \\mathbb{E}[Y_i]$$\nSince the $Y_i$ are identically distributed, $\\mathbb{E}[Y_i] = \\mathbb{E}[Y_1]$ for all $i$. Therefore:\n$$\\mathbb{E}[X] = r \\mathbb{E}[Y_1]$$\nWe compute $\\mathbb{E}[Y_1]$ directly from its definition:\n$$\\mathbb{E}[Y_1] = \\sum_{k=0}^{\\infty} k \\cdot P(Y_1=k) = \\sum_{k=0}^{\\infty} k p^k (1-p) = (1-p) \\sum_{k=0}^{\\infty} k p^k$$\nTo evaluate the summation, we use the formula for a geometric series and its derivative. For $|z|1$, we have:\n$$\\sum_{k=0}^{\\infty} z^k = \\frac{1}{1-z}$$\nDifferentiating both sides with respect to $z$ gives:\n$$\\frac{d}{dz} \\sum_{k=0}^{\\infty} z^k = \\sum_{k=1}^{\\infty} k z^{k-1} = \\frac{d}{dz} \\left(\\frac{1}{1-z}\\right) = \\frac{1}{(1-z)^2}$$\nMultiplying by $z$:\n$$\\sum_{k=1}^{\\infty} k z^k = \\frac{z}{(1-z)^2}$$\nSince the $k=0$ term in $\\sum k z^k$ is zero, this is also equal to $\\sum_{k=0}^{\\infty} k z^k$. Substituting $z=p$ (since $p \\in (0,1)$, we have $|p|1$):\n$$\\sum_{k=0}^{\\infty} k p^k = \\frac{p}{(1-p)^2}$$\nNow, we substitute this back into the expression for $\\mathbb{E}[Y_1]$:\n$$\\mathbb{E}[Y_1] = (1-p) \\left( \\frac{p}{(1-p)^2} \\right) = \\frac{p}{1-p}$$\nFinally, the expectation of $X$ is:\n$$\\mathbb{E}[X] = r \\mathbb{E}[Y_1] = \\frac{rp}{1-p}$$\n\n**Derivation of Variance $\\mathrm{Var}(X)$**\n\nSince the random variables $Y_1, Y_2, \\dots, Y_r$ are independent, the variance of their sum is the sum of their variances:\n$$\\mathrm{Var}(X) = \\mathrm{Var}\\left(\\sum_{i=1}^{r} Y_i\\right) = \\sum_{i=1}^{r} \\mathrm{Var}(Y_i)$$\nAs the $Y_i$ are identically distributed, $\\mathrm{Var}(Y_i) = \\mathrm{Var}(Y_1)$ for all $i$. Thus:\n$$\\mathrm{Var}(X) = r \\mathrm{Var}(Y_1)$$\nThe variance of $Y_1$ is given by $\\mathrm{Var}(Y_1) = \\mathbb{E}[Y_1^2] - (\\mathbb{E}[Y_1])^2$. We already found $\\mathbb{E}[Y_1]$. We now need to compute $\\mathbb{E}[Y_1^2]$:\n$$\\mathbb{E}[Y_1^2] = \\sum_{k=0}^{\\infty} k^2 \\cdot P(Y_1=k) = \\sum_{k=0}^{\\infty} k^2 p^k (1-p) = (1-p) \\sum_{k=0}^{\\infty} k^2 p^k$$\nTo evaluate $\\sum k^2 p^k$, we differentiate the series $\\sum k z^k = \\frac{z}{(1-z)^2}$ with respect to $z$:\n$$\\frac{d}{dz} \\sum_{k=1}^{\\infty} k z^k = \\sum_{k=1}^{\\infty} k^2 z^{k-1} = \\frac{d}{dz} \\left( \\frac{z}{(1-z)^2} \\right)$$\nUsing the quotient rule:\n$$\\frac{d}{dz} \\left( \\frac{z}{(1-z)^2} \\right) = \\frac{(1)(1-z)^2 - z(2(1-z)(-1))}{((1-z)^2)^2} = \\frac{(1-z)^2 + 2z(1-z)}{(1-z)^4} = \\frac{1-z+2z}{(1-z)^3} = \\frac{1+z}{(1-z)^3}$$\nMultiplying by $z$:\n$$\\sum_{k=1}^{\\infty} k^2 z^k = \\frac{z(1+z)}{(1-z)^3}$$\nThis is equivalent to $\\sum_{k=0}^{\\infty} k^2 z^k$. We substitute $z=p$:\n$$\\sum_{k=0}^{\\infty} k^2 p^k = \\frac{p(1+p)}{(1-p)^3}$$\nNow we compute $\\mathbb{E}[Y_1^2]$:\n$$\\mathbb{E}[Y_1^2] = (1-p) \\left( \\frac{p(1+p)}{(1-p)^3} \\right) = \\frac{p(1+p)}{(1-p)^2}$$\nWe can now calculate the variance of $Y_1$:\n$$\\mathrm{Var}(Y_1) = \\mathbb{E}[Y_1^2] - (\\mathbb{E}[Y_1])^2 = \\frac{p(1+p)}{(1-p)^2} - \\left(\\frac{p}{1-p}\\right)^2$$\n$$\\mathrm{Var}(Y_1) = \\frac{p+p^2}{(1-p)^2} - \\frac{p^2}{(1-p)^2} = \\frac{p+p^2-p^2}{(1-p)^2} = \\frac{p}{(1-p)^2}$$\nFinally, the variance of $X$ is:\n$$\\mathrm{Var}(X) = r \\mathrm{Var}(Y_1) = \\frac{rp}{(1-p)^2}$$\n\nThe derived expressions for the expectation and variance are $\\mathbb{E}[X] = \\frac{rp}{1-p}$ and $\\mathrm{Var}(X) = \\frac{rp}{(1-p)^2}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{rp}{1-p}  \\frac{rp}{(1-p)^2} \\end{pmatrix}}$$"
        },
        {
            "introduction": "In applied statistical modeling, non-linear transformations are ubiquitous, but they can introduce subtle biases if not handled with care. This practice problem confronts a classic issue in medical research: estimating the mean of a log-normally distributed quantity, such as a biomarker concentration. You will demonstrate why simply exponentiating the sample mean of the log-transformed data results in a biased estimator and then derive the correct, model-based estimator , providing a crucial lesson on the interplay between expectation, variance, and non-linear functions where $\\mathbb{E}[g(X)] \\neq g(\\mathbb{E}[X])$.",
            "id": "4962594",
            "problem": "In a longitudinal clinical study of an anti-inflammatory therapy, the within-patient plasma concentration of a biomarker is modeled as log-normal due to multiplicative biological variability. Specifically, for a given time point, suppose $Y$ denotes the biomarker concentration (measured on the original scale), and let $X = \\ln(Y)$. Assume a parametric model in which $X$ is independently and identically distributed across patients as a normal random variable with mean $\\mu$ and variance $\\sigma^{2}$, that is, $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. From an observed sample $\\{Y_{1},\\dots,Y_{n}\\}$, define $X_{i} = \\ln(Y_{i})$ and denote the maximum likelihood estimators (MLE) for the normal parameters by $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ and $\\hat{\\sigma}^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i} - \\hat{\\mu})^{2}$. A widely used “naive” estimator of the expected concentration $\\mathbb{E}[Y]$ is $\\exp(\\hat{\\mu})$. \n\nUsing only definitions of expectation and well-established properties of the normal distribution, provide a first-principles derivation of $\\mathbb{E}[Y]$ in terms of $\\mu$ and $\\sigma^{2}$, and then show that the naive estimator $\\exp(\\hat{\\mu})$ is biased for $\\mathbb{E}[Y]$ under the specified model. Based on these derivations, propose and simplify a closed-form, model-based plug-in estimator that corrects for the transformation-induced bias when estimating $\\mathbb{E}[Y]$ from the observed sample, expressed solely in terms of $\\hat{\\mu}$ and $\\hat{\\sigma}^{2}$. Your final answer must be a single closed-form analytic expression. No rounding is required, and no units should be included in your final expression.",
            "solution": "The problem asks for a derivation of the expected value of a log-normally distributed random variable, an analysis of the bias of a naive estimator for this expected value, and the formulation of a corrected plug-in estimator. The validation of the problem statement confirms that it is scientifically grounded, well-posed, and objective. We can proceed with the solution.\n\nThe problem defines a random variable $Y$ representing a biomarker concentration, such that $X = \\ln(Y)$ follows a normal distribution with mean $\\mu$ and variance $\\sigma^2$. We write this as $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The random variable $Y$ is therefore log-normally distributed.\n\nFirst, we derive the expected value of $Y$, denoted $\\mathbb{E}[Y]$, from first principles. Since $Y = \\exp(X)$, its expectation is given by the integral of $\\exp(x)$ weighted by the probability density function (PDF) of $X$. The PDF of $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is:\n$$f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\nThe expectation of $Y$ is then:\n$$\\mathbb{E}[Y] = \\mathbb{E}[\\exp(X)] = \\int_{-\\infty}^{\\infty} \\exp(x) f_X(x) dx = \\int_{-\\infty}^{\\infty} \\exp(x) \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx$$\nWe can combine the arguments of the exponential functions:\n$$\\mathbb{E}[Y] = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int_{-\\infty}^{\\infty} \\exp\\left(x - \\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx$$\nTo solve this integral, we complete the square for the term in the exponent:\n$$x - \\frac{(x-\\mu)^2}{2\\sigma^2} = x - \\frac{x^2 - 2\\mu x + \\mu^2}{2\\sigma^2} = \\frac{2\\sigma^2 x - (x^2 - 2\\mu x + \\mu^2)}{2\\sigma^2}$$\n$$= -\\frac{1}{2\\sigma^2} [x^2 - 2\\mu x - 2\\sigma^2 x + \\mu^2] = -\\frac{1}{2\\sigma^2} [x^2 - 2(\\mu+\\sigma^2)x + \\mu^2]$$\nWe complete the square for the polynomial in $x$: $x^2 - 2(\\mu+\\sigma^2)x + \\mu^2 = [x - (\\mu+\\sigma^2)]^2 - (\\mu+\\sigma^2)^2 + \\mu^2 = [x - (\\mu+\\sigma^2)]^2 - (\\mu^2 + 2\\mu\\sigma^2 + \\sigma^4) + \\mu^2 = [x - (\\mu+\\sigma^2)]^2 - 2\\mu\\sigma^2 - \\sigma^4$.\nSubstituting this back into the exponent:\n$$-\\frac{1}{2\\sigma^2} \\left( [x - (\\mu+\\sigma^2)]^2 - 2\\mu\\sigma^2 - \\sigma^4 \\right) = -\\frac{[x - (\\mu+\\sigma^2)]^2}{2\\sigma^2} + \\frac{2\\mu\\sigma^2 + \\sigma^4}{2\\sigma^2} = -\\frac{[x - (\\mu+\\sigma^2)]^2}{2\\sigma^2} + \\mu + \\frac{\\sigma^2}{2}$$\nWe can now rewrite the integral for $\\mathbb{E}[Y]$:\n$$\\mathbb{E}[Y] = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{[x - (\\mu+\\sigma^2)]^2}{2\\sigma^2} + \\mu + \\frac{\\sigma^2}{2}\\right) dx$$\nThe term $\\exp(\\mu + \\frac{\\sigma^2}{2})$ is a constant with respect to $x$ and can be factored out:\n$$\\mathbb{E}[Y] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{[x - (\\mu+\\sigma^2)]^2}{2\\sigma^2}\\right) dx$$\nThe integral is the total area under the PDF of a normal distribution with mean $\\mu' = \\mu+\\sigma^2$ and variance $\\sigma^2$. This integral is equal to $1$.\nTherefore, the expected value of $Y$ is:\n$$\\mathbb{E}[Y] = \\exp\\left(\\mu + \\frac{1}{2}\\sigma^2\\right)$$\nThis result can also be obtained by recognizing that $\\mathbb{E}[\\exp(X)]$ is the moment-generating function (MGF) of $X$, $M_X(t) = \\mathbb{E}[\\exp(tX)]$, evaluated at $t=1$. For $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the MGF is $M_X(t) = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$. Evaluating at $t=1$ yields the same result.\n\nNext, we address the bias of the naive estimator $\\exp(\\hat{\\mu})$. The estimator for $\\mu$ is the sample mean of the log-transformed data, $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. Since each $X_i$ is an independent random variable from $\\mathcal{N}(\\mu, \\sigma^2)$, $\\hat{\\mu}$ is also normally distributed. Its mean is $\\mathbb{E}[\\hat{\\mu}] = \\mathbb{E}[\\frac{1}{n}\\sum X_i] = \\frac{1}{n}\\sum \\mathbb{E}[X_i] = \\frac{1}{n}(n\\mu) = \\mu$. Its variance is $\\mathrm{Var}(\\hat{\\mu}) = \\mathrm{Var}(\\frac{1}{n}\\sum X_i) = \\frac{1}{n^2}\\sum \\mathrm{Var}(X_i) = \\frac{1}{n^2}(n\\sigma^2) = \\frac{\\sigma^2}{n}$.\nThus, $\\hat{\\mu} \\sim \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n})$.\n\nTo evaluate the bias of the estimator $\\exp(\\hat{\\mu})$, we calculate its expectation, $\\mathbb{E}[\\exp(\\hat{\\mu})]$. This is the MGF of $\\hat{\\mu}$ evaluated at $t=1$. Using the MGF formula for a normal variable with mean $\\mu$ and variance $\\sigma^2/n$:\n$$\\mathbb{E}[\\exp(\\hat{\\mu})] = \\exp\\left(\\mu \\cdot 1 + \\frac{1}{2}\\left(\\frac{\\sigma^2}{n}\\right) \\cdot 1^2\\right) = \\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right)$$\nThe bias of the estimator is the difference between its expectation and the true value of the quantity being estimated:\n$$B(\\exp(\\hat{\\mu})) = \\mathbb{E}[\\exp(\\hat{\\mu})] - \\mathbb{E}[Y] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right) - \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$$\nFor any finite sample size $n > 1$ and non-zero biological variability $\\sigma^2 > 0$, we have $\\frac{\\sigma^2}{2n}  \\frac{\\sigma^2}{2}$. Since the exponential function is strictly increasing, $\\exp(\\mu + \\frac{\\sigma^2}{2n})  \\exp(\\mu + \\frac{\\sigma^2}{2})$. Therefore, the bias is non-zero (specifically, it is negative), and the estimator $\\exp(\\hat{\\mu})$ is biased for $\\mathbb{E}[Y]$. It systematically underestimates the true mean concentration. This bias arises from applying the non-linear exponential transformation to an estimator of the mean on the log scale, while ignoring the contribution of the variance (a consequence of Jensen's inequality). The \"naive\" estimator effectively targets $\\exp(\\mu)$ rather than the correct quantity $\\exp(\\mu + \\frac{1}{2}\\sigma^2)$.\n\nFinally, we propose a model-based plug-in estimator that corrects for this transformation-induced bias. The derivation of $\\mathbb{E}[Y]$ shows that a correct estimation of the mean requires accounting for both $\\mu$ and $\\sigma^2$. The \"correction\" for the bias of the naive estimator involves incorporating the variance term $\\frac{1}{2}\\sigma^2$ into the expression. The corrected target expression is $\\exp(\\mu + \\frac{1}{2}\\sigma^2)$.\nA \"plug-in\" estimator is formed by substituting parameter estimators for the true parameters in the expression for the quantity of interest. The problem provides the maximum likelihood estimators (MLEs) for $\\mu$ and $\\sigma^2$:\n$$\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$$\n$$\\hat{\\sigma}^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i} - \\hat{\\mu})^{2}$$\nSubstituting these estimators into the expression for $\\mathbb{E}[Y]$ gives the model-based plug-in estimator, which we denote as $\\widehat{\\mathbb{E}[Y]}$:\n$$\\widehat{\\mathbb{E}[Y]} = \\exp\\left(\\hat{\\mu} + \\frac{1}{2}\\hat{\\sigma}^2\\right)$$\nThis estimator explicitly uses the model structure ($Y$ is log-normal) and corrects the primary deficiency of the naive estimator by including an estimate of the variance term. The expression is already in its simplest closed form in terms of the specified estimators $\\hat{\\mu}$ and $\\hat{\\sigma}^2$.",
            "answer": "$$\\boxed{\\exp\\left(\\hat{\\mu} + \\frac{1}{2}\\hat{\\sigma}^{2}\\right)}$$"
        },
        {
            "introduction": "While exact finite-sample moments are ideal, much of statistical inference relies on large-sample approximations to quantify the uncertainty of estimators. This exercise introduces the Delta Method, a cornerstone of asymptotic theory that uses calculus to approximate the variance of a function of an estimator. You will apply this powerful technique to derive the asymptotic variance of the sample logit, a key quantity in logistic regression , thereby learning a generalizable skill for variance estimation in complex modeling scenarios.",
            "id": "4962586",
            "problem": "In a multicenter cardiovascular safety surveillance study, suppose each enrolled patient independently experiences a binary outcome indicating the occurrence of a clinically relevant arrhythmia during the monitoring period, with true probability $p$ where $0p1$. Let $n$ denote the total number of patients observed, and let $Y$ be the count of patients with the arrhythmia. The empirical risk is $\\hat{p}=Y/n$, and for risk communication on the logit scale, clinicians report $\\hat{\\beta}=\\ln\\!\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right)$.\n\nStarting from the definition of the Binomial model for $Y$, the large-sample behavior of the sample proportion $\\hat{p}$ justified by the Central Limit Theorem (CLT), and the delta method, derive the asymptotic variance of $\\hat{\\beta}$ as $n \\to \\infty$. Express your final answer as a single closed-form analytical expression in terms of $n$ and $p$, and explicitly relate it to $p(1-p)$. No numerical approximation or rounding is required, and no units should be included. The final answer must be a single expression.",
            "solution": "The problem asks for the asymptotic variance of the logit-transformed empirical risk, $\\hat{\\beta} = \\ln\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right)$, where $\\hat{p} = Y/n$ is the sample proportion of events from $n$ independent Bernoulli trials. The derivation will proceed by first establishing the statistical properties of the count variable $Y$ and the sample proportion $\\hat{p}$, then applying the Central Limit Theorem (CLT) and finally using the delta method to find the asymptotic variance of the transformed statistic $\\hat{\\beta}$.\n\n**Step 1: The Binomial Model and Properties of $\\hat{p}$**\n\nThe number of patients experiencing an arrhythmia, $Y$, is the sum of $n$ independent and identically distributed Bernoulli trials, each with a success probability (probability of arrhythmia) of $p$. Therefore, $Y$ follows a Binomial distribution:\n$$Y \\sim \\text{Binomial}(n, p)$$\nThe expected value and variance of $Y$ are given by:\n$$\\mathbb{E}[Y] = np$$\n$$\\mathrm{Var}(Y) = np(1-p)$$\nThe empirical risk, $\\hat{p}$, is defined as the sample proportion $\\hat{p} = Y/n$. We can find its exact mean and variance using the properties of expectation and variance:\n$$\\mathbb{E}[\\hat{p}] = \\mathbb{E}\\left[\\frac{Y}{n}\\right] = \\frac{1}{n}\\mathbb{E}[Y] = \\frac{np}{n} = p$$\nThis shows that $\\hat{p}$ is an unbiased estimator of $p$.\n$$\\mathrm{Var}(\\hat{p}) = \\mathrm{Var}\\left(\\frac{Y}{n}\\right) = \\frac{1}{n^2}\\mathrm{Var}(Y) = \\frac{np(1-p)}{n^2} = \\frac{p(1-p)}{n}$$\n\n**Step 2: Large-Sample Behavior of $\\hat{p}$ via the Central Limit Theorem**\n\nThe Central Limit Theorem (CLT) states that for a large sample size $n$, the distribution of the sample mean (or proportion) $\\hat{p}$ is approximately normal. More formally, the scaled and centered statistic converges in distribution to a standard normal random variable:\n$$\\frac{\\hat{p} - \\mathbb{E}[\\hat{p}]}{\\sqrt{\\mathrm{Var}(\\hat{p})}} = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\xrightarrow{d} \\mathcal{N}(0, 1) \\quad \\text{as } n \\to \\infty$$\nMultiplying the numerator and denominator by $\\sqrt{n}$, we can express this in the standard form used for the delta method:\n$$\\sqrt{n}(\\hat{p} - p) \\xrightarrow{d} \\mathcal{N}(0, p(1-p))$$\nThis result establishes the large-sample behavior of $\\hat{p}$, showing it is asymptotically normal with a variance parameter of $p(1-p)$.\n\n**Step 3: Application of the Delta Method**\n\nThe delta method is used to determine the asymptotic distribution of a function of an asymptotically normal random variable. Let $g(\\theta)$ be a continuously differentiable function such that $g'(\\theta) \\neq 0$. If an estimator $\\hat{\\theta}_n$ satisfies\n$$\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)$$\nthen the distribution of the transformed estimator $g(\\hat{\\theta}_n)$ is given by\n$$\\sqrt{n}(g(\\hat{\\theta}_n) - g(\\theta)) \\xrightarrow{d} \\mathcal{N}(0, [g'(\\theta)]^2 \\sigma^2)$$\nIn our problem, the estimator is $\\hat{\\theta}_n = \\hat{p}$, the true parameter is $\\theta = p$, and from the CLT, the asymptotic variance parameter is $\\sigma^2 = p(1-p)$. The transformation is the logit function, $g(p) = \\ln\\left(\\frac{p}{1-p}\\right)$. The statistic of interest is $\\hat{\\beta} = g(\\hat{p})$.\n\nFirst, we must compute the derivative of $g(p)$. It is convenient to first rewrite $g(p)$ using properties of logarithms:\n$$g(p) = \\ln(p) - \\ln(1-p)$$\nThe derivative with respect to $p$ is:\n$$g'(p) = \\frac{d}{dp}[\\ln(p) - \\ln(1-p)] = \\frac{1}{p} - \\frac{1}{1-p} \\cdot (-1) = \\frac{1}{p} + \\frac{1}{1-p}$$\nCombining these terms over a common denominator gives:\n$$g'(p) = \\frac{(1-p) + p}{p(1-p)} = \\frac{1}{p(1-p)}$$\nThe condition $0  p  1$ ensures that this derivative is well-defined and non-zero.\n\nNow, we can apply the delta method formula. The variance of the asymptotic distribution for $\\sqrt{n}(g(\\hat{p})-g(p))$ is $[g'(p)]^2 \\sigma^2$:\n$$[g'(p)]^2 \\sigma^2 = \\left(\\frac{1}{p(1-p)}\\right)^2 \\cdot [p(1-p)] = \\frac{1}{[p(1-p)]^2} \\cdot p(1-p) = \\frac{1}{p(1-p)}$$\nTherefore, the asymptotic distribution of $\\hat{\\beta}$ is:\n$$\\sqrt{n}(\\hat{\\beta} - \\beta) \\xrightarrow{d} \\mathcal{N}\\left(0, \\frac{1}{p(1-p)}\\right)$$\nwhere $\\beta = g(p)$ is the true logit-transformed risk.\n\n**Step 4: Deriving the Asymptotic Variance of $\\hat{\\beta}$**\n\nThe result from the delta method implies that for large $n$, the sampling distribution of $\\hat{\\beta}$ can be approximated by a normal distribution:\n$$\\hat{\\beta} \\approx \\mathcal{N}\\left(\\beta, \\frac{1}{n}\\left(\\frac{1}{p(1-p)}\\right)\\right)$$\nThe term \"asymptotic variance of $\\hat{\\beta}$,\" denoted $\\mathrm{AsyVar}(\\hat{\\beta})$, refers to the variance of this approximating normal distribution. Thus, we have:\n$$\\mathrm{AsyVar}(\\hat{\\beta}) = \\frac{1}{np(1-p)}$$\nThis expression is the required closed-form analytical result. It is inversely proportional to both the sample size, $n$, and the variance of a single Bernoulli trial, $p(1-p)$. The variance of the estimator is largest when $p$ is close to $0$ or $1$, where the logit function is steepest, and smallest when $p=0.5$.",
            "answer": "$$\n\\boxed{\\frac{1}{np(1-p)}}\n$$"
        }
    ]
}