{
    "hands_on_practices": [
        {
            "introduction": "A robust machine learning pipeline begins with meticulous data preprocessing. A common but critical error is 'data leakage,' where information from the test set inadvertently influences the training process, leading to optimistically biased performance estimates. This exercise  provides a hands-on opportunity to quantify the mathematical discrepancy introduced by performing feature normalization before, rather than after, data splitting, solidifying your understanding of why a strict separation of training and testing data is paramount.",
            "id": "4389509",
            "problem": "A systems biomedicine study uses a single messenger ribonucleic acid (mRNA) expression biomarker $X$ to classify a binary phenotype (e.g., disease versus control) with supervised learning. The dataset contains $n$ independent and identically distributed samples drawn from a balanced population where the unconditional distribution of $X$ has mean $0$ and variance $v$. You perform $K$-fold cross-validation (CV) with $K \\geq 2$, producing $K$ disjoint test folds of size $n_{\\mathrm{te}} = n/K$ and training folds of size $n_{\\mathrm{tr}} = n - n_{\\mathrm{te}} = n(K-1)/K$.\n\nConsider the feature normalization map $\\phi(x) = (x - \\hat{\\mu})/\\sigma$, where $\\hat{\\mu}$ is an empirical mean and $\\sigma = \\sqrt{v}$ is a known scaling constant derived from prior technical calibration (assume $\\sigma$ is known and fixed, and equal to the true standard deviation of $X$). Two pipelines are compared:\n\n- Leaky pipeline: compute $\\hat{\\mu}_{\\mathrm{full}}$ from all $n$ samples and apply $\\phi_{\\mathrm{full}}(x) = (x - \\hat{\\mu}_{\\mathrm{full}})/\\sigma$ to all samples before splitting into folds.\n- Correct pipeline: for each fold $f$, compute $\\hat{\\mu}_{f,\\mathrm{tr}}$ from only the training samples in that fold and define $\\phi_{f}(x) = (x - \\hat{\\mu}_{f,\\mathrm{tr}})/\\sigma$. Apply $\\phi_{f}$ to both training and test samples in fold $f$.\n\nUsing the core definition that cross-validation risk estimates require independence between training and test data, and the well-established facts about sample means of independent and identically distributed random variables, derive the expected squared discrepancy between the leaky standardized test value and the correctly standardized test value for a randomly selected test sample $X_{j}$ within an arbitrary fold $f$, namely\n$$\n\\mathbb{E}\\left[\\left(\\phi_{\\mathrm{full}}(X_{j}) - \\phi_{f}(X_{j})\\right)^{2}\\right],\n$$\nwhere the expectation is taken over the data-generating process and the random partition into folds. Express your final answer as a single closed-form analytical expression in terms of $n$ and $K$.\n\nAdditionally, in your solution, briefly articulate the corrected pipeline in which $\\phi$ is fit only on training folds and then applied to the corresponding test fold, explaining why this prevents leakage at the normalization step.\n\nNo rounding is required. Provide your final answer as a single analytical expression.",
            "solution": "The problem requires the derivation of the expected squared discrepancy between a \"leaky\" and a \"correct\" feature normalization scheme within a $K$-fold cross-validation framework. Let us begin by formally defining the quantities involved and then proceeding with the derivation.\n\nThe dataset consists of $n$ independent and identically distributed (i.i.d.) samples $\\{X_1, X_2, \\dots, X_n\\}$ of a biomarker $X$. The underlying distribution of $X$ has mean $\\mathbb{E}[X_i] = 0$ and variance $\\mathrm{Var}[X_i] = v$ for all $i \\in \\{1, \\dots, n\\}$. The normalization constant $\\sigma = \\sqrt{v}$ is known.\n\nThe quantity to be computed is:\n$$\n\\mathbb{E}\\left[\\left(\\phi_{\\mathrm{full}}(X_{j}) - \\phi_{f}(X_{j})\\right)^{2}\\right]\n$$\nwhere $X_j$ is a sample in the test set of an arbitrary fold $f$.\n\nFirst, we substitute the definitions of the normalization maps:\n$$\n\\phi_{\\mathrm{full}}(X_{j}) = \\frac{X_{j} - \\hat{\\mu}_{\\mathrm{full}}}{\\sigma}\n$$\n$$\n\\phi_{f}(X_{j}) = \\frac{X_{j} - \\hat{\\mu}_{f,\\mathrm{tr}}}{\\sigma}\n$$\nThe difference between these two standardized values is:\n$$\n\\phi_{\\mathrm{full}}(X_{j}) - \\phi_{f}(X_{j}) = \\frac{1}{\\sigma} \\left( (X_{j} - \\hat{\\mu}_{\\mathrm{full}}) - (X_{j} - \\hat{\\mu}_{f,\\mathrm{tr}}) \\right) = \\frac{\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}}}{\\sigma}\n$$\nThe expected squared discrepancy can thus be written as:\n$$\n\\mathbb{E}\\left[\\left(\\frac{\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}}}{\\sigma}\\right)^{2}\\right] = \\frac{1}{\\sigma^2} \\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}})^2\\right]\n$$\nGiven that $\\sigma^2 = v$, this becomes:\n$$\n\\frac{1}{v} \\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}})^2\\right]\n$$\nOur main task is to compute the expectation $\\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}})^2\\right]$.\n\nLet $I_{\\mathrm{tr}}$ be the set of indices of the training samples for fold $f$, and $I_{\\mathrm{te}}$ be the set of indices for the test samples of fold $f$. We have $|I_{\\mathrm{tr}}| = n_{\\mathrm{tr}} = n(K-1)/K$ and $|I_{\\mathrm{te}}| = n_{\\mathrm{te}} = n/K$. The full set of indices is $I = I_{\\mathrm{tr}} \\cup I_{\\mathrm{te}}$.\n\nThe empirical means are defined as:\n- The mean over all $n$ samples: $\\hat{\\mu}_{\\mathrm{full}} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$.\n- The mean over the training samples of fold $f$: $\\hat{\\mu}_{f,\\mathrm{tr}} = \\frac{1}{n_{\\mathrm{tr}}} \\sum_{i \\in I_{\\mathrm{tr}}} X_i$.\n- Let us also define the mean over the test samples of fold $f$: $\\hat{\\mu}_{f,\\mathrm{te}} = \\frac{1}{n_{\\mathrm{te}}} \\sum_{i \\in I_{\\mathrm{te}}} X_i$.\n\nWe can express $\\hat{\\mu}_{\\mathrm{full}}$ as a weighted average of the training and test means for fold $f$:\n$$\n\\hat{\\mu}_{\\mathrm{full}} = \\frac{1}{n} \\left( \\sum_{i \\in I_{\\mathrm{tr}}} X_i + \\sum_{i \\in I_{\\mathrm{te}}} X_i \\right) = \\frac{1}{n} (n_{\\mathrm{tr}} \\hat{\\mu}_{f,\\mathrm{tr}} + n_{\\mathrm{te}} \\hat{\\mu}_{f,\\mathrm{te}}) = \\frac{n_{\\mathrm{tr}}}{n} \\hat{\\mu}_{f,\\mathrm{tr}} + \\frac{n_{\\mathrm{te}}}{n} \\hat{\\mu}_{f,\\mathrm{te}}\n$$\nNow, we can express the difference $(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}})$:\n$$\n\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}} = \\hat{\\mu}_{f,\\mathrm{tr}} - \\left(\\frac{n_{\\mathrm{tr}}}{n} \\hat{\\mu}_{f,\\mathrm{tr}} + \\frac{n_{\\mathrm{te}}}{n} \\hat{\\mu}_{f,\\mathrm{te}}\\right) = \\left(1 - \\frac{n_{\\mathrm{tr}}}{n}\\right) \\hat{\\mu}_{f,\\mathrm{tr}} - \\frac{n_{\\mathrm{te}}}{n} \\hat{\\mu}_{f,\\mathrm{te}}\n$$\nSince $n = n_{\\mathrm{tr}} + n_{\\mathrm{te}}$, we have $1 - n_{\\mathrm{tr}}/n = n_{\\mathrm{te}}/n$. Also, $n_{\\mathrm{te}}/n = (n/K)/n = 1/K$. Therefore:\n$$\n\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}} = \\frac{n_{\\mathrm{te}}}{n}(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{f,\\mathrm{te}}) = \\frac{1}{K}(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{f,\\mathrm{te}})\n$$\nNow we compute the expectation of the square of this difference:\n$$\n\\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}})^2\\right] = \\mathbb{E}\\left[\\left(\\frac{1}{K}(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{f,\\mathrm{te}})\\right)^2\\right] = \\frac{1}{K^2} \\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{f,\\mathrm{te}})^2\\right]\n$$\nLet's expand the term inside the expectation:\n$$\n\\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{f,\\mathrm{te}})^2\\right] = \\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{tr}}^2 - 2\\hat{\\mu}_{f,\\mathrm{tr}}\\hat{\\mu}_{f,\\mathrm{te}} + \\hat{\\mu}_{f,\\mathrm{te}}^2] = \\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{tr}}^2] - 2\\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{tr}}\\hat{\\mu}_{f,\\mathrm{te}}] + \\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{te}}^2]\n$$\nThe sets of samples used to compute $\\hat{\\mu}_{f,\\mathrm{tr}}$ and $\\hat{\\mu}_{f,\\mathrm{te}}$ are disjoint. Since the original samples $X_i$ are i.i.d., the random variables $\\hat{\\mu}_{f,\\mathrm{tr}}$ and $\\hat{\\mu}_{f,\\mathrm{te}}$ are independent.\nThe expectation of any sample mean is $\\mathbb{E}[\\hat{\\mu}] = \\mathbb{E}[\\frac{1}{m}\\sum X_i] = \\frac{1}{m}\\sum\\mathbb{E}[X_i] = 0$. Thus, $\\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{tr}}] = 0$ and $\\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{te}}] = 0$.\nDue to independence, the expectation of the cross-product is $\\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{tr}}\\hat{\\mu}_{f,\\mathrm{te}}] = \\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{tr}}]\\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{te}}] = 0 \\cdot 0 = 0$.\nThe expectation simplifies to:\n$$\n\\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{f,\\mathrm{te}})^2\\right] = \\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{tr}}^2] + \\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{te}}^2]\n$$\nFor any random variable $Y$ with $\\mathbb{E}[Y]=0$, we have $\\mathrm{Var}[Y] = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\mathbb{E}[Y^2]$. The variance of a sample mean of $m$ i.i.d. random variables with variance $v$ is $v/m$.\nTherefore:\n$$\n\\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{tr}}^2] = \\mathrm{Var}[\\hat{\\mu}_{f,\\mathrm{tr}}] = \\frac{v}{n_{\\mathrm{tr}}}\n$$\n$$\n\\mathbb{E}[\\hat{\\mu}_{f,\\mathrm{te}}^2] = \\mathrm{Var}[\\hat{\\mu}_{f,\\mathrm{te}}] = \\frac{v}{n_{\\mathrm{te}}}\n$$\nSubstituting these back:\n$$\n\\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{f,\\mathrm{te}})^2\\right] = \\frac{v}{n_{\\mathrm{tr}}} + \\frac{v}{n_{\\mathrm{te}}} = v \\left(\\frac{1}{n_{\\mathrm{tr}}} + \\frac{1}{n_{\\mathrm{te}}}\\right)\n$$\nNow, we substitute $n_{\\mathrm{tr}} = n(K-1)/K$ and $n_{\\mathrm{te}} = n/K$:\n$$\n\\frac{1}{n_{\\mathrm{tr}}} + \\frac{1}{n_{\\mathrm{te}}} = \\frac{K}{n(K-1)} + \\frac{K}{n} = \\frac{K}{n} \\left(\\frac{1}{K-1} + 1\\right) = \\frac{K}{n} \\left(\\frac{1 + K - 1}{K-1}\\right) = \\frac{K^2}{n(K-1)}\n$$\nSo, we have:\n$$\n\\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{te}})^2\\right] = v \\frac{K^2}{n(K-1)}\n$$\nWe can now calculate $\\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}})^2\\right]$:\n$$\n\\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}})^2\\right] = \\frac{1}{K^2} \\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{f,\\mathrm{te}})^2\\right] = \\frac{1}{K^2} \\left( v \\frac{K^2}{n(K-1)} \\right) = \\frac{v}{n(K-1)}\n$$\nFinally, we substitute this result into the expression for the desired expected squared discrepancy:\n$$\n\\mathbb{E}\\left[\\left(\\phi_{\\mathrm{full}}(X_{j}) - \\phi_{f}(X_{j})\\right)^{2}\\right] = \\frac{1}{v} \\mathbb{E}\\left[(\\hat{\\mu}_{f,\\mathrm{tr}} - \\hat{\\mu}_{\\mathrm{full}})^2\\right] = \\frac{1}{v} \\left(\\frac{v}{n(K-1)}\\right) = \\frac{1}{n(K-1)}\n$$\n\nAs stipulated, we briefly articulate the corrected pipeline and the principle it upholds. In the correct pipeline, for each of the $K$ iterations of cross-validation, the dataset is partitioned into a training set and a test set. All data-driven preprocessing steps, including the calculation of the normalization parameters (here, the empirical mean $\\hat{\\mu}_{f,\\mathrm{tr}}$), are performed using only the training data for that specific fold. The resulting transformation, $\\phi_f$, is then applied to both the training and the test sets of that fold. This procedure prevents data leakage by strictly maintaining the independence of the test set. The test set remains \"unseen\" during all phases of model fitting, including preprocessing, thereby yielding an unbiased estimate of the model's performance on new data. The leaky pipeline violates this principle by computing a global mean $\\hat{\\mu}_{\\mathrm{full}}$ using data from all folds, which contaminates the training process with information from the test set.",
            "answer": "$$\n\\boxed{\\frac{1}{n(K-1)}}\n$$"
        },
        {
            "introduction": "Biological datasets are often 'clustered,' for instance, when multiple biopsies are collected from a single patient. Treating these non-independent samples as independent during cross-validation is a severe form of data leakage that invalidates performance estimates. This practice problem  challenges you to design a correct data-splitting strategy and analyze how the inherent data correlation impacts the statistical uncertainty of your results, a crucial skill for working with real-world clinical data.",
            "id": "4389578",
            "problem": "A translational oncology study assembles a cohort of $N_p$ patients, each contributing $m_i \\geq 1$ multi-region tumor biopsies. For patient $i$, the $j$-th biopsy has a feature vector $\\mathbf{x}_{ij} \\in \\mathbb{R}^d$ measured by bulk ribonucleic acid sequencing (RNA-seq) and a binary patient-level label $y_i \\in \\{0,1\\}$ indicating immune checkpoint therapy response assessed by clinical criteria. The label $y_i$ is constant across biopsies from the same patient. The goal is to train a supervised classifier $f: \\mathbb{R}^d \\to \\{0,1\\}$ to predict $y_i$ from biopsy-level features $\\mathbf{x}_{ij}$ and to estimate the generalization performance with reliable uncertainty. Investigators suspect intra-patient dependence: for any fixed patient $i$, biopsy-level prediction errors for $f$ are positively correlated across biopsies from that patient due to shared microenvironment and processing effects.\n\nLet the cohort have $N_p = 180$ patients and $N = \\sum_{i=1}^{N_p} m_i = 540$ biopsies, with an empirical mean cluster size $\\bar{m} = \\frac{1}{N_p} \\sum_{i=1}^{N_p} m_i = 3$. Suppose an exploratory analysis of biopsy-level residuals yields an estimated intra-class correlation coefficient $\\hat{\\rho} = 0.4$ for within-patient error indicators. You will construct a data-splitting strategy that prevents patient-level leakage when multiple biopsies per patient exist and analyze its implications for the variance of performance estimates relative to a naive sample-level approach.\n\nWhich option best satisfies both of the following requirements: it prevents patient-level leakage in cross-validation, and it provides a correct qualitative and quantitative implication for the variance of performance estimates in the presence of intra-patient correlation and average cluster size as summarized above?\n\nA. Use stratified $K$-fold cross-validation at the biopsy level, randomly assigning individual biopsies to folds while preserving the overall response prevalence per fold. Estimate uncertainty by nonparametric bootstrap resampling of biopsies. This avoids leakage because stratification controls label distributions, and the variance of the performance estimate is unchanged, since more biopsies increase precision.\n\nB. Use stratified Group $K$-fold cross-validation at the patient level, assigning all biopsies from a patient to the same fold and preserving patient-level response prevalence across folds. Evaluate performance at the patient level by aggregating biopsy-level predictions per patient (for example, majority vote), and quantify uncertainty by cluster (patient-level) bootstrap that resamples patients with all their biopsies. Relative to a naive biopsy-level independent-and-identically-distributed analysis, the variance of a biopsy-level average performance estimate inflates approximately by a design effect factor that scales like $1 + (\\bar{m} - 1)\\rho$, so preventing leakage and accounting for clustering yields wider and more honest confidence intervals than sample-level splitting.\n\nC. For each patient, perform a holdout where one biopsy is placed in the test fold and the remaining biopsies from the same patient remain in the training fold, cycling through all biopsies. Compute uncertainty using the standard independent-and-identically-distributed variance formula with $N$ in the denominator. This prevents leakage because each biopsy is held out at least once, and the variance decreases because the training sets are large.\n\nD. Use Leave-One-Patient-Out cross-validation, ensuring that each fold’s test set contains exactly one patient and the training set contains the remaining patients. Compute the variance of the sample-level accuracy by treating biopsies as independent Bernoulli trials, using $\\widehat{\\mathrm{Var}}(\\widehat{\\mathrm{acc}}) \\approx \\widehat{\\mathrm{acc}}(1 - \\widehat{\\mathrm{acc}})/N$. This prevents leakage because patients are left out, and variance is correctly estimated using the total number of biopsies, which captures the precision contributed by multiple biopsies per patient.",
            "solution": "### Step 1: Problem Validation\n\nI will begin by validating the problem statement as per the required protocol.\n\n#### Extraction of Givens\n- A cohort consists of $N_p$ patients.\n- Each patient $i$ contributes $m_i \\geq 1$ multi-region tumor biopsies.\n- The feature vector for the $j$-th biopsy from patient $i$ is $\\mathbf{x}_{ij} \\in \\mathbb{R}^d$.\n- The patient-level label for patient $i$ is $y_i \\in \\{0,1\\}$, indicating immune checkpoint therapy response.\n- The label $y_i$ is constant for all biopsies from the same patient $i$.\n- The goal is to train a classifier $f: \\mathbb{R}^d \\to \\{0,1\\}$.\n- A secondary goal is to estimate the generalization performance with reliable uncertainty.\n- There is suspected intra-patient dependence: prediction errors for biopsies from the same patient are positively correlated.\n- Number of patients: $N_p = 180$.\n- Total number of biopsies: $N = \\sum_{i=1}^{N_p} m_i = 540$.\n- Empirical mean cluster size: $\\bar{m} = \\frac{1}{N_p} \\sum_{i=1}^{N_p} m_i = 3$.\n- Estimated intra-class correlation coefficient for within-patient error indicators: $\\hat{\\rho} = 0.4$.\n- The core task is to identify a data-splitting strategy that (1) prevents patient-level leakage and (2) correctly analyzes the implications for the variance of performance estimates.\n\n#### Validation using Extracted Givens\n1.  **Scientifically Grounded**: The problem is set in a realistic translational oncology context. Using RNA-seq data from biopsies to predict therapy response is a standard application of supervised learning in systems biomedicine. The central statistical challenge—the presence of correlated data due to multiple samples from the same subject (patient)—is a fundamental and critical issue in biostatistics and medical machine learning. The term \"patient-level leakage\" refers to a well-documented pitfall in model validation that leads to overly optimistic performance estimates. The numerical values provided ($N_p=180$, $N=540$, $\\bar{m}=3$, $\\rho=0.4$) are plausible for such a study. The problem is firmly rooted in established statistical and machine learning principles.\n\n2.  **Well-Posed**: The question asks to evaluate several options against two clear and distinct criteria: (1) prevention of data leakage and (2) correct qualitative and quantitative assessment of the variance of performance estimators. The problem provides all necessary information to judge each option against these criteria. A unique, best answer among the choices can be determined through rigorous application of statistical principles.\n\n3.  **Objective**: The problem is stated using precise, objective, and standard technical terminology from statistics, machine learning, and oncology. There are no subjective claims, ambiguities, or opinion-based statements.\n\n_Verdict_: The problem statement is valid. It is scientifically sound, well-posed, objective, and presents a non-trivial but standard challenge in the field of biomedical data science. I will proceed with the solution.\n\n### Step 2: Solution Derivation\n\nThe problem requires a solution that addresses two distinct but related issues: data splitting strategy and the statistical properties of performance estimates.\n\n#### 1. Preventing Patient-Level Leakage\n\nThe dataset has a hierarchical or clustered structure: biopsies are nested within patients. The patient-level label $y_i$ is shared across all biopsies $\\mathbf{x}_{i1}, \\mathbf{x}_{i2}, \\dots, \\mathbf{x}_{im_i}$ from patient $i$. Furthermore, the problem states that there is intra-patient correlation in prediction errors, a consequence of shared genetic background, tumor microenvironment, and sample processing effects.\n\nIf a cross-validation scheme splits data at the biopsy level, it is highly likely that biopsies from the same patient will be distributed across both the training and test sets. A classifier trained on a biopsy from patient $i$ will learn patient-specific features. When it is then tested on another biopsy from the same patient $i$, it will achieve an artificially high prediction accuracy. This phenomenon is known as data leakage or information leakage. The resulting performance estimate will be optimistically biased and will not reflect the model's ability to generalize to entirely new, unseen patients.\n\nTo prevent this leakage, the unit of randomization for cross-validation must be the patient, not the biopsy. All data associated with a single patient must be exclusively assigned to either the training fold or the test fold, but never split between them. This methodology is known as Group $K$-fold cross-validation, where the \"groups\" are the patients. Specialized versions include Leave-One-Group-Out (here, Leave-One-Patient-Out) cross-validation.\n\n#### 2. Variance of Performance Estimates with Correlated Data\n\nThe second requirement is to correctly account for the impact of data clustering on the variance of performance estimates. The biopsies are not independent and identically distributed (i.i.d.). The intra-class correlation coefficient, $\\rho$, quantifies the average correlation between observations within the same cluster (patient). A positive $\\rho$ signifies that observations within a cluster are more similar to each other than to observations from different clusters.\n\nWhen data are clustered, the effective sample size is smaller than the total number of individual observations, $N$. A naive variance calculation that assumes i.i.d. data (e.g., using a denominator of $N$) will underestimate the true variance. The correct variance of a sample mean (or a proportion like accuracy) from clustered data is inflated by a factor known as the \"design effect\" (DEFF). For clusters of varying sizes, the design effect is approximated as:\n$$\n\\text{DEFF} \\approx 1 + (\\bar{m} - 1)\\rho\n$$\nwhere $\\bar{m}$ is the average cluster size and $\\rho$ is the intra-class correlation.\n\nUsing the provided values:\n- Average cluster size: $\\bar{m} = 3$\n- Intra-class correlation coefficient: $\\rho = 0.4$\n\nThe approximate design effect is:\n$$\n\\text{DEFF} \\approx 1 + (3 - 1) \\times 0.4 = 1 + 2 \\times 0.4 = 1 + 0.8 = 1.8\n$$\nThis means that the variance of a biopsy-level performance metric (e.g., average accuracy) is approximately $1.8$ times larger than what would be calculated under a false i.i.d. assumption. Consequently, confidence intervals must be wider by a factor of $\\sqrt{\\text{DEFF}} \\approx \\sqrt{1.8} \\approx 1.34$ to be statistically valid (\"honest\"). Any analysis that ignores this effect is flawed.\n\n### Step 3: Option-by-Option Analysis\n\nNow I will evaluate each option based on these principles.\n\n**A. Use stratified $K$-fold cross-validation at the biopsy level, randomly assigning individual biopsies to folds while preserving the overall response prevalence per fold. Estimate uncertainty by nonparametric bootstrap resampling of biopsies. This avoids leakage because stratification controls label distributions, and the variance of the performance estimate is unchanged, since more biopsies increase precision.**\n\n-   **Leakage Prevention**: This method fails. Splitting at the biopsy level is the canonical example of what causes patient-level leakage. Biopsies from the same patient can be in both the training and test sets. Stratification on the label $y_i$ does not prevent this.\n-   **Variance Implication**: The claim that \"the variance of the performance estimate is unchanged\" is false. The presence of positive intra-patient correlation ($\\rho = 0.4 > 0$) strictly increases the variance of sample averages compared to the i.i.d. case. The proposed bootstrap resampling of individual biopsies also fails to respect the cluster structure and would yield invalid, overly narrow uncertainty estimates.\n-   **Verdict**: **Incorrect**.\n\n**B. Use stratified Group $K$-fold cross-validation at the patient level, assigning all biopsies from a patient to the same fold and preserving patient-level response prevalence across folds. Evaluate performance at the patient level by aggregating biopsy-level predictions per patient (for example, majority vote), and quantify uncertainty by cluster (patient-level) bootstrap that resamples patients with all their biopsies. Relative to a naive biopsy-level independent-and-identically-distributed analysis, the variance of a biopsy-level average performance estimate inflates approximately by a design effect factor that scales like $1 + (\\bar{m} - 1)\\rho$, so preventing leakage and accounting for clustering yields wider and more honest confidence intervals than sample-level splitting.**\n\n-   **Leakage Prevention**: This method is correct. Using Group $K$-fold cross-validation at the patient level ensures that all biopsies from a given patient are contained within a single fold, thereby preventing leakage. Stratifying by the patient-level label is also a sound practice.\n-   **Variance Implication**: This option correctly identifies the quantitative effect of clustering on variance. It correctly states that the variance inflates by a design effect factor that scales as $1 + (\\bar{m} - 1)\\rho$. It also correctly concludes that this leads to wider, more \"honest\" confidence intervals. The suggested methods for evaluation (patient-level aggregation) and uncertainty quantification (cluster bootstrap) are state-of-the-art for this type of data structure.\n-   **Verdict**: **Correct**.\n\n**C. For each patient, perform a holdout where one biopsy is placed in the test fold and the remaining biopsies from the same patient remain in the training fold, cycling through all biopsies. Compute uncertainty using the standard independent-and-identically-distributed variance formula with $N$ in the denominator. This prevents leakage because each biopsy is held out at least once, and the variance decreases because the training sets are large.**\n\n-   **Leakage Prevention**: This method is catastrophically wrong. It explicitly proposes training a model on biopsies from a patient and then testing it on another biopsy from the *same* patient. This is the most direct form of data leakage imaginable and will lead to grossly overestimated performance. The claim that it \"prevents leakage\" is false.\n-   **Variance Implication**: The proposed variance calculation is the naive i.i.d. formula, which is incorrect for correlated data. The entire premise of this option is flawed.\n-   **Verdict**: **Incorrect**.\n\n**D. Use Leave-One-Patient-Out cross-validation, ensuring that each fold’s test set contains exactly one patient and the training set contains the remaining patients. Compute the variance of the sample-level accuracy by treating biopsies as independent Bernoulli trials, using $\\widehat{\\mathrm{Var}}(\\widehat{\\mathrm{acc}}) \\approx \\widehat{\\mathrm{acc}}(1 - \\widehat{\\mathrm{acc}})/N$. This prevents leakage because patients are left out, and variance is correctly estimated using the total number of biopsies, which captures the precision contributed by multiple biopsies per patient.**\n\n-   **Leakage Prevention**: This method is correct. Leave-One-Patient-Out is a specific form of Group K-fold cross-validation (with $K=N_p$) and successfully prevents patient-level leakage.\n-   **Variance Implication**: The proposed variance calculation, $\\widehat{\\mathrm{acc}}(1 - \\widehat{\\mathrm{acc}})/N$, is the standard formula for the variance of a proportion from $N$ i.i.d. Bernoulli trials. This explicitly and incorrectly ignores the clustered data structure and the given intra-class correlation $\\rho$. It fails to account for the design effect, leading to an underestimation of the true variance. The justification that using $N$ \"captures the precision\" is misleading; it captures the raw sample count but not the effective statistical information, which is lower due to correlation. This option's analysis of variance is therefore incorrect.\n-   **Verdict**: **Incorrect**.\n\n### Conclusion\n\nOption B is the only one that correctly identifies both the appropriate data splitting strategy (Group $K$-fold CV) to prevent leakage and the correct qualitative and quantitative implications of intra-patient correlation on the variance of performance estimates (the design effect factor $1 + (\\bar{m} - 1)\\rho$).",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "To build a truly generalizable classifier, one must rigorously separate the process of model selection (hyperparameter tuning) from the final performance assessment. This principle leads to protocols like nested cross-validation, which provides an unbiased performance estimate, followed by evaluation on a final held-out test set. In this exercise , you will not only outline this robust protocol but also delve into the statistical properties of the resulting performance estimators, calculating their variance to appreciate how design choices affect the reliability of your conclusions.",
            "id": "4389555",
            "problem": "You are designing a binary classifier to distinguish septic from non-septic patients using whole-blood transcriptomics within the systems biomedicine pipeline. To ensure robust performance estimation and model selection, you will use a protocol that combines nested cross-validation (CV) and a held-out test set. The data-generation and evaluation assumptions are as follows:\n\n- The development cohort contains $N_{\\text{dev}} = 1000$ independent and identically distributed patients, with a positive-class (sepsis) fraction of $\\pi_{\\text{dev}} = 0.40$ and a negative-class (non-sepsis) fraction of $1 - \\pi_{\\text{dev}}$. The held-out test cohort contains $N_{\\text{test}} = 300$ independent patients, with a positive-class fraction of $\\pi_{\\text{test}} = 0.35$ and a negative-class fraction of $1 - \\pi_{\\text{test}}$.\n- The target performance metric is balanced accuracy (BA), defined as the average of sensitivity (true positive rate) and specificity (true negative rate).\n- Nested cross-validation is performed with $K_{\\text{outer}} = 5$ disjoint outer folds on the development cohort. For each outer fold, an inner CV is used for hyperparameter selection via empirical risk minimization on training data, and the outer-fold validation data are used only for performance estimation.\n- After nested CV determines the hyperparameters, a single model is refit on the entire development cohort and then evaluated once on the held-out test cohort.\n- Assume the final selected operating point of the model has population true positive rate $\\theta_{+} = 0.78$ and population true negative rate $\\theta_{-} = 0.83$. Assume that, conditional on class membership, predictions for different patients are independent Bernoulli trials with success probabilities $\\theta_{+}$ for positives and $\\theta_{-}$ for negatives.\n\nTasks:\n1. Briefly outline a scientifically sound protocol for model selection using nested cross-validation with a held-out test set, adhering to the above constraints.\n2. Under the independence and Bernoulli assumptions, and taking balanced accuracy as the performance metric, compute the variance of the difference between:\n   - the nested CV estimator of balanced accuracy, defined as the mean of the $K_{\\text{outer}}$ outer-fold balanced accuracy estimates, and\n   - the held-out test balanced accuracy estimate calculated on the held-out test cohort.\n   Treat outer-fold validation sets and the held-out test set as mutually independent, and take the outer folds to have equal class counts induced by $\\pi_{\\text{dev}}$.\n\nProvide your final numeric value for the variance of this difference as a decimal number, rounded to four significant figures. Do not use a percent sign; express any proportions as decimals.",
            "solution": "The problem statement has been critically validated and is deemed scientifically grounded, well-posed, and objective. All necessary data and assumptions for a unique solution are provided. The problem describes a standard, rigorous methodology for machine learning model evaluation in a biomedical context and poses a formal statistical question about the properties of the performance estimators. The problem is valid.\n\n### Task 1: Protocol Outline\n\nA scientifically sound protocol for model selection and evaluation, adhering to the specified constraints, is as follows:\n\n1.  **Global Data Partitioning**: The initial dataset of patients is partitioned into two mutually exclusive sets:\n    *   A **development cohort** of $N_{\\text{dev}} = 1000$ patients. This set is used for all model development activities, including hyperparameter tuning and performance estimation via cross-validation.\n    *   A **held-out test cohort** of $N_{\\text{test}} = 300$ patients. This set is sequestered and must not be used in any way during the model development process. It is reserved for a single, final evaluation of the chosen model.\n\n2.  **Nested Cross-Validation (on the Development Cohort)**: This procedure is executed on the $N_{\\text{dev}} = 1000$ patients to obtain a reliable estimate of the model's generalization performance and to guide hyperparameter selection.\n    *   **Outer Loop**: The development cohort is partitioned into $K_{\\text{outer}} = 5$ disjoint folds of equal size ($200$ patients each). The process iterates $K_{\\text{outer}}$ times. In each iteration $k \\in \\{1, 2, 3, 4, 5\\}$:\n        *   Fold $k$ is designated as the **outer validation set**.\n        *   The remaining $K_{\\text{outer}} - 1 = 4$ folds are combined to form the **outer training set** ($800$ patients).\n    *   **Inner Loop**: Within each outer loop iteration $k$, an inner cross-validation procedure is performed exclusively on the corresponding outer training set to select the optimal hyperparameters for that iteration.\n        *   For a pre-defined grid of hyperparameters, a model is trained and validated (e.g., using a separate $J$-fold CV on the outer training data).\n        *   The hyperparameter set that maximizes the average balanced accuracy across the inner folds is selected as the optimal set for outer fold $k$.\n    *   **Outer Fold Performance Estimation**: Using the optimal hyperparameters found in the inner loop, a model is trained on the entire outer training set (the $4$ folds). This model's performance, specifically its balanced accuracy $BA_k$, is then calculated on the outer validation set (fold $k$). This value $BA_k$ is an unbiased estimate of the performance of a model trained on $800$ samples.\n\n3.  **Nested CV Performance Estimate**: After the outer loop completes, the overall performance estimate from nested cross-validation is the mean of the balanced accuracies from the $5$ outer validation folds: $\\widehat{BA}_{\\text{NCV}} = \\frac{1}{K_{\\text{outer}}} \\sum_{k=1}^{K_{\\text{outer}}} BA_k$. The standard deviation of the $BA_k$ values provides an estimate of the stability of the model's performance.\n\n4.  **Final Model Training and Evaluation**:\n    *   A final set of hyperparameters is chosen, typically by selecting the set that performed best on average or occurred most frequently during the inner loops of the nested CV.\n    *   A single, final model is trained using these chosen hyperparameters on the **entire development cohort** ($N_{\\text{dev}} = 1000$ patients).\n    *   This final model is evaluated exactly once on the **held-out test cohort** ($N_{\\text{test}} = 300$ patients). The resulting balanced accuracy, $\\widehat{BA}_{\\text{test}}$, serves as the final, reported estimate of the model's generalization performance on unseen data.\n\n### Task 2: Variance Calculation\n\nWe are asked to compute the variance of the difference between the nested CV estimator of balanced accuracy, $\\widehat{BA}_{\\text{NCV}}$, and the held-out test balanced accuracy estimate, $\\widehat{BA}_{\\text{test}}$. Let this difference be $D = \\widehat{BA}_{\\text{NCV}} - \\widehat{BA}_{\\text{test}}$.\n\nThe problem states that the outer-fold validation sets and the held-out test set are mutually independent. This implies that the random variables $\\widehat{BA}_1, \\dots, \\widehat{BA}_{K_{\\text{outer}}}$ and $\\widehat{BA}_{\\text{test}}$ are all mutually independent. Therefore, the variance of the difference is the sum of the variances:\n$$\n\\text{Var}(D) = \\text{Var}(\\widehat{BA}_{\\text{NCV}} - \\widehat{BA}_{\\text{test}}) = \\text{Var}(\\widehat{BA}_{\\text{NCV}}) + \\text{Var}(\\widehat{BA}_{\\text{test}})\n$$\n\nWe will compute each variance term separately.\n\n**General Formula for Variance of Balanced Accuracy**\nBalanced accuracy is defined as $\\widehat{BA} = \\frac{1}{2}(\\widehat{TPR} + \\widehat{TNR})$, where $\\widehat{TPR}$ is the estimated true positive rate (sensitivity) and $\\widehat{TNR}$ is the estimated true negative rate (specificity).\nLet $N^{+}$ be the number of positive samples and $N^{-}$ be the number of negative samples. Let $TP$ be the number of true positives and $TN$ be the number of true negatives. Then $\\widehat{TPR} = \\frac{TP}{N^{+}}$ and $\\widehat{TNR} = \\frac{TN}{N^{-}}$.\nUnder the given Bernoulli trial assumption, $TP \\sim \\text{Binomial}(N^{+}, \\theta_{+})$ and $TN \\sim \\text{Binomial}(N^{-}, \\theta_{-})$. The variances of these proportions are:\n$$\n\\text{Var}(\\widehat{TPR}) = \\frac{\\theta_{+}(1 - \\theta_{+})}{N^{+}}\n\\quad \\text{and} \\quad\n\\text{Var}(\\widehat{TNR}) = \\frac{\\theta_{-}(1 - \\theta_{-})}{N^{-}}\n$$\nSince $\\widehat{TPR}$ and $\\widehat{TNR}$ are calculated on disjoint sets of patients (positives and negatives), they are independent. The variance of the balanced accuracy is:\n$$\n\\text{Var}(\\widehat{BA}) = \\text{Var}\\left(\\frac{1}{2}(\\widehat{TPR} + \\widehat{TNR})\\right) = \\frac{1}{4}\\left(\\text{Var}(\\widehat{TPR}) + \\text{Var}(\\widehat{TNR})\\right) = \\frac{1}{4}\\left(\\frac{\\theta_{+}(1-\\theta_{+})}{N^{+}} + \\frac{\\theta_{-}(1-\\theta_{-})}{N^{-}}\\right)\n$$\n\n**1. Variance of the Held-Out Test Estimator, $\\text{Var}(\\widehat{BA}_{\\text{test}})$**\nFor the test cohort, we have $N_{\\text{test}} = 300$ and $\\pi_{\\text{test}} = 0.35$.\nThe number of positive and negative patients are:\n$N^{+}_{\\text{test}} = N_{\\text{test}}\\pi_{\\text{test}} = 300 \\times 0.35 = 105$\n$N^{-}_{\\text{test}} = N_{\\text{test}}(1 - \\pi_{\\text{test}}) = 300 \\times 0.65 = 195$\n\nThe population TPR and TNR are given as $\\theta_{+} = 0.78$ and $\\theta_{-} = 0.83$.\n$\\theta_{+}(1 - \\theta_{+}) = 0.78 \\times (1 - 0.78) = 0.78 \\times 0.22 = 0.1716$\n$\\theta_{-}(1 - \\theta_{-}) = 0.83 \\times (1 - 0.83) = 0.83 \\times 0.17 = 0.1411$\n\nUsing the general formula for $\\text{Var}(\\widehat{BA})$:\n$$\n\\text{Var}(\\widehat{BA}_{\\text{test}}) = \\frac{1}{4}\\left(\\frac{0.1716}{105} + \\frac{0.1411}{195}\\right)\n$$\n$$\n\\text{Var}(\\widehat{BA}_{\\text{test}}) = \\frac{1}{4}(0.0016342857... + 0.0007235897...) = \\frac{1}{4}(0.0023578754...) \\approx 0.0005894688\n$$\n\n**2. Variance of the Nested CV Estimator, $\\text{Var}(\\widehat{BA}_{\\text{NCV}})$**\nThe estimator is the mean of the outer-fold estimates: $\\widehat{BA}_{\\text{NCV}} = \\frac{1}{K_{\\text{outer}}} \\sum_{k=1}^{K_{\\text{outer}}} \\widehat{BA}_k$.\nSince the $\\widehat{BA}_k$ are independent and identically distributed (by the assumption of equal fold compositions), the variance is:\n$$\n\\text{Var}(\\widehat{BA}_{\\text{NCV}}) = \\frac{1}{K_{\\text{outer}}^2} \\sum_{k=1}^{K_{\\text{outer}}} \\text{Var}(\\widehat{BA}_k) = \\frac{K_{\\text{outer}}}{K_{\\text{outer}}^2} \\text{Var}(\\widehat{BA}_{\\text{fold}}) = \\frac{1}{K_{\\text{outer}}} \\text{Var}(\\widehat{BA}_{\\text{fold}})\n$$\nwhere $\\text{Var}(\\widehat{BA}_{\\text{fold}})$ is the variance of the balanced accuracy on a single outer validation fold. We have $K_{\\text{outer}} = 5$.\n\nFor each outer fold, the size is $N_{\\text{fold}} = N_{\\text{dev}} / K_{\\text{outer}} = 1000 / 5 = 200$.\nThe class distribution is based on $\\pi_{\\text{dev}} = 0.40$.\n$N^{+}_{\\text{fold}} = N_{\\text{fold}}\\pi_{\\text{dev}} = 200 \\times 0.40 = 80$\n$N^{-}_{\\text{fold}} = N_{\\text{fold}}(1 - \\pi_{\\text{dev}}) = 200 \\times 0.60 = 120$\n\nUsing the general formula for $\\text{Var}(\\widehat{BA})$ on a single fold:\n$$\n\\text{Var}(\\widehat{BA}_{\\text{fold}}) = \\frac{1}{4}\\left(\\frac{\\theta_{+}(1-\\theta_{+})}{N^{+}_{\\text{fold}}} + \\frac{\\theta_{-}(1-\\theta_{-})}{N^{-}_{\\text{fold}}}\\right) = \\frac{1}{4}\\left(\\frac{0.1716}{80} + \\frac{0.1411}{120}\\right)\n$$\n$$\n\\text{Var}(\\widehat{BA}_{\\text{fold}}) = \\frac{1}{4}(0.002145 + 0.0011758333...) = \\frac{1}{4}(0.0033208333...) \\approx 0.0008302083\n$$\nNow, we find the variance of the NCV estimator:\n$$\n\\text{Var}(\\widehat{BA}_{\\text{NCV}}) = \\frac{1}{5} \\times \\text{Var}(\\widehat{BA}_{\\text{fold}}) \\approx \\frac{1}{5} \\times 0.0008302083 \\approx 0.0001660417\n$$\n\n**3. Total Variance of the Difference**\nFinally, we sum the two variances:\n$$\n\\text{Var}(D) = \\text{Var}(\\widehat{BA}_{\\text{NCV}}) + \\text{Var}(\\widehat{BA}_{\\text{test}})\n$$\n$$\n\\text{Var}(D) \\approx 0.0001660417 + 0.0005894688 = 0.0007555105\n$$\nRounding to four significant figures, the result is $0.0007555$.",
            "answer": "$$\n\\boxed{0.0007555}\n$$"
        }
    ]
}