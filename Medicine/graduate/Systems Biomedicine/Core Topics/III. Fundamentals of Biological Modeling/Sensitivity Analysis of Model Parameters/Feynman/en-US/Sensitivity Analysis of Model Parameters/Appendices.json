{
    "hands_on_practices": [
        {
            "introduction": "To begin, we will practice a fundamental technique for calculating local parameter sensitivities: the derivation of sensitivity equations. This powerful method allows us to find how a system's state changes with respect to a parameter by solving a dedicated ordinary differential equation (ODE) for the sensitivity itself. This exercise  will apply this method to a simple mono-exponential decay model, providing a clear and analytically tractable foundation for understanding how sensitivities evolve dynamically.",
            "id": "4385596",
            "problem": "In a pharmacokinetic model of mono-exponential decay for a short-lived biomolecule, the concentration $x(t)$ follows the ordinary differential equation (ODE) $dx/dt=-k\\,x$ with patient-specific initial condition $x(0)=x_{0}(p)$. Here, $k>0$ is the first-order elimination rate constant, and $x_{0}(p)$ is the baseline concentration determined by a patient covariate $p$. For local parameter sensitivity analysis, treat $k$ and $x_{0}$ as independent parameters.\n\nUsing the definition of a local sensitivity $S_{x,\\theta}(t)=\\partial x(t)/\\partial \\theta$ for a parameter $\\theta$, and starting from the fundamental operations of differentiation and the chain rule applied to the state ODE, perform the following:\n\n1. Derive the sensitivity ODEs and initial conditions for $S_{x,k}(t)$ and $S_{x,x_{0}}(t)$ that follow from differentiating the state ODE with respect to $k$ and $x_{0}$, respectively.\n2. Solve these sensitivity ODEs to obtain closed-form expressions for $S_{x,k}(t)$ and $S_{x,x_{0}}(t)$ in terms of $t$, $k$, and $x_{0}$.\n\nProvide your final answer as exact analytic expressions. Do not approximate, and do not include units. Express both sensitivities together as a single row matrix $\\bigl[S_{x,k}(t)\\;\\;S_{x,x_{0}}(t)\\bigr]$.",
            "solution": "The problem asks for the derivation and solution of the sensitivity equations for a mono-exponential decay model. The model is described by the ordinary differential equation (ODE) for the concentration $x(t)$:\n$$\n\\frac{dx}{dt} = -k\\,x\n$$\nwith the initial condition:\n$$\nx(0) = x_{0}\n$$\nThe parameters for the sensitivity analysis are the elimination rate constant $k$ and the initial concentration $x_{0}$. The local sensitivity of the state $x(t)$ with respect to a generic parameter $\\theta$ is defined as $S_{x,\\theta}(t) = \\frac{\\partial x(t)}{\\partial \\theta}$.\n\nFirst, we solve the state ODE. This is a separable first-order linear ODE.\n$$\n\\frac{dx}{x} = -k\\,dt\n$$\nIntegrating both sides gives $\\ln(x) = -kt + C$, where $C$ is the integration constant. Exponentiating yields $x(t) = \\exp(-kt+C) = A \\exp(-kt)$, where $A = \\exp(C)$. Applying the initial condition $x(0) = x_{0}$, we find $A = x_{0}$.\nThus, the solution to the state equation is:\n$$\nx(t) = x_{0} \\exp(-kt)\n$$\nThis expression for $x(t)$ will be used in the derivation of the sensitivity equations.\n\n**1. Derivation and Solution for Sensitivity with respect to $k$**\n\nLet $S_{x,k}(t) = \\frac{\\partial x(t)}{\\partial k}$. To find the ODE governing $S_{x,k}(t)$, we differentiate the state ODE with respect to $k$. We can interchange the order of differentiation with respect to $t$ and $k$:\n$$\n\\frac{d}{dt} S_{x,k}(t) = \\frac{d}{dt}\\left(\\frac{\\partial x}{\\partial k}\\right) = \\frac{\\partial}{\\partial k}\\left(\\frac{dx}{dt}\\right)\n$$\nApplying this to the right-hand side of the state ODE, using the product rule:\n$$\n\\frac{\\partial}{\\partial k}(-k\\,x) = -\\left(\\frac{\\partial k}{\\partial k} \\cdot x + k \\cdot \\frac{\\partial x}{\\partial k}\\right) = -(1 \\cdot x + k \\cdot S_{x,k})\n$$\nEquating the results gives the sensitivity ODE for $S_{x,k}(t)$:\n$$\n\\frac{d S_{x,k}}{dt} = -x - k S_{x,k}\n$$\nSubstituting the solution for $x(t) = x_{0} \\exp(-kt)$:\n$$\n\\frac{d S_{x,k}}{dt} + k S_{x,k} = -x_{0} \\exp(-kt)\n$$\nThe initial condition for $S_{x,k}(t)$ is found by differentiating the state initial condition $x(0) = x_{0}$ with respect to $k$. Since $x_{0}$ is treated as an independent parameter from $k$:\n$$\nS_{x,k}(0) = \\frac{\\partial x(0)}{\\partial k} = \\frac{\\partial x_{0}}{\\partial k} = 0\n$$\nWe solve this first-order linear non-homogeneous ODE using an integrating factor, $I(t) = \\exp\\left(\\int k \\, dt\\right) = \\exp(kt)$. Multiplying the ODE by $I(t)$:\n$$\n\\exp(kt)\\frac{d S_{x,k}}{dt} + k\\exp(kt) S_{x,k} = -x_{0} \\exp(-kt) \\exp(kt)\n$$\nThe left side simplifies to the derivative of a product:\n$$\n\\frac{d}{dt}\\left(S_{x,k}(t) \\exp(kt)\\right) = -x_{0}\n$$\nIntegrating with respect to $t$:\n$$\nS_{x,k}(t) \\exp(kt) = \\int -x_{0} \\, dt = -x_{0}t + C_{1}\n$$\nwhere $C_{1}$ is the integration constant. Solving for $S_{x,k}(t)$:\n$$\nS_{x,k}(t) = (-x_{0}t + C_{1}) \\exp(-kt)\n$$\nApplying the initial condition $S_{x,k}(0) = 0$:\n$$\n0 = (-x_{0} \\cdot 0 + C_{1}) \\exp(0) \\implies C_{1} = 0\n$$\nThus, the solution for the sensitivity with respect to $k$ is:\n$$\nS_{x,k}(t) = -x_{0} t \\exp(-kt)\n$$\n\n**2. Derivation and Solution for Sensitivity with respect to $x_0$**\n\nLet $S_{x,x_{0}}(t) = \\frac{\\partial x(t)}{\\partial x_{0}}$. Following the same procedure, we differentiate the state ODE with respect to $x_{0}$:\n$$\n\\frac{d}{dt} S_{x,x_{0}}(t) = \\frac{d}{dt}\\left(\\frac{\\partial x}{\\partial x_{0}}\\right) = \\frac{\\partial}{\\partial x_{0}}\\left(\\frac{dx}{dt}\\right)\n$$\nApplying this to the right-hand side:\n$$\n\\frac{\\partial}{\\partial x_{0}}(-k\\,x) = -k \\frac{\\partial x}{\\partial x_{0}} = -k S_{x,x_{0}}\n$$\nThis yields the sensitivity ODE:\n$$\n\\frac{d S_{x,x_{0}}}{dt} = -k S_{x,x_{0}}\n$$\nThe initial condition is found by differentiating $x(0) = x_{0}$ with respect to $x_{0}$:\n$$\nS_{x,x_{0}}(0) = \\frac{\\partial x(0)}{\\partial x_{0}} = \\frac{\\partial x_{0}}{\\partial x_{0}} = 1\n$$\nThe ODE for $S_{x,x_{0}}(t)$ is a simple exponential decay equation. Its solution is:\n$$\nS_{x,x_{0}}(t) = S_{x,x_{0}}(0) \\exp(-kt)\n$$\nSubstituting the initial condition $S_{x,x_{0}}(0) = 1$:\n$$\nS_{x,x_{0}}(t) = \\exp(-kt)\n$$\nThe problem requires the final answer to be presented as a row matrix $\\bigl[S_{x,k}(t)\\;\\;S_{x,x_{0}}(t)\\bigr]$. Based on our derivations, this is:\n$$\n\\begin{bmatrix} -x_{0} t \\exp(-kt) & \\exp(-kt) \\end{bmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{bmatrix} -x_{0} t \\exp(-kt) & \\exp(-kt) \\end{bmatrix}}\n$$"
        },
        {
            "introduction": "Building upon the basics, we now extend our analysis to a non-linear system governed by Michaelis-Menten kinetics, a cornerstone of modeling enzyme and transport processes. This practice  requires deriving the sensitivity of the system's state with respect to the maximum reaction rate, $V_{\\max}$. It reveals how a parameter's influence is not constant but can change dynamically depending on the system's state, such as the degree of substrate saturation.",
            "id": "4385597",
            "problem": "Consider a single-compartment biomolecular species with concentration $x(t)$ undergoing saturable elimination with Michaelisâ€“Menten structure. The state dynamics are given by the ordinary differential equation\n$$\n\\dot{x}(t) \\equiv \\frac{dx}{dt} \\;=\\; -\\,\\frac{V_{\\max}\\,x(t)}{K_{m} + x(t)},\n$$\nwith initial condition $x(0)=x_{0}>0$, where $K_{m}>0$ and $V_{\\max}>0$ are constant parameters. The measured output is $y(t)=x(t)$. Assume that $x_{0}$ does not depend on $V_{\\max}$. Using only the foundational definition of local parametric sensitivity as a partial derivative and standard calculus (e.g., separation of variables, differentiation with respect to a parameter), derive an explicit analytic expression for the unscaled local sensitivity\n$$\nS_{y,V_{\\max}}(t) = \\frac{\\partial y(t)}{\\partial V_{\\max}}\n$$\nevaluated along the solution trajectory $x(t)$, expressed in terms of $x(t)$, $K_{m}$, and $t$. Then, based on your derived expression, state how $S_{y,V_{\\max}}(t)$ varies with $x(t)$ in sign and magnitude. Your final reported answer must be the single explicit analytic expression for $S_{y,V_{\\max}}(t)$. No numerical approximation is required and no rounding is needed. Express the final answer without units.",
            "solution": "The problem requires the derivation of the unscaled local parametric sensitivity of the state variable $x(t)$ with respect to the parameter $V_{\\max}$. The state dynamics are given by the Michaelis-Menten ordinary differential equation (ODE):\n$$\n\\frac{dx}{dt} = -\\frac{V_{\\max} x(t)}{K_{m} + x(t)}\n$$\nwith the initial condition $x(0) = x_{0} > 0$. The parameters $V_{\\max}$ and $K_{m}$ are positive constants. The measured output is $y(t) = x(t)$. We are asked to find the sensitivity $S_{y,V_{\\max}}(t)$, defined as:\n$$\nS_{y,V_{\\max}}(t) = \\frac{\\partial y(t)}{\\partial V_{\\max}} = \\frac{\\partial x(t)}{\\partial V_{\\max}}\n$$\nWe will refer to this quantity as $S(t)$ for notational simplicity. The problem specifies that the initial condition $x_{0}$ is independent of $V_{\\max}$, which implies $\\frac{\\partial x_{0}}{\\partial V_{\\max}} = 0$.\n\nA direct approach to find $S(t)$ is to first obtain a relationship between $x(t)$, time $t$, and the model parameters by integrating the state ODE, and then differentiate this relationship with respect to $V_{\\max}$.\n\nFirst, we solve the ODE by separation of variables.\n$$\n\\frac{K_{m} + x(t)}{x(t)} dx = -V_{\\max} dt\n$$\nRearranging the left side gives:\n$$\n\\left( \\frac{K_{m}}{x(t)} + 1 \\right) dx = -V_{\\max} dt\n$$\nWe integrate both sides from the initial time $t=0$ to an arbitrary time $t$. The concentration at these times are $x(0)=x_{0}$ and $x(t)$, respectively.\n$$\n\\int_{x_{0}}^{x(t)} \\left( \\frac{K_{m}}{z} + 1 \\right) dz = \\int_{0}^{t} -V_{\\max} d\\tau\n$$\nEvaluating the integrals yields:\n$$\n\\left[ K_{m} \\ln(z) + z \\right]_{x_{0}}^{x(t)} = \\left[ -V_{\\max} \\tau \\right]_{0}^{t}\n$$\nSince $x_{0} > 0$ and the rate of change $\\frac{dx}{dt}$ is negative for $x>0$, the concentration $x(t)$ remains positive for all $t \\geq 0$. Thus, we do not need the absolute value within the logarithm.\n$$\n\\left( K_{m} \\ln(x(t)) + x(t) \\right) - \\left( K_{m} \\ln(x_{0}) + x_{0} \\right) = -V_{\\max} t\n$$\nThis equation provides an implicit solution for $x(t)$.\n\nNext, we find the sensitivity $S(t) = \\frac{\\partial x(t)}{\\partial V_{\\max}}$ by differentiating this implicit solution with respect to $V_{\\max}$. We treat $x(t)$ as a function of $V_{\\max}$ (and $t$), and apply the chain rule where appropriate. The terms involving only $x_{0}$ and $K_{m}$ have zero derivatives with respect to $V_{\\max}$, as per the problem statement ($\\frac{\\partial x_{0}}{\\partial V_{\\max}} = 0$) and the fact that $K_{m}$ is a parameter not dependent on $V_{\\max}$.\n$$\n\\frac{\\partial}{\\partial V_{\\max}} \\left[ K_{m} \\ln(x(t)) + x(t) - K_{m} \\ln(x_{0}) - x_{0} \\right] = \\frac{\\partial}{\\partial V_{\\max}} \\left[ -V_{\\max} t \\right]\n$$\nDifferentiating the left-hand side term by term:\n$$\n\\frac{\\partial}{\\partial V_{\\max}} (K_{m} \\ln(x(t))) = K_{m} \\frac{1}{x(t)} \\frac{\\partial x(t)}{\\partial V_{\\max}} = \\frac{K_{m}}{x(t)} S(t)\n$$\n$$\n\\frac{\\partial}{\\partial V_{\\max}} (x(t)) = \\frac{\\partial x(t)}{\\partial V_{\\max}} = S(t)\n$$\n$$\n\\frac{\\partial}{\\partial V_{\\max}} (- K_{m} \\ln(x_{0}) - x_{0}) = 0\n$$\nSo the derivative of the left-hand side is:\n$$\n\\frac{K_{m}}{x(t)} S(t) + S(t) = \\left( \\frac{K_{m}}{x(t)} + 1 \\right) S(t) = \\left( \\frac{K_{m} + x(t)}{x(t)} \\right) S(t)\n$$\nDifferentiating the right-hand side with respect to $V_{\\max}$ (treating $t$ as a constant in this context) gives:\n$$\n\\frac{\\partial}{\\partial V_{\\max}} (-V_{\\max} t) = -t\n$$\nEquating the derivatives of both sides, we obtain an algebraic equation for $S(t)$:\n$$\n\\left( \\frac{K_{m} + x(t)}{x(t)} \\right) S(t) = -t\n$$\nSolving for $S(t)$ yields the final expression for the sensitivity:\n$$\nS(t) = -t \\left( \\frac{x(t)}{K_{m} + x(t)} \\right)\n$$\nThis is the explicit analytic expression for the unscaled local sensitivity $S_{y,V_{\\max}}(t)$ as requested, expressed in terms of $x(t)$, $K_{m}$, and $t$.\n\nFinally, we analyze how $S_{y,V_{\\max}}(t)$ varies.\nFor $t > 0$, the time $t$ is positive. Since $x(t) > 0$ and $K_{m} > 0$, the fraction $\\frac{x(t)}{K_{m} + x(t)}$ is always positive. Due to the leading negative sign, the sensitivity $S_{y,V_{\\max}}(t)$ is always negative for $t > 0$. At $t=0$, $S(0)=0$. This negative sign is physically consistent: an increase in the maximum elimination rate $V_{\\max}$ will cause the concentration $x(t)$ to be lower at any subsequent time, hence the partial derivative must be negative.\n\nThe magnitude of the sensitivity is $|S(t)| = t \\frac{x(t)}{K_{m} + x(t)}$. This magnitude is a product of two terms: time $t$, which increases monotonically, and the term $\\frac{x(t)}{K_{m} + x(t)}$. Since $x(t)$ is a monotonically decreasing function of time, and the function $f(z) = \\frac{z}{K_{m}+z}$ is monotonically increasing for $z>0$, the term $\\frac{x(t)}{K_{m} + x(t)}$ is also a monotonically decreasing function of time.\nAt $t \\to 0$, $|S(t)| \\to 0$. As $t \\to \\infty$, $x(t) \\to 0$. In the regime where $x(t) \\ll K_{m}$, the state dynamics are approximately $\\frac{dx}{dt} \\approx -\\frac{V_{\\max}}{K_{m}}x(t)$, leading to an exponential decay $x(t) \\sim \\exp(-\\frac{V_{\\max}}{K_{m}}t)$. The magnitude of the sensitivity then behaves as $|S(t)| \\approx t \\frac{x(t)}{K_{m}} \\sim t \\exp(-\\frac{V_{\\max}}{K_{m}}t)$, which approaches $0$ as $t \\to \\infty$ because the exponential decay dominates the linear growth of $t$.\nSince the magnitude of the sensitivity starts at $0$ at $t=0$ and returns to $0$ as $t \\to \\infty$, it must achieve a maximum value at some intermediate time $t > 0$. This implies that the system's output $x(t)$ is most sensitive to changes in $V_{\\max}$ at an intermediate point along the decay curve, not at the beginning or far into the elimination process.",
            "answer": "$$\\boxed{-t \\frac{x(t)}{K_{m} + x(t)}}$$"
        },
        {
            "introduction": "Our final practice shifts perspective from local to global sensitivity analysis (GSA), which assesses a parameter's importance across its entire range of uncertainty. Using a simple linear model, this exercise  introduces the foundational concepts of variance-based GSA by asking for the derivation of the first-order ($S_i$) and total-effect ($S_{T_i}$) Sobol indices. This will provide a clear, analytical understanding of how to attribute output variance to individual parameters and their interactions.",
            "id": "4385604",
            "problem": "In a linearized systems biomedicine model of a pathway-controlled biomarker, the observable output $Y$ is approximated near a reference state by $Y=\\sum_{j=1}^{m} a_{j} p_{j}$, where $p_{j}$ are independent random parameters representing zero-mean fluctuations in log-transformed reaction rate constants and $a_{j}\\in\\mathbb{R}$ are fixed local sensitivities. The parameters satisfy $\\mathbb{E}[p_{j}]=0$ and $\\operatorname{Var}(p_{j})=\\sigma_{j}^{2}$, with all $\\sigma_{j}^{2}>0$ known. Assume no measurement noise and that the linear approximation is exact for the purpose of sensitivity quantification.\n\nUsing the foundational variance decomposition of random variables and the standard definitions from Global Sensitivity Analysis (GSA), namely that the first-order index $S_{i}$ measures the fraction of output variance attributable to parameter $p_{i}$ alone and the total-effect index $S_{T_{i}}$ measures the fraction of output variance attributable to $p_{i}$ including all its interactions, derive analytic expressions for $S_{i}$ and $S_{T_{i}}$ for a fixed index $i\\in\\{1,\\dots,m\\}$ in terms of $\\{a_{j}\\}$ and $\\{\\sigma_{j}^{2}\\}$.\n\nYour final answer must be given as closed-form analytic expressions. No rounding is required. The indices are dimensionless; do not include units in your final answer.",
            "solution": "The observable output $Y$ is given by the linear model $Y=\\sum_{j=1}^{m} a_{j} p_{j}$, where $p_{j}$ are independent random parameters with $\\mathbb{E}[p_{j}]=0$ and $\\operatorname{Var}(p_{j})=\\sigma_{j}^{2}$. The coefficients $a_{j}$ are fixed constants.\n\nFirst, we calculate the total variance of the output, $\\operatorname{Var}(Y)$. This quantity serves as the normalizing factor for the sensitivity indices. The expectation of $Y$ is $\\mathbb{E}[Y] = \\mathbb{E}\\left[\\sum_{j=1}^{m} a_{j} p_{j}\\right] = \\sum_{j=1}^{m} a_{j} \\mathbb{E}[p_{j}] = \\sum_{j=1}^{m} a_{j} \\cdot 0 = 0$.\nThe variance of $Y$ is given by:\n$$ \\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\mathbb{E}[Y^2] = \\operatorname{Var}\\left(\\sum_{j=1}^{m} a_{j} p_{j}\\right) $$\nDue to the independence of the parameters $p_{j}$, the variance of the sum is the sum of the variances:\n$$ \\operatorname{Var}(Y) = \\sum_{j=1}^{m} \\operatorname{Var}(a_{j} p_{j}) $$\nUsing the property $\\operatorname{Var}(cX) = c^2 \\operatorname{Var}(X)$, where $c$ is a constant, we have:\n$$ \\operatorname{Var}(Y) = \\sum_{j=1}^{m} a_{j}^{2} \\operatorname{Var}(p_{j}) = \\sum_{j=1}^{m} a_{j}^{2} \\sigma_{j}^{2} $$\nSince all $\\sigma_{j}^{2} > 0$ and we can assume at least one $a_j \\neq 0$ for a non-trivial model, the total variance $\\operatorname{Var}(Y) > 0$.\n\nNext, we derive the first-order sensitivity index, $S_{i}$. By definition, $S_{i}$ measures the fraction of the total output variance that can be explained by the variance of parameter $p_{i}$ alone. Mathematically, it is defined as:\n$$ S_{i} = \\frac{\\operatorname{Var}_{p_{i}}(\\mathbb{E}_{p_{\\sim i}}[Y | p_{i}])}{\\operatorname{Var}(Y)} $$\nwhere $p_{\\sim i}$ denotes the set of all parameters except $p_{i}$.\nFirst, we compute the conditional expectation $\\mathbb{E}_{p_{\\sim i}}[Y | p_{i}]$. We can split the sum in the expression for $Y$:\n$$ Y = a_{i}p_{i} + \\sum_{j \\neq i} a_{j}p_{j} $$\nTaking the expectation with respect to all $p_j$ where $j \\neq i$, while treating $p_i$ as a constant:\n$$ \\mathbb{E}_{p_{\\sim i}}[Y | p_{i}] = \\mathbb{E}_{p_{\\sim i}}\\left[a_{i}p_{i} + \\sum_{j \\neq i} a_{j}p_{j}\\right] = a_{i}p_{i} + \\sum_{j \\neq i} a_{j}\\mathbb{E}[p_{j}] $$\nSince $\\mathbb{E}[p_{j}]=0$ for all $j$:\n$$ \\mathbb{E}_{p_{\\sim i}}[Y | p_{i}] = a_{i}p_{i} $$\nNow, we compute the variance of this quantity with respect to $p_{i}$:\n$$ \\operatorname{Var}_{p_{i}}(\\mathbb{E}_{p_{\\sim i}}[Y | p_{i}]) = \\operatorname{Var}_{p_{i}}(a_{i}p_{i}) = a_{i}^{2}\\operatorname{Var}(p_{i}) = a_{i}^{2}\\sigma_{i}^{2} $$\nSubstituting this numerator and the total variance into the definition of $S_{i}$ yields:\n$$ S_{i} = \\frac{a_{i}^{2}\\sigma_{i}^{2}}{\\sum_{j=1}^{m} a_{j}^{2}\\sigma_{j}^{2}} $$\n\nNow, we derive the total-effect index, $S_{T_{i}}$. This index quantifies the contribution of $p_i$ to the output variance, including its main effect and all orders of its interactions with other parameters. It is defined as:\n$$ S_{T_{i}} = \\frac{\\mathbb{E}_{p_{\\sim i}}[\\operatorname{Var}_{p_{i}}(Y | p_{\\sim i})]}{\\operatorname{Var}(Y)} $$\nFirst, we compute the inner conditional variance, $\\operatorname{Var}_{p_{i}}(Y | p_{\\sim i})$. When conditioning on $p_{\\sim i}$, all parameters $p_{j}$ for $j \\neq i$ are treated as fixed constants.\n$$ \\operatorname{Var}_{p_{i}}(Y | p_{\\sim i}) = \\operatorname{Var}_{p_{i}}\\left(a_{i}p_{i} + \\sum_{j \\neq i} a_{j}p_{j} \\Bigg| p_{\\sim i}\\right) $$\nUsing the property $\\operatorname{Var}(X+c) = \\operatorname{Var}(X)$, where the term $\\sum_{j \\neq i} a_{j}p_{j}$ is constant with respect to the variable $p_{i}$:\n$$ \\operatorname{Var}_{p_{i}}(Y | p_{\\sim i}) = \\operatorname{Var}_{p_{i}}(a_{i}p_{i}) = a_{i}^{2}\\operatorname{Var}(p_{i}) = a_{i}^{2}\\sigma_{i}^{2} $$\nNext, we take the expectation of this result with respect to $p_{\\sim i}$:\n$$ \\mathbb{E}_{p_{\\sim i}}[\\operatorname{Var}_{p_{i}}(Y | p_{\\sim i})] = \\mathbb{E}_{p_{\\sim i}}[a_{i}^{2}\\sigma_{i}^{2}] $$\nSince $a_{i}^{2}\\sigma_{i}^{2}$ is a constant, its expectation is itself:\n$$ \\mathbb{E}_{p_{\\sim i}}[a_{i}^{2}\\sigma_{i}^{2}] = a_{i}^{2}\\sigma_{i}^{2} $$\nFinally, substituting this numerator and the total variance into the definition of $S_{T_{i}}$ yields:\n$$ S_{T_{i}} = \\frac{a_{i}^{2}\\sigma_{i}^{2}}{\\sum_{j=1}^{m} a_{j}^{2}\\sigma_{j}^{2}} $$\n\nThe result shows that for this specific model, $S_{i} = S_{T_{i}}$. This is a direct consequence of the model's structure. The model is a linear combination of independent input parameters. In such 'additive' models, there are no interaction terms between parameters. The total-effect index $S_{T_{i}}$ accounts for the main effect of $p_{i}$ plus all interaction effects involving $p_{i}$. Since all interaction variances are zero, $S_{T_{i}}$ reduces to the first-order index $S_{i}$. The sum of the first-order indices is $\\sum_{i=1}^{m} S_{i} = \\sum_{i=1}^{m} \\frac{a_{i}^{2}\\sigma_{i}^{2}}{\\sum_{j=1}^{m} a_{j}^{2}\\sigma_{j}^{2}} = 1$, which confirms that the model variance is fully explained by the sum of the first-order effects, as expected for an additive model.",
            "answer": "$$\n\\boxed{\\begin{bmatrix} \\frac{a_{i}^{2} \\sigma_{i}^{2}}{\\sum_{j=1}^{m} a_{j}^{2} \\sigma_{j}^{2}} & \\frac{a_{i}^{2} \\sigma_{i}^{2}}{\\sum_{j=1}^{m} a_{j}^{2} \\sigma_{j}^{2}} \\end{bmatrix}}\n$$"
        }
    ]
}