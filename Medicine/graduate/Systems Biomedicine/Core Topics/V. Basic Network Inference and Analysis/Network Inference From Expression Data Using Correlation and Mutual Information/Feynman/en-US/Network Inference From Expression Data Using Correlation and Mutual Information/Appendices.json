{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of co-expression network inference is quantifying the association between pairs of genes. This first practice grounds you in the fundamentals by focusing on the Pearson correlation coefficient, the classic measure of linear dependence. You will not only compute the correlation from raw expression data but also derive the statistical test for its significance, connecting it directly to the principles of linear regression . This exercise reinforces core statistical skills and highlights the critical importance of understanding the assumptions—and limitations—of the tools we use.",
            "id": "4365175",
            "problem": "You are studying co-expression relationships to infer regulatory interactions in a gene network from steady-state messenger ribonucleic acid (mRNA) expression measured across independent biological samples. Consider two genes, denoted by $G_{1}$ and $G_{2}$, with normalized log-transformed expression values across $n=10$ samples given by\n$G_{1}: \\{2.0,\\ 2.5,\\ 3.0,\\ 3.5,\\ 4.0,\\ 4.5,\\ 5.0,\\ 5.5,\\ 6.0,\\ 8.0\\}$ and $G_{2}: \\{1.9,\\ 2.6,\\ 2.7,\\ 3.6,\\ 3.9,\\ 4.7,\\ 5.1,\\ 5.4,\\ 6.1,\\ 6.5\\}$. The goal is to quantify the linear association between $G_{1}$ and $G_{2}$ and assess its statistical significance under commonly used assumptions in co-expression network inference.\n\nStarting from the definitions of sample mean, sample covariance, and Pearson product-moment correlation coefficient, compute the sample Pearson correlation $r$ between $G_{1}$ and $G_{2}$. Then, starting from the classical linear model assumptions that $(G_{1},G_{2})$ are jointly Gaussian with independent and identically distributed (i.i.d.) samples and test the null hypothesis $H_{0}:\\rho=0$, derive the null distribution of the correlation-based test statistic by connecting it to the ordinary least squares $t$-test for the slope in simple linear regression of $G_{2}$ on $G_{1}$. Use this derivation to compute the value of the $t$-statistic for the data above.\n\nRound the computed $t$-statistic to four significant figures.\n\nFinally, discuss how heteroscedasticity (variance changing with the mean) and outliers (including high-leverage points) in expression data can affect the validity of this correlation-based $t$-test and the downstream network inference; in particular, explain implications for false positive edges and contrast with mutual information-based inference under the Gaussian assumption and with nonparametric estimators.\n\nYour final reported value must be the single real number equal to the $t$-statistic, rounded to four significant figures, with no units.",
            "solution": "The user has provided a problem that requires the calculation of a statistical measure of association between two variables, the derivation of a related test statistic, and a discussion of the underlying assumptions.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n-   Gene 1 expression data ($G_{1}$): $\\{2.0,\\ 2.5,\\ 3.0,\\ 3.5,\\ 4.0,\\ 4.5,\\ 5.0,\\ 5.5,\\ 6.0,\\ 8.0\\}$\n-   Gene 2 expression data ($G_{2}$): $\\{1.9,\\ 2.6,\\ 2.7,\\ 3.6,\\ 3.9,\\ 4.7,\\ 5.1,\\ 5.4,\\ 6.1,\\ 6.5\\}$\n-   Number of samples ($n$): $10$\n-   Task 1: Compute the sample Pearson correlation coefficient $r$.\n-   Task 2: Derive the null distribution of the correlation-based test statistic by connecting it to the ordinary least squares (OLS) $t$-test for the slope in simple linear regression.\n-   Task 3: Compute the value of the $t$-statistic.\n-   Task 4: Round the $t$-statistic to four significant figures.\n-   Task 5: Discuss the effects of heteroscedasticity and outliers on the validity of the test and compare with mutual information-based inference.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a standard exercise in biostatistics, specifically in the analysis of gene expression data for network inference. The concepts of Pearson correlation, linear regression, $t$-tests, and the assumptions of these models are fundamental statistical principles. The discussion points (heteroscedasticity, outliers, mutual information) are central to robust statistical inference in genomics. The problem is scientifically sound.\n-   **Well-Posed**: The problem provides all necessary data and clearly specifies the required calculations, derivation, and discussion topics. A unique numerical answer exists for the computational part.\n-   **Objective**: The problem is stated using precise, objective, and standard scientific terminology.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full solution.\n\n***\n\n**Part 1: Computation of the Sample Pearson Correlation Coefficient ($r$)**\n\nLet the expression values for gene $G_{1}$ be denoted by the set $\\{x_i\\}$ and for gene $G_{2}$ by $\\{y_i\\}$, for $i=1, \\dots, n$, where $n=10$.\n\nThe sample Pearson correlation coefficient $r$ is defined as:\n$$r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}} = \\frac{SS_{xy}}{\\sqrt{SS_{xx} SS_{yy}}}$$\nwhere $\\bar{x}$ and $\\bar{y}$ are the sample means, and $SS_{xx}$, $SS_{yy}$, and $SS_{xy}$ are the sums of squares and cross-products.\n\nFirst, we compute the sample means:\n$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{10}(2.0+2.5+3.0+3.5+4.0+4.5+5.0+5.5+6.0+8.0) = \\frac{44.0}{10} = 4.4$$\n$$\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i = \\frac{1}{10}(1.9+2.6+2.7+3.6+3.9+4.7+5.1+5.4+6.1+6.5) = \\frac{42.5}{10} = 4.25$$\n\nNext, we compute the sums of squares and cross-products:\n$$SS_{xx} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2 = (2.0-4.4)^2 + (2.5-4.4)^2 + \\dots + (8.0-4.4)^2$$\n$$SS_{xx} = (-2.4)^2 + (-1.9)^2 + (-1.4)^2 + (-0.9)^2 + (-0.4)^2 + (0.1)^2 + (0.6)^2 + (1.1)^2 + (1.6)^2 + (3.6)^2$$\n$$SS_{xx} = 5.76 + 3.61 + 1.96 + 0.81 + 0.16 + 0.01 + 0.36 + 1.21 + 2.56 + 12.96 = 29.4$$\n\n$$SS_{yy} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = (1.9-4.25)^2 + (2.6-4.25)^2 + \\dots + (6.5-4.25)^2$$\n$$SS_{yy} = (-2.35)^2 + (-1.65)^2 + (-1.55)^2 + (-0.65)^2 + (-0.35)^2 + (0.45)^2 + (0.85)^2 + (1.15)^2 + (1.85)^2 + (2.25)^2$$\n$$SS_{yy} = 5.5225 + 2.7225 + 2.4025 + 0.4225 + 0.1225 + 0.2025 + 0.7225 + 1.3225 + 3.4225 + 5.0625 = 21.925$$\n\n$$SS_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) = (2.0-4.4)(1.9-4.25) + \\dots + (8.0-4.4)(6.5-4.25)$$\n$$SS_{xy} = (-2.4)(-2.35) + (-1.9)(-1.65) + (-1.4)(-1.55) + (-0.9)(-0.65) + (-0.4)(-0.35) + (0.1)(0.45) + (0.6)(0.85) + (1.1)(1.15) + (1.6)(1.85) + (3.6)(2.25)$$\n$$SS_{xy} = 5.64 + 3.135 + 2.17 + 0.585 + 0.14 + 0.045 + 0.51 + 1.265 + 2.96 + 8.1 = 24.55$$\n\nFinally, we compute $r$:\n$$r = \\frac{24.55}{\\sqrt{29.4 \\times 21.925}} = \\frac{24.55}{\\sqrt{644.595}} \\approx 0.96696025$$\n\n**Part 2: Derivation of the Null Distribution for the Correlation Test Statistic**\n\nWe aim to test the null hypothesis $H_0: \\rho = 0$, where $\\rho$ is the population correlation coefficient. This is equivalent to testing the null hypothesis $H_0: \\beta_1 = 0$ for the slope in the simple linear regression model $Y = \\beta_0 + \\beta_1 X + \\epsilon$, where we regress $G_2$ (as $Y$) on $G_1$ (as $X$).\n\nThe test statistic for the OLS slope estimate, $b_1$, is given by:\n$$t = \\frac{b_1 - \\beta_1}{SE(b_1)}$$\nUnder $H_0: \\beta_1 = 0$, this simplifies to $t = \\frac{b_1}{SE(b_1)}$. This statistic is known to follow a Student's $t$-distribution with $n-2$ degrees of freedom, provided the model assumptions (linearity, independence, homoscedasticity, normality of errors) hold.\n\nThe OLS estimate for the slope is $b_1 = \\frac{SS_{xy}}{SS_{xx}}$.\nThe standard error of the slope estimate is $SE(b_1) = \\frac{\\hat{\\sigma}}{\\sqrt{SS_{xx}}}$, where $\\hat{\\sigma}^2$ is the mean squared error (MSE), an unbiased estimator of the error variance $\\sigma^2_ \\epsilon$.\nThe MSE is calculated as $MSE = \\frac{SSE}{n-2}$, where $SSE$ is the sum of squared errors (residuals).\nThe SSE can be expressed in terms of the sums of squares: $SSE = SS_{yy} - \\frac{(SS_{xy})^2}{SS_{xx}}$.\n\nNow, we rewrite the $t$-statistic in terms of $r$. First, recognize the relationship between $r$ and $b_1$:\n$$r = \\frac{SS_{xy}}{\\sqrt{SS_{xx} SS_{yy}}} \\implies b_1 = \\frac{SS_{xy}}{SS_{xx}} = \\frac{r \\sqrt{SS_{xx} SS_{yy}}}{SS_{xx}} = r \\sqrt{\\frac{SS_{yy}}{SS_{xx}}}$$\n\nNext, we express $SSE$ in terms of $r$:\n$$SSE = SS_{yy} - \\frac{(r \\sqrt{SS_{xx} SS_{yy}})^2}{SS_{xx}} = SS_{yy} - \\frac{r^2 SS_{xx} SS_{yy}}{SS_{xx}} = SS_{yy} (1 - r^2)$$\n\nNow substitute these expressions back into the formula for the $t$-statistic:\n$$t = \\frac{b_1}{SE(b_1)} = \\frac{b_1}{\\sqrt{\\frac{SSE}{n-2}} / \\sqrt{SS_{xx}}} = \\frac{b_1 \\sqrt{SS_{xx}}}{\\sqrt{\\frac{SS_{yy}(1 - r^2)}{n-2}}}$$\n\nSubstituting the expression for $b_1$:\n$$t = \\frac{\\left(r \\sqrt{\\frac{SS_{yy}}{SS_{xx}}}\\right) \\sqrt{SS_{xx}}}{\\sqrt{\\frac{SS_{yy}(1 - r^2)}{n-2}}} = \\frac{r \\sqrt{SS_{yy}}}{\\frac{\\sqrt{SS_{yy}} \\sqrt{1 - r^2}}{\\sqrt{n-2}}}$$\n$$t = \\frac{r \\sqrt{n-2}}{\\sqrt{1 - r^2}}$$\nThis is the desired test statistic. Under the null hypothesis $H_0: \\rho = 0$, this statistic follows a Student's $t$-distribution with $n-2$ degrees of freedom. This is the null distribution.\n\n**Part 3: Computation of the t-statistic**\n\nUsing the values computed in Part 1:\n$n = 10$\n$r \\approx 0.96696025$\n\n$$t = \\frac{0.96696025 \\sqrt{10-2}}{\\sqrt{1 - (0.96696025)^2}} = \\frac{0.96696025 \\sqrt{8}}{\\sqrt{1 - 0.93501192}} = \\frac{0.96696025 \\times 2.828427}{\\sqrt{0.06498808}} \\approx \\frac{2.73500}{0.2549178} \\approx 10.7290$$\nRounding the result to four significant figures, we get $10.73$.\n\n**Part 4: Discussion on Model Validity and Comparison**\n\nThe validity of the $t$-test for correlation, and by extension co-expression network inference based on it, critically depends on the assumptions of the underlying linear model. Two major violations are heteroscedasticity and the presence of outliers.\n\n**Heteroscedasticity**: This refers to the situation where the variance of the residuals is not constant across a range of predictor values. In gene expression data, it is common for genes with higher mean expression levels to also exhibit higher variance. The OLS procedure assumes homoscedasticity (constant variance). When this assumption is violated, the OLS estimator for the slope ($b_1$) remains unbiased, but the formula for its standard error ($SE(b_1)$) is incorrect and typically biased. This bias results in an incorrectly calculated $t$-statistic, which no longer follows the $t$-distribution under the null hypothesis. The consequence is an invalid p-value and often an inflated Type I error rate. For network inference, this means an increased number of **false positive edges**, where a significant correlation is declared due to the flawed statistical test, not an underlying biological association.\n\n**Outliers and High-Leverage Points**: An outlier is a data point that deviates markedly from other observations. A high-leverage point is an observation with an extreme predictor value (e.g., the point $(x_{10}, y_{10}) = (8.0, 6.5)$ in this problem, where $x_{10}=8.0$ is distant from $\\bar{x}=4.4$). A point that is both an outlier and has high leverage is an influential point. The Pearson correlation coefficient is known to be highly sensitive to such points. A single influential point can create a strong, statistically significant correlation where none exists for the bulk of the data, or it can mask a true underlying correlation. This leads to both **false positives** (inferring an edge due to an artifact) and **false negatives** (failing to infer a true edge). Inferred network connections become highly dependent on single, potentially erroneous, data points rather than the overall biological trend.\n\n**Contrast with Mutual Information (MI)**:\n\n1.  **MI under the Gaussian Assumption**: If the data $(G_1, G_2)$ are truly from a bivariate Gaussian distribution, then the mutual information $I(G_1, G_2)$ is a monotonic function of the squared Pearson correlation coefficient $\\rho^2$: $I(G_1, G_2) = -\\frac{1}{2} \\ln(1 - \\rho^2)$. In this specific, idealized case, testing for $I=0$ is completely equivalent to testing for $\\rho=0$. There is no advantage to using MI over correlation, as they measure the same underlying linear association.\n\n2.  **MI with Nonparametric Estimators**: The true strength of MI is realized when used with nonparametric estimators (e.g., k-nearest neighbor or kernel density estimators) that do not assume a Gaussian (or any other) distribution.\n    -   **Capturing Non-linearity**: MI quantifies any statistical dependency, not just linear ones. For regulatory relationships that are switch-like or sigmoidal, the Pearson correlation may be close to zero, leading to a false negative. MI can detect such non-linear associations.\n    -   **Robustness**: Nonparametric MI is generally more robust to certain types of outliers than Pearson correlation. Because it is based on the probability densities (often estimated from ranks or local densities), it is less distorted by the magnitude of extreme values. It can mitigate the false positives and false negatives driven by influential points that plague correlation-based methods.\n    -   **Disadvantage**: The primary challenge with nonparametric MI is its estimation from limited data. It is prone to systematic estimation bias, especially with small sample sizes ($n=10$ is extremely small for this purpose), and its variance can be high. Accurate MI estimation is data-intensive, which can itself lead to erroneous inferences if not carefully handled.\n\nIn summary, for network inference, a significant correlation-based $t$-test is only a reliable indicator of a linear association under strict assumptions. Violations like heteroscedasticity and outliers, common in real data, can lead to spurious network edges. Nonparametric MI offers a more robust and a more general alternative for detecting dependencies, but its practical application requires larger sample sizes and careful implementation to avoid its own estimation pitfalls.",
            "answer": "$$\n\\boxed{10.73}\n$$"
        },
        {
            "introduction": "While Pearson correlation is a powerful tool, biological relationships are often non-linear. This practice introduces Mutual Information (MI) as a more general measure capable of capturing any type of statistical dependency. Through a guided coding exercise , you will demonstrate a fundamental difference between correlation and MI: MI's invariance to the marginal distributions of the variables. This property allows MI to robustly quantify the underlying dependence structure, providing a more reliable ranking of potential gene interactions than correlation when faced with non-Gaussian data.",
            "id": "4365189",
            "problem": "You are tasked with implementing a complete, runnable program that investigates the ordering of inferred network edges when scored by correlation versus mutual information, using synthetic gene expression data. The context is systems biomedicine, where a network edge corresponds to a pair of genes whose expression profiles are measured across samples. The program must demonstrate that for jointly Gaussian variables, ranking edges by correlation or by mutual information yields identical orderings, and must quantify how these orderings diverge when the marginal distributions are non-Gaussian while the dependence structure remains Gaussian. All mathematical entities must be written in LaTeX.\n\nBegin from the following fundamental base:\n\n- The Pearson correlation coefficient for two real-valued random variables $X$ and $Y$ with finite second moments is defined by\n$$\n\\rho = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)}\\sqrt{\\operatorname{Var}(Y)}}.\n$$\n- The mutual information (MI) between two continuous random variables $X$ and $Y$ with joint probability density function $p_{X,Y}(x,y)$ and marginal density functions $p_X(x)$ and $p_Y(y)$ is defined by\n$$\nI(X;Y) = \\iint p_{X,Y}(x,y)\\,\\log\\left(\\frac{p_{X,Y}(x,y)}{p_X(x)\\,p_Y(y)}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y,\n$$\nwhich is nonnegative and measures statistical dependence without assuming linearity.\n- For a zero-mean, jointly Gaussian vector $(X,Y)$ with covariance matrix\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_X^2 & \\rho\\,\\sigma_X\\sigma_Y \\\\ \\rho\\,\\sigma_X\\sigma_Y & \\sigma_Y^2 \\end{pmatrix},\n$$\nits differential entropy is\n$$\nh(X,Y) = \\frac{1}{2}\\log\\left((2\\pi e)^2 \\det \\Sigma\\right),\n$$\nand the entropy of each marginal is $h(Z) = \\frac{1}{2}\\log\\left(2\\pi e\\,\\sigma_Z^2\\right)$ for $Z \\in \\{X,Y\\}$. Using these facts, one can derive that the mutual information of a bivariate normal depends only on the correlation coefficient:\n$$\nI(X;Y) = -\\frac{1}{2}\\log\\left(1 - \\rho^2\\right).\n$$\n- Mutual information is invariant under invertible transformations of the marginals: if $f$ and $g$ are invertible functions, then $I(X;Y) = I(f(X);g(Y))$. The copula, which captures dependence independent of marginals, is preserved under monotonic transforms.\n\nYour program must implement the following logic:\n\n1. Simulate synthetic latent Gaussian expression data for a set of network edges. Each edge $i$ corresponds to a pair $(X_i, Y_i)$ generated as $n$ independent samples from a bivariate normal distribution with unit variances and specified correlation $\\rho_i$, i.e., covariance matrix\n$$\n\\Sigma_i = \\begin{pmatrix} 1 & \\rho_i \\\\ \\rho_i & 1 \\end{pmatrix}.\n$$\n2. For the Gaussian case, treat $(X_i, Y_i)$ as observed data. Compute, for each edge:\n   - The absolute Pearson correlation $\\lvert \\hat{\\rho}_i \\rvert$ estimated from samples.\n   - The mutual information $I_i$ computed by plugging $\\hat{\\rho}_i$ into the Gaussian formula, i.e.,\n   $$\n   I_i = -\\frac{1}{2}\\log\\left(1 - \\hat{\\rho}_i^2\\right).\n   $$\n   Rank edges in descending order by $\\lvert \\hat{\\rho}_i \\rvert$ and by $I_i$, and determine whether these two rankings are exactly identical (a boolean).\n3. For the non-Gaussian marginal case, apply strictly monotonic transformations to the marginals of each edge’s latent Gaussian samples to obtain observed data $(X'_i,Y'_i)$:\n   - Use the following invertible monotonic functions to induce diverse non-Gaussian marginals, applied per edge $i$:\n     - $f_1(x) = \\log\\left(1 + \\exp\\left(\\operatorname{clip}(x,-20,20)\\right)\\right)$ and $g_1(y) = \\log\\left(1 + \\exp\\left(\\operatorname{clip}(y,-20,20)\\right)\\right)$,\n     - $f_2(x) = \\log\\left(1 + \\exp\\left(\\operatorname{clip}(x,-20,20)\\right)\\right)$ and $g_2(y) = \\sinh\\left(\\operatorname{clip}(y,-4,4)\\right)$,\n     - $f_3(x) = \\log\\left(1 + \\exp\\left(\\operatorname{clip}(x,-20,20)\\right)\\right)$ and $g_3(y) = \\tanh(y)$,\n     - $f_4(x) = \\tanh(x)$ and $g_4(y) = y^3$,\n     - $f_5(x) = \\sinh\\left(\\operatorname{clip}(x,-4,4)\\right)$ and $g_5(y) = \\log\\left(1 + \\exp\\left(\\operatorname{clip}(y,-20,20)\\right)\\right)$.\n     Here $\\operatorname{clip}(z,a,b)$ denotes truncating $z$ to the interval $[a,b]$ to avoid numerical overflow while preserving monotonicity.\n   - For each edge $i$, compute the absolute Pearson correlation $\\lvert \\widehat{\\rho}'_i \\rvert$ on $(X'_i,Y'_i)$.\n   - Compute the mutual information $I_i$ using the known latent correlation parameter $\\rho_i$ via the Gaussian formula $I_i = -\\frac{1}{2}\\log\\left(1 - \\rho_i^2\\right)$, relying on invariance under invertible marginals and the preserved Gaussian copula.\n   - Rank edges by $\\lvert \\widehat{\\rho}'_i \\rvert$ and by $I_i$ (descend both), and quantify divergence between the two rankings using the normalized inversion fraction\n     $$\n     D = \\frac{\\text{number of discordant ordered pairs}}{\\binom{m}{2}},\n     $$\n     where $m$ is the number of edges and an ordered pair is discordant if the relative order of the two edges differs between the two rankings. Report $D$ as a float rounded to six decimal places.\n4. Use a fixed random seed to ensure reproducibility.\n\nTest Suite:\n\n- Case A (Gaussian, happy path): $n=2000$, $m=4$ edges, $\\rho$ values $[0.0, 0.2, 0.5, 0.8]$. Output a boolean indicating whether the correlation-based and mutual information-based rankings are identical.\n- Case B (Non-Gaussian marginals, happy path): $n=2000$, $m=4$ edges, latent $\\rho$ values $[0.0, 0.2, 0.5, 0.8]$, with $(f_1,g_1)$, $(f_2,g_2)$, $(f_3,g_3)$, $(f_4,g_4)$ applied respectively to edges $1$ through $4$. Output the normalized inversion fraction $D$ rounded to six decimal places.\n- Case C (Non-Gaussian marginals, boundary condition with near-perfect correlation and small sample size): $n=200$, $m=5$ edges, latent $\\rho$ values $[0.1, 0.95, 0.6, 0.3, 0.85]$, with $(f_1,g_1)$, $(f_2,g_2)$, $(f_3,g_3)$, $(f_4,g_4)$, $(f_5,g_5)$ applied respectively to edges $1$ through $5$. Output the normalized inversion fraction $D$ rounded to six decimal places.\n\nFinal Output Format:\n\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. The first entry must be the boolean for Case A. The second and third entries must be the floats $D$ for Case B and Case C respectively, each rounded to six decimal places. For example, the output should look like `[True,0.123456,0.000000]`. No other text should be printed.",
            "solution": "The problem statement is critically evaluated for validity before a solution is constructed.\n\n### Step 1: Extract Givens\n\n- **Pearson Correlation Coefficient**: $\\rho = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)}\\sqrt{\\operatorname{Var}(Y)}}$\n- **Mutual Information (MI)**: $I(X;Y) = \\iint p_{X,Y}(x,y)\\,\\log\\left(\\frac{p_{X,Y}(x,y)}{p_X(x)\\,p_Y(y)}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y$\n- **MI for Bivariate Gaussian**: For a zero-mean, jointly Gaussian vector $(X,Y)$ with correlation $\\rho$, the mutual information is $I(X;Y) = -\\frac{1}{2}\\log\\left(1 - \\rho^2\\right)$.\n- **MI Invariance**: For invertible functions $f$ and $g$, $I(X;Y) = I(f(X);g(Y))$.\n- **Simulation Setup**: Synthesize data for $m$ edges. Each edge $i$ corresponds to a pair $(X_i, Y_i)$ generated as $n$ independent samples from a bivariate normal distribution with covariance matrix $\\Sigma_i = \\begin{pmatrix} 1 & \\rho_i \\\\ \\rho_i & 1 \\end{pmatrix}$.\n- **Task 1 (Gaussian Case)**:\n    - Use $(X_i, Y_i)$ as observed data.\n    - Compute estimated absolute correlation $\\lvert \\hat{\\rho}_i \\rvert$.\n    - Compute MI as $I_i = -\\frac{1}{2}\\log\\left(1 - \\hat{\\rho}_i^2\\right)$.\n    - Rank edges by $\\lvert \\hat{\\rho}_i \\rvert$ and $I_i$ and check if rankings are identical.\n- **Task 2 (Non-Gaussian Marginal Case)**:\n    - Transform latent Gaussian samples to $(X'_i,Y'_i)$ using specified strictly monotonic functions.\n    - **Transformations**: Five pairs of functions $(f_k, g_k)$ are provided, using $\\log(1+\\exp(\\cdot))$, $\\sinh(\\cdot)$, $\\tanh(\\cdot)$, and $x^3$. The $\\operatorname{clip}(z,a,b)$ function is defined for numerical stability.\n    - Compute estimated absolute correlation $\\lvert \\widehat{\\rho}'_i \\rvert$ on $(X'_i,Y'_i)$.\n    - Compute MI using the *latent* correlation $\\rho_i$: $I_i = -\\frac{1}{2}\\log\\left(1 - \\rho_i^2\\right)$.\n    - Quantify ranking divergence using the normalized inversion fraction: $D = \\frac{\\text{number of discordant ordered pairs}}{\\binom{m}{2}}$.\n- **Reproducibility**: Use a fixed random seed.\n- **Test Suite**:\n    - **Case A**: Gaussian case, $n=2000$, $m=4$, $\\rho = [0.0, 0.2, 0.5, 0.8]$. Output: boolean.\n    - **Case B**: Non-Gaussian case, $n=2000$, $m=4$, latent $\\rho = [0.0, 0.2, 0.5, 0.8]$, transforms $(f_1,g_1), \\dots, (f_4,g_4)$. Output: float $D$ rounded to $6$ decimal places.\n    - **Case C**: Non-Gaussian case, $n=200$, $m=5$, latent $\\rho = [0.1, 0.95, 0.6, 0.3, 0.85]$, transforms $(f_1,g_1), \\dots, (f_5,g_5)$. Output: float $D$ rounded to $6$ decimal places.\n- **Final Output Format**: A single line `[<boolean>,<float>,<float>]`.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is well-grounded in statistics and information theory. The relationship between mutual information and Pearson correlation for Gaussian variables is a standard textbook result. The use of copula theory (implicitly, via monotonic marginal transformations) to study dependence structures is a valid and widely used statistical concept, particularly in quantitative finance and systems biology. All mathematical formulas and principles are correct.\n- **Well-Posed**: The problem is completely specified. All parameters ($n, m, \\rho$ values), data generation procedures, computational tasks, and output metrics are defined unambiguously. A unique, stable, and meaningful solution can be computed for a given random seed.\n- **Objective**: The problem is stated in precise, objective mathematical and algorithmic language, free from any subjective or speculative claims.\n\nThe problem does not exhibit any of the flaws listed in the invalidity checklist. It is scientifically sound, formalizable, complete, feasible, and well-posed.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be constructed.\n\n### Solution\n\nThe objective is to programmatically demonstrate a fundamental difference between Pearson correlation and mutual information as measures of statistical dependence for inferring network edges. The core principles are:\n$1$. For jointly Gaussian variables, mutual information is a monotonic function of the squared Pearson correlation, leading to identical rankings.\n$2$. Mutual information is invariant under invertible (e.g., strictly monotonic) transformations of the marginal variables, while Pearson correlation is not. This means MI captures the underlying dependence structure (the \"copula\") regardless of the marginal distributions.\n\nThe algorithm will proceed by first setting a fixed random seed for reproducibility. Then, for each test case, we will generate the required synthetic data and perform the specified calculations.\n\n**Data Generation**\nFor each edge $i$, we require $n$ samples from a bivariate normal distribution with a specified correlation $\\rho_i$ and unit variances. This can be achieved by first generating $n$ samples for two independent standard normal variables, $Z_1 \\sim \\mathcal{N}(0,1)$ and $Z_2 \\sim \\mathcal{N}(0,1)$. The desired correlated variables, $X_i$ and $Y_i$, can then be constructed as linear combinations:\n$$X_i = Z_1$$\n$$Y_i = \\rho_i Z_1 + \\sqrt{1 - \\rho_i^2} Z_2$$\nThis construction ensures that $\\operatorname{Var}(X_i)=1$, $\\operatorname{Var}(Y_i) = \\rho_i^2 \\operatorname{Var}(Z_1) + (1-\\rho_i^2)\\operatorname{Var}(Z_2) = \\rho_i^2 + 1 - \\rho_i^2 = 1$, and $\\operatorname{Cov}(X_i, Y_i) = \\operatorname{E}[X_i Y_i] = \\operatorname{E}[Z_1 (\\rho_i Z_1 + \\sqrt{1-\\rho_i^2} Z_2)] = \\rho_i \\operatorname{E}[Z_1^2] = \\rho_i$. Thus, the correlation is $\\rho(X_i, Y_i) = \\rho_i$.\n\n**Case A: Gaussian Data**\nFor this case, we simulate $m=4$ edge datasets with $n=2000$ samples each, using the true correlations $\\rho_i \\in \\{0.0, 0.2, 0.5, 0.8\\}$. For each edge $i$, we perform the following steps:\n$1$. Generate the data matrix $(X_i, Y_i)$.\n$2$. Compute the sample Pearson correlation coefficient, $\\hat{\\rho}_i$, from the data and take its absolute value, $\\lvert \\hat{\\rho}_i \\rvert$.\n$3$. Compute a score for mutual information using the formula for Gaussian variables, $I_i = -\\frac{1}{2}\\log(1 - \\hat{\\rho}_i^2)$.\nThe function $f(r) = -\\frac{1}{2}\\log(1 - r^2)$ is strictly monotonically increasing for $r \\in [0, 1)$. Therefore, for any two scores $\\lvert \\hat{\\rho}_a \\rvert$ and $\\lvert \\hat{\\rho}_b \\rvert$, if $\\lvert \\hat{\\rho}_a \\rvert > \\lvert \\hat{\\rho}_b \\rvert$, it is guaranteed that $I_a > I_b$. Consequently, ranking the edges in descending order of $\\lvert \\hat{\\rho}_i \\rvert$ must yield the exact same ordering as ranking them by $I_i$. The programmatic-ally determined result should be `True`.\n\n**Case B and Case C: Non-Gaussian Marginals**\nThese cases illustrate the divergence of the two metrics. For each edge $i$, the process is as follows:\n$1$. Generate the latent Gaussian data $(X_i, Y_i)$ with $n$ samples and a specified latent correlation $\\rho_i$.\n$2$. Apply the specified pair of strictly monotonic, invertible transformations $(f_k(x), g_k(y))$ to the marginals to obtain the observed data $(X'_i, Y'_i) = (f_k(X_i), g_k(Y_i))$. For Case B, $m=4$ and for Case C, $m=5$.\n$3$. **Correlation-based Ranking**: Compute the sample Pearson correlation $\\widehat{\\rho}'_i$ from the transformed data $(X'_i, Y'_i)$. The score for this ranking is $\\lvert \\widehat{\\rho}'_i \\rvert$. These transformations are generally non-linear and will distort the linear relationship, so $\\widehat{\\rho}'_i$ will differ from $\\rho_i$.\n$4$. **MI-based Ranking**: The problem correctly leverages the MI invariance property. Since the transformations are invertible, $I(X'_i; Y'_i) = I(X_i; Y_i)$. The mutual information of the original latent Gaussian variables is known from the generating parameter $\\rho_i$: $I(X_i; Y_i) = -\\frac{1}{2}\\log(1 - \\rho_i^2)$. We use this value as the MI score for edge $i$. This represents an ideal MI estimator that has access to the true underlying dependence structure.\n$5$. **Divergence Calculation**: We now have two sets of scores for the $m$ edges: one from the empirical correlation on transformed data, $\\{\\lvert \\widehat{\\rho}'_i \\rvert\\}_{i=1}^m$, and one from the theoretical MI, $\\{I_i\\}_{i=1}^m$. We create two ranked lists of edge indices based on these scores. We then quantify the discordance between these two rankings using the normalized inversion fraction, $D$. This is computed by counting the number of pairs of edges $(j, k)$ whose relative order is different in the two rankings, and dividing by the total number of unique pairs, $\\binom{m}{2}$. For two score lists, `scores_1` and `scores_2` of length $m$, the number of discordant pairs can be calculated by iterating through all pairs of indices $j < k$ and checking if $(\\text{scores\\_1}[j] > \\text{scores\\_1}[k]) \\neq (\\text{scores\\_2}[j] > \\text{scores\\_2}[k])$.\nThe final value of $D$ is then rounded to $6$ decimal places. The expected result is $D > 0$, demonstrating that Pearson correlation can give a misleading ranking of dependencies when the data are not jointly Gaussian, even if the underlying dependence structure (copula) is simple.\n\nThe program will encapsulate this logic, run the three test cases, and print the results in the specified format.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements the full logic to compare network edge rankings by correlation\n    and mutual information for Gaussian and non-Gaussian data.\n    \"\"\"\n    \n    # Use a fixed random seed for reproducibility.\n    RANDOM_SEED = 42\n\n    # Define the monotonic transformation functions as specified in the problem.\n    def f1(x): return np.log(1 + np.exp(np.clip(x, -20, 20)))\n    def g1(y): return np.log(1 + np.exp(np.clip(y, -20, 20)))\n    \n    def f2(x): return np.log(1 + np.exp(np.clip(x, -20, 20)))\n    def g2(y): return np.sinh(np.clip(y, -4, 4))\n    \n    def f3(x): return np.log(1 + np.exp(np.clip(x, -20, 20)))\n    def g3(y): return np.tanh(y)\n    \n    def f4(x): return np.tanh(x)\n    def g4(y): return np.power(y, 3)\n    \n    def f5(x): return np.sinh(np.clip(x, -4, 4))\n    def g5(y): return np.log(1 + np.exp(np.clip(y, -20, 20)))\n    \n    transforms_list = [(f1, g1), (f2, g2), (f3, g3), (f4, g4), (f5, g5)]\n\n    # Define the test cases.\n    # Each case is a tuple: (case_id, n_samples, n_edges, rho_values, is_gaussian)\n    test_cases = [\n        ('A', 2000, 4, [0.0, 0.2, 0.5, 0.8], True),\n        ('B', 2000, 4, [0.0, 0.2, 0.5, 0.8], False),\n        ('C', 200, 5, [0.1, 0.95, 0.6, 0.3, 0.85], False)\n    ]\n\n    results = []\n    \n    # Helper to generate bivariate normal data.\n    def generate_gaussian_data(n, rho, rng):\n        z1 = rng.standard_normal(n)\n        z2 = rng.standard_normal(n)\n        x = z1\n        y = rho * z1 + np.sqrt(1 - rho**2) * z2\n        return x, y\n\n    # Helper to calculate normalized inversion fraction.\n    def calculate_inversion_fraction(scores1, scores2):\n        m = len(scores1)\n        if m < 2:\n            return 0.0\n        \n        discordant_pairs = 0\n        for i in range(m):\n            for j in range(i + 1, m):\n                s1_order = scores1[i] > scores1[j]\n                s2_order = scores2[i] > scores2[j]\n                if s1_order != s2_order:\n                    # Check for ties. A tie in one list but not the other is a partial discordance.\n                    # This standard definition counts it as a full discordance.\n                    if (scores1[i] != scores1[j]) and (scores2[i] != scores2[j]):\n                         discordant_pairs += 1\n        \n        total_pairs = m * (m - 1) / 2\n        return discordant_pairs / total_pairs\n\n    # Master random number generator\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    for case_id, n, m, rhos, is_gaussian in test_cases:\n        corr_scores = []\n        mi_scores = []\n\n        for i in range(m):\n            rho = rhos[i]\n            x_latent, y_latent = generate_gaussian_data(n, rho, rng)\n\n            if is_gaussian:\n                # Case A: Gaussian data\n                x_obs, y_obs = x_latent, y_latent\n                \n                # Estimate correlation from data\n                corr_matrix = np.corrcoef(x_obs, y_obs)\n                est_rho = corr_matrix[0, 1]\n                corr_scores.append(np.abs(est_rho))\n                \n                # Calculate MI from estimated rho\n                # np.clip is used to prevent log(0) if |est_rho| is 1.0\n                mi_scores.append(-0.5 * np.log(1 - np.clip(est_rho**2, 0, 1-1e-15)))\n\n            else:\n                # Case B & C: Non-Gaussian marginals\n                f, g = transforms_list[i]\n                x_obs = f(x_latent)\n                y_obs = g(y_latent)\n\n                # Estimate correlation from transformed data\n                corr_matrix = np.corrcoef(x_obs, y_obs)\n                est_rho_prime = corr_matrix[0, 1]\n                corr_scores.append(np.abs(est_rho_prime))\n                \n                # Calculate MI from the TRUE latent rho, based on invariance\n                mi_scores.append(-0.5 * np.log(1 - np.clip(rho**2, 0, 1-1e-15)))\n\n        if is_gaussian:\n            # For Case A, check if rankings are identical.\n            # Since I(rho) is monotonic in |rho|, the rankings must be identical.\n            # We sort indices based on scores and compare the sorted lists of indices.\n            rank_corr = np.argsort(corr_scores)\n            rank_mi = np.argsort(mi_scores)\n            results.append(np.array_equal(rank_corr, rank_mi))\n        else:\n            # For Cases B and C, calculate the normalized inversion fraction.\n            divergence = calculate_inversion_fraction(corr_scores, mi_scores)\n            results.append(round(divergence, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{results[0]},{results[1]:.6f},{results[2]:.6f}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from individual pairwise associations to a full network requires a statistically rigorous, scalable workflow. This final practice guides you through building a complete pipeline for network inference using Mutual Information in a high-dimensional setting. You will implement a non-parametric MI estimator, use permutation resampling to generate empirical $p$-values, and apply the Benjamini-Hochberg procedure to control the False Discovery Rate across thousands of tests . This capstone exercise synthesizes fundamental concepts into a practical application, providing you with a robust framework for identifying significant interactions in genome-scale expression data.",
            "id": "4365174",
            "problem": "You are given a setting in systems biomedicine where a gene expression matrix is measured for $n$ samples across $p$ genes. The task is to infer pairwise associations using Mutual Information (MI) and to assess their statistical significance by converting MI estimates to $p$-values via resampling under a null hypothesis, followed by correction for multiple testing across all $\\binom{p}{2}$ pairs using a False Discovery Rate (FDR) control procedure. Your program must implement the following from first principles.\n\nStart from the base definitions of probability and information. For two continuous random variables $X$ and $Y$ with joint density $f_{X,Y}(x,y)$ and marginals $f_X(x)$ and $f_Y(y)$, the Mutual Information (MI) is defined as\n$$\nI(X;Y) = \\int \\int f_{X,Y}(x,y) \\log\\left(\\frac{f_{X,Y}(x,y)}{f_X(x) f_Y(y)}\\right) \\, dx \\, dy,\n$$\nwhich is estimated in practice by empirical plug-in estimators. Let $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^n$ denote the observed samples for two variables. Partition the real line into $b$ histogram bins for $x$ and $y$ separately, compute the empirical joint frequency counts on the $b \\times b$ grid, normalize by $n$ to produce empirical probabilities $\\hat{p}_{XY}(i,j)$ and marginals $\\hat{p}_X(i)$ and $\\hat{p}_Y(j)$, and define the plug-in estimator\n$$\n\\hat{I}(X;Y) = \\sum_{i=1}^{b} \\sum_{j=1}^{b} \\hat{p}_{XY}(i,j) \\log\\left(\\frac{\\hat{p}_{XY}(i,j)}{\\hat{p}_{X}(i)\\hat{p}_{Y}(j)}\\right),\n$$\nwith the convention that terms with $\\hat{p}_{XY}(i,j) = 0$ are omitted. Use the natural logarithm.\n\nTo convert $\\hat{I}(X;Y)$ to a $p$-value for the null hypothesis that $X$ and $Y$ are independent, construct a resampling null distribution by $B$ independent uniform random permutations of the sample indices of $y$ while keeping $x$ fixed. For each permutation $b \\in \\{1,\\dots,B\\}$, compute $\\hat{I}^{(b)}(X;Y_{\\pi_b})$ using the same histogram bin edges as for the observed data to maintain a stable discretization. Define the resampling-based $p$-value as\n$$\np\\text{-value} = \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\left\\{\\hat{I}^{(b)}(X;Y_{\\pi_b}) \\ge \\hat{I}^{(\\text{obs})}(X;Y)\\right\\}}{B + 1},\n$$\nwhere $\\hat{I}^{(\\text{obs})}(X;Y)$ is the observed MI and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nAcross all $\\binom{p}{2}$ pairs, correct the resulting $p$-values for multiple testing using the Benjamini–Hochberg (BH) procedure for controlling the False Discovery Rate (FDR). For $m = \\binom{p}{2}$ tests with $p$-values $p_1,\\dots,p_m$, order them $p_{(1)} \\le \\dots \\le p_{(m)}$, compute thresholds $t_i = \\frac{i}{m} q$ for an FDR level $q \\in (0,1)$, find $k = \\max\\{i : p_{(i)} \\le t_i\\}$ if such $i$ exists, and reject the null hypotheses for all $i \\le k$. Report the count of rejected edges for each dataset.\n\nAngle unit specification: Use radians for any trigonometric functions. No physical units are involved in this problem.\n\nImplement a single program that:\n- Constructs the specified datasets deterministically using the given pseudorandom seeds.\n- Estimates MI for every gene pair using a histogram plug-in estimator with the specified number of bins.\n- Converts MI to $p$-values via $B$ random permutations of one variable per pair, using plus-one smoothing as specified.\n- Applies the Benjamini–Hochberg (BH) procedure at the specified FDR level $q$ across all pairs within each dataset.\n- Produces the final output as a single line with a list of integers, each giving the number of significant edges after BH correction for the corresponding test case.\n\nDatasets, parameters, and test suite:\n- Case $1$ (happy path with linear, nonlinear, and a constant variable):\n  - Samples $n = 200$, genes $p = 6$.\n  - Seed $1337$.\n  - Generate base noise as independent draws where specified using a standard normal distribution.\n  - Construct columns of the expression matrix $X \\in \\mathbb{R}^{n \\times p}$ as:\n    - $X_{\\cdot,1} = Z_1$, where $Z_1 \\sim \\mathcal{N}(0,1)$.\n    - $X_{\\cdot,2} = X_{\\cdot,1} + 0.2 Z_2$, where $Z_2 \\sim \\mathcal{N}(0,1)$.\n    - $X_{\\cdot,3} = Z_3$, where $Z_3 \\sim \\mathcal{N}(0,1)$.\n    - $X_{\\cdot,4} = \\sin(3 X_{\\cdot,1}) + 0.2 Z_4$, with angle in radians and $Z_4 \\sim \\mathcal{N}(0,1)$.\n    - $X_{\\cdot,5} = 0.8 X_{\\cdot,3} + 0.2 Z_5$, where $Z_5 \\sim \\mathcal{N}(0,1)$.\n    - $X_{\\cdot,6} = 1$ (a constant vector).\n  - Use bins $b = 8$, permutations $B = 500$, FDR level $q = 0.05$.\n\n- Case $2$ (boundary condition with all independent variables):\n  - Samples $n = 60$, genes $p = 5$.\n  - Seed $2025$.\n  - Construct $X_{\\cdot,j} = Z_j$, $j = 1,\\dots,5$, with $Z_j \\sim \\mathcal{N}(0,1)$ independently.\n  - Use bins $b = 6$, permutations $B = 400$, FDR level $q = 0.05$.\n\n- Case $3$ (higher-dimensional setting with mixed dependencies and distributions):\n  - Samples $n = 300$, genes $p = 7$.\n  - Seed $777$.\n  - Generate $U \\sim \\text{Uniform}(-1,1)$ and independent $Z$ variables as indicated.\n  - Construct columns:\n    - $X_{\\cdot,1} = Z_1$, $Z_1 \\sim \\mathcal{N}(0,1)$.\n    - $X_{\\cdot,2} = -0.9 X_{\\cdot,1} + 0.1 Z_2$, $Z_2 \\sim \\mathcal{N}(0,1)$.\n    - $X_{\\cdot,3} = Z_3$, $Z_3 \\sim \\mathcal{N}(0,1)$.\n    - $X_{\\cdot,4} = \\tanh(2 X_{\\cdot,3}) + 0.1 Z_4$, $Z_4 \\sim \\mathcal{N}(0,1)$.\n    - $X_{\\cdot,5} = Z_5$, $Z_5 \\sim \\mathcal{N}(0,1)$.\n    - $X_{\\cdot,6} = X_{\\cdot,5} + 0.4 Z_6$, $Z_6 \\sim \\mathcal{N}(0,1)$.\n    - $X_{\\cdot,7} = U$, $U \\sim \\text{Uniform}(-1,1)$.\n  - Use bins $b = 8$, permutations $B = 600$, FDR level $q = 0.05$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$), where $r_i$ is the number of rejected edges for Case $i$ after Benjamini–Hochberg correction.\n\nDesign for coverage:\n- Case $1$ tests a mixture of strong linear, nonlinear, moderate linear, and degenerate (constant) variables.\n- Case $2$ tests the null scenario with independence across all variables.\n- Case $3$ tests a larger number of pairs with both linear and nonlinear dependencies and mixed marginal distributions.",
            "solution": "The problem statement has been meticulously reviewed and is determined to be **valid**. It is a well-posed, scientifically grounded, and self-contained problem in computational statistics, specifically as applied to systems biology for network inference. All necessary parameters, data generation procedures, and statistical methods (Mutual Information estimation, permutation testing, and False Discovery Rate control) are defined with sufficient rigor and clarity to permit a unique, verifiable solution.\n\nThe solution proceeds by implementing the specified statistical pipeline. The process for each test case is comprised of four main stages:\n\n1.  **Deterministic Data Generation**: For each case, an expression matrix $X \\in \\mathbb{R}^{n \\times p}$ is constructed according to the provided rules. Data generation is made deterministic by using a pseudorandom number generator initialized with the specified seed for that case. This ensures reproducibility. The columns of the matrix, $X_{\\cdot,j}$ for genes $j \\in \\{1, \\dots, p\\}$, represent the variables for which we will infer pairwise associations.\n\n2.  **Pairwise Mutual Information (MI) Estimation**:\n    The association between every pair of genes $(X_{\\cdot,i}, X_{\\cdot,j})$ with $i < j$ is quantified using the Mutual Information, $I(X;Y)$. Due to the continuous nature of the data, the MI must be estimated. The problem specifies a histogram-based plug-in estimator. For two sample vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$, the procedure is as follows:\n    a. The data range for each vector is independently partitioned into $b$ equal-width bins. This defines a $b \\times b$ grid.\n    b. A two-dimensional histogram is constructed by counting the number of samples $(x_k, y_k)$ that fall into each grid cell $(i,j)$. Let this count be $N_{ij}$.\n    c. The counts are normalized by the total number of samples, $n$, to yield the empirical joint probability distribution: $\\hat{p}_{XY}(i,j) = N_{ij} / n$.\n    d. The empirical marginal probabilities are computed by summing over the rows and columns of the joint distribution: $\\hat{p}_X(i) = \\sum_{j=1}^{b} \\hat{p}_{XY}(i,j)$ and $\\hat{p}_Y(j) = \\sum_{i=1}^{b} \\hat{p}_{XY}(i,j)$.\n    e. The plug-in estimator for MI is then calculated using the natural logarithm, summing only over terms where the joint probability is non-zero to adhere to the convention $0 \\log 0 = 0$:\n    $$\n    \\hat{I}(X;Y) = \\sum_{i=1}^{b} \\sum_{j=1}^{b} \\hat{p}_{XY}(i,j) \\log\\left(\\frac{\\hat{p}_{XY}(i,j)}{\\hat{p}_{X}(i)\\hat{p}_{Y}(j)}\\right)\n    $$\n    A special case arises if one of the vectors is constant (e.g., Gene $6$ in Case 1). The MI between a random variable and a constant is mathematically zero. The estimator correctly yields $\\hat{I}=0$ in this situation, as all data for the constant vector falls into a single bin, causing the ratio inside the logarithm to become $1$. The implementation must handle this gracefully, for instance, by checking if the point-to-point range of a vector is zero.\n\n3.  **Significance Testing via Permutation**:\n    To assess the statistical significance of an observed MI value, $\\hat{I}^{(\\text{obs})}$, we test the null hypothesis $H_0$ that the two variables are independent. This is achieved by generating an empirical null distribution through resampling:\n    a. One of the vectors, say $\\mathbf{y}$, is randomly permuted $B$ times. Each permutation, $\\mathbf{y}_{\\pi_b}$, breaks any true association with $\\mathbf{x}$ while preserving the marginal distribution of $\\mathbf{y}$.\n    b. For each permuted vector $\\mathbf{y}_{\\pi_b}$, the MI, $\\hat{I}^{(b)}(X;Y_{\\pi_b})$, is calculated. It is critical that these calculations use the same histogram bin edges that were established from the original, unpermuted data $(\\mathbf{x}, \\mathbf{y})$. This ensures that the null distribution is not confounded by changes in the discretization scheme.\n    c. The resampling-based $p$-value is computed by counting the number of permuted MI values that are greater than or equal to the observed MI value, with a \"plus-one\" smoothing adjustment to avoid $p$-values of zero and improve stability:\n    $$\n    p\\text{-value} = \\frac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\left\\{\\hat{I}^{(b)}(X;Y_{\\pi_b}) \\ge \\hat{I}^{(\\text{obs})}(X;Y)\\right\\}}{B + 1}\n    $$\n    For pairs involving a constant vector, the MI is deterministically zero, and there is no statistical evidence against the null hypothesis of independence. A $p$-value of $1.0$ is assigned to such pairs.\n\n4.  **Multiple Testing Correction with Benjamini-Hochberg (BH)**:\n    Since we perform $m = \\binom{p}{2}$ hypothesis tests simultaneously (one for each gene pair), we must correct the resulting $p$-values to control the experiment-wise error rate. The problem specifies controlling the False Discovery Rate (FDR) at a level $q$ using the Benjamini-Hochberg procedure:\n    a. The $m$ calculated $p$-values are collected and sorted in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n    b. A critical value is defined for each rank $i$: $t_i = \\frac{i}{m} q$.\n    c. We find the largest rank $k$ such that the corresponding sorted $p$-value is less than or equal to its critical value: $k = \\max\\{i : p_{(i)} \\le t_i\\}$.\n    d. If such a $k$ exists, all null hypotheses corresponding to the $p$-values $p_{(1)}, \\dots, p_{(k)}$ are rejected. The number of significant associations (rejected edges) is thus $k$. If no such $k$ exists, no hypotheses are rejected, and the count is $0$.\n\nThe final algorithm integrates these four stages. It iterates through the specified test cases, and for each case, it performs the pairwise MI calculation, permutation testing, and BH correction, ultimately reporting the total number of significant edges found.",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef calculate_mi(x, y, bins):\n    \"\"\"\n    Estimates the mutual information between two vectors x and y using a\n    histogram-based plug-in estimator.\n    \"\"\"\n    # np.histogram2d returns the frequency counts, and the bin edges.\n    counts, _, _ = np.histogram2d(x, y, bins=bins)\n    \n    n = np.sum(counts)\n    if n == 0:\n        return 0.0\n\n    joint_prob = counts / n\n    \n    # Calculate marginal probabilities by summing over axes of the joint probability matrix.\n    p_x = np.sum(joint_prob, axis=1)\n    p_y = np.sum(joint_prob, axis=0)\n\n    # Calculate the outer product of marginals. The shape is broadcast to match joint_prob.\n    p_x_p_y = p_x[:, np.newaxis] * p_y[np.newaxis, :]\n\n    # Find indices where joint probability is non-zero to avoid log(0) and 0/0.\n    # The convention is that terms with p_ij=0 contribute 0 to the sum.\n    nonzero_indices = joint_prob > 0\n    \n    # If p_ij is > 0, then its marginals p_i and p_j must also be > 0.\n    # Therefore, p_x_p_y[nonzero_indices] will also be > 0.\n    mi = np.sum(\n        joint_prob[nonzero_indices] * \n        np.log(joint_prob[nonzero_indices] / p_x_p_y[nonzero_indices])\n    )\n    \n    return mi\n\ndef process_dataset(X, b, B, q, rng):\n    \"\"\"\n    Processes a single expression matrix X to find the number of significant edges.\n    \"\"\"\n    n, p = X.shape\n    num_pairs = p * (p - 1) // 2\n    \n    # Generate all unique pairs of gene indices (i, j) where i < j.\n    gene_pairs = list(combinations(range(p), 2))\n    p_values = []\n\n    for i, j in gene_pairs:\n        x = X[:, i]\n        y = X[:, j]\n\n        # Handle constant vectors: MI is 0, p-value is 1 (no evidence against null).\n        # np.ptp (peak-to-peak) is max-min. If 0, the vector is constant.\n        if np.ptp(x) == 0.0 or np.ptp(y) == 0.0:\n            p_values.append(1.0)\n            continue\n\n        # Define bin edges based on the full range of the original data.\n        # Add a small epsilon to the max to ensure the max value is included in bins.\n        x_edges = np.linspace(x.min(), x.max() + 1e-9, b + 1)\n        y_edges = np.linspace(y.min(), y.max() + 1e-9, b + 1)\n        bin_edges = [x_edges, y_edges]\n\n        # Calculate observed MI\n        mi_obs = calculate_mi(x, y, bins=bin_edges)\n\n        # Build null distribution via permutation\n        mi_null_dist = np.zeros(B)\n        for k in range(B):\n            y_permuted = rng.permutation(y)\n            mi_perm = calculate_mi(x, y_permuted, bins=bin_edges)\n            mi_null_dist[k] = mi_perm\n            \n        # Calculate p-value with plus-one smoothing\n        # Count how many null MI values are >= observed MI\n        count_ge = np.sum(mi_null_dist >= mi_obs)\n        p_val = (1.0 + count_ge) / (B + 1.0)\n        p_values.append(p_val)\n\n    # Apply Benjamini-Hochberg correction\n    p_values = np.array(p_values)\n    m = len(p_values)\n    \n    if m == 0:\n        return 0\n\n    # Sort p-values and keep track of original order if needed (not here)\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = p_values[sorted_indices]\n    \n    # Calculate BH thresholds: t_i = (i/m)*q\n    # Rank i is 1-based, array index is 0-based.\n    ranks = np.arange(1, m + 1)\n    bh_thresholds = (ranks / m) * q\n\n    # Find the largest k such that p_(k) <= t_k\n    significant_mask = sorted_p_values <= bh_thresholds\n    \n    if np.any(significant_mask):\n        # np.where returns a tuple of arrays; we need the array of indices\n        significant_indices = np.where(significant_mask)[0]\n        k = np.max(significant_indices)\n        num_rejected = k + 1 # Number of rejections is the rank of the last one\n    else:\n        num_rejected = 0\n        \n    return num_rejected\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Parameters for each test case\n    test_cases = [\n        {'n': 200, 'p': 6, 'seed': 1337, 'b': 8, 'B': 500, 'q': 0.05},\n        {'n': 60, 'p': 5, 'seed': 2025, 'b': 6, 'B': 400, 'q': 0.05},\n        {'n': 300, 'p': 7, 'seed': 777, 'b': 8, 'B': 600, 'q': 0.05}\n    ]\n\n    results = []\n\n    # Case 1\n    params = test_cases[0]\n    n, p, seed = params['n'], params['p'], params['seed']\n    rng = np.random.default_rng(seed)\n    X = np.zeros((n, p))\n    X[:, 0] = rng.standard_normal(size=n)\n    X[:, 1] = X[:, 0] + 0.2 * rng.standard_normal(size=n)\n    X[:, 2] = rng.standard_normal(size=n)\n    X[:, 3] = np.sin(3 * X[:, 0]) + 0.2 * rng.standard_normal(size=n)\n    X[:, 4] = 0.8 * X[:, 2] + 0.2 * rng.standard_normal(size=n)\n    X[:, 5] = 1.0\n    results.append(process_dataset(X, params['b'], params['B'], params['q'], rng))\n\n    # Case 2\n    params = test_cases[1]\n    n, p, seed = params['n'], params['p'], params['seed']\n    rng = np.random.default_rng(seed)\n    X = rng.standard_normal(size=(n, p))\n    results.append(process_dataset(X, params['b'], params['B'], params['q'], rng))\n\n    # Case 3\n    params = test_cases[2]\n    n, p, seed = params['n'], params['p'], params['seed']\n    rng = np.random.default_rng(seed)\n    X = np.zeros((n, p))\n    X[:, 0] = rng.standard_normal(size=n)\n    X[:, 1] = -0.9 * X[:, 0] + 0.1 * rng.standard_normal(size=n)\n    X[:, 2] = rng.standard_normal(size=n)\n    X[:, 3] = np.tanh(2 * X[:, 2]) + 0.1 * rng.standard_normal(size=n)\n    X[:, 4] = rng.standard_normal(size=n)\n    X[:, 5] = X[:, 4] + 0.4 * rng.standard_normal(size=n)\n    X[:, 6] = rng.uniform(-1, 1, size=n)\n    results.append(process_dataset(X, params['b'], params['B'], params['q'], rng))\n    \n    # Print final output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}