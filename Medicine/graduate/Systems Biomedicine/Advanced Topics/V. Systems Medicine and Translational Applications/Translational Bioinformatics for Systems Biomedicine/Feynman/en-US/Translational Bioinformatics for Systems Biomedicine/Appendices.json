{
    "hands_on_practices": [
        {
            "introduction": "A foundational task in translational bioinformatics is the integration and harmonization of genetic data from diverse sources. Before any meaningful analysis can be performed, genetic variants must be described in a consistent, standardized nomenclature. This exercise challenges you to build a bioinformatics pipeline to convert genomic coordinates into the standard Human Genome Variation Society (HGVS) nomenclature, a critical skill for anyone working with genomic data in a research or clinical setting . You will grapple with the complexities of gene models, transcription strands, and how these factors influence a variant's annotation.",
            "id": "4396025",
            "problem": "You are tasked with designing and implementing a computational pipeline that harmonizes single-nucleotide variant annotations across multiple Variant Call Format (VCF) files by converting them into Human Genome Variation Society (HGVS) coding DNA (c.) nomenclature using Ensembl-like gene models, and then computing the proportion of records that exhibit ambiguous transcript mappings. The pipeline must start from core biological definitions and universally accepted facts about gene structure and transcription, and it must be fully implementable in code.\n\nFundamental basis:\n- The Central Dogma of molecular biology states that deoxyribonucleic acid (DNA) is transcribed into ribonucleic acid (RNA) and translated into protein. A gene consists of exons (expressed regions) and introns (intervening regions). A transcript is a spliced messenger RNA (mRNA) composed of exons in a specific order and orientation, defined by a strand: plus strand (“$+$”) transcripts follow increasing genomic coordinates, and minus strand (“$-$”) transcripts follow decreasing genomic coordinates.\n- Coding DNA sequence (CDS) coordinates for a transcript are measured along the spliced cDNA, which is the concatenation of all exons in transcript order.\n- Base pairing in double-stranded DNA uses Watson–Crick complementarity: adenine–thymine and cytosine–guanine pairs. On the minus strand, the transcript sequence corresponds to the reverse complement of the genomic positive strand. Therefore, when expressing HGVS c. substitutions for minus-strand transcripts, alleles must be complemented.\n\nPipeline requirements and definitions:\n- Input consists of multiple VCF-like collections of single-nucleotide variants (SNVs), each record defined as a tuple $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$ where $\\text{pos}$ is $1$-based and inclusive, and $\\text{ref}, \\text{alt} \\in \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$ and satisfy $|\\text{ref}| = |\\text{alt}| = 1$.\n- Transcripts are defined by tuples $(\\text{id}, \\text{chrom}, \\text{strand}, \\text{exons})$ where $\\text{exons}$ is an ordered list of closed genomic intervals $[\\text{start}, \\text{end}]$ with $\\text{start} \\le \\text{end}$, and all positions are $1$-based and inclusive. Assume the entire exonic span is coding (no untranslated regions), and exon lists are non-overlapping within a transcript.\n- A variant maps to a transcript if its genomic position falls within any exon of that transcript on the same chromosome.\n- The coding DNA coordinate $c$ for a variant at genomic position $g$ within a transcript with exons $E_1, E_2, \\dots, E_n$ is defined as:\n  - For plus strand (“$+$”): let $L_k = \\sum_{i=1}^{k-1} (|E_i|)$ be the cumulative length of exons preceding the exon $E_k$ that contains $g$, and let $E_k = [s_k, e_k]$. Then\n    $$ c = L_k + (g - s_k + 1). $$\n  - For minus strand (“$-$”): define the transcript order as decreasing genomic coordinate. Let $E'_1, E'_2, \\dots, E'_n$ be exons sorted by decreasing start (equivalently, by decreasing end). Let $L'_k = \\sum_{i=1}^{k-1} (|E'_i|)$ and let $E'_k = [s'_k, e'_k]$ be the exon in this order that contains $g$. Then\n    $$ c = L'_k + (e'_k - g + 1). $$\n- The HGVS coding DNA nomenclature for a substitution is expressed as $\\text{id}:\\mathrm{c}.c\\ \\text{r}>\\text{a}$, where $\\text{id}$ is the transcript identifier, $c$ is the coding DNA coordinate, and $\\text{r}$ and $\\text{a}$ are the reference and alternate alleles, respectively. For minus-strand transcripts, use nucleotide complements for both alleles when forming $\\text{r}$ and $\\text{a}$.\n- Ambiguous mapping is defined as a single unique genomic variant $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$ that maps to more than one transcript (producing more than one valid HGVS c. string).\n- Harmonization across VCF files must deduplicate identical variant records across files using the key $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$.\n- The requested statistic is the proportion of ambiguous records among the set of unique records that map to at least one transcript. Formally, if $A$ is the number of unique variants with at least two transcript mappings and $M$ is the number of unique variants with at least one transcript mapping, report\n  $$ p = \\begin{cases}\n  \\dfrac{A}{M} & \\text{if } M > 0,\\\\\n  0.0 & \\text{if } M = 0.\n  \\end{cases} $$\n- Angles do not appear. All answers must be real-valued decimals. Report each proportion rounded to exactly $3$ decimal places.\n\nImplementation constraints:\n- Restrict to single-nucleotide variants as defined above.\n- For minus-strand transcripts, generate HGVS c. substitutions using complemented alleles.\n- Exon intervals are inclusive and may be shared across different transcripts.\n\nTest suite:\n- Test Case $1$:\n  - Transcripts:\n    - ENST000001: chromosome “chr1”, strand “$+$”, exons $[100,110]$ and $[200,210]$.\n    - ENST000002: chromosome “chr1”, strand “$+$”, exons $[205,215]$.\n    - ENST000003: chromosome “chr1”, strand “$-$”, exons $[500,505]$ and $[510,515]$.\n  - VCF A unique records:\n    - $(\\text{chr1}, 205, \\text{G}, \\text{A})$.\n    - $(\\text{chr1}, 102, \\text{A}, \\text{C})$.\n    - $(\\text{chr1}, 600, \\text{T}, \\text{C})$.\n  - VCF B unique records:\n    - $(\\text{chr1}, 205, \\text{G}, \\text{A})$ (duplicate of VCF A).\n    - $(\\text{chr1}, 512, \\text{C}, \\text{T})$.\n    - $(\\text{chr1}, 106, \\text{T}, \\text{G})$.\n  - Expected behavior: After harmonization, the unique set is of size $5$; compute $p$ as defined.\n- Test Case $2$:\n  - Transcripts:\n    - ENST000010: chromosome “chr2”, strand “$+$”, exons $[1000,1005]$ and $[1010,1015]$.\n    - ENST000011: chromosome “chr2”, strand “$+$”, exons $[1005,1012]$.\n    - ENST000012: chromosome “chr2$, strand “$-$”, exons $[2000,2003]$ and $[2005,2008]$.\n  - VCF A unique records:\n    - $(\\text{chr2}, 1005, \\text{A}, \\text{G})$.\n    - $(\\text{chr2}, 1012, \\text{C}, \\text{A})$.\n    - $(\\text{chr2}, 2006, \\text{G}, \\text{A})$.\n  - VCF B unique records:\n    - $(\\text{chr2}, 1005, \\text{A}, \\text{G})$ (duplicate of VCF A).\n    - $(\\text{chr2}, 3000, \\text{T}, \\text{G})$.\n    - $(\\text{chr2}, 2002, \\text{T}, \\text{C})$.\n  - Expected behavior: After harmonization, the unique set is of size $5$; compute $p$ as defined.\n- Test Case $3$:\n  - Transcripts:\n    - ENST000020: chromosome “chr3”, strand “$-$”, exons $[400,405]$.\n  - VCF A unique records:\n    - $(\\text{chr3}, 100, \\text{A}, \\text{C})$.\n    - $(\\text{chr3}, 200, \\text{G}, \\text{T})$.\n  - VCF B unique records:\n    - $(\\text{chr3}, 300, \\text{T}, \\text{A})$.\n  - Expected behavior: No variant maps to any transcript; report $p = 0.0$ by definition.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the three proportions for Test Cases $1$, $2$, and $3$, rounded to exactly $3$ decimal places, enclosed in square brackets. For example, the output must look like “$[0.123,0.456,0.789]$” but with the correct values for this problem.",
            "solution": "The supplied problem is scientifically and computationally valid. It is grounded in fundamental principles of molecular biology, including the Central Dogma, gene structure (exons and introns), transcription, and standardized nomenclature for genetic variation (HGVS). The problem is well-posed, providing a complete and consistent set of definitions, rules, and constraints that permit the derivation of a unique, verifiable solution. The algorithmic task is to implement a bioinformatics pipeline for harmonizing single-nucleotide variant (SNV) data and quantifying transcript mapping ambiguity, a relevant problem in translational genomics.\n\nThe solution is designed by implementing the specified pipeline in a series of logical steps.\n\n**Step 1: Harmonization of Variant Records**\n\nThe initial step involves aggregating variant records from multiple input sources (VCF-like collections) into a single, non-redundant set. A variant is uniquely identified by the tuple $(\\text{chrom}, \\text{pos}, \\text{ref}, \\text{alt})$, where $\\text{chrom}$ is the chromosome, $\\text{pos}$ is the $1$-based genomic position, $\\text{ref}$ is the reference allele, and $\\text{alt}$ is the alternate allele. By inserting all variant tuples from all sources into a set data structure, duplicates are automatically eliminated. This process yields the set of unique genomic variants to be analyzed.\n\n**Step 2: Variant-to-Transcript Mapping**\n\nFor each unique variant, the pipeline must determine which transcripts it affects. A variant at genomic position $g$ on chromosome $\\text{chrom}_v$ is considered to map to a transcript if:\n$1$. The transcript is located on the same chromosome, $\\text{chrom}_t = \\text{chrom}_v$.\n$2$. The variant's position $g$ falls within any of the transcript's exons. An exon is a closed interval $[s, e]$, so the condition is $s \\le g \\le e$.\n\nThe process involves iterating through each unique variant and, for each variant, iterating through the entire list of provided transcripts. A count of mappings is maintained for each variant.\n\n**Step 3: Definition and Identification of Ambiguity**\n\nAn ambiguous mapping occurs when a single unique genomic variant maps to more than one transcript. This is a common scenario in the human genome due to alternative splicing, where a single gene can produce multiple transcript isoforms with different exon compositions. Each distinct mapping results in a different HGVS c. annotation.\n\nThe problem requires the ability to generate these annotations, which depends on the transcript's strand.\n- For a **plus-strand (\"$+$\")** transcript with exons $E_1, E_2, \\dots, E_n$ sorted by increasing genomic coordinates, the coding DNA (cDNA) coordinate $c$ for a variant at genomic position $g$ within exon $E_k = [s_k, e_k]$ is given by:\n$$ c = \\left( \\sum_{i=1}^{k-1} (e_i - s_i + 1) \\right) + (g - s_k + 1) $$\nThe term $\\sum_{i=1}^{k-1} (e_i - s_i + 1)$ represents the cumulative length of the exons preceding $E_k$.\n\n- For a **minus-strand (\"$-$\")** transcript, the exons $E'_1, E'_2, \\dots, E'_n$ are considered in order of decreasing genomic coordinates. The cDNA coordinate $c$ for a variant at genomic position $g$ within exon $E'_k = [s'_k, e'_k]$ is calculated as:\n$$ c = \\left( \\sum_{i=1}^{k-1} (e'_i - s'_i + 1) \\right) + (e'_k - g + 1) $$\nFurthermore, because the transcript is encoded on the reverse-complement strand, the reference and alternate alleles must be complemented for the HGVS string (A$\\leftrightarrow$T, C$\\leftrightarrow$G).\n\nA variant mapping to two different transcripts, say, ENST001 and ENST002, would generate two different HGVS strings, e.g., `ENST001:c.150G>A` and `ENST002:c.95G>A`. This satisfies the definition of ambiguity. To calculate the required statistic, it is sufficient to count the number of transcripts each variant maps to, without necessarily constructing the full HGVS string. A count greater than $1$ signifies ambiguity.\n\n**Step 4: Calculation of the Ambiguity Proportion**\n\nThe final objective is to compute the proportion, $p$, of ambiguously mapped variants. This is defined by the following formula:\n$$ p = \\frac{A}{M} $$\nwhere:\n- $M$ is the total count of unique variants that map to at least one transcript.\n- $A$ is the count of unique variants from the set $M$ that map to more than one transcript (i.e., have at least two mappings).\n\nIn the specific case where no variants map to any transcripts, $M=0$, the proportion $p$ is defined to be $0.0$.\n\nThe algorithmic procedure to compute $p$ is as follows:\n$1$. Initialize two counters: `mapped_variants_count` ($M$) to $0$ and `ambiguous_variants_count` ($A$) to $0$.\n$2$. For each unique variant in the harmonized set:\n    a. Count the number of transcripts it maps to, let this be `num_mappings`.\n    b. If `num_mappings` $\\ge 1$, increment `mapped_variants_count`.\n    c. If `num_mappings` $> 1$, increment `ambiguous_variants_count`.\n$3$. After processing all unique variants, if `mapped_variants_count` $> 0$, calculate the proportion $p = \\text{ambiguous\\_variants\\_count} / \\text{mapped\\_variants\\_count}$. Otherwise, $p = 0.0$.\n$4$. The final result is rounded to $3$ decimal places as required. This procedure is applied systematically to each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and compute the ambiguity proportion.\n    \"\"\"\n    \n    # Test Case 1 Data\n    transcripts_1 = [\n        (\"ENST000001\", \"chr1\", \"+\", [(100, 110), (200, 210)]),\n        (\"ENST000002\", \"chr1\", \"+\", [(205, 215)]),\n        (\"ENST000003\", \"chr1\", \"-\", [(500, 505), (510, 515)]),\n    ]\n    vcf_a_1 = [\n        (\"chr1\", 205, \"G\", \"A\"),\n        (\"chr1\", 102, \"A\", \"C\"),\n        (\"chr1\", 600, \"T\", \"C\"),\n    ]\n    vcf_b_1 = [\n        (\"chr1\", 205, \"G\", \"A\"),\n        (\"chr1\", 512, \"C\", \"T\"),\n        (\"chr1\", 106, \"T\", \"G\"),\n    ]\n\n    # Test Case 2 Data\n    transcripts_2 = [\n        (\"ENST000010\", \"chr2\", \"+\", [(1000, 1005), (1010, 1015)]),\n        (\"ENST000011\", \"chr2\", \"+\", [(1005, 1012)]),\n        (\"ENST000012\", \"chr2\", \"-\", [(2000, 2003), (2005, 2008)]),\n    ]\n    vcf_a_2 = [\n        (\"chr2\", 1005, \"A\", \"G\"),\n        (\"chr2\", 1012, \"C\", \"A\"),\n        (\"chr2\", 2006, \"G\", \"A\"),\n    ]\n    vcf_b_2 = [\n        (\"chr2\", 1005, \"A\", \"G\"),\n        (\"chr2\", 3000, \"T\", \"G\"),\n        (\"chr2\", 2002, \"T\", \"C\"),\n    ]\n\n    # Test Case 3 Data\n    transcripts_3 = [\n        (\"ENST000020\", \"chr3\", \"-\", [(400, 405)]),\n    ]\n    vcf_a_3 = [\n        (\"chr3\", 100, \"A\", \"C\"),\n        (\"chr3\", 200, \"G\", \"T\"),\n    ]\n    vcf_b_3 = [\n        (\"chr3\", 300, \"T\", \"A\"),\n    ]\n\n    test_cases = [\n        (transcripts_1, vcf_a_1, vcf_b_1),\n        (transcripts_2, vcf_a_2, vcf_b_2),\n        (transcripts_3, vcf_a_3, vcf_b_3),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        transcripts, vcf_a, vcf_b = case\n        \n        # Step 1: Harmonize variant records into a unique set\n        unique_variants = set(vcf_a) | set(vcf_b)\n        \n        mapped_variants_count = 0  # Counter for M\n        ambiguous_variants_count = 0  # Counter for A\n        \n        # Step 2: Iterate through unique variants to find mappings\n        for variant in unique_variants:\n            v_chrom, v_pos, _, _ = variant\n            \n            num_mappings = 0\n            for transcript in transcripts:\n                t_id, t_chrom, _, t_exons = transcript\n                \n                # Check for chromosome match first\n                if v_chrom != t_chrom:\n                    continue\n                \n                # Check if variant position is within any exon\n                for exon_start, exon_end in t_exons:\n                    if exon_start = v_pos = exon_end:\n                        num_mappings += 1\n                        # A variant can't be in two different exons of the same transcript\n                        # as exons are non-overlapping. So we can break.\n                        break\n            \n            # Step 3: Tally counts for M and A\n            if num_mappings >= 1:\n                mapped_variants_count += 1\n            if num_mappings > 1:\n                ambiguous_variants_count += 1\n        \n        # Step 4: Calculate the proportion p\n        if mapped_variants_count > 0:\n            proportion = ambiguous_variants_count / mapped_variants_count\n        else:\n            proportion = 0.0\n            \n        results.append(proportion)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Modern 'omics technologies, such as spatial transcriptomics, generate vast amounts of data from tissues, but each measurement often represents a mixture of different cell types. To understand the underlying biology, we must computationally 'unmix' or deconvolute these signals to estimate the cellular composition. This practice guides you through formulating and solving a deconvolution problem using constrained regression, a powerful technique to infer cell-type proportions from bulk expression data . This exercise provides a hands-on introduction to modeling biological systems as linear mixtures and applying optimization to find biologically plausible solutions.",
            "id": "4396026",
            "problem": "Consider a Spatial Transcriptomics (ST) spot measured across $G$ genes, and a single-cell reference panel of $K$ cell types providing gene-wise mean expression profiles. Let the reference be a matrix $X \\in \\mathbb{R}^{G \\times K}$ where the $k$-th column contains the mean expression of the $K$-th cell type across the $G$ genes, and let the spot-level observed expression be a vector $y \\in \\mathbb{R}^{G}$. Under a linear mixture model justified by the Central Dogma of Molecular Biology and the assumption that spot-level expression is an additive mixture of cell-type-specific expression with measurement noise modeled as an independent and identically distributed Gaussian random variable, formulate the following constrained regression problem: find a vector of cell-type proportions $w \\in \\mathbb{R}^{K}$ that minimizes the squared residual norm subject to non-negativity and a sum-to-one constraint. More precisely, solve the program\n$$\n\\min_{w \\in \\mathbb{R}^{K}} \\ \\Vert y - X w \\Vert_2^2 \\quad \\text{subject to} \\quad w_i \\ge 0 \\ \\text{for all} \\ i \\in \\{1,\\dots,K\\}, \\ \\text{and} \\ \\sum_{i=1}^{K} w_i = 1.\n$$\nImplement an algorithm that solves this problem for each test case using constrained optimization with non-negativity and sum-to-one constraints. Each solution should be reported as a list of floats whose entries sum to one and are each greater than or equal to zero.\n\nUse the following test suite, where each test case specifies $G$, $K$, $X$, and $y$ as concrete numeric arrays. All numbers are given as exact integers or rationals.\n\n- Test case $1$ (general mixture, $G=4$, $K=3$):\n  - $X = \\begin{bmatrix} 10  2  0 \\\\ 0  5  1 \\\\ 3  0  8 \\\\ 1  1  1 \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 5.6 \\\\ 1.7 \\\\ 3.1 \\\\ 1.0 \\end{bmatrix}$\n- Test case $2$ (boundary case, single cell type dominates, same $X$ as test case $1$, $G=4$, $K=3$):\n  - $X = \\begin{bmatrix} 10  2  0 \\\\ 0  5  1 \\\\ 3  0  8 \\\\ 1  1  1 \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 10 \\\\ 0 \\\\ 3 \\\\ 1 \\end{bmatrix}$\n- Test case $3$ (underdetermined case with more cell types than genes, $G=3$, $K=4$):\n  - $X = \\begin{bmatrix} 1  2  1  0 \\\\ 0  1  1  2 \\\\ 1  0  2  1 \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$\n- Test case $4$ (zero entries and mixed dominance, $G=3$, $K=3$):\n  - $X = \\begin{bmatrix} 0  1  2 \\\\ 0  3  0 \\\\ 1  0  1 \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 1.4 \\\\ 1.8 \\\\ 0.4 \\end{bmatrix}$\n\nYour program should compute $w$ for each test case by solving the constrained regression problem described above. The final output format must be a single line with the results aggregated as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of the $K$ floats for that test case, rounded to four decimal places. For example, an output with three test cases might look like $[[0.5000,0.3000,0.2000],[1.0000,0.0000,0.0000],[0.2500,0.2500,0.2500,0.2500]]$.",
            "solution": "The mixture model posits that spot-level gene expression is an additive combination of cell-type specific expression contributions. This aligns with the Central Dogma of Molecular Biology, where messenger ribonucleic acid levels result from transcriptional activity and can be modeled as linear combinations when multiple cell types contribute within a spot. Formally, for $G$ genes and $K$ cell types, the single-cell reference matrix $X \\in \\mathbb{R}^{G \\times K}$ has entries $X_{gk}$ representing the mean expression of gene $g$ in cell type $k$. The spot-level measured expression $y \\in \\mathbb{R}^G$ satisfies\n$$\ny = X w + \\epsilon,\n$$\nwhere $w \\in \\mathbb{R}^K$ are the unknown cell-type proportions and $\\epsilon \\in \\mathbb{R}^G$ is a noise vector. The biological constraints on $w$ reflect that cell-type proportions are non-negative and sum to one:\n$$\nw_i \\ge 0 \\ \\text{for all} \\ i \\in \\{1,\\dots,K\\}, \\quad \\sum_{i=1}^{K} w_i = 1.\n$$\nAssuming the noise $\\epsilon$ is independent and identically distributed Gaussian with zero mean, the maximum likelihood estimator under these constraints reduces to minimizing the squared residual norm:\n$$\n\\min_{w \\in \\mathbb{R}^{K}} \\ \\Vert y - X w \\Vert_2^2 \\quad \\text{subject to} \\quad w \\in \\Delta^{K-1},\n$$\nwhere $\\Delta^{K-1}$ denotes the probability simplex defined by non-negativity and sum-to-one constraints. This is a convex Quadratic Programming (QP) problem since the objective is a convex quadratic function and the feasible set is a convex polytope.\n\nTo solve this program algorithmically, one can use Sequential Least Squares Programming (SLSQP), which handles smooth objectives with linear equality constraints and bound constraints. Define the objective function\n$$\nf(w) = \\frac{1}{2}\\Vert y - X w \\Vert_2^2 = \\frac{1}{2} (y - Xw)^\\top (y - Xw),\n$$\nwhich is differentiable with gradient\n$$\n\\nabla f(w) = -X^\\top (y - Xw) = X^\\top X w - X^\\top y.\n$$\nThe constraint $\\sum_{i=1}^{K} w_i = 1$ is linear and can be represented as $c(w) = \\sum_{i=1}^{K} w_i - 1 = 0$. The non-negativity constraints are represented as bound constraints $w_i \\ge 0$ for each $i$.\n\nThe Karush–Kuhn–Tucker (KKT) conditions for this constrained problem characterize optimality. Let $\\lambda \\in \\mathbb{R}$ be the Lagrange multiplier for the equality constraint and $\\mu \\in \\mathbb{R}^{K}$ be the multipliers for the inequality constraints $w_i \\ge 0$, each satisfying $\\mu_i \\ge 0$. The Lagrangian is\n$$\n\\mathcal{L}(w,\\lambda,\\mu) = \\frac{1}{2}\\Vert y - X w \\Vert_2^2 + \\lambda \\left(\\sum_{i=1}^{K} w_i - 1\\right) - \\sum_{i=1}^{K} \\mu_i w_i.\n$$\nThe stationarity condition requires\n$$\n\\nabla_w \\mathcal{L}(w,\\lambda,\\mu) = X^\\top X w - X^\\top y + \\lambda \\mathbf{1} - \\mu = 0,\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{K}$ is the vector of ones. The primal feasibility conditions are $w_i \\ge 0$ for all $i$ and $\\sum_{i=1}^{K} w_i = 1$. The dual feasibility conditions are $\\mu_i \\ge 0$ for all $i$. The complementary slackness conditions are $\\mu_i w_i = 0$ for all $i$. Because the objective is strictly convex in $w$ when $X^\\top X$ is positive definite and the feasible set is convex, the solution, when it exists, is unique under typical conditions; when $X^\\top X$ is only positive semidefinite, solutions are still well-defined on the feasible set and SLSQP yields a valid minimizer given the constraints.\n\nAlgorithmic design:\n- Use an initial guess $w^{(0)}$ computed by projecting the unconstrained least-squares solution onto the simplex. The unconstrained least-squares solution is given by $w_{\\text{LS}} = \\operatorname{argmin}_w \\Vert y - X w \\Vert_2^2$, which can be computed via a pseudoinverse. This $w_{\\text{LS}}$ is then projected onto the probability simplex $\\Delta^{K-1}$ using the Euclidean projection, guaranteeing feasibility.\n- Optimize $f(w)$ using SLSQP with the equality constraint $c(w) = 0$ and bound constraints $0 \\le w_i \\le 1$. Provide the gradient $\\nabla f(w)$ to improve convergence.\n- As a post-processing step, enforce numerical feasibility by clipping tiny negative values to $0$ and renormalizing to ensure $\\sum_i w_i = 1$ if rounding introduces small numerical errors.\n\nTest suite coverage:\n- Test case $1$ assesses recovery of a mixed composition with $G=4$ and $K=3$ where $y$ is exactly a convex combination of columns of $X$; the expected output is close to the generating mixture.\n- Test case $2$ probes a boundary condition where one cell type dominates; the expected output is a one-hot vector.\n- Test case $3$ addresses an underdetermined scenario with $K=4$ and $G=3$, where symmetry suggests a uniform solution on the simplex consistent with $y$.\n- Test case $4$ challenges the solver with zero entries and mixed dominance, ensuring the algorithm handles sparse references and still respects constraints.\n\nThe program computes the solution for each test case and reports the list of proportion vectors rounded to four decimal places. The final output is a single line containing a comma-separated list enclosed in square brackets, where each element is the list of floats for one test case in the specified order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef project_to_simplex(v):\n    \"\"\"\n    Euclidean projection of a vector v onto the probability simplex:\n    {w | w_i >= 0, sum_i w_i = 1}\n    Reference: Efficient Projections onto the l1-Ball for Learning in High Dimensions (Duchi et al.)\n    \"\"\"\n    v = np.asarray(v, dtype=float)\n    n = v.size\n    if n == 0:\n        return v\n    # Sort v in descending order\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - 1))[0]\n    if rho.size == 0:\n        theta = (cssv[-1] - 1) / n\n    else:\n        rho = rho[-1]\n        theta = (cssv[rho] - 1) / (rho + 1)\n    w = np.maximum(v - theta, 0.0)\n    # Numerical safety for sum deviations\n    s = w.sum()\n    if s == 0:\n        # If all zeros due to numerical issues, return uniform\n        w = np.ones(n) / n\n    else:\n        w = w / s\n    return w\n\ndef constrained_regression_simplex(X, y):\n    \"\"\"\n    Solve min_w 0.5 * ||y - X w||^2 subject to w >= 0 and sum(w) = 1.\n    Uses SLSQP with analytical gradient and a simplex-feasible initialization.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    G, K = X.shape\n\n    # Initial guess: project least-squares solution onto simplex\n    # Use pseudoinverse to handle under/overdetermined cases\n    try:\n        w_ls, *_ = np.linalg.lstsq(X, y, rcond=None)\n    except np.linalg.LinAlgError:\n        w_ls = np.ones(K) / K\n    w0 = project_to_simplex(w_ls)\n\n    def objective(w):\n        diff = y - X @ w\n        return 0.5 * np.dot(diff, diff)\n\n    def gradient(w):\n        # grad = X^T X w - X^T y\n        return X.T @ (X @ w) - X.T @ y\n\n    # Equality constraint: sum(w) = 1\n    cons = ({\n        'type': 'eq',\n        'fun': lambda w: np.sum(w) - 1.0,\n        'jac': lambda w: np.ones_like(w)\n    },)\n\n    # Bounds: 0 = w_i = 1\n    bounds = [(0.0, 1.0) for _ in range(K)]\n\n    res = minimize(\n        objective,\n        w0,\n        method='SLSQP',\n        jac=gradient,\n        bounds=bounds,\n        constraints=cons,\n        options={'maxiter': 500, 'ftol': 1e-12, 'disp': False}\n    )\n\n    w = res.x if res.success else w0\n    # Post-process to ensure feasibility and numerical stability\n    w = np.clip(w, 0.0, None)\n    s = w.sum()\n    if s = 0:\n        w = np.ones(K) / K\n    else:\n        w = w / s\n    return w\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: general mixture\n        {\n            \"X\": np.array([[10, 2, 0],\n                           [ 0, 5, 1],\n                           [ 3, 0, 8],\n                           [ 1, 1, 1]], dtype=float),\n            \"y\": np.array([5.6, 1.7, 3.1, 1.0], dtype=float)\n        },\n        # Test case 2: boundary case (single cell type dominates)\n        {\n            \"X\": np.array([[10, 2, 0],\n                           [ 0, 5, 1],\n                           [ 3, 0, 8],\n                           [ 1, 1, 1]], dtype=float),\n            \"y\": np.array([10.0, 0.0, 3.0, 1.0], dtype=float)\n        },\n        # Test case 3: underdetermined (more cell types than genes)\n        {\n            \"X\": np.array([[1, 2, 1, 0],\n                           [0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=float),\n            \"y\": np.array([1.0, 1.0, 1.0], dtype=float)\n        },\n        # Test case 4: zero entries and mixed dominance\n        {\n            \"X\": np.array([[0, 1, 2],\n                           [0, 3, 0],\n                           [1, 0, 1]], dtype=float),\n            \"y\": np.array([1.4, 1.8, 0.4], dtype=float)\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        w = constrained_regression_simplex(X, y)\n        # Round to four decimals as specified\n        w_rounded = [float(f\"{val:.4f}\") for val in w.tolist()]\n        results.append(w_rounded)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Distinguishing correlation from causation is a central challenge in systems biomedicine. Mendelian Randomization (MR) offers a powerful framework for inferring causal relationships between an exposure and an outcome by using genetic variants as instrumental variables. This practice delves into MR-Egger regression, a method designed to detect and adjust for directional pleiotropy—a key potential bias in MR studies . By implementing the regression from first principles, you will gain a deeper understanding of the assumptions underpinning causal inference and learn how to test for their violation.",
            "id": "4396045",
            "problem": "You are given a summary-level Mendelian randomization problem grounded in instrumental variables for translational bioinformatics in systems biomedicine. Consider a set of independent genetic variants (instruments) indexed by $i \\in \\{1,\\dots,n\\}$. For each instrument $i$, denote by $\\hat{\\beta}_{XGi}$ the estimated association of instrument $i$ with the exposure, and by $\\hat{\\beta}_{YGi}$ the estimated association of instrument $i$ with the outcome. Assume known standard errors $\\sigma_{Yi}$ for $\\hat{\\beta}_{YGi}$, with inverse-variance weights $w_i = 1/\\sigma_{Yi}^2$. The structural summary-level model is\n$$\n\\mathbb{E}[\\hat{\\beta}_{YGi} \\mid \\hat{\\beta}_{XGi}] = \\beta_0 + \\beta_{\\text{MR}}\\hat{\\beta}_{XGi},\n$$\nwhere $\\beta_{\\text{MR}}$ is the causal effect parameter to be estimated and $\\beta_0$ is an intercept that captures the average direct (pleiotropic) effect across instruments. The Mendelian Randomization Egger (MR-Egger) approach uses weighted linear regression with an intercept. The intercept provides a test for directional pleiotropy under the Instrument Strength Independent of Direct Effect (InSIDE) assumption, which states that the instrument strengths for the exposure are independent of their direct effects on the outcome.\n\nStarting from the definitions of weighted least squares and linear regression, derive from first principles the weighted normal equations and use them to compute the MR-Egger estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_{\\text{MR}}$ along with their estimated covariance matrix, treating the weights as $w_i = 1/\\sigma_{Yi}^2$. Use the residual weighted sum of squares to estimate the residual variance and then compute a two-sided $t$-test p-value for the intercept $\\beta_0$ with degrees of freedom $n - 2$ to assess directional pleiotropy.\n\nYou must implement a program that:\n- Simulates summary-level data under a realistic linear data-generating mechanism for each test case as described below.\n- Computes the MR-Egger weighted least squares estimates, their standard errors, and the two-sided p-value for the intercept as a test for directional pleiotropy.\n- Produces a single line of output containing the concatenated results across all test cases as a flat list.\n\nData-generating mechanism for each instrument $i$:\n- Draw instrument strength for the exposure $ \\alpha_i \\sim \\mathcal{N}(0, \\sigma_\\alpha^2)$.\n- Draw direct (pleiotropic) effect $ \\delta_i $ such that $ \\delta_i $ has mean $\\mu$ and correlation $\\rho$ with $\\alpha_i$, implemented via\n$$\n\\delta_i = \\mu + \\rho \\frac{\\sigma_\\delta}{\\sigma_\\alpha} \\alpha_i + \\sqrt{1 - \\rho^2}\\,\\sigma_\\delta z_i,\\quad z_i \\sim \\mathcal{N}(0,1).\n$$\n- The true gene-outcome effect is $ \\beta \\alpha_i + \\delta_i $, where $\\beta$ is the true causal effect.\n- Observed summary statistics are\n$$\n\\hat{\\beta}_{XGi} = \\alpha_i + \\varepsilon_{Xi},\\quad \\varepsilon_{Xi} \\sim \\mathcal{N}(0, \\sigma_X^2),\n$$\n$$\n\\hat{\\beta}_{YGi} = \\beta \\alpha_i + \\delta_i + \\varepsilon_{Yi},\\quad \\varepsilon_{Yi} \\sim \\mathcal{N}(0, \\sigma_Y^2),\n$$\nwith $\\sigma_{Yi} = \\sigma_Y$ for all $i$.\n\nYour program must:\n- Use a fixed random seed per test case to ensure reproducibility.\n- For each case, perform a weighted regression of $\\hat{\\beta}_{YGi}$ on $\\hat{\\beta}_{XGi}$ with an intercept using weights $w_i = 1/\\sigma_{Yi}^2$.\n- Return for each test case: the MR-Egger slope estimate $\\hat{\\beta}_{\\text{MR}}$, the intercept estimate $\\hat{\\beta}_0$, and the two-sided p-value for the intercept. Round each value to $4$ decimal places.\n- Aggregate all results into a single line that is a comma-separated list enclosed in square brackets, in the order $[\\hat{\\beta}_{\\text{MR}}^{(1)}, \\hat{\\beta}_0^{(1)}, p_0^{(1)}, \\hat{\\beta}_{\\text{MR}}^{(2)}, \\hat{\\beta}_0^{(2)}, p_0^{(2)}, \\hat{\\beta}_{\\text{MR}}^{(3)}, \\hat{\\beta}_0^{(3)}, p_0^{(3)}]$, where the superscript denotes the test case index.\n\nTest suite (each case is a tuple of parameters $(\\text{seed}, n, \\beta, \\mu, \\rho, \\sigma_\\alpha, \\sigma_\\delta, \\sigma_X, \\sigma_Y)$):\n- Case $1$ (happy path, no directional pleiotropy under InSIDE): $(\\;12345,\\; 50,\\; 0.2,\\; 0.0,\\; 0.0,\\; 0.1,\\; 0.05,\\; 0.005,\\; 0.05\\;)$.\n- Case $2$ (directional pleiotropy under InSIDE): $(\\;67890,\\; 50,\\; 0.2,\\; 0.1,\\; 0.0,\\; 0.1,\\; 0.05,\\; 0.005,\\; 0.05\\;)$.\n- Case $3$ (InSIDE violation with no directional mean effect): $(\\;54321,\\; 50,\\; 0.2,\\; 0.0,\\; 0.9,\\; 0.1,\\; 0.05,\\; 0.005,\\; 0.05\\;)$.\n\nAngle units are not applicable. No physical units are required. Express all outputs as real numbers rounded to $4$ decimal places with no additional text.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$). The exact required output for this problem is $[\\hat{\\beta}_{\\text{MR}}^{(1)}, \\hat{\\beta}_0^{(1)}, p_0^{(1)}, \\hat{\\beta}_{\\text{MR}}^{(2)}, \\hat{\\beta}_0^{(2)}, p_0^{(2)}, \\hat{\\beta}_{\\text{MR}}^{(3)}, \\hat{\\beta}_0^{(3)}, p_0^{(3)}]$ with all values rounded to $4$ decimal places.",
            "solution": "The problem requires the derivation and implementation of the Mendelian Randomization (MR) Egger regression estimators from first principles of weighted least squares (WLS), and the computation of a p-value for the intercept to test for directional pleiotropy.\n\n### **1. Problem Formulation**\n\nLet the instrumental variable summary statistics for $n$ independent genetic variants be denoted by $(\\hat{\\beta}_{XGi}, \\hat{\\beta}_{YGi})$ for $i=1, \\dots, n$. The MR-Egger model is a linear regression of the gene-outcome associations $\\hat{\\beta}_{YGi}$ on the gene-exposure associations $\\hat{\\beta}_{XGi}$. To simplify notation, let $Y_i = \\hat{\\beta}_{YGi}$ and $X_i = \\hat{\\beta}_{XGi}$. The model is:\n$$\nY_i = \\beta_0 + \\beta_{\\text{MR}} X_i + \\epsilon_i\n$$\nwhere $\\beta_0$ is the intercept, representing the average pleiotropic effect, and $\\beta_{\\text{MR}}$ is the slope, representing the causal effect estimate. The regression is weighted by $w_i = 1/\\sigma_{Yi}^2$, where $\\sigma_{Yi}^2$ is the variance of the estimate $\\hat{\\beta}_{YGi}$.\n\n### **2. Derivation of Weighted Least Squares Estimators**\n\nThe WLS method finds the parameters $(\\beta_0, \\beta_{\\text{MR}})$ that minimize the weighted sum of squared residuals (WSSR):\n$$\nS(\\beta_0, \\beta_{\\text{MR}}) = \\sum_{i=1}^n w_i (Y_i - \\beta_0 - \\beta_{\\text{MR}} X_i)^2\n$$\nTo find the minimum, we compute the partial derivatives of $S$ with respect to $\\beta_0$ and $\\beta_{\\text{MR}}$ and set them to zero. These are the weighted normal equations.\n\n**Derivative with respect to $\\beta_0$:**\n$$\n\\frac{\\partial S}{\\partial \\beta_0} = \\sum_{i=1}^n 2 w_i (Y_i - \\beta_0 - \\beta_{\\text{MR}} X_i)(-1) = 0\n$$\n$$\n\\implies \\sum_{i=1}^n w_i (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{MR}} X_i) = 0\n$$\n$$\n\\implies \\hat{\\beta}_0 \\sum_{i=1}^n w_i + \\hat{\\beta}_{\\text{MR}} \\sum_{i=1}^n w_i X_i = \\sum_{i=1}^n w_i Y_i \\quad (1)\n$$\n\n**Derivative with respect to $\\beta_{\\text{MR}}$:**\n$$\n\\frac{\\partial S}{\\partial \\beta_{\\text{MR}}} = \\sum_{i=1}^n 2 w_i (Y_i - \\beta_0 - \\beta_{\\text{MR}} X_i)(-X_i) = 0\n$$\n$$\n\\implies \\sum_{i=1}^n w_i X_i (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{MR}} X_i) = 0\n$$\n$$\n\\implies \\hat{\\beta}_0 \\sum_{i=1}^n w_i X_i + \\hat{\\beta}_{\\text{MR}} \\sum_{i=1}^n w_i X_i^2 = \\sum_{i=1}^n w_i X_i Y_i \\quad (2)\n$$\n\nThese two linear equations can be solved for the estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_{\\text{MR}}$. A more general and computationally convenient approach is to use matrix algebra.\n\n### **3. Matrix Formulation**\n\nLet $\\mathbf{Y}$ be the $n \\times 1$ vector of outcomes $[Y_1, \\dots, Y_n]^T$, $\\mathbf{W}$ be the $n \\times n$ diagonal matrix of weights with $W_{ii} = w_i$, and $\\mathbf{X}$ be the $n \\times 2$ design matrix with the first column being all ones and the second column containing the predictor values $[X_1, \\dots, X_n]^T$.\n$$\n\\mathbf{Y} = \\begin{pmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{pmatrix}, \\quad\n\\mathbf{X} = \\begin{pmatrix} 1  X_1 \\\\ \\vdots  \\vdots \\\\ 1  X_n \\end{pmatrix}, \\quad\n\\mathbf{W} = \\begin{pmatrix} w_1   0 \\\\  \\ddots  \\\\ 0   w_n \\end{pmatrix}\n$$\nThe parameter vector is $\\boldsymbol{\\beta} = [\\beta_0, \\beta_{\\text{MR}}]^T$. The normal equations in matrix form are:\n$$\n(\\mathbf{X}^T \\mathbf{W} \\mathbf{X}) \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{W} \\mathbf{Y}\n$$\nThe solution for the estimated parameter vector $\\hat{\\boldsymbol{\\beta}}$ is:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{Y}\n$$\nwhere $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}}]^T$.\n\n### **4. Estimation of Standard Errors and Hypothesis Testing**\n\nTo perform a hypothesis test for the intercept $\\beta_0$, we need its standard error. This requires an estimate of the model's residual variance and the covariance matrix of the estimators.\n\n**Residual Variance:**\nThe residual for the $i$-th observation is $e_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_{\\text{MR}} X_i)$. The residual weighted sum of squares (WSSR) is:\n$$\n\\text{WSSR} = \\sum_{i=1}^n w_i e_i^2\n$$\nAn unbiased estimator for the residual variance, $\\hat{\\sigma}^2$, is the WSSR divided by the degrees of freedom, which is $n-p$ where $n$ is the number of observations and $p$ is the number of estimated parameters. Here, $p=2$ (for $\\beta_0$ and $\\beta_{\\text{MR}}$).\n$$\n\\hat{\\sigma}^2 = \\frac{\\text{WSSR}}{n-2}\n$$\n\n**Covariance Matrix of Estimators:**\nThe estimated covariance matrix of the parameter vector $\\hat{\\boldsymbol{\\beta}}$ is given by:\n$$\n\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2 (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}\n$$\nThis is a $2 \\times 2$ matrix:\n$$\n\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}}) = \\begin{pmatrix} \\widehat{\\text{Var}}(\\hat{\\beta}_0)  \\widehat{\\text{Cov}}(\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}}) \\\\ \\widehat{\\text{Cov}}(\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}})  \\widehat{\\text{Var}}(\\hat{\\beta}_{\\text{MR}}) \\end{pmatrix}\n$$\nThe standard error of the intercept, $SE(\\hat{\\beta}_0)$, is the square root of its estimated variance:\n$$\nSE(\\hat{\\beta}_0) = \\sqrt{\\widehat{\\text{Var}}(\\hat{\\beta}_0)} = \\sqrt{[\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}})]_{1,1}}\n$$\n\n**t-test for the Intercept:**\nTo test for directional pleiotropy, we test the null hypothesis $H_0: \\beta_0 = 0$. The test statistic is a $t$-statistic:\n$$\nt_{\\text{stat}} = \\frac{\\hat{\\beta}_0 - 0}{SE(\\hat{\\beta}_0)}\n$$\nUnder the null hypothesis, this statistic follows a $t$-distribution with $n-2$ degrees of freedom. The two-sided p-value is calculated as:\n$$\np_0 = 2 \\cdot P(T_{n-2} \\ge |t_{\\text{stat}}|)\n$$\nwhere $T_{n-2}$ is a random variable following the $t$-distribution with $n-2$ degrees of freedom.\n\n### **5. Implementation Strategy**\n\nThe program will implement the following steps for each test case:\n1.  **Data Simulation:** Using the provided parameters ($\\text{seed}, n, \\beta, \\mu, \\rho, \\dots$), generate the summary statistics $\\hat{\\beta}_{XGi}$ and $\\hat{\\beta}_{YGi}$ for $n$ instruments according to the specified data-generating mechanism.\n2.  **WLS Regression:**\n    *   Construct the response vector $\\mathbf{Y}$ from $\\hat{\\beta}_{YGi}$ and the design matrix $\\mathbf{X}$ from a column of ones and the vector of $\\hat{\\beta}_{XGi}$.\n    *   Construct the diagonal weight matrix $\\mathbf{W}$ using $w_i = 1/\\sigma_Y^2$.\n    *   Compute the parameter estimates $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_{\\text{MR}}]^T$ using the matrix formula $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{Y}$.\n3.  **P-value Calculation:**\n    *   Calculate the predicted values $\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$ and the residuals $\\mathbf{e} = \\mathbf{Y} - \\hat{\\mathbf{Y}}$.\n    *   Compute the residual variance estimator $\\hat{\\sigma}^2$.\n    *   Calculate the parameter covariance matrix $\\widehat{\\text{Cov}}(\\hat{\\boldsymbol{\\beta}})$.\n    *   Extract the standard error for the intercept, $SE(\\hat{\\beta}_0)$.\n    *   Compute the $t$-statistic and the corresponding two-sided p-value using the $t$-distribution with $n-2$ degrees of freedom.\n4.  **Output Formatting:** Round the resulting $\\hat{\\beta}_{\\text{MR}}$, $\\hat{\\beta}_0$, and $p_0$ to $4$ decimal places and aggregate them into a single comma-separated list as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to run the MR-Egger simulation and analysis for all test cases.\n    \"\"\"\n    # Test suite (each case is a tuple of parameters \n    # (seed, n, beta, mu, rho, sigma_alpha, sigma_delta, sigma_X, sigma_Y))\n    test_cases = [\n        # Case 1 (happy path, no directional pleiotropy under InSIDE)\n        (12345, 50, 0.2, 0.0, 0.0, 0.1, 0.05, 0.005, 0.05),\n        # Case 2 (directional pleiotropy under InSIDE)\n        (67890, 50, 0.2, 0.1, 0.0, 0.1, 0.05, 0.005, 0.05),\n        # Case 3 (InSIDE violation with no directional mean effect)\n        (54321, 50, 0.2, 0.0, 0.9, 0.1, 0.05, 0.005, 0.05),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_mr_egger_analysis(*case)\n        all_results.extend(result)\n    \n    # Format the final list as a string \"[val1,val2,...]\" with 4 decimal places.\n    formatted_results = [f'{val:.4f}' for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_mr_egger_analysis(seed, n, beta, mu, rho, sigma_alpha, sigma_delta, sigma_X, sigma_Y):\n    \"\"\"\n    Simulates summary-level data and computes MR-Egger estimates and statistics for one case.\n    \n    Args:\n        seed (int): Random seed for reproducibility.\n        n (int): Number of instruments.\n        beta (float): True causal effect.\n        mu (float): Mean of direct (pleiotropic) effects.\n        rho (float): Correlation between instrument strength and direct effects.\n        sigma_alpha (float): Standard deviation of instrument strength.\n        sigma_delta (float): Standard deviation of direct effects.\n        sigma_X (float): Standard deviation of measurement error for exposure associations.\n        sigma_Y (float): Standard deviation of measurement error for outcome associations.\n\n    Returns:\n        tuple[float, float, float]: A tuple containing the estimated slope (beta_MR_hat),\n                                     intercept (beta0_hat), and the p-value for the intercept.\n    \"\"\"\n    # 1. Data Simulation\n    rng = np.random.default_rng(seed)\n    \n    # Instrument strength for exposure\n    alpha = rng.normal(0, sigma_alpha, n)\n    \n    # Direct (pleiotropic) effect\n    z = rng.normal(0, 1, n)\n    delta = mu + rho * (sigma_delta / sigma_alpha) * alpha + np.sqrt(1 - rho**2) * sigma_delta * z\n    \n    # Measurement errors\n    eps_X = rng.normal(0, sigma_X, n)\n    eps_Y = rng.normal(0, sigma_Y, n)\n    \n    # Observed summary statistics\n    beta_X_hat = alpha + eps_X\n    beta_Y_hat = beta * alpha + delta + eps_Y\n\n    # 2. MR-Egger Weighted Least Squares Regression\n    # Let Y_vec = beta_Y_hat, X_vec = beta_X_hat\n    Y_vec = beta_Y_hat\n    \n    # Construct the design matrix X_mat (n x 2)\n    X_mat = np.vstack((np.ones(n), beta_X_hat)).T\n    \n    # Construct the diagonal weight matrix W (n x n)\n    # Weights are constant since sigma_Yi is constant\n    weights = np.full(n, 1.0 / (sigma_Y**2))\n    W = np.diag(weights)\n\n    # Compute (X^T * W * X)\n    XTW = X_mat.T @ W\n    XTWX = XTW @ X_mat\n    \n    # Compute inverse of (X^T * W * X)\n    try:\n        XTWX_inv = np.linalg.inv(XTWX)\n    except np.linalg.LinAlgError:\n        # Handle cases of singular matrix (collinear predictors)\n        return (np.nan, np.nan, np.nan)\n\n    # Compute (X^T * W * Y)\n    XTWY = XTW @ Y_vec\n\n    # Compute parameter estimates: beta_hat = (X^T*W*X)^-1 * (X^T*W*Y)\n    beta_hats = XTWX_inv @ XTWY\n    beta0_hat = beta_hats[0]\n    beta_MR_hat = beta_hats[1]\n    \n    # 3. Standard Error and P-value Calculation\n    # Predicted values and residuals\n    Y_hat = X_mat @ beta_hats\n    residuals = Y_vec - Y_hat\n    \n    # Residual weighted sum of squares (WSSR)\n    wssr = np.sum(weights * (residuals**2))\n    \n    # Degrees of freedom\n    df = n - 2\n    if df = 0:\n        return (beta_MR_hat, beta0_hat, np.nan)\n\n    # Residual variance estimate\n    residual_variance = wssr / df\n    \n    # Covariance matrix of parameter estimates\n    cov_beta_hats = residual_variance * XTWX_inv\n    \n    # Standard error of the intercept\n    var_beta0_hat = cov_beta_hats[0, 0]\n    se_beta0_hat = np.sqrt(var_beta0_hat)\n    \n    # t-statistic for the intercept (H0: beta0 = 0)\n    if se_beta0_hat > 0:\n        t_stat = beta0_hat / se_beta0_hat\n        # Two-sided p-value\n        p_value_intercept = 2 * t.sf(np.abs(t_stat), df=df)\n    else:\n        p_value_intercept = np.nan\n\n    return (beta_MR_hat, beta0_hat, p_value_intercept)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}