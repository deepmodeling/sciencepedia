{
    "hands_on_practices": [
        {
            "introduction": "A fundamental goal of experimental design is to reduce uncertainty about the parameters of a scientific model. This practice delves into the heart of Bayesian optimal experimental design by calculating the expected reduction in parameter variance. By working through a simple linear model (), you will derive a key result that shows how the choice of an experimental design directly impacts the precision of our posterior beliefs, providing a concrete foundation for understanding more advanced design criteria.",
            "id": "4313149",
            "problem": "In systems biomedicine, consider the task of Active Learning (AL) for Optimal Experimental Design (OED) in calibrating a linear, dose-response relationship for a single transcription factor regulating a reporter gene. Let the measurement model be $y=\\theta x+\\varepsilon$, where $x$ is a controllable experimental design variable (for example, ligand concentration), $\\theta$ is an unknown sensitivity parameter, and $\\varepsilon$ is measurement noise. Assume $\\varepsilon\\sim\\mathcal{N}(0,\\sigma^{2})$ and the prior on $\\theta$ is $\\theta\\sim\\mathcal{N}(\\mu_{0},\\tau_{0}^{2})$. You perform one experiment at design $x$ and obtain one measurement $y$.\n\nStarting from Bayes' rule and the definitions of Gaussian likelihood and Gaussian prior, derive the posterior distribution $p(\\theta\\mid y,x)$ by explicitly completing the square in the exponent, and then compute the expected posterior variance after observing one measurement at design $x$, defined as $\\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big]$. Express your final answer as a closed-form analytic expression in terms of $x$, $\\sigma^{2}$, and $\\tau_{0}^{2}$. No numerical evaluation is required. The final answer must be a single expression with no units.",
            "solution": "### Derivation of the Posterior Distribution and Expected Posterior Variance\n\nThe objective is to find the expected posterior variance $\\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big]$. We begin by deriving the posterior distribution $p(\\theta\\mid y,x)$ using Bayes' rule.\n\nThe two components required for Bayes' rule are the likelihood of the data and the prior distribution of the parameter.\n\n$1$. **Likelihood**: The measurement model is $y = \\theta x + \\varepsilon$, where the noise term $\\varepsilon$ is drawn from a normal distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that for a given $\\theta$ and $x$, the measurement $y$ is also normally distributed. Specifically, $y \\mid \\theta, x \\sim \\mathcal{N}(\\theta x, \\sigma^2)$. The likelihood function $p(y \\mid \\theta, x)$ is therefore:\n$$ p(y \\mid \\theta, x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y - \\theta x)^2}{2\\sigma^2} \\right) $$\n\n$2$. **Prior**: The prior belief about the parameter $\\theta$ is given as a normal distribution, $\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$. The prior probability density function $p(\\theta)$ is:\n$$ p(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} \\right) $$\n\n$3$. **Posterior Distribution**: According to Bayes' rule, the posterior distribution $p(\\theta \\mid y, x)$ is proportional to the product of the likelihood and the prior:\n$$ p(\\theta \\mid y, x) \\propto p(y \\mid \\theta, x) p(\\theta) $$\nSubstituting the expressions for the likelihood and prior, and dropping the normalization constants which do not depend on $\\theta$:\n$$ p(\\theta \\mid y, x) \\propto \\exp\\left( -\\frac{(y - \\theta x)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} \\right) $$\nWe can combine the exponents:\n$$ p(\\theta \\mid y, x) \\propto \\exp\\left( -\\frac{(y - \\theta x)^2}{2\\sigma^2} - \\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} \\right) $$\nLet's focus on the argument of the exponential, which we will denote as $\\Phi$:\n$$ \\Phi = -\\frac{1}{2} \\left( \\frac{(y - \\theta x)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right) $$\nTo find the form of the posterior distribution, we complete the square with respect to $\\theta$. We expand the squared terms inside the parenthesis:\n$$ \\Phi = -\\frac{1}{2} \\left( \\frac{y^2 - 2y\\theta x + \\theta^2 x^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\theta\\mu_0 + \\mu_0^2}{\\tau_0^2} \\right) $$\nNow, we collect terms based on the powers of $\\theta$:\n$$ \\Phi = -\\frac{1}{2} \\left[ \\theta^2 \\left( \\frac{x^2}{\\sigma^2} + \\frac{1}{\\tau_0^2} \\right) - 2\\theta \\left( \\frac{yx}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) + \\left( \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} \\right) \\right] $$\nThe posterior distribution for $\\theta$ is known to be a Gaussian, as the prior and likelihood are conjugate. A general Gaussian density for $\\theta$ has an exponent of the form $-\\frac{(\\theta - \\mu_1)^2}{2\\tau_1^2}$, where $\\mu_1$ is the posterior mean and $\\tau_1^2$ is the posterior variance. Expanding this gives:\n$$ -\\frac{1}{2\\tau_1^2}(\\theta^2 - 2\\theta\\mu_1 + \\mu_1^2) = -\\frac{1}{2} \\left[ \\theta^2 \\left(\\frac{1}{\\tau_1^2}\\right) - 2\\theta \\left(\\frac{\\mu_1}{\\tau_1^2}\\right) + \\frac{\\mu_1^2}{\\tau_1^2} \\right] $$\nBy comparing the coefficient of the $\\theta^2$ term in our expression for $\\Phi$ with the general form, we can identify the reciprocal of the posterior variance, which is also known as the posterior precision.\n$$ \\frac{1}{\\tau_1^2} = \\frac{x^2}{\\sigma^2} + \\frac{1}{\\tau_0^2} $$\nThe posterior variance, $\\operatorname{Var}(\\theta \\mid y, x)$, is $\\tau_1^2$. We can solve for it:\n$$ \\frac{1}{\\tau_1^2} = \\frac{x^2\\tau_0^2 + \\sigma^2}{\\sigma^2\\tau_0^2} $$\n$$ \\tau_1^2 = \\operatorname{Var}(\\theta \\mid y, x) = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2} $$\nAlthough not required for the final answer, we could also find the posterior mean $\\mu_1$ by comparing the coefficients of the $\\theta$ term:\n$$ \\frac{\\mu_1}{\\tau_1^2} = \\frac{yx}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\implies \\mu_1 = \\tau_1^2 \\left( \\frac{yx}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) = \\frac{\\tau_0^2 yx + \\sigma^2 \\mu_0}{\\sigma^2 + x^2\\tau_0^2} $$\nSo, the posterior distribution is $p(\\theta \\mid y, x) = \\mathcal{N}(\\mu_1, \\tau_1^2)$.\n\n$4$. **Expected Posterior Variance**: The final step is to compute the expected posterior variance, $\\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big]$. The expectation is taken over the distribution of possible measurements $y$ for a given experimental design $x$. This distribution is the marginal likelihood or evidence, $p(y \\mid x) = \\int p(y \\mid \\theta, x)p(\\theta) d\\theta$.\n\nThe posterior variance we derived is:\n$$ \\operatorname{Var}(\\theta \\mid y, x) = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2} $$\nCrucially, we observe that this expression for the posterior variance depends only on the experimental design $x$, the measurement noise variance $\\sigma^2$, and the prior variance $\\tau_0^2$. It does **not** depend on the specific measurement value $y$.\n\nTherefore, when we take the expectation with respect to $y$, we are taking the expectation of a quantity that is constant with respect to $y$. For any random variable $Y$ and any constant $C$, the expectation is $\\mathbb{E}[C] = C$.\n$$ \\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big] = \\mathbb{E}_{y\\mid x}\\left[\\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2}\\right] $$\nSince the term inside the expectation is constant with respect to $y$, the expectation is simply the term itself:\n$$ \\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big] = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2} $$\nThis result signifies that for this linear-Gaussian model, the reduction in uncertainty about the parameter $\\theta$ can be calculated before the experiment is even performed. It depends only on where we choose to measure (the design $x$), not on the outcome of the measurement ($y$). This is a key principle used in optimal experimental design to choose $x$ such that this expected posterior variance is minimized.",
            "answer": "$$\n\\boxed{\\frac{\\sigma^{2}\\tau_{0}^{2}}{\\sigma^{2} + x^{2}\\tau_{0}^{2}}}\n$$"
        },
        {
            "introduction": "Scientific inquiry is often a sequential process where each new experiment builds upon the knowledge gained from previous ones. This exercise () introduces a powerful active learning strategy, Thompson sampling, within the context of a multi-armed bandit problem. You will simulate the decision-making process for choosing the next experiment to run, learning how to balance the 'exploration' of uncertain options with the 'exploitation' of those that currently seem most promising.",
            "id": "4313130",
            "problem": "In a systems biomedicine optimization task, you are designing follow-up experiments on three microenvironmental conditions to improve a binary assay’s success rate (success equals $1$, failure equals $0$). Each condition $i \\in \\{1,2,3\\}$ has an unknown success probability $\\theta_{i} \\in (0,1)$. You model outcomes as independent and identically distributed Bernoulli random variables with parameter $\\theta_{i}$ for condition $i$, and you place a Beta prior on each $\\theta_{i}$.\n\nFundamental base:\n- A Bernoulli likelihood for aggregated binary data under condition $i$ with $S_{i}$ successes and $F_{i}$ failures is $p(D_{i} \\mid \\theta_{i}) \\propto \\theta_{i}^{S_{i}} (1 - \\theta_{i})^{F_{i}}$.\n- A Beta prior satisfies $p(\\theta_{i}) \\propto \\theta_{i}^{\\alpha_{i}-1} (1 - \\theta_{i})^{\\beta_{i}-1}$.\n- Bayes’ rule gives $p(\\theta_{i} \\mid D_{i}) \\propto p(D_{i} \\mid \\theta_{i}) p(\\theta_{i})$.\n\nYou initialize each condition with the same prior $\\theta_{i} \\sim \\mathrm{Beta}(\\alpha_{i}, \\beta_{i})$ with $(\\alpha_{i}, \\beta_{i}) = (1,1)$. A pilot study yields the following observed counts:\n- Condition $1$: $S_{1} = 1$, $F_{1} = 0$.\n- Condition $2$: $S_{2} = 0$, $F_{2} = 1$.\n- Condition $3$: $S_{3} = 2$, $F_{3} = 0$.\n\nYou intend to choose the next condition to assay using Thompson sampling: independently draw one sample $\\tilde{\\theta}_{i}$ from the posterior of each $\\theta_{i}$ and select the condition whose sampled value is maximal. To make the sampling reproducible, you will use inverse transform sampling with independent uniform variates $u_{i} \\sim \\mathrm{Uniform}(0,1)$ supplied as:\n- $u_{1} = 0.36$,\n- $u_{2} = 0.49$,\n- $u_{3} = 0.20$.\n\nTasks:\n1. Using Bayes’ rule and the definitions above, derive the posterior distribution for each $\\theta_{i}$ after observing the pilot data.\n2. Using inverse transform sampling, compute the sampled values $\\tilde{\\theta}_{i}$ given the provided $u_{i}$ and the posterior distributions from Task $1$, exploiting the fact that when either the first or second Beta parameter equals $1$, the cumulative distribution function simplifies and admits a closed-form inverse.\n3. Under Thompson sampling, determine the index $i \\in \\{1,2,3\\}$ of the next condition to assay, i.e., the condition whose $\\tilde{\\theta}_{i}$ is largest.\n\nAnswer form requirement:\n- Report only the single index $i$ of the selected condition as your final answer. No rounding is necessary and no units are required.",
            "solution": "### Solution\n\n**Task 1: Derive the Posterior Distributions**\n\nThe Beta distribution is a conjugate prior for the Bernoulli likelihood. Given a prior distribution $\\theta \\sim \\mathrm{Beta}(\\alpha, \\beta)$ and data consisting of $S$ successes and $F$ failures, the posterior distribution is given by:\n$$\np(\\theta \\mid S, F) \\sim \\mathrm{Beta}(\\alpha + S, \\beta + F)\n$$\nThe initial prior for each condition $i \\in \\{1,2,3\\}$ is given as $\\theta_{i} \\sim \\mathrm{Beta}(\\alpha_{i}, \\beta_{i})$ where $\\alpha_{i}=1$ and $\\beta_{i}=1$. We update each of these priors with the corresponding pilot study data.\n\nFor Condition $1$:\n- Prior: $\\theta_{1} \\sim \\mathrm{Beta}(1, 1)$.\n- Data: $S_{1} = 1$, $F_{1} = 0$.\n- Posterior parameters: $\\alpha'_{1} = \\alpha_{1} + S_{1} = 1 + 1 = 2$, $\\beta'_{1} = \\beta_{1} + F_{1} = 1 + 0 = 1$.\n- Posterior distribution: $p(\\theta_{1} \\mid D_{1}) \\sim \\mathrm{Beta}(2, 1)$.\n\nFor Condition $2$:\n- Prior: $\\theta_{2} \\sim \\mathrm{Beta}(1, 1)$.\n- Data: $S_{2} = 0$, $F_{2} = 1$.\n- Posterior parameters: $\\alpha'_{2} = \\alpha_{2} + S_{2} = 1 + 0 = 1$, $\\beta'_{2} = \\beta_{2} + F_{2} = 1 + 1 = 2$.\n- Posterior distribution: $p(\\theta_{2} \\mid D_{2}) \\sim \\mathrm{Beta}(1, 2)$.\n\nFor Condition $3$:\n- Prior: $\\theta_{3} \\sim \\mathrm{Beta}(1, 1)$.\n- Data: $S_{3} = 2$, $F_{3} = 0$.\n- Posterior parameters: $\\alpha'_{3} = \\alpha_{3} + S_{3} = 1 + 2 = 3$, $\\beta'_{3} = \\beta_{3} + F_{3} = 1 + 0 = 1$.\n- Posterior distribution: $p(\\theta_{3} \\mid D_{3}) \\sim \\mathrm{Beta}(3, 1)$.\n\n**Task 2: Compute Sampled Values $\\tilde{\\theta}_{i}$**\n\nInverse transform sampling requires finding the inverse of the cumulative distribution function (CDF), $F^{-1}$. For a Beta distribution, the CDF is the regularized incomplete beta function, $F(x; \\alpha, \\beta) = I_x(\\alpha, \\beta)$. We need to solve $I_{\\tilde{\\theta}}(\\alpha, \\beta) = u$ for $\\tilde{\\theta}$. The problem provides a hint for special cases where one parameter is $1$.\n\nCase 1: $\\mathrm{Beta}(\\alpha, 1)$ distribution.\nThe PDF is $p(x;\\alpha,1) = \\alpha x^{\\alpha-1}$ for $x \\in (0,1)$.\nThe CDF is $F(x;\\alpha,1) = \\int_{0}^{x} \\alpha t^{\\alpha-1} dt = x^{\\alpha}$.\nSetting $F(\\tilde{\\theta}) = u$ gives $\\tilde{\\theta}^{\\alpha} = u$, so the inverse CDF is $F^{-1}(u) = u^{1/\\alpha}$.\n\nCase 2: $\\mathrm{Beta}(1, \\beta)$ distribution.\nThe PDF is $p(x;1, \\beta) = \\beta (1-x)^{\\beta-1}$ for $x \\in (0,1)$.\nThe CDF is $F(x;1,\\beta) = \\int_{0}^{x} \\beta(1-t)^{\\beta-1} dt = [-(1-t)^{\\beta}]_{0}^{x} = 1 - (1-x)^{\\beta}$.\nSetting $F(\\tilde{\\theta}) = u$ gives $1 - (1-\\tilde{\\theta})^{\\beta} = u$, so $(1-\\tilde{\\theta})^{\\beta} = 1-u$, which yields the inverse CDF $F^{-1}(u) = 1 - (1-u)^{1/\\beta}$.\n\nNow, we apply these formulas to our posteriors using the given uniform variates $u_{i}$.\n\nFor Condition $1$:\n- Posterior: $\\mathrm{Beta}(2, 1)$. This is Case 1 with $\\alpha'_{1} = 2$.\n- Uniform variate: $u_{1} = 0.36$.\n- Sampled value: $\\tilde{\\theta}_{1} = (u_{1})^{1/\\alpha'_{1}} = (0.36)^{1/2} = \\sqrt{0.36} = 0.6$.\n\nFor Condition $2$:\n- Posterior: $\\mathrm{Beta}(1, 2)$. This is Case 2 with $\\beta'_{2} = 2$.\n- Uniform variate: $u_{2} = 0.49$.\n- Sampled value: $\\tilde{\\theta}_{2} = 1 - (1-u_{2})^{1/\\beta'_{2}} = 1 - (1-0.49)^{1/2} = 1 - (0.51)^{1/2}$.\n\nFor Condition $3$:\n- Posterior: $\\mathrm{Beta}(3, 1)$. This is Case 1 with $\\alpha'_{3} = 3$.\n- Uniform variate: $u_{3} = 0.20$.\n- Sampled value: $\\tilde{\\theta}_{3} = (u_{3})^{1/\\alpha'_{3}} = (0.20)^{1/3} = \\sqrt[3]{0.20}$.\n\n**Task 3: Determine the Selected Condition**\n\nAccording to Thompson sampling, we select the condition $i$ that maximizes the sampled value $\\tilde{\\theta}_{i}$. We must compare the values of $\\tilde{\\theta}_{1}$, $\\tilde{\\theta}_{2}$, and $\\tilde{\\theta}_{3}$.\n\n- $\\tilde{\\theta}_{1} = 0.6$.\n- $\\tilde{\\theta}_{2} = 1 - \\sqrt{0.51}$.\n- $\\tilde{\\theta}_{3} = \\sqrt[3]{0.20}$.\n\nLet's compare $\\tilde{\\theta}_{1}$ with $\\tilde{\\theta}_{2}$ and $\\tilde{\\theta}_{3}$.\n\nComparison of $\\tilde{\\theta}_{1}$ and $\\tilde{\\theta}_{2}$:\nWe want to determine the sign of $\\tilde{\\theta}_{1} - \\tilde{\\theta}_{2} = 0.6 - (1 - \\sqrt{0.51}) = \\sqrt{0.51} - 0.4$.\nWe can compare $\\sqrt{0.51}$ and $0.4$ by squaring both positive numbers: $(\\sqrt{0.51})^2 = 0.51$ and $(0.4)^2 = 0.16$.\nSince $0.51 > 0.16$, it follows that $\\sqrt{0.51} > 0.4$.\nTherefore, $\\tilde{\\theta}_{1} - \\tilde{\\theta}_{2} > 0$, which means $\\tilde{\\theta}_{1} > \\tilde{\\theta}_{2}$.\n\nComparison of $\\tilde{\\theta}_{1}$ and $\\tilde{\\theta}_{3}$:\nWe want to compare $\\tilde{\\theta}_{1} = 0.6$ with $\\tilde{\\theta}_{3} = (0.20)^{1/3}$.\nWe can compare them by cubing both positive numbers: $(\\tilde{\\theta}_{1})^3 = (0.6)^3 = 0.216$ and $(\\tilde{\\theta}_{3})^3 = ((0.20)^{1/3})^3 = 0.20$.\nSince $0.216 > 0.20$, it follows that $0.6 > (0.20)^{1/3}$.\nTherefore, $\\tilde{\\theta}_{1} > \\tilde{\\theta}_{3}$.\n\nSince $\\tilde{\\theta}_{1}$ is greater than both $\\tilde{\\theta}_{2}$ and $\\tilde{\\theta}_{3}$, it is the maximum value among the three samples.\n$$\n\\tilde{\\theta}_{1} = \\max(\\tilde{\\theta}_{1}, \\tilde{\\theta}_{2}, \\tilde{\\theta}_{3})\n$$\nThus, under Thompson sampling, the next condition to assay is condition $1$. The index is $i=1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Beyond estimating parameters, a crucial role for active learning is to design experiments that can powerfully discriminate between competing mechanistic models. This computational practice () guides you through the process of finding the most informative experiments for telling two models apart, using the Kullback-Leibler divergence as a measure of information gain. By implementing this approach, you will gain hands-on experience with a sophisticated design criterion used at the forefront of systems biology to resolve scientific uncertainty.",
            "id": "4313184",
            "problem": "You are designing an algorithm for active learning in systems biomedicine to discriminate between two mechanistic candidate models of a scalar biomolecular response under a two-dimensional stimulus. Let the stimulus be a vector $x = (x_1, x_2)$ with $x_1 \\in [0,1]$ and $x_2 \\in [0,1]$, sampled from a discrete grid. For each candidate model $k \\in \\{1,2\\}$, the predictive distribution for the response $y$ under a given stimulus $x$ is assumed to be Gaussian (normal), i.e., $p_k(y \\mid x) = \\mathcal{N}(y \\,;\\, \\mu_k(x), \\sigma_k^2(x))$, where $\\mu_k(x)$ and $\\sigma_k^2(x)$ are the model-specific predictive mean and variance, respectively. You will implement a program that, for several test cases, selects a set of $K$ stimuli that maximizes the information for model discrimination under an independence assumption, quantified by the Kullback–Leibler (KL) divergence. The following foundational principles and definitions apply and must be used as the base of your derivation and algorithm:\n- The Kullback–Leibler (KL) divergence $D_{\\mathrm{KL}}(p \\,\\|\\, q)$ between two continuous distributions with densities $p(y)$ and $q(y)$ is defined as $D_{\\mathrm{KL}}(p \\,\\|\\, q) = \\int p(y) \\log \\frac{p(y)}{q(y)} \\, dy$.\n- The Gaussian (normal) density with mean $\\mu$ and variance $\\sigma^2$ is $f(y) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\!\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$.\n- Under independent and identically distributed (i.i.d.) experiments, information accumulated from separate measurements adds: if independent experiments at stimuli $x^{(1)}, \\ldots, x^{(K)}$ are performed, the total information for discriminating $p_1(\\cdot \\mid x)$ from $p_2(\\cdot \\mid x)$ is the sum $\\sum_{j=1}^{K} D_{\\mathrm{KL}}(p_1(\\cdot \\mid x^{(j)}) \\,\\|\\, p_2(\\cdot \\mid x^{(j)}))$.\n\nStimuli grid and indexing:\n- Define the candidate stimulus grid as the Cartesian product $X = X_1 \\times X_2$ where $X_1 = \\{\\, 0.0, 0.25, 0.5, 0.75, 1.0 \\,\\}$ and $X_2 = \\{\\, 0.0, 0.25, 0.5, 0.75, 1.0 \\,\\}$. Each element of $X$ is an ordered pair $(x_1, x_2)$. For computational purposes, assign a unique nonnegative integer index to each grid point via row-major ordering. Specifically, if $X_1$ is enumerated in ascending order as $x_1[0], x_1[1], \\ldots, x_1[n_1-1]$ with $n_1 = 5$ and $X_2$ similarly as $x_2[0], x_2[1], \\ldots, x_2[n_2-1]$ with $n_2 = 5$, then the linear index of $(x_1[i], x_2[j])$ is $I(i,j) = i \\cdot n_2 + j$, where $i \\in \\{0,1,2,3,4\\}$ and $j \\in \\{0,1,2,3,4\\}$.\n\nModel structure:\n- For $k \\in \\{1,2\\}$, define the mean function\n$$\n\\mu_k(x_1,x_2) \\;=\\; E_{\\max,k} \\left( h_k(x_1) + h_k(x_2) + \\beta_k \\, h_k(x_1) \\, h_k(x_2) \\right),\n$$\nwhere the Hill-type function $h_k(x)$ is $h_k(x) = \\frac{x}{K_{d,k} + x}$ with $K_{d,k} > 0$, and $E_{\\max,k} > 0$, $\\beta_k \\in \\mathbb{R}$. The variance functions $\\sigma_k^2(x_1,x_2)$ will be specified per test case as either constant or explicitly dependent on $(x_1,x_2)$.\n\nOptimization objective:\n- For each test case, select a subset $A \\subset X$ with $|A| = K$ that maximizes the sum of KL divergences from model $1$ to model $2$, i.e.,\n$$\n\\max_{A \\subset X, \\, |A| = K} \\;\\; \\sum_{x \\in A} D_{\\mathrm{KL}}\\!\\left( p_1(\\cdot \\mid x) \\,\\|\\, p_2(\\cdot \\mid x) \\right).\n$$\n- Ties and determinism: if multiple subsets achieve the same maximum objective value due to equal per-stimulus KL contributions, break ties by choosing the subset constructed by taking, in order of decreasing per-stimulus KL, the smallest available linear indices until the budget $K$ is exhausted. Report the final selected indices for each test case in ascending order.\n\nTest suite:\nImplement exactly the following four test cases. In all cases, use the fixed grid $X$ defined above and the linear index mapping $I(i,j) = i \\cdot 5 + j$.\n- Test case $1$ (general discrimination with equal variances): $E_{\\max,1} = 1.0$, $K_{d,1} = 0.3$, $\\beta_1 = 0.5$, and $\\sigma_1^2(x_1,x_2) = 0.02$; $E_{\\max,2} = 1.1$, $K_{d,2} = 0.5$, $\\beta_2 = 0.2$, and $\\sigma_2^2(x_1,x_2) = 0.02$; budget $K = 5$.\n- Test case $2$ (boundary condition $K=0$): identical to Test case $1$ in all parameters, but budget $K = 0$.\n- Test case $3$ (identical models, complete ties): $E_{\\max,1} = 0.8$, $K_{d,1} = 0.4$, $\\beta_1 = 0.3$, and $\\sigma_1^2(x_1,x_2) = 0.05$; $E_{\\max,2} = 0.8$, $K_{d,2} = 0.4$, $\\beta_2 = 0.3$, and $\\sigma_2^2(x_1,x_2) = 0.05$; budget $K = 3$.\n- Test case $4$ (variance-mismatch discrimination with equal means): $E_{\\max,1} = 1.0$, $K_{d,1} = 0.4$, $\\beta_1 = 0.3$, and $\\sigma_1^2(x_1,x_2) = 0.01$; $E_{\\max,2} = 1.0$, $K_{d,2} = 0.4$, $\\beta_2 = 0.3$, and $\\sigma_2^2(x_1,x_2) = 0.015 + 0.02 \\cdot \\frac{h_2(x_1) + h_2(x_2)}{2}$; budget $K = 4$.\n\nAngle units are not applicable. There are no physical units in this problem; all quantities are dimensionless. The answer for each test case must be a list of integers representing the selected linear indices as defined above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the four case-specific lists, enclosed in square brackets, with no spaces. For example, the syntactic structure should be $[\\,[i\\_1,i\\_2,\\ldots],\\,[\\ldots],\\,[\\ldots],\\,[\\ldots]\\,]$, where each inner bracket corresponds to one test case and contains the selected indices in ascending order.",
            "solution": "The problem asks to design an optimal experiment by selecting a subset of $K$ stimuli that maximizes the information gained for discriminating between two candidate models, $p_1$ and $p_2$. The information is quantified by the sum of Kullback-Leibler (KL) divergences over the selected stimuli.\n\nThe core of the solution involves three main steps:\n1.  Deriving the analytical expression for the KL divergence between the two model predictions.\n2.  Formulating an algorithm to select the $K$ stimuli that maximize the total KL divergence.\n3.  Applying this algorithm to the four specified test cases.\n\n**1. Kullback-Leibler Divergence for Gaussian Distributions**\nThe predictive distribution for each model $k \\in \\{1,2\\}$ at a stimulus $x$ is given as a Gaussian: $p_k(y \\mid x) = \\mathcal{N}(y \\,;\\, \\mu_k(x), \\sigma_k^2(x))$. We need to compute the KL divergence from model $1$ to model $2$, denoted $D_{\\mathrm{KL}}(p_1(\\cdot|x) \\,\\|\\, p_2(\\cdot|x))$. For notational simplicity, let's denote the distributions at a given $x$ as $p_1(y) = \\mathcal{N}(y \\,;\\, \\mu_1, \\sigma_1^2)$ and $p_2(y) = \\mathcal{N}(y \\,;\\, \\mu_2, \\sigma_2^2)$.\n\nThe definition of KL divergence is $D_{\\mathrm{KL}}(p_1 \\,\\|\\, p_2) = \\int p_1(y) \\log \\frac{p_1(y)}{p_2(y)} \\, dy$. The logarithm of a Gaussian density $\\mathcal{N}(y \\,;\\, \\mu, \\sigma^2)$ is $\\log \\mathcal{N}(y \\,;\\, \\mu, \\sigma^2) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y-\\mu)^2}{2\\sigma^2}$.\nSubstituting this into the KL divergence definition and evaluating the integral yields the well-known closed-form expression for two univariate Gaussian distributions:\n$$\nD_{\\mathrm{KL}}(p_1 \\,\\|\\, p_2) = \\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\nThis can be rewritten entirely in terms of variances $\\sigma_1^2$ and $\\sigma_2^2$:\n$$\nD_{\\mathrm{KL}}(p_1 \\,\\|\\, p_2) = \\frac{1}{2} \\left[ \\log\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) + \\frac{\\sigma_1^2}{\\sigma_2^2} + \\frac{(\\mu_1 - \\mu_2)^2}{\\sigma_2^2} - 1 \\right]\n$$\nThis formula will be used to compute the information gain at each stimulus point $x$ on the grid. The means $\\mu_k(x)$ and variances $\\sigma_k^2(x)$ are calculated for each $x$ according to the model definitions provided in the problem.\n\n**2. Optimization Algorithm**\nThe objective is to find a subset of stimuli $A \\subset X$ with cardinality $|A|=K$ that maximizes the total information:\n$$\n\\max_{A \\subset X, \\, |A| = K} \\;\\; \\sum_{x \\in A} D_{\\mathrm{KL}}\\!\\left( p_1(\\cdot \\mid x) \\,\\|\\, p_2(\\cdot \\mid x) \\right)\n$$\nSince the total information is a simple sum of the contributions from each stimulus point and the KL divergence is always non-negative ($D_{\\mathrm{KL}} \\ge 0$), the optimization problem reduces to a straightforward \"top-K\" selection problem. The optimal set $A$ will consist of the $K$ individual stimuli that have the highest KL divergence values.\n\nThe algorithm is as follows:\n1.  Generate all $25$ stimulus points $x=(x_1, x_2)$ from the Cartesian product of $X_1 = \\{0.0, 0.25, 0.5, 0.75, 1.0\\}$ and $X_2 = \\{0.0, 0.25, 0.5, 0.75, 1.0\\}$.\n2.  For each stimulus point $x$ and its corresponding linear index $I(x)$:\n    a. Calculate the Hill function values $h_k(x_1)$ and $h_k(x_2)$ for each model $k \\in \\{1,2\\}$.\n    b. Calculate the model means $\\mu_1(x)$ and $\\mu_2(x)$.\n    c. Determine the model variances $\\sigma_1^2(x)$ and $\\sigma_2^2(x)$ based on the test case specification.\n    d. Compute the KL divergence $D_{\\mathrm{KL}}(x)$ using the formula derived above.\n    e. Store the pair $(D_{\\mathrm{KL}}(x), I(x))$.\n3.  Sort the list of pairs. The primary sorting criterion is the KL divergence value in descending order. The secondary criterion, for tie-breaking, is the linear index in ascending order.\n4.  Select the first $K$ pairs from the sorted list.\n5.  Extract the linear indices from these $K$ pairs.\n6.  Sort these selected indices in ascending order for the final report, as required.\n\n**3. Application to Test Cases**\nThis algorithm is applied to each of the four test cases.\n-   For **Test Case 1**, variances are equal ($\\sigma_1^2 = \\sigma_2^2 = 0.02$). The KL divergence formula simplifies to $\\frac{(\\mu_1(x) - \\mu_2(x))^2}{2\\sigma_2^2}$, meaning selection is driven solely by the difference in mean predictions.\n-   For **Test Case 2**, the budget is $K=0$. The algorithm correctly returns an empty set of indices.\n-   For **Test Case 3**, the two models are identical ($p_1 = p_2$). This implies $\\mu_1(x)=\\mu_2(x)$ and $\\sigma_1^2(x)=\\sigma_2^2(x)$ for all $x$. The KL divergence is therefore $D_{\\mathrm{KL}}(x) = 0$ for all $x$. The selection is determined entirely by the tie-breaking rule, choosing the stimuli with the smallest linear indices, i.e., $\\{0, 1, \\dots, K-1\\}$.\n-   For **Test Case 4**, the mean functions are identical ($\\mu_1(x) = \\mu_2(x)$). The KL divergence simplifies to $\\frac{1}{2} \\left[ \\log\\left(\\frac{\\sigma_2^2(x)}{\\sigma_1^2}\\right) + \\frac{\\sigma_1^2}{\\sigma_2^2(x)} - 1 \\right]$. Discrimination is driven exclusively by the mismatch in predictive variances. Points where the ratio $\\sigma_2^2(x)/\\sigma_1^2$ is furthest from $1$ will be selected.\n\nThe accompanying Python code implements this logic systematically to produce the final results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the active learning problem for all test cases.\n    \"\"\"\n\n    def _calculate_optimal_stimuli(params):\n        \"\"\"\n        Calculates the optimal set of stimuli for a single test case.\n        \"\"\"\n        # Unpack parameters for the current test case\n        k1_params = params['k1']\n        k2_params = params['k2']\n        sigma1_sq_def = params['s1_sq']\n        sigma2_sq_def = params['s2_sq']\n        k_budget = params['K']\n\n        # Stimulus grid definition\n        x_coords = [0.0, 0.25, 0.5, 0.75, 1.0]\n        n_grid = len(x_coords)\n\n        # Handle the trivial case where the budget is zero\n        if k_budget == 0:\n            return []\n\n        # Define the Hill and mean functions as per the problem statement\n        def h_func(x, K_d):\n            return x / (K_d + x)\n\n        def mu_func(x1, x2, p):\n            h_sub = lambda val: h_func(val, p['K_d'])\n            h_x1 = h_sub(x1)\n            h_x2 = h_sub(x2)\n            return p['E_max'] * (h_x1 + h_x2 + p['beta'] * h_x1 * h_x2)\n\n        # Calculate KL divergence for each point on the grid\n        kl_values = []\n        for i in range(n_grid):\n            for j in range(n_grid):\n                x1 = x_coords[i]\n                x2 = x_coords[j]\n                index = i * n_grid + j\n\n                # Calculate means for both models\n                mu1 = mu_func(x1, x2, k1_params)\n                mu2 = mu_func(x1, x2, k2_params)\n\n                # Calculate variances for both models\n                if callable(sigma1_sq_def):\n                    h1 = lambda val: h_func(val, k1_params['K_d'])\n                    sigma1_sq = sigma1_sq_def(x1, x2, h1)\n                else:\n                    sigma1_sq = sigma1_sq_def\n\n                if callable(sigma2_sq_def):\n                    h2 = lambda val: h_func(val, k2_params['K_d'])\n                    sigma2_sq = sigma2_sq_def(x1, x2, h2)\n                else:\n                    sigma2_sq = sigma2_sq_def\n                \n                # Calculate KL Divergence D_KL(p1 || p2)\n                # Use np.isclose for robust float comparison in the identical model case\n                if np.isclose(sigma1_sq, sigma2_sq) and np.isclose(mu1, mu2):\n                    kl_div = 0.0\n                else:\n                    log_term = np.log(sigma2_sq / sigma1_sq)\n                    ratio_term = sigma1_sq / sigma2_sq\n                    mean_diff_term = ((mu1 - mu2)**2) / sigma2_sq\n                    kl_div = 0.5 * (log_term + ratio_term + mean_diff_term - 1.0)\n                \n                kl_values.append((kl_div, index))\n        \n        # Sort by KL divergence (descending) and then by index (ascending) for tie-breaking\n        kl_values.sort(key=lambda item: (-item[0], item[1]))\n        \n        # Select the top K indices based on the sorted list\n        selected_indices = [idx for kl, idx in kl_values[:k_budget]]\n        \n        # Sort the final selected indices in ascending order for the output\n        selected_indices.sort()\n        \n        return selected_indices\n\n    # Define the four test cases as specified in the problem\n    test_cases = [\n        # Test case 1\n        {\n            'k1': {'E_max': 1.0, 'K_d': 0.3, 'beta': 0.5},\n            'k2': {'E_max': 1.1, 'K_d': 0.5, 'beta': 0.2},\n            's1_sq': 0.02,\n            's2_sq': 0.02,\n            'K': 5,\n        },\n        # Test case 2\n        {\n            'k1': {'E_max': 1.0, 'K_d': 0.3, 'beta': 0.5},\n            'k2': {'E_max': 1.1, 'K_d': 0.5, 'beta': 0.2},\n            's1_sq': 0.02,\n            's2_sq': 0.02,\n            'K': 0,\n        },\n        # Test case 3\n        {\n            'k1': {'E_max': 0.8, 'K_d': 0.4, 'beta': 0.3},\n            'k2': {'E_max': 0.8, 'K_d': 0.4, 'beta': 0.3},\n            's1_sq': 0.05,\n            's2_sq': 0.05,\n            'K': 3,\n        },\n        # Test case 4\n        {\n            'k1': {'E_max': 1.0, 'K_d': 0.4, 'beta': 0.3},\n            'k2': {'E_max': 1.0, 'K_d': 0.4, 'beta': 0.3},\n            's1_sq': 0.01,\n            's2_sq': lambda x1, x2, h2: 0.015 + 0.01 * (h2(x1) + h2(x2)),\n            'K': 4,\n        }\n    ]\n\n    # Process all test cases and collect the results\n    all_results = []\n    for case in test_cases:\n        result = _calculate_optimal_stimuli(case)\n        all_results.append(result)\n\n    # Print the final results in the specified format\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}