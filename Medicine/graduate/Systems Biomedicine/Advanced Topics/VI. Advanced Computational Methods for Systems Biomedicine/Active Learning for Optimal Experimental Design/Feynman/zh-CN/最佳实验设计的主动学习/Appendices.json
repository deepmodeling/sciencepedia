{
    "hands_on_practices": [
        {
            "introduction": "本节的实践练习旨在通过具体的计算来巩固您的理论知识。我们将从一个基本但至关重要的练习开始：在一个简单的线性模型中，推导贝叶斯更新后的预期后验方差。这个练习将帮助您理解实验设计如何直接影响我们对模型参数不确定性的量化，这是所有最优实验设计（OED）问题的核心。",
            "id": "4313149",
            "problem": "在系统生物医学中，考虑在校准一个调节报告基因的单一转录因子的线性剂量-反应关系时，用于优化实验设计 (OED) 的主动学习 (AL) 任务。设测量模型为 $y=\\theta x+\\varepsilon$，其中 $x$ 是一个可控的实验设计变量（例如，配体浓度），$\\theta$ 是一个未知的敏感性参数，$\\varepsilon$ 是测量噪声。假设 $\\varepsilon\\sim\\mathcal{N}(0,\\sigma^{2})$ 且 $\\theta$ 的先验为 $\\theta\\sim\\mathcal{N}(\\mu_{0},\\tau_{0}^{2})$。您在设计 $x$ 处进行一次实验，并获得一个测量值 $y$。\n\n从贝叶斯法则以及高斯似然和高斯先验的定义出发，通过在指数上显式地配方法来推导后验分布 $p(\\theta\\mid y,x)$，然后计算在设计 $x$ 处观察到一次测量后的期望后验方差，定义为 $\\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big]$。将您的最终答案表示为一个关于 $x$、$\\sigma^{2}$ 和 $\\tau_{0}^{2}$ 的闭式解析表达式。无需进行数值计算。最终答案必须是一个无单位的单一表达式。",
            "solution": "用户要求推导线性模型中参数的期望后验方差，这是贝叶斯优化实验设计的核心任务。我将首先验证问题陈述。\n\n### 步骤 1：提取给定条件\n- 测量模型：$y = \\theta x + \\varepsilon$\n- 实验设计变量：$x$\n- 未知参数：$\\theta$\n- 测量噪声分布：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$\n- 参数的先验分布：$\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$\n- 任务：通过配方法推导后验分布 $p(\\theta \\mid y, x)$。\n- 任务：计算期望后验方差 $\\mathbb{E}_{y \\mid x}[\\operatorname{Var}(\\theta \\mid y, x)]$。\n- 最终答案格式：一个关于 $x$、$\\sigma^2$ 和 $\\tau_0^2$ 的闭式表达式。\n\n### 步骤 2：使用提取的给定条件进行验证\n该问题具有科学依据，提法恰当且客观。它呈现了一个涉及共轭高斯分布的标准贝叶斯推断场景，这是包括系统生物医学在内的许多科学学科中一个基础且广泛使用的模型。模型 $y = \\theta x + \\varepsilon$ 是一个基本的线性回归。为参数 $\\theta$ 使用高斯先验和为数据 $y$ 使用高斯似然是一个典型的例子。待计算的量，即期望后验方差，是优化实验设计领域的一个标准准则（与 A-最优性相关）。所有组成部分都定义清晰，没有矛盾、歧义或事实错误。该问题是一个有效、可解的数理统计问题。\n\n### 步骤 3：结论与行动\n问题有效。我将继续进行详细求解。\n\n### 后验分布和期望后验方差的推导\n\n目标是求期望后验方差 $\\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big]$。我们首先使用贝叶斯法则推导后验分布 $p(\\theta\\mid y,x)$。\n\n贝叶斯法则所需的两个组成部分是数据的似然和参数的先验分布。\n\n$1$. **似然**：测量模型为 $y = \\theta x + \\varepsilon$，其中噪声项 $\\varepsilon$ 从均值为 $0$、方差为 $\\sigma^2$ 的正态分布中抽取，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$。这意味着对于给定的 $\\theta$ 和 $x$，测量值 $y$ 也服从正态分布。具体来说，$y \\mid \\theta, x \\sim \\mathcal{N}(\\theta x, \\sigma^2)$。因此，似然函数 $p(y \\mid \\theta, x)$ 为：\n$$ p(y \\mid \\theta, x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y - \\theta x)^2}{2\\sigma^2} \\right) $$\n\n$2$. **先验**：关于参数 $\\theta$ 的先验信念被假定为一个正态分布，$\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$。先验概率密度函数 $p(\\theta)$ 为：\n$$ p(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} \\right) $$\n\n$3$. **后验分布**：根据贝叶斯法则，后验分布 $p(\\theta \\mid y, x)$ 与似然和先验的乘积成正比：\n$$ p(\\theta \\mid y, x) \\propto p(y \\mid \\theta, x) p(\\theta) $$\n代入似然和先验的表达式，并省略不依赖于 $\\theta$ 的归一化常数：\n$$ p(\\theta \\mid y, x) \\propto \\exp\\left( -\\frac{(y - \\theta x)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} \\right) $$\n我们可以合并指数部分：\n$$ p(\\theta \\mid y, x) \\propto \\exp\\left( -\\frac{(y - \\theta x)^2}{2\\sigma^2} - \\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} \\right) $$\n我们关注指数的参数，记为 $\\Phi$：\n$$ \\Phi = -\\frac{1}{2} \\left( \\frac{(y - \\theta x)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right) $$\n为了找到后验分布的形式，我们对 $\\theta$ 进行配方。我们展开括号内的平方项：\n$$ \\Phi = -\\frac{1}{2} \\left( \\frac{y^2 - 2y\\theta x + \\theta^2 x^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\theta\\mu_0 + \\mu_0^2}{\\tau_0^2} \\right) $$\n现在，我们根据 $\\theta$ 的幂次收集项：\n$$ \\Phi = -\\frac{1}{2} \\left[ \\theta^2 \\left( \\frac{x^2}{\\sigma^2} + \\frac{1}{\\tau_0^2} \\right) - 2\\theta \\left( \\frac{yx}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) + \\left( \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} \\right) \\right] $$\n已知 $\\theta$ 的后验分布是高斯分布，因为先验和似然是共轭的。$\\theta$ 的一般高斯密度具有 $-\\frac{(\\theta - \\mu_1)^2}{2\\tau_1^2}$ 形式的指数，其中 $\\mu_1$ 是后验均值，$\\tau_1^2$ 是后验方差。展开此式得到：\n$$ -\\frac{1}{2\\tau_1^2}(\\theta^2 - 2\\theta\\mu_1 + \\mu_1^2) = -\\frac{1}{2} \\left[ \\theta^2 \\left(\\frac{1}{\\tau_1^2}\\right) - 2\\theta \\left(\\frac{\\mu_1}{\\tau_1^2}\\right) + \\frac{\\mu_1^2}{\\tau_1^2} \\right] $$\n通过将我们 $\\Phi$ 表达式中 $\\theta^2$ 项的系数与一般形式进行比较，我们可以确定后验方差的倒数，也称为后验精度。\n$$ \\frac{1}{\\tau_1^2} = \\frac{x^2}{\\sigma^2} + \\frac{1}{\\tau_0^2} $$\n后验方差 $\\operatorname{Var}(\\theta \\mid y, x)$ 即为 $\\tau_1^2$。我们可以解出它：\n$$ \\frac{1}{\\tau_1^2} = \\frac{x^2\\tau_0^2 + \\sigma^2}{\\sigma^2\\tau_0^2} $$\n$$ \\tau_1^2 = \\operatorname{Var}(\\theta \\mid y, x) = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2} $$\n虽然最终答案不需要，我们也可以通过比较 $\\theta$ 项的系数来找到后验均值 $\\mu_1$：\n$$ \\frac{\\mu_1}{\\tau_1^2} = \\frac{yx}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\implies \\mu_1 = \\tau_1^2 \\left( \\frac{yx}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) = \\frac{\\tau_0^2 yx + \\sigma^2 \\mu_0}{\\sigma^2 + x^2\\tau_0^2} $$\n因此，后验分布为 $p(\\theta \\mid y, x) = \\mathcal{N}(\\mu_1, \\tau_1^2)$。\n\n$4$. **期望后验方差**：最后一步是计算期望后验方差 $\\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big]$。期望是针对给定实验设计 $x$ 的可能测量值 $y$ 的分布来计算的。这个分布是边际似然或证据，$p(y \\mid x) = \\int p(y \\mid \\theta, x)p(\\theta) d\\theta$。\n\n我们推导出的后验方差是：\n$$ \\operatorname{Var}(\\theta \\mid y, x) = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2} $$\n关键是，我们观察到这个后验方差的表达式仅依赖于实验设计 $x$、测量噪声方差 $\\sigma^2$ 和先验方差 $\\tau_0^2$。它并**不**依赖于具体的测量值 $y$。\n\n因此，当我们对 $y$ 取期望时，我们是在对一个相对于 $y$ 是常数的量取期望。对于任何随机变量 $Y$ 和任何常数 $C$，期望为 $\\mathbb{E}[C] = C$。\n$$ \\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big] = \\mathbb{E}_{y\\mid x}\\left[\\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2}\\right] $$\n由于期望内的项相对于 $y$ 是常数，所以期望就是该项本身：\n$$ \\mathbb{E}_{y\\mid x}\\big[\\operatorname{Var}(\\theta\\mid y,x)\\big] = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + x^2\\tau_0^2} $$\n这个结果表明，对于这个线性高斯模型，在实验进行之前就可以计算出参数 $\\theta$ 不确定性的减少量。它只取决于我们选择在哪里测量（设计 $x$），而不取决于测量的结果（$y$）。这是优化实验设计中用于选择 $x$ 以最小化此期望后验方差的一个关键原则。",
            "answer": "$$\n\\boxed{\\frac{\\sigma^{2}\\tau_{0}^{2}}{\\sigma^{2} + x^{2}\\tau_{0}^{2}}}\n$$"
        },
        {
            "introduction": "在掌握了量化不确定性的基本方法后，我们将应用这些概念于更复杂的动态系统中，这是系统生物学中的常见情景。本练习要求您分析一个简单的一阶动力学模型，推导其灵敏度矩阵，并探讨其与参数可辨识性的关系。最终，您将通过最大化费雪信息矩阵（Fisher Information Matrix, FIM）的行列式来确定一个D-最优的采样时间点，这展示了如何将理论与实际的实验设计决策联系起来。",
            "id": "4313143",
            "problem": "哺乳动物细胞系中的一个信号生物标志物被建模为一个具有恒定配体输注的单室过程。其状态动力学和测量模型由以下常微分方程和输出方程给出\n$$\n\\frac{dx(t)}{dt} \\;=\\; -\\theta_{1}\\,x(t) \\;+\\; \\theta_{2}\\,u(t), \\quad x(0)=0, \\quad y(t)=x(t),\n$$\n其中 $u(t)=U$ 是一个已知的恒定输注幅度，且 $U>0$，参数向量为 $\\theta = (\\theta_{1},\\theta_{2})$，且 $\\theta_{1}>0$ 和 $\\theta_{2}\\neq 0$。在两个采样时间 $t_{1}>0$ 和 $t_{2}>0$ 收集测量数据，每个 $y(t_{i})$ 都加入了方差为 $\\sigma^{2}$ 的独立同方差高斯噪声。定义敏感度矩阵，其元素为 $S_{ij}=\\partial y(t_{i};\\theta)/\\partial \\theta_{j}$，$i\\in\\{1,2\\}$，$j\\in\\{1,2\\}$。\n\n从模型方程以及非线性回归中敏感度和局部可辨识性的核心定义出发，完成以下任务：\n\n1. 求解 $y(t)$，并推导出 $\\partial y(t)/\\partial \\theta_{1}$ 和 $\\partial y(t)/\\partial \\theta_{2}$ 的精确表达式。\n\n2. 仅使用第一性原理，推导当 $(t_{1},t_{2},U,\\theta_{1},\\theta_{2})$ 满足何种条件时，$2\\times 2$ 敏感度矩阵 $S$ 具有满列秩。然后，解释在设计 $(t_{1},t_{2})$ 下 $S$ 的满列秩如何与线性化反问题意义下，在标称参数处 $\\theta$ 的局部结构可辨识性相关联。\n\n3. 在主动学习以进行最优实验设计的背景下，假设您选择的 $t_{2}$ 足够大，以至于 $t_{2}\\gg 1/\\theta_{1}$（即 $e^{-\\theta_{1}t_{2}}\\approx 0$）。对于这种双时间点设计，最大化高斯噪声下的费雪信息矩阵（FIM）的行列式，该矩阵定义为 $F=\\sigma^{-2} S^{\\top} S$。证明最大化 $\\det(F)$ 等价于最大化 $|\\det(S)|$，然后在 $t_{2}\\to\\infty$ 的极限情况下，确定能使 $|\\det(S)|$ 最大化的最优第一次采样时间 $t_{1}^{\\star}$ 的精确解析表达式（用 $\\theta_{1}$ 表示）。\n\n您的最终答案必须是 $t_{1}^{\\star}$ 的单一闭式解析表达式。不要四舍五入；提供精确表达式。如果您选择报告一个数值，请以秒为单位表示时间，但最终方框内的答案必须仅为符号表达式。",
            "solution": "经评估，用户提供的问题是有效的。它在科学上基于化学动力学和系统理论的原理，问题提法恰当且客观。该问题是动态系统中参数估计的敏感度分析和最优实验设计的一个标准练习。因此，我们可以进行完整解答。\n\n根据问题陈述的要求，分析分为三个部分。\n\n第1部分：常微分方程的求解和敏感度函数的推导。\n\n状态方程是一个带常数输入项的一阶线性常微分方程（ODE）：\n$$\n\\frac{dx(t)}{dt} = -\\theta_{1}x(t) + \\theta_{2}u(t)\n$$\n鉴于输入 $u(t)$ 是一个恒定的输注幅度 $U$，方程变为：\n$$\n\\frac{dx(t)}{dt} + \\theta_{1}x(t) = \\theta_{2}U\n$$\n这是一个标准形式，可以使用积分因子 $I(t) = \\exp(\\int \\theta_{1} dt) = \\exp(\\theta_{1}t)$ 求解。将常微分方程两边乘以 $I(t)$ 得到：\n$$\n\\exp(\\theta_{1}t)\\frac{dx(t)}{dt} + \\theta_{1}\\exp(\\theta_{1}t)x(t) = \\theta_{2}U\\exp(\\theta_{1}t)\n$$\n左侧是微分乘法法则的结果，即 $\\frac{d}{dt}[x(t)\\exp(\\theta_{1}t)]$。因此，\n$$\n\\frac{d}{dt}\\left[x(t)\\exp(\\theta_{1}t)\\right] = \\theta_{2}U\\exp(\\theta_{1}t)\n$$\n对两边关于 $t$ 积分得到：\n$$\nx(t)\\exp(\\theta_{1}t) = \\int \\theta_{2}U\\exp(\\theta_{1}t) dt = \\frac{\\theta_{2}U}{\\theta_{1}}\\exp(\\theta_{1}t) + C\n$$\n其中 $C$ 是积分常数。两边除以 $\\exp(\\theta_1 t)$ 得到 $x(t)$ 的通解：\n$$\nx(t) = \\frac{\\theta_{2}U}{\\theta_{1}} + C\\exp(-\\theta_{1}t)\n$$\n我们使用初始条件 $x(0)=0$ 来求 $C$：\n$$\nx(0) = 0 = \\frac{\\theta_{2}U}{\\theta_{1}} + C\\exp(0) \\implies C = -\\frac{\\theta_{2}U}{\\theta_{1}}\n$$\n将 $C$ 代回通解，我们得到 $x(t)$ 的特解，它等于测量函数 $y(t)$：\n$$\ny(t) = x(t) = \\frac{\\theta_{2}U}{\\theta_{1}} - \\frac{\\theta_{2}U}{\\theta_{1}}\\exp(-\\theta_{1}t) = \\frac{\\theta_{2}U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t))\n$$\n接下来，我们推导敏感度函数，它们是 $y(t)$ 关于参数 $\\theta_1$ 和 $\\theta_2$ 的偏导数。\n\n关于 $\\theta_1$ 的敏感度是 $\\frac{\\partial y(t)}{\\partial \\theta_{1}}$。使用乘法法则和链式法则：\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\frac{\\partial}{\\partial \\theta_{1}}\\left[ \\theta_{2}U \\left( \\frac{1 - \\exp(-\\theta_{1}t)}{\\theta_{1}} \\right) \\right]\n$$\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\theta_{2}U \\left[ \\frac{\\theta_{1}\\frac{\\partial}{\\partial\\theta_{1}}(1 - \\exp(-\\theta_{1}t)) - (1 - \\exp(-\\theta_{1}t))\\frac{\\partial}{\\partial\\theta_{1}}(\\theta_{1})}{\\theta_{1}^2} \\right]\n$$\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\theta_{2}U \\left[ \\frac{\\theta_{1}(t\\exp(-\\theta_{1}t)) - (1 - \\exp(-\\theta_{1}t))}{\\theta_{1}^2} \\right]\n$$\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\frac{\\theta_{2}U}{\\theta_{1}^2} \\left[ \\theta_{1}t\\exp(-\\theta_{1}t) - 1 + \\exp(-\\theta_{1}t) \\right]\n$$\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\frac{\\theta_{2}U}{\\theta_{1}^2} \\left[ -1 + (1 + \\theta_{1}t)\\exp(-\\theta_{1}t) \\right]\n$$\n关于 $\\theta_2$ 的敏感度是 $\\frac{\\partial y(t)}{\\partial \\theta_{2}}$：\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{2}} = \\frac{\\partial}{\\partial \\theta_{2}}\\left[ \\frac{\\theta_{2}U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t)) \\right] = \\frac{U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t))\n$$\n\n第2部分：敏感度矩阵满秩的条件及其与可辨识性的关系。\n\n在时间点 $t_1$ 和 $t_2$ 进行测量时的敏感度矩阵 $S$ 是一个 $2 \\times 2$ 矩阵：\n$$\nS = \\begin{pmatrix} \\frac{\\partial y(t_{1})}{\\partial \\theta_{1}}  \\frac{\\partial y(t_{1})}{\\partial \\theta_{2}} \\\\ \\frac{\\partial y(t_{2})}{\\partial \\theta_{1}}  \\frac{\\partial y(t_{2})}{\\partial \\theta_{2}} \\end{pmatrix}\n$$\n$S$ 具有满列秩当且仅当其列向量线性无关。对于方阵，这等价于矩阵可逆，即其行列式不为零，$\\det(S) \\neq 0$。如果一列是另一列的标量倍数，则列向量线性相关。如果行向量的分量之比相等，则行向量线性相关，写作：\n$$\n\\frac{\\partial y(t_{1})/\\partial \\theta_{1}}{\\partial y(t_{1})/\\partial \\theta_{2}} = \\frac{\\partial y(t_{2})/\\partial \\theta_{1}}{\\partial y(t_{2})/\\partial \\theta_{2}}\n$$\n此条件等价于 $\\det(S) = 0$。我们定义比率函数 $g(t) = \\frac{\\partial y(t)/\\partial \\theta_{1}}{\\partial y(t)/\\partial \\theta_{2}}$：\n$$\ng(t) = \\frac{\\frac{\\theta_{2}U}{\\theta_{1}^2} \\left[ -1 + (1 + \\theta_{1}t)\\exp(-\\theta_{1}t) \\right]}{\\frac{U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t))} = \\frac{\\theta_{2}}{\\theta_{1}} \\left( \\frac{-1 + (1 + \\theta_{1}t)\\exp(-\\theta_{1}t)}{1 - \\exp(-\\theta_{1}t)} \\right)\n$$\n矩阵 $S$ 满秩当且仅当 $g(t_1) \\neq g(t_2)$。如果 $g(t)$ 对于 $t>0$ 是一个严格单调函数，则此条件对于任何 $t_1 \\neq t_2$ 都成立。让我们分析 $g(t)$ 核心部分的单调性，令 $z = \\theta_{1}t > 0$：\n$$\nh(z) = \\frac{-1 + (1+z)\\exp(-z)}{1-\\exp(-z)}\n$$\n我们计算导数 $h'(z)$：\n$$\nh'(z) = \\frac{[-z\\exp(-z)](1-\\exp(-z)) - [-1 + (1+z)\\exp(-z)](\\exp(-z))}{(1-\\exp(-z))^2}\n$$\n$$\nh'(z) = \\frac{-z\\exp(-z) + z\\exp(-2z) + \\exp(-z) - (1+z)\\exp(-2z)}{(1-\\exp(-z))^2}\n$$\n$$\nh'(z) = \\frac{\\exp(-z)(1-z) - \\exp(-2z)}{(1-\\exp(-z))^2} = \\frac{\\exp(-z)(1-z-\\exp(-z))}{(1-\\exp(-z))^2}\n$$\n$h'(z)$ 的符号由项 $k(z) = 1-z-\\exp(-z)$ 决定。我们分析 $z>0$ 时的 $k(z)$。\n$k(0) = 1-0-1 = 0$。其导数为 $k'(z) = -1+\\exp(-z)$。对于 $z>0$，$\\exp(-z)<1$，所以 $k'(z)<0$。这意味着 $k(z)$ 在 $z>0$ 时是严格递减的。因为 $k(0)=0$，所以对于所有 $z>0$，$k(z)<0$。\n因此，对于所有 $z>0$，$h'(z)<0$。这证明了 $h(z)$ 是严格单调的。由于 $g(t)$ 是 $h(\\theta_1 t)$ 的一个缩放版本，所以 $g(t)$ 在 $t>0$ 时也是严格单调的。\n因此，$g(t_1) = g(t_2)$ 当且仅当 $t_1 = t_2$。为使 $S$ 满秩，我们要求 $t_1 \\neq t_2$。\n问题指定了 $t_1 > 0$，$t_2 > 0$，$U>0$，$\\theta_1>0$ 和 $\\theta_2 \\neq 0$。这些条件确保 $g(t)$ 中的分母不为零，且敏感度不恒为零。因此，满秩的条件是 $t_1 \\neq t_2$。\n\n与局部结构可辨识性的关系是直接的。在标称参数向量 $\\theta$ 处的局部结构可辨识性意味着，在 $\\theta$ 附近存在一个邻域，其中没有其他参数向量能产生相同的模型输出。在非线性回归的背景下，参数的变化 $\\Delta\\theta$ 会导致可观测量向量 $\\mathbf{y} = (y(t_1), y(t_2))^\\top$ 的一阶变化，由 $\\Delta\\mathbf{y} \\approx S \\Delta\\theta$ 给出。为了从观测到的变化 $\\Delta\\mathbf{y}$ 中唯一确定参数变化 $\\Delta\\theta$，必须能够求解这个关于 $\\Delta\\theta$ 的线性系统。唯一解存在的充要条件是矩阵 $S$ 具有满列秩。因此，对于给定的实验设计 $(t_1, t_2)$，敏感度矩阵的满列秩是使用该设计时参数 $\\theta$ 局部结构可辨识的充分必要条件。\n\n第3部分：最优实验设计。\n\n对于同方差高斯噪声，费雪信息矩阵（FIM）为 $F = \\frac{1}{\\sigma^2} S^{\\top}S$。我们希望在 $t_2 \\to \\infty$ 的极限下，关于 $t_1$ 的选择来最大化其行列式。\n使用行列式的性质，其中 $S$ 是一个 $2\\times 2$ 矩阵：\n$$\n\\det(F) = \\det\\left(\\frac{1}{\\sigma^2} S^{\\top}S\\right) = \\left(\\frac{1}{\\sigma^2}\\right)^2 \\det(S^{\\top}S) = \\frac{1}{\\sigma^4} \\det(S^{\\top})\\det(S) = \\frac{1}{\\sigma^4} (\\det(S))^2\n$$\n由于 $\\sigma^2$ 是一个正常数，最大化 $\\det(F)$ 等价于最大化 $(\\det(S))^2$，也就等价于最大化 $|\\det(S)|$。\n行列式为 $\\det(S) = \\frac{\\partial y(t_{1})}{\\partial \\theta_{1}}\\frac{\\partial y(t_{2})}{\\partial \\theta_{2}} - \\frac{\\partial y(t_{2})}{\\partial \\theta_{1}}\\frac{\\partial y(t_{1})}{\\partial \\theta_{2}}$。\n我们考察当 $t \\to \\infty$ 时敏感度函数的极限：\n$$\n\\lim_{t \\to \\infty} \\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\lim_{t \\to \\infty} \\frac{\\theta_{2}U}{\\theta_{1}^2} \\left[ -1 + (1 + \\theta_{1}t)\\exp(-\\theta_{1}t) \\right] = \\frac{\\theta_{2}U}{\\theta_{1}^2}(-1+0) = -\\frac{\\theta_{2}U}{\\theta_{1}^2}\n$$\n$$\n\\lim_{t \\to \\infty} \\frac{\\partial y(t)}{\\partial \\theta_{2}} = \\lim_{t \\to \\infty} \\frac{U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t)) = \\frac{U}{\\theta_{1}}(1-0) = \\frac{U}{\\theta_{1}}\n$$\n令 $D(t_1) = \\lim_{t_2 \\to \\infty} \\det(S)$。\n$$\nD(t_1) = \\frac{\\partial y(t_{1})}{\\partial \\theta_{1}} \\left(\\frac{U}{\\theta_{1}}\\right) - \\left(-\\frac{\\theta_{2}U}{\\theta_{1}^2}\\right) \\frac{\\partial y(t_{1})}{\\partial \\theta_{2}}\n$$\n代入在 $t_1$ 时的敏感度表达式：\n$$\nD(t_1) = \\left( \\frac{\\theta_{2}U}{\\theta_{1}^2} \\left[ -1 + (1 + \\theta_{1}t_1)\\exp(-\\theta_{1}t_1) \\right] \\right) \\left(\\frac{U}{\\theta_{1}}\\right) + \\left(\\frac{\\theta_{2}U}{\\theta_{1}^2}\\right) \\left( \\frac{U}{\\theta_{1}}(1 - \\exp(-\\theta_{1}t_1)) \\right)\n$$\n提出公因式 $\\frac{\\theta_{2}U^2}{\\theta_{1}^3}$：\n$$\nD(t_1) = \\frac{\\theta_{2}U^2}{\\theta_{1}^3} \\left( \\left[ -1 + (1 + \\theta_{1}t_1)\\exp(-\\theta_{1}t_1) \\right] + \\left[ 1 - \\exp(-\\theta_{1}t_1) \\right] \\right)\n$$\n$$\nD(t_1) = \\frac{\\theta_{2}U^2}{\\theta_{1}^3} \\left( -1 + \\exp(-\\theta_{1}t_1) + \\theta_{1}t_1\\exp(-\\theta_{1}t_1) + 1 - \\exp(-\\theta_{1}t_1) \\right)\n$$\n$$\nD(t_1) = \\frac{\\theta_{2}U^2}{\\theta_{1}^3} \\left( \\theta_{1}t_1\\exp(-\\theta_{1}t_1) \\right) = \\frac{\\theta_{2}U^2}{\\theta_{1}^2} t_1 \\exp(-\\theta_{1}t_1)\n$$\n为了最大化 $|D(t_1)|$，我们必须最大化函数 $f(t_1) = t_1 \\exp(-\\theta_{1}t_1)$（对于 $t_1 > 0$），因为前置因子 $\\frac{\\theta_{2}U^2}{\\theta_{1}^2}$ 是一个常数。我们通过将 $f(t_1)$ 的一阶导数设为零来找到最大值：\n$$\n\\frac{df(t_1)}{dt_1} = \\frac{d}{dt_1} \\left(t_1 \\exp(-\\theta_{1}t_1)\\right) = 1 \\cdot \\exp(-\\theta_{1}t_1) + t_1 \\cdot (-\\theta_1 \\exp(-\\theta_{1}t_1))\n$$\n$$\n\\frac{df(t_1)}{dt_1} = \\exp(-\\theta_{1}t_1) (1 - \\theta_{1}t_1)\n$$\n将导数设为零得到 $1 - \\theta_{1}t_1 = 0$，因为 $\\exp(-\\theta_{1}t_1) \\neq 0$。\n这得到了最优时间 $t_{1}^{\\star}$：\n$$\nt_{1}^{\\star} = \\frac{1}{\\theta_{1}}\n$$\n为了确认这是一个最大值，我们使用二阶导数检验：\n$$\n\\frac{d^2f(t_1)}{dt_1^2} = -\\theta_1 \\exp(-\\theta_{1}t_1)(1 - \\theta_{1}t_1) + \\exp(-\\theta_{1}t_1)(-\\theta_1) = -\\theta_1 \\exp(-\\theta_{1}t_1)(2 - \\theta_{1}t_1)\n$$\n在 $t_1 = 1/\\theta_1$ 处求值：\n$$\n\\left. \\frac{d^2f(t_1)}{dt_1^2} \\right|_{t_1=1/\\theta_1} = -\\theta_1 \\exp(-1)(2 - 1) = -\\frac{\\theta_1}{e}\n$$\n由于 $\\theta_1 > 0$，二阶导数为负，确认了这是一个局部最大值。由于 $f(0)=0$ 且 $\\lim_{t_1 \\to \\infty} f(t_1) = 0$，并且对于 $t_1>0$ 有 $f(t_1) > 0$，因此这个局部最大值就是全局最大值。最优的第一次采样时间是 $1/\\theta_{1}$。",
            "answer": "$$\\boxed{\\frac{1}{\\theta_1}}$$"
        },
        {
            "introduction": "最后的练习将带您进入主动学习的计算实践领域，解决一个核心挑战：如何设计实验来最有效地辨别多个竞争性模型。您将通过编写一个程序，在一个离散的刺激网格上计算每一点的KL散度（Kullback-Leibler divergence），从而选出最具信息量的实验点组合。这个练习不仅巩固了信息论在OED中的应用，还提供了将理论算法转化为可执行代码的宝贵经验。",
            "id": "4313184",
            "problem": "您正在为系统生物医学设计一种主动学习算法，用于在二维刺激下区分一个标量生物分子响应的两个候选机理模型。设刺激为一个向量 $x = (x_1, x_2)$，其中 $x_1 \\in [0,1]$ 且 $x_2 \\in [0,1]$，从一个离散网格中采样。对于每个候选模型 $k \\in \\{1,2\\}$，给定刺激 $x$ 下响应 $y$ 的预测分布假定为高斯（正态）分布，即 $p_k(y \\mid x) = \\mathcal{N}(y \\,;\\, \\mu_k(x), \\sigma_k^2(x))$，其中 $\\mu_k(x)$ 和 $\\sigma_k^2(x)$ 分别是特定于模型的预测均值和方差。您将实现一个程序，对于几个测试用例，该程序选择一个包含 $K$ 个刺激的集合，以在独立性假设下最大化用于模型区分的信息，该信息由 Kullback–Leibler (KL) 散度量化。以下基本原理和定义适用，并且必须作为您推导和算法的基础：\n- 两个具有密度 $p(y)$ 和 $q(y)$ 的连续分布之间的 Kullback–Leibler (KL) 散度 $D_{\\mathrm{KL}}(p \\,\\|\\, q)$ 定义为 $D_{\\mathrm{KL}}(p \\,\\|\\, q) = \\int p(y) \\log \\frac{p(y)}{q(y)} \\, dy$。\n- 均值为 $\\mu$、方差为 $\\sigma^2$ 的高斯（正态）密度为 $f(y) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\!\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$。\n- 在独立同分布 (i.i.d.) 实验下，从独立测量中累积的信息是可加的：如果在刺激 $x^{(1)}, \\ldots, x^{(K)}$ 处进行独立实验，则用于区分 $p_1(\\cdot \\mid x)$ 和 $p_2(\\cdot \\mid x)$ 的总信息是和 $\\sum_{j=1}^{K} D_{\\mathrm{KL}}(p_1(\\cdot \\mid x^{(j)}) \\,\\|\\, p_2(\\cdot \\mid x^{(j)}))$。\n\n刺激网格和索引：\n- 将候选刺激网格定义为笛卡尔积 $X = X_1 \\times X_2$，其中 $X_1 = \\{\\, 0.0, 0.25, 0.5, 0.75, 1.0 \\,\\}$ 且 $X_2 = \\{\\, 0.0, 0.25, 0.5, 0.75, 1.0 \\,\\}$。$X$ 的每个元素都是一个有序对 $(x_1, x_2)$。出于计算目的，通过行主序为每个网格点分配一个唯一的非负整数索引。具体来说，如果 $X_1$ 按升序枚举为 $x_1[0], x_1[1], \\ldots, x_1[n_1-1]$（其中 $n_1 = 5$），$X_2$ 同样枚举为 $x_2[0], x_2[1], \\ldots, x_2[n_2-1]$（其中 $n_2 = 5$），那么 $(x_1[i], x_2[j])$ 的线性索引为 $I(i,j) = i \\cdot n_2 + j$，其中 $i \\in \\{0,1,2,3,4\\}$ 且 $j \\in \\{0,1,2,3,4\\}$。\n\n模型结构：\n- 对于 $k \\in \\{1,2\\}$，定义均值函数\n$$\n\\mu_k(x_1,x_2) \\;=\\; E_{\\max,k} \\left( h_k(x_1) + h_k(x_2) + \\beta_k \\, h_k(x_1) \\, h_k(x_2) \\right),\n$$\n其中 Hill 型函数 $h_k(x)$ 为 $h_k(x) = \\frac{x}{K_{d,k} + x}$，其中 $K_{d,k} > 0$，$E_{\\max,k} > 0$，$\\beta_k \\in \\mathbb{R}$。方差函数 $\\sigma_k^2(x_1,x_2)$ 将根据每个测试用例指定，其形式为常数或明确依赖于 $(x_1,x_2)$。\n\n优化目标：\n- 对于每个测试用例，选择一个子集 $A \\subset X$ 且 $|A| = K$，该子集最大化从模型 1 到模型 2 的 KL 散度之和，即：\n$$\n\\max_{A \\subset X, \\, |A| = K} \\;\\; \\sum_{x \\in A} D_{\\mathrm{KL}}\\!\\left( p_1(\\cdot \\mid x) \\,\\|\\, p_2(\\cdot \\mid x) \\right).\n$$\n- 平局处理和确定性：如果由于每个刺激的 KL 贡献相等而导致多个子集达到相同的最大目标值，则通过按每个刺激 KL 值的降序选择可用的最小线性索引来打破平局，直到预算 $K$ 用尽。为每个测试用例按升序报告最终选定的索引。\n\n测试套件：\n精确实现以下四个测试用例。在所有情况下，均使用上面定义的固定网格 $X$ 和线性索引映射 $I(i,j) = i \\cdot 5 + j$。\n- 测试用例 1 (等方差下的一般区分)：$E_{\\max,1} = 1.0$, $K_{d,1} = 0.3$, $\\beta_1 = 0.5$，且 $\\sigma_1^2(x_1,x_2) = 0.02$；$E_{\\max,2} = 1.1$, $K_{d,2} = 0.5$, $\\beta_2 = 0.2$，且 $\\sigma_2^2(x_1,x_2) = 0.02$；预算 $K = 5$。\n- 测试用例 2 (边界条件 K=0)：所有参数与测试用例 1 相同，但预算 $K = 0$。\n- 测试用例 3 (相同模型，完全平局)：$E_{\\max,1} = 0.8$, $K_{d,1} = 0.4$, $\\beta_1 = 0.3$，且 $\\sigma_1^2(x_1,x_2) = 0.05$；$E_{\\max,2} = 0.8$, $K_{d,2} = 0.4$, $\\beta_2 = 0.3$，且 $\\sigma_2^2(x_1,x_2) = 0.05$；预算 $K = 3$。\n- 测试用例 4 (等均值下的方差不匹配区分)：$E_{\\max,1} = 1.0$, $K_{d,1} = 0.4$, $\\beta_1 = 0.3$，且 $\\sigma_1^2(x_1,x_2) = 0.01$；$E_{\\max,2} = 1.0$, $K_{d,2} = 0.4$, $\\beta_2 = 0.3$，且 $\\sigma_2^2(x_1,x_2) = 0.015 + 0.02 \\cdot \\frac{h_2(x_1) + h_2(x_2)}{2}$；预算 $K = 4$。\n\n角度单位不适用。此问题中没有物理单位；所有量均为无量纲。每个测试用例的答案必须是一个整数列表，表示如上定义的所选线性索引。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，格式为由四个特定于用例的列表组成的逗号分隔列表，并用方括号括起来，不含空格。例如，其语法结构应为 $[\\,[i\\_1,i\\_2,\\ldots],\\,[\\ldots],\\,[\\ldots],\\,[\\ldots]\\,]$，其中每个内层括号对应一个测试用例，并包含按升序排列的所选索引。",
            "solution": "经评估，用户提供的问题是有效的。它在科学上基于信息论和动力学建模，是适定的，具有明确的目标和约束，并以客观、数学精确的方式表述。未检测到与科学不合理、不完整性、矛盾或模糊性相关的缺陷。\n\n该问题要求通过选择一个包含 $K$ 个刺激的子集来设计一个最优实验，以最大化用于区分两个候选模型 $p_1$ 和 $p_2$ 的信息。该信息由所选刺激上的 Kullback-Leibler (KL) 散度之和来量化。\n\n解决方案的核心包括三个主要步骤：\n1.  推导两个模型预测之间 KL 散度的解析表达式。\n2.  构建一个算法来选择最大化总 KL 散度的 $K$ 个刺激。\n3.  将此算法应用于四个指定的测试用例。\n\n**1. 高斯分布的 Kullback-Leibler 散度**\n每个模型 $k \\in \\{1,2\\}$ 在刺激 $x$ 处的预测分布为一个高斯分布：$p_k(y \\mid x) = \\mathcal{N}(y \\,;\\, \\mu_k(x), \\sigma_k^2(x))$。我们需要计算从模型 1 到模型 2 的 KL 散度，记为 $D_{\\mathrm{KL}}(p_1(\\cdot|x) \\,\\|\\, p_2(\\cdot|x))$。为简化符号，我们将在给定 $x$ 处的分布表示为 $p_1(y) = \\mathcal{N}(y \\,;\\, \\mu_1, \\sigma_1^2)$ 和 $p_2(y) = \\mathcal{N}(y \\,;\\, \\mu_2, \\sigma_2^2)$。\n\nKL 散度的定义是 $D_{\\mathrm{KL}}(p_1 \\,\\|\\, p_2) = \\int p_1(y) \\log \\frac{p_1(y)}{p_2(y)} \\, dy$。高斯密度 $\\mathcal{N}(y \\,;\\, \\mu, \\sigma^2)$ 的对数为 $\\log \\mathcal{N}(y \\,;\\, \\mu, \\sigma^2) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y-\\mu)^2}{2\\sigma^2}$。将此代入 KL 散度定义并计算积分，可得出两个单变量高斯分布的众所周知的闭式表达式：\n$$\nD_{\\mathrm{KL}}(p_1 \\,\\|\\, p_2) = \\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\n这可以完全用方差 $\\sigma_1^2$ 和 $\\sigma_2^2$ 重写为：\n$$\nD_{\\mathrm{KL}}(p_1 \\,\\|\\, p_2) = \\frac{1}{2} \\left[ \\log\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) + \\frac{\\sigma_1^2}{\\sigma_2^2} + \\frac{(\\mu_1 - \\mu_2)^2}{\\sigma_2^2} - 1 \\right]\n$$\n这个公式将用于计算网格上每个刺激点 $x$ 处的信息增益。均值 $\\mu_k(x)$ 和方差 $\\sigma_k^2(x)$ 根据问题中提供的模型定义为每个 $x$ 计算。\n\n**2. 优化算法**\n目标是找到一个基数为 $|A|=K$ 的刺激子集 $A \\subset X$，以最大化总信息：\n$$\n\\max_{A \\subset X, \\, |A| = K} \\;\\; \\sum_{x \\in A} D_{\\mathrm{KL}}\\!\\left( p_1(\\cdot \\mid x) \\,\\|\\, p_2(\\cdot \\mid x) \\right)\n$$\n由于总信息是每个刺激点贡献的简单加和，且 KL 散度总是非负的（$D_{\\mathrm{KL}} \\ge 0$），因此优化问题简化为一个直接的“前 K 个”选择问题。最优集合 $A$ 将由具有最高 KL 散度值的 $K$ 个单个刺激组成。\n\n算法如下：\n1.  从 $X_1 = \\{0.0, 0.25, 0.5, 0.75, 1.0\\}$ 和 $X_2 = \\{0.0, 0.25, 0.5, 0.75, 1.0\\}$ 的笛卡尔积中生成所有 25 个刺激点 $x=(x_1, x_2)$。\n2.  对于每个刺激点 $x$ 及其对应的线性索引 $I(x)$：\n    a. 为每个模型 $k \\in \\{1,2\\}$ 计算 Hill 函数值 $h_k(x_1)$ 和 $h_k(x_2)$。\n    b. 计算模型均值 $\\mu_1(x)$ 和 $\\mu_2(x)$。\n    c. 根据测试用例的规范确定模型方差 $\\sigma_1^2(x)$ 和 $\\sigma_2^2(x)$。\n    d. 使用上面推导的公式计算 KL 散度 $D_{\\mathrm{KL}}(x)$。\n    e. 存储序对 $(D_{\\mathrm{KL}}(x), I(x))$。\n3.  对序对列表进行排序。主排序标准是 KL 散度值降序。次排序标准（用于打破平局）是线性索引升序。\n4.  从排序后的列表中选择前 $K$ 个序对。\n5.  从这 $K$ 个序对中提取线性索引。\n6.  按照要求，将这些选定的索引按升序排序以用于最终报告。\n\n**3. 应用于测试用例**\n此算法应用于四个测试用例中的每一个。\n-   对于**测试用例 1**，方差相等 ($\\sigma_1^2 = \\sigma_2^2 = 0.02$)。KL 散度公式简化为 $\\frac{(\\mu_1(x) - \\mu_2(x))^2}{2\\sigma_2^2}$，这意味着选择完全由均值预测的差异驱动。\n-   对于**测试用例 2**，预算为 $K=0$。算法正确地返回一个空的索引集。\n-   对于**测试用例 3**，两个模型相同 ($p_1 = p_2$)。这意味着对于所有 $x$，$\\mu_1(x)=\\mu_2(x)$ 且 $\\sigma_1^2(x)=\\sigma_2^2(x)$。因此，对于所有 $x$，KL 散度为 $D_{\\mathrm{KL}}(x) = 0$。选择完全由平局打破规则决定，即选择具有最小线性索引的刺激，即 $\\{0, 1, \\dots, K-1\\}$。\n-   对于**测试用例 4**，均值函数相同 ($\\mu_1(x) = \\mu_2(x)$)。KL 散度简化为 $\\frac{1}{2} \\left[ \\log\\left(\\frac{\\sigma_2^2(x)}{\\sigma_1^2}\\right) + \\frac{\\sigma_1^2}{\\sigma_2^2(x)} - 1 \\right]$。区分完全由预测方差的不匹配驱动。比率 $\\sigma_2^2(x)/\\sigma_1^2$ 离 1 最远的点将被选中。\n\n附带的 Python 代码系统地实现了这一逻辑以生成最终结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the active learning problem for all test cases.\n    \"\"\"\n\n    def _calculate_optimal_stimuli(params):\n        \"\"\"\n        Calculates the optimal set of stimuli for a single test case.\n        \"\"\"\n        # Unpack parameters for the current test case\n        k1_params = params['k1']\n        k2_params = params['k2']\n        sigma1_sq_def = params['s1_sq']\n        sigma2_sq_def = params['s2_sq']\n        k_budget = params['K']\n\n        # Stimulus grid definition\n        x_coords = [0.0, 0.25, 0.5, 0.75, 1.0]\n        n_grid = len(x_coords)\n\n        # Handle the trivial case where the budget is zero\n        if k_budget == 0:\n            return []\n\n        # Define the Hill and mean functions as per the problem statement\n        def h_func(x, K_d):\n            return x / (K_d + x) if K_d + x != 0 else 0\n\n        def mu_func(x1, x2, p):\n            h_sub = lambda val: h_func(val, p['K_d'])\n            h_x1 = h_sub(x1)\n            h_x2 = h_sub(x2)\n            return p['E_max'] * (h_x1 + h_x2 + p['beta'] * h_x1 * h_x2)\n\n        # Calculate KL divergence for each point on the grid\n        kl_values = []\n        for i in range(n_grid):\n            for j in range(n_grid):\n                x1 = x_coords[i]\n                x2 = x_coords[j]\n                index = i * n_grid + j\n\n                # Calculate means for both models\n                mu1 = mu_func(x1, x2, k1_params)\n                mu2 = mu_func(x1, x2, k2_params)\n\n                # Calculate variances for both models\n                if callable(sigma1_sq_def):\n                    h1 = lambda val: h_func(val, k1_params['K_d'])\n                    sigma1_sq = sigma1_sq_def(x1, x2, h1)\n                else:\n                    sigma1_sq = sigma1_sq_def\n\n                if callable(sigma2_sq_def):\n                    h2 = lambda val: h_func(val, k2_params['K_d'])\n                    sigma2_sq = sigma2_sq_def(x1, x2, h2)\n                else:\n                    sigma2_sq = sigma2_sq_def\n                \n                # Calculate KL Divergence D_KL(p1 || p2)\n                # Use np.isclose for robust float comparison in the identical model case\n                if np.isclose(sigma1_sq, sigma2_sq) and np.isclose(mu1, mu2):\n                    kl_div = 0.0\n                else:\n                    log_term = np.log(sigma2_sq / sigma1_sq)\n                    ratio_term = sigma1_sq / sigma2_sq\n                    mean_diff_term = ((mu1 - mu2)**2) / sigma2_sq\n                    kl_div = 0.5 * (log_term + ratio_term + mean_diff_term - 1.0)\n                \n                kl_values.append((kl_div, index))\n        \n        # Sort by KL divergence (descending) and then by index (ascending) for tie-breaking\n        kl_values.sort(key=lambda item: (-item[0], item[1]))\n        \n        # Select the top K indices based on the sorted list\n        selected_indices = [idx for kl, idx in kl_values[:k_budget]]\n        \n        # Sort the final selected indices in ascending order for the output\n        selected_indices.sort()\n        \n        return selected_indices\n\n    # Define the four test cases as specified in the problem\n    test_cases = [\n        # Test case 1\n        {\n            'k1': {'E_max': 1.0, 'K_d': 0.3, 'beta': 0.5},\n            'k2': {'E_max': 1.1, 'K_d': 0.5, 'beta': 0.2},\n            's1_sq': 0.02,\n            's2_sq': 0.02,\n            'K': 5,\n        },\n        # Test case 2\n        {\n            'k1': {'E_max': 1.0, 'K_d': 0.3, 'beta': 0.5},\n            'k2': {'E_max': 1.1, 'K_d': 0.5, 'beta': 0.2},\n            's1_sq': 0.02,\n            's2_sq': 0.02,\n            'K': 0,\n        },\n        # Test case 3\n        {\n            'k1': {'E_max': 0.8, 'K_d': 0.4, 'beta': 0.3},\n            'k2': {'E_max': 0.8, 'K_d': 0.4, 'beta': 0.3},\n            's1_sq': 0.05,\n            's2_sq': 0.05,\n            'K': 3,\n        },\n        # Test case 4\n        {\n            'k1': {'E_max': 1.0, 'K_d': 0.4, 'beta': 0.3},\n            'k2': {'E_max': 1.0, 'K_d': 0.4, 'beta': 0.3},\n            's1_sq': 0.01,\n            's2_sq': lambda x1, x2, h2: 0.015 + 0.01 * (h2(x1) + h2(x2)),\n            'K': 4,\n        }\n    ]\n\n    # Process all test cases and collect the results\n    all_results = []\n    for case in test_cases:\n        result = _calculate_optimal_stimuli(case)\n        all_results.append(result)\n\n    # Print the final results in the specified format\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}