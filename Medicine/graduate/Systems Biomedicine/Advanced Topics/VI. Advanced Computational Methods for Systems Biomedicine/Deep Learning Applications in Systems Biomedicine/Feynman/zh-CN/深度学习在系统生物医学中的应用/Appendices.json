{
    "hands_on_practices": [
        {
            "introduction": "在生物医学图像分析中，U-Net 等深度学习架构已成为细胞分割等任务的基石。然而，要有效设计和调试这些复杂的网络，必须对其内部工作原理有扎实的理解。此练习  提供了一个实践机会，让您逐层跟踪数据在典型 U-Net 架构中的流动，从而精确计算特征图的空间维度如何通过编码器和解码器的卷积、池化及上采样操作进行变换。掌握这项技能对于解决跳跃连接中的维度不匹配等常见问题以及为基于图块的分析选择最佳填充策略至关重要。",
            "id": "4332691",
            "problem": "在用于全切片组织病理学分析的系统生物医学流程中，使用 U-Net 家族的卷积编码器-解码器网络来分割多重免疫荧光图像中的细胞区室。考虑一个具有 $3$ 个下采样阶段和 $3$ 个上采样阶段的 $2$ 维 U-Net。我们只关心空间维度。输入图像的空间尺寸为 $(H, W)$，其中 $H$ 和 $W$ 是正整数。架构如下，并精确指定了空间操作：\n\n- 编码器阶段 $1$：两个卷积层，卷积核大小为 $3 \\times 3$，步幅为 $1$，扩张率为 $1$，零填充选择为在每次卷积时保持空间尺寸（“same”填充）；然后是一个最大池化层，池化核大小为 $2 \\times 2$，步幅为 $2$，零填充为 $0$。\n- 编码器阶段 $2$：两个卷积层，卷积核大小为 $3 \\times 3$，步幅为 $1$，扩张率为 $1$，采用“same”填充；然后是一个最大池化层，池化核大小为 $2 \\times 2$，步幅为 $2$，零填充为 $0$。\n- 编码器阶段 $3$：两个卷积层，卷积核大小为 $3 \\times 3$，步幅为 $1$，扩张率为 $1$，采用“same”填充；然后是一个最大池化层，池化核大小为 $2 \\times 2$，步幅为 $2$，零填充为 $0$。\n- 瓶颈层：两个卷积层，卷积核大小为 $3 \\times 3$，步幅为 $1$，扩张率为 $1$，采用“same”填充。\n- 解码器阶段 $3$：一个转置卷积（“上卷积”），卷积核大小为 $2 \\times 2$，步幅为 $2$，零填充为 $0$，输出填充为 $0$；与其两次“same”填充卷积之后、最大池化之前获取的编码器阶段 $3$ 特征图进行拼接（如果空间尺寸不完全匹配，则对跳跃连接的特征图使用对称中心裁剪）；然后是两个卷积层，卷积核大小为 $3 \\times 3$，步幅为 $1$，扩张率为 $1$，采用“same”填充。\n- 解码器阶段 $2$：一个转置卷积，卷积核大小为 $2 \\times 2$，步幅为 $2$，零填充为 $0$，输出填充为 $0$；与其两次“same”填充卷积之后、最大池化之前获取的编码器阶段 $2$ 特征图进行拼接（如果需要，使用对称中心裁剪）；然后是两个卷积层，卷积核大小为 $3 \\times 3$，步幅为 $1$，扩张率为 $1$，采用“same”填充。\n- 解码器阶段 $1$：一个转置卷积，卷积核大小为 $2 \\times 2$，步幅为 $2$，零填充为 $0$，输出填充为 $0$；与其两次“same”填充卷积之后、最大池化之前获取的编码器阶段 $1$ 特征图进行拼接（如果需要，使用对称中心裁剪）；然后是两个卷积层，卷积核大小为 $3 \\times 3$，步幅为 $1$，扩张率为 $1$，采用“same”填充。\n- 最终预测层：一个步幅为 $1$ 的 $1 \\times 1$ 卷积。\n\n假设所有操作都沿着 $H$ 和 $W$ 维度相同且独立地应用；通道数不影响空间尺寸，可以忽略。拼接操作不改变空间尺寸，并且仅在通过如上所述的对跳跃连接特征图进行对称裁剪以精确匹配空间尺寸后才执行。\n\n任务：\n- 仅使用离散卷积、步幅、填充和池化的基本定义，计算编码器和解码器中每个空间操作输出的空间尺寸，作为 $(H, W)$ 的函数，并说明在适用情况下舍入是如何产生的。明确指出任何需要对称裁剪跳跃连接的位置，并量化每个跳跃连接处的尺寸不匹配，将其表示为 $H$ 和 $W$ 的函数。\n- 根据上述架构和裁剪规则，推导最终分割图空间尺寸作为 $(H, W)$ 函数的闭式解析表达式。\n- 简要讨论卷积层中填充选择（“same”与“valid”）将如何改变逐层尺寸和最终输出尺寸，并论证哪种填充策略对于基于图块的组织病理学推断以减轻边界伪影更可取，从离散卷积和采样理论的第一性原理出发证明你的答案。\n\n将最终空间尺寸的闭式表达式以行矩阵的形式提供，分别包含高度和宽度的两个表达式。不需要单位。如果选择表示中间的舍入，请使用向下取整函数符号。最终答案框中不要包含任何文本。",
            "solution": "问题陈述定义明确，以深度学习架构原理为科学基础，且内部一致。它描述了一个具有特定层参数的标准 U-Net 架构，为计算和分析提供了清晰的基础。这些任务是客观的，并且可以使用所提供的信息解决。因此，该问题是有效的。\n\n我们将分析由指定的 U-Net 架构各层变换的空间维度，表示为 $(H, W)$。对高度维度 $H$ 和宽度维度 $W$ 的操作是相同且独立的。我们将推导 $H$ 的表达式，而 $W$ 的结果可依此类推。\n\n对于给定的输入尺寸 $S_{in}$、卷积核大小 $k$、步幅 $s$、填充 $p$ 和扩张率 $d$，输出尺寸 $S_{out}$ 的基本公式是：\n- 对于标准 2D 卷积：$S_{out} = \\lfloor \\frac{S_{in} + 2p - d(k-1) - 1}{s} \\rfloor + 1$。\n- 对于最大池化层：$S_{out} = \\lfloor \\frac{S_{in} + 2p - k}{s} \\rfloor + 1$。\n- 对于转置卷积：$S_{out} = (S_{in} - 1)s - 2p + k + p_{out}$，其中 $p_{out}$ 是输出填充。\n\n让我们将这些应用到指定的架构中。\n\n**1. 逐层空间尺寸计算**\n\n设输入图像尺寸为 $(H, W)$。令 $f_{down}(x) = \\lfloor x/2 \\rfloor$。\n\n**编码器路径**\n- **输入：** 初始空间尺寸为 $(H, W)$。\n\n- **编码器阶段 1：**\n  - 两个卷积层，卷积核大小 $k=3$，步幅 $s=1$，扩张率 $d=1$，以及“same”填充。“Same”填充被定义为保持空间尺寸。对于 $k=3, s=1, d=1$，这要求 $p=1$。这两次卷积后的尺寸保持为 $(H, W)$。该特征图（表示为 $S_1$）被传递给跳跃连接。\n  - 一个最大池化层，参数为 $k=2, s=2, p=0$。输出尺寸为 $S_{out} = \\lfloor \\frac{S_{in} - 2}{2} \\rfloor + 1 = \\lfloor S_{in}/2 - 1 \\rfloor + 1 = \\lfloor S_{in}/2 \\rfloor = f_{down}(S_{in})$。\n  - 阶段 1 的输出尺寸：$(f_{down}(H), f_{down}(W))$。\n\n- **编码器阶段 2：**\n  - 输入尺寸：$(f_{down}(H), f_{down}(W))$。\n  - 两次“same”填充的卷积保持尺寸不变。用于跳跃连接的特征图 $S_2$ 的尺寸为 $(f_{down}(H), f_{down}(W))$。\n  - 一个最大池化层减小尺寸。\n  - 阶段 2 的输出尺寸：$(f_{down}(f_{down}(H)), f_{down}(f_{down}(W)))$。\n\n- **编码器阶段 3：**\n  - 输入尺寸：$(f_{down}(f_{down}(H)), f_{down}(f_{down}(W)))$。\n  - 两次“same”填充的卷积保持尺寸不变。用于跳跃连接的特征图 $S_3$ 的尺寸为 $(f_{down}(f_{down}(H)), f_{down}(f_{down}(W)))$。\n  - 一个最大池化层减小尺寸。\n  - 阶段 3 的输出尺寸：$(f_{down}(f_{down}(f_{down}(H))), f_{down}(f_{down}(f_{down}(W))))$.\n\n**瓶颈层**\n- 输入尺寸：$(f_{down}(f_{down}(f_{down}(H))), f_{down}(f_{down}(f_{down}(W))))$.\n- 两次“same”填充的卷积保持尺寸不变。\n- 瓶颈层的输出尺寸：$(f_{down}(f_{down}(f_{down}(H))), f_{down}(f_{down}(f_{down}(W))))$. 让我们将其表示为 $(H_b, W_b)$。\n\n**解码器路径和跳跃连接尺寸不匹配**\n令 $f_{up}(x) = 2x$。\n\n- **解码器阶段 3：**\n  - 来自瓶颈层的输入：$(H_b, W_b) = (f_{down}(f_{down}(f_{down}(H))), f_{down}(f_{down}(f_{down}(W))))$.\n  - 一个转置卷积，参数为 $k=2, s=2, p=0, p_{out}=0$。输出尺寸为 $S_{out} = (S_{in}-1) \\cdot 2 - 0 + 2 + 0 = 2S_{in}$。尺寸变为 $(2H_b, 2W_b)$。我们称之为 $(H_{u3}, W_{u3})$。\n  - 跳跃连接 $S_3$ 的尺寸：$(f_{down}(f_{down}(H)), f_{down}(f_{down}(W)))$。\n  - **H 维度的不匹配：** 不匹配量为 $\\Delta H_3 = f_{down}(f_{down}(H)) - 2H_b = f_{down}(f_{down}(H)) - 2 f_{down}(f_{down}(f_{down}(H)))$。\n    这个差值是 $f_{down}(f_{down}(H)) \\pmod 2$，如果 $f_{down}(f_{down}(H))$ 是偶数，则为 $0$，如果是奇数，则为 $1$。如果此不匹配非零，则需要对 $S_3$ 进行对称裁剪。宽度不匹配 $\\Delta W_3$ 是类似的。\n  - 裁剪和拼接后，尺寸为 $(2H_b, 2W_b)$。\n  - 两次“same”卷积保持尺寸不变。\n  - 解码器阶段 3 的输出尺寸：$(H_{d3}, W_{d3}) = (2H_b, 2W_b)$。\n\n- **解码器阶段 2：**\n  - 来自解码器阶段 3 的输入：$(H_{d3}, W_{d3}) = (2H_b, 2W_b)$。\n  - 一个转置卷积将尺寸加倍至 $(2 H_{d3}, 2 W_{d3}) = (4H_b, 4W_b)$。我们称之为 $(H_{u2}, W_{u2})$。\n  - 跳跃连接 $S_2$ 的尺寸：$(f_{down}(H), f_{down}(W))$。\n  - **H 维度的不匹配：** $\\Delta H_2 = f_{down}(H) - 4H_b = f_{down}(H) - 4 f_{down}(f_{down}(f_{down}(H)))$。需要对 $S_2$ 进行对称裁剪以匹配尺寸 $(4H_b, 4W_b)$。\n  - 裁剪和拼接后，尺寸为 $(4H_b, 4W_b)$。\n  - 两次“same”卷积保持尺寸不变。\n  - 解码器阶段 2 的输出尺寸：$(H_{d2}, W_{d2}) = (4H_b, 4W_b)$。\n\n- **解码器阶段 1：**\n  - 来自解码器阶段 2 的输入：$(H_{d2}, W_{d2}) = (4H_b, 4W_b)$。\n  - 一个转置卷积将尺寸加倍至 $(2 H_{d2}, 2 W_{d2}) = (8H_b, 8W_b)$。我们称之为 $(H_{u1}, W_{u1})$。\n  - 跳跃连接 $S_1$ 的尺寸：$(H, W)$。\n  - **H 维度的不匹配：** $\\Delta H_1 = H - 8H_b = H - 8 f_{down}(f_{down}(f_{down}(H)))$。需要对 $S_1$ 进行对称裁剪以匹配尺寸 $(8H_b, 8W_b)$。\n  - 裁剪和拼接后，尺寸为 $(8H_b, 8W_b)$。\n  - 两次“same”卷积保持尺寸不变。\n  - 解码器阶段 1 的输出尺寸：$(H_{d1}, W_{d1}) = (8H_b, 8W_b)$。\n\n**最终预测层**\n- 输入尺寸：$(8H_b, 8W_b)$。\n- 步幅为 1 的 $1 \\times 1$ 卷积保持空间尺寸。\n- 最终输出尺寸：$(H_{out}, W_{out}) = (8H_b, 8W_b)$。\n\n**2. 最终尺寸的闭式表达式**\n\n最终输出高度由下式给出：\n$$ H_{out} = 8 H_b = 8 \\cdot f_{down}(f_{down}(f_{down}(H))) = 8 \\left\\lfloor \\frac{\\lfloor \\frac{\\lfloor H/2 \\rfloor}{2} \\rfloor}{2} \\right\\rfloor $$\n这个表达式可以简化。向下取整函数的一个性质是 $\\lfloor \\lfloor x/n \\rfloor / m \\rfloor = \\lfloor x/(nm) \\rfloor$。迭代应用此性质：\n$$ H_{out} = 8 \\left\\lfloor \\frac{\\lfloor H/4 \\rfloor}{2} \\right\\rfloor = 8 \\left\\lfloor \\frac{H}{8} \\right\\rfloor $$\n这个表达式计算小于或等于 $H$ 的 8 的最大倍数。\n同样的逻辑适用于宽度维度 $W$。因此，分割图的最终空间尺寸是：\n$$ (H_{out}, W_{out}) = \\left( 8 \\left\\lfloor \\frac{H}{8} \\right\\rfloor, 8 \\left\\lfloor \\frac{W}{8} \\right\\rfloor \\right) $$\n\n**3. 填充策略的讨论**\n\n卷积层中填充的选择会显著影响网络的行为，特别是对于基于图块的分析。\n\n- **“Same”填充（如指定）：** 使用“same”填充，每个编码器和解码器块内的卷积不会缩小特征图。尺寸减小仅发生在最大池化层。这简化了架构，但正如所示，导致了下采样路径（$S_{out} = \\lfloor S_{in}/2 \\rfloor$）和上采样路径（$S_{out} = 2S_{in}$）之间的不对称性。这种不对称性使得在拼接前必须裁剪来自跳跃连接的高分辨率特征图。最终输出的尺寸比输入小，除非输入维度是 $2^D$ 的倍数，其中 $D=3$ 是下采样阶段的数量。在处理图块时，输出图块有相同的维度问题。虽然如果输入尺寸被仔细选择（例如，8的倍数），这使得拼接在概念上看起来很简单，但每个输出图块边界处的预测本质上是不可靠的。这是因为这些输出像素的感受野延伸到了原始输入图块的零填充区域，而这是人为的数据。简单地拼接这些图块会在最终的全切片分割中产生一个充满伪影的网格状接缝。这将需要一个显式的后处理步骤来从每个输出图块中裁剪掉不可靠的边界。\n\n- **“Valid”填充：** 使用“valid”填充（$p=0$），每次卷积都会减小空间尺寸。对于一个 $3 \\times 3$ 的卷积核，每层的尺寸会减小 $2$（$S_{out} = S_{in} - 2$）。一个包含两次卷积的块会使尺寸减小 $4$。这会在跳跃连接处产生更大的尺寸差异，需要更大幅度的裁剪。网络的最终输出会比输入小得多。\n  在基于图块的组织病理学推断的背景下，这通常是更可取的策略。“重叠-图块”方法是减轻边界伪影的标准技术。从全切片图像中提取重叠的输入图块，通过网络处理每一个，然后在将有效的中心部分拼接在一起之前，丢弃每个输出预测图的不可靠边界区域。带有“valid”卷积的 U-Net 自然地实现了边界的丢弃。特征图的缩小意味着最终较小输出图中的每个像素都是基于完全包含在输入图块的有效（非填充）区域内的感受野计算出来的。从采样理论的角度来看，这确保了每个输出都仅是真实图像信号的函数，而不是人为填充的函数。这提高了结果的保真度并简化了拼接逻辑，因为网络架构本身定义了输出的“有效”区域，从而免除了在后处理中需要单独手动裁剪参数的需求。因此，为了从图块生成高质量、无缝的大图像分割，“valid”填充是更具原则性且更鲁棒的选择。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 8 \\left\\lfloor \\frac{H}{8} \\right\\rfloor & 8 \\left\\lfloor \\frac{W}{8} \\right\\rfloor \\end{pmatrix} } $$"
        },
        {
            "introduction": "深度学习在系统生物学中的强大能力不仅源于其预测能力，也源于其作为复杂概率模型的潜力，尤其是在基因组学领域。要真正掌握这些模型，我们必须从其统计基础入手。此练习  将引导您从泊松-伽马混合的第一性原理出发，推导出负二项分布的对数似然函数——这是对单细胞RNA测序（scRNA-seq）等计数数据进行建模的基石。通过计算该似然函数相对于模型参数的梯度，您将深入了解这些深度生成模型是如何通过梯度优化进行训练的。",
            "id": "4332692",
            "problem": "一个用于单细胞RNA测序（scRNA-seq）中唯一分子标识符（UMI）计数的深度生成模型，通常假设每个细胞中每个基因的计数来自负二项分布，其背后动机是一个具有伽马分布速率的泊松过程。考虑单个细胞中的单个基因，其观测到的UMI计数为 $y \\in \\{0,1,2,\\ldots\\}$，模型参数包括：由变分自编码器（VAE）的解码器产生的均值参数 $\\mu > 0$，以及一个控制过离散的逆离散参数 $\\theta > 0$。标准的混合构造是：抽取一个潜速率 $\\lambda \\sim \\operatorname{Gamma}(\\text{形状}=\\theta,\\ \\text{率}=\\theta/\\mu)$，然后采样观测计数 $y \\mid \\lambda \\sim \\operatorname{Poisson}(\\lambda)$。这种构造确保了 $\\mathbb{E}[\\lambda]=\\mu$，并得到 $y$ 的边际负二项分布。\n\n从泊松概率质量函数和伽马概率密度函数的定义出发，并仅使用经过充分检验的伽马函数积分恒等式，推导由该混合构造所隐含的UMI计数 $y$ 的边际对数似然 $\\log p(y \\mid \\mu,\\theta)$。然后，在保持 $\\theta$ 固定的情况下，计算梯度 $\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta)$。将您的最终答案表示为关于 $\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta)$ 的单个闭式表达式。无需四舍五入，也无物理单位。",
            "solution": "该问题要求推导一个服从泊松-伽马混合分布的计数变量 $y$ 的边际对数似然，并随后计算其关于均值参数 $\\mu$ 的梯度。\n\n该分层模型定义如下：\n$1$. 潜速率 $\\lambda$ 从一个形状参数为 $\\theta > 0$、率参数为 $\\theta/\\mu$（其中 $\\mu > 0$）的伽马分布中抽取。$\\lambda$ 的概率密度函数 (PDF) 由下式给出：\n$$p(\\lambda \\mid \\mu, \\theta) = \\frac{(\\theta/\\mu)^\\theta}{\\Gamma(\\theta)} \\lambda^{\\theta-1} \\exp\\left(-\\frac{\\theta}{\\mu}\\lambda\\right)$$\n对于 $\\lambda > 0$。其中，$\\Gamma(\\cdot)$ 表示伽马函数。\n\n$2$. 观测到的UMI计数 $y \\in \\{0, 1, 2, \\ldots\\}$ 从一个速率为 $\\lambda$ 的泊松分布中抽取。给定 $\\lambda$ 时，$y$ 的概率质量函数 (PMF) 为：\n$$p(y \\mid \\lambda) = \\frac{\\lambda^y \\exp(-\\lambda)}{y!}$$\n\n给定参数 $\\mu$ 和 $\\theta$，观测到计数 $y$ 的边际概率，记为 $p(y \\mid \\mu, \\theta)$，可以通过将联合概率 $p(y, \\lambda \\mid \\mu, \\theta) = p(y \\mid \\lambda) p(\\lambda \\mid \\mu, \\theta)$ 对潜变量 $\\lambda$ 的所有可能值进行积分得到。\n$$p(y \\mid \\mu, \\theta) = \\int_0^\\infty p(y \\mid \\lambda) p(\\lambda \\mid \\mu, \\theta) \\, d\\lambda$$\n代入泊松PMF和伽马PDF的表达式：\n$$p(y \\mid \\mu, \\theta) = \\int_0^\\infty \\left( \\frac{\\lambda^y \\exp(-\\lambda)}{y!} \\right) \\left( \\frac{(\\theta/\\mu)^\\theta}{\\Gamma(\\theta)} \\lambda^{\\theta-1} \\exp\\left(-\\frac{\\theta}{\\mu}\\lambda\\right) \\right) \\, d\\lambda$$\n我们可以通过将相对于 $\\lambda$ 的常数项移到积分外来重新整理各项：\n$$p(y \\mid \\mu, \\theta) = \\frac{(\\theta/\\mu)^\\theta}{y! \\Gamma(\\theta)} \\int_0^\\infty \\lambda^y \\lambda^{\\theta-1} \\exp(-\\lambda) \\exp\\left(-\\frac{\\theta}{\\mu}\\lambda\\right) \\, d\\lambda$$\n$$p(y \\mid \\mu, \\theta) = \\frac{\\theta^\\theta}{\\mu^\\theta y! \\Gamma(\\theta)} \\int_0^\\infty \\lambda^{y+\\theta-1} \\exp\\left(-\\left(1 + \\frac{\\theta}{\\mu}\\right)\\lambda\\right) \\, d\\lambda$$\n该积分具有伽马函数的形式。具体来说，我们使用恒等式 $\\int_0^\\infty x^{k-1} \\exp(-\\beta x) \\, dx = \\frac{\\Gamma(k)}{\\beta^k}$。在我们的表达式中，形状为 $k = y+\\theta$，率为 $\\beta = 1 + \\frac{\\theta}{\\mu} = \\frac{\\mu+\\theta}{\\mu}$。\n应用此恒等式，积分计算结果为：\n$$\\int_0^\\infty \\lambda^{y+\\theta-1} \\exp\\left(-\\left(\\frac{\\mu+\\theta}{\\mu}\\right)\\lambda\\right) \\, d\\lambda = \\frac{\\Gamma(y+\\theta)}{\\left(\\frac{\\mu+\\theta}{\\mu}\\right)^{y+\\theta}}$$\n将此结果代回到 $p(y \\mid \\mu, \\theta)$ 的表达式中：\n$$p(y \\mid \\mu, \\theta) = \\frac{\\theta^\\theta}{\\mu^\\theta y! \\Gamma(\\theta)} \\cdot \\frac{\\Gamma(y+\\theta)}{\\left(\\frac{\\mu+\\theta}{\\mu}\\right)^{y+\\theta}}$$\n我们可以简化这个表达式：\n$$p(y \\mid \\mu, \\theta) = \\frac{\\Gamma(y+\\theta)}{y! \\Gamma(\\theta)} \\frac{\\theta^\\theta}{\\mu^\\theta} \\frac{\\mu^{y+\\theta}}{(\\mu+\\theta)^{y+\\theta}}$$\n$$p(y \\mid \\mu, \\theta) = \\frac{\\Gamma(y+\\theta)}{y! \\Gamma(\\theta)} \\frac{\\theta^\\theta \\mu^y}{(\\mu+\\theta)^{y+\\theta}}$$\n这可以写成负二项分布PMF的常见形式：\n$$p(y \\mid \\mu, \\theta) = \\frac{\\Gamma(y+\\theta)}{y! \\Gamma(\\theta)} \\left(\\frac{\\theta}{\\mu+\\theta}\\right)^\\theta \\left(\\frac{\\mu}{\\mu+\\theta}\\right)^y$$\n\n下一步是求边际对数似然 $\\log p(y \\mid \\mu, \\theta)$。对上面推导出的表达式取自然对数：\n$$\\log p(y \\mid \\mu, \\theta) = \\log\\left( \\frac{\\Gamma(y+\\theta)}{y! \\Gamma(\\theta)} \\right) + \\log\\left( \\left(\\frac{\\theta}{\\mu+\\theta}\\right)^\\theta \\right) + \\log\\left( \\left(\\frac{\\mu}{\\mu+\\theta}\\right)^y \\right)$$\n使用对数性质 $\\log(a^b) = b\\log(a)$ 和 $\\log(a/b) = \\log(a) - \\log(b)$：\n$$\\log p(y \\mid \\mu, \\theta) = \\log(\\Gamma(y+\\theta)) - \\log(y!) - \\log(\\Gamma(\\theta)) + \\theta \\log\\left(\\frac{\\theta}{\\mu+\\theta}\\right) + y \\log\\left(\\frac{\\mu}{\\mu+\\theta}\\right)$$\n$$\\log p(y \\mid \\mu, \\theta) = \\log(\\Gamma(y+\\theta)) - \\log(y!) - \\log(\\Gamma(\\theta)) + \\theta(\\log\\theta - \\log(\\mu+\\theta)) + y(\\log\\mu - \\log(\\mu+\\theta))$$\n合并各项：\n$$\\log p(y \\mid \\mu, \\theta) = \\log(\\Gamma(y+\\theta)) - \\log(y!) - \\log(\\Gamma(\\theta)) + \\theta\\log\\theta + y\\log\\mu - (y+\\theta)\\log(\\mu+\\theta)$$\n\n最后，我们计算对数似然关于 $\\mu$ 的梯度，同时保持 $\\theta$ 固定。我们将 $\\log p(y \\mid \\mu, \\theta)$ 的表达式对 $\\mu$ 求导。$\\log(\\Gamma(y+\\theta))$、$\\log(y!)$、$\\log(\\Gamma(\\theta))$ 和 $\\theta\\log\\theta$ 这些项不依赖于 $\\mu$，因此它们的导数为0。\n$$\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta) = \\frac{\\partial}{\\partial \\mu} \\left[ y\\log\\mu - (y+\\theta)\\log(\\mu+\\theta) \\right]$$\n应用求导法则 $\\frac{d}{dx}\\log(x) = \\frac{1}{x}$ 和链式法则：\n$$\\frac{\\partial}{\\partial \\mu} (y\\log\\mu) = \\frac{y}{\\mu}$$\n$$\\frac{\\partial}{\\partial \\mu} (-(y+\\theta)\\log(\\mu+\\theta)) = -(y+\\theta) \\cdot \\frac{1}{\\mu+\\theta} \\cdot \\frac{\\partial}{\\partial \\mu}(\\mu+\\theta) = -(y+\\theta) \\cdot \\frac{1}{\\mu+\\theta} \\cdot 1 = -\\frac{y+\\theta}{\\mu+\\theta}$$\n将这些结果合并，得到梯度：\n$$\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta) = \\frac{y}{\\mu} - \\frac{y+\\theta}{\\mu+\\theta}$$\n为了将其表示为单个分数，我们找到公分母，即 $\\mu(\\mu+\\theta)$：\n$$\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta) = \\frac{y(\\mu+\\theta) - \\mu(y+\\theta)}{\\mu(\\mu+\\theta)}$$\n$$= \\frac{y\\mu + y\\theta - \\mu y - \\mu\\theta}{\\mu(\\mu+\\theta)}$$\n$$= \\frac{y\\theta - \\mu\\theta}{\\mu(\\mu+\\theta)}$$\n从分子中提出公因子 $\\theta$ 得到最终的闭式表达式：\n$$\\frac{\\partial}{\\partial \\mu}\\log p(y \\mid \\mu,\\theta) = \\frac{\\theta(y-\\mu)}{\\mu(\\mu+\\theta)}$$\n这个表达式表示模型的对数似然相对于其均值参数 $\\mu$ 的变化率，这个量对于VAE解码器的基于梯度的优化至关重要。",
            "answer": "$$\\boxed{\\frac{\\theta(y-\\mu)}{\\mu(\\mu+\\theta)}}$$"
        },
        {
            "introduction": "在医学等高风险领域，深度学习模型的预测结果往往不足以令人信服；理解模型“为什么”会做出某个特定预测同等重要。特征归因方法为打开神经网络这个“黑箱”提供了强有力的工具。此练习  旨在揭示“积分梯度”（Integrated Gradients）这一优雅方法的神秘面纱，它通过巧妙地运用微积分基本定理，将模型的总输出变化精确地分配给每个输入特征。通过完成这个推导和计算，您将获得对模型可解释性的基本理解，这对于验证模型可靠性和驱动新的科学发现至关重要。",
            "id": "4332698",
            "problem": "一个系统生物医学流程使用一个可微标量预测模型 $F:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$，将标准化分子特征（例如，转录本丰度或蛋白质标记物）的向量 $x\\in\\mathbb{R}^{n}$ 映射到一个疾病活动评分。为了解释特征的贡献，我们考虑积分梯度（Integrated Gradients, IG），该方法为一个代表健康稳态的基线 $x'\\in\\mathbb{R}^{n}$ 以及从 $x'$ 到 $x$ 的一条直线路径定义。仅从梯度定义、链式法则以及保守向量场的曲线积分基本定理（即，对于一个可微标量场 $F$，$\\nabla F$ 沿任意两点间平滑路径的积分等于 $F$ 在这两个端点值的差）出发，推导特征 $i$ 沿从 $x'$ 到 $x$ 的直线路径的归因的解析表达式。然后，将您的表达式特化为线性模型 $F(x)=w^{\\top}x+b$，并使用以下数值计算特征 $i=2$ 的归因值：\n- $w=\\left(0.92,\\,-0.68,\\,0.11,\\,0.35\\right)$，\n- $x=\\left(0.50,\\,1.25,\\,-0.30,\\,0.80\\right)$，\n- $x'=\\left(0.10,\\,0.15,\\,-0.30,\\,0.20\\right)$。\n提供特征 $i=2$ 归因的最终数值，该数值应为精确值，无需四舍五入。不要包含单位。",
            "solution": "本题要求从基本原理出发，为特征 $i$ 的积分梯度（IG）归因推导一个解析表达式。然后，必须将此表达式特化用于线性模型，并用其计算一个具体的数值。\n\n首先，我们推导归因的通用表达式。题目指出，我们应该使用保守向量场的曲线积分基本定理。对于一个可微标量场 $F:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$，其梯度 $\\nabla F$ 是一个保守向量场。该定理表明，$\\nabla F$ 沿从点 $x'$ 到点 $x$ 的任意平滑路径 $\\gamma$ 的积分，等于该标量场在两个端点值的差：\n$$F(x) - F(x') = \\int_{\\gamma} \\nabla F(\\mathbf{r}) \\cdot d\\mathbf{r}$$\n题目指定了一条从基线 $x'$ 到输入 $x$ 的直线路径。我们可以将此路径参数化为 $\\gamma(\\alpha) = x' + \\alpha(x - x')$，其中参数 $\\alpha \\in [0, 1]$。当 $\\alpha=0$ 时，我们有 $\\gamma(0) = x'$；当 $\\alpha=1$ 时，我们有 $\\gamma(1) = x' + (x - x') = x$。路径微分元为 $d\\mathbf{r} = \\frac{d\\gamma}{d\\alpha}d\\alpha = (x - x')d\\alpha$。\n\n将此参数化代入曲线积分，得到：\n$$F(x) - F(x') = \\int_{0}^{1} \\nabla F(\\gamma(\\alpha)) \\cdot \\frac{d\\gamma}{d\\alpha} d\\alpha$$\n$$F(x) - F(x') = \\int_{0}^{1} \\nabla F(x' + \\alpha(x - x')) \\cdot (x - x') d\\alpha$$\n梯度向量为 $\\nabla F(\\mathbf{r}) = \\left(\\frac{\\partial F}{\\partial x_1}, \\frac{\\partial F}{\\partial x_2}, \\dots, \\frac{\\partial F}{\\partial x_n}\\right)$，向量 $(x - x')$ 为 $(x_1 - x'_1, x_2 - x'_2, \\dots, x_n - x'_n)$。被积函数中的点积为：\n$$\\nabla F(x' + \\alpha(x - x')) \\cdot (x - x') = \\sum_{i=1}^{n} \\frac{\\partial F}{\\partial x_i}\\bigg|_{\\mathbf{r}=x' + \\alpha(x - x')} (x_i - x'_i)$$\n其中 $\\frac{\\partial F}{\\partial x_i}\\big|_{\\mathbf{r}=\\dots}$ 表示在路径上的点 $\\mathbf{r}$ 处计算的偏导数。\n\n将此和式代入表示 $F$ 总变化的积分表达式中：\n$$F(x) - F(x') = \\int_{0}^{1} \\sum_{i=1}^{n} \\frac{\\partial F}{\\partial x_i}\\bigg|_{x' + \\alpha(x - x')} (x_i - x'_i) d\\alpha$$\n由于和是有限的，我们可以交换求和与积分算子：\n$$F(x) - F(x') = \\sum_{i=1}^{n} \\int_{0}^{1} \\frac{\\partial F}{\\partial x_i}\\bigg|_{x' + \\alpha(x - x')} (x_i - x'_i) d\\alpha$$\n项 $(x_i - x'_i)$ 相对于积分变量 $\\alpha$ 是常数，因此可以移到积分符号外：\n$$F(x) - F(x') = \\sum_{i=1}^{n} (x_i - x'_i) \\int_{0}^{1} \\frac{\\partial F}{\\partial x_i}\\bigg|_{x' + \\alpha(x - x')} d\\alpha$$\n积分梯度框架将特征 $i$ 的归因（记为 $IG_i(x, x')$）定义为此求和中的第 $i$ 项。这确保了所有特征的归因之和等于总变化量 $F(x) - F(x')$。因此，分配给特征 $i$ 的归因的解析表达式为：\n$$IG_i(x, x') = (x_i - x'_i) \\int_{0}^{1} \\frac{\\partial F}{\\partial x_i}\\bigg|_{x' + \\alpha(x - x')} d\\alpha$$\n这就完成了问题的第一部分。\n\n接下来，我们将此表达式特化为线性模型 $F(x) = w^{\\top}x + b$，它可以写作 $F(x) = \\sum_{j=1}^{n} w_j x_j + b$。我们需要求 $F$ 相对于特征 $x_i$ 的偏导数：\n$$\\frac{\\partial F}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left(\\sum_{j=1}^{n} w_j x_j + b\\right) = w_i$$\n值得注意的是，对于线性模型，偏导数 $\\frac{\\partial F}{\\partial x_i}$ 是一个常数 $w_i$，不依赖于输入向量 $x$。因此，它在整个积分路径上的值是恒定的。\n\n将此代入通用的归因公式：\n$$IG_i(x, x') = (x_i - x'_i) \\int_{0}^{1} w_i d\\alpha$$\n常数 $w_i$ 在区间 $[0, 1]$ 上的积分为：\n$$\\int_{0}^{1} w_i d\\alpha = w_i [\\alpha]_{0}^{1} = w_i (1 - 0) = w_i$$\n因此，对于线性模型，特征 $i$ 的归因简化为：\n$$IG_i(x, x') = w_i (x_i - x'_i)$$\n\n最后，我们使用给定的数值计算特征 $i=2$ 的归因：\n- 权重向量：$w=\\left(0.92,\\,-0.68,\\,0.11,\\,0.35\\right)$，所以特征 2 的权重是 $w_2 = -0.68$。\n- 输入特征向量：$x=\\left(0.50,\\,1.25,\\,-0.30,\\,0.80\\right)$，所以特征 2 的值是 $x_2 = 1.25$。\n- 基线特征向量：$x'=\\left(0.10,\\,0.15,\\,-0.30,\\,0.20\\right)$，所以特征 2 的基线值是 $x'_2 = 0.15$。\n\n使用线性模型的特化公式：\n$$IG_2(x, x') = w_2 (x_2 - x'_2)$$\n代入数值：\n$$IG_2(x, x') = (-0.68) \\times (1.25 - 0.15)$$\n$$IG_2(x, x') = (-0.68) \\times (1.10)$$\n$$IG_2(x, x') = -0.748$$\n特征 $i=2$ 的归因值为 $-0.748$。",
            "answer": "$$\\boxed{-0.748}$$"
        }
    ]
}