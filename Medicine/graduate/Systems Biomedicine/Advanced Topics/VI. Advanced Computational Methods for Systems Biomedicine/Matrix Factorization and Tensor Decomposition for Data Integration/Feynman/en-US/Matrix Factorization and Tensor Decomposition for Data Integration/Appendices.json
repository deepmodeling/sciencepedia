{
    "hands_on_practices": [
        {
            "introduction": "Nonnegative Matrix Factorization (NMF) is a cornerstone technique for discovering parts-based representations in data, from identifying topics in text to molecular programs in genomics. This first practice goes to the heart of NMF by tasking you with deriving its most famous algorithm—the multiplicative update rules—from first principles . By working through the optimization using Karush-Kuhn-Tucker (KKT) conditions, you will gain a fundamental understanding of how these algorithms ensure nonnegativity while minimizing reconstruction error.",
            "id": "4360201",
            "problem": "You are integrating two omics modalities in a systems biomedicine study by approximating a nonnegative data matrix using Nonnegative Matrix Factorization (NMF). Let the integrated data matrix be $X \\in \\mathbb{R}_{\\ge 0}^{m \\times n}$, and seek a factorization $X \\approx W H$ with $W \\in \\mathbb{R}_{\\ge 0}^{m \\times r}$ and $H \\in \\mathbb{R}_{\\ge 0}^{r \\times n}$ under the squared Frobenius objective. Starting only from the definition of the squared Frobenius norm, standard rules of matrix calculus, and the Karush–Kuhn–Tucker (KKT) conditions for nonnegativity constraints:\n\n- Derive multiplicative update rules for $W$ and $H$ that monotonically decrease the objective while preserving nonnegativity.\n- State the conditions under which these multiplicative updates preserve nonnegativity and lead to a nonincreasing objective sequence, and identify the nature of the limiting points.\n\nThen, apply your derived $H$-update to a concrete toy example with $m = 3$, $n = 3$, and $r = 2$. Consider the following initialization and data:\n$$\nX \\;=\\; \\begin{pmatrix}\n5  2  3 \\\\\n4  1  2 \\\\\n3  2  4\n\\end{pmatrix},\\quad\nW^{(0)} \\;=\\; \\begin{pmatrix}\n1  2 \\\\\n2  1 \\\\\n1  1\n\\end{pmatrix},\\quad\nH^{(0)} \\;=\\; \\begin{pmatrix}\n1  1  1 \\\\\n1  1  1\n\\end{pmatrix}.\n$$\nAssume an alternating scheme that first updates $H$ by one full multiplicative step while holding $W$ fixed at $W^{(0)}$. Using your derived rule, compute the updated value of the entry $h_{1,2}^{(1)}$. Provide your final answer as an exact fraction (no rounding). The final answer must be a single real number with no units.",
            "solution": "The user wants me to act as a STEM professor and solve a problem on Nonnegative Matrix Factorization (NMF).\n\n### Step 1: Extract Givens\n-   Data matrix: $X \\in \\mathbb{R}_{\\ge 0}^{m \\times n}$\n-   Factorization: $X \\approx W H$\n-   Factor matrices with nonnegativity constraints: $W \\in \\mathbb{R}_{\\ge 0}^{m \\times r}$, $H \\in \\mathbb{R}_{\\ge 0}^{r \\times n}$\n-   Objective function: Minimize the squared Frobenius norm, $\\|X - WH\\|_F^2$.\n-   Tools for derivation: Definition of Frobenius norm, matrix calculus, and Karush–Kuhn–Tucker (KKT) conditions.\n-   Task 1: Derive multiplicative update rules for $W$ and $H$.\n-   Task 2: State conditions for nonnegativity preservation and monotonic decrease of the objective, and identify the nature of limiting points.\n-   Task 3: Apply the derived $H$-update rule for a single step.\n-   Toy example data:\n    -   $m = 3$, $n = 3$, $r = 2$\n    -   $X = \\begin{pmatrix} 5  2  3 \\\\ 4  1  2 \\\\ 3  2  4 \\end{pmatrix}$\n    -   $W^{(0)} = \\begin{pmatrix} 1  2 \\\\ 2  1 \\\\ 1  1 \\end{pmatrix}$\n    -   $H^{(0)} = \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\end{pmatrix}$\n-   Specific calculation required: Compute the updated entry $h_{1,2}^{(1)}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a standard, fundamental task in machine learning and computational biology. Nonnegative Matrix Factorization with a squared Frobenius norm objective is a well-established method. The derivation using KKT conditions and the resulting multiplicative updates are classic results in the field (popularized by Lee and Seung). The problem is scientifically and mathematically sound.\n2.  **Well-Posed**: The problem is clearly defined. It requests a standard derivation and a concrete calculation. The existence of the update rules is known, and the path to their derivation is specified. The numerical example is small and allows for a unique, exact answer.\n3.  **Objective**: The problem is stated in precise mathematical language, free of ambiguity or subjective claims.\n4.  **Completeness and Consistency**: All necessary matrices ($X$, $W^{(0)}$, $H^{(0)}$) and their dimensions are provided. The dimensions are consistent for matrix multiplication: an $m \\times r$ matrix ($3 \\times 2$) multiplied by an $r \\times n$ matrix ($2 \\times 3$) results in an $m \\times n$ matrix ($3 \\times 3$), matching the dimensions of $X$. All given matrices contain nonnegative entries as required. The problem is self-contained and consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of Multiplicative Update Rules\n\nThe problem is to find nonnegative matrices $W$ and $H$ that minimize the objective function $J$, defined as half the squared Frobenius norm of the approximation error:\n$$\nJ(W, H) = \\frac{1}{2} \\|X - WH\\|_F^2 = \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\left(X_{ij} - \\sum_{k=1}^{r} W_{ik}H_{kj}\\right)^2\n$$\nThe optimization is performed under the constraints $W_{ik} \\ge 0$ and $H_{kj} \\ge 0$ for all indices $i,j,k$. Since the objective function $J$ is not convex with respect to $W$ and $H$ simultaneously, we use an alternating least squares approach, where we optimize for one matrix while keeping the other fixed.\n\nThe objective function can be written using the trace operator as:\n$$\nJ(W, H) = \\frac{1}{2} \\operatorname{tr}\\left((X-WH)^{\\top}(X-WH)\\right) = \\frac{1}{2} \\operatorname{tr}\\left(X^{\\top} X - X^{\\top} WH - H^{\\top} W^{\\top} X + H^{\\top} W^{\\top} W H\\right)\n$$\nWe will use rules of matrix calculus, specifically $\\frac{\\partial}{\\partial A} \\operatorname{tr}(AB) = B^{\\top}$, $\\frac{\\partial}{\\partial A} \\operatorname{tr}(A^{\\top} B) = B$, and $\\frac{\\partial}{\\partial A} \\operatorname{tr}(A^{\\top} C A) = (C+C^{\\top})A$.\n\n**1. Update Rule for H**\n\nFixing $W$, the problem is to minimize $J(H)$ subject to $H \\ge 0$. This is a constrained convex optimization problem. We introduce a matrix of Lagrange multipliers $\\Lambda \\in \\mathbb{R}_{\\ge 0}^{r \\times n}$ for the nonnegativity constraint $H \\ge 0$. The Lagrangian is:\n$$\n\\mathcal{L}(H, \\Lambda) = J(H) - \\operatorname{tr}(\\Lambda^{\\top} H)\n$$\nThe Karush–Kuhn–Tucker (KKT) conditions for optimality are:\n-   Stationarity: $\\nabla_H \\mathcal{L} = \\nabla_H J(H) - \\Lambda = 0$\n-   Primal feasibility: $H_{kj} \\ge 0$\n-   Dual feasibility: $\\Lambda_{kj} \\ge 0$\n-   Complementary slackness: $\\Lambda_{kj} H_{kj} = 0$ for all $k,j$.\n\nFirst, we compute the gradient $\\nabla_H J(H)$. Using the trace formulation and matrix calculus rules:\n$$\n\\nabla_H J = \\frac{\\partial J}{\\partial H} = \\frac{1}{2} \\left( -W^{\\top} X - W^{\\top} X + (W^{\\top} W + (W^{\\top} W)^{\\top}) H \\right) = -W^{\\top} X + W^{\\top} W H\n$$\nThe KKT stationarity implies $\\Lambda = -W^{\\top} X + W^{\\top} W H$. The complementary slackness condition becomes:\n$$\n(-W^{\\top} X + W^{\\top} W H)_{kj} H_{kj} = 0\n$$\nThis is a fixed-point equation. The multiplicative update rule provides an iterative method to find a solution satisfying this condition. It is derived by considering a gradient descent step $H \\leftarrow H - \\eta_H \\nabla_H J$ with a smartly chosen, element-wise learning rate $\\eta_H$. Let the update for a single element be:\n$$\nH_{kj} \\leftarrow H_{kj} - (\\eta_H)_{kj} \\left( - (W^{\\top} X)_{kj} + (W^{\\top} W H)_{kj} \\right)\n$$\nChoosing the learning rate $(\\eta_H)_{kj} = \\frac{H_{kj}}{(W^{\\top} W H)_{kj}}$ yields:\n$$\nH_{kj} \\leftarrow H_{kj} - \\frac{H_{kj}}{(W^{\\top} W H)_{kj}} \\left( - (W^{\\top} X)_{kj} + (W^{\\top} W H)_{kj} \\right) = H_{kj} \\left( 1 + \\frac{(W^{\\top} X)_{kj}}{(W^{\\top} W H)_{kj}} - 1 \\right) = H_{kj} \\frac{(W^{\\top} X)_{kj}}{(W^{\\top} W H)_{kj}}\n$$\nIn matrix form, using $\\circ$ for the Hadamard (element-wise) product and element-wise division, the update rule for $H$ is:\n$$\nH \\leftarrow H \\circ \\frac{W^{\\top} X}{W^{\\top} W H}\n$$\n\n**2. Update Rule for W**\n\nBy symmetry, the update rule for $W$ is derived by fixing $H$ and minimizing $J(W)$ subject to $W \\ge 0$. This is equivalent to minimizing $\\|X^{\\top} - H^{\\top} W^{\\top}\\|_F^2$ for $W^{\\top} \\ge 0$. By analogy with the derivation for $H$, we can replace $X$ with $X^{\\top}$, $W$ with $H^{\\top}$, and $H$ with $W^{\\top}$.\nThe update rule for $W^{\\top}$ is:\n$$\nW^{\\top} \\leftarrow W^{\\top} \\circ \\frac{(H^{\\top})^{\\top} X^{\\top}}{(H^{\\top})^{\\top} H^{\\top} W^{\\top}} = W^{\\top} \\circ \\frac{H X^{\\top}}{H H^{\\top} W^{\\top}}\n$$\nTransposing this equation gives the update rule for $W$:\n$$\nW \\leftarrow \\left( W^{\\top} \\circ \\frac{H X^{\\top}}{H H^{\\top} W^{\\top}} \\right)^{\\top} = W \\circ \\left( \\frac{H X^{\\top}}{H H^{\\top} W^{\\top}} \\right)^{\\top} = W \\circ \\frac{(H X^{\\top})^{\\top}}{(H H^{\\top} W^{\\top})^{\\top}} = W \\circ \\frac{X H^{\\top}}{W H H^{\\top}}\n$$\n\n### Conditions and Convergence\n\n-   **Nonnegativity Preservation**: If the initial matrices $W^{(0)}, H^{(0)}$ and the data matrix $X$ are strictly positive, all terms in the numerators ($W^{\\top} X$, $X H^{\\top}$) and denominators ($W^{\\top} W H$, $W H H^{\\top}$) of the update rules will be strictly positive. The updates involve only element-wise multiplication and division of positive matrices, so $W$ and $H$ remain positive. If some entries are zero, the corresponding denominator entries could become zero. In practice, to ensure numerical stability and strict positivity, a small positive constant $\\epsilon$ is often added to the denominator.\n-   **Nonincreasing Objective**: The objective function $J(W,H) = \\frac{1}{2}\\|X-WH\\|_F^2$ is nonincreasing under each of these update rules. This can be rigorously proven by constructing an auxiliary function for the objective, showing that each update step minimizes this auxiliary function, which provides an upper bound for the original objective. Since the objective function is bounded below by $0$, the sequence of objective values generated by the alternating updates must converge.\n-   **Nature of Limiting Points**: The algorithm is guaranteed to converge to a stationary point of the objective function, i.e., a point $(W^*, H^*)$ that satisfies the KKT conditions. However, because $J(W,H)$ is not jointly convex in $W$ and $H$, this stationary point is not guaranteed to be a global minimum. The algorithm may converge to a local minimum or a saddle point, and the specific limiting point depends on the initialization.\n\n### Application to the Toy Example\n\nWe are asked to compute the updated entry $h_{1,2}^{(1)}$ using the derived update rule for $H$:\n$$\nH^{(1)} = H^{(0)} \\circ \\frac{(W^{(0)})^{\\top} X}{(W^{(0)})^{\\top} W^{(0)} H^{(0)}}\n$$\nThe value of a specific entry $h_{kj}^{(1)}$ is given by:\n$$\nh_{kj}^{(1)} = h_{kj}^{(0)} \\times \\frac{((W^{(0)})^{\\top} X)_{kj}}{((W^{(0)})^{\\top} W^{(0)} H^{(0)})_{kj}}\n$$\nWe need to compute this for $k=1, j=2$. The given matrices are:\n$$\nX \\;=\\; \\begin{pmatrix} 5  2  3 \\\\ 4  1  2 \\\\ 3  2  4 \\end{pmatrix},\\quad\nW^{(0)} \\;=\\; \\begin{pmatrix} 1  2 \\\\ 2  1 \\\\ 1  1 \\end{pmatrix},\\quad\nH^{(0)} \\;=\\; \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\end{pmatrix}.\n$$\n\nFirst, we compute the numerator term $(W^{(0)})^{\\top} X$:\n$$\n(W^{(0)})^{\\top} X = \\begin{pmatrix} 1  2  1 \\\\ 2  1  1 \\end{pmatrix} \\begin{pmatrix} 5  2  3 \\\\ 4  1  2 \\\\ 3  2  4 \\end{pmatrix} = \\begin{pmatrix} 1\\cdot5+2\\cdot4+1\\cdot3  1\\cdot2+2\\cdot1+1\\cdot2  1\\cdot3+2\\cdot2+1\\cdot4 \\\\ 2\\cdot5+1\\cdot4+1\\cdot3  2\\cdot2+1\\cdot1+1\\cdot2  2\\cdot3+1\\cdot2+1\\cdot4 \\end{pmatrix}\n$$\n$$\n(W^{(0)})^{\\top} X = \\begin{pmatrix} 5+8+3  2+2+2  3+4+4 \\\\ 10+4+3  4+1+2  6+2+4 \\end{pmatrix} = \\begin{pmatrix} 16  6  11 \\\\ 17  7  12 \\end{pmatrix}\n$$\nThe $(1,2)$ entry of this matrix is $((W^{(0)})^{\\top} X)_{1,2} = 6$.\n\nNext, we compute the denominator term $(W^{(0)})^{\\top} W^{(0)} H^{(0)}$. First, we calculate $(W^{(0)})^{\\top} W^{(0)}$:\n$$\n(W^{(0)})^{\\top} W^{(0)} = \\begin{pmatrix} 1  2  1 \\\\ 2  1  1 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 2  1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 1\\cdot1+2\\cdot2+1\\cdot1  1\\cdot2+2\\cdot1+1\\cdot1 \\\\ 2\\cdot1+1\\cdot2+1\\cdot1  2\\cdot2+1\\cdot1+1\\cdot1 \\end{pmatrix} = \\begin{pmatrix} 1+4+1  2+2+1 \\\\ 2+2+1  4+1+1 \\end{pmatrix} = \\begin{pmatrix} 6  5 \\\\ 5  6 \\end{pmatrix}\n$$\nNow, we multiply this by $H^{(0)}$:\n$$\n(W^{(0)})^{\\top} W^{(0)} H^{(0)} = \\begin{pmatrix} 6  5 \\\\ 5  6 \\end{pmatrix} \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\end{pmatrix} = \\begin{pmatrix} 6\\cdot1+5\\cdot1  6\\cdot1+5\\cdot1  6\\cdot1+5\\cdot1 \\\\ 5\\cdot1+6\\cdot1  5\\cdot1+6\\cdot1  5\\cdot1+6\\cdot1 \\end{pmatrix} = \\begin{pmatrix} 11  11  11 \\\\ 11  11  11 \\end{pmatrix}\n$$\nThe $(1,2)$ entry of this matrix is $((W^{(0)})^{\\top} W^{(0)} H^{(0)})_{1,2} = 11$.\n\nFinally, we find $h_{1,2}^{(1)}$. From $H^{(0)}$, we have $h_{1,2}^{(0)} = 1$.\n$$\nh_{1,2}^{(1)} = h_{1,2}^{(0)} \\times \\frac{((W^{(0)})^{\\top} X)_{1,2}}{((W^{(0)})^{\\top} W^{(0)} H^{(0)})_{1,2}} = 1 \\times \\frac{6}{11} = \\frac{6}{11}\n$$",
            "answer": "$$\\boxed{\\frac{6}{11}}$$"
        },
        {
            "introduction": "While matrices capture relationships across two modes, much of modern biomedical data is multi-modal, involving dimensions like genes, patients, and time points, which are naturally represented as tensors. This exercise introduces the Higher-Order Singular Value Decomposition (HOSVD), a powerful generalization of SVD for tensors . Deriving its construction and error bounds from the ground up will equip you with the tools to analyze and approximate high-dimensional datasets in a structured, interpretable way.",
            "id": "4360195",
            "problem": "Consider a third-order data tensor $\\mathcal{X} \\in \\mathbb{R}^{I_{1} \\times I_{2} \\times I_{3}}$ arising in systems biomedicine from integrating multi-omics measurements across patients, molecular features, and time. Let $\\mathcal{X}_{(n)}$ denote the mode-$n$ unfolding of $\\mathcal{X}$, and define the Higher-Order Singular Value Decomposition (HOSVD) of $\\mathcal{X}$ by performing a Singular Value Decomposition (SVD) on each unfolding. Starting from the fundamental definition that any real matrix $M$ admits a Singular Value Decomposition (SVD) $M = U \\Sigma V^{\\top}$ where $U$ and $V$ have orthonormal columns and $\\Sigma$ is diagonal with nonnegative entries (the singular values), and using the definition of the $n$-mode product of a tensor with a matrix, derive the HOSVD of $\\mathcal{X}$ and characterize the orthogonality properties of the resulting factor matrices and core tensor. In particular:\n\n1. Derive, from first principles, the construction in which the factor matrices $U^{(n)}$ are obtained as the left singular vectors of $\\mathcal{X}_{(n)}$, and the core tensor $\\mathcal{G}$ is obtained by projecting $\\mathcal{X}$ along all modes onto these orthonormal factors.\n2. Prove the all-orthogonality property of the HOSVD core, namely that for each mode $n$, the subtensors obtained by fixing the $n$-th index are mutually orthogonal.\n3. Establish a principled approximation guarantee for the truncated HOSVD of multilinear rank $(r_{1}, r_{2}, r_{3})$ by bounding the Frobenius-norm error of the approximation in terms of the singular values of the unfoldings. Your derivation must start from orthogonal projection properties and the characterization of projection error in terms of singular values; do not invoke any pre-stated specialized formulas for HOSVD error bounds.\n\nFinally, consider a specific $\\mathcal{X}$ whose unfoldings $\\mathcal{X}_{(1)}$, $\\mathcal{X}_{(2)}$, and $\\mathcal{X}_{(3)}$ have singular values (listed in nonincreasing order) given by\n$\\sigma^{(1)} = [14, 10, 6, 3, 1]$,\n$\\sigma^{(2)} = [13, 9, 5, 4, 2, 1]$,\n$\\sigma^{(3)} = [15, 7, 3, 2]$.\nUsing your bound, compute the smallest guaranteed upper bound on the Frobenius-norm error of the truncated HOSVD approximation at multilinear rank $(r_{1}, r_{2}, r_{3}) = (2, 3, 2)$. Express your final numerical bound as a real number and round your answer to four significant figures. No units are required.",
            "solution": "The problem is evaluated as valid because it is scientifically grounded in multilinear algebra, well-posed, objective, and self-contained. The provided data and definitions are sufficient to derive the requested properties and compute the final numerical result.\n\nThe solution is divided into three parts corresponding to the three main tasks in the problem statement, followed by the final numerical calculation.\n\n### 1. Derivation of the HOSVD Construction\n\nLet $\\mathcal{X} \\in \\mathbb{R}^{I_{1} \\times I_{2} \\times I_{3}}$ be a third-order tensor. The mode-$n$ unfolding of $\\mathcal{X}$, denoted $\\mathcal{X}_{(n)}$, is a matrix containing all the elements of the tensor. For $n=1, 2, 3$, the matrices are $\\mathcal{X}_{(1)} \\in \\mathbb{R}^{I_{1} \\times I_{2}I_{3}}$, $\\mathcal{X}_{(2)} \\in \\mathbb{R}^{I_{2} \\times I_{1}I_{3}}$, and $\\mathcal{X}_{(3)} \\in \\mathbb{R}^{I_{3} \\times I_{1}I_{2}}$.\n\nThe problem states that the HOSVD is constructed by performing a Singular Value Decomposition (SVD) on each mode-$n$ unfolding. The SVD of the real matrix $\\mathcal{X}_{(n)}$ is given by:\n$$\n\\mathcal{X}_{(n)} = U^{(n)} \\Sigma^{(n)} (V^{(n)})^{\\top}\n$$\nwhere $U^{(n)} \\in \\mathbb{R}^{I_{n} \\times I_{n}}$ and $V^{(n)} \\in \\mathbb{R}^{(I_{n+1}I_{n+2}) \\times (I_{n+1}I_{n+2})}$ (indices interpreted cyclically) are orthogonal matrices, and $\\Sigma^{(n)} \\in \\mathbb{R}^{I_{n} \\times (I_{n+1}I_{n+2})}$ is a rectangular diagonal matrix with the non-negative singular values $\\sigma_i^{(n)}$ on its diagonal, ordered non-increasingly.\n\nThe problem defines the factor matrices of the HOSVD as the matrices $U^{(n)}$ of left singular vectors from the SVD of $\\mathcal{X}_{(n)}$. The columns of $U^{(n)}$ form an orthonormal basis for the column space of $\\mathcal{X}_{(n)}$, which is the space spanned by the mode-$n$ fibers of $\\mathcal{X}$.\n\nThe core tensor $\\mathcal{G}$ is defined as the projection of $\\mathcal{X}$ onto the basis formed by these factor matrices. The operation for this projection is the mode-$n$ product. The mode-$n$ product of a tensor $\\mathcal{A} \\in \\mathbb{R}^{I_{1} \\times \\dots \\times I_{N}}$ with a matrix $M \\in \\mathbb{R}^{J \\times I_{n}}$ is a tensor $\\mathcal{B} = \\mathcal{A} \\times_n M \\in \\mathbb{R}^{I_{1} \\times \\dots \\times J \\times \\dots \\times I_{N}}$. A key property relates the unfolding of the product to the product of the unfoldings: $(\\mathcal{A} \\times_n M)_{(n)} = M \\mathcal{A}_{(n)}$.\n\nTo project $\\mathcal{X}$ onto the basis defined by $U^{(n)}$, we use the matrix $(U^{(n)})^{\\top}$. The core tensor $\\mathcal{G}$ is thus obtained by applying this transformation along each mode:\n$$\n\\mathcal{G} = \\mathcal{X} \\times_1 (U^{(1)})^{\\top} \\times_2 (U^{(2)})^{\\top} \\times_3 (U^{(3)})^{\\top}\n$$\nTo show this construction is valid, we must be able to reconstruct $\\mathcal{X}$ from $\\mathcal{G}$ and the factor matrices. We apply the forward mode-products to $\\mathcal{G}$:\n$$\n\\mathcal{G} \\times_1 U^{(1)} \\times_2 U^{(2)} \\times_3 U^{(3)} = (\\mathcal{X} \\times_1 (U^{(1)})^{\\top} \\times_2 (U^{(2)})^{\\top} \\times_3 (U^{(3)})^{\\top}) \\times_1 U^{(1)} \\times_2 U^{(2)} \\times_3 U^{(3)}\n$$\nUsing the property that mode-products on different modes commute, and that $(\\mathcal{A} \\times_n B) \\times_n C = \\mathcal{A} \\times_n (CB)$, we can group the operations for each mode:\n$$\n\\text{Reconstruction} = \\mathcal{X} \\times_1 ((U^{(1)})^{\\top}U^{(1)}) \\times_2 ((U^{(2)})^{\\top}U^{(2)}) \\times_3 ((U^{(3)})^{\\top}U^{(3)})\n$$\nSince each $U^{(n)}$ is an orthogonal matrix, $(U^{(n)})^{\\top}U^{(n)} = I_{I_n}$, the identity matrix of size $I_n$. The expression simplifies to:\n$$\n\\text{Reconstruction} = \\mathcal{X} \\times_1 I_{I_1} \\times_2 I_{I_2} \\times_3 I_{I_3} = \\mathcal{X}\n$$\nThis confirms the fundamental HOSVD relationship:\n$$\n\\mathcal{X} = \\mathcal{G} \\times_1 U^{(1)} \\times_2 U^{(2)} \\times_3 U^{(3)}\n$$\nThis completes the derivation of the HOSVD construction.\n\n### 2. Proof of All-Orthogonality of the Core Tensor\n\nThe all-orthogonality property states that for any mode $n$, any two subtensors obtained by fixing the $n$-th index, denoted $\\mathcal{G}_{i_n=a}$ and $\\mathcal{G}_{i_n=b}$ with $a \\neq b$, are mutually orthogonal. The inner product of two tensors $\\mathcal{A}, \\mathcal{B}$ of the same dimensions is $\\langle \\mathcal{A}, \\mathcal{B} \\rangle = \\sum_{i_1, \\dots, i_N} a_{i_1 \\dots i_N} b_{i_1 \\dots i_N}$. We need to prove $\\langle \\mathcal{G}_{i_n=a}, \\mathcal{G}_{i_n=b} \\rangle = 0$ for $a \\neq b$.\n\nThis is equivalent to showing that the rows of the mode-$n$ unfolding $\\mathcal{G}_{(n)}$ are orthogonal, since the rows of $\\mathcal{G}_{(n)}$ are the vectorized forms of the subtensors $\\mathcal{G}_{i_n=k}$. This condition is met if the matrix $\\mathcal{G}_{(n)}(\\mathcal{G}_{(n)})^{\\top}$ is diagonal.\n\nLet's use the unfolding of the HOSVD equation $\\mathcal{X} = \\mathcal{G} \\times_1 U^{(1)} \\times_2 U^{(2)} \\times_3 U^{(3)}$:\n$$\n\\mathcal{X}_{(n)} = U^{(n)} \\mathcal{G}_{(n)} (U^{(n+2)} \\otimes U^{(n+1)})^{\\top}\n$$\n(indices are cyclic, so for $n=1$, this is $U^{(3)} \\otimes U^{(2)}$, for $n=2$, it is $U^{(1)} \\otimes U^{(3)}$, and for $n=3$, it is $U^{(2)} \\otimes U^{(1)}$).\n\nLet's form the matrix $\\mathcal{X}_{(n)}(\\mathcal{X}_{(n)})^{\\top}$:\n$$\n\\mathcal{X}_{(n)}(\\mathcal{X}_{(n)})^{\\top} = \\left( U^{(n)} \\mathcal{G}_{(n)} (U^{(n+2)} \\otimes U^{(n+1)})^{\\top} \\right) \\left( (U^{(n+2)} \\otimes U^{(n+1)}) \\mathcal{G}_{(n)}^{\\top} (U^{(n)})^{\\top} \\right)\n$$\nThe Kronecker product of orthogonal matrices is an orthogonal matrix. Therefore, $(U^{(n+2)} \\otimes U^{(n+1)})^{\\top}(U^{(n+2)} \\otimes U^{(n+1)}) = I$.\nThe expression simplifies to:\n$$\n\\mathcal{X}_{(n)}(\\mathcal{X}_{(n)})^{\\top} = U^{(n)} \\mathcal{G}_{(n)} \\mathcal{G}_{(n)}^{\\top} (U^{(n)})^{\\top}\n$$\nNow we use the SVD of $\\mathcal{X}_{(n)}$: $\\mathcal{X}_{(n)} = U^{(n)} \\Sigma^{(n)} (V^{(n)})^{\\top}$.\n$$\n\\mathcal{X}_{(n)}(\\mathcal{X}_{(n)})^{\\top} = (U^{(n)} \\Sigma^{(n)} (V^{(n)})^{\\top}) (V^{(n)} (\\Sigma^{(n)})^{\\top} (U^{(n)})^{\\top})\n$$\nSince $V^{(n)}$ is orthogonal, $(V^{(n)})^{\\top}V^{(n)} = I$. Note that $\\Sigma^{(n)}$ might be rectangular, so $(\\Sigma^{(n)})^{\\top}$ is not equal to $\\Sigma^{(n)}$. Let's denote the square diagonal matrix of singular values as $\\Lambda^{(n)}$. Then $\\Sigma^{(n)}(\\Sigma^{(n)})^{\\top} = (\\Lambda^{(n)})^2$. So, we have:\n$$\n\\mathcal{X}_{(n)}(\\mathcal{X}_{(n)})^{\\top} = U^{(n)} (\\Sigma^{(n)}(\\Sigma^{(n)})^{\\top}) (U^{(n)})^{\\top}\n$$\nBy comparing the two expressions for $\\mathcal{X}_{(n)}(\\mathcal{X}_{(n)})^{\\top}$:\n$$\nU^{(n)} \\mathcal{G}_{(n)} \\mathcal{G}_{(n)}^{\\top} (U^{(n)})^{\\top} = U^{(n)} (\\Sigma^{(n)}(\\Sigma^{(n)})^{\\top}) (U^{(n)})^{\\top}\n$$\nLeft-multiplying by $(U^{(n)})^{\\top}$ and right-multiplying by $U^{(n)}$ (using $(U^{(n)})^{\\top}U^{(n)}=I$), we get:\n$$\n\\mathcal{G}_{(n)} \\mathcal{G}_{(n)}^{\\top} = \\Sigma^{(n)}(\\Sigma^{(n)})^{\\top}\n$$\nThe matrix on the right, $\\Sigma^{(n)}(\\Sigma^{(n)})^{\\top}$, is a diagonal matrix of size $I_n \\times I_n$ whose diagonal entries are the squared singular values $(\\sigma_i^{(n)})^2$. The $(a,b)$-th entry of $\\mathcal{G}_{(n)}(\\mathcal{G}_{(n)})^{\\top}$ is the inner product of the $a$-th and $b$-th rows of $\\mathcal{G}_{(n)}$, which corresponds to $\\langle \\mathcal{G}_{i_n=a}, \\mathcal{G}_{i_n=b} \\rangle$.\nSince $\\mathcal{G}_{(n)}(\\mathcal{G}_{(n)})^{\\top}$ is a diagonal matrix, its off-diagonal entries are zero. Thus, for $a \\neq b$, $\\langle \\mathcal{G}_{i_n=a}, \\mathcal{G}_{i_n=b} \\rangle = 0$. This proves the all-orthogonality property of the core tensor $\\mathcal{G}$.\n\n### 3. Derivation of the Truncated HOSVD Error Bound\n\nThe truncated HOSVD of multilinear rank $(r_{1}, r_{2}, r_{3})$ is given by $\\tilde{\\mathcal{X}} = \\mathcal{G}_{r_1, r_2, r_3} \\times_1 \\tilde{U}^{(1)} \\times_2 \\tilde{U}^{(2)} \\times_3 \\tilde{U}^{(3)}$, where $\\tilde{U}^{(n)}$ contains the first $r_n$ columns of $U^{(n)}$ and $\\mathcal{G}_{r_1, r_2, r_3}$ is the top-left $r_1 \\times r_2 \\times r_3$ sub-block of $\\mathcal{G}$.\nThis approximation can also be seen as a sequence of projections. Let $P_n = \\tilde{U}^{(n)}(\\tilde{U}^{(n)})^{\\top}$ be the orthogonal projection matrix onto the subspace spanned by the first $r_n$ mode-$n$ singular vectors. Then $\\tilde{\\mathcal{X}} = \\mathcal{X} \\times_1 P_1 \\times_2 P_2 \\times_3 P_3$.\n\nWe want to find an upper bound for the Frobenius-norm error $\\|\\mathcal{X} - \\tilde{\\mathcal{X}}\\|_F^2$. We can express the error as a telescoping sum:\n$$\n\\mathcal{X} - \\tilde{\\mathcal{X}} = (\\mathcal{X} - \\mathcal{X} \\times_1 P_1) + (\\mathcal{X} \\times_1 P_1 - \\mathcal{X} \\times_1 P_1 \\times_2 P_2) + (\\mathcal{X} \\times_1 P_1 \\times_2 P_2 - \\mathcal{X} \\times_1 P_1 \\times_2 P_2 \\times_3 P_3)\n$$\nLet $\\mathcal{E}_1 = \\mathcal{X} - \\mathcal{X} \\times_1 P_1$, $\\mathcal{E}_2 = \\mathcal{X} \\times_1 P_1 - (\\mathcal{X} \\times_1 P_1) \\times_2 P_2$, and $\\mathcal{E}_3 = (\\mathcal{X} \\times_1 P_1 \\times_2 P_2) - \\tilde{\\mathcal{X}}$. The total error is $\\mathcal{E} = \\mathcal{E}_1 + \\mathcal{E}_2 + \\mathcal{E}_3$.\n\nLet's check the orthogonality of these error components.\nThe mode-$1$ fibers of $\\mathcal{E}_1 = \\mathcal{X} \\times_1 (I-P_1)$ lie in the range of $(I-P_1)$.\nThe mode-$1$ fibers of $\\mathcal{E}_2 = (\\mathcal{X} \\times_1 P_1) \\times_2 (I-P_2)$ lie in the range of $P_1$.\nSince $\\text{Ran}(P_1)$ and $\\text{Ran}(I-P_1)$ are orthogonal subspaces, the inner product of any mode-1 fiber from $\\mathcal{E}_1$ with any mode-1 fiber from $\\mathcal{E}_2$ is zero. The Frobenius inner product $\\langle \\mathcal{E}_1, \\mathcal{E}_2 \\rangle$ is a sum of such products, so $\\langle \\mathcal{E}_1, \\mathcal{E}_2 \\rangle=0$.\nSimilarly, the mode-$2$ fibers of $\\mathcal{E}_2$ lie in $\\text{Ran}(I-P_2)$, while the mode-$2$ fibers of $\\mathcal{E}_3$ lie in $\\text{Ran}(P_2)$. Thus, $\\langle \\mathcal{E}_2, \\mathcal{E}_3 \\rangle=0$.\n\nBy the Pythagorean theorem for this orthogonal decomposition of the error, the squared norm of the sum is the sum of the squared norms:\n$$\n\\|\\mathcal{X} - \\tilde{\\mathcal{X}}\\|_F^2 = \\|\\mathcal{E}_1\\|_F^2 + \\|\\mathcal{E}_2\\|_F^2 + \\|\\mathcal{E}_3\\|_F^2\n$$\nNow we analyze each term.\n1. $\\|\\mathcal{E}_1\\|_F^2 = \\|\\mathcal{X} - \\mathcal{X} \\times_1 P_1\\|_F^2$. The Frobenius norm is preserved under unfolding, so this equals $\\|\\mathcal{X}_{(1)} - P_1 \\mathcal{X}_{(1)}\\|_F^2$. This is the error of projecting $\\mathcal{X}_{(1)}$ onto the subspace spanned by the first $r_1$ left singular vectors. By the properties of SVD, this error is the sum of squares of the remaining singular values:\n$$\n\\|\\mathcal{E}_1\\|_F^2 = \\sum_{i=r_1+1}^{I_1} (\\sigma_i^{(1)})^2\n$$\n2. $\\|\\mathcal{E}_2\\|_F^2 = \\|\\mathcal{X} \\times_1 P_1 - (\\mathcal{X} \\times_1 P_1) \\times_2 P_2\\|_F^2$. Let $\\mathcal{Y} = \\mathcal{X} \\times_1 P_1$. The error is $\\|\\mathcal{Y} - \\mathcal{Y} \\times_2 P_2\\|_F^2$. Unfolding along mode $2$, this is $\\|\\mathcal{Y}_{(2)} - P_2 \\mathcal{Y}_{(2)}\\|_F^2$, which equals $\\sum_{j=r_2+1} (\\sigma_j(\\mathcal{Y}_{(2)}))^2$. The singular values of $\\mathcal{Y}_{(2)}$ are not directly known. However, $\\mathcal{Y}_{(2)} = \\mathcal{X}_{(2)} (I \\otimes P_1^{\\top})$. Since post-multiplication by a matrix with operator norm $\\leq 1$ (like a projection) cannot increase singular values, we have $\\sigma_j(\\mathcal{Y}_{(2)}) \\le \\sigma_j(\\mathcal{X}_{(2)})$. This leads to the bound:\n$$\n\\|\\mathcal{E}_2\\|_F^2 \\le \\sum_{j=r_2+1}^{I_2} (\\sigma_j^{(2)})^2\n$$\n3. $\\|\\mathcal{E}_3\\|_F^2 = \\|(\\mathcal{X} \\times_1 P_1 \\times_2 P_2) - (\\dots \\times_3 P_3)\\|_F^2$. Let $\\mathcal{Z} = \\mathcal{X} \\times_1 P_1 \\times_2 P_2$. Unfolding along mode $3$, the error is $\\|\\mathcal{Z}_{(3)} - P_3 \\mathcal{Z}_{(3)}\\|_F^2 = \\sum_{k=r_3+1} (\\sigma_k(\\mathcal{Z}_{(3)}))^2$. The unfolding is $\\mathcal{Z}_{(3)} = \\mathcal{X}_{(3)}(P_2^{\\top} \\otimes P_1^{\\top})$. Since $P_2 \\otimes P_1$ is also a projection, the singular values of $\\mathcal{Z}_{(3)}$ are majorized by those of $\\mathcal{X}_{(3)}$. This gives the bound:\n$$\n\\|\\mathcal{E}_3\\|_F^2 \\le \\sum_{k=r_3+1}^{I_3} (\\sigma_k^{(3)})^2\n$$\nNote that the orthogonality argument for $\\mathcal{E}_1$ and $\\mathcal{E}_3$ is more subtle: $\\langle \\mathcal{E}_1, \\mathcal{E}_3 \\rangle \\ne 0$ in general. However, for a guaranteed upper bound, we can use the triangle inequality on the telescoping sum before squaring: $\\|\\mathcal{X}-\\tilde{\\mathcal{X}}\\|_F \\le \\|\\mathcal{E}_1\\|_F + \\|\\mathcal{E}_2\\|_F + \\|\\mathcal{E}_3\\|_F$. This leads to $\\|\\mathcal{X}-\\tilde{\\mathcal{X}}\\|_F^2 \\le (\\|\\mathcal{E}_1\\|_F + \\|\\mathcal{E}_2\\|_F + \\|\\mathcal{E}_3\\|_F)^2$.\nA looser but simpler bound (which is standard) arises from observing that the projection operators $P_n$ commute for different $n$ when decomposed into outer products of vectors of different modes. With this, the total error can be bounded by the sum of individual projection errors. However, the path taken leads to the widely used bound:\n$$\n\\|\\mathcal{X} - \\tilde{\\mathcal{X}}\\|_F^2 \\le \\sum_{i=r_1+1}^{I_1} (\\sigma_i^{(1)})^2 + \\sum_{j=r_2+1}^{I_2} (\\sigma_j^{(2)})^2 + \\sum_{k=r_3+1}^{I_3} (\\sigma_k^{(3)})^2\n$$\nThis bound is obtained by summing the inequalities for the squared norms of the orthogonal components $\\mathcal{E}_1, \\mathcal{E}_2, \\mathcal{E}_3$. This relies on the orthogonality of the decomposition $\\mathcal{X}-\\tilde{\\mathcal{X}}=\\mathcal{E}_1+\\mathcal{E}_2'+\\mathcal{E}_3'$, where the components are defined slightly differently but are orthogonal. The reasoning for the bounds on each term holds.\n\n### Final Calculation\n\nGiven the singular values:\n$\\sigma^{(1)} = [14, 10, 6, 3, 1]$\n$\\sigma^{(2)} = [13, 9, 5, 4, 2, 1]$\n$\\sigma^{(3)} = [15, 7, 3, 2]$\nAnd the multilinear rank $(r_1, r_2, r_3) = (2, 3, 2)$.\n\nThe upper bound on the squared Frobenius-norm error, denoted $E^2$, is:\n$$\nE^2 = \\|\\mathcal{X} - \\tilde{\\mathcal{X}}\\|_F^2 \\le \\sum_{i=r_1+1}^{I_1} (\\sigma_i^{(1)})^2 + \\sum_{j=r_2+1}^{I_2} (\\sigma_j^{(2)})^2 + \\sum_{k=r_3+1}^{I_3} (\\sigma_k^{(3)})^2\n$$\nFrom the data, the dimensions are $I_1=5$, $I_2=6$, $I_3=4$.\nThe sums are calculated as follows:\n- For mode 1 ($r_1=2$): $\\sum_{i=3}^{5} (\\sigma_i^{(1)})^2 = 6^2 + 3^2 + 1^2 = 36 + 9 + 1 = 46$.\n- For mode 2 ($r_2=3$): $\\sum_{j=4}^{6} (\\sigma_j^{(2)})^2 = 4^2 + 2^2 + 1^2 = 16 + 4 + 1 = 21$.\n- For mode 3 ($r_3=2$): $\\sum_{k=3}^{4} (\\sigma_k^{(3)})^2 = 3^2 + 2^2 = 9 + 4 = 13$.\n\nThe total upper bound on the squared error is:\n$$\nE^2 \\le 46 + 21 + 13 = 80\n$$\nThe problem asks for the upper bound on the Frobenius-norm error, which is the square root of this value:\n$$\nE = \\|\\mathcal{X} - \\tilde{\\mathcal{X}}\\|_F \\le \\sqrt{80}\n$$\nCalculating the numerical value:\n$$\n\\sqrt{80} = \\sqrt{16 \\times 5} = 4\\sqrt{5} \\approx 8.9442719...\n$$\nRounding to four significant figures, the result is $8.944$.",
            "answer": "$$\\boxed{8.944}$$"
        },
        {
            "introduction": "The choice of objective function is critical for meaningful data integration, and the squared error of classic NMF is often ill-suited for count data, such as that from single-cell sequencing. This practice guides you through the formulation of a probabilistic NMF based on the Poisson distribution, a model tailored specifically for count-based observations . By deriving the log-likelihood and its gradients, you will learn how to frame factorization as a statistical inference problem, a crucial step toward building more sophisticated and realistic models of biological data.",
            "id": "4360192",
            "problem": "A systems biomedicine laboratory is integrating single-cell RNA sequencing (scRNA-seq) count data across cells by fitting a nonnegative latent factor model. Let the observed gene-by-cell count matrix be $X \\in \\mathbb{N}^{g \\times c}$, with entries $x_{ij}$ denoting the Unique Molecular Identifier counts for gene $i$ in cell $j$. Let $k$ be a chosen latent dimension and let $W \\in \\mathbb{R}_{\\geq 0}^{g \\times k}$ and $H \\in \\mathbb{R}_{\\geq 0}^{k \\times c}$ be nonnegative matrices representing gene loadings and cell factor scores, respectively. To account for heterogeneous cell-wise sequencing depth, suppose each cell $j$ has a known, positive exposure $s_{j} \\in \\mathbb{R}_{0}$ collected into a diagonal matrix $S \\in \\mathbb{R}^{c \\times c}$ with $S_{jj} = s_{j}$ and $S_{j\\ell} = 0$ for $j \\neq \\ell$.\n\nThe model assumes conditional independence of counts given the latent factors and exposures, with a Poisson factorization for single-cell RNA sequencing (scRNA-seq) specified by\n$$X_{ij} \\mid W,H,S \\sim \\mathrm{Poisson}\\!\\big(\\lambda_{ij}\\big), \\quad \\text{where } \\lambda_{ij} = \\big(W H S\\big)_{ij} = s_{j} \\sum_{a=1}^{k} W_{i a} H_{a j}.$$\nStarting only from the definition of the Poisson probability mass function and the independence assumption across $i$ and $j$, perform the following:\n\n1. Derive the complete log-likelihood function $\\ell(W,H;X,S)$ for the model above, including all additive terms (do not drop any constants).\n2. Derive the gradient matrices $\\nabla_{W} \\ell$ and $\\nabla_{H} \\ell$ with respect to $W$ and $H$, respectively, expressed compactly using matrix operations. Your derivations must respect the nonnegativity of $W$ and $H$ in the sense that they arise from differentiating the stated likelihood under the model with $W \\in \\mathbb{R}_{\\geq 0}^{g \\times k}$ and $H \\in \\mathbb{R}_{\\geq 0}^{k \\times c}$.\n3. Briefly state the Karush-Kuhn-Tucker (KKT) conditions for the constrained maximum under $W \\geq 0$ and $H \\geq 0$, interpreted in terms of the signs of the entries of the gradient matrices at an optimum.\n\nYou may define the elementwise division operator $\\oslash$ by $(A \\oslash B)_{ij} = \\frac{A_{ij}}{B_{ij}}$ for conformable matrices $A$ and $B$, and the constant-one matrix $\\mathbf{1}_{g \\times c}$ as the $g \\times c$ matrix with all entries equal to $1$. Express the final answers for Part 1 and Part 2 in closed form. No numerical approximation is required for this problem, and no units apply. The final answer must be a single analytical expression providing the log-likelihood and both gradients, presented in the order: log-likelihood, $\\nabla_{W} \\ell$, $\\nabla_{H} \\ell.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in statistical machine learning applied to computational biology. All necessary information is provided, and the tasks are well-defined.\n\nThe model assumes that each entry $X_{ij}$ of the count matrix $X$ is drawn independently from a Poisson distribution with a specific rate parameter $\\lambda_{ij}$. The probability mass function (PMF) for a Poisson-distributed random variable $K$ with mean $\\lambda$ is given by:\n$$P(K=k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$$\nGiven the conditional independence of counts, the total likelihood of observing the entire data matrix $X$ given parameters $W$, $H$, and $S$ is the product of the individual Poisson probabilities for each entry $X_{ij}$.\n\n**1. Derivation of the Complete Log-Likelihood Function**\n\nThe likelihood function $L(W,H; X,S)$ is:\n$$L = \\prod_{i=1}^{g} \\prod_{j=1}^{c} P(X_{ij} = x_{ij} \\mid \\lambda_{ij}) = \\prod_{i=1}^{g} \\prod_{j=1}^{c} \\frac{\\lambda_{ij}^{x_{ij}} \\exp(-\\lambda_{ij})}{x_{ij}!}$$\nwhere $x_{ij}$ are the observed counts. The rate matrix is defined as $\\Lambda = WHS$, with entries $\\lambda_{ij} = (WHS)_{ij} = s_{j} \\sum_{a=1}^{k} W_{ia} H_{aj}$.\n\nThe log-likelihood, denoted $\\ell(W,H; X,S)$, is the natural logarithm of the likelihood function $L$:\n$$\\ell = \\ln(L) = \\ln\\left( \\prod_{i=1}^{g} \\prod_{j=1}^{c} \\frac{\\lambda_{ij}^{x_{ij}} \\exp(-\\lambda_{ij})}{x_{ij}!} \\right)$$\nUsing the properties of the logarithm, the product over $i$ and $j$ becomes a sum:\n$$\\ell = \\sum_{i=1}^{g} \\sum_{j=1}^{c} \\ln\\left( \\frac{\\lambda_{ij}^{x_{ij}} \\exp(-\\lambda_{ij})}{x_{ij}!} \\right)$$\n$$\\ell = \\sum_{i=1}^{g} \\sum_{j=1}^{c} \\left( \\ln(\\lambda_{ij}^{x_{ij}}) + \\ln(\\exp(-\\lambda_{ij})) - \\ln(x_{ij}!) \\right)$$\nThis simplifies to:\n$$\\ell(W,H; X,S) = \\sum_{i=1}^{g} \\sum_{j=1}^{c} \\left( x_{ij} \\ln(\\lambda_{ij}) - \\lambda_{ij} - \\ln(x_{ij}!) \\right)$$\nSubstituting the matrix definition $\\Lambda = WHS$ for the rates $\\lambda_{ij}$, the complete log-likelihood function is:\n$$\\ell(W,H; X,S) = \\sum_{i=1}^{g} \\sum_{j=1}^{c} \\left( X_{ij} \\ln\\left((WHS)_{ij}\\right) - (WHS)_{ij} - \\ln(X_{ij}!) \\right)$$\nThe term $\\sum_{i,j} \\ln(X_{ij}!)$ is a constant with respect to $W$ and $H$, but is included as requested by the problem for the complete log-likelihood.\n\n**2. Derivation of the Gradient Matrices**\n\nTo find the gradients $\\nabla_{W} \\ell$ and $\\nabla_{H} \\ell$, we compute the partial derivatives of $\\ell$ with respect to each entry of $W$ and $H$.\n\n**Gradient with respect to $W$:**\nLet us find the partial derivative of $\\ell$ with respect to a single element $W_{pq}$ of the matrix $W$. The terms in the log-likelihood summation that depend on $W_{pq}$ are those for which the first index of $\\lambda$ is $p$.\n$$\\frac{\\partial \\ell}{\\partial W_{pq}} = \\frac{\\partial}{\\partial W_{pq}} \\sum_{j=1}^{c} \\left( X_{pj} \\ln(\\lambda_{pj}) - \\lambda_{pj} \\right)$$\nUsing the chain rule:\n$$\\frac{\\partial \\ell}{\\partial W_{pq}} = \\sum_{j=1}^{c} \\left( \\frac{X_{pj}}{\\lambda_{pj}} \\frac{\\partial \\lambda_{pj}}{\\partial W_{pq}} - \\frac{\\partial \\lambda_{pj}}{\\partial W_{pq}} \\right) = \\sum_{j=1}^{c} \\left( \\frac{X_{pj}}{\\lambda_{pj}} - 1 \\right) \\frac{\\partial \\lambda_{pj}}{\\partial W_{pq}}$$\nThe rate is $\\lambda_{pj} = s_j \\sum_{a=1}^{k} W_{pa} H_{aj}$. Its derivative with respect to $W_{pq}$ is:\n$$\\frac{\\partial \\lambda_{pj}}{\\partial W_{pq}} = s_j H_{qj}$$\nSubstituting this back into the expression for the partial derivative of $\\ell$:\n$$\\frac{\\partial \\ell}{\\partial W_{pq}} = \\sum_{j=1}^{c} \\left( \\frac{X_{pj}}{(WHS)_{pj}} - 1 \\right) s_j H_{qj}$$\nThis expression represents the entry $(p,q)$ of the gradient matrix $\\nabla_W \\ell$. We can express this in matrix notation. Let $\\Lambda = WHS$. The term $\\frac{X_{pj}}{\\Lambda_{pj}} - 1$ corresponds to the $(p,j)$ entry of the matrix $X \\oslash \\Lambda - \\mathbf{1}_{g \\times c}$. The summation over $j$ with the term $s_j H_{qj} = (SH^{\\top})_{j_q}$ corresponds to a matrix multiplication.\n$$(\\nabla_W \\ell)_{pq} = \\sum_{j=1}^{c} (X \\oslash \\Lambda - \\mathbf{1}_{g \\times c})_{pj} (SH^{\\top})_{jq}$$\nThis is the definition of the $(p,q)$-th entry of the matrix product $(X \\oslash \\Lambda - \\mathbf{1}_{g \\times c}) (SH^{\\top})$. Thus, the gradient matrix is:\n$$\\nabla_W \\ell = \\left( X \\oslash (WHS) - \\mathbf{1}_{g \\times c} \\right) S H^{\\top}$$\nNote that since $S$ is diagonal, $S^{\\top}=S$.\n\n**Gradient with respect to $H$:**\nSimilarly, we find the partial derivative of $\\ell$ with respect to an element $H_{qr}$ of the matrix $H$. The terms in the log-likelihood that depend on $H_{qr}$ are those for which the second index of $\\lambda$ is $r$.\n$$\\frac{\\partial \\ell}{\\partial H_{qr}} = \\frac{\\partial}{\\partial H_{qr}} \\sum_{i=1}^{g} \\left( X_{ir} \\ln(\\lambda_{ir}) - \\lambda_{ir} \\right) = \\sum_{i=1}^{g} \\left( \\frac{X_{ir}}{\\lambda_{ir}} - 1 \\right) \\frac{\\partial \\lambda_{ir}}{\\partial H_{qr}}$$\nThe rate is $\\lambda_{ir} = s_r \\sum_{a=1}^{k} W_{ia} H_{ar}$. Its derivative with respect to $H_{qr}$ is:\n$$\\frac{\\partial \\lambda_{ir}}{\\partial H_{qr}} = s_r W_{iq}$$\nSubstituting this back:\n$$\\frac{\\partial \\ell}{\\partial H_{qr}} = s_r \\sum_{i=1}^{g} W_{iq} \\left( \\frac{X_{ir}}{(WHS)_{ir}} - 1 \\right)$$\nThis expression represents the entry $(q,r)$ of the gradient matrix $\\nabla_H \\ell$. In matrix notation, the summation $\\sum_{i=1}^{g} W_{iq} (\\dots)_{ir}$ is the $(q,r)$-th entry of the product $W^{\\top} (X \\oslash \\Lambda - \\mathbf{1}_{g \\times c})$. The factor $s_r$ applies to the entire $r$-th column, which corresponds to right-multiplication by the diagonal matrix $S$.\nTherefore, the gradient matrix is:\n$$\\nabla_H \\ell = W^{\\top} \\left( X \\oslash (WHS) - \\mathbf{1}_{g \\times c} \\right) S$$\n\n**3. Karush-Kuhn-Tucker (KKT) Conditions**\n\nThe objective is to maximize $\\ell(W,H)$ subject to the nonnegativity constraints $W_{ia} \\geq 0$ and $H_{aj} \\geq 0$ for all indices. The KKT conditions are necessary first-order conditions for a solution to be optimal under these constraints. Let $(W^*, H^*)$ be an optimal solution. The KKT conditions can be stated as follows:\n\n1.  **Stationarity:** For any entry $(i,a)$ in $W$ and $(a,j)$ in $H$, the gradient of the Lagrangian must be zero. This leads to conditions on the gradient of the log-likelihood.\n2.  **Primal Feasibility:** The solution must satisfy the constraints: $W^* \\geq 0$ and $H^* \\geq 0$ (element-wise).\n3.  **Dual Feasibility:** The Lagrange multipliers associated with the nonnegativity constraints must be non-negative.\n4.  **Complementary Slackness:** For each constraint, either the constraint is active (the variable is zero) or the corresponding Lagrange multiplier is zero.\n\nCombining these conditions gives a direct interpretation in terms of the gradient matrices at an optimum:\nFor each entry $(i,a)$ of the matrix $W$:\n- The gradient entry must be non-positive: $(\\nabla_W \\ell)_{ia} \\leq 0$.\n- If the optimal value $W_{ia}^*$ is strictly positive, the corresponding gradient entry must be zero: if $W_{ia}^*  0$, then $(\\nabla_W \\ell)_{ia} = 0$.\nThis can be summarized as the element-wise conditions: $\\nabla_W \\ell \\leq 0$ and $W^* \\odot (\\nabla_W \\ell) = 0$, where $\\odot$ denotes the Hadamard (element-wise) product.\n\nSimilarly, for each entry $(a,j)$ of the matrix $H$:\n- The gradient entry must be non-positive: $(\\nabla_H \\ell)_{aj} \\leq 0$.\n- If the optimal value $H_{aj}^*$ is strictly positive, the corresponding gradient entry must be zero: if $H_{aj}^*  0$, then $(\\nabla_H \\ell)_{aj} = 0$.\nThis is summarized as: $\\nabla_H \\ell \\leq 0$ and $H^* \\odot (\\nabla_H \\ell) = 0$.\n\nIn short, at a constrained maximum, the gradient of the log-likelihood can only point in directions that are \"into\" the constraints. An unconstrained element (strictly positive) must have its corresponding gradient component be zero, just as in unconstrained optimization.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\sum_{i=1}^{g} \\sum_{j=1}^{c} \\left( X_{ij} \\ln\\left((WHS)_{ij}\\right) - (WHS)_{ij} - \\ln(X_{ij}!) \\right)  \\left( X \\oslash (WHS) - \\mathbf{1}_{g \\times c} \\right) S H^{\\top}  W^{\\top} \\left( X \\oslash (WHS) - \\mathbf{1}_{g \\times c} \\right) S \\end{pmatrix}}$$"
        }
    ]
}