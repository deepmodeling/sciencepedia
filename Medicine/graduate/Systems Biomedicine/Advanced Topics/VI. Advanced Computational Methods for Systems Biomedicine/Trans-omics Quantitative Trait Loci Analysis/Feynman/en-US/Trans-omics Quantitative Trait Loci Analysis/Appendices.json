{
    "hands_on_practices": [
        {
            "introduction": "Before we can interpret signals across different omics layers, we must first understand how those signals are generated. This exercise  explores the statistical foundation of QTL analysis, asking you to derive the widely used $z$-statistic from a simple linear regression model. Crucially, it demonstrates how the non-independence of genetic variants due to Linkage Disequilibrium (LD) shapes the joint distribution of these statistics, a fundamental concept that underpins subsequent fine-mapping and causal inference methods.",
            "id": "4395278",
            "problem": "Consider a trans-omics Quantitative Trait Loci (QTL) analysis in systems biomedicine where the molecular phenotype is an integrated trans-omics trait constructed as a standardized aggregate of messenger ribonucleic acid (mRNA), protein, and metabolite levels. Let there be $n$ individuals and $p$ Single Nucleotide Polymorphisms (SNPs). Denote by $\\mathbf{y} \\in \\mathbb{R}^{n}$ the standardized trans-omics trait vector (mean $0$, variance $1$), and by $\\mathbf{G} \\in \\mathbb{R}^{n \\times p}$ the standardized genotype matrix of the $p$ SNPs (each SNP column has mean $0$ and variance $1$). Assume the linear model\n$$\n\\mathbf{y} = \\mathbf{G}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\boldsymbol{\\theta} \\in \\mathbb{R}^{p}$ are SNP effect sizes on the trans-omics trait, and $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$ represents independent residual variation across individuals. Define the Linkage Disequilibrium (LD) matrix by\n$$\n\\mathbf{R} = \\frac{1}{n}\\mathbf{G}^{\\top}\\mathbf{G},\n$$\nwhich is the empirical correlation matrix among SNPs due to LD under the stated standardizations.\n\nA summary-based association statistic for SNP $j$ is constructed by fitting a separate Ordinary Least Squares (OLS) regression per SNP,\n$$\n\\mathbf{y} = \\alpha_{j}\\mathbf{1} + \\beta_{j}\\mathbf{g}_{j} + \\boldsymbol{\\varepsilon}_{j},\n$$\nwhere $\\mathbf{g}_{j}$ denotes column $j$ of $\\mathbf{G}$, and then forming the $z$-statistic as the ratio of the estimated effect to its standard error. Starting from the OLS estimator and its sampling variance, derive the form of the $z$-statistic and the joint sampling distribution of the vector of $z$-statistics across SNPs under the null hypothesis $\\boldsymbol{\\theta} = \\mathbf{0}$, making explicit how LD (through $\\mathbf{R}$) enters this distribution.\n\nThen, consider a locus with $p = 2$ SNPs with LD correlation $r$ so that\n$$\n\\mathbf{R} = \\begin{pmatrix} 1 & r \\\\ r & 1 \\end{pmatrix}.\n$$\nUnder the null hypothesis $\\boldsymbol{\\theta} = \\mathbf{0}$ and the modeling assumptions above, compute the variance of the linear contrast $z_{1} - z_{2}$ in terms of $r$. Provide your final scalar expression for $\\mathrm{Var}(z_{1} - z_{2})$ solely as a function of $r$. No units are required. Do not round; an exact closed-form expression is expected.",
            "solution": "The problem asks for two main results: first, to derive the form of the single-SNP $z$-statistic and the joint sampling distribution of the vector of $z$-statistics under the null hypothesis $\\boldsymbol{\\theta} = \\mathbf{0}$; second, to compute the variance of the contrast $z_{1} - z_{2}$ for a two-SNP locus with linkage disequilibrium correlation $r$.\n\nThe analysis proceeds in two parts. First, we derive the general distribution of the $z$-statistics. Second, we apply this result to the specific case provided.\n\n**Part 1: Derivation of the $z$-statistic and its Distribution**\n\nWe begin by examining the single-SNP Ordinary Least Squares (OLS) regression model for SNP $j$:\n$$\n\\mathbf{y} = \\alpha_{j}\\mathbf{1} + \\beta_{j}\\mathbf{g}_{j} + \\boldsymbol{\\varepsilon}_{j}\n$$\nHere, $\\mathbf{y} \\in \\mathbb{R}^{n}$ is the phenotype vector, $\\mathbf{g}_{j} \\in \\mathbb{R}^{n}$ is the genotype vector for SNP $j$, and $\\mathbf{1}$ is a vector of ones. The problem states that both the phenotype vector $\\mathbf{y}$ and each genotype vector $\\mathbf{g}_{j}$ are standardized to have a mean of $0$ and a variance of $1$.\n\nLet $\\mathbf{X}_{j} = [\\mathbf{1}, \\mathbf{g}_j]$ be the design matrix for this regression. The OLS estimators for $\\alpha_j$ and $\\beta_j$ are given by $\\begin{pmatrix} \\hat{\\alpha}_j \\\\ \\hat{\\beta}_j \\end{pmatrix} = (\\mathbf{X}_{j}^{\\top}\\mathbf{X}_{j})^{-1}\\mathbf{X}_{j}^{\\top}\\mathbf{y}$.\n\nDue to the standardization, the columns of $\\mathbf{X}_j$ are orthogonal. We have $\\mathbf{1}^{\\top}\\mathbf{g}_{j} = \\sum_{i=1}^{n} g_{ij} = n \\bar{g}_{j} = 0$.\nThe matrix $\\mathbf{X}_{j}^{\\top}\\mathbf{X}_{j}$ is therefore block-diagonal:\n$$\n\\mathbf{X}_{j}^{\\top}\\mathbf{X}_{j} = \\begin{pmatrix} \\mathbf{1}^{\\top}\\mathbf{1} & \\mathbf{1}^{\\top}\\mathbf{g}_{j} \\\\ \\mathbf{g}_{j}^{\\top}\\mathbf{1} & \\mathbf{g}_{j}^{\\top}\\mathbf{g}_{j} \\end{pmatrix} = \\begin{pmatrix} n & 0 \\\\ 0 & \\mathbf{g}_{j}^{\\top}\\mathbf{g}_{j} \\end{pmatrix}\n$$\nThe condition that $\\mathbf{g}_{j}$ has variance $1$ means $\\frac{1}{n} \\sum_{i=1}^n (g_{ij} - \\bar{g}_j)^2 = 1$. Since $\\bar{g}_j = 0$, this simplifies to $\\frac{1}{n}\\mathbf{g}_{j}^{\\top}\\mathbf{g}_{j} = 1$, which implies $\\mathbf{g}_{j}^{\\top}\\mathbf{g}_{j} = n$.\nSubstituting this, we get:\n$$\n\\mathbf{X}_{j}^{\\top}\\mathbf{X}_{j} = \\begin{pmatrix} n & 0 \\\\ 0 & n \\end{pmatrix} \\quad \\implies \\quad (\\mathbf{X}_{j}^{\\top}\\mathbf{X}_{j})^{-1} = \\begin{pmatrix} \\frac{1}{n} & 0 \\\\ 0 & \\frac{1}{n} \\end{pmatrix}\n$$\nThe vector of OLS estimates is then:\n$$\n\\begin{pmatrix} \\hat{\\alpha}_j \\\\ \\hat{\\beta}_j \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{n} & 0 \\\\ 0 & \\frac{1}{n} \\end{pmatrix} \\begin{pmatrix} \\mathbf{1}^{\\top}\\mathbf{y} \\\\ \\mathbf{g}_{j}^{\\top}\\mathbf{y} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{n}\\mathbf{1}^{\\top}\\mathbf{y} \\\\ \\frac{1}{n}\\mathbf{g}_{j}^{\\top}\\mathbf{y} \\end{pmatrix}\n$$\nSince $\\mathbf{y}$ is also standardized to have mean $0$, $\\mathbf{1}^{\\top}\\mathbf{y} = n\\bar{y} = 0$. Thus, the estimator for the intercept is $\\hat{\\alpha}_{j} = 0$. The estimator for the SNP effect size $\\beta_j$ is:\n$$\n\\hat{\\beta}_{j} = \\frac{1}{n}\\mathbf{g}_{j}^{\\top}\\mathbf{y}\n$$\nThe sampling variance of $\\hat{\\beta}_j$ is given by $\\mathrm{Var}(\\hat{\\beta}_{j}) = (\\sigma_{j}^{2}) [(\\mathbf{X}_{j}^{\\top}\\mathbf{X}_{j})^{-1}]_{22}$, where $(\\sigma_{j}^{2})$ is the variance of the residuals $\\boldsymbol{\\varepsilon}_{j}$. From the full model $\\mathbf{y} = \\mathbf{G}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$, the residual variance is $\\sigma^2$. So, we have $\\mathrm{Var}(\\hat{\\beta}_{j}) = \\sigma^2 \\cdot \\frac{1}{n} = \\frac{\\sigma^2}{n}$.\n\nThe $z$-statistic for SNP $j$ is the ratio of the estimated effect to its standard error, $z_j = \\hat{\\beta}_j / \\mathrm{SE}(\\hat{\\beta}_j)$.\n$$\nz_{j} = \\frac{\\hat{\\beta}_{j}}{\\sqrt{\\sigma^2/n}} = \\frac{\\frac{1}{n}\\mathbf{g}_{j}^{\\top}\\mathbf{y}}{\\sigma/\\sqrt{n}} = \\frac{1}{\\sigma\\sqrt{n}}\\mathbf{g}_{j}^{\\top}\\mathbf{y}\n$$\nUnder the null hypothesis $H_0: \\boldsymbol{\\theta} = \\mathbf{0}$, the full model simplifies to $\\mathbf{y} = \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$. The stated condition that the vector $\\mathbf{y}$ is standardized to have variance $1$ means that under $H_0$, the variance of any component $y_i$ is $1$. Since $\\mathrm{Var}(y_i) = \\mathrm{Var}(\\varepsilon_i) = \\sigma^2$, this implies $\\sigma^2 = 1$.\n\nWith $\\sigma^2=1$, the $z$-statistic becomes:\n$$\nz_{j} = \\frac{1}{\\sqrt{n}}\\mathbf{g}_{j}^{\\top}\\mathbf{y}\n$$\nWe can express the vector of $z$-statistics $\\mathbf{z} = (z_1, z_2, \\ldots, z_p)^{\\top}$ in matrix form.\n$$\n\\mathbf{z} = \\frac{1}{\\sqrt{n}}\\mathbf{G}^{\\top}\\mathbf{y}\n$$\nNow we find the joint distribution of $\\mathbf{z}$ under $H_0$. Under $H_0$, $\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{n})$, since $\\sigma^{2} = 1$. As $\\mathbf{z}$ is a linear transformation of a normally distributed vector $\\mathbf{y}$, $\\mathbf{z}$ itself follows a multivariate normal distribution.\n\nThe mean of $\\mathbf{z}$ is:\n$$\nE[\\mathbf{z}] = E\\left[\\frac{1}{\\sqrt{n}}\\mathbf{G}^{\\top}\\mathbf{y}\\right] = \\frac{1}{\\sqrt{n}}\\mathbf{G}^{\\top}E[\\mathbf{y}] = \\frac{1}{\\sqrt{n}}\\mathbf{G}^{\\top}\\mathbf{0} = \\mathbf{0}\n$$\nThe covariance matrix of $\\mathbf{z}$ is:\n$$\n\\mathrm{Cov}(\\mathbf{z}) = \\mathrm{Var}(\\mathbf{z}) = \\mathrm{Var}\\left(\\frac{1}{\\sqrt{n}}\\mathbf{G}^{\\top}\\mathbf{y}\\right) = \\left(\\frac{1}{\\sqrt{n}}\\mathbf{G}^{\\top}\\right) \\mathrm{Var}(\\mathbf{y}) \\left(\\frac{1}{\\sqrt{n}}\\mathbf{G}^{\\top}\\right)^{\\top}\n$$\nSubstituting $\\mathrm{Var}(\\mathbf{y}) = \\mathbf{I}_{n}$:\n$$\n\\mathrm{Cov}(\\mathbf{z}) = \\frac{1}{n} \\mathbf{G}^{\\top}\\mathbf{I}_{n}\\mathbf{G} = \\frac{1}{n}\\mathbf{G}^{\\top}\\mathbf{G}\n$$\nThe problem defines the Linkage Disequilibrium (LD) matrix as $\\mathbf{R} = \\frac{1}{n}\\mathbf{G}^{\\top}\\mathbf{G}$. Therefore, $\\mathrm{Cov}(\\mathbf{z}) = \\mathbf{R}$.\n\nThe joint sampling distribution of the vector of $z$-statistics under the null hypothesis is a multivariate normal distribution with mean vector $\\mathbf{0}$ and covariance matrix $\\mathbf{R}$:\n$$\n\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R})\n$$\n\n**Part 2: Variance of the contrast $z_1 - z_2$**\n\nWe are asked to compute $\\mathrm{Var}(z_{1} - z_{2})$ for a locus with $p=2$ SNPs, where the LD matrix is given by:\n$$\n\\mathbf{R} = \\begin{pmatrix} 1 & r \\\\ r & 1 \\end{pmatrix}\n$$\nFrom the result of Part 1, the covariance matrix of the vector $\\mathbf{z} = (z_1, z_2)^{\\top}$ is $\\mathbf{R}$. We can write:\n$$\n\\mathrm{Cov}(\\mathbf{z}) = \\begin{pmatrix} \\mathrm{Var}(z_1) & \\mathrm{Cov}(z_1, z_2) \\\\ \\mathrm{Cov}(z_2, z_1) & \\mathrm{Var}(z_2) \\end{pmatrix} = \\begin{pmatrix} 1 & r \\\\ r & 1 \\end{pmatrix}\n$$\nThis gives us $\\mathrm{Var}(z_1) = 1$, $\\mathrm{Var}(z_2) = 1$, and $\\mathrm{Cov}(z_1, z_2) = r$.\n\nUsing the general formula for the variance of a difference of two random variables:\n$$\n\\mathrm{Var}(z_1 - z_2) = \\mathrm{Var}(z_1) + \\mathrm{Var}(z_2) - 2\\mathrm{Cov}(z_1, z_2)\n$$\nSubstituting the values from the covariance matrix:\n$$\n\\mathrm{Var}(z_1 - z_2) = 1 + 1 - 2(r) = 2 - 2r\n$$\nThe variance of the linear contrast $z_1 - z_2$ is $2 - 2r$.",
            "answer": "$$\n\\boxed{2 - 2r}\n$$"
        },
        {
            "introduction": "A central promise of trans-omics is the ability to trace the flow of biological information from the genome outwards. This practice problem  provides a tangible example of this process by modeling how genetic variants influencing gene expression (eQTLs) subsequently impact protein abundance (pQTLs). By partitioning the variance explained by multiple variants and propagating it through a simple mediation model, you will gain hands-on experience in quantifying these cascading effects of genetic regulation.",
            "id": "4395232",
            "problem": "In a trans-omics quantitative trait locus analysis spanning messenger ribonucleic acid (mRNA) and protein for a single gene, you are given three independent cis variants in Hardy–Weinberg equilibrium. Let the genotype dosage for variant $i$ be $G_i \\in \\{0,1,2\\}$ counting the number of effect alleles, with allele frequency $p_i$. The transcript abundance $Y_{m}$ has been standardized to unit variance, $\\operatorname{Var}(Y_{m}) = 1$. The following population quantities are known from a joint additive model of transcript abundance on all three variants:\n- Allele frequencies: $p_1 = 0.20$, $p_2 = 0.35$, $p_3 = 0.50$.\n- Per-allele effects on standardized transcript abundance (from a single joint regression including all three variants, with centered predictors): $b_1 = 0.18$, $b_2 = -0.12$, $b_3 = 0.10$.\n\nAssume:\n- Genotypes $(G_1,G_2,G_3)$ are mutually independent and each follows Hardy–Weinberg equilibrium.\n- The regression is additive with no interaction terms (no epistasis) and uses centered genotypes $G_i - \\mathbb{E}[G_i]$.\n- The residual is uncorrelated with all predictors.\n\nFurther, the protein abundance $Y_{p}$ is modeled by a linear mediation of transcript with independent residual as $Y_{p} = \\alpha Y_{m} + \\eta$, where $\\eta$ is independent of $Y_{m}$ and $\\operatorname{Var}(Y_{p}) = 1$ by standardization. The coupling coefficient is $\\alpha = 0.60$.\n\nUsing only fundamental definitions of variance, covariance, and linear models, and the Central Dogma of molecular biology as conceptual justification for the mediation structure, derive from first principles how to partition the variance in $Y_{m}$ explained by the three variants into additive components, and then propagate this partition through the linear mediation to obtain the proportion of variance in $Y_{p}$ explained by these cis variants via $Y_{m}$. Express your final answer as a decimal proportion and round to four significant figures. Report only the final protein-level proportion of variance explained by the cis variants mediated through transcript in your final answer.",
            "solution": "The objective is to determine the proportion of variance in protein abundance ($Y_p$) that is explained by three independent cis-genetic variants, where their effect is mediated through the abundance of messenger RNA ($Y_m$). This requires a two-step process: first, calculating the variance in $Y_m$ explained by the variants, and second, propagating this explained variance to $Y_p$ through the given mediation model.\n\nStep 1: Calculate the variance of each genotype.\nThe problem states that for each variant $i$, the genotype dosage $G_i$ is an integer in $\\{0, 1, 2\\}$ representing the count of the effect allele. The population is in Hardy-Weinberg equilibrium (HWE) with an effect allele frequency of $p_i$. Under HWE, the probabilities of the three possible genotypes are:\n- $P(G_i=2) = p_i^2$ (homozygous for the effect allele)\n- $P(G_i=1) = 2p_i(1-p_i)$ (heterozygous)\n- $P(G_i=0) = (1-p_i)^2$ (homozygous for the non-effect allele)\n\nFrom first principles, the expectation of the genotype dosage $G_i$ is:\n$$ \\mathbb{E}[G_i] = (2 \\cdot p_i^2) + (1 \\cdot 2p_i(1-p_i)) + (0 \\cdot (1-p_i)^2) = 2p_i^2 + 2p_i - 2p_i^2 = 2p_i $$\nThe variance of $G_i$ is $\\operatorname{Var}(G_i) = \\mathbb{E}[G_i^2] - (\\mathbb{E}[G_i])^2$. First we compute the second moment:\n$$ \\mathbb{E}[G_i^2] = (2^2 \\cdot p_i^2) + (1^2 \\cdot 2p_i(1-p_i)) + (0^2 \\cdot (1-p_i)^2) = 4p_i^2 + 2p_i - 2p_i^2 = 2p_i^2 + 2p_i $$\nTherefore, the variance is:\n$$ \\operatorname{Var}(G_i) = (2p_i^2 + 2p_i) - (2p_i)^2 = 2p_i^2 + 2p_i - 4p_i^2 = 2p_i - 2p_i^2 = 2p_i(1-p_i) $$\nUsing the given allele frequencies:\n- For variant $1$, $p_1 = 0.20$.\n  $\\operatorname{Var}(G_1) = 2p_1(1-p_1) = 2(0.20)(1-0.20) = 2(0.20)(0.80) = 0.32$.\n- For variant $2$, $p_2 = 0.35$.\n  $\\operatorname{Var}(G_2) = 2p_2(1-p_2) = 2(0.35)(1-0.35) = 2(0.35)(0.65) = 0.455$.\n- For variant $3$, $p_3 = 0.50$.\n  $\\operatorname{Var}(G_3) = 2p_3(1-p_3) = 2(0.50)(1-0.50) = 2(0.50)(0.50) = 0.50$.\n\nStep 2: Calculate the proportion of variance in transcript abundance ($Y_m$) explained by the variants.\nThe transcript abundance $Y_m$ is modeled as a linear additive function of the centered genotypes:\n$$ Y_m = \\beta_0 + \\sum_{i=1}^{3} b_i (G_i - \\mathbb{E}[G_i]) + \\epsilon_m $$\nwhere $b_i$ are the per-allele effects, and $\\epsilon_m$ is the residual term, uncorrelated with the genotypes. The total variance of $Y_m$ can be partitioned. The part of $Y_m$ explained by the genetics is $Y_{m,g} = \\sum_{i=1}^{3} b_i (G_i - \\mathbb{E}[G_i])$. The variance of this genetic component is:\n$$ \\operatorname{Var}(Y_{m,g}) = \\operatorname{Var}\\left(\\sum_{i=1}^{3} b_i (G_i - \\mathbb{E}[G_i])\\right) $$\nSince the variants ($G_1, G_2, G_3$) are given to be mutually independent, the covariance between any two terms $b_i(G_i - \\mathbb{E}[G_i])$ and $b_j(G_j - \\mathbb{E}[G_j])$ for $i \\neq j$ is zero. Therefore, the variance of the sum is the sum of the variances:\n$$ \\operatorname{Var}(Y_{m,g}) = \\sum_{i=1}^{3} \\operatorname{Var}(b_i (G_i - \\mathbb{E}[G_i])) = \\sum_{i=1}^{3} b_i^2 \\operatorname{Var}(G_i - \\mathbb{E}[G_i]) = \\sum_{i=1}^{3} b_i^2 \\operatorname{Var}(G_i) $$\nThe proportion of variance in $Y_m$ explained by the variants, denoted $R^2_m$, is the ratio of the genetic variance to the total variance:\n$$ R^2_m = \\frac{\\operatorname{Var}(Y_{m,g})}{\\operatorname{Var}(Y_m)} $$\nGiven that $Y_m$ is standardized to unit variance, $\\operatorname{Var}(Y_m) = 1$. Thus, $R^2_m = \\operatorname{Var}(Y_{m,g})$.\nSubstituting the given effect sizes $b_1 = 0.18$, $b_2 = -0.12$, $b_3 = 0.10$ and the calculated genotype variances:\n$$ R^2_m = b_1^2 \\operatorname{Var}(G_1) + b_2^2 \\operatorname{Var}(G_2) + b_3^2 \\operatorname{Var}(G_3) $$\n$$ R^2_m = (0.18)^2(0.32) + (-0.12)^2(0.455) + (0.10)^2(0.50) $$\n$$ R^2_m = (0.0324)(0.32) + (0.0144)(0.455) + (0.01)(0.50) $$\n$$ R^2_m = 0.010368 + 0.006552 + 0.005 $$\n$$ R^2_m = 0.02192 $$\n\nStep 3: Propagate the explained variance through the mediation model to protein abundance ($Y_p$).\nThe protein abundance $Y_p$ is modeled as a linear function of $Y_m$:\n$$ Y_p = \\alpha Y_m + \\eta $$\nwhere $\\alpha = 0.60$ is the coupling coefficient and $\\eta$ is a residual term independent of $Y_m$.\nTo find the component of $Y_p$ that is explained by the cis variants, we substitute the decomposition of $Y_m$ into the equation. Let $Y_m = Y_{m,g} + \\epsilon'_m$, where $Y_{m,g}$ is the genetic component and $\\epsilon'_m$ is the non-genetic component (residual, including the intercept).\n$$ Y_p = \\alpha (Y_{m,g} + \\epsilon'_m) + \\eta = \\alpha Y_{m,g} + \\alpha \\epsilon'_m + \\eta $$\nThe term representing the genetic effect of the cis variants on protein abundance, mediated via the transcript, is $\\alpha Y_{m,g}$.\nThe variance of this component is:\n$$ \\operatorname{Var}(\\alpha Y_{m,g}) = \\alpha^2 \\operatorname{Var}(Y_{m,g}) = \\alpha^2 R^2_m $$\nThe proportion of variance in $Y_p$ explained by the cis variants via $Y_m$ is the ratio of this variance to the total variance of $Y_p$:\n$$ \\text{Proportion} = \\frac{\\operatorname{Var}(\\alpha Y_{m,g})}{\\operatorname{Var}(Y_p)} $$\nThe problem states that $Y_p$ is also standardized to unit variance, so $\\operatorname{Var}(Y_p) = 1$. The desired proportion is therefore:\n$$ \\text{Proportion} = \\frac{\\alpha^2 R^2_m}{1} = \\alpha^2 R^2_m $$\nSubstituting the values for $\\alpha$ and $R^2_m$:\n$$ \\text{Proportion} = (0.60)^2 \\times 0.02192 = 0.36 \\times 0.02192 = 0.0078912 $$\nFinally, rounding the result to four significant figures gives $0.007891$.\nThis value represents the proportion of variance in protein abundance that can be attributed to the given cis-acting genetic variants, with their influence being transmitted through their effect on transcript abundance.",
            "answer": "$$\n\\boxed{0.007891}\n$$"
        },
        {
            "introduction": "Identifying a statistically significant QTL is only the first step; accurately estimating its effect size is equally important for understanding its biological impact. This exercise  addresses the \"winner's curse,\" a pervasive selection bias that inflates the effect sizes of top-ranked associations discovered in genome-wide screens. You will derive the mathematical basis for this bias and apply a conditional likelihood approach to obtain a more accurate, bias-corrected estimate of the true genetic effect.",
            "id": "4395285",
            "problem": "A multi-layer trans-omics study investigates an additive genotype-to-metabolite quantitative trait locus in a systems biomedicine cohort. The single nucleotide variant dosage $g \\in \\{0,1,2\\}$ is regressed on the natural logarithm of metabolite concentration $m$ (metabolomics layer), adjusting for covariates, in a linear model whose maximum likelihood estimator of the per-allele effect is denoted by $\\hat{\\beta}$. Under standard regularity conditions for linear regression with independent, identically distributed errors, assume the large-sample distribution $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, s^{2})$, where $\\beta$ is the true per-allele log-fold change and $s$ is the standard error.\n\nDiscovery uses a genome-wide screening rule applied at the metabolomics layer: only loci with a positive estimated effect surpassing a standardized threshold are carried forward for multi-omics integration. Specifically, let $z_{\\star}$ be the standardized cutoff and let $c = z_{\\star} s$; the selection event is $\\hat{\\beta} > c$. This selection rule can inflate $\\hat{\\beta}$ among discovered loci (“winner’s curse”).\n\nGiven a discovered locus with observed $\\hat{\\beta} = 0.90$ and $s = 0.12$, and a screening threshold $z_{\\star} = 5$ (so $c = z_{\\star} s = 0.60$), do the following:\n\n1. Starting from the asymptotic normality of $\\hat{\\beta}$ and the selection event $\\hat{\\beta} > c$, derive the conditional expectation $\\mathbb{E}[\\hat{\\beta} \\mid \\hat{\\beta} > c]$ and show that the difference $\\mathbb{E}[\\hat{\\beta} \\mid \\hat{\\beta} > c] - \\beta$ is strictly positive for any finite $\\beta$ and $s > 0$, thereby quantifying the winner’s curse.\n\n2. Using the conditional (truncated) likelihood for a single observed $\\hat{\\beta} = x$ under the event $\\hat{\\beta} > c$, derive the estimating equation for the bias-corrected effect $\\tilde{\\beta}$ by maximizing the conditional likelihood with respect to $\\beta$. Then, using $x = 0.90$, $s = 0.12$, and $c = 0.60$, solve this estimating equation numerically to obtain the conditional-likelihood corrected estimate $\\tilde{\\beta}$.\n\n3. Propose a parametric bootstrap scheme (conceptually, without running simulations) that would estimate and subtract the selection-induced bias for the same setting, and explain how it relates to the truncated-normal expectation derived in part 1.\n\nReport the final corrected per-allele effect $\\tilde{\\beta}$ from part 2 as a single numerical value. Round your answer to four significant figures. Express the corrected effect as a natural-log fold change per allele.",
            "solution": "The problem is first assessed for validity.\n\n### Step 1: Extract Givens\n-   The estimator for the per-allele effect, $\\hat{\\beta}$, follows a large-sample normal distribution: $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, s^{2})$, where $\\beta$ is the true effect and $s$ is the standard error.\n-   The selection rule for a locus to be carried forward is $\\hat{\\beta} > c$.\n-   The selection cutoff is defined as $c = z_{\\star} s$.\n-   Observed data for a single discovered locus:\n    -   Observed effect estimate: an observation of $\\hat{\\beta}$ which we denote as $x = 0.90$.\n    -   Standard error: $s = 0.12$.\n    -   Standardized screening threshold: $z_{\\star} = 5$.\n-   From the given values, the selection cutoff is $c = z_{\\star} s = 5 \\times 0.12 = 0.60$. The selection event is thus $\\hat{\\beta} > 0.60$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the well-established statistical field of quantitative genetics and addresses the common issue of selection bias, or \"winner's curse,\" in genome-wide association studies. The use of a normal approximation for the regression coefficient $\\hat{\\beta}$ is standard under asymptotic theory. The problem is well-posed, providing all necessary parameters and a clear set of tasks (a derivation, a numerical calculation, and a conceptual explanation). The language is objective and precise. The provided data are internally consistent (the observed value $x=0.90$ satisfies the selection criterion $\\hat{\\beta} > 0.60$) and physically plausible within the context of genetic association studies. The problem is therefore deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution is provided below.\n\n---\n\nThe solution is presented in three parts as requested.\n\n### Part 1: Conditional Expectation and Winner's Curse\n\nLet $Y$ be a random variable representing the estimator $\\hat{\\beta}$, so $Y \\sim \\mathcal{N}(\\beta, s^{2})$. Its probability density function (PDF) is $f_Y(y) = \\frac{1}{s\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\beta)^2}{2s^2}\\right)$. Let $\\phi(\\cdot)$ and $\\Phi(\\cdot)$ denote the PDF and cumulative distribution function (CDF) of the standard normal distribution, $\\mathcal{N}(0, 1)$, respectively. The PDF of $Y$ can be written as $f_Y(y) = \\frac{1}{s} \\phi\\left(\\frac{y-\\beta}{s}\\right)$.\n\nThe probability of the selection event is:\n$$ P(Y > c) = \\int_c^{\\infty} \\frac{1}{s} \\phi\\left(\\frac{y-\\beta}{s}\\right) dy $$\nLet $u = \\frac{y-\\beta}{s}$, so $dy = s du$. The lower integration limit becomes $\\alpha = \\frac{c-\\beta}{s}$.\n$$ P(Y > c) = \\int_{\\alpha}^{\\infty} \\phi(u) du = 1 - \\Phi(\\alpha) = 1 - \\Phi\\left(\\frac{c-\\beta}{s}\\right) $$\nThe conditional PDF of $Y$ given $Y > c$ is:\n$$ f_{Y|Y>c}(y) = \\frac{f_Y(y)}{P(Y > c)} = \\frac{\\frac{1}{s}\\phi\\left(\\frac{y-\\beta}{s}\\right)}{1 - \\Phi\\left(\\frac{c-\\beta}{s}\\right)} \\quad \\text{for } y > c $$\nThe conditional expectation $\\mathbb{E}[Y \\mid Y > c]$ is:\n$$ \\mathbb{E}[Y \\mid Y > c] = \\int_c^{\\infty} y f_{Y|Y>c}(y) dy = \\frac{1}{1 - \\Phi(\\alpha)} \\int_c^{\\infty} y \\frac{1}{s} \\phi\\left(\\frac{y-\\beta}{s}\\right) dy $$\nUsing the same substitution $u = \\frac{y-\\beta}{s}$ (so $y = su + \\beta$):\n$$ \\int_c^{\\infty} y \\frac{1}{s} \\phi\\left(\\frac{y-\\beta}{s}\\right) dy = \\int_{\\alpha}^{\\infty} (su+\\beta) \\phi(u) du = s \\int_{\\alpha}^{\\infty} u\\phi(u)du + \\beta \\int_{\\alpha}^{\\infty} \\phi(u)du $$\nThe second integral is $\\beta (1 - \\Phi(\\alpha))$. For the first integral, we note that $\\frac{d}{du}\\phi(u) = -u\\phi(u)$.\n$$ s \\int_{\\alpha}^{\\infty} u\\phi(u)du = -s \\int_{\\alpha}^{\\infty} (-\\phi'(u))du = s [-\\phi(u)]_{\\alpha}^{\\infty} = s(0 - (-\\phi(\\alpha))) = s\\phi(\\alpha) $$\nCombining the terms, the numerator becomes $s\\phi(\\alpha) + \\beta(1-\\Phi(\\alpha))$. Dividing by the denominator $P(Y>c)=1-\\Phi(\\alpha)$:\n$$ \\mathbb{E}[\\hat{\\beta} \\mid \\hat{\\beta} > c] = \\frac{s\\phi(\\alpha) + \\beta(1-\\Phi(\\alpha))}{1 - \\Phi(\\alpha)} = \\beta + s \\frac{\\phi(\\alpha)}{1 - \\Phi(\\alpha)} $$\nSubstituting back $\\alpha = \\frac{c-\\beta}{s}$:\n$$ \\mathbb{E}[\\hat{\\beta} \\mid \\hat{\\beta} > c] = \\beta + s \\frac{\\phi\\left(\\frac{c-\\beta}{s}\\right)}{1-\\Phi\\left(\\frac{c-\\beta}{s}\\right)} $$\nThe term $\\frac{\\phi(z)}{1-\\Phi(z)}$ is known as the inverse Mills ratio. The selection bias, or \"winner's curse\", is the difference between the conditional expectation and the true parameter $\\beta$:\n$$ \\text{Bias} = \\mathbb{E}[\\hat{\\beta} \\mid \\hat{\\beta} > c] - \\beta = s \\frac{\\phi\\left(\\frac{c-\\beta}{s}\\right)}{1-\\Phi\\left(\\frac{c-\\beta}{s}\\right)} $$\nTo show this bias is strictly positive, we analyze its components. The standard error $s$ is given to be greater than $0$. The standard normal PDF $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$ is strictly positive for all real $z$. The standard normal CDF $\\Phi(z)$ is a strictly increasing function with range $(0,1)$, which means $1 - \\Phi(z)$ is strictly positive for all real $z$. Since the bias is a product and ratio of strictly positive quantities, it is strictly positive for any finite $\\beta$ and $s > 0$.\n\n### Part 2: Conditional Likelihood Estimation\n\nGiven a single observation $x$ of $\\hat{\\beta}$ that is known to have been selected because $\\hat{\\beta} > c$, the relevant likelihood is the conditional likelihood. This is based on the conditional PDF derived in Part 1, evaluated at $x$:\n$$ L(\\beta; x | \\hat{\\beta} > c) = f_{\\hat{\\beta}|\\hat{\\beta}>c}(x) = \\frac{\\frac{1}{s}\\phi\\left(\\frac{x-\\beta}{s}\\right)}{1-\\Phi\\left(\\frac{c-\\beta}{s}\\right)} $$\nTo find the maximum likelihood estimator (MLE) for $\\beta$ under this conditional model, which we denote $\\tilde{\\beta}$, we maximize the log-likelihood function $\\ell(\\beta) = \\ln L(\\beta; x)$.\n$$ \\ell(\\beta) = \\ln\\left(\\frac{1}{s}\\phi\\left(\\frac{x-\\beta}{s}\\right)\\right) - \\ln\\left(1-\\Phi\\left(\\frac{c-\\beta}{s}\\right)\\right) $$\n$$ \\ell(\\beta) = -\\ln(s) - \\ln(\\sqrt{2\\pi}) - \\frac{(x-\\beta)^2}{2s^2} - \\ln\\left(1-\\Phi\\left(\\frac{c-\\beta}{s}\\right)\\right) $$\nWe differentiate $\\ell(\\beta)$ with respect to $\\beta$ and set the result to $0$. Let $u = \\frac{c-\\beta}{s}$, so $\\frac{du}{d\\beta} = -\\frac{1}{s}$.\n$$ \\frac{\\partial\\ell}{\\partial\\beta} = -\\frac{2(x-\\beta)(-1)}{2s^2} - \\frac{1}{1-\\Phi(u)} \\cdot \\frac{d}{d\\beta}\\left(1-\\Phi(u)\\right) $$\n$$ \\frac{\\partial\\ell}{\\partial\\beta} = \\frac{x-\\beta}{s^2} - \\frac{1}{1-\\Phi(u)} \\left(-\\phi(u) \\frac{du}{d\\beta}\\right) = \\frac{x-\\beta}{s^2} - \\frac{\\phi(u)}{1-\\Phi(u)} \\left(-\\frac{1}{s}\\right) $$\n$$ \\frac{\\partial\\ell}{\\partial\\beta} = \\frac{x-\\beta}{s^2} - \\frac{1}{s} \\frac{\\phi\\left(\\frac{c-\\beta}{s}\\right)}{1-\\Phi\\left(\\frac{c-\\beta}{s}\\right)} $$\nSetting the derivative to zero yields the estimating equation for $\\tilde{\\beta}$:\n$$ \\frac{x-\\tilde{\\beta}}{s^2} = \\frac{1}{s} \\frac{\\phi\\left(\\frac{c-\\tilde{\\beta}}{s}\\right)}{1-\\Phi\\left(\\frac{c-\\tilde{\\beta}}{s}\\right)} $$\n$$ x - \\tilde{\\beta} = s \\frac{\\phi\\left(\\frac{c-\\tilde{\\beta}}{s}\\right)}{1-\\Phi\\left(\\frac{c-\\tilde{\\beta}}{s}\\right)} $$\nThis can be rearranged to $\\tilde{\\beta} = x - s \\frac{\\phi\\left(\\frac{c-\\tilde{\\beta}}{s}\\right)}{1-\\Phi\\left(\\frac{c-\\tilde{\\beta}}{s}\\right)}$. The correction term is the analytical bias from Part 1, evaluated at the corrected estimate $\\tilde{\\beta}$.\n\nWe solve this equation numerically using the given values: $x=0.90$, $s=0.12$, and $c=0.60$.\n$$ \\tilde{\\beta} = 0.90 - 0.12 \\frac{\\phi\\left(\\frac{0.60-\\tilde{\\beta}}{0.12}\\right)}{1-\\Phi\\left(\\frac{0.60-\\tilde{\\beta}}{0.12}\\right)} $$\nThis is a nonlinear equation that can be solved with a fixed-point iteration or a root-finding algorithm. Let $\\beta_0 = x = 0.90$ be the initial guess.\nThe first iteration is:\n$$ \\beta_1 = 0.90 - 0.12 \\frac{\\phi\\left(\\frac{0.60-0.90}{0.12}\\right)}{1-\\Phi\\left(\\frac{0.60-0.90}{0.12}\\right)} = 0.90 - 0.12 \\frac{\\phi(-2.5)}{1-\\Phi(-2.5)} $$\nUsing standard normal table values, $\\phi(-2.5) \\approx 0.017528$ and $\\Phi(-2.5) \\approx 0.006210$. Note that $1 - \\Phi(-2.5) = \\Phi(2.5) \\approx 0.99379$.\n$$ \\beta_1 \\approx 0.90 - 0.12 \\left(\\frac{0.017528}{0.99379}\\right) \\approx 0.90 - 0.12(0.017637) \\approx 0.90 - 0.002116 = 0.897884 $$\nContinuing this iterative process until convergence:\n$\\beta_2 \\approx 0.897791$\n$\\beta_3 \\approx 0.897786$\nThe sequence converges rapidly. The solution for $\\tilde{\\beta}$, rounded to four significant figures, is $0.8978$.\n\n### Part 3: Parametric Bootstrap Scheme\n\nA parametric bootstrap can be used to estimate the selection bias and correct the observed estimate $x$. The goal is to estimate the bias term $\\mathbb{E}[\\hat{\\beta} \\mid \\hat{\\beta} > c] - \\beta$, which depends on the unknown true $\\beta$.\n\nThe conceptual scheme is as follows:\n1.  **Initial Estimate**: Obtain an initial, plausible estimate for the true effect $\\beta$. A suitable choice is the conditional MLE, $\\tilde{\\beta} \\approx 0.8978$, calculated in Part 2. Let's denote this by $\\hat{\\beta}_{\\text{init}}$.\n\n2.  **Bootstrap Simulation**: Perform a large number of bootstrap replications, $B$ (e.g., $B=10,000$). For each replication $b=1, \\dots, B$:\n    a.  **Generate Data**: Simulate a new effect estimate $\\hat{\\beta}^{*}_b$ from the assumed sampling distribution, using the initial estimate as the true mean: $\\hat{\\beta}^{*}_b \\sim \\mathcal{N}(\\hat{\\beta}_{\\text{init}}, s^2)$. Here, a sample is drawn from $\\mathcal{N}((0.8978), (0.12)^2)$.\n    b.  **Apply Selection**: Mimic the study's discovery process by applying the selection rule. Keep $\\hat{\\beta}^{*}_b$ only if it exceeds the cutoff $c$. That is, if $\\hat{\\beta}^{*}_b > 0.60$.\n\n3.  **Bias Estimation**: After all $B$ replications, compute the average of the *selected* bootstrap estimates. Let this set be $S = \\{\\hat{\\beta}^{*}_b \\mid \\hat{\\beta}^{*}_b > c\\}$. The average is $\\bar{\\beta}^{*} = \\frac{1}{|S|} \\sum_{\\hat{\\beta}^{*} \\in S} \\hat{\\beta}^{*}$.\n    The estimated bias is the difference between this conditional mean and the mean used for simulation: $\\widehat{\\text{Bias}} = \\bar{\\beta}^{*} - \\hat{\\beta}_{\\text{init}}$.\n\n4.  **Bias Correction**: Subtract the estimated bias from the original, uncorrected observation $x$:\n    $$ \\hat{\\beta}_{\\text{corrected}} = x - \\widehat{\\text{Bias}} = 0.90 - (\\bar{\\beta}^{*} - 0.8978) $$\n\n**Relation to Part 1**: The parametric bootstrap provides a numerical, Monte Carlo approximation to the analytical results of Part 1. The average of the selected bootstrap samples, $\\bar{\\beta}^{*}$, is a numerical estimate of the conditional expectation $\\mathbb{E}[\\hat{\\beta} \\mid \\hat{\\beta} > c]$ when the true effect is assumed to be $\\hat{\\beta}_{\\text{init}}$. The estimated bias, $\\widehat{\\text{Bias}}$, is therefore a numerical estimate of the analytical bias quantity derived in Part 1:\n$$ \\widehat{\\text{Bias}} = \\bar{\\beta}^{*} - \\hat{\\beta}_{\\text{init}} \\approx s \\frac{\\phi\\left(\\frac{c-\\hat{\\beta}_{\\text{init}}}{s}\\right)}{1-\\Phi\\left(\\frac{c-\\hat{\\beta}_{\\text{init}}}{s}\\right)} $$\nEssentially, the bootstrap procedure simulates the data generation and selection process to empirically measure the inflation (winner's curse) that the analytical formula from Part 1 describes in closed form.",
            "answer": "$$\\boxed{0.8978}$$"
        }
    ]
}