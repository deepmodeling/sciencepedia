{
    "hands_on_practices": [
        {
            "introduction": "The first step in any robust cell type annotation pipeline is confronting the reality of technical artifacts. In single-cell sequencing, one of the most common issues is the \"doublet\"—two cells being captured and profiled as one. This practice  moves beyond simply running a pre-built tool, challenging you to derive a generative model for doublets from first principles and use a likelihood ratio test to distinguish them from true single cells. This exercise builds a foundational understanding of how statistical modeling can be used to identify and handle technical noise in your data.",
            "id": "4324320",
            "problem": "In single-cell RNA sequencing (scRNA-seq), cell type identification and annotation often rely on a generative model for gene counts grounded in molecular sampling: transcripts are captured from a cell and sequenced, yielding counts per gene. A well-tested starting point is to model gene-level counts as independent Poisson random variables with rates proportional to a cell’s transcriptome profile scaled by a library size parameter. A doublet is a technical artifact in which two cells are captured together and sequenced as if they were one; in such cases, the observed profile is expected to be a convex combination of two single-cell profiles. Consider a two-gene system with reference profiles for two known cell types, labeled A and B. Let the reference profiles be probability vectors over genes, denoted by $q_{\\mathrm{A}} = (q_{\\mathrm{A},1}, q_{\\mathrm{A},2})$ and $q_{\\mathrm{B}} = (q_{\\mathrm{B},1}, q_{\\mathrm{B},2})$, each satisfying $q_{\\mathrm{A},1} + q_{\\mathrm{A},2} = 1$ and $q_{\\mathrm{B},1} + q_{\\mathrm{B},2} = 1$. Assume an observed cell has gene counts $x = (x_{1}, x_{2})$ with $x_{1}, x_{2} \\in \\mathbb{N}$ and total count $n = x_{1} + x_{2}$. Under the singlet model, $x_{g} \\sim \\mathrm{Poisson}(s \\, q_{k,g})$ independently across genes $g \\in \\{1,2\\}$ for some $k \\in \\{\\mathrm{A}, \\mathrm{B}\\}$ and unknown library size $s > 0$. Under the doublet model, $x_{g} \\sim \\mathrm{Poisson}(s \\, p_{g}(\\alpha))$ independently with $p(\\alpha) = \\alpha \\, q_{\\mathrm{A}} + (1 - \\alpha) \\, q_{\\mathrm{B}}$ for some unknown mixing weight $\\alpha \\in [0,1]$ and unknown library size $s > 0$. All logarithms are natural logarithms.\n  \nStarting from these assumptions and without invoking any ready-made doublet-detection formulas, derive the log-likelihoods under the singlet and doublet models, obtain the maximum likelihood estimators for the nuisance library size parameter $s$ in each model, and deduce the profile log-likelihoods in terms of the profile probabilities only. Then, for the specific data and references\n$q_{\\mathrm{A}} = (0.8, 0.2)$, $q_{\\mathrm{B}} = (0.2, 0.8)$, and $x = (8, 8)$,\ncarry out the following steps:\n\n1. Determine the maximized singlet log-likelihood by choosing the more likely of A or B.\n2. Determine the maximized doublet log-likelihood by optimizing over $\\alpha \\in [0,1]$ and $s > 0$.\n3. Define the Likelihood Ratio Test (LRT) statistic for testing the singlet model against the doublet alternative as $2$ times the difference between the maximized doublet and singlet log-likelihoods. Compute this statistic for the given $q_{\\mathrm{A}}$, $q_{\\mathrm{B}}$, and $x$.\n\nExpress your final answer as a single dimensionless real number equal to the LRT statistic, rounded to four significant figures. No other output is requested.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of single-cell transcriptomics, well-posed with sufficient information for a unique solution, and expressed in objective, formal language. We may therefore proceed with the derivation and calculation.\n\nFirst, we derive the general expressions for the log-likelihoods and the maximum likelihood estimator (MLE) for the library size parameter $s$.\n\nLet the observed gene counts be $x = (x_1, x_2)$ with total count $n = x_1 + x_2$.\nLet a general cell profile be given by the probability vector $\\theta = (\\theta_1, \\theta_2)$ where $\\theta_1 + \\theta_2 = 1$. The counts $x_g$ for gene $g \\in \\{1, 2\\}$ are modeled as independent Poisson random variables:\n$$x_g \\sim \\mathrm{Poisson}(s \\theta_g)$$\nThe likelihood function $L(s, \\theta | x)$ is the product of the probabilities of observing $x_1$ and $x_2$:\n$$L(s, \\theta | x) = \\frac{\\exp(-s \\theta_1) (s \\theta_1)^{x_1}}{x_1!} \\cdot \\frac{\\exp(-s \\theta_2) (s \\theta_2)^{x_2}}{x_2!}$$\nThis can be simplified by combining terms:\n$$L(s, \\theta | x) = \\frac{\\exp(-s(\\theta_1 + \\theta_2)) s^{x_1+x_2} \\theta_1^{x_1} \\theta_2^{x_2}}{x_1! x_2!}$$\nSince $\\theta_1 + \\theta_2 = 1$ and $n = x_1 + x_2$, the likelihood becomes:\n$$L(s, \\theta | x) = \\frac{\\exp(-s) s^n \\theta_1^{x_1} \\theta_2^{x_2}}{x_1! x_2!}$$\nThe log-likelihood, denoted $\\ell(s, \\theta | x)$, is the natural logarithm of the likelihood:\n$$\\ell(s, \\theta | x) = -s + n \\ln(s) + x_1 \\ln(\\theta_1) + x_2 \\ln(\\theta_2) - \\ln(x_1!) - \\ln(x_2!)$$\nTo find the MLE for the nuisance library size parameter $s$, we take the partial derivative of $\\ell$ with respect to $s$ and set it to zero:\n$$\\frac{\\partial \\ell}{\\partial s} = -1 + \\frac{n}{s} = 0$$\nThis yields the MLE for $s$, which we denote as $\\hat{s}$:\n$$\\hat{s} = n$$\nSubstituting $\\hat{s} = n$ back into the log-likelihood function gives the profile log-likelihood, which depends only on the profile probabilities $\\theta$:\n$$\\ell_{profile}(\\theta | x) = \\ell(\\hat{s}, \\theta | x) = -n + n \\ln(n) + x_1 \\ln(\\theta_1) + x_2 \\ln(\\theta_2) - \\ln(x_1!) - \\ln(x_2!)$$\nThe term $-n + n \\ln(n) - \\ln(x_1!) - \\ln(x_2!)$ is a constant with respect to the cell profile parameters. Let us denote this constant by $C$.\n$$\\ell_{profile}(\\theta | x) = C + x_1 \\ln(\\theta_1) + x_2 \\ln(\\theta_2)$$\nNow we apply this general framework to the singlet and doublet models for the given data.\n\nThe specific data are $q_{\\mathrm{A}} = (0.8, 0.2)$, $q_{\\mathrm{B}} = (0.2, 0.8)$, and $x = (8, 8)$.\nFrom the data, we have $x_1 = 8$, $x_2 = 8$, and the total count is $n = x_1 + x_2 = 16$.\nThe MLE for the library size is $\\hat{s} = n = 16$ for both models.\n\n1. Maximized Singlet Log-Likelihood\nUnder the singlet model, the cell is of type A or type B. The profile $\\theta$ is either $q_{\\mathrm{A}}$ or $q_{\\mathrm{B}}$. We calculate the profile log-likelihood for each and take the maximum.\nFor type A, $\\theta = q_{\\mathrm{A}} = (0.8, 0.2)$:\n$$\\ell_{singlet, \\mathrm{A}} = C + x_1 \\ln(q_{\\mathrm{A},1}) + x_2 \\ln(q_{\\mathrm{A},2}) = C + 8 \\ln(0.8) + 8 \\ln(0.2) = C + 8 \\ln(0.8 \\times 0.2) = C + 8 \\ln(0.16)$$\nFor type B, $\\theta = q_{\\mathrm{B}} = (0.2, 0.8)$:\n$$\\ell_{singlet, \\mathrm{B}} = C + x_1 \\ln(q_{\\mathrm{B},1}) + x_2 \\ln(q_{\\mathrm{B},2}) = C + 8 \\ln(0.2) + 8 \\ln(0.8) = C + 8 \\ln(0.2 \\times 0.8) = C + 8 \\ln(0.16)$$\nThe log-likelihoods are identical. Thus, the maximized singlet log-likelihood is:\n$$\\ell_{singlet, max} = \\max(\\ell_{singlet, \\mathrm{A}}, \\ell_{singlet, \\mathrm{B}}) = C + 8 \\ln(0.16)$$\n\n2. Maximized Doublet Log-Likelihood\nUnder the doublet model, the profile is a mixture $p(\\alpha) = \\alpha q_{\\mathrm{A}} + (1 - \\alpha) q_{\\mathrm{B}}$ for $\\alpha \\in [0,1]$.\nThe components of $p(\\alpha)$ are:\n$$p_1(\\alpha) = \\alpha q_{\\mathrm{A},1} + (1-\\alpha) q_{\\mathrm{B},1} = \\alpha(0.8) + (1-\\alpha)(0.2) = 0.6\\alpha + 0.2$$\n$$p_2(\\alpha) = \\alpha q_{\\mathrm{A},2} + (1-\\alpha) q_{\\mathrm{B},2} = \\alpha(0.2) + (1-\\alpha)(0.8) = 0.8 - 0.6\\alpha$$\nThe profile log-likelihood for the doublet model is a function of $\\alpha$:\n$$\\ell_{doublet}(\\alpha) = C + x_1 \\ln(p_1(\\alpha)) + x_2 \\ln(p_2(\\alpha)) = C + 8 \\ln(0.6\\alpha + 0.2) + 8 \\ln(0.8 - 0.6\\alpha)$$\nTo find the maximized doublet log-likelihood, we must find the value of $\\alpha \\in [0,1]$ that maximizes $\\ell_{doublet}(\\alpha)$. This is equivalent to maximizing the term $V(\\alpha) = 8 \\ln(p_1(\\alpha)) + 8 \\ln(p_2(\\alpha))$.\n$$\\frac{dV}{d\\alpha} = 8 \\left( \\frac{0.6}{0.6\\alpha + 0.2} - \\frac{0.6}{0.8 - 0.6\\alpha} \\right)$$\nSetting the derivative to zero:\n$$\\frac{0.6}{0.6\\alpha + 0.2} = \\frac{0.6}{0.8 - 0.6\\alpha} \\implies 0.6\\alpha + 0.2 = 0.8 - 0.6\\alpha$$\n$$1.2\\alpha = 0.6 \\implies \\hat{\\alpha} = 0.5$$\nSince $\\hat{\\alpha}=0.5$ is within the valid range $[0,1]$, it is the MLE for $\\alpha$. We evaluate the profile at $\\hat{\\alpha} = 0.5$:\n$$p_1(0.5) = 0.6(0.5) + 0.2 = 0.3 + 0.2 = 0.5$$\n$$p_2(0.5) = 0.8 - 0.6(0.5) = 0.8 - 0.3 = 0.5$$\nThe maximized doublet log-likelihood is obtained by substituting $\\hat{\\alpha}=0.5$ into $\\ell_{doublet}(\\alpha)$:\n$$\\ell_{doublet, max} = C + 8 \\ln(0.5) + 8 \\ln(0.5) = C + 16 \\ln(0.5)$$\n\n3. Likelihood Ratio Test (LRT) Statistic\nThe LRT statistic, $\\Lambda$, is defined as $2$ times the difference between the maximized doublet and singlet log-likelihoods.\n$$\\Lambda = 2 (\\ell_{doublet, max} - \\ell_{singlet, max})$$\nSubstituting the expressions derived above:\n$$\\Lambda = 2 \\left( (C + 16 \\ln(0.5)) - (C + 8 \\ln(0.16)) \\right)$$\nThe constant $C$ cancels out:\n$$\\Lambda = 2 (16 \\ln(0.5) - 8 \\ln(0.16)) = 32 \\ln(0.5) - 16 \\ln(0.16)$$\nUsing logarithm properties, $\\ln(a^b) = b\\ln(a)$ and $\\ln(a) - \\ln(b) = \\ln(a/b)$:\n$$\\Lambda = 16 (2 \\ln(0.5) - \\ln(0.16)) = 16 (\\ln(0.5^2) - \\ln(0.16))$$\n$$\\Lambda = 16 (\\ln(0.25) - \\ln(0.16)) = 16 \\ln\\left(\\frac{0.25}{0.16}\\right)$$\nThis simplifies to:\n$$\\Lambda = 16 \\ln\\left(\\frac{25}{16}\\right)$$\nNow we compute the numerical value:\n$$\\Lambda = 16 \\ln(1.5625) \\approx 16 \\times 0.4462871 \\approx 7.1405936$$\nRounding to four significant figures, we get $7.141$.",
            "answer": "$$\\boxed{7.141}$$"
        },
        {
            "introduction": "Once technical artifacts are addressed, the next challenge is to disentangle meaningful biological signals of cell identity from other sources of biological variation. The cell cycle is a classic example of a biological process that can dominate expression profiles, potentially obscuring the more subtle differences between cell types. This exercise  guides you through the derivation of a regression-based method to remove such confounding effects, using the geometric framework of linear projections. It also crucially explores the potential downside of this process, forcing you to consider when correction is helpful versus when it might harm the true biological signal you aim to study.",
            "id": "4324318",
            "problem": "Consider single-cell ribonucleic acid sequencing (scRNA-seq) measurements organized as a matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of cells and $p$ is the number of genes. Rows of $X$ are cells and columns are genes. To identify and annotate cell types, one often wishes to remove variation arising from the cell cycle that can confound type-specific signals. Let $S \\in \\mathbb{R}^{n \\times q}$ be a matrix of $q$ cell-level covariates quantifying cell cycle state (for example, $q=2$ scores for $\\mathrm{G1/S}$ and $\\mathrm{G2/M}$ programs), and let $\\mathbf{1} \\in \\mathbb{R}^{n}$ denote the intercept vector of ones. Define the design matrix $Z = [\\mathbf{1}\\;\\;S] \\in \\mathbb{R}^{n \\times (q+1)}$. Assume that $X$ has been transformed to a scale on which linear effects are appropriate and that variation in each gene’s expression across cells can be decomposed into additive contributions from the intercept, the cell cycle covariates, and residuals.\n\nStarting only from the definition of ordinary least squares (OLS) as the minimizer of the sum of squared residuals and basic properties of Euclidean projections in $\\mathbb{R}^{n}$, do the following:\n\n1. Derive, for an arbitrary gene indexed by $j \\in \\{1,\\dots,p\\}$, the OLS solution for the regression of the cell expression vector $X_{\\cdot j} \\in \\mathbb{R}^{n}$ onto the columns of $Z$, and then obtain a matrix expression for the residualized expression matrix $\\tilde{X} \\in \\mathbb{R}^{n \\times p}$ that has cell cycle effects removed across all genes simultaneously.\n\n2. To interrogate when removing cell cycle effects is appropriate versus harmful to true type differences, let $T \\in \\mathbb{R}^{n}$ be a centered cell type contrast vector (for example, $T$ might indicate two types with entries summing to zero), representing a direction in cell space along which true type identity varies. Let $P_{S}$ denote the orthogonal projector onto the column space of $S$, and let $\\theta \\in [0,\\pi/2]$ be the angle between the vector $T$ and the subspace spanned by the columns of $S$, defined by $\\cos(\\theta) = \\|P_{S}T\\|/\\|T\\|$. Using only the definitions of projection and angle in Euclidean space, derive the fraction $f(\\theta)$ of the true type variance in $T$ that is removed by regressing out (and subtracting) the cell cycle covariates.\n\nProvide your final answer as the closed-form analytic expression for $f(\\theta)$. No numerical rounding is required. If you introduce any acronym, define it on first use by its full name followed by the acronym in parentheses.",
            "solution": "The posed problem is scientifically grounded, well-posed, and objective. It describes a standard and valid procedure in systems biomedicine, namely the correction for confounding variables in single-cell ribonucleic acid sequencing (scRNA-seq) data using linear regression. The problem is framed using precise mathematical language and definitions from linear algebra, and it contains sufficient information to derive unique and meaningful solutions. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe problem is divided into two parts. The first part requires the derivation of the residualized expression matrix after regressing out confounding effects. The second part requires the calculation of the fraction of variance removed from a specific signal vector by this procedure.\n\n1.  Derivation of the residualized expression matrix $\\tilde{X}$.\n\nLet $X_{\\cdot j} \\in \\mathbb{R}^{n}$ denote the expression vector for an arbitrary gene $j$, where $j \\in \\{1, \\dots, p\\}$. The linear model for this gene's expression as a function of the intercept and cell cycle covariates is given by:\n$$X_{\\cdot j} = Z \\beta_j + \\epsilon_j$$\nwhere $Z = [\\mathbf{1}\\;\\;S] \\in \\mathbb{R}^{n \\times (q+1)}$ is the design matrix, $\\beta_j \\in \\mathbb{R}^{q+1}$ is the vector of regression coefficients for gene $j$, and $\\epsilon_j \\in \\mathbb{R}^{n}$ is the vector of residuals.\n\nThe method of ordinary least squares (OLS) seeks to find the estimate $\\hat{\\beta}_j$ that minimizes the sum of squared residuals (SSR), which is equivalent to minimizing the squared Euclidean norm of the residual vector $\\epsilon_j$.\n$$SSR(\\beta_j) = \\|\\epsilon_j\\|^2 = \\|X_{\\cdot j} - Z\\beta_j\\|^2$$\nTo find the minimum, we can expand the squared norm and differentiate with respect to $\\beta_j$.\n$$SSR(\\beta_j) = (X_{\\cdot j} - Z\\beta_j)^T (X_{\\cdot j} - Z\\beta_j) = X_{\\cdot j}^T X_{\\cdot j} - 2 X_{\\cdot j}^T Z \\beta_j + \\beta_j^T Z^T Z \\beta_j$$\nThe gradient of the SSR with respect to $\\beta_j$ is:\n$$\\frac{\\partial(SSR)}{\\partial \\beta_j} = -2 Z^T X_{\\cdot j} + 2 Z^T Z \\beta_j$$\nSetting the gradient to zero to find the minimum gives the normal equations:\n$$Z^T Z \\hat{\\beta}_j = Z^T X_{\\cdot j}$$\nAssuming the matrix $Z^T Z$ is invertible (which is true if the columns of $Z$ are linearly independent, a standard assumption in OLS), we can solve for the OLS estimate of the coefficients:\n$$\\hat{\\beta}_j = (Z^T Z)^{-1} Z^T X_{\\cdot j}$$\nThe vector of fitted values, $\\hat{X}_{\\cdot j}$, is the projection of the original data vector $X_{\\cdot j}$ onto the column space of $Z$, $\\mathrm{col}(Z)$.\n$$\\hat{X}_{\\cdot j} = Z \\hat{\\beta}_j = Z (Z^T Z)^{-1} Z^T X_{\\cdot j}$$\nWe define the orthogonal projection matrix onto $\\mathrm{col}(Z)$ as $P_Z = Z (Z^T Z)^{-1} Z^T$. Thus, the fitted values are $\\hat{X}_{\\cdot j} = P_Z X_{\\cdot j}$.\n\nThe residual vector for gene $j$, which represents the gene's expression after the effects of the covariates in $Z$ have been removed, is the difference between the observed and fitted values:\n$$\\tilde{X}_{\\cdot j} = X_{\\cdot j} - \\hat{X}_{\\cdot j} = X_{\\cdot j} - P_Z X_{\\cdot j} = (I - P_Z) X_{\\cdot j}$$\nwhere $I$ is the $n \\times n$ identity matrix. The matrix $I - P_Z$ is the orthogonal projection matrix onto the subspace orthogonal to $\\mathrm{col}(Z)$.\n\nTo obtain the entire residualized expression matrix $\\tilde{X} \\in \\mathbb{R}^{n \\times p}$, we apply this operation to every gene (i.e., every column of $X$) simultaneously. This can be expressed in a single matrix equation:\n$$\\tilde{X} = X - P_Z X = (I - P_Z) X$$\n\n2.  Derivation of the fraction of variance removed from the true type contrast vector $T$.\n\nWe are asked to find the fraction $f(\\theta)$ of the true type variance in a centered contrast vector $T \\in \\mathbb{R}^n$ that is removed by regressing out the cell cycle covariates, which are the columns of the matrix $S \\in \\mathbb{R}^{n \\times q}$.\n\nThe \"variance\" of a vector in the geometric context of regression analysis corresponds to its sum of squares. Since the vector $T$ is centered (its entries sum to zero), its total sum of squares is simply its squared Euclidean norm, $\\|T\\|^2$.\n\nThe process of \"regressing out\" the effects of the covariates in $S$ from the vector $T$ means projecting $T$ onto the column space of $S$, denoted $\\mathrm{col}(S)$. The resulting vector of fitted values, which represents the part of $T$ that is explained by (and thus \"removed\" by) the cell cycle covariates, is given by:\n$$\\hat{T}_S = P_S T$$\nwhere $P_S$ is the orthogonal projection matrix onto $\\mathrm{col}(S)$.\n\nThe variance of this removed component is its sum of squares, $\\|\\hat{T}_S\\|^2 = \\|P_S T\\|^2$. This quantity is the explained sum of squares in the regression of $T$ on $S$. Since $T$ is centered, this is the correct expression for the explained variance.\n\nThe fraction of the total variance of $T$ that is removed by the regression is the ratio of the removed variance to the total variance:\n$$f = \\frac{\\text{Removed Variance}}{\\text{Total Variance}} = \\frac{\\|P_S T\\|^2}{\\|T\\|^2}$$\nThe problem provides the definition of the angle $\\theta \\in [0, \\pi/2]$ between the vector $T$ and the subspace $\\mathrm{col}(S)$:\n$$\\cos(\\theta) = \\frac{\\|P_S T\\|}{\\|T\\|}$$\nThis is the cosine of the angle between a vector and its projection onto a subspace. Squaring both sides gives:\n$$\\cos^2(\\theta) = \\left( \\frac{\\|P_S T\\|}{\\|T\\|} \\right)^2 = \\frac{\\|P_S T\\|^2}{\\|T\\|^2}$$\nBy direct substitution, we find that the fraction of variance removed, $f$, is equal to the square of the cosine of the angle $\\theta$:\n$$f(\\theta) = \\cos^2(\\theta)$$\nThis result quantifies the extent to which the biological signal of interest ($T$) is confounded with the cell cycle signal ($S$). If $T$ is orthogonal to the cell cycle subspace ($\\theta = \\pi/2$), then $\\cos^2(\\theta) = 0$ and no variance is removed. If $T$ lies entirely within the cell cycle subspace ($\\theta = 0$), then $\\cos^2(\\theta) = 1$ and all the variance is removed, indicating that the 'true type' signal is indistinguishable from cell cycle effects under this model.",
            "answer": "$$\\boxed{\\cos^2(\\theta)}$$"
        },
        {
            "introduction": "After data has been cleaned, corrected, and passed through a classifier, the final and most critical task is to evaluate the results. Simply measuring overall accuracy can be dangerously misleading, especially in biological systems where some cell types are exceedingly rare compared to others. This practice  provides a hands-on opportunity to compute and contrast two essential performance metrics, the macro- and micro-averaged $F1$ scores, from a real-world confusion matrix. By working through this calculation, you will gain a concrete appreciation for why the choice of evaluation metric is paramount for fairly assessing classifier performance on imbalanced datasets.",
            "id": "4324372",
            "problem": "In a single-cell transcriptomics study, a supervised classifier assigns each cell to one of four immunologic cell types during cell type identification and annotation: T cells ($\\mathrm{T}$), B cells ($\\mathrm{B}$), monocytes ($\\mathrm{Mo}$), and dendritic cells ($\\mathrm{DC}$). There are $400$ cells total. The ground-truth counts are $250$ for $\\mathrm{T}$, $100$ for $\\mathrm{B}$, $40$ for $\\mathrm{Mo}$, and $10$ for $\\mathrm{DC}$. The resulting confusion matrix $M$ (rows indexed by ground-truth class in the order $\\mathrm{T}, \\mathrm{B}, \\mathrm{Mo}, \\mathrm{DC}$; columns indexed by predicted class in the same order) is\n$$\nM \\;=\\;\n\\begin{pmatrix}\n220 & 15 & 10 & 5 \\\\\n8 & 85 & 5 & 2 \\\\\n4 & 3 & 30 & 3 \\\\\n1 & 1 & 3 & 5\n\\end{pmatrix}.\n$$\nStarting only from the core definitions of true positives, false positives, false negatives, precision, recall, and the harmonic mean, derive the class-wise F1 scores for each of the four classes, then compute the macro-averaged F1 score (the unweighted arithmetic mean of the class-wise F1 scores) and the micro-averaged F1 score (computed by pooling all decisions across classes before calculating precision and recall). Round both final numeric answers to $4$ significant figures. In your derivation, explain which averaging scheme better reflects performance for rare cell types in this setting.",
            "solution": "The problem involves multi-class, single-label classification. The fundamental base consists of the following core definitions. For a given class $c$:\n- The true positives are $TP_c$, the number of instances of class $c$ correctly predicted as $c$.\n- The false positives are $FP_c$, the number of instances predicted as $c$ that are not truly $c$.\n- The false negatives are $FN_c$, the number of instances truly $c$ that are not predicted as $c$.\n- Precision is $P_c = \\frac{TP_c}{TP_c + FP_c}$.\n- Recall is $R_c = \\frac{TP_c}{TP_c + FN_c}$.\n- The F1 score is the harmonic mean of precision and recall, which from the harmonic mean definition can be written as\n$$\nF1_c \\;=\\; \\frac{2\\,P_c\\,R_c}{P_c + R_c}.\n$$\nUsing $P_c = \\frac{TP_c}{TP_c + FP_c}$ and $R_c = \\frac{TP_c}{TP_c + FN_c}$, substitution and algebra yield the equivalent expression\n$$\nF1_c \\;=\\; \\frac{2\\,TP_c}{2\\,TP_c + FP_c + FN_c}.\n$$\nFor the macro-averaged F1 score, we compute the unweighted mean of $F1_c$ over classes. For the micro-averaged F1 score, we first sum $TP_c$, $FP_c$, and $FN_c$ across classes to obtain global $TP$, $FP$, and $FN$, then compute precision and recall using the same definitions and finally their harmonic mean.\n\nWe extract $TP_c$, $FP_c$, and $FN_c$ per class from the confusion matrix $M$. Let the classes be ordered as $\\mathrm{T}, \\mathrm{B}, \\mathrm{Mo}, \\mathrm{DC}$. For each class $c$:\n- $TP_c$ is the diagonal element $M_{cc}$.\n- The column sums are the total predicted counts per class, and the row sums are the total ground-truth counts per class. Then $FP_c$ equals the column sum for class $c$ minus $TP_c$, and $FN_c$ equals the row sum for class $c$ minus $TP_c$.\n\nCompute row sums (ground truth totals): $\\mathrm{T}$ has $250$, $\\mathrm{B}$ has $100$, $\\mathrm{Mo}$ has $40$, $\\mathrm{DC}$ has $10$; these match the given totals. Compute column sums (predicted totals):\n- Predicted $\\mathrm{T}$: $220 + 8 + 4 + 1 = 233$.\n- Predicted $\\mathrm{B}$: $15 + 85 + 3 + 1 = 104$.\n- Predicted $\\mathrm{Mo}$: $10 + 5 + 30 + 3 = 48$.\n- Predicted $\\mathrm{DC}$: $5 + 2 + 3 + 5 = 15$.\n\nNow compute per-class quantities and class-wise F1 scores.\n\nFor $\\mathrm{T}$:\n- $TP_{\\mathrm{T}} = 220$.\n- $FP_{\\mathrm{T}} = 233 - 220 = 13$.\n- $FN_{\\mathrm{T}} = 250 - 220 = 30$.\n- $F1_{\\mathrm{T}} = \\frac{2 \\cdot 220}{2 \\cdot 220 + 13 + 30} = \\frac{440}{483}$.\n\nFor $\\mathrm{B}$:\n- $TP_{\\mathrm{B}} = 85$.\n- $FP_{\\mathrm{B}} = 104 - 85 = 19$.\n- $FN_{\\mathrm{B}} = 100 - 85 = 15$.\n- $F1_{\\mathrm{B}} = \\frac{2 \\cdot 85}{2 \\cdot 85 + 19 + 15} = \\frac{170}{204} = \\frac{85}{102} = \\frac{5}{6}$.\n\nFor $\\mathrm{Mo}$:\n- $TP_{\\mathrm{Mo}} = 30$.\n- $FP_{\\mathrm{Mo}} = 48 - 30 = 18$.\n- $FN_{\\mathrm{Mo}} = 40 - 30 = 10$.\n- $F1_{\\mathrm{Mo}} = \\frac{2 \\cdot 30}{2 \\cdot 30 + 18 + 10} = \\frac{60}{88} = \\frac{15}{22}$.\n\nFor $\\mathrm{DC}$:\n- $TP_{\\mathrm{DC}} = 5$.\n- $FP_{\\mathrm{DC}} = 15 - 5 = 10$.\n- $FN_{\\mathrm{DC}} = 10 - 5 = 5$.\n- $F1_{\\mathrm{DC}} = \\frac{2 \\cdot 5}{2 \\cdot 5 + 10 + 5} = \\frac{10}{25} = \\frac{2}{5}$.\n\nCompute the macro-averaged F1 score:\n$$\nF1_{\\mathrm{macro}} \\;=\\; \\frac{1}{4} \\left( \\frac{440}{483} + \\frac{5}{6} + \\frac{15}{22} + \\frac{2}{5} \\right).\n$$\nTo evaluate numerically, approximate each term:\n- $\\frac{440}{483} \\approx 0.910971$,\n- $\\frac{5}{6} \\approx 0.833333$,\n- $\\frac{15}{22} \\approx 0.681818$,\n- $\\frac{2}{5} = 0.4$.\nSum: $0.910971 + 0.833333 + 0.681818 + 0.4 = 2.826122$. Divide by $4$ to obtain\n$$\nF1_{\\mathrm{macro}} \\approx 0.7065305.\n$$\nRounded to $4$ significant figures: $0.7065$.\n\nFor the micro-averaged F1 score, compute global $TP$, $FP$, and $FN$ by summing over classes:\n- $TP = 220 + 85 + 30 + 5 = 340$.\n- The total number of predictions is $400$, so $FP = 400 - TP = 60$. By the single-label property, $FN$ also equals $60$ because each misclassification contributes one $FP$ for the predicted class and one $FN$ for the true class.\nGlobal precision and recall are\n$$\nP_{\\mathrm{micro}} = \\frac{TP}{TP + FP} = \\frac{340}{340 + 60} = \\frac{340}{400} = \\frac{17}{20} = 0.85,\n$$\n$$\nR_{\\mathrm{micro}} = \\frac{TP}{TP + FN} = \\frac{340}{340 + 60} = \\frac{340}{400} = \\frac{17}{20} = 0.85.\n$$\nThus\n$$\nF1_{\\mathrm{micro}} = \\frac{2\\,P_{\\mathrm{micro}}\\,R_{\\mathrm{micro}}}{P_{\\mathrm{micro}} + R_{\\mathrm{micro}}} = \\frac{2 \\cdot 0.85 \\cdot 0.85}{0.85 + 0.85} = 0.85,\n$$\nequivalently,\n$$\nF1_{\\mathrm{micro}} = \\frac{2\\,TP}{2\\,TP + FP + FN} = \\frac{680}{800} = \\frac{17}{20} = 0.85.\n$$\nRounded to $4$ significant figures: $0.8500$.\n\nRegarding which averaging scheme better reflects performance for rare cell types, macro-averaging weights each class equally in the mean, independent of class prevalence. In this dataset, $\\mathrm{DC}$ and $\\mathrm{Mo}$ are rare with $10$ and $40$ cells, respectively, yet their class-wise F1 scores ($\\frac{2}{5}$ and $\\frac{15}{22}$) contribute equally to $F1_{\\mathrm{macro}}$ as the more prevalent classes. In contrast, micro-averaging pools all decisions and is dominated by the prevalent classes ($\\mathrm{T}$ and $\\mathrm{B}$, totaling $350$ cells), yielding a high $F1_{\\mathrm{micro}}$ that largely reflects performance on abundant types. Therefore, for assessing performance on rare cell types in systems biomedicine cell annotation, $F1_{\\mathrm{macro}}$ better reflects performance sensitivity to rare classes than $F1_{\\mathrm{micro}}$, which largely reflects overall accuracy dominated by common classes.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.7065 & 0.8500\\end{pmatrix}}$$"
        }
    ]
}