{
    "hands_on_practices": [
        {
            "introduction": "Building a robust dynamic model begins with a firm grasp of the underlying physical principles. This first practice challenges you to construct a pharmacokinetic/pharmacodynamic (PK/PD) model from the ground up, starting with mass conservation. By applying dimensional analysis and nondimensionalization, you will learn how to ensure your model is physically consistent and how to simplify its structure to reveal the essential parameter combinations that govern its behavior .",
            "id": "4336926",
            "problem": "Consider an individual-level one-compartment pharmacokinetic and pharmacodynamic (PK/PD) model described by the following principles. The pharmacokinetic state variable is the amount in the central compartment, denoted by $A_{c}(t)$. Mass conservation gives a balance between input and elimination fluxes. The input is a controllable infusion rate $u(t)$, and elimination is proportional to concentration in the central compartment. The concentration is defined by the central compartment amount divided by the apparent volume of distribution $V$. Clearance $CL$ is defined as the hypothetical volume of plasma fully cleared of drug per unit time. The pharmacodynamic output $y(t)$ is a measured biomarker whose instantaneous level is an increasing saturable function of the pharmacokinetic concentration via a maximum effect model with baseline. All model variables and parameters are positive. The independent variable is time $t$.\n\nTasks:\n1) Using dimensional analysis grounded in mass conservation and the definitions of clearance and concentration, assign physical units to $A_{c}$, $CL$, $V$, $u$, and $y$. Express units in terms of milligram (mg) for mass, liter (L) for volume, hour (h) for time, and biomarker unit (B) for the measured output.\n2) Starting from the mass balance principle “rate of change of amount equals inflow minus outflow” and the definition of clearance “elimination rate equals clearance times concentration,” write the ordinary differential equation (ODE) for $A_{c}(t)$ and specify an instantaneous-effect pharmacodynamic map for $y(t)$ using a maximum effect with baseline formulation with parameters $E_{0}$ (baseline effect), $E_{\\max}$ (maximum drug-induced effect), and $EC_{50}$ (half-maximal effective concentration).\n3) Construct a nondimensionalization that reduces the number of free parameters while preserving the model’s predictive structure as follows. Define a characteristic time scale from the pharmacokinetics and choose a concentration scaling equal to the half-maximal effective concentration. Define a dimensionless state, time, input, and output by appropriately scaling $A_{c}(t)$, $t$, $u(t)$, and $y(t)$. Derive the fully nondimensional ODE and output map. Identify which combinations of the original parameters survive as independent dimensionless groups and how many such groups remain after these choices.\n4) Consider a constant infusion $u(t) = u_{0}$ initiated at $t = 0$. Linearize the dimensionless ODE about its steady state and determine the unique nonzero eigenvalue of the linearized dynamics. Provide this eigenvalue as your final answer. No rounding is required, and the final answer must be given as an exact number with no units.",
            "solution": "The problem is evaluated to be scientifically grounded, well-posed, and objective. It is based on canonical principles of pharmacokinetic and pharmacodynamic modeling. All necessary definitions and conditions are provided to derive a unique solution. Proceeding to solution.\n\nThe problem consists of four parts, which will be addressed sequentially.\n\n1) **Dimensional Analysis**\nThe physical units for the specified variables and parameters are determined using their definitions and the principle of dimensional homogeneity. The base units are given as milligram ($mg$) for mass, liter ($L$) for volume, hour ($h$) for time, and biomarker unit ($B$) for the output.\n\n- $A_{c}(t)$: The amount of drug in the central compartment. This is a mass. Its unit is $mg$.\n- $V$: The apparent volume of distribution. This is a volume. Its unit is $L$.\n- Concentration $C(t)$ is defined as $A_{c}(t)/V$. Its units are therefore $\\frac{mg}{L}$.\n- $CL$: Clearance is defined as the volume of plasma cleared per unit time. Its unit is $\\frac{L}{h}$.\n- The elimination rate is given as the product of clearance and concentration, $CL \\times C(t)$. The units of this rate are $(\\frac{L}{h}) \\times (\\frac{mg}{L}) = \\frac{mg}{h}$, which correctly represents a mass flux (mass per unit time).\n- $u(t)$: The infusion rate is an input mass flux. Its unit is $\\frac{mg}{h}$.\n- $y(t)$: The measured biomarker output. Its unit is given as $B$.\n\nIn summary:\n- Unit of $A_{c}$: $mg$\n- Unit of $CL$: $\\frac{L}{h}$\n- Unit of $V$: $L$\n- Unit of $u$: $\\frac{mg}{h}$\n- Unit of $y$: $B$\n\n2) **Model Formulation**\nThe pharmacokinetic (PK) model for $A_{c}(t)$ is derived from the mass balance principle: \"rate of change of amount equals inflow minus outflow\".\n\n- Rate of change of amount: $\\frac{dA_{c}(t)}{dt}$.\n- Inflow rate: $u(t)$.\n- Outflow rate (elimination): $CL \\times C(t) = CL \\times \\frac{A_{c}(t)}{V}$.\n\nCombining these terms yields the ordinary differential equation (ODE) for the pharmacokinetic state:\n$$\n\\frac{dA_{c}(t)}{dt} = u(t) - \\frac{CL}{V} A_{c}(t)\n$$\nThe pharmacodynamic (PD) model for the output $y(t)$ is an instantaneous, saturable function of concentration $C(t)$ described by a maximum effect model with a baseline. The parameters are the baseline effect $E_{0}$, the maximum drug-induced effect $E_{\\max}$, and the half-maximal effective concentration $EC_{50}$. The standard formulation is:\n$$\ny(t) = E_{0} + \\frac{E_{\\max} \\cdot C(t)}{EC_{50} + C(t)}\n$$\nSince $C(t) = \\frac{A_{c}(t)}{V}$, this can also be expressed in terms of the amount $A_c(t)$:\n$$\ny(t) = E_{0} + \\frac{E_{\\max} \\cdot \\frac{A_{c}(t)}{V}}{EC_{50} + \\frac{A_{c}(t)}{V}} = E_{0} + \\frac{E_{\\max} \\cdot A_{c}(t)}{V \\cdot EC_{50} + A_{c}(t)}\n$$\n\n3) **Nondimensionalization**\nNext, we construct a nondimensional version of the model.\n\n- **Characteristic Time Scale:** In the PK ODE, the term $\\frac{CL}{V}$ has units of $\\frac{1}{h}$. Its reciprocal defines the characteristic time scale $\\tau$.\n$$\n\\tau = \\frac{V}{CL}\n$$\nWe define dimensionless time $\\bar{t}$ as:\n$$\n\\bar{t} = \\frac{t}{\\tau} = \\frac{t \\cdot CL}{V}\n$$\n- **Characteristic Concentration and Amount Scale:** The problem specifies using $EC_{50}$ as the concentration scale.\n$$\nC_{ref} = EC_{50}\n$$\nThe corresponding characteristic amount scale $A_{c,ref}$ is derived from $A_c = C \\cdot V$:\n$$\nA_{c,ref} = C_{ref} \\cdot V = EC_{50} \\cdot V\n$$\nThe dimensionless state (amount) $\\bar{A}_{c}$ is defined as:\n$$\n\\bar{A}_{c}(\\bar{t}) = \\frac{A_{c}(t)}{A_{c,ref}} = \\frac{A_{c}(t)}{EC_{50} \\cdot V}\n$$\nNote that $\\bar{A}_{c} = \\frac{A_c/V}{EC_{50}} = \\frac{C}{EC_{50}}$, so the dimensionless amount is equivalent to the dimensionless concentration.\n\n- **Characteristic Input Scale:** To determine the input scale $u_{ref}$, we analyze the characteristic magnitudes of the terms in the PK ODE. The characteristic magnitude of both $\\frac{dA_{c}}{dt}$ and $\\frac{CL}{V}A_c$ is $\\frac{A_{c,ref}}{\\tau} = \\frac{EC_{50} \\cdot V}{V/CL} = EC_{50} \\cdot CL$. To maintain balance, we choose $u_{ref}$ to be this value.\n$$\nu_{ref} = EC_{50} \\cdot CL\n$$\nThe dimensionless input $\\bar{u}$ is:\n$$\n\\bar{u}(\\bar{t}) = \\frac{u(t)}{u_{ref}} = \\frac{u(t)}{EC_{50} \\cdot CL}\n$$\n- **Derivation of Dimensionless ODE:** We rewrite the PK ODE using the chain rule for the time derivative: $\\frac{dA_{c}}{dt} = \\frac{d(A_{c,ref} \\bar{A}_{c})}{d(\\tau \\bar{t})} = \\frac{A_{c,ref}}{\\tau}\\frac{d\\bar{A}_{c}}{d\\bar{t}}$.\n$$\n\\frac{A_{c,ref}}{\\tau}\\frac{d\\bar{A}_{c}}{d\\bar{t}} = u_{ref}\\bar{u} - \\frac{CL}{V} (A_{c,ref}\\bar{A}_{c})\n$$\nSubstituting the scaling factors:\n$$\n(EC_{50} \\cdot CL)\\frac{d\\bar{A}_{c}}{d\\bar{t}} = (EC_{50} \\cdot CL)\\bar{u} - \\frac{CL}{V} (EC_{50} \\cdot V)\\bar{A}_{c}\n$$\nSimplifying this equation by dividing all terms by the common factor $(EC_{50} \\cdot CL)$ yields the dimensionless ODE:\n$$\n\\frac{d\\bar{A}_{c}}{d\\bar{t}} = \\bar{u}(\\bar{t}) - \\bar{A}_{c}(\\bar{t})\n$$\n- **Derivation of Dimensionless Output Map:** The PD map is $y(t) = E_{0} + \\frac{E_{\\max} \\cdot C(t)}{EC_{50} + C(t)}$. We define a dimensionless output $\\bar{y}$ by scaling the drug-induced effect:\n$$\n\\bar{y} = \\frac{y(t) - E_{0}}{E_{\\max}} = \\frac{C(t)}{EC_{50} + C(t)}\n$$\nDividing the numerator and denominator by $EC_{50}$:\n$$\n\\bar{y} = \\frac{C(t)/EC_{50}}{1 + C(t)/EC_{50}}\n$$\nSince $\\bar{A}_{c} = C/EC_{50}$, the dimensionless output map is:\n$$\n\\bar{y}(\\bar{t}) = \\frac{\\bar{A}_{c}(\\bar{t})}{1 + \\bar{A}_{c}(\\bar{t})}\n$$\n- **Independent Dimensionless Groups:** The original model has five parameters: $\\{V, CL, E_{0}, E_{\\max}, EC_{50}\\}$. The derived dimensionless PK model for $\\bar{A}_{c}$ is parameter-free. The dimensionless PD map for $\\bar{y}$ is also parameter-free. However, to recover the physical output $y(t)$ from the dimensionless state solution $\\bar{A}_{c}(\\bar{t})$, one must use the relationship $y(t) = E_{0} + E_{\\max} \\cdot \\bar{y}(\\bar{t}) = E_{0} + E_{\\max} \\frac{\\bar{A}_{c}(\\bar{t})}{1 + \\bar{A}_{c}(\\bar{t})}$. This final step depends on two original parameters, $E_{0}$ and $E_{\\max}$. Thus, two independent parameter groups survive the nondimensionalization.\n\n4) **Linearization and Eigenvalue**\nConsider a constant infusion $u(t) = u_{0}$ for $t \\ge 0$. The dimensionless input becomes a constant, $\\bar{u}(\\bar{t}) = \\bar{u}_{0} = \\frac{u_{0}}{EC_{50} \\cdot CL}$. The dimensionless ODE is:\n$$\n\\frac{d\\bar{A}_{c}}{d\\bar{t}} = \\bar{u}_{0} - \\bar{A}_{c}\n$$\nThis is a first-order linear time-invariant (LTI) system. We are asked to linearize it about its steady state. Let $f(\\bar{A}_{c}) = \\bar{u}_{0} - \\bar{A}_{c}$. The steady state, $\\bar{A}_{c,ss}$, is found by setting the time derivative to zero:\n$$\nf(\\bar{A}_{c,ss}) = 0 \\implies \\bar{u}_{0} - \\bar{A}_{c,ss} = 0 \\implies \\bar{A}_{c,ss} = \\bar{u}_{0}\n$$\nThe linearization of the system $\\frac{d\\bar{A}_{c}}{d\\bar{t}} = f(\\bar{A}_{c})$ around $\\bar{A}_{c,ss}$ is given by $\\frac{d(\\delta\\bar{A}_{c})}{d\\bar{t}} = J \\cdot \\delta\\bar{A}_{c}$, where $\\delta\\bar{A}_{c} = \\bar{A}_{c} - \\bar{A}_{c,ss}$ and $J$ is the Jacobian matrix (a scalar in this one-dimensional case) evaluated at the steady state.\n$$\nJ = \\frac{df}{d\\bar{A}_{c}} \\bigg|_{\\bar{A}_{c} = \\bar{A}_{c,ss}}\n$$\nWe compute the derivative of $f$:\n$$\n\\frac{df}{d\\bar{A}_{c}} = \\frac{d}{d\\bar{A}_{c}}(\\bar{u}_{0} - \\bar{A}_{c}) = -1\n$$\nThe Jacobian is a constant, $J = -1$. The linearized dynamics are therefore:\n$$\n\\frac{d(\\delta\\bar{A}_{c})}{d\\bar{t}} = -1 \\cdot \\delta\\bar{A}_{c}\n$$\nThe eigenvalue of the linearized dynamics is the coefficient of the state deviation, which is the value of the Jacobian.\nThe unique nonzero eigenvalue is $-1$.",
            "answer": "$$\n\\boxed{-1}\n$$"
        },
        {
            "introduction": "A mathematically sound model is of little practical use if its parameters cannot be determined from experimental data. This exercise delves into the crucial concept of parameter identifiability, exploring how it is not just a property of the model, but is intimately linked to the experimental design. Through a computational approach using forward sensitivity analysis, you will investigate how the timing and frequency of data collection can make the difference between an informative and an uninformative experiment .",
            "id": "4336931",
            "problem": "Consider a Two-Compartment Pharmacokinetic (PK) Model with Intravenous (IV) Bolus dosing, used in Systems Biomedicine to study dynamic drug response in an individual. Let $A_1(t)$ and $A_2(t)$ denote the drug amounts in the central and peripheral compartments, respectively, at time $t$. The initial condition is an IV bolus dose $D$ into the central compartment, so $A_1(0)=D$ and $A_2(0)=0$. The central compartment concentration is $C(t)=A_1(t)/V_1$, where $V_1$ is the central volume of distribution. The model parameters are the inter-compartment transfer rates $k_{12}$ (central to peripheral), $k_{21}$ (peripheral to central), and the elimination rate from the central compartment $k_{10}$.\n\nThe mass-balance Ordinary Differential Equations (ODEs) are grounded in conservation of mass and linear first-order kinetics:\n$$\n\\frac{dA_1}{dt} = -\\left(k_{10}+k_{12}\\right)A_1 + k_{21}A_2, \\quad\n\\frac{dA_2}{dt} = k_{12}A_1 - k_{21}A_2.\n$$\n\nLocal structural identifiability of the parameter vector $\\theta=\\left[k_{12},k_{21},k_{10}\\right]^T$ at a nominal parameter set can be assessed by the rank of the sensitivity matrix of the observable $C(t)$ with respect to $\\theta$. Let sample times be $t_1,\\dots,t_m$. Define the sensitivity matrix $S \\in \\mathbb{R}^{m \\times 3}$ by $S_{i,j} = \\frac{\\partial C(t_i)}{\\partial \\theta_j}$. If $\\operatorname{rank}(S)=3$, the three parameters are locally structurally identifiable given the sampling design; otherwise, they are not.\n\nYou are to implement a program that, using forward sensitivity analysis, constructs $S$ for different sampling schedules and reports the matrix rank, demonstrating how sampling design (e.g., sparse vs dense early sampling) affects identifiability.\n\nUse the following scientifically plausible nominal values for a typical small-molecule in an adult:\n- Dose $D = 100$ mg,\n- Central volume $V_1 = 10$ L,\n- Parameters $(k_{12}, k_{21}, k_{10}) = (0.7, 0.3, 0.2)$ $\\text{hour}^{-1}$.\n\nAll times must be treated in hours, amounts in milligrams (mg), and volumes in liters (L). While intermediate variables may carry units, the final outputs are integers without units.\n\nImplement forward sensitivities via the standard linearization of the ODEs. For each parameter $\\theta_j$, the sensitivity state $s^{(j)}(t) = \\left[\\frac{\\partial A_1}{\\partial \\theta_j}(t), \\frac{\\partial A_2}{\\partial \\theta_j}(t)\\right]^T$ satisfies\n$$\n\\frac{d}{dt} s^{(j)}(t) = J_x(t)\\, s^{(j)}(t) + g^{(j)}(t),\n$$\nwhere $J_x(t)$ is the Jacobian of the right-hand side with respect to the state $(A_1, A_2)$,\n$$\nJ_x(t) = \\begin{bmatrix}\n-\\left(k_{10}+k_{12}\\right) & k_{21} \\\\\nk_{12} & -k_{21}\n\\end{bmatrix},\n$$\nand $g^{(j)}(t)$ is the partial derivative of the right-hand side with respect to $\\theta_j$,\n$$\ng^{(k_{12})}(t) = \\begin{bmatrix} -A_1(t) \\\\ A_1(t) \\end{bmatrix}, \\quad\ng^{(k_{21})}(t) = \\begin{bmatrix} A_2(t) \\\\ -A_2(t) \\end{bmatrix}, \\quad\ng^{(k_{10})}(t) = \\begin{bmatrix} -A_1(t) \\\\ 0 \\end{bmatrix}.\n$$\nUse initial conditions $s^{(j)}(0) = \\left[0, 0\\right]^T$ since the dose is independent of $\\theta$.\n\nFrom $s^{(j)}(t)$, the concentration sensitivities are $\\frac{\\partial C(t)}{\\partial \\theta_j} = \\frac{1}{V_1}\\frac{\\partial A_1(t)}{\\partial \\theta_j}$.\n\nConstruct the sensitivity matrix $S$ by evaluating these sensitivities at the specified sampling times. Compute the matrix rank using the singular value decomposition criterion with a standard numerical tolerance. Report the ranks for the following test suite of sampling designs:\n\n- Case A (dense early and adequate late): $t = [0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 4, 8, 12]$.\n- Case B (sparse early, mostly mid-to-late): $t = [0.5, 2, 8, 12]$.\n- Case C (late-only): $t = [8, 12, 24]$.\n- Case D (very sparse two points): $t = [2, 12]$.\n\nYour program should produce a single line of output containing the ranks, in order for Cases A through D, as a comma-separated list enclosed in square brackets (for example, $[3,2,1,2]$). The outputs are integers.",
            "solution": "The user has provided a problem from the field of systems biomedicine, specifically focusing on pharmacokinetic (PK) modeling. The task is to assess the local structural identifiability of a two-compartment PK model for various experimental sampling designs. This assessment will be performed using forward sensitivity analysis.\n\n**Problem-Statement Validation**\n\nFirst, I will validate the provided problem statement against the established criteria.\n\n**Step 1: Extracted Givens**\n-   **Model:** Two-Compartment Pharmacokinetic Model with Intravenous (IV) Bolus dosing.\n-   **State Variables:** $A_1(t)$ and $A_2(t)$ are the drug amounts in the central and peripheral compartments, respectively.\n-   **Initial Conditions:** At time $t=0$, $A_1(0)=D$ and $A_2(0)=0$.\n-   **Observable:** The concentration in the central compartment, $C(t)=A_1(t)/V_1$.\n-   **Parameter Vector:** $\\theta=\\left[k_{12},k_{21},k_{10}\\right]^T$, where $k_{12}$ is the rate from central to peripheral, $k_{21}$ from peripheral to central, and $k_{10}$ is the elimination rate from central.\n-   **Governing ODEs:**\n    $$\n    \\frac{dA_1}{dt} = -\\left(k_{10}+k_{12}\\right)A_1 + k_{21}A_2 \\\\\n    \\frac{dA_2}{dt} = k_{12}A_1 - k_{21}A_2\n    $$\n-   **Identifiability Method:** The rank of the sensitivity matrix $S$, where $S_{i,j} = \\frac{\\partial C(t_i)}{\\partial \\theta_j}$ at sampling times $t_i$. Identifiability requires $\\operatorname{rank}(S)=3$.\n-   **Sensitivity Equations (Forward Method):** The sensitivities of the states, $s^{(j)}(t) = \\left[\\frac{\\partial A_1}{\\partial \\theta_j}(t), \\frac{\\partial A_2}{\\partial \\theta_j}(t)\\right]^T$, are governed by:\n    $$ \\frac{d}{dt} s^{(j)}(t) = J_x(t)\\, s^{(j)}(t) + g^{(j)}(t) $$\n    -   **Jacobian:** $J_x(t) = \\begin{bmatrix} -\\left(k_{10}+k_{12}\\right) & k_{21} \\\\ k_{12} & -k_{21} \\end{bmatrix}$. Note that for this linear time-invariant system, $J_x$ is constant.\n    -   **Forcing Terms:**\n        $g^{(k_{12})}(t) = \\begin{bmatrix} -A_1(t) \\\\ A_1(t) \\end{bmatrix}$,\n        $g^{(k_{21})}(t) = \\begin{bmatrix} A_2(t) \\\\ -A_2(t) \\end{bmatrix}$,\n        $g^{(k_{10})}(t) = \\begin{bmatrix} -A_1(t) \\\\ 0 \\end{bmatrix}$.\n    -   **Sensitivity Initial Conditions:** $s^{(j)}(0) = \\left[0, 0\\right]^T$.\n-   **Observable Sensitivity:** $\\frac{\\partial C(t)}{\\partial \\theta_j} = \\frac{1}{V_1}\\frac{\\partial A_1(t)}{\\partial \\theta_j}$.\n-   **Nominal Values:** Dose $D = 100$ mg, central volume $V_1 = 10$ L, rate constants $(k_{12}, k_{21}, k_{10}) = (0.7, 0.3, 0.2)$ $\\text{hour}^{-1}$.\n-   **Sampling Schedules (Test Cases):**\n    -   Case A: $t = [0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 4, 8, 12]$\n    -   Case B: $t = [0.5, 2, 8, 12]$\n    -   Case C: $t = [8, 12, 24]$\n    -   Case D: $t = [2, 12]$\n\n**Step 2: Validation of Givens**\n-   **Scientific Grounding:** The problem is founded on the standard two-compartment model, a fundamental concept in pharmacokinetics. The use of sensitivity analysis for identifiability is a well-established, rigorous technique in systems modeling. The derivation of the sensitivity equations is correct. The parameter values are plausible for a real-world drug. The problem is scientifically sound.\n-   **Well-Posedness & Completeness:** The problem is fully specified. It provides a complete system of linear ODEs, all necessary initial conditions, parameter values, and a clear, objective task (calculating matrix ranks for specified cases). The existence and uniqueness of the solution to the ODE system are assured. There are no contradictions.\n-   **Objectivity:** The problem is stated using precise mathematical formalism and is devoid of any subjective language.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. It is scientifically sound, well-posed, and objective. I will proceed with the solution.\n\n**Principle-Based Solution**\n\nThe core of the problem is to determine how the choice of measurement times affects our ability to uniquely determine the model parameters $\\theta = [k_{12}, k_{21}, k_{10}]^T$ from concentration data $C(t)$. Local structural identifiability is assessed by the rank of the sensitivity matrix $S$, whose columns are the sensitivity profiles of the observable $C(t)$ with respect to each parameter. If the columns are linearly independent, the rank will equal the number of parameters ($3$), and the parameters are locally identifiable. Linear dependence, which can be caused by a poor sampling schedule, results in a rank deficit (rank $<3$) and a loss of identifiability.\n\nTo construct the sensitivity matrix, we must compute the values of $\\frac{\\partial C(t_i)}{\\partial \\theta_j}$. This requires solving not only the original ODEs for the states $A_1(t)$ and $A_2(t)$ but also a set of ODEs for the state sensitivities, $\\frac{\\partial A_1}{\\partial \\theta_j}$ and $\\frac{\\partial A_2}{\\partial \\theta_j}$.\n\nThe solution involves coupling the original state ODEs with the sensitivity ODEs into a single, larger system. This augmented system describes the time evolution of both the drug amounts and their sensitivities to each parameter.\n\nLet the augmented state vector be $Y(t)$, an $8$-dimensional vector:\n$$\nY(t) = \\left[ A_1, A_2, \\frac{\\partial A_1}{\\partial k_{12}}, \\frac{\\partial A_2}{\\partial k_{12}}, \\frac{\\partial A_1}{\\partial k_{21}}, \\frac{\\partial A_2}{\\partial k_{21}}, \\frac{\\partial A_1}{\\partial k_{10}}, \\frac{\\partial A_2}{\\partial k_{10}} \\right]^T\n$$\nThe initial condition for this augmented system at $t=0$ is:\n$$\nY(0) = \\left[ D, 0, 0, 0, 0, 0, 0, 0 \\right]^T = \\left[ 100, 0, 0, 0, 0, 0, 0, 0 \\right]^T\n$$\nThe state initial conditions are $A_1(0)=D$ and $A_2(0)=0$. The sensitivity initial conditions are all zero because the initial dose $D$ is a fixed constant, independent of the parameters $\\theta$.\n\nThe system of $8$ coupled first-order ODEs, $\\frac{dY}{dt}$, is defined as follows:\n1.  $\\frac{dA_1}{dt} = -(k_{10} + k_{12})A_1 + k_{21}A_2$\n2.  $\\frac{dA_2}{dt} = k_{12}A_1 - k_{21}A_2$\n3.  $\\frac{d}{dt}\\left(\\frac{\\partial A_1}{\\partial k_{12}}\\right) = -(k_{10} + k_{12})\\frac{\\partial A_1}{\\partial k_{12}} + k_{21}\\frac{\\partial A_2}{\\partial k_{12}} - A_1$\n4.  $\\frac{d}{dt}\\left(\\frac{\\partial A_2}{\\partial k_{12}}\\right) = k_{12}\\frac{\\partial A_1}{\\partial k_{12}} - k_{21}\\frac{\\partial A_2}{\\partial k_{12}} + A_1$\n5.  $\\frac{d}{dt}\\left(\\frac{\\partial A_1}{\\partial k_{21}}\\right) = -(k_{10} + k_{12})\\frac{\\partial A_1}{\\partial k_{21}} + k_{21}\\frac{\\partial A_2}{\\partial k_{21}} + A_2$\n6.  $\\frac{d}{dt}\\left(\\frac{\\partial A_2}{\\partial k_{21}}\\right) = k_{12}\\frac{\\partial A_1}{\\partial k_{21}} - k_{21}\\frac{\\partial A_2}{\\partial k_{21}} - A_2$\n7.  $\\frac{d}{dt}\\left(\\frac{\\partial A_1}{\\partial k_{10}}\\right) = -(k_{10} + k_{12})\\frac{\\partial A_1}{\\partial k_{10}} + k_{21}\\frac{\\partial A_2}{\\partial k_{10}} - A_1$\n8.  $\\frac{d}{dt}\\left(\\frac{\\partial A_2}{\\partial k_{10}}\\right) = k_{12}\\frac{\\partial A_1}{\\partial k_{10}} - k_{21}\\frac{\\partial A_2}{\\partial k_{10}}$\n\nThis augmented system is solved numerically using a standard ODE solver from $t=0$ to the maximum time point required by any test case ($t=24$ hr). To optimize computation, we solve the system once over all unique time points specified across the four cases.\n\nFor each case, we extract the solutions for the sensitivities of the central compartment, $\\frac{\\partial A_1(t_i)}{\\partial \\theta_j}$, at the relevant time points $t_i$. The corresponding concentration sensitivities are then $\\frac{\\partial C(t_i)}{\\partial \\theta_j} = \\frac{1}{V_1} \\frac{\\partial A_1(t_i)}{\\partial \\theta_j}$.\n\nThese values are used to construct the sensitivity matrix $S$ for that case. For a case with $m$ time points, $S$ is an $m \\times 3$ matrix:\n$$\nS = \\frac{1}{V_1}\n\\begin{bmatrix}\n\\frac{\\partial A_1(t_1)}{\\partial k_{12}} & \\frac{\\partial A_1(t_1)}{\\partial k_{21}} & \\frac{\\partial A_1(t_1)}{\\partial k_{10}} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\frac{\\partial A_1(t_m)}{\\partial k_{12}} & \\frac{\\partial A_1(t_m)}{\\partial k_{21}} & \\frac{\\partial A_1(t_m)}{\\partial k_{10}}\n\\end{bmatrix}\n$$\nFinally, the rank of $S$ is computed using singular value decomposition (SVD), which is the numerically robust method for determining matrix rank. The number of non-zero (within a tolerance) singular values gives the rank. This procedure is repeated for all four sampling designs.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef model_and_sensitivities_ode(t, y, k12, k21, k10):\n    \"\"\"\n    Defines the augmented system of ODEs for the two-compartment model\n    and its sensitivities.\n\n    Args:\n        t (float): Time.\n        y (np.ndarray): Augmented state vector of size 8.\n            y[0]: A1 (amount in central compartment)\n            y[1]: A2 (amount in peripheral compartment)\n            y[2]: sA1_k12 (sensitivity of A1 w.r.t. k12)\n            y[3]: sA2_k12 (sensitivity of A2 w.r.t. k12)\n            y[4]: sA1_k21 (sensitivity of A1 w.r.t. k21)\n            y[5]: sA2_k21 (sensitivity of A2 w.r.t. k21)\n            y[6]: sA1_k10 (sensitivity of A1 w.r.t. k10)\n            y[7]: sA2_k10 (sensitivity of A2 w.r.t. k10)\n        k12, k21, k10 (float): Model parameters.\n\n    Returns:\n        np.ndarray: The derivatives dy/dt for the augmented system.\n    \"\"\"\n    A1, A2, sA1_k12, sA2_k12, sA1_k21, sA2_k21, sA1_k10, sA2_k10 = y\n\n    # Original model ODEs\n    dA1_dt = -(k10 + k12) * A1 + k21 * A2\n    dA2_dt = k12 * A1 - k21 * A2\n\n    # Jacobian matrix components related terms\n    J00 = -(k10 + k12)\n    J01 = k21\n    J10 = k12\n    J11 = -k21\n    \n    # Sensitivity ODEs w.r.t. k12\n    # Forcing term g_k12 = [-A1, A1]\n    dsA1_k12_dt = J00 * sA1_k12 + J01 * sA2_k12 - A1\n    dsA2_k12_dt = J10 * sA1_k12 + J11 * sA2_k12 + A1\n\n    # Sensitivity ODEs w.r.t. k21\n    # Forcing term g_k21 = [A2, -A2]\n    dsA1_k21_dt = J00 * sA1_k21 + J01 * sA2_k21 + A2\n    dsA2_k21_dt = J10 * sA1_k21 + J11 * sA2_k21 - A2\n\n    # Sensitivity ODEs w.r.t. k10\n    # Forcing term g_k10 = [-A1, 0]\n    dsA1_k10_dt = J00 * sA1_k10 + J01 * sA2_k10 - A1\n    dsA2_k10_dt = J10 * sA1_k10 + J11 * sA2_k10\n    \n    return np.array([\n        dA1_dt, dA2_dt, \n        dsA1_k12_dt, dsA2_k12_dt, \n        dsA1_k21_dt, dsA2_k21_dt, \n        dsA1_k10_dt, dsA2_k10_dt\n    ])\n\ndef solve():\n    \"\"\"\n    Solves the problem of determining identifiability for different sampling designs.\n    \"\"\"\n    # Define nominal values from the problem statement\n    D = 100.0  # mg\n    V1 = 10.0  # L\n    k12 = 0.7  # hr^-1\n    k21 = 0.3  # hr^-1\n    k10 = 0.2  # hr^-1\n    params = (k12, k21, k10)\n\n    # Define the sampling schedules for the test suite\n    test_cases = {\n        'A': [0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 4.0, 8.0, 12.0],\n        'B': [0.5, 2.0, 8.0, 12.0],\n        'C': [8.0, 12.0, 24.0],\n        'D': [2.0, 12.0]\n    }\n\n    # Collect all unique time points for a single, efficient ODE solve\n    all_times = sorted(list(set(t for case_times in test_cases.values() for t in case_times)))\n\n    # Initial conditions for the augmented system\n    # Y(0) = [A1, A2, sA1_k12, sA2_k12, sA1_k21, sA2_k21, sA1_k10, sA2_k10]\n    y0 = np.array([D, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\n    # Time span for integration\n    t_span = [0.0, max(all_times)]\n\n    # Solve the augmented ODE system\n    # LSODA is a robust choice suitable for potentially stiff systems.\n    sol = solve_ivp(\n        model_and_sensitivities_ode,\n        t_span,\n        y0,\n        args=params,\n        t_eval=all_times,\n        method='LSODA',\n        rtol=1e-6,\n        atol=1e-8\n    )\n\n    # Create a mapping from time to solution index for easy lookup\n    time_to_idx = {t: i for i, t in enumerate(all_times)}\n    \n    ranks = []\n    # Loop through the ordered cases A, B, C, D\n    for case_key in sorted(test_cases.keys()):\n        case_times = test_cases[case_key]\n        \n        # Get indices for the current case's time points\n        indices = [time_to_idx[t] for t in case_times]\n        \n        # Extract sensitivities for the central compartment (A1) at required times\n        # The solution `sol.y` has shape (8, num_all_times)\n        sA1_k12 = sol.y[2, indices]\n        sA1_k21 = sol.y[4, indices]\n        sA1_k10 = sol.y[6, indices]\n\n        # Construct the sensitivity matrix S for the observable C(t) = A1(t)/V1\n        # S_ij = dC(t_i)/d_theta_j = (1/V1) * sA1_theta_j(t_i)\n        S = np.column_stack([\n            sA1_k12,\n            sA1_k21,\n            sA1_k10\n        ]) / V1\n\n        # Calculate and store the rank of the sensitivity matrix\n        rank = np.linalg.matrix_rank(S)\n        ranks.append(rank)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, ranks))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        },
        {
            "introduction": "The ultimate goal of individual drug response modeling is often to make predictions that inform clinical decisions, typically with very limited data. This final practice confronts the challenge of inference and prediction under uncertainty by comparing three major statistical frameworks: Maximum Likelihood, Maximum A Posteriori, and full Bayesian inference. By working through this scenario, you will develop criteria for selecting the most appropriate method based on the available data, the reliability of prior knowledge, and the clinical need to quantify the probability of treatment success or failure .",
            "id": "4336946",
            "problem": "A clinician is using a one-compartment pharmacokinetic model with first-order elimination to guide dosing for an individual under sparse sampling. The goal is to predict the next trough concentration to decide whether a dose adjustment is needed. Let the individual elimination rate constant be $k_e$ (units $\\text{h}^{-1}$), and assume the log-trough concentration at time $\\tau$ after the previous dose is modeled as $y = \\gamma - \\tau k_e + \\eta$, where $y$ is the logarithm of the trough concentration, $\\gamma$ is a known constant determined by the dose and volume of distribution, and $\\eta \\sim \\mathcal{N}(0,\\sigma^2)$ models combined residual and assay error. The clinician observes a single log-trough $y_1$ at time $\\tau_1$ after the dose, and wishes to predict the next log-trough $y_2$ at time $\\tau_2$ under the same regimen. Assume a population-informed prior on $k_e$ given by $k_e \\sim \\mathcal{N}(m_0,s_0^2)$.\n\nFor this individual, take $\\gamma = 1.5$, $\\tau_1 = 12$, $\\tau_2 = 12$, $\\sigma = 0.2$, $m_0 = 0.1$, $s_0 = 0.05$, and the observed $y_1 = 0.7$, with a clinical target threshold $T = 0.9$ on the log scale. Consider three estimators/predictors of $y_2$:\n\n- Maximum Likelihood Estimation (MLE): estimate $\\hat{k}_{\\mathrm{MLE}}$ by maximizing the likelihood of $y_1$ given $k_e$, and predict $\\hat{y}_{2,\\mathrm{MLE}} = \\gamma - \\tau_2 \\hat{k}_{\\mathrm{MLE}}$.\n- Maximum A Posteriori (MAP): estimate $\\hat{k}_{\\mathrm{MAP}}$ by maximizing the posterior density of $k_e$ given $y_1$ under the stated prior, and predict $\\hat{y}_{2,\\mathrm{MAP}} = \\gamma - \\tau_2 \\hat{k}_{\\mathrm{MAP}}$.\n- Full Bayesian inference: derive the posterior distribution of $k_e$ given $y_1$, and use it to form the posterior predictive distribution of $y_2$ given $y_1$; take the posterior predictive mean $\\mathbb{E}[y_2 \\mid y_1]$ as the point prediction and use the full posterior predictive distribution to quantify uncertainty.\n\nStarting from Bayes’ theorem and the linear-Gaussian model specified above, and without invoking any result that is not derivable from these foundations, derive the following:\n\n1. Expressions for $\\hat{k}_{\\mathrm{MLE}}$, $\\hat{k}_{\\mathrm{MAP}}$ (which, under normality, equals the posterior mean), and the posterior variance of $k_e$ given $y_1$.\n2. Expressions for the expected squared prediction error $\\mathbb{E}\\left[(y_2 - \\hat{y}_2)^2\\right]$ under each of the three approaches, where for full Bayesian use the posterior predictive mean for $\\hat{y}_2$. Evaluate these numerically for the given parameters.\n3. The posterior predictive probability $\\mathbb{P}(y_2 < T \\mid y_1)$, evaluated numerically for the given parameters.\n\nThen, select the option that correctly compares the performance of MLE, MAP, and full Bayesian inference for predicting individual trough levels under sparse data and proposes scientifically grounded criteria for choosing among them in clinical decision-making:\n\nA. For the given parameters, $\\hat{k}_{\\mathrm{MLE}} = 0.0667\\,\\text{h}^{-1}$, $\\hat{k}_{\\mathrm{MAP}} = 0.0700\\,\\text{h}^{-1}$; the full Bayesian posterior predictive mean equals the MAP-based point prediction, and the expected predictive mean squared errors are $0.080$ (MLE) and $0.076$ (MAP and full Bayesian). The full Bayesian approach yields $\\mathbb{P}(y_2 < 0.9 \\mid y_1) \\approx 0.81$. Criteria: prefer full Bayesian when probability of target attainment must be quantified or when decisions require uncertainty bounds; prefer MAP when a point forecast with minimal mean squared error is sufficient and the prior is credible; prefer MLE when the prior is suspected to be severely mis-specified or when the data-information-to-prior-information ratio $\\tau_1^2 s_0^2 / \\sigma^2$ is large.\n\nB. MLE yields the lowest expected mean squared prediction error regardless of the prior variance; for the given parameters its error is $0.076$ while MAP and full Bayesian have $0.080$. Probability of target attainment $\\mathbb{P}(y_2 < T \\mid y_1)$ cannot be computed without additional data.\n\nC. MAP and full Bayesian give different point predictions because the posterior mode differs from the posterior mean; for the given parameters $\\hat{k}_{\\mathrm{MAP}} = 0.0667\\,\\text{h}^{-1}$, the full Bayesian predictive variance is $0.080$, and $\\mathbb{P}(y_2 < 0.9 \\mid y_1) \\approx 0.50$. In sparse data, always favor MAP because it is less biased than MLE irrespective of prior mis-specification.\n\nD. All three methods provide identical point and distributional predictions in linear-Gaussian models; selection among them should be based solely on computational time, choosing MLE when it is fastest, even when decisions require probability-of-target-attainment.",
            "solution": "We begin with the observation model $y = \\gamma - \\tau k_e + \\eta$ with $\\eta \\sim \\mathcal{N}(0,\\sigma^2)$ and prior $k_e \\sim \\mathcal{N}(m_0,s_0^2)$. Define $z = \\gamma - y$, so that $z = \\tau k_e + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$. This is a linear-Gaussian regression of $z$ on $k_e$ with design value $\\tau$.\n\nMaximum Likelihood Estimation (MLE): For a single observation $(z_1,\\tau_1)$, the likelihood for $k_e$ is $\\mathcal{N}(z_1; \\tau_1 k_e, \\sigma^2)$. The MLE is the least-squares estimator,\n$$\n\\hat{k}_{\\mathrm{MLE}} = \\frac{z_1}{\\tau_1} = \\frac{\\gamma - y_1}{\\tau_1}.\n$$\n\nMaximum A Posteriori (MAP) and full Bayesian posterior: Under a normal prior $k_e \\sim \\mathcal{N}(m_0,s_0^2)$ and normal likelihood, the posterior is normal. Using conjugate updating,\n$$\ns_n^2 = \\left(\\frac{1}{s_0^2} + \\frac{\\tau_1^2}{\\sigma^2}\\right)^{-1}, \\quad\nm_n = s_n^2\\left(\\frac{m_0}{s_0^2} + \\frac{\\tau_1 z_1}{\\sigma^2}\\right).\n$$\nBecause the posterior is normal, the Maximum A Posteriori (MAP) estimator equals the posterior mean, thus $\\hat{k}_{\\mathrm{MAP}} = m_n$, and the posterior variance of $k_e$ is $s_n^2$.\n\nPosterior predictive for $y_2$: The model for $y_2$ given $k_e$ is $y_2 = \\gamma - \\tau_2 k_e + \\eta_2$ with $\\eta_2 \\sim \\mathcal{N}(0,\\sigma^2)$ independent of all else. Integrating over the posterior for $k_e$ yields the posterior predictive distribution\n$$\ny_2 \\mid y_1 \\sim \\mathcal{N}\\left(\\gamma - \\tau_2 m_n,\\; \\tau_2^2 s_n^2 + \\sigma^2\\right).\n$$\nTherefore the posterior predictive mean equals the MAP-based point prediction: $\\mathbb{E}[y_2 \\mid y_1] = \\gamma - \\tau_2 m_n = \\hat{y}_{2,\\mathrm{MAP}}$.\n\nExpected squared prediction error under each approach: For any point predictor $\\hat{y}_2$, the actual $y_2 = \\gamma - \\tau_2 k_e + \\eta_2$. Writing the error,\n$$\ny_2 - \\hat{y}_2 = -\\tau_2(k_e - \\hat{k}) + \\eta_2,\n$$\nwhere $\\hat{k}$ is the estimator used by the method. Taking expectation over the joint distribution consistent with the approach (and noting independence of $\\eta_2$), we obtain\n$$\n\\mathbb{E}\\left[(y_2 - \\hat{y}_2)^2\\right] = \\tau_2^2\\,\\mathbb{E}\\left[(k_e - \\hat{k})^2\\right] + \\sigma^2.\n$$\nThus, comparing methods reduces to comparing $\\mathbb{E}[(k_e - \\hat{k})^2]$.\n\n- For MLE, with $z_1 = \\tau_1 k_e + \\epsilon_1$, $\\hat{k}_{\\mathrm{MLE}} = z_1/\\tau_1 = k_e + \\epsilon_1/\\tau_1$. Therefore $k_e - \\hat{k}_{\\mathrm{MLE}} = -\\epsilon_1/\\tau_1$, and\n$$\n\\mathbb{E}\\left[(k_e - \\hat{k}_{\\mathrm{MLE}})^2\\right] = \\frac{\\sigma^2}{\\tau_1^2}.\n$$\nHence\n$$\n\\mathbb{E}\\left[(y_2 - \\hat{y}_{2,\\mathrm{MLE}})^2\\right] = \\tau_2^2 \\frac{\\sigma^2}{\\tau_1^2} + \\sigma^2.\n$$\n\n- For MAP (posterior mean), a standard result for linear-Gaussian conjugate models is that the Bayes estimator under squared error is the posterior mean, with Bayes risk equal to the posterior variance. We can confirm directly that\n$$\n\\mathbb{E}\\left[(k_e - \\hat{k}_{\\mathrm{MAP}})^2\\right] = s_n^2.\n$$\nThen\n$$\n\\mathbb{E}\\left[(y_2 - \\hat{y}_{2,\\mathrm{MAP}})^2\\right] = \\tau_2^2 s_n^2 + \\sigma^2.\n$$\n\n- For full Bayesian with posterior predictive mean, the expected squared error equals the posterior predictive variance,\n$$\n\\mathbb{E}\\left[(y_2 - \\mathbb{E}[y_2 \\mid y_1])^2\\right] = \\mathrm{Var}(y_2 \\mid y_1) = \\tau_2^2 s_n^2 + \\sigma^2,\n$$\nwhich matches the MAP result for the point prediction under squared loss.\n\nNumerical evaluation with the given parameters: Compute $z_1 = \\gamma - y_1 = 1.5 - 0.7 = 0.8$. Then\n$$\n\\hat{k}_{\\mathrm{MLE}} = \\frac{z_1}{\\tau_1} = \\frac{0.8}{12} \\approx 0.0667\\,\\text{h}^{-1}.\n$$\nCompute posterior variance and mean:\n$$\n\\frac{1}{s_0^2} = \\frac{1}{(0.05)^2} = 400,\\quad \\frac{\\tau_1^2}{\\sigma^2} = \\frac{12^2}{(0.2)^2} = \\frac{144}{0.04} = 3600,\n$$\n$$\ns_n^2 = \\left(400 + 3600\\right)^{-1} = \\frac{1}{4000} = 0.00025,\n$$\n$$\n\\frac{m_0}{s_0^2} = \\frac{0.1}{0.0025} = 40,\\quad \\frac{\\tau_1 z_1}{\\sigma^2} = \\frac{12 \\cdot 0.8}{0.04} = 240,\n$$\n$$\nm_n = s_n^2\\left(40 + 240\\right) = 0.00025 \\cdot 280 = 0.0700\\,\\text{h}^{-1}.\n$$\nTherefore $\\hat{k}_{\\mathrm{MAP}} = 0.0700\\,\\text{h}^{-1}$. The posterior predictive mean for $y_2$ is\n$$\n\\mathbb{E}[y_2 \\mid y_1] = \\gamma - \\tau_2 m_n = 1.5 - 12 \\cdot 0.0700 = 1.5 - 0.84 = 0.66.\n$$\nExpected squared prediction errors:\n- MLE: $\\mathbb{E}\\left[(y_2 - \\hat{y}_{2,\\mathrm{MLE}})^2\\right] = \\tau_2^2 \\frac{\\sigma^2}{\\tau_1^2} + \\sigma^2 = 12^2 \\cdot \\frac{0.04}{12^2} + 0.04 = 0.04 + 0.04 = 0.08.$\n- MAP: $\\mathbb{E}\\left[(y_2 - \\haty}_{2,\\mathrm{MAP}})^2\\right] = \\tau_2^2 s_n^2 + \\sigma^2 = 12^2 \\cdot 0.00025 + 0.04 = 144 \\cdot 0.00025 + 0.04 = 0.036 + 0.04 = 0.076.$\n- Full Bayesian predictive mean: identical to MAP for the point prediction under squared loss, with expected squared error $0.076$ and full predictive variance $\\tau_2^2 s_n^2 + \\sigma^2 = 0.076$.\n\nPosterior predictive probability of target attainment: Since $y_2 \\mid y_1 \\sim \\mathcal{N}(0.66, 0.076)$, the standard deviation is $\\sqrt{0.076} \\approx 0.2758$. The $z$-score for $T = 0.9$ is\n$$\n\\frac{T - \\text{mean}}{\\text{sd}} = \\frac{0.9 - 0.66}{0.2758} \\approx \\frac{0.24}{0.2758} \\approx 0.870.\n$$\nThus\n$$\n\\mathbb{P}(y_2 < 0.9 \\mid y_1) = \\Phi(0.870) \\approx 0.81,\n$$\nwhere $\\Phi(\\cdot)$ is the standard normal cumulative distribution function.\n\nOption-by-option analysis:\n- Option A states numerical values $\\hat{k}_{\\mathrm{MLE}} \\approx 0.0667\\,\\text{h}^{-1}$, $\\hat{k}_{\\mathrm{MAP}} \\approx 0.0700\\,\\text{h}^{-1}$, expected predictive mean squared errors $0.080$ for MLE and $0.076$ for MAP and full Bayesian, and $\\mathbb{P}(y_2 < 0.9 \\mid y_1) \\approx 0.81$. All match our derivations. It further gives correct criteria: full Bayesian for probability-of-target-attainment and uncertainty; MAP for point forecasting with credible prior; MLE when prior is unreliable or data overwhelms prior (large $\\tau_1^2 s_0^2 / \\sigma^2$). Verdict — Correct.\n\n- Option B claims MLE yields the lowest mean squared error regardless of prior and reverses the numerical values. This contradicts the Bayes-optimality of the posterior mean under squared loss and our computed $0.076 < 0.080$. It also incorrectly asserts that $\\mathbb{P}(y_2 < T \\mid y_1)$ cannot be computed without additional data; full Bayesian provides it directly. Verdict — Incorrect.\n\n- Option C asserts a mode-mean discrepancy for the normal posterior (there is none) and gives wrong numerical values (assigns $\\hat{k}_{\\mathrm{MAP}}$ equal to the MLE and predictive variance $0.080$), and a probability $\\approx 0.50$ inconsistent with the computed $\\approx 0.81$. It also gives an overgeneralized recommendation favoring MAP irrespective of prior mis-specification, which is not sound; prior mis-specification can degrade MAP performance. Verdict — Incorrect.\n\n- Option D asserts identical point and distributional predictions across all three methods; while MAP and full Bayesian point predictions coincide under squared loss in this linear-Gaussian case, MLE differs, and distributional predictions differ because only full Bayesian yields a posterior predictive distribution that quantifies uncertainty. The recommended selection based solely on computational time ignores critical clinical decision requirements (e.g., probability-of-target-attainment). Verdict — Incorrect.\n\nTherefore, Option A is the correct choice.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}