{
    "hands_on_practices": [
        {
            "introduction": "Calculating network proximity often begins with a critical preprocessing step: deciding how to handle the disconnected nature of real-world protein-protein interaction networks. Since shortest-path distances are undefined between separate components, a common strategy is to restrict the analysis to the network's Largest Connected Component (LCC). This exercise  challenges you to justify this methodological choice and to quantitatively assess its impact by calculating the bias introduced when compared to a global approach that penalizes for disconnection.",
            "id": "4366923",
            "problem": "A protein–protein interaction (PPI) network is represented as an undirected, unweighted graph $G=(V,E)$ that may be disconnected. The network comprises multiple components; the Largest Connected Component (LCC) contains $15{,}000$ nodes out of a total of $18{,}000$ nodes. Consider a drug target set $S$ and a disease protein set $T$ embedded in $G$, where $|S|=10$ and $|T|=12$. Of the drug targets, $7$ are in the LCC and $3$ are outside. Of the disease proteins, $9$ are in the LCC and $3$ are outside. \n\nDefine the network proximity between $S$ and $T$ as the mean nearest-neighbor shortest-path distance from $S$ to $T$:\n$$\n\\delta(S,T)=\\frac{1}{|S|}\\sum_{s\\in S}\\min_{t\\in T} d_G(s,t),\n$$\nwhere $d_G(s,t)$ denotes the shortest-path length in hop counts in $G$. For pairs in different components, adopt a finite penalty distance $D_{\\infty}=7$ so that if $s$ and all $t\\in T$ lie in components disconnected from $s$, then $\\min_{t\\in T} d_G(s,t)$ is taken to be $D_{\\infty}$. \n\nTo emulate the common restriction to the Largest Connected Component (LCC) in systems biomedicine, define the LCC-restricted proximity\n$$\n\\delta_{\\mathrm{LCC}}(S,T)=\\frac{1}{|S\\cap \\mathrm{LCC}|}\\sum_{s\\in S\\cap \\mathrm{LCC}} \\min_{t\\in T\\cap \\mathrm{LCC}} d_G(s,t),\n$$\ncomputed by removing all nodes not in the LCC before measuring distances.\n\nEmpirical observations on $G$ show:\n- For the $7$ drug targets in $S\\cap \\mathrm{LCC}$, the nearest-neighbor distances to $T\\cap \\mathrm{LCC}$ are $\\{1,2,2,3,3,4,5\\}$ hops.\n- Of the $3$ drug targets outside the LCC, exactly one shares a small component with a single disease protein in $T$ at a distance of $2$ hops; the remaining two drug targets lie in components with no disease proteins.\n\nStarting from the foundational definitions of connected components, shortest-path distance, and the above proximity definitions, justify the rationale for restricting computations to the LCC when using shortest-path-based proximity in disconnected PPI networks. Then, compute the signed bias defined as\n$b=\\delta_{\\mathrm{LCC}}(S,T)-\\delta(S,T),$\ngiven the data above. Round your final numeric answer to four significant figures. No units are required.",
            "solution": "The problem statement is evaluated as scientifically grounded, well-posed, objective, and self-contained. It presents a standard task in network-based systems biomedicine, providing all necessary definitions, data, and constraints to arrive at a unique, verifiable solution. The premises are consistent and the terminology is precise. Therefore, the problem is valid.\n\nThe solution is divided into two parts, as requested by the problem statement: first, a justification for restricting network proximity computations to the Largest Connected Component (LCC), and second, the calculation of the signed bias $b$.\n\n**Part 1: Rationale for LCC Restriction in Proximity Calculations**\n\nThe use of shortest-path distance, $d_G(s,t)$, is fundamental to many network proximity measures. In a graph $G$ that is not connected, the graph is partitioned into two or more connected components. By definition, no path exists between nodes residing in different components. Consequently, the shortest-path distance between such nodes is infinite, i.e., $d_G(s,t) = \\infty$.\n\nThis poses a significant challenge for proximity metrics that average these distances. An average involving even one infinite value is itself infinite, rendering the metric non-informative. For instance, the definition $\\delta(S,T)=\\frac{1}{|S|}\\sum_{s\\in S}\\min_{t\\in T} d_G(s,t)$ becomes problematic if any drug target $s \\in S$ is in a component devoid of any disease proteins from $T$. In this case, $\\min_{t\\in T} d_G(s,t) = \\infty$, causing $\\delta(S,T)$ to diverge.\n\nTwo primary strategies exist to address this issue:\n1.  **Assigning a Finite Penalty:** This approach, explicitly mentioned in the problem, replaces infinite distances with a large, finite penalty value, $D_{\\infty}$. Here, if a source node $s$ is disconnected from all target nodes $t \\in T$, its contribution to the sum is set to $D_{\\infty}=7$. While this makes the computation tractable, it introduces a critical dependency on an arbitrary parameter. The choice of $D_{\\infty}$ can substantially alter the final proximity score and any subsequent statistical assessment (e.g., a Z-score). A higher $D_{\\infty}$ penalizes disconnectedness more severely. The lack of a theoretically justified value for $D_{\\infty}$ makes this method subjective and potentially brittle.\n\n2.  **Restricting to the LCC:** Protein-protein interaction (PPI) networks derived from high-throughput experiments typically feature a \"giant\" or Largest Connected Component (LCC) that contains a large majority of the nodes (in this case, $15,000$ out of $18,000$ nodes, or about $83.3\\%$). The remaining nodes are scattered across numerous small, isolated components. The scientific rationale for restricting analysis to the LCC is that this component represents the core, interconnected functional machinery of the cell. The smaller components are often interpreted as containing less functionally central proteins, or may represent experimental artifacts or incomplete data. By focusing on the LCC, all nodes within the scope of analysis are, by definition, mutually reachable, ensuring all shortest-path distances $d_G(s,t)$ are finite and well-defined. This approach eliminates the need for an arbitrary penalty like $D_{\\infty}$, leading to a more robust and parameter-free distance calculation. The primary drawback is the loss of information from nodes outside the LCC, which may harbor relevant biological signal. The problem's calculation of bias is designed to quantify the effect of this information loss.\n\nIn summary, restricting computations to the LCC is a pragmatic and common approach in systems biomedicine to ensure that shortest-path-based metrics are well-defined and not skewed by arbitrary penalty parameters, under the assumption that the most significant biological processes are captured within the network's largest interconnected region.\n\n**Part 2: Calculation of the Signed Bias $b$**\n\nThe signed bias is defined as $b=\\delta_{\\mathrm{LCC}}(S,T)-\\delta(S,T)$. We will compute each term separately.\n\n**Calculating $\\delta_{\\mathrm{LCC}}(S,T)$:**\nThe LCC-restricted proximity is defined as:\n$$\n\\delta_{\\mathrm{LCC}}(S,T)=\\frac{1}{|S\\cap \\mathrm{LCC}|}\\sum_{s\\in S\\cap \\mathrm{LCC}} \\min_{t\\in T\\cap \\mathrm{LCC}} d_G(s,t)\n$$\nFrom the problem statement, the number of drug targets in the LCC is $|S\\cap \\mathrm{LCC}| = 7$. The set of nearest-neighbor distances for these $7$ targets to the disease proteins within the LCC ($T\\cap \\mathrm{LCC}$) is given as $\\{1,2,2,3,3,4,5\\}$.\nThe sum of these distances is:\n$$\n\\sum_{s\\in S\\cap \\mathrm{LCC}} \\min_{t\\in T\\cap \\mathrm{LCC}} d_G(s,t) = 1+2+2+3+3+4+5 = 20\n$$\nTherefore, the LCC-restricted proximity is:\n$$\n\\delta_{\\mathrm{LCC}}(S,T) = \\frac{20}{7}\n$$\n\n**Calculating $\\delta(S,T)$:**\nThe global proximity is defined as:\n$$\n\\delta(S,T)=\\frac{1}{|S|}\\sum_{s\\in S}\\min_{t\\in T} d_G(s,t)\n$$\nWe are given $|S|=10$. The sum can be partitioned into a sum over targets in the LCC and a sum over targets outside the LCC:\n$$\n\\sum_{s\\in S}\\min_{t\\in T} d_G(s,t) = \\sum_{s\\in S\\cap \\mathrm{LCC}}\\min_{t\\in T} d_G(s,t) + \\sum_{s\\in S\\setminus \\mathrm{LCC}}\\min_{t\\in T} d_G(s,t)\n$$\nFor the first term, consider a target $s \\in S\\cap \\mathrm{LCC}$. It is connected to all $t \\in T\\cap \\mathrm{LCC}$ but disconnected from all $t \\in T\\setminus \\mathrm{LCC}$. Thus, the minimum distance will be to a node within its own component (the LCC):\n$$\n\\min_{t\\in T} d_G(s,t) = \\min(\\min_{t\\in T\\cap \\mathrm{LCC}} d_G(s,t), \\min_{t\\in T\\setminus \\mathrm{LCC}} d_G(s,t)) = \\min(\\min_{t\\in T\\cap \\mathrm{LCC}} d_G(s,t), \\infty) = \\min_{t\\in T\\cap \\mathrm{LCC}} d_G(s,t)\n$$\nThe sum for these $7$ targets is the same as calculated before:\n$$\n\\sum_{s\\in S\\cap \\mathrm{LCC}}\\min_{t\\in T} d_G(s,t) = 20\n$$\nFor the second term, we consider the $|S\\setminus \\mathrm{LCC}| = 3$ targets outside the LCC.\n- One target lies in a small component with a single disease protein from $T$ at a distance of $2$ hops. For this target, its minimum distance to any protein in $T$ is this value, since all other proteins in $T$ are in different components (the LCC or others) and thus infinitely far. So, its contribution is $2$.\n- The remaining two targets lie in components with no disease proteins from $T$. According to the problem's rule, for these targets, the minimum distance is assigned the penalty value $D_{\\infty}=7$. The contribution from each of these two targets is $7$.\nThe sum for the targets outside the LCC is:\n$$\n\\sum_{s\\in S\\setminus \\mathrm{LCC}}\\min_{t\\in T} d_G(s,t) = 2 + 7 + 7 = 16\n$$\nThe total sum for all targets in $S$ is $20 + 16 = 36$.\nTherefore, the global proximity is:\n$$\n\\delta(S,T) = \\frac{36}{10} = 3.6\n$$\n\n**Calculating the Bias $b$:**\nNow we compute the signed bias:\n$$\nb = \\delta_{\\mathrm{LCC}}(S,T) - \\delta(S,T) = \\frac{20}{7} - 3.6 = \\frac{20}{7} - \\frac{36}{10} = \\frac{20}{7} - \\frac{18}{5}\n$$\nTo subtract the fractions, we find a common denominator, which is $35$:\n$$\nb = \\frac{20 \\times 5}{35} - \\frac{18 \\times 7}{35} = \\frac{100 - 126}{35} = -\\frac{26}{35}\n$$\nFinally, we convert this fraction to a decimal and round to four significant figures:\n$$\nb = -\\frac{26}{35} \\approx -0.742857...\n$$\nRounding to four significant figures gives:\n$$\nb \\approx -0.7429\n$$\nThe negative bias indicates that restricting the analysis to the LCC resulted in a lower (i.e., closer) proximity value compared to the global calculation that includes a penalty for disconnected nodes. This is because the global calculation was inflated by the high penalty cost ($D_\\infty=7$) assigned to the two completely isolated drug targets.",
            "answer": "$$\\boxed{-0.7429}$$"
        },
        {
            "introduction": "Once a network proximity score is calculated, its scientific meaning can only be established through rigorous statistical testing. The cornerstone of this process is the null model, which provides a baseline for what to expect by chance. This exercise  delves into the critical choice between two canonical null models, label permutation and edge rewiring, forcing you to analyze how each one accounts for the network's complex topology and why selecting the appropriate model is essential for drawing valid biological inferences.",
            "id": "4366983",
            "problem": "A human protein-protein interaction (PPI) network is modeled as a simple, connected graph $G = (V,E)$, where $V$ is the set of proteins and $E$ is the set of physical interactions. Let $|V| = N$, and let $d(u,v)$ denote the unweighted shortest path distance between nodes $u \\in V$ and $v \\in V$. Consider a drug characterized by a set of targets $T \\subset V$ with $|T| = m$, and a disease characterized by a set of associated genes $S \\subset V$ with $|S| = n$. A widely used drug-disease network proximity is defined as the average cross shortest path distance\n$$\n\\Delta(T,S) = \\frac{1}{mn}\\sum_{t \\in T}\\sum_{s \\in S} d(t,s).\n$$\nYou aim to test whether the observed $\\Delta(T,S)$ is significantly smaller than expected by chance, indicating proximity between the drug targets and disease genes. Two null models are under consideration:\n\nNull Model 1 (Label-Permutation Null): On the fixed network $G$, randomize the labels by sampling $T' \\subset V$ with $|T'| = m$ and $S' \\subset V$ with $|S'| = n$ under a degree-matching constraint such that the empirical degree distributions of $T'$ and $S'$ match those of $T$ and $S$ (within allowable tolerance), and compute $\\Delta(T',S')$ over many realizations to obtain an empirical expectation.\n\nNull Model 2 (Edge-Rewiring Null): Generate surrogates $G'$ by degree-preserving edge rewiring (configuration-like rewiring that preserves the degree sequence exactly), compute $\\Delta(T,S)$ on each $G'$, and aggregate to obtain an empirical expectation.\n\nStarting from the definitions of shortest path distance on graphs, the meaning of degree sequence preservation, and the role of higher-order topology (e.g., clustering, assortativity, and modularity) in determining path length distributions, analyze which null better controls for network topology when testing drug-disease proximity. Your analysis must begin from these core definitions and proceed by deriving expressions for the expected proximity under each null and by explaining how network features beyond the degree sequence influence $\\Delta(T,S)$.\n\nTo ground the analysis, consider the following scientifically plausible scenario: A human PPI network has $N = 5000$, an average degree $\\langle k \\rangle \\approx 8$, an empirical average pairwise shortest path distance across all node pairs of $\\bar{d}_G \\approx 3.2$, high clustering coefficient, and modular community structure. Under degree-preserving rewiring, the surrogate networks $G'$ reduce clustering and partially disrupt modularity, yielding an empirical average pairwise shortest path distance $\\bar{d}_{G'} \\approx 2.7$ with similar diameter quantiles. For a particular drug and disease, the observed proximity is $\\Delta(T,S) = 2.9$ with $m = 20$ and $n = 30$.\n\nWhich statement is most accurate for controlling network topology in the statistical test of drug-disease proximity?\n\nA. Label-permutation with degree matching better controls for network topology because it holds $G$ fixed, preserving all path distances $d(u,v)$ induced by clustering and modularity, while adjusting for degree-driven selection bias in $T$ and $S$, thus estimating the expected $\\Delta$ against the actual topology of $G$.\n\nB. Edge-rewiring with degree preservation better controls for network topology because it removes biases from clustering and community structure, leaving only degree effects; therefore it provides the most neutral baseline for $\\Delta$ on any network.\n\nC. If the degree sequence is preserved, both nulls are equivalent with respect to controlling topology, since shortest path distances depend only on the degree sequence.\n\nD. Label-permutation without degree matching is preferable for topology control because $T'$ and $S'$ are sampled uniformly, thereby canceling degree effects and community structure simultaneously on $G$.\n\nE. Combining both nulls by requiring significance under label-permutation on $G$ and edge-rewiring on $G'$ provides strictly better topology control than either alone because any discrepancy across nulls reflects true biological signal rather than structural artifacts.\n\nSelect the single best option that correctly characterizes the null model that better controls for network topology in this context and justify your choice using the provided scenario and first-principles reasoning on graphs and shortest paths.",
            "solution": "The problem asks for an analysis of two null models for assessing the statistical significance of drug-disease proximity in a protein-protein interaction (PPI) network. The core of the task is to determine which null model better controls for network topology.\n\n### Step 1: Problem Validation\n\nI will first extract the given information and validate the problem statement.\n\n**Givens:**\n1.  **Network Model:** A simple, connected graph $G = (V,E)$, where $V$ is the set of proteins and $E$ is the set of physical interactions.\n2.  **Network Size:** $|V| = N$.\n3.  **Distance Metric:** $d(u,v)$ is the unweighted shortest path distance between nodes $u, v \\in V$.\n4.  **Drug/Disease Sets:** A drug is represented by its target set $T \\subset V$ with size $|T| = m$. A disease is represented by its associated gene set $S \\subset V$ with size $|S| = n$.\n5.  **Proximity Measure:** The average cross shortest path distance is defined as $\\Delta(T,S) = \\frac{1}{mn}\\sum_{t \\in T}\\sum_{s \\in S} d(t,s)$.\n6.  **Hypothesis Test:** The goal is to test if the observed $\\Delta(T,S)$ is significantly smaller than expected by chance.\n7.  **Null Model 1 (Label-Permutation):** On the fixed graph $G$, random sets $T'$ and $S'$ are sampled with $|T'| = m$ and $|S'| = n$ under a degree-matching constraint. The null distribution is formed from $\\Delta(T',S')$.\n8.  **Null Model 2 (Edge-Rewiring):** The sets $T$ and $S$ are fixed, but the graph $G$ is randomized into surrogate graphs $G'$ via degree-preserving edge rewiring. The null distribution is formed from $\\Delta(T,S)$ computed on each $G'$.\n9.  **Scenario Data:**\n    *   $N = 5000$.\n    *   Average degree $\\langle k \\rangle \\approx 8$.\n    *   Average pairwise shortest path distance on $G$: $\\bar{d}_G \\approx 3.2$.\n    *   $G$ has high clustering and modular community structure.\n    *   Surrogate graphs $G'$ from rewiring have reduced clustering and modularity.\n    *   Average pairwise shortest path distance on $G'$: $\\bar{d}_{G'} \\approx 2.7$.\n    *   Observed proximity for a specific case: $\\Delta(T,S) = 2.9$.\n    *   Set sizes: $m = 20$, $n = 30$.\n\n**Validation:**\n1.  **Scientific Groundedness:** The problem is firmly rooted in network science and systems biomedicine. The modeling of PPI networks as graphs, the use of shortest path distance for proximity, and the application of null models for statistical testing are standard and well-established methodologies. The two null models described—label permutation with degree matching and degree-preserving edge rewiring—are canonical approaches in network analysis.\n2.  **Well-Posedness:** The question is precise: \"analyze which null better controls for network topology\". The definitions are clear and the provided scenario data are consistent and serve to illustrate the core concepts. A unique, reasoned answer can be derived.\n3.  **Objectivity:** The problem is stated in objective, formal language without subjective or biased phrasing.\n\n**Verdict:** The problem statement is valid. It is scientifically sound, well-posed, objective, and complete. It presents a non-trivial conceptual challenge regarding the choice of statistical null models in network analysis. I will now proceed with the solution derivation.\n\n### Step 2: Derivation and Analysis\n\nThe central question is what it means to \"control for network topology\" in the context of this hypothesis test. The topology of the network $G$ encompasses all its structural features, from the local (degree sequence) to the meso-scale (clustering, motifs, community structure) and global (path length distribution, diameter). These features collectively determine the set of all pairwise shortest path distances $\\{d(u,v)\\}_{u,v \\in V}$. The proximity measure $\\Delta(T,S)$ is an average over a subset of these distances.\n\nThe purpose of a null model is to generate an ensemble of \"random\" configurations that share certain properties with the observed data, against which the observed statistic can be compared. An effective null model for this problem should preserve the structural context (the network topology) in which the biological hypothesis is being tested, while randomizing the specific element under investigation (the choice of genes/targets).\n\n**Analysis of Null Model 1 (Label-Permutation with Degree Matching)**\n\nThis model operates on the fixed, real network $G$. This means that the entire topological structure of the network—including its high clustering and modularity, as mentioned in the scenario—is preserved. The set of all possible shortest path distances $\\{d(u,v)\\}$ is invariant. The null hypothesis tests whether the specific sets of nodes $T$ and $S$ are significantly closer to each other than would be expected for randomly chosen sets $T'$ and $S'$ that have similar degree characteristics, *on this very same network*.\n\nThe expected value of the proximity under this null model, $E_{1}[\\Delta(T',S')]$, represents the average distance between two sets of nodes of size $m$ and $n$ with specified degree distributions on the graph $G$. This value will be influenced by the global path length properties of $G$. Given the scenario's $\\bar{d}_G \\approx 3.2$, we can anticipate $E_{1}[\\Delta(T',S')]$ to be near this value, modulated slightly by the degree-matching constraint (e.g., if $T$ and $S$ consist of high-degree hubs, the expected distance will be smaller than the global average).\n\nBy comparing the observed $\\Delta(T,S)$ to the distribution of $\\Delta(T',S')$, we are isolating the effect of the specific choice of nodes in $T$ and $S$ from the background structure of the network and from any potential sampling bias related to node degrees. This is the correct way to test for a specific, localized relationship within the fixed context of the biological network. Therefore, this model properly controls for the full network topology.\n\n**Analysis of Null Model 2 (Edge-Rewiring)**\n\nThis model fixes the node sets $T$ and $S$ but randomizes the network's edge structure, creating an ensemble of surrogate graphs $\\{G'\\}$. The only topological property that is strictly preserved is the degree sequence. As stated in the problem, higher-order structures like clustering and modularity, which are characteristic of real biological networks, are destroyed.\n\nThis destruction of topology has a direct and significant impact on shortest path distances. The process of rewiring transforms a structured, \"lumpy\" network into a more homogeneous, random one. In such random graphs, shortcuts are more plentiful, leading to a general decrease in path lengths. The scenario quantifies this effect: the average path length drops from $\\bar{d}_G \\approx 3.2$ to $\\bar{d}_{G'} \\approx 2.7$.\n\nThe expected value of the proximity under this null, $E_{2}[\\Delta(T,S)_{\\text{on } G'}]$, will therefore be centered around $\\bar{d}_{G'} \\approx 2.7$. The null hypothesis here asks: \"Is the distance between the fixed sets $T$ and $S$ on the real network $G$ different from their expected distance on a random graph that only shares the same degree sequence?\"\n\nIn our scenario, the observed proximity is $\\Delta(T,S) = 2.9$. Comparing this to the null expectation of $\\approx 2.7$, we find that the observed value is *larger* than the expectation. This test would fail to find significant proximity; it might even conclude that the nodes are significantly *separated*. This counter-intuitive result arises because the null model has created a baseline where everything is artificially close. The test is no longer about the relationship between $T$ and $S$. Instead, it is implicitly testing whether the real network's modularity and clustering (which are absent in $G'$) serve to increase path lengths relative to a random graph. This is a different scientific question and is not what is meant by testing for drug-disease proximity. This model fails to control for the relevant topological context; in fact, it actively obliterates it.\n\n**Conclusion of Analysis**\nNull Model 1 (Label-Permutation) is superior for controlling for network topology because it preserves the entire topological structure of the network $G$ as the context for the test. Null Model 2 (Edge-Rewiring) discards crucial higher-order topology, fundamentally altering the path length distribution and thus providing an inappropriate baseline for comparison.\n\n### Step 3: Evaluation of Options\n\n**A. Label-permutation with degree matching better controls for network topology because it holds $G$ fixed, preserving all path distances $d(u,v)$ induced by clustering and modularity, while adjusting for degree-driven selection bias in $T$ and $S$, thus estimating the expected $\\Delta$ against the actual topology of $G$.**\nThis statement is a precise and correct summary of the analysis. It correctly identifies that holding $G$ fixed preserves the complete topology. It accurately describes the role of degree-matching in correcting for selection bias. It rightly concludes that this procedure establishes a null hypothesis benchmarked against the actual network structure. This option is **Correct**.\n\n**B. Edge-rewiring with degree preservation better controls for network topology because it removes biases from clustering and community structure, leaving only degree effects; therefore it provides the most neutral baseline for $\\Delta$ on any network.**\nThis statement mischaracterizes clustering and community structure as \"biases\" to be removed. In the context of a biological network, these features are fundamental aspects of its organization and function, not artifacts. Destroying them does not create a \"neutral baseline\" for the original network; it creates a baseline for a different, random network. The reasoning is flawed. This option is **Incorrect**.\n\n**C. If the degree sequence is preserved, both nulls are equivalent with respect to controlling topology, since shortest path distances depend only on the degree sequence.**\nThe premise of this statement is factually wrong. Shortest path distances do not depend only on the degree sequence. Two graphs with identical degree sequences can have vastly different structures (e.g., clustering, diameter, path length distributions). The provided scenario proves this with $\\bar{d}_G \\approx 3.2$ and $\\bar{d}_{G'} \\approx 2.7$ for graphs with the same degree sequence. Therefore, the conclusion that the nulls are equivalent is also false. This option is **Incorrect**.\n\n**D. Label-permutation without degree matching is preferable for topology control because $T'$ and $S'$ are sampled uniformly, thereby canceling degree effects and community structure simultaneously on $G$.**\nWhile label-permutation on a fixed $G$ correctly preserves the topology, omitting degree matching is a serious flaw. If the true sets $T$ and $S$ are biased towards high-degree nodes (a common scenario), comparing their proximity to that of uniformly random sets will likely produce a spurious significant result. The test would simply be detecting that hubs are close to each other, not a specific functional link. The claim that this \"cancel[s] ... community structure\" is also nonsensical; sampling does not alter the structure of $G$. This option proposes a weaker null model than the one described in the problem. This option is **Incorrect**.\n\n**E. Combining both nulls by requiring significance under label-permutation on $G$ and edge-rewiring on $G'$ provides strictly better topology control than either alone because any discrepancy across nulls reflects true biological signal rather than structural artifacts.**\nThis approach confounds two distinct statistical questions. As analyzed, the two nulls test different hypotheses. A discrepancy between their outcomes is not necessarily \"true biological signal\" but rather a reflection of the different properties being controlled for. In the provided scenario, a drug-disease pair could be significantly proximal under Null Model 1 but significantly separated (or not proximal) under Null Model 2. Requiring significance under both is an overly conservative and conceptually muddled strategy, not a \"strictly better\" one. This option is **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "With a well-defined proximity metric and a suitable null model, the final step is to perform statistical inference by comparing the observed statistic to the null distribution generated via simulation. This practice  guides you through the practical mechanics of this process, focusing on how to compute and interpret empirical $p$-values from Monte Carlo replicates. You will explore how to construct valid tests for potentially skewed distributions and understand the pivotal role of the scientific hypothesis in choosing between a one-sided and a two-sided test.",
            "id": "4366966",
            "problem": "You are given a human protein–protein interaction network $G=(V,E)$. A drug $A$ has a set of targets $T_A \\subset V$ and a disease $B$ is associated with a gene module $D_B \\subset V$. Define the network proximity statistic $s_{AB}$ as the average of the shortest-path distances from each drug target to the nearest disease gene, that is:\n$$\ns_{AB} = \\frac{1}{|T_A|} \\sum_{t \\in T_A} \\min_{d \\in D_B} \\operatorname{dist}(t,d)\n$$\nSuppose that $s_{AB}$ is to be assessed under a null model where both $T_A$ and $D_B$ are randomized in the same network $G$ by sampling sets that match the observed set sizes and approximately preserve the degree sequence of the original nodes (to control for degree-related biases). You perform $M=10000$ Monte Carlo draws of null pairs $(T_A^{(m)},D_B^{(m)})$ that satisfy $|T_A^{(m)}|=|T_A|$ and $|D_B^{(m)}|=|D_B|$, and each node in the randomized sets is chosen to match the degree of a distinct node in the corresponding observed set as closely as possible.\n\nIn your real data, the observed proximity is $s_{AB}^{\\mathrm{obs}} = 2.10$. Across the $M=10000$ null draws, you compute the empirical null mean $\\hat{\\mu} = 2.70$ for $s_{AB}^{(m)}$, and you record the following tail counts:\n- $c_{\\mathrm{left}} = 285$ of the $M$ null values satisfy $s_{AB}^{(m)} \\le 2.10$,\n- $c_{\\mathrm{right}} = 21$ of the $M$ null values satisfy $s_{AB}^{(m)} \\ge 3.30$.\n\nAssume that the null distribution of $s_{AB}$ may be skewed and that you desire $p$-values that remain valid in finite samples.\n\nWhich of the following statements about computing empirical $p$-values and interpreting one-sided versus two-sided tests in this context are correct? Select all that apply.\n\nA) An appropriate empirical one-sided $p$-value for testing whether the drug is closer to the disease module than expected by chance (left-tailed proximity test) is given by the proportion of null replicates with $s_{AB}^{(m)} \\le s_{AB}^{\\mathrm{obs}}$ using an add-one correction, namely $p_{\\mathrm{prox}} = \\dfrac{1 + c_{\\mathrm{left}}}{1 + M}$. With $M=10000$ and $c_{\\mathrm{left}}=285$, this yields $p_{\\mathrm{prox}} \\approx 0.0286$.\n\nB) A valid empirical two-sided $p$-value can always be obtained by doubling the one-sided left-tail $p$-value, i.e., $p_{\\mathrm{two}} = 2\\, p_{\\mathrm{prox}}$, regardless of the shape or skewness of the null distribution.\n\nC) A valid empirical two-sided $p$-value that is robust to skewness is obtained by counting null replicates at least as extreme as $s_{AB}^{\\mathrm{obs}}$ in either tail relative to the empirical null mean $\\hat{\\mu}$, using the add-one correction:\n$$\np_{\\mathrm{two}} = \\frac{1 + \\#\\{m : s_{AB}^{(m)} \\le \\hat{\\mu} - |s_{AB}^{\\mathrm{obs}} - \\hat{\\mu}|\\} + \\#\\{m : s_{AB}^{(m)} \\ge \\hat{\\mu} + |s_{AB}^{\\mathrm{obs}} - \\hat{\\mu}|\\}}{1 + M}\n$$\nWith $\\hat{\\mu}=2.70$, $s_{AB}^{\\mathrm{obs}}=2.10$, $M=10000$, $c_{\\mathrm{left}}=285$, and $c_{\\mathrm{right}}=21$, this gives $p_{\\mathrm{two}} \\approx 0.0307$.\n\nD) Randomizing $T_A$ and $D_B$ by drawing uniformly from $V$ without regard to the degree sequence is generally unbiased for testing proximity, because degree effects average out across permutations.\n\nE) If the a priori scientific hypothesis specifies that therapeutically relevant drugs tend to be closer than expected to the disease module, then a one-sided left-tailed test is appropriate and typically more powerful than a two-sided test; a two-sided test is preferable only when deviations in either direction (proximity or separation) are of scientific interest.",
            "solution": "The problem statement has been validated and is deemed sound. It presents a standard scenario in systems biomedicine, involving the statistical assessment of network proximity between a drug's targets and a disease's associated genes. All definitions are clear, the provided data are consistent, and the questions pertain to established principles of statistical hypothesis testing.\n\nHere follows a detailed analysis of each option.\n\nThe core data provided are:\n- The protein-protein interaction network is $G=(V,E)$.\n- The drug target set is $T_A \\subset V$.\n- The disease gene set is $D_B \\subset V$.\n- The proximity statistic is $s_{AB} = \\frac{1}{|T_A|} \\sum_{t \\in T_A} \\min_{d \\in D_B} \\operatorname{dist}(t,d)$.\n- The observed proximity is $s_{AB}^{\\mathrm{obs}} = 2.10$.\n- The number of Monte Carlo null replicates is $M = 10000$.\n- The empirical mean of the null distribution is $\\hat{\\mu} = 2.70$.\n- The left-tail count is $c_{\\mathrm{left}} = \\#\\{m : s_{AB}^{(m)} \\le 2.10\\} = 285$.\n- The right-tail count for a specific threshold is $c_{\\mathrm{right}} = \\#\\{m : s_{AB}^{(m)} \\ge 3.30\\} = 21$.\n\nThe primary hypothesis is whether the drug is significantly close to the disease module, which implies a left-tailed test on the proximity (distance) statistic $s_{AB}$. A smaller value of $s_{AB}$ indicates greater proximity.\n\n### Option A Analysis\n\nThis option proposes a formula for the one-sided empirical $p$-value for a left-tailed test. The proposed formula is $p_{\\mathrm{prox}} = \\dfrac{1 + c_{\\mathrm{left}}}{1 + M}$. This is a standard and recommended formula for calculating a $p$-value from a Monte Carlo simulation. The count $c_{\\mathrm{left}}$ is the number of null replicates where the statistic is as extreme or more extreme than the observed value ($s_{AB}^{(m)} \\le s_{AB}^{\\mathrm{obs}}$). The addition of $1$ to both the numerator and the denominator, often called the \"add-one\" correction, is a device to ensure that the $p$-value is never $0$, making it a more conservative and stable estimator, especially when $c_{\\mathrm{left}}$ is very small. This formulation ensures that under the null hypothesis, the expectation of the $p$-value is not biased.\n\nLet's verify the calculation:\nGiven $M = 10000$ and $c_{\\mathrm{left}} = 285$:\n$$\np_{\\mathrm{prox}} = \\frac{1 + 285}{1 + 10000} = \\frac{286}{10001} \\approx 0.02859714\n$$\nRounding to four decimal places gives $0.0286$. The formula and the calculation are correct. This represents a valid approach for calculating the one-sided empirical $p$-value.\n\nVerdict: **Correct**.\n\n### Option B Analysis\n\nThis option claims that a valid two-sided $p$-value can **always** be obtained by doubling the one-sided left-tail $p$-value ($p_{\\mathrm{two}} = 2\\, p_{\\mathrm{prox}}$). This is a common procedure, but its validity is strictly conditional on the null distribution being symmetric about its center. The problem explicitly states that the null distribution of $s_{AB}$ \"may be skewed\". For a skewed distribution, the two tails are not of equal size. Given our data, the observed value $s_{AB}^{\\mathrm{obs}} = 2.10$ is in the left tail, and the mean is $\\hat{\\mu}=2.70$. The distance from the mean is $|2.10 - 2.70| = 0.60$. A point equidistant in the other tail would be at $2.70 + 0.60 = 3.30$. The number of null values in the left tail is $c_{\\mathrm{left}}=285$, while the number in the corresponding right tail (at or beyond $3.30$) is $c_{\\mathrm{right}}=21$. Since $285 \\neq 21$, the null distribution is demonstrably skewed. To double the left-tail probability would be to assume, incorrectly, that an equally extreme deviation in the opposite direction would have the same probability. This would lead to an inaccurate two-sided $p$-value. Therefore, the statement that this method is valid \"regardless of the shape or skewness\" is false.\n\nVerdict: **Incorrect**.\n\n### Option C Analysis\n\nThis option proposes a method for calculating a two-sided $p$-value that is robust to skewness. The method defines the region of \"extremeness\" as all values at least as far from the empirical mean $\\hat{\\mu}$ as the observed value $s_{AB}^{\\mathrm{obs}}$. The proposed formula is:\n$$\np_{\\mathrm{two}} = \\frac{1 + \\#\\{m : s_{AB}^{(m)} \\le \\hat{\\mu} - |s_{AB}^{\\mathrm{obs}} - \\hat{\\mu}|\\} + \\#\\{m : s_{AB}^{(m)} \\ge \\hat{\\mu} + |s_{AB}^{\\mathrm{obs}} - \\hat{\\mu}|\\}}{1 + M}\n$$\nThis is a principled approach. It sums the counts in both tails that are equidistant or further from the center of the null distribution. Let's compute the thresholds and counts:\n- The deviation from the mean is $|s_{AB}^{\\mathrm{obs}} - \\hat{\\mu}| = |2.10 - 2.70| = 0.60$.\n- The left-tail threshold is $\\hat{\\mu} - 0.60 = 2.70 - 0.60 = 2.10$. The number of null values at or below this threshold is given as $c_{\\mathrm{left}} = 285$.\n- The right-tail threshold is $\\hat{\\mu} + 0.60 = 2.70 + 0.60 = 3.30$. The number of null values at or above this threshold is given as $c_{\\mathrm{right}} = 21$.\n\nThe total count of extreme values is $c_{\\mathrm{left}} + c_{\\mathrm{right}} = 285 + 21 = 306$.\nApplying the add-one formula:\n$$\np_{\\mathrm{two}} = \\frac{1 + (c_{\\mathrm{left}} + c_{\\mathrm{right}})}{1 + M} = \\frac{1 + 306}{1 + 10000} = \\frac{307}{10001} \\approx 0.03069693\n$$\nRounding to four decimal places gives $0.0307$. The formula is a valid and robust method for computing a two-sided empirical $p$-value for a potentially skewed distribution, and the calculation is correct.\n\nVerdict: **Correct**.\n\n### Option D Analysis\n\nThis option suggests that a simpler null model, where random sets of nodes $T_A$ and $D_B$ are drawn uniformly from the set of all vertices $V$ without considering node degrees, is \"generally unbiased\". This statement is fundamentally incorrect and represents a major methodological flaw in network analysis. Protein-protein interaction networks are known to have highly heterogeneous degree distributions (they are \"scale-free\"). High-degree nodes (hubs) have, by definition, short paths to a large fraction of the network. If drug targets or disease genes happen to be high-degree proteins, they will exhibit small shortest-path distances to many other proteins, including each other, simply as a consequence of their high connectivity, not necessarily due to a specific biological relationship. A null model that does not control for this degree bias will systematically find \"significant\" proximity for any group of genes that includes hubs. The problem statement itself correctly notes the need to \"approximately preserve the degree sequence ... to control for degree-related biases\". Therefore, randomizing without regard to degree is a biased procedure that leads to an inflated rate of false positives.\n\nVerdict: **Incorrect**.\n\n### Option E Analysis\n\nThis option discusses the choice between a one-sided and a two-sided test. It states that if the *a priori* hypothesis is that therapeutic drugs are closer to the disease module, a one-sided left-tailed test is appropriate and more powerful. This is a core principle of statistical hypothesis testing. A one-sided test is justified when there is a directional scientific hypothesis formulated before observing the data. By concentrating the entire rejection region (e.g., $\\alpha = 0.05$) in one tail, it has greater power to detect an effect in the hypothesized direction compared to a two-sided test, which splits the rejection region between two tails (e.g., $\\alpha/2 = 0.025$ in each). The statement also correctly notes that a two-sided test is preferable when any significant deviation from the null, either proximity (closer) or separation (farther), is of scientific interest. In the context of finding a therapeutic effect, the hypothesis is almost always directional (proximity), making the one-sided test the more appropriate and powerful choice.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}