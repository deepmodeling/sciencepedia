{
    "hands_on_practices": [
        {
            "introduction": "A fundamental decision in network modeling is whether to preserve edge directionality, which is often present in biological networks like gene regulatory pathways, or to use a simpler undirected model. This exercise provides a hands-on opportunity to quantify the impact of this choice . By implementing Random Walk with Restart (RWR) on both a directed network and its symmetrized counterpart, you will directly compare the resulting node rankings and explore how information flow is altered when direction is ignored.",
            "id": "4366480",
            "problem": "A directed gene regulatory network is represented by an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ with nonnegative entries, where $A_{ij}$ denotes the strength of directed regulation from node $i$ to node $j$. Its symmetrized counterpart is defined by $A^{\\mathrm{sym}} = \\frac{1}{2}\\left(A + A^\\top\\right)$, which preserves weights while eliminating directional asymmetry. To model network propagation of a seed signal, consider a Random Walk with Restart (RWR) process on a row-stochastic transition matrix $W \\in \\mathbb{R}^{n \\times n}$ obtained by normalizing each row of $A$ to sum to $1$; if a row has sum $0$, then set $W_{ii} = 1$ for that row to create a self-loop. Let the seed distribution be a nonnegative row vector $b \\in \\mathbb{R}^{1 \\times n}$ with $\\sum_{i=1}^{n} b_i = 1$. For a restart parameter $\\alpha \\in (0,1)$, define the iterative dynamics by $s^{(0)} = b$ and\n$$\ns^{(t+1)} = \\alpha b + (1 - \\alpha) s^{(t)} W,\n$$\nfor $t = 0,1,2,\\dots$, and iterate until convergence under the $\\ell_1$ norm, i.e., until $\\left\\lVert s^{(t+1)} - s^{(t)} \\right\\rVert_1 < \\varepsilon$ for a specified tolerance $\\varepsilon > 0$. Apply this process to both the directed network $A$ and the symmetrized network $A^{\\mathrm{sym}}$ to obtain stationary propagated score vectors $s^\\star_{\\mathrm{dir}}$ and $s^\\star_{\\mathrm{sym}}$. Quantify the importance of directionality by computing two metrics: the $\\ell_1$ difference\n$$\nd_1 = \\sum_{i=1}^{n} \\left| s^\\star_{\\mathrm{dir}, i} - s^\\star_{\\mathrm{sym}, i} \\right|,\n$$\nand the Spearman rank correlation coefficient $\\rho$ between $s^\\star_{\\mathrm{dir}}$ and $s^\\star_{\\mathrm{sym}}$ (ties handled by average ranks). If the two vectors are elementwise identical within numerical tolerance, set $\\rho = 1$. If the rank correlation is undefined due to constant ranks, set $\\rho = 0$.\n\nImplement this procedure and compute $(d_1, \\rho)$ for each of the following test cases. All quantities below are dimensionless and must be treated numerically. Use $\\varepsilon = 10^{-12}$ for convergence.\n\nTest Case $1$ (happy path):\n- Size: $n = 5$.\n- Directed adjacency $A^{(1)}$:\n$$\n\\begin{bmatrix}\n0 & 1 & 0.5 & 0 & 0 \\\\\n0.2 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0.1 & 0 & 0.4 & 0 & 0\n\\end{bmatrix}\n$$\n- Seed set: nodes $\\{0, 1\\}$ with equal weights, i.e., $b = \\left[\\frac{1}{2}, \\frac{1}{2}, 0, 0, 0\\right]$.\n- Restart parameter: $\\alpha = 0.3$.\n\nTest Case $2$ (boundary condition: already symmetric directed network):\n- Size: $n = 5$.\n- Directed adjacency $A^{(2)}$:\n$$\n\\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 0\n\\end{bmatrix}\n$$\n- Seed set: node $\\{2\\}$, i.e., $b = \\left[0, 0, 1, 0, 0\\right]$.\n- Restart parameter: $\\alpha = 0.3$.\n\nTest Case $3$ (edge case: dangling sink emphasizes directionality):\n- Size: $n = 5$.\n- Directed adjacency $A^{(3)}$:\n$$\n\\begin{bmatrix}\n0 & 1 & 0.5 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n- Seed set: node $\\{0\\}$, i.e., $b = \\left[1, 0, 0, 0, 0\\right]$.\n- Restart parameter: $\\alpha = 0.15$.\n\nYour program must:\n- Construct $W$ by row-normalizing each given $A$, with the self-loop convention for zero-outgoing rows.\n- Construct $A^{\\mathrm{sym}} = \\frac{1}{2}(A + A^\\top)$ and its corresponding row-stochastic $W^{\\mathrm{sym}}$ with the same zero-row self-loop convention.\n- Iterate the RWR dynamics until convergence to obtain $s^\\star_{\\mathrm{dir}}$ and $s^\\star_{\\mathrm{sym}}$ for each test case.\n- Compute $d_1$ and $\\rho$ for each test case.\n- Output a single line containing the $6$ floating-point results formatted as a comma-separated list enclosed in square brackets, ordered as $[d_1^{(1)}, \\rho^{(1)}, d_1^{(2)}, \\rho^{(2)}, d_1^{(3)}, \\rho^{(3)}]$, with each value rounded to six decimal places.",
            "solution": "The problem is valid as it is scientifically grounded in network science, well-posed, objective, and provides a complete and consistent set of definitions and data. The task is to implement the Random Walk with Restart (RWR) algorithm to quantify the impact of network directionality.\n\nThe core of the problem lies in comparing the stationary distribution of a random walk on a directed network versus its symmetrized counterpart. A directed network is described by an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$, where $A_{ij}$ represents the directed edge weight from node $i$ to node $j$. The RWR algorithm simulates a diffusion process on this network, requiring a row-stochastic transition matrix $W$.\n\nFirst, we construct the transition matrix $W$ from a given adjacency matrix (either $A$ or its symmetrized version). For each row $i$ of the adjacency matrix, the corresponding row in $W$ is computed by normalizing its entries to sum to $1$. If a row's sum is $0$ (a node with no outgoing edges), a self-loop of weight $1$ is created for that node in $W$. This procedure can be expressed as:\n$$\nW_{ij} = \\begin{cases}\nA_{ij} / \\sum_{k=1}^{n} A_{ik} & \\text{if } \\sum_{k=1}^n A_{ik} \\neq 0 \\\\\n\\delta_{ij} & \\text{if } \\sum_{k=1}^n A_{ik} = 0\n\\end{cases}\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta.\n\nThe RWR process models the propagation of a signal from a seed distribution, represented by a probability vector $b \\in \\mathbb{R}^{1 \\times n}$. The process evolves according to the iterative equation:\n$$\ns^{(t+1)} = \\alpha b + (1 - \\alpha) s^{(t)} W\n$$\nstarting from $s^{(0)} = b$. The parameter $\\alpha \\in (0,1)$ is the restart probability, which ensures that the walker periodically returns to the seed nodes, preventing diffusion from being entirely lost in the network structure. The term $(1 - \\alpha) s^{(t)} W$ describes a single step of the random walk. This iterative process is a contraction mapping, which guarantees convergence to a unique stationary probability distribution $s^\\star$. We iterate until the $\\ell_1$ norm of the difference between successive score vectors is less than a specified tolerance $\\varepsilon = 10^{-12}$, i.e., $\\left\\lVert s^{(t+1)} - s^{(t)} \\right\\rVert_1 < \\varepsilon$.\n\nTo assess the role of directionality, we perform this RWR process on two networks:\n$1$. The original directed network, represented by its adjacency matrix $A$, yielding the stationary score vector $s^\\star_{\\mathrm{dir}}$.\n$2$. A symmetrized version of the network, represented by $A^{\\mathrm{sym}} = \\frac{1}{2}(A + A^\\top)$. This creates an undirected graph where the connection strength between two nodes is the average of the directed weights in both directions. The RWR process on this network yields $s^\\star_{\\mathrm{sym}}$.\n\nThe difference between these two outcomes is quantified using two metrics:\n$1$. The $\\ell_1$ difference, $d_1 = \\sum_{i=1}^{n} \\left| s^\\star_{\\mathrm{dir}, i} - s^\\star_{\\mathrm{sym}, i} \\right|$, which measures the total absolute deviation in node scores.\n$2$. The Spearman rank correlation coefficient, $\\rho$, which measures the monotonic agreement between the node rankings generated by the directed and symmetrized propagations. The problem specifies that if the vectors are elementwise identical, $\\rho=1$, and if the correlation is undefined due to constant ranks, $\\rho=0$.\n\nOur implementation strategy involves creating a function to perform the RWR iterative process for any given adjacency matrix, seed vector, and restart parameter. For each test case, we first compute $s^\\star_{\\mathrm{dir}}$ using the provided directed adjacency matrix $A$. We then compute $A^{\\mathrm{sym}}$, and use it to find $s^\\star_{\\mathrm{sym}}$. Finally, we calculate the $d_1$ and $\\rho$ metrics from these two vectors. The `numpy` library is used for all numerical and matrix operations, and `scipy.stats.spearmanr` is used to compute the rank correlation, with additional logic to handle the specified special cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr\n\ndef row_normalize(A):\n    \"\"\"\n    Normalizes the rows of an adjacency matrix to create a stochastic matrix W.\n    Handles rows with a sum of 0 by creating a self-loop.\n    \"\"\"\n    W = np.zeros_like(A, dtype=float)\n    row_sums = A.sum(axis=1)\n    \n    non_zero_rows_mask = row_sums != 0\n    W[non_zero_rows_mask] = A[non_zero_rows_mask] / row_sums[non_zero_rows_mask, np.newaxis]\n    \n    zero_rows_indices = np.where(row_sums == 0)[0]\n    if zero_rows_indices.size > 0:\n        W[zero_rows_indices, zero_rows_indices] = 1.0\n        \n    return W\n\ndef run_rwr(A, b, alpha, epsilon):\n    \"\"\"\n    Performs Random Walk with Restart until convergence.\n    \"\"\"\n    W = row_normalize(A)\n    s_current = b.copy()\n    \n    while True:\n        s_next = alpha * b + (1 - alpha) * s_current @ W\n        if np.linalg.norm(s_next - s_current, ord=1) < epsilon:\n            break\n        s_current = s_next\n        \n    return s_next\n\ndef calculate_spearman_rho(s1, s2):\n    \"\"\"\n    Calculates the Spearman rank correlation with special case handling.\n    \"\"\"\n    if np.allclose(s1, s2, atol=1e-12, rtol=1e-12):\n        return 1.0\n    \n    rho, _ = spearmanr(s1, s2)\n    \n    if np.isnan(rho):\n        return 0.0\n    \n    return rho\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    epsilon = 1e-12\n\n    # Test Case 1\n    A1 = np.array([\n        [0.0, 1.0, 0.5, 0.0, 0.0],\n        [0.2, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0],\n        [0.1, 0.0, 0.4, 0.0, 0.0]\n    ], dtype=float)\n    b1 = np.array([0.5, 0.5, 0.0, 0.0, 0.0], dtype=float)\n    alpha1 = 0.3\n\n    # Test Case 2\n    A2 = np.array([\n        [0.0, 1.0, 0.0, 0.0, 0.0],\n        [1.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 1.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0]\n    ], dtype=float)\n    b2 = np.array([0.0, 0.0, 1.0, 0.0, 0.0], dtype=float)\n    alpha2 = 0.3\n\n    # Test Case 3\n    A3 = np.array([\n        [0.0, 1.0, 0.5, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0],\n        [0.0, 0.0, 0.0, 0.0, 0.0]\n    ], dtype=float)\n    b3 = np.array([1.0, 0.0, 0.0, 0.0, 0.0], dtype=float)\n    alpha3 = 0.15\n\n    test_cases = [\n        (A1, b1, alpha1),\n        (A2, b2, alpha2),\n        (A3, b3, alpha3)\n    ]\n\n    all_results = []\n    for A, b, alpha in test_cases:\n        # Directed network propagation\n        s_star_dir = run_rwr(A, b, alpha, epsilon)\n        \n        # Symmetrized network propagation\n        A_sym = 0.5 * (A + A.T)\n        s_star_sym = run_rwr(A_sym, b, alpha, epsilon)\n        \n        # Compute metrics\n        d1 = np.linalg.norm(s_star_dir - s_star_sym, ord=1)\n        rho = calculate_spearman_rho(s_star_dir, s_star_sym)\n        \n        all_results.extend([d1, rho])\n\n    formatted_results = [f\"{x:.6f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Network propagation algorithms, such as heat diffusion, have parameters that must be carefully tuned to achieve optimal performance for tasks like disease gene prioritization. The diffusion time $t$ is a critical hyperparameter that controls the extent of signal spread across the network. This practice guides you through a complete validation workflow to find a robust range for $t$ , using the mean Average Precision (mAP) metric to systematically evaluate performance against a ground-truth set of disease genes.",
            "id": "4366551",
            "problem": "You are given undirected protein-protein interaction networks represented by symmetric adjacency matrices and benchmark disease gene sets split into training and testing subsets. Your task is to implement heat kernel propagation on the graph, perform a grid search over the diffusion time parameter, and evaluate performance using precision–recall to identify a robust range of diffusion times. The problem must be solved by writing a complete, runnable program as specified below.\n\nFundamental basis. Use the following fundamental definitions and well-tested facts.\n\n1. Graph and Laplacian: For an undirected simple graph with $n$ nodes, let the adjacency matrix be $A \\in \\mathbb{R}^{n \\times n}$ with entries $A_{ij} \\in \\{0,1\\}$ indicating the presence of an edge. Let the degree matrix be $D = \\mathrm{diag}(d_1, \\dots, d_n)$ where $d_i = \\sum_{j=1}^n A_{ij}$. The combinatorial graph Laplacian is $L = D - A$. The Laplacian is symmetric positive-semidefinite.\n\n2. Heat diffusion on graphs: The diffusion of a signal $f(t) \\in \\mathbb{R}^n$ over time $t \\ge 0$ under the heat equation on a graph is governed by the ordinary differential equation $\\frac{d}{dt} f(t) = - L f(t)$ with initial condition $f(0) = f_0$. The solution uses the matrix exponential and is $f(t) = \\exp(-t L) f_0$ for all $t \\ge 0$.\n\n3. Heat kernel propagation for gene prioritization: Given a set of seed nodes (training disease genes), construct an initial seed vector $y \\in \\mathbb{R}^n$ with $y_i = 1$ if node $i$ is a training positive and $y_i = 0$ otherwise. Normalize $y$ to sum to $1$ (i.e., $\\sum_i y_i = 1$) to avoid trivial scaling with $t$. For each $t$ on a specified grid, compute the diffusion score $s(t) = \\exp(-t L) y$.\n\n4. Precision–recall evaluation: To evaluate $s(t)$ for a disease gene set with a train/test split, exclude the training positives from ranking. Among the remaining nodes, define binary labels with $1$ for testing positives and $0$ for all other nodes. Rank nodes by $s_i(t)$ in descending order and compute the average precision, which is the sum of the precision values at each rank where a positive item is found, divided by the total number of positive items. Formally, if the sorted list of labels is $\\ell_1, \\dots, \\ell_m \\in \\{0,1\\}$ and there are $P = \\sum_{i=1}^m \\ell_i$ positives, let $\\mathrm{Prec}(k) = \\frac{\\sum_{i=1}^k \\ell_i}{k}$, then the average precision is $\\mathrm{AP} = \\frac{1}{P} \\sum_{k:\\,\\ell_k = 1} \\mathrm{Prec}(k)$. If a disease has no testing positives, skip it; otherwise include it. For a set of diseases, compute the mean average precision $\\mathrm{mAP}(t)$ as the arithmetic mean of $\\mathrm{AP}(t)$ across diseases included.\n\n5. Robust range: Given a grid of times $\\{t_j\\}_{j=0}^{T-1}$ and a tolerance $\\epsilon > 0$, define the robust index set $\\mathcal{R} = \\{ j \\in \\{0, \\dots, T-1\\} : \\mathrm{mAP}(t_j) \\ge \\max_{k} \\mathrm{mAP}(t_k) - \\epsilon \\}$. Compress $\\mathcal{R}$ into maximal contiguous index intervals $[a,b]$ with $a \\le b$, using zero-based indices.\n\nNetworks and disease gene sets.\n\n- Network $\\mathcal{G}_1$ with $n = 12$ nodes labeled $0$ through $11$. The undirected edge set is:\n  - Module A edges: $(0,1)$, $(1,2)$, $(2,3)$, $(3,4)$, $(4,0)$, $(1,3)$.\n  - Module B edges: $(5,6)$, $(6,7)$, $(7,8)$, $(8,5)$, $(6,8)$.\n  - Module C edges: $(9,10)$, $(10,11)$, $(11,9)$.\n  - Inter-module edges: $(2,5)$, $(3,6)$, $(4,7)$, $(8,9)$, $(1,10)$.\n  Construct $A$ by setting $A_{ij} = 1$ for each listed edge $(i,j)$ and $A_{ji} = 1$ by symmetry, and $A_{ii} = 0$ for all $i$.\n\n- Network $\\mathcal{G}_2$ with $n = 12$ nodes labeled $0$ through $11$. It is identical to $\\mathcal{G}_1$ but with all inter-module edges removed. That is, only the module edges listed above remain; the graph has three disconnected components.\n\n- Disease gene sets (same for both networks):\n  - Disease $\\mathcal{D}_1$: positives $\\{0,1,2,3,4\\}$, with training positives $\\{0,2,4\\}$ and testing positives $\\{1,3\\}$.\n  - Disease $\\mathcal{D}_2$: positives $\\{5,6,7,8\\}$, with training positives $\\{5,7\\}$ and testing positives $\\{6,8\\}$.\n  - Disease $\\mathcal{D}_3$: positives $\\{9,10,11\\}$, with training positives $\\{9\\}$ and testing positives $\\{10,11\\}$.\n\nInstructions. Implement the following for each specified test case:\n\n1. Build the adjacency matrix $A$, degree matrix $D$, and Laplacian $L = D - A$.\n2. For each disease $\\mathcal{D}_k$, build the initial seed vector $y^{(k)}$ normalized to sum to $1$ using the disease’s training positives. For each $t$ on the case’s grid, compute scores $s^{(k)}(t) = \\exp(-t L) y^{(k)}$.\n3. For each disease, evaluate $s^{(k)}(t)$ by excluding training positives from consideration, labeling testing positives as ones and all other considered nodes as zeros, computing the average precision $\\mathrm{AP}^{(k)}(t)$ as defined above. Compute $\\mathrm{mAP}(t)$ as the mean of $\\mathrm{AP}^{(k)}(t)$ across $k$ with non-empty testing sets.\n4. Over the grid, compute $\\mathcal{R}$ and compress it into maximal contiguous index intervals $[a,b]$ using zero-based indexing.\n5. Report, for each test case, the list of intervals $[a,b]$ as lists of two integers.\n\nTest suite. Run your program on the following three test cases:\n\n- Case $1$: Network $\\mathcal{G}_1$, time grid $t \\in \\{0.1, 0.2, 0.3, \\dots, 2.0\\}$ in steps of $0.1$ (i.e., $20$ values), tolerance $\\epsilon = 0.01$.\n\n- Case $2$: Network $\\mathcal{G}_1$, time grid $t \\in \\{0.0, 0.05, 0.1, 0.2, 0.5\\}$, tolerance $\\epsilon = 0.005$.\n\n- Case $3$: Network $\\mathcal{G}_2$, time grid $t \\in \\{0.05, 0.1, 0.2, 0.5, 1.0, 2.0\\}$, tolerance $\\epsilon = 0.01$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one case and is itself a list of intervals $[a,b]$ represented as a list of two integers. For example, a valid output could look like \"[[[0,3],[5,7]],[[2,2]],[[0,5]]]\" if there are two robust intervals for case $1$, one for case $2$, and one for case $3$. No additional text should be printed.",
            "solution": "The problem statement provided is self-contained, scientifically grounded in established principles of graph theory and network science, and computationally well-posed. All definitions, data, and constraints are specified with sufficient precision to permit a unique and verifiable solution. The underlying concepts—the graph Laplacian, the heat equation on graphs solved via the matrix exponential, and performance evaluation using mean average precision—are standard and correctly formulated. The problem is therefore deemed valid.\n\nThe solution is implemented by methodically following the principles and instructions outlined.\n\n**1. Graph Representation and the Laplacian Operator**\n\nThe foundation of network diffusion is the graph's structure, which is mathematically represented by the adjacency matrix $A$ and the graph Laplacian $L$. For an undirected graph with $n$ nodes, the adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ has entries $A_{ij} = 1$ if an edge exists between nodes $i$ and $j$, and $A_{ij} = 0$ otherwise. The degree of a node $i$, $d_i$, is the number of edges connected to it, calculated as $d_i = \\sum_{j=1}^n A_{ij}$. The degree matrix $D$ is a diagonal matrix with degrees on the diagonal, $D = \\mathrm{diag}(d_1, \\dots, d_n)$.\n\nThe combinatorial graph Laplacian is defined as $L = D - A$. This operator is central to diffusion processes on graphs, as it is the discrete analogue of the continuous Laplace-Beltrami operator. For this problem, we construct the Laplacian matrices $L_1$ and $L_2$ corresponding to networks $\\mathcal{G}_1$ and $\\mathcal{G}_2$, respectively. $\\mathcal{G}_1$ is a single connected component, while $\\mathcal{G}_2$ is composed of three disconnected components, which results in a block-diagonal Laplacian matrix $L_2$.\n\n**2. Heat Kernel Propagation for Gene Prioritization**\n\nThe diffusion of information (in this context, \"heat\" representing disease association) from a set of seed nodes is modeled by the graph heat equation:\n$$\n\\frac{d}{dt} f(t) = - L f(t)\n$$\nwhere $f(t) \\in \\mathbb{R}^n$ is a vector of scores on the nodes at time $t$. Given an initial distribution of heat $f(0) = f_0$, the solution at time $t$ is given by:\n$$\nf(t) = \\exp(-t L) f_0\n$$\nThe matrix $K(t) = \\exp(-t L)$ is known as the heat kernel.\n\nIn our context, for each disease $\\mathcal{D}_k$, we define an initial seed vector $y^{(k)}$. This vector is non-zero only at the indices corresponding to the training disease genes for $\\mathcal{D}_k$, where it is set to $1$. To ensure the total \"heat\" is conserved and comparable across diseases, this vector is normalized to sum to $1$. The propagation is then performed for each time $t$ in the specified grid by computing the score vector $s^{(k)}(t)$:\n$$\ns^{(k)}(t) = \\exp(-t L) y^{(k)}\n$$\nThis computation is performed using a numerically stable algorithm for the matrix exponential, available in scientific computing libraries.\n\n**3. Performance Evaluation via Mean Average Precision (mAP)**\n\nTo quantify the effectiveness of the diffusion process for prioritizing disease genes, we use the average precision (AP) metric. For each disease $\\mathcal{D}_k$ and time point $t$, we evaluate the ranking produced by the score vector $s^{(k)}(t)$. The set of training positives for $\\mathcal{D}_k$ is excluded from the ranking, as their scores are trivially high due to being the sources of diffusion. The remaining nodes are ranked in descending order of their scores.\n\nThe set of testing positives for $\\mathcal{D}_k$ serves as the ground truth. The average precision, $\\mathrm{AP}^{(k)}(t)$, is calculated based on the ranks of these testing positives. If there are $P$ testing positives, and they are found at ranks $r_1, r_2, \\dots, r_P$, the precision at each of these ranks, $\\mathrm{Prec}(r_i)$, is the fraction of true positives found up to that rank. The AP is the average of these precision values:\n$$\n\\mathrm{AP}^{(k)}(t) = \\frac{1}{P} \\sum_{i=1}^{P} \\mathrm{Prec}(r_i)\n$$\nThis metric rewards algorithms that rank positive items higher. To obtain a single performance measure across all diseases at a given time $t$, we compute the mean average precision, $\\mathrm{mAP}(t)$, which is the arithmetic mean of the $\\mathrm{AP}^{(k)}(t)$ values for all diseases with a non-empty set of testing positives.\n\n**4. Identification of the Robust Parameter Range**\n\nThe diffusion time $t$ is a critical hyperparameter. A robust algorithm should perform well not just at a single optimal $t$, but over a range of values. To identify this range, we first calculate $\\mathrm{mAP}(t)$ for all $t$ in the specified grid. We then find the maximum performance, $\\max_{k} \\mathrm{mAP}(t_k)$.\n\nThe robust index set $\\mathcal{R}$ is defined as the set of all zero-based indices $j$ for which the performance is close to the maximum, controlled by a tolerance parameter $\\epsilon$:\n$$\n\\mathcal{R} = \\{ j \\mid \\mathrm{mAP}(t_j) \\ge \\max_{k} \\mathrm{mAP}(t_k) - \\epsilon \\}\n$$\nFinally, this set of indices is compressed into a minimal set of maximal contiguous intervals $[a, b]$, where $a$ and $b$ are the start and end indices of each contiguous block. This provides a compact representation of the time parameter ranges where the method yields robust, near-optimal performance. The entire procedure is systematically applied to each test case to derive the final results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm\nimport json\n\ndef get_network_laplacian(name):\n    \"\"\"\n    Builds the adjacency matrix and combinatorial Laplacian for G1 or G2.\n    \"\"\"\n    n = 12\n    A = np.zeros((n, n), dtype=float)\n    \n    module_A_edges = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 0), (1, 3)]\n    module_B_edges = [(5, 6), (6, 7), (7, 8), (8, 5), (6, 8)]\n    module_C_edges = [(9, 10), (10, 11), (11, 9)]\n    inter_module_edges = [(2, 5), (3, 6), (4, 7), (8, 9), (1, 10)]\n\n    edges = module_A_edges + module_B_edges + module_C_edges\n    if name == 'G1':\n        edges += inter_module_edges\n\n    for i, j in edges:\n        A[i, j] = 1\n        A[j, i] = 1\n        \n    D = np.diag(np.sum(A, axis=1))\n    L = D - A\n    return L\n\ndef calculate_ap(scores, train_pos, test_pos, n_nodes):\n    \"\"\"\n    Calculates Average Precision for a given ranking.\n    \"\"\"\n    if not test_pos:\n        return np.nan  # As per problem, skip diseases with no test positives\n\n    all_nodes = set(range(n_nodes))\n    eval_nodes_set = all_nodes - train_pos\n    eval_nodes = sorted(list(eval_nodes_set))\n    \n    eval_scores = scores[eval_nodes]\n    \n    # Use stable sort to handle ties consistently by node index\n    sorted_indices = np.argsort(eval_scores, kind='stable')[::-1]\n    \n    ranked_nodes = [eval_nodes[i] for i in sorted_indices]\n    \n    num_positives = len(test_pos)\n    hits = 0\n    sum_prec = 0.0\n    \n    for i, node in enumerate(ranked_nodes):\n        if node in test_pos:\n            hits += 1\n            precision_at_k = hits / (i + 1)\n            sum_prec += precision_at_k\n            \n    return sum_prec / num_positives if num_positives > 0 else 0.0\n\ndef find_robust_intervals(maps, epsilon):\n    \"\"\"\n    Finds maximal contiguous robust index intervals from a list of mAP scores.\n    \"\"\"\n    if not maps:\n        return []\n\n    maps_array = np.array(maps)\n    max_map = np.max(maps_array)\n    # np.where returns a tuple of arrays, we need the first one\n    robust_indices = np.where(maps_array >= max_map - epsilon)[0].tolist()\n    \n    if not robust_indices:\n        return []\n\n    intervals = []\n    start = robust_indices[0]\n    end = robust_indices[0]\n\n    for i in range(1, len(robust_indices)):\n        if robust_indices[i] == end + 1:\n            end = robust_indices[i]\n        else:\n            intervals.append([start, end])\n            start = robust_indices[i]\n            end = robust_indices[i]\n    intervals.append([start, end])\n    \n    return intervals\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'network': 'G1', 'times': np.round(np.arange(0.1, 2.01, 0.1), 2), 'epsilon': 0.01},\n        {'network': 'G1', 'times': np.array([0.0, 0.05, 0.1, 0.2, 0.5]), 'epsilon': 0.005},\n        {'network': 'G2', 'times': np.array([0.05, 0.1, 0.2, 0.5, 1.0, 2.0]), 'epsilon': 0.01},\n    ]\n\n    diseases = [\n        {'train': {0, 2, 4}, 'test': {1, 3}},\n        {'train': {5, 7}, 'test': {6, 8}},\n        {'train': {9}, 'test': {10, 11}},\n    ]\n    \n    n_nodes = 12\n\n    # Pre-calculate Laplacians and initial seed vectors\n    laplacians = {\n        'G1': get_network_laplacian('G1'),\n        'G2': get_network_laplacian('G2'),\n    }\n    \n    y_vectors = []\n    for disease in diseases:\n        y = np.zeros(n_nodes)\n        if disease['train']:\n            y[list(disease['train'])] = 1.0\n            y /= np.sum(y)\n        y_vectors.append(y)\n        \n    all_case_results = []\n    \n    for case in test_cases:\n        L = laplacians[case['network']]\n        times = case['times']\n        epsilon = case['epsilon']\n        \n        map_over_time = []\n        for t in times:\n            if t == 0.0:\n                K = np.identity(n_nodes)\n            else:\n                K = expm(-t * L)\n\n            ap_scores_for_t = []\n            for i, disease in enumerate(diseases):\n                y = y_vectors[i]\n                scores = K @ y\n                ap = calculate_ap(scores, disease['train'], disease['test'], n_nodes)\n                if not np.isnan(ap):\n                    ap_scores_for_t.append(ap)\n            \n            mAP = np.mean(ap_scores_for_t) if ap_scores_for_t else 0.0\n            map_over_time.append(mAP)\n\n        intervals = find_robust_intervals(map_over_time, epsilon)\n        all_case_results.append(intervals)\n    \n    # Format output to be compact JSON-like string, without spaces\n    formatted_results = [json.dumps(res, separators=(',', ':')) for res in all_case_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A common challenge when interpreting the results of network propagation is \"degree confounding,\" where high-degree nodes (hubs) tend to receive high scores simply because they have many connections, not necessarily because they are close to the seed nodes. This can obscure true biological signal. This exercise provides a clear, calculable example of how to identify and correct for this bias , using a least-squares approach to partial out the contribution of node degree from the final scores and revealing a more meaningful ranking.",
            "id": "4366495",
            "problem": "You are given a small Protein-Protein Interaction (PPI) network modeled as an undirected simple graph to study degree confounding in network propagation within systems biomedicine. Let the vertex set be $\\{A,B,C,D,E\\}$ and define the adjacency matrix $A \\in \\mathbb{R}^{5 \\times 5}$ (rows and columns ordered as $A,B,C,D,E$) by\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 0\n\\end{pmatrix}.\n$$\nLet the degree vector be $d \\in \\mathbb{R}^{5}$ with entries $d_i$ equal to the degree of node $i$, so $d = A \\mathbf{1}$, where $\\mathbf{1} \\in \\mathbb{R}^{5}$ is the all-ones vector. Define the diagonal degree matrix $D = \\mathrm{diag}(d)$ and the row-stochastic transition matrix $P = D^{-1} A$. Consider a one-step diffusion of a disease-relevance seed signal $y \\in \\mathbb{R}^{5}$ given by $y = (1,0,0,0,3)^{\\top}$, representing relative initial intensities on nodes $A$ and $E$. The one-step propagated score is $s = P y$.\n\nDegree confounding in propagation refers to the tendency of $s$ to correlate with $d$, which can inflate scores for high-degree nodes independent of biologically meaningful signal. To analyze and correct for this, derive a degree-corrected score vector $s_{\\mathrm{corr}}$ by removing, in the least-squares sense, the components of $s$ explained by the subspace spanned by $\\mathbf{1}$ and $d$. That is, find coefficients $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$ that minimize $\\| s - a \\mathbf{1} - b d \\|_{2}$, and define $s_{\\mathrm{corr}} = s - a \\mathbf{1} - b d$.\n\nThen:\n- Compute the original propagated score $s$ and the corrected score $s_{\\mathrm{corr}}$.\n- Rank the nodes in descending order by $s$ and by $s_{\\mathrm{corr}}$. In the presence of ties, assign average ranks to tied nodes.\n- Compute the Spearman's rank correlation coefficient (Spearman's rho) between the two rankings.\n\nRound your final Spearman's rho value to four significant figures. No units are required. Express the final numerical value only, without any additional text, inside the answer box.",
            "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed scientific problem. The procedure will be to solve it step-by-step as requested.\n\nThe problem asks for the analysis of degree confounding in a network propagation scenario on a small graph. The steps are:\n1.  Compute the relevant network matrices and the propagated score vector $s$.\n2.  Solve a least-squares problem to find the projection of $s$ onto the subspace spanned by the all-ones vector and the degree vector.\n3.  Compute the degree-corrected score vector $s_{\\mathrm{corr}}$.\n4.  Rank the nodes according to both $s$ and $s_{\\mathrm{corr}}$.\n5.  Compute the Spearman's rank correlation coefficient between the two rankings.\n\n**Step 1: Compute network properties and the propagated score $s$**\n\nThe adjacency matrix is given as:\n$$\nA = \\begin{pmatrix}\n0 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 0\n\\end{pmatrix}\n$$\nThe vertex set is $\\{A,B,C,D,E\\}$, corresponding to indices $1, 2, 3, 4, 5$. The degree vector $d$ is the sum of rows of $A$:\n$d_1 = d_A = 0+1+1+0+0 = 2$\n$d_2 = d_B = 1+0+1+0+0 = 2$\n$d_3 = d_C = 1+1+0+1+0 = 3$\n$d_4 = d_D = 0+0+1+0+1 = 2$\n$d_5 = d_E = 0+0+0+1+0 = 1$\nSo, the degree vector is $d = (2, 2, 3, 2, 1)^{\\top}$.\n\nThe diagonal degree matrix $D$ and its inverse $D^{-1}$ are:\n$$\nD = \\mathrm{diag}(2, 2, 3, 2, 1) = \\begin{pmatrix}\n2 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 \\\\\n0 & 0 & 3 & 0 & 0 \\\\\n0 & 0 & 0 & 2 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{pmatrix}, \\quad D^{-1} = \\begin{pmatrix}\n1/2 & 0 & 0 & 0 & 0 \\\\\n0 & 1/2 & 0 & 0 & 0 \\\\\n0 & 0 & 1/3 & 0 & 0 \\\\\n0 & 0 & 0 & 1/2 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThe row-stochastic transition matrix $P$ is $P = D^{-1} A$:\n$$\nP = \\begin{pmatrix}\n1/2 & 0 & 0 & 0 & 0 \\\\\n0 & 1/2 & 0 & 0 & 0 \\\\\n0 & 0 & 1/3 & 0 & 0 \\\\\n0 & 0 & 0 & 1/2 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 0\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 & 1/2 & 1/2 & 0 & 0 \\\\\n1/2 & 0 & 1/2 & 0 & 0 \\\\\n1/3 & 1/3 & 0 & 1/3 & 0 \\\\\n0 & 0 & 1/2 & 0 & 1/2 \\\\\n0 & 0 & 0 & 1 & 0\n\\end{pmatrix}\n$$\nThe initial seed signal is $y = (1, 0, 0, 0, 3)^{\\top}$. The one-step propagated score is $s = Py$:\n$$\ns = \\begin{pmatrix}\n0 & 1/2 & 1/2 & 0 & 0 \\\\\n1/2 & 0 & 1/2 & 0 & 0 \\\\\n1/3 & 1/3 & 0 & 1/3 & 0 \\\\\n0 & 0 & 1/2 & 0 & 1/2 \\\\\n0 & 0 & 0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 3\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 \\\\\n1/2 \\\\\n1/3 \\\\\n3/2 \\\\\n0\n\\end{pmatrix}\n$$\nSo, the original propagated score vector is $s = (0, 1/2, 1/3, 3/2, 0)^{\\top}$.\n\n**Step 2: Solve the least-squares problem**\n\nWe want to find coefficients $a, b \\in \\mathbb{R}$ that minimize $\\| s - a \\mathbf{1} - b d \\|_{2}$, where $\\mathbf{1} = (1, 1, 1, 1, 1)^{\\top}$ and $d = (2, 2, 3, 2, 1)^{\\top}$. This is a linear least-squares problem. Let $X = [\\mathbf{1} \\; d]$. The solution $\\begin{pmatrix} a \\\\ b \\end{pmatrix}$ is given by the normal equations: $X^{\\top}X \\begin{pmatrix} a \\\\ b \\end{pmatrix} = X^{\\top}s$.\n\nFirst, we compute the matrices $X^{\\top}X$ and $X^{\\top}s$:\n$\\mathbf{1}^{\\top}\\mathbf{1} = 1^2+1^2+1^2+1^2+1^2 = 5$\n$\\mathbf{1}^{\\top}d = d^{\\top}\\mathbf{1} = 1(2)+1(2)+1(3)+1(2)+1(1) = 10$\n$d^{\\top}d = 2^2+2^2+3^2+2^2+1^2 = 4+4+9+4+1 = 22$\n$$\nX^{\\top}X = \\begin{pmatrix}\n\\mathbf{1}^{\\top}\\mathbf{1} & \\mathbf{1}^{\\top}d \\\\\nd^{\\top}\\mathbf{1} & d^{\\top}d\n\\end{pmatrix} = \\begin{pmatrix}\n5 & 10 \\\\\n10 & 22\n\\end{pmatrix}\n$$\nNext, we compute $X^{\\top}s$:\n$\\mathbf{1}^{\\top}s = 0 + 1/2 + 1/3 + 3/2 + 0 = 2 + 1/3 = 7/3$\n$d^{\\top}s = 2(0) + 2(1/2) + 3(1/3) + 2(3/2) + 1(0) = 0 + 1 + 1 + 3 + 0 = 5$\n$$\nX^{\\top}s = \\begin{pmatrix} \\mathbf{1}^{\\top}s \\\\ d^{\\top}s \\end{pmatrix} = \\begin{pmatrix} 7/3 \\\\ 5 \\end{pmatrix}\n$$\nWe solve the system $\\begin{pmatrix} 5 & 10 \\\\ 10 & 22 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} 7/3 \\\\ 5 \\end{pmatrix}$.\nThe inverse of $X^{\\top}X$ is:\n$(X^{\\top}X)^{-1} = \\frac{1}{5(22) - 10(10)} \\begin{pmatrix} 22 & -10 \\\\ -10 & 5 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 22 & -10 \\\\ -10 & 5 \\end{pmatrix}$\nNow, we find $a$ and $b$:\n$$\n\\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 22 & -10 \\\\ -10 & 5 \\end{pmatrix} \\begin{pmatrix} 7/3 \\\\ 5 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 22(7/3) - 10(5) \\\\ -10(7/3) + 5(5) \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 154/3 - 150/3 \\\\ -70/3 + 75/3 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4/3 \\\\ 5/3 \\end{pmatrix} = \\begin{pmatrix} 4/30 \\\\ 5/30 \\end{pmatrix}\n$$\nSo, $a = 2/15$ and $b = 1/6$.\n\n**Step 3: Compute the corrected score $s_{\\mathrm{corr}}$**\n\nThe corrected score is $s_{\\mathrm{corr}} = s - a \\mathbf{1} - b d$.\n$s_{\\mathrm{corr}, A} = s_1 - a(1) - b(d_1) = 0 - 2/15 - (1/6)(2) = -2/15 - 1/3 = -2/15 - 5/15 = -7/15$\n$s_{\\mathrm{corr}, B} = s_2 - a(1) - b(d_2) = 1/2 - 2/15 - (1/6)(2) = 1/2 - 2/15 - 1/3 = 15/30 - 4/30 - 10/30 = 1/30$\n$s_{\\mathrm{corr}, C} = s_3 - a(1) - b(d_3) = 1/3 - 2/15 - (1/6)(3) = 1/3 - 2/15 - 1/2 = 10/30 - 4/30 - 15/30 = -9/30 = -3/10$\n$s_{\\mathrm{corr}, D} = s_4 - a(1) - b(d_4) = 3/2 - 2/15 - (1/6)(2) = 3/2 - 2/15 - 1/3 = 45/30 - 4/30 - 10/30 = 31/30$\n$s_{\\mathrm{corr}, E} = s_5 - a(1) - b(d_5) = 0 - 2/15 - (1/6)(1) = -2/15 - 1/6 = -4/30 - 5/30 = -9/30 = -3/10$\nThus, the corrected score vector is $s_{\\mathrm{corr}} = (-7/15, 1/30, -3/10, 31/30, -3/10)^{\\top}$.\n\n**Step 4: Rank the nodes**\n\nWe rank the nodes in descending order for both score vectors.\nOriginal scores $s$: $A(0)$, $B(0.5)$, $C(1/3 \\approx 0.333)$, $D(1.5)$, $E(0)$.\nThe order is $D > B > C > A, E$.\nThe ranks $R_s$ are:\n- $D$: Rank $1$\n- $B$: Rank $2$\n- $C$: Rank $3$\n- $A, E$: Tied for ranks $4$ and $5$. Average rank is $(4+5)/2 = 4.5$.\nSo, $R_s = (4.5, 2, 3, 1, 4.5)^{\\top}$.\n\nCorrected scores $s_{\\mathrm{corr}}$: $A(-7/15 \\approx -0.467)$, $B(1/30 \\approx 0.033)$, $C(-3/10 = -0.3)$, $D(31/30 \\approx 1.033)$, $E(-3/10 = -0.3)$.\nThe order is $D > B > C, E > A$.\nThe ranks $R_{s_{\\mathrm{corr}}}$ are:\n- $D$: Rank $1$\n- $B$: Rank $2$\n- $C, E$: Tied for ranks $3$ and $4$. Average rank is $(3+4)/2 = 3.5$.\n- $A$: Rank $5$\nSo, $R_{s_{\\mathrm{corr}}} = (5, 2, 3.5, 1, 3.5)^{\\top}$.\n\n**Step 5: Compute Spearman's rank correlation coefficient**\n\nSpearman's rho, $\\rho$, is the Pearson correlation coefficient of the rank variables.\nFor two rank vectors $R_x$ and $R_y$ of size $n$, the formula is:\n$$ \\rho = \\frac{\\sum_{i=1}^n (R_{x,i} - \\bar{R}_x)(R_{y,i} - \\bar{R}_y)}{\\sqrt{\\sum_{i=1}^n (R_{x,i} - \\bar{R}_x)^2 \\sum_{i=1}^n (R_{y,i} - \\bar{R}_y)^2}} $$\nHere, $n=5$. The mean rank is $\\bar{R} = (n+1)/2 = (5+1)/2 = 3$.\nLet $x_i = R_{s,i}$ and $y_i = R_{s_{\\mathrm{corr}},i}$.\n$R_s = (4.5, 2, 3, 1, 4.5)$\n$R_{s_{\\mathrm{corr}}} = (5, 2, 3.5, 1, 3.5)$\nDeviations from the mean $\\bar{R}=3$:\n$R_s - \\bar{R} = (1.5, -1, 0, -2, 1.5)$\n$R_{s_{\\mathrm{corr}}} - \\bar{R} = (2, -1, 0.5, -2, 0.5)$\n\nNumerator (covariance term):\n$\\sum_{i=1}^5 (R_{s,i} - \\bar{R})(R_{s_{\\mathrm{corr}},i} - \\bar{R}) = (1.5)(2) + (-1)(-1) + (0)(0.5) + (-2)(-2) + (1.5)(0.5)$\n$= 3 + 1 + 0 + 4 + 0.75 = 8.75$\n\nDenominator (variance terms):\n$\\sum_{i=1}^5 (R_{s,i} - \\bar{R})^2 = (1.5)^2 + (-1)^2 + 0^2 + (-2)^2 + (1.5)^2 = 2.25 + 1 + 0 + 4 + 2.25 = 9.5$\n$\\sum_{i=1}^5 (R_{s_{\\mathrm{corr}},i} - \\bar{R})^2 = 2^2 + (-1)^2 + (0.5)^2 + (-2)^2 + (0.5)^2 = 4 + 1 + 0.25 + 4 + 0.25 = 9.5$\n\nNow, compute $\\rho$:\n$$ \\rho = \\frac{8.75}{\\sqrt{9.5 \\cdot 9.5}} = \\frac{8.75}{9.5} = \\frac{35/4}{19/2} = \\frac{35}{4} \\cdot \\frac{2}{19} = \\frac{35}{38} $$\nTo get the final numerical value, we perform the division:\n$\\rho = 35 / 38 \\approx 0.92105263...$\nRounding to four significant figures gives $0.9211$.",
            "answer": "$$\\boxed{0.9211}$$"
        }
    ]
}