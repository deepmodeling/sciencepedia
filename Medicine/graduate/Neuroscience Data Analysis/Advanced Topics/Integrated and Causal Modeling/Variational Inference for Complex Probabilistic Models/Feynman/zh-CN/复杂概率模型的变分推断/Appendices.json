{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握变分自编码器（VAEs），就必须理解其优化机制。本练习将引导您推导证据下界（ELBO）的两个基本组成部分：使用重参数化技巧估算重构项的梯度，以及计算KL散度正则化项的闭合形式解。这些技能对于实现和调试现代深度生成模型至关重要。",
            "id": "4203098",
            "problem": "一个神经科学实验室正在使用带有变分自编码器 (VAE) 的潜变量模型来建模高维神经记录 $x \\in \\mathbb{R}^{n}$（例如，分箱的脉冲计数或钙荧光特征）。设潜变量为 $z \\in \\mathbb{R}^{d}$。生成模型指定了先验 $p(z)$ 和条件似然 $p_{\\theta}(x \\mid z)$。摊销近似后验为 $q_{\\phi}(z \\mid x)$，训练目标是证据下界 (ELBO)，对于单个观测值 $x$ 定义为\n$$\n\\mathcal{L}(x;\\theta,\\phi) \\equiv \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\ln p_{\\theta}(x \\mid z) \\right] - \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right),\n$$\n其中 $\\mathrm{KL}(\\cdot \\,\\|\\, \\cdot)$ 表示 Kullback-Leibler 散度。\n\n假设以下标准选择：\n- 先验是标准正态分布，$p(z) = \\mathcal{N}(z; 0, I_{d})$。\n- 变分族是对角高斯分布，$q_{\\phi}(z \\mid x) = \\mathcal{N}\\!\\bigl(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x))\\bigr)$，其中 $\\mu_{\\phi}(x) \\in \\mathbb{R}^{d}$ 且 $\\sigma_{\\phi}(x) \\in \\mathbb{R}_{+}^{d}$。等价地，您可以通过对数方差向量 $\\ell_{\\phi}(x) \\in \\mathbb{R}^{d}$ 进行参数化，其中对于每个坐标 $i \\in \\{1,\\dots,d\\}$，有 $\\sigma_{\\phi,i}^{2}(x) = \\exp(\\ell_{\\phi,i}(x))$。\n- 条件似然 $p_{\\theta}(x \\mid z)$ 关于 $z$ 对于几乎所有的 $z$ 都是可微的。\n\n任务：\n- 使用重参数化技巧，其中 $\\epsilon \\sim \\mathcal{N}(0, I_{d})$ 且 $z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon$，从第一性原理推导梯度 $\\nabla_{\\phi}\\,\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\ln p_{\\theta}(x \\mid z) \\right]$ 的表达式，该表达式应为关于 $\\epsilon$ 的期望。您的表达式应明确说明 $\\ln p_{\\theta}(x \\mid z)$ 关于 $z$ 的梯度如何与 $\\mu_{\\phi}(x)$ 和 $\\sigma_{\\phi}(x)$ (或 $\\ell_{\\phi}(x)$) 关于 $\\phi$ 的雅可比矩阵耦合。\n- 计算正则化项 $\\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)$ 的闭式表达式，用 $\\mu_{\\phi}(x)$ 和 $\\sigma_{\\phi}(x)$ (或等价地 $\\ell_{\\phi}(x)$) 表示。假设 $d \\geq 1$ 是有限的。\n\n对于单个观测值 $x$，以 Kullback-Leibler 散度的单个闭式解析表达式的形式提供最终答案。不需要数值近似或四舍五入。最终答案中不要包含单位。",
            "solution": "问题陈述已经过验证，被认为是有效的。它具有科学依据、良定且客观。它为变分自编码器 (VAE) 提供了一套标准的推导，VAE 是现代机器学习及其应用（包括神经科学）中的一个基础模型。所有必需的信息都已提供，并且任务在数学上是精确的。\n\n问题包括两个任务：第一，推导证据下界 (ELBO) 的重建项关于变分参数 $\\phi$ 的梯度表达式；第二，计算 Kullback-Leibler (KL) 散度项的闭式表达式。\n\n让我们将变分后验的参数向量表示为 $\\phi \\in \\mathbb{R}^P$。量 $\\mu_{\\phi}(x)$ 和 $\\sigma_{\\phi}(x)$ 是由 $\\phi$ 参数化的 $x$ 的函数。\n\n**任务1：重建项的梯度**\n\n第一个任务是推导梯度 $\\nabla_{\\phi}\\,\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\ln p_{\\theta}(x \\mid z) \\right]$ 的表达式。这一项通常被称为重建项。直接求导是有问题的，因为求期望所依据的分布 $q_{\\phi}(z \\mid x)$ 依赖于参数 $\\phi$。我们使用重参数化技巧来解决这个问题。\n\n让我们感兴趣的项为 $J(\\phi) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\ln p_{\\theta}(x \\mid z) \\right]$。\n变分后验给出为 $q_{\\phi}(z \\mid x) = \\mathcal{N}\\!\\bigl(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x))\\bigr)$。一个随机变量 $z \\sim q_{\\phi}(z \\mid x)$ 可以表示为一个无参数随机变量 $\\epsilon \\sim \\mathcal{N}(0, I_{d})$ 的确定性变换。重参数化由以下公式给出：\n$$ z = g(\\phi, \\epsilon, x) = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon $$\n其中 $\\odot$ 表示逐元素乘积。这里，$\\mu_{\\phi}(x) \\in \\mathbb{R}^d$，$\\sigma_{\\phi}(x) \\in \\mathbb{R}^d$，以及 $\\epsilon \\in \\mathbb{R}^d$。$z$ 的第 $i$ 个分量是 $z_i = \\mu_{\\phi,i}(x) + \\sigma_{\\phi,i}(x) \\epsilon_i$。\n\n使用这种重参数化，我们可以将关于 $q_{\\phi}(z \\mid x)$ 的期望重写为关于固定分布 $p(\\epsilon) = \\mathcal{N}(\\epsilon; 0, I_d)$ 的期望：\n$$ J(\\phi) = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I_d)}\\!\\left[ \\ln p_{\\theta}(x \\mid z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon) \\right] $$\n现在，关于 $\\phi$ 的梯度可以移到期望内部，因为 $\\epsilon$ 的分布不依赖于 $\\phi$。\n$$ \\nabla_{\\phi} J(\\phi) = \\nabla_{\\phi} \\mathbb{E}_{\\epsilon}\\!\\left[ \\ln p_{\\theta}(x \\mid z) \\right] = \\mathbb{E}_{\\epsilon}\\!\\left[ \\nabla_{\\phi} \\ln p_{\\theta}(x \\mid z) \\right] $$\n我们应用多变量链式法则来计算内部梯度。参数 $\\phi$ 仅通过 $z$ 影响 $\\ln p_{\\theta}(x \\mid z)$。\n$$ \\nabla_{\\phi} \\ln p_{\\theta}(x \\mid z) = \\left( \\frac{\\partial z}{\\partial \\phi} \\right)^T \\nabla_{z} \\ln p_{\\theta}(x \\mid z) $$\n这里，$\\frac{\\partial z}{\\partial \\phi}$ 是变换 $g$ 关于 $\\phi$ 的雅可比矩阵，大小为 $d \\times P$。让我们考虑单个参数 $\\phi_k$。\n$$ \\frac{\\partial}{\\partial \\phi_k} \\ln p_{\\theta}(x \\mid z) = \\sum_{i=1}^d \\frac{\\partial \\ln p_{\\theta}(x \\mid z)}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\phi_k} $$\n$z_i$ 关于 $\\phi_k$ 的导数是：\n$$ \\frac{\\partial z_i}{\\partial \\phi_k} = \\frac{\\partial \\mu_{\\phi,i}(x)}{\\partial \\phi_k} + \\epsilon_i \\frac{\\partial \\sigma_{\\phi,i}(x)}{\\partial \\phi_k} $$\n将此代入 $J(\\phi)$ 关于 $\\phi_k$ 的梯度表达式中：\n$$ \\frac{\\partial J(\\phi)}{\\partial \\phi_k} = \\mathbb{E}_{\\epsilon}\\!\\left[ \\sum_{i=1}^d \\frac{\\partial \\ln p_{\\theta}(x \\mid z)}{\\partial z_i} \\left( \\frac{\\partial \\mu_{\\phi,i}(x)}{\\partial \\phi_k} + \\epsilon_i \\frac{\\partial \\sigma_{\\phi,i}(x)}{\\partial \\phi_k} \\right) \\right] $$\n这个表达式可以使用雅可比矩阵写成向量形式。设 $J_{\\mu}(\\phi)$ 和 $J_{\\sigma}(\\phi)$ 分别是 $\\mu_{\\phi}(x)$ 和 $\\sigma_{\\phi}(x)$ 关于 $\\phi$ 的雅可比矩阵。它们是 $d \\times P$ 的矩阵，其中 $(J_{\\mu}(\\phi))_{ik} = \\frac{\\partial \\mu_{\\phi,i}(x)}{\\partial \\phi_k}$。完整的梯度向量 $\\nabla_{\\phi} J(\\phi)$ 是：\n$$ \\nabla_{\\phi} J(\\phi) = \\mathbb{E}_{\\epsilon}\\!\\left[ J_{\\mu}(\\phi)^T \\nabla_z \\ln p_{\\theta}(x \\mid z) + J_{\\sigma}(\\phi)^T \\left(\\epsilon \\odot \\nabla_z \\ln p_{\\theta}(x \\mid z)\\right) \\right] $$\n其中 $z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon$。这种形式明确了对数似然关于潜变量 $z$ 的梯度与均值和标准差函数关于变分参数 $\\phi$ 的雅可比矩阵之间的耦合关系。\n\n如果我们使用对数方差参数化 $\\sigma_{\\phi,i}^2(x) = \\exp(\\ell_{\\phi,i}(x))$，那么 $\\sigma_{\\phi,i}(x) = \\exp(\\frac{1}{2}\\ell_{\\phi,i}(x))$。导数变为：\n$$ \\frac{\\partial \\sigma_{\\phi,i}(x)}{\\partial \\phi_k} = \\exp\\left(\\frac{1}{2}\\ell_{\\phi,i}(x)\\right) \\cdot \\frac{1}{2} \\frac{\\partial \\ell_{\\phi,i}(x)}{\\partial \\phi_k} = \\frac{1}{2} \\sigma_{\\phi,i}(x) \\frac{\\partial \\ell_{\\phi,i}(x)}{\\partial \\phi_k} $$\n梯度表达式则变为对数方差的雅可比矩阵 $J_{\\ell}(\\phi)$ 的函数。\n\n**任务2：KL 正则化项的闭式形式**\n\n第二个任务是计算正则化项 $\\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)$ 的闭式表达式。这两个分布是：\n-   $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu, \\Sigma)$，其中 $\\mu = \\mu_{\\phi}(x)$ 且 $\\Sigma = \\operatorname{diag}(\\sigma_{\\phi,1}^2(x), \\dots, \\sigma_{\\phi,d}^2(x))$。\n-   $p(z) = \\mathcal{N}(z; 0, I_d)$。\n\nKL 散度定义为 $\\mathrm{KL}(q\\|p) = \\int q(z) \\ln \\frac{q(z)}{p(z)} dz = \\mathbb{E}_{z \\sim q}[\\ln q(z) - \\ln p(z)]$。\n一个 $d$ 维高斯分布 $\\mathcal{N}(z; \\mu_0, \\Sigma_0)$ 的对数概率密度函数是：\n$$ \\ln \\mathcal{N}(z; \\mu_0, \\Sigma_0) = -\\frac{1}{2} (z - \\mu_0)^T \\Sigma_0^{-1} (z - \\mu_0) - \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(\\Sigma_0)) $$\n我们需要计算 $\\mathbb{E}_{z \\sim q}[\\ln q(z)]$ 和 $\\mathbb{E}_{z \\sim q}[\\ln p(z)]$。\n\n首先，对于 $\\mathbb{E}_{z \\sim q}[\\ln q(z)]$（$q$ 的负熵）：\n$$ \\mathbb{E}_{z \\sim q}[\\ln q(z)] = \\mathbb{E}_{z \\sim q}\\left[ -\\frac{1}{2} (z - \\mu)^T \\Sigma^{-1} (z - \\mu) - \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(\\Sigma)) \\right] $$\n二次项的期望是：\n$$ \\mathbb{E}_{z \\sim q}\\left[ (z - \\mu)^T \\Sigma^{-1} (z - \\mu) \\right] = \\mathbb{E}_{z \\sim q}\\left[ \\operatorname{tr}\\left((z - \\mu)^T \\Sigma^{-1} (z - \\mu)\\right) \\right] = \\mathbb{E}_{z \\sim q}\\left[ \\operatorname{tr}\\left(\\Sigma^{-1} (z - \\mu)(z - \\mu)^T\\right) \\right] $$\n$$ = \\operatorname{tr}\\left( \\Sigma^{-1} \\mathbb{E}_{z \\sim q}\\left[(z - \\mu)(z - \\mu)^T\\right] \\right) = \\operatorname{tr}(\\Sigma^{-1} \\Sigma) = \\operatorname{tr}(I_d) = d $$\n所以，$\\mathbb{E}_{z \\sim q}[\\ln q(z)] = -\\frac{d}{2} - \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(\\Sigma))$。\n\n其次，对于 $\\mathbb{E}_{z \\sim q}[\\ln p(z)]$（负交叉熵）：\n$$ \\mathbb{E}_{z \\sim q}[\\ln p(z)] = \\mathbb{E}_{z \\sim q}\\left[ -\\frac{1}{2} z^T I_d^{-1} z - \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(I_d)) \\right] = -\\frac{1}{2} \\mathbb{E}_{z \\sim q}[z^T z] - \\frac{d}{2} \\ln(2\\pi) $$\n期望是 $\\mathbb{E}_{z \\sim q}[z^T z] = \\mathbb{E}_{z \\sim q}[ \\operatorname{tr}(zz^T)] = \\operatorname{tr}(\\mathbb{E}_{z \\sim q}[zz^T])$。因为 $\\Sigma = \\mathbb{E}[(z-\\mu)(z-\\mu)^T] = \\mathbb{E}[zz^T] - \\mu\\mu^T$，我们有 $\\mathbb{E}[zz^T] = \\Sigma + \\mu\\mu^T$。\n$$ \\mathbb{E}_{z \\sim q}[z^T z] = \\operatorname{tr}(\\Sigma + \\mu\\mu^T) = \\operatorname{tr}(\\Sigma) + \\operatorname{tr}(\\mu\\mu^T) = \\operatorname{tr}(\\Sigma) + \\mu^T \\mu $$\n所以，$\\mathbb{E}_{z \\sim q}[\\ln p(z)] = -\\frac{1}{2}(\\operatorname{tr}(\\Sigma) + \\mu^T\\mu) - \\frac{d}{2}\\ln(2\\pi)$。\n\n综合这些结果：\n$$ \\mathrm{KL}(q\\|p) = \\mathbb{E}_{z \\sim q}[\\ln q(z)] - \\mathbb{E}_{z \\sim q}[\\ln p(z)] $$\n$$ = \\left(-\\frac{d}{2} - \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(\\Sigma))\\right) - \\left(-\\frac{1}{2}(\\operatorname{tr}(\\Sigma) + \\mu^T\\mu) - \\frac{d}{2}\\ln(2\\pi)\\right) $$\n$$ = \\frac{1}{2} \\left[ \\operatorname{tr}(\\Sigma) + \\mu^T\\mu - d - \\ln(\\det(\\Sigma)) \\right] $$\n现在，我们代入 $\\mu$ 和 $\\Sigma$ 的具体形式。设 $\\mu_i = \\mu_{\\phi,i}(x)$ 且 $\\sigma_i = \\sigma_{\\phi,i}(x)$。\n- $\\mu^T\\mu = \\sum_{i=1}^d \\mu_i^2$。\n- $\\Sigma = \\operatorname{diag}(\\sigma_1^2, \\dots, \\sigma_d^2)$，所以 $\\operatorname{tr}(\\Sigma) = \\sum_{i=1}^d \\sigma_i^2$。\n- $\\det(\\Sigma) = \\prod_{i=1}^d \\sigma_i^2$，所以 $\\ln(\\det(\\Sigma)) = \\sum_{i=1}^d \\ln(\\sigma_i^2)$。\n\n将这些代入 KL 散度公式中：\n$$ \\mathrm{KL}(q\\|p) = \\frac{1}{2} \\left[ \\sum_{i=1}^d \\sigma_i^2 + \\sum_{i=1}^d \\mu_i^2 - d - \\sum_{i=1}^d \\ln(\\sigma_i^2) \\right] $$\n这可以通过对求和内的项进行分组来重写：\n$$ \\mathrm{KL}(q\\|p) = \\frac{1}{2} \\sum_{i=1}^d \\left( \\mu_i^2 + \\sigma_i^2 - \\ln(\\sigma_i^2) - 1 \\right) $$\n用 $\\mu_i$ 和 $\\sigma_i$ 的完整表示法替换它们，我们得到最终表达式。这是对角高斯分布与标准正态分布之间 KL 散度的闭式解。该项作为一个正则化项，促使近似后验分布接近先验分布。",
            "answer": "$$\n\\boxed{\\frac{1}{2} \\sum_{i=1}^{d} \\left( \\mu_{\\phi,i}^{2}(x) + \\sigma_{\\phi,i}^{2}(x) - \\ln\\left(\\sigma_{\\phi,i}^{2}(x)\\right) - 1 \\right)}\n$$"
        },
        {
            "introduction": "除了深度生成模型，变分推断也为经典贝叶斯模型提供了一个强大的框架，尤其是在坐标上升优化方法中。本练习专注于推导贝叶斯高斯混合模型的变分贝叶斯期望最大化（VBEM）算法，这是一种在神经科学数据中识别潜在群体结构的常用工具。通过完成E步和M步的更新推导，您将深入理解共轭模型中平均场变分推断的运作机制。",
            "id": "4203212",
            "problem": "您正在分析一项行为任务中，$D$ 个胞外记录神经元在短时间窗内的瞬时发放率向量。令 $\\mathbf{x}_{n} \\in \\mathbb{R}^{D}$ 表示在时间窗 $n$ 上的发放率向量，其中 $n = 1,\\dots,N$。假设该神经元群体在不同任务条件下表现出潜在的多模态性，并使用一个由 $K$ 个多元高斯分量组成的有限混合模型来对 $\\mathbf{x}_{n}$ 的分布进行建模。该生成模型由以下经过充分检验的概率结构指定：\n- 混合权重 $\\boldsymbol{\\pi} = (\\pi_{1},\\dots,\\pi_{K})$ 满足 $\\sum_{k=1}^{K} \\pi_{k} = 1$，并服从超参数为 $\\boldsymbol{\\alpha}_{0} = (\\alpha_{0,1},\\dots,\\alpha_{0,K})$ 的狄利克雷先验分布。\n- 对于每个分量 $k \\in \\{1,\\dots,K\\}$，精度矩阵 $\\boldsymbol{\\Lambda}_{k}$ 服从自由度为 $\\nu_{0} > D - 1$、正定尺度矩阵为 $\\mathbf{W}_{0}$ 的威沙特先验分布。均值 $\\boldsymbol{\\mu}_{k}$ 在给定 $\\boldsymbol{\\Lambda}_{k}$ 的条件下，服从均值为 $\\mathbf{m}_{0}$、经精度缩放的协方差为 $(\\beta_{0} \\boldsymbol{\\Lambda}_{k})^{-1}$ 的多元正态先验分布。$(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$ 的这种联合先验是共轭正态-威沙特族。\n- 对于观测值 $n$，其潜分配 $z_{n} \\in \\{1,\\dots,K\\}$ 服从分类分布 $\\mathrm{Cat}(\\boldsymbol{\\pi})$。在给定 $z_{n} = k$ 和分量参数 $(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$ 的条件下，发放率向量 $\\mathbf{x}_{n}$ 服从分布 $\\mathcal{N}(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k}^{-1})$。\n\n从贝叶斯推断和证据下界 (ELBO) 的核心定义出发，构建一个平均场变分族 $q(\\boldsymbol{\\pi}) \\prod_{k=1}^{K} q(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k}) \\prod_{n=1}^{N} q(z_{n})$，并为此模型推导坐标上升变分贝叶斯期望最大化 (VBEM) 算法。您的推导必须从第一性原理（贝叶斯法则、共轭性以及期望完全数据对数似然）出发，不得引用任何快捷公式。\n\n具体而言：\n1. 在当前变分因子 $q(\\boldsymbol{\\pi})$ 和 $q(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$ 下，将 E 步中的变分责任 $r_{nk} = q(z_{n} = k)$ 推导为期望完全数据对数似然的归一化指数形式。用变分分布下 $\\ln \\pi_{k}$、$\\ln |\\boldsymbol{\\Lambda}_{k}|$ 以及二次型 $(\\mathbf{x}_{n} - \\boldsymbol{\\mu}_{k})^{\\top} \\boldsymbol{\\Lambda}_{k} (\\mathbf{x}_{n} - \\boldsymbol{\\mu}_{k})$ 的期望来表示 $r_{nk}$。\n2. 通过对每个因子最大化 ELBO，推导 M 步中 $q(\\boldsymbol{\\pi})$ 和 $q(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$ 的变分参数的更新（坐标上升更新）。证明最优的 $q(\\boldsymbol{\\pi})$ 仍然是狄利克雷分布，最优的 $q(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$ 仍然是正态-威沙特分布，并用责任 $r_{nk}$ 和数据 $\\{\\mathbf{x}_{n}\\}_{n=1}^{N}$ 来表示更新后的参数。\n\n最后，为了得出一个具体的、可检验的解析量，将模型特化到 $K = 2$ 个混合分量和任意维度 $D \\geq 1$ 的情况。设 $q(\\boldsymbol{\\pi})$ 的当前变分参数为 $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\alpha_{2})$，并且对于每个 $k \\in \\{1,2\\}$，设 $q(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$ 的参数为正态-威沙特族中的 $(\\mathbf{m}_{k}, \\beta_{k}, \\mathbf{W}_{k}, \\nu_{k})$。仅使用与这些分布族和您的推导一致的基本期望，写出对于单个观测值 $\\mathbf{x}_{n}$ 的 E 步责任 $r_{n1}$ 关于 $(\\alpha_{1}, \\alpha_{2})$、$(\\mathbf{m}_{1}, \\beta_{1}, \\mathbf{W}_{1}, \\nu_{1})$、$(\\mathbf{m}_{2}, \\beta_{2}, \\mathbf{W}_{2}, \\nu_{2})$ 和 $\\mathbf{x}_{n}$ 的闭式解析表达式。您的最终答案必须是一个单一的解析表达式。不包含任何单位。无需进行数值计算。",
            "solution": "用户提供的问题陈述是有效的。这是一个在计算神经科学和机器学习领域内良定的、有科学依据的客观任务。该问题描述了一个标准的贝叶斯高斯混合模型 (GMM)，并要求推导变分贝叶斯期望最大化 (VBEM) 算法，并随后给出一个特定的解析结果。模型和变分族的所有必要组成部分都已提供，不存在矛盾或歧义。因此，我们可以进行完整的推导。\n\n令所有变量的完整集合为 $\\boldsymbol{\\Psi} = \\{\\mathbf{Z}, \\boldsymbol{\\pi}, \\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\}_{k=1}^K\\}$，其中 $\\mathbf{Z} = \\{z_n\\}_{n=1}^N$ 是潜分配，$\\{\\boldsymbol{\\pi}, \\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\}\\}$ 是模型参数。观测数据为 $\\mathbf{X} = \\{\\mathbf{x}_n\\}_{n=1}^N$。所有变量的联合概率分布是 $p(\\mathbf{X}, \\boldsymbol{\\Psi})$。\n\n变分贝叶斯旨在用一个更简单的、可分解的分布 $q(\\boldsymbol{\\Psi})$ 来近似真实的后验分布 $p(\\boldsymbol{\\Psi}|\\mathbf{X})$。指定使用的平均场变分族为：\n$$q(\\boldsymbol{\\Psi}) = q(\\mathbf{Z}) q(\\boldsymbol{\\pi}) q(\\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\}) = \\left( \\prod_{n=1}^N q(z_n) \\right) q(\\boldsymbol{\\pi}) \\left( \\prod_{k=1}^K q(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) \\right)$$\n目标是最小化 KL 散度 (Kullback-Leibler divergence) $\\mathrm{KL}(q(\\boldsymbol{\\Psi}) || p(\\boldsymbol{\\Psi}|\\mathbf{X}))$，这等价于最大化证据下界 (ELBO) $\\mathcal{L}(q)$：\n$$\\mathcal{L}(q) = \\mathbb{E}_{q(\\boldsymbol{\\Psi})} \\left[ \\ln \\frac{p(\\mathbf{X}, \\boldsymbol{\\Psi})}{q(\\boldsymbol{\\Psi})} \\right]$$\n坐标上升算法通过保持其他因子固定，迭代地优化 $q(\\boldsymbol{\\Psi})$ 的每个因子 $q_j$。因子 $q_j(\\boldsymbol{\\psi}_j)$ 的一般更新公式由下式给出：\n$$\\ln q_j^*(\\boldsymbol{\\psi}_j) = \\mathbb{E}_{q_{\\neg j}}[\\ln p(\\mathbf{X}, \\boldsymbol{\\Psi})] + \\mathrm{const}$$\n其中 $\\mathbb{E}_{q_{\\neg j}}$ 表示关于 $q(\\boldsymbol{\\Psi})$ 中除 $q_j$ 之外的所有因子的期望。\n\n完全数据联合概率 $p(\\mathbf{X}, \\boldsymbol{\\Psi})$ 的对数是：\n\\begin{align*}\n\\ln p(\\mathbf{X}, \\mathbf{Z}, \\boldsymbol{\\pi}, \\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\}) =  \\ln p(\\boldsymbol{\\pi}|\\boldsymbol{\\alpha}_0) + \\sum_{k=1}^K \\ln p(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k|\\mathbf{m}_0, \\beta_0, \\mathbf{W}_0, \\nu_0) \\\\\n + \\sum_{n=1}^N \\ln p(z_n|\\boldsymbol{\\pi}) + \\sum_{n=1}^N \\ln p(\\mathbf{x}_n|z_n, \\boldsymbol{\\mu}_{z_n}, \\boldsymbol{\\Lambda}_{z_n})\n\\end{align*}\n引入指示变量 $z_{nk}$，当 $z_n=k$ 时 $z_{nk}=1$，否则为 $0$。我们可以写出：\n\\begin{align*}\n\\ln p(\\mathbf{X}, \\mathbf{Z}, \\dots) =  \\ln p(\\boldsymbol{\\pi}) + \\sum_{k=1}^K \\ln p(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) + \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\ln \\pi_k \\\\\n + \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\left( \\frac{1}{2} \\ln |\\boldsymbol{\\Lambda}_k| - \\frac{D}{2} \\ln(2\\pi) - \\frac{1}{2}(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Lambda}_k (\\mathbf{x}_n - \\boldsymbol{\\mu}_k) \\right)\n\\end{align*}\n\n**1. E 步：责任 $r_{nk}$ 的推导**\n\n我们针对单个观测值 $n$ 优化因子 $q(z_n)$。根据更新法则可得：\n$$\\ln q^*(z_n) = \\mathbb{E}_{q(\\boldsymbol{\\pi})q(\\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\})} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z}, \\dots) \\right] + \\mathrm{const}$$\n我们只需要依赖于 $z_n$ 的项。令 $q(z_n=k) = r_{nk}$。\n$$\\ln q^*(z_n) = \\sum_{k=1}^K z_{nk} \\left( \\mathbb{E}[\\ln \\pi_k] + \\mathbb{E}\\left[\\frac{1}{2}\\ln|\\boldsymbol{\\Lambda}_k| - \\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^\\top\\boldsymbol{\\Lambda}_k(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)\\right] \\right) + \\mathrm{const}$$\n其中不依赖于 $k$ 的常数项已被吸收到常数项中。这表明 $q^*(z_n)$ 是一个分类分布。\n将观测值 $n$ 分配给分量 $k$ 的概率，即责任 $r_{nk}$，为：\n$$r_{nk} = q^*(z_n=k) = \\frac{\\exp(\\ln \\rho_{nk})}{\\sum_{j=1}^K \\exp(\\ln \\rho_{nj})}$$\n其中 $\\ln \\rho_{nk}$ 是上面括号中的项：\n$$\\ln \\rho_{nk} = \\mathbb{E}[\\ln \\pi_k] + \\frac{1}{2}\\mathbb{E}[\\ln|\\boldsymbol{\\Lambda}_k|] - \\frac{1}{2}\\mathbb{E}[(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^\\top\\boldsymbol{\\Lambda}_k(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)]$$\n该表达式根据题目要求，用参数的当前变分分布下的期望定义了变分责任 $r_{nk}$。\n\n**2. M 步：参数更新的推导**\n\n**对 $q(\\boldsymbol{\\pi})$ 的更新**：\n我们优化 $q(\\boldsymbol{\\pi})$，并保持其他因子固定。\n$$\\ln q^*(\\boldsymbol{\\pi}) = \\mathbb{E}_{q(\\mathbf{Z})q(\\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\})} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z}, \\dots) \\right] + \\mathrm{const}$$\n相关项是 $\\boldsymbol{\\pi}$ 的先验和分配 $\\mathbf{Z}$ 的似然：\n$$\\ln q^*(\\boldsymbol{\\pi}) = \\ln p(\\boldsymbol{\\pi}|\\boldsymbol{\\alpha}_0) + \\mathbb{E}_{q(\\mathbf{Z})} \\left[\\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\ln \\pi_k\\right] + \\mathrm{const}$$\n先验分布为 $p(\\boldsymbol{\\pi}|\\boldsymbol{\\alpha}_0) \\propto \\prod_{k=1}^K \\pi_k^{\\alpha_{0,k}-1}$。期望为 $\\mathbb{E}[z_{nk}] = r_{nk}$。\n$$\\ln q^*(\\boldsymbol{\\pi}) = \\sum_{k=1}^K (\\alpha_{0,k}-1)\\ln\\pi_k + \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} \\ln\\pi_k + \\mathrm{const} = \\sum_{k=1}^K (\\alpha_{0,k} + \\sum_{n=1}^N r_{nk} - 1)\\ln\\pi_k + \\mathrm{const}$$\n这是狄利克雷分布的对数核，因此 $q^*(\\boldsymbol{\\pi}) = \\mathrm{Dir}(\\boldsymbol{\\pi}|\\boldsymbol{\\alpha})$，其更新后的参数为：\n$$\\alpha_k = \\alpha_{0,k} + N_k, \\quad \\text{其中} \\quad N_k = \\sum_{n=1}^N r_{nk}$$\n\n**对 $q(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k)$ 的更新**：\n每个分量 $k$ 的因子是独立的，因此我们逐个更新它们。\n$$\\ln q^*(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) = \\mathbb{E}_{q(\\mathbf{Z})q(\\boldsymbol{\\pi})q_{\\neg k}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z}, \\dots) \\right] + \\mathrm{const}$$\n相关项是 $(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k)$ 的先验和分量 $k$ 的数据似然：\n$$\\ln q^*(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) = \\ln p(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) + \\sum_{n=1}^N \\mathbb{E}[z_{nk}] \\ln \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k^{-1}) + \\mathrm{const}$$\n使用 $p(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) = p(\\boldsymbol{\\mu}_k|\\boldsymbol{\\Lambda}_k)p(\\boldsymbol{\\Lambda}_k)$，对数先验为：\n$$\\ln p(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) = \\frac{\\nu_0 - D - 1}{2}\\ln|\\boldsymbol{\\Lambda}_k| - \\frac{1}{2}\\mathrm{tr}(\\mathbf{W}_0^{-1}\\boldsymbol{\\Lambda}_k) + \\frac{1}{2}\\ln|\\boldsymbol{\\Lambda}_k| + \\frac{D}{2}\\ln\\beta_0 - \\frac{\\beta_0}{2}(\\boldsymbol{\\mu}_k-\\mathbf{m}_0)^\\top\\boldsymbol{\\Lambda}_k(\\boldsymbol{\\mu}_k-\\mathbf{m}_0) + \\dots$$\n对数似然部分是：\n$$\\sum_{n=1}^N r_{nk} \\left( \\frac{1}{2}\\ln|\\boldsymbol{\\Lambda}_k| - \\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^\\top\\boldsymbol{\\Lambda}_k(\\mathbf{x}_n-\\boldsymbol{\\mu}_k) \\right) + \\dots$$\n将所有涉及 $\\boldsymbol{\\mu}_k$ 和 $\\boldsymbol{\\Lambda}_k$ 的项合并，并按这些变量分组，我们可以识别出更新后的正态-威沙特后验分布的形式。这涉及到对 $\\boldsymbol{\\mu}_k$ 中的项进行配方，并收集 $\\boldsymbol{\\Lambda}_k$ 的项。计算表明 $q^*(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k)$ 是一个正态-威沙特分布 $\\mathcal{NW}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k | \\mathbf{m}_k, \\beta_k, \\mathbf{W}_k, \\nu_k)$，其更新后的参数如下：\n$N_k = \\sum_{n=1}^N r_{nk}$\n$\\overline{\\mathbf{x}}_k = \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} \\mathbf{x}_n$\n$\\mathbf{S}_k = \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} (\\mathbf{x}_n - \\overline{\\mathbf{x}}_k)(\\mathbf{x}_n - \\overline{\\mathbf{x}}_k)^\\top$\n\n更新后的参数为：\n$\\beta_k = \\beta_0 + N_k$\n$\\mathbf{m}_k = \\frac{\\beta_0 \\mathbf{m}_0 + N_k \\overline{\\mathbf{x}}_k}{\\beta_k}$\n$\\nu_k = \\nu_0 + N_k$\n$\\mathbf{W}_k^{-1} = \\mathbf{W}_0^{-1} + N_k \\mathbf{S}_k + \\frac{\\beta_0 N_k}{\\beta_0 + N_k}(\\overline{\\mathbf{x}}_k - \\mathbf{m}_0)(\\overline{\\mathbf{x}}_k - \\mathbf{m}_0)^\\top$\n\n**$K=2$ 时 $r_{n1}$ 的最终表达式**\n\n对于 $K=2$ 的情况，责任 $r_{n1}$ 为 $r_{n1} = (1 + \\exp(\\ln\\rho_{n2} - \\ln\\rho_{n1}))^{-1}$。我们必须使用给定的变分参数 $(\\alpha_1, \\alpha_2)$ 和 $(\\mathbf{m}_k, \\beta_k, \\mathbf{W}_k, \\nu_k)$（其中 $k \\in \\{1,2\\}$）来计算 $\\ln\\rho_{nk}$ 中的项。我们需要以下标准期望：\n1.  对于 $q(\\boldsymbol{\\pi}) = \\mathrm{Dir}(\\boldsymbol{\\pi}|\\alpha_1, \\alpha_2)$：$\\mathbb{E}[\\ln \\pi_k] = \\psi(\\alpha_k) - \\psi(\\alpha_1+\\alpha_2)$，其中 $\\psi(\\cdot)$ 是 digamma 函数。\n2.  对于 $q(\\boldsymbol{\\Lambda}_k) = \\mathcal{W}(\\boldsymbol{\\Lambda}_k|\\mathbf{W}_k, \\nu_k)$：$\\mathbb{E}[\\ln|\\boldsymbol{\\Lambda}_k|] = \\sum_{i=1}^D \\psi\\left(\\frac{\\nu_k+1-i}{2}\\right) + D \\ln 2 + \\ln|\\mathbf{W}_k|$。\n3.  对于 $q(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) = \\mathcal{NW}(\\cdot|\\mathbf{m}_k, \\beta_k, \\mathbf{W}_k, \\nu_k)$：\n    二次型的期望是 $\\mathbb{E}[(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Lambda}_k (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)] = \\mathbb{E}[\\mathrm{tr}((\\mathbf{x}_n - \\boldsymbol{\\mu}_k)(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Lambda}_k)]$。\n    内部的期望是 $\\mathbb{E}_{\\boldsymbol{\\mu}_k|\\boldsymbol{\\Lambda}_k}[(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top] = \\mathrm{Cov}(\\boldsymbol{\\mu}_k|\\boldsymbol{\\Lambda}_k) + (\\mathbf{x}_n - \\mathbb{E}[\\boldsymbol{\\mu}_k|\\boldsymbol{\\Lambda}_k])(\\mathbf{x}_n - \\mathbb{E}[\\boldsymbol{\\mu}_k|\\boldsymbol{\\Lambda}_k])^\\top = (\\beta_k \\boldsymbol{\\Lambda}_k)^{-1} + (\\mathbf{x}_n - \\mathbf{m}_k)(\\mathbf{x}_n - \\mathbf{m}_k)^\\top$。\n    对 $\\boldsymbol{\\Lambda}_k$ 求期望，然后求迹，得到：\n    $\\mathbb{E}[(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Lambda}_k (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)] = \\nu_k (\\mathbf{x}_n - \\mathbf{m}_k)^\\top \\mathbf{W}_k (\\mathbf{x}_n - \\mathbf{m}_k) + \\frac{D}{\\beta_k}$。\n\n将这些代入 $\\ln \\rho_{nk}$ 的表达式中，并构成差值 $\\ln\\rho_{n2} - \\ln\\rho_{n1}$，所有关于 $k$ 的常数项（如 $\\psi(\\alpha_1+\\alpha_2)$、$D\\ln 2$、$D\\ln(2\\pi)$）都会消去。\n那么 $r_{n1}$ 的最终表达式就由这些部分构成。\n\n$\\ln\\rho_{n2} - \\ln\\rho_{n1} = \\left(\\psi(\\alpha_2) - \\psi(\\alpha_1)\\right) + \\frac{1}{2}\\left(\\mathbb{E}[\\ln|\\boldsymbol{\\Lambda}_2|] - \\mathbb{E}[\\ln|\\boldsymbol{\\Lambda}_1|]\\right) - \\frac{1}{2}\\left(\\mathbb{E}[q_2] - \\mathbb{E}[q_1]\\right)$\n其中 $q_k = (\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^\\top\\boldsymbol{\\Lambda}_k(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)$。\n\n代入期望的完整表达式：\n$\\ln\\rho_{n2} - \\ln\\rho_{n1} = \\left(\\psi(\\alpha_2) - \\psi(\\alpha_1)\\right) + \\frac{1}{2}\\left( \\ln|\\mathbf{W}_2| - \\ln|\\mathbf{W}_1| + \\sum_{i=1}^D \\left(\\psi\\left(\\frac{\\nu_2+1-i}{2}\\right) - \\psi\\left(\\frac{\\nu_1+1-i}{2}\\right)\\right) \\right) - \\frac{1}{2}\\left( \\left(\\nu_2(\\mathbf{x}_n-\\mathbf{m}_2)^\\top\\mathbf{W}_2(\\mathbf{x}_n-\\mathbf{m}_2)+\\frac{D}{\\beta_2}\\right) - \\left(\\nu_1(\\mathbf{x}_n-\\mathbf{m}_1)^\\top\\mathbf{W}_1(\\mathbf{x}_n-\\mathbf{m}_1)+\\frac{D}{\\beta_1}\\right) \\right)$。\n\n这就给出了 $r_{n1}$ 的最终表达式。",
            "answer": "$$\n\\boxed{\n\\left( 1 + \\exp\\left( \\psi(\\alpha_2) - \\psi(\\alpha_1) + \\frac{1}{2}\\left(\\ln|\\mathbf{W}_2| - \\ln|\\mathbf{W}_1| + \\sum_{i=1}^{D}\\left(\\psi\\left(\\frac{\\nu_2+1-i}{2}\\right) - \\psi\\left(\\frac{\\nu_1+1-i}{2}\\right)\\right) - \\frac{D}{\\beta_2} + \\frac{D}{\\beta_1} - \\nu_2(\\mathbf{x}_n-\\mathbf{m}_2)^{\\top}\\mathbf{W}_2(\\mathbf{x}_n-\\mathbf{m}_2) + \\nu_1(\\mathbf{x}_n-\\mathbf{m}_1)^{\\top}\\mathbf{W}_1(\\mathbf{x}_n-\\mathbf{m}_1)\\right) \\right) \\right)^{-1}\n}\n$$"
        },
        {
            "introduction": "变分推断的用途不仅限于参数估计，更延伸到模型选择这一关键科学任务。证据下界（ELBO）可以作为模型边缘似然（即证据）的一个可计算的代理。本练习要求您推导ELBO与贝叶斯因子之间的关系，并利用它来比较分析神经脉冲计数数据的竞争模型，从而展示变分推断在量化假设检验中的强大应用。",
            "id": "4203133",
            "problem": "您正在分析在 $T$ 个时间区间内记录的单个神经元的细胞外尖峰计数数据。该数据由一个共享的协变量设计矩阵 $X \\in \\mathbb{R}^{T \\times p}$ 驱动，并使用两个相互竞争的生成模型对计数 $y_{1:T}$ 进行建模：\n\n- 模型 $\\mathcal{M}_{\\mathrm{P}}$：一个泊松广义线性模型 (GLM)，具有标准对数连接函数、潜系数 $\\beta \\in \\mathbb{R}^{p}$ 以及绝对连续且正常的先验 $p(\\beta \\mid \\mathcal{M}_{\\mathrm{P}})$。\n- 模型 $\\mathcal{M}_{\\mathrm{NB}}$：一个负二项广义线性模型 (GLM)，具有标准对数连接函数、潜系数 $\\beta \\in \\mathbb{R}^{p}$ 和逆离散度 $\\kappa > 0$，以及绝对连续且正常的先验 $p(\\beta, \\kappa \\mid \\mathcal{M}_{\\mathrm{NB}})$。\n\n假设以下基本前提：\n- 在任何模型 $\\mathcal{M}$ 下，边际似然（模型证据）为 $p(y_{1:T} \\mid \\mathcal{M}) = \\int p(y_{1:T}, \\theta \\mid \\mathcal{M}) \\,\\mathrm{d}\\theta$，其中 $\\theta$ 表示 $\\mathcal{M}$ 下的所有潜变量和参数。\n- 对于任何在 $p(y_{1:T}, \\theta \\mid \\mathcal{M}) > 0$ 处具有全支撑的变分分布 $q(\\theta)$，根据 Jensen 不等式，证据下界 (ELBO) $\\mathcal{L}(q, \\mathcal{M}) = \\mathbb{E}_{q}[\\ln p(y_{1:T}, \\theta \\mid \\mathcal{M}) - \\ln q(\\theta)]$ 满足 $\\ln p(y_{1:T} \\mid \\mathcal{M}) \\ge \\mathcal{L}(q, \\mathcal{M})$。\n- Kullback–Leibler (KL) 散度满足 $\\mathrm{KL}\\!\\left(q(\\theta) \\,\\|\\, p(\\theta \\mid y_{1:T}, \\mathcal{M})\\right) \\ge 0$ 以及恒等式 $\\ln p(y_{1:T} \\mid \\mathcal{M}) = \\mathcal{L}(q, \\mathcal{M}) + \\mathrm{KL}\\!\\left(q(\\theta) \\,\\|\\, p(\\theta \\mid y_{1:T}, \\mathcal{M})\\right)$。\n\n任务：\n1. 从上述基本前提开始，推导贝叶斯因子 $\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = \\dfrac{p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{P}})}{p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{NB}})}$ 的一个变分近似，该近似应使用分别在 $\\mathcal{M}_{\\mathrm{P}}$ 和 $\\mathcal{M}_{\\mathrm{NB}}$ 下通过变分推断获得的优化 ELBO $\\mathcal{L}^{\\star}_{\\mathrm{P}}$ 和 $\\mathcal{L}^{\\star}_{\\mathrm{NB}}$ 来表示。您的推导必须从边际似然、ELBO 和 KL 散度的定义开始，并得出一个仅用 $\\mathcal{L}^{\\star}_{\\mathrm{P}}$ 和 $\\mathcal{L}^{\\star}_{\\mathrm{NB}}$ 表示的变分贝叶斯因子代理的闭式解析表达式。\n2. 使用推导出的表达式，考虑一个真实数据集的拟合情况，其中收敛的变分推断产生的优化 ELBO 分别为 $\\mathcal{L}^{\\star}_{\\mathrm{P}} = -1250.3$ 和 $\\mathcal{L}^{\\star}_{\\mathrm{NB}} = -1253.8$。计算 $\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}}$ 的变分近似值，结果为一个无单位标量。将您的答案四舍五入到四位有效数字。\n3. 用文字描述（无需计算），讨论在何种充分条件下，ELBO 差异代理是真实贝叶斯因子偏好的可靠指标。请将您的推理与上述 KL 恒等式以及神经尖峰计数模型中可能出现的统计特性明确联系起来。\n\n将第 2 部分中最终的贝叶斯因子近似值表示为一个无单位的单一数字。四舍五入到四位有效数字。",
            "solution": "该问题要求进行一个三部分的分析，涉及使用变分推断比较两种用于神经尖峰计数数据的统计模型。这三部分是：推导贝叶斯因子的变分近似，计算该近似值，以及定性讨论该近似的可靠性。\n\n### 第 1 部分：变分贝叶斯因子近似的推导\n\n目标是利用变分推断得到的优化证据下界 (ELBOs) 来推导贝叶斯因子 $\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}}$ 的一个近似。用于比较模型 $\\mathcal{M}_{\\mathrm{P}}$ 和模型 $\\mathcal{M}_{\\mathrm{NB}}$ 的贝叶斯因子定义为其边际似然（模型证据）的比值：\n$$\n\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = \\frac{p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{P}})}{p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{NB}})}\n$$\n为了处理一个更方便的量，我们取贝叶斯因子的自然对数：\n$$\n\\ln \\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = \\ln p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{P}}) - \\ln p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{NB}})\n$$\n问题陈述提供了连接对数边际似然、ELBO 以及变分近似 $q(\\theta)$ 与真实后验 $p(\\theta \\mid y_{1:T}, \\mathcal{M})$ 之间 Kullback-Leibler (KL) 散度的基本恒等式。对于任何模型 $\\mathcal{M}$，该恒等式为：\n$$\n\\ln p(y_{1:T} \\mid \\mathcal{M}) = \\mathcal{L}(q, \\mathcal{M}) + \\mathrm{KL}\\!\\left(q(\\theta) \\,\\|\\, p(\\theta \\mid y_{1:T}, \\mathcal{M})\\right)\n$$\n其中 $\\mathcal{L}(q, \\mathcal{M}) = \\mathbb{E}_{q}[\\ln p(y_{1:T}, \\theta \\mid \\mathcal{M}) - \\ln q(\\theta)]$。\n\n变分推断旨在在一个选定的分布族中找到一个最优变分分布 $q^{\\star}(\\theta)$ 来最大化 ELBO。由于对数边际似然 $\\ln p(y_{1:T} \\mid \\mathcal{M})$ 相对于 $q$ 是一个常数，因此最大化 ELBO 等价于最小化 KL 散度项。设 $q^{\\star}_{\\mathrm{P}}$ 和 $q^{\\star}_{\\mathrm{NB}}$ 分别是模型 $\\mathcal{M}_{\\mathrm{P}}$ 和 $\\mathcal{M}_{\\mathrm{NB}}$ 的优化变分分布。相应的最大化 ELBO 记为 $\\mathcal{L}^{\\star}_{\\mathrm{P}} = \\mathcal{L}(q^{\\star}_{\\mathrm{P}}, \\mathcal{M}_{\\mathrm{P}})$ 和 $\\mathcal{L}^{\\star}_{\\mathrm{NB}} = \\mathcal{L}(q^{\\star}_{\\mathrm{NB}}, \\mathcal{M}_{\\mathrm{NB}})$。\n\n我们可以使用这些优化后的量来写出每个模型的精确对数边际似然：\n对于模型 $\\mathcal{M}_{\\mathrm{P}}$（潜变量为 $\\theta_{\\mathrm{P}} = \\beta$）：\n$$\n\\ln p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{P}}) = \\mathcal{L}^{\\star}_{\\mathrm{P}} + \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{P}}(\\theta_{\\mathrm{P}}) \\,\\|\\, p(\\theta_{\\mathrm{P}} \\mid y_{1:T}, \\mathcal{M}_{\\mathrm{P}})\\right)\n$$\n对于模型 $\\mathcal{M}_{\\mathrm{NB}}$（潜变量为 $\\theta_{\\mathrm{NB}} = (\\beta, \\kappa)$）：\n$$\n\\ln p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{NB}}) = \\mathcal{L}^{\\star}_{\\mathrm{NB}} + \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{NB}}(\\theta_{\\mathrm{NB}}) \\,\\|\\, p(\\theta_{\\mathrm{NB}} \\mid y_{1:T}, \\mathcal{M}_{\\mathrm{NB}})\\right)\n$$\n将这些表达式代入对数贝叶斯因子的方程中，得到：\n$$\n\\ln \\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = \\left( \\mathcal{L}^{\\star}_{\\mathrm{P}} + \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{P}} \\,\\|\\, p_{\\mathrm{P}}\\right) \\right) - \\left( \\mathcal{L}^{\\star}_{\\mathrm{NB}} + \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{NB}} \\,\\|\\, p_{\\mathrm{NB}}\\right) \\right)\n$$\n其中 $p_{\\mathcal{M}}$ 是模型 $\\mathcal{M}$ 下真实后验的简写。重新整理各项，我们得到：\n$$\n\\ln \\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = (\\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}}) + \\left( \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{P}} \\,\\|\\, p_{\\mathrm{P}}\\right) - \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{NB}} \\,\\|\\, p_{\\mathrm{NB}}\\right) \\right)\n$$\nELBO $\\mathcal{L}(q, \\mathcal{M})$ 是对数边际证据 $\\ln p(y_{1:T} \\mid \\mathcal{M})$ 的一个下界。在变分贝叶斯模型比较中，一种常见的做法是用其最大化的下界 $\\mathcal{L}^{\\star}_{\\mathcal{M}}$ 来近似对数边际证据。这种近似等价于假设残余的 KL 散度项可以忽略不计，或者更准确地说，假设两个模型的残余 KL 项之差接近于零。\n$$\n\\ln p(y_{1:T} \\mid \\mathcal{M}) \\approx \\mathcal{L}^{\\star}_{\\mathcal{M}}\n$$\n将此近似应用于对数贝叶斯因子，我们得到变分近似：\n$$\n\\ln \\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} \\approx \\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}}\n$$\n对两边取指数，得到我们所期望的变分贝叶斯因子代理的闭式解析表达式，我们将其记为 $\\widehat{\\mathrm{BF}}_{\\mathrm{P}, \\mathrm{NB}}$：\n$$\n\\widehat{\\mathrm{BF}}_{\\mathrm{P}, \\mathrm{NB}} = \\exp(\\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}})\n$$\n\n### 第 2 部分：数值计算\n\n给定优化后的 ELBO 值 $\\mathcal{L}^{\\star}_{\\mathrm{P}} = -1250.3$ 和 $\\mathcal{L}^{\\star}_{\\mathrm{NB}} = -1253.8$，我们可以计算贝叶斯因子 $\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}}$ 的变分近似值。\n\n首先，我们计算 ELBO 的差值：\n$$\n\\Delta\\mathcal{L} = \\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}} = -1250.3 - (-1253.8) = -1250.3 + 1253.8 = 3.5\n$$\n接下来，我们将此值代入第 1 部分推导出的表达式中：\n$$\n\\widehat{\\mathrm{BF}}_{\\mathrm{P}, \\mathrm{NB}} = \\exp(\\Delta\\mathcal{L}) = \\exp(3.5)\n$$\n计算其数值：\n$$\n\\exp(3.5) \\approx 33.1154519...\n$$\n四舍五入到四位有效数字，我们得到 $33.12$。这个值远大于 $1$，表明在变分近似的假设下，数据为泊松模型 $\\mathcal{M}_{\\mathrm{P}}$ 提供了比负二项模型 $\\mathcal{M}_{\\mathrm{NB}}$ 更强的证据。\n\n### 第 3 部分：关于近似可靠性的讨论\n\nELBO 差异代理 $\\widehat{\\mathrm{BF}}_{\\mathrm{P}, \\mathrm{NB}} = \\exp(\\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}})$ 作为真实贝叶斯因子 $\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}}$ 指标的可靠性，完全取决于推导过程中被忽略的误差项的大小。根据第 1 部分，精确的对数贝叶斯因子是：\n$$\n\\ln \\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = (\\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}}) + \\left( \\mathrm{KL}^{\\star}_{\\mathrm{P}} - \\mathrm{KL}^{\\star}_{\\mathrm{NB}} \\right)\n$$\n其中 $\\mathrm{KL}^{\\star}_{\\mathcal{M}} = \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathcal{M}} \\,\\|\\, p(\\theta_{\\mathcal{M}} \\mid y_{1:T}, \\mathcal{M})\\right)$ 是模型 $\\mathcal{M}$ 的最优变分后验与真实后验之间的 KL 散度。该近似是可靠的，当且仅当这些残余 KL 散度的差值接近于零：\n$$\n\\mathrm{KL}^{\\star}_{\\mathrm{P}} - \\mathrm{KL}^{\\star}_{\\mathrm{NB}} \\approx 0 \\quad \\implies \\quad \\mathrm{KL}^{\\star}_{\\mathrm{P}} \\approx \\mathrm{KL}^{\\star}_{\\mathrm{NB}}\n$$\n该近似可靠的一个充分条件是，对于两个模型，变分分布对真实后验的拟合质量大致相等。项 $\\mathrm{KL}^{\\star}_{\\mathcal{M}}$ 代表了“近似差距”或因使用 $q^{\\star}_{\\mathcal{M}}$ 代替真实后验而丢失的信息。因此，其可靠性取决于这样一个假设：对于泊松模型和负二项模型，这个差距的大小是相似的。\n\n如果两个模型下真实后验的几何复杂度相似，并且所选的变分族（例如，平均场）对两者的近似效果同样好（或同样差），则此条件可能成立。例如，如果两个真实后验都是单峰且近似高斯的，一个简单的变分族将能很好地近似两者，从而导致 $\\mathrm{KL}^{\\star}$ 值既小又相似。\n\n反之，该近似也可能不可靠。考虑具有显著过度离散（方差大于均值）的神经尖峰计数数据的情况。泊松模型 $\\mathcal{M}_{\\mathrm{P}}$ 无法解释这一点，将其拟合到过度离散的数据可能会导致 $p(\\beta \\mid y_{1:T}, \\mathcal{M}_{\\mathrm{P}})$ 的后验几何形状设定不当且难以处理。而带有额外离散参数 $\\kappa$ 的负二项模型 $\\mathcal{M}_{\\mathrm{NB}}$ 正是为处理这种情况而设计的，可能会产生一个表现更好的后验 $p(\\beta, \\kappa \\mid y_{1:T}, \\mathcal{M}_{\\mathrm{NB}})$。如果变分族能比拟合勉强的泊松后验更准确地捕捉负二项后验，就可能导致 $\\mathrm{KL}^{\\star}_{\\mathrm{NB}} \\ll \\mathrm{KL}^{\\star}_{\\mathrm{P}}$。在这种情况下，差值 $(\\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}})$ 将无法准确反映真实的对数贝叶斯因子，因为它会被 KL 项之间大的正差值所混淆。如果泊松模型较小的 ELBO 是由于较差的变分拟合而非较低的模型证据所致，那么基于 ELBO 的比较可能会错误地偏好泊松模型。\n\n总之，要使 ELBO 差异代理成为贝叶斯因子偏好的可靠指标，一个充分条件是所选的变分族对两个竞争模型的真实后验提供了同样好（或同样差）的近似，从而导致几乎相等的残余 KL 散度。这是一个强假设，它取决于数据生成过程、模型设定以及变分族的灵活性之间的相互作用。",
            "answer": "$$\n\\boxed{33.12}\n$$"
        }
    ]
}