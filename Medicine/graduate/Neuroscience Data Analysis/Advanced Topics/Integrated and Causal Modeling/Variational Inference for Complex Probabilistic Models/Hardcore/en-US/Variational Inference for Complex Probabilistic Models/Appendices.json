{
    "hands_on_practices": [
        {
            "introduction": "To begin, we tackle the mechanics of training a Variational Autoencoder (VAE), a cornerstone of modern generative modeling in neuroscience. This practice guides you through deriving the reparameterization trick, which enables low-variance gradient estimation through the stochastic latent space, and computing the closed-form Kullback-Leibler divergence term that regularizes the posterior. Mastering these steps is essential for building and troubleshooting any deep latent variable model.",
            "id": "4203098",
            "problem": "A neuroscience lab is modeling high-dimensional neural recordings $x \\in \\mathbb{R}^{n}$ (for example, binned spike counts or calcium fluorescence features) using a latent variable model with a Variational Autoencoder (VAE). Let the latent variable be $z \\in \\mathbb{R}^{d}$. The generative model specifies a prior $p(z)$ and a conditional likelihood $p_{\\theta}(x \\mid z)$. The amortized approximate posterior is $q_{\\phi}(z \\mid x)$, and the training objective is the Evidence Lower Bound (ELBO), defined for a single observation $x$ as\n$$\n\\mathcal{L}(x;\\theta,\\phi) \\equiv \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\ln p_{\\theta}(x \\mid z) \\right] - \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right),\n$$\nwhere $\\mathrm{KL}(\\cdot \\,\\|\\, \\cdot)$ denotes the Kullback-Leibler divergence.\n\nAssume the following standard choices:\n- The prior is standard normal, $p(z) = \\mathcal{N}(z; 0, I_{d})$.\n- The variational family is diagonal Gaussian, $q_{\\phi}(z \\mid x) = \\mathcal{N}\\!\\bigl(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x))\\bigr)$, where $\\mu_{\\phi}(x) \\in \\mathbb{R}^{d}$ and $\\sigma_{\\phi}(x) \\in \\mathbb{R}_{+}^{d}$. Equivalently, you may parameterize by the log-variance vector $\\ell_{\\phi}(x) \\in \\mathbb{R}^{d}$ with $\\sigma_{\\phi,i}^{2}(x) = \\exp(\\ell_{\\phi,i}(x))$ for each coordinate $i \\in \\{1,\\dots,d\\}$.\n- The conditional likelihood $p_{\\theta}(x \\mid z)$ is differentiable with respect to $z$ for almost every $z$.\n\nTasks:\n- Using the reparameterization trick with $\\epsilon \\sim \\mathcal{N}(0, I_{d})$ and $z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon$, derive from first principles an expression for the gradient $\\nabla_{\\phi}\\,\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\ln p_{\\theta}(x \\mid z) \\right]$ as an expectation over $\\epsilon$. Your expression should make explicit how the gradient of $\\ln p_{\\theta}(x \\mid z)$ with respect to $z$ couples with the Jacobians of $\\mu_{\\phi}(x)$ and $\\sigma_{\\phi}(x)$ (or $\\ell_{\\phi}(x)$) with respect to $\\phi$.\n- Compute the closed-form expression of the regularizer $\\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)$ in terms of $\\mu_{\\phi}(x)$ and $\\sigma_{\\phi}(x)$ (or equivalently $\\ell_{\\phi}(x)$). Assume $d \\geq 1$ is finite.\n\nProvide the final answer as a single analytic expression for the Kullback-Leibler divergence in closed form for one observation $x$. No numerical approximation or rounding is required. Do not include units in your final answer.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective. It presents a standard set of derivations for a Variational Autoencoder (VAE), a foundational model in modern machine learning and its applications, including neuroscience. All required information is provided, and the tasks are mathematically precise.\n\nThe problem consists of two tasks: first, to derive an expression for the gradient of the reconstruction term of the Evidence Lower Bound (ELBO) with respect to the variational parameters $\\phi$, and second, to compute the closed-form expression for the Kullback-Leibler (KL) divergence term.\n\nLet us denote the vector of parameters for the variational posterior as $\\phi \\in \\mathbb{R}^P$. The quantities $\\mu_{\\phi}(x)$ and $\\sigma_{\\phi}(x)$ are functions of $x$ parameterized by $\\phi$.\n\n**Task 1: Gradient of the Reconstruction Term**\n\nThe first task is to derive an expression for the gradient $\\nabla_{\\phi}\\,\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\ln p_{\\theta}(x \\mid z) \\right]$. This term is often called the reconstruction term. A direct differentiation is problematic because the distribution with respect to which the expectation is taken, $q_{\\phi}(z \\mid x)$, depends on the parameters $\\phi$. We use the reparameterization trick to address this.\n\nLet the term of interest be $J(\\phi) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\ln p_{\\theta}(x \\mid z) \\right]$.\nThe variational posterior is given as $q_{\\phi}(z \\mid x) = \\mathcal{N}\\!\\bigl(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^{2}(x))\\bigr)$. A random variable $z \\sim q_{\\phi}(z \\mid x)$ can be expressed as a deterministic transformation of a parameter-free random variable $\\epsilon \\sim \\mathcal{N}(0, I_{d})$. The reparameterization is given by:\n$$ z = g(\\phi, \\epsilon, x) = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon $$\nwhere $\\odot$ denotes the element-wise product. Here, $\\mu_{\\phi}(x) \\in \\mathbb{R}^d$, $\\sigma_{\\phi}(x) \\in \\mathbb{R}^d$, and $\\epsilon \\in \\mathbb{R}^d$. The $i$-th component of $z$ is $z_i = \\mu_{\\phi,i}(x) + \\sigma_{\\phi,i}(x) \\epsilon_i$.\n\nUsing this reparameterization, we can rewrite the expectation over $q_{\\phi}(z \\mid x)$ as an expectation over the fixed distribution $p(\\epsilon) = \\mathcal{N}(\\epsilon; 0, I_d)$:\n$$ J(\\phi) = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I_d)}\\!\\left[ \\ln p_{\\theta}(x \\mid z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon) \\right] $$\nNow, the gradient with respect to $\\phi$ can be moved inside the expectation, because the distribution of $\\epsilon$ does not depend on $\\phi$.\n$$ \\nabla_{\\phi} J(\\phi) = \\nabla_{\\phi} \\mathbb{E}_{\\epsilon}\\!\\left[ \\ln p_{\\theta}(x \\mid z) \\right] = \\mathbb{E}_{\\epsilon}\\!\\left[ \\nabla_{\\phi} \\ln p_{\\theta}(x \\mid z) \\right] $$\nWe apply the multivariable chain rule to compute the inner gradient. The parameters $\\phi$ influence $\\ln p_{\\theta}(x \\mid z)$ only through $z$.\n$$ \\nabla_{\\phi} \\ln p_{\\theta}(x \\mid z) = \\left( \\frac{\\partial z}{\\partial \\phi} \\right)^T \\nabla_{z} \\ln p_{\\theta}(x \\mid z) $$\nHere, $\\frac{\\partial z}{\\partial \\phi}$ is the Jacobian matrix of the transformation $g$ with respect to $\\phi$, of size $d \\times P$. Let us consider a single parameter $\\phi_k$.\n$$ \\frac{\\partial}{\\partial \\phi_k} \\ln p_{\\theta}(x \\mid z) = \\sum_{i=1}^d \\frac{\\partial \\ln p_{\\theta}(x \\mid z)}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\phi_k} $$\nThe derivative of $z_i$ with respect to $\\phi_k$ is:\n$$ \\frac{\\partial z_i}{\\partial \\phi_k} = \\frac{\\partial \\mu_{\\phi,i}(x)}{\\partial \\phi_k} + \\epsilon_i \\frac{\\partial \\sigma_{\\phi,i}(x)}{\\partial \\phi_k} $$\nSubstituting this into the expression for the gradient of $J(\\phi)$ with respect to $\\phi_k$:\n$$ \\frac{\\partial J(\\phi)}{\\partial \\phi_k} = \\mathbb{E}_{\\epsilon}\\!\\left[ \\sum_{i=1}^d \\frac{\\partial \\ln p_{\\theta}(x \\mid z)}{\\partial z_i} \\left( \\frac{\\partial \\mu_{\\phi,i}(x)}{\\partial \\phi_k} + \\epsilon_i \\frac{\\partial \\sigma_{\\phi,i}(x)}{\\partial \\phi_k} \\right) \\right] $$\nThis expression can be written in vector form using Jacobians. Let $J_{\\mu}(\\phi)$ and $J_{\\sigma}(\\phi)$ be the Jacobians of $\\mu_{\\phi}(x)$ and $\\sigma_{\\phi}(x)$ with respect to $\\phi$. These are $d \\times P$ matrices where $(J_{\\mu}(\\phi))_{ik} = \\frac{\\partial \\mu_{\\phi,i}(x)}{\\partial \\phi_k}$. The full gradient vector $\\nabla_{\\phi} J(\\phi)$ is:\n$$ \\nabla_{\\phi} J(\\phi) = \\mathbb{E}_{\\epsilon}\\!\\left[ J_{\\mu}(\\phi)^T \\nabla_z \\ln p_{\\theta}(x \\mid z) + J_{\\sigma}(\\phi)^T \\left(\\epsilon \\odot \\nabla_z \\ln p_{\\theta}(x \\mid z)\\right) \\right] $$\nwhere $z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon$. This form makes explicit the coupling between the gradient of the log-likelihood with respect to the latent variables $z$ and the Jacobians of the mean and standard deviation functions with respect to the variational parameters $\\phi$.\n\nIf we use the log-variance parameterization $\\sigma_{\\phi,i}^2(x) = \\exp(\\ell_{\\phi,i}(x))$, then $\\sigma_{\\phi,i}(x) = \\exp(\\frac{1}{2}\\ell_{\\phi,i}(x))$. The derivative becomes:\n$$ \\frac{\\partial \\sigma_{\\phi,i}(x)}{\\partial \\phi_k} = \\exp\\left(\\frac{1}{2}\\ell_{\\phi,i}(x)\\right) \\cdot \\frac{1}{2} \\frac{\\partial \\ell_{\\phi,i}(x)}{\\partial \\phi_k} = \\frac{1}{2} \\sigma_{\\phi,i}(x) \\frac{\\partial \\ell_{\\phi,i}(x)}{\\partial \\phi_k} $$\nThe gradient expression then becomes a function of the Jacobian of the log-variance, $J_{\\ell}(\\phi)$.\n\n**Task 2: Closed-form of the KL Regularizer**\n\nThe second task is to compute the closed-form expression for the regularizer term, $\\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)$.\nThe two distributions are:\n-   $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu, \\Sigma)$ where $\\mu = \\mu_{\\phi}(x)$ and $\\Sigma = \\operatorname{diag}(\\sigma_{\\phi,1}^2(x), \\dots, \\sigma_{\\phi,d}^2(x))$.\n-   $p(z) = \\mathcal{N}(z; 0, I_d)$.\n\nThe KL divergence is defined as $\\mathrm{KL}(q\\|p) = \\int q(z) \\ln \\frac{q(z)}{p(z)} dz = \\mathbb{E}_{z \\sim q}[\\ln q(z) - \\ln p(z)]$.\nThe log-probability density function for a $d$-dimensional Gaussian distribution $\\mathcal{N}(z; \\mu_0, \\Sigma_0)$ is:\n$$ \\ln \\mathcal{N}(z; \\mu_0, \\Sigma_0) = -\\frac{1}{2} (z - \\mu_0)^T \\Sigma_0^{-1} (z - \\mu_0) - \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(\\Sigma_0)) $$\nWe need to calculate $\\mathbb{E}_{z \\sim q}[\\ln q(z)]$ and $\\mathbb{E}_{z \\sim q}[\\ln p(z)]$.\n\nFirst, for $\\mathbb{E}_{z \\sim q}[\\ln q(z)]$ (the negative entropy of $q$):\n$$ \\mathbb{E}_{z \\sim q}[\\ln q(z)] = \\mathbb{E}_{z \\sim q}\\left[ -\\frac{1}{2} (z - \\mu)^T \\Sigma^{-1} (z - \\mu) - \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(\\Sigma)) \\right] $$\nThe expectation of the quadratic term is:\n$$ \\mathbb{E}_{z \\sim q}\\left[ (z - \\mu)^T \\Sigma^{-1} (z - \\mu) \\right] = \\mathbb{E}_{z \\sim q}\\left[ \\operatorname{tr}\\left((z - \\mu)^T \\Sigma^{-1} (z - \\mu)\\right) \\right] = \\mathbb{E}_{z \\sim q}\\left[ \\operatorname{tr}\\left(\\Sigma^{-1} (z - \\mu)(z - \\mu)^T\\right) \\right] $$\n$$ = \\operatorname{tr}\\left( \\Sigma^{-1} \\mathbb{E}_{z \\sim q}\\left[(z - \\mu)(z - \\mu)^T\\right] \\right) = \\operatorname{tr}(\\Sigma^{-1} \\Sigma) = \\operatorname{tr}(I_d) = d $$\nSo, $\\mathbb{E}_{z \\sim q}[\\ln q(z)] = -\\frac{d}{2} - \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(\\Sigma))$.\n\nSecond, for $\\mathbb{E}_{z \\sim q}[\\ln p(z)]$ (the negative cross-entropy):\n$$ \\mathbb{E}_{z \\sim q}[\\ln p(z)] = \\mathbb{E}_{z \\sim q}\\left[ -\\frac{1}{2} z^T I_d^{-1} z - \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(I_d)) \\right] = -\\frac{1}{2} \\mathbb{E}_{z \\sim q}[z^T z] - \\frac{d}{2} \\ln(2\\pi) $$\nThe expectation is $\\mathbb{E}_{z \\sim q}[z^T z] = \\mathbb{E}_{z \\sim q}[ \\operatorname{tr}(zz^T)] = \\operatorname{tr}(\\mathbb{E}_{z \\sim q}[zz^T])$. Since $\\Sigma = \\mathbb{E}[(z-\\mu)(z-\\mu)^T] = \\mathbb{E}[zz^T] - \\mu\\mu^T$, we have $\\mathbb{E}[zz^T] = \\Sigma + \\mu\\mu^T$.\n$$ \\mathbb{E}_{z \\sim q}[z^T z] = \\operatorname{tr}(\\Sigma + \\mu\\mu^T) = \\operatorname{tr}(\\Sigma) + \\mu^T \\mu $$\nSo, $\\mathbb{E}_{z \\sim q}[\\ln p(z)] = -\\frac{1}{2}(\\operatorname{tr}(\\Sigma) + \\mu^T\\mu) - \\frac{d}{2}\\ln(2\\pi)$.\n\nCombining these results:\n$$ \\mathrm{KL}(q\\|p) = \\mathbb{E}_{z \\sim q}[\\ln q(z)] - \\mathbb{E}_{z \\sim q}[\\ln p(z)] $$\n$$ = \\left(-\\frac{d}{2} - \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(\\Sigma))\\right) - \\left(-\\frac{1}{2}(\\operatorname{tr}(\\Sigma) + \\mu^T\\mu) - \\frac{d}{2}\\ln(2\\pi)\\right) $$\n$$ = \\frac{1}{2} \\left[ \\operatorname{tr}(\\Sigma) + \\mu^T\\mu - d - \\ln(\\det(\\Sigma)) \\right] $$\nNow, we substitute the specific forms of $\\mu$ and $\\Sigma$. Let $\\mu_i = \\mu_{\\phi,i}(x)$ and $\\sigma_i = \\sigma_{\\phi,i}(x)$.\n- $\\mu^T\\mu = \\sum_{i=1}^d \\mu_i^2$.\n- $\\Sigma = \\operatorname{diag}(\\sigma_1^2, \\dots, \\sigma_d^2)$, so $\\operatorname{tr}(\\Sigma) = \\sum_{i=1}^d \\sigma_i^2$.\n- $\\det(\\Sigma) = \\prod_{i=1}^d \\sigma_i^2$, so $\\ln(\\det(\\Sigma)) = \\sum_{i=1}^d \\ln(\\sigma_i^2)$.\n\nSubstituting these into the KL divergence formula:\n$$ \\mathrm{KL}(q\\|p) = \\frac{1}{2} \\left[ \\sum_{i=1}^d \\sigma_i^2 + \\sum_{i=1}^d \\mu_i^2 - d - \\sum_{i=1}^d \\ln(\\sigma_i^2) \\right] $$\nThis can be rewritten by grouping terms inside the summation:\n$$ \\mathrm{KL}(q\\|p) = \\frac{1}{2} \\sum_{i=1}^d \\left( \\mu_i^2 + \\sigma_i^2 - \\ln(\\sigma_i^2) - 1 \\right) $$\nReplacing $\\mu_i$ and $\\sigma_i$ with their full notation, we obtain the final expression. This is the closed-form solution for the KL divergence between a diagonal Gaussian and a standard normal distribution. This term acts as a regularizer, encouraging the approximate posterior to stay close to the prior.",
            "answer": "$$\n\\boxed{\\frac{1}{2} \\sum_{i=1}^{d} \\left( \\mu_{\\phi,i}^{2}(x) + \\sigma_{\\phi,i}^{2}(x) - \\ln\\left(\\sigma_{\\phi,i}^{2}(x)\\right) - 1 \\right)}\n$$"
        },
        {
            "introduction": "We now shift from the amortized inference of VAEs to the iterative coordinate-ascent framework of Variational Bayes. This exercise focuses on deriving the full algorithm for a Bayesian Gaussian Mixture Model, a widely used tool for clustering neural population activity. By working through the E-step and M-step updates, you will gain a deep appreciation for how conjugate priors lead to elegant, analytical solutions for the variational posterior parameters.",
            "id": "4203212",
            "problem": "You are analyzing simultaneous firing rate vectors from $D$ extracellularly recorded neurons in short time bins during a behavioral task. Let $\\mathbf{x}_{n} \\in \\mathbb{R}^{D}$ denote the firing rate vector on bin $n$, with $n = 1,\\dots,N$. Assume that the neural population exhibits latent multimodality across task conditions, and model the distribution of $\\mathbf{x}_{n}$ with a finite mixture of $K$ multivariate Gaussian components. The generative model is specified by the following well-tested probabilistic structure:\n- The mixture weights $\\boldsymbol{\\pi} = (\\pi_{1},\\dots,\\pi_{K})$ satisfy $\\sum_{k=1}^{K} \\pi_{k} = 1$ and have a Dirichlet prior with hyperparameters $\\boldsymbol{\\alpha}_{0} = (\\alpha_{0,1},\\dots,\\alpha_{0,K})$.\n- For each component $k \\in \\{1,\\dots,K\\}$, the precision matrix $\\boldsymbol{\\Lambda}_{k}$ has a Wishart prior with positive definite scale matrix $\\mathbf{W}_{0}$ and degrees of freedom $\\nu_{0}  D - 1$, and the mean $\\boldsymbol{\\mu}_{k}$ conditional on $\\boldsymbol{\\Lambda}_{k}$ has a multivariate Normal prior with mean $\\mathbf{m}_{0}$ and precision-scaled covariance $(\\beta_{0} \\boldsymbol{\\Lambda}_{k})^{-1}$. This joint prior on $(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$ is the conjugate Normal–Wishart family.\n- The latent assignment $z_{n} \\in \\{1,\\dots,K\\}$ for observation $n$ follows a categorical distribution $\\mathrm{Cat}(\\boldsymbol{\\pi})$, and conditional on $z_{n} = k$ and component parameters $(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$, the firing rate vector $\\mathbf{x}_{n}$ is distributed as $\\mathcal{N}(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k}^{-1})$.\n\nStarting from the core definitions of Bayesian inference and the Evidence Lower Bound (ELBO), formulate a mean-field variational family $q(\\boldsymbol{\\pi}) \\prod_{k=1}^{K} q(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k}) \\prod_{n=1}^{N} q(z_{n})$ and derive the coordinate-ascent Variational Bayes Expectation–Maximization (VBEM) algorithm for this model. Your derivation must proceed from first principles (Bayes’ rule, conjugacy, and expected complete-data log likelihood) without invoking shortcut formulas.\n\nSpecifically:\n1. Derive the E-step variational responsibilities $r_{nk} = q(z_{n} = k)$ as normalized exponentials of the expected complete-data log likelihood under the current variational factors $q(\\boldsymbol{\\pi})$ and $q(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$. Express $r_{nk}$ in terms of expectations of $\\ln \\pi_{k}$, $\\ln |\\boldsymbol{\\Lambda}_{k}|$, and the quadratic form $(\\mathbf{x}_{n} - \\boldsymbol{\\mu}_{k})^{\\top} \\boldsymbol{\\Lambda}_{k} (\\mathbf{x}_{n} - \\boldsymbol{\\mu}_{k})$ under the variational distributions.\n2. Derive the M-step updates (coordinate ascent updates) for the variational parameters of $q(\\boldsymbol{\\pi})$ and $q(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$ by maximizing the ELBO with respect to each factor. Show that the optimal $q(\\boldsymbol{\\pi})$ remains Dirichlet and the optimal $q(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$ remains Normal–Wishart, and provide the updated parameters in terms of the responsibilities $r_{nk}$ and the data $\\{\\mathbf{x}_{n}\\}_{n=1}^{N}$.\n\nFinally, to produce a concrete, checkable analytic quantity, specialize to $K = 2$ mixture components and an arbitrary dimension $D \\geq 1$. Let the current variational parameters be $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\alpha_{2})$ for $q(\\boldsymbol{\\pi})$, and for each $k \\in \\{1,2\\}$, let $q(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Lambda}_{k})$ have parameters $(\\mathbf{m}_{k}, \\beta_{k}, \\mathbf{W}_{k}, \\nu_{k})$ in the Normal–Wishart family. Using only the fundamental expectations consistent with these families and your derivation, write the closed-form analytic expression for the E-step responsibility $r_{n1}$ for a single observation $\\mathbf{x}_{n}$ in terms of $(\\alpha_{1}, \\alpha_{2})$, $(\\mathbf{m}_{1}, \\beta_{1}, \\mathbf{W}_{1}, \\nu_{1})$, $(\\mathbf{m}_{2}, \\beta_{2}, \\mathbf{W}_{2}, \\nu_{2})$, and $\\mathbf{x}_{n}$. Your final answer must be a single analytic expression. Do not include any units. No numerical evaluation is required.",
            "solution": "The user-provided problem statement is valid. It is a well-posed, scientifically grounded, and objective task within the domain of computational neuroscience and machine learning. The problem describes a standard Bayesian Gaussian Mixture Model (GMM) and requests the derivation of the Variational Bayes Expectation-Maximization (VBEM) algorithm, followed by a specific analytical result. All necessary components of the model and the variational family are provided, and there are no contradictions or ambiguities. We may therefore proceed with a full derivation.\n\nLet the complete set of variables be $\\boldsymbol{\\Psi} = \\{\\mathbf{Z}, \\boldsymbol{\\pi}, \\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\}_{k=1}^K\\}$, where $\\mathbf{Z} = \\{z_n\\}_{n=1}^N$ are the latent assignments and $\\{\\boldsymbol{\\pi}, \\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\}\\}$ are the model parameters. The observed data are $\\mathbf{X} = \\{\\mathbf{x}_n\\}_{n=1}^N$. The joint probability distribution over all variables is $p(\\mathbf{X}, \\boldsymbol{\\Psi})$.\n\nVariational Bayes aims to approximate the true posterior $p(\\boldsymbol{\\Psi}|\\mathbf{X})$ with a simpler, factorized distribution $q(\\boldsymbol{\\Psi})$. The specified mean-field variational family is:\n$$q(\\boldsymbol{\\Psi}) = q(\\mathbf{Z}) q(\\boldsymbol{\\pi}) q(\\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\}) = \\left( \\prod_{n=1}^N q(z_n) \\right) q(\\boldsymbol{\\pi}) \\left( \\prod_{k=1}^K q(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) \\right)$$\nThe goal is to minimize the Kullback-Leibler (KL) divergence $\\mathrm{KL}(q(\\boldsymbol{\\Psi}) || p(\\boldsymbol{\\Psi}|\\mathbf{X}))$, which is equivalent to maximizing the Evidence Lower Bound (ELBO), $\\mathcal{L}(q)$:\n$$\\mathcal{L}(q) = \\mathbb{E}_{q(\\boldsymbol{\\Psi})} \\left[ \\ln \\frac{p(\\mathbf{X}, \\boldsymbol{\\Psi})}{q(\\boldsymbol{\\Psi})} \\right]$$\nThe coordinate-ascent algorithm iteratively optimizes each factor $q_j$ of $q(\\boldsymbol{\\Psi})$ while holding the others fixed. The general update for a factor $q_j(\\boldsymbol{\\psi}_j)$ is given by:\n$$\\ln q_j^*(\\boldsymbol{\\psi}_j) = \\mathbb{E}_{q_{\\neg j}}[\\ln p(\\mathbf{X}, \\boldsymbol{\\Psi})] + \\mathrm{const}$$\nwhere $\\mathbb{E}_{q_{\\neg j}}$ denotes the expectation with respect to all factors in $q(\\boldsymbol{\\Psi})$ except for $q_j$.\n\nThe logarithm of the complete-data joint probability $p(\\mathbf{X}, \\boldsymbol{\\Psi})$ is:\n\\begin{align*}\n\\ln p(\\mathbf{X}, \\mathbf{Z}, \\boldsymbol{\\pi}, \\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\}) =  \\ln p(\\boldsymbol{\\pi}|\\boldsymbol{\\alpha}_0) + \\sum_{k=1}^K \\ln p(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k|\\mathbf{m}_0, \\beta_0, \\mathbf{W}_0, \\nu_0) \\\\\n + \\sum_{n=1}^N \\ln p(z_n|\\boldsymbol{\\pi}) + \\sum_{n=1}^N \\ln p(\\mathbf{x}_n|z_n, \\boldsymbol{\\mu}_{z_n}, \\boldsymbol{\\Lambda}_{z_n})\n\\end{align*}\nIntroducing indicator variables $z_{nk}$ where $z_{nk}=1$ if $z_n=k$ and $0$ otherwise, we can write:\n\\begin{align*}\n\\ln p(\\mathbf{X}, \\mathbf{Z}, \\dots) =  \\ln p(\\boldsymbol{\\pi}) + \\sum_{k=1}^K \\ln p(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) + \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\ln \\pi_k \\\\\n + \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\left( \\frac{1}{2} \\ln |\\boldsymbol{\\Lambda}_k| - \\frac{D}{2} \\ln(2\\pi) - \\frac{1}{2}(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Lambda}_k (\\mathbf{x}_n - \\boldsymbol{\\mu}_k) \\right)\n\\end{align*}\n\n**1. E-step: Derivation of the responsibilities $r_{nk}$**\n\nWe optimize the factor $q(z_n)$ for a single observation $n$. The update rule gives:\n$$\\ln q^*(z_n) = \\mathbb{E}_{q(\\boldsymbol{\\pi})q(\\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\})} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z}, \\dots) \\right] + \\mathrm{const}$$\nWe only need terms depending on $z_n$. Let $q(z_n=k) = r_{nk}$.\n$$\\ln q^*(z_n) = \\sum_{k=1}^K z_{nk} \\left( \\mathbb{E}[\\ln \\pi_k] + \\mathbb{E}\\left[\\frac{1}{2}\\ln|\\boldsymbol{\\Lambda}_k| - \\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^\\top\\boldsymbol{\\Lambda}_k(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)\\right] \\right) + \\mathrm{const}$$\nwhere constant terms not depending on $k$ have been absorbed. This shows that $q^*(z_n)$ is a categorical distribution.\nThe probability for assigning observation $n$ to component $k$, i.e., the responsibility $r_{nk}$, is:\n$$r_{nk} = q^*(z_n=k) = \\frac{\\exp(\\ln \\rho_{nk})}{\\sum_{j=1}^K \\exp(\\ln \\rho_{nj})}$$\nwhere $\\ln \\rho_{nk}$ is the term in the parenthesis above:\n$$\\ln \\rho_{nk} = \\mathbb{E}[\\ln \\pi_k] + \\frac{1}{2}\\mathbb{E}[\\ln|\\boldsymbol{\\Lambda}_k|] - \\frac{1}{2}\\mathbb{E}[(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^\\top\\boldsymbol{\\Lambda}_k(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)]$$\nThis expression defines the variational responsibility $r_{nk}$ in terms of expectations under the current variational distributions for the parameters, as requested.\n\n**2. M-step: Derivation of parameter updates**\n\n**Update for $q(\\boldsymbol{\\pi})$**:\nWe optimize $q(\\boldsymbol{\\pi})$, keeping other factors fixed.\n$$\\ln q^*(\\boldsymbol{\\pi}) = \\mathbb{E}_{q(\\mathbf{Z})q(\\{\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k\\})} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z}, \\dots) \\right] + \\mathrm{const}$$\nThe relevant terms are the prior on $\\boldsymbol{\\pi}$ and the likelihood of the assignments $\\mathbf{Z}$:\n$$\\ln q^*(\\boldsymbol{\\pi}) = \\ln p(\\boldsymbol{\\pi}|\\boldsymbol{\\alpha}_0) + \\mathbb{E}_{q(\\mathbf{Z})} \\left[\\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\ln \\pi_k\\right] + \\mathrm{const}$$\nThe prior is $p(\\boldsymbol{\\pi}|\\boldsymbol{\\alpha}_0) \\propto \\prod_{k=1}^K \\pi_k^{\\alpha_{0,k}-1}$. The expectation is $\\mathbb{E}[z_{nk}] = r_{nk}$.\n$$\\ln q^*(\\boldsymbol{\\pi}) = \\sum_{k=1}^K (\\alpha_{0,k}-1)\\ln\\pi_k + \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} \\ln\\pi_k + \\mathrm{const} = \\sum_{k=1}^K (\\alpha_{0,k} + \\sum_{n=1}^N r_{nk} - 1)\\ln\\pi_k + \\mathrm{const}$$\nThis is the log-kernel of a Dirichlet distribution, so $q^*(\\boldsymbol{\\pi}) = \\mathrm{Dir}(\\boldsymbol{\\pi}|\\boldsymbol{\\alpha})$ with updated parameters:\n$$\\alpha_k = \\alpha_{0,k} + N_k, \\quad \\text{where} \\quad N_k = \\sum_{n=1}^N r_{nk}$$\n\n**Update for $q(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k)$**:\nThe factors for each component $k$ are independent, so we update them one at a time.\n$$\\ln q^*(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) = \\mathbb{E}_{q(\\mathbf{Z})q(\\boldsymbol{\\pi})q_{\\neg k}} \\left[ \\ln p(\\mathbf{X}, \\mathbf{Z}, \\dots) \\right] + \\mathrm{const}$$\nThe relevant terms are the prior on $(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k)$ and the data likelihood for component $k$:\n$$\\ln q^*(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) = \\ln p(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) + \\sum_{n=1}^N \\mathbb{E}[z_{nk}] \\ln \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k^{-1}) + \\mathrm{const}$$\nUsing $p(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) = p(\\boldsymbol{\\mu}_k|\\boldsymbol{\\Lambda}_k)p(\\boldsymbol{\\Lambda}_k)$, the log-prior is:\n$$\\ln p(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) = \\frac{\\nu_0 - D - 1}{2}\\ln|\\boldsymbol{\\Lambda}_k| - \\frac{1}{2}\\mathrm{tr}(\\mathbf{W}_0^{-1}\\boldsymbol{\\Lambda}_k) + \\frac{1}{2}\\ln|\\boldsymbol{\\Lambda}_k| + \\frac{D}{2}\\ln\\beta_0 - \\frac{\\beta_0}{2}(\\boldsymbol{\\mu}_k-\\mathbf{m}_0)^\\top\\boldsymbol{\\Lambda}_k(\\boldsymbol{\\mu}_k-\\mathbf{m}_0) + \\dots$$\nThe log-likelihood part is:\n$$\\sum_{n=1}^N r_{nk} \\left( \\frac{1}{2}\\ln|\\boldsymbol{\\Lambda}_k| - \\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^\\top\\boldsymbol{\\Lambda}_k(\\mathbf{x}_n-\\boldsymbol{\\mu}_k) \\right) + \\dots$$\nCombining all terms involving $\\boldsymbol{\\mu}_k$ and $\\boldsymbol{\\Lambda}_k$ and grouping by these variables, we can identify the form of the updated Normal-Wishart posterior. This involves completing the square for terms in $\\boldsymbol{\\mu}_k$ and collecting terms in $\\boldsymbol{\\Lambda}_k$. The calculation reveals that $q^*(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k)$ is a Normal-Wishart distribution $\\mathcal{NW}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k | \\mathbf{m}_k, \\beta_k, \\mathbf{W}_k, \\nu_k)$ with the following updated parameters:\n$N_k = \\sum_{n=1}^N r_{nk}$\n$\\overline{\\mathbf{x}}_k = \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} \\mathbf{x}_n$\n$\\mathbf{S}_k = \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} (\\mathbf{x}_n - \\overline{\\mathbf{x}}_k)(\\mathbf{x}_n - \\overline{\\mathbf{x}}_k)^\\top$\n\nThe updated parameters are:\n$\\beta_k = \\beta_0 + N_k$\n$\\mathbf{m}_k = \\frac{\\beta_0 \\mathbf{m}_0 + N_k \\overline{\\mathbf{x}}_k}{\\beta_k}$\n$\\nu_k = \\nu_0 + N_k$\n$\\mathbf{W}_k^{-1} = \\mathbf{W}_0^{-1} + N_k \\mathbf{S}_k + \\frac{\\beta_0 N_k}{\\beta_0 + N_k}(\\overline{\\mathbf{x}}_k - \\mathbf{m}_0)(\\overline{\\mathbf{x}}_k - \\mathbf{m}_0)^\\top$\n\n**Final Expression for $r_{n1}$ with $K=2$**\n\nFor $K=2$, the responsibility $r_{n1}$ is $r_{n1} = (1 + \\exp(\\ln\\rho_{n2} - \\ln\\rho_{n1}))^{-1}$. We must evaluate the terms in $\\ln\\rho_{nk}$ using the given variational parameters $(\\alpha_1, \\alpha_2)$ and $(\\mathbf{m}_k, \\beta_k, \\mathbf{W}_k, \\nu_k)$ for $k \\in \\{1,2\\}$. We need the following standard expectations:\n1.  For $q(\\boldsymbol{\\pi}) = \\mathrm{Dir}(\\boldsymbol{\\pi}|\\alpha_1, \\alpha_2)$: $\\mathbb{E}[\\ln \\pi_k] = \\psi(\\alpha_k) - \\psi(\\alpha_1+\\alpha_2)$, where $\\psi(\\cdot)$ is the digamma function.\n2.  For $q(\\boldsymbol{\\Lambda}_k) = \\mathcal{W}(\\boldsymbol{\\Lambda}_k|\\mathbf{W}_k, \\nu_k)$: $\\mathbb{E}[\\ln|\\boldsymbol{\\Lambda}_k|] = \\sum_{i=1}^D \\psi\\left(\\frac{\\nu_k+1-i}{2}\\right) + D \\ln 2 + \\ln|\\mathbf{W}_k|$.\n3.  For $q(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Lambda}_k) = \\mathcal{NW}(\\cdot|\\mathbf{m}_k, \\beta_k, \\mathbf{W}_k, \\nu_k)$:\n    The expectation of the quadratic form is $\\mathbb{E}[(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Lambda}_k (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)] = \\mathbb{E}[\\mathrm{tr}((\\mathbf{x}_n - \\boldsymbol{\\mu}_k)(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Lambda}_k)]$.\n    The inner expectation is $\\mathbb{E}_{\\boldsymbol{\\mu}_k|\\boldsymbol{\\Lambda}_k}[(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top] = \\mathrm{Cov}(\\boldsymbol{\\mu}_k|\\boldsymbol{\\Lambda}_k) + (\\mathbf{x}_n - \\mathbb{E}[\\boldsymbol{\\mu}_k|\\boldsymbol{\\Lambda}_k])(\\mathbf{x}_n - \\mathbb{E}[\\boldsymbol{\\mu}_k|\\boldsymbol{\\Lambda}_k])^\\top = (\\beta_k \\boldsymbol{\\Lambda}_k)^{-1} + (\\mathbf{x}_n - \\mathbf{m}_k)(\\mathbf{x}_n - \\mathbf{m}_k)^\\top$.\n    Taking expectation over $\\boldsymbol{\\Lambda}_k$ and then the trace gives:\n    $\\mathbb{E}[(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Lambda}_k (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)] = \\nu_k (\\mathbf{x}_n - \\mathbf{m}_k)^\\top \\mathbf{W}_k (\\mathbf{x}_n - \\mathbf{m}_k) + \\frac{D}{\\beta_k}$.\n\nSubstituting these into the expression for $\\ln \\rho_{nk}$, and forming the difference $\\ln\\rho_{n2} - \\ln\\rho_{n1}$, all terms that are constant with respect to $k$ (like $\\psi(\\alpha_1+\\alpha_2)$, $D\\ln 2$, $D\\ln(2\\pi)$) cancel.\nThe resulting expression for $r_{n1}$ is then built from these pieces.\n\n$\\ln\\rho_{n2} - \\ln\\rho_{n1} = \\left(\\psi(\\alpha_2) - \\psi(\\alpha_1)\\right) + \\frac{1}{2}\\left(\\mathbb{E}[\\ln|\\boldsymbol{\\Lambda}_2|] - \\mathbb{E}[\\ln|\\boldsymbol{\\Lambda}_1|]\\right) - \\frac{1}{2}\\left(\\mathbb{E}[q_2] - \\mathbb{E}[q_1]\\right)$\nwhere $q_k = (\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^\\top\\boldsymbol{\\Lambda}_k(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)$.\n\nSubstituting the full expressions for the expectations:\n$\\ln\\rho_{n2} - \\ln\\rho_{n1} = \\left(\\psi(\\alpha_2) - \\psi(\\alpha_1)\\right) + \\frac{1}{2}\\left( \\ln|\\mathbf{W}_2| - \\ln|\\mathbf{W}_1| + \\sum_{i=1}^D \\left(\\psi\\left(\\frac{\\nu_2+1-i}{2}\\right) - \\psi\\left(\\frac{\\nu_1+1-i}{2}\\right)\\right) \\right) - \\frac{1}{2}\\left( \\left(\\nu_2(\\mathbf{x}_n-\\mathbf{m}_2)^\\top\\mathbf{W}_2(\\mathbf{x}_n-\\mathbf{m}_2)+\\frac{D}{\\beta_2}\\right) - \\left(\\nu_1(\\mathbf{x}_n-\\mathbf{m}_1)^\\top\\mathbf{W}_1(\\mathbf{x}_n-\\mathbf{m}_1)+\\frac{D}{\\beta_1}\\right) \\right)$.\n\nThis gives the final expression for $r_{n1}$.",
            "answer": "$$\n\\boxed{\n\\left( 1 + \\exp\\left( \\psi(\\alpha_2) - \\psi(\\alpha_1) + \\frac{1}{2}\\left(\\ln|\\mathbf{W}_2| - \\ln|\\mathbf{W}_1| + \\sum_{i=1}^{D}\\left(\\psi\\left(\\frac{\\nu_2+1-i}{2}\\right) - \\psi\\left(\\frac{\\nu_1+1-i}{2}\\right)\\right) - \\frac{D}{\\beta_2} + \\frac{D}{\\beta_1} - \\nu_2(\\mathbf{x}_n-\\mathbf{m}_2)^{\\top}\\mathbf{W}_2(\\mathbf{x}_n-\\mathbf{m}_2) + \\nu_1(\\mathbf{x}_n-\\mathbf{m}_1)^{\\top}\\mathbf{W}_1(\\mathbf{x}_n-\\mathbf{m}_1)\\right) \\right) \\right)^{-1}\n}\n$$"
        },
        {
            "introduction": "Having explored methods for approximating posteriors, our final practice addresses a higher-level question: how can we use these approximations to compare competing scientific hypotheses? This problem asks you to derive the connection between the Evidence Lower Bound (ELBO) and the Bayes factor, a principled tool for model selection. This exercise not only provides a practical method for comparing models but also deepens your understanding of the ELBO as an approximation to the log marginal likelihood.",
            "id": "4203133",
            "problem": "You are analyzing extracellular spike count data from a single neuron recorded over $T$ time bins under two competing generative models for counts $y_{1:T}$ driven by a shared covariate design matrix $X \\in \\mathbb{R}^{T \\times p}$:\n\n- Model $\\mathcal{M}_{\\mathrm{P}}$: a Poisson Generalized Linear Model (GLM) with canonical log link, latent coefficients $\\beta \\in \\mathbb{R}^{p}$, and prior $p(\\beta \\mid \\mathcal{M}_{\\mathrm{P}})$ that is absolutely continuous and proper.\n- Model $\\mathcal{M}_{\\mathrm{NB}}$: a Negative Binomial GLM with canonical log link, latent coefficients $\\beta \\in \\mathbb{R}^{p}$ and inverse-dispersion $\\kappa  0$, with prior $p(\\beta, \\kappa \\mid \\mathcal{M}_{\\mathrm{NB}})$ that is absolutely continuous and proper.\n\nAssume the following fundamental base:\n- The marginal likelihood (model evidence) under any model $\\mathcal{M}$ is $p(y_{1:T} \\mid \\mathcal{M}) = \\int p(y_{1:T}, \\theta \\mid \\mathcal{M}) \\,\\mathrm{d}\\theta$, where $\\theta$ denotes all latent variables and parameters under $\\mathcal{M}$.\n- For any variational distribution $q(\\theta)$ with full support where $p(y_{1:T}, \\theta \\mid \\mathcal{M})  0$, Jensen’s inequality implies the Evidence Lower Bound (ELBO) $\\mathcal{L}(q, \\mathcal{M}) = \\mathbb{E}_{q}[\\ln p(y_{1:T}, \\theta \\mid \\mathcal{M}) - \\ln q(\\theta)]$ satisfies $\\ln p(y_{1:T} \\mid \\mathcal{M}) \\ge \\mathcal{L}(q, \\mathcal{M})$.\n- The Kullback–Leibler (KL) divergence satisfies $\\mathrm{KL}\\!\\left(q(\\theta) \\,\\|\\, p(\\theta \\mid y_{1:T}, \\mathcal{M})\\right) \\ge 0$ and the identity $\\ln p(y_{1:T} \\mid \\mathcal{M}) = \\mathcal{L}(q, \\mathcal{M}) + \\mathrm{KL}\\!\\left(q(\\theta) \\,\\|\\, p(\\theta \\mid y_{1:T}, \\mathcal{M})\\right)$.\n\nTask:\n1. Starting from the base above, derive a variational approximation to the Bayes factor $\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = \\dfrac{p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{P}})}{p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{NB}})}$ in terms of the optimized ELBOs $\\mathcal{L}^{\\star}_{\\mathrm{P}}$ and $\\mathcal{L}^{\\star}_{\\mathrm{NB}}$ obtained by variational inference under $\\mathcal{M}_{\\mathrm{P}}$ and $\\mathcal{M}_{\\mathrm{NB}}$, respectively. Your derivation must begin from the definitions of marginal likelihood, ELBO, and KL divergence, and proceed to a closed-form analytic expression for a variational Bayes factor proxy in terms of $\\mathcal{L}^{\\star}_{\\mathrm{P}}$ and $\\mathcal{L}^{\\star}_{\\mathrm{NB}}$ only.\n2. Using the derived expression, consider a real dataset fit where converged variational inference produced optimized ELBOs $\\mathcal{L}^{\\star}_{\\mathrm{P}} = -1250.3$ and $\\mathcal{L}^{\\star}_{\\mathrm{NB}} = -1253.8$. Compute the variational approximation to $\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}}$ as a unitless scalar. Round your answer to four significant figures.\n3. In words (no calculation required), discuss sufficient conditions under which the ELBO-difference proxy is a reliable indicator of the true Bayes factor’s preference, explicitly tying your reasoning to the KL identities above and to statistical properties that can arise in neural spike-count models.\n\nExpress the final Bayes factor approximation in part $2$ as a single number with no units. Round to four significant figures.",
            "solution": "The problem asks for a three-part analysis involving the comparison of two statistical models for neural spike count data using variational inference. The parts are: a derivation of a variational approximation to the Bayes factor, a numerical calculation of this approximation, and a qualitative discussion of the approximation's reliability.\n\n### Part 1: Derivation of the Variational Bayes Factor Approximation\n\nThe objective is to derive an approximation for the Bayes factor $\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}}$ using the optimized Evidence Lower Bounds (ELBOs) from variational inference. The Bayes factor for comparing Model $\\mathcal{M}_{\\mathrm{P}}$ to Model $\\mathcal{M}_{\\mathrm{NB}}$ is defined as the ratio of their marginal likelihoods (model evidences):\n$$\n\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = \\frac{p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{P}})}{p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{NB}})}\n$$\nTo work with a more convenient quantity, we take the natural logarithm of the Bayes factor:\n$$\n\\ln \\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = \\ln p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{P}}) - \\ln p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{NB}})\n$$\nThe problem statement provides the fundamental identity connecting the log marginal likelihood, the ELBO, and the Kullback-Leibler (KL) divergence between the variational approximation $q(\\theta)$ and the true posterior $p(\\theta \\mid y_{1:T}, \\mathcal{M})$. For any model $\\mathcal{M}$, this identity is:\n$$\n\\ln p(y_{1:T} \\mid \\mathcal{M}) = \\mathcal{L}(q, \\mathcal{M}) + \\mathrm{KL}\\!\\left(q(\\theta) \\,\\|\\, p(\\theta \\mid y_{1:T}, \\mathcal{M})\\right)\n$$\nwhere $\\mathcal{L}(q, \\mathcal{M}) = \\mathbb{E}_{q}[\\ln p(y_{1:T}, \\theta \\mid \\mathcal{M}) - \\ln q(\\theta)]$.\n\nVariational inference seeks to find an optimal variational distribution $q^{\\star}(\\theta)$ within a chosen family that maximizes the ELBO. Since the log marginal likelihood $\\ln p(y_{1:T} \\mid \\mathcal{M})$ is a constant with respect to $q$, maximizing the ELBO is equivalent to minimizing the KL divergence term. Let $q^{\\star}_{\\mathrm{P}}$ and $q^{\\star}_{\\mathrm{NB}}$ be the optimized variational distributions for models $\\mathcal{M}_{\\mathrm{P}}$ and $\\mathcal{M}_{\\mathrm{NB}}$, respectively. The corresponding maximized ELBOs are denoted $\\mathcal{L}^{\\star}_{\\mathrm{P}} = \\mathcal{L}(q^{\\star}_{\\mathrm{P}}, \\mathcal{M}_{\\mathrm{P}})$ and $\\mathcal{L}^{\\star}_{\\mathrm{NB}} = \\mathcal{L}(q^{\\star}_{\\mathrm{NB}}, \\mathcal{M}_{\\mathrm{NB}})$.\n\nWe can write the exact log marginal likelihood for each model using the optimized quantities:\nFor Model $\\mathcal{M}_{\\mathrm{P}}$ (with latent variables $\\theta_{\\mathrm{P}} = \\beta$):\n$$\n\\ln p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{P}}) = \\mathcal{L}^{\\star}_{\\mathrm{P}} + \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{P}}(\\theta_{\\mathrm{P}}) \\,\\|\\, p(\\theta_{\\mathrm{P}} \\mid y_{1:T}, \\mathcal{M}_{\\mathrm{P}})\\right)\n$$\nFor Model $\\mathcal{M}_{\\mathrm{NB}}$ (with latent variables $\\theta_{\\mathrm{NB}} = (\\beta, \\kappa)$):\n$$\n\\ln p(y_{1:T} \\mid \\mathcal{M}_{\\mathrm{NB}}) = \\mathcal{L}^{\\star}_{\\mathrm{NB}} + \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{NB}}(\\theta_{\\mathrm{NB}}) \\,\\|\\, p(\\theta_{\\mathrm{NB}} \\mid y_{1:T}, \\mathcal{M}_{\\mathrm{NB}})\\right)\n$$\nSubstituting these expressions into the equation for the log Bayes factor yields:\n$$\n\\ln \\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = \\left( \\mathcal{L}^{\\star}_{\\mathrm{P}} + \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{P}} \\,\\|\\, p_{\\mathrm{P}}\\right) \\right) - \\left( \\mathcal{L}^{\\star}_{\\mathrm{NB}} + \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{NB}} \\,\\|\\, p_{\\mathrm{NB}}\\right) \\right)\n$$\nwhere $p_{\\mathcal{M}}$ is shorthand for the true posterior under model $\\mathcal{M}$. Rearranging terms, we get:\n$$\n\\ln \\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = (\\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}}) + \\left( \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{P}} \\,\\|\\, p_{\\mathrm{P}}\\right) - \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathrm{NB}} \\,\\|\\, p_{\\mathrm{NB}}\\right) \\right)\n$$\nThe ELBO $\\mathcal{L}(q, \\mathcal{M})$ is a lower bound on the log marginal evidence, $\\ln p(y_{1:T} \\mid \\mathcal{M})$. A common practice in variational Bayesian model comparison is to approximate the log marginal evidence with its maximized lower bound, $\\mathcal{L}^{\\star}_{\\mathcal{M}}$. This approximation is equivalent to assuming that the residual KL divergence term is negligible or, more precisely, that the difference between the residual KL terms for the two models is close to zero.\n$$\n\\ln p(y_{1:T} \\mid \\mathcal{M}) \\approx \\mathcal{L}^{\\star}_{\\mathcal{M}}\n$$\nApplying this approximation to the log Bayes factor, we obtain the variational approximation:\n$$\n\\ln \\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} \\approx \\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}}\n$$\nExponentiating both sides gives the desired closed-form analytic expression for the variational Bayes factor proxy, which we denote $\\widehat{\\mathrm{BF}}_{\\mathrm{P}, \\mathrm{NB}}$:\n$$\n\\widehat{\\mathrm{BF}}_{\\mathrm{P}, \\mathrm{NB}} = \\exp(\\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}})\n$$\n\n### Part 2: Numerical Calculation\n\nGiven the optimized ELBO values $\\mathcal{L}^{\\star}_{\\mathrm{P}} = -1250.3$ and $\\mathcal{L}^{\\star}_{\\mathrm{NB}} = -1253.8$, we can compute the variational approximation to the Bayes factor $\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}}$.\n\nFirst, we calculate the difference in the ELBOs:\n$$\n\\Delta\\mathcal{L} = \\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}} = -1250.3 - (-1253.8) = -1250.3 + 1253.8 = 3.5\n$$\nNext, we substitute this value into the expression derived in Part 1:\n$$\n\\widehat{\\mathrm{BF}}_{\\mathrm{P}, \\mathrm{NB}} = \\exp(\\Delta\\mathcal{L}) = \\exp(3.5)\n$$\nCalculating the numerical value:\n$$\n\\exp(3.5) \\approx 33.1154519...\n$$\nRounding to four significant figures, we get $33.12$. This value, being significantly greater than $1$, suggests that the data provide strong evidence in favor of the Poisson model $\\mathcal{M}_{\\mathrm{P}}$ over the Negative Binomial model $\\mathcal{M}_{\\mathrm{NB}}$, under the assumptions of the variational approximation.\n\n### Part 3: Discussion on Reliability of the Approximation\n\nThe reliability of the ELBO-difference proxy, $\\widehat{\\mathrm{BF}}_{\\mathrm{P}, \\mathrm{NB}} = \\exp(\\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}})$, as an indicator of the true Bayes factor, $\\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}}$, depends entirely on the magnitude of the error term neglected in the derivation. From Part 1, the exact log Bayes factor is:\n$$\n\\ln \\mathrm{BF}_{\\mathrm{P}, \\mathrm{NB}} = (\\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}}) + \\left( \\mathrm{KL}^{\\star}_{\\mathrm{P}} - \\mathrm{KL}^{\\star}_{\\mathrm{NB}} \\right)\n$$\nwhere $\\mathrm{KL}^{\\star}_{\\mathcal{M}} = \\mathrm{KL}\\!\\left(q^{\\star}_{\\mathcal{M}} \\,\\|\\, p(\\theta_{\\mathcal{M}} \\mid y_{1:T}, \\mathcal{M})\\right)$ is the KL divergence between the optimal variational posterior and the true posterior for model $\\mathcal{M}$. The approximation is reliable if and only if the difference in these residual KL divergences is close to zero:\n$$\n\\mathrm{KL}^{\\star}_{\\mathrm{P}} - \\mathrm{KL}^{\\star}_{\\mathrm{NB}} \\approx 0 \\quad \\implies \\quad \\mathrm{KL}^{\\star}_{\\mathrm{P}} \\approx \\mathrm{KL}^{\\star}_{\\mathrm{NB}}\n$$\nA sufficient condition for the approximation to be reliable is that the quality of the variational fit to the true posterior is approximately equal for both models. The term $\\mathrm{KL}^{\\star}_{\\mathcal{M}}$ represents the \"approximation gap\" or the information lost by using $q^{\\star}_{\\mathcal{M}}$ instead of the true posterior. Thus, the reliability hinges on the assumption that this gap is of a similar size for both the Poisson and Negative Binomial models.\n\nThis condition may hold if the geometric complexities of the true posteriors under both models are similar and the chosen variational family (e.g., mean-field) is equally well-suited (or equally ill-suited) to approximating both. For instance, if both true posteriors are unimodal and nearly Gaussian, a simple variational family will approximate both well, leading to small and similar $\\mathrm{KL}^{\\star}$ values.\n\nConversely, the approximation can be unreliable. Consider the case of neural spike count data with significant overdispersion (variance greater than the mean). The Poisson model $\\mathcal{M}_{\\mathrm{P}}$ cannot account for this, and fitting it to overdispersed data may result in a misspecified and challenging posterior geometry for $p(\\beta \\mid y_{1:T}, \\mathcal{M}_{\\mathrm{P}})$. The Negative Binomial model $\\mathcal{M}_{\\mathrm{NB}}$, with its extra dispersion parameter $\\kappa$, is designed to handle this and might yield a better-behaved posterior $p(\\beta, \\kappa \\mid y_{1:T}, \\mathcal{M}_{\\mathrm{NB}})$. If the variational family can capture the NB posterior more accurately than the strained Poisson posterior, it could lead to $\\mathrm{KL}^{\\star}_{\\mathrm{NB}} \\ll \\mathrm{KL}^{\\star}_{\\mathrm{P}}$. In this scenario, the difference $(\\mathcal{L}^{\\star}_{\\mathrm{P}} - \\mathcal{L}^{\\star}_{\\mathrm{NB}})$ would not accurately reflect the true log Bayes factor, as it would be confounded by the large, positive difference in KL terms. The ELBO-based comparison might wrongly favor the Poisson model if its smaller ELBO is due to a poor variational fit rather than lower model evidence.\n\nIn summary, a sufficient condition for the ELBO-difference proxy to be a reliable indicator of the Bayes factor's preference is that the chosen variational family provides an equally good (or equally poor) approximation to the true posteriors of both competing models, resulting in nearly equal residual KL divergences. This is a strong assumption that depends on the interaction between the data-generating process, the model specifications, and the flexibility of the variational family.",
            "answer": "$$\n\\boxed{33.12}\n$$"
        }
    ]
}