{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握典型相关分析（CCA），我们必须从其数学基础——一个约束最优化问题——开始。这个练习将引导你通过一个具体的例子，一步步完成从拉格朗日乘子法到特征值问题的推导，从而将抽象的理论与实际的计算联系起来，加深对CCA核心思想的理解。",
            "id": "4322614",
            "problem": "在系统生物医学中，整合互补的分子视图（例如，信使核糖核酸（mRNA）基因表达和代谢物丰度）可以揭示跨受试者的协同生物学变异。考虑在同一个队列中收集的两个中心化数据视图，其中包含 $p=2$ 个基因表达变量和 $q=2$ 个代谢物变量。假设基因表达视图的样本协方差由下式给出\n$$\n\\Sigma_{XX} = \\begin{pmatrix}\n1  0.3 \\\\\n0.3  1.2\n\\end{pmatrix},\n$$\n代谢物视图的样本协方差由下式给出\n$$\n\\Sigma_{YY} = \\begin{pmatrix}\n1.5  0.4 \\\\\n0.4  0.8\n\\end{pmatrix},\n$$\n互协方差由下式给出\n$$\n\\Sigma_{XY} = \\begin{pmatrix}\n0.7  0.2 \\\\\n0.1  0.5\n\\end{pmatrix}.\n$$\n典范相关分析（CCA）旨在寻找线性投影 $a \\in \\mathbb{R}^{2}$ 和 $b \\in \\mathbb{R}^{2}$，以在各自协方差几何结构的单位方差约束下，最大化投影视图的协方差。形式上，求解以下约束最大化问题\n$$\n\\max_{a \\in \\mathbb{R}^{2},\\, b \\in \\mathbb{R}^{2}} \\; a^{\\top}\\Sigma_{XY} b \\quad \\text{subject to} \\quad a^{\\top}\\Sigma_{XX} a = 1,\\; b^{\\top}\\Sigma_{YY} b = 1.\n$$\n从上述基本定义出发，构建必要的优化条件，并将问题简化为一个特征值问题，对所提供的矩阵进行显式求解，从而推导出第一个典范相关及其相关的典范方向。给出第一个典范相关的数值，四舍五入到四位有效数字。最终数值答案应表示为纯小数（无单位）。最终答案框中只应包含第一个典范相关的数值。",
            "solution": "问题是为两个数据视图 $X$ 和 $Y$ 找到第一个典范相关和相关的典范方向，其中给定了样本协方差矩阵 $\\Sigma_{XX}$ 和 $\\Sigma_{YY}$，以及互协方差矩阵 $\\Sigma_{XY}$。该问题被表述为一个约束最大化问题：\n$$ \\max_{a \\in \\mathbb{R}^{2},\\, b \\in \\mathbb{R}^{2}} \\; a^{\\top}\\Sigma_{XY} b \\quad \\text{subject to} \\quad a^{\\top}\\Sigma_{XX} a = 1,\\; b^{\\top}\\Sigma_{YY} b = 1. $$\n量 $a^{\\top}\\Sigma_{XY} b$ 代表投影变量 $X a$ 和 $Y b$ 之间的协方差。约束条件将这些投影变量的方差归一化为 $1$。因此，被最大化的量是典范变量之间的相关性，即 $\\text{corr}(a^{\\top}X, b^{\\top}Y)$。\n\n为解决此约束优化问题，我们使用拉格朗日乘数法。拉格朗日函数 $\\mathcal{L}$ 为：\n$$ \\mathcal{L}(a, b, \\lambda_a, \\lambda_b) = a^{\\top}\\Sigma_{XY} b - \\frac{\\lambda_a}{2}(a^{\\top}\\Sigma_{XX} a - 1) - \\frac{\\lambda_b}{2}(b^{\\top}\\Sigma_{YY} b - 1) $$\n其中 $\\frac{\\lambda_a}{2}$ 和 $\\frac{\\lambda_b}{2}$ 是拉格朗日乘数（因子 $\\frac{1}{2}$ 是为了代数计算的方便）。\n\n为了找到最优的 $a$ 和 $b$，我们将 $\\mathcal{L}$ 分别对 $a$ 和 $b$ 求偏导数，并令其为零。\n对于对称矩阵 $B$，使用恒等式 $\\frac{\\partial}{\\partial v}(u^{\\top}Av) = A^{\\top}u$ 和 $\\frac{\\partial}{\\partial v}(v^{\\top}Bv) = 2Bv$：\n\n1.  对 $a$ 的导数：\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial a} = \\Sigma_{XY} b - \\lambda_a \\Sigma_{XX} a = 0 \\implies \\Sigma_{XY} b = \\lambda_a \\Sigma_{XX} a $$\n\n2.  对 $b$ 的导数：\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\Sigma_{XY}^{\\top} a - \\lambda_b \\Sigma_{YY} b = 0 \\implies \\Sigma_{XY}^{\\top} a = \\lambda_b \\Sigma_{YY} b $$\n\n现在，我们来确定拉格朗日乘数的含义。用 $a^{\\top}$ 左乘第一个方程：\n$$ a^{\\top}\\Sigma_{XY} b = \\lambda_a a^{\\top}\\Sigma_{XX} a $$\n使用约束条件 $a^{\\top}\\Sigma_{XX} a = 1$，我们得到 $\\lambda_a = a^{\\top}\\Sigma_{XY} b$。这正是我们想要最大化的量。\n\n类似地，用 $b^{\\top}$ 左乘第二个方程：\n$$ b^{\\top}\\Sigma_{XY}^{\\top} a = \\lambda_b b^{\\top}\\Sigma_{YY} b $$\n使用约束条件 $b^{\\top}\\Sigma_{YY} b = 1$，我们得到 $\\lambda_b = b^{\\top}\\Sigma_{XY}^{\\top} a$。\n由于 $a^{\\top}\\Sigma_{XY} b$ 是一个标量，它等于其转置，即 $(a^{\\top}\\Sigma_{XY} b)^{\\top} = b^{\\top}\\Sigma_{XY}^{\\top} a$。因此，$\\lambda_a = \\lambda_b$。我们用 $\\rho$ 表示这个共同的值，即典范相关。\n\n优化条件成为一个由两个耦合方程组成的系统：\n$$ \\Sigma_{XY} b = \\rho \\Sigma_{XX} a \\quad (1) $$\n$$ \\Sigma_{XY}^{\\top} a = \\rho \\Sigma_{YY} b \\quad (2) $$\n\n为了求解这个系统，我们可以将其简化为一个特征值问题。协方差矩阵 $\\Sigma_{XX}$ 和 $\\Sigma_{YY}$ 是对称且正定的（因为它们的行列式 $1.11 > 0$ 和 $1.04 > 0$，且对角线元素为正），因此它们的逆矩阵 $\\Sigma_{XX}^{-1}$ 和 $\\Sigma_{YY}^{-1}$ 存在。\n\n从方程（1），我们可以用 $b$ 来表示 $a$（假设 $\\rho \\neq 0$）：\n$$ a = \\frac{1}{\\rho} \\Sigma_{XX}^{-1} \\Sigma_{XY} b $$\n将 $a$ 的这个表达式代入方程（2）：\n$$ \\Sigma_{XY}^{\\top} \\left( \\frac{1}{\\rho} \\Sigma_{XX}^{-1} \\Sigma_{XY} b \\right) = \\rho \\Sigma_{YY} b $$\n$$ \\frac{1}{\\rho} \\Sigma_{XY}^{\\top} \\Sigma_{XX}^{-1} \\Sigma_{XY} b = \\rho \\Sigma_{YY} b $$\n乘以 $\\rho$，然后从左侧乘以 $\\Sigma_{YY}^{-1}$，得到：\n$$ \\Sigma_{YY}^{-1} \\Sigma_{XY}^{\\top} \\Sigma_{XX}^{-1} \\Sigma_{XY} b = \\rho^2 b $$\n这是一个形式为 $K b = \\lambda b$ 的标准特征值问题，其中矩阵为 $K = \\Sigma_{YY}^{-1} \\Sigma_{XY}^{\\top} \\Sigma_{XX}^{-1} \\Sigma_{XY}$，特征值为 $\\lambda = \\rho^2$，即典范相关的平方。特征向量 $b$ 是 $Y$ 视图的典范方向向量。\n\n现在我们用给定的矩阵进行显式计算：\n$$ \\Sigma_{XX} = \\begin{pmatrix} 1  0.3 \\\\ 0.3  1.2 \\end{pmatrix}, \\quad \\Sigma_{YY} = \\begin{pmatrix} 1.5  0.4 \\\\ 0.4  0.8 \\end{pmatrix}, \\quad \\Sigma_{XY} = \\begin{pmatrix} 0.7  0.2 \\\\ 0.1  0.5 \\end{pmatrix} $$\n首先，我们计算逆矩阵：\n$$ \\det(\\Sigma_{XX}) = 1(1.2) - (0.3)^2 = 1.11 \\implies \\Sigma_{XX}^{-1} = \\frac{1}{1.11} \\begin{pmatrix} 1.2  -0.3 \\\\ -0.3  1 \\end{pmatrix} $$\n$$ \\det(\\Sigma_{YY}) = 1.5(0.8) - (0.4)^2 = 1.04 \\implies \\Sigma_{YY}^{-1} = \\frac{1}{1.04} \\begin{pmatrix} 0.8  -0.4 \\\\ -0.4  1.5 \\end{pmatrix} $$\n现在我们逐步构建矩阵 $K$。\n我们来计算 $M_1 = \\Sigma_{XX}^{-1} \\Sigma_{XY}$：\n$$ M_1 = \\frac{1}{1.11} \\begin{pmatrix} 1.2  -0.3 \\\\ -0.3  1 \\end{pmatrix} \\begin{pmatrix} 0.7  0.2 \\\\ 0.1  0.5 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.84 - 0.03  0.24 - 0.15 \\\\ -0.21 + 0.1  -0.06 + 0.5 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.81  0.09 \\\\ -0.11  0.44 \\end{pmatrix} $$\n接下来，我们计算 $M_2 = \\Sigma_{XY}^{\\top} M_1$：\n$$ M_2 = \\begin{pmatrix} 0.7  0.1 \\\\ 0.2  0.5 \\end{pmatrix} \\frac{1}{1.11} \\begin{pmatrix} 0.81  0.09 \\\\ -0.11  0.44 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.567 - 0.011  0.063 + 0.044 \\\\ 0.162 - 0.055  0.018 + 0.22 \\end{pmatrix} = \\frac{1}{1.11} \\begin{pmatrix} 0.556  0.107 \\\\ 0.107  0.238 \\end{pmatrix} $$\n最后，我们计算 $K = \\Sigma_{YY}^{-1} M_2$：\n$$ K = \\frac{1}{1.04} \\begin{pmatrix} 0.8  -0.4 \\\\ -0.4  1.5 \\end{pmatrix} \\frac{1}{1.11} \\begin{pmatrix} 0.556  0.107 \\\\ 0.107  0.238 \\end{pmatrix} $$\n$$ K = \\frac{1}{1.1544} \\begin{pmatrix} 0.8(0.556) - 0.4(0.107)  0.8(0.107) - 0.4(0.238) \\\\ -0.4(0.556) + 1.5(0.107)  -0.4(0.107) + 1.5(0.238) \\end{pmatrix} $$\n$$ K = \\frac{1}{1.1544} \\begin{pmatrix} 0.4448 - 0.0428  0.0856 - 0.0952 \\\\ -0.2224 + 0.1605  -0.0428 + 0.357 \\end{pmatrix} = \\frac{1}{1.1544} \\begin{pmatrix} 0.402  -0.0096 \\\\ -0.0619  0.3142 \\end{pmatrix} $$\n$$ K \\approx \\begin{pmatrix} 0.348233  -0.008316 \\\\ -0.053612  0.272176 \\end{pmatrix} $$\n$K$ 的特征值 $\\lambda=\\rho^2$ 可通过求解特征方程 $\\det(K-\\lambda I)=0$ 得到：\n$$ (0.348233 - \\lambda)(0.272176 - \\lambda) - (-0.008316)(-0.053612) = 0 $$\n$$ \\lambda^2 - (\\text{tr}(K))\\lambda + \\det(K) = 0 $$\n$$ \\text{tr}(K) \\approx 0.348233 + 0.272176 = 0.620409 $$\n$$ \\det(K) \\approx (0.348233)(0.272176) - (0.0004458) \\approx 0.094781 - 0.000446 = 0.094335 $$\n特征方程近似为：\n$$ \\lambda^2 - 0.620409 \\lambda + 0.094335 = 0 $$\n求解这个关于 $\\lambda$ 的二次方程：\n$$ \\lambda = \\frac{0.620409 \\pm \\sqrt{(0.620409)^2 - 4(0.094335)}}{2} = \\frac{0.620409 \\pm \\sqrt{0.384907 - 0.37734}}{2} $$\n$$ \\lambda = \\frac{0.620409 \\pm \\sqrt{0.007567}}{2} = \\frac{0.620409 \\pm 0.086988}{2} $$\n两个特征值是：\n$$ \\lambda_1 = \\frac{0.620409 + 0.086988}{2} = \\frac{0.707397}{2} \\approx 0.35370 $$\n$$ \\lambda_2 = \\frac{0.620409 - 0.086988}{2} = \\frac{0.533421}{2} \\approx 0.26671 $$\n目标是最大化相关性，所以我们取最大的特征值 $\\lambda_1$。第一个典范相关 $\\rho_1$ 是它的平方根：\n$$ \\rho_1 = \\sqrt{\\lambda_1} \\approx \\sqrt{0.35370} \\approx 0.594726 $$\n问题要求将数值四舍五入到四位有效数字。\n$$ \\rho_1 \\approx 0.5947 $$\n相关的典范方向 $b_1$ 是 $K$ 对应于 $\\lambda_1$ 的特征向量。然后通过 $a_1 = \\frac{1}{\\rho_1} \\Sigma_{XX}^{-1} \\Sigma_{XY} b_1$ 求得方向向量 $a_1$，其中 $b_1$ 和 $a_1$ 经过缩放以满足单位方差约束。根据问题的最终答案要求，这些向量的显式计算虽然是完整分析的一部分，但相对于求相关值本身是次要的。",
            "answer": "$$\\boxed{0.5947}$$"
        },
        {
            "introduction": "在将CCA应用于实际数据（尤其是在高维神经科学数据中）之前，评估数据的基本属性至关重要。这个练习旨在探讨样本量（$n$）和特征数（$p, q$）之间的关系如何决定协方差矩阵的秩，进而影响经典CCA方法的可行性。通过这个实践，你将学会识别何时需要采用正则化或基于SVD的更高级方法。",
            "id": "4144760",
            "problem": "您正在典型相关分析 (Canonical Correlation Analysis, CCA) 应用于神经科学数据的背景下分析两个多变量数据集，其中一个数据集代表神经特征，另一个代表行为或刺激特征。设神经数据集为一个 $n \\times p$ 矩阵 $X$，行为数据集为一个 $n \\times q$ 矩阵 $Y$，其中 $n$ 是观测数量（例如，试验次数），$p$ 是神经特征的数量，$q$ 是行为特征的数量。$X$ 和 $Y$ 都经过按列标准化（z-score 处理），意味着每列的均值为零，方差为一。\n\n从基本定义开始：\n- 对于中心化数据，样本协方差矩阵定义为\n$$\nS_{xx} = \\frac{1}{n-1} X^\\top X, \\quad S_{yy} = \\frac{1}{n-1} Y^\\top Y, \\quad S_{xy} = \\frac{1}{n-1} X^\\top Y,\n$$\n这依赖于从中心化数据中经验性估计协方差的成熟原则。\n- 对于任何中心化的 $n \\times p$ 矩阵，其样本协方差矩阵的秩上界为 $\\min(p, n-1)$。对于交叉协方差，其秩上界为 $\\min(p, q, n-1)$。\n- 在典型相关分析 (CCA) 中，经典的广义特征值公式是否可行取决于 $S_{xx}$ 和 $S_{yy}$ 的可逆性，而这又取决于这些矩阵是否为满秩。如果 $S_{xx}$ 和 $S_{yy}$ 不是满秩，则必须使用正则化或基于奇异值分解 (SVD) 的方法。\n\n您的任务是编写一个完整的、可运行的程序，该程序针对指定的测试套件，构建经过 z-score 处理的矩阵 $X$ 和 $Y$，计算 $S_{xx}$、$S_{yy}$ 和 $S_{xy}$，并报告以下内容：\n1. $S_{xx}$、$S_{yy}$ 和 $S_{xy}$ 的维度。\n2. 使用基于机器精度的奇异值阈值得出的 $S_{xx}$、$S_{yy}$ 和 $S_{xy}$ 的数值估计秩。\n3. 秩的理论上界：$S_{xx}$ 为 $\\min(p, n-1)$，$S_{yy}$ 为 $\\min(q, n-1)$，$S_{xy}$ 为 $\\min(p, q, n-1)$。\n4. 一个布尔值，指示经典的广义特征值 CCA 是否可行，其定义为 $S_{xx}$ 和 $S_{yy}$ 是否均为满秩（即秩分别为 $p$ 和 $q$）。\n5. 数据可提供的最大典型维度数，定义为 $\\min(\\operatorname{rank}(S_{xx}), \\operatorname{rank}(S_{yy}))$。\n\n为确保科学真实性和可复现性，请使用以下测试套件。对于每种情况，使用固定的随机种子将 $X$ 和 $Y$ 生成为独立的标准正态矩阵，然后通过减去经验均值并除以经验标准差来对每列进行 z-score 处理。在指定了共线性的情况下，通过从其基列的子集中创建完全重复的列来构造 $Y$，以降低其秩。\n\n测试套件：\n- 情况 1：$n=200$, $p=50$, $q=60$，种子 $0$，无诱导共线性。\n- 情况 2：$n=40$, $p=50$, $q=10$，种子 $1$，无诱导共线性。\n- 情况 3：$n=120$, $p=80$, $q=90$，种子 $2$，在 $Y$ 中诱导 30 个重复列（因此，$Y$ 的构造方式是首先创建一个大小为 $n \\times (q-30)$ 的基础矩阵，然后附加 30 个从随机选择的基础列中完全复制的列）。\n- 情况 4：$n=51$, $p=50$, $q=10$，种子 $3$，无诱导共线性。\n\n数值秩估算规则：\n- 对于矩阵 $M$，通过奇异值分解计算其奇异值 $\\sigma_i$。设 $\\varepsilon$ 为双精度浮点数的机器精度。将阈值定义为\n$$\n\\tau = \\max(\\text{number of rows of } M, \\text{number of columns of } M) \\cdot \\varepsilon \\cdot \\max_i \\sigma_i.\n$$\n数值秩是大于 $\\tau$ 的奇异值的数量。\n\n对计算典型向量的影响：\n- 当且仅当 $S_{xx}$ 的秩为 $p$ 且 $S_{yy}$ 的秩为 $q$ 时，报告布尔值 $true$，这表明经典的 CCA 广义特征值公式可以直接应用。否则，报告 $false$，表明需要正则化或基于 SVD 的 CCA。\n- 将最大典型维度数报告为整数 $\\min(\\operatorname{rank}(S_{xx}), \\operatorname{rank}(S_{yy}))$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果本身必须是一个列表，按顺序包含以下元素：\n$$\n[\\; p,\\; q,\\; n,\\; \\text{dim}(S_{xx})\\_1,\\; \\text{dim}(S_{xx})\\_2,\\; \\text{dim}(S_{yy})\\_1,\\; \\text{dim}(S_{yy})\\_2,\\; \\text{dim}(S_{xy})\\_1,\\; \\text{dim}(S_{xy})\\_2,\\; \\operatorname{rank}(S_{xx}),\\; \\operatorname{rank}(S_{yy}),\\; \\operatorname{rank}(S_{xy}),\\; \\min(p, n-1),\\; \\min(q, n-1),\\; \\min(p, q, n-1),\\; \\text{eigen\\_CCA\\_possible},\\; \\text{max\\_canonical\\_dims}\\; ].\n$$\n例如，整体输出应如下所示：\n$$\n[\\;[\\text{case1\\_values}],\\;[\\text{case2\\_values}],\\;[\\text{case3\\_values}],\\;[\\text{case4\\_values}]\\;]\n$$\n不涉及角度和物理单位。所有输出均为整数、布尔值或这些类型的列表。程序必须是自包含的，不需要用户输入、外部文件或网络访问。",
            "solution": "该问题要求分析从两个多变量数据集（由矩阵 $X$ 和 $Y$ 表示）派生出的样本协方差矩阵的性质，以及这些性质对典型相关分析 (CCA) 的影响。该分析将针对一个指定的测试套件进行，考察矩阵维度、数值秩、理论秩界限以及经典 CCA 的可行性。\n\n首先，我们建立数据生成和预处理的流程。对于每个测试用例，我们给定观测数量 $n$、第一个数据集中的特征数量 $p$ 以及第二个数据集中的特征数量 $q$。我们通过从标准正态分布中抽样来生成初始数据矩阵 $X$ 和 $Y$。这是通过使用带种子的伪随机数生成器以保证可复现性来实现的。问题指定矩阵应按列进行标准化（z-score 处理）。对于任意数据矩阵 $M$，每列 $m_j$ 根据以下公式转换为标准化列 $z_j$：\n$$\nz_j = \\frac{m_j - \\bar{m}_j}{s_j}\n$$\n其中 $\\bar{m}_j$ 是该列的样本均值，$s_j$ 是其样本标准差。样本标准差的计算在分母中使用 $n-1$，以与样本协方差矩阵的定义保持一致。这种标准化确保了最终矩阵 $X$ 和 $Y$ 的每一列均值为 0，样本方差为 1。\n\n接下来，我们计算样本协方差矩阵。由于数据矩阵 $X$ 和 $Y$ 是中心化的（列均值为零），样本协方差矩阵由以下表达式给出：\n$$\nS_{xx} = \\frac{1}{n-1} X^\\top X\n$$\n$$\nS_{yy} = \\frac{1}{n-1} Y^\\top Y\n$$\n$$\nS_{xy} = \\frac{1}{n-1} X^\\top Y\n$$\n这些矩阵的维度由 $X$（$n \\times p$）和 $Y$（$n \\times q$）的维度决定。$S_{xx}$ 是一个 $p \\times p$ 矩阵，$S_{yy}$ 是一个 $q \\times q$ 矩阵，交叉协方差矩阵 $S_{xy}$ 是一个 $p \\times q$ 矩阵。\n\n本次分析的一个核心部分是确定矩阵的秩。我们同时考虑理论界限和数值估计。\n将 $n$ 个观测向量（$X$ 的行）中心化后，它们位于一个维度至多为 $n-1$ 的子空间中。因此，矩阵 $X$ 的秩受限于 $\\min(p, n-1)$。由于 $\\operatorname{rank}(S_{xx}) = \\operatorname{rank}(X^\\top X) = \\operatorname{rank}(X)$，所以 $S_{xx}$ 的秩的理论上界是：\n$$\n\\operatorname{rank}(S_{xx}) \\le \\min(p, n-1)\n$$\n同样，对于 $S_{yy}$：\n$$\n\\operatorname{rank}(S_{yy}) \\le \\min(q, n-1)\n$$\n对于交叉协方差矩阵 $S_{xy}$，其秩受限于 $X$ 和 $Y$ 的秩的最小值。这导出了理论界限：\n$$\n\\operatorname{rank}(S_{xy}) \\le \\min(\\operatorname{rank}(X), \\operatorname{rank}(Y)) \\le \\min(\\min(p, n-1), \\min(q, n-1)) = \\min(p, q, n-1)\n$$\n在实践中，由于有限精度算术，我们必须数值化地估计秩。指定的方法涉及奇异值分解 (SVD)。对于任何矩阵 $M$，我们计算其奇异值 $\\sigma_i$，并从大到小排序。阈值 $\\tau$ 定义为：\n$$\n\\tau = \\max(\\text{rows of } M, \\text{columns of } M) \\cdot \\varepsilon \\cdot \\sigma_1\n$$\n其中 $\\varepsilon$ 是浮点类型的机器精度，$\\sigma_1$ 是最大的奇异值。数值秩是严格大于此阈值 $\\tau$ 的奇异值的数量。这能稳健地区分出显著的奇异值和那些在数值上与零无法区分的奇异值。\n\n最后，我们评估其对 CCA 的影响。CCA 的经典公式解决一个广义特征值问题，例如 $(S_{xx}^{-1} S_{xy} S_{yy}^{-1} S_{yx}) w_x = \\rho^2 w_x$。这个公式关键性地要求矩阵 $S_{xx}$ 和 $S_{yy}$ 是可逆的。一个方阵是可逆的当且仅当它是满秩的。因此，经典的、基于直接特征值分解的 CCA 的可行性为真，当且仅当 $\\operatorname{rank}(S_{xx}) = p$ 且 $\\operatorname{rank}(S_{yy}) = q$。如果此条件不满足（例如，在 $p > n-1$ 或 $q > n-1$ 的高维设置中，或在多重共线性的情况下），则必须采用替代方法，如正则化 CCA 或基于 SVD 的 CCA。可以从数据中提取的最大典型变量对的数量（因此也就是非零的典型相关系数）受限于每个数据集的“信息含量”，这由其各自协方差矩阵的秩所捕捉。因此，这个数字由 $\\min(\\operatorname{rank}(S_{xx}), \\operatorname{rank}(S_{yy}))$ 给出。\n\n我们现在将此完整流程应用于测试套件中的每个案例。\n\n案例 1：$n=200$, $p=50$, $q=60$。由于 $p  n-1$ 且 $q  n-1$，我们预期 $S_{xx}$ 和 $S_{yy}$ 是满秩的，即 $\\operatorname{rank}(S_{xx})=50$ 和 $\\operatorname{rank}(S_{yy})=60$。经典 CCA 应该是可行的。最大典型维度数应为 $\\min(50, 60)=50$。\n\n案例 2：$n=40$, $p=50$, $q=10$。这里，$p > n-1$（$50 > 39$）。因此，$S_{xx}$ 必须是秩亏的。其秩最多为 $n-1=39$。$S_{yy}$ 应该是满秩的，因为 $q  n-1$（$10  39$）。由于 $S_{xx}$ 不是满秩，经典 CCA 是不可行的。最大典型维度数为 $\\min(\\operatorname{rank}(S_{xx}), \\operatorname{rank}(S_{yy})) = \\min(39, 10)=10$。\n\n案例 3：$n=120$, $p=80$, $q=90$，在 $Y$ 中有诱导共线性。$Y$ 被构造成有 30 个重复列，因此它只有 $90-30=60$ 个线性无关的列。虽然 $\\operatorname{rank}(S_{yy})$ 的理论界限是 $\\min(90, 119)=90$，但显式构造将其真实秩限制为 60。$S_{xx}$ 应该是满秩的（80）。因为 $\\operatorname{rank}(S_{yy}) = 60 \\neq q=90$，经典 CCA 是不可行的。最大典型维度数为 $\\min(\\operatorname{rank}(S_{xx}), \\operatorname{rank}(S_{yy})) = \\min(80, 60)=60$。\n\n案例 4：$n=51$, $p=50$, $q=10$。这是一个边界情况，$p = n-1 = 50$。中心化矩阵 $X$ 的 50 列从一个连续分布中抽取，并位于一个维度为 50 的子空间中。它们将以概率 1 线性无关。因此，我们预期 $\\operatorname{rank}(S_{xx}) = 50 = p$。对于 $Y$，$q  n-1$，所以我们预期 $\\operatorname{rank}(S_{yy})=10=q$。两个矩阵都应该是满秩的，使得经典 CCA 可行。最大维度为：$\\min(50, 10)=10$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n\n    def zscore(matrix):\n        \"\"\"\n        Standardizes a matrix column-wise (z-scoring).\n        Uses sample standard deviation (ddof=1) for consistency with the\n        sample covariance matrix definition.\n        \"\"\"\n        mean = matrix.mean(axis=0)\n        # Use ddof=1 for sample standard deviation\n        std = matrix.std(axis=0, ddof=1)\n        # Handle columns with zero standard deviation to avoid division by zero\n        std[std == 0] = 1.0\n        return (matrix - mean) / std\n\n    def compute_numerical_rank(matrix):\n        \"\"\"\n        Computes the numerical rank of a matrix based on its singular values.\n        \"\"\"\n        if matrix.size == 0:\n            return 0\n            \n        sv = np.linalg.svd(matrix, compute_uv=False)\n        \n        # The largest singular value is the first element\n        sigma_max = sv[0] if sv.size  0 else 0\n        if sigma_max == 0:\n            return 0\n            \n        # Machine precision for the matrix's data type\n        eps = np.finfo(matrix.dtype).eps\n        \n        # Threshold for considering a singular value as non-zero\n        threshold = max(matrix.shape) * eps * sigma_max\n        \n        # Count singular values greater than the threshold\n        rank = np.sum(sv  threshold)\n        return int(rank)\n\n    def run_case(n, p, q, seed, num_collinear=0):\n        \"\"\"\n        Processes a single test case.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate data matrices\n        X_raw = rng.standard_normal(size=(n, p))\n        \n        if num_collinear  0:\n            q_base = q - num_collinear\n            Y_base = rng.standard_normal(size=(n, q_base))\n            # Randomly select columns from the base to duplicate\n            duplicate_indices = rng.choice(q_base, size=num_collinear, replace=True)\n            Y_duplicates = Y_base[:, duplicate_indices]\n            Y_raw = np.hstack((Y_base, Y_duplicates))\n        else:\n            Y_raw = rng.standard_normal(size=(n, q))\n\n        # 2. Z-score the matrices\n        X = zscore(X_raw)\n        Y = zscore(Y_raw)\n\n        # 3. Compute sample covariance matrices\n        S_xx = (X.T @ X) / (n - 1)\n        S_yy = (Y.T @ Y) / (n - 1)\n        S_xy = (X.T @ Y) / (n - 1)\n\n        # 4. Report dimensions\n        dim_sxx = S_xx.shape\n        dim_syy = S_yy.shape\n        dim_sxy = S_xy.shape\n\n        # 5. Estimate numerical ranks\n        rank_sxx = compute_numerical_rank(S_xx)\n        rank_syy = compute_numerical_rank(S_yy)\n        rank_sxy = compute_numerical_rank(S_xy)\n\n        # 6. Compute theoretical rank bounds\n        bound_sxx = min(p, n - 1)\n        bound_syy = min(q, n - 1)\n        bound_sxy = min(p, q, n - 1)\n\n        # 7. Determine CCA feasibility and max dimensions\n        eigen_cca_possible = (rank_sxx == p) and (rank_syy == q)\n        max_canonical_dims = min(rank_sxx, rank_syy)\n\n        return [\n            p, q, n,\n            dim_sxx[0], dim_sxx[1],\n            dim_syy[0], dim_syy[1],\n            dim_sxy[0], dim_sxy[1],\n            rank_sxx, rank_syy, rank_sxy,\n            bound_sxx, bound_syy, bound_sxy,\n            eigen_cca_possible,\n            max_canonical_dims\n        ]\n\n    test_cases = [\n        {'n': 200, 'p': 50, 'q': 60, 'seed': 0, 'num_collinear': 0},\n        {'n': 40, 'p': 50, 'q': 10, 'seed': 1, 'num_collinear': 0},\n        {'n': 120, 'p': 80, 'q': 90, 'seed': 2, 'num_collinear': 30},\n        {'n': 51, 'p': 50, 'q': 10, 'seed': 3, 'num_collinear': 0},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_case(**case)\n        all_results.append(str(result).replace(\" \", \"\"))\n\n    # Print the final output in the required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "经典CCA在处理高维数据（例如$pn$）时会遇到协方差矩阵奇异的问题，这要求我们采用更稳健的计算策略。本练习将指导你实现一种现代CCA的标准方法：基于奇异值分解（SVD）的算法。通过构造伪逆平方根变换，该方法能够有效处理病态或奇异的协方差矩阵，是现代多元分析工具箱中的关键技术。",
            "id": "4144749",
            "problem": "考虑两个维度分别为 $p$ 和 $q$ 的多元神经测量空间，以及三个数值矩阵：一个对称的类协方差矩阵 $S_{xx} \\in \\mathbb{R}^{p \\times p}$，一个对称的类协方差矩阵 $S_{yy} \\in \\mathbb{R}^{q \\times q}$，以及一个类互协方差矩阵 $S_{xy} \\in \\mathbb{R}^{p \\times q}$。假设所有矩阵均为实值，并代表从联合观测的随机向量中得到的良态二阶统计量，这在典型相关分析（CCA）中是标准做法。目标是通过首先对 $S_{xx}$ 和 $S_{yy}$ 进行特征分解来构建其逆平方根变换，然后分析白化互协方差的奇异值，从而计算典型相关值。\n\n基本原理和定义：\n- 零均值随机向量 $x$ 的协方差矩阵定义为 $S_{xx} = \\mathbb{E}[x x^\\top]$，其中 $\\mathbb{E}[\\cdot]$ 表示期望。类似地，对于同时测量的零均值随机向量 $y$，$S_{yy} = \\mathbb{E}[y y^\\top]$，互协方差为 $S_{xy} = \\mathbb{E}[x y^\\top]$。\n- 一个实对称矩阵允许进行特征分解 $S = Q \\Lambda Q^\\top$，其中 $Q$ 是标准正交矩阵，$\\Lambda$ 是对角矩阵。通过对正特征值取平方根并求倒数，同时将来自非正特征值的贡献设为零，来构造 $S$ 的伪逆平方根。\n- 白化互协方差是乘积 $C = S_{xx}^{-1/2} S_{xy} S_{yy}^{-1/2}$。\n- 实矩阵 $C$ 的奇异值分解（SVD）产生非负奇异值。\n\n算法要求：\n- 通过特征分解 $S = Q \\Lambda Q^\\top$ 计算 $S_{xx}^{-1/2}$ 和 $S_{yy}^{-1/2}$。使用数值阈值 $\\tau = 10^{-10}$：对于每个特征值 $\\lambda_i$，如果 $\\lambda_i  \\tau$，则将其逆平方根权重设为 $1/\\sqrt{\\lambda_i}$，否则设为 $0$。构造 $S^{-1/2} = Q \\operatorname{diag}(w_i) Q^\\top$，其中 $w_i$ 是权重。\n- 构建 $C = S_{xx}^{-1/2} S_{xy} S_{yy}^{-1/2}$ 并计算其奇异值。\n- 令 $m = \\min\\left(p, q, \\operatorname{rank}(S_{xy})\\right)$，其中 $\\operatorname{rank}(S_{xy})$ 计算为 $S_{xy}$ 的奇异值中严格大于 $\\tau$ 的数量。\n- 按降序返回 $C$ 的前 $m$ 个奇异值。将每个值四舍五入到 $6$ 位小数，并表示为十进制浮点数。\n\n测试套件：\n对于以下每种情况，$p$ 和 $q$ 由矩阵的形状隐含给出。\n\n情况 1（一般良态）：\n$$\nS_{xx} = \\begin{bmatrix}\n2.0  0.5  0.0 \\\\\n0.5  1.5  0.1 \\\\\n0.0  0.1  1.2\n\\end{bmatrix},\\quad\nS_{yy} = \\begin{bmatrix}\n1.3  0.2 \\\\\n0.2  1.1\n\\end{bmatrix},\\quad\nS_{xy} = \\begin{bmatrix}\n0.5  0.1 \\\\\n0.3  0.4 \\\\\n0.2  0.6\n\\end{bmatrix}.\n$$\n\n情况 2（无互协方差信号）：\n$$\nS_{xx} = \\begin{bmatrix}\n1.0  0.1 \\\\\n0.1  1.5\n\\end{bmatrix},\\quad\nS_{yy} = \\begin{bmatrix}\n1.2  0.0  0.1 \\\\\n0.0  1.1  0.0 \\\\\n0.1  0.0  1.3\n\\end{bmatrix},\\quad\nS_{xy} = \\begin{bmatrix}\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0\n\\end{bmatrix}.\n$$\n\n情况 3（秩亏互协方差）：\n$$\nS_{xx} = \\begin{bmatrix}\n1.0  0.2 \\\\\n0.2  2.0\n\\end{bmatrix},\\quad\nS_{yy} = \\begin{bmatrix}\n1.5  0.1 \\\\\n0.1  1.4\n\\end{bmatrix},\\quad\nS_{xy} = \\begin{bmatrix}\n1.0  2.0 \\\\\n0.5  1.0\n\\end{bmatrix}.\n$$\n\n情况 4（通过伪逆平方根处理的病态 $S_{xx}$）：\n$$\nS_{xx} = \\begin{bmatrix}\n1.0  0.0  0.0 \\\\\n0.0  0.5  0.0 \\\\\n0.0  0.0  10^{-8}\n\\end{bmatrix},\\quad\nS_{yy} = \\begin{bmatrix}\n1.2  0.0  0.0 \\\\\n0.0  0.8  0.0 \\\\\n0.0  0.0  0.9\n\\end{bmatrix},\\quad\nS_{xy} = \\begin{bmatrix}\n0.3  0.0  0.2 \\\\\n0.1  0.4  0.0 \\\\\n0.5  0.2  0.3\n\\end{bmatrix}.\n$$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的、逗号分隔的结果列表。每个元素对应一个测试用例，其本身是按降序排列的浮点数列表，并按规定四舍五入到 $6$ 位小数。例如，输出应类似于 $[ [\\dots], [\\dots], [\\dots], [\\dots] ]$，但打印时无空格，如 $[[\\dots],[\\dots],[\\dots],[\\dots]]$。",
            "solution": "用户提供了一个多元统计领域的计算问题，具体是典型相关分析（CCA），该方法常应用于神经科学数据分析。在进行求解之前，我将首先验证问题陈述的科学有效性、一致性和完整性。\n\n### 步骤 1：提取已知条件\n\n问题提供了以下数据、定义和要求：\n- **数据**：两个维度分别为 $p$ 和 $q$ 的多元测量空间。为每个测试用例提供了三个实值矩阵：一个对称矩阵 $S_{xx} \\in \\mathbb{R}^{p \\times p}$，一个对称矩阵 $S_{yy} \\in \\mathbb{R}^{q \\times q}$，以及一个互协方差矩阵 $S_{xy} \\in \\mathbb{R}^{p \\times q}$。\n- **定义**：\n    - 对于零均值随机向量 $x$ 和 $y$，协方差矩阵为 $S_{xx} = \\mathbb{E}[x x^\\top]$ 和 $S_{yy} = \\mathbb{E}[y y^\\top]$，互协方差为 $S_{xy} = \\mathbb{E}[x y^\\top]$。\n    - 实对称矩阵 $S$ 的特征分解为 $S = Q \\Lambda Q^\\top$。\n    - $S$ 的伪逆平方根，表示为 $S^{-1/2}$，构造为 $S^{-1/2} = Q \\operatorname{diag}(w_i) Q^\\top$。权重 $w_i$ 基于 $S$ 的特征值 $\\lambda_i$ 和一个阈值 $\\tau = 10^{-10}$ 定义：如果 $\\lambda_i  \\tau$，则 $w_i = 1/\\sqrt{\\lambda_i}$，否则 $w_i = 0$。\n    - 白化互协方差矩阵是 $C = S_{xx}^{-1/2} S_{xy} S_{yy}^{-1/2}$。\n    - 典型相关是 $C$ 的奇异值。\n- **算法要求**：\n    1. 使用指定的伪逆平方根方法计算 $S_{xx}^{-1/2}$ 和 $S_{yy}^{-1/2}$。\n    2. 构造矩阵 $C$。\n    3. 计算 $C$ 的奇异值。\n    4. 确定要返回的值的数量，$m = \\min\\left(p, q, \\operatorname{rank}(S_{xy})\\right)$。$S_{xy}$ 的秩定义为其奇异值中严格大于 $\\tau = 10^{-10}$ 的数量。\n    5. 返回 $C$ 的前 $m$ 个奇异值，按降序排序，每个值四舍五入到 6 位小数。\n- **测试套件**：提供了四组特定的矩阵 $S_{xx}$、$S_{yy}$ 和 $S_{xy}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n我现在将根据提取的信息评估问题的有效性。\n\n- **科学依据充分**：该问题基于典型相关分析（CCA），这是多元统计分析的基石。所描述的方法——将典型相关计算为白化互协方差矩阵的奇异值——是一种标准且计算稳定的方法。使用特征分解求矩阵平方根，并使用正则化阈值 $\\tau$ 构造伪逆平方根，是处理潜在奇异或病态协方差矩阵的成熟数值技术。整个公式植根于线性代数和统计学。\n- **适定性**：问题是适定的。所有必需的输入都已指定。算法步骤是确定且无歧义的，对每个测试用例都能得出唯一的解。$m$ 的定义确保了返回值的数量是明确定义的，并对应于两组变量之间关系的有效秩。\n- **客观性**：问题以精确、客观的数学语言陈述，没有任何主观性或歧义。\n- **缺陷检查清单**：\n    1.  **科学/事实不健全**：无。CCA的原理和相关的线性代数是健全的。\n    2.  **不可形式化/不相关**：无。问题是对一个特定统计算法的形式化实现请求，该算法与指定领域相关。\n    3.  **不完整/矛盾的设置**：无。所有必要的矩阵、参数（$\\tau$）和过程都已明确定义。\n    4.  **不切实际/不可行**：无。所提供的矩阵是协方差结构的小维度表示，测试用例旨在探究不同的有效场景（良态、秩亏、零协方差、病态），这些在真实数据中都是合理的。\n    5.  **病态/结构不良**：无。问题结构清晰，能够导出一个唯一的、稳定的解。\n    6.  **伪深刻/琐碎**：无。问题需要正确实现一个非平凡的统计方法，包括数值稳定性方面的考虑。\n    7.  **超出科学可验证性**：无。结果是数学上可推导和可验证的。\n\n### 步骤 3：结论与行动\n\n问题是**有效的**。这是一个清晰、一致且科学上合理的请求，要求实现一个标准的统计程序。我现在将提供完整的解决方案。\n\n### 解法\n\n目标是计算两组变量之间的典型相关，这两组变量由其协方差结构 $S_{xx}$、$S_{yy}$ 和 $S_{xy}$ 表示。典型相关衡量第一组变量的线性组合与第二组变量的线性组合之间的线性相关性。我们将遵循标准方法，该方法涉及将问题转化为寻找“白化”互协方差矩阵的奇异值。\n\n该过程包括四个主要步骤：\n1.  **构造白化变换**：对每组内部变量进行去相关的变换由其各自协方差矩阵的逆平方根 $S_{xx}^{-1/2}$ 和 $S_{yy}^{-1/2}$ 给出。为了处理 $S_{xx}$ 或 $S_{yy}$ 可能为奇异（即不可逆）的情况，我们构造一个正则化的伪逆平方根。对于给定的对称矩阵 $S \\in \\mathbb{R}^{k \\times k}$，这通过其特征分解 $S = Q \\Lambda Q^\\top$ 来完成，其中 $Q$ 是特征向量的标准正交矩阵，$\\Lambda$ 是相应特征值 $\\{\\lambda_i\\}_{i=1}^k$ 的对角矩阵。伪逆平方根 $S^{-1/2}$ 于是为：\n    $$ S^{-1/2} = Q W Q^\\top $$\n    其中 $W$ 是一个对角矩阵，其对角元素 $w_i$ 计算如下：\n    $$\n    w_i =\n    \\begin{cases}\n    1 / \\sqrt{\\lambda_i}  \\text{若 } \\lambda_i  \\tau \\\\\n    0  \\text{若 } \\lambda_i \\le \\tau\n    \\end{cases}\n    $$\n    其中数值稳定性阈值为 $\\tau = 10^{-10}$。此过程将应用于 $S_{xx}$ 和 $S_{yy}$。\n\n2.  **白化互协方差**：我们将白化变换应用于互协方差矩阵 $S_{xy}$。得到的白化互协方差矩阵 $C$ 由下式给出：\n    $$ C = S_{xx}^{-1/2} S_{xy} S_{yy}^{-1/2} $$\n    矩阵 $C$ 表示第一组白化变量与第二组白化变量之间的协方差。该矩阵 $C$ 的奇异值正是典型相关。\n\n3.  **确定有意义的相关数量**：非零典型相关的数量受限于数据集的维度和互协方差矩阵的秩。问题将这个数量 $m$ 定义为：\n    $$ m = \\min\\left(p, q, \\operatorname{rank}(S_{xy})\\right) $$\n    其中 $p$ 和 $q$ 分别是 $S_{xx}$ 和 $S_{yy}$ 的维度。$S_{xy}$ 的秩在数值上计算为其奇异值中大于阈值 $\\tau$ 的数量。\n\n4.  **计算典型相关**：典型相关是矩阵 $C$ 的奇异值。我们计算 $C$ 的奇异值分解（SVD）。得到的奇异值为非负，并按惯例以降序返回。然后我们选择这些奇异值中的前 $m$ 个作为我们的结果。\n\n这个完整的程序将应用于所提供的四个测试用例中的每一个。最终的数值结果将四舍五入到 6 位小数，并以指定的格式呈现。实现将使用 `numpy.linalg.eigh` 对称矩阵 $S_{xx}$ 和 $S_{yy}$ 进行特征分解，并使用 `numpy.linalg.svd` 计算奇异值以确定秩和找到最终的典型相关。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes canonical correlations for a series of test cases based on a\n    specified algorithm involving pseudo-inverse square-roots and SVD.\n    \"\"\"\n    \n    # Numerical threshold for regularization and rank calculation.\n    TAU = 1e-10\n\n    test_cases = [\n        {\n            \"Sxx\": np.array([\n                [2.0, 0.5, 0.0],\n                [0.5, 1.5, 0.1],\n                [0.0, 0.1, 1.2]\n            ]),\n            \"Syy\": np.array([\n                [1.3, 0.2],\n                [0.2, 1.1]\n            ]),\n            \"Sxy\": np.array([\n                [0.5, 0.1],\n                [0.3, 0.4],\n                [0.2, 0.6]\n            ])\n        },\n        {\n            \"Sxx\": np.array([\n                [1.0, 0.1],\n                [0.1, 1.5]\n            ]),\n            \"Syy\": np.array([\n                [1.2, 0.0, 0.1],\n                [0.0, 1.1, 0.0],\n                [0.1, 0.0, 1.3]\n            ]),\n            \"Sxy\": np.array([\n                [0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0]\n            ])\n        },\n        {\n            \"Sxx\": np.array([\n                [1.0, 0.2],\n                [0.2, 2.0]\n            ]),\n            \"Syy\": np.array([\n                [1.5, 0.1],\n                [0.1, 1.4]\n            ]),\n            \"Sxy\": np.array([\n                [1.0, 2.0],\n                [0.5, 1.0]\n            ])\n        },\n        {\n            \"Sxx\": np.array([\n                [1.0, 0.0, 0.0],\n                [0.0, 0.5, 0.0],\n                [0.0, 0.0, 1e-8]\n            ]),\n            \"Syy\": np.array([\n                [1.2, 0.0, 0.0],\n                [0.0, 0.8, 0.0],\n                [0.0, 0.0, 0.9]\n            ]),\n            \"Sxy\": np.array([\n                [0.3, 0.0, 0.2],\n                [0.1, 0.4, 0.0],\n                [0.5, 0.2, 0.3]\n            ])\n        }\n    ]\n\n    def compute_pseudo_inv_sqrt(S, tau):\n        \"\"\"\n        Computes the pseudo-inverse square-root of a symmetric matrix S\n        using eigendecomposition and a regularization threshold.\n        \"\"\"\n        # eigh is for hermitian (symmetric real) matrices.\n        # It returns eigenvalues in ascending order.\n        eigvals, eigvecs = np.linalg.eigh(S)\n        \n        # Apply threshold to compute inverse square-root of eigenvalues.\n        inv_sqrt_eigvals = np.where(eigvals  tau, 1.0 / np.sqrt(eigvals), 0)\n        \n        # Reconstruct the matrix S^{-1/2} = Q W Q^T\n        S_inv_sqrt = eigvecs @ np.diag(inv_sqrt_eigvals) @ eigvecs.T\n        return S_inv_sqrt\n\n    results = []\n    for case in test_cases:\n        Sxx = case[\"Sxx\"]\n        Syy = case[\"Syy\"]\n        Sxy = case[\"Sxy\"]\n\n        p = Sxx.shape[0]\n        q = Syy.shape[0]\n\n        # Step 1: Compute pseudo-inverse square-roots\n        Sxx_inv_sqrt = compute_pseudo_inv_sqrt(Sxx, TAU)\n        Syy_inv_sqrt = compute_pseudo_inv_sqrt(Syy, TAU)\n\n        # Step 2: Form the whitened cross-covariance matrix C\n        C = Sxx_inv_sqrt @ Sxy @ Syy_inv_sqrt\n\n        # Step 3: Determine the number of correlations to return, m\n        # compute_uv=False is more efficient as we only need the singular values.\n        s_Sxy = np.linalg.svd(Sxy, compute_uv=False)\n        rank_Sxy = np.sum(s_Sxy  TAU)\n        m = min(p, q, rank_Sxy)\n\n        # Step 4: Compute the canonical correlations (singular values of C)\n        # SVD returns singular values in descending order.\n        canonical_correlations = np.linalg.svd(C, compute_uv=False)\n\n        # Select the top m correlations and round them\n        final_values = [round(val, 6) for val in canonical_correlations[:m]]\n        results.append(final_values)\n    \n    # Format the final output string to be without spaces\n    # Example: '[[val1,val2],[val3]]'\n    string_results = [str(res_list).replace(\" \", \"\") for res_list in results]\n    final_output = f\"[{','.join(string_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}