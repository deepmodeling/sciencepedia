{
    "hands_on_practices": [
        {
            "introduction": "A foundational principle of Dynamic Causal Modeling is that the modeled neural system must be stable. This practice will ground you in the core mathematics of the DCM state equation by exploring how an experimental input can influence a system's stability. By calculating the eigenvalues of the effective system matrix, you will determine whether the neural dynamics will settle to a steady state or diverge, a crucial first check before interpreting any connectivity estimates .",
            "id": "4157574",
            "problem": "In Dynamic Causal Modeling (DCM) for effective connectivity, the neuronal state equation linearized around a fixed point can be written as $ \\frac{d\\mathbf{x}}{dt} = \\left( A + \\sum_{j} u_{j} B^{(j)} \\right) \\mathbf{x} + C \\mathbf{u} $, where $A$ encodes context-independent (fixed) effective connectivity, $B^{(j)}$ encodes modulatory effects of the $j$-th input on connections, and $C$ maps exogenous inputs to state perturbations. For stability of the homogeneous linear time-invariant (LTI) system $ \\frac{d\\mathbf{x}}{dt} = M \\mathbf{x} $ with $ M = A + \\bar{u} B^{(1)} $ under a constant scalar input level $ \\bar{u} $, one examines the eigenvalues of $M$. The spectral abscissa is defined as $ \\alpha = \\max_{i} \\Re\\{\\lambda_{i}(M)\\} $, where $ \\lambda_{i}(M) $ are the eigenvalues of $M$. The system is asymptotically stable if and only if $ \\alpha  0 $.\n\nConsider a $2 \\times 2$ DCM with\n$$\nA = \\begin{pmatrix}\n-1.2  0.3 \\\\\n0.1  -0.8\n\\end{pmatrix}, \n\\quad\nB^{(1)} = \\begin{pmatrix}\n-0.5  0.2 \\\\\n0.0  -0.1\n\\end{pmatrix},\n\\quad\n\\bar{u} = 0.5.\n$$\nUsing only the foundational facts stated above, compute the spectral abscissa $ \\alpha $ of $ M = A + \\bar{u} B^{(1)} $. Report $ \\alpha $ in $ \\mathrm{s}^{-1} $. Round your answer to four significant figures.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the principles of linear systems theory as applied to computational neuroscience (specifically, Dynamic Causal Modeling), is well-posed, and objective. All necessary data and definitions are provided for a unique solution.\n\nThe objective is to compute the spectral abscissa $\\alpha$ of the system matrix $M$ for a $2 \\times 2$ Dynamic Causal Model (DCM) under a constant input. The spectral abscissa is defined as the maximum real part of the eigenvalues of $M$.\n\nThe system matrix $M$ is given by the expression $M = A + \\bar{u} B^{(1)}$.\nThe given matrices and scalar are:\n$$ A = \\begin{pmatrix} -1.2  0.3 \\\\ 0.1  -0.8 \\end{pmatrix}, \\quad B^{(1)} = \\begin{pmatrix} -0.5  0.2 \\\\ 0.0  -0.1 \\end{pmatrix}, \\quad \\bar{u} = 0.5 $$\nFirst, we compute the product $\\bar{u} B^{(1)}$:\n$$ \\bar{u} B^{(1)} = 0.5 \\begin{pmatrix} -0.5  0.2 \\\\ 0.0  -0.1 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\times (-0.5)  0.5 \\times 0.2 \\\\ 0.5 \\times 0.0  0.5 \\times (-0.1) \\end{pmatrix} = \\begin{pmatrix} -0.25  0.1 \\\\ 0.0  -0.05 \\end{pmatrix} $$\nNext, we compute the matrix $M$ by adding $A$ and $\\bar{u} B^{(1)}$:\n$$ M = A + \\bar{u} B^{(1)} = \\begin{pmatrix} -1.2  0.3 \\\\ 0.1  -0.8 \\end{pmatrix} + \\begin{pmatrix} -0.25  0.1 \\\\ 0.0  -0.05 \\end{pmatrix} $$\n$$ M = \\begin{pmatrix} -1.2 - 0.25  0.3 + 0.1 \\\\ 0.1 + 0.0  -0.8 - 0.05 \\end{pmatrix} = \\begin{pmatrix} -1.45  0.4 \\\\ 0.1  -0.85 \\end{pmatrix} $$\nThe eigenvalues $\\lambda$ of $M$ are the roots of the characteristic equation $\\det(M - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$ M - \\lambda I = \\begin{pmatrix} -1.45 - \\lambda  0.4 \\\\ 0.1  -0.85 - \\lambda \\end{pmatrix} $$\nThe determinant is:\n$$ \\det(M - \\lambda I) = (-1.45 - \\lambda)(-0.85 - \\lambda) - (0.4)(0.1) = 0 $$\nExpanding the expression:\n$$ (\\lambda + 1.45)(\\lambda + 0.85) - 0.04 = 0 $$\n$$ \\lambda^2 + 0.85\\lambda + 1.45\\lambda + (1.45)(0.85) - 0.04 = 0 $$\n$$ \\lambda^2 + 2.3\\lambda + 1.2325 - 0.04 = 0 $$\n$$ \\lambda^2 + 2.3\\lambda + 1.1925 = 0 $$\nThis is a quadratic equation of the form $a\\lambda^2 + b\\lambda + c = 0$, with $a=1$, $b=2.3$, and $c=1.1925$. We use the quadratic formula to find the eigenvalues $\\lambda$:\n$$ \\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$\nSubstituting the coefficients:\n$$ \\lambda = \\frac{-2.3 \\pm \\sqrt{(2.3)^2 - 4(1)(1.1925)}}{2(1)} $$\n$$ \\lambda = \\frac{-2.3 \\pm \\sqrt{5.29 - 4.77}}{2} $$\n$$ \\lambda = \\frac{-2.3 \\pm \\sqrt{0.52}}{2} $$\nThe discriminant is positive, so the eigenvalues are real. The two eigenvalues are:\n$$ \\lambda_1 = \\frac{-2.3 + \\sqrt{0.52}}{2} \\quad \\text{and} \\quad \\lambda_2 = \\frac{-2.3 - \\sqrt{0.52}}{2} $$\nThe spectral abscissa $\\alpha$ is defined as $\\alpha = \\max_{i} \\Re\\{\\lambda_{i}(M)\\}$. Since the eigenvalues are real, this simplifies to $\\alpha = \\max(\\lambda_1, \\lambda_2)$. The larger of the two eigenvalues is $\\lambda_1$.\n$$ \\alpha = \\lambda_1 = \\frac{-2.3 + \\sqrt{0.52}}{2} $$\nNow, we compute the numerical value:\n$$ \\sqrt{0.52} \\approx 0.721110255 $$\n$$ \\alpha \\approx \\frac{-2.3 + 0.721110255}{2} = \\frac{-1.578889745}{2} \\approx -0.78944487 $$\nThe problem requires the answer to be rounded to four significant figures.\nThe value is $-0.78944487...$. The first four significant figures are $7, 8, 9, 4$. The fifth digit is $4$, which is less than $5$, so we round down (i.e., truncate).\n$$ \\alpha \\approx -0.7894 $$\nThe units of the entries of $A$ and $B^{(j)}$ in DCM are typically $\\mathrm{s}^{-1}$ (or rate constants), thus the eigenvalues also have units of $\\mathrm{s}^{-1}$. The system is asymptotically stable since $\\alpha = -0.7894  0$.",
            "answer": "$$ \\boxed{-0.7894} $$"
        },
        {
            "introduction": "In practice, we never know the one true model of a neural circuit; instead, we compare a set of plausible hypotheses. This exercise introduces a cornerstone of inference in DCM: Bayesian Model Averaging (BMA). You will learn how to synthesize the results from multiple competing models, weighted by their evidence, to produce a single, robust estimate of an effective connectivity parameter that accounts for model uncertainty .",
            "id": "4157694",
            "problem": "A researcher is analyzing effective connectivity between two cortical regions, region $\\mathcal{R}_1$ and region $\\mathcal{R}_2$, using Dynamic Causal Modeling (DCM). The state equation for a bilinear DCM can be written as $\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\sum_{j} u_{j} \\mathbf{B}^{(j)} \\mathbf{x} + \\mathbf{C}\\mathbf{u}$, where $\\mathbf{A}$ encodes baseline effective connectivity (in $\\mathrm{s}^{-1}$). The researcher is interested in the baseline effective connectivity parameter from $\\mathcal{R}_1$ to $\\mathcal{R}_2$, denoted $a_{12}$, which is present in all candidate models. Model comparison has been performed and has yielded a set of three plausible models $\\{M_1, M_2, M_3\\}$ with the following posterior model probabilities and model-conditional posterior expectations for $a_{12}$:\n- $p(M_1 \\mid y) = 0.58$, $\\mathbb{E}[a_{12} \\mid y, M_1] = 0.18$,\n- $p(M_2 \\mid y) = 0.27$, $\\mathbb{E}[a_{12} \\mid y, M_2] = 0.22$,\n- $p(M_3 \\mid y) = 0.15$, $\\mathbb{E}[a_{12} \\mid y, M_3] = -0.04$.\n\nAssume the model-conditional posteriors for $a_{12}$ are approximately Gaussian and that these are the posterior means under each model. Using foundational principles of Bayesian inference, compute the Bayesian Model Averaging (BMA) posterior expectation $\\mathbb{E}[a_{12} \\mid y]$ across models, weighted by their posterior probabilities. Express your final answer in $\\mathrm{s}^{-1}$ and round your answer to four significant figures. Then, in words, interpret whether the averaged connectivity indicates net excitatory or inhibitory influence from $\\mathcal{R}_1$ to $\\mathcal{R}_2$ and comment on the magnitude in the context of typical DCM rate constants. The final numeric answer must be provided as specified above.",
            "solution": "The problem is valid. The premises are scientifically sound, pertaining to standard practices in Dynamic Causal Modeling (DCM) within computational neuroscience. All necessary data are provided, the values are consistent (posterior probabilities sum to $1$), and the question is well-posed, leading to a unique and meaningful solution.\n\nThe problem requires the computation of the Bayesian Model Averaging (BMA) posterior expectation of the effective connectivity parameter $a_{12}$. The fundamental principle of BMA is that the full posterior distribution of a parameter $\\theta$, given data $y$, is a weighted average of the posterior distributions of that parameter under each model $M_i$. The weights are the posterior probabilities of the models themselves. This is expressed as:\n$$p(\\theta \\mid y) = \\sum_{i} p(\\theta \\mid y, M_i) p(M_i \\mid y)$$\nHere, $\\theta$ is the parameter of interest, $a_{12}$, and the sum is over the set of candidate models $\\{M_1, M_2, M_3\\}$.\n\nTo find the posterior expectation of $\\theta$, we apply the law of total expectation. The overall posterior expectation $\\mathbb{E}[\\theta \\mid y]$ is the expectation of the model-conditional expectations $\\mathbb{E}[\\theta \\mid y, M_i]$, weighted by the posterior probabilities of each model $p(M_i \\mid y)$. The formula is:\n$$\\mathbb{E}[\\theta \\mid y] = \\sum_{i} \\mathbb{E}[\\theta \\mid y, M_i] p(M_i \\mid y)$$\nThis formula allows us to synthesize the evidence about the parameter from multiple, competing models into a single, averaged estimate.\n\nIn this problem, the parameter is $\\theta = a_{12}$, and we are given the following values:\n-   Posterior probabilities for models $M_1, M_2, M_3$:\n    -   $p(M_1 \\mid y) = 0.58$\n    -   $p(M_2 \\mid y) = 0.27$\n    -   $p(M_3 \\mid y) = 0.15$\n-   Model-conditional posterior expectations for $a_{12}$:\n    -   $\\mathbb{E}[a_{12} \\mid y, M_1] = 0.18 \\, \\mathrm{s}^{-1}$\n    -   $\\mathbb{E}[a_{12} \\mid y, M_2] = 0.22 \\, \\mathrm{s}^{-1}$\n    -   $\\mathbb{E}[a_{12} \\mid y, M_3] = -0.04 \\, \\mathrm{s}^{-1}$\n\nWe can now substitute these values into the BMA formula for the posterior expectation:\n$$\\mathbb{E}[a_{12} \\mid y] = \\mathbb{E}[a_{12} \\mid y, M_1] p(M_1 \\mid y) + \\mathbb{E}[a_{12} \\mid y, M_2] p(M_2 \\mid y) + \\mathbb{E}[a_{12} \\mid y, M_3] p(M_3 \\mid y)$$\n$$\\mathbb{E}[a_{12} \\mid y] = (0.18) \\times (0.58) + (0.22) \\times (0.27) + (-0.04) \\times (0.15)$$\nPerforming the multiplication for each term:\n$$0.18 \\times 0.58 = 0.1044$$\n$$0.22 \\times 0.27 = 0.0594$$\n$$-0.04 \\times 0.15 = -0.0060$$\nSumming these products gives the BMA posterior expectation:\n$$\\mathbb{E}[a_{12} \\mid y] = 0.1044 + 0.0594 - 0.0060$$\n$$\\mathbb{E}[a_{12} \\mid y] = 0.1638 - 0.0060 = 0.1578$$\nThe units of this parameter are $\\mathrm{s}^{-1}$. The problem requires the answer to be rounded to four significant figures. The calculated value $0.1578$ already has four significant figures.\n\nIn terms of interpretation, the sign of an effective connectivity parameter in DCM indicates the nature of the influence. A positive value signifies an excitatory connection, meaning that an increase in activity in the source region promotes an increase in activity in the target region. A negative value signifies an inhibitory connection. Since the BMA posterior expectation $\\mathbb{E}[a_{12} \\mid y] = 0.1578 \\, \\mathrm{s}^{-1}$ is positive, the averaged evidence across the three models supports a net excitatory influence from region $\\mathcal{R}_1$ to region $\\mathcal{R}_2$.\n\nRegarding the magnitude, intrinsic decay rates in DCM (diagonal elements of the $\\mathbf{A}$ matrix) are typically fixed to $-0.5 \\, \\mathrm{s}^{-1}$ or estimated around that value. First-order rate constants for inter-regional connections, like $a_{12}$, often have posterior expectations in the range of approximately $0.1$ to $1.0 \\, \\mathrm{s}^{-1}$ for moderately strong to strong connections. Thus, a value of $0.1578 \\, \\mathrm{s}^{-1}$ represents a weak to moderate excitatory connection. It is noteworthy that BMA incorporates evidence from all models; the strong excitatory evidence from the more probable models ($M_1$ and $M_2$) is tempered by the inhibitory evidence from the less probable model ($M_3$), resulting in a final estimate that is more conservative than one based solely on the best model, $M_1$.",
            "answer": "$$\\boxed{0.1578}$$"
        },
        {
            "introduction": "The ability to accurately estimate effective connectivity is not just a function of the model, but also of the experiment itself. This advanced practice explores the critical issue of experimental design, specifically how collinearity between task regressors can impact the precision of your parameter estimates. By deriving and calculating the posterior uncertainty, you will gain a quantitative understanding of why thoughtful experimental design is essential for making reliable inferences about neural circuits .",
            "id": "4157703",
            "problem": "You are analyzing a two-region Dynamic Causal Modeling (DCM) for effective connectivity of neural systems, where the baseline directed connection from region $1$ to region $2$ is fixed and known, and there are two modulatory parameters, collected in the vector $\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_{1}  \\beta_{2} \\end{pmatrix}^{\\top}$, that scale the same directed connection under two experimental inputs $u_{1}(t)$ and $u_{2}(t)$. The neuronal and hemodynamic mappings are differentiable, and the Blood Oxygenation Level Dependent (BOLD) observations are corrupted by zero-mean Gaussian noise with variance $\\sigma^{2}$. The prior over $\\boldsymbol{\\beta}$ is Gaussian with zero mean and isotropic covariance $\\tau^{2}\\mathbf{I}$.\n\nConsider a variational Laplace approximation around the posterior mode, where the generative mapping $g(\\boldsymbol{\\beta})$ is linearized and the predicted sensitivity of the BOLD signal to the modulatory parameters is captured by the Jacobian columns $\\mathbf{j}_{1}$ and $\\mathbf{j}_{2}$, corresponding to $\\beta_{1}$ and $\\beta_{2}$, respectively. Suppose that after convolution with the hemodynamic response and sampling over $T$ time points, the following hold:\n- The columns are centered and have equal Euclidean norms $\\|\\mathbf{j}_{1}\\|_{2} = \\|\\mathbf{j}_{2}\\|_{2} = s$ with $s^{2} = 100$.\n- Their inner product is $\\mathbf{j}_{1}^{\\top}\\mathbf{j}_{2} = s^{2}\\rho$.\n- The observation noise variance is $\\sigma^{2} = 1$.\n- The prior variance is $\\tau^{2} = 1$ (so prior precision $\\lambda = \\tau^{-2} = 1$).\n\nYou want to quantify how collinearity between the modulatory regressors impacts the identifiability of $\\beta_{1}$, and how a remedial design change that reduces regressor overlap improves precision.\n\nAssume the current design has high collinearity $\\rho = 0.95$ (due to highly overlapping $u_{1}(t)$ and $u_{2}(t)$ after hemodynamic convolution). You then consider a remedial design that reduces overlap (e.g., by jittering onsets and interleaving null events) to $\\rho' = 0.20$, while keeping $s$ unchanged.\n\nStarting only from Bayesâ€™ rule with Gaussian likelihood and Gaussian prior, and the linearization of $g(\\boldsymbol{\\beta})$ around the posterior mode, derive the approximate posterior covariance of $\\boldsymbol{\\beta}$ and compute the factor by which the posterior standard deviation of $\\beta_{1}$ is reduced, defined as\n$$\nR \\equiv \\frac{\\text{posterior standard deviation of }\\beta_{1}\\text{ before the remedial change}}{\\text{posterior standard deviation of }\\beta_{1}\\text{ after the remedial change}}.\n$$\nReport $R$ as a dimensionless number, rounded to four significant figures.",
            "solution": "The user-provided problem is assessed to be valid as it is scientifically grounded in the principles of Bayesian statistical inference applied to neuroimaging data analysis, is well-posed with a complete and consistent set of givens, and is expressed in objective, formal language. The problem is a standard exercise in calculating posterior uncertainty in a Bayesian linear model framework, which is the result of the linearization inherent in the variational Laplace approximation for Dynamic Causal Modeling (DCM). We can therefore proceed with a formal derivation.\n\nThe posterior probability distribution of the modulatory parameters $\\boldsymbol{\\beta}$ given the Blood Oxygenation Level Dependent (BOLD) data $\\mathbf{y}$ is given by Bayes' rule:\n$$\np(\\boldsymbol{\\beta} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\boldsymbol{\\beta}) p(\\boldsymbol{\\beta})\n$$\nwhere $p(\\mathbf{y} | \\boldsymbol{\\beta})$ is the likelihood and $p(\\boldsymbol{\\beta})$ is the prior.\n\nThe problem states that the BOLD observations are corrupted by zero-mean Gaussian noise with variance $\\sigma^2$. The generative model $g(\\boldsymbol{\\beta})$ is linearized. This results in a likelihood function that is Gaussian in $\\boldsymbol{\\beta}$. The model for the data, after centering, can be written as $\\mathbf{y} = \\mathbf{J}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, where $\\mathbf{J} = \\begin{pmatrix} \\mathbf{j}_{1}  \\mathbf{j}_{2} \\end{pmatrix}$ is the Jacobian matrix (or design matrix) and $\\boldsymbol{\\epsilon}$ is a vector of Gaussian noise, $\\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\sigma^{2}\\mathbf{I})$. The log-likelihood is, up to a constant:\n$$\n\\ln p(\\mathbf{y} | \\boldsymbol{\\beta}) = -\\frac{1}{2\\sigma^2}(\\mathbf{y} - \\mathbf{J}\\boldsymbol{\\beta})^{\\top}(\\mathbf{y} - \\mathbf{J}\\boldsymbol{\\beta})\n$$\nThe prior over $\\boldsymbol{\\beta}$ is a zero-mean Gaussian with covariance $\\tau^2\\mathbf{I}$. The log-prior is, up to a constant:\n$$\n\\ln p(\\boldsymbol{\\beta}) = -\\frac{1}{2\\tau^2}\\boldsymbol{\\beta}^{\\top}\\boldsymbol{\\beta}\n$$\nThe log-posterior is the sum of the log-likelihood and the log-prior:\n$$\n\\ln p(\\boldsymbol{\\beta} | \\mathbf{y}) \\approx -\\frac{1}{2\\sigma^2}(\\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{y}^{\\top}\\mathbf{J}\\boldsymbol{\\beta} + \\boldsymbol{\\beta}^{\\top}\\mathbf{J}^{\\top}\\mathbf{J}\\boldsymbol{\\beta}) - \\frac{1}{2\\tau^2}\\boldsymbol{\\beta}^{\\top}\\boldsymbol{\\beta} + \\text{const.}\n$$\nTo identify the form of the posterior distribution, we collect terms that are quadratic and linear in $\\boldsymbol{\\beta}$:\n$$\n\\ln p(\\boldsymbol{\\beta} | \\mathbf{y}) \\approx -\\frac{1}{2}\\left( \\boldsymbol{\\beta}^{\\top}\\left( \\frac{1}{\\sigma^2}\\mathbf{J}^{\\top}\\mathbf{J} + \\frac{1}{\\tau^2}\\mathbf{I} \\right)\\boldsymbol{\\beta} - 2\\left( \\frac{1}{\\sigma^2}\\mathbf{y}^{\\top}\\mathbf{J} \\right)\\boldsymbol{\\beta} \\right) + \\text{const.}\n$$\nThis is a quadratic form in $\\boldsymbol{\\beta}$, which implies that the posterior distribution $p(\\boldsymbol{\\beta} | \\mathbf{y})$ is a multivariate Gaussian. The inverse of the posterior covariance matrix, known as the posterior precision matrix $\\mathbf{\\Sigma}_{\\boldsymbol{\\beta}}^{-1}$, is the term multiplying the quadratic form $\\frac{1}{2}\\boldsymbol{\\beta}^\\top(\\cdot)\\boldsymbol{\\beta}$:\n$$\n\\mathbf{\\Sigma}_{\\boldsymbol{\\beta}}^{-1} = \\frac{1}{\\sigma^2}\\mathbf{J}^{\\top}\\mathbf{J} + \\frac{1}{\\tau^2}\\mathbf{I}\n$$\nThe posterior covariance is the inverse of this precision matrix:\n$$\n\\mathbf{\\Sigma}_{\\boldsymbol{\\beta}} = \\left( \\frac{1}{\\sigma^2}\\mathbf{J}^{\\top}\\mathbf{J} + \\frac{1}{\\tau^2}\\mathbf{I} \\right)^{-1}\n$$\nThe matrix $\\mathbf{J}^{\\top}\\mathbf{J}$ is constructed from the givens. For the $2$-parameter case with $\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_{1}  \\beta_{2} \\end{pmatrix}^{\\top}$, the Jacobian is $\\mathbf{J} = \\begin{pmatrix} \\mathbf{j}_{1}  \\mathbf{j}_{2} \\end{pmatrix}$.\n$$\n\\mathbf{J}^{\\top}\\mathbf{J} = \\begin{pmatrix} \\mathbf{j}_{1}^{\\top}\\mathbf{j}_{1}  \\mathbf{j}_{1}^{\\top}\\mathbf{j}_{2} \\\\ \\mathbf{j}_{2}^{\\top}\\mathbf{j}_{1}  \\mathbf{j}_{2}^{\\top}\\mathbf{j}_{2} \\end{pmatrix}\n$$\nWe are given $\\|\\mathbf{j}_{1}\\|_{2}^{2} = \\mathbf{j}_{1}^{\\top}\\mathbf{j}_{1} = s^2$ and $\\|\\mathbf{j}_{2}\\|_{2}^{2} = \\mathbf{j}_{2}^{\\top}\\mathbf{j}_{2} = s^2$. We are also given $\\mathbf{j}_{1}^{\\top}\\mathbf{j}_{2} = s^2\\rho$. Thus,\n$$\n\\mathbf{J}^{\\top}\\mathbf{J} = \\begin{pmatrix} s^2  s^2\\rho \\\\ s^2\\rho  s^2 \\end{pmatrix}\n$$\nThe problem specifies the values: $s^2 = 100$, $\\sigma^2 = 1$, and $\\tau^2 = 1$. The prior precision is $\\lambda = \\tau^{-2} = 1$. Substituting these into the expression for the posterior precision matrix:\n$$\n\\mathbf{\\Sigma}_{\\boldsymbol{\\beta}}^{-1} = \\frac{1}{1}\\begin{pmatrix} 100  100\\rho \\\\ 100\\rho  100 \\end{pmatrix} + \\frac{1}{1}\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 101  100\\rho \\\\ 100\\rho  101 \\end{pmatrix}\n$$\nTo find the posterior covariance matrix $\\mathbf{\\Sigma}_{\\boldsymbol{\\beta}}$, we must invert this $2 \\times 2$ matrix. The determinant is:\n$$\n\\det(\\mathbf{\\Sigma}_{\\boldsymbol{\\beta}}^{-1}) = (101)(101) - (100\\rho)(100\\rho) = 101^2 - (100\\rho)^2 = 10201 - 10000\\rho^2\n$$\nThe inverse is:\n$$\n\\mathbf{\\Sigma}_{\\boldsymbol{\\beta}} = \\frac{1}{10201 - 10000\\rho^2} \\begin{pmatrix} 101  -100\\rho \\\\ -100\\rho  101 \\end{pmatrix}\n$$\nThe posterior variance of the parameter $\\beta_{1}$ is the first diagonal element of $\\mathbf{\\Sigma}_{\\boldsymbol{\\beta}}$:\n$$\n\\text{Var}(\\beta_{1}) = (\\mathbf{\\Sigma}_{\\boldsymbol{\\beta}})_{11} = \\frac{101}{10201 - 10000\\rho^2}\n$$\nThe posterior standard deviation is the square root of the variance:\n$$\n\\text{std.dev.}(\\beta_1) = \\sqrt{\\frac{101}{10201 - 10000\\rho^2}}\n$$\nWe now compute this quantity for the two experimental designs.\n\nCase $1$: Before the remedial change (high collinearity).\nHere, $\\rho = 0.95$. The posterior standard deviation of $\\beta_{1}$ is:\n$$\n\\text{std.dev.}_1(\\beta_1) = \\sqrt{\\frac{101}{10201 - 10000(0.95)^2}} = \\sqrt{\\frac{101}{10201 - 10000(0.9025)}} = \\sqrt{\\frac{101}{10201 - 9025}} = \\sqrt{\\frac{101}{1176}}\n$$\n\nCase $2$: After the remedial change (low collinearity).\nHere, $\\rho' = 0.20$. The posterior standard deviation of $\\beta_{1}$ is:\n$$\n\\text{std.dev.}_2(\\beta_1) = \\sqrt{\\frac{101}{10201 - 10000(0.20)^2}} = \\sqrt{\\frac{101}{10201 - 10000(0.04)}} = \\sqrt{\\frac{101}{10201 - 400}} = \\sqrt{\\frac{101}{9801}}\n$$\nThe problem asks for the factor $R$ by which the posterior standard deviation of $\\beta_{1}$ is reduced:\n$$\nR = \\frac{\\text{std.dev.}_1(\\beta_1)}{\\text{std.dev.}_2(\\beta_1)} = \\frac{\\sqrt{\\frac{101}{1176}}}{\\sqrt{\\frac{101}{9801}}} = \\sqrt{\\frac{101}{1176} \\times \\frac{9801}{101}} = \\sqrt{\\frac{9801}{1176}}\n$$\nNow, we compute the numerical value of $R$:\n$$\nR = \\sqrt{8.33418367...} \\approx 2.88690022...\n$$\nRounding to four significant figures, we get:\n$$\nR \\approx 2.887\n$$\nThis demonstrates that reducing the collinearity between regressors from $\\rho = 0.95$ to $\\rho' = 0.20$ results in a nearly threefold reduction in the posterior standard deviation of the parameter $\\beta_1$, thereby substantially improving its identifiability.",
            "answer": "$$\n\\boxed{2.887}\n$$"
        }
    ]
}