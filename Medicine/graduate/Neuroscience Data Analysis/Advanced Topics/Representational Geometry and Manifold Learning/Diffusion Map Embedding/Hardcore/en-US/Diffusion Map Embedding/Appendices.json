{
    "hands_on_practices": [
        {
            "introduction": "To truly understand diffusion map embedding, it is essential to see it in action. This first practice challenges you to implement the core algorithm from first principles on a simple, yet highly illustrative, graph structure. By comparing the resulting diffusion distance to the more conventional shortest-path distance, you will develop a concrete intuition for how the diffusion process is uniquely sensitive to bottlenecks and community structure, a property that makes it invaluable for data analysis .",
            "id": "4155990",
            "problem": "You are to construct a family of finite, weighted graphs representing two internally coherent neuronal populations connected by a single weak bridge and to compare two distance notions: the shortest-path distance and the diffusion distance derived from Diffusion Map Embedding (DME). The task is purely mathematical and algorithmic, independent of any specific neurobiological measurement modality, and focuses on how stochastic diffusion over the graph geometry captures community structure. Begin from fundamental definitions of random walks on graphs and spectral decomposition of linear operators, and do not assume any pre-given formulas for the diffusion distance or any embedding coordinates.\n\nThe family of graphs is defined as follows. For two positive integers $n_A$ and $n_B$, create two disjoint node sets $A$ and $B$ with cardinalities $|A| = n_A$ and $|B| = n_B$. Within each set, connect every pair of distinct nodes with an undirected edge of uniform weight $w_{\\mathrm{in}}  0$. Between the sets, add exactly one undirected bridge edge connecting a designated node $a_0 \\in A$ to a designated node $b_0 \\in B$ with weight $w_{\\mathrm{bridge}}  0$. All other inter-set edges are absent. Denote the resulting symmetric, nonnegative weighted adjacency matrix by $W \\in \\mathbb{R}^{n \\times n}$ with $n = n_A + n_B$.\n\nYour program must, for each specified parameter set in the test suite, construct $W$, compute the following quantities from first principles, and produce the requested outputs:\n\n1. Compute the unweighted shortest-path distance between two pairs of nodes: one pair within $A$ (specifically $a_0$ and $a_1$ for some $a_1 \\in A$ with $a_1 \\neq a_0$), and one pair across the bridge ($a_0$ in $A$ and $b_0$ in $B$). Treat an edge as present whenever its weight is strictly positive, and use an unweighted graph distance (number of hops). Denote these distances by $d_{\\mathrm{sp}}^{\\mathrm{intra}}$ and $d_{\\mathrm{sp}}^{\\mathrm{cross}}$.\n\n2. From the definition of a Markov chain on a finite state space induced by a random walk on the weighted graph, derive the row-stochastic Markov transition matrix $P$ and the associated symmetric normalization that enables spectral decomposition. Use these to derive the diffusion distance at diffusion time $t \\in \\mathbb{N}$ between the same pairs of nodes as in item $1$. Denote these diffusion distances by $D_t^{\\mathrm{intra}}$ and $D_t^{\\mathrm{cross}}$.\n\n3. Compute the spectrum of the symmetric normalization operator corresponding to the random walk and report the second and third largest eigenvalues (excluding the trivial leading eigenvalue that equals one). Denote these by $\\lambda_1$ and $\\lambda_2$ and define the spectral gap $g = \\lambda_1 - \\lambda_2$.\n\nDefine the separation advantage as the real number\n$$\nA = \\left(D_t^{\\mathrm{cross}} - D_t^{\\mathrm{intra}}\\right) - \\left(d_{\\mathrm{sp}}^{\\mathrm{cross}} - d_{\\mathrm{sp}}^{\\mathrm{intra}}\\right).\n$$\nIntuitively, a positive value of $A$ indicates that the diffusion distance separates the across-cluster pair more strongly than the shortest-path distance, demonstrating the sensitivity of diffusion to bottlenecks compared to geodesic paths.\n\nTest Suite. Your program must evaluate the following parameter sets, which together probe typical behavior, an edge case, a boundary condition, and cluster-size imbalance:\n\n- Case $1$ (happy path): $n_A = 6$, $n_B = 6$, $w_{\\mathrm{in}} = 1.0$, $w_{\\mathrm{bridge}} = 0.05$, $t = 2$.\n- Case $2$ (stronger bridge): $n_A = 6$, $n_B = 6$, $w_{\\mathrm{in}} = 1.0$, $w_{\\mathrm{bridge}} = 0.5$, $t = 2$.\n- Case $3$ (large diffusion time boundary): $n_A = 6$, $n_B = 6$, $w_{\\mathrm{in}} = 1.0$, $w_{\\mathrm{bridge}} = 0.05$, $t = 20$.\n- Case $4$ (cluster-size imbalance): $n_A = 3$, $n_B = 9$, $w_{\\mathrm{in}} = 1.0$, $w_{\\mathrm{bridge}} = 0.05$, $t = 2$.\n\nFor each case, choose $a_0$ as the node with index $0$ within $A$, choose $a_1$ as the node with index $1$ within $A$, and choose $b_0$ as the node with index $0$ within $B$. The bridge connects $a_0$ to $b_0$.\n\nFinal Output Format. Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, where each case’s result is itself a comma-separated list $[A,\\lambda_1,g]$ enclosed in square brackets. All floating-point values must be rounded to $6$ decimal places. For example, the output should look like\n$$\n[[A_1,\\lambda_{1,1},g_1],[A_2,\\lambda_{1,2},g_2],[A_3,\\lambda_{1,3},g_3],[A_4,\\lambda_{1,4},g_4]]\n$$\nwith no spaces.",
            "solution": "The problem requires the construction of a specific family of graphs and the comparison of two distance metrics—shortest-path distance and diffusion distance—on these graphs. We will proceed by first formally defining the graph and the distance metrics from first principles, and then outlining the computational steps to find the required quantities.\n\nThe analysis is divided into the following sections:\n1.  Construction of the Weighted Adjacency Matrix $W$.\n2.  Calculation of the Unweighted Shortest-Path Distances $d_{\\mathrm{sp}}$.\n3.  Derivation of the Diffusion Distance $D_t$ from the principles of random walks on graphs.\n4.  Definition of the Spectral Quantities $\\lambda_1$, $\\lambda_2$, and $g$.\n5.  Formulation of the Separation Advantage $A$.\n\n### 1. Graph Construction\n\nThe graph consists of $n = n_A + n_B$ nodes, partitioned into two sets $A$ and $B$ of sizes $n_A$ and $n_B$. We can index the nodes in $A$ as $\\{0, 1, \\dots, n_A-1\\}$ and the nodes in $B$ as $\\{n_A, n_A+1, \\dots, n_A+n_B-1\\}$. The symmetric weighted adjacency matrix $W \\in \\mathbb{R}^{n \\times n}$ is defined as follows:\n-   **Intra-cluster edges:** The nodes within each set form a complete graph (a clique) with edge weight $w_{\\mathrm{in}}$.\n    $$\n    W_{ij} = w_{\\mathrm{in}} \\quad \\text{if } (i, j \\in A \\text{ and } i \\neq j) \\text{ or } (i, j \\in B \\text{ and } i \\neq j)\n    $$\n-   **Inter-cluster bridge:** A single edge of weight $w_{\\mathrm{bridge}}$ connects node $a_0$ (index $0$) in set $A$ to node $b_0$ (index $n_A$) in set $B$.\n    $$\n    W_{0, n_A} = W_{n_A, 0} = w_{\\mathrm{bridge}}\n    $$\n-   All other entries of $W$ are $0$, including the diagonal, $W_{ii} = 0$.\n\n### 2. Shortest-Path Distance ($d_{\\mathrm{sp}}$)\n\nThe shortest-path distance is calculated on the unweighted version of the graph, where an edge exists if its corresponding weight in $W$ is positive. The distance is the minimum number of edges in a path between two nodes.\n\n-   **Intra-cluster distance $d_{\\mathrm{sp}}^{\\mathrm{intra}}$:** This is the distance between nodes $a_0$ (index $0$) and $a_1$ (index $1$). Both nodes are in set $A$. Since all nodes in $A$ form a clique, there is a direct edge between $a_0$ and $a_1$ (with weight $w_{\\mathrm{in}}  0$). Therefore, the shortest-path distance is $1$.\n    $$\n    d_{\\mathrm{sp}}^{\\mathrm{intra}} = 1\n    $$\n-   **Cross-cluster distance $d_{\\mathrm{sp}}^{\\mathrm{cross}}$:** This is the distance between nodes $a_0$ (index $0$) and $b_0$ (index $n_A$). These two nodes are directly connected by the bridge edge (with weight $w_{\\mathrm{bridge}}  0$). Therefore, the shortest-path distance is also $1$.\n    $$\n    d_{\\mathrm{sp}}^{\\mathrm{cross}} = 1\n    $$\nThis result demonstrates that the simple shortest-path metric is insensitive to the community structure of the graph; it treats the weak bridge edge as equivalent to a strong intra-cluster edge.\n\n### 3. Diffusion Distance ($D_t$)\n\nThe diffusion distance is derived from the behavior of a random walk on the weighted graph.\n\n-   **Random Walk and Transition Matrix:** Let $d(i) = \\sum_{j=0}^{n-1} W_{ij}$ be the degree of node $i$, which is the sum of weights of all incident edges. The degree matrix $D$ is a diagonal matrix with $D_{ii} = d(i)$. A random walker at node $i$ moves to an adjacent node $j$ with probability $P_{ij} = W_{ij} / d(i)$. The matrix $P = D^{-1}W$ is the one-step transition matrix of a Markov chain. It is row-stochastic, i.e., $\\sum_j P_{ij} = 1$.\n\n-   **Symmetric Normalization and Spectral Decomposition:** The transition matrix $P$ is generally not symmetric. To utilize the powerful tools of spectral theory for symmetric matrices, we introduce a symmetrically normalized matrix $M$:\n    $$\n    M = D^{1/2} P D^{-1/2} = D^{1/2} (D^{-1}W) D^{-1/2} = D^{-1/2} W D^{-1/2}\n    $$\n    Since $W$ is symmetric and $D$ is diagonal, $M$ is a real symmetric matrix. As such, it admits a full set of real eigenvalues $1 = \\lambda_0 \\ge |\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_{n-1}|$ and a corresponding orthonormal basis of eigenvectors $\\{\\phi_k\\}_{k=0}^{n-1}$, such that $M\\phi_k = \\lambda_k \\phi_k$. The eigenvalues of $M$ are identical to the eigenvalues of $P$. The right eigenvectors of $P$, denoted $\\{\\psi_k\\}$, are related to the eigenvectors of $M$ by the transformation $\\psi_k = D^{-1/2} \\phi_k$.\n\n-   **Diffusion Map Embedding:** The diffusion map embedding at time $t$ maps each node $i$ into a Euclidean space using the eigenvectors and eigenvalues of the transition operator. The embedding of node $i$ is given by the vector:\n    $$\n    \\Psi_t(i) = \\left( \\lambda_1^t \\psi_1(i), \\lambda_2^t \\psi_2(i), \\dots, \\lambda_{n-1}^t \\psi_{n-1}(i) \\right) \\in \\mathbb{R}^{n-1}\n    $$\n    The first eigenvector $\\psi_0$ (corresponding to $\\lambda_0=1$) is omitted as it is constant for a connected graph and carries no geometric information.\n\n-   **Diffusion Distance Definition:** The diffusion distance $D_t(i, j)$ between two nodes $i$ and $j$ is defined as the standard Euclidean distance between their embedded representations in the diffusion space:\n    $$\n    D_t(i, j) = \\left\\| \\Psi_t(i) - \\Psi_t(j) \\right\\|_2\n    $$\n    Squaring this expression yields the formula used for computation:\n    $$\n    D_t(i, j)^2 = \\sum_{k=1}^{n-1} \\left( \\lambda_k^t \\psi_k(i) - \\lambda_k^t \\psi_k(j) \\right)^2 = \\sum_{k=1}^{n-1} \\lambda_k^{2t} \\left( \\psi_k(i) - \\psi_k(j) \\right)^2\n    $$\n    This distance captures the similarity of two nodes based on the evolution of probability distributions starting from them. If two nodes are in the same well-connected cluster, random walks starting from them will have very similar distributions after time $t$, resulting in a small diffusion distance. If they are in different clusters connected by a bottleneck, the distributions will remain distinct for a longer time, resulting in a large diffusion distance.\n\n### 4. Spectral Quantities\n\nThe problem requires reporting the second and third largest eigenvalues of the transition operator, which are obtained from the spectral decomposition of $M$. We denote these as $\\lambda_1$ and $\\lambda_2$. The spectral gap is defined as the difference between these two eigenvalues:\n$$\ng = \\lambda_1 - \\lambda_2\n$$\nA large spectral gap $g$ is a hallmark of a graph with a clear community structure. The eigenvector $\\psi_1$ corresponding to $\\lambda_1$ typically acts as a coarse-grained coordinate that separates the main communities of the graph.\n\n### 5. Separation Advantage\n\nThe separation advantage $A$ is defined to quantify the increased separation of cross-cluster nodes by diffusion distance compared to shortest-path distance.\n$$\nA = \\left(D_t^{\\mathrm{cross}} - D_t^{\\mathrm{intra}}\\right) - \\left(d_{\\mathrm{sp}}^{\\mathrm{cross}} - d_{\\mathrm{sp}}^{\\mathrm{intra}}\\right)\n$$\nAs shown in Section 2, $d_{\\mathrm{sp}}^{\\mathrm{cross}} - d_{\\mathrm{sp}}^{\\mathrm{intra}} = 1 - 1 = 0$. The formula thus simplifies to:\n$$\nA = D_t^{\\mathrm{cross}} - D_t^{\\mathrm{intra}}\n$$\nwhere $D_t^{\\mathrm{cross}} = D_t(a_0, b_0)$ and $D_t^{\\mathrm{intra}} = D_t(a_0, a_1)$. A positive value of $A$ signifies that the diffusion metric successfully identifies the weak bridge as a more significant barrier than the strong intra-cluster connections, a feature missed by the shortest-path metric.\n\n### Summary of the Algorithm\n\nFor each set of parameters $(n_A, n_B, w_{\\mathrm{in}}, w_{\\mathrm{bridge}}, t)$:\n1.  Construct the $n \\times n$ adjacency matrix $W$, where $n = n_A+n_B$.\n2.  Compute the diagonal degree matrix $D$ where $D_{ii} = \\sum_j W_{ij}$.\n3.  Compute the matrix $D^{-1/2}$, which is a diagonal matrix with entries $1 / \\sqrt{d(i)}$.\n4.  Form the symmetric normalized matrix $M = D^{-1/2} W D^{-1/2}$.\n5.  Compute the eigenvalues and eigenvectors of $M$ using a numerical solver. Let the eigenvalues be $\\lambda_k$ (sorted descending) and eigenvectors be $\\phi_k$.\n6.  Extract $\\lambda_1$ and $\\lambda_2$ and calculate the spectral gap $g = \\lambda_1 - \\lambda_2$.\n7.  Transform the eigenvectors of $M$ to the right eigenvectors of $P$ via $\\psi_k = D^{-1/2}\\phi_k$. This is done by matrix multiplication: $\\Psi = D^{-1/2} \\Phi$, where $\\Psi$ and $\\Phi$ are matrices whose columns are the eigenvectors.\n8.  Identify the node indices: $a_0 = 0$, $a_1 = 1$, and $b_0 = n_A$.\n9.  Calculate the squared diffusion distances:\n    $$\n    (D_t^{\\mathrm{intra}})^2 = \\sum_{k=1}^{n-1} \\lambda_k^{2t} (\\psi_k(0) - \\psi_k(1))^2\n    $$\n    $$\n    (D_t^{\\mathrm{cross}})^2 = \\sum_{k=1}^{n-1} \\lambda_k^{2t} (\\psi_k(0) - \\psi_k(n_A))^2\n    $$\n10. Calculate $A = \\sqrt{(D_t^{\\mathrm{cross}})^2} - \\sqrt{(D_t^{\\mathrm{intra}})^2}$.\n11. Collect and format the results $[A, \\lambda_1, g]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite by constructing graphs,\n    computing diffusion distances, spectral properties, and the separation advantage.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\"n_A\": 6, \"n_B\": 6, \"w_in\": 1.0, \"w_bridge\": 0.05, \"t\": 2},\n        # Case 2 (stronger bridge)\n        {\"n_A\": 6, \"n_B\": 6, \"w_in\": 1.0, \"w_bridge\": 0.5, \"t\": 2},\n        # Case 3 (large diffusion time boundary)\n        {\"n_A\": 6, \"n_B\": 6, \"w_in\": 1.0, \"w_bridge\": 0.05, \"t\": 20},\n        # Case 4 (cluster-size imbalance)\n        {\"n_A\": 3, \"n_B\": 9, \"w_in\": 1.0, \"w_bridge\": 0.05, \"t\": 2},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = calculate_metrics(**params)\n        results.append(result)\n\n    # Format the final output string as specified in the problem statement.\n    formatted_results = [f\"[{A:.6f},{l1:.6f},{g:.6f}]\" for A, l1, g in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef calculate_metrics(n_A, n_B, w_in, w_bridge, t):\n    \"\"\"\n    Performs the calculations for a single parameter set.\n    \n    1. Constructs the weighted adjacency matrix W.\n    2. Derives the diffusion distance from first principles via spectral decomposition.\n    3. Computes the required spectral quantities and the separation advantage A.\n    \"\"\"\n    n = n_A + n_B\n    a0_idx, a1_idx, b0_idx = 0, 1, n_A\n\n    # 1. Construct the weighted adjacency matrix W\n    W = np.zeros((n, n), dtype=float)\n    # Intra-cluster connections for cluster A\n    W[:n_A, :n_A] = w_in\n    # Intra-cluster connections for cluster B\n    W[n_A:, n_A:] = w_in\n    # Set diagonal to zero\n    np.fill_diagonal(W, 0)\n    # Add the single bridge edge\n    W[a0_idx, b0_idx] = W[b0_idx, a0_idx] = w_bridge\n\n    # For verification: shortest path distances\n    # d_sp_intra is d(a0, a1) = 1 (direct edge in clique A)\n    # d_sp_cross is d(a0, b0) = 1 (direct bridge edge)\n    # So (d_sp_cross - d_sp_intra) = 0\n    # A = D_t_cross - D_t_intra\n\n    # 2. Derive diffusion operator and its spectrum\n    # Degree matrix D\n    d = np.sum(W, axis=1)\n    # Check for isolated nodes to avoid division by zero\n    if np.any(d == 0):\n        # In this problem setup, the graph is always connected\n        # for positive weights, so this is just a safeguard.\n        raise ValueError(\"Graph contains isolated nodes.\")\n    D_inv_sqrt = np.diag(1.0 / np.sqrt(d))\n\n    # Symmetrically normalized matrix M = D^(-1/2) * W * D^(-1/2)\n    M = D_inv_sqrt @ W @ D_inv_sqrt\n\n    # 3. Spectral decomposition of M\n    # eigh is for symmetric matrices and returns eigenvalues in ascending order\n    # and corresponding eigenvectors as columns.\n    eigenvalues, phi = eigh(M)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    lambdas = eigenvalues[sorted_indices]\n    phi = phi[:, sorted_indices]\n\n    # 4. Compute spectral quantities\n    # The largest eigenvalue lambda_0 is ~1.0\n    # lambda_1 is the second largest, lambda_2 is the third largest.\n    lambda_1 = lambdas[1]\n    lambda_2 = lambdas[2]\n    spectral_gap = lambda_1 - lambda_2\n\n    # 5. Compute right eigenvectors of the transition matrix P\n    # psi = D^(-1/2) * phi\n    psi = D_inv_sqrt @ phi\n\n    # 6. Compute diffusion distances\n    # The sum is over k=1 to n-1\n    k_indices = np.arange(1, n)\n    lambdas_pow_2t = np.power(lambdas[k_indices], 2 * t)\n\n    # D_t_intra = D_t(a0, a1)\n    psi_diff_intra = psi[a0_idx, k_indices] - psi[a1_idx, k_indices]\n    Dt_intra_sq = np.sum(lambdas_pow_2t * (psi_diff_intra ** 2))\n    Dt_intra = np.sqrt(Dt_intra_sq)\n\n    # D_t_cross = D_t(a0, b0)\n    psi_diff_cross = psi[a0_idx, k_indices] - psi[b0_idx, k_indices]\n    Dt_cross_sq = np.sum(lambdas_pow_2t * (psi_diff_cross ** 2))\n    Dt_cross = np.sqrt(Dt_cross_sq)\n    \n    # 7. Compute Separation Advantage A\n    A = Dt_cross - Dt_intra\n    \n    return A, lambda_1, spectral_gap\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from idealized graphs to real-world neural data introduces new challenges, chief among them being the choice of diffusion parameters. The kernel bandwidth $\\epsilon$ and diffusion time $t$ jointly determine the scale of analysis, and a poor choice can either obscure fine details or fail to reveal global structure. This exercise asks you to analyze how to select these parameters, particularly for data with heterogeneous density, by connecting the discrete algorithm back to the governing principles of heat flow on a manifold .",
            "id": "4155965",
            "problem": "You are analyzing a high-dimensional neural population dataset, consisting of $n$ activity vectors $\\{x_i\\}_{i=1}^n$ with $x_i \\in \\mathbb{R}^p$, where the neural states are hypothesized to lie near a smooth $d$-dimensional manifold $\\mathcal{M} \\subset \\mathbb{R}^p$. You plan to use diffusion map embedding to obtain a geometry-aware low-dimensional representation. Let the kernel be defined by $k_\\epsilon(x,y) = \\exp\\!\\left(-\\frac{\\|x-y\\|^2}{4 \\epsilon}\\right)$ and the corresponding degree function by $d_\\epsilon(x) = \\sum_{j=1}^n k_\\epsilon(x, x_j)$. The row-normalized Markov operator associated with a single diffusion step is\n$$\n(P_\\epsilon f)(x_i) = \\sum_{j=1}^n \\frac{k_\\epsilon(x_i, x_j)}{d_\\epsilon(x_i)} f(x_j),\n$$\nand a $t$-step diffusion acts as $P_\\epsilon^t$. You will construct the diffusion map coordinates from the leading nontrivial eigenvalues and eigenvectors of $P_\\epsilon$ raised to the power $t$.\n\nOversmoothing in this context means that the diffusion process connects points across distinct dynamical regimes or behavioral conditions, washing out meaningful structure; undersmoothing means that the diffusion process remains too local and fails to denoise or connect within-condition variability. Your data are sampled with heterogeneous local densities, so neighborhoods based on Euclidean distance vary substantially across $\\mathcal{M}$.\n\nStarting from the fundamental base that heat flow on a manifold is governed by the heat equation $\\partial_\\tau u = \\Delta_{\\mathcal{M}} u$ with semigroup $\\exp(\\tau \\Delta_{\\mathcal{M}})$ and small-time heat kernel that decays approximately like $\\exp\\!\\left(-\\frac{\\|x-y\\|^2}{4 \\tau}\\right)$, analyze how the diffusion time $t$ interacts with the kernel bandwidth $\\epsilon$ in determining the locality of $P_\\epsilon^t$, and choose the best joint selection strategy for $(t,\\epsilon)$ that avoids oversmoothing and undersmoothing under heterogeneous sampling density.\n\nWhich option is most consistent with the governing heat-flow principles on $\\mathcal{M}$, yields a well-controlled locality across the dataset, and is practically implementable for neural data?\n\nA. Fix a global $\\epsilon$ using a rule from kernel density estimation (KDE) based on ambient dimension $p$ (for example, Silverman’s rule-of-thumb) to capture a desired average degree, then choose $t$ by maximizing the spectral gap of $P_\\epsilon$.\n\nB. Use a variable-bandwidth scheme where a local scale $\\sigma(x_i)$ is estimated from $k$-nearest neighbors (kNN), for example $\\sigma(x_i)$ proportional to the $k$-nearest neighbor radius at $x_i$, define a kernel $k_{\\epsilon}(x_i, x_j)=\\exp\\!\\left(-\\frac{\\|x_i-x_j\\|^2}{4\\,\\epsilon\\,\\sigma(x_i)\\sigma(x_j)}\\right)$, select a target effective diffusion length based on a robust statistic of the kNN radii (for example, the median), and choose $(t,\\epsilon)$ so that the corresponding effective diffusion time matches this target while verifying that the retained spectral energy $\\sum_{\\ell=2}^{L} \\lambda_\\ell^{2 t}$ exceeds a threshold.\n\nC. Choose a constant $\\epsilon$ so that the average graph degree exceeds $50$ and increase $t$ until the diffusion map embedding visually stabilizes across resamplings; this avoids undersmoothing by construction and relies on $t$ to prevent oversmoothing.\n\nD. Fix $t=1$ to preserve locality and select $\\epsilon$ by minimizing the reconstruction error of the first few diffusion coordinates under principal component analysis (PCA), thus balancing smoothing against dimensionality reduction quality.\n\nE. Set the anisotropic normalization parameter $\\alpha=1$ to remove sampling density bias and enforce a fixed product $\\tau = t\\,\\epsilon = 1$ across all datasets, so locality is standardized regardless of data geometry.",
            "solution": "The user wants a critical analysis of a problem concerning parameter selection for diffusion map embedding of high-dimensional neural data, specifically addressing heterogeneous sampling density.\n\n### Step 1: Extract Givens\n- **Dataset**: A set of $n$ data points $\\{x_i\\}_{i=1}^n$, where each $x_i \\in \\mathbb{R}^p$.\n- **Manifold Hypothesis**: The points $\\{x_i\\}$ lie near a smooth $d$-dimensional manifold $\\mathcal{M} \\subset \\mathbb{R}^p$.\n- **Kernel Function**: $k_\\epsilon(x,y) = \\exp\\!\\left(-\\frac{\\|x-y\\|^2}{4 \\epsilon}\\right)$.\n- **Degree Function**: $d_\\epsilon(x) = \\sum_{j=1}^n k_\\epsilon(x, x_j)$.\n- **Markov Operator**: $(P_\\epsilon f)(x_i) = \\sum_{j=1}^n \\frac{k_\\epsilon(x_i, x_j)}{d_\\epsilon(x_i)} f(x_j)$.\n- **Multi-step Diffusion**: The operator $P_\\epsilon$ is applied $t$ times, yielding $P_\\epsilon^t$.\n- **Embedding**: Diffusion map coordinates are derived from the leading nontrivial eigenvalues and eigenvectors of $P_\\epsilon^t$.\n- **Problem Context**: The data exhibits heterogeneous local densities.\n- **Problem Objective**: Avoid oversmoothing (connecting distinct regimes) and undersmoothing (failing to connect within-regime variability).\n- **Theoretical Foundation**: The analysis should be based on the principle that the discrete diffusion process approximates the continuous heat flow on the manifold $\\mathcal{M}$, which is governed by the heat equation $\\partial_\\tau u = \\Delta_{\\mathcal{M}} u$. The semigroup is $\\exp(\\tau \\Delta_{\\mathcal{M}})$ and the short-time heat kernel is approximately $\\exp\\!\\left(-\\frac{\\|x-y\\|^2}{4 \\tau}\\right)$.\n- **Question**: Identify the best joint selection strategy for $(t, \\epsilon)$ that is consistent with the heat-flow principle, provides well-controlled locality across the dataset, and is practically implementable.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound, well-posed, and objective.\n- **Scientific Grounding**: The problem correctly formulates the diffusion map algorithm as a discretization of the heat diffusion process on a manifold. The connection between the graph Laplacian (derived from $P_\\epsilon$) and the Laplace-Beltrami operator $\\Delta_{\\mathcal{M}}$ is a fundamental result in manifold learning. The challenge of heterogeneous sampling density is a well-known and critical issue in the practical application of these methods. All mathematical definitions are standard and correct.\n- **Well-Posedness**: The question asks for the \"best\" strategy, but qualifies this by providing clear criteria: consistency with heat-flow principles, control of locality under heterogeneous density, and practical implementability. This transforms the question from a subjective choice into a principled evaluation of methodologies. A unique, meaningful answer in the form of the most appropriate methodology can be determined.\n- **Objectivity**: The problem is stated using precise technical language. The definitions of oversmoothing and undersmoothing are given within the specific context, removing ambiguity.\n\nThe problem statement is valid. It presents a standard, non-trivial challenge in the field of manifold learning and computational neuroscience.\n\n### Step 3: Derivation and Option Analysis\n\nThe core of the problem lies in the connection between the discrete diffusion operator $P_\\epsilon^t$ and the continuous heat semigroup $\\exp(\\tau \\Delta_{\\mathcal{M}})$. The theory of diffusion maps establishes that, under appropriate conditions, the graph Laplacian derived from the kernel matrix approximates the Laplace-Beltrami operator $\\Delta_{\\mathcal{M}}$ on the manifold. Specifically, the operator $P_\\epsilon$ relates to an infinitesimal generator that converges to a multiple of $\\Delta_{\\mathcal{M}}$ as $n \\to \\infty$ and $\\epsilon \\to 0$. The powers of the operator, $P_\\epsilon^t$, then approximate the heat semigroup $\\exp(\\tau \\Delta_{\\mathcal{M}})$, where the effective diffusion time is $\\tau \\propto t\\epsilon$. This time parameter $\\tau$ controls the scale of the diffusion and, therefore, the locality of the resulting embedding.\n\nA critical challenge highlighted is the heterogeneous sampling density. A fixed, global kernel bandwidth $\\epsilon$ fails in this scenario. In a dense region of $\\mathcal{M}$, a fixed $\\epsilon$ will create a highly connected local graph, effectively averaging out fine-scale structure and causing oversmoothing. In a sparse region, the same $\\epsilon$ may be too small to connect a sufficient number of neighbors, leaving points isolated and failing to capture the manifold's continuous structure, thus causing undersmoothing. The heat-flow analogy breaks down because the discrete approximation is not uniform across the manifold.\n\nThe standard and principled solution to this problem is to adapt the kernel bandwidth to the local density. This ensures that the scale of the neighborhood graph is geometrically consistent across the manifold, rather than being determined by the arbitrary sampling density. A common approach is to introduce a local scaling factor, $\\sigma(x_i)$, often related to the distance to the $k$-th nearest neighbor of $x_i$.\n\nWith this foundation, we can evaluate the options.\n\n**A. Fix a global $\\epsilon$ using a rule from kernel density estimation (KDE) based on ambient dimension $p$ (for example, Silverman’s rule-of-thumb) to capture a desired average degree, then choose $t$ by maximizing the spectral gap of $P_\\epsilon$.**\n\nThis strategy is fundamentally flawed. It uses a \"global $\\epsilon$\", which, as established, is the primary cause of problems with heterogeneous density. Rules like Silverman's are designed for density estimation in the ambient space $\\mathbb{R}^p$ and are not suitable for discovering the geometry of a low-dimensional manifold $\\mathcal{M}$, where the intrinsic dimension $d$ is the relevant quantity. Maximizing the spectral gap is a heuristic for determining the number of clusters (eigenvectors to use), not for setting the diffusion time $t$. This approach fails to address the central challenge of the problem.\n**Verdict: Incorrect.**\n\n**B. Use a variable-bandwidth scheme where a local scale $\\sigma(x_i)$ is estimated from $k$-nearest neighbors (kNN), for example $\\sigma(x_i)$ proportional to the $k$-nearest neighbor radius at $x_i$, define a kernel $k_{\\epsilon}(x_i, x_j)=\\exp\\!\\left(-\\frac{\\|x_i-x_j\\|^2}{4\\,\\epsilon\\,\\sigma(x_i)\\sigma(x_j)}\\right)$, select a target effective diffusion length based on a robust statistic of the kNN radii (for example, the median), and choose $(t,\\epsilon)$ so that the corresponding effective diffusion time matches this target while verifying that the retained spectral energy $\\sum_{\\ell=2}^{L} \\lambda_\\ell^{2 t}$ exceeds a threshold.**\n\nThis strategy directly and correctly addresses the problem of heterogeneous density by employing a variable-bandwidth (or anisotropic) kernel. The use of local scales $\\sigma(x_i)$ derived from kNN distances ensures that the notion of \"neighborhood\" is adapted to the local data density. The kernel form $k_{\\epsilon}(x_i, x_j)=\\exp\\!\\left(-\\frac{\\|x_i-x_j\\|^2}{4\\,\\epsilon\\,\\sigma(x_i)\\sigma(x_j)}\\right)$ effectively normalizes pairwise distances by the local geometric scale. The strategy of selecting a target effective diffusion length/time based on a robust data-driven statistic (e.g., median of kNN radii) provides a principled method for setting the overall smoothing level, making it consistent across the dataset. The joint selection of $(t, \\epsilon)$ to achieve this target acknowledges their coupled role in determining the effective diffusion time $\\tau \\propto t\\epsilon$. Finally, checking the spectral energy is a practical step for choosing the embedding dimension $L$. This approach is fully consistent with the heat-flow principles, robust to heterogeneous density, and practically implementable.\n**Verdict: Correct.**\n\n**C. Choose a constant $\\epsilon$ so that the average graph degree exceeds $50$ and increase $t$ until the diffusion map embedding visually stabilizes across resamplings; this avoids undersmoothing by construction and relies on $t$ to prevent oversmoothing.**\n\nThis option suffers from multiple fallacies. It uses a \"constant $\\epsilon$\", which fails for heterogeneous data. Forcing a high average degree (e.g., $50$) will likely cause severe oversmoothing in dense regions. The premise that increasing $t$ *prevents* oversmoothing is incorrect; increasing $t$ increases the effective diffusion time $\\tau \\propto t\\epsilon$, leading to *more* smoothing, not less. Relying on \"visual stabilization\" is a subjective, non-reproducible heuristic, not a principled scientific method.\n**Verdict: Incorrect.**\n\n**D. Fix $t=1$ to preserve locality and select $\\epsilon$ by minimizing the reconstruction error of the first few diffusion coordinates under principal component analysis (PCA), thus balancing smoothing against dimensionality reduction quality.**\n\nThis strategy is misguided. Fixing $t=1$ is a common simplification but discards a useful degree of freedom. More importantly, the criterion for selecting $\\epsilon$ is not well-founded. Diffusion coordinates and principal components capture different aspects of the data geometry (diffusion distances vs. variance). Minimizing PCA reconstruction error on diffusion coordinates is a convoluted objective with no clear connection to the fundamental goal of approximating the manifold's geometry or controlling the diffusion process. It does not directly address the heterogeneous density issue and conflates two distinct dimensionality reduction philosophies.\n**Verdict: Incorrect.**\n\n**E. Set the anisotropic normalization parameter $\\alpha=1$ to remove sampling density bias and enforce a fixed product $\\tau = t\\,\\epsilon = 1$ across all datasets, so locality is standardized regardless of data geometry.**\n\nThis option contains a grain of truth but is ultimately flawed. Using a normalization with parameter $\\alpha=1$ (i.e., defining a new kernel $K'_{ij} = k_\\epsilon(x_i, x_j) / (d_\\epsilon(x_i) d_\\epsilon(x_j))$ before building the transition matrix) is a valid and powerful method to correct for sampling density. However, the problem statement explicitly defines the operator $P_\\epsilon$ which corresponds to a different normalization scheme ($\\alpha=0$ with row-stochasticization). Even if we were to adopt the $\\alpha=1$ scheme, the second part of the statement is incorrect. Enforcing a fixed product $t\\epsilon=1$ for all datasets is an arbitrary and rigid constraint. The appropriate diffusion time $\\tau$ is an intrinsic property of the specific dataset's geometry and scales, and it should be chosen based on the data itself (as in option B), not set to a universal constant. A strategy that is \"standardized regardless of data geometry\" is precisely the wrong approach; the strategy must be adaptive *to* the data geometry.\n**Verdict: Incorrect.**\n\nIn summary, only option B presents a coherent, theoretically justified, and practical strategy for selecting diffusion map parameters in the challenging but common scenario of heterogeneous data density.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "This final practice integrates the preceding concepts into a complete, end-to-end workflow for analyzing neural data. You will simulate datasets with a known latent dimensionality, apply a full diffusion map pipeline, and—most importantly—implement a non-parametric hypothesis test to determine the number of dimensions that are statistically significant. This exercise demonstrates how to elevate diffusion map embedding from a visualization tool to a rigorous method for quantitative scientific inference .",
            "id": "4155964",
            "problem": "You are given a set of simulated neural population activity datasets, each representing a task with a different intrinsic latent complexity. Your goal is to construct a Diffusion Map Embedding (DME) pipeline from first principles, derive a hypothesis test that validates the chosen embedding dimension by comparing eigenvalues of the diffusion operator against a non-geometric null, and implement this pipeline to produce quantifiable results for a provided test suite.\n\nFundamental base and core definitions to use:\n- A dataset is represented as a matrix of neural activity $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of samples and $p$ is the number of neurons.\n- Define a Gaussian kernel on the data by $K_{ij} = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{4 \\epsilon}\\right)$, where $\\epsilon  0$ is a bandwidth parameter and $\\|\\cdot\\|$ is the Euclidean norm.\n- Perform density normalization (anisotropic kernel) using parameter $\\alpha \\in [0,1]$: compute $q_i = \\sum_{j=1}^{n} K_{ij}$, then define $K^{(\\alpha)}_{ij} = \\frac{K_{ij}}{q_i^{\\alpha} q_j^{\\alpha}}$.\n- Construct a row-stochastic Markov transition matrix $P$ by normalizing rows of $K^{(\\alpha)}$: $d_i = \\sum_{j=1}^{n} K^{(\\alpha)}_{ij}$, then $P_{ij} = \\frac{K^{(\\alpha)}_{ij}}{d_i}$. This defines a Markov chain on the dataset.\n- The Diffusion Map Embedding (DME) of diffusion time $t$ is constructed using the eigenvalues $\\{\\lambda_\\ell\\}$ and right eigenvectors $\\{\\psi_\\ell\\}$ of $P$, with effective spectral scaling $\\mu_\\ell = \\lambda_\\ell^t$.\n- The embedding dimension is operationally defined as the number of nontrivial eigenvalues (excluding the trivial largest eigenvalue equal to $1$) that are significantly larger than what would be expected from a non-geometric null distribution of data.\n\nHypothesis test design requirements:\n- Construct a null distribution for nontrivial eigenvalues by destroying local geometric structure while preserving marginal distributions of individual neurons. Let $X^{(b)}$ be a surrogate dataset obtained by independently permuting the entries of each column of $X$, for $b = 1, \\dots, B$, where $B$ is the number of surrogates. This preserves per-neuron empirical distributions but removes multivariate geometry.\n- For each surrogate $X^{(b)}$, construct the same DME operator $P^{(b)}$ and compute its eigenvalues $\\{\\lambda^{(b)}_\\ell\\}$ and effective spectral values $\\mu^{(b)}_\\ell = \\left(\\lambda^{(b)}_\\ell\\right)^t$, for the top ranks $\\ell = 1, \\dots, r_{\\max}$, where $r_{\\max}$ is the maximum rank considered. Exclude the trivial largest eigenvalue at rank $\\ell = 1$ and analyze ranks $\\ell = 2, \\dots, r_{\\max}$.\n- At a fixed significance level $\\alpha_{\\mathrm{sig}} \\in (0,1)$, define rank-specific thresholds $\\tau_\\ell$ as the $(1 - \\alpha_{\\mathrm{sig}})$ quantiles of the null distribution of $\\mu^{(b)}_\\ell$ over $b = 1, \\dots, B$, for each rank $\\ell$. The selected embedding dimension $\\hat{d}$ is the largest integer $k \\in \\{0,1,\\dots,r_{\\max}-1\\}$ such that for all ranks $\\ell = 2, \\dots, k+1$ we have $\\mu_\\ell  \\tau_\\ell$. This yields a conservative, sequential test that counts the number of consecutive significant nontrivial eigenvalues starting from the largest.\n- In all constructions that involve angles, angles must be in radians.\n\nAlgorithmic construction constraints:\n- Use a $k$-nearest neighbor graph to sparsify the kernel $K$, with $k$ chosen appropriately for each test case, and symmetrize by averaging $K$ with its transpose. Set $\\epsilon$ to the median of the squared distances between each point and its $k$ nearest neighbors. Then apply the anisotropic normalization with $\\alpha$.\n- Compute the top $r_{\\max}$ eigenvalues of $P$ and of each surrogate $P^{(b)}$ using a suitable numerical method for sparse matrices. Sort eigenvalues by their real parts in descending order. Use diffusion time $t$ to rescale eigenvalues $\\lambda_\\ell$ into $\\mu_\\ell = \\lambda_\\ell^t$.\n\nTest suite:\nFor each case below, simulate $X$ in a scientifically realistic manner: draw $d_{\\mathrm{true}}$ latent angular variables $\\theta_k \\sim \\mathrm{Uniform}(0, 2\\pi)$, $k = 1, \\dots, d_{\\mathrm{true}}$, compute latent coordinates $z_k = \\sin(\\theta_k)$, form a matrix $Z \\in \\mathbb{R}^{n \\times d_{\\mathrm{true}}}$ with columns $z_k$, map to $p$-dimensional neural activity by $Y = \\tanh(Z W^\\top) + \\eta$, where $W \\in \\mathbb{R}^{d_{\\mathrm{true}} \\times p}$ has independent standard normal entries and $\\eta$ is independent Gaussian noise $\\mathcal{N}(0, \\sigma^2)$ applied elementwise. For the null case, set $Z$ to identically zero and sample $Y$ as independent Gaussian noise with mean zero and variance $\\sigma^2$. Use angles in radians. The kernel bandwidth construction and Markov normalization must follow the above pipeline.\n\nSpecify and use the following parameter sets as the test suite:\n- Case $1$ (low complexity): $n = 300$, $p = 25$, $d_{\\mathrm{true}} = 1$, $\\sigma = 0.05$, $k = 20$, $\\alpha = 1$, $t = 20$, $B = 40$, $r_{\\max} = 6$, seed $= 13$, significance level $\\alpha_{\\mathrm{sig}} = 0.05$.\n- Case $2$ (moderate complexity): $n = 400$, $p = 25$, $d_{\\mathrm{true}} = 2$, $\\sigma = 0.07$, $k = 20$, $\\alpha = 1$, $t = 20$, $B = 40$, $r_{\\max} = 6$, seed $= 17$, significance level $\\alpha_{\\mathrm{sig}} = 0.05$.\n- Case $3$ (higher complexity): $n = 350$, $p = 30$, $d_{\\mathrm{true}} = 3$, $\\sigma = 0.10$, $k = 20$, $\\alpha = 1$, $t = 20$, $B = 35$, $r_{\\max} = 6$, seed $= 19$, significance level $\\alpha_{\\mathrm{sig}} = 0.05$.\n- Case $4$ (null geometry baseline): $n = 300$, $p = 25$, $d_{\\mathrm{true}} = 0$, $\\sigma = 0.10$, $k = 20$, $\\alpha = 1$, $t = 20$, $B = 30$, $r_{\\max} = 6$, seed $= 23$, significance level $\\alpha_{\\mathrm{sig}} = 0.05$.\n\nRequired output:\n- For each test case, compute the selected embedding dimension $\\hat{d}$ using the hypothesis test above. Produce a single line containing a Python list of results, one per test case, where each result is the list $[\\hat{d}, d_{\\mathrm{true}}, \\text{match}]$ with $\\text{match}$ equal to the boolean value of $(\\hat{d} = d_{\\mathrm{true}})$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[ [\\hat{d}_1, d_{\\mathrm{true},1}, \\text{match}_1], [\\hat{d}_2, d_{\\mathrm{true},2}, \\text{match}_2], \\dots ]$). There are no physical units in this problem, and angles must be treated in radians.\n\nImplement all computations in a self-contained program. No user input or external files are permitted. Use only the specified libraries and environment. Ensure scientific realism in the simulation and correctness in the hypothesis testing procedure derived from the core definitions above.",
            "solution": "The problem requires the implementation and validation of a Diffusion Map Embedding (DME) pipeline to estimate the intrinsic dimensionality of simulated neuroscience datasets. This involves data simulation, construction of a diffusion operator, spectral analysis, and a non-parametric hypothesis test to determine the number of significant dimensions. The entire process will be detailed based on the provided first principles.\n\n### 1. Theoretical Framework\n\nThe core idea is to treat a dataset $X \\in \\mathbb{R}^{n \\times p}$, comprising $n$ samples of $p$-dimensional neural activity, as a graph where samples are nodes. The connectivity between nodes is determined by their similarity. This structure is then analyzed through a random walk on the graph, and the spectral properties of the associated transition matrix reveal the underlying geometric structure of the data.\n\n#### 1.1. Data Simulation\n\nFor each test case, we generate a synthetic dataset $X$ with a known latent dimensionality $d_{\\mathrm{true}}$. This is achieved by first sampling $d_{\\mathrm{true}}$ latent angular variables $\\{\\theta_k\\}_{k=1}^{d_{\\mathrm{true}}}$ from a uniform distribution $\\mathrm{Uniform}(0, 2\\pi)$. These are then transformed into latent coordinates $z_k = \\sin(\\theta_k)$, forming a latent data matrix $Z \\in \\mathbb{R}^{n \\times d_{\\mathrm{true}}}$. A linear transformation by a random projection matrix $W \\in \\mathbb{R}^{d_{\\mathrm{true}} \\times p}$ (with entries from $\\mathcal{N}(0,1)$), followed by a non-linear squashing function $\\tanh$, maps the data into the high-dimensional neural space. Finally, additive Gaussian noise $\\eta \\in \\mathbb{R}^{n \\times p}$ with variance $\\sigma^2$ is introduced. The full model is:\n$$X = \\tanh(Z W^\\top) + \\eta$$\nFor the null case where $d_{\\mathrm{true}} = 0$, the matrix $Z$ is identically zero, so $X$ consists only of noise, $X = \\eta$.\n\n#### 1.2. Construction of the Diffusion Operator\n\nThe process of building the diffusion operator $P$ from the raw data $X$ follows several critical steps.\n\n**Step 1: Kernel Sparsification.**\nFirst, we compute the matrix of all pairwise squared Euclidean distances $D_{ij} = \\|x_i - x_j\\|^2$. To focus on local geometry and improve computational efficiency, we sparsify this information. For each data point $x_i$, we identify its $k$ nearest neighbors. The kernel bandwidth parameter, $\\epsilon$, is robustly set to the median of all squared distances from points to their $k$-th nearest neighbors. This adaptive choice of $\\epsilon$ scales with the local density of the data. A sparse kernel matrix is then constructed using the Gaussian kernel, $K_{ij} = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{4 \\epsilon}\\right)$, where $K_{ij}$ is non-zero only if $x_j$ is among the $k$ nearest neighbors of $x_i$. To ensure the resulting operator is related to a symmetric one (which guarantees real eigenvalues), the sparse kernel is symmetrized by averaging: $K \\leftarrow \\frac{1}{2}(K + K^\\top)$.\n\n**Step 2: Anisotropic Density Normalization.**\nThe kernel $K$ is then normalized to account for non-uniform sampling density on the manifold. With the parameter $\\alpha \\in [0, 1]$, we define a density measure for each point $q_i = \\sum_{j=1}^{n} K_{ij}$. The kernel is then re-weighted as:\n$$K^{(\\alpha)}_{ij} = \\frac{K_{ij}}{q_i^{\\alpha} q_j^{\\alpha}}$$\nFor $\\alpha=1$, this normalization makes the operator converge to the Laplace-Beltrami operator on the manifold, which is invariant to the sampling density.\n\n**Step 3: Markov Transition Matrix.**\nFinally, a row-stochastic Markov transition matrix $P$ is constructed by normalizing the rows of $K^{(\\alpha)}$. Letting $d_i = \\sum_{j=1}^{n} K^{(\\alpha)}_{ij}$, the entries of $P$ are given by:\n$$P_{ij} = \\frac{K^{(\\alpha)}_{ij}}{d_i}$$\nThe matrix $P$ describes the probabilities of one-step transitions in a random walk on the data graph.\n\n#### 1.3. Spectral Analysis and Embedding\n\nThe structure of the data manifold is encoded in the spectrum of $P$. We compute the top $r_{\\max}$ eigenvalues $\\{\\lambda_\\ell\\}_{\\ell=1}^{r_{\\max}}$ and corresponding right eigenvectors $\\{\\psi_\\ell\\}_{\\ell=1}^{r_{\\max}}$ of $P$, sorted such that $1 = \\lambda_1 \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_{r_{\\max}}|$. The top eigenvalue $\\lambda_1=1$ is trivial. The subsequent eigenvalues $\\lambda_2, \\lambda_3, \\dots$ quantify the connectivity of the graph along different geometric directions. Diffusion over time $t$ smooths the data and sharpens the spectral gap, which is modeled by scaling the eigenvalues:\n$$\\mu_\\ell = \\lambda_\\ell^t$$\nThe collection of eigenvectors corresponding to the large eigenvalues forms the Diffusion Map Embedding. The number of such significant eigenvalues corresponds to the intrinsic dimensionality of the data.\n\n#### 1.4. Hypothesis Test for Dimensionality Estimation\n\nTo objectively determine the embedding dimension $\\hat{d}$, we must distinguish eigenvalues that reflect true geometric structure from those arising from random noise. A non-parametric hypothesis test is designed for this purpose.\n\n**Null Hypothesis:** The data lacks any coherent low-dimensional geometric structure. The observed eigenvalues are consistent with those from a dataset with similar marginal statistics but no multivariate dependencies.\n\n**Null Distribution Generation:** We create $B$ surrogate datasets $\\{X^{(b)}\\}_{b=1}^B$ by independently permuting the time series of each neuron (i.e., shuffling each column of $X$). This procedure preserves the marginal distribution of each neuron's activity but destroys the specific temporal coordination between neurons that defines the geometric manifold.\n\n**Statistical Test:** For each surrogate dataset $X^{(b)}$, we compute the entire DME pipeline to obtain a set of scaled eigenvalues $\\{\\mu_\\ell^{(b)}\\}_{\\ell=2}^{r_{\\max}}$. For each rank $\\ell$, this yields an empirical null distribution of $B$ eigenvalues. We define a rank-specific significance threshold $\\tau_\\ell$ as the $(1 - \\alpha_{\\mathrm{sig}})$-quantile of the null distribution for that rank.\n\n**Decision Rule:** The estimated dimension $\\hat{d}$ is determined by a sequential test. We count the number of consecutive non-trivial eigenvalues, starting from $\\mu_2$, that exceed their respective thresholds. Formally, $\\hat{d}$ is the largest integer $k \\in \\{0, 1, \\dots, r_{\\max}-1\\}$ for which $\\mu_\\ell  \\tau_\\ell$ for all $\\ell = 2, \\dots, k+1$. If $\\mu_2 \\le \\tau_2$, the estimated dimension is $\\hat{d}=0$, indicating no significant geometric structure was detected.\n\nThis rigorous, data-driven procedure provides a principled way to estimate the intrinsic complexity of the neural population activity.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom scipy.sparse import lil_matrix, csc_matrix\nfrom scipy.sparse.linalg import eigs\n\ndef simulate_data(n, p, d_true, sigma, rng):\n    \"\"\"\n    Simulates neural population activity data based on a latent manifold.\n    \"\"\"\n    if d_true == 0:\n        # Null case: pure noise\n        return rng.normal(0, sigma, size=(n, p))\n\n    # Generate latent variables\n    thetas = rng.uniform(0, 2 * np.pi, size=(n, d_true))\n    Z = np.sin(thetas)\n    \n    # Random projection matrix\n    W = rng.standard_normal(size=(d_true, p))\n    \n    # Generate neural activity with noise\n    Y = np.tanh(Z @ W)\n    noise = rng.normal(0, sigma, size=(n, p))\n    X = Y + noise\n    \n    return X\n\ndef compute_dme_eigenvalues(X, k, alpha, t, r_max):\n    \"\"\"\n    Constructs the DME pipeline and computes the top scaled eigenvalues.\n    \"\"\"\n    n, p = X.shape\n\n    # 1. Compute pairwise distances and find k-NN\n    # Using squared Euclidean distance as per kernel definition\n    sq_dists = cdist(X, X, 'sqeuclidean')\n    \n    # Find k nearest neighbors for each point (excluding self)\n    # Using argpartition for efficiency. The k+1 smallest distances are in the first k+1 columns.\n    neighbor_indices = np.argpartition(sq_dists, kth=k, axis=1)[:, 1:k+1]\n    \n    # 2. Compute epsilon (bandwidth parameter)\n    # Gather all squared distances to the k nearest neighbors\n    nn_sq_dists = sq_dists[np.arange(n)[:, None], neighbor_indices]\n    if nn_sq_dists.size == 0:\n        # Handle case where k=0 or n is small\n        epsilon = 1.0\n    else:\n        epsilon = np.median(nn_sq_dists)\n    \n    # Prevent epsilon from being zero, which would cause division by zero\n    if epsilon == 0:\n        epsilon = 1e-9\n\n    # 3. Construct sparse kernel K\n    K_sparse = lil_matrix((n, n), dtype=np.float64)\n    rows = np.arange(n).repeat(k)\n    cols = neighbor_indices.flatten()\n    vals = sq_dists[rows, cols]\n    \n    K_sparse[rows, cols] = np.exp(-vals / (4 * epsilon))\n    \n    # 4. Symmetrize the kernel\n    K_symm = (K_sparse + K_sparse.T) / 2.0\n    \n    # 5. Anisotropic normalization (alpha=1 for all cases here)\n    q = np.array(K_symm.sum(axis=1)).flatten()\n    q[q == 0] = 1.0 # Avoid division by zero\n    \n    if alpha > 0:\n        inv_q_alpha = np.power(q, -alpha)\n        # Element-wise multiplication with outer product of inv_q_alpha\n        K_alpha = K_symm.multiply(np.outer(inv_q_alpha, inv_q_alpha))\n    else:\n        K_alpha = K_symm\n        \n    # 6. Construct Markov transition matrix P\n    d = np.array(K_alpha.sum(axis=1)).flatten()\n    d[d == 0] = 1.0 # Avoid division by zero\n    inv_d = np.power(d, -1)\n    \n    P = csc_matrix(K_alpha).multiply(inv_d[:, np.newaxis])\n    \n    # 7. Compute eigenvalues\n    try:\n        # Using 'LR' for largest real part. This is more robust for non-symmetric matrices.\n        # Although P should be similar to a symmetric matrix, numerical issues can occur.\n        evals = eigs(P, k=r_max, which='LR', return_eigenvectors=False)\n    except Exception:\n        # If solver fails, return zeros, which will result in d_hat=0\n        return np.zeros(r_max)\n        \n    # Sort eigenvalues by their real part in descending order\n    evals = np.sort(evals.real)[::-1]\n    \n    # 8. Scale eigenvalues by diffusion time t\n    # Handle negative eigenvalues by taking absolute value before power, though for\n    # this construction they should be non-negative.\n    mu = np.power(np.abs(evals), t)\n    \n    return mu\n\ndef solve():\n    test_cases = [\n        # Case 1 (low complexity)\n        {'n': 300, 'p': 25, 'd_true': 1, 'sigma': 0.05, 'k': 20, 'alpha': 1, 't': 20, 'B': 40, 'r_max': 6, 'seed': 13, 'alpha_sig': 0.05},\n        # Case 2 (moderate complexity)\n        {'n': 400, 'p': 25, 'd_true': 2, 'sigma': 0.07, 'k': 20, 'alpha': 1, 't': 20, 'B': 40, 'r_max': 6, 'seed': 17, 'alpha_sig': 0.05},\n        # Case 3 (higher complexity)\n        {'n': 350, 'p': 30, 'd_true': 3, 'sigma': 0.10, 'k': 20, 'alpha': 1, 't': 20, 'B': 35, 'r_max': 6, 'seed': 19, 'alpha_sig': 0.05},\n        # Case 4 (null geometry baseline)\n        {'n': 300, 'p': 25, 'd_true': 0, 'sigma': 0.10, 'k': 20, 'alpha': 1, 't': 20, 'B': 30, 'r_max': 6, 'seed': 23, 'alpha_sig': 0.05}\n    ]\n\n    results = []\n    for params in test_cases:\n        rng = np.random.default_rng(params['seed'])\n        \n        # --- Simulate data and compute eigenvalues for the original dataset ---\n        X = simulate_data(params['n'], params['p'], params['d_true'], params['sigma'], rng)\n        mu_observed = compute_dme_eigenvalues(X, params['k'], params['alpha'], params['t'], params['r_max'])\n        \n        # --- Generate null distribution from surrogate datasets ---\n        surrogate_mus = np.zeros((params['B'], params['r_max']))\n        for b in range(params['B']):\n            X_surr = X.copy()\n            for col in range(params['p']):\n                rng.shuffle(X_surr[:, col])\n            \n            surrogate_mus[b, :] = compute_dme_eigenvalues(X_surr, params['k'], params['alpha'], params['t'], params['r_max'])\n            \n        # --- Perform hypothesis test ---\n        # Analyze non-trivial eigenvalues (from index 1, i.e., rank 2)\n        nontrivial_mus = mu_observed[1:params['r_max']]\n        surrogate_nontrivial_mus = surrogate_mus[:, 1:params['r_max']]\n        \n        # Calculate thresholds from null distribution\n        q = (1 - params['alpha_sig']) * 100\n        thresholds = np.percentile(surrogate_nontrivial_mus, q, axis=0)\n\n        # Apply sequential test to find d_hat\n        d_hat = 0\n        for i in range(len(nontrivial_mus)):\n            if nontrivial_mus[i] > thresholds[i]:\n                d_hat += 1\n            else:\n                break\n        \n        match = (d_hat == params['d_true'])\n        # Store results as a raw list for proper string conversion later\n        results.append([d_hat, params['d_true'], match])\n\n    # Convert boolean to lowercase 'true'/'false' for the final output string\n    result_strings = []\n    for res in results:\n        # Note: str(True) is 'True', we need 'true'\n        res_str = f\"[{res[0]}, {res[1]}, {str(res[2]).lower()}]\"\n        result_strings.append(res_str)\n\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}