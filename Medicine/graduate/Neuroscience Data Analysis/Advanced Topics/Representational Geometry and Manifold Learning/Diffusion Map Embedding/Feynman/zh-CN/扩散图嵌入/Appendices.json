{
    "hands_on_practices": [
        {
            "introduction": "掌握扩散映射的第一步是理解扩散距离为何优于更简单的度量，例如最短路径距离。本练习  使用一个经典的“双团簇”图模型，通过一个弱连接将两个紧密的节点簇连接起来。通过从头开始计算两种距离，您将亲手验证扩散距离如何自然地揭示图的社区结构和瓶颈，而这正是最短路径距离所忽略的，从而为扩散几何的强大功能建立起直观的认识。",
            "id": "4155990",
            "problem": "你需要构建一个有限加权图族，该图族代表由单个弱连接桥相连的两个内部紧密的神经元群，并比较两种距离概念：最短路径距离和源自扩散图嵌入（Diffusion Map Embedding, DME）的扩散距离。此任务纯粹是数学和算法性质的，不依赖于任何特定的神经生物学测量模式，其重点在于图几何结构上的随机扩散如何捕捉社群结构。请从图上随机游走和线性算子谱分解的基本定义出发，不要假设任何预先给定的扩散距离公式或嵌入坐标。\n\n该图族定义如下。对于两个正整数 $n_A$ 和 $n_B$，创建两个不相交的节点集 $A$ 和 $B$，其基数分别为 $|A| = n_A$ 和 $|B| = n_B$。在每个集合内部，用权重为 $w_{\\mathrm{in}} > 0$ 的无向边连接每对不同的节点。在集合之间，添加一条且仅一条无向桥边，连接集合 $A$ 中的指定节点 $a_0 \\in A$ 和集合 $B$ 中的指定节点 $b_0 \\in B$，其权重为 $w_{\\mathrm{bridge}} > 0$。所有其他的集合间边均不存在。将得到的对称非负加权邻接矩阵记为 $W \\in \\mathbb{R}^{n \\times n}$，其中 $n = n_A + n_B$。\n\n对于测试套件中指定的每组参数，你的程序必须构建 $W$，从第一性原理计算以下量，并产生所要求的输出：\n\n1. 计算两对节点之间的无权最短路径距离：一对在 $A$ 内部（具体为 $a_0$ 和某个 $a_1 \\in A$ 且 $a_1 \\neq a_0$），另一对跨越桥（$A$ 中的 $a_0$ 和 $B$ 中的 $b_0$）。只要边的权重为严格正值，就认为该边存在，并使用无权图距离（跳数）。将这些距离记为 $d_{\\mathrm{sp}}^{\\mathrm{intra}}$ 和 $d_{\\mathrm{sp}}^{\\mathrm{cross}}$。\n\n2. 根据加权图上随机游走所导出的有限状态空间上的马尔可夫链的定义，推导出 行随机马尔可夫转移矩阵 $P$ 以及能够进行谱分解的相关对称归一化。使用这些来推导在扩散时间 $t \\in \\mathbb{N}$ 时，与第1项中相同的节点对之间的扩散距离。将这些扩散距离记为 $D_t^{\\mathrm{intra}}$ 和 $D_t^{\\mathrm{cross}}$。\n\n3. 计算与随机游走对应的对称归一化算子的谱，并报告第二大和第三大特征值（不包括等于1的平凡前导特征值）。将它们记为 $\\lambda_1$ 和 $\\lambda_2$，并定义谱间隙 $g = \\lambda_1 - \\lambda_2$。\n\n将分离优势定义为实数\n$$\nA = \\left(D_t^{\\mathrm{cross}} - D_t^{\\mathrm{intra}}\\right) - \\left(d_{\\mathrm{sp}}^{\\mathrm{cross}} - d_{\\mathrm{sp}}^{\\mathrm{intra}}\\right).\n$$\n直观上，一个正的 $A$ 值表示扩散距离比最短路径距离更能有效地区分跨簇节点对，这表明了扩散对瓶颈的敏感性，而测地路径则不具备此特性。\n\n测试套件。你的程序必须评估以下参数集，这些参数集共同探测了典型行为、边缘情况、边界条件和簇大小不平衡的情况：\n\n- 情况1（理想情况）：$n_A = 6$, $n_B = 6$, $w_{\\mathrm{in}} = 1.0$, $w_{\\mathrm{bridge}} = 0.05$, $t = 2$。\n- 情况2（更强的桥）：$n_A = 6$, $n_B = 6$, $w_{\\mathrm{in}} = 1.0$, $w_{\\mathrm{bridge}} = 0.5$, $t = 2$。\n- 情况3（大扩散时间边界）：$n_A = 6$, $n_B = 6$, $w_{\\mathrm{in}} = 1.0$, $w_{\\mathrm{bridge}} = 0.05$, $t = 20$。\n- 情况4（簇大小不平衡）：$n_A = 3$, $n_B = 9$, $w_{\\mathrm{in}} = 1.0$, $w_{\\mathrm{bridge}} = 0.05$, $t = 2$。\n\n对于每种情况，选择 $a_0$ 为 $A$ 中索引为 $0$ 的节点，选择 $a_1$ 为 $A$ 中索引为 $1$ 的节点，选择 $b_0$ 为 $B$ 中索引为 $0$ 的节点。桥连接 $a_0$ 和 $b_0$。\n\n最终输出格式。你的程序应生成单行输出，其中包含四种情况的结果，形式为用方括号括起来的逗号分隔列表，每种情况的结果本身也是一个用方括号括起来的逗号分隔列表 $[A,\\lambda_1,g]$。所有浮点值必须四舍五入到 $6$ 位小数。例如，输出应如下所示\n$$\n[[A_1,\\lambda_{1,1},g_1],[A_2,\\lambda_{1,2},g_2],[A_3,\\lambda_{1,3},g_3],[A_4,\\lambda_{1,4},g_4]]\n$$\n不含空格。",
            "solution": "该问题要求构建一个特定的图族，并在此图上比较两种距离度量——最短路径距离和扩散距离。我们将首先从第一性原理上正式定义图和距离度量，然后概述计算所需量的步骤。\n\n分析分为以下几个部分：\n1.  构建加权邻接矩阵 $W$。\n2.  计算无权最短路径距离 $d_{\\mathrm{sp}}$。\n3.  从图上随机游走的原理推导扩散距离 $D_t$。\n4.  定义谱量 $\\lambda_1$、$\\lambda_2$ 和 $g$。\n5.  构建分离优势 $A$ 的公式。\n\n### 1. 图的构建\n\n该图包含 $n = n_A + n_B$ 个节点，划分为大小分别为 $n_A$ 和 $n_B$ 的两个集合 $A$ 和 $B$。我们可以将 $A$ 中的节点索引为 $\\{0, 1, \\dots, n_A-1\\}$，将 $B$ 中的节点索引为 $\\{n_A, n_A+1, \\dots, n_A+n_B-1\\}$。对称加权邻接矩阵 $W \\in \\mathbb{R}^{n \\times n}$ 定义如下：\n-   **簇内边：** 每个集合内的节点形成一个完全图（一个团），边的权重为 $w_{\\mathrm{in}}$。\n    $$\n    W_{ij} = w_{\\mathrm{in}} \\quad \\text{如果 } (i, j \\in A \\text{ 且 } i \\neq j) \\text{ 或 } (i, j \\in B \\text{ 且 } i \\neq j)\n    $$\n-   **簇间桥：** 一条权重为 $w_{\\mathrm{bridge}}$ 的边连接集合 $A$ 中的节点 $a_0$（索引 $0$）和集合 $B$ 中的节点 $b_0$（索引 $n_A$）。\n    $$\n    W_{0, n_A} = W_{n_A, 0} = w_{\\mathrm{bridge}}\n    $$\n-   $W$ 的所有其他项均为 $0$，包括对角线元素，$W_{ii} = 0$。\n\n### 2. 最短路径距离 ($d_{\\mathrm{sp}}$)\n\n最短路径距离是在图的无权版本上计算的，其中如果 $W$ 中对应的权重为正，则认为边存在。距离是两节点之间路径中的最小边数。\n\n-   **簇内距离 $d_{\\mathrm{sp}}^{\\mathrm{intra}}$：** 这是节点 $a_0$（索引 $0$）和 $a_1$（索引 $1$）之间的距离。两个节点都在集合 $A$ 中。由于 $A$ 中的所有节点构成一个团，因此 $a_0$ 和 $a_1$ 之间有一条直接的边（权重为 $w_{\\mathrm{in}} > 0$）。因此，最短路径距离为 $1$。\n    $$\n    d_{\\mathrm{sp}}^{\\mathrm{intra}} = 1\n    $$\n-   **跨簇距离 $d_{\\mathrm{sp}}^{\\mathrm{cross}}$：** 这是节点 $a_0$（索引 $0$）和 $b_0$（索引 $n_A$）之间的距离。这两个节点由桥边直接相连（权重为 $w_{\\mathrm{bridge}} > 0$）。因此，最短路径距离也为 $1$。\n    $$\n    d_{\\mathrm{sp}}^{\\mathrm{cross}} = 1\n    $$\n这个结果表明，简单的最短路径度量对图的社群结构不敏感；它将弱桥边与强簇内边等同看待。\n\n### 3. 扩散距离 ($D_t$)\n\n扩散距离源于在加权图上随机游走的行为。\n\n-   **随机游走与转移矩阵：** 令 $d(i) = \\sum_{j=0}^{n-1} W_{ij}$ 为节点 $i$ 的度，即所有入射边权重之和。度矩阵 $D$ 是一个对角矩阵，其对角线元素为 $D_{ii} = d(i)$。在节点 $i$ 的随机游走者以概率 $P_{ij} = W_{ij} / d(i)$ 移动到相邻节点 $j$。矩阵 $P = D^{-1}W$ 是马尔可夫链的单步转移矩阵。它是行随机的，即 $\\sum_j P_{ij} = 1$。\n\n-   **对称归一化与谱分解：** 转移矩阵 $P$ 通常不是对称的。为了利用对称矩阵强大的谱论工具，我们引入一个对称归一化矩阵 $M$：\n    $$\n    M = D^{1/2} P D^{-1/2} = D^{1/2} (D^{-1}W) D^{-1/2} = D^{-1/2} W D^{-1/2}\n    $$\n    由于 $W$ 是对称的且 $D$ 是对角的，所以 $M$ 是一个实对称矩阵。因此，它拥有一整套实特征值 $1 = \\lambda_0 \\ge |\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_{n-1}|$ 和一个对应的正交归一的特征向量基 $\\{\\phi_k\\}_{k=0}^{n-1}$，使得 $M\\phi_k = \\lambda_k \\phi_k$。$M$ 的特征值与 $P$ 的特征值相同。$P$ 的右特征向量记为 $\\{\\psi_k\\}$，与 $M$ 的特征向量通过变换 $\\psi_k = D^{-1/2} \\phi_k$ 相关联。\n\n-   **扩散图嵌入：** 在时间 $t$ 的扩散图嵌入使用转移算子的特征向量和特征值将每个节点 $i$ 映射到一个欧几里得空间。节点 $i$ 的嵌入由以下向量给出：\n    $$\n    \\Psi_t(i) = \\left( \\lambda_1^t \\psi_1(i), \\lambda_2^t \\psi_2(i), \\dots, \\lambda_{n-1}^t \\psi_{n-1}(i) \\right) \\in \\mathbb{R}^{n-1}\n    $$\n    第一个特征向量 $\\psi_0$（对应于 $\\lambda_0=1$）被省略，因为它对于连通图是常数且不携带几何信息。\n\n-   **扩散距离定义：** 两个节点 $i$ 和 $j$ 之间的扩散距离 $D_t(i, j)$ 定义为它们在扩散空间中嵌入表示之间的标准欧几里得距离：\n    $$\n    D_t(i, j) = \\left\\| \\Psi_t(i) - \\Psi_t(j) \\right\\|_2\n    $$\n    对此表达式进行平方，得到用于计算的公式：\n    $$\n    D_t(i, j)^2 = \\sum_{k=1}^{n-1} \\left( \\lambda_k^t \\psi_k(i) - \\lambda_k^t \\psi_k(j) \\right)^2 = \\sum_{k=1}^{n-1} \\lambda_k^{2t} \\left( \\psi_k(i) - \\psi_k(j) \\right)^2\n    $$\n    这个距离捕捉了基于从两个节点开始的概率分布演化的相似性。如果两个节点位于同一个连接良好的簇中，从它们开始的随机游走在时间 $t$ 后将具有非常相似的分布，从而导致较小的扩散距离。如果它们位于由瓶颈连接的不同簇中，这些分布将在更长的时间内保持不同，从而导致较大的扩散距离。\n\n### 4. 谱量\n\n问题要求报告转移算子的第二大和第三大特征值，这些值通过 $M$ 的谱分解获得。我们将它们记为 $\\lambda_1$ 和 $\\lambda_2$。谱间隙定义为这两个特征值之差：\n$$\ng = \\lambda_1 - \\lambda_2\n$$\n一个大的谱间隙 $g$ 是具有清晰社群结构的图的一个标志。与 $\\lambda_1$ 对应的特征向量 $\\psi_1$ 通常作为一个粗粒度坐标，用于分离图的主要社群。\n\n### 5. 分离优势\n\n分离优势 $A$ 的定义旨在量化扩散距离相比于最短路径距离对跨簇节点增加的分离度。\n$$\nA = \\left(D_t^{\\mathrm{cross}} - D_t^{\\mathrm{intra}}\\right) - \\left(d_{\\mathrm{sp}}^{\\mathrm{cross}} - d_{\\mathrm{sp}}^{\\mathrm{intra}}\\right)\n$$\n如第2节所示，$d_{\\mathrm{sp}}^{\\mathrm{cross}} - d_{\\mathrm{sp}}^{\\mathrm{intra}} = 1 - 1 = 0$。因此，公式简化为：\n$$\nA = D_t^{\\mathrm{cross}} - D_t^{\\mathrm{intra}}\n$$\n其中 $D_t^{\\mathrm{cross}} = D_t(a_0, b_0)$ 且 $D_t^{\\mathrm{intra}} = D_t(a_0, a_1)$。一个正的 $A$ 值表明扩散度量成功地将弱桥识别为一个比强簇内连接更显著的障碍，这是最短路径度量所忽略的特征。\n\n### 算法摘要\n\n对于每组参数 $(n_A, n_B, w_{\\mathrm{in}}, w_{\\mathrm{bridge}}, t)$:\n1.  构建 $n \\times n$ 的邻接矩阵 $W$，其中 $n = n_A+n_B$。\n2.  计算对角度矩阵 $D$，其中 $D_{ii} = \\sum_j W_{ij}$。\n3.  计算矩阵 $D^{-1/2}$，它是一个对角矩阵，其元素为 $1 / \\sqrt{d(i)}$。\n4.  形成对称归一化矩阵 $M = D^{-1/2} W D^{-1/2}$。\n5.  使用数值求解器计算 $M$ 的特征值和特征向量。设特征值为 $\\lambda_k$（降序排列），特征向量为 $\\phi_k$。\n6.  提取 $\\lambda_1$ 和 $\\lambda_2$，并计算谱间隙 $g = \\lambda_1 - \\lambda_2$。\n7.  通过 $\\psi_k = D^{-1/2}\\phi_k$ 将 $M$ 的特征向量转换为 $P$ 的右特征向量。这通过矩阵乘法完成：$\\Psi = D^{-1/2} \\Phi$，其中 $\\Psi$ 和 $\\Phi$ 是列为特征向量的矩阵。\n8.  确定节点索引：$a_0 = 0$，$a_1 = 1$，以及 $b_0 = n_A$。\n9.  计算扩散距离的平方：\n    $$\n    (D_t^{\\mathrm{intra}})^2 = \\sum_{k=1}^{n-1} \\lambda_k^{2t} (\\psi_k(0) - \\psi_k(1))^2\n    $$\n    $$\n    (D_t^{\\mathrm{cross}})^2 = \\sum_{k=1}^{n-1} \\lambda_k^{2t} (\\psi_k(0) - \\psi_k(n_A))^2\n    $$\n10. 计算 $A = \\sqrt{(D_t^{\\mathrm{cross}})^2} - \\sqrt{(D_t^{\\mathrm{intra}})^2}$。\n11. 收集并格式化结果 $[A, \\lambda_1, g]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite by constructing graphs,\n    computing diffusion distances, spectral properties, and the separation advantage.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\"n_A\": 6, \"n_B\": 6, \"w_in\": 1.0, \"w_bridge\": 0.05, \"t\": 2},\n        # Case 2 (stronger bridge)\n        {\"n_A\": 6, \"n_B\": 6, \"w_in\": 1.0, \"w_bridge\": 0.5, \"t\": 2},\n        # Case 3 (large diffusion time boundary)\n        {\"n_A\": 6, \"n_B\": 6, \"w_in\": 1.0, \"w_bridge\": 0.05, \"t\": 20},\n        # Case 4 (cluster-size imbalance)\n        {\"n_A\": 3, \"n_B\": 9, \"w_in\": 1.0, \"w_bridge\": 0.05, \"t\": 2},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = calculate_metrics(**params)\n        results.append(result)\n\n    # Format the final output string as specified in the problem statement.\n    formatted_results = [f\"[{A:.6f},{l1:.6f},{g:.6f}]\" for A, l1, g in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef calculate_metrics(n_A, n_B, w_in, w_bridge, t):\n    \"\"\"\n    Performs the calculations for a single parameter set.\n    \n    1. Constructs the weighted adjacency matrix W.\n    2. Derives the diffusion distance from first principles via spectral decomposition.\n    3. Computes the required spectral quantities and the separation advantage A.\n    \"\"\"\n    n = n_A + n_B\n    a0_idx, a1_idx, b0_idx = 0, 1, n_A\n\n    # 1. Construct the weighted adjacency matrix W\n    W = np.zeros((n, n), dtype=float)\n    # Intra-cluster connections for cluster A\n    W[:n_A, :n_A] = w_in\n    # Intra-cluster connections for cluster B\n    W[n_A:, n_A:] = w_in\n    # Set diagonal to zero\n    np.fill_diagonal(W, 0)\n    # Add the single bridge edge\n    W[a0_idx, b0_idx] = W[b0_idx, a0_idx] = w_bridge\n\n    # For verification: shortest path distances\n    # d_sp_intra is d(a0, a1) = 1 (direct edge in clique A)\n    # d_sp_cross is d(a0, b0) = 1 (direct bridge edge)\n    # So (d_sp_cross - d_sp_intra) = 0\n    # A = D_t_cross - D_t_intra\n\n    # 2. Derive diffusion operator and its spectrum\n    # Degree matrix D\n    d = np.sum(W, axis=1)\n    # Check for isolated nodes to avoid division by zero\n    if np.any(d == 0):\n        # In this problem setup, the graph is always connected\n        # for positive weights, so this is just a safeguard.\n        raise ValueError(\"Graph contains isolated nodes.\")\n    D_inv_sqrt = np.diag(1.0 / np.sqrt(d))\n\n    # Symmetrically normalized matrix M = D^(-1/2) * W * D^(-1/2)\n    M = D_inv_sqrt @ W @ D_inv_sqrt\n\n    # 3. Spectral decomposition of M\n    # eigh is for symmetric matrices and returns eigenvalues in ascending order\n    # and corresponding eigenvectors as columns.\n    eigenvalues, phi = eigh(M)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    lambdas = eigenvalues[sorted_indices]\n    phi = phi[:, sorted_indices]\n\n    # 4. Compute spectral quantities\n    # The largest eigenvalue lambda_0 is ~1.0\n    # lambda_1 is the second largest, lambda_2 is the third largest.\n    lambda_1 = lambdas[1]\n    lambda_2 = lambdas[2]\n    spectral_gap = lambda_1 - lambda_2\n\n    # 5. Compute right eigenvectors of the transition matrix P\n    # psi = D^(-1/2) * phi\n    psi = D_inv_sqrt @ phi\n\n    # 6. Compute diffusion distances\n    # The sum is over k=1 to n-1\n    k_indices = np.arange(1, n)\n    lambdas_pow_2t = np.power(lambdas[k_indices], 2 * t)\n\n    # D_t_intra = D_t(a0, a1)\n    psi_diff_intra = psi[a0_idx, k_indices] - psi[a1_idx, k_indices]\n    Dt_intra_sq = np.sum(lambdas_pow_2t * (psi_diff_intra ** 2))\n    Dt_intra = np.sqrt(Dt_intra_sq)\n\n    # D_t_cross = D_t(a0, b0)\n    psi_diff_cross = psi[a0_idx, k_indices] - psi[b0_idx, k_indices]\n    Dt_cross_sq = np.sum(lambdas_pow_2t * (psi_diff_cross ** 2))\n    Dt_cross = np.sqrt(Dt_cross_sq)\n    \n    # 7. Compute Separation Advantage A\n    A = Dt_cross - Dt_intra\n    \n    return A, lambda_1, spectral_gap\n\nsolve()\n```"
        },
        {
            "introduction": "从理想化的模型转向真实数据分析时，我们必须面对各种实际挑战。一个有意义的扩散过程要求底层的图是连通的，否则马尔可夫链将不具备遍历性，导致谱分析失效。本练习  聚焦于一个核心的实践问题：如何诊断并修复因参数选择不当（如核带宽 $\\epsilon$ 或邻域大小 $k$）而导致的图连通性问题。您的任务是分析并识别出确保扩散过程稳健性的有效策略。",
            "id": "4155998",
            "problem": "您正在分析从 $N$ 个试次中收集的试次平均神经群体活动，每个试次表示为一个向量 $x_i \\in \\mathbb{R}^d$，其中 $i \\in \\{1,\\dots,N\\}$ 且 $d \\gg 1$。您计划构建一个扩散图（Diffusion Map, DM）嵌入来揭示潜在结构，使用一个由高斯亲和度构建的加权图，并可选择性地限制为 $k$-最近邻（k-NN）图。您构建亲和矩阵 $W$，其元素为 $W_{ij} = \\exp\\!\\big(-\\|x_i - x_j\\|_2^2 / \\epsilon\\big)$，条件是 $j$ 是 $i$ 的 $k$ 个最近邻之一（否则为零，并进行对称化处理），以及行随机马尔可夫转移矩阵 $P = D^{-1} W$，其中 $D$ 是对角度矩阵。在采用小带宽 $ \\epsilon $ 和小 $ k $ 的探索性分析中，您注意到主要的扩散图坐标存在不稳定性，并怀疑图可能是不连通或近乎不连通的，这破坏了遍历性并影响了谱收敛。\n\n根据基本原理，回顾一下：(i) 一个马尔可夫链是遍历的，如果它是不可约且非周期的；(ii) 底层图的连通性对应于 $P$ 的不可约性；以及 (iii) $P$ 及相关图拉普拉斯算子的谱包含了连通分量和亚稳态的信息。您希望选择 $\\epsilon$ 和 $k$，使得该链是遍历的，同时不破坏编码神经相似性的局部几何结构。\n\n选择所有正确描述了检测导致不连通或近乎不连通图的参数区域的策略，以及确保遍历性同时保留局部几何结构的补救措施的选项。\n\nA. 通过计算对称图拉普拉斯算子 $L_{\\mathrm{sym}}$ 的特征值 $0$ 的重数，或等效地计算 $P$ 的特征值 $1$ 的重数来检测不连通性；如果重数超过 $1$，则图是不连通的。补救措施是采用具有局部尺度的可变带宽（自调节）核，并使用相互 $k$-NN 图，然后选择能产生一个连通分量的最小 $k$ 值，以在恢复连通性的同时保持局部性。\n\nB. 通过在 $\\epsilon$ 变化时仅监测平均度 $\\bar{d}$ 来检测不连通性；如果 $\\bar{d}$ 超过 $\\sqrt{N}$，则图必然是连通的。补救措施是将所有距离全局缩放至单位方差，这在保证遍历性的同时保留了局部邻域。\n\nC. 通过运行广度优先搜索来计算在一系列 $k$ 值下 $k$-NN 图的连通分量数量来检测不连通性；补救措施是将 $k$ 增加到使图连通的最小值，然后使用惰性随机游走 $P_{\\ell} = \\tfrac{1}{2}(I + P)$ 来避免周期性，而不改变局部边结构。\n\nD. 通过检查 $P$ 的最大特征值是否严格小于 $1$ 来检测不连通性；如果它等于 $1$，则图必然是不连通的。补救措施是重新归一化 $W$ 的行，使其和为 $1$，这确保了不可约性。\n\nE. 通过检查 $P$ 的谱中是否存在多个非常接近 $1$ 的特征值（小谱隙）来检测近乎不连通性，这表明存在亚稳态的近乎不连通集合；补救措施是添加一个小的瞬移项 $P_{\\tau} = (1 - \\tau) P + \\tau \\mathbf{1} \\pi^{\\top}$，其中 $\\pi$ 是一个严格正分布且 $0  \\tau \\ll 1$，以强制实现不可约性，同时保持由局部转移编码的主导几何结构。",
            "solution": "用户提出了一个关于在高维数据上实际应用扩散图（DM）的问题，特别关注图的连通性问题及其对最终嵌入结果的影响。问题的核心是识别出用于检测和补救不连通或近乎不连通图的有效策略，这种情况可能由核带宽 $\\epsilon$ 和邻域大小 $k$ 的选择引起。一个不连通或近乎不连通的图会导致一个非遍历或混合非常慢的马尔可夫链，这反过来又会使扩散图提供的谱嵌入不稳定。\n\n### 问题陈述的验证\n\n**第一步：提取已知信息**\n- **数据**：一个包含 $N$ 个数据点的集合 $\\{x_i\\}_{i=1}^N$，其中每个 $x_i \\in \\mathbb{R}^d$ 且 $d \\gg 1$。\n- **图的构建**：构建一个加权图。\n- **亲和矩阵（$W$）**：元素为 $W_{ij} = \\exp(-\\|x_i - x_j\\|_2^2 / \\epsilon)$。该图可选择性地为 $k$-最近邻（k-NN）图，意味着只有当 $j$ 是 $i$ 的 $k$-NN 时 $W_{ij}$ 才非零。得到的矩阵经过对称化处理。\n- **马尔可夫转移矩阵（$P$）**：$P = D^{-1} W$，其中 $D$ 是对角度矩阵，其对角元素为 $D_{ii} = \\sum_j W_{ij}$。\n- **观察到的问题**：对于小的 $\\epsilon$ 和小的 $k$，主要的扩散图坐标不稳定。\n- **假设的原因**：图是不连通或近乎不连通的，破坏了遍历性。\n- **陈述的原理**：\n    1. 遍历性 = 不可约性 + 非周期性。\n    2. 图的连通性 $\\iff$ $P$ 的不可约性。\n    3. $P$ 和拉普拉斯算子的谱编码了连通性和亚稳态信息。\n- **目标**：找到一种选择 $\\epsilon$ 和 $k$ 的方法，以确保遍历性，同时保留局部几何结构。\n\n**第二步：使用提取的已知信息进行验证**\n问题陈述在科学上和数学上是合理的。\n- **科学基础**：对扩散图的描述，包括使用高斯核构建亲和矩阵 $W$、可选的 $k$-NN 稀疏化以及通过行随机归一化得到 $P$，是流形学习中的标准方法。图的连通性、相关马尔可夫链的不可约性以及图拉普拉斯算子和转移矩阵的特征值之间的联系是谱图理论的基本结果。这些方法在神经科学中被广泛用于分析神经群体数据。\n- **良置性**：问题是良置的。它要求在一组选项中，根据既定原则识别出正确的程序。通过对照这些原则评估每个选项，可以得出一个确定的答案。\n- **客观性**：问题以精确、客观和技术性的语言陈述，没有歧义或主观性陈述。\n\n**第三步：结论与行动**\n问题陈述是**有效**的。该设定是应用流形学习技术时遇到的一个真实世界挑战的典型代表。我将对各个选项进行完整的解答和分析。\n\n### 解答与选项分析\n\n问题在于识别出在构建扩散图时，检测和补救图的不连通（缺乏不可约性）或近乎不连通（亚稳态）的正确策略。需要一个遍历的马尔可夫链，以使平稳分布和扩散特征向量有良好定义且稳定。遍历性要求链是不可约和非周期的。对于在无向图（由“对称化”的亲和矩阵所暗示）上的随机游走，不可约性等同于图是连通的。\n\n我们来分析行随机矩阵 $P$ 和对称归一化拉普拉斯算子 $L_{\\mathrm{sym}}$ 之间的关系。矩阵 $P = D^{-1}W$ 通常不是对称的。然而，它与对称矩阵 $S = D^{-1/2}WD^{-1/2}$ 相似，因为 $P = D^{1/2} S D^{-1/2}$。因此，$P$ 和 $S$ 拥有相同的特征值。对称归一化拉普拉斯算子定义为 $L_{\\mathrm{sym}} = I - S = I - D^{-1/2}WD^{-1/2}$。因此，$P$ 的特征值 $\\lambda_i(P)$ 和 $L_{\\mathrm{sym}}$ 的特征值 $\\mu_i(L_{\\mathrm{sym}})$ 通过 $\\mu_i(L_{\\mathrm{sym}}) = 1 - \\lambda_i(P)$ 相关联。\n\n谱图理论的一个基本结果表明，图中连通分量的数量等于 $L_{\\mathrm{sym}}$ 的特征值 $0$ 的重数。等价地，这也是 $P$ 的特征值 $1$ 的重数。由于 $P$ 是一个随机矩阵，其最大特征值总是 $1$。当且仅当特征值 $1$ 的重数恰好为一时，图是连通的。\n\n**选项 A：通过计算对称图拉普拉斯算子 $L_{\\mathrm{sym}}$ 的特征值 $0$ 的重数，或等效地计算 $P$ 的特征值 $1$ 的重数来检测不连通性；如果重数超过 $1$，则图是不连通的。补救措施是采用具有局部尺度的可变带宽（自调节）核，并使用相互 $k$-NN 图，然后选择能产生一个连通分量的最小 $k$ 值，以在恢复连通性的同时保持局部性。**\n\n- **检测策略**：这正是确定连通分量数量的标准谱方法。如上所述，$L_{\\mathrm{sym}}$ 的特征值 $\\mu=0$（或 $P$ 的特征值 $\\lambda=1$）的重数计算了连通分量的数量。重数大于 $1$ 表示图不连通。这是**正确**的。\n- **补救策略**：使用可变带宽核（其中 $\\epsilon$ 在局部进行调整，例如，$\\epsilon_i$ 根据点 $x_i$ 到其第 $m$ 个邻居的距离设定）是处理非均匀数据密度（这是导致不连通的常见原因）的一种成熟技术。相互 $k$-NN 图（其中一条边 $(i,j)$ 仅在 $i$ 是 $j$ 的 $k$-NN 且 $j$ 是 $i$ 的 $k$-NN 时存在）可以创建更清晰、更有意义的图结构，尽管它可能更稀疏。关键的是，迭代增加 $k$ 直到图只有一个连通分量的策略，是强制实现连通性同时保持图尽可能稀疏和局部的一种直接且标准的启发式方法。这些技术的组合构成了一个可靠且精密的补救措施。这是**正确**的。\n- **结论**：选项 A 是**正确**的。\n\n**选项 B：通过在 $\\epsilon$ 变化时仅监测平均度 $\\bar{d}$ 来检测不连通性；如果 $\\bar{d}$ 超过 $\\sqrt{N}$，则图必然是连通的。补救措施是将所有距离全局缩放至单位方差，这在保证遍历性的同时保留了局部邻域。**\n\n- **检测策略**：这个陈述是有缺陷的。虽然在随机图论中（例如 Erdős–Rényi 图），足够高的平均度使得连通性很有可能，但这并非确定性的保证，特别是对于由真实数据构建的几何图。一个数据集可能由两个非常密集但相距很远的簇组成。平均度 $\\bar{d}$ 可能非常大（轻易超过 $\\sqrt{N}$），但图仍然不连通。因此，仅监测平均度是检测不连通性的不可靠方法。这是**不正确**的。\n- **补救策略**：将数据全局缩放至例如单位方差是一个常见且通常有用的预处理步骤。它可以简化全局带宽 $\\epsilon$ 的选择。然而，它绝对不*保证*连通性或遍历性。数据的拓扑结构，例如存在分离良好的簇，在这种缩放下被保留。如果数据本质上是聚类的，选择一个小的 $\\epsilon$ 或 $k$ 仍将导致一个不连通的图。这是**不正确**的。\n- **结论**：选项 B 是**不正确**的。\n\n**选项 C：通过运行广度优先搜索来计算在一系列 $k$ 值下 $k$-NN 图的连通分量数量来检测不连通性；补救措施是将 $k$ 增加到使图连通的最小值，然后使用惰性随机游走 $P_{\\ell} = \\tfrac{1}{2}(I + P)$ 来避免周期性，而不改变局部边结构。**\n\n- **检测策略**：使用如图遍历算法，如广度优先搜索（BFS）或深度优先搜索（DFS），是寻找和计算图的连通分量的最直接、计算上最高效的标准方法。这是一个完全有效的检测策略。这是**正确**的。\n- **补救策略**：补救措施有两部分。首先，将 $k$ 增加到使图连通的最小值是确保不可约性的直接且可靠的方法。这是一种非常普遍的做法。其次，问题陈述遍历性需要不可约性和非周期性。虽然 k-NN 图不总是二分图，但周期性在某些图结构中可能是一个问题。通过定义 $P_{\\ell} = \\alpha I + (1-\\alpha) P$（对于某个 $0  \\alpha  1$，这里 $\\alpha=1/2$）引入惰性随机游走是保证非周期性的标准技术，因为它确保了每个节点都有非零的自环概率。这个两步补救措施正确地确保了遍历性的两个条件。这是**正确**的。\n- **结论**：选项 C 是**正确**的。\n\n**选项 D：通过检查 $P$ 的最大特征值是否严格小于 $1$ 来检测不连通性；如果它等于 $1$，则图必然是不连通的。补救措施是重新归一化 $W$ 的行，使其和为 $1$，这确保了不可约性。**\n\n- **检测策略**：这个策略基于一个根本性的误解。对于任何对应于非空图的行随机矩阵 $P$，其最大特征值*总是*等于 $1$，这是 Perron-Frobenius 定理的一个推论。它不可能严格小于 $1$。此外，条件 $\\lambda_{\\max}(P) = 1$ 对*连通*和*不连通*的图都成立。不连通的指标是特征值 $1$ 的*重数*，而不是它的值。这是**不正确**的。\n- **补救策略**：矩阵 $P$ 被定义为 $P = D^{-1}W$。根据构造，对于至少有一条边的节点，其对应的 $P$ 的行和已经为 $1$。因此，重新归一化行的“补救措施”是多余的。更重要的是，这个归一化步骤本身并不会在没有边的地方创建边，因此它无法修复一个不连通的图或确保不可约性。这是**不正确**的。\n- **结论**：选项 D 是**不正确**的。\n\n**选项 E：通过检查 $P$ 的谱中是否存在多个非常接近 $1$ 的特征值（小谱隙）来检测近乎不连通性，这表明存在亚稳态的近乎不连通集合；补救措施是添加一个小的瞬移项 $P_{\\tau} = (1 - \\tau) P + \\tau \\mathbf{1} \\pi^{\\top}$，其中 $\\pi$ 是一个严格正分布且 $0  \\tau \\ll 1$，以强制实现不可约性，同时保持由局部转移编码的主导几何结构。**\n\n- **检测策略**：问题明确提到了“近乎不连通的图”。一个近乎不连通的图表现为一个具有亚稳态的马尔可夫链，意味着链可能会在某些区域“被困”很长时间。对此的定量度量是谱隙，对于矩阵 $P$ 来说，通常是 $1 - |\\lambda_2|$，其中 $\\lambda_2$ 是模第二大的特征值。如果 $\\lambda_2$ 非常接近 $1$，谱隙就很小，链的混合时间就很长。与接近 $1$ 的特征值对应的特征向量在这些亚稳态、近乎不连通的集合上将近似为常数。因此，这种检测策略是识别近乎不连通性的经典方法。这是**正确**的。\n- **补救策略**：提议的补救措施是一种 PageRank 风格的修改。通过将新的转移矩阵 $P_{\\tau}$ 创建为原始 $P$ 和一个瞬移矩阵 $\\mathbf{1}\\pi^{\\top}$ 的凸组合，引入了一个小的概率 $\\tau$，使得从任何节点可以跳转到从分布 $\\pi$ 中随机抽取的一个节点。如果 $\\pi$ 是严格正的（即对所有 $i$ 都有 $\\pi_i > 0$），这就确保了在图中任何两个节点之间都可能发生转移，从而使得修改后的链 $P_{\\tau}$ 是不可约和非周期的（因此是遍历的），而不管原始图的连通性如何。因为 $\\tau$ 很小，这是一个微小的扰动，保留了 $P$ 中编码的主导几何结构。这是一个对不连通和近乎不连通图都有效的补救措施。这是**正确**的。\n- **结论**：选项 E 是**正确**的。\n\n### 正确选项总结\n选项 A、C 和 E 都提出了在扩散图背景下诊断和修复与图连通性相关问题的有效且标准的策略。\n- A 使用谱属性进行检测，并使用参数调整作为补救措施。\n- C 使用直接的算法方法进行检测，并结合参数调整和惰性游走作为补救措施。\n- E 解决了更微妙但同样重要的近乎不连通问题，使用谱隙进行检测，并采用基于瞬移的补救措施。\n这三个选项都正确地描述了合理的策略。",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "在构建了有效的扩散算子之后，一个关键的科学问题随之而来：我们应如何确定嵌入空间的“正确”维度？这个维度反映了数据内在的自由度。本综合练习  将指导您完成一个端到端的项目：首先，您将生成具有已知内在维度的模拟神经数据；然后，您将构建一个完整的扩散映射流程，并通过与统计上严格的零模型进行比较，来估计数据的内在维度。这项练习将理论与数据驱动的假设检验相结合，是计算神经科学研究中的一项核心技能。",
            "id": "4155964",
            "problem": "给定您一组模拟的神经群体活动数据集，每个数据集代表一个具有不同内在潜在复杂度的任务。您的目标是从第一性原理构建一个扩散映射嵌入（DME）流水线，推导出一个假设检验，通过将扩散算子的特征值与非几何零假设进行比较来验证所选的嵌入维度，并实现此流水线，为提供的测试套件生成可量化的结果。\n\n使用的基本和核心定义：\n- 一个数据集表示为神经活动矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其中 $n$ 是样本数， $p$ 是神经元数。\n- 对数据定义一个高斯核 $K_{ij} = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{4 \\epsilon}\\right)$，其中 $\\epsilon  0$ 是一个带宽参数，$\\|\\cdot\\|$ 是欧几里得范数。\n- 使用参数 $\\alpha \\in [0,1]$ 进行密度归一化（各向异性核）：计算 $q_i = \\sum_{j=1}^{n} K_{ij}$，然后定义 $K^{(\\alpha)}_{ij} = \\frac{K_{ij}}{q_i^{\\alpha} q_j^{\\alpha}}$。\n- 通过归一化 $K^{(\\alpha)}$ 的行来构造一个行随机马尔可夫转移矩阵 $P$：$d_i = \\sum_{j=1}^{n} K^{(\\alpha)}_{ij}$，然后 $P_{ij} = \\frac{K^{(\\alpha)}_{ij}}{d_i}$。这在数据集上定义了一个马尔可夫链。\n- 扩散时间为 $t$ 的扩散映射嵌入（DME）是使用 $P$ 的特征值 $\\{\\lambda_\\ell\\}$ 和右特征向量 $\\{\\psi_\\ell\\}$ 构建的，其有效谱缩放为 $\\mu_\\ell = \\lambda_\\ell^t$。\n- 嵌入维度在操作上定义为不平凡特征值（不包括等于1的平凡最大特征值）的数量，这些特征值显著大于从数据的非几何零分布中预期的值。\n\n假设检验设计要求：\n- 通过破坏局部几何结构同时保留单个神经元的边缘分布来构造不平凡特征值的零分布。令 $X^{(b)}$ 是一个代理数据集，通过独立地置换 $X$ 的每一列的条目获得，其中 $b = 1, \\dots, B$， $B$ 是代理数据集的数量。这保留了每个神经元的经验分布，但消除了多变量几何结构。\n- 对于每个代理数据集 $X^{(b)}$，构造相同的DME算子 $P^{(b)}$ 并计算其特征值 $\\{\\lambda^{(b)}_\\ell\\}$ 和有效谱值 $\\mu^{(b)}_\\ell = \\left(\\lambda^{(b)}_\\ell\\right)^t$，对于顶部的秩 $\\ell = 1, \\dots, r_{\\max}$，其中 $r_{\\max}$ 是所考虑的最大秩。排除秩 $\\ell=1$ 处的平凡最大特征值，并分析秩 $\\ell = 2, \\dots, r_{\\max}$。\n- 在固定的显著性水平 $\\alpha_{\\mathrm{sig}} \\in (0,1)$ 下，将特定秩的阈值 $\\tau_\\ell$ 定义为 $\\mu^{(b)}_\\ell$ 在 $b = 1, \\dots, B$ 上的零分布的 $(1 - \\alpha_{\\mathrm{sig}})$ 分位数，对每个秩 $\\ell$ 都如此。所选的嵌入维度 $\\hat{d}$ 是最大的整数 $k \\in \\{0,1,\\dots,r_{\\max}-1\\}$，使得对于所有秩 $\\ell = 2, \\dots, k+1$，我们都有 $\\mu_\\ell  \\tau_\\ell$。这产生一个保守的序贯检验，它计算从最大的不平凡特征值开始的连续显著特征值的数量。\n- 在所有涉及角度的构造中，角度必须以弧度为单位。\n\n算法构造约束：\n- 使用一个 $k$-最近邻图来稀疏化核 $K$，其中 $k$ 对每个测试案例进行适当选择，并通过将 $K$ 与其转置求平均来进行对称化。将 $\\epsilon$ 设置为每个点与其 $k$ 个最近邻之间的平方距离的中位数。然后应用带有 $\\alpha$ 的各向异性归一化。\n- 使用适合稀疏矩阵的数值方法计算 $P$ 和每个代理 $P^{(b)}$ 的前 $r_{\\max}$ 个特征值。按实部降序对特征值进行排序。使用扩散时间 $t$ 将特征值 $\\lambda_\\ell$ 重新缩放为 $\\mu_\\ell = \\lambda_\\ell^t$。\n\n测试套件：\n对于下面的每个案例，以科学上现实的方式模拟 $X$：抽取 $d_{\\mathrm{true}}$ 个潜在角度变量 $\\theta_k \\sim \\mathrm{Uniform}(0, 2\\pi)$，$k = 1, \\dots, d_{\\mathrm{true}}$，计算潜在坐标 $z_k = \\sin(\\theta_k)$，形成一个矩阵 $Z \\in \\mathbb{R}^{n \\times d_{\\mathrm{true}}}$，其列为 $z_k$，通过 $Y = \\tanh(Z W^\\top) + \\eta$ 映射到 $p$ 维神经活动，其中 $W \\in \\mathbb{R}^{d_{\\mathrm{true}} \\times p}$ 具有独立的标准正态分布项，而 $\\eta$ 是逐元素应用的独立高斯噪声 $\\mathcal{N}(0, \\sigma^2)$。对于零假设情况，将 $Z$ 设置为恒等于零，并将 $Y$ 抽样为均值为零、方差为 $\\sigma^2$ 的独立高斯噪声。使用弧度作为角度单位。核带宽的构造和马尔可夫归一化必须遵循上述流水线。\n\n指定并使用以下参数集作为测试套件：\n- 案例 1 (低复杂度): $n = 300$, $p = 25$, $d_{\\mathrm{true}} = 1$, $\\sigma = 0.05$, $k = 20$, $\\alpha = 1$, $t = 20$, $B = 40$, $r_{\\max} = 6$, seed $= 13$, 显著性水平 $\\alpha_{\\mathrm{sig}} = 0.05$。\n- 案例 2 (中等复杂度): $n = 400$, $p = 25$, $d_{\\mathrm{true}} = 2$, $\\sigma = 0.07$, $k = 20$, $\\alpha = 1$, $t = 20$, $B = 40$, $r_{\\max} = 6$, seed $= 17$, 显著性水平 $\\alpha_{\\mathrm{sig}} = 0.05$。\n- 案例 3 (较高复杂度): $n = 350$, $p = 30$, $d_{\\mathrm{true}} = 3$, $\\sigma = 0.10$, $k = 20$, $\\alpha = 1$, $t = 20$, $B = 35$, $r_{\\max} = 6$, seed $= 19$, 显著性水平 $\\alpha_{\\mathrm{sig}} = 0.05$。\n- 案例 4 (零几何基线): $n = 300$, $p = 25$, $d_{\\mathrm{true}} = 0$, $\\sigma = 0.10$, $k = 20$, $\\alpha = 1$, $t = 20$, $B = 30$, $r_{\\max} = 6$, seed $= 23$, 显著性水平 $\\alpha_{\\mathrm{sig}} = 0.05$。\n\n要求输出：\n- 对于每个测试案例，使用上述假设检验计算所选的嵌入维度 $\\hat{d}$。生成一行包含Python列表的结果，每个测试案例一个结果，每个结果是列表 $[\\hat{d}, d_{\\mathrm{true}}, \\text{match}]$，其中 $\\text{match}$ 等于布尔值 $(\\hat{d} = d_{\\mathrm{true}})$。\n- 您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[ [\\hat{d}_1, d_{\\mathrm{true},1}, \\text{match}_1], [\\hat{d}_2, d_{\\mathrm{true},2}, \\text{match}_2], \\dots ]$）。此问题中没有物理单位，角度必须以弧度处理。\n\n在一个自包含的程序中实现所有计算。不允许用户输入或外部文件。仅使用指定的库和环境。确保模拟的科学真实性和基于上述核心定义推导出的假设检验程序的正确性。",
            "solution": "该问题要求实现并验证一个扩散映射嵌入（DME）流水线，以估计模拟神经科学数据集的内在维度。这涉及数据模拟、扩散算子的构建、谱分析以及一个非参数假设检验，以确定显著维度的数量。整个过程将根据提供的第一性原理进行详细说明。\n\n### 1. 理论框架\n\n核心思想是将一个数据集 $X \\in \\mathbb{R}^{n \\times p}$（包含 $n$ 个 $p$ 维神经活动样本）视为一个图，其中样本是节点。节点之间的连通性由它们的相似性决定。然后通过图上的随机游走来分析这种结构，相关转移矩阵的谱特性揭示了数据潜在的几何结构。\n\n#### 1.1. 数据模拟\n\n对于每个测试案例，我们生成一个具有已知潜在维度 $d_{\\mathrm{true}}$ 的合成数据集 $X$。这是通过首先从均匀分布 $\\mathrm{Uniform}(0, 2\\pi)$ 中抽样 $d_{\\mathrm{true}}$ 个潜在角度变量 $\\{\\theta_k\\}_{k=1}^{d_{\\mathrm{true}}}$ 来实现的。然后将这些变量转换为潜在坐标 $z_k = \\sin(\\theta_k)$，形成一个潜在数据矩阵 $Z \\in \\mathbb{R}^{n \\times d_{\\mathrm{true}}}$。通过一个随机投影矩阵 $W \\in \\mathbb{R}^{d_{\\mathrm{true}} \\times p}$（其条目来自 $\\mathcal{N}(0,1)$）进行线性变换，然后是一个非线性压缩函数 $\\tanh$，将数据映射到高维神经空间。最后，引入方差为 $\\sigma^2$ 的加性高斯噪声 $\\eta \\in \\mathbb{R}^{n \\times p}$。完整的模型是：\n$$X = \\tanh(Z W^\\top) + \\eta$$\n对于 $d_{\\mathrm{true}} = 0$ 的零假设情况，矩阵 $Z$ 恒等于零，因此 $X$ 仅由噪声组成，$X = \\eta$。\n\n#### 1.2. 扩散算子的构建\n\n从原始数据 $X$ 构建扩散算子 $P$ 的过程遵循几个关键步骤。\n\n**步骤 1：核稀疏化**\n首先，我们计算所有成对平方欧几里得距离的矩阵 $D_{ij} = \\|x_i - x_j\\|^2$。为了关注局部几何结构并提高计算效率，我们对这些信息进行稀疏化。对于每个数据点 $x_i$，我们识别其 $k$ 个最近邻。核带宽参数 $\\epsilon$ 被稳健地设置为所有点到其第 $k$ 个最近邻的平方距离的中位数。这种自适应的 $\\epsilon$ 选择随数据的局部密度而变化。然后使用高斯核构建一个稀疏核矩阵，$K_{ij} = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{4 \\epsilon}\\right)$，其中仅当 $x_j$ 是 $x_i$ 的 $k$ 个最近邻之一时，$K_{ij}$ 才为非零。为确保得到的算子与一个对称算子相关（这保证了实数特征值），通过平均来对称化稀疏核：$K \\leftarrow \\frac{1}{2}(K + K^\\top)$。\n\n**步骤 2：各向异性密度归一化**\n然后对核 $K$ 进行归一化，以考虑流形上非均匀的采样密度。使用参数 $\\alpha \\in [0, 1]$，我们为每个点定义一个密度度量 $q_i = \\sum_{j=1}^{n} K_{ij}$。然后将核重新加权为：\n$$K^{(\\alpha)}_{ij} = \\frac{K_{ij}}{q_i^{\\alpha} q_j^{\\alpha}}$$\n对于 $\\alpha=1$，这种归一化使算子收敛于流形上的拉普拉斯-贝尔特拉米算子，该算子对采样密度不变。\n\n**步骤 3：马尔可夫转移矩阵**\n最后，通过归一化 $K^{(\\alpha)}$ 的行来构建一个行随机马尔可夫转移矩阵 $P$。令 $d_i = \\sum_{j=1}^{n} K^{(\\alpha)}_{ij}$，则 $P$ 的条目由下式给出：\n$$P_{ij} = \\frac{K^{(\\alpha)}_{ij}}{d_i}$$\n矩阵 $P$ 描述了数据图上随机游走中一步转移的概率。\n\n#### 1.3. 谱分析与嵌入\n\n数据流形的结构编码在 $P$ 的谱中。我们计算 $P$ 的前 $r_{\\max}$ 个特征值 $\\{\\lambda_\\ell\\}_{\\ell=1}^{r_{\\max}}$ 和相应的右特征向量 $\\{\\psi_\\ell\\}_{\\ell=1}^{r_{\\max}}$，并进行排序，使得 $1 = \\lambda_1 \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_{r_{\\max}}|$。最大的特征值 $\\lambda_1=1$ 是平凡的。随后的特征值 $\\lambda_2, \\lambda_3, \\dots$ 量化了图沿不同几何方向的连通性。随时间 $t$ 的扩散平滑了数据并锐化了谱隙，这通过缩放特征值来建模：\n$$\\mu_\\ell = \\lambda_\\ell^t$$\n与大特征值对应的特征向量集合构成了扩散映射嵌入。这些显著特征值的数量对应于数据的内在维度。\n\n#### 1.4. 用于维度估计的假设检验\n\n为了客观地确定嵌入维度 $\\hat{d}$，我们必须区分反映真实几何结构的特征值和由随机噪声产生的特征值。为此设计了一个非参数假设检验。\n\n**零假设：** 数据缺乏任何连贯的低维几何结构。观测到的特征值与具有相似边缘统计数据但没有多变量依赖关系的数据集所产生的特征值一致。\n\n**零分布生成：** 我们通过独立置换每个神经元的时间序列（即，打乱 $X$ 的每一列）来创建 $B$ 个代理数据集 $\\{X^{(b)}\\}_{b=1}^B$。这个过程保留了每个神经元活动的边缘分布，但破坏了定义几何流形的神经元之间的特定时间协调关系。\n\n**统计检验：** 对于每个代理数据集 $X^{(b)}$，我们计算整个DME流水线以获得一组缩放后的特征值 $\\{\\mu_\\ell^{(b)}\\}_{\\ell=2}^{r_{\\max}}$。对于每个秩 $\\ell$，这产生了一个包含 $B$ 个特征值的经验零分布。我们将特定秩的显著性阈值 $\\tau_\\ell$ 定义为该秩的零分布的 $(1 - \\alpha_{\\mathrm{sig}})$ 分位数。\n\n**决策规则：** 估计的维度 $\\hat{d}$ 由一个序贯检验确定。我们计算从 $\\mu_2$ 开始，连续超过其各自阈值的不平凡特征值的数量。形式上，$\\hat{d}$ 是最大的整数 $k \\in \\{0, 1, \\dots, r_{\\max}-1\\}$，对于所有 $\\ell = 2, \\dots, k+1$，都有 $\\mu_\\ell  \\tau_\\ell$。如果 $\\mu_2 \\le \\tau_2$，则估计维度为 $\\hat{d}=0$，表示未检测到显著的几何结构。\n\n这个严谨、数据驱动的过程提供了一种有原则的方法来估计神经群体活动的内在复杂度。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom scipy.sparse import lil_matrix, csc_matrix\nfrom scipy.sparse.linalg import eigs\n\ndef simulate_data(n, p, d_true, sigma, rng):\n    \"\"\"\n    Simulates neural population activity data based on a latent manifold.\n    \"\"\"\n    if d_true == 0:\n        # Null case: pure noise\n        return rng.normal(0, sigma, size=(n, p))\n\n    # Generate latent variables\n    thetas = rng.uniform(0, 2 * np.pi, size=(n, d_true))\n    Z = np.sin(thetas)\n    \n    # Random projection matrix\n    W = rng.standard_normal(size=(d_true, p))\n    \n    # Generate neural activity with noise\n    Y = np.tanh(Z @ W)\n    noise = rng.normal(0, sigma, size=(n, p))\n    X = Y + noise\n    \n    return X\n\ndef compute_dme_eigenvalues(X, k, alpha, t, r_max):\n    \"\"\"\n    Constructs the DME pipeline and computes the top scaled eigenvalues.\n    \"\"\"\n    n, p = X.shape\n\n    # 1. Compute pairwise distances and find k-NN\n    # Using squared Euclidean distance as per kernel definition\n    sq_dists = cdist(X, X, 'sqeuclidean')\n    \n    # Find k nearest neighbors for each point (excluding self)\n    # Using argpartition for efficiency. The k+1 smallest distances are in the first k+1 columns.\n    neighbor_indices = np.argpartition(sq_dists, kth=k, axis=1)[:, 1:k+1]\n    \n    # 2. Compute epsilon (bandwidth parameter)\n    # Gather all squared distances to the k nearest neighbors\n    nn_sq_dists = sq_dists[np.arange(n)[:, None], neighbor_indices]\n    if nn_sq_dists.size == 0:\n        # Handle case where k=0 or n is small\n        epsilon = 1.0\n    else:\n        epsilon = np.median(nn_sq_dists)\n    \n    # Prevent epsilon from being zero, which would cause division by zero\n    if epsilon == 0:\n        epsilon = 1e-9\n\n    # 3. Construct sparse kernel K\n    K_sparse = lil_matrix((n, n), dtype=np.float64)\n    rows = np.arange(n).repeat(k)\n    cols = neighbor_indices.flatten()\n    vals = sq_dists[rows, cols]\n    \n    K_sparse[rows, cols] = np.exp(-vals / (4 * epsilon))\n    \n    # 4. Symmetrize the kernel\n    K_symm = (K_sparse + K_sparse.T) / 2.0\n    \n    # 5. Anisotropic normalization (alpha=1 for all cases here)\n    q = np.array(K_symm.sum(axis=1)).flatten()\n    q[q == 0] = 1.0 # Avoid division by zero\n    \n    if alpha > 0:\n        inv_q_alpha = np.power(q, -alpha)\n        # Element-wise multiplication with outer product of inv_q_alpha\n        K_alpha = K_symm.multiply(np.outer(inv_q_alpha, inv_q_alpha))\n    else:\n        K_alpha = K_symm\n        \n    # 6. Construct Markov transition matrix P\n    d = np.array(K_alpha.sum(axis=1)).flatten()\n    d[d == 0] = 1.0 # Avoid division by zero\n    inv_d = np.power(d, -1)\n    \n    P = csc_matrix(K_alpha).multiply(inv_d[:, np.newaxis])\n    \n    # 7. Compute eigenvalues\n    try:\n        # Using 'LR' for largest real part. This is more robust for non-symmetric matrices.\n        # Although P should be similar to a symmetric matrix, numerical issues can occur.\n        evals = eigs(P, k=r_max, which='LR', return_eigenvectors=False)\n    except Exception:\n        # If solver fails, return zeros, which will result in d_hat=0\n        return np.zeros(r_max)\n        \n    # Sort eigenvalues by their real part in descending order\n    evals = np.sort(evals.real)[::-1]\n    \n    # 8. Scale eigenvalues by diffusion time t\n    # Handle negative eigenvalues by taking absolute value before power, though for\n    # this construction they should be non-negative.\n    mu = np.power(np.abs(evals), t)\n    \n    return mu\n\ndef solve():\n    test_cases = [\n        # Case 1 (low complexity)\n        {'n': 300, 'p': 25, 'd_true': 1, 'sigma': 0.05, 'k': 20, 'alpha': 1, 't': 20, 'B': 40, 'r_max': 6, 'seed': 13, 'alpha_sig': 0.05},\n        # Case 2 (moderate complexity)\n        {'n': 400, 'p': 25, 'd_true': 2, 'sigma': 0.07, 'k': 20, 'alpha': 1, 't': 20, 'B': 40, 'r_max': 6, 'seed': 17, 'alpha_sig': 0.05},\n        # Case 3 (higher complexity)\n        {'n': 350, 'p': 30, 'd_true': 3, 'sigma': 0.10, 'k': 20, 'alpha': 1, 't': 20, 'B': 35, 'r_max': 6, 'seed': 19, 'alpha_sig': 0.05},\n        # Case 4 (null geometry baseline)\n        {'n': 300, 'p': 25, 'd_true': 0, 'sigma': 0.10, 'k': 20, 'alpha': 1, 't': 20, 'B': 30, 'r_max': 6, 'seed': 23, 'alpha_sig': 0.05}\n    ]\n\n    results = []\n    for params in test_cases:\n        rng = np.random.default_rng(params['seed'])\n        \n        # --- Simulate data and compute eigenvalues for the original dataset ---\n        X = simulate_data(params['n'], params['p'], params['d_true'], params['sigma'], rng)\n        mu_observed = compute_dme_eigenvalues(X, params['k'], params['alpha'], params['t'], params['r_max'])\n        \n        # --- Generate null distribution from surrogate datasets ---\n        surrogate_mus = np.zeros((params['B'], params['r_max']))\n        for b in range(params['B']):\n            X_surr = X.copy()\n            for col in range(params['p']):\n                rng.shuffle(X_surr[:, col])\n            \n            surrogate_mus[b, :] = compute_dme_eigenvalues(X_surr, params['k'], params['alpha'], params['t'], params['r_max'])\n            \n        # --- Perform hypothesis test ---\n        # Analyze non-trivial eigenvalues (from index 1, i.e., rank 2)\n        nontrivial_mus = mu_observed[1:params['r_max']]\n        surrogate_nontrivial_mus = surrogate_mus[:, 1:params['r_max']]\n        \n        # Calculate thresholds from null distribution\n        q = (1 - params['alpha_sig']) * 100\n        thresholds = np.percentile(surrogate_nontrivial_mus, q, axis=0)\n\n        # Apply sequential test to find d_hat\n        d_hat = 0\n        for i in range(len(nontrivial_mus)):\n            if nontrivial_mus[i] > thresholds[i]:\n                d_hat += 1\n            else:\n                break\n        \n        match = (d_hat == params['d_true'])\n        # Store results as a raw list for proper string conversion later\n        results.append([d_hat, params['d_true'], match])\n\n    # Convert boolean to lowercase 'true'/'false' for the final output string\n    result_strings = []\n    for res in results:\n        # Note: str(True) is 'True', we need 'true'\n        res_str = f\"[{res[0]}, {res[1]}, {str(res[2]).lower()}]\"\n        result_strings.append(res_str)\n\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}