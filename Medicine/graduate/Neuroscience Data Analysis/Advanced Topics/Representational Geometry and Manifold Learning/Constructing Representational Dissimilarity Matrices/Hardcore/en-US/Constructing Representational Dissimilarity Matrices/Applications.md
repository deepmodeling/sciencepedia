## Applications and Interdisciplinary Connections

Having established the principles and mechanisms for constructing Representational Dissimilarity Matrices (RDMs) in the preceding chapter, we now turn to their application. The true power of the RDM framework lies in its role as an analytical nexus, a "common currency" that enables the comparison of data from disparate sources. This chapter explores how RDMs are utilized to map neural representations in space and time, to bridge the explanatory gap between brain, behavior, and computational models, and to visualize the abstract geometry of neural codes. Furthermore, we will examine advanced applications and the extension of this framework beyond its neuroscientific origins, demonstrating its broad utility as a tool for understanding complex information processing systems.

### Mapping Representational Spaces in the Brain

A primary application of RDMs is to characterize how information is represented in the brain. The RDM provides a signature of a region's or system's representational geometry, which can be measured using various neurophysiological techniques, each offering unique advantages in spatial or temporal resolution.

#### Spatially-Resolved Brain Mapping with fMRI

In functional Magnetic Resonance Imaging (fMRI), RDMs allow researchers to move beyond [simple activation](@entry_id:1131661)-based analyses and probe the rich, multivariate information content within a brain region. A neural RDM is typically constructed from the condition-specific activity patterns, often the [general linear model](@entry_id:170953) (GLM) beta coefficients, estimated for each experimental condition. To obtain a robust and statistically unbiased estimate of the dissimilarity between two conditions, it is crucial to use data from independent measurement partitions (e.g., odd vs. even scanning runs). This [cross-validation](@entry_id:164650) procedure prevents the shared measurement noise from artificially inflating the apparent similarity of the patterns. Furthermore, advanced dissimilarity metrics like the cross-validated Mahalanobis distance can account for the spatial correlation and differing variance of noise across voxels, yielding a more sensitive and reliable RDM. 

While this approach is powerful for a pre-specified region of interest (ROI), the searchlight RSA method extends this logic to the entire brain, enabling spatially unbiased, exploratory mapping. This technique involves iterating a small, spherical neighborhood (a "searchlight") across every voxel in the brain. At each location, a local RDM is computed using only the voxels within the searchlight. This local neural RDM is then compared to one or more theoretical model RDMs. The resulting model-fit value, typically a correlation coefficient, is assigned to the center voxel of the searchlight. By repeating this process for all voxels, a whole-brain map is generated, revealing where in the brain the local representational geometry is consistent with a given hypothesis. This transforms RSA from a purely ROI-based method into a powerful tool for discovering the spatial layout of functionally distinct representations. 

#### Uncovering Representational Dynamics with M/EEG

Techniques like Magnetoencephalography (MEG) and Electroencephalography (EEG) offer superb [temporal resolution](@entry_id:194281), making them ideal for tracking the evolution of neural representations on a millisecond timescale. Using these methods, a separate RDM can be constructed for each time point, treating the vector of sensor readings (or source-reconstructed activities) as the multivariate pattern. This results in a time-resolved series of RDMs—a "movie" that captures the dynamic unfolding of the representational geometry in response to a stimulus or cognitive event. As with fMRI, careful preprocessing is essential. Trial-wise [baseline correction](@entry_id:746683) is used to remove pre-stimulus offsets, and cross-validation across trials is necessary to obtain unbiased dissimilarity estimates at each time point. 

To analyze this rich temporal information, researchers often employ a **temporal generalization analysis**. This involves computing the correlation between RDMs from all pairs of time points, $(t_1, t_2)$. The result is a $T \times T$ Temporal Generalization Matrix (TGM), where $T$ is the number of time points. A high value in the TGM at $(t_1, t_2)$ indicates that the representational geometry present at time $t_1$ is reactivated or sustained at time $t_2$. The structure of the TGM is highly informative: a strong diagonal with weak off-diagonals suggests a sequence of transient, non-recurring neural codes. Conversely, a square, block-like structure off the diagonal would indicate a stable representation that persists for an extended period. This method allows for a detailed characterization of the brain's representational dynamics, distinguishing between fleeting and stable neural codes. 

#### From Single Neurons to Population Codes

RSA is not limited to non-invasive neuroimaging. The framework can be applied at the microcircuit level using data from single-unit or multi-unit electrophysiological recordings. To do so, the spike trains from a population of simultaneously recorded neurons are first converted into feature vectors for each experimental condition. A common approach is to compute a [peri-stimulus time histogram](@entry_id:1129517) (PSTH) for each neuron and concatenate these vectors to form a high-dimensional representation of the population response. Once these condition-specific population vectors are formed, an RDM can be constructed. Again, to obtain unbiased estimates of dissimilarity that are interpretable in terms of the information available for a linear readout, it is vital to use cross-validated and noise-normalized metrics such as the cross-validated Mahalanobis distance. This application of RSA provides a powerful link between the activity of individual neurons and the emergent [representational geometry](@entry_id:1130876) of the population code. 

### Bridging Brain, Behavior, and Models

The true utility of a neural RDM is realized when it is compared to other RDMs. By abstracting from the specific implementation of a representation to its geometric form, RSA provides a common ground for relating neural data to behavioral judgments and computational theories.

#### Building Models of Representation

A key component of RSA is the **model RDM**, a theoretical prediction of the [representational geometry](@entry_id:1130876). These models can be derived from a wide variety of sources.

- **Feature-Based Models:** A straightforward approach is to define a set of explicit features for each stimulus and compute the pairwise distances between stimuli in this feature space. For instance, visual stimuli could be described by features like color, shape, and texture. When constructing such a model, it is often important to consider scaling the feature dimensions. A theoretically motivated approach is to scale each feature inversely to its assumed noise level, which gives more weight to more reliable features. This is mathematically equivalent to computing a Mahalanobis distance in the feature space. 

- **Behavioral Models:** RDMs can be constructed directly from behavioral data, providing a powerful way to test whether neural representations reflect perception or cognition. For example, participants can be asked to rate the similarity of all pairs of stimuli. These similarity judgments can then be converted into a [dissimilarity matrix](@entry_id:636728), for instance, by inverting and normalizing the ratings. This behavioral RDM captures the structure of the subjective psychological space, which can then be directly compared to a neural RDM. 

- **Computational Models:** With the rise of artificial intelligence, [deep neural networks](@entry_id:636170) (DNNs) have become a particularly fruitful source of models for RSA. A stimulus set can be presented to a trained DNN, and the activation patterns at each layer can be extracted. By computing an RDM for each layer, researchers can obtain a set of model RDMs that reflect the network's processing hierarchy. Comparing this sequence of RDMs to RDMs from different stages of a biological neural pathway (e.g., V1, V4, IT in the visual system) allows for a quantitative test of the hypothesis that DNNs and brains use similar representational transformations to solve complex tasks. 

#### The Brain-Model Interface: Comparing RDMs

Once a neural RDM and one or more model RDMs have been constructed, their correspondence must be quantified. The standard method is to vectorize the upper-triangular entries of each RDM and compute a [correlation coefficient](@entry_id:147037) between the vectors. Spearman's [rank correlation](@entry_id:175511) is often preferred, as it is robust to non-linear but monotonic relationships between the dissimilarity scales of the brain and the model. A high correlation provides evidence that the brain region in question implements a representational geometry consistent with the model. Statistical significance is typically assessed via a non-parametric permutation test, where condition labels are randomly shuffled to generate a null distribution of correlations.  

When multiple competing models are available, **regression RSA** offers a more sophisticated approach. In this method, the neural RDM is modeled as a weighted [linear combination](@entry_id:155091) of the model RDMs. The coefficients, or beta weights, for each model are estimated using [multiple linear regression](@entry_id:141458). This allows one to assess the unique contribution of each model to explaining the neural data, even when the models themselves are correlated. To handle such multicollinearity, the regression is often solved using the Moore-Penrose [pseudoinverse](@entry_id:140762). The significance of each model's contribution can then be tested with a permutation scheme, providing a powerful framework for model adjudication. 

### Interpreting and Visualizing Representational Geometry

RDMs are matrices of numbers that can be difficult to interpret by simple inspection. Visualization techniques are therefore essential for gaining an intuitive understanding of the underlying representational structure.

#### Low-Dimensional Projections with Multidimensional Scaling (MDS)

Multidimensional Scaling (MDS) is a technique that projects the high-dimensional structure captured in an RDM into a low-dimensional space, typically 2D or 3D, for visualization. The algorithm arranges the stimuli in this low-dimensional space such that the distances between them are as close as possible to the dissimilarities in the original RDM. The classical MDS procedure, which is exact for Euclidean distances, involves converting the squared [dissimilarity matrix](@entry_id:636728) into a Gram (inner-product) matrix through a process called double-centering. An [eigendecomposition](@entry_id:181333) of this Gram matrix then yields the coordinates for each stimulus in the [embedding space](@entry_id:637157). The resulting plot provides an intuitive "map" of the representational space, where stimuli that are represented similarly in the brain appear close together, and stimuli that are represented distinctly appear far apart. 

#### Uncovering Taxonomy with Hierarchical Clustering

While MDS is excellent for showing the continuous spatial relationships between stimuli, [hierarchical clustering](@entry_id:268536) reveals their nested categorical structure, or representational [taxonomy](@entry_id:172984). This method starts by treating each stimulus as its own cluster and iteratively merges the two most similar clusters until only a single cluster remains. The choice of [linkage criterion](@entry_id:634279)—how the dissimilarity between two clusters is defined—is critical. For example, under [average linkage](@entry_id:636087), the distance between two clusters is the average of all pairwise dissimilarities between their members. The sequence of merges and the dissimilarity values at which they occur can be visualized as a dendrogram, or tree diagram. The structure of this tree illustrates which groups of stimuli are represented as coherent categories by the neural population, and the heights of the branches reflect the dissimilarity between these categories. 

### Advanced Applications and Interdisciplinary Connections

The RDM framework is not only a descriptive tool but also a bridge to deeper theoretical insights and applications in other fields.

#### The Link to Information-Theoretic Analysis

A common question is how RSA relates to decoding-based [multivariate pattern analysis](@entry_id:1128353) (MVPA), where a classifier is trained to predict a stimulus from the corresponding neural activity pattern. There is, in fact, a formal mathematical link between the two. For the case of two conditions whose responses are drawn from Gaussian distributions with a shared covariance, the decoding accuracy of a Bayes-optimal [linear classifier](@entry_id:637554) is a monotonic function of the squared Mahalanobis distance ($d^2$) between the two condition means. The exact relationship is given by $Accuracy = \Phi(\sqrt{d^2}/2)$, where $\Phi$ is the [cumulative distribution function](@entry_id:143135) of the [standard normal distribution](@entry_id:184509). This elegant result demonstrates that the Mahalanobis distance, a primary dissimilarity measure in RSA, is not just a geometric descriptor but is directly and quantitatively related to the amount of linearly decodable information present in the neural code. A larger dissimilarity in an RDM implies a greater separability of the underlying representations. 

#### Systems-Level Analysis: Representational Connectivity

RSA can be scaled up to investigate systems-level interactions across the brain. In a **representational connectivity** analysis, brain regions are treated as the nodes of a network. The weight of the edge connecting any two regions is defined by the similarity of their RDMs. A high edge weight indicates that the two regions share a similar representational geometry, suggesting they are part of a common functional network or that they exchange information in a format-preserving way. Standard network science tools, such as [community detection algorithms](@entry_id:1122700), can then be applied to this graph. This allows researchers to discover modules of brain regions that work in concert, processing information using a common representational code. Significance of such [community structure](@entry_id:153673) is assessed via carefully constructed [permutation tests](@entry_id:175392) that break the cross-region alignment while preserving within-region properties. 

#### Methodological Considerations: Group-Level Analysis

When analyzing data from multiple subjects, a key decision is how to aggregate the subject-specific RDMs to produce a group-level result. Two common approaches are direct arithmetic averaging of the dissimilarity values and rank-averaging. Simple averaging preserves the full metric information from each subject but can be sensitive to [outliers](@entry_id:172866) or idiosyncratic scaling of the dissimilarity space in one subject. In contrast, rank-averaging—where each subject's dissimilarities are first converted to ranks before averaging—is robust to such scaling differences but discards the metric information. The choice between these methods involves a trade-off between increasing the reliability of the group estimate and preserving the potentially meaningful individual differences in representational magnitude. 

#### Beyond Neuroscience: RSA in Machine Learning and Materials Science

The principles of RSA are general and can be applied to any system that produces high-dimensional representations. This has made RSA a valuable tool in machine learning for interpreting and understanding the internal workings of complex models. For instance, in the context of transfer learning, RSA can be used to analyze how the representations in a GNN (Graph Neural Network) change when it is adapted from a source task (e.g., predicting [formation energy](@entry_id:142642) of a material) to a target task (e.g., predicting diffusion barrier). By computing RDMs for each layer of the network before and after [fine-tuning](@entry_id:159910), one can quantify the change in representational geometry at each stage of processing. This analysis can reveal which layers learn task-general features (whose geometry remains stable) and which layers learn task-specific features (whose geometry changes significantly), providing critical insights for improving model architecture and training strategies. This application highlights the power of RSA as a domain-general framework for comparing and understanding representations. 

### Conclusion

As this chapter has demonstrated, the construction of Representational Dissimilarity Matrices is not an end in itself, but a gateway to a rich and versatile suite of analytical techniques. From mapping the spatial and temporal dynamics of neural codes to providing a quantitative basis for comparing brains, behavior, and computational models, the RDM framework has proven to be an indispensable tool in modern neuroscience. Its ability to abstract complex, high-dimensional patterns into a common format of [representational geometry](@entry_id:1130876), combined with its applicability to systems beyond the brain, ensures its continued relevance and broad impact across scientific and engineering disciplines.