{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of Representational Similarity Analysis (RSA) is the Representational Dissimilarity Matrix (RDM), which provides a snapshot of the representational geometry within a neural population. This initial exercise guides you through the fundamental computation of an RDM from multivoxel activity patterns using the Euclidean distance. By quantifying within-category and between-category dissimilarities, you will gain a first-hand understanding of how the RDM’s structure visually and quantitatively reveals the information encoded in neural representations .",
            "id": "4015383",
            "problem": "A core tool in Representational Similarity Analysis (RSA) is the representational dissimilarity matrix (RDM), which summarizes pairwise dissimilarities between condition-evoked multivoxel patterns. Consider the condition-by-voxel response matrix $X \\in \\mathbb{R}^{4 \\times 3}$ for $4$ experimental conditions measured over $3$ voxels:\n$$\nX \\;=\\;\n\\begin{pmatrix}\n5 & 1 & 0 \\\\\n4 & 2 & 0 \\\\\n0 & 4 & 5 \\\\\n1 & 5 & 4\n\\end{pmatrix}.\n$$\nAssume conditions $1$ and $2$ belong to category $\\mathcal{A}$ and conditions $3$ and $4$ belong to category $\\mathcal{B}$. Using the Euclidean distance across voxel responses, compute the $4 \\times 4$ RDM whose $(i,j)$ entry is the distance between the multivoxel patterns of conditions $i$ and $j$. Then, quantify category separability by first computing the within-category average dissimilarity as the mean of the distances for the pairs $\\{(1,2),(3,4)\\}$, and the between-category average dissimilarity as the mean of the distances for the pairs $\\{(1,3),(1,4),(2,3),(2,4)\\}$. Define the separability index\n$$\nS \\;=\\; \\frac{d_{\\mathrm{between}} - d_{\\mathrm{within}}}{d_{\\mathrm{within}}}.\n$$\nDiscuss how the structure of the RDM reflects category separability in this dataset, based on these distances. Report the single numerical value of $S$ as your final answer, rounded to $4$ significant figures. The answer is a dimensionless ratio.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- The condition-by-voxel response matrix $X \\in \\mathbb{R}^{4 \\times 3}$ is given:\n$$\nX \\;=\\;\n\\begin{pmatrix}\n5 & 1 & 0 \\\\\n4 & 2 & 0 \\\\\n0 & 4 & 5 \\\\\n1 & 5 & 4\n\\end{pmatrix}\n$$\n- The problem involves $4$ conditions and $3$ voxels.\n- Category memberships are defined: conditions $1$ and $2$ belong to category $\\mathcal{A}$, and conditions $3$ and $4$ belong to category $\\mathcal{B}$.\n- The dissimilarity measure is the Euclidean distance.\n- The task is to compute the $4 \\times 4$ representational dissimilarity matrix (RDM).\n- The within-category average dissimilarity, $d_{\\mathrm{within}}$, is the mean of distances for the pairs $\\{(1,2),(3,4)\\}$.\n- The between-category average dissimilarity, $d_{\\mathrm{between}}$, is the mean of distances for the pairs $\\{(1,3),(1,4),(2,3),(2,4)\\}$.\n- The separability index is defined as $S = \\frac{d_{\\mathrm{between}} - d_{\\mathrm{within}}}{d_{\\mathrm{within}}}$.\n- The final answer required is the numerical value of $S$, rounded to $4$ significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as Representational Similarity Analysis (RSA), RDMs, and category separability are standard concepts in computational neuroscience and brain modeling. The problem is well-posed, providing all necessary data (the matrix $X$), definitions (category assignments), and formulas (Euclidean distance, $d_{\\mathrm{within}}$, $d_{\\mathrm{between}}$, and $S$) to arrive at a unique solution. The language is objective and precise. The problem is self-contained, consistent, and does not violate any fundamental principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution\nThe multivoxel response patterns for the $4$ conditions are the row vectors of the matrix $X$. Let these vectors be denoted by $\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4 \\in \\mathbb{R}^3$:\n$$\n\\mathbf{x}_1 = (5, 1, 0)\n$$\n$$\n\\mathbf{x}_2 = (4, 2, 0)\n$$\n$$\n\\mathbf{x}_3 = (0, 4, 5)\n$$\n$$\n\\mathbf{x}_4 = (1, 5, 4)\n$$\nThe $(i,j)$ entry of the RDM is the Euclidean distance $d_{ij}$ between the response vectors $\\mathbf{x}_i$ and $\\mathbf{x}_j$. The formula for the Euclidean distance between two vectors $\\mathbf{a}=(a_1, a_2, a_3)$ and $\\mathbf{b}=(b_1, b_2, b_3)$ is $d(\\mathbf{a}, \\mathbf{b}) = \\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + (a_3-b_3)^2}$.\n\nFirst, we compute the pairwise distances required for the analysis. These distances form the off-diagonal elements of the $4 \\times 4$ RDM. The RDM is symmetric ($d_{ij} = d_{ji}$) with zeros on the diagonal ($d_{ii}=0$).\n\nThe within-category distances are $d_{12}$ (for category $\\mathcal{A}$) and $d_{34}$ (for category $\\mathcal{B}$):\n$$\nd_{12} = \\sqrt{(5-4)^2 + (1-2)^2 + (0-0)^2} = \\sqrt{1^2 + (-1)^2 + 0^2} = \\sqrt{1+1} = \\sqrt{2}\n$$\n$$\nd_{34} = \\sqrt{(0-1)^2 + (4-5)^2 + (5-4)^2} = \\sqrt{(-1)^2 + (-1)^2 + 1^2} = \\sqrt{1+1+1} = \\sqrt{3}\n$$\n\nThe between-category distances are $d_{13}$, $d_{14}$, $d_{23}$, and $d_{24}$:\n$$\nd_{13} = \\sqrt{(5-0)^2 + (1-4)^2 + (0-5)^2} = \\sqrt{5^2 + (-3)^2 + (-5)^2} = \\sqrt{25+9+25} = \\sqrt{59}\n$$\n$$\nd_{14} = \\sqrt{(5-1)^2 + (1-5)^2 + (0-4)^2} = \\sqrt{4^2 + (-4)^2 + (-4)^2} = \\sqrt{16+16+16} = \\sqrt{48}\n$$\n$$\nd_{23} = \\sqrt{(4-0)^2 + (2-4)^2 + (0-5)^2} = \\sqrt{4^2 + (-2)^2 + (-5)^2} = \\sqrt{16+4+25} = \\sqrt{45}\n$$\n$$\nd_{24} = \\sqrt{(4-1)^2 + (2-5)^2 + (0-4)^2} = \\sqrt{3^2 + (-3)^2 + (-4)^2} = \\sqrt{9+9+16} = \\sqrt{34}\n$$\n\nThe full RDM is:\n$$\n\\mathrm{RDM} =\n\\begin{pmatrix}\n0 & \\sqrt{2} & \\sqrt{59} & \\sqrt{48} \\\\\n\\sqrt{2} & 0 & \\sqrt{45} & \\sqrt{34} \\\\\n\\sqrt{59} & \\sqrt{45} & 0 & \\sqrt{3} \\\\\n\\sqrt{48} & \\sqrt{34} & \\sqrt{3} & 0\n\\end{pmatrix}\n$$\n\nNext, we compute the within-category and between-category average dissimilarities.\nThe within-category average dissimilarity, $d_{\\mathrm{within}}$, is the mean of the dissimilarities for pairs within the same category, which are $(1,2)$ and $(3,4)$.\n$$\nd_{\\mathrm{within}} = \\frac{d_{12} + d_{34}}{2} = \\frac{\\sqrt{2} + \\sqrt{3}}{2}\n$$\nThe between-category average dissimilarity, $d_{\\mathrm{between}}$, is the mean of dissimilarities for pairs in different categories, which are $(1,3)$, $(1,4)$, $(2,3)$, and $(2,4)$.\n$$\nd_{\\mathrm{between}} = \\frac{d_{13} + d_{14} + d_{23} + d_{24}}{4} = \\frac{\\sqrt{59} + \\sqrt{48} + \\sqrt{45} + \\sqrt{34}}{4}\n$$\n\nThe structure of the RDM reflects category separability. If the conditions are ordered by category, the RDM takes on a block structure. The on-diagonal blocks contain within-category dissimilarities (e.g., the top-left $2 \\times 2$ block for category $\\mathcal{A}$ and the bottom-right $2 \\times 2$ block for category $\\mathcal{B}$). The off-diagonal blocks contain between-category dissimilarities. For this dataset, the within-category dissimilarities ($\\sqrt{2} \\approx 1.414$ and $\\sqrt{3} \\approx 1.732$) are considerably smaller than the between-category dissimilarities ($\\sqrt{59} \\approx 7.68$, $\\sqrt{48} \\approx 6.93$, $\\sqrt{45} \\approx 6.71$, $\\sqrt{34} \\approx 5.83$). This visual pattern in the RDM, with low values on the diagonal blocks and high values on the off-diagonal blocks, indicates that the neural representation patterns for items within the same category are more similar to each other than to patterns for items in a different category. This implies a strong categorical structure is present in the data.\n\nFinally, we compute the separability index $S$:\n$$\nS = \\frac{d_{\\mathrm{between}} - d_{\\mathrm{within}}}{d_{\\mathrm{within}}}\n$$\nWe substitute the expressions for $d_{\\mathrm{within}}$ and $d_{\\mathrm{between}}$:\n$$\nS = \\frac{\\frac{\\sqrt{59} + \\sqrt{48} + \\sqrt{45} + \\sqrt{34}}{4} - \\frac{\\sqrt{2} + \\sqrt{3}}{2}}{\\frac{\\sqrt{2} + \\sqrt{3}}{2}}\n$$\nTo obtain the final numerical answer, we evaluate these expressions:\n$$\nd_{\\mathrm{within}} \\approx \\frac{1.41421 + 1.73205}{2} = \\frac{3.14626}{2} \\approx 1.57313\n$$\n$$\nd_{\\mathrm{between}} \\approx \\frac{7.68115 + 6.92820 + 6.70820 + 5.83095}{4} = \\frac{27.14850}{4} \\approx 6.78713\n$$\nNow, we compute $S$:\n$$\nS \\approx \\frac{6.78713 - 1.57313}{1.57313} = \\frac{5.21400}{1.57313} \\approx 3.31441\n$$\nRounding to $4$ significant figures, we get $S \\approx 3.314$. The large positive value of $S$ quantitatively confirms the strong category separability observed qualitatively from the RDM structure.",
            "answer": "$$\\boxed{3.314}$$"
        },
        {
            "introduction": "While simple distance measures are intuitive, they can be biased by measurement noise, potentially inflating dissimilarity estimates. To build more robust analyses, this practice introduces the cross-validated Mahalanobis distance, often called the \"crossnobis\" distance, which yields an unbiased estimate of squared dissimilarity in a noise-normalized space. This exercise will not only walk you through its calculation using split-half data but also challenge you to interpret the meaning of a negative distance estimate, a crucial concept for understanding the statistical properties of unbiased estimators in RSA .",
            "id": "4015382",
            "problem": "Consider two experimental conditions, denoted $A$ and $B$, and a single region of interest with $p=3$ voxels. You have obtained independent split-half pattern estimates from two non-overlapping subsets of trials: split $1$ and split $2$. The voxel-wise pattern estimates for each condition and split are given by\n$$\n\\mathbf{b}_{A}^{(1)} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{b}_{B}^{(1)} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad\n\\mathbf{b}_{A}^{(2)} = \\begin{pmatrix} 1.1 \\\\ 2.2 \\\\ 1.7 \\end{pmatrix}, \\quad\n\\mathbf{b}_{B}^{(2)} = \\begin{pmatrix} 1.2 \\\\ 1.3 \\\\ 1.0 \\end{pmatrix}.\n$$\nA noise covariance estimate across voxels, derived from baseline or residuals, is provided as\n$$\n\\hat{\\Sigma} = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0.5\n\\end{pmatrix}.\n$$\nUsing the foundational definitions of Representational Similarity Analysis (RSA), where dissimilarity is quantified by distances in a noise-normalized representational space, and using cross-validation across split halves to obtain an unbiased estimate of the squared dissimilarity, compute the cross-validated squared Mahalanobis distance (the crossnobis distance) between conditions $A$ and $B$ from the data given. Round your answer to four significant figures. Additionally, based on the underlying statistical principles, explain how negative values of this estimator should be interpreted in terms of the representational geometry and noise.",
            "solution": "The goal is to compute a cross-validated, noise-normalized dissimilarity between conditions $A$ and $B$, consistent with Representational Similarity Analysis (RSA). We start from fundamental definitions:\n\n1. In RSA, a representational dissimilarity between two condition-specific pattern means is often quantified by a distance in a space where noise has been normalized, so that voxels with higher noise variance contribute less, and correlations are adjusted by the covariance structure. A well-tested choice is the Mahalanobis distance, which uses the inverse noise covariance (the precision) to define the metric.\n\n2. Cross-validation is used to obtain an unbiased estimate of the true squared distance by computing the inner product of independently estimated pattern differences. When two independent halves are available, the unbiased estimator is formed by taking the difference of the pattern estimates in each half and computing their inner product in the noise-normalized space.\n\n3. Concretely, let $\\Delta^{(1)} = \\mathbf{b}_{A}^{(1)} - \\mathbf{b}_{B}^{(1)}$ and $\\Delta^{(2)} = \\mathbf{b}_{A}^{(2)} - \\mathbf{b}_{B}^{(2)}$. Let the precision matrix be $\\hat{\\Sigma}^{-1}$. The cross-validated squared Mahalanobis distance (crossnobis) is given by the inner product of the two difference vectors in the Mahalanobis metric, which can be written as\n$$\nd_{\\text{cross}} = \\left(\\Delta^{(1)}\\right)^{\\top} \\hat{\\Sigma}^{-1} \\Delta^{(2)}.\n$$\nThis arises from prewhitening by $\\hat{\\Sigma}^{-1/2}$: in the whitened space, the squared Euclidean distance equals the Mahalanobis distance, and the cross-product across independent halves is an unbiased estimator of the true squared distance.\n\nWe now compute each component step by step.\n\nCompute the split-half differences:\n$$\n\\Delta^{(1)} = \\mathbf{b}_{A}^{(1)} - \\mathbf{b}_{B}^{(1)} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\end{pmatrix}.\n$$\n$$\n\\Delta^{(2)} = \\mathbf{b}_{A}^{(2)} - \\mathbf{b}_{B}^{(2)} = \\begin{pmatrix} 1.1 \\\\ 2.2 \\\\ 1.7 \\end{pmatrix} - \\begin{pmatrix} 1.2 \\\\ 1.3 \\\\ 1.0 \\end{pmatrix} = \\begin{pmatrix} -0.1 \\\\ 0.9 \\\\ 0.7 \\end{pmatrix}.\n$$\n\nCompute the precision matrix. Since $\\hat{\\Sigma}$ is diagonal, its inverse is the diagonal of reciprocals:\n$$\n\\hat{\\Sigma}^{-1} = \\begin{pmatrix}\n\\frac{1}{2} & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & \\frac{1}{0.5}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.5 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}.\n$$\n\nNow compute the crossnobis distance:\n$$\nd_{\\text{cross}} = \\left(\\Delta^{(1)}\\right)^{\\top} \\hat{\\Sigma}^{-1} \\Delta^{(2)}.\n$$\nFirst compute $\\hat{\\Sigma}^{-1} \\Delta^{(2)}$:\n$$\n\\hat{\\Sigma}^{-1} \\Delta^{(2)} = \n\\begin{pmatrix}\n0.5 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\n-0.1 \\\\ 0.9 \\\\ 0.7\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.5 \\cdot (-0.1) \\\\ 1 \\cdot 0.9 \\\\ 2 \\cdot 0.7\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-0.05 \\\\ 0.9 \\\\ 1.4\n\\end{pmatrix}.\n$$\nThen compute the inner product with $\\Delta^{(1)}$:\n$$\nd_{\\text{cross}} = \n\\begin{pmatrix}\n1 & -1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\n-0.05 \\\\ 0.9 \\\\ 1.4\n\\end{pmatrix}\n=\n(1)(-0.05) + (-1)(0.9) + (-1)(1.4)\n=\n-0.05 - 0.9 - 1.4\n=\n-2.35.\n$$\n\nRounding to four significant figures gives $-2.350$.\n\nInterpretation of possible negative values: The cross-validated squared Mahalanobis distance is an unbiased estimator of the true squared representational distance in the noise-normalized space. The true squared distance is non-negative. However, because the estimator uses independent noisy estimates from split halves, its sampling distribution can include negative values when the true distance is small relative to noise. A negative estimate therefore indicates that, under the null hypothesis of no true difference, the expected value is $0$, and the observed negative value is consistent with noise fluctuations; it suggests that the evidence for a non-zero representational separation is weak or absent. In other words, negative values should not be interpreted as a negative true distance (which is impossible), but as an unbiased estimator fluctuating around zero due to noise, providing information about uncertainty in the representational geometry.",
            "answer": "$$\\boxed{-2.350}$$"
        },
        {
            "introduction": "Once we have a data RDM, we can test how well it correlates with theoretical model RDMs, but how high can we expect this correlation to be? The \"noise ceiling\" provides a vital benchmark by estimating the performance of the (unknown) true model given the level of noise and variability in the data. This practice demonstrates how to compute the upper and lower bounds of the noise ceiling from multi-subject data, providing a practical tool to determine if a model's performance is limited by its own inadequacy or by the quality of the data itself .",
            "id": "4190804",
            "problem": "A neuroscience study applies Representational Similarity Analysis (RSA) to functional neuroimaging data from $3$ subjects viewing $4$ stimulus conditions $\\{A,B,C,D\\}$. For each subject, a Representational Dissimilarity Matrix (RDM) is computed using a cross-validated dissimilarity measure on neural response patterns, and the $6$ unique off-diagonal dissimilarities are vectorized in the fixed order $(A,B),(A,C),(A,D),(B,C),(B,D),(C,D)$. Let the subject-specific dissimilarity vectors be\n$$\n\\text{Subject }1: \\; [\\,0.9,\\;0.2,\\;0.4,\\;0.3,\\;0.6,\\;0.5\\,], \\\\\n\\text{Subject }2: \\; [\\,0.8,\\;0.25,\\;0.35,\\;0.4,\\;0.65,\\;0.45\\,], \\\\\n\\text{Subject }3: \\; [\\,0.85,\\;0.15,\\;0.32,\\;0.38,\\;0.7,\\;0.55\\,].\n$$\nUsing only foundational definitions from RSA, compute the upper and lower bounds of the noise ceiling for this dataset as follows:\n- Compute the group-average RDM vector as the element-wise mean across subjects. Define the upper bound as the mean Spearman rank correlation across subjects between each subject’s vectorized RDM and the group-average RDM computed using all subjects.\n- For the lower bound, for each subject, compute the leave-one-out group-average RDM vector (excluding that subject), then take the mean Spearman rank correlation between each subject’s vectorized RDM and its corresponding leave-one-out group-average RDM.\n\nUse Spearman rank correlation on the $6$ off-diagonal entries, with ranks assigned so that rank $1$ corresponds to the smallest dissimilarity and rank $6$ to the largest, and treat any ties (none occur here) by average ranks. In your reasoning, explain why the upper bound is optimistic and the lower bound conservative, referencing only core definitions and well-tested facts. Express the final answer as a row matrix containing, in order, the lower bound and the upper bound, using exact values (no rounding).",
            "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- Number of subjects: $3$.\n- Number of stimulus conditions: $4$, denoted $\\{A,B,C,D\\}$.\n- Representational Dissimilarity Matrix (RDM) vectorization: The $6$ unique off-diagonal dissimilarities are vectorized for each subject in the fixed order $(A,B), (A,C), (A,D), (B,C), (B,D), (C,D)$.\n- Subject-specific dissimilarity vectors ($\\mathbf{d}_i$):\n  - $\\mathbf{d}_1 = [\\,0.9, 0.2, 0.4, 0.3, 0.6, 0.5\\,]$\n  - $\\mathbf{d}_2 = [\\,0.8, 0.25, 0.35, 0.4, 0.65, 0.45\\,]$\n  - $\\mathbf{d}_3 = [\\,0.85, 0.15, 0.32, 0.38, 0.7, 0.55\\,]$\n- Correlation measure: Spearman rank correlation ($\\rho$).\n- Ranking convention: Rank $1$ for the smallest value to rank $6$ for the largest.\n- Upper bound definition: The mean Spearman correlation between each subject’s RDM vector and the group-average RDM vector calculated from all subjects.\n- Lower bound definition: The mean Spearman correlation between each subject’s RDM vector and the group-average RDM vector calculated by leaving that subject out.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated for validity.\n- **Scientifically Grounded**: The problem describes the calculation of the noise ceiling in Representational Similarity Analysis (RSA), a standard and well-validated technique in computational neuroscience. The definitions for the upper and lower bounds are consistent with established literature (e.g., Nili et al., 2014, PLoS Comp. Bio.). The procedure is scientifically sound.\n- **Well-Posed**: The problem provides all necessary numerical data and unambiguously defines the procedures for calculating the upper and lower bounds. A unique, stable, and meaningful solution exists.\n- **Objective**: The problem is stated using precise, objective language and quantitative data. It is free from subjective or opinion-based claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be derived.\n\nThe solution proceeds as follows. First, we convert the dissimilarity vectors for each subject into rank vectors. Let $n=6$ be the number of dissimilarities.\n\nThe dissimilarity vector for Subject $1$ is $\\mathbf{d}_1 = [\\,0.9, 0.2, 0.4, 0.3, 0.6, 0.5\\,]$. Sorting these values gives $0.2 < 0.3 < 0.4 < 0.5 < 0.6 < 0.9$, which correspond to ranks $1, 2, 3, 4, 5, 6$. The rank vector $\\mathbf{r}_1$ is therefore $\\mathbf{r}_1 = [\\,6, 1, 3, 2, 5, 4\\,]$.\n\nThe dissimilarity vector for Subject $2$ is $\\mathbf{d}_2 = [\\,0.8, 0.25, 0.35, 0.4, 0.65, 0.45\\,]$. Sorting these values gives $0.25 < 0.35 < 0.4 < 0.45 < 0.65 < 0.8$. The rank vector $\\mathbf{r}_2$ is $\\mathbf{r}_2 = [\\,6, 1, 2, 3, 5, 4\\,]$.\n\nThe dissimilarity vector for Subject $3$ is $\\mathbf{d}_3 = [\\,0.85, 0.15, 0.32, 0.38, 0.7, 0.55\\,]$. Sorting these values gives $0.15 < 0.32 < 0.38 < 0.55 < 0.7 < 0.85$. The rank vector $\\mathbf{r}_3$ is $\\mathbf{r}_3 = [\\,6, 1, 2, 3, 5, 4\\,]$.\nWe note that $\\mathbf{r}_2 = \\mathbf{r}_3$.\n\nThe Spearman rank correlation $\\rho$ between two rank vectors $\\mathbf{x}$ and $\\mathbf{y}$ of length $n$ with no ties can be calculated using the formula for Pearson's correlation on the ranks, or more simply with $\\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2-1)}$, where $d_i = x_i - y_i$. For $n=6$, the denominator is $6(6^2-1) = 6(35) = 210$.\n\n**Calculation of the Upper Bound**\n\nFirst, we compute the group-average RDM vector $\\overline{\\mathbf{d}}$ by taking the element-wise mean of the subject vectors:\n$$ \\overline{\\mathbf{d}} = \\frac{1}{3}(\\mathbf{d}_1 + \\mathbf{d}_2 + \\mathbf{d}_3) $$\n$$ \\overline{\\mathbf{d}} = \\frac{1}{3} \\begin{pmatrix} 0.9+0.8+0.85 \\\\ 0.2+0.25+0.15 \\\\ 0.4+0.35+0.32 \\\\ 0.3+0.4+0.38 \\\\ 0.6+0.65+0.7 \\\\ 0.5+0.45+0.55 \\end{pmatrix}^T = \\frac{1}{3}[2.55, 0.60, 1.07, 1.08, 1.95, 1.50]^T $$\n$$ \\overline{\\mathbf{d}} = [0.85, 0.2, 0.356\\overline{6}, 0.36, 0.65, 0.5]^T $$\nSorting the elements of $\\overline{\\mathbf{d}}$ gives $0.2 < 0.356\\overline{6} < 0.36 < 0.5 < 0.65 < 0.85$. The corresponding rank vector is $\\mathbf{r}_{\\text{avg}} = [\\,6, 1, 2, 3, 5, 4\\,]$.\n\nNow, we compute the Spearman correlation between each subject's rank vector and $\\mathbf{r}_{\\text{avg}}$.\n1. For Subject 1: $\\rho(\\mathbf{r}_1, \\mathbf{r}_{\\text{avg}})$.\n$\\mathbf{r}_1 = [\\,6, 1, 3, 2, 5, 4\\,]$, $\\mathbf{r}_{\\text{avg}} = [\\,6, 1, 2, 3, 5, 4\\,]$.\nThe rank differences are $d = \\mathbf{r}_1 - \\mathbf{r}_{\\text{avg}} = [\\,0, 0, 1, -1, 0, 0\\,]$.\n$\\sum d_i^2 = 0^2+0^2+1^2+(-1)^2+0^2+0^2 = 2$.\n$\\rho_1 = 1 - \\frac{6 \\times 2}{210} = 1 - \\frac{12}{210} = 1 - \\frac{2}{35} = \\frac{33}{35}$.\n\n2. For Subject 2: $\\rho(\\mathbf{r}_2, \\mathbf{r}_{\\text{avg}})$.\n$\\mathbf{r}_2 = [\\,6, 1, 2, 3, 5, 4\\,]$ and $\\mathbf{r}_{\\text{avg}} = [\\,6, 1, 2, 3, 5, 4\\,]$.\nThe vectors are identical, so the correlation is $\\rho_2 = 1$.\n\n3. For Subject 3: $\\rho(\\mathbf{r}_3, \\mathbf{r}_{\\text{avg}})$.\n$\\mathbf{r}_3 = [\\,6, 1, 2, 3, 5, 4\\,]$ and $\\mathbf{r}_{\\text{avg}} = [\\,6, 1, 2, 3, 5, 4\\,]$.\nThe vectors are identical, so the correlation is $\\rho_3 = 1$.\n\nThe upper bound of the noise ceiling is the mean of these correlations:\n$$ U = \\frac{1}{3}(\\rho_1 + \\rho_2 + \\rho_3) = \\frac{1}{3}\\left(\\frac{33}{35} + 1 + 1\\right) = \\frac{1}{3}\\left(\\frac{33+35+35}{35}\\right) = \\frac{1}{3}\\left(\\frac{103}{35}\\right) = \\frac{103}{105}. $$\n\n**Calculation of the Lower Bound**\n\nThe lower bound is computed using a leave-one-out procedure.\n1. For Subject 1: We compute the average RDM of Subjects 2 and 3, $\\overline{\\mathbf{d}}_{\\text{not }1} = \\frac{1}{2}(\\mathbf{d}_2 + \\mathbf{d}_3)$.\n$$ \\overline{\\mathbf{d}}_{\\text{not }1} = \\frac{1}{2}[1.65, 0.40, 0.67, 0.78, 1.35, 1.00]^T = [0.825, 0.2, 0.335, 0.39, 0.675, 0.5]^T $$\nThe rank vector is $\\mathbf{r}_{\\text{not }1} = [\\,6, 1, 2, 3, 5, 4\\,]$.\nThe correlation is $\\rho'_1 = \\rho(\\mathbf{r}_1, \\mathbf{r}_{\\text{not }1}) = \\rho([\\,6, 1, 3, 2, 5, 4\\,], [\\,6, 1, 2, 3, 5, 4\\,])$. This is the same calculation as for $\\rho_1$ above, so $\\rho'_1 = \\frac{33}{35}$.\n\n2. For Subject 2: We compute the average of Subjects 1 and 3, $\\overline{\\mathbf{d}}_{\\text{not }2} = \\frac{1}{2}(\\mathbf{d}_1 + \\mathbf{d}_3)$.\n$$ \\overline{\\mathbf{d}}_{\\text{not }2} = \\frac{1}{2}[1.75, 0.35, 0.72, 0.68, 1.30, 1.05]^T = [0.875, 0.175, 0.36, 0.34, 0.65, 0.525]^T $$\nThe rank vector is $\\mathbf{r}_{\\text{not }2} = [\\,6, 1, 3, 2, 5, 4\\,]$.\nThe correlation is $\\rho'_2 = \\rho(\\mathbf{r}_2, \\mathbf{r}_{\\text{not }2}) = \\rho([\\,6, 1, 2, 3, 5, 4\\,], [\\,6, 1, 3, 2, 5, 4\\,])$. The rank difference vector is $d = [0, 0, -1, 1, 0, 0]$, so $\\sum d_i^2 = 2$.\n$\\rho'_2 = 1 - \\frac{6 \\times 2}{210} = \\frac{33}{35}$.\n\n3. For Subject 3: We compute the average of Subjects 1 and 2, $\\overline{\\mathbf{d}}_{\\text{not }3} = \\frac{1}{2}(\\mathbf{d}_1 + \\mathbf{d}_2)$.\n$$ \\overline{\\mathbf{d}}_{\\text{not }3} = \\frac{1}{2}[1.70, 0.45, 0.75, 0.70, 1.25, 0.95]^T = [0.85, 0.225, 0.375, 0.35, 0.625, 0.475]^T $$\nThe rank vector is $\\mathbf{r}_{\\text{not }3} = [\\,6, 1, 3, 2, 5, 4\\,]$.\nThe correlation is $\\rho'_3 = \\rho(\\mathbf{r}_3, \\mathbf{r}_{\\text{not }3}) = \\rho([\\,6, 1, 2, 3, 5, 4\\,], [\\,6, 1, 3, 2, 5, 4\\,])$. This is the same calculation as for $\\rho'_2$, so $\\rho'_3 = \\frac{33}{35}$.\n\nThe lower bound of the noise ceiling is the mean of these leave-one-out correlations:\n$$ L = \\frac{1}{3}(\\rho'_1 + \\rho'_2 + \\rho'_3) = \\frac{1}{3}\\left(\\frac{33}{35} + \\frac{33}{35} + \\frac{33}{35}\\right) = \\frac{33}{35}. $$\n\n**Conceptual Rationale**\n\nThe upper bound is deemed \"optimistic\" because it is statistically biased. For each subject, the correlation is computed against a group average that includes that subject's own data. This circularity, a form of non-independence often called \"double-dipping,\" means that the subject's noise is correlated with itself, artificially inflating the correlation coefficient. The group average is \"pulled\" toward the subject's RDM, making the similarity appear greater than it would if compared against a truly independent estimate of the group's central tendency. This procedure overestimates the true reliability of the data.\n\nThe lower bound is \"conservative\" because it corrects for this bias by using a strict leave-one-out cross-validation. Each subject's RDM is compared to a group average from which it was excluded, ensuring statistical independence between the test data (the subject's RDM) and the training data (the remaining subjects' RDMs used to form the average). However, this leave-one-out average is computed from a smaller sample ($N-1$ subjects) than the full group average ($N$ subjects). An average from a smaller sample is a noisier, less stable estimate of the true, unobservable population RDM. Correlating a subject's RDM with this noisier estimate will, on average, yield a lower value than a correlation with the true population RDM would. Therefore, this method provides a pessimistic estimate, or a lower bound, on the true reliability. The true reliability is expected to lie between these two bounds.",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{33}{35} & \\frac{103}{105} \\end{pmatrix}} $$"
        }
    ]
}