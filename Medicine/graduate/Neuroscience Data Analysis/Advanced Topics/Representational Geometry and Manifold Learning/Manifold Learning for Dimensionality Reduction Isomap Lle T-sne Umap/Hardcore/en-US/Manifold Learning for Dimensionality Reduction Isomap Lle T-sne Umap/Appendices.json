{
    "hands_on_practices": [
        {
            "introduction": "Locally Linear Embedding (LLE) operates on the elegant assumption that each data point can be reconstructed from a linear combination of its neighbors. A key step is finding these reconstruction weights, but this can be unstable if a neighborhood is ill-conditioned, a common issue with noisy experimental data. This practice demonstrates how Tikhonov regularization addresses this by adding a penalty term to the LLE objective, ensuring robust and stable embeddings by managing the trade-off between reconstruction accuracy and weight stability .",
            "id": "4176802",
            "problem": "A neuroscientist applies Locally Linear Embedding (LLE) to a high-dimensional neural population activity dataset, seeking a faithful low-dimensional embedding of the observed states. For a fixed state indexed by $i$, let the $K$ nearest neighbor differences be collected as columns of the matrix $Z_i \\in \\mathbb{R}^{d \\times K}$, where $d$ is the number of recorded neurons and $K \\geq 2$. The LLE reconstruction weights $w_i \\in \\mathbb{R}^{K}$ are defined to minimize the local reconstruction energy subject to a barycentric constraint. Begin from the following fundamental base:\n\n- The LLE reconstruction energy for state $i$ is the squared norm of the linear combination of neighbor offsets, written as $\\|Z_i w_i\\|_2^2$, with the affine constraint $\\sum_{j=1}^{K} w_{i,j} = 1$.\n- The local covariance (Gram) matrix is $C_i = Z_i^{\\top} Z_i \\in \\mathbb{R}^{K \\times K}$, which is symmetric and positive semidefinite.\n- Tikhonov regularization is the addition of a quadratic penalty on the parameter vector, that is, a term $\\lambda \\|w_i\\|_2^2$ with $\\lambda > 0$.\n\nPart 1. Show that augmenting the LLE objective with Tikhonov regularization, $\\|Z_i w_i\\|_2^2 + \\lambda \\|w_i\\|_2^2$ subject to $\\sum_{j=1}^{K} w_{i,j} = 1$, is equivalent to replacing $C_i$ with $C_i + \\lambda I$ in the normal equations for the constrained minimizer.\n\nPart 2. Assume that $C_i$ has the eigendecomposition $C_i = U \\,\\mathrm{diag}(\\mu_1, \\dots, \\mu_K)\\, U^{\\top}$ with orthonormal columns $U = [u_1 \\ \\cdots \\ u_K]$ and eigenvalues $\\mu_k \\geq 0$. Let $p \\in \\mathbb{R}^{K}$ denote the constraint vector $p = \\mathbf{1}$ and define the coefficients $\\alpha_k = u_k^{\\top} p$ for $k = 1, \\dots, K$. Suppose the observed neighbor activity vectors are corrupted by independent, mean-zero Gaussian noise with per-neuron variance $\\sigma^2$ in each coordinate, so that the per-coordinate expected squared reconstruction error due solely to noise is $E_i(\\lambda) = \\sigma^2 \\sum_{j=1}^{K} w_{i,j}(\\lambda)^2$, where $w_{i}(\\lambda)$ is the regularized LLE weight vector obtained in Part 1.\n\nDerive a closed-form analytic expression for $E_i(\\lambda)$ in terms of $\\lambda$, $\\{\\mu_k\\}_{k=1}^{K}$, and $\\{\\alpha_k\\}_{k=1}^{K}$, expressing your final answer as a single simplified symbolic expression. No numerical approximation is required; do not round. Express your final answer without any units as an analytic function of the given symbols.",
            "solution": "The problem is subjected to validation and is confirmed to be scientifically grounded, well-posed, and objective, with all necessary information provided for a rigorous mathematical derivation.\n\nThe problem is divided into two parts. We will address them sequentially.\n\nPart 1: Equivalence of Regularized Objective and Modified Normal Equations\n\nThe objective is to find the reconstruction weights $w_i \\in \\mathbb{R}^{K}$ that minimize the regularized reconstruction energy, subject to a barycentric constraint. The optimization problem is:\n$$\n\\underset{w_i}{\\text{minimize}} \\quad \\|Z_i w_i\\|_2^2 + \\lambda \\|w_i\\|_2^2\n$$\n$$\n\\text{subject to} \\quad \\sum_{j=1}^{K} w_{i,j} = 1\n$$\nwhere $w_i = [w_{i,1}, \\dots, w_{i,K}]^{\\top}$.\n\nFirst, we rewrite the objective function in matrix form. The first term is a quadratic form:\n$$\n\\|Z_i w_i\\|_2^2 = (Z_i w_i)^{\\top} (Z_i w_i) = w_i^{\\top} Z_i^{\\top} Z_i w_i\n$$\nUsing the definition of the local covariance matrix, $C_i = Z_i^{\\top} Z_i$, this term becomes $w_i^{\\top} C_i w_i$.\nThe regularization term is $\\|w_i\\|_2^2 = w_i^{\\top} w_i = w_i^{\\top} I w_i$, where $I$ is the $K \\times K$ identity matrix.\nCombining these, the objective function $J(w_i)$ is:\n$$\nJ(w_i) = w_i^{\\top} C_i w_i + \\lambda w_i^{\\top} I w_i = w_i^{\\top} (C_i + \\lambda I) w_i\n$$\nThe constraint can be written in vector notation as $\\mathbf{1}^{\\top} w_i = 1$, where $\\mathbf{1}$ is a $K$-dimensional column vector of ones.\n\nThe optimization problem is now:\n$$\n\\underset{w_i}{\\text{minimize}} \\quad w_i^{\\top} (C_i + \\lambda I) w_i\n$$\n$$\n\\text{subject to} \\quad \\mathbf{1}^{\\top} w_i = 1\n$$\nTo solve this constrained optimization problem, we use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}(w_i, \\nu)$ is:\n$$\n\\mathcal{L}(w_i, \\nu) = w_i^{\\top} (C_i + \\lambda I) w_i - \\nu (\\mathbf{1}^{\\top} w_i - 1)\n$$\nwhere $\\nu$ is the Lagrange multiplier. For a minimum, the gradient of the Lagrangian with respect to $w_i$ must be zero. Let $G_{\\lambda} = C_i + \\lambda I$. Since $C_i$ is symmetric, $G_{\\lambda}$ is also symmetric. The gradient is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 2(C_i + \\lambda I) w_i - \\nu \\mathbf{1} = 0\n$$\nThis gives the first of the normal equations:\n$$\n(C_i + \\lambda I) w_i = \\frac{\\nu}{2} \\mathbf{1}\n$$\nThe second normal equation is the constraint itself, $\\mathbf{1}^{\\top} w_i = 1$. The system of equations $\\{ (C_i + \\lambda I) w_i = \\eta \\mathbf{1}, \\mathbf{1}^{\\top} w_i = 1 \\}$, where $\\eta = \\nu/2$ is a constant to be determined, defines the solution for the regularized weights $w_i$.\n\nNow, consider the unregularized LLE problem. The objective is to minimize $w_i^{\\top} C_i w_i$ subject to $\\mathbf{1}^{\\top} w_i = 1$. The corresponding normal equations are derived in the same manner, yielding $\\{ C_i w_i = \\eta_0 \\mathbf{1}, \\mathbf{1}^{\\top} w_i = 1 \\}$ for some multiplier $\\eta_0$.\n\nBy comparing the two systems of normal equations, it is evident that the equations for the regularized problem are obtained from the equations for the unregularized problem by simply replacing the matrix $C_i$ with $C_i + \\lambda I$. This proves the statement in Part 1.\n\nPart 2: Derivation of the Expected Reconstruction Error\n\nThe goal is to derive an expression for $E_i(\\lambda) = \\sigma^2 \\|w_i(\\lambda)\\|_2^2$. To do this, we first need to find an explicit expression for the weight vector $w_i(\\lambda)$, which we denote as $w_i$ for brevity.\n\nFrom Part 1, we have the system:\n1. $(C_i + \\lambda I) w_i = \\eta \\mathbf{1}$\n2. $\\mathbf{1}^{\\top} w_i = 1$\n\nSince $C_i$ is positive semidefinite and $\\lambda > 0$, the matrix $C_i + \\lambda I$ is symmetric and positive definite, and thus invertible. From equation (1), we can solve for $w_i$:\n$$\nw_i = \\eta (C_i + \\lambda I)^{-1} \\mathbf{1}\n$$\nNow, we use equation (2) to find the multiplier $\\eta$:\n$$\n\\mathbf{1}^{\\top} (\\eta (C_i + \\lambda I)^{-1} \\mathbf{1}) = 1 \\implies \\eta \\left( \\mathbf{1}^{\\top} (C_i + \\lambda I)^{-1} \\mathbf{1} \\right) = 1\n$$\n$$\n\\eta = \\frac{1}{\\mathbf{1}^{\\top} (C_i + \\lambda I)^{-1} \\mathbf{1}}\n$$\nSubstituting $\\eta$ back into the expression for $w_i$:\n$$\nw_i = \\frac{(C_i + \\lambda I)^{-1} \\mathbf{1}}{\\mathbf{1}^{\\top} (C_i + \\lambda I)^{-1} \\mathbf{1}}\n$$\nWe need to compute $\\|w_i\\|_2^2 = w_i^{\\top} w_i$. Let $S = \\mathbf{1}^{\\top} (C_i + \\lambda I)^{-1} \\mathbf{1}$. Then $w_i = S^{-1} (C_i + \\lambda I)^{-1} \\mathbf{1}$.\n$$\n\\|w_i\\|_2^2 = \\left( S^{-1} (C_i + \\lambda I)^{-1} \\mathbf{1} \\right)^{\\top} \\left( S^{-1} (C_i + \\lambda I)^{-1} \\mathbf{1} \\right)\n$$\n$$\n\\|w_i\\|_2^2 = S^{-2} \\mathbf{1}^{\\top} \\left((C_i + \\lambda I)^{-1}\\right)^{\\top} (C_i + \\lambda I)^{-1} \\mathbf{1}\n$$\nSince $C_i + \\lambda I$ is symmetric, its inverse is also symmetric. Thus, $\\left((C_i + \\lambda I)^{-1}\\right)^{\\top} = (C_i + \\lambda I)^{-1}$.\n$$\n\\|w_i\\|_2^2 = S^{-2} \\mathbf{1}^{\\top} (C_i + \\lambda I)^{-2} \\mathbf{1} = \\frac{\\mathbf{1}^{\\top} (C_i + \\lambda I)^{-2} \\mathbf{1}}{\\left( \\mathbf{1}^{\\top} (C_i + \\lambda I)^{-1} \\mathbf{1} \\right)^2}\n$$\nNow we introduce the eigendecomposition $C_i = U \\Lambda U^{\\top}$, where $\\Lambda = \\mathrm{diag}(\\mu_1, \\dots, \\mu_K)$.\nThe term $C_i + \\lambda I$ becomes:\n$$\nC_i + \\lambda I = U \\Lambda U^{\\top} + \\lambda U I U^{\\top} = U (\\Lambda + \\lambda I) U^{\\top}\n$$\nIts inverse is $(C_i + \\lambda I)^{-1} = U (\\Lambda + \\lambda I)^{-1} U^{\\top}$.\nAnd its inverse squared is $(C_i + \\lambda I)^{-2} = U (\\Lambda + \\lambda I)^{-2} U^{\\top}$.\nThe diagonal matrix $(\\Lambda + \\lambda I)^{-1}$ has diagonal entries $1/(\\mu_k + \\lambda)$, and $(\\Lambda + \\lambda I)^{-2}$ has diagonal entries $1/(\\mu_k + \\lambda)^2$.\n\nLet's evaluate the numerator and the denominator of the expression for $\\|w_i\\|_2^2$. We use the given definitions $p = \\mathbf{1}$ and $\\alpha_k = u_k^{\\top} p$. The vector $\\alpha = [\\alpha_1, \\dots, \\alpha_K]^{\\top}$ is given by $\\alpha = U^{\\top} p$.\n\nThe denominator term is:\n$$\n\\mathbf{1}^{\\top} (C_i + \\lambda I)^{-1} \\mathbf{1} = p^{\\top} U (\\Lambda + \\lambda I)^{-1} U^{\\top} p = (U^{\\top}p)^{\\top} (\\Lambda + \\lambda I)^{-1} (U^{\\top}p)\n$$\n$$\n= \\alpha^{\\top} (\\Lambda + \\lambda I)^{-1} \\alpha = \\sum_{k=1}^{K} \\frac{\\alpha_k^2}{\\mu_k + \\lambda}\n$$\nThe numerator term is:\n$$\n\\mathbf{1}^{\\top} (C_i + \\lambda I)^{-2} \\mathbf{1} = p^{\\top} U (\\Lambda + \\lambda I)^{-2} U^{\\top} p = (U^{\\top}p)^{\\top} (\\Lambda + \\lambda I)^{-2} (U^{\\top}p)\n$$\n$$\n= \\alpha^{\\top} (\\Lambda + \\lambda I)^{-2} \\alpha = \\sum_{k=1}^{K} \\frac{\\alpha_k^2}{(\\mu_k + \\lambda)^2}\n$$\nSubstituting these back into the expression for $\\|w_i\\|_2^2$:\n$$\n\\|w_i\\|_2^2 = \\frac{\\sum_{k=1}^{K} \\frac{\\alpha_k^2}{(\\mu_k + \\lambda)^2}}{\\left( \\sum_{k=1}^{K} \\frac{\\alpha_k^2}{\\mu_k + \\lambda} \\right)^2}\n$$\nFinally, the expected squared reconstruction error due to noise, $E_i(\\lambda) = \\sigma^2 \\|w_i\\|_2^2$, is given by:\n$$\nE_i(\\lambda) = \\sigma^2 \\frac{\\sum_{k=1}^{K} \\frac{\\alpha_k^2}{(\\mu_k + \\lambda)^2}}{\\left( \\sum_{k=1}^{K} \\frac{\\alpha_k^2}{\\mu_k + \\lambda} \\right)^2}\n$$\nThis is the closed-form analytic expression for $E_i(\\lambda)$ in terms of the given quantities.",
            "answer": "$$\n\\boxed{\\sigma^{2} \\frac{\\sum_{k=1}^{K} \\frac{\\alpha_k^2}{(\\mu_k + \\lambda)^{2}}}{\\left( \\sum_{k=1}^{K} \\frac{\\alpha_k^2}{\\mu_k + \\lambda} \\right)^{2}}}\n$$"
        },
        {
            "introduction": "Unlike locally-focused methods, Isometric Feature Mapping (Isomap) aims to preserve the global geometry of a manifold by approximating geodesic distances. A practical limitation of many manifold algorithms, however, is the lack of a function to map new, out-of-sample points into the embedding. This exercise addresses this critical issue by guiding you through the derivation of a Nyström extension, a powerful technique that projects new data into an existing Isomap embedding, making the model far more useful in dynamic research settings .",
            "id": "4176781",
            "problem": "Consider a low-dimensional manifold of neural population trajectories in a motor cortex recording session. A subset of $m$ landmark trials is selected, and Isometric Feature Mapping (Isomap) is constructed in the landmark-only setting. Let the squared geodesic distance matrix among landmarks be denoted by $D_{L}^{2} \\in \\mathbb{R}^{m \\times m}$, where the $(i,j)$ entry equals the square of the geodesic distance between the $i$-th and $j$-th landmark trials, measured along a $k$-nearest neighbor graph built from temporally aligned activity vectors. Classical Multidimensional Scaling (MDS) is applied to the landmark geodesic distances using the double-centering matrix $H = I_{m} - \\frac{1}{m} \\mathbf{1}\\mathbf{1}^{\\top}$ to form the centered Gram matrix $B = -\\frac{1}{2} H D_{L}^{2} H$. The top $p$ eigenpairs of $B$ are retained, with $U_{p} \\in \\mathbb{R}^{m \\times p}$ containing orthonormal eigenvectors and $\\Lambda_{p} \\in \\mathbb{R}^{p \\times p}$ a diagonal matrix of positive eigenvalues, yielding the landmark embedding coordinates $Z = U_{p} \\Lambda_{p}^{1/2} \\in \\mathbb{R}^{m \\times p}$.\n\nA new trial $x_{\\text{new}}$ is recorded, and its geodesic distances to the $m$ landmarks are estimated via shortest paths on the same $k$-nearest neighbor graph, producing a vector $d_{\\text{new}} \\in \\mathbb{R}^{m}$ of distances to each landmark. Define the element-wise square of a vector by $a^{\\odot 2} = a \\odot a$, where $\\odot$ denotes the Hadamard product.\n\nStarting only from the fundamental relationship between squared Euclidean distances and the centered Gram matrix used in Classical Multidimensional Scaling (MDS), derive a Nyström extension that projects $x_{\\text{new}}$ into the existing $p$-dimensional Isomap embedding using its geodesic distances to the landmarks. Express the final answer as a single closed-form analytic expression for the embedding coordinates of $x_{\\text{new}}$ in terms of $H$, $U_{p}$, $\\Lambda_{p}$, and $d_{\\text{new}}$, using the definition of $a^{\\odot 2}$ above.\n\nAdditionally, discuss how additive errors in the geodesic distance vector, modeled as $d_{\\text{new}} \\mapsto d_{\\text{new}} + \\Delta d_{\\text{new}}$, propagate to the embedded coordinates through first-order linearization. Your discussion should start from the above base and reason to a quantitative bound in operator norm, but the answer to the problem must be the single analytic expression specified for the embedding coordinates. No rounding is required, and no physical units or angles are involved.",
            "solution": "### Derivation of the Embedding Coordinates\nThe objective is to find the embedding coordinates $z_{\\text{new}} \\in \\mathbb{R}^{p}$ for the new trial $x_{\\text{new}}$. The derivation begins from the fundamental relationship in Multidimensional Scaling (MDS), which relates distances to inner products in the embedding space.\n\nThe landmark coordinates are given by the rows of $Z = U_p \\Lambda_p^{1/2}$. Let these coordinate vectors be $\\{z_i\\}_{i=1}^m \\subset \\mathbb{R}^p$. The Gram matrix is $B = Z Z^\\top$. The use of the centering matrix $H$ ensures that the embedded landmark coordinates are centered at the origin, i.e., $\\sum_{i=1}^m z_i = \\mathbf{0}$.\n\nThe core principle of Isomap is that the geodesic distance between two points on the manifold should be approximated by the Euclidean distance between their corresponding points in the embedding space. For the new point $x_{\\text{new}}$ with embedding coordinates $z_{\\text{new}}$ and the $i$-th landmark with coordinates $z_i$, this relationship is:\n$$(d_{\\text{new}})_i^2 \\approx \\|z_{\\text{new}} - z_i\\|^2$$\nwhere $(d_{\\text{new}})_i$ is the $i$-th component of the vector $d_{\\text{new}}$. Expanding the squared Euclidean norm gives:\n$$(d_{\\text{new}})_i^2 = \\|z_{\\text{new}}\\|^2 - 2 z_i^\\top z_{\\text{new}} + \\|z_i\\|^2$$\nThis equation links the known squared geodesic distance $(d_{\\text{new}})_i^2$ to the unknown coordinate vector $z_{\\text{new}}$. The term $\\|z_i\\|^2$ is the squared norm of the $i$-th landmark's coordinate vector, which is precisely the $i$-th diagonal element of the Gram matrix $B = ZZ^\\top$, so $\\|z_i\\|^2 = B_{ii}$.\n\nThe Nyström method provides a way to compute $z_{\\text{new}}$ from the vector of inner products between the new point and the landmarks. Let $b_{\\text{new}} \\in \\mathbb{R}^m$ be this vector, with components $(b_{\\text{new}})_i = z_i^\\top z_{\\text{new}}$. In matrix form, $b_{\\text{new}} = Z z_{\\text{new}}$. Substituting $Z = U_p \\Lambda_p^{1/2}$, we have $b_{\\text{new}} = U_p \\Lambda_p^{1/2} z_{\\text{new}}$. Since the columns of $U_p$ are orthonormal ($U_p^\\top U_p = I_p$) and the eigenvalues in $\\Lambda_p$ are positive, we can solve for $z_{\\text{new}}$:\n$$z_{\\text{new}} = (\\Lambda_p^{1/2})^{-1} U_p^\\top b_{\\text{new}} = \\Lambda_p^{-1/2} U_p^\\top b_{\\text{new}}$$\nOur task now reduces to finding the vector $b_{\\text{new}}$. From the expanded distance equation, we can write for each $i$:\n$$2 z_i^\\top z_{\\text{new}} = \\|z_{\\text{new}}\\|^2 + \\|z_i\\|^2 - (d_{\\text{new}})_i^2$$\nExpressing this in vector form for all $m$ landmarks:\n$$2 b_{\\text{new}} = \\|z_{\\text{new}}\\|^2 \\mathbf{1} + \\mathbf{diag}(B) - d_{\\text{new}}^{\\odot 2}$$\nwhere $\\mathbf{diag}(B)$ is the vector of diagonal elements of $B$, and $d_{\\text{new}}^{\\odot 2}$ is the element-wise square of $d_{\\text{new}}$. This expression for $b_{\\text{new}}$ depends on the unknown quantity $\\|z_{\\text{new}}\\|^2$.\n\nTo resolve this, we substitute this expression for $b_{\\text{new}}$ into the equation for $z_{\\text{new}}$:\n$$z_{\\text{new}} = \\Lambda_p^{-1/2} U_p^\\top \\left( \\frac{1}{2} (\\|z_{\\text{new}}\\|^2 \\mathbf{1} + \\mathbf{diag}(B) - d_{\\text{new}}^{\\odot 2}) \\right)$$\n$$z_{\\text{new}} = \\frac{1}{2} \\Lambda_p^{-1/2} \\left( \\|z_{\\text{new}}\\|^2 U_p^\\top \\mathbf{1} + U_p^\\top (\\mathbf{diag}(B) - d_{\\text{new}}^{\\odot 2}) \\right)$$\nA critical property of the eigenvectors of a centered Gram matrix $B$ (formed using $H$) is that any eigenvector corresponding to a non-zero eigenvalue is orthogonal to the vector $\\mathbf{1}$. Since the columns of $U_p$ are such eigenvectors, we have $U_p^\\top \\mathbf{1} = \\mathbf{0}$. This property is a direct consequence of the centering operation for which $H$ is defined ($Z^\\top\\mathbf{1} = \\mathbf{0} \\implies \\Lambda_p^{1/2}U_p^\\top\\mathbf{1} = \\mathbf{0} \\implies U_p^\\top\\mathbf{1} = \\mathbf{0}$).\n\nApplying this property, the term involving the unknown norm $\\|z_{\\text{new}}\\|^2$ vanishes:\n$$z_{\\text{new}} = \\frac{1}{2} \\Lambda_p^{-1/2} \\left( \\mathbf{0} + U_p^\\top (\\mathbf{diag}(B) - d_{\\text{new}}^{\\odot 2}) \\right)$$\n$$z_{\\text{new}} = \\frac{1}{2} \\Lambda_p^{-1/2} U_p^\\top (\\mathbf{diag}(B) - d_{\\text{new}}^{\\odot 2})$$\nThe problem requires the answer in terms of $U_p$, $\\Lambda_p$, and $d_{\\text{new}}$ (and implicitly $H$). The term $\\mathbf{diag}(B)$ must be expressed using the provided quantities. Since $B = U_p \\Lambda_p U_p^\\top$, its diagonal is $\\mathbf{diag}(B) = \\mathbf{diag}(U_p \\Lambda_p U_p^\\top)$.\nSubstituting this in provides the final closed-form expression for the embedding coordinates of the new point:\n$$z_{\\text{new}} = \\frac{1}{2} \\Lambda_p^{-1/2} U_p^\\top \\left( \\mathbf{diag}(U_p \\Lambda_p U_p^\\top) - d_{\\text{new}}^{\\odot 2} \\right)$$\n\n### Discussion on Error Propagation\nThe problem also requests a discussion on the propagation of additive errors in the distance vector, $d_{\\text{new}} \\mapsto d'_{\\text{new}} = d_{\\text{new}} + \\Delta d_{\\text{new}}$.\nThe derived expression for the embedding is a function of $d_{\\text{new}}^{\\odot 2}$:\n$$z_{\\text{new}}(d_{\\text{new}}) = \\frac{1}{2} \\Lambda_p^{-1/2} U_p^\\top \\left( \\mathbf{diag}(B) - d_{\\text{new}}^{\\odot 2} \\right)$$\nThe perturbed squared distance vector is $(d'_{\\text{new}})^{\\odot 2} = (d_{\\text{new}} + \\Delta d_{\\text{new}})^{\\odot 2} = d_{\\text{new}}^{\\odot 2} + 2 d_{\\text{new}} \\odot \\Delta d_{\\text{new}} + (\\Delta d_{\\text{new}})^{\\odot 2}$.\nFor a first-order linearization, we assume the error $\\Delta d_{\\text{new}}$ is small and neglect the quadratic term $(\\Delta d_{\\text{new}})^{\\odot 2}$.\nThe change in the embedding coordinates, $\\Delta z_{\\text{new}} = z_{\\text{new}}(d'_{\\text{new}}) - z_{\\text{new}}(d_{\\text{new}})$, is then approximated by:\n$$\\Delta z_{\\text{new}} \\approx \\frac{1}{2} \\Lambda_p^{-1/2} U_p^\\top \\left( \\mathbf{diag}(B) - (d_{\\text{new}}^{\\odot 2} + 2 d_{\\text{new}} \\odot \\Delta d_{\\text{new}}) \\right) - z_{\\text{new}}(d_{\\text{new}})$$\n$$\\Delta z_{\\text{new}} \\approx \\frac{1}{2} \\Lambda_p^{-1/2} U_p^\\top (-2 d_{\\text{new}} \\odot \\Delta d_{\\text{new}})$$\n$$\\Delta z_{\\text{new}} \\approx - \\Lambda_p^{-1/2} U_p^\\top (d_{\\text{new}} \\odot \\Delta d_{\\text{new}})$$\nThis expression can be written as a linear transformation of the error vector $\\Delta d_{\\text{new}}$ via the Jacobian matrix $J = -\\Lambda_p^{-1/2} U_p^\\top \\text{diag}(d_{\\text{new}})$, where $\\text{diag}(d_{\\text{new}})$ is the diagonal matrix with elements of $d_{\\text{new}}$.\nThe magnitude of the output error can be bounded using the operator norm of $J$:\n$$\\|\\Delta z_{\\text{new}}\\|_2 \\le \\|J\\|_2 \\|\\Delta d_{\\text{new}}\\|_2$$\nUsing the submultiplicativity of the operator norm, $\\|J\\|_2 \\le \\|\\Lambda_p^{-1/2}\\|_2 \\|U_p^\\top\\|_2 \\|\\text{diag}(d_{\\text{new}})\\|_2$.\nThe norms are:\n- $\\|\\Lambda_p^{-1/2}\\|_2 = 1/\\sqrt{\\lambda_p}$, where $\\lambda_p$ is the smallest eigenvalue in $\\Lambda_p$.\n- $\\|U_p^\\top\\|_2 = 1$, as its singular values are all $1$.\n- $\\|\\text{diag}(d_{\\text{new}})\\|_2 = \\max_i |(d_{\\text{new}})_i| = \\|d_{\\text{new}}\\|_\\infty$.\n\nThus, a first-order bound on the error is:\n$$\\|\\Delta z_{\\text{new}}\\|_2 \\lesssim \\frac{\\|d_{\\text{new}}\\|_\\infty}{\\sqrt{\\lambda_p}} \\|\\Delta d_{\\text{new}}\\|_2$$\nThis shows that the error in the embedding is amplified by a factor that grows with the maximum distance to any landmark and is inversely proportional to the square root of the smallest eigenvalue retained in the embedding. This highlights a potential instability if small eigenvalues are kept or if the new point is very far from all landmarks.",
            "answer": "$$ \\boxed{ \\frac{1}{2} \\Lambda_{p}^{-1/2} U_{p}^{\\top} \\left( \\mathbf{diag}(U_{p} \\Lambda_{p} U_{p}^{\\top}) - d_{\\text{new}}^{\\odot 2} \\right) } $$"
        },
        {
            "introduction": "t-SNE has become a dominant tool for visualizing high-dimensional data due to its remarkable ability to reveal fine-grained cluster structure. However, its tendency to distort global relationships can be misleading if not properly understood. This exercise takes you directly to the heart of the algorithm's objective function, the Kullback-Leibler divergence, to build a first-principles understanding of why t-SNE excels at preserving local neighborhoods at the expense of global distances .",
            "id": "4176778",
            "problem": "A neural population recording yields a high-dimensional dataset where each sample is a vector of firing rates from $N$ simultaneously recorded neurons. Suppose $N$ is large (e.g., $N$ in the hundreds), and the samples are collected across multiple behavioral contexts, producing two well-separated manifolds of neural states: manifold $\\mathcal{M}_{1}$ (e.g., preparatory states) and manifold $\\mathcal{M}_{2}$ (e.g., execution states). Empirically, for points $i$ and $j$ within the same manifold, the high-dimensional Euclidean distance is small, while for points across manifolds, the distance is large. You apply t-distributed Stochastic Neighbor Embedding (t-SNE) to reduce dimensionality with a standard perplexity tuned to capture local neighborhoods (e.g., around $k$ nearest neighbors). Let $P_{ij}$ denote the pairwise similarity in the high-dimensional space derived from a Gaussian kernel and symmetrized to satisfy $\\sum_{i\\neq j} P_{ij} = 1$, and let $Q_{ij}$ denote the pairwise similarity in the low-dimensional embedded space derived from a Student $t$-distribution and symmetrized to satisfy $\\sum_{i\\neq j} Q_{ij} = 1$. The t-SNE objective is the Kullback–Leibler divergence\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right).\n$$\nConsider the following controlled scenario constructed to reflect typical outcomes in this dataset:\n- For a representative within-manifold neighbor pair, $P_{ij} \\approx 5 \\times 10^{-4}$, while the low-dimensional similarity after embedding is $Q_{ij} \\approx 4 \\times 10^{-4}$.\n- For a representative cross-manifold far pair, $P_{ij} \\approx 1 \\times 10^{-8}$, while due to the heavy tail of the Student $t$-kernel in low dimensions and limited embedding space, $Q_{ij} \\approx 1 \\times 10^{-4}$.\n\nYou are asked to reason from first principles about how t-SNE balances local neighborhood preservation against global distance distortion, using the governing objective above and the qualitative properties of the Gaussian-based $P_{ij}$ and Student $t$-based $Q_{ij}$, together with the empirical scenario. Which option best explains why local neighborhoods are preserved while global distances are distorted, and correctly relates this to the mismatch between $P_{ij}$ and $Q_{ij}$ for far-away pairs?\n\nA. The divergence $D_{\\mathrm{KL}}(P \\parallel Q)$ weights discrepancies by $P_{ij}$, so near-neighbor pairs (with larger $P_{ij}$) dominate the objective. For the given within-manifold pair, the contribution is $P_{ij} \\log(P_{ij}/Q_{ij}) \\approx (5 \\times 10^{-4}) \\log\\left(\\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}}\\right)$, which is orders of magnitude larger than the contribution from the far cross-manifold pair, $(1 \\times 10^{-8}) \\log\\left(\\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}}\\right)$. Because far-away pairs have $P_{ij} \\approx 0$, their mismatch with $Q_{ij}$ is weakly penalized, allowing global distances to be distorted while preserving local neighborhoods.\n\nB. The divergence $D_{\\mathrm{KL}}(Q \\parallel P)$ is minimized in t-SNE, so discrepancies are weighted by $Q_{ij}$, which heavily penalizes large $Q_{ij}$ assigned to far-away pairs and prevents global distortion. Therefore, near-neighbor preservation is a consequence of minimizing $D_{\\mathrm{KL}}(Q \\parallel P)$.\n\nC. Manifold learning methods that preserve global geodesic distances, such as Isometric Mapping (Isomap), prevent global distortions by computing shortest paths on the neighborhood graph; since t-SNE performs a similar geodesic computation implicitly, global distances are preserved and local neighborhoods may be distorted.\n\nD. Increasing perplexity in t-SNE forces $P_{ij}$ to be nearly uniform across all pairs, making $D_{\\mathrm{KL}}(P \\parallel Q)$ weight near and far pairs almost equally. This eliminates global distortion while preserving local neighborhoods.\n\nE. In low dimensions, the Student $t$-kernel used to construct $Q_{ij}$ is short-tailed, driving $Q_{ij} \\approx 0$ for all but the very nearest neighbors. This ensures global distances are preserved because far-away pairs have both $P_{ij}$ and $Q_{ij}$ near zero, keeping the divergence small uniformly across all pairs.\n\nSelect the single best option.",
            "solution": "The problem asks for an explanation of why the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm preserves local neighborhood structures while distorting global distances. The explanation must be derived from the properties of its objective function, the Kullback-Leibler (KL) divergence, and a provided numerical scenario.\n\n### Step 1: Problem Statement Validation\n\nFirst, I will validate the problem statement.\n\n**Givens:**\n- A high-dimensional dataset of neural firing rates from $N$ neurons.\n- The data forms two well-separated manifolds, $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$.\n- High-dimensional similarities are given by $P_{ij}$, derived from a Gaussian kernel, symmetrized, and normalized such that $\\sum_{i\\neq j} P_{ij} = 1$.\n- Low-dimensional similarities are given by $Q_{ij}$, derived from a Student $t$-distribution, symmetrized, and normalized such that $\\sum_{i\\neq j} Q_{ij} = 1$.\n- The t-SNE objective function to be minimized is the KL divergence $D_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right)$.\n- A representative within-manifold neighbor pair has $P_{ij} \\approx 5 \\times 10^{-4}$ and $Q_{ij} \\approx 4 \\times 10^{-4}$.\n- A representative cross-manifold far pair has $P_{ij} \\approx 1 \\times 10^{-8}$ and $Q_{ij} \\approx 1 \\times 10^{-4}$.\n\n**Validation:**\n1.  **Scientific Groundedness**: The problem statement is scientifically sound. It accurately describes the t-SNE algorithm, including its objective function, the definitions of the high-dimensional similarities ($P_{ij}$) based on a Gaussian kernel, and the low-dimensional similarities ($Q_{ij}$) based on a Student $t$-distribution. The description of neural data forming manifolds is a standard concept in computational neuroscience. The stated properties of the algorithm (preserving local structure at the expense of global structure) are well-documented characteristics of t-SNE.\n2.  **Well-Posedness**: The problem is well-posed. It provides a clear objective and sufficient information (the form of the cost function and numerical examples) to reason about the underlying mechanism and evaluate the given options.\n3.  **Objectivity**: The problem is stated in precise, objective, and technical language, free from any subjectivity or ambiguity.\n\n**Verdict:** The problem statement is valid. I will proceed with the solution.\n\n### Step 2: Derivation and Analysis\n\nThe objective of t-SNE is to find a low-dimensional embedding that minimizes the KL divergence between the joint probability distribution $P$, representing similarities in the high-dimensional space, and the joint probability distribution $Q$, representing similarities in the low-dimensional space. The objective function is:\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right)\n$$\nThis can be expanded as:\n$$\nD_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i \\neq j} P_{ij} \\log(P_{ij}) - \\sum_{i \\neq j} P_{ij} \\log(Q_{ij})\n$$\nSince the $P_{ij}$ values are fixed (derived from the input data), the term $\\sum_{i \\neq j} P_{ij} \\log(P_{ij})$ is a constant. Minimizing $D_{\\mathrm{KL}}(P \\parallel Q)$ is therefore equivalent to maximizing the cross-entropy term $\\sum_{i \\neq j} P_{ij} \\log(Q_{ij})$.\n\nThe core of the analysis lies in understanding how individual pairs $(i, j)$ contribute to the total KL divergence. The contribution of a single pair is $C_{ij} = P_{ij} \\log(P_{ij}/Q_{ij})$. The optimization process adjusts the positions of points in the low-dimensional space to change the values of $Q_{ij}$ in order to minimize the sum of all such contributions.\n\nThe crucial feature of this cost function is the pre-factor $P_{ij}$. This factor acts as a weight for the logarithmic term $\\log(P_{ij}/Q_{ij})$, which measures the \"mismatch\" between the similarities.\n\nLet's analyze the two scenarios provided:\n\n**1. Within-manifold neighbor pair (Local Structure):**\n- Given: $P_{ij} \\approx 5 \\times 10^{-4}$ and $Q_{ij} \\approx 4 \\times 10^{-4}$.\n- Here, $P_{ij}$ is relatively large, because points $i$ and $j$ are close neighbors in the high-dimensional space.\n- Any significant mismatch between $P_{ij}$ and $Q_{ij}$ will be multiplied by this relatively large $P_{ij}$, resulting in a substantial contribution to the cost. For example, if the algorithm were to place these points far apart in the low-dimensional map, $Q_{ij}$ would become very small, making $\\log(P_{ij}/Q_{ij})$ a large positive number. The resulting contribution $P_{ij} \\log(P_{ij}/Q_{ij})$ would be large, creating a strong \"force\" in the optimization to bring the points closer together (thus increasing $Q_{ij}$ to better match $P_{ij}$).\n- The algorithm is therefore highly sensitive to getting the similarities right for nearby points. This is the mechanism for local structure preservation.\n\n**2. Cross-manifold far pair (Global Structure):**\n- Given: $P_{ij} \\approx 1 \\times 10^{-8}$ and $Q_{ij} \\approx 1 \\times 10^{-4}$.\n- Here, $P_{ij}$ is extremely small, because points $i$ and $j$ are far apart in the high-dimensional space.\n- The contribution to the cost is $C_{ij} = P_{ij} \\log(P_{ij}/Q_{ij})$.\n- Even if there is a massive mismatch between $P_{ij}$ and $Q_{ij}$ (as given in the scenario), the contribution to the total cost is weighted by the tiny $P_{ij}$. The optimization is largely indifferent to this mismatch. Let's compute the approximate contributions to see this quantitatively.\n\nContribution from the near pair:\n$$\nC_{\\text{near}} = P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right) \\approx (5 \\times 10^{-4}) \\ln \\left( \\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}} \\right) = (5 \\times 10^{-4}) \\ln(1.25) \\approx (5 \\times 10^{-4})(0.223) \\approx 1.12 \\times 10^{-4}\n$$\nContribution from the far pair:\n$$\nC_{\\text{far}} = P_{ij} \\log \\left( \\frac{P_{ij}}{Q_{ij}} \\right) \\approx (1 \\times 10^{-8}) \\ln \\left( \\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}} \\right) = (1 \\times 10^{-8}) \\ln(10^{-4}) \\approx (1 \\times 10^{-8})(-9.21) \\approx -9.21 \\times 10^{-8}\n$$\nThe magnitude of the contribution from the near pair ($|C_{\\text{near}}| \\approx 1.12 \\times 10^{-4}$) is about $1200$ times larger than the magnitude of the contribution from the far pair ($|C_{\\text{far}}| \\approx 9.21 \\times 10^{-8}$).\n\nThis confirms that the objective function is dominated by the contributions from nearby points (where $P_{ij}$ is large). The algorithm expends its effort on arranging these points correctly. It is weakly penalized for misrepresenting the distances between far-away points (where $P_{ij}$ is nearly zero), allowing for significant distortion of the global structure.\n\n### Step 3: Option-by-Option Analysis\n\n**A. The divergence $D_{\\mathrm{KL}}(P \\parallel Q)$ weights discrepancies by $P_{ij}$, so near-neighbor pairs (with larger $P_{ij}$) dominate the objective. For the given within-manifold pair, the contribution is $P_{ij} \\log(P_{ij}/Q_{ij}) \\approx (5 \\times 10^{-4}) \\log\\left(\\frac{5 \\times 10^{-4}}{4 \\times 10^{-4}}\\right)$, which is orders of magnitude larger than the contribution from the far cross-manifold pair, $(1 \\times 10^{-8}) \\log\\left(\\frac{1 \\times 10^{-8}}{1 \\times 10^{-4}}\\right)$. Because far-away pairs have $P_{ij} \\approx 0$, their mismatch with $Q_{ij}$ is weakly penalized, allowing global distances to be distorted while preserving local neighborhoods.**\n- This option correctly identifies that the cost function is weighted by $P_{ij}$. It correctly concludes that this causes the contributions from near pairs (large $P_{ij}$) to dominate the objective. The numerical calculation confirms that the near-pair contribution is orders of magnitude larger than the far-pair contribution. The final conclusion—that this weighting weakly penalizes global mismatches, thus allowing global distance distortion while preserving local neighborhoods—is a direct and accurate consequence of the analysis.\n- **Verdict: Correct.**\n\n**B. The divergence $D_{\\mathrm{KL}}(Q \\parallel P)$ is minimized in t-SNE, so discrepancies are weighted by $Q_{ij}$, which heavily penalizes large $Q_{ij}$ assigned to far-away pairs and prevents global distortion. Therefore, near-neighbor preservation is a consequence of minimizing $D_{\\mathrm{KL}}(Q \\parallel P)$.**\n- This option is factually incorrect. The t-SNE algorithm minimizes $D_{\\mathrm{KL}}(P \\parallel Q)$, not $D_{\\mathrm{KL}}(Q \\parallel P)$. These two KL divergences are not symmetric and have very different properties. The logic presented applies to minimizing the reverse KL divergence, which is not what t-SNE does.\n- **Verdict: Incorrect.**\n\n**C. Manifold learning methods that preserve global geodesic distances, such as Isometric Mapping (Isomap), prevent global distortions by computing shortest paths on the neighborhood graph; since t-SNE performs a similar geodesic computation implicitly, global distances are preserved and local neighborhoods may be distorted.**\n- This option incorrectly describes t-SNE. While it correctly describes Isomap, it falsely claims that t-SNE performs a similar geodesic computation. t-SNE's computation of affinities is based on local Gaussian kernels, not on shortest path distances in a graph. Its objective function, as shown above, leads to the distortion of global distances, not their preservation.\n- **Verdict: Incorrect.**\n\n**D. Increasing perplexity in t-SNE forces $P_{ij}$ to be nearly uniform across all pairs, making $D_{\\mathrm{KL}}(P \\parallel Q)$ weight near and far pairs almost equally. This eliminates global distortion while preserving local neighborhoods.**\n- While it is true that a very high perplexity would make $P_{ij}$ more uniform, the conclusion is flawed. If $P_{ij}$ were uniform, it would contain no information about the local structure of the data. The algorithm's goal of preserving local neighborhoods would be subverted from the start. This approach would destroy local structure information, not preserve it.\n- **Verdict: Incorrect.**\n\n**E. In low dimensions, the Student $t$-kernel used to construct $Q_{ij}$ is short-tailed, driving $Q_{ij} \\approx 0$ for all but the very nearest neighbors. This ensures global distances are preserved because far-away pairs have both $P_{ij}$ and $Q_{ij}$ near zero, keeping the divergence small uniformly across all pairs.**\n- This option makes a factually incorrect statement about the Student $t$-distribution. The $t$-distribution with one degree of freedom, used in t-SNE, is *heavy-tailed*, not short-tailed. Its slower, power-law decay (compared to the exponential decay of a Gaussian) is a key feature that helps to separate clusters and alleviate the \"crowding problem\". The premise that the kernel is short-tailed is fundamentally wrong. Consequently, the reasoning that follows from this incorrect premise is also invalid.\n- **Verdict: Incorrect.**\n\nThe analysis confirms that Option A provides the only accurate and complete explanation for the behavior of t-SNE as described in the problem.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}