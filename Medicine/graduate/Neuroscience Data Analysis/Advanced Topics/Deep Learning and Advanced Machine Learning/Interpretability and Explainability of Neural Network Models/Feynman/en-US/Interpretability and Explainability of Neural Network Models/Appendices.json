{
    "hands_on_practices": [
        {
            "introduction": "To build a robust understanding of interpretability, we begin with a foundational attribution method in a transparent setting. This exercise  asks you to compute Integrated Gradients for a simple linear model, a scenario where the mechanics of the path integral become exceptionally clear. By applying this technique to a hypothetical fMRI analysis, you will develop core intuition about how attribution methods decompose a model's output into contributions from individual input features, such as voxel intensities.",
            "id": "4171579",
            "problem": "A neuroscience lab uses a linear readout to map preprocessed Functional Magnetic Resonance Imaging (fMRI) voxel intensities to a unitless scalar score indicating predicted task engagement. Consider a region of interest with $d=3$ voxels. The model is $F(x)=w^{\\top}x$, where $x \\in \\mathbb{R}^{3}$ is the voxel intensity vector and $w \\in \\mathbb{R}^{3}$ are learned weights. The lab adopts the zero pattern as its baseline, $\\tilde{x}=\\mathbf{0}$, representing rest. Using the formal definition of integrated gradients along the straight-line path from $\\tilde{x}$ to $x$, compute the attribution vector for the input $x$ with weights\n$$w=\\begin{pmatrix}0.8\\\\-0.3\\\\0.5\\end{pmatrix},\\quad x=\\begin{pmatrix}2.0\\\\1.5\\\\0.0\\end{pmatrix}.$$\nReport the final attribution vector as a row matrix. No rounding is required. Provide a brief interpretation in terms of voxel contributions to the model’s scalar score in the context of fMRI.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. All necessary data and definitions are provided, and the task is a direct application of a standard interpretability method in machine learning to a simplified but valid scenario in computational neuroscience.\n\nThe problem asks for the attribution vector for a given input $x$ using the Integrated Gradients (IG) method. The attribution for the $i$-th feature, $\\text{IG}_i(x)$, is formally defined as:\n$$\n\\text{IG}_i(x) = (x_i - \\tilde{x}_i) \\times \\int_{\\alpha=0}^{1} \\frac{\\partial F(\\tilde{x} + \\alpha(x - \\tilde{x}))}{\\partial x_i} \\, d\\alpha\n$$\nwhere $x$ is the input, $\\tilde{x}$ is the baseline, and $F$ is the model function. The path of integration is the straight line from the baseline $\\tilde{x}$ to the input $x$.\n\nThe given parameters are:\nModel: $F(x) = w^{\\top}x$\nInput dimension: $d=3$\nWeights: $w=\\begin{pmatrix}0.8\\\\-0.3\\\\0.5\\end{pmatrix}$\nInput vector: $x=\\begin{pmatrix}2.0\\\\1.5\\\\0.0\\end{pmatrix}$\nBaseline vector: $\\tilde{x}=\\mathbf{0}=\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}$\n\nFirst, we determine the gradient of the model function $F(x)$. The model is a linear function:\n$$\nF(x) = w^{\\top}x = w_1 x_1 + w_2 x_2 + w_3 x_3\n$$\nThe partial derivative of $F(x)$ with respect to an input feature $x_i$ is:\n$$\n\\frac{\\partial F(x)}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} (w_1 x_1 + w_2 x_2 + w_3 x_3) = w_i\n$$\nThe gradient vector, $\\nabla F(x)$, is therefore simply the weight vector $w$:\n$$\n\\nabla F(x) = \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{pmatrix} = w\n$$\nA critical observation for a linear model is that the gradient is constant and does not depend on the input $x$.\n\nThe integration path is defined as $\\gamma(\\alpha) = \\tilde{x} + \\alpha(x - \\tilde{x})$. Since the baseline is the zero vector, $\\tilde{x} = \\mathbf{0}$, the path simplifies to $\\gamma(\\alpha) = \\alpha x$.\nThe partial derivative evaluated along this path is:\n$$\n\\frac{\\partial F(\\gamma(\\alpha))}{\\partial x_i} = \\frac{\\partial F(\\alpha x)}{\\partial x_i} = w_i\n$$\nThis is because the gradient is constant everywhere, including along the integration path.\n\nNow we can evaluate the integral in the IG definition:\n$$\n\\int_{\\alpha=0}^{1} \\frac{\\partial F(\\tilde{x} + \\alpha(x - \\tilde{x}))}{\\partial x_i} \\, d\\alpha = \\int_{\\alpha=0}^{1} w_i \\, d\\alpha\n$$\nSince $w_i$ is a constant with respect to the integration variable $\\alpha$, we have:\n$$\n\\int_{\\alpha=0}^{1} w_i \\, d\\alpha = w_i \\left[ \\alpha \\right]_{\\alpha=0}^{1} = w_i (1-0) = w_i\n$$\nSubstituting this result back into the IG formula gives a simplified expression for a linear model with a zero baseline:\n$$\n\\text{IG}_i(x) = (x_i - \\tilde{x}_i) \\times w_i = (x_i - 0) \\times w_i = x_i w_i\n$$\nThis means the attribution vector is the element-wise product (Hadamard product) of the input vector $x$ and the weight vector $w$, i.e., $\\text{IG}(x) = x \\odot w$.\n\nWe can now compute the attributions for each of the $d=3$ voxels using the given values:\nFor the first voxel ($i=1$):\n$$\n\\text{IG}_1(x) = x_1 w_1 = (2.0) \\times (0.8) = 1.6\n$$\nFor the second voxel ($i=2$):\n$$\n\\text{IG}_2(x) = x_2 w_2 = (1.5) \\times (-0.3) = -0.45\n$$\nFor the third voxel ($i=3$):\n$$\n\\text{IG}_3(x) = x_3 w_3 = (0.0) \\times (0.5) = 0.0\n$$\nThe resulting attribution vector is $\\begin{pmatrix} 1.6 \\\\ -0.45 \\\\ 0.0 \\end{pmatrix}$. The problem requests this as a row matrix.\n\nA key property of Integrated Gradients is completeness, which states that the sum of the attributions equals the total change in the model's output from baseline to input: $\\sum_{i=1}^{d} \\text{IG}_i(x) = F(x) - F(\\tilde{x})$.\nLet's verify this.\nThe sum of attributions is: $1.6 + (-0.45) + 0.0 = 1.15$.\nThe model output for the input is: $F(x) = w^{\\top}x = (0.8)(2.0) + (-0.3)(1.5) + (0.5)(0.0) = 1.6 - 0.45 + 0 = 1.15$.\nThe model output for the baseline is: $F(\\tilde{x}) = F(\\mathbf{0}) = w^{\\top}\\mathbf{0} = 0$.\nThe difference is $F(x) - F(\\tilde{x}) = 1.15 - 0 = 1.15$.\nThe completeness property holds, confirming our calculation.\n\nInterpretation: The model's predicted task engagement score for input $x$ is $1.15$. The attribution vector $\\begin{pmatrix} 1.6 & -0.45 & 0.0 \\end{pmatrix}$ decomposes this score into contributions from each voxel.\n- Voxel 1 contributes positively with a value of $1.6$. This indicates that its activation is the primary driver increasing the predicted engagement score.\n- Voxel 2 contributes negatively with a value of $-0.45$. Its activation is interpreted as evidence against task engagement, actively reducing the score.\n- Voxel 3 contributes $0.0$. Although the model has learned a positive weight for this voxel ($w_3=0.5$), its intensity in the given input is zero ($x_3=0.0$), so it has no impact on this specific prediction.\n\nThe attribution vector thus provides a localized explanation, attributing the model's output to the individual input features (voxel intensities).",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.6 & -0.45 & 0.0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While simple gradients provide local sensitivity, they can be deeply misleading when applied to the nonlinear models prevalent in neuroscience. This practice  confronts a critical failure mode known as gradient saturation, where a feature's strong inhibitory effect is rendered invisible to gradient-based attribution. By contrasting the vanishing gradient with the non-zero attribution from DeepLIFT in a simple ReLU model, you will understand why reference-based explanations are essential for capturing the full causal impact of a feature on a neuron's activity.",
            "id": "4171630",
            "problem": "Consider a single-neuron model used to predict a sensory neuron's mean firing rate from a two-dimensional stimulus, implemented as a Rectified Linear Unit (ReLU). The model maps an input vector $\\mathbf{x} = (x_{1}, x_{2})$ to an output $y$ via a two-layer composition: an affine preactivation $z$, a ReLU hidden activation $h$, and a final affine output. Specifically,\n$$\nz = w_{1} x_{1} + w_{2} x_{2} + b,\\quad h = \\max(0, z),\\quad y = a\\,h + d,\n$$\nwhere $w_{1} = 2$, $w_{2} = -3$, $b = 1$, $a = 2$, and $d = 0$. Let the reference baseline input be $\\mathbf{x}^{0} = (0, 0)$, interpreted as the resting stimulus condition. The actual observed input is $\\mathbf{x} = (0, 1)$.\n\nUsing only core definitions of gradients and the differences-from-reference principle underlying Deep Learning Important FeaTures (DeepLIFT), reason from first principles to:\n- Establish whether the gradient-based attribution to $x_{2}$ at $\\mathbf{x}$ vanishes due to saturation, and explain why this occurs in terms of the model's nonlinearity.\n- Derive the DeepLIFT attribution assigned to $x_{2}$ for the change in output $y - y^{0}$ relative to the reference baseline, starting from the requirement that local contributions satisfy a summation-to-delta property through the network's layers.\n\nExpress the final numerical value of the DeepLIFT attribution to $x_{2}$ as a real number. No rounding is required. In your explanation, discuss the neuroscientific implications of saturation in such thresholded rate models for interpretability in neuroscience data analysis.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Model Definition**: A single-neuron model mapping an input vector $\\mathbf{x} = (x_{1}, x_{2})$ to an output $y$.\n- **Model Equations**:\n  $$z = w_{1} x_{1} + w_{2} x_{2} + b$$\n  $$h = \\max(0, z)$$\n  $$y = a\\,h + d$$\n- **Model Parameters**:\n  - $w_{1} = 2$\n  - $w_{2} = -3$\n  - $b = 1$\n  - $a = 2$\n  - $d = 0$\n- **Reference Input**: $\\mathbf{x}^{0} = (0, 0)$\n- **Actual Input**: $\\mathbf{x} = (0, 1)$\n- **Task**:\n  1. Determine if the gradient-based attribution to $x_{2}$ at $\\mathbf{x}$ vanishes due to saturation.\n  2. Derive the DeepLIFT attribution for $x_{2}$ for the output change $y - y^{0}$.\n  3. Provide the final numerical value of the DeepLIFT attribution.\n  4. Discuss neuroscientific implications.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The model described is a Rectified Linear Unit (ReLU), a standard and fundamental component in modern neural networks. Its use to model a neuron's firing rate (a linear-nonlinear model) is a common simplification in computational neuroscience. Gradient-based attribution and DeepLIFT are established methods in machine learning interpretability. The problem is scientifically sound.\n- **Well-Posed**: All parameters ($w_1, w_2, b, a, d$) and inputs ($\\mathbf{x}^0, \\mathbf{x}$) are explicitly defined. The functions are non-ambiguous. A unique, stable, and meaningful solution can be derived.\n- **Objective**: The problem is stated with precise mathematical definitions and an objective task. There is no subjective or opinion-based language.\n\nAll other validation criteria (completeness, realism, structure) are met. The problem does not exhibit any invalidating flaws.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A detailed solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds in three parts as requested: analysis of gradient-based attribution, derivation of DeepLIFT attribution, and a discussion of the implications.\n\nFirst, we compute the state of the neuron model at both the reference input $\\mathbf{x}^{0}$ and the actual input $\\mathbf{x}$.\n\n**At the reference input $\\mathbf{x}^{0} = (0, 0)$:**\n- Preactivation: $z^{0} = w_{1} x_{1}^{0} + w_{2} x_{2}^{0} + b = (2)(0) + (-3)(0) + 1 = 1$.\n- Hidden activation: $h^{0} = \\max(0, z^{0}) = \\max(0, 1) = 1$.\n- Output: $y^{0} = a h^{0} + d = (2)(1) + 0 = 2$.\n\n**At the actual input $\\mathbf{x} = (0, 1)$:**\n- Preactivation: $z = w_{1} x_{1} + w_{2} x_{2} + b = (2)(0) + (-3)(1) + 1 = -2$.\n- Hidden activation: $h = \\max(0, z) = \\max(0, -2) = 0$.\n- Output: $y = a h + d = (2)(0) + 0 = 0$.\n\nFrom these, we can determine the change (delta, $\\Delta$) in each variable:\n- $\\Delta y = y - y^{0} = 0 - 2 = -2$.\n- $\\Delta h = h - h^{0} = 0 - 1 = -1$.\n- $\\Delta z = z - z^{0} = -2 - 1 = -3$.\n- $\\Delta x_{1} = x_{1} - x_{1}^{0} = 0 - 0 = 0$.\n- $\\Delta x_{2} = x_{2} - x_{2}^{0} = 1 - 0 = 1$.\n\n#### Part 1: Gradient-Based Attribution and Saturation\n\nGradient-based attribution methods approximate the importance of an input feature by the local gradient of the output with respect to that input. The attribution to $x_{2}$ is based on the partial derivative $\\frac{\\partial y}{\\partial x_{2}}$ evaluated at the actual input $\\mathbf{x} = (0, 1)$.\n\nUsing the chain rule:\n$$\n\\frac{\\partial y}{\\partial x_{2}} = \\frac{\\partial y}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z} \\cdot \\frac{\\partial z}{\\partial x_{2}}\n$$\nWe compute each term:\n- $\\frac{\\partial y}{\\partial h} = a = 2$.\n- $\\frac{\\partial z}{\\partial x_{2}} = w_{2} = -3$.\n- The derivative of the ReLU function, $h(z) = \\max(0, z)$, is:\n$$\n\\frac{\\partial h}{\\partial z} = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{if } z  0 \\end{cases}\n$$\n(The derivative is undefined at $z=0$, but for the purpose of backpropagation, it is typically set to $0$).\n\nAt the input $\\mathbf{x} = (0, 1)$, we calculated that $z = -2$. Since $z  0$, the neuron is in the inactive or \"saturated\" region.\nTherefore, at this point, $\\frac{\\partial h}{\\partial z} = 0$.\n\nThe full gradient is:\n$$\n\\frac{\\partial y}{\\partial x_{2}}\\bigg|_{\\mathbf{x}=(0,1)} = (2) \\cdot (0) \\cdot (-3) = 0\n$$\nThe gradient-based attribution to $x_{2}$ is zero. This occurs because the gradient is a local measure of sensitivity. At the point $\\mathbf{x}=(0,1)$, the preactivation $z=-2$ is below the ReLU threshold of $0$. Infinitesimal changes to $x_{2}$ will not change $z$ enough to cross the threshold, so the output $y$ remains fixed at $0$. The model's nonlinearity (the ReLU threshold) causes the gradient to vanish, a phenomenon known as the saturation problem.\n\n#### Part 2: DeepLIFT Attribution\n\nDeepLIFT assigns attribution scores by propagating the total change in the output, $\\Delta y$, back to the changes in the inputs, $\\Delta x_{i}$, based on the principle of summation-to-delta, which demands that the sum of attributions equals the total change in the output. For a given layer with output change $\\Delta O$ and input changes $\\Delta I_j$, the attributions must satisfy $\\sum_j C_{\\Delta I_j \\Delta O} = \\Delta O$. We derive the attribution of $\\Delta x_2$ to $\\Delta y$, denoted $C_{\\Delta x_2 \\Delta y}$, by propagating contributions from the output back to the input.\n\n1.  **Attribution from Output Layer ($y = ah+d$)**:\n    The change $\\Delta y = -2$ is caused entirely by the change $\\Delta h = -1$ through the linear transformation $\\Delta y = a \\Delta h$. We want to find the contribution of $\\Delta x_2$ to $\\Delta y$. By linearity of this layer, this is $C_{\\Delta x_2 \\Delta y} = a \\cdot C_{\\Delta x_2 \\Delta h}$, where $C_{\\Delta x_2 \\Delta h}$ is the contribution of the input change $\\Delta x_2$ to the intermediate change $\\Delta h$. The constant is $a=2$, so we need to find $C_{\\Delta x_2 \\Delta h}$.\n\n2.  **Attribution from Hidden Activation Layer ($h = \\max(0,z)$)**:\n    Here, we attribute the change $\\Delta h$ to the change in its input, $\\Delta z$. DeepLIFT uses a multiplier (or \"rescuer\" term) that approximates the local derivative with a \"secant\" slope between the reference and actual activation points: $m_{\\Delta z \\to \\Delta h} = \\frac{\\Delta h}{\\Delta z} = \\frac{-1}{-3} = \\frac{1}{3}$. This non-zero multiplier allows attribution to flow through the saturated neuron.\n    To find $C_{\\Delta x_2 \\Delta h}$, we apply the DeepLIFT chain rule, which distributes the total contribution to $\\Delta h$ (which is $\\Delta h$ itself, as $z$ is the sole input) according to the input contributions to $\\Delta z$.\n    $$C_{\\Delta x_2 \\Delta h} = \\Delta h \\times \\frac{C_{\\Delta x_2 \\Delta z}}{\\Delta z}$$\n    We need to find $C_{\\Delta x_2 \\Delta z}$, the contribution of $\\Delta x_2$ to $\\Delta z$.\n\n3.  **Attribution from Input Affine Layer ($z = w_1 x_1 + w_2 x_2 + b$)**:\n    The change in preactivation is $\\Delta z = w_1 \\Delta x_1 + w_2 \\Delta x_2$. For this linear layer, DeepLIFT assigns contributions based on each input's direct role in the change.\n    - Contribution of $\\Delta x_1$ to $\\Delta z$: $C_{\\Delta x_1 \\Delta z} = w_1 \\Delta x_1 = (2)(0) = 0$.\n    - Contribution of $\\Delta x_2$ to $\\Delta z$: $C_{\\Delta x_2 \\Delta z} = w_2 \\Delta x_2 = (-3)(1) = -3$.\n    The sum of these contributions is $0 + (-3) = -3$, which correctly equals $\\Delta z$.\n\n4.  **Final Calculation**:\n    Now we substitute the values back up the chain.\n    - Using the result from step 3 in step 2:\n      $$C_{\\Delta x_2 \\Delta h} = \\Delta h \\times \\frac{C_{\\Delta x_2 \\Delta z}}{\\Delta z} = (-1) \\times \\frac{-3}{-3} = -1 \\times 1 = -1$$\n    - Using this result in step 1:\n      $$C_{\\Delta x_2 \\Delta y} = a \\cdot C_{\\Delta x_2 \\Delta h} = (2) \\cdot (-1) = -2$$\n\nThe DeepLIFT attribution assigned to $x_2$ is $-2$. This non-zero value correctly reflects that the change in $x_2$ from $0$ to $1$ was responsible for the neuron's output changing from $y^0=2$ to $y=0$. The summation-to-delta property is satisfied: $C_{\\Delta x_1 \\Delta y} + C_{\\Delta x_2 \\Delta y} = 0 + (-2) = -2 = \\Delta y$.\n\n#### Part 3: Neuroscientific Implications\n\nThe disparity between gradient-based attribution ($0$) and DeepLIFT attribution ($-2$) has significant neuroscientific implications for the interpretability of models analyzing neuroscience data.\n\n1.  **Importance of Inhibition**: In biological neural circuits, inhibition is as crucial as excitation. A sensory feature (like $x_2$) that strongly inhibits a neuron, causing it to be silent, is biologically highly significant. Gradient-based methods would assign zero importance to this feature because the neuron's firing rate is not locally sensitive to it—it's already silent. This is a failure to capture the feature's causal role.\n\n2.  **Reference-Based Explanations**: DeepLIFT overcomes this by using a reference point (e.g., a baseline stimulus $\\mathbf{x}^0$). Neuroscientific inquiry is often comparative, asking how a neuron's response *changes* from a control/baseline condition to a stimulus condition. DeepLIFT's methodology directly mirrors this experimental logic. It explains the *difference* in activity ($\\Delta y$), not just the final activity state. It correctly identifies that the change in $x_2$ caused the neuron to switch from an active state (at baseline) to an inactive one.\n\n3.  **Avoiding Misleading Interpretations**: Relying on simple gradients for interpretability in thresholded rate models can be highly misleading. One might incorrectly conclude that a feature has no effect, when in reality, it has a powerful silencing effect. For understanding sensory coding, this distinction is critical. DeepLIFT and similar methods provide a more robust account of feature importance that aligns better with the underlying causal structure of both the model and the biological system it represents.\n\nIn summary, the saturation of simple gradient methods is a fundamental flaw when interpreting models with thresholds, a common feature of neural systems. Reference-based methods like DeepLIFT provide a more causally faithful explanation of a feature's contribution to a change in neural state.",
            "answer": "$$\n\\boxed{-2}\n$$"
        },
        {
            "introduction": "Beyond explaining individual predictions, a key goal of interpretability is to understand the general features a neuron has learned to detect. This exercise  introduces Activation Maximization, a powerful feature visualization technique that synthesizes an 'optimal' input to reveal a neuron's preferences. You will formulate this as a regularized optimization problem and derive a solution, learning how constraints based on neurophysiological structure and imaging physics are crucial for generating scientifically plausible and interpretable visualizations.",
            "id": "4171554",
            "problem": "Consider a Deep Neural Network (DNN) trained on neuroimaging data, such as Magnetic Resonance Imaging (MRI), functional Magnetic Resonance Imaging (fMRI), or Electroencephalography (EEG), where an internal unit indexed by $k$ produces a scalar activation $F_{k}(x)$ for an input $x \\in \\mathbb{R}^{n}$ representing a standardized neuroimage (for example, voxel intensities of a volume or sensor-time features). In activation maximization, the goal is to construct an input $x$ that maximizes the chosen unit’s activation while avoiding inputs that are implausible given neurophysiological structure and acquisition physics. Starting from first principles, formulate activation maximization as a resource-constrained optimization problem that enforces a plausibility constraint via a penalty functional. Then, derive the unconstrained penalty formulation using a Lagrange multiplier, making explicit the objective to be optimized.\n\nTo obtain a tractable analytic solution suitable for scientific inspection, assume a local first-order approximation around a fixed baseline $x_{0} \\in \\mathbb{R}^{n}$,\n$$\nF_{k}(x) \\approx F_{k}(x_{0}) + g^{\\top}(x - x_{0}),\n$$\nwhere $g = \\nabla_{x} F_{k}(x_{0}) \\in \\mathbb{R}^{n}$ is the gradient of the unit activation with respect to input at $x_{0}$. Consider a regularizer appropriate for volumetric neuroimaging with spatial adjacency represented by a symmetric positive semidefinite discrete Laplacian $L \\in \\mathbb{R}^{n \\times n}$ (Neumann boundaries), namely\n$$\nR(x) = \\alpha \\|x\\|_{2}^{2} + \\beta x^{\\top} L x,\n$$\nwith $\\alpha > 0$ and $\\beta \\ge 0$. Using the penalty formulation with a Lagrange multiplier $\\lambda > 0$, derive the stationary condition and compute the closed-form analytic maximizer $x^{\\star} \\in \\mathbb{R}^{n}$ of the linearized penalized objective.\n\nIn addition, justify why choices like the above $R(x)$, and alternatives such as total variation, band-limited smoothness, and spatiotemporal coupling are appropriate for neuroimaging inputs by referencing neurophysiological structure (for example, spatial contiguity of tissue) and acquisition physics (for example, point-spread and reconstruction). Your final answer must be the closed-form analytic expression for $x^{\\star}$ only. Do not include any units. If you introduce any additional hyperparameters, clearly state that they are positive. No numerical rounding is required. Express your final answer as a single closed-form analytic expression.",
            "solution": "The problem statement is evaluated as valid, as it is scientifically grounded in the principles of machine learning and neuroimaging, is well-posed with a unique solution, and is expressed in objective, formal language. It is free of contradictions, ambiguities, or factual unsoundness.\n\nThe task is to formulate and solve an activation maximization problem for a unit in a Deep Neural Network (DNN) trained on neuroimaging data. The solution should be a closed-form expression for the input $x^{\\star}$ that maximizes a linearized and regularized objective.\n\nFirst, we formulate activation maximization as a constrained optimization problem. The goal is to find an input $x \\in \\mathbb{R}^{n}$ that maximizes the activation $F_k(x)$ of a neuron $k$, subject to the constraint that $x$ is plausible. Plausibility is enforced by penalizing inputs with a high value for a regularizer $R(x)$. This can be expressed as constraining the \"cost\" of the input, as measured by $R(x)$, to be less than or equal to some budget $c > 0$. The constrained optimization problem is:\n$$\n\\max_{x \\in \\mathbb{R}^n} F_k(x) \\quad \\text{subject to} \\quad R(x) \\le c\n$$\n\nTo solve this, we convert it into an unconstrained optimization problem using a Lagrange multiplier $\\lambda > 0$. This is the penalty formulation, where we aim to maximize the activation $F_k(x)$ while simultaneously minimizing the penalty $R(x)$. The trade-off is controlled by $\\lambda$. The unconstrained objective function $J(x)$ to be maximized is:\n$$\nJ(x) = F_k(x) - \\lambda R(x)\n$$\nWe seek $x^{\\star} = \\arg\\max_{x \\in \\mathbb{R}^n} J(x)$.\n\nThe problem specifies a first-order Taylor approximation for $F_k(x)$ around a baseline input $x_0 \\in \\mathbb{R}^{n}$:\n$$\nF_k(x) \\approx F_k(x_0) + g^{\\top}(x - x_0)\n$$\nwhere $g = \\nabla_{x} F_k(x_0)$ is the gradient of the activation with respect to the input, evaluated at $x_0$.\n\nThe problem also provides a specific regularizer $R(x)$ suitable for volumetric neuroimaging:\n$$\nR(x) = \\alpha \\|x\\|_{2}^{2} + \\beta x^{\\top} L x\n$$\nwith parameters $\\alpha > 0$, $\\beta \\ge 0$, and $L$ being a symmetric positive semidefinite discrete Laplacian matrix. Note that $\\|x\\|_{2}^{2}$ can be written as $x^{\\top}x = x^{\\top}Ix$, where $I$ is the $n \\times n$ identity matrix.\n\nSubstituting the approximations into the objective function $J(x)$, we get the linearized penalized objective:\n$$\nJ(x) \\approx F_k(x_0) + g^{\\top}(x - x_0) - \\lambda (\\alpha x^{\\top} I x + \\beta x^{\\top} L x)\n$$\nTo find the maximizer $x^{\\star}$, we must find the stationary point by taking the gradient of $J(x)$ with respect to $x$ and setting it to zero. The terms $F_k(x_0)$ and $g^{\\top}x_0$ are constant with respect to $x$, so their gradients are zero. We compute the gradient of the remaining terms:\n$$\n\\nabla_x (g^{\\top}x) = g\n$$\n$$\n\\nabla_x (\\lambda \\alpha x^{\\top} I x) = 2 \\lambda \\alpha I x = 2 \\lambda \\alpha x\n$$\n$$\n\\nabla_x (\\lambda \\beta x^{\\top} L x) = \\lambda \\beta (L + L^{\\top})x = 2 \\lambda \\beta L x \\quad (\\text{since } L \\text{ is symmetric})\n$$\nCombining these, the gradient of the objective function $J(x)$ is:\n$$\n\\nabla_x J(x) = g - 2 \\lambda \\alpha x - 2 \\lambda \\beta L x\n$$\nSetting the gradient to zero to find the stationary point, denoted $x^{\\star}$:\n$$\ng - 2 \\lambda \\alpha x^{\\star} - 2 \\lambda \\beta L x^{\\star} = 0\n$$\nWe can now solve for $x^{\\star}$:\n$$\ng = 2 \\lambda \\alpha x^{\\star} + 2 \\lambda \\beta L x^{\\star}\n$$\n$$\ng = 2 \\lambda (\\alpha I + \\beta L) x^{\\star}\n$$\nThe matrix $(\\alpha I + \\beta L)$ is invertible. This is because $L$ is positive semidefinite, meaning all its eigenvalues $\\mu_i$ satisfy $\\mu_i \\ge 0$. The eigenvalues of $(\\alpha I + \\beta L)$ are $(\\alpha + \\beta \\mu_i)$. Given $\\alpha > 0$ and $\\beta \\ge 0$, all eigenvalues $(\\alpha + \\beta \\mu_i)$ are strictly positive. A matrix with strictly positive eigenvalues is positive definite and therefore invertible.\n\nPre-multiplying by the inverse of $2 \\lambda (\\alpha I + \\beta L)$, we obtain the closed-form solution for the optimal input $x^{\\star}$:\n$$\nx^{\\star} = (2 \\lambda (\\alpha I + \\beta L))^{-1} g\n$$\n$$\nx^{\\star} = \\frac{1}{2\\lambda} (\\alpha I + \\beta L)^{-1} g\n$$\nThis expression for $x^{\\star}$ represents the synthetic input that maximizes the linearized activation of unit $k$, subject to plausibility constraints encoded by the regularizer $R(x)$.\n\nJustification for the regularizer choices in neuroimaging:\nThe regularizer $R(x) = \\alpha \\|x\\|_{2}^{2} + \\beta x^{\\top} L x$ is well-suited for neuroimaging because its components encode strong and realistic prior beliefs about the data.\n1.  The $\\alpha \\|x\\|_{2}^{2}$ term, an L2-norm penalty, discourages solutions with excessively large voxel or sensor values. This is physically motivated, as real neuroimaging signals have a finite dynamic range, and extreme values are often artifacts. This term promotes \"low-energy\" solutions that appear more natural.\n2.  The $\\beta x^{\\top} L x$ term is a spatial smoothness prior. The quadratic form involving the Laplacian $L$ penalizes the squared magnitude of the local spatial variations in the input $x$. This is appropriate for two main reasons:\n    -   **Neurophysiological Structure**: Brain tissue and neural activity are spatially contiguous. Anatomical structures in MRI, like gray and white matter, are continuous regions. Functional activity measured by fMRI or EEG is also spatially correlated due to the clustered organization of functional areas and physical effects like volume conduction. A smoothness prior ensures the synthesized input $x^{\\star}$ respects this fundamental biological organization.\n    -   **Acquisition Physics**: All neuroimaging modalities have a finite resolution and are subject to a point-spread function (PSF) that inherently blurs the acquired signal. This means that physically plausible inputs must be relatively smooth and cannot contain frequencies beyond what the scanner can resolve. The Laplacian regularizer enforces this smoothness, generating inputs that are consistent with the physics of data acquisition.\n\nAlternative regularizers like Total Variation ($\\|\\nabla x\\|_{1}$) are also useful as they promote piecewise smoothness, which can better preserve sharp boundaries between different tissues in MRI. For dynamic data (e.g., fMRI time-series), spatiotemporal regularizers can enforce smoothness in both space and time, reflecting the expectation that neural activity patterns evolve continuously. Band-limited smoothness constraints directly limit the frequency content of the solution, which can also be motivated by the physics of the imaging device.",
            "answer": "$$\\boxed{\\frac{1}{2\\lambda} (\\alpha I + \\beta L)^{-1} g}$$"
        }
    ]
}