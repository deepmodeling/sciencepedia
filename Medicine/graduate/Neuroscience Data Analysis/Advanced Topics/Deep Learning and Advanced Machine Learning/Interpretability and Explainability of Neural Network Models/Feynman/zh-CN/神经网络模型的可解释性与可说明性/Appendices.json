{
    "hands_on_practices": [
        {
            "introduction": "可解释性的一个基本目标是将模型的输出归因于其输入特征。本练习在一个简化的线性模型背景下，介绍了积分梯度（Integrated Gradients）这一强大的归因方法。通过解决这个问题，我们可以在将其应用于更复杂的模型之前，建立对基于路径的归因方法如何运作的核心直觉。",
            "id": "4171579",
            "problem": "一个神经科学实验室使用线性读出来将预处理的功能性磁共振成像（fMRI）体素强度映射到一个无单位的标量分数，该分数表示预测的任务参与度。考虑一个包含 $d=3$ 个体素的感兴趣区域。模型为 $F(x)=w^{\\top}x$，其中 $x\\in\\mathbb{R}^{3}$ 是体素强度向量，$w\\in\\mathbb{R}^{3}$ 是学习到的权重。该实验室采用零模式作为其基线，即 $\\tilde{x}=\\mathbf{0}$，代表静息状态。使用积分梯度沿从 $\\tilde{x}$ 到 $x$ 的直线路径的形式化定义，计算输入 $x$ 在权重\n$$w=\\begin{pmatrix}0.8\\\\-0.3\\\\0.5\\end{pmatrix},\\quad x=\\begin{pmatrix}2.0\\\\1.5\\\\0.0\\end{pmatrix}$$\n下的归因向量。\n将最终的归因向量以行矩阵的形式报告。无需四舍五入。在 fMRI 的背景下，就体素对模型标量分数的贡献提供简要解释。",
            "solution": "该问题陈述被验证为有科学依据、适定、客观且完整的。所有必要的数据和定义都已提供，该任务是将机器学习中的一种标准可解释性方法直接应用于计算神经科学中一个简化但有效的场景。\n\n该问题要求使用积分梯度（IG）方法计算给定输入 $x$ 的归因向量。第 $i$ 个特征的归因 $\\text{IG}_i(x)$ 的形式化定义为：\n$$\n\\text{IG}_i(x) = (x_i - \\tilde{x}_i) \\times \\int_{\\alpha=0}^{1} \\frac{\\partial F(\\tilde{x} + \\alpha(x - \\tilde{x}))}{\\partial x_i} \\, d\\alpha\n$$\n其中 $x$ 是输入，$\\tilde{x}$ 是基线，$F$ 是模型函数。积分路径是从基线 $\\tilde{x}$ 到输入 $x$ 的直线。\n\n给定参数如下：\n模型：$F(x) = w^{\\top}x$\n输入维度：$d=3$\n权重：$w=\\begin{pmatrix}0.8\\\\-0.3\\\\0.5\\end{pmatrix}$\n输入向量：$x=\\begin{pmatrix}2.0\\\\1.5\\\\0.0\\end{pmatrix}$\n基线向量：$\\tilde{x}=\\mathbf{0}=\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}$\n\n首先，我们确定模型函数 $F(x)$ 的梯度。该模型是一个线性函数：\n$$\nF(x) = w^{\\top}x = w_1 x_1 + w_2 x_2 + w_3 x_3\n$$\n$F(x)$ 关于输入特征 $x_i$ 的偏导数为：\n$$\n\\frac{\\partial F(x)}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} (w_1 x_1 + w_2 x_2 + w_3 x_3) = w_i\n$$\n因此，梯度向量 $\\nabla F(x)$ 就是权重向量 $w$：\n$$\n\\nabla F(x) = \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{pmatrix} = w\n$$\n对于线性模型，一个关键的观察是梯度是恒定的，不依赖于输入 $x$。\n\n积分路径定义为 $\\gamma(\\alpha) = \\tilde{x} + \\alpha(x - \\tilde{x})$。由于基线是零向量，即 $\\tilde{x} = \\mathbf{0}$，路径简化为 $\\gamma(\\alpha) = \\alpha x$。\n沿此路径计算的偏导数为：\n$$\n\\frac{\\partial F(\\gamma(\\alpha))}{\\partial x_i} = \\frac{\\partial F(\\alpha x)}{\\partial x_i} = w_i\n$$\n这是因为梯度在任何地方都是恒定的，包括在积分路径上。\n\n现在我们可以计算 IG 定义中的积分：\n$$\n\\int_{\\alpha=0}^{1} \\frac{\\partial F(\\tilde{x} + \\alpha(x - \\tilde{x}))}{\\partial x_i} \\, d\\alpha = \\int_{\\alpha=0}^{1} w_i \\, d\\alpha\n$$\n由于 $w_i$ 相对于积分变量 $\\alpha$ 是一个常数，我们有：\n$$\n\\int_{\\alpha=0}^{1} w_i \\, d\\alpha = w_i \\left[ \\alpha \\right]_{\\alpha=0}^{1} = w_i (1-0) = w_i\n$$\n将此结果代回 IG 公式，得到一个具有零基线的线性模型的简化表达式：\n$$\n\\text{IG}_i(x) = (x_i - \\tilde{x}_i) \\times w_i = (x_i - 0) \\times w_i = x_i w_i\n$$\n这意味着归因向量是输入向量 $x$ 和权重向量 $w$ 的逐元素乘积（哈达玛积），即 $\\text{IG}(x) = x \\odot w$。\n\n现在我们可以使用给定值计算 $d=3$ 个体素中每一个的归因：\n对于第一个体素（$i=1$）：\n$$\n\\text{IG}_1(x) = x_1 w_1 = (2.0) \\times (0.8) = 1.6\n$$\n对于第二个体素（$i=2$）：\n$$\n\\text{IG}_2(x) = x_2 w_2 = (1.5) \\times (-0.3) = -0.45\n$$\n对于第三个体素（$i=3$）：\n$$\n\\text{IG}_3(x) = x_3 w_3 = (0.0) \\times (0.5) = 0.0\n$$\n最终的归因向量是 $\\begin{pmatrix} 1.6 \\\\ -0.45 \\\\ 0.0 \\end{pmatrix}$。题目要求将其表示为行矩阵。\n\n积分梯度的一个关键属性是完备性，即所有归因的总和等于模型输出从基线到输入的变化量：$\\sum_{i=1}^{d} \\text{IG}_i(x) = F(x) - F(\\tilde{x})$。\n让我们来验证一下。\n归因之和为：$1.6 + (-0.45) + 0.0 = 1.15$。\n模型对输入的输出为：$F(x) = w^{\\top}x = (0.8)(2.0) + (-0.3)(1.5) + (0.5)(0.0) = 1.6 - 0.45 + 0 = 1.15$。\n模型对基线的输出为：$F(\\tilde{x}) = F(\\mathbf{0}) = w^{\\top}\\mathbf{0} = 0$。\n差值为 $F(x) - F(\\tilde{x}) = 1.15 - 0 = 1.15$。\n完备性属性成立，证实了我们的计算。\n\n解释：模型对输入 $x$ 的预测任务参与度分数为 1.15。归因向量 $\\begin{pmatrix} 1.6  -0.45  0.0 \\end{pmatrix}$ 将这个分数分解为来自每个体素的贡献。\n- 体素1的贡献为正，值为1.6。这表明其激活是提高预测参与度分数的主要驱动因素。\n- 体素2的贡献为负，值为-0.45。其激活被解释为反对任务参与的证据，主动降低了该分数。\n- 体素3的贡献为0.0。尽管模型为该体素学习到了一个正权重（$w_3=0.5$），但在给定输入中其强度为零（$x_3=0.0$），因此它对这个特定的预测没有影响。\n\n因此，归因向量提供了一个局部化的解释，将模型的输出归因于单个输入特征（体素强度）。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.6 & -0.45 & 0.0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "虽然简单梯度是衡量特征重要性的一种直观方法，但在深度非线性网络中，由于饱和等问题，它可能会产生误导。本练习构建了一个场景，其中标准的基于梯度的归因完全失效，然后演示了像DeepLIFT这样的基于参考的方法如何提供更有意义的解释。这突出了选择能够恰当处理模型非线性的归因方法的重要性。",
            "id": "4171630",
            "problem": "考虑一个单神经元模型，该模型实现为整流线性单元 (ReLU)，用于根据二维刺激预测感觉神经元的平均发放率。该模型通过一个两层组合将输入向量 $\\mathbf{x} = (x_{1}, x_{2})$ 映射到输出 $y$：一个仿射预激活 $z$，一个 ReLU 隐藏层激活 $h$，以及一个最终的仿射输出。具体来说，\n$$\nz = w_{1} x_{1} + w_{2} x_{2} + b,\\quad h = \\max(0, z),\\quad y = a\\,h + d,\n$$\n其中 $w_{1} = 2$, $w_{2} = -3$, $b = 1$, $a = 2$, 且 $d = 0$。设参考基线输入为 $\\mathbf{x}^{0} = (0, 0)$，解释为静息刺激条件。实际观测到的输入为 $\\mathbf{x} = (0, 1)$。\n\n仅使用梯度的核心定义和深度学习重要特征 (DeepLIFT) 背后的“与参考值的差异”原则，从第一性原理出发进行推理，以：\n- 确定在 $\\mathbf{x}$ 处对 $x_{2}$ 的基于梯度的归因是否因饱和而消失，并从模型的非线性角度解释其发生原因。\n- 从局部贡献必须在网络各层满足“总和为增量”性质的要求出发，推导相对于参考基线，分配给输出变化 $y - y^{0}$ 中 $x_{2}$ 的 DeepLIFT 归因。\n\n将对 $x_{2}$ 的 DeepLIFT 归因的最终数值表示为一个实数。无需四舍五入。在您的解释中，讨论此类阈值速率模型中的饱和现象对于神经科学数据分析中可解释性的神经科学意义。",
            "solution": "首先验证问题，以确保其具有科学依据、良定且客观。\n\n### 步骤 1：提取已知条件\n- **模型定义**：一个将输入向量 $\\mathbf{x} = (x_{1}, x_{2})$ 映射到输出 $y$ 的单神经元模型。\n- **模型方程**：\n  $$z = w_{1} x_{1} + w_{2} x_{2} + b$$\n  $$h = \\max(0, z)$$\n  $$y = a\\,h + d$$\n- **模型参数**：\n  - $w_{1} = 2$\n  - $w_{2} = -3$\n  - $b = 1$\n  - $a = 2$\n  - $d = 0$\n- **参考输入**：$\\mathbf{x}^{0} = (0, 0)$\n- **实际输入**：$\\mathbf{x} = (0, 1)$\n- **任务**：\n  1. 确定在 $\\mathbf{x}$ 处对 $x_{2}$ 的基于梯度的归因是否因饱和而消失。\n  2. 推导输出变化 $y - y^{0}$ 中 $x_{2}$ 的 DeepLIFT 归因。\n  3. 给出 DeepLIFT 归因的最终数值。\n  4. 讨论神经科学意义。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **科学依据**：所描述的模型是一个整流线性单元 (ReLU)，这是现代神经网络中一个标准且基础的组件。用它来模拟神经元的发放率（一个线性-非线性模型）是计算神经科学中常见的简化方法。基于梯度的归因和 DeepLIFT 是机器学习可解释性领域中已确立的方法。该问题在科学上是合理的。\n- **良定性**：所有参数 ($w_1, w_2, b, a, d$) 和输入 ($\\mathbf{x}^0, \\mathbf{x}$) 都被明确定义。函数没有歧义。可以推导出唯一、稳定且有意义的解。\n- **客观性**：问题以精确的数学定义和一个客观的任务进行陈述。没有主观或基于观点的语言。\n\n所有其他验证标准（完整性、真实性、结构性）均已满足。该问题没有任何使其无效的缺陷。\n\n### 步骤 3：结论与行动\n该问题是**有效**的。将提供详细的解决方案。\n\n### 解答推导\n\n解决方案按要求分为三部分：基于梯度的归因分析、DeepLIFT 归因的推导，以及对相关意义的讨论。\n\n首先，我们计算神经元模型在参考输入 $\\mathbf{x}^{0}$ 和实际输入 $\\mathbf{x}$ 时的状态。\n\n**在参考输入 $\\mathbf{x}^{0} = (0, 0)$ 时：**\n- 预激活：$z^{0} = w_{1} x_{1}^{0} + w_{2} x_{2}^{0} + b = (2)(0) + (-3)(0) + 1 = 1$。\n- 隐藏层激活：$h^{0} = \\max(0, z^{0}) = \\max(0, 1) = 1$。\n- 输出：$y^{0} = a h^{0} + d = (2)(1) + 0 = 2$。\n\n**在实际输入 $\\mathbf{x} = (0, 1)$ 时：**\n- 预激活：$z = w_{1} x_{1} + w_{2} x_{2} + b = (2)(0) + (-3)(1) + 1 = -2$。\n- 隐藏层激活：$h = \\max(0, z) = \\max(0, -2) = 0$。\n- 输出：$y = a h + d = (2)(0) + 0 = 0$。\n\n基于这些结果，我们可以确定每个变量的变化量（delta, $\\Delta$）：\n- $\\Delta y = y - y^{0} = 0 - 2 = -2$。\n- $\\Delta h = h - h^{0} = 0 - 1 = -1$。\n- $\\Delta z = z - z^{0} = -2 - 1 = -3$。\n- $\\Delta x_{1} = x_{1} - x_{1}^{0} = 0 - 0 = 0$。\n- $\\Delta x_{2} = x_{2} - x_{2}^{0} = 1 - 0 = 1$。\n\n#### 第 1 部分：基于梯度的归因与饱和\n\n基于梯度的归因方法通过输出对输入特征的局部梯度来近似该特征的重要性。对 $x_{2}$ 的归因基于在实际输入 $\\mathbf{x} = (0, 1)$ 处计算的偏导数 $\\frac{\\partial y}{\\partial x_{2}}$。\n\n使用链式法则：\n$$\n\\frac{\\partial y}{\\partial x_{2}} = \\frac{\\partial y}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z} \\cdot \\frac{\\partial z}{\\partial x_{2}}\n$$\n我们计算每一项：\n- $\\frac{\\partial y}{\\partial h} = a = 2$。\n- $\\frac{\\partial z}{\\partial x_{2}} = w_{2} = -3$。\n- ReLU 函数 $h(z) = \\max(0, z)$ 的导数是：\n$$\n\\frac{\\partial h}{\\partial z} = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{if } z  0 \\end{cases}\n$$\n（导数在 $z=0$ 处未定义，但为了反向传播的目的，通常将其设为 $0$）。\n\n在输入 $\\mathbf{x} = (0, 1)$ 处，我们计算出 $z = -2$。由于 $z  0$，神经元处于非激活或“饱和”区域。\n因此，在这一点上，$\\frac{\\partial h}{\\partial z} = 0$。\n\n完整的梯度是：\n$$\n\\frac{\\partial y}{\\partial x_{2}}\\bigg|_{\\mathbf{x}=(0,1)} = (2) \\cdot (0) \\cdot (-3) = 0\n$$\n对 $x_{2}$ 的基于梯度的归因为零。这是因为梯度是敏感性的局部度量。在点 $\\mathbf{x}=(0,1)$ 处，预激活值 $z=-2$ 低于 ReLU 的阈值 $0$。对 $x_{2}$ 的无穷小改变不足以使 $z$ 跨越阈值，因此输出 $y$ 保持在 $0$ 不变。模型的非线性（ReLU 阈值）导致梯度消失，这一现象被称为饱和问题。\n\n#### 第 2 部分：DeepLIFT 归因\n\nDeepLIFT 通过将输出的总变化量 $\\Delta y$ 反向传播到输入的变化量 $\\Delta x_{i}$ 来分配归因分数，其依据是“总和为增量”原则，该原则要求所有归因的总和等于输出的总变化量。对于给定层，其输出变化为 $\\Delta O$，输入变化为 $\\Delta I_j$，其归因必须满足 $\\sum_j C_{\\Delta I_j \\Delta O} = \\Delta O$。我们通过将贡献从输出反向传播到输入，来推导 $\\Delta x_2$ 对 $\\Delta y$ 的归因，记为 $C_{\\Delta x_2 \\Delta y}$。\n\n1.  **来自输出层的归因 ($y = ah+d$)**：\n    变化量 $\\Delta y = -2$ 完全是由变化量 $\\Delta h = -1$ 通过线性变换 $\\Delta y = a \\Delta h$ 引起的。我们想找到 $\\Delta x_2$ 对 $\\Delta y$ 的贡献。根据该层的线性性质，这等于 $C_{\\Delta x_2 \\Delta y} = a \\cdot C_{\\Delta x_2 \\Delta h}$，其中 $C_{\\Delta x_2 \\Delta h}$ 是输入变化 $\\Delta x_2$ 对中间变化 $\\Delta h$ 的贡献。常数 $a=2$，因此我们需要求出 $C_{\\Delta x_2 \\Delta h}$。\n\n2.  **来自隐藏激活层的归因 ($h = \\max(0,z)$)**：\n    在这里，我们将变化量 $\\Delta h$ 归因于其输入的变化量 $\\Delta z$。DeepLIFT 使用一个乘子（或“拯救者”项），该乘子用参考激活点和实际激活点之间的“割线”斜率来近似局部导数：$m_{\\Delta z \\to \\Delta h} = \\frac{\\Delta h}{\\Delta z} = \\frac{-1}{-3} = \\frac{1}{3}$。这个非零乘子使得归因能够流过饱和的神经元。\n    为求得 $C_{\\Delta x_2 \\Delta h}$，我们应用 DeepLIFT 链式法则，该法则根据对 $\\Delta z$ 的输入贡献来分配对 $\\Delta h$ 的总贡献（由于 $z$ 是唯一输入，总贡献即为 $\\Delta h$ 本身）。\n    $$C_{\\Delta x_2 \\Delta h} = \\Delta h \\times \\frac{C_{\\Delta x_2 \\Delta z}}{\\Delta z}$$\n    我们需要求出 $C_{\\Delta x_2 \\Delta z}$，即 $\\Delta x_2$ 对 $\\Delta z$ 的贡献。\n\n3.  **来自输入仿射层的归因 ($z = w_1 x_1 + w_2 x_2 + b$)**：\n    预激活的变化量为 $\\Delta z = w_1 \\Delta x_1 + w_2 \\Delta x_2$。对于这个线性层，DeepLIFT 根据每个输入在变化中的直接作用来分配贡献。\n    - $\\Delta x_1$ 对 $\\Delta z$ 的贡献：$C_{\\Delta x_1 \\Delta z} = w_1 \\Delta x_1 = (2)(0) = 0$。\n    - $\\Delta x_2$ 对 $\\Delta z$ 的贡献：$C_{\\Delta x_2 \\Delta z} = w_2 \\Delta x_2 = (-3)(1) = -3$。\n    这些贡献的总和为 $0 + (-3) = -3$，这与 $\\Delta z$ 的值相等，是正确的。\n\n4.  **最终计算**：\n    现在我们将这些值代回链式计算中。\n    - 将步骤 3 的结果代入步骤 2：\n      $$C_{\\Delta x_2 \\Delta h} = \\Delta h \\times \\frac{C_{\\Delta x_2 \\Delta z}}{\\Delta z} = (-1) \\times \\frac{-3}{-3} = -1 \\times 1 = -1$$\n    - 将此结果代入步骤 1：\n      $$C_{\\Delta x_2 \\Delta y} = a \\cdot C_{\\Delta x_2 \\Delta h} = (2) \\cdot (-1) = -2$$\n\n分配给 $x_2$ 的 DeepLIFT 归因是 $-2$。这个非零值正确地反映了 $x_2$ 从 $0$ 变为 $1$ 导致了神经元输出从 $y^0=2$ 变为 $y=0$。总和为增量性质得到满足：$C_{\\Delta x_1 \\Delta y} + C_{\\Delta x_2 \\Delta y} = 0 + (-2) = -2 = \\Delta y$。\n\n#### 第 3 部分：神经科学意义\n\n基于梯度的归因 ($0$) 和 DeepLIFT 归因 ($-2$) 之间的差异，对于分析神经科学数据的模型的可解释性具有重要的神经科学意义。\n\n1.  **抑制的重要性**：在生物神经回路中，抑制与兴奋同等重要。一个强烈抑制神经元使其静息的感觉特征（如 $x_2$）在生物学上是极其重要的。基于梯度的方法会给这个特征分配零重要性，因为神经元的发放率对它不具有局部敏感性——它已经处于静息状态。这未能捕捉到该特征的因果作用。\n\n2.  **基于参考的解释**：DeepLIFT 通过使用一个参考点（例如，基线刺激 $\\mathbf{x}^0$）来克服这个问题。神经科学研究通常是比较性的，探究神经元的反应如何从一个控制/基线条件*变化*到一个刺激条件。DeepLIFT 的方法论直接反映了这种实验逻辑。它解释的是活动的*差异*（$\\Delta y$），而不仅仅是最终的活动状态。它正确地识别出 $x_2$ 的变化导致神经元从一个激活状态（在基线时）转换到一个非激活状态。\n\n3.  **避免误导性解释**：在阈值速率模型中依赖简单梯度来进行解释可能会产生严重的误导。人们可能错误地断定某个特征没有影响，而实际上它具有强大的静息效应。对于理解感觉编码而言，这一区别至关重要。DeepLIFT 及类似方法提供了一种更鲁棒的特征重要性说明，与模型及其所代表的生物系统的底层因果结构更吻合。\n\n总之，在解释带有阈值（这是神经系统的普遍特征）的模型时，简单梯度方法的饱和问题是一个根本性缺陷。像 DeepLIFT 这样基于参考的方法，为特征对神经状态变化的贡献提供了一种在因果上更忠实的解释。",
            "answer": "$$\n\\boxed{-2}\n$$"
        },
        {
            "introduction": "除了解释单个预测之外，我们通常还想了解神经元通常学会了检测哪些特征。本练习探讨了激活最大化（Activation Maximization）技术，该技术用于合成一个能最大程度激活特定神经元的“最优”输入。这个推导过程包含了来自神经影像学的实际正则化项，展示了先验知识如何指导生成合理且可解释的可视化结果。",
            "id": "4171554",
            "problem": "考虑一个在神经影像数据上训练的深度神经网络（DNN），例如磁共振成像（MRI）、功能性磁共振成像（fMRI）或脑电图（EEG），其中由 $k$ 索引的内部单元为代表标准化神经影像（例如，一个体积的体素强度或传感器-时间特征）的输入 $x \\in \\mathbb{R}^{n}$ 产生一个标量激活值 $F_{k}(x)$。在激活最大化中，目标是构建一个输入 $x$，以最大化所选单元的激活，同时避免那些在给定神经生理结构和采集物理学下不合理的输入。从第一性原理出发，将激活最大化公式化为一个资源受限的优化问题，该问题通过惩罚泛函强制施加合理性约束。然后，使用拉格朗日乘子推导无约束惩罚形式，并明确待优化的目标。\n\n为了获得一个适合科学检验的易于处理的解析解，假设在固定基线 $x_{0} \\in \\mathbb{R}^{n}$ 周围存在一个局部一阶近似，\n$$\nF_{k}(x) \\approx F_{k}(x_{0}) + g^{\\top}(x - x_{0}),\n$$\n其中 $g = \\nabla_{x} F_{k}(x_{0}) \\in \\mathbb{R}^{n}$ 是单元激活值相对于在 $x_{0}$ 处的输入的梯度。考虑一个适用于体积神经影像的正则化器，其空间邻接性由一个对称半正定离散拉普拉斯算子 $L \\in \\mathbb{R}^{n \\times n}$（诺伊曼边界）表示，即\n$$\nR(x) = \\alpha \\|x\\|_{2}^{2} + \\beta x^{\\top} L x,\n$$\n其中 $\\alpha  0$ 且 $\\beta \\ge 0$。使用带有拉格朗日乘子 $\\lambda  0$ 的惩罚形式，推导平稳条件并计算线性化惩罚目标的闭式解析最大化器 $x^{\\star} \\in \\mathbb{R}^{n}$。\n\n此外，通过参考神经生理结构（例如，组织的空间连续性）和采集物理学（例如，点扩散和重建），论证为何像上述 $R(x)$ 这样的选择以及诸如全变分、带限平滑度和时空耦合等替代方案适用于神经影像输入。您的最终答案必须仅为 $x^{\\star}$ 的闭式解析表达式。不要包含任何单位。如果您引入任何额外的超参数，请明确说明它们是正数。不需要进行数值舍入。将您的最终答案表示为单个闭式解析表达式。",
            "solution": "该问题陈述被评估为有效，因为它在科学上基于机器学习和神经影像的原理，是具有唯一解的适定问题，并以客观、正式的语言表达。它没有矛盾、歧义或事实性错误。\n\n任务是为一个在神经影像数据上训练的深度神经网络（DNN）中的单元，公式化并求解一个激活最大化问题。解应该是一个输入 `x^{\\star}` 的闭式表达式，该输入可以最大化一个线性化和正则化的目标。\n\n首先，我们将激活最大化公式化为一个约束优化问题。目标是找到一个输入 `x \\in \\mathbb{R}^{n}`，它能最大化神经元 `k` 的激活值 `F_k(x)`，同时受到 `x` 是合理的这一约束。合理性是通过对具有高正则化器 `R(x)` 值的输入进行惩罚来强制执行的。这可以表示为将由 `R(x)` 度量的输入“成本”约束为小于或等于某个预算 `c  0`。该约束优化问题是：\n$$\n\\max_{x \\in \\mathbb{R}^n} F_k(x) \\quad \\text{subject to} \\quad R(x) \\le c\n$$\n\n为求解此问题，我们使用拉格朗日乘子 `\\lambda  0` 将其转换为一个无约束优化问题。这就是惩罚形式，我们的目标是最大化激活值 `F_k(x)`，同时最小化惩罚 `R(x)`。这种权衡由 `\\lambda` 控制。待最大化的无约束目标函数 `J(x)` 是：\n$$\nJ(x) = F_k(x) - \\lambda R(x)\n$$\n我们寻求 `x^{\\star} = \\arg\\max_{x \\in \\mathbb{R}^n} J(x)`。\n\n问题指定了 `F_k(x)` 在基线输入 `x_0 \\in \\mathbb{R}^{n}` 周围的一阶泰勒近似：\n$$\nF_k(x) \\approx F_k(x_0) + g^{\\top}(x - x_0)\n$$\n其中 `g = \\nabla_{x} F_k(x_0)` 是在 `x_0` 处求值的激活值相对于输入的梯度。\n\n问题还提供了一个适用于体积神经影像的特定正则化器 `R(x)`：\n$$\nR(x) = \\alpha \\|x\\|_{2}^{2} + \\beta x^{\\top} L x\n$$\n其中参数 `\\alpha  0`，`\\beta \\ge 0`，且 `L` 是一个对称半正定离散拉普拉斯矩阵。请注意，`\\|x\\|_{2}^{2}` 可以写成 `x^{\\top}x = x^{\\top}Ix`，其中 `I` 是 `n \\times n` 的单位矩阵。\n\n将近似代入目标函数 `J(x)`，我们得到线性化惩罚目标：\n$$\nJ(x) \\approx F_k(x_0) + g^{\\top}(x - x_0) - \\lambda (\\alpha x^{\\top} I x + \\beta x^{\\top} L x)\n$$\n为了找到最大化器 `x^{\\star}`，我们必须通过求 `J(x)` 相对于 `x` 的梯度并将其设为零来找到平稳点。项 `F_k(x_0)` 和 `g^{\\top}x_0` 相对于 `x` 是常数，因此它们的梯度为零。我们计算其余项的梯度：\n$$\n\\nabla_x (g^{\\top}x) = g\n$$\n$$\n\\nabla_x (\\lambda \\alpha x^{\\top} I x) = 2 \\lambda \\alpha I x = 2 \\lambda \\alpha x\n$$\n$$\n\\nabla_x (\\lambda \\beta x^{\\top} L x) = \\lambda \\beta (L + L^{\\top})x = 2 \\lambda \\beta L x \\quad (\\text{因为 } L \\text{ 是对称的})\n$$\n综合这些，目标函数 `J(x)` 的梯度是：\n$$\n\\nabla_x J(x) = g - 2 \\lambda \\alpha x - 2 \\lambda \\beta L x\n$$\n将梯度设为零以找到平稳点，记作 `x^{\\star}`：\n$$\ng - 2 \\lambda \\alpha x^{\\star} - 2 \\lambda \\beta L x^{\\star} = 0\n$$\n我们现在可以求解 `x^{\\star}`：\n$$\ng = 2 \\lambda \\alpha x^{\\star} + 2 \\lambda \\beta L x^{\\star}\n$$\n$$\ng = 2 \\lambda (\\alpha I + \\beta L) x^{\\star}\n$$\n矩阵 `(\\alpha I + \\beta L)` 是可逆的。这是因为 `L` 是半正定的，意味着其所有特征值 `\\mu_i` 满足 `\\mu_i \\ge 0`。`(\\alpha I + \\beta L)` 的特征值是 `(\\alpha + \\beta \\mu_i)`。给定 `\\alpha  0` 和 `\\beta \\ge 0`，所有特征值 `(\\alpha + \\beta \\mu_i)` 都是严格为正的。具有严格正特征值的矩阵是正定的，因此是可逆的。\n\n左乘 `2 \\lambda (\\alpha I + \\beta L)` 的逆矩阵，我们得到最优输入 `x^{\\star}` 的闭式解：\n$$\nx^{\\star} = (2 \\lambda (\\alpha I + \\beta L))^{-1} g\n$$\n$$\nx^{\\star} = \\frac{1}{2\\lambda} (\\alpha I + \\beta L)^{-1} g\n$$\n这个 `x^{\\star}` 的表达式代表了合成输入，它在由正则化器 `R(x)` 编码的合理性约束下，最大化了单元 `k` 的线性化激活。\n\n神经影像中正则化器选择的理由：\n正则化器 `R(x) = \\alpha \\|x\\|_{2}^{2} + \\beta x^{\\top} L x` 非常适用于神经影像，因为它的组成部分编码了关于数据的强且现实的先验信念。\n1.  `\\alpha \\|x\\|_{2}^{2}` 项，即L2范数惩罚，不鼓励具有过大体素或传感器值的解。这具有物理动机，因为真实的神经影像信号具有有限的动态范围，极端值通常是伪影。该项促进了看起来更自然的“低能量”解。\n2.  `\\beta x^{\\top} L x` 项是一个空间平滑先验。涉及拉普拉斯算子 `L` 的二次型惩罚了输入 `x` 中局部空间变化的平方大小。这在两个主要方面是合适的：\n    -   **神经生理结构**：脑组织和神经活动在空间上是连续的。MRI中的解剖结构，如灰质和白质，是连续的区域。由于功能区的集群组织和像容积传导这样的物理效应，通过fMRI或EEG测量的功能活动在空间上也是相关的。平滑先验确保了合成的输入 `x^{\\star}` 尊重这种基本的生物组织结构。\n    -   **采集物理学**：所有神经影像模态都具有有限分辨率，并受点扩散函数（PSF）的影响，该函数内在地模糊了采集到的信号。这意味着物理上合理的输入必须相对平滑，并且不能包含超出扫描仪可分辨范围的频率。拉普拉斯正则化器强制执行这种平滑性，生成与数据采集物理原理一致的输入。\n\n像全变分（`\\|\\nabla x\\|_{1}`）这样的替代正则化器也很有用，因为它们促进分段平滑，这可以更好地保留MRI中不同组织之间的清晰边界。对于动态数据（例如，fMRI时间序列），时空正则化器可以在空间和时间上强制平滑，反映了神经活动模式连续演化的预期。带限平滑约束直接限制解的频率内容，这也可以由成像设备的物理原理来解释。",
            "answer": "$$\\boxed{\\frac{1}{2\\lambda} (\\alpha I + \\beta L)^{-1} g}$$"
        }
    ]
}