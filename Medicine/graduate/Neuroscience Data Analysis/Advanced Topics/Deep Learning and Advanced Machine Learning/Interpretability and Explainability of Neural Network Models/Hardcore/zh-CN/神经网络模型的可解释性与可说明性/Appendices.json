{
    "hands_on_practices": [
        {
            "introduction": "要理解一个模型，一个基本任务是量化每个输入特征对模型最终输出的贡献。积分梯度（Integrated Gradients）是一种强大的归因方法，它通过一个公理化的框架来确保归因的公平性和完整性。这个练习将引导你通过一个简化的线性模型，一步步计算积分梯度，从而清晰地揭示其核心机制和重要性质。",
            "id": "4171579",
            "problem": "一个神经科学实验室使用线性读出模型，将预处理后的功能性磁共振成像（fMRI）体素强度映射到一个无单位的标量分数，该分数表示预测的任务参与度。考虑一个包含 $d=3$ 个体素的感兴趣区域。模型为 $F(x)=w^{\\top}x$，其中 $x\\in\\mathbb{R}^{3}$ 是体素强度向量，$w\\in\\mathbb{R}^{3}$ 是学习到的权重。该实验室采用零模式作为其基线，即 $\\tilde{x}=\\mathbf{0}$，代表静息状态。使用积分梯度沿从 $\\tilde{x}$ 到 $x$ 的直线路径的形式化定义，计算输入 $x$ 的归因向量，其权重为\n$$w=\\begin{pmatrix}0.8\\\\-0.3\\\\0.5\\end{pmatrix},\\quad x=\\begin{pmatrix}2.0\\\\1.5\\\\0.0\\end{pmatrix}.$$\n以行矩阵的形式报告最终的归因向量。无需四舍五入。在 fMRI 的背景下，就体素对模型标量分数的贡献提供一个简要的解释。",
            "solution": "问题陈述被验证为具有科学依据、适定、客观和完整。所有必要的数据和定义都已提供，该任务是将机器学习中的一种标准可解释性方法直接应用于计算神经科学中一个简化但有效的场景。\n\n问题要求使用积分梯度（IG）方法计算给定输入 $x$ 的归因向量。第 $i$ 个特征的归因 $\\text{IG}_i(x)$ 的形式化定义为：\n$$\n\\text{IG}_i(x) = (x_i - \\tilde{x}_i) \\times \\int_{\\alpha=0}^{1} \\frac{\\partial F(\\tilde{x} + \\alpha(x - \\tilde{x}))}{\\partial x_i} \\, d\\alpha\n$$\n其中 $x$ 是输入，$\\tilde{x}$ 是基线，$F$ 是模型函数。积分路径是从基线 $\\tilde{x}$ 到输入 $x$ 的直线。\n\n给定参数如下：\n模型：$F(x) = w^{\\top}x$\n输入维度：$d=3$\n权重：$w=\\begin{pmatrix}0.8\\\\-0.3\\\\0.5\\end{pmatrix}$\n输入向量：$x=\\begin{pmatrix}2.0\\\\1.5\\\\0.0\\end{pmatrix}$\n基线向量：$\\tilde{x}=\\mathbf{0}=\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}$\n\n首先，我们确定模型函数 $F(x)$ 的梯度。该模型是一个线性函数：\n$$\nF(x) = w^{\\top}x = w_1 x_1 + w_2 x_2 + w_3 x_3\n$$\n$F(x)$ 关于输入特征 $x_i$ 的偏导数是：\n$$\n\\frac{\\partial F(x)}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} (w_1 x_1 + w_2 x_2 + w_3 x_3) = w_i\n$$\n因此，梯度向量 $\\nabla F(x)$ 就是权重向量 $w$：\n$$\n\\nabla F(x) = \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{pmatrix} = w\n$$\n对于线性模型，一个关键的观察是梯度是恒定的，不依赖于输入 $x$。\n\n积分路径定义为 $\\gamma(\\alpha) = \\tilde{x} + \\alpha(x - \\tilde{x})$。由于基线是零向量 $\\tilde{x} = \\mathbf{0}$，路径简化为 $\\gamma(\\alpha) = \\alpha x$。\n沿此路径计算的偏导数为：\n$$\n\\frac{\\partial F(\\gamma(\\alpha))}{\\partial x_i} = \\frac{\\partial F(\\alpha x)}{\\partial x_i} = w_i\n$$\n这是因为梯度在任何地方都是恒定的，包括在积分路径上。\n\n现在我们可以计算 IG 定义中的积分：\n$$\n\\int_{\\alpha=0}^{1} \\frac{\\partial F(\\tilde{x} + \\alpha(x - \\tilde{x}))}{\\partial x_i} \\, d\\alpha = \\int_{\\alpha=0}^{1} w_i \\, d\\alpha\n$$\n由于 $w_i$ 相对于积分变量 $\\alpha$ 是一个常数，我们有：\n$$\n\\int_{\\alpha=0}^{1} w_i \\, d\\alpha = w_i \\left[ \\alpha \\right]_{\\alpha=0}^{1} = w_i (1-0) = w_i\n$$\n将此结果代回 IG 公式，得到一个对于具有零基线的线性模型的简化表达式：\n$$\n\\text{IG}_i(x) = (x_i - \\tilde{x}_i) \\times w_i = (x_i - 0) \\times w_i = x_i w_i\n$$\n这意味着归因向量是输入向量 $x$ 和权重向量 $w$ 的逐元素乘积（哈达玛积），即 $\\text{IG}(x) = x \\odot w$。\n\n现在我们可以使用给定的值计算 $d=3$ 个体素中每一个的归因：\n对于第一个体素 ($i=1$)：\n$$\n\\text{IG}_1(x) = x_1 w_1 = (2.0) \\times (0.8) = 1.6\n$$\n对于第二个体素 ($i=2$)：\n$$\n\\text{IG}_2(x) = x_2 w_2 = (1.5) \\times (-0.3) = -0.45\n$$\n对于第三个体素 ($i=3$)：\n$$\n\\text{IG}_3(x) = x_3 w_3 = (0.0) \\times (0.5) = 0.0\n$$\n得到的归因向量是 $\\begin{pmatrix} 1.6 \\\\ -0.45 \\\\ 0.0 \\end{pmatrix}$。问题要求将其表示为行矩阵。\n\n积分梯度的一个关键属性是完整性（completeness），该属性表明所有归因的总和等于模型输出从基线到输入的变化量：$\\sum_{i=1}^{d} \\text{IG}_i(x) = F(x) - F(\\tilde{x})$。\n让我们来验证一下。\n归因总和为：$1.6 + (-0.45) + 0.0 = 1.15$。\n模型的输入输出为：$F(x) = w^{\\top}x = (0.8)(2.0) + (-0.3)(1.5) + (0.5)(0.0) = 1.6 - 0.45 + 0 = 1.15$。\n模型的基线输出为：$F(\\tilde{x}) = F(\\mathbf{0}) = w^{\\top}\\mathbf{0} = 0$。\n差值为 $F(x) - F(\\tilde{x}) = 1.15 - 0 = 1.15$。\n完整性属性成立，证实了我们的计算。\n\n解释：对于输入 $x$，模型预测的任务参与度分数为 $1.15$。归因向量 $\\begin{pmatrix} 1.6 & -0.45 & 0.0 \\end{pmatrix}$ 将此分数分解为来自每个体素的贡献。\n- 体素1的贡献值为 $1.6$，是正贡献。这表明它的激活是提高预测参与度分数的主要驱动因素。\n- 体素2的贡献值为 $-0.45$，是负贡献。它的激活被解释为反对任务参与的证据，主动地降低了分数。\n- 体素3的贡献值为 $0.0$。尽管模型为该体素学习到了一个正权重（$w_3=0.5$），但其在给定输入中的强度为零（$x_3=0.0$），因此它对这个特定预测没有影响。\n\n因此，归因向量提供了一种局部化的解释，将模型的输出归因于各个输入特征（体素强度）。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.6 & -0.45 & 0.0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "虽然积分梯度等方法很强大，但理解更朴素方法（如简单梯度）的局限性也同样重要。这个练习聚焦于“梯度饱和”问题，即在某些情况下，简单梯度会错误地为一个有重要影响的特征赋予零重要性。通过对比简单的梯度方法和一种基于参考的先进方法DeepLIFT，你将理解如何获得在因果上更忠实的归因，尤其是在处理神经科学模型中常见的带有阈值非线性的情况时。",
            "id": "4171630",
            "problem": "考虑一个单神经元模型，该模型通过一个整流线性单元（ReLU）实现，用于从二维刺激中预测感觉神经元的平均发放率。该模型通过一个两层复合结构将输入向量 $\\mathbf{x} = (x_{1}, x_{2})$ 映射到输出 $y$：一个仿射预激活 $z$，一个 ReLU 隐藏层激活 $h$，以及一个最终的仿射输出。具体而言，\n$$\nz = w_{1} x_{1} + w_{2} x_{2} + b,\\quad h = \\max(0, z),\\quad y = a\\,h + d,\n$$\n其中 $w_{1} = 2$，$w_{2} = -3$，$b = 1$，$a = 2$，且 $d = 0$。设参考基线输入为 $\\mathbf{x}^{0} = (0, 0)$，这被解释为静息刺激条件。实际观测到的输入为 $\\mathbf{x} = (0, 1)$。\n\n仅使用梯度的核心定义以及“深度学习重要特征”（DeepLIFT）背后的“与参考值的差异”原则，从基本原理出发进行推导：\n- 确定在输入 $\\mathbf{x}$ 处，对 $x_{2}$ 的基于梯度的归因是否因饱和而消失，并从模型的非线性角度解释其原因。\n- 导出 DeepLIFT 对输出变化 $y - y^{0}$（相对于参考基线）分配给 $x_{2}$ 的归因，推导过程需从局部贡献在网络各层满足“总和等于差值”性质这一要求出发。\n\n将 DeepLIFT 对 $x_{2}$ 的归因的最终数值表示为一个实数。无需四舍五入。在你的解释中，讨论在这种阈值速率模型中，饱和现象对于神经科学数据分析中可解释性的神经科学意义。",
            "solution": "首先验证问题，以确保其在科学上成立、良定且客观。\n\n### 第1步：提取已知条件\n- **模型定义**：一个将输入向量 $\\mathbf{x} = (x_{1}, x_{2})$ 映射到输出 $y$ 的单神经元模型。\n- **模型方程**：\n  $$z = w_{1} x_{1} + w_{2} x_{2} + b$$\n  $$h = \\max(0, z)$$\n  $$y = a\\,h + d$$\n- **模型参数**：\n  - $w_{1} = 2$\n  - $w_{2} = -3$\n  - $b = 1$\n  - $a = 2$\n  - $d = 0$\n- **参考输入**：$\\mathbf{x}^{0} = (0, 0)$\n- **实际输入**：$\\mathbf{x} = (0, 1)$\n- **任务**：\n  1. 确定在输入 $\\mathbf{x}$ 处，对 $x_{2}$ 的基于梯度的归因是否因饱和而消失。\n  2. 导出 DeepLIFT 对输出变化 $y - y^{0}$ 分配给 $x_{2}$ 的归因。\n  3. 提供 DeepLIFT 归因的最终数值。\n  4. 讨论神经科学意义。\n\n### 第2步：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **科学上成立**：所述模型是一个整流线性单元（ReLU），这是现代神经网络中一个标准且基本的组成部分。用它来模拟神经元的发放率（一个线性-非线性模型）是计算神经科学中一种常见的简化方法。基于梯度的归因和 DeepLIFT 是机器学习可解释性领域已确立的方法。该问题在科学上是合理的。\n- **良定**：所有参数（$w_1, w_2, b, a, d$）和输入（$\\mathbf{x}^0, \\mathbf{x}$）都已明确定义。函数没有歧义。可以推导出唯一、稳定且有意义的解。\n- **客观**：问题以精确的数学定义和一个客观的任务陈述。没有主观或基于意见的语言。\n\n所有其他验证标准（完整性、真实性、结构性）均已满足。该问题没有任何使其无效的缺陷。\n\n### 第3步：结论与行动\n问题是**有效的**。将提供详细的解决方案。\n\n### 解答推导\n\n该解答按要求分为三部分：基于梯度的归因分析、DeepLIFT 归因的推导，以及对意义的讨论。\n\n首先，我们计算神经元模型在参考输入 $\\mathbf{x}^{0}$ 和实际输入 $\\mathbf{x}$ 时的状态。\n\n**在参考输入 $\\mathbf{x}^{0} = (0, 0)$ 时：**\n- 预激活：$z^{0} = w_{1} x_{1}^{0} + w_{2} x_{2}^{0} + b = (2)(0) + (-3)(0) + 1 = 1$。\n- 隐藏层激活：$h^{0} = \\max(0, z^{0}) = \\max(0, 1) = 1$。\n- 输出：$y^{0} = a h^{0} + d = (2)(1) + 0 = 2$。\n\n**在实际输入 $\\mathbf{x} = (0, 1)$ 时：**\n- 预激活：$z = w_{1} x_{1} + w_{2} x_{2} + b = (2)(0) + (-3)(1) + 1 = -2$。\n- 隐藏层激活：$h = \\max(0, z) = \\max(0, -2) = 0$。\n- 输出：$y = a h + d = (2)(0) + 0 = 0$。\n\n由此，我们可以确定每个变量的变化量（delta, $\\Delta$）：\n- $\\Delta y = y - y^{0} = 0 - 2 = -2$。\n- $\\Delta h = h - h^{0} = 0 - 1 = -1$。\n- $\\Delta z = z - z^{0} = -2 - 1 = -3$。\n- $\\Delta x_{1} = x_{1} - x_{1}^{0} = 0 - 0 = 0$。\n- $\\Delta x_{2} = x_{2} - x_{2}^{0} = 1 - 0 = 1$。\n\n#### 第1部分：基于梯度的归因与饱和\n\n基于梯度的归因方法通过输出对输入特征的局部梯度来近似该特征的重要性。对 $x_{2}$ 的归因基于在实际输入 $\\mathbf{x} = (0, 1)$ 处计算的偏导数 $\\frac{\\partial y}{\\partial x_{2}}$。\n\n使用链式法则：\n$$\n\\frac{\\partial y}{\\partial x_{2}} = \\frac{\\partial y}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z} \\cdot \\frac{\\partial z}{\\partial x_{2}}\n$$\n我们计算每一项：\n- $\\frac{\\partial y}{\\partial h} = a = 2$。\n- $\\frac{\\partial z}{\\partial x_{2}} = w_{2} = -3$。\n- ReLU 函数 $h(z) = \\max(0, z)$ 的导数是：\n$$\n\\frac{\\partial h}{\\partial z} = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{if } z  0 \\end{cases}\n$$\n（导数在 $z=0$ 处未定义，但为了反向传播的目的，通常将其设为 $0$）。\n\n在输入 $\\mathbf{x} = (0, 1)$ 处，我们计算出 $z = -2$。由于 $z  0$，该神经元处于非激活或“饱和”区域。\n因此，在该点 $\\frac{\\partial h}{\\partial z} = 0$。\n\n完整的梯度是：\n$$\n\\frac{\\partial y}{\\partial x_{2}}\\bigg|_{\\mathbf{x}=(0,1)} = (2) \\cdot (0) \\cdot (-3) = 0\n$$\n对 $x_{2}$ 的基于梯度的归因为零。这是因为梯度是一个局部敏感性的度量。在点 $\\mathbf{x}=(0,1)$ 处，预激活值 $z=-2$ 低于 ReLU 的阈值 $0$。对 $x_{2}$ 的无穷小改变不足以使 $z$ 跨越该阈值，因此输出 $y$ 保持在 $0$ 不变。模型的非线性（ReLU 阈值）导致梯度消失，这一现象被称为饱和问题。\n\n#### 第2部分：DeepLIFT 归因\n\nDeepLIFT 通过将输出的总变化量 $\\Delta y$ 反向传播到输入的总变化量 $\\Delta x_{i}$ 来分配归因分数，其依据是“总和等于差值”原则，该原则要求归因的总和等于输出的总变化量。对于给定的层，若其输出变化为 $\\Delta O$，输入变化为 $\\Delta I_j$，则归因必须满足 $\\sum_j C_{\\Delta I_j \\Delta O} = \\Delta O$。我们通过将贡献从输出反向传播到输入，来推导 $\\Delta x_2$ 对 $\\Delta y$ 的归因，记为 $C_{\\Delta x_2 \\Delta y}$。\n\n1.  **来自输出层的归因 ($y = ah+d$)**:\n    变化量 $\\Delta y = -2$ 完全是由变化量 $\\Delta h = -1$ 通过线性变换 $\\Delta y = a \\Delta h$ 引起的。我们想要找到 $\\Delta x_2$ 对 $\\Delta y$ 的贡献。根据该层的线性性质，这个贡献是 $C_{\\Delta x_2 \\Delta y} = a \\cdot C_{\\Delta x_2 \\Delta h}$，其中 $C_{\\Delta x_2 \\Delta h}$ 是输入变化 $\\Delta x_2$ 对中间变化 $\\Delta h$ 的贡献。常数是 $a=2$，所以我们需要求出 $C_{\\Delta x_2 \\Delta h}$。\n\n2.  **来自隐藏激活层的归因 ($h = \\max(0,z)$)**:\n    在这里，我们将变化量 $\\Delta h$ 归因于其输入的变化量 $\\Delta z$。DeepLIFT 使用一个乘子（或“拯救者”项），该乘子用参考激活点和实际激活点之间的“割线”斜率来近似局部导数：$m_{\\Delta z \\to \\Delta h} = \\frac{\\Delta h}{\\Delta z} = \\frac{-1}{-3} = \\frac{1}{3}$。这个非零乘子使得归因能够流经饱和的神经元。为了求出 $C_{\\Delta x_2 \\Delta h}$，我们应用 DeepLIFT 链式法则，该法则根据对 $\\Delta z$ 的输入贡献，来分配对 $\\Delta h$ 的总贡献（即 $\\Delta h$ 本身，因为 $z$ 是唯一的输入）。\n    $$C_{\\Delta x_2 \\Delta h} = \\Delta h \\times \\frac{C_{\\Delta x_2 \\Delta z}}{\\Delta z}$$\n    我们需要求出 $C_{\\Delta x_2 \\Delta z}$，即 $\\Delta x_2$ 对 $\\Delta z$ 的贡献。\n\n3.  **来自输入仿射层的归因 ($z = w_1 x_1 + w_2 x_2 + b$)**:\n    预激活的变化量是 $\\Delta z = w_1 \\Delta x_1 + w_2 \\Delta x_2$。对于这个线性层，DeepLIFT 根据每个输入在变化中的直接作用来分配贡献。\n    - $\\Delta x_1$ 对 $\\Delta z$ 的贡献：$C_{\\Delta x_1 \\Delta z} = w_1 \\Delta x_1 = (2)(0) = 0$。\n    - $\\Delta x_2$ 对 $\\Delta z$ 的贡献：$C_{\\Delta x_2 \\Delta z} = w_2 \\Delta x_2 = (-3)(1) = -3$。\n    这些贡献的总和是 $0 + (-3) = -3$，这与 $\\Delta z$ 的值是相符的。\n\n4.  **最终计算**:\n    现在我们将这些值代回到链式计算中。\n    - 将第3步的结果用于第2步：\n      $$C_{\\Delta x_2 \\Delta h} = \\Delta h \\times \\frac{C_{\\Delta x_2 \\Delta z}}{\\Delta z} = (-1) \\times \\frac{-3}{-3} = -1 \\times 1 = -1$$\n    - 将此结果用于第1步：\n      $$C_{\\Delta x_2 \\Delta y} = a \\cdot C_{\\Delta x_2 \\Delta h} = (2) \\cdot (-1) = -2$$\n\n分配给 $x_2$ 的 DeepLIFT 归因值为 $-2$。这个非零值正确地反映了 $x_2$ 从 $0$ 变为 $1$ 导致神经元输出从 $y^0=2$ 变为 $y=0$。“总和等于差值”性质得到满足：$C_{\\Delta x_1 \\Delta y} + C_{\\Delta x_2 \\Delta y} = 0 + (-2) = -2 = \\Delta y$。\n\n#### 第3部分：神经科学意义\n\n基于梯度的归因（$0$）与 DeepLIFT 归因（$-2$）之间的差异，对于分析神经科学数据的模型的可解释性具有重要的神经科学意义。\n\n1.  **抑制的重要性**：在生物神经回路中，抑制与兴奋同样至关重要。一个强烈抑制神经元使其静默的感觉特征（如 $x_2$）在生物学上是非常重要的。基于梯度的方法会给这个特征分配零重要性，因为神经元的发放率对它不具有局部敏感性——它已经处于静默状态。这未能捕捉到该特征的因果作用。\n\n2.  **基于参考的解释**：DeepLIFT 通过使用一个参考点（例如，基线刺激 $\\mathbf{x}^0$）来克服这个问题。神经科学研究通常是比较性的，探究神经元的反应如何从一个控制/基线条件*变化*到一个刺激条件。DeepLIFT 的方法论直接反映了这种实验逻辑。它解释的是活动的*差异*（$\\Delta y$），而不仅仅是最终的活动状态。它正确地识别出 $x_2$ 的变化导致神经元从一个激活状态（在基线时）切换到一个非激活状态。\n\n3.  **避免误导性解释**：在阈值速率模型中，依赖简单梯度来进行解释可能会产生严重的误导。人们可能错误地断定某个特征没有影响，而实际上它具有强大的静默效应。对于理解感觉编码而言，这种区分至关重要。DeepLIFT 及类似方法提供了一种更稳健的特征重要性说明，它更好地与模型及其所代表的生物系统的底层因果结构保持一致。\n\n总之，在解释具有阈值的模型（这是神经系统的普遍特征）时，简单梯度方法的饱和问题是一个根本性缺陷。像 DeepLIFT 这样基于参考的方法，为特征对神经状态变化的贡献提供了一种因果上更忠实的解释。",
            "answer": "$$\n\\boxed{-2}\n$$"
        },
        {
            "introduction": "除了为特定预测进行归因，可解释性的另一个核心问题是：“这个神经元或特征探测器到底在寻找什么样的模式？” 这个练习将介绍激活最大化（activation maximization）技术，它旨在通过优化合成一个能最大程度激活网络中特定单元的“理想”输入。此练习的重点在于将此问题构建为一个正则化优化问题，并展示如何将现实的神经生理学先验知识融入其中，以生成看似合理且易于解释的特征可视化。",
            "id": "4171554",
            "problem": "考虑一个在神经影像数据上训练的深度神经网络 (DNN)，例如磁共振成像 (MRI)、功能性磁共振成像 (fMRI) 或脑电图 (EEG)，其中由 $k$ 索引的内部单元对于代表标准化神经影像（例如，一个体积的体素强度或传感器-时间特征）的输入 $x \\in \\mathbb{R}^{n}$ 产生一个标量激活值 $F_{k}(x)$。在激活最大化中，目标是构建一个输入 $x$，以最大化所选单元的激活，同时避免那些在神经生理结构和采集物理原理下不合理的输入。从第一性原理出发，将激活最大化表述为一个资源受限的优化问题，该问题通过惩罚泛函强制施加合理性约束。然后，使用拉格朗日乘子推导出无约束的惩罚形式，并明确待优化的目标。\n\n为获得适合科学检验的易于处理的解析解，假设在固定基线 $x_{0} \\in \\mathbb{R}^{n}$ 周围进行局部一阶近似，\n$$\nF_{k}(x) \\approx F_{k}(x_{0}) + g^{\\top}(x - x_{0}),\n$$\n其中 $g = \\nabla_{x} F_{k}(x_{0}) \\in \\mathbb{R}^{n}$ 是单元激活在 $x_{0}$ 处相对于输入的梯度。考虑一个适用于体积神经影像的正则化器，其空间邻接性由一个对称半正定离散拉普拉斯算子 $L \\in \\mathbb{R}^{n \\times n}$ (诺伊曼边界) 表示，即\n$$\nR(x) = \\alpha \\|x\\|_{2}^{2} + \\beta x^{\\top} L x,\n$$\n其中 $\\alpha > 0$ 且 $\\beta \\ge 0$。使用带有拉格朗日乘子 $\\lambda > 0$ 的惩罚形式，推导平稳条件并计算线性化惩罚目标的闭式解析最大化器 $x^{\\star} \\in \\mathbb{R}^{n}$。\n\n此外，通过参考神经生理结构（例如，组织的空间连续性）和采集物理原理（例如，点扩散和重建），证明为何像上述 $R(x)$ 这样的选择，以及诸如全变分、带限平滑度和时空耦合等替代方案，对于神经影像输入是合适的。您的最终答案必须仅为 $x^{\\star}$ 的闭式解析表达式。不要包含任何单位。如果您引入任何额外的超参数，请明确说明它们是正数。不需要进行数值舍入。将您的最终答案表示为单个闭式解析表达式。",
            "solution": "问题陈述被评估为有效，因为它在科学上基于机器学习和神经影像的原理，问题定义良好且有唯一解，并以客观、正式的语言表达。它没有矛盾、歧义或事实上的不健全之处。\n\n任务是为在神经影像数据上训练的深度神经网络 (DNN) 中的一个单元，构建并求解一个激活最大化问题。解应该是一个能最大化线性化和正则化目标的输入 $x^{\\star}$ 的闭式表达式。\n\n首先，我们将激活最大化表述为一个约束优化问题。目标是找到一个输入 $x \\in \\mathbb{R}^{n}$ 来最大化神经元 $k$ 的激活 $F_k(x)$，约束条件是 $x$ 是合理的。合理性通过惩罚具有高正则化器 $R(x)$ 值的输入来强制执行。这可以表示为将由 $R(x)$ 衡量的输入“成本”约束为小于或等于某个预算 $c > 0$。该约束优化问题是：\n$$\n\\max_{x \\in \\mathbb{R}^n} F_k(x) \\quad \\text{subject to} \\quad R(x) \\le c\n$$\n\n为了解决这个问题，我们使用拉格朗日乘子 $\\lambda > 0$ 将其转换为一个无约束优化问题。这就是惩罚形式，我们的目标是最大化激活 $F_k(x)$，同时最小化惩罚 $R(x)$。这两者之间的权衡由 $\\lambda$ 控制。待最大化的无约束目标函数 $J(x)$ 是：\n$$\nJ(x) = F_k(x) - \\lambda R(x)\n$$\n我们寻求 $x^{\\star} = \\arg\\max_{x \\in \\mathbb{R}^n} J(x)$。\n\n问题指定了 $F_k(x)$ 在基线输入 $x_0 \\in \\mathbb{R}^{n}$ 周围的一阶泰勒近似：\n$$\nF_k(x) \\approx F_k(x_0) + g^{\\top}(x - x_0)\n$$\n其中 $g = \\nabla_{x} F_k(x_0)$ 是在 $x_0$ 处评估的激活相对于输入的梯度。\n\n问题还提供了一个适用于体积神经影像的特定正则化器 $R(x)$：\n$$\nR(x) = \\alpha \\|x\\|_{2}^{2} + \\beta x^{\\top} L x\n$$\n其参数为 $\\alpha > 0$、$\\beta \\ge 0$，且 $L$ 是一个对称半正定离散拉普拉斯矩阵。注意 $\\|x\\|_{2}^{2}$ 可以写成 $x^{\\top}x = x^{\\top}Ix$，其中 $I$ 是 $n \\times n$ 的单位矩阵。\n\n将近似式代入目标函数 $J(x)$，我们得到线性化惩罚目标：\n$$\nJ(x) \\approx F_k(x_0) + g^{\\top}(x - x_0) - \\lambda (\\alpha x^{\\top} I x + \\beta x^{\\top} L x)\n$$\n为了找到最大化器 $x^{\\star}$，我们必须通过求 $J(x)$ 相对于 $x$ 的梯度并令其为零来找到驻点。$F_k(x_0)$ 和 $g^{\\top}x_0$ 项相对于 $x$ 是常数，因此它们的梯度为零。我们计算其余项的梯度：\n$$\n\\nabla_x (g^{\\top}x) = g\n$$\n$$\n\\nabla_x (\\lambda \\alpha x^{\\top} I x) = 2 \\lambda \\alpha I x = 2 \\lambda \\alpha x\n$$\n$$\n\\nabla_x (\\lambda \\beta x^{\\top} L x) = \\lambda \\beta (L + L^{\\top})x = 2 \\lambda \\beta L x \\quad (\\text{因为 } L \\text{ 是对称的})\n$$\n结合这些，目标函数 $J(x)$ 的梯度是：\n$$\n\\nabla_x J(x) = g - 2 \\lambda \\alpha x - 2 \\lambda \\beta L x\n$$\n将梯度设为零以找到驻点，记为 $x^{\\star}$：\n$$\ng - 2 \\lambda \\alpha x^{\\star} - 2 \\lambda \\beta L x^{\\star} = 0\n$$\n现在我们可以解出 $x^{\\star}$：\n$$\ng = 2 \\lambda \\alpha x^{\\star} + 2 \\lambda \\beta L x^{\\star}\n$$\n$$\ng = 2 \\lambda (\\alpha I + \\beta L) x^{\\star}\n$$\n矩阵 $(\\alpha I + \\beta L)$ 是可逆的。这是因为 $L$ 是半正定的，意味着其所有特征值 $\\mu_i$ 满足 $\\mu_i \\ge 0$。$(\\alpha I + \\beta L)$ 的特征值是 $(\\alpha + \\beta \\mu_i)$。给定 $\\alpha > 0$ 和 $\\beta \\ge 0$，所有特征值 $(\\alpha + \\beta \\mu_i)$ 都严格为正。具有严格为正的特征值的矩阵是正定的，因此是可逆的。\n\n左乘 $2 \\lambda (\\alpha I + \\beta L)$ 的逆矩阵，我们得到最优输入 $x^{\\star}$ 的闭式解：\n$$\nx^{\\star} = (2 \\lambda (\\alpha I + \\beta L))^{-1} g\n$$\n$$\nx^{\\star} = \\frac{1}{2\\lambda} (\\alpha I + \\beta L)^{-1} g\n$$\n这个 $x^{\\star}$ 的表达式代表了在由正则化器 $R(x)$ 编码的合理性约束下，能最大化单元 $k$ 的线性化激活的合成输入。\n\n神经影像中正则化器选择的理由：\n正则化器 $R(x) = \\alpha \\|x\\|_{2}^{2} + \\beta x^{\\top} L x$ 非常适合神经影像，因为它的组成部分编码了关于数据的强大而现实的先验信念。\n1. $\\alpha \\|x\\|_{2}^{2}$ 项，一个L2范数惩罚，抑制了具有过大体素或传感器值的解。这有其物理动机，因为真实的神经影像信号具有有限的动态范围，极端值通常是伪影。该项促进了看起来更自然的“低能量”解。\n2. $\\beta x^{\\top} L x$ 项是一个空间平滑先验。涉及拉普拉斯算子 $L$ 的二次型惩罚了输入 $x$ 中局部空间变化的平方大小。这之所以合适，主要有两个原因：\n    - **神经生理结构**：脑组织和神经活动在空间上是连续的。MRI中的解剖结构，如灰质和白质，是连续的区域。由于功能区的集群组织和像容积传导这样的物理效应，由fMRI或EEG测量的功能活动在空间上也是相关的。平滑先验确保合成的输入 $x^{\\star}$ 尊重这种基本的生物组织结构。\n    - **采集物理原理**：所有神经影像模态都具有有限的分辨率，并受到点扩散函数 (PSF) 的影响，该函数固有地模糊了采集到的信号。这意味着物理上合理的输入必须相对平滑，并且不能包含超出扫描仪可分辨范围的频率。拉普拉斯正则化器强制执行这种平滑性，生成与数据采集的物理原理相一致的输入。\n\n像全变分 ($\\|\\nabla x\\|_{1}$) 这样的替代正则化器也很有用，因为它们促进分段平滑，这可以更好地保留MRI中不同组织之间的清晰边界。对于动态数据（例如fMRI时间序列），时空正则化器可以在空间和时间上都强制平滑，这反映了神经活动模式连续演变的期望。带限平滑度约束直接限制了解的频率内容，这也可以由成像设备的物理原理来解释。",
            "answer": "$$\\boxed{\\frac{1}{2\\lambda} (\\alpha I + \\beta L)^{-1} g}$$"
        }
    ]
}