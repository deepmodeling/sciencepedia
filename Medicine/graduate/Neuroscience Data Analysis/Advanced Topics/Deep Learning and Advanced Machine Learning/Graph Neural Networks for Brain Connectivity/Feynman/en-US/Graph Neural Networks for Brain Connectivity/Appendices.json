{
    "hands_on_practices": [
        {
            "introduction": "The first step in applying Graph Neural Networks to neuroimaging data is constructing a meaningful graph. This practice guides you through the fundamental pipeline of converting functional MRI (fMRI) time series into a functional connectome. By computing correlations, applying statistical transformations, and exploring the effects of thresholding, you will gain hands-on experience with how preprocessing choices shape the final graph structure and the node-level features that a GNN will ultimately learn from .",
            "id": "4167854",
            "problem": "You are given three synthetic Region of Interest (ROI) time series datasets intended to model brain connectivity, where each dataset consists of $n$ regions and $T$ time points. The task is to construct undirected graphs from these time series by computing the Pearson correlation matrix, applying the Fisher $z$-transform, and performing proportional thresholding to retain a specified fraction of strongest absolute Fisher values. You must then quantify how the threshold level affects the graph’s degree distribution and the node features that would be fed to a Graph Neural Network (GNN).\n\nFundamental base and definitions to use:\n- Let $X \\in \\mathbb{R}^{T \\times n}$ be the ROI time series matrix with columns indexed by regions. The sample Pearson correlation between ROI $i$ and ROI $j$ is defined by\n$$\nr_{ij} = \\frac{\\sum_{t=1}^{T} \\left(X_{t,i} - \\bar{X}_i\\right)\\left(X_{t,j} - \\bar{X}_j\\right)}{\\sqrt{\\sum_{t=1}^{T} \\left(X_{t,i} - \\bar{X}_i\\right)^2} \\, \\sqrt{\\sum_{t=1}^{T} \\left(X_{t,j} - \\bar{X}_j\\right)^2}},\n$$\nwhere $\\bar{X}_i$ is the sample mean of the time series for ROI $i$.\n- The Fisher $z$-transform of $r_{ij}$ is\n$$\nz_{ij} = \\operatorname{atanh}(r_{ij}) = \\frac{1}{2}\\ln\\left(\\frac{1 + r_{ij}}{1 - r_{ij}}\\right),\n$$\nwhich maps correlation coefficients from $(-1,1)$ to $\\mathbb{R}$ and is used to stabilize variance under standard assumptions for correlation estimation.\n- Proportional thresholding at level $p \\in (0,1]$ for an undirected, loopless graph with $n$ nodes retains the $K$ strongest undirected edges, where\n$$\nK = \\left\\lfloor p \\cdot \\frac{n(n-1)}{2} \\right\\rfloor.\n$$\nDefine the weighted adjacency matrix $W^{(p)} \\in \\mathbb{R}^{n \\times n}$ such that for the $K$ selected undirected pairs $(i,j)$ with $i < j$, the weight is $W^{(p)}_{ij} = W^{(p)}_{ji} = \\left|z_{ij}\\right|$, and $W^{(p)}_{ii} = 0$. Define the corresponding binary adjacency matrix $A^{(p)} \\in \\{0,1\\}^{n \\times n}$ with $A^{(p)}_{ij} = 1$ if $(i,j)$ is selected and $A^{(p)}_{ij} = 0$ otherwise, and $A^{(p)}_{ii} = 0$.\n- For a graph represented by $A^{(p)}$, the degree of node $i$ is\n$$\nk_i^{(p)} = \\sum_{j=1}^{n} A^{(p)}_{ij}.\n$$\nThe coefficient of variation of the degree distribution is\n$$\n\\operatorname{CV}^{(p)} = \\frac{\\operatorname{std}\\left(\\{k_i^{(p)}\\}_{i=1}^n\\right)}{\\operatorname{mean}\\left(\\{k_i^{(p)}\\}_{i=1}^n\\right)},\n$$\nwith the convention that if the mean is $0$, then $\\operatorname{CV}^{(p)} = 0$.\n- Define the node strength under $W^{(p)}$ as\n$$\ns_i^{(p)} = \\sum_{j=1}^{n} W^{(p)}_{ij}.\n$$\nLet the full, unthresholded node strength reference be\n$$\ns_i^{\\mathrm{full}} = \\sum_{j=1}^{n} \\left|z_{ij}\\right| \\quad \\text{with} \\quad z_{ii} = 0.\n$$\nDefine the normalized degree and normalized strength for node $i$ as\n$$\n\\tilde{k}_i^{(p)} = \\frac{k_i^{(p)}}{n-1}, \\quad \\tilde{s}_i^{(p)} = \n\\begin{cases}\n\\frac{s_i^{(p)}}{s_i^{\\mathrm{full}}}, & \\text{if } s_i^{\\mathrm{full}} > 0,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nDefine the local binary clustering coefficient at node $i$ under $A^{(p)}$ as\n$$\nc_i^{(p)} = \n\\begin{cases}\n\\frac{2\\, \\tau_i^{(p)}}{k_i^{(p)}\\left(k_i^{(p)} - 1\\right)}, & \\text{if } k_i^{(p)} \\ge 2,\\\\\n0, & \\text{otherwise},\n\\end{cases}\n$$\nwhere $\\tau_i^{(p)}$ is the number of triangles including node $i$ counted on $A^{(p)}$.\n- Define the node feature vector intended for a Graph Neural Network (GNN) at threshold $p$ as\n$$\n\\mathbf{f}_i^{(p)} = \\left[\\tilde{k}_i^{(p)}, \\, \\tilde{s}_i^{(p)}, \\, c_i^{(p)}\\right] \\in \\mathbb{R}^3,\n$$\nand the feature matrix $F^{(p)} \\in \\mathbb{R}^{n \\times 3}$ stacks these row-wise. Quantify feature change between successive thresholds by the Frobenius norm\n$$\n\\Delta^{(p_\\ell)} = \n\\begin{cases}\n0, & \\text{if } \\ell = 1,\\\\\n\\left\\|F^{(p_\\ell)} - F^{(p_{\\ell-1})}\\right\\|_F, & \\text{if } \\ell \\ge 2,\n\\end{cases}\n$$\nfor an ordered threshold list $\\{p_\\ell\\}_{\\ell=1}^m$.\n\nSynthetic datasets and thresholds:\nFor each dataset below, form $X$ according to the specified deterministic signals and mixing, then follow the pipeline described above. Use the threshold list $\\{p_\\ell\\} = \\{0.2, 0.5, 1.0\\}$.\n\n- Dataset $1$ ($n = 5$, $T = 200$):\n  - Base signals for $t \\in \\{0,1,\\dots,199\\}$:\n    $s_1[t] = \\sin(2\\pi \\cdot 0.03 \\cdot t)$,\n    $s_2[t] = \\cos(2\\pi \\cdot 0.07 \\cdot t + \\pi/4)$,\n    $s_3[t] = \\sin(2\\pi \\cdot 0.13 \\cdot t)$.\n  - Mixing matrix $W \\in \\mathbb{R}^{5 \\times 3}$:\n    $W = \\begin{bmatrix}\n    1.0 & 0.5 & 0.0\\\\\n    0.9 & -0.4 & 0.1\\\\\n    1.1 & 0.6 & -0.2\\\\\n    -1.0 & -0.5 & 0.3\\\\\n    0.2 & 0.0 & 1.0\n    \\end{bmatrix}$.\n  - Deterministic noise: for ROI $i \\in \\{0,1,2,3,4\\}$,\n    $\\eta_i[t] = 0.1 \\cdot \\sin\\left(2\\pi \\cdot (0.23 + 0.01 i) \\cdot t\\right)$.\n  - Construct $S \\in \\mathbb{R}^{200 \\times 3}$ with columns $s_1, s_2, s_3$, and $X = S W^\\top + N$ where $N_{t,i} = \\eta_i[t]$.\n\n- Dataset $2$ ($n = 4$, $T = 30$):\n  - Base signals for $t \\in \\{0,1,\\dots,29\\}$:\n    $s_1[t] = \\sin(2\\pi \\cdot 0.12 \\cdot t)$,\n    $s_2[t] = \\sin(2\\pi \\cdot 0.12 \\cdot t + \\pi)$,\n    $s_3[t] = \\cos(2\\pi \\cdot 0.05 \\cdot t)$.\n  - Mixing matrix $W \\in \\mathbb{R}^{4 \\times 3}$:\n    $W = \\begin{bmatrix}\n    1.0 & 0.0 & 0.5\\\\\n    -1.0 & 0.0 & -0.2\\\\\n    0.3 & 0.7 & 0.0\\\\\n    0.0 & -0.5 & 0.8\n    \\end{bmatrix}$.\n  - Deterministic noise: for ROI $i \\in \\{0,1,2,3\\}$,\n    $\\eta_i[t] = 0.2 \\cdot \\cos\\left(2\\pi \\cdot (0.31 + 0.02 i) \\cdot t\\right)$.\n  - Construct $S \\in \\mathbb{R}^{30 \\times 3}$ with columns $s_1, s_2, s_3$, and $X = S W^\\top + N$.\n\n- Dataset $3$ ($n = 6$, $T = 100$):\n  - Base signals for $t \\in \\{0,1,\\dots,99\\}$:\n    $s_A[t] = \\sin(2\\pi \\cdot 0.04 \\cdot t)$,\n    $s_B[t] = \\sin(2\\pi \\cdot 0.08 \\cdot t + \\pi/3)$,\n    $s_C[t] = 0.3 \\cdot \\cos(2\\pi \\cdot 0.02 \\cdot t)$.\n  - Mixing matrices:\n    $W_{AB} \\in \\mathbb{R}^{6 \\times 2}$:\n    $W_{AB} = \\begin{bmatrix}\n    0.8 & 0.1\\\\\n    0.9 & 0.0\\\\\n    0.85 & -0.1\\\\\n    0.0 & 0.9\\\\\n    -0.1 & 0.8\\\\\n    0.2 & 0.85\n    \\end{bmatrix}$,\n    and $w_C \\in \\mathbb{R}^{6}$:\n    $w_C = \\begin{bmatrix}\n    0.2\\\\ -0.2\\\\ 0.1\\\\ -0.1\\\\ 0.05\\\\ -0.05\n    \\end{bmatrix}$.\n  - Deterministic noise: for ROI $i \\in \\{0,1,2,3,4,5\\}$,\n    $\\eta_i[t] = 0.05 \\cdot \\sin\\left(2\\pi \\cdot (0.17 + 0.01 i) \\cdot t\\right)$.\n  - Construct $S_{AB} \\in \\mathbb{R}^{100 \\times 2}$ with columns $s_A, s_B$, and $X_{AB} = S_{AB} W_{AB}^\\top$. Construct $S_C \\in \\mathbb{R}^{100}$ from $s_C$ and $X_C = S_C \\cdot w_C^\\top$. Then $X = X_{AB} + X_C + N$.\n\nProcessing and quantification tasks for each dataset:\n1. Compute the Pearson correlation matrix $C \\in \\mathbb{R}^{n \\times n}$ from $X$.\n2. Compute the Fisher $z$-transform matrix $Z \\in \\mathbb{R}^{n \\times n}$ element-wise from $C$. For numerical stability, ensure $r_{ij} \\in (-1,1)$ before applying $\\operatorname{atanh}$ by clipping $r_{ij}$ to $(-1+\\epsilon, 1-\\epsilon)$ for a small $\\epsilon$.\n3. For each threshold $p \\in \\{0.2, 0.5, 1.0\\}$, perform proportional thresholding on $\\left|Z\\right|$ by selecting exactly $K = \\left\\lfloor p \\cdot \\frac{n(n-1)}{2} \\right\\rfloor$ undirected edges with largest values of $\\left|Z_{ij}\\right|$ for $i<j$. Construct $W^{(p)}$ and $A^{(p)}$ as defined above.\n4. Compute the degree vector $\\{k_i^{(p)}\\}$, the coefficient of variation $\\operatorname{CV}^{(p)}$, and the node feature matrix $F^{(p)}$ using $\\mathbf{f}_i^{(p)} = [\\tilde{k}_i^{(p)}, \\tilde{s}_i^{(p)}, c_i^{(p)}]$ with normalization by $s_i^{\\mathrm{full}}$ computed from the full $\\left|Z\\right|$ (no threshold).\n5. For the ordered thresholds $\\{0.2, 0.5, 1.0\\}$, compute the feature change norms $\\Delta^{(0.2)} = 0$, $\\Delta^{(0.5)} = \\left\\|F^{(0.5)} - F^{(0.2)}\\right\\|_F$, and $\\Delta^{(1.0)} = \\left\\|F^{(1.0)} - F^{(0.5)}\\right\\|_F$.\n\nTest suite and output specification:\n- Apply the above pipeline to each of the three datasets.\n- For each dataset, produce two lists:\n  - The list of degree distribution coefficients of variation $\\left[\\operatorname{CV}^{(0.2)}, \\operatorname{CV}^{(0.5)}, \\operatorname{CV}^{(1.0)}\\right]$.\n  - The list of feature change norms $\\left[\\Delta^{(0.2)}, \\Delta^{(0.5)}, \\Delta^{(1.0)}\\right]$.\n- Your program should produce a single line of output containing the results for the three datasets as a comma-separated list of three items, each item itself being a comma-separated list of two lists, enclosed in square brackets. The exact format is:\n$$\n\\big[ [\\,[\\operatorname{CVs\\ for\\ dataset\\ 1}], [\\Delta\\text{s for dataset 1}]\\,], \\; [\\,[\\operatorname{CVs\\ for\\ dataset\\ 2}], [\\Delta\\text{s for dataset 2}]\\,], \\; [\\,[\\operatorname{CVs\\ for\\ dataset\\ 3}], [\\Delta\\text{s for dataset 3}]\\,] \\big],\n$$\nwith numeric values printed as decimal floats. For example, the structure is like $\\big[ [[x_1,x_2,x_3],[y_1,y_2,y_3]], [[\\dots],[\\dots]], [[\\dots],[\\dots]] \\big]$.",
            "solution": "The user-provided problem is a well-defined computational task in the domain of network neuroscience and graph signal processing. It requires constructing and analyzing graphs from synthetic time-series data, a common procedure for benchmarking methods like Graph Neural Networks (GNNs). The problem is scientifically grounded, logically consistent, and provides all necessary information for a unique, deterministic solution. All definitions and computational steps are formally specified. Therefore, the problem is valid, and a full solution is presented below.\n\nThe solution follows a multi-step pipeline for each of the three provided datasets. The core steps involve generating the time series, constructing a brain connectivity graph, and extracting node features at different sparsity levels, finally quantifying the changes in these features.\n\n### Step 1: Time Series Generation\n\nFor each dataset, a multivariate time series matrix $X \\in \\mathbb{R}^{T \\times n}$ is generated, where $n$ is the number of Regions of Interest (ROIs) and $T$ is the number of time points. The construction is based on a linear mixing model, a standard approach for creating synthetic data with a known ground-truth structure.\n\n- A set of base signals (sinusoids) are defined, forming the columns of a matrix $S$.\n- These signals are linearly combined using a dataset-specific mixing matrix $W$.\n- A deterministic noise signal $N$ is added.\n\nThe final time series matrix is formed as $X = \\text{signal} + \\text{noise}$. The specifics for each dataset are as follows:\n\n- **Dataset 1 ($n=5, T=200$):** $X = S W^\\top + N$, where $S \\in \\mathbb{R}^{200 \\times 3}$ contains three base sinusoidal signals.\n- **Dataset 2 ($n=4, T=30$):** $X = S W^\\top + N$, where $S \\in \\mathbb{R}^{30 \\times 3}$ contains three base signals. One base signal is perfectly anti-correlated with another ($s_2[t] = -s_1[t]$).\n- **Dataset 3 ($n=6, T=100$):** $X = S_{AB} W_{AB}^\\top + S_C w_C^\\top + N$. This involves a more complex mixing of three base signals ($s_A, s_B, s_C$) via two separate mixing operations before summation.\n\n### Step 2: Correlation and Fisher $z$-Transform\n\nFrom the generated time series matrix $X$, a functional connectivity graph is derived.\n\n- **Pearson Correlation:** The sample Pearson correlation matrix $C \\in \\mathbb{R}^{n \\times n}$ is computed. Each element $r_{ij}$ measures the linear correlation between the time series of ROI $i$ and ROI $j$.\n$$\nr_{ij} = \\frac{\\operatorname{Cov}(X_i, X_j)}{\\sigma_i \\sigma_j}\n$$\n- **Fisher $z$-Transform:** Correlation coefficients $r_{ij}$ are transformed to $z_{ij}$ scores using the Fisher transformation. This transformation stabilizes the variance of the correlation estimates and maps the range $(-1, 1)$ to $(-\\infty, \\infty)$.\n$$\nz_{ij} = \\operatorname{atanh}(r_{ij}) = \\frac{1}{2}\\ln\\left(\\frac{1 + r_{ij}}{1 - r_{ij}}\\right)\n$$\nNumerically, if any $r_{ij}$ is exactly $\\pm 1$, $\\operatorname{atanh}(r_{ij})$ is undefined. This is handled by clipping the correlation values to a range slightly within $(-1, 1)$, for instance, $[-1 + \\epsilon, 1 - \\epsilon]$ for a small machine precision constant $\\epsilon > 0$. The resulting matrix is denoted by $Z$. The diagonal elements $z_{ii}$ are set to $0$.\n\n### Step 3: Proportional Thresholding\n\nTo study the effect of graph sparsity, the dense connectivity matrix $|Z|$ is thresholded to retain a specific proportion $p$ of the strongest connections.\n\n- For a given threshold level $p \\in \\{0.2, 0.5, 1.0\\}$, the number of edges to retain is calculated as $K = \\left\\lfloor p \\cdot \\frac{n(n-1)}{2} \\right\\rfloor$.\n- The $\\frac{n(n-1)}{2}$ unique off-diagonal absolute Fisher scores $|z_{ij}|$ for $i < j$ are sorted in descending order. The top $K$ edges are selected.\n- This procedure yields two matrices for each $p$:\n    1. A binary adjacency matrix $A^{(p)}$, where $A^{(p)}_{ij} = 1$ if the edge $(i, j)$ is retained, and $0$ otherwise.\n    2. A weighted adjacency matrix $W^{(p)}$, where $W^{(p)}_{ij} = |z_{ij}|$ for retained edges and $0$ otherwise.\n\n### Step 4: Graph Feature Extraction\n\nFor each thresholded graph, a set of node-level features are computed. These features are commonly used as input to GNN models for tasks like node classification or graph classification.\n\n- **Degree and Coefficient of Variation (CV):** The degree of each node, $k_i^{(p)} = \\sum_j A^{(p)}_{ij}$, is calculated. The heterogeneity of the degree distribution is measured by its coefficient of variation:\n$$\n\\operatorname{CV}^{(p)} = \\frac{\\operatorname{std}\\left(\\{k_i^{(p)}\\}\\right)}{\\operatorname{mean}\\left(\\{k_i^{(p)}\\}\\right)}\n$$\n- **Node Strength:** The strength of each node, $s_i^{(p)} = \\sum_j W^{(p)}_{ij}$, is computed from the weighted matrix.\n- **Local Clustering Coefficient:** This metric quantifies how close the neighbors of a node are to being a clique. It is defined as:\n$$\nc_i^{(p)} = \n\\begin{cases}\n\\frac{2\\, \\tau_i^{(p)}}{k_i^{(p)}\\left(k_i^{(p)} - 1\\right)}, & \\text{if } k_i^{(p)} \\ge 2,\\\\\n0, & \\text{otherwise},\n\\end{cases}\n$$\nwhere $\\tau_i^{(p)} = \\frac{1}{2}((A^{(p)})^3)_{ii}$ is the number of triangles connected to node $i$.\n- **Node Feature Vector:** The computed metrics are normalized and assembled into a $3$-dimensional feature vector for each node:\n$$\n\\mathbf{f}_i^{(p)} = \\left[\\tilde{k}_i^{(p)}, \\, \\tilde{s}_i^{(p)}, \\, c_i^{(p)}\\right]\n$$\nThe normalized degree is $\\tilde{k}_i^{(p)} = k_i^{(p)} / (n-1)$. The normalized strength $\\tilde{s}_i^{(p)} = s_i^{(p)} / s_i^{\\mathrm{full}}$ is relative to the node's strength in the full, unthresholded graph, where $s_i^{\\mathrm{full}} = \\sum_{j \\neq i} |z_{ij}|$. These vectors are stacked to form the feature matrix $F^{(p)} \\in \\mathbb{R}^{n \\times 3}$.\n\n### Step 5: Feature Change Quantification\n\nFinally, to assess the stability of the node features across different sparsity levels, the change between feature matrices at successive thresholds is quantified using the Frobenius norm.\n\n- For an ordered list of thresholds $\\{p_1, p_2, \\dots\\}$, the change is calculated as:\n$$\n\\Delta^{(p_\\ell)} = \\left\\|F^{(p_\\ell)} - F^{(p_{\\ell-1})}\\right\\|_F\n$$\nBy convention, $\\Delta^{(p_1)} = 0$. For this problem, the sequence is $p_1=0.2, p_2=0.5, p_3=1.0$.\n\nThis comprehensive pipeline is applied to each of the three datasets, and the resulting lists of $\\operatorname{CV}^{(p)}$ and $\\Delta^{(p)}$ values are collected and formatted as per the problem specification.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the three specified datasets.\n    \"\"\"\n\n    def process_dataset(n, T, create_X_func, thresholds):\n        \"\"\"\n        Executes the full pipeline for a single dataset.\n        \n        Args:\n            n (int): Number of ROIs.\n            T (int): Number of time points.\n            create_X_func (function): A function that returns the time series matrix X.\n            thresholds (list): A list of proportional thresholds p.\n\n        Returns:\n            tuple: A tuple containing the list of CVs and the list of feature change norms.\n        \"\"\"\n        # Step 1: Generate Time Series Data\n        X = create_X_func(n, T)\n\n        # Step 2: Compute Correlation and Fisher Z-transform\n        # rowvar=False because columns are variables (ROIs)\n        C = np.corrcoef(X, rowvar=False)\n        \n        # Handle potential perfect correlations leading to inf in atanh\n        # A small epsilon for numerical stability\n        epsilon = 1e-15\n        C_clipped = np.clip(C, -1.0 + epsilon, 1.0 - epsilon)\n        \n        Z = np.arctanh(C_clipped)\n        np.fill_diagonal(Z, 0)\n        \n        # Step 4 (Part 1): Compute full node strength for normalization\n        s_full = np.sum(np.abs(Z), axis=1)\n        # Handle case where s_full could be zero\n        s_full_inv = np.zeros_like(s_full)\n        s_full_inv[s_full > 0] = 1.0 / s_full[s_full > 0]\n        \n        # Initialize storage for results\n        cv_list = []\n        delta_list = []\n        feature_matrices = {}\n\n        # Get upper triangle indices for edge selection\n        iu_indices = np.triu_indices(n, k=1)\n        \n        # Get flattened upper triangle of |Z|\n        z_abs_flat = np.abs(Z[iu_indices])\n        \n        # Get indices that would sort the edge weights in descending order\n        sorted_edge_indices = np.argsort(z_abs_flat)[::-1]\n        \n        num_possible_edges = n * (n - 1) // 2\n\n        for i, p in enumerate(thresholds):\n            # Step 3: Proportional Thresholding\n            K = int(np.floor(p * num_possible_edges))\n            \n            A_p = np.zeros((n, n), dtype=int)\n            W_p = np.zeros((n, n), dtype=float)\n            \n            if K > 0:\n                # Select top K edges\n                top_k_flat_indices = sorted_edge_indices[:K]\n                \n                # Get the row and column indices for the top K edges\n                top_k_rows = iu_indices[0][top_k_flat_indices]\n                top_k_cols = iu_indices[1][top_k_flat_indices]\n\n                # Populate A_p and W_p\n                A_p[top_k_rows, top_k_cols] = 1\n                A_p[top_k_cols, top_k_rows] = 1\n                W_p[top_k_rows, top_k_cols] = np.abs(Z[top_k_rows, top_k_cols])\n                W_p[top_k_cols, top_k_rows] = W_p[top_k_rows, top_k_cols]\n\n            # Step 4 (Part 2): Compute Graph Metrics and Features\n            k_p = np.sum(A_p, axis=1)\n            \n            # Coefficient of Variation\n            mean_k = np.mean(k_p)\n            cv_p = np.std(k_p) / mean_k if mean_k > 0 else 0.0\n            cv_list.append(cv_p)\n            \n            # Node strength\n            s_p = np.sum(W_p, axis=1)\n            \n            # Normalized degree and strength\n            k_tilde_p = k_p / (n - 1)\n            s_tilde_p = s_p * s_full_inv\n            \n            # Clustering Coefficient\n            c_p = np.zeros(n, dtype=float)\n            if K > 0: # Avoid matrix power of zero matrix\n                A_p_cubed = np.linalg.matrix_power(A_p, 3)\n                triangles_i = 0.5 * np.diag(A_p_cubed)\n                \n                for node_idx in range(n):\n                    if k_p[node_idx] >= 2:\n                        c_p[node_idx] = (2 * triangles_i[node_idx]) / (k_p[node_idx] * (k_p[node_idx] - 1))\n            \n            # Assemble feature matrix\n            F_p = np.stack([k_tilde_p, s_tilde_p, c_p], axis=1)\n            feature_matrices[p] = F_p\n\n            # Step 5: Quantify Feature Change\n            if i == 0:\n                delta_p = 0.0\n            else:\n                F_prev = feature_matrices[thresholds[i-1]]\n                delta_p = np.linalg.norm(F_p - F_prev, 'fro')\n            delta_list.append(delta_p)\n            \n        return cv_list, delta_list\n\n    # --- Dataset Definitions ---\n    thresholds = [0.2, 0.5, 1.0]\n    \n    # Dataset 1\n    def create_X1(n, T):\n        t = np.arange(T)\n        s1 = np.sin(2 * np.pi * 0.03 * t)\n        s2 = np.cos(2 * np.pi * 0.07 * t + np.pi / 4)\n        s3 = np.sin(2 * np.pi * 0.13 * t)\n        S = np.stack([s1, s2, s3], axis=1)\n        W = np.array([\n            [1.0, 0.5, 0.0],\n            [0.9, -0.4, 0.1],\n            [1.1, 0.6, -0.2],\n            [-1.0, -0.5, 0.3],\n            [0.2, 0.0, 1.0]\n        ])\n        N = np.zeros((T, n))\n        for i in range(n):\n            N[:, i] = 0.1 * np.sin(2 * np.pi * (0.23 + 0.01 * i) * t)\n        return S @ W.T + N\n\n    # Dataset 2\n    def create_X2(n, T):\n        t = np.arange(T)\n        s1 = np.sin(2 * np.pi * 0.12 * t)\n        s2 = np.sin(2 * np.pi * 0.12 * t + np.pi)\n        s3 = np.cos(2 * np.pi * 0.05 * t)\n        S = np.stack([s1, s2, s3], axis=1)\n        W = np.array([\n            [1.0, 0.0, 0.5],\n            [-1.0, 0.0, -0.2],\n            [0.3, 0.7, 0.0],\n            [0.0, -0.5, 0.8]\n        ])\n        N = np.zeros((T, n))\n        for i in range(n):\n            N[:, i] = 0.2 * np.cos(2 * np.pi * (0.31 + 0.02 * i) * t)\n        return S @ W.T + N\n        \n    # Dataset 3\n    def create_X3(n, T):\n        t = np.arange(T)\n        sA = np.sin(2 * np.pi * 0.04 * t)\n        sB = np.sin(2 * np.pi * 0.08 * t + np.pi / 3)\n        sC = 0.3 * np.cos(2 * np.pi * 0.02 * t)\n        S_AB = np.stack([sA, sB], axis=1)\n        W_AB = np.array([\n            [0.8, 0.1],\n            [0.9, 0.0],\n            [0.85, -0.1],\n            [0.0, 0.9],\n            [-0.1, 0.8],\n            [0.2, 0.85]\n        ])\n        w_C = np.array([0.2, -0.2, 0.1, -0.1, 0.05, -0.05])\n        X_AB = S_AB @ W_AB.T\n        X_C = np.outer(sC, w_C)\n        N = np.zeros((T, n))\n        for i in range(n):\n            N[:, i] = 0.05 * np.sin(2 * np.pi * (0.17 + 0.01 * i) * t)\n        return X_AB + X_C + N\n\n    datasets = [\n        {'n': 5, 'T': 200, 'func': create_X1},\n        {'n': 4, 'T': 30, 'func': create_X2},\n        {'n': 6, 'T': 100, 'func': create_X3}\n    ]\n    \n    all_results = []\n    for d in datasets:\n        cvs, deltas = process_dataset(d['n'], d['T'], d['func'], thresholds)\n        all_results.append([cvs, deltas])\n        \n    # Format the final output string\n    result_str = str(all_results).replace(\" \", \"\")\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond functional data, structural connectomes derived from diffusion MRI (dMRI) provide a physical wiring diagram of the brain. However, raw metrics like streamline counts can be biased by non-biological factors such as region size. This exercise  demonstrates the importance of normalization by comparing how raw versus volume-corrected connectivity weights affect the message-passing mechanism in a Graph Convolutional Network (GCN), offering critical insight into building robust and interpretable models.",
            "id": "4167796",
            "problem": "You are given Diffusion Magnetic Resonance Imaging (dMRI) tractography streamline counts between Regions of Interest (ROI) in the brain represented by a symmetric matrix of counts $S \\in \\mathbb{R}^{n \\times n}$ with nonnegative entries $s_{ij}$ and a vector of ROI volumes $\\mathrm{vol} \\in \\mathbb{R}^{n}$ with strictly positive entries $\\mathrm{vol}_i$ in $\\mathrm{mm}^3$. You must construct a volume-normalized structural adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ defined element-wise by $A_{ij} = \\dfrac{s_{ij}}{\\mathrm{vol}_i \\mathrm{vol}_j}$ and analyze how the choice of normalization affects the scaling of messages in a single layer of a Graph Convolutional Network (GCN). Consider a node feature matrix $X \\in \\mathbb{R}^{n \\times f}$ with $f$ features per ROI. Let $W$ denote an adjacency-like weight matrix used for message passing. The symmetric degree-normalized propagation operator for message passing is defined as $P = D^{-1/2} W D^{-1/2}$, where $D \\in \\mathbb{R}^{n \\times n}$ is the diagonal degree matrix with $D_{ii} = \\sum_{j=1}^n W_{ij}$. If a node has zero degree, set the corresponding inverse square root degree to $0$, that is, use $(D^{-1/2})_{ii} = 0$ whenever $D_{ii} = 0$. For a given $X$, define the message scaling ratio $\\rho(W, X)$ as $\\rho(W, X) = \\dfrac{\\lVert P X \\rVert_F}{\\lVert X \\rVert_F}$, where $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm. You must compute and compare $\\rho(S, X)$ and $\\rho(A, X)$ for each test case below to quantify how volume normalization interacts with degree normalization to affect the scaling of messages in the GCN layer.\n\nAll physical volumes must be treated in $\\mathrm{mm}^3$, streamline counts are unitless, and the final reported scaling ratios are unitless. Angles do not appear in this problem. Percentages do not appear in this problem.\n\nThe test suite comprises three cases that collectively probe typical behavior, heterogeneity in ROI sizes, and an edge case with an isolated ROI. For each case, you are provided $S$, $\\mathrm{vol}$, and $X$:\n\nCase $1$ (moderate connectivity and balanced ROI volumes):\n$$\nS^{(1)} = \\begin{pmatrix}\n0 & 120 & 80 & 60 \\\\\n120 & 0 & 90 & 70 \\\\\n80 & 90 & 0 & 50 \\\\\n60 & 70 & 50 & 0\n\\end{pmatrix}, \\quad\n\\mathrm{vol}^{(1)} = \\begin{pmatrix} 3200 \\\\ 2800 \\\\ 4000 \\\\ 3500 \\end{pmatrix}\\,\\mathrm{mm}^3, \\quad\nX^{(1)} = \\begin{pmatrix}\n2.0 & 0.5 & -1.0 \\\\\n1.5 & -0.2 & 0.3 \\\\\n-0.7 & 1.2 & 0.8 \\\\\n0.0 & -1.0 & 0.5\n\\end{pmatrix}.\n$$\n\nCase $2$ (strong hub to very small ROI and large volume disparity):\n$$\nS^{(2)} = \\begin{pmatrix}\n0 & 300 & 250 & 200 \\\\\n300 & 0 & 60 & 40 \\\\\n250 & 60 & 0 & 30 \\\\\n200 & 40 & 30 & 0\n\\end{pmatrix}, \\quad\n\\mathrm{vol}^{(2)} = \\begin{pmatrix} 800 \\\\ 5000 \\\\ 4800 \\\\ 5100 \\end{pmatrix}\\,\\mathrm{mm}^3, \\quad\nX^{(2)} = \\begin{pmatrix}\n1.0 & -0.5 & 0.2 \\\\\n-0.3 & 0.8 & -0.1 \\\\\n0.5 & 0.5 & 0.5 \\\\\n-1.2 & 0.0 & 0.7\n\\end{pmatrix}.\n$$\n\nCase $3$ (an isolated ROI with zero degree in the raw counts):\n$$\nS^{(3)} = \\begin{pmatrix}\n0 & 50 & 0 & 0 \\\\\n50 & 0 & 40 & 0 \\\\\n0 & 40 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}, \\quad\n\\mathrm{vol}^{(3)} = \\begin{pmatrix} 3000 \\\\ 3100 \\\\ 2900 \\\\ 3200 \\end{pmatrix}\\,\\mathrm{mm}^3, \\quad\nX^{(3)} = \\begin{pmatrix}\n0.2 & 0.1 & -0.4 \\\\\n1.0 & -1.0 & 0.0 \\\\\n-0.5 & 0.7 & 0.3 \\\\\n0.9 & -0.8 & 0.2\n\\end{pmatrix}.\n$$\n\nYour program must, for each case $(S^{(k)}, \\mathrm{vol}^{(k)}, X^{(k)})$ with $k \\in \\{1,2,3\\}$, compute the two scaling ratios $\\rho(S^{(k)}, X^{(k)})$ and $\\rho(A^{(k)}, X^{(k)})$ with $A^{(k)}_{ij} = \\dfrac{s^{(k)}_{ij}}{\\mathrm{vol}^{(k)}_i \\mathrm{vol}^{(k)}_j}$, using the symmetric degree-normalized propagation operator $P = D^{-1/2} W D^{-1/2}$ defined above. Handle zero degrees using $(D^{-1/2})_{ii} = 0$ exactly as specified. The final output format must be a single line containing a list with three inner lists, each inner list containing the two floating-point results in the order $[\\rho(S^{(k)}, X^{(k)}), \\rho(A^{(k)}, X^{(k)})]$. For example, the output must have the form $[[r_{1,\\mathrm{raw}}, r_{1,\\mathrm{vol}}],[r_{2,\\mathrm{raw}}, r_{2,\\mathrm{vol}}],[r_{3,\\mathrm{raw}}, r_{3,\\mathrm{vol}}]]$ with all entries expressed as decimal floats and no additional text.\n\nThe answers for all test cases are floats. The program must be entirely self-contained, must not require any user input or external files, and must run deterministically under the specified execution environment.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the fields of computational neuroscience and graph-based machine learning, mathematically well-posed, and provides all necessary information for a unique, verifiable solution.\n\nThe objective is to analyze the effect of two different graph construction methods on the message scaling properties of a single Graph Convolutional Network (GCN) layer. The two methods for constructing the graph's weight matrix $W$ are:\n1.  Using the raw streamline counts, $W = S$.\n2.  Using a volume-normalized count, $W = A$, where $A_{ij} = \\dfrac{s_{ij}}{\\mathrm{vol}_i \\mathrm{vol}_j}$.\n\nThe scaling of messages is quantified by the ratio $\\rho(W, X)$, which measures the change in the Frobenius norm of the node feature matrix $X$ after a single GCN propagation step.\n\nThe core of the GCN propagation rule is the symmetric degree-normalized operator $P$. Given a weight matrix $W \\in \\mathbb{R}^{n \\times n}$ and a node feature matrix $X \\in \\mathbb{R}^{n \\times f}$, the propagation is defined as:\n$Y = P X$\nwhere $Y \\in \\mathbb{R}^{n \\times f}$ is the matrix of updated node features.\n\nThe propagation operator $P$ is constructed as follows:\n$P = D^{-1/2} W D^{-1/2}$\n\nHere, $D$ is the diagonal degree matrix, whose elements are the sum of weights for each node:\n$D_{ii} = \\sum_{j=1}^{n} W_{ij}$\n\nThe matrix $D^{-1/2}$ is a diagonal matrix containing the reciprocal of the square root of the degrees. A crucial detail is the handling of nodes with zero degree ($D_{ii}=0$), which would otherwise lead to division by zero. As specified, for such nodes, the corresponding entry in $D^{-1/2}$ is set to zero:\n$$\n(D^{-1/2})_{ii} = \n\\begin{cases} \n(D_{ii})^{-1/2} & \\text{if } D_{ii} > 0 \\\\\n0 & \\text{if } D_{ii} = 0 \n\\end{cases}\n$$\n\nThe message scaling ratio $\\rho(W, X)$ is then defined as the ratio of the Frobenius norms of the output and input feature matrices:\n$$\n\\rho(W, X) = \\frac{\\lVert P X \\rVert_F}{\\lVert X \\rVert_F}\n$$\nThe Frobenius norm of a matrix $M \\in \\mathbb{R}^{n \\times f}$ is given by $\\lVert M \\rVert_F = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{f} M_{ij}^2}$. A ratio $\\rho > 1$ indicates an amplification of the feature signal, while $\\rho < 1$ indicates attenuation.\n\nThe computational procedure for each test case $(S^{(k)}, \\mathrm{vol}^{(k)}, X^{(k)})$ involves two parallel calculations:\n\n1.  **Calculation for Raw Counts ($W=S^{(k)}$)**:\n    a. Calculate the degree matrix $D_S$ from $S^{(k)}$.\n    b. Construct the inverse square root degree matrix $D_S^{-1/2}$ following the specified rule for zero degrees.\n    c. Compute the propagation operator $P_S = D_S^{-1/2} S^{(k)} D_S^{-1/2}$.\n    d. Compute the propagated features $Y_S = P_S X^{(k)}$.\n    e. Calculate the norms $\\lVert Y_S \\rVert_F$ and $\\lVert X^{(k)} \\rVert_F$.\n    f. Compute the ratio $\\rho(S^{(k)}, X^{(k)}) = \\lVert Y_S \\rVert_F / \\lVert X^{(k)} \\rVert_F$.\n\n2.  **Calculation for Volume-Normalized Counts ($W=A^{(k)}$)**:\n    a. First, construct the volume-normalized matrix $A^{(k)}$ with elements $A^{(k)}_{ij} = \\frac{s^{(k)}_{ij}}{\\mathrm{vol}^{(k)}_i \\mathrm{vol}^{(k)}_j}$.\n    b. Calculate the degree matrix $D_A$ from $A^{(k)}$.\n    c. Construct the inverse square root degree matrix $D_A^{-1/2}$.\n    d. Compute the propagation operator $P_A = D_A^{-1/2} A^{(k)} D_A^{-1/2}$.\n    e. Compute the propagated features $Y_A = P_A X^{(k)}$.\n    f. Calculate the norms $\\lVert Y_A \\rVert_F$ and $\\lVert X^{(k)} \\rVert_F$.\n    g. Compute the ratio $\\rho(A^{(k)}, X^{(k)}) = \\lVert Y_A \\rVert_F / \\lVert X^{(k)} \\rVert_F$.\n\nThis procedure is systematically applied to all three test cases provided in the problem statement. Case $3$ is particularly important as it tests the specified handling of an isolated node (node $4$), which has a degree of zero in both the $S$ and $A$ matrices. The implementation correctly sets the corresponding entry in $D^{-1/2}$ to $0$, ensuring that the propagation for this node results in a zero feature vector, effectively isolating it from the message passing process. The final results are presented as pairs $[\\rho(S^{(k)}, X^{(k)}), \\rho(A^{(k)}, X^{(k)})]$ for each case $k \\in \\{1, 2, 3\\}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares GCN message scaling ratios for raw and volume-normalized\n    brain connectivity matrices across three test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1\n        {\n            \"S\": np.array([\n                [0, 120, 80, 60],\n                [120, 0, 90, 70],\n                [80, 90, 0, 50],\n                [60, 70, 50, 0]\n            ], dtype=float),\n            \"vol\": np.array([3200, 2800, 4000, 3500], dtype=float),\n            \"X\": np.array([\n                [2.0, 0.5, -1.0],\n                [1.5, -0.2, 0.3],\n                [-0.7, 1.2, 0.8],\n                [0.0, -1.0, 0.5]\n            ], dtype=float),\n        },\n        # Case 2\n        {\n            \"S\": np.array([\n                [0, 300, 250, 200],\n                [300, 0, 60, 40],\n                [250, 60, 0, 30],\n                [200, 40, 30, 0]\n            ], dtype=float),\n            \"vol\": np.array([800, 5000, 4800, 5100], dtype=float),\n            \"X\": np.array([\n                [1.0, -0.5, 0.2],\n                [-0.3, 0.8, -0.1],\n                [0.5, 0.5, 0.5],\n                [-1.2, 0.0, 0.7]\n            ], dtype=float),\n        },\n        # Case 3\n        {\n            \"S\": np.array([\n                [0, 50, 0, 0],\n                [50, 0, 40, 0],\n                [0, 40, 0, 0],\n                [0, 0, 0, 0]\n            ], dtype=float),\n            \"vol\": np.array([3000, 3100, 2900, 3200], dtype=float),\n            \"X\": np.array([\n                [0.2, 0.1, -0.4],\n                [1.0, -1.0, 0.0],\n                [-0.5, 0.7, 0.3],\n                [0.9, -0.8, 0.2]\n            ], dtype=float),\n        }\n    ]\n\n    def compute_scaling_ratio(W, X):\n        \"\"\"\n        Calculates the message scaling ratio rho(W, X).\n        \"\"\"\n        norm_X = np.linalg.norm(X, 'fro')\n        \n        # If X is a zero matrix, the ratio is undefined, but for this problem, we can return 0.\n        if norm_X == 0:\n            return 0.0\n\n        # Calculate degree vector d from the weight matrix W\n        d = W.sum(axis=1)\n\n        # Calculate D^{-1/2} handling zero degrees\n        # np.power with `where` argument efficiently handles d_ii > 0\n        d_inv_sqrt_vals = np.power(d, -0.5, where=(d > 0))\n        D_inv_sqrt = np.diag(d_inv_sqrt_vals)\n\n        # Compute the symmetric degree-normalized propagation operator P\n        P = D_inv_sqrt @ W @ D_inv_sqrt\n\n        # Propagate features: Y = P * X\n        Y = P @ X\n\n        # Calculate the Frobenius norm of the propagated features\n        norm_Y = np.linalg.norm(Y, 'fro')\n\n        # Compute the scaling ratio\n        rho = norm_Y / norm_X\n        \n        return rho\n\n    results = []\n    for case in test_cases:\n        S = case[\"S\"]\n        vol = case[\"vol\"]\n        X = case[\"X\"]\n\n        # Compute rho for raw streamline counts S\n        rho_S = compute_scaling_ratio(S, X)\n\n        # Compute volume-normalized adjacency matrix A\n        vol_outer = np.outer(vol, vol)\n        # Element-wise division, handling potential division by zero\n        # Since vol_i > 0, vol_outer will have all non-zero entries.\n        A = S / vol_outer\n        \n        # Compute rho for volume-normalized matrix A\n        rho_A = compute_scaling_ratio(A, X)\n\n        results.append([rho_S, rho_A])\n    \n    # Format the final output string as required\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from a single graph to large-scale datasets with thousands of subjects introduces significant computational challenges. This final practice addresses the engineering side of applying GNNs to connectomics, focusing on scalability and memory efficiency . You will implement a GCN layer using sparse matrix operations and chunked processing, essential techniques for building pipelines that can handle the vast amount of data typical in modern neuroscience research while ensuring numerical correctness.",
            "id": "4167809",
            "problem": "You are given a collection of brain connectivity graphs, one per subject, and a single shared linear layer for a Graph Neural Network (GNN). Each subject’s network is an undirected weighted graph with symmetric adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ and node feature matrix $X \\in \\mathbb{R}^{n \\times f}$. The task is to implement a pipeline that, under a memory constraint, precomputes a symmetrically normalized adjacency with self-loops, caches node features, and uses sparse operations to compute one forward pass of a Graph Convolutional Network (GCN) layer for thousands of subjects. The pipeline must produce results identical (up to an absolute tolerance) to those obtained with dense linear algebra.\n\nFundamental base and core definitions to start from:\n- A brain connectivity graph is represented by a symmetric, nonnegative weighted adjacency matrix $A$ where $A_{ij}$ denotes the strength of connectivity between node $i$ and node $j$.\n- The degree of node $i$ is defined as $d_i = \\sum_{j=1}^n \\hat{A}_{ij}$ where $\\hat{A}$ is the adjacency matrix with self-loops added, and $D = \\mathrm{diag}(d_1, \\dots, d_n)$ is the degree matrix.\n- The symmetrically normalized operator used by a GCN layer is constructed by first adding self-loops to $A$ and then applying degree-based symmetric normalization, ensuring that contributions are scaled by local degree to avoid dominance by high-degree nodes. You must derive the precise normalized operator from these definitions; do not assume shortcut formulas.\n\nPipeline requirements:\n- Precompute the normalized adjacency with self-loops (denote the operator by $\\tilde{A}$) for each subject from the given adjacency $A$, using only the definitions above.\n- Cache node features $X$ by subject identifier to avoid recomputation.\n- Use sparse tensor operations for the multiplication $\\tilde{A} X$, and for the final multiplication $\\left(\\tilde{A} X\\right) W$, where $W \\in \\mathbb{R}^{f \\times g}$ is a shared linear layer across subjects, to scale computation across many subjects under a memory constraint.\n- Implement chunked processing: given a memory budget $M$ in bytes, choose a chunk size based on the estimated memory footprint per subject and process that many subjects at a time, guaranteeing at least one subject per chunk.\n\nCorrectness criterion:\n- For each subject, compute the output of a single linear GCN layer $H = \\tilde{A} X W$ using sparse operations. Also compute the same quantity using dense operations. The subject passes if the maximum absolute difference between the sparse result and the dense result over all entries of $H$ is less than a tolerance of $10^{-12}$ (expressed as the decimal $10^{-12}$).\n\nYour program must implement the pipeline and evaluate the following test suite. For each test case, return a boolean indicating whether all subjects in that case pass the correctness criterion.\n\nTest suite:\n- Test Case $1$ (happy path, modest connectivity):\n  - Single subject ($1$ subject), $n = 5$, $f = 3$, $g = 2$.\n  - Adjacency $A$:\n    $$\n    A = \\begin{bmatrix}\n    0 & 0.3 & 0 & 0 & 0.1 \\\\\n    0.3 & 0 & 0.2 & 0 & 0 \\\\\n    0 & 0.2 & 0 & 0.25 & 0 \\\\\n    0 & 0 & 0.25 & 0 & 0.4 \\\\\n    0.1 & 0 & 0 & 0.4 & 0\n    \\end{bmatrix}\n    $$\n  - Node features $X$:\n    $$\n    X = \\begin{bmatrix}\n    0.5 & 0.1 & -0.3 \\\\\n    1.2 & -0.7 & 0.0 \\\\\n    0.0 & 0.4 & 0.9 \\\\\n    -0.6 & 0.3 & 0.2 \\\\\n    0.8 & -1.0 & 1.5\n    \\end{bmatrix}\n    $$\n  - Shared weights $W$:\n    $$\n    W = \\begin{bmatrix}\n    0.2 & -0.1 \\\\\n    0.0 & 0.4 \\\\\n    0.5 & 0.3\n    \\end{bmatrix}\n    $$\n  - Memory budget $M = 300$ bytes.\n\n- Test Case $2$ (boundary: initially zero-degree nodes, disconnected components):\n  - Single subject ($1$ subject), $n = 4$, $f = 2$, $g = 1$.\n  - Adjacency $A$:\n    $$\n    A = \\begin{bmatrix}\n    0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0.1 & 0 \\\\\n    0 & 0.1 & 0 & 0 \\\\\n    0 & 0 & 0 & 0\n    \\end{bmatrix}\n    $$\n  - Node features $X$:\n    $$\n    X = \\begin{bmatrix}\n    1 & 0 \\\\\n    0 & 1 \\\\\n    0.5 & -0.5 \\\\\n    0 & 0\n    \\end{bmatrix}\n    $$\n  - Shared weights $W$:\n    $$\n    W = \\begin{bmatrix}\n    1.0 \\\\\n    -2.0\n    \\end{bmatrix}\n    $$\n  - Memory budget $M = 200$ bytes.\n\n- Test Case $3$ (scaling: many subjects with small graphs, chunked processing):\n  - Multiple subjects ($S = 50$ subjects), each with $n = 3$, $f = 2$, $g = 2$.\n  - For subject index $s \\in \\{1, 2, \\dots, 50\\}$, define $w_s = s / 50$ and adjacency $A^{(s)}$:\n    $$\n    A^{(s)} = \\begin{bmatrix}\n    0 & w_s & 0 \\\\\n    w_s & 0 & w_s \\\\\n    0 & w_s & 0\n    \\end{bmatrix}\n    $$\n  - Node features $X^{(s)}$:\n    $$\n    X^{(s)} = \\begin{bmatrix}\n    0.01 s & -0.01 s \\\\\n    0.02 s & 0.005 s \\\\\n    -\\frac{0.01}{3} s & 0.0025 s\n    \\end{bmatrix}\n    $$\n  - Shared weights $W$:\n    $$\n    W = \\begin{bmatrix}\n    0.3 & -0.2 \\\\\n    1.1 & 0.7\n    \\end{bmatrix}\n    $$\n  - Memory budget $M = 1024$ bytes.\n\nProgram output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the three test cases, output a list of three booleans in the order of Test Case $1$, Test Case $2$, Test Case $3$, for example $[{\\tt True},{\\tt False},{\\tt True}]$.\n\nNotes:\n- Angle units are not applicable.\n- No physical units are involved.\n- All computations must be performed using sparse operations for the pipeline path and dense operations for the reference path. The tolerance for equality checking is $10^{-12}$, expressed as a decimal.",
            "solution": "The problem requires the implementation and validation of a computational pipeline for a single Graph Convolutional Network (GCN) layer, designed for large-scale analysis of brain connectivity data. The solution must be robust, memory-efficient through sparse operations and chunking, and numerically accurate.\n\nThe foundation of a GCN is the propagation rule, which updates node representations by aggregating information from their local neighborhoods. For a single linear GCN layer, the output node features $H \\in \\mathbb{R}^{n \\times g}$ are computed from the input node features $X \\in \\mathbb{R}^{n \\times f}$ and a shared weight matrix $W \\in \\mathbb{R}^{f \\times g}$ as:\n$$\nH = \\tilde{A} X W\n$$\nHere, $\\tilde{A} \\in \\mathbb{R}^{n \\times n}$ is the symmetrically normalized adjacency matrix with self-loops, which is the crux of the graph convolution operation.\n\n### Derivation of the Symmetrically Normalized Adjacency Matrix\n\nThe operator $\\tilde{A}$ is constructed to prevent the scale of the output features from being dependent on node degrees. The process, as specified by the problem's base definitions, involves two steps:\n\n1.  **Addition of Self-Loops**: To ensure that a node's own features are included in the aggregation process, self-loops are added to the adjacency matrix $A$. This is achieved by adding the identity matrix $I_n$ to $A$, resulting in a modified adjacency matrix $\\hat{A}$.\n    $$\n    \\hat{A} = A + I_n\n    $$\n    Given that $A$ is symmetric and has non-negative entries, $\\hat{A}$ will also be symmetric with a diagonal of at least $1$.\n\n2.  **Symmetric Normalization**: Normalization is performed to stabilize the learning process by keeping the feature vectors within a reasonable numerical range. The symmetric normalization scheme is defined using the degrees of the graph represented by $\\hat{A}$. The degree $d_i$ of a node $i$ is the sum of its connection weights, calculated from $\\hat{A}$:\n    $$\n    d_i = \\sum_{j=1}^{n} \\hat{A}_{ij}\n    $$\n    These degrees form the diagonal of the degree matrix $D = \\mathrm{diag}(d_1, d_2, \\dots, d_n)$. Since self-loops of weight $1$ have been added, every node $i$ has a degree $d_i \\ge 1$, which guarantees that $D$ is invertible. The symmetrically normalized adjacency matrix $\\tilde{A}$ is then computed as:\n    $$\n    \\tilde{A} = D^{-1/2} \\hat{A} D^{-1/2}\n    $$\n    where $D^{-1/2} = \\mathrm{diag}(d_1^{-1/2}, d_2^{-1/2}, \\dots, d_n^{-1/2})$. This operation scales the message passing between any two nodes $i$ and $j$ by the geometric mean of their degrees, preventing nodes with high degrees from dominating the aggregation. The resulting matrix $\\tilde{A}$ remains symmetric.\n\n### Computational Pipeline: Sparse vs. Dense Implementation\n\nThe problem mandates two computational paths for verification purposes: a memory-efficient sparse implementation for the pipeline and a dense implementation for the reference calculation.\n\n1.  **Dense Path (Reference Calculation)**: This path uses standard dense matrix representations, typically `numpy.ndarray`. The computation proceeds exactly as described by the mathematical formulas:\n    - $\\hat{A}_{dense} = A_{dense} + np.identity(n)$\n    - $d = \\hat{A}_{dense}.sum(axis=1)$\n    - $D^{-1/2}_{dense} = np.diag(d^{-0.5})$\n    - $\\tilde{A}_{dense} = D^{-1/2}_{dense} @ \\hat{A}_{dense} @ D^{-1/2}_{dense}$\n    - $H_{dense} = (\\tilde{A}_{dense} @ X) @ W$\n\n2.  **Sparse Path (Pipeline Implementation)**: This path is designed for scalability and memory efficiency. It leverages sparse matrix formats, such as the Compressed Sparse Row (CSR) format provided by `scipy.sparse`. The CSR format is particularly efficient for matrix-vector and matrix-matrix products.\n    - $A_{sparse}$ is created from the input $A$. `scipy.sparse.csr_matrix` is used.\n    - An identity matrix $I_{sparse}$ is created using `scipy.sparse.identity`.\n    - $\\hat{A}_{sparse} = A_{sparse} + I_{sparse}$.\n    - The degree vector $d$ is efficiently computed by summing the rows of $\\hat{A}_{sparse}$.\n    - The matrix $D^{-1/2}_{sparse}$ is constructed as a sparse diagonal matrix using `scipy.sparse.diags`. For any nodes with degree $0$ (which is impossible here due to self-loops), $d_i^{-1/2}$ would be infinite; this case is handled by setting the corresponding value to $0$.\n    - $\\tilde{A}_{sparse} = D^{-1/2}_{sparse} @ \\hat{A}_{sparse} @ D^{-1/2}_{sparse}$.\n    - $H_{sparse} = (\\tilde{A}_{sparse} @ X) @ W$. The multiplication of a sparse matrix with a dense matrix ($\\tilde{A}_{sparse} @ X$) is an efficient operation that yields a dense matrix.\n\nThe correctness criterion is met if the maximum absolute difference between the elements of $H_{sparse}$ and $H_{dense}$ is less than a specified tolerance of $10^{-12}$.\n$$\n\\max_{i,j} |(H_{sparse})_{ij} - (H_{dense})_{ij}| < 10^{-12}\n$$\n\n### Memory Management with Chunked Processing\n\nTo handle thousands of subjects under a fixed memory budget $M$, the pipeline must process subjects in batches, or chunks.\n\n1.  **Memory Footprint Estimation**: A crucial step is to estimate the memory required to process a single subject. This informs the chunk size. A conservative and robust estimate can be based on the memory required to store the main input matrices, $A$ and $X$, in a dense format. For a graph with $n$ nodes and $f$ features, and assuming `float64` (8 bytes) precision, this estimate is:\n    $$\n    mem_{subject} = (n \\times n + n \\times f) \\times 8 \\text{ bytes}\n    $$\n2.  **Chunk Size Calculation**: Given the total memory budget $M$, the number of subjects that can be processed in a single chunk is:\n    $$\n    \\text{chunk\\_size} = \\lfloor M / mem_{subject} \\rfloor\n    $$\n    To comply with the requirement that at least one subject is processed, the final chunk size is $\\max(1, \\text{chunk\\_size})$.\n3.  **Processing Loop**: The pipeline iterates through the subjects, processing `chunk_size` subjects at a time. For each subject in the chunk, it performs the sparse and dense computations, compares the results, and aggregates the pass/fail status. If any subject within any chunk fails the validation, the entire test case is considered failed.\n\nThis principled design ensures that the GCN layer is implemented correctly according to its theoretical definition, while the pipeline architecture provides the scalability and memory management necessary for large-scale scientific applications.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import csr_matrix, identity, diags\n\ndef compute_gcn_sparse(A_dense, X, W):\n    \"\"\"\n    Computes one GCN layer pass using sparse matrix operations.\n    \"\"\"\n    n = A_dense.shape[0]\n    \n    # Convert A to sparse and add self-loops\n    A_sparse = csr_matrix(A_dense, dtype=np.float64)\n    I_sparse = identity(n, format='csr', dtype=np.float64)\n    A_hat_sparse = A_sparse + I_sparse\n    \n    # Compute degrees from A_hat\n    d = np.array(A_hat_sparse.sum(axis=1)).flatten()\n    \n    # Compute D^-0.5, handling potential division by zero\n    d_inv_sqrt_vals = np.power(d, -0.5)\n    d_inv_sqrt_vals[np.isinf(d_inv_sqrt_vals)] = 0.0\n    D_inv_sqrt_sparse = diags(d_inv_sqrt_vals, format='csr')\n\n    # Compute symmetrically normalized adjacency matrix\n    A_tilde_sparse = D_inv_sqrt_sparse @ A_hat_sparse @ D_inv_sqrt_sparse\n    \n    # GCN forward pass\n    H = (A_tilde_sparse @ X) @ W\n    return H\n\ndef compute_gcn_dense(A, X, W):\n    \"\"\"\n    Computes one GCN layer pass using dense matrix operations.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Add self-loops\n    A_hat = A + np.identity(n, dtype=np.float64)\n    \n    # Compute degrees from A_hat\n    d = A_hat.sum(axis=1)\n    \n    # Compute D^-0.5, handling potential division by zero\n    d_inv_sqrt_vals = np.power(d, -0.5)\n    d_inv_sqrt_vals[np.isinf(d_inv_sqrt_vals)] = 0.0\n    D_inv_sqrt = np.diag(d_inv_sqrt_vals)\n\n    # Compute symmetrically normalized adjacency matrix\n    A_tilde = D_inv_sqrt @ A_hat @ D_inv_sqrt\n    \n    # GCN forward pass\n    H = (A_tilde @ X) @ W\n    return H\n\ndef process_case(subjects, W, n, f, g, M, is_generator=False, total_subjects=None):\n    \"\"\"\n    Processes a test case, including chunking logic.\n    \"\"\"\n    # Estimate memory footprint per subject based on dense storage of A and X.\n    # float64 uses 8 bytes.\n    mem_per_subject = (n*n + n*f) * 8\n    \n    # Determine chunk size, ensuring at least one subject per chunk.\n    chunk_size = max(1, M // mem_per_subject)\n    \n    num_subjects = total_subjects if is_generator else len(subjects)\n    \n    for i in range(0, num_subjects, chunk_size):\n        start_idx = i\n        end_idx = min(i + chunk_size, num_subjects)\n        \n        for subject_idx in range(start_idx, end_idx):\n            if is_generator:\n                # Subject indices for generation are 1-based in the problem statement\n                A, X = subjects(subject_idx + 1)\n            else:\n                A, X = subjects[subject_idx]\n\n            # Ensure data is float64 for precision\n            A = np.array(A, dtype=np.float64)\n            X = np.array(X, dtype=np.float64)\n            W_proc = np.array(W, dtype=np.float64)\n\n            # Compute results using both paths\n            H_sparse = compute_gcn_sparse(A, X, W_proc)\n            H_dense = compute_gcn_dense(A, X, W_proc)\n            \n            # Verify correctness\n            max_abs_diff = np.max(np.abs(H_sparse - H_dense))\n            tolerance = 1e-12\n            \n            if max_abs_diff >= tolerance:\n                return False  # A single subject failure fails the whole case\n    \n    return True # All subjects passed\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite.\n    \"\"\"\n    # Test Case 1\n    A1 = np.array([\n        [0, 0.3, 0, 0, 0.1],\n        [0.3, 0, 0.2, 0, 0],\n        [0, 0.2, 0, 0.25, 0],\n        [0, 0, 0.25, 0, 0.4],\n        [0.1, 0, 0, 0.4, 0]\n    ])\n    X1 = np.array([\n        [0.5, 0.1, -0.3],\n        [1.2, -0.7, 0.0],\n        [0.0, 0.4, 0.9],\n        [-0.6, 0.3, 0.2],\n        [0.8, -1.0, 1.5]\n    ])\n    W1 = np.array([\n        [0.2, -0.1],\n        [0.0, 0.4],\n        [0.5, 0.3]\n    ])\n    M1 = 300\n    subjects1 = [(A1, X1)]\n    result1 = process_case(subjects1, W1, n=5, f=3, g=2, M=M1)\n\n    # Test Case 2\n    A2 = np.array([\n        [0, 0, 0, 0],\n        [0, 0, 0.1, 0],\n        [0, 0.1, 0, 0],\n        [0, 0, 0, 0]\n    ])\n    X2 = np.array([\n        [1, 0],\n        [0, 1],\n        [0.5, -0.5],\n        [0, 0]\n    ])\n    W2 = np.array([\n        [1.0],\n        [-2.0]\n    ])\n    M2 = 200\n    subjects2 = [(A2, X2)]\n    result2 = process_case(subjects2, W2, n=4, f=2, g=1, M=M2)\n\n    # Test Case 3\n    def generate_subject_data_tc3(s):\n        w_s = s / 50.0\n        A = np.array([\n            [0, w_s, 0],\n            [w_s, 0, w_s],\n            [0, w_s, 0]\n        ])\n        X = s * np.array([\n            [0.01, -0.01],\n            [0.02, 0.005],\n            [-0.01/3.0, 0.0025]\n        ])\n        return A, X\n    \n    W3 = np.array([\n        [0.3, -0.2],\n        [1.1, 0.7]\n    ])\n    M3 = 1024\n    num_subjects_tc3 = 50\n    result3 = process_case(generate_subject_data_tc3, W3, n=3, f=2, g=2, M=M3, \n                           is_generator=True, total_subjects=num_subjects_tc3)\n\n    results = [result1, result2, result3]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}