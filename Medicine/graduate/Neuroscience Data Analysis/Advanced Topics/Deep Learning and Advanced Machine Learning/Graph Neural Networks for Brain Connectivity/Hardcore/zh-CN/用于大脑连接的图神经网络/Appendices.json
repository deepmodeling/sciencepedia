{
    "hands_on_practices": [
        {
            "introduction": "将原始的功能性磁共振成像（fMRI）时间序列数据转换为图是连接组学分析的第一步。本练习将引导您模拟从计算相关性到应用阈值化以控制图稀疏性的完整流程。通过这项实践，您将亲身体验这些关键的预处理决策如何塑造最终的图结构以及图神经网络（GNN）将从中学习的节点特征。",
            "id": "4167854",
            "problem": "您将获得三个用于模拟大脑连接性的人工合成兴趣区域 (ROI) 时间序列数据集，每个数据集包含 $n$ 个区域和 $T$ 个时间点。任务是通过计算 Pearson 相关系数矩阵、应用 Fisher $z$ 变换并执行比例阈值法，从这些时间序列构建无向图，以保留指定比例的最强 Fisher 绝对值。之后，您必须量化阈值水平如何影响图的度分布以及将输入到图神经网络（GNN, Graph Neural Network）的节点特征。\n\n使用的基本原理和定义：\n- 令 $X \\in \\mathbb{R}^{T \\times n}$ 为 ROI 时间序列矩阵，其列由区域索引。ROI $i$ 和 ROI $j$ 之间的样本 Pearson 相关系数定义为\n$$\nr_{ij} = \\frac{\\sum_{t=1}^{T} \\left(X_{t,i} - \\bar{X}_i\\right)\\left(X_{t,j} - \\bar{X}_j\\right)}{\\sqrt{\\sum_{t=1}^{T} \\left(X_{t,i} - \\bar{X}_i\\right)^2} \\, \\sqrt{\\sum_{t=1}^{T} \\left(X_{t,j} - \\bar{X}_j\\right)^2}},\n$$\n其中 $\\bar{X}_i$ 是 ROI $i$ 时间序列的样本均值。\n- $r_{ij}$ 的 Fisher $z$ 变换为\n$$\nz_{ij} = \\operatorname{atanh}(r_{ij}) = \\frac{1}{2}\\ln\\left(\\frac{1 + r_{ij}}{1 - r_{ij}}\\right),\n$$\n该变换将相关系数从 $(-1,1)$ 映射到 $\\mathbb{R}$，并用于在相关性估计的标准假设下稳定方差。\n- 对于一个有 $n$ 个节点的无向无环图，在水平 $p \\in (0,1]$ 上的比例阈值法保留 $K$ 条最强的无向边，其中\n$$\nK = \\left\\lfloor p \\cdot \\frac{n(n-1)}{2} \\right\\rfloor.\n$$\n定义加权邻接矩阵 $W^{(p)} \\in \\mathbb{R}^{n \\times n}$，使得对于 $K$ 个选定的无向对 $(i,j)$（其中 $i  j$），权重为 $W^{(p)}_{ij} = W^{(p)}_{ji} = \\left|z_{ij}\\right|$，且 $W^{(p)}_{ii} = 0$。定义相应的二元邻接矩阵 $A^{(p)} \\in \\{0,1\\}^{n \\times n}$，如果 $(i,j)$ 被选中，则 $A^{(p)}_{ij} = 1$，否则为 $0$，且 $A^{(p)}_{ii} = 0$。\n- 对于由 $A^{(p)}$ 表示的图，节点 $i$ 的度为\n$$\nk_i^{(p)} = \\sum_{j=1}^{n} A^{(p)}_{ij}.\n$$\n度分布的变异系数为\n$$\n\\operatorname{CV}^{(p)} = \\frac{\\operatorname{std}\\left(\\{k_i^{(p)}\\}_{i=1}^n\\right)}{\\operatorname{mean}\\left(\\{k_i^{(p)}\\}_{i=1}^n\\right)},\n$$\n约定如果均值为 $0$，则 $\\operatorname{CV}^{(p)} = 0$。\n- 定义在 $W^{(p)}$ 下的节点强度为\n$$\ns_i^{(p)} = \\sum_{j=1}^{n} W^{(p)}_{ij}.\n$$\n令完整、未经阈值处理的节点强度参考为\n$$\ns_i^{\\mathrm{full}} = \\sum_{j=1}^{n} \\left|z_{ij}\\right| \\quad \\text{with} \\quad z_{ii} = 0.\n$$\n定义节点 $i$ 的归一化度和归一化强度为\n$$\n\\tilde{k}_i^{(p)} = \\frac{k_i^{(p)}}{n-1}, \\quad \\tilde{s}_i^{(p)} = \n\\begin{cases}\n\\frac{s_i^{(p)}}{s_i^{\\mathrm{full}}},  \\text{if } s_i^{\\mathrm{full}} > 0,\\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n定义在 $A^{(p)}$ 下节点 $i$ 的局部二元聚类系数为\n$$\nc_i^{(p)} = \n\\begin{cases}\n\\frac{2\\, \\tau_i^{(p)}}{k_i^{(p)}\\left(k_i^{(p)} - 1\\right)},  \\text{if } k_i^{(p)} \\ge 2,\\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\n其中 $\\tau_i^{(p)}$ 是在 $A^{(p)}$ 上计数的包含节点 $i$ 的三角形数量。\n- 定义用于图神经网络 (GNN, Graph Neural Network) 在阈值 $p$ 处的节点特征向量为\n$$\n\\mathbf{f}_i^{(p)} = \\left[\\tilde{k}_i^{(p)}, \\, \\tilde{s}_i^{(p)}, \\, c_i^{(p)}\\right] \\in \\mathbb{R}^3,\n$$\n特征矩阵 $F^{(p)} \\in \\mathbb{R}^{n \\times 3}$ 将这些向量按行堆叠。通过 Frobenius 范数量化连续阈值之间的特征变化\n$$\n\\Delta^{(p_\\ell)} = \n\\begin{cases}\n0,  \\text{if } \\ell = 1,\\\\\n\\left\\|F^{(p_\\ell)} - F^{(p_{\\ell-1})}\\right\\|_F,  \\text{if } \\ell \\ge 2,\n\\end{cases}\n$$\n对于一个有序阈值列表 $\\{p_\\ell\\}_{\\ell=1}^m$。\n\n人工数据集和阈值：\n对于下面的每个数据集，根据指定的确定性信号和混合方式构建 $X$，然后遵循上述流程。使用阈值列表 $\\{p_\\ell\\} = \\{0.2, 0.5, 1.0\\}$。\n\n- 数据集 1 ($n = 5$, $T = 200$):\n  - 对于 $t \\in \\{0,1,\\dots,199\\}$ 的基本信号：\n    $s_1[t] = \\sin(2\\pi \\cdot 0.03 \\cdot t)$,\n    $s_2[t] = \\cos(2\\pi \\cdot 0.07 \\cdot t + \\pi/4)$,\n    $s_3[t] = \\sin(2\\pi \\cdot 0.13 \\cdot t)$.\n  - 混合矩阵 $W \\in \\mathbb{R}^{5 \\times 3}$:\n    $W = \\begin{bmatrix}\n    1.0  0.5  0.0\\\\\n    0.9  -0.4  0.1\\\\\n    1.1  0.6  -0.2\\\\\n    -1.0  -0.5  0.3\\\\\n    0.2  0.0  1.0\n    \\end{bmatrix}$.\n  - 确定性噪声：对于 ROI $i \\in \\{0,1,2,3,4\\}$，\n    $\\eta_i[t] = 0.1 \\cdot \\sin\\left(2\\pi \\cdot (0.23 + 0.01 i) \\cdot t\\right)$.\n  - 构建 $S \\in \\mathbb{R}^{200 \\times 3}$，其列为 $s_1, s_2, s_3$，并且 $X = S W^\\top + N$，其中 $N_{t,i} = \\eta_i[t]$。\n\n- 数据集 2 ($n = 4$, $T = 30$):\n  - 对于 $t \\in \\{0,1,\\dots,29\\}$ 的基本信号：\n    $s_1[t] = \\sin(2\\pi \\cdot 0.12 \\cdot t)$,\n    $s_2[t] = \\sin(2\\pi \\cdot 0.12 \\cdot t + \\pi)$,\n    $s_3[t] = \\cos(2\\pi \\cdot 0.05 \\cdot t)$.\n  - 混合矩阵 $W \\in \\mathbb{R}^{4 \\times 3}$:\n    $W = \\begin{bmatrix}\n    1.0  0.0  0.5\\\\\n    -1.0  0.0  -0.2\\\\\n    0.3  0.7  0.0\\\\\n    0.0  -0.5  0.8\n    \\end{bmatrix}$.\n  - 确定性噪声：对于 ROI $i \\in \\{0,1,2,3\\}$，\n    $\\eta_i[t] = 0.2 \\cdot \\cos\\left(2\\pi \\cdot (0.31 + 0.02 i) \\cdot t\\right)$.\n  - 构建 $S \\in \\mathbb{R}^{30 \\times 3}$，其列为 $s_1, s_2, s_3$，并且 $X = S W^\\top + N$。\n\n- 数据集 3 ($n = 6$, $T = 100$):\n  - 对于 $t \\in \\{0,1,\\dots,99\\}$ 的基本信号：\n    $s_A[t] = \\sin(2\\pi \\cdot 0.04 \\cdot t)$,\n    $s_B[t] = \\sin(2\\pi \\cdot 0.08 \\cdot t + \\pi/3)$,\n    $s_C[t] = 0.3 \\cdot \\cos(2\\pi \\cdot 0.02 \\cdot t)$.\n  - 混合矩阵：\n    $W_{AB} \\in \\mathbb{R}^{6 \\times 2}$:\n    $W_{AB} = \\begin{bmatrix}\n    0.8  0.1\\\\\n    0.9  0.0\\\\\n    0.85  -0.1\\\\\n    0.0  0.9\\\\\n    -0.1  0.8\\\\\n    0.2  0.85\n    \\end{bmatrix}$,\n    以及 $w_C \\in \\mathbb{R}^{6}$:\n    $w_C = \\begin{bmatrix}\n    0.2\\\\ -0.2\\\\ 0.1\\\\ -0.1\\\\ 0.05\\\\ -0.05\n    \\end{bmatrix}$.\n  - 确定性噪声：对于 ROI $i \\in \\{0,1,2,3,4,5\\}$，\n    $\\eta_i[t] = 0.05 \\cdot \\sin\\left(2\\pi \\cdot (0.17 + 0.01 i) \\cdot t\\right)$.\n  - 构建 $S_{AB} \\in \\mathbb{R}^{100 \\times 2}$，其列为 $s_A, s_B$，以及 $X_{AB} = S_{AB} W_{AB}^\\top$。从 $s_C$ 构建 $S_C \\in \\mathbb{R}^{100}$，以及 $X_C = S_C \\cdot w_C^\\top$。然后 $X = X_{AB} + X_C + N$。\n\n每个数据集的处理和量化任务：\n1. 从 $X$ 计算 Pearson 相关系数矩阵 $C \\in \\mathbb{R}^{n \\times n}$。\n2. 从 $C$ 逐元素计算 Fisher $z$ 变换矩阵 $Z \\in \\mathbb{R}^{n \\times n}$。为保证数值稳定性，在应用 $\\operatorname{atanh}$ 之前，通过将 $r_{ij}$ 裁剪到 $(-1+\\epsilon, 1-\\epsilon)$（其中 $\\epsilon$ 是一个很小的数）来确保 $r_{ij} \\in (-1,1)$。\n3. 对于每个阈值 $p \\in \\{0.2, 0.5, 1.0\\}$，通过为 $ij$ 的 $\\frac{n(n-1)}{2}$ 个唯一的 $|z_{ij}|$ 绝对值选择前 $K = \\lfloor p \\cdot \\frac{n(n-1)}{2} \\rfloor$ 个值来应用比例阈值法。\n4. 对于每个阈值 $p$，计算并存储度分布的变异系数 $\\operatorname{CV}^{(p)}$。\n5. 对于每个阈值 $p$，计算并存储节点特征矩阵 $F^{(p)}$。\n6. 对于每个阈值 $p$（从第二个开始），计算并存储特征矩阵变化 $\\Delta^{(p)}$。\n7. 对每个数据集重复以上步骤，并将所有计算出的 $\\operatorname{CV}^{(p)}$ 和 $\\Delta^{(p)}$ 值收集到一个列表中。\n8. 格式化最终输出，作为一个包含三个数据集结果的列表，每个数据集的结果是两个列表：一个用于 $\\operatorname{CV}^{(p)}$ 值，一个用于 $\\Delta^{(p)}$ 值。例如：`[[[cv1_p1, cv1_p2, ...], [delta1_p2, delta1_p3, ...]], ...]`。",
            "solution": "用户提供的问题是网络神经科学和图信号处理领域一个定义明确的计算任务。它要求从人工时间序列数据构建和分析图，这是如图神经网络 (GNN) 等方法的常见基准测试程序。该问题具有科学依据、逻辑一致，并为获得唯一、确定性的解提供了所有必要信息。所有定义和计算步骤都已形式化指定。因此，该问题是有效的，下面将给出完整解答。\n\n该解决方案对所提供的三个数据集均遵循一个多步骤流程。核心步骤包括生成时间序列、构建大脑连接图、在不同稀疏度水平上提取节点特征，最后量化这些特征的变化。\n\n### 第 1 步：时间序列生成\n\n对于每个数据集，都会生成一个多变量时间序列矩阵 $X \\in \\mathbb{R}^{T \\times n}$，其中 $n$ 是兴趣区域 (ROI) 的数量，$T$ 是时间点的数量。其构建基于线性混合模型，这是一种创建具有已知真实结构的人工数据的标准方法。\n\n- 定义一组基本信号（正弦波），构成矩阵 $S$ 的列。\n- 这些信号使用特定于数据集的混合矩阵 $W$ 进行线性组合。\n- 然后加入一个确定性噪声信号 $N$。\n\n最终的时间序列矩阵由 $X = \\text{信号} + \\text{噪声}$ 形成。每个数据集的具体情况如下：\n\n- **数据集 1 ($n=5, T=200$):** $X = S W^\\top + N$，其中 $S \\in \\mathbb{R}^{200 \\times 3}$ 包含三个基本正弦信号。\n- **数据集 2 ($n=4, T=30$):** $X = S W^\\top + N$，其中 $S \\in \\mathbb{R}^{30 \\times 3}$ 包含三个基本信号。一个基本信号与另一个完全反相关 ($s_2[t] = -s_1[t]$)。\n- **数据集 3 ($n=6, T=100$):** $X = S_{AB} W_{AB}^\\top + S_C w_C^\\top + N$。这涉及在求和之前通过两个独立的混合操作对三个基本信号（$s_A, s_B, s_C$）进行更复杂的混合。\n\n### 第 2 步：相关性和 Fisher $z$ 变换\n\n从生成的时间序列矩阵 $X$ 中，推导出功能连接图。\n\n- **Pearson 相关性：** 计算样本 Pearson 相关系数矩阵 $C \\in \\mathbb{R}^{n \\times n}$。每个元素 $r_{ij}$ 衡量 ROI $i$ 和 ROI $j$ 时间序列之间的线性相关性。\n$$\nr_{ij} = \\frac{\\operatorname{Cov}(X_i, X_j)}{\\sigma_i \\sigma_j}\n$$\n- **Fisher $z$ 变换：** 使用 Fisher 变换将相关系数 $r_{ij}$ 转换为 $z_{ij}$ 分数。该变换能稳定相关性估计值的方差，并将范围 $(-1, 1)$ 映射到 $(-\\infty, \\infty)$。\n$$\nz_{ij} = \\operatorname{atanh}(r_{ij}) = \\frac{1}{2}\\ln\\left(\\frac{1 + r_{ij}}{1 - r_{ij}}\\right)\n$$\n在数值上，如果任何 $r_{ij}$ 恰好为 $\\pm 1$，则 $\\operatorname{atanh}(r_{ij})$ 是未定义的。这通过将相关值裁剪到 $(-1, 1)$ 内部的一个稍小范围来处理，例如，对于一个小的机器精度常数 $\\epsilon > 0$，裁剪到 $[-1 + \\epsilon, 1 - \\epsilon]$。结果矩阵记为 $Z$。对角线元素 $z_{ii}$ 设置为 $0$。\n\n### 第 3 步：比例阈值法\n\n为了研究图稀疏性的影响，对稠密连接矩阵 $|Z|$ 进行阈值处理，以保留特定比例 $p$ 的最强连接。\n\n- 对于给定的阈值水平 $p \\in \\{0.2, 0.5, 1.0\\}$，要保留的边数计算为 $K = \\left\\lfloor p \\cdot \\frac{n(n-1)}{2} \\right\\rfloor$。\n- 对于 $i  j$ 的 $\\frac{n(n-1)}{2}$ 个唯一的非对角线 Fisher 绝对值分数 $|z_{ij}|$ 进行降序排序。选择前 $K$ 条边。\n- 这个过程为每个 $p$ 产生两个矩阵：\n    1. 一个二元邻接矩阵 $A^{(p)}$，其中如果边 $(i, j)$ 被保留，则 $A^{(p)}_{ij} = 1$，否则为 $0$。\n    2. 一个加权邻接矩阵 $W^{(p)}$，其中对于保留的边，$W^{(p)}_{ij} = |z_{ij}|$，否则为 $0$。\n\n### 第 4 步：图特征提取\n\n对于每个经过阈值处理的图，会计算一组节点级特征。这些特征通常用作 GNN 模型的输入，用于节点分类或图分类等任务。\n\n- **度与变异系数 (CV):** 计算每个节点的度 $k_i^{(p)} = \\sum_j A^{(p)}_{ij}$。度分布的异质性通过其变异系数来衡量：\n$$\n\\operatorname{CV}^{(p)} = \\frac{\\operatorname{std}\\left(\\{k_i^{(p)}\\}\\right)}{\\operatorname{mean}\\left(\\{k_i^{(p)}\\}\\right)}\n$$\n- **节点强度：** 从加权矩阵计算每个节点的强度 $s_i^{(p)} = \\sum_j W^{(p)}_{ij}$。\n- **局部聚类系数：** 该指标量化一个节点的邻居们形成一个团（clique）的紧密程度。其定义为：\n$$\nc_i^{(p)} = \n\\begin{cases}\n\\frac{2\\, \\tau_i^{(p)}}{k_i^{(p)}\\left(k_i^{(p)} - 1\\right)},  \\text{if } k_i^{(p)} \\ge 2,\\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\n其中 $\\tau_i^{(p)} = \\frac{1}{2}((A^{(p)})^3)_{ii}$ 是连接到节点 $i$ 的三角形数量。\n- **节点特征向量：** 计算出的指标被归一化并为每个节点组装成一个三维特征向量：\n$$\n\\mathbf{f}_i^{(p)} = \\left[\\tilde{k}_i^{(p)}, \\, \\tilde{s}_i^{(p)}, \\, c_i^{(p)}\\right]\n$$\n归一化度为 $\\tilde{k}_i^{(p)} = k_i^{(p)} / (n-1)$。归一化强度 $\\tilde{s}_i^{(p)} = s_i^{(p)} / s_i^{\\mathrm{full}}$ 是相对于该节点在完整、未经阈值处理的图中的强度，其中 $s_i^{\\mathrm{full}} = \\sum_{j \\neq i} |z_{ij}|$。这些向量被堆叠起来形成特征矩阵 $F^{(p)} \\in \\mathbb{R}^{n \\times 3}$。\n\n### 第 5 步：特征变化量化\n\n最后，为评估节点特征在不同稀疏度水平上的稳定性，使用 Frobenius 范数量化连续阈值下特征矩阵之间的变化。\n\n- 对于一个有序阈值列表 $\\{p_1, p_2, \\dots\\}$，变化计算如下：\n$$\n\\Delta^{(p_\\ell)} = \\left\\|F^{(p_\\ell)} - F^{(p_{\\ell-1})}\\right\\|_F\n$$\n按照惯例，$\\Delta^{(p_1)} = 0$。对于本问题，序列为 $p_1=0.2, p_2=0.5, p_3=1.0$。\n\n将此综合流程应用于三个数据集中的每一个，并将所得的 $\\operatorname{CV}^{(p)}$ 和 $\\Delta^{(p)}$ 值列表收集起来，并按照问题规范进行格式化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the three specified datasets.\n    \"\"\"\n\n    def process_dataset(n, T, create_X_func, thresholds):\n        \"\"\"\n        Executes the full pipeline for a single dataset.\n        \n        Args:\n            n (int): Number of ROIs.\n            T (int): Number of time points.\n            create_X_func (function): A function that returns the time series matrix X.\n            thresholds (list): A list of proportional thresholds p.\n\n        Returns:\n            tuple: A tuple containing the list of CVs and the list of feature change norms.\n        \"\"\"\n        # Step 1: Generate Time Series Data\n        X = create_X_func(n, T)\n\n        # Step 2: Compute Correlation and Fisher Z-transform\n        # rowvar=False because columns are variables (ROIs)\n        C = np.corrcoef(X, rowvar=False)\n        \n        # Handle potential perfect correlations leading to inf in atanh\n        # A small epsilon for numerical stability\n        epsilon = 1e-15\n        C_clipped = np.clip(C, -1.0 + epsilon, 1.0 - epsilon)\n        \n        Z = np.arctanh(C_clipped)\n        np.fill_diagonal(Z, 0)\n        \n        # Step 4 (Part 1): Compute full node strength for normalization\n        s_full = np.sum(np.abs(Z), axis=1)\n        # Handle case where s_full could be zero\n        s_full_inv = np.zeros_like(s_full)\n        s_full_inv[s_full > 0] = 1.0 / s_full[s_full > 0]\n        \n        # Initialize storage for results\n        cv_list = []\n        delta_list = []\n        feature_matrices = {}\n\n        # Get upper triangle indices for edge selection\n        iu_indices = np.triu_indices(n, k=1)\n        \n        # Get flattened upper triangle of |Z|\n        z_abs_flat = np.abs(Z[iu_indices])\n        \n        # Get indices that would sort the edge weights in descending order\n        sorted_edge_indices = np.argsort(z_abs_flat)[::-1]\n        \n        num_possible_edges = n * (n - 1) // 2\n\n        for i, p in enumerate(thresholds):\n            # Step 3: Proportional Thresholding\n            K = int(np.floor(p * num_possible_edges))\n            \n            A_p = np.zeros((n, n), dtype=int)\n            W_p = np.zeros((n, n), dtype=float)\n            \n            if K > 0:\n                # Select top K edges\n                top_k_flat_indices = sorted_edge_indices[:K]\n                \n                # Get the row and column indices for the top K edges\n                top_k_rows = iu_indices[0][top_k_flat_indices]\n                top_k_cols = iu_indices[1][top_k_flat_indices]\n\n                # Populate A_p and W_p\n                A_p[top_k_rows, top_k_cols] = 1\n                A_p[top_k_cols, top_k_rows] = 1\n                W_p[top_k_rows, top_k_cols] = np.abs(Z[top_k_rows, top_k_cols])\n                W_p[top_k_cols, top_k_rows] = W_p[top_k_rows, top_k_cols]\n\n            # Step 4 (Part 2): Compute Graph Metrics and Features\n            k_p = np.sum(A_p, axis=1)\n            \n            # Coefficient of Variation\n            mean_k = np.mean(k_p)\n            cv_p = np.std(k_p) / mean_k if mean_k > 0 else 0.0\n            cv_list.append(cv_p)\n            \n            # Node strength\n            s_p = np.sum(W_p, axis=1)\n            \n            # Normalized degree and strength\n            k_tilde_p = k_p / (n - 1)\n            s_tilde_p = s_p * s_full_inv\n            \n            # Clustering Coefficient\n            c_p = np.zeros(n, dtype=float)\n            if K > 0: # Avoid matrix power of zero matrix\n                A_p_cubed = np.linalg.matrix_power(A_p, 3)\n                triangles_i = 0.5 * np.diag(A_p_cubed)\n                \n                for node_idx in range(n):\n                    if k_p[node_idx] >= 2:\n                        c_p[node_idx] = (2 * triangles_i[node_idx]) / (k_p[node_idx] * (k_p[node_idx] - 1))\n            \n            # Assemble feature matrix\n            F_p = np.stack([k_tilde_p, s_tilde_p, c_p], axis=1)\n            feature_matrices[p] = F_p\n\n            # Step 5: Quantify Feature Change\n            if i == 0:\n                delta_p = 0.0\n            else:\n                F_prev = feature_matrices[thresholds[i-1]]\n                delta_p = np.linalg.norm(F_p - F_prev, 'fro')\n            delta_list.append(delta_p)\n            \n        return cv_list, delta_list\n\n    # --- Dataset Definitions ---\n    thresholds = [0.2, 0.5, 1.0]\n    \n    # Dataset 1\n    def create_X1(n, T):\n        t = np.arange(T)\n        s1 = np.sin(2 * np.pi * 0.03 * t)\n        s2 = np.cos(2 * np.pi * 0.07 * t + np.pi / 4)\n        s3 = np.sin(2 * np.pi * 0.13 * t)\n        S = np.stack([s1, s2, s3], axis=1)\n        W = np.array([\n            [1.0, 0.5, 0.0],\n            [0.9, -0.4, 0.1],\n            [1.1, 0.6, -0.2],\n            [-1.0, -0.5, 0.3],\n            [0.2, 0.0, 1.0]\n        ])\n        N = np.zeros((T, n))\n        for i in range(n):\n            N[:, i] = 0.1 * np.sin(2 * np.pi * (0.23 + 0.01 * i) * t)\n        return S @ W.T + N\n\n    # Dataset 2\n    def create_X2(n, T):\n        t = np.arange(T)\n        s1 = np.sin(2 * np.pi * 0.12 * t)\n        s2 = np.sin(2 * np.pi * 0.12 * t + np.pi)\n        s3 = np.cos(2 * np.pi * 0.05 * t)\n        S = np.stack([s1, s2, s3], axis=1)\n        W = np.array([\n            [1.0, 0.0, 0.5],\n            [-1.0, 0.0, -0.2],\n            [0.3, 0.7, 0.0],\n            [0.0, -0.5, 0.8]\n        ])\n        N = np.zeros((T, n))\n        for i in range(n):\n            N[:, i] = 0.2 * np.cos(2 * np.pi * (0.31 + 0.02 * i) * t)\n        return S @ W.T + N\n        \n    # Dataset 3\n    def create_X3(n, T):\n        t = np.arange(T)\n        sA = np.sin(2 * np.pi * 0.04 * t)\n        sB = np.sin(2 * np.pi * 0.08 * t + np.pi / 3)\n        sC = 0.3 * np.cos(2 * np.pi * 0.02 * t)\n        S_AB = np.stack([sA, sB], axis=1)\n        W_AB = np.array([\n            [0.8, 0.1],\n            [0.9, 0.0],\n            [0.85, -0.1],\n            [0.0, 0.9],\n            [-0.1, 0.8],\n            [0.2, 0.85]\n        ])\n        w_C = np.array([0.2, -0.2, 0.1, -0.1, 0.05, -0.05])\n        X_AB = S_AB @ W_AB.T\n        X_C = np.outer(sC, w_C)\n        N = np.zeros((T, n))\n        for i in range(n):\n            N[:, i] = 0.05 * np.sin(2 * np.pi * (0.17 + 0.01 * i) * t)\n        return X_AB + X_C + N\n\n    datasets = [\n        {'n': 5, 'T': 200, 'func': create_X1},\n        {'n': 4, 'T': 30, 'func': create_X2},\n        {'n': 6, 'T': 100, 'func': create_X3}\n    ]\n    \n    all_results = []\n    for d in datasets:\n        cvs, deltas = process_dataset(d['n'], d['T'], d['func'], thresholds)\n        all_results.append([cvs, deltas])\n        \n    # Format the final output string\n    result_str = str(all_results).replace(\" \", \"\")\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "除了简单地计算连接数量，源于扩散磁共振成像（dMRI）的结构连接组在用于图神经网络（GNN）分析前必须进行归一化，以确保其生物学意义和数值稳定性。本练习将引导您探索如何校正感兴趣区域（ROI）体积等因素，并量化这种归一化如何与图卷积网络（GCN）的核心消息传递机制相互作用。这项实践将为您提供关于如何防止解剖学属性引入模型偏见的关键洞见。",
            "id": "4167796",
            "problem": "给定大脑中感兴趣区域 (ROI) 之间的扩散磁共振成像 (dMRI) 纤维束示踪流线计数，表示为一个具有非负项 $s_{ij}$ 的对称计数矩阵 $S \\in \\mathbb{R}^{n \\times n}$，以及一个 ROI 体积向量 $\\mathrm{vol} \\in \\mathbb{R}^{n}$，其项 $\\mathrm{vol}_i$ 均为严格正值，单位为 $\\mathrm{mm}^3$。你必须构建一个体积归一化的结构邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其元素定义为 $A_{ij} = \\dfrac{s_{ij}}{\\mathrm{vol}_i \\mathrm{vol}_j}$，并分析归一化的选择如何影响图卷积网络 (GCN) 单个层中消息的缩放。考虑一个节点特征矩阵 $X \\in \\mathbb{R}^{n \\times f}$，其中每个 ROI 有 $f$ 个特征。设 $W$ 表示用于消息传递的类邻接权重矩阵。用于消息传递的对称度归一化传播算子定义为 $P = D^{-1/2} W D^{-1/2}$，其中 $D \\in \\mathbb{R}^{n \\times n}$ 是对角度矩阵，其对角元素为 $D_{ii} = \\sum_{j=1}^n W_{ij}$。如果一个节点的度为零，则将相应的度的负二分之一次方设置为 $0$，即当 $D_{ii} = 0$ 时，使用 $(D^{-1/2})_{ii} = 0$。对于给定的 $X$，将消息缩放比 $\\rho(W, X)$ 定义为 $\\rho(W, X) = \\dfrac{\\lVert P X \\rVert_F}{\\lVert X \\rVert_F}$，其中 $\\lVert \\cdot \\rVert_F$ 表示弗罗贝尼乌斯范数。你必须为下面的每个测试用例计算并比较 $\\rho(S, X)$ 和 $\\rho(A, X)$，以量化体积归一化如何与度归一化相互作用，从而影响 GCN 层中消息的缩放。\n\n所有物理体积都必须以 $\\mathrm{mm}^3$ 为单位处理，流线计数无单位，最终报告的缩放比也无单位。本问题不涉及角度。本问题不涉及百分比。\n\n测试套件包含三个案例，这些案例共同探讨了典型行为、ROI 大小的异质性以及包含孤立 ROI 的边缘情况。对于每个案例，都提供了 $S$、$\\mathrm{vol}$ 和 $X$：\n\n案例 1（中等连接度和均衡的 ROI 体积）：\n$$\nS^{(1)} = \\begin{pmatrix}\n0  120  80  60 \\\\\n120  0  90  70 \\\\\n80  90  0  50 \\\\\n60  70  50  0\n\\end{pmatrix}, \\quad\n\\mathrm{vol}^{(1)} = \\begin{pmatrix} 3200 \\\\ 2800 \\\\ 4000 \\\\ 3500 \\end{pmatrix}\\,\\mathrm{mm}^3, \\quad\nX^{(1)} = \\begin{pmatrix}\n2.0  0.5  -1.0 \\\\\n1.5  -0.2  0.3 \\\\\n-0.7  1.2  0.8 \\\\\n0.0  -1.0  0.5\n\\end{pmatrix}.\n$$\n\n案例 2（强中心节点连接到非常小的 ROI 以及巨大的体积差异）：\n$$\nS^{(2)} = \\begin{pmatrix}\n0  300  250  200 \\\\\n300  0  60  40 \\\\\n250  60  0  30 \\\\\n200  40  30  0\n\\end{pmatrix}, \\quad\n\\mathrm{vol}^{(2)} = \\begin{pmatrix} 800 \\\\ 5000 \\\\ 4800 \\\\ 5100 \\end{pmatrix}\\,\\mathrm{mm}^3, \\quad\nX^{(2)} = \\begin{pmatrix}\n1.0  -0.5  0.2 \\\\\n-0.3  0.8  -0.1 \\\\\n0.5  0.5  0.5 \\\\\n-1.2  0.0  0.7\n\\end{pmatrix}.\n$$\n\n案例 3（原始计数中存在度为零的孤立 ROI）：\n$$\nS^{(3)} = \\begin{pmatrix}\n0  50  0  0 \\\\\n50  0  40  0 \\\\\n0  40  0  0 \\\\\n0  0  0  0\n\\end{pmatrix}, \\quad\n\\mathrm{vol}^{(3)} = \\begin{pmatrix} 3000 \\\\ 3100 \\\\ 2900 \\\\ 3200 \\end{pmatrix}\\,\\mathrm{mm}^3, \\quad\nX^{(3)} = \\begin{pmatrix}\n0.2  0.1  -0.4 \\\\\n1.0  -1.0  0.0 \\\\\n-0.5  0.7  0.3 \\\\\n0.9  -0.8  0.2\n\\end{pmatrix}.\n$$\n\n你的程序必须为每个案例 $(S^{(k)}, \\mathrm{vol}^{(k)}, X^{(k)})$（其中 $k \\in \\{1,2,3\\}$），使用上面定义的对称度归一化传播算子 $P = D^{-1/2} W D^{-1/2}$，计算两个缩放比 $\\rho(S^{(k)}, X^{(k)})$ 和 $\\rho(A^{(k)}, X^{(k)})$，其中 $A^{(k)}_{ij} = \\dfrac{s^{(k)}_{ij}}{\\mathrm{vol}^{(k)}_i \\mathrm{vol}^{(k)}_j}$。必须严格按照规定，使用 $(D^{-1/2})_{ii} = 0$ 来处理零度。最终输出格式必须是包含一个列表的单行，该列表包含三个内部列表，每个内部列表按顺序 $[\\rho(S^{(k)}, X^{(k)}), \\rho(A^{(k)}, X^{(k)})]$ 包含两个浮点数结果。例如，输出形式必须为 $[[r_{1,\\mathrm{raw}}, r_{1,\\mathrm{vol}}],[r_{2,\\mathrm{raw}}, r_{2,\\mathrm{vol}}],[r_{3,\\mathrm{raw}}, r_{3,\\mathrm{vol}}]]$，所有条目都表示为十进制浮点数，且无额外文本。\n\n所有测试用例的答案都是浮点数。程序必须完全自包含，不得需要任何用户输入或外部文件，并且必须在指定的执行环境下确定性地运行。",
            "solution": "问题陈述已经过验证，被认为是可靠的。它在计算神经科学和基于图的机器学习领域有科学依据，在数学上是适定的，并为获得唯一、可验证的解提供了所有必要信息。\n\n目标是分析两种不同的图构建方法对单个图卷积网络 (GCN) 层的消息缩放属性的影响。构建图的权重矩阵 $W$ 的两种方法是：\n1.  使用原始流线计数，$W = S$。\n2.  使用体积归一化计数，$W = A$，其中 $A_{ij} = \\dfrac{s_{ij}}{\\mathrm{vol}_i \\mathrm{vol}_j}$。\n\n消息的缩放由比率 $\\rho(W, X)$ 量化，该比率衡量节点特征矩阵 $X$ 在单个 GCN 传播步骤后其弗罗贝尼乌斯范数的变化。\n\nGCN 传播规则的核心是对称度归一化算子 $P$。给定权重矩阵 $W \\in \\mathbb{R}^{n \\times n}$ 和节点特征矩阵 $X \\in \\mathbb{R}^{n \\times f}$，传播定义为：\n$Y = P X$\n其中 $Y \\in \\mathbb{R}^{n \\times f}$ 是更新后的节点特征矩阵。\n\n传播算子 $P$ 的构建方式如下：\n$P = D^{-1/2} W D^{-1/2}$\n\n此处，$D$ 是对角度矩阵，其元素是每个节点的权重之和：\n$D_{ii} = \\sum_{j=1}^{n} W_{ij}$\n\n矩阵 $D^{-1/2}$ 是一个对角矩阵，其对角线元素为度的平方根的倒数。一个关键的细节是处理度为零的节点 ($D_{ii}=0$)，否则会导致除以零的错误。按照规定，对于此类节点， $D^{-1/2}$ 中的相应条目被设置为零：\n$$\n(D^{-1/2})_{ii} = \n\\begin{cases} \n(D_{ii})^{-1/2}  \\text{if } D_{ii}  0 \\\\\n0  \\text{if } D_{ii} = 0 \n\\end{cases}\n$$\n\n消息缩放比 $\\rho(W, X)$ 接着被定义为输出和输入特征矩阵的弗罗贝尼乌斯范数之比：\n$$\n\\rho(W, X) = \\frac{\\lVert P X \\rVert_F}{\\lVert X \\rVert_F}\n$$\n矩阵 $M \\in \\mathbb{R}^{n \\times f}$ 的弗罗贝尼乌斯范数由 $\\lVert M \\rVert_F = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{f} M_{ij}^2}$ 给出。比率 $\\rho  1$ 表示特征信号的放大，而 $\\rho  1$ 表示衰减。\n\n每个测试用例 $(S^{(k)}, \\mathrm{vol}^{(k)}, X^{(k)})$ 的计算过程涉及两个并行的计算：\n\n1.  **原始计数的计算 ($W=S^{(k)}$)**：\n    a. 从 $S^{(k)}$ 计算度矩阵 $D_S$。\n    b. 遵循零度的指定规则，构建度的负二分之一次方矩阵 $D_S^{-1/2}$。\n    c. 计算传播算子 $P_S = D_S^{-1/2} S^{(k)} D_S^{-1/2}$。\n    d. 计算传播后的特征 $Y_S = P_S X^{(k)}$。\n    e. 计算范数 $\\lVert Y_S \\rVert_F$ 和 $\\lVert X^{(k)} \\rVert_F$。\n    f. 计算比率 $\\rho(S^{(k)}, X^{(k)}) = \\lVert Y_S \\rVert_F / \\lVert X^{(k)} \\rVert_F$。\n\n2.  **体积归一化计数的计算 ($W=A^{(k)}$)**：\n    a. 首先，构建体积归一化矩阵 $A^{(k)}$，其元素为 $A^{(k)}_{ij} = \\frac{s^{(k)}_{ij}}{\\mathrm{vol}^{(k)}_i \\mathrm{vol}^{(k)}_j}$。\n    b. 从 $A^{(k)}$ 计算度矩阵 $D_A$。\n    c. 构建度的负二分之一次方矩阵 $D_A^{-1/2}$。\n    d. 计算传播算子 $P_A = D_A^{-1/2} A^{(k)} D_A^{-1/2}$。\n    e. 计算传播后的特征 $Y_A = P_A X^{(k)}$。\n    f. 计算范数 $\\lVert Y_A \\rVert_F$ 和 $\\lVert X^{(k)} \\rVert_F$。\n    g. 计算比率 $\\rho(A^{(k)}, X^{(k)}) = \\lVert Y_A \\rVert_F / \\lVert X^{(k)} \\rVert_F$。\n\n该过程被系统地应用于问题陈述中提供的所有三个测试用例。案例 3 特别重要，因为它测试了对孤立节点（节点 4）的指定处理方式，该节点在 $S$ 和 $A$ 矩阵中的度均为零。实现正确地将 $D^{-1/2}$ 中的相应条目设置为 0，确保该节点的传播产生零特征向量，从而有效地将其从消息传递过程中隔离出来。最终结果以配对 $[\\rho(S^{(k)}, X^{(k)}), \\rho(A^{(k)}, X^{(k)})]$ 的形式呈现，适用于每个案例 $k \\in \\{1, 2, 3\\}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares GCN message scaling ratios for raw and volume-normalized\n    brain connectivity matrices across three test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1\n        {\n            \"S\": np.array([\n                [0, 120, 80, 60],\n                [120, 0, 90, 70],\n                [80, 90, 0, 50],\n                [60, 70, 50, 0]\n            ], dtype=float),\n            \"vol\": np.array([3200, 2800, 4000, 3500], dtype=float),\n            \"X\": np.array([\n                [2.0, 0.5, -1.0],\n                [1.5, -0.2, 0.3],\n                [-0.7, 1.2, 0.8],\n                [0.0, -1.0, 0.5]\n            ], dtype=float),\n        },\n        # Case 2\n        {\n            \"S\": np.array([\n                [0, 300, 250, 200],\n                [300, 0, 60, 40],\n                [250, 60, 0, 30],\n                [200, 40, 30, 0]\n            ], dtype=float),\n            \"vol\": np.array([800, 5000, 4800, 5100], dtype=float),\n            \"X\": np.array([\n                [1.0, -0.5, 0.2],\n                [-0.3, 0.8, -0.1],\n                [0.5, 0.5, 0.5],\n                [-1.2, 0.0, 0.7]\n            ], dtype=float),\n        },\n        # Case 3\n        {\n            \"S\": np.array([\n                [0, 50, 0, 0],\n                [50, 0, 40, 0],\n                [0, 40, 0, 0],\n                [0, 0, 0, 0]\n            ], dtype=float),\n            \"vol\": np.array([3000, 3100, 2900, 3200], dtype=float),\n            \"X\": np.array([\n                [0.2, 0.1, -0.4],\n                [1.0, -1.0, 0.0],\n                [-0.5, 0.7, 0.3],\n                [0.9, -0.8, 0.2]\n            ], dtype=float),\n        }\n    ]\n\n    def compute_scaling_ratio(W, X):\n        \"\"\"\n        Calculates the message scaling ratio rho(W, X).\n        \"\"\"\n        norm_X = np.linalg.norm(X, 'fro')\n        \n        # If X is a zero matrix, the ratio is undefined, but for this problem, we can return 0.\n        if norm_X == 0:\n            return 0.0\n\n        # Calculate degree vector d from the weight matrix W\n        d = W.sum(axis=1)\n\n        # Calculate D^{-1/2} handling zero degrees\n        # np.power with `where` argument efficiently handles d_ii > 0\n        d_inv_sqrt_vals = np.power(d, -0.5, where=(d > 0))\n        D_inv_sqrt = np.diag(d_inv_sqrt_vals)\n\n        # Compute the symmetric degree-normalized propagation operator P\n        P = D_inv_sqrt @ W @ D_inv_sqrt\n\n        # Propagate features: Y = P * X\n        Y = P @ X\n\n        # Calculate the Frobenius norm of the propagated features\n        norm_Y = np.linalg.norm(Y, 'fro')\n\n        # Compute the scaling ratio\n        rho = norm_Y / norm_X\n        \n        return rho\n\n    results = []\n    for case in test_cases:\n        S = case[\"S\"]\n        vol = case[\"vol\"]\n        X = case[\"X\"]\n\n        # Compute rho for raw streamline counts S\n        rho_S = compute_scaling_ratio(S, X)\n\n        # Compute volume-normalized adjacency matrix A\n        vol_outer = np.outer(vol, vol)\n        # Element-wise division, handling potential division by zero\n        # Since vol_i > 0, vol_outer will have all non-zero entries.\n        A = S / vol_outer\n        \n        # Compute rho for volume-normalized matrix A\n        rho_A = compute_scaling_ratio(A, X)\n\n        results.append([rho_S, rho_A])\n    \n    # Format the final output string as required\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "由于纤维束示踪等技术的局限性，现实世界中的连接组数据往往是不完整的，存在着缺失的连接。本项高级练习介绍了一种基于图扩散理论的强大插补方法，利用图拉普拉斯算子来估计缺失连接的强度。您将亲手实现这一技术，并量化其对图拓扑结构和图神经网络（GNN）消息传递过程的影响，从而学习到一种处理数据不完美性的原则性方法。",
            "id": "4167865",
            "problem": "您将获得由对角线为零的对称邻接矩阵表示的无向加权脑连接图。在基于扩散的插补中，其直观理解是结构连接性支持信号在图上的扩散。当由于纤维束成像失败而导致连接缺失时，可以通过扩散将相应节点对耦合的强度来估计其权重。使用以下基本原理：具有邻接矩阵 $A$ 的无向加权图的组合图拉普拉斯算子 $L$ 定义为 $L = D - A$，其中 $D$ 是对角度矩阵 $D_{ii} = \\sum_j A_{ij}$。图热方程是线性常微分方程 $dZ(t)/dt = -L Z(t)$，其解为 $Z(t) = \\exp(-t L) Z(0)$，其中 $\\exp(\\cdot)$ 表示矩阵指数，$t \\ge 0$ 是扩散时间。对于一对节点 $i$ 和 $j$，项 $\\left[\\exp(-t L)\\right]_{ij}$ 量化了节点 $i$ 和 $j$ 在时间 $t$ 的扩散亲和力。\n\n请按如下方式设计并实现一个基于图扩散的插补方案。给定一个观测到的邻接矩阵 $A_{\\mathrm{obs}}$ 和一个对角线为零的缺失边对称二元掩码 $M$，通过将仅缺失节点对之间的扩散亲和力（由一个非负标量 $\\lambda$ 缩放）加到 $A_{\\mathrm{obs}}$ 上，来构建一个插补后的邻接矩阵 $A_{\\mathrm{imp}}$。具体来说，计算热核 $K_t = \\exp(-t L_{\\mathrm{obs}})$，其中 $L_{\\mathrm{obs}} = D_{\\mathrm{obs}} - A_{\\mathrm{obs}}$，并设置 $B = K_t$，同时将其对角线强制置零。然后使用哈达玛积仅更新缺失的条目，以获得 $A_{\\mathrm{imp}} = \\mathrm{sym}\\big(A_{\\mathrm{obs}} + \\lambda \\,(M \\circ B)\\big)$，其中 $\\mathrm{sym}(X) = \\tfrac{1}{2}(X + X^\\top)$ 用于强制对称。所有矩阵都是实值的，且 $A_{\\mathrm{obs}}$ 的项为非负，对角线为零。\n\n量化插补对以下两个量的影响：\n- 节点度：计算度变化向量 $\\Delta d = d_{\\mathrm{imp}} - d_{\\mathrm{obs}}$，其中 $d_{\\mathrm{obs}} = A_{\\mathrm{obs}} \\mathbf{1}$ 和 $d_{\\mathrm{imp}} = A_{\\mathrm{imp}} \\mathbf{1}$，$\\mathbf{1}$ 是全一向量。\n- 由单层图卷积网络（GCN，一类图神经网络 GNN）使用的单步消息传递：令 $\\tilde{A} = A + I$ 添加自环，$\\tilde{D}$ 为相应的度矩阵，其中 $\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$，且 $S = \\tilde{D}^{-1/2} \\tilde{A}\\,\\tilde{D}^{-1/2}$。对于收集在矩阵 $X$ 中的节点特征，经过一个使用单位权重的层后，其激活前消息为 $M = S X$。计算在使用 $A_{\\mathrm{obs}}$ 和 $A_{\\mathrm{imp}}$ 时，消息的逐节点欧几里得范数变化，即向量 $r$，其条目为 $r_i = \\lVert ((S_{\\mathrm{imp}} - S_{\\mathrm{obs}}) X)_{i,:} \\rVert_2$。\n\n实现一个程序，针对下述每个测试用例，计算并输出：\n- 度变化向量 $\\Delta d$，形式为浮点数列表。\n- 逐节点消息变化向量 $r$，形式为浮点数列表。\n将所有浮点数输出四舍五入到恰好 $6$ 位小数。\n\n使用以下测试套件。在所有情况下，特征矩阵 $X \\in \\mathbb{R}^{4 \\times 2}$ 为\n$$\nX = \\begin{bmatrix}\n1  0\\\\\n0  1\\\\\n1  1\\\\\n2  -1\n\\end{bmatrix}.\n$$\n测试用例 1（正常路径）：\n- $A_{\\mathrm{obs}} \\in \\mathbb{R}^{4 \\times 4}$:\n$$\n\\begin{bmatrix}\n0  0.8  0  0.2\\\\\n0.8  0  0.6  0\\\\\n0  0.6  0  0.7\\\\\n0.2  0  0.7  0\n\\end{bmatrix}.\n$$\n- 缺失边掩码 $M$:\n$$\n\\begin{bmatrix}\n0  0  1  0\\\\\n0  0  0  1\\\\\n1  0  0  0\\\\\n0  1  0  0\n\\end{bmatrix}.\n$$\n- 扩散时间 $t = 0.5$，缩放系数 $\\lambda = 1.0$。\n\n测试用例 2（无缺失边）：\n- $A_{\\mathrm{obs}}$ 与测试用例 1 相同。\n- 缺失边掩码 $M$ 是大小为 $4 \\times 4$ 的零矩阵。\n- 扩散时间 $t = 0.5$，缩放系数 $\\lambda = 1.0$。\n\n测试用例 3（不连通分量；跨分量的缺失边）：\n- $A_{\\mathrm{obs}} \\in \\mathbb{R}^{4 \\times 4}$:\n$$\n\\begin{bmatrix}\n0  0.5  0  0\\\\\n0.5  0  0  0\\\\\n0  0  0  0.5\\\\\n0  0  0.5  0\n\\end{bmatrix}.\n$$\n- 缺失边掩码 $M$:\n$$\n\\begin{bmatrix}\n0  0  0  1\\\\\n0  0  1  0\\\\\n0  1  0  0\\\\\n1  0  0  0\n\\end{bmatrix}.\n$$\n- 扩散时间 $t = 1.0$，缩放系数 $\\lambda = 1.0$。\n\n测试用例 4（时间边界，零扩散）：\n- $A_{\\mathrm{obs}}$ 和 $M$ 与测试用例 1 相同。\n- 扩散时间 $t = 0.0$，缩放系数 $\\lambda = 1.0$。\n\n测试用例 5（强扩散与部分缩放）：\n- $A_{\\mathrm{obs}}$ 和 $M$ 与测试用例 1 相同。\n- 扩散时间 $t = 3.0$，缩放系数 $\\lambda = 0.5$。\n\n对于每个测试用例，计算度变化向量 $\\Delta d$ 和逐节点消息变化向量 $r$，并将每个值四舍五入到 $6$ 位小数。您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试用例，其本身是一个包含两个列表的列表：第一个是 $\\Delta d$，第二个是 $r$。例如，输出格式必须类似于 $[\\,[\\,[\\Delta d\\_\\mathrm{case1}], [r\\_\\mathrm{case1}]\\,], \\ldots, [\\,[\\Delta d\\_\\mathrm{case5}], [r\\_\\mathrm{case5}]\\,]\\,]$，并用实际的数字列表替换。不应打印任何额外文本。",
            "solution": "问题陈述经评估有效。它在科学上基于图论和线性代数，特别是利用图拉普拉斯算子和矩阵指数来完成基于扩散的插补任务。该问题是适定的，为定义的测试用例提供了清晰的算法描述和所有必要数据。目标是定量的，不含主观因素。在逐节点消息变化向量 $r$ 的定义中存在一个微小的歧义，通过将“逐节点欧几里得范数”解释为变化矩阵的行范数来解决，这是图机器学习领域的标准约定。\n\n任务是实现一个图插补方案，并评估其对两个图属性的影响：节点度和单步图卷积网络（GCN）消息传递操作的输出。该过程是确定性的，可以使用标准数值线性代数例程来实现。\n\n插补方法如下：\n给定一个观测到的邻接矩阵 $A_{\\mathrm{obs}}$（对称、非负、零对角线）、一个指示缺失边的对称二元掩码 $M$、一个扩散时间 $t \\ge 0$ 和一个缩放因子 $\\lambda \\ge 0$。\n\n1.  **构造图拉普拉斯算子**：组合拉普拉斯算子 $L_{\\mathrm{obs}}$ 是从观测图中计算出来的。\n    $$\n    L_{\\mathrm{obs}} = D_{\\mathrm{obs}} - A_{\\mathrm{obs}}\n    $$\n    其中 $D_{\\mathrm{obs}}$ 是对角度矩阵，其满足 $(D_{\\mathrm{obs}})_{ii} = \\sum_j (A_{\\mathrm{obs}})_{ij}$。\n\n2.  **计算热核**：热核 $K_t$ 是图热方程的解，通过矩阵指数计算得出。\n    $$\n    K_t = \\exp(-t L_{\\mathrm{obs}})\n    $$\n    由于对于无向图，$L_{\\mathrm{obs}}$ 是实对称的，因此 $K_t$ 也是实对称的。\n\n3.  **形成插补矩阵**：通过取热核 $K_t$ 并将其对角线条目置零来形成矩阵 $B$。这移除了自扩散项。\n    $$\n    B_{ij} = \\begin{cases} (K_t)_{ij}  i \\neq j \\\\ 0  i = j \\end{cases}\n    $$\n    新的边权重由 $B$ 中对应于掩码 $M$ 指定的缺失边的条目确定，并由 $\\lambda$ 缩放。更新是逐元素应用的（哈达玛积 $\\circ$）。\n\n4.  **计算插补后的邻接矩阵**：插补后的邻接矩阵 $A_{\\mathrm{imp}}$ 是通过将新的边权重加到 $A_{\\mathrm{obs}}$ 并确保对称性来形成的。\n    $$\n    A_{\\mathrm{imp}} = \\mathrm{sym}\\big(A_{\\mathrm{obs}} + \\lambda \\,(M \\circ B)\\big) = \\frac{1}{2}\\left( (A_{\\mathrm{obs}} + \\lambda \\,(M \\circ B)) + (A_{\\mathrm{obs}} + \\lambda \\,(M \\circ B))^\\top \\right)\n    $$\n    由于 $A_{\\mathrm{obs}}$、$M$ 和 $B$ 都是对称的，它们的和及哈达玛积也是对称的，这使得 $\\mathrm{sym}(\\cdot)$ 操作在形式上是多余的，但属于良好实践。\n\n计算出 $A_{\\mathrm{imp}}$ 后，我们评估其影响。\n\n1.  **度变化向量 $\\Delta d$**：节点的度是其入射边权重的总和。度向量 $d$ 由 $d = A \\mathbf{1}$ 给出，其中 $\\mathbf{1}$ 是全一向量。度的变化为：\n    $$\n    \\Delta d = d_{\\mathrm{imp}} - d_{\\mathrm{obs}} = A_{\\mathrm{imp}}\\mathbf{1} - A_{\\mathrm{obs}}\\mathbf{1}\n    $$\n\n2.  **逐节点消息变化向量 $r$**：这量化了单个 GCN 层输出的变化。归一化传播矩阵为 $S = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$，其中 $\\tilde{A} = A + I$，$\\tilde{D}$ 是 $\\tilde{A}$ 的度矩阵。给定一个节点特征矩阵 $X$，传播后特征的变化是 $(S_{\\mathrm{imp}} - S_{\\mathrm{obs}})X$。“逐节点”变化是这个结果矩阵每一行的欧几里得范数。令 $\\Delta M = (S_{\\mathrm{imp}} - S_{\\mathrm{obs}})X$。向量 $r$ 的条目为：\n    $$\n    r_i = \\left\\lVert (\\Delta M)_{i,:} \\right\\rVert_2 = \\sqrt{\\sum_{k} (\\Delta M_{ik})^2}\n    $$\n    其中 $(\\Delta M)_{i,:}$ 表示矩阵 $\\Delta M$ 的第 $i$ 行。\n\n我们将实现一个程序，对每个提供的测试用例执行这些步骤。矩阵指数 $\\exp(\\cdot)$ 将使用 `scipy.linalg.expm` 计算，所有其他操作将使用 `numpy`。向量 $\\Delta d$ 和 $r$ 的最终数值结果将四舍五入到 $6$ 位小数。\n\n特殊情况提供了有用的验证。如果 $M$ 是零矩阵（测试用例 2）或者 $t=0$（测试用例 4），那么 $K_t = I$，导致 $B=0$。在这两种情况下，插补项 $\\lambda(M \\circ B)$ 为零，导致 $A_{\\mathrm{imp}} = A_{\\mathrm{obs}}$。因此，$\\Delta d$ 和 $r$ 都必须是零向量，这可作为对实现的合理性检查。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    \n    # Node feature matrix X, common to all test cases.\n    X = np.array([\n        [1.0, 0.0],\n        [0.0, 1.0],\n        [1.0, 1.0],\n        [2.0, -1.0]\n    ])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A_obs\": np.array([\n                [0.0, 0.8, 0.0, 0.2],\n                [0.8, 0.0, 0.6, 0.0],\n                [0.0, 0.6, 0.0, 0.7],\n                [0.2, 0.0, 0.7, 0.0]\n            ]),\n            \"M\": np.array([\n                [0.0, 0.0, 1.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0],\n                [1.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0]\n            ]),\n            \"t\": 0.5,\n            \"lam\": 1.0\n        },\n        {\n            \"A_obs\": np.array([\n                [0.0, 0.8, 0.0, 0.2],\n                [0.8, 0.0, 0.6, 0.0],\n                [0.0, 0.6, 0.0, 0.7],\n                [0.2, 0.0, 0.7, 0.0]\n            ]),\n            \"M\": np.zeros((4, 4)),\n            \"t\": 0.5,\n            \"lam\": 1.0\n        },\n        {\n            \"A_obs\": np.array([\n                [0.0, 0.5, 0.0, 0.0],\n                [0.5, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.5],\n                [0.0, 0.0, 0.5, 0.0]\n            ]),\n            \"M\": np.array([\n                [0.0, 0.0, 0.0, 1.0],\n                [0.0, 0.0, 1.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0],\n                [1.0, 0.0, 0.0, 0.0]\n            ]),\n            \"t\": 1.0,\n            \"lam\": 1.0\n        },\n        {\n            \"A_obs\": np.array([\n                [0.0, 0.8, 0.0, 0.2],\n                [0.8, 0.0, 0.6, 0.0],\n                [0.0, 0.6, 0.0, 0.7],\n                [0.2, 0.0, 0.7, 0.0]\n            ]),\n            \"M\": np.array([\n                [0.0, 0.0, 1.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0],\n                [1.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0]\n            ]),\n            \"t\": 0.0,\n            \"lam\": 1.0\n        },\n        {\n            \"A_obs\": np.array([\n                [0.0, 0.8, 0.0, 0.2],\n                [0.8, 0.0, 0.6, 0.0],\n                [0.0, 0.6, 0.0, 0.7],\n                [0.2, 0.0, 0.7, 0.0]\n            ]),\n            \"M\": np.array([\n                [0.0, 0.0, 1.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0],\n                [1.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0]\n            ]),\n            \"t\": 3.0,\n            \"lam\": 0.5\n        }\n    ]\n\n    def compute_effects(A_obs, M, t, lam, X_feat):\n        \"\"\"\n        Computes the imputed adjacency and the resulting changes.\n        \"\"\"\n        n = A_obs.shape[0]\n\n        # Step 1: Compute A_imp\n        D_obs_diag = A_obs.sum(axis=1)\n        D_obs = np.diag(D_obs_diag)\n        L_obs = D_obs - A_obs\n        \n        K_t = expm(-t * L_obs)\n        B = K_t - np.diag(np.diag(K_t))\n        \n        A_imp_pre_sym = A_obs + lam * (M * B)\n        A_imp = 0.5 * (A_imp_pre_sym + A_imp_pre_sym.T)\n\n        # Step 2: Compute degree change Delta d\n        d_obs = A_obs.sum(axis=1)\n        d_imp = A_imp.sum(axis=1)\n        delta_d = d_imp - d_obs\n\n        # Step 3: Compute message change r\n        def get_S(A):\n            A_tilde = A + np.eye(n)\n            d_tilde = A_tilde.sum(axis=1)\n            # Create D_tilde^(-1/2) safely\n            d_tilde_inv_sqrt = np.power(d_tilde, -0.5, where=d_tilde > 0)\n            D_tilde_inv_sqrt = np.diag(d_tilde_inv_sqrt)\n            return D_tilde_inv_sqrt @ A_tilde @ D_tilde_inv_sqrt\n\n        S_obs = get_S(A_obs)\n        S_imp = get_S(A_imp)\n        \n        delta_M = (S_imp - S_obs) @ X_feat\n        r = np.linalg.norm(delta_M, axis=1)\n\n        return delta_d, r\n\n    all_results = []\n    for case in test_cases:\n        delta_d, r = compute_effects(case[\"A_obs\"], case[\"M\"], case[\"t\"], case[\"lam\"], X)\n        \n        # Round results to 6 decimal places\n        delta_d_rounded = [round(val, 6) for val in delta_d]\n        r_rounded = [round(val, 6) for val in r]\n        \n        all_results.append([delta_d_rounded, r_rounded])\n\n    # Manually construct the final output string to match the required format\n    def format_list(data_list):\n        return f\"[{','.join(f'{x:.6f}' for x in data_list)}]\"\n\n    output_parts = []\n    for result_pair in all_results:\n        delta_d_str = format_list(result_pair[0])\n        r_str = format_list(result_pair[1])\n        pair_str = f\"[{delta_d_str},{r_str}]\"\n        output_parts.append(pair_str)\n\n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}