{
    "hands_on_practices": [
        {
            "introduction": "Recurrent Neural Networks are not just black-box predictors; they are dynamical systems whose temporal evolution can be rigorously analyzed. This perspective is particularly powerful in neuroscience, where network dynamics are thought to underlie functions like memory and decision-making. This exercise grounds RNNs in the theory of dynamical systems by asking you to analyze the stability of a fixed pointâ€”a key step in understanding a network's computational capabilities. By linearizing the dynamics and examining the eigenstructure of the Jacobian, you will gain hands-on experience in identifying stable and unstable manifolds that shape the network's flow field .",
            "id": "4189514",
            "problem": "You are given a continuous-time two-dimensional ($2$D) rate-based Recurrent Neural Network (RNN) defined by the dynamical system\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x}) + \\mathbf{b},\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^2$ is the state vector, $\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ is the recurrent connectivity matrix, $\\mathbf{b} \\in \\mathbb{R}^2$ is a constant input bias, and $\\boldsymbol{\\phi}(\\mathbf{x})$ is an elementwise nonlinearity with components $\\phi(x_i) = \\tanh(x_i)$ for $i \\in \\{1,2\\}$. Assume $\\mathbf{b} = \\mathbf{0}$ so that $\\mathbf{x}^\\star = \\mathbf{0}$ is always a fixed point. The Jacobian of the vector field at the fixed point is\n$$\n\\mathbf{J} = \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star} = -\\mathbf{I} + \\mathbf{W}\\,\\mathrm{diag}\\left(\\phi'(x_1^\\star), \\phi'(x_2^\\star)\\right),\n$$\nand since $\\phi'(0) = 1$, this simplifies to $\\mathbf{J} = -\\mathbf{I} + \\mathbf{W}$.\n\nStarting from the fundamental definitions of fixed points and linearization for autonomous dynamical systems, and using the well-established fact that the local behavior near a hyperbolic fixed point is governed by the eigenstructure of the Jacobian, your task is to:\n- Compute the eigenvalues and eigenvectors of $\\mathbf{J}$ at $\\mathbf{x}^\\star$.\n- Classify the local invariant manifolds: the stable manifold has dimension equal to the number of eigenvalues of $\\mathbf{J}$ with strictly negative real part, the unstable manifold has dimension equal to the number with strictly positive real part, and the center manifold has dimension equal to the number with real part equal to zero.\n- Construct initialization directions for trajectory simulation by using the eigenvectors of $\\mathbf{J}$. For real eigenvectors, use the eigenvector and its negative. For complex-conjugate eigenpairs in two dimensions, form a real basis using the real and imaginary parts of one complex eigenvector, and include both signs of each real direction.\n- Simulate forward trajectories initialized at $\\mathbf{x}(0) = \\mathbf{x}^\\star + \\varepsilon\\,\\mathbf{d}$ for each direction $\\mathbf{d}$ constructed above, with a small displacement magnitude $\\varepsilon$, over a finite time horizon $T$. Use the continuous-time dynamics given above, with $\\boldsymbol{\\phi}(\\mathbf{x}) = \\tanh(\\mathbf{x})$.\n\nUse the following fixed simulation parameters for all test cases:\n- Displacement magnitude $\\varepsilon = 10^{-3}$.\n- Time horizon $T = 3$ (time unit is an abstract modeling unit; you do not need to output any physical unit).\n- Spectral classification tolerance $\\delta = 10^{-8}$, applied to the real parts of eigenvalues: real part $< -\\delta$ is stable, $> \\delta$ is unstable, and $|\\cdot| \\le \\delta$ is center.\n\nFor each test case, compute:\n1. The integer $n_s$, the dimension of the stable manifold.\n2. The integer $n_u$, the dimension of the unstable manifold.\n3. The integer $n_c$, the dimension of the center manifold.\n4. A boolean $q$ indicating whether all simulated trajectories along directions associated with stable eigenvalues strictly decrease in Euclidean distance to $\\mathbf{x}^\\star$ over time $T$, and all simulated trajectories along directions associated with unstable eigenvalues strictly increase in Euclidean distance to $\\mathbf{x}^\\star$ over time $T$. If there are no stable or no unstable directions, treat the corresponding condition as vacuously true.\n\nYour program must implement the above and produce a single line of output containing the results for all test cases as a comma-separated list of lists, each inner list in the form $[n_s, n_u, n_c, q]$, enclosed in square brackets. For example, the output format should be like\n$$\n[[n_{s,1}, n_{u,1}, n_{c,1}, q_1],[n_{s,2}, n_{u,2}, n_{c,2}, q_2],\\dots]\n$$\nwith no additional text.\n\nTest Suite:\nUse the following five recurrent connectivity matrices $\\mathbf{W}$, each paired implicitly with $\\mathbf{b} = \\mathbf{0}$ and $\\boldsymbol{\\phi}(x) = \\tanh(x)$:\n- Case $1$ (saddle): $\\mathbf{W} = \\begin{bmatrix} 1.5 & 0.0 \\\\ 0.0 & 0.5 \\end{bmatrix}$.\n- Case $2$ (stable node): $\\mathbf{W} = \\begin{bmatrix} 0.2 & 0.0 \\\\ 0.0 & 0.3 \\end{bmatrix}$.\n- Case $3$ (unstable node): $\\mathbf{W} = \\begin{bmatrix} 1.3 & 0.0 \\\\ 0.0 & 1.4 \\end{bmatrix}$.\n- Case $4$ (stable spiral): $\\mathbf{W} = \\begin{bmatrix} 0.5 & 1.8 \\\\ -1.8 & 0.5 \\end{bmatrix}$.\n- Case $5$ (center, purely imaginary linearization): $\\mathbf{W} = \\begin{bmatrix} 1.0 & 1.0 \\\\ -1.0 & 1.0 \\end{bmatrix}$.\n\nYour program must be self-contained, must not read any input, and must apply the above procedure to the provided test suite. The final output must be printed exactly as specified, on a single line.",
            "solution": "The problem requires an analysis of the local dynamics of a two-dimensional continuous-time Recurrent Neural Network (RNN) around a fixed point. The analysis involves linearization, eigen-decomposition of the resulting Jacobian matrix, classification of invariant manifolds, and numerical verification of the dynamics along eigendirections.\n\nThe dynamical system is given by the ordinary differential equation (ODE):\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x}) + \\mathbf{b}\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^2$ is the state, $\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ is the connectivity matrix, $\\mathbf{b} \\in \\mathbb{R}^2$ is a bias, and $\\boldsymbol{\\phi}$ is the elementwise hyperbolic tangent activation function, $\\phi(x_i) = \\tanh(x_i)$.\n\nFirst, we establish the framework for the analysis based on fundamental principles of dynamical systems.\n\n**1. Fixed Point and Linearization**\n\nA fixed point $\\mathbf{x}^\\star$ of the system is a state where the dynamics cease, i.e., $\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}^\\star) = \\mathbf{0}$. We are given that the bias $\\mathbf{b} = \\mathbf{0}$. We can verify that the origin $\\mathbf{x}^\\star = \\mathbf{0}$ is a fixed point:\n$$\n\\mathbf{f}(\\mathbf{0}) = -\\mathbf{0} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{0}) = \\mathbf{0} + \\mathbf{W}\\,\\mathbf{0} = \\mathbf{0}\n$$\nsince $\\tanh(0) = 0$.\n\nThe behavior of the system in the vicinity of a fixed point can be approximated by a linear system. This is achieved by taking the first-order Taylor expansion of $\\mathbf{f}(\\mathbf{x})$ around $\\mathbf{x}^\\star$. Let $\\mathbf{x} = \\mathbf{x}^\\star + \\delta\\mathbf{x}$, where $\\delta\\mathbf{x}$ is a small perturbation.\n$$\n\\frac{d(\\mathbf{x}^\\star + \\delta\\mathbf{x})}{dt} = \\frac{d\\delta\\mathbf{x}}{dt} \\approx \\mathbf{f}(\\mathbf{x}^\\star) + \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star} \\delta\\mathbf{x}\n$$\nSince $\\mathbf{f}(\\mathbf{x}^\\star) = \\mathbf{0}$, the linearized dynamics are governed by the Jacobian matrix $\\mathbf{J} = \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star}$:\n$$\n\\frac{d\\delta\\mathbf{x}}{dt} \\approx \\mathbf{J}\\,\\delta\\mathbf{x}\n$$\nThe Jacobian of $\\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x})$ is:\n$$\n\\mathbf{J} = \\frac{\\partial}{\\partial \\mathbf{x}} (-\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x})) = -\\mathbf{I} + \\mathbf{W}\\,\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}}\n$$\nwhere $\\mathbf{I}$ is the $2 \\times 2$ identity matrix and $\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}}$ is the Jacobian of the elementwise activation function. For $\\phi(x_i) = \\tanh(x_i)$, the derivative is $\\phi'(x_i) = 1 - \\tanh^2(x_i) = \\mathrm{sech}^2(x_i)$. The Jacobian of $\\boldsymbol{\\phi}$ is a diagonal matrix:\n$$\n\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}} = \\mathrm{diag}(\\phi'(x_1), \\phi'(x_2))\n$$\nEvaluating at the fixed point $\\mathbf{x}^\\star = \\mathbf{0}$, we have $\\phi'(0) = \\mathrm{sech}^2(0) = 1$. Thus, the diagonal matrix becomes the identity matrix, $\\mathbf{I}$. The Jacobian at the origin simplifies to:\n$$\n\\mathbf{J} = -\\mathbf{I} + \\mathbf{W}\\,\\mathbf{I} = -\\mathbf{I} + \\mathbf{W}\n$$\n\n**2. Eigenstructure and Stability Classification**\n\nThe Hartman-Grobman theorem states that for a hyperbolic fixed point (one for which the Jacobian has no eigenvalues with zero real part), the qualitative behavior of the nonlinear system near the fixed point is topologically equivalent to that of its linearization. The stability of the fixed point is determined by the eigenvalues $\\{\\lambda_i\\}$ of $\\mathbf{J}$.\n- If $\\mathrm{Re}(\\lambda_i) < 0$ for all $i$, the fixed point is stable.\n- If $\\mathrm{Re}(\\lambda_i) > 0$ for at least one $i$, the fixed point is unstable.\n- If $\\mathrm{Re}(\\lambda_i) = 0$ for some $i$ and $\\mathrm{Re}(\\lambda_j) \\le 0$ for all other $j$, the fixed point is non-hyperbolic, and linear analysis may be inconclusive about stability without examining higher-order terms. However, we can still classify the invariant manifolds.\n\nThe dimensions of the stable, unstable, and center manifolds ($n_s$, $n_u$, $n_c$) are determined by counting the eigenvalues of $\\mathbf{J}$ based on the sign of their real parts. Using the specified tolerance $\\delta = 10^{-8}$:\n- $n_s$ is the number of eigenvalues $\\lambda_i$ with $\\mathrm{Re}(\\lambda_i) < -\\delta$.\n- $n_u$ is the number of eigenvalues $\\lambda_i$ with $\\mathrm{Re}(\\lambda_i) > \\delta$.\n- $n_c$ is the number of eigenvalues $\\lambda_i$ with $|\\mathrm{Re}(\\lambda_i)| \\le \\delta$.\nFor a $2$D system, we must have $n_s + n_u + n_c = 2$.\n\n**3. Initialization Directions from Eigenvectors**\n\nThe eigenvectors of $\\mathbf{J}$ span the invariant subspaces of the linear system. By initiating trajectories along these directions, we can observe the behavior characteristic of each manifold. Let $\\{\\lambda_i, \\mathbf{v}_i\\}$ be the eigenpairs of $\\mathbf{J}$.\n- **Real Eigenvectors**: If an eigenvector $\\mathbf{v}$ is real, it defines a line through the origin that is invariant under the linear flow. We test the dynamics by initializing trajectories at $\\mathbf{x}(0) = \\varepsilon \\mathbf{v}$ and $\\mathbf{x}(0) = -\\varepsilon \\mathbf{v}$ for a small $\\varepsilon$.\n- **Complex Eigenvectors**: If $\\mathbf{J}$ is real, its complex eigenvalues come in conjugate pairs, $\\lambda, \\bar{\\lambda}$, with corresponding complex conjugate eigenvectors $\\mathbf{v}, \\bar{\\mathbf{v}}$. Let $\\mathbf{v} = \\mathbf{a} + i\\mathbf{b}$. The real vectors $\\mathbf{a}$ and $\\mathbf{b}$ form a basis for the two-dimensional invariant subspace associated with this pair of eigenvalues. We test the dynamics within this plane by initializing trajectories along four directions: $\\mathbf{x}(0) = \\pm\\varepsilon\\mathbf{a}$ and $\\mathbf{x}(0) = \\pm\\varepsilon\\mathbf{b}$.\n\n**4. Numerical Simulation and Verification**\n\nTo verify that the nonlinear dynamics conform to the linear prediction for small perturbations, we numerically integrate the original ODE, $\\frac{d\\mathbf{x}}{dt} = -\\mathbf{x} + \\mathbf{W}\\,\\tanh(\\mathbf{x})$, for each constructed direction $\\mathbf{d}$. The initial condition is $\\mathbf{x}(0) = \\varepsilon \\mathbf{d}$ with $\\varepsilon = 10^{-3}$, and the simulation runs for a time horizon of $T = 3$.\n\nWe then compute the boolean flag $q$. For each trajectory, we calculate the Euclidean distance to the fixed point, $\\|\\mathbf{x}(t)\\|$, at multiple time points.\n- For a direction $\\mathbf{d}$ associated with a stable eigenvalue ($\\mathrm{Re}(\\lambda) < -\\delta$), the distance $\\|\\mathbf{x}(t)\\|$ must strictly decrease over the entire simulation interval.\n- For a direction $\\mathbf{d}$ associated with an unstable eigenvalue ($\\mathrm{Re}(\\lambda) > \\delta$), the distance $\\|\\mathbf{x}(t)\\|$ must strictly increase.\n\nThe flag $q$ is true if and only if all trajectories associated with stable directions exhibit strictly decreasing distance and all trajectories associated with unstable directions exhibit strictly increasing distance. If there are no stable or no unstable directions, the respective condition is vacuously true.\n\nThe implementation will proceed by iterating through each provided matrix $\\mathbf{W}$, performing these four steps, and collecting the results $[n_s, n_u, n_c, q]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the RNN dynamics analysis problem for a suite of test cases.\n    \"\"\"\n    \n    # Fixed simulation parameters\n    epsilon = 1e-3\n    T = 3.0\n    delta = 1e-8\n    \n    # Test suite of connectivity matrices W\n    test_cases = [\n        # Case 1 (saddle)\n        np.array([[1.5, 0.0], [0.0, 0.5]]),\n        # Case 2 (stable node)\n        np.array([[0.2, 0.0], [0.0, 0.3]]),\n        # Case 3 (unstable node)\n        np.array([[1.3, 0.0], [0.0, 1.4]]),\n        # Case 4 (stable spiral)\n        np.array([[0.5, 1.8], [-1.8, 0.5]]),\n        # Case 5 (center, purely imaginary linearization)\n        np.array([[1.0, 1.0], [-1.0, 1.0]]),\n    ]\n    \n    # List to store results for all test cases\n    all_results = []\n    \n    # ODE definition for the RNN dynamics\n    def f(t, x, W):\n        return -x + W @ np.tanh(x)\n\n    for W in test_cases:\n        # 1. Compute Jacobian at the origin\n        J = -np.eye(2) + W\n        \n        # 2. Compute eigenvalues and eigenvectors\n        eigvals, eigvecs = np.linalg.eig(J)\n        \n        # 3. Classify manifolds and count dimensions\n        ns, nu, nc = 0, 0, 0\n        real_parts = np.real(eigvals)\n        \n        for r in real_parts:\n            if r  -delta:\n                ns += 1\n            elif r > delta:\n                nu += 1\n            else:\n                nc += 1\n        \n        # 4. Construct initialization directions\n        stable_dirs = []\n        unstable_dirs = []\n        \n        # The eigenvalues are either both real or a complex conjugate pair\n        if np.iscomplexobj(eigvals):\n            # Complex conjugate pair\n            v = eigvecs[:, 0]\n            real_part = np.real(v)\n            imag_part = np.imag(v)\n            \n            # Normalize for consistency, although epsilon scaling dominates\n            if np.linalg.norm(real_part) > 1e-9: real_part /= np.linalg.norm(real_part)\n            if np.linalg.norm(imag_part) > 1e-9: imag_part /= np.linalg.norm(imag_part)\n\n            dirs_from_complex = [real_part, -real_part, imag_part, -imag_part]\n            \n            if real_parts[0]  -delta:\n                stable_dirs.extend(dirs_from_complex)\n            elif real_parts[0] > delta:\n                unstable_dirs.extend(dirs_from_complex)\n            # Center manifold directions are not checked for the 'q' flag\n        else:\n            # Two real eigenvalues\n            for i in range(2):\n                v = eigvecs[:, i]\n                # eig gives normalized eigenvectors\n                dirs_from_real = [v, -v]\n                \n                if real_parts[i]  -delta:\n                    stable_dirs.extend(dirs_from_real)\n                elif real_parts[i] > delta:\n                    unstable_dirs.extend(dirs_from_real)\n        \n        # 5. Simulate trajectories and verify behavior for 'q' flag\n        q = True\n        \n        # Check stable directions\n        if stable_dirs:\n            is_stable_ok = True\n            for d in stable_dirs:\n                if np.linalg.norm(d)  1e-9: continue\n                x0 = epsilon * d\n                sol = solve_ivp(f, [0, T], x0, args=(W,), dense_output=True, t_eval=np.linspace(0, T, num=100))\n                distances = np.linalg.norm(sol.y, axis=0)\n                # Check for strict decrease; allow for numerical noise near zero\n                if not np.all(np.diff(distances)  0):\n                    # Check if it settles at the origin making diffs zero.\n                    if not (np.all(distances[-10:]  1e-9) and distances[0] > distances[-1]):\n                         is_stable_ok = False\n                         break\n            if not is_stable_ok:\n                q = False\n\n        # Check unstable directions if stable ones passed\n        if q and unstable_dirs:\n            is_unstable_ok = True\n            for d in unstable_dirs:\n                if np.linalg.norm(d)  1e-9: continue\n                x0 = epsilon * d\n                sol = solve_ivp(f, [0, T], x0, args=(W,), dense_output=True, t_eval=np.linspace(0, T, num=100))\n                distances = np.linalg.norm(sol.y, axis=0)\n                if not np.all(np.diff(distances) > 0):\n                    is_unstable_ok = False\n                    break\n            if not is_unstable_ok:\n                q = False\n                \n        all_results.append([ns, nu, nc, q])\n        \n    # Format the final output string\n    result_str = \",\".join([str(res) for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While simple RNNs provide a basic model for temporal processing, their practical utility is often limited by the vanishing gradient problem. The Long Short-Term Memory (LSTM) architecture was designed specifically to overcome this challenge using a sophisticated system of internal gates. This practice demystifies the inner workings of an LSTM cell by guiding you through a concrete forward pass calculation. By manually computing the state updates, you will see precisely how the forget gate modulates the retention of information, providing a tangible understanding of the mechanisms that allow LSTMs to capture long-range dependencies .",
            "id": "4189572",
            "problem": "Consider a one-dimensional Long Short-Term Memory (LSTM) recurrent neural network used to model the latent temporal dynamics of a trial-averaged neural signal in a neuroscience data analysis setting. The LSTM at time step $t$ updates its gates and states according to the standard definitions: the input gate $i_t = \\sigma(z_{i,t})$, forget gate $f_t = \\sigma(z_{f,t})$, output gate $o_t = \\sigma(z_{o,t})$, and cell candidate $g_t = \\tanh(z_{g,t})$, where $\\sigma(u) = \\frac{1}{1+\\exp(-u)}$ is the logistic function and $\\tanh(u)$ is the hyperbolic tangent. The cell state and hidden state evolve as $c_t = f_t \\, c_{t-1} + i_t \\, g_t$ and $h_t = o_t \\, \\tanh(c_t)$. The pre-activations are affine functions of the current input $x_t$ and previous hidden state $h_{t-1}$: $z_{i,t} = W_{xi}\\,x_t + W_{hi}\\,h_{t-1} + b_i$, $z_{f,t} = W_{xf}\\,x_t + W_{hf}\\,h_{t-1} + b_f$, $z_{o,t} = W_{xo}\\,x_t + W_{ho}\\,h_{t-1} + b_o$, and $z_{g,t} = W_{xg}\\,x_t + W_{hg}\\,h_{t-1} + b_g$.\n\nYou are given a sequence of inputs over $T=3$ time steps designed to contain a transient at $t=2$: $x_1 = 0$, $x_2 = \\ln(3)$, and $x_3 = 0$. The initial states are $c_0 = 0$ and $h_0 = 0$. The LSTM parameters are scalars and specified as follows:\n- $W_{xi} = 1$, $W_{hi} = 0$, $b_i = 0$,\n- $W_{xf} = -\\frac{\\ln(2)}{\\ln(3)}$, $W_{hf} = 0$, $b_f = \\ln(4)$,\n- $W_{xo} = 0$, $W_{ho} = 0$, $b_o = \\ln(2)$,\n- $W_{xg} = 0$, $W_{hg} = 0$, $b_g = \\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)$.\n\nUsing only the definitions provided, compute the forward pass to obtain the gates and states for $t=1,2,3$ and use this to explicitly verify how the forget gate $f_t$ modulates the retention of the transient input. Report, as your final result, the scalar contribution to the final cell state $c_3$ that arises solely from the transient input at $t=2$ (i.e., the portion of $c_3$ attributable to $x_2$ through the LSTM dynamics). Provide the exact value; no rounding is required.",
            "solution": "The problem requires the computation of a forward pass through a one-dimensional Long Short-Term Memory (LSTM) network for $3$ time steps and the calculation of a specific contribution to the final cell state. The problem is well-posed and all necessary parameters and initial conditions are provided.\n\nFirst, we substitute the given scalar parameters into the general LSTM equations. The pre-activation functions are:\n$z_{i,t} = W_{xi}\\,x_t + W_{hi}\\,h_{t-1} + b_i = (1)x_t + (0)h_{t-1} + 0 = x_t$\n$z_{f,t} = W_{xf}\\,x_t + W_{hf}\\,h_{t-1} + b_f = \\left(-\\frac{\\ln(2)}{\\ln(3)}\\right)x_t + (0)h_{t-1} + \\ln(4) = -\\frac{\\ln(2)}{\\ln(3)}x_t + 2\\ln(2)$\n$z_{o,t} = W_{xo}\\,x_t + W_{ho}\\,h_{t-1} + b_o = (0)x_t + (0)h_{t-1} + \\ln(2) = \\ln(2)$\n$z_{g,t} = W_{xg}\\,x_t + W_{hg}\\,h_{t-1} + b_g = (0)x_t + (0)h_{t-1} + \\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)$\n\nThe gates and cell candidate are updated as follows:\n$i_t = \\sigma(z_{i,t}) = \\sigma(x_t)$\n$f_t = \\sigma(z_{f,t}) = \\sigma\\left(-\\frac{\\ln(2)}{\\ln(3)}x_t + 2\\ln(2)\\right)$\n$o_t = \\sigma(z_{o,t}) = \\sigma(\\ln(2))$\n$g_t = \\tanh(z_{g,t}) = \\tanh\\left(\\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)\\right)$\n\nThe output gate $o_t$ and the cell candidate $g_t$ are constant for all time steps. Let us compute their values. The sigmoid function is $\\sigma(u) = (1+\\exp(-u))^{-1}$.\n$o_t = \\sigma(\\ln(2)) = \\frac{1}{1+\\exp(-\\ln(2))} = \\frac{1}{1+\\frac{1}{2}} = \\frac{1}{\\frac{3}{2}} = \\frac{2}{3}$.\n$g_t = \\tanh\\left(\\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)\\right) = \\frac{1}{2}$.\n\nThe cell state and hidden state updates are:\n$c_t = f_t c_{t-1} + i_t g_t = f_t c_{t-1} + \\frac{1}{2} i_t$\n$h_t = o_t \\tanh(c_t) = \\frac{2}{3} \\tanh(c_t)$\n\nThe initial conditions are $c_0 = 0$ and $h_0 = 0$. The input sequence is $x_1 = 0$, $x_2 = \\ln(3)$, and $x_3 = 0$. We proceed with the forward pass.\n\n**Time step $t=1$:**\nInput: $x_1 = 0$.\nThe pre-activations for the input and forget gates are:\n$z_{i,1} = x_1 = 0$\n$z_{f,1} = -\\frac{\\ln(2)}{\\ln(3)}(0) + 2\\ln(2) = 2\\ln(2) = \\ln(4)$\nThe gate activations are:\n$i_1 = \\sigma(0) = \\frac{1}{1+\\exp(0)} = \\frac{1}{2}$\n$f_1 = \\sigma(\\ln(4)) = \\frac{1}{1+\\exp(-\\ln(4))} = \\frac{1}{1+\\frac{1}{4}} = \\frac{4}{5}$\nThe cell and hidden states are updated:\n$c_1 = f_1 c_0 + i_1 g_1 = \\left(\\frac{4}{5}\\right)(0) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4}$\n$h_1 = \\frac{2}{3}\\tanh(c_1) = \\frac{2}{3}\\tanh\\left(\\frac{1}{4}\\right)$\n\n**Time step $t=2$:**\nInput: $x_2 = \\ln(3)$. This is the specified transient input.\nThe pre-activations for the input and forget gates are:\n$z_{i,2} = x_2 = \\ln(3)$\n$z_{f,2} = -\\frac{\\ln(2)}{\\ln(3)} (\\ln(3)) + 2\\ln(2) = -\\ln(2) + 2\\ln(2) = \\ln(2)$\nThe gate activations are:\n$i_2 = \\sigma(\\ln(3)) = \\frac{1}{1+\\exp(-\\ln(3))} = \\frac{1}{1+\\frac{1}{3}} = \\frac{3}{4}$\n$f_2 = \\sigma(\\ln(2)) = \\frac{1}{1+\\exp(-\\ln(2))} = \\frac{1}{1+\\frac{1}{2}} = \\frac{2}{3}$\nThe cell state is updated:\n$c_2 = f_2 c_1 + i_2 g_2 = \\left(\\frac{2}{3}\\right)\\left(\\frac{1}{4}\\right) + \\left(\\frac{3}{4}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{6} + \\frac{3}{8} = \\frac{4}{24} + \\frac{9}{24} = \\frac{13}{24}$\nThe hidden state is $h_2 = \\frac{2}{3}\\tanh(c_2) = \\frac{2}{3}\\tanh\\left(\\frac{13}{24}\\right)$.\n\n**Time step $t=3$:**\nInput: $x_3 = 0$.\nThe pre-activations for the input and forget gates are the same as for $t=1$:\n$z_{i,3} = x_3 = 0 \\implies i_3 = \\frac{1}{2}$\n$z_{f,3} = \\ln(4) \\implies f_3 = \\frac{4}{5}$\nThe final cell state $c_3$ is:\n$c_3 = f_3 c_2 + i_3 g_3 = \\left(\\frac{4}{5}\\right)\\left(\\frac{13}{24}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{13}{5 \\cdot 6} + \\frac{1}{4} = \\frac{13}{30} + \\frac{1}{4} = \\frac{26}{60} + \\frac{15}{60} = \\frac{41}{60}$\nThe final hidden state is $h_3 = \\frac{2}{3}\\tanh(c_3) = \\frac{2}{3}\\tanh\\left(\\frac{41}{60}\\right)$.\n\nThe problem asks for the contribution to the final cell state $c_3$ from the transient input at $t=2$. To determine this, we can express $c_3$ purely in terms of the inputs at each step.\n$c_3 = f_3 c_2 + i_3 g_3$\nSubstituting $c_2 = f_2 c_1 + i_2 g_2$:\n$c_3 = f_3 (f_2 c_1 + i_2 g_2) + i_3 g_3 = f_3 f_2 c_1 + f_3 i_2 g_2 + i_3 g_3$\nSubstituting $c_1 = f_1 c_0 + i_1 g_1$ and using $c_0=0$:\n$c_3 = f_3 f_2 (i_1 g_1) + f_3 i_2 g_2 + i_3 g_3$\n\nThis expression decomposes $c_3$ into a sum of contributions from the inputs at each time step:\n- Contribution from $x_1$: $f_3 f_2 i_1 g_1$\n- Contribution from $x_2$: $f_3 i_2 g_2$\n- Contribution from $x_3$: $i_3 g_3$\n\nThe term $i_2 g_2$ represents the new information added to the cell state at $t=2$ due to the input $x_2$. The forget gate at the next step, $f_3$, modulates the retention of this information into the state $c_3$. Thus, the portion of $c_3$ attributable to $x_2$ is $f_3 i_2 g_2$.\n\nWe calculate the value of this term using the gate values we found:\n$f_3 = \\frac{4}{5}$\n$i_2 = \\frac{3}{4}$\n$g_2 = \\frac{1}{2}$\n\nContribution from $x_2$ to $c_3 = f_3 \\, i_2 \\, g_2 = \\left(\\frac{4}{5}\\right) \\left(\\frac{3}{4}\\right) \\left(\\frac{1}{2}\\right) = \\frac{12}{40} = \\frac{3}{10}$.\n\nThis calculation explicitly demonstrates how the forget gate $f_3 = \\frac{4}{5}$ acts on the information $i_2 g_2 = \\frac{3}{8}$ that was written to the cell due to the transient input, retaining $80\\%$ of it in the next time step. The transient input $x_2 = \\ln(3)$ induced a smaller forget gate value $f_2 = \\frac{2}{3}$ compared to when the input was zero ($f_1 = f_3 = \\frac{4}{5}$), causing the network to \"forget\" more of its past state $c_1$ when the significant event occurred.\n\nThe required final result is the scalar contribution to $c_3$ arising from the input at $t=2$.",
            "answer": "$$\\boxed{\\frac{3}{10}}$$"
        },
        {
            "introduction": "Training RNNs effectively requires navigating numerical stability challenges, the most prominent of which is the exploding gradient problem. During backpropagation through time, repeated matrix multiplications can cause gradients to grow exponentially, leading to divergent training updates. Gradient clipping is a simple yet indispensable technique for stabilizing this process. This exercise provides a hands-on implementation of gradient clipping by norm, where you will programmatically enforce a maximum magnitude on the gradient vector, ensuring that parameter updates remain within a reasonable scale and training proceeds smoothly .",
            "id": "4189540",
            "problem": "Consider training a Recurrent Neural Network (RNN) for temporal dynamics in neuroscience data analysis, where gradients propagated by Backpropagation Through Time (BPTT) can become large due to repeated multiplication by Jacobians across time steps. The fundamental update rule of gradient descent is derived from a first-order Taylor approximation of the loss function in parameter space: the parameter update is $\\Delta \\theta = -\\alpha \\nabla_{\\theta} \\mathcal{L}$, where $\\alpha$ is the learning rate, $\\theta$ is the parameter vector, and $\\nabla_{\\theta} \\mathcal{L}$ is the gradient of the loss function with respect to parameters. To mitigate exploding gradients, practitioners apply gradient clipping by norm before the update. Clipping by norm is defined as the Euclidean projection of the raw gradient vector onto the closed $\\ell_{2}$ ball of radius $\\tau$ centered at the origin. The $\\ell_{2}$ norm is defined by $\\|\\mathbf{g}\\|_{2} = \\sqrt{\\sum_{i} g_{i}^{2}}$. The projection operation is the solution of the constrained optimization problem \"find $\\mathbf{v}$ that is closest in Euclidean distance to the raw gradient vector $\\mathbf{g}$ subject to $\\|\\mathbf{v}\\|_{2} \\le \\tau$\". Your task is to implement gradient clipping by norm with threshold $\\tau$ and then compute the parameter update $\\Delta \\theta$ using the clipped gradient.\n\nStarting from the core definitions above, implement a program that:\n- Computes the clipped gradient as the Euclidean projection of the raw gradient $\\mathbf{g}$ onto the closed $\\ell_{2}$ ball of radius $\\tau$ centered at the origin.\n- Uses the clipped gradient to compute the parameter update $\\Delta \\theta = -\\alpha \\, \\mathbf{g}_{\\text{clipped}}$ for each test case.\n- Handles the boundary case $\\|\\mathbf{g}\\|_{2} = \\tau$ correctly and avoids division by zero when $\\|\\mathbf{g}\\|_{2} = 0$.\n\nImplement the program with the following test suite of parameter values. Each test case is a triple $(\\mathbf{g}, \\tau, \\alpha)$:\n- Case $1$: $\\mathbf{g} = [3.0, 4.0]$, $\\tau = 2.0$, $\\alpha = 0.1$.\n- Case $2$: $\\mathbf{g} = [2.0, 0.0]$, $\\tau = 2.0$, $\\alpha = 0.05$.\n- Case $3$: $\\mathbf{g} = [0.6, 0.8]$, $\\tau = 2.0$, $\\alpha = 0.2$.\n- Case $4$: $\\mathbf{g} = [0.0, 0.0]$, $\\tau = 1.0$, $\\alpha = 1.0$.\n- Case $5$: $\\mathbf{g} = [1.0, -2.0, 3.0, -4.0, 5.0]$, $\\tau = 1.0$, $\\alpha = 0.01$.\n- Case $6$: $\\mathbf{g} = [1000000.0, -1000000.0]$, $\\tau = 0.001$, $\\alpha = 0.001$.\n\nAll quantities are unitless parameters and vectors in $\\mathbb{R}^{n}$; no physical units are involved.\n\nFor each test case, your program should output the parameter update vector $\\Delta \\theta$ as a Python-style list of floats. Aggregate the results of all six test cases into a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, for example, $[\\text{result}_{1},\\text{result}_{2},\\dots,\\text{result}_{6}]$, where each $\\text{result}_{i}$ is itself a Python-style list. The final output must therefore look like a single line string such as $[[a_{11},a_{12}], [a_{21},a_{22}], \\dots]$, but with no spaces anywhere, for instance $[[a_{11},a_{12}],[a_{21},a_{22}],\\dots]$. The outputs must be lists of floats without any rounding instructions beyond standard floating-point representation.",
            "solution": "The task is to implement a program that calculates the parameter update vector $\\Delta\\theta$ in a gradient descent procedure, where the gradient is first subjected to clipping by norm. The fundamental relationship is $\\Delta\\theta = -\\alpha \\, \\mathbf{g}_{\\text{clipped}}$, where $\\alpha$ is the learning rate and $\\mathbf{g}_{\\text{clipped}}$ is the clipped gradient derived from the raw gradient $\\mathbf{g}$.\n\nThe core of the problem lies in the correct implementation of gradient clipping. The problem statement defines clipping by norm as the Euclidean projection of the raw gradient vector $\\mathbf{g}$ onto the closed $\\ell_2$ ball of radius $\\tau$ centered at the origin. Mathematically, this corresponds to finding a vector $\\mathbf{v}$ that solves the constrained optimization problem:\n$$\n\\begin{align*}\n\\underset{\\mathbf{v}}{\\text{minimize}}  \\quad \\|\\mathbf{v} - \\mathbf{g}\\|_2 \\\\\n\\text{subject to}  \\quad \\|\\mathbf{v}\\|_2 \\le \\tau\n\\end{align*}\n$$\nThe solution to this problem, $\\mathbf{v}$, is the clipped gradient, so we set $\\mathbf{g}_{\\text{clipped}} = \\mathbf{v}$. We derive the solution by considering two mutually exclusive and exhaustive cases based on the $\\ell_2$ norm of the raw gradient, $\\|\\mathbf{g}\\|_2$.\n\nCase I: The gradient norm is within the threshold.\nIf $\\|\\mathbf{g}\\|_2 \\le \\tau$, the vector $\\mathbf{g}$ already resides within or on the boundary of the feasible set (the closed $\\ell_2$ ball of radius $\\tau$). The point in this set that is closest to $\\mathbf{g}$ is $\\mathbf{g}$ itself, as the distance $\\|\\mathbf{g} - \\mathbf{g}\\|_2 = 0$ is the minimum possible. Therefore, the gradient is not modified.\n$$\n\\mathbf{g}_{\\text{clipped}} = \\mathbf{g} \\quad \\text{if} \\quad \\|\\mathbf{g}\\|_2 \\le \\tau\n$$\nThis case correctly handles the situations where the gradient norm is strictly less than the threshold, exactly equal to the threshold (a boundary case), and also when the gradient is the zero vector ($\\mathbf{g} = \\mathbf{0}$), for which $\\|\\mathbf{g}\\|_2 = 0 \\le \\tau$ (since $\\tau$ as a radius must be non-negative).\n\nCase II: The gradient norm exceeds the threshold.\nIf $\\|\\mathbf{g}\\|_2  \\tau$, the vector $\\mathbf{g}$ is outside the feasible region. The projection of an external point onto a closed ball lies on the ball's surface (a sphere) and is located on the line segment connecting the origin to the external point. Thus, the clipped gradient $\\mathbf{g}_{\\text{clipped}}$ must have the same direction as $\\mathbf{g}$ but its magnitude must be scaled down to $\\tau$. The direction of $\\mathbf{g}$ is given by the unit vector $\\frac{\\mathbf{g}}{\\|\\mathbf{g}\\|_2}$. To obtain a vector of length $\\tau$ in this direction, we multiply the unit vector by $\\tau$.\n$$\n\\mathbf{g}_{\\text{clipped}} = \\tau \\frac{\\mathbf{g}}{\\|\\mathbf{g}\\|_2} \\quad \\text{if} \\quad \\|\\mathbf{g}\\|_2  \\tau\n$$\n\nCombining these two cases provides a complete algorithm for computing the clipped gradient. First, we compute the $\\ell_2$ norm of the raw gradient, $N = \\|\\mathbf{g}\\|_2 = \\sqrt{\\sum_i g_i^2}$. Then, we apply the following rule:\n$$\n\\mathbf{g}_{\\text{clipped}} =\n\\begin{cases}\n\\mathbf{g}  \\text{if } N \\le \\tau \\\\\n\\mathbf{g} \\cdot \\frac{\\tau}{N}  \\text{if } N  \\tau\n\\end{cases}\n$$\nThis procedure is numerically robust. It naturally avoids division by zero, because the division by $N$ only occurs when $N  \\tau$. Since $\\tau$ must be non-negative, $N$ cannot be zero in this branch. If $N=0$, the condition $N  \\tau$ is false, and the algorithm correctly returns $\\mathbf{g}_{\\text{clipped}} = \\mathbf{g} = \\mathbf{0}$.\n\nFinally, after determining $\\mathbf{g}_{\\text{clipped}}$ for a given test case $(\\mathbf{g}, \\tau, \\alpha)$, the parameter update vector $\\Delta\\theta$ is calculated using the provided learning rate $\\alpha$:\n$$\n\\Delta\\theta = -\\alpha \\, \\mathbf{g}_{\\text{clipped}}\n$$\nThe following program implements this logic for all specified test cases, computes the corresponding $\\Delta\\theta$ for each, and formats the output as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the parameter update vector after applying gradient clipping by norm.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (raw_gradient_g, threshold_tau, learning_rate_alpha)\n    test_cases = [\n        ([3.0, 4.0], 2.0, 0.1),\n        ([2.0, 0.0], 2.0, 0.05),\n        ([0.6, 0.8], 2.0, 0.2),\n        ([0.0, 0.0], 1.0, 1.0),\n        ([1.0, -2.0, 3.0, -4.0, 5.0], 1.0, 0.01),\n        ([1000000.0, -1000000.0], 0.001, 0.001),\n    ]\n\n    all_results = []\n    for g_list, tau, alpha in test_cases:\n        # Convert the raw gradient list to a NumPy array for vector operations.\n        # Using float64 for better precision with large numbers.\n        g = np.array(g_list, dtype=np.float64)\n        \n        # Compute the L2 norm of the raw gradient vector.\n        g_norm = np.linalg.norm(g)\n        \n        g_clipped = None\n        # Check if the norm of the gradient exceeds the clipping threshold.\n        if g_norm > tau:\n            # If it exceeds, scale the gradient vector down to have a norm equal to the threshold.\n            # This is the projection onto the L2 ball.\n            # The check g_norm > tau implicitly handles the g_norm = 0 case, preventing division by zero.\n            scale_factor = tau / g_norm\n            g_clipped = g * scale_factor\n        else:\n            # If the norm is within the threshold, the gradient is not modified.\n            g_clipped = g\n            \n        # Compute the parameter update using the (potentially clipped) gradient.\n        delta_theta = -alpha * g_clipped\n        \n        # Append the result as a standard Python list.\n        all_results.append(delta_theta.tolist())\n\n    # Format the final output string according to the specified format.\n    # The format requires a string representation of a list of lists, with no spaces.\n    # e.g., [[-0.12,-0.16],[-0.1,0.0],...]\n    result_strings = []\n    for res_list in all_results:\n        # Format each inner list into a string like '[-0.12,-0.16]'\n        s = f\"[{','.join(map(str, res_list))}]\"\n        result_strings.append(s)\n        \n    # Join the inner list strings with commas and enclose in brackets.\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}