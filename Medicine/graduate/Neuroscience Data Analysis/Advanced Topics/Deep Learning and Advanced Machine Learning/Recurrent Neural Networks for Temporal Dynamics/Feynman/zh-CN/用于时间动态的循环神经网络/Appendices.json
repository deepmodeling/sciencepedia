{
    "hands_on_practices": [
        {
            "introduction": "要有效应用长短期记忆（LSTM）网络，我们必须超越其“黑箱”的认知，深入理解其内部工作原理。本练习将引导你手动完成一个简化的LSTM单元的前向传播计算，让你亲身体验输入门、遗忘门和输出门如何协同工作来更新细胞状态。通过这个过程，你将清晰地看到遗忘门 $f_t$ 如何精确地调控信息在时间步之间的保留与遗忘，这是掌握LSTM处理瞬态信号和长期依赖关系能力的关键。",
            "id": "4189572",
            "problem": "考虑一个一维长短期记忆（LSTM）循环神经网络，用于在神经科学数据分析场景中对试验平均神经信号的潜在时间动态进行建模。在时间步 $t$，LSTM 根据标准定义更新其门和状态：输入门 $i_t = \\sigma(z_{i,t})$、遗忘门 $f_t = \\sigma(z_{f,t})$、输出门 $o_t = \\sigma(z_{o,t})$ 和候选细胞 $g_t = \\tanh(z_{g,t})$，其中 $\\sigma(u) = \\frac{1}{1+\\exp(-u)}$ 是 logistic 函数，$\\tanh(u)$ 是双曲正切函数。细胞状态和隐藏状态的演化方式为 $c_t = f_t \\, c_{t-1} + i_t \\, g_t$ 和 $h_t = o_t \\, \\tanh(c_t)$。激活前的值是当前输入 $x_t$ 和前一个隐藏状态 $h_{t-1}$ 的仿射函数：$z_{i,t} = W_{xi}\\,x_t + W_{hi}\\,h_{t-1} + b_i$， $z_{f,t} = W_{xf}\\,x_t + W_{hf}\\,h_{t-1} + b_f$， $z_{o,t} = W_{xo}\\,x_t + W_{ho}\\,h_{t-1} + b_o$，以及 $z_{g,t} = W_{xg}\\,x_t + W_{hg}\\,h_{t-1} + b_g$。\n\n给定一个在 $T=3$ 个时间步上的输入序列，该序列被设计为在 $t=2$ 时包含一个瞬态：$x_1 = 0$, $x_2 = \\ln(3)$ 和 $x_3 = 0$。初始状态为 $c_0 = 0$ 和 $h_0 = 0$。LSTM 参数均为标量，具体指定如下：\n- $W_{xi} = 1$, $W_{hi} = 0$, $b_i = 0$,\n- $W_{xf} = -\\frac{\\ln(2)}{\\ln(3)}$, $W_{hf} = 0$, $b_f = \\ln(4)$,\n- $W_{xo} = 0$, $W_{ho} = 0$, $b_o = \\ln(2)$,\n- $W_{xg} = 0$, $W_{hg} = 0$, $b_g = \\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)$.\n\n仅使用所提供的定义，计算 $t=1,2,3$ 时的前向传播以获得门和状态，并以此明确验证遗忘门 $f_t$ 是如何调节瞬态输入的保留的。作为最终结果，报告仅由 $t=2$ 时的瞬态输入对最终细胞状态 $c_3$ 产生的标量贡献（即，通过 LSTM 动态，可归因于 $x_2$ 的那部分 $c_3$）。请提供精确值，无需四舍五入。",
            "solution": "问题要求计算一个一维长短期记忆（LSTM）网络在 $3$ 个时间步上的前向传播，并计算对最终细胞状态的特定贡献。这个问题是良构的，所有必要的参数和初始条件都已提供。\n\n首先，我们将给定的标量参数代入通用的 LSTM 方程。激活前的值函数为：\n$z_{i,t} = W_{xi}\\,x_t + W_{hi}\\,h_{t-1} + b_i = (1)x_t + (0)h_{t-1} + 0 = x_t$\n$z_{f,t} = W_{xf}\\,x_t + W_{hf}\\,h_{t-1} + b_f = \\left(-\\frac{\\ln(2)}{\\ln(3)}\\right)x_t + (0)h_{t-1} + \\ln(4) = -\\frac{\\ln(2)}{\\ln(3)}x_t + 2\\ln(2)$\n$z_{o,t} = W_{xo}\\,x_t + W_{ho}\\,h_{t-1} + b_o = (0)x_t + (0)h_{t-1} + \\ln(2) = \\ln(2)$\n$z_{g,t} = W_{xg}\\,x_t + W_{hg}\\,h_{t-1} + b_g = (0)x_t + (0)h_{t-1} + \\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)$\n\n门和候选细胞更新如下：\n$i_t = \\sigma(z_{i,t}) = \\sigma(x_t)$\n$f_t = \\sigma(z_{f,t}) = \\sigma\\left(-\\frac{\\ln(2)}{\\ln(3)}x_t + 2\\ln(2)\\right)$\n$o_t = \\sigma(z_{o,t}) = \\sigma(\\ln(2))$\n$g_t = \\tanh(z_{g,t}) = \\tanh\\left(\\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)\\right)$\n\n输出门 $o_t$ 和候选细胞 $g_t$ 在所有时间步上都是恒定的。我们来计算它们的值。Sigmoid 函数是 $\\sigma(u) = (1+\\exp(-u))^{-1}$。\n$o_t = \\sigma(\\ln(2)) = \\frac{1}{1+\\exp(-\\ln(2))} = \\frac{1}{1+\\frac{1}{2}} = \\frac{1}{\\frac{3}{2}} = \\frac{2}{3}$。\n$g_t = \\tanh\\left(\\operatorname{arctanh}\\!\\left(\\frac{1}{2}\\right)\\right) = \\frac{1}{2}$。\n\n细胞状态和隐藏状态的更新是：\n$c_t = f_t c_{t-1} + i_t g_t = f_t c_{t-1} + \\frac{1}{2} i_t$\n$h_t = o_t \\tanh(c_t) = \\frac{2}{3} \\tanh(c_t)$\n\n初始条件为 $c_0 = 0$ 和 $h_0 = 0$。输入序列为 $x_1 = 0$, $x_2 = \\ln(3)$ 和 $x_3 = 0$。我们进行前向传播。\n\n**时间步 $t=1$:**\n输入：$x_1 = 0$。\n输入门和遗忘门的激活前的值为：\n$z_{i,1} = x_1 = 0$\n$z_{f,1} = -\\frac{\\ln(2)}{\\ln(3)}(0) + 2\\ln(2) = 2\\ln(2) = \\ln(4)$\n门激活值为：\n$i_1 = \\sigma(0) = \\frac{1}{1+\\exp(0)} = \\frac{1}{2}$\n$f_1 = \\sigma(\\ln(4)) = \\frac{1}{1+\\exp(-\\ln(4))} = \\frac{1}{1+\\frac{1}{4}} = \\frac{4}{5}$\n细胞和隐藏状态更新为：\n$c_1 = f_1 c_0 + i_1 g_1 = \\left(\\frac{4}{5}\\right)(0) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4}$\n$h_1 = \\frac{2}{3}\\tanh(c_1) = \\frac{2}{3}\\tanh\\left(\\frac{1}{4}\\right)$\n\n**时间步 $t=2$:**\n输入：$x_2 = \\ln(3)$。这是指定的瞬态输入。\n输入门和遗忘门的激活前的值为：\n$z_{i,2} = x_2 = \\ln(3)$\n$z_{f,2} = -\\frac{\\ln(2)}{\\ln(3)} (\\ln(3)) + 2\\ln(2) = -\\ln(2) + 2\\ln(2) = \\ln(2)$\n门激活值为：\n$i_2 = \\sigma(\\ln(3)) = \\frac{1}{1+\\exp(-\\ln(3))} = \\frac{1}{1+\\frac{1}{3}} = \\frac{3}{4}$\n$f_2 = \\sigma(\\ln(2)) = \\frac{1}{1+\\exp(-\\ln(2))} = \\frac{1}{1+\\frac{1}{2}} = \\frac{2}{3}$\n细胞状态更新为：\n$c_2 = f_2 c_1 + i_2 g_2 = \\left(\\frac{2}{3}\\right)\\left(\\frac{1}{4}\\right) + \\left(\\frac{3}{4}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{6} + \\frac{3}{8} = \\frac{4}{24} + \\frac{9}{24} = \\frac{13}{24}$\n隐藏状态为 $h_2 = \\frac{2}{3}\\tanh(c_2) = \\frac{2}{3}\\tanh\\left(\\frac{13}{24}\\right)$。\n\n**时间步 $t=3$:**\n输入：$x_3 = 0$。\n输入门和遗忘门的激活前的值与 $t=1$ 时相同：\n$z_{i,3} = x_3 = 0 \\implies i_3 = \\frac{1}{2}$\n$z_{f,3} = \\ln(4) \\implies f_3 = \\frac{4}{5}$\n最终细胞状态 $c_3$ 为：\n$c_3 = f_3 c_2 + i_3 g_3 = \\left(\\frac{4}{5}\\right)\\left(\\frac{13}{24}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{13}{5 \\cdot 6} + \\frac{1}{4} = \\frac{13}{30} + \\frac{1}{4} = \\frac{26}{60} + \\frac{15}{60} = \\frac{41}{60}$\n最终隐藏状态为 $h_3 = \\frac{2}{3}\\tanh(c_3) = \\frac{2}{3}\\tanh\\left(\\frac{41}{60}\\right)$。\n\n问题要求计算来自 $t=2$ 时瞬态输入对最终细胞状态 $c_3$ 的贡献。为了确定这一点，我们可以将 $c_3$ 完全用每一步的输入来表示。\n$c_3 = f_3 c_2 + i_3 g_3$\n代入 $c_2 = f_2 c_1 + i_2 g_2$:\n$c_3 = f_3 (f_2 c_1 + i_2 g_2) + i_3 g_3 = f_3 f_2 c_1 + f_3 i_2 g_2 + i_3 g_3$\n代入 $c_1 = f_1 c_0 + i_1 g_1$ 并使用 $c_0=0$:\n$c_3 = f_3 f_2 (i_1 g_1) + f_3 i_2 g_2 + i_3 g_3$\n\n这个表达式将 $c_3$ 分解为来自每个时间步输入的贡献之和：\n- 来自 $x_1$ 的贡献：$f_3 f_2 i_1 g_1$\n- 来自 $x_2$ 的贡献：$f_3 i_2 g_2$\n- 来自 $x_3$ 的贡献：$i_3 g_3$\n\n项 $i_2 g_2$ 表示在 $t=2$ 时由于输入 $x_2$ 而添加到细胞状态的新信息。下一步的遗忘门 $f_3$ 调节了这些信息保留到状态 $c_3$ 的程度。因此，可归因于 $x_2$ 的那部分 $c_3$ 是 $f_3 i_2 g_2$。\n\n我们使用我们找到的门值来计算该项的值：\n$f_3 = \\frac{4}{5}$\n$i_2 = \\frac{3}{4}$\n$g_2 = \\frac{1}{2}$\n\n来自 $x_2$ 对 $c_3$ 的贡献 $= f_3 \\, i_2 \\, g_2 = \\left(\\frac{4}{5}\\right) \\left(\\frac{3}{4}\\right) \\left(\\frac{1}{2}\\right) = \\frac{12}{40} = \\frac{3}{10}$。\n\n这个计算明确地展示了遗忘门 $f_3 = \\frac{4}{5}$ 如何作用于因瞬态输入而写入细胞的信息 $i_2 g_2 = \\frac{3}{8}$，在下一个时间步中保留了其中的 $80\\%$。瞬态输入 $x_2 = \\ln(3)$ 导致了比输入为零时（$f_1 = f_3 = \\frac{4}{5}$）更小的遗忘门值 $f_2 = \\frac{2}{3}$，这使得网络在重要事件发生时“忘记”了更多其过去的状态 $c_1$。\n\n所要求的最终结果是来自 $t=2$ 时输入对 $c_3$ 的标量贡献。",
            "answer": "$$\\boxed{\\frac{3}{10}}$$"
        },
        {
            "introduction": "在神经科学数据分析中，我们处理的神经活动序列（如不同试次的记录）长度往往各不相同。为了高效地利用GPU进行批量训练，我们需要将这些可变长度的序列填充成统一的长度，并使用掩码（masking）技术来忽略填充部分的影响。本练习要求你为一个简单的RNN实现完整的带掩码前向传播和反向传播过程，让你掌握如何正确计算掩码损失以及如何确保梯度不会从填充的时间步中“泄漏”，这是构建稳健的RNN训练流程的核心技能。",
            "id": "4189567",
            "problem": "您将为一个单层循环神经网络（RNN）实现一个完整的前向和反向传播过程，该网络具有加性输入和循环连接，使用掩码处理可变长度序列的小批量数据。目标是构建掩码序列，计算掩码均方误差损失，使用链式法则推导并实现梯度，并数值验证梯度不会从序列末尾的填充时间步泄漏。此外，您还将测试并演示序列中途出现的掩码时间步的独特行为（由于因果传播，这些时间步可能仍会携带梯度），并处理序列中有效时间步为零的边界情况。\n\n起点定义和规则：\n- 使用以下前向循环公式，用于一个具有双曲正切激活函数的单层循环神经网络（RNN）。对于批次索引 $b$ 和时间索引 $t$，隐藏状态为\n$$\n\\mathbf{h}_{b,t} = \\tanh\\left(\\mathbf{a}_{b,t}\\right), \\quad \\mathbf{a}_{b,t} = \\mathbf{x}_{b,t}\\mathbf{W}_{xh} + \\mathbf{h}_{b,t-1}\\mathbf{W}_{hh} + \\mathbf{b}_h,\n$$\n初始状态为 $\\mathbf{h}_{b,-1} = \\mathbf{0}$。输出为\n$$\n\\mathbf{y}_{b,t} = \\mathbf{h}_{b,t}\\mathbf{W}_{hy} + \\mathbf{b}_y.\n$$\n- 设每个时间步每个序列的损失为半平方误差\n$$\n\\ell_{b,t} = \\frac{1}{2}\\left\\|\\mathbf{y}_{b,t} - \\mathbf{r}_{b,t}\\right\\|_2^2,\n$$\n并设 $\\mathbf{m}_{b,t} \\in \\{0,1\\}$ 为一个二元掩码，用于指示有效时间步（$1$）和填充时间步（$0$）。掩码损失是所有有效时间步的平均值，\n$$\nL = \\frac{\\sum_{b=1}^{B}\\sum_{t=0}^{T-1} \\mathbf{m}_{b,t} \\, \\ell_{b,t}}{\\sum_{b=1}^{B}\\sum_{t=0}^{T-1} \\mathbf{m}_{b,t}},\n$$\n约定如果分母等于 $0$，则 $L = 0$ 且所有梯度为零。\n- 使用标准的微积分链式法则进行微分，以及双曲正切的导数，即\n$$\n\\frac{\\partial}{\\partial \\mathbf{a}} \\tanh(\\mathbf{a}) = \\mathbf{1} - \\tanh(\\mathbf{a}) \\odot \\tanh(\\mathbf{a}),\n$$\n其中 $\\odot$ 表示逐元素乘法。\n- 所有计算都必须以实值算术执行。不涉及物理单位或角度，所有量均为无量纲。\n\n您的程序必须：\n- 构建掩码小批量序列并计算掩码损失 $L$。\n- 使用带掩码的随时间反向传播算法，计算所有参数 $\\mathbf{W}_{xh}$、$\\mathbf{W}_{hh}$、$\\mathbf{W}_{hy}$、$\\mathbf{b}_h$ 和 $\\mathbf{b}_y$ 的精确梯度。\n- 数值验证对于尾部填充的时间步（序列最后一个有效时间步之后掩码中的零值），在这些时间步的直接输出梯度和隐藏状态梯度在指定的绝对容差范围内均为零。\n- 证明对序列中途的时间步进行掩码会使这些时间步的直接输出梯度为零，但由于未来有效时间步的因果传播，允许这些时间步的隐藏状态梯度为非零。\n- 正确处理批处理中所有时间步都被掩码（零分母）的边界情况，产生零损失和零梯度。\n\n模型维度和固定参数：\n- 输入维度 $d_x = 2$，隐藏维度 $d_h = 3$，以及输出维度 $d_y = 2$。\n- 参数矩阵和向量是固定的，由以下给出\n$$\n\\mathbf{W}_{xh} = \\begin{bmatrix} 0.1  -0.2  0.3 \\\\ 0.05  0.4  -0.1 \\end{bmatrix}, \\quad\n\\mathbf{W}_{hh} = \\begin{bmatrix} 0.2  0.0  -0.1 \\\\ 0.1  0.3  0.0 \\\\ -0.2  0.05  0.25 \\end{bmatrix},\n$$\n$$\n\\mathbf{b}_h = \\begin{bmatrix} 0.0  0.0  0.0 \\end{bmatrix}, \\quad\n\\mathbf{W}_{hy} = \\begin{bmatrix} 0.3  -0.2 \\\\ -0.1  0.4 \\\\ 0.2  0.1 \\end{bmatrix}, \\quad\n\\mathbf{b}_y = \\begin{bmatrix} 0.0  0.0 \\end{bmatrix}.\n$$\n- 对于所有测试用例，输入和目标均按如下方式确定性地构建。对于批大小 $B$ 和序列长度 $T$，为每个 $b \\in \\{0, \\ldots, B-1\\}$ 和每个 $t \\in \\{0, \\ldots, T-1\\}$ 定义：\n$$\n\\mathbf{x}_{b,t} = 0.1 \\cdot \\begin{bmatrix} b+1 \\\\ t+1 \\end{bmatrix}, \\quad\n\\mathbf{r}_{b,t} = \\begin{bmatrix} 0.05\\,(b+1)\\,(t+1) \\\\ -0.02\\,(t+1) \\end{bmatrix}.\n$$\n掩码根据下文为每个测试用例唯一指定。\n\n测试套件：\n- 测试用例 $1$（带尾部填充的正常路径）：\n  - 批大小 $B = 3$，序列长度 $T = 5$，每个序列的有效长度为 $\\left[5, 3, 4\\right]$。\n  - 当 $t  \\text{length}_b$ 时，掩码 $\\mathbf{m}_{b,t} = 1$，否则 $\\mathbf{m}_{b,t} = 0$。\n  - 输出：一个列表，包含掩码损失 $L$（浮点数）和一个布尔值，该布尔值指示所有尾部填充时间步的梯度在绝对容差 $\\varepsilon = 10^{-12}$ 内是否数值上为零。\n- 测试用例 $2$（包含一个零长度序列）：\n  - 批大小 $B = 2$，序列长度 $T = 4$，每个序列的有效长度为 $\\left[0, 4\\right]$。\n  - 掩码定义如上。第一个序列有零个有效时间步。\n  - 输出：一个列表，包含 $L$（浮点数）和一个布尔值，该布尔值指示所有源自零长度序列的梯度在 $\\varepsilon = 10^{-12}$ 内是否数值上为零。\n- 测试用例 $3$（所有序列均为零长度，零分母）：\n  - 批大小 $B = 2$，序列长度 $T = 3$，每个序列的有效长度为 $\\left[0, 0\\right]$。\n  - 掩码定义如上。\n  - 输出：一个列表，包含 $L$（浮点数，必须为 $0.0$）和一个布尔值，该布尔值指示所有参数梯度在 $\\varepsilon = 10^{-12}$ 内是否数值上为零。\n- 测试用例 $4$（混合序列中途掩码和尾部填充）：\n  - 批大小 $B = 2$，序列长度 $T = 5$，每个序列的有效长度为 $\\left[5, 3\\right]$。\n  - 对于第一个序列（$b = 0$），设置所有 $t$ 的 $\\mathbf{m}_{0,t} = 1$，除了 $\\mathbf{m}_{0,2} = 0$（被掩码的序列中途时间步）。\n  - 对于第二个序列（$b = 1$），使用尾部填充，当 $t \\in \\{0,1,2\\}$ 时 $\\mathbf{m}_{1,t} = 1$，否则 $\\mathbf{m}_{1,t} = 0$。\n  - 输出：一个列表，包含 $L$（浮点数），一个布尔值指示尾部填充时间步的梯度在 $\\varepsilon = 10^{-12}$ 内是否数值上为零，以及一个布尔值指示第一个序列中被掩码的中途时间步是否由于因果传播而携带非零的隐藏状态梯度，而其直接输出梯度为零。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例必须生成如上所述的列表，并且程序应将这些每个测试用例的列表聚合到一个单一的外部列表中。例如，输出必须看起来像 $[\\text{case1\\_list},\\text{case2\\_list},\\text{case3\\_list},\\text{case4\\_list}]$。\n\n所有对掩码时间步梯度的数值相等性检查都必须使用绝对容差 $\\varepsilon = 10^{-12}$，并且所有输出必须是布尔值、整数、浮点数或这些基本类型的列表。",
            "solution": "该问题要求为一个单层循环神经网络（RNN）实现并验证一个完整的带掩码的前向和反向传播过程，以处理可变长度的序列。解决方案涉及使用随时间反向传播（BPTT）推导梯度表达式，实现该算法，并在几个探测掩码行为的测试用例上验证其正确性。\n\n### 1. 模型和损失函数\n\nRNN 的动态由以下方程定义，其中 b 为批次索引，t 为时间索引：\n隐藏层的预激活：\n$$ \\mathbf{a}_{b,t} = \\mathbf{x}_{b,t}\\mathbf{W}_{xh} + \\mathbf{h}_{b,t-1}\\mathbf{W}_{hh} + \\mathbf{b}_h $$\n隐藏状态，初始状态为 $\\mathbf{h}_{b,-1} = \\mathbf{0}$：\n$$ \\mathbf{h}_{b,t} = \\tanh(\\mathbf{a}_{b,t}) $$\n网络输出：\n$$ \\mathbf{y}_{b,t} = \\mathbf{h}_{b,t}\\mathbf{W}_{hy} + \\mathbf{b}_y $$\n损失函数是掩码均方误差。每个时间步的损失为：\n$$ \\ell_{b,t} = \\frac{1}{2}\\left\\|\\mathbf{y}_{b,t} - \\mathbf{r}_{b,t}\\right\\|_2^2 $$\n总损失 $L$ 是对有效时间步的归一化总和，由二元掩码 $\\mathbf{m}_{b,t} \\in \\{0,1\\}$ 指示：\n$$ L = \\frac{\\sum_{b,t} \\mathbf{m}_{b,t} \\, \\ell_{b,t}}{\\sum_{b,t} \\mathbf{m}_{b,t}} = \\frac{1}{N_{valid}} \\sum_{b,t} \\mathbf{m}_{b,t} \\, \\ell_{b,t} $$\n其中 $N_{valid} = \\sum_{b,t} \\mathbf{m}_{b,t}$。如果 $N_{valid} = 0$，则 L=0 且所有梯度均为零。\n\n### 2. 梯度推导（随时间反向传播）\n\n为了训练网络，我们必须计算损失 $L$ 相对于所有参数：$\\mathbf{W}_{xh}$、$\\mathbf{W}_{hh}$、$\\mathbf{b}_h$、$\\mathbf{W}_{hy}$ 和 $\\mathbf{b}_y$ 的梯度。我们使用链式法则。\n\n$L$ 相对于输出 $\\mathbf{y}_{b,t}$ 的梯度是反向传播的起点。\n$$ \\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} = \\frac{\\partial L}{\\partial \\ell_{b,t}} \\frac{\\partial \\ell_{b,t}}{\\partial \\mathbf{y}_{b,t}} = \\frac{\\mathbf{m}_{b,t}}{N_{valid}} (\\mathbf{y}_{b,t} - \\mathbf{r}_{b,t}) $$\n我们将此梯度表示为 $\\mathbf{gY}_{b,t}$。掩码 $\\mathbf{m}_{b,t}$ 确保填充的时间步（$m_{b,t}=0$）对梯度没有任何贡献。\n\n#### 2.1. 输出层（$\\mathbf{W}_{hy}, \\mathbf{b}_y$）的梯度\n\n输出层参数的梯度是通过对所有时间步的贡献求和来计算的。\n$$ \\frac{\\partial L}{\\partial \\mathbf{W}_{hy}} = \\sum_{b,t} \\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{W}_{hy}} = \\sum_{b,t} \\mathbf{h}_{b,t}^T \\mathbf{gY}_{b,t} $$\n$$ \\frac{\\partial L}{\\partial \\mathbf{b}_y} = \\sum_{b,t} \\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{b}_y} = \\sum_{b,t} \\mathbf{gY}_{b,t} $$\n这些梯度可以在前向传播之后，一旦所有 $\\mathbf{h}_{b,t}$ 和 $\\mathbf{gY}_{b,t}$ 都已知时进行计算。\n\n#### 2.2. 循环层的梯度\n\n相对于隐藏状态 $\\mathbf{h}_{b,t}$ 的梯度有两个分量：一个来自同一时间步的输出 $\\mathbf{y}_{b,t}$，另一个是从后续隐藏状态 $\\mathbf{h}_{b,t+1}$ 传播而来的。这导致了一个反向循环。\n\n设 $\\mathbf{gH}_{b,t} = \\frac{\\partial L}{\\partial \\mathbf{h}_{b,t}}$ 和 $\\mathbf{gA}_{b,t} = \\frac{\\partial L}{\\partial \\mathbf{a}_{b,t}}$。$\\mathbf{gH}_{b,t}$ 的循环关系是：\n$$ \\mathbf{gH}_{b,t} = \\underbrace{\\frac{\\partial L}{\\partial \\mathbf{y}_{b,t}} \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{h}_{b,t}}}_{\\text{来自输出}} + \\underbrace{\\frac{\\partial L}{\\partial \\mathbf{a}_{b,t+1}} \\frac{\\partial \\mathbf{a}_{b,t+1}}{\\partial \\mathbf{h}_{b,t}}}_{\\text{来自未来}} $$\n计算各项：\n$$ \\frac{\\partial \\mathbf{y}_{b,t}}{\\partial \\mathbf{h}_{b,t}} = \\mathbf{W}_{hy}^T \\quad \\text{和} \\quad \\frac{\\partial \\mathbf{a}_{b,t+1}}{\\partial \\mathbf{h}_{b,t}} = \\mathbf{W}_{hh} $$\n所以循环关系变为：\n$$ \\mathbf{gH}_{b,t} = \\mathbf{gY}_{b,t} \\mathbf{W}_{hy}^T + \\mathbf{gA}_{b,t+1} \\mathbf{W}_{hh} $$\n我们还需要相对于预激活 $\\mathbf{a}_{b,t}$ 的梯度，这通过链式法则和 $\\tanh$ 的导数得到：\n$$ \\mathbf{gA}_{b,t} = \\frac{\\partial L}{\\partial \\mathbf{h}_{b,t}} \\frac{\\partial \\mathbf{h}_{b,t}}{\\partial \\mathbf{a}_{b,t}} = \\mathbf{gH}_{b,t} \\odot (\\mathbf{1} - \\mathbf{h}_{b,t} \\odot \\mathbf{h}_{b,t}) $$\n其中 $\\odot$ 是逐元素乘法。\n\nBPTT 算法通过从 $t=T-1$ 到 $t=0$ 的时间逆向迭代来进行。我们将从未来流入的梯度初始化为零，即 $\\mathbf{gA}_{b,T} = \\mathbf{0}$。\n\n对于 $t = T-1, \\dots, 0$：\n1. 计算 $\\mathbf{gH}_{b,t} = \\mathbf{gY}_{b,t} \\mathbf{W}_{hy}^T + \\mathbf{gA}_{b,t+1} \\mathbf{W}_{hh}$。（其中 $\\mathbf{gA}_{b,T}=\\mathbf{0}$）\n2. 计算 $\\mathbf{gA}_{b,t} = \\mathbf{gH}_{b,t} \\odot (\\mathbf{1} - \\mathbf{h}_{b,t}^2)$。\n3. 累积循环层参数的梯度：\n   $$ \\frac{\\partial L}{\\partial \\mathbf{W}_{xh}} += \\sum_{b} \\mathbf{x}_{b,t}^T \\mathbf{gA}_{b,t} $$\n   $$ \\frac{\\partial L}{\\partial \\mathbf{W}_{hh}} += \\sum_{b} \\mathbf{h}_{b,t-1}^T \\mathbf{gA}_{b,t} \\quad (\\text{其中 } \\mathbf{h}_{b,-1} = \\mathbf{0})$$\n   $$ \\frac{\\partial L}{\\partial \\mathbf{b}_h} += \\sum_{b} \\mathbf{gA}_{b,t} $$\n\n### 3. 实现与验证策略\n\n总体算法如下：\n1.  **前向传播**：从 $t=0$ 迭代到 $T-1$，计算并存储所有的 $\\mathbf{h}_{b,t}$ 和 $\\mathbf{y}_{b,t}$。\n2.  **损失计算**：计算有效时间步的总数 $N_{valid}$ 和最终损失 $L$。如果 $N_{valid}=0$，则设置 $L=0$ 且所有梯度为零。\n3.  **反向传播**：\n    a. 计算初始输出梯度 $\\mathbf{gY}_{b,t}$，应用掩码。\n    b. 计算并存储输出层参数 $\\mathbf{W}_{hy}$ 和 $\\mathbf{b}_y$ 的梯度。\n    c. 初始化一个总的隐藏状态梯度张量 $\\mathbf{gH}$ 为零。\n    d. 从 $t=T-1$ 向下迭代到 $0$，使用循环关系计算 $\\mathbf{gH}_{:,t,:}$。这捕获了关于每个隐藏状态的完整梯度。\n    e. 根据最终的 $\\mathbf{gH}$ 和存储的激活值计算预激活梯度 $\\mathbf{gA}$。\n    f. 通过在整个 $\\mathbf{gA}$ 上进行张量缩并（例如 `einsum`）来计算循环层参数的梯度（$\\mathbf{W}_{xh}$、$\\mathbf{W}_{hh}$、$\\mathbf{b}_h$）。\n\n#### 掩码行为验证：\n-   **尾部填充**：对于长度为 $L_b  T$ 的序列 $b$，所有时间步 $t \\ge L_b$ 都是填充的。我们必须验证对于这些时间步，$\\mathbf{gY}_{b,t}$ 和 $\\mathbf{gH}_{b,t}$ 在数值上为零。这表明梯度不会“泄漏”到填充区域。\n-   **序列中途掩码**：即使一个时间步 $t$ 被有效时间步包围，它也可能被掩码。在这种情况下，直接输出梯度 $\\mathbf{gY}_{b,t}$ 将为零。然而，隐藏状态梯度 $\\mathbf{gH}_{b,t}$ 可能为非零，因为梯度会从未来的有效时间步（$t+1, t+2, \\dots$）反向流回。我们必须验证这种独特的行为。\n-   **零长度序列**：对于长度为 0 的序列，掩码全为零。这意味着所有相关的梯度（$\\mathbf{gY}_{b,:}$、$\\mathbf{gH}_{b,:}$）都为零。如果一个批次中的所有序列长度都为零，$N_{valid}=0$，那么所有最终的参数梯度都必须为零。\n\n这些原则在提供的 Python 代码中得以实现，该代码系统地执行每个测试用例并执行所需的数值检查。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the RNN forward/backward pass.\n    \"\"\"\n\n    # Fixed parameters\n    W_xh = np.array([[0.1, -0.2, 0.3], [0.05, 0.4, -0.1]])\n    W_hh = np.array([[0.2, 0.0, -0.1], [0.1, 0.3, 0.0], [-0.2, 0.05, 0.25]])\n    b_h = np.array([0.0, 0.0, 0.0])\n    W_hy = np.array([[0.3, -0.2], [-0.1, 0.4], [0.2, 0.1]])\n    b_y = np.array([0.0, 0.0])\n    params = (W_xh, W_hh, b_h, W_hy, b_y)\n    \n    tol = 1e-12\n\n    test_cases = [\n        {'B': 3, 'T': 5, 'lengths': [5, 3, 4], 'id': 1},\n        {'B': 2, 'T': 4, 'lengths': [0, 4], 'id': 2},\n        {'B': 2, 'T': 3, 'lengths': [0, 0], 'id': 3},\n        {'B': 2, 'T': 5, 'lengths': [5, 3], 'id': 4}\n    ]\n\n    results = []\n    for case in test_cases:\n        B, T, lengths = case['B'], case['T'], case['lengths']\n        \n        # --- Data and Mask Generation ---\n        X = np.zeros((B, T, W_xh.shape[0]))\n        R = np.zeros((B, T, W_hy.shape[1]))\n        for b in range(B):\n            for t in range(T):\n                X[b, t] = 0.1 * np.array([b + 1, t + 1])\n                R[b, t] = np.array([0.05 * (b + 1) * (t + 1), -0.02 * (t + 1)])\n\n        M = np.zeros((B, T))\n        if case['id'] == 4:\n            # Special mask for TC4\n            M[0] = np.array([1, 1, 0, 1, 1])\n            M[1, :lengths[1]] = 1\n        else:\n            for b in range(B):\n                M[b, :lengths[b]] = 1\n\n        # --- Run RNN pass ---\n        loss, grads, aux_data = rnn_forward_backward(X, R, M, params)\n        gW_xh, gW_hh, gb_h, gW_hy, gb_y = grads\n        gY, gH = aux_data['gY'], aux_data['gH']\n\n        case_result = []\n        if case['id'] == 1:\n            # TC1: Check tail padding gradients\n            case_result.append(loss)\n            tail_grads_are_zero = True\n            for b in range(B):\n                len_b = lengths[b]\n                if len_b  T:\n                    # Check gY (direct output grad) and gH (total hidden state grad)\n                    gY_tail = gY[b, len_b:]\n                    gH_tail = gH[b, len_b:]\n                    if np.any(np.abs(gY_tail) > tol) or np.any(np.abs(gH_tail) > tol):\n                        tail_grads_are_zero = False\n                        break\n            case_result.append(tail_grads_are_zero)\n\n        elif case['id'] == 2:\n            # TC2: Check zero-length sequence gradients\n            case_result.append(loss)\n            zero_len_grads_are_zero = True\n            # Sequence 0 has length 0\n            if np.any(np.abs(gY[0]) > tol) or np.any(np.abs(gH[0]) > tol):\n                zero_len_grads_are_zero = False\n            case_result.append(zero_len_grads_are_zero)\n\n        elif case['id'] == 3:\n            # TC3: Check zero loss and zero gradients for fully masked batch\n            case_result.append(loss) # Should be 0.0\n            all_grads_zero = True\n            for g in grads:\n                if np.any(np.abs(g) > tol):\n                    all_grads_zero = False\n                    break\n            case_result.append(all_grads_zero)\n\n        elif case['id'] == 4:\n            # TC4: Check tail padding and mid-sequence mask\n            case_result.append(loss)\n            \n            # 1. Check tail padding on sequence 1 (length 3)\n            tail_grads_are_zero = True\n            len_b1 = lengths[1]\n            if len_b1  T:\n                gY_tail = gY[1, len_b1:]\n                gH_tail = gH[1, len_b1:]\n                if np.any(np.abs(gY_tail) > tol) or np.any(np.abs(gH_tail) > tol):\n                    tail_grads_are_zero = False\n            case_result.append(tail_grads_are_zero)\n\n            # 2. Check mid-sequence mask on sequence 0 at t=2\n            mid_mask_t = 2\n            gY_mid_is_zero = np.all(np.abs(gY[0, mid_mask_t])  tol)\n            gH_mid_is_nonzero = np.any(np.abs(gH[0, mid_mask_t]) > tol)\n            mid_mask_behavior_correct = gY_mid_is_zero and gH_mid_is_nonzero\n            case_result.append(mid_mask_behavior_correct)\n            \n        results.append(case_result)\n\n    # Final print statement\n    print(str(results).replace(\" \", \"\"))\n\ndef rnn_forward_backward(X, R, M, params):\n    \"\"\"\n    Performs a full forward and backward pass for the specified RNN.\n    \"\"\"\n    W_xh, W_hh, b_h, W_hy, b_y = params\n    B, T, d_x = X.shape\n    d_h = W_hh.shape[0]\n    d_y = W_hy.shape[1]\n\n    # --- Forward Pass ---\n    H_padded = np.zeros((B, T + 1, d_h))\n    H = H_padded[:, 1:, :]  # H is a view into H_padded\n    Y = np.zeros((B, T, d_y))\n\n    for t in range(T):\n        h_prev = H_padded[:, t, :]\n        A_t = X[:, t, :] @ W_xh + h_prev @ W_hh + b_h\n        H[:, t, :] = np.tanh(A_t)\n        Y[:, t, :] = H[:, t, :] @ W_hy + b_y\n\n    # --- Loss Calculation ---\n    num_valid_steps = np.sum(M)\n    if num_valid_steps == 0:\n        loss = 0.0\n        grads = [np.zeros_like(p) for p in params]\n        aux_data = {'gY': np.zeros_like(Y), 'gH': np.zeros_like(H)}\n        return loss, grads, aux_data\n\n    errors = Y - R\n    loss_per_step = 0.5 * np.sum(errors**2, axis=2)\n    masked_loss_sum = np.sum(loss_per_step * M)\n    loss = masked_loss_sum / num_valid_steps\n\n    # --- Backward Pass ---\n    # Initialize parameter gradients\n    gW_xh, gW_hh, gb_h, gW_hy, gb_y = [np.zeros_like(p) for p in params]\n\n    # Gradient of loss w.r.t. network outputs Y\n    gY = errors / num_valid_steps\n    gY *= M[:, :, np.newaxis]  # Apply mask\n\n    # Gradients for output layer\n    gW_hy = np.einsum('bth,bty->hy', H, gY)\n    gb_y = np.sum(gY, axis=(0, 1))\n\n    # Gradients for recurrent layer (BPTT)\n    gH = np.zeros_like(H)\n    gh_carry = np.zeros((B, d_h))\n    gH_from_Y = gY @ W_hy.T\n    \n    # First pass: compute total gradient w.r.t hidden states H\n    for t in reversed(range(T)):\n        gh_total_t = gH_from_Y[:, t, :] + gh_carry\n        gH[:, t, :] = gh_total_t\n        ga_t = gh_total_t * (1 - H[:, t, :]**2)\n        gh_carry = ga_t @ W_hh\n        \n    # Second pass: compute parameter gradients from gH\n    gA = gH * (1 - H**2)\n    gW_xh = np.einsum('btx,bth->xh', X, gA)\n    gW_hh = np.einsum('bth,btH->hH', H_padded[:, :-1, :], gA)\n    gb_h = np.sum(gA, axis=(0, 1))\n\n    grads = (gW_xh, gW_hh, gb_h, gW_hy, gb_y)\n    aux_data = {'gY': gY, 'gH': gH}\n\n    return loss, grads, aux_data\n\nsolve()\n```"
        },
        {
            "introduction": "训练好的RNN不仅是一个预测工具，更是一个可以揭示神经计算原理的动力系统。本练习将引导你使用动力系统理论来分析一个二维连续时间RNN的局部动态特性，通过计算系统在不动点处的雅可比矩阵 $(\\mathbf{J})$ 及其特征值与特征向量，你将能够识别和分类决定系统行为的稳定与不稳定流形。这项实践将机器学习模型与计算神经科学的分析方法紧密联系起来，为你提供一种强有力的工具来解释RNN学到的计算机制。",
            "id": "4189514",
            "problem": "您将获得一个由以下动力系统定义的连续时间二维 ($2$D) 基于速率的循环神经网络 (RNN)：\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x}) + \\mathbf{b},\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^2$ 是状态向量，$\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ 是循环连接矩阵，$\\mathbf{b} \\in \\mathbb{R}^2$ 是一个恒定输入偏置，$\\boldsymbol{\\phi}(\\mathbf{x})$ 是一个逐元素非线性函数，其分量为 $\\phi(x_i) = \\tanh(x_i)$ (对于 $i \\in \\{1,2\\}$)。假设 $\\mathbf{b} = \\mathbf{0}$，因此 $\\mathbf{x}^\\star = \\mathbf{0}$ 始终是一个不动点。该向量场在不动点处的雅可比矩阵为\n$$\n\\mathbf{J} = \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star} = -\\mathbf{I} + \\mathbf{W}\\,\\mathrm{diag}\\left(\\phi'(x_1^\\star), \\phi'(x_2^\\star)\\right),\n$$\n并且由于 $\\phi'(0) = 1$，该式可简化为 $\\mathbf{J} = -\\mathbf{I} + \\mathbf{W}$。\n\n从自治动力系统的不动点和线性化的基本定义出发，并利用一个公认的事实，即双曲不动点附近的局部行为由雅可比矩阵的特征结构决定，您的任务是：\n- 计算 $\\mathbf{J}$ 在 $\\mathbf{x}^\\star$ 处的特征值和特征向量。\n- 对局部不变流形进行分类：稳定流形的维数等于 $\\mathbf{J}$ 具有严格负实部特征值的数量，不稳定流形的维数等于具有严格正实部特征值的数量，中心流形的维数等于实部为零的特征值的数量。\n- 利用 $\\mathbf{J}$ 的特征向量构建用于轨迹模拟的初始化方向。对于实特征向量，使用该特征向量及其负向量。对于二维中的复共轭特征对，使用一个复特征向量的实部和虚部形成一个实基，并包括每个实方向的正负两个方向。\n- 对于上面构建的每个方向 $\\mathbf{d}$，在有限时间范围 $T$ 内，使用一个小的位移幅度 $\\varepsilon$，在 $\\mathbf{x}(0) = \\mathbf{x}^\\star + \\varepsilon\\,\\mathbf{d}$ 处初始化并模拟前向轨迹。使用上面给出的连续时间动力学，其中 $\\boldsymbol{\\phi}(\\mathbf{x}) = \\tanh(\\mathbf{x})$。\n\n对所有测试用例使用以下固定的模拟参数：\n- 位移幅度 $\\varepsilon = 10^{-3}$。\n- 时间范围 $T = 3$ (时间单位是一个抽象的建模单位；您无需输出任何物理单位)。\n- 谱分类容差 $\\delta = 10^{-8}$，应用于特征值的实部：实部 $ -\\delta$ 为稳定，$> \\delta$ 为不稳定，以及 $|\\cdot| \\le \\delta$ 为中心。\n\n对于每个测试用例，计算：\n1. 整数 $n_s$，稳定流形的维数。\n2. 整数 $n_u$，不稳定流形的维数。\n3. 整数 $n_c$，中心流形的维数。\n4. 一个布尔值 $q$，表示在时间 $T$ 内，所有沿着与稳定特征值相关方向模拟的轨迹到 $\\mathbf{x}^\\star$ 的欧几里得距离是否严格减小，以及所有沿着与不稳定特征值相关方向模拟的轨迹到 $\\mathbf{x}^\\star$ 的欧几里得距离是否严格增大。如果没有稳定或不稳定方向，则将相应条件视为空真 (vacuously true)。\n\n您的程序必须实现上述内容，并生成一行输出，其中包含所有测试用例的结果，格式为一个由列表组成的逗号分隔列表，每个内部列表的形式为 $[n_s, n_u, n_c, q]$，并用方括号括起来。例如，输出格式应如下所示\n$$\n[[n_{s,1}, n_{u,1}, n_{c,1}, q_1],[n_{s,2}, n_{u,2}, n_{c,2}, q_2],\\dots]\n$$\n不含任何附加文本。\n\n测试套件：\n使用以下五个循环连接矩阵 $\\mathbf{W}$，每个矩阵都隐式地与 $\\mathbf{b} = \\mathbf{0}$ 和 $\\boldsymbol{\\phi}(x) = \\tanh(x)$ 配对：\n- 情况 1 (鞍点): $\\mathbf{W} = \\begin{bmatrix} 1.5  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$。\n- 情况 2 (稳定节点): $\\mathbf{W} = \\begin{bmatrix} 0.2  0.0 \\\\ 0.0  0.3 \\end{bmatrix}$。\n- 情况 3 (不稳定节点): $\\mathbf{W} = \\begin{bmatrix} 1.3  0.0 \\\\ 0.0  1.4 \\end{bmatrix}$。\n- 情况 4 (稳定螺旋): $\\mathbf{W} = \\begin{bmatrix} 0.5  1.8 \\\\ -1.8  0.5 \\end{bmatrix}$。\n- 情况 5 (中心，纯虚线性化): $\\mathbf{W} = \\begin{bmatrix} 1.0  1.0 \\\\ -1.0  1.0 \\end{bmatrix}$。\n\n您的程序必须是自包含的，不得读取任何输入，并且必须将上述过程应用于提供的测试套件。最终输出必须严格按照指定格式打印在单行上。",
            "solution": "该问题要求分析一个二维连续时间循环神经网络 (RNN) 在不动点附近的局部动力学。该分析涉及线性化、所得雅可比矩阵的特征分解、不变流形的分类以及沿特征方向的动力学数值验证。\n\n该动力系统由以下常微分方程 (ODE) 给出：\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x}) + \\mathbf{b}\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^2$ 是状态，$\\mathbf{W} \\in \\mathbb{R}^{2 \\times 2}$ 是连接矩阵，$\\mathbf{b} \\in \\mathbb{R}^2$ 是一个偏置，$\\boldsymbol{\\phi}$ 是逐元素的双曲正切激活函数，$\\phi(x_i) = \\tanh(x_i)$。\n\n首先，我们基于动力学的基本原理建立分析框架。\n\n**1. 不动点与线性化**\n\n系统的一个不动点 $\\mathbf{x}^\\star$ 是动力学停止的状态，即 $\\frac{d\\mathbf{x}}{dt} = \\mathbf{f}(\\mathbf{x}^\\star) = \\mathbf{0}$。我们已知偏置 $\\mathbf{b} = \\mathbf{0}$。我们可以验证原点 $\\mathbf{x}^\\star = \\mathbf{0}$ 是一个不动点：\n$$\n\\mathbf{f}(\\mathbf{0}) = -\\mathbf{0} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{0}) = \\mathbf{0} + \\mathbf{W}\\,\\mathbf{0} = \\mathbf{0}\n$$\n因为 $\\tanh(0) = 0$。\n\n系统在不动点附近的行为可以用一个线性系统来近似。这可以通过在 $\\mathbf{x}^\\star$ 附近对 $\\mathbf{f}(\\mathbf{x})$ 进行一阶泰勒展开来实现。设 $\\mathbf{x} = \\mathbf{x}^\\star + \\delta\\mathbf{x}$，其中 $\\delta\\mathbf{x}$ 是一个线性扰动。\n$$\n\\frac{d(\\mathbf{x}^\\star + \\delta\\mathbf{x})}{dt} = \\frac{d\\delta\\mathbf{x}}{dt} \\approx \\mathbf{f}(\\mathbf{x}^\\star) + \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star} \\delta\\mathbf{x}\n$$\n由于 $\\mathbf{f}(\\mathbf{x}^\\star) = \\mathbf{0}$，线性化动力学由雅可比矩阵 $\\mathbf{J} = \\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\mathbf{x}^\\star}$ 控制：\n$$\n\\frac{d\\delta\\mathbf{x}}{dt} \\approx \\mathbf{J}\\,\\delta\\mathbf{x}\n$$\n$\\mathbf{f}(\\mathbf{x}) = -\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x})$ 的雅可比矩阵是：\n$$\n\\mathbf{J} = \\frac{\\partial}{\\partial \\mathbf{x}} (-\\mathbf{x} + \\mathbf{W}\\,\\boldsymbol{\\phi}(\\mathbf{x})) = -\\mathbf{I} + \\mathbf{W}\\,\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}}\n$$\n其中 $\\mathbf{I}$ 是 $2 \\times 2$ 的单位矩阵，$\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}}$ 是逐元素激活函数的雅可比矩阵。对于 $\\phi(x_i) = \\tanh(x_i)$，其导数为 $\\phi'(x_i) = 1 - \\tanh^2(x_i) = \\mathrm{sech}^2(x_i)$。$\\boldsymbol{\\phi}$ 的雅可比矩阵是一个对角矩阵：\n$$\n\\frac{\\partial\\boldsymbol{\\phi}(\\mathbf{x})}{\\partial \\mathbf{x}} = \\mathrm{diag}(\\phi'(x_1), \\phi'(x_2))\n$$\n在不动点 $\\mathbf{x}^\\star = \\mathbf{0}$ 处求值，我们有 $\\phi'(0) = \\mathrm{sech}^2(0) = 1$。因此，该对角矩阵变成单位矩阵 $\\mathbf{I}$。原点处的雅可比矩阵简化为：\n$$\n\\mathbf{J} = -\\mathbf{I} + \\mathbf{W}\\,\\mathbf{I} = -\\mathbf{I} + \\mathbf{W}\n$$\n\n**2. 特征结构与稳定性分类**\n\nHartman-Grobman 定理指出，对于一个双曲不动点（即其雅可比矩阵没有实部为零的特征值），非线性系统在不动点附近的定性行为与其线性化系统的定性行为在拓扑上是等价的。不动点的稳定性由 $\\mathbf{J}$ 的特征值 $\\{\\lambda_i\\}$ 决定。\n- 如果对所有 $i$ 都有 $\\mathrm{Re}(\\lambda_i)  0$，则不动点是稳定的。\n- 如果至少有一个 $i$ 使得 $\\mathrm{Re}(\\lambda_i) > 0$，则不动点是不稳定的。\n- 如果对某个 $i$ 有 $\\mathrm{Re}(\\lambda_i) = 0$，且对所有其他 $j$ 有 $\\mathrm{Re}(\\lambda_j) \\le 0$，则该不动点是非双曲的，如果不考察高阶项，线性分析可能无法确定其稳定性。然而，我们仍然可以对不变流形进行分类。\n\n稳定、不稳定和中心流形的维数 ($n_s$, $n_u$, $n_c$) 是通过根据 $\\mathbf{J}$ 特征值实部的符号来计数确定的。使用指定的容差 $\\delta = 10^{-8}$：\n- $n_s$ 是满足 $\\mathrm{Re}(\\lambda_i)  -\\delta$ 的特征值 $\\lambda_i$ 的数量。\n- $n_u$ 是满足 $\\mathrm{Re}(\\lambda_i) > \\delta$ 的特征值 $\\lambda_i$ 的数量。\n- $n_c$ 是满足 $|\\mathrm{Re}(\\lambda_i)| \\le \\delta$ 的特征值 $\\lambda_i$ 的数量。\n对于一个 $2$D 系统，我们必须有 $n_s + n_u + n_c = 2$。\n\n**3. 从特征向量确定初始化方向**\n\n$\\mathbf{J}$ 的特征向量张成了线性系统的不变子空间。通过沿这些方向启动轨迹，我们可以观察到每个流形的特征行为。设 $\\{\\lambda_i, \\mathbf{v}_i\\}$ 是 $\\mathbf{J}$ 的特征对。\n- **实特征向量**：如果一个特征向量 $\\mathbf{v}$ 是实的，它定义了一条穿过原点且在线性流下不变的直线。我们通过在 $\\mathbf{x}(0) = \\varepsilon \\mathbf{v}$ 和 $\\mathbf{x}(0) = -\\varepsilon \\mathbf{v}$ 处初始化轨迹来测试动力学，其中 $\\varepsilon$ 是一个很小的数。\n- **复特征向量**：如果 $\\mathbf{J}$ 是实的，它的复特征值会成共轭对出现，即 $\\lambda, \\bar{\\lambda}$，相应的特征向量也是复共轭的，即 $\\mathbf{v}, \\bar{\\mathbf{v}}$。设 $\\mathbf{v} = \\mathbf{a} + i\\mathbf{b}$。实向量 $\\mathbf{a}$ 和 $\\mathbf{b}$ 构成了与这对特征值相关的二维不变子空间的一个基。我们通过沿四个方向初始化轨迹来测试该平面内的动力学：$\\mathbf{x}(0) = \\pm\\varepsilon\\mathbf{a}$ 和 $\\mathbf{x}(0) = \\pm\\varepsilon\\mathbf{b}$。\n\n**4. 数值模拟与验证**\n\n为了验证非线性动力学在小扰动下与线性预测相符，我们对每个构造的方向 $\\mathbf{d}$ 数值积分原始的 ODE $\\frac{d\\mathbf{x}}{dt} = -\\mathbf{x} + \\mathbf{W}\\,\\tanh(\\mathbf{x})$。初始条件为 $\\mathbf{x}(0) = \\varepsilon \\mathbf{d}$，其中 $\\varepsilon = 10^{-3}$，模拟在时间范围 $T = 3$ 内运行。\n\n然后我们计算布尔标志 $q$。对于每个轨迹，我们在多个时间点计算到不动点的欧几里得距离 $\\|\\mathbf{x}(t)\\|$。\n- 对于与稳定特征值 ($\\mathrm{Re}(\\lambda)  -\\delta$) 相关的方向 $\\mathbf{d}$，距离 $\\|\\mathbf{x}(t)\\|$ 必须在整个模拟区间内严格减小。\n- 对于与不稳定特征值 ($\\mathrm{Re}(\\lambda) > \\delta$) 相关的方向 $\\mathbf{d}$，距离 $\\|\\mathbf{x}(t)\\|$ 必须严格增大。\n\n当且仅当所有与稳定方向相关的轨迹都表现出严格减小的距离，并且所有与不稳定方向相关的轨迹都表现出严格增大的距离时，标志 $q$ 才为真。如果没有稳定或不稳定方向，则相应的条件视为空真。\n\n实现过程将是遍历每个提供的矩阵 $\\mathbf{W}$，执行这四个步骤，并收集结果 $[n_s, n_u, n_c, q]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the RNN dynamics analysis problem for a suite of test cases.\n    \"\"\"\n    \n    # Fixed simulation parameters\n    epsilon = 1e-3\n    T = 3.0\n    delta = 1e-8\n    \n    # Test suite of connectivity matrices W\n    test_cases = [\n        # Case 1 (saddle)\n        np.array([[1.5, 0.0], [0.0, 0.5]]),\n        # Case 2 (stable node)\n        np.array([[0.2, 0.0], [0.0, 0.3]]),\n        # Case 3 (unstable node)\n        np.array([[1.3, 0.0], [0.0, 1.4]]),\n        # Case 4 (stable spiral)\n        np.array([[0.5, 1.8], [-1.8, 0.5]]),\n        # Case 5 (center, purely imaginary linearization)\n        np.array([[1.0, 1.0], [-1.0, 1.0]]),\n    ]\n    \n    # List to store results for all test cases\n    all_results = []\n    \n    # ODE definition for the RNN dynamics\n    def f(t, x, W):\n        return -x + W @ np.tanh(x)\n\n    for W in test_cases:\n        # 1. Compute Jacobian at the origin\n        J = -np.eye(2) + W\n        \n        # 2. Compute eigenvalues and eigenvectors\n        eigvals, eigvecs = np.linalg.eig(J)\n        \n        # 3. Classify manifolds and count dimensions\n        ns, nu, nc = 0, 0, 0\n        real_parts = np.real(eigvals)\n        \n        for r in real_parts:\n            if r  -delta:\n                ns += 1\n            elif r > delta:\n                nu += 1\n            else:\n                nc += 1\n        \n        # 4. Construct initialization directions\n        stable_dirs = []\n        unstable_dirs = []\n        \n        # The eigenvalues are either both real or a complex conjugate pair\n        if np.iscomplexobj(eigvals):\n            # Complex conjugate pair\n            v = eigvecs[:, 0]\n            real_part = np.real(v)\n            imag_part = np.imag(v)\n            \n            # Normalize for consistency, although epsilon scaling dominates\n            if np.linalg.norm(real_part) > 1e-9: real_part /= np.linalg.norm(real_part)\n            if np.linalg.norm(imag_part) > 1e-9: imag_part /= np.linalg.norm(imag_part)\n\n            dirs_from_complex = [real_part, -real_part, imag_part, -imag_part]\n            \n            if real_parts[0]  -delta:\n                stable_dirs.extend(dirs_from_complex)\n            elif real_parts[0] > delta:\n                unstable_dirs.extend(dirs_from_complex)\n            # Center manifold directions are not checked for the 'q' flag\n        else:\n            # Two real eigenvalues\n            for i in range(2):\n                v = eigvecs[:, i]\n                # eig gives normalized eigenvectors\n                dirs_from_real = [v, -v]\n                \n                if real_parts[i]  -delta:\n                    stable_dirs.extend(dirs_from_real)\n                elif real_parts[i] > delta:\n                    unstable_dirs.extend(dirs_from_real)\n        \n        # 5. Simulate trajectories and verify behavior for 'q' flag\n        q = True\n        \n        # Check stable directions\n        if stable_dirs:\n            is_stable_ok = True\n            for d in stable_dirs:\n                if np.linalg.norm(d)  1e-9: continue\n                x0 = epsilon * d\n                sol = solve_ivp(f, [0, T], x0, args=(W,), dense_output=True, t_eval=np.linspace(0, T, num=100))\n                distances = np.linalg.norm(sol.y, axis=0)\n                # Check for strict decrease; allow for numerical noise near zero\n                if not np.all(np.diff(distances)  0):\n                    # Check if it settles at the origin making diffs zero.\n                    if not (np.all(distances[-10:]  1e-9) and distances[0] > distances[-1]):\n                         is_stable_ok = False\n                         break\n            if not is_stable_ok:\n                q = False\n\n        # Check unstable directions if stable ones passed\n        if q and unstable_dirs:\n            is_unstable_ok = True\n            for d in unstable_dirs:\n                if np.linalg.norm(d)  1e-9: continue\n                x0 = epsilon * d\n                sol = solve_ivp(f, [0, T], x0, args=(W,), dense_output=True, t_eval=np.linspace(0, T, num=100))\n                distances = np.linalg.norm(sol.y, axis=0)\n                if not np.all(np.diff(distances) > 0):\n                    is_unstable_ok = False\n                    break\n            if not is_unstable_ok:\n                q = False\n                \n        all_results.append([ns, nu, nc, q])\n        \n    # Format the final output string\n    result_str = \",\".join([str(res) for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}