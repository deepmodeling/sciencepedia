{
    "hands_on_practices": [
        {
            "introduction": "The first step in building any CNN is defining its architecture. This fundamental exercise focuses on the geometry of the forward pass: determining the size of the output feature map given the input dimensions and layer hyperparameters like kernel size, stride, and padding. Mastering this calculation  is essential for designing networks that correctly process and transform stimulus data into feature representations of a desired spatial scale.",
            "id": "4149684",
            "problem": "A visual encoding model in neuroscience uses a Convolutional Neural Network (CNN) to transform a two-dimensional stimulus into predicted responses arranged on a retinotopic grid. Consider a single two-dimensional convolutional layer applied to an input tensor of spatial height $H$, spatial width $W$, and channels $C$, written as shape $(H, W, C)$. The layer uses square kernels of spatial size $k \\times k$, a stride $s$ in both spatial dimensions, and zero padding of width $p$ applied symmetrically to the spatial boundaries. The layer has $F$ filters, each producing one output feature map. Assume the convolution is implemented in the standard discrete form with stride and zero padding, and that kernels do not extend beyond the padded input.\n\nStarting from the definition of discrete convolution on a zero-padded input and the interpretation of stride as the step size between successive kernel placements, derive the general expressions for the output spatial dimensions that result when the kernel is placed only at positions where it fully overlaps the padded input. Then, for a concrete encoding scenario in which the stimulus is a single-channel retinotopic map with $H=95$, $W=127$, $C=1$, and the convolutional layer has $k=11$, $s=4$, $p=3$, and $F=32$, compute the resulting output tensor shape $(H_{\\text{out}}, W_{\\text{out}}, F)$. Express your final answer as the ordered triple $(H_{\\text{out}}, W_{\\text{out}}, F)$. No rounding is required.",
            "solution": "The problem asks for the derivation of the general formula for the output dimensions of a two-dimensional convolutional layer and for the calculation of these dimensions for a specific set of parameters.\n\nFirst, we will derive the general expressions. Let us consider a single spatial dimension, for instance, the height. The input has a height of $H$. Symmetrical zero padding of width $p$ is applied, meaning $p$ rows of zeros are added to the top and $p$ to the bottom. The effective height of this padded input tensor is therefore $H' = H + 2p$.\n\nThe convolutional kernel has a spatial height of $k$. We need to determine the number of possible positions at which this kernel can be placed on the padded input. The kernel is applied at locations separated by a stride $s$. The positions are determined by the top-left corner of the kernel.\n\nLet the indices of the padded height dimension run from $0$ to $H' - 1$. The first placement of the kernel will have its top edge at index $0$. The second placement will be at index $s$, the third at $2s$, and so on. Let the $j$-th placement have its top edge at index $i_j = j \\cdot s$, where $j$ is a zero-based index ($j = 0, 1, 2, \\dots$).\n\nThe problem states that the kernel must fully overlap the padded input. This means the entire kernel, of height $k$, must fit within the bounds of the padded input of height $H'$. If the top edge of the kernel is at index $i_j$, the kernel spans indices from $i_j$ to $i_j + k - 1$. For the kernel to be fully contained, we must have:\n$$i_j + k - 1 \\leq H' - 1$$\n$$i_j \\leq H' - k$$\nSubstituting $i_j = j \\cdot s$ and $H' = H + 2p$, we get:\n$$j \\cdot s \\leq (H + 2p) - k$$\nTo find the maximum number of valid placements, we need to find the maximum integer value of $j$, let's call it $j_{\\max}$, that satisfies this inequality.\n$$j_{\\max} = \\left\\lfloor \\frac{H + 2p - k}{s} \\right\\rfloor$$\nThe possible values for $j$ are $0, 1, 2, \\dots, j_{\\max}$. The total number of such values is $j_{\\max} + 1$. Each value of $j$ corresponds to one position in the output feature map. Therefore, the height of the output feature map, $H_{\\text{out}}$, is:\n$$H_{\\text{out}} = j_{\\max} + 1 = \\left\\lfloor \\frac{H + 2p - k}{s} \\right\\rfloor + 1$$\nThe same logic applies independently to the width dimension. An input of width $W$ with symmetric padding $p$ becomes a padded input of width $W' = W + 2p$. The output width, $W_{\\text{out}}$, is given by the analogous formula:\n$$W_{\\text{out}} = \\left\\lfloor \\frac{W + 2p - k}{s} \\right\\rfloor + 1$$\nThe number of channels (or depth) of the output tensor is determined by the number of filters, $F$. Each filter convolves with the input volume (across all $C$ input channels) to produce a single two-dimensional feature map. Therefore, with $F$ filters, the output tensor will have a depth of $F$. The shape of the output tensor is thus $(H_{\\text{out}}, W_{\\text{out}}, F)$.\n\nNow, we apply these derived formulas to the specific scenario provided. The given parameters are:\n- Input height $H = 95$\n- Input width $W = 127$\n- Input channels $C = 1$\n- Kernel size $k = 11$\n- Stride $s = 4$\n- Padding width $p = 3$\n- Number of filters $F = 32$\n\nWe compute the output height, $H_{\\text{out}}$:\n$$H_{\\text{out}} = \\left\\lfloor \\frac{H + 2p - k}{s} \\right\\rfloor + 1$$\n$$H_{\\text{out}} = \\left\\lfloor \\frac{95 + 2(3) - 11}{4} \\right\\rfloor + 1$$\n$$H_{\\text{out}} = \\left\\lfloor \\frac{95 + 6 - 11}{4} \\right\\rfloor + 1$$\n$$H_{\\text{out}} = \\left\\lfloor \\frac{90}{4} \\right\\rfloor + 1$$\n$$H_{\\text{out}} = \\lfloor 22.5 \\rfloor + 1$$\n$$H_{\\text{out}} = 22 + 1 = 23$$\nNext, we compute the output width, $W_{\\text{out}}$:\n$$W_{\\text{out}} = \\left\\lfloor \\frac{W + 2p - k}{s} \\right\\rfloor + 1$$\n$$W_{\\text{out}} = \\left\\lfloor \\frac{127 + 2(3) - 11}{4} \\right\\rfloor + 1$$\n$$W_{\\text{out}} = \\left\\lfloor \\frac{127 + 6 - 11}{4} \\right\\rfloor + 1$$\n$$W_{\\text{out}} = \\left\\lfloor \\frac{122}{4} \\right\\rfloor + 1$$\n$$W_{\\text{out}} = \\lfloor 30.5 \\rfloor + 1$$\n$$W_{\\text{out}} = 30 + 1 = 31$$\nThe number of output channels is equal to the number of filters, $F = 32$.\n\nTherefore, the resulting output tensor shape $(H_{\\text{out}}, W_{\\text{out}}, F)$ is $(23, 31, 32)$. The final answer is requested as an ordered triple.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 23 & 31 & 32 \\end{pmatrix} } $$"
        },
        {
            "introduction": "After defining a model's forward pass, the next crucial step is understanding how it learns from data. This practice  demystifies the training process by having you compute the gradient of a loss function with respect to a model's parameters from first principles. By manually deriving and implementing the gradient calculation for a simple 1D convolution, you will gain a deep intuition for the backpropagation algorithm that powers modern deep learning frameworks.",
            "id": "4149671",
            "problem": "You are analyzing an encoding model used in neuroscience data analysis, where a one-dimensional temporal stimulus is mapped to a predicted neural response by a single-filter Convolutional Neural Network (CNN). In this setting, the model output is the valid one-dimensional convolution of an input sequence with a filter kernel, and the objective is to minimize the Mean Squared Error (MSE) between the predicted and target neural responses. You are asked to implement a program that computes the gradient of the MSE loss with respect to both the input sequence and the kernel in two ways: manually derived from first principles and via Automatic Differentiation (AD). Your program must verify that both gradient computations agree for a set of test cases.\n\nUse the following fundamental base:\n- Convolutional Neural Network (CNN): a class of models that compute outputs via convolution operators acting on input sequences; a single-filter temporal CNN computes a one-dimensional convolution.\n- Mean Squared Error (MSE): for predicted outputs $ \\hat{y} $ and target outputs $ y $, the loss is defined by $ \\mathcal{L} = \\frac{1}{N} \\sum_{n=0}^{N-1} (\\hat{y}_n - y_n)^2 $, where $ N $ is the number of output samples.\n- Chain rule of calculus: the derivative of a composite function is the product of derivatives along the composition.\n\nModel specifics:\n- Use valid one-dimensional convolution with stride $ 1 $ and no padding.\n- Let the input sequence be $ x \\in \\mathbb{R}^T $ and the kernel be $ w \\in \\mathbb{R}^K $. The valid convolution output length is $ N = T - K + 1 $.\n- The predicted output is $ \\hat{y}_n = \\sum_{k=0}^{K-1} w_k \\, x_{n+k} $ for $ n \\in \\{0,1,\\dots,N-1\\} $.\n- The loss to be minimized is $ \\mathcal{L} = \\frac{1}{N} \\sum_{n=0}^{N-1} (\\hat{y}_n - y_n)^2 $.\n\nYour tasks:\n- Derive, from the stated base and without invoking shortcut formulas, the manual gradient of $ \\mathcal{L} $ with respect to $ x $ and $ w $ by applying the chain rule and the linearity of convolution.\n- Implement a simple reverse-mode Automatic Differentiation (AD) mechanism that constructs a computational graph from scalar operations and performs backpropagation to compute the gradients of $ \\mathcal{L} $ with respect to the scalar entries of $ x $ and $ w $.\n- For each test case, compute both gradients and return the maximum absolute difference between the manual gradient and the AD gradient across all entries of $ x $ and $ w $.\n\nAngle units and physical units are not applicable to this problem.\n\nTest suite:\n- Case $ 1 $: $ x = [1,2,3] $, $ w = [1,-1] $, $ y = [0,0] $.\n- Case $ 2 $: $ x = [1,2,3] $, $ w = [1,-1] $, $ y $ equal to the valid convolution of $ x $ with $ w $ (that is, $ y = \\hat{y} $ computed from the model for these $ x $ and $ w $).\n- Case $ 3 $: $ x = [0,0,0] $, $ w = [2,-3] $, $ y = [1,-1] $.\n- Case $ 4 $: $ x = [1,0,-1,2,-2] $, $ w = [0.5,-1.5,1.0] $, $ y = [1.0,-0.5,2.0] $.\n\nAnswer specification:\n- For each case, produce a single floating-point value equal to $ \\max \\left( \\{ |g^{\\text{manual}}_{x,i} - g^{\\text{AD}}_{x,i}| \\}_{i=0}^{T-1} \\cup \\{ |g^{\\text{manual}}_{w,j} - g^{\\text{AD}}_{w,j}| \\}_{j=0}^{K-1} \\right) $, where $ g^{\\text{manual}} $ and $ g^{\\text{AD}} $ denote the manual and AD gradients, respectively.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $ [r_1,r_2,r_3,r_4] $), where $ r_i $ is the float for Case $ i $.\n\nScientific realism and consistency:\n- Interpret the input sequences as stimulus time series and the kernel as a temporal receptive field. The target outputs represent measured neural responses. All values are unitless real numbers.\n- The problem must be solved in purely mathematical and logical terms by computing derivatives and verifying equality of results between the manual derivation and AD.",
            "solution": "The problem requires the computation and verification of gradients for a simple one-dimensional convolutional encoding model. The gradients of the Mean Squared Error (MSE) loss function with respect to the model's input sequence and its filter kernel are to be determined in two ways: first, by manual analytical derivation from first principles, and second, via a custom implementation of reverse-mode Automatic Differentiation (AD). The consistency between these two methods will be validated.\n\nLet the input sequence be $x \\in \\mathbb{R}^T$ and the kernel be $w \\in \\mathbb{R}^K$. The model predicts a neural response $\\hat{y} \\in \\mathbb{R}^N$ via a valid one-dimensional convolution. The length of the output is $N = T - K + 1$. The predicted output at time step $n$ is given by:\n$$ \\hat{y}_n = \\sum_{k=0}^{K-1} w_k x_{n+k} \\quad \\text{for } n \\in \\{0, 1, \\dots, N-1\\} $$\nThe objective is to minimize the MSE loss function, $\\mathcal{L}$, defined with respect to a target neural response $y \\in \\mathbb{R}^N$:\n$$ \\mathcal{L}(x, w) = \\frac{1}{N} \\sum_{n=0}^{N-1} (\\hat{y}_n - y_n)^2 $$\n\n### Manual Gradient Derivation\n\nWe derive the gradients $\\frac{\\partial \\mathcal{L}}{\\partial w}$ and $\\frac{\\partial \\mathcal{L}}{\\partial x}$ by applying the chain rule of calculus. For any parameter $\\theta$ in the model (either a weight $w_j$ or an input element $x_i$), the gradient is given by:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\sum_{n=0}^{N-1} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial \\theta} $$\nThe first term, the derivative of the loss with respect to a predicted output $\\hat{y}_n$, is common to all gradient calculations:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_n} = \\frac{\\partial}{\\partial \\hat{y}_n} \\left[ \\frac{1}{N} \\sum_{m=0}^{N-1} (\\hat{y}_m - y_m)^2 \\right] = \\frac{1}{N} \\cdot 2 (\\hat{y}_n - y_n) $$\nLet us denote this error signal as $\\delta_n = \\frac{2}{N} (\\hat{y}_n - y_n)$.\n\n**1. Gradient with respect to the kernel weights ($w_j$)**\n\nWe seek the partial derivative of $\\mathcal{L}$ with respect to a specific weight $w_j$, where $j \\in \\{0, 1, \\dots, K-1\\}$. We first need the derivative of $\\hat{y}_n$ with respect to $w_j$:\n$$ \\frac{\\partial \\hat{y}_n}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\left( \\sum_{k=0}^{K-1} w_k x_{n+k} \\right) = x_{n+j} $$\nThe derivative is non-zero only when the summation index $k$ equals $j$. Substituting this into the chain rule expression:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\sum_{n=0}^{N-1} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial w_j} = \\sum_{n=0}^{N-1} \\delta_n x_{n+j} $$\nThis operation is a valid cross-correlation between the input signal $x$ and the error signal $\\delta$. The gradient vector $\\nabla_w \\mathcal{L}$ has components $(\\nabla_w \\mathcal{L})_j = \\frac{\\partial \\mathcal{L}}{\\partial w_j}$.\n\n**2. Gradient with respect to the input sequence ($x_i$)**\n\nNext, we find the partial derivative of $\\mathcal{L}$ with respect to an input element $x_i$, where $i \\in \\{0, 1, \\dots, T-1\\}$. We determine the derivative of $\\hat{y}_n$ with respect to $x_i$:\n$$ \\frac{\\partial \\hat{y}_n}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left( \\sum_{k=0}^{K-1} w_k x_{n+k} \\right) $$\nThe term $x_i$ contributes to the sum if there exists a $k$ such that $n+k = i$. This implies $k = i-n$. The derivative is therefore non-zero only if $0 \\le i-n \\le K-1$, in which case it is equal to the corresponding weight $w_{i-n}$. Thus:\n$$ \\frac{\\partial \\hat{y}_n}{\\partial x_i} = w_{i-n} \\quad \\text{for } 0 \\le i-n  K, \\text{ and } 0 \\text{ otherwise.} $$\nSubstituting this into the chain rule formula gives the gradient with respect to $x_i$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial x_i} = \\sum_{n=0}^{N-1} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial x_i} = \\sum_{n=\\max(0, i-K+1)}^{\\min(N-1, i)} \\delta_n w_{i-n} $$\nThis expression corresponds to a 'full' convolution between the error signal $\\delta$ and the kernel $w$. The resulting gradient vector $\\nabla_x \\mathcal{L}$ will have length $T$.\n\n### Automatic Differentiation (AD) Design\n\nReverse-mode Automatic Differentiation (AD) is an algorithmic approach to compute gradients. It operates in two phases: a forward pass and a backward pass.\n\n**1. Forward Pass: Computational Graph Construction**\nWe define a class, `Value`, to represent each scalar in the computation. A `Value` object stores its numerical `data`, its `grad` (the gradient of the final loss with respect to it), and its provenance (the operation `_op` and parent `Value` objects `_prev` that produced it).\n\nBy overloading standard arithmetic operators (`+`, `*`, `**`, etc.), we can write the model's forward computation naturally. Each operation between `Value` objects creates a new `Value` object, dynamically building a computational graph that tracks all dependencies. For each new `Value` created, a `_backward` function is also defined and stored. This function knows how to propagate a gradient from the child node back to its parents, based on the local partial derivatives of the operation that created it. For instance, for an addition $c = a + b$, the `_backward` function for $c$ will execute `a.grad += c.grad` and `b.grad += c.grad`, as $\\frac{\\partial c}{\\partial a} = 1$ and $\\frac{\\partial c}{\\partial b} = 1$. Similarly, for a multiplication $c = a \\times b$, it will execute `a.grad += c.grad * b.data` and `b.grad += c.grad * a.data`, as $\\frac{\\partial c}{\\partial a} = b$ and $\\frac{\\partial c}{\\partial b} = a$.\n\n**2. Backward Pass: Gradient Propagation**\nOnce the final scalar loss $\\mathcal{L}$ is computed as a `Value` object, the backward pass begins.\nFirst, the gradient of the loss with respect to itself, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathcal{L}}$, is initialized to $1$.\nThen, a topological sort of the computational graph is performed, starting from the final loss node. This yields a linear ordering of all `Value` objects involved in the computation, from inputs to output.\nFinally, we iterate through this sorted list in reverse order. For each `Value` object (node), we call its stored `_backward` function. This process systematically applies the chain rule, propagating gradients from the loss all the way back to the input nodes ($x$ and $w$), correctly accumulating the partial derivatives at each step. After the pass is complete, the `grad` attribute of each initial `Value` object for $x$ and $w$ holds the total derivative of the loss with respect to it.\n\nThe final program will implement both the manual formulas and the AD framework, compute the gradients for each test case, and report the maximum absolute difference between the two results over all components of $x$ and $w$. This verifies the correctness of both the analytical derivation and the AD implementation.",
            "answer": "```python\nimport numpy as np\n\nclass Value:\n    \"\"\"A scalar value for building a computational graph and performing AD.\"\"\"\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"power must be int/float\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * (self.data ** (other - 1))) * out.grad\n        out._backward = _backward\n        return out\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * (other ** -1)\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __rsub__(self, other):\n        return other + (-self)\n\n    def __rtruediv__(self, other):\n        return other * (self ** -1)\n\n    def backward(self):\n        \"\"\"Performs backpropagation starting from this node.\"\"\"\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        \n        build_topo(self)\n        \n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\ndef manual_gradient(x, w, y):\n    \"\"\"Computes gradients using manually derived analytical formulas.\"\"\"\n    x = np.array(x, dtype=float)\n    w = np.array(w, dtype=float)\n    y = np.array(y, dtype=float)\n\n    T = len(x)\n    K = len(w)\n    N = T - K + 1\n\n    # Forward pass\n    y_hat = np.array([np.sum(w * x[n:n+K]) for n in range(N)])\n\n    # Gradient pre-computation\n    delta = (2.0 / N) * (y_hat - y)\n\n    # Gradient for weights w\n    grad_w = np.zeros(K)\n    for j in range(K):\n        grad_w[j] = np.sum(delta * x[j : j+N])\n\n    # Gradient for input x (full convolution of delta and w)\n    w_flipped = w[::-1]\n    grad_x = np.convolve(delta, w_flipped, mode='full')\n    \n    return grad_x, grad_w\n\ndef ad_gradient(x_data, w_data, y_data):\n    \"\"\"Computes gradients using reverse-mode Automatic Differentiation.\"\"\"\n    x = [Value(d) for d in x_data]\n    w = [Value(d) for d in w_data]\n    y = y_data\n\n    T = len(x)\n    K = len(w)\n    N = T - K + 1\n    \n    # Forward pass to build the graph\n    y_hat = []\n    for n in range(N):\n        y_hat_n = Value(0.0)\n        for k in range(K):\n            y_hat_n += w[k] * x[n+k]\n        y_hat.append(y_hat_n)\n\n    # Loss calculation\n    errors_sq = []\n    for n in range(N):\n        error = y_hat[n] - y[n]\n        errors_sq.append(error**2)\n        \n    sum_sq_err = Value(0.0)\n    for err_sq in errors_sq:\n        sum_sq_err += err_sq\n        \n    loss = sum_sq_err / N\n    \n    # Backward pass to compute gradients\n    loss.backward()\n\n    grad_x = np.array([v.grad for v in x])\n    grad_w = np.array([v.grad for v in w])\n    \n    return grad_x, grad_w\n\ndef solve():\n    \"\"\"Main function to run test cases and verify gradient computations.\"\"\"\n    \n    test_cases_params = [\n        ([1, 2, 3], [1, -1], [0, 0]),\n        ([1, 2, 3], [1, -1], None),  # y will be computed as y_hat\n        ([0, 0, 0], [2, -3], [1, -1]),\n        ([1, 0, -1, 2, -2], [0.5, -1.5, 1.0], [1.0, -0.5, 2.0]),\n    ]\n    \n    results = []\n    \n    for i, (x_p, w_p, y_p) in enumerate(test_cases_params):\n        x = np.array(x_p, dtype=float)\n        w = np.array(w_p, dtype=float)\n\n        if y_p is None:\n            # For Case 2, y is the model's prediction\n            T, K = len(x), len(w)\n            N = T - K + 1\n            y = np.array([np.sum(w * x[n:n+K]) for n in range(N)])\n        else:\n            y = np.array(y_p, dtype=float)\n\n        # Compute gradients using both methods\n        grad_x_manual, grad_w_manual = manual_gradient(x, w, y)\n        grad_x_ad, grad_w_ad = ad_gradient(x, w, y)\n        \n        # Calculate maximum absolute difference\n        diff_x = np.abs(grad_x_manual - grad_x_ad)\n        diff_w = np.abs(grad_w_manual - grad_w_ad)\n        \n        max_diff = np.max(np.concatenate((diff_x, diff_w)))\n        results.append(max_diff)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once an encoding model is trained, how should we evaluate its performance? Neural data is inherently noisy, which places a theoretical limit on how well any deterministic model can predict responses. This exercise  guides you through the derivation and calculation of the \"noise ceiling,\" a critical metric that quantifies this upper performance bound and enables you to interpret your model's predictive accuracy in a meaningful, data-informed context.",
            "id": "4149696",
            "problem": "A Convolutional Neural Network (CNN) encoding model maps a visual stimulus $s$ to a predicted neural response $\\hat{\\mu}_{s}$ via a deterministic function of stimulus-derived features. You are analyzing single-neuron responses to $S$ natural images, each presented $T = 5$ times. For each stimulus $s \\in \\{1, \\dots, S\\}$ and trial $t \\in \\{1, \\dots, T\\}$, the recorded response is $y_{s,t}$. Define the per-stimulus trial average as $\\bar{y}_{s} = \\frac{1}{T} \\sum_{t=1}^{T} y_{s,t}$ and the sample variance across stimuli of these averages as $v_{\\bar{y}}$. Also define the within-stimulus sample variance across repeats for stimulus $s$ as $v_{\\text{within}}(s)$, and its average across stimuli as $v_{\\text{within}} = \\frac{1}{S} \\sum_{s=1}^{S} v_{\\text{within}}(s)$.\n\nAssume the standard repeated-measures generative model $y_{s,t} = \\mu_{s} + \\epsilon_{s,t}$, where $\\mu_{s}$ is the stimulus-locked mean response, and $\\epsilon_{s,t}$ is trial noise that is independent across $t$, zero-mean, and homoscedastic across stimuli with variance $\\sigma_{\\epsilon}^{2}$. The CNNâ€™s cross-validated Coefficient of Determination (R-squared, $R^{2}$) computed between $\\hat{\\mu}_{s}$ and $\\bar{y}_{s}$ across stimuli is $0.72$. From the dataset, you have estimated $v_{\\bar{y}} = 1.76$ and $v_{\\text{within}} = 0.8$.\n\nStarting from the generative model and variance decomposition, derive the noise ceiling for $R^{2}$ when evaluating deterministic encoding models against $\\bar{y}_{s}$, and then compute the ceiling-normalized $R^{2}$ equal to $R^{2}$ divided by the noise ceiling. Express your final answer as a decimal rounded to four significant figures. No physical units are required.",
            "solution": "The goal is to derive the noise ceiling for the coefficient of determination ($R^2$) and use it to normalize the model's given $R^2$ performance. The noise ceiling represents the maximum possible $R^2$ any deterministic model can achieve, limited by the inherent noise in the neural data.\n\n**1. Variance Decomposition**\n\nWe start with the generative model for the recorded neural response $y_{s,t}$ to stimulus $s$ on trial $t$:\n$$y_{s,t} = \\mu_{s} + \\epsilon_{s,t}$$\nwhere $\\mu_{s}$ is the true, stimulus-locked mean response rate, and $\\epsilon_{s,t}$ is i.i.d. trial noise with mean zero and variance $\\sigma_{\\epsilon}^{2}$.\n\nWe are evaluating models against the trial-averaged response, $\\bar{y}_{s}$:\n$$\\bar{y}_{s} = \\frac{1}{T} \\sum_{t=1}^{T} y_{s,t} = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu_{s} + \\epsilon_{s,t}) = \\mu_{s} + \\frac{1}{T} \\sum_{t=1}^{T} \\epsilon_{s,t} = \\mu_{s} + \\bar{\\epsilon}_{s}$$\nHere, $\\bar{\\epsilon}_{s}$ is the noise averaged over $T$ trials for stimulus $s$.\n\nThe total variance of these trial-averaged responses across stimuli, $\\text{Var}(\\bar{y}_{s})$, can be decomposed. Assuming the signal $\\mu_s$ and noise $\\bar{\\epsilon}_s$ are uncorrelated, the variance of their sum is the sum of their variances:\n$$\\text{Var}(\\bar{y}_{s}) = \\text{Var}(\\mu_{s}) + \\text{Var}(\\bar{\\epsilon}_{s})$$\n$\\text{Var}(\\mu_{s})$ is the \"signal variance,\" the portion of variance driven by the different stimuli. $\\text{Var}(\\bar{\\epsilon}_{s})$ is the \"noise variance\" remaining in the trial-averaged data. The variance of the average of $T$ independent noise variables is:\n$$\\text{Var}(\\bar{\\epsilon}_{s}) = \\text{Var}\\left(\\frac{1}{T}\\sum_{t=1}^{T} \\epsilon_{s,t}\\right) = \\frac{1}{T^2} \\sum_{t=1}^{T} \\text{Var}(\\epsilon_{s,t}) = \\frac{T \\cdot \\sigma_{\\epsilon}^2}{T^2} = \\frac{\\sigma_{\\epsilon}^2}{T}$$\nSo, our final variance decomposition is:\n$$\\text{Var}(\\bar{y}_{s}) = \\text{Var}(\\mu_{s}) + \\frac{\\sigma_{\\epsilon}^2}{T}$$\n\n**2. Estimating Variances from Data**\n\nThe problem provides sample estimates for the population variances:\n- The total variance of the trial-averaged responses is estimated by $v_{\\bar{y}}$: $\\text{Var}(\\bar{y}_{s}) \\approx v_{\\bar{y}} = 1.76$.\n- The trial-to-trial noise variance $\\sigma_{\\epsilon}^2$ is estimated by the average within-stimulus variance $v_{\\text{within}}$: $\\sigma_{\\epsilon}^2 \\approx v_{\\text{within}} = 0.8$.\n\nUsing these estimates, we can find the noise variance in the averaged data:\n$$\\text{Noise Variance} = \\frac{\\sigma_{\\epsilon}^2}{T} \\approx \\frac{v_{\\text{within}}}{T} = \\frac{0.8}{5} = 0.16$$\nAnd the signal variance:\n$$\\text{Signal Variance} = \\text{Var}(\\mu_{s}) \\approx \\text{Var}(\\bar{y}_{s}) - \\frac{\\sigma_{\\epsilon}^2}{T} \\approx v_{\\bar{y}} - \\frac{v_{\\text{within}}}{T} = 1.76 - 0.16 = 1.60$$\n\n**3. Deriving and Calculating the Noise Ceiling**\n\nThe noise ceiling for $R^2$, denoted $R^2_{\\text{ceiling}}$, is the fraction of the total variance in the measured data ($\\bar{y}_s$) that is attributable to the true signal ($\\mu_s$). It represents the $R^2$ of a perfect model where $\\hat{\\mu}_s = \\mu_s$.\n$$R^2_{\\text{ceiling}} = \\frac{\\text{Signal Variance}}{\\text{Total Variance}} = \\frac{\\text{Var}(\\mu_s)}{\\text{Var}(\\bar{y}_s)}$$\nPlugging in our estimated values:\n$$R^2_{\\text{ceiling}} \\approx \\frac{1.60}{1.76} = \\frac{160}{176} = \\frac{10 \\times 16}{11 \\times 16} = \\frac{10}{11}$$\nSo, the noise ceiling is approximately $0.9090...$.\n\n**4. Normalizing the Model's Performance**\n\nThe final step is to compute the ceiling-normalized $R^2$, which is the ratio of the model's observed $R^2$ to the maximum achievable $R^2$ (the noise ceiling).\n$$\\text{Ceiling-normalized } R^2 = \\frac{R^2_{\\text{model}}}{R^2_{\\text{ceiling}}}$$\nGiven the model's performance $R^2 = 0.72$:\n$$\\text{Ceiling-normalized } R^2 = \\frac{0.72}{10/11} = 0.72 \\times \\frac{11}{10} = 0.792$$\nRounding to four significant figures as requested, the final answer is $0.7920$.",
            "answer": "$$\n\\boxed{0.7920}\n$$"
        }
    ]
}