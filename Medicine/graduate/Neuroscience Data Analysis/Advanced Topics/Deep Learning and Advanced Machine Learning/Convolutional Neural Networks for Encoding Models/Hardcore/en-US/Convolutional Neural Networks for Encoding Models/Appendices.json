{
    "hands_on_practices": [
        {
            "introduction": "Before building or interpreting any Convolutional Neural Network, it is crucial to understand how its core component, the convolutional layer, transforms data. The spatial dimensions of the output feature maps are a direct function of the input dimensions and the layer's hyperparameters, including kernel size, stride, and padding. This foundational exercise  will guide you through deriving and applying the formula for a layer's output shape, a fundamental skill for designing and debugging any CNN architecture.",
            "id": "4149684",
            "problem": "A visual encoding model in neuroscience uses a Convolutional Neural Network (CNN) to transform a two-dimensional stimulus into predicted responses arranged on a retinotopic grid. Consider a single two-dimensional convolutional layer applied to an input tensor of spatial height $H$, spatial width $W$, and channels $C$, written as shape $(H, W, C)$. The layer uses square kernels of spatial size $k \\times k$, a stride $s$ in both spatial dimensions, and zero padding of width $p$ applied symmetrically to the spatial boundaries. The layer has $F$ filters, each producing one output feature map. Assume the convolution is implemented in the standard discrete form with stride and zero padding, and that kernels do not extend beyond the padded input.\n\nStarting from the definition of discrete convolution on a zero-padded input and the interpretation of stride as the step size between successive kernel placements, derive the general expressions for the output spatial dimensions that result when the kernel is placed only at positions where it fully overlaps the padded input. Then, for a concrete encoding scenario in which the stimulus is a single-channel retinotopic map with $H=95$, $W=127$, $C=1$, and the convolutional layer has $k=11$, $s=4$, $p=3$, and $F=32$, compute the resulting output tensor shape $(H_{\\text{out}}, W_{\\text{out}}, F)$. Express your final answer as the ordered triple $(H_{\\text{out}}, W_{\\text{out}}, F)$. No rounding is required.",
            "solution": "The problem asks for the derivation of the general formula for the output dimensions of a two-dimensional convolutional layer and for the calculation of these dimensions for a specific set of parameters.\n\nFirst, we will derive the general expressions. Let us consider a single spatial dimension, for instance, the height. The input has a height of $H$. Symmetrical zero padding of width $p$ is applied, meaning $p$ rows of zeros are added to the top and $p$ to the bottom. The effective height of this padded input tensor is therefore $H' = H + 2p$.\n\nThe convolutional kernel has a spatial height of $k$. We need to determine the number of possible positions at which this kernel can be placed on the padded input. The kernel is applied at locations separated by a stride $s$. The positions are determined by the top-left corner of the kernel.\n\nLet the indices of the padded height dimension run from $0$ to $H' - 1$. The first placement of the kernel will have its top edge at index $0$. The second placement will be at index $s$, the third at $2s$, and so on. Let the $j$-th placement have its top edge at index $i_j = j \\cdot s$, where $j$ is a zero-based index ($j = 0, 1, 2, \\dots$).\n\nThe problem states that the kernel must fully overlap the padded input. This means the entire kernel, of height $k$, must fit within the bounds of the padded input of height $H'$. If the top edge of the kernel is at index $i_j$, the kernel spans indices from $i_j$ to $i_j + k - 1$. For the kernel to be fully contained, we must have:\n$$i_j + k - 1 \\leq H' - 1$$\n$$i_j \\leq H' - k$$\nSubstituting $i_j = j \\cdot s$ and $H' = H + 2p$, we get:\n$$j \\cdot s \\leq (H + 2p) - k$$\nTo find the maximum number of valid placements, we need to find the maximum integer value of $j$, let's call it $j_{\\max}$, that satisfies this inequality.\n$$j_{\\max} = \\left\\lfloor \\frac{H + 2p - k}{s} \\right\\rfloor$$\nThe possible values for $j$ are $0, 1, 2, \\dots, j_{\\max}$. The total number of such values is $j_{\\max} + 1$. Each value of $j$ corresponds to one position in the output feature map. Therefore, the height of the output feature map, $H_{\\text{out}}$, is:\n$$H_{\\text{out}} = j_{\\max} + 1 = \\left\\lfloor \\frac{H + 2p - k}{s} \\right\\rfloor + 1$$\nThe same logic applies independently to the width dimension. An input of width $W$ with symmetric padding $p$ becomes a padded input of width $W' = W + 2p$. The output width, $W_{\\text{out}}$, is given by the analogous formula:\n$$W_{\\text{out}} = \\left\\lfloor \\frac{W + 2p - k}{s} \\right\\rfloor + 1$$\nThe number of channels (or depth) of the output tensor is determined by the number of filters, $F$. Each filter convolves with the input volume (across all $C$ input channels) to produce a single two-dimensional feature map. Therefore, with $F$ filters, the output tensor will have a depth of $F$. The shape of the output tensor is thus $(H_{\\text{out}}, W_{\\text{out}}, F)$.\n\nNow, we apply these derived formulas to the specific scenario provided. The given parameters are:\n- Input height $H = 95$\n- Input width $W = 127$\n- Input channels $C = 1$\n- Kernel size $k = 11$\n- Stride $s = 4$\n- Padding width $p = 3$\n- Number of filters $F = 32$\n\nWe compute the output height, $H_{\\text{out}}$:\n$$H_{\\text{out}} = \\left\\lfloor \\frac{H + 2p - k}{s} \\right\\rfloor + 1$$\n$$H_{\\text{out}} = \\left\\lfloor \\frac{95 + 2(3) - 11}{4} \\right\\rfloor + 1$$\n$$H_{\\text{out}} = \\left\\lfloor \\frac{95 + 6 - 11}{4} \\right\\rfloor + 1$$\n$$H_{\\text{out}} = \\left\\lfloor \\frac{90}{4} \\right\\rfloor + 1$$\n$$H_{\\text{out}} = \\lfloor 22.5 \\rfloor + 1$$\n$$H_{\\text{out}} = 22 + 1 = 23$$\nNext, we compute the output width, $W_{\\text{out}}$:\n$$W_{\\text{out}} = \\left\\lfloor \\frac{W + 2p - k}{s} \\right\\rfloor + 1$$\n$$W_{\\text{out}} = \\left\\lfloor \\frac{127 + 2(3) - 11}{4} \\right\\rfloor + 1$$\n$$W_{\\text{out}} = \\left\\lfloor \\frac{127 + 6 - 11}{4} \\right\\rfloor + 1$$\n$$W_{\\text{out}} = \\left\\lfloor \\frac{122}{4} \\right\\rfloor + 1$$\n$$W_{\\text{out}} = \\lfloor 30.5 \\rfloor + 1$$\n$$W_{\\text{out}} = 30 + 1 = 31$$\nThe number of output channels is equal to the number of filters, $F = 32$.\n\nTherefore, the resulting output tensor shape $(H_{\\text{out}}, W_{\\text{out}}, F)$ is $(23, 31, 32)$. The final answer is requested as an ordered triple.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 23 & 31 & 32 \\end{pmatrix} } $$"
        },
        {
            "introduction": "A network's ability to learn rests on the process of backpropagation, where the model's parameters are adjusted to minimize a loss function. This process relies on calculating the gradient of the loss with respect to every parameter, a task governed by the chain rule of calculus. By manually deriving and implementing the gradients for a simple convolutional model and verifying your results against an automatic differentiation (AD) framework , you will gain a profound understanding of the core mechanism that drives learning in deep neural networks.",
            "id": "4149671",
            "problem": "You are analyzing an encoding model used in neuroscience data analysis, where a one-dimensional temporal stimulus is mapped to a predicted neural response by a single-filter Convolutional Neural Network (CNN). In this setting, the model output is the valid one-dimensional convolution of an input sequence with a filter kernel, and the objective is to minimize the Mean Squared Error (MSE) between the predicted and target neural responses. You are asked to implement a program that computes the gradient of the MSE loss with respect to both the input sequence and the kernel in two ways: manually derived from first principles and via Automatic Differentiation (AD). Your program must verify that both gradient computations agree for a set of test cases.\n\nUse the following fundamental base:\n- Convolutional Neural Network (CNN): a class of models that compute outputs via convolution operators acting on input sequences; a single-filter temporal CNN computes a one-dimensional convolution.\n- Mean Squared Error (MSE): for predicted outputs $ \\hat{y} $ and target outputs $ y $, the loss is defined by $ \\mathcal{L} = \\frac{1}{N} \\sum_{n=0}^{N-1} (\\hat{y}_n - y_n)^2 $, where $ N $ is the number of output samples.\n- Chain rule of calculus: the derivative of a composite function is the product of derivatives along the composition.\n\nModel specifics:\n- Use valid one-dimensional convolution with stride $ 1 $ and no padding.\n- Let the input sequence be $ x \\in \\mathbb{R}^T $ and the kernel be $ w \\in \\mathbb{R}^K $. The valid convolution output length is $ N = T - K + 1 $.\n- The predicted output is $ \\hat{y}_n = \\sum_{k=0}^{K-1} w_k \\, x_{n+k} $ for $ n \\in \\{0,1,\\dots,N-1\\} $.\n- The loss to be minimized is $ \\mathcal{L} = \\frac{1}{N} \\sum_{n=0}^{N-1} (\\hat{y}_n - y_n)^2 $.\n\nYour tasks:\n- Derive, from the stated base and without invoking shortcut formulas, the manual gradient of $ \\mathcal{L} $ with respect to $ x $ and $ w $ by applying the chain rule and the linearity of convolution.\n- Implement a simple reverse-mode Automatic Differentiation (AD) mechanism that constructs a computational graph from scalar operations and performs backpropagation to compute the gradients of $ \\mathcal{L} $ with respect to the scalar entries of $ x $ and $ w $.\n- For each test case, compute both gradients and return the maximum absolute difference between the manual gradient and the AD gradient across all entries of $ x $ and $ w $.\n\nAngle units and physical units are not applicable to this problem.\n\nTest suite:\n- Case $ 1 $: $ x = [1,2,3] $, $ w = [1,-1] $, $ y = [0,0] $.\n- Case $ 2 $: $ x = [1,2,3] $, $ w = [1,-1] $, $ y $ equal to the valid convolution of $ x $ with $ w $ (that is, $ y = \\hat{y} $ computed from the model for these $ x $ and $ w $).\n- Case $ 3 $: $ x = [0,0,0] $, $ w = [2,-3] $, $ y = [1,-1] $.\n- Case $ 4 $: $ x = [1,0,-1,2,-2] $, $ w = [0.5,-1.5,1.0] $, $ y = [1.0,-0.5,2.0] $.\n\nAnswer specification:\n- For each case, produce a single floating-point value equal to $ \\max \\left( \\{ |g^{\\text{manual}}_{x,i} - g^{\\text{AD}}_{x,i}| \\}_{i=0}^{T-1} \\cup \\{ |g^{\\text{manual}}_{w,j} - g^{\\text{AD}}_{w,j}| \\}_{j=0}^{K-1} \\right) $, where $ g^{\\text{manual}} $ and $ g^{\\text{AD}} $ denote the manual and AD gradients, respectively.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $ [r_1,r_2,r_3,r_4] $), where $ r_i $ is the float for Case $ i $.\n\nScientific realism and consistency:\n- Interpret the input sequences as stimulus time series and the kernel as a temporal receptive field. The target outputs represent measured neural responses. All values are unitless real numbers.\n- The problem must be solved in purely mathematical and logical terms by computing derivatives and verifying equality of results between the manual derivation and AD.",
            "solution": "The problem requires the computation and verification of gradients for a simple one-dimensional convolutional encoding model. The gradients of the Mean Squared Error (MSE) loss function with respect to the model's input sequence and its filter kernel are to be determined in two ways: first, by manual analytical derivation from first principles, and second, via a custom implementation of reverse-mode Automatic Differentiation (AD). The consistency between these two methods will be validated.\n\nLet the input sequence be $x \\in \\mathbb{R}^T$ and the kernel be $w \\in \\mathbb{R}^K$. The model predicts a neural response $\\hat{y} \\in \\mathbb{R}^N$ via a valid one-dimensional convolution. The length of the output is $N = T - K + 1$. The predicted output at time step $n$ is given by:\n$$ \\hat{y}_n = \\sum_{k=0}^{K-1} w_k x_{n+k} \\quad \\text{for } n \\in \\{0, 1, \\dots, N-1\\} $$\nThe objective is to minimize the MSE loss function, $\\mathcal{L}$, defined with respect to a target neural response $y \\in \\mathbb{R}^N$:\n$$ \\mathcal{L}(x, w) = \\frac{1}{N} \\sum_{n=0}^{N-1} (\\hat{y}_n - y_n)^2 $$\n\n### Manual Gradient Derivation\n\nWe derive the gradients $\\frac{\\partial \\mathcal{L}}{\\partial w}$ and $\\frac{\\partial \\mathcal{L}}{\\partial x}$ by applying the chain rule of calculus. For any parameter $\\theta$ in the model (either a weight $w_j$ or an input element $x_i$), the gradient is given by:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\sum_{n=0}^{N-1} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial \\theta} $$\nThe first term, the derivative of the loss with respect to a predicted output $\\hat{y}_n$, is common to all gradient calculations:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_n} = \\frac{\\partial}{\\partial \\hat{y}_n} \\left[ \\frac{1}{N} \\sum_{m=0}^{N-1} (\\hat{y}_m - y_m)^2 \\right] = \\frac{1}{N} \\cdot 2 (\\hat{y}_n - y_n) $$\nLet us denote this error signal as $\\delta_n = \\frac{2}{N} (\\hat{y}_n - y_n)$.\n\n**1. Gradient with respect to the kernel weights ($w_j$)**\n\nWe seek the partial derivative of $\\mathcal{L}$ with respect to a specific weight $w_j$, where $j \\in \\{0, 1, \\dots, K-1\\}$. We first need the derivative of $\\hat{y}_n$ with respect to $w_j$:\n$$ \\frac{\\partial \\hat{y}_n}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\left( \\sum_{k=0}^{K-1} w_k x_{n+k} \\right) = x_{n+j} $$\nThe derivative is non-zero only when the summation index $k$ equals $j$. Substituting this into the chain rule expression:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\sum_{n=0}^{N-1} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial w_j} = \\sum_{n=0}^{N-1} \\delta_n x_{n+j} $$\nThis operation is a valid cross-correlation between the input signal $x$ and the error signal $\\delta$. The gradient vector $\\nabla_w \\mathcal{L}$ has components $(\\nabla_w \\mathcal{L})_j = \\frac{\\partial \\mathcal{L}}{\\partial w_j}$.\n\n**2. Gradient with respect to the input sequence ($x_i$)**\n\nNext, we find the partial derivative of $\\mathcal{L}$ with respect to an input element $x_i$, where $i \\in \\{0, 1, \\dots, T-1\\}$. We determine the derivative of $\\hat{y}_n$ with respect to $x_i$:\n$$ \\frac{\\partial \\hat{y}_n}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left( \\sum_{k=0}^{K-1} w_k x_{n+k} \\right) $$\nThe term $x_i$ contributes to the sum if there exists a $k$ such that $n+k = i$. This implies $k = i-n$. The derivative is therefore non-zero only if $0 \\le i-n \\le K-1$, in which case it is equal to the corresponding weight $w_{i-n}$. Thus:\n$$ \\frac{\\partial \\hat{y}_n}{\\partial x_i} = w_{i-n} \\quad \\text{for } 0 \\le i-n < K, \\text{ and } 0 \\text{ otherwise.} $$\nSubstituting this into the chain rule formula gives the gradient with respect to $x_i$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial x_i} = \\sum_{n=0}^{N-1} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial x_i} = \\sum_{n=\\max(0, i-K+1)}^{\\min(N-1, i)} \\delta_n w_{i-n} $$\nThis expression corresponds to a 'full' convolution between the error signal $\\delta$ and the kernel $w$, which can be written as $(\\delta * w)_i$. The resulting gradient vector $\\nabla_x \\mathcal{L}$ will have length $T$.\n\n### Automatic Differentiation (AD) Design\n\nReverse-mode Automatic Differentiation (AD) is an algorithmic approach to compute gradients. It operates in two phases: a forward pass and a backward pass.\n\n**1. Forward Pass: Computational Graph Construction**\nWe define a class, `Value`, to represent each scalar in the computation. A `Value` object stores its numerical `data`, its `grad` (the gradient of the final loss with respect to it), and its provenance (the operation `_op` and parent `Value` objects `_prev` that produced it).\n\nBy overloading standard arithmetic operators (`+`, `*`, `**`, etc.), we can write the model's forward computation naturally. Each operation between `Value` objects creates a new `Value` object, dynamically building a computational graph that tracks all dependencies. For each new `Value` created, a `_backward` function is also defined and stored. This function knows how to propagate a gradient from the child node back to its parents, based on the local partial derivatives of the operation that created it. For instance, for an addition $c = a + b$, the `_backward` function for $c$ will execute `a.grad += c.grad` and `b.grad += c.grad`, as $\\frac{\\partial c}{\\partial a} = 1$ and $\\frac{\\partial c}{\\partial b} = 1$. Similarly, for a multiplication $c = a \\times b$, it will execute `a.grad += c.grad * b.data` and `b.grad += c.grad * a.data`, as $\\frac{\\partial c}{\\partial a} = b$ and $\\frac{\\partial c}{\\partial b} = a$.\n\n**2. Backward Pass: Gradient Propagation**\nOnce the final scalar loss $\\mathcal{L}$ is computed as a `Value` object, the backward pass begins.\nFirst, the gradient of the loss with respect to itself, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathcal{L}}$, is initialized to $1$.\nThen, a topological sort of the computational graph is performed, starting from the final loss node. This yields a linear ordering of all `Value` objects involved in the computation, from inputs to output.\nFinally, we iterate through this sorted list in reverse order. For each `Value` object (node), we call its stored `_backward` function. This process systematically applies the chain rule, propagating gradients from the loss all the way back to the input nodes ($x$ and $w$), correctly accumulating the partial derivatives at each step. After the pass is complete, the `grad` attribute of each initial `Value` object for $x$ and $w$ holds the total derivative of the loss with respect to it.\n\nThe final program will implement both the manual formulas and the AD framework, compute the gradients for each test case, and report the maximum absolute difference between the two results over all components of $x$ and $w$. This verifies the correctness of both the analytical derivation and the AD implementation.",
            "answer": "```python\nimport numpy as np\n\nclass Value:\n    \"\"\"A scalar value for building a computational graph and performing AD.\"\"\"\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"power must be int/float\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * (self.data ** (other - 1))) * out.grad\n        out._backward = _backward\n        return out\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * (other ** -1)\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __rsub__(self, other):\n        return other + (-self)\n\n    def __rtruediv__(self, other):\n        return other * (self ** -1)\n\n    def backward(self):\n        \"\"\"Performs backpropagation starting from this node.\"\"\"\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        \n        build_topo(self)\n        \n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\ndef manual_gradient(x, w, y):\n    \"\"\"Computes gradients using manually derived analytical formulas.\"\"\"\n    x = np.array(x, dtype=float)\n    w = np.array(w, dtype=float)\n    y = np.array(y, dtype=float)\n\n    T = len(x)\n    K = len(w)\n    N = T - K + 1\n\n    # Forward pass\n    y_hat = np.array([np.sum(w * x[n:n+K]) for n in range(N)])\n\n    # Gradient pre-computation\n    delta = (2.0 / N) * (y_hat - y)\n\n    # Gradient for weights w\n    grad_w = np.zeros(K)\n    for j in range(K):\n        grad_w[j] = np.sum(delta * x[j : j+N])\n\n    # Gradient for input x (full convolution of delta and w)\n    grad_x = np.convolve(delta, w, mode='full')\n    \n    return grad_x, grad_w\n\ndef ad_gradient(x_data, w_data, y_data):\n    \"\"\"Computes gradients using reverse-mode Automatic Differentiation.\"\"\"\n    x = [Value(d) for d in x_data]\n    w = [Value(d) for d in w_data]\n    y = y_data\n\n    T = len(x)\n    K = len(w)\n    N = T - K + 1\n    \n    # Forward pass to build the graph\n    y_hat = []\n    for n in range(N):\n        y_hat_n = Value(0.0)\n        for k in range(K):\n            y_hat_n += w[k] * x[n+k]\n        y_hat.append(y_hat_n)\n\n    # Loss calculation\n    # Using sum of squares first, then dividing by N at the end\n    errors_sq = []\n    for n in range(N):\n        error = y_hat[n] - y[n]\n        errors_sq.append(error**2)\n        \n    sum_sq_err = Value(0.0)\n    for err_sq in errors_sq:\n        sum_sq_err += err_sq\n        \n    loss = sum_sq_err / N\n    \n    # Backward pass to compute gradients\n    loss.backward()\n\n    grad_x = np.array([v.grad for v in x])\n    grad_w = np.array([v.grad for v in w])\n    \n    return grad_x, grad_w\n\ndef solve():\n    \"\"\"Main function to run test cases and verify gradient computations.\"\"\"\n    \n    test_cases_params = [\n        ([1, 2, 3], [1, -1], [0, 0]),\n        ([1, 2, 3], [1, -1], None),  # y will be computed as y_hat\n        ([0, 0, 0], [2, -3], [1, -1]),\n        ([1, 0, -1, 2, -2], [0.5, -1.5, 1.0], [1.0, -0.5, 2.0]),\n    ]\n    \n    results = []\n    \n    for i, (x_p, w_p, y_p) in enumerate(test_cases_params):\n        x = np.array(x_p, dtype=float)\n        w = np.array(w_p, dtype=float)\n\n        if y_p is None:\n            # For Case 2, y is the model's prediction\n            T, K = len(x), len(w)\n            N = T - K + 1\n            y = np.array([np.sum(w * x[n:n+K]) for n in range(N)])\n        else:\n            y = np.array(y_p, dtype=float)\n\n        # Compute gradients using both methods\n        grad_x_manual, grad_w_manual = manual_gradient(x, w, y)\n        grad_x_ad, grad_w_ad = ad_gradient(x, w, y)\n        \n        # Calculate maximum absolute difference\n        diff_x = np.abs(grad_x_manual - grad_x_ad)\n        diff_w = np.abs(grad_w_manual - grad_w_ad)\n        \n        max_diff = np.max(np.concatenate((diff_x, diff_w)))\n        results.append(max_diff)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A primary goal of using CNNs as encoding models is to test whether their internal representations mirror those found in the brain. Representational Similarity Analysis (RSA) is a powerful framework for quantitatively comparing such representational geometries, but this comparison demands statistical rigor. This advanced practice  walks you through a complete, real-world analysis pipeline, from generating synthetic data to performing permutation tests for significance and applying False Discovery Rate (FDR) correction for multiple comparisons, equipping you with the tools to draw robust scientific conclusions.",
            "id": "4149629",
            "problem": "You are tasked with designing and implementing a complete, permutation-based hypothesis test to assess the significance of Representational Similarity Analysis (RSA) alignment between convolutional neural network (CNN) layers and brain regions of interest (ROIs) within the context of neuroscience encoding models, and then performing multiple comparison correction using False Discovery Rate (FDR). The solution must be a runnable program and must produce deterministic outputs.\n\nBegin from the following fundamental base applicable to this domain:\n- RSA compares representational dissimilarity matrices (RDMs) between two spaces (for example, a CNN layer feature space and a neural ROI response space) by quantifying the statistical association between the upper-triangular entries of their RDMs.\n- An RDM for $n$ stimuli is a symmetric matrix with zero diagonal whose entry at $(i,j)$ is the dissimilarity between stimulus $i$ and stimulus $j$ in a given feature space. A widely used dissimilarity is Euclidean distance: for feature vectors $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $\\mathbf{x}_j \\in \\mathbb{R}^d$, the Euclidean distance is $d_{ij} = \\left\\|\\mathbf{x}_i - \\mathbf{x}_j\\right\\|_2$.\n- Spearman rank correlation measures the statistical association between two variables by first replacing values with their ranks and then computing the Pearson correlation between the ranks. Pearson correlation between vectors $\\mathbf{u}$ and $\\mathbf{v}$ is defined as\n$$\n\\rho(\\mathbf{u},\\mathbf{v}) = \\frac{\\sum_{k=1}^{m} (u_k - \\bar{u})(v_k - \\bar{v})}{\\sqrt{\\sum_{k=1}^{m}(u_k - \\bar{u})^2}\\sqrt{\\sum_{k=1}^{m}(v_k - \\bar{v})^2}},\n$$\nwhere $\\bar{u}$ and $\\bar{v}$ denote the means of $\\mathbf{u}$ and $\\mathbf{v}$ respectively.\n- A permutation test constructs a null distribution by breaking the mapping between stimuli across spaces. For RDMs, this is achieved by jointly permuting stimulus indices in one RDM, preserving its symmetry, and recomputing the alignment metric. The empirical $p$-value for a one-sided test of positive alignment is computed as\n$$\np = \\frac{1 + \\sum_{b=1}^{K} \\mathbb{I}\\big(r_b \\ge r_{\\text{obs}}\\big)}{1 + K},\n$$\nwhere $r_{\\text{obs}}$ is the observed Spearman correlation between the two RDMs, $r_b$ are the correlations under $K$ random permutations, and $\\mathbb{I}$ is the indicator function. The additive $1$ in numerator and denominator ensures a valid estimate even when no null samples exceed $r_{\\text{obs}}$.\n- The Benjamini–Hochberg procedure for controlling the False Discovery Rate (FDR) at a target level $q$ (expressed as a decimal, not a percentage) over $m$ hypotheses operates as follows: Sort the $p$-values $p_{(1)} \\le p_{(2)} \\le \\cdots \\le p_{(m)}$, find the largest index $i^\\star$ such that $p_{(i^\\star)} \\le \\frac{i^\\star}{m}q$, and declare all hypotheses with $p \\le p_{(i^\\star)}$ significant. If no such index exists, no hypothesis is declared significant.\n\nImplement the following requirements precisely:\n1. Construct RDMs for CNN layers and neural ROIs from synthetic stimulus feature responses as described below. For each layer–region pair, compute RSA alignment as the Spearman rank correlation between the vectorized upper-triangular entries of their RDMs.\n2. For each layer–region pair, perform a permutation test with $K$ permutations according to the null of no alignment (achieved by randomly permuting stimulus indices of exactly one RDM per pair). Compute the one-sided empirical $p$-value for positive alignment using the formula given above.\n3. Apply Benjamini–Hochberg FDR correction separately for each test case across all layer–region pairs in that case using the target level $q$.\n4. For each test case, output a single integer equal to the count of layer–region pairs that are significant after FDR correction.\n\nSynthetic data generation protocol:\n- Use independent, fixed random seeds for each test case to ensure determinism.\n- For each region, generate a matrix of feature responses $\\mathbf{X}^{(r)} \\in \\mathbb{R}^{n \\times d_r}$ for $n$ stimuli by sampling entries from a standard normal distribution.\n- For each layer, generate a matrix of feature responses $\\mathbf{Y}^{(l)} \\in \\mathbb{R}^{n \\times d_l}$ according to the design specified per test case below, aimed at producing a mixture of aligned and unaligned scenarios.\n\nRepresentational dissimilarity computation:\n- For any feature matrix $\\mathbf{Z} \\in \\mathbb{R}^{n \\times d}$, construct the RDM $\\mathbf{D} \\in \\mathbb{R}^{n \\times n}$ where $\\mathbf{D}_{ij} = \\left\\|\\mathbf{z}_i - \\mathbf{z}_j\\right\\|_2$ for $i \\neq j$ and $\\mathbf{D}_{ii} = 0$.\n\nRSA alignment metric:\n- For two RDMs $\\mathbf{D}^{(1)}$ and $\\mathbf{D}^{(2)}$, vectorize the strict upper-triangular entries into $\\mathbf{v}^{(1)}$ and $\\mathbf{v}^{(2)}$, compute Spearman rank correlation between $\\mathbf{v}^{(1)}$ and $\\mathbf{v}^{(2)}$, and denote the result by $r_{\\text{obs}}$.\n\nPermutation test:\n- For each permutation $b \\in \\{1,\\dots,K\\}$, sample a random permutation $\\pi_b$ of $\\{1,\\dots,n\\}$ and construct the permuted RDM $\\tilde{\\mathbf{D}}^{(2)}$ by applying $\\pi_b$ simultaneously to rows and columns of $\\mathbf{D}^{(2)}$, i.e., $\\tilde{\\mathbf{D}}^{(2)}_{ij} = \\mathbf{D}^{(2)}_{\\pi_b(i),\\pi_b(j)}$. Compute $r_b$ as the Spearman correlation between the upper-triangular vectors of $\\mathbf{D}^{(1)}$ and $\\tilde{\\mathbf{D}}^{(2)}$. Use the empirical $p$-value formula above.\n\nMultiple comparison correction:\n- Within each test case, pool all $p$-values across all layer–region pairs ($m$ total) and apply Benjamini–Hochberg at level $q$ as described above to produce a boolean decision per pair.\n\nTest suite and data design:\nImplement exactly the following three test cases.\n- Test case $1$ (happy path): $n=12$ stimuli, $L=3$ CNN layers, $R=2$ ROIs, $K=300$ permutations, $q=0.1$ (decimal). Random seed $0$. Data design: generate $\\mathbf{X}^{(1)}$ and $\\mathbf{X}^{(2)}$ with $d_1=5$ and $d_2=5$. Generate CNN layer features as follows: layer $1$ aligned to ROI $1$ via $\\mathbf{Y}^{(1)} = \\mathbf{X}^{(1)} + \\boldsymbol{\\epsilon}$ with $\\boldsymbol{\\epsilon}$ i.i.d. normal noise of standard deviation $0.01$; layer $2$ unaligned (independent standard normal features with $d_2'=6$); layer $3$ aligned to ROI $2$ via $\\mathbf{Y}^{(3)} = \\mathbf{X}^{(2)} + \\boldsymbol{\\epsilon}'$ with $\\boldsymbol{\\epsilon}'$ i.i.d. normal noise of standard deviation $0.01$. Compute FDR across all $L \\times R = 6$ pairs.\n- Test case $2$ (boundary condition): $n=5$ stimuli, $L=1$ CNN layer, $R=1$ ROI, $K=100$ permutations, $q=0.05$ (decimal). Random seed $1$. Data design: generate $\\mathbf{X}^{(1)}$ with $d_1=4$; set $\\mathbf{Y}^{(1)} = \\mathbf{X}^{(1)}$ exactly. Compute FDR across $1$ pair.\n- Test case $3$ (edge case with no alignment): $n=10$ stimuli, $L=2$ CNN layers, $R=2$ ROIs, $K=200$ permutations, $q=0.1$ (decimal). Random seed $2$. Data design: generate $\\mathbf{X}^{(1)}$ and $\\mathbf{X}^{(2)}$ with $d_1=6$ and $d_2=6$; set both layers to independent standard normal features with $d'_1=7$ and $d'_2=8$ (unaligned). Compute FDR across all $L \\times R = 4$ pairs.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3]$), where $r_t$ is an integer equal to the number of significant layer–region pairs after FDR correction for test case $t \\in \\{1,2,3\\}$.",
            "solution": "The problem statement is a valid, well-posed, and scientifically grounded exercise in computational neuroscience data analysis. It requires the implementation of a standard, albeit complex, statistical pipeline: Representational Similarity Analysis (RSA), permutation-based hypothesis testing, and multiple comparison correction using the Benjamini-Hochberg False Discovery Rate (FDR) procedure. All components are defined with mathematical and algorithmic precision, and the test cases are specified with sufficient detail (including random seeds) to ensure a unique, deterministic, and verifiable outcome.\n\nThe procedure will be implemented by adhering to the following scientific and mathematical principles.\n\n**Step 1: Problem Validation**\n\n*   **Extraction of Givens**:\n    *   **Core Method**: Representational Similarity Analysis (RSA) to compare convolutional neural network (CNN) layers and brain regions of interest (ROIs).\n    *   **Representational Dissimilarity Matrix (RDM)**: For $n$ stimuli, an $n \\times n$ symmetric matrix $\\mathbf{D}$ with $\\mathbf{D}_{ii}=0$. For feature vectors $\\mathbf{x}_i, \\mathbf{x}_j \\in \\mathbb{R}^d$, the dissimilarity is the Euclidean distance $\\mathbf{D}_{ij} = \\left\\|\\mathbf{x}_i - \\mathbf{x}_j\\right\\|_2$.\n    *   **Alignment Metric**: Spearman rank correlation, $\\rho$, between the vectorized upper-triangular entries of two RDMs.\n    *   **Permutation Test**: A null distribution is created by permuting stimulus labels for one RDM. For $K$ permutations, the one-sided empirical $p$-value for positive alignment is $p = \\frac{1 + \\sum_{b=1}^{K} \\mathbb{I}(r_b \\ge r_{\\text{obs}})}{1 + K}$, where $r_{\\text{obs}}$ is the observed correlation, $r_b$ are correlations from the null distribution, and $\\mathbb{I}$ is the indicator function. The permutation $\\pi_b$ is applied to an RDM $\\mathbf{D}^{(2)}$ as $\\tilde{\\mathbf{D}}^{(2)}_{ij} = \\mathbf{D}^{(2)}_{\\pi_b(i),\\pi_b(j)}$.\n    *   **Multiple Comparison Correction**: Benjamini–Hochberg procedure at a target FDR level $q$. For $m$ sorted $p$-values $p_{(1)} \\le \\cdots \\le p_{(m)}$, find the largest index $i^\\star$ such that $p_{(i^\\star)} \\le \\frac{i^\\star}{m}q$. Hypotheses with $p \\le p_{(i^\\star)}$ are deemed significant.\n    *   **Synthetic Data**: Generated using fixed random seeds. ROI features $\\mathbf{X}^{(r)} \\in \\mathbb{R}^{n \\times d_r}$ are from a standard normal distribution. CNN features $\\mathbf{Y}^{(l)} \\in \\mathbb{R}^{n \\times d_l}$ are generated as specified per case.\n    *   **Test Case 1**: $n=12$, $L=3$ layers, $R=2$ ROIs, $K=300$ permutations, $q=0.1$, seed=$0$. ROI features $\\mathbf{X}^{(1)}, \\mathbf{X}^{(2)} \\in \\mathbb{R}^{12 \\times 5}$. CNN features: $\\mathbf{Y}^{(1)} = \\mathbf{X}^{(1)} + \\boldsymbol{\\epsilon}$ (noise stddev $0.01$), $\\mathbf{Y}^{(2)} \\in \\mathbb{R}^{12 \\times 6}$ (independent standard normal), $\\mathbf{Y}^{(3)} = \\mathbf{X}^{(2)} + \\boldsymbol{\\epsilon}'$ (noise stddev $0.01$).\n    *   **Test Case 2**: $n=5$, $L=1$ layer, $R=1$ ROI, $K=100$ permutations, $q=0.05$, seed=$1$. ROI features $\\mathbf{X}^{(1)} \\in \\mathbb{R}^{5 \\times 4}$. CNN features: $\\mathbf{Y}^{(1)} = \\mathbf{X}^{(1)}$.\n    *   **Test Case 3**: $n=10$, $L=2$ layers, $R=2$ ROIs, $K=200$ permutations, $q=0.1$, seed=$2$. ROI features $\\mathbf{X}^{(1)}, \\mathbf{X}^{(2)} \\in \\mathbb{R}^{10 \\times 6}$. CNN features: $\\mathbf{Y}^{(1)} \\in \\mathbb{R}^{10 \\times 7}$, $\\mathbf{Y}^{(2)} \\in \\mathbb{R}^{10 \\times 8}$ (all independent standard normal).\n    *   **Output**: For each test case, the integer count of significant layer-region pairs after FDR correction.\n\n*   **Validation Verdict**: The problem is **valid**. It is scientifically grounded in established statistical methods within computational neuroscience, is well-posed with a clear and deterministic path to a solution, and its definitions are objective and unambiguous. It does not violate any of the invalidity criteria.\n\n**Principle-Based Solution Design**\n\nThe task requires a pipeline of several computational and statistical modules, which we will construct step-by-step.\n\n**1. RDM Construction**\nFor a given set of $n$ feature vectors (stimulus responses) $\\{\\mathbf{z}_1, \\dots, \\mathbf{z}_n\\}$, where each $\\mathbf{z}_i \\in \\mathbb{R}^d$, the Representational Dissimilarity Matrix (RDM), denoted $\\mathbf{D}$, is an $n \\times n$ matrix. Each entry $\\mathbf{D}_{ij}$ quantifies the dissimilarity between the representations of stimulus $i$ and stimulus $j$. As specified, we use the Euclidean distance:\n$$\n\\mathbf{D}_{ij} = \\left\\| \\mathbf{z}_i - \\mathbf{z}_j \\right\\|_2\n$$\nThe diagonal elements $\\mathbf{D}_{ii}$ are necessarily $0$, as the distance from a vector to itself is zero. The matrix is symmetric, since $\\mathbf{D}_{ij} = \\mathbf{D}_{ji}$.\n\n**2. RSA Alignment using Spearman Correlation**\nTo compare two representational geometries, one from a CNN layer and one from a neural ROI, we compute the similarity of their respective RDMs, $\\mathbf{D}^{(\\text{CNN})}$ and $\\mathbf{D}^{(\\text{ROI})}$. Since the RDMs are symmetric with zero diagonals, all information is contained in the upper (or lower) triangular part. We extract the strict upper-triangular elements of each RDM into vectors, let's call them $\\mathbf{v}^{(\\text{CNN})}$ and $\\mathbf{v}^{(\\text{ROI})}$. The number of such elements is $m_{\\text{RDM}} = n(n-1)/2$.\n\nThe alignment is then quantified by the Spearman rank correlation between these two vectors. Spearman's $\\rho$ is equivalent to the Pearson correlation coefficient calculated on the rank-transformed variables. This choice is principled as it is robust to non-linear but monotonic relationships between the dissimilarities of the two spaces, a common scenario in neural data analysis.\n\n**3. Hypothesis Testing via Permutations**\nTo assess if the observed correlation, $r_{\\text{obs}}$, is statistically significant, we must compare it to a null distribution. The null hypothesis, $H_0$, is that there is no systematic relationship between the stimulus representations in the CNN layer and the brain ROI. A permutation test is a non-parametric method to generate data under this null hypothesis.\n\nThe procedure is as follows: we take one of the RDMs, say $\\mathbf{D}^{(\\text{CNN})}$, and randomly permute its rows and columns according to the same permutation of stimulus labels. If $\\pi$ is a permutation of the indices $\\{0, 1, \\dots, n-1\\}$, the permuted RDM $\\tilde{\\mathbf{D}}^{(\\text{CNN})}$ has entries $\\tilde{\\mathbf{D}}_{ij}^{(\\text{CNN})} = \\mathbf{D}_{\\pi(i), \\pi(j)}^{(\\text{CNN})}$. This operation breaks the correspondence between the stimuli in the two RDMs while preserving the internal geometric structure of the permuted RDM.\n\nWe repeat this process $K$ times, each time generating a new random permutation $\\pi_b$ and calculating a null correlation coefficient $r_b$ between the vectorized upper triangles of $\\mathbf{D}^{(\\text{ROI})}$ and the permuted $\\tilde{\\mathbf{D}}^{(\\text{CNN})}$. This yields a null distribution $\\{r_1, r_2, \\dots, r_K\\}$.\n\nThe one-sided empirical $p$-value, testing for a positive correlation, is the proportion of null correlations that are greater than or equal to the observed correlation. The formula provided,\n$$\np = \\frac{1 + \\sum_{b=1}^{K} \\mathbb{I}(r_b \\ge r_{\\text{obs}})}{1 + K}\n$$\nis a standard approach to compute this. The addition of $1$ to the numerator and denominator acts as a pseudocount, ensuring that if $r_{\\text{obs}}$ is larger than all $r_b$, the $p$-value is $1/(K+1)$ rather than $0$, providing a more conservative and stable estimate.\n\n**4. Multiple Comparison Correction with FDR**\nWhen performing multiple hypothesis tests simultaneously (in our case, one for each CNN layer-ROI pair), the probability of making at least one false discovery (a Type I error) inflates. The Benjamini-Hochberg (BH) procedure controls the False Discovery Rate (FDR), which is the expected proportion of false positives among all rejected null hypotheses.\n\nFor each test case, we have $m = L \\times R$ comparisons, yielding $m$ p-values $\\{p_1, \\dots, p_m\\}$. The BH procedure is:\n1.  Order the $p$-values: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$. Let the original hypotheses corresponding to these sorted $p$-values be $H_{(1)}, \\dots, H_{(m)}$.\n2.  For a given FDR level $q$, find the largest integer $i^\\star$ such that the $i^\\star$-th smallest $p$-value satisfies:\n    $$\n    p_{(i^\\star)} \\le \\frac{i^\\star}{m} q\n    $$\n3.  If such an $i^\\star$ exists, we reject the null hypotheses for all tests $j$ where $p_j \\le p_{(i^\\star)}$. This means all hypotheses $H_{(1)}, \\dots, H_{(i^\\star)}$ are declared significant.\n4.  If no such $i^\\star$ exists, we fail to reject any null hypothesis.\n\nThe final output for each test case is the total count of pairs whose $p$-values meet this criterion. The implementation will carefully handle the 1-based indexing of the BH formula versus the 0-based indexing in programming arrays.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import spearmanr\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the execution of all test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path)\n        {\n            \"n\": 12, \"L\": 3, \"R\": 2, \"K\": 300, \"q\": 0.1, \"seed\": 0,\n            \"data_dims\": {\n                \"rois\": [5, 5],\n                \"layers\": [None, 6, None] # Dim depends on aligned ROI\n            },\n            \"alignments\": {\n                0: 0, # Layer 0 aligns with ROI 0\n                2: 1  # Layer 2 aligns with ROI 1\n            },\n            \"noise_std\": 0.01\n        },\n        # Test case 2 (boundary condition)\n        {\n            \"n\": 5, \"L\": 1, \"R\": 1, \"K\": 100, \"q\": 0.05, \"seed\": 1,\n            \"data_dims\": {\n                \"rois\": [4],\n                \"layers\": [None]\n            },\n            \"alignments\": {\n                0: 0 # Layer 0 aligns with ROI 0\n            },\n            \"noise_std\": 0.0 # Perfect alignment\n        },\n        # Test case 3 (edge case with no alignment)\n        {\n            \"n\": 10, \"L\": 2, \"R\": 2, \"K\": 200, \"q\": 0.1, \"seed\": 2,\n            \"data_dims\": {\n                \"rois\": [6, 6],\n                \"layers\": [7, 8]\n            },\n            \"alignments\": {}, # No alignments\n            \"noise_std\": 0.0\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(**case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_test_case(n, L, R, K, q, seed, data_dims, alignments, noise_std):\n    \"\"\"\n    Executes a single test case for RSA significance testing.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate synthetic data\n    roi_features = [rng.standard_normal(size=(n, d)) for d in data_dims[\"rois\"]]\n    \n    cnn_features = []\n    for l in range(L):\n        aligned_roi_idx = alignments.get(l)\n        if aligned_roi_idx is not None:\n            # Aligned layer\n            base_features = roi_features[aligned_roi_idx]\n            noise = rng.normal(scale=noise_std, size=base_features.shape)\n            cnn_features.append(base_features + noise)\n        else:\n            # Unaligned layer\n            d_l = data_dims[\"layers\"][l]\n            cnn_features.append(rng.standard_normal(size=(n, d_l)))\n\n    # 2. Compute RDMs\n    roi_rdms = [compute_rdm(features) for features in roi_features]\n    cnn_rdms = [compute_rdm(features) for features in cnn_features]\n\n    # Get upper triangle indices, which are constant for a given n\n    triu_indices = np.triu_indices(n, k=1)\n\n    p_values = []\n    # 3. Perform permutation test for each layer-region pair\n    for l in range(L):\n        rdm_cnn_vec = cnn_rdms[l][triu_indices]\n        for r in range(R):\n            rdm_roi_vec = roi_rdms[r][triu_indices]\n\n            # Observed correlation\n            r_obs, _ = spearmanr(rdm_cnn_vec, rdm_roi_vec)\n            \n            # Null distribution\n            null_correlations = np.zeros(K)\n            for b in range(K):\n                perm = rng.permutation(n)\n                # Permute one RDM (e.g., the CNN rdm)\n                rdm_cnn_permuted = cnn_rdms[l][perm, :][:, perm]\n                rdm_cnn_permuted_vec = rdm_cnn_permuted[triu_indices]\n                r_b, _ = spearmanr(rdm_cnn_permuted_vec, rdm_roi_vec)\n                null_correlations[b] = r_b\n            \n            # Empirical p-value\n            p_val = (1.0 + np.sum(null_correlations >= r_obs)) / (1.0 + K)\n            p_values.append(p_val)\n\n    # 4. Apply Benjamini-Hochberg FDR correction\n    p_values = np.array(p_values)\n    m = len(p_values)\n    \n    if m == 0:\n        return 0\n\n    # Sort p-values while keeping original indices\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = p_values[sorted_indices]\n    \n    # Find the largest i* such that p_(i*) = (i*/m)*q\n    i = np.arange(1, m + 1)\n    thresholds = (i / m) * q\n    \n    significant_mask = sorted_p_values = thresholds\n    \n    if not np.any(significant_mask):\n        return 0\n    else:\n        # Find the rank of the last p-value that was below its threshold\n        max_significant_index_in_sorted_array = np.where(significant_mask)[0].max()\n        # The p-value at this index is our significance threshold\n        p_threshold = sorted_p_values[max_significant_index_in_sorted_array]\n        \n        # Count how many of the original p-values are = this threshold\n        num_significant = np.sum(p_values = p_threshold)\n        return int(num_significant)\n\ndef compute_rdm(feature_matrix):\n    \"\"\"\n    Computes the Representational Dissimilarity Matrix (RDM)\n    from a feature matrix using Euclidean distance.\n    \"\"\"\n    # pdist computes the condensed distance matrix (upper triangle)\n    distances = pdist(feature_matrix, 'euclidean')\n    # squareform converts it to a full, symmetric matrix with zero diagonal\n    rdm = squareform(distances)\n    return rdm\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}