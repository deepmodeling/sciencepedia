{
    "hands_on_practices": [
        {
            "introduction": "本次实践将深入探讨“贝叶斯大脑”假说的核心机制。我们将推导一个理想观察者如何将关于世界的先验信念与带有噪声的感官证据进行最优结合。通过这个基础性的高斯分布示例，您将具体理解后验信念是如何成为先验和似然的精确度加权平均，这是规范模型中的一个基本计算。",
            "id": "4182819",
            "problem": "大脑功能的一个规范模型假定，神经系统在不确定性下执行概率推断，以估计潜在的感官原因。考虑一个简单的规范观察者，其任务是从内部观察值 $o$ 估计一个潜在的刺激强度 $s$。设潜在刺激 $s$ 具有高斯先验 $s \\sim \\mathcal{N}(\\mu_{0},\\sigma_{0}^{2})$，代表从经验中形成的先验期望；并设观察模型为带加性噪声的高斯模型，$o \\mid s \\sim \\mathcal{N}(s,\\sigma^{2})$，代表神经编码中的内部变异性。使用贝叶斯定理和平方误差损失的定义，推导后验分布 $p(s \\mid o)$ 的闭式解，并确定使期望平方误差损失最小化的估计量。具体要求：\n- 推导后验均值和后验方差 $p(s \\mid o)$，表示为关于 $\\mu_{0}$、$\\sigma_{0}^{2}$、$\\sigma^{2}$ 和 $o$ 的显式表达式。\n- 确定从 $o$ 到 $s$ 的估计值的映射，即该估计量能使期望平方误差损失最小化。\n\n请提供精确的、闭式解析表达式；不要引入数值或近似值。你的最终答案必须是一个单行矩阵，按顺序包含后验均值、后验方差和使平方误差损失最小化的估计量。",
            "solution": "该问题在贝叶斯推断框架内是适定的，并且具有科学依据，这是规范建模和计算神经科学中的一种标准方法。所有必要的组成部分都已提供，可以推导出唯一且有意义的解。\n\n我们给定一个代表刺激强度的潜变量 $s$ 和一个观察值 $o$。生成模型由 $s$ 的先验分布和给定 $s$ 时 $o$ 的似然函数指定。\n\n刺激 $s$ 的先验分布是高斯分布：\n$$s \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$$\n先验的概率密度函数 (PDF) 为：\n$$p(s) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{(s-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)$$\n\n观察模型，它定义了在给定刺激 $s$ 的情况下观察到 $o$ 的似然，也是高斯分布：\n$$o \\mid s \\sim \\mathcal{N}(s, \\sigma^{2})$$\n似然的概率密度函数 (PDF) 为：\n$$p(o \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(o-s)^{2}}{2\\sigma^{2}}\\right)$$\n注意，对于固定的 $o$，$p(o \\mid s)$ 作为 $s$ 的函数是似然函数，记为 $\\mathcal{L}(s; o)$。\n\n我们的第一个目标是使用贝叶斯定理推导后验分布 $p(s \\mid o)$：\n$$p(s \\mid o) = \\frac{p(o \\mid s) p(s)}{p(o)}$$\n分母 $p(o) = \\int p(o \\mid s) p(s) ds$ 是一个不依赖于 $s$ 的归一化常数。因此，我们可以使用正比关系：\n$$p(s \\mid o) \\propto p(o \\mid s) p(s)$$\n代入概率密度函数：\n$$p(s \\mid o) \\propto \\exp\\left(-\\frac{(o-s)^{2}}{2\\sigma^{2}}\\right) \\exp\\left(-\\frac{(s-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)$$\n$$p(s \\mid o) \\propto \\exp\\left(-\\left[\\frac{(s-o)^{2}}{2\\sigma^{2}} + \\frac{(s-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right]\\right)$$\n两个高斯函数的乘积是一个未归一化的高斯函数。为了找到其参数，我们通过展开平方项并按 $s$ 的幂次收集项来分析指数部分。设括号中的项为 $\\Phi(s)$：\n$$\\Phi(s) = \\frac{s^{2} - 2so + o^{2}}{2\\sigma^{2}} + \\frac{s^{2} - 2s\\mu_{0} + \\mu_{0}^{2}}{2\\sigma_{0}^{2}}$$\n$$\\Phi(s) = s^{2}\\left(\\frac{1}{2\\sigma^{2}} + \\frac{1}{2\\sigma_{0}^{2}}\\right) - s\\left(\\frac{o}{\\sigma^{2}} + \\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\right) + \\left(\\frac{o^{2}}{2\\sigma^{2}} + \\frac{\\mu_{0}^{2}}{2\\sigma_{0}^{2}}\\right)$$\n这个表达式是关于 $s$ 的二次式。我们可以通过配方法将其转换为高斯指数的规范形式，即 $-\\frac{(s - \\mu_{\\text{post}})^{2}}{2\\sigma_{\\text{post}}^{2}} + \\text{const}$ 的形式。通过比较 $\\Phi(s)$ 中 $s^2$ 和 $s$ 的系数与规范高斯形式中的系数，我们可以确定后验方差 $\\sigma_{\\text{post}}^{2}$ 和后验均值 $\\mu_{\\text{post}}$。\n\n高斯指数的一般形式是：\n$$-\\frac{(s-\\mu)^{2}}{2\\sigma^{2}} = -\\frac{s^{2}}{2\\sigma^{2}} + \\frac{s\\mu}{\\sigma^{2}} - \\frac{\\mu^{2}}{2\\sigma^{2}}$$\n将 $-\\Phi(s)$ 中 $s^{2}$ 的系数与 $-\\frac{1}{2\\sigma_{\\text{post}}^{2}}$ 进行比较：\n$$-\\left(\\frac{1}{2\\sigma^{2}} + \\frac{1}{2\\sigma_{0}^{2}}\\right) = -\\frac{1}{2\\sigma_{\\text{post}}^{2}}$$\n$$\\frac{1}{\\sigma_{\\text{post}}^{2}} = \\frac{1}{\\sigma^{2}} + \\frac{1}{\\sigma_{0}^{2}} = \\frac{\\sigma_{0}^{2} + \\sigma^{2}}{\\sigma^{2}\\sigma_{0}^{2}}$$\n这给出了后验方差（或者更精确地说，是它的倒数，即后验精度）：\n$$\\sigma_{\\text{post}}^{2} = \\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$$\n\n接下来，将 $-\\Phi(s)$ 中 $s$ 的系数与 $\\frac{\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^{2}}$ 进行比较：\n$$\\left(\\frac{o}{\\sigma^{2}} + \\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\right) = \\frac{\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^{2}}$$\n求解后验均值 $\\mu_{\\text{post}}$：\n$$\\mu_{\\text{post}} = \\sigma_{\\text{post}}^{2} \\left(\\frac{o}{\\sigma^{2}} + \\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\right)$$\n代入 $\\sigma_{\\text{post}}^{2}$ 的表达式：\n$$\\mu_{\\text{post}} = \\left(\\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}\\right) \\left(\\frac{o\\sigma_{0}^{2} + \\mu_{0}\\sigma^{2}}{\\sigma^{2}\\sigma_{0}^{2}}\\right)$$\n$$\\mu_{\\text{post}} = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$$\n因此，后验分布 $p(s \\mid o)$ 是一个高斯分布 $\\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$，其均值和方差如上推导。\n\n第二个目标是找到使期望平方误差损失最小化的估计量 $\\hat{s}(o)$。平方误差损失函数为 $L(s, \\hat{s}) = (s - \\hat{s})^2$。对于给定的观察值 $o$，期望损失（或风险）是该损失函数在 $s$ 的后验分布上的期望：\n$$R(\\hat{s}) = E_{s \\mid o}[(s - \\hat{s}(o))^2] = \\int_{-\\infty}^{\\infty} (s - \\hat{s}(o))^2 p(s \\mid o) ds$$\n为了找到使该风险最小化的估计量 $\\hat{s}(o)$，我们将 $R(\\hat{s})$ 对 $\\hat{s}$ 求导，并令结果为零：\n$$\\frac{d R(\\hat{s})}{d\\hat{s}} = \\frac{d}{d\\hat{s}} \\int_{-\\infty}^{\\infty} (s - \\hat{s})^{2} p(s \\mid o) ds$$\n$$= \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial\\hat{s}} (s - \\hat{s})^{2} p(s \\mid o) ds = \\int_{-\\infty}^{\\infty} 2(s - \\hat{s})(-1) p(s \\mid o) ds$$\n$$= -2 \\left( \\int_{-\\infty}^{\\infty} s p(s \\mid o) ds - \\hat{s} \\int_{-\\infty}^{\\infty} p(s \\mid o) ds \\right)$$\n令导数为零：\n$$\\int_{-\\infty}^{\\infty} s p(s \\mid o) ds - \\hat{s} \\int_{-\\infty}^{\\infty} p(s \\mid o) ds = 0$$\n第一项是后验均值 $E[s \\mid o]$ 的定义。第二个积分为 $1$，因为 $p(s \\mid o)$ 是一个概率密度函数。\n$$E[s \\mid o] - \\hat{s} = 0$$\n$$\\hat{s} = E[s \\mid o]$$\n使期望平方误差损失最小化的估计量是后验分布的均值。我们已经推导出它就是 $\\mu_{\\text{post}}$。\n因此，最优估计量为：\n$$\\hat{s}(o) = \\mu_{\\text{post}} = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$$\n\n所要求的三个量是后验均值、后验方差和最优估计量。\n后验均值：$\\mu_{\\text{post}} = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$\n后验方差：$\\sigma_{\\text{post}}^{2} = \\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$\n最优估计量：$\\hat{s}(o) = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}} & \\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}} & \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}} \\end{pmatrix} } $$"
        },
        {
            "introduction": "了解了如何结合信息之后，我们现在要问：一开始有多少信息可供利用？本练习引入费雪信息（Fisher Information）这一关键工具，用以量化神经元集群编码刺激的精确度。通过为标准的泊松（Poisson）群体编码推导费雪信息，您将看到诸如调谐曲线形状和放电率等因素如何决定了感官估计的最终极限，正如克拉默-拉奥下界（Cramér-Rao bound）所定义的那样。",
            "id": "4182833",
            "problem": "考虑大脑功能规范建模中的一个群体编码：大脑中的一个下游估计器试图从一个由$M$个神经元组成的群体在持续时间为$T$的固定观测窗口内记录的脉冲计数中推断出标量刺激参数$\\theta$。假设在给定$\\theta$的条件下，神经元是条件独立的，并且神经元$i$产生的脉冲计数$n_i$服从均值为$T r_i(\\theta)$的泊松分布，其中$r_i(\\theta)$是一个可微的调谐函数。该群体的联合似然由所有神经元的泊松概率质量函数的乘积给出。假设在生成模型下，下游估计器$\\hat{\\theta}$对于$\\theta$是无偏的。\n\n仅使用统计估计理论的基本定义——即似然函数、对数似然、定义为对数似然关于$\\theta$的导数的分数函数，以及由柯西-施瓦茨不等式对无偏估计量所给出的方差下界——推导这个独立泊松群体中关于$\\theta$的费雪信息（FI）的表达式，将其与克拉默-拉奥下界（CRB）结合以获得任何无偏估计量的方差界，并确定在调谐函数$r_i(\\theta) \\mapsto c\\, r_i(\\theta)$（其中$c > 0$为常数）的均匀增益缩放变换下，该界如何变化。\n\n哪个选项正确地陈述了该模型的FI、任何无偏估计器$\\hat{\\theta}$的CRB，以及均匀增益缩放对该界的影响？\n\nA. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)}$，$\\mathrm{Var}(\\hat{\\theta}) \\ge \\dfrac{1}{I(\\theta)}$，并且在$r_i(\\theta) \\mapsto c\\, r_i(\\theta)$变换下，有$I(\\theta) \\mapsto c\\, I(\\theta)$，因此$\\mathrm{Var}(\\hat{\\theta}) \\ge \\dfrac{1}{c\\, I(\\theta)}$。\n\nB. $I(\\theta) = T \\sum_{i=1}^{M} \\left(\\partial_{\\theta} \\log r_i(\\theta)\\right)^2$，$\\mathrm{Var}(\\hat{\\theta}) \\ge \\dfrac{1}{I(\\theta)}$，并且在$r_i(\\theta) \\mapsto c\\, r_i(\\theta)$变换下，$I(\\theta)$不变，所以该界也不变。\n\nC. $I(\\theta) = T \\sum_{i=1}^{M} r_i(\\theta)$，$\\mathrm{Var}(\\hat{\\theta}) \\ge \\dfrac{1}{I(\\theta)}$，并且在$r_i(\\theta) \\mapsto c\\, r_i(\\theta)$变换下，有$I(\\theta) \\mapsto c\\, I(\\theta)$，因此$\\mathrm{Var}(\\hat{\\theta}) \\ge \\dfrac{1}{c\\, I(\\theta)}$。\n\nD. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\partial_{\\theta} r_i(\\theta)}{r_i(\\theta)}$，$\\mathrm{Var}(\\hat{\\theta}) \\ge \\dfrac{1}{I(\\theta)}$，并且在$r_i(\\theta) \\mapsto c\\, r_i(\\theta)$变换下，$I(\\theta)$不变，所以该界也不变。\n\n请给出你的选择，并基于一个从联合泊松似然开始，经过分数函数及其在该模型下的二阶矩的推导来证明你的选择。",
            "solution": "问题陈述已经过验证，被认为是合理的。它描述了理论神经科学中的一个标准模型，并要求从统计估计理论进行基本推导。所有术语都有明确定义，前提在科学上和数学上都是一致的。\n\n我们现在将为给定的神经元群体推导费雪信息（FI），陈述相应的克拉默-拉奥下界（CRB），并分析均匀增益缩放对调谐函数的影响。\n\n假设神经元$i$的脉冲计数$n_i$服从均值为$\\lambda_i(\\theta) = T r_i(\\theta)$的泊松分布。单个神经元的概率质量函数（PMF）为：\n$$\nP(n_i | \\theta) = \\frac{(\\lambda_i(\\theta))^{n_i} e^{-\\lambda_i(\\theta)}}{n_i!} = \\frac{(T r_i(\\theta))^{n_i} e^{-T r_i(\\theta)}}{n_i!}\n$$\n\n鉴于神经元的条件独立性，群体脉冲计数$\\mathbf{n} = (n_1, \\ldots, n_M)$的联合似然是单个PMF的乘积：\n$$\nL(\\theta; \\mathbf{n}) = \\prod_{i=1}^{M} P(n_i | \\theta) = \\prod_{i=1}^{M} \\frac{(T r_i(\\theta))^{n_i} e^{-T r_i(\\theta)}}{n_i!}\n$$\n\n对数似然$\\mathcal{L}(\\theta) = \\log L(\\theta; \\mathbf{n})$为：\n$$\n\\mathcal{L}(\\theta) = \\sum_{i=1}^{M} \\log P(n_i | \\theta) = \\sum_{i=1}^{M} \\left( n_i \\log(T r_i(\\theta)) - T r_i(\\theta) - \\log(n_i!) \\right)\n$$\n我们可以将依赖于$\\theta$的项分离开来：\n$$\n\\mathcal{L}(\\theta) = \\sum_{i=1}^{M} \\left( n_i \\log r_i(\\theta) - T r_i(\\theta) \\right) + \\sum_{i=1}^{M} \\left( n_i \\log T - \\log(n_i!) \\right)\n$$\n\n分数函数$S(\\theta)$是对数似然关于参数$\\theta$的导数：\n$$\nS(\\theta) = \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\sum_{i=1}^{M} \\left( n_i \\log r_i(\\theta) - T r_i(\\theta) \\right)\n$$\n使用链式法则，并将$\\partial_{\\theta} r_i(\\theta)$记为$r_i(\\theta)$关于$\\theta$的导数：\n$$\nS(\\theta) = \\sum_{i=1}^{M} \\left( \\frac{n_i}{r_i(\\theta)} \\partial_{\\theta} r_i(\\theta) - T \\partial_{\\theta} r_i(\\theta) \\right) = \\sum_{i=1}^{M} \\left( \\frac{n_i}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta)\n$$\n\n对于一个正则估计问题，费雪信息$I(\\theta)$定义为分数函数的方差，即$I(\\theta) = \\mathrm{Var}(S(\\theta))$。（正则似然的一个性质是分数的期望为零，$E[S(\\theta)]=0$，因此方差为$E[S(\\theta)^2]$）。我们首先验证$E[S(\\theta)]=0$：\n$$\nE[S(\\theta)] = E \\left[ \\sum_{i=1}^{M} \\left( \\frac{n_i}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta) \\right] = \\sum_{i=1}^{M} \\left( \\frac{E[n_i]}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta)\n$$\n由于$E[n_i] = T r_i(\\theta)$：\n$$\nE[S(\\theta)] = \\sum_{i=1}^{M} \\left( \\frac{T r_i(\\theta)}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta) = \\sum_{i=1}^{M} (T - T) \\partial_{\\theta} r_i(\\theta) = 0\n$$\n由于期望为零，所以$I(\\theta) = \\mathrm{Var}(S(\\theta)) = E[S(\\theta)^2]$。因为神经元是条件独立的，所以脉冲计数$n_i$是独立变量。分数$S(\\theta)$是各项之和，每项对应一个神经元，因此和的方差等于各项方差之和：\n$$\nI(\\theta) = \\mathrm{Var}(S(\\theta)) = \\sum_{i=1}^{M} \\mathrm{Var}\\left( \\left( \\frac{n_i}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta) \\right)\n$$\n导数项$\\partial_{\\theta} r_i(\\theta)$不是随机变量，所以我们可以将其因子提出（以平方的形式）：\n$$\nI(\\theta) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\mathrm{Var}\\left( \\frac{n_i}{r_i(\\theta)} - T \\right) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\mathrm{Var}\\left( \\frac{n_i}{r_i(\\theta)} \\right)\n$$\n使用性质$\\mathrm{Var}(aX) = a^2 \\mathrm{Var}(X)$：\n$$\nI(\\theta) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\frac{1}{(r_i(\\theta))^2} \\mathrm{Var}(n_i)\n$$\n对于泊松分布，方差等于均值：$\\mathrm{Var}(n_i) = E[n_i] = T r_i(\\theta)$。将其代入：\n$$\nI(\\theta) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\frac{1}{(r_i(\\theta))^2} (T r_i(\\theta)) = \\sum_{i=1}^{M} T \\frac{(\\partial_{\\theta} r_i(\\theta))^2}{r_i(\\theta)}\n$$\n这可以更紧凑地写为：\n$$\nI(\\theta) = T \\sum_{i=1}^{M} \\frac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)}\n$$\n克拉默-拉奥下界（CRB）指出，对于参数$\\theta$的任何无偏估计量$\\hat{\\theta}$，其方差的下界是费雪信息的倒数：\n$$\n\\mathrm{Var}(\\hat{\\theta}) \\ge \\frac{1}{I(\\theta)}\n$$\n\n现在，我们来分析均匀增益缩放$r_i(\\theta) \\mapsto \\tilde{r}_i(\\theta) = c \\, r_i(\\theta)$（其中$c > 0$为常数）的影响。我们计算新的费雪信息$\\tilde{I}(\\theta)$。新的调谐函数的导数是：\n$$\n\\partial_{\\theta} \\tilde{r}_i(\\theta) = \\partial_{\\theta} (c \\, r_i(\\theta)) = c \\, \\partial_{\\theta} r_i(\\theta)\n$$\n将这些代入费雪信息的公式中：\n$$\n\\tilde{I}(\\theta) = T \\sum_{i=1}^{M} \\frac{\\left(\\partial_{\\theta} \\tilde{r}_i(\\theta)\\right)^2}{\\tilde{r}_i(\\theta)} = T \\sum_{i=1}^{M} \\frac{\\left(c \\, \\partial_{\\theta} r_i(\\theta)\\right)^2}{c \\, r_i(\\theta)} = T \\sum_{i=1}^{M} \\frac{c^2 \\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{c \\, r_i(\\theta)}\n$$\n$$\n\\tilde{I}(\\theta) = c \\left( T \\sum_{i=1}^{M} \\frac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)} \\right) = c \\, I(\\theta)\n$$\n费雪信息被因子$c$缩放。因此，新的方差界为：\n$$\n\\mathrm{Var}(\\hat{\\theta}) \\ge \\frac{1}{\\tilde{I}(\\theta)} = \\frac{1}{c \\, I(\\theta)}\n$$\n\n现在我们根据我们的推导来评估给定的选项。\n\nA. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)}$，$\\mathrm{Var}(\\hat{\\theta}) \\ge \\dfrac{1}{I(\\theta)}$，并且在$r_i(\\theta) \\mapsto c\\, r_i(\\theta)$变换下，有$I(\\theta) \\mapsto c\\, I(\\theta)$，因此$\\mathrm{Var}(\\hat{\\theta}) \\ge \\dfrac{1}{c\\, I(\\theta)}$。\n此选项与我们推导出的费雪信息表达式、克拉默-拉奥下界的陈述以及我们对增益缩放的分析完全匹配。\n**结论：正确。**\n\nB. $I(\\theta) = T \\sum_{i=1}^{M} \\left(\\partial_{\\theta} \\log r_i(\\theta)\\right)^2$。\n这个表达式展开为 $T \\sum_{i=1}^{M} \\left(\\frac{\\partial_{\\theta} r_i(\\theta)}{r_i(\\theta)}\\right)^2 = T \\sum_{i=1}^{M} \\frac{(\\partial_{\\theta} r_i(\\theta))^2}{(r_i(\\theta))^2}$。这与我们推导的公式 $T \\sum_{i=1}^{M} \\frac{(\\partial_{\\theta} r_i(\\theta))^2}{r_i(\\theta)}$ 在分母上相差一个因子$r_i(\\theta)$。其缩放分析也是不正确的，因为它基于这个错误的FI公式。\n**结论：错误。**\n\nC. $I(\\theta) = T \\sum_{i=1}^{M} r_i(\\theta)$。\n这个费雪信息的表达式是错误的。它表示在时间间隔$T$内群体中总的期望脉冲数，但它完全忽略了调谐函数的导数$\\partial_{\\theta} r_i(\\theta)$，而这才是关于$\\theta$的信息来源。\n**结论：错误。**\n\nD. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\partial_{\\theta} r_i(\\theta)}{r_i(\\theta)}$。\n这个表达式是错误的。它缺少了导数项上的平方，即$\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2$。这也使得它在量纲上与费雪信息不一致。\n**结论：错误。**\n\n基于严谨的推导，只有选项A是正确的。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "真实的神经元集群表现出相关的活动，这是简化模型中被忽略的一个特征。这项高级实践探讨了这种相关性对信息编码的深远影响。通过分析一个具有共享噪声的神经元集群，您将推导出相关性在何种条件下会“限制”信息，即增加神经元数量不再能提高编码保真度，这是理解大规模神经编码效率和约束的一个关键概念。",
            "id": "4182855",
            "problem": "在神经科学数据分析中，一个核心的规范性原则是，对于从神经元群体响应中估计标量刺激 $s$ 的问题，克拉美-罗界（Cramér–Rao bound）指出，任何无偏估计量的方差下界由费雪信息（Fisher information, FI）的倒数决定。考虑一个包含 $N$ 个神经元的群体，其对刺激 $s$ 的响应向量 $\\mathbf{r} \\in \\mathbb{R}^N$ 被建模为均值为 $\\mathbf{f}(s)$ 且协方差 $\\boldsymbol{\\Sigma}$ 与刺激无关的高斯分布，因此 $p(\\mathbf{r} \\mid s) = \\mathcal{N}(\\mathbf{f}(s), \\boldsymbol{\\Sigma})$。令 $\\mathbf{g}(s) \\equiv \\partial \\mathbf{f}(s) / \\partial s$ 表示局部敏感度向量。假设 $\\boldsymbol{\\Sigma}$ 分解为一个独立噪声项加上一个共享的低秩项，形式为 $\\boldsymbol{\\Sigma} = \\mathbf{A} + \\tau^{2} \\mathbf{u} \\mathbf{u}^{\\top}$，其中 $\\mathbf{A} = \\operatorname{diag}(\\sigma_{1}^{2}, \\dots, \\sigma_{N}^{2})$ 是对角矩阵，$\\tau^{2} > 0$，且 $\\mathbf{u} \\in \\mathbb{R}^{N}$ 是一个固定的共享变异性方向。从费雪信息的定义 $J(s) = \\mathbb{E}\\left[\\left(\\partial \\ln p(\\mathbf{r} \\mid s)/\\partial s\\right)^{2}\\right]$ 出发，推导出一个关于 $\\mathbf{g}(s)$、$\\mathbf{A}$、$\\mathbf{u}$ 和 $\\tau^{2}$ 的 $J(s)$ 的解析表达式，并用它来确定共享噪声方向 $\\mathbf{u}$ 上产生信息限制性相关的结构条件，即当 $N$ 增加时 $J(s)$ 不会无界增长。然后，将问题特化到一个工作点 $s_{0}$，在该点上所有神经元具有相同的局部敏感度 $\\mathbf{g}(s_{0}) = \\alpha \\mathbf{1}$（其中 $\\alpha > 0$），以及相同的独立噪声 $\\mathbf{A} = \\sigma^{2} \\mathbf{I}$（其中 $\\sigma^{2} > 0$）。此外，令共享噪声方向与敏感度方向一致，即 $\\mathbf{u} = \\mathbf{g}(s_{0})$。在这种特化情况下，计算精确的费雪信息 $J(s_{0})$ 并确定其在 $N \\to \\infty$ 时的极限。请将 $J(s_{0})$ 的极限值，以 $\\tau$ 的单个闭式解析表达式的形式，作为最终答案报告。不需要数值近似，最终答案中也不应包含单位。",
            "solution": "基本的出发点是标量参数 $s$ 的费雪信息的定义，由 $J(s) = \\mathbb{E}\\left[\\left(\\partial \\ln p(\\mathbf{r} \\mid s)/\\partial s\\right)^{2}\\right]$ 给出，其中期望是关于 $p(\\mathbf{r} \\mid s)$ 计算的。对于均值依赖于刺激 $\\mathbf{f}(s)$、协方差与刺激无关 $\\boldsymbol{\\Sigma}$ 的多元正态分布，其对数似然为\n$$\n\\ln p(\\mathbf{r} \\mid s) = -\\frac{1}{2}(\\mathbf{r} - \\mathbf{f}(s))^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r} - \\mathbf{f}(s)) - \\frac{1}{2} \\ln \\det(2\\pi \\boldsymbol{\\Sigma}).\n$$\n因为 $\\boldsymbol{\\Sigma}$ 不依赖于 $s$，所以对 $s$ 的导数只作用于 $\\mathbf{f}(s)$：\n$$\n\\frac{\\partial}{\\partial s} \\ln p(\\mathbf{r} \\mid s) = \\left(\\frac{\\partial \\mathbf{f}(s)}{\\partial s}\\right)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\left(\\mathbf{r} - \\mathbf{f}(s)\\right) = \\mathbf{g}(s)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\left(\\mathbf{r} - \\mathbf{f}(s)\\right).\n$$\n对平方项关于 $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{f}(s), \\boldsymbol{\\Sigma})$ 取期望，我们使用 $\\mathbb{E}[(\\mathbf{r} - \\mathbf{f}(s))(\\mathbf{r} - \\mathbf{f}(s))^{\\top}] = \\boldsymbol{\\Sigma}$ 来得到\n$$\nJ(s) = \\mathbb{E}\\left[\\left(\\mathbf{g}(s)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\left(\\mathbf{r} - \\mathbf{f}(s)\\right)\\right)^{2}\\right] = \\mathbf{g}(s)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma}^{-1} \\mathbf{g}(s) = \\mathbf{g}(s)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\mathbf{g}(s).\n$$\n因此，费雪信息简化为 $\\boldsymbol{\\Sigma}^{-1}$ 沿着敏感度方向 $\\mathbf{g}(s)$ 的二次型。\n\n接下来，对于 $\\boldsymbol{\\Sigma} = \\mathbf{A} + \\tau^{2} \\mathbf{u} \\mathbf{u}^{\\top}$，我们应用Woodbury矩阵恒等式（一个经过充分检验的线性代数结果）来求 $\\boldsymbol{\\Sigma}$ 的逆。令 $\\mathbf{A}$ 可逆，并定义 $\\mathbf{U} = \\tau \\mathbf{u}$ 和 $\\mathbf{V}^{\\top} = (\\tau \\mathbf{u})^{\\top}$，因此 $\\boldsymbol{\\Sigma} = \\mathbf{A} + \\mathbf{U} \\mathbf{V}^{\\top}$。Woodbury恒等式给出\n$$\n\\boldsymbol{\\Sigma}^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} \\left(\\mathbf{I} + \\mathbf{V}^{\\top} \\mathbf{A}^{-1} \\mathbf{U}\\right)^{-1} \\mathbf{V}^{\\top} \\mathbf{A}^{-1}.\n$$\n因为 $\\mathbf{U}$ 和 $\\mathbf{V}$ 是向量，所以 $\\mathbf{I} + \\mathbf{V}^{\\top} \\mathbf{A}^{-1} \\mathbf{U}$ 是标量 $1 + \\tau^{2} \\mathbf{u}^{\\top} \\mathbf{A}^{-1} \\mathbf{u}$，因此\n$$\n\\boldsymbol{\\Sigma}^{-1} = \\mathbf{A}^{-1} - \\frac{\\tau^{2}}{1 + \\tau^{2} \\mathbf{u}^{\\top} \\mathbf{A}^{-1} \\mathbf{u}} \\, \\mathbf{A}^{-1} \\mathbf{u} \\mathbf{u}^{\\top} \\mathbf{A}^{-1}.\n$$\n代入 $J(s) = \\mathbf{g}(s)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\mathbf{g}(s)$ 可得\n$$\nJ(s) = \\mathbf{g}(s)^{\\top} \\mathbf{A}^{-1} \\mathbf{g}(s) - \\frac{\\tau^{2}}{1 + \\tau^{2} \\mathbf{u}^{\\top} \\mathbf{A}^{-1} \\mathbf{u}} \\left(\\mathbf{g}(s)^{\\top} \\mathbf{A}^{-1} \\mathbf{u}\\right)^{2}.\n$$\n该表达式明确地显示了相关噪声如何通过敏感度方向 $\\mathbf{g}(s)$ 和共享噪声方向 $\\mathbf{u}$ 在由 $\\mathbf{A}^{-1}$ 导出的度量下的重叠来影响费雪信息。\n\n为了确定信息限制性相关的结构条件，我们考虑 $J(s)$ 如何随 $N$ 变化。假设独立噪声项 $\\mathbf{A}$ 的方差有界（例如，其元素数量级为常数），并且敏感度向量 $\\mathbf{g}(s)$ 的元素不随 $N$ 减小（例如，一个均匀的敏感度分布）。那么第一项 $\\mathbf{g}(s)^{\\top} \\mathbf{A}^{-1} \\mathbf{g}(s)$ 通常按 $N$ 的数量级缩放。第二项减去了一个非负量，该量与平方重叠 $\\left(\\mathbf{g}(s)^{\\top} \\mathbf{A}^{-1} \\mathbf{u}\\right)^{2}$ 成正比，并由 $1 + \\tau^{2} \\mathbf{u}^{\\top} \\mathbf{A}^{-1} \\mathbf{u}$ 归一化。为了使这个减法抵消掉随 $N$ 线性增长的项并产生有限的渐近线，共享噪声方向必须在 $\\mathbf{A}^{-1}$-度量下与敏感度方向对齐，这样分子和分母都与 $\\mathbf{g}(s)^{\\top} \\mathbf{A}^{-1} \\mathbf{g}(s)$ 成比例缩放，从而得到一个有界的比率。具体来说，信息限制的条件是 $\\mathbf{u}$ 有一个与 $\\mathbf{g}(s)$ 成比例的分量，并且主要的共享噪声分量沿着 $\\mathbf{g}(s)$ 的方向；在严格的秩为1的情况下，即 $\\mathbf{u} \\propto \\mathbf{g}(s)$，极限是有限的。如果 $\\mathbf{u}$ 在 $\\mathbf{A}^{-1}$-度量下与 $\\mathbf{g}(s)$ 正交，则第二项消失，$J(s)$ 会随 $N$ 无界增长。\n\n我们现在特化到所述的工作点 $s_{0}$，其中 $\\mathbf{g}(s_{0}) = \\alpha \\mathbf{1}$，$\\mathbf{A} = \\sigma^{2} \\mathbf{I}$，且 $\\mathbf{u} = \\mathbf{g}(s_{0})$。在这些假设下，\n$$\n\\mathbf{A}^{-1} = \\frac{1}{\\sigma^{2}} \\mathbf{I}, \\quad \\mathbf{g}(s_{0})^{\\top} \\mathbf{A}^{-1} \\mathbf{g}(s_{0}) = \\frac{1}{\\sigma^{2}} \\mathbf{g}(s_{0})^{\\top} \\mathbf{g}(s_{0}) = \\frac{\\alpha^{2} N}{\\sigma^{2}}.\n$$\n因为 $\\mathbf{u} = \\mathbf{g}(s_{0})$，我们也有\n$$\n\\mathbf{u}^{\\top} \\mathbf{A}^{-1} \\mathbf{u} = \\frac{\\alpha^{2} N}{\\sigma^{2}}, \\quad \\mathbf{g}(s_{0})^{\\top} \\mathbf{A}^{-1} \\mathbf{u} = \\frac{\\alpha^{2} N}{\\sigma^{2}}.\n$$\n代入 $J(s)$ 的一般表达式可得\n$$\nJ(s_{0}) = \\frac{\\alpha^{2} N}{\\sigma^{2}} - \\frac{\\tau^{2}}{1 + \\tau^{2} \\frac{\\alpha^{2} N}{\\sigma^{2}}} \\left(\\frac{\\alpha^{2} N}{\\sigma^{2}}\\right)^{2}.\n$$\n这在代数上可以如下简化。令 $x \\equiv \\frac{\\alpha^{2} N}{\\sigma^{2}}$。那么\n$$\nJ(s_{0}) = x - \\frac{\\tau^{2} x^{2}}{1 + \\tau^{2} x} = \\frac{x(1 + \\tau^{2} x) - \\tau^{2} x^{2}}{1 + \\tau^{2} x} = \\frac{x}{1 + \\tau^{2} x}.\n$$\n因此，\n$$\nJ(s_{0}) = \\frac{\\frac{\\alpha^{2} N}{\\sigma^{2}}}{1 + \\tau^{2} \\frac{\\alpha^{2} N}{\\sigma^{2}}}.\n$$\n取 $N \\to \\infty$ 的极限，我们注意到分子和分母中按 $\\tau^{2}$ 缩放的项都随 $N$ 线性增长，其比值趋向于 $\\tau^{2}$ 的倒数：\n$$\n\\lim_{N \\to \\infty} J(s_{0}) = \\lim_{N \\to \\infty} \\frac{\\frac{\\alpha^{2} N}{\\sigma^{2}}}{1 + \\tau^{2} \\frac{\\alpha^{2} N}{\\sigma^{2}}} = \\frac{1}{\\tau^{2}}.\n$$\n这个有限的渐近线是信息限制性相关的标志：与敏感度方向 $\\mathbf{g}(s)$ 对齐的共享噪声分量在群体规模增长时限制了可达到的费雪信息。\n\n因此，在所述的特化情况下，当 $N \\to \\infty$ 时，极限费雪信息是 $\\frac{1}{\\tau^{2}}$，这就是所要求的以 $\\tau$ 表示的闭式解析表达式。",
            "answer": "$$\\boxed{\\frac{1}{\\tau^{2}}}$$"
        }
    ]
}