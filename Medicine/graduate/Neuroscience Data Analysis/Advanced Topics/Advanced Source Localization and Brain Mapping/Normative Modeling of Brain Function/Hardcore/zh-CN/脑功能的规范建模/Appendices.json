{
    "hands_on_practices": [
        {
            "introduction": "规范模型的核心思想是，大脑通过概率推断来处理不确定性。本练习将带您深入探讨这一核心机制，通过一个基础的贝叶斯推断问题，您将推导如何将先验知识（先前经验）与感觉证据（似然）相结合，以形成对外部世界的最佳估计（后验）。通过这个练习，您将亲手揭示理想观察者如何根据不确定性来权衡证据和信念。",
            "id": "4182819",
            "problem": "一个关于大脑功能的规范模型假设，神经系统在不确定性下执行概率推断，以估计潜在的感官原因。考虑一个简单的规范观察者，其任务是从内部观测 $o$ 中估计潜在的刺激强度 $s$。设潜在刺激 $s$ 具有高斯先验 $s \\sim \\mathcal{N}(\\mu_{0},\\sigma_{0}^{2})$，代表从经验中形成的先验预期；并设观测模型为带加性噪声的高斯模型，$o \\mid s \\sim \\mathcal{N}(s,\\sigma^{2})$，代表神经编码中的内部变异性。使用贝叶斯定理和平方误差损失的定义，推导闭式后验分布 $p(s \\mid o)$，并确定最小化期望平方误差损失的估计量。具体来说：\n- 将 $p(s \\mid o)$ 的后验均值和后验方差推导为以 $\\mu_{0}$、$\\sigma_{0}^{2}$、$\\sigma^{2}$ 和 $o$ 表示的显式表达式。\n- 确定从 $o$ 映射到 $s$ 的估计量的映射，该估计量能最小化期望平方误差损失。\n\n提供精确的闭式解析表达式；不要引入数值或近似值。你的最终答案必须是一个单行矩阵，按顺序包含后验均值、后验方差和最小化平方误差损失的估计量。",
            "solution": "这个问题在贝叶斯推断的框架内是适定的，并且有科学依据，这是规范建模和计算神经科学中的一种标准方法。所有必要的组成部分都已提供，可以推导出唯一且有意义的解。\n\n我们给定一个代表刺激强度的潜在变量 $s$ 和一个观测值 $o$。生成模型由 $s$ 的先验分布和给定 $s$ 时 $o$ 的似然函数指定。\n\n刺激 $s$ 的先验分布是高斯分布：\n$$s \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$$\n先验的概率密度函数 (PDF) 为：\n$$p(s) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{(s-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)$$\n\n观测模型也为高斯模型，它定义了在给定刺激 $s$ 的情况下观测到 $o$ 的似然：\n$$o \\mid s \\sim \\mathcal{N}(s, \\sigma^{2})$$\n似然的 PDF 为：\n$$p(o \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(o-s)^{2}}{2\\sigma^{2}}\\right)$$\n注意，对于固定的 $o$，$p(o \\mid s)$ 作为 $s$ 的函数是似然函数，记为 $\\mathcal{L}(s; o)$。\n\n我们的第一个目标是使用贝叶斯定理推导后验分布 $p(s \\mid o)$：\n$$p(s \\mid o) = \\frac{p(o \\mid s) p(s)}{p(o)}$$\n分母 $p(o) = \\int p(o \\mid s) p(s) ds$ 是一个不依赖于 $s$ 的归一化常数。因此，我们可以使用正比关系：\n$$p(s \\mid o) \\propto p(o \\mid s) p(s)$$\n代入 PDF：\n$$p(s \\mid o) \\propto \\exp\\left(-\\frac{(o-s)^{2}}{2\\sigma^{2}}\\right) \\exp\\left(-\\frac{(s-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)$$\n$$p(s \\mid o) \\propto \\exp\\left(-\\left[\\frac{(s-o)^{2}}{2\\sigma^{2}} + \\frac{(s-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right]\\right)$$\n两个高斯函数的乘积是一个未归一化的高斯函数。为了找到其参数，我们通过展开平方项并按 $s$ 的幂次收集项来分析指数。设方括号中的项为 $\\Phi(s)$：\n$$\\Phi(s) = \\frac{s^{2} - 2so + o^{2}}{2\\sigma^{2}} + \\frac{s^{2} - 2s\\mu_{0} + \\mu_{0}^{2}}{2\\sigma_{0}^{2}}$$\n$$\\Phi(s) = s^{2}\\left(\\frac{1}{2\\sigma^{2}} + \\frac{1}{2\\sigma_{0}^{2}}\\right) - s\\left(\\frac{o}{\\sigma^{2}} + \\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\right) + \\left(\\frac{o^{2}}{2\\sigma^{2}} + \\frac{\\mu_{0}^{2}}{2\\sigma_{0}^{2}}\\right)$$\n这个表达式是关于 $s$ 的二次式。我们可以通过配方法将其转换为高斯指数的标准形式，即 $-\\frac{(s - \\mu_{\\text{post}})^{2}}{2\\sigma_{\\text{post}}^{2}} + \\text{const}$。通过比较 $\\Phi(s)$ 中 $s^2$ 和 $s$ 的系数与高斯标准形式中的系数，我们可以确定后验方差 $\\sigma_{\\text{post}}^{2}$ 和后验均值 $\\mu_{\\text{post}}$。\n\n高斯指数的一般形式是：\n$$-\\frac{(s-\\mu)^{2}}{2\\sigma^{2}} = -\\frac{s^{2}}{2\\sigma^{2}} + \\frac{s\\mu}{\\sigma^{2}} - \\frac{\\mu^{2}}{2\\sigma^{2}}$$\n将 $-\\Phi(s)$ 中 $s^{2}$ 的系数与 $-\\frac{1}{2\\sigma_{\\text{post}}^{2}}$ 进行比较：\n$$-\\left(\\frac{1}{2\\sigma^{2}} + \\frac{1}{2\\sigma_{0}^{2}}\\right) = -\\frac{1}{2\\sigma_{\\text{post}}^{2}}$$\n$$\\frac{1}{\\sigma_{\\text{post}}^{2}} = \\frac{1}{\\sigma^{2}} + \\frac{1}{\\sigma_{0}^{2}} = \\frac{\\sigma_{0}^{2} + \\sigma^{2}}{\\sigma^{2}\\sigma_{0}^{2}}$$\n这就给出了后验方差（或者更精确地说，是它的倒数，即后验精度）：\n$$\\sigma_{\\text{post}}^{2} = \\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$$\n\n接下来，将 $-\\Phi(s)$ 中 $s$ 的系数与 $\\frac{\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^{2}}$ 进行比较：\n$$\\left(\\frac{o}{\\sigma^{2}} + \\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\right) = \\frac{\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^{2}}$$\n求解后验均值 $\\mu_{\\text{post}}$：\n$$\\mu_{\\text{post}} = \\sigma_{\\text{post}}^{2} \\left(\\frac{o}{\\sigma^{2}} + \\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\right)$$\n代入 $\\sigma_{\\text{post}}^{2}$ 的表达式：\n$$\\mu_{\\text{post}} = \\left(\\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}\\right) \\left(\\frac{o\\sigma_{0}^{2} + \\mu_{0}\\sigma^{2}}{\\sigma^{2}\\sigma_{0}^{2}}\\right)$$\n$$\\mu_{\\text{post}} = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$$\n因此，后验分布 $p(s \\mid o)$ 是一个高斯分布 $\\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$，其均值和方差如上所推导。\n\n第二个目标是找到最小化期望平方误差损失的估计量 $\\hat{s}(o)$。平方误差损失函数为 $L(s, \\hat{s}) = (s - \\hat{s})^2$。期望损失（或风险）是该损失函数在给定观测 $o$ 的情况下对 $s$ 的后验分布的期望：\n$$R(\\hat{s}) = E_{s \\mid o}[(s - \\hat{s}(o))^2] = \\int_{-\\infty}^{\\infty} (s - \\hat{s}(o))^2 p(s \\mid o) ds$$\n为了找到最小化此风险的估计量 $\\hat{s}(o)$，我们将 $R(\\hat{s})$ 对 $\\hat{s}$ 求导，并令结果为零：\n$$\\frac{d R(\\hat{s})}{d\\hat{s}} = \\frac{d}{d\\hat{s}} \\int_{-\\infty}^{\\infty} (s - \\hat{s})^{2} p(s \\mid o) ds$$\n$$= \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial\\hat{s}} (s - \\hat{s})^{2} p(s \\mid o) ds = \\int_{-\\infty}^{\\infty} 2(s - \\hat{s})(-1) p(s \\mid o) ds$$\n$$= -2 \\left( \\int_{-\\infty}^{\\infty} s p(s \\mid o) ds - \\hat{s} \\int_{-\\infty}^{\\infty} p(s \\mid o) ds \\right)$$\n令导数为零：\n$$\\int_{-\\infty}^{\\infty} s p(s \\mid o) ds - \\hat{s} \\int_{-\\infty}^{\\infty} p(s \\mid o) ds = 0$$\n第一项是后验均值 $E[s \\mid o]$ 的定义。第二个积分为 $1$，因为 $p(s \\mid o)$ 是一个概率密度函数。\n$$E[s \\mid o] - \\hat{s} = 0$$\n$$\\hat{s} = E[s \\mid o]$$\n最小化期望平方误差损失的估计量是后验分布的均值。我们已经推导出它就是 $\\mu_{\\text{post}}$。\n因此，最优估计量是：\n$$\\hat{s}(o) = \\mu_{\\text{post}} = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$$\n\n所要求的三个量是后验均值、后验方差和最优估计量。\n后验均值： $\\mu_{\\text{post}} = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$\n后验方差： $\\sigma_{\\text{post}}^{2} = \\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$\n最优估计量： $\\hat{s}(o) = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}} & \\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}} & \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}} \\end{pmatrix} } $$"
        },
        {
            "introduction": "在理解了大脑如何形成估计之后，下一个自然的问题是：这种估计的精度极限在哪里？本练习引入了费雪信息（Fisher Information）这一强大工具，它量化了神经活动（如脉冲计数）中包含的关于外部刺激的信息量。您将为一个泊松神经元群体推导费雪信息，并应用克拉默-拉奥下界（Cramér-Rao Bound），从而确定任何无偏估计器所能达到的最高精度。这使我们能够从理论上理解神经编码的效率和局限性。",
            "id": "4182833",
            "problem": "考虑在脑功能规范建模中的一个种群编码：大脑中的一个下游估计器试图从一个由 $M$ 个神经元组成的种群在持续时间为 $T$ 的固定观测窗口内记录到的脉冲计数中，推断一个标量刺激参数 $\\theta$。假设在给定 $\\theta$ 的条件下神经元是条件独立的，并且神经元 $i$ 产生的脉冲计数 $n_i$ 服从均值为 $T r_i(\\theta)$ 的泊松分布，其中 $r_i(\\theta)$ 是一个可微的调谐函数。该种群的联合似然由各个神经元的泊松概率质量函数的乘积给出。在生成模型下，下游估计器 $\\hat{\\theta}$ 被假定为 $\\theta$ 的无偏估计器。\n\n仅使用统计估计理论中的基本定义——即似然函数、对数似然、定义为对数似然相对于 $\\theta$ 的导数的得分函数，以及由柯西-施瓦茨不等式对无偏估计器所推导出的方差下界——推导在这个独立泊松种群中关于 $\\theta$ 的费雪信息（FI）的表达式，将其与克拉默-拉奥界（CRB）结合以获得任何无偏估计器的方差界，并确定在调谐函数 $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$（其中 $c > 0$ 为常数）的均匀增益缩放下方差界如何变化。\n\n哪个选项正确地陈述了该模型的费雪信息（FI）、任何无偏估计器 $\\hat{\\theta}$ 的克拉默-拉奥界（CRB），以及均匀增益缩放对该界的影响？\n\nA. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)}$，$\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{I(\\theta)}$，并且在 $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$ 的变换下，我们有 $I(\\theta) \\mapsto c\\, I(\\theta)$，因此 $\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{c\\, I(\\theta)}$。\n\nB. $I(\\theta) = T \\sum_{i=1}^{M} \\left(\\partial_{\\theta} \\log r_i(\\theta)\\right)^2$，$\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{I(\\theta)}$，并且在 $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$ 的变换下，$I(\\theta)$ 不变，所以界也不变。\n\nC. $I(\\theta) = T \\sum_{i=1}^{M} r_i(\\theta)$，$\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{I(\\theta)}$，并且在 $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$ 的变换下，我们有 $I(\\theta) \\mapsto c\\, I(\\theta)$，因此 $\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{c\\, I(\\theta)}$。\n\nD. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\partial_{\\theta} r_i(\\theta)}{r_i(\\theta)}$，$\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{I(\\theta)}$，并且在 $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$ 的变换下，$I(\\theta)$ 不变，所以界也不变。\n\n请给出你的选择，并基于从联合泊松似然开始，经过得分函数及其在模型下的二阶矩的推导来证明你的选择。",
            "solution": "问题陈述已经过验证，并且被认为是合理的。它描述了理论神经科学中的一个标准模型，并要求从统计估计理论进行基本推导。所有术语都有明确的定义，前提在科学上和数学上都是一致的。\n\n我们现在将为给定的神经元种群推导费雪信息（FI），陈述相应的克拉默-拉奥界（CRB），并分析均匀增益缩放对调谐函数的影响。\n\n神经元 $i$ 的脉冲计数 $n_i$ 假定服从均值为 $\\lambda_i(\\theta) = T r_i(\\theta)$ 的泊松分布。单个神经元的概率质量函数（PMF）为：\n$$\nP(n_i | \\theta) = \\frac{(\\lambda_i(\\theta))^{n_i} e^{-\\lambda_i(\\theta)}}{n_i!} = \\frac{(T r_i(\\theta))^{n_i} e^{-T r_i(\\theta)}}{n_i!}\n$$\n\n鉴于神经元的条件独立性，种群脉冲计数 $\\mathbf{n} = (n_1, \\ldots, n_M)$ 的联合似然是单个概率质量函数的乘积：\n$$\nL(\\theta; \\mathbf{n}) = \\prod_{i=1}^{M} P(n_i | \\theta) = \\prod_{i=1}^{M} \\frac{(T r_i(\\theta))^{n_i} e^{-T r_i(\\theta)}}{n_i!}\n$$\n\n对数似然 $\\mathcal{L}(\\theta) = \\log L(\\theta; \\mathbf{n})$ 为：\n$$\n\\mathcal{L}(\\theta) = \\sum_{i=1}^{M} \\log P(n_i | \\theta) = \\sum_{i=1}^{M} \\left( n_i \\log(T r_i(\\theta)) - T r_i(\\theta) - \\log(n_i!) \\right)\n$$\n我们可以分离出依赖于 $\\theta$ 的项：\n$$\n\\mathcal{L}(\\theta) = \\sum_{i=1}^{M} \\left( n_i \\log r_i(\\theta) - T r_i(\\theta) \\right) + \\sum_{i=1}^{M} \\left( n_i \\log T - \\log(n_i!) \\right)\n$$\n\n得分函数 $S(\\theta)$ 是对数似然函数关于参数 $\\theta$ 的导数：\n$$\nS(\\theta) = \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\sum_{i=1}^{M} \\left( n_i \\log r_i(\\theta) - T r_i(\\theta) \\right)\n$$\n使用链式法则，并将 $r_i(\\theta)$ 关于 $\\theta$ 的导数记为 $\\partial_{\\theta} r_i(\\theta)$：\n$$\nS(\\theta) = \\sum_{i=1}^{M} \\left( \\frac{n_i}{r_i(\\theta)} \\partial_{\\theta} r_i(\\theta) - T \\partial_{\\theta} r_i(\\theta) \\right) = \\sum_{i=1}^{M} \\left( \\frac{n_i}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta)\n$$\n\n对于一个正则估计问题，费雪信息 $I(\\theta)$ 定义为得分函数的方差，即 $I(\\theta) = \\mathrm{Var}(S(\\theta))$。（正则似然的一个性质是得分函数的期望为零，即 $E[S(\\theta)]=0$，因此方差为 $E[S(\\theta)^2]$）。我们首先验证 $E[S(\\theta)]=0$：\n$$\nE[S(\\theta)] = E \\left[ \\sum_{i=1}^{M} \\left( \\frac{n_i}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta) \\right] = \\sum_{i=1}^{M} \\left( \\frac{E[n_i]}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta)\n$$\n因为 $E[n_i] = T r_i(\\theta)$：\n$$\nE[S(\\theta)] = \\sum_{i=1}^{M} \\left( \\frac{T r_i(\\theta)}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta) = \\sum_{i=1}^{M} (T - T) \\partial_{\\theta} r_i(\\theta) = 0\n$$\n由于期望为零，所以 $I(\\theta) = \\mathrm{Var}(S(\\theta)) = E[S(\\theta)^2]$。因为神经元是条件独立的，所以脉冲计数 $n_i$ 是独立的变量。得分函数 $S(\\theta)$ 是每个神经元对应项的总和，因此和的方差是方差的和：\n$$\nI(\\theta) = \\mathrm{Var}(S(\\theta)) = \\sum_{i=1}^{M} \\mathrm{Var}\\left( \\left( \\frac{n_i}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta) \\right)\n$$\n导数项 $\\partial_{\\theta} r_i(\\theta)$ 不是随机变量，所以我们可以将其（平方后）提出来：\n$$\nI(\\theta) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\mathrm{Var}\\left( \\frac{n_i}{r_i(\\theta)} - T \\right) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\mathrm{Var}\\left( \\frac{n_i}{r_i(\\theta)} \\right)\n$$\n使用性质 $\\mathrm{Var}(aX) = a^2 \\mathrm{Var}(X)$：\n$$\nI(\\theta) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\frac{1}{(r_i(\\theta))^2} \\mathrm{Var}(n_i)\n$$\n对于泊松分布，方差等于均值：$\\mathrm{Var}(n_i) = E[n_i] = T r_i(\\theta)$。将其代入：\n$$\nI(\\theta) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\frac{1}{(r_i(\\theta))^2} (T r_i(\\theta)) = \\sum_{i=1}^{M} T \\frac{(\\partial_{\\theta} r_i(\\theta))^2}{r_i(\\theta)}\n$$\n这可以更紧凑地写成：\n$$\nI(\\theta) = T \\sum_{i=1}^{M} \\frac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)}\n$$\n克拉默-拉奥界（CRB）指出，对于参数 $\\theta$ 的任何无偏估计器 $\\hat{\\theta}$，其方差的下界是费雪信息的倒数：\n$$\n\\mathrm{Var}(\\hat{\\theta}) \\ge \\frac{1}{I(\\theta)}\n$$\n\n现在，我们来分析均匀增益缩放 $r_i(\\theta) \\mapsto \\tilde{r}_i(\\theta) = c \\, r_i(\\theta)$（其中常数 $c > 0$）的影响。我们计算新的费雪信息 $\\tilde{I}(\\theta)$。新的调谐函数的导数是：\n$$\n\\partial_{\\theta} \\tilde{r}_i(\\theta) = \\partial_{\\theta} (c \\, r_i(\\theta)) = c \\, \\partial_{\\theta} r_i(\\theta)\n$$\n将这些代入费雪信息的公式中：\n$$\n\\tilde{I}(\\theta) = T \\sum_{i=1}^{M} \\frac{\\left(\\partial_{\\theta} \\tilde{r}_i(\\theta)\\right)^2}{\\tilde{r}_i(\\theta)} = T \\sum_{i=1}^{M} \\frac{\\left(c \\, \\partial_{\\theta} r_i(\\theta)\\right)^2}{c \\, r_i(\\theta)} = T \\sum_{i=1}^{M} \\frac{c^2 \\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{c \\, r_i(\\theta)}\n$$\n$$\n\\tilde{I}(\\theta) = c \\left( T \\sum_{i=1}^{M} \\frac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)} \\right) = c \\, I(\\theta)\n$$\n费雪信息被因子 $c$ 缩放。因此，新的方差界是：\n$$\n\\mathrm{Var}(\\hat{\\theta}) \\ge \\frac{1}{\\tilde{I}(\\theta)} = \\frac{1}{c \\, I(\\theta)}\n$$\n\n现在我们根据我们的推导来评估给出的选项。\n\nA. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)}$，$\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{I(\\theta)}$，并且在 $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$ 的变换下，我们有 $I(\\theta) \\mapsto c\\, I(\\theta)$，因此 $\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{c\\, I(\\theta)}$。\n此选项完美匹配我们推导出的费雪信息表达式、克拉默-拉奥界的陈述以及我们对增益缩放的分析。\n**结论：正确。**\n\nB. $I(\\theta) = T \\sum_{i=1}^{M} \\left(\\partial_{\\theta} \\log r_i(\\theta)\\right)^2$。\n该表达式展开为 $T \\sum_{i=1}^{M} \\left(\\frac{\\partial_{\\theta} r_i(\\theta)}{r_i(\\theta)}\\right)^2 = T \\sum_{i=1}^{M} \\frac{(\\partial_{\\theta} r_i(\\theta))^2}{(r_i(\\theta))^2}$。这与我们推导出的公式 $T \\sum_{i=1}^{M} \\frac{(\\partial_{\\theta} r_i(\\theta))^2}{r_i(\\theta)}$ 在分母上相差一个因子 $r_i(\\theta)$。其缩放分析也是不正确的，因为它基于这个错误的 FI 公式。\n**结论：不正确。**\n\nC. $I(\\theta) = T \\sum_{i=1}^{M} r_i(\\theta)$。\n这个费雪信息的表达式是不正确的。它代表了在时间间隔 $T$ 内种群的总期望脉冲数，但它完全忽略了调谐函数的导数 $\\partial_{\\theta} r_i(\\theta)$，而这才是关于 $\\theta$ 的信息来源。\n**结论：不正确。**\n\nD. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\partial_{\\theta} r_i(\\theta)}{r_i(\\theta)}$。\n这个表达式是不正确的。它缺少了导数项上的平方，即 $\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2$。这也使得它在量纲上与费雪信息不一致。\n**结论：不正确。**\n\n基于严谨的推导，只有选项 A 是正确的。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "大脑的计算并非没有成本，它受到严格的代谢资源限制。因此，一个完整的规范模型必须考虑编码精度与生物成本之间的权衡。本练习将信息论中的率失真理论（Rate-Distortion Theory）应用于神经编码，通过推导高斯信源的率失真函数，您将能定量地分析在给定的失真（误差）水平下，维持神经表征所需的最小信息率（成本）。这揭示了大脑在资源受限的情况下如何实现高效编码的深刻原理。",
            "id": "4182886",
            "problem": "考虑一个单个皮层神经元的规范模型，其中一个潜标量刺激 $X$ 必须在资源约束下被编码为神经表示 $\\hat{X}$。该刺激被建模为 $X \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$，其中 $\\sigma_{x}^{2}$ 是 $X$ 的方差。编码成本由互信息 $I(X; \\hat{X})$ 衡量，保真度由均方误差 (MSE) 失真 $d(x, \\hat{x}) = (x - \\hat{x})^{2}$ 衡量。在这种规范设定下，作为可容许失真函数的最小所需信息率由 Shannon 的率失真理论中的率失真 (RD) 函数给出：\n$$\nR(D) = \\inf_{p(\\hat{x} \\mid x): \\ \\mathbb{E}\\big[(X - \\hat{X})^{2}\\big] \\leq D} I(X; \\hat{X}),\n$$\n其中 $D$ 是失真预算。\n\n从互信息和 RD 函数的定义出发，仅使用高斯分布和带拉格朗日乘子的凸优化的基本性质，推导在均方误差失真度量下，对于 $0  D  \\sigma_{x}^{2}$ 的情况，$R(D)$ 的闭式表达式（单位为奈特/样本）。然后，计算导数 $\\frac{d}{dD} R(D)$，并将其符号和大小解释为在规范模型中控制降低失真的边际率成本的权衡参数。最终的率以奈特/样本表示，斜率以奈特/单位失真表示。无需四舍五入。以解析形式提供你的最终答案，形式为序对 $\\big(R(D), \\frac{d}{dD} R(D)\\big)$。",
            "solution": "用户希望通过一个严谨的问题验证过程来解决该问题。\n\n### 步骤 1：提取已知条件\n- 潜标量刺激是一个随机变量 $X$，其分布为 $X \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$。\n- 神经表示由 $\\hat{X}$ 表示。\n- 编码成本是互信息 $I(X; \\hat{X})$。\n- 失真度量是均方误差 (MSE)，$d(x, \\hat{x}) = (x - \\hat{x})^{2}$。\n- 率失真 (RD) 函数定义为 $R(D) = \\inf_{p(\\hat{x} \\mid x): \\ \\mathbb{E}\\big[(X - \\hat{X})^{2}\\big] \\leq D} I(X; \\hat{X})$。\n- 可容许失真预算 $D$ 的范围是 $0  D  \\sigma_{x}^{2}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n所提出的问题是 Shannon 率失真理论中的一个经典基础问题。\n- **科学依据：** 该问题牢固地植根于信息论。对于高斯源和均方误差失真度量，率失真函数的特性是该领域的一个基石性成果。其在神经科学规范建模中的应用是一种标准且广为接受的方法。前提在科学上和数学上都是合理的。\n- **适定性：** 该问题是适定的。目标函数 $I(X; \\hat{X})$ 需要在一组满足凸约束 $\\mathbb{E}[(X - \\hat{X})^{2}] \\leq D$ 的条件分布 $p(\\hat{x}|x)$ 上进行最小化。对于高斯源和均方误差失真，已知存在唯一解，并且率失真函数是良定义的。指定的范围 $0  D  \\sigma_{x}^{2}$ 是该问题的非平凡域；对于 $D \\geq \\sigma_{x}^{2}$，率为零；对于 $D \\to 0$，率趋于无穷大。\n- **客观性：** 问题陈述以精确、客观的数学语言表达。所有使用的术语（$X \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$，$I(X; \\hat{X})$，MSE）都有严谨、明确的定义。\n\n### 步骤 3：结论与行动\n该问题是有效的。它是一个来自信息论的标准、自洽且适定的问题。我将继续进行推导。\n\n### 推导\n问题是求解高斯源的率失真函数 $R(D)$。率由互信息 $I(X; \\hat{X})$ 衡量，它可以表示为微分熵的形式：\n$$\nI(X; \\hat{X}) = h(X) - h(X | \\hat{X})\n$$\n其中 $h(X)$ 是源 $X$ 的微分熵，$h(X | \\hat{X})$ 是给定其表示 $\\hat{X}$ 的 $X$ 的条件微分熵。\n\n源 $X$ 服从高斯分布，$X \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$。其微分熵是一个常数，由下式给出：\n$$\nh(X) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{x}^{2})\n$$\n由于 $h(X)$ 相对于编码分布 $p(\\hat{x}|x)$ 的选择是恒定的，因此最小化 $I(X; \\hat{X})$ 等价于最大化条件熵 $h(X | \\hat{X})$。\n因此，问题可以重述为：\n$$\nR(D) = h(X) - \\sup_{p(\\hat{x} \\mid x): \\ \\mathbb{E}\\big[(X - \\hat{X})^{2}\\big] \\leq D} h(X | \\hat{X})\n$$\n这是一个约束优化问题。我们引入一个拉格朗日乘子 $\\beta > 0$ 来求解。目标是最大化拉格朗日量：\n$$\nL = h(X | \\hat{X}) - \\beta \\mathbb{E}\\big[(X - \\hat{X})^{2}\\big]\n$$\n条件熵可以写成关于 $\\hat{X}$ 的期望：$h(X|\\hat{X}) = \\mathbb{E}_{\\hat{X}}[h(X|\\hat{X}=\\hat{x})]$。类似地，期望失真是 $\\mathbb{E}[(X - \\hat{X})^2] = \\mathbb{E}_{\\hat{X}}[\\mathbb{E}[(X - \\hat{X})^2 | \\hat{X}=\\hat{x}]]$。可以对每个 $\\hat{x}$ 的值独立进行优化。对于一个固定的 $\\hat{x}$，我们必须选择条件概率密度 $p(x|\\hat{x})$ 来最大化：\n$$\nh(p(x|\\hat{x})) - \\beta \\mathbb{E}[(X - \\hat{x})^2 | \\hat{X}=\\hat{x}]\n$$\n设条件均值为 $\\mathbb{E}[X|\\hat{X}=\\hat{x}] = \\mu_{\\hat{x}}$，条件方差为 $\\mathbb{E}[(X-\\mu_{\\hat{x}})^2|\\hat{X}=\\hat{x}] = \\sigma_{X|\\hat{x}}^2$。项 $\\mathbb{E}[(X - \\hat{x})^2 | \\hat{X}=\\hat{x}]$ 可以展开为：\n$$\n\\mathbb{E}[(X - \\mu_{\\hat{x}} + \\mu_{\\hat{x}} - \\hat{x})^2 | \\hat{X}=\\hat{x}] = \\sigma_{X|\\hat{x}}^2 + (\\mu_{\\hat{x}} - \\hat{x})^2\n$$\n已知最优表示 $\\hat{X}$ 应该是源的条件均值，即 $\\hat{X} = \\mathbb{E}[X|\\hat{X}]$。对于给定的表示，这个选择最小化了均方误差。在这种标准解释下，均方误差成为条件方差，$\\mathbb{E}[(X - \\hat{X})^2] = \\mathbb{E}[\\sigma_{X|\\hat{X}}^2]$。对于每个 $\\hat{x}$，需要最大化的拉格朗日量简化为在 $p(x|\\hat{x})$ 的选择上最大化 $h(p(x|\\hat{x})) - \\beta \\sigma_{X|\\hat{x}}^2$。\n\n对于一个固定的方差 $\\sigma^2$，最大化微分熵 $h$ 的分布是高斯分布，其 $h = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)$。因此，对于每一个 $\\hat{x}$，最优的条件分布 $p(x|\\hat{x})$ 都必须是高斯分布。设其方差为 $\\sigma_{X|\\hat{x}}^2$。我们需要最大化：\n$$\nf(\\sigma_{X|\\hat{x}}^2) = \\frac{1}{2}\\ln(2\\pi e \\sigma_{X|\\hat{x}}^2) - \\beta \\sigma_{X|\\hat{x}}^2\n$$\n关于 $\\sigma_{X|\\hat{x}}^2$。求导并令其为零，得到：\n$$\n\\frac{d f}{d (\\sigma_{X|\\hat{x}}^2)} = \\frac{1}{2\\sigma_{X|\\hat{x}}^2} - \\beta = 0 \\implies \\sigma_{X|\\hat{x}}^2 = \\frac{1}{2\\beta}\n$$\n这个结果表明，最优的条件方差是一个常数，与 $\\hat{x}$ 的值无关。在最优点，均方误差约束是激活的，所以我们有 $D = \\mathbb{E}[\\sigma_{X|\\hat{x}}^2] = \\sigma_{X|\\hat{x}}^2 = \\frac{1}{2\\beta}$。\n\n因此，当条件分布 $p(x|\\hat{x})$ 是一个方差为 $D$ 的高斯分布时，达到最大条件熵。这个最大熵的值为：\n$$\n\\sup h(X | \\hat{X}) = h(\\mathcal{N}(\\cdot, D)) = \\frac{1}{2}\\ln(2\\pi e D)\n$$\n我们现在可以将其代回率失真函数的表达式中：\n$$\nR(D) = h(X) - \\sup h(X | \\hat{X}) = \\frac{1}{2}\\ln(2\\pi e \\sigma_{x}^{2}) - \\frac{1}{2}\\ln(2\\pi e D)\n$$\n$$\nR(D) = \\frac{1}{2} \\left[ \\ln(2\\pi e \\sigma_{x}^{2}) - \\ln(2\\pi e D) \\right] = \\frac{1}{2} \\ln\\left(\\frac{2\\pi e \\sigma_{x}^{2}}{2\\pi e D}\\right)\n$$\n这简化为率失真函数的最终表达式：\n$$\nR(D) = \\frac{1}{2} \\ln\\left(\\frac{\\sigma_{x}^{2}}{D}\\right)\n$$\n该表达式对于 $0  D \\leq \\sigma_{x}^{2}$ 有效。由于使用了自然对数，率的单位是奈特/样本。\n\n接下来，我们计算 $R(D)$ 对 $D$ 的导数：\n$$\nR(D) = \\frac{1}{2}(\\ln(\\sigma_{x}^{2}) - \\ln(D))\n$$\n$$\n\\frac{d}{dD} R(D) = \\frac{d}{dD} \\left( \\frac{1}{2}\\ln(\\sigma_{x}^{2}) - \\frac{1}{2}\\ln(D) \\right) = 0 - \\frac{1}{2} \\cdot \\frac{1}{D}\n$$\n$$\n\\frac{d}{dD} R(D) = -\\frac{1}{2D}\n$$\n这个斜率的单位是奈特/单位失真。\n\n对于 $D>0$，导数的符号为负，这表示所需的信息率 $R(D)$ 随着允许的失真 $D$ 的增加而减小。这是一个直观的权衡：更高的保真度（更低的 $D$）需要更高的信息率。斜率的大小 $|\\frac{d}{dD} R(D)| = \\frac{1}{2D}$，代表了失真增量变化的边际率成本。当 $D$ 趋近于 $0$ 时，这个成本发散到无穷大，表明实现完美重构（$D=0$）需要无穷大的信息率。这个斜率与拉格朗日乘子直接相关，因为 $\\frac{dR}{dD} = -\\beta$。我们的计算得出 $\\frac{dR}{dD} = -\\frac{1}{2D}$，这与我们之前发现的 $\\beta = \\frac{1}{2D}$ 一致。\n\n最终答案由序对 $\\big(R(D), \\frac{d}{dD} R(D)\\big)$ 组成。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2} \\ln\\left(\\frac{\\sigma_{x}^{2}}{D}\\right)  -\\frac{1}{2D} \\end{pmatrix}}\n$$"
        }
    ]
}