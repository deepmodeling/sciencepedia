{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of normative modeling is the \"Bayesian brain\" hypothesis, which posits that the brain performs optimal inference by combining prior knowledge with sensory evidence. This exercise provides hands-on practice with the core mechanics of this process, asking you to derive the posterior distribution for the canonical case of a Gaussian prior and Gaussian likelihood . Mastering this derivation solidifies the mathematical foundation for understanding how an ideal observer should update its beliefs and reveals the form of the estimator that minimizes decision error.",
            "id": "4182819",
            "problem": "A normative model of brain function posits that neural systems perform probabilistic inference to estimate latent sensory causes under uncertainty. Consider a simple normative observer tasked with estimating a latent stimulus intensity $s$ from an internal observation $o$. Let the latent stimulus $s$ have a Gaussian prior $s \\sim \\mathcal{N}(\\mu_{0},\\sigma_{0}^{2})$, representing prior expectations formed from experience, and let the observation model be Gaussian with additive noise, $o \\mid s \\sim \\mathcal{N}(s,\\sigma^{2})$, representing internal variability in neural encoding. Using Bayes' theorem and the definition of squared error loss, derive the closed-form posterior distribution $p(s \\mid o)$ and determine the estimator that minimizes expected squared error loss. Specifically:\n- Derive the posterior mean and posterior variance of $p(s \\mid o)$ as explicit expressions in terms of $\\mu_{0}$, $\\sigma_{0}^{2}$, $\\sigma^{2}$, and $o$.\n- Identify the estimator mapping from $o$ to an estimate of $s$ that minimizes the expected squared error loss.\n\nProvide exact, closed-form analytic expressions; do not introduce numerical values or approximations. Your final answer must be a single row matrix containing, in order, the posterior mean, the posterior variance, and the estimator that minimizes squared error loss.",
            "solution": "The problem is well-posed and scientifically grounded within the framework of Bayesian inference, a standard approach in normative modeling and computational neuroscience. All necessary components are provided to derive a unique, meaningful solution.\n\nWe are given a latent variable $s$ representing stimulus intensity and an observation $o$. The generative model is specified by a prior distribution for $s$ and a likelihood function for $o$ given $s$.\n\nThe prior distribution for the stimulus $s$ is Gaussian:\n$$s \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$$\nThe probability density function (PDF) for the prior is:\n$$p(s) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{(s-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)$$\n\nThe observation model, which defines the likelihood of observing $o$ given a stimulus $s$, is also Gaussian:\n$$o \\mid s \\sim \\mathcal{N}(s, \\sigma^{2})$$\nThe PDF for the likelihood is:\n$$p(o \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(o-s)^{2}}{2\\sigma^{2}}\\right)$$\nNote that $p(o \\mid s)$ as a function of $s$ for a fixed $o$ is the likelihood function, denoted $\\mathcal{L}(s; o)$.\n\nOur first objective is to derive the posterior distribution $p(s \\mid o)$ using Bayes' theorem:\n$$p(s \\mid o) = \\frac{p(o \\mid s) p(s)}{p(o)}$$\nThe denominator, $p(o) = \\int p(o \\mid s) p(s) ds$, is a normalization constant that does not depend on $s$. Therefore, we can work with proportionality:\n$$p(s \\mid o) \\propto p(o \\mid s) p(s)$$\nSubstituting the PDFs:\n$$p(s \\mid o) \\propto \\exp\\left(-\\frac{(o-s)^{2}}{2\\sigma^{2}}\\right) \\exp\\left(-\\frac{(s-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)$$\n$$p(s \\mid o) \\propto \\exp\\left(-\\left[\\frac{(s-o)^{2}}{2\\sigma^{2}} + \\frac{(s-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right]\\right)$$\nThe product of two Gaussian functions is an unnormalized Gaussian function. To find its parameters, we analyze the exponent by expanding the squared terms and collecting terms in powers of $s$. Let the term in the brackets be $\\Phi(s)$:\n$$\\Phi(s) = \\frac{s^{2} - 2so + o^{2}}{2\\sigma^{2}} + \\frac{s^{2} - 2s\\mu_{0} + \\mu_{0}^{2}}{2\\sigma_{0}^{2}}$$\n$$\\Phi(s) = s^{2}\\left(\\frac{1}{2\\sigma^{2}} + \\frac{1}{2\\sigma_{0}^{2}}\\right) - s\\left(\\frac{o}{\\sigma^{2}} + \\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\right) + \\left(\\frac{o^{2}}{2\\sigma^{2}} + \\frac{\\mu_{0}^{2}}{2\\sigma_{0}^{2}}\\right)$$\nThis expression is a quadratic in $s$. We can complete the square to bring it into the canonical form of a Gaussian exponent, which is of the form $-\\frac{(s - \\mu_{\\text{post}})^{2}}{2\\sigma_{\\text{post}}^{2}} + \\text{const}$. By comparing the coefficients of $s^2$ and $s$ from $\\Phi(s)$ with those of the canonical Gaussian form, we can identify the posterior variance $\\sigma_{\\text{post}}^{2}$ and posterior mean $\\mu_{\\text{post}}$.\n\nThe general form of a Gaussian exponent is:\n$$-\\frac{(s-\\mu)^{2}}{2\\sigma^{2}} = -\\frac{s^{2}}{2\\sigma^{2}} + \\frac{s\\mu}{\\sigma^{2}} - \\frac{\\mu^{2}}{2\\sigma^{2}}$$\nComparing the coefficient of $s^{2}$ in $-\\Phi(s)$ with $-\\frac{1}{2\\sigma_{\\text{post}}^{2}}$:\n$$-\\left(\\frac{1}{2\\sigma^{2}} + \\frac{1}{2\\sigma_{0}^{2}}\\right) = -\\frac{1}{2\\sigma_{\\text{post}}^{2}}$$\n$$\\frac{1}{\\sigma_{\\text{post}}^{2}} = \\frac{1}{\\sigma^{2}} + \\frac{1}{\\sigma_{0}^{2}} = \\frac{\\sigma_{0}^{2} + \\sigma^{2}}{\\sigma^{2}\\sigma_{0}^{2}}$$\nThis gives the posterior variance (or more precisely, its inverse, the posterior precision):\n$$\\sigma_{\\text{post}}^{2} = \\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$$\n\nNext, comparing the coefficient of $s$ in $-\\Phi(s)$ with $\\frac{\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^{2}}$:\n$$\\left(\\frac{o}{\\sigma^{2}} + \\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\right) = \\frac{\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^{2}}$$\nSolving for the posterior mean $\\mu_{\\text{post}}$:\n$$\\mu_{\\text{post}} = \\sigma_{\\text{post}}^{2} \\left(\\frac{o}{\\sigma^{2}} + \\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\right)$$\nSubstituting the expression for $\\sigma_{\\text{post}}^{2}$:\n$$\\mu_{\\text{post}} = \\left(\\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}\\right) \\left(\\frac{o\\sigma_{0}^{2} + \\mu_{0}\\sigma^{2}}{\\sigma^{2}\\sigma_{0}^{2}}\\right)$$\n$$\\mu_{\\text{post}} = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$$\nThus, the posterior distribution $p(s \\mid o)$ is a Gaussian distribution $\\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$ with the derived mean and variance.\n\nThe second objective is to find the estimator $\\hat{s}(o)$ that minimizes the expected squared error loss. The squared error loss function is $L(s, \\hat{s}) = (s - \\hat{s})^2$. The expected loss, or risk, is the expectation of this loss function over the posterior distribution of $s$ for a given observation $o$:\n$$R(\\hat{s}) = E_{s \\mid o}[(s - \\hat{s}(o))^2] = \\int_{-\\infty}^{\\infty} (s - \\hat{s}(o))^2 p(s \\mid o) ds$$\nTo find the estimator $\\hat{s}(o)$ that minimizes this risk, we differentiate $R(\\hat{s})$ with respect to $\\hat{s}$ and set the result to zero:\n$$\\frac{d R(\\hat{s})}{d\\hat{s}} = \\frac{d}{d\\hat{s}} \\int_{-\\infty}^{\\infty} (s - \\hat{s})^{2} p(s \\mid o) ds$$\n$$= \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial\\hat{s}} (s - \\hat{s})^{2} p(s \\mid o) ds = \\int_{-\\infty}^{\\infty} 2(s - \\hat{s})(-1) p(s \\mid o) ds$$\n$$= -2 \\left( \\int_{-\\infty}^{\\infty} s p(s \\mid o) ds - \\hat{s} \\int_{-\\infty}^{\\infty} p(s \\mid o) ds \\right)$$\nSetting the derivative to zero:\n$$\\int_{-\\infty}^{\\infty} s p(s \\mid o) ds - \\hat{s} \\int_{-\\infty}^{\\infty} p(s \\mid o) ds = 0$$\nThe first term is the definition of the posterior mean, $E[s \\mid o]$. The second integral is $1$ since $p(s \\mid o)$ is a PDF.\n$$E[s \\mid o] - \\hat{s} = 0$$\n$$\\hat{s} = E[s \\mid o]$$\nThe estimator that minimizes the expected squared error loss is the mean of the posterior distribution. We have already derived this to be $\\mu_{\\text{post}}$.\nTherefore, the optimal estimator is:\n$$\\hat{s}(o) = \\mu_{\\text{post}} = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$$\n\nThe three requested quantities are the posterior mean, the posterior variance, and the optimal estimator.\nPosterior Mean: $\\mu_{\\text{post}} = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$\nPosterior Variance: $\\sigma_{\\text{post}}^{2} = \\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$\nOptimal Estimator: $\\hat{s}(o) = \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}}$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}} & \\frac{\\sigma^{2}\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}} & \\frac{\\mu_{0}\\sigma^{2} + o\\sigma_{0}^{2}}{\\sigma^{2} + \\sigma_{0}^{2}} \\end{pmatrix} } $$"
        },
        {
            "introduction": "To connect abstract Bayesian principles to neural circuits, we need tools to quantify how much information a population of neurons encodes about a stimulus. This practice introduces Fisher information, a central concept that sets a fundamental limit on the precision of any unbiased estimator . By deriving the Fisher information for a population of independent neurons with Poisson variability, you will forge a direct, quantitative link between the tuning properties of neurons and the brain's potential decoding performance.",
            "id": "4182833",
            "problem": "Consider a population code in normative modeling of brain function: a downstream estimator in the brain seeks to infer a scalar stimulus parameter $\\theta$ from the spike counts of a population of $M$ neurons recorded over a fixed observation window of duration $T$. Assume conditional independence of neurons given $\\theta$, and that neuron $i$ produces a spike count $n_i$ that is Poisson distributed with mean $T r_i(\\theta)$, where $r_i(\\theta)$ is a differentiable tuning function. The joint likelihood for the population is given by the product of Poisson probability mass functions across neurons. The downstream estimator $\\hat{\\theta}$ is assumed unbiased for $\\theta$ under the generative model.\n\nUsing only fundamental definitions from statistical estimation theory—namely the likelihood function, the log-likelihood, the score function defined as the derivative of the log-likelihood with respect to $\\theta$, and the variance lower bound implied by the Cauchy-Schwarz inequality on unbiased estimators—derive the expression for the Fisher Information (FI) about $\\theta$ in this independent Poisson population, combine it with the Cramér-Rao Bound (CRB) to obtain the variance bound on any unbiased estimator, and determine how the bound changes under a uniform gain scaling of tuning functions $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$ for a constant $c > 0$.\n\nWhich option correctly states the FI for this model, the CRB for any unbiased estimator $\\hat{\\theta}$, and the effect of uniform gain scaling on the bound?\n\nA. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)}$, $\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{I(\\theta)}$, and under $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$ one has $I(\\theta) \\mapsto c\\, I(\\theta)$ so $\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{c\\, I(\\theta)}$.\n\nB. $I(\\theta) = T \\sum_{i=1}^{M} \\left(\\partial_{\\theta} \\log r_i(\\theta)\\right)^2$, $\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{I(\\theta)}$, and under $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$, $I(\\theta)$ is unchanged, so the bound is unchanged.\n\nC. $I(\\theta) = T \\sum_{i=1}^{M} r_i(\\theta)$, $\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{I(\\theta)}$, and under $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$ one has $I(\\theta) \\mapsto c\\, I(\\theta)$ so $\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{c\\, I(\\theta)}$.\n\nD. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\partial_{\\theta} r_i(\\theta)}{r_i(\\theta)}$, $\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{I(\\theta)}$, and under $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$, $I(\\theta)$ is unchanged so the bound is unchanged.\n\nProvide your choice and justify it based on a derivation that starts from the joint Poisson likelihood and proceeds through the score function and its second-order moment under the model.",
            "solution": "The problem statement has been validated and is deemed sound. It describes a standard model in theoretical neuroscience and asks for a fundamental derivation from statistical estimation theory. All terms are well-defined, and the premises are scientifically and mathematically consistent.\n\nWe will now derive the Fisher Information (FI) for the given population of neurons, state the corresponding Cramér-Rao Bound (CRB), and analyze the effect of a uniform gain scaling on the tuning functions.\n\nThe spike count $n_i$ of neuron $i$ is assumed to follow a Poisson distribution with mean $\\lambda_i(\\theta) = T r_i(\\theta)$. The probability mass function (PMF) for a single neuron is:\n$$\nP(n_i | \\theta) = \\frac{(\\lambda_i(\\theta))^{n_i} e^{-\\lambda_i(\\theta)}}{n_i!} = \\frac{(T r_i(\\theta))^{n_i} e^{-T r_i(\\theta)}}{n_i!}\n$$\n\nGiven the conditional independence of neurons, the joint likelihood for the population spike counts $\\mathbf{n} = (n_1, \\ldots, n_M)$ is the product of the individual PMFs:\n$$\nL(\\theta; \\mathbf{n}) = \\prod_{i=1}^{M} P(n_i | \\theta) = \\prod_{i=1}^{M} \\frac{(T r_i(\\theta))^{n_i} e^{-T r_i(\\theta)}}{n_i!}\n$$\n\nThe log-likelihood, $\\mathcal{L}(\\theta) = \\log L(\\theta; \\mathbf{n})$, is:\n$$\n\\mathcal{L}(\\theta) = \\sum_{i=1}^{M} \\log P(n_i | \\theta) = \\sum_{i=1}^{M} \\left( n_i \\log(T r_i(\\theta)) - T r_i(\\theta) - \\log(n_i!) \\right)\n$$\nWe can separate the terms that depend on $\\theta$:\n$$\n\\mathcal{L}(\\theta) = \\sum_{i=1}^{M} \\left( n_i \\log r_i(\\theta) - T r_i(\\theta) \\right) + \\sum_{i=1}^{M} \\left( n_i \\log T - \\log(n_i!) \\right)\n$$\n\nThe score function, $S(\\theta)$, is the derivative of the log-likelihood with respect to the parameter $\\theta$:\n$$\nS(\\theta) = \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\sum_{i=1}^{M} \\left( n_i \\log r_i(\\theta) - T r_i(\\theta) \\right)\n$$\nUsing the chain rule, and denoting $\\partial_{\\theta} r_i(\\theta)$ as the derivative of $r_i(\\theta)$ with respect to $\\theta$:\n$$\nS(\\theta) = \\sum_{i=1}^{M} \\left( \\frac{n_i}{r_i(\\theta)} \\partial_{\\theta} r_i(\\theta) - T \\partial_{\\theta} r_i(\\theta) \\right) = \\sum_{i=1}^{M} \\left( \\frac{n_i}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta)\n$$\n\nThe Fisher Information, $I(\\theta)$, for a regular estimation problem is defined as the variance of the score function, $I(\\theta) = \\mathrm{Var}(S(\\theta))$. (It is a property of regular likelihoods that the expectation of the score is zero, $E[S(\\theta)]=0$, so the variance is $E[S(\\theta)^2]$). Let's first verify $E[S(\\theta)]=0$:\n$$\nE[S(\\theta)] = E \\left[ \\sum_{i=1}^{M} \\left( \\frac{n_i}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta) \\right] = \\sum_{i=1}^{M} \\left( \\frac{E[n_i]}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta)\n$$\nSince $E[n_i] = T r_i(\\theta)$:\n$$\nE[S(\\theta)] = \\sum_{i=1}^{M} \\left( \\frac{T r_i(\\theta)}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta) = \\sum_{i=1}^{M} (T - T) \\partial_{\\theta} r_i(\\theta) = 0\n$$\nAs the expectation is zero, $I(\\theta) = \\mathrm{Var}(S(\\theta)) = E[S(\\theta)^2]$. Since the neurons are conditionally independent, the spike counts $n_i$ are independent variables. The score $S(\\theta)$ is a sum of terms, one for each neuron, so the variance of the sum is the sum of the variances:\n$$\nI(\\theta) = \\mathrm{Var}(S(\\theta)) = \\sum_{i=1}^{M} \\mathrm{Var}\\left( \\left( \\frac{n_i}{r_i(\\theta)} - T \\right) \\partial_{\\theta} r_i(\\theta) \\right)\n$$\nThe derivative term $\\partial_{\\theta} r_i(\\theta)$ is not a random variable, so we can factor it out (squared):\n$$\nI(\\theta) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\mathrm{Var}\\left( \\frac{n_i}{r_i(\\theta)} - T \\right) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\mathrm{Var}\\left( \\frac{n_i}{r_i(\\theta)} \\right)\n$$\nUsing the property $\\mathrm{Var}(aX) = a^2 \\mathrm{Var}(X)$:\n$$\nI(\\theta) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\frac{1}{(r_i(\\theta))^2} \\mathrm{Var}(n_i)\n$$\nFor a Poisson distribution, the variance equals the mean: $\\mathrm{Var}(n_i) = E[n_i] = T r_i(\\theta)$. Substituting this in:\n$$\nI(\\theta) = \\sum_{i=1}^{M} \\left( \\partial_{\\theta} r_i(\\theta) \\right)^2 \\frac{1}{(r_i(\\theta))^2} (T r_i(\\theta)) = \\sum_{i=1}^{M} T \\frac{(\\partial_{\\theta} r_i(\\theta))^2}{r_i(\\theta)}\n$$\nThis can be written more compactly as:\n$$\nI(\\theta) = T \\sum_{i=1}^{M} \\frac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)}\n$$\nThe Cramér-Rao Bound (CRB) states that for any unbiased estimator $\\hat{\\theta}$ of the parameter $\\theta$, its variance is lower-bounded by the reciprocal of the Fisher Information:\n$$\n\\mathrm{Var}(\\hat{\\theta}) \\ge \\frac{1}{I(\\theta)}\n$$\n\nNow, let's analyze the effect of a uniform gain scaling, $r_i(\\theta) \\mapsto \\tilde{r}_i(\\theta) = c \\, r_i(\\theta)$ for a constant $c > 0$. We compute the new Fisher Information, $\\tilde{I}(\\theta)$. The derivative of the new tuning function is:\n$$\n\\partial_{\\theta} \\tilde{r}_i(\\theta) = \\partial_{\\theta} (c \\, r_i(\\theta)) = c \\, \\partial_{\\theta} r_i(\\theta)\n$$\nSubstituting these into the formula for Fisher Information:\n$$\n\\tilde{I}(\\theta) = T \\sum_{i=1}^{M} \\frac{\\left(\\partial_{\\theta} \\tilde{r}_i(\\theta)\\right)^2}{\\tilde{r}_i(\\theta)} = T \\sum_{i=1}^{M} \\frac{\\left(c \\, \\partial_{\\theta} r_i(\\theta)\\right)^2}{c \\, r_i(\\theta)} = T \\sum_{i=1}^{M} \\frac{c^2 \\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{c \\, r_i(\\theta)}\n$$\n$$\n\\tilde{I}(\\theta) = c \\left( T \\sum_{i=1}^{M} \\frac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)} \\right) = c \\, I(\\theta)\n$$\nThe Fisher Information is scaled by the factor $c$. Consequently, the new variance bound is:\n$$\n\\mathrm{Var}(\\hat{\\theta}) \\ge \\frac{1}{\\tilde{I}(\\theta)} = \\frac{1}{c \\, I(\\theta)}\n$$\n\nNow we evaluate the given options based on our derivation.\n\nA. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2}{r_i(\\theta)}$, $\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{I(\\theta)}$, and under $r_i(\\theta) \\mapsto c\\, r_i(\\theta)$ one has $I(\\theta) \\mapsto c\\, I(\\theta)$ so $\\mathrm{Var}\\!\\left(\\hat{\\theta}\\right) \\ge \\dfrac{1}{c\\, I(\\theta)}$.\nThis option perfectly matches our derived expression for the Fisher Information, the statement of the Cramér-Rao Bound, and our analysis of the gain scaling.\n**Verdict: Correct.**\n\nB. $I(\\theta) = T \\sum_{i=1}^{M} \\left(\\partial_{\\theta} \\log r_i(\\theta)\\right)^2$.\nThis expression expands to $T \\sum_{i=1}^{M} \\left(\\frac{\\partial_{\\theta} r_i(\\theta)}{r_i(\\theta)}\\right)^2 = T \\sum_{i=1}^{M} \\frac{(\\partial_{\\theta} r_i(\\theta))^2}{(r_i(\\theta))^2}$. This differs from our derived formula $T \\sum_{i=1}^{M} \\frac{(\\partial_{\\theta} r_i(\\theta))^2}{r_i(\\theta)}$ by a factor of $r_i(\\theta)$ in the denominator. The scaling analysis is also incorrect because it is based on this faulty FI formula.\n**Verdict: Incorrect.**\n\nC. $I(\\theta) = T \\sum_{i=1}^{M} r_i(\\theta)$.\nThis expression for Fisher Information is incorrect. It represents the total expected spike count in the population over the time interval $T$, but it completely neglects the derivative of the tuning functions, $\\partial_{\\theta} r_i(\\theta)$, which is the source of information about $\\theta$.\n**Verdict: Incorrect.**\n\nD. $I(\\theta) = T \\sum_{i=1}^{M} \\dfrac{\\partial_{\\theta} r_i(\\theta)}{r_i(\\theta)}$.\nThis expression is incorrect. It is missing the square on the derivative term, $\\left(\\partial_{\\theta} r_i(\\theta)\\right)^2$. This also makes it dimensionally inconsistent for Fisher Information.\n**Verdict: Incorrect.**\n\nBased on the rigorous derivation, only option A is correct.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While models of independent neurons are instructive, real neural populations exhibit correlated activity, a feature with profound consequences for coding. This advanced exercise tackles this issue head-on, exploring how noise correlations impact Fisher information and place a ceiling on the benefits of pooling signals across large populations . Working through this problem reveals the crucial and non-intuitive concept of \"information-limiting correlations,\" a cornerstone of modern theories of population coding.",
            "id": "4182855",
            "problem": "A core normative principle in neuroscience data analysis is that, for estimating a scalar stimulus $s$ from neural population responses, the Cramér–Rao bound states that any unbiased estimator has variance bounded below by the inverse of the Fisher information (FI). Consider a population of $N$ neurons whose response vector $\\mathbf{r} \\in \\mathbb{R}^N$ to a stimulus $s$ is modeled as Gaussian with mean $\\mathbf{f}(s)$ and stimulus-independent covariance $\\boldsymbol{\\Sigma}$, so that $p(\\mathbf{r} \\mid s) = \\mathcal{N}(\\mathbf{f}(s), \\boldsymbol{\\Sigma})$. Let $\\mathbf{g}(s) \\equiv \\frac{\\partial \\mathbf{f}(s)}{\\partial s}$ denote the vector of local sensitivities. Assume $\\boldsymbol{\\Sigma}$ decomposes into an independent-noise term plus a shared, low-rank term of the form $\\boldsymbol{\\Sigma} = \\mathbf{A} + \\tau^{2} \\mathbf{u} \\mathbf{u}^{\\top}$, where $\\mathbf{A} = \\operatorname{diag}(\\sigma_{1}^{2}, \\dots, \\sigma_{N}^{2})$ is diagonal, $\\tau^{2} > 0$, and $\\mathbf{u} \\in \\mathbb{R}^{N}$ is a fixed direction of shared variability. Starting from the definition of Fisher information $J(s) = \\mathbb{E}\\!\\left[\\left(\\frac{\\partial \\ln p(\\mathbf{r} \\mid s)}{\\partial s}\\right)^{2}\\right]$, derive an analytic expression for $J(s)$ in terms of $\\mathbf{g}(s)$, $\\mathbf{A}$, $\\mathbf{u}$, and $\\tau^{2}$, and use it to identify the structural condition on the shared-noise direction $\\mathbf{u}$ that produces information-limiting correlations, in the sense that $J(s)$ does not grow without bound as $N$ increases. Then, specialize to an operating point $s_{0}$ where all neurons have identical local sensitivity, $\\mathbf{g}(s_{0}) = \\alpha \\mathbf{1}$ with $\\alpha > 0$, and identical independent noise, $\\mathbf{A} = \\sigma^{2} \\mathbf{I}$ with $\\sigma^{2} > 0$. Further, let the shared-noise direction align with the sensitivity, $\\mathbf{u} = \\mathbf{g}(s_{0})$. In this specialization, compute the exact Fisher information $J(s_{0})$ and determine its limit as $N \\to \\infty$. Report, as your final answer, the limiting value of $J(s_{0})$ as a single closed-form analytic expression in terms of $\\tau$. No numerical approximation is required, and no units are to be included in the final answer.",
            "solution": "The fundamental starting point is the definition of Fisher information for a scalar parameter $s$, given by $J(s) = \\mathbb{E}\\!\\left[\\left(\\partial \\ln p(\\mathbf{r} \\mid s)/\\partial s\\right)^{2}\\right]$, where the expectation is taken with respect to $p(\\mathbf{r} \\mid s)$. For a multivariate normal distribution with stimulus-dependent mean $\\mathbf{f}(s)$ and stimulus-independent covariance $\\boldsymbol{\\Sigma}$, the log-likelihood is\n$$\n\\ln p(\\mathbf{r} \\mid s) = -\\frac{1}{2}(\\mathbf{r} - \\mathbf{f}(s))^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r} - \\mathbf{f}(s)) - \\frac{1}{2} \\ln \\det(2\\pi \\boldsymbol{\\Sigma}).\n$$\nBecause $\\boldsymbol{\\Sigma}$ does not depend on $s$, the derivative with respect to $s$ acts only on $\\mathbf{f}(s)$:\n$$\n\\frac{\\partial}{\\partial s} \\ln p(\\mathbf{r} \\mid s) = \\left(\\frac{\\partial \\mathbf{f}(s)}{\\partial s}\\right)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\left(\\mathbf{r} - \\mathbf{f}(s)\\right) = \\mathbf{g}(s)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\left(\\mathbf{r} - \\mathbf{f}(s)\\right).\n$$\nTaking the expectation of the square with respect to $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{f}(s), \\boldsymbol{\\Sigma})$, we use $\\mathbb{E}[(\\mathbf{r} - \\mathbf{f}(s))(\\mathbf{r} - \\mathbf{f}(s))^{\\top}] = \\boldsymbol{\\Sigma}$ to obtain\n$$\nJ(s) = \\mathbb{E}\\!\\left[\\left(\\mathbf{g}(s)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\left(\\mathbf{r} - \\mathbf{f}(s)\\right)\\right)^{2}\\right] = \\mathbf{g}(s)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma}^{-1} \\mathbf{g}(s) = \\mathbf{g}(s)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\mathbf{g}(s).\n$$\nThus the Fisher information reduces to the quadratic form of $\\boldsymbol{\\Sigma}^{-1}$ along the sensitivity direction $\\mathbf{g}(s)$.\n\nNext, with $\\boldsymbol{\\Sigma} = \\mathbf{A} + \\tau^{2} \\mathbf{u} \\mathbf{u}^{\\top}$, we apply the Woodbury matrix identity (a well-tested linear algebra result) to invert $\\boldsymbol{\\Sigma}$. Let $\\mathbf{A}$ be invertible, and define $\\mathbf{U} = \\tau \\mathbf{u}$ and $\\mathbf{V}^{\\top} = (\\tau \\mathbf{u})^{\\top}$ so that $\\boldsymbol{\\Sigma} = \\mathbf{A} + \\mathbf{U} \\mathbf{V}^{\\top}$. The Woodbury identity gives\n$$\n\\boldsymbol{\\Sigma}^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} \\left(\\mathbf{I} + \\mathbf{V}^{\\top} \\mathbf{A}^{-1} \\mathbf{U}\\right)^{-1} \\mathbf{V}^{\\top} \\mathbf{A}^{-1}.\n$$\nBecause $\\mathbf{U}$ and $\\mathbf{V}$ are vectors, $\\mathbf{I} + \\mathbf{V}^{\\top} \\mathbf{A}^{-1} \\mathbf{U}$ is the scalar $1 + \\tau^{2} \\mathbf{u}^{\\top} \\mathbf{A}^{-1} \\mathbf{u}$, hence\n$$\n\\boldsymbol{\\Sigma}^{-1} = \\mathbf{A}^{-1} - \\frac{\\tau^{2}}{1 + \\tau^{2} \\mathbf{u}^{\\top} \\mathbf{A}^{-1} \\mathbf{u}} \\, \\mathbf{A}^{-1} \\mathbf{u} \\mathbf{u}^{\\top} \\mathbf{A}^{-1}.\n$$\nSubstituting into $J(s) = \\mathbf{g}(s)^{\\top} \\boldsymbol{\\Sigma}^{-1} \\mathbf{g}(s)$ yields\n$$\nJ(s) = \\mathbf{g}(s)^{\\top} \\mathbf{A}^{-1} \\mathbf{g}(s) - \\frac{\\tau^{2}}{1 + \\tau^{2} \\mathbf{u}^{\\top} \\mathbf{A}^{-1} \\mathbf{u}} \\left(\\mathbf{g}(s)^{\\top} \\mathbf{A}^{-1} \\mathbf{u}\\right)^{2}.\n$$\nThis expression shows explicitly how the correlated noise affects the Fisher information through the overlap between the sensitivity direction $\\mathbf{g}(s)$ and the shared-noise direction $\\mathbf{u}$ under the metric induced by $\\mathbf{A}^{-1}$.\n\nTo identify the structural condition for information-limiting correlations, consider how $J(s)$ scales with $N$. Suppose the independent-noise term $\\mathbf{A}$ has bounded variances (for instance, entries on the order of a constant), and the sensitivity vector $\\mathbf{g}(s)$ has entries that do not diminish with $N$ (for instance, a homogeneous sensitivity profile). Then the first term $\\mathbf{g}(s)^{\\top} \\mathbf{A}^{-1} \\mathbf{g}(s)$ typically scales on the order of $N$. The second term subtracts a nonnegative quantity proportional to the squared overlap $\\left(\\mathbf{g}(s)^{\\top} \\mathbf{A}^{-1} \\mathbf{u}\\right)^{2}$, normalized by $1 + \\tau^{2} \\mathbf{u}^{\\top} \\mathbf{A}^{-1} \\mathbf{u}$. For this subtraction to cancel the linear-in-$N$ growth and produce a finite asymptote, the shared-noise direction must align with the sensitivity direction in the $\\mathbf{A}^{-1}$-metric so that both the numerator and denominator scale proportionally to $\\mathbf{g}(s)^{\\top} \\mathbf{A}^{-1} \\mathbf{g}(s)$, yielding a bounded ratio. Concretely, the information-limiting condition is that $\\mathbf{u}$ has a component proportional to $\\mathbf{g}(s)$, and the dominant shared-noise component is along $\\mathbf{g}(s)$; in the strict rank-$1$ case with $\\mathbf{u} \\propto \\mathbf{g}(s)$, the limit is finite. If $\\mathbf{u}$ is orthogonal to $\\mathbf{g}(s)$ in the $\\mathbf{A}^{-1}$-metric, the second term vanishes and $J(s)$ grows without bound with $N$.\n\nWe now specialize to the stated operating point $s_{0}$ with $\\mathbf{g}(s_{0}) = \\alpha \\mathbf{1}$, $\\mathbf{A} = \\sigma^{2} \\mathbf{I}$, and $\\mathbf{u} = \\mathbf{g}(s_{0})$. Under these assumptions,\n$$\n\\mathbf{A}^{-1} = \\frac{1}{\\sigma^{2}} \\mathbf{I}, \\quad \\mathbf{g}(s_{0})^{\\top} \\mathbf{A}^{-1} \\mathbf{g}(s_{0}) = \\frac{1}{\\sigma^{2}} \\mathbf{g}(s_{0})^{\\top} \\mathbf{g}(s_{0}) = \\frac{\\alpha^{2} N}{\\sigma^{2}}.\n$$\nBecause $\\mathbf{u} = \\mathbf{g}(s_{0})$, we also have\n$$\n\\mathbf{u}^{\\top} \\mathbf{A}^{-1} \\mathbf{u} = \\frac{\\alpha^{2} N}{\\sigma^{2}}, \\quad \\mathbf{g}(s_{0})^{\\top} \\mathbf{A}^{-1} \\mathbf{u} = \\frac{\\alpha^{2} N}{\\sigma^{2}}.\n$$\nSubstituting into the general expression for $J(s)$ gives\n$$\nJ(s_{0}) = \\frac{\\alpha^{2} N}{\\sigma^{2}} - \\frac{\\tau^{2}}{1 + \\tau^{2} \\frac{\\alpha^{2} N}{\\sigma^{2}}} \\left(\\frac{\\alpha^{2} N}{\\sigma^{2}}\\right)^{2}.\n$$\nThis simplifies algebraically as follows. Let $x \\equiv \\frac{\\alpha^{2} N}{\\sigma^{2}}$. Then\n$$\nJ(s_{0}) = x - \\frac{\\tau^{2} x^{2}}{1 + \\tau^{2} x} = \\frac{x(1 + \\tau^{2} x) - \\tau^{2} x^{2}}{1 + \\tau^{2} x} = \\frac{x}{1 + \\tau^{2} x}.\n$$\nTherefore,\n$$\nJ(s_{0}) = \\frac{\\frac{\\alpha^{2} N}{\\sigma^{2}}}{1 + \\tau^{2} \\frac{\\alpha^{2} N}{\\sigma^{2}}}.\n$$\nTaking the limit as $N \\to \\infty$, we note that the numerator and the $\\tau^{2}$-scaled term in the denominator both grow linearly in $N$, and the ratio tends to the inverse of $\\tau^{2}$:\n$$\n\\lim_{N \\to \\infty} J(s_{0}) = \\lim_{N \\to \\infty} \\frac{\\frac{\\alpha^{2} N}{\\sigma^{2}}}{1 + \\tau^{2} \\frac{\\alpha^{2} N}{\\sigma^{2}}} = \\frac{1}{\\tau^{2}}.\n$$\nThis finite asymptote is the hallmark of information-limiting correlations: a shared-noise component aligned with the sensitivity direction $\\mathbf{g}(s)$ caps the achievable Fisher information as population size grows.\n\nThus, under the stated specialization, the limiting Fisher information as $N \\to \\infty$ is $\\frac{1}{\\tau^{2}}$, which is the requested closed-form analytic expression in terms of $\\tau$.",
            "answer": "$$\\boxed{\\frac{1}{\\tau^{2}}}$$"
        }
    ]
}