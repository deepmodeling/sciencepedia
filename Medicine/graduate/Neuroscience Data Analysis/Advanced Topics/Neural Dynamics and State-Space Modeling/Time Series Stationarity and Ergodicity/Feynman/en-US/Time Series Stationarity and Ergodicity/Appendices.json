{
    "hands_on_practices": [
        {
            "introduction": "Understanding the temporal structure of neural signals is fundamental, but our estimates are only reliable if the underlying process is stationary. This exercise challenges you to derive the bias in the sample autocovariance estimator, first under the ideal conditions of stationarity and then in the presence of a contaminating linear trend . Completing this derivation provides a crucial, first-principles understanding of why non-stationarity can distort our conclusions about neural dynamics.",
            "id": "4200519",
            "problem": "A research group is analyzing a single-trial Local Field Potential (LFP) time series in a sensory cortex recording, sampled at uniform intervals and indexed as $X_{1}, X_{2}, \\dots, X_{T}$. They are interested in estimating the temporal dependence structure using the sample autocovariance estimator at lag $\\tau$, defined via the sample mean. Let $\\bar{X} = \\frac{1}{T} \\sum_{t=1}^{T} X_{t}$ and consider lags $\\tau \\in \\{0, 1, \\dots, T-1\\}$. The estimator is\n$$\n\\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} \\left( X_{t} - \\bar{X} \\right) \\left( X_{t+\\tau} - \\bar{X} \\right).\n$$\nPart A (stationarity): Assume the data are generated by a wide-sense stationary (WSS) process $\\{Y_{t}\\}$ with mean $0$ and autocovariance function $\\gamma_{Y}(\\tau) = \\mathbb{E}[Y_{t} Y_{t+\\tau}]$ such that the process is ergodic in the mean and autocovariance. Express $\\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right]$ in terms of $T$, $\\tau$, and $\\gamma_{Y}(\\cdot)$, and justify the finite-sample bias of $\\hat{\\gamma}(\\tau)$ relative to $\\gamma_{Y}(\\tau)$ under WSS. Explain, using first principles and the definition of ergodicity, why the estimator is consistent as $T \\to \\infty$.\n\nPart B (linear trend violation): Now suppose the LFP is contaminated by a deterministic linear trend and an offset,\n$$\nX_{t} = \\mu + \\beta t + Y_{t},\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$ are fixed, and $\\{Y_{t}\\}$ is the same WSS, mean $0$ process with autocovariance $\\gamma_{Y}(\\tau)$. Derive, from first principles, the exact closed-form expression for the additive bias in $\\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right]$ that arises purely due to the linear trend (i.e., the component of $\\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right]$ contributed by $\\mu + \\beta t$ after sample-mean centering). Define the bias due to the trend as\n$$\n\\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right] - \\mathbb{E}_{Y}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right],\n$$\nwhere $\\hat{\\gamma}_{Y}(\\tau)$ denotes the same estimator applied to the stationary component $Y_{t}$ alone. Provide your final answer as a single, closed-form analytical expression in terms of $\\beta$, $T$, and $\\tau$. No numerical approximation is required.",
            "solution": "The problem statement has been validated and is deemed a well-posed, scientifically grounded problem in time series analysis. It contains sufficient information and is internally consistent.\n\nThe problem is divided into two parts. Part A addresses the properties of the sample autocovariance estimator for a wide-sense stationary (WSS) process. Part B investigates the bias introduced into this estimator by a deterministic linear trend, a common violation of stationarity.\n\n### Part A: Stationarity, Bias, and Consistency\n\nFor this part, the time series is $X_{t} = Y_{t}$, where $\\{Y_{t}\\}$ is a WSS process with $\\mathbb{E}[Y_{t}] = 0$ and autocovariance function $\\gamma_{Y}(\\tau) = \\mathbb{E}[Y_{t}Y_{t+\\tau}]$. The sample autocovariance estimator is given by $\\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} (X_{t} - \\bar{X})(X_{t+\\tau} - \\bar{X})$. Since $X_{t}=Y_{t}$, we can write this as $\\hat{\\gamma}_{Y}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} (Y_{t} - \\bar{Y})(Y_{t+\\tau} - \\bar{Y})$, with $\\bar{Y} = \\frac{1}{T}\\sum_{t=1}^{T} Y_{t}$.\n\n**1. Expectation of the Estimator**\n\nTo find the expected value, we expand the terms and use the linearity of the expectation operator.\n$$ \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} \\mathbb{E}\\left[ (Y_{t} - \\bar{Y})(Y_{t+\\tau} - \\bar{Y}) \\right] $$\n$$ \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} \\mathbb{E}\\left[ Y_{t}Y_{t+\\tau} - Y_{t}\\bar{Y} - Y_{t+\\tau}\\bar{Y} + \\bar{Y}^2 \\right] $$\n$$ \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} \\left( \\mathbb{E}[Y_{t}Y_{t+\\tau}] - \\mathbb{E}[Y_{t}\\bar{Y}] - \\mathbb{E}[Y_{t+\\tau}\\bar{Y}] + \\mathbb{E}[\\bar{Y}^2] \\right) $$\nLet's evaluate each expectation term:\n- $\\mathbb{E}[Y_{t}Y_{t+\\tau}] = \\gamma_{Y}(\\tau)$ by definition, as the process is WSS.\n- $\\mathbb{E}[Y_{t}\\bar{Y}] = \\mathbb{E}\\left[Y_{t} \\frac{1}{T}\\sum_{s=1}^{T}Y_{s}\\right] = \\frac{1}{T}\\sum_{s=1}^{T}\\mathbb{E}[Y_{t}Y_{s}] = \\frac{1}{T}\\sum_{s=1}^{T}\\gamma_{Y}(t-s)$.\n- Similarly, $\\mathbb{E}[Y_{t+\\tau}\\bar{Y}] = \\frac{1}{T}\\sum_{s=1}^{T}\\gamma_{Y}(t+\\tau-s)$.\n- $\\mathbb{E}[\\bar{Y}^2] = \\mathrm{Var}(\\bar{Y}) + (\\mathbb{E}[\\bar{Y}])^2$. Since $\\mathbb{E}[Y_{t}]=0$, we have $\\mathbb{E}[\\bar{Y}] = \\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}[Y_t] = 0$. Thus, $\\mathbb{E}[\\bar{Y}^2] = \\mathrm{Var}(\\bar{Y})$.\n$$ \\mathrm{Var}(\\bar{Y}) = \\mathrm{Var}\\left(\\frac{1}{T}\\sum_{t=1}^{T}Y_{t}\\right) = \\frac{1}{T^2} \\sum_{t=1}^{T}\\sum_{s=1}^{T} \\mathrm{Cov}(Y_{t},Y_{s}) = \\frac{1}{T^2}\\sum_{t=1}^{T}\\sum_{s=1}^{T}\\gamma_{Y}(t-s) $$\nCombining these results, the expectation of the estimator is:\n$$ \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} \\left( \\gamma_{Y}(\\tau) - \\frac{1}{T}\\sum_{s=1}^{T}\\gamma_{Y}(t-s) - \\frac{1}{T}\\sum_{s=1}^{T}\\gamma_{Y}(t+\\tau-s) + \\frac{1}{T^2}\\sum_{u=1}^{T}\\sum_{v=1}^{T}\\gamma_{Y}(u-v) \\right) $$\nSumming over $t$ from $1$ to $T-\\tau$ gives:\n$$ \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{T-\\tau}{T}\\gamma_{Y}(\\tau) - \\frac{1}{T^2}\\sum_{t=1}^{T-\\tau}\\sum_{s=1}^{T}\\left(\\gamma_{Y}(t-s)+\\gamma_{Y}(t+\\tau-s)\\right) + \\frac{T-\\tau}{T^3}\\sum_{u=1}^{T}\\sum_{v=1}^{T}\\gamma_{Y}(u-v) $$\nThis is the expression for $\\mathbb{E}[\\hat{\\gamma}_{Y}(\\tau)]$ in terms of $T$, $\\tau$, and $\\gamma_{Y}(\\cdot)$.\n\n**2. Finite-Sample Bias**\nThe bias of the estimator is $\\mathrm{Bias}[\\hat{\\gamma}_{Y}(\\tau)] = \\mathbb{E}[\\hat{\\gamma}_{Y}(\\tau)] - \\gamma_{Y}(\\tau)$.\nFrom the expression above, the bias has two main sources for finite $T$:\na) The leading term is $\\frac{T-\\tau}{T}\\gamma_{Y}(\\tau) = (1-\\frac{\\tau}{T})\\gamma_{Y}(\\tau)$, which differs from $\\gamma_{Y}(\\tau)$. This is partly due to the divisor $1/T$ instead of a more natural choice like $1/(T-\\tau)$. This component of the bias is $-\\frac{\\tau}{T}\\gamma_{Y}(\\tau)$.\nb) The remaining terms arise from using the sample mean $\\bar{Y}$ instead of the true mean $\\mathbb{E}[Y_t]=0$. Subtracting $\\bar{Y}$ introduces dependencies between all terms in the sum, as each $(Y_t - \\bar{Y})$ term is a function of the entire sample. The additional terms involving sums over the autocovariance function capture these complex dependencies. For large $T$, these terms are of order $O(1/T)$, assuming the autocovariance function decays sufficiently quickly (i.e., $\\sum_{k=-\\infty}^{\\infty}|\\gamma_{Y}(k)|  \\infty$). Therefore, for any finite sample size $T$, $\\hat{\\gamma}_{Y}(\\tau)$ is a biased estimator of $\\gamma_{Y}(\\tau)$.\n\n**3. Consistency**\nAn estimator is consistent if it converges in probability to the true value as the sample size approaches infinity. We need to show that $\\hat{\\gamma}_{Y}(\\tau) \\xrightarrow{p} \\gamma_{Y}(\\tau)$ as $T \\to \\infty$. This can be established by showing its bias and variance both tend to zero.\n\nFirst, consider the bias as $T \\to \\infty$. The lead term in the expectation, $(1-\\frac{\\tau}{T})\\gamma_{Y}(\\tau)$, converges to $\\gamma_{Y}(\\tau)$. All other terms are scaled by factors of at least $1/T$. For a process that is ergodic in the mean, $\\lim_{T\\to\\infty} T \\cdot \\mathrm{Var}(\\bar{Y})$ is finite, which means $\\mathrm{Var}(\\bar{Y})$ is $O(1/T)$. The other summed terms, when normalized, also vanish. Hence, $\\lim_{T\\to\\infty} \\mathbb{E}[\\hat{\\gamma}_{Y}(\\tau)] = \\gamma_{Y}(\\tau)$, meaning the estimator is asymptotically unbiased.\n\nSecond, we rely on the given property of ergodicity. Ergodicity implies that time averages of the process converge to their corresponding ensemble averages.\nWe can write the estimator as:\n$$ \\hat{\\gamma}_{Y}(\\tau) = \\frac{1}{T}\\sum_{t=1}^{T-\\tau}Y_{t}Y_{t+\\tau} - \\frac{\\bar{Y}}{T}\\sum_{t=1}^{T-\\tau}Y_{t} - \\frac{\\bar{Y}}{T}\\sum_{t=1}^{T-\\tau}Y_{t+\\tau} + \\frac{T-\\tau}{T}\\bar{Y}^2 $$\nAs $T\\to\\infty$:\n- The process being ergodic in the mean implies that the sample mean converges to the ensemble mean: $\\bar{Y} \\xrightarrow{p} \\mathbb{E}[Y_{t}]=0$.\n- The process being ergodic in the autocovariance implies that time averages of products converge to their ensemble averages. Specifically, for the stationary process $Z_{t} = Y_{t}Y_{t+\\tau}$, its time average converges to its expectation:\n$$ \\frac{1}{T-\\tau}\\sum_{t=1}^{T-\\tau}Y_{t}Y_{t+\\tau} \\xrightarrow{p} \\mathbb{E}[Y_{t}Y_{t+\\tau}] = \\gamma_{Y}(\\tau) $$\nConsidering the first term of $\\hat{\\gamma}_{Y}(\\tau)$: $\\frac{1}{T}\\sum_{t=1}^{T-\\tau}Y_{t}Y_{t+\\tau} = \\frac{T-\\tau}{T} \\left( \\frac{1}{T-\\tau}\\sum_{t=1}^{T-\\tau}Y_{t}Y_{t+\\tau} \\right)$. Since $\\frac{T-\\tau}{T} \\to 1$ and the term in parentheses converges to $\\gamma_{Y}(\\tau)$, this whole term converges in probability to $\\gamma_{Y}(\\tau)$.\n- All other terms involve $\\bar{Y}$ or $\\bar{Y}^2$. Since $\\bar{Y} \\xrightarrow{p} 0$, and the other sums remain bounded in probability, all subsequent terms converge to $0$ by Slutsky's theorem.\nTherefore, $\\hat{\\gamma}_{Y}(\\tau) \\xrightarrow{p} \\gamma_{Y}(\\tau)$ as $T\\to\\infty$, establishing the estimator's consistency under the assumption of ergodicity.\n\n### Part B: Bias from a Linear Trend\n\nNow, $X_{t} = \\mu + \\beta t + Y_{t}$. We want to find the additive bias $\\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right] - \\mathbb{E}_{Y}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right]$.\n\nFirst, we compute the sample mean $\\bar{X}$:\n$$ \\bar{X} = \\frac{1}{T} \\sum_{t=1}^{T} (\\mu + \\beta t + Y_t) = \\mu + \\beta \\left(\\frac{1}{T} \\sum_{t=1}^{T} t \\right) + \\frac{1}{T}\\sum_{t=1}^{T} Y_t $$\nUsing the formula for the sum of the first $T$ integers, $\\sum_{t=1}^{T} t = \\frac{T(T+1)}{2}$, we get:\n$$ \\bar{X} = \\mu + \\beta \\frac{T+1}{2} + \\bar{Y} $$\nNext, we find the centered term $X_t - \\bar{X}$:\n$$ X_t - \\bar{X} = (\\mu + \\beta t + Y_t) - \\left( \\mu + \\beta \\frac{T+1}{2} + \\bar{Y} \\right) = \\beta\\left(t - \\frac{T+1}{2}\\right) + (Y_t - \\bar{Y}) $$\nLet $d_t = \\beta(t - \\frac{T+1}{2})$ be the deterministic component. Then $X_t - \\bar{X} = d_t + (Y_t - \\bar{Y})$.\n\nNow we substitute this into the estimator $\\hat{\\gamma}(\\tau)$:\n$$ \\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} (X_t - \\bar{X})(X_{t+\\tau} - \\bar{X}) $$\n$$ \\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} [d_t + (Y_t - \\bar{Y})] [d_{t+\\tau} + (Y_{t+\\tau} - \\bar{Y})] $$\nExpanding the product:\n$$ \\hat{\\gamma}(\\tau) = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} d_t d_{t+\\tau} + \\frac{1}{T} \\sum_{t=1}^{T-\\tau} d_t (Y_{t+\\tau} - \\bar{Y}) + \\frac{1}{T} \\sum_{t=1}^{T-\\tau} d_{t+\\tau} (Y_t - \\bar{Y}) + \\frac{1}{T} \\sum_{t=1}^{T-\\tau} (Y_t - \\bar{Y})(Y_{t+\\tau} - \\bar{Y}) $$\nThe last term is exactly $\\hat{\\gamma}_{Y}(\\tau)$. Let's take the expectation over the randomness in $Y_t$:\n$$ \\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right] = \\mathbb{E}\\left[ \\frac{1}{T}\\sum d_t d_{t+\\tau} \\right] + \\mathbb{E}\\left[ \\frac{1}{T}\\sum d_t (Y_{t+\\tau} - \\bar{Y}) \\right] + \\mathbb{E}\\left[ \\frac{1}{T}\\sum d_{t+\\tau} (Y_t - \\bar{Y}) \\right] + \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] $$\nThe terms $d_t$ are deterministic. The cross-terms' expectations are zero because $\\mathbb{E}[Y_t - \\bar{Y}] = \\mathbb{E}[Y_t] - \\mathbb{E}[\\bar{Y}] = 0 - 0 = 0$.\n$$ \\mathbb{E}\\left[ \\frac{1}{T}\\sum d_t (Y_{t+\\tau} - \\bar{Y}) \\right] = \\frac{1}{T}\\sum d_t \\mathbb{E}[Y_{t+\\tau} - \\bar{Y}] = 0 $$\nThus, we have:\n$$ \\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} d_t d_{t+\\tau} + \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] $$\nThe additive bias due to the trend is therefore purely deterministic:\n$$ \\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\mathbb{E}\\left[\\hat{\\gamma}(\\tau)\\right] - \\mathbb{E}\\left[\\hat{\\gamma}_{Y}(\\tau)\\right] = \\frac{1}{T} \\sum_{t=1}^{T-\\tau} d_t d_{t+\\tau} $$\nSubstituting the expression for $d_t$:\n$$ \\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\frac{\\beta^2}{T} \\sum_{t=1}^{T-\\tau} \\left(t - \\frac{T+1}{2}\\right) \\left(t+\\tau - \\frac{T+1}{2}\\right) $$\nLet $K = \\frac{T+1}{2}$ and $N = T-\\tau$. The summation becomes:\n$$ S = \\sum_{t=1}^{N} (t - K)(t+\\tau-K) = \\sum_{t=1}^{N} [t^2 + (\\tau-2K)t + (K^2 - \\tau K)] $$\n$$ S = \\sum_{t=1}^{N} t^2 + (\\tau-2K)\\sum_{t=1}^{N} t + N(K^2 - \\tau K) $$\nUsing the standard sum formulas $\\sum_{t=1}^{N} t = \\frac{N(N+1)}{2}$ and $\\sum_{t=1}^{N} t^2 = \\frac{N(N+1)(2N+1)}{6}$:\nSubstituting $K=\\frac{T+1}{2}$ and $N=T-\\tau$, we have $\\tau-2K = \\tau-(T+1) = -(T-\\tau+1)=-(N+1)$.\n$$ S = \\frac{N(N+1)(2N+1)}{6} - (N+1)\\frac{N(N+1)}{2} + N(K^2 - \\tau K) $$\nThe first two terms simplify:\n$$ \\frac{N(N+1)}{6} [ (2N+1) - 3(N+1) ] = \\frac{N(N+1)}{6}[-N-2] = -\\frac{N(N+1)(N+2)}{6} $$\nThe last term is $N(K^2 - \\tau K) = N K(K-\\tau) = N \\frac{T+1}{2}(\\frac{T+1}{2}-\\tau) = N \\frac{(T+1)(T+1-2\\tau)}{4}$.\nSubstitute $T=N+\\tau$: $T+1=N+\\tau+1$ and $T+1-2\\tau = N-\\tau+1$.\n$N(K^2 - \\tau K) = N \\frac{(N+\\tau+1)(N-\\tau+1)}{4} = N \\frac{(N+1)^2-\\tau^2}{4}$.\nSo, $S = -\\frac{N(N+1)(N+2)}{6} + \\frac{N((N+1)^2-\\tau^2)}{4}$.\nCombining over a common denominator of $12$:\n$$ S = \\frac{N}{12} [-2(N+1)(N+2) + 3((N+1)^2 - \\tau^2)] $$\n$$ S = \\frac{N}{12} [-2(N^2+3N+2) + 3(N^2+2N+1-\\tau^2)] $$\n$$ S = \\frac{N}{12} [-2N^2 - 6N - 4 + 3N^2 + 6N + 3 - 3\\tau^2] = \\frac{N}{12} [N^2 - 1 - 3\\tau^2] $$\nSubstituting $N=T-\\tau$ back:\n$$ S = \\frac{T-\\tau}{12} [(T-\\tau)^2 - 1 - 3\\tau^2] = \\frac{T-\\tau}{12} [T^2 - 2T\\tau + \\tau^2 - 1 - 3\\tau^2] $$\n$$ S = \\frac{T-\\tau}{12} [T^2 - 2T\\tau - 2\\tau^2 - 1] $$\nThe bias is $\\frac{\\beta^2}{T}S$:\n$$ \\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\frac{\\beta^2(T-\\tau)}{12T} (T^2 - 2T\\tau - 2\\tau^2 - 1) $$\nExpanding this expression gives:\n$$ \\mathrm{Bias}_{\\mathrm{trend}}(\\tau) = \\frac{\\beta^2}{12T} (T(T^2 - 2T\\tau - 2\\tau^2 - 1) - \\tau(T^2 - 2T\\tau - 2\\tau^2 - 1)) $$\n$$ = \\frac{\\beta^2}{12T} (T^3 - 2T^2\\tau - 2T\\tau^2 - T - \\tau T^2 + 2T\\tau^2 + 2\\tau^3 + \\tau) $$\n$$ = \\frac{\\beta^2}{12T} (T^3 - 3T^2\\tau + 2\\tau^3 - T + \\tau) $$\nThis is the final closed-form expression for the additive bias due to the linear trend.",
            "answer": "$$\n\\boxed{\\frac{\\beta^2}{12T} \\left( T^{3} - 3T^{2}\\tau + 2\\tau^{3} - T + \\tau \\right)}\n$$"
        },
        {
            "introduction": "While visual inspection can suggest non-stationarity, rigorous analysis demands formal statistical testing. This practice guides you through the derivation of the Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test, one of the most widely used methods for detecting stochastic trends . By building the test statistic from its underlying model, you will gain a robust understanding of how to formally distinguish a stationary process from one with a unit root.",
            "id": "4200528",
            "problem": "You are analyzing a baseline segment of a single-channel local field potential recorded from rodent dorsal hippocampus during immobility. Let the discrete-time series be $\\{y_t\\}_{t=1}^{T}$ with sampling frequency $f_s$, and suppose the series admits the decomposition\n$$\ny_t \\;=\\; \\mu \\;+\\; r_t \\;+\\; \\varepsilon_t,\n$$\nwhere $r_t = r_{t-1} + \\xi_t$ with $\\{ \\xi_t \\}$ independent and identically distributed mean-zero innovations, and $\\{ \\varepsilon_t \\}$ is a zero-mean weakly stationary and ergodic process with finite long-run variance. Assume $\\{ \\xi_t \\}$ is independent of $\\{ \\varepsilon_t \\}$, and that $\\{ \\varepsilon_t \\}$ has autocovariance function $\\gamma_k = \\operatorname{Cov}(\\varepsilon_t,\\varepsilon_{t-k})$ satisfying $\\sum_{k=-\\infty}^{\\infty} |\\gamma_k|  \\infty$.\n\nYour tasks are:\n1) Using the above decomposition as the foundational representation, formally define the null and alternative hypotheses for the Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test of level stationarity of $\\{y_t\\}$.\n\n2) Starting from the core definitions of weak stationarity and the long-run variance, derive the KPSS test statistic for level stationarity based on partial sums of residuals from an intercept-only regression $y_t = \\mu + u_t$. Define the residuals $\\hat{\\varepsilon}_t = y_t - \\hat{\\mu}$, where $\\hat{\\mu}$ is the ordinary least squares (OLS) estimator of $\\mu$, and the partial sums $S_t = \\sum_{i=1}^{t} \\hat{\\varepsilon}_i$. Use a heteroskedasticity-and-autocorrelation consistent (HAC) estimator for the long-run variance with a Bartlett kernel and bandwidth $L$, i.e., $\\hat{\\sigma}^2 = \\hat{\\gamma}_0 + 2\\sum_{j=1}^{L}\\left(1 - \\frac{j}{L+1}\\right)\\hat{\\gamma}_j$, where $\\hat{\\gamma}_j = \\frac{1}{T}\\sum_{t=j+1}^{T} \\hat{\\varepsilon}_t \\hat{\\varepsilon}_{t-j}$.\n\n3) Provide the final closed-form analytic expression for the KPSS test statistic in terms of $T$, the partial sums $S_t$, and the HAC long-run variance estimate $\\hat{\\sigma}^2$. No numerical evaluation is required. The final answer must be a single analytic expression or a single row matrix of analytic expressions. No units should be included in the final answer.",
            "solution": "The problem is valid as it is scientifically grounded in established statistical theory, well-posed, and objective. All components, including the time series decomposition, definitions for residuals and partial sums, and the structure of the long-run variance estimator, are standard in the analysis of non-stationary time series. We proceed with the solution.\n\nThe problem requires a three-part answer concerning the Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test for a time series $\\{y_t\\}_{t=1}^{T}$.\n\n1) Definition of the null and alternative hypotheses.\n\nThe provided model for the time series is\n$$\ny_t = \\mu + r_t + \\varepsilon_t\n$$\nwhere $\\{ \\varepsilon_t \\}$ is a zero-mean weakly stationary process and $r_t$ is a random walk defined by $r_t = r_{t-1} + \\xi_t$, with $\\{ \\xi_t \\}$ being an independent and identically distributed process with mean $E[\\xi_t]=0$ and variance $\\operatorname{Var}(\\xi_t) = \\sigma_\\xi^2$. We can assume without loss of generality that the initial value of the random walk is $r_0=0$, as any non-zero constant $r_0$ can be absorbed into the intercept term $\\mu$.\n\nThe process $\\{y_t\\}$ is level stationary if and only if the random walk component $r_t$ is absent, meaning it remains constant over time. Since $r_t = r_{t-1} + \\xi_t$, for $r_t$ to be constant, each innovation $\\xi_t$ must be zero. Given that $E[\\xi_t]=0$, this is equivalent to the condition that the variance of the innovations is zero, i.e., $\\sigma_\\xi^2 = 0$. If $\\sigma_\\xi^2  0$, the variance of $r_t$, which is $\\operatorname{Var}(r_t) = t\\sigma_\\xi^2$, grows with time, inducing a stochastic trend and rendering $\\{y_t\\}$ non-stationary.\n\nTherefore, the KPSS test for level stationarity evaluates the following hypotheses:\nThe null hypothesis, $H_0$, states that the time series is level stationary. This corresponds to the variance of the random walk innovations being zero.\n$$\nH_0: \\sigma_\\xi^2 = 0\n$$\nUnder $H_0$, the model simplifies to $y_t = \\mu + \\varepsilon_t$, which describes a stationary process fluctuating around a constant level $\\mu$.\n\nThe alternative hypothesis, $H_1$, states that the time series has a unit root (is difference-stationary), which implies the presence of a stochastic trend. This corresponds to a positive variance for the random walk innovations.\n$$\nH_1: \\sigma_\\xi^2  0\n$$\nUnder $H_1$, the random walk component $r_t$ is present, and $\\{y_t\\}$ is non-stationary.\n\n2) Derivation of the KPSS test statistic.\n\nThe KPSS test is a Lagrange Multiplier (LM) test designed to detect the presence of a random walk component. The statistic is constructed using the residuals from a regression of $y_t$ under the null hypothesis. For level stationarity, the regression model under $H_0$ is $y_t = \\mu + u_t$.\n\nFirst, we estimate the intercept $\\mu$ via Ordinary Least Squares (OLS). The OLS estimator for $\\mu$ is the sample mean of $y_t$:\n$$\n\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^{T} y_t\n$$\nThe residuals are then calculated as the deviation from this sample mean:\n$$\n\\hat{\\varepsilon}_t = y_t - \\hat{\\mu}\n$$\nThese residuals, $\\hat{\\varepsilon}_t$, are the empirical counterpart to the unobserved stationary component $\\varepsilon_t$ (after demeaning).\n\nThe core of the test statistic is built upon the partial sums of these residuals. The partial sum process is defined as:\n$$\nS_t = \\sum_{i=1}^{t} \\hat{\\varepsilon}_i\n$$\nUnder the null hypothesis of stationarity, the process $\\{\\varepsilon_t\\}$ has a constant mean (assumed to be zero), so the partial sums of the residuals, $S_t$, are expected to fluctuate around zero. Under the alternative hypothesis, the presence of the unit root component would cause the residuals to accumulate and the partial sum process $S_t$ to diverge, behaving like a random walk itself. The test statistic is designed to measure the magnitude of this partial sum process.\n\nThe KPSS statistic is based on the sum of the squared partial sums, $\\sum_{t=1}^{T} S_t^2$. To obtain a statistic with a well-defined asymptotic distribution, this sum must be properly scaled. Based on the Functional Central Limit Theorem applied to weakly dependent processes, the scaled partial sum process $\\frac{1}{\\sqrt{T}\\sigma} S_{\\lfloor Tr \\rfloor}$ (for $r \\in [0,1]$) converges in distribution to a standard Brownian bridge, $V(r) = W(r) - rW(1)$, where $W(r)$ is a standard Wiener process. The term $\\sigma^2$ is the long-run variance of the process $\\{\\varepsilon_t\\}$, defined as:\n$$\n\\sigma^2 = \\lim_{T \\to \\infty} \\frac{1}{T} \\operatorname{Var}\\left(\\sum_{t=1}^{T} \\varepsilon_t\\right) = \\sum_{k=-\\infty}^{\\infty} \\gamma_k = \\gamma_0 + 2\\sum_{k=1}^{\\infty}\\gamma_k\n$$\nThe condition $\\sum_{k=-\\infty}^{\\infty} |\\gamma_k|  \\infty$ ensures this limit is finite.\n\nThe test statistic is constructed as an empirical analogue of the integral of the squared Brownian bridge, $\\int_0^1 V(r)^2 dr$. The sum $\\sum_{t=1}^{T} S_t^2$ corresponds to the integral, and the appropriate scaling factor is $\\frac{1}{T^2}$. The full statistic is:\n$$\n\\text{KPSS} = \\frac{1}{T^2\\sigma^2} \\sum_{t=1}^{T} S_t^2\n$$\nSince the true long-run variance $\\sigma^2$ is unknown, it must be replaced with a consistent estimator, $\\hat{\\sigma}^2$. The problem specifies a heteroskedasticity-and-autocorrelation consistent (HAC) estimator using a Bartlett kernel with bandwidth $L$:\n$$\n\\hat{\\sigma}^2 = \\hat{\\gamma}_0 + 2\\sum_{j=1}^{L}\\left(1 - \\frac{j}{L+1}\\right)\\hat{\\gamma}_j\n$$\nwhere $\\hat{\\gamma}_j = \\frac{1}{T}\\sum_{t=j+1}^{T} \\hat{\\varepsilon}_t \\hat{\\varepsilon}_{t-j}$ is the sample autocovariance at lag $j$.\n\nSubstituting this estimator into the expression for the statistic gives the final, operational form of the KPSS test statistic for level stationarity.\n\n3) Final closed-form analytic expression for the KPSS test statistic.\n\nCombining the components derived above, the KPSS test statistic, often denoted $\\eta_{\\mu}$, for level stationarity is given by the sum of squared partial sums of residuals, scaled by the sample size squared and the estimated long-run variance. In terms of the given quantities $T$, $S_t$, and $\\hat{\\sigma}^2$, the expression is:\n$$\n\\eta_{\\mu} = \\frac{\\sum_{t=1}^{T} S_t^2}{T^2 \\hat{\\sigma}^2}\n$$\nThis expression constitutes the final answer.",
            "answer": "$$\n\\boxed{\\frac{\\sum_{t=1}^{T} S_t^2}{T^2 \\hat{\\sigma}^2}}\n$$"
        },
        {
            "introduction": "The concept of stationarity is just as critical for point process data, such as neural spike trains, as it is for continuous signals. This hands-on coding challenge requires you to implement a sophisticated stationarity test based on the powerful time-rescaling theorem . By transforming interspike intervals and testing for uniformity, you will build a practical and theoretically grounded tool for validating the assumptions of renewal process models in spike train analysis.",
            "id": "4200576",
            "problem": "You are given the task of designing and implementing a complete, runnable program that tests the stationarity of interspike interval distributions in spike trains using the time-rescaling theorem and Kolmogorov–Smirnov statistics. The setting is neuroscience data analysis at the advanced graduate level. The program must be self-contained and produce a single line output in the specified format without any user input.\n\nFundamental bases and definitions to be used:\n- A spike train is modeled as a point process on time, with spike times denoted by $t_1, t_2, \\dots, t_n$ in seconds. The interspike intervals are $X_k = t_k - t_{k-1}$ for $k \\geq 2$.\n- A process has a stationary interspike interval distribution under the renewal assumption if the distribution of $X_k$ is invariant to absolute time, meaning the sequence $\\{X_k\\}$ is independent and identically distributed.\n- The conditional intensity $\\lambda(t \\mid \\mathcal{H}_t)$ of a point process is defined as the limit, for small $\\Delta t$, of the probability of an event in $[t, t+\\Delta t)$ given the history $\\mathcal{H}_t$, divided by $\\Delta t$. Under a renewal process with stationary interspike interval distribution, $\\lambda(t \\mid \\mathcal{H}_t)$ depends only on the elapsed time since the last spike, denoted $\\tau$, and equals the hazard function $h(\\tau)$.\n- The hazard function $h(\\tau)$ is related to the interspike interval cumulative distribution function $F(\\tau)$ and survival function $S(\\tau) = 1 - F(\\tau)$ by $h(\\tau) = \\frac{f(\\tau)}{S(\\tau)}$ when $f$ is the probability density function, and the integrated hazard is $H(\\tau) = \\int_0^{\\tau} h(u)\\,du = -\\log S(\\tau)$.\n- The time-rescaling theorem states that for a point process with conditional intensity $\\lambda(t \\mid \\mathcal{H}_t)$, the transformed interspike intervals $$Z_k = \\int_{t_{k-1}}^{t_k} \\lambda(u \\mid \\mathcal{H}_u)\\,du$$ are independent and identically distributed exponential random variables with mean $1$. Equivalently, $$U_k = 1 - \\exp(-Z_k)$$ are independent and identically distributed uniform random variables on $[0, 1]$.\n- Under the renewal assumption with stationary interspike interval distribution, the conditional intensity depends only on the elapsed time since last spike, so $Z_k = H(X_k)$ and $U_k = 1 - \\exp(-H(X_k)) = F(X_k)$. Therefore, testing stationarity of the interspike interval distribution reduces to testing whether the transformed values $U_k$ are uniform on $[0,1]$.\n- The Kolmogorov–Smirnov (KS) test, formally Kolmogorov–Smirnov, is a nonparametric test that compares the empirical cumulative distribution function of a sample to a reference distribution; the test statistic is $$D_n = \\sup_{u \\in [0,1]} \\left| F_n(u) - u \\right|,$$ where $F_n$ is the empirical distribution function of the sample, and under the null hypothesis of uniformity on $[0,1]$, the distribution of $D_n$ yields a $p$-value.\n\nTask requirements:\n1. Implement a nonparametric estimate of the interspike interval cumulative distribution function $F(\\tau)$ using leave-one-out Gaussian kernel smoothing. Given interspike intervals $\\{X_k\\}_{k=2}^{n}$, define a bandwidth $h$ via a robust rule-of-thumb (you may use a standard choice such as Silverman’s) and compute, for each interval $X_i$, the leave-one-out kernel-smoothed estimate $$\\widehat{F}_{-i}(X_i) = \\frac{1}{n-2} \\sum_{\\substack{j=2 \\\\ j \\neq i}}^{n} \\Phi\\!\\left(\\frac{X_i - X_j}{h}\\right),$$ where $\\Phi(\\cdot)$ is the standard normal cumulative distribution function. Use angles in radians for any trigonometric functions that may arise in test-case generation.\n2. Use the time-rescaling theorem under the renewal assumption to define $$U_i = \\widehat{F}_{-i}(X_i)$$ as approximate uniform variates if the interspike interval distribution is stationary.\n3. Apply the one-sample Kolmogorov–Smirnov test to the collection $\\{U_i\\}$ against the uniform distribution on $[0,1]$ to obtain a $p$-value. At significance level $\\alpha = 0.05$, reject the null hypothesis of stationary interspike interval distribution if $p  \\alpha$.\n\nAlgorithmic constraints:\n- The program must generate the spike trains internally according to the provided test suite and must not require any external input.\n- All time quantities must be handled in seconds. Any sine or cosine functions used in intensity definitions must interpret their angles in radians.\n- If the number of interspike intervals is less than $5$ in any test case, the test should default to “do not reject” due to insufficient data.\n\nTest suite to implement (units: seconds):\n- Case 1 (stationary, renewal gamma): Generate $n = 1200$ spikes by independent gamma-distributed interspike intervals with shape $k = 3$ and scale $\\theta = 0.02$, so mean $k\\theta = 0.06$.\n- Case 2 (nonstationary change-point): Generate $n = 1200$ spikes where the first $600$ interspike intervals are gamma with shape $k = 3$ and scale $\\theta = 0.02$, and the remaining $600$ are gamma with shape $k = 3$ and scale $\\theta = 0.04$.\n- Case 3 (nonstationary inhomogeneous Poisson): Simulate spike times on $[0, T]$ with $T = 20$ seconds by thinning a homogeneous Poisson process with rate $\\lambda_{\\max} = \\lambda_0 (1 + a)$, where the inhomogeneous rate is $$\\lambda(t) = \\lambda_0 \\left(1 + a \\sin(2\\pi f t)\\right),$$ with $\\lambda_0 = 30$ Hz, amplitude $a = 0.5$, and frequency $f = 2$ Hz. Use angles in radians for the argument of $\\sin(\\cdot)$.\n- Case 4 (low sample stationary gamma): Generate $n = 50$ spikes by independent gamma-distributed interspike intervals with shape $k = 3$ and scale $\\theta = 0.02$.\n- Case 5 (stationary, renewal log-normal): Generate $n = 1200$ spikes by independent log-normal interspike intervals with parameters $\\mu$ and $\\sigma$ chosen to yield mean approximately $0.06$ seconds; set $\\sigma = 0.5$ and solve $\\mu = \\log(0.06) - \\sigma^2/2$.\n\nImplementation and output specification:\n- Your program must construct the spike trains for the five cases, compute interspike intervals, perform the leave-one-out kernel cumulative distribution function estimation and the Kolmogorov–Smirnov test, and decide to reject or not reject the null hypothesis at $\\alpha = 0.05$ for each case.\n- The final output must be a single line containing a comma-separated list enclosed in square brackets with five boolean values in order for Cases 1–5, where each boolean indicates whether the null hypothesis of stationary interspike interval distribution is rejected for that case (True means reject, False means do not reject). For example: \"[False,True,True,False,False]\".",
            "solution": "The problem requires the design and implementation of a statistical test to validate the stationarity of interspike interval (ISI) distributions in neural spike trains. This will be accomplished by leveraging the time-rescaling theorem in conjunction with a non-parametric, leave-one-out kernel-smoothed estimate of the ISI cumulative distribution function (CDF) and the Kolmogorov-Smirnov (KS) test. The solution is presented as a self-contained program that evaluates five test cases, covering stationary, non-stationary, and non-renewal process models.\n\n### Theoretical Framework\n\nThe core of the problem rests on the properties of a stationary renewal process. For such a process, the sequence of interspike intervals, $\\{X_k\\}_{k \\ge 2}$ where $X_k = t_k - t_{k-1}$, is composed of independent and identically distributed (i.i.d.) random variables. The null hypothesis, $H_0$, for our test is that the observed spike train is generated by such a process.\n\nThe **time-rescaling theorem** provides a powerful method for analyzing point processes. It states that for any point process with a known conditional intensity function $\\lambda(t \\mid \\mathcal{H}_t)$, the transformed intervals\n$$Z_k = \\int_{t_{k-1}}^{t_k} \\lambda(u \\mid \\mathcal{H}_u)\\,du$$\nare i.i.d. exponential random variables with mean $1$. Consequently, the variables $U_k = 1 - \\exp(-Z_k)$ are i.i.d. uniform random variables on the interval $[0, 1]$.\n\nUnder the renewal assumption, the conditional intensity depends only on the time since the last spike, $\\tau = t - t_{k-1}$, and is equal to the hazard function of the ISI distribution, $h(\\tau)$. The integrated hazard is $H(\\tau) = \\int_0^\\tau h(u)\\,du$. The integral for $Z_k$ then simplifies to $Z_k = H(X_k)$. Using the relationship $H(\\tau) = -\\log(S(\\tau)) = -\\log(1 - F(\\tau))$, where $F(\\tau)$ is the ISI CDF and $S(\\tau)$ is the survival function, the rescaled variables become:\n$$U_k = 1 - \\exp(-H(X_k)) = 1 - \\exp(\\log(1 - F(X_k))) = 1 - (1 - F(X_k)) = F(X_k)$$\nThus, to test for stationarity under the renewal assumption, we can transform the observed ISIs $X_k$ into a new sequence $U_k$ using their own CDF, and then test whether the sequence $\\{U_k\\}$ is uniformly distributed on $[0,1]$.\n\n### Algorithmic Implementation\n\nSince the true ISI distribution $F(\\cdot)$ is unknown, it must be estimated from the data. The overall algorithm proceeds as follows:\n\n1.  **Spike Train Generation and ISI Extraction**: For each of the five test cases, the corresponding spike train is generated according to the specified model. The sequence of interspike intervals $\\{X_k\\}$ is then computed by taking the differences between consecutive spike times.\n    -   **Cases 1, 4, 5 (Renewal Processes)**: ISIs are directly sampled from Gamma and Log-Normal distributions.\n    -   **Case 2 (Change-Point Process)**: Two sets of ISIs are generated from Gamma distributions with different parameters and concatenated to simulate a non-stationary process. To resolve a minor contradiction in the problem statement, we adhere to the explicit spike count, generating $600$ ISIs of the first type and $1199 - 600 = 599$ of the second.\n    -   **Case 3 (Inhomogeneous Poisson Process)**: The thinning algorithm is used to generate spike times from a time-varying intensity $\\lambda(t) = \\lambda_0 (1 + a \\sin(2\\pi f t))$.\n\n2.  **Bandwidth Selection for Kernel Smoothing**: Before estimating the CDF, a suitable bandwidth parameter $h$ must be determined. We employ Silverman's rule of thumb, a standard and robust choice:\n    $$h = 0.9 \\cdot A \\cdot N_{ISI}^{-1/5}$$\n    where $N_{ISI}$ is the number of interspike intervals, and $A = \\min(\\hat{\\sigma}, \\frac{\\text{IQR}}{1.349})$. Here, $\\hat{\\sigma}$ is the sample standard deviation and $\\text{IQR}$ is the interquartile range of the observed ISIs.\n\n3.  **Leave-One-Out CDF Estimation**: To transform the ISIs $\\{X_k\\}$ to uniform variates $\\{U_k\\}$, we need an estimate of $F(X_k)$. A naive kernel CDF estimate evaluated at the data points used to construct it would be biased. To mitigate this, a leave-one-out cross-validation approach is specified. For each interval $X_i$, we compute an estimate of the CDF, $\\widehat{F}_{-i}$, built from all other intervals $\\{X_j\\}_{j \\neq i}$. The transformed variable $U_i$ is then:\n    $$U_i = \\widehat{F}_{-i}(X_i) = \\frac{1}{N_{ISI} - 1} \\sum_{j \\neq i} \\Phi\\left(\\frac{X_i - X_j}{h}\\right)$$\n    where $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution, which serves as the kernel. The normalization factor $N_{ISI}-1$ is the number of terms in the summation.\n\n4.  **Hypothesis Testing with Kolmogorov-Smirnov**: The sequence of transformed intervals $\\{U_i\\}$ is tested for uniformity on $[0,1]$ using the one-sample Kolmogorov-Smirnov (KS) test. The KS test statistic is the maximum absolute difference between the empirical CDF of the sample $\\{U_i\\}$ and the CDF of the standard uniform distribution:\n    $$D = \\sup_{u \\in [0,1]} |\\widehat{G}(u) - u|$$\n    where $\\widehat{G}(u)$ is the empirical CDF of the $\\{U_i\\}$. A $p$-value is calculated based on the distribution of $D$.\n\n5.  **Decision**: The null hypothesis of a stationary ISI distribution is rejected if the obtained $p$-value is less than the significance level $\\alpha = 0.05$. In accordance with the problem constraints, if a test case yields fewer than $5$ ISIs, the test is not performed and defaults to \"do not reject\". This complete procedure is applied to each of the five test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import gamma, lognorm, norm, kstest\n\ndef solve():\n    \"\"\"\n    Designs and implements a test for the stationarity of interspike interval \n    distributions using the time-rescaling theorem and Kolmogorov-Smirnov statistics.\n    \"\"\"\n\n    # Set a random seed for reproducibility of the stochastic simulations.\n    np.random.seed(42)\n\n    def generate_case1_data():\n        \"\"\"\n        Case 1: Stationary renewal gamma process.\n        Generates 1199 ISIs from a Gamma(k=3, theta=0.02) distribution.\n        \"\"\"\n        n_spikes = 1200\n        n_isi = n_spikes - 1\n        k, theta = 3.0, 0.02\n        return gamma.rvs(a=k, scale=theta, size=n_isi)\n\n    def generate_case2_data():\n        \"\"\"\n        Case 2: Nonstationary change-point process.\n        Generates 1199 ISIs, with a change in distribution parameters midway.\n        This resolves the minor contradiction in the problem statement by strictly\n        adhering to the n=1200 spikes count (1199 ISIs).\n        \"\"\"\n        k, theta1, theta2 = 3.0, 0.02, 0.04\n        isis1 = gamma.rvs(a=k, scale=theta1, size=600)\n        isis2 = gamma.rvs(a=k, scale=theta2, size=599)\n        return np.concatenate((isis1, isis2))\n\n    def generate_case3_data():\n        \"\"\"\n        Case 3: Nonstationary inhomogeneous Poisson process.\n        Simulates spikes using a thinning algorithm with a sinusoidal rate.\n        \"\"\"\n        T, lambda_0, a, f = 20.0, 30.0, 0.5, 2.0\n        lambda_max = lambda_0 * (1.0 + a)\n        \n        t = 0.0\n        spike_times = []\n        while t  T:\n            # Generate next candidate spike time from homogeneous process\n            t += np.random.exponential(scale=1.0 / lambda_max)\n            if t >= T:\n                break\n            \n            # Acceptance/rejection step (thinning)\n            lambda_t = lambda_0 * (1.0 + a * np.sin(2 * np.pi * f * t))\n            if np.random.uniform(0.0, 1.0)  lambda_t / lambda_max:\n                spike_times.append(t)\n        \n        return np.diff(spike_times) if len(spike_times) > 1 else np.array([])\n    \n    def generate_case4_data():\n        \"\"\"\n        Case 4: Low sample stationary renewal gamma process.\n        Generates 49 ISIs from a Gamma(k=3, theta=0.02) distribution.\n        \"\"\"\n        n_spikes = 50\n        n_isi = n_spikes - 1\n        k, theta = 3.0, 0.02\n        return gamma.rvs(a=k, scale=theta, size=n_isi)\n\n    def generate_case5_data():\n        \"\"\"\n        Case 5: Stationary renewal log-normal process.\n        Generates 1199 ISIs from a Log-Normal distribution.\n        \"\"\"\n        n_spikes = 1200\n        n_isi = n_spikes - 1\n        sigma_param, mean_isi = 0.5, 0.06\n        mu_param = np.log(mean_isi) - sigma_param**2 / 2.0\n        # For scipy.stats.lognorm, s=sigma and scale=exp(mu).\n        return lognorm.rvs(s=sigma_param, scale=np.exp(mu_param), size=n_isi)\n\n    def perform_stationarity_test(isis, alpha=0.05):\n        \"\"\"\n        Performs the stationarity test on a sequence of ISIs.\n        \"\"\"\n        n_isi = len(isis)\n        if n_isi  5:\n            return False  # Do not reject for insufficient data\n\n        # Step 1: Calculate bandwidth h using Silverman's rule of thumb\n        std_dev = np.std(isis, ddof=1)\n        iqr = np.percentile(isis, 75) - np.percentile(isis, 25)\n        \n        if iqr > 1e-9:  # Add tolerance for IQR being zero\n            a_val = min(std_dev, iqr / 1.349)\n        else:\n            a_val = std_dev\n\n        h = 0.9 * a_val * (n_isi**(-1/5))\n        if h  1e-9:  # Handle degenerate case of zero bandwidth\n            return False\n\n        # Step 2: Calculate U_i values using leave-one-out kernel-smoothed CDF\n        u_values = np.zeros(n_isi)\n        for i in range(n_isi):\n            # Vectorized computation for the sum over j != i\n            diffs = (isis[i] - np.delete(isis, i)) / h\n            u_values[i] = np.sum(norm.cdf(diffs)) / (n_isi - 1)\n            \n        # Step 3: Perform one-sample KS test against a uniform distribution\n        ks_result = kstest(u_values, 'uniform')\n        \n        # Step 4: Reject null hypothesis if p-value is below alpha\n        return ks_result.pvalue  alpha\n\n    # Define the list of test case generation functions.\n    test_case_generators = [\n        generate_case1_data,\n        generate_case2_data,\n        generate_case3_data,\n        generate_case4_data,\n        generate_case5_data\n    ]\n\n    # Run the test for each case and collect the results.\n    results = []\n    for gen_func in test_case_generators:\n        isis_data = gen_func()\n        is_rejected = perform_stationarity_test(isis_data)\n        results.append(is_rejected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}