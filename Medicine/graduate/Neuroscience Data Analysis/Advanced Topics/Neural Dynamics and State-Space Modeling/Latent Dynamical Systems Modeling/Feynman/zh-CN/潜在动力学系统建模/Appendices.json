{
    "hands_on_practices": [
        {
            "introduction": "为了将潜动力系统模型与观测数据进行拟合，一个核心步骤是评估模型参数在多大程度上能够解释这些数据。本练习将探讨线性高斯状态空间模型（LGSSM）中一个奠基性的计算过程：推导边际似然。通过这个练习，你将把模型拟合的抽象目标与卡尔曼滤波器（一种在时间序列分析中至关重要的算法）的具体步骤联系起来，从而理解模型选择与参数估计的理论基础。",
            "id": "4173383",
            "problem": "考虑一个用于介观双光子钙成像实验中潜在群体活动的线性高斯状态空间模型（LGSSM）。潜在状态 $\\mathbf{x}_t \\in \\mathbb{R}^n$ 捕捉了低维神经动力学，观测值 $\\mathbf{y}_t \\in \\mathbb{R}^m$ 代表了 $m$ 个感兴趣区域的荧光强度。该模型由以下生成性假设指定：\n$$\n\\mathbf{x}_1 \\sim \\mathcal{N}(\\mathbf{m}_0, \\mathbf{P}_0), \\quad \\mathbf{x}_{t+1} = \\mathbf{A}\\,\\mathbf{x}_t + \\mathbf{w}_t, \\quad \\mathbf{w}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q}),\n$$\n$$\n\\mathbf{y}_t = \\mathbf{C}\\,\\mathbf{x}_t + \\mathbf{v}_t, \\quad \\mathbf{v}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R}),\n$$\n其中 $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$、$\\mathbf{C} \\in \\mathbb{R}^{m \\times n}$、$\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$、$\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$、$\\mathbf{m}_0 \\in \\mathbb{R}^n$ 和 $\\mathbf{P}_0 \\in \\mathbb{R}^{n \\times n}$ 是已知的。随机变量 $\\mathbf{x}_1$、$\\{\\mathbf{w}_t\\}_{t=1}^{T-1}$ 和 $\\{\\mathbf{v}_t\\}_{t=1}^{T}$ 相互独立，并且该过程对于 $\\mathbf{x}_t$ 是一阶马尔可夫过程，在给定 $\\mathbf{x}_t$ 的条件下观测值是条件独立的。假设 $\\mathbf{y}_{1:T}$ 是在 $T$ 个时间步长内观测到的荧光测量值，其中 $T \\in \\mathbb{N}$。\n\n从这些假设所隐含的联合分布因子分解出发，并且只使用多元高斯分布的性质和条件期望的塔式性质，对潜在状态执行递归高斯积分，以获得边际似然 $p(\\mathbf{y}_{1:T})$。您的推导过程必须通过将 $p(\\mathbf{y}_{1:T})$ 表示为由卡尔曼滤波器的创新序列构建的单步预测观测密度的乘积。根据卡尔曼预测均值和协方差来定义创新及其协方差，但不要假设任何稳态或渐近公式。\n\n请以单一闭式解析表达式的形式，提供 $p(\\mathbf{y}_{1:T})$ 的最终答案，该表达式应使用作用于 $\\mathbf{y}_{1:T}$ 的卡尔曼滤波器生成的创新向量 $\\{\\mathbf{e}_t\\}_{t=1}^{T}$ 和创新协方差 $\\{\\mathbf{S}_t\\}_{t=1}^{T}$ 来表示。此外，请说明模型参数以及由此产生的预测协方差需满足的最小条件，以使该计算是可行的（即可积，且所需的矩阵逆和行列式存在且为有限值）。不需要进行数值评估。请以符号形式表达最终答案，不要包含任何单位。",
            "solution": "### 边际似然的推导\n\n目标是计算观测值的边际似然 $p(\\mathbf{y}_{1:T})$。这个量是通过对所有潜在状态 $\\mathbf{x}_{1:T}$ 的联合概率分布进行积分得到的：\n$$\np(\\mathbf{y}_{1:T}) = \\int p(\\mathbf{y}_{1:T}, \\mathbf{x}_{1:T}) d\\mathbf{x}_{1:T}\n$$\n问题规定，推导过程应通过将 $p(\\mathbf{y}_{1:T})$ 表示为单步预测观测密度的乘积来进行。这可以通过对观测序列应用概率的链式法则来实现：\n$$\np(\\mathbf{y}_{1:T}) = p(\\mathbf{y}_T | \\mathbf{y}_{1:T-1}) p(\\mathbf{y}_{T-1} | \\mathbf{y}_{1:T-2}) \\dots p(\\mathbf{y}_2 | \\mathbf{y}_1) p(\\mathbf{y}_1)\n$$\n这可以紧凑地写成：\n$$\np(\\mathbf{y}_{1:T}) = \\prod_{t=1}^{T} p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})\n$$\n其中，当 $t=1$ 时，条件集为空，因此第一项就是边际密度 $p(\\mathbf{y}_1)$。\n\n推导的核心是为该乘积中的每一项 $p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})$ 找到一个表达式。由于整个模型是一个线性高斯状态空间模型，所以整个联合分布 $p(\\mathbf{x}_{1:T}, \\mathbf{y}_{1:T})$ 是高斯的。因此，从中导出的所有边际分布和条件分布也都是高斯的。所以，单步预测观测密度 $p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})$ 是高斯的，我们的任务就简化为找到它的均值和协方差。\n\n这正是卡尔曼滤波算法所计算的。卡尔曼滤波器提供了一种获取状态后验分布的递归方法。让我们将以截至时间 $t-1$ 的观测值为条件的状态分布的均值和协方差表示为：\n$$\np(\\mathbf{x}_t | \\mathbf{y}_{1:t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\mathbf{m}_{t|t-1}, \\mathbf{P}_{t|t-1})\n$$\n这是状态的单步预测分布。密度 $p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})$ 可以通过对 $\\mathbf{x}_t$ 进行边缘化得到：\n$$\np(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\int p(\\mathbf{y}_t | \\mathbf{x}_t, \\mathbf{y}_{1:t-1}) p(\\mathbf{x}_t | \\mathbf{y}_{1:t-1}) d\\mathbf{x}_t\n$$\n根据模型中的条件独立性假设，$p(\\mathbf{y}_t | \\mathbf{x}_t, \\mathbf{y}_{1:t-1}) = p(\\mathbf{y}_t | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{y}_t; \\mathbf{C}\\mathbf{x}_t, \\mathbf{R})$。因此，我们是在对两个高斯分布的乘积进行积分，结果仍为高斯分布。\n\n我们可以使用条件期望和协方差定律来求得 $p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})$ 的均值和协方差。\n均值通过期望的塔式性质求得：\n$$\nE[\\mathbf{y}_t | \\mathbf{y}_{1:t-1}] = E[E[\\mathbf{y}_t | \\mathbf{x}_t, \\mathbf{y}_{1:t-1}] | \\mathbf{y}_{1:t-1}] = E[E[\\mathbf{C}\\mathbf{x}_t + \\mathbf{v}_t | \\mathbf{x}_t] | \\mathbf{y}_{1:t-1}]\n$$\n$$\nE[\\mathbf{y}_t | \\mathbf{y}_{1:t-1}] = E[\\mathbf{C}\\mathbf{x}_t | \\mathbf{y}_{1:t-1}] = \\mathbf{C} E[\\mathbf{x}_t | \\mathbf{y}_{1:t-1}] = \\mathbf{C} \\mathbf{m}_{t|t-1}\n$$\n协方差为：\n$$\n\\text{Cov}(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\text{Cov}(\\mathbf{C}\\mathbf{x}_t + \\mathbf{v}_t | \\mathbf{y}_{1:t-1})\n$$\n由于 $\\mathbf{v}_t$ 独立于 $\\mathbf{x}_t$ 和所有先前的观测值 $\\mathbf{y}_{1:t-1}$，其协方差与条件无关：\n$$\n\\text{Cov}(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\text{Cov}(\\mathbf{C}\\mathbf{x}_t | \\mathbf{y}_{1:t-1}) + \\text{Cov}(\\mathbf{v}_t) = \\mathbf{C} \\text{Cov}(\\mathbf{x}_t | \\mathbf{y}_{1:t-1}) \\mathbf{C}^T + \\mathbf{R}\n$$\n$$\n\\text{Cov}(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T + \\mathbf{R}\n$$\n这个量被定义为创新协方差，$\\mathbf{S}_t$。\n$$\n\\mathbf{S}_t \\equiv \\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T + \\mathbf{R}\n$$\n所以，单步预测观测分布为：\n$$\np(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\mathcal{N}(\\mathbf{y}_t; \\mathbf{C} \\mathbf{m}_{t|t-1}, \\mathbf{S}_t)\n$$\n创新向量 $\\mathbf{e}_t$ 定义为实际观测值 $\\mathbf{y}_t$ 与其单步预测值之间的差：\n$$\n\\mathbf{e}_t \\equiv \\mathbf{y}_t - E[\\mathbf{y}_t | \\mathbf{y}_{1:t-1}] = \\mathbf{y}_t - \\mathbf{C} \\mathbf{m}_{t|t-1}\n$$\n因此，创新的分布（以过去观测值为条件）为 $\\mathbf{e}_t | \\mathbf{y}_{1:t-1} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{S}_t)$。因此，密度 $p(\\mathbf{y}_t | \\mathbf{y}_{1:t-1})$ 可以优雅地表示为创新向量的概率密度：\n$$\np(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\frac{1}{\\sqrt{(2\\pi)^m \\det(\\mathbf{S}_t)}} \\exp\\left(-\\frac{1}{2} (\\mathbf{y}_t - \\mathbf{C}\\mathbf{m}_{t|t-1})^T \\mathbf{S}_t^{-1} (\\mathbf{y}_t - \\mathbf{C}\\mathbf{m}_{t|t-1})\\right)\n$$\n$$\np(\\mathbf{y}_t | \\mathbf{y}_{1:t-1}) = \\frac{1}{\\sqrt{(2\\pi)^m \\det(\\mathbf{S}_t)}} \\exp\\left(-\\frac{1}{2} \\mathbf{e}_t^T \\mathbf{S}_t^{-1} \\mathbf{e}_t\\right)\n$$\n这里，$m$ 是观测向量 $\\mathbf{y}_t$ 的维度。量 $\\mathbf{m}_{t|t-1}$ 和 $\\mathbf{P}_{t|t-1}$ 由卡尔曼滤波器的预测步骤产生，该步骤更新了上一步的滤波估计值 $\\mathbf{m}_{t-1|t-1}$ 和 $\\mathbf{P}_{t-1|t-1}$。递归从 $t=1$ 开始，使用先验信息 $p(\\mathbf{x}_1) = \\mathcal{N}(\\mathbf{x}_1; \\mathbf{m}_0, \\mathbf{P}_0)$，这意味着我们设置 $\\mathbf{m}_{1|0} = \\mathbf{m}_0$ 和 $\\mathbf{P}_{1|0} = \\mathbf{P}_0$。\n\n将此表达式代入边际似然的乘积形式中，我们得到：\n$$\np(\\mathbf{y}_{1:T}) = \\prod_{t=1}^{T} \\left[ \\frac{1}{\\sqrt{(2\\pi)^m \\det(\\mathbf{S}_t)}} \\exp\\left(-\\frac{1}{2} \\mathbf{e}_t^T \\mathbf{S}_t^{-1} \\mathbf{e}_t\\right) \\right]\n$$\n这个表达式可以通过将乘积分解为一个归一化常数项和一个指数项来重写：\n$$\np(\\mathbf{y}_{1:T}) = \\left(\\prod_{t=1}^{T} (2\\pi)^{-m/2} (\\det(\\mathbf{S}_t))^{-1/2} \\right) \\exp\\left(-\\frac{1}{2} \\sum_{t=1}^{T} \\mathbf{e}_t^T \\mathbf{S}_t^{-1} \\mathbf{e}_t\\right)\n$$\n这是用创新及其协方差表示的边际似然的最终解析表达式。\n\n### 可行性条件\n\n为了使边际似然 $p(\\mathbf{y}_{1:T})$ 的计算可行，乘积中的每一项都必须是有明确定义且为有限值。每一步 $t$ 的高斯密度公式都涉及两个关键运算：行列式 $\\det(\\mathbf{S}_t)$ 和逆 $\\mathbf{S}_t^{-1}$。\n1.  行列式 $\\det(\\mathbf{S}_t)$ 出现在分母中，所以它必须非零。\n2.  矩阵的逆 $\\mathbf{S}_t^{-1}$ 必须存在。\n这两个条件当且仅当矩阵 $\\mathbf{S}_t$ 对所有 $t=1, \\dots, T$ 都是可逆的时候才满足。\n\n矩阵 $\\mathbf{S}_t = \\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T + \\mathbf{R}$ 是两个对称半正定矩阵（因为 $\\mathbf{P}_{t|t-1}$ 和 $\\mathbf{R}$ 是协方差矩阵）的和，因此 $\\mathbf{S}_t$ 本身也是对称半正定的。对于一个对称半正定矩阵，可逆等价于正定。\n\n因此，计算可行的最小充分必要条件是创新协方差矩阵 $\\mathbf{S}_t$ 对所有 $t \\in \\{1, \\dots, T\\}$ 都必须是正定的。\n\n一个能确保此条件的、对模型参数而言常见的实用充分条件是观测噪声协方差矩阵 $\\mathbf{R}$ 是正定的。如果 $\\mathbf{R}$ 是正定的，那么对于任何非零向量 $\\mathbf{z} \\in \\mathbb{R}^m$，有 $\\mathbf{z}^T \\mathbf{R} \\mathbf{z} > 0$。由于 $\\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T$ 是半正定的，所以 $\\mathbf{z}^T (\\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T) \\mathbf{z} \\ge 0$。因此，和 $\\mathbf{z}^T \\mathbf{S}_t \\mathbf{z} = \\mathbf{z}^T (\\mathbf{C} \\mathbf{P}_{t|t-1} \\mathbf{C}^T) \\mathbf{z} + \\mathbf{z}^T \\mathbf{R} \\mathbf{z}$ 将严格为正，从而证明 $\\mathbf{S}_t$ 是正定的。",
            "answer": "$$\n\\boxed{\\left(\\prod_{t=1}^{T} (2\\pi)^{-m/2} (\\det(\\mathbf{S}_t))^{-1/2} \\right) \\exp\\left(-\\frac{1}{2} \\sum_{t=1}^{T} \\mathbf{e}_t^T \\mathbf{S}_t^{-1} \\mathbf{e}_t\\right)}\n$$"
        },
        {
            "introduction": "在处理真实的神经科学数据（如神经元发放计数）时，我们通常需要从理想的高斯假设转向更符合数据特征的非高斯统计模型，例如泊松分布。本练习要求你推导泊松对数似然函数相对于模型参数的梯度和Hessian矩阵。这些导数是使用高效优化算法（如牛顿法）来拟合模型参数的关键，掌握这项计算是将在神经元脉冲序列上应用潜变量模型的关键技能。",
            "id": "4173371",
            "problem": "考虑一个用于神经科学数据分析的潜（在）动力系统模型，其中多神经元脉冲计数被建模为在给定固定潜在轨迹下的条件独立泊松随机变量。假设在 $T$ 个时间窗内记录了 $n$ 个神经元，维度为 $k$ 的潜在状态由 $\\{x_t\\}_{t=1}^{T}$ 表示，其中 $x_t \\in \\mathbb{R}^{k}$。观测到的脉冲计数为 $\\{y_t\\}_{t=1}^{T}$，其中 $y_t \\in \\mathbb{N}^{n}$，时间 $t$ 的条件强度由逐元素的指数链接函数建模\n$$\n\\lambda_t = \\exp(C x_t + d),\n$$\n其中 $C \\in \\mathbb{R}^{n \\times k}$ 和 $d \\in \\mathbb{R}^{n}$ 是目标参数，$\\exp(\\cdot)$ 逐元素作用。泊松对数似然（不含与 $(C,d)$ 无关的加性常数）为\n$$\n\\mathcal{L}(C,d) = \\sum_{t=1}^{T} y_t^{\\top} (C x_t + d) - \\mathbf{1}^{\\top} \\exp(C x_t + d),\n$$\n其中 $\\mathbf{1} \\in \\mathbb{R}^{n}$ 是全为1的向量。\n\n从泊松模型的基本原理、多元微积分的链式法则以及基本的矩阵微积分恒等式出发，推导以下各项的闭式表达式：\n- $\\mathcal{L}(C,d)$ 关于 $C$ 和 $d$ 的梯度，以及\n- $\\mathcal{L}(C,d)$ 关于 $(C,d)$ 的海森矩阵（以分块形式表示）。\n\n您的推导必须清楚地表明对 $\\{x_t\\}_{t=1}^{T}$ 和 $\\{y_t\\}_{t=1}^{T}$ 的依赖关系，并酌情使用标准算子，包括向量化算子 $\\operatorname{vec}(\\cdot)$ 和克罗内克积 $\\otimes$。您可以使用恒等式 $\\operatorname{vec}(A X B) = (B^{\\top} \\otimes A)\\operatorname{vec}(X)$，其中矩阵 $A, X, B$ 的维度是相容的。假设 $\\exp(\\cdot)$ 表示逐元素指数运算，$\\operatorname{diag}(v)$ 表示由向量 $v$ 构成的对角矩阵。\n\n将您的最终答案表示为解析表达式。最终答案必须使用 $\\operatorname{pmatrix}$ 环境，将梯度和海森矩阵作为一个复合对象在一个行矩阵中一并呈现。无需进行数值近似。",
            "solution": "### 梯度与海森矩阵的推导\n\n对数似然函数 $\\mathcal{L}(C,d)$ 是对每个时间步长的对数似然 $\\mathcal{L}_t(C,d)$ 的总和：\n$$\n\\mathcal{L}(C,d) = \\sum_{t=1}^{T} \\mathcal{L}_t(C,d) = \\sum_{t=1}^{T} \\left( y_t^{\\top} (C x_t + d) - \\mathbf{1}^{\\top} \\exp(C x_t + d) \\right)\n$$\n由于求导是线性运算，我们可以先计算 $\\mathcal{L}_t$ 的导数，然后对所有时间步 $t$ 求和。\n\n为了简化计算，我们定义指数函数的线性输入为 $u_t = C x_t + d$，其中 $u_t \\in \\mathbb{R}^{n}$。条件强度（发放率）则为 $\\lambda_t = \\exp(u_t)$，其中指数函数是逐元素应用的。单个时间步的对数似然可以写作 $\\mathcal{L}_t = y_t^{\\top} u_t - \\mathbf{1}^{\\top} \\lambda_t$。\n\n#### 1. 梯度计算\n\n我们首先使用链式法则，计算 $\\mathcal{L}_t$ 关于中间变量 $u_t$ 的梯度。这是一个偏导数向量，其第 $i$ 个元素为：\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial u_{it}} = \\frac{\\partial}{\\partial u_{it}} \\left( \\sum_{j=1}^{n} y_{jt} u_{jt} - \\sum_{j=1}^{n} \\exp(u_{jt}) \\right) = y_{it} - \\exp(u_{it}) = y_{it} - \\lambda_{it}\n$$\n写成向量形式，关于 $u_t$ 的梯度是：\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial u_t} = y_t - \\lambda_t\n$$\n\n**关于 $d$ 的梯度**：\n$u_t = C x_t + d$ 关于 $d$ 的雅可比矩阵是单位矩阵，即 $\\frac{\\partial u_t}{\\partial d} = I_n \\in \\mathbb{R}^{n \\times n}$。应用链式法则：\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial d} = \\left(\\frac{\\partial u_t}{\\partial d}\\right)^{\\top} \\frac{\\partial \\mathcal{L}_t}{\\partial u_t} = I_n^{\\top} (y_t - \\lambda_t) = y_t - \\lambda_t\n$$\n对所有时间步 $t$ 求和，得到总梯度：\n$$\n\\nabla_d \\mathcal{L}(C,d) = \\sum_{t=1}^{T} (y_t - \\lambda_t) = \\sum_{t=1}^{T} (y_t - \\exp(C x_t + d))\n$$\n\n**关于 $C$ 的梯度**：\n这是一个矩阵导数。我们可以通过计算梯度矩阵的第 $(i,j)$ 个元素 $\\frac{\\partial \\mathcal{L}_t}{\\partial C_{ij}}$ 来推导。\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial C_{ij}} = \\sum_{p=1}^{n} \\frac{\\partial \\mathcal{L}_t}{\\partial u_{pt}} \\frac{\\partial u_{pt}}{\\partial C_{ij}}\n$$\n由于 $u_{pt} = \\sum_{q=1}^{k} C_{pq} x_{qt} + d_p$，其关于 $C_{ij}$ 的导数为 $\\frac{\\partial u_{pt}}{\\partial C_{ij}} = \\delta_{pi} x_{jt}$（其中 $\\delta_{pi}$ 是克罗内克德尔塔）。因此，求和中只有 $p=i$ 的项非零：\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial C_{ij}} = \\frac{\\partial \\mathcal{L}_t}{\\partial u_{it}} \\frac{\\partial u_{it}}{\\partial C_{ij}} = (y_{it} - \\lambda_{it}) x_{jt}\n$$\n这恰好是外积 $(y_t - \\lambda_t) x_t^{\\top}$ 的第 $(i,j)$ 个元素。因此，梯度矩阵为：\n$$\n\\nabla_C \\mathcal{L}_t(C,d) = (y_t - \\lambda_t) x_t^{\\top}\n$$\n对所有时间步 $t$ 求和：\n$$\n\\nabla_C \\mathcal{L}(C,d) = \\sum_{t=1}^{T} (y_t - \\lambda_t) x_t^{\\top} = \\sum_{t=1}^{T} (y_t - \\exp(C x_t + d)) x_t^{\\top}\n$$\n\n#### 2. 海森矩阵计算\n\n海森矩阵是二阶导数的矩阵。我们将参数构造成一个组合向量 $\\theta = \\begin{pmatrix} \\operatorname{vec}(C) \\\\ d \\end{pmatrix}$。海森矩阵 $H = \\nabla^2_{\\theta} \\mathcal{L}(C,d)$ 具有一个 $2 \\times 2$ 的分块结构，我们将计算每个分块 $H_t$ 并求和。\n\n**右下角分块 ($H_{dd,t}$)**：\n我们对 $\\frac{\\partial \\mathcal{L}_t}{\\partial d} = y_t - \\lambda_t$ 关于 $d^{\\top}$ 求导。\n$$\n\\frac{\\partial^2 \\mathcal{L}_t}{\\partial d \\partial d^{\\top}} = \\frac{\\partial}{\\partial d^{\\top}}(y_t - \\lambda_t) = -\\frac{\\partial \\lambda_t}{\\partial d^{\\top}} = -\\frac{\\partial \\exp(u_t)}{\\partial u_t^{\\top}} \\frac{\\partial u_t}{\\partial d^{\\top}}\n$$\n雅可比矩阵 $\\frac{\\partial \\exp(u_t)}{\\partial u_t^{\\top}}$ 是一个对角矩阵 $\\operatorname{diag}(\\lambda_t)$，而 $\\frac{\\partial u_t}{\\partial d^{\\top}}$ 是单位矩阵 $I_n$。\n$$\n\\frac{\\partial^2 \\mathcal{L}_t}{\\partial d \\partial d^{\\top}} = -\\operatorname{diag}(\\lambda_t)\n$$\n\n**非对角分块 ($H_{dC,t}$ 和 $H_{Cd,t}$)**：\n我们对 $\\frac{\\partial \\mathcal{L}_t}{\\partial d} = y_t - \\lambda_t$ 关于 $\\operatorname{vec}(C)^{\\top}$ 求导。\n$$\n\\frac{\\partial^2 \\mathcal{L}_t}{\\partial d \\partial \\operatorname{vec}(C)^{\\top}} = -\\frac{\\partial \\lambda_t}{\\partial \\operatorname{vec}(C)^{\\top}} = -\\frac{\\partial \\lambda_t}{\\partial u_t^{\\top}} \\frac{\\partial u_t}{\\partial \\operatorname{vec}(C)^{\\top}} = -\\operatorname{diag}(\\lambda_t) \\frac{\\partial u_t}{\\partial \\operatorname{vec}(C)^{\\top}}\n$$\n我们有 $u_t - d = C x_t$。对其进行向量化并使用恒等式 $\\operatorname{vec}(AXB) = (B^\\top \\otimes A) \\operatorname{vec}(X)$，可得 $\\operatorname{vec}(C x_t) = (x_t^\\top \\otimes I_n) \\operatorname{vec}(C)$。因此，雅可比矩阵 $\\frac{\\partial u_t}{\\partial \\operatorname{vec}(C)^{\\top}} = x_t^\\top \\otimes I_n$。\n$$\nH_{dC,t} = \\frac{\\partial^2 \\mathcal{L}_t}{\\partial d \\partial \\operatorname{vec}(C)^{\\top}} = -\\operatorname{diag}(\\lambda_t) (x_t^\\top \\otimes I_n) = -x_t^\\top \\otimes \\operatorname{diag}(\\lambda_t)\n$$\n右上角分块是其转置：$H_{Cd,t} = (H_{dC,t})^{\\top} = -x_t \\otimes \\operatorname{diag}(\\lambda_t)$。\n\n**左上角分块 ($H_{CC,t}$)**：\n对梯度 $\\nabla_C \\mathcal{L}_t = (y_t - \\lambda_t) x_t^\\top$ 进行向量化，得到 $\\operatorname{vec}(\\nabla_C \\mathcal{L}_t) = x_t \\otimes (y_t - \\lambda_t)$。我们对它关于 $\\operatorname{vec}(C)^\\top$ 求导。\n$$\nH_{CC,t} = \\frac{\\partial \\operatorname{vec}(\\nabla_C \\mathcal{L}_t)}{\\partial \\operatorname{vec}(C)^\\top} = \\frac{\\partial}{\\partial \\operatorname{vec}(C)^\\top} \\left( x_t \\otimes (y_t - \\lambda_t) \\right) = x_t \\otimes \\frac{\\partial(y_t - \\lambda_t)}{\\partial \\operatorname{vec}(C)^\\top}\n$$\n其中导数项为 $-\\frac{\\partial \\lambda_t}{\\partial \\operatorname{vec}(C)^\\top} = - \\operatorname{diag}(\\lambda_t) (x_t^\\top \\otimes I_n)$。\n$$\nH_{CC,t} = x_t \\otimes \\left(-\\operatorname{diag}(\\lambda_t) (x_t^\\top \\otimes I_n)\\right)\n$$\n使用克罗内克积的性质，这可以简化为：\n$$\nH_{CC,t} = -(x_t x_t^\\top) \\otimes \\operatorname{diag}(\\lambda_t)\n$$\n\n**结果总结**\n将各分块组合起来，并对所有时间步长 $t$ 求和，得到完整的海森矩阵：\n$$\nH(C,d) = \\sum_{t=1}^T H_t = -\\sum_{t=1}^{T} \\begin{pmatrix} (x_t x_t^\\top) \\otimes \\operatorname{diag}(\\lambda_t)  x_t \\otimes \\operatorname{diag}(\\lambda_t) \\\\ x_t^\\top \\otimes \\operatorname{diag}(\\lambda_t)  \\operatorname{diag}(\\lambda_t) \\end{pmatrix}\n$$\n其中在所有表达式中，$\\lambda_t = \\exp(C x_t + d)$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\begin{pmatrix} \\operatorname{vec}\\left(\\sum_{t=1}^{T} (y_t - \\exp(C x_t + d)) x_t^{\\top}\\right) \\\\ \\sum_{t=1}^{T} (y_t - \\exp(C x_t + d)) \\end{pmatrix}\n\n-\\sum_{t=1}^{T} \\begin{pmatrix} (x_t x_t^\\top) \\otimes \\operatorname{diag}(\\exp(C x_t + d))  x_t \\otimes \\operatorname{diag}(\\exp(C x_t + d)) \\\\ x_t^\\top \\otimes \\operatorname{diag}(\\exp(C x_t + d))  \\operatorname{diag}(\\exp(C x_t + d)) \\end{pmatrix}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "当模型成功拟合数据后，关键的下一步是从中提取科学见解。本练习将指导你如何通过分析一个已拟合的非线性动力系统来解释其内在动力学。你将学习如何通过寻找系统的不动点并评估其局部稳定性，来揭示模型所捕捉到的计算原理，而线性化和特征值分析正是实现这一目标的核心技术。",
            "id": "4173330",
            "problem": "考虑一个用于神经群体活动的潜在自主连续时间动力学系统，其模型为常微分方程（ODE）：$$\\frac{d\\mathbf{x}}{dt} = f_{\\theta}(\\mathbf{x}),$$ 其中，估计的向量场通过逐元素的双曲正切函数逐分量定义为 $$f_{\\theta}(\\mathbf{x}) = \\mathbf{W}\\,\\tanh\\!\\big(\\mathbf{V}\\,\\mathbf{x} + \\mathbf{b}\\big) + \\mathbf{C}\\,\\mathbf{x} + \\mathbf{d},$$ 参数元组为 $\\theta = (\\mathbf{W}, \\mathbf{V}, \\mathbf{C}, \\mathbf{b}, \\mathbf{d})$。一个候选不动点 $\\mathbf{x}^{\\star}$ 满足 $f_{\\theta}(\\mathbf{x}^{\\star}) = \\mathbf{0}$。在 $\\mathbf{x}^{\\star}$ 附近的局部线性化使用雅可比矩阵 $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$，该矩阵由多元微积分的核心原理（链式法则和线性化定义）定义，而连续时间系统的局部稳定性由 $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$ 的特征值决定：当且仅当所有特征值的实部都严格为负时，该不动点是渐近稳定的。此问题不涉及任何物理单位。\n\n你的任务是编写一个完整的程序，对每个提供的测试用例，纯粹根据第一性原理执行以下步骤：\n- 使用雅可比矩阵的定义以及针对线性映射和逐元素 $\\tanh(\\cdot)$ 函数复合的链式法则，计算给定候选不动点处的雅可比矩阵 $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$。\n- 计算 $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$ 的特征值。\n- 通过检查所有特征值的实部是否严格为负来判断候选不动点的稳定性。\n- 对每个测试用例返回一个布尔结果：如果候选不动点是渐近稳定的，则返回 $\\,\\texttt{True}\\,$，否则返回 $\\,\\texttt{False}\\,$。\n\n双曲正切函数的逐元素导数是一个经过充分检验的数学事实：$$\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^{2}(z).$$ 利用这一事实以及链式法则来构建复合向量场的雅可比矩阵。\n\n测试套件。使用以下科学上一致的参数集。矩阵和向量明确指定如下：\n\n- 测试用例 1（维度 2，存在非线性，稳定）：\n  $$\\mathbf{W} = \\begin{bmatrix} -0.3  0 \\\\ 0  -0.3 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.4  0 \\\\ 0  -0.4 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.$$\n\n- 测试用例 2（维度 2，鞍点型行为）：\n  $$\\mathbf{W} = \\begin{bmatrix} 0.2  0 \\\\ 0  0.2 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 3.0  0 \\\\ 0  0.2 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.1  0 \\\\ 0  -0.05 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.$$\n\n- 测试用例 3（维度 3，纯线性不稳定）：\n  $$\\mathbf{W} = \\begin{bmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} 0.1  0  0 \\\\ 0  0.2  0 \\\\ 0  0  0.3 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.$$\n\n- 测试用例 4（维度 2，临界情况，存在零特征值）：\n  $$\\mathbf{W} = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.1  0 \\\\ 0  0.0 \\end{bmatrix},$$\n  $$\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}^{\\star} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.$$\n\n- 测试用例 5（维度 1，在非零参数处存在非平凡非线性，稳定）：\n  $$\\mathbf{W} = \\begin{bmatrix} -0.8 \\end{bmatrix},\\quad \\mathbf{V} = \\begin{bmatrix} 2.5 \\end{bmatrix},\\quad \\mathbf{C} = \\begin{bmatrix} -0.05 \\end{bmatrix},$$\n  $$\\mathbf{x}^{\\star} = \\begin{bmatrix} 0.5 \\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix} -0.05 \\end{bmatrix},\\quad \\mathbf{d} = \\begin{bmatrix} 0.6919232 \\end{bmatrix}.$$\n\n对于每个测试用例，计算如上所述的布尔稳定性结果。你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，例如 $$[\\texttt{result1},\\texttt{result2},\\texttt{result3},\\texttt{result4},\\texttt{result5}].$$",
            "solution": "该问题要求分析给定连续时间自主系统的候选不动点的局部渐近稳定性。系统的演化由常微分方程（ODE）$\\frac{d\\mathbf{x}}{dt} = f_{\\theta}(\\mathbf{x})$ 描述，其中状态向量 $\\mathbf{x}$ 代表神经群体活动，向量场 $f_{\\theta}(\\mathbf{x})$ 由参数 $\\theta = (\\mathbf{W}, \\mathbf{V}, \\mathbf{C}, \\mathbf{b}, \\mathbf{d})$ 参数化。如果点 $\\mathbf{x}^{\\star}$ 满足条件 $f_{\\theta}(\\mathbf{x}^{\\star}) = \\mathbf{0}$，则它是一个不动点。\n\n根据 Hartman-Grobman 定理和连续时间系统的线性化原理，不动点 $\\mathbf{x}^{\\star}$ 的局部稳定性由在该不动点处求值的向量场雅可比矩阵 $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x})$ 的特征值决定。当且仅当雅可比矩阵 $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$ 的所有特征值都具有严格为负的实部时，不动点 $\\mathbf{x}^{\\star}$ 是局部渐近稳定的。\n\n此任务的核心是推导雅可比矩阵 $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x})$ 的解析形式，然后用它来评估每个提供的测试用例的稳定性。向量场由下式给出：\n$$f_{\\theta}(\\mathbf{x}) = \\mathbf{W}\\,\\tanh\\!\\big(\\mathbf{V}\\,\\mathbf{x} + \\mathbf{b}\\big) + \\mathbf{C}\\,\\mathbf{x} + \\mathbf{d}$$\n雅可比矩阵 $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x})$ 是一个偏导数矩阵，其中第 $(i, j)$ 个元素由 $J_{ij} = \\frac{\\partial f_i}{\\partial x_j}$ 给出。由于微分算子的线性性质，向量函数之和的雅可比矩阵是它们各自雅可比矩阵的和。因此，我们可以分别分析每一项。\n\n$1$. 项 $\\mathbf{C}\\mathbf{x}$ 是 $\\mathbf{x}$ 的线性变换。其雅可比矩阵就是矩阵 $\\mathbf{C}$。\n$2$. 项 $\\mathbf{d}$ 是一个常数向量。它相对于 $\\mathbf{x}$ 的导数是零矩阵 $\\mathbf{0}$。\n$3$. 第一项 $\\mathbf{g}(\\mathbf{x}) = \\mathbf{W}\\,\\tanh(\\mathbf{V}\\,\\mathbf{x} + \\mathbf{b})$ 是一个复合函数。我们必须应用多元链式法则。让我们如下定义这个复合过程：\n- 令 $\\mathbf{u}(\\mathbf{x}) = \\mathbf{V}\\mathbf{x} + \\mathbf{b}$。这是一个仿射变换。它相对于 $\\mathbf{x}$ 的雅可比矩阵是 $\\mathbf{J}_{\\mathbf{u}} = \\mathbf{V}$。\n- 令 $\\mathbf{h}(\\mathbf{u}) = \\tanh(\\mathbf{u})$，其中 $\\tanh$ 函数是逐元素应用的。第 $k$ 个分量是 $h_k(\\mathbf{u}) = \\tanh(u_k)$。偏导数 $\\frac{\\partial h_k}{\\partial u_l}$ 仅在 $k=l$ 时非零。因此，$\\mathbf{h}$ 相对于 $\\mathbf{u}$ 的雅可比矩阵是一个对角矩阵。使用所给的恒等式 $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$，该雅可比矩阵的对角线元素为 $\\frac{d h_k}{d u_k} = 1 - \\tanh^2(u_k)$。我们可以将此雅可比矩阵表示为：\n$$ \\mathbf{J}_{\\mathbf{h}}(\\mathbf{u}) = \\text{diag}\\left(1 - \\tanh^2(\\mathbf{u})\\right) $$\n其中 $\\text{diag}(\\cdot)$ 算子中的表达式是一个向量，其分量为 $1 - \\tanh^2(u_k)$。\n- 完整项是对 $\\mathbf{h}(\\mathbf{u}(\\mathbf{x}))$ 应用的线性变换 $\\mathbf{W}$。这个最终线性变换的雅可比矩阵就是 $\\mathbf{W}$。\n\n根据链式法则，复合函数 $\\mathbf{g}(\\mathbf{x}) = \\mathbf{W}(\\mathbf{h}(\\mathbf{u}(\\mathbf{x})))$ 的雅可比矩阵是其组成部分在相应点求值的雅可比矩阵的乘积：\n$$ \\mathbf{J}_{\\mathbf{g}}(\\mathbf{x}) = \\mathbf{W} \\cdot \\mathbf{J}_{\\mathbf{h}}(\\mathbf{u}(\\mathbf{x})) \\cdot \\mathbf{J}_{\\mathbf{u}}(\\mathbf{x}) $$\n代入各分量雅可比矩阵的表达式，我们得到：\n$$ \\mathbf{J}_{\\mathbf{g}}(\\mathbf{x}) = \\mathbf{W} \\, \\text{diag}\\left(1 - \\tanh^2(\\mathbf{V}\\mathbf{x} + \\mathbf{b})\\right) \\, \\mathbf{V} $$\n\n结合所有项的雅可比矩阵，向量场 $f_{\\theta}(\\mathbf{x})$ 的完整雅可比矩阵是：\n$$ \\mathbf{J}_{f_{\\theta}}(\\mathbf{x}) = \\mathbf{W} \\, \\text{diag}\\left(1 - \\tanh^2(\\mathbf{V}\\mathbf{x} + \\mathbf{b})\\right) \\, \\mathbf{V} + \\mathbf{C} $$\n\n解决每个测试用例的算法步骤如下：\n$1$. 对于给定的一组参数 $(\\mathbf{W}, \\mathbf{V}, \\mathbf{C}, \\mathbf{b}, \\mathbf{d})$ 和一个候选不动点 $\\mathbf{x}^{\\star}$，首先计算双曲正切函数的参数：$\\mathbf{u}^{\\star} = \\mathbf{V}\\mathbf{x}^{\\star} + \\mathbf{b}$。\n$2$. 构建对角矩阵 $\\mathbf{D}^{\\star} = \\text{diag}(1 - \\tanh^2(\\mathbf{u}^{\\star}))$，其中运算是逐元素执行的。\n$3$. 计算不动点处的数值雅可比矩阵：$\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star}) = \\mathbf{W}\\mathbf{D}^{\\star}\\mathbf{V} + \\mathbf{C}$。\n$4$. 计算 $\\mathbf{J}_{f_{\\theta}}(\\mathbf{x}^{\\star})$ 的特征值。\n$5$. 通过检查每个特征值的实部是否严格小于零来确定稳定性。如果此条件成立，则不动点是渐近稳定的（$\\texttt{True}$）；否则，它不是（$\\texttt{False}$）。\n将对每个测试用例执行这一系列操作，以生成最终的结果列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the stability of fixed points for several\n    dynamical systems.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (dimension 2, nonlinearity present, stable)\n        {\n            \"W\": np.array([[-0.3, 0], [0, -0.3]]),\n            \"V\": np.array([[1, 0], [0, 1]]),\n            \"C\": np.array([[-0.4, 0], [0, -0.4]]),\n            \"b\": np.array([[0], [0]]),\n            \"d\": np.array([[0], [0]]),\n            \"x_star\": np.array([[0], [0]]),\n        },\n        # Test case 2 (dimension 2, saddle-type behavior)\n        {\n            \"W\": np.array([[0.2, 0], [0, 0.2]]),\n            \"V\": np.array([[3.0, 0], [0, 0.2]]),\n            \"C\": np.array([[-0.1, 0], [0, -0.05]]),\n            \"b\": np.array([[0], [0]]),\n            \"d\": np.array([[0], [0]]),\n            \"x_star\": np.array([[0], [0]]),\n        },\n        # Test case 3 (dimension 3, purely linear unstable)\n        {\n            \"W\": np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n            \"V\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]),\n            \"C\": np.array([[0.1, 0, 0], [0, 0.2, 0], [0, 0, 0.3]]),\n            \"b\": np.array([[0], [0], [0]]),\n            \"d\": np.array([[0], [0], [0]]),\n            \"x_star\": np.array([[0], [0], [0]]),\n        },\n        # Test case 4 (dimension 2, marginal with a zero eigenvalue)\n        {\n            \"W\": np.array([[0, 0], [0, 0]]),\n            \"V\": np.array([[1, 0], [0, 1]]),\n            \"C\": np.array([[-0.1, 0], [0, 0.0]]),\n            \"b\": np.array([[0], [0]]),\n            \"d\": np.array([[0], [0]]),\n            \"x_star\": np.array([[0], [0]]),\n        },\n        # Test case 5 (dimension 1, nontrivial nonlinearity at nonzero argument, stable)\n        {\n            \"W\": np.array([[-0.8]]),\n            \"V\": np.array([[2.5]]),\n            \"C\": np.array([[-0.05]]),\n            \"b\": np.array([[-0.05]]),\n            \"d\": np.array([[0.6919232]]),\n            \"x_star\": np.array([[0.5]]),\n        },\n    ]\n\n    def check_stability(params):\n        \"\"\"\n        Computes the Jacobian and checks the stability of the given fixed point.\n\n        Args:\n            params (dict): A dictionary containing the parameters W, V, C, b, and x_star.\n\n        Returns:\n            bool: True if the fixed point is asymptotically stable, False otherwise.\n        \"\"\"\n        W = params[\"W\"]\n        V = params[\"V\"]\n        C = params[\"C\"]\n        b = params[\"b\"]\n        x_star = params[\"x_star\"]\n\n        # 1. Compute the argument of the tanh function\n        u_star = V @ x_star + b\n\n        # 2. Compute the diagonal elements of the Jacobian of the nonlinearity\n        # The derivative of tanh(z) is 1 - tanh^2(z).\n        # We need to flatten the result for np.diag as it expects a 1D array.\n        tanh_deriv_vec = 1 - np.tanh(u_star)**2\n        D_star = np.diag(tanh_deriv_vec.flatten())\n\n        # 3. Compute the full Jacobian matrix at the fixed point\n        # J = W * D * V + C\n        J = W @ D_star @ V + C\n\n        # 4. Calculate the eigenvalues of the Jacobian\n        eigenvalues = np.linalg.eigvals(J)\n        \n        # 5. Check if all eigenvalues have strictly negative real parts\n        is_stable = np.all(np.real(eigenvalues)  0)\n\n        return is_stable\n\n    results = []\n    for case in test_cases:\n        result = check_stability(case)\n        results.append(result)\n\n    # Format the final output as a comma-separated list of lowercase strings.\n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}