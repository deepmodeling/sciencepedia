## 引言
随着神经科学技术的发展，我们现在能够同时记录成百上千个神经元的活动，这为我们以前所未有的分辨率探索大脑的计算原理提供了机会。然而，这些高维、嘈杂的数据也带来了巨大的分析挑战：我们如何从复杂的群体放电模式中提取出有意义的、可解释的[神经信号](@entry_id:153963)？潜在变量模型（Latent Variable Models, LVMs）为解决这一问题提供了一个强大而原则性的框架。其核心思想是，观测到的高维神经活动是由少数几个未被观测到的、共享的“潜在”变量所驱动的，这些变量构成了[神经计算](@entry_id:154058)的低维基础。

本文旨在对用于[神经元群体分析](@entry_id:1128609)的LVMs进行一次全面而深入的介绍。我们将不仅探讨这些模型的数学基础，还将展示它们如何被应用于解决真实的神经科学问题。通过学习本文，您将能够理解不同LVMs背后的假设，评估它们的优缺点，并将其应用于您自己的研究中。

文章分为三个核心部分。在“原理与机制”一章中，我们将深入探讨LVMs的数学骨架，从经典的因子分析到捕捉时间动态的状态空间模型，并讨论[模型辨识](@entry_id:139651)性等根本性挑战。接着，在“应用与跨学科交叉”一章中，我们将展示这些模型如何作为连接神经活动、认知与行为的桥梁，应用于可视化神经动态、[解耦](@entry_id:160890)任务信号，甚至形式化地验证心理学构念。最后，“动手实践”部分将引导您通过具体的编程练习，将理论知识转化为解决实际问题的能力。

## 原理与机制

在上一章中，我们介绍了在分析大规模神经元[群体活动](@entry_id:1129935)时，使用潜在变量模型（Latent Variable Models, LVMs）的基本动机。本章将深入探讨这些模型的数学原理和核心机制。我们将从基本假设出发，构建一个连贯的理论框架，阐明这些模型如何帮助我们从高维神经数据中提取有意义的低维结构。我们将系统地讨论模型的构建、时间动态的整合、[模型辨识](@entry_id:139651)性的关键挑战以及在实践中应用这些模型的关键考量。

### 核心思想：分解[神经变异性](@entry_id:1128630)

潜在变量模型的核心假设是，观察到的神经元群体活动可以被分解为两个主要部分：一个由少数共享的、未被观察到的（即**潜在的**）信号驱动的**共享变异性**（shared variability），以及每个神经元独立的**私有变异性**（private variability）。

让我们从最经典的模型——**因子分析**（Factor Analysis, FA）——开始，来形式化这个思想。假设在时间点 $t$，我们观察到一个包含 $N$ 个神经元活动的向量 $y_t \in \mathbb{R}^N$。FA 模型假设这个观测向量是由一个低维的潜在状态向量 $x_t \in \mathbb{R}^k$（其中 $k \ll N$）通过线性变换生成的，并叠加了独立的噪声：

$y_t = C x_t + \epsilon_t$

在这个模型中：
- $x_t$ 是 $k$ 维的**潜在状态**或**因子**向量，它捕捉了在时间点 $t$ 驱动整个神经元群体的共享信号。
- $C \in \mathbb{R}^{N \times k}$ 是**载荷矩阵**（loading matrix），它的第 $i$ 行 $c_i^\top$ 描述了第 $i$ 个神经元如何“读取”或“耦合”到 $k$ 个潜在因子。
- $\epsilon_t \in \mathbb{R}^N$ 是**私有噪声**向量，代表了每个神经元特有的、不能被共享因子解释的随机波动。

为了完成模型的定义，我们还需要指定这些[随机变量](@entry_id:195330)的统计属性。通常假设潜在状态 $x_t$ 和噪声 $\epsilon_t$ 都是零均值的[随机变量](@entry_id:195330)，并且相互独立。我们假设它们的[协方差矩阵](@entry_id:139155)分别为 $\mathrm{Cov}(x_t) = Q$ 和 $\mathrm{Cov}(\epsilon_t) = R$。一个关键的假设是，私有噪声在神经元之间是独立的，这意味着[噪声协方差](@entry_id:1128754)矩阵 $R$ 是一个**[对角矩阵](@entry_id:637782)**，即 $R = \mathrm{diag}(\sigma_1^2, \dots, \sigma_N^2)$，其中 $\sigma_i^2$ 是第 $i$ 个神经元的私有方差。

基于这些假设，我们可以推导出观测数据 $y_t$ 的协方差矩阵 $\Sigma_y$  。根据协方差的定义和变量的独立性：

$$\Sigma_y = \mathrm{Cov}(y_t) = \mathrm{Cov}(C x_t + \epsilon_t) = C \mathrm{Cov}(x_t) C^\top + \mathrm{Cov}(\epsilon_t) = C Q C^\top + R$$

这个方程是理解 LVMs 的基石。它明确地将总[协方差矩阵](@entry_id:139155) $\Sigma_y$ 分解为两部分：
1.  **共享协方差** ($C Q C^\top$)：这一项源于所有神经元对共同潜在状态 $x_t$ 的依赖。由于 $C$ 是一个 $N \times k$ 的矩阵（$k \ll N$），$C Q C^\top$ 是一个 $N \times N$ 的**低秩矩阵**（其秩最多为 $k$）。它捕捉了神经元之间所有的成对协方差。具体来说，神经元 $i$ 和 $j$ ($i \neq j$) 之间的协方差完全由共享部分决定：$\mathrm{Cov}(y_{it}, y_{jt}) = (C Q C^\top)_{ij} = c_i^\top Q c_j$。
2.  **私有方差** ($R$)：这是一个[对角矩阵](@entry_id:637782)，其对角线上的元素 $\sigma_i^2$ 代表了每个神经元 $i$ 的独立噪声方差。它对神经元之间的协方差没有贡献。

这个分解引出了 LVM 的一个核心特征：**[条件独立性](@entry_id:262650)**。模型假设，一旦我们知道了潜在状态 $x_t$ 的值，所有神经元的活动就变得[相互独立](@entry_id:273670)。这是因为 $x_t$ 解释了它们之间所有的相关性。在给定 $x_t$ 的情况下，任何剩余的变异性都来自于独立的私有噪声 $\epsilon_t$。因此，给定 $x_t$，神经元 $i$ 和 $j$ 的协方差为零 。这与**直接耦合模型**（如[高斯图模型](@entry_id:269263)或耦合 GLM）形成鲜明对比，后者假设神经元之间存在直接的、非由共同输入介导的相互作用 。

一个重要的推论是，私有噪声的大小会影响神经元之间的**相关性**，但不会影响它们的**协方差**。[相关系数](@entry_id:147037) $\rho_{ij}$ 的分母中包含每个神经元的总方差（$\mathrm{Var}(y_{it}) = c_i^\top Q c_i + \sigma_i^2$）。因此，即使共享的协方差 $c_i^\top Q c_j$ 保持不变，增加私有噪声 $\sigma_i^2$ 或 $\sigma_j^2$ 也会增加分母，从而降低[相关系数](@entry_id:147037)的绝对值。当私有噪声趋于无穷大时，相关系数趋于零 。

### 生成模型与描述性方法：FA 与 PCA 的对比

为了更深刻地理解 LVM 作为**生成模型**（generative model）的价值，我们有必要将它与一种广泛使用的**描述性方法**（descriptive method）——**主成分分析**（Principal Component Analysis, PCA）——进行对比。

PCA 的目标是找到一组正交的投影轴（主成分），使得数据在这些轴上的投影方差最大化。它旨在用尽可能少的维度来“概括”数据的总方差。然而，PCA 并不对数据的生成过程做任何假设，它不区分共享变异性和私有变异性。

考虑一个假设场景 ：一个包含三个神经元的群体，其活动由一个单一的潜在驱动信号共同调节。然而，由于电极漂移等技术问题，第一个神经元的记录包含了大量的独立噪声，使其总方差远大于其他两个神经元。假设其（中心化后）样本协方差矩阵为：

$$C = \begin{pmatrix} 10  1  1 \\ 1  1.1  1 \\ 1  1  1.1 \end{pmatrix}$$

矩阵中的非对角元素均为 $1$，这表明三个神经元之间存在一致的、正向的共享协方差，符合单一共同驱动的假设。然而，第一个神经元的方差（$10$）远大于其他两个（均为 $1.1$）。

- **PCA 的视角**：PCA 寻找最大化投影方差 $v^\top C v$ 的方向 $v$。由于第一个神经元的方差占总方差的绝大部分（$10 / 12.2 \approx 82\%$），PCA 的第一主成分将不可避免地与第一个神经元的轴线高度对齐。这个主成分将主要反映第一个神经元的高噪声，而不是所有神经元共享的信号。因此，PCA 会给出一个具有误导性的结论，将私有噪声与共享机制混为一谈。

- **FA 的视角**：FA 采用生成模型的思维，试图将协方差矩阵分解为 $C = \Lambda \Lambda^\top + \Psi$。对于单一潜在驱动（$k=1$），FA 能够完美地解释这个协方差结构。它可以找到一个[载荷向量](@entry_id:635284) $\Lambda = \begin{pmatrix} 1  1  1 \end{pmatrix}^\top$ 和一个私有方差矩阵 $\Psi = \mathrm{diag}(9, 0.1, 0.1)$。这个解的解释非常清晰：存在一个共享因子，它对三个神经元都有大小为 $1$ 的贡献（$\Lambda$）；同时，第一个神经元有高达 $9$ 的私有方差，而其他两个神经元只有 $0.1$。这个结果精确地分离了共享信号和私有噪声，与我们的初始假设完全吻合。

这个例子鲜明地展示了，当我们的目标是探究数据背后的**机制**时，像 FA 这样的[生成模型](@entry_id:177561)比 PCA 这样的描述性方法提供了更具解释性和更可靠的视角。

### 模拟神经数据：超越[高斯假设](@entry_id:170316)

到目前为止，我们主要讨论了基于[高斯假设](@entry_id:170316)的线性模型。然而，神经活动数据通常以**尖峰计数**（spike counts）的形式出现，这是一种非负的离散数据。直接应用高斯模型是不得当的，原因有二：高斯分布的支撑集是整个[实数轴](@entry_id:147286)，可能产生无意义的负预测；其次，它通常假设方差是恒定的（[同方差性](@entry_id:634679)），而尖峰计数的方差往往随其均值的变化而变化（[异方差性](@entry_id:895761)）。

因此，我们需要为计数数据构建更合适的生成模型。一个自然的选择是**[泊松分布](@entry_id:147769)**（Poisson distribution），它是对计数事件的经典统计描述。这引导我们进入**广义线性模型**（Generalized Linear Models, GLMs）的框架，构建**泊松潜在变量模型** 。

一个标准的泊松 LVM 假设，在给定潜在状态 $x_t$ 的情况下，神经元 $i$ 在时间 $t$ 的尖峰计数 $y_{it}$ 服从泊松分布：

$y_{it} \mid x_t \sim \mathrm{Poisson}(\lambda_{it})$

其中，[泊松分布](@entry_id:147769)的**率参数**（intensity） $\lambda_{it}$（也是其条件均值和方差）与潜在状态 $x_t$ 通过一个**链接函数**（link function）相关联。为了确保 $\lambda_{it}$ 始终为正，通常使用指数函数作为链接函数，构建一个对数-线性模型：

$$\lambda_{it} = \exp(c_i^\top x_t + d_i)$$

这里的 $c_i$ 和 $d_i$ 分别是神经元 $i$ 的[载荷向量](@entry_id:635284)和基线偏移量。

对这个模型的参数解释需要特别注意 ：
- **载荷的解释**：由于对数关系 $\ln(\lambda_{it}) = c_i^\top x_t + d_i$，[载荷向量](@entry_id:635284)的每个分量 $c_{ik}$ 衡量了第 $k$ 个潜在因子对**对数发放率**的**加性**影响。这意味着，如果潜在因子 $k$ 的值 $x_{kt}$ 增加 $\Delta$，对数发放率会增加 $c_{ik}\Delta$，而实际发放率 $\lambda_{it}$ 则会**乘以**一个因子 $\exp(c_{ik}\Delta)$。
- **条件 Fano 因子**：Fano 因子定义为方差与均值的比率。对于泊松分布，其均值和方差都等于 $\lambda_{it}$。因此，在给定潜在状态 $x_t$ 的条件下，该模型的**条件 Fano 因子** $\mathrm{Var}(y_{it} \mid x_t) / \mathbb{E}[y_{it} \mid x_t]$ 恒等于 $1$。这是一个很强的假设，真实神经数据的 Fano 因子通常不为 $1$（即存在“超泊松”或“亚泊松”现象）。
- **模型扩展**：为了处理真实数据中普遍存在的超泊松现象（即方差大于均值），可以将观测模型从[泊松分布](@entry_id:147769)替换为**负二项分布**（Negative Binomial distribution），它引入了一个额外的参数来独立地控制方差，从而提供了更大的灵活性 。

### 建模时间动态：[状态空间模型](@entry_id:137993)

神经活动是在时间上连续展开的过程。为了捕捉这种时间结构，我们可以将单时间点的 LVM 扩展为**状态空间模型**（State-Space Models）。这类模型不仅定义了观测如何从潜在状态生成，还定义了潜在状态本身如何随时间演化。

一个标准的状态空间模型由两个核心方程定义 ：
1.  **状态[转移方程](@entry_id:160254)**：描述潜在状态如何从一个时间点演化到下一个时间点。对于**[线性动力系统](@entry_id:1127277)**（Linear Dynamical System, LDS），它具有以下形式：
    $$x_{t+1} = A x_t + w_t$$
    其中 $A \in \mathbb{R}^{k \times k}$ 是**动力学矩阵**，决定了潜在状态的内在动态特性（如振荡、衰减等），$w_t \sim \mathcal{N}(0, Q)$ 是**过程噪声**，代表了状态演化中的随机扰动。

2.  **观测方程**：描述在任何给定时间点，观测数据如何从当时的潜在状态生成。这与我们之前讨论的单时间点模型相同：
    $$y_t = C x_t + v_t$$
    其中 $v_t \sim \mathcal{N}(0, R)$ 是**观测噪声**。

这个模型完整的概率结构依赖于一系列关键的独立性假设 ：
- 潜在状态序列 $\{x_t\}$ 具有**一阶马尔可夫性**（Markov property）：给定当前状态 $x_t$，未来状态 $x_{t+1}$ 与所有过去的状态 $x_{0:t-1}$ 条件独立。
- 观测 $y_t$ **条件独立于**所有其他变量（其他时间的观测、其他时间的状态），仅依赖于**当前**的潜在状态 $x_t$。
- 过程噪声 $\{w_t\}$ 和观测噪声 $\{v_t\}$ 都是“白噪声”序列（即在时间上独立），且彼此之间[相互独立](@entry_id:273670)。

在这些假设下，整个数据序列的联合概率分布可以优美地分解为：
$$p(x_{0:T}, y_{1:T}) = p(x_0) \prod_{t=0}^{T-1} p(x_{t+1} \mid x_t) \prod_{t=1}^{T} p(y_t \mid x_t)$$
这个分解形式是所有基于状态空间模型的推断算法（如卡尔曼滤波/平滑）的理论基础。

除了[参数化](@entry_id:265163)的 LDS 模型，**[高斯过程因子分析](@entry_id:1125536)**（Gaussian Process Factor Analysis, GPFA）提供了一种强大且灵活的[非参数方法](@entry_id:138925)来对潜在轨迹建模 。在 GPFA 中，我们不假设一个特定的、由矩阵 $A$ 定义的[线性动力学](@entry_id:177848)，而是为每个潜在维度 $x_k(t)$ 赋予一个**[高斯过程](@entry_id:182192)**（Gaussian Process, GP）先验。GP 是一种对函数的分布，它由一个[均值函数](@entry_id:264860)（通常为零）和一个**协方差函数**（或核函数）$K(\tau)$ 定义。[核函数](@entry_id:145324) $K(t, t')$ 指定了任意两个时间点 $t$ 和 $t'$ 的潜在状态值之间的协方差，从而编码了关于轨迹平滑性等先验知识。例如，一个常用的核函数是[平方指数核](@entry_id:191141)，它倾向于生成非常平滑的轨迹。

GPFA 的完整概率模型，特别是当考虑所有时间点的堆叠向量时，可以用多维高斯分布的语言精确地表达出来。这需要用到[克罗内克积](@entry_id:182766)（Kronecker product）等矩阵工具来构建巨大的[协方差矩阵](@entry_id:139155)，并可以通过高斯分布的条件化公式来推导潜在轨迹的[后验分布](@entry_id:145605) 。

### 一个根本性挑战：[模型辨识](@entry_id:139651)性

无论是 FA、LDS 还是 GPFA，所有这些模型都面临一个共同的、根本性的挑战：**[模型辨识](@entry_id:139651)性**（identifiability）问题。具体来说，它们都存在**旋转模糊性**（rotational ambiguity）。

这个问题源于这样一个事实：我们可以对潜在空间进行任意的**旋转**（或更广义的[可逆线性变换](@entry_id:149915)），并通过对载荷矩阵进行相应的“反向”旋转来完全补偿这种变换，而最终生成的数据分布保持不变  。

以 FA 模型为例，考虑任意一个 $k \times k$ 的[正交矩阵](@entry_id:169220) $M$（满足 $M M^\top = I$）。我们可以定义一组新的潜在变量 $x'_t = M x_t$ 和一个新的载荷矩阵 $C' = C M^\top$。那么，新的模型 $C' x'_t = (C M^\top)(M x_t) = C (M^\top M) x_t = C x_t$ 与原模型完[全等](@entry_id:273198)价。由于存在无限多个这样的旋转矩阵 $M$，也就存在无限多组参数 $(C, \{x_t\})$ 能够同样好地解释观测数据。

这种模糊性意味着，如果没有额外的约束，从数据中估计出的载荷矩阵 $C$ 和潜在轨迹 $\{x_t\}$ 不是唯一的。这对于科学解释构成了严重障碍，因为我们无法确定一个“真实”的潜在坐标系。

在实践中，有多种方法可以解决或缓解这个问题 ：
- **施加数学约束**：我们可以对载荷矩阵 $C$ 施加足够的约束来唯一地固定旋转。例如，强制要求 $C$ 是一个下[三角矩阵](@entry_id:636278)，并规定其对角[线元](@entry_id:196833)素为正。另一种常见的方法是，将[潜在空间](@entry_id:171820)的坐标轴与数据的主成分（PCA）轴对齐。虽然这些方法在数学上解决了模糊性，但所得到的坐标系可能缺乏直接的生理学意义。
- **利用先验知识**：在贝叶斯框架下，我们可以引入打破旋转对称性的先验。例如：
    - **稀疏性先验**：如果我们假设每个潜在因子只影响一小部分神经元（即 $C$ 的每一列中有很多零），那么一个通用的旋转会破坏这种[稀疏结构](@entry_id:755138)。因此，模型会偏好一个能够保持[稀疏性](@entry_id:136793)的旋转。
    - **非负性约束**：在某些情况下，假设载荷为非负可能符合生理学直觉。由于一个非负矩阵在旋转后通常会包含负值，这个约束也极大地限制了旋转的自由度。
    - **[非交换性](@entry_id:153545)先验**：对于时间序列模型如 GPFA，我们可以为每个潜在维度赋予具有不同超参数（如不同时间尺度）的 GP 先验。这使得潜在维度变得“不可交换”，因为旋转会混合这些具有不同时间特征的轨迹，从而改变先验的概率。模型会选择一个不混合它们的旋转角度 。

### 实践中的考量

在将 LVMs 应用于真实数据时，研究者需要做出一些关键的决策。其中最重要的一项是选择[潜在空间](@entry_id:171820)的维度 $k$。

#### 选择潜在维度 $k$

选择一个合适的 $k$ 是一个典型的模型选择问题，它需要在模型的**拟合优度**（goodness-of-fit）和**复杂性**（complexity）之间进行权衡。一个过小的 $k$ 会导致模型[欠拟合](@entry_id:634904)，无法捕捉数据中重要的共享结构；而一个过大的 $k$ 则可能导致[过拟合](@entry_id:139093)，模型会开始拟合样本中的随机噪声，从而降低其泛化到新数据的能力。

有几种主流的方法用于选择 $k$，每种方法都有其自身的优点和偏见 ：
1.  **交叉验证的预测对数似然**（Cross-validated predictive log-likelihood）：这是一种原则性的统计方法。它将数据分为训练集和[测试集](@entry_id:637546)，在[训练集](@entry_id:636396)上拟合模型，然后在测试集上评估模型赋予未见数据的概率（的对数）。这种方法评估的是整个预测分布的质量，而不仅仅是均值预测。它的一个主要优点是对模型的统计假设（如噪声分布）非常敏感。如果模型假设（例如，高斯噪声）与数据的真实属性（例如，泊松分布）不匹配，预测对数似然会受到惩罚，通常会倾向于选择一个更简单的模型（较小的 $k$）。
2.  **留出法解释的方差**（Held-out variance explained）：这个指标，也常被称为 $R^2$ 值，衡量的是模型对测试数据方差的解释比例。它主要关注模型预测均值 $\hat{y}_{n,t}$ 与真实值 $y_{n,t}$ 之间的[均方误差](@entry_id:175403)。与[对数似然](@entry_id:273783)不同，它对噪声分布的具体形式不敏感。因此，即使噪声模型被错误设定，只要增加 $k$ 能让模型更好地拟合数据的均值动态，解释的方差就可能持续增加。这可能导致它相对于[对数似然](@entry_id:273783)方法倾向于选择更大的 $k$。
3.  **[贝叶斯证据](@entry_id:746709)或边际似然**（Marginal likelihood）：在贝叶斯框架下，我们可以计算数据在给定模型（由 $k$ 定义）下的“证据”，即 $p(\mathbf{Y} \mid k)$。这个量通过对所有可能的参数进行积分，自然地体现了**[奥卡姆剃刀](@entry_id:142853)**原理：一个更复杂的模型（更大的 $k$）拥有更大的参数空间，除非数据非常有力地支持该模型中的一小块区域，否则其平均[似然](@entry_id:167119)（即证据）会更低。这种方法能自动惩罚不必要的复杂性，在数据量较少时尤其倾向于选择更简单的模型。

在执行交叉验证时，必须极其小心以避免**信息泄露**（information leakage）。例如，在进行“留一神经元法”交叉验证时，如果在推断潜在状态 $\{z_t\}$ 时使用了包括被留出的测试神经元在内的所有神经元的数据，那么这些状态就已经“偷看”了答案。这会导致模型表现被严重高估，并错误地偏好于更大的 $k$。正确的做法是，在每个[交叉验证](@entry_id:164650)折叠中，必须仅使用训练神经元的数据来推断潜在状态 。

### 高级主题：神经表征的非平稳性

一个标准 LVM 的核心假设是模型参数（如 $C$ 和 $A$）是固定的。然而，在许多神经科学实验中（尤其是在涉及学习的过程中），神经表征本身可能会随时间演化，即系统是**非平稳的**（non-stationary）。

一个优雅的建模思路是将这种[非平稳性](@entry_id:180513)想象为潜在子空间的**缓慢旋转** 。也就是说，载荷矩阵随时间变化：$C_t = C_0 R_t$，其中 $C_0$ 是一个固定的“基”载荷矩阵，而 $R_t$ 是一个随时间平滑变化的[正交矩阵](@entry_id:169220)。

这种[非平稳性](@entry_id:180513)给动力学解释带来了严峻的挑战。如果我们天真地用一个平稳模型去拟合由这种[非平稳过程](@entry_id:269756)产生的数据，我们估计出的潜在轨迹 $z_t$ 将会是真实轨迹 $x_t$ 的一个旋转版本，即 $z_t = R_t x_t$。对 $z_t$ 求导可以发现，它所遵循的“有效”动力学为：

$$\frac{d}{dt} z_t = \Big(R_t A R_t^\top + \dot{R}_t R_t^\top \Big) z_t + \text{变换后的噪声}$$

我们从数据中估计出的动力学矩阵实际上是 $A_{\text{eff}}(t) = R_t A R_t^\top + \dot{R}_t R_t^\top$。这个估计值被两个效应所“污染”：一个是由 $R_t$ 引起的对真实动力学 $A$ 的时变相似性变换，另一个是附加的、由旋转速度决定的**[斜对称矩阵](@entry_id:155998)**项 $\dot{R}_t R_t^\top$。如果不了解 $R_t$，就不可能从 $A_{\text{eff}}(t)$ 中唯一地恢复出真实的、恒定的动力学矩阵 $A$。

为了应对这一挑战，可以采用在相邻的时间窗口上分别拟合模型，然后将估计出的子空间进行“对齐”的策略。一种标准的对齐方法是求解**正交普罗克汝斯忒斯问题**（orthogonal Procrustes problem），即寻找一个旋转矩阵，使得一个窗口的载荷矩阵在旋转后与下一个窗口的载荷矩阵尽可能接近。通过将这些相对旋转串联起来，我们就可以追踪潜在子空间随时间的演化，从而为分析非平稳神经数据背后的动态机制提供了一条途径 。