## Introduction
Understanding the brain's complex dynamics from high-dimensional neural recordings is a central challenge in modern neuroscience. Neural populations often exhibit activity that switches between distinct, semi-stable patterns, reflecting shifts in underlying cognitive processes or network configurations. Hidden Markov Models (HMMs) provide a powerful probabilistic framework for uncovering this latent structure from sequential data like neural spike trains. They allow us to move beyond simple descriptions of activity to formally model the unobserved "states" of a neural system and the dynamics governing transitions between them. This article addresses the need for a principled method to identify and characterize these discrete neural states from noisy observational data.

Across the following chapters, you will gain a comprehensive understanding of HMMs for neural data analysis. We will first delve into the foundational "Principles and Mechanisms" of the model, including its formal definition, key assumptions, and the core inference algorithms that make it computationally tractable. Next, in "Applications and Interdisciplinary Connections," we will explore how the basic HMM is adapted and extended to better fit the complexities of real neural data and integrated with other modeling frameworks. Finally, a series of "Hands-On Practices" will provide you with the opportunity to implement and assess these models, solidifying your theoretical knowledge. We begin by dissecting the mathematical underpinnings of the HMM, laying the groundwork for its practical application.

## Principles and Mechanisms

This chapter delineates the fundamental principles and computational mechanisms of the Hidden Markov Model (HMM) as applied to the analysis of discrete neural states from spike train data. We will begin by formally defining the model and its components, explore its structural properties and modeling philosophy, and then detail the core algorithms for inference. Finally, we will discuss the interpretation of model parameters and key practical considerations, such as numerical stability and [model identifiability](@entry_id:186414).

### The Formal Definition of a Poisson-HMM

A Hidden Markov Model is a doubly stochastic process, composed of an unobserved (latent or hidden) [stochastic process](@entry_id:159502) that governs the properties of an observed [stochastic process](@entry_id:159502). In the context of neuroscience, the hidden process represents a sequence of discrete neural states, while the observed process consists of measured neural activity, such as spike counts.

#### The Latent State Process

The core of the HMM is a sequence of latent states $\{z_t\}_{t=1}^T$, where each state $z_t$ takes a value from a finite set $\{1, 2, \dots, K\}$. This sequence is modeled as a **first-order, time-homogeneous Markov chain**. This assumption implies two key properties:

1.  **The Markov Property:** The state at time $t$, $z_t$, is conditionally independent of all past states given the immediately preceding state, $z_{t-1}$. Formally, $P(z_t | z_{t-1}, z_{t-2}, \dots, z_1) = P(z_t | z_{t-1})$.
2.  **Time Homogeneity:** The rules governing transitions between states are constant over time.

This latent process is fully specified by two parameters:

-   An **initial state distribution**, denoted by the vector $\pi$ of length $K$, where each element $\pi_k = P(z_1 = k)$ represents the probability that the system starts in state $k$. Naturally, $\sum_{k=1}^K \pi_k = 1$.

-   A **[transition probability matrix](@entry_id:262281)**, denoted by the $K \times K$ matrix $A$, where each element $A_{ij} = P(z_t = j | z_{t-1} = i)$ represents the probability of transitioning from state $i$ to state $j$ in a single time step. As this matrix contains conditional probabilities, each row must sum to one: $\sum_{j=1}^K A_{ij} = 1$ for all $i \in \{1, \dots, K\}$.

The probability of any specific sequence of latent states $z_{1:T} = (z_1, z_2, \dots, z_T)$ can be written as the product of the initial probability and the subsequent [transition probabilities](@entry_id:158294):
$$
p(z_{1:T}) = p(z_1) \prod_{t=2}^T p(z_t | z_{t-1}) = \pi_{z_1} \prod_{t=2}^T A_{z_{t-1}, z_t}
$$

#### The Observation Process

The HMM posits that the observations are generated by the latent states. For recordings of $N$ neurons over $T$ [discrete time](@entry_id:637509) bins, the observations are a set of spike counts $\{y_{t,n}\}$ for $t=1,\dots,T$ and $n=1,\dots,N$. Let $y_t = (y_{t,1}, \dots, y_{t,N})$ be the vector of spike counts from all $N$ neurons at time $t$.

The model is defined by two [conditional independence](@entry_id:262650) assumptions regarding the observations:

1.  The observation at time $t$, $y_t$, is conditionally independent of all other states and observations given the current latent state, $z_t$.
2.  Within a time bin $t$, the spike counts of the individual neurons, $y_{t,1}, \dots, y_{t,N}$, are conditionally independent given the latent state $z_t$.

For spike [count data](@entry_id:270889), a natural choice for the emission distribution is the **Poisson distribution**. In a Poisson-HMM, the spike count of neuron $n$ at time $t$, $y_{t,n}$, given that the system is in latent state $z_t = k$, is modeled as a Poisson random variable with a specific [rate parameter](@entry_id:265473) $\lambda_{k,n}$. This parameter depends on both the state $k$ and the neuron $n$. The probability [mass function](@entry_id:158970) (PMF) is:
$$
p(y_{t,n} | z_t = k) = \frac{\exp(-\lambda_{k,n}) \lambda_{k,n}^{y_{t,n}}}{y_{t,n}!}
$$
Due to the [conditional independence](@entry_id:262650) across neurons, the probability of observing the entire vector $y_t$ given state $z_t=k$ is the product of the individual Poisson PMFs:
$$
p(y_t | z_t=k) = \prod_{n=1}^N p(y_{t,n} | z_t=k) = \prod_{n=1}^N \frac{\exp(-\lambda_{k,n}) \lambda_{k,n}^{y_{t,n}}}{y_{t,n}!}
$$
These state- and neuron-specific firing rates, $\lambda_{k,n}$, are the parameters of the emission model. They define the characteristic firing pattern associated with each latent neural state. 

#### The Complete Joint Distribution

By combining the latent process and the observation process, we can write the complete [joint probability](@entry_id:266356) of a sequence of states $z_{1:T}$ and a sequence of observations $y_{1:T}$. This is achieved by applying the chain rule and the model's core independence assumptions:
$$
p(z_{1:T}, y_{1:T}) = p(z_{1:T}) p(y_{1:T} | z_{1:T}) = \left(\pi_{z_1} \prod_{t=2}^T A_{z_{t-1}, z_t}\right) \left(\prod_{t=1}^T p(y_t | z_t)\right)
$$
Substituting the Poisson emission model gives the full joint PMF:
$$
p(z_{1:T}, y_{1:T}) = \pi_{z_1} \left( \prod_{t=2}^T A_{z_{t-1}, z_t} \right) \left( \prod_{t=1}^T \prod_{n=1}^N \frac{\exp(-\lambda_{z_t,n}) \lambda_{z_t,n}^{y_{t,n}}}{y_{t,n}!} \right)
$$
This expression is the foundation upon which all inference algorithms for HMMs are built. To make this tangible, consider a simple case with $T=3$ time steps, $K=2$ states, and $N=2$ neurons. Given a fixed state sequence $z_{1:3} = (1, 2, 2)$ and observation sequence $y_1=(1,0), y_2=(2,1), y_3=(0,1)$, the joint probability is the product of the individual probabilistic events: the probability of starting in state 1, transitioning from 1 to 2, transitioning from 2 to 2, and the probabilities of observing $y_1, y_2, y_3$ from states 1, 2, and 2, respectively. If we are given the model parameters (e.g., $\pi=(0.6, 0.4)$, specific [transition probabilities](@entry_id:158294) from matrix $A$, and specific rate vectors $\lambda_1, \lambda_2$), we can compute the precise numerical value of this joint probability, which combines the plausibility of the state path with the likelihood of the data under that path. 

### Structural Properties and Modeling Philosophy

Understanding the HMM requires appreciating its underlying graphical model structure and the modeling philosophy it embodies.

#### Conditional Independence and Markov Blankets

The HMM can be visualized as a directed graphical model, often called a dynamic Bayesian network. The structure consists of a chain of latent state nodes ($z_1 \to z_2 \to \dots \to z_T$), with each state node $z_t$ having a corresponding observation node $y_t$ as its child ($z_t \to y_t$). This graph precisely encodes the model's [conditional independence](@entry_id:262650) properties.

These properties can be formally understood using the concept of **[d-separation](@entry_id:748152)**. For instance, the path from a past observation $y_{t-1}$ to a future observation $y_{t+1}$ is $y_{t-1} \leftarrow z_{t-1} \rightarrow z_t \rightarrow z_{t+1} \rightarrow y_{t+1}$. This path is blocked if we condition on the state $z_t$, but it is not blocked if we only condition on the observation $y_t$.

A key concept for understanding the local structure of the graph is the **Markov blanket**. The Markov blanket of a node is the set of its parents, its children, and its children's other parents. Conditioning on its Markov blanket renders a node independent of all other nodes in the graph. For an HMM:

-   The **Markov blanket of an observation node $y_t$** is simply its parent state, $\{z_t\}$. This formalizes the assumption that the observation at time $t$ is generated solely by the state at time $t$. Given $z_t$, $y_t$ is independent of all other states and observations.

-   The **Markov blanket of a latent state node $z_t$** (for an interior time point $t$) consists of its parent $z_{t-1}$, its children $z_{t+1}$ and $y_t$. That is, $\operatorname{MB}(z_t) = \{z_{t-1}, z_{t+1}, y_t\}$. This means that to infer the state at time $t$, the only relevant information is the state of the system just before, the state just after, and the observation at that exact moment. This local dependency structure is what makes efficient inference algorithms possible. 

#### Piecewise Stationarity

Why is the HMM a compelling model for neural activity? Many neural systems exhibit behavior that appears to switch between distinct, semi-stable modes of operation. For example, a network might switch between an "attentive" state and a "resting" state, with different characteristic firing patterns in each. The HMM formalizes this intuition through the concept of **piecewise stationarity**.

Conditional on the latent state sequence remaining fixed at some value $k$ for a period of time, the observations $\{y_t\}$ during that period are modeled as [independent and identically distributed](@entry_id:169067) (IID) draws from the fixed emission distribution $p(y | z=k)$. An IID process is a strong form of a [stationary process](@entry_id:147592). The HMM, therefore, models the full, non-stationary sequence of neural activity as a [concatenation](@entry_id:137354) of segments, where each segment is a [stationary process](@entry_id:147592) governed by the parameters of one of the $K$ latent states. The transitions between states in the latent Markov chain account for the [non-stationarity](@entry_id:138576) of the overall data stream. 

It is crucial to contrast this with a simpler model, such as a fully observed Markov chain directly on the observation space $\{y_t\}$. In such a model, the system's future depends only on its present observed state. However, the observed data sequence generated by an HMM is, in general, **not a first-order Markov process**. Information about the [hidden state](@entry_id:634361) $z_t$ is contained in the entire history of observations $y_{1:t}$. Therefore, knowing $y_{t-1}$ provides information about $z_t$ that is not redundant with the information from $y_t$. This means that $p(y_{t+1} | y_t, y_{t-1}) \neq p(y_{t+1} | y_t)$. The latent variable structure induces long-range statistical dependencies in the observed data, a feature that makes HMMs much richer than simple Markov models. 

### Interpreting Model Parameters: State Dynamics

Once an HMM is fitted to data, its parameters provide insight into the underlying [neural dynamics](@entry_id:1128578). The transition matrix $A$ is particularly informative about the temporal structure of the latent states.

The diagonal elements of the transition matrix, $A_{kk}$, represent the probability of remaining in state $k$ in the next time step. This self-[transition probability](@entry_id:271680) directly controls the **persistence** or **stability** of a state. We can formalize this by analyzing the **dwell time**, $D$, defined as the number of consecutive time bins spent in a state before transitioning to a different state.

Assuming the system has just entered state $k$, the probability of staying for exactly $d$ time steps is the probability of $d-1$ self-transitions followed by one transition to any other state. The probability of a self-transition is $A_{kk}$, and the probability of exiting is $1-A_{kk}$. Due to the Markov property, the probability [mass function](@entry_id:158970) for the dwell time is:
$$
P(D=d) = (A_{kk})^{d-1} (1-A_{kk})
$$
This is the PMF of a **Geometric distribution**. From this, we can derive the [expected value and variance](@entry_id:180795) of the dwell time:

-   **Expected Dwell Time:** $\mathbb{E}[D] = \frac{1}{1-A_{kk}}$
-   **Variance of Dwell Time:** $\operatorname{Var}(D) = \frac{A_{kk}}{(1-A_{kk})^2}$

These formulas provide a direct, quantitative link between a model parameter and an interpretable biological property. As $A_{kk} \to 1$, the expected dwell time $\mathbb{E}[D] \to \infty$, indicating a highly persistent or stable state. As $A_{kk} \to 0$, $\mathbb{E}[D] \to 1$, indicating a very transient state that is typically exited immediately. The variance also increases dramatically as $A_{kk} \to 1$, implying that highly stable states not only last longer on average but also exhibit much greater variability in their duration from one occurrence to the next. 

### Core Inference Problems and Algorithms

Given a set of observations $y_{1:T}$ and a model with specified parameters $(\pi, A, \Lambda)$, there are three fundamental inference problems we need to solve:

1.  **Likelihood Computation:** What is the probability of the data given the model, $p(y_{1:T})$? This is essential for [model evaluation](@entry_id:164873) and comparison.
2.  **State Estimation (Smoothing):** What is the probability distribution over the state at a single time $t$, given the *entire* sequence of observations, $p(z_t | y_{1:T})$?
3.  **Decoding:** What is the single most likely sequence of latent states that generated the observations, $z_{1:T}^* = \arg\max_{z_{1:T}} p(z_{1:T} | y_{1:T})$?

These problems are solved efficiently using dynamic programming algorithms.

#### The Forward Algorithm: Computing Likelihood

To compute the data likelihood $p(y_{1:T})$, we can marginalize the [joint distribution](@entry_id:204390) over all $K^T$ possible state sequences, but this is computationally infeasible. The **[forward algorithm](@entry_id:165467)** provides an efficient solution.

We define the **forward message**, $\alpha_t(i)$, as the joint probability of the observations up to time $t$ and the system being in state $i$ at time $t$:
$$
\alpha_t(i) \triangleq p(y_{1:t}, z_t = i)
$$
The algorithm proceeds in two steps:

1.  **Initialization ($t=1$):**
    $$
    \alpha_1(i) = p(y_1, z_1=i) = p(z_1=i) p(y_1 | z_1=i) = \pi_i p(y_1 | z_1=i)
    $$

2.  **Recursion ($t=2, \dots, T$):** We derive the value of $\alpha_t(i)$ by summing over all possible previous states $j$:
    $$
    \alpha_t(i) = p(y_t | z_t=i) \sum_{j=1}^K p(z_t=i | z_{t-1}=j) \alpha_{t-1}(j) = p(y_t | z_t=i) \sum_{j=1}^K A_{ji} \alpha_{t-1}(j)
    $$

After the recursion completes at $t=T$, the total likelihood of the data is simply the sum of the final forward messages:
$$
p(y_{1:T}) = \sum_{i=1}^K p(y_{1:T}, z_T=i) = \sum_{i=1}^K \alpha_T(i)
$$

The [time complexity](@entry_id:145062) of this algorithm is $O(TK^2)$, a dramatic improvement over the [exponential complexity](@entry_id:270528) of brute-force summation. 

A critical practical issue is **numerical [underflow](@entry_id:635171)**. The forward messages $\alpha_t(i)$ are joint probabilities that are products of many small numbers (emission probabilities). For a long sequence, these values will quickly become smaller than the smallest representable number in standard [floating-point arithmetic](@entry_id:146236) (e.g., $\approx 10^{-308}$ for [double precision](@entry_id:172453)), causing them to be rounded to zero. 

The [standard solution](@entry_id:183092) is to perform all calculations in the **log domain**. We work with $\log \alpha_t(i)$ instead of $\alpha_t(i)$. The products in the [recursion](@entry_id:264696) become sums. The summation, however, becomes a sum of exponentials, which must be handled carefully using the **[log-sum-exp trick](@entry_id:634104)**:
$$
\log \left( \sum_{j=1}^K \exp(x_j) \right) = m + \log \left( \sum_{j=1}^K \exp(x_j - m) \right), \quad \text{where } m = \max_j x_j
$$
This stabilization prevents [underflow](@entry_id:635171) and maintains numerical precision. The log-domain recursion becomes:
$$
\log \alpha_t(i) = \log p(y_t | z_t=i) + \underset{j \in \{1,\dots,K\}}{\text{logsumexp}} \left( \log \alpha_{t-1}(j) + \log A_{ji} \right)
$$
An alternative method is to use **scaling factors**, where the vector $\alpha_t$ is renormalized at each time step to sum to 1. The total likelihood can then be recovered from the sequence of scaling factors. Both methods effectively solve the [underflow](@entry_id:635171) problem.  

#### The Forward-Backward Algorithm: State Smoothing

To find the probability of being in state $i$ at time $t$ given all data, we use the **Forward-Backward algorithm**. This algorithm introduces the **backward message**, $\beta_t(i)$, defined as the [conditional probability](@entry_id:151013) of all future observations given that the system is in state $i$ at time $t$:
$$
\beta_t(i) \triangleq p(y_{t+1:T} | z_t = i)
$$
The backward messages are computed via a similar recursion, but starting from $t=T-1$ and moving backward in time.

The **smoothed posterior probability**, often denoted $\gamma_t(i) = p(z_t=i | y_{1:T})$, can be elegantly expressed by combining the forward and backward messages:
$$
\gamma_t(i) = p(z_t=i | y_{1:T}) = \frac{p(y_{1:T}, z_t=i)}{p(y_{1:T})} = \frac{\alpha_t(i) \beta_t(i)}{\sum_{j=1}^K \alpha_T(j)}
$$
The numerator $\alpha_t(i) \beta_t(i)$ represents the [joint probability](@entry_id:266356) of all the data and the system being in state $i$ at time $t$, a consequence of the conditional independence encoded in the Markov blankets. The denominator is the total data likelihood. This smoothed probability represents our best inference about the latent neural state at time $t$, having observed the entire experiment from start to finish. 

#### The Viterbi Algorithm: Decoding the Most Likely Path

While smoothing gives us the most likely state at each time point individually, it does not guarantee that the sequence of these most likely states is itself a valid or probable path. To find the single best overall state sequence, $z_{1:T}^*$, we use the **Viterbi algorithm**.

The Viterbi algorithm is another [dynamic programming](@entry_id:141107) approach that is structurally similar to the [forward algorithm](@entry_id:165467) but replaces summation with maximization. We define a new variable, $\delta_t(i)$, as the probability of the *most probable* state sequence ending in state $i$ at time $t$:
$$
\delta_t(i) \triangleq \max_{z_{1:t-1}} p(z_{1:t-1}, z_t = i, y_{1:t})
$$
The [recursion](@entry_id:264696) for $\delta_t(i)$ is:
$$
\delta_t(i) = p(y_t | z_t=i) \max_{j \in \{1,\dots,K\}} (\delta_{t-1}(j) A_{ji})
$$
To reconstruct the path, we must also store **backpointers**, $\psi_t(i)$, which record the preceding state $j$ that achieved the maximum for each $i$ at time $t$:
$$
\psi_t(i) = \arg\max_{j \in \{1,\dots,K\}} (\delta_{t-1}(j) A_{ji})
$$
The algorithm proceeds in two passes:
1.  **Forward Pass:** Compute and store $\delta_t(i)$ and $\psi_t(i)$ for all $t=1,\dots,T$ and $i=1,\dots,K$.
2.  **Backward Pass (Backtracking):** Find the most probable final state, $z_T^* = \arg\max_i \delta_T(i)$. Then, trace the path backward using the stored pointers: $z_{t-1}^* = \psi_t(z_t^*)$ for $t=T, T-1, \dots, 2$.

This procedure efficiently finds the globally optimal state sequence without having to enumerate all possibilities. 

### Practical Considerations: Model Identifiability

A crucial limitation of HMMs is **identifiability**. A model is identifiable if distinct sets of parameters produce distinct distributions over the observable data. If two different parameter sets produce nearly identical observation distributions, it becomes impossible to reliably distinguish them based on the data, no matter how much data is collected.

This issue is particularly acute in HMMs when the emission distributions of different states overlap significantly. Consider a simple HMM with two states whose Bernoulli emission probabilities are nearly identical (e.g., $r_1=0.51, r_2=0.49$). Now, compare two models: Model A with highly persistent states (e.g., [transition probabilities](@entry_id:158294) $a=0.9, b=0.9$) and Model B with highly anti-persistent states (e.g., $a=0.1, b=0.1$). Because the observations from state 1 and state 2 are so similar, the models can partially compensate for their different dynamics. A detailed calculation reveals that the **[total variation distance](@entry_id:143997)** between the distributions of observation sequences generated by these two very different models is extremely small. 

The implication for neuroscience is profound. If the neural firing patterns associated with two putative latent states are not sufficiently distinct, an HMM may fit the data well, but the inferred parameters—particularly the transition matrix $A$—may not be trustworthy. The model might discover latent states that are statistically convenient but do not correspond to genuinely distinct, interpretable biological processes. Therefore, assessing the separation of the learned emission distributions is a critical step in validating and interpreting the results of an HMM analysis.