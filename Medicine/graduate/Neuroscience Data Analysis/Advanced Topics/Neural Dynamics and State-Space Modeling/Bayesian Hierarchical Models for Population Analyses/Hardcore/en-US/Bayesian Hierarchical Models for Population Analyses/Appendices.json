{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of Bayesian analysis is the process of updating our beliefs—encoded in a prior distribution—in light of new data. This exercise  provides a foundational walkthrough of this process using a canonical model for neural spike counts. By combining a Poisson likelihood for observed spikes with a conjugate Gamma prior for the underlying firing rate, you will derive the posterior distribution, illustrating how prior knowledge and empirical evidence are mathematically synthesized to refine our estimate of a neuron's activity.",
            "id": "4141070",
            "problem": "In a population analysis of single-neuron spike trains across multiple recording intervals, suppose neuron $i$ produces spike counts $\\{y_{it}\\}_{t=1}^{n_i}$ over observation windows with known exposure durations $\\{T_{it}\\}_{t=1}^{n_i}$ (in seconds). Assume the following generative model grounded in standard point-process approximations for spike counts: conditional on a neuron-specific firing rate parameter $\\lambda_i$, the spike counts across intervals are conditionally independent and distributed as Poisson random variables with means proportional to exposure duration, that is, $y_{it} \\mid \\lambda_i, T_{it} \\sim \\text{Poisson}(\\lambda_i T_{it})$. At the population level, place a Gamma prior on $\\lambda_i$ with shape parameter $\\alpha$ and rate parameter $\\beta$, denoted $\\lambda_i \\mid \\alpha,\\beta \\sim \\text{Gamma}(\\alpha,\\beta)$, where the probability density function is $p(\\lambda_i \\mid \\alpha,\\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda_i^{\\alpha-1} \\exp(-\\beta \\lambda_i)$ for $\\lambda_i > 0$. Treat $\\alpha$ and $\\beta$ as known hyperparameters shared across neurons. Using Bayes’ rule together with the standard forms of the Poisson probability mass function and the Gamma probability density function, derive the posterior distribution $p(\\lambda_i \\mid \\{y_{it}\\}_{t=1}^{n_i}, \\{T_{it}\\}_{t=1}^{n_i}, \\alpha, \\beta)$ and identify it as a member of a named parametric family. Express your final answer as a single analytic expression giving the family name and its parameters in terms of $\\alpha$, $\\beta$, $\\sum_{t=1}^{n_i} y_{it}$, and $\\sum_{t=1}^{n_i} T_{it}$. Do not simplify numerically. Your final answer must be a single closed-form analytic expression.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n-   Data for neuron $i$: Spike counts $\\{y_{it}\\}_{t=1}^{n_i}$ and exposure durations $\\{T_{it}\\}_{t=1}^{n_i}$.\n-   Likelihood model for one observation: $y_{it} \\mid \\lambda_i, T_{it} \\sim \\text{Poisson}(\\lambda_i T_{it})$.\n-   Conditional independence: The spike counts $y_{it}$ are conditionally independent given the neuron-specific firing rate parameter $\\lambda_i$.\n-   Prior model for the firing rate: $\\lambda_i \\mid \\alpha,\\beta \\sim \\text{Gamma}(\\alpha,\\beta)$.\n-   PDF of the prior: $p(\\lambda_i \\mid \\alpha,\\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda_i^{\\alpha-1} \\exp(-\\beta \\lambda_i)$ for $\\lambda_i > 0$.\n-   Hyperparameters: $\\alpha$ and $\\beta$ are known constants.\n-   Objective: Derive the posterior distribution $p(\\lambda_i \\mid \\{y_{it}\\}_{t=1}^{n_i}, \\{T_{it}\\}_{t=1}^{n_i}, \\alpha, \\beta)$ and identify its family and parameters.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, employing the standard Poisson model for neural spike counts and a Gamma prior, which is the conjugate prior for the Poisson rate parameter. This is a canonical problem in Bayesian statistics and computational neuroscience. The problem is well-posed, as all necessary components (likelihood, prior, data) are specified, leading to a unique posterior distribution. The language is objective and mathematically precise. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\nThe objective is to derive the posterior distribution of the firing rate parameter $\\lambda_i$ for a single neuron $i$. Let the full dataset for neuron $i$ be denoted by $D_i = (\\{y_{it}\\}_{t=1}^{n_i}, \\{T_{it}\\}_{t=1}^{n_i})$. We apply Bayes' theorem, which states that the posterior distribution is proportional to the product of the likelihood and the prior distribution.\n\n$$\np(\\lambda_i \\mid D_i, \\alpha, \\beta) \\propto p(D_i \\mid \\lambda_i) \\times p(\\lambda_i \\mid \\alpha, \\beta)\n$$\n\nFirst, we formulate the likelihood function, $p(D_i \\mid \\lambda_i)$. The problem states that the spike counts $y_{it}$ are conditionally independent Poisson random variables, given $\\lambda_i$. The probability mass function (PMF) for a single observation $y_{it}$ is:\n$$\np(y_{it} \\mid \\lambda_i, T_{it}) = \\frac{(\\lambda_i T_{it})^{y_{it}} \\exp(-\\lambda_i T_{it})}{y_{it}!}\n$$\nDue to conditional independence, the total likelihood for all $n_i$ observations for neuron $i$ is the product of the individual PMFs:\n$$\nL(\\lambda_i; D_i) = p(D_i \\mid \\lambda_i) = \\prod_{t=1}^{n_i} p(y_{it} \\mid \\lambda_i, T_{it}) = \\prod_{t=1}^{n_i} \\frac{(\\lambda_i T_{it})^{y_{it}} \\exp(-\\lambda_i T_{it})}{y_{it}!}\n$$\nIn Bayesian inference, we are interested in the functional form of the likelihood with respect to the parameter $\\lambda_i$. We can therefore drop any terms that do not depend on $\\lambda_i$, as they will be absorbed into the normalization constant.\n$$\nL(\\lambda_i; D_i) \\propto \\prod_{t=1}^{n_i} (\\lambda_i T_{it})^{y_{it}} \\exp(-\\lambda_i T_{it})\n$$\nWe can separate the terms involving $\\lambda_i$ and simplify the products:\n$$\nL(\\lambda_i; D_i) \\propto \\prod_{t=1}^{n_i} \\lambda_i^{y_{it}} \\cdot \\prod_{t=1}^{n_i} T_{it}^{y_{it}} \\cdot \\prod_{t=1}^{n_i} \\exp(-\\lambda_i T_{it})\n$$\nAgain, dropping terms that are not functions of $\\lambda_i$ (specifically, $\\prod T_{it}^{y_{it}}$):\n$$\nL(\\lambda_i; D_i) \\propto \\left( \\prod_{t=1}^{n_i} \\lambda_i^{y_{it}} \\right) \\left( \\prod_{t=1}^{n_i} \\exp(-\\lambda_i T_{it}) \\right)\n$$\nUsing the properties of exponents, this simplifies to:\n$$\nL(\\lambda_i; D_i) \\propto \\lambda_i^{\\sum_{t=1}^{n_i} y_{it}} \\exp\\left(-\\lambda_i \\sum_{t=1}^{n_i} T_{it}\\right)\n$$\nNext, we consider the prior distribution for $\\lambda_i$, which is given as a Gamma distribution with shape $\\alpha$ and rate $\\beta$:\n$$\np(\\lambda_i \\mid \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda_i^{\\alpha-1} \\exp(-\\beta \\lambda_i)\n$$\nThe kernel of this prior distribution, which contains all the dependence on $\\lambda_i$, is:\n$$\np(\\lambda_i \\mid \\alpha, \\beta) \\propto \\lambda_i^{\\alpha-1} \\exp(-\\beta \\lambda_i)\n$$\nNow, we multiply the kernel of the likelihood by the kernel of the prior to find the kernel of the posterior distribution:\n$$\np(\\lambda_i \\mid D_i, \\alpha, \\beta) \\propto \\left( \\lambda_i^{\\sum_{t=1}^{n_i} y_{it}} \\exp\\left(-\\lambda_i \\sum_{t=1}^{n_i} T_{it}\\right) \\right) \\times \\left( \\lambda_i^{\\alpha-1} \\exp(-\\beta \\lambda_i) \\right)\n$$\nCombining terms with the same base:\n$$\np(\\lambda_i \\mid D_i, \\alpha, \\beta) \\propto \\lambda_i^{\\left(\\sum_{t=1}^{n_i} y_{it}\\right) + (\\alpha-1)} \\exp\\left(-\\lambda_i \\sum_{t=1}^{n_i} T_{it} - \\beta \\lambda_i\\right)\n$$\n$$\np(\\lambda_i \\mid D_i, \\alpha, \\beta) \\propto \\lambda_i^{\\left(\\alpha + \\sum_{t=1}^{n_i} y_{it}\\right) - 1} \\exp\\left(-\\left(\\beta + \\sum_{t=1}^{n_i} T_{it}\\right) \\lambda_i\\right)\n$$\nThis resulting functional form is the kernel of a Gamma distribution. A random variable $X$ that follows a Gamma distribution with shape parameter $\\alpha'$ and rate parameter $\\beta'$, denoted $X \\sim \\text{Gamma}(\\alpha', \\beta')$, has a probability density function proportional to $x^{\\alpha'-1}\\exp(-\\beta'x)$.\n\nBy comparing this form to our posterior kernel, we can identify the parameters of the posterior distribution.\nThe posterior shape parameter, $\\alpha'$, is:\n$$\n\\alpha' = \\alpha + \\sum_{t=1}^{n_i} y_{it}\n$$\nThe posterior rate parameter, $\\beta'$, is:\n$$\n\\beta' = \\beta + \\sum_{t=1}^{n_i} T_{it}\n$$\nTherefore, the posterior distribution of $\\lambda_i$ is a Gamma distribution with these updated parameters. This result demonstrates the conjugacy between the Gamma prior and the Poisson likelihood. The posterior distribution is:\n$$\n\\lambda_i \\mid D_i, \\alpha, \\beta \\sim \\text{Gamma}\\left(\\alpha + \\sum_{t=1}^{n_i} y_{it}, \\beta + \\sum_{t=1}^{n_i} T_{it}\\right)\n$$\nThe parameters are intuitively updated: the posterior shape is the prior shape plus the total number of observed spikes, and the posterior rate is the prior rate plus the total exposure duration.",
            "answer": "$$\n\\boxed{\\text{Gamma}\\left(\\alpha + \\sum_{t=1}^{n_i} y_{it}, \\beta + \\sum_{t=1}^{n_i} T_{it}\\right)}\n$$"
        },
        {
            "introduction": "Hierarchical models do more than just estimate parameters for individual units; they improve those estimates by \"borrowing statistical strength\" across the population, a phenomenon known as shrinkage or partial pooling. This exercise  offers a striking demonstration of this principle's power, showing how differential shrinkage, driven by varying measurement uncertainty, can lead to a counter-intuitive reversal of the rank-ordering of neural firing rates. Completing this derivation will provide a deep insight into how hierarchical models produce more robust inferences by intelligently weighting information from individual observations against the population's collective behavior.",
            "id": "4141091",
            "problem": "Consider evoked firing rate measurements from a population of neurons recorded under a single stimulus condition in a systems neuroscience experiment. For each unit $i \\in \\{A,B,C\\}$, denote the latent evoked firing rate by $\\theta_i$ (in $\\mathrm{spikes/s}$) and its observed sample mean across trials by $y_i$ (in $\\mathrm{spikes/s}$). Assume a Gaussian hierarchical model in which the latent evoked rates are exchangeable around a known population mean $ \\mu $ with prior variance $ \\tau^2 $, and the observations are corrupted by independent Gaussian measurement noise with known variances:\n- Prior: $ \\theta_i \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2) $.\n- Likelihood: $ y_i \\mid \\theta_i \\sim \\mathcal{N}(\\theta_i, \\sigma_i^2) $.\nAssume the population mean is known as $ \\mu = 20 $ (in $\\mathrm{spikes/s}$) based on a large historical dataset, and the current experiment yields the following observed means and measurement variances:\n- Unit $A$: $ y_A = 12 $, $ \\sigma_A^2 = 9 $.\n- Unit $B$: $ y_B = 15 $, $ \\sigma_B^2 = 1 $.\n- Unit $C$: $ y_C = 40 $, $ \\sigma_C^2 = 4 $.\nAll quantities $ y_i $ and $ \\sigma_i^2 $ are in their respective units stated above. The naive difference $ y_B - y_A $ is positive. Starting from Bayes’ theorem and the conjugacy of the Gaussian likelihood and Gaussian prior, derive the posterior mean $ \\mathbb{E}[\\theta_i \\mid y_i, \\mu, \\tau^2] $ for each unit $ i $, express the posterior difference $ \\mathbb{E}[\\theta_B \\mid y_B, \\mu, \\tau^2] - \\mathbb{E}[\\theta_A \\mid y_A, \\mu, \\tau^2] $ in terms of $ \\tau^2 $, and show analytically that there exists a critical prior variance $ \\tau_{\\star}^2 $ at which the posterior means for units $A$ and $B$ are equal. Compute this critical value $ \\tau_{\\star}^2 $ for the numbers given above and explain how, for $ \\tau^2 < \\tau_{\\star}^2 $, shrinkage reverses the direction of the naive difference. Express your final numerical answer for $ \\tau_{\\star}^2 $ in $(\\mathrm{spikes/s})^2$ and round to four significant figures.",
            "solution": "The problem statement poses a standard and well-defined question in Bayesian statistical analysis, specifically within the context of a hierarchical model for neuroscience data. All parameters and distributions are clearly specified, the model is scientifically sound and commonly used, and the question is mathematically tractable. The provided data are internally consistent. The problem is therefore deemed valid and a full solution can be derived.\n\nThe problem describes a Gaussian-Gaussian hierarchical model. For each unit $i$, the prior distribution for the latent firing rate $\\theta_i$ is given by\n$$ p(\\theta_i \\mid \\mu, \\tau^2) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{(\\theta_i - \\mu)^2}{2\\tau^2}\\right) $$\nwhich is a normal distribution $\\mathcal{N}(\\mu, \\tau^2)$. The likelihood of observing the sample mean firing rate $y_i$ given the latent rate $\\theta_i$ is\n$$ p(y_i \\mid \\theta_i, \\sigma_i^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left(-\\frac{(y_i - \\theta_i)^2}{2\\sigma_i^2}\\right) $$\nwhich is a normal distribution $\\mathcal{N}(\\theta_i, \\sigma_i^2)$.\n\nOur first task is to derive the posterior distribution for $\\theta_i$ given the observation $y_i$ and the hyperparameters $\\mu$ and $\\tau^2$. Using Bayes' theorem, the posterior probability density is proportional to the product of the likelihood and the prior:\n$$ p(\\theta_i \\mid y_i, \\mu, \\tau^2) \\propto p(y_i \\mid \\theta_i, \\sigma_i^2) p(\\theta_i \\mid \\mu, \\tau^2) $$\nSince the product of two Gaussian distributions is proportional to another Gaussian distribution (a property known as conjugacy), the posterior $p(\\theta_i \\mid y_i, \\mu, \\tau^2)$ will be a Gaussian distribution. We can find its parameters by examining the exponent of the product.\n$$ p(\\theta_i \\mid y_i, \\mu, \\tau^2) \\propto \\exp\\left(-\\frac{(y_i - \\theta_i)^2}{2\\sigma_i^2}\\right) \\exp\\left(-\\frac{(\\theta_i - \\mu)^2}{2\\tau^2}\\right) $$\n$$ \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(\\theta_i - y_i)^2}{\\sigma_i^2} + \\frac{(\\theta_i - \\mu)^2}{\\tau^2} \\right] \\right) $$\nTo find the posterior mean, we complete the square for $\\theta_i$ in the exponent. Let the posterior be $\\mathcal{N}(\\hat{\\theta}_i, \\sigma_{\\text{post},i}^2)$. The exponent will have the form $-\\frac{(\\theta_i - \\hat{\\theta}_i)^2}{2\\sigma_{\\text{post},i}^2}$. Expanding the terms in the bracket:\n$$ \\frac{\\theta_i^2 - 2\\theta_i y_i + y_i^2}{\\sigma_i^2} + \\frac{\\theta_i^2 - 2\\theta_i \\mu + \\mu^2}{\\tau^2} $$\nCollecting terms in $\\theta_i^2$ and $\\theta_i$:\n$$ \\theta_i^2 \\left(\\frac{1}{\\sigma_i^2} + \\frac{1}{\\tau^2}\\right) - 2\\theta_i \\left(\\frac{y_i}{\\sigma_i^2} + \\frac{\\mu}{\\tau^2}\\right) + \\dots $$\nThe posterior precision (inverse variance) is the coefficient of $\\theta_i^2/(-2)$ after completing the square, which is the sum of the individual precisions:\n$$ \\frac{1}{\\sigma_{\\text{post},i}^2} = \\frac{1}{\\sigma_i^2} + \\frac{1}{\\tau^2} $$\nThe posterior mean $\\hat{\\theta}_i = \\mathbb{E}[\\theta_i \\mid y_i, \\mu, \\tau^2]$ is given by the ratio of the coefficient of $-2\\theta_i$ to the coefficient of $\\theta_i^2$:\n$$ \\hat{\\theta}_i = \\frac{\\frac{y_i}{\\sigma_i^2} + \\frac{\\mu}{\\tau^2}}{\\frac{1}{\\sigma_i^2} + \\frac{1}{\\tau^2}} $$\nThis demonstrates that the posterior mean is a precision-weighted average of the observed data $y_i$ and the prior mean $\\mu$. By multiplying the numerator and denominator by $\\sigma_i^2 \\tau^2$, we obtain a more convenient form:\n$$ \\hat{\\theta}_i = \\frac{y_i \\tau^2 + \\mu \\sigma_i^2}{\\tau^2 + \\sigma_i^2} $$\nThis expression can be interpreted as a weighted average:\n$$ \\hat{\\theta}_i = \\left(\\frac{\\tau^2}{\\sigma_i^2 + \\tau^2}\\right) y_i + \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\tau^2}\\right) \\mu = (1 - B_i) y_i + B_i \\mu $$\nwhere $B_i = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\tau^2}$ is the \"shrinkage factor,\" which quantifies how much the estimate is shrunk from the observation $y_i$ toward the prior mean $\\mu$.\n\nThe problem asks for the condition under which the posterior means for units $A$ and $B$ are equal. Let $\\hat{\\theta}_A = \\mathbb{E}[\\theta_A \\mid y_A, \\mu, \\tau^2]$ and $\\hat{\\theta}_B = \\mathbb{E}[\\theta_B \\mid y_B, \\mu, \\tau^2]$. We set $\\hat{\\theta}_A = \\hat{\\theta}_B$:\n$$ \\frac{y_A \\tau^2 + \\mu \\sigma_A^2}{\\tau^2 + \\sigma_A^2} = \\frac{y_B \\tau^2 + \\mu \\sigma_B^2}{\\tau^2 + \\sigma_B^2} $$\nTo find the critical variance $\\tau_{\\star}^2$ where this equality holds, we solve for $\\tau^2$.\n$$ (y_A \\tau^2 + \\mu \\sigma_A^2)(\\tau^2 + \\sigma_B^2) = (y_B \\tau^2 + \\mu \\sigma_B^2)(\\tau^2 + \\sigma_A^2) $$\n$$ y_A (\\tau^2)^2 + y_A \\tau^2 \\sigma_B^2 + \\mu \\sigma_A^2 \\tau^2 + \\mu \\sigma_A^2 \\sigma_B^2 = y_B (\\tau^2)^2 + y_B \\tau^2 \\sigma_A^2 + \\mu \\sigma_B^2 \\tau^2 + \\mu \\sigma_B^2 \\sigma_A^2 $$\nThe term $\\mu \\sigma_A^2 \\sigma_B^2$ cancels on both sides. We rearrange to collect terms in $(\\tau^2)^2$ and $\\tau^2$:\n$$ (\\tau^2)^2 (y_A - y_B) + \\tau^2 (y_A \\sigma_B^2 + \\mu \\sigma_A^2 - y_B \\sigma_A^2 - \\mu \\sigma_B^2) = 0 $$\n$$ \\tau^2 \\left[ \\tau^2 (y_A - y_B) + (y_A \\sigma_B^2 - y_B \\sigma_A^2 + \\mu \\sigma_A^2 - \\mu \\sigma_B^2) \\right] = 0 $$\nOne solution is the trivial case $\\tau^2 = 0$. The non-trivial solution $\\tau_{\\star}^2$ is found by setting the term in the brackets to zero:\n$$ \\tau_{\\star}^2 (y_A - y_B) = - (y_A \\sigma_B^2 - y_B \\sigma_A^2 + \\mu \\sigma_A^2 - \\mu \\sigma_B^2) $$\n$$ \\tau_{\\star}^2 (y_A - y_B) = y_B \\sigma_A^2 - y_A \\sigma_B^2 - \\mu \\sigma_A^2 + \\mu \\sigma_B^2 $$\n$$ \\tau_{\\star}^2 (y_A - y_B) = \\sigma_A^2 (y_B - \\mu) - \\sigma_B^2 (y_A - \\mu) $$\nIsolating $\\tau_{\\star}^2$:\n$$ \\tau_{\\star}^2 = \\frac{\\sigma_A^2 (y_B - \\mu) - \\sigma_B^2 (y_A - \\mu)}{y_A - y_B} $$\nThis shows analytically that such a critical variance exists, provided it is positive. We now substitute the given numerical values: $\\mu = 20$, $y_A = 12$, $\\sigma_A^2 = 9$, $y_B = 15$, $\\sigma_B^2 = 1$.\n$$ \\tau_{\\star}^2 = \\frac{9(15 - 20) - 1(12 - 20)}{12 - 15} $$\n$$ \\tau_{\\star}^2 = \\frac{9(-5) - 1(-8)}{-3} = \\frac{-45 + 8}{-3} = \\frac{-37}{-3} = \\frac{37}{3} $$\nSince $\\tau_{\\star}^2 = 37/3 > 0$, a physically meaningful critical variance exists. The numerical value is approximately $12.3333...$ $(\\mathrm{spikes/s})^2$. Rounded to four significant figures, this is $12.33$ $(\\mathrm{spikes/s})^2$.\n\nFinally, we must explain the reversal of the naive difference for $\\tau^2 < \\tau_{\\star}^2$. The naive difference is $y_B - y_A = 15 - 12 = 3 > 0$. We want to analyze the sign of the posterior difference $\\hat{\\theta}_B - \\hat{\\theta}_A$.\n$$ \\hat{\\theta}_B - \\hat{\\theta}_A = \\frac{y_B \\tau^2 + \\mu \\sigma_B^2}{\\tau^2 + \\sigma_B^2} - \\frac{y_A \\tau^2 + \\mu \\sigma_A^2}{\\tau^2 + \\sigma_A^2} $$\nThe common denominator is $(\\tau^2 + \\sigma_B^2)(\\tau^2 + \\sigma_A^2)$, which is positive. The sign of the difference is determined by the numerator:\n$$ N(\\tau^2) = (y_B \\tau^2 + \\mu \\sigma_B^2)(\\tau^2 + \\sigma_A^2) - (y_A \\tau^2 + \\mu \\sigma_A^2)(\\tau^2 + \\sigma_B^2) $$\nThis is the negative of the expression we set to zero earlier. From the previous derivation:\n$$ N(\\tau^2) = \\tau^2 (\\tau^2(y_B - y_A) - [\\sigma_A^2(y_A - \\mu) - \\sigma_B^2(y_B - \\mu)]) $$\nUsing the definition of $\\tau_{\\star}^2$: $\\tau_{\\star}^2 (y_A - y_B) = \\sigma_A^2(y_B - \\mu) - \\sigma_B^2(y_A - \\mu)$, which is equivalent to $\\tau_{\\star}^2 (y_B - y_A) = -[\\sigma_A^2(y_A - \\mu) - \\sigma_B^2(y_B - \\mu)]$.\nSubstituting this into the expression for $N(\\tau^2)$:\n$$ N(\\tau^2) = \\tau^2 (\\tau^2(y_B - y_A) - [-\\tau_{\\star}^2(y_B - y_A)]) $$\n$$ N(\\tau^2) = \\tau^2 (y_B - y_A) (\\tau^2 - \\tau_{\\star}^2) $$\nThe sign of $\\hat{\\theta}_B - \\hat{\\theta}_A$ is the sign of $N(\\tau^2)$. We are given that $y_B - y_A = 3 > 0$. Also, $\\tau^2 \\ge 0$. Therefore, the sign is determined by the term $(\\tau^2 - \\tau_{\\star}^2)$.\nFor $\\tau^2 < \\tau_{\\star}^2$, the term $(\\tau^2 - \\tau_{\\star}^2)$ is negative.\nThus, the sign of $N(\\tau^2)$ is $(+)(+)(-) = (-)$.\nThis implies that for $\\tau^2 < \\tau_{\\star}^2$, we have $\\hat{\\theta}_B - \\hat{\\theta}_A < 0$, or $\\hat{\\theta}_B < \\hat{\\theta}_A$.\nThe naive difference $y_B > y_A$ has been reversed to $\\hat{\\theta}_B < \\hat{\\theta}_A$. This phenomenon is a consequence of differential shrinkage. Both observed means, $y_A = 12$ and $y_B = 15$, are below the population mean $\\mu=20$. The Bayesian model shrinks both estimates toward $\\mu$, increasing their values. The strength of this shrinkage is determined by the shrinkage factor $B_i = \\sigma_i^2/(\\sigma_i^2 + \\tau^2)$. We compare the shrinkage factors for $A$ and $B$:\n$$ B_A = \\frac{9}{9+\\tau^2} \\quad \\text{and} \\quad B_B = \\frac{1}{1+\\tau^2} $$\nSince $9(1+\\tau^2) = 9+9\\tau^2 > 9+\\tau^2 = 1(9+\\tau^2)$, it follows that $\\frac{9}{9+\\tau^2} > \\frac{1}{1+\\tau^2}$, so $B_A > B_B$.\nUnit $A$ has a much larger measurement variance ($\\sigma_A^2=9$) than unit $B$ ($\\sigma_B^2=1$), indicating its measurement is less reliable. Consequently, its posterior mean $\\hat{\\theta}_A$ is shrunk more strongly towards the population mean $\\mu$. Since both $y_A$ and $y_B$ are below $\\mu$, $\\hat{\\theta}_A$ is \"pulled up\" from $12$ more than $\\hat{\\theta}_B$ is pulled up from $15$. When the prior is sufficiently informative (i.e., $\\tau^2$ is small, specifically $\\tau^2 < \\tau_{\\star}^2$), this differential pull is strong enough to cause $\\hat{\\theta}_A$ to overtake $\\hat{\\theta}_B$, reversing the direction of the difference observed in the raw data.\nThe posterior difference is given by\n$$ \\hat{\\theta}_B - \\hat{\\theta}_A = \\frac{\\tau^2 (y_B - y_A) (\\tau^2 - \\tau_{\\star}^2)}{(\\tau^2 + \\sigma_A^2)(\\tau^2 + \\sigma_B^2)} $$",
            "answer": "$$\\boxed{12.33}$$"
        },
        {
            "introduction": "While conjugate models offer analytical elegance, real-world neuroscience data often demand more flexible frameworks like Generalized Linear Models (GLMs) that incorporate stimulus covariates but lack conjugacy. This advanced exercise  tackles this challenge head-on by guiding you through the derivation of the Laplace approximation. This powerful technique allows us to approximate the marginal posterior distribution of hyperparameters by integrating out unit-level parameters, providing a practical and widely-used method for fitting complex, non-conjugate hierarchical models.",
            "id": "4141059",
            "problem": "Consider spike count data from $N$ simultaneously recorded neurons responding to a time-varying stimulus. For neuron $i \\in \\{1,\\dots,N\\}$ and time bin $t \\in \\{1,\\dots,T_i\\}$, let $y_{it} \\in \\{0,1,2,\\dots\\}$ denote the observed spike count, and let $x_{it} \\in \\mathbb{R}^{p}$ denote a $p$-dimensional stimulus feature vector constructed from the experimental design (for example, recent stimulus history and covariates). Assume a hierarchical Generalized Linear Model (GLM) with a Poisson observation model and canonical log link for each neuron:\n$$\ny_{it} \\mid w_i \\sim \\text{Poisson}\\!\\left(\\lambda_{it}\\right), \\quad \\lambda_{it} = \\exp\\!\\left(x_{it}^{\\top} w_i\\right),\n$$\nwhere $w_i \\in \\mathbb{R}^{p}$ are unit-level parameters (weights) for neuron $i$. Place a multivariate normal population prior on the unit-level parameters,\n$$\nw_i \\mid \\mu, \\Sigma \\sim \\mathcal{N}\\!\\left(\\mu, \\Sigma\\right),\n$$\nwith hyperparameters $\\mu \\in \\mathbb{R}^{p}$ and a positive-definite covariance matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$. Assume a proper hyperprior density $p(\\mu,\\Sigma)$ supported on $\\mathbb{R}^{p} \\times \\{\\text{positive-definite matrices}\\}$.\n\nStarting from Bayes’ rule and the definition of the Poisson likelihood and Gaussian prior above, derive an analytically tractable approximation to the marginal posterior density of the hyperparameters given all neurons’ data by integrating out the unit-level parameters $\\{w_i\\}_{i=1}^{N}$ using a second-order Taylor expansion around the per-neuron mode. Your derivation must proceed from first principles (Bayes’ rule, independence assumptions, and Taylor’s theorem), and must identify the per-neuron mode $w_i^{\\star}$ as the maximizer of the joint log-density in $w_i$, as well as the corresponding negative Hessian matrix. You may omit any constants that do not depend on $\\mu$ or $\\Sigma$.\n\nProvide the final result as a single closed-form analytic expression for the Laplace-approximated marginal posterior density $p(\\mu,\\Sigma \\mid \\{y_{it}\\}, \\{x_{it}\\})$ up to proportionality, expressed in terms of $\\mu$, $\\Sigma$, the data $\\{y_{it}\\}$, $\\{x_{it}\\}$, the per-neuron modes $w_i^{\\star}$, and the negative Hessians evaluated at $w_i^{\\star}$. No numerical evaluation is required, and no rounding is necessary. Express all logarithms as natural logarithms and all exponentials using $\\exp(\\cdot)$ notation.",
            "solution": "The objective is to derive an approximation for the marginal posterior density of the hyperparameters, $p(\\mu, \\Sigma \\mid \\{y_{it}\\}, \\{x_{it}\\})$, by integrating out the unit-level parameters $\\{w_i\\}_{i=1}^N$. This is achieved using the Laplace approximation method.\n\nWe begin with Bayes' rule for the joint posterior of all parameters:\n$$\np(\\{w_i\\}_{i=1}^N, \\mu, \\Sigma \\mid \\{y_{it}\\}, \\{x_{it}\\}) \\propto p(\\{y_{it}\\}, \\{x_{it}\\} \\mid \\{w_i\\}_{i=1}^N, \\mu, \\Sigma) \\times p(\\{w_i\\}_{i=1}^N, \\mu, \\Sigma)\n$$\nUsing the conditional independence assumptions specified in the problem statement, we can factorize the terms. The likelihood depends only on $\\{w_i\\}$, and the prior can be factored into the hyperprior and the prior on $\\{w_i\\}$ given the hyperparameters:\n$$\np(\\{y_{it}\\}, \\{x_{it}\\} \\mid \\{w_i\\}_{i=1}^N, \\mu, \\Sigma) = p(\\{y_{it}\\} \\mid \\{w_i\\}_{i=1}^N, \\{x_{it}\\}) = \\prod_{i=1}^N p(\\{y_{it}\\}_{t=1}^{T_i} \\mid w_i, \\{x_{it}\\}_{t=1}^{T_i})\n$$\n$$\np(\\{w_i\\}_{i=1}^N, \\mu, \\Sigma) = p(\\{w_i\\}_{i=1}^N \\mid \\mu, \\Sigma) p(\\mu, \\Sigma) = \\left( \\prod_{i=1}^N p(w_i \\mid \\mu, \\Sigma) \\right) p(\\mu, \\Sigma)\n$$\nFor notational simplicity, we will denote the full dataset $(\\{y_{it}\\}, \\{x_{it}\\})$ as $\\mathcal{D}$. The marginal posterior for the hyperparameters is obtained by integrating out the weights $\\{w_i\\}$:\n$$\np(\\mu, \\Sigma \\mid \\mathcal{D}) = \\int \\dots \\int p(\\{w_i\\}_{i=1}^N, \\mu, \\Sigma \\mid \\mathcal{D}) \\, dw_1 \\dots dw_N\n$$\nSubstituting the factored form of the joint posterior, we have:\n$$\np(\\mu, \\Sigma \\mid \\mathcal{D}) \\propto \\int \\dots \\int \\left( \\prod_{i=1}^N p(\\{y_{it}\\}_{t=1}^{T_i} \\mid w_i) p(w_i \\mid \\mu, \\Sigma) \\right) p(\\mu, \\Sigma) \\, dw_1 \\dots dw_N\n$$\nSince $p(\\mu, \\Sigma)$ does not depend on any $w_i$, we can move it outside the integrals. The integral of a product is the product of integrals due to the independence of the $(w_i, \\text{data}_i)$ pairs, conditional on $(\\mu, \\Sigma)$:\n$$\np(\\mu, \\Sigma \\mid \\mathcal{D}) \\propto p(\\mu, \\Sigma) \\prod_{i=1}^N \\int p(\\{y_{it}\\}_{t=1}^{T_i} \\mid w_i) p(w_i \\mid \\mu, \\Sigma) \\, dw_i\n$$\nWe apply the Laplace approximation to each integral in the product. For a single neuron $i$, the integral is:\n$$\nI_i(\\mu, \\Sigma) = \\int p(\\{y_{it}\\}_{t=1}^{T_i} \\mid w_i) p(w_i \\mid \\mu, \\Sigma) \\, dw_i = \\int \\exp\\left( \\ln\\left( p(\\{y_{it}\\}_{t=1}^{T_i} \\mid w_i) p(w_i \\mid \\mu, \\Sigma) \\right) \\right) \\, dw_i\n$$\nLet $L_i(w_i; \\mu, \\Sigma)$ be the logarithm of the integrand, which is the log-posterior for $w_i$ (up to a constant independent of $w_i$):\n$$\nL_i(w_i; \\mu, \\Sigma) = \\ln p(\\{y_{it}\\}_{t=1}^{T_i} \\mid w_i) + \\ln p(w_i \\mid \\mu, \\Sigma)\n$$\nThe problem defines the per-neuron mode $w_i^{\\star}$ as the maximizer of this joint log-density:\n$$\nw_i^{\\star} = \\arg\\max_{w_i} L_i(w_i; \\mu, \\Sigma)\n$$\nThe Laplace approximation relies on a second-order Taylor expansion of $L_i$ around $w_i^{\\star}$:\n$$\nL_i(w_i; \\mu, \\Sigma) \\approx L_i(w_i^{\\star}; \\mu, \\Sigma) + (w_i - w_i^{\\star})^{\\top} \\nabla_{w_i} L_i(w_i^{\\star}) + \\frac{1}{2} (w_i - w_i^{\\star})^{\\top} \\nabla_{w_i}^2 L_i(w_i^{\\star}) (w_i - w_i^{\\star})\n$$\nBy definition of the mode, $\\nabla_{w_i} L_i(w_i^{\\star}) = 0$. Let $A_i^{\\star}$ be the negative Hessian matrix evaluated at the mode:\n$$\nA_i^{\\star} = -\\nabla_{w_i}^2 L_i(w_i^{\\star}; \\mu, \\Sigma)\n$$\nThe approximation for $L_i$ becomes:\n$$\nL_i(w_i; \\mu, \\Sigma) \\approx L_i(w_i^{\\star}; \\mu, \\Sigma) - \\frac{1}{2} (w_i - w_i^{\\star})^{\\top} A_i^{\\star} (w_i - w_i^{\\star})\n$$\nSubstituting this back into the integral $I_i$, we get:\n$$\nI_i(\\mu, \\Sigma) \\approx \\int \\exp\\left( L_i(w_i^{\\star}; \\mu, \\Sigma) - \\frac{1}{2} (w_i - w_i^{\\star})^{\\top} A_i^{\\star} (w_i - w_i^{\\star}) \\right) \\, dw_i\n$$\n$$\nI_i(\\mu, \\Sigma) \\approx \\exp(L_i(w_i^{\\star}; \\mu, \\Sigma)) \\int \\exp\\left(-\\frac{1}{2} (w_i - w_i^{\\star})^{\\top} A_i^{\\star} (w_i - w_i^{\\star})\\right) \\, dw_i\n$$\nThe remaining integral is the unnormalized density of a multivariate normal distribution with mean $w_i^{\\star}$ and covariance matrix $(A_i^{\\star})^{-1}$. Its value is $(2\\pi)^{p/2} |(A_i^{\\star})^{-1}|^{1/2} = (2\\pi)^{p/2} |A_i^{\\star}|^{-1/2}$.\nThus, the Laplace approximation for the integral is:\n$$\nI_i(\\mu, \\Sigma) \\approx (2\\pi)^{p/2} |A_i^{\\star}|^{-1/2} \\exp(L_i(w_i^{\\star}; \\mu, \\Sigma))\n$$\nNow we must find the explicit forms of $L_i(w_i^{\\star}; \\mu, \\Sigma)$ and $A_i^{\\star}$.\nThe log-likelihood for neuron $i$ is (up to constants):\n$$\n\\ln p(\\{y_{it}\\} \\mid w_i) \\propto \\sum_{t=1}^{T_i} \\left( y_{it} \\ln(\\lambda_{it}) - \\lambda_{it} \\right) = \\sum_{t=1}^{T_i} \\left( y_{it} (x_{it}^{\\top} w_i) - \\exp(x_{it}^{\\top} w_i) \\right)\n$$\nThe log-prior for $w_i$ is (up to constants):\n$$\n\\ln p(w_i \\mid \\mu, \\Sigma) \\propto -\\frac{1}{2} \\ln|\\Sigma| - \\frac{1}{2} (w_i - \\mu)^{\\top} \\Sigma^{-1} (w_i - \\mu)\n$$\nThe full log-density for $w_i$ is:\n$$\nL_i(w_i; \\mu, \\Sigma) \\propto \\sum_{t=1}^{T_i} \\left( y_{it} x_{it}^{\\top} w_i - \\exp(x_{it}^{\\top} w_i) \\right) - \\frac{1}{2} (w_i - \\mu)^{\\top} \\Sigma^{-1} (w_i - \\mu) - \\frac{1}{2}\\ln|\\Sigma|\n$$\nThe negative Hessian $A_i^{\\star}$ is found by differentiating twice and negating. The first derivative (gradient) is:\n$$\n\\nabla_{w_i} L_i = \\sum_{t=1}^{T_i} x_{it} \\left( y_{it} - \\exp(x_{it}^{\\top} w_i) \\right) - \\Sigma^{-1} (w_i - \\mu)\n$$\nThe second derivative (Hessian) is:\n$$\n\\nabla_{w_i}^2 L_i = \\sum_{t=1}^{T_i} -x_{it} \\exp(x_{it}^{\\top} w_i) x_{it}^{\\top} - \\Sigma^{-1} = -\\left( \\sum_{t=1}^{T_i} \\exp(x_{it}^{\\top} w_i) x_{it} x_{it}^{\\top} + \\Sigma^{-1} \\right)\n$$\nThe negative Hessian, evaluated at the mode $w_i^{\\star}$, is therefore:\n$$\nA_i^{\\star} = \\sum_{t=1}^{T_i} \\exp(x_{it}^{\\top} w_i^{\\star}) x_{it} x_{it}^{\\top} + \\Sigma^{-1}\n$$\nNow, assemble the marginal posterior for the hyperparameters, omitting constants independent of $\\mu$ and $\\Sigma$, such as $(2\\pi)^{p/2}$:\n$$\np(\\mu, \\Sigma \\mid \\mathcal{D}) \\propto p(\\mu, \\Sigma) \\prod_{i=1}^N \\left( |A_i^{\\star}|^{-1/2} \\exp(L_i(w_i^{\\star}; \\mu, \\Sigma)) \\right)\n$$\nSubstituting the expression for $L_i(w_i^{\\star}; \\mu, \\Sigma)$ and dropping additive constant terms within the exponential:\n$$\np(\\mu, \\Sigma \\mid \\mathcal{D}) \\propto p(\\mu, \\Sigma) \\prod_{i=1}^N \\left( |A_i^{\\star}|^{-1/2} \\exp\\left( \\sum_{t=1}^{T_i} \\left[ y_{it} x_{it}^{\\top} w_i^{\\star} - \\exp(x_{it}^{\\top} w_i^{\\star}) \\right] - \\frac{1}{2} (w_i^{\\star} - \\mu)^{\\top} \\Sigma^{-1} (w_i^{\\star} - \\mu) - \\frac{1}{2}\\ln|\\Sigma| \\right) \\right)\n$$\nWe can combine the terms in the product. The term $\\exp(-\\frac{1}{2}\\ln|\\Sigma|) = |\\Sigma|^{-1/2}$ appears for each of the $N$ neurons.\n$$\np(\\mu, \\Sigma \\mid \\mathcal{D}) \\propto p(\\mu, \\Sigma) \\left( \\prod_{i=1}^N |A_i^{\\star}|^{-1/2} \\right) \\left( \\prod_{i=1}^N |\\Sigma|^{-1/2} \\right) \\exp\\left( \\sum_{i=1}^N \\left( \\sum_{t=1}^{T_i} \\left[ y_{it} x_{it}^{\\top} w_i^{\\star} - \\exp(x_{it}^{\\top} w_i^{\\star}) \\right] - \\frac{1}{2} (w_i^{\\star} - \\mu)^{\\top} \\Sigma^{-1} (w_i^{\\star} - \\mu) \\right) \\right)\n$$\nSimplifying the product of determinants gives the final expression for the Laplace-approximated marginal posterior density:\n$$\np(\\mu, \\Sigma \\mid \\{y_{it}\\}, \\{x_{it}\\}) \\propto p(\\mu,\\Sigma) |\\Sigma|^{-N/2} \\left( \\prod_{i=1}^N |A_i^{\\star}|^{-1/2} \\right) \\exp\\left( -\\frac{1}{2} \\sum_{i=1}^N (w_i^{\\star} - \\mu)^{\\top} \\Sigma^{-1} (w_i^{\\star} - \\mu) + \\sum_{i=1}^N \\sum_{t=1}^{T_i} \\left( y_{it} x_{it}^{\\top} w_i^{\\star} - \\exp(x_{it}^{\\top} w_i^{\\star}) \\right) \\right)\n$$\nHere, $w_i^{\\star}$ is the mode for neuron $i$, and $A_i^{\\star} = \\sum_{t=1}^{T_i} \\exp(x_{it}^{\\top} w_i^{\\star}) x_{it} x_{it}^{\\top} + \\Sigma^{-1}$ is the corresponding negative Hessian.",
            "answer": "$$\n\\boxed{p(\\mu,\\Sigma) |\\Sigma|^{-N/2} \\left( \\prod_{i=1}^N |A_i^{\\star}|^{-1/2} \\right) \\exp\\left( -\\frac{1}{2} \\sum_{i=1}^N (w_i^{\\star} - \\mu)^{\\top} \\Sigma^{-1} (w_i^{\\star} - \\mu) + \\sum_{i=1}^N \\sum_{t=1}^{T_i} \\left( y_{it} x_{it}^{\\top} w_i^{\\star} - \\exp(x_{it}^{\\top} w_i^{\\star}) \\right) \\right)}\n$$"
        }
    ]
}