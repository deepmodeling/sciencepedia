{
    "hands_on_practices": [
        {
            "introduction": "The Network-Based Statistic (NBS) begins with a mass-univariate analysis, where a statistical test is performed independently at every edge of the connectome. This practice solidifies your understanding of this foundational step by guiding you through the derivation of the studentized t-statistic from the principles of the General Linear Model (GLM). A firm grasp of how this statistic is constructed is essential for correctly applying and interpreting connectome-wide hypothesis tests .",
            "id": "4181109",
            "problem": "In Network-Based Statistic (NBS) hypothesis testing for connectomes, one computes an edge-wise test statistic across subjects and then applies a component-level correction. Consider an advanced graduate-level setting with $n$ subjects and $p$ regressors, where each edge $e$ yields a subject-wise response vector $y_e \\in \\mathbb{R}^n$. Assume the General Linear Model (GLM) $y_e = X \\beta_e + \\varepsilon_e$ with a fixed, full-column-rank design matrix $X \\in \\mathbb{R}^{n \\times p}$ and errors $\\varepsilon_e \\sim \\mathcal{N}(0, \\sigma_e^2 I_n)$ that are independent and identically distributed across subjects for that edge. A scientific contrast of interest is encoded by a vector $c \\in \\mathbb{R}^p$, and the null hypothesis for edge $e$ is $H_0: c^\\top \\beta_e = 0$.\n\nStarting from the GLM assumptions and ordinary least squares, derive the edge-wise studentized test statistic for $H_0: c^\\top \\beta_e = 0$, and precisely define the constituent quantities and their estimators in terms of $X$ and $y_e$. Then, identify which option below correctly gives the final expression and all definitions needed to compute it in practice for an NBS pipeline. Select the single best answer.\n\nA. $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\hat{\\sigma}_e \\sqrt{c^\\top (X^\\top X)^{-1} c}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, $\\mathrm{RSS}_e = \\|y_e - X \\hat{\\beta}_e\\|_2^2$, $\\hat{\\sigma}_e^2 = \\dfrac{\\mathrm{RSS}_e}{n - p}$, and under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n - p$ degrees of freedom.\n\nB. $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\sqrt{\\hat{\\sigma}_e^2 \\, c^\\top (X X^\\top)^{-1} c}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, $\\mathrm{RSS}_e = \\|y_e - X \\hat{\\beta}_e\\|_2^2$, $\\hat{\\sigma}_e^2 = \\dfrac{\\mathrm{RSS}_e}{n}$, and under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n$ degrees of freedom.\n\nC. $t_e = \\dfrac{\\mathbf{1}^\\top \\hat{\\beta}_e}{\\hat{\\sigma} \\sqrt{\\mathbf{1}^\\top (X^\\top X)^{-1} \\mathbf{1}}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, and $\\hat{\\sigma}^2$ is a pooled variance estimated by aggregating $\\mathrm{RSS}_e$ across all edges and dividing by $n$; under $H_0$ the statistic $t_e$ follows a standard normal distribution.\n\nD. $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\hat{s}_e}$, where $\\hat{s}_e^2 = \\dfrac{\\|y_e\\|_2^2}{n - p}$, with no dependence on $X$ in the variance estimate; under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n - p$ degrees of freedom.",
            "solution": "The user wants me to validate the problem statement and, if valid, derive the solution and evaluate the given options.\n\n### Problem Validation\n\nI will first validate the problem statement according to the specified procedure.\n\n**Step 1: Extract Givens**\n\n*   Number of subjects: $n$\n*   Number of regressors: $p$\n*   Subject-wise response vector for edge $e$: $y_e \\in \\mathbb{R}^n$\n*   General Linear Model (GLM): $y_e = X \\beta_e + \\varepsilon_e$\n*   Design matrix: $X \\in \\mathbb{R}^{n \\times p}$, fixed and of full-column-rank\n*   Error distribution: $\\varepsilon_e \\sim \\mathcal{N}(0, \\sigma_e^2 I_n)$, independent and identically distributed (i.i.d.) across subjects for edge $e$\n*   Contrast vector: $c \\in \\mathbb{R}^p$\n*   Null hypothesis for edge $e$: $H_0: c^\\top \\beta_e = 0$\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem is firmly grounded in the theory of the General Linear Model, a cornerstone of statistical analysis in many scientific fields, including neuroscience. The formulation—using a design matrix, contrast vector, and hypothesis testing—is standard practice for analyzing fMRI, EEG/MEG, and structural connectome data (as in NBS).\n2.  **Well-Posed:** The problem is well-posed. The assumption that the design matrix $X$ is of full-column-rank ensures that the matrix $X^\\top X$ is invertible, which is necessary for finding a unique Ordinary Least Squares (OLS) solution for the parameters. The statistical assumptions (i.i.d. Gaussian errors) lead to a well-defined distributional theory for the test statistic.\n3.  **Objective:** The problem is stated in precise, objective mathematical and statistical language, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. It provides all necessary information to derive the required test statistic from first principles. I will now proceed with the solution derivation.\n\n### Solution Derivation\n\nThe objective is to derive the studentized test statistic for the null hypothesis $H_0: c^\\top \\beta_e = 0$ based on the specified GLM.\n\n**1. Estimation of Parameters $\\beta_e$**\n\nThe parameters $\\beta_e$ are estimated using Ordinary Least Squares (OLS), which minimizes the Residual Sum of Squares, $\\mathrm{RSS}_e = \\|y_e - X\\beta_e\\|_2^2$. The OLS estimator $\\hat{\\beta}_e$ is the solution to the normal equations:\n$$ (X^\\top X) \\hat{\\beta}_e = X^\\top y_e $$\nSince $X$ is full-column-rank, $X^\\top X$ is a $p \\times p$ invertible matrix. Thus, the estimator is unique:\n$$ \\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e $$\n\n**2. Distribution of the Estimator $\\hat{\\beta}_e$**\n\nThe estimator $\\hat{\\beta}_e$ is a linear transformation of the observation vector $y_e$. Since $y_e$ is normally distributed ($y_e \\sim \\mathcal{N}(X\\beta_e, \\sigma_e^2 I_n)$), $\\hat{\\beta}_e$ is also normally distributed.\n*   **Mean:**\n    $E[\\hat{\\beta}_e] = E[(X^\\top X)^{-1} X^\\top y_e] = (X^\\top X)^{-1} X^\\top E[y_e] = (X^\\top X)^{-1} X^\\top (X\\beta_e) = \\beta_e$.\n    The estimator is unbiased.\n*   **Variance-Covariance:**\n    $\\mathrm{Var}(\\hat{\\beta}_e) = \\mathrm{Var}((X^\\top X)^{-1} X^\\top y_e) = ((X^\\top X)^{-1} X^\\top) \\mathrm{Var}(y_e) ((X^\\top X)^{-1} X^\\top)^\\top$\n    $= (X^\\top X)^{-1} X^\\top (\\sigma_e^2 I_n) X (X^\\top X)^{-1} = \\sigma_e^2 (X^\\top X)^{-1} (X^\\top X) (X^\\top X)^{-1} = \\sigma_e^2 (X^\\top X)^{-1}$.\nSo, the distribution of the estimator is $\\hat{\\beta}_e \\sim \\mathcal{N}(\\beta_e, \\sigma_e^2 (X^\\top X)^{-1})$.\n\n**3. Distribution of the Contrast Estimate $c^\\top \\hat{\\beta}_e$**\n\nThe quantity of interest, $c^\\top \\beta_e$, is estimated by $c^\\top \\hat{\\beta}_e$. As a linear combination of the components of the multivariate normal variable $\\hat{\\beta}_e$, it is also normally distributed.\n*   **Mean:** $E[c^\\top \\hat{\\beta}_e] = c^\\top E[\\hat{\\beta}_e] = c^\\top \\beta_e$.\n*   **Variance:** $\\mathrm{Var}(c^\\top \\hat{\\beta}_e) = c^\\top \\mathrm{Var}(\\hat{\\beta}_e) c = c^\\top (\\sigma_e^2 (X^\\top X)^{-1}) c = \\sigma_e^2 c^\\top (X^\\top X)^{-1} c$.\n\nUnder the null hypothesis $H_0: c^\\top \\beta_e = 0$, we have $E[c^\\top \\hat{\\beta}_e] = 0$. The distribution is:\n$$ c^\\top \\hat{\\beta}_e \\sim \\mathcal{N}(0, \\sigma_e^2 c^\\top (X^\\top X)^{-1} c) \\quad (\\text{under } H_0) $$\n\n**4. Estimation of the Error Variance $\\sigma_e^2$**\n\nThe error variance $\\sigma_e^2$ is unknown and must be estimated from the data. The standard unbiased estimator for $\\sigma_e^2$ is derived from the Residual Sum of Squares ($\\mathrm{RSS}_e$):\n$$ \\mathrm{RSS}_e = \\|y_e - X\\hat{\\beta}_e\\|_2^2 $$\nThe degrees of freedom for the error is the number of observations ($n$) minus the number of estimated parameters in $\\beta_e$ ($p$), i.e., $df = n-p$. The unbiased estimator for $\\sigma_e^2$ is:\n$$ \\hat{\\sigma}_e^2 = \\frac{\\mathrm{RSS}_e}{n - p} $$\nIt is a known result from linear model theory that $\\frac{(n-p)\\hat{\\sigma}_e^2}{\\sigma_e^2} = \\frac{\\mathrm{RSS}_e}{\\sigma_e^2}$ follows a chi-squared distribution with $n-p$ degrees of freedom, $\\chi_{n-p}^2$. Furthermore, $\\hat{\\sigma}_e^2$ is statistically independent of $\\hat{\\beta}_e$.\n\n**5. Construction of the Student's $t$-statistic**\n\nA Student's $t$-distributed random variable is formed by the ratio of a standard normal variable to the square root of an independent chi-squared variable divided by its degrees of freedom.\nLet's define a standard normal variable $Z$ under $H_0$:\n$$ Z = \\frac{c^\\top \\hat{\\beta}_e - E[c^\\top \\hat{\\beta}_e]}{\\sqrt{\\mathrm{Var}(c^\\top \\hat{\\beta}_e)}} = \\frac{c^\\top \\hat{\\beta}_e}{\\sigma_e \\sqrt{c^\\top (X^\\top X)^{-1} c}} \\sim \\mathcal{N}(0, 1) $$\nAnd an independent chi-squared variable divided by its degrees of freedom, $V$:\n$$ V = \\frac{\\hat{\\sigma}_e^2}{\\sigma_e^2} = \\frac{\\mathrm{RSS}_e / (n-p)}{\\sigma_e^2} \\sim \\frac{\\chi_{n-p}^2}{n-p} $$\nThe $t$-statistic is $t_e = Z / \\sqrt{V}$:\n$$ t_e = \\frac{ \\frac{c^\\top \\hat{\\beta}_e}{\\sigma_e \\sqrt{c^\\top (X^\\top X)^{-1} c}} }{ \\sqrt{\\frac{\\hat{\\sigma}_e^2}{\\sigma_e^2}} } = \\frac{ \\frac{c^\\top \\hat{\\beta}_e}{\\sigma_e \\sqrt{c^\\top (X^\\top X)^{-1} c}} }{ \\frac{\\hat{\\sigma}_e}{\\sigma_e} } = \\frac{c^\\top \\hat{\\beta}_e}{\\hat{\\sigma}_e \\sqrt{c^\\top (X^\\top X)^{-1} c}} $$\nThis statistic, under $H_0$, follows a Student's $t$-distribution with $n-p$ degrees of freedom.\n\n### Option-by-Option Analysis\n\n*   **Option A**: $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\hat{\\sigma}_e \\sqrt{c^\\top (X^\\top X)^{-1} c}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, $\\mathrm{RSS}_e = \\|y_e - X \\hat{\\beta}_e\\|_2^2$, $\\hat{\\sigma}_e^2 = \\dfrac{\\mathrm{RSS}_e}{n - p}$, and under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n - p$ degrees of freedom.\n\n    This option correctly states the formula for the $t$-statistic. It correctly defines the OLS estimator $\\hat{\\beta}_e$. It correctly defines the Residual Sum of Squares $\\mathrm{RSS}_e$ and the unbiased estimator for the error variance $\\hat{\\sigma}_e^2$ using the correct degrees of freedom, $n-p$. Finally, it correctly identifies the distribution of the statistic under the null hypothesis as a Student's $t$-distribution with $n-p$ degrees of freedom. All components are correct.\n    **Verdict: Correct.**\n\n*   **Option B**: $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\sqrt{\\hat{\\sigma}_e^2 \\, c^\\top (X X^\\top)^{-1} c}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, $\\mathrm{RSS}_e = \\|y_e - X \\hat{\\beta}_e\\|_2^2$, $\\hat{\\sigma}_e^2 = \\dfrac{\\mathrm{RSS}_e}{n}$, and under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n$ degrees of freedom.\n\n    This option contains multiple errors.\n    1. The denominator uses $(X X^\\top)^{-1}$ instead of $(X^\\top X)^{-1}$. $X X^\\top$ is an $n \\times n$ matrix, which is typically not invertible in the common case where $n > p$. Even if it were, the dimensions would be incorrect for multiplication with the $p$-dimensional contrast vector $c$.\n    2. The estimator for the variance is $\\hat{\\sigma}_e^2 = \\dfrac{\\mathrm{RSS}_e}{n}$. This is the biased maximum likelihood estimator, not the unbiased estimator used for the $t$-statistic.\n    3. The degrees of freedom for the $t$-distribution are stated as $n$, which is incorrect. The correct degrees of freedom are $n-p$.\n    **Verdict: Incorrect.**\n\n*   **Option C**: $t_e = \\dfrac{\\mathbf{1}^\\top \\hat{\\beta}_e}{\\hat{\\sigma} \\sqrt{\\mathbf{1}^\\top (X^\\top X)^{-1} \\mathbf{1}}}$, where $\\hat{\\beta}_e = (X^\\top X)^{-1} X^\\top y_e$, and $\\hat{\\sigma}^2$ is a pooled variance estimated by aggregating $\\mathrm{RSS}_e$ across all edges and dividing by $n$; under $H_0$ the statistic $t_e$ follows a standard normal distribution.\n\n    This option contains multiple errors.\n    1. It replaces the general contrast vector $c$ with a vector of ones, $\\mathbf{1}$. This is a specific contrast that tests $H_0: \\sum_{i=1}^p \\beta_{ei} = 0$, which is not the general hypothesis $H_0: c^\\top \\beta_e = 0$.\n    2. It uses a pooled variance $\\hat{\\sigma}^2$ estimated across all edges. The problem defines the model with an edge-specific variance $\\sigma_e^2$, so pooling is not part of the specified model.\n    3. It claims the statistic follows a standard normal distribution. For a finite sample size $n$ where the variance is estimated, the distribution is Student's $t$, not normal.\n    **Verdict: Incorrect.**\n\n*   **Option D**: $t_e = \\dfrac{c^\\top \\hat{\\beta}_e}{\\hat{s}_e}$, where $\\hat{s}_e^2 = \\dfrac{\\|y_e\\|_2^2}{n - p}$, with no dependence on $X$ in the variance estimate; under $H_0$ the statistic $t_e$ follows a Student $t$ distribution with $n - p$ degrees of freedom.\n\n    This option is incorrect because it misspecifies the standard error of the numerator. The denominator of the $t$-statistic should be the estimated standard deviation of $c^\\top \\hat{\\beta}_e$, which is $\\sqrt{\\hat{\\sigma}_e^2 c^\\top(X^\\top X)^{-1}c}$. The proposed quantity $\\hat{s}_e^2 = \\dfrac{\\|y_e\\|_2^2}{n-p}$ is an estimate of the total variance of the data $y_e$ (divided by $n-p$), not the variance of the estimated contrast. It incorrectly uses the total sum of squares $\\|y_e\\|_2^2$ instead of the residual sum of squares, and it entirely omits the crucial term $c^\\top (X^\\top X)^{-1} c$ that accounts for the design and the specific contrast being tested.\n    **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Moving from general theory to practical application, this exercise challenges you to translate a common experimental question into a precise statistical model. You will construct a design matrix for a multi-group study that includes nuisance covariates, a crucial skill for isolating effects of interest from potential confounds like age and sex. Properly specifying the model and the corresponding contrast vector is the most critical step in ensuring the validity of your hypothesis test .",
            "id": "4181134",
            "problem": "You are analyzing a functional connectome comprising $n$ subjects partitioned into three mutually exclusive groups: Group A, Group B, and Group C. For each subject $i \\in \\{1,\\dots,n\\}$ and each edge $e$ in the connectome, you observe an edge weight $y_{i}^{(e)}$. You plan to use the Network-Based Statistic (NBS) approach to test for group differences at the level of connected edge components using an edgewise General Linear Model (GLM) as the base model for constructing the test statistic at each edge.\n\nStarting from the standard General Linear Model foundation, in which for each edge $e$ the response vector $y^{(e)} \\in \\mathbb{R}^{n}$ satisfies the model\n$$\ny^{(e)} \\;=\\; X \\beta^{(e)} + \\varepsilon^{(e)},\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta^{(e)} \\in \\mathbb{R}^{p}$ is the edge-specific parameter vector, and $\\varepsilon^{(e)} \\in \\mathbb{R}^{n}$ is the error term with zero mean and finite variance, you will incorporate subject-level covariates for age and sex to control for confounding.\n\nConstruct a scientifically valid, full-rank design matrix $X$ for a three-group comparison that includes:\n- An intercept,\n- Group membership encoding for Group A and Group B using binary indicator variables with Group C as the reference category,\n- A mean-centered age covariate,\n- A sex covariate coded as $0$ for female and $1$ for male.\n\nEnsure that your construction avoids perfect multicollinearity and is suitable for edgewise GLM estimation in the context of NBS. Then, based on this $X$, specify a single contrast vector that tests the null hypothesis that the mean edge weight in Group A equals the mean edge weight in Group B, while controlling for age and sex, at each edge.\n\nYour final reported answer must be the contrast vector expressed as a single closed-form analytic expression. If you introduce any numerical constants in the construction, they must be defined symbolically and not instantiated numerically. No rounding is required. Provide the contrast vector as your final answer.",
            "solution": "The problem is valid. It is scientifically grounded in the established principles of the General Linear Model (GLM) as applied to statistical hypothesis testing in neuroscience. The problem is well-posed, objective, and contains all necessary information to construct the required design matrix and contrast vector without ambiguity or contradiction.\n\nThe objective is to construct a contrast vector for testing a specific hypothesis within the framework of a GLM. The model for the observed edge weight $y^{(e)} \\in \\mathbb{R}^n$ at a given edge $e$ is:\n$$\ny^{(e)} = X \\beta^{(e)} + \\varepsilon^{(e)}\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta^{(e)} \\in \\mathbb{R}^p$ is the vector of parameters, and $\\varepsilon^{(e)}$ is the error term.\n\nFirst, we construct the design matrix $X$ according to the specified requirements. The problem states there are three groups (A, B, C), and we need to include an intercept, group indicators for A and B with C as the reference, a mean-centered age covariate, and a sex covariate. This defines a model with $p=5$ parameters. Let us define the columns of the design matrix $X$. For each subject $i \\in \\{1, \\dots, n\\}$, the $i$-th row of $X$, denoted $x_i^T$, is constructed as follows:\n\nLet $G_{iA}$ be an indicator variable such that $G_{iA} = 1$ if subject $i$ is in Group A, and $G_{iA} = 0$ otherwise.\nLet $G_{iB}$ be an indicator variable such that $G_{iB} = 1$ if subject $i$ is in Group B, and $G_{iB} = 0$ otherwise.\nIf subject $i$ is in Group C, then $G_{iA} = 0$ and $G_{iB} = 0$, making Group C the reference category as required.\n\nLet $Age_i$ be the age of subject $i$. The problem specifies a mean-centered age covariate. Let $\\bar{A} = \\frac{1}{n} \\sum_{j=1}^n Age_j$ be the mean age across all subjects. The mean-centered age for subject $i$ is $A_i = Age_i - \\bar{A}$.\n\nLet $S_i$ be the sex covariate for subject $i$, coded as $S_i = 0$ for female and $S_i = 1$ for male.\n\nThe design matrix $X$ is an $n \\times 5$ matrix. The $i$-th row of $X$ is given by the vector $x_i^T$:\n$$\nx_i^T = \\begin{pmatrix} 1 & G_{iA} & G_{iB} & A_i & S_i \\end{pmatrix}\n$$\nThe corresponding parameter vector for edge $e$, $\\beta^{(e)}$, is a column vector in $\\mathbb{R}^5$:\n$$\n\\beta^{(e)} = \\begin{pmatrix} \\beta_0^{(e)} \\\\ \\beta_1^{(e)} \\\\ \\beta_2^{(e)} \\\\ \\beta_3^{(e)} \\\\ \\beta_4^{(e)} \\end{pmatrix}\n$$\nThe expected edge weight for subject $i$ at edge $e$, $E[y_i^{(e)}]$, is given by the model:\n$$\nE[y_i^{(e)}] = \\beta_0^{(e)} \\cdot 1 + \\beta_1^{(e)} G_{iA} + \\beta_2^{(e)} G_{iB} + \\beta_3^{(e)} A_i + \\beta_4^{(e)} S_i\n$$\nThe interpretation of the parameters (omitting the superscript $(e)$ for clarity) is as follows:\n- $\\beta_0$: The mean edge weight for a subject in the reference group (Group C), of average age ($A_i=0$), and of the reference sex (female, $S_i=0$).\n- $\\beta_1$: The difference in mean edge weight between Group A and Group C, controlling for age and sex.\n- $\\beta_2$: The difference in mean edge weight between Group B and Group C, controlling for age and sex.\n- $\\beta_3$: The effect of age on edge weight (slope).\n- $\\beta_4$: The difference in mean edge weight between males and females, controlling for group and age.\n\nNext, we formulate the null hypothesis: the mean edge weight in Group A equals the mean edge weight in Group B, controlling for age and sex. To express this in terms of the model parameters, we write the adjusted mean for each group. The adjusted mean for a group is the expected edge weight for a subject in that group with covariates set to a common reference value (e.g., average age $A_i=0$ and a specific sex $s$).\n\nThe adjusted mean for Group A, $\\mu_A$, is:\n$$\n\\mu_A = E[y^{(e)} | \\text{Group A, avg age, sex } s] = \\beta_0 + \\beta_1(1) + \\beta_2(0) + \\beta_3(0) + \\beta_4 s = \\beta_0 + \\beta_1 + \\beta_4 s\n$$\nThe adjusted mean for Group B, $\\mu_B$, is:\n$$\n\\mu_B = E[y^{(e)} | \\text{Group B, avg age, sex } s] = \\beta_0 + \\beta_1(0) + \\beta_2(1) + \\beta_3(0) + \\beta_4 s = \\beta_0 + \\beta_2 + \\beta_4 s\n$$\nThe null hypothesis, $H_0: \\mu_A = \\mu_B$, can now be written using the parameters:\n$$\nH_0: \\beta_0 + \\beta_1 + \\beta_4 s = \\beta_0 + \\beta_2 + \\beta_4 s\n$$\nThis simplifies to:\n$$\nH_0: \\beta_1 = \\beta_2\n$$\nor equivalently,\n$$\nH_0: \\beta_1 - \\beta_2 = 0\n$$\nA linear hypothesis is expressed in the form $c^T \\beta^{(e)} = 0$, where $c$ is the contrast vector. Comparing our hypothesis to this form:\n$$\nc^T \\beta^{(e)} = c_0 \\beta_0 + c_1 \\beta_1 + c_2 \\beta_2 + c_3 \\beta_3 + c_4 \\beta_4 = 0\n$$\nWe require this to be equivalent to $1 \\cdot \\beta_1 + (-1) \\cdot \\beta_2 = 0$. By matching the coefficients for each $\\beta_j$, we find the components of the contrast vector $c$:\n$c_0 = 0$\n$c_1 = 1$\n$c_2 = -1$\n$c_3 = 0$\n$c_4 = 0$\n\nThus, the contrast vector, written as a row vector, is $[0, 1, -1, 0, 0]$. This vector specifies the linear combination of parameter estimates whose sum is tested to be zero under the null hypothesis. The order of elements in the contrast vector must match the order of parameters in $\\beta^{(e)}$, which is determined by the order of columns in the design matrix $X$: intercept, Group A indicator, Group B indicator, mean-centered age, and sex.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 1 & -1 & 0 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The truest measure of understanding a complex method is the ability to implement it. This capstone exercise tasks you with building an entire NBS simulation and analysis pipeline in code, from generating synthetic connectome data with a known effect to performing permutation testing to derive a final component-level p-value. Successfully completing this practice demonstrates a comprehensive mastery of the NBS workflow and provides a powerful tool for power analysis and methods validation .",
            "id": "4181079",
            "problem": "You are to implement a complete simulation and hypothesis testing pipeline for the Network-Based Statistic (NBS) on synthetic connectome data. The simulation must construct a planted subnetwork effect and generate time series whose edge correlations are close to prescribed target values. The pipeline must produce component-level significance estimates via permutation testing and aggregate the results across a test suite.\n\nUse the following definitions as the fundamental base:\n- The Pearson correlation coefficient between two node time series $x$ and $y$ of length $T$ is $r = \\frac{\\sum_{t=1}^{T} (x_t - \\bar{x})(y_t - \\bar{y})}{\\sqrt{\\sum_{t=1}^{T} (x_t - \\bar{x})^2} \\sqrt{\\sum_{t=1}^{T} (y_t - \\bar{y})^2}}$.\n- The Fisher $z$-transform is $z = \\operatorname{atanh}(r)$, which under independent and identically distributed Gaussian sampling has approximately normal sampling distribution with variance $1/(T-3)$ for sufficiently large $T$.\n- The Welch two-sample $t$-statistic for edge-level differences between groups is $t = \\frac{\\bar{z}_A - \\bar{z}_B}{\\sqrt{s_A^2 / n_A + s_B^2 / n_B}}$ where $\\bar{z}_A$ and $\\bar{z}_B$ are sample means of Fisher $z$-transformed edge correlations for Group $A$ and Group $B$, $s_A^2$ and $s_B^2$ are the corresponding sample variances, and $n_A$ and $n_B$ are the numbers of subjects.\n- The Network-Based Statistic (NBS) forms a graph on $p$ nodes by thresholding edge-wise statistics at a primary threshold and then identifying connected components; for each component, its “mass” is the sum of suprathreshold statistics over its edges. Permutation testing of group labels yields the null distribution of the maximum component mass, from which a component-level $p$-value is obtained as $\\frac{1 + \\#\\{\\text{permutations with max mass} \\ge \\text{observed max mass}\\}}{1 + P}$ where $P$ is the number of permutations.\n\nYour program must perform the following steps for each test case:\n1. Construct two target correlation matrices for $p$ nodes, one per group. Group $B$ (control) has uniform baseline off-diagonal correlation $\\rho$ for all edges. Group $A$ (effect) has the same baseline $\\rho$ except within a planted subnetwork $S \\subset \\{1,\\dots,p\\}$ of size $s$, where off-diagonal correlations are increased by $\\delta$ to $\\rho + \\delta$. Ensure the constructed matrices are valid correlation matrices (symmetric positive definite with unit diagonal).\n2. Choose the time series length $T$ to achieve a target accuracy of edge correlations by enforcing the Fisher $z$ standard deviation bound. Specifically, for a given tolerance $\\epsilon$, choose $T$ satisfying $1/\\sqrt{T-3} \\le \\epsilon$. Use $T = \\lceil 3 + 1/\\epsilon^2 \\rceil$.\n3. Simulate, for each subject in each group, $T$ independent Gaussian time points of dimension $p$ with zero mean and covariance equal to the target correlation matrix of the group. Use a method that ensures the cross-node covariance at each time point equals the target correlation matrix (for example, Cholesky factorization).\n4. For every subject, compute the $p \\times p$ sample Pearson correlation matrix across time. Extract the upper-triangular edge correlations and apply the Fisher $z$-transform to obtain edge-wise values per subject.\n5. Compute the Welch two-sample $t$-statistic per edge comparing Group $A$ versus Group $B$ on the Fisher $z$ values. Apply a one-sided primary threshold $\\tau$ retaining edges with $t > \\tau$. Identify connected components on the $p$-node graph induced by suprathreshold edges, and compute each component’s mass as the sum of $t$-values over its suprathreshold edges. Record the observed maximum component mass $M_{\\text{obs}}$. If there are no suprathreshold edges, set $M_{\\text{obs}} = 0$.\n6. Perform $P$ permutations of group labels. For each permutation, recompute edge-wise $t$-statistics, threshold, compute the maximum component mass $M_{\\text{perm}}$, and form the empirical null distribution of $M_{\\text{perm}}$. Compute the NBS component-level $p$-value $p_{\\text{NBS}} = \\frac{1 + \\#\\{M_{\\text{perm}} \\ge M_{\\text{obs}}\\}}{1 + P}$. If $M_{\\text{obs}} = 0$, define $p_{\\text{NBS}} = 1$.\n\nImplement the above with a fixed random seed so that outputs are deterministic.\n\nTest Suite:\nProvide exactly three test cases with the following parameters, covering a general case, a lower-effect case, and a boundary threshold case:\n- Case $1$: $p = 12$, $s = 5$, subnetwork indices $S = \\{0,1,2,3,4\\}$, baseline $\\rho = 0.2$, increase $\\delta = 0.25$, group sizes $n_A = 22$, $n_B = 22$, tolerance $\\epsilon = 0.07$, primary threshold $\\tau = 3.0$, permutations $P = 200$.\n- Case $2$: $p = 10$, $s = 3$, subnetwork indices $S = \\{0,1,2\\}$, baseline $\\rho = 0.1$, increase $\\delta = 0.1$, group sizes $n_A = 20$, $n_B = 20$, tolerance $\\epsilon = 0.08$, primary threshold $\\tau = 3.5$, permutations $P = 200$.\n- Case $3$: $p = 12$, $s = 5$, subnetwork indices $S = \\{0,1,2,3,4\\}$, baseline $\\rho = 0.2$, increase $\\delta = 0.25$, group sizes $n_A = 22$, $n_B = 22$, tolerance $\\epsilon = 0.07$, primary threshold $\\tau = 9.0$, permutations $P = 200$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$). Each $result$ must be the NBS component-level $p$-value for the corresponding test case, represented as a float. No other text should be printed.",
            "solution": "The user requires an implementation of the Network-Based Statistic (NBS) pipeline for connectome hypothesis testing. This involves simulating synthetic brain connectivity data for two groups, one with a \"planted\" subnetwork effect, and then applying the NBS procedure to detect this effect and calculate its statistical significance. The process must be implemented for a specific set of test cases, and the results must be deterministic.\n\nBefore providing the solution, a validation of the problem statement is performed. The problem is found to be scientifically sound, well-posed, objective, and complete. It describes a standard, widely-used methodology in computational neuroscience (Zalesky et al., 2010) and provides all necessary mathematical definitions, algorithmic steps, and parameters. The choice of parameters in the test suite is reasonable and probes different aspects of the method's performance. Therefore, the problem is deemed valid, and a full solution is warranted.\n\nThe solution proceeds by implementing the six steps specified in the problem statement.\n\n### Step 1: Target Correlation Matrix Construction\n\nFor each group (A: effect, B: control), we construct a target $p \\times p$ correlation matrix, $\\boldsymbol{\\Sigma}$. These matrices model the desired population-level connectivity structure.\n- For the control group (Group B), the matrix $\\boldsymbol{\\Sigma}_B$ is defined with all diagonal elements equal to $1$ and all off-diagonal elements equal to a baseline correlation $\\rho$. This matrix is positive definite if and only if $-\\frac{1}{p-1} < \\rho < 1$.\n- For the effect group (Group A), the matrix $\\boldsymbol{\\Sigma}_A$ starts with the same baseline structure but has the correlation increased by an amount $\\delta$ for all pairs of nodes within a specified subnetwork $S$. The off-diagonal elements $(i, j)$ are thus:\n$$\n(\\boldsymbol{\\Sigma}_A)_{ij} = \\begin{cases}\n1 & \\text{if } i=j \\\\\n\\rho + \\delta & \\text{if } i \\neq j \\text{ and } i, j \\in S \\\\\n\\rho & \\text{if } i \\neq j \\text{ and } i \\notin S \\text{ or } j \\notin S\n\\end{cases}\n$$\nThe parameters provided in the test suite ensure that all constructed matrices are valid correlation matrices (i.e., symmetric positive definite with a unit diagonal).\n\n### Step 2: Time Series Length Determination\n\nThe length of the simulated time series, $T$, is chosen to control the precision of the sample correlation estimates. The Fisher $z$-transform of a Pearson correlation coefficient $r$, given by $z = \\operatorname{atanh}(r)$, has an approximate normal sampling distribution with a standard deviation of $1/\\sqrt{T-3}$. To ensure this standard deviation is no larger than a specified tolerance $\\epsilon$, we set a lower bound on $T$:\n$$\n\\frac{1}{\\sqrt{T-3}} \\le \\epsilon \\implies T-3 \\ge \\frac{1}{\\epsilon^2} \\implies T \\ge 3 + \\frac{1}{\\epsilon^2}\n$$\nTo satisfy this with the smallest integer $T$, we use $T = \\lceil 3 + 1/\\epsilon^2 \\rceil$.\n\n### Step 3: Synthetic Data Generation\n\nFor each of the $n_A$ and $n_B$ subjects in their respective groups, we simulate a multivariate time series of length $T$ for $p$ nodes. The data are drawn from a $p$-variate normal distribution with a mean of zero and a covariance matrix equal to the group's target correlation matrix ($\\boldsymbol{\\Sigma}_A$ or $\\boldsymbol{\\Sigma}_B$). This is achieved using the Cholesky decomposition. If $\\boldsymbol{\\Sigma} = \\boldsymbol{L}\\boldsymbol{L}^T$, where $\\boldsymbol{L}$ is a lower triangular matrix, we can generate a matrix $\\boldsymbol{Z}$ of size $T \\times p$ with entries drawn from the standard normal distribution $\\mathcal{N}(0, 1)$. The desired time series data, $\\boldsymbol{X}$, is then given by $\\boldsymbol{X} = \\boldsymbol{Z}\\boldsymbol{L}^T$. Each row of $\\boldsymbol{X}$ is a sample from $\\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})$.\n\n### Step 4: Subject-Level Edge Value Calculation\n\nFor each simulated subject, we compute the $p \\times p$ sample Pearson correlation matrix from their $T \\times p$ time series data. The correlations from the upper triangle of this matrix are extracted. To stabilize variance and normalize the distribution of these correlation values, each is transformed using the Fisher $z$-transform, $z = \\operatorname{atanh}(r)$. This results in a vector of edge-wise $z$-values for each subject.\n\n### Step 5: Observed Test Statistic Calculation\n\nTo test for a group difference at each edge, the Welch's two-sample $t$-statistic is computed on the Fisher $z$-values. For each edge, comparing Group A and Group B:\n$$\nt = \\frac{\\bar{z}_A - \\bar{z}_B}{\\sqrt{s_A^2 / n_A + s_B^2 / n_B}}\n$$\nwhere $\\bar{z}$ and $s^2$ are the sample mean and variance of the $z$-values for an edge within a group. A primary one-sided threshold $\\tau$ is applied to these $t$-statistics, retaining only edges where $t > \\tau$. These suprathreshold edges define a graph on the $p$ nodes.\n\nThe next step is to identify the connected components within this graph. For each component, its \"mass\" is calculated as the sum of the $t$-statistics of all edges that constitute that component. The maximum of these component masses is the observed NBS test statistic, $M_{\\text{obs}}$. If no edges survive the primary thresholding, $M_{\\text{obs}}$ is set to $0$.\n\n### Step 6: Permutation Testing and Significance Evaluation\n\nTo assess the significance of $M_{\\text{obs}}$, a null distribution is empirically generated via permutation testing. The group labels of all subjects are randomly permuted $P$ times. For each permutation, the entire process of calculating edge-wise $t$-statistics and finding the maximum component mass is repeated, yielding a maximum mass value $M_{\\text{perm}}$. The collection of these $P$ values forms the empirical null distribution of the maximum component mass.\n\nThe final NBS component-level $p$-value is then calculated by comparing the observed statistic $M_{\\text{obs}}$ to this null distribution:\n$$\np_{\\text{NBS}} = \\frac{1 + \\text{number of permutations where } M_{\\text{perm}} \\ge M_{\\text{obs}}}{1 + P}\n$$\nThe inclusion of '$1$' in both the numerator and denominator accounts for the observed data itself and prevents a $p$-value of $0$. If $M_{\\text{obs}} = 0$, the $p$-value is defined as $1$, since no effect was detected. This entire procedure is performed for each test case to produce the final results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse.csgraph import connected_components\nimport math\n\ndef _calculate_max_mass(z_A, z_B, p, tau):\n    \"\"\"\n    Computes t-statistics, thresholds, finds components, and returns max component mass.\n    \"\"\"\n    n_A = z_A.shape[0]\n    n_B = z_B.shape[0]\n\n    # Handle cases with zero variance for robustness\n    with np.errstate(divide='ignore', invalid='ignore'):\n        mean_A = np.mean(z_A, axis=0)\n        mean_B = np.mean(z_B, axis=0)\n        var_A = np.var(z_A, axis=0, ddof=1)\n        var_B = np.var(z_B, axis=0, ddof=1)\n        \n        # Welch's t-statistic\n        denominator = np.sqrt(var_A / n_A + var_B / n_B)\n        t_stats = np.divide(mean_A - mean_B, denominator, out=np.zeros_like(mean_A), where=denominator!=0)\n\n    # Apply primary threshold\n    suprathreshold_idx = np.where(t_stats > tau)[0]\n\n    if suprathreshold_idx.size == 0:\n        return 0.0\n\n    # Build adjacency matrix from suprathreshold edges\n    adj_matrix = np.zeros((p, p))\n    rows, cols = np.triu_indices(p, k=1)\n    \n    suprathreshold_rows = rows[suprathreshold_idx]\n    suprathreshold_cols = cols[suprathreshold_idx]\n    \n    adj_matrix[suprathreshold_rows, suprathreshold_cols] = 1\n    adj_matrix = adj_matrix + adj_matrix.T\n\n    # Find connected components\n    n_components, labels = connected_components(csgraph=adj_matrix, directed=False)\n    \n    if n_components == p: # No connections formed\n        return 0.0\n\n    # Calculate component masses\n    suprathreshold_t_stats = t_stats[suprathreshold_idx]\n    # The component label for an edge (u,v) is labels[u] (or labels[v])\n    edge_component_labels = labels[suprathreshold_rows]\n    \n    component_masses = np.bincount(edge_component_labels, weights=suprathreshold_t_stats, minlength=n_components)\n    \n    return np.max(component_masses) if component_masses.size > 0 else 0.0\n\n\ndef run_nbs_test(p, S, rho, delta, n_A, n_B, epsilon, tau, P, rng):\n    \"\"\"\n    Runs one full NBS simulation and hypothesis test.\n    \"\"\"\n    # 1. Construct target correlation matrices\n    corr_B = np.full((p, p), rho)\n    np.fill_diagonal(corr_B, 1)\n\n    corr_A = corr_B.copy()\n    subnetwork_indices = np.ix_(S, S)\n    corr_A[subnetwork_indices] = rho + delta\n    np.fill_diagonal(corr_A, 1)\n\n    # 2. Choose time series length T\n    T = math.ceil(3 + 1 / epsilon**2)\n\n    # 3. Simulate time series data and 4. Compute subject-level edge values\n    n_edges = p * (p - 1) // 2\n    z_vals_A = np.zeros((n_A, n_edges))\n    z_vals_B = np.zeros((n_B, n_edges))\n    \n    # Cholesky factors\n    L_A = np.linalg.cholesky(corr_A)\n    L_B = np.linalg.cholesky(corr_B)\n    \n    triu_indices = np.triu_indices(p, k=1)\n\n    # Group A\n    for i in range(n_A):\n        Z = rng.standard_normal(size=(T, p))\n        X = Z @ L_A.T\n        sample_corr = np.corrcoef(X, rowvar=False)\n        r_vals = sample_corr[triu_indices]\n        # Clipping to avoid inf in atanh, a practical necessity\n        r_vals = np.clip(r_vals, -1.0 + 1e-12, 1.0 - 1e-12)\n        z_vals_A[i, :] = np.arctanh(r_vals)\n\n    # Group B\n    for i in range(n_B):\n        Z = rng.standard_normal(size=(T, p))\n        X = Z @ L_B.T\n        sample_corr = np.corrcoef(X, rowvar=False)\n        r_vals = sample_corr[triu_indices]\n        r_vals = np.clip(r_vals, -1.0 + 1e-12, 1.0 - 1e-12)\n        z_vals_B[i, :] = np.arctanh(r_vals)\n\n    # 5. Compute observed maximum component mass\n    M_obs = _calculate_max_mass(z_vals_A, z_vals_B, p, tau)\n\n    if M_obs == 0:\n        return 1.0\n\n    # 6. Perform permutation testing\n    all_z_vals = np.vstack((z_vals_A, z_vals_B))\n    n_total = n_A + n_B\n    max_mass_perms = np.zeros(P)\n\n    for i in range(P):\n        perm_indices = rng.permutation(n_total)\n        perm_z_vals = all_z_vals[perm_indices]\n        \n        perm_z_A = perm_z_vals[:n_A, :]\n        perm_z_B = perm_z_vals[n_A:, :]\n        \n        max_mass_perms[i] = _calculate_max_mass(perm_z_A, perm_z_B, p, tau)\n\n    # Compute p-value\n    p_nbs = (1 + np.sum(max_mass_perms >= M_obs)) / (1 + P)\n    \n    return p_nbs\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # A fixed seed ensures the entire simulation is deterministic.\n    # We use a Random Number Generator for modern, encapsulated randomness.\n    rng = np.random.default_rng(42)\n\n    test_cases = [\n        {'p': 12, 's': 5, 'S': [0, 1, 2, 3, 4], 'rho': 0.2, 'delta': 0.25, 'n_A': 22, 'n_B': 22, 'epsilon': 0.07, 'tau': 3.0, 'P': 200},\n        {'p': 10, 's': 3, 'S': [0, 1, 2], 'rho': 0.1, 'delta': 0.1, 'n_A': 20, 'n_B': 20, 'epsilon': 0.08, 'tau': 3.5, 'P': 200},\n        {'p': 12, 's': 5, 'S': [0, 1, 2, 3, 4], 'rho': 0.2, 'delta': 0.25, 'n_A': 22, 'n_B': 22, 'epsilon': 0.07, 'tau': 9.0, 'P': 200}\n    ]\n\n    results = []\n    for case in test_cases:\n        p_val = run_nbs_test(\n            p=case['p'], S=case['S'], rho=case['rho'], delta=case['delta'],\n            n_A=case['n_A'], n_B=case['n_B'], epsilon=case['epsilon'],\n            tau=case['tau'], P=case['P'], rng=rng\n        )\n        results.append(p_val)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}