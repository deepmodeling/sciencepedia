{
    "hands_on_practices": [
        {
            "introduction": "Before we can meaningfully assess the relationship between brain regions, we must first confront the inherent noisiness of functional neuroimaging data. Raw Blood Oxygenation Level Dependent (BOLD) signals are a mixture of true neural activity and various non-neuronal fluctuations, such as head motion and physiological artifacts. This exercise walks you through the fundamental preprocessing step of nuisance regression, demonstrating how ordinary least squares is used to project the raw time series data away from these confounding signals, yielding cleaned residuals that form the basis of a valid functional connectivity analysis .",
            "id": "4147895",
            "problem": "In resting-state functional connectivity analysis, raw Blood Oxygenation Level Dependent (BOLD) time series are commonly preprocessed by regressing out nuisance signals such as head motion, cerebrospinal fluid (CSF), and white matter (WM) before computing correlations between regions. Consider a single-session dataset with $T$ time points. Let $X \\in \\mathbb{R}^{T \\times p}$ denote a nuisance design matrix whose columns span the nuisance subspace, and let $y \\in \\mathbb{R}^{T}$ denote a raw regional BOLD time series. Residualization is performed by fitting an ordinary least squares model and subtracting the fitted nuisance component from $y$. Starting from the definition of ordinary least squares as the minimizer of the sum of squared residuals and the definition of orthogonal projection onto a subspace, derive the linear operator that maps $y$ to its regression residuals with respect to the column space of $X$, and then apply it to the following concrete example for constructing functional connectivity.\n\nYou are given $T=5$ time points for two regions with raw BOLD time series $y^{(1)}$ and $y^{(2)}$:\n$$\ny^{(1)} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 5 \\\\ 7 \\end{pmatrix}, \n\\quad\ny^{(2)} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 4 \\\\ 3 \\\\ 6 \\end{pmatrix}.\n$$\nFor nuisance regression, use a two-regressor design matrix $X = [x_{0}, x_{1}] \\in \\mathbb{R}^{5 \\times 2}$ whose columns are\n$$\nx_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\quad \\text{(intercept)}, \n\\qquad\nx_{1} = \\begin{pmatrix} -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix} \\quad \\text{(a composite nuisance capturing CSF and WM physiology and head motion, standardized to zero mean)}.\n$$\nUsing only first principles of ordinary least squares and orthogonal projections, compute the Pearson correlation coefficient between the residualized time series for the two regions obtained by regressing $y^{(1)}$ and $y^{(2)}$ on the columns of $X$. Express your final answer as a single exact simplified expression with no units. Do not round.",
            "solution": "The problem is valid as it is scientifically grounded in standard statistical methods used in neuroscience data analysis (ordinary least squares, functional connectivity), is well-posed with a unique solution, and is expressed objectively with all necessary data provided.\n\nThe first step is to derive the linear operator that maps a raw time series vector $y \\in \\mathbb{R}^{T}$ to its regression residuals with respect to the column space of a design matrix $X \\in \\mathbb{R}^{T \\times p}$. The ordinary least squares (OLS) model is $y = X\\beta + e$, where $\\beta \\in \\mathbb{R}^{p}$ is the vector of coefficients and $e \\in \\mathbb{R}^{T}$ is the vector of residuals. OLS seeks to find the estimate $\\hat{\\beta}$ that minimizes the sum of squared residuals, which is the squared Euclidean norm of the residual vector $e$:\n$$S(\\beta) = \\|e\\|^2 = \\|y - X\\beta\\|^2 = (y - X\\beta)^T(y - X\\beta)$$\nExpanding this expression gives:\n$$S(\\beta) = y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta$$\nSince $\\beta^T X^T y$ is a scalar, it is equal to its transpose $(y^T X\\beta)^T$, which is $y^T X\\beta$. So, we can write:\n$$S(\\beta) = y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta$$\nTo find the minimum, we compute the gradient of $S(\\beta)$ with respect to $\\beta$ and set it to zero:\n$$\\frac{\\partial S}{\\partial \\beta} = -2X^T y + 2X^T X \\beta = 0$$\nThis yields the normal equations:\n$$X^T X \\hat{\\beta} = X^T y$$\nAssuming the columns of $X$ are linearly independent, the matrix $X^T X$ is invertible. The OLS estimate for the coefficients is then:\n$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\nThe vector of fitted values, $\\hat{y}$, is the projection of $y$ onto the column space of $X$. It is given by:\n$$\\hat{y} = X\\hat{\\beta} = X(X^T X)^{-1} X^T y$$\nThe matrix $P = X(X^T X)^{-1} X^T$ is the orthogonal projection matrix onto the column space of $X$. The regression residuals are the difference between the original data and the fitted values:\n$$e = y - \\hat{y} = y - Py = (I - P)y$$\nTherefore, the linear operator that maps $y$ to its residuals is the matrix $M = I - P = I - X(X^T X)^{-1} X^T$. This operator is itself an orthogonal projection matrix onto the subspace orthogonal to the column space of $X$.\n\nNow, we apply this to the given data. We are given $T=5$ and the design matrix $X = [x_0, x_1]$:\n$$X = \\begin{pmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix}$$\nFirst, we compute $X^T X$:\n$$X^T X = \\begin{pmatrix} 1  1  1  1  1 \\\\ -2  -1  0  1  2 \\end{pmatrix} \\begin{pmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} = \\begin{pmatrix} 5  0 \\\\ 0  10 \\end{pmatrix}$$\nThe off-diagonal elements are zero, which confirms that the columns $x_0$ and $x_1$ are orthogonal. This simplifies the calculations. The inverse is:\n$$(X^T X)^{-1} = \\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  \\frac{1}{10} \\end{pmatrix}$$\nWe need to find the residuals for $y^{(1)}$ and $y^{(2)}$. Let's denote them $e^{(1)}$ and $e^{(2)}$.\n\nFor $y^{(1)} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 5 \\\\ 7 \\end{pmatrix}$:\nWe first compute $X^T y^{(1)}$:\n$$X^T y^{(1)} = \\begin{pmatrix} 1  1  1  1  1 \\\\ -2  -1  0  1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 2 \\\\ 5 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1+3+2+5+7 \\\\ -2-3+0+5+14 \\end{pmatrix} = \\begin{pmatrix} 18 \\\\ 14 \\end{pmatrix}$$\nNow we find the coefficients $\\hat{\\beta}^{(1)}$:\n$$\\hat{\\beta}^{(1)} = (X^T X)^{-1} X^T y^{(1)} = \\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 18 \\\\ 14 \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{5} \\\\ \\frac{14}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{5} \\\\ \\frac{7}{5} \\end{pmatrix}$$\nThe fitted values are $\\hat{y}^{(1)} = X\\hat{\\beta}^{(1)}$:\n$$\\hat{y}^{(1)} = \\begin{pmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} \\frac{18}{5} \\\\ \\frac{7}{5} \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 18 - 14 \\\\ 18 - 7 \\\\ 18 - 0 \\\\ 18 + 7 \\\\ 18 + 14 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 4 \\\\ 11 \\\\ 18 \\\\ 25 \\\\ 32 \\end{pmatrix}$$\nThe residuals $e^{(1)}$ are $y^{(1)} - \\hat{y}^{(1)}$:\n$$e^{(1)} = \\frac{1}{5} \\begin{pmatrix} 5 \\\\ 15 \\\\ 10 \\\\ 25 \\\\ 35 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 4 \\\\ 11 \\\\ 18 \\\\ 25 \\\\ 32 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 1 \\\\ 4 \\\\ -8 \\\\ 0 \\\\ 3 \\end{pmatrix}$$\n\nFor $y^{(2)} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 4 \\\\ 3 \\\\ 6 \\end{pmatrix}$:\nWe compute $X^T y^{(2)}$:\n$$X^T y^{(2)} = \\begin{pmatrix} 1  1  1  1  1 \\\\ -2  -1  0  1  2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ 4 \\\\ 3 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 2+1+4+3+6 \\\\ -4-1+0+3+12 \\end{pmatrix} = \\begin{pmatrix} 16 \\\\ 10 \\end{pmatrix}$$\nThe coefficients $\\hat{\\beta}^{(2)}$ are:\n$$\\hat{\\beta}^{(2)} = (X^T X)^{-1} X^T y^{(2)} = \\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 16 \\\\ 10 \\end{pmatrix} = \\begin{pmatrix} \\frac{16}{5} \\\\ \\frac{10}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{16}{5} \\\\ 1 \\end{pmatrix}$$\nThe fitted values are $\\hat{y}^{(2)} = X\\hat{\\beta}^{(2)}$:\n$$\\hat{y}^{(2)} = \\begin{pmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} \\frac{16}{5} \\\\ 1 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 16 - 10 \\\\ 16 - 5 \\\\ 16 - 0 \\\\ 16 + 5 \\\\ 16 + 10 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 6 \\\\ 11 \\\\ 16 \\\\ 21 \\\\ 26 \\end{pmatrix}$$\nThe residuals $e^{(2)}$ are $y^{(2)} - \\hat{y}^{(2)}$:\n$$e^{(2)} = \\frac{1}{5} \\begin{pmatrix} 10 \\\\ 5 \\\\ 20 \\\\ 15 \\\\ 30 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 6 \\\\ 11 \\\\ 16 \\\\ 21 \\\\ 26 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 4 \\\\ -6 \\\\ 4 \\\\ -6 \\\\ 4 \\end{pmatrix}$$\n\nFinally, we compute the Pearson correlation coefficient, $r$, between $e^{(1)}$ and $e^{(2)}$. Since the regression included an intercept term ($x_0$), the residuals for both time series have a mean of zero. This simplifies the Pearson correlation formula to the cosine of the angle between the two vectors:\n$$r = \\frac{ \\sum_{i=1}^T e_i^{(1)} e_i^{(2)} }{ \\sqrt{\\sum_{i=1}^T (e_i^{(1)})^2} \\sqrt{\\sum_{i=1}^T (e_i^{(2)})^2} } = \\frac{e^{(1) T} e^{(2)}}{\\|e^{(1)}\\| \\|e^{(2)}\\|}$$\nWe calculate the dot product $e^{(1) T} e^{(2)}$:\n$$e^{(1) T} e^{(2)} = \\left(\\frac{1}{5}\\right)^2 \\begin{pmatrix} 1  4  -8  0  3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -6 \\\\ 4 \\\\ -6 \\\\ 4 \\end{pmatrix} = \\frac{1}{25}(1(4) + 4(-6) + (-8)(4) + 0(-6) + 3(4))$$\n$$e^{(1) T} e^{(2)} = \\frac{1}{25}(4 - 24 - 32 + 0 + 12) = \\frac{1}{25}(16 - 56) = \\frac{-40}{25} = -\\frac{8}{5}$$\nNext, we calculate the squared norms of the residual vectors:\n$$\\|e^{(1)}\\|^2 = \\left(\\frac{1}{5}\\right)^2 (1^2 + 4^2 + (-8)^2 + 0^2 + 3^2) = \\frac{1}{25}(1+16+64+0+9) = \\frac{90}{25} = \\frac{18}{5}$$\n$$\\|e^{(2)}\\|^2 = \\left(\\frac{1}{5}\\right)^2 (4^2 + (-6)^2 + 4^2 + (-6)^2 + 4^2) = \\frac{1}{25}(16+36+16+36+16) = \\frac{1}{25}(48+72) = \\frac{120}{25} = \\frac{24}{5}$$\nNow we assemble the correlation coefficient:\n$$r = \\frac{-\\frac{8}{5}}{\\sqrt{\\frac{18}{5}} \\sqrt{\\frac{24}{5}}} = \\frac{-\\frac{8}{5}}{\\sqrt{\\frac{18 \\times 24}{25}}} = \\frac{-\\frac{8}{5}}{\\frac{\\sqrt{432}}{5}} = \\frac{-8}{\\sqrt{432}}$$\nTo simplify the square root, we factor $432$: $432 = 144 \\times 3 = 12^2 \\times 3$. So, $\\sqrt{432} = 12\\sqrt{3}$.\n$$r = \\frac{-8}{12\\sqrt{3}} = \\frac{-2}{3\\sqrt{3}}$$\nRationalizing the denominator gives the final answer:\n$$r = \\frac{-2\\sqrt{3}}{3\\sqrt{3}\\sqrt{3}} = \\frac{-2\\sqrt{3}}{9}$$\nThe Pearson correlation coefficient between the residualized time series is $-\\frac{2\\sqrt{3}}{9}$.",
            "answer": "$$\\boxed{-\\frac{2\\sqrt{3}}{9}}$$"
        },
        {
            "introduction": "With cleaned time series in hand, the next critical choice is how to measure connectivity. While Pearson correlation is a common starting point, it can sometimes be deceptive, suggesting a direct link between two regions that is actually a mirage caused by a third, confounding area influencing them both. This practice uses a simple, intuitive model to demonstrate this exact pitfall and introduces partial correlation as a powerful tool to disentangle these effects, allowing you to isolate the true, direct relationship between any pair of regions .",
            "id": "4147921",
            "problem": "Consider three Regions of Interest (ROI) in a functional neuroimaging experiment, labeled $i$, $j$, and $k$. Let the latent neural signal in the confounding region $k$ be a zero-mean Gaussian random variable $X_{k}$ with variance $\\operatorname{Var}(X_{k})=1$. Let the observed signals in regions $i$ and $j$ be generated by a linear mixture of $X_{k}$ and independent additive noise, specifically\n$$\nX_{i} = 2 X_{k} + \\epsilon_{i}, \\quad X_{j} = - X_{k} + \\epsilon_{j},\n$$\nwhere $\\epsilon_{i}$ and $\\epsilon_{j}$ are zero-mean Gaussian noises, mutually independent and also independent of $X_{k}$, with $\\operatorname{Var}(\\epsilon_{i}) = 1$ and $\\operatorname{Var}(\\epsilon_{j}) = 1$. Assume all variables are jointly Gaussian and consider single-time-point population covariances.\n\nUsing first principles—namely, the definitions of covariance and correlation for jointly Gaussian variables, and the foundational relationship between conditional independence and partial correlation in multivariate normal models—derive the population-level full correlation between $X_{i}$ and $X_{j}$ and explain why it suggests a connection under a correlation-based functional connectivity matrix. Then, construct the partial correlation between $X_{i}$ and $X_{j}$ that removes the influence of the confounder $X_{k}$. Compute this partial correlation exactly.\n\nYour final answer must be the single real-valued number equal to the partial correlation between $X_{i}$ and $X_{j}$ controlling for $X_{k}$, denoted $\\rho_{ij \\cdot k}$. Express the result exactly; no rounding is required. Correlations are unitless.",
            "solution": "The problem asks for the calculation of the full correlation and the partial correlation between two observed signals, $X_i$ and $X_j$, which are influenced by a common latent signal $X_k$. The analysis will be based on first principles of statistics for jointly Gaussian variables.\n\nFirst, we establish the statistical properties of the observed signals $X_i$ and $X_j$ based on the given generative model.\nThe signals are defined as:\n$$\nX_{i} = 2 X_{k} + \\epsilon_{i}\n$$\n$$\nX_{j} = - X_{k} + \\epsilon_{j}\n$$\nAll variables $X_k$, $\\epsilon_i$, and $\\epsilon_j$ are given as zero-mean Gaussian random variables. Due to the linearity of expectation, the means of $X_i$ and $X_j$ are also zero:\n$$\nE[X_i] = E[2 X_k + \\epsilon_i] = 2 E[X_k] + E[\\epsilon_i] = 2(0) + 0 = 0\n$$\n$$\nE[X_j] = E[-X_k + \\epsilon_j] = -E[X_k] + E[\\epsilon_j] = -0 + 0 = 0\n$$\n\nThe variances of the observed signals are calculated using the property that for independent random variables $A$ and $B$, $\\operatorname{Var}(aA + bB) = a^2\\operatorname{Var}(A) + b^2\\operatorname{Var}(B)$. We are given that $X_k$ is independent of both $\\epsilon_i$ and $\\epsilon_j$.\n$$\n\\operatorname{Var}(X_i) = \\operatorname{Var}(2 X_k + \\epsilon_i) = 2^2 \\operatorname{Var}(X_k) + \\operatorname{Var}(\\epsilon_i) = 4(1) + 1 = 5\n$$\n$$\n\\operatorname{Var}(X_j) = \\operatorname{Var}(- X_k + \\epsilon_j) = (-1)^2 \\operatorname{Var}(X_k) + \\operatorname{Var}(\\epsilon_j) = 1(1) + 1 = 2\n$$\nThe standard deviations are $\\sigma_i = \\sqrt{\\operatorname{Var}(X_i)} = \\sqrt{5}$ and $\\sigma_j = \\sqrt{\\operatorname{Var}(X_j)} = \\sqrt{2}$.\n\nNext, we compute the full correlation between $X_i$ and $X_j$, denoted $\\rho_{ij}$. This requires the covariance, $\\operatorname{Cov}(X_i, X_j)$. Using the bilinearity of the covariance operator:\n$$\n\\operatorname{Cov}(X_i, X_j) = \\operatorname{Cov}(2 X_k + \\epsilon_i, -X_k + \\epsilon_j)\n$$\n$$\n= \\operatorname{Cov}(2X_k, -X_k) + \\operatorname{Cov}(2X_k, \\epsilon_j) + \\operatorname{Cov}(\\epsilon_i, -X_k) + \\operatorname{Cov}(\\epsilon_i, \\epsilon_j)\n$$\nGiven that $\\epsilon_i$, $\\epsilon_j$, and $X_k$ are mutually independent, the covariances of pairs of different variables are zero: $\\operatorname{Cov}(X_k, \\epsilon_j)=0$, $\\operatorname{Cov}(\\epsilon_i, X_k)=0$, and $\\operatorname{Cov}(\\epsilon_i, \\epsilon_j)=0$. The expression simplifies to:\n$$\n\\operatorname{Cov}(X_i, X_j) = \\operatorname{Cov}(2X_k, -X_k) = 2(-1)\\operatorname{Cov}(X_k, X_k) = -2\\operatorname{Var}(X_k)\n$$\nSince $\\operatorname{Var}(X_k)=1$, we have:\n$$\n\\operatorname{Cov}(X_i, X_j) = -2\n$$\nThe full correlation coefficient $\\rho_{ij}$ is the covariance normalized by the product of the standard deviations:\n$$\n\\rho_{ij} = \\frac{\\operatorname{Cov}(X_i, X_j)}{\\sqrt{\\operatorname{Var}(X_i)\\operatorname{Var}(X_j)}} = \\frac{-2}{\\sqrt{5 \\cdot 2}} = \\frac{-2}{\\sqrt{10}}\n$$\nThis is a non-zero, moderately strong negative correlation ($\\rho_{ij} \\approx -0.632$). In a standard functional connectivity analysis based on correlation, this finding would suggest a significant functional relationship between ROIs $i$ and $j$. However, the generative model reveals that there is no direct causal link or shared variance between $X_i$ and $X_j$ other than their common dependence on $X_k$. The observed correlation is entirely induced by this common input from the confounding region $k$.\n\nTo correctly assess the relationship between $X_i$ and $X_j$, we must account for the influence of $X_k$. In a multivariate normal model, this is precisely the purpose of the partial correlation, $\\rho_{ij \\cdot k}$. The partial correlation measures the correlation between $X_i$ and $X_j$ after removing the linear effects of $X_k$. For jointly Gaussian variables, $\\rho_{ij \\cdot k}$ is fundamentally defined as the correlation between the residuals of $X_i$ and $X_j$ after they are linearly regressed on $X_k$.\n\nLet the residual of $X_i$ after regressing on $X_k$ be $R_i = X_i - E[X_i | X_k]$.\nLet the residual of $X_j$ after regressing on $X_k$ be $R_j = X_j - E[X_j | X_k]$.\nFor the linear Gaussian model given, the conditional expectations are:\n$$\nE[X_i | X_k] = E[2X_k + \\epsilon_i | X_k] = 2E[X_k|X_k] + E[\\epsilon_i|X_k] = 2X_k + 0 = 2X_k\n$$\nThe last step follows because $\\epsilon_i$ is independent of $X_k$, so $E[\\epsilon_i|X_k] = E[\\epsilon_i] = 0$.\nSimilarly,\n$$\nE[X_j | X_k] = E[-X_k + \\epsilon_j | X_k] = -E[X_k|X_k] + E[\\epsilon_j|X_k] = -X_k + 0 = -X_k\n$$\nThe residuals are therefore:\n$$\nR_i = X_i - 2X_k = (2X_k + \\epsilon_i) - 2X_k = \\epsilon_i\n$$\n$$\nR_j = X_j - (-X_k) = (-X_k + \\epsilon_j) + X_k = \\epsilon_j\n$$\nThe partial correlation is the correlation between these residuals:\n$$\n\\rho_{ij \\cdot k} = \\operatorname{Corr}(R_i, R_j) = \\operatorname{Corr}(\\epsilon_i, \\epsilon_j)\n$$\nThe problem states that the noise terms $\\epsilon_i$ and $\\epsilon_j$ are mutually independent. For independent variables, the covariance is zero, and thus the correlation is also zero.\n$$\n\\operatorname{Cov}(\\epsilon_i, \\epsilon_j) = 0\n$$\nTherefore,\n$$\n\\rho_{ij \\cdot k} = \\frac{\\operatorname{Cov}(\\epsilon_i, \\epsilon_j)}{\\sqrt{\\operatorname{Var}(\\epsilon_i)\\operatorname{Var}(\\epsilon_j)}} = \\frac{0}{\\sqrt{1 \\cdot 1}} = 0\n$$\nThe partial correlation between $X_i$ and $X_j$ after controlling for $X_k$ is exactly zero. This result correctly reflects the underlying structure of the model: there is no direct connection between regions $i$ and $j$. Their full correlation was spurious, caused entirely by the common influence of region $k$. This demonstrates the critical importance of accounting for confounders in functional connectivity studies. The value of $\\rho_{ij \\cdot k}$ is the single real-valued number requested.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "Constructing the connectivity matrix is a major milestone, but it is not the final step. To uncover principles of brain organization, we often summarize this matrix using tools from graph theory, a step that requires a critical decision: should we analyze the full weighted network or a simplified, binarized version? This hands-on coding exercise delves into the practical consequences of this choice, guiding you to compute how fundamental network properties like clustering and path length are altered by binarization, providing crucial insight into the trade-offs that shape our interpretation of brain network topology .",
            "id": "4147918",
            "problem": "You are tasked with constructing functional connectivity matrices from synthetic multivariate time series and evaluating the impact of binarizing weighted connectivity on two canonical graph metrics: the clustering coefficient and the characteristic path length. You must produce a single, runnable program that computes the requested quantities for a specified test suite and prints the final results in the exact format described at the end. All computations must be derived from core definitions and well-tested formulas in graph theory and statistics.\n\nThe fundamental base of this problem is as follows. You will generate synthetic multivariate time series $X \\in \\mathbb{R}^{T \\times N}$ from a zero-mean multivariate Normal distribution $\\mathcal{N}(0, \\Sigma)$ whose covariance matrix $\\Sigma \\in \\mathbb{R}^{N \\times N}$ is specified for each test case. From $X$, you will compute the Pearson Correlation Coefficient (PCC) between each pair of node time series to obtain a correlation matrix $R \\in \\mathbb{R}^{N \\times N}$, where each entry $r_{ij}$ is the PCC between node $i$ and node $j$. You must construct a weighted undirected functional connectivity adjacency $W \\in \\mathbb{R}^{N \\times N}$ using $w_{ij} = |r_{ij}|$ for $i \\neq j$ and $w_{ii} = 0$ for all $i$.\n\nTo evaluate the effect of binarization, define a binary adjacency $B \\in \\{0,1\\}^{N \\times N}$ using a threshold $\\tau  0$ such that $b_{ij} = 1$ if $w_{ij} \\ge \\tau$ and $b_{ij} = 0$ otherwise, with $b_{ii} = 0$. All graphs are assumed undirected and unweighted for $B$, and undirected with positive weights for $W$.\n\nYou will compute the following graph metrics:\n- The node-wise clustering coefficient for the binary graph. The clustering coefficient at node $i$ is the ratio of the number of triangles incident to node $i$ to the number of possible triangles incident to node $i$, and the global clustering coefficient is the average over all nodes. For nodes with degree less than $2$, the clustering coefficient is defined to be $0$.\n- The node-wise weighted clustering coefficient for the weighted graph using the Onnela formulation: let $\\hat{W}$ be the weight-normalized adjacency with $\\hat{w}_{ij} = \\frac{w_{ij}}{\\max_{p,q} w_{pq}}$. The weighted clustering coefficient at node $i$ is the average of the geometric means of triangle edge weights adjacent to node $i$, normalized by the number of potential neighbor pairs, and the global coefficient is the average over all nodes. For nodes with degree less than $2$, this coefficient is defined to be $0$.\n- The characteristic path length for the binary graph, defined as the mean shortest path length over all pairs of distinct nodes that are connected, with each edge having unit length.\n- The characteristic path length for the weighted graph, defined as the mean shortest path length over all pairs of distinct nodes, where the cost of traversing an edge is $c_{ij} = \\frac{1}{w_{ij}}$ for $w_{ij}  0$. If no pair of nodes is connected under the binary graph, define the characteristic path length as $\\mathrm{NaN}$.\n\nYou will compute, for each test case, the impact of binarization on these metrics as the differences\n$$\n\\Delta C = \\bar{C}_B - \\bar{C}_W, \\quad \\Delta L = L_B - L_W,\n$$\nwhere $\\bar{C}_B$ is the global clustering coefficient of the binary graph, $\\bar{C}_W$ is the global weighted clustering coefficient of the weighted graph, $L_B$ is the binary characteristic path length, and $L_W$ is the weighted characteristic path length.\n\nAll quantities must be computed using the following core definitions:\n- The Pearson Correlation Coefficient (PCC) between two real-valued sequences $x, y \\in \\mathbb{R}^{T}$ is\n$$\nr(x,y) = \\frac{\\sum_{t=1}^{T} \\left(x_t - \\bar{x}\\right)\\left(y_t - \\bar{y}\\right)}{\\sqrt{\\sum_{t=1}^{T} \\left(x_t - \\bar{x}\\right)^2} \\sqrt{\\sum_{t=1}^{T} \\left(y_t - \\bar{y}\\right)^2}},\n$$\nwhere $\\bar{x}$ and $\\bar{y}$ are sample means.\n- The binary clustering coefficient at node $i$ is\n$$\nC_i = \\frac{\\text{number of triangles incident to node } i}{\\binom{k_i}{2}},\n$$\nfor degree $k_i \\ge 2$, and $C_i = 0$ otherwise. The global binary clustering coefficient is\n$$\n\\bar{C}_B = \\frac{1}{N} \\sum_{i=1}^{N} C_i.\n$$\n- The weighted clustering coefficient at node $i$ (Onnela) is\n$$\nC_i^w = \\frac{1}{\\binom{k_i}{2}} \\sum_{\\{j,k\\} \\subset \\mathcal{N}(i)} \\left(\\hat{w}_{ij} \\hat{w}_{ik} \\hat{w}_{jk}\\right)^{1/3},\n$$\nfor degree $k_i \\ge 2$, and $C_i^w = 0$ otherwise, where $\\mathcal{N}(i)$ is the neighbor set of node $i$ in the weighted graph, and $\\hat{w}_{ij}$ is the normalized weight. The global weighted clustering coefficient is\n$$\n\\bar{C}_W = \\frac{1}{N} \\sum_{i=1}^{N} C_i^w.\n$$\n- The characteristic path length for a graph with distance matrix $D$ is\n$$\nL = \\frac{1}{|\\{(i,j): ij, d_{ij}  \\infty\\}|} \\sum_{ij, d_{ij}  \\infty} d_{ij},\n$$\nwhere $d_{ij}$ is the shortest path distance between nodes $i$ and $j$, computed from $D$ using standard shortest-path algorithms. For the binary case, set $D_{ij} = 1$ if there is an edge and $D_{ij} = \\infty$ otherwise, with $D_{ii} = 0$. For the weighted case, set $D_{ij} = \\frac{1}{w_{ij}}$ if $w_{ij}  0$ and $D_{ij} = \\infty$ otherwise, with $D_{ii} = 0$.\n\nYou must implement shortest path computations exactly using the Floyd–Warshall algorithm over the relevant distance matrix to ensure correctness for all small graphs in the test suite.\n\nThe test suite consists of the following four parameter sets, each specifying the number of nodes $N$, the time series length $T$, the covariance matrix $\\Sigma$, the binarization threshold $\\tau$, and the random number generator seed $s$. For each case, sample $X$ from $\\mathcal{N}(0,\\Sigma)$ using the specified seed $s$.\n\n- Test Case $1$: $N=6$, $T=3000$, threshold $\\tau = 0.4$, seed $s = 123$, with covariance\n$$\n\\Sigma^{(1)} = \\begin{bmatrix}\n1  0.8  0.8  0.2  0.2  0.2 \\\\\n0.8  1  0.8  0.2  0.2  0.2 \\\\\n0.8  0.8  1  0.2  0.2  0.2 \\\\\n0.2  0.2  0.2  1  0.5  0.5 \\\\\n0.2  0.2  0.2  0.5  1  0.5 \\\\\n0.2  0.2  0.2  0.5  0.5  1\n\\end{bmatrix}.\n$$\n- Test Case $2$: $N=6$, $T=3000$, threshold $\\tau = 0.5$, seed $s = 456$, with covariance\n$$\n\\Sigma^{(2)} = \\begin{bmatrix}\n1  0.6  0.6  0.6  0.6  0.6 \\\\\n0.6  1  0.6  0.6  0.6  0.6 \\\\\n0.6  0.6  1  0.6  0.6  0.6 \\\\\n0.6  0.6  0.6  1  0.6  0.6 \\\\\n0.6  0.6  0.6  0.6  1  0.6 \\\\\n0.6  0.6  0.6  0.6  0.6  1\n\\end{bmatrix}.\n$$\n- Test Case $3$: $N=6$, $T=3000$, threshold $\\tau = 0.2$, seed $s = 789$, with covariance\n$$\n\\Sigma^{(3)} = \\begin{bmatrix}\n1  0.7  0.7  0.7  0.7  0.7 \\\\\n0.7  1  0.05  0.05  0.05  0.05 \\\\\n0.7  0.05  1  0.05  0.05  0.05 \\\\\n0.7  0.05  0.05  1  0.05  0.05 \\\\\n0.7  0.05  0.05  0.05  1  0.05 \\\\\n0.7  0.05  0.05  0.05  0.05  1\n\\end{bmatrix}.\n$$\n- Test Case $4$: $N=5$, $T=5000$, threshold $\\tau = 0.25$, seed $s = 101112$, with covariance representing a ring-like structure\n$$\n\\Sigma^{(4)} = \\begin{bmatrix}\n1  0.3  0.05  0.05  0.3 \\\\\n0.3  1  0.3  0.05  0.05 \\\\\n0.05  0.3  1  0.3  0.05 \\\\\n0.05  0.05  0.3  1  0.3 \\\\\n0.3  0.05  0.05  0.3  1\n\\end{bmatrix}.\n$$\n\nFor each test case, you must:\n- Generate $X \\sim \\mathcal{N}(0,\\Sigma^{(k)})$ for the specified $T$, $N$, and seed $s$.\n- Compute $R$, then $W$ by $w_{ij} = |r_{ij}|$ with $w_{ii} = 0$.\n- Compute $B$ with threshold $\\tau$.\n- Compute $\\bar{C}_B$, $\\bar{C}_W$, $L_B$, and $L_W$.\n- Compute $\\Delta C$ and $\\Delta L$ and round each to $6$ decimal places.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-float list in the form $[\\Delta C, \\Delta L]$ for each test case in the order listed above. For example, the output must look like\n$$\n[\\,[0.123456,-0.654321],[0.000000,0.000000],\\dots]\n$$\nwith each float rounded to $6$ decimal places. No other text may be printed. No physical units or angle units apply in this problem. Percentages are not used; all quantities are pure dimensionless floats.",
            "solution": "The problem has been analyzed and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information and definitions to proceed with a solution. The provided covariance matrices are positive-definite, ensuring that valid multivariate normal distributions can be defined. The computational tasks are clearly specified with explicit mathematical formulas and algorithmic requirements.\n\nThe solution proceeds by implementing the specified steps for each test case.\n\n**1. Generation of Synthetic Time Series**\nFor each test case, we are given the number of nodes $N$, the number of time points $T$, a covariance matrix $\\Sigma \\in \\mathbb{R}^{N \\times N}$, and a random number generator seed $s$. We generate a multivariate time series matrix $X \\in \\mathbb{R}^{T \\times N}$ by drawing $T$ samples from a zero-mean multivariate normal distribution, $X \\sim \\mathcal{N}(0, \\Sigma)$. The use of a specific seed ensures reproducibility.\n\n**2. Construction of Connectivity Matrices**\nFrom the time series data $X$, we derive two functional connectivity matrices: a weighted adjacency matrix $W$ and a binary adjacency matrix $B$.\n\n*   **Pearson Correlation Matrix ($R$)**: First, we compute the Pearson Correlation Coefficient (PCC) between every pair of time series (columns) in $X$. The PCC between two series $x, y \\in \\mathbb{R}^T$ is given by:\n    $$\n    r(x,y) = \\frac{\\sum_{t=1}^{T} (x_t - \\bar{x})(y_t - \\bar{y})}{\\sqrt{\\sum_{t=1}^{T} (x_t - \\bar{x})^2} \\sqrt{\\sum_{t=1}^{T} (y_t - \\bar{y})^2}}\n    $$\n    This results in an $N \\times N$ correlation matrix $R$, where $R_{ij} = r(X_i, X_j)$ and $X_i$ is the time series for node $i$.\n\n*   **Weighted Adjacency Matrix ($W$)**: The weighted, undirected adjacency matrix $W$ is constructed by taking the absolute value of the correlations:\n    $$\n    w_{ij} = |r_{ij}| \\quad \\text{for } i \\neq j, \\quad \\text{and} \\quad w_{ii} = 0.\n    $$\n\n*   **Binary Adjacency Matrix ($B$)**: The binary adjacency matrix $B$ is obtained by applying a threshold $\\tau$ to the weighted matrix $W$:\n    $$\n    b_{ij} = \\begin{cases} 1  \\text{if } w_{ij} \\ge \\tau \\\\ 0  \\text{otherwise} \\end{cases}, \\quad \\text{with } b_{ii} = 0.\n    $$\n\n**3. Computation of Graph Theoretic Metrics**\n\n**3.1. Clustering Coefficient**\nThe clustering coefficient measures the tendency of nodes in a graph to cluster together. We compute the global clustering coefficient for both the binary and weighted graphs.\n\n*   **Binary Global Clustering Coefficient ($\\bar{C}_B$)**: For each node $i$ in the binary graph $B$, the local clustering coefficient $C_i$ is the fraction of its neighbors that are also connected to each other.\n    $$\n    C_i = \\frac{\\text{number of triangles around node } i}{\\text{number of possible triangles around node } i} = \\frac{|\\{(j,k) \\mid b_{ij}=1, b_{ik}=1, b_{jk}=1\\}|}{\\binom{k_i}{2}}\n    $$\n    where $k_i$ is the degree of node $i$ ($k_i = \\sum_j b_{ij}$). If $k_i  2$, $C_i$ is defined as $0$. The global coefficient is the average over all nodes: $\\bar{C}_B = \\frac{1}{N} \\sum_{i=1}^{N} C_i$.\n\n*   **Weighted Global Clustering Coefficient ($\\bar{C}_W$)**: We use the Onnela formulation. First, the weight matrix $W$ is normalized by its maximum entry: $\\hat{w}_{ij} = w_{ij} / \\max_{p,q} w_{pq}$. The weighted clustering coefficient for node $i$ is:\n    $$\n    C_i^w = \\frac{1}{\\binom{k_i}{2}} \\sum_{\\{j,k\\} \\subset \\mathcal{N}(i)} \\left(\\hat{w}_{ij} \\hat{w}_{ik} \\hat{w}_{jk}\\right)^{1/3}\n    $$\n    Here, $\\mathcal{N}(i)$ is the set of neighbors of node $i$ (nodes $j$ for which $w_{ij}0$) and $k_i = |\\mathcal{N}(i)|$. If $k_i  2$, $C_i^w=0$. The global coefficient is the average: $\\bar{C}_W = \\frac{1}{N} \\sum_{i=1}^{N} C_i^w$.\n\n**3.2. Characteristic Path Length**\nThe characteristic path length is a measure of the average separation between nodes in a network. We are required to compute all-pairs shortest paths using the Floyd-Warshall algorithm.\n\n*   **Floyd-Warshall Algorithm**: Given an initial distance matrix `Dist`, where `Dist`$[i][j]$ is the direct distance (or cost) between nodes $i$ and $j$ (`infinity` if not connected), the algorithm finds all-pairs shortest paths by iteratively considering each node $k$ as an intermediate point in paths between any two nodes $i$ and $j$:\n    `for k from 1 to N: for i from 1 to N: for j from 1 to N: Dist[i][j] = min(Dist[i][j], Dist[i][k] + Dist[k][j])`\n\n*   **Binary Characteristic Path Length ($L_B$)**: We first construct a distance matrix where the length of each edge in $B$ is $1$. For any pair $(i,j)$, $d_{ij}=1$ if $b_{ij}=1$ and $d_{ij}=\\infty$ otherwise, with $d_{ii}=0$. After running the Floyd-Warshall algorithm, we obtain the matrix of shortest path lengths. $L_B$ is the mean of all finite shortest path lengths between distinct pairs of nodes. If no path exists between any pair of nodes, $L_B$ is defined as `NaN`.\n\n*   **Weighted Characteristic Path Length ($L_W$)**: The cost of traversing an edge $(i, j)$ in the weighted graph $W$ is defined as the reciprocal of its weight, $c_{ij} = 1/w_{ij}$ for $w_{ij}  0$. The initial distance matrix is populated with these costs ($d_{ij} = c_{ij}$ if $w_{ij}0$, $\\infty$ otherwise, and $d_{ii}=0$). After running the Floyd-Warshall algorithm, $L_W$ is the mean of all shortest path lengths between distinct pairs of nodes. Since the sampled correlation matrix almost surely results in a fully connected weighted graph, all pairs will have a finite path length.\n\n**4. Final Computation of Differences**\nFinally, for each test case, we compute the differences between the metrics for the binarized and weighted graphs, rounding the results to $6$ decimal places:\n$$\n\\Delta C = \\bar{C}_B - \\bar{C}_W\n$$\n$$\n\\Delta L = L_B - L_W\n$$\nThese values are formatted and printed according to the specified output structure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _floyd_warshall(dist_matrix):\n    \"\"\"Computes all-pairs shortest paths using the Floyd-Warshall algorithm.\"\"\"\n    n = dist_matrix.shape[0]\n    sp_matrix = dist_matrix.copy()\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                if sp_matrix[i, k] + sp_matrix[k, j]  sp_matrix[i, j]:\n                    sp_matrix[i, j] = sp_matrix[i, k] + sp_matrix[k, j]\n    return sp_matrix\n\ndef calculate_global_clustering(adj_matrix, is_weighted):\n    \"\"\"\n    Computes the global clustering coefficient for a given adjacency matrix.\n    Handles both binary and weighted (Onnela) cases.\n    \"\"\"\n    n = adj_matrix.shape[0]\n    node_coeffs = []\n    \n    if is_weighted:\n        max_w = np.max(adj_matrix)\n        if max_w == 0:\n            return 0.0\n        w_hat = adj_matrix / max_w\n    \n    for i in range(n):\n        if is_weighted:\n            neighbors = np.where(adj_matrix[i] > 0)[0]\n        else:\n            neighbors = np.where(adj_matrix[i] == 1)[0]\n        \n        k_i = len(neighbors)\n        \n        if k_i  2:\n            node_coeffs.append(0.0)\n            continue\n            \n        num_possible_triangles = k_i * (k_i - 1) / 2.0\n        \n        if is_weighted:\n            triangle_sum = 0.0\n            for j_idx in range(k_i):\n                for k_idx in range(j_idx + 1, k_i):\n                    j, k = neighbors[j_idx], neighbors[k_idx]\n                    triangle_sum += (w_hat[i, j] * w_hat[i, k] * w_hat[j, k])**(1.0/3.0)\n            c_i = triangle_sum / num_possible_triangles if num_possible_triangles > 0 else 0.0\n        else:\n            num_actual_triangles = 0\n            for j_idx in range(k_i):\n                for k_idx in range(j_idx + 1, k_i):\n                    j, k = neighbors[j_idx], neighbors[k_idx]\n                    if adj_matrix[j, k] == 1:\n                        num_actual_triangles += 1\n            c_i = num_actual_triangles / num_possible_triangles if num_possible_triangles > 0 else 0.0\n        \n        node_coeffs.append(c_i)\n        \n    return np.mean(node_coeffs)\n\ndef calculate_char_path_length(adj_matrix, is_weighted):\n    \"\"\"\n    Computes the characteristic path length for a given adjacency matrix.\n    Handles both binary and weighted cases.\n    \"\"\"\n    n = adj_matrix.shape[0]\n    dist_matrix = np.full((n, n), np.inf)\n    \n    if is_weighted:\n        non_zero_edges = adj_matrix > 0\n        dist_matrix[non_zero_edges] = 1.0 / adj_matrix[non_zero_edges]\n    else:\n        edges = adj_matrix == 1\n        dist_matrix[edges] = 1.0\n        \n    np.fill_diagonal(dist_matrix, 0)\n    \n    sp_matrix = _floyd_warshall(dist_matrix)\n    \n    # Extract unique paths from the upper triangle\n    paths = sp_matrix[np.triu_indices(n, k=1)]\n    \n    if is_weighted:\n        # Weighted graph from correlation is almost always fully connected\n        return np.mean(paths)\n    else:\n        # For binary graph, only consider connected pairs\n        connected_paths = paths[np.isfinite(paths)]\n        if len(connected_paths) == 0:\n            return np.nan\n        else:\n            return np.mean(connected_paths)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute results.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 6, \"T\": 3000, \"tau\": 0.4, \"seed\": 123,\n            \"Sigma\": np.array([\n                [1, 0.8, 0.8, 0.2, 0.2, 0.2],\n                [0.8, 1, 0.8, 0.2, 0.2, 0.2],\n                [0.8, 0.8, 1, 0.2, 0.2, 0.2],\n                [0.2, 0.2, 0.2, 1, 0.5, 0.5],\n                [0.2, 0.2, 0.2, 0.5, 1, 0.5],\n                [0.2, 0.2, 0.2, 0.5, 0.5, 1]\n            ])\n        },\n        {\n            \"N\": 6, \"T\": 3000, \"tau\": 0.5, \"seed\": 456,\n            \"Sigma\": np.array([\n                [1, 0.6, 0.6, 0.6, 0.6, 0.6],\n                [0.6, 1, 0.6, 0.6, 0.6, 0.6],\n                [0.6, 0.6, 1, 0.6, 0.6, 0.6],\n                [0.6, 0.6, 0.6, 1, 0.6, 0.6],\n                [0.6, 0.6, 0.6, 0.6, 1, 0.6],\n                [0.6, 0.6, 0.6, 0.6, 0.6, 1]\n            ])\n        },\n        {\n            \"N\": 6, \"T\": 3000, \"tau\": 0.2, \"seed\": 789,\n            \"Sigma\": np.array([\n                [1, 0.7, 0.7, 0.7, 0.7, 0.7],\n                [0.7, 1, 0.05, 0.05, 0.05, 0.05],\n                [0.7, 0.05, 1, 0.05, 0.05, 0.05],\n                [0.7, 0.05, 0.05, 1, 0.05, 0.05],\n                [0.7, 0.05, 0.05, 0.05, 1, 0.05],\n                [0.7, 0.05, 0.05, 0.05, 0.05, 1]\n            ])\n        },\n        {\n            \"N\": 5, \"T\": 5000, \"tau\": 0.25, \"seed\": 101112,\n            \"Sigma\": np.array([\n                [1, 0.3, 0.05, 0.05, 0.3],\n                [0.3, 1, 0.3, 0.05, 0.05],\n                [0.05, 0.3, 1, 0.3, 0.05],\n                [0.05, 0.05, 0.3, 1, 0.3],\n                [0.3, 0.05, 0.05, 0.3, 1]\n            ])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N, T, Sigma, tau, seed = case[\"N\"], case[\"T\"], case[\"Sigma\"], case[\"tau\"], case[\"seed\"]\n\n        rng = np.random.default_rng(seed)\n        mean = np.zeros(N)\n        X = rng.multivariate_normal(mean, Sigma, size=T, check_valid='warn')\n\n        R = np.corrcoef(X, rowvar=False)\n        W = np.abs(R)\n        np.fill_diagonal(W, 0)\n        B = (W >= tau).astype(int)\n\n        C_B = calculate_global_clustering(B, is_weighted=False)\n        C_W = calculate_global_clustering(W, is_weighted=True)\n        L_B = calculate_char_path_length(B, is_weighted=False)\n        L_W = calculate_char_path_length(W, is_weighted=True)\n        \n        delta_C = C_B - C_W\n        delta_L = L_B - L_W\n        \n        all_results.append([delta_C, delta_L])\n\n    formatted_results = []\n    for dc, dl in all_results:\n        dc_str = f\"{dc:.6f}\"\n        dl_str = \"nan\" if np.isnan(dl) else f\"{dl:.6f}\"\n        formatted_results.append(f\"[{dc_str},{dl_str}]\")\n    \n    print(f\"[[{','.join(formatted_results)}]]\")\n\nsolve()\n```"
        }
    ]
}