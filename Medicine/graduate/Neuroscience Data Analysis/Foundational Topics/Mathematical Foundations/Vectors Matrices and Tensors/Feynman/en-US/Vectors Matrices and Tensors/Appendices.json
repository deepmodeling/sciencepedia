{
    "hands_on_practices": [
        {
            "introduction": "A central task in systems neuroscience is comparing how populations of neurons represent different stimuli or conditions. This exercise explores the fundamental vector properties used to quantify such comparisons. By calculating vector norms and the angle between two hypothetical neural activity vectors, you will develop an intuition for when to use measures of magnitude versus measures of directional similarity, a crucial distinction for interpreting the geometry of neural representations. ",
            "id": "4203703",
            "problem": "A neural population response is summarized by a feature vector where each component is a standardized (z-scored) activity measure, making the entries dimensionless. Consider two trial-averaged population activity vectors across three matched epochs, $x=(3,0,4)$ and $y=(2,-2,2)$, representing two stimulus conditions recorded from the same set of neurons. Using only the core definitions of vector norms, inner products, and angles in $\\mathbb{R}^n$, compute the following quantities: the Euclidean norm $\\|x\\|_2$, the Manhattan norm $\\|x\\|_1$, and the angle $\\theta=\\arccos\\left(\\frac{x^\\top y}{\\|x\\|_2\\|y\\|_2}\\right)$ between $x$ and $y$. Then, based on the mathematical properties of these measures, articulate which measure better captures similarity for correlated neural activity when overall response magnitude may differ across conditions, and explain why.\n\nReport the numeric tuple $(\\|x\\|_2,\\|x\\|_1,\\theta)$ exactly (do not round). Express the angle $\\theta$ in radians. The entries of $x$ and $y$ are standardized and therefore dimensionless, so no physical units are required for the norms.",
            "solution": "**Part 1: Calculation of the Required Quantities**\n\nFirst, we compute the Euclidean norm (or $L_2$-norm) of the vector $x$. The definition of the Euclidean norm for a vector $v = (v_1, v_2, \\dots, v_n)$ in $\\mathbb{R}^n$ is $\\|v\\|_2 = \\sqrt{\\sum_{i=1}^n v_i^2}$.\nFor the vector $x = (3, 0, 4)$, the Euclidean norm is:\n$$\n\\|x\\|_2 = \\sqrt{3^2 + 0^2 + 4^2} = \\sqrt{9 + 0 + 16} = \\sqrt{25} = 5\n$$\n\nNext, we compute the Manhattan norm (or $L_1$-norm) of the vector $x$. The definition of the Manhattan norm for a vector $v$ is $\\|v\\|_1 = \\sum_{i=1}^n |v_i|$.\nFor the vector $x = (3, 0, 4)$, the Manhattan norm is:\n$$\n\\|x\\|_1 = |3| + |0| + |4| = 3 + 0 + 4 = 7\n$$\n\nFinally, we compute the angle $\\theta$ between vectors $x$ and $y$. The formula is given as $\\theta = \\arccos\\left(\\frac{x^\\top y}{\\|x\\|_2\\|y\\|_2}\\right)$. We need to calculate the inner product $x^\\top y$ and the Euclidean norm of $y$, $\\|y\\|_2$.\nThe inner product (or dot product) of $x=(3, 0, 4)$ and $y=(2, -2, 2)$ is:\n$$\nx^\\top y = (3)(2) + (0)(-2) + (4)(2) = 6 + 0 + 8 = 14\n$$\nThe Euclidean norm of $y=(2, -2, 2)$ is:\n$$\n\\|y\\|_2 = \\sqrt{2^2 + (-2)^2 + 2^2} = \\sqrt{4 + 4 + 4} = \\sqrt{12} = \\sqrt{4 \\times 3} = 2\\sqrt{3}\n$$\nNow we can compute the argument of the $\\arccos$ function, which is the cosine of the angle between the vectors (also known as cosine similarity):\n$$\n\\frac{x^\\top y}{\\|x\\|_2\\|y\\|_2} = \\frac{14}{5 \\times 2\\sqrt{3}} = \\frac{14}{10\\sqrt{3}} = \\frac{7}{5\\sqrt{3}}\n$$\nFor a canonical representation, we can rationalize the denominator:\n$$\n\\frac{7}{5\\sqrt{3}} \\times \\frac{\\sqrt{3}}{\\sqrt{3}} = \\frac{7\\sqrt{3}}{5 \\times 3} = \\frac{7\\sqrt{3}}{15}\n$$\nThus, the angle $\\theta$ in radians is:\n$$\n\\theta = \\arccos\\left(\\frac{7\\sqrt{3}}{15}\\right)\n$$\n\nThe required numerical tuple is $(\\|x\\|_2, \\|x\\|_1, \\theta) = (5, 7, \\arccos(\\frac{7\\sqrt{3}}{15}))$.\n\n**Part 2: Conceptual Analysis**\n\nThe problem asks which measure, a norm or the angle, better captures similarity for correlated neural activity when the overall response magnitude may differ across conditions.\n\nVector norms, such as the Euclidean norm $\\|x\\|_2$ or the Manhattan norm $\\|x\\|_1$, are measures of a vector's magnitude or \"length\". In the context of neural activity, the norm quantifies the overall strength or intensity of the population response. If two response vectors have very different norms, it means the overall level of neural activity is different between the two conditions. Therefore, norms are inherently sensitive to response magnitude.\n\nThe angle $\\theta$ between two vectors, however, is a measure of their orientation relative to each other. It is derived from the cosine similarity, $\\cos(\\theta) = \\frac{x^\\top y}{\\|x\\|_2\\|y\\|_2}$. The key feature of this expression is the normalization by the product of the vector norms in the denominator. This normalization makes the measure invariant to the magnitudes (lengths) of the vectors. The angle exclusively captures the similarity in the *pattern* of the response across the neural population. A small angle $\\theta$ (corresponding to a cosine similarity close to $1$) indicates that the vectors point in nearly the same direction. In neuroscience terms, this means the relative pattern of activation across the neurons is very similar, even if the absolute level of activation is different. For instance, if vector $y$ were a scaled version of vector $x$, say $y=c x$ for some scalar $c > 0$, the angle between them would be $\\theta = 0$, indicating a perfect match in the response pattern, while their norms would differ by the factor $c$.\n\nGiven the scenario of \"correlated neural activity\" (implying a similar pattern) where \"overall response magnitude may differ,\" the **angle $\\theta$** is the superior measure of similarity. It isolates the representational geometry (the shape of the activity pattern) from the overall gain or excitability of the neural population. Using a distance metric like Euclidean distance, $\\|x-y\\|_2$, would confound differences in pattern with differences in an overall gain factor, making it difficult to determine if two stimuli are encoded differently or if one is simply a more intense version of the other. The angle, by being magnitude-invariant, directly addresses this issue and provides a pure measure of pattern similarity.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 5 & 7 & \\arccos\\left(\\frac{7\\sqrt{3}}{15}\\right) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "General Linear Models (GLMs) are a cornerstone of neuroimaging data analysis, but their validity hinges on the properties of the design matrix. This practice invites you to analyze a simple design matrix to uncover linear dependencies, or multicollinearity, among its regressors. By determining the rank, column space, and null space, you will see how abstract linear algebra provides a powerful diagnostic tool for assessing model identifiability and the redundancy of experimental predictors. ",
            "id": "4203709",
            "problem": "In a general linear model (GLM) for calcium imaging analysis, you construct a design matrix $X \\in \\mathbb{R}^{3 \\times 3}$ with $3$ regressors sampled at $3$ time points. Each column corresponds to a regressor encoding a hypothesized component of neural activity, and each row corresponds to a time sample. You are given\n$$\nX=\\begin{bmatrix}\n1 & 2 & 3\\\\\n2 & 4 & 6\\\\\n1 & 1 & 1\n\\end{bmatrix}.\n$$\nUsing only core linear algebra definitions and operations valid for finite-dimensional vector spaces, determine the following:\n1) The column space of $X$ as a subspace of $\\mathbb{R}^{3}$, specified by a basis consisting of columns of $X$.\n2) The rank of $X$.\n3) The null space of $X$, specified by a basis.\nThen, interpret the redundancy of the regressors in the context of identifiability of the GLM, stating how many regressors are linearly redundant and what explicit linear relation among the columns of $X$ demonstrates this redundancy.\n\nReport as your final answer the number of redundant regressors implied by $X$. No rounding is required. The final answer must be a single real number.",
            "solution": "The problem requires an analysis of the provided design matrix $X$ to determine its properties and interpret them in the context of a general linear model (GLM). The analysis must adhere to core principles of linear algebra.\n\nThe given design matrix is:\n$$\nX = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n1 & 1 & 1\n\\end{bmatrix}\n$$\nLet the columns of $X$ be denoted as vectors $\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3 \\in \\mathbb{R}^3$:\n$$\n\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{v}_2 = \\begin{pmatrix} 2 \\\\ 4 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{v}_3 = \\begin{pmatrix} 3 \\\\ 6 \\\\ 1 \\end{pmatrix}\n$$\nThese vectors represent the three regressors in the GLM.\n\nFirst, we will establish the linear relationship between these column vectors. We seek coefficients $c_1, c_2, c_3$ not all zero, such that $c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + c_3\\mathbf{v}_3 = \\mathbf{0}$. This corresponds to solving the homogeneous system of linear equations $X\\mathbf{c} = \\mathbf{0}$, where $\\mathbf{c} = (c_1, c_2, c_3)^T$. The augmented matrix for this system is:\n$$\n\\left[\\begin{array}{ccc|c}\n1 & 2 & 3 & 0 \\\\\n2 & 4 & 6 & 0 \\\\\n1 & 1 & 1 & 0\n\\end{array}\\right]\n$$\nWe perform Gaussian elimination.\nSubtract $2$ times the first row from the second row ($R_2 \\to R_2 - 2R_1$):\n$$\n\\left[\\begin{array}{ccc|c}\n1 & 2 & 3 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n1 & 1 & 1 & 0\n\\end{array}\\right]\n$$\nSubtract the first row from the third row ($R_3 \\to R_3 - R_1$):\n$$\n\\left[\\begin{array}{ccc|c}\n1 & 2 & 3 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & -1 & -2 & 0\n\\end{array}\\right]\n$$\nSwap the second and third rows ($R_2 \\leftrightarrow R_3$) and multiply the new second row by $-1$ ($R_2 \\to -R_2$):\n$$\n\\left[\\begin{array}{ccc|c}\n1 & 2 & 3 & 0 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right]\n$$\nFinally, subtract $2$ times the new second row from the first row ($R_1 \\to R_1 - 2R_2$):\n$$\n\\left[\\begin{array}{ccc|c}\n1 & 0 & -1 & 0 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right]\n$$\nThis is the reduced row echelon form. The system of equations is:\n$c_1 - c_3 = 0 \\implies c_1 = c_3$\n$c_2 + 2c_3 = 0 \\implies c_2 = -2c_3$\nThe variable $c_3$ is a free variable. Let $c_3 = k$ for any scalar $k \\in \\mathbb{R}$. The solution vector is $\\mathbf{c} = (k, -2k, k)^T = k(1, -2, 1)^T$.\nChoosing a non-zero value, for instance $k=1$, gives a non-trivial linear combination that equals the zero vector:\n$$ 1\\mathbf{v}_1 - 2\\mathbf{v}_2 + 1\\mathbf{v}_3 = \\mathbf{0} $$\nThis is the explicit linear relation among the columns of $X$, demonstrating their linear dependence.\n\nWith this relation established, we can address the specific questions.\n\n1) The column space of $X$, denoted $C(X)$, is the subspace of $\\mathbb{R}^3$ spanned by its column vectors: $C(X) = \\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}$. Since the vectors are linearly dependent, we can remove one of them from the spanning set without changing the subspace. From the relation $\\mathbf{v}_3 = 2\\mathbf{v}_2 - \\mathbf{v}_1$, we see that $\\mathbf{v}_3$ is a linear combination of $\\mathbf{v}_1$ and $\\mathbf{v}_2$. Therefore, $C(X) = \\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2\\}$. To form a basis, we must check if $\\mathbf{v}_1$ and $\\mathbf{v}_2$ are linearly independent. They are independent because neither is a scalar multiple of the other (if $\\mathbf{v}_2 = c\\mathbf{v}_1$, then $2=c \\cdot 1$ and $1=c \\cdot 1$, which implies $c=2$ and $c=1$, a contradiction). Thus, a basis for the column space of $X$ is $\\{\\mathbf{v}_1, \\mathbf{v}_2\\}$.\nA basis for $C(X)$ is $\\left\\{ \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 4 \\\\ 1 \\end{pmatrix} \\right\\}$.\n\n2) The rank of a matrix, $\\text{rank}(X)$, is defined as the dimension of its column space. Since we found a basis for the column space consisting of two vectors, the dimension is $2$.\nTherefore, $\\text{rank}(X) = 2$.\nThis is also confirmed by the row echelon form of $X$, which has $2$ non-zero rows.\n\n3) The null space of $X$, denoted $N(X)$, is the set of all vectors $\\mathbf{c}$ such that $X\\mathbf{c} = \\mathbf{0}$. We found the general solution to this system to be $\\mathbf{c} = k(1, -2, 1)^T$ for any scalar $k$.\nThe null space is the subspace spanned by the vector $(1, -2, 1)^T$.\nA basis for $N(X)$ is $\\left\\{ \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix} \\right\\}$.\nThe dimension of the null space, or nullity, is $1$. This is consistent with the rank-nullity theorem: $\\text{rank}(X) + \\text{nullity}(X) = 2 + 1 = 3$, which is the number of columns of $X$.\n\n4) In a GLM, the regressors (columns of $X$) are considered redundant if they are not linearly independent. A lack of linear independence, known as multicollinearity, implies that the model is over-parameterized. The contribution of each regressor to the observed data cannot be uniquely determined. The number of linearly independent regressors is given by the rank of the matrix $X$, which is $2$. The total number of regressors is $3$.\nThe number of linearly redundant regressors is the total number of regressors minus the number of linearly independent ones.\nNumber of redundant regressors = (Number of columns) - $\\text{rank}(X) = 3 - 2 = 1$.\nThis is also equal to the nullity of the matrix. The single vector in the basis of the null space, $(1, -2, 1)^T$, provides the coefficients of the linear relation $1\\mathbf{v}_1 - 2\\mathbf{v}_2 + 1\\mathbf{v}_3 = \\mathbf{0}$, which explicitly demonstrates how one regressor's effect is perfectly accounted for by a combination of the others (e.g., $\\mathbf{v}_3 = 2\\mathbf{v}_2 - \\mathbf{v}_1$). This redundancy means that the regression coefficients $\\beta_1, \\beta_2, \\beta_3$ in the model $\\mathbf{y} = X\\mathbf{\\beta}$ cannot be uniquely estimated. There is one redundant regressor in the set.\n\nThe final answer requested is the number of redundant regressors implied by $X$. This value is $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "As we move towards more complex models of neural dynamics, we often need to handle transformations involving multiple matrices, a stepping stone to tensor algebra. This exercise focuses on the Kronecker product and its deep connection to the vectorization of matrix expressions, a common operation in frameworks like Dynamic Causal Modelling (DCM). By working through the relationship between $\\operatorname{vec}(AXB)$ and $(B^T \\otimes A)\\operatorname{vec}(X)$, you will gain hands-on experience with the algebraic machinery that linearizes higher-order interactions. ",
            "id": "4203712",
            "problem": "In a two-source cortical estimation scenario using a Multi-Electrode Array (MEA), suppose a linear mixing model relates latent source activity to electrode observations through a bilinear transformation. Let $A \\in \\mathbb{R}^{2 \\times 2}$ encode a local connectivity transform between two latent sources and let $B \\in \\mathbb{R}^{2 \\times 2}$ encode a sensor-space mixing transform. A common task in General Linear Model (GLM) and Dynamic Causal Modelling (DCM) pipelines is to reshape second-order statistics (for example, trial-averaged cross-covariances) into vectors and apply structured linear operators, which are formed from basic tensor operations.\n\nGiven the specific matrices\n$$\nA = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 & 0 \\\\ 3 & 4 \\end{bmatrix},\n$$\nperform the following:\n\n1. Using only the fundamental definition of the Kronecker product that assembles a block matrix from scalar-matrix products, compute the Kronecker product $A \\otimes B$ explicitly.\n\n2. Let $X \\in \\mathbb{R}^{2 \\times 2}$ be an arbitrary cross-covariance matrix written as\n$$\nX = \\begin{bmatrix} x_{1} & x_{2} \\\\ x_{3} & x_{4} \\end{bmatrix}.\n$$\nUsing only the definitions of standard matrix multiplication and column-stacking vectorization, where $\\operatorname{vec}(X)$ is formed by stacking the columns of $X$ into a single column vector, derive the linear operator $M \\in \\mathbb{R}^{4 \\times 4}$ such that\n$$\n\\operatorname{vec}(A X B) = M \\, \\operatorname{vec}(X),\n$$\nand compute this $M$ explicitly for the given $A$ and $B$. Then, compare $M$ to the Kronecker product you computed in part $1$ to determine the relationship between these operators and explain why your relationship holds from first principles of the definitions you used.\n\nExpress your final answer as the explicit $4 \\times 4$ matrix for $A \\otimes B$. No rounding is required, and no physical units are involved.",
            "solution": "The solution is presented in three parts as requested by the problem statement.\n\n**Part 1: Computation of the Kronecker Product $A \\otimes B$**\n\nThe Kronecker product $A \\otimes B$ of a matrix $A = [a_{ij}] \\in \\mathbb{R}^{m \\times n}$ and a matrix $B \\in \\mathbb{R}^{p \\times q}$ is a block matrix in $\\mathbb{R}^{mp \\times nq}$ defined as:\n$$\nA \\otimes B = \\begin{bmatrix} a_{11}B & a_{12}B & \\cdots & a_{1n}B \\\\ a_{21}B & a_{22}B & \\cdots & a_{2n}B \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1}B & a_{m2}B & \\cdots & a_{mn}B \\end{bmatrix}\n$$\nThe given matrices are $A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}$ and $B = \\begin{bmatrix} 1 & 0 \\\\ 3 & 4 \\end{bmatrix}$. Here, $a_{11}=1$, $a_{12}=2$, $a_{21}=0$, and $a_{22}=1$. Applying the definition:\n$$\nA \\otimes B = \\begin{bmatrix}\na_{11}B & a_{12}B \\\\\na_{21}B & a_{22}B\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 \\cdot B & 2 \\cdot B \\\\\n0 \\cdot B & 1 \\cdot B\n\\end{bmatrix}\n$$\nWe compute the scalar-matrix products:\n$1 \\cdot B = \\begin{bmatrix} 1 & 0 \\\\ 3 & 4 \\end{bmatrix}$\n$2 \\cdot B = \\begin{bmatrix} 2(1) & 2(0) \\\\ 2(3) & 2(4) \\end{bmatrix} = \\begin{bmatrix} 2 & 0 \\\\ 6 & 8 \\end{bmatrix}$\n$0 \\cdot B = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$\nSubstituting these blocks into the expression for $A \\otimes B$:\n$$\nA \\otimes B = \\begin{bmatrix}\n1 & 0 & 2 & 0 \\\\\n3 & 4 & 6 & 8 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 3 & 4\n\\end{bmatrix}\n$$\n\n**Part 2: Derivation and Computation of the Operator $M$**\n\nWe seek the matrix $M$ that satisfies the identity $\\operatorname{vec}(A X B) = M \\, \\operatorname{vec}(X)$. We will first derive the general form of $M$ from first principles and then compute it for the specific matrices $A$ and $B$.\n\nLet $A \\in \\mathbb{R}^{m \\times n}$, $X \\in \\mathbb{R}^{n \\times p}$, and $B \\in \\mathbb{R}^{p \\times q}$. We can represent $X$ and $B$ by their columns:\n$$\nX = \\begin{bmatrix} \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_p \\end{bmatrix} \\quad \\text{and} \\quad B = \\begin{bmatrix} b_{11} & b_{12} & \\cdots & b_{1q} \\\\ b_{21} & b_{22} & \\cdots & b_{2q} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{p1} & b_{p2} & \\cdots & b_{pq} \\end{bmatrix}\n$$\nwhere each $\\mathbf{x}_j \\in \\mathbb{R}^n$. The $k$-th column of the product $XB$ is a linear combination of the columns of $X$:\n$$\n(XB)_{:k} = X \\mathbf{b}_k = \\sum_{j=1}^p b_{jk} \\mathbf{x}_j\n$$\nwhere $\\mathbf{b}_k$ is the $k$-th column of $B$. Now, we multiply by $A$ on the left. Since matrix multiplication is a linear operator, we have for the $k$-th column of $AXB$:\n$$\n(AXB)_{:k} = A (XB)_{:k} = A \\left( \\sum_{j=1}^p b_{jk} \\mathbf{x}_j \\right) = \\sum_{j=1}^p b_{jk} (A \\mathbf{x}_j)\n$$\nThe vectorization operator $\\operatorname{vec}(AXB)$ is formed by stacking the columns of $AXB$:\n$$\n\\operatorname{vec}(AXB) = \\begin{pmatrix} (AXB)_{:1} \\\\ (AXB)_{:2} \\\\ \\vdots \\\\ (AXB)_{:q} \\end{pmatrix} = \\begin{pmatrix} \\sum_{j=1}^p b_{j1} (A \\mathbf{x}_j) \\\\ \\sum_{j=1}^p b_{j2} (A \\mathbf{x}_j) \\\\ \\vdots \\\\ \\sum_{j=1}^p b_{jq} (A \\mathbf{x}_j) \\end{pmatrix}\n$$\nThis is a block matrix-vector product. We can write it as:\n$$\n\\operatorname{vec}(AXB) = \\begin{bmatrix}\nb_{11}A & b_{21}A & \\cdots & b_{p1}A \\\\\nb_{12}A & b_{22}A & \\cdots & b_{p2}A \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{1q}A & b_{2q}A & \\cdots & b_{pq}A\n\\end{bmatrix}\n\\begin{pmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_p \\end{pmatrix}\n$$\nThe vector on the right is $\\operatorname{vec}(X)$. The matrix on the left has the block structure of a Kronecker product. Its blocks are $c_{kj}A$, where $c_{kj}=b_{jk}$. The matrix $C=[c_{kj}]$ is the transpose of $B$, i.e., $C = B^T$. Thus, the block matrix is precisely $B^T \\otimes A$.\nThis establishes the general identity from first principles:\n$$\n\\operatorname{vec}(AXB) = (B^T \\otimes A) \\operatorname{vec}(X)\n$$\nTherefore, the operator $M$ is $M = B^T \\otimes A$.\n\nNow, we compute $M$ for the given matrices $A$ and $B$.\nFirst, we find the transpose of $B$:\n$$\nB = \\begin{bmatrix} 1 & 0 \\\\ 3 & 4 \\end{bmatrix} \\implies B^T = \\begin{bmatrix} 1 & 3 \\\\ 0 & 4 \\end{bmatrix}\n$$\nNext, we compute the Kronecker product $M = B^T \\otimes A$:\n$$\nM = B^T \\otimes A = \\begin{bmatrix} 1 & 3 \\\\ 0 & 4 \\end{bmatrix} \\otimes A = \\begin{bmatrix} 1 \\cdot A & 3 \\cdot A \\\\ 0 \\cdot A & 4 \\cdot A \\end{bmatrix}\n$$\nWe have $A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}$. The required blocks are:\n$1 \\cdot A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}$\n$3 \\cdot A = \\begin{bmatrix} 3 & 6 \\\\ 0 & 3 \\end{bmatrix}$\n$0 \\cdot A = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$\n$4 \\cdot A = \\begin{bmatrix} 4 & 8 \\\\ 0 & 4 \\end{bmatrix}$\nSubstituting these into the expression for $M$:\n$$\nM = \\begin{bmatrix}\n1 & 2 & 3 & 6 \\\\\n0 & 1 & 0 & 3 \\\\\n0 & 0 & 4 & 8 \\\\\n0 & 0 & 0 & 4\n\\end{bmatrix}\n$$\nThis is the explicit matrix for the operator $M$.\n\n**Part 3: Comparison and Explanation**\n\nWe compare the matrix $A \\otimes B$ from Part 1 with the matrix $M$ from Part 2.\n$$\nA \\otimes B = \\begin{bmatrix}\n1 & 0 & 2 & 0 \\\\\n3 & 4 & 6 & 8 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 3 & 4\n\\end{bmatrix}\n\\quad \\text{and} \\quad\nM = \\begin{bmatrix}\n1 & 2 & 3 & 6 \\\\\n0 & 1 & 0 & 3 \\\\\n0 & 0 & 4 & 8 \\\\\n0 & 0 & 0 & 4\n\\end{bmatrix}\n$$\nClearly, $A \\otimes B \\neq M$.\n\nThe relationship between $M$ and the given matrices $A$ and $B$, as derived from first principles in Part 2, is $M = B^T \\otimes A$. The operator $M$ that transforms $\\operatorname{vec}(X)$ to $\\operatorname{vec}(AXB)$ is not $A \\otimes B$, but rather $B^T \\otimes A$.\n\nThe reason this relationship holds is embodied in the derivation in Part 2. When expanding the expression for the columns of $AXB$, the elements of $B$ determine the linear combination of the column vectors $(A\\mathbf{x}_j)$. The structure of the resulting vectorized equation shows that the coefficients from the $k$-th column of $B$ (i.e., the $k$-th row of $B^T$) are used to construct the $k$-th block of rows in the final operator matrix $M$. This naturally leads to the structure of $B^T \\otimes A$.\n\nThe inequality $M \\neq A \\otimes B$ stems from two facts:\n1. The Kronecker product is not commutative in general, i.e., $C \\otimes D \\neq D \\otimes C$ unless special conditions apply.\n2. In this specific case, $B$ is not a symmetric matrix, so $B \\neq B^T$. Therefore, $B^T \\otimes A$ is distinct from $B \\otimes A$, and there is no reason for it to equal $A \\otimes B$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 0 & 2 & 0 \\\\ 3 & 4 & 6 & 8 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 3 & 4 \\end{pmatrix}}\n$$"
        }
    ]
}