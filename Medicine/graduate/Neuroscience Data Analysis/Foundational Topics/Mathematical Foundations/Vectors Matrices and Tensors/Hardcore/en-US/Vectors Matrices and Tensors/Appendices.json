{
    "hands_on_practices": [
        {
            "introduction": "In computational neuroscience, the collective activity of a neural population is often summarized as a high-dimensional vector. This exercise focuses on the fundamental task of comparing such neural representations, for instance, in response to different stimuli. By calculating vector norms and the angle between two population vectors , you will develop a concrete understanding of how to distinguish between changes in overall response magnitude and changes in the underlying activity pattern, a critical concept in representational similarity analysis.",
            "id": "4203703",
            "problem": "A neural population response is summarized by a feature vector where each component is a standardized (z-scored) activity measure, making the entries dimensionless. Consider two trial-averaged population activity vectors across three matched epochs, $x=(3,0,4)$ and $y=(2,-2,2)$, representing two stimulus conditions recorded from the same set of neurons. Using only the core definitions of vector norms, inner products, and angles in $\\mathbb{R}^n$, compute the following quantities: the Euclidean norm $\\|x\\|_2$, the Manhattan norm $\\|x\\|_1$, and the angle $\\theta=\\arccos\\left(\\frac{x^\\top y}{\\|x\\|_2\\|y\\|_2}\\right)$ between $x$ and $y$. Then, based on the mathematical properties of these measures, articulate which measure better captures similarity for correlated neural activity when overall response magnitude may differ across conditions, and explain why.\n\nReport the numeric tuple $(\\|x\\|_2,\\|x\\|_1,\\theta)$ exactly (do not round). Express the angle $\\theta$ in radians. The entries of $x$ and $y$ are standardized and therefore dimensionless, so no physical units are required for the norms.",
            "solution": "The problem statement is first validated for correctness and completeness.\n\n**Step 1: Extract Givens**\n-   Population activity vectors in $\\mathbb{R}^3$: $x = (3, 0, 4)$ and $y = (2, -2, 2)$.\n-   Vector components are dimensionless, standardized (z-scored) activity measures.\n-   Quantities to compute:\n    1.  The Euclidean norm of $x$, denoted as $\\|x\\|_2$.\n    2.  The Manhattan norm of $x$, denoted as $\\|x\\|_1$.\n    3.  The angle $\\theta$ between $x$ and $y$, defined by $\\theta = \\arccos\\left(\\frac{x^\\top y}{\\|x\\|_2\\|y\\|_2}\\right)$.\n-   Conceptual task: Articulate which measure (norm or angle) is better for comparing correlated neural activity when response magnitudes differ, and explain the reasoning.\n-   The final output should be the exact numeric tuple $(\\|x\\|_2, \\|x\\|_1, \\theta)$, with $\\theta$ in radians.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding**: The problem uses standard vector analysis techniques (norms, inner products, angles) to compare neural population vectors. This is a common and fundamental practice in computational neuroscience for analyzing neural coding and representational similarity. The setup is scientifically sound.\n-   **Well-Posedness**: The vectors are explicitly defined. The mathematical operations are standard and lead to a unique, well-defined solution. The conceptual question is based on the mathematical properties of the defined measures, making it a standard analytical reasoning task.\n-   **Objectivity and Completeness**: The problem is stated in precise, objective language. All information required for the calculations is provided. There are no contradictions or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically grounded, well-posed, and complete. I will now proceed with the solution.\n\n**Part 1: Calculation of the Required Quantities**\n\nFirst, we compute the Euclidean norm (or $L_2$-norm) of the vector $x$. The definition of the Euclidean norm for a vector $v = (v_1, v_2, \\dots, v_n)$ in $\\mathbb{R}^n$ is $\\|v\\|_2 = \\sqrt{\\sum_{i=1}^n v_i^2}$.\nFor the vector $x = (3, 0, 4)$, the Euclidean norm is:\n$$\n\\|x\\|_2 = \\sqrt{3^2 + 0^2 + 4^2} = \\sqrt{9 + 0 + 16} = \\sqrt{25} = 5\n$$\n\nNext, we compute the Manhattan norm (or $L_1$-norm) of the vector $x$. The definition of the Manhattan norm for a vector $v$ is $\\|v\\|_1 = \\sum_{i=1}^n |v_i|$.\nFor the vector $x = (3, 0, 4)$, the Manhattan norm is:\n$$\n\\|x\\|_1 = |3| + |0| + |4| = 3 + 0 + 4 = 7\n$$\n\nFinally, we compute the angle $\\theta$ between vectors $x$ and $y$. The formula is given as $\\theta = \\arccos\\left(\\frac{x^\\top y}{\\|x\\|_2\\|y\\|_2}\\right)$. We need to calculate the inner product $x^\\top y$ and the Euclidean norm of $y$, $\\|y\\|_2$.\nThe inner product (or dot product) of $x=(3, 0, 4)$ and $y=(2, -2, 2)$ is:\n$$\nx^\\top y = (3)(2) + (0)(-2) + (4)(2) = 6 + 0 + 8 = 14\n$$\nThe Euclidean norm of $y=(2, -2, 2)$ is:\n$$\n\\|y\\|_2 = \\sqrt{2^2 + (-2)^2 + 2^2} = \\sqrt{4 + 4 + 4} = \\sqrt{12} = \\sqrt{4 \\times 3} = 2\\sqrt{3}\n$$\nNow we can compute the argument of the $\\arccos$ function, which is the cosine of the angle between the vectors (also known as cosine similarity):\n$$\n\\frac{x^\\top y}{\\|x\\|_2\\|y\\|_2} = \\frac{14}{5 \\times 2\\sqrt{3}} = \\frac{14}{10\\sqrt{3}} = \\frac{7}{5\\sqrt{3}}\n$$\nFor a canonical representation, we can rationalize the denominator:\n$$\n\\frac{7}{5\\sqrt{3}} \\times \\frac{\\sqrt{3}}{\\sqrt{3}} = \\frac{7\\sqrt{3}}{5 \\times 3} = \\frac{7\\sqrt{3}}{15}\n$$\nThus, the angle $\\theta$ in radians is:\n$$\n\\theta = \\arccos\\left(\\frac{7\\sqrt{3}}{15}\\right)\n$$\n\nThe required numerical tuple is $(\\|x\\|_2, \\|x\\|_1, \\theta) = (5, 7, \\arccos(\\frac{7\\sqrt{3}}{15}))$.\n\n**Part 2: Conceptual Analysis**\n\nThe problem asks which measure, a norm or the angle, better captures similarity for correlated neural activity when the overall response magnitude may differ across conditions.\n\nVector norms, such as the Euclidean norm $\\|x\\|_2$ or the Manhattan norm $\\|x\\|_1$, are measures of a vector's magnitude or \"length\". In the context of neural activity, the norm quantifies the overall strength or intensity of the population response. If two response vectors have very different norms, it means the overall level of neural activity is different between the two conditions. Therefore, norms are inherently sensitive to response magnitude.\n\nThe angle $\\theta$ between two vectors, however, is a measure of their orientation relative to each other. It is derived from the cosine similarity, $\\cos(\\theta) = \\frac{x^\\top y}{\\|x\\|_2\\|y\\|_2}$. The key feature of this expression is the normalization by the product of the vector norms in the denominator. This normalization makes the measure invariant to the magnitudes (lengths) of the vectors. The angle exclusively captures the similarity in the *pattern* of the response across the neural population. A small angle $\\theta$ (corresponding to a cosine similarity close to $1$) indicates that the vectors point in nearly the same direction. In neuroscience terms, this means the relative pattern of activation across the neurons is very similar, even if the absolute level of activation is different. For instance, if vector $y$ were a scaled version of vector $x$, say $y=c x$ for some scalar $c > 0$, the angle between them would be $\\theta = 0$, indicating a perfect match in the response pattern, while their norms would differ by the factor $c$.\n\nGiven the scenario of \"correlated neural activity\" (implying a similar pattern) where \"overall response magnitude may differ,\" the **angle $\\theta$** is the superior measure of similarity. It isolates the representational geometry (the shape of the activity pattern) from the overall gain or excitability of the neural population. Using a distance metric like Euclidean distance, $\\|x-y\\|_2$, would confound differences in pattern with differences in an overall gain factor, making it difficult to determine if two stimuli are encoded differently or if one is simply a more intense version of the other. The angle, by being magnitude-invariant, directly addresses this issue and provides a pure measure of pattern similarity.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 5  7  \\arccos\\left(\\frac{7\\sqrt{3}}{15}\\right) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The General Linear Model (GLM) is an indispensable tool for analyzing functional neuroimaging data, allowing us to model brain activity in terms of experimental variables. The heart of a GLM is its design matrix, $X$, whose columns represent the regressors. This practice  guides you through a crucial diagnostic step: analyzing the linear independence of these regressors using the concepts of rank and null space, which is essential for ensuring that the model parameters are uniquely identifiable and interpretable.",
            "id": "4203709",
            "problem": "In a general linear model (GLM) for calcium imaging analysis, you construct a design matrix $X \\in \\mathbb{R}^{3 \\times 3}$ with $3$ regressors sampled at $3$ time points. Each column corresponds to a regressor encoding a hypothesized component of neural activity, and each row corresponds to a time sample. You are given\n$$\nX=\\begin{bmatrix}\n1  2  3\\\\\n2  4  6\\\\\n1  1  1\n\\end{bmatrix}.\n$$\nUsing only core linear algebra definitions and operations valid for finite-dimensional vector spaces, determine the following:\n1) The column space of $X$ as a subspace of $\\mathbb{R}^{3}$, specified by a basis consisting of columns of $X$.\n2) The rank of $X$.\n3) The null space of $X$, specified by a basis.\nThen, interpret the redundancy of the regressors in the context of identifiability of the GLM, stating how many regressors are linearly redundant and what explicit linear relation among the columns of $X$ demonstrates this redundancy.\n\nReport as your final answer the number of redundant regressors implied by $X$. No rounding is required. The final answer must be a single real number.",
            "solution": "The problem requires an analysis of the provided design matrix $X$ to determine its properties and interpret them in the context of a general linear model (GLM). The analysis must adhere to core principles of linear algebra.\n\nThe given design matrix is:\n$$\nX = \\begin{bmatrix}\n1  2  3 \\\\\n2  4  6 \\\\\n1  1  1\n\\end{bmatrix}\n$$\nLet the columns of $X$ be denoted as vectors $\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3 \\in \\mathbb{R}^3$:\n$$\n\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{v}_2 = \\begin{pmatrix} 2 \\\\ 4 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{v}_3 = \\begin{pmatrix} 3 \\\\ 6 \\\\ 1 \\end{pmatrix}\n$$\nThese vectors represent the three regressors in the GLM.\n\nFirst, we will establish the linear relationship between these column vectors. We seek coefficients $c_1, c_2, c_3$ not all zero, such that $c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + c_3\\mathbf{v}_3 = \\mathbf{0}$. This corresponds to solving the homogeneous system of linear equations $X\\mathbf{c} = \\mathbf{0}$, where $\\mathbf{c} = (c_1, c_2, c_3)^T$. The augmented matrix for this system is:\n$$\n\\left[\\begin{array}{ccc|c}\n1  2  3  0 \\\\\n2  4  6  0 \\\\\n1  1  1  0\n\\end{array}\\right]\n$$\nWe perform Gaussian elimination.\nSubtract $2$ times the first row from the second row ($R_2 \\to R_2 - 2R_1$):\n$$\n\\left[\\begin{array}{ccc|c}\n1  2  3  0 \\\\\n0  0  0  0 \\\\\n1  1  1  0\n\\end{array}\\right]\n$$\nSubtract the first row from the third row ($R_3 \\to R_3 - R_1$):\n$$\n\\left[\\begin{array}{ccc|c}\n1  2  3  0 \\\\\n0  0  0  0 \\\\\n0  -1  -2  0\n\\end{array}\\right]\n$$\nSwap the second and third rows ($R_2 \\leftrightarrow R_3$) and multiply the new second row by $-1$ ($R_2 \\to -R_2$):\n$$\n\\left[\\begin{array}{ccc|c}\n1  2  3  0 \\\\\n0  1  2  0 \\\\\n0  0  0  0\n\\end{array}\\right]\n$$\nFinally, subtract $2$ times the new second row from the first row ($R_1 \\to R_1 - 2R_2$):\n$$\n\\left[\\begin{array}{ccc|c}\n1  0  -1  0 \\\\\n0  1  2  0 \\\\\n0  0  0  0\n\\end{array}\\right]\n$$\nThis is the reduced row echelon form. The system of equations is:\n$c_1 - c_3 = 0 \\implies c_1 = c_3$\n$c_2 + 2c_3 = 0 \\implies c_2 = -2c_3$\nThe variable $c_3$ is a free variable. Let $c_3 = k$ for any scalar $k \\in \\mathbb{R}$. The solution vector is $\\mathbf{c} = (k, -2k, k)^T = k(1, -2, 1)^T$.\nChoosing a non-zero value, for instance $k=1$, gives a non-trivial linear combination that equals the zero vector:\n$$ 1\\mathbf{v}_1 - 2\\mathbf{v}_2 + 1\\mathbf{v}_3 = \\mathbf{0} $$\nThis is the explicit linear relation among the columns of $X$, demonstrating their linear dependence.\n\nWith this relation established, we can address the specific questions.\n\n1) The column space of $X$, denoted $C(X)$, is the subspace of $\\mathbb{R}^3$ spanned by its column vectors: $C(X) = \\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}$. Since the vectors are linearly dependent, we can remove one of them from the spanning set without changing the subspace. From the relation $\\mathbf{v}_3 = 2\\mathbf{v}_2 - \\mathbf{v}_1$, we see that $\\mathbf{v}_3$ is a linear combination of $\\mathbf{v}_1$ and $\\mathbf{v}_2$. Therefore, $C(X) = \\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2\\}$. To form a basis, we must check if $\\mathbf{v}_1$ and $\\mathbf{v}_2$ are linearly independent. They are independent because neither is a scalar multiple of the other (if $\\mathbf{v}_2 = c\\mathbf{v}_1$, then $2=c \\cdot 1$ and $1=c \\cdot 1$, which implies $c=2$ and $c=1$, a contradiction). Thus, a basis for the column space of $X$ is $\\{\\mathbf{v}_1, \\mathbf{v}_2\\}$.\nA basis for $C(X)$ is $\\left\\{ \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 4 \\\\ 1 \\end{pmatrix} \\right\\}$.\n\n2) The rank of a matrix, $\\text{rank}(X)$, is defined as the dimension of its column space. Since we found a basis for the column space consisting of two vectors, the dimension is $2$.\nTherefore, $\\text{rank}(X) = 2$.\nThis is also confirmed by the row echelon form of $X$, which has $2$ non-zero rows.\n\n3) The null space of $X$, denoted $N(X)$, is the set of all vectors $\\mathbf{c}$ such that $X\\mathbf{c} = \\mathbf{0}$. We found the general solution to this system to be $\\mathbf{c} = k(1, -2, 1)^T$ for any scalar $k$.\nThe null space is the subspace spanned by the vector $(1, -2, 1)^T$.\nA basis for $N(X)$ is $\\left\\{ \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix} \\right\\}$.\nThe dimension of the null space, or nullity, is $1$. This is consistent with the rank-nullity theorem: $\\text{rank}(X) + \\text{nullity}(X) = 2 + 1 = 3$, which is the number of columns of $X$.\n\n4) In a GLM, the regressors (columns of $X$) are considered redundant if they are not linearly independent. A lack of linear independence, known as multicollinearity, implies that the model is over-parameterized. The contribution of each regressor to the observed data cannot be uniquely determined. The number of linearly independent regressors is given by the rank of the matrix $X$, which is $2$. The total number of regressors is $3$.\nThe number of linearly redundant regressors is the total number of regressors minus the number of linearly independent ones.\nNumber of redundant regressors = (Number of columns) - $\\text{rank}(X) = 3 - 2 = 1$.\nThis is also equal to the nullity of the matrix. The single vector in the basis of the null space, $(1, -2, 1)^T$, provides the coefficients of the linear relation $1\\mathbf{v}_1 - 2\\mathbf{v}_2 + 1\\mathbf{v}_3 = \\mathbf{0}$, which explicitly demonstrates how one regressor's effect is perfectly accounted for by a combination of the others (e.g., $\\mathbf{v}_3 = 2\\mathbf{v}_2 - \\mathbf{v}_1$). This redundancy means that the regression coefficients $\\beta_1, \\beta_2, \\beta_3$ in the model $\\mathbf{y} = X\\mathbf{\\beta}$ cannot be uniquely estimated. There is one redundant regressor in the set.\n\nThe final answer requested is the number of redundant regressors implied by $X$. This value is $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Modern neuroscience increasingly views the brain as a complex network of interacting regions. Spectral graph theory offers a powerful mathematical framework for analyzing this network structure, with the graph Laplacian matrix being a central object. In this exercise , you will construct the Laplacian for a simple network of brain regions and compute its eigenvalues and eigenvectors to uncover its community structure. This will provide you with direct experience in how the spectrum of the Laplacian reveals fundamental topological properties of a network, such as its most natural partitions.",
            "id": "4203701",
            "problem": "A common preprocessing step in functional Magnetic Resonance Imaging (fMRI) Blood Oxygen Level Dependent (BOLD) connectivity analysis is to represent the relationships among a small set of Region of Interest (ROI) time series by an unweighted graph, where edges indicate pairwise statistical associations above a threshold and the topology encodes anatomical adjacency constraints. Consider a simplified case in which $4$ ROIs lie along a single white-matter pathway, and thresholding yields a path graph on nodes $\\{1,2,3,4\\}$ with edges only between consecutive nodes, that is, $(1,2)$, $(2,3)$, and $(3,4)$. Let $A$ be the adjacency matrix, $D$ the diagonal degree matrix, and $L$ the combinatorial graph Laplacian defined by $L = D - A$.\n\nStarting from the definitions of $A$, $D$, and $L$ and the definition of eigenvalues via the characteristic polynomial $\\det(L - \\lambda I)$, do the following:\n\n- Construct $A$, $D$, and $L$ for this path graph.\n- Derive, from first principles, the eigenvalues of $L$ by explicitly computing $\\det(L - \\lambda I)$ and solving for $\\lambda$.\n- Identify the Fiedler vector (the eigenvector associated with the second-smallest eigenvalue) up to an arbitrary nonzero scaling and sign, and interpret its entries as a partitioning direction for a two-way cut of the ROIs. Explain the implied partition in terms of contiguous ROI groups on the path and discuss why this is consistent with low-frequency structure in the connectivity.\n\nFor grading purposes, report the algebraic connectivity, denoted $\\lambda_{2}$ (the second-smallest eigenvalue of $L$), as a single exact closed-form expression. No rounding is required, and no physical units are involved.",
            "solution": "The problem is valid as it is scientifically grounded in standard graph theory and linear algebra, well-posed with a clear and complete setup, and objective in its formulation. It presents a standard application of spectral graph theory to a simplified neuroscience context.\n\nThe problem describes a set of $4$ Regions of Interest (ROIs), which we label as nodes $\\{1, 2, 3, 4\\}$. The connectivity is given as a path graph with edges only between consecutive nodes: $(1,2)$, $(2,3)$, and $(3,4)$. This is an unweighted, undirected graph.\n\nFirst, we construct the adjacency matrix $A$, degree matrix $D$, and the combinatorial graph Laplacian $L$.\n\nThe adjacency matrix $A$ has entries $A_{ij} = 1$ if an edge exists between node $i$ and node $j$, and $A_{ij} = 0$ otherwise. For the given path graph, $A$ is a $4 \\times 4$ matrix:\n$$\nA = \\begin{pmatrix}\n0  1  0  0 \\\\\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n0  0  1  0\n\\end{pmatrix}\n$$\n\nThe degree matrix $D$ is a diagonal matrix where each diagonal element $D_{ii}$ is the degree of node $i$ (the number of edges connected to it).\nThe degrees are:\n- $\\text{deg}(1) = 1$\n- $\\text{deg}(2) = 2$\n- $\\text{deg}(3) = 2$\n- $\\text{deg}(4) = 1$\n\nThus, the degree matrix $D$ is:\n$$\nD = \\begin{pmatrix}\n1  0  0  0 \\\\\n0  2  0  0 \\\\\n0  0  2  0 \\\\\n0  0  0  1\n\\end{pmatrix}\n$$\n\nThe combinatorial graph Laplacian $L$ is defined as $L = D - A$:\n$$\nL = \\begin{pmatrix}\n1  0  0  0 \\\\\n0  2  0  0 \\\\\n0  0  2  0 \\\\\n0  0  0  1\n\\end{pmatrix} - \\begin{pmatrix}\n0  1  0  0 \\\\\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n0  0  1  0\n\\end{pmatrix} = \\begin{pmatrix}\n1  -1  0  0 \\\\\n-1  2  -1  0 \\\\\n0  -1  2  -1 \\\\\n0  0  -1  1\n\\end{pmatrix}\n$$\n\nNext, we derive the eigenvalues of $L$ by solving the characteristic equation $\\det(L - \\lambda I) = 0$, where $I$ is the $4 \\times 4$ identity matrix and $\\lambda$ represents an eigenvalue.\n$$\nL - \\lambda I = \\begin{pmatrix}\n1-\\lambda  -1  0  0 \\\\\n-1  2-\\lambda  -1  0 \\\\\n0  -1  2-\\lambda  -1 \\\\\n0  0  -1  1-\\lambda\n\\end{pmatrix}\n$$\n\nWe compute the determinant using cofactor expansion along the first row:\n$$\n\\det(L - \\lambda I) = (1-\\lambda) \\det \\begin{pmatrix}\n2-\\lambda  -1  0 \\\\\n-1  2-\\lambda  -1 \\\\\n0  -1  1-\\lambda\n\\end{pmatrix} - (-1) \\det \\begin{pmatrix}\n-1  -1  0 \\\\\n0  2-\\lambda  -1 \\\\\n0  -1  1-\\lambda\n\\end{pmatrix}\n$$\n\nLet's evaluate the two $3 \\times 3$ determinants:\nThe first determinant is:\n$$\n(2-\\lambda)((2-\\lambda)(1-\\lambda) - 1) - (-1)(-(1-\\lambda)) = (2-\\lambda)(\\lambda^2 - 3\\lambda + 1) - (1-\\lambda) = -\\lambda^3 + 5\\lambda^2 - 6\\lambda + 1\n$$\nThe second determinant is:\n$$\n-1((2-\\lambda)(1-\\lambda)-1) = -(\\lambda^2-3\\lambda+1) = -\\lambda^2+3\\lambda-1\n$$\n\nSubstituting these back into the expression for $\\det(L - \\lambda I)$:\n$$\n\\det(L - \\lambda I) = (1-\\lambda)(-\\lambda^3 + 5\\lambda^2 - 6\\lambda + 1) + (-\\lambda^2 + 3\\lambda - 1)\n$$\n$$\n= (-\\lambda^3 + 5\\lambda^2 - 6\\lambda + 1) + (\\lambda^4 - 5\\lambda^3 + 6\\lambda^2 - \\lambda) - \\lambda^2 + 3\\lambda - 1\n$$\n$$\n= \\lambda^4 - 6\\lambda^3 + 10\\lambda^2 - 4\\lambda\n$$\n\nThe characteristic equation is $\\lambda^4 - 6\\lambda^3 + 10\\lambda^2 - 4\\lambda = 0$. We factor out $\\lambda$:\n$$\n\\lambda(\\lambda^3 - 6\\lambda^2 + 10\\lambda - 4) = 0\n$$\nOne eigenvalue is $\\lambda_1 = 0$. For the cubic factor, we test for rational roots, which must be divisors of $-4$. Testing $\\lambda=2$: $2^3 - 6(2^2) + 10(2) - 4 = 8 - 24 + 20 - 4 = 0$. So, $\\lambda=2$ is a root. We perform polynomial division of $\\lambda^3 - 6\\lambda^2 + 10\\lambda - 4$ by $(\\lambda-2)$, which yields the quadratic $\\lambda^2 - 4\\lambda + 2$.\nWe solve $\\lambda^2 - 4\\lambda + 2 = 0$ using the quadratic formula:\n$$\n\\lambda = \\frac{-(-4) \\pm \\sqrt{(-4)^2 - 4(1)(2)}}{2(1)} = \\frac{4 \\pm \\sqrt{16-8}}{2} = \\frac{4 \\pm \\sqrt{8}}{2} = \\frac{4 \\pm 2\\sqrt{2}}{2} = 2 \\pm \\sqrt{2}\n$$\nThe four eigenvalues are $0$, $2$, $2-\\sqrt{2}$, and $2+\\sqrt{2}$. We order them from smallest to largest:\n$\\lambda_1 = 0$\n$\\lambda_2 = 2 - \\sqrt{2} \\approx 0.586$\n$\\lambda_3 = 2$\n$\\lambda_4 = 2 + \\sqrt{2} \\approx 3.414$\n\nThe second-smallest eigenvalue, $\\lambda_2 = 2 - \\sqrt{2}$, is the algebraic connectivity of the graph.\n\nNow, we find the Fiedler vector, which is the eigenvector $v_2$ corresponding to $\\lambda_2$. We solve the system $(L - \\lambda_2 I)v_2 = 0$:\n$$\n\\left(L - (2-\\sqrt{2})I\\right) v_2 = \\begin{pmatrix}\n1-(2-\\sqrt{2})  -1  0  0 \\\\\n-1  2-(2-\\sqrt{2})  -1  0 \\\\\n0  -1  2-(2-\\sqrt{2})  -1 \\\\\n0  0  -1  1-(2-\\sqrt{2})\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix}\n\\sqrt{2}-1  -1  0  0 \\\\\n-1  \\sqrt{2}  -1  0 \\\\\n0  -1  \\sqrt{2}  -1 \\\\\n0  0  -1  \\sqrt{2}-1\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, $(\\sqrt{2}-1)x_1 - x_2 = 0$, so $x_2 = (\\sqrt{2}-1)x_1$.\nFrom the second row, $-x_1 + \\sqrt{2}x_2 - x_3 = 0$. Substituting for $x_2$: $-x_1 + \\sqrt{2}(\\sqrt{2}-1)x_1 = x_3$, giving $x_3 = (-1 + 2 - \\sqrt{2})x_1 = (1-\\sqrt{2})x_1$.\nFrom the third row, $-x_2 + \\sqrt{2}x_3 - x_4 = 0$. Substituting for $x_2, x_3$: $-(\\sqrt{2}-1)x_1 + \\sqrt{2}(1-\\sqrt{2})x_1 = x_4$, giving $x_4 = (-\\sqrt{2}+1+\\sqrt{2}-2)x_1 = -x_1$.\nThe fourth equation, $-x_3 + (\\sqrt{2}-1)x_4 = 0$, confirms this: $-(1-\\sqrt{2})x_1 + (\\sqrt{2}-1)(-x_1) = (\\sqrt{2}-1)x_1 - (\\sqrt{2}-1)x_1 = 0$.\nChoosing $x_1=1$, the Fiedler vector is $v_2 = \\begin{pmatrix} 1  \\sqrt{2}-1  1-\\sqrt{2}  -1 \\end{pmatrix}^T$.\n\nThe Fiedler vector is used for spectral bisection. The signs of its components suggest a partition of the graph's nodes. The components are approximately $v_2 \\approx \\begin{pmatrix} 1  0.414  -0.414  -1 \\end{pmatrix}^T$. The signs of the entries are $(+,+,-,-)$. This partitions the nodes into two sets: $V_1 = \\{1, 2\\}$ (positive entries) and $V_2 = \\{3, 4\\}$ (negative entries).\nThis partition corresponds to a cut that severs the edge $(2,3)$, separating the path graph into two contiguous groups of ROIs: $\\{1,2\\}$ and $\\{3,4\\}$. This is the most \"natural\" bisection of the linear structure.\n\nThe Laplacian eigenvectors are analogous to Fourier modes on a graph, with the eigenvalues corresponding to frequencies. The smallest eigenvalue $\\lambda_1=0$ corresponds to a zero-frequency (constant) mode. The Fiedler vector, associated with the smallest non-zero eigenvalue $\\lambda_2$, represents the lowest-frequency or \"smoothest\" non-trivial mode of variation over the graph. \"Smoothness\" is quantified by the Rayleigh quotient, $R(v) = \\frac{v^T L v}{v^T v} = \\frac{\\sum_{(i,j) \\in E} (v_i - v_j)^2}{\\sum_i v_i^2}$. The Fiedler vector minimizes this quantity among all vectors orthogonal to the constant vector. This means the values of the Fiedler vector on adjacent nodes are as close as possible, reflecting large-scale structure rather than rapid, local fluctuations. The single sign change in the Fiedler vector along the path indicates it captures the most fundamental division of the graph, which is consistent with identifying low-frequency (i.e., large-scale) patterns in the connectivity structure. This partition into two contiguous blocks $\\{1,2\\}$ and $\\{3,4\\}$ is precisely such a pattern.\n\nThe final answer required is the algebraic connectivity, $\\lambda_2$.\n$$\n\\lambda_2 = 2 - \\sqrt{2}\n$$",
            "answer": "$$\\boxed{2 - \\sqrt{2}}$$"
        }
    ]
}