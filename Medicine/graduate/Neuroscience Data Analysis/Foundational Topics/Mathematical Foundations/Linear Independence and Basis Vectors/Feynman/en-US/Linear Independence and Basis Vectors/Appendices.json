{
    "hands_on_practices": [
        {
            "introduction": "The General Linear Model (GLM) is a cornerstone of statistical analysis in neuroscience, particularly in fMRI research. The model's validity hinges on the design matrix, whose columns represent the factors of your experiment. This exercise simulates a common experimental design flaw that leads to linear dependence, or multicollinearity, among these factors. By calculating the rank of the provided design matrix $X$, you will practice the fundamental skill of diagnosing model redundancy, a critical first step in ensuring your statistical results are trustworthy and interpretable .",
            "id": "4175062",
            "problem": "A research team is modeling blood-oxygen-level-dependent responses in functional Magnetic Resonance Imaging (fMRI) using the General Linear Model (GLM), written as $Y = X\\beta + \\varepsilon$, where $Y \\in \\mathbb{R}^{n}$ is the observed signal, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^{p}$ are the parameters, and $\\varepsilon \\in \\mathbb{R}^{n}$ represents noise. The experiment consists of $n = 9$ consecutive scans with no resting baseline and three mutually exclusive task conditions (cognitive states) labeled $A$, $B$, and $C$. At each scan, exactly one condition is active. The analyst constructs a design matrix $X$ with $p = 5$ columns: an intercept, three dummy-coded condition regressors (one each for $A$, $B$, and $C$), and a “task-on” regressor equal to the sum of the three dummy-coded condition regressors. All regressors are unit-amplitude boxcar columns, not convolved with any hemodynamic kernel.\n\nThe schedule assigns scans $1$–$3$ to condition $A$, scans $4$–$6$ to condition $B$, and scans $7$–$9$ to condition $C$. The resulting design matrix $X \\in \\mathbb{R}^{9 \\times 5}$ is\n$$\nX = \\begin{pmatrix}\n1 & 1 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 0 & 1 & 1 \\\\\n1 & 0 & 0 & 1 & 1 \\\\\n1 & 0 & 0 & 1 & 1\n\\end{pmatrix},\n$$\nwhere the first column is the intercept (all ones), columns $2$–$4$ are the dummy-coded regressors for conditions $A$, $B$, and $C$ respectively, and the fifth column is the “task-on” regressor defined as the sum of the three dummy-coded columns.\n\nUsing only the core definitions of linear independence, rank, and basis from linear algebra, and the GLM formulation above, determine the rank of $X$. Express your final answer as a single integer with no units. No rounding is required.",
            "solution": "The rank of a matrix is defined as the dimension of its column space, which is the vector space spanned by its columns. This dimension is equal to the maximum number of linearly independent columns in the matrix. The problem requires us to determine the rank of the design matrix $X \\in \\mathbb{R}^{9 \\times 5}$ given by:\n$$\nX = \\begin{pmatrix}\n1 & 1 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 0 & 1 & 1 \\\\\n1 & 0 & 0 & 1 & 1 \\\\\n1 & 0 & 0 & 1 & 1\n\\end{pmatrix}\n$$\nLet the columns of $X$ be denoted by the vectors $x_1, x_2, x_3, x_4, x_5 \\in \\mathbb{R}^{9}$, where the subscript corresponds to the column index. A set of vectors $\\{v_1, \\dots, v_k\\}$ is linearly dependent if there exist scalars $c_1, \\dots, c_k$, not all zero, such that the equation $c_1 v_1 + \\dots + c_k v_k = \\mathbf{0}$ is satisfied. The rank of $X$ is the size of the largest linearly independent subset of $\\{x_1, x_2, x_3, x_4, x_5\\}$.\n\nFirst, we identify any linear dependencies among the columns of $X$. By inspection of the matrix, the first column $x_1$ (the intercept) and the fifth column $x_5$ (the \"task-on\" regressor) are identical.\n$$\nx_1 = x_5\n$$\nThis relationship implies a linear dependency, which can be expressed as a non-trivial linear combination of the columns that equals the zero vector:\n$$\n(1) x_1 + (0) x_2 + (0) x_3 + (0) x_4 + (-1) x_5 = \\mathbf{0}\n$$\nSince the coefficients are not all zero, the set of all $5$ columns is linearly dependent. This proves that the rank of $X$ is less than $5$.\n\nFurthermore, the problem states that the fifth column is defined as the sum of the three dummy-coded condition regressors (columns $2, 3,$ and $4$). Let's verify this using the provided vectors:\n$$\nx_2 + x_3 + x_4 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe resulting vector is identical to the first column, $x_1$. Thus, we have a second linear dependency:\n$$\nx_1 = x_2 + x_3 + x_4\n$$\nThis can be written as another non-trivial linear combination that equals the zero vector:\n$$\n(-1) x_1 + (1) x_2 + (1) x_3 + (1) x_4 + (0) x_5 = \\mathbf{0}\n$$\nThese dependencies indicate that some columns are redundant for the purpose of spanning the column space of $X$, denoted $C(X)$.\nThe column space is $C(X) = \\text{span}\\{x_1, x_2, x_3, x_4, x_5\\}$.\nSince $x_5 = x_1$, the vector $x_5$ is redundant, and we can write:\n$$\nC(X) = \\text{span}\\{x_1, x_2, x_3, x_4\\}\n$$\nFurthermore, since $x_1 = x_2 + x_3 + x_4$, the vector $x_1$ is a linear combination of $x_2, x_3,$ and $x_4$. Thus, $x_1$ is also redundant in the spanning set:\n$$\nC(X) = \\text{span}\\{x_2, x_3, x_4\\}\n$$\nThe rank of $X$ is the dimension of this space. The dimension is equal to the number of vectors in a basis for the space. If the set $\\{x_2, x_3, x_4\\}$ is linearly independent, it forms a basis for $C(X)$. We must test for linear independence by seeking scalars $c_2, c_3, c_4$ such that:\n$$\nc_2 x_2 + c_3 x_3 + c_4 x_4 = \\mathbf{0}\n$$\nSubstituting the vector components:\n$$\nc_2 \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + c_3 \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + c_4 \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis single vector equation is equivalent to the following system of scalar equations:\n$$\n\\begin{pmatrix} c_2 \\\\ c_2 \\\\ c_2 \\\\ c_3 \\\\ c_3 \\\\ c_3 \\\\ c_4 \\\\ c_4 \\\\ c_4 \\end{pmatrix} = \\mathbf{0}\n$$\nFrom the first $3$ rows, we deduce $c_2 = 0$. From rows $4, 5,$ and $6$, we deduce $c_3 = 0$. From rows $7, 8,$ and $9$, we deduce $c_4 = 0$. The only solution is the trivial solution, $c_2 = 0, c_3 = 0, c_4 = 0$.\nTherefore, the set of vectors $\\{x_2, x_3, x_4\\}$ is linearly independent.\n\nSince the set $\\{x_2, x_3, x_4\\}$ is linearly independent and it spans the column space $C(X)$, it constitutes a basis for $C(X)$. By definition, the rank of a matrix is the number of vectors in any basis for its column space. The basis we have found, $\\{x_2, x_3, x_4\\}$, contains $3$ vectors.\n\nThus, the rank of the matrix $X$ is $3$. This result reveals that the design matrix exhibits perfect multicollinearity, a condition where the rank ($3$) is less than the number of columns ($p=5$). In the context of the GLM, this means that the matrix $X^{\\top} X$ is singular, and a unique Ordinary Least Squares solution for the parameter vector $\\beta$ cannot be determined.",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "Having learned to identify linear dependence, we now investigate its profound consequence: model non-identifiability. When a design matrix is rank-deficient, there is no single, unique \"best-fit\" solution; instead, an entire family of parameter vectors can explain the data equally well. This practice makes this abstract issue concrete by guiding you to characterize this family of solutions as an affine subspace, built from a particular solution and the null space of the matrix . Understanding this concept is crucial for recognizing when a model's parameters are ambiguous and cannot be uniquely interpreted.",
            "id": "4175036",
            "problem": "You are analyzing a General Linear Model (GLM) in neuroscience data analysis for a short recording of local field potential (LFP) amplitude at four time points, modeled as a linear combination of three regressors: a baseline term, a stimulus event for condition A, and a composite nuisance regressor that was inadvertently constructed as the sum of the baseline and condition A regressors. Let the response vector be $$\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix},$$ and let the design matrix be $$\\mathbf{X} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 2 \\\\ 1 & 1 & 2 \\\\ 1 & 0 & 1 \\end{pmatrix},$$ where the columns are, respectively, baseline, condition A, and the composite nuisance.\n\nStarting from the core definitions of linear independence, matrix rank, null space of a matrix, and the least-squares estimator defined by minimizing the squared error $$\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^{2},$$ carry out the following:\n\n1. Use the algebraic definition of the null space to determine a nonzero vector in the null space of $$\\mathbf{X},$$ thereby demonstrating that the collinearity among the regressors creates a non-trivial null space.\n\n2. Use the least-squares criterion together with the properties of the null space to characterize the manifold of parameter vectors that yield identical fitted responses. Choose as a particular representative the solution obtained by fitting only the baseline and condition A regressors (i.e., excluding the composite nuisance column), and then embed this solution into the full three-parameter space to obtain one least-squares solution $$\\boldsymbol{\\beta}_{0} \\in \\mathbb{R}^{3}.$$\n\n3. Combine your results to write the one-dimensional affine subspace of equivalent parameter vectors (the manifold of indistinguishable solutions under this design) in closed form as a parametric expression in terms of a free scalar parameter $$t \\in \\mathbb{R}.$$\n\nProvide your final answer as this single parametric expression for the manifold in $$\\mathbb{R}^{3}.$$ No rounding is required. Express no units in your final answer.",
            "solution": "The problem asks for an analysis of a least-squares estimation problem where the design matrix $\\mathbf{X}$ has linearly dependent columns. The goal is to find the complete set of parameter vectors $\\boldsymbol{\\beta}$ that minimize the squared error $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^{2}$. This set of solutions forms an affine subspace, which we will characterize.\n\nThe given response vector is $\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix}$, and the design matrix is $\\mathbf{X} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 2 \\\\ 1 & 1 & 2 \\\\ 1 & 0 & 1 \\end{pmatrix}$. The parameter vector is $\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix}$, corresponding to the baseline, condition A, and composite nuisance regressors.\n\n**1. Determination of the Null Space of $\\mathbf{X}$**\n\nThe problem states that the third column of $\\mathbf{X}$, which we denote $\\mathbf{c}_3$, is the sum of the first two columns, $\\mathbf{c}_1$ and $\\mathbf{c}_2$. We can verify this:\n$$\\mathbf{c}_1 + \\mathbf{c}_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\mathbf{c}_3$$\nThis linear dependency can be written as $\\mathbf{c}_1 + \\mathbf{c}_2 - \\mathbf{c}_3 = \\mathbf{0}$. This equation represents a linear combination of the columns of $\\mathbf{X}$ that results in the zero vector.\n\nBy definition, the null space of a matrix $\\mathbf{X}$, denoted $N(\\mathbf{X})$, is the set of all vectors $\\mathbf{v}$ such that $\\mathbf{X}\\mathbf{v} = \\mathbf{0}$. The equation $\\mathbf{c}_1 + \\mathbf{c}_2 - \\mathbf{c}_3 = \\mathbf{0}$ can be expressed in matrix form as:\n$$\\mathbf{X} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\mathbf{0}$$\nTherefore, the nonzero vector $\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}$ is in the null space of $\\mathbf{X}$. The first two columns of $\\mathbf{X}$ are linearly independent, so the rank of $\\mathbf{X}$ is $2$. By the rank-nullity theorem, the dimension of the null space (the nullity) is the number of columns minus the rank, which is $3 - 2 = 1$. Thus, the null space of $\\mathbf{X}$ is the one-dimensional subspace spanned by $\\mathbf{v}$:\n$$N(\\mathbf{X}) = \\{ t \\cdot \\mathbf{v} \\mid t \\in \\mathbb{R} \\} = \\left\\{ t \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} \\mid t \\in \\mathbb{R} \\right\\}$$\n\n**2. Finding a Particular Least-Squares Solution $\\boldsymbol{\\beta}_0$**\n\nThe least-squares solutions are the vectors $\\boldsymbol{\\beta}$ that minimize $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^{2}$. These solutions satisfy the normal equations $\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^{\\top}\\mathbf{y}$. Because $\\mathbf{X}$ has linearly dependent columns, the matrix $\\mathbf{X}^{\\top}\\mathbf{X}$ is singular, and there is no unique solution for $\\boldsymbol{\\beta}$. However, the set of solutions is non-empty.\n\nTo find a particular solution, we can solve an equivalent, well-posed problem. Since $\\mathbf{c}_3$ is redundant, the column space of $\\mathbf{X}$ is identical to the column space of the matrix $\\mathbf{X}_{red}$ formed by its first two (linearly independent) columns.\n$$\\mathbf{X}_{red} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 1 \\\\ 1 & 0 \\end{pmatrix}$$\nWe can find a unique parameter vector $\\boldsymbol{\\beta}_{red} = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}$ by solving the reduced least-squares problem: minimize $\\|\\mathbf{y} - \\mathbf{X}_{red}\\boldsymbol{\\beta}_{red}\\|^{2}$. The corresponding normal equations are $\\mathbf{X}_{red}^{\\top}\\mathbf{X}_{red}\\boldsymbol{\\beta}_{red} = \\mathbf{X}_{red}^{\\top}\\mathbf{y}$.\n\nFirst, we compute $\\mathbf{X}_{red}^{\\top}\\mathbf{X}_{red}$:\n$$\\mathbf{X}_{red}^{\\top}\\mathbf{X}_{red} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 0 & 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 4 & 2 \\\\ 2 & 2 \\end{pmatrix}$$\nNext, we compute $\\mathbf{X}_{red}^{\\top}\\mathbf{y}$:\n$$\\mathbf{X}_{red}^{\\top}\\mathbf{y} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 0 & 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2+4+5+3 \\\\ 0+4+5+0 \\end{pmatrix} = \\begin{pmatrix} 14 \\\\ 9 \\end{pmatrix}$$\nWe now solve the system of linear equations:\n$$\\begin{pmatrix} 4 & 2 \\\\ 2 & 2 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix} = \\begin{pmatrix} 14 \\\\ 9 \\end{pmatrix}$$\nThis gives us the system:\n1. $4\\beta_1 + 2\\beta_2 = 14 \\implies 2\\beta_1 + \\beta_2 = 7$\n2. $2\\beta_1 + 2\\beta_2 = 9$\nSubtracting the first equation from the second yields $\\beta_2 = 2$. Substituting this back into the first equation: $2\\beta_1 + 2 = 7 \\implies 2\\beta_1 = 5 \\implies \\beta_1 = \\frac{5}{2}$.\nSo, $\\boldsymbol{\\beta}_{red} = \\begin{pmatrix} 5/2 \\\\ 2 \\end{pmatrix}$.\n\nWe can embed this solution into the three-dimensional parameter space by setting the coefficient for the redundant regressor to zero. This gives a particular solution $\\boldsymbol{\\beta}_0$ for the original problem:\n$$\\boldsymbol{\\beta}_{0} = \\begin{pmatrix} 5/2 \\\\ 2 \\\\ 0 \\end{pmatrix}$$\nThis vector $\\boldsymbol{\\beta}_0$ is a valid least-squares solution because the predicted response $\\mathbf{X}\\boldsymbol{\\beta}_0 = \\mathbf{X}_{red}\\boldsymbol{\\beta}_{red}$ is the orthogonal projection of $\\mathbf{y}$ onto the column space of $\\mathbf{X}$.\n\n**3. Characterization of the Manifold of All Solutions**\n\nThe set of all least-squares solutions consists of all vectors $\\boldsymbol{\\beta}$ that generate the same minimum squared error. This is equivalent to finding all $\\boldsymbol{\\beta}$ that produce the same predicted response vector, $\\hat{\\mathbf{y}}$, as our particular solution $\\boldsymbol{\\beta}_0$.\n$$\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}\\boldsymbol{\\beta}_{0}$$\nRearranging this equation, we get:\n$$\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{X}\\boldsymbol{\\beta}_{0} = \\mathbf{0} \\implies \\mathbf{X}(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_{0}) = \\mathbf{0}$$\nThis equation shows that the difference $\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_{0}$ must be a vector in the null space of $\\mathbf{X}$.\nFrom Part 1, we know that any vector in $N(\\mathbf{X})$ can be written as $t \\cdot \\mathbf{v}$ for some scalar $t \\in \\mathbb{R}$, where $\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}$.\nTherefore, $\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_{0} = t\\mathbf{v}$, which implies that the general form of any least-squares solution is:\n$$\\boldsymbol{\\beta}(t) = \\boldsymbol{\\beta}_{0} + t\\mathbf{v}$$\nSubstituting the vectors we found for $\\boldsymbol{\\beta}_{0}$ and $\\mathbf{v}$:\n$$\\boldsymbol{\\beta}(t) = \\begin{pmatrix} 5/2 \\\\ 2 \\\\ 0 \\end{pmatrix} + t \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 5/2 + t \\\\ 2 + t \\\\ 0 - t \\end{pmatrix} = \\begin{pmatrix} 5/2 + t \\\\ 2 + t \\\\ -t \\end{pmatrix}$$\nThis parametric expression defines the one-dimensional affine subspace of all parameter vectors that minimize the least-squares criterion, representing the manifold of indistinguishable solutions for this rank-deficient model.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{5}{2} + t \\\\\n2 + t \\\\\n-t\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "After diagnosing the problems of linear dependence, we now explore a powerful solution: transforming our data into a new, more informative basis. This exercise introduces this idea through the lens of Principal Component Analysis (PCA), where the goal is to find a basis of eigenvectors that diagonalizes the data's covariance matrix . By doing so, you will transform a hypothetical set of correlated neural features into decorrelated principal components, revealing the primary axes of variation in the data and providing a more parsimonious and interpretable representation.",
            "id": "4175038",
            "problem": "You analyze trial-averaged neural features from a single cortical site during a sensory decoding experiment. On each of $T$ trials, you record a feature vector $x \\in \\mathbb{R}^{2}$ whose entries are: $x_{1}$, the mean spike count in a fixed post-stimulus window, and $x_{2}$, the mean local field potential amplitude in the same window. After centering the features so that their empirical means over trials are zero, you compute the sample covariance matrix\n$$\nC \\;=\\; \\begin{pmatrix} 5 & 2 \\\\ 2 & 1 \\end{pmatrix}.\n$$\nUsing only core linear algebra and probability definitions, construct an orthogonal change of basis that diagonalizes $C$ so that the transformed features are decorrelated. Specifically, consider a rotation by angle $\\theta$, represented by the orthogonal matrix\n$$\nR(\\theta) \\;=\\; \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix},\n$$\nand derive, from first principles, the condition on $\\theta$ under which the covariance matrix in the rotated coordinates, $C' \\;=\\; R(\\theta)^{\\top} C \\, R(\\theta)$, is diagonal. Interpret the columns of $R(\\theta)$ as basis vectors for the feature space and explain how decorrelation corresponds to selecting a basis of linearly independent directions aligned with the covariance structure of the data.\n\nFinally, for the specific matrix $C$ above, determine the unique rotation angle $\\theta \\in \\left(0, \\frac{\\pi}{4}\\right)$ (in radians) that achieves decorrelation. Express your final answer as an exact analytic expression in radians. Do not approximate or round the result. The final answer must be only the value of $\\theta$.",
            "solution": "Let the original covariance matrix be denoted generally as:\n$$\nC = \\begin{pmatrix} C_{11} & C_{12} \\\\ C_{21} & C_{22} \\end{pmatrix}\n$$\nwhere $C_{12} = C_{21}$ due to the symmetry of the covariance matrix. The transformed covariance matrix in the rotated coordinate system is $C' = R(\\theta)^{\\top} C R(\\theta)$. The transpose of the rotation matrix $R(\\theta)$ is:\n$$\nR(\\theta)^{\\top} = \\begin{pmatrix} \\cos\\theta & \\sin\\theta \\\\ -\\sin\\theta & \\cos\\theta \\end{pmatrix}\n$$\nWe perform the matrix multiplication to find the entries of $C'$. Let's\nuse the shorthand $c = \\cos\\theta$ and $s = \\sin\\theta$.\n$$\nC' = \\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix} \\begin{pmatrix} C_{11} & C_{12} \\\\ C_{12} & C_{22} \\end{pmatrix} \\begin{pmatrix} c & -s \\\\ s & c \\end{pmatrix}\n$$\nFirst, we compute the product $C R(\\theta)$:\n$$\nC R(\\theta) = \\begin{pmatrix} C_{11}c + C_{12}s & -C_{11}s + C_{12}c \\\\ C_{12}c + C_{22}s & -C_{12}s + C_{22}c \\end{pmatrix}\n$$\nNext, we pre-multiply by $R(\\theta)^{\\top}$:\n$$\nC' = \\begin{pmatrix} c(C_{11}c + C_{12}s) + s(C_{12}c + C_{22}s) & c(-C_{11}s + C_{12}c) + s(-C_{12}s + C_{22}c) \\\\ -s(C_{11}c + C_{12}s) + c(C_{12}c + C_{22}s) & -s(-C_{11}s + C_{12}c) + c(-C_{12}s + C_{22}c) \\end{pmatrix}\n$$\nFor the transformed features to be decorrelated, the off-diagonal elements of $C'$ must be zero. Let's focus on the element $C'_{12}$:\n$$\nC'_{12} = c(-C_{11}s + C_{12}c) + s(-C_{12}s + C_{22}c)\n$$\n$$\nC'_{12} = -C_{11}sc + C_{12}c^2 - C_{12}s^2 + C_{22}sc\n$$\n$$\nC'_{12} = (C_{22} - C_{11})sc + C_{12}(c^2 - s^2)\n$$\nUsing the double-angle trigonometric identities $\\sin(2\\theta) = 2\\sin\\theta\\cos\\theta = 2sc$ and $\\cos(2\\theta) = \\cos^2\\theta - \\sin^2\\theta = c^2 - s^2$, we can rewrite the expression for $C'_{12}$:\n$$\nC'_{12} = (C_{22} - C_{11})\\frac{\\sin(2\\theta)}{2} + C_{12}\\cos(2\\theta)\n$$\nThe condition for decorrelation is $C'_{12} = 0$:\n$$\nC_{12}\\cos(2\\theta) = -(C_{22} - C_{11})\\frac{\\sin(2\\theta)}{2} = (C_{11} - C_{22})\\frac{\\sin(2\\theta)}{2}\n$$\nAssuming $\\cos(2\\theta) \\neq 0$ and $C_{11} \\neq C_{22}$, we can rearrange this to find the condition on $\\theta$:\n$$\n\\frac{\\sin(2\\theta)}{\\cos(2\\theta)} = \\tan(2\\theta) = \\frac{2C_{12}}{C_{11} - C_{22}}\n$$\nThis is the general condition on the rotation angle $\\theta$ that diagonalizes any $2 \\times 2$ symmetric matrix $C$.\n\nNow, for the interpretation. The columns of the rotation matrix $R(\\theta)$ are the new basis vectors for the feature space:\n$$\nv_1 = \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}, \\quad v_2 = \\begin{pmatrix} -\\sin\\theta \\\\ \\cos\\theta \\end{pmatrix}\n$$\nThese vectors are orthogonal and have unit length, forming an orthonormal basis. The condition we derived for $\\theta$ is precisely the condition that makes these basis vectors the eigenvectors of the covariance matrix $C$. When the basis vectors are eigenvectors of $C$, the matrix representation of the linear transformation defined by $C$ in this new basis is diagonal. The diagonal entries are the eigenvalues of $C$, which represent the variance of the data along the new basis directions (the principal components). By aligning the basis vectors with the principal axes of the data's covariance structure (the directions of maximal and minimal variance), we ensure that the projections of the data onto these new axes are uncorrelated. Thus, decorrelation corresponds to selecting a basis of linearly independent vectors that are the eigenvectors of the covariance matrix.\n\nFinally, we apply this result to the specific covariance matrix given:\n$$\nC = \\begin{pmatrix} 5 & 2 \\\\ 2 & 1 \\end{pmatrix}\n$$\nHere, $C_{11} = 5$, $C_{22} = 1$, and $C_{12} = 2$.\nSubstituting these values into our derived condition:\n$$\n\\tan(2\\theta) = \\frac{2 C_{12}}{C_{11} - C_{22}} = \\frac{2(2)}{5 - 1} = \\frac{4}{4} = 1\n$$\nWe need to solve $\\tan(2\\theta) = 1$ for $\\theta \\in \\left(0, \\frac{\\pi}{4}\\right)$.\nThe general solution for $2\\theta$ is $2\\theta = \\arctan(1) + n\\pi$ for any integer $n$. The principal value of $\\arctan(1)$ is $\\frac{\\pi}{4}$.\nSo, $2\\theta = \\frac{\\pi}{4} + n\\pi$.\nThis gives $\\theta = \\frac{\\pi}{8} + \\frac{n\\pi}{2}$.\nWe must find the unique solution in the interval $\\theta \\in \\left(0, \\frac{\\pi}{4}\\right)$.\n- If $n = 0$, $\\theta = \\frac{\\pi}{8}$. Since $0 < \\frac{\\pi}{8} < \\frac{2\\pi}{8} = \\frac{\\pi}{4}$, this solution is in the specified interval.\n- If $n = 1$, $\\theta = \\frac{\\pi}{8} + \\frac{\\pi}{2} = \\frac{5\\pi}{8}$, which is greater than $\\frac{\\pi}{4}$.\n- If $n = -1$, $\\theta = \\frac{\\pi}{8} - \\frac{\\pi}{2} = -\\frac{3\\pi}{8}$, which is less than $0$.\nTherefore, the unique angle in the given range is $\\frac{\\pi}{8}$.\nThis angle corresponds to a rotation that aligns the new basis vectors with the eigenvectors of $C$, thereby diagonalizing $C$ and decorrelating the transformed features.",
            "answer": "$$\\boxed{\\frac{\\pi}{8}}$$"
        }
    ]
}