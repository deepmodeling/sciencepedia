{
    "hands_on_practices": [
        {
            "introduction": "In neuroscience, we often analyze the same neural activity pattern from different perspectives, which mathematically corresponds to representing a vector in different bases. This exercise  provides a concrete scenario of translating between an anatomically-defined basis and one derived from Independent Component Analysis (ICA). By working through this problem, you will derive the fundamental rules for coordinate transformation and solidify your understanding of why a change-of-basis matrix must be invertible.",
            "id": "4175034",
            "problem": "A common operation in neuroscience data analysis is to express the same neural activity pattern in different coordinate systems induced by distinct bases, such as anatomically defined regions-of-interest versus statistically extracted components. Consider the real vector space $\\mathbb{R}^{3}$ of trial-averaged response patterns, with coordinates corresponding to three latent features extracted from calcium imaging. Let $\\mathcal{B} = \\{b_{1}, b_{2}, b_{3}\\}$ be a basis associated with anatomically defined region features and let $\\mathcal{B}' = \\{b'_{1}, b'_{2}, b'_{3}\\}$ be a basis obtained by Independent Component Analysis (ICA).\n\nStarting from only the definitions of a basis, linear independence, and the coordinate representation of vectors with respect to a basis, derive how coordinates of a vector $y \\in \\mathbb{R}^{3}$ transform between $\\mathcal{B}$ and $\\mathcal{B}'$ via a change-of-basis matrix. Assume the change-of-basis matrix $T$ maps coordinates in $\\mathcal{B}$ to coordinates in $\\mathcal{B}'$ (so $T$ is the matrix representation of the identity linear operator on $\\mathbb{R}^{3}$ when the domain is expressed in $\\mathcal{B}$ and the codomain in $\\mathcal{B}'$). Explain why $T$ must be invertible if both $\\mathcal{B}$ and $\\mathcal{B}'$ are bases.\n\nThen, for a specific dataset alignment, you are given\n$$\nT \\;=\\;\n\\begin{pmatrix}\n2  -1  0 \\\\\n1  1  3 \\\\\n0  2  1\n\\end{pmatrix}.\n$$\nA particular trial’s neural activity pattern has ICA-basis coordinates\n$$\n[y]_{\\mathcal{B}'} \\;=\\;\n\\begin{pmatrix}\n9 \\\\ -9 \\\\ 18\n\\end{pmatrix}.\n$$\nUsing the derived transformation rule, compute the anatomical region basis coordinates $[y]_{\\mathcal{B}}$ exactly. Express your final answer as a row vector in simplest integer form; no rounding is required and no physical units are to be reported.",
            "solution": "The solution is divided into three parts as requested: the derivation of the transformation rule, the explanation for the invertibility of the change-of-basis matrix, and the specific calculation.\n\n**1. Derivation of the Coordinate Transformation Rule**\n\nLet $y$ be an arbitrary vector in the vector space $\\mathbb{R}^{3}$.\nSince $\\mathcal{B} = \\{b_1, b_2, b_3\\}$ is a basis for $\\mathbb{R}^{3}$, there exists a unique set of scalars $c_1, c_2, c_3 \\in \\mathbb{R}$ such that\n$$ y = c_1 b_1 + c_2 b_2 + c_3 b_3 $$\nThese scalars form the coordinate vector of $y$ with respect to the basis $\\mathcal{B}$, denoted as $[y]_{\\mathcal{B}}$:\n$$ [y]_{\\mathcal{B}} = \\begin{pmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{pmatrix} $$\nSimilarly, since $\\mathcal{B}' = \\{b'_1, b'_2, b'_3\\}$ is also a basis for $\\mathbb{R}^{3}$, there exists another unique set of scalars $d_1, d_2, d_3 \\in \\mathbb{R}$ such that\n$$ y = d_1 b'_1 + d_2 b'_2 + d_3 b'_3 $$\nThese scalars form the coordinate vector $[y]_{\\mathcal{B}'}$:\n$$ [y]_{\\mathcal{B}'} = \\begin{pmatrix} d_1 \\\\ d_2 \\\\ d_3 \\end{pmatrix} $$\nThe problem states that the matrix $T$ represents the identity linear transformation $id: \\mathbb{R}^{3} \\to \\mathbb{R}^{3}$ where $id(y) = y$, with the domain space having basis $\\mathcal{B}$ and the codomain space having basis $\\mathcal{B}'$. By the definition of a matrix representation of a linear operator, the matrix $T$ transforms the coordinate vector of an input vector in the domain's basis to the coordinate vector of the output vector in the codomain's basis.\nTherefore, for any vector $y$, we have:\n$$ [id(y)]_{\\mathcal{B}'} = T [y]_{\\mathcal{B}} $$\nSince $id(y) = y$, the rule for changing coordinates from basis $\\mathcal{B}$ to basis $\\mathcal{B}'$ is:\n$$ [y]_{\\mathcal{B}'} = T [y]_{\\mathcal{B}} $$\nThis is the fundamental relationship for the change of basis. The columns of $T$ are the coordinate vectors of the old basis vectors (from $\\mathcal{B}$) expressed in the new basis ($\\mathcal{B}'$). That is, the $j$-th column of $T$ is the vector $[b_j]_{\\mathcal{B}'}$.\n\n**2. Invertibility of the Change-of-Basis Matrix $T$**\n\nA square matrix is invertible if and only if its columns are linearly independent. As established, the columns of $T$ are the coordinate vectors $[b_1]_{\\mathcal{B}'}$, $[b_2]_{\\mathcal{B}'}$, and $[b_3]_{\\mathcal{B}'}$. To prove that $T$ is invertible, we must show that these column vectors are linearly independent.\nConsider a linear combination of the columns of $T$ that equals the zero vector:\n$$ c_1 [b_1]_{\\mathcal{B}'} + c_2 [b_2]_{\\mathcal{B}'} + c_3 [b_3]_{\\mathcal{B}'} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe mapping from a vector to its coordinate representation is a linear transformation (an isomorphism). Thus, we can combine the terms on the left side:\n$$ [c_1 b_1 + c_2 b_2 + c_3 b_3]_{\\mathcal{B}'} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe only vector whose coordinate vector is the zero vector is the zero vector of the vector space itself. Therefore,\n$$ c_1 b_1 + c_2 b_2 + c_3 b_3 = \\mathbf{0} $$\nwhere $\\mathbf{0}$ is the zero vector in $\\mathbb{R}^3$.\nBy definition, a set of vectors is a basis only if its elements are linearly independent. Since $\\mathcal{B} = \\{b_1, b_2, b_3\\}$ is a basis, the vectors $b_1, b_2, b_3$ are linearly independent. This implies that the only solution to the equation above is the trivial solution, where all scalar coefficients are zero:\n$$ c_1 = 0, \\quad c_2 = 0, \\quad c_3 = 0 $$\nThis shows that the only linear combination of the columns of $T$ that yields the zero vector is the trivial combination. Hence, the columns of $T$ are linearly independent. A square matrix with linearly independent columns is invertible. Thus, the change-of-basis matrix $T$ from one basis to another must be invertible.\n\n**3. Computation of the Anatomical Region Basis Coordinates**\n\nWe are given the transformation rule $[y]_{\\mathcal{B}'} = T [y]_{\\mathcal{B}}$, the matrix $T$, and the coordinate vector $[y]_{\\mathcal{B}'}$. We need to find $[y]_{\\mathcal{B}}$. Since $T$ is invertible, we can multiply by its inverse $T^{-1}$ from the left:\n$$ T^{-1} [y]_{\\mathcal{B}'} = T^{-1} T [y]_{\\mathcal{B}} = I [y]_{\\mathcal{B}} = [y]_{\\mathcal{B}} $$\nSo, we must compute $[y]_{\\mathcal{B}} = T^{-1} [y]_{\\mathcal{B}'}$.\nFirst, we find the inverse of $T = \\begin{pmatrix} 2  -1  0 \\\\ 1  1  3 \\\\ 0  2  1 \\end{pmatrix}$.\nThe determinant of $T$ is:\n$$ \\det(T) = 2(1 \\cdot 1 - 3 \\cdot 2) - (-1)(1 \\cdot 1 - 3 \\cdot 0) + 0(1 \\cdot 2 - 1 \\cdot 0) $$\n$$ \\det(T) = 2(1 - 6) + 1(1) = 2(-5) + 1 = -10 + 1 = -9 $$\nSince $\\det(T) \\neq 0$, the inverse exists.\nThe inverse is given by $T^{-1} = \\frac{1}{\\det(T)} \\text{adj}(T)$, where $\\text{adj}(T)$ is the adjugate matrix (the transpose of the cofactor matrix).\nThe matrix of cofactors is:\n$$ C = \\begin{pmatrix} +|{1 \\atop 2} {3 \\atop 1}|  -|{1 \\atop 0} {3 \\atop 1}|  +|{1 \\atop 0} {1 \\atop 2}| \\\\ -|{-1 \\atop 2} {0 \\atop 1}|  +|{2 \\atop 0} {0 \\atop 1}|  -|{2 \\atop 0} {-1 \\atop 2}| \\\\ +|{-1 \\atop 1} {0 \\atop 3}|  -|{2 \\atop 1} {0 \\atop 3}|  +|{2 \\atop 1} {-1 \\atop 1}| \\end{pmatrix} = \\begin{pmatrix} -5  -1  2 \\\\ 1  2  -4 \\\\ -3  -6  3 \\end{pmatrix} $$\nThe adjugate matrix is the transpose of the cofactor matrix:\n$$ \\text{adj}(T) = C^T = \\begin{pmatrix} -5  1  -3 \\\\ -1  2  -6 \\\\ 2  -4  3 \\end{pmatrix} $$\nThe inverse matrix is:\n$$ T^{-1} = \\frac{1}{-9} \\begin{pmatrix} -5  1  -3 \\\\ -1  2  -6 \\\\ 2  -4  3 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 5  -1  3 \\\\ 1  -2  6 \\\\ -2  4  -3 \\end{pmatrix} $$\nNow we can calculate $[y]_{\\mathcal{B}}$:\n$$ [y]_{\\mathcal{B}} = T^{-1} [y]_{\\mathcal{B}'} = \\frac{1}{9} \\begin{pmatrix} 5  -1  3 \\\\ 1  -2  6 \\\\ -2  4  -3 \\end{pmatrix} \\begin{pmatrix} 9 \\\\ -9 \\\\ 18 \\end{pmatrix} $$\nFirst, we perform the matrix-vector multiplication:\n$$ \\begin{pmatrix} 5  -1  3 \\\\ 1  -2  6 \\\\ -2  4  -3 \\end{pmatrix} \\begin{pmatrix} 9 \\\\ -9 \\\\ 18 \\end{pmatrix} = \\begin{pmatrix} 5(9) + (-1)(-9) + 3(18) \\\\ 1(9) + (-2)(-9) + 6(18) \\\\ -2(9) + 4(-9) + (-3)(18) \\end{pmatrix} = \\begin{pmatrix} 45 + 9 + 54 \\\\ 9 + 18 + 108 \\\\ -18 - 36 - 54 \\end{pmatrix} = \\begin{pmatrix} 108 \\\\ 135 \\\\ -108 \\end{pmatrix} $$\nFinally, we multiply by the scalar $\\frac{1}{9}$:\n$$ [y]_{\\mathcal{B}} = \\frac{1}{9} \\begin{pmatrix} 108 \\\\ 135 \\\\ -108 \\end{pmatrix} = \\begin{pmatrix} \\frac{108}{9} \\\\ \\frac{135}{9} \\\\ \\frac{-108}{9} \\end{pmatrix} = \\begin{pmatrix} 12 \\\\ 15 \\\\ -12 \\end{pmatrix} $$\nThe coordinate vector of $y$ with respect to the anatomical basis $\\mathcal{B}$ is $\\begin{pmatrix} 12 \\\\ 15 \\\\ -12 \\end{pmatrix}$. The problem requests this answer as a row vector.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 12  15  -12 \\end{pmatrix} } $$"
        },
        {
            "introduction": "The General Linear Model (GLM) is a cornerstone of fMRI data analysis, but its power depends on a well-constructed design matrix. This practice problem  simulates a common experimental design flaw where the chosen regressors are not linearly independent, a situation known as perfect multicollinearity. By determining the rank of the design matrix, you will learn to diagnose this critical issue, which prevents the unique estimation of model parameters.",
            "id": "4175062",
            "problem": "A research team is modeling blood-oxygen-level-dependent responses in functional Magnetic Resonance Imaging (fMRI) using the General Linear Model (GLM), written as $Y = X\\beta + \\varepsilon$, where $Y \\in \\mathbb{R}^{n}$ is the observed signal, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^{p}$ are the parameters, and $\\varepsilon \\in \\mathbb{R}^{n}$ represents noise. The experiment consists of $n = 9$ consecutive scans with no resting baseline and three mutually exclusive task conditions (cognitive states) labeled $A$, $B$, and $C$. At each scan exactly one condition is active. The analyst constructs a design matrix $X$ with $p = 5$ columns: an intercept, three dummy-coded condition regressors (one each for $A$, $B$, and $C$), and a “task-on” regressor equal to the sum of the three dummy-coded condition regressors. All regressors are unit-amplitude boxcar columns, not convolved with any hemodynamic kernel.\n\nThe schedule assigns scans $1$–$3$ to condition $A$, scans $4$–$6$ to condition $B$, and scans $7$–$9$ to condition $C$. The resulting design matrix $X \\in \\mathbb{R}^{9 \\times 5}$ is\n$$\nX = \\begin{pmatrix}\n1  1  0  0  1 \\\\\n1  1  0  0  1 \\\\\n1  1  0  0  1 \\\\\n1  0  1  0  1 \\\\\n1  0  1  0  1 \\\\\n1  0  1  0  1 \\\\\n1  0  0  1  1 \\\\\n1  0  0  1  1 \\\\\n1  0  0  1  1\n\\end{pmatrix},\n$$\nwhere the first column is the intercept (all ones), columns $2$–$4$ are the dummy-coded regressors for conditions $A$, $B$, and $C$ respectively, and the fifth column is the “task-on” regressor defined as the sum of the three dummy-coded columns.\n\nUsing only the core definitions of linear independence, rank, and basis from linear algebra, and the GLM formulation above, determine the rank of $X$. Express your final answer as a single integer with no units. No rounding is required.",
            "solution": "The rank of a matrix is defined as the dimension of its column space, which is the vector space spanned by its columns. This dimension is equal to the maximum number of linearly independent columns in the matrix. The problem requires us to determine the rank of the design matrix $X \\in \\mathbb{R}^{9 \\times 5}$ given by:\n$$\nX = \\begin{pmatrix}\n1  1  0  0  1 \\\\\n1  1  0  0  1 \\\\\n1  1  0  0  1 \\\\\n1  0  1  0  1 \\\\\n1  0  1  0  1 \\\\\n1  0  1  0  1 \\\\\n1  0  0  1  1 \\\\\n1  0  0  1  1 \\\\\n1  0  0  1  1\n\\end{pmatrix}\n$$\nLet the columns of $X$ be denoted by the vectors $x_1, x_2, x_3, x_4, x_5 \\in \\mathbb{R}^{9}$, where the subscript corresponds to the column index. A set of vectors $\\{v_1, \\dots, v_k\\}$ is linearly dependent if there exist scalars $c_1, \\dots, c_k$, not all zero, such that the equation $c_1 v_1 + \\dots + c_k v_k = \\mathbf{0}$ is satisfied. The rank of $X$ is the size of the largest linearly independent subset of $\\{x_1, x_2, x_3, x_4, x_5\\}$.\n\nFirst, we identify any linear dependencies among the columns of $X$. By inspection of the matrix, the first column $x_1$ (the intercept) and the fifth column $x_5$ (the \"task-on\" regressor) are identical.\n$$\nx_1 = x_5\n$$\nThis relationship implies a linear dependency, which can be expressed as a non-trivial linear combination of the columns that equals the zero vector:\n$$\n(1) x_1 + (0) x_2 + (0) x_3 + (0) x_4 + (-1) x_5 = \\mathbf{0}\n$$\nSince the coefficients are not all zero, the set of all $5$ columns is linearly dependent. This proves that the rank of $X$ is less than $5$.\n\nFurthermore, the problem states that the fifth column is defined as the sum of the three dummy-coded condition regressors (columns $2, 3,$ and $4$). Let's verify this using the provided vectors:\n$$\nx_2 + x_3 + x_4 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe resulting vector is identical to the first column, $x_1$. Thus, we have a second linear dependency:\n$$\nx_1 = x_2 + x_3 + x_4\n$$\nThis can be written as another non-trivial linear combination that equals the zero vector:\n$$\n(-1) x_1 + (1) x_2 + (1) x_3 + (1) x_4 + (0) x_5 = \\mathbf{0}\n$$\nThese dependencies indicate that some columns are redundant for the purpose of spanning the column space of $X$, denoted $C(X)$.\nThe column space is $C(X) = \\text{span}\\{x_1, x_2, x_3, x_4, x_5\\}$.\nSince $x_5 = x_1$, the vector $x_5$ is redundant, and we can write:\n$$\nC(X) = \\text{span}\\{x_1, x_2, x_3, x_4\\}\n$$\nFurthermore, since $x_1 = x_2 + x_3 + x_4$, the vector $x_1$ is a linear combination of $x_2, x_3,$ and $x_4$. Thus, $x_1$ is also redundant in the spanning set:\n$$\nC(X) = \\text{span}\\{x_2, x_3, x_4\\}\n$$\nThe rank of $X$ is the dimension of this space. The dimension is equal to the number of vectors in a basis for the space. If the set $\\{x_2, x_3, x_4\\}$ is linearly independent, it forms a basis for $C(X)$. We must test for linear independence by seeking scalars $c_2, c_3, c_4$ such that:\n$$\nc_2 x_2 + c_3 x_3 + c_4 x_4 = \\mathbf{0}\n$$\nSubstituting the vector components:\n$$\nc_2 \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + c_3 \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + c_4 \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis single vector equation is equivalent to the following system of scalar equations:\n$$\n\\begin{pmatrix} c_2 \\\\ c_2 \\\\ c_2 \\\\ c_3 \\\\ c_3 \\\\ c_3 \\\\ c_4 \\\\ c_4 \\\\ c_4 \\end{pmatrix} = \\mathbf{0}\n$$\nFrom the first $3$ rows, we deduce $c_2 = 0$. From rows $4, 5,$ and $6$, we deduce $c_3 = 0$. From rows $7, 8,$ and $9$, we deduce $c_4 = 0$. The only solution is the trivial solution, $c_2 = 0, c_3 = 0, c_4 = 0$.\nTherefore, the set of vectors $\\{x_2, x_3, x_4\\}$ is linearly independent.\n\nSince the set $\\{x_2, x_3, x_4\\}$ is linearly independent and it spans the column space $C(X)$, it constitutes a basis for $C(X)$. By definition, the rank of a matrix is the number of vectors in any basis for its column space. The basis we have found, $\\{x_2, x_3, x_4\\}$, contains $3$ vectors.\n\nThus, the rank of the matrix $X$ is $3$. This result reveals that the design matrix exhibits perfect multicollinearity, a condition where the rank ($3$) is less than the number of columns ($p=5$). In the context of the GLM, this means that the matrix $X^T X$ is singular, and a unique Ordinary Least Squares solution for the parameter vector $\\beta$ cannot be determined.",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "Neural features recorded simultaneously are often correlated, complicating interpretation and modeling. A powerful technique to address this is to find a new basis in which the features are decorrelated, which lies at the heart of Principal Component Analysis (PCA). This exercise  will guide you to construct a specific change of basis—a rotation—that diagonalizes a covariance matrix, providing a first-principles understanding of how to align a coordinate system with the intrinsic variance of your data.",
            "id": "4175038",
            "problem": "You analyze trial-averaged neural features from a single cortical site during a sensory decoding experiment. On each of $T$ trials, you record a feature vector $x \\in \\mathbb{R}^{2}$ whose entries are: $x_{1}$, the mean spike count in a fixed post-stimulus window, and $x_{2}$, the mean local field potential amplitude in the same window. After centering the features so that their empirical means over trials are zero, you compute the sample covariance matrix\n$$\nC \\;=\\; \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix}.\n$$\nUsing only core linear algebra and probability definitions, construct an orthogonal change of basis that diagonalizes $C$ so that the transformed features are decorrelated. Specifically, consider a rotation by angle $\\theta$, represented by the orthogonal matrix\n$$\nR(\\theta) \\;=\\; \\begin{pmatrix} \\cos\\theta  -\\sin\\theta \\\\ \\sin\\theta  \\cos\\theta \\end{pmatrix},\n$$\nand derive, from first principles, the condition on $\\theta$ under which the covariance matrix in the rotated coordinates, $C' \\;=\\; R(\\theta)^{\\top} C \\, R(\\theta)$, is diagonal. Interpret the columns of $R(\\theta)$ as basis vectors for the feature space and explain how decorrelation corresponds to selecting a basis of linearly independent directions aligned with the covariance structure of the data.\n\nFinally, for the specific matrix $C$ above, determine the unique rotation angle $\\theta \\in \\left(0, \\frac{\\pi}{4}\\right)$ (in radians) that achieves decorrelation. Express your final answer as an exact analytic expression in radians. Do not approximate or round the result. The final answer must be only the value of $\\theta$.",
            "solution": "The problem requires us to find an orthogonal change of basis, specifically a rotation, that diagonalizes a given sample covariance matrix $C$. This process is equivalent to finding the principal components of the data, which are the directions of maximum variance. Decorrelating the features means transforming them into a new coordinate system where their covariance is zero.\n\nLet the original covariance matrix be denoted generally as:\n$$\nC = \\begin{pmatrix} C_{11}  C_{12} \\\\ C_{21}  C_{22} \\end{pmatrix}\n$$\nwhere $C_{12} = C_{21}$ due to the symmetry of the covariance matrix. The transformed covariance matrix in the rotated coordinate system is $C' = R(\\theta)^{\\top} C R(\\theta)$. The transpose of the rotation matrix $R(\\theta)$ is:\n$$\nR(\\theta)^{\\top} = \\begin{pmatrix} \\cos\\theta  \\sin\\theta \\\\ -\\sin\\theta  \\cos\\theta \\end{pmatrix}\n$$\nWe perform the matrix multiplication to find the entries of $C'$. Let's\nuse the shorthand $c = \\cos\\theta$ and $s = \\sin\\theta$.\n$$\nC' = \\begin{pmatrix} c  s \\\\ -s  c \\end{pmatrix} \\begin{pmatrix} C_{11}  C_{12} \\\\ C_{12}  C_{22} \\end{pmatrix} \\begin{pmatrix} c  -s \\\\ s  c \\end{pmatrix}\n$$\nFirst, we compute the product $C R(\\theta)$:\n$$\nC R(\\theta) = \\begin{pmatrix} C_{11}c + C_{12}s  -C_{11}s + C_{12}c \\\\ C_{12}c + C_{22}s  -C_{12}s + C_{22}c \\end{pmatrix}\n$$\nNext, we pre-multiply by $R(\\theta)^{\\top}$:\n$$\nC' = \\begin{pmatrix} c(C_{11}c + C_{12}s) + s(C_{12}c + C_{22}s)  c(-C_{11}s + C_{12}c) + s(-C_{12}s + C_{22}c) \\\\ -s(C_{11}c + C_{12}s) + c(C_{12}c + C_{22}s)  -s(-C_{11}s + C_{12}c) + c(-C_{12}s + C_{22}c) \\end{pmatrix}\n$$\nFor the transformed features to be decorrelated, the off-diagonal elements of $C'$ must be zero. Let's focus on the element $C'_{12}$:\n$$\nC'_{12} = c(-C_{11}s + C_{12}c) + s(-C_{12}s + C_{22}c)\n$$\n$$\nC'_{12} = -C_{11}sc + C_{12}c^2 - C_{12}s^2 + C_{22}sc\n$$\n$$\nC'_{12} = (C_{22} - C_{11})sc + C_{12}(c^2 - s^2)\n$$\nUsing the double-angle trigonometric identities $\\sin(2\\theta) = 2\\sin\\theta\\cos\\theta = 2sc$ and $\\cos(2\\theta) = \\cos^2\\theta - \\sin^2\\theta = c^2 - s^2$, we can rewrite the expression for $C'_{12}$:\n$$\nC'_{12} = (C_{22} - C_{11})\\frac{\\sin(2\\theta)}{2} + C_{12}\\cos(2\\theta)\n$$\nThe condition for decorrelation is $C'_{12} = 0$:\n$$\nC_{12}\\cos(2\\theta) = -(C_{22} - C_{11})\\frac{\\sin(2\\theta)}{2} = (C_{11} - C_{22})\\frac{\\sin(2\\theta)}{2}\n$$\nAssuming $\\cos(2\\theta) \\neq 0$ and $C_{11} \\neq C_{22}$, we can rearrange this to find the condition on $\\theta$:\n$$\n\\frac{\\sin(2\\theta)}{\\cos(2\\theta)} = \\tan(2\\theta) = \\frac{2C_{12}}{C_{11} - C_{22}}\n$$\nThis is the general condition on the rotation angle $\\theta$ that diagonalizes any $2 \\times 2$ symmetric matrix $C$.\n\nNow, for the interpretation. The columns of the rotation matrix $R(\\theta)$ are the new basis vectors for the feature space:\n$$\nv_1 = \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}, \\quad v_2 = \\begin{pmatrix} -\\sin\\theta \\\\ \\cos\\theta \\end{pmatrix}\n$$\nThese vectors are orthogonal and have unit length, forming an orthonormal basis. The condition we derived for $\\theta$ is precisely the condition that makes these basis vectors the eigenvectors of the covariance matrix $C$. When the basis vectors are eigenvectors of $C$, the matrix representation of the linear transformation defined by $C$ in this new basis is diagonal. The diagonal entries are the eigenvalues of $C$, which represent the variance of the data along the new basis directions (the principal components). By aligning the basis vectors with the principal axes of the data's covariance structure (the directions of maximal and minimal variance), we ensure that the projections of the data onto these new axes are uncorrelated. Thus, decorrelation corresponds to selecting a basis of linearly independent vectors that are the eigenvectors of the covariance matrix.\n\nFinally, we apply this result to the specific covariance matrix given:\n$$\nC = \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix}\n$$\nHere, $C_{11} = 5$, $C_{22} = 1$, and $C_{12} = 2$.\nSubstituting these values into our derived condition:\n$$\n\\tan(2\\theta) = \\frac{2 C_{12}}{C_{11} - C_{22}} = \\frac{2(2)}{5 - 1} = \\frac{4}{4} = 1\n$$\nWe need to solve $\\tan(2\\theta) = 1$ for $\\theta \\in \\left(0, \\frac{\\pi}{4}\\right)$.\nThe general solution for $2\\theta$ is $2\\theta = \\arctan(1) + n\\pi$ for any integer $n$. The principal value of $\\arctan(1)$ is $\\frac{\\pi}{4}$.\nSo, $2\\theta = \\frac{\\pi}{4} + n\\pi$.\nThis gives $\\theta = \\frac{\\pi}{8} + \\frac{n\\pi}{2}$.\nWe must find the unique solution in the interval $\\theta \\in \\left(0, \\frac{\\pi}{4}\\right)$.\n- If $n = 0$, $\\theta = \\frac{\\pi}{8}$. Since $0  \\frac{\\pi}{8}  \\frac{2\\pi}{8} = \\frac{\\pi}{4}$, this solution is in the specified interval.\n- If $n = 1$, $\\theta = \\frac{\\pi}{8} + \\frac{\\pi}{2} = \\frac{5\\pi}{8}$, which is greater than $\\frac{\\pi}{4}$.\n- If $n = -1$, $\\theta = \\frac{\\pi}{8} - \\frac{\\pi}{2} = -\\frac{3\\pi}{8}$, which is less than $0$.\nTherefore, the unique angle in the given range is $\\frac{\\pi}{8}$.\nThis angle corresponds to a rotation that aligns the new basis vectors with the eigenvectors of $C$, thereby diagonalizing $C$ and decorrelating the transformed features.",
            "answer": "$$\\boxed{\\frac{\\pi}{8}}$$"
        }
    ]
}