{
    "hands_on_practices": [
        {
            "introduction": "In many data acquisition processes, such as the line-by-line scanning in two-photon microscopy, geometric distortions can arise that are not simple rotations or scalings. This exercise explores the fundamental nature of one such distortion: a shear transformation. By analyzing the spectral properties of a shear matrix, you will discover why it cannot be diagonalized and instead requires the computation of its Jordan normal form, providing a canonical representation for this important class of non-rigid transformations. ",
            "id": "4177175",
            "problem": "A two-dimensional slice of a calcium imaging plane obtained via two-photon microscopy (2PM) is post-processed to correct for line-by-line drift. A prevalent artifact is a horizontal shear induced by the scanning trajectory: each row is shifted by an amount that is proportional to its vertical coordinate. In a canonical basis, this preprocessing step is modeled by a linear transformation acting on planar coordinates $x$ and $y$ via\n$$\n\\begin{pmatrix}\nx' \\\\ y'\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  \\alpha \\\\\n0  1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\ y\n\\end{pmatrix},\n$$\nwhere $\\alpha \\neq 0$ is a dimensionless shear parameter estimated from the data. Treating the transformation as a linear operator on $\\mathbb{R}^{2}$, analyze its spectral structure using only the definitions of eigenvalues, eigenvectors, generalized eigenvectors, and the Jordan normal form.\n\nStarting from the definitions:\n- An eigenvalue $\\lambda$ of a linear operator $A$ satisfies $\\det(A - \\lambda I) = 0$, and a corresponding eigenvector $v \\neq \\mathbf{0}$ satisfies $(A - \\lambda I)v = \\mathbf{0}$.\n- A generalized eigenvector $w$ of rank $2$ satisfies $(A - \\lambda I)w = v$, where $v$ is an eigenvector.\n- The Jordan normal form of $A$ is a block-diagonal matrix with Jordan blocks corresponding to each eigenvalue, obtained via a similarity transformation $A = P J P^{-1}$ where the columns of $P$ form chains of generalized eigenvectors.\n\nFor the shear matrix\n$$\nS(\\alpha) = \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix},\n$$\ndemonstrate whether the eigenbasis is defective and compute the Jordan normal form $J$ explicitly. Construct a valid chain of generalized eigenvectors and exhibit a similarity matrix $P$ such that $S(\\alpha) = P J P^{-1}$, verifying the decomposition from first principles. As your final answer, report the Jordan normal form matrix $J$ as a single analytic expression. No rounding is required, and no units are involved. Express any angles, if they appear, in radians.",
            "solution": "The problem requires an analysis of the spectral structure of the horizontal shear matrix $S(\\alpha) = \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix}$ for $\\alpha \\neq 0$, culminating in its Jordan normal form. The analysis will proceed from the fundamental definitions of eigenvalues and eigenvectors.\n\nFirst, we find the eigenvalues of $S(\\alpha)$ by solving the characteristic equation $\\det(S(\\alpha) - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\n\\det\\left( \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\right) = \\det \\begin{pmatrix} 1-\\lambda  \\alpha \\\\ 0  1-\\lambda \\end{pmatrix} = 0\n$$\nThe determinant is $(1-\\lambda)^2 - (\\alpha)(0) = (1-\\lambda)^2$.\nSetting this to zero yields $(1-\\lambda)^2 = 0$, which has a single solution $\\lambda = 1$. This eigenvalue has an algebraic multiplicity of $2$.\n\nNext, we find the eigenvectors corresponding to the eigenvalue $\\lambda = 1$ by solving the equation $(S(\\alpha) - \\lambda I)v = \\mathbf{0}$ for a non-zero vector $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$.\n$$\n(S(\\alpha) - 1 \\cdot I)v = \\begin{pmatrix} 1-1  \\alpha \\\\ 0  1-1 \\end{pmatrix} \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix} = \\begin{pmatrix} 0  \\alpha \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis matrix-vector equation corresponds to the linear system:\n$$\n\\begin{cases}\n0 \\cdot v_x + \\alpha \\cdot v_y = 0 \\\\\n0 \\cdot v_x + 0 \\cdot v_y = 0\n\\end{cases}\n$$\nThe second equation is trivial ($0=0$). The first equation is $\\alpha v_y = 0$. Since the problem statement specifies that $\\alpha \\neq 0$, we must have $v_y = 0$. The component $v_x$ is a free variable. Thus, any eigenvector must be of the form $v = \\begin{pmatrix} k \\\\ 0 \\end{pmatrix}$ for some scalar $k \\neq 0$.\nThe eigenspace associated with $\\lambda=1$ is the set of all vectors of this form, which is spanned by the single vector $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. The dimension of this eigenspace is $1$, which means the geometric multiplicity of the eigenvalue $\\lambda=1$ is $1$.\n\nThe algebraic multiplicity of $\\lambda=1$ is $2$, while its geometric multiplicity is $1$. Since the geometric multiplicity is less than the algebraic multiplicity, there is no basis of eigenvectors for $\\mathbb{R}^2$. Therefore, the matrix $S(\\alpha)$ is defective and is not diagonalizable.\n\nSince $S(\\alpha)$ is a $2 \\times 2$ matrix with one eigenvalue $\\lambda=1$ of algebraic multiplicity $2$ and geometric multiplicity $1$, its Jordan normal form $J$ must consist of a single Jordan block of size $2$ corresponding to $\\lambda=1$. This block has the form $\\begin{pmatrix} \\lambda  1 \\\\ 0  \\lambda \\end{pmatrix}$.\nThus, the Jordan normal form of $S(\\alpha)$ is:\n$$\nJ = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}\n$$\nTo find the similarity transformation $S(\\alpha) = P J P^{-1}$, we must construct the matrix $P$ from a chain of generalized eigenvectors. Let the chain be $\\{v_1, v_2\\}$, where $v_1$ is an eigenvector and $v_2$ is a generalized eigenvector of rank $2$. These vectors are defined by the relations:\n$$\n(S(\\alpha) - I) v_1 = \\mathbf{0}\n$$\n$$\n(S(\\alpha) - I) v_2 = v_1\n$$\nWe need to find a vector $v_2$ that is not an eigenvector itself, but whose image under the transformation $(S(\\alpha)-I)$ is an eigenvector. A generalized eigenvector $v_2$ belongs to the null space of $(S(\\alpha)-I)^2$ but not to the null space of $(S(\\alpha)-I)$.\nFirst, we compute $(S(\\alpha)-I)^2$:\n$$\n(S(\\alpha)-I)^2 = \\begin{pmatrix} 0  \\alpha \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  \\alpha \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}\n$$\nSince $(S(\\alpha)-I)^2$ is the zero matrix, its null space is the entire space $\\mathbb{R}^2$. We can choose any vector $v_2$ that is not an eigenvector (i.e., not a multiple of $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$). A simple choice is $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nNow, we find $v_1$ using the chain definition:\n$$\nv_1 = (S(\\alpha) - I) v_2 = \\begin{pmatrix} 0  \\alpha \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix}\n$$\nThis vector $v_1 = \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix}$ is indeed a valid eigenvector, as it is a non-zero multiple of $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe chain of generalized eigenvectors is $\\{v_1, v_2\\} = \\left\\{ \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right\\}$.\n\nThe similarity matrix $P$ is formed by taking these chain vectors as its columns, in order:\n$$\nP = \\begin{pmatrix} v_1  v_2 \\end{pmatrix} = \\begin{pmatrix} \\alpha  0 \\\\ 0  1 \\end{pmatrix}\n$$\nTo verify the decomposition $S(\\alpha) = P J P^{-1}$, we first compute the inverse of $P$. Since $P$ is a diagonal matrix, its inverse is straightforward:\n$$\nP^{-1} = \\begin{pmatrix} \\frac{1}{\\alpha}  0 \\\\ 0  1 \\end{pmatrix}\n$$\nNow we compute the product $P J P^{-1}$:\n$$\nP J P^{-1} = \\begin{pmatrix} \\alpha  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\alpha}  0 \\\\ 0  1 \\end{pmatrix}\n$$\nFirst, multiply $P$ and $J$:\n$$\nP J = \\begin{pmatrix} \\alpha  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} (\\alpha)(1) + (0)(0)  (\\alpha)(1) + (0)(1) \\\\ (0)(1) + (1)(0)  (0)(1) + (1)(1) \\end{pmatrix} = \\begin{pmatrix} \\alpha  \\alpha \\\\ 0  1 \\end{pmatrix}\n$$\nNow, right-multiply by $P^{-1}$:\n$$\n(P J) P^{-1} = \\begin{pmatrix} \\alpha  \\alpha \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\alpha}  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} (\\alpha)(\\frac{1}{\\alpha}) + (\\alpha)(0)  (\\alpha)(0) + (\\alpha)(1) \\\\ (0)(\\frac{1}{\\alpha}) + (1)(0)  (0)(0) + (1)(1) \\end{pmatrix} = \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix}\n$$\nThis resulting matrix is indeed $S(\\alpha)$, which verifies the decomposition. The spectral analysis is complete, and the Jordan normal form $J$ has been explicitly computed and verified.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1 \\\\\n0  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Neural data often undergoes preprocessing steps that apply complex linear transformations, mixing rotation, scaling, and shear. The QR decomposition is a powerful and numerically stable technique to disentangle these components, separating a general matrix $A$ into an orthonormal part $Q$ (rotation or reflection) and an upper-triangular part $R$ (scaling and shear). This computational practice demonstrates how to extract and interpret the geometric meaning from a general linear operator and will provide critical insight into the importance of numerical stability when analyzing real-world, and potentially ill-conditioned, data. ",
            "id": "4177165",
            "problem": "A multivariate neural feature preprocessing step often applies a general linear operator that mixes, scales, and shears recorded features before downstream analysis. Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ that linearly maps $n$ neural features (columns) into $m$ observation channels (rows). The goal is to express $A$ as the product $A = Q R$ where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ is upper triangular, then interpret $Q$ as an orthonormal basis change capturing rotation or reflection, and interpret $R$ as encoding scaling and shear. Starting from the fundamental definitions of the Euclidean inner product, norm, orthogonality, and the construction of orthonormal bases in finite-dimensional inner product spaces, use a decomposition strategy grounded in these principles to extract an orthonormal basis change from $A$ and quantify how stable that extraction is under different algorithms.\n\nYour program must:\n- For each test matrix $A$, compute a decomposition $A = Q R$ with $Q$ having orthonormal columns and $R$ upper triangular; enforce the convention that all diagonal entries of $R$ are nonnegative by adjusting signs consistently so that the decomposition is unique under this convention.\n- Interpret $Q$ as an orthonormal basis change. For square $A$ (i.e., $m = n$), classify whether $Q$ represents a proper rotation ($\\det(Q)  0$) versus an improper rotation/reflection ($\\det(Q)  0$). For rectangular $A$ (i.e., $m \\neq n$), set this classification to the Boolean value $False$ because a determinant is not defined.\n- Interpret $R$ as encoding scaling and shear relative to the orthonormal basis given by $Q$. Quantify a shear-to-scale ratio defined as the sum of absolute values of the strictly upper-triangular entries of $R$ divided by the sum of absolute values of the diagonal entries of $R$; explicitly,\n$$\n\\text{shear\\_ratio}(R) = \\frac{\\sum_{1 \\leq i  j \\leq n} |R_{ij}|}{\\sum_{i=1}^{n} |R_{ii}|}.\n$$\n- Quantify numerical stability for extracting the orthonormal basis by comparing three algorithms grounded in the inner product structure: Classical Gram-Schmidt, Modified Gram-Schmidt, and Householder-based $QR$. For each algorithm, measure the Frobenius-norm orthogonality defect\n$$\n\\|Q^\\top Q - I_n\\|_F,\n$$\nand for the Householder-based $QR$, also measure the reconstruction residual\n$$\n\\|A - Q R\\|_F.\n$$\n- Report the $2$-norm condition number of $A$ defined by the ratio of the largest to smallest singular value.\n\nTest suite:\nProvide results for the following $5$ matrices, which cover typical and edge cases relevant to neuroscience data analysis and matrix transformations (rotation, scaling, shear):\n1. $A_1 \\in \\mathbb{R}^{2 \\times 2}$ constructed as a rotation by angle $\\theta = 0.3$ (in radians), followed by anisotropic scaling by $\\operatorname{diag}(3.0, 1.0)$, followed by an upper-triangular shear with parameter $s = 0.2$. Explicitly, $A_1 = R(\\theta) \\operatorname{diag}(3.0, 1.0) \\begin{pmatrix} 1  0.2 \\\\ 0  1 \\end{pmatrix}$, where $R(\\theta) = \\begin{pmatrix} \\cos(\\theta)  -\\sin(\\theta) \\\\ \\sin(\\theta)  \\cos(\\theta) \\end{pmatrix}$.\n2. $A_2 \\in \\mathbb{R}^{3 \\times 3}$ constructed as a rotation about the $z$-axis by angle $\\phi = 0.5$ (in radians), followed by scaling by $\\operatorname{diag}(10.0, 0.1, 1.0)$, followed by an upper-triangular shear $\\begin{pmatrix} 1  5  -2 \\\\ 0  1  3 \\\\ 0  0  1 \\end{pmatrix}$.\n3. $A_3 \\in \\mathbb{R}^{3 \\times 3}$ nearly rank-deficient and ill-conditioned, with columns that are almost collinear: \n$$\nA_3 = \\begin{pmatrix}\n1.0  1.0  0.0 \\\\\n10^{-8}  2 \\cdot 10^{-8}  0.0 \\\\\n0.0  0.0  10^{-12}\n\\end{pmatrix}.\n$$\n4. $A_4 \\in \\mathbb{R}^{5 \\times 3}$ tall random matrix with column-wise dynamic range, constructed by sampling independent standard normal entries and scaling columns by $10^{3}$, $10^{-3}$, and $1.0$; use a fixed random seed for reproducibility.\n5. $A_5 \\in \\mathbb{R}^{4 \\times 4}$ nearly singular with strong shear: construct an orthonormal matrix $Q_0$ by applying $QR$ to a random matrix (fixed seed), then set $A_5 = Q_0 \\operatorname{diag}(1.0, 10^{-6}, 10^{-9}, 1.0) S$ where $S$ is upper-triangular with large off-diagonals, specifically $S_{0,1} = 1000.0$, $S_{1,2} = 500.0$, $S_{2,3} = -300.0$, other off-diagonals zero, and diagonal entries equal to $1.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of per-case result lists, enclosed in square brackets. For each test matrix $A$, output the list\n$$\n[\\text{proper\\_rotation\\_flag}, \\text{shear\\_ratio}, \\|Q^\\top Q - I_n\\|_F^{\\text{Householder}}, \\|Q^\\top Q - I_n\\|_F^{\\text{Classical}}, \\|Q^\\top Q - I_n\\|_F^{\\text{Modified}}, \\|A - Q R\\|_F^{\\text{Householder}}, \\kappa_2(A)],\n$$\nwhere $\\kappa_2(A)$ denotes the $2$-norm condition number. The Boolean $\\text{proper\\_rotation\\_flag}$ is $True$ if $\\det(Q)  0$ for square $A$, otherwise $False$. Angles are not part of the required outputs, so no angle unit is needed. The single printed line must look like\n$$\n[[b_1, r_1, e_{Q,1}^{H}, e_{Q,1}^{C}, e_{Q,1}^{M}, e_{A,1}^{H}, \\kappa_2(A_1)], \\ldots, [b_5, r_5, e_{Q,5}^{H}, e_{Q,5}^{C}, e_{Q,5}^{M}, e_{A,5}^{H}, \\kappa_2(A_5)]],\n$$\nwith numerical values in decimal form and Boolean values $True$ or $False$.",
            "solution": "The problem requires us to decompose a given real matrix $A \\in \\mathbb{R}^{m \\times n}$ into the product $A = QR$, where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix. This decomposition is central to numerical linear algebra and has a profound physical and geometric interpretation. It separates a general linear transformation $A$ into an orthonormal component $Q$, which represents a rotation or reflection, and a component $R$ that represents scaling and shearing with respect to the new orthonormal basis defined by the columns of $Q$.\n\nThe theoretical foundation for this decomposition is the Gram-Schmidt orthogonalization process. Given a set of linearly independent vectors $\\{a_1, a_2, \\ldots, a_n\\}$, which we take as the columns of the matrix $A$, the process constructs a set of orthonormal vectors $\\{q_1, q_2, \\ldots, q_n\\}$ that span the same subspaces. Specifically, for any $k \\in \\{1, \\ldots, n\\}$, the span of the first $k$ columns of $A$ is identical to the span of the first $k$ columns of $Q$:\n$$\n\\text{span}\\{a_1, \\ldots, a_k\\} = \\text{span}\\{q_1, \\ldots, q_k\\}\n$$\nThis implies that each vector $a_k$ can be written as a linear combination of the vectors $\\{q_1, \\ldots, q_k\\}$. Since the vectors $\\{q_1, \\ldots, q_n\\}$ are orthonormal, the coefficients of this expansion are given by inner products. The relationship is captured by the matrix equation $A = QR$. The columns of $A$ are related to the columns of $Q$ by:\n$$\na_k = \\sum_{i=1}^{k} R_{ik} q_i\n$$\nThis equation reveals why $R$ must be upper triangular: $a_k$ depends only on the first $k$ vectors of the orthonormal basis, so $R_{ik} = 0$ for $i  k$. The coefficients are $R_{ik} = \\langle q_i, a_k \\rangle$ for $i \\le k$.\n\nTo ensure the decomposition is unique for any full-rank matrix $A$, we enforce the convention that all diagonal entries of $R$ must be non-negative, $R_{ii} \\ge 0$. If any $R_{ii}$ resulting from a computation is negative, we can multiply the $i$-th column of $Q$ and the $i$-th row of $R$ by $-1$. This preserves the product $QR = (Q D)(D R) = A$, where $D$ is a diagonal matrix with entries $\\pm 1$, and enforces the sign convention.\n\nWe will analyze three distinct algorithms for computing the QR decomposition:\n\n1.  **Classical Gram-Schmidt (CGS):** This algorithm directly implements the mathematical definition. For each column $a_k$ of $A$, it computes the projection onto the subspace spanned by the previously computed orthonormal vectors $\\{q_1, \\ldots, q_{k-1}\\}$ and subtracts this projection to find an orthogonal vector, which is then normalized. The orthogonal vector $u_k$ is computed as $u_k = a_k - \\sum_{i=1}^{k-1} \\langle q_i, a_k \\rangle q_i$, and then $q_k = u_k / \\|u_k\\|$. While mathematically sound, CGS is notoriously unstable numerically. In finite-precision arithmetic, the computed $q_i$ vectors gradually lose their mutual orthogonality. When $a_k$ is nearly linearly dependent on the previous columns, the subtraction involves nearly equal numbers, leading to catastrophic cancellation and a resulting $q_k$ that is far from orthogonal to the other vectors.\n\n2.  **Modified Gram-Schmidt (MGS):** This is an algebraic rearrangement of CGS that is far more stable. Instead of projecting $a_k$ onto each $q_i$ individually, MGS iteratively modifies the remaining columns of $A$. After computing $q_k$, it immediately subtracts its component from all subsequent columns $\\{a_{k+1}, \\ldots, a_n\\}$. This process ensures that at each step, the vector being orthogonalized is already orthogonal to the previous $q_i$ vectors *to machine precision*, mitigating the loss of orthogonality that plagues CGS.\n\n3.  **Householder QR:** This method takes a different approach based on geometric reflections. It applies a sequence of orthogonal transformations, called Householder reflectors, to the matrix $A$ to introduce zeros below the main diagonal. A Householder reflector is a matrix of the form $H = I - 2vv^\\top/\\|v\\|^2_2$, which reflects vectors across the hyperplane orthogonal to $v$. Such transformations are perfectly orthogonal. The product of these reflectors forms the final $Q$ matrix (or its transpose, depending on the implementation details), and the resulting upper triangular matrix is $R$. This method is the standard for its excellent numerical stability, even for ill-conditioned matrices.\n\nTo evaluate these algorithms, we use several metrics:\n- **Condition Number $\\kappa_2(A)$:** Defined as the ratio of the largest to the smallest singular value of $A$, $\\kappa_2(A) = \\sigma_{\\max}/\\sigma_{\\min}$. It quantifies the sensitivity of the matrix to perturbations. A large condition number signifies an ill-conditioned matrix, for which numerical algorithms are expected to struggle.\n- **Orthogonality Defect $\\|Q^\\top Q - I_n\\|_F$:** The Frobenius norm of the difference between $Q^\\top Q$ and the identity matrix $I_n$. For a perfect orthonormal basis $Q$, this value would be zero. In practice, it measures how much the computed $Q$ deviates from orthogonality due to floating-point errors. We expect this defect to be small (near machine epsilon) for MGS and Householder, but potentially large for CGS on ill-conditioned matrices.\n- **Reconstruction Residual $\\|A - QR\\|_F$:** This measures how accurately the computed factors reconstruct the original matrix. All three algorithms typically exhibit a small reconstruction residual, as this property is less sensitive to the loss of orthogonality in $Q$ than the orthogonality defect itself.\n\nThe interpretation of the decomposition is as follows:\n- **$Q$ (Orthonormal Transformation):** For a square matrix $A \\in \\mathbb{R}^{n \\times n}$, $Q$ is an orthogonal matrix. Its determinant is either $+1$ (a proper rotation, preserving orientation) or $-1$ (an improper rotation/reflection, reversing orientation). For a rectangular matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m  n$, $Q$ is not square and lacks a determinant, so this classification is not applicable.\n- **$R$ (Scaling and Shear):** The diagonal entries $R_{ii}$ correspond to scaling factors along the new basis directions defined by the columns of $Q$. The strictly upper-triangular entries $R_{ij}$ for $i  j$ represent shear, indicating how the $j$-th basis vector is distorted along the direction of the $i$-th basis vector. The `shear_ratio` provides a single scalar metric to quantify the magnitude of shear relative to scaling.\n\nThe solution proceeds by constructing the specified test matrices, applying each of the three QR algorithms, enforcing the uniqueness convention, and then computing the required interpretive and stability metrics.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef make_qr_unique(Q, R):\n    \"\"\"Enforce R's diagonal entries to be non-negative for a unique QR.\"\"\"\n    m, n = Q.shape\n    if R.shape[1]  n: # R might be smaller in some cases\n        return Q, R\n        \n    signs = np.sign(np.diag(R))\n    signs[signs == 0] = 1.0\n    D = np.diag(signs)\n    \n    Q_corrected = Q @ D\n    R_corrected = D @ R\n    \n    return Q_corrected, R_corrected\n\ndef cgs_qr(A):\n    \"\"\"Classical Gram-Schmidt QR decomposition.\"\"\"\n    m, n = A.shape\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    \n    for j in range(n):\n        u_j = A[:, j].copy()\n        \n        # First, compute all projection coefficients using the original A[:, j]\n        for i in range(j):\n            R[i, j] = Q[:, i].T @ A[:, j]\n        \n        # Second, subtract all the corresponding projections\n        for i in range(j):\n            u_j -= R[i, j] * Q[:, i]\n            \n        norm_u = np.linalg.norm(u_j)\n        if norm_u  1e-15:\n            R[j, j] = norm_u\n            Q[:, j] = u_j / norm_u\n        # else, leave column as zero, indicating rank deficiency\n        \n    return Q, R\n\ndef mgs_qr(A):\n    \"\"\"Modified Gram-Schmidt QR decomposition.\"\"\"\n    m, n = A.shape\n    V = A.copy()\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    \n    for i in range(n):\n        norm_v = np.linalg.norm(V[:, i])\n        R[i, i] = norm_v\n        if norm_v  1e-15:\n            Q[:, i] = V[:, i] / norm_v\n        else:\n            Q[:, i] = 0.0 # Rank deficient\n        \n        for j in range(i + 1, n):\n            R[i, j] = Q[:, i].T @ V[:, j]\n            V[:, j] -= R[i, j] * Q[:, i]\n            \n    return Q, R\n\ndef analyze_matrix(A):\n    \"\"\"\n    Computes all required metrics for a given matrix A.\n    \"\"\"\n    m, n = A.shape\n\n    # 1. Condition Number\n    try:\n        s = np.linalg.svd(A, compute_uv=False)\n        cond_num = s[0] / s[-1] if s[-1]  1e-15 else np.inf\n    except np.linalg.LinAlgError:\n        cond_num = np.inf\n\n    # 2. Householder QR (using NumPy's stable implementation)\n    Q_h, R_h_raw = np.linalg.qr(A) # 'reduced' is default for m=n\n    Q_h, R_h = make_qr_unique(Q_h, R_h_raw)\n\n    # 3. Classical Gram-Schmidt\n    Q_c, R_c_raw = cgs_qr(A)\n    Q_c, R_c = make_qr_unique(Q_c, R_c_raw)\n\n    # 4. Modified Gram-Schmidt\n    Q_m, R_m_raw = mgs_qr(A)\n    Q_m, R_m = make_qr_unique(Q_m, R_m_raw)\n\n    # 5. proper_rotation_flag (using stable Householder result)\n    if m == n:\n        try:\n            det_q = np.linalg.det(Q_h)\n            proper_rot_flag = det_q  0\n        except np.linalg.LinAlgError:\n            proper_rot_flag = False\n    else:\n        proper_rot_flag = False\n\n    # 6. shear_ratio (using stable Householder result)\n    r_diag_sum = np.sum(np.abs(np.diag(R_h)))\n    if r_diag_sum  1e-15:\n        shear_ratio = np.inf\n    else:\n        shear_ratio = np.sum(np.abs(np.triu(R_h, k=1))) / r_diag_sum\n\n    # 7. Orthogonality defects\n    Id_n = np.eye(n)\n    defect_h = np.linalg.norm(Q_h.T @ Q_h - Id_n, 'fro')\n    defect_c = np.linalg.norm(Q_c.T @ Q_c - Id_n, 'fro')\n    defect_m = np.linalg.norm(Q_m.T @ Q_m - Id_n, 'fro')\n    \n    # 8. Reconstruction residual\n    resid_h = np.linalg.norm(A - (Q_h @ R_h), 'fro')\n\n    return [\n        proper_rot_flag,\n        shear_ratio,\n        defect_h,\n        defect_c,\n        defect_m,\n        resid_h,\n        cond_num\n    ]\n\ndef solve():\n    # Define the 5 test matrices\n    test_cases = []\n\n    # Case 1\n    theta = 0.3\n    R_theta = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n    D_scale1 = np.diag([3.0, 1.0])\n    S_shear1 = np.array([[1, 0.2], [0, 1]])\n    A1 = R_theta @ D_scale1 @ S_shear1\n    test_cases.append(A1)\n\n    # Case 2\n    phi = 0.5\n    R_phi = np.array([[np.cos(phi), -np.sin(phi), 0], [np.sin(phi), np.cos(phi), 0], [0, 0, 1]])\n    D_scale2 = np.diag([10.0, 0.1, 1.0])\n    S_shear2 = np.array([[1, 5, -2], [0, 1, 3], [0, 0, 1]])\n    A2 = R_phi @ D_scale2 @ S_shear2\n    test_cases.append(A2)\n\n    # Case 3\n    A3 = np.array([[1.0, 1.0, 0.0], [1e-8, 2e-8, 0.0], [0.0, 0.0, 1e-12]])\n    test_cases.append(A3)\n\n    # Case 4\n    rng4 = np.random.default_rng(seed=42)\n    A4_base = rng4.standard_normal((5, 3))\n    scaling4 = np.array([1e3, 1e-3, 1.0])\n    A4 = A4_base * scaling4 # Element-wise scaling of columns\n    test_cases.append(A4)\n\n    # Case 5\n    rng5 = np.random.default_rng(seed=123)\n    rand_mat5 = rng5.standard_normal((4, 4))\n    Q0, _ = np.linalg.qr(rand_mat5)\n    D_scale5 = np.diag([1.0, 1e-6, 1e-9, 1.0])\n    S_shear5 = np.eye(4)\n    S_shear5[0, 1] = 1000.0\n    S_shear5[1, 2] = 500.0\n    S_shear5[2, 3] = -300.0\n    A5 = Q0 @ D_scale5 @ S_shear5\n    test_cases.append(A5)\n\n    results = []\n    for A in test_cases:\n        case_result = analyze_matrix(A)\n        # Format the list of numbers into a string list for printing\n        str_case_result = [str(x) for x in case_result]\n        results.append(f\"[{','.join(str_case_result)}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Smoothing motion estimates in dynamic neuroimaging data, such as fMRI, is essential for artifact correction, yet naively averaging rotation parameters can lead to invalid, non-rigid results. This is because the group of 3D rotations, $\\mathrm{SO}(3)$, has a non-Euclidean geometry. This exercise introduces unit quaternions and Spherical Linear Interpolation (SLERP), the gold-standard method for finding the shortest, constant-speed path between two orientations. By working through this problem, you will learn how to properly interpolate rotations, a critical skill for ensuring that smoothed time-series of motion data remain physically plausible. ",
            "id": "4177172",
            "problem": "A common challenge in functional Magnetic Resonance Imaging (fMRI) motion correction is to temporally smooth estimated head orientations without introducing non-rigid artifacts such as scaling or shear. Consider the three-dimensional rotation group $\\mathrm{SO}(3)$ and its double cover by the unit quaternions, identified with the three-sphere $\\mathbb{S}^{3} \\subset \\mathbb{R}^{4}$. A unit quaternion $q = (q_{w}, q_{x}, q_{y}, q_{z})$ represents a rotation, and the antipodal identification $q \\sim -q$ accounts for the double cover of $\\mathrm{SO}(3)$. The intrinsic geometric notion of shortest-path interpolation on $\\mathbb{S}^{3}$ (the geodesic with respect to the standard round metric induced by the Euclidean inner product on $\\mathbb{R}^{4}$) yields a constant-speed path along a great circle connecting two unit quaternions, providing a principled way to interpolate rotations without inducing scaling or shear when converted to rotation matrices.\n\nStarting from the following foundational facts:\n- Rotations in three dimensions form the special orthogonal group $\\mathrm{SO}(3)$, and any rotation can be represented by a unit quaternion $q \\in \\mathbb{S}^{3}$ with the equivalence $q \\sim -q$.\n- The geodesics on $\\mathbb{S}^{3}$ under the standard Riemannian metric are great circles, and the shortest path between two non-antipodal points is the unique great-circle arc with constant speed.\n- The inner product in $\\mathbb{R}^{4}$, given by $\\langle q_{0}, q_{1} \\rangle = q_{0w} q_{1w} + q_{0x} q_{1x} + q_{0y} q_{1y} + q_{0z} q_{1z}$, determines the angle between unit quaternions by $\\Omega = \\arccos(\\langle q_{0}, q_{1} \\rangle)$.\n\nDerive the expression for the unique constant-speed shortest-path interpolant on $\\mathbb{S}^{3}$ between two unit quaternions $q_{0}$ and $q_{1}$, and then evaluate it in the following neuroscientific motion-correction scenario: let $q_{0} = (1, 0, 0, 0)$ (the identity rotation) and let $q_{1}$ be the unit quaternion representing a rotation by angle $\\pi/2$ radians about the $z$-axis. Compute the interpolated quaternion at fraction $t = \\frac{1}{3}$ of the path length along the shortest geodesic. Express your final answer as the quaternion components $(q_{w}, q_{x}, q_{y}, q_{z})$ in exact closed form, without numerical rounding.\n\nIn your derivation, justify why this geodesic interpolation avoids scaling and shear when applied as a rotation matrix to volumetric data, and explain briefly how such interpolation supports temporal smoothing of head motion estimates in fMRI. The final answer must be a single analytic expression for the interpolated quaternion, written as a row matrix using the `pmatrix` environment. Angles are to be treated in radians throughout.",
            "solution": "The problem is valid as it is scientifically grounded in the mathematics of rotations and quaternions, well-posed with a unique solution, and objectively stated. It presents a standard application of quaternion interpolation (Spherical Linear Interpolation, or SLERP) in the context of fMRI data analysis.\n\nThe solution process consists of four parts:\n1.  Derivation of the general formula for constant-speed shortest-path interpolation between two unit quaternions on the three-sphere $\\mathbb{S}^3$.\n2.  Justification of why this interpolation method preserves rigidity (i.e., avoids scaling and shear).\n3.  Explanation of the method's application in temporal smoothing of fMRI motion parameters.\n4.  Application of the derived formula to the specific case given in the problem.\n\n**1. Derivation of the Geodesic Interpolant (SLERP)**\n\nLet $q_0$ and $q_1$ be two distinct, non-antipodal unit quaternions in $\\mathbb{R}^4$. As vectors, they lie on the unit three-sphere, $\\mathbb{S}^3$. The shortest path between them on the sphere is the shorter arc of the great circle that passes through them. This great circle lies in the two-dimensional plane spanned by the vectors $q_0$ and $q_1$.\n\nThe angle $\\Omega$ between these two vectors is given by their inner product:\n$$ \\Omega = \\arccos(\\langle q_0, q_1 \\rangle) $$\nSince $q_0$ and $q_1$ are unit quaternions, $\\|q_0\\|=1$ and $\\|q_1\\|=1$. For the shortest path, we can ensure $\\Omega \\in [0, \\pi)$ by replacing $q_1$ with $-q_1$ if $\\langle q_0, q_1 \\rangle  0$, since $q_1$ and $-q_1$ represent the same rotation. We assume this has been done, so $\\Omega \\in [0, \\pi/2]$.\n\nAny point $q(t)$ on the great circle arc can be expressed as a linear combination of an orthonormal basis for the plane containing the arc. We can choose $q_0$ as the first basis vector. The second basis vector, which we will call $u$, must be a unit vector in the plane spanned by $\\{q_0, q_1\\}$ and orthogonal to $q_0$. We can construct such a vector using the Gram-Schmidt process. Let $v = q_1 - \\langle q_1, q_0 \\rangle q_0$. This vector $v$ is orthogonal to $q_0$. Its squared norm is:\n$$ \\|v\\|^2 = \\langle q_1 - \\langle q_1, q_0 \\rangle q_0, q_1 - \\langle q_1, q_0 \\rangle q_0 \\rangle = \\|q_1\\|^2 - 2\\langle q_1, q_0 \\rangle^2 + \\langle q_1, q_0 \\rangle^2 \\|q_0\\|^2 $$\nSince $\\|q_0\\|=\\|q_1\\|=1$ and $\\langle q_0, q_1 \\rangle = \\cos(\\Omega)$, we have:\n$$ \\|v\\|^2 = 1 - 2\\cos^2(\\Omega) + \\cos^2(\\Omega) = 1 - \\cos^2(\\Omega) = \\sin^2(\\Omega) $$\nThus, $\\|v\\| = \\sin(\\Omega)$ (since $\\Omega \\in [0, \\pi)$, $\\sin(\\Omega) \\ge 0$). The unit vector is $u = \\frac{v}{\\|v\\|} = \\frac{q_1 - \\cos(\\Omega)q_0}{\\sin(\\Omega)}$.\n\nThe set $\\{q_0, u\\}$ forms an orthonormal basis for the plane. A constant-speed interpolation corresponds to traversing the arc at a constant angular velocity. To reach fraction $t$ of the path from $q_0$ to $q_1$, we must traverse an angle of $t\\Omega$ from $q_0$. In the basis $\\{q_0, u\\}$, this is a simple 2D rotation:\n$$ q(t) = \\cos(t\\Omega) q_0 + \\sin(t\\Omega) u $$\nSubstituting the expression for $u$:\n$$ q(t) = \\cos(t\\Omega) q_0 + \\sin(t\\Omega) \\left( \\frac{q_1 - \\cos(\\Omega)q_0}{\\sin(\\Omega)} \\right) $$\nGrouping the terms for $q_0$ and $q_1$:\n$$ q(t) = \\left( \\cos(t\\Omega) - \\frac{\\sin(t\\Omega)\\cos(\\Omega)}{\\sin(\\Omega)} \\right) q_0 + \\left( \\frac{\\sin(t\\Omega)}{\\sin(\\Omega)} \\right) q_1 $$\nUsing the trigonometric identity $\\sin(A-B) = \\sin(A)\\cos(B) - \\cos(A)\\sin(B)$, the coefficient of $q_0$ can be simplified:\n$$ \\frac{\\sin(\\Omega)\\cos(t\\Omega) - \\cos(\\Omega)\\sin(t\\Omega)}{\\sin(\\Omega)} = \\frac{\\sin(\\Omega - t\\Omega)}{\\sin(\\Omega)} = \\frac{\\sin((1-t)\\Omega)}{\\sin(\\Omega)} $$\nThis yields the Spherical Linear Interpolation (SLERP) formula:\n$$ q(t) = \\frac{\\sin((1-t)\\Omega)}{\\sin(\\Omega)} q_0 + \\frac{\\sin(t\\Omega)}{\\sin(\\Omega)} q_1 $$\nThis formula defines the unique constant-speed shortest-path interpolant for $t \\in [0, 1]$.\n\n**2. Justification for Avoiding Scaling and Shear**\n\nA transformation matrix represents a pure rotation if and only if it belongs to the special orthogonal group $\\mathrm{SO}(3)$. These matrices are orthogonal ($R^T R = I$) with determinant $+1$. A key property is that they preserve lengths and angles, hence they are rigid-body transformations without scaling or shear.\n\nAny unit quaternion $q$ corresponds to a rotation matrix in $\\mathrm{SO}(3)$. Therefore, to show that the interpolated transformation is always a pure rotation, we only need to show that the interpolated quaternion $q(t)$ is a unit quaternion for all $t \\in [0, 1]$.\n\nUsing the expression $q(t) = \\cos(t\\Omega) q_0 + \\sin(t\\Omega) u$ and the fact that $\\{q_0, u\\}$ is an orthonormal basis, we compute the squared norm of $q(t)$:\n$$ \\|q(t)\\|^2 = \\langle \\cos(t\\Omega) q_0 + \\sin(t\\Omega) u, \\cos(t\\Omega) q_0 + \\sin(t\\Omega) u \\rangle $$\n$$ \\|q(t)\\|^2 = \\cos^2(t\\Omega) \\langle q_0, q_0 \\rangle + \\sin^2(t\\Omega) \\langle u, u \\rangle + 2\\cos(t\\Omega)\\sin(t\\Omega)\\langle q_0, u \\rangle $$\nSince $\\langle q_0, q_0 \\rangle = \\|q_0\\|^2 = 1$, $\\langle u, u \\rangle = \\|u\\|^2 = 1$, and $\\langle q_0, u \\rangle = 0$:\n$$ \\|q(t)\\|^2 = \\cos^2(t\\Omega) \\cdot 1 + \\sin^2(t\\Omega) \\cdot 1 + 0 = 1 $$\nSince $\\|q(t)\\|=1$ for all $t \\in [0, 1]$, the interpolated quaternion is always a unit quaternion. It therefore always represents a pure rotation in $\\mathrm{SO}(3)$, guaranteeing that the interpolation does not introduce non-rigid artifacts like scaling or shear.\n\n**3. Application in fMRI Motion Smoothing**\n\nIn fMRI analysis, head motion during scanning is a significant source of artifacts. Motion correction algorithms estimate the rigid-body transformation (3D rotation and 3D translation) required to align each brain volume to a reference volume. This results in a time series of rotation parameters, one for each acquired volume. This series can be noisy or contain physiologically implausible jumps, either due to estimation errors or jerky subject movements.\n\nTemporal smoothing of these parameters is desirable. However, naively filtering or averaging common rotation representations like Euler angles or rotation matrices is problematic. Euler angles suffer from gimbal lock and non-uniform representation of the rotation space. Averaging rotation matrices does not generally yield a valid rotation matrix.\n\nBy representing the rotations as unit quaternions, we move the problem to the manifold $\\mathbb{S}^3$. Smoothing or interpolation can then be performed using the intrinsic geometry of this space. SLERP provides the principled way to find an intermediate rotation between two given rotations. For temporal filtering (e.g., a moving average), this concept is extended to find the geometric mean of multiple quaternions. This process ensures the entire smoothed trajectory of head orientations consists of valid rotations, corresponding to a smooth, physically-plausible path in the space of all possible head orientations.\n\n**4. Specific Calculation**\n\nWe are asked to find the interpolated quaternion at $t = \\frac{1}{3}$ between $q_0 = (1, 0, 0, 0)$ and $q_1$, which represents a rotation by $\\theta = \\pi/2$ about the $z$-axis.\n\nFirst, we determine $q_1$. For a rotation by angle $\\theta$ about a unit axis $\\mathbf{u}=(u_x, u_y, u_z)$, the corresponding unit quaternion is $q = (\\cos(\\theta/2), u_x \\sin(\\theta/2), u_y \\sin(\\theta/2), u_z \\sin(\\theta/2))$.\nHere, $\\theta = \\pi/2$ and $\\mathbf{u}=(0, 0, 1)$. Thus, $\\theta/2 = \\pi/4$.\n$$ q_1 = (\\cos(\\pi/4), 0 \\cdot \\sin(\\pi/4), 0 \\cdot \\sin(\\pi/4), 1 \\cdot \\sin(\\pi/4)) = (\\frac{\\sqrt{2}}{2}, 0, 0, \\frac{\\sqrt{2}}{2}) $$\n\nNext, we compute the angle $\\Omega$ between $q_0$ and $q_1$:\n$$ \\langle q_0, q_1 \\rangle = (1)(\\frac{\\sqrt{2}}{2}) + (0)(0) + (0)(0) + (0)(\\frac{\\sqrt{2}}{2}) = \\frac{\\sqrt{2}}{2} $$\n$$ \\Omega = \\arccos(\\frac{\\sqrt{2}}{2}) = \\frac{\\pi}{4} $$\n\nNow, we use the SLERP formula with $t = \\frac{1}{3}$:\n$$ q(\\frac{1}{3}) = \\frac{\\sin((1-\\frac{1}{3})\\frac{\\pi}{4})}{\\sin(\\frac{\\pi}{4})} q_0 + \\frac{\\sin(\\frac{1}{3}\\frac{\\pi}{4})}{\\sin(\\frac{\\pi}{4})} q_1 $$\n$$ q(\\frac{1}{3}) = \\frac{\\sin(\\frac{2}{3} \\frac{\\pi}{4})}{\\sin(\\frac{\\pi}{4})} q_0 + \\frac{\\sin(\\frac{\\pi}{12})}{\\sin(\\frac{\\pi}{4})} q_1 = \\frac{\\sin(\\frac{\\pi}{6})}{\\sin(\\frac{\\pi}{4})} q_0 + \\frac{\\sin(\\frac{\\pi}{12})}{\\sin(\\frac{\\pi}{4})} q_1 $$\n\nWe need the values for the sine functions:\n- $\\sin(\\pi/6) = \\frac{1}{2}$\n- $\\sin(\\pi/4) = \\frac{\\sqrt{2}}{2}$\n- $\\sin(\\pi/12) = \\sin(15^\\circ) = \\sin(45^\\circ - 30^\\circ) = \\sin(45^\\circ)\\cos(30^\\circ) - \\cos(45^\\circ)\\sin(30^\\circ) = \\frac{\\sqrt{2}}{2}\\frac{\\sqrt{3}}{2} - \\frac{\\sqrt{2}}{2}\\frac{1}{2} = \\frac{\\sqrt{6}-\\sqrt{2}}{4}$\n\nNow compute the coefficients:\n- Coefficient for $q_0$: $\\frac{\\sin(\\pi/6)}{\\sin(\\pi/4)} = \\frac{1/2}{\\sqrt{2}/2} = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2}$\n- Coefficient for $q_1$: $\\frac{\\sin(\\pi/12)}{\\sin(\\pi/4)} = \\frac{(\\sqrt{6}-\\sqrt{2})/4}{\\sqrt{2}/2} = \\frac{\\sqrt{6}-\\sqrt{2}}{2\\sqrt{2}} = \\frac{\\sqrt{2}(\\sqrt{3}-1)}{2\\sqrt{2}} = \\frac{\\sqrt{3}-1}{2}$\n\nFinally, we compute the interpolated quaternion $q(\\frac{1}{3})$:\n$$ q(\\frac{1}{3}) = \\frac{\\sqrt{2}}{2} (1, 0, 0, 0) + \\frac{\\sqrt{3}-1}{2} (\\frac{\\sqrt{2}}{2}, 0, 0, \\frac{\\sqrt{2}}{2}) $$\nThe components are:\n- $q_w = \\frac{\\sqrt{2}}{2} \\cdot 1 + \\frac{\\sqrt{3}-1}{2} \\cdot \\frac{\\sqrt{2}}{2} = \\frac{\\sqrt{2}}{2} + \\frac{\\sqrt{6}-\\sqrt{2}}{4} = \\frac{2\\sqrt{2} + \\sqrt{6}-\\sqrt{2}}{4} = \\frac{\\sqrt{6}+\\sqrt{2}}{4}$\n- $q_x = \\frac{\\sqrt{2}}{2} \\cdot 0 + \\frac{\\sqrt{3}-1}{2} \\cdot 0 = 0$\n- $q_y = \\frac{\\sqrt{2}}{2} \\cdot 0 + \\frac{\\sqrt{3}-1}{2} \\cdot 0 = 0$\n- $q_z = \\frac{\\sqrt{2}}{2} \\cdot 0 + \\frac{\\sqrt{3}-1}{2} \\cdot \\frac{\\sqrt{2}}{2} = \\frac{\\sqrt{6}-\\sqrt{2}}{4}$\n\nThe interpolated quaternion is $(\\frac{\\sqrt{6}+\\sqrt{2}}{4}, 0, 0, \\frac{\\sqrt{6}-\\sqrt{2}}{4})$. This corresponds to a rotation by angle $t\\theta = \\frac{1}{3} \\frac{\\pi}{2} = \\frac{\\pi}{6}$ about the $z$-axis, as $\\cos(\\pi/12) = \\frac{\\sqrt{6}+\\sqrt{2}}{4}$ and $\\sin(\\pi/12) = \\frac{\\sqrt{6}-\\sqrt{2}}{4}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sqrt{6}+\\sqrt{2}}{4}  0  0  \\frac{\\sqrt{6}-\\sqrt{2}}{4}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}