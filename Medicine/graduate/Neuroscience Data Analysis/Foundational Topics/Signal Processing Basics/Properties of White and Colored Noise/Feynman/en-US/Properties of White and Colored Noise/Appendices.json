{
    "hands_on_practices": [
        {
            "introduction": "A core principle in signal processing is the equivalence of time-domain and frequency-domain representations. This exercise provides a foundational bridge by asking you to derive the relationship between the time-domain variance of a discrete white noise process and the constant level of its power spectral density (PSD). Completing this derivation  is essential for correctly interpreting PSD plots and understanding how signal power is quantified in the frequency domain.",
            "id": "4188424",
            "problem": "In a neural recording experiment, you acquire a zero-mean discrete-time noise sequence $\\{x[n]\\}_{n \\in \\mathbb{Z}}$ that is experimentally verified to be wide-sense stationary and white, with time-domain variance $\\sigma_{x}^{2}$ (in $\\mathrm{V}^{2}$). The sequence is produced by ideal uniform sampling at sampling frequency $f_{s}$ (in $\\mathrm{Hz}$). Using the definition that the Power Spectral Density (PSD) is the Fourier transform of the autocorrelation function (Wiener–Khinchin theorem), and the normalization that connects the discrete-time frequency variable $\\omega$ (in $\\mathrm{rad/sample}$) to the continuous frequency $f$ (in $\\mathrm{Hz}$) via $f = \\omega f_{s} / (2\\pi)$, derive the constant level $S_{1}(f)$ of the one-sided PSD (in $\\mathrm{V}^{2}/\\mathrm{Hz}$) for $0  f  f_{s}/2$. Your derivation must start from the autocorrelation function of white noise and enforce that integrating the one-sided PSD over the Nyquist band recovers the time-domain variance.\n\nExpress your final result as a single closed-form analytic expression in terms of $\\sigma_{x}^{2}$ and $f_{s}$. Do not include units in your final boxed answer; assume $\\mathrm{V}^{2}/\\mathrm{Hz}$. No numerical approximation is required.",
            "solution": "The starting point is the definition of white noise in discrete time and the Wiener–Khinchin theorem. For a zero-mean wide-sense stationary white noise sequence $x[n]$ with variance $\\sigma_{x}^{2}$, the autocorrelation function is\n$$\nR_{x}[k] = \\mathbb{E}\\{x[n] x[n+k]\\} = \\sigma_{x}^{2} \\,\\delta[k],\n$$\nwhere $\\delta[k]$ is the Kronecker delta. By the Wiener–Khinchin theorem, the discrete-time power spectral density $S_{x}^{(d)}(\\omega)$ (in conventional discrete-time units associated with $\\omega$ measured in $\\mathrm{rad/sample}$) is the discrete-time Fourier transform of $R_{x}[k]$:\n$$\nS_{x}^{(d)}(\\omega) = \\sum_{k=-\\infty}^{\\infty} R_{x}[k] \\exp(-\\mathrm{i}\\,\\omega k) = \\sigma_{x}^{2},\n$$\nwhich is flat (constant) for $\\omega \\in [-\\pi,\\pi]$.\n\nTo relate this to a PSD expressed per unit Hertz, denoted $S_{x}^{(2)}(f)$ for the two-sided PSD, we use the normalization that preserves total power (variance) under the change of variables from $\\omega$ to $f$:\n$$\nf = \\frac{\\omega f_{s}}{2\\pi}, \\quad \\mathrm{d}f = \\frac{f_{s}}{2\\pi}\\,\\mathrm{d}\\omega.\n$$\nThe variance $\\sigma_{x}^{2}$ must equal the area under the two-sided PSD across the fundamental Nyquist interval:\n$$\n\\sigma_{x}^{2} = \\int_{-f_{s}/2}^{f_{s}/2} S_{x}^{(2)}(f)\\,\\mathrm{d}f.\n$$\nIn discrete-time frequency variables, the same variance is\n$$\n\\sigma_{x}^{2} = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} S_{x}^{(d)}(\\omega)\\,\\mathrm{d}\\omega = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\sigma_{x}^{2}\\,\\mathrm{d}\\omega = \\sigma_{x}^{2}.\n$$\nTo enforce consistency of the integrals under the change of variables, we require\n$$\nS_{x}^{(2)}(f)\\,\\mathrm{d}f = \\frac{1}{2\\pi} S_{x}^{(d)}(\\omega)\\,\\mathrm{d}\\omega,\n$$\nwhich yields\n$$\nS_{x}^{(2)}(f) = \\frac{1}{2\\pi} S_{x}^{(d)}(\\omega)\\,\\frac{\\mathrm{d}\\omega}{\\mathrm{d}f} = \\frac{1}{2\\pi}\\,\\sigma_{x}^{2}\\,\\frac{2\\pi}{f_{s}} = \\frac{\\sigma_{x}^{2}}{f_{s}},\n$$\nfor $f \\in (-f_{s}/2, f_{s}/2)$. Thus, the two-sided PSD in units of $\\mathrm{V}^{2}/\\mathrm{Hz}$ is flat at level $\\sigma_{x}^{2}/f_{s}$.\n\nThe one-sided PSD $S_{1}(f)$ over $f \\in (0, f_{s}/2)$ is defined so that integrating it over the positive frequencies recovers the total variance. This is achieved by doubling the two-sided level for strictly positive frequencies:\n$$\nS_{1}(f) = 2\\,S_{x}^{(2)}(f) = \\frac{2\\,\\sigma_{x}^{2}}{f_{s}}, \\quad 0  f  \\frac{f_{s}}{2}.\n$$\nFinally, we verify that the one-sided PSD integrates to the variance:\n$$\n\\int_{0}^{f_{s}/2} S_{1}(f)\\,\\mathrm{d}f = \\int_{0}^{f_{s}/2} \\frac{2\\,\\sigma_{x}^{2}}{f_{s}}\\,\\mathrm{d}f = \\frac{2\\,\\sigma_{x}^{2}}{f_{s}} \\cdot \\frac{f_{s}}{2} = \\sigma_{x}^{2}.\n$$\nTherefore, the constant one-sided PSD level in $\\mathrm{V}^{2}/\\mathrm{Hz}$ is\n$$\nS_{1}(f) = \\frac{2\\,\\sigma_{x}^{2}}{f_{s}} \\quad \\text{for} \\quad 0  f  \\frac{f_{s}}{2}.\n$$",
            "answer": "$$\\boxed{\\frac{2\\,\\sigma_{x}^{2}}{f_{s}}}$$"
        },
        {
            "introduction": "In neuroscience modeling, a crucial diagnostic step is to examine the model's residuals to ensure they contain no remaining structure; ideally, they should resemble white noise. This hands-on coding practice  guides you through the implementation of the Ljung–Box test, a widely used statistical method for determining if a time series, such as model residuals, is consistent with the white noise hypothesis. This exercise provides a vital tool for validating the goodness-of-fit of statistical models applied to neural data.",
            "id": "4188384",
            "problem": "You are tasked with deriving and implementing a statistical test to assess whether a sequence of residuals from a Generalized Linear Model (GLM) used in neural spike train analysis is consistent with white noise. Begin from core definitions and widely accepted facts, and do not assume any specialized or shortcut formulas. Specifically, use the following foundational base:\n\n- Define white noise: a sequence $\\{e_t\\}_{t=1}^n$ with $E[e_t] = 0$, $\\operatorname{Var}(e_t) = \\sigma^2$ constant over time, and zero autocovariance $\\gamma(k) = E[(e_t - E[e_t])(e_{t-k} - E[e_{t-k}])] = 0$ for all lags $k \\ge 1$.\n- Define the sample mean $\\bar{e} = \\frac{1}{n} \\sum_{t=1}^n e_t$, sample autocovariance at lag $k$ as $\\hat{\\gamma}(k) = \\frac{1}{n} \\sum_{t=k+1}^n (e_t - \\bar{e})(e_{t-k} - \\bar{e})$, and the sample autocorrelation at lag $k$ as $\\hat{r}_k = \\hat{\\gamma}(k) / \\hat{\\gamma}(0)$ when $\\hat{\\gamma}(0)  0$.\n- Use the well-tested fact that under the null hypothesis of independence (white noise), for large $n$, the collection $\\{\\hat{r}_1, \\ldots, \\hat{r}_h\\}$ is approximately centered around zero and, after appropriate scaling, yields a portmanteau statistic whose distribution can be approximated by a chi-squared distribution with $h$ degrees of freedom.\n\nYour tasks are:\n\n1. Derive a portmanteau test statistic that aggregates the evidence of nonzero sample autocorrelations up to lag $h$, ensuring that the statistic has, under the null hypothesis, an approximate chi-squared distribution with $h$ degrees of freedom.\n\n2. State a decision rule that, given a significance level $\\alpha$, determines whether the residuals are consistent with white noise based on the derived test statistic and its null distribution.\n\n3. Implement a complete program that:\n   - Computes the sample autocorrelation $\\hat{r}_k$ for $k = 1, \\ldots, h$ from a provided residual sequence (assume $h \\le n-1$; if $h  n-1$, use $h' = \\min(h, n-1)$ in place of $h$).\n   - Constructs the derived test statistic and computes the corresponding $p$-value using the appropriate chi-squared distribution.\n   - Outputs a boolean decision for each test case indicating whether residuals are consistent with white noise at the specified $\\alpha$ (output `True` if consistent, `False` otherwise).\n   - Handles the edge case where $\\hat{\\gamma}(0) = 0$ (all residuals equal), by defining $\\hat{r}_k = 0$ for all $k$.\n\nTest Suite Specification:\n\nUse the following four test cases to validate your implementation. For stochastic simulations, use the specified pseudo-random seeds to ensure reproducibility.\n\n- Case A (Neural GLM, correctly specified): Simulate independent Poisson spike counts $\\{y_t\\}_{t=1}^n$ with $n = 200$, constant rate $\\lambda = 10$ using seed $s_1 = 12345$. Use the correctly specified model $\\hat{\\mu}_t = \\lambda$ and form Pearson residuals $e_t = (y_t - \\hat{\\mu}_t)/\\sqrt{\\hat{\\mu}_t}$. Use lag $h = 10$ and significance level $\\alpha = 0.05$.\n\n- Case B (Neural GLM, mis-specified with colored noise): Simulate dependent Poisson spike counts $\\{y_t\\}_{t=1}^n$ with $n = 200$ using seed $s_2 = 24680$, autoregressive intensity $\\lambda_t = \\lambda_0 + \\phi y_{t-1}$ with $\\lambda_0 = 10$, $\\phi = 0.6$, and initial count $y_0 = \\lambda_0$. Use a mis-specified constant-rate model $\\hat{\\mu}_t = \\lambda_0$ and form Pearson residuals $e_t = (y_t - \\hat{\\mu}_t)/\\sqrt{\\hat{\\mu}_t}$. Use lag $h = 10$ and significance level $\\alpha = 0.05$.\n\n- Case C (Boundary lag, white Gaussian): Simulate a white Gaussian residual sequence $\\{e_t\\}_{t=1}^{50}$ with mean $0$ and variance $1$ using seed $s_3 = 13579$. Use lag $h = 1$ and significance level $\\alpha = 0.05$.\n\n- Case D (Zero-variance edge case): Use a constant residual sequence $e_t = 0$ for $t = 1, \\ldots, 100$. Use lag $h = 10$ and significance level $\\alpha = 0.05$.\n\nFinal Output Format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases A, B, C, D. For example, `[resultA,resultB,resultC,resultD]`, where each `resultX` is a boolean indicating whether the residuals are consistent with white noise at the given $\\alpha`.",
            "solution": "We begin from the definition of white noise. A residual sequence $\\{e_t\\}_{t=1}^n$ is white noise if $E[e_t] = 0$, $\\operatorname{Var}(e_t) = \\sigma^2$ is constant, and the autocovariance function $\\gamma(k) = E[(e_t - E[e_t])(e_{t-k} - E[e_{t-k}])]$ satisfies $\\gamma(k) = 0$ for all $k \\ge 1$. Thus, the core property to assess is the absence of autocorrelation at nonzero lags.\n\nFrom observed data, we use the sample mean $\\bar{e} = \\frac{1}{n}\\sum_{t=1}^n e_t$, the sample autocovariance $\\hat{\\gamma}(k) = \\frac{1}{n}\\sum_{t=k+1}^n (e_t - \\bar{e})(e_{t-k} - \\bar{e})$, and the sample autocorrelation $\\hat{r}_k = \\hat{\\gamma}(k)/\\hat{\\gamma}(0)$ provided $\\hat{\\gamma}(0)  0$. If $\\hat{\\gamma}(0) = 0$ (all residuals equal), then every centered product is zero, and we can define $\\hat{r}_k = 0$ for all $k$.\n\nUnder the null hypothesis $H_0$ of independence and stationarity (white noise), a well-tested asymptotic fact is that for fixed $h$ and large $n$, the vector $\\sqrt{n}(\\hat{r}_1, \\ldots, \\hat{r}_h)$ is approximately multivariate normal with mean zero and covariance matrix close to the identity for independent, identically distributed residuals. Intuitively, each $\\hat{r}_k$ is an average of $n$ centered products, and by the central limit theorem it is approximately normal with variance decreasing as $1/n$. Consequently, a natural portmanteau statistic that aggregates deviations from zero across lags is a weighted sum of squares of $\\hat{r}_k$.\n\nThe Box–Pierce statistic uses $Q_{\\mathrm{BP}} = n \\sum_{k=1}^h \\hat{r}_k^2$, and its asymptotic null distribution is $\\chi^2_h$. The Ljung–Box test refines the finite-sample performance by adjusting each term to account for the effective number of pairs at lag $k$, yielding\n$$\nQ = n(n + 2)\\sum_{k=1}^h \\frac{\\hat{r}_k^2}{n - k}.\n$$\nThis scaling arises by approximating the variance of $\\hat{r}_k$ with $\\operatorname{Var}(\\hat{r}_k) \\approx \\frac{1}{n}\\left(1 + \\frac{2k}{n}\\right)$, which leads to the multiplicative correction $\\frac{n(n+2)}{n-k}$ inside the sum, improving the chi-squared approximation for moderate $n$. Under $H_0$ and for large $n$, $Q$ is approximately distributed as a chi-squared random variable with $h$ degrees of freedom, denoted $\\chi^2_h$.\n\nThe decision rule at significance level $\\alpha$ is:\n- Compute $Q$ from $\\hat{r}_k$ for $k = 1, \\ldots, h'$ where $h' = \\min(h, n - 1)$.\n- Compute the $p$-value $p = P(\\chi^2_{h'} \\ge Q)$, that is, the chi-squared survival function evaluated at $Q$ with $h'$ degrees of freedom.\n- If $p \\ge \\alpha$, fail to reject $H_0$ and decide that the residuals are consistent with white noise; otherwise, reject $H_0$ and decide that the residuals are not consistent with white noise (colored noise).\n\nAlgorithmic design:\n- For an input residual sequence and lag $h$, compute $\\bar{e}$ and $\\hat{\\gamma}(0)$. If $\\hat{\\gamma}(0) = 0$, set all $\\hat{r}_k = 0$ and $Q = 0$ by definition, yielding $p = 1$ and a white-noise-consistent decision.\n- Otherwise, for each $k = 1, \\ldots, h'$, compute $\\hat{r}_k$ by summing $(e_t - \\bar{e})(e_{t-k} - \\bar{e})$ over $t = k+1, \\ldots, n$, dividing by $n$, and normalizing by $\\hat{\\gamma}(0)$.\n- Compute $Q = n(n + 2)\\sum_{k=1}^{h'} \\hat{r}_k^2/(n - k)$ and $p$ from the chi-squared distribution.\n- Apply the decision rule with the given $\\alpha$.\n\nApplication to test cases:\n- Case A: Independent Poisson counts with constant rate and correctly specified model yield Pearson residuals that are independent with no autocorrelation beyond sampling noise; the test should return `True` at $\\alpha = 0.05$.\n- Case B: Dependent Poisson counts with autoregressive intensity and a mis-specified constant-rate model produce Pearson residuals with positive autocorrelation; the test should detect colored noise and return `False`.\n- Case C: White Gaussian residuals with $h = 1$ test immediate-lag autocorrelation and should return `True`.\n- Case D: The residuals have zero variance; our definition sets all $\\hat{r}_k = 0$, $Q = 0$, $p = 1$, and the decision is `True`.\n\nThe implementation uses a fixed random number generator seed for each stochastic simulation to ensure reproducibility and adheres to the specified final output format `[resultA,resultB,resultC,resultD]`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef ljung_box_test(residuals: np.ndarray, h: int, alpha: float = 0.05):\n    \"\"\"\n    Compute the Ljung–Box Q statistic, its p-value, and the decision\n    on whether the residuals are consistent with white noise.\n\n    Parameters:\n        residuals: 1D numpy array of residuals e_t.\n        h: maximum lag for testing autocorrelation (integer).\n        alpha: significance level for decision (float).\n\n    Returns:\n        decision: bool, True if residuals are consistent with white noise (fail to reject H0).\n        Q: float, Ljung–Box statistic value.\n        p_value: float, chi-squared survival probability at Q with h_eff degrees of freedom.\n    \"\"\"\n    r = np.asarray(residuals, dtype=float)\n    n = r.size\n    h_eff = max(0, min(h, n - 1))\n\n    # Center residuals\n    mean_r = np.mean(r)\n    centered = r - mean_r\n\n    # Sample autocovariance at lag 0 (variance proxy)\n    # Using 1/n normalization per problem statement base.\n    gamma0 = np.sum(centered * centered) / n\n\n    # Handle zero variance edge case: define autocorrelations as zero.\n    if gamma0 == 0.0 or h_eff == 0:\n        Q = 0.0\n        p_value = 1.0\n        decision = p_value = alpha\n        return decision, Q, p_value\n\n    # Compute sample autocorrelations for k = 1..h_eff\n    r_k = np.empty(h_eff, dtype=float)\n    for k in range(1, h_eff + 1):\n        # Sum over t = k .. n-1 = indices t and t-k\n        prod_sum = np.dot(centered[k:], centered[:-k])\n        gamma_k = prod_sum / n\n        r_k[k - 1] = gamma_k / gamma0\n\n    # Ljung–Box Q statistic\n    # Q = n(n+2) * sum_{k=1}^h r_k^2 / (n - k)\n    ks = np.arange(1, h_eff + 1, dtype=float)\n    denom = (n - ks)\n    Q = n * (n + 2) * np.sum((r_k * r_k) / denom)\n\n    # p-value from chi-squared distribution with h_eff degrees of freedom\n    p_value = chi2.sf(Q, df=h_eff)\n\n    decision = p_value = alpha\n    return decision, Q, p_value\n\ndef simulate_case_A(seed=12345, n=200, lam=10.0):\n    rng = np.random.default_rng(seed)\n    y = rng.poisson(lam, size=n)\n    mu_hat = lam\n    residuals = (y - mu_hat) / np.sqrt(mu_hat)\n    return residuals\n\ndef simulate_case_B(seed=24680, n=200, lam0=10.0, phi=0.6):\n    rng = np.random.default_rng(seed)\n    y = np.empty(n, dtype=float)\n    # Initial count set to lam0 as specified\n    y0 = lam0\n    y[0] = y0\n    for t in range(1, n):\n        lam_t = lam0 + phi * y[t - 1]\n        y[t] = rng.poisson(lam_t)\n    mu_hat = lam0  # mis-specified constant-rate model\n    residuals = (y - mu_hat) / np.sqrt(mu_hat)\n    return residuals\n\ndef simulate_case_C(seed=13579, n=50):\n    rng = np.random.default_rng(seed)\n    residuals = rng.normal(loc=0.0, scale=1.0, size=n)\n    return residuals\n\ndef simulate_case_D(n=100):\n    return np.zeros(n, dtype=float)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (residuals, h, alpha)\n    caseA_resid = simulate_case_A(seed=12345, n=200, lam=10.0)\n    caseB_resid = simulate_case_B(seed=24680, n=200, lam0=10.0, phi=0.6)\n    caseC_resid = simulate_case_C(seed=13579, n=50)\n    caseD_resid = simulate_case_D(n=100)\n\n    test_cases = [\n        (caseA_resid, 10, 0.05),  # Case A\n        (caseB_resid, 10, 0.05),  # Case B\n        (caseC_resid, 1, 0.05),   # Case C\n        (caseD_resid, 10, 0.05),  # Case D\n    ]\n\n    results = []\n    for resid, h, alpha in test_cases:\n        decision, Q, p_value = ljung_box_test(resid, h, alpha)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "When a time series is clearly not white noise, a more sophisticated question arises: can its structure be explained by linear correlations alone (i.e., as colored noise), or are nonlinear dynamics at play? This advanced practice introduces the surrogate data method, specifically the Iterative Amplitude Adjusted Fourier Transform (IAAFT), to test for nonlinearity . By generating surrogate datasets that preserve the power spectrum and amplitude distribution of the original data, you can test whether specific features of your signal are statistically significant or merely byproducts of a linear process.",
            "id": "4188408",
            "problem": "You are given the task of implementing a surrogate data method based on the Iterative Amplitude Adjusted Fourier Transform (IAAFT) to test whether observed temporal structure in a univariate time series exceeds what would be expected under a colored-noise null model. The null hypothesis is that the process is a realization of a stationary linear stochastic process with a specified amplitude distribution and power spectral density that together capture the linear properties of the data.\n\nUse the following foundational base:\n- A discrete-time, zero-mean stochastic process $x[t], t = 0,1,\\dots,N-1$, has a power spectral density (PSD) $S_{xx}(f)$, where, for discrete signals, the PSD is related to the Discrete Fourier Transform (DFT). Let $X[k]$ denote the DFT of $x[t]$ defined by\n$$\nX[k] = \\sum_{t=0}^{N-1} x[t] e^{-i 2 \\pi k t / N}, \\quad k = 0,1,\\dots,N-1,\n$$\nand let $|X[k]|$ be its magnitude. White noise is characterized by a flat PSD, whereas colored noise has a non-flat PSD (for example, autoregressive processes exhibit colored spectra).\n- The IAAFT method aims to produce surrogate signals $y[t]$ that preserve two properties of the original data: the amplitude distribution (the empirical distribution of sample values) and the magnitude of the DFT $|X[k]|$, hence preserving the linear autocorrelation structure encoded in the PSD, while randomizing phase to break nonlinear dependencies.\n\nDefine the test statistic for time-reversal asymmetry to detect nonlinear structure:\n- Let the increment sequence be $d[t] = x[t+1] - x[t]$ for $t = 0,1,\\dots,N-2$.\n- Define the normalized third moment of increments\n$$\nT(x) = \\frac{1}{N-1} \\sum_{t=0}^{N-2} \\frac{(d[t])^3}{\\sigma_d^3 + \\varepsilon},\n$$\nwhere $\\sigma_d$ is the standard deviation of $d[t]$ and $\\varepsilon$ is a small positive constant to avoid division by zero. For linear Gaussian processes with symmetric distributions, $T(x)$ is expected to be near zero, while nonlinear or asymmetric temporal dependencies can yield $T(x)$ significantly different from zero.\n\nYour program must:\n1. Implement the IAAFT surrogate generation algorithm. Starting from an initial random permutation $y^{(0)}[t]$ of $x[t]$, iterate the following projections until convergence or a maximum iteration count is reached:\n   - Spectral projection: compute $Y^{(m)}[k]$ as the DFT of $y^{(m)}[t]$, set\n   $$\n   \\tilde{Y}^{(m)}[k] = |X[k]| e^{i \\angle Y^{(m)}[k]},\n   $$\n   and invert to obtain $\\tilde{y}^{(m)}[t]$.\n   - Amplitude distribution projection: rank-order $\\tilde{y}^{(m)}[t]$ and replace its sorted values with the sorted values of $x[t]$ to obtain $y^{(m+1)}[t]$.\n   - Use a relative spectral error to monitor convergence,\n   $$\n   E^{(m)} = \\frac{\\left\\|\\,| \\mathrm{DFT}(y^{(m)}[t])| - |X[k]|\\,\\right\\|_2}{\\left\\||X[k]|\\right\\|_2},\n   $$\n   and stop when $E^{(m)}$ falls below a tolerance or the iteration count reaches a preset maximum.\n\n2. For each test case, generate a specified number of IAAFT surrogates, compute $T(y_s)$ for each surrogate $y_s$, and compute a two-sided empirical $p$-value for the observed statistic $T(x)$ relative to the surrogate distribution. Use the surrogate mean $\\mu_s$ of $T(y_s)$ and compute the absolute deviation of the observed statistic from $\\mu_s$. The empirical two-sided $p$-value is\n$$\np = \\frac{1 + \\#\\{s: |T(y_s) - \\mu_s| \\ge |T(x) - \\mu_s|\\}}{1 + S},\n$$\nwhere $S$ is the number of surrogates. A significant result indicates that the observed structure exceeds what is expected under the colored-noise null preserved by amplitude distribution and PSD.\n\n3. Decide significance using a threshold $\\alpha = 0.01$ (expressed as a decimal). Output a boolean for each test case indicating whether $p  \\alpha$.\n\nUse the following test suite, where all time series must be standardized to zero mean and unit variance before analysis:\n- Case A (happy path, white noise): Generate white Gaussian noise with length $N = 2048$.\n- Case B (colored linear noise, autoregressive of order one): Generate $x[t] = \\phi x[t-1] + e[t]$ with $\\phi = 0.9$, $e[t]$ Gaussian, and length $N = 2048$.\n- Case C (nonlinear colored process): Generate $x[t] = \\phi x[t-1] + \\beta x[t-1]^2 + e[t]$ with $\\phi = 0.8$, $\\beta = 0.3$, $e[t]$ Gaussian, and length $N = 2048$.\n- Case D (edge case, short colored linear noise): Generate $x[t] = \\phi x[t-1] + e[t]$ with $\\phi = 0.95$, $e[t]$ Gaussian, and length $N = 512$.\n\nImplementation constraints:\n- Use $S = 40$ surrogates per test case.\n- Use a maximum of $100$ IAAFT iterations and a relative spectral tolerance of $10^{-6}$.\n- Use a small $\\varepsilon = 10^{-12}$ in the definition of $T(x)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets (e.g., \"[True,False,True,False]\"), corresponding to Cases A, B, C, and D in order.",
            "solution": "The task is to implement a statistical test for nonlinearity in a time series using the Iterative Amplitude Adjusted Fourier Transform (IAAFT) surrogate data method. The null hypothesis, $H_0$, posits that the observed time series is a realization of a stationary, linear, stochastic Gaussian process. A rejection of $H_0$ suggests the presence of nonlinear structure. The analysis proceeds by validating the problem, outlining the theoretical principles, and then detailing the algorithmic implementation.\n\nThe problem is deemed valid as it is scientifically grounded in established methods of nonlinear time series analysis, is well-posed with all necessary parameters and definitions provided, and is expressed in objective, formal language. There are no contradictions, ambiguities, or factual inaccuracies.\n\nThe core principle of the surrogate data method is to generate an ensemble of time series, known as surrogates, that share specific properties of the original data consistent with the null hypothesis, while being random in other respects. For the given null hypothesis, the surrogates must preserve the linear properties of the original data, which are captured by the autocorrelation function or, equivalently, the power spectral density (PSD). The IAAFT algorithm is specifically designed to create surrogates that match both the PSD and the amplitude distribution of the original series.\n\nLet the original, discrete-time series be $x[t]$ for $t = 0, 1, \\dots, N-1$. It is first standardized to have a zero mean and unit variance. Its Discrete Fourier Transform (DFT) is $X[k]$. The PSD is proportional to $|X[k]|^2$. The IAAFT method generates a surrogate series $y[t]$ that aims to satisfy two constraints simultaneously:\n$1$. The set of values in $y[t]$ is a permutation of the values in $x[t]$, thus preserving the amplitude distribution.\n$2$. The magnitude of the DFT of $y[t]$, denoted $|Y[k]|$, is equal to the magnitude of the DFT of $x[t]$, i.e., $|Y[k]| = |X[k]|$.\n\nThe IAAFT algorithm achieves this iteratively. Starting with an initial random permutation of $x[t]$ as the first surrogate guess, $y^{(0)}[t]$, the algorithm alternates between two projection steps:\n$1$. **Spectral Projection**: The current surrogate $y^{(m)}[t]$ is transformed to the frequency domain to obtain its DFT, $Y^{(m)}[k]$. The phases of $Y^{(m)}[k]$, denoted $\\angle Y^{(m)}[k]$, are retained, but the magnitudes are replaced by the target magnitudes $|X[k]|$ from the original data. This creates a new complex spectrum $\\tilde{Y}^{(m)}[k] = |X[k]| e^{i \\angle Y^{(m)}[k]}$. An inverse DFT is then applied to obtain a time-domain signal $\\tilde{y}^{(m)}[t]$. This step enforces the desired PSD but disrupts the amplitude distribution.\n$2$. **Amplitude Projection**: The amplitude distribution of $\\tilde{y}^{(m)}[t]$ is adjusted to match that of $x[t]$. This is achieved by rank-ordering. The values of $\\tilde{y}^{(m)}[t]$ are replaced by the values of $x[t]$ according to their rank. Specifically, the smallest value in $\\tilde{y}^{(m)}[t]$ is replaced by the smallest value in $x[t]$, the second-smallest by the second-smallest, and so on. This process yields the next iteration of the surrogate, $y^{(m+1)}[t]$. This step enforces the desired amplitude distribution but perturbs the PSD.\n\nThese two projections are repeated until the surrogate simultaneously satisfies both constraints to a desired precision. Convergence is monitored by the relative spectral error, $E^{(m)} = \\frac{\\left\\|\\,| \\mathrm{DFT}(y^{(m)}[t])| - |X[k]|\\,\\right\\|_2}{\\left\\||X[k]|\\right\\|_2}$, where $\\|\\cdot\\|_2$ denotes the Euclidean norm. The iteration stops when $E^{(m)}$ is below a tolerance $\\tau$ or a maximum number of iterations is reached.\n\nTo test for nonlinearity, a test statistic sensitive to properties not preserved by the surrogates is required. The problem specifies a measure of time-reversal asymmetry, defined via the normalized third moment of the time series increments. The increment sequence is $d[t] = x[t+1] - x[t]$ for $t = 0, \\dots, N-2$. The test statistic is:\n$$\nT(x) = \\frac{1}{N-1} \\sum_{t=0}^{N-2} \\frac{(d[t])^3}{\\sigma_d^3 + \\varepsilon}\n$$\nwhere $\\sigma_d$ is the standard deviation of the increments $d[t]$ and $\\varepsilon$ is a small regularization constant. For a linear Gaussian process, the distribution of increments is symmetric, and $T(x)$ is expected to be close to zero. Significant deviations from zero suggest time-reversal asymmetry, a hallmark of certain nonlinear dynamics.\n\nThe statistical test proceeds as follows:\n$1$. Calculate the statistic $T_{obs} = T(x)$ for the original data.\n$2$. Generate a large number, $S$, of IAAFT surrogates, $\\{y_s[t]\\}_{s=1}^S$.\n$3$. Calculate the statistic $T_s = T(y_s)$ for each surrogate, forming a surrogate distribution.\n$4$. Calculate the mean of the surrogate statistics, $\\mu_s = \\frac{1}{S} \\sum_{s=1}^S T_s$.\n$5$. Compute a two-sided empirical $p$-value, which quantifies the probability of observing a deviation from the surrogate mean as large as or larger than the observed deviation, under the null hypothesis:\n$$\np = \\frac{1 + \\#\\{s: |T_s - \\mu_s| \\ge |T_{obs} - \\mu_s|\\}}{1 + S}\n$$\n$6$. If the calculated $p$-value is less than a predetermined significance level $\\alpha$ (here, $\\alpha=0.01$), the null hypothesis is rejected, and the data are considered to contain significant nonlinear structure.\n\nThe implementation generates four time series as specified: white Gaussian noise (Case A), a linear autoregressive process (Case B), a nonlinear autoregressive process (Case C), and a shorter linear autoregressive process (Case D). For the autoregressive models, a burn-in period is used to ensure the generated series is stationary. Each series is then standardized. For each case, $S=40$ surrogates are generated using the IAAFT algorithm with a maximum of $100$ iterations and a tolerance of $10^{-6}$. The test statistic $T(x)$ is computed for the original data and all surrogates, and the $p$-value is determined. The final output is a boolean indicating whether $p  \\alpha$ for each case. We expect to find no significant nonlinearity for the linear processes (Cases A, B, D) and significant nonlinearity for the nonlinear process (Case C).",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final result.\n    \"\"\"\n    # Set a seed for reproducibility of random processes.\n    np.random.seed(42)\n\n    # --- Problem Parameters ---\n    S = 40  # Number of surrogates\n    MAX_ITER = 100  # Max IAAFT iterations\n    TOL = 1e-6  # IAAFT convergence tolerance\n    EPSILON = 1e-12  # Regularization term for T-statistic\n    ALPHA = 0.01  # Significance level\n\n    # --- Test Case Definitions ---\n    # Case A: White noise\n    x_A = np.random.randn(2048)\n\n    # Case B: AR(1) process\n    x_B = generate_ar_process(N=2048, phi=0.9)\n\n    # Case C: Nonlinear process\n    x_C = generate_ar_process(N=2048, phi=0.8, beta=0.3)\n\n    # Case D: Short AR(1) process\n    x_D = generate_ar_process(N=512, phi=0.95)\n\n    test_cases = [x_A, x_B, x_C, x_D]\n    results = []\n\n    for x_orig in test_cases:\n        is_significant = perform_significance_test(\n            x_orig, S, MAX_ITER, TOL, EPSILON, ALPHA\n        )\n        results.append(is_significant)\n    \n    # Format and print the final output as a boolean list.\n    # The boolean values are capitalized in Python, so convert to lowercase 'true'/'false'\n    # for full compliance with example if interpreted as JSON boolean.\n    # Here, string representation of Python's bool (`True`/`False`) is used as per common interpretation.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_ar_process(N, phi, beta=0.0, burn_in=500):\n    \"\"\"\n    Generates a linear or nonlinear autoregressive time series.\n    \n    Args:\n        N (int): The desired length of the final time series.\n        phi (float): The AR(1) coefficient.\n        beta (float, optional): The coefficient for the nonlinear term x[t-1]^2. Defaults to 0.0.\n        burn_in (int, optional): Number of initial points to discard. Defaults to 500.\n\n    Returns:\n        np.ndarray: The generated time series of length N.\n    \"\"\"\n    length = N + burn_in\n    e = np.random.randn(length)\n    x = np.zeros(length, dtype=np.float64)\n    for t in range(1, length):\n        x[t] = phi * x[t-1] + beta * x[t-1]**2 + e[t]\n    return x[burn_in:]\n\ndef standardize(x):\n    \"\"\"\n    Standardizes a time series to zero mean and unit variance.\n    \n    Args:\n        x (np.ndarray): The input time series.\n\n    Returns:\n        np.ndarray: The standardized time series.\n    \"\"\"\n    return (x - np.mean(x)) / np.std(x)\n\ndef calculate_T(x, epsilon):\n    \"\"\"\n    Calculates the time-reversal asymmetry test statistic T(x).\n    \n    Args:\n        x (np.ndarray): The input time series.\n        epsilon (float): Small constant to prevent division by zero.\n\n    Returns:\n        float: The value of the test statistic T(x).\n    \"\"\"\n    N = len(x)\n    d = x[1:] - x[:-1]\n    sigma_d = np.std(d)\n    # The problem specifies epsilon is added to sigma_d^3\n    denominator = sigma_d**3 + epsilon\n    if denominator == 0:\n        return 0.0 # Should not happen with epsilon, but as a safeguard.\n    T_val = (1.0 / (N - 1)) * np.sum(np.power(d, 3) / denominator)\n    return T_val\n\ndef generate_iaaft_surrogate(x, max_iter, tol):\n    \"\"\"\n    Generates one IAAFT surrogate for the given time series.\n    \n    Args:\n        x (np.ndarray): The original time series (must be pre-standardized).\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance for relative spectral error.\n\n    Returns:\n        np.ndarray: The generated surrogate time series.\n    \"\"\"\n    # Store properties of the original series\n    x_sorted = np.sort(x)\n    X_mag = np.abs(np.fft.fft(x))\n    X_mag_norm = np.linalg.norm(X_mag)\n    if X_mag_norm == 0:\n        return x # Return original if it's a zero series\n\n    # 1. Start with a random permutation of x\n    y = np.random.permutation(x)\n\n    for _ in range(max_iter):\n        # 2a. Spectral projection: Impose the power spectrum of x\n        Y = np.fft.fft(y)\n        Y_phases = np.angle(Y)\n        Y_tilde = X_mag * np.exp(1j * Y_phases)\n        y_tilde = np.real(np.fft.ifft(Y_tilde))\n\n        # 2b. Amplitude projection: Impose the amplitude distribution of x\n        sort_indices = np.argsort(y_tilde)\n        y_next = np.zeros_like(y)\n        y_next[sort_indices] = x_sorted\n        y = y_next\n\n        # 3. Check for convergence\n        Y_mag_current = np.abs(np.fft.fft(y))\n        error = np.linalg.norm(Y_mag_current - X_mag) / X_mag_norm\n        if error  tol:\n            break\n            \n    return y\n\ndef perform_significance_test(x_orig, S, max_iter, tol, epsilon, alpha):\n    \"\"\"\n    Performs the full significance test for a given time series.\n    \n    Args:\n        x_orig (np.ndarray): The original time series.\n        S (int): Number of surrogates to generate.\n        max_iter (int): Max IAAFT iterations.\n        tol (float): IAAFT convergence tolerance.\n        epsilon (float): Regularizer for T-statistic.\n        alpha (float): Significance level for p-value.\n\n    Returns:\n        bool: True if the result is significant (p  alpha), False otherwise.\n    \"\"\"\n    # All time series must be standardized before analysis\n    x = standardize(x_orig)\n    \n    # Calculate statistic for observed data\n    T_obs = calculate_T(x, epsilon)\n    \n    # Generate surrogates and calculate their statistics\n    T_surr_list = []\n    for _ in range(S):\n        y = generate_iaaft_surrogate(x, max_iter, tol)\n        T_surr = calculate_T(y, epsilon)\n        T_surr_list.append(T_surr)\n        \n    T_surr_arr = np.array(T_surr_list)\n    mu_s = np.mean(T_surr_arr)\n    \n    # Calculate two-sided p-value\n    dev_obs = np.abs(T_obs - mu_s)\n    dev_surr = np.abs(T_surr_arr - mu_s)\n    \n    # Count how many surrogate deviations are = observed deviation\n    count = np.sum(dev_surr = dev_obs)\n    \n    p_val = (1.0 + count) / (1.0 + S)\n    \n    return p_val  alpha\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}