{
    "hands_on_practices": [
        {
            "introduction": "泊松过程是描述神经元尖峰序列的基础模型。在构建更复杂的模型之前，我们必须掌握其基本原理：如何从观测到的尖峰计数中估计其唯一的参数——发放率 $\\lambda$。这项练习将引导你从最大似然估计（MLE）的基本定义出发，推导出 $\\lambda$ 的估计量，并进一步推导克拉默-拉奥下界（CRB），该下界为任何无偏估计量的方差设定了一个理论上的极限 。",
            "id": "4181776",
            "problem": "在持续时间为 $T > 0$ 秒的固定观测窗口内记录单个神经元。在神经脉冲序列的标准噪声模型下，假设该窗口内的脉冲生成可以由一个恒定速率参数 $\\lambda > 0$ 的齐次泊松过程很好地描述，因此在窗口中观测到的总脉冲数 $K$ 是来自均值为 $\\lambda T$ 的泊松分布的单次抽样。从泊松模型的似然函数的基本定义和费雪信息 (Fisher information) 的定义出发，根据观测到的脉冲数 $K$ 推导 $\\lambda$ 的最大似然估计 (Maximum Likelihood Estimate, MLE)，并使用此观测模型推导 $\\lambda$ 的任何无偏估计量方差的克拉美-罗界 (Cramér-Rao Bound, CRB)。将您的最终答案表示为无单位的闭式符号表达式。将这两个表达式放在一个单行矩阵中，其中第一个条目是 MLE，第二个条目是此模型下 $\\operatorname{Var}(\\hat{\\lambda})$ 的 CRB。",
            "solution": "该问题要求推导与神经元放电率 $\\lambda$ 估计相关的两个量。数据模型为，在持续时间为 $T > 0$ 的固定时间窗口中观测到的总脉冲数 $K$ 是来自均值为 $\\lambda T$ 的泊松分布的单次抽样，其中 $\\lambda > 0$ 是恒定速率参数。\n\n首先，我们将推导 $\\lambda$ 的最大似然估计 (MLE)。MLE 的基础是似然函数，它是观测数据 $K$ 作为参数 $\\lambda$ 的函数的概率。均值为 $\\mu$ 的泊松分布的概率质量函数是 $P(\\text{count}=k) = \\frac{\\mu^k \\exp(-\\mu)}{k!}$。对于我们的模型，均值为 $\\mu = \\lambda T$。因此，观测到特定脉冲数 $K$ 的似然函数 $\\mathcal{L}(\\lambda | K)$ 为：\n$$\\mathcal{L}(\\lambda | K) = \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!}$$\n为了找到 MLE，我们寻求使该函数最大化的 $\\lambda$ 值。这等价于最大化似然函数的自然对数，即对数似然函数 $\\ell(\\lambda | K) = \\ln(\\mathcal{L}(\\lambda | K))$，这在数学上更为方便。\n$$\\ell(\\lambda | K) = \\ln\\left( \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!} \\right)$$\n利用对数的性质，我们展开这个表达式：\n$$\\ell(\\lambda | K) = \\ln((\\lambda T)^K) - \\ln(\\exp(\\lambda T)) - \\ln(K!)$$\n$$\\ell(\\lambda | K) = K \\ln(\\lambda T) - \\lambda T - \\ln(K!)$$\n这可以进一步展开为：\n$$\\ell(\\lambda | K) = K \\ln(\\lambda) + K \\ln(T) - \\lambda T - \\ln(K!)$$\n为了找到最大值，我们对 $\\ell(\\lambda | K)$ 关于 $\\lambda$ 求一阶导数，并令结果为零。注意，在此微分中，$K$ 和 $T$ 被视为常数。\n$$\\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda} (K \\ln(\\lambda) + K \\ln(T) - \\lambda T - \\ln(K!)) = K \\cdot \\frac{1}{\\lambda} + 0 - T - 0 = \\frac{K}{\\lambda} - T$$\n将此导数设为零，得到 $\\lambda$ 的 MLE（表示为 $\\hat{\\lambda}_{MLE}$）的方程：\n$$\\frac{K}{\\hat{\\lambda}_{MLE}} - T = 0$$\n求解 $\\hat{\\lambda}_{MLE}$：\n$$\\hat{\\lambda}_{MLE} = \\frac{K}{T}$$\n为了验证该值对应于一个最大值，我们检查对数似然函数的二阶导数：\n$$\\frac{d^2\\ell}{d\\lambda^2} = \\frac{d}{d\\lambda} \\left(\\frac{K}{\\lambda} - T\\right) = -\\frac{K}{\\lambda^2}$$\n由于脉冲数 $K$ 是一个非负整数 ($K \\geq 0$) 且速率 $\\lambda$ 为正 ($\\lambda > 0$)，二阶导数 $\\frac{d^2\\ell}{d\\lambda^2} = -\\frac{K}{\\lambda^2}$ 始终为非正。如果 $K > 0$，二阶导数严格为负，证实了 $\\hat{\\lambda}_{MLE} = \\frac{K}{T}$ 是一个局部最大值。如果 $K = 0$，似然函数 $\\mathcal{L}(\\lambda|0) = \\exp(-\\lambda T)$ 是一个关于 $\\lambda$ 的严格递减函数，其最大值在 $\\lambda \\to 0$ 时达到。我们的公式 $\\hat{\\lambda}_{MLE} = 0/T = 0$ 与这种情况一致。因此，第一个表达式是 $\\frac{K}{T}$。\n\n其次，我们将推导 $\\lambda$ 的任何无偏估计量方差的克拉美-罗界 (CRB)。CRB 是费雪信息 $I(\\lambda)$ 的倒数。参数 $\\lambda$ 的费雪信息定义为：\n$$I(\\lambda) = -E\\left[\\frac{d^2}{d\\lambda^2}\\ell(\\lambda | K)\\right]$$\n期望 $E[\\cdot]$ 是关于数据 $K$ 的概率分布计算的，其中 $K \\sim \\text{Poisson}(\\lambda T)$。我们已经计算了对数似然函数的二阶导数：\n$$\\frac{d^2\\ell}{d\\lambda^2} = -\\frac{K}{\\lambda^2}$$\n现在，我们计算其期望：\n$$E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = E\\left[-\\frac{K}{\\lambda^2}\\right]$$\n由于 $\\lambda$ 是我们正在估计的参数，因此在对 $K$ 求期望时，它被视为一个常数：\n$$E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = -\\frac{1}{\\lambda^2}E[K]$$\n泊松分布随机变量的期望值是其均值参数。这里，$E[K] = \\lambda T$。将此代入方程：\n$$E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = -\\frac{1}{\\lambda^2}(\\lambda T) = -\\frac{T}{\\lambda}$$\n利用这个结果，我们得到费雪信息：\n$$I(\\lambda) = -E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = -\\left(-\\frac{T}{\\lambda}\\right) = \\frac{T}{\\lambda}$$\n克拉美-罗不等式指出，对于 $\\lambda$ 的任何无偏估计量 $\\hat{\\lambda}$，其方差的下界是费雪信息的倒数：\n$$\\operatorname{Var}(\\hat{\\lambda}) \\ge \\frac{1}{I(\\lambda)}$$\n克拉美-罗界就是这个下限：\n$$\\text{CRB} = \\frac{1}{I(\\lambda)} = \\frac{1}{T/\\lambda} = \\frac{\\lambda}{T}$$\n这是第二个所需的表达式。\n\n解答要求将这两个表达式，即 MLE 和 CRB，放在一个单行矩阵中。\nMLE 是 $\\frac{K}{T}$。\n$\\operatorname{Var}(\\hat{\\lambda})$ 的 CRB 是 $\\frac{\\lambda}{T}$。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{K}{T} \\quad \\frac{\\lambda}{T} \\end{pmatrix}}$$"
        },
        {
            "introduction": "真实的神经数据通常表现出比简单泊松过程所能解释的更大的变异性，这种现象被称为“过度离散”（overdispersion）。这项练习将深入探讨建模的一个关键环节：为你的数据选择最佳模型。你将通过编程实现标准泊松模型与更灵活的负二项分布模型之间的比较，使用诸如对数分数和布莱尔分数等严谨的评分规则，来评估它们在保留数据集上的预测性能 。",
            "id": "4181830",
            "problem": "您将获得在固定时长观测窗口中收集的离散试验脉冲发放计数，这些计数被建模为整数值随机变量。对此类计数，两种典范噪声模型是泊松分布和负二项分布，它们在神经科学数据分析中被频繁使用，分别用于捕捉等离散和过离散现象。本问题要求您通过计算指定试验计数上的留出对数得分和Brier得分，来比较这些模型的预测校准情况，其中需使用基于原则的估计程序和定义明确的预测评分规则。\n\n使用以下基础知识：\n- 率参数为 $\\lambda$ 的泊松分布定义在 $\\{0,1,2,\\dots\\}$ 上，其概率质量函数为 $p(y \\mid \\lambda) = \\frac{\\exp(-\\lambda) \\lambda^{y}}{y!}$。在固定窗口内，独立发放假设下的脉冲发放计数通常被建模为泊松分布。\n- 在“获得k次成功前失败的次数”参数化下，参数为 $(k, p)$ 的负二项分布的概率质量函数为 $p(y \\mid k, p) = \\binom{y + k - 1}{y} (1 - p)^{y} p^{k}$，适用于 $y \\in \\{0,1,2,\\dots\\}$，其均值为 $\\mu = k (1 - p) / p$，方差为 $\\sigma^{2} = \\mu + \\mu^{2} / k$。当 $\\sigma^{2} > \\mu$ 时，该分布捕捉了相对于泊松分布的过离散现象。\n- 在独立试验中，泊松率的最大似然估计产生的估计量 $\\hat{\\lambda}$ 等于训练计数的样本均值。\n- 对于均值为 $\\mu$、尺寸参数为 $k$ 的负二项分布，矩估计法使用训练样本均值 $m$ 和样本方差 $s^{2}$ 来求解 $m = \\mu$ 和 $s^{2} = \\mu + \\mu^{2} / k$。如果 $s^{2} > m$，则 $\\hat{k} = m^{2} / (s^{2} - m)$ 且 $\\hat{p} = \\hat{k} / (\\hat{k} + m)$。如果 $s^{2} \\le m$，则将负二项预测分布视为均值为 $m$ 的泊松分布（等价于 $\\hat{k} \\to \\infty$）。\n\n定义预测评分规则如下：\n- 对于一个测试计数 $y$，留出对数得分是 $\\log p(y \\mid \\text{predictive})$，其中 $p$ 是拟合模型下的预测概率质量。请使用自然对数。对于一个测试集 $\\{y_{i}\\}_{i=1}^{n}$，报告平均留出对数得分 $\\frac{1}{n} \\sum_{i=1}^{n} \\log p(y_{i} \\mid \\text{predictive})$。\n- 对于一个测试计数 $y$ 和一个在多个区间上的离散预测分布 $p_{k}$，Brier得分是预测概率与观测区间独热编码之间的平方 $\\ell_{2}$ 误差。对于具有无界支撑集的计数数据，定义一个带有区间 $\\{0,1,\\dots,K\\}$ 和一个尾部区间 $K^{+}$ 的截断多类别Brier得分。令 $p_{k} = P(Y = k)$ 对于 $k \\in \\{0,\\dots,K\\}$，以及 $p_{K^{+}} = 1 - \\sum_{k=0}^{K} p_{k}$。令 $y_{k} = 1$ 如果 $y = k$，否则 $y_{k} = 0$，对于 $k \\in \\{0,\\dots,K\\}$；并令 $y_{K^{+}} = 1$ 如果 $y > K$，否则 $y_{K^{+}} = 0$。单个观测的Brier得分是 $\\sum_{b \\in \\{0,\\dots,K,K^{+}\\}} (p_{b} - y_{b})^{2}$，而一个测试集上的平均Brier得分是所有测试观测的平均值。\n\n为每个测试用例实现以下程序：\n- 通过 $\\hat{\\lambda} = m$ 拟合泊松率，其中 $m$ 是训练样本均值。\n- 通过矩估计法拟合负二项分布：计算训练样本均值 $m$ 和无偏样本方差 $s^{2}$，如果 $s^{2} > m$，则设 $\\hat{k} = m^{2} / (s^{2} - m)$ 和 $\\hat{p} = \\hat{k} / (\\hat{k} + m)$；否则，对负二项模型使用泊松预测分布。\n- 计算两个模型在测试集上的平均留出对数得分。\n- 使用特定于测试用例的截断参数 $K$（包括尾部区间 $K^{+}$），计算两个模型在测试集上的平均截断Brier得分。\n\n测试套件：\n- 用例1：训练计数 $\\{2,0,3,1,4,0,2,5,1,3\\}$，测试计数 $\\{0,2,1,4,6\\}$，截断参数 $K = 10$。\n- 用例2：训练计数 $\\{1,0,1,1,0,2,1,0,1,1\\}$，测试计数 $\\{0,1,1,2,0\\}$，截断参数 $K = 5$。\n- 用例3：训练计数 $\\{0,0,0,1,0,0,2,0,0,0\\}$，测试计数 $\\{0,0,1,0,3\\}$，截断参数 $K = 6$。\n- 用例4：训练计数 $\\{10,12,8,20,9,17\\}$，测试计数 $\\{7,20,13\\}$，截断参数 $K = 30$。\n\n必需的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。\n- 对于每个测试用例，按顺序输出四个保留 $6$ 位小数的浮点数：泊松模型下的平均留出对数得分，负二项模型下的平均留出对数得分，泊松模型下的平均Brier得分，以及负二项模型下的平均Brier得分。\n- 最终输出列表将所有测试用例的这四个浮点数连接起来，形成一个长度为 $16$ 的单一列表。例如，格式应类似于 `[r_1,r_2,...,r_16]`，其中每个 $r_{i}$ 是一个保留 $6$ 位小数的浮点数。",
            "solution": "我们从泊松分布和负二项分布的核心定义、它们在脉冲发放计数建模中的作用，以及从第一性原理推导出的标准估计程序开始。\n\n对于泊松分布，一个整数计数 $y \\in \\{0,1,2,\\dots\\}$ 在率参数为 $\\lambda$ 时的概率质量函数是 $p(y \\mid \\lambda) = \\frac{\\exp(-\\lambda) \\lambda^{y}}{y!}$。在固定观测窗口内独立脉冲发放的假设下，脉冲数量被建模为泊松分布，对于独立训练观测值 $\\{y_{i}\\}_{i=1}^{n}$ 的联合似然为 $L(\\lambda) = \\prod_{i=1}^{n} \\frac{\\exp(-\\lambda) \\lambda^{y_{i}}}{y_{i}!}$。对数似然为 $\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + y_{i} \\log \\lambda - \\log y_{i}! \\right ) = -n \\lambda + \\left( \\sum_{i=1}^{n} y_{i} \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log y_{i}!$。关于 $\\lambda$ 求导得到 $\\frac{\\partial \\ell}{\\partial \\lambda} = -n + \\left( \\sum_{i=1}^{n} y_{i} \\right ) \\frac{1}{\\lambda}$，令其为零可得 $\\hat{\\lambda} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$，即样本均值。在泊松模型下，这个最大似然估计量是一致且无偏的。\n\n对于“在获得k次成功前失败的次数”参数化下的负二项分布，其参数为 $(k, p)$，对于 $y \\in \\{0,1,2,\\dots\\}$ 的概率质量函数是 $p(y \\mid k, p) = \\binom{y + k - 1}{y} (1 - p)^{y} p^{k}$。其均值和方差满足 $\\mu = k (1 - p) / p$ 和 $\\sigma^{2} = \\mu + \\mu^{2} / k$。该分布常用于对过离散的脉冲计数（方差超过均值）进行建模，以捕捉异质性或超泊松变异性。矩估计法拟合使用训练样本均值 $m = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$ 和无偏样本方差 $s^{2} = \\frac{1}{n - 1} \\sum_{i=1}^{n} (y_{i} - m)^{2}$，通过 $s^{2} = m + m^{2} / k$ 来匹配 $\\mu = m$ 和 $\\sigma^{2} = s^{2}$，只要 $s^{2} > m$，即可得到 $k = m^{2} / (s^{2} - m)$。求解 $p$ 可得 $p = k / (k + m)$。当 $s^{2} \\le m$ 时，负二项分布无法表示相对于泊松分布的欠离散，因此一个基于原则的极限 $k \\to \\infty$ 将负二项分布简化为均值为 $m$ 的泊松分布。因此，在实践中，我们在这种情况下将负二项预测分布视为泊松分布。\n\n预测校准通过正常评分规则进行评估。对于一个观测到的测试计数 $y$，在预测分布 $p(\\cdot)$ 下的留出对数得分是 $\\log p(y)$，使用自然对数。对于一个测试集 $\\{y_{i}\\}_{i=1}^{n}$，平均留出对数得分是 $\\frac{1}{n} \\sum_{i=1}^{n} \\log p(y_{i})$。该得分是严格正常的：它在期望上被真实的预测分布所最大化。\n\n对于具有无界支撑集的计数，Brier得分（最初为二元结果定义）通过考虑将结果划分为有限的区间，推广到多类别设置。使用截断参数 $K$，定义区间 $\\{0,1,\\dots,K\\}$ 和一个尾部区间 $K^{+}$。对于一个具有预测质量函数 $p(y)$ 的模型，令 $p_{k} = P(Y = k)$ 对于 $k \\in \\{0,\\dots,K\\}$，以及 $p_{K^{+}} = 1 - \\sum_{k=0}^{K} p_{k}$。对于一个已实现的测试计数 $y$，定义在这些区间上的独热向量 $y_{b}$：对于 $k \\in \\{0,\\dots,K\\}$，$y_k = 1$ 如果 $y=k$ 否则为 $0$；$y_{K^{+}} = 1$ 如果 $y>K$ 否则为 $0$。Brier得分是预测概率向量与独热标签之间的平方欧几里得距离，即 $\\sum_{b \\in \\{0,\\dots,K,K^{+}\\}} (p_{b} - y_{b})^{2}$。一个测试集上的平均Brier得分是该量在所有测试中的平均值。当选择的 $K$ 使得尾部质量很小或通过 $K^{+}$ 明确地加以考虑时，这个截断定义是科学上有意义的。\n\n每个测试用例的算法步骤：\n- 计算训练样本均值 $m$ 和无偏样本方差 $s^{2}$。\n- 拟合泊松参数 $\\hat{\\lambda} = m$。\n- 通过矩估计法拟合负二项参数：如果 $s^{2} > m$，则设 $\\hat{k} = m^{2} / (s^{2} - m)$ 和 $\\hat{p} = \\hat{k} / (\\hat{k} + m)$；否则，设置一个标志以对负二项得分使用泊松预测分布。\n- 对于每个测试计数 $y$，使用拟合的泊松和拟合的负二项预测分布计算对数得分 $\\log p(y)$，并在测试集上取平均值。\n- 对于每个测试计数 $y$，使用每个用例指定的截断参数 $K$ 计算截断多类别Brier得分，区间为 $\\{0,\\dots,K,K^{+}\\}$。对于每个模型，从其概率质量函数计算 $k \\in \\{0,\\dots,K\\}$ 的 $p_{k}$，并使用在 $K$ 处的累积分布函数计算 $p_{K^{+}} = 1 - \\sum_{k=0}^{K} p_{k}$。形成独热向量 $y_{b}$ 并计算平方误差和。在测试集上取平均值。\n- 将所有报告的得分四舍五入到 $6$ 位小数。\n- 按顺序连接每个用例的四个得分：平均留出对数得分（泊松）、平均留出对数得分（负二项）、平均Brier得分（泊松）、平均Brier得分（负二项）。\n- 汇总所有用例的结果，并打印一个包含在方括号中的逗号分隔列表的单行。\n\n边缘情况和鲁棒性：\n- $s^{2} \\le m$ 的情况产生的负二项参数意味着 $k \\to \\infty$；预测分布收敛于均值为 $m$ 的泊松分布。在这种情况下，实现对负二项得分使用泊松分布。\n- 通过使用经过充分测试的概率质量函数和累积分布函数实现来保持数值稳定性。\n- 截断参数 $K$ 定义了一个尾部区间 $K^{+}$，它聚合了超过 $K$ 的质量，确保了在一个有限区间集合上的正常多类别Brier得分。\n\n将此程序应用于提供的测试套件，可以对用于神经试验计数的泊松模型和负二项模型之间的预测校准进行可复现的、程序化的比较。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson, nbinom\n\ndef fit_poisson(train_counts):\n    # MLE for Poisson rate: sample mean\n    lam = float(np.mean(train_counts)) if len(train_counts) > 0 else 0.0\n    return lam\n\ndef fit_nb_moments(train_counts, eps=1e-12):\n    # Method-of-moments for NB: match mean and variance\n    n = len(train_counts)\n    if n == 0:\n        # Degenerate; default to Poisson with lambda 0\n        return {\"use_poisson\": True, \"mu\": 0.0, \"k\": np.inf, \"p\": 1.0}\n    m = float(np.mean(train_counts))\n    # Unbiased sample variance; if n==1, set variance to 0\n    s2 = float(np.var(train_counts, ddof=1)) if n > 1 else 0.0\n    if s2 > m + eps:\n        k = m**2 / (s2 - m)\n        p = k / (k + m)\n        return {\"use_poisson\": False, \"mu\": m, \"k\": k, \"p\": p}\n    else:\n        # Underdispersion: NB reduces to Poisson\n        return {\"use_poisson\": True, \"mu\": m, \"k\": np.inf, \"p\": 1.0}\n\ndef mean_log_score(test_counts, model_dist):\n    # model_dist is a scipy.stats distribution object providing logpmf\n    log_scores = model_dist.logpmf(test_counts)\n    return float(np.mean(log_scores))\n\ndef mean_brier_score(test_counts, model_dist, K):\n    # Compute truncated multi-category Brier score with bins 0..K and tail K+\n    ks = np.arange(K + 1, dtype=int)\n    pmf_vals = model_dist.pmf(ks)  # shape (K+1,)\n    # Tail probability K+ = 1 - CDF(K)\n    tail_prob = 1.0 - model_dist.cdf(K)\n    # For each test count, compute Brier score\n    briers = []\n    for y in test_counts:\n        # One-hot vector over bins 0..K and tail\n        y_vec = np.zeros(K + 2, dtype=float)\n        if y = K:\n            y_vec[y] = 1.0\n        else:\n            y_vec[K + 1] = 1.0\n        p_vec = np.concatenate([pmf_vals, np.array([tail_prob])])\n        brier = np.sum((p_vec - y_vec) ** 2)\n        briers.append(brier)\n    return float(np.mean(briers))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (training_counts, test_counts, K)\n    test_cases = [\n        (np.array([2,0,3,1,4,0,2,5,1,3], dtype=int), np.array([0,2,1,4,6], dtype=int), 10),\n        (np.array([1,0,1,1,0,2,1,0,1,1], dtype=int), np.array([0,1,1,2,0], dtype=int), 5),\n        (np.array([0,0,0,1,0,0,2,0,0,0], dtype=int), np.array([0,0,1,0,3], dtype=int), 6),\n        (np.array([10,12,8,20,9,17], dtype=int), np.array([7,20,13], dtype=int), 30),\n    ]\n\n    results = []\n    for train_counts, test_counts, K in test_cases:\n        # Fit Poisson\n        lam = fit_poisson(train_counts)\n        pois_dist = poisson(mu=lam)\n\n        # Fit Negative Binomial\n        nb_params = fit_nb_moments(train_counts)\n        if nb_params[\"use_poisson\"]:\n            nb_dist = poisson(mu=nb_params[\"mu\"])\n        else:\n            # scipy.stats.nbinom parameterization: number of successes n=k, probability p\n            k = nb_params[\"k\"]\n            p = nb_params[\"p\"]\n            nb_dist = nbinom(n=k, p=p)\n\n        # Mean held-out log scores\n        ls_pois = mean_log_score(test_counts, pois_dist)\n        ls_nb = mean_log_score(test_counts, nb_dist)\n\n        # Mean Brier scores (truncated with tail bin)\n        brier_pois = mean_brier_score(test_counts, pois_dist, K)\n        brier_nb = mean_brier_score(test_counts, nb_dist, K)\n\n        # Round to 6 decimals and append in required order\n        results.extend([\n            round(ls_pois, 6),\n            round(ls_nb, 6),\n            round(brier_pois, 6),\n            round(brier_nb, 6),\n        ])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "神经元的发放率很少是恒定的；它们会动态地波动以编码信息。这项练习介绍了用于建模时变潜在变量的强大工具——状态空间框架。你将从贝叶斯推断的第一性原理出发，推导著名的卡尔曼滤波器方程，学习如何在一个简化的线性高斯模型下，从一系列带噪声的观测中最佳地追踪一个变化的潜在发放率 。",
            "id": "4181853",
            "problem": "您正在分析单个神经元分箱后的脉冲计数。假设潜在放电率被建模为一维离散时间线性高斯状态，其先验为随机游走，观测模型为高斯分布，该模型根据中心极限定理（CLT）近似了脉冲计数的变异性。具体来说，假设潜在状态 $x_{t}$ 的演化遵循 $x_{t} = x_{t-1} + w_{t}$，其中 $w_{t} \\sim \\mathcal{N}(0, Q)$，观测值 $y_{t}$ 满足 $y_{t} = x_{t} + v_{t}$，其中 $v_{t} \\sim \\mathcal{N}(0, R)$。在时间 $t=0$ 时的先验为 $x_{0} \\sim \\mathcal{N}(m_{0}, P_{0})$，并且噪声 $w_{t}$ 和 $v_{t}$ 相互独立且在时间上独立。\n\n从贝叶斯法则和正态（高斯）分布的性质出发——不假设或调用卡尔曼滤波器公式——推导上述一维情况下后验均值 $m_{t \\mid t}$ 和后验方差 $P_{t \\mid t}$ 的递归预测和更新方程。您的推导必须明确展示高斯先验和高斯似然如何结合产生高斯后验，并且必须阐明 $Q$ 和 $R$ 的作用。\n\n然后，使用 $m_{0} = 0$、$P_{0} = 4$、$Q = 1$ 和 $R = 9$，计算前三次观测（$t = 1, 2, 3$）后每一次的后验方差减少量 $\\Delta_{t} = P_{t \\mid t-1} - P_{t \\mid t}$。将最终的方差减少量表示为一个单行矩阵中的精确有理数。不需要四舍五入。",
            "solution": "该问题要求两个主要部分：首先，从第一性原理出发，推导一维线性高斯状态空间模型的后验均值和方差的递归预测和更新方程；其次，使用给定的数值计算前三个时间步的后验方差减少量。\n\n模型定义如下：\n状态转移：$x_{t} = x_{t-1} + w_{t}$，其中 $w_{t} \\sim \\mathcal{N}(0, Q)$\n观测：$y_{t} = x_{t} + v_{t}$，其中 $v_{t} \\sim \\mathcal{N}(0, R)$\n在 $t=0$ 时的先验：$p(x_0) = \\mathcal{N}(x_0; m_{0}, P_{0})$\n\n我们将时间 $t-1$ 时的后验分布表示为 $p(x_{t-1} \\mid y_{1:t-1}) = \\mathcal{N}(x_{t-1}; m_{t-1 \\mid t-1}, P_{t-1 \\mid t-1})$。目标是找到时间 $t$ 时的后验分布 $p(x_t \\mid y_{1:t}) = \\mathcal{N}(x_t; m_{t \\mid t}, P_{t \\mid t})$。这通过两个步骤实现：预测和更新。\n\n### 递归方程的推导\n\n#### 1. 预测步骤\n预测步骤计算在给定截至时间 $t-1$ 的所有观测值的情况下，状态 $x_t$ 在时间 $t$ 的先验分布。这个分布是 $p(x_t \\mid y_{1:t-1})$。我们通过对联合分布 $p(x_t, x_{t-1} \\mid y_{1:t-1})$ 对 $x_{t-1}$ 进行边缘化来求得它：\n$$p(x_t \\mid y_{1:t-1}) = \\int p(x_t, x_{t-1} \\mid y_{1:t-1}) \\, dx_{t-1}$$\n使用概率的链式法则，$p(x_t, x_{t-1} \\mid y_{1:t-1}) = p(x_t \\mid x_{t-1}, y_{1:t-1}) p(x_{t-1} \\mid y_{1:t-1})$。由于状态空间模型的马尔可夫性质，$x_t$ 的分布仅依赖于 $x_{t-1}$，所以 $p(x_t \\mid x_{t-1}, y_{1:t-1}) = p(x_t \\mid x_{t-1})$。这得到：\n$$p(x_t \\mid y_{1:t-1}) = \\int p(x_t \\mid x_{t-1}) p(x_{t-1} \\mid y_{1:t-1}) \\, dx_{t-1}$$\n积分中的两个分布是已知的：\n- $p(x_t \\mid x_{t-1})$ 是状态转移模型：$x_t = x_{t-1} + w_t$，所以 $x_t \\sim \\mathcal{N}(x_{t-1}, Q)$。\n- $p(x_{t-1} \\mid y_{1:t-1})$ 是上一步的后验分布：$x_{t-1} \\sim \\mathcal{N}(m_{t-1 \\mid t-1}, P_{t-1 \\mid t-1})$。\n\n该积分表示两个独立高斯随机变量之和的分布：$x_t = x_{t-1} + w_t$。结果分布也是高斯分布。其均值是均值之和，其方差是方差之和。\n预测均值 $m_{t \\mid t-1}$ 是：\n$$m_{t \\mid t-1} = E[x_t \\mid y_{1:t-1}] = E[x_{t-1} + w_t \\mid y_{1:t-1}] = E[x_{t-1} \\mid y_{1:t-1}] + E[w_t] = m_{t-1 \\mid t-1} + 0 = m_{t-1 \\mid t-1}$$\n预测方差 $P_{t \\mid t-1}$ 是：\n$$P_{t \\mid t-1} = \\text{Var}(x_t \\mid y_{1:t-1}) = \\text{Var}(x_{t-1} + w_t \\mid y_{1:t-1}) = \\text{Var}(x_{t-1} \\mid y_{1:t-1}) + \\text{Var}(w_t) = P_{t-1 \\mid t-1} + Q$$\n这里，$Q$ 的作用很明确：它表示在从 $t-1$ 到 $t$ 的演化过程中，由于过程噪声导致的状态不确定性的增加。\n因此，预测分布为 $p(x_t \\mid y_{1:t-1}) = \\mathcal{N}(x_t; m_{t \\mid t-1}, P_{t \\mid t-1})$。\n\n#### 2. 更新步骤\n更新步骤结合了新的观测值 $y_t$ 以将预测分布精炼为后验分布 $p(x_t \\mid y_{1:t})$。我们使用贝叶斯法则：\n$$p(x_t \\mid y_{1:t})= p(x_t \\mid y_t, y_{1:t-1}) \\propto p(y_t \\mid x_t, y_{1:t-1}) p(x_t \\mid y_{1:t-1})$$\n根据模型结构，观测值 $y_t$ 仅依赖于当前状态 $x_t$，所以 $p(y_t \\mid x_t, y_{1:t-1}) = p(y_t \\mid x_t)$。这得到：\n$$p(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t) p(x_t \\mid y_{1:t-1})$$\n后验分布与似然和先验的乘积成正比：\n- 似然：$p(y_t \\mid x_t)$ 来自观测模型 $y_t = x_t + v_t$，所以 $y_t \\sim \\mathcal{N}(x_t, R)$，这意味着 $p(y_t \\mid x_t) = \\mathcal{N}(y_t; x_t, R)$。\n- 先验：$p(x_t \\mid y_{1:t-1})$ 是上一步的预测分布，$\\mathcal{N}(x_t; m_{t \\mid t-1}, P_{t \\mid t-1})$。\n\n后验分布与两个高斯概率密度函数的乘积成正比：\n$$p(x_t \\mid y_{1:t}) \\propto \\exp\\left(-\\frac{(y_t - x_t)^2}{2R}\\right) \\exp\\left(-\\frac{(x_t - m_{t \\mid t-1})^2}{2P_{t \\mid t-1}}\\right)$$\n两个高斯函数的乘积是一个未归一化的高斯函数。为了找到其参数，我们合并指数中的项并对 $x_t$ 进行配方：\n$$-\\frac{1}{2} \\left[ \\frac{(x_t - y_t)^2}{R} + \\frac{(x_t - m_{t \\mid t-1})^2}{P_{t \\mid t-1}} \\right] = -\\frac{1}{2} \\left[ \\frac{x_t^2 - 2x_t y_t + y_t^2}{R} + \\frac{x_t^2 - 2x_t m_{t \\mid t-1} + m_{t \\mid t-1}^2}{P_{t \\mid t-1}} \\right]$$\n$$= -\\frac{1}{2} \\left[ x_t^2 \\left(\\frac{1}{R} + \\frac{1}{P_{t \\mid t-1}}\\right) - 2x_t \\left(\\frac{y_t}{R} + \\frac{m_{t \\mid t-1}}{P_{t \\mid t-1}}\\right) + \\text{const} \\right]$$\n得到的后验分布是一个高斯分布，$p(x_t \\mid y_{1:t}) = \\mathcal{N}(x_t; m_{t \\mid t}, P_{t \\mid t})$，其密度与 $\\exp\\left(-\\frac{(x_t - m_{t \\mid t})^2}{2P_{t \\mid t}}\\right) = \\exp\\left(-\\frac{1}{2P_{t \\mid t}}(x_t^2 - 2x_t m_{t \\mid t} + m_{t \\mid t}^2) \\right)$ 成正比。\n通过比较指数中 $x_t^2$ 和 $x_t$ 的系数，我们可以确定后验参数。\n后验方差 $P_{t \\mid t}$ 的倒数是先验和似然的方差倒数（精度）之和：\n$$\\frac{1}{P_{t \\mid t}} = \\frac{1}{P_{t \\mid t-1}} + \\frac{1}{R} \\implies P_{t \\mid t} = \\left(\\frac{1}{P_{t \\mid t-1}} + \\frac{1}{R}\\right)^{-1} = \\frac{P_{t \\mid t-1} R}{P_{t \\mid t-1} + R}$$\n后验均值 $m_{t \\mid t}$ 由下式给出：\n$$\\frac{m_{t \\mid t}}{P_{t \\mid t}} = \\frac{m_{t \\mid t-1}}{P_{t \\mid t-1}} + \\frac{y_t}{R}$$\n$$m_{t \\mid t} = P_{t \\mid t} \\left( \\frac{m_{t \\mid t-1}}{P_{t \\mid t-1}} + \\frac{y_t}{R} \\right) = \\frac{P_{t \\mid t-1} R}{P_{t \\mid t-1} + R} \\left( \\frac{m_{t \\mid t-1}}{P_{t \\mid t-1}} + \\frac{y_t}{R} \\right)$$\n$$m_{t \\mid t} = \\frac{R}{P_{t \\mid t-1} + R} m_{t \\mid t-1} + \\frac{P_{t \\mid t-1}}{P_{t \\mid t-1} + R} y_t$$\n这可以重排为带有增益项的熟悉形式：\n$$m_{t \\mid t} = m_{t \\mid t-1} + \\frac{P_{t \\mid t-1}}{P_{t \\mid t-1} + R} (y_t - m_{t \\mid t-1})$$\n定义卡尔曼增益 $K_t = \\frac{P_{t \\mid t-1}}{P_{t \\mid t-1} + R}$，更新方程为：\n$$m_{t \\mid t} = m_{t \\mid t-1} + K_t (y_t - m_{t \\mid t-1})$$\n$$P_{t \\mid t} = (1 - K_t) P_{t \\mid t-1}$$\n$R$ 的作用是调节赋予新观测值的权重。小的 $R$（低观测噪声）会导致大的增益 $K_t$，意味着我们更相信新数据。大的 $R$（高观测噪声）会导致小的 $K_t$，意味着我们更依赖于我们的预测。\n\n### 方差减少量的计算\n我们需要计算 $t=1, 2, 3$ 时的 $\\Delta_{t} = P_{t \\mid t-1} - P_{t \\mid t}$。\n根据方差的更新方程，$P_{t \\mid t} = P_{t \\mid t-1} - K_t P_{t \\mid t-1}$。\n因此，方差减少量为：\n$$\\Delta_{t} = K_t P_{t \\mid t-1} = \\frac{P_{t \\mid t-1}}{P_{t \\mid t-1} + R} P_{t \\mid t-1} = \\frac{P_{t \\mid t-1}^2}{P_{t \\mid t-1} + R}$$\n初始条件为 $P_{0 \\mid 0} = 4$（使用记法 $P_0=P_{0|0}$），$Q=1$ 和 $R=9$。\n\n**对于 $t=1$:**\n预测：$P_{1 \\mid 0} = P_{0 \\mid 0} + Q = 4 + 1 = 5$。\n方差减少量：$\\Delta_1 = \\frac{P_{1 \\mid 0}^2}{P_{1 \\mid 0} + R} = \\frac{5^2}{5 + 9} = \\frac{25}{14}$。\n用于下一步的更新后方差：$P_{1 \\mid 1} = P_{1 \\mid 0} - \\Delta_1 = 5 - \\frac{25}{14} = \\frac{70 - 25}{14} = \\frac{45}{14}$。\n\n**对于 $t=2$:**\n预测：$P_{2 \\mid 1} = P_{1 \\mid 1} + Q = \\frac{45}{14} + 1 = \\frac{45+14}{14} = \\frac{59}{14}$。\n方差减少量：$\\Delta_2 = \\frac{P_{2 \\mid 1}^2}{P_{2 \\mid 1} + R} = \\frac{\\left(\\frac{59}{14}\\right)^2}{\\frac{59}{14} + 9} = \\frac{\\frac{3481}{196}}{\\frac{59 + 126}{14}} = \\frac{\\frac{3481}{196}}{\\frac{185}{14}} = \\frac{3481}{196} \\cdot \\frac{14}{185} = \\frac{3481}{14 \\cdot 185} = \\frac{3481}{2590}$。\n用于下一步的更新后方差：$P_{2 \\mid 2} = P_{2 \\mid 1} - \\Delta_2 = \\frac{59}{14} - \\frac{3481}{2590} = \\frac{59 \\times 185}{2590} - \\frac{3481}{2590} = \\frac{10915 - 3481}{2590} = \\frac{7434}{2590} = \\frac{531}{185}$。\n\n**对于 $t=3$:**\n预测：$P_{3 \\mid 2} = P_{2 \\mid 2} + Q = \\frac{531}{185} + 1 = \\frac{531+185}{185} = \\frac{716}{185}$。\n方差减少量：$\\Delta_3 = \\frac{P_{3 \\mid 2}^2}{P_{3 \\mid 2} + R} = \\frac{\\left(\\frac{716}{185}\\right)^2}{\\frac{716}{185} + 9} = \\frac{\\frac{716^2}{185^2}}{\\frac{716 + 9 \\cdot 185}{185}} = \\frac{\\frac{512656}{185^2}}{\\frac{716 + 1665}{185}} = \\frac{\\frac{512656}{185^2}}{\\frac{2381}{185}} = \\frac{512656}{185 \\cdot 2381} = \\frac{512656}{440485}$。\n\n三个方差减少量为 $\\Delta_1 = \\frac{25}{14}$，$\\Delta_2 = \\frac{3481}{2590}$ 和 $\\Delta_3 = \\frac{512656}{440485}$。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{25}{14} \\quad \\frac{3481}{2590} \\quad \\frac{512656}{440485} \\end{pmatrix}}$$"
        }
    ]
}