{
    "hands_on_practices": [
        {
            "introduction": "均匀泊松过程是描述神经元尖峰序列的基础模型。第一个练习  将带您回到基本原理，要求您从简单的尖峰计数中推导出模型核心参数——发放率$\\lambda$的估计量。通过同时推导克拉默-拉奥下界，您不仅将练习最大似然估计，还将深入理解在此背景下统计推断的根本极限。",
            "id": "4181776",
            "problem": "在时长为 $T > 0$ 秒的固定观测窗口内记录单个神经元。根据神经脉冲序列的标准噪声模型，假设此窗口中的脉冲生成可以由一个具有恒定速率参数 $\\lambda > 0$ 的齐次泊松过程很好地描述，因此在该窗口中观测到的总脉冲数 $K$ 是来自均值为 $\\lambda T$ 的泊松分布的单次抽样。从泊松模型的似然函数基本定义和费雪信息的定义出发，推导基于观测计数值 $K$ 的 $\\lambda$ 的最大似然估计（MLE），并使用此观测模型推导 $\\lambda$ 的任何无偏估计量方差的克拉默-拉奥下界（CRB）。将您的最终答案表示为闭合形式的符号表达式，不带单位。将这两个表达式放在一个单行矩阵中，其中第一个条目是 MLE，第二个条目是此模型下 $\\operatorname{Var}(\\hat{\\lambda})$ 的 CRB。",
            "solution": "该问题要求推导与神经元放电率 $\\lambda$ 估计相关的两个量。数据的模型是，在时长为 $T > 0$ 的固定时间窗口中观测到的总脉冲数 $K$ 是来自均值为 $\\lambda T$ 的泊松分布的单次抽样，其中 $\\lambda > 0$ 是恒定速率参数。\n\n首先，我们将推导 $\\lambda$ 的最大似然估计（MLE）。MLE 的基础是似然函数，它是观测到数据 $K$ 的概率，作为参数 $\\lambda$ 的函数。对于均值为 $\\mu$ 的泊松分布，其概率质量函数为 $P(\\text{count}=k) = \\frac{\\mu^k \\exp(-\\mu)}{k!}$。在我们的模型中，均值为 $\\mu = \\lambda T$。因此，观测到特定计数值 $K$ 的似然函数 $\\mathcal{L}(\\lambda | K)$ 是：\n$$\\mathcal{L}(\\lambda | K) = \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!}$$\n为了找到 MLE，我们寻求使此函数最大化的 $\\lambda$ 值。等价且在数学上更方便的做法是最大化似然函数的自然对数，即对数似然函数 $\\ell(\\lambda | K) = \\ln(\\mathcal{L}(\\lambda | K))$。\n$$\\ell(\\lambda | K) = \\ln\\left( \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!} \\right)$$\n利用对数的性质，我们展开此表达式：\n$$\\ell(\\lambda | K) = \\ln((\\lambda T)^K) - \\ln(\\exp(\\lambda T)) - \\ln(K!)$$\n$$\\ell(\\lambda | K) = K \\ln(\\lambda T) - \\lambda T - \\ln(K!)$$\n这可以进一步展开为：\n$$\\ell(\\lambda | K) = K \\ln(\\lambda) + K \\ln(T) - \\lambda T - \\ln(K!)$$\n为了找到最大值，我们对 $\\ell(\\lambda | K)$ 关于 $\\lambda$ 求一阶导数，并令其结果为零。注意，在此微分中，$K$ 和 $T$ 被视为常数。\n$$\\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda} (K \\ln(\\lambda) + K \\ln(T) - \\lambda T - \\ln(K!)) = K \\cdot \\frac{1}{\\lambda} + 0 - T - 0 = \\frac{K}{\\lambda} - T$$\n将此导数设为零，得到 $\\lambda$ 的 MLE（表示为 $\\hat{\\lambda}_{MLE}$）的方程：\n$$\\frac{K}{\\hat{\\lambda}_{MLE}} - T = 0$$\n解出 $\\hat{\\lambda}_{MLE}$：\n$$\\hat{\\lambda}_{MLE} = \\frac{K}{T}$$\n为了验证该值对应于一个最大值，我们检查对数似然的二阶导数：\n$$\\frac{d^2\\ell}{d\\lambda^2} = \\frac{d}{d\\lambda} \\left(\\frac{K}{\\lambda} - T\\right) = -\\frac{K}{\\lambda^2}$$\n由于脉冲计数 $K$ 是一个非负整数（$K \\geq 0$）且速率 $\\lambda$ 是正数（$\\lambda > 0$），二阶导数 $\\frac{d^2\\ell}{d\\lambda^2} = -\\frac{K}{\\lambda^2}$ 总是非正的。如果 $K > 0$，二阶导数严格为负，确认 $\\hat{\\lambda}_{MLE} = \\frac{K}{T}$ 是一个局部最大值。如果 $K = 0$，似然函数 $\\mathcal{L}(\\lambda|0) = \\exp(-\\lambda T)$ 是 $\\lambda$ 的严格递减函数，其最大值在 $\\lambda \\to 0$ 时逼近。我们的公式 $\\hat{\\lambda}_{MLE} = 0/T = 0$ 与这种情况一致。因此，第一个表达式是 $\\frac{K}{T}$。\n\n其次，我们将推导 $\\lambda$ 的任何无偏估计量方差的克拉默-拉奥下界（CRB）。CRB 是费雪信息 $I(\\lambda)$ 的倒数。参数 $\\lambda$ 的费雪信息定义为：\n$$I(\\lambda) = -E\\left[\\frac{d^2}{d\\lambda^2}\\ell(\\lambda | K)\\right]$$\n期望 $E[\\cdot]$ 是关于数据 $K$ 的概率分布（即 $K \\sim \\text{Poisson}(\\lambda T)$）计算的。我们已经计算了对数似然的二阶导数：\n$$\\frac{d^2\\ell}{d\\lambda^2} = -\\frac{K}{\\lambda^2}$$\n现在，我们计算其期望：\n$$E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = E\\left[-\\frac{K}{\\lambda^2}\\right]$$\n由于 $\\lambda$ 是我们正在估计的参数，它在关于 $K$ 的期望中被视为常数：\n$$E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = -\\frac{1}{\\lambda^2}E[K]$$\n泊松分布随机变量的期望值是其均值参数。此处，$E[K] = \\lambda T$。将此代入方程：\n$$E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = -\\frac{1}{\\lambda^2}(\\lambda T) = -\\frac{T}{\\lambda}$$\n利用此结果，我们求得费雪信息：\n$$I(\\lambda) = -E\\left[\\frac{d^2\\ell}{d\\lambda^2}\\right] = -\\left(-\\frac{T}{\\lambda}\\right) = \\frac{T}{\\lambda}$$\n克拉默-拉奥不等式指出，对于 $\\lambda$ 的任何无偏估计量 $\\hat{\\lambda}$，其方差的下界是费雪信息的倒数：\n$$\\operatorname{Var}(\\hat{\\lambda}) \\ge \\frac{1}{I(\\lambda)}$$\n克拉默-拉奥下界就是这个下限：\n$$\\text{CRB} = \\frac{1}{I(\\lambda)} = \\frac{1}{T/\\lambda} = \\frac{\\lambda}{T}$$\n这是第二个所需的表达式。\n\n解答要求将这两个表达式，即 MLE 和 CRB，放在一个单行矩阵中。\nMLE 是 $\\frac{K}{T}$。\n$\\operatorname{Var}(\\hat{\\lambda})$ 的 CRB 是 $\\frac{\\lambda}{T}$。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{K}{T} \\quad \\frac{\\lambda}{T} \\end{pmatrix}}$$"
        },
        {
            "introduction": "尽管泊松模型提供了一个简洁优雅的起点，但其均值等于方差的假设常常无法捕捉真实神经数据的复杂性。这个动手编程练习  旨在挑战您将泊松模型与更灵活的负二项分布模型进行比较，后者能够解释数据中的“过度离散”现象。通过在留出数据上实现模型拟合和使用规范的评分规则，您将培养在量化模型选择和验证方面的基本技能。",
            "id": "4181830",
            "problem": "给定在固定持续时间的观测窗口中收集的离散试验尖峰计数，这些计数被建模为整数值随机变量。两种用于此类计数的经典噪声模型是泊松分布和负二项分布，它们在神经科学数据分析中被频繁使用，以分别捕捉等离散和过离散现象。本问题要求您通过在指定的试验计数上计算留出对数得分和Brier得分，来比较这些模型的预测校准情况，其中需使用原则性估计程序和明确定义的预测评分规则。\n\n使用以下基础知识：\n- 率参数为 $\\lambda$ 的泊松分布定义在 $\\{0,1,2,\\dots\\}$ 上，其概率质量函数为 $p(y \\mid \\lambda) = \\exp(-\\lambda) \\lambda^{y} / y!$。在固定窗口内，独立发放假设下的尖峰计数通常被建模为泊松分布。\n- 在“直到k次成功前的失败次数”参数化中，参数为 $(k, p)$ 的负二项分布的概率质量函数为 $p(y \\mid k, p) = \\binom{y + k - 1}{y} (1 - p)^{y} p^{k}$，其中 $y \\in \\{0,1,2,\\dots\\}$，均值为 $\\mu = k (1 - p) / p$，方差为 $\\sigma^{2} = \\mu + \\mu^{2} / k$。当 $\\sigma^{2} > \\mu$ 时，此分布可以捕捉相对于泊松分布的过离散现象。\n- 在独立试验中，对泊松分布率参数的最大似然估计得到的估计量 $\\hat{\\lambda}$ 等于训练计数的样本均值。\n- 对于均值为 $\\mu$、尺寸参数为 $k$ 的负二项分布，其矩估计法使用训练样本均值 $m$ 和样本方差 $s^{2}$ 来求解 $m = \\mu$ 和 $s^{2} = \\mu + \\mu^{2} / k$。如果 $s^{2} > m$，则 $\\hat{k} = m^{2} / (s^{2} - m)$ 且 $\\hat{p} = \\hat{k} / (\\hat{k} + m)$。如果 $s^{2} \\le m$，则将负二项预测分布视为均值为 $m$ 的泊松分布（等价于 $\\hat{k} \\to \\infty$）。\n\n定义预测评分规则：\n- 对于一个测试计数 $y$，留出对数得分为 $\\log p(y \\mid \\text{predictive})$，其中 $p$ 是拟合模型下的预测概率质量。请使用自然对数。对于测试集 $\\{y_{i}\\}_{i=1}^{n}$，报告平均留出对数得分 $\\frac{1}{n} \\sum_{i=1}^{n} \\log p(y_{i} \\mid \\text{predictive})$。\n- 对于一个测试计数 $y$ 和一个在多个区间上的离散预测分布 $p_{k}$，Brier得分是预测概率与观测区间独热编码之间的平方 $\\ell_{2}$ 误差。对于具有无界支撑集的计数数据，定义一个截断多类别Brier得分，其区间为 $\\{0,1,\\dots,K\\}$ 和一个尾部区间 $K^{+}$。令 $p_{k} = p(Y = k)$（对于 $k \\in \\{0,\\dots,K\\}$），且 $p_{K^{+}} = 1 - \\sum_{k=0}^{K} p_{k}$。令 $y_{k} = 1$（如果 $y = k$）否则 $y_{k} = 0$（对于 $k \\in \\{0,\\dots,K\\}$），并令 $y_{K^{+}} = 1$（如果 $y > K$）否则 $y_{K^{+}} = 0$。单个观测的Brier得分为 $\\sum_{b \\in \\{0,\\dots,K,K^{+}\\}} (p_{b} - y_{b})^{2}$，而测试集上的平均Brier得分是所有测试观测得分的平均值。\n\n为每个测试用例实现以下步骤：\n- 通过 $\\hat{\\lambda} = m$ 拟合泊松分布率，其中 $m$ 是训练样本均值。\n- 通过矩估计法拟合负二项分布：计算训练样本均值 $m$ 和无偏样本方差 $s^{2}$，如果 $s^{2} > m$，则设 $\\hat{k} = m^{2} / (s^{2} - m)$ 和 $\\hat{p} = \\hat{k} / (\\hat{k} + m)$；否则，对负二项模型使用泊松预测分布。\n- 计算两种模型在测试集上的平均留出对数得分。\n- 使用特定于测试用例的截断参数 $K$，计算两种模型在测试集上的平均截断Brier得分，包括尾部区间 $K^{+}$。\n\n测试套件：\n- 用例1：训练计数 $\\{2,0,3,1,4,0,2,5,1,3\\}$，测试计数 $\\{0,2,1,4,6\\}$，截断参数 $K = 10$。\n- 用例2：训练计数 $\\{1,0,1,1,0,2,1,0,1,1\\}$，测试计数 $\\{0,1,1,2,0\\}$，截断参数 $K = 5$。\n- 用例3：训练计数 $\\{0,0,0,1,0,0,2,0,0,0\\}$，测试计数 $\\{0,0,1,0,3\\}$，截断参数 $K = 6$。\n- 用例4：训练计数 $\\{10,12,8,20,9,17\\}$，测试计数 $\\{7,20,13\\}$，截断参数 $K = 30$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果。\n- 按顺序为每个测试用例输出四个浮点数，保留 $6$ 位小数：泊松模型下的平均留出对数得分，负二项模型下的平均留出对数得分，泊松模型下的平均Brier得分，以及负二项模型下的平均Brier得分。\n- 最终的输出列表将所有测试用例的这四个浮点数连接起来，形成一个长度为 $16$ 的单一列表。例如，格式应为 $\\texttt{[r\\_1,r\\_2,\\dots,r\\_{16}]}$，其中每个 $r_{i}$ 是一个保留 $6$ 位小数的浮点数。",
            "solution": "我们从泊松分布和负二项分布的核心定义、它们在尖峰计数建模中的作用以及从第一性原理推导出的标准估计程序开始。\n\n对于泊松分布，一个整数计数 $y \\in \\{0,1,2,\\dots\\}$ 在率参数为 $\\lambda$ 时的概率质量函数是 $p(y \\mid \\lambda) = \\exp(-\\lambda) \\lambda^{y} / y!$。在独立尖峰生成于一个固定观测窗口的假设下，尖峰数量被建模为泊松分布，对于独立训练观测值 $\\{y_{i}\\}_{i=1}^{n}$，其联合似然函数为 $L(\\lambda) = \\prod_{i=1}^{n} \\exp(-\\lambda) \\lambda^{y_{i}} / y_{i}!$。对数似然函数是 $\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + y_{i} \\log \\lambda - \\log y_{i}! \\right ) = -n \\lambda + \\left( \\sum_{i=1}^{n} y_{i} \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log y_{i}!$。对 $\\lambda$ 求导得到 $\\frac{\\partial \\ell}{\\partial \\lambda} = -n + \\left( \\sum_{i=1}^{n} y_{i} \\right ) \\frac{1}{\\lambda}$，令其为零可得 $\\hat{\\lambda} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$，即样本均值。在泊松模型下，这个最大似然估计量是一致且无偏的。\n\n对于“直到k次成功前的失败次数”参数化的负二项分布，其参数为 $(k, p)$，概率质量函数为 $p(y \\mid k, p) = \\binom{y + k - 1}{y} (1 - p)^{y} p^{k}$，其中 $y \\in \\{0,1,2,\\dots\\}$。均值和方差满足 $\\mu = k (1 - p) / p$ 和 $\\sigma^{2} = \\mu + \\mu^{2} / k$。该分布常用于对过离散的尖峰计数（方差超过均值）进行建模，以捕捉异质性或泊松分布之外的变异性。矩估计法拟合使用训练样本均值 $m = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$ 和无偏样本方差 $s^{2} = \\frac{1}{n - 1} \\sum_{i=1}^{n} (y_{i} - m)^{2}$ 来匹配 $\\mu = m$ 和 $\\sigma^{2} = s^{2}$，通过 $s^{2} = m + m^{2} / k$，得到 $k = m^{2} / (s^{2} - m)$，前提是 $s^{2} > m$。求解 $p$ 得到 $p = k / (k + m)$。当 $s^{2} \\le m$ 时，负二项分布无法表示相对于泊松分布的欠离散情况，因此一个原则性的极限 $k \\to \\infty$ 将负二项分布简化为均值为 $m$ 的泊松分布。因此，在实践中，我们在这种情况下将负二项预测分布视为泊松分布。\n\n预测校准通过正常评分规则（proper scoring rules）进行评估。对于一个观测到的测试计数 $y$，在一个预测分布 $p(\\cdot)$ 下的留出对数得分为 $\\log p(y)$，使用自然对数。对于一个测试集 $\\{y_{i}\\}_{i=1}^{n}$，平均留出对数得分为 $\\frac{1}{n} \\sum_{i=1}^{n} \\log p(y_{i})$。此得分为严格正常（strictly proper）：其期望值在真实预测分布下达到最大。\n\n对于具有无界支撑集的计数数据，Brier得分（最初为二元结果定义）通过将结果划分为有限的区间，推广到多类别场景。使用截断参数 $K$，定义区间 $\\{0,1,\\dots,K\\}$ 和一个尾部区间 $K^{+}$。对于一个具有预测质量函数 $p(y)$ 的模型，令 $p_{k} = p(Y = k)$（对于 $k \\in \\{0,\\dots,K\\}$），且 $p_{K^{+}} = 1 - \\sum_{k=0}^{K} p_{k}$。对于一个实现的测试计数 $y$，定义在这些区间上的独热向量 $y_{b}$：$y_{k} = 1$（如果 $y = k$）否则为 $0$（对于 $k \\in \\{0,\\dots,K\\}$），且 $y_{K^{+}} = 1$（如果 $y > K$）否则为 $0$。Brier得分是预测概率向量和独热标签之间的欧几里得距离平方，即 $\\sum_{b \\in \\{0,\\dots,K,K^{+}\\}} (p_{b} - y_{b})^{2}$。测试集上的平均Brier得分是该量在所有测试中的平均值。当 $K$ 的选择使得尾部质量很小或通过 $K^{+}$ 明确考虑时，这个截断定义在科学上是有意义的。\n\n每个测试用例的算法步骤：\n- 计算训练样本均值 $m$ 和无偏样本方差 $s^{2}$。\n- 拟合泊松参数 $\\hat{\\lambda} = m$。\n- 通过矩估计法拟合负二项分布参数：如果 $s^{2} > m$，设 $\\hat{k} = m^{2} / (s^{2} - m)$ 和 $\\hat{p} = \\hat{k} / (\\hat{k} + m)$；否则，设置一个标志，以便在这种情况下为负二项模型得分使用泊松预测分布。\n- 对于每个测试计数 $y$，使用拟合的泊松和拟合的负二项预测分布计算对数得分 $\\log p(y)$，并在整个测试集上取平均值。\n- 对于每个测试计数 $y$，使用每个用例指定的截断参数 $K$，计算截断多类别Brier得分，区间为 $\\{0,\\dots,K,K^{+}\\}$。对于每个模型，从其概率质量函数计算 $p_{k}$（对于 $k \\in \\{0,\\dots,K\\}$），并使用在 $K$ 处的累积分布函数计算 $p_{K^{+}} = 1 - \\sum_{k=0}^{K} p_{k}$。构建独热向量 $y_{b}$ 并计算平方误差和。在整个测试集上取平均值。\n- 将所有报告的得分四舍五入至 $6$ 位小数。\n- 按顺序连接每个用例的四个得分：平均留出对数得分（泊松）、平均留出对数得分（负二项）、平均Brier得分（泊松）、平均Brier得分（负二项）。\n- 汇总所有用例的结果，并打印一个由方括号括起来的逗号分隔列表的单行输出。\n\n边界情况与稳健性：\n- $s^{2} \\le m$ 的情况会产生意味着 $k \\to \\infty$ 的负二项分布参数；预测分布收敛于均值为 $m$ 的泊松分布。在这种场景下，实现中使用泊松分布计算负二项模型的得分。\n- 通过使用经过良好测试的概率质量函数和累积分布函数实现来保持数值稳定性。\n- 截断参数 $K$ 定义了一个尾部区间 $K^{+}$，它汇集了超过 $K$ 的所有概率质量，从而确保在一个有限的区间集合上计算出一个正常的（proper）多类别Brier得分。\n\n将此程序应用于所提供的测试套件，可以对用于神经试验计数的泊松模型和负二项模型的预测校准进行可复现的、程序化的比较。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson, nbinom\n\ndef fit_poisson(train_counts):\n    # MLE for Poisson rate: sample mean\n    lam = float(np.mean(train_counts)) if len(train_counts) > 0 else 0.0\n    return lam\n\ndef fit_nb_moments(train_counts, eps=1e-12):\n    # Method-of-moments for NB: match mean and variance\n    n = len(train_counts)\n    if n == 0:\n        # Degenerate; default to Poisson with lambda 0\n        return {\"use_poisson\": True, \"mu\": 0.0, \"k\": np.inf, \"p\": 1.0}\n    m = float(np.mean(train_counts))\n    # Unbiased sample variance; if n==1, set variance to 0\n    s2 = float(np.var(train_counts, ddof=1)) if n > 1 else 0.0\n    if s2 > m + eps:\n        k = m**2 / (s2 - m)\n        p = k / (k + m)\n        return {\"use_poisson\": False, \"mu\": m, \"k\": k, \"p\": p}\n    else:\n        # Underdispersion: NB reduces to Poisson\n        return {\"use_poisson\": True, \"mu\": m, \"k\": np.inf, \"p\": 1.0}\n\ndef mean_log_score(test_counts, model_dist):\n    # model_dist is a scipy.stats distribution object providing logpmf\n    log_scores = model_dist.logpmf(test_counts)\n    return float(np.mean(log_scores))\n\ndef mean_brier_score(test_counts, model_dist, K):\n    # Compute truncated multi-category Brier score with bins 0..K and tail K+\n    ks = np.arange(K + 1, dtype=int)\n    pmf_vals = model_dist.pmf(ks)  # shape (K+1,)\n    # Tail probability K+ = 1 - CDF(K)\n    tail_prob = 1.0 - model_dist.cdf(K)\n    # For each test count, compute Brier score\n    briers = []\n    for y in test_counts:\n        # One-hot vector over bins 0..K and tail\n        y_vec = np.zeros(K + 2, dtype=float)\n        if y = K:\n            y_vec[y] = 1.0\n        else:\n            y_vec[K + 1] = 1.0\n        p_vec = np.concatenate([pmf_vals, np.array([tail_prob])])\n        brier = np.sum((p_vec - y_vec) ** 2)\n        briers.append(brier)\n    return float(np.mean(briers))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (training_counts, test_counts, K)\n    test_cases = [\n        (np.array([2,0,3,1,4,0,2,5,1,3], dtype=int), np.array([0,2,1,4,6], dtype=int), 10),\n        (np.array([1,0,1,1,0,2,1,0,1,1], dtype=int), np.array([0,1,1,2,0], dtype=int), 5),\n        (np.array([0,0,0,1,0,0,2,0,0,0], dtype=int), np.array([0,0,1,0,3], dtype=int), 6),\n        (np.array([10,12,8,20,9,17], dtype=int), np.array([7,20,13], dtype=int), 30),\n    ]\n\n    results = []\n    for train_counts, test_counts, K in test_cases:\n        # Fit Poisson\n        lam = fit_poisson(train_counts)\n        pois_dist = poisson(mu=lam)\n\n        # Fit Negative Binomial\n        nb_params = fit_nb_moments(train_counts)\n        if nb_params[\"use_poisson\"]:\n            nb_dist = poisson(mu=nb_params[\"mu\"])\n        else:\n            # scipy.stats.nbinom parameterization: number of successes n=k, probability p\n            k = nb_params[\"k\"]\n            p = nb_params[\"p\"]\n            nb_dist = nbinom(n=k, p=p)\n\n        # Mean held-out log scores\n        ls_pois = mean_log_score(test_counts, pois_dist)\n        ls_nb = mean_log_score(test_counts, nb_dist)\n\n        # Mean Brier scores (truncated with tail bin)\n        brier_pois = mean_brier_score(test_counts, pois_dist, K)\n        brier_nb = mean_brier_score(test_counts, nb_dist, K)\n\n        # Round to 6 decimals and append in required order\n        results.extend([\n            round(ls_pois, 6),\n            round(ls_nb, 6),\n            round(brier_pois, 6),\n            round(brier_nb, 6),\n        ])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "神经活动本质上是动态的，其发放率会随刺激或内部状态而波动。这个高级计算实践  引入了一个状态空间框架来模拟时变潜在发放率，这是向更真实的神经模型迈出的重要一步。由于精确解难以获得，您将实现一个近似推断算法，从而直接体验解决现代神经科学研究中常见的非线性、非高斯滤波问题所需的强大技术。",
            "id": "4181816",
            "problem": "考虑一个用于脉冲计数的单神经元离散时间状态空间模型 (SSM)，其潜在对数率遵循自回归 (AR) 动态。设时间索引为 $t \\in \\{1,2,\\dots,T\\}$，观测到的脉冲计数为 $y_t \\in \\{0,1,2,\\dots\\}$，潜在对数率为 $x_t = \\log \\Lambda_t \\in \\mathbb{R}$。潜在动态是一阶自回归 (AR) 和高斯的：$x_t$ 服从 $x_t = \\mu + \\phi (x_{t-1} - \\mu) + \\sigma \\epsilon_t$，其中 $\\epsilon_t \\sim \\mathcal{N}(0,1)$ 在时间 $t$ 上独立同分布，参数为 $\\mu \\in \\mathbb{R}$，$\\phi \\in (-1,1)$ 和 $\\sigma  0$。观测模型是泊松模型：以 $x_t$ 为条件，计数 $y_t$ 作为速率参数为 $\\Lambda_t = \\exp(x_t)$ 的泊松随机变量分布，即 $y_t \\sim \\mathrm{Poisson}(\\Lambda_t)$。\n\n从泊松似然、AR 动态的高斯先验和 Bayes 法则的基本定义出发，推导前向滤波方程，以在给定观测值 $y_{1:t}$ 的情况下推断潜在变量 $x_t$（对于 $t = 1,\\dots,T$）。由于带有对数链接的泊松观测与高斯先验不是共轭的，因此请使用一种基于原则的近似方法，该方法基于对数后验在其最大后验 (MAP) 估计处对 $x_t$ 进行二阶展开，从而在每个时间点 $t$ 获得滤波分布的高斯近似。从这个近似中，提取给定 $y_{1:t}$ 时 $x_t$ 的近似后验均值和方差，然后使用由 $x_t$ 的高斯近似所蕴含的相应矩变换，计算率的滤波后验期望 $E[\\Lambda_t \\mid y_{1:t}]$。\n\n实现一个程序，对于以下每个测试用例，对整个观测序列执行前向滤波，并输出最终的率的滤波后验期望 $E[\\Lambda_T \\mid y_{1:T}]$。所有计算都是无单位的。您的算法必须是通用的，并且仅依赖于定义和所描述的二阶近似；不得使用来自外部来源的封闭形式的快捷公式。\n\n使用以下测试套件，它涵盖了一般的正常情况、接近单位的 AR 系数、全零计数和负 AR 系数：\n\n-   案例 1：$T = 7$，$\\phi = 0.9$，$\\sigma = 0.3$，$\\mu = \\log 3$，初始先验 $x_0 \\sim \\mathcal{N}(m_0, P_0)$，其中 $m_0 = \\log 3$，$P_0 = 0.25$，观测值 $y_{1:7} = [0,1,2,0,3,1,0]$。\n-   案例 2：$T = 5$，$\\phi = 0.99$，$\\sigma = 0.1$，$\\mu = \\log 10$，初始先验 $m_0 = \\log 10$，$P_0 = 0.04$，观测值 $y_{1:5} = [12,8,15,9,11]$。\n-   案例 3：$T = 6$，$\\phi = 0.2$，$\\sigma = 1.0$，$\\mu = \\log 0.5$，初始先验 $m_0 = \\log 0.5$，$P_0 = 1.0$，观测值 $y_{1:6} = [0,0,0,0,0,0]$。\n-   案例 4：$T = 5$，$\\phi = -0.5$，$\\sigma = 0.5$，$\\mu = \\log 2$，初始先验 $m_0 = \\log 2$，$P_0 = 0.5$，观测值 $y_{1:5} = [2,0,4,1,3]$。\n\n对于每个案例，运行前向滤波器遍历所有时间步 $t = 1,\\dots,T$，并返回单个浮点数 $E[\\Lambda_T \\mid y_{1:T}]$。您的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来，按上述案例的顺序排列，四舍五入到六位小数（例如，$[1.234567,2.345678,3.456789,4.567890]$）。",
            "solution": "该问题要求推导并实现一个用于单神经元状态空间模型的前向滤波算法。该模型描述了潜在对数发放率 $x_t$ 的演变以及观测到的脉冲计数 $y_t$ 的生成。该模型定义在离散时间步 $t \\in \\{1, 2, \\dots, T\\}$上。\n\n该模型包含两个主要部分：\n1.  **状态方程**：潜在对数率 $x_t = \\log \\Lambda_t \\in \\mathbb{R}$ 服从带高斯噪声的一阶自回归 AR(1) 过程。其动态由 $x_t = \\mu + \\phi (x_{t-1} - \\mu) + \\sigma \\epsilon_t$ 给出，其中 $\\epsilon_t \\sim \\mathcal{N}(0,1)$。这意味着状态转移概率分布是高斯的：$p(x_t \\mid x_{t-1}) \\sim \\mathcal{N}(x_t \\mid \\mu + \\phi(x_{t-1} - \\mu), \\sigma^2)$。参数包括平均对数率 $\\mu \\in \\mathbb{R}$、持续性参数 $\\phi \\in (-1,1)$ 以及过程噪声标准差 $\\sigma  0$。\n2.  **观测方程**：在给定潜在状态 $x_t$ 的条件下，观测到的脉冲计数 $y_t \\in \\{0, 1, 2, \\dots\\}$ 是条件独立的，并服从速率为 $\\Lambda_t = \\exp(x_t)$ 的泊松分布。因此，观测似然为 $p(y_t \\mid x_t) = \\text{Poisson}(y_t \\mid \\Lambda_t) = \\frac{\\Lambda_t^{y_t} e^{-\\Lambda_t}}{y_t!}$。\n\n目标是执行递归贝叶斯滤波，以在给定初始状态 $p(x_0) \\sim \\mathcal{N}(x_0 \\mid m_0, P_0)$ 的高斯先验下，估计潜在状态的后验分布 $p(x_t \\mid y_{1:t})$，其中 $t=1, \\dots, T$。滤波递归在每个时间点包括两个步骤：预测和更新。\n\n我们假设在时间步 $t-1$ 结束时，滤波分布 $p(x_{t-1} \\mid y_{1:t-1})$ 可以很好地由一个高斯分布近似：\n$$p(x_{t-1} \\mid y_{1:t-1}) \\approx \\mathcal{N}(x_{t-1} \\mid m_{t-1|t-1}, P_{t-1|t-1})$$\n在开始时，对于 $t=1$，我们使用给定的 $x_0$ 的先验，所以我们用 $m_{0|0} = m_0$ 和 $P_{0|0} = P_0$ 进行初始化。\n\n**1. 预测步骤（时间更新）**\n\n预测步骤仅使用状态动态将状态分布从时间 $t-1$ 演化到时间 $t$。我们的目标是通过对前一状态 $x_{t-1}$ 进行边缘化来计算预测分布 $p(x_t \\mid y_{1:t-1})$：\n$$p(x_t \\mid y_{1:t-1}) = \\int p(x_t \\mid x_{t-1}) p(x_{t-1} \\mid y_{1:t-1}) \\, dx_{t-1}$$\n该积分表示两个高斯分布的卷积：高斯状态转移 $p(x_t \\mid x_{t-1})$ 和上一步的高斯后验 $p(x_{t-1} \\mid y_{1:t-1})$。由于状态转移在 $x_{t-1}$ 上是线性的，得到的预测分布也是高斯的，记作 $p(x_t \\mid y_{1:t-1}) \\approx \\mathcal{N}(x_t \\mid m_{t|t-1}, P_{t|t-1})$。\n\n该预测分布的参数计算如下：\n-   **预测均值 $m_{t|t-1}$**：使用全期望定律：\n    $$m_{t|t-1} = E[x_t \\mid y_{1:t-1}] = E[E[x_t \\mid x_{t-1}, y_{1:t-1}] \\mid y_{1:t-1}]$$\n    根据状态方程，$E[x_t \\mid x_{t-1}] = \\mu + \\phi(x_{t-1} - \\mu)$。因此，\n    $$m_{t|t-1} = E[\\mu + \\phi(x_{t-1} - \\mu) \\mid y_{1:t-1}] = \\mu + \\phi(E[x_{t-1} \\mid y_{1:t-1}] - \\mu) = \\mu + \\phi(m_{t-1|t-1} - \\mu)$$\n-   **预测方差 $P_{t|t-1}$**：使用全方差定律：\n    $$P_{t|t-1} = \\mathrm{Var}(x_t \\mid y_{1:t-1}) = E[\\mathrm{Var}(x_t \\mid x_{t-1})] + \\mathrm{Var}(E[x_t \\mid x_{t-1}] \\mid y_{1:t-1})$$\n    条件方差为 $\\mathrm{Var}(x_t \\mid x_{t-1}) = \\sigma^2$。条件期望的方差为 $\\mathrm{Var}(\\mu + \\phi(x_{t-1} - \\mu) \\mid y_{1:t-1}) = \\phi^2 \\mathrm{Var}(x_{t-1} \\mid y_{1:t-1}) = \\phi^2 P_{t-1|t-1}$。因此，\n    $$P_{t|t-1} = \\sigma^2 + \\phi^2 P_{t-1|t-1}$$\n预测分布由 $m_{t|t-1}$ 和 $P_{t|t-1}$ 完全确定。\n\n**2. 更新步骤（测量更新）**\n\n更新步骤引入新的观测值 $y_t$，将预测分布提炼为滤波分布 $p(x_t \\mid y_{1:t})$。根据 Bayes 法则：\n$$p(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t) p(x_t \\mid y_{1:t-1})$$\n泊松似然 $p(y_t \\mid x_t)$ 与高斯预测分布（在此步骤中作为我们的先验）不是共轭的。为了维持高斯后验近似，问题指定使用对数后验在其最大后验 (MAP) 估计处的二阶展开。这是一种拉普拉斯近似。\n\n$x_t$ 的对数后验，忽略与 $x_t$ 无关的常数项，为：\n$$J(x_t) = \\log p(x_t \\mid y_{1:t}) \\propto \\log p(y_t \\mid x_t) + \\log p(x_t \\mid y_{1:t-1})$$\n代入泊松对数似然和高斯对数先验密度：\n$$J(x_t) = \\left(y_t \\log(\\Lambda_t) - \\Lambda_t \\right) - \\frac{1}{2 P_{t|t-1}}(x_t - m_{t|t-1})^2 = y_t x_t - \\exp(x_t) - \\frac{1}{2 P_{t|t-1}}(x_t - m_{t|t-1})^2$$\n为了找到 MAP 估计 $\\hat{x}_t^{\\text{MAP}}$，我们将 $J(x_t)$ 的一阶导数设为零：\n$$J'(x_t) = \\frac{dJ}{dx_t} = y_t - \\exp(x_t) - \\frac{x_t - m_{t|t-1}}{P_{t|t-1}} = 0$$\n这是一个关于 $\\hat{x}_t^{\\text{MAP}}$ 的超越方程，必须进行数值求解。我们使用牛顿-拉夫逊方法。令 $g(x_t) = J'(x_t)$。迭代更新为 $x_t^{(k+1)} = x_t^{(k)} - g(x_t^{(k)})/g'(x_t^{(k)})$，其中 $g'(x_t)$ 是对数后验的二阶导数：\n$$J''(x_t) = \\frac{d^2J}{dx_t^2} = -\\exp(x_t) - \\frac{1}{P_{t|t-1}}$$\n由于对所有 $x_t$，$J''(x_t)  0$，函数 $J(x_t)$ 是严格凹的，这保证了存在唯一的最大值，并确保牛顿-拉夫逊算法的稳健收敛。\n\n在计算出 $\\hat{x}_t^{\\text{MAP}}$ 后，我们用以该众数为中心的二阶泰勒级数来近似 $J(x_t)$：\n$$J(x_t) \\approx J(\\hat{x}_t^{\\text{MAP}}) + J'(\\hat{x}_t^{\\text{MAP}})(x_t - \\hat{x}_t^{\\text{MAP}}) + \\frac{1}{2} J''(\\hat{x}_t^{\\text{MAP}})(x_t - \\hat{x}_t^{\\text{MAP}})^2$$\n根据众数的定义，$J'(\\hat{x}_t^{\\text{MAP}}) = 0$，因此我们有：\n$$J(x_t) \\approx \\text{const.} + \\frac{1}{2} J''(\\hat{x}_t^{\\text{MAP}})(x_t - \\hat{x}_t^{\\text{MAP}})^2$$\n对该近似进行指数化，表明 $p(x_t \\mid y_{1:t})$ 近似为高斯分布，$p(x_t \\mid y_{1:t}) \\approx \\mathcal{N}(x_t \\mid m_{t|t}, P_{t|t})$。通过将指数部分与高斯对数密度的标准形式进行比较，我们可以确定后验均值和方差：\n-   **后验均值 $m_{t|t}$**：$m_{t|t} = \\hat{x}_t^{\\text{MAP}}$\n-   **后验方差 $P_{t|t}$**：$P_{t|t} = (-J''(\\hat{x}_t^{\\text{MAP}}))^{-1} = \\left(\\exp(\\hat{x}_t^{\\text{MAP}}) + \\frac{1}{P_{t|t-1}}\\right)^{-1}$\n这些参数 $m_{t|t}$ 和 $P_{t|t}$ 定义了时间 $t$ 的滤波分布，并作为时间 $t+1$ 预测步骤的输入。\n\n**3. 最终计算：后验期望率**\n\n最终要求的输出是最后一个时间步的率的滤波后验期望 $E[\\Lambda_T \\mid y_{1:T}]$。给定我们的高斯近似 $p(x_T \\mid y_{1:T}) \\approx \\mathcal{N}(x_T \\mid m_{T|T}, P_{T|T})$ 和关系 $\\Lambda_T = \\exp(x_T)$，我们计算一个对数正态变量的期望。\n如果一个随机变量 $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$，其矩生成函数为 $M_X(s) = E[\\exp(sX)] = \\exp(\\mu s + \\frac{1}{2}\\sigma^2 s^2)$。所需的期望对应于在 $s=1$ 处评估矩生成函数：\n$$E[\\Lambda_T \\mid y_{1:T}] = E[\\exp(x_T) \\mid y_{1:T}] \\approx \\exp\\left(m_{T|T} \\cdot 1 + \\frac{1}{2}P_{T|T} \\cdot 1^2\\right) = \\exp\\left(m_{T|T} + \\frac{1}{2}P_{T|T}\\right)$$\n程序将为每个测试用例执行这个完整的递归过程，以计算最终值。",
            "answer": "```python\nimport numpy as np\n\ndef run_filter(T, phi, sigma, mu, m0, P0, y):\n    \"\"\"\n    Performs forward filtering for the Poisson-AR(1) state-space model.\n\n    This function implements a Gaussian-approximated filter (Laplace approximation)\n    to recursively estimate the posterior distribution of the latent log-rate.\n\n    Args:\n        T (int): Number of time steps.\n        phi (float): AR(1) coefficient.\n        sigma (float): Standard deviation of process noise.\n        mu (float): Mean of the AR(1) process.\n        m0 (float): Mean of the initial prior distribution for x_0.\n        P0 (float): Variance of the initial prior distribution for x_0.\n        y (list or np.ndarray): Sequence of observed spike counts.\n\n    Returns:\n        float: The filtered posterior expectation of the rate at the final time T,\n               E[Lambda_T | y_{1:T}].\n    \"\"\"\n    # Initialize filtered mean and variance with prior at t=0\n    m_filt = m0\n    P_filt = P0\n\n    for t_idx in range(T):\n        y_t = y[t_idx]\n\n        # 1. Prediction (Time Update)\n        # Predict the state at time t given observations up to t-1.\n        m_pred = mu + phi * (m_filt - mu)\n        P_pred = phi**2 * P_filt + sigma**2\n\n        # 2. Update (Measurement Update)\n        # Find the MAP estimate for x_t using Newton-Raphson method.\n        # The objective is to find the root of the log-posterior's first derivative.\n        \n        # Initial guess for the mode is the predicted mean.\n        x_map = m_pred\n        \n        # We iterate a fixed number of times; convergence is very fast.\n        for _ in range(30):\n            exp_x = np.exp(x_map)\n            \n            # First derivative of log-posterior J'(x)\n            g = y_t - exp_x - (x_map - m_pred) / P_pred\n            \n            # Second derivative of log-posterior J''(x)\n            g_prime = -exp_x - 1.0 / P_pred\n            \n            # Newton-Raphson update step\n            x_map = x_map - g / g_prime\n\n        # The updated (filtered) mean is the MAP estimate.\n        m_filt = x_map\n        \n        # The updated (filtered) variance is the inverse of the negative second derivative\n        # evaluated at the MAP estimate.\n        P_filt = 1.0 / (np.exp(x_map) + 1.0 / P_pred)\n\n    # After the loop, m_filt is m_{T|T} and P_filt is P_{T|T}.\n\n    # 3. Compute Final Expected Rate\n    # E[Lambda_T | y_{1:T}] = exp(m_{T|T} + P_{T|T}/2) due to log-normal property.\n    expected_rate_T = np.exp(m_filt + 0.5 * P_filt)\n    \n    return expected_rate_T\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the filter for each, and prints the results.\n    \"\"\"\n    test_cases = [\n        # Case 1: General happy path\n        {'T': 7, 'phi': 0.9, 'sigma': 0.3, 'mu': np.log(3), 'm0': np.log(3), 'P0': 0.25, 'y': [0, 1, 2, 0, 3, 1, 0]},\n        # Case 2: Near-unit AR coefficient\n        {'T': 5, 'phi': 0.99, 'sigma': 0.1, 'mu': np.log(10), 'm0': np.log(10), 'P0': 0.04, 'y': [12, 8, 15, 9, 11]},\n        # Case 3: All-zero counts\n        {'T': 6, 'phi': 0.2, 'sigma': 1.0, 'mu': np.log(0.5), 'm0': np.log(0.5), 'P0': 1.0, 'y': [0, 0, 0, 0, 0, 0]},\n        # Case 4: Negative AR coefficient\n        {'T': 5, 'phi': -0.5, 'sigma': 0.5, 'mu': np.log(2), 'm0': np.log(2), 'P0': 0.5, 'y': [2, 0, 4, 1, 3]},\n    ]\n    \n    results = []\n    for case in test_cases:\n        result = run_filter(case['T'], case['phi'], case['sigma'], case['mu'], case['m0'], case['P0'], case['y'])\n        results.append(result)\n\n    # Format the output as a comma-separated list of floats rounded to 6 decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n\n```"
        }
    ]
}