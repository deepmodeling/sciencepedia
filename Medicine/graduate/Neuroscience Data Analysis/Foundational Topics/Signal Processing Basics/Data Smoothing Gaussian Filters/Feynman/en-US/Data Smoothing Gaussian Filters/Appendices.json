{
    "hands_on_practices": [
        {
            "introduction": "A core trade-off in signal processing is balancing noise reduction with the preservation of signal features. This exercise  provides a quantitative way to understand this trade-off by analyzing how a Gaussian filter attenuates the amplitude of sharp, idealized signals like steps and impulses. Mastering this calculation builds crucial intuition about the relationship between filter width $\\sigma$ and the inevitable blurring of important transient events in your data.",
            "id": "4153092",
            "problem": "In neuroscience data analysis, Gaussian smoothing is often used to reduce noise in time series such as synaptic currents or calcium fluorescence signals. Consider a continuous-time signal $x(t)$ smoothed by convolution with a normalized Gaussian kernel $G_{\\sigma}(t)$ of standard deviation $\\sigma$, defined by\n$$\nG_{\\sigma}(t) = \\frac{1}{\\sqrt{2 \\pi} \\, \\sigma} \\exp\\!\\left(-\\frac{t^{2}}{2 \\sigma^{2}}\\right), \\quad \\int_{-\\infty}^{\\infty} G_{\\sigma}(t) \\, dt = 1,\n$$\nand let the smoothed output be $y(t) = (x * G_{\\sigma})(t) = \\int_{-\\infty}^{\\infty} x(\\tau) \\, G_{\\sigma}(t - \\tau) \\, d\\tau$.\n\nTwo canonical sharp transients are considered:\n- A step input $x_{\\text{step}}(t) = A \\, H(t)$, where $H(t)$ is the Heaviside step function and $A > 0$ is the step height. Define the pointwise amplitude estimate at the transition by $y_{\\text{step}}(0)$, and the attenuation factor $b_{\\text{step}}(\\sigma) = y_{\\text{step}}(0)/A$.\n- An impulse-like input modeled as a narrow rectangular pulse of width $\\varepsilon > 0$ and amplitude $A > 0$ centered at $t = 0$,\n$$\nx_{\\text{imp},\\varepsilon}(t) = A \\, \\mathbf{1}_{\\{|t| \\le \\varepsilon/2\\}}(t),\n$$\nwhere $\\mathbf{1}_{\\{|t| \\le \\varepsilon/2\\}}$ is the indicator function of the interval $[-\\varepsilon/2, \\varepsilon/2]$. For the smoothed response $y_{\\text{imp},\\varepsilon}(t) = (x_{\\text{imp},\\varepsilon} * G_{\\sigma})(t)$, define the amplitude estimate as the peak value $y_{\\text{imp},\\varepsilon}(0)$ and the attenuation factor $b_{\\text{imp}}(\\sigma, \\varepsilon) = y_{\\text{imp},\\varepsilon}(0)/A$.\n\nStarting only from the definitions above and the properties of convolution with even, normalized kernels, derive expressions for:\n- $b_{\\text{step}}(\\sigma)$ as a closed-form constant.\n- The leading-order approximation of $b_{\\text{imp}}(\\sigma, \\varepsilon)$ in the small-width regime $\\varepsilon/\\sigma \\ll 1$.\n\nProvide your final answer as a single row vector $\\big[b_{\\text{step}}(\\sigma), \\, b_{\\text{imp}}(\\sigma, \\varepsilon)\\big]$ consisting of closed-form analytic expressions. No numerical evaluation is required, and no units need be reported in the final answer. If any approximation is needed, report the leading-order term in $\\varepsilon/\\sigma$ only.",
            "solution": "The user wants to solve a problem regarding Gaussian smoothing of signals, a common technique in neuroscience data analysis.\n\n### Step 1: Problem Validation\n\n**Extraction of Givens:**\n*   **Gaussian Kernel:** $G_{\\sigma}(t) = \\frac{1}{\\sqrt{2 \\pi} \\, \\sigma} \\exp\\left(-\\frac{t^{2}}{2 \\sigma^{2}}\\right)$\n*   **Normalization:** $\\int_{-\\infty}^{\\infty} G_{\\sigma}(t) \\, dt = 1$\n*   **Convolution:** $y(t) = (x * G_{\\sigma})(t) = \\int_{-\\infty}^{\\infty} x(\\tau) \\, G_{\\sigma}(t - \\tau) \\, d\\tau$\n*   **Step Input:** $x_{\\text{step}}(t) = A \\, H(t)$, where $H(t)$ is the Heaviside step function and $A > 0$.\n*   **Step Attenuation Factor:** $b_{\\text{step}}(\\sigma) = y_{\\text{step}}(0)/A$, where $y_{\\text{step}}(0)$ is the smoothed output at $t=0$.\n*   **Impulse-like Input (Rectangular Pulse):** $x_{\\text{imp},\\varepsilon}(t) = A \\, \\mathbf{1}_{\\{|t| \\le \\varepsilon/2\\}}(t)$, where $\\varepsilon > 0$ and $A > 0$. $\\mathbf{1}_{\\{\\cdot\\}}$ is the indicator function.\n*   **Impulse Attenuation Factor:** $b_{\\text{imp}}(\\sigma, \\varepsilon) = y_{\\text{imp},\\varepsilon}(0)/A$, where $y_{\\text{imp},\\varepsilon}(0)$ is the smoothed output at $t=0$.\n*   **Task 1:** Derive a closed-form expression for $b_{\\text{step}}(\\sigma)$.\n*   **Task 2:** Derive the leading-order approximation of $b_{\\text{imp}}(\\sigma, \\varepsilon)$ for the condition $\\varepsilon/\\sigma \\ll 1$.\n\n**Validation Analysis:**\n1.  **Scientific Grounding:** The problem is firmly rooted in signal processing and Fourier analysis, employing standard definitions for convolution, Gaussian functions, and idealized signals like the Heaviside step and rectangular pulse. These are fundamental tools in analyzing time-series data in many scientific fields, including neuroscience. The problem is scientifically sound.\n2.  **Well-Posedness:** The objectives are clearly stated, and all necessary mathematical definitions and constraints are provided. The problem is self-contained and structured to lead to unique, meaningful solutions for the requested quantities.\n3.  **Objectivity:** The problem is phrased using precise mathematical language, free from any subjective or ambiguous terminology.\n\n**Verdict:**\nThe problem is valid. It is a standard, well-posed problem in signal analysis that requires the application of integral calculus and approximation methods based on the provided definitions.\n\n### Step 2: Derivation of the Solution\n\n#### Part 1: Attenuation of a Step Input, $b_{\\text{step}}(\\sigma)$\n\nThe smoothed output for the step input $x_{\\text{step}}(t) = A \\, H(t)$ is given by the convolution:\n$$\ny_{\\text{step}}(t) = \\int_{-\\infty}^{\\infty} x_{\\text{step}}(\\tau) \\, G_{\\sigma}(t - \\tau) \\, d\\tau\n$$\nSubstituting the expression for $x_{\\text{step}}(\\tau)$:\n$$\ny_{\\text{step}}(t) = \\int_{-\\infty}^{\\infty} A \\, H(\\tau) \\, G_{\\sigma}(t - \\tau) \\, d\\tau\n$$\nThe Heaviside function $H(\\tau)$ is $0$ for $\\tau < 0$ and $1$ for $\\tau \\ge 0$. This property changes the lower limit of integration from $-\\infty$ to $0$:\n$$\ny_{\\text{step}}(t) = A \\int_{0}^{\\infty} G_{\\sigma}(t - \\tau) \\, d\\tau\n$$\nWe must evaluate this expression at $t=0$ to find the pointwise amplitude estimate $y_{\\text{step}}(0)$:\n$$\ny_{\\text{step}}(0) = A \\int_{0}^{\\infty} G_{\\sigma}(-\\tau) \\, d\\tau\n$$\nThe Gaussian kernel $G_{\\sigma}(t)$ is an even function, as its definition depends on $t^2$. Thus, $G_{\\sigma}(-\\tau) = G_{\\sigma}(\\tau)$. Substituting this into the integral gives:\n$$\ny_{\\text{step}}(0) = A \\int_{0}^{\\infty} G_{\\sigma}(\\tau) \\, d\\tau\n$$\nWe are given that the kernel is normalized, i.e., $\\int_{-\\infty}^{\\infty} G_{\\sigma}(\\tau) \\, d\\tau = 1$. Because $G_{\\sigma}(\\tau)$ is an even function, its integral over the positive real axis is exactly half of its total integral:\n$$\n\\int_{0}^{\\infty} G_{\\sigma}(\\tau) \\, d\\tau = \\frac{1}{2} \\int_{-\\infty}^{\\infty} G_{\\sigma}(\\tau) \\, d\\tau = \\frac{1}{2}\n$$\nSubstituting this result back into the expression for $y_{\\text{step}}(0)$:\n$$\ny_{\\text{step}}(0) = A \\cdot \\frac{1}{2} = \\frac{A}{2}\n$$\nThe attenuation factor $b_{\\text{step}}(\\sigma)$ is defined as $y_{\\text{step}}(0)/A$. Therefore:\n$$\nb_{\\text{step}}(\\sigma) = \\frac{A/2}{A} = \\frac{1}{2}\n$$\nThis result is a constant, independent of the standard deviation $\\sigma$ of the kernel.\n\n#### Part 2: Attenuation of an Impulse-like Input, $b_{\\text{imp}}(\\sigma, \\varepsilon)$\n\nThe smoothed output for the rectangular pulse input $x_{\\text{imp},\\varepsilon}(t)$ is:\n$$\ny_{\\text{imp},\\varepsilon}(t) = \\int_{-\\infty}^{\\infty} x_{\\text{imp},\\varepsilon}(\\tau) \\, G_{\\sigma}(t - \\tau) \\, d\\tau\n$$\nSubstituting the expression for $x_{\\text{imp},\\varepsilon}(\\tau) = A \\, \\mathbf{1}_{\\{|\\tau| \\le \\varepsilon/2\\}}(\\tau)$:\n$$\ny_{\\text{imp},\\varepsilon}(t) = \\int_{-\\infty}^{\\infty} A \\, \\mathbf{1}_{\\{|\\tau| \\le \\varepsilon/2\\}}(\\tau) \\, G_{\\sigma}(t - \\tau) \\, d\\tau\n$$\nThe indicator function restricts the integration interval to $[-\\varepsilon/2, \\varepsilon/2]$:\n$$\ny_{\\text{imp},\\varepsilon}(t) = A \\int_{-\\varepsilon/2}^{\\varepsilon/2} G_{\\sigma}(t - \\tau) \\, d\\tau\n$$\nThe amplitude estimate is the peak value, which, due to the symmetry of both the input pulse and the kernel, occurs at $t=0$:\n$$\ny_{\\text{imp},\\varepsilon}(0) = A \\int_{-\\varepsilon/2}^{\\varepsilon/2} G_{\\sigma}(-\\tau) \\, d\\tau\n$$\nAgain, using the even property $G_{\\sigma}(-\\tau) = G_{\\sigma}(\\tau)$:\n$$\ny_{\\text{imp},\\varepsilon}(0) = A \\int_{-\\varepsilon/2}^{\\varepsilon/2} G_{\\sigma}(\\tau) \\, d\\tau\n$$\nThe attenuation factor $b_{\\text{imp}}(\\sigma, \\varepsilon)$ is defined as $y_{\\text{imp},\\varepsilon}(0)/A$:\n$$\nb_{\\text{imp}}(\\sigma, \\varepsilon) = \\int_{-\\varepsilon/2}^{\\varepsilon/2} G_{\\sigma}(\\tau) \\, d\\tau\n$$\nWe are asked for the leading-order approximation in the regime $\\varepsilon/\\sigma \\ll 1$. This condition implies that the integration interval $[-\\varepsilon/2, \\varepsilon/2]$ is very narrow compared to the width $\\sigma$ of the Gaussian function. Over this small interval, we can approximate the integrand $G_{\\sigma}(\\tau)$ by its value at the center of the interval, $\\tau = 0$. This corresponds to taking the zeroth-order term of the Taylor expansion of $G_{\\sigma}(\\tau)$ around $\\tau=0$.\n$$\nG_{\\sigma}(\\tau) \\approx G_{\\sigma}(0) \\quad \\text{for} \\quad \\tau \\in [-\\varepsilon/2, \\varepsilon/2]\n$$\nThe value of the Gaussian kernel at its center is:\n$$\nG_{\\sigma}(0) = \\frac{1}{\\sqrt{2 \\pi} \\, \\sigma} \\exp\\left(-\\frac{0^{2}}{2 \\sigma^{2}}\\right) = \\frac{1}{\\sqrt{2 \\pi} \\, \\sigma}\n$$\nSubstituting this constant approximation into the integral for $b_{\\text{imp}}(\\sigma, \\varepsilon)$:\n$$\nb_{\\text{imp}}(\\sigma, \\varepsilon) \\approx \\int_{-\\varepsilon/2}^{\\varepsilon/2} G_{\\sigma}(0) \\, d\\tau = G_{\\sigma}(0) \\int_{-\\varepsilon/2}^{\\varepsilon/2} 1 \\, d\\tau\n$$\nThe integral simply evaluates to the length of the interval, which is $\\varepsilon/2 - (-\\varepsilon/2) = \\varepsilon$.\n$$\nb_{\\text{imp}}(\\sigma, \\varepsilon) \\approx G_{\\sigma}(0) \\cdot \\varepsilon = \\left(\\frac{1}{\\sqrt{2 \\pi} \\, \\sigma}\\right) \\varepsilon\n$$\nRewriting this to emphasize the dimensionless ratio $\\varepsilon/\\sigma$:\n$$\nb_{\\text{imp}}(\\sigma, \\varepsilon) \\approx \\frac{\\varepsilon}{\\sigma \\sqrt{2 \\pi}}\n$$\nThis is the required leading-order approximation.\n\n#### Summary of Results\n\nThe two derived quantities are:\n1.  $b_{\\text{step}}(\\sigma) = \\frac{1}{2}$\n2.  $b_{\\text{imp}}(\\sigma, \\varepsilon) \\approx \\frac{\\varepsilon}{\\sigma \\sqrt{2 \\pi}}$\n\nThe final answer should be presented as a row vector containing these two expressions.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2} & \\frac{\\varepsilon}{\\sigma \\sqrt{2 \\pi}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While theoretical analyses often assume infinite signals, real-world data is always finite. This practical limitation becomes critical when applying a filter near data boundaries, as the operation requires information from beyond the signal's edge. This exercise  explores how different \"padding\" schemes address this issue and asks you to quantify the magnitude of the resulting edge artifacts, a vital skill for accurately interpreting smoothed data like Peri-Stimulus Time Histograms (PSTHs).",
            "id": "4153097",
            "problem": "A common operation in neuroscience data analysis is smoothing a Peri-Stimulus Time Histogram (PSTH) using a Gaussian filter to estimate time-varying firing rates. Consider a continuous-time PSTH modeled as a step change at stimulus onset, with baseline firing rate $B \\ge 0$ for times $t < 0$ and a step to $B + A$ for times $t \\ge 0$, where $A > 0$ is the step amplitude. Let the Gaussian filter kernel be $g_{\\sigma}(t) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\!\\left(-\\frac{t^{2}}{2 \\sigma^{2}}\\right)$ for standard deviation $\\sigma > 0$, and define Gaussian smoothing of a signal $x(t)$ as the convolution $(x * g_{\\sigma})(t) = \\int_{-\\infty}^{\\infty} g_{\\sigma}(\\tau)\\, x(t - \\tau)\\, d\\tau$. The ideal full-data smoothed output is $y^{\\star}(t) = (x_{\\star} * g_{\\sigma})(t)$ for the full-data signal $x_{\\star}(t)$ defined by $x_{\\star}(t) = B$ for $t < 0$ and $x_{\\star}(t) = B + A$ for $t \\ge 0$.\n\nIn practice, smoothing near the stimulus onset boundary at $t = 0$ requires a padding scheme because only data for $t \\ge 0$ are available. Define three padding schemes that extend the observed signal $x(t)$ (known only for $t \\ge 0$) to a padded signal $\\tilde{x}(t)$ for all $t \\in \\mathbb{R}$:\n- Zero padding: $\\tilde{x}(t) = x(t)$ for $t \\ge 0$ and $\\tilde{x}(t) = 0$ for $t < 0$.\n- Reflection padding: $\\tilde{x}(t) = x(t)$ for $t \\ge 0$ and $\\tilde{x}(t) = x(|t|)$ for $t < 0$.\n- Constant extension padding: $\\tilde{x}(t) = x(t)$ for $t \\ge 0$ and $\\tilde{x}(t) = x(0)$ for $t < 0$.\n\nFor each padding scheme, derive the smoothed output $\\tilde{y}(t) = (\\tilde{x} * g_{\\sigma})(t)$ at times $t \\ge 0$, starting from the convolution definition and the properties of the Gaussian kernel. Then, define the edge artifact magnitude $M$ for each scheme as the maximum absolute deviation of the padded smoothed output from the ideal full-data smoothed output over nonnegative times,\n$$\nM \\equiv \\sup_{t \\ge 0} \\left| \\tilde{y}(t) - y^{\\star}(t) \\right|.\n$$\nExpress the final results as dimensionless fractions of the step amplitude by reporting $M/A$ for the three schemes in the order specified above (zero padding, reflection padding, constant extension padding). Provide the three values in a single row matrix. No rounding is required.",
            "solution": "The problem is scientifically grounded, well-posed, and objective. All terms and conditions are formally defined, allowing for a rigorous mathematical solution. The problem is therefore deemed valid.\n\nThe solution requires calculating the smoothed output for three different padding schemes and comparing each to an ideal smoothed output. The core of the problem involves computing convolution integrals with a Gaussian kernel.\n\nFirst, we establish the necessary definitions and compute the ideal smoothed output, which serves as a common reference. The ideal signal, representing a step change in firing rate, is given by\n$$\nx_{\\star}(t) = \\begin{cases} B & t < 0 \\\\ B+A & t \\ge 0 \\end{cases}\n$$\nThis can be expressed using the Heaviside step function $H(t)$ (where $H(t)=1$ for $t \\ge 0$ and $H(t)=0$ for $t < 0$) as $x_{\\star}(t) = B + A H(t)$.\n\nThe Gaussian filter kernel is $g_{\\sigma}(t) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{t^{2}}{2 \\sigma^{2}}\\right)$. The integral of this kernel is central to the convolution operation. Let $\\Phi(u)$ be the cumulative distribution function (CDF) of the standard normal distribution $N(0, 1)$:\n$$\n\\Phi(u) = \\int_{-\\infty}^{u} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{v^2}{2}\\right) dv\n$$\nWith a change of variables $v = \\tau/\\sigma$, we can show that the integral of the specified Gaussian kernel is related to $\\Phi(u)$:\n$$\n\\int_{-\\infty}^{t} g_{\\sigma}(\\tau) d\\tau = \\int_{-\\infty}^{t/\\sigma} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{v^2}{2}\\right) dv = \\Phi\\left(\\frac{t}{\\sigma}\\right)\n$$\nA key property of $\\Phi(u)$ is that $\\Phi(0) = 1/2$.\n\nThe ideal smoothed output $y^{\\star}(t)$ is the convolution $(x_{\\star} * g_{\\sigma})(t)$:\n$$\ny^{\\star}(t) = \\int_{-\\infty}^{\\infty} g_{\\sigma}(\\tau) x_{\\star}(t - \\tau) d\\tau\n$$\nBy linearity of convolution, $y^{\\star}(t) = ((B + A H) * g_{\\sigma})(t) = (B * g_{\\sigma})(t) + (A H * g_{\\sigma})(t)$.\nThe convolution of a constant $B$ with the normalized kernel $g_{\\sigma}$ is simply $B$. The convolution of the scaled Heaviside function $A H(t)$ is:\n$$\n(A H * g_{\\sigma})(t) = A \\int_{-\\infty}^{\\infty} g_{\\sigma}(\\tau) H(t - \\tau) d\\tau = A \\int_{-\\infty}^{t} g_{\\sigma}(\\tau) d\\tau = A \\Phi\\left(\\frac{t}{\\sigma}\\right)\n$$\nTherefore, the ideal smoothed output is:\n$$\ny^{\\star}(t) = B + A \\Phi\\left(\\frac{t}{\\sigma}\\right)\n$$\n\nNow we analyze each padding scheme for the observed signal $x(t) = B+A$ for $t \\ge 0$.\n\n**1. Zero Padding**\nThe padded signal $\\tilde{x}_{zero}(t)$ is defined as $\\tilde{x}_{zero}(t) = x(t)$ for $t \\ge 0$ and $\\tilde{x}_{zero}(t) = 0$ for $t < 0$. This can be written as $\\tilde{x}_{zero}(t) = (B+A)H(t)$.\nThe smoothed output $\\tilde{y}_{zero}(t)$ is:\n$$\n\\tilde{y}_{zero}(t) = (\\tilde{x}_{zero} * g_{\\sigma})(t) = ((B+A)H * g_{\\sigma})(t) = (B+A) \\Phi\\left(\\frac{t}{\\sigma}\\right)\n$$\nThe deviation from the ideal output for $t \\ge 0$ is:\n$$\nD_{zero}(t) = \\tilde{y}_{zero}(t) - y^{\\star}(t) = (B+A) \\Phi\\left(\\frac{t}{\\sigma}\\right) - \\left(B + A \\Phi\\left(\\frac{t}{\\sigma}\\right)\\right) = B \\Phi\\left(\\frac{t}{\\sigma}\\right) - B = B\\left(\\Phi\\left(\\frac{t}{\\sigma}\\right) - 1\\right)\n$$\nThe edge artifact magnitude $M_{zero}$ is the supremum of the absolute deviation for $t \\ge 0$:\n$$\nM_{zero} = \\sup_{t \\ge 0} \\left| B\\left(\\Phi\\left(\\frac{t}{\\sigma}\\right) - 1\\right) \\right|\n$$\nSince $B \\ge 0$ and $\\Phi(u) \\le 1$, the expression inside the absolute value is non-positive. Thus:\n$$\nM_{zero} = \\sup_{t \\ge 0} -B\\left(\\Phi\\left(\\frac{t}{\\sigma}\\right) - 1\\right) = \\sup_{t \\ge 0} B\\left(1 - \\Phi\\left(\\frac{t}{\\sigma}\\right)\\right)\n$$\nFor $t \\ge 0$, the function $\\Phi(t/\\sigma)$ is monotonically increasing, starting from $\\Phi(0)=1/2$. Therefore, $1 - \\Phi(t/\\sigma)$ is monotonically decreasing. The supremum occurs at the lower bound of the domain, $t=0$:\n$$\nM_{zero} = B\\left(1 - \\Phi(0)\\right) = B\\left(1 - \\frac{1}{2}\\right) = \\frac{B}{2}\n$$\nThe required dimensionless fraction is $M_{zero}/A = \\frac{B}{2A}$.\n\n**2. Reflection Padding**\nThe padded signal $\\tilde{x}_{refl}(t)$ is defined as $\\tilde{x}_{refl}(t) = x(t)$ for $t \\ge 0$ and $\\tilde{x}_{refl}(t) = x(|t|)$ for $t < 0$.\nGiven $x(t) = B+A$ for $t \\ge 0$, we have:\n- For $t \\ge 0$, $\\tilde{x}_{refl}(t) = B+A$.\n- For $t < 0$, $|t| > 0$, so $\\tilde{x}_{refl}(t) = x(|t|) = B+A$.\nThus, $\\tilde{x}_{refl}(t) = B+A$ for all $t \\in \\mathbb{R}$.\nThe smoothed output $\\tilde{y}_{refl}(t)$ is the convolution of a constant with $g_{\\sigma}(t)$:\n$$\n\\tilde{y}_{refl}(t) = \\int_{-\\infty}^{\\infty} g_{\\sigma}(\\tau) (B+A) d\\tau = (B+A) \\int_{-\\infty}^{\\infty} g_{\\sigma}(\\tau) d\\tau = B+A\n$$\nThe deviation from the ideal output for $t \\ge 0$ is:\n$$\nD_{refl}(t) = \\tilde{y}_{refl}(t) - y^{\\star}(t) = (B+A) - \\left(B + A \\Phi\\left(\\frac{t}{\\sigma}\\right)\\right) = A - A \\Phi\\left(\\frac{t}{\\sigma}\\right) = A\\left(1 - \\Phi\\left(\\frac{t}{\\sigma}\\right)\\right)\n$$\nThe edge artifact magnitude $M_{refl}$ is:\n$$\nM_{refl} = \\sup_{t \\ge 0} \\left| A\\left(1 - \\Phi\\left(\\frac{t}{\\sigma}\\right)\\right) \\right|\n$$\nSince $A > 0$ and $1 - \\Phi(u) \\ge 0$ for $u \\ge 0$, the absolute value is redundant. The function to be maximized is monotonically decreasing for $t \\ge 0$, so the supremum is at $t=0$:\n$$\nM_{refl} = A\\left(1 - \\Phi(0)\\right) = A\\left(1 - \\frac{1}{2}\\right) = \\frac{A}{2}\n$$\nThe required dimensionless fraction is $M_{refl}/A = \\frac{1}{2}$.\n\n**3. Constant Extension Padding**\nThe padded signal $\\tilde{x}_{const}(t)$ is defined as $\\tilde{x}_{const}(t) = x(t)$ for $t \\ge 0$ and $\\tilde{x}_{const}(t) = x(0)$ for $t < 0$.\nGiven $x(t) = B+A$ for $t \\ge 0$, we have $x(0)=B+A$. Thus:\n- For $t \\ge 0$, $\\tilde{x}_{const}(t) = B+A$.\n- For $t < 0$, $\\tilde{x}_{const}(t) = x(0) = B+A$.\nThis yields $\\tilde{x}_{const}(t) = B+A$ for all $t \\in \\mathbb{R}$, which is the same padded signal as in the reflection padding case for this specific problem.\nConsequently, all subsequent calculations are identical to the reflection padding case.\n$$\n\\tilde{y}_{const}(t) = B+A\n$$\n$$\nM_{const} = \\frac{A}{2}\n$$\nThe required dimensionless fraction is $M_{const}/A = \\frac{1}{2}$.\n\nThe three dimensionless artifact magnitudes $M/A$ in the specified order (zero, reflection, constant extension) are $\\frac{B}{2A}$, $\\frac{1}{2}$, and $\\frac{1}{2}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{B}{2A} & \\frac{1}{2} & \\frac{1}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The most critical parameter in Gaussian smoothing is the filter's bandwidth, $\\sigma$, which dictates the degree of smoothing. An optimal choice balances noise suppression against signal distortion, but this choice is data-dependent. This capstone practice  moves from analysis to principled optimization, guiding you to derive and implement a Leave-One-Out Cross-Validation (LOOCV) objective to select a data-driven optimal bandwidth for estimating a neural firing rate, a cornerstone technique in computational neuroscience.",
            "id": "4153064",
            "problem": "A fundamental task in neuroscience data analysis is to estimate an instantaneous spike rate from event times and to select an appropriate smoothing bandwidth for a Gaussian filter. Consider a spike train represented as a sum of Dirac delta functions, $x(t) = \\sum_{i=1}^{N} \\delta(t - t_i)$, observed over a finite recording window $[0,T]$ in seconds. Assume spikes arise from an inhomogeneous Poisson process with rate function $\\lambda(t)$ whose likelihood for observed events is specified by the standard inhomogeneous Poisson formulation: the likelihood is $\\mathcal{L}(\\lambda) = \\exp\\left(-\\int_{0}^{T} \\lambda(t) \\, dt\\right) \\prod_{i=1}^{N} \\lambda(t_i)$. Let the firing rate be estimated by smoothing the spike train using a Gaussian filter of standard deviation $\\sigma$ (in seconds), with kernel $K_\\sigma(t) = \\dfrac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\left(-\\dfrac{t^2}{2\\sigma^2}\\right)$, yielding an estimator $\\hat{\\lambda}_\\sigma(t) = \\sum_{i=1}^{N} K_\\sigma(t - t_i)$.\n\nYour tasks are:\n- Derive, from first principles and the likelihood above, a cross-validated objective for bandwidth selection as a function of $\\sigma$ that eliminates the self-influence of each spike on itself. Use Leave-One-Out Cross-Validation (LOOCV) to define a log-likelihood-based objective in which the evaluation of $\\hat{\\lambda}_\\sigma$ at $t_i$ excludes the contribution from the spike at $t_i$. The recording window is finite, so do not assume integrals are over the entire real line; ensure your derivation accounts for the boundaries at $t=0$ and $t=T$.\n- Implement this objective and perform selection of $\\sigma$ by maximizing the objective over a fixed grid of candidate values.\n\nUnits: All times, including spike times $t_i$, the recording window length $T$, and the Gaussian bandwidth $\\sigma$, must be treated in seconds. Your program must report each selected bandwidth in seconds, rounded to six decimal places.\n\nTest Suite:\nFor the following four test cases, compute and return the bandwidth $\\sigma$ (in seconds) that maximizes your derived LOOCV log-likelihood objective over the candidate grid $\\mathcal{S} = [\\,0.002,\\,0.005,\\,0.010,\\,0.020,\\,0.050,\\,0.100,\\,0.200,\\,0.300\\,]$ (seconds). Each case specifies $T$ (seconds) and spike times $\\{t_i\\}$ (seconds).\n1. Happy path (approximately uniform spike spacing): $T = 1.0$, spikes $t_i = \\dfrac{i + 0.5}{50}$ for $i \\in \\{0,1,\\dots,49\\}$.\n2. Burst plus background: $T = 1.0$, spikes defined by the union of a burst set and a background set. Burst set: $t_i^{\\text{burst}} = 0.5 + 0.0005\\,(i-10)$ for $i \\in \\{0,1,\\dots,19\\}$. Background set: $t_j^{\\text{bg}} = \\dfrac{j + 0.5}{10}$ for $j \\in \\{0,1,\\dots,9\\}$.\n3. Sparse: $T = 1.0$, spikes at $\\{0.1,\\,0.5,\\,0.9\\}$.\n4. Two close spikes: $T = 1.0$, spikes at $\\{0.5,\\,0.505\\}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the selected bandwidths, in seconds, for the four test cases, as a comma-separated list enclosed in square brackets, rounded to six decimal places (e.g., \"[0.020000,0.015000,0.100000,0.005000]\").",
            "solution": "The task is to derive a cross-validated objective for selecting the optimal bandwidth $\\sigma$ for Gaussian kernel smoothing of a neural spike train, and then to implement this method to find the optimal $\\sigma$ for several test cases from a given grid of candidates. The spike train is modeled as a realization of an inhomogeneous Poisson process.\n\nThe foundation of our approach is the log-likelihood function for an inhomogeneous Poisson process with rate function $\\lambda(t)$ over a time interval $[0, T]$, given observed spike times $\\{t_1, t_2, \\dots, t_N\\}$:\n$$ \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{N} \\log \\lambda(t_i) - \\int_{0}^{T} \\lambda(t) \\, dt $$\nThe rate function $\\lambda(t)$ is unknown and is estimated from the data by convolving the spike train, represented as a sum of Dirac delta functions $x(t) = \\sum_{i=1}^{N} \\delta(t - t_i)$, with a Gaussian kernel $K_\\sigma(t)$. This yields the rate estimate:\n$$ \\hat{\\lambda}_\\sigma(t) = \\sum_{j=1}^{N} K_\\sigma(t - t_j) = \\sum_{j=1}^{N} \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\left(-\\frac{(t - t_j)^2}{2\\sigma^2}\\right) $$\nA naive approach would be to substitute $\\hat{\\lambda}_\\sigma(t)$ directly into the log-likelihood function and maximize the result with respect to $\\sigma$. This fails because the term $\\sum_{i=1}^{N} \\log \\hat{\\lambda}_\\sigma(t_i)$ contains a pathology. When evaluating the rate at a spike time $t_i$, the sum for $\\hat{\\lambda}_\\sigma(t_i)$ includes the term $K_\\sigma(t_i - t_i) = K_\\sigma(0)$. The value of the Gaussian kernel at its center, $K_\\sigma(0) = 1/(\\sqrt{2\\pi}\\sigma)$, diverges to infinity as $\\sigma \\to 0$. This \"self-influence\" would cause the log-likelihood to become arbitrarily large for infinitesimally small $\\sigma$, leading to the selection of $\\sigma=0$, which corresponds to a completely over-fitted model (a series of delta functions) and is not a useful rate estimate.\n\nTo circumvent this problem, we employ Leave-One-Out Cross-Validation (LOOCV). The core principle of LOOCV is to assess the model's predictive power on data not used in its training. For each spike $t_i$, we construct a rate estimate using all other spikes, $\\{t_j\\}_{j \\neq i}$. This leave-one-out estimator is:\n$$ \\hat{\\lambda}_{\\sigma, (-i)}(t) = \\sum_{j=1, j\\neq i}^{N} K_\\sigma(t - t_j) $$\nWe then evaluate the log-likelihood of the held-out spike $t_i$ using this partial estimate. The cross-validated objective function, which we denote $J(\\sigma)$, is constructed by summing the log-likelihood contributions from each left-out spike and subtracting a penalty term corresponding to the integral of the full rate estimate. The penalty term accounts for the overall complexity and magnitude of the estimated rate function. The objective to be maximized is thus:\n$$ J(\\sigma) = \\sum_{i=1}^{N} \\log \\left( \\hat{\\lambda}_{\\sigma, (-i)}(t_i) \\right) - \\int_{0}^{T} \\hat{\\lambda}_\\sigma(t) \\, dt $$\nLet us now derive the explicit form of each term.\n\nThe first term is the sum of the log-predictive probabilities, which quantifies the model's ability to predict each spike's location based on the others:\n$$ \\text{Term 1} = \\sum_{i=1}^{N} \\log \\left( \\sum_{j=1, j\\neq i}^{N} K_\\sigma(t_i - t_j) \\right) = \\sum_{i=1}^{N} \\log \\left( \\sum_{j=1, j\\neq i}^{N} \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(t_i - t_j)^2}{2\\sigma^2}\\right) \\right) $$\nThis term is well-behaved as $\\sigma \\to 0$. If $\\sigma$ is very small, the argument of the logarithm becomes the rate contribution from the nearest neighbors, avoiding the divergence caused by self-contribution.\n\nThe second term is the penalty, which is the integral of the full rate estimate over the recording window $[0, T]$. This term must account for the finite boundaries of integration.\n$$ \\text{Term 2} = \\int_{0}^{T} \\hat{\\lambda}_\\sigma(t) \\, dt = \\int_{0}^{T} \\sum_{j=1}^{N} K_\\sigma(t - t_j) \\, dt = \\sum_{j=1}^{N} \\int_{0}^{T} K_\\sigma(t - t_j) \\, dt $$\nFor each individual spike $j$, the integral is:\n$$ \\int_{0}^{T} \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(t - t_j)^2}{2\\sigma^2}\\right) dt $$\nWe perform a change of variables with $u = \\frac{t - t_j}{\\sigma}$, which implies $dt = \\sigma \\, du$. The integration limits change from $t=0$ and $t=T$ to $u = \\frac{0 - t_j}{\\sigma} = -\\frac{t_j}{\\sigma}$ and $u = \\frac{T - t_j}{\\sigma}$, respectively. The integral becomes:\n$$ \\int_{-t_j/\\sigma}^{(T-t_j)/\\sigma} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) du $$\nThis integral is the probability mass of a standard normal distribution, $N(0, 1)$, over the interval $[ -t_j/\\sigma, (T-t_j)/\\sigma ]$. It can be expressed using the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-v^2/2} dv$:\n$$ \\int_{0}^{T} K_\\sigma(t - t_j) \\, dt = \\Phi\\left(\\frac{T-t_j}{\\sigma}\\right) - \\Phi\\left(-\\frac{t_j}{\\sigma}\\right) $$\nSumming over all spikes $j$ gives the complete penalty term:\n$$ \\text{Term 2} = \\sum_{j=1}^{N} \\left[ \\Phi\\left(\\frac{T-t_j}{\\sigma}\\right) - \\Phi\\left(-\\frac{t_j}{\\sigma}\\right) \\right] $$\nThis boundary-corrected term properly handles the loss of probability mass for kernels centered near the edges of the recording window. For numerical implementation, the CDF $\\Phi(z)$ is computed using the error function, $\\text{erf}(x)$, via the relation $\\Phi(z) = \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right)$.\n\nCombining both terms, the final LOOCV objective function to be maximized over the grid of candidate bandwidths $\\mathcal{S}$ is:\n$$ J(\\sigma) = \\sum_{i=1}^{N} \\log \\left( \\sum_{j \\neq i} \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(t_i - t_j)^2}{2\\sigma^2}\\right) \\right) - \\sum_{j=1}^{N} \\left[ \\Phi\\left(\\frac{T-t_j}{\\sigma}\\right) - \\Phi\\left(-\\frac{t_j}{\\sigma}\\right) \\right] $$\nThe implementation will calculate $J(\\sigma)$ for each $\\sigma \\in \\mathcal{S}$ and select the one that yields the largest value. Vectorized computations are used for efficiency, particularly for calculating the pairwise interactions in the first term.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Derives and applies a LOOCV objective to select Gaussian filter bandwidths for spike trains.\n    \"\"\"\n\n    def norm_cdf(x):\n        \"\"\"\n        Calculates the cumulative distribution function (CDF) of the standard normal distribution.\n        Uses the error function (erf) for computation.\n        Î¦(z) = 0.5 * (1 + erf(z / sqrt(2)))\n        \"\"\"\n        return 0.5 * (1.0 + erf(x / np.sqrt(2.0)))\n\n    def calculate_loocv_objective(sigma, spikes, T):\n        \"\"\"\n        Calculates the Leave-One-Out Cross-Validation log-likelihood objective J(sigma).\n        \n        Args:\n            sigma (float): The bandwidth (standard deviation) of the Gaussian kernel.\n            spikes (np.ndarray): An array of spike times.\n            T (float): The total recording time.\n        \n        Returns:\n            float: The value of the objective function J(sigma).\n        \"\"\"\n        N = len(spikes)\n        # For N <= 1, the sum over j!=i is empty, log(0) -> -inf.\n        # This is a sensible result as bandwidth selection is ill-defined.\n        if N <= 1:\n            return -np.inf\n        # sigma must be positive.\n        if sigma <= 0:\n            return -np.inf\n\n        # Term 1: Sum of log-likelihoods of left-out spikes\n        # spikes[:, np.newaxis] creates a column vector for broadcasting.\n        dists_sq = (spikes[:, np.newaxis] - spikes)**2\n        \n        # Evaluate the exponential part of the kernel for all pairs.\n        exp_terms = np.exp(-dists_sq / (2.0 * sigma**2))\n        \n        # Exclude self-influence by setting diagonal (i=j) terms to zero.\n        np.fill_diagonal(exp_terms, 0.0)\n        \n        # Sum contributions for each leave-one-out estimate.\n        sum_exp_terms = np.sum(exp_terms, axis=1)\n\n        # If sum_exp_terms is zero (e.g., for very sparse spikes and small sigma),\n        # the estimated rate is zero, and its log is -inf.\n        if np.any(sum_exp_terms <= 1e-300): # Use a small threshold to avoid log(0)\n            return -np.inf\n            \n        constant_factor = 1.0 / (np.sqrt(2.0 * np.pi) * sigma)\n        loo_rates = constant_factor * sum_exp_terms\n        \n        log_lik_term = np.sum(np.log(loo_rates))\n\n        # Term 2: Integral of the full rate estimate over [0, T]\n        # This is the sum of boundary-corrected kernel integrals.\n        z_upper = (T - spikes) / sigma\n        z_lower = -spikes / sigma\n        \n        integral_contributions = norm_cdf(z_upper) - norm_cdf(z_lower)\n        integral_term = np.sum(integral_contributions)\n        \n        return log_lik_term - integral_term\n\n    def find_optimal_sigma(T, spikes, sigma_grid):\n        \"\"\"\n        Finds the optimal sigma from a grid by maximizing the LOOCV objective.\n        \"\"\"\n        objective_values = [calculate_loocv_objective(sigma, spikes, T) for sigma in sigma_grid]\n        \n        # In case all values are -inf, np.argmax would pick the first one. This is an acceptable tie-break.\n        max_index = np.argmax(objective_values)\n        return sigma_grid[max_index]\n\n    # Define the grid of candidate bandwidths (in seconds).\n    sigma_grid = np.array([0.002, 0.005, 0.010, 0.020, 0.050, 0.100, 0.200, 0.300])\n\n    # ---- Define the test cases from the problem statement. ----\n    \n    # Case 1: Happy path (approximately uniform spike spacing)\n    T1 = 1.0\n    spikes1 = (np.arange(50) + 0.5) / 50.0\n\n    # Case 2: Burst plus background\n    T2 = 1.0\n    burst = 0.5 + 0.0005 * (np.arange(20) - 10.0)\n    background = (np.arange(10) + 0.5) / 10.0\n    spikes2 = np.sort(np.concatenate((burst, background)))\n\n    # Case 3: Sparse\n    T3 = 1.0\n    spikes3 = np.array([0.1, 0.5, 0.9])\n\n    # Case 4: Two close spikes\n    T4 = 1.0\n    spikes4 = np.array([0.5, 0.505])\n\n    test_cases = [\n        (T1, spikes1),\n        (T2, spikes2),\n        (T3, spikes3),\n        (T4, spikes4),\n    ]\n\n    results = []\n    for T, spikes in test_cases:\n        optimal_sigma = find_optimal_sigma(T, spikes, sigma_grid)\n        results.append(optimal_sigma)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}