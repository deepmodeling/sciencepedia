{
    "hands_on_practices": [
        {
            "introduction": "While conceptually simple, a naive implementation of a moving average filter can be computationally prohibitive for large datasets. This exercise  guides you through the derivation of an efficient recursive algorithm that computes the smoothed output with constant cost per sample, a crucial optimization in practice. By deriving this 'running-sum' method from the convolution sum, you will gain insight into the filter's structure and its effect on a simple trending signal.",
            "id": "4153544",
            "problem": "A neurophysiology laboratory acquires a discrete-time neuronal firing-rate series $x[n]$, $n \\in \\mathbb{Z}$, by binning spikes at a fixed sampling interval $\\Delta t$. To reduce high-frequency variability while preserving slow drifts, the team applies a causal $M$-point simple moving average filter. The filter is implemented as a linear time-invariant (LTI) system whose impulse response $h[n]$ is given by $h[n] = \\frac{1}{M}$ for $n = 0, 1, \\dots, M-1$ and $h[n] = 0$ otherwise. The output $y[n]$ is defined by the discrete-time convolution $y[n] = \\sum_{k=-\\infty}^{\\infty} x[k]\\,h[n-k]$. Assume zero-padding of the input, i.e., $x[n] = 0$ for $n < 0$.\n\nStarting only from the discrete-time convolution definition and the specified impulse response, derive an efficient causal recursion for a running sum $s[n]$ that can be used to compute $y[n]$ with constant per-sample cost, and express $y[n]$ in terms of $s[n]$ and $M$. Do not assume any unproven shortcut formulas.\n\nThen, suppose the input is a ramp-plus-offset $x[n] = \\alpha n + \\beta$ for all integers $n \\geq 0$ and $x[n] = 0$ for $n < 0$, with real constants $\\alpha$ and $\\beta$. Using the recursion you derived, determine a closed-form expression for the smoothed output $y[n]$ valid for all integer $n \\geq M - 1$. Express your final result explicitly as a single closed-form analytic expression in $n$, $M$, $\\alpha$, and $\\beta$. No rounding is required.",
            "solution": "The problem is evaluated as valid. It is scientifically grounded in standard linear systems theory, is well-posed, objective, and contains all necessary information for a unique solution.\n\nThe problem consists of two parts. First, we must derive an efficient recursive algorithm for a simple moving average filter. Second, we must apply this filter to a specific input signal and find the closed-form expression for the output.\n\nPart 1: Derivation of the Recursive Algorithm\n\nThe output $y[n]$ of the linear time-invariant (LTI) system is given by the discrete-time convolution of the input signal $x[n]$ and the system's impulse response $h[n]$:\n$$\ny[n] = \\sum_{k=-\\infty}^{\\infty} x[k]\\,h[n-k]\n$$\nThe impulse response $h[n]$ for a causal $M$-point simple moving average filter is given as:\n$$\nh[n] = \\begin{cases} \\frac{1}{M} & \\text{for } 0 \\le n \\le M-1 \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe term $h[n-k]$ in the convolution sum is non-zero only when its argument, $n-k$, is in the range $[0, M-1]$. This implies $0 \\le n-k \\le M-1$. Solving for $k$, we get $n-M+1 \\le k \\le n$.\nTherefore, the convolution sum can be rewritten with finite limits:\n$$\ny[n] = \\sum_{k=n-M+1}^{n} x[k]\\,h[n-k]\n$$\nWithin this summation range, $h[n-k] = \\frac{1}{M}$. Thus, we have:\n$$\ny[n] = \\frac{1}{M} \\sum_{k=n-M+1}^{n} x[k]\n$$\nTo create an efficient computation method, we define a running sum $s[n]$ as:\n$$\ns[n] = \\sum_{k=n-M+1}^{n} x[k]\n$$\nWith this definition, the output $y[n]$ is simply expressed in terms of $s[n]$ and $M$ as:\n$$\ny[n] = \\frac{s[n]}{M}\n$$\nTo find a recursion for $s[n]$, we examine the relationship between $s[n]$ and $s[n-1]$. The expression for $s[n-1]$ is:\n$$\ns[n-1] = \\sum_{k=(n-1)-M+1}^{n-1} x[k] = \\sum_{k=n-M}^{n-1} x[k]\n$$\nWe can write out the terms for $s[n]$:\n$$\ns[n] = x[n] + x[n-1] + \\dots + x[n-M+1]\n$$\nAnd for $s[n-1]$:\n$$\ns[n-1] = x[n-1] + x[n-2] + \\dots + x[n-M]\n$$\nBy comparing these two sums, we can see that $s[n]$ can be obtained from $s[n-1]$ by adding the new sample $x[n]$ and subtracting the oldest sample from the previous window, $x[n-M]$. This gives the recursion:\n$$\ns[n] = s[n-1] + x[n] - x[n-M]\n$$\nThis recursion is highly efficient as it requires only one addition and one subtraction for each new sample $n$, regardless of the filter length $M$. This constitutes a constant per-sample computational cost. Note that because the input is zero-padded ($x[n]=0$ for $n<0$), the term $x[n-M]$ is zero for $n < M$. The recursion is fully valid for $n \\ge M$.\n\nPart 2: Application to a Ramp-Plus-Offset Input\n\nWe are given the input signal $x[n] = \\alpha n + \\beta$ for $n \\ge 0$, and $x[n]=0$ for $n<0$. We need to find a closed-form expression for $y[n]$ for all integers $n \\ge M-1$. We will use the recursive relationship derived for the running sum $s[n]$.\n\nThe recursion for $s[n]$ is driven by the difference $x[n] - x[n-M]$. For $n \\ge M$, both $n$ and $n-M$ are non-negative, so we can use the expression $x[n] = \\alpha n + \\beta$.\n$$\nx[n] - x[n-M] = (\\alpha n + \\beta) - (\\alpha(n-M) + \\beta)\n$$\n$$\nx[n] - x[n-M] = \\alpha n + \\beta - \\alpha n + \\alpha M - \\beta = \\alpha M\n$$\nFor $n \\ge M$, the difference equation for the running sum becomes:\n$$\ns[n] = s[n-1] + \\alpha M\n$$\nThis is a first-order linear non-homogeneous difference equation with a constant forcing term. The solution indicates that $s[n]$ forms an arithmetic progression for $n \\ge M$.\n\nTo find a closed-form for $s[n]$ for $n \\ge M-1$, we can express $s[n]$ in terms of an initial value, $s[M-1]$, and the subsequent increments.\n$$\ns[n] = s[M-1] + \\sum_{j=M}^{n} (s[j] - s[j-1])\n$$\nFor $j \\ge M$, we have $s[j] - s[j-1] = \\alpha M$. The sum runs from $j=M$ to $n$, which contains $n-M+1$ terms.\n$$\ns[n] = s[M-1] + \\sum_{j=M}^{n} \\alpha M = s[M-1] + (n-M+1)\\alpha M\n$$\nThis formula is valid for $n \\ge M-1$. For $n=M-1$, the sum is empty (or has zero terms), so $s[M-1]=s[M-1]$.\n\nNext, we must compute the initial value $s[M-1]$ by direct summation. Since $n=M-1$, the sum for $s[M-1]$ runs from $k=(M-1)-M+1=0$ to $k=M-1$.\n$$\ns[M-1] = \\sum_{k=0}^{M-1} x[k]\n$$\nFor this range of $k$, $k \\ge 0$, so $x[k] = \\alpha k + \\beta$.\n$$\ns[M-1] = \\sum_{k=0}^{M-1} (\\alpha k + \\beta) = \\alpha \\sum_{k=0}^{M-1} k + \\beta \\sum_{k=0}^{M-1} 1\n$$\nUsing the formulas for the sum of the first integers and for the sum of a constant:\n$$\n\\sum_{k=0}^{M-1} k = \\frac{(M-1)M}{2}\n$$\n$$\n\\sum_{k=0}^{M-1} 1 = M\n$$\nSubstituting these results, we get:\n$$\ns[M-1] = \\alpha \\frac{(M-1)M}{2} + \\beta M\n$$\nNow we substitute this expression for $s[M-1]$ back into the equation for $s[n]$:\n$$\ns[n] = \\left(\\alpha \\frac{(M-1)M}{2} + \\beta M\\right) + (n-M+1)\\alpha M\n$$\nWe can factor out $\\alpha M$ and combine terms:\n$$\ns[n] = \\beta M + \\alpha M \\left[ \\frac{M-1}{2} + (n-M+1) \\right]\n$$\n$$\ns[n] = \\beta M + \\alpha M \\left[ \\frac{M-1 + 2n - 2M + 2}{2} \\right]\n$$\n$$\ns[n] = \\beta M + \\alpha M \\left[ \\frac{2n - M + 1}{2} \\right]\n$$\nFinally, we compute the output $y[n] = s[n]/M$:\n$$\ny[n] = \\frac{1}{M} \\left( \\beta M + \\alpha M \\left[ \\frac{2n - M + 1}{2} \\right] \\right)\n$$\n$$\ny[n] = \\beta + \\alpha \\left( \\frac{2n - M + 1}{2} \\right)\n$$\nThis expression can be rearranged into a more interpretable form:\n$$\ny[n] = \\beta + \\alpha \\left( n - \\frac{M-1}{2} \\right) = \\alpha n + \\beta - \\frac{\\alpha(M-1)}{2}\n$$\nThis is the closed-form expression for the output $y[n]$ valid for all integers $n \\ge M-1$. It shows that the smoothed output is also a ramp with the same slope $\\alpha$, but with a modified offset. The term $-\\frac{\\alpha(M-1)}{2}$ represents a constant vertical shift, which is related to the group delay of the moving average filter.",
            "answer": "$$\\boxed{\\alpha n + \\beta - \\frac{\\alpha(M-1)}{2}}$$"
        },
        {
            "introduction": "A moving average filter is fundamentally a low-pass filter, but how selective is it? This practice  explores this question by guiding you through the derivation of the filter's frequency response from its time-domain definition. Understanding how the filter's gain changes with frequency allows you to quantitatively predict its effect on complex signals, such as separating low-frequency neural oscillations from high-frequency noise.",
            "id": "4153579",
            "problem": "A neuroscience laboratory records a bursty population spike train from cortex and estimates an instantaneous firing intensity modeled as a deterministic mean rate plus oscillatory components representing burst modulation and fast microstructure. Specifically, the rate is modeled as $r(t) = r_{0} + b \\cos(2 \\pi f_{b} t) + c \\cos(2 \\pi f_{h} t)$, where $r_{0}$ is a nonnegative constant baseline, $b$ and $c$ are nonnegative amplitudes, and $f_{b}$ and $f_{h}$ are positive frequencies in hertz with $f_{h} > f_{b}$. To reduce high-frequency fluctuations prior to burst detection, the laboratory applies a causal moving average smoother with a rectangular kernel of duration $T$, defined by $h(t) = \\frac{1}{T}$ for $0 \\leq t \\leq T$ and $h(t) = 0$ otherwise, producing the smoothed rate $\\tilde{r}(t) = (r * h)(t)$ via time-domain convolution.\n\nUsing only the foundational definitions of time-domain convolution and the Fourier transform, derive an analytical expression for the magnitude of the frequency response of the moving average smoother as a function of angular frequency $\\omega$. Then, using that result, compute the ratio of attenuation magnitudes of the high-frequency component to the burst component,\n$$\\rho = \\frac{A(f_{h})}{A(f_{b})},$$\nwhere $A(f)$ is the amplitude gain applied by the smoother to a cosine at frequency $f$. Evaluate $\\rho$ for $T = 0.075$ s, $f_{b} = 20$ Hz, and $f_{h} = 70$ Hz, and express your final answer as a single exact analytical expression. Do not round or approximate your final answer. State the ratio without units.",
            "solution": "The problem as stated is scientifically grounded, objective, and well-posed. All provided information is consistent and sufficient for a unique solution. The concepts invoked—convolution, Fourier transforms, and frequency response—are fundamental principles in signal processing and are appropriately applied to the context of neuroscience data analysis. Therefore, a solution will be derived.\n\nThe first task is to derive the magnitude of the frequency response, $|H(\\omega)|$, for the causal moving average smoother. The smoother is defined by its impulse response, the rectangular kernel:\n$$h(t) = \\begin{cases} \\frac{1}{T} & 0 \\leq t \\leq T \\\\ 0 & \\text{otherwise} \\end{cases}$$\nThe frequency response, $H(\\omega)$, is the Fourier transform of the impulse response $h(t)$. The definition of the Fourier transform is:\n$$H(\\omega) = \\mathcal{F}\\{h(t)\\} = \\int_{-\\infty}^{\\infty} h(t) e^{-i\\omega t} dt$$\nSubstituting the expression for $h(t)$ into the integral:\n$$H(\\omega) = \\int_{0}^{T} \\frac{1}{T} e^{-i\\omega t} dt$$\nWe can evaluate this integral directly. For $\\omega \\neq 0$:\n$$H(\\omega) = \\frac{1}{T} \\left[ \\frac{e^{-i\\omega t}}{-i\\omega} \\right]_{0}^{T} = \\frac{1}{T} \\left( \\frac{e^{-i\\omega T} - e^{0}}{-i\\omega} \\right) = \\frac{1 - e^{-i\\omega T}}{i\\omega T}$$\nTo find the magnitude, $|H(\\omega)|$, it is advantageous to manipulate this complex expression. We factor out a term $e^{-i\\omega T/2}$ from the numerator $1 - e^{-i\\omega T}$ to create a symmetric expression:\n$$1 - e^{-i\\omega T} = e^{-i\\omega T/2}(e^{i\\omega T/2} - e^{-i\\omega T/2})$$\nUsing Euler's formula, $e^{i\\theta} - e^{-i\\theta} = 2i\\sin(\\theta)$, the term in parentheses becomes $2i \\sin(\\frac{\\omega T}{2})$. Substituting this back into the expression for $H(\\omega)$:\n$$H(\\omega) = \\frac{e^{-i\\omega T/2} \\left(2i \\sin\\left(\\frac{\\omega T}{2}\\right)\\right)}{i\\omega T} = e^{-i\\omega T/2} \\frac{2\\sin(\\frac{\\omega T}{2})}{\\omega T}$$\nThis expression can be written in terms of the cardinal sine function, $\\text{sinc}(x) = \\frac{\\sin(\\pi x)}{\\pi x}$, but we will proceed with the current form. The magnitude of the frequency response is:\n$$|H(\\omega)| = \\left| e^{-i\\omega T/2} \\frac{2\\sin(\\frac{\\omega T}{2})}{\\omega T} \\right| = |e^{-i\\omega T/2}| \\left| \\frac{2\\sin(\\frac{\\omega T}{2})}{\\omega T} \\right|$$\nSince $|e^{-i\\phi}| = 1$ for any real $\\phi$, and noting that $\\frac{2\\sin(\\frac{\\omega T}{2})}{\\omega T} = \\frac{\\sin(\\frac{\\omega T}{2})}{\\frac{\\omega T}{2}}$, we have:\n$$|H(\\omega)| = \\left| \\frac{\\sin(\\frac{\\omega T}{2})}{\\frac{\\omega T}{2}} \\right|$$\nThis is the analytical expression for the magnitude of the frequency response as a function of angular frequency $\\omega$.\n\nThe problem defines $A(f)$ as the amplitude gain applied by the smoother to a cosine at frequency $f$. For a linear time-invariant (LTI) system, the amplitude gain at a given frequency is precisely the magnitude of the frequency response evaluated at the corresponding angular frequency, $\\omega = 2\\pi f$. Therefore:\n$$A(f) = |H(2\\pi f)| = \\left| \\frac{\\sin(\\frac{2\\pi f T}{2})}{\\frac{2\\pi f T}{2}} \\right| = \\left| \\frac{\\sin(\\pi f T)}{\\pi f T} \\right|$$\nWe are asked to compute the ratio of attenuation magnitudes $\\rho = \\frac{A(f_h)}{A(f_b)}$. Substituting the expression for $A(f)$:\n$$\\rho = \\frac{\\left| \\frac{\\sin(\\pi f_{h} T)}{\\pi f_{h} T} \\right|}{\\left| \\frac{\\sin(\\pi f_{b} T)}{\\pi f_{b} T} \\right|}$$\nSince $f_b$, $f_h$, and $T$ are all positive, the denominators $\\pi f_h T$ and $\\pi f_b T$ are also positive. We can rewrite the expression as:\n$$\\rho = \\frac{|\\sin(\\pi f_{h} T)|}{\\pi f_{h} T} \\cdot \\frac{\\pi f_{b} T}{|\\sin(\\pi f_{b} T)|} = \\frac{f_b}{f_h} \\frac{|\\sin(\\pi f_{h} T)|}{|\\sin(\\pi f_{b} T)|} = \\frac{f_b}{f_h} \\left| \\frac{\\sin(\\pi f_{h} T)}{\\sin(\\pi f_{b} T)} \\right|$$\nNow, we substitute the given numerical values: $T = 0.075$ s, $f_b = 20$ Hz, and $f_h = 70$ Hz.\nFirst, we compute the arguments of the sine functions:\nFor the burst component frequency $f_b$:\n$$\\pi f_b T = \\pi (20) (0.075) = \\pi (1.5) = \\frac{3\\pi}{2}$$\nFor the high-frequency component $f_h$:\n$$\\pi f_h T = \\pi (70) (0.075) = \\pi (5.25) = \\frac{21\\pi}{4}$$\nNext, we evaluate the sine functions for these arguments:\n$$\\sin(\\pi f_b T) = \\sin\\left(\\frac{3\\pi}{2}\\right) = -1$$\n$$\\sin(\\pi f_h T) = \\sin\\left(\\frac{21\\pi}{4}\\right) = \\sin\\left(\\frac{16\\pi + 5\\pi}{4}\\right) = \\sin\\left(4\\pi + \\frac{5\\pi}{4}\\right) = \\sin\\left(\\frac{5\\pi}{4}\\right)$$\nSince $\\frac{5\\pi}{4}$ lies in the third quadrant, its sine is negative. The reference angle is $\\frac{\\pi}{4}$.\n$$\\sin\\left(\\frac{5\\pi}{4}\\right) = -\\sin\\left(\\frac{\\pi}{4}\\right) = -\\frac{\\sqrt{2}}{2}$$\nFinally, we substitute these values into the expression for $\\rho$:\n$$\\rho = \\frac{20}{70} \\left| \\frac{-\\frac{\\sqrt{2}}{2}}{-1} \\right| = \\frac{2}{7} \\left| \\frac{\\sqrt{2}}{2} \\right| = \\frac{2}{7} \\cdot \\frac{\\sqrt{2}}{2}$$\nSimplifying the expression yields the final exact answer:\n$$\\rho = \\frac{\\sqrt{2}}{7}$$",
            "answer": "$$\\boxed{\\frac{\\sqrt{2}}{7}}$$"
        },
        {
            "introduction": "Theoretical filters often assume infinite signals, but real-world data is finite. This hands-on coding challenge  addresses the critical issue of edge effects by having you implement a robust, renormalized moving average. You will see why simply dividing by the window length and letting your convolution library zero-pad the data can introduce artifacts, and how renormalizing the average at the boundaries preserves the signal's local mean.",
            "id": "4153595",
            "problem": "You are given a finite discrete-time signal and asked to implement a smoothing operation that is suitable for neuroscience time series, such as neuronal fluorescence traces or instantaneous spike-rate estimates. Consider a real-valued discrete-time signal $x[n]$ of length $N$, indexed by $n \\in \\{0,1,\\dots,N-1\\}$. A moving average smoothing with half-width $w \\in \\mathbb{Z}_{\\ge 0}$ uses a symmetric window of nominal length $2w+1$ centered at each sample. Near the boundaries ($n$ close to $0$ or $N-1$), this window extends beyond the available samples. You must implement a renormalized moving average that, for each index $n$, divides by the actual number of samples included in the truncated window rather than the nominal window length, and you must demonstrate that this normalization preserves unit gain for constant signals.\n\nStart from the foundational definitions of discrete-time signals and linear smoothing operators. Treat the smoothing process as an operation that replaces each $x[n]$ with a local mean computed over a symmetric neighborhood around $n$. Do not assume an infinite-length signal, and do not use any assumption that inserts artificial values beyond the boundaries. The algorithm must only use available data and must be renormalized at the edges by the number of truly included samples.\n\nProgram requirements:\n- Implement a function that takes $x[n]$ and an integer half-width $w$ and returns the renormalized moving average output $y[n]$ for all $n \\in \\{0,1,\\dots,N-1\\}$. The implementation must be correct for all $w \\ge 0$, including cases where $2w+1 > N$.\n- Implement, for comparison, a naive zero-padded moving average that uses a uniform window of length $2w+1$, divides by $2w+1$, and implicitly treats out-of-bounds samples as zeros via standard discrete convolution with a rectangular kernel.\n- Use these implementations to compute the quantities specified in the test suite below.\n- Demonstrate unit gain preservation for constant signals by quantifying the deviation of the renormalized moving average from the original constant value.\n\nTest suite:\n- Case $1$ (constant signal, interior-edge behavior): Let $N=100$, $w=5$, and $x[n]=3.7$ for all $n$. Compute the renormalized moving average output $y[n]$ and return the maximum absolute deviation $\\max_{0 \\le n \\le N-1} |y[n]-3.7|$ as a float.\n- Case $2$ (window longer than signal): Let $N=20$, $w=20$, and $x[n]=n$ for all $n$. Compute the renormalized moving average output $y[n]$ and return the maximum absolute deviation from the global mean $\\mu=\\frac{1}{N}\\sum_{n=0}^{N-1} x[n]$, i.e., $\\max_{0 \\le n \\le N-1} |y[n]-\\mu|$, as a float.\n- Case $3$ (identity boundary case): Let $N=15$, $w=0$, and $x[n]=(-1)^n$ for all $n$. Compute the renormalized moving average output $y[n]$ and return the maximum absolute deviation $\\max_{0 \\le n \\le N-1} |y[n]-x[n]|$ as a float.\n- Case $4$ (unit gain comparison, naive method): Let $N=50$, $w=3$, and $x[n]=5$ for all $n$. Compute the naive zero-padded moving average output $y_{\\text{naive}}[n]$. Return the gain at the first sample $g_{\\text{naive}} = \\frac{y_{\\text{naive}}[0]}{5}$ as a float.\n- Case $5$ (unit gain comparison, renormalized method): Using the same setup as Case $4$, compute the renormalized moving average output $y[n]$. Return the gain at the first sample $g_{\\text{renorm}} = \\frac{y[0]}{5}$ as a float.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where $r_1$ through $r_5$ correspond to the outputs for Cases $1$ through $5$ respectively, in the order listed above. All values must be printed as Python floats. No physical units are involved; treat all quantities as dimensionless.",
            "solution": "The problem requires the implementation and analysis of two types of moving average filters applied to a finite discrete-time signal, with a focus on their behavior at the signal boundaries. This analysis is especially pertinent in fields like neuroscience, where preserving the baseline of physiological signals is critical.\n\nA finite, real-valued, discrete-time signal is a sequence of numbers $x[n]$ of length $N$, where the index $n$ belongs to the set $\\{0, 1, \\dots, N-1\\}$. Smoothing this signal involves replacing each sample $x[n]$ with an average of its neighbors.\n\nFirst, we define the **renormalized moving average**. This method is designed to maintain unit gain for constant signals, also known as preserving the DC component, across the entire signal, including its boundaries. For a given half-width $w \\in \\mathbb{Z}_{\\ge 0}$, the filter considers a symmetric window of nominal length $2w+1$ centered at each index $n$. The output signal $y[n]$ is computed as follows:\n\nFor each sample index $n \\in \\{0, 1, \\dots, N-1\\}$, the set of indices to be averaged is determined by the intersection of the window $[n-w, n+w]$ and the valid signal domain $[0, N-1]$. The start and end indices of this effective window are:\n$$k_{start}(n) = \\max(0, n-w)$$\n$$k_{end}(n) = \\min(N-1, n+w)$$\n\nThe non-normalized sum of the signal values within this window is:\n$$S[n] = \\sum_{k=k_{start}(n)}^{k_{end}(n)} x[k]$$\n\nThe crucial step in renormalization is to divide this sum by the actual number of samples included in the sum, not by the nominal window length $2w+1$. The number of samples in the effective window is:\n$$C[n] = k_{end}(n) - k_{start}(n) + 1$$\n\nThe renormalized moving average output $y[n]$ is then given by:\n$$y[n] = \\frac{S[n]}{C[n]} = \\frac{\\sum_{k=\\max(0, n-w)}^{\\min(N-1, n+w)} x[k]}{\\min(N-1, n+w) - \\max(0, n-w) + 1}$$\n\nThis normalization scheme guarantees unit gain for a constant signal. To demonstrate this, let $x[n] = c$ for all $n$, where $c$ is a constant. The sum $S[n]$ becomes:\n$$S[n] = \\sum_{k=k_{start}(n)}^{k_{end}(n)} c = c \\cdot (k_{end}(n) - k_{start}(n) + 1) = c \\cdot C[n]$$\nTherefore, the output is:\n$$y[n] = \\frac{c \\cdot C[n]}{C[n]} = c$$\nThis shows that $y[n] = x[n]$ for all $n$, meaning the constant signal passes through the filter unchanged. This property is vital for avoiding artificial attenuation at the edges of a dataset. An efficient algorithm to compute $S[n]$ for all $n$ involves pre-calculating the cumulative sum of $x[n]$, which allows each $S[n]$ to be computed in $O(1)$ time, leading to an overall $O(N)$ algorithm.\n\nSecond, we consider the **naive zero-padded moving average**. This approach reflects the behavior of standard discrete convolution with zero-padding. A filter kernel (or impulse response) $h[k]$ is defined as a normalized rectangular window:\n$$\nh[k] = \\begin{cases}\n\\frac{1}{2w+1} & \\text{for } -w \\le k \\le w \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThe output $y_{\\text{naive}}[n]$ is the discrete convolution of the signal $x[n]$ with the kernel $h[k]$, formally written as $(x*h)[n]$. For a finite signal, this operation is often implemented by extending the signal with zeros beyond its boundaries. The output is:\n$$y_{\\text{naive}}[n] = \\sum_{k=-w}^{w} x_{\\text{padded}}[n-k] \\cdot \\frac{1}{2w+1}$$\nwhere $x_{\\text{padded}}[m] = x[m]$ if $0 \\le m \\le N-1$ and $x_{\\text{padded}}[m] = 0$ otherwise. Unlike the renormalized filter, the divisor is always the constant $2w+1$.\n\nThis constancy of the divisor leads to gain attenuation at the boundaries. Consider a constant signal $x[n]=c>0$. At the left boundary, for $n=0$, the sum includes terms $x_{\\text{padded}}[-k]$ for $k \\in [-w, w]$. The non-zero terms are for $k \\in [0, w]$, a total of $w+1$ terms.\n$$y_{\\text{naive}}[0] = \\frac{1}{2w+1} \\sum_{k=0}^{w} x[k] = \\frac{1}{2w+1} \\sum_{k=0}^{w} c = \\frac{c(w+1)}{2w+1}$$\nThe gain at $n=0$ is the ratio of the output to the input, $g_{\\text{naive}} = \\frac{y_{\\text{naive}}[0]}{c}$:\n$$g_{\\text{naive}} = \\frac{w+1}{2w+1}$$\nFor any $w > 0$, this gain is less than $1$, demonstrating the attenuation effect. In contrast, the renormalized filter yields a gain of $1$ at all points.\n\nThe provided test suite cases are designed to verify these exact properties:\n- Case $1$: Confirms the unit gain property of the renormalized filter for a constant signal. The deviation should be zero, subject to floating-point precision.\n- Case $2$: Explores the behavior when the window width exceeds the signal length. The renormalized filter should compute the global mean of the signal for every output sample.\n- Case $3$: The case $w=0$ corresponds to a window of length $1$, which should result in an identity operation ($y[n]=x[n]$).\n- Cases $4$ and $5$: Directly compare the gain of the naive and renormalized filters at the signal boundary, highlighting the attenuation of the former and the unit gain of the latter.\n\nThe implementation will follow these principles, using a cumulative sum approach for the renormalized filter and library convolution for the naive filter.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef renormalized_moving_average(x: np.ndarray, w: int) -> np.ndarray:\n    \"\"\"\n    Computes the renormalized moving average of a signal.\n    \n    For each sample, the output is the average of the valid signal values\n    within a symmetric window of half-width w. The divisor is the number\n    of valid samples, which may be less than 2w+1 at the boundaries.\n    \n    Args:\n        x: The 1D input signal as a numpy array.\n        w: The integer half-width of the moving average window.\n        \n    Returns:\n        The smoothed signal as a numpy array.\n    \"\"\"\n    N = len(x)\n    if w == 0:\n        return x.copy()\n\n    # Use cumulative sum for an efficient O(N) implementation.\n    # The cumulative sum X[k] = sum(x[0]...x[k-1]).\n    # We pad with a 0 at the beginning to handle sums starting from index 0.\n    # Sum over x[a..b] is X[b+1] - X[a].\n    x_cumsum = np.concatenate(([0], np.cumsum(x)))\n    \n    y = np.zeros(N)\n    \n    for n in range(N):\n        k_start = max(0, n - w)\n        k_end = min(N - 1, n + w)\n        \n        # Calculate sum from pre-computed cumulative sum\n        current_sum = x_cumsum[k_end + 1] - x_cumsum[k_start]\n        \n        # Calculate the number of points in the truncated window\n        count = k_end - k_start + 1\n        \n        y[n] = current_sum / count\n        \n    return y\n\ndef naive_zero_padded_moving_average(x: np.ndarray, w: int) -> np.ndarray:\n    \"\"\"\n    Computes a naive zero-padded moving average using standard convolution.\n    \n    The output is computed by convolving the signal with a rectangular\n    kernel of length 2w+1, normalized by 2w+1. The 'same' mode implicitly\n    pads the signal with zeros.\n    \n    Args:\n        x: The 1D input signal as a numpy array.\n        w: The integer half-width of the moving average window.\n        \n    Returns:\n        The smoothed signal as a numpy array.\n    \"\"\"\n    if w == 0:\n        return x.copy()\n\n    window_len = 2 * w + 1\n    kernel = np.ones(window_len) / window_len\n    \n    # 'same' mode ensures output is same size as input, and handles\n    # boundaries by assuming the signal is zero-padded.\n    return np.convolve(x, kernel, mode='same')\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the final results.\n    \"\"\"\n    test_cases = [\n        # Case 1: constant signal, interior-edge behavior\n        {'N': 100, 'w': 5, 'x_func': lambda N: np.full(N, 3.7), 'id': 'case1'},\n        # Case 2: window longer than signal\n        {'N': 20, 'w': 20, 'x_func': lambda N: np.arange(N, dtype=float), 'id': 'case2'},\n        # Case 3: identity boundary case\n        {'N': 15, 'w': 0, 'x_func': lambda N: (-1.0)**np.arange(N), 'id': 'case3'},\n        # Case 4: unit gain comparison, naive method\n        {'N': 50, 'w': 3, 'x_func': lambda N: np.full(N, 5.0), 'id': 'case4'},\n        # Case 5: unit gain comparison, renormalized method\n        {'N': 50, 'w': 3, 'x_func': lambda N: np.full(N, 5.0), 'id': 'case5'},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case['N']\n        w = case['w']\n        x = case['x_func'](N)\n        \n        if case['id'] == 'case1':\n            y = renormalized_moving_average(x, w)\n            result = np.max(np.abs(y - 3.7))\n            results.append(result)\n            \n        elif case['id'] == 'case2':\n            y = renormalized_moving_average(x, w)\n            mu = np.mean(x)\n            result = np.max(np.abs(y - mu))\n            results.append(result)\n\n        elif case['id'] == 'case3':\n            y = renormalized_moving_average(x, w)\n            result = np.max(np.abs(y - x))\n            results.append(result)\n\n        elif case['id'] == 'case4':\n            y_naive = naive_zero_padded_moving_average(x, w)\n            # Gain at the first sample\n            g_naive = y_naive[0] / 5.0\n            results.append(g_naive)\n            \n        elif case['id'] == 'case5':\n            y_renorm = renormalized_moving_average(x, w)\n            # Gain at the first sample\n            g_renorm = y_renorm[0] / 5.0\n            results.append(g_renorm)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}