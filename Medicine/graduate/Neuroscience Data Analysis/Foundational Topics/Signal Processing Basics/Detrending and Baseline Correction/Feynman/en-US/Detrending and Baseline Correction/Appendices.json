{
    "hands_on_practices": [
        {
            "introduction": "Baseline correction is a ubiquitous step in analyzing event-related data, intended to remove pre-stimulus offsets. However, this seemingly simple procedure carries a critical assumption that, when violated, can introduce serious artifacts. This exercise provides a first-principles derivation to formalize how baseline correction systematically propagates any true pre-stimulus activity into the post-stimulus window, creating a bias that can be mistaken for a genuine experimental effect . Mastering this concept is fundamental to designing clean experiments and correctly interpreting event-related neural responses.",
            "id": "4155672",
            "problem": "Consider a typical event-related analysis in Electroencephalography (EEG) where for each experimental condition $c \\in \\{1,2\\}$ and trial $i \\in \\{1,\\dots,N_c\\}$ an epoch $s_{c,i}(t)$ is extracted around stimulus onset $t=0$ with a pre-stimulus interval $t \\in [-T_{pre},0)$ and a post-stimulus interval $t \\in [0,T_{post}]$. Assume the following generative model that captures ongoing activity and stimulus-evoked changes in a piecewise-constant mean structure with additive noise:\n$$\ns_{c,i}(t) \\;=\\; \\mu_{c}(t) \\;+\\; \\eta_{c,i}(t),\n$$\nwhere $\\mu_{c}(t)$ is condition-specific and piecewise constant,\n$$\n\\mu_{c}(t) \\;=\\; \\begin{cases}\n\\mu_{pre,c}, & t \\in [-T_{pre},0),\\\\\n\\mu_{post,c}, & t \\in [0,T_{post}],\n\\end{cases}\n$$\nand $\\eta_{c,i}(t)$ is a zero-mean stochastic process modeling noise and background activity with $\\mathbb{E}[\\eta_{c,i}(t)]=0$ for all $t$, $i$, and $c$. Let the standard baseline correction procedure be applied per trial by subtracting the pre-stimulus sample mean from the entire epoch. Define the per-trial pre-stimulus and post-stimulus sample means on the raw (uncorrected) signal as\n$$\n\\hat{\\mu}_{pre,c,i} \\;=\\; \\frac{1}{T_{pre}} \\int_{-T_{pre}}^{0} s_{c,i}(t)\\,dt, \\qquad\n\\hat{\\mu}_{post,c,i} \\;=\\; \\frac{1}{T_{post}} \\int_{0}^{T_{post}} s_{c,i}(t)\\,dt,\n$$\nand the baseline-corrected amplitude estimator for condition $c$ on trial $i$ as\n$$\n\\hat{A}_{c,i} \\;=\\; \\hat{\\mu}_{post,c,i} \\;-\\; \\hat{\\mu}_{pre,c,i}.\n$$\nThe condition-level amplitude estimator is the average over trials,\n$$\n\\hat{A}_{c} \\;=\\; \\frac{1}{N_c} \\sum_{i=1}^{N_c} \\hat{A}_{c,i}.\n$$\nYou are to derive, from first principles of expectation and the linearity of averaging, how differences in pre-stimulus activity across conditions impact the expected amplitude after baseline correction, and to formalize the expected bias in the estimator $\\hat{\\mu}_{post} - \\hat{\\mu}_{pre}$ at the condition level relative to the true post-stimulus mean $\\mu_{post,c}$. Specifically, under the model and assumptions stated above, derive an analytic expression for the expected bias\n$$\nb_c \\;=\\; \\mathbb{E}\\!\\left[\\hat{\\mu}_{post,c} \\;-\\; \\hat{\\mu}_{pre,c}\\right] \\;-\\; \\mu_{post,c}.\n$$\nGive your final expression as a closed-form analytic expression in terms of condition-level quantities. If you were to evaluate any numerical instance of this bias, express it in microvolts. No rounding is required for the final analytic expression.",
            "solution": "The user wants to validate and solve the following problem.\n\n### Step 1: Extract Givens\n- **Model for EEG signal epoch:** $s_{c,i}(t) \\;=\\; \\mu_{c}(t) \\;+\\; \\eta_{c,i}(t)$ for condition $c \\in \\{1,2\\}$, trial $i \\in \\{1,\\dots,N_c\\}$, and time $t \\in [-T_{pre}, T_{post}]$.\n- **Piecewise constant mean:** $\\mu_{c}(t) \\;=\\; \\begin{cases} \\mu_{pre,c}, & t \\in [-T_{pre},0) \\\\ \\mu_{post,c}, & t \\in [0,T_{post}] \\end{cases}$\n- **Noise process:** $\\eta_{c,i}(t)$ is a stochastic process with zero mean, $\\mathbb{E}[\\eta_{c,i}(t)]=0$.\n- **Per-trial pre-stimulus sample mean:** $\\hat{\\mu}_{pre,c,i} \\;=\\; \\frac{1}{T_{pre}} \\int_{-T_{pre}}^{0} s_{c,i}(t)\\,dt$.\n- **Per-trial post-stimulus sample mean:** $\\hat{\\mu}_{post,c,i} \\;=\\; \\frac{1}{T_{post}} \\int_{0}^{T_{post}} s_{c,i}(t)\\,dt$.\n- **Per-trial baseline-corrected amplitude estimator:** $\\hat{A}_{c,i} \\;=\\; \\hat{\\mu}_{post,c,i} \\;-\\; \\hat{\\mu}_{pre,c,i}$.\n- **Condition-level amplitude estimator:** $\\hat{A}_{c} \\;=\\; \\frac{1}{N_c} \\sum_{i=1}^{N_c} \\hat{A}_{c,i}$.\n- **Alternative notation for condition-level estimators:** The problem uses $\\hat{\\mu}_{post,c}$ and $\\hat{\\mu}_{pre,c}$ to denote the condition-level averages, such that $\\hat{A}_c = \\hat{\\mu}_{post,c} - \\hat{\\mu}_{pre,c}$.\n- **Quantity to be derived:** The expected bias $b_c \\;=\\; \\mathbb{E}\\!\\left[\\hat{\\mu}_{post,c} \\;-\\; \\hat{\\mu}_{pre,c}\\right] \\;-\\; \\mu_{post,c}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding:** The problem describes a simplified but standard generative model for event-related potentials (ERPs) in EEG analysis. The concepts of baseline correction, averaging, and expectation are fundamental to signal processing and statistics. The problem is scientifically and mathematically sound.\n2.  **Well-Posedness:** The problem is well-posed. All variables and quantities are clearly defined. The use of expectation on the defined estimators within the given model leads to a unique, derivable solution.\n3.  **Objectivity:** The problem statement is objective and uses precise mathematical and statistical language. There are no subjective or opinion-based claims.\n4.  **Completeness:** The problem is self-contained. All necessary equations, definitions, and assumptions (like the zero-mean noise) are provided to perform the derivation.\n5.  **Consistency and Feasibility:** The definitions and model are internally consistent. The model, while a simplification of real neurophysiological signals, is a standard tool for theoretical analysis and is not infeasible or contradictory.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe objective is to derive an analytical expression for the expected bias, $b_c$, defined as:\n$$\nb_c \\;=\\; \\mathbb{E}\\!\\left[\\hat{\\mu}_{post,c} \\;-\\; \\hat{\\mu}_{pre,c}\\right] \\;-\\; \\mu_{post,c}\n$$\nThe term $\\hat{\\mu}_{post,c} - \\hat{\\mu}_{pre,c}$ is the condition-level baseline-corrected amplitude estimator, which we have denoted as $\\hat{A}_c$. So, we are solving for:\n$$\nb_c \\;=\\; \\mathbb{E}[\\hat{A}_c] \\;-\\; \\mu_{post,c}\n$$\nThe first step is to calculate the expected value of the estimator $\\hat{A}_c$. The estimator $\\hat{A}_c$ is defined as the average of the trial-level estimators $\\hat{A}_{c,i}$:\n$$\n\\hat{A}_c \\;=\\; \\frac{1}{N_c} \\sum_{i=1}^{N_c} \\hat{A}_{c,i} \\;=\\; \\frac{1}{N_c} \\sum_{i=1}^{N_c} (\\hat{\\mu}_{post,c,i} \\;-\\; \\hat{\\mu}_{pre,c,i})\n$$\nBy linearity of the expectation operator, the expectation of the average is the average of the expectations.\n$$\n\\mathbb{E}[\\hat{A}_c] \\;=\\; \\mathbb{E}\\left[\\frac{1}{N_c} \\sum_{i=1}^{N_c} \\hat{A}_{c,i}\\right] \\;=\\; \\frac{1}{N_c} \\sum_{i=1}^{N_c} \\mathbb{E}[\\hat{A}_{c,i}]\n$$\nSince the generative model is identical for all trials $i$ within a condition $c$, the expectation $\\mathbb{E}[\\hat{A}_{c,i}]$ is the same for all $i$. Therefore, the sum becomes:\n$$\n\\mathbb{E}[\\hat{A}_c] \\;=\\; \\frac{1}{N_c} (N_c \\cdot \\mathbb{E}[\\hat{A}_{c,1}]) \\;=\\; \\mathbb{E}[\\hat{A}_{c,1}]\n$$\nwhere we use trial $i=1$ as a representative. Again, using linearity of expectation:\n$$\n\\mathbb{E}[\\hat{A}_c] \\;=\\; \\mathbb{E}[\\hat{\\mu}_{post,c,1} \\;-\\; \\hat{\\mu}_{pre,c,1}] \\;=\\; \\mathbb{E}[\\hat{\\mu}_{post,c,1}] \\;-\\; \\mathbb{E}[\\hat{\\mu}_{pre,c,1}]\n$$\nWe now compute the expected value for each sample mean estimator separately. Let's start with the pre-stimulus sample mean, $\\hat{\\mu}_{pre,c,i}$:\n$$\n\\mathbb{E}[\\hat{\\mu}_{pre,c,i}] \\;=\\; \\mathbb{E}\\left[\\frac{1}{T_{pre}} \\int_{-T_{pre}}^{0} s_{c,i}(t)\\,dt\\right]\n$$\nWe can swap the expectation and integral operators (by the Fubini-Tonelli theorem) and substitute the generative model $s_{c,i}(t) = \\mu_c(t) + \\eta_{c,i}(t)$:\n$$\n\\mathbb{E}[\\hat{\\mu}_{pre,c,i}] \\;=\\; \\frac{1}{T_{pre}} \\int_{-T_{pre}}^{0} \\mathbb{E}[s_{c,i}(t)]\\,dt \\;=\\; \\frac{1}{T_{pre}} \\int_{-T_{pre}}^{0} \\mathbb{E}[\\mu_c(t) + \\eta_{c,i}(t)]\\,dt\n$$\nThe mean $\\mu_c(t)$ is deterministic, and the noise is zero-mean, $\\mathbb{E}[\\eta_{c,i}(t)]=0$. Thus, $\\mathbb{E}[s_{c,i}(t)] = \\mu_c(t)$. For the pre-stimulus interval $t \\in [-T_{pre}, 0)$, we are given $\\mu_c(t) = \\mu_{pre,c}$.\n$$\n\\mathbb{E}[\\hat{\\mu}_{pre,c,i}] \\;=\\; \\frac{1}{T_{pre}} \\int_{-T_{pre}}^{0} \\mu_{pre,c}\\,dt \\;=\\; \\frac{\\mu_{pre,c}}{T_{pre}} [t]_{-T_{pre}}^{0} \\;=\\; \\frac{\\mu_{pre,c}}{T_{pre}}(0 - (-T_{pre})) \\;=\\; \\mu_{pre,c}\n$$\nThe pre-stimulus sample mean is an unbiased estimator of the true pre-stimulus mean.\n\nNext, we follow the same procedure for the post-stimulus sample mean, $\\hat{\\mu}_{post,c,i}$:\n$$\n\\mathbb{E}[\\hat{\\mu}_{post,c,i}] \\;=\\; \\mathbb{E}\\left[\\frac{1}{T_{post}} \\int_{0}^{T_{post}} s_{c,i}(t)\\,dt\\right] \\;=\\; \\frac{1}{T_{post}} \\int_{0}^{T_{post}} \\mathbb{E}[s_{c,i}(t)]\\,dt\n$$\nIn the post-stimulus interval $t \\in [0, T_{post}]$, we are given $\\mu_c(t) = \\mu_{post,c}$.\n$$\n\\mathbb{E}[\\hat{\\mu}_{post,c,i}] \\;=\\; \\frac{1}{T_{post}} \\int_{0}^{T_{post}} \\mu_{post,c}\\,dt \\;=\\; \\frac{\\mu_{post,c}}{T_{post}} [t]_{0}^{T_{post}} \\;=\\; \\frac{\\mu_{post,c}}{T_{post}}(T_{post} - 0) \\;=\\; \\mu_{post,c}\n$$\nThe post-stimulus sample mean is an unbiased estimator of the true post-stimulus mean.\n\nNow, we substitute these expected values back into the expression for $\\mathbb{E}[\\hat{A}_c]$:\n$$\n\\mathbb{E}[\\hat{A}_c] \\;=\\; \\mathbb{E}[\\hat{\\mu}_{post,c,i}] \\;-\\; \\mathbb{E}[\\hat{\\mu}_{pre,c,i}] \\;=\\; \\mu_{post,c} \\;-\\; \\mu_{pre,c}\n$$\nThis result shows that the baseline-corrected amplitude estimator $\\hat{A}_c$ is an unbiased estimator of the *true change in mean amplitude*, $\\mu_{post,c} - \\mu_{pre,c}$.\n\nFinally, we calculate the bias $b_c$ as defined in the problem. The problem specifies the bias of the estimator $\\hat{A}_c$ *relative to the true post-stimulus mean $\\mu_{post,c}$*. This is a non-standard but well-defined quantity that highlights a key property of baseline correction.\n$$\nb_c \\;=\\; \\mathbb{E}[\\hat{A}_c] \\;-\\; \\mu_{post,c}\n$$\nSubstituting our result for $\\mathbb{E}[\\hat{A}_c]$:\n$$\nb_c \\;=\\; (\\mu_{post,c} \\;-\\; \\mu_{pre,c}) \\;-\\; \\mu_{post,c} \\;=\\; -\\mu_{pre,c}\n$$\nThe derivation shows that the expected bias of the baseline-corrected amplitude $\\hat{A}_c$, when used as an estimator for the post-stimulus mean activity $\\mu_{post,c}$, is equal to the negative of the true pre-stimulus mean activity $\\mu_{pre,c}$. This formalizes the crucial insight that any pre-stimulus activity level directly biases the post-stimulus estimate after baseline correction. If there are differences in pre-stimulus activity across experimental conditions (i.e., $\\mu_{pre,1} \\neq \\mu_{pre,2}$), baseline correction will introduce a artifactual difference into the post-stimulus amplitudes, confounding the interpretation of the results. The final derived expression is a closed-form analytic expression in terms of a condition-level quantity, as requested.",
            "answer": "$$\\boxed{-\\mu_{pre,c}}$$"
        },
        {
            "introduction": "While baseline correction handles simple offsets, neuroscience data often contains slower, more complex drifts that require flexible detrending methods like polynomial regression. This practice addresses the significant risk of overfitting, where the detrending model is too flexible and removes not only the noise but also the low-frequency signal of interest. Through a hands-on coding challenge, you will simulate this scenario and implement a suite of powerful diagnostics—including residual spectral analysis, leverage, and the nested-model $F$-test—to detect when your detrending method has gone too far . This provides you with a practical toolkit for validating your analysis pipeline.",
            "id": "4155651",
            "problem": "You are given a time series analysis task in the context of neuroscience data analysis focused on detrending and baseline correction. The objective is to construct a scientifically realistic scenario in which polynomial detrending of degree $3$ removes part of a slow task-related oscillatory component (overfitting) and to implement diagnostics to detect this. Assume a one-dimensional time series $y[n]$ sampled at frequency $f_s$ in hertz over $N$ samples, with time $t_n$ in seconds defined by $t_n = n / f_s$ for $n \\in \\{0,1,\\dots,N-1\\}$. The signal is modeled as the sum of a baseline drift (a low-order polynomial in $t_n$), a slow task-related oscillation, and additive independent zero-mean Gaussian noise. All amplitudes are dimensionless. Angles used in trigonometric functions are in radians. No physical units besides seconds and hertz are involved, and all frequencies must be treated in hertz.\n\nStarting from the core definitions below, you must implement a complete program that performs polynomial detrending of degree $3$ (cubic) using ordinary least squares and then computes diagnostics that can indicate overfitting when the detrending removes non-baseline task-related content.\n\nFundamental bases to use:\n\n- Ordinary least squares for linear models with design matrix columns formed by powers of centered and scaled time, where a degree-$p$ polynomial model has $p+1$ parameters and minimizes the sum of squared residuals.\n- The Discrete Fourier Transform (DFT) to define the residual spectrum and the periodogram as squared magnitudes of DFT coefficients at frequencies $k f_s / N$, for integer $k$.\n- The hat matrix definition for linear models and its diagonal entries (leverage), where leverage quantifies the influence of each sample on the fit.\n- The nested-model $F$-test comparing a lower-degree polynomial model to a higher-degree model.\n\nYour program must:\n\n1. Construct the cubic polynomial design matrix with columns $[1, \\tilde{t}, \\tilde{t}^2, \\tilde{t}^3]$, where $\\tilde{t}$ is the centered-and-scaled time defined by $\\tilde{t}_n = (t_n - \\bar{t}) / (t_{\\max} - t_{\\min})$, with $\\bar{t}$ the mean time, $t_{\\max}$ the maximum time, and $t_{\\min}$ the minimum time. Also construct the linear model design matrix with columns $[1, \\tilde{t}]$.\n2. Fit both polynomial models by ordinary least squares to the observed $y[n]$ and compute residuals.\n3. Compute the residual spectrum diagnostic by:\n   - Performing the Discrete Fourier Transform on the original signal and on the cubic-detrended residual using the real-valued DFT over nonnegative frequencies.\n   - Identifying the nearest DFT bin to the known task frequency $f_{\\text{task}}$ and computing the periodogram power at that bin both before detrending and after cubic detrending.\n   - Forming the suppression ratio $r = P_{\\text{res}} / P_{\\text{orig}}$, where $P_{\\text{res}}$ is the residual power and $P_{\\text{orig}}$ is the original power at the nearest bin to $f_{\\text{task}}$; to avoid division by an extremely small value, when $P_{\\text{orig}}$ is less than a small constant $\\epsilon$, set $r = 1$.\n4. Compute the leverage diagnostic by:\n   - Forming the hat matrix $H = X (X^\\top X)^{-1} X^\\top$ for the cubic model, where $X$ is the cubic design matrix.\n   - Extracting the diagonal entries $h_{ii}$ and computing $h_{\\max} = \\max_i h_{ii}$.\n5. Compute the nested-model $F$-test diagnostic by:\n   - Using the linear model as the reduced model with $p_1 = 2$ parameters and the cubic model as the full model with $p_3 = 4$ parameters.\n   - Computing residual sums of squares $\\mathrm{RSS}_1$ and $\\mathrm{RSS}_3$.\n   - Computing the $F$-statistic with numerator degrees $m = p_3 - p_1 = 2$ and denominator degrees $N - p_3$.\n   - Computing the associated tail probability (p-value) for the $F$-distribution.\n6. Decide whether overfitting is detected by requiring that at least two of the three diagnostics indicate overfitting using the following thresholds:\n   - Residual spectrum suppression: $r < 0.3$.\n   - High leverage: $h_{\\max} > 3 p_3 / N$, where $p_3 = 4$.\n   - Significant $F$-test: $p\\text{-value} < 0.01$.\n   The final decision is a boolean indicating whether at least two of these conditions are satisfied.\n\nTest suite specification:\n\nImplement and run your program on the following test cases. In all cases, use sampling frequency $f_s = 1$ hertz, and set the noise standard deviation to the given value. The oscillatory component is $A \\sin(2 \\pi f_{\\text{task}} t_n)$, with amplitude $A$ and frequency $f_{\\text{task}}$. The baseline drift polynomial is $b(t_n) = c_0 + c_1 t_n + c_2 t_n^2 + c_3 t_n^3$.\n\n- Test case $1$ (expected to be a general case where overfitting is present): $N = 600$, $A = 2.0$, $f_{\\text{task}} = 1 / 600$, $(c_0, c_1, c_2, c_3) = (0.0, 0.0, 0.0, 0.0)$, noise standard deviation $= 0.05$.\n- Test case $2$ (boundary case with no task oscillation): $N = 600$, $A = 0.0$, $f_{\\text{task}} = 1 / 600$, $(c_0, c_1, c_2, c_3) = (0.0, 0.01, 0.0, 0.0)$, noise standard deviation $= 0.05$.\n- Test case $3$ (edge case with short duration and half-cycle oscillation): $N = 80$, $A = 1.5$, $f_{\\text{task}} = 1 / 160$, $(c_0, c_1, c_2, c_3) = (0.0, 0.0, 0.0, 0.0)$, noise standard deviation $= 0.10$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result must be a boolean indicating overfitting detected ($\\mathrm{True}$) or not ($\\mathrm{False}$) for the corresponding test case, evaluated by the decision rule in item $6$ above. No additional text should be printed. Use a fixed random seed for the noise to ensure reproducibility.\n\nAll mathematical entities must be written using LaTeX notation in this problem statement. The time unit is seconds and the frequency unit is hertz. No angle units other than radians are used, and no percentages are used; ratios such as $r$ must be treated as decimal numbers.",
            "solution": "We begin from the foundational definitions of ordinary least squares, the Discrete Fourier Transform, the hat matrix and leverage, and the nested-model $F$-test. We will integrate these into an algorithmic pipeline to simulate data, detrend by a cubic polynomial, and compute diagnostics to detect overfitting.\n\nSignal model and sampling: The observed signal is $y[n] = b(t_n) + s(t_n) + \\eta[n]$, where $b(t)$ is a baseline polynomial drift, $s(t)$ is a slow task-related oscillation, and $\\eta[n]$ is additive independent Gaussian noise with zero mean and specified standard deviation. We have $t_n = n / f_s$ for $n \\in \\{0,1,\\dots,N-1\\}$ and $f_s = 1$ hertz, so $t_n$ is in seconds. The oscillatory component is $s(t_n) = A \\sin(2 \\pi f_{\\text{task}} t_n)$ with amplitude $A$ and frequency $f_{\\text{task}}$ in hertz. The baseline drift is a polynomial $b(t_n) = c_0 + c_1 t_n + c_2 t_n^2 + c_3 t_n^3$ in the test cases.\n\nPolynomial regression and detrending: For a polynomial of degree $p$, the linear model can be written as $y = X \\beta + \\varepsilon$, where $X$ is the design matrix, $\\beta$ is the parameter vector, and $\\varepsilon$ is noise. To promote numerical stability and interpretability, we form the time regressor by centering and scaling time:\n$$\n\\tilde{t}_n = \\frac{t_n - \\bar{t}}{t_{\\max} - t_{\\min}},\n$$\nwhere $\\bar{t}$ is the mean of the time samples, $t_{\\max}$ is the maximum time, and $t_{\\min}$ is the minimum time. For the degree-$3$ model, the design matrix is\n$$\nX_3 = \\begin{bmatrix}\n1 & \\tilde{t}_0 & \\tilde{t}_0^2 & \\tilde{t}_0^3 \\\\\n1 & \\tilde{t}_1 & \\tilde{t}_1^2 & \\tilde{t}_1^3 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & \\tilde{t}_{N-1} & \\tilde{t}_{N-1}^2 & \\tilde{t}_{N-1}^3\n\\end{bmatrix},\n$$\nand for the degree-$1$ model, the design matrix is\n$$\nX_1 = \\begin{bmatrix}\n1 & \\tilde{t}_0 \\\\\n1 & \\tilde{t}_1 \\\\\n\\vdots & \\vdots \\\\\n1 & \\tilde{t}_{N-1}\n\\end{bmatrix}.\n$$\nThe ordinary least squares estimate minimizes $\\sum_{n=0}^{N-1} (y[n] - (X \\beta)[n])^2$ and is given by\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y.\n$$\nThe residuals are $r = y - X \\hat{\\beta}$. Detrending consists of subtracting the fitted polynomial component $X_3 \\hat{\\beta}_3$ from $y$ to yield the residual signal $r_3$.\n\nResidual spectrum diagnostic: To assess whether the detrending has removed power at the task frequency, we compute the periodogram at the DFT bin nearest to $f_{\\text{task}}$. Define the real-valued Discrete Fourier Transform over nonnegative frequencies via\n$$\nY[k] = \\sum_{n=0}^{N-1} y[n] e^{-2 \\pi i n k / N}, \\quad k = 0,1,\\dots,\\left\\lfloor \\frac{N}{2} \\right\\rfloor,\n$$\nwith corresponding frequencies $\\omega_k = k f_s / N$. We identify the index $k^*$ that minimizes $| \\omega_k - f_{\\text{task}} |$. The periodogram power at $k^*$ is $P_{\\text{orig}} = |Y[k^*]|^2 / N$. For the residual after cubic detrending, compute its transform $R_3[k]$ and power $P_{\\text{res}} = |R_3[k^*]|^2 / N$. The suppression ratio is\n$$\nr = \\begin{cases}\n\\frac{P_{\\text{res}}}{P_{\\text{orig}}}, & P_{\\text{orig}} \\ge \\epsilon, \\\\\n1, & P_{\\text{orig}} < \\epsilon,\n\\end{cases}\n$$\nwhere $\\epsilon$ is a small positive constant to prevent division by a negligible denominator. Overfitting is indicated if $r < 0.3$, signifying that the cubic detrend has suppressed the task-related power substantially.\n\nLeverage diagnostic: The hat matrix for the cubic model is\n$$\nH_3 = X_3 (X_3^\\top X_3)^{-1} X_3^\\top,\n$$\nwhose diagonal entries $h_{ii}$ quantify the leverage of sample $i$. A property of the hat matrix is $\\mathrm{trace}(H_3) = p_3$, where $p_3 = 4$ is the number of parameters in the cubic model. The average leverage is $p_3 / N$. High leverage relative to the average suggests potential overfitting or instability. We compute $h_{\\max} = \\max_i h_{ii}$ and declare high leverage if\n$$\nh_{\\max} > 3 \\frac{p_3}{N}.\n$$\n\nNested-model $F$-test diagnostic: To compare the linear model (reduced) with parameter count $p_1 = 2$ to the cubic model (full) with parameter count $p_3 = 4$, compute residual sums of squares\n$$\n\\mathrm{RSS}_1 = \\| y - X_1 \\hat{\\beta}_1 \\|_2^2, \\quad \\mathrm{RSS}_3 = \\| y - X_3 \\hat{\\beta}_3 \\|_2^2.\n$$\nThe $F$-statistic is\n$$\nF = \\frac{(\\mathrm{RSS}_1 - \\mathrm{RSS}_3)/m}{\\mathrm{RSS}_3 / (N - p_3)},\n$$\nwhere $m = p_3 - p_1 = 2$. Under the null hypothesis that the additional cubic terms do not improve the fit beyond noise, $F$ follows the $F$-distribution with $m$ and $N - p_3$ degrees of freedom. Compute the tail probability ($p$-value) as $p = \\Pr(F_{m, N-p_3} \\ge F)$. We declare significance if\n$$\np < 0.01.\n$$\n\nDecision rule: We detect overfitting if at least two of the following are true:\n- $r < 0.3$ (residual spectrum suppression),\n- $h_{\\max} > 3 p_3 / N$ (high leverage),\n- $p < 0.01$ (significant $F$-test).\n\nRationale: If cubic detrending removes genuine task-related slow oscillatory content, the periodogram at the task frequency will show reduced power relative to the original signal. The leverage criterion guards against fits dominated by extreme samples, which is more likely in short or poorly conditioned designs. The $F$-test indicates that the additional polynomial degrees provide a statistically significant reduction in residual variance, which may be evidence of modeling content beyond baseline drift. Requiring at least two conditions reduces false positives from any single diagnostic.\n\nTest suite coverage: The specified test cases exercise multiple facets:\n- Test case $1$ simulates a slow oscillation with one cycle across the entire duration ($f_{\\text{task}} = 1/600$ hertz). A cubic polynomial can absorb nontrivial low-frequency structure, leading to suppression and significant model improvement, thus overfitting should be detected.\n- Test case $2$ has no oscillation ($A = 0$), so suppression at $f_{\\text{task}}$ is not expected; the leverage and $F$-test may not simultaneously cross thresholds, and overfitting should not be detected.\n- Test case $3$ has short duration $N = 80$ with a half-cycle oscillation ($f_{\\text{task}} = 1/160$ hertz). The cubic model may overfit due to higher leverage (larger $h_{\\max}$ compared to average) and suppression of low-frequency power, leading to detection.\n\nAlgorithmic implementation steps:\n- For each test case, simulate $y[n]$ given $N$, $f_s$, $A$, $f_{\\text{task}}$, baseline coefficients $(c_0, c_1, c_2, c_3)$, and noise standard deviation. Use a fixed random seed to ensure reproducibility.\n- Build $X_1$ and $X_3$ with centered-and-scaled $\\tilde{t}$.\n- Fit $\\hat{\\beta}_1$ and $\\hat{\\beta}_3$ by ordinary least squares and compute residuals.\n- Compute the real-valued DFT over nonnegative frequencies for original and residual signals, find $k^*$ corresponding to $f_{\\text{task}}$, and compute powers $P_{\\text{orig}}$ and $P_{\\text{res}}$ and the ratio $r$.\n- Compute $h_{ii}$ via $H_3$ and find $h_{\\max}$.\n- Compute the $F$-statistic and $p$-value using the $F$-distribution.\n- Apply the decision rule to produce a boolean for each case.\n\nThe final output will be a single line containing a list of booleans $[\\mathrm{result}_1, \\mathrm{result}_2, \\mathrm{result}_3]$, corresponding to the test cases in order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef build_time(N, Fs):\n    t = np.arange(N) / Fs\n    t_center = t.mean()\n    t_range = t.max() - t.min()\n    # Center and scale to stabilize polynomial columns\n    t_tilde = (t - t_center) / (t_range if t_range != 0 else 1.0)\n    return t, t_tilde\n\ndef design_matrix_poly(t_tilde, degree):\n    # Columns: [1, t, t^2, ..., t^degree]\n    cols = [np.ones_like(t_tilde)]\n    for d in range(1, degree + 1):\n        cols.append(t_tilde ** d)\n    X = np.column_stack(cols)\n    return X\n\ndef ols_fit_and_residual(y, X):\n    XtX = X.T @ X\n    XtX_inv = np.linalg.inv(XtX)\n    beta = XtX_inv @ (X.T @ y)\n    y_hat = X @ beta\n    residual = y - y_hat\n    return beta, residual, XtX_inv\n\ndef periodogram_power_at_freq(signal, Fs, f_task, eps=1e-12):\n    N = signal.size\n    # Real FFT non-negative frequencies\n    spec = np.fft.rfft(signal)\n    freqs = np.fft.rfftfreq(N, d=1.0/Fs)\n    # Nearest bin to task frequency\n    idx = int(np.argmin(np.abs(freqs - f_task)))\n    power = (np.abs(spec[idx]) ** 2) / N\n    return power, idx\n\ndef leverage_diag(X, XtX_inv):\n    # Hat matrix H = X (X^T X)^{-1} X^T\n    # Diagonal h_ii = row_i(X) * (X^T X)^{-1} * row_i(X)^T\n    # Efficient computation: For each row x_i, h_ii = x_i^T * XtX_inv * x_i\n    # Compute B = XtX_inv\n    B = XtX_inv\n    # h_ii for each row\n    h_diag = np.sum((X @ B) * X, axis=1)\n    return h_diag\n\ndef f_test_nested(y, X_reduced, X_full):\n    # Fit reduced and full models\n    _, res_reduced, _ = ols_fit_and_residual(y, X_reduced)\n    _, res_full, _ = ols_fit_and_residual(y, X_full)\n    RSS_reduced = float(res_reduced.T @ res_reduced)\n    RSS_full = float(res_full.T @ res_full)\n    p_reduced = X_reduced.shape[1]\n    p_full = X_full.shape[1]\n    N = y.size\n    m = p_full - p_reduced\n    df_den = N - p_full\n    # Guard against degenerate denominators\n    if df_den <= 0 or m <= 0:\n        return np.nan, 1.0\n    # F statistic\n    num = (RSS_reduced - RSS_full) / m\n    den = RSS_full / df_den\n    # If numerical issues produce negative num, clip at zero (no improvement)\n    if num < 0:\n        num = 0.0\n    F = num / den if den > 0 else np.inf\n    # p-value from F-distribution survival function\n    p_value = f_dist.sf(F, m, df_den) if np.isfinite(F) else 0.0\n    return F, p_value\n\ndef detect_overfitting(y, Fs, f_task, t_tilde, eps=1e-12):\n    # Build design matrices\n    X1 = design_matrix_poly(t_tilde, degree=1)\n    X3 = design_matrix_poly(t_tilde, degree=3)\n    # Fit models\n    _, res1, _ = ols_fit_and_residual(y, X1)\n    beta3, res3, XtX_inv3 = ols_fit_and_residual(y, X3)\n    # Residual spectrum suppression\n    P_orig, idx = periodogram_power_at_freq(y, Fs, f_task, eps=eps)\n    P_res, _ = periodogram_power_at_freq(res3, Fs, f_task, eps=eps)\n    r = P_res / P_orig if P_orig >= eps else 1.0\n    # Leverage\n    h_diag = leverage_diag(X3, XtX_inv3)\n    h_max = float(np.max(h_diag))\n    p3 = X3.shape[1]\n    N = y.size\n    leverage_threshold = 3.0 * p3 / N\n    high_leverage = h_max > leverage_threshold\n    # F-test\n    F_stat, p_value = f_test_nested(y, X1, X3)\n    # Decision: at least two diagnostics positive\n    suppression_positive = r < 0.3\n    ftest_positive = p_value < 0.01\n    positives = sum([suppression_positive, high_leverage, ftest_positive])\n    overfit = positives >= 2\n    return overfit, {\n        \"r\": r,\n        \"h_max\": h_max,\n        \"lev_thr\": leverage_threshold,\n        \"F\": F_stat,\n        \"p_value\": p_value\n    }\n\ndef simulate_signal(N, Fs, A_task, f_task, coeffs, noise_sigma, rng):\n    t, t_tilde = build_time(N, Fs)\n    c0, c1, c2, c3 = coeffs\n    baseline = c0 + c1 * t + c2 * (t ** 2) + c3 * (t ** 3)\n    oscillation = A_task * np.sin(2.0 * np.pi * f_task * t)\n    noise = rng.normal(loc=0.0, scale=noise_sigma, size=N)\n    y = baseline + oscillation + noise\n    return y, t_tilde\n\ndef solve():\n    # Fixed random seed for reproducibility\n    rng = np.random.default_rng(seed=123456)\n    # Define the test cases from the problem statement.\n    # Each case: (N, Fs, A_task, f_task, (c0,c1,c2,c3), noise_sigma)\n    test_cases = [\n        (600, 1.0, 2.0, 1.0/600.0, (0.0, 0.0, 0.0, 0.0), 0.05),   # Case 1: general overfitting\n        (600, 1.0, 0.0, 1.0/600.0, (0.0, 0.01, 0.0, 0.0), 0.05),  # Case 2: boundary, no oscillation\n        (80,  1.0, 1.5, 1.0/160.0, (0.0, 0.0, 0.0, 0.0), 0.10),   # Case 3: edge, short duration half-cycle\n    ]\n\n    results = []\n    for N, Fs, A_task, f_task, coeffs, noise_sigma in test_cases:\n        y, t_tilde = simulate_signal(N, Fs, A_task, f_task, coeffs, noise_sigma, rng)\n        overfit, _ = detect_overfitting(y, Fs, f_task, t_tilde, eps=1e-12)\n        results.append(overfit)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The previous exercises highlight the potential pitfalls of applying a given detrending method without validation. We now escalate to a more fundamental question: how do we choose the right approach in the first place? This practice delves into the crucial distinction between two primary forms of non-stationarity: trend-stationary processes, which are corrected by detrending, and difference-stationary (unit-root) processes, which are corrected by differencing. By implementing and interpreting a pair of complementary hypothesis tests (ADF and KPSS), you will learn the formal statistical framework for identifying the nature of non-stationarity in your data, enabling a principled choice of correction method .",
            "id": "4155611",
            "problem": "Consider a univariate time series that represents a noisy neurophysiological measurement with possible baseline drift. Let the latent autoregressive process of order one (AR(1)) be defined by $$z_t = \\phi z_{t-1} + \\varepsilon_t,$$ where $$\\{\\varepsilon_t\\}$$ is an independent and identically distributed sequence with $$\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$$ and $$z_0 = 0$$. The observed signal is given by a superposition of a deterministic linear trend and the latent process, $$y_t = \\beta t + z_t,$$ for $$t = 1,2,\\dots,T$$. The goal is to decide whether the observed nonstationarity is better explained by a deterministic trend (trend stationarity after baseline correction) or by a unit root (difference stationarity), and to justify the testing approach.\n\nStarting from the core definitions of stationarity for time series and the AR(1) model, design a method that:\n- Implements the Augmented Dickey-Fuller (ADF) test including a deterministic trend term to test the null hypothesis of a unit root in $$y_t$$.\n- Implements the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test for trend stationarity in $$y_t$$, based on residuals from ordinary least squares regression of $$y_t$$ on a constant and a linear time trend, using a Bartlett window estimator for the long-run variance with bandwidth $$\\ell = \\lfloor \\sqrt{T} \\rfloor$$.\n- Quantifies whether the linear trend is statistically significant by an ordinary least squares regression of $$y_t$$ on a constant and $$t$$, using a two-sided t-test at significance level $$\\alpha = 0.05$$ with Student’s t critical value computed for $$T-2$$ degrees of freedom.\n- Optionally validates that detrending resolves nonstationarity by running an ADF test without a trend term on the detrended residuals at significance level $$\\alpha = 0.05$$.\n\nUse the following well-tested decision logic grounded in complementary hypotheses:\n- If the ADF test with trend does not reject the unit root and the KPSS test rejects trend stationarity, classify the series as unit-root nonstationary.\n- If the ADF test with trend rejects the unit root, the KPSS test does not reject trend stationarity, and the trend slope is significant, classify the series as trend-stationary (i.e., nonstationarity primarily due to a deterministic trend that can be baseline corrected).\n- If neither a significant trend nor evidence of a unit root is present, classify the series as stationary.\n- Otherwise, return an inconclusive classification.\n\nImplement the tests with the following standard asymptotic critical values:\n- For the ADF test with trend at $$\\alpha = 0.05$$, reject the unit root if the t-statistic for the lagged level term is less than $$-3.41$$.\n- For the ADF test without trend (on detrended residuals) at $$\\alpha = 0.05$$, reject the unit root if the t-statistic for the lagged level term is less than $$-2.86$$.\n- For the KPSS test for trend stationarity at $$\\alpha = 0.05$$, reject trend stationarity if the statistic exceeds $$0.146$$.\n\nYour program must:\n1. Simulate the observed series $$\\{y_t\\}_{t=1}^T$$ for each parameter set in the test suite below using independent Gaussian noise with the specified $$\\sigma$$ and a fixed random seed for reproducibility.\n2. Apply the above method to classify each series according to the following integer codes:\n   - $$0$$: stationary,\n   - $$1$$: trend-stationary (deterministic trend present; baseline correction appropriate),\n   - $$2$$: unit-root nonstationary,\n   - $$3$$: inconclusive.\n3. Produce a single line of output containing the classification codes as a comma-separated list enclosed in square brackets, for example [$c_1, c_2, \\dots, c_n$], where each $c_i$ is one of $\\{0, 1, 2, 3\\}$ corresponding to the $i$-th test case.\n\nTest Suite (each tuple is [$T, \\phi, \\beta, \\sigma$, seed]):\n- Case $$1$$: $$[512, 1.0, 0.5, 1.0, 1]$$\n- Case $$2$$: $$[512, 0.7, 0.5, 1.0, 2]$$\n- Case $$3$$: $$[512, 1.0, 0.0, 1.0, 3]$$\n- Case $$4$$: $$[512, 0.0, 0.5, 1.0, 4]$$\n- Case $$5$$: $$[256, -0.8, 0.3, 1.0, 5]$$\n- Case $$6$$: $$[128, 0.7, 0.0, 1.0, 6]$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, [$c_1, c_2, c_3, c_4, c_5, c_6$]). No physical units are required. All angles, if any were to appear, must be in radians, but this problem does not require angles. Express any proportions as decimals if needed, but in this problem they are embedded in the calculations and do not appear in the final output.",
            "solution": "The problem requires designing a method to classify a univariate time series as stationary, trend-stationary, or unit-root nonstationary. This is a fundamental task in analyzing time series data, such as neurophysiological measurements, as the type of nonstationarity dictates the appropriate methods for detrending, statistical inference, and forecasting. The provided model for the observed signal is $y_t = \\beta t + z_t$, where $z_t$ is an autoregressive process of order one (AR($1$)) given by $z_t = \\phi z_{t-1} + \\varepsilon_t$. This structure allows for two primary forms of nonstationarity: a deterministic trend (if $\\beta \\neq 0$) and a stochastic trend or unit root (if $|\\phi| \\ge 1$).\n\nA process is called **trend-stationary** if it is composed of a deterministic trend and a stationary stochastic component. In this case, the series can be rendered stationary by regressing it on time and taking the residuals. The shocks, $\\varepsilon_t$, have a transient effect on the level of $y_t$.\nA process is called **difference-stationary** (or has a unit root) if it becomes stationary only after differencing (e.g., taking $\\Delta y_t = y_t - y_{t-1}$). In a difference-stationary process, such as a random walk ($\\phi=1$), shocks have a permanent effect on the level of the series.\n\nTo distinguish between these cases, we employ a standard framework of complementary hypothesis tests, augmented with a direct test for the significance of the deterministic trend.\n\n**1. Statistical Tests Implementation**\n\n**a) Trend Significance Test**\nTo quantify the presence of a deterministic linear trend, we perform an Ordinary Least Squares (OLS) regression of the observed series $y_t$ on a constant and a time index $t$:\n$$y_t = c + b t + v_t$$\nwhere $c$ is the intercept, $b$ is the trend coefficient, and $v_t$ is the residual term. A two-sided t-test is conducted on the estimated coefficient $\\hat{b}$. The null hypothesis is $H_0: b = 0$. The test statistic is $t_b = \\hat{b} / SE(\\hat{b})$, where $SE(\\hat{b})$ is the standard error of the estimate. If the absolute value of this statistic exceeds the critical value from a Student's t-distribution with $T-2$ degrees of freedom at a significance level $\\alpha=0.05$, we conclude that a statistically significant linear trend is present.\n\n**b) Augmented Dickey-Fuller (ADF) Test with Trend**\nThe ADF test is used to test for the presence of a unit root. The null hypothesis is $H_0: \\text{The series has a unit root}$ (i.e., it is difference-stationary). The alternative is $H_A: \\text{The series is stationary (or trend-stationary)}$. To account for a potential deterministic trend, the test regression is formulated as:\n$$\\Delta y_t = c_0 + c_1 t + \\gamma y_{t-1} + u_t$$\nThe test focuses on the coefficient $\\gamma$. The underlying AR($1$) process $y_t = \\rho y_{t-1} + \\dots$ can be rewritten as $\\Delta y_t = (\\rho-1)y_{t-1} + \\dots$, so $\\gamma = \\rho-1$. If $\\rho=1$ (a unit root), then $\\gamma=0$. The test is a one-sided t-test on the estimate $\\hat{\\gamma}$. A large negative value of the t-statistic for $\\hat{\\gamma}$ provides evidence against the null hypothesis. The problem specifies that $H_0$ is rejected if this t-statistic is less than the asymptotic critical value of $-3.41$. Since the data generating process has an AR($1$) structure for its stochastic component, no additional lagged difference terms ($\\Delta y_{t-k}$) are required in the test regression, meaning we implement the standard Dickey-Fuller test.\n\n**c) Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test**\nThe KPSS test has the complementary set of hypotheses to the ADF test. Its null hypothesis is $H_0: \\text{The series is stationary around a deterministic trend}$. The alternative is $H_A: \\text{The series has a unit root}$. The procedure is as follows:\n1.  Regress $y_t$ on a constant and a time trend $t$ to obtain the residuals, $\\hat{e}_t$.\n2.  Compute the partial sum process of the residuals: $S_t = \\sum_{i=1}^t \\hat{e}_i$.\n3.  Compute the test statistic:\n    $$LM = \\frac{1}{T^2 \\hat{\\sigma}_{LR}^2} \\sum_{t=1}^T S_t^2$$\n4.  $\\hat{\\sigma}_{LR}^2$ is a consistent estimator of the long-run variance of the residuals. It is estimated using a Bartlett window (Newey-West estimator) to account for potential serial correlation:\n    $$\\hat{\\sigma}_{LR}^2 = \\hat{\\gamma}_0 + 2 \\sum_{j=1}^{\\ell} \\left(1 - \\frac{j}{\\ell+1}\\right) \\hat{\\gamma}_j$$\n    where $\\hat{\\gamma}_j$ is the sample autocovariance of $\\hat{e}_t$ at lag $j$, and the bandwidth is set to $\\ell = \\lfloor \\sqrt{T} \\rfloor$.\nThe null hypothesis of trend stationarity is rejected if the $LM$ statistic exceeds the critical value of $0.146$ for $\\alpha=0.05$.\n\n**2. Decision Logic for Classification**\n\nBy combining the results of these three tests, we can classify the series with greater confidence:\n- **Unit-Root Nonstationary (Code 2):** If the ADF test fails to reject its null hypothesis of a unit root, and the KPSS test rejects its null hypothesis of trend stationarity, there is strong, consistent evidence that the series is difference-stationary.\n- **Trend-Stationary (Code 1):** If the ADF test rejects the unit root, the KPSS test does not reject trend stationarity, and the OLS t-test confirms a significant deterministic trend, the evidence points to a trend-stationary process. Detrending is the appropriate action.\n- **Stationary (Code 0):** If the ADF test rejects the unit root and the trend is found to be not significant, the series is classified as stationary around a constant mean. In this scenario, a non-rejection by the KPSS test is also expected.\n- **Inconclusive (Code 3):** Any other combination of outcomes is considered inconclusive. This frequently occurs when tests yield conflicting results (e.g., both reject their nulls) or when neither test provides a clear rejection, indicating that the data are not informative enough to distinguish between the two types of nonstationarity.\n\nThis systematic approach, grounded in established statistical theory, provides a robust method for characterizing the nature of nonstationarity in a time series.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as student_t\nfrom scipy.linalg import inv\n\ndef generate_series(T, phi, beta, sigma, seed):\n    \"\"\"\n    Generates a time series y_t = beta*t + z_t, where z_t is an AR(1) process.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    eps = rng.normal(loc=0.0, scale=sigma, size=T)\n    z = np.zeros(T + 1)  # z_0, z_1, ..., z_T\n    # z_0 is 0\n    for t_idx in range(T):\n        z[t_idx + 1] = phi * z[t_idx] + eps[t_idx]\n    \n    z_series = z[1:]  # z_1, ..., z_T\n    t_values = np.arange(1, T + 1)\n    y = beta * t_values + z_series\n    return y\n\ndef adf_test_with_trend(y):\n    \"\"\"\n    Performs an Augmented Dickey-Fuller test with a constant and a trend.\n    Returns True if the null hypothesis of a unit root is rejected.\n    \"\"\"\n    T = len(y)\n    # Regression: delta_y_t = c_0 + c_1*t + gamma*y_{t-1} + u_t\n    # for t=2,...,T\n    delta_y = np.diff(y)\n    y_lagged = y[:-1]\n    \n    n_obs = T - 1  # Number of observations in the regression\n    \n    # Design matrix X: [constant, trend, y_lagged]\n    trend_regressor = np.arange(2, T + 1)\n    X = np.vstack([np.ones(n_obs), trend_regressor, y_lagged]).T\n    \n    # OLS estimation\n    try:\n        XTX = X.T @ X\n        XTX_inv = inv(XTX)\n        coeffs = XTX_inv @ X.T @ delta_y\n    except np.linalg.LinAlgError:\n         # In case of singular matrix, cannot reject\n        return False\n        \n    gamma_hat = coeffs[2]\n    \n    # Calculate t-statistic for gamma\n    residuals = delta_y - X @ coeffs\n    # n_obs - k, where k=3 regressors\n    sigma_u_sq = (residuals @ residuals) / (n_obs - 3)\n    \n    var_gamma_hat = sigma_u_sq * XTX_inv[2, 2]\n    se_gamma_hat = np.sqrt(var_gamma_hat)\n    \n    if se_gamma_hat == 0:\n        return False # Avoid division by zero\n        \n    t_stat_gamma = gamma_hat / se_gamma_hat\n    \n    # Critical value at alpha=0.05\n    adf_crit_val = -3.41\n    return t_stat_gamma < adf_crit_val\n\ndef kpss_test_trend(y):\n    \"\"\"\n    Performs a KPSS test for trend stationarity.\n    Returns True if the null hypothesis of trend stationarity is rejected.\n    \"\"\"\n    T = len(y)\n    # 1. Regress y on a constant and trend\n    t_trend = np.arange(1, T + 1)\n    X = np.vstack([np.ones(T), t_trend]).T\n    \n    try:\n        coeffs = inv(X.T @ X) @ X.T @ y\n    except np.linalg.LinAlgError:\n        # A singular matrix likely means y is a perfect linear trend.\n        # This implies it is trend-stationary, so we do not reject H0.\n        return False # Do not reject trend-stationarity\n        \n    residuals = y - X @ coeffs\n    \n    # 2. Compute the partial sum process\n    S_t = np.cumsum(residuals)\n    \n    # 3. Compute the long-run variance estimator (Bartlett window)\n    ell = int(np.floor(np.sqrt(T)))\n    \n    gamma_0 = np.sum(residuals**2) / T\n    \n    lr_var_sum = 0.0\n    for j in range(1, ell + 1):\n        autocov_j = np.sum(residuals[j:] * residuals[:-j]) / T\n        weight_j = 1.0 - j / (ell + 1.0)\n        lr_var_sum += weight_j * autocov_j\n        \n    sigma_lr_sq = gamma_0 + 2 * lr_var_sum\n    \n    if sigma_lr_sq <= 1e-10: # Handle near-zero variance\n        return False # Cannot compute statistic, assume no rejection\n\n    # 4. Compute the KPSS statistic\n    kpss_stat = np.sum(S_t**2) / (T**2 * sigma_lr_sq)\n    \n    # Critical value at alpha=0.05\n    kpss_crit_val = 0.146\n    return kpss_stat > kpss_crit_val\n\ndef trend_significance_test(y):\n    \"\"\"\n    Tests if the linear trend coefficient is statistically significant.\n    Returns True if the trend is significant.\n    \"\"\"\n    T = len(y)\n    t_trend = np.arange(1, T + 1)\n    X = np.vstack([np.ones(T), t_trend]).T\n    \n    n_obs = T\n    k_regressors = 2\n    \n    try:\n        XTX_inv = inv(X.T @ X)\n        coeffs = XTX_inv @ X.T @ y\n    except np.linalg.LinAlgError:\n        # If matrix is singular, likely highly collinear, no significant trend can be established\n        return False\n\n    b_hat = coeffs[1]\n    \n    # Calculate t-statistic for b_hat\n    residuals = y - X @ coeffs\n    df = n_obs - k_regressors\n    if df <= 0: return False # Cannot compute\n    \n    sigma_v_sq = (residuals @ residuals) / df\n    var_b_hat = sigma_v_sq * XTX_inv[1, 1]\n    se_b_hat = np.sqrt(var_b_hat)\n    \n    if se_b_hat == 0:\n        return np.abs(b_hat) > 1e-9 # A non-zero coefficient with zero error is significant.\n        \n    t_stat_b = b_hat / se_b_hat\n    \n    # Two-sided t-test critical value at alpha=0.05\n    alpha = 0.05\n    t_crit = student_t.ppf(1 - alpha / 2.0, df)\n    \n    return np.abs(t_stat_b) > t_crit\n\ndef classify_series(params):\n    \"\"\"\n    Classifies a time series based on a battery of tests.\n    \"\"\"\n    T, phi, beta, sigma, seed = params\n    y = generate_series(T, phi, beta, sigma, seed)\n    \n    adf_rejects_h0 = adf_test_with_trend(y)\n    kpss_rejects_h0 = kpss_test_trend(y)\n    trend_is_significant = trend_significance_test(y)\n    \n    # Apply the decision logic from the problem statement\n    if not adf_rejects_h0 and kpss_rejects_h0:\n        return 2  # Unit-root nonstationary\n    elif adf_rejects_h0 and not kpss_rejects_h0 and trend_is_significant:\n        return 1  # Trend-stationary\n    elif adf_rejects_h0 and not trend_is_significant:\n        return 0  # Stationary\n    else:\n        return 3  # Inconclusive\n\ndef solve():\n    # Test Suite: [T, phi, beta, sigma, seed]\n    test_cases = [\n        [512, 1.0, 0.5, 1.0, 1],\n        [512, 0.7, 0.5, 1.0, 2],\n        [512, 1.0, 0.0, 1.0, 3],\n        [512, 0.0, 0.5, 1.0, 4],\n        [256, -0.8, 0.3, 1.0, 5],\n        [128, 0.7, 0.0, 1.0, 6],\n    ]\n\n    results = []\n    for case in test_cases:\n        classification_code = classify_series(case)\n        results.append(classification_code)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}