{
    "hands_on_practices": [
        {
            "introduction": "The validity of causal claims from an experiment rests fundamentally on the randomization procedure. This exercise moves beyond standard t-tests to the core logic of randomization inference by asking you to derive the exact null distribution for a test statistic using only combinatorial principles. By working through this problem , you will gain a deep appreciation for how randomization, not statistical modeling, allows us to make powerful, assumption-free statements about treatment effects.",
            "id": "4161397",
            "problem": "A team is investigating the causal effect of Transcranial Magnetic Stimulation (TMS) on motor cortex excitability in a subject-level randomized experiment. Forty neurologically healthy subjects are randomized into two arms, TMS and sham, with equal group sizes $n_1 = n_2 = 20$ using complete randomization: every assignment that chooses $n_1$ of the $N=40$ subjects for TMS is equally likely. The response for each subject is a binary indicator $Y_i \\in \\{0,1\\}$ of whether their post-intervention motor evoked potential exceeds their baseline. The test statistic is the difference in sample means,\n$$\nT \\;=\\; \\bar{Y}_T - \\bar{Y}_C \\;=\\; \\frac{1}{n_1} \\sum_{i: Z_i=1} Y_i \\;-\\; \\frac{1}{n_2} \\sum_{i: Z_i=0} Y_i,\n$$\nwhere $Z_i \\in \\{0,1\\}$ indicates assignment to TMS ($Z_i=1$) or sham ($Z_i=0$). Under the sharp null hypothesis of no treatment effect, the responses $\\{Y_i\\}_{i=1}^{40}$ are fixed and any randomness arises only from the assignment mechanism.\n\nIn this experiment, the observed data satisfy $\\sum_{i=1}^{40} Y_i = 22$ and $\\sum_{i: Z_i=1} Y_i = 15$, so that the observed value of the test statistic is $T_{\\mathrm{obs}}$.\n\nStarting from the randomization principle that each of the $\\binom{N}{n_1}$ balanced assignments is equally likely under the sharp null, derive the exact randomization distribution of $T$ for a two-arm subject-level complete randomization with $n_1=n_2=20$, using only combinatorial counting implied by the design. Then, using this distribution, express the exact two-sided randomization $p$-value for $T_{\\mathrm{obs}}$ as a single closed-form analytic expression that involves only binomial coefficients and finite sums. Do not invoke any modeling assumptions (such as independence beyond random assignment or parametric likelihoods) and do not use asymptotic approximations. Your final answer must be that single closed-form expression for the two-sided $p$-value. If you choose to provide a numerical approximation, round your answer to four significant figures; otherwise, provide the exact expression.",
            "solution": "The problem requires the derivation of an exact two-sided $p$-value for a randomization test under the sharp null hypothesis of no treatment effect. The derivation must be based solely on the combinatorial properties of the complete randomization design.\n\nFirst, we formalize the problem. We have $N=40$ subjects, with $n_1=20$ assigned to the treatment group (TMS) and $n_2=20$ assigned to the control group (sham). The response $Y_i$ is binary, $Y_i \\in \\{0, 1\\}$. The observed data show that the total number of subjects with $Y_i=1$ (successes) is $S = \\sum_{i=1}^{40} Y_i = 22$. Consequently, the total number of subjects with $Y_i=0$ (failures) is $F = N - S = 40 - 22 = 18$.\n\nThe sharp null hypothesis states that the treatment has no effect on any individual. This means that each subject's response $Y_i$ would be the same regardless of whether they received TMS or sham. Under this hypothesis, the set of $40$ outcomes, consisting of $22$ ones and $18$ zeros, can be considered fixed. The only source of randomness in the experiment is the random assignment of subjects to the treatment and control groups.\n\nThe experiment involves choosing $n_1=20$ subjects for the treatment group from the total of $N=40$ subjects. The number of ways to do this is given by the binomial coefficient $\\binom{N}{n_1} = \\binom{40}{20}$. Under complete randomization, each of these assignments is equally likely.\n\nLet $k$ be the random variable representing the number of successes (subjects with $Y_i=1$) in the treatment group. Under the null hypothesis, the assignment of subjects to the treatment group is equivalent to drawing a sample of size $n_1=20$ without replacement from a population of $N=40$ containing $S=22$ successes and $F=18$ failures. The probability of observing exactly $k$ successes in the treatment group follows a hypergeometric distribution. The probability mass function (PMF) for $k$ is:\n$$ P(k) = \\frac{\\binom{S}{k} \\binom{F}{n_1-k}}{\\binom{N}{n_1}} = \\frac{\\binom{22}{k} \\binom{18}{20-k}}{\\binom{40}{20}} $$\nThe possible values for $k$ (the support of the distribution) are constrained. The number of successes $k$ cannot exceed the number of subjects in the group, $n_1=20$, nor the total number of successes, $S=22$. So, $k \\le \\min(n_1, S) = 20$. The number of failures in the treatment group, $n_1-k$, cannot exceed the total number of failures, $F=18$. This implies $20-k \\le 18$, so $k \\ge 2$. Thus, the support for $k$ is the set of integers $\\{k \\in \\mathbb{Z} \\mid 2 \\le k \\le 20\\}$.\n\nThe test statistic is given by $T = \\bar{Y}_T - \\bar{Y}_C$. Let's express $T$ as a function of $k$.\nThe number of successes in the treatment group is $\\sum_{i: Z_i=1} Y_i = k$.\nThe mean for the treatment group is $\\bar{Y}_T = \\frac{k}{n_1}$.\nThe number of successes in the control group is $S-k$.\nThe mean for the control group is $\\bar{Y}_C = \\frac{S-k}{n_2}$.\nSo, the test statistic is:\n$$ T(k) = \\frac{k}{n_1} - \\frac{S-k}{n_2} $$\nSubstituting the given values $n_1=20$, $n_2=20$, and $S=22$:\n$$ T(k) = \\frac{k}{20} - \\frac{22-k}{20} = \\frac{k - (22-k)}{20} = \\frac{2k-22}{20} $$\nThis expression shows that $T$ is a strictly increasing linear function of $k$.\n\nThe problem states that the observed number of successes in the TMS group is $\\sum_{i:Z_i=1} Y_i = 15$. This is the observed value of our random variable, $k_{\\mathrm{obs}}=15$. The observed value of the test statistic is:\n$$ T_{\\mathrm{obs}} = T(15) = \\frac{2(15) - 22}{20} = \\frac{30 - 22}{20} = \\frac{8}{20} = 0.4 $$\n\nThe two-sided $p$-value is the probability, under the sharp null hypothesis, of observing a test statistic at least as extreme as the one observed. Mathematically, this is $p = P(|T| \\ge |T_{\\mathrm{obs}}|)$.\n$$ |T(k)| \\ge |T_{\\mathrm{obs}}| \\implies \\left|\\frac{2k-22}{20}\\right| \\ge |0.4| $$\n$$ |2k-22| \\ge 8 $$\nThis inequality can be split into two cases:\n1. $2k - 22 \\ge 8 \\implies 2k \\ge 30 \\implies k \\ge 15$.\n2. $2k - 22 \\le -8 \\implies 2k \\le 14 \\implies k \\le 7$.\n\nSo, the set of values for $k$ that are as or more extreme than the observed value $k_{\\mathrm{obs}}=15$ is $\\{k \\mid k \\le 7 \\text{ or } k \\ge 15\\}$.\nTo calculate the $p$-value, we must sum the probabilities $P(k)$ for all values of $k$ in this set that fall within the support of the distribution, which is $[2, 20]$.\nThe union of these conditions gives the set of integers for which we must sum the probabilities: $\\{2, 3, 4, 5, 6, 7\\} \\cup \\{15, 16, 17, 18, 19, 20\\}$.\n\nThe $p$-value is the sum of the probabilities for these values of $k$:\n$$ p = \\sum_{k \\in \\{2..7\\} \\cup \\{15..20\\}} P(k) = \\sum_{k=2}^{7} P(k) + \\sum_{k=15}^{20} P(k) $$\nSubstituting the expression for $P(k)$:\n$$ p = \\sum_{k=2}^{7} \\frac{\\binom{22}{k} \\binom{18}{20-k}}{\\binom{40}{20}} + \\sum_{k=15}^{20} \\frac{\\binom{22}{k} \\binom{18}{20-k}}{\\binom{40}{20}} $$\nThis can be written as a single closed-form expression:\n$$ p = \\frac{1}{\\binom{40}{20}} \\left( \\sum_{k=2}^{7} \\binom{22}{k} \\binom{18}{20-k} + \\sum_{k=15}^{20} \\binom{22}{k} \\binom{18}{20-k} \\right) $$\nThis expression is the exact two-sided randomization $p$-value, derived using only combinatorial counting as required. It involves only binomial coefficients and finite sums.",
            "answer": "$$ \\boxed{ \\frac{1}{\\binom{40}{20}} \\left( \\sum_{k=2}^{7} \\binom{22}{k} \\binom{18}{20-k} + \\sum_{k=15}^{20} \\binom{22}{k} \\binom{18}{20-k} \\right) } $$"
        },
        {
            "introduction": "In neuroscience, we often wish to investigate numerous factors, but testing every combination in a full factorial design can be prohibitively expensive and time-consuming. This practice introduces fractional factorial designs, an elegant solution for efficiently screening multiple factors . You will derive the design's \"alias structure,\" a critical step that reveals which effects are deliberately confounded, allowing for an informed trade-off between experimental resources and statistical resolution.",
            "id": "4161390",
            "problem": "A systems neuroscience laboratory plans to study how multiple experimental factors jointly affect mean firing rate in cultured cortical neurons under controlled stimulation. The team considers $5$ binary factors, coded at two levels each using the standard $\\pm 1$ convention: factor $A$ (synaptic blocker present vs. absent), factor $B$ (high vs. low extracellular potassium), factor $C$ (optogenetic stimulation on vs. off), factor $D$ (neuromodulator present vs. absent), and factor $E$ (membrane potential clamp high vs. low). To reduce the number of experimental runs while enabling estimation of main effects under the assumption that most higher-order interactions are negligible, they adopt a fractional factorial design with $2^{5-2}$ runs defined by the generators $D=AB$ and $E=AC$.\n\nStarting only from the core definitions of two-level fractional factorial designs, generators, defining relations, and aliasing via the defining relation, derive the defining relation and the alias structure for the main effects in this design. Using your derived alias structure, determine which two-factor interactions are confounded with main effects.\n\nLet $N$ denote the total number of distinct two-factor interactions that are aliased with at least one main effect in this design. Report the value of $N$. Your final numeric answer should be exact; no rounding is required.",
            "solution": "The problem requires the analysis of a $2^{5-2}$ fractional factorial design. This is a design for $k=5$ factors in $2^{5-2} = 2^3 = 8$ runs. The design is specified by $p=2$ generators, which define how the levels of some factors are determined by the levels of others.\n\nThe factors are denoted by capital letters $A, B, C, D, E$. In the standard $\\pm 1$ notation for factor levels, the generators $D=AB$ and $E=AC$ mean that the column for factor $D$ in the design matrix is the element-wise product of the columns for $A$ and $B$, and the column for $E$ is the product of columns for $A$ and $C$.\n\n**1. Deriving the Defining Relation**\nThe generators can be rewritten in identity form by multiplying both sides by the factors on the right-hand side. Using the property that for any factor $X$, $X^2=I$ (where $I$ is the identity element, a column of $+1$s), we get:\n$D = AB \\implies D(AB) = (AB)(AB) \\implies ABD = A^2B^2 = I \\cdot I = I$. So, the first generator in identity form is $g_1 = ABD$.\n$E = AC \\implies E(AC) = (AC)(AC) \\implies ACE = A^2C^2 = I \\cdot I = I$. So, the second generator in identity form is $g_2 = ACE$.\n\nThe defining relation for the design is a group formed by the identity $I$, the generators, and all their possible products. In this case, we need to find the product of the two generators:\n$g_1 g_2 = (ABD)(ACE) = A^2BCDE = I \\cdot BCDE = BCDE$.\n\nThe complete defining relation is the set of all such \"words\" that are equivalent to the identity $I$. Therefore, the defining relation is:\n$$I = ABD = ACE = BCDE$$\nThis relation defines the confounding pattern of the design. Any effect or interaction multiplied by a word in the defining relation (other than $I$) yields an aliased effect.\n\n**2. Deriving the Alias Structure for Main Effects**\nThe alias structure for any effect is found by multiplying it by each word in the defining relation. The set of resulting terms are all confounded with each other. We are asked to find the alias structure specifically for the main effects ($A, B, C, D, E$).\n\n- **Alias structure for Main Effect A:**\n  Multiply $A$ by the defining relation:\n  $A \\cdot I = A$\n  $A \\cdot (ABD) = A^2BD = BD$\n  $A \\cdot (ACE) = A^2CE = CE$\n  $A \\cdot (BCDE) = ABCDE$\n  So, the main effect $A$ is aliased with the two-factor interactions $BD$ and $CE$, and the five-factor interaction $ABCDE$. We write this as $[A] = A + BD + CE + ABCDE$.\n\n- **Alias structure for Main Effect B:**\n  Multiply $B$ by the defining relation:\n  $B \\cdot I = B$\n  $B \\cdot (ABD) = AB^2D = AD$\n  $B \\cdot (ACE) = ABCE$\n  $B \\cdot (BCDE) = B^2CDE = CDE$\n  So, the main effect $B$ is aliased with the two-factor interaction $AD$ and the three- and four-factor interactions $CDE$ and $ABCE$. We write this as $[B] = B + AD + CDE + ABCE$.\n\n- **Alias structure for Main Effect C:**\n  Multiply $C$ by the defining relation:\n  $C \\cdot I = C$\n  $C \\cdot (ABD) = ABCD$\n  $C \\cdot (ACE) = AC^2E = AE$\n  $C \\cdot (BCDE) = BC^2DE = BDE$\n  So, the main effect $C$ is aliased with the two-factor interaction $AE$ and the three- and four-factor interactions $BDE$ and $ABCD$. We write this as $[C] = C + AE + BDE + ABCD$.\n\n- **Alias structure for Main Effect D:**\n  Multiply $D$ by the defining relation:\n  $D \\cdot I = D$\n  $D \\cdot (ABD) = ABD^2 = AB$\n  $D \\cdot (ACE) = ACDE$\n  $D \\cdot (BCDE) = BCD^2E = BCE$\n  So, the main effect $D$ is aliased with the two-factor interaction $AB$ and the three- and four-factor interactions $BCE$ and $ACDE$. We write this as $[D] = D + AB + BCE + ACDE$.\n\n- **Alias structure for Main Effect E:**\n  Multiply $E$ by the defining relation:\n  $E \\cdot I = E$\n  $E \\cdot (ABD) = ABDE$\n  $E \\cdot (ACE) = ACE^2 = AC$\n  $E \\cdot (BCDE) = BCD$\n  So, the main effect $E$ is aliased with the two-factor interaction $AC$ and the three- and four-factor interactions $BCD$ and $ABDE$. We write this as $[E] = E + AC + BCD + ABDE$.\n\n**3. Identifying Confounded Two-Factor Interactions and Counting Them**\nThe problem asks for $N$, the total number of distinct two-factor interactions that are aliased with at least one main effect. We can extract these directly from the alias structures derived above. Under the common assumption that interactions of three or more factors are negligible, the primary concern is the aliasing of main effects with two-factor interactions.\n\n- From $[A] = A + BD + CE + ABCDE$, the two-factor interactions are $BD$ and $CE$.\n- From $[B] = B + AD + CDE + ABCE$, the two-factor interaction is $AD$.\n- From $[C] = C + AE + BDE + ABCD$, the two-factor interaction is $AE$.\n- From $[D] = D + AB + BCE + ACDE$, the two-factor interaction is $AB$.\n- From $[E] = E + AC + BCD + ABDE$, the two-factor interaction is $AC$.\n\nLet's compile a list of all such unique two-factor interactions:\n1. $AB$ (aliased with $D$)\n2. $AC$ (aliased with $E$)\n3. $AD$ (aliased with $B$)\n4. $AE$ (aliased with $C$)\n5. $BD$ (aliased with $A$)\n6. $CE$ (aliased with $A$)\n\nThese are all distinct two-factor interactions. We now count the number of interactions in this set. There are $6$ such interactions.\n\nThus, the total number of distinct two-factor interactions aliased with a main effect is $N=6$.\nThe design resolution is the length of the shortest word in the defining relation. Here, the shortest words are $ABD$ and $ACE$, both of length $3$. Therefore, this is a Resolution III design. In a Resolution III design, main effects are aliased with two-factor interactions, which is consistent with our findings.\n\nFinal compilation of the two-factor interactions confounded with main effects:\n- $A$ is confounded with $BD$ and $CE$.\n- $B$ is confounded with $AD$.\n- $C$ is confounded with $AE$.\n- $D$ is confounded with $AB$.\n- $E$ is confounded with $AC$.\n\nThe complete set of two-factor interactions aliased with a main effect is $\\{AB, AC, AD, AE, BD, CE\\}$. The number of elements in this set, $N$, is $6$.",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "While we strive for perfectly balanced experiments, practical constraints often lead to unequal sample sizes across conditions, creating an unbalanced design. This exercise explores the analytical challenges posed by such designs, where the clean separation of effects—a property known as orthogonality—is lost . By deriving the normal equations for a general linear model and examining the correlation between factor contrasts, you will understand precisely how and why the analysis and interpretation of main effects become more complex in the presence of unbalanced data.",
            "id": "4161349",
            "problem": "A laboratory neuroscience experiment aims to quantify mean spike-rate responses to sensory stimulation recorded from neurons under a two-factor experimental design with independent trials. Factor $A$ denotes cortical region, with levels $a \\in \\{1,\\dots,A\\}$. Factor $B$ denotes stimulation pattern, with levels $b \\in \\{1,\\dots,B\\}$. The design is unbalanced due to variable trial availability: each cell $(a,b)$ has $n_{ab} \\geq 1$ independent trial replicates indexed by $k \\in \\{1,\\dots,n_{ab}\\}$. Trials are randomized within each cell to mitigate time-varying confounders. The data generating mechanism is modeled as a fixed-effects two-way factorial general linear model (GLM) with homoscedastic noise:\n$$\ny_{abk} \\;=\\; \\mu \\;+\\; \\alpha_{a} \\;+\\; \\beta_{b} \\;+\\; (\\alpha\\beta)_{ab} \\;+\\; \\varepsilon_{abk},\n$$\nwhere $y_{abk}$ is the spike-rate on trial $k$ in cell $(a,b)$, $\\mu$ is the grand mean, $\\alpha_{a}$ is the effect of cortical region $a$, $\\beta_{b}$ is the effect of stimulation pattern $b$, $(\\alpha\\beta)_{ab}$ is the interaction for cell $(a,b)$, and $\\varepsilon_{abk}$ are independent errors satisfying $\\mathbb{E}[\\varepsilon_{abk}] = 0$ and $\\operatorname{Var}(\\varepsilon_{abk}) = \\sigma^{2}$ for all $a,b,k$. To render the parametrization identifiable, impose the standard sum-to-zero constraints:\n$$\n\\sum_{a=1}^{A} \\alpha_{a} = 0,\\quad \\sum_{b=1}^{B} \\beta_{b} = 0,\\quad \\sum_{b=1}^{B} (\\alpha\\beta)_{ab} = 0 \\;\\;\\text{for each}\\;\\; a,\\quad \\sum_{a=1}^{A} (\\alpha\\beta)_{ab} = 0 \\;\\;\\text{for each}\\;\\; b.\n$$\nDefine the cell mean $\\bar{y}_{ab} = \\frac{1}{n_{ab}} \\sum_{k=1}^{n_{ab}} y_{abk}$, the marginal totals $n_{a\\cdot} = \\sum_{b=1}^{B} n_{ab}$ and $n_{\\cdot b} = \\sum_{a=1}^{A} n_{ab}$, and the grand total $N = \\sum_{a=1}^{A} \\sum_{b=1}^{B} n_{ab}$. Consider the least squares criterion as the foundational base: ordinary least squares (OLS) estimates minimize the sum of squared residuals.\n\nTask 1. Starting from the principle that OLS minimizes the sum of squared errors, derive the normal equations for $\\mu$, $\\alpha_{a}$, $\\beta_{b}$, and $(\\alpha\\beta)_{ab}$ under the above sum-to-zero constraints, presenting the equations in terms of $\\bar{y}_{ab}$ and the replication counts $n_{ab}$.\n\nTask 2. Let an $A$-main-effect contrast be defined by coefficients $d_{1},\\dots,d_{A}$ satisfying $\\sum_{a=1}^{A} d_{a} = 0$, and a $B$-main-effect contrast be defined by coefficients $e_{1},\\dots,e_{B}$ satisfying $\\sum_{b=1}^{B} e_{b} = 0$. Represent these contrasts as column vectors in the GLM design matrix using effect coding consistent with the above constraints and define the inner product between two design columns $u$ and $v$ as $\\langle u, v \\rangle = \\sum_{a=1}^{A} \\sum_{b=1}^{B} n_{ab} u_{ab} v_{ab}$, which weights by replication counts. Derive, from first principles and without invoking shortcut formulas, the normalized inner product (i.e., correlation) between the $A$-main-effect contrast vector with coefficients $d_{a}$ and the $B$-main-effect contrast vector with coefficients $e_{b}$, expressed purely in terms of $n_{ab}$, $n_{a\\cdot}$, $n_{\\cdot b}$, $d_{a}$, and $e_{b}$. Then, using this expression and the structure of the normal equations, argue which classes of contrasts remain orthogonal in the unbalanced design and under what structural conditions of the $n_{ab}$ they do.\n\nAnswer specification. Your final reported quantity must be the single closed-form analytic expression for the normalized inner product between the $A$- and $B$-main-effect contrast columns as a function of $n_{ab}$, $n_{a\\cdot}$, $n_{\\cdot b}$, $d_{a}$, and $e_{b}$. No rounding is required. Do not include any units in your final answer.",
            "solution": "The solution is presented in two parts, corresponding to the two tasks specified in the problem statement.\n\n### Task 1: Derivation of the Normal Equations\n\nThe ordinary least squares (OLS) principle requires minimization of the sum of squared errors (SSE). The SSE is given by:\n$$\nS = \\sum_{a=1}^{A} \\sum_{b=1}^{B} \\sum_{k=1}^{n_{ab}} \\varepsilon_{abk}^2 = \\sum_{a=1}^{A} \\sum_{b=1}^{B} \\sum_{k=1}^{n_{ab}} \\left( y_{abk} - \\mu - \\alpha_a - \\beta_b - (\\alpha\\beta)_{ab} \\right)^2\n$$\nLet $\\mu_{ab} = \\mu + \\alpha_a + \\beta_b + (\\alpha\\beta)_{ab}$ denote the theoretical mean of cell $(a,b)$. The SSE can be decomposed by adding and subtracting the cell sample mean $\\bar{y}_{ab}$:\n$$\nS = \\sum_{a,b,k} \\left( (y_{abk} - \\bar{y}_{ab}) + (\\bar{y}_{ab} - \\mu_{ab}) \\right)^2\n$$\nExpanding the square and noting that the cross-term $\\sum_k (y_{abk} - \\bar{y}_{ab})(\\bar{y}_{ab} - \\mu_{ab})$ is zero, we get:\n$$\nS = \\sum_{a,b} \\sum_{k=1}^{n_{ab}} (y_{abk} - \\bar{y}_{ab})^2 + \\sum_{a,b} n_{ab} (\\bar{y}_{ab} - \\mu_{ab})^2\n$$\nThe first term is the within-cell sum of squares, which does not depend on the model parameters $\\mu, \\alpha_a, \\beta_b, (\\alpha\\beta)_{ab}$. Minimizing $S$ is therefore equivalent to minimizing the second term, a weighted sum of squares of the cell means:\n$$\nS' = \\sum_{a=1}^{A} \\sum_{b=1}^{B} n_{ab} (\\bar{y}_{ab} - \\mu - \\alpha_a - \\beta_b - (\\alpha\\beta)_{ab})^2\n$$\nThe normal equations are obtained by setting the partial derivatives of $S'$ with respect to each parameter to zero. As the model includes the full interaction term $(\\alpha\\beta)_{ab}$, it is saturated. This means there are as many independent parameters as there are cell means.\n\nLet $\\hat{\\mu}, \\hat{\\alpha}_a, \\hat{\\beta}_b, \\widehat{(\\alpha\\beta)}_{ab}$ be the OLS estimates. The partial derivative with respect to an interaction parameter $\\widehat{(\\alpha\\beta)}_{a'b'}$ is:\n$$\n\\frac{\\partial S'}{\\partial (\\alpha\\beta)_{a'b'}} = -2 n_{a'b'} (\\bar{y}_{a'b'} - \\hat{\\mu} - \\hat{\\alpha}_{a'} - \\hat{\\beta}_{b'} - \\widehat{(\\alpha\\beta)}_{a'b'}) = 0\n$$\nSince each cell has at least one replicate ($n_{ab} \\geq 1$), this equation implies that the estimated cell mean must equal the sample cell mean:\n$$\n\\hat{\\mu} + \\hat{\\alpha}_a + \\hat{\\beta}_b + \\widehat{(\\alpha\\beta)}_{ab} = \\bar{y}_{ab} \\quad \\text{for all } a \\in \\{1,\\dots,A\\}, b \\in \\{1,\\dots,B\\}.\n$$\nWhen this condition holds, the term inside the parenthesis in $S'$ is zero for every cell $(a,b)$. Consequently, all other partial derivatives, which are weighted sums of these terms, are also zero. For example,\n$$\n\\frac{\\partial S'}{\\partial \\alpha_{a'}} = -2 \\sum_{b=1}^{B} n_{a'b} (\\bar{y}_{a'b'} - (\\hat{\\mu} + \\dots)) = -2 \\sum_{b=1}^{B} n_{a'b} (0) = 0.\n$$\nThis confirms that fitting the saturated model reduces to perfectly fitting the observed cell means. The model is over-parameterized, and a unique solution for the parameters is obtained by imposing the sum-to-zero constraints. Thus, the complete system of normal equations, which defines the OLS estimates under the given constraints, is the combination of the cell mean fit and the constraints themselves:\n1.  Fit to cell means: $\\hat{\\mu} + \\hat{\\alpha}_a + \\hat{\\beta}_b + \\widehat{(\\alpha\\beta)}_{ab} = \\bar{y}_{ab}$ for all $a, b$.\n2.  Constraints on main effects: $\\sum_{a=1}^{A} \\hat{\\alpha}_a = 0$ and $\\sum_{b=1}^{B} \\hat{\\beta}_b = 0$.\n3.  Constraints on interactions: $\\sum_{b=1}^{B} \\widehat{(\\alpha\\beta)}_{ab} = 0$ for each $a \\in \\{1,\\dots,A\\}$, and $\\sum_{a=1}^{A} \\widehat{(\\alpha\\beta)}_{ab} = 0$ for each $b \\in \\{1,\\dots,B\\}$.\n\nThis system of linear equations uniquely determines the parameter estimates from the sample cell means $\\bar{y}_{ab}$. The replication counts $n_{ab}$ are used to calculate $\\bar{y}_{ab}$ from the raw data $y_{abk}$.\n\n### Task 2: Normalized Inner Product of Contrasts\n\nAn $A$-main-effect contrast is a linear combination of the main-effect parameters, $\\sum_{a=1}^{A} d_a \\alpha_a$, where $\\sum_{a=1}^{A} d_a=0$. In the vector space of observations, this contrast corresponds to a vector $u$ whose value for any observation in cell $(a,b)$ is simply $d_a$. We denote the component of this vector for cell $(a,b)$ as $u_{ab} = d_a$. This vector is constant across the levels of factor $B$.\n\nSimilarly, a $B$-main-effect contrast, $\\sum_{b=1}^{B} e_b \\beta_b$ with $\\sum_{b=1}^{B} e_b=0$, corresponds to a vector $v$ with components $v_{ab} = e_b$.\n\nThe problem defines a weighted inner product between two such vectors $u$ and $v$ as:\n$$\n\\langle u, v \\rangle = \\sum_{a=1}^{A} \\sum_{b=1}^{B} n_{ab} u_{ab} v_{ab}\n$$\nThe \"normalized inner product\" is the cosine of the angle between the vectors, or their correlation, given by $\\rho = \\frac{\\langle u, v \\rangle}{\\|u\\| \\|v\\|}$, where the norm is induced by the inner product: $\\|u\\|^2 = \\langle u, u \\rangle$.\n\nFirst, we compute the inner product between the $A$-contrast vector $u$ (with components $d_a$) and the $B$-contrast vector $v$ (with components $e_b$):\n$$\n\\langle u, v \\rangle = \\sum_{a=1}^{A} \\sum_{b=1}^{B} n_{ab} d_a e_b\n$$\nThis expression can be grouped by $a$ or $b$: $\\sum_{a=1}^{A} d_a \\left( \\sum_{b=1}^{B} n_{ab} e_b \\right)$.\n\nNext, we compute the squared norms of these vectors.\nFor the $A$-contrast vector $u$:\n$$\n\\|u\\|^2 = \\langle u, u \\rangle = \\sum_{a=1}^{A} \\sum_{b=1}^{B} n_{ab} d_a^2 = \\sum_{a=1}^{A} d_a^2 \\left( \\sum_{b=1}^{B} n_{ab} \\right) = \\sum_{a=1}^{A} d_a^2 n_{a\\cdot}\n$$\nFor the $B$-contrast vector $v$:\n$$\n\\|v\\|^2 = \\langle v, v \\rangle = \\sum_{a=1}^{A} \\sum_{b=1}^{B} n_{ab} e_b^2 = \\sum_{b=1}^{B} e_b^2 \\left( \\sum_{a=1}^{A} n_{ab} \\right) = \\sum_{b=1}^{B} e_b^2 n_{\\cdot b}\n$$\nThe normalized inner product, $\\rho(u,v)$, is the ratio of the inner product to the product of the norms:\n$$\n\\rho(u,v) = \\frac{\\sum_{a=1}^{A} \\sum_{b=1}^{B} n_{ab} d_a e_b}{\\sqrt{\\left( \\sum_{a=1}^{A} d_a^2 n_{a\\cdot} \\right) \\left( \\sum_{b=1}^{B} e_b^2 n_{\\cdot b} \\right)}}\n$$\nThis is the required closed-form expression.\n\nFor the final part of the task, we analyze the condition for orthogonality, which is $\\langle u, v \\rangle = 0$.\nThe entire class of $A$-main-effect contrasts is orthogonal to the entire class of $B$-main-effect contrasts if $\\sum_{a,b} n_{ab} d_a e_b = 0$ for all sets of coefficients $\\{d_a\\}$ and $\\{e_b\\}$ satisfying $\\sum d_a = 0$ and $\\sum e_b = 0$.\nThe sum can be written as $\\sum_a d_a \\left( \\sum_b n_{ab} e_b \\right)$. For this to be zero for all contrasts $\\{d_a\\}$, the term in the parenthesis, $\\sum_b n_{ab} e_b$, must be orthogonal to the space of A-contrasts. This means it must be a constant, independent of $a$.\nSo, $\\sum_b n_{ab} e_b = C(e)$ for all $a=1, \\dots, A$. This must hold for every B-contrast $\\{e_b\\}$.\nLet's test this with basis contrasts, e.g., $e_j = (0, \\dots, 1, \\dots, -1, \\dots, 0)$, with a $1$ at position $j$ and $-1$ at position $B$. For this contrast, the condition becomes $n_{aj} - n_{aB} = C_j$, a constant for all $a$. This must hold for all $j=1, \\dots, B-1$.\nThis implies $n_{aj} - n_{a'j} = n_{aB} - n_{a'B}$ for any pair $a, a'$. The right-hand side is a constant for any $j$. Thus, $n_{aj} - n_{a'j}$ is constant for all $j=1, \\dots, B$.\nThis structural condition on the matrix $N = (n_{ab})$ is equivalent to stating that $n_{ab}$ must be of an additive form: $n_{ab} = u_a + v_b$ for some set of constants $\\{u_a\\}$ and $\\{v_b\\}$. A balanced design, where $n_{ab}=c$ is a special case of this ($u_a=c, v_b=0$).\n\nA specific $A$-contrast, defined by a fixed vector $d$, is orthogonal to all $B$-contrasts if $\\sum_b \\left( \\sum_a n_{ab} d_a \\right) e_b = 0$ for all $B$-contrasts $e$. This requires the vector with components $w_b = \\sum_a n_{ab} d_a$ to be constant for all $b$. Symmetrically, a specific $B$-contrast $e$ is orthogonal to all $A$-contrasts if $\\sum_b n_{ab} e_b$ is constant for all $a$. In an unbalanced design, orthogonality is not guaranteed and depends on the specific structure of the cell counts $n_{ab}$ and the contrast coefficients.",
            "answer": "$$\\boxed{\\frac{\\sum_{a=1}^{A} \\sum_{b=1}^{B} n_{ab} d_{a} e_{b}}{\\sqrt{\\left(\\sum_{a=1}^{A} d_{a}^{2} n_{a\\cdot}\\right) \\left(\\sum_{b=1}^{B} e_{b}^{2} n_{\\cdot b}\\right)}}}$$"
        }
    ]
}