{
    "hands_on_practices": [
        {
            "introduction": "At the heart of any randomized experiment is the powerful idea that the act of randomization itself provides a valid basis for causal inference. This exercise takes you back to first principles, asking you to derive the exact probability of an experimental outcome without relying on parametric models or asymptotic approximations. By working through the combinatorics of a randomization test under the sharp null hypothesis, you will gain a foundational understanding of how randomization-based $p$-values are constructed and interpreted. ",
            "id": "4161397",
            "problem": "A team is investigating the causal effect of Transcranial Magnetic Stimulation (TMS) on motor cortex excitability in a subject-level randomized experiment. Forty neurologically healthy subjects are randomized into two arms, TMS and sham, with equal group sizes $n_1 = n_2 = 20$ using complete randomization: every assignment that chooses $n_1$ of the $N=40$ subjects for TMS is equally likely. The response for each subject is a binary indicator $Y_i \\in \\{0,1\\}$ of whether their post-intervention motor evoked potential exceeds their baseline. The test statistic is the difference in sample means,\n$$\nT \\;=\\; \\bar{Y}_T - \\bar{Y}_C \\;=\\; \\frac{1}{n_1} \\sum_{i: Z_i=1} Y_i \\;-\\; \\frac{1}{n_2} \\sum_{i: Z_i=0} Y_i,\n$$\nwhere $Z_i \\in \\{0,1\\}$ indicates assignment to TMS ($Z_i=1$) or sham ($Z_i=0$). Under the sharp null hypothesis of no treatment effect, the responses $\\{Y_i\\}_{i=1}^{40}$ are fixed and any randomness arises only from the assignment mechanism.\n\nIn this experiment, the observed data satisfy $\\sum_{i=1}^{40} Y_i = 22$ and $\\sum_{i: Z_i=1} Y_i = 15$, so that the observed value of the test statistic is $T_{\\mathrm{obs}}$.\n\nStarting from the randomization principle that each of the $\\binom{N}{n_1}$ balanced assignments is equally likely under the sharp null, derive the exact randomization distribution of $T$ for a two-arm subject-level complete randomization with $n_1=n_2=20$, using only combinatorial counting implied by the design. Then, using this distribution, express the exact two-sided randomization $p$-value for $T_{\\mathrm{obs}}$ as a single closed-form analytic expression that involves only binomial coefficients and finite sums. Do not invoke any modeling assumptions (such as independence beyond random assignment or parametric likelihoods) and do not use asymptotic approximations. Your final answer must be that single closed-form expression for the two-sided $p$-value. If you choose to provide a numerical approximation, round your answer to four significant figures; otherwise, provide the exact expression.",
            "solution": "The problem requires the derivation of an exact two-sided $p$-value for a randomization test under the sharp null hypothesis of no treatment effect. The derivation must be based solely on the combinatorial properties of the complete randomization design.\n\nFirst, we formalize the problem. We have $N=40$ subjects, with $n_1=20$ assigned to the treatment group (TMS) and $n_2=20$ assigned to the control group (sham). The response $Y_i$ is binary, $Y_i \\in \\{0, 1\\}$. The observed data show that the total number of subjects with $Y_i=1$ (successes) is $S = \\sum_{i=1}^{40} Y_i = 22$. Consequently, the total number of subjects with $Y_i=0$ (failures) is $F = N - S = 40 - 22 = 18$.\n\nThe sharp null hypothesis states that the treatment has no effect on any individual. This means that each subject's response $Y_i$ would be the same regardless of whether they received TMS or sham. Under this hypothesis, the set of $40$ outcomes, consisting of $22$ ones and $18$ zeros, can be considered fixed. The only source of randomness in the experiment is the random assignment of subjects to the treatment and control groups.\n\nThe experiment involves choosing $n_1=20$ subjects for the treatment group from the total of $N=40$ subjects. The number of ways to do this is given by the binomial coefficient $\\binom{N}{n_1} = \\binom{40}{20}$. Under complete randomization, each of these assignments is equally likely.\n\nLet $k$ be the random variable representing the number of successes (subjects with $Y_i=1$) in the treatment group. Under the null hypothesis, the assignment of subjects to the treatment group is equivalent to drawing a sample of size $n_1=20$ without replacement from a population of $N=40$ containing $S=22$ successes and $F=18$ failures. The probability of observing exactly $k$ successes in the treatment group follows a hypergeometric distribution. The probability mass function (PMF) for $k$ is:\n$$ P(k) = \\frac{\\binom{S}{k} \\binom{F}{n_1-k}}{\\binom{N}{n_1}} = \\frac{\\binom{22}{k} \\binom{18}{20-k}}{\\binom{40}{20}} $$\nThe possible values for $k$ (the support of the distribution) are constrained. The number of successes $k$ cannot exceed the number of subjects in the group, $n_1=20$, nor the total number of successes, $S=22$. So, $k \\le \\min(n_1, S) = 20$. The number of failures in the treatment group, $n_1-k$, cannot exceed the total number of failures, $F=18$. This implies $20-k \\le 18$, so $k \\ge 2$. Thus, the support for $k$ is the set of integers $\\{k \\in \\mathbb{Z} \\mid 2 \\le k \\le 20\\}$.\n\nThe test statistic is given by $T = \\bar{Y}_T - \\bar{Y}_C$. Let's express $T$ as a function of $k$.\nThe number of successes in the treatment group is $\\sum_{i: Z_i=1} Y_i = k$.\nThe mean for the treatment group is $\\bar{Y}_T = \\frac{k}{n_1}$.\nThe number of successes in the control group is $S-k$.\nThe mean for the control group is $\\bar{Y}_C = \\frac{S-k}{n_2}$.\nSo, the test statistic is:\n$$ T(k) = \\frac{k}{n_1} - \\frac{S-k}{n_2} $$\nSubstituting the given values $n_1=20$, $n_2=20$, and $S=22$:\n$$ T(k) = \\frac{k}{20} - \\frac{22-k}{20} = \\frac{k - (22-k)}{20} = \\frac{2k-22}{20} $$\nThis expression shows that $T$ is a strictly increasing linear function of $k$.\n\nThe problem states that the observed number of successes in the TMS group is $\\sum_{i:Z_i=1} Y_i = 15$. This is the observed value of our random variable, $k_{\\mathrm{obs}}=15$. The observed value of the test statistic is:\n$$ T_{\\mathrm{obs}} = T(15) = \\frac{2(15) - 22}{20} = \\frac{30 - 22}{20} = \\frac{8}{20} = 0.4 $$\n\nThe two-sided $p$-value is the probability, under the sharp null hypothesis, of observing a test statistic at least as extreme as the one observed. Mathematically, this is $p = P(|T| \\ge |T_{\\mathrm{obs}}|)$.\n$$ |T(k)| \\ge |T_{\\mathrm{obs}}| \\implies \\left|\\frac{2k-22}{20}\\right| \\ge |0.4| $$\n$$ |2k-22| \\ge 8 $$\nThis inequality can be split into two cases:\n1. $2k - 22 \\ge 8 \\implies 2k \\ge 30 \\implies k \\ge 15$.\n2. $2k - 22 \\le -8 \\implies 2k \\le 14 \\implies k \\le 7$.\n\nSo, the set of values for $k$ that are as or more extreme than the observed value $k_{\\mathrm{obs}}=15$ is $\\{k \\mid k \\le 7 \\text{ or } k \\ge 15\\}$.\nTo calculate the $p$-value, we must sum the probabilities $P(k)$ for all values of $k$ in this set that fall within the support of the distribution, which is $[2, 20]$.\nThe union of these conditions gives the set of integers for which we must sum the probabilities: $\\{2, 3, 4, 5, 6, 7\\} \\cup \\{15, 16, 17, 18, 19, 20\\}$.\n\nThe $p$-value is the sum of the probabilities for these values of $k$:\n$$ p = \\sum_{k \\in \\{2..7\\} \\cup \\{15..20\\}} P(k) = \\sum_{k=2}^{7} P(k) + \\sum_{k=15}^{20} P(k) $$\nSubstituting the expression for $P(k)$:\n$$ p = \\sum_{k=2}^{7} \\frac{\\binom{22}{k} \\binom{18}{20-k}}{\\binom{40}{20}} + \\sum_{k=15}^{20} \\frac{\\binom{22}{k} \\binom{18}{20-k}}{\\binom{40}{20}} $$\nThis can be written as a single closed-form expression:\n$$ p = \\frac{1}{\\binom{40}{20}} \\left( \\sum_{k=2}^{7} \\binom{22}{k} \\binom{18}{20-k} + \\sum_{k=15}^{20} \\binom{22}{k} \\binom{18}{20-k} \\right) $$\nThis expression is the exact two-sided randomization $p$-value, derived using only combinatorial counting as required. It involves only binomial coefficients and finite sums.",
            "answer": "$$ \\boxed{ \\frac{1}{\\binom{40}{20}} \\left( \\sum_{k=2}^{7} \\binom{22}{k} \\binom{18}{20-k} + \\sum_{k=15}^{20} \\binom{22}{k} \\binom{18}{20-k} \\right) } $$"
        },
        {
            "introduction": "Factorial designs are the workhorse of experimental neuroscience, allowing us to investigate not just the individual effects of factors but also their interactions. The Analysis of Variance (ANOVA) is the primary tool for dissecting the results of such experiments, and a critical first step is understanding the partitioning of variability through degrees of freedom. This practice challenges you to derive the degrees of freedom for a two-way between-subjects design from fundamental principles, solidifying your ability to construct and interpret the $F$-tests for main effects and interactions. ",
            "id": "4161379",
            "problem": "A neuroscience laboratory is conducting a two-factor between-subjects experiment using Functional Magnetic Resonance Imaging (fMRI) to assess regional blood-oxygen-level-dependent (BOLD) signal amplitudes in a predefined region of interest. The design manipulates two categorical factors: factor $A$ (with $3$ levels) representing a neuromodulatory drug condition, and factor $B$ (with $3$ levels) representing task training regime. Participants are randomly assigned to one of the $3 \\times 3$ treatment combinations using block randomization across scanner-days to maintain balance per cell and to control nuisance variability due to day-specific scanner drift. Each treatment combination (cell) contains $n=15$ independent participants, and data are analyzed using a two-way analysis of variance for independent groups.\n\nAssume the following standard conditions for the two-factor between-subjects linear model for the cell-level structure: additivity of effects, identifiability via sum-to-zero constraints on factor effects and their interaction, homoscedastic independent errors within and across cells, and orthogonality ensured by balanced randomization. Starting from the definitions of the general linear model and the partition of sums of squares into components attributable to factor $A$, factor $B$, their interaction $A \\times B$, and residual error (within-cell variability), derive the degrees of freedom for each component and the total. Then, specify the corresponding $F$-tests for the main effects and the interaction, identifying the appropriate numerator and denominator mean squares.\n\nCompute and report the degrees of freedom for factor $A$, factor $B$, interaction $A \\times B$, error, and total, in that order. Express the final answer as a single row matrix. No rounding is required. No units are required.",
            "solution": "The problem presents a two-factor between-subjects experimental design and asks for the derivation and computation of the degrees of freedom for a corresponding two-way analysis of variance (ANOVA), as well as the specification of the relevant $F$-tests.\n\nFirst, we establish the statistical model. Let $y_{ijk}$ be the measured BOLD signal for the $k$-th participant in the treatment combination corresponding to the $i$-th level of factor $A$ and the $j$-th level of factor $B$. The factors have $a$ and $b$ levels, respectively, and there are $n$ participants per cell. In this problem, we are given $a=3$, $b=3$, and $n=15$. The total number of participants is $N = a \\times b \\times n = 3 \\times 3 \\times 15 = 135$.\n\nThe linear model for a two-way ANOVA is:\n$$y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}$$\nwhere $i=1, \\dots, a$; $j=1, \\dots, b$; and $k=1, \\dots, n$. In this model:\n- $\\mu$ is the overall grand mean BOLD signal.\n- $\\alpha_i$ is the main effect of the $i$-th level of factor $A$ (drug condition).\n- $\\beta_j$ is the main effect of the $j$-th level of factor $B$ (training regime).\n- $(\\alpha\\beta)_{ij}$ is the interaction effect between the $i$-th level of factor $A$ and the $j$-th level of factor $B$.\n- $\\epsilon_{ijk}$ represents the random error for each participant, assumed to be independent and identically distributed following a normal distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\epsilon_{ijk} \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nFor the model to be identifiable, sum-to-zero constraints are imposed on the effect parameters:\n$$ \\sum_{i=1}^{a} \\alpha_i = 0 $$\n$$ \\sum_{j=1}^{b} \\beta_j = 0 $$\n$$ \\sum_{i=1}^{a} (\\alpha\\beta)_{ij} = 0 \\quad \\text{for each } j, \\quad \\text{and} \\quad \\sum_{j=1}^{b} (\\alpha\\beta)_{ij} = 0 \\quad \\text{for each } i $$\n\nThe fundamental principle of ANOVA is the partitioning of the total sum of squares ($SS_{Total}$) into components attributable to each factor, their interaction, and the residual error. The total sum of squares measures the total variability in the data:\n$$SS_{Total} = \\sum_{i=1}^{a} \\sum_{j=1}^{b} \\sum_{k=1}^{n} (y_{ijk} - \\bar{y}_{...})^2$$\nwhere $\\bar{y}_{...}$ is the grand mean of all observations. This is partitioned as:\n$$SS_{Total} = SS_A + SS_B + SS_{AB} + SS_{Error}$$\n\nThe degrees of freedom ($df$) for each sum of squares component correspond to the number of independent pieces of information used to calculate it.\n\n1.  **Total Degrees of Freedom ($df_{Total}$):** The total dataset consists of $N = abn$ observations. The calculation of $SS_{Total}$ involves these $N$ values, but one degree of freedom is lost by estimating the grand mean $\\bar{y}_{...}$ from the data. Thus:\n    $$df_{Total} = N - 1 = abn - 1$$\n\n2.  **Degrees of Freedom for Factor A ($df_A$):** This component is associated with the $a$ main effects, $\\alpha_i$. Due to the constraint $\\sum_{i=1}^{a} \\alpha_i = 0$, only $a-1$ of these effects are independent; the last one is fixed once the others are known. Therefore:\n    $$df_A = a - 1$$\n\n3.  **Degrees of Freedom for Factor B ($df_B$):** Similarly, for the $b$ main effects of factor $B$, $\\beta_j$, the constraint $\\sum_{j=1}^{b} \\beta_j = 0$ implies that only $b-1$ effects are independent. Thus:\n    $$df_B = b - 1$$\n\n4.  **Degrees of Freedom for Interaction ($df_{AB}$):** There are $ab$ interaction effects, $(\\alpha\\beta)_{ij}$. These are constrained by $a$ row-sum constraints and $b$ column-sum constraints. However, one of these constraints is redundant. The total number of independent constraints is $a+b-1$. The number of independent interaction effects is the total number of effects minus the number of independent constraints: $ab - (a+b-1) = ab - a - b + 1$. This can be factored:\n    $$df_{AB} = (a-1)(b-1)$$\n\n5.  **Degrees of Freedom for Error ($df_{Error}$):** The error sum of squares, $SS_{Error}$, represents the pooled within-cell variability, $SS_{Error} = \\sum_{i=1}^{a} \\sum_{j=1}^{b} \\sum_{k=1}^{n} (y_{ijk} - \\bar{y}_{ij.})^2$, where $\\bar{y}_{ij.}$ is the mean of the cell $(i,j)$. For each of the $ab$ cells, we calculate a sum of squares from $n$ observations, which has $n-1$ degrees of freedom (one lost for estimating the cell mean). Since the design is balanced, the total error degrees of freedom is the sum over all cells:\n    $$df_{Error} = ab(n-1)$$\n\nThe degrees of freedom partition is additive, which we can verify:\n$df_A + df_B + df_{AB} + df_{Error} = (a-1) + (b-1) + (a-1)(b-1) + ab(n-1)$\n$= a - 1 + b - 1 + (ab - a - b + 1) + abn - ab$\n$= abn - 1 = df_{Total}$. The partition is consistent.\n\nNext, we specify the $F$-tests. An $F$-statistic is the ratio of two mean squares ($MS$), where $MS = SS/df$. The denominator for all tests in this design is the mean square for error, $MS_{Error} = SS_{Error}/df_{Error}$, which is the pooled within-cell variance estimate.\n\n-   **Test for the main effect of Factor A:** The test statistic is $F_A = \\frac{MS_A}{MS_{Error}} = \\frac{SS_A/df_A}{SS_{Error}/df_{Error}}$. This statistic is compared to an $F$-distribution with $df_A$ and $df_{Error}$ degrees of freedom.\n-   **Test for the main effect of Factor B:** The test statistic is $F_B = \\frac{MS_B}{MS_{Error}} = \\frac{SS_B/df_B}{SS_{Error}/df_{Error}}$. This statistic is compared to an $F$-distribution with $df_B$ and $df_{Error}$ degrees of freedom.\n-   **Test for the interaction effect A $\\times$ B:** The test statistic is $F_{AB} = \\frac{MS_{AB}}{MS_{Error}} = \\frac{SS_{AB}/df_{AB}}{SS_{Error}/df_{Error}}$. This statistic is compared to an $F$-distribution with $df_{AB}$ and $df_{Error}$ degrees of freedom.\n\nFinally, we compute the specific values of the degrees of freedom given $a=3$, $b=3$, and $n=15$.\n-   $df_A = a - 1 = 3 - 1 = 2$.\n-   $df_B = b - 1 = 3 - 1 = 2$.\n-   $df_{AB} = (a-1)(b-1) = (3-1)(3-1) = 2 \\times 2 = 4$.\n-   $df_{Error} = ab(n-1) = (3)(3)(15-1) = 9 \\times 14 = 126$.\n-   $df_{Total} = abn - 1 = (3)(3)(15) - 1 = 135 - 1 = 134$.\n\nThe computed degrees of freedom are $2$ for factor $A$, $2$ for factor $B$, $4$ for the interaction $A \\times B$, $126$ for the error, and $134$ for the total.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2  2  4  126  134\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While full factorial designs are comprehensive, they quickly become infeasible as the number of factors grows. Fractional factorial designs offer a resource-efficient alternative for screening many variables, but this efficiency comes at the cost of confounding, or 'aliasing,' where some effects become indistinguishable from others. This exercise requires you to derive the alias structure for a specific fractional factorial design, a crucial skill for choosing an appropriate design and for correctly interpreting its results by knowing precisely which effects are entangled. ",
            "id": "4161390",
            "problem": "A systems neuroscience laboratory plans to study how multiple experimental factors jointly affect mean firing rate in cultured cortical neurons under controlled stimulation. The team considers $5$ binary factors, coded at two levels each using the standard $\\pm 1$ convention: factor $A$ (synaptic blocker present vs. absent), factor $B$ (high vs. low extracellular potassium), factor $C$ (optogenetic stimulation on vs. off), factor $D$ (neuromodulator present vs. absent), and factor $E$ (membrane potential clamp high vs. low). To reduce the number of experimental runs while enabling estimation of main effects under the assumption that most higher-order interactions are negligible, they adopt a fractional factorial design with $2^{5-2}$ runs defined by the generators $D=AB$ and $E=AC$.\n\nStarting only from the core definitions of two-level fractional factorial designs, generators, defining relations, and aliasing via the defining relation, derive the defining relation and the alias structure for the main effects in this design. Using your derived alias structure, determine which two-factor interactions are confounded with main effects.\n\nLet $N$ denote the total number of distinct two-factor interactions that are aliased with at least one main effect in this design. Report the value of $N$. Your final numeric answer should be exact; no rounding is required.",
            "solution": "The problem requires the analysis of a $2^{5-2}$ fractional factorial design. This is a design for $k=5$ factors in $2^{5-2} = 2^3 = 8$ runs. The design is specified by $p=2$ generators, which define how the levels of some factors are determined by the levels of others.\n\nThe factors are denoted by capital letters $A, B, C, D, E$. In the standard $\\pm 1$ notation for factor levels, the generators $D=AB$ and $E=AC$ mean that the column for factor $D$ in the design matrix is the element-wise product of the columns for $A$ and $B$, and the column for $E$ is the product of columns for $A$ and $C$.\n\n**1. Deriving the Defining Relation**\nThe generators can be rewritten in identity form by multiplying both sides by the factors on the right-hand side. Using the property that for any factor $X$, $X^2=I$ (where $I$ is the identity element, a column of $+1$s), we get:\n$D = AB \\implies D(AB) = (AB)(AB) \\implies ABD = A^2B^2 = I \\cdot I = I$. So, the first generator in identity form is $g_1 = ABD$.\n$E = AC \\implies E(AC) = (AC)(AC) \\implies ACE = A^2C^2 = I \\cdot I = I$. So, the second generator in identity form is $g_2 = ACE$.\n\nThe defining relation for the design is a group formed by the identity $I$, the generators, and all their possible products. In this case, we need to find the product of the two generators:\n$g_1 g_2 = (ABD)(ACE) = A^2BCDE = I \\cdot BCDE = BCDE$.\n\nThe complete defining relation is the set of all such \"words\" that are equivalent to the identity $I$. Therefore, the defining relation is:\n$$I = ABD = ACE = BCDE$$\nThis relation defines the confounding pattern of the design. Any effect or interaction multiplied by a word in the defining relation (other than $I$) yields an aliased effect.\n\n**2. Deriving the Alias Structure for Main Effects**\nThe alias structure for any effect is found by multiplying it by each word in the defining relation. The set of resulting terms are all confounded with each other. We are asked to find the alias structure specifically for the main effects ($A, B, C, D, E$).\n\n- **Alias structure for Main Effect A:**\n  Multiply $A$ by the defining relation:\n  $A \\cdot I = A$\n  $A \\cdot (ABD) = A^2BD = BD$\n  $A \\cdot (ACE) = A^2CE = CE$\n  $A \\cdot (BCDE) = ABCDE$\n  So, the main effect $A$ is aliased with the two-factor interactions $BD$ and $CE$, and the five-factor interaction $ABCDE$. We write this as $[A] = A + BD + CE + ABCDE$.\n\n- **Alias structure for Main Effect B:**\n  Multiply $B$ by the defining relation:\n  $B \\cdot I = B$\n  $B \\cdot (ABD) = AB^2D = AD$\n  $B \\cdot (ACE) = ABCE$\n  $B \\cdot (BCDE) = B^2CDE = CDE$\n  So, the main effect $B$ is aliased with the two-factor interaction $AD$ and the three- and four-factor interactions $CDE$ and $ABCE$. We write this as $[B] = B + AD + CDE + ABCE$.\n\n- **Alias structure for Main Effect C:**\n  Multiply $C$ by the defining relation:\n  $C \\cdot I = C$\n  $C \\cdot (ABD) = ABCD$\n  $C \\cdot (ACE) = AC^2E = AE$\n  $C \\cdot (BCDE) = BC^2DE = BDE$\n  So, the main effect $C$ is aliased with the two-factor interaction $AE$ and the three- and four-factor interactions $BDE$ and $ABCD$. We write this as $[C] = C + AE + BDE + ABCD$.\n\n- **Alias structure for Main Effect D:**\n  Multiply $D$ by the defining relation:\n  $D \\cdot I = D$\n  $D \\cdot (ABD) = ABD^2 = AB$\n  $D \\cdot (ACE) = ACDE$\n  $D \\cdot (BCDE) = BCD^2E = BCE$\n  So, the main effect $D$ is aliased with the two-factor interaction $AB$ and the three- and four-factor interactions $BCE$ and $ACDE$. We write this as $[D] = D + AB + BCE + ACDE$.\n\n- **Alias structure for Main Effect E:**\n  Multiply $E$ by the defining relation:\n  $E \\cdot I = E$\n  $E \\cdot (ABD) = ABDE$\n  $E \\cdot (ACE) = ACE^2 = AC$\n  $E \\cdot (BCDE) = BCD$\n  So, the main effect $E$ is aliased with the two-factor interaction $AC$ and the three- and four-factor interactions $BCD$ and $ABDE$. We write this as $[E] = E + AC + BCD + ABDE$.\n\n**3. Identifying Confounded Two-Factor Interactions and Counting Them**\nThe problem asks for $N$, the total number of distinct two-factor interactions that are aliased with at least one main effect. We can extract these directly from the alias structures derived above. Under the common assumption that interactions of three or more factors are negligible, the primary concern is the aliasing of main effects with two-factor interactions.\n\n- From $[A] = A + BD + CE + ABCDE$, the two-factor interactions are $BD$ and $CE$.\n- From $[B] = B + AD + CDE + ABCE$, the two-factor interaction is $AD$.\n- From $[C] = C + AE + BDE + ABCD$, the two-factor interaction is $AE$.\n- From $[D] = D + AB + BCE + ACDE$, the two-factor interaction is $AB$.\n- From $[E] = E + AC + BCD + ABDE$, the two-factor interaction is $AC$.\n\nLet's compile a list of all such unique two-factor interactions:\n1. $AB$ (aliased with $D$)\n2. $AC$ (aliased with $E$)\n3. $AD$ (aliased with $B$)\n4. $AE$ (aliased with $C$)\n5. $BD$ (aliased with $A$)\n6. $CE$ (aliased with $A$)\n\nThese are all distinct two-factor interactions. We now count the number of interactions in this set. There are $6$ such interactions.\n\nThus, the total number of distinct two-factor interactions aliased with a main effect is $N=6$.\nThe design resolution is the length of the shortest word in the defining relation. Here, the shortest words are $ABD$ and $ACE$, both of length $3$. Therefore, this is a Resolution III design. In a Resolution III design, main effects are aliased with two-factor interactions, which is consistent with our findings.",
            "answer": "$$\\boxed{6}$$"
        }
    ]
}