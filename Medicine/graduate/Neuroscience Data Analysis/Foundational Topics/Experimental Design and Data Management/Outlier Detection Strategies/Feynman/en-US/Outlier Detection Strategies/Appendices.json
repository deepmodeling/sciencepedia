{
    "hands_on_practices": [
        {
            "introduction": "While the z-score is a popular and intuitive tool for outlier detection, its reliability hinges on the critical assumption that the underlying data follows a Gaussian distribution. This practice  provides a computational framework to quantify the error—in terms of false positive rates—when this method is applied to the non-negative, skewed data common in fields like neuroscience. By working through this problem, you will gain a concrete understanding of why it is crucial to match your statistical tools to the underlying structure of your data.",
            "id": "4183422",
            "problem": "You are tasked with designing and implementing a program that evaluates the reliability of classical outlier detection using the z-score in the context of calcium imaging fluorescence traces. Calcium imaging signals are nonnegative and often exhibit asymmetric, positively skewed distributions due to sparse, upward transients. The classical z-score outlier detector defines a one-sided high threshold based on the mean and standard deviation, and assumes a Gaussian baseline. When the underlying signal distribution is skewed, this assumption can misestimate the false positive rate.\n\nYour program must operationalize the following steps, expressed purely in mathematical and logical terms, without using domain-specific file formats or external data. All quantities should be treated as dimensionless.\n\n1. Formalize the classical one-sided z-score threshold. Let a scalar random variable $X$ have population mean $ \\mu $ and population standard deviation $ \\sigma $. For a chosen nonnegative constant $ c $, define the classical high outlier threshold as the value $ \\tau $ such that values satisfying $ X > \\tau $ are flagged as outliers, where $ \\tau = \\mu + c \\sigma $.\n\n2. Define the nominal false positive rate under the Gaussian assumption. If the standardized variable $ Z = (X - \\mu)/\\sigma $ were standard normal, the nominal one-sided false positive rate corresponding to the threshold $ \\tau $ would be $ \\mathbb{P}(Z > c) $. This quantity will be used as the nominal benchmark.\n\n3. Quantify the actual false positive rate under a specified skewed distribution. For a specified distribution family and parameters that yield positive skew, compute the actual one-sided false positive rate $ \\mathbb{P}(X > \\tau) $, where $ \\tau $ is computed from the true $ \\mu $ and $ \\sigma $ of that distribution. Use population-level parameters; do not approximate $ \\mu $ or $ \\sigma $ using finite samples.\n\n4. Quantify the error due to skew. Compute the absolute deviation between the nominal benchmark and the actual false positive rate, defined as $ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right| $.\n\n5. Compute the expected number of false alarms for a given trace length. For a positive integer trace length $ T $ (number of independent samples), compute the expected number of false alarms as $ T \\times \\mathbb{P}(X > \\tau) $.\n\nYour program must implement the above for each of the following five test cases, forming the test suite. In each case, the random variable $ X $ follows a specified positively skewed distribution typical of nonnegative fluorescence intensities. For log-normal and gamma distributions, interpret parameters as follows: for the log-normal distribution, $ \\mu_{\\log} $ is the mean of $ \\log X $ and $ \\sigma_{\\log} $ is the standard deviation of $ \\log X $; for the gamma distribution, $ \\kappa $ is the shape and $ \\theta $ is the scale. Use the population moments of each distribution to compute $ \\mu $ and $ \\sigma $. The test suite is:\n\n- Case $1$: Log-normal with $ \\mu_{\\log} = 0 $, $ \\sigma_{\\log} = 0.5 $, $ c = 3 $, $ T = 10000 $.\n- Case $2$: Log-normal with $ \\mu_{\\log} = 0 $, $ \\sigma_{\\log} = 0.5 $, $ c = 0 $, $ T = 10000 $.\n- Case $3$: Gamma with $ \\kappa = 20 $, $ \\theta = 0.05 $, $ c = 3 $, $ T = 100000 $.\n- Case $4$: Gamma with $ \\kappa = 1 $, $ \\theta = 1 $, $ c = 3 $, $ T = 10000 $.\n- Case $5$: Log-normal with $ \\mu_{\\log} = 0 $, $ \\sigma_{\\log} = 1.0 $, $ c = 3 $, $ T = 10000 $.\n\nFor each case, your program must compute:\n- The classical one-sided high threshold $ \\tau $.\n- The nominal one-sided false positive rate $ \\mathbb{P}(Z > c) $ under the Gaussian assumption.\n- The actual one-sided false positive rate $ \\mathbb{P}(X > \\tau) $ under the specified skewed distribution.\n- The absolute error $ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right| $.\n- The expected false alarms $ T \\times \\mathbb{P}(X > \\tau) $.\n\nFinal output format: Your program should produce a single line of output containing the results aggregated for all five test cases as a comma-separated list enclosed in square brackets, with each test case represented by an inner list of the five computed floats in the order $ [\\tau, \\mathbb{P}(Z > c), \\mathbb{P}(X > \\tau), \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right|, T \\times \\mathbb{P}(X > \\tau)] $. For example, a valid output for two hypothetical cases would look like $ [[a_1,b_1,c_1,d_1,e_1],[a_2,b_2,c_2,d_2,e_2]] $, where each symbol denotes a computed float. All probabilities and expectations must be represented as decimals, not percentages.",
            "solution": "The goal is to measure how the classical z-score thresholding, which presumes Gaussianity, deviates when the true signal distribution is positively skewed. We construct the derivation from core definitions of standardization, cumulative distribution functions, and population moments.\n\n1. Threshold construction from the classical z-score. Let $ X $ be a real-valued random variable with population mean $ \\mu $ and population standard deviation $ \\sigma $. The classical z-score of an observation $ x $ is defined as $ z = (x - \\mu)/\\sigma $. For a chosen $ c \\ge 0 $, the classical one-sided high outlier threshold is $ \\tau = \\mu + c \\sigma $, since $ x $ is flagged if $ z > c $, equivalently $ x > \\mu + c \\sigma $.\n\n2. Nominal false positive rate under Gaussian assumption. If the standardized variable $ Z = (X - \\mu)/\\sigma $ were standard normal, $ Z \\sim \\mathcal{N}(0,1) $, then the probability of incorrectly flagging a baseline observation as an outlier at threshold $ c $ is the standard normal survival probability $ \\mathbb{P}(Z > c) $. Using the cumulative distribution function $ \\Phi $ of $ \\mathcal{N}(0,1) $, this is $ 1 - \\Phi(c) $. This nominal benchmark depends only on $ c $.\n\n3. Actual false positive rate under the true skewed distribution. The actual baseline distribution may be positively skewed. We consider two well-tested families that model nonnegative skew common in calcium imaging:\n\n   - Log-normal: If $ Y = \\log X $ is normal with mean $ \\mu_{\\log} $ and standard deviation $ \\sigma_{\\log} $, then $ X $ is log-normal. Its population mean and variance follow from properties of the exponential of a normal variable. Specifically,\n     $$ \\mu = \\mathbb{E}[X] = \\exp\\left(\\mu_{\\log} + \\frac{\\sigma_{\\log}^{2}}{2}\\right), $$\n     $$ \\operatorname{Var}(X) = \\left(\\exp(\\sigma_{\\log}^{2}) - 1\\right) \\exp\\left(2 \\mu_{\\log} + \\sigma_{\\log}^{2}\\right), $$\n     so the population standard deviation is $ \\sigma = \\sqrt{\\operatorname{Var}(X)} $. The survival function $ \\mathbb{P}(X > x) $ is given by the log-normal survival function evaluated at $ x $ with parameters $ \\mu_{\\log} $ and $ \\sigma_{\\log} $. For the threshold $ \\tau = \\mu + c \\sigma $, the actual false positive rate is $ \\mathbb{P}(X > \\tau) $ from the log-normal survival.\n\n   - Gamma: If $ X $ follows a gamma distribution with shape $ \\kappa $ and scale $ \\theta $, its population mean and variance are\n     $$ \\mu = \\kappa \\theta, \\quad \\operatorname{Var}(X) = \\kappa \\theta^{2}, $$\n     giving $ \\sigma = \\sqrt{\\kappa} \\theta $. The survival function $ \\mathbb{P}(X > x) $ is given by the gamma survival function evaluated at $ x $ with parameters $ \\kappa $ and $ \\theta $. For $ \\tau = \\mu + c \\sigma $, the actual false positive rate is $ \\mathbb{P}(X > \\tau) $ from the gamma survival.\n\n   For both families, we rely on the well-tested population moment formulas and standard survival functions from probability theory, ensuring population-level calculations that avoid finite-sample variability.\n\n4. Error quantification due to skew. The absolute error between the classical Gaussian benchmark and the actual skewed distribution is\n   $$ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right|. $$\n   This measures the miscalibration when Gaussian thresholds are applied to skewed data. For one-sided high thresholds, calcium imaging’s positive transients make this relevant.\n\n5. Expected false alarms for a trace length. Assuming independent samples, the expected number of baseline samples exceeding the threshold in a trace of length $ T $ is\n   $$ T \\times \\mathbb{P}(X > \\tau). $$\n   This converts probabilities into expected counts, providing a tangible measure of performance in analysis pipelines.\n\n6. Test suite coverage. We choose five cases to probe different facets:\n   - Case $1$ (happy path): Log-normal with moderate skew ($ \\mu_{\\log} = 0 $, $ \\sigma_{\\log} = 0.5 $) and $ c = 3 $ to evaluate realistic mismatch between nominal and actual false positive rates.\n   - Case $2$ (boundary condition): Same log-normal but $ c = 0 $, yielding $ \\tau = \\mu $; the nominal probability is $ \\mathbb{P}(Z > 0) = 0.5 $, while the actual probability $ \\mathbb{P}(X > \\mu) $ reflects the fact that, for positively skewed distributions, the mean exceeds the median, so the probability of being above the mean is typically less than $ 0.5 $.\n   - Case $3$ (near-normal): Gamma with $ \\kappa = 20 $ and $ \\theta = 0.05 $ has mean $ \\mu = 1 $ and variance $ \\operatorname{Var}(X) = 0.05 $, resulting in relatively small skew; the Gaussian nominal and actual should be close when $ c = 3 $.\n   - Case $4$ (extreme skew): Gamma with $ \\kappa = 1 $ and $ \\theta = 1 $ is exponential-like, producing substantial skew; a $ c = 3 $ threshold will deviate significantly between nominal and actual.\n   - Case $5$ (heavy skew in log-normal): Log-normal with $ \\mu_{\\log} = 0 $ and $ \\sigma_{\\log} = 1.0 $ substantially increases skew and heavy tails, testing robustness of the Gaussian assumption at $ c = 3 $.\n\n7. Output specification. For each case, compute $ \\tau $, $ \\mathbb{P}(Z > c) $, $ \\mathbb{P}(X > \\tau) $, $ \\left| \\mathbb{P}(Z > c) - \\mathbb{P}(X > \\tau) \\right| $, and $ T \\times \\mathbb{P}(X > \\tau) $. Aggregate all five cases into a single list of lists and print it as one line in the requested format. All probabilities and expectations must be decimals without percentage signs.\n\nThe program uses the standard normal survival function for $ \\mathbb{P}(Z > c) $, and the log-normal and gamma survival functions for $ \\mathbb{P}(X > \\tau) $, ensuring accurate population-level computations for the specified parameters.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, lognorm, gamma\n\ndef compute_lognormal_moments(mu_log: float, sigma_log: float):\n    \"\"\"\n    Compute population mean and std for a log-normal with log-mean mu_log\n    and log-std sigma_log.\n    \"\"\"\n    # Mean: exp(mu_log + 0.5 * sigma_log^2)\n    mean = np.exp(mu_log + 0.5 * (sigma_log ** 2))\n    # Variance: (exp(sigma_log^2) - 1) * exp(2*mu_log + sigma_log^2)\n    var = (np.exp(sigma_log ** 2) - 1.0) * np.exp(2.0 * mu_log + (sigma_log ** 2))\n    std = np.sqrt(var)\n    return mean, std\n\ndef compute_gamma_moments(kappa: float, theta: float):\n    \"\"\"\n    Compute population mean and std for a gamma with shape kappa and scale theta.\n    \"\"\"\n    mean = kappa * theta\n    var = kappa * (theta ** 2)\n    std = np.sqrt(var)\n    return mean, std\n\ndef nominal_fp_rate(c: float) -> float:\n    \"\"\"\n    Nominal one-sided false positive rate under N(0,1): P(Z > c).\n    \"\"\"\n    return float(norm.sf(c))\n\ndef actual_fp_rate_lognorm(tau: float, mu_log: float, sigma_log: float) -> float:\n    \"\"\"\n    Actual one-sided false positive rate for log-normal: P(X > tau).\n    SciPy's lognorm parameterization uses shape=sigma_log, scale=exp(mu_log).\n    \"\"\"\n    s = sigma_log\n    scale = np.exp(mu_log)\n    return float(lognorm.sf(tau, s=s, scale=scale))\n\ndef actual_fp_rate_gamma(tau: float, kappa: float, theta: float) -> float:\n    \"\"\"\n    Actual one-sided false positive rate for gamma: P(X > tau).\n    SciPy's gamma parameterization uses a=shape (kappa), scale=theta.\n    \"\"\"\n    return float(gamma.sf(tau, a=kappa, scale=theta))\n\ndef format_nested_list_no_spaces(nested):\n    \"\"\"\n    Format a nested list into a compact string representation with no spaces.\n    \"\"\"\n    def format_item(x):\n        if isinstance(x, list):\n            return \"[\" + \",\".join(format_item(y) for y in x) + \"]\"\n        else:\n            # Ensure standard float/integer string without spaces\n            return str(x)\n    return format_item(nested)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: dict with keys 'dist', parameters, 'c', 'T'.\n    test_cases = [\n        # Case 1: Log-normal moderate skew\n        {\"dist\": \"lognorm\", \"mu_log\": 0.0, \"sigma_log\": 0.5, \"c\": 3.0, \"T\": 10000},\n        # Case 2: Log-normal boundary c=0\n        {\"dist\": \"lognorm\", \"mu_log\": 0.0, \"sigma_log\": 0.5, \"c\": 0.0, \"T\": 10000},\n        # Case 3: Gamma near-normal (large shape)\n        {\"dist\": \"gamma\", \"kappa\": 20.0, \"theta\": 0.05, \"c\": 3.0, \"T\": 100000},\n        # Case 4: Gamma extreme skew (exponential-like)\n        {\"dist\": \"gamma\", \"kappa\": 1.0, \"theta\": 1.0, \"c\": 3.0, \"T\": 10000},\n        # Case 5: Log-normal heavy skew\n        {\"dist\": \"lognorm\", \"mu_log\": 0.0, \"sigma_log\": 1.0, \"c\": 3.0, \"T\": 10000},\n    ]\n\n    results = []\n    for case in test_cases:\n        c = float(case[\"c\"])\n        T = int(case[\"T\"])\n\n        if case[\"dist\"] == \"lognorm\":\n            mu_log = float(case[\"mu_log\"])\n            sigma_log = float(case[\"sigma_log\"])\n            # Population moments\n            mu, sigma = compute_lognormal_moments(mu_log, sigma_log)\n            tau = mu + c * sigma\n            p_nom = nominal_fp_rate(c)\n            p_act = actual_fp_rate_lognorm(tau, mu_log, sigma_log)\n        elif case[\"dist\"] == \"gamma\":\n            kappa = float(case[\"kappa\"])\n            theta = float(case[\"theta\"])\n            # Population moments\n            mu, sigma = compute_gamma_moments(kappa, theta)\n            tau = mu + c * sigma\n            p_nom = nominal_fp_rate(c)\n            p_act = actual_fp_rate_gamma(tau, kappa, theta)\n        else:\n            # Should not happen in the defined test suite.\n            raise ValueError(\"Unsupported distribution type\")\n\n        abs_err = abs(p_nom - p_act)\n        exp_false_alarms = T * p_act\n\n        results.append([tau, p_nom, p_act, abs_err, exp_false_alarms])\n\n    # Final print statement in the exact required format: single line, no spaces.\n    print(format_nested_list_no_spaces(results))\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond single-variable analysis, this practice  introduces a cornerstone of multivariate outlier detection: the Mahalanobis distance. This powerful metric accounts for the correlations between features, allowing for the detection of anomalies that are not extreme on any single axis but are unusual in their combination. You will implement this method from first principles and learn how to establish a statistically-grounded detection threshold using the $\\chi^2$ distribution, a standard technique in fields analyzing high-dimensional data like EEG.",
            "id": "4183436",
            "problem": "You are given several numerical datasets, each representing features extracted from Independent Component Analysis (ICA) of Electroencephalography (EEG) components. For each dataset, treat the rows as observations (components) and the columns as features. Your task is to implement a principled multivariate outlier detection procedure using the Mahalanobis distance and the chi-square distribution, starting only from foundational statistical definitions. You must produce a program that, for each dataset, flags the indices (zero-based) of observations whose squared Mahalanobis distance exceeds a chi-square quantile-based threshold, and then aggregates the flagged-index lists across all datasets into a single top-level list.\n\nFundamental base and definitions to use:\n- Let $X \\in \\mathbb{R}^{n \\times p}$ denote the feature matrix with $n$ observations and $p$ features. Let $\\mathbf{x}_i \\in \\mathbb{R}^p$ be the $i$-th row of $X$.\n- The sample mean vector is $\\boldsymbol{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{x}_i$.\n- The unbiased sample covariance matrix is $\\boldsymbol{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^{n} (\\mathbf{x}_i - \\boldsymbol{\\mu})(\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top}$.\n- In case $\\boldsymbol{\\Sigma}$ is singular or ill-conditioned, use the Moore–Penrose pseudoinverse, denoted $\\boldsymbol{\\Sigma}^{+}$.\n- The squared Mahalanobis distance of $\\mathbf{x}_i$ is $\\delta_i^2 = (\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{+} (\\mathbf{x}_i - \\boldsymbol{\\mu})$.\n- Under the assumption that the features are approximately jointly Gaussian and well-modeled by $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$, the statistic $\\delta_i^2$ is compared to the upper quantile of a chi-square distribution with $p$ degrees of freedom. For a given significance level $\\alpha \\in (0,1)$, define the threshold $\\tau = F^{-1}_{\\chi^2_p}(1-\\alpha)$, where $F^{-1}_{\\chi^2_p}$ is the inverse cumulative distribution function (quantile function) of the chi-square distribution with $p$ degrees of freedom. Flag observation $i$ if $\\delta_i^2 > \\tau$.\n\nYour program must implement the above procedure with the following test suite (each test case is a pair $(X, \\alpha)$). All indices are zero-based, and you must return the flagged indices in strictly increasing order for each case.\n\nTest case $1$ (happy path, moderate dimension $p = 4$):\n- $X \\in \\mathbb{R}^{8 \\times 4}$ with rows\n  - $[0.2, -0.1, 0.3, 0.0]$\n  - $[-0.3, 0.4, -0.2, 0.5]$\n  - $[0.1, 0.2, 0.0, -0.1]$\n  - $[-0.2, -0.3, 0.1, 0.2]$\n  - $[0.0, 0.1, -0.1, 0.1]$\n  - $[0.3, -0.2, 0.2, -0.2]$\n  - $[6.0, 6.5, 5.8, 6.2]$\n  - $[4.0, -4.5, 3.8, -4.2]$\n- $\\alpha = 0.01$.\n\nTest case $2$ (near-singular covariance, $p = 3$):\n- $X \\in \\mathbb{R}^{5 \\times 3}$ with rows\n  - $[-1.0, -2.0, 1.0]$\n  - $[0.0, 0.1, 0.0]$\n  - $[1.0, 2.1, -1.1]$\n  - $[1.1, 2.2, -1.05]$\n  - $[5.0, 10.2, -5.1]$\n- $\\alpha = 0.05$.\n\nTest case $3$ (small sample $n = p = 3$):\n- $X \\in \\mathbb{R}^{3 \\times 3}$ with rows\n  - $[0.0, 0.0, 0.0]$\n  - $[0.2, -0.1, 0.1]$\n  - $[3.5, 3.5, 3.5]$\n- $\\alpha = 0.05$.\n\nTest case $4$ (no outliers under a stringent threshold, $p = 2$):\n- $X \\in \\mathbb{R}^{6 \\times 2}$ with rows\n  - $[0.1, -0.2]$\n  - $[-0.2, 0.3]$\n  - $[0.2, -0.1]$\n  - $[0.0, 0.0]$\n  - $[0.3, 0.2]$\n  - $[-0.1, -0.3]$\n- $\\alpha = 0.001$.\n\nRequired output:\n- For each test case, compute the set $\\{\\,i \\in \\{0,1,\\dots,n-1\\} \\mid \\delta_i^2 > \\tau \\,\\}$, return it as a list of indices in ascending order.\n- Aggregate these per-case lists into a single top-level list in the same order as the test cases.\n- Your program should produce a single line of output containing this aggregated list, formatted as a comma-separated list enclosed in square brackets with no spaces, for example: $[[],[0,2]]$. Note that there must be no spaces anywhere in the printed line.\n- All quantities are pure numbers. Do not use percentage symbols; significance levels must be treated as decimals such as $\\alpha = 0.05$.\n\nAssumptions:\n- A pseudoinverse $\\boldsymbol{\\Sigma}^{+}$ must be used whenever needed. You may use the singular value decomposition-based pseudoinverse or a numerically stable equivalent.\n- Angles and physical units do not apply in this problem.\n\nYour code must be a complete, runnable program that implements the above and prints the final result in the specified format.",
            "solution": "The problem requires the implementation of a principled multivariate outlier detection algorithm based on the Mahalanobis distance. This statistical procedure is particularly well-suited for identifying anomalous observations in a multidimensional feature space, such as those derived from Independent Component Analysis (ICA) of Electroencephalography (EEG) data. The method leverages the covariance structure of the data to define a distance metric that is invariant to linear transformations of the feature space. The core logic involves modeling the data distribution and flagging points that lie in low-probability regions of that distribution.\n\nThe solution is developed from first principles as follows:\n\n1.  **Statistical Parameter Estimation**: For a given dataset represented by a matrix $X \\in \\mathbb{R}^{n \\times p}$, with $n$ observations and $p$ features, the first step is to estimate the parameters of the underlying data distribution. We assume the data can be adequately described by its first two moments: the center and the spread.\n    -   The center of the data cloud is estimated by the **sample mean vector**, $\\boldsymbol{\\mu} \\in \\mathbb{R}^p$. It is calculated as the arithmetic average of the observation vectors:\n        $$ \\boldsymbol{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{x}_i $$\n        where $\\mathbf{x}_i$ is the $i$-th row (observation) of $X$.\n    -   The spread, and more importantly, the correlation structure between the features, is estimated by the **unbiased sample covariance matrix**, $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$. It is defined as:\n        $$ \\boldsymbol{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^{n} (\\mathbf{x}_i - \\boldsymbol{\\mu})(\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} $$\n        The denominator $n-1$ represents Bessel's correction, which yields an unbiased estimate of the true population covariance.\n\n2.  **Mahalanobis Distance Calculation**: The Mahalanobis distance provides a measure of how far a point $\\mathbf{x}_i$ is from the center of the data mass, taking into account the covariance of the dataset. The squared Mahalanobis distance is given by:\n    $$ \\delta_i^2 = (\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}) $$\n    A critical issue arises when the covariance matrix $\\boldsymbol{\\Sigma}$ is singular (non-invertible) or ill-conditioned. This is guaranteed to happen if the number of observations is less than or equal to the number of features ($n \\le p$), as the rank of $\\boldsymbol{\\Sigma}$ will be at most $n-1$. To handle this, the standard matrix inverse $\\boldsymbol{\\Sigma}^{-1}$ is replaced by the **Moore-Penrose pseudoinverse**, denoted $\\boldsymbol{\\Sigma}^{+}$. The pseudoinverse provides a stable and unique generalized inverse for any matrix. The robust formula for the squared Mahalanobis distance is therefore:\n    $$ \\delta_i^2 = (\\mathbf{x}_i - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{+} (\\mathbf{x}_i - \\boldsymbol{\\mu}) $$\n\n3.  **Outlier Thresholding via the Chi-Square Distribution**: The final step is to determine which distances are large enough to be considered\n    anomalous. Under the assumption that the data are drawn from a multivariate normal distribution, the squared Mahalanobis distance statistic, $\\delta^2$, follows a chi-square ($\\chi^2$) distribution with $p$ degrees of freedom, where $p$ is the dimensionality of the data. This statistical property allows us to establish a formal hypothesis test for outlier detection.\n    -   Given a significance level $\\alpha$, which represents the probability of falsely classifying a typical observation as an outlier, we define a critical value, or threshold, $\\tau$.\n    -   This threshold is derived from the inverse cumulative distribution function (CDF), or quantile function, of the chi-square distribution with $p$ degrees of freedom ($df=p$). Specifically, we select the upper $(1-\\alpha)$ quantile:\n        $$ \\tau = F^{-1}_{\\chi^2_p}(1-\\alpha) $$\n    -   An observation $\\mathbf{x}_i$ is flagged as an outlier if its squared Mahalanobis distance exceeds this threshold:\n        $$ \\delta_i^2 > \\tau $$\n\n**Algorithmic Procedure**\n\nFor each test case, consisting of a data matrix $X$ and a significance level $\\alpha$, the following computational steps are executed:\n1.  Determine the number of observations $n$ and features $p$ from the dimensions of $X$.\n2.  Compute the mean vector $\\boldsymbol{\\mu}$ of shape $(p,)$.\n3.  Compute the unbiased sample covariance matrix $\\boldsymbol{\\Sigma}$ of shape $(p,p)$.\n4.  Compute the Moore-Penrose pseudoinverse $\\boldsymbol{\\Sigma}^{+}$ of the covariance matrix.\n5.  Center the data by subtracting the mean vector from each observation, creating a matrix of deviations $X - \\boldsymbol{\\mu}$.\n6.  Calculate the squared Mahalanobis distance $\\delta_i^2$ for each observation $i=0, \\dots, n-1$.\n7.  Calculate the threshold $\\tau$ using the quantile function of the $\\chi^2$ distribution with $p$ degrees of freedom at probability $1-\\alpha$.\n8.  Identify all zero-based indices $i$ for which $\\delta_i^2 > \\tau$.\n9.  Return these indices as a sorted list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\nfrom scipy.linalg import pinv\n\ndef solve():\n    \"\"\"\n    Solves the multivariate outlier detection problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [0.2, -0.1, 0.3, 0.0],\n            [-0.3, 0.4, -0.2, 0.5],\n            [0.1, 0.2, 0.0, -0.1],\n            [-0.2, -0.3, 0.1, 0.2],\n            [0.0, 0.1, -0.1, 0.1],\n            [0.3, -0.2, 0.2, -0.2],\n            [6.0, 6.5, 5.8, 6.2],\n            [4.0, -4.5, 3.8, -4.2]\n        ]), 0.01),\n        (np.array([\n            [-1.0, -2.0, 1.0],\n            [0.0, 0.1, 0.0],\n            [1.0, 2.1, -1.1],\n            [1.1, 2.2, -1.05],\n            [5.0, 10.2, -5.1]\n        ]), 0.05),\n        (np.array([\n            [0.0, 0.0, 0.0],\n            [0.2, -0.1, 0.1],\n            [3.5, 3.5, 3.5]\n        ]), 0.05),\n        (np.array([\n            [0.1, -0.2],\n            [-0.2, 0.3],\n            [0.2, -0.1],\n            [0.0, 0.0],\n            [0.3, 0.2],\n            [-0.1, -0.3]\n        ]), 0.001)\n    ]\n\n    all_results = []\n    for X, alpha in test_cases:\n        # 1. Get dimensions n (observations) and p (features).\n        n, p = X.shape\n\n        # 2. Compute the sample mean vector.\n        mu = np.mean(X, axis=0)\n\n        # 3. Compute the unbiased sample covariance matrix.\n        # np.cov with rowvar=False treats columns as variables.\n        # The default ddof=1 computes the unbiased sample covariance (divides by n-1).\n        # Handle the case where n <= p, np.cov may fail if n=1. \n        # For n=1, covariance is undefined. Problem constraints ensure n>1.\n        if n > 1:\n            cov = np.cov(X, rowvar=False)\n        else: # Covariance is ill-defined for a single point\n            cov = np.zeros((p, p))\n\n        # 4. Compute the Moore-Penrose pseudoinverse.\n        # This handles singular and ill-conditioned matrices, which is essential\n        # when n <= p, as the covariance matrix will be singular.\n        cov_inv = pinv(cov)\n        \n        # 5. Center the data.\n        X_centered = X - mu\n\n        # 6. Calculate squared Mahalanobis distances.\n        # A numerically efficient way is to compute (X_c @ C_inv) * X_c and sum rows.\n        # This is equivalent to diag(X_c @ C_inv @ X_c.T) but avoids computing the full n x n matrix.\n        mahal_sq_distances = np.sum((X_centered @ cov_inv) * X_centered, axis=1)\n\n        # 7. Compute the chi-square threshold.\n        # The degrees of freedom (df) is the number of features, p.\n        # We need the (1 - alpha) quantile of the chi-square distribution.\n        threshold = chi2.ppf(1 - alpha, df=p)\n\n        # 8. Find indices where distance exceeds the threshold.\n        # np.where returns a tuple of arrays; we take the first element.\n        outlier_indices = np.where(mahal_sq_distances > threshold)[0]\n\n        # 9. Store the sorted list of indices.\n        all_results.append(outlier_indices.tolist())\n\n    # Final print statement in the exact required format.\n    # str() creates a string representation '[[], [1, 2]]'\n    # .replace(\" \", \"\") removes all spaces to match the format '[[],[1,2]]'\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Classical statistical estimators like the sample mean and covariance are themselves sensitive to outliers, a problem known as the \"masking effect\" where extreme points can distort the very parameters used to detect them. This exercise  demonstrates a powerful solution using robust statistics, specifically the Minimum Covariance Determinant (MCD) estimator. By directly comparing the performance of classical Mahalanobis distance with its robust counterpart, you will see firsthand how robust methods can unmask outliers in contaminated datasets where classical approaches may fail.",
            "id": "4183406",
            "problem": "A researcher is analyzing multi-channel Local Field Potentials (LFP) feature vectors to detect artifacts and physiological anomalies that manifest as outliers. Each observation is a $p$-dimensional vector representing features derived from LFP signals. The researcher wants to compare outlier detection when using the classical covariance estimator against a robust strategy based on the Minimum Covariance Determinant (MCD) and compute Mahalanobis distances for both. The objective is to quantify the difference in flagged outliers between the two strategies.\n\nStarting from the following foundational definitions and facts:\n- The Mahalanobis distance between a point $\\mathbf{x} \\in \\mathbb{R}^p$ and a distribution characterized by mean $\\boldsymbol{\\mu}$ and positive definite covariance matrix $\\boldsymbol{\\Sigma}$ is defined by $d(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\sqrt{(\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})}$.\n- When the data are well-modeled by a multivariate normal distribution with dimension $p$, the squared Mahalanobis distances approximately follow a chi-square distribution with $p$ degrees of freedom, so that $d(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})^2 \\sim \\chi^2_p$ under the ideal model.\n- The Minimum Covariance Determinant (MCD) estimator is a robust estimator of location and scatter that seeks a subset of size $h$ (with $h \\in \\{p+1, \\dots, n\\}$) whose covariance determinant is minimal. The robust location is the mean of that subset and the robust scatter is its covariance.\n\nImplement the following tasks for each test dataset:\n1. Construct the dataset by sampling inliers from a multivariate normal distribution with given mean $\\boldsymbol{\\mu}$ and covariance $\\boldsymbol{\\Sigma}$, and by appending a specified number of outliers formed by adding a deterministic offset vector $\\boldsymbol{\\delta}$ plus small independent Gaussian noise to the mean $\\boldsymbol{\\mu}$. The total number of samples is $n$. Use the specified random seed for reproducibility.\n2. Compute the classical Mahalanobis distances using the full-sample mean and full-sample covariance.\n3. Compute the robust Mahalanobis distances using the MCD estimator obtained by exact combinatorial search: set $h = \\lfloor \\alpha n \\rfloor$ with $\\alpha = 0.75$, ensure $h \\ge p+1$, enumerate all subsets of size $h$, and select the subset whose sample covariance has the smallest determinant (ignore subsets whose covariance is singular or near-singular). Use the mean and covariance of this subset as the robust location and scatter, respectively.\n4. For both classical and robust distances, flag an observation as an outlier if its distance $d$ exceeds the threshold $\\tau_p = \\sqrt{F^{-1}_{\\chi^2_p}(0.975)}$, where $F^{-1}_{\\chi^2_p}$ denotes the inverse cumulative distribution function of the chi-square distribution with $p$ degrees of freedom at probability $0.975$.\n5. For each dataset, compute a single integer metric equal to the difference between the number of outliers flagged by the robust strategy and the number flagged by the classical strategy.\n\nYour program must implement the above without any user input, and must produce a single line of output containing the three integer results as a comma-separated list enclosed in square brackets.\n\nTest suite specification:\n- Case A (happy path with moderate dimension and a couple of strong outliers):\n  - $n = 10$, $p = 3$,\n  - $\\boldsymbol{\\mu} = [0.0, 0.0, 0.0]$,\n  - $\\boldsymbol{\\Sigma} = \\text{diag}([1.0, 0.5, 0.8])$,\n  - number of outliers $= 2$,\n  - $\\boldsymbol{\\delta} = [9.0, 9.0, 9.0]$,\n  - random seed $= 1234$.\n- Case B (higher dimension with correlated features and several outliers):\n  - $n = 12$, $p = 4$,\n  - $\\boldsymbol{\\mu} = [0.3, -0.2, 0.0, 0.1]$,\n  - build $\\boldsymbol{\\Sigma}$ as $\\boldsymbol{\\Sigma} = \\mathbf{A}\\mathbf{A}^\\top$ with\n    $\\mathbf{A} = \\begin{bmatrix}\n    1.0 & 0.2 & -0.1 & 0.0 \\\\\n    0.0 & 0.9 & 0.3 & -0.2 \\\\\n    0.1 & 0.0 & 1.1 & 0.2 \\\\\n    0.0 & -0.1 & 0.2 & 0.8\n    \\end{bmatrix}$,\n  - number of outliers $= 3$,\n  - $\\boldsymbol{\\delta} = [7.0, -8.0, 9.0, -7.0]$,\n  - random seed $= 42$.\n- Case C (boundary-like small sample size and a single extreme outlier):\n  - $n = 9$, $p = 3$,\n  - $\\boldsymbol{\\mu} = [0.0, 0.0, 0.0]$,\n  - build $\\boldsymbol{\\Sigma}$ as $\\boldsymbol{\\Sigma} = \\mathbf{B}\\mathbf{B}^\\top$ with\n    $\\mathbf{B} = \\begin{bmatrix}\n    1.0 & 0.4 & 0.0 \\\\\n    0.2 & 0.8 & 0.3 \\\\\n    0.0 & 0.1 & 0.9\n    \\end{bmatrix}$,\n  - number of outliers $= 1$,\n  - $\\boldsymbol{\\delta} = [10.0, 0.0, -10.0]$,\n  - random seed $= 2021$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_A,r_B,r_C]$), where $r_A$, $r_B$, and $r_C$ are the integer differences (robust minus classical flagged counts) for Cases A, B, and C, respectively. No other text should be printed.",
            "solution": "The problem requires a comparison between a classical and a robust statistical method for outlier detection in multivariate data. The core of the task is to implement both methods, apply them to synthetically generated datasets, and quantify the difference in their performance. The analysis hinges on the Mahalanobis distance, a fundamental tool in multivariate statistics.\n\n### Principle 1: The Mahalanobis Distance and Outlier Detection\n\nAn outlier is an observation that deviates significantly from other observations in a sample. For $p$-dimensional data, this deviation must be assessed in a $p$-dimensional space. The Mahalanobis distance provides a sound basis for this assessment. Given a dataset assumed to be drawn from a distribution with mean $\\boldsymbol{\\mu} \\in \\mathbb{R}^p$ and a positive definite covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$, the Mahalanobis distance of a point $\\mathbf{x} \\in \\mathbb{R}^p$ from the center of the distribution is defined as:\n$$\nd(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\sqrt{(\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})}\n$$\nThis distance is superior to the Euclidean distance because it is scale-invariant and accounts for the correlation structure of the data. It effectively measures the distance of a point from the mean in units of standard deviation along the principal component axes of the data.\n\nUnder the null hypothesis that the data follows a multivariate normal distribution $N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, the squared Mahalanobis distances, $d^2$, are known to follow a chi-square distribution with $p$ degrees of freedom, i.e., $d^2 \\sim \\chi^2_p$. This statistical property allows us to establish a formal threshold for outlier detection. An observation is flagged as an outlier if its distance exceeds a critical value derived from the $\\chi^2_p$ distribution. The problem specifies a threshold corresponding to a probability of $0.975$, so the threshold distance is $\\tau_p = \\sqrt{F^{-1}_{\\chi^2_p}(0.975)}$, where $F^{-1}_{\\chi^2_p}$ is the quantile function (inverse CDF) of the $\\chi^2_p$ distribution.\n\n### Principle 2: The Classical Approach and its Limitations\n\nIn practice, the true population parameters $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ are unknown and must be estimated from the data sample $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$. The classical approach uses the sample mean $\\hat{\\boldsymbol{\\mu}}$ and sample covariance matrix $\\hat{\\boldsymbol{\\Sigma}}$ as estimators:\n$$\n\\hat{\\boldsymbol{\\mu}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i \\quad \\text{and} \\quad \\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{n-1} \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})^\\top\n$$\nThe classical Mahalanobis distance is then computed as $d(\\mathbf{x}_i; \\hat{\\boldsymbol{\\mu}}, \\hat{\\boldsymbol{\\Sigma}})$.\n\nThe critical flaw of this approach is that the estimators $\\hat{\\boldsymbol{\\mu}}$ and $\\hat{\\boldsymbol{\\Sigma}}$ are themselves highly sensitive to outliers. A single extreme outlier can pull the sample mean towards it and inflate the sample covariance, particularly along the direction of the outlier. This phenomenon is known as the **masking effect**: the presence of outliers contaminates the estimates of location and scatter, which in turn can cause the Mahalanobis distances of the outliers to be artificially small, potentially preventing their detection.\n\n### Principle 3: The Robust Approach using the Minimum Covariance Determinant (MCD)\n\nTo overcome the masking effect, robust estimators of location and scatter are employed. The Minimum Covariance Determinant (MCD) estimator is a prominent robust method. The goal of MCD is to find a subset of $h$ observations (where $p+1 \\le h \\le n$) out of the total $n$ whose classical covariance matrix has the smallest possible determinant. The intuition is that the determinant of a covariance matrix is proportional to the squared volume of the ellipsoid containing the data. Finding the subset with the minimum determinant is equivalent to finding the most concentrated \"core\" group of $h$ points. This core subset is assumed to be free of outliers.\n\nFollowing the problem specification, the robust estimates of location, $\\boldsymbol{\\mu}_{\\text{MCD}}$, and scatter, $\\boldsymbol{\\Sigma}_{\\text{MCD}}$, are simply the sample mean and sample covariance of this optimal $h$-point subset. The robust Mahalanobis distances are then computed for all $n$ points using these robust estimates: $d(\\mathbf{x}_i; \\boldsymbol{\\mu}_{\\text{MCD}}, \\boldsymbol{\\Sigma}_{\\text{MCD}})$. Because $\\boldsymbol{\\mu}_{\\text{MCD}}$ and $\\boldsymbol{\\Sigma}_{\\text{MCD}}$ are not influenced by the outliers, the resulting distances will be large for the true outliers and small for the inliers, leading to a much more reliable separation.\n\n### Algorithmic Implementation\n\nThe solution is implemented by following these steps for each test case:\n\n1.  **Data Generation**: The dataset $\\mathbf{X}$ of $n$ points is constructed. First, $n_{inliers} = n - n_{outliers}$ are sampled from the specified multivariate normal distribution $N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$. Second, $n_{outliers}$ are generated by sampling from a distribution centered at a significantly shifted mean, $N(\\boldsymbol{\\mu} + \\boldsymbol{\\delta}, \\sigma^2 \\mathbf{I})$, where $\\sigma^2$ is a small variance (e.g., $0.01$) to introduce minor variations as per the problem's \"small independent Gaussian noise\" clause. A specific random seed is used for reproducibility.\n\n2.  **Classical Analysis**: The classical sample mean $\\hat{\\boldsymbol{\\mu}}$ and covariance $\\hat{\\boldsymbol{\\Sigma}}$ are computed from the full dataset $\\mathbf{X}$. The inverse covariance matrix $\\hat{\\boldsymbol{\\Sigma}}^{-1}$ is calculated, and the classical Mahalanobis distance $d_{classical}$ is computed for each of the $n$ points.\n\n3.  **Robust MCD Analysis**: The parameter $h$ is set to $\\lfloor \\alpha n \\rfloor$ with $\\alpha=0.75$. An exact combinatorial search is performed over all $\\binom{n}{h}$ subsets of indices of size $h$. For each subset:\n    *   Its sample covariance matrix is computed.\n    *   The determinant of this covariance matrix is calculated. Subsets with a determinant near zero are discarded as they are singular or ill-conditioned.\n    *   The subset that yields the minimum determinant is identified. The mean and covariance of this optimal subset are stored as $\\boldsymbol{\\mu}_{\\text{MCD}}$ and $\\boldsymbol{\\Sigma}_{\\text{MCD}}$.\n\n    The inverse of the robust covariance matrix, $\\boldsymbol{\\Sigma}_{\\text{MCD}}^{-1}$, is then used to compute the robust Mahalanobis distances $d_{robust}$ for all $n$ points in the original dataset $\\mathbf{X}$.\n\n4.  **Outlier Flagging and Comparison**: The detection threshold $\\tau_p = \\sqrt{F^{-1}_{\\chi^2_p}(0.975)}$ is calculated using the inverse CDF of the chi-square distribution. The number of outliers flagged by each method is determined by counting how many points have a Mahalanobis distance greater than $\\tau_p$.\n    *   $N_{classical} = |\\{ i \\mid d_{classical}(\\mathbf{x}_i) > \\tau_p \\}|$\n    *   $N_{robust} = |\\{ i \\mid d_{robust}(\\mathbf{x}_i) > \\tau_p \\}|$\n\n5.  **Metric Calculation**: The final result for the test case is the integer difference $N_{robust} - N_{classical}$. This metric quantifies how many more (or fewer) outliers the robust method identifies compared to the classical one. A positive value indicates that the robust method is more sensitive, likely by successfully overcoming the masking effect that hampers the classical method.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves the outlier detection problem for three specified test cases.\n    \"\"\"\n\n    # Case A: Happy path with moderate dimension and a couple of strong outliers\n    case_a = {\n        \"n\": 10, \"p\": 3,\n        \"mu\": [0.0, 0.0, 0.0],\n        \"Sigma\": np.diag([1.0, 0.5, 0.8]),\n        \"n_out\": 2,\n        \"delta\": [9.0, 9.0, 9.0],\n        \"seed\": 1234\n    }\n\n    # Case B: Higher dimension with correlated features and several outliers\n    A_b = np.array([\n        [1.0, 0.2, -0.1, 0.0],\n        [0.0, 0.9, 0.3, -0.2],\n        [0.1, 0.0, 1.1, 0.2],\n        [0.0, -0.1, 0.2, 0.8]\n    ])\n    Sigma_b = A_b @ A_b.T\n    case_b = {\n        \"n\": 12, \"p\": 4,\n        \"mu\": [0.3, -0.2, 0.0, 0.1],\n        \"Sigma\": Sigma_b,\n        \"n_out\": 3,\n        \"delta\": [7.0, -8.0, 9.0, -7.0],\n        \"seed\": 42\n    }\n\n    # Case C: Boundary-like small sample size and a single extreme outlier\n    B_c = np.array([\n        [1.0, 0.4, 0.0],\n        [0.2, 0.8, 0.3],\n        [0.0, 0.1, 0.9]\n    ])\n    Sigma_c = B_c @ B_c.T\n    case_c = {\n        \"n\": 9, \"p\": 3,\n        \"mu\": [0.0, 0.0, 0.0],\n        \"Sigma\": Sigma_c,\n        \"n_out\": 1,\n        \"delta\": [10.0, 0.0, -10.0],\n        \"seed\": 2021\n    }\n\n    test_cases = [case_a, case_b, case_c]\n    results = []\n\n    for case in test_cases:\n        n, p = case[\"n\"], case[\"p\"]\n        mu, Sigma = np.array(case[\"mu\"]), np.array(case[\"Sigma\"])\n        n_out, delta, seed = case[\"n_out\"], np.array(case[\"delta\"]), case[\"seed\"]\n\n        np.random.seed(seed)\n\n        # 1. Construct the dataset\n        n_in = n - n_out\n        inliers = np.random.multivariate_normal(mu, Sigma, n_in)\n        \n        # Outlier generation from N(mu + delta, 0.01 * I)\n        outlier_mean = mu + delta\n        outlier_cov = np.eye(p) * 0.01\n        outliers = np.random.multivariate_normal(outlier_mean, outlier_cov, n_out)\n        \n        X = np.vstack((inliers, outliers))\n\n        # 4. Outlier flagging threshold\n        threshold_sq = chi2.ppf(0.975, df=p)\n        threshold = np.sqrt(threshold_sq)\n\n        # 2. Compute classical Mahalanobis distances\n        mu_classical = np.mean(X, axis=0)\n        Sigma_classical = np.cov(X, rowvar=False)\n        inv_Sigma_classical = np.linalg.inv(Sigma_classical)\n        \n        diffs_classical = X - mu_classical\n        d_classical_sq = np.sum(diffs_classical @ inv_Sigma_classical * diffs_classical, axis=1)\n        d_classical = np.sqrt(d_classical_sq)\n        \n        n_outliers_classical = np.sum(d_classical > threshold)\n\n        # 3. Compute robust Mahalanobis distances (MCD)\n        h = int(0.75 * n)\n        \n        min_det = np.inf\n        mu_mcd, Sigma_mcd = None, None\n        \n        indices = range(n)\n        for subset_indices in combinations(indices, h):\n            subset = X[list(subset_indices), :]\n            current_cov = np.cov(subset, rowvar=False)\n            \n            # Using slogdet to handle potential underflow/overflow of small/large determinants\n            sign, logdet = np.linalg.slogdet(current_cov)\n\n            # Check for positive definite (sign=1) and near-singularity\n            if sign == 1:\n                # In log space, smaller logdet means smaller determinant\n                if logdet < min_det:\n                    min_det = logdet\n                    mu_mcd = np.mean(subset, axis=0)\n                    Sigma_mcd = current_cov\n\n        if Sigma_mcd is None:\n            # Fallback in case no valid subset is found (highly unlikely for this problem)\n            raise ValueError(\"Could not find a non-singular subset for MCD.\")\n            \n        inv_Sigma_mcd = np.linalg.inv(Sigma_mcd)\n        diffs_robust = X - mu_mcd\n        d_robust_sq = np.sum(diffs_robust @ inv_Sigma_mcd * diffs_robust, axis=1)\n        d_robust = np.sqrt(d_robust_sq)\n        \n        n_outliers_robust = np.sum(d_robust > threshold)\n\n        # 5. Compute the final metric\n        diff = n_outliers_robust - n_outliers_classical\n        results.append(diff)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}