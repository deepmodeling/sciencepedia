{
    "hands_on_practices": [
        {
            "introduction": "A core objective in many fMRI studies is to compare the magnitude of neural activation across different subjects. However, direct comparison of raw signal is often invalidated by subject-specific multiplicative gain factors, such as those arising from coil sensitivity. This practice challenges you to reason from a first-principles signal model which of two common unitless normalization techniques—percent signal change or z-scoring—more reliably preserves inter-subject comparability in the presence of such nuisance variability .",
            "id": "4153887",
            "problem": "In a functional Magnetic Resonance Imaging (fMRI) experiment, assume the measured voxel time series for subject $i$ is modeled by the linear superposition\n$$\nx_i(t) \\;=\\; c_i\\,[\\,b \\;+\\; s(t)\\,] \\;+\\; \\epsilon_i(t),\n$$\nwhere $t$ indexes time samples, $b>0$ is a constant baseline signal attributable to static factors (for example, $T_2^*$ weighting and coil sensitivity averaged across the run), $s(t)$ is the task-evoked modulation common across subjects, $c_i>0$ is a subject-specific multiplicative gain that produces varying baselines across subjects, and $\\epsilon_i(t)$ is additive noise. Suppose $E[\\epsilon_i(t)]=0$, $\\mathrm{Var}(\\epsilon_i(t))=\\sigma_{\\epsilon,i}^2$, and $\\epsilon_i(t)$ is independent of both $c_i$ and $s(t)$. The task design ensures that $s(t)$ is zero during baseline epochs and, during activation epochs, $s(t)$ adds a fixed amplitude $a>0$ (for example, $s(t)=a$ at activation time points and $s(t)=0$ otherwise). Let $i\\in\\{1,2\\}$ denote two subjects with different gains $c_1\\neq c_2$ and potentially different noise variances $\\sigma_{\\epsilon,1}^2\\neq \\sigma_{\\epsilon,2}^2$.\n\nAn investigator wishes to compare the magnitude of the activation effect across subjects using two unitless normalizations computed on the same time series: (i) percent signal change, which expresses the activation amplitude relative to the baseline level, and (ii) $z$-scoring, which standardizes the time series by subtracting its mean and dividing by its standard deviation. Using the core definitions of expectation, variance, and the sample mean and standard deviation, and treating $s(t)$ and $\\epsilon_i(t)$ as the only sources of variation aside from the constant baseline $b$, reason from first principles about how each normalization transforms the effect amplitude and the dependence on $c_i$ and $\\sigma_{\\epsilon,i}^2$.\n\nWhich statement best describes which normalization more reliably preserves inter-subject comparability of effect magnitudes under varying baselines induced by the multiplicative gains $c_i$?\n\nA. Percent signal change produces effect estimates that are invariant to the multiplicative gains $c_i$ and thus better preserve inter-subject comparability when baselines differ; $z$-scoring can obscure comparability because it depends on the subject-specific standard deviation, which varies with both task variance and noise.\n\nB. $z$-scoring is strictly superior because it removes both offsets and scaling; it yields identical effect estimates across subjects even when their noise variances differ.\n\nC. Both percent signal change and $z$-scoring equally preserve inter-subject comparability under multiplicative baseline variation regardless of noise structure, since both remove $c_i$ entirely.\n\nD. Neither percent signal change nor $z$-scoring can preserve inter-subject comparability when baselines differ multiplicatively; only raw amplitudes should be compared.\n\nE. Percent signal change fails under multiplicative baseline differences but succeeds when baselines differ additively; $z$-scoring is invariant only to multiplicative, not additive, differences.",
            "solution": "The user's request is to validate the problem statement and, if valid, provide a detailed solution based on first principles.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Model:** The measured voxel time series for subject $i$ is $x_i(t) \\;=\\; c_i\\,[\\,b \\;+\\; s(t)\\,] \\;+\\; \\epsilon_i(t)$.\n- **Variables and Constants:**\n    - $t$: time index.\n    - $b>0$: constant baseline signal.\n    - $s(t)$: task-evoked modulation, common across subjects.\n    - $c_i>0$: subject-specific multiplicative gain.\n    - $\\epsilon_i(t)$: additive noise.\n- **Properties of Variables:**\n    - $E[\\epsilon_i(t)]=0$.\n    - $\\mathrm{Var}(\\epsilon_i(t))=\\sigma_{\\epsilon,i}^2$.\n    - $\\epsilon_i(t)$ is independent of both $c_i$ and $s(t)$.\n    - $s(t)$ is zero during baseline epochs.\n    - $s(t)$ adds a fixed amplitude $a>0$ during activation epochs (i.e., $s(t)=a$ at activation time points and $s(t)=0$ otherwise).\n- **Subjects:** $i\\in\\{1,2\\}$, with $c_1\\neq c_2$ and potentially $\\sigma_{\\epsilon,1}^2\\neq \\sigma_{\\epsilon,2}^2$.\n- **Task:** Compare two normalizations: (i) percent signal change and (ii) $z$-scoring, to determine which better preserves inter-subject comparability of effect magnitudes under varying multiplicative gains $c_i$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The linear model presented, $x_i(t) \\;=\\; c_i\\,[\\,b \\;+\\; s(t)\\,] \\;+\\; \\epsilon_i(t)$, is a standard and simplified but physically plausible model for fMRI signals. It accounts for a static baseline ($b$), task-related signal changes ($s(t)$), a multiplicative gain factor ($c_i$) that can represent physiological or instrumental scaling (like coil sensitivity), and additive noise ($\\epsilon_i(t)$). These concepts are fundamental in fMRI data analysis. The problem is scientifically sound.\n2.  **Well-Posed:** The problem is well-posed. All variables and statistical properties are clearly defined. The question asks for a comparison of two specific, well-defined normalization techniques based on the provided model. The structure allows for a unique and meaningful solution through mathematical derivation.\n3.  **Objective:** The problem is stated in objective, mathematical language, free from subjective or ambiguous terms.\n4.  **Completeness and Consistency:** The model and its parameters are specified sufficiently for the required analysis. There are no internal contradictions.\n5.  **Relevance:** The problem directly addresses the topic of data standardization and normalization in the context of neuroscience data analysis, which is its specified domain.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically grounded, well-posed, objective, and contains sufficient information for a rigorous analysis. I will proceed with the solution.\n\n### Solution Derivation\n\nThe core task is to determine how percent signal change and $z$-scoring affect the comparability of the task-evoked effect amplitude across subjects with different multiplicative gains $c_i$. The true underlying effect is the amplitude $a$ of the signal modulation $s(t)$. An ideal normalization would yield a measure of this effect that is independent of the subject-specific nuisance parameter $c_i$.\n\nLet's first define the expected signal levels.\n-   During baseline epochs, $s(t) = 0$. The expected signal for subject $i$ is:\n    $$E[x_i(t) | s(t)=0] = E[c_i(b+0) + \\epsilon_i(t)] = c_i b + E[\\epsilon_i(t)] = c_i b$$\n-   During activation epochs, $s(t) = a$. The expected signal for subject $i$ is:\n    $$E[x_i(t) | s(t)=a] = E[c_i(b+a) + \\epsilon_i(t)] = c_i(b+a) + E[\\epsilon_i(t)] = c_i(b+a)$$\n\nThe raw effect amplitude, defined as the difference in expected signal between activation and baseline, is:\n$$ \\Delta E_i = E[x_i(t) | s(t)=a] - E[x_i(t) | s(t)=0] = c_i(b+a) - c_ib = c_i a $$\nThis raw amplitude is confounded by the subject-specific gain $c_i$, making it unsuitable for direct inter-subject comparison.\n\n**Analysis of Method (i): Percent Signal Change (PSC)**\n\nPercent signal change is defined as the change in signal relative to the baseline level. Using the expected values as estimates:\n$$ \\text{PSC}_i = \\frac{\\text{Activation Signal} - \\text{Baseline Signal}}{\\text{Baseline Signal}} $$\nSubstituting the expected values:\n$$ \\text{PSC}_i = \\frac{E[x_i(t) | s(t)=a] - E[x_i(t) | s(t)=0]}{E[x_i(t) | s(t)=0]} = \\frac{c_i(b+a) - c_ib}{c_ib} = \\frac{c_ia}{c_ib} = \\frac{a}{b} $$\nThe resulting PSC estimate, $a/b$, is independent of both the subject-specific gain $c_i$ and the noise variance $\\sigma_{\\epsilon,i}^2$. Since $a$ and $b$ are common to all subjects, the PSC provides an identical, comparable value for all subjects, perfectly reflecting the relative magnitude of the activation. Thus, it successfully removes the confounding effect of the multiplicative gain $c_i$.\n\n**Analysis of Method (ii): $z$-scoring**\n\n$z$-scoring transforms the time series $x_i(t)$ into $z_i(t)$ by subtracting its mean $\\mu_{x,i}$ and dividing by its standard deviation $\\sigma_{x,i}$. We must first compute these two moments for the entire time series.\n\nLet $p_a$ be the proportion of time points corresponding to activation epochs, and $p_b = 1-p_a$ be the proportion for baseline.\n-   **Mean of $x_i(t)$:**\n    $$ \\mu_{x,i} = E[x_i(t)] = E[c_i(b+s(t)) + \\epsilon_i(t)] = c_i(b+E[s(t)]) $$\n    The mean of $s(t)$ is $E[s(t)] = a \\cdot p_a + 0 \\cdot p_b = a p_a$.\n    $$ \\mu_{x,i} = c_i(b + a p_a) $$\n-   **Variance of $x_i(t)$:**\n    Since $\\epsilon_i(t)$ and $s(t)$ are independent sources of variation,\n    $$ \\sigma_{x,i}^2 = \\mathrm{Var}(x_i(t)) = \\mathrm{Var}(c_i(b+s(t)) + \\epsilon_i(t)) = \\mathrm{Var}(c_i s(t)) + \\mathrm{Var}(\\epsilon_i(t)) = c_i^2 \\mathrm{Var}(s(t)) + \\sigma_{\\epsilon,i}^2 $$\n    The variance of the task signal $s(t)$ is $\\mathrm{Var}(s(t)) = E[s(t)^2] - (E[s(t)])^2$.\n    $E[s(t)^2] = a^2 \\cdot p_a + 0^2 \\cdot p_b = a^2 p_a$.\n    $\\mathrm{Var}(s(t)) = a^2 p_a - (a p_a)^2 = a^2 p_a (1-p_a)$.\n    Therefore, the total variance is:\n    $$ \\sigma_{x,i}^2 = c_i^2 a^2 p_a(1-p_a) + \\sigma_{\\epsilon,i}^2 $$\n    The standard deviation is $\\sigma_{x,i} = \\sqrt{c_i^2 a^2 p_a(1-p_a) + \\sigma_{\\epsilon,i}^2}$.\n\nNow, we compute the effect amplitude in $z$-scored units. This is the difference in the expected $z$-score between activation and baseline states.\n$$ z_i(t) = \\frac{x_i(t) - \\mu_{x,i}}{\\sigma_{x,i}} $$\n-   Expected $z$-score during activation:\n    $$ E[z_i(t)|s(t)=a] = \\frac{E[x_i(t)|s(t)=a] - \\mu_{x,i}}{\\sigma_{x,i}} = \\frac{c_i(b+a) - c_i(b+ap_a)}{\\sigma_{x,i}} = \\frac{c_i a(1-p_a)}{\\sigma_{x,i}} $$\n-   Expected $z$-score during baseline:\n    $$ E[z_i(t)|s(t)=0] = \\frac{E[x_i(t)|s(t)=0] - \\mu_{x,i}}{\\sigma_{x,i}} = \\frac{c_ib - c_i(b+ap_a)}{\\sigma_{x,i}} = \\frac{-c_i a p_a}{\\sigma_{x,i}} $$\nThe $z$-scored effect amplitude is the difference between these two expected values:\n$$ \\Delta z_i = E[z_i(t)|s(t)=a] - E[z_i(t)|s(t)=0] = \\frac{c_i a(1-p_a) - (-c_i a p_a)}{\\sigma_{x,i}} = \\frac{c_i a}{\\sigma_{x,i}} $$\nSubstituting the expression for $\\sigma_{x,i}$:\n$$ \\Delta z_i = \\frac{c_i a}{\\sqrt{c_i^2 a^2 p_a(1-p_a) + \\sigma_{\\epsilon,i}^2}} $$\nTo analyze the dependence on $c_i$, we can rewrite this as:\n$$ \\Delta z_i = \\frac{a}{\\sqrt{a^2 p_a(1-p_a) + \\sigma_{\\epsilon,i}^2/c_i^2}} $$\nThis expression for the $z$-scored effect amplitude, $\\Delta z_i$, clearly depends on both the subject-specific gain $c_i$ and the subject-specific noise variance $\\sigma_{\\epsilon,i}^2$. Because $c_1 \\neq c_2$, their $z$-scored effect amplitudes $\\Delta z_1$ and $\\Delta z_2$ will not be equal, even if all other parameters (including noise variance) were identical. $z$-scoring expresses the activation magnitude relative to the total variability of the time series, which is a composite of task-related signal variance and noise variance. Since both of these components are scaled differently by $c_i$ (the signal variance by $c_i^2$, the noise variance not at all), the resulting ratio is not invariant to $c_i$.\n\n**Conclusion of Comparison**\n\n-   **Percent Signal Change** yields an effect estimate of $a/b$, which is invariant to $c_i$ and $\\sigma_{\\epsilon,i}^2$. It provides a stable and comparable measure of the relative activation across subjects.\n-   **$z$-scoring** yields an effect estimate $\\Delta z_i$ that is a complex function of $c_i$, $\\sigma_{\\epsilon,i}^2$, and the task design $p_a$. It does not remove the influence of the multiplicative gain and thus does not produce a directly comparable effect magnitude across subjects.\n\nTherefore, under the given model of multiplicative baseline variation, percent signal change is the superior method for preserving inter-subject comparability of effect magnitudes.\n\n### Option-by-Option Analysis\n\n**A. Percent signal change produces effect estimates that are invariant to the multiplicative gains $c_i$ and thus better preserve inter-subject comparability when baselines differ; $z$-scoring can obscure comparability because it depends on the subject-specific standard deviation, which varies with both task variance and noise.**\n-   Our derivation shows that the PSC estimate is $a/b$, which is indeed invariant to $c_i$. This correctly leads to better inter-subject comparability.\n-   Our derivation shows the $z$-scored effect $\\Delta z_i$ depends on $\\sigma_{x,i} = \\sqrt{c_i^2 \\mathrm{Var}(s(t)) + \\sigma_{\\epsilon,i}^2}$, the subject-specific standard deviation. This standard deviation is composed of a term related to task variance ($\\mathrm{Var}(s(t))$) and a term related to noise variance ($\\sigma_{\\epsilon,i}^2$).\n-   This statement is a perfect summary of our findings.\n-   **Verdict: Correct.**\n\n**B. $z$-scoring is strictly superior because it removes both offsets and scaling; it yields identical effect estimates across subjects even when their noise variances differ.**\n-   This is false. Our derivation for $\\Delta z_i$ explicitly shows that the resulting effect estimate is not identical across subjects and depends on both $c_i$ and $\\sigma_{\\epsilon,i}^2$.\n-   **Verdict: Incorrect.**\n\n**C. Both percent signal change and $z$-scoring equally preserve inter-subject comparability under multiplicative baseline variation regardless of noise structure, since both remove $c_i$ entirely.**\n-   This is false. Our derivation shows that $z$-scoring does *not* remove the dependency on $c_i$ from the effect estimate. Therefore, they do not equally preserve comparability.\n-   **Verdict: Incorrect.**\n\n**D. Neither percent signal change nor $z$-scoring can preserve inter-subject comparability when baselines differ multiplicatively; only raw amplitudes should be compared.**\n-   This is false. PSC *does* preserve comparability, yielding the estimate $a/b$.\n-   Comparing raw amplitudes ($c_ia$) is precisely the problem to be avoided, as they are confounded by $c_i$.\n-   **Verdict: Incorrect.**\n\n**E. Percent signal change fails under multiplicative baseline differences but succeeds when baselines differ additively; $z$-scoring is invariant only to multiplicative, not additive, differences.**\n-   This statement is incorrect on multiple counts. The first clause, \"Percent signal change fails under multiplicative baseline differences,\" is the opposite of what we proved. PSC *succeeds* with multiplicative differences.\n-   The rest of the statement is also flawed, but the incorrectness of the first part is sufficient to invalidate it.\n-   **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Building on the principle of ensuring inter-subject comparability, this exercise moves from method selection to application strategy in the context of multi-site studies. A critical choice when normalizing data from multiple sources is whether to apply scaling parameters derived independently for each subject or to use a single set of global parameters computed across the entire dataset. This hands-on coding practice directs you to implement both per-subject and across-subject min-max scaling, asking you to quantify how each approach affects the similarity of the underlying neural patterns .",
            "id": "4153892",
            "problem": "You are given a synthetic multisite functional Magnetic Resonance Imaging (fMRI) dataset modeled to analyze the effect of data normalization strategies on between-subject comparability. Each dataset consists of $S$ subjects, each represented by a region-of-interest feature vector $x^{(s)} \\in \\mathbb{R}^{V}$, constructed from a shared latent pattern $g \\in \\mathbb{R}^{V}$ that is modified by subject-specific multiplicative amplitude and site-specific additive offset, plus independent Gaussian noise. The goal is to compare min-max scaling performed per subject versus min-max scaling performed across subjects and to quantify how each approach affects between-subject comparability.\n\nStart from fundamental definitions and properties related to affine transformations and similarity measures. Use the following generative model for each subject $s \\in \\{1,\\dots,S\\}$:\n$$\nx^{(s)} = a_s \\, g + b_{\\text{site}(s)} \\cdot \\mathbf{1} + \\varepsilon^{(s)},\n$$\nwhere $a_s \\in \\mathbb{R}$ is the subject amplitude, $b_{\\text{site}(s)} \\in \\mathbb{R}$ is the site-specific offset for the site to which subject $s$ belongs, $\\mathbf{1} \\in \\mathbb{R}^{V}$ is the vector of all ones, and $\\varepsilon^{(s)} \\in \\mathbb{R}^{V}$ is Gaussian noise with independent and identically distributed entries of standard deviation $\\sigma_s$. The shared latent pattern $g$ is drawn once per test case from a standard Normal distribution.\n\nDefine two normalization strategies:\n- Per-subject min-max scaling: for each $s$, map $x^{(s)}$ to $\\tilde{x}^{(s)}$ by independently applying min-max scaling using the $\\min$ and $\\max$ of $x^{(s)}$.\n- Across-subject min-max scaling: map each $x^{(s)}$ to $\\hat{x}^{(s)}$ by applying min-max scaling using the global $\\min$ and $\\max$ computed over the concatenation of all subjects’ vectors in the dataset.\n\nTo quantify between-subject comparability under each normalization, compute the mean pairwise cosine similarity across subjects. For a given collection of subject vectors $\\{y^{(1)},\\dots,y^{(S)}\\}$, the mean pairwise cosine similarity is defined as the average of the cosine similarity over all unordered subject pairs, with the cosine similarity of two nonzero vectors defined by the standard inner-product expression. For any subject vector with zero norm, define its cosine with any other vector to be $0$ to avoid undefined behavior. When applying min-max scaling, to prevent division by zero, use a lower bound $\\epsilon = 10^{-12}$ on denominators of the form $(\\max - \\min)$ by replacing any denominator smaller than $\\epsilon$ with $\\epsilon$.\n\nFor each test case, compute the scalar difference\n$$\n\\Delta C = C_{\\text{global}} - C_{\\text{per}},\n$$\nwhere $C_{\\text{global}}$ is the mean pairwise cosine similarity after across-subject min-max scaling, and $C_{\\text{per}}$ is the mean pairwise cosine similarity after per-subject min-max scaling.\n\nImplement the program to generate the datasets exactly as specified below, using the stated seeds for reproducibility. Each test case defines $S$, $V$, site assignments, amplitudes, site offsets, noise standard deviations, and a seed for the shared latent pattern $g$.\n\nTest Suite:\n- Case A (general heterogeneous case):\n  - $S = 6$, $V = 150$.\n  - Site labels for subjects $s = 1,\\dots,6$: $\\text{site}(s)$ given by $[0,0,1,1,0,1]$.\n  - Site offsets $b_{\\text{site}} = [30.0, 55.0]$.\n  - Subject amplitudes $[0.7, 1.2, 0.9, 1.6, 1.1, 0.5]$.\n  - Noise standard deviations $\\sigma_s = [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]$.\n  - Shared latent pattern seed: $123$.\n- Case B (near-identical distributions across subjects and sites):\n  - $S = 6$, $V = 150$.\n  - Site labels: $[0,0,1,1,0,1]$.\n  - Site offsets $b_{\\text{site}} = [40.0, 40.0]$.\n  - Subject amplitudes $[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]$.\n  - Noise standard deviations $\\sigma_s = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$.\n  - Shared latent pattern seed: $321$.\n- Case C (presence of a narrow-range outlier subject):\n  - $S = 6$, $V = 150$.\n  - Site labels: $[0,0,1,1,0,1]$.\n  - Site offsets $b_{\\text{site}} = [25.0, 65.0]$.\n  - Subject amplitudes $[1.0, 1.1, 0.9, 1.3, 0.01, 1.0]$ (subject $5$ is an outlier with very small amplitude).\n  - Noise standard deviations $\\sigma_s = [0.5, 0.5, 0.5, 0.5, 0.01, 0.5]$ (subject $5$ has very low noise).\n  - Shared latent pattern seed: $999$.\n\nYour program should implement:\n- Construction of $x^{(s)}$ given $a_s$, $b_{\\text{site}(s)}$, $\\sigma_s$, and the seeded $g$.\n- Per-subject and across-subject min-max scaling with the $\\epsilon$ safeguard as specified.\n- Computation of $C_{\\text{per}}$, $C_{\\text{global}}$, and the difference $\\Delta C$ for each case.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$), where each $r_i$ is the floating-point value of $\\Delta C$ for the corresponding test case, expressed as a decimal number.",
            "solution": "The problem asks for an analysis and quantification of the difference between two data normalization strategies—per-subject and across-subject min-max scaling—on a synthetic multi-site fMRI dataset. The core of the task is to compute the difference in mean pairwise cosine similarity, $\\Delta C = C_{\\text{global}} - C_{\\text{per}}$, resulting from these two strategies.\n\nFirst, we formalize the generative model and the normalization procedures. The data vector $x^{(s)} \\in \\mathbb{R}^{V}$ for each subject $s \\in \\{1, \\dots, S\\}$ is generated according to the linear model:\n$$\nx^{(s)} = a_s \\, g + b_{\\text{site}(s)} \\cdot \\mathbf{1} + \\varepsilon^{(s)}\n$$\nHere, $g \\in \\mathbb{R}^{V}$ is a shared latent pattern vector drawn from a standard Normal distribution. This pattern is scaled by a subject-specific amplitude $a_s \\in \\mathbb{R}$, shifted by a site-specific offset $b_{\\text{site}(s)} \\in \\mathbb{R}$ (where $\\mathbf{1}$ is the vector of all ones), and corrupted by i.i.d. Gaussian noise $\\varepsilon^{(s)}$ with standard deviation $\\sigma_s$. This model captures key sources of variance in multi-site studies: a common underlying signal ($g$), subject-level gain differences ($a_s$), scanner/site-level baseline shifts ($b_{\\text{site}(s)}$), and random measurement error ($\\varepsilon^{(s)}$).\n\nThe objective is to achieve high between-subject comparability, which we measure using the mean pairwise cosine similarity. For a set of vectors $\\{y^{(1)}, \\dots, y^{(S)}\\}$, the cosine similarity between a pair of non-zero vectors is $\\text{cossim}(y^{(i)}, y^{(j)}) = \\frac{y^{(i)} \\cdot y^{(j)}}{\\|y^{(i)}\\| \\|y^{(j)}\\|}$. The mean pairwise cosine similarity $C$ is the average of this value over all $\\frac{S(S-1)}{2}$ unique pairs of subjects.\n\nLet us analyze the two normalization strategies.\n\n1.  **Per-Subject Min-Max Scaling**: For each subject vector $x^{(s)}$, we compute its minimum $m_s = \\min_i x^{(s)}_i$ and maximum $M_s = \\max_i x^{(s)}_i$. The transformed vector $\\tilde{x}^{(s)}$ is given by:\n    $$\n    \\tilde{x}^{(s)} = \\frac{x^{(s)} - m_s \\cdot \\mathbf{1}}{D_s}\n    $$\n    where the denominator $D_s = M_s - m_s$ is lower-bounded by $\\epsilon = 10^{-12}$ to ensure numerical stability. This is an affine transformation applied independently to each subject's data. Its effect is to map the range of each subject's feature vector to $[0, 1]$.\n    Let us analyze the effect of this transformation on the generative model. Ignoring the noise term $\\varepsilon^{(s)}$ for clarity, we have $x^{(s)} \\approx a_s g + b_{\\text{site}(s)} \\mathbf{1}$. Assuming $a_s > 0$, the minimum and maximum are approximately $m_s \\approx a_s \\min(g) + b_{\\text{site}(s)}$ and $M_s \\approx a_s \\max(g) + b_{\\text{site}(s)}$. The denominator becomes $D_s = M_s - m_s \\approx a_s (\\max(g) - \\min(g))$.\n    Substituting these into the transformation equation:\n    $$\n    \\tilde{x}^{(s)} \\approx \\frac{(a_s g + b_{\\text{site}(s)} \\mathbf{1}) - (a_s \\min(g) + b_{\\text{site}(s)}) \\mathbf{1}}{a_s (\\max(g) - \\min(g))} = \\frac{a_s g - a_s \\min(g) \\mathbf{1}}{a_s (\\max(g) - \\min(g))} = \\frac{g - \\min(g) \\mathbf{1}}{\\max(g) - \\min(g)}\n    $$\n    This result is profound: per-subject min-max scaling approximately recovers a standardized version of the latent pattern $g$, irrespective of the subject-specific amplitude $a_s$ and site-specific offset $b_{\\text{site}(s)}$. Consequently, all transformed vectors $\\{\\tilde{x}^{(1)}, \\dots, \\tilde{x}^{(S)}\\}$ become nearly identical to each other. This implies that their pairwise cosine similarities will be very close to $1$, and thus the mean similarity $C_{\\text{per}}$ will be high.\n\n2.  **Across-Subject (Global) Min-Max Scaling**: We compute a global minimum $m_{\\text{global}} = \\min_{s,i} x^{(s)}_i$ and a global maximum $M_{\\text{global}} = \\max_{s,i} x^{(s)}_i$ across all data from all subjects. Each subject vector $x^{(s)}$ is then transformed to $\\hat{x}^{(s)}$:\n    $$\n    \\hat{x}^{(s)} = \\frac{x^{(s)} - m_{\\text{global}} \\cdot \\mathbf{1}}{D_{\\text{global}}}\n    $$\n    where $D_{\\text{global}} = M_{\\text{global}} - m_{\\text{global}}$ (also safeguarded by $\\epsilon$). This constitutes a *single* affine transformation applied uniformly to all subject vectors.\n    Cosine similarity is invariant to multiplication by a common positive scalar ($1/D_{\\text{global}}$), so $\\text{cossim}(\\hat{x}^{(i)}, \\hat{x}^{(j)}) = \\text{cossim}(x^{(i)} - m_{\\text{global}}\\mathbf{1}, x^{(j)} - m_{\\text{global}}\\mathbf{1})$. The transformation is equivalent to a common shift of all vectors. This procedure does not remove the subject-specific differences encoded by $a_s$ and $b_{\\text{site}(s)}$. The transformed vectors are approximately $\\hat{x}^{(s)} \\approx C(a_s g + (b_{\\text{site}(s)} - m_{\\text{global}})\\mathbf{1})$, for a constant $C = 1/D_{\\text{global}}$. The differences in amplitudes and offsets between subjects are preserved, which will typically result in lower pairwise cosine similarities compared to the per-subject case where these differences are removed.\n\nTherefore, we predict that $C_{\\text{global}} < C_{\\text{per}}$, making $\\Delta C = C_{\\text{global}} - C_{\\text{per}}$ negative, especially in cases with significant heterogeneity in $a_s$ and $b_{\\text{site}(s)}$ (Cases A and C). For Case B, where subjects are nearly identical to begin with ($a_s=1$, $b_{\\text{site}(s)}$ is constant), the individual statistics ($m_s, M_s$) will be very close to the global statistics ($m_{\\text{global}}, M_{\\text{global}}$). The two normalization methods will thus produce very similar results, and $\\Delta C$ is expected to be close to $0$. In Case C, the outlier subject with a very small amplitude $a_s$ will have its feature range compressed into a tiny portion of the global range by the across-subject scaling, making its vector nearly constant and hence nearly orthogonal to all other vectors. This will drastically reduce $C_{\\text{global}}$, while $C_{\\text{per}}$ remains high, leading to a large negative $\\Delta C$.\n\nThe implementation will proceed by first generating the datasets for each case using the specified parameters and random seeds. Then, for each dataset, both normalization procedures will be applied. The mean pairwise cosine similarity will be calculated for each set of normalized vectors using matrix operations for efficiency. Finally, the difference $\\Delta C$ is computed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing per-subject vs. across-subject\n    data normalization on synthetic fMRI data.\n    \"\"\"\n    \n    # Epsilon for safeguarding against division by zero in min-max scaling.\n    epsilon = 1e-12\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: General heterogeneous case\n        {\n            \"S\": 6, \"V\": 150,\n            \"site_labels\": [0, 0, 1, 1, 0, 1],\n            \"site_offsets\": [30.0, 55.0],\n            \"amplitudes\": [0.7, 1.2, 0.9, 1.6, 1.1, 0.5],\n            \"noise_stds\": [0.8] * 6,\n            \"seed\": 123\n        },\n        # Case B: Near-identical distributions\n        {\n            \"S\": 6, \"V\": 150,\n            \"site_labels\": [0, 0, 1, 1, 0, 1],\n            \"site_offsets\": [40.0, 40.0],\n            \"amplitudes\": [1.0] * 6,\n            \"noise_stds\": [0.5] * 6,\n            \"seed\": 321\n        },\n        # Case C: Outlier subject\n        {\n            \"S\": 6, \"V\": 150,\n            \"site_labels\": [0, 0, 1, 1, 0, 1],\n            \"site_offsets\": [25.0, 65.0],\n            \"amplitudes\": [1.0, 1.1, 0.9, 1.3, 0.01, 1.0],\n            \"noise_stds\": [0.5, 0.5, 0.5, 0.5, 0.01, 0.5],\n            \"seed\": 999\n        }\n    ]\n\n    results = []\n    \n    def compute_mean_pairwise_cosine(data_matrix):\n        \"\"\"\n        Computes the mean pairwise cosine similarity for a set of vectors.\n        data_matrix: A numpy array of shape (S, V) where S is number of subjects\n                     and V is number of features.\n        \"\"\"\n        S, V = data_matrix.shape\n        if S < 2:\n            return 0.0\n\n        norms = np.linalg.norm(data_matrix, axis=1)\n        \n        # Handle zero-norm vectors according to the problem statement.\n        # Create a matrix of zeros and fill only rows for non-zero vectors.\n        normalized_matrix = np.zeros_like(data_matrix, dtype=float)\n        \n        non_zero_mask = norms > 0\n        if np.any(non_zero_mask):\n            normalized_matrix[non_zero_mask] = \\\n                data_matrix[non_zero_mask] / norms[non_zero_mask, np.newaxis]\n\n        # Compute all pairwise dot products (cosine similarities for normalized vectors)\n        cosine_matrix = normalized_matrix @ normalized_matrix.T\n        \n        # We need the sum of the upper triangle of the matrix (excluding the diagonal)\n        num_pairs = S * (S - 1) / 2\n        total_cosine_similarity = np.sum(np.triu(cosine_matrix, k=1))\n        \n        if num_pairs > 0:\n            return total_cosine_similarity / num_pairs\n        else:\n            return 0.0\n\n    for case in test_cases:\n        S = case[\"S\"]\n        V = case[\"V\"]\n        seed = case[\"seed\"]\n        \n        # Initialize random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n        \n        # Generate shared latent pattern g\n        g = rng.standard_normal(size=V)\n        \n        # Generate dataset for S subjects\n        X = np.zeros((S, V))\n        for s in range(S):\n            a_s = case[\"amplitudes\"][s]\n            site_idx = case[\"site_labels\"][s]\n            b_site_s = case[\"site_offsets\"][site_idx]\n            sigma_s = case[\"noise_stds\"][s]\n            \n            # Generate noise for subject s\n            epsilon_s = rng.normal(loc=0.0, scale=sigma_s, size=V)\n            \n            # Construct subject's feature vector\n            X[s, :] = a_s * g + b_site_s + epsilon_s\n            \n        # 1. Per-subject min-max scaling\n        X_per = np.zeros_like(X)\n        for s in range(S):\n            min_s = np.min(X[s, :])\n            max_s = np.max(X[s, :])\n            denom_s = max_s - min_s\n            if denom_s < epsilon:\n                denom_s = epsilon\n            X_per[s, :] = (X[s, :] - min_s) / denom_s\n        \n        # 2. Across-subject min-max scaling\n        min_global = np.min(X)\n        max_global = np.max(X)\n        denom_global = max_global - min_global\n        if denom_global < epsilon:\n            denom_global = epsilon\n        X_global = (X - min_global) / denom_global\n        \n        # Compute mean pairwise cosine similarities\n        C_per = compute_mean_pairwise_cosine(X_per)\n        C_global = compute_mean_pairwise_cosine(X_global)\n        \n        # Compute the difference\n        delta_C = C_global - C_per\n        results.append(delta_C)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A robust analysis depends not only on choosing the right methods but also on assembling them in the correct sequence. This final practice situates normalization within a complete, end-to-end machine learning pipeline for EEG-based decoding, where its placement relative to artifact removal and feature extraction has profound implications. You are tasked with designing a pipeline that correctly orders these steps and adheres to strict data hygiene, ensuring that parameters for any transformation are learned without \"peeking\" at the test set, thereby preventing information leakage and enabling an unbiased assessment of your model's generalization performance .",
            "id": "4153843",
            "problem": "You are designing a supervised decoding pipeline for Electroencephalography (EEG) data in a motor imagery study. Each subject contributes $n$ trials of $p$ channels of continuous time series, sampled at frequency $f_s$ Hz, segmented into epochs aligned to a cue. You split the dataset into a training set and a held-out test set at the subject level to assess generalization. Let $x_{i,c}(t)$ denote the observed signal for trial $i$ and channel $c$, modeled as $x_{i,c}(t) = s_{i,c}(t) + a_{i,c}(t) + \\epsilon_{i,c}(t)$, where $s_{i,c}(t)$ is the neural signal of interest, $a_{i,c}(t)$ are artifacts (ocular, muscle, line noise), and $\\epsilon_{i,c}(t)$ is background noise. You will use Independent Component Analysis (ICA) for artifact subtraction, then extract log-bandpower features in canonical frequency bands from each channel, and finally fit a linear classifier. Assume the feature extractor $\\phi$ maps each trial to a vector $z \\in \\mathbb{R}^d$ defined by $z_k = \\log\\left(\\int_{B_k} |X(f)|^2 \\, df\\right)$ for disjoint bands $B_k$, where $X(f)$ is the Fourier transform of the cleaned time series in the epoch. Scaling refers to standardization (z-scoring) with parameters $\\mu \\in \\mathbb{R}^d$ and $\\sigma \\in \\mathbb{R}^d$ applied as $S_{\\mu,\\sigma}(z) = (z - \\mu) \\oslash \\sigma$, where $\\oslash$ denotes elementwise division.  \n\nYour goal is to place artifact removal, normalization, and feature extraction in an order that minimizes downstream estimator bias and variance, and that avoids information leakage between the held-out test set and the training set. Base your reasoning on the following fundamental definitions and facts:\n\n- A linear time-invariant filter or projection $A$ applied to $x$ is an operator on the time series and will commute with other linear time-domain operations but not generally with nonlinear feature maps $\\phi$.\n- The feature extractor $\\phi$ is nonlinear in $x$ (via squaring, integration over frequency, and log), so in general $\\phi(A(x)) \\neq A(\\phi(x))$.\n- The standardization operator $S_{\\mu,\\sigma}$ is an affine map on feature space, and if $\\mu,\\sigma$ are estimated using any function of the test set, this constitutes information leakage.\n- The expected squared prediction error of a learned predictor $\\hat{f}$ at a point $z$ decomposes as $\\mathbb{E}\\left[(\\hat{f}(z) - y)^2\\right] = \\mathrm{Bias}[\\hat{f}(z)]^2 + \\mathrm{Var}[\\hat{f}(z)] + \\sigma_\\epsilon^2$, where $\\sigma_\\epsilon^2$ is the irreducible noise variance.\n\nWhich of the following end-to-end pipeline specifications best minimizes estimator bias and variance while preventing leakage? Each option specifies the order of operations and how parameters are estimated. Assume ICA requires fitting an unmixing matrix $W$; in each option, clarify which data are used to fit $W$ and the scaling parameters $(\\mu,\\sigma)$.\n\nA. Artifact removal, then feature extraction, then normalization in feature space. Specifically: fit ICA unmixing matrix $W$ on the training set only, apply artifact subtraction $A_W$ to both training and test epochs, compute $z = \\phi(A_W(x))$ for all epochs, fit $(\\mu,\\sigma)$ on training features $\\{z\\}$ only, and transform both training and test features with $S_{\\mu,\\sigma}$.\n\nB. Normalization on raw time series across the entire dataset, then artifact removal, then feature extraction. Specifically: per channel, compute mean and standard deviation over all epochs from both training and test subjects to produce time-domain z-scored signals $\\tilde{x}$, fit ICA $W$ on all time-domain normalized data, apply $A_W$, then compute features $z = \\phi(A_W(\\tilde{x}))$.\n\nC. Feature extraction on raw signals, then artifact removal on the extracted features, then normalization with parameters pooled across training and test. Specifically: compute $z = \\phi(x)$, then regress out artifact-related components in feature space using an ICA trained on all features (training and test pooled), then compute $(\\mu,\\sigma)$ on the pooled de-artifacted features and z-score all features.\n\nD. Artifact removal, then per-trial normalization that forces each trial’s feature vector to have zero mean and unit variance across dimensions, then feature extraction. Specifically: fit ICA $W$ on the training set only, apply $A_W$ to all epochs, then for each trial independently compute its own feature-wise mean and standard deviation and z-score that trial’s data before computing bandpower features.\n\nChoose the option that yields the pipeline ordering and parameter fitting strategy that most directly minimizes estimator variance without inflating bias or causing information leakage. Provide a justification based on the operator properties and the bias-variance decomposition. There may be more than one correct element in an option, but select the single option that overall satisfies the criteria. Label your answer with the letter corresponding to your choice.",
            "solution": "The problem statement will first be validated for scientific and logical integrity.\n\n### Step 1: Extract Givens\n- **Data Source:** Electroencephalography (EEG) data from a motor imagery study.\n- **Data Structure:** For each subject, there are $n$ trials of $p$ channels of continuous time series.\n- **Sampling Frequency:** $f_s$ Hz.\n- **Data Split:** The dataset is split into a training set and a held-out test set at the subject level.\n- **Signal Model:** The observed signal $x_{i,c}(t)$ for trial $i$ and channel $c$ is modeled as $x_{i,c}(t) = s_{i,c}(t) + a_{i,c}(t) + \\epsilon_{i,c}(t)$, where $s_{i,c}(t)$ is the neural signal, $a_{i,c}(t)$ are artifacts, and $\\epsilon_{i,c}(t)$ is background noise.\n- **Processing Steps:** Independent Component Analysis (ICA) for artifact subtraction, log-bandpower feature extraction, and fitting a linear classifier.\n- **Feature Extractor $\\phi$:** A nonlinear map from a trial's time series $x$ to a feature vector $z \\in \\mathbb{R}^d$. The features are defined as $z_k = \\log\\left(\\int_{B_k} |X(f)|^2 \\, df\\right)$ for disjoint frequency bands $B_k$, where $X(f)$ is the Fourier transform of the cleaned time series.\n- **Standardization/Scaling $S_{\\mu,\\sigma}$:** An affine map on the feature space, defined as $S_{\\mu,\\sigma}(z) = (z - \\mu) \\oslash \\sigma$, where $\\oslash$ is elementwise division, and $\\mu, \\sigma \\in \\mathbb{R}^d$ are the feature-wise mean and standard deviation.\n- **Goal:** To identify the pipeline ordering (artifact removal, normalization, feature extraction) and parameter estimation strategy that minimizes estimator bias and variance while preventing information leakage between the test and training sets.\n- **Givens/Facts:**\n    - A linear time-invariant filter $A$ does not generally commute with a nonlinear feature map $\\phi$, i.e., $\\phi(A(x)) \\neq A(\\phi(x))$.\n    - The feature extractor $\\phi$ is nonlinear.\n    - Estimating scaling parameters $\\mu, \\sigma$ using any part of the test set constitutes information leakage.\n    - The expected squared prediction error is $\\mathbb{E}\\left[(\\hat{f}(z) - y)^2\\right] = \\mathrm{Bias}[\\hat{f}(z)]^2 + \\mathrm{Var}[\\hat{f}(z)] + \\sigma_\\epsilon^2$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is firmly grounded in the established practices of neuroscience data analysis and machine learning. The signal model, use of ICA for artifact removal, log-bandpower features for motor imagery decoding, subject-level cross-validation, and concerns about information leakage are all standard and critical concepts in the field of Brain-Computer Interfaces (BCI).\n2.  **Well-Posed:** The problem is well-posed. It asks for the best ordering and parameter estimation strategy among a set of clearly defined options, based on explicit criteria (minimizing bias/variance, avoiding leakage). A unique, reasoned answer can be derived.\n3.  **Objective:** The problem statement is objective, using precise technical language. The criteria for judgment are based on established mathematical and statistical principles (bias-variance tradeoff, definition of information leakage).\n4.  **Flaw Checklist:** The problem statement does not violate any of the specified flaws. It is not unsound, incomplete, contradictory, unrealistic, or ill-posed. The concepts are central to the field and not trivial.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be derived.\n\n### Derivation of the Optimal Pipeline\n\nThe primary goal is to build a supervised learning pipeline that generalizes well to unseen data. This requires strict adherence to the principle of not using any information from the test set during the training of the model or the fitting of any preprocessing parameters. This is essential for obtaining an unbiased estimate of the model's performance.\n\nLet's analyze the sequence of operations based on the nature of the data and operators.\n\n1.  **Artifact Removal (ICA):** ICA is a blind source separation technique that linearly unmixes the observed channel signals $x(t)$ into statistically independent source signals. This is fundamentally a time-domain operation. The goal is to isolate and remove artifactual sources $a(t)$ to better recover the neural signal of interest $s(t)$. Therefore, artifact removal must be performed on the time-series data. The unmixing matrix $W$ that defines the linear projection $A_W$ must be learned from the data. To prevent information leakage, $W$ must be fit exclusively on the training set. The resulting transformation $A_W$ is then a fixed linear operator that can be applied to any new data, including the test set.\n\n2.  **Feature Extraction ($\\phi$):** The feature extractor $\\phi$ is given as a nonlinear function that computes log-bandpower from the time series. This process involves a Fourier transform, squaring, integration, and a logarithm. To obtain features that are as clean as possible, feature extraction should be performed *after* the signal has been cleaned of artifacts. This corresponds to the composition $\\phi(A_W(x))$. The reverse order, $A_W(\\phi(x))$, is conceptually flawed. ICA is not designed to operate on feature vectors of log-powers; its statistical assumptions apply to the time-domain mixture of source signals.\n\n3.  **Normalization ($S_{\\mu,\\sigma}$):** Standardization (z-scoring) is a feature-space operation. Its purpose is to scale the components of the feature vector $z$ to have zero mean and unit standard deviation. This is beneficial for many linear classifiers, as it can stabilize training and prevent features with intrinsically large variance from dominating the learning process, which helps in minimizing the variance of the learned estimator $\\hat{f}$. The parameters for this scaling, the mean vector $\\mu$ and standard deviation vector $\\sigma$, must be estimated from the distribution of features. To prevent information leakage, they must be computed using *only* the training set features. The same fixed scaling transformation $S_{\\mu,\\sigma}$ is then applied to both the training features (before fitting the classifier) and the test features (before evaluation).\n\n**Conclusion on Optimal Order and Parameter Fitting:**\nThe logically sound and methodologically correct pipeline is:\n1.  Split data into training and test sets.\n2.  Fit the ICA unmixing matrix $W$ on the training set time series.\n3.  Apply the learned artifact subtraction operator $A_W$ to both the training and test set time series.\n4.  Apply the feature extractor $\\phi$ to the cleaned time series for both sets to get training features $\\{z_{train}\\}$ and test features $\\{z_{test}\\}$.\n5.  Compute the scaling parameters $(\\mu,\\sigma)$ from $\\{z_{train}\\}$ only.\n6.  Apply the scaling operator $S_{\\mu,\\sigma}$ to both $\\{z_{train}\\}$ and $\\{z_{test}\\}$.\n7.  Train the linear classifier on the scaled training features and evaluate on the scaled test features.\n\nThis pipeline ensures that all learned components ($W, \\mu, \\sigma$, and the classifier parameters) depend only on the training data, thus avoiding information leakage and providing an unbiased estimate of generalization error. It applies each operation in the correct domain (time vs. feature) and in a logical sequence.\n\n### Evaluation of Options\n\n**A. Artifact removal, then feature extraction, then normalization in feature space. Specifically: fit ICA unmixing matrix $W$ on the training set only, apply artifact subtraction $A_W$ to both training and test epochs, compute $z = \\phi(A_W(x))$ for all epochs, fit $(\\mu,\\sigma)$ on training features $\\{z\\}$ only, and transform both training and test features with $S_{\\mu,\\sigma}$.**\n\n-   **Order of Operations:** Artifact removal $\\rightarrow$ Feature extraction $\\rightarrow$ Normalization. This matches our derived optimal order.\n-   **Parameter Fitting:**\n    -   ICA matrix $W$ is fit on the training set only. This is correct and prevents leakage.\n    -   Scaling parameters $(\\mu, \\sigma)$ are fit on the training features only. This is correct and prevents leakage.\n-   **Overall Assessment:** This option perfectly aligns with the principles of valid machine learning pipelines. It ensures no information leakage, applies operators in the correct domain and order, and represents the standard best practice for this type of analysis. This approach is designed to yield an unbiased estimate of the generalization error and correctly prepares the data to minimize the variance of the downstream estimator without artificially inflating bias.\n\n**Verdict: Correct**\n\n**B. Normalization on raw time series across the entire dataset, then artifact removal, then feature extraction. Specifically: per channel, compute mean and standard deviation over all epochs from both training and test subjects to produce time-domain z-scored signals $\\tilde{x}$, fit ICA $W$ on all time-domain normalized data, apply $A_W$, then compute features $z = \\phi(A_W(\\tilde{x}))$.**\n\n-   **Order of Operations:** Normalization (time-domain) $\\rightarrow$ Artifact removal $\\rightarrow$ Feature extraction.\n-   **Parameter Fitting:**\n    -   Time-domain normalization parameters are computed from **all epochs (training and test)**. This is a severe form of **information leakage**.\n    -   ICA matrix $W$ is fit on **all data (training and test)**. This is also a severe form of **information leakage**.\n-   **Overall Assessment:** This option is fundamentally flawed. By using statistics from the test set to transform the training data (and vice-versa), it contaminates the training process and renders the test set evaluation invalid. The resulting performance estimate would be optimistically biased and not representative of true generalization.\n\n**Verdict: Incorrect**\n\n**C. Feature extraction on raw signals, then artifact removal on the extracted features, then normalization with parameters pooled across training and test. Specifically: compute $z = \\phi(x)$, then regress out artifact-related components in feature space using an ICA trained on all features (training and test pooled), then compute $(\\mu,\\sigma)$ on the pooled de-artifacted features and z-score all features.**\n\n-   **Order of Operations:** Feature extraction $\\rightarrow$ Artifact removal (feature-space) $\\rightarrow$ Normalization.\n-   **Conceptual Flaw:** Applying ICA to artifact-remove on the feature space of log-bandpowers is statistically unsound. The linear mixture model assumption of ICA is not valid for features that result from the highly nonlinear $\\phi$ operator. Artifacts are mixed linearly in the time-domain sensor recordings, not in the log-power feature space.\n-   **Parameter Fitting:**\n    -   ICA is trained on **pooled features (training and test)**. This is **information leakage**.\n    -   Scaling parameters $(\\mu, \\sigma)$ are computed on **pooled features**. This is also **information leakage**.\n-   **Overall Assessment:** This option is incorrect due to both a conceptually flawed pipeline order and multiple instances of information leakage.\n\n**Verdict: Incorrect**\n\n**D. Artifact removal, then per-trial normalization that forces each trial’s feature vector to have zero mean and unit variance across dimensions, then feature extraction. Specifically: fit ICA $W$ on the training set only, apply $A_W$ to all epochs, then for each trial independently compute its own feature-wise mean and standard deviation and z-score that trial’s data before computing bandpower features.**\n\n-   **Order of Operations:** The description is slightly ambiguous: \"z-score that trial’s data before computing bandpower features\" suggests time-domain normalization. However, the description \"forces each trial’s feature vector to have zero mean and unit variance\" implies feature-space normalization. Let's analyze the more plausible feature-space interpretation.\n-   **Normalization Scheme:** The proposed normalization is `per-trial`, where for each feature vector $z^{(i)}$, it is transformed using its own mean and standard deviation calculated across its dimensions. This scheme does not cause information leakage between train and test sets, as each trial is processed independently.\n-   **Information Loss:** This normalization scheme is highly problematic. The relative power levels across different frequency bands are a primary source of information for classification. Forcing every trial's feature vector to have a mean of $0$ and a standard deviation of $1$ destroys this information, effectively making a trial with low overall power indistinguishable from a trial with high overall power, a key feature in event-related desynchronization/synchronization (ERD/ERS) paradigms common in motor imagery. This drastic alteration of the feature space is very likely to increase the bias of the classifier by removing relevant predictive information, even if it might reduce some form of variance. It is not a method that \"minimizes estimator variance without inflating bias.\"\n-   **Overall Assessment:** While this option correctly fits ICA on the training set and avoids leakage, its proposed normalization step is information-destroying and detrimental to model performance. It is a suboptimal choice compared to a standard population-based normalization.\n\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}