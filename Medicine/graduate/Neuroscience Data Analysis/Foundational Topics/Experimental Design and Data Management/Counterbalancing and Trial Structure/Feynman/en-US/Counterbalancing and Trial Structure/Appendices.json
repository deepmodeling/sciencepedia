{
    "hands_on_practices": [
        {
            "introduction": "The gold standard for controlling order effects in an experiment is full counterbalancing, where every possible sequence of conditions is presented to an equal number of participants. This first practice challenges you to explore the theoretical underpinnings and practical limits of this approach. By deriving the relationship between the number of conditions ($k$) and the required sample size ($N$), you will gain a rigorous, quantitative understanding of why full counterbalancing, while ideal in theory, is often infeasible in practice .",
            "id": "4150593",
            "problem": "A cognitive neuroscience experiment presents $k$ distinct task conditions to each participant exactly once, in some order, within a single session. The design objective is full counterbalancing, defined as each possible order of the $k$ conditions being assigned equally often across participants. Using fundamental counting principles and standard asymptotic analysis, derive the exact number of unique orders available and then determine, for a given total sample size $N$, an analytic approximation for the largest $k$ for which full counterbalancing is feasible in the sense that each unique order can appear at least once across participants.\n\nYour derivation must start from the fundamental counting principle for sequences without replacement and may employ a well-tested asymptotic approximation for $k!$ together with the defining property of the Lambert $W$ function. Provide a closed-form expression for a continuous relaxation $k^{\\star}(N)$ that approximates the maximal feasible $k$ given $N$ by equating the asymptotic approximation of the total number of orders to $N$ and solving for $k$. Do not assume any particular numerical value of $N$ or $k$; your final expression must depend symbolically on $N$.\n\nIn your reasoning, also explain why full counterbalancing is typically infeasible for moderate $N$ in neuroscience (e.g., $N$ on the order of tens), and why it may not be necessary under first-order carryover assumptions and linear modeling of order effects. However, the only item you should report as your final answer is the closed-form analytic expression for $k^{\\star}(N)$.",
            "solution": "The problem requires the derivation of the number of unique orders for $k$ distinct task conditions, an analysis of the feasibility of full counterbalancing, and the derivation of an analytic approximation for the maximum number of conditions, $k^{\\star}(N)$, that can be feasibly counterbalanced with a sample size of $N$ participants.\n\nFirst, we determine the exact number of unique orders available for an experiment with $k$ distinct task conditions, where each condition is presented exactly once. This is a classical permutation problem. The number of ways to arrange $k$ distinct items in a sequence is given by the factorial function. Therefore, the total number of unique orders, which we denote as $P_k$, is:\n$$P_k = k!$$\nwhere $k! = k \\times (k-1) \\times \\dots \\times 2 \\times 1$.\n\nThe problem defines full counterbalancing as a design where each possible order is assigned equally often across participants. The feasibility condition given is that each unique order can appear at least once across the $N$ participants. This implies that the number of participants $N$ must be at least as large as the number of unique orders $P_k$.\n$$N \\ge k!$$\n\nThis requirement demonstrates why full counterbalancing is often infeasible in practice, especially in neuroscience where sample sizes are frequently limited by cost, time, and participant availability. The factorial function grows extremely rapidly. For example:\n- For $k=3$ conditions, $3! = 6$ orders are possible. A sample size of $N \\ge 6$ is required, which is easily achievable.\n- For $k=4$ conditions, $4! = 24$ orders are possible. A sample size of $N \\ge 24$ is required, which is at the lower end of typical study sizes.\n- For $k=5$ conditions, $5! = 120$ orders are possible. A sample size of $N \\ge 120$ is required, a number that is prohibitively large for many neuroimaging (fMRI, MEG) or clinical studies.\n- For $k=6$ conditions, $6! = 720$ orders are possible, requiring $N \\ge 720$, which is almost always infeasible.\nThe super-exponential growth of $k!$ makes full counterbalancing impractical for even a moderate number of conditions.\n\nIn situations where full counterbalancing is infeasible, alternative strategies exist. The primary purpose of counterbalancing is to control for order effects. If one assumes that only first-order carryover effects are significant (i.e., a trial is only influenced by the immediately preceding trial), a full permutation of all orders is not necessary. A Latin Square design, which ensures each condition appears at each serial position exactly once, can be used. More advanced balanced Latin Squares can also ensure that each condition precedes and follows every other condition exactly once. These designs require a number of participants that is a multiple of $k$, not $k!$, making them vastly more efficient. Additionally, order effects can be addressed analytically. By including trial number or presentation order as a continuous or categorical predictor variable in a general linear model (GLM), the variance associated with systematic order effects (like practice or fatigue) can be statistically estimated and removed from the condition-specific effects of interest.\n\nWe now proceed to derive the analytic approximation for the largest feasible $k$, denoted as a continuous relaxation $k^{\\star}(N)$, given a sample size $N$. The problem is to find the value of $k$ that solves the equation at the boundary of feasibility:\n$$k! = N$$\nTo solve for $k$, we must use an approximation for the factorial function. The problem asks to use a well-tested asymptotic approximation, for which we use the logarithmic form of Stirling's approximation, which is highly accurate for non-small $k$:\n$$\\ln(k!) \\approx k \\ln(k) - k$$\nSubstituting this into the natural logarithm of our feasibility equation, $\\ln(k!) = \\ln(N)$, we get:\n$$k \\ln(k) - k \\approx \\ln(N)$$\nTo solve this transcendental equation for $k$, we rearrange it to match the defining form of the Lambert $W$ function, which states that for a given $z$, $W(z)$ is the value that satisfies $W(z) \\exp(W(z)) = z$.\n\nLet's manipulate our equation:\n$$k(\\ln(k) - 1) = \\ln(N)$$\nUsing the identity $1 = \\ln(e)$, we have:\n$$k(\\ln(k) - \\ln(e)) = \\ln(N)$$\nUsing the quotient rule for logarithms:\n$$k \\ln\\left(\\frac{k}{e}\\right) = \\ln(N)$$\nTo get this into the form $Y \\exp(Y) = Z$, let us define a new variable $Y = \\ln\\left(\\frac{k}{e}\\right)$. From this definition, we can exponentiate both sides to get $\\exp(Y) = \\frac{k}{e}$, which can be rearranged to express $k$ in terms of $Y$: $k = e \\exp(Y)$.\n\nNow, we substitute this expression for $k$ back into our equation $k \\ln\\left(\\frac{k}{e}\\right) = \\ln(N)$:\n$$(e \\exp(Y)) Y = \\ln(N)$$\nDividing by the constant $e$, we arrive at the canonical form for the Lambert $W$ function:\n$$Y \\exp(Y) = \\frac{\\ln(N)}{e}$$\nBy the definition of the Lambert $W$ function, the solution for $Y$ is:\n$$Y = W\\left(\\frac{\\ln(N)}{e}\\right)$$\nWe now substitute back our original definition of $Y$, which was $Y = \\ln\\left(\\frac{k}{e}\\right)$:\n$$\\ln\\left(\\frac{k}{e}\\right) = W\\left(\\frac{\\ln(N)}{e}\\right)$$\nTo solve for $k$, we first exponentiate both sides:\n$$\\frac{k}{e} = \\exp\\left(W\\left(\\frac{\\ln(N)}{e}\\right)\\right)$$\nFinally, we multiply by $e$ to isolate $k$. This gives us the continuous approximation $k^{\\star}(N)$:\n$$k^{\\star}(N) = e \\exp\\left(W\\left(\\frac{\\ln(N)}{e}\\right)\\right)$$\nThis expression can be simplified by using the identity $\\exp(W(z)) = \\frac{z}{W(z)}$. Setting $z = \\frac{\\ln(N)}{e}$, we have:\n$$k^{\\star}(N) = e \\left[ \\frac{\\frac{\\ln(N)}{e}}{W\\left(\\frac{\\ln(N)}{e}\\right)} \\right]$$\n$$k^{\\star}(N) = \\frac{\\ln(N)}{W\\left(\\frac{\\ln(N)}{e}\\right)}$$\nThis is the closed-form analytic expression for the continuous approximation of the maximal feasible number of conditions $k$ for a given sample size $N$, derived using Stirling's approximation and the Lambert $W$ function.",
            "answer": "$$\n\\boxed{\\frac{\\ln(N)}{W\\left(\\frac{\\ln(N)}{e}\\right)}}\n$$"
        },
        {
            "introduction": "Since full counterbalancing is often impractical, experimental designers rely on more efficient structures that control for the most likely sources of bias. This exercise introduces the Balanced Latin Square, a powerful design that specifically neutralizes first-order carryover effects—where one trial influences the next. You will move from theory to application by constructing a Balanced Latin Square and analytically verifying its defining properties, gaining hands-on experience with a cornerstone of robust experimental design .",
            "id": "4150560",
            "problem": "A cognitive neuroscience experiment compares $k=6$ stimulus conditions, labeled $1,2,3,4,5,6$, and seeks to eliminate first-order carryover effects by counterbalancing the immediate predecessor and successor relationships among conditions across trial orders. A Balanced Latin Square (BLS) is a design in which, across a set of $k$ orders, each condition appears exactly once at each ordinal position, and for any ordered pair of distinct conditions $(a,b)$ with $a \\neq b$, there is exactly one order in which $a$ immediately precedes $b$ and exactly one order in which $a$ immediately follows $b$.\n\nStarting from the fundamental definitions of a Latin square and first-order carryover effects, derive a Balanced Latin Square of size $k=6$ by explicitly enumerating the $6$ orders. Using your construction, show that the resulting set of orders satisfies the property that each condition immediately precedes and immediately follows every other distinct condition exactly once (ignore wrap-around adjacency from the last position back to the first).\n\nDefine the $6 \\times 6$ adjacency matrix $\\mathbf{A}$ by setting $A_{ab}$ equal to the total number of times condition $a$ immediately precedes condition $b$ across the $6$ orders, with $A_{aa}=0$ for all $a \\in \\{1,2,3,4,5,6\\}$. Compute the determinant of $\\mathbf{A}$ as a single real number. No rounding is required.",
            "solution": "Let the set of $k=6$ stimulus conditions be $C = \\{1, 2, 3, 4, 5, 6\\}$. A Balanced Latin Square for an even number of conditions $k$ can be constructed using the algorithm for a Williams Square.\n\nFirst, the first row (order) of the square is constructed using the sequence:\n$1, 2, k, 3, k-1, 4, k-2, \\dots$\nFor $k=6$, this yields the sequence for the first order:\n$1, 2, 6, 3, 5, 4$\n\nThe subsequent $k-1=5$ rows are generated by adding $j$ to each element of the first row, for $j=1, 2, \\dots, 5$. The addition is performed modulo $k$, where the set of residues is taken as $\\{1, 2, \\dots, k\\}$ (i.e., if a sum is $k+m$, it becomes $m$; if it is $k$, it remains $k$). This process generates the complete square of orders:\n\n$$\n\\begin{pmatrix}\n1 & 2 & 6 & 3 & 5 & 4 \\\\\n2 & 3 & 1 & 4 & 6 & 5 \\\\\n3 & 4 & 2 & 5 & 1 & 6 \\\\\n4 & 5 & 3 & 6 & 2 & 1 \\\\\n5 & 6 & 4 & 1 & 3 & 2 \\\\\n6 & 1 & 5 & 2 & 4 & 3\n\\end{pmatrix}\n$$\nThis is a Latin Square, as each condition from $\\{1, ..., 6\\}$ appears exactly once in each row and each column.\n\nNext, we must show that this design is balanced for first-order carryover effects. This requires showing that for any two distinct conditions $a$ and $b$, the ordered pair $(a,b)$ appears as immediate neighbors exactly once. There are $k(k-1) = 6 \\times 5 = 30$ such ordered pairs. The total number of adjacent pairs in the design is $k \\times (k-1) = 6 \\times 5 = 30$. This property is guaranteed by the construction of the first row. The differences between adjacent elements $(c_{i+1} - c_i) \\pmod 6$ in the first row are $\\{1, 4, 3, 2, 5\\}$, which is the set of all non-zero residues modulo $6$. Because each non-zero difference $d$ appears exactly once in the first row, and each subsequent row is a full cyclic shift of the elements, every possible ordered pair $(a,b)$ with $a \\neq b$ will appear exactly once across all orders. This demonstrates that the constructed square is a Balanced Latin Square as defined.\n\nNow, we define the $6 \\times 6$ adjacency matrix $\\mathbf{A}$, where $A_{ab}$ is the number of times condition $a$ immediately precedes condition $b$. From the property just proven, for any pair of distinct conditions $a, b \\in \\{1, \\dots, 6\\}$, $a$ precedes $b$ exactly once. Therefore, $A_{ab} = 1$ for all $a \\neq b$. The problem specifies that the diagonal elements are zero, so $A_{aa} = 0$.\nThe resulting matrix $\\mathbf{A}$ is:\n$$\n\\mathbf{A} = \\begin{pmatrix}\n0 & 1 & 1 & 1 & 1 & 1 \\\\\n1 & 0 & 1 & 1 & 1 & 1 \\\\\n1 & 1 & 0 & 1 & 1 & 1 \\\\\n1 & 1 & 1 & 0 & 1 & 1 \\\\\n1 & 1 & 1 & 1 & 0 & 1 \\\\\n1 & 1 & 1 & 1 & 1 & 0\n\\end{pmatrix}\n$$\nThis matrix can be expressed as $\\mathbf{A} = \\mathbf{J} - \\mathbf{I}$, where $\\mathbf{J}$ is the $6 \\times 6$ matrix of all ones and $\\mathbf{I}$ is the $6 \\times 6$ identity matrix.\n\nFinally, we compute the determinant of $\\mathbf{A}$. The determinant of a matrix is the product of its eigenvalues. We can find the eigenvalues of $\\mathbf{A}$ by first finding the eigenvalues of $\\mathbf{J}$.\nFor an $n \\times n$ matrix $\\mathbf{J}$ of all ones, its rank is $1$. This implies that there is an eigenvalue $\\lambda=0$ with multiplicity $n-1$. The sum of the eigenvalues is equal to the trace of the matrix, $\\text{Tr}(\\mathbf{J}) = n$. Since $n-1$ eigenvalues are $0$, the remaining eigenvalue must be $n$.\nFor our case, $n=6$, so the eigenvalues of $\\mathbf{J}$ are $6$ (with multiplicity $1$) and $0$ (with multiplicity $5$).\n\nThe matrix $\\mathbf{A}$ is related to $\\mathbf{J}$ by $\\mathbf{A} = \\mathbf{J} - \\mathbf{I}$. If $\\mathbf{v}$ is an eigenvector of $\\mathbf{J}$ with eigenvalue $\\lambda_{\\mathbf{J}}$, then:\n$$ \\mathbf{A}\\mathbf{v} = (\\mathbf{J} - \\mathbf{I})\\mathbf{v} = \\mathbf{J}\\mathbf{v} - \\mathbf{I}\\mathbf{v} = \\lambda_{\\mathbf{J}}\\mathbf{v} - \\mathbf{v} = (\\lambda_{\\mathbf{J}} - 1)\\mathbf{v} $$\nThus, the eigenvalues of $\\mathbf{A}$ are $\\lambda_{\\mathbf{A}} = \\lambda_{\\mathbf{J}} - 1$.\nUsing the eigenvalues of $\\mathbf{J}_6$:\n- The eigenvalue $6$ becomes $6-1=5$ (multiplicity $1$).\n- The eigenvalue $0$ becomes $0-1=-1$ (multiplicity $5$).\n\nThe determinant of $\\mathbf{A}$ is the product of its eigenvalues:\n$$ \\det(\\mathbf{A}) = (5)^1 \\times (-1)^5 = 5 \\times (-1) = -5 $$\n\nAlternatively, a known formula for the determinant of a matrix of this form, $\\mathbf{A}_n = \\mathbf{J}_n - \\mathbf{I}_n$, is $\\det(\\mathbf{A}_n) = (-1)^{n-1}(n-1)$.\nFor $n=6$, this yields:\n$$ \\det(\\mathbf{A}_6) = (-1)^{6-1}(6-1) = (-1)^5(5) = -5 $$\nBoth methods yield the same result.",
            "answer": "$$\n\\boxed{-5}\n$$"
        },
        {
            "introduction": "A perfectly executed experimental design is a beautiful thing, but reality is often messy. In this final practice, we confront a scenario common in neuroscience research: a carefully counterbalanced design that becomes unbalanced due to unavoidable data loss from artifacts and participant errors. This problem will sharpen your critical thinking about the entire lifecycle of an experiment, from design to analysis, by forcing you to diagnose the statistical consequences of broken counterbalancing and identify valid analytical remedies to ensure the integrity of your scientific conclusions .",
            "id": "4150669",
            "problem": "An investigator is analyzing Electroencephalography (EEG) Event-Related Potentials (ERP) from a cognitive neuroscience experiment with $4$ stimulus conditions $\\{C_1, C_2, C_3, C_4\\}$ presented under a fully counterbalanced schedule. Counterbalancing is defined as equal representation of each condition across nuisance factors such as blocks, response sides, and time, implemented by random assignment and structured trial scheduling to achieve equal counts per condition within each block. The experiment contains $8$ blocks, each with $20$ trials, planned as $5$ trials per condition per block, for a total planned count of $40$ trials per condition.\n\nTrials were excluded for two reasons: artifact-related trial rejection and behavioral trial error. The experimenter applied standard preprocessing that rejects trials with ocular artifacts detected by Electrooculography (EOG) and excludes incorrect-response trials from ERP averaging. Empirical exclusion counts are as follows.\n\nArtifact-related trial rejections by block:\n- Block $1$: $C_1$ rejected $4$, $C_2$ rejected $1$, $C_3$ rejected $1$, $C_4$ rejected $0$.\n- Block $2$: $C_1$ rejected $2$, $C_2$ rejected $1$, $C_3$ rejected $1$, $C_4$ rejected $0$.\n- Block $5$: $C_3$ rejected $2$.\n- All other blocks: $0$ rejections.\n\nBehavioral trial errors (incorrect responses), approximately uniform across blocks but condition-dependent in total:\n- $C_1$: $2$ errors, $C_2$: $6$ errors, $C_3$: $10$ errors, $C_4$: $3$ errors.\n\nAssume that ERP averages will be computed only from trials that are both artifact-free and correct. Using only fundamental definitions of counterbalancing (equal condition representation across nuisance factors) and randomization (assignment independent of outcomes), and basic missing data classifications (missing completely at random, missing at random, missing not at random), analyze how these exclusions affect the integrity of the counterbalanced trial structure and the validity of condition comparisons.\n\nWhich option correctly defines trial rejections and trial errors in this context, correctly characterizes their impact on counterbalancing and trial structure integrity in the given scenario, and proposes a scientifically valid analytic remedy?\n\nA. Trial rejection is removal of entire blocks when the participant refuses to continue, and trial error is any noise in the EEG time series. Because the original schedule was randomized, post hoc exclusions do not alter counterbalancing, so the investigator should proceed with unadjusted condition averages.\n\nB. Trial rejection is exclusion of individual trials due to measurement artifact (for example, ocular contamination detected by Electrooculography), and trial error is exclusion of trials with incorrect behavioral responses. Here, exclusions are condition- and block-dependent, violating missing completely at random and inducing both global and within-block imbalances (for example, in blocks $1$–$2$: $C_1$ retains $4$ of $10$ planned trials, $C_2$ retains $8$, $C_3$ retains $8$, $C_4$ retains $10$). This breaks counterbalancing by correlating condition with block/time and difficulty. A valid remedy is to analyze with block-stratified methods that equalize or weight per-condition counts within each block (for example, stratified resampling or inverse probability weighting) and to include block (and other nuisance factors) as fixed effects in a mixed-effects model, thereby restoring comparability without fabricating neural data.\n\nC. Trial rejection is exclusion of trials with incorrect responses, and trial error is experimenter mislabeling of condition. Because behavioral errors add only variance but not bias, removing error trials always improves counterbalancing; further, dropping blocks with many rejections will restore perfect balance without affecting inference.\n\nD. Trial rejection is the same as missing data at random, and trial error is negligible at the aggregate level. To preserve counterbalancing after exclusions, the investigator should impute the rejected ERP epochs by interpolation or should randomly relabel remaining trials to achieve equal counts per condition.",
            "solution": "This problem requires analyzing the statistical consequences of trial exclusions in a counterbalanced design. The correct answer is **B**, and the reasoning is as follows:\n\n1.  **Correct Definitions**: The first step is to correctly define the terms.\n    *   **Trial rejection** refers to the exclusion of trials based on data quality issues, such as physiological artifacts (e.g., eye blinks, muscle activity) that contaminate the EEG signal. This is a standard procedure in ERP analysis.\n    *   **Trial error** refers to the exclusion of trials where the participant made an incorrect behavioral response. This is also standard, as the cognitive processes on error trials are different from those on correct trials.\n    Option B correctly defines these terms. Options A and C provide incorrect definitions.\n\n2.  **Impact on Counterbalancing**: The core of the problem is understanding how these exclusions affect the experimental design. An ideal counterbalanced design ensures that conditions are orthogonal to (i.e., independent of) nuisance factors like time (block number).\n    *   **Unequal Trial Counts**: The total number of valid trials becomes unequal across conditions: $C_1=34$, $C_2=32$, $C_3=26$, $C_4=37$. This breaks the global balance.\n    *   **Systematic Bias**: The critical issue is that the data loss is not random.\n        *   **Condition-dependent loss**: Condition $C_3$ has the most behavioral errors (10), suggesting it may be the most difficult. This means that the remaining $C_3$ trials are not just fewer, but they are also a non-random subset (only the easier or more successfully processed trials).\n        *   **Block-dependent loss**: Artifact rejections for $C_1$ are concentrated in the first two blocks. Out of 10 planned $C_1$ trials in Blocks 1-2, 6 were rejected for artifacts, leaving only 4. In contrast, all 10 planned $C_4$ trials were retained. This means that our measurement of the $C_1$ effect is now disproportionately weighted toward the later blocks, while the $C_4$ effect is evenly weighted across all blocks.\n    *   **Broken Orthogonality**: This systematic data loss breaks the designed counterbalancing. The factor of `Condition` is no longer orthogonal to nuisance factors like `Block` (time) and `Difficulty` (as inferred from error rates). A simple comparison of the average ERPs for each condition would be biased. For instance, a difference between $C_1$ and $C_4$ could be due to the task difference or due to a time-on-task effect (e.g., practice or fatigue), as the data for the two conditions are drawn from different parts of the experiment.\n\n3.  **Missing Data Characterization**: The data are not *Missing Completely At Random* (MCAR), because the probability of a trial being excluded depends on the condition and the block number. It is at best *Missing At Random* (MAR), as the missingness depends on observed variables. Simply ignoring the missing data (i.e., analyzing the remaining trials with simple averages) leads to biased estimates.\n\n4.  **Valid Analytical Remedy**: Given the broken counterbalancing, the analysis must statistically account for the induced confounds.\n    *   **Invalid Remedies**: Option A's suggestion to proceed with unadjusted averages is wrong because it ignores the bias. Option C's claims are incorrect. Option D's suggestions to impute data or randomly relabel trials are forms of data fabrication and are scientifically invalid.\n    *   **Valid Remedy**: Option B proposes the correct solution. A **linear mixed-effects model** is the modern standard for analyzing such unbalanced data. The model can include `Condition` as a fixed effect of interest, while also including `Block` as a fixed or random effect to explicitly model and account for any time-dependent changes in the signal. By including these nuisance factors in the model, their variance is partitioned out, allowing for a less biased estimate of the true `Condition` effects. Other methods like stratified resampling or inverse probability weighting can also address the imbalance but are often precursors or alternatives to a comprehensive mixed-effects model.\n\nIn summary, Option B correctly identifies the problem (non-random data loss breaking counterbalancing) and proposes a valid, modern statistical solution (using a mixed-effects model to account for the induced confounds).",
            "answer": "$$\n\\boxed{B}\n$$"
        }
    ]
}