{
    "hands_on_practices": [
        {
            "introduction": "At the heart of any experimental design is the method of assigning trials to conditions. Even with just two conditions, simple randomization can lead to unwanted runs and imbalances, potentially confounding condition effects with time-varying factors like fatigue or practice. This exercise provides a quantitative foundation for understanding why more structured methods like permuted block randomization are essential, allowing you to appreciate how this technique ensures a more robust trial structure by directly calculating and comparing the probabilities of imbalance .",
            "id": "4150481",
            "problem": "A neuroscience laboratory is designing a two-condition trial structure for a within-session experiment, where each trial is assigned to one of two conditions, denoted as condition $A$ and condition $B$. The laboratory is concerned with the probability of interim imbalance in the counts of conditions during data collection, because imbalance can confound estimates of neural response trajectories within small sample segments.\n\nDefine the imbalance at time $t$ as $I(t) = |n_{A}(t) - n_{B}(t)|$, where $n_{A}(t)$ and $n_{B}(t)$ are the numbers of trials assigned to conditions $A$ and $B$, respectively, among the first $t$ trials. Consider $t = 6$.\n\nTwo randomization schemes are under consideration:\n- Simple randomization: each trial is independently assigned to condition $A$ or condition $B$ with probability $1/2$.\n- Permuted block randomization with fixed block size $4$: every block of $4$ assignments contains exactly $2$ trials of condition $A$ and $2$ trials of condition $B$, in uniformly random order; blocks are concatenated to produce the full sequence.\n\nStarting from the core definitions of independence of assignments under simple randomization and exact within-block balance under permuted block randomization, analytically derive, without invoking any shortcut formulas, the probability that $I(6) \\geq 2$ under each scheme. Then compute the ratio\n$$R = \\frac{\\Pr\\big(I(6) \\geq 2 \\text{ under simple randomization}\\big)}{\\Pr\\big(I(6) \\geq 2 \\text{ under permuted block randomization}\\big)}.$$\nProvide $R$ as a single reduced rational number. Do not round. No units are required in the final answer.",
            "solution": "We begin by formalizing the two schemes and the imbalance event $I(6) \\geq 2$ using basic probability principles.\n\nUnder simple randomization, each of the first $6$ trials is independently assigned to condition $A$ with probability $1/2$ and to condition $B$ with probability $1/2$. Let $X$ denote the number of condition $A$ assignments among the first $6$ trials. By independence and identical distribution, $X$ is distributed as a binomial random variable with parameters $n=6$ and $p=1/2$. The imbalance is $I(6) = |n_{A}(6) - n_{B}(6)| = |X - (6 - X)| = |2X - 6| = 2|X - 3|$. Therefore, the event $I(6) \\geq 2$ occurs exactly when $|X - 3| \\geq 1$, which is equivalent to $X \\neq 3$.\n\nThus,\n$$\n\\Pr\\big(I(6) \\geq 2 \\text{ under simple randomization}\\big) = 1 - \\Pr(X = 3).\n$$\nBy counting arguments, $\\Pr(X = 3)$ equals the number of sequences with exactly $3$ assignments to condition $A$ out of $6$, divided by the total number of sequences. The number of sequences with $3$ assignments to $A$ is $\\binom{6}{3}$, and the total number of sequences is $2^{6}$. Hence,\n$$\n\\Pr(X = 3) = \\frac{\\binom{6}{3}}{2^{6}} = \\frac{20}{64} = \\frac{5}{16}.\n$$\nTherefore,\n$$\n\\Pr\\big(I(6) \\geq 2 \\text{ under simple randomization}\\big) = 1 - \\frac{5}{16} = \\frac{11}{16}.\n$$\n\nUnder permuted block randomization with block size $4$, the first block of $4$ trials contains exactly $2$ of condition $A$ and $2$ of condition $B$ in uniformly random order. After $4$ trials, the imbalance is $I(4) = 0$ by construction. The next $2$ trials belong to the second block, which also contains exactly $2$ of condition $A$ and $2$ of condition $B$ arranged in uniformly random order. Consider the first $2$ draws from this second block. Within this block, the composition prior to any draws is $2$ of $A$ and $2$ of $B$. The probability that the first two draws from the block are both condition $A$ is\n$$\n\\Pr(\\text{AA in first two draws}) = \\frac{2}{4} \\cdot \\frac{1}{3} = \\frac{1}{6},\n$$\nand the probability that the first two draws are both condition $B$ is the same,\n$$\n\\Pr(\\text{BB in first two draws}) = \\frac{2}{4} \\cdot \\frac{1}{3} = \\frac{1}{6}.\n$$\nThe probability that the first two draws are one of each (either $AB$ or $BA$) is\n$$\n\\Pr(\\text{one } A \\text{ and one } B) = 1 - \\left(\\frac{1}{6} + \\frac{1}{6}\\right) = \\frac{2}{3}.\n$$\nCombining with the first block, which contributes $2$ of $A$ and $2$ of $B$, the total counts after $6$ trials are:\n- If the two draws are both $A$, then $n_{A}(6) = 4$ and $n_{B}(6) = 2$, so $I(6) = |4 - 2| = 2$.\n- If the two draws are both $B$, then $n_{A}(6) = 2$ and $n_{B}(6) = 4$, so $I(6) = |2 - 4| = 2$.\n- If the two draws are one $A$ and one $B$, then $n_{A}(6) = 3$ and $n_{B}(6) = 3$, so $I(6) = 0$.\n\nTherefore,\n$$\n\\Pr\\big(I(6) \\geq 2 \\text{ under permuted block randomization}\\big) = \\Pr(\\text{AA or BB in first two draws of block}) = \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{3}.\n$$\n\nWe now compute the ratio\n$$\nR = \\frac{\\Pr\\big(I(6) \\geq 2 \\text{ under simple randomization}\\big)}{\\Pr\\big(I(6) \\geq 2 \\text{ under permuted block randomization}\\big)} = \\frac{\\frac{11}{16}}{\\frac{1}{3}} = \\frac{11}{16} \\cdot 3 = \\frac{33}{16}.\n$$\nThis provides an analytical quantification of how permuted blocking reduces the probability of imbalance relative to simple randomization at $t=6$; the ratio exceeds $1$, confirming the reduction.",
            "answer": "$$\\boxed{\\frac{33}{16}}$$"
        },
        {
            "introduction": "When an experiment involves more than two conditions, we must not only balance their frequency but also control for how they influence one another. First-order carryover effects, where a preceding trial biases the response to the current trial, are a common concern in cognitive neuroscience. This practice demonstrates how to construct a Balanced Latin Square, an elegant and efficient design that systematically neutralizes these carryover effects, allowing you to master a practical tool for designing complex multi-condition experiments .",
            "id": "4150560",
            "problem": "A cognitive neuroscience experiment compares $k=6$ stimulus conditions, labeled $1,2,3,4,5,6$, and seeks to eliminate first-order carryover effects by counterbalancing the immediate predecessor and successor relationships among conditions across trial orders. A Balanced Latin Square (BLS) is a design in which, across a set of $k$ orders, each condition appears exactly once at each ordinal position, and for any ordered pair of distinct conditions $(a,b)$ with $a \\neq b$, there is exactly one order in which $a$ immediately precedes $b$ and exactly one order in which $a$ immediately follows $b$.\n\nStarting from the fundamental definitions of a Latin square and first-order carryover effects, derive a Balanced Latin Square of size $k=6$ by explicitly enumerating the $6$ orders. Using your construction, show that the resulting set of orders satisfies the property that each condition immediately precedes and immediately follows every other distinct condition exactly once (ignore wrap-around adjacency from the last position back to the first).\n\nDefine the $6 \\times 6$ adjacency matrix $\\mathbf{A}$ by setting $A_{ab}$ equal to the total number of times condition $a$ immediately precedes condition $b$ across the $6$ orders, with $A_{aa}=0$ for all $a \\in \\{1,2,3,4,5,6\\}$. Compute the determinant of $\\mathbf{A}$ as a single real number. No rounding is required.",
            "solution": "To construct a Balanced Latin Square for $k=6$ conditions, we use the standard algorithm for a Williams Square. First, construct the initial order using the sequence $1, 2, k, 3, k-1, \\dots$. For $k=6$, this yields:\n$1, 2, 6, 3, 5, 4$\n\nThe subsequent $k-1=5$ orders are generated by cyclically permuting the elements. We add $j$ to each element of the first order for $j=1, 2, \\dots, 5$, where the addition is performed modulo $k$ (with residues taken as $\\{1, 2, \\dots, k\\}$). The resulting $k=6$ orders are:\n- Order 1: $1, 2, 6, 3, 5, 4$\n- Order 2: $2, 3, 1, 4, 6, 5$\n- Order 3: $3, 4, 2, 5, 1, 6$\n- Order 4: $4, 5, 3, 6, 2, 1$\n- Order 5: $5, 6, 4, 1, 3, 2$\n- Order 6: $6, 1, 5, 2, 4, 3$\n\nThe complete $6 \\times 6$ square is:\n$$\n\\begin{pmatrix}\n1  2  6  3  5  4 \\\\\n2  3  1  4  6  5 \\\\\n3  4  2  5  1  6 \\\\\n4  5  3  6  2  1 \\\\\n5  6  4  1  3  2 \\\\\n6  1  5  2  4  3\n\\end{pmatrix}\n$$\nThis design is a Balanced Latin Square, which means that across all 6 orders, every condition is immediately preceded by every other distinct condition exactly once. There are $k \\times (k-1) = 6 \\times 5 = 30$ adjacent pairs in the design, corresponding to the $k(k-1)=30$ possible ordered pairs of distinct conditions.\n\nThe adjacency matrix $\\mathbf{A}$ has elements $A_{ab}$ equal to the number of times condition $a$ immediately precedes condition $b$. From the property of the BLS, $A_{ab} = 1$ for all $a \\neq b$. The diagonal elements are $A_{aa} = 0$. The resulting matrix $\\mathbf{A}$ is:\n$$\n\\mathbf{A} = \\begin{pmatrix}\n0  1  1  1  1  1 \\\\\n1  0  1  1  1  1 \\\\\n1  1  0  1  1  1 \\\\\n1  1  1  0  1  1 \\\\\n1  1  1  1  0  1 \\\\\n1  1  1  1  1  0\n\\end{pmatrix}\n$$\nThis matrix can be written as $\\mathbf{A} = \\mathbf{J} - \\mathbf{I}$, where $\\mathbf{J}$ is the $6 \\times 6$ matrix of all ones and $\\mathbf{I}$ is the $6 \\times 6$ identity matrix.\n\nTo compute the determinant of $\\mathbf{A}$, we find its eigenvalues. The eigenvalues of the all-ones matrix $\\mathbf{J}_n$ are $n$ (with multiplicity 1) and $0$ (with multiplicity $n-1$). For $n=6$, the eigenvalues of $\\mathbf{J}$ are $6$ and $0$ (multiplicity 5). The eigenvalues of $\\mathbf{A} = \\mathbf{J} - \\mathbf{I}$ are obtained by subtracting 1 from the eigenvalues of $\\mathbf{J}$. Thus, the eigenvalues of $\\mathbf{A}$ are:\n- $6 - 1 = 5$ (multiplicity 1)\n- $0 - 1 = -1$ (multiplicity 5)\n\nThe determinant is the product of the eigenvalues:\n$$ \\det(\\mathbf{A}) = (5)^1 \\times (-1)^5 = 5 \\times (-1) = -5 $$",
            "answer": "$$\n\\boxed{-5}\n$$"
        },
        {
            "introduction": "An experimental design that is perfectly counterbalanced on paper rarely survives intact through the process of data collection. Inevitable trial losses from physiological artifacts and participant errors can systematically break the intended balance, reintroducing the very confounds the design was meant to eliminate. This scenario-based problem challenges you to think like a data analyst, diagnosing how real-world data loss compromises an experimental design and evaluating the analytical strategies required to maintain valid inferences in the face of imperfect data .",
            "id": "4150669",
            "problem": "An investigator is analyzing Electroencephalography (EEG) Event-Related Potentials (ERP) from a cognitive neuroscience experiment with $4$ stimulus conditions $\\{C_1, C_2, C_3, C_4\\}$ presented under a fully counterbalanced schedule. Counterbalancing is defined as equal representation of each condition across nuisance factors such as blocks, response sides, and time, implemented by random assignment and structured trial scheduling to achieve equal counts per condition within each block. The experiment contains $8$ blocks, each with $20$ trials, planned as $5$ trials per condition per block, for a total planned count of $40$ trials per condition.\n\nTrials were excluded for two reasons: artifact-related trial rejection and behavioral trial error. The experimenter applied standard preprocessing that rejects trials with ocular artifacts detected by Electrooculography (EOG) and excludes incorrect-response trials from ERP averaging. Empirical exclusion counts are as follows.\n\nArtifact-related trial rejections by block:\n- Block $1$: $C_1$ rejected $4$, $C_2$ rejected $1$, $C_3$ rejected $1$, $C_4$ rejected $0$.\n- Block $2$: $C_1$ rejected $2$, $C_2$ rejected $1$, $C_3$ rejected $1$, $C_4$ rejected $0$.\n- Block $5$: $C_3$ rejected $2$.\n- All other blocks: $0$ rejections.\n\nBehavioral trial errors (incorrect responses), approximately uniform across blocks but condition-dependent in total:\n- $C_1$: $2$ errors, $C_2$: $6$ errors, $C_3$: $10$ errors, $C_4$: $3$ errors.\n\nAssume that ERP averages will be computed only from trials that are both artifact-free and correct. Using only fundamental definitions of counterbalancing (equal condition representation across nuisance factors) and randomization (assignment independent of outcomes), and basic missing data classifications (missing completely at random, missing at random, missing not at random), analyze how these exclusions affect the integrity of the counterbalanced trial structure and the validity of condition comparisons.\n\nWhich option correctly defines trial rejections and trial errors in this context, correctly characterizes their impact on counterbalancing and trial structure integrity in the given scenario, and proposes a scientifically valid analytic remedy?\n\nA. Trial rejection is removal of entire blocks when the participant refuses to continue, and trial error is any noise in the EEG time series. Because the original schedule was randomized, post hoc exclusions do not alter counterbalancing, so the investigator should proceed with unadjusted condition averages.\n\nB. Trial rejection is exclusion of individual trials due to measurement artifact (for example, ocular contamination detected by Electrooculography), and trial error is exclusion of trials with incorrect behavioral responses. Here, exclusions are condition- and block-dependent, violating missing completely at random and inducing both global and within-block imbalances (for example, in blocks $1$â€“$2$: $C_1$ retains $4$ of $10$ planned trials, $C_2$ retains $8$, $C_3$ retains $8$, $C_4$ retains $10$). This breaks counterbalancing by correlating condition with block/time and difficulty. A valid remedy is to analyze with block-stratified methods that equalize or weight per-condition counts within each block (for example, stratified resampling or inverse probability weighting) and to include block (and other nuisance factors) as fixed effects in a mixed-effects model, thereby restoring comparability without fabricating neural data.\n\nC. Trial rejection is exclusion of trials with incorrect responses, and trial error is experimenter mislabeling of condition. Because behavioral errors add only variance but not bias, removing error trials always improves counterbalancing; further, dropping blocks with many rejections will restore perfect balance without affecting inference.\n\nD. Trial rejection is the same as missing data at random, and trial error is negligible at the aggregate level. To preserve counterbalancing after exclusions, the investigator should impute the rejected ERP epochs by interpolation or should randomly relabel remaining trials to achieve equal counts per condition.",
            "solution": "Let's analyze the situation based on the provided data and definitions.\n\n1.  **Definitions**: The problem describes two sources of trial exclusion. \"Artifact-related trial rejection\" refers to removing trials due to data quality issues (e.g., eye blinks contaminating the EEG signal), which is a standard step in EEG/ERP analysis. \"Behavioral trial error\" refers to removing trials where the participant did not perform the task correctly (e.g., pressed the wrong button). Option B correctly defines these terms. Options A and C provide incorrect definitions.\n\n2.  **Impact on Counterbalancing**: A counterbalanced design aims to ensure that conditions are represented equally across nuisance factors like time (blocks). The original design had a perfect balance: 5 trials per condition in each of the 8 blocks. The trial exclusions disrupt this balance.\n    - **Artifact Rejections**: These are not random. They are concentrated in blocks 1 and 2, and within those blocks, they disproportionately affect condition $C_1$. For example, in Block 1, $C_1$ loses 4 trials while $C_4$ loses 0. This means that after exclusions, the remaining $C_1$ trials are underrepresented in the early part of the experiment compared to other conditions. This creates a confound between condition and time/block.\n    - **Behavioral Errors**: These are also not random. They are condition-dependent ($C_3$ has 10 errors, while $C_1$ has 2). This means that the trials included in the final analysis are not an unbiased sample. For $C_3$, the analysis is based on the trials that were easiest for the participant, which can lead to biased ERP estimates.\n    - **Combined Effect**: The combination of these non-random exclusions breaks the original counterbalancing. The data are no longer \"missing completely at random\" (MCAR). The pattern of missingness depends on both the block (time) and the condition itself, which fits the definition of \"missing at random\" (MAR) or \"missing not at random\" (MNAR). This violates the core assumptions needed for simple averaging. Option B correctly identifies this breakdown, even providing a concrete calculation: in blocks 1-2, the planned 10 trials per condition become 4 for $C_1$, 8 for $C_2$, 8 for $C_3$, and 10 for $C_4$.\n\n3.  **Analytic Remedy**: Since the design's balance is broken, a simple comparison of the final average ERPs for each condition is invalid.\n    - Option A's proposal to proceed with unadjusted averages is incorrect because the broken balance introduces bias.\n    - Option C's idea that removing errors improves counterbalancing is wrong. Dropping blocks is also a poor solution as it discards valid data and doesn't guarantee a fix.\n    - Option D's suggestions to impute ERP data or relabel trials are statistically questionable and often invalid. Imputation for complex, high-dimensional data like ERPs is non-trivial and can introduce its own biases. Relabeling is data fabrication.\n    - Option B proposes the most scientifically sound remedies. Using block-stratified analysis methods (like weighting or resampling to equalize contributions from each block) or, more powerfully, using a linear mixed-effects model (LMM) that includes `block` as a factor, allows the analysis to statistically control for the confounding effects of time that were introduced by the trial loss. The LMM can appropriately model the variance structure and account for the imbalances. This approach uses all available valid data while mitigating the bias from the broken counterbalancing.\n\nTherefore, option B provides the most accurate and complete analysis of the problem and its solution.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}