{
    "hands_on_practices": [
        {
            "introduction": "Characterizing the statistical properties of a neuron's firing pattern is a fundamental task in computational neuroscience. The Gamma distribution is a flexible and widely used model for the time between consecutive spikes, known as the interspike interval (ISI). This exercise will guide you through deriving the mean and variance of the Gamma distribution from first principles, using the powerful technique of the moment generating function (MGF) . Mastering this derivation provides a deep understanding of how a distribution's parameters, such as shape $k$ and scale $\\theta$, determine its central tendency and spread.",
            "id": "4188611",
            "problem": "A single neuron recorded in a controlled in vitro setting exhibits interspike intervals (interspike interval; ISI) whose empirical distribution is well modeled by a gamma distribution with shape parameter $k0$ and scale parameter $\\theta0$. The probability density function (pdf) of a gamma-distributed random variable $X$ is given by\n$$\nf_X(x;k,\\theta)=\\frac{1}{\\Gamma(k)\\,\\theta^{k}}\\,x^{k-1}\\,\\exp\\!\\left(-\\frac{x}{\\theta}\\right),\\quad x0,\n$$\nwhere $\\Gamma(k)$ denotes the gamma function. Starting strictly from the definition of the moment generating function (moment generating function; MGF),\n$$\nM_X(t)=\\mathbb{E}\\!\\left[\\exp(tX)\\right],\n$$\nand the given pdf, derive analytical expressions for the expectation $\\mathbb{E}[X]$ and the variance $\\mathrm{Var}(X)$ of the ISI $X$. Your derivation must proceed from first principles: compute $M_X(t)$ as an integral with respect to the pdf, identify the domain of $t$ on which $M_X(t)$ exists, justify any interchange of differentiation and expectation needed to obtain moments, and then obtain $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$ in terms of $k$ and $\\theta$ only. Express your final answer as a single row of two analytic expressions, corresponding to $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$, respectively. No numerical rounding is required.",
            "solution": "The problem requires the derivation of the expectation $\\mathbb{E}[X]$ and variance $\\mathrm{Var}(X)$ of a gamma-distributed random variable $X$, starting from the definition of the moment generating function (MGF). The random variable $X$ has a probability density function (PDF) given by\n$$\nf_X(x;k,\\theta)=\\frac{1}{\\Gamma(k)\\,\\theta^{k}}\\,x^{k-1}\\,\\exp\\!\\left(-\\frac{x}{\\theta}\\right),\\quad x0\n$$\nwhere $k0$ is the shape parameter and $\\theta0$ is the scale parameter.\n\nThe MGF, $M_X(t)$, is defined as the expected value of $\\exp(tX)$:\n$$\nM_X(t) = \\mathbb{E}\\!\\left[\\exp(tX)\\right] = \\int_{-\\infty}^{\\infty} \\exp(tx) f_X(x) \\,dx\n$$\nSince the support of the gamma distribution is $x0$, the integral is taken from $0$ to $\\infty$:\n$$\nM_X(t) = \\int_{0}^{\\infty} \\exp(tx) \\left[ \\frac{1}{\\Gamma(k)\\,\\theta^{k}}\\,x^{k-1}\\,\\exp\\!\\left(-\\frac{x}{\\theta}\\right) \\right] \\,dx\n$$\nWe group the constant terms and combine the exponentials:\n$$\nM_X(t) = \\frac{1}{\\Gamma(k)\\,\\theta^{k}} \\int_{0}^{\\infty} x^{k-1} \\exp\\!\\left(tx - \\frac{x}{\\theta}\\right) \\,dx\n$$\n$$\nM_X(t) = \\frac{1}{\\Gamma(k)\\,\\theta^{k}} \\int_{0}^{\\infty} x^{k-1} \\exp\\!\\left(-x\\left(\\frac{1}{\\theta} - t\\right)\\right) \\,dx\n$$\nFor this integral to converge, the coefficient of $-x$ in the exponent must be positive. This defines the domain of $t$ for which the MGF exists:\n$$\n\\frac{1}{\\theta} - t  0 \\implies t  \\frac{1}{\\theta}\n$$\nTo evaluate the integral, we recognize its form is related to the gamma function, $\\Gamma(z) = \\int_0^\\infty y^{z-1} \\exp(-y) \\,dy$. We perform a change of variables. Let $y = x\\left(\\frac{1}{\\theta} - t\\right) = x\\left(\\frac{1-t\\theta}{\\theta}\\right)$. Then $x = y \\left(\\frac{\\theta}{1-t\\theta}\\right)$ and $dx = \\left(\\frac{\\theta}{1-t\\theta}\\right) \\,dy$. The limits of integration remain $0$ and $\\infty$. Substituting into the integral:\n$$\n\\int_{0}^{\\infty} \\left(y \\frac{\\theta}{1-t\\theta}\\right)^{k-1} \\exp(-y) \\left(\\frac{\\theta}{1-t\\theta}\\right) \\,dy = \\left(\\frac{\\theta}{1-t\\theta}\\right)^k \\int_{0}^{\\infty} y^{k-1} \\exp(-y) \\,dy\n$$\nThe integral on the right is the definition of $\\Gamma(k)$. Therefore, the integral part evaluates to $\\left(\\frac{\\theta}{1-t\\theta}\\right)^k \\Gamma(k)$.\nSubstituting this back into the expression for $M_X(t)$:\n$$\nM_X(t) = \\frac{1}{\\Gamma(k)\\,\\theta^{k}} \\left[ \\left(\\frac{\\theta}{1-t\\theta}\\right)^k \\Gamma(k) \\right] = \\frac{\\Gamma(k)}{\\Gamma(k)\\,\\theta^{k}} \\frac{\\theta^k}{(1-t\\theta)^k} = \\frac{1}{(1-t\\theta)^k}\n$$\nSo, the MGF is $M_X(t) = (1-t\\theta)^{-k}$ for $t  1/\\theta$.\n\nThe moments of $X$ are obtained by differentiating $M_X(t)$ with respect to $t$ and evaluating at $t=0$. Specifically, $\\mathbb{E}[X^n] = M_X^{(n)}(0)$. The interchange of differentiation and expectation is justified under the Leibniz integral rule, as for any $t$ in a closed interval within the domain of convergence, the derivatives of the integrand $\\exp(tx)f_X(x)$ are bounded by an integrable function.\n\nFirst, we find the first moment, $\\mathbb{E}[X]$, by computing the first derivative of $M_X(t)$:\n$$\nM_X'(t) = \\frac{d}{dt}(1-t\\theta)^{-k} = -k(1-t\\theta)^{-k-1}(-\\theta) = k\\theta(1-t\\theta)^{-k-1}\n$$\nEvaluating at $t=0$:\n$$\n\\mathbb{E}[X] = M_X'(0) = k\\theta(1-0)^{-k-1} = k\\theta\n$$\n\nNext, we find the second moment, $\\mathbb{E}[X^2]$, by computing the second derivative of $M_X(t)$:\n$$\nM_X''(t) = \\frac{d}{dt}\\left[ k\\theta(1-t\\theta)^{-k-1} \\right] = k\\theta(-k-1)(1-t\\theta)^{-k-2}(-\\theta) = k(k+1)\\theta^2(1-t\\theta)^{-k-2}\n$$\nEvaluating at $t=0$:\n$$\n\\mathbb{E}[X^2] = M_X''(0) = k(k+1)\\theta^2(1-0)^{-k-2} = k(k+1)\\theta^2 = (k^2+k)\\theta^2\n$$\n\nFinally, the variance, $\\mathrm{Var}(X)$, is given by the relation $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\n$$\n\\mathrm{Var}(X) = (k^2+k)\\theta^2 - (k\\theta)^2 = k^2\\theta^2 + k\\theta^2 - k^2\\theta^2 = k\\theta^2\n$$\nThe expectation of the interspike interval is $\\mathbb{E}[X]=k\\theta$ and the variance is $\\mathrm{Var}(X)=k\\theta^2$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} k\\theta  k\\theta^2 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In data analysis, we often transform raw signals into more informative features, such as calculating instantaneous power from voltage fluctuations. This process corresponds to a mathematical transformation of a random variable, and a key challenge is to determine the probability distribution of the new feature. This practice focuses on deriving the distribution of a power-like surrogate, $Y=X^2$, from a signal $X$ modeled by a Laplace distribution . You will use the cumulative distribution function (CDF) method, a robust and fundamental technique for finding the density of a transformed variable, especially when the transformation is not one-to-one.",
            "id": "4188638",
            "problem": "A neuroscientist models spontaneous Local Field Potential (LFP) amplitude fluctuations recorded from a single cortical site as a symmetric continuous random variable $X$ with probability density function $f_{X}(x)$. In a post-hoc analysis of event energetics, the instantaneous power surrogate $Y$ is defined by the non-monotone transformation $Y = g(X) = X^{2}$. Assume that $X$ is absolutely continuous and symmetric about zero, and that its density is given by the Laplace (double-exponential) model\n$$\nf_{X}(x) = \\frac{1}{2 b} \\exp\\!\\left(-\\frac{|x|}{b}\\right), \\quad b0.\n$$\nStarting only from core definitions in probability theory (the cumulative distribution function and the probability density function) and the transformation $Y = X^{2}$, derive the probability density function $f_{Y}(y)$ for $y0$ by explicitly handling the non-monotonicity of $g$ through appropriate partitioning of the domain of $X$. Express your final answer as a single closed-form analytic expression in terms of $y$ and $b$. Do not attach physical units to your expression. No numerical rounding is required.",
            "solution": "The problem is to derive the probability density function (PDF) of a transformed random variable. We are given a continuous random variable $X$ with a Laplace distribution, and a transformation $Y=X^2$. We must find the PDF of $Y$, denoted $f_Y(y)$.\n\nThe given probability density function for $X$ is\n$$\nf_{X}(x) = \\frac{1}{2 b} \\exp\\left(-\\frac{|x|}{b}\\right)\n$$\nfor a parameter $b0$. The random variable $X$ can take any real value, so its support is $(-\\infty, \\infty)$. The transformation is $Y = g(X) = X^2$. Since $X$ is a real-valued variable, $Y$ must be non-negative. We are asked to find the PDF $f_Y(y)$ for $y  0$.\n\nThe prescribed method is to start from the definition of the cumulative distribution function (CDF). The CDF of $Y$, denoted $F_Y(y)$, is defined as the probability that the random variable $Y$ takes a value less than or equal to $y$.\n$$\nF_Y(y) = P(Y \\le y)\n$$\nSince $Y$ must be non-negative, $F_Y(y) = 0$ for any $y  0$. For $y \\ge 0$, we substitute the definition of $Y$ in terms of $X$:\n$$\nF_Y(y) = P(X^2 \\le y)\n$$\nThe core of the problem lies in handling the non-monotonic nature of the function $g(X) = X^2$. The event $X^2 \\le y$ for a given $y0$ is equivalent to the event that $X$ lies within the closed interval $[-\\sqrt{y}, \\sqrt{y}]$. This is because if $x$ is in this interval, its square $x^2$ will be less than or equal to $(\\pm\\sqrt{y})^2 = y$. Conversely, if $x^2 \\le y$, then $|x| \\le \\sqrt{y}$, which means $-\\sqrt{y} \\le x \\le \\sqrt{y}$.\nThus, we can write the CDF of $Y$ in terms of an interval probability for $X$:\n$$\nF_Y(y) = P(-\\sqrt{y} \\le X \\le \\sqrt{y})\n$$\nThis probability can be calculated by integrating the PDF of $X$ over this interval:\n$$\nF_Y(y) = \\int_{-\\sqrt{y}}^{\\sqrt{y}} f_X(x) \\, dx\n$$\nSubstituting the given expression for $f_X(x)$:\n$$\nF_Y(y) = \\int_{-\\sqrt{y}}^{\\sqrt{y}} \\frac{1}{2b} \\exp\\left(-\\frac{|x|}{b}\\right) \\, dx\n$$\nThe integrand, $\\frac{1}{2b} \\exp\\left(-\\frac{|x|}{b}\\right)$, is an even function of $x$ because $|-x| = |x|$. The integral of an even function over a symmetric interval $[-a, a]$ is equal to twice the integral over $[0, a]$. Therefore, we can simplify the integral:\n$$\nF_Y(y) = 2 \\int_{0}^{\\sqrt{y}} \\frac{1}{2b} \\exp\\left(-\\frac{|x|}{b}\\right) \\, dx = \\frac{1}{b} \\int_{0}^{\\sqrt{y}} \\exp\\left(-\\frac{|x|}{b}\\right) \\, dx\n$$\nFor $x$ in the integration range $[0, \\sqrt{y}]$, we have $x \\ge 0$, so $|x|=x$. The expression for the CDF becomes:\n$$\nF_Y(y) = \\frac{1}{b} \\int_{0}^{\\sqrt{y}} \\exp\\left(-\\frac{x}{b}\\right) \\, dx\n$$\nWe now evaluate this definite integral:\n$$\nF_Y(y) = \\frac{1}{b} \\left[ -b \\exp\\left(-\\frac{x}{b}\\right) \\right]_{0}^{\\sqrt{y}} = - \\left[ \\exp\\left(-\\frac{x}{b}\\right) \\right]_{0}^{\\sqrt{y}}\n$$\n$$\nF_Y(y) = - \\left( \\exp\\left(-\\frac{\\sqrt{y}}{b}\\right) - \\exp\\left(-\\frac{0}{b}\\right) \\right) = - \\left( \\exp\\left(-\\frac{\\sqrt{y}}{b}\\right) - 1 \\right)\n$$\nThis gives the closed-form expression for the CDF of $Y$ for $y0$:\n$$\nF_Y(y) = 1 - \\exp\\left(-\\frac{\\sqrt{y}}{b}\\right)\n$$\nTo find the probability density function $f_Y(y)$, we differentiate the CDF $F_Y(y)$ with respect to $y$. This is valid for $y0$ where the CDF is differentiable.\n$$\nf_Y(y) = \\frac{d}{dy} F_Y(y) = \\frac{d}{dy} \\left( 1 - \\exp\\left(-\\frac{\\sqrt{y}}{b}\\right) \\right)\n$$\nThe derivative of the constant $1$ is $0$. We apply the chain rule to the exponential term. Let $u(y) = -\\frac{\\sqrt{y}}{b} = -\\frac{1}{b}y^{1/2}$. The derivative of $u(y)$ is $\\frac{du}{dy} = -\\frac{1}{b} \\left(\\frac{1}{2} y^{-1/2}\\right) = -\\frac{1}{2b\\sqrt{y}}$.\n$$\nf_Y(y) = 0 - \\frac{d}{dy} \\exp\\left(-\\frac{\\sqrt{y}}{b}\\right) = - \\exp\\left(-\\frac{\\sqrt{y}}{b}\\right) \\cdot \\frac{d}{dy}\\left(-\\frac{\\sqrt{y}}{b}\\right)\n$$\n$$\nf_Y(y) = - \\exp\\left(-\\frac{\\sqrt{y}}{b}\\right) \\cdot \\left(-\\frac{1}{2b\\sqrt{y}}\\right)\n$$\nSimplifying this expression yields the final PDF for $Y$:\n$$\nf_Y(y) = \\frac{1}{2b\\sqrt{y}} \\exp\\left(-\\frac{\\sqrt{y}}{b}\\right)\n$$\nThis expression is the probability density function for the transformed variable $Y=X^2$ for $y0$.",
            "answer": "$$\\boxed{\\frac{1}{2b\\sqrt{y}} \\exp\\left(-\\frac{\\sqrt{y}}{b}\\right)}$$"
        },
        {
            "introduction": "A central goal of data analysis is to infer latent, unobservable properties from measured data. In spike train analysis, we observe discrete spike counts and wish to infer the neuron's underlying firing rate, $\\Lambda$. This exercise introduces the Bayesian approach to this problem, where prior knowledge about the firing rate is formally combined with observed data to yield an updated, or posterior, understanding . You will work through a classic model using a Poisson likelihood for the spike counts and its conjugate Gamma prior for the rate, a combination that forms a cornerstone of Bayesian modeling in neuroscience.",
            "id": "4188620",
            "problem": "In a spike-train experiment, a single neuron's action potentials are binned into $m$ disjoint time windows with known exposure durations $\\{t_{i}\\}_{i=1}^{m}$, where each $t_{i}  0$. Let $X_{i}$ denote the spike count observed in window $i$, and assume conditional independence given a constant firing rate $\\Lambda$ across the session. Adopt the probabilistic model that, given $\\Lambda$, each $X_{i}$ follows a Poisson distribution with mean $t_{i}\\Lambda$, that is $X_{i} \\mid \\Lambda \\sim \\mathrm{Poisson}(t_{i}\\Lambda)$, independently for $i = 1,\\dots,m$. Place a Gamma prior on the firing rate with shape $\\alpha0$ and rate $\\beta0$, written $\\Lambda \\sim \\mathrm{Gamma}(\\alpha,\\beta)$, with density $p(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda)$ for $\\lambda0$.\n\nUsing only the definition of the Poisson probability mass function and Bayesâ€™ rule, derive the posterior distribution $p(\\lambda \\mid x_{1:m})$ for $\\Lambda$ given observed counts $x_{1},\\dots,x_{m}$. Then, provide a single closed-form analytic expression for the posterior mean $\\mathbb{E}[\\Lambda \\mid X_{1}=x_{1},\\dots,X_{m}=x_{m}]$ in terms of $\\alpha$, $\\beta$, $\\{x_{i}\\}_{i=1}^{m}$, and $\\{t_{i}\\}_{i=1}^{m}$. Your final answer must be a single simplified analytic expression.",
            "solution": "The objective is to derive the posterior distribution of the firing rate $\\Lambda$ and then compute its expected value. We are given the following:\n1.  The likelihood for the spike count $X_i$ in a time window of duration $t_i$ is governed by a Poisson distribution with mean $t_i\\Lambda$. The probability mass function (PMF) is $P(X_i=x_i \\mid \\Lambda=\\lambda) = \\frac{(t_i\\lambda)^{x_i} \\exp(-t_i\\lambda)}{x_i!}$.\n2.  The prior distribution for the unknown firing rate $\\Lambda$ is a Gamma distribution, $\\Lambda \\sim \\mathrm{Gamma}(\\alpha, \\beta)$, with the probability density function (PDF) given by $p(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta\\lambda)$ for $\\lambda0$.\n3.  The observations $X_1, \\dots, X_m$ are conditionally independent given $\\Lambda$.\n\nWe apply Bayes' rule to find the posterior distribution $p(\\lambda \\mid x_1, \\dots, x_m)$. For notational convenience, let $x_{1:m}$ represent the set of observations $\\{x_1, \\dots, x_m\\}$. Bayes' rule states that the posterior is proportional to the product of the likelihood and the prior:\n$$p(\\lambda \\mid x_{1:m}) \\propto p(x_{1:m} \\mid \\lambda) p(\\lambda)$$\n\nFirst, we formulate the joint likelihood $p(x_{1:m} \\mid \\lambda)$. Due to the conditional independence of the observations $X_i$, the joint likelihood is the product of the individual likelihoods:\n$$p(x_{1:m} \\mid \\lambda) = \\prod_{i=1}^{m} p(x_i \\mid \\lambda) = \\prod_{i=1}^{m} \\frac{(t_i \\lambda)^{x_i} \\exp(-t_i \\lambda)}{x_i!}$$\n\nWe can separate the terms that depend on $\\lambda$ from those that do not:\n$$p(x_{1:m} \\mid \\lambda) = \\left( \\prod_{i=1}^{m} \\frac{t_i^{x_i}}{x_i!} \\right) \\left( \\prod_{i=1}^{m} \\lambda^{x_i} \\right) \\left( \\prod_{i=1}^{m} \\exp(-t_i\\lambda) \\right)$$\nSimplifying the products involving $\\lambda$:\n$$\\prod_{i=1}^{m} \\lambda^{x_i} = \\lambda^{\\sum_{i=1}^{m} x_i}$$\n$$\\prod_{i=1}^{m} \\exp(-t_i\\lambda) = \\exp\\left(-\\sum_{i=1}^{m} t_i\\lambda\\right) = \\exp\\left(-\\lambda \\sum_{i=1}^{m} t_i\\right)$$\nSince we are concerned with the posterior distribution of $\\lambda$, we can treat any term not involving $\\lambda$ as part of the proportionality constant. Thus, the likelihood is proportional to:\n$$p(x_{1:m} \\mid \\lambda) \\propto \\lambda^{\\sum_{i=1}^{m} x_i} \\exp\\left(-\\lambda \\sum_{i=1}^{m} t_i\\right)$$\n\nNext, we introduce the prior distribution for $\\Lambda$. The PDF is given as $p(\\lambda) \\propto \\lambda^{\\alpha-1} \\exp(-\\beta\\lambda)$.\n\nNow, we multiply the likelihood and the prior to find the unnormalized posterior distribution:\n$$p(\\lambda \\mid x_{1:m}) \\propto \\left( \\lambda^{\\sum_{i=1}^{m} x_i} \\exp\\left(-\\lambda \\sum_{i=1}^{m} t_i\\right) \\right) \\cdot \\left( \\lambda^{\\alpha-1} \\exp(-\\beta\\lambda) \\right)$$\nCombining the exponents of $\\lambda$ and the arguments of the exponential function, we get:\n$$p(\\lambda \\mid x_{1:m}) \\propto \\lambda^{\\left(\\alpha + \\sum_{i=1}^{m} x_i\\right) - 1} \\exp\\left(-\\left(\\beta + \\sum_{i=1}^{m} t_i\\right)\\lambda\\right)$$\n\nThis expression is the kernel of a Gamma distribution. A general Gamma distribution, $\\mathrm{Gamma}(\\alpha_{\\text{shape}}, \\beta_{\\text{rate}})$, has a PDF of the form $p(y) \\propto y^{\\alpha_{\\text{shape}}-1}\\exp(-\\beta_{\\text{rate}} y)$. By comparing our posterior functional form to this general form, we can identify the parameters of the posterior distribution for $\\Lambda$.\n\nThe posterior shape parameter, let's call it $\\alpha'$, is:\n$$\\alpha' = \\alpha + \\sum_{i=1}^{m} x_i$$\nThe posterior rate parameter, let's call it $\\beta'$, is:\n$$\\beta' = \\beta + \\sum_{i=1}^{m} t_i$$\nThus, the posterior distribution of $\\Lambda$ given the data $x_{1:m}$ is a Gamma distribution:\n$$\\Lambda \\mid X_{1:m}=x_{1:m} \\sim \\mathrm{Gamma}\\left(\\alpha + \\sum_{i=1}^{m} x_i, \\beta + \\sum_{i=1}^{m} t_i\\right)$$\nThis answers the first part of the problem.\n\nThe second part of the problem is to find the posterior mean, $\\mathbb{E}[\\Lambda \\mid x_{1:m}]$. A key property of the Gamma distribution $\\mathrm{Gamma}(\\alpha_{\\text{shape}}, \\beta_{\\text{rate}})$ is that its mean (expected value) is given by the ratio of its shape and rate parameters: $\\mathbb{E}[Y] = \\frac{\\alpha_{\\text{shape}}}{\\beta_{\\text{rate}}}$.\n\nApplying this formula to our posterior distribution with parameters $\\alpha'$ and $\\beta'$, we find the posterior mean of $\\Lambda$:\n$$\\mathbb{E}[\\Lambda \\mid X_{1}=x_{1},\\dots,X_{m}=x_{m}] = \\frac{\\alpha'}{\\beta'} = \\frac{\\alpha + \\sum_{i=1}^{m} x_i}{\\beta + \\sum_{i=1}^{m} t_i}$$\nThis is the final closed-form analytic expression for the posterior mean in terms of the given parameters and data.",
            "answer": "$$\\boxed{\\frac{\\alpha + \\sum_{i=1}^{m} x_i}{\\beta + \\sum_{i=1}^{m} t_i}}$$"
        }
    ]
}