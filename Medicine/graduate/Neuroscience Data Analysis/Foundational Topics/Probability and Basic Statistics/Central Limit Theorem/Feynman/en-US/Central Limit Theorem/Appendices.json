{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's explore a foundational application of the Central Limit Theorem combined with the Delta method. In neuroscience, the firing patterns of neurons are often modeled using probability distributions, and the exponential distribution is a common choice for describing the time between consecutive spikes (inter-spike intervals). This exercise will guide you through calculating the uncertainty (asymptotic variance) of an estimate that is a function of the neuron's firing rate, a fundamental skill for quantifying the reliability of derived neural parameters .",
            "id": "852388",
            "problem": "Consider a set of $n$ independent and identically distributed (i.i.d.) random variables, $X_1, X_2, \\ldots, X_n$, drawn from an exponential distribution with a rate parameter $\\lambda > 0$. The probability density function (PDF) for a single observation $X_i$ is given by:\n$$f(x_i; \\lambda) = \\lambda e^{-\\lambda x_i} \\quad \\text{for} \\quad x_i \\ge 0$$\n\nLet $\\hat{\\lambda}_n$ be the maximum likelihood estimator (MLE) for the parameter $\\lambda$. From the asymptotic theory of MLEs, it is known that $\\hat{\\lambda}_n$ is asymptotically normal. Specifically,\n$$\n\\sqrt{n}(\\hat{\\lambda}_n - \\lambda) \\xrightarrow{d} N\\left(0, I(\\lambda)^{-1}\\right)\n$$\nas $n \\to \\infty$, where $I(\\lambda)$ is the Fisher information contained in a single observation from this distribution.\n\nThe Delta method provides a way to find the asymptotic distribution of a transformed estimator. If a sequence of random variables $T_n$ satisfies $\\sqrt{n}(T_n - \\theta) \\xrightarrow{d} N(0, \\sigma^2)$, then for a continuously differentiable function $g$ with $g'(\\theta) \\neq 0$, the transformed sequence $g(T_n)$ satisfies:\n$$\n\\sqrt{n}(g(T_n) - g(\\theta)) \\xrightarrow{d} N\\left(0, [g'(\\theta)]^2 \\sigma^2\\right)\n$$\nThe term $[g'(\\theta)]^2 \\sigma^2$ is defined as the asymptotic variance of the estimator $g(T_n)$.\n\nYour task is to derive the asymptotic variance of the MLE for the parameter $\\theta = \\lambda^2$. The estimator for $\\theta$ is constructed by substituting the MLE of $\\lambda$, i.e., $\\hat{\\theta}_n = (\\hat{\\lambda}_n)^2$.",
            "solution": "1. The Fisher information for one exponential observation with rate $\\lambda$ is:\n$$I(\\lambda)=\\mathrm{E}\\left[\\left(\\frac{\\partial}{\\partial\\lambda}\\ln f(X;\\lambda)\\right)^2\\right]=\\frac{1}{\\lambda^2}.$$\nThe asymptotic variance of the MLE $\\hat\\lambda_n$ is $I(\\lambda)^{-1}/n = \\lambda^2/n$. This implies that the scaled estimator converges in distribution as follows:\n$$\\sqrt{n}(\\hat\\lambda_n-\\lambda)\\xrightarrow{d}N\\bigl(0,\\lambda^2\\bigr).$$\n2. We are interested in the parameter $\\theta=\\lambda^2$. We define the transformation as $g(\\lambda)=\\lambda^2$. The derivative is $g'(\\lambda)=2\\lambda$. By the Delta method, the transformed estimator converges in distribution:\n$$\\sqrt{n}\\bigl(g(\\hat\\lambda_n)-g(\\lambda)\\bigr)\\xrightarrow{d}N\\bigl(0,(g'(\\lambda))^2\\lambda^2\\bigr).$$\n3. The asymptotic variance of $g(\\hat\\lambda_n)$ is the variance term in the limiting normal distribution:\n$$(g'(\\lambda))^2\\,\\lambda^2=(2\\lambda)^2\\lambda^2=4\\lambda^4.$$",
            "answer": "$$\\boxed{4\\lambda^4}$$"
        },
        {
            "introduction": "Building on the univariate case, we now move into multiple dimensions, a scenario frequently encountered when analyzing spatial or multidimensional neural data. Imagine you have estimated the center of a neuron's receptive field as a 2D coordinate; you might then want to describe this location in polar coordinates to understand its preferred direction and eccentricity. This practice demonstrates how to use the multivariate Delta method to propagate uncertainty from one coordinate system to another, a vital technique for understanding how the variances and covariances of your estimates are transformed .",
            "id": "852428",
            "problem": "Let $(X_i, Y_i)$ for $i=1, \\dots, n$ be a sequence of $n$ independent and identically distributed (i.i.d.) random vectors, where each $(X_i, Y_i)$ is drawn from a uniform distribution on the unit square $[0, 1] \\times [0, 1]$. Let $(\\bar{X}_n, \\bar{Y}_n)^T = \\frac{1}{n} \\sum_{i=1}^n (X_i, Y_i)^T$ be the sample mean vector.\n\nThe multivariate Central Limit Theorem states that if $\\mathbf{Z}_n$ is the sample mean of i.i.d. random vectors with population mean $\\mu$ and covariance matrix $\\Sigma$, then the random vector $\\sqrt{n}(\\mathbf{Z}_n - \\mu)$ converges in distribution to a multivariate normal distribution $N(\\mathbf{0}, \\Sigma)$.\n\nThe multivariate Delta method provides the asymptotic distribution for a transformed random vector. If $g: \\mathbb{R}^k \\to \\mathbb{R}^m$ is a continuously-differentiable function, then $\\sqrt{n}(g(\\mathbf{Z}_n) - g(\\mu))$ converges in distribution to $N(\\mathbf{0}, G \\Sigma G^T)$, where $G$ is the $m \\times k$ Jacobian matrix of $g$ with entries $G_{ij} = \\frac{\\partial g_i}{\\partial z_j}$, evaluated at the point $\\mu$.\n\nLet $(R_n, \\Theta_n)$ be the representation of the sample mean vector $(\\bar{X}_n, \\bar{Y}_n)$ in polar coordinates, where the radius is $R_n = \\sqrt{\\bar{X}_n^2 + \\bar{Y}_n^2}$ and the angle is $\\Theta_n = \\arctan(\\bar{Y}_n / \\bar{X}_n)$. Let $(r_\\mu, \\theta_\\mu)$ be the polar coordinates corresponding to the population mean vector $\\mu = E[(X_i, Y_i)^T]$.\n\nLet $\\Sigma_{R\\Theta}$ be the $2 \\times 2$ covariance matrix of the limiting normal distribution for the vector $(\\sqrt{n}(R_n - r_\\mu), \\sqrt{n}(\\Theta_n - \\theta_\\mu))^T$.\n\nDerive the trace of this asymptotic covariance matrix, $\\text{Tr}(\\Sigma_{R\\Theta})$.",
            "solution": "1. The population mean is $\\mu = (E[X], E[Y])^T = (1/2, 1/2)^T$. The covariance matrix is $\\Sigma = \\text{diag}(\\text{Var}(X), \\text{Var}(Y)) = \\text{diag}(1/12, 1/12)$.\n2. Define the transformation $g(x,y) = (r, \\theta)^T$ where $r = \\sqrt{x^2+y^2}$ and $\\theta = \\arctan(y/x)$. The Jacobian matrix $G$ has entries:\n$$ \\frac{\\partial r}{\\partial x} = \\frac{x}{r}, \\quad \\frac{\\partial r}{\\partial y} = \\frac{y}{r}, \\quad \\frac{\\partial \\theta}{\\partial x} = -\\frac{y}{x^2+y^2}, \\quad \\frac{\\partial \\theta}{\\partial y} = \\frac{x}{x^2+y^2} $$\n3. At the mean $\\mu=(1/2, 1/2)$, we have $r_\\mu = \\sqrt{(1/2)^2 + (1/2)^2} = 1/\\sqrt{2}$. Evaluating the partial derivatives at $\\mu$:\n$$ \\frac{\\partial r}{\\partial x} = \\frac{1/2}{1/\\sqrt{2}} = \\frac{\\sqrt{2}}{2}, \\quad \\frac{\\partial r}{\\partial y} = \\frac{1/2}{1/\\sqrt{2}} = \\frac{\\sqrt{2}}{2} $$\n$$ \\frac{\\partial \\theta}{\\partial x} = -\\frac{1/2}{(1/2)^2+(1/2)^2} = -1, \\quad \\frac{\\partial \\theta}{\\partial y} = \\frac{1/2}{(1/2)^2+(1/2)^2} = 1 $$\n4. The Jacobian evaluated at $\\mu$ is $G = \\begin{pmatrix} \\sqrt{2}/2 & \\sqrt{2}/2 \\\\ -1 & 1 \\end{pmatrix}$. The asymptotic covariance matrix is $\\Sigma_{R\\Theta} = G \\Sigma G^T = \\frac{1}{12} G G^T$.\n5. The trace is $\\text{Tr}(\\Sigma_{R\\Theta}) = \\frac{1}{12} \\text{Tr}(G G^T)$. Using the property that $\\text{Tr}(GG^T)$ is the sum of the squared elements of $G$:\n$$ \\text{Tr}(\\Sigma_{R\\Theta}) = \\frac{1}{12} \\left[ \\left(\\frac{\\sqrt{2}}{2}\\right)^2 + \\left(\\frac{\\sqrt{2}}{2}\\right)^2 + (-1)^2 + 1^2 \\right] = \\frac{1}{12} \\left( \\frac{1}{2} + \\frac{1}{2} + 1 + 1 \\right) = \\frac{3}{12} = \\frac{1}{4} $$",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "This final practice integrates the concepts into a common experimental paradigm: comparing two populations. Neuroscientists often need to determine whether a neuron's behavior changes between two conditions, for example, before and after administering a drug. The log-odds ratio is a powerful metric for quantifying the magnitude of this change in probability. Here, you will derive the asymptotic variance of the log-odds ratio estimator, providing the statistical foundation needed to construct confidence intervals and test hypotheses about the efficacy of an experimental manipulation .",
            "id": "852421",
            "problem": "Consider two independent random samples, of sizes $n_1$ and $n_2$ respectively, drawn from two distinct Bernoulli populations. The first population has a success probability of $p_1$, and the second has a success probability of $p_2$. Let $X_1$ and $X_2$ be the number of successes observed in the first and second samples, respectively. The sample proportions are then given by $\\hat{p}_1 = \\frac{X_1}{n_1}$ and $\\hat{p}_2 = \\frac{X_2}{n_2}$.\n\nIn many statistical analyses, particularly in epidemiology and clinical trials, the log-odds ratio is a quantity of significant interest. The true log-odds ratio is defined as $\\theta = \\log\\left(\\frac{p_1/(1-p_1)}{p_2/(1-p_2)}\\right)$. An estimator for this quantity, based on the sample proportions, is the sample log-odds ratio:\n$$\n\\hat{\\theta} = \\log\\left(\\frac{\\hat{p}_1/(1-\\hat{p}_1)}{\\hat{p}_2/(1-\\hat{p}_2)}\\right)\n$$\nAssuming that the sample sizes $n_1$ and $n_2$ are sufficiently large, the Central Limit Theorem can be extended via the Delta Method to find the approximate distribution of $\\hat{\\theta}$.\n\nDerive the asymptotic variance of the log-odds ratio estimator, $\\text{Var}(\\hat{\\theta})$.",
            "solution": "The problem asks for the asymptotic variance of the log-odds ratio estimator $\\hat{\\theta}$. We can find this using the multivariate Delta Method.\n\nFirst, by the Central Limit Theorem, for large sample sizes $n_1$ and $n_2$, the sample proportions $\\hat{p}_1$ and $\\hat{p}_2$ are approximately normally distributed:\n$$\n\\hat{p}_1 \\approx N\\left(p_1, \\frac{p_1(1-p_1)}{n_1}\\right)\n$$\n$$\n\\hat{p}_2 \\approx N\\left(p_2, \\frac{p_2(1-p_2)}{n_2}\\right)\n$$\nSince the two samples are independent, the random variables $\\hat{p}_1$ and $\\hat{p}_2$ are also independent. Therefore, the vector of sample proportions $\\hat{\\mathbf{p}} = (\\hat{p}_1, \\hat{p}_2)^T$ has an asymptotic bivariate normal distribution with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$:\n$$\n\\boldsymbol{\\mu} = E[\\hat{\\mathbf{p}}] = \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix}\n$$\n$$\n\\boldsymbol{\\Sigma} = \\text{Cov}(\\hat{\\mathbf{p}}) = \\begin{pmatrix} \\text{Var}(\\hat{p}_1) & \\text{Cov}(\\hat{p}_1, \\hat{p}_2) \\\\ \\text{Cov}(\\hat{p}_1, \\hat{p}_2) & \\text{Var}(\\hat{p}_2) \\end{pmatrix} = \\begin{pmatrix} \\frac{p_1(1-p_1)}{n_1} & 0 \\\\ 0 & \\frac{p_2(1-p_2)}{n_2} \\end{pmatrix}\n$$\nThe estimator $\\hat{\\theta}$ is a function of $\\hat{p}_1$ and $\\hat{p}_2$. Let this function be $g(x, y)$:\n$$\ng(x, y) = \\log\\left(\\frac{x/(1-x)}{y/(1-y)}\\right) = \\log(x) - \\log(1-x) - \\log(y) + \\log(1-y)\n$$\nThe multivariate Delta Method states that the asymptotic variance of a function $g(\\hat{\\mathbf{p}})$ is given by:\n$$\n\\text{Var}(g(\\hat{\\mathbf{p}})) \\approx (\\nabla g(\\boldsymbol{\\mu}))^T \\boldsymbol{\\Sigma} (\\nabla g(\\boldsymbol{\\mu}))\n$$\nwhere $\\nabla g(\\boldsymbol{\\mu})$ is the gradient of $g$ evaluated at the mean vector $\\boldsymbol{\\mu} = (p_1, p_2)^T$.\n\nFirst, we compute the gradient of $g(x, y)$:\n$$\n\\frac{\\partial g}{\\partial x} = \\frac{1}{x} - \\frac{1}{1-x}(-1) = \\frac{1}{x} + \\frac{1}{1-x} = \\frac{1-x+x}{x(1-x)} = \\frac{1}{x(1-x)}\n$$\n$$\n\\frac{\\partial g}{\\partial y} = -\\frac{1}{y} + \\frac{1}{1-y}(-1) = -\\frac{1}{y} - \\frac{1}{1-y} = -\\frac{1-y+y}{y(1-y)} = -\\frac{1}{y(1-y)}\n$$\nSo the gradient vector is $\\nabla g(x,y) = \\left(\\frac{1}{x(1-x)}, -\\frac{1}{y(1-y)}\\right)^T$.\n\nNext, we evaluate the gradient at the mean $\\boldsymbol{\\mu} = (p_1, p_2)^T$:\n$$\n\\nabla g(\\boldsymbol{\\mu}) = \\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n$$\nNow, we can substitute the gradient and the covariance matrix into the Delta Method formula for the variance:\n$$\n\\text{Var}(\\hat{\\theta}) \\approx \n\\begin{pmatrix} \\frac{1}{p_1(1-p_1)} & -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n\\begin{pmatrix} \\frac{p_1(1-p_1)}{n_1} & 0 \\\\ 0 & \\frac{p_2(1-p_2)}{n_2} \\end{pmatrix}\n\\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n$$\nSince $\\boldsymbol{\\Sigma}$ is diagonal, this simplifies to the sum of the squared partial derivatives weighted by the respective variances:\n$$\n\\text{Var}(\\hat{\\theta}) \\approx \\left(\\frac{\\partial g}{\\partial p_1}\\right)^2 \\text{Var}(\\hat{p}_1) + \\left(\\frac{\\partial g}{\\partial p_2}\\right)^2 \\text{Var}(\\hat{p}_2)\n$$\n$$\n= \\left(\\frac{1}{p_1(1-p_1)}\\right)^2 \\left(\\frac{p_1(1-p_1)}{n_1}\\right) + \\left(-\\frac{1}{p_2(1-p_2)}\\right)^2 \\left(\\frac{p_2(1-p_2)}{n_2}\\right)\n$$\n$$\n= \\frac{1}{n_1 p_1 (1-p_1)} + \\frac{1}{n_2 p_2 (1-p_2)}\n$$\nThis is the asymptotic variance of the log-odds ratio estimator.",
            "answer": "$$\n\\boxed{\\frac{1}{n_1 p_1 (1-p_1)} + \\frac{1}{n_2 p_2 (1-p_2)}}\n$$"
        }
    ]
}