{
    "hands_on_practices": [
        {
            "introduction": "神经元放电率的精确估计是系统神经科学中的一项基本任务。这个练习将泊松过程作为神经尖峰计数的模型，并引导你推导关于放电率参数的关键统计量——费雪信息。通过从第一性原理出发，你将深入理解我们的观测数据中包含了多少关于潜在放电率的信息，以及这个信息量如何随着观测时间的延长而变化，为实验设计和评估估计精度提供了理论基础。",
            "id": "4146747",
            "problem": "在系统神经科学的一项尖峰计数实验中，单个神经元被建模为根据一个具有未知恒定速率参数 $\\lambda$（尖峰数/秒）的齐次泊松过程（homogeneous Poisson process）发放尖峰。您在一个时长为 $T$（秒）的连续观测窗口内观测到总尖峰数 $K$。假设在给定 $\\lambda$ 的条件下，计数值 $K$ 是一个均值为 $\\lambda T$ 的泊松随机变量。请仅使用基本定义和经过验证的结论来推导 $K$ 中包含的关于 $\\lambda$ 的费雪信息（Fisher information）。\n\n从以下基本要素出发：泊松随机变量的概率质量函数，观测数据的似然（likelihood）和对数似然（log-likelihood）的定义，费雪信息作为对数似然函数关于参数的负二阶导数的期望的定义，以及可以从泊松分布定义中证明的标准矩恒等式。请勿在未经推导的情况下假设或引用任何关于泊松分布费雪信息的特定结果。\n\n此外，假设观测窗口被划分为 $N$ 个独立的、长度相等的区间，每个区间的长度为 $\\Delta$，使得 $T = N \\Delta$。设 $K_{1},\\dots,K_{N}$ 表示这些区间内的尖峰计数值。在齐次泊松过程模型下，证明独立观测的信息可加性如何能恢复出您为单窗口情况推导出的相同表达式，并解释对于尖峰速率估计而言，费雪信息如何随总观测时间 $T$ 缩放。\n\n请以仅含 $\\lambda$ 和 $T$ 的单个闭式解析表达式的形式给出最终答案。最终答案中请勿包含单位。本题无需进行数值取整。",
            "solution": "本题要求在给定观测时长 $T$ 内的尖峰计数 $K$ 的情况下，推导齐次泊松过程速率参数 $\\lambda$ 的费雪信息。推导过程必须从第一性原理出发。\n\n首先，我们建立统计模型。在时长为 $T$ 的时间区间内，尖峰计数 $K$ 是一个服从泊松分布的随机变量，其均值为 $\\mu = \\lambda T$。观测到 $k$ 个尖峰的概率质量函数 (PMF) 由下式给出：\n$$P(K=k | \\lambda) = \\frac{(\\lambda T)^k \\exp(-\\lambda T)}{k!}$$\n其中 $k \\in \\{0, 1, 2, \\dots\\}$。\n\n对于一个观测到的计数值 $K$，似然函数 $L(\\lambda|K)$ 被定义为视为参数 $\\lambda$ 的函数的概率质量函数：\n$$L(\\lambda|K) = P(K=K | \\lambda) = \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!}$$\n对数似然函数 $\\ell(\\lambda|K) = \\ln L(\\lambda|K)$ 通常在求导时更为方便：\n$$\\ell(\\lambda|K) = \\ln\\left( \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!} \\right)$$\n利用对数的性质，上式可简化为：\n$$\\ell(\\lambda|K) = K \\ln(\\lambda T) - \\lambda T - \\ln(K!)$$\n$$\\ell(\\lambda|K) = K(\\ln \\lambda + \\ln T) - \\lambda T - \\ln(K!)$$\n\n费雪信息 $I(\\lambda)$ 定义为对数似然函数关于参数 $\\lambda$ 的负二阶导数的期望。我们首先计算导数。一阶导数，即得分函数（score function），为：\n$$\\frac{\\partial \\ell}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left[ K\\ln \\lambda + K\\ln T - \\lambda T - \\ln(K!) \\right]$$\n在对 $\\lambda$ 求导时，将 $K$ 和 $T$ 视为常数：\n$$\\frac{\\partial \\ell}{\\partial \\lambda} = \\frac{K}{\\lambda} - T$$\n\n接下来，我们计算二阶导数：\n$$\\frac{\\partial^2 \\ell}{\\partial \\lambda^2} = \\frac{\\partial}{\\partial \\lambda} \\left( \\frac{K}{\\lambda} - T \\right) = -\\frac{K}{\\lambda^2}$$\n\n费雪信息 $I(\\lambda)$ 随后通过取该二阶导数负值的期望来计算。期望 $E[\\cdot]$ 是关于数据分布（在本例中即随机变量 $K \\sim \\text{Poisson}(\\lambda T)$）计算的。\n$$I(\\lambda) = E\\left[ - \\frac{\\partial^2 \\ell}{\\partial \\lambda^2} \\right] = E\\left[ - \\left( -\\frac{K}{\\lambda^2} \\right) \\right] = E\\left[ \\frac{K}{\\lambda^2} \\right]$$\n由于 $\\lambda$ 相对于关于 $K$ 的期望是一个常数，我们可以写出：\n$$I(\\lambda) = \\frac{1}{\\lambda^2} E[K]$$\n均值为 $\\lambda T$ 的泊松随机变量的期望恰好是其均值。因此，$E[K] = \\lambda T$。将此代入 $I(\\lambda)$ 的方程中：\n$$I(\\lambda) = \\frac{1}{\\lambda^2} (\\lambda T) = \\frac{T}{\\lambda}$$\n这就是单个观测值 $K$ 中所包含的关于 $\\lambda$ 的费雪信息。\n\n现在，我们来解决问题的第二部分。总时长为 $T$ 的观测窗口被划分为 $N$ 个独立的、等长的区间，每个区间的长度为 $\\Delta = T/N$。设 $K_1, \\dots, K_N$ 为这些相应区间内的尖峰计数值。对于齐次泊松过程，不相交时间区间内的尖峰计数是独立的随机变量。每个区间 $i$ 内的计数值 $K_i$ 服从均值为 $\\lambda \\Delta$ 的泊松分布。\n\n由于观测的独立性，观测集 $\\{K_1, \\dots, K_N\\}$ 的总似然是各个似然的乘积：\n$$L_{total}(\\lambda | K_1, \\dots, K_N) = \\prod_{i=1}^N L_i(\\lambda | K_i)$$\n因此，总对数似然是各个对数似然的和：\n$$\\ell_{total}(\\lambda | K_1, \\dots, K_N) = \\ln\\left( \\prod_{i=1}^N L_i(\\lambda | K_i) \\right) = \\sum_{i=1}^N \\ln(L_i(\\lambda | K_i)) = \\sum_{i=1}^N \\ell_i(\\lambda | K_i)$$\n这组独立观测的费雪信息是从总对数似然中推导出来的。由于微分和期望的线性性质，总费雪信息是来自每个独立观测的费雪信息之和：\n$$I_{total}(\\lambda) = E\\left[ -\\frac{\\partial^2 \\ell_{total}}{\\partial \\lambda^2} \\right] = E\\left[ -\\sum_{i=1}^N \\frac{\\partial^2 \\ell_i}{\\partial \\lambda^2} \\right] = \\sum_{i=1}^N E\\left[ -\\frac{\\partial^2 \\ell_i}{\\partial \\lambda^2} \\right] = \\sum_{i=1}^N I_i(\\lambda)$$\n每个 $I_i(\\lambda)$ 是来自时长为 $\\Delta$ 的单个观测值 $K_i$ 的费雪信息。我们可以使用前面的结果，将 $T$ 替换为 $\\Delta$：\n$$I_i(\\lambda) = \\frac{\\Delta}{\\lambda}$$\n总信息是所有 $N$ 个区间的总和：\n$$I_{total}(\\lambda) = \\sum_{i=1}^N \\frac{\\Delta}{\\lambda} = N \\frac{\\Delta}{\\lambda}$$\n由于总时长为 $T = N \\Delta$，我们可以将其代回：\n$$I_{total}(\\lambda) = \\frac{N\\Delta}{\\lambda} = \\frac{T}{\\lambda}$$\n这个结果证实了独立观测的信息可加性原理，并恢复了为单个未分割观测窗口推导出的相同表达式。\n\n结果 $I(\\lambda) = T/\\lambda$ 的解释是，数据提供的关于速率参数 $\\lambda$ 的信息量与总观测时间 $T$ 成线性关系。这是很直观的：更长的观测周期提供更多的数据，从而可以更精确地估计潜在速率。克拉默-拉奥下界（Cramér-Rao Lower Bound）指出，任何无偏估计量 $\\hat{\\lambda}$ 的方差都有一个下界 $\\text{Var}(\\hat{\\lambda}) \\ge 1/I(\\lambda)$，这意味着 $\\text{Var}(\\hat{\\lambda}) \\ge \\lambda/T$。这表明，可达到的最小估计方差随观测时间成反比减小，意味着速率估计的精度随观测时间的平方根而提高。",
            "answer": "$$\\boxed{\\frac{T}{\\lambda}}$$"
        },
        {
            "introduction": "虽然泊松分布是尖峰计数的基石模型，但真实的神经数据通常表现出“超离散”现象，即计数的变异性大于其均值。这个练习通过构建一个分层模型，即伽马-泊松混合模型，来解决这个问题，你将亲手证明这个混合模型如何自然地引出负二项分布。通过将理论推导与编程实现相结合，你将学会如何为潜在放电率的试次间波动进行建模，这是捕捉神经活动真实变异性的关键一步。",
            "id": "4146731",
            "problem": "一个短暂分析时间窗内的神经元脉冲计数可以被建模为从具有瞬时发放率的泊松 (Poisson) 分布中的一次抽样。在现实场景中，由于潜在调制因子的影响，该发放率在重复试验中会发生波动。建模这种波动的一种原则性方法是假设潜在发放率本身是随机的。考虑一个分层模型，其中潜在发放率 $\\lambda$ 在试验间是随机的，而给定 $\\lambda$ 条件下的观测计数 $N$ 服从泊松分布。\n\n你需要从第一性原理出发，论证该分层模型如何导出一个负二项 (Negative Binomial) 分布的 $N$，然后实现一个模拟器，使用这种混合构造来生成合成的脉冲计数。然后，应针对几个测试用例对该模拟器进行验证，这些测试用例探究了不同的离散度区间，包括一个接近泊松极限的区间和一个具有强过度离散的区间，这些区间对于神经元变异性分析具有科学意义。验证应依据模型所蕴含的理论均值和方差进行。\n\n仅使用以下基础理论：泊松 (Poisson) 随机变量的概率质量函数、伽马 (Gamma) 随机变量的概率密度函数、伽马函数的性质、全概率定律以及全期望和全方差定律。在推导过程中，不要使用任何负二项分布的目标公式；相反，应从这些基础元素推导出所需的表达式。\n\n待完成的任务：\n\n- 从条件模型 $N \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$ 和先验 $\\lambda \\sim \\mathrm{Gamma}(k,\\beta)$ 出发，其中 $k>0$ 是形状参数，$\\beta>0$ 是速率参数。仅使用全概率定律以及泊松概率质量函数和伽马概率密度函数的定义，推导 $N$ 的边缘分布，并证明它是一个由 $k$ 和一个用 $\\beta$ 表示的成功概率 $p$ 参数化的负二项分布。然后，仅使用全期望定律和全方差定律，推导出 $\\mathbb{E}[N]$ 和 $\\mathrm{Var}(N)$ 关于 $k$ 和 $\\beta$ 的表达式。\n\n- 将混合抽样过程解释为对潜在发放率波动的抽样：解释从伽马 (Gamma) 分布中抽取 $\\lambda$ 如何建模试验间的发放率变异性，以及随后的泊松 (Poisson) 抽样如何建模试验内的计数噪声。\n\n- 设计并实现一个算法，通过先抽样 $\\lambda$ 然后抽样 $N \\mid \\lambda$ 的方式，在该分层模型下模拟 $N$ 的 $T$ 次独立抽样。实现必须接受元组 $(k,\\beta,T,\\mathrm{seed})$，并返回模拟计数的经验均值和方差，以及它们的理论对应值和一个基于相对误差容限的通过/失败布尔值。\n\n- 验证和输出规范。对于每个测试用例，使用固定的伪随机种子模拟计数并计算：\n  - 经验均值 $\\hat{m}$，\n  - 经验方差 $\\hat{v}$，\n  - 理论均值 $m_\\mathrm{th}$，\n  - 理论方差 $v_\\mathrm{th}$，\n  - 经验方差均值比 $r_\\mathrm{emp}=\\hat{v}/\\hat{m}$，\n  - 理论方差均值比 $r_\\mathrm{th}=v_\\mathrm{th}/m_\\mathrm{th}$。\n  均值的相对误差容限为 $\\tau_m=0.02$，方差的相对误差容限为 $\\tau_v=0.05$。如果一个测试用例同时满足 $\\lvert \\hat{m}-m_\\mathrm{th}\\rvert/m_\\mathrm{th}\\le \\tau_m$ 和 $\\lvert \\hat{v}-v_\\mathrm{th}\\rvert/v_\\mathrm{th}\\le \\tau_v$，则该测试用例通过。\n\n待实现和评估的测试套件：\n\n- 用例 A (一般过度离散)：$k=10.0$，$\\beta=2.0$， $T=120000$，$\\mathrm{seed}=12345$。\n- 用例 B (通过小潜在方差接近泊松极限)：$k=1000.0$，$\\beta=200.0$，$T=200000$，$\\mathrm{seed}=24680$。\n- 用例 C (强过度离散，重尾发放率)：$k=0.5$，$\\beta=0.5$，$T=150000$，$\\mathrm{seed}=13579$。\n\n最终输出格式要求：\n\n你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果。每个对应于一个测试用例的元素本身必须是一个形式为 $[\\hat{m},\\hat{v},m_\\mathrm{th},v_\\mathrm{th},r_\\mathrm{emp},r_\\mathrm{th},\\mathrm{pass}]$ 的列表，其中各项条目需严格按此顺序排列，数值量为浮点类型，通过指示符为布尔类型。因此，最终输出应类似于一个包含三个内部列表的单一列表，每个内部列表对应一个测试用例，例如 $[[\\cdots],[\\cdots],[\\cdots]]$，不含任何附加文本。此任务不涉及物理单位或角度，所有量均为无量纲。",
            "solution": "该问题要求从第一性原理出发，论证用于神经元脉冲计数的伽马-泊松 (Gamma-Poisson) 混合模型会得到一个负二项 (Negative Binomial) 分布。这涉及到推导脉冲计数 $N$ 的边缘概率质量函数 (PMF) 及其理论均值和方差。此外，必须实现一个模拟来验证这些理论结果。整个过程都基于概率论的基本原理。\n\n首先，我们对该分层模型提供一个概念性解释。该模型分两个阶段指定：\n1. 潜在发放率 $\\lambda$ 从伽马 (Gamma) 分布中抽取：$\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$，其中 $k>0$ 是形状参数，$\\beta>0$ 是速率参数。这个阶段建模了神经元兴奋性在试验间的缓慢波动。在实验环境中，诸如注意力、动机或缓慢的神经调质变化等因素，可能导致神经元的平均发放率在重复试验中发生变化。对于建模这种非负的连续发放率变量，伽马分布是一个灵活且在数学上方便的选择。该分布的方差 $\\mathrm{Var}(\\lambda) = k/\\beta^2$ 量化了这种试验间发放率变异性的大小。\n2. 在给定试验的特定发放率 $\\lambda$ 的条件下，观测到的脉冲计数 $N$ 从泊松 (Poisson) 分布中抽取：$N \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$。这个阶段建模了脉冲生成的内在随机性。对于一个固定的潜在神经元状态（即固定的 $\\lambda$），发放动作电位是一个时间上的随机点过程，可以很好地用泊松过程来近似。该过程的方差等于其均值，即 $\\mathrm{Var}(N \\mid \\lambda) = \\lambda$。这通常被称为“计数噪声”或“试验内”变异性。\n\n通过结合这两种随机性来源，该模型捕捉了真实神经元数据的一个关键特征：过度离散，即试验间脉冲计数的总方差大于平均计数。\n\n接下来，我们推导 $N$ 的边缘分布。问题陈述了给定发放率 $\\lambda$ 的计数 $N$ 的条件分布是泊松分布，发放率 $\\lambda$ 的先验分布是伽马分布。各自的概率函数是：\n- 泊松 PMF：$P(N=n \\mid \\lambda) = \\frac{\\lambda^n e^{-\\lambda}}{n!}$，对于 $n \\in \\{0, 1, 2, \\dots\\}$。\n- 伽马 PDF：$p(\\lambda \\mid k, \\beta) = \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} e^{-\\beta\\lambda}$，对于 $\\lambda > 0$。\n\n为了求 $N$ 的边缘概率质量函数，我们应用全概率定律，对潜在变量 $\\lambda$ 的所有可能值进行积分：\n$$P(N=n) = \\int_{0}^{\\infty} P(N=n \\mid \\lambda) \\, p(\\lambda \\mid k, \\beta) \\, d\\lambda$$\n代入泊松 PMF 和伽马 PDF 的具体形式：\n$$P(N=n) = \\int_{0}^{\\infty} \\left( \\frac{\\lambda^n e^{-\\lambda}}{n!} \\right) \\left( \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} e^{-\\beta\\lambda} \\right) d\\lambda$$\n我们可以将不依赖于 $\\lambda$ 的项组合到积分符号之外：\n$$P(N=n) = \\frac{\\beta^k}{n! \\Gamma(k)} \\int_{0}^{\\infty} \\lambda^{n+k-1} e^{-(\\beta+1)\\lambda} d\\lambda$$\n该积分具有伽马分布核的形式。回想一下，一个形状为 $\\alpha$、速率为 $\\delta$ 的伽马 PDF 的积分为 1：\n$$\\int_{0}^{\\infty} \\frac{\\delta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\delta x} dx = 1 \\implies \\int_{0}^{\\infty} x^{\\alpha-1} e^{-\\delta x} dx = \\frac{\\Gamma(\\alpha)}{\\delta^\\alpha}$$\n在我们的积分中，我们可以确定形状为 $\\alpha = n+k$，速率为 $\\delta = \\beta+1$。应用这个公式，该积分的计算结果为：\n$$\\int_{0}^{\\infty} \\lambda^{(n+k)-1} e^{-(\\beta+1)\\lambda} d\\lambda = \\frac{\\Gamma(n+k)}{(\\beta+1)^{n+k}}$$\n将此结果代回到 $P(N=n)$ 的表达式中：\n$$P(N=n) = \\frac{\\beta^k}{n! \\Gamma(k)} \\frac{\\Gamma(n+k)}{(\\beta+1)^{n+k}}$$\n为了证明这是一个负二项分布，我们重新整理这些项。二项式系数 $\\binom{a}{b}$ 可以用伽马函数表示为 $\\frac{\\Gamma(a+1)}{b! \\Gamma(a-b+1)}$。一个更通用的形式是 $\\binom{n+r-1}{n} = \\frac{\\Gamma(n+r)}{n! \\Gamma(r)}$。使用这个恒等式，并令 $r=k$：\n$$P(N=n) = \\frac{\\Gamma(n+k)}{n! \\Gamma(k)} \\frac{\\beta^k}{(\\beta+1)^k (\\beta+1)^n} = \\binom{n+k-1}{n} \\left(\\frac{\\beta}{\\beta+1}\\right)^k \\left(\\frac{1}{\\beta+1}\\right)^n$$\n这是参数为 $r=k$、成功概率为 $p = \\frac{\\beta}{\\beta+1}$ 的负二项 (Negative Binomial) 分布的 PMF。它表示在取得 $r$ 次成功之前失败了 $n$ 次的概率。因此，我们证明了 $N \\sim \\mathrm{NB}(k, p = \\frac{\\beta}{\\beta+1})$。\n\n接下来，我们使用全期望定律和全方差定律推导 $N$ 的理论均值 ($m_\\mathrm{th}$) 和方差 ($v_\\mathrm{th}$)，而不依赖于负二项分布的已知公式。\n全期望定律表述为：$\\mathbb{E}[N] = \\mathbb{E}[\\mathbb{E}[N \\mid \\lambda]]$。\n- 内部期望是参数为 $\\lambda$ 的泊松分布的均值：$\\mathbb{E}[N \\mid \\lambda] = \\lambda$。\n- 外部期望是 $\\lambda$ 的期望，它服从 $\\mathrm{Gamma}(k, \\beta)$ 分布。伽马分布的均值是形状/速率。所以，$\\mathbb{E}[\\lambda] = k/\\beta$。\n- 因此，$N$ 的理论均值为：\n$$m_\\mathrm{th} = \\mathbb{E}[N] = \\mathbb{E}[\\lambda] = \\frac{k}{\\beta}$$\n\n全方差定律表述为：$\\mathrm{Var}(N) = \\mathbb{E}[\\mathrm{Var}(N \\mid \\lambda)] + \\mathrm{Var}(\\mathbb{E}[N \\mid \\lambda])$。\n- 对于第一项，$\\mathbb{E}[\\mathrm{Var}(N \\mid \\lambda)]$：泊松分布的方差等于其均值，所以 $\\mathrm{Var}(N \\mid \\lambda) = \\lambda$。因此该项的期望是 $\\mathbb{E}[\\lambda] = k/\\beta$。\n- 对于第二项，$\\mathrm{Var}(\\mathbb{E}[N \\mid \\lambda])$：我们已经确定 $\\mathbb{E}[N \\mid \\lambda] = \\lambda$。因此该项就是 $\\mathrm{Var}(\\lambda)$。$\\mathrm{Gamma}(k, \\beta)$ 分布的方差是形状/速率$^2$，所以 $\\mathrm{Var}(\\lambda) = k/\\beta^2$。\n- 将这两项相加得到 $N$ 的理论方差：\n$$v_\\mathrm{th} = \\mathrm{Var}(N) = \\frac{k}{\\beta} + \\frac{k}{\\beta^2}$$\n这证实了方差大于均值，因为在给定的约束条件 $k>0$ 和 $\\beta>0$ 下，$k/\\beta^2 > 0$。\n\n最后，理论方差均值比为：\n$$r_\\mathrm{th} = \\frac{v_\\mathrm{th}}{m_\\mathrm{th}} = \\frac{k/\\beta + k/\\beta^2}{k/\\beta} = 1 + \\frac{1}{\\beta}$$\n这个比率，也称为 Fano 因子，是离散度的一种度量。值为 1 表示一个泊松过程，而大于 1 的值表示过度离散。在该模型中，过度离散的程度仅由伽马速率参数 $\\beta$ 控制。当 $\\beta \\to \\infty$ 时，$r_\\mathrm{th} \\to 1$，模型接近泊松极限。这是因为 $\\mathrm{Var}(\\lambda) = k/\\beta^2 \\to 0$，意味着潜在发放率 $\\lambda$ 变为非随机的。\n\n该实现将模拟这个两阶段过程，计算经验统计量 ($\\hat{m}$, $\\hat{v}$)，使用 $\\tau_m=0.02$ 和 $\\tau_v=0.05$ 的相对误差容限将其与理论值 ($m_\\mathrm{th}$, $v_\\mathrm{th}$)进行比较，并报告每个测试用例的通过/失败状态。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation_and_validation(k, beta, T, seed):\n    \"\"\"\n    Simulates spike counts from a Gamma-Poisson mixture model and validates\n    empirical statistics against theoretical predictions.\n\n    Args:\n        k (float): Shape parameter of the Gamma distribution for the rate λ.\n        beta (float): Rate parameter of the Gamma distribution for the rate λ.\n        T (int): Number of independent trials to simulate.\n        seed (int): Seed for the pseudo-random number generator.\n\n    Returns:\n        list: A list containing the following values in order:\n              [empirical mean, empirical variance, theoretical mean,\n               theoretical variance, empirical variance-to-mean ratio,\n               theoretical variance-to-mean ratio, pass/fail boolean].\n    \"\"\"\n    # Set up the random number generator with a specific seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # --- Simulation Step ---\n    # Step 1: Sample T latent rates from the Gamma distribution.\n    # numpy.random.Generator.gamma uses a scale parameter, which is 1/rate.\n    scale = 1.0 / beta\n    lambda_samples = rng.gamma(shape=k, scale=scale, size=T)\n\n    # Step 2: For each sampled rate, sample a spike count from the Poisson distribution.\n    n_samples = rng.poisson(lam=lambda_samples)\n\n    # --- Analysis and Validation Step ---\n    # Calculate empirical statistics from the simulated counts\n    m_hat = np.mean(n_samples)\n    # Use ddof=1 for the unbiased sample variance estimate\n    v_hat = np.var(n_samples, ddof=1)\n    \n    # Avoid division by zero if the empirical mean is zero\n    if m_hat == 0:\n        r_emp = np.nan\n    else:\n        r_emp = v_hat / m_hat\n\n    # Calculate theoretical statistics based on the derived formulas\n    m_th = k / beta\n    v_th = (k / beta) + (k / (beta**2))\n    \n    if m_th == 0:\n        r_th = np.nan\n    else:\n        r_th = v_th / m_th\n\n    # Define validation tolerances\n    tau_m = 0.02  # Relative error tolerance for the mean\n    tau_v = 0.05  # Relative error tolerance for the variance\n\n    # Check if the simulation passes the validation criteria\n    # Handle cases where theoretical mean/variance could be zero to avoid division by zero\n    passed = True\n    if m_th > 0:\n        err_m = abs(m_hat - m_th) / m_th\n        if err_m > tau_m:\n            passed = False\n    elif m_hat != 0: # If m_th is 0, m_hat should also be 0\n        passed = False\n        \n    if v_th > 0:\n        err_v = abs(v_hat - v_th) / v_th\n        if err_v > tau_v:\n            passed = False\n    elif v_hat != 0: # If v_th is 0, v_hat should also be 0\n        passed = False\n\n    return [m_hat, v_hat, m_th, v_th, r_emp, r_th, passed]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (k, beta, T, seed)\n    test_cases = [\n        (10.0, 2.0, 120000, 12345),   # Case A: General overdispersion\n        (1000.0, 200.0, 200000, 24680), # Case B: Near-Poisson limit\n        (0.5, 0.5, 150000, 13579),    # Case C: Strong overdispersion\n    ]\n\n    results = []\n    for case in test_cases:\n        k, beta, T, seed = case\n        result_case = run_simulation_and_validation(k, beta, T, seed)\n        results.append(result_case)\n\n    # Final print statement in the exact required format.\n    # The default string representation for a Python list is used.\n    # map(str, results) would stringify each inner list, which is not the desired format.\n    # Instead, we directly stringify the list of lists.\n    # To match the required output format of [val,val,...] instead of Python's [val, val, ...],\n    # we build the string manually.\n    outer_list_str = []\n    for res in results:\n        # Convert boolean to lowercase 'true'/'false' if needed, but 'True'/'False' is standard\n        # The prompt just says \"boolean\", so Python's default str() is acceptable. Let's use it.\n        inner_list_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]},{res[5]},{res[6]}]\"\n        outer_list_str.append(inner_list_str)\n\n    print(f\"[{','.join(outer_list_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "从尖峰计数转向对连续特征的分析，我们将探讨高斯分布在神经科学数据分析中的核心应用之一：尖峰分选。当细胞外记录捕捉到多个神经元的活动时，我们需要将混合的尖峰波形分离到各自的来源。本练习将引导你为从波形中提取的特征向量构建一个高斯混合模型，并从第一性原理推导期望最大化（EM）算法的更新方程，这是在存在未观测的潜在变量（即每个尖峰的来源神经元）时，进行模型参数估计的强大框架。",
            "id": "4146702",
            "problem": "皮层微电极阵列的细胞外记录产生多单元活动（MUA），其中包含来自多个神经元同时产生的尖峰事件。对于每个检测到的尖峰事件，通过标准预处理提取一个特征向量，例如，将波形投影到通过主成分分析（PCA）获得的前几个主成分上。假设有 $N$ 个事件，其特征向量为 $\\mathbf{x}_{n} \\in \\mathbb{R}^{d}$，其中 $n \\in \\{1,\\dots,N\\}$。假设每个事件源于 $K$ 个潜在神经元来源之一，并且以来源 $k \\in \\{1,\\dots,K\\}$ 为条件，其特征分布是具有各向同性协方差的多元高斯分布。具体来说，我们设定高斯混合模型为\n$$\np(\\mathbf{x}_{n} \\mid \\boldsymbol{\\Theta}) \\;=\\; \\sum_{k=1}^{K} \\pi_{k} \\,\\mathcal{N}\\!\\big(\\mathbf{x}_{n} \\,\\big|\\, \\boldsymbol{\\mu}_{k}, \\sigma_{k}^{2}\\mathbf{I}\\big),\n$$\n其中 $\\boldsymbol{\\Theta} = \\big\\{ \\{\\pi_{k}\\}_{k=1}^{K}, \\{\\boldsymbol{\\mu}_{k}\\}_{k=1}^{K}, \\{\\sigma_{k}^{2}\\}_{k=1}^{K} \\big\\}$，$\\pi_{k} \\in (0,1)$ 是满足 $\\sum_{k=1}^{K} \\pi_{k} = 1$ 的混合权重，$\\boldsymbol{\\mu}_{k} \\in \\mathbb{R}^{d}$ 是成分均值向量，$\\sigma_{k}^{2} > 0$ 是成分方差。具有各向同性协方差的多元高斯密度定义为\n$$\n\\mathcal{N}\\!\\big(\\mathbf{x} \\,\\big|\\, \\boldsymbol{\\mu}, \\sigma^{2}\\mathbf{I}\\big) \\;=\\; (2\\pi \\sigma^{2})^{-\\tfrac{d}{2}} \\exp\\!\\Big(-\\tfrac{1}{2\\sigma^{2}} \\big\\|\\mathbf{x}-\\boldsymbol{\\mu}\\big\\|^{2}\\Big).\n$$\n使用最大似然原理和多元高斯密度的定义，并引入潜在指示变量 $z_{nk} \\in \\{0,1\\}$（满足 $\\sum_{k=1}^{K} z_{nk} = 1$）来表示未知的来源分配，为该模型构建期望最大化（EM）算法。从第一性原理推导以下各项的显式更新方程：\n- 在期望步骤中，后验概率（responsibilities）$r_{nk} \\equiv p(z_{nk}=1 \\mid \\mathbf{x}_{n}, \\boldsymbol{\\Theta})$，\n- 在最大化步骤中，成分均值 $\\boldsymbol{\\mu}_{k}$，\n- 在最大化步骤中，成分方差 $\\sigma_{k}^{2}$，假设每个成分 $k$ 都具有各向同性协方差 $\\sigma_{k}^{2}\\mathbf{I}$。\n\n您的最终答案必须将 $r_{nk}$、$\\boldsymbol{\\mu}_{k}$ 和 $\\sigma_{k}^{2}$ 的三个闭式解析表达式作为一个复合表达式呈现。无需四舍五入。最终答案无需单位。确保所有符号与上述定义一致。",
            "solution": "用户希望通过期望最大化（EM）算法，为具有各向同性协方差的高斯混合模型（GMM）找到更新方程。\n\n### 第一步：问题验证\n\n**1.1. 提取已知条件**\n\n- **模型：** 针对 $N$ 个特征向量 $\\mathbf{x}_{n} \\in \\mathbb{R}^{d}$ 的高斯混合模型（GMM）。\n- **混合模型密度：** $p(\\mathbf{x}_{n} \\mid \\boldsymbol{\\Theta}) = \\sum_{k=1}^{K} \\pi_{k} \\,\\mathcal{N}\\!\\big(\\mathbf{x}_{n} \\,\\big|\\, \\boldsymbol{\\mu}_{k}, \\sigma_{k}^{2}\\mathbf{I}\\big)$。\n- **参数：** $\\boldsymbol{\\Theta} = \\big\\{ \\{\\pi_{k}\\}_{k=1}^{K}, \\{\\boldsymbol{\\mu}_{k}\\}_{k=1}^{K}, \\{\\sigma_{k}^{2}\\}_{k=1}^{K} \\big\\}$。\n- **混合权重：** $\\pi_{k} \\in (0,1)$，满足 $\\sum_{k=1}^{K} \\pi_{k} = 1$。\n- **成分均值：** $\\boldsymbol{\\mu}_{k} \\in \\mathbb{R}^{d}$。\n- **成分方差：** $\\sigma_{k}^{2} > 0$。\n- **各向同性高斯密度：** $\\mathcal{N}\\!\\big(\\mathbf{x} \\,\\big|\\, \\boldsymbol{\\mu}, \\sigma^{2}\\mathbf{I}\\big) = (2\\pi \\sigma^{2})^{-\\tfrac{d}{2}} \\exp\\!\\Big(-\\tfrac{1}{2\\sigma^{2}} \\big\\|\\mathbf{x}-\\boldsymbol{\\mu}\\big\\|^{2}\\Big)$。\n- **潜在变量：** $z_{nk} \\in \\{0,1\\}$，满足 $\\sum_{k=1}^{K} z_{nk} = 1$。\n- **任务：** 推导后验概率 $r_{nk} \\equiv p(z_{nk}=1 \\mid \\mathbf{x}_{n}, \\boldsymbol{\\Theta})$、成分均值 $\\boldsymbol{\\mu}_{k}$ 和成分方差 $\\sigma_{k}^{2}$ 的 EM 更新方程。\n\n**1.2. 使用提取的已知条件进行验证**\n\n- **科学依据：** 该问题描述了使用高斯混合模型进行尖峰排序，这是计算神经科学中一种标准且成熟的技术。EM 算法是拟合此类模型的经典方法。所有概念都是统计学和机器学习的基础。\n- **良态问题：** 该问题是统计学中一个标准的推导任务。给定模型和目标（通过 EM 实现最大似然），推导过程会得出一组唯一的更新方程。\n- **客观性：** 该问题使用精确的数学符号和术语进行陈述，没有主观或模糊的语言。\n- **完整性：** 该问题提供了执行推导所需的所有必要定义和约束。\n- **未发现其他缺陷。**\n\n**1.3. 结论与行动**\n\n问题有效。将提供完整的推导过程。\n\n### 第二步：EM 算法的推导\n\n期望最大化（EM）算法是一种迭代方法，用于为包含潜在变量的模型寻找最大似然估计。其目标是最大化观测数据 $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$ 的对数似然。\n\n对数似然由下式给出：\n$$\n\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\Theta}) = \\ln \\prod_{n=1}^{N} p(\\mathbf{x}_n \\mid \\boldsymbol{\\Theta}) = \\sum_{n=1}^{N} \\ln \\left( \\sum_{k=1}^{K} \\pi_{k} \\,\\mathcal{N}\\!\\big(\\mathbf{x}_{n} \\,\\big|\\, \\boldsymbol{\\mu}_{k}, \\sigma_{k}^{2}\\mathbf{I}\\big) \\right)\n$$\n由于对数函数内部存在求和，直接最大化该表达式很困难。EM 算法通过引入潜在变量 $z_{nk}$ 来解决这个问题，其中如果数据点 $\\mathbf{x}_n$ 由成分 $k$ 生成，则 $z_{nk}=1$，否则 $z_{nk}=0$。\n\n使用观测数据 $\\mathbf{X}$ 和潜在数据 $\\mathbf{Z}=\\{z_{nk}\\}$ 的完整数据对数似然为：\n$$\n\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} \\ln \\left[ \\pi_k \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\sigma_k^2 \\mathbf{I}) \\right]\n$$\n$$\n\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} \\left[ \\ln \\pi_k - \\frac{d}{2}\\ln(2\\pi) - \\frac{d}{2}\\ln(\\sigma_k^2) - \\frac{1}{2\\sigma_k^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k\\|^2 \\right]\n$$\nEM 算法在两个步骤之间迭代：期望（E）步骤和最大化（M）步骤。\n\n**2.1. 期望步骤（E-步）**\n\n在 E-步中，我们计算完整数据对数似然关于给定数据和当前参数估计 $\\boldsymbol{\\Theta}^{\\text{old}}$ 下潜在变量的后验分布的期望。这定义了函数 $\\mathcal{Q}(\\boldsymbol{\\Theta}, \\boldsymbol{\\Theta}^{\\text{old}})$。\n$$\n\\mathcal{Q}(\\boldsymbol{\\Theta}, \\boldsymbol{\\Theta}^{\\text{old}}) = \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\Theta}^{\\text{old}}}[\\ln p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta})]\n$$\n由于期望的线性性质，我们只需要潜在变量的期望 $\\mathbb{E}[z_{nk}]$。这个期望是数据点 $\\mathbf{x}_n$ 属于成分 $k$ 的后验概率，定义为后验概率（responsibility）$r_{nk}$。\n$$\nr_{nk} = \\mathbb{E}[z_{nk}] = p(z_{nk}=1 \\mid \\mathbf{x}_n, \\boldsymbol{\\Theta}^{\\text{old}})\n$$\n使用贝叶斯定理：\n$$\nr_{nk} = \\frac{p(z_{nk}=1 \\mid \\boldsymbol{\\Theta}^{\\text{old}}) \\, p(\\mathbf{x}_n \\mid z_{nk}=1, \\boldsymbol{\\Theta}^{\\text{old}})}{p(\\mathbf{x}_n \\mid \\boldsymbol{\\Theta}^{\\text{old}})} = \\frac{\\pi_k^{\\text{old}} \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k^{\\text{old}}, (\\sigma_k^2)^{\\text{old}} \\mathbf{I})}{\\sum_{j=1}^{K} \\pi_j^{\\text{old}} \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_j^{\\text{old}}, (\\sigma_j^2)^{\\text{old}} \\mathbf{I})}\n$$\n这是第一个所需的表达式。E-步涉及使用当前参数计算所有 $n \\in \\{1,\\dots,N\\}$ 和 $k \\in \\{1,\\dots,K\\}$ 的 $r_{nk}$ 值。\n\n**2.2. 最大化步骤（M-步）**\n\n在 M-步中，我们相对于参数 $\\boldsymbol{\\Theta} = \\{\\pi_k, \\boldsymbol{\\mu}_k, \\sigma_k^2\\}$ 最大化 $\\mathcal{Q}$ 函数，以获得更新后的参数 $\\boldsymbol{\\Theta}^{\\text{new}}$。后验概率 $r_{nk}$ 被视为固定常数。\n$$\n\\mathcal{Q}(\\boldsymbol{\\Theta}, \\boldsymbol{\\Theta}^{\\text{old}}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} \\left[ \\ln \\pi_k - \\frac{d}{2}\\ln(2\\pi\\sigma_k^2) - \\frac{1}{2\\sigma_k^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k\\|^2 \\right]\n$$\n\n**成分均值 $\\boldsymbol{\\mu}_k$ 的更新**\n\n我们通过将 $\\mathcal{Q}$ 对 $\\boldsymbol{\\mu}_k$ 的梯度设为零来找到最大值，仅考虑依赖于 $\\boldsymbol{\\mu}_k$ 的项。\n$$\n\\frac{\\partial \\mathcal{Q}}{\\partial \\boldsymbol{\\mu}_k} = \\frac{\\partial}{\\partial \\boldsymbol{\\mu}_k} \\sum_{n=1}^{N} \\sum_{j=1}^{K} r_{nj} \\left[ - \\frac{1}{2\\sigma_j^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_j\\|^2 \\right] = \\sum_{n=1}^{N} r_{nk} \\left[ - \\frac{1}{2\\sigma_k^2} \\frac{\\partial}{\\partial \\boldsymbol{\\mu}_k} (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^T(\\mathbf{x}_n - \\boldsymbol{\\mu}_k) \\right]\n$$\n$$\n\\frac{\\partial \\mathcal{Q}}{\\partial \\boldsymbol{\\mu}_k} = \\sum_{n=1}^{N} r_{nk} \\left[ - \\frac{1}{2\\sigma_k^2} (-2(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)) \\right] = \\frac{1}{\\sigma_k^2} \\sum_{n=1}^{N} r_{nk} (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)\n$$\n将梯度设为零：\n$$\n\\sum_{n=1}^{N} r_{nk} (\\mathbf{x}_n - \\boldsymbol{\\mu}_k) = \\mathbf{0} \\quad\\implies\\quad \\sum_{n=1}^{N} r_{nk} \\mathbf{x}_n = \\boldsymbol{\\mu}_k \\sum_{n=1}^{N} r_{nk}\n$$\n求解新的均值 $\\boldsymbol{\\mu}_k^{\\text{new}}$：\n$$\n\\boldsymbol{\\mu}_k^{\\text{new}} = \\frac{\\sum_{n=1}^{N} r_{nk} \\mathbf{x}_{n}}{\\sum_{n=1}^{N} r_{nk}}\n$$\n这是第二个所需的表达式。我们将簇 $k$ 中的有效点数定义为 $N_k = \\sum_{n=1}^{N} r_{nk}$。那么 $\\boldsymbol{\\mu}_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{n=1}^{N} r_{nk} \\mathbf{x}_{n}$。\n\n**成分方差 $\\sigma_k^2$ 的更新**\n\n我们通过将 $\\mathcal{Q}$ 对 $\\sigma_k^2$ 的导数设为零来找到最大值。请注意，在此步骤中我们必须使用新计算出的均值 $\\boldsymbol{\\mu}_k^{\\text{new}}$。\n$$\n\\frac{\\partial \\mathcal{Q}}{\\partial (\\sigma_k^2)} = \\frac{\\partial}{\\partial (\\sigma_k^2)} \\sum_{n=1}^{N} r_{nk} \\left[ -\\frac{d}{2}\\ln(\\sigma_k^2) - \\frac{1}{2\\sigma_k^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2 \\right]\n$$\n$$\n\\frac{\\partial \\mathcal{Q}}{\\partial (\\sigma_k^2)} = \\sum_{n=1}^{N} r_{nk} \\left[ -\\frac{d}{2\\sigma_k^2} + \\frac{1}{2(\\sigma_k^2)^2} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2 \\right]\n$$\n将导数设为零：\n$$\n\\sum_{n=1}^{N} r_{nk} \\left( -\\frac{d}{2\\sigma_k^2} + \\frac{\\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2}{2(\\sigma_k^2)^2} \\right) = 0\n$$\n乘以 $2(\\sigma_k^2)^2$ 得：\n$$\n\\sum_{n=1}^{N} r_{nk} \\left( -d \\sigma_k^2 + \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2 \\right) = 0\n$$\n$$\nd \\sigma_k^2 \\sum_{n=1}^{N} r_{nk} = \\sum_{n=1}^{N} r_{nk} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2\n$$\n求解新的方差 $(\\sigma_k^2)^{\\text{new}}$：\n$$\n(\\sigma_k^2)^{\\text{new}} = \\frac{\\sum_{n=1}^{N} r_{nk} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k^{\\text{new}}\\|^2}{d \\sum_{n=1}^{N} r_{nk}}\n$$\n这是第三个所需的表达式。因子 $d$ 源于各向同性高斯分布的维度。\n\n### 更新方程总结\n在 EM 算法的每次迭代中，我们首先使用当前参数计算后验概率 $r_{nk}$（E-步），然后使用这些后验概率重新估计模型参数（M-步）。为方便表示，省略了上标“old”和“new”，但应理解计算是按顺序进行的。\n\n**E-步：**\n$$ r_{nk} = \\frac{\\pi_{k} \\,\\mathcal{N}(\\mathbf{x}_{n} \\mid \\boldsymbol{\\mu}_{k}, \\sigma_{k}^{2}\\mathbf{I})}{\\sum_{j=1}^{K} \\pi_{j} \\,\\mathcal{N}(\\mathbf{x}_{n} \\mid \\boldsymbol{\\mu}_{j}, \\sigma_{j}^{2}\\mathbf{I})} $$\n**M-步：**\n$$ \\boldsymbol{\\mu}_{k} = \\frac{\\sum_{n=1}^{N} r_{nk} \\mathbf{x}_{n}}{\\sum_{n=1}^{N} r_{nk}} $$\n$$ \\sigma_{k}^{2} = \\frac{\\sum_{n=1}^{N} r_{nk} \\|\\mathbf{x}_{n} - \\boldsymbol{\\mu}_{k}\\|^{2}}{d \\sum_{n=1}^{N} r_{nk}} $$\n（注意：$\\pi_k$ 的更新公式为 $\\pi_k = \\frac{1}{N} \\sum_{n=1}^N r_{nk}$，但这并非题目要求。）\n这三个表达式构成了该模型 EM 算法的核心。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nr_{nk} \\\\\n\\boldsymbol{\\mu}_{k} \\\\\n\\sigma_{k}^{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{\\pi_{k} (\\sigma_{k}^{2})^{-\\frac{d}{2}} \\exp(-\\frac{1}{2\\sigma_{k}^{2}} \\|\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{k}\\|^{2})}{\\sum_{j=1}^{K} \\pi_{j} (\\sigma_{j}^{2})^{-\\frac{d}{2}} \\exp(-\\frac{1}{2\\sigma_{j}^{2}} \\|\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{j}\\|^{2})} \\\\\n\\frac{\\sum_{n=1}^{N} r_{nk} \\mathbf{x}_{n}}{\\sum_{n=1}^{N} r_{nk}} \\\\\n\\frac{\\sum_{n=1}^{N} r_{nk} \\|\\mathbf{x}_{n} - \\boldsymbol{\\mu}_{k}\\|^{2}}{d \\sum_{n=1}^{N} r_{nk}}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}