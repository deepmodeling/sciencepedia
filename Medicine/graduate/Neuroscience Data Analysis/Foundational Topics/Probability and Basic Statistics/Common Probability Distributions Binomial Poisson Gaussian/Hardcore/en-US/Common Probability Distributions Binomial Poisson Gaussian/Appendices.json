{
    "hands_on_practices": [
        {
            "introduction": "Estimating a neuron's firing rate is a fundamental task in neuroscience. This exercise explores how the precision of this estimate is related to the duration of the observation. By deriving the Fisher information for a Poisson process from first principles, you will build a crucial theoretical link between experimental design (choosing an observation time $T$) and statistical inference, revealing how longer recordings provide more information for a more precise estimate .",
            "id": "4146747",
            "problem": "In a spike-counting experiment in systems neuroscience, a single neuron is modeled as emitting spikes according to a homogeneous Poisson process with unknown constant rate parameter $\\lambda$ (spikes per second). You observe the total spike count $K$ over a single continuous observation window of duration $T$ (seconds). Assume that, conditional on $\\lambda$, the count $K$ is a Poisson random variable with mean $\\lambda T$. Use only fundamental definitions and well-tested facts to derive the Fisher information about $\\lambda$ contained in $K$.\n\nStart from the following foundational elements: the probability mass function of a Poisson random variable, the definition of the likelihood and log-likelihood for observed data, the definition of Fisher information as the expectation of the negative second derivative of the log-likelihood with respect to the parameter, and standard moment identities for the Poisson distribution that can be justified from its definition. Do not assume or invoke any specialized result about Fisher information for the Poisson distribution without derivation.\n\nAdditionally, suppose the observation window is partitioned into $N$ independent equal-duration bins, each of length $\\Delta$ such that $T = N \\Delta$, and let $K_{1},\\dots,K_{N}$ denote the spike counts in the bins. Under the homogeneous Poisson process model, justify how information additivity across independent observations recovers the same expression you derived for the single-window case, and interpret how Fisher information scales with the total observation time $T$ for spike rate estimation.\n\nProvide your final answer as a single closed-form analytic expression in terms of $\\lambda$ and $T$ only. Do not include units in your final answer. No numerical rounding is required for this problem.",
            "solution": "The problem requires the derivation of the Fisher information about the rate parameter $\\lambda$ of a homogeneous Poisson process, given a spike count $K$ over an observation duration $T$. The derivation must proceed from first principles.\n\nFirst, we establish the statistical model. The spike count $K$ in a time interval of duration $T$ is a random variable following a Poisson distribution with mean $\\mu = \\lambda T$. The probability mass function (PMF) for observing $k$ spikes is given by:\n$$P(K=k | \\lambda) = \\frac{(\\lambda T)^k \\exp(-\\lambda T)}{k!}$$\nfor $k \\in \\{0, 1, 2, \\dots\\}$.\n\nFor an observed count $K$, the likelihood function $L(\\lambda|K)$ is defined as the PMF viewed as a function of the parameter $\\lambda$:\n$$L(\\lambda|K) = P(K=K | \\lambda) = \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!}$$\nThe log-likelihood function, $\\ell(\\lambda|K) = \\ln L(\\lambda|K)$, is often more convenient for differentiation:\n$$\\ell(\\lambda|K) = \\ln\\left( \\frac{(\\lambda T)^K \\exp(-\\lambda T)}{K!} \\right)$$\nUsing the properties of the logarithm, this simplifies to:\n$$\\ell(\\lambda|K) = K \\ln(\\lambda T) - \\lambda T - \\ln(K!)$$\n$$\\ell(\\lambda|K) = K(\\ln \\lambda + \\ln T) - \\lambda T - \\ln(K!)$$\n\nThe Fisher information, $I(\\lambda)$, is defined as the expectation of the negative second derivative of the log-likelihood function with respect to the parameter $\\lambda$. We first compute the derivatives. The first derivative, known as the score function, is:\n$$\\frac{\\partial \\ell}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left[ K\\ln \\lambda + K\\ln T - \\lambda T - \\ln(K!) \\right]$$\nTreating $K$ and $T$ as constants with respect to the differentiation by $\\lambda$:\n$$\\frac{\\partial \\ell}{\\partial \\lambda} = \\frac{K}{\\lambda} - T$$\n\nNext, we compute the second derivative:\n$$\\frac{\\partial^2 \\ell}{\\partial \\lambda^2} = \\frac{\\partial}{\\partial \\lambda} \\left( \\frac{K}{\\lambda} - T \\right) = -\\frac{K}{\\lambda^2}$$\n\nThe Fisher information $I(\\lambda)$ is then calculated by taking the expectation of the negative of this second derivative. The expectation $E[\\cdot]$ is taken with respect to the distribution of the data, which in this case is the random variable $K \\sim \\text{Poisson}(\\lambda T)$.\n$$I(\\lambda) = E\\left[ - \\frac{\\partial^2 \\ell}{\\partial \\lambda^2} \\right] = E\\left[ - \\left( -\\frac{K}{\\lambda^2} \\right) \\right] = E\\left[ \\frac{K}{\\lambda^2} \\right]$$\nSince $\\lambda$ is a constant with respect to the expectation over $K$, we can write:\n$$I(\\lambda) = \\frac{1}{\\lambda^2} E[K]$$\nThe expectation of a Poisson random variable with mean $\\lambda T$ is precisely its mean. Thus, $E[K] = \\lambda T$. Substituting this into the equation for $I(\\lambda)$:\n$$I(\\lambda) = \\frac{1}{\\lambda^2} (\\lambda T) = \\frac{T}{\\lambda}$$\nThis is the Fisher information about $\\lambda$ contained in the single observation $K$.\n\nNow, we address the second part of the problem. The observation window of total duration $T$ is partitioned into $N$ independent, equal-duration bins, each of length $\\Delta = T/N$. Let $K_1, \\dots, K_N$ be the spike counts in these respective bins. For a homogeneous Poisson process, the spike counts in disjoint time intervals are independent random variables. The count $K_i$ in each bin $i$ follows a Poisson distribution with mean $\\lambda \\Delta$.\n\nThe total likelihood for the set of observations $\\{K_1, \\dots, K_N\\}$ is the product of the individual likelihoods due to their independence:\n$$L_{total}(\\lambda | K_1, \\dots, K_N) = \\prod_{i=1}^N L_i(\\lambda | K_i)$$\nConsequently, the total log-likelihood is the sum of the individual log-likelihoods:\n$$\\ell_{total}(\\lambda | K_1, \\dots, K_N) = \\ln\\left( \\prod_{i=1}^N L_i(\\lambda | K_i) \\right) = \\sum_{i=1}^N \\ln(L_i(\\lambda | K_i)) = \\sum_{i=1}^N \\ell_i(\\lambda | K_i)$$\nThe Fisher information for this set of independent observations is derived from the total log-likelihood. Due to the linearity of differentiation and expectation, the total Fisher information is the sum of the Fisher information from each independent observation:\n$$I_{total}(\\lambda) = E\\left[ -\\frac{\\partial^2 \\ell_{total}}{\\partial \\lambda^2} \\right] = E\\left[ -\\sum_{i=1}^N \\frac{\\partial^2 \\ell_i}{\\partial \\lambda^2} \\right] = \\sum_{i=1}^N E\\left[ -\\frac{\\partial^2 \\ell_i}{\\partial \\lambda^2} \\right] = \\sum_{i=1}^N I_i(\\lambda)$$\nEach $I_i(\\lambda)$ is the Fisher information from a single observation $K_i$ over a duration $\\Delta$. We can use our previous result, replacing $T$ with $\\Delta$:\n$$I_i(\\lambda) = \\frac{\\Delta}{\\lambda}$$\nThe total information is the sum over all $N$ bins:\n$$I_{total}(\\lambda) = \\sum_{i=1}^N \\frac{\\Delta}{\\lambda} = N \\frac{\\Delta}{\\lambda}$$\nSince the total duration is $T = N \\Delta$, we can substitute this back:\n$$I_{total}(\\lambda) = \\frac{N\\Delta}{\\lambda} = \\frac{T}{\\lambda}$$\nThis result confirms the principle of information additivity for independent observations and recovers the same expression derived for the single, unpartitioned observation window.\n\nThe interpretation of the result $I(\\lambda) = T/\\lambda$ is that the amount of information the data provides about the rate parameter $\\lambda$ scales linearly with the total observation time $T$. This is intuitive: a longer observation period provides more data, which in turn allows for a more precise estimation of the underlying rate. The Cramér-Rao Lower Bound, which states that the variance of any unbiased estimator $\\hat{\\lambda}$ is bounded by $\\text{Var}(\\hat{\\lambda}) \\ge 1/I(\\lambda)$, implies $\\text{Var}(\\hat{\\lambda}) \\ge \\lambda/T$. This shows that the minimum achievable estimation variance decreases inversely with observation time, meaning the precision of the rate estimate improves as the square root of the observation time.",
            "answer": "$$\\boxed{\\frac{T}{\\lambda}}$$"
        },
        {
            "introduction": "Real-world data collection is often imperfect; for example, a spike detection system might only store data when a spike is present, discarding silent periods. This practice addresses the statistical consequences of such data truncation. You will learn to correctly model this scenario by deriving the likelihood for a zero-truncated Poisson distribution, a critical skill for robustly analyzing data where the observation process itself introduces a selection bias .",
            "id": "4146717",
            "problem": "A common practice in extracellular neural spike detection is to discretize time into bins of width $\\,\\Delta t\\,$ and assume that, under stationarity over the analyzed epoch, the spike count $\\,Y\\,$ in each bin follows a Poisson distribution with parameter $\\,\\lambda>0\\,$. In many acquisition pipelines, only bins that contain at least one detected spike are stored for downstream analysis, while bins with zero spikes are discarded (for example, due to threshold-triggered storage that ignores quiescent periods). You observe $\\,n\\,$ independent counts $\\,y_{1},\\dots,y_{n}\\,$, each strictly positive, $\\,y_{i}\\in\\{1,2,\\dots\\}\\,$, generated by this process.\n\nStarting only from the definition of conditional probability $\\,\\Pr(A\\mid B)=\\Pr(A\\cap B)/\\Pr(B)\\,$ and the Poisson probability mass function $\\,\\Pr(Y=y)=\\exp(-\\lambda)\\lambda^{y}/y!\\,$ for integer $\\,y\\ge 0\\,$, derive the exact likelihood $\\,L(\\lambda; y_{1},\\dots,y_{n})\\,$ for the observed data under the model that each recorded count is distributed as a Poisson random variable conditioned on being strictly positive. Clearly express the final likelihood in closed form, simplified in terms of $\\,n\\,$, $\\,\\sum_{i=1}^{n} y_{i}\\,$, and $\\,\\prod_{i=1}^{n} y_{i}!\\,$, and assume independence across bins.\n\nBriefly explain, in your derivation, why excluding zero-count bins in spike detection implies conditioning on the event $\\,Y>0\\,$ and thus requires a zero-truncated model. Your final reported answer must be the single analytic expression for $\\,L(\\lambda; y_{1},\\dots,y_{n})\\,$. No numerical evaluation is required, and no rounding is needed. The likelihood is dimensionless; do not attach units to the final answer.",
            "solution": "The problem requires the derivation of the likelihood function for a set of $n$ independent spike counts, $\\{y_{1}, \\dots, y_{n}\\}$, where each count is assumed to be drawn from a Poisson distribution that is conditioned on being strictly positive. This conditioning arises from the data acquisition procedure, which discards time bins with zero spikes.\n\nFirst, we must formalize why this experimental procedure necessitates a conditional probability model. The underlying physical process is assumed to generate spike counts $Y$ in any given bin according to a Poisson distribution with parameter $\\lambda > 0$. The sample space for this process is the set of non-negative integers, $\\{0, 1, 2, \\dots\\}$. However, the measurement or recording process is selective; it only stores a count $y_i$ if $y_i \\ge 1$. This means that any observed count $y_i$ is a realization of the random variable $Y$ given that the event $\\{Y>0\\}$ has occurred. The original sample space has been truncated to the set of positive integers $\\{1, 2, 3, \\dots\\}$. Therefore, to correctly model the probability of the observed data, we cannot use the standard Poisson probability mass function (PMF) directly. Instead, we must use the PMF of a Poisson distribution conditioned on the outcome being greater than zero. This is known as a zero-truncated Poisson distribution.\n\nLet $Y$ be a random variable following a Poisson distribution with parameter $\\lambda$, so its PMF is $\\Pr(Y=y) = \\frac{\\exp(-\\lambda)\\lambda^{y}}{y!}$ for $y \\in \\{0, 1, 2, \\dots\\}$. We need to find the PMF of an observed count, which we denote $y_i$, given that $y_i > 0$. Using the definition of conditional probability, $\\Pr(A \\mid B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}$, we can define the probability of observing a specific count $y_i \\in \\{1, 2, \\dots\\}$. Let the event $A$ be $\\{Y=y_i\\}$ and the event $B$ be $\\{Y>0\\}$. The conditional probability is:\n$$\n\\Pr(Y=y_i \\mid Y>0) = \\frac{\\Pr(\\{Y=y_i\\} \\cap \\{Y>0\\})}{\\Pr(Y>0)}\n$$\nSince each observed count $y_i$ is strictly positive ($y_i \\ge 1$), the event $\\{Y=y_i\\}$ is a subset of the event $\\{Y>0\\}$. Therefore, their intersection is simply the event $\\{Y=y_i\\}$ itself: $\\{Y=y_i\\} \\cap \\{Y>0\\} = \\{Y=y_i\\}$. The expression simplifies to:\n$$\n\\Pr(Y=y_i \\mid Y>0) = \\frac{\\Pr(Y=y_i)}{\\Pr(Y>0)}\n$$\nThe denominator, $\\Pr(Y>0)$, is the probability of the conditioning event. It can be calculated as $1$ minus the probability of the complementary event, $\\{Y=0\\}$:\n$$\n\\Pr(Y>0) = 1 - \\Pr(Y=0)\n$$\nFor a Poisson distribution, the probability of zero counts is:\n$$\n\\Pr(Y=0) = \\frac{\\exp(-\\lambda)\\lambda^{0}}{0!} = \\frac{\\exp(-\\lambda) \\cdot 1}{1} = \\exp(-\\lambda)\n$$\nSubstituting this back, we find the probability of the conditioning event:\n$$\n\\Pr(Y>0) = 1 - \\exp(-\\lambda)\n$$\nNow we can write the complete PMF for a single observed count $y_i$ from the zero-truncated distribution:\n$$\n\\Pr(Y=y_i \\mid Y>0) = \\frac{\\frac{\\exp(-\\lambda)\\lambda^{y_i}}{y_i!}}{1 - \\exp(-\\lambda)}\n$$\nThis is the probability of a single observation $y_i$. The likelihood function, $L(\\lambda; y_{1}, \\dots, y_{n})$, is the probability of observing the entire dataset given the parameter $\\lambda$. Since the observations $y_1, \\dots, y_n$ are stated to be independent, the joint probability (and thus the likelihood) is the product of the individual probabilities for each observation:\n$$\nL(\\lambda; y_{1}, \\dots, y_{n}) = \\prod_{i=1}^{n} \\Pr(Y=y_i \\mid Y>0)\n$$\nSubstituting the PMF we derived for each term in the product:\n$$\nL(\\lambda; y_{1}, \\dots, y_{n}) = \\prod_{i=1}^{n} \\left( \\frac{\\frac{\\exp(-\\lambda)\\lambda^{y_i}}{y_i!}}{1 - \\exp(-\\lambda)} \\right)\n$$\nTo simplify this expression, we can separate the components of the product. The denominator $(1 - \\exp(-\\lambda))$ is constant with respect to the index $i$, so it is multiplied $n$ times. The numerator terms can be grouped:\n$$\nL(\\lambda; y_{1}, \\dots, y_{n}) = \\frac{1}{(1 - \\exp(-\\lambda))^n} \\prod_{i=1}^{n} \\left( \\frac{\\exp(-\\lambda)\\lambda^{y_i}}{y_i!} \\right)\n$$\nLet's expand the product term:\n$$\n\\prod_{i=1}^{n} \\left( \\frac{\\exp(-\\lambda)\\lambda^{y_i}}{y_i!} \\right) = \\left( \\prod_{i=1}^{n} \\exp(-\\lambda) \\right) \\left( \\prod_{i=1}^{n} \\lambda^{y_i} \\right) \\left( \\prod_{i=1}^{n} \\frac{1}{y_i!} \\right)\n$$\nWe can simplify each of these three products:\n1.  $\\prod_{i=1}^{n} \\exp(-\\lambda) = (\\exp(-\\lambda))^n = \\exp(-n\\lambda)$\n2.  $\\prod_{i=1}^{n} \\lambda^{y_i} = \\lambda^{\\sum_{i=1}^{n} y_i}$\n3.  $\\prod_{i=1}^{n} \\frac{1}{y_i!} = \\frac{1}{\\prod_{i=1}^{n} y_i!}$\n\nCombining these simplified parts, the product becomes:\n$$\n\\exp(-n\\lambda) \\lambda^{\\sum_{i=1}^{n} y_i} \\frac{1}{\\prod_{i=1}^{n} y_i!}\n$$\nFinally, we substitute this back into the expression for the likelihood $L(\\lambda; y_{1}, \\dots, y_{n})$:\n$$\nL(\\lambda; y_{1}, \\dots, y_{n}) = \\frac{1}{(1 - \\exp(-\\lambda))^n} \\cdot \\frac{\\exp(-n\\lambda) \\lambda^{\\sum_{i=1}^{n} y_i}}{\\prod_{i=1}^{n} y_i!}\n$$\nRearranging to a single fraction gives the final closed-form expression for the likelihood, expressed in terms of $n$, $\\sum_{i=1}^{n} y_{i}$, and $\\prod_{i=1}^{n} y_{i}!$:\n$$\nL(\\lambda; y_{1}, \\dots, y_{n}) = \\frac{\\exp(-n\\lambda) \\lambda^{\\sum_{i=1}^{n} y_i}}{ (1 - \\exp(-\\lambda))^n \\left(\\prod_{i=1}^{n} y_i!\\right) }\n$$",
            "answer": "$$\n\\boxed{\n\\frac{\\exp(-n\\lambda) \\lambda^{\\sum_{i=1}^{n} y_i}}{(1 - \\exp(-\\lambda))^{n} \\left(\\prod_{i=1}^{n} y_i!\\right)}\n}\n$$"
        },
        {
            "introduction": "While the Poisson model assumes a constant firing rate, the activity of real neurons often fluctuates due to factors like attention or learning, leading to \"overdispersion\" where count variance exceeds the mean. This comprehensive practice guides you through building a more realistic hierarchical model. By treating the Poisson rate parameter $\\lambda$ as a random variable drawn from a Gamma distribution, you will derive the Negative Binomial distribution and implement a simulation to confirm your theoretical results, gaining a deep understanding of how to model and interpret neural variability .",
            "id": "4146731",
            "problem": "A neuronal spike count in a short analysis bin can be modeled as a draw from a Poisson distribution with an instantaneous rate. In realistic scenarios, the rate fluctuates across repeated trials due to latent modulators. One principled way to model this fluctuation is to assume that the latent rate itself is random. Consider a hierarchical model in which the latent rate $\\lambda$ is random across trials and the observed count $N$ conditional on $\\lambda$ is Poisson.\n\nYou are asked to demonstrate, from first principles, how this hierarchical model leads to a negative binomial distribution for $N$, and then to implement a simulator that uses this mixture construction to generate synthetic spike counts. The simulator should then be validated against the theoretical mean and variance implied by the model for several test cases that probe different regimes of dispersion, including a regime approaching the Poisson limit and a regime with strong overdispersion, which are scientifically relevant for neuronal variability analysis.\n\nUse the following foundational base only: the probability mass function of a Poisson random variable, the probability density function of a Gamma random variable, properties of the Gamma function, the law of total probability, and the law of total expectation and variance. Do not use any target formulas for the negative binomial distribution in your derivations; instead, derive the required expressions from these foundational elements.\n\nTasks to be completed:\n\n- Start from the conditional model $N \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$ and the prior $\\lambda \\sim \\mathrm{Gamma}(k,\\beta)$ where $k>0$ is the shape and $\\beta>0$ is the rate. Using only the law of total probability and the definitions of the Poisson probability mass function and Gamma probability density function, derive the marginal distribution of $N$ and show that it is a negative binomial distribution parameterized by $k$ and a success probability $p$ expressed in terms of $\\beta$. Then, using only the law of total expectation and the law of total variance, derive expressions for $\\mathbb{E}[N]$ and $\\mathrm{Var}(N)$ in terms of $k$ and $\\beta$.\n\n- Interpret the mixture sampling procedure as sampling latent rate fluctuations: explain how drawing $\\lambda$ from a Gamma distribution models across-trial rate variability and how the subsequent Poisson sampling models within-trial counting noise.\n\n- Design and implement an algorithm to simulate $T$ independent draws of $N$ under this hierarchical model by first sampling $\\lambda$ and then sampling $N \\mid \\lambda$. The implementation must accept the tuple $(k,\\beta,T,\\mathrm{seed})$ and return the empirical mean and variance of the simulated counts along with their theoretical counterparts and a pass/fail boolean based on relative error tolerances.\n\n- Validation and output specification. For each test case, simulate counts with a fixed pseudo-random seed and compute:\n  - the empirical mean $\\hat{m}$,\n  - the empirical variance $\\hat{v}$,\n  - the theoretical mean $m_\\mathrm{th}$,\n  - the theoretical variance $v_\\mathrm{th}$,\n  - the empirical variance-to-mean ratio $r_\\mathrm{emp}=\\hat{v}/\\hat{m}$,\n  - the theoretical variance-to-mean ratio $r_\\mathrm{th}=v_\\mathrm{th}/m_\\mathrm{th}$.\n  Use relative error tolerances of $\\tau_m=0.02$ for the mean and $\\tau_v=0.05$ for the variance. A test case passes if both $\\lvert \\hat{m}-m_\\mathrm{th}\\rvert/m_\\mathrm{th}\\le \\tau_m$ and $\\lvert \\hat{v}-v_\\mathrm{th}\\rvert/v_\\mathrm{th}\\le \\tau_v$.\n\nTest suite to be implemented and evaluated:\n\n- Case A (general overdispersion): $k=10.0$, $\\beta=2.0$, $T=120000$, $\\mathrm{seed}=12345$.\n- Case B (near-Poisson limit via small latent variance): $k=1000.0$, $\\beta=200.0$, $T=200000$, $\\mathrm{seed}=24680$.\n- Case C (strong overdispersion, heavy-tailed rates): $k=0.5$, $\\beta=0.5$, $T=150000$, $\\mathrm{seed}=13579$.\n\nFinal output format requirement:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponding to a test case must itself be a list of the form $[\\hat{m},\\hat{v},m_\\mathrm{th},v_\\mathrm{th},r_\\mathrm{emp},r_\\mathrm{th},\\mathrm{pass}]$ where the entries are in that exact order and types are floats for the numerical quantities and a boolean for the pass indicator. The final output should therefore look like a single list of three inner lists, one per test case, for example $[[\\cdots],[\\cdots],[\\cdots]]$ with no additional text. No physical units or angles are involved in this task, and all quantities are dimensionless.",
            "solution": "The problem requires a demonstration from first principles that a Gamma-Poisson mixture model for neuronal spike counts results in a Negative Binomial distribution. This involves deriving the marginal probability mass function (PMF) of the spike count $N$, as well as its theoretical mean and variance. Further, a simulation must be implemented to validate these theoretical results. The entire process is grounded in fundamental principles of probability theory.\n\nFirst, we provide a conceptual interpretation of the hierarchical model. The model is specified in two stages:\n$1$. The latent firing rate $\\lambda$ is drawn from a Gamma distribution: $\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$, where $k>0$ is the shape parameter and $\\beta>0$ is the rate parameter. This stage models the slow, trial-to-trial fluctuations in neuronal excitability. In experimental settings, factors like attention, motivation, or slow neuromodulatory changes can cause the average firing rate of a neuron to vary across repeated trials. The Gamma distribution is a flexible and mathematically convenient choice for modeling this non-negative, continuous rate variable. The variance of this distribution, $\\mathrm{Var}(\\lambda) = k/\\beta^2$, quantifies the magnitude of this across-trial rate variability.\n$2$. Conditional on a specific rate $\\lambda$ for a given trial, the observed spike count $N$ is drawn from a Poisson distribution: $N \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$. This stage models the inherent stochasticity of spike generation. For a fixed underlying neuronal state (i.e., a fixed $\\lambda$), the process of emitting action potentials is a random, point-like process in time, which is well-approximated by a Poisson process. The variance of this process is equal to its mean, $\\mathrm{Var}(N \\mid \\lambda) = \\lambda$. This is often termed \"counting noise\" or \"within-trial\" variability.\n\nBy combining these two sources of randomness, the model captures a key feature of real neuronal data: overdispersion, where the total variance of spike counts across trials is greater than the mean count.\n\nNext, we derive the marginal distribution of $N$. The problem states the conditional distribution of the count $N$ given the rate $\\lambda$ is Poisson, and the prior distribution of the rate $\\lambda$ is Gamma. The respective probability functions are:\n- Poisson PMF: $P(N=n \\mid \\lambda) = \\frac{\\lambda^n e^{-\\lambda}}{n!}$ for $n \\in \\{0, 1, 2, \\dots\\}$.\n- Gamma PDF: $p(\\lambda \\mid k, \\beta) = \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} e^{-\\beta\\lambda}$ for $\\lambda > 0$.\n\nTo find the marginal PMF of $N$, we apply the law of total probability, integrating over all possible values of the latent variable $\\lambda$:\n$$P(N=n) = \\int_{0}^{\\infty} P(N=n \\mid \\lambda) \\, p(\\lambda \\mid k, \\beta) \\, d\\lambda$$\nSubstituting the specific forms of the Poisson PMF and Gamma PDF:\n$$P(N=n) = \\int_{0}^{\\infty} \\left( \\frac{\\lambda^n e^{-\\lambda}}{n!} \\right) \\left( \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} e^{-\\beta\\lambda} \\right) d\\lambda$$\nWe can group the terms that do not depend on $\\lambda$ outside the integral:\n$$P(N=n) = \\frac{\\beta^k}{n! \\Gamma(k)} \\int_{0}^{\\infty} \\lambda^{n+k-1} e^{-(\\beta+1)\\lambda} d\\lambda$$\nThe integral has the form of the kernel of a Gamma distribution. Recall that a Gamma PDF with shape $\\alpha$ and rate $\\delta$ integrates to $1$:\n$$\\int_{0}^{\\infty} \\frac{\\delta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\delta x} dx = 1 \\implies \\int_{0}^{\\infty} x^{\\alpha-1} e^{-\\delta x} dx = \\frac{\\Gamma(\\alpha)}{\\delta^\\alpha}$$\nIn our integral, we can identify the shape as $\\alpha = n+k$ and the rate as $\\delta = \\beta+1$. Applying this formula, the integral evaluates to:\n$$\\int_{0}^{\\infty} \\lambda^{(n+k)-1} e^{-(\\beta+1)\\lambda} d\\lambda = \\frac{\\Gamma(n+k)}{(\\beta+1)^{n+k}}$$\nSubstituting this result back into the expression for $P(N=n)$:\n$$P(N=n) = \\frac{\\beta^k}{n! \\Gamma(k)} \\frac{\\Gamma(n+k)}{(\\beta+1)^{n+k}}$$\nTo show this is a Negative Binomial distribution, we rearrange the terms. The binomial coefficient $\\binom{a}{b}$ can be expressed using Gamma functions as $\\frac{\\Gamma(a+1)}{b! \\Gamma(a-b+1)}$. A more general form is $\\binom{n+r-1}{n} = \\frac{\\Gamma(n+r)}{n! \\Gamma(r)}$. Using this identity with $r=k$:\n$$P(N=n) = \\frac{\\Gamma(n+k)}{n! \\Gamma(k)} \\frac{\\beta^k}{(\\beta+1)^k (\\beta+1)^n} = \\binom{n+k-1}{n} \\left(\\frac{\\beta}{\\beta+1}\\right)^k \\left(\\frac{1}{\\beta+1}\\right)^n$$\nThis is the PMF of a Negative Binomial distribution with parameters $r=k$ and a success probability $p = \\frac{\\beta}{\\beta+1}$. The number of failures before $r$ successes is $n$. Thus, we have shown that $N \\sim \\mathrm{NB}(k, p = \\frac{\\beta}{\\beta+1})$.\n\nNext, we derive the theoretical mean ($m_\\mathrm{th}$) and variance ($v_\\mathrm{th}$) of $N$ using the laws of total expectation and variance, without relying on the known formulas for the Negative Binomial distribution.\nThe law of total expectation states: $\\mathbb{E}[N] = \\mathbb{E}[\\mathbb{E}[N \\mid \\lambda]]$.\n- The inner expectation is the mean of a Poisson distribution with parameter $\\lambda$: $\\mathbb{E}[N \\mid \\lambda] = \\lambda$.\n- The outer expectation is the expectation of $\\lambda$, which follows a $\\mathrm{Gamma}(k, \\beta)$ distribution. The mean of a Gamma distribution is shape/rate. So, $\\mathbb{E}[\\lambda] = k/\\beta$.\n- Therefore, the theoretical mean of $N$ is:\n$$m_\\mathrm{th} = \\mathbb{E}[N] = \\mathbb{E}[\\lambda] = \\frac{k}{\\beta}$$\n\nThe law of total variance states: $\\mathrm{Var}(N) = \\mathbb{E}[\\mathrm{Var}(N \\mid \\lambda)] + \\mathrm{Var}(\\mathbb{E}[N \\mid \\lambda])$.\n- For the first term, $\\mathbb{E}[\\mathrm{Var}(N \\mid \\lambda)]$: The variance of a Poisson distribution is equal to its mean, so $\\mathrm{Var}(N \\mid \\lambda) = \\lambda$. The expectation is therefore $\\mathbb{E}[\\lambda] = k/\\beta$.\n- For the second term, $\\mathrm{Var}(\\mathbb{E}[N \\mid \\lambda])$: We already established that $\\mathbb{E}[N \\mid \\lambda] = \\lambda$. This term is thus $\\mathrm{Var}(\\lambda)$. The variance of a $\\mathrm{Gamma}(k, \\beta)$ distribution is shape/rate$^2$, so $\\mathrm{Var}(\\lambda) = k/\\beta^2$.\n- Summing the two terms gives the theoretical variance of $N$:\n$$v_\\mathrm{th} = \\mathrm{Var}(N) = \\frac{k}{\\beta} + \\frac{k}{\\beta^2}$$\nThis confirms that the variance is greater than the mean, as $k/\\beta^2 > 0$ for the given constraints $k>0$ and $\\beta>0$.\n\nFinally, the theoretical variance-to-mean ratio is:\n$$r_\\mathrm{th} = \\frac{v_\\mathrm{th}}{m_\\mathrm{th}} = \\frac{k/\\beta + k/\\beta^2}{k/\\beta} = 1 + \\frac{1}{\\beta}$$\nThis ratio, also known as the Fano factor, is a measure of dispersion. A value of $1$ signifies a Poisson process, while a value greater than $1$ signifies overdispersion. The degree of overdispersion in this model is controlled solely by the Gamma rate parameter $\\beta$. As $\\beta \\to \\infty$, $r_\\mathrm{th} \\to 1$, and the model approaches the Poisson limit. This occurs because $\\mathrm{Var}(\\lambda) = k/\\beta^2 \\to 0$, meaning the latent rate $\\lambda$ becomes non-random.\n\nThe implementation will simulate this two-stage process, calculate the empirical statistics ($\\hat{m}$, $\\hat{v}$), compare them against the theoretical values ($m_\\mathrm{th}$, $v_\\mathrm{th}$) using relative error tolerances of $\\tau_m=0.02$ and $\\tau_v=0.05$, and report a pass/fail status for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation_and_validation(k, beta, T, seed):\n    \"\"\"\n    Simulates spike counts from a Gamma-Poisson mixture model and validates\n    empirical statistics against theoretical predictions.\n\n    Args:\n        k (float): Shape parameter of the Gamma distribution for the rate λ.\n        beta (float): Rate parameter of the Gamma distribution for the rate λ.\n        T (int): Number of independent trials to simulate.\n        seed (int): Seed for the pseudo-random number generator.\n\n    Returns:\n        list: A list containing the following values in order:\n              [empirical mean, empirical variance, theoretical mean,\n               theoretical variance, empirical variance-to-mean ratio,\n               theoretical variance-to-mean ratio, pass/fail boolean].\n    \"\"\"\n    # Set up the random number generator with a specific seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # --- Simulation Step ---\n    # Step 1: Sample T latent rates from the Gamma distribution.\n    # numpy.random.Generator.gamma uses a scale parameter, which is 1/rate.\n    scale = 1.0 / beta\n    lambda_samples = rng.gamma(shape=k, scale=scale, size=T)\n\n    # Step 2: For each sampled rate, sample a spike count from the Poisson distribution.\n    n_samples = rng.poisson(lam=lambda_samples)\n\n    # --- Analysis and Validation Step ---\n    # Calculate empirical statistics from the simulated counts\n    m_hat = np.mean(n_samples)\n    # Use ddof=1 for the unbiased sample variance estimate\n    v_hat = np.var(n_samples, ddof=1)\n    \n    # Avoid division by zero if the empirical mean is zero\n    if m_hat == 0:\n        r_emp = np.nan\n    else:\n        r_emp = v_hat / m_hat\n\n    # Calculate theoretical statistics based on the derived formulas\n    m_th = k / beta\n    v_th = (k / beta) + (k / (beta**2))\n    \n    if m_th == 0:\n        r_th = np.nan\n    else:\n        r_th = v_th / m_th\n\n    # Define validation tolerances\n    tau_m = 0.02  # Relative error tolerance for the mean\n    tau_v = 0.05  # Relative error tolerance for the variance\n\n    # Check if the simulation passes the validation criteria\n    # Handle cases where theoretical mean/variance could be zero to avoid division by zero\n    passed = True\n    if m_th > 0:\n        err_m = abs(m_hat - m_th) / m_th\n        if err_m > tau_m:\n            passed = False\n    elif m_hat != 0: # If m_th is 0, m_hat should also be 0\n        passed = False\n        \n    if v_th > 0:\n        err_v = abs(v_hat - v_th) / v_th\n        if err_v > tau_v:\n            passed = False\n    elif v_hat != 0: # If v_th is 0, v_hat should also be 0\n        passed = False\n\n    return [m_hat, v_hat, m_th, v_th, r_emp, r_th, passed]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (k, beta, T, seed)\n    test_cases = [\n        (10.0, 2.0, 120000, 12345),   # Case A: General overdispersion\n        (1000.0, 200.0, 200000, 24680), # Case B: Near-Poisson limit\n        (0.5, 0.5, 150000, 13579),    # Case C: Strong overdispersion\n    ]\n\n    results = []\n    for case in test_cases:\n        k, beta, T, seed = case\n        result_case = run_simulation_and_validation(k, beta, T, seed)\n        results.append(result_case)\n\n    # Final print statement in the exact required format.\n    # The default string representation for a Python list is used.\n    # map(str, results) would stringify each inner list, which is not the desired format.\n    # Instead, we directly stringify the list of lists.\n    # To match the required output format of [val,val,...] instead of Python's [val, val, ...],\n    # we build the string manually.\n    outer_list_str = []\n    for res in results:\n        # Convert boolean to lowercase 'true'/'false' if needed, but 'True'/'False' is standard\n        # The prompt just says \"boolean\", so Python's default str() is acceptable. Let's use it.\n        inner_list_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]},{res[5]},{res[6]}]\"\n        outer_list_str.append(inner_list_str)\n\n    print(f\"[{','.join(outer_list_str)}]\")\n\nsolve()\n```"
        }
    ]
}