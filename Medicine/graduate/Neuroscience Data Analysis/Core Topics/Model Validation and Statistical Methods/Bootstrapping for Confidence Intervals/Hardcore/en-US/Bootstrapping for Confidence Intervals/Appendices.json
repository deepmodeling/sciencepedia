{
    "hands_on_practices": [
        {
            "introduction": "The foundation of applying bootstrap methods is understanding how to implement the core resampling algorithm. This first practice guides you through constructing a basic bootstrap confidence interval for a common task in neuroscience: comparing the means of two independent groups . By writing the code to generate bootstrap replicates and calculate the interval, you will solidify your understanding of how resampling can approximate the sampling distribution of an estimator like the difference in means.",
            "id": "4142996",
            "problem": "You are given two independent groups of functional Magnetic Resonance Imaging (fMRI) subjects, each group containing subject-level regression coefficients (\"beta coefficients\") for a single condition extracted from a region of interest. Under the standard assumption that within each group the subject-level beta coefficients are independent and identically distributed, and that the two groups are independent of each other, consider the difference in mean beta coefficients between the two groups as the estimand of interest. Let $x_1, x_2, \\dots, x_{n_1}$ denote the first group's beta coefficients and $y_1, y_2, \\dots, y_{n_2}$ denote the second group's beta coefficients. The estimator for the difference in means is\n$$\n\\hat{\\Delta} = \\frac{1}{n_2} \\sum_{j=1}^{n_2} y_j \\;-\\; \\frac{1}{n_1} \\sum_{i=1}^{n_1} x_i.\n$$\nYou must compute a basic bootstrap confidence interval for $\\hat{\\Delta}$ using $B=2000$ bootstrap resamples, resampling subjects within each group with replacement, and report the resulting interval for a confidence level of $1-\\alpha$ with $\\alpha = 0.05$ (expressed as a decimal).\n\nThe basic bootstrap interval is defined by inverting the empirical distribution of the estimator’s error relative to the observed estimate. For each bootstrap resample, construct a bootstrap replicate $\\hat{\\Delta}^\\ast$ by resampling indices within each group and recomputing the difference in means. Define the bootstrap error replicate $E^\\ast = \\hat{\\Delta}^\\ast - \\hat{\\Delta}$. Let $q_{p}$ be the empirical $p$-quantile of the bootstrap error distribution $\\{E^\\ast_b\\}_{b=1}^B$. The $100 \\cdot (1-\\alpha)\\%$ basic bootstrap interval for $\\hat{\\Delta}$ is\n$$\n\\left[ \\hat{\\Delta} - q_{1-\\alpha/2}, \\; \\hat{\\Delta} - q_{\\alpha/2} \\right].\n$$\n\nImplement this procedure and apply it to the following test suite. For each case, $n_1 = 20$ and $n_2 = 22$ (numbers of subjects), and $B = 2000$. Use a fixed random number generator seed equal to $12345$ to ensure reproducibility of bootstrap resampling. The data arrays below list the observed subject-level coefficients for each group; treat these observed values as the entire available sample for bootstrapping.\n\nTest case $1$ (approximately symmetric distributions with a moderate difference in means):\n- Group $1$: $x^{(1)} = \\{0.39, 0.61, 0.52, 0.47, 0.58, 0.44, 0.73, 0.36, 0.49, 0.67, 0.42, 0.55, 0.63, 0.40, 0.51, 0.59, 0.46, 0.57, 0.48, 0.60\\}$\n- Group $2$: $y^{(1)} = \\{0.70, 0.68, 0.75, 0.81, 0.62, 0.77, 0.69, 0.73, 0.66, 0.79, 0.71, 0.74, 0.78, 0.65, 0.72, 0.76, 0.67, 0.80, 0.64, 0.82, 0.63, 0.85\\}$\n\nTest case $2$ (skewed, heavy-tailed positive coefficients):\n- Group $1$: $x^{(2)} = \\{0.12, 0.09, 0.15, 0.08, 0.20, 0.07, 0.11, 0.10, 0.13, 0.06, 0.18, 0.05, 0.25, 0.14, 0.09, 0.30, 0.04, 0.21, 0.16, 0.28\\}$\n- Group $2$: $y^{(2)} = \\{0.18, 0.16, 0.24, 0.20, 0.27, 0.15, 0.19, 0.23, 0.21, 0.26, 0.17, 0.22, 0.30, 0.14, 0.28, 0.25, 0.33, 0.13, 0.29, 0.31, 0.12, 0.35\\}$\n\nTest case $3$ (equal means by construction):\n- Group $1$: $x^{(3)} = \\{0.45, 0.55, 0.50, 0.52, 0.48, 0.51, 0.49, 0.53, 0.47, 0.54, 0.46, 0.56, 0.44, 0.57, 0.43, 0.58, 0.42, 0.59, 0.41, 0.60\\}$\n- Group $2$: $y^{(3)} = \\{0.45, 0.55, 0.50, 0.52, 0.48, 0.51, 0.49, 0.53, 0.47, 0.54, 0.46, 0.56, 0.44, 0.57, 0.43, 0.58, 0.42, 0.59, 0.41, 0.60, 0.505, 0.505\\}$\n\nTest case $4$ (low variance, near-constant coefficients with a small mean shift):\n- Group $1$: $x^{(4)} = \\{1.00, 1.01, 0.99, 1.02, 0.98, 1.00, 1.01, 0.99, 1.02, 0.98, 1.00, 1.01, 0.99, 1.02, 0.98, 1.00, 1.01, 0.99, 1.02, 0.98\\}$\n- Group $2$: $y^{(4)} = \\{1.03, 1.04, 1.02, 1.05, 1.01, 1.03, 1.04, 1.02, 1.05, 1.01, 1.03, 1.04, 1.02, 1.05, 1.01, 1.03, 1.04, 1.02, 1.05, 1.01, 1.03, 1.04\\}$\n\nYour program must:\n- Implement the basic bootstrap confidence interval as defined above using $B = 2000$ resamples and $\\alpha = 0.05$.\n- For each test case, compute and return the lower and upper bounds of the interval for $\\hat{\\Delta}$, where $\\hat{\\Delta}$ is defined as the mean of group $2$ minus the mean of group $1$.\n- Use a fixed random number generator seed equal to $12345$ for all bootstrap resampling steps.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[\\text{lower}, \\text{upper}]$ of floats for the corresponding test case, in order from test case $1$ to test case $4$. For example, the output format must be of the form $[[L_1,U_1],[L_2,U_2],[L_3,U_3],[L_4,U_4]]$.",
            "solution": "The problem is valid. It presents a well-defined computational statistics task based on standard, scientifically sound principles of bootstrap resampling for constructing confidence intervals, applied to a realistic scenario in neuroscience data analysis. All necessary data, parameters, and definitions are provided, and the problem is self-contained and objective.\n\nThe objective is to compute a basic bootstrap confidence interval for the difference in means between two independent groups of subjects. Let the observed data for the two groups be denoted by the sets $X = \\{x_1, x_2, \\dots, x_{n_1}\\}$ and $Y = \\{y_1, y_2, \\dots, y_{n_2}\\}$, where $n_1$ and $n_2$ are the respective sample sizes.\n\nThe parameter of interest (the estimand) is the true difference in population means, $\\Delta = \\mu_Y - \\mu_X$. The point estimate for this parameter is the observed difference in sample means:\n$$\n\\hat{\\Delta} = \\bar{y} - \\bar{x} = \\frac{1}{n_2} \\sum_{j=1}^{n_2} y_j \\;-\\; \\frac{1}{n_1} \\sum_{i=1}^{n_1} x_i\n$$\nWe are tasked with constructing a $100 \\cdot (1-\\alpha)\\%$ confidence interval for $\\Delta$, where the confidence level is specified by $\\alpha = 0.05$. The method to be used is the basic bootstrap, also known as the reverse percentile interval.\n\nThe fundamental idea of the bootstrap is to approximate the sampling distribution of an estimator by repeatedly resampling from the observed data. The basic bootstrap interval is derived by assuming that the distribution of the estimation error, $\\hat{\\Delta} - \\Delta$, can be approximated by the empirical distribution of bootstrap errors, $E^\\ast = \\hat{\\Delta}^\\ast - \\hat{\\Delta}$.\n\nThe algorithm proceeds as follows:\n\n1.  **Calculate the Observed Statistic**: First, compute the difference in means from the original data samples, $\\hat{\\Delta}$. This serves as our point estimate.\n\n2.  **Generate Bootstrap Resamples and Replicates**: Repeat the following procedure $B$ times, for $b = 1, 2, \\dots, B$:\n    a.  Generate a bootstrap sample for group $1$, denoted $X^{\\ast b}$, by drawing $n_1$ observations from the original sample $X$ with replacement.\n    b.  Generate a bootstrap sample for group $2$, denoted $Y^{\\ast b}$, by drawing $n_2$ observations from the original sample $Y$ with replacement.\n    c.  For each pair of bootstrap samples $(X^{\\ast b}, Y^{\\ast b})$, compute the bootstrap replicate of the statistic:\n        $$\n        \\hat{\\Delta}^{\\ast b} = \\overline{y^{\\ast b}} - \\overline{x^{\\ast b}}\n        $$\n    This process yields a collection of $B$ bootstrap replicates, $\\{\\hat{\\Delta}^{\\ast b}\\}_{b=1}^B$.\n\n3.  **Compute the Bootstrap Error Distribution**: From the bootstrap replicates, we construct the empirical distribution of bootstrap errors. For each replicate, the error is defined as the deviation from the original estimate:\n    $$\n    E^{\\ast b} = \\hat{\\Delta}^{\\ast b} - \\hat{\\Delta}\n    $$\n    This gives a set of $B$ bootstrap error values, $\\{E^{\\ast b}\\}_{b=1}^B$.\n\n4.  **Determine Quantiles of the Error Distribution**: Let $q_p$ denote the empirical $p$-quantile of the sorted bootstrap error distribution $\\{E^{\\ast b}\\}$. We need the quantiles corresponding to the tails of the confidence interval, specifically $q_{\\alpha/2}$ and $q_{1-\\alpha/2}$. For $\\alpha=0.05$, these are the $0.025$ and $0.975$ quantiles (i.e., the $2.5^{th}$ and $97.5^{th}$ percentiles).\n\n5.  **Construct the Confidence Interval**: The $100 \\cdot (1-\\alpha)\\%$ basic bootstrap confidence interval for $\\Delta$ is constructed by pivoting on the observed statistic $\\hat{\\Delta}$ using the error quantiles:\n    $$\n    \\text{CI}_{1-\\alpha}(\\Delta) = \\left[ \\hat{\\Delta} - q_{1-\\alpha/2}, \\quad \\hat{\\Delta} - q_{\\alpha/2} \\right]\n    $$\n    This construction inverts the error distribution around the point estimate. The lower bound of the confidence interval is corrected by the upper quantile of the error distribution, and the upper bound is corrected by the lower quantile of the error distribution.\n\nFor this problem, we are given $n_1 = 20$, $n_2 = 22$, the number of bootstrap resamples $B = 2000$, and a confidence parameter $\\alpha = 0.05$. A fixed random number generator seed of $12345$ is used to ensure the reproducibility of the resampling process. This entire procedure will be applied to each of the four provided test cases.\n\nThe implementation will utilize the `numpy` library for efficient array operations, resampling via `numpy.random.choice`, and quantile calculation via `numpy.quantile`. A single random number generator instance, seeded with $12345$, will be used for all resampling steps across all test cases to ensure strict adherence to the problem's reproducibility requirement.",
            "answer": "```python\nimport numpy as np\n\ndef compute_basic_bootstrap_ci(x, y, B, alpha, rng):\n    \"\"\"\n    Computes the basic bootstrap confidence interval for the difference in means.\n\n    Args:\n        x (np.ndarray): Data for group 1.\n        y (np.ndarray): Data for group 2.\n        B (int): Number of bootstrap resamples.\n        alpha (float): Significance level for the confidence interval.\n        rng (np.random.Generator): A NumPy random number generator.\n\n    Returns:\n        list: A list containing the lower and upper bounds of the confidence interval.\n    \"\"\"\n    # Get sample sizes\n    n1 = len(x)\n    n2 = len(y)\n\n    # Step 1: Calculate the observed statistic (difference in means)\n    delta_hat = np.mean(y) - np.mean(x)\n\n    # Step 2  3: Generate bootstrap replicates and compute the statistic\n    bootstrap_deltas = np.empty(B)\n    for i in range(B):\n        # Resample with replacement from each group\n        x_star = rng.choice(x, size=n1, replace=True)\n        y_star = rng.choice(y, size=n2, replace=True)\n        \n        # Compute the bootstrap replicate of the statistic\n        bootstrap_deltas[i] = np.mean(y_star) - np.mean(x_star)\n\n    # Step 4: Compute the bootstrap distribution of the error\n    bootstrap_errors = bootstrap_deltas - delta_hat\n\n    # Step 5: Find the quantiles of the error distribution\n    q_alpha_2 = np.quantile(bootstrap_errors, alpha / 2.0)\n    q_1_minus_alpha_2 = np.quantile(bootstrap_errors, 1 - alpha / 2.0)\n\n    # Step 6: Construct the confidence interval\n    lower_bound = delta_hat - q_1_minus_alpha_2\n    upper_bound = delta_hat - q_alpha_2\n\n    return [lower_bound, upper_bound]\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([0.39, 0.61, 0.52, 0.47, 0.58, 0.44, 0.73, 0.36, 0.49, 0.67, 0.42, 0.55, 0.63, 0.40, 0.51, 0.59, 0.46, 0.57, 0.48, 0.60]),\n            np.array([0.70, 0.68, 0.75, 0.81, 0.62, 0.77, 0.69, 0.73, 0.66, 0.79, 0.71, 0.74, 0.78, 0.65, 0.72, 0.76, 0.67, 0.80, 0.64, 0.82, 0.63, 0.85]),\n        ),\n        (\n            np.array([0.12, 0.09, 0.15, 0.08, 0.20, 0.07, 0.11, 0.10, 0.13, 0.06, 0.18, 0.05, 0.25, 0.14, 0.09, 0.30, 0.04, 0.21, 0.16, 0.28]),\n            np.array([0.18, 0.16, 0.24, 0.20, 0.27, 0.15, 0.19, 0.23, 0.21, 0.26, 0.17, 0.22, 0.30, 0.14, 0.28, 0.25, 0.33, 0.13, 0.29, 0.31, 0.12, 0.35]),\n        ),\n        (\n            np.array([0.45, 0.55, 0.50, 0.52, 0.48, 0.51, 0.49, 0.53, 0.47, 0.54, 0.46, 0.56, 0.44, 0.57, 0.43, 0.58, 0.42, 0.59, 0.41, 0.60]),\n            np.array([0.45, 0.55, 0.50, 0.52, 0.48, 0.51, 0.49, 0.53, 0.47, 0.54, 0.46, 0.56, 0.44, 0.57, 0.43, 0.58, 0.42, 0.59, 0.41, 0.60, 0.505, 0.505]),\n        ),\n        (\n            np.array([1.00, 1.01, 0.99, 1.02, 0.98, 1.00, 1.01, 0.99, 1.02, 0.98, 1.00, 1.01, 0.99, 1.02, 0.98, 1.00, 1.01, 0.99, 1.02, 0.98]),\n            np.array([1.03, 1.04, 1.02, 1.05, 1.01, 1.03, 1.04, 1.02, 1.05, 1.01, 1.03, 1.04, 1.02, 1.05, 1.01, 1.03, 1.04, 1.02, 1.05, 1.01, 1.03, 1.04]),\n        ),\n    ]\n\n    # Parameters from the problem\n    B = 2000\n    alpha = 0.05\n    seed = 12345\n\n    # Initialize random number generator once for all resampling steps\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for x_data, y_data in test_cases:\n        ci = compute_basic_bootstrap_ci(x_data, y_data, B, alpha, rng)\n        results.append(ci)\n\n    # Custom formatting to avoid spaces inside the inner lists, though\n    # the template suggests `map(str, results)` which would add spaces.\n    # To be fully compliant with the example `[L_1,U_1]`, this manual format is better.\n    inner_results_str = [f\"[{l},{u}]\" for l, u in results]\n    print(f\"[{','.join(inner_results_str)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "Neuroscience experiments often produce complex datasets with a nested or hierarchical structure, such as trials within neurons and neurons within subjects. This exercise demonstrates how the bootstrap framework can be elegantly extended to handle such data . You will implement a hierarchical bootstrap, learning to preserve the data's structure by resampling at each level, which is essential for accurately estimating population-level statistics and their uncertainty.",
            "id": "4142939",
            "problem": "You are given a hierarchical dataset of neural responses recorded as trial-level real-valued observations from multiple neurons nested within multiple subjects. The inferential target is the population-level median response across neurons, respecting the hierarchical nesting of neurons within subjects and trials within neurons. Let the dataset be represented as a three-level nested list, where the outer list indexes subjects, the next inner list indexes neurons within a subject, and the innermost list contains the trial-level responses for a neuron. Denote subjects by $s \\in \\{1,\\dots,S\\}$, neurons within subject $s$ by $i \\in \\{1,\\dots,N_s\\}$, and trials within neuron $(s,i)$ by $j \\in \\{1,\\dots,T_{s,i}\\}$. The observed trial responses are $x_{s,i,j} \\in \\mathbb{R}$. For each neuron $(s,i)$, define the neuron-level median $m_{s,i}$ as the median of $\\{x_{s,i,1},\\dots,x_{s,i,T_{s,i}}\\}$. The population-level statistic of interest is the across-neuron median\n$$\nM = \\operatorname{median}\\left(\\{m_{s,i} : s \\in \\{1,\\dots,S\\}, i \\in \\{1,\\dots,N_s\\}\\}\\right).\n$$\n\nStarting from the following foundational principles:\n- The bootstrap resampling principle: approximate the sampling distribution of a statistic by resampling with replacement from the empirical distribution at appropriate levels while preserving the hierarchical structure.\n- Exchangeability within levels under the sampling design: subjects are exchangeable at the subject level, neurons are exchangeable within subjects, and trials are exchangeable within neurons.\n- The definition of the median and quantiles: for a real-valued random variable with empirical distribution, the median is the $0.5$ quantile, and a two-sided confidence interval at level $(1-\\alpha)$ can be constructed from bootstrap replicates by taking the empirical $(\\alpha/2)$ and $(1-\\alpha/2)$ quantiles.\n\nYour task is to design and implement a program that constructs a hierarchical bootstrap percentile interval for $M$ by:\n- Resampling subjects with replacement: draw $S$ subject indices with replacement from $\\{1,\\dots,S\\}$.\n- For each resampled subject, resample neurons with replacement: draw $N_s$ neuron indices with replacement from $\\{1,\\dots,N_s\\}$ for that subject.\n- For each resampled neuron, resample trials with replacement: draw $T_{s,i}$ trial indices with replacement from $\\{1,\\dots,T_{s,i}\\}$ for that neuron.\n- For each bootstrapped neuron, compute the neuron-level median of its resampled trials, and then compute the overall median $M^\\ast$ across all bootstrapped neurons in the resample.\n- Repeat the above to obtain $B$ bootstrap replicates $\\{M^\\ast_b\\}_{b=1}^B$.\n- Form the two-sided percentile confidence interval at level $(1-\\alpha)$ for $M$ as $[q_{\\alpha/2}, q_{1-\\alpha/2}]$, where $q_p$ denotes the empirical $p$-quantile of the bootstrap replicate distribution.\n\nImplement the above without any additional assumptions beyond exchangeability within levels and the observed data, and ensure your program produces reproducible results by using the specified random seeds via a pseudo-random number generator.\n\nTest Suite and Parameters:\nFor each of the following test cases, compute the hierarchical bootstrap percentile interval $[q_{\\alpha/2}, q_{1-\\alpha/2}]$ for $M$ and return it as a list of two floats. The test cases specify the dataset, the number of bootstrap replicates $B$, the confidence level parameter $\\alpha$, and a random seed for reproducibility.\n\n- Test Case $1$ (general case with variability across levels):\n  - Data $D_1$:\n    - Subject $1$: Neuron $1$ trials $[0.8, 0.9, 1.1, 1.0, 0.95]$, Neuron $2$ trials $[1.2, 1.1, 1.3, 1.25, 1.15]$.\n    - Subject $2$: Neuron $1$ trials $[0.5, 0.55, 0.6]$, Neuron $2$ trials $[0.7, 0.75, 0.72, 0.74]$, Neuron $3$ trials $[0.65, 0.6, 0.62]$.\n    - Subject $3$: Neuron $1$ trials $[1.5, 1.45, 1.55, 1.6]$, Neuron $2$ trials $[1.0, 1.05, 0.95, 1.02]$.\n  - Replicates $B = 5000$.\n  - Alpha $\\alpha = 0.05$.\n  - Seed $123$.\n\n- Test Case $2$ (boundary case: single subject, single neuron, single trial):\n  - Data $D_2$: Subject $1$: Neuron $1$ trials $[0.37]$.\n  - Replicates $B = 1000$.\n  - Alpha $\\alpha = 0.10$.\n  - Seed $7$.\n\n- Test Case $3$ (edge case: unbalanced numbers of neurons and trials across subjects):\n  - Data $D_3$:\n    - Subject $1$: Neuron $1$ trials $[0.2, 0.25]$, Neuron $2$ trials $[0.3]$, Neuron $3$ trials $[0.18, 0.22, 0.24]$.\n    - Subject $2$: Neuron $1$ trials $[0.5, 0.55, 0.58, 0.52]$, Neuron $2$ trials $[0.4, 0.42]$.\n  - Replicates $B = 4000$.\n  - Alpha $\\alpha = 0.10$.\n  - Seed $999$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s confidence interval is itself a two-element list of floats. For example, the output structure must be of the form $[[\\ell_1,u_1],[\\ell_2,u_2],[\\ell_3,u_3]]$ with no additional text before or after. The floats may be formatted to a fixed number of decimal places; the unit is arbitrary real values with no physical unit associated. The angle unit is not applicable. Percentages are not to be used; any confidence level specification is via the decimal $\\alpha$ values provided above.",
            "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded problem in computational statistics. It is self-contained, consistent, and free of any ambiguities or fallacies. The task is to implement a hierarchical bootstrap procedure to construct a percentile confidence interval for a population-level median ($M$) derived from a nested data structure, common in neuroscience.\n\n**Principles and Algorithmic Design**\n\nThe solution is designed by directly translating the provided foundational principles into a computational algorithm. The core of the problem lies in correctly simulating the sampling process for a hierarchically structured dataset.\n\n**1. Data Representation and Target Statistic**\n\nThe dataset consists of real-valued trial responses $x_{s,i,j}$ organized in a three-level hierarchy: trials ($j$) are nested within neurons ($i$), which are nested within subjects ($s$).\nThe statistic of interest is the population-level median, $M$, defined as:\n$$\nM = \\operatorname{median}\\left(\\{m_{s,i} : s \\in \\{1,\\dots,S\\}, i \\in \\{1,\\dots,N_s\\}\\}\\right)\n$$\nwhere $m_{s,i}$ is the median of the trial responses for neuron $(s,i)$:\n$$\nm_{s,i} = \\operatorname{median}\\left(\\{x_{s,i,1}, \\dots, x_{s,i,T_{s,i}}\\}\\right)\n$$\nHere, $S$ is the total number of subjects, $N_s$ is the number of neurons for subject $s$, and $T_{s,i}$ is the number of trials for neuron $(s,i)$.\n\n**2. The Hierarchical Bootstrap Principle**\n\nThe central tenet of bootstrapping is to approximate the sampling distribution of a statistic by repeatedly resampling from the observed data. For hierarchical data, this resampling must respect the nested structure to be valid. This is justified by the principle of **exchangeability at each leve**l:\n-   Subjects are assumed to be exchangeable drawings from a larger population of subjects.\n-   Within a given subject, its neurons are assumed to be exchangeable.\n-   Within a given neuron, its trials are assumed to be exchangeable.\n\nThe algorithm implements this by resampling with replacement at each level of the hierarchy. For a single bootstrap replicate, the procedure is as follows:\n\n-   **Step 2a: Subject Resampling.** A new set of $S$ subjects is created by drawing $S$ times with replacement from the original set of subjects $\\{1, \\dots, S\\}$. This means some original subjects may appear multiple times in the bootstrap sample, while others may not appear at all.\n\n-   **Step 2b: Neuron Resampling.** For each subject selected in the step above (e.g., a resampled copy of original subject $s$), its neurons are resampled. If the original subject $s$ had $N_s$ neurons, we draw $N_s$ times with replacement from that subject's original pool of neurons. This is performed independently for each subject in the resampled subject list.\n\n-   **Step 2c: Trial Resampling.** For each neuron selected in the step above (e.g., a resampled copy of original neuron $(s,i)$ with $T_{s,i}$ trials), its trials are resampled. We draw $T_{s,i}$ times with replacement from that neuron's original set of trials $\\{x_{s,i,1}, \\dots, x_{s,i,T_{s,i}}\\}$.\n\n**3. Generation of the Bootstrap Distribution**\n\nThe multi-level resampling process generates one \"bootstrap dataset\". From this dataset, we compute a single bootstrap replicate of the statistic of interest, $M^\\ast_b$:\n1.  For each resampled neuron in the bootstrap dataset, we compute its median from its resampled trials. This gives a set of bootstrapped neuron-level medians. The total number of such medians is equal to the total number of neurons in the original dataset, $\\sum_{s=1}^S N_s$.\n2.  The median of this set of bootstrapped neuron-level medians is calculated. This value is one bootstrap replicate, $M^\\ast_b$.\n\nThis entire procedure is repeated $B$ times, where $B$ is a large number (e.g., $B=5000$ in Test Case $1$), to generate a collection of replicates $\\{M^\\ast_1, M^\\ast_2, \\dots, M^\\ast_B\\}$. This collection serves as an empirical approximation of the sampling distribution of $M$.\n\n**4. Confidence Interval Construction**\n\nThe final step is to construct the $(1-\\alpha)$ two-sided percentile confidence interval. This is achieved by computing the empirical quantiles of the sorted bootstrap distribution.\n-   The lower bound of the confidence interval, $q_{\\alpha/2}$, is the $(\\alpha/2)$-quantile of the distribution $\\{M^\\ast_b\\}_{b=1}^B$.\n-   The upper bound, $q_{1-\\alpha/2}$, is the $(1-\\alpha/2)$-quantile of the same distribution.\n\nFor a test case with confidence parameter $\\alpha = 0.05$, the interval is formed by the $0.025$ and $0.975$ quantiles of the bootstrap replicates. This method directly applies the principles provided in the problem statement. The use of a seeded pseudo-random number generator ensures that the random sampling process is deterministic and the results are reproducible.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes hierarchical bootstrap percentile confidence intervals for the \n    population-level median of neuron-level medians for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"data\": [\n                # Subject 1\n                [[0.8, 0.9, 1.1, 1.0, 0.95], [1.2, 1.1, 1.3, 1.25, 1.15]],\n                # Subject 2\n                [[0.5, 0.55, 0.6], [0.7, 0.75, 0.72, 0.74], [0.65, 0.6, 0.62]],\n                # Subject 3\n                [[1.5, 1.45, 1.55, 1.6], [1.0, 1.05, 0.95, 1.02]]\n            ],\n            \"B\": 5000,\n            \"alpha\": 0.05,\n            \"seed\": 123\n        },\n        {\n            \"data\": [[[0.37]]],  # Subject 1, Neuron 1, Trial 1\n            \"B\": 1000,\n            \"alpha\": 0.10,\n            \"seed\": 7\n        },\n        {\n            \"data\": [\n                # Subject 1\n                [[0.2, 0.25], [0.3], [0.18, 0.22, 0.24]],\n                # Subject 2\n                [[0.5, 0.55, 0.58, 0.52], [0.4, 0.42]]\n            ],\n            \"B\": 4000,\n            \"alpha\": 0.10,\n            \"seed\": 999\n        }\n    ]\n\n    results = []\n    \n    # Pre-convert trial data from lists to numpy arrays for efficient processing.\n    for case in test_cases:\n        case['data'] = [[np.array(neuron_trials) for neuron_trials in subject_neurons] for subject_neurons in case['data']]\n\n    for case in test_cases:\n        data = case['data']\n        B = case['B']\n        alpha = case['alpha']\n        seed = case['seed']\n        \n        # Initialize a pseudo-random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        num_subjects = len(data)\n        \n        bootstrap_replicates = np.zeros(B)\n        \n        for b in range(B):\n            # This list will hold the neuron-level medians for one bootstrap sample.\n            bootstrapped_neuron_medians = []\n            \n            # Level 1: Resample subjects with replacement.\n            resampled_subject_indices = rng.choice(num_subjects, size=num_subjects, replace=True)\n            \n            for subject_idx in resampled_subject_indices:\n                original_subject_neurons = data[subject_idx]\n                num_neurons_in_subject = len(original_subject_neurons)\n                \n                if num_neurons_in_subject == 0:\n                    continue\n\n                # Level 2: Resample neurons within the selected subject with replacement.\n                resampled_neuron_indices = rng.choice(num_neurons_in_subject, size=num_neurons_in_subject, replace=True)\n                \n                for neuron_idx in resampled_neuron_indices:\n                    original_neuron_trials = original_subject_neurons[neuron_idx]\n                    num_trials_in_neuron = len(original_neuron_trials)\n                    \n                    if num_trials_in_neuron == 0:\n                        continue\n\n                    # Level 3: Resample trials within the selected neuron with replacement.\n                    resampled_trial_indices = rng.choice(num_trials_in_neuron, size=num_trials_in_neuron, replace=True)\n                    resampled_trials = original_neuron_trials[resampled_trial_indices]\n                    \n                    # Compute the median for the resampled neuron.\n                    neuron_median = np.median(resampled_trials)\n                    bootstrapped_neuron_medians.append(neuron_median)\n\n            # Compute the population-level median for this bootstrap replicate.\n            if bootstrapped_neuron_medians:\n                M_star = np.median(bootstrapped_neuron_medians)\n                bootstrap_replicates[b] = M_star\n            else:\n                # This case occurs if the bootstrap sample contains no trials.\n                bootstrap_replicates[b] = np.nan\n\n        # Remove any NaN replicates before computing quantiles.\n        valid_replicates = bootstrap_replicates[~np.isnan(bootstrap_replicates)]\n\n        # Construct the percentile confidence interval.\n        lower_quantile = alpha / 2.0\n        upper_quantile = 1.0 - (alpha / 2.0)\n        \n        if len(valid_replicates) > 0:\n            ci = np.quantile(valid_replicates, [lower_quantile, upper_quantile])\n        else:\n            # If no valid replicates, CI is undefined.\n            ci = [np.nan, np.nan]\n        \n        # Store the result as a list of floats.\n        results.append(list(ci))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A powerful feature of the bootstrap is that the distribution of bootstrap replicates provides a window into the sampling distribution of your statistic. This conceptual practice shifts our focus from implementation to interpretation, asking you to diagnose the properties of a bootstrap distribution using standard visualization tools . Learning to identify issues like bias and skew is critical for choosing between different types of bootstrap intervals and ensuring your scientific conclusions are built on robust statistical inference.",
            "id": "4142960",
            "problem": "You are analyzing a trial-level neural synchrony measure, the Phase-Locking Value (PLV), between single-unit spike times and local field potential (LFP) at $20$ Hz during a sensory task. The PLV is bounded between $0$ and $1$ and you summarize synchrony across $n = 120$ trials by an estimator $\\hat{\\theta}$, defined as the sample mean of trial PLV values. To quantify uncertainty, you generate $B = 10000$ bootstrap replicates $\\hat{\\theta}^{\\ast}_1, \\ldots, \\hat{\\theta}^{\\ast}_B$ using resampling with replacement from the $n$ trials.\n\nAs a diagnostic, you inspect two plots of the bootstrap replicates:\n- A histogram of $\\hat{\\theta}^{\\ast}$ showing a unimodal distribution with a long right tail.\n- A Quantile-Quantile (QQ) plot of $\\hat{\\theta}^{\\ast}$ versus a standard normal reference that is approximately linear through the center but shows curvature in the tails: points in the upper tail lie above the reference line while points in the lower tail lie below the line.\n\nYou also note that the median of the bootstrap replicates exceeds $\\hat{\\theta}$, suggesting positive bias. You aim to choose a $95\\%$ Confidence Interval (CI) that has reliable coverage for $\\theta$ given the observed skewness and tail behavior.\n\nStarting from the following fundamental bases:\n- The nonparametric bootstrap approximates the sampling distribution of an estimator by the empirical distribution of resampled data, so the distribution of $\\hat{\\theta}^{\\ast}$ serves as a proxy for the sampling distribution of $\\hat{\\theta}$.\n- In a QQ plot against a normal reference, systematic deviations from the $45^\\circ$ line indicate departures from normality: curvature where the upper tail points lie above the line and lower tail points lie below the line indicates heavier tails than normal and right-skewness; approximate linearity indicates approximate normality.\n- Confidence interval procedures differ in how they respond to asymmetry and bias: percentile intervals use empirical quantiles of $\\hat{\\theta}^{\\ast}$; basic intervals reflect symmetry around $\\hat{\\theta}$ on the estimator scale; studentized intervals rely on approximate normality of a standardized statistic; bias-corrected and accelerated (BCa) intervals adjust quantile levels using bias-correction and acceleration terms estimated from data to accommodate skewness and bias.\n\nWhich of the following statements correctly explain how to use the histogram and QQ plot of $\\hat{\\theta}^{\\ast}$ to assess skewness and tail behavior and, based on that assessment, to guide interval choice for $\\theta$ in this neural application?\n\nA. When the QQ plot of $\\hat{\\theta}^{\\ast}$ against a standard normal shows upper-tail points above the line and lower-tail points below, this indicates heavier tails than normal and right-skewness; therefore a Bias-Corrected and Accelerated (BCa) bootstrap CI is preferred over a studentized interval because BCa adapts to skewness and bias without assuming symmetric error.\n\nB. A histogram of $\\hat{\\theta}^{\\ast}$ with a long right tail implies the percentile bootstrap CI will be symmetric around $\\hat{\\theta}$ and thus narrower on the right tail, avoiding overcoverage in the upper tail.\n\nC. If the QQ plot is approximately linear except for slight deviation at the extremes, the studentized bootstrap CI is appropriate; if the histogram reveals strong skewness, studentization still guarantees correct coverage because the bootstrap $t$ statistic is normal by the Central Limit Theorem regardless of skewness.\n\nD. When the bootstrap replicates cluster near the lower boundary (for a bounded measure like PLV) with a pronounced right tail and the median of $\\hat{\\theta}^{\\ast}$ exceeds $\\hat{\\theta}$, the BCa interval’s bias-correction and acceleration will typically reallocate tail probabilities to better match the asymmetric sampling distribution, reducing undercoverage on the heavy-tailed side compared to basic or symmetric intervals.\n\nE. Histogram and QQ plots primarily inform central tendency and variance of $\\hat{\\theta}^{\\ast}$; they do not reveal skewness or tail behavior and therefore should not influence CI method selection.",
            "solution": "The user wants me to validate and solve a problem concerning the choice of a bootstrap confidence interval method based on diagnostic plots of bootstrap replicates.\n\n### Step 1: Extract Givens\n-   **Analyte**: Phase-Locking Value (PLV), a trial-level neural synchrony measure.\n-   **Bounds**: PLV is bounded between $0$ and $1$.\n-   **Sample Size**: $n = 120$ trials.\n-   **Estimator**: $\\hat{\\theta}$, the sample mean of the $n$ trial PLV values.\n-   **Bootstrap**: $B = 10000$ bootstrap replicates, $\\hat{\\theta}^{\\ast}_1, \\ldots, \\hat{\\theta}^{\\ast}_B$, generated by resampling with replacement.\n-   **Diagnostic 1 (Histogram)**: The histogram of $\\hat{\\theta}^{\\ast}$ is unimodal and has a long right tail.\n-   **Diagnostic 2 (QQ Plot)**: A Quantile-Quantile plot of $\\hat{\\theta}^{\\ast}$ versus a standard normal distribution is approximately linear in the center, but the upper tail points lie above the reference line and the lower tail points lie below the reference line.\n-   **Diagnostic 3 (Bias)**: The median of the bootstrap replicates is greater than the original estimate $\\hat{\\theta}$.\n-   **Goal**: Select a $95\\%$ Confidence Interval (CI) for the true parameter $\\theta$ that has reliable coverage, given the observed distribution properties.\n-   **Provided Principles**:\n    1.  The distribution of $\\hat{\\theta}^{\\ast}$ approximates the sampling distribution of $\\hat{\\theta}$.\n    2.  The specified QQ plot curvature (upper tail above, lower tail below the line) indicates heavier-than-normal tails and right-skewness.\n    3.  Properties of CI methods (percentile, basic, studentized, BCa) are defined, noting that BCa is designed to adapt to skewness and bias.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is well-grounded in statistics and its application to neuroscience. PLV is a standard measure, and bootstrapping is a common and powerful technique for statistical inference in this field. The descriptions of the statistical methods (BCa, studentized, etc.) and diagnostic plots (histogram, QQ plot) are accurate.\n-   **Well-Posed**: The problem is well-posed. It presents a clear set of empirical observations about a bootstrap distribution and asks for a reasoned conclusion about the choice of a confidence interval method, based on provided statistical principles. A unique set of correct and incorrect statements can be identified.\n-   **Objective**: The problem is stated objectively using standard terminology from statistics and neurophysiology. The diagnostic findings are presented as facts to be interpreted.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically sound, well-posed, and objective, describing a realistic data analysis scenario. I will proceed to derive the solution and evaluate each option.\n\n### Solution Derivation\nThe diagnostic information points to a specific set of properties for the sampling distribution of $\\hat{\\theta}$.\n1.  **Skewness**: The histogram of $\\hat{\\theta}^{\\ast}$ with a \"long right tail\" is a direct visual indication of a right-skewed distribution.\n2.  **Tail Behavior and Skewness (from QQ Plot)**: The QQ plot provides a more technical confirmation. The y-axis represents the quantiles of the data ($\\hat{\\theta}^{\\ast}$), and the x-axis represents the quantiles of a standard normal distribution.\n    -   \"upper tail points lie above the reference line\": This means that for large quantiles, the data is more extreme (larger) than a normal distribution would be. This indicates a heavy or long right tail.\n    -   \"lower tail points lie below the line\": This means that for low quantiles (negative values on the x-axis), the data is less extreme (less negative, i.e., larger) than a normal distribution. For instance, the $5^{th}$ percentile of the data is greater than the $5^{th}$ percentile of the standard normal. This indicates a light or short left tail.\n    -   A combination of a heavy right tail and a light left tail is the definition of right-skewness. The description in the provided principles (\"heavier tails than normal and right-skewness\") is a correct interpretation of this pattern.\n3.  **Bias**: The observation that the median of the bootstrap replicates exceeds the original estimate ($\\text{median}(\\hat{\\theta}^{\\ast}) > \\hat{\\theta}$) suggests positive bias. The bootstrap estimate of bias is $\\text{bias}_{\\text{boot}} = E[\\hat{\\theta}^{\\ast}] - \\hat{\\theta}$, which is the mean of the bootstrap distribution minus the sample estimate. In a right-skewed distribution, the mean is typically greater than the median. Therefore, if $\\text{median}(\\hat{\\theta}^{\\ast}) > \\hat{\\theta}$, it is very likely that $\\text{mean}(\\hat{\\theta}^{\\ast}) > \\hat{\\theta}$, confirming positive bias.\n\nIn summary, the sampling distribution of $\\hat{\\theta}$ is inferred to be non-normal, specifically right-skewed and positively biased. The choice of a confidence interval method must account for these features to achieve accurate coverage.\n\n-   **Standard \"t-interval\" / \"Symmetric CI\"**: Methods that assume symmetry (e.g., $\\hat{\\theta} \\pm z_{\\alpha/2} \\cdot \\text{SE}$) will perform poorly. They will create an interval that is symmetric around $\\hat{\\theta}$, failing to capture the asymmetry of the true sampling distribution. This typically leads to undercoverage in the long tail (the upper bound is too low) and overcoverage in the short tail (the lower bound is too low).\n-   **Studentized Bootstrap CI**: This method can have excellent theoretical properties (higher-order accuracy) but relies on the studentized statistic $t^{\\ast} = (\\hat{\\theta}^{\\ast} - \\hat{\\theta}) / \\text{se}(\\hat{\\theta}^{\\ast})$ having a distribution that is approximately symmetric and pivotal. Given the strong skewness in the distribution of $\\hat{\\theta}^{\\ast}$ itself, assuming the distribution of $t^{\\ast}$ will be nicely symmetric is risky without further diagnostics. Furthermore, it requires a stable estimate of the standard error for each bootstrap replicate, $\\text{se}(\\hat{\\theta}^{\\ast})$, which can be computationally intensive (requiring a second level of bootstrapping) and potentially unstable.\n-   **BCa (Bias-Corrected and Accelerated) Bootstrap CI**: This method is specifically designed to address the issues of bias and skewness. It adjusts the quantiles of the bootstrap distribution used for the interval endpoints. The bias-correction parameter, $\\hat{z}_0$, explicitly uses the proportion of bootstrap replicates less than $\\hat{\\theta}$ to correct for median bias. The acceleration parameter, $\\hat{a}$, corrects for the fact that the standard error of the estimator may not be constant (i.e., skewness). Given the clear evidence of bias and skewness, the BCa interval is the most appropriate advanced method among the standard choices.\n\n### Option-by-Option Analysis\n\n**A. When the QQ plot of $\\hat{\\theta}^{\\ast}$ against a standard normal shows upper-tail points above the line and lower-tail points below, this indicates heavier tails than normal and right-skewness; therefore a Bias-Corrected and Accelerated (BCa) bootstrap CI is preferred over a studentized interval because BCa adapts to skewness and bias without assuming symmetric error.**\n-   This statement correctly interprets the specified QQ plot behavior as indicative of right-skewness. As analyzed above, a heavy right tail and light left tail produce this shape.\n-   It correctly identifies the BCa interval as being adaptive to skewness and bias.\n-   It correctly contrasts this with the studentized interval, whose performance relies more heavily on the symmetry of the standardized statistic. In the presence of strong skewness and bias, the BCa is generally a more robust choice.\n-   The reasoning is sound and directly applies the provided principles to the problem's diagnostics.\n-   **Verdict: Correct**\n\n**B. A histogram of $\\hat{\\theta}^{\\ast}$ with a long right tail implies the percentile bootstrap CI will be symmetric around $\\hat{\\theta}$ and thus narrower on the right tail, avoiding overcoverage in the upper tail.**\n-   The percentile CI is constructed from the quantiles of the bootstrap distribution, $[\\hat{\\theta}^{\\ast}_{(\\alpha/2)}, \\hat{\\theta}^{\\ast}_{(1-\\alpha/2)}]$.\n-   If the bootstrap distribution has a long right tail (is right-skewed), the distance from its center (e.g., median) to the upper quantile will be *larger* than the distance to the lower quantile.\n-   Therefore, the resulting percentile CI will be *asymmetric* and *wider* on the right side, not narrower. Furthermore, the interval is not centered on or symmetric around $\\hat{\\theta}$. The statement is incorrect on multiple factual points.\n-   **Verdict: Incorrect**\n\n**C. If the QQ plot is approximately linear except for slight deviation at the extremes, the studentized bootstrap CI is appropriate; if the histogram reveals strong skewness, studentization still guarantees correct coverage because the bootstrap $t$ statistic is normal by the Central Limit Theorem regardless of skewness.**\n-   This statement contains a critical flaw in statistical theory. The Central Limit Theorem (CLT) applies to the standardized *mean* of a sample, ensuring it approaches a normal distribution as the sample size grows. The CLT does *not* state that the bootstrap *t* statistic, $t^{\\ast} = (\\hat{\\theta}^{\\ast} - \\hat{\\theta}) / \\text{se}(\\hat{\\theta}^{\\ast})$, will be normally distributed regardless of the underlying skewness. In fact, for finite samples from a skewed distribution, the distribution of $t^{\\ast}$ can also be skewed. The claim that studentization \"guarantees\" correct coverage is far too strong and the justification is false.\n-   **Verdict: Incorrect**\n\n**D. When the bootstrap replicates cluster near the lower boundary (for a bounded measure like PLV) with a pronounced right tail and the median of $\\hat{\\theta}^{\\ast}$ exceeds $\\hat{\\theta}$, the BCa interval’s bias-correction and acceleration will typically reallocate tail probabilities to better match the asymmetric sampling distribution, reducing undercoverage on the heavy-tailed side compared to basic or symmetric intervals.**\n-   This statement accurately describes the scenario. A measure bounded at $0$ (like PLV) with a long right tail implies the distribution is concentrated at lower values.\n-   It correctly connects the observation \"median of $\\hat{\\theta}^{\\ast}$ exceeds $\\hat{\\theta}$\" to the presence of positive bias, which the BCa method explicitly corrects via its $\\hat{z}_0$ term.\n-   It provides an excellent mechanistic description of what BCa does: it \"reallocates tail probabilities\" (i.e., adjusts the quantiles from $\\alpha/2$ and $1-\\alpha/2$) to create an asymmetric interval.\n-   It correctly states the consequence: this adjustment improves coverage by making the interval appropriately wider on the heavy-tailed (right) side, thus \"reducing undercoverage\" where simpler symmetric or basic intervals would likely fail.\n-   **Verdict: Correct**\n\n**E. Histogram and QQ plots primarily inform central tendency and variance of $\\hat{\\theta}^{\\ast}$; they do not reveal skewness or tail behavior and therefore should not influence CI method selection.**\n-   This statement is fundamentally false. While a histogram and QQ plot provide information about central tendency and variance, their primary purpose in diagnostic checks is to assess the overall *shape* of the distribution, which critically includes skewness (asymmetry) and kurtosis (tail behavior). These properties are precisely what determine whether assumptions of normality are met and are paramount in selecting an appropriate inference procedure like a confidence interval.\n-   **Verdict: Incorrect**",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}