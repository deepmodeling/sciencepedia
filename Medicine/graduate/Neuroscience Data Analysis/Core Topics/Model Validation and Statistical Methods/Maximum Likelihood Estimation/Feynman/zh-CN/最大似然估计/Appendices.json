{
    "hands_on_practices": [
        {
            "introduction": "我们将从一个基础的例子开始，它直接模拟了神经科学中的一个常见任务：为神经元放电率建模。指数分布常用于描述泊松过程中连续事件之间的时间间隔，这使其成为对神经脉冲间隔（inter-spike intervals）进行建模的理想选择。通过这个练习，你将实践最大似然估计的核心逻辑：构建似然函数，并通过最大化该函数来找到最能解释观测数据的参数。",
            "id": "1933604",
            "problem": "一位网络工程师正在分析一个数据网络上的流量。到达某一特定路由器的连续数据包之间的时间间隔被建模为独立同分布的随机变量。设单个此类到达间隔时间由随机变量 $X$ 表示。根据大量先前的研究，该时间被假定服从指数分布，其概率密度函数 (PDF) 为：\n$$f(x; \\theta) = \\frac{1}{\\theta} \\exp\\left(-\\frac{x}{\\theta}\\right), \\quad \\text{for } x > 0$$\n其中 $\\theta > 0$ 是未知的真实平均到达间隔时间。为了估计当前网络条件下的这个参数，该工程师收集了一个包含 $n$ 个到达间隔时间的随机样本：$x_1, x_2, \\dots, x_n$。\n\n你的任务是确定平均到达间隔时间 $\\theta$ 的最大似然估计量 (MLE)。将你的答案表示为样本观测值 $x_1, x_2, \\dots, x_n$ 的函数。",
            "solution": "我们将 $X_{1},\\dots,X_{n}$ 建模为独立同分布的随机变量，其概率密度函数为 $f(x;\\theta)=\\theta^{-1}\\exp(-x/\\theta)$，其中 $x>0$ 且 $\\theta>0$。对于观测值 $x_{1},\\dots,x_{n}$，其似然函数为\n$$\nL(\\theta)=\\prod_{i=1}^{n}f(x_{i};\\theta)=\\prod_{i=1}^{n}\\left(\\theta^{-1}\\exp\\left(-\\frac{x_{i}}{\\theta}\\right)\\right)=\\theta^{-n}\\exp\\left(-\\frac{\\sum_{i=1}^{n}x_{i}}{\\theta}\\right).\n$$\n对数似然函数为\n$$\n\\ell(\\theta)=\\ln L(\\theta)=-n\\ln\\theta-\\frac{1}{\\theta}\\sum_{i=1}^{n}x_{i}.\n$$\n对 $\\theta$ 求导并令其为零以找到临界点：\n$$\n\\frac{d\\ell}{d\\theta}=-\\frac{n}{\\theta}+\\frac{\\sum_{i=1}^{n}x_{i}}{\\theta^{2}}=0.\n$$\n两边同乘以 $\\theta^{2}$ 得\n$$\n-\\!n\\theta+\\sum_{i=1}^{n}x_{i}=0 \\quad\\Rightarrow\\quad \\hat{\\theta}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}.\n$$\n为验证这是一个最大值，计算二阶导数：\n$$\n\\frac{d^{2}\\ell}{d\\theta^{2}}=\\frac{n}{\\theta^{2}}-\\frac{2\\sum_{i=1}^{n}x_{i}}{\\theta^{3}}.\n$$\n在 $\\hat{\\theta}$ 处求值，并使用 $\\sum_{i=1}^{n}x_{i}=n\\hat{\\theta}$，可得\n$$\n\\left.\\frac{d^{2}\\ell}{d\\theta^{2}}\\right|_{\\theta=\\hat{\\theta}}=\\frac{n}{\\hat{\\theta}^{2}}-\\frac{2n\\hat{\\theta}}{\\hat{\\theta}^{3}}=-\\frac{n}{\\hat{\\theta}^{2}}  0,\n$$\n因此 $\\hat{\\theta}$ 使对数似然函数最大化。由于每个 $x_{i}0$，该估计量为正且位于参数空间内。因此，$\\theta$ 的最大似然估计量是样本均值。",
            "answer": "$$\\boxed{\\frac{1}{n}\\sum_{i=1}^{n}x_{i}}$$"
        },
        {
            "introduction": "在掌握了单参数估计的基础后，我们来处理一个更接近真实研究场景的双参数问题。我们将使用高斯分布——信号处理中的主力模型——来为局部场电位（LFP）数据建模。这项练习不仅要求你推导出均值和方差的估计量，更重要的是，它将引导你探讨估计量偏差这一关键统计概念，让你能够辨析最大似然估计量与无偏样本方差之间的差异。",
            "id": "4177490",
            "problem": "一个实验室在一次持续性行为任务中，记录了来自海马区的细胞外局部场电位 (LFP) 信号。经过带通滤波以分离出特定的振荡频带后，分析人员提取了 $n$ 个简短且不重叠的时间窗口，并通过振荡在某个固定相位上的瞬时振幅 $y_i$ (其中 $i=1,\\dots,n$) 来对每个窗口进行总结。在一个针对该频带内聚合突触输入的标准生成模型下，假设这些振幅是独立同分布的，即 $y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$，其中 $\\mu$ 和 $\\sigma^2$ 分别表示该带限振幅的未知均值和方差。\n\n仅从高斯分布的概率密度函数和独立性假设出发，运用最大似然原理推导 $\\mu$ 和 $\\sigma^2$ 的估计量。然后，在真实模型下将这些估计量视为随机变量，分析方差估计量的期望，以量化其偏差。最后，将此偏差与使用贝塞尔校正的无偏方差估计量进行对比。\n\n你的任务：\n- 基于样本 $\\{y_i\\}_{i=1}^{n}$，推导 $\\mu$ 和 $\\sigma^2$ 的最大似然估计量 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^2$。\n- 在真实模型下计算 $\\mathbb{E}[\\hat{\\sigma}^2]$，并以 $n$ 和 $\\sigma^2$ 的形式，得到偏差 $\\mathrm{Bias}(\\hat{\\sigma}^2)=\\mathbb{E}[\\hat{\\sigma}^2]-\\sigma^2$ 的闭式解析表达式。\n- 简要解释将分母 $n$ 替换为 $n-1$ 的无偏估计量是如何消除此偏差的。\n\n最终答案仅需提供 $\\mathrm{Bias}(\\hat{\\sigma}^2)$ 的闭式解。无需四舍五入，也无需单位。",
            "solution": "该问题要求推导高斯分布参数的最大似然估计量，然后分析方差估计量的偏差。\n\n数据由一组 $n$ 个独立同分布 (i.i.d.) 的观测值 $\\{y_i\\}_{i=1}^{n}$ 组成，其中每个 $y_i$ 都从一个未知均值为 $\\mu$、未知方差为 $\\sigma^2$ 的高斯（正态）分布中抽取。单个观测值 $y_i$ 的概率密度函数 (PDF) 由下式给出：\n$$f(y_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)$$\n\n**步骤1：推导最大似然估计量 (MLE)**\n\n根据独立同分布假设，整个样本的似然函数 $L(\\mu, \\sigma^2)$ 是各个独立概率密度函数的乘积：\n$$L(\\mu, \\sigma^2 | \\{y_i\\}) = \\prod_{i=1}^{n} f(y_i | \\mu, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right)$$\n为了找到使该函数最大化的参数，在计算上更简单的方法是最大化对数似然函数 $\\ell(\\mu, \\sigma^2) = \\ln(L(\\mu, \\sigma^2))$：\n$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\n最大似然估计量 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^2$ 是使 $\\ell$ 最大化的 $\\mu$ 和 $\\sigma^2$ 的值。我们通过将 $\\ell$ 对 $\\mu$ 和 $\\sigma^2$ 的偏导数设为零来找到它们。\n\n首先，我们对 $\\mu$ 求导：\n$$\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} 2(y_i - \\mu)(-1) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)$$\n将导数设为零，求解估计量 $\\hat{\\mu}$：\n$$\\frac{1}{\\hat{\\sigma}^2}\\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0 \\implies \\sum_{i=1}^{n} y_i - n\\hat{\\mu} = 0 \\implies \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$$\n均值的最大似然估计量 $\\hat{\\mu}$ 是样本均值。\n\n接下来，我们对 $\\sigma^2$ 求导。为方便起见，令 $\\theta = \\sigma^2$。\n$$\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( -\\frac{n}{2}\\ln(\\theta) - \\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) = -\\frac{n}{2\\theta} + \\frac{1}{2\\theta^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\n将导数设为零，并代入估计量 $\\hat{\\theta} = \\hat{\\sigma}^2$ 和 $\\hat{\\mu}$：\n$$-\\frac{n}{2\\hat{\\sigma}^2} + \\frac{1}{2(\\hat{\\sigma}^2)^2}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0$$\n两边乘以 $2(\\hat{\\sigma}^2)^2$ 可得：\n$$-n\\hat{\\sigma}^2 + \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$$\n方差的最大似然估计量是 $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$。\n\n**步骤2：方差估计量 $\\hat{\\sigma}^2$ 的偏差**\n\n一个估计量的偏差是其期望值与被估计参数的真实值之间的差。我们需要计算 $\\mathrm{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2$。首先，我们计算期望 $\\mathbb{E}[\\hat{\\sigma}^2]$。\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right]$$\n我们通过加上并减去真实均值 $\\mu$ 来重写求和号内的项：\n$$y_i - \\hat{\\mu} = (y_i - \\mu) - (\\hat{\\mu} - \\mu)$$\n平方和变为：\n$$\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = \\sum_{i=1}^{n} [(y_i - \\mu) - (\\hat{\\mu} - \\mu)]^2 = \\sum_{i=1}^{n} \\left[ (y_i - \\mu)^2 - 2(y_i - \\mu)(\\hat{\\mu} - \\mu) + (\\hat{\\mu} - \\mu)^2 \\right]$$\n$$= \\sum_{i=1}^{n}(y_i - \\mu)^2 - 2(\\hat{\\mu} - \\mu)\\sum_{i=1}^{n}(y_i - \\mu) + n(\\hat{\\mu} - \\mu)^2$$\n注意到 $\\sum_{i=1}^{n}(y_i - \\mu) = n(\\frac{1}{n}\\sum y_i - \\mu) = n(\\hat{\\mu} - \\mu)$，我们将其代入上式：\n$$\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = \\sum_{i=1}^{n}(y_i - \\mu)^2 - 2(\\hat{\\mu} - \\mu)n(\\hat{\\mu} - \\mu) + n(\\hat{\\mu} - \\mu)^2 = \\sum_{i=1}^{n}(y_i - \\mu)^2 - n(\\hat{\\mu} - \\mu)^2$$\n现在我们利用期望的线性性质对该表达式求期望：\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n}(y_i - \\mu)^2\\right] - n\\mathbb{E}\\left[(\\hat{\\mu} - \\mu)^2\\right]$$\n第一项是 $\\mathbb{E}\\left[\\sum_{i=1}^{n}(y_i - \\mu)^2\\right] = \\sum_{i=1}^{n}\\mathbb{E}[(y_i - \\mu)^2] = \\sum_{i=1}^{n}\\sigma^2 = n\\sigma^2$，因为 $\\mathbb{E}[(y_i - \\mu)^2]$ 是方差 $\\sigma^2$ 的定义。\n第二项涉及样本均值的方差 $\\mathrm{Var}(\\hat{\\mu})$。估计量 $\\hat{\\mu}$ 是无偏的，所以 $\\mathbb{E}[\\hat{\\mu}] = \\mu$。因此，$\\mathbb{E}[(\\hat{\\mu} - \\mu)^2] = \\mathrm{Var}(\\hat{\\mu})$。\n$$\\mathrm{Var}(\\hat{\\mu}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\mathrm{Var}(y_i) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\sigma^2 = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$$\n将这些结果代回：\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = n\\sigma^2 - n\\left(\\frac{\\sigma^2}{n}\\right) = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2$$\n最后，我们计算 $\\mathbb{E}[\\hat{\\sigma}^2]$：\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\frac{1}{n}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n}(n-1)\\sigma^2 = \\frac{n-1}{n}\\sigma^2$$\n那么偏差为：\n$$\\mathrm{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\n方差的最大似然估计量 $\\hat{\\sigma}^2$ 是一个有偏估计量，因为它会系统性地以 $\\frac{n-1}{n}$ 的因子低估真实方差 $\\sigma^2$。\n\n**步骤3：贝塞尔校正**\n\n偏差的产生是因为离差平方和是围绕样本均值 $\\hat{\\mu}$ 计算的，而 $\\hat{\\mu}$ 本身是从数据中推导出来的，并非真实均值 $\\mu$。由于样本均值能使给定样本的离差平方和最小化，因此对于包括 $\\mu$ 在内的任何其他常数 $c$，$\\sum (y_i - \\hat{\\mu})^2$ 总是小于或等于 $\\sum (y_i - c)^2$。这导致了低估。\n\n无偏样本方差（通常记为 $S^2$）通过将分母 $n$ 替换为 $n-1$ 来引入贝塞尔校正：\n$$S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$$\n这个校正后估计量的期望是：\n$$\\mathbb{E}[S^2] = \\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right]$$\n使用我们之前得到的结果 $\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = (n-1)\\sigma^2$：\n$$\\mathbb{E}[S^2] = \\frac{1}{n-1}(n-1)\\sigma^2 = \\sigma^2$$\n因此，$\\mathrm{Bias}(S^2) = \\mathbb{E}[S^2] - \\sigma^2 = 0$。将校正因子 $\\frac{n}{n-1}$ 应用于有偏的最大似然估计量 $\\hat{\\sigma}^2$ 可以完全消除其偏差，从而得到一个无偏估计量。分母 $n-1$ 被解释为平方和的自由度，它考虑了因估计样本均值而损失的一个自由度。",
            "answer": "$$\\boxed{-\\frac{\\sigma^2}{n}}$$"
        },
        {
            "introduction": "最后，我们将最大似然框架应用于神经科学中一个更复杂且广泛使用的工具：广义线性模型（GLM）。这个练习呈现了逻辑回归中一种常见但棘手的情况，称为“完全分离”，在这种情况下，标准的最大似然估计无法收敛到有限的解。通过分析这一失效模式，你将深入理解最大似然估计的局限性，并认识到为何需要正则化等技术来获得稳定且有意义的结果。",
            "id": "4177431",
            "problem": "在尖峰序列分析中，您将时间划分成小窗口，并记录一个二元响应 $y_i \\in \\{0,1\\}$，指示在时间窗口 $i$ 中是否发生了一个尖峰。您使用带有逻辑斯蒂链接的伯努利广义线性模型（GLM）对条件尖峰概率进行建模，即 $y_i \\mid \\mathbf{x}_i \\sim \\text{Bernoulli}(p_i)$，其中 $p_i = \\sigma(\\eta_i)$ 且 $\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$，而 $\\sigma(z) = \\dfrac{1}{1+e^{-z}}$。假设设计包含一个截距项和单个二元协变量 $x_i \\in \\{0,1\\}$，指示是否施加了刺激。根据经验，您观察到每当 $x_i = 1$ 时，总会有一个尖峰（$y_i=1$），而每当 $x_i = 0$ 时，从不产生尖峰（$y_i=0$）。假设 $x_i=1$ 和 $x_i=0$ 的行都至少出现一次。在此模型下，考虑 $\\boldsymbol{\\beta} = (\\beta_0,\\beta_1)^{\\top}$ 的无惩罚最大似然估计（MLE），以及在负对数似然上添加强度为 $\\lambda  0$ 的 $\\ell_2$ 惩罚项的惩罚似然（等价于对 $\\boldsymbol{\\beta}$ 施加一个零均值高斯先验，其方差与 $\\lambda^{-1}$ 成正比）。\n\n从伯努利对数似然 $\\ell(\\boldsymbol{\\beta}) = \\sum_i \\big( y_i \\log p_i + (1-y_i)\\log(1-p_i) \\big)$ 的定义和逻辑斯蒂链接出发，论证在所描述的数据配置下 $\\beta_0$ 和 $\\beta_1$ 的可辨识性、无惩罚 MLE 的存在性与行为、Fisher 信息的行为，以及 $\\ell_2$ 正则化对估计量存在性和唯一性的影响。\n\n选择所有正确的陈述。\n\nA. 在这些数据下，无惩罚的 MLE 不作为有限向量存在；对数似然仅在 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ 的极限下，沿着使每当 $y_i=1$ 时 $p_i \\to 1$ 以及每当 $y_i=0$ 时 $p_i \\to 0$ 的方向上达到最大化，且对数似然的上确界等于 $0$。\n\nB. 无惩罚的 MLE 存在且唯一，但具有非常大的标准误；得分函数的发散是一种数值假象，会随着更多迭代而消失。\n\nC. 无惩罚的对数似然有多个不同的有限最大化点，对应于不同的分离超平面，并且在任何最大化点处的 Hessian 矩阵（观测信息）都是负定的，因此标准的渐近正态推断适用。\n\nD. 对协变量 $x_i$ 进行仿射重缩放（中心化和缩放）可以恢复一个有限的无惩罚 MLE，而不会改变拟合概率，因为似然函数在此类变换下是不变的。\n\nE. 在负对数似然上添加 $\\ell_2$ 惩罚项会产生一个严格凹的惩罚目标函数，该函数具有唯一的有限最大化点；由此产生的估计量将原本发散的系数朝向 $0$ 收缩，并解决了由分离引起的不可辨识性问题。",
            "solution": "用户需要对问题陈述进行严格验证，然后进行完整的推导和逐项分析。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n-   **模型**：带有逻辑斯蒂链接函数的伯努利广义线性模型（GLM）。\n-   **响应变量**：$y_i \\in \\{0,1\\}$，指示在时间窗口 $i$ 中是否发生尖峰。\n-   **条件概率**：$y_i \\mid \\mathbf{x}_i \\sim \\text{Bernoulli}(p_i)$。\n-   **链接函数**：$p_i = \\sigma(\\eta_i)$，其中 $\\sigma(z) = \\dfrac{1}{1+e^{-z}}$。\n-   **线性预测器**：$\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$。\n-   **设计**：模型包含一个截距项和单个二元协变量 $x_i \\in \\{0,1\\}$。设计向量为 $\\mathbf{x}_i = (1, x_i)^{\\top}$，参数向量为 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)^{\\top}$。因此，$\\eta_i = \\beta_0 + x_i\\beta_1$。\n-   **数据配置**：\n    -   如果 $x_i = 1$，则 $y_i = 1$（总是）。\n    -   如果 $x_i = 0$，则 $y_i = 0$（总是）。\n-   **数据假设**：$x_i=1$ 和 $x_i=0$ 两种情况在数据集中都至少出现一次。\n-   **估计方法**：\n    1.  无惩罚最大似然估计（MLE）。\n    2.  在负对数似然上添加强度为 $\\lambda  0$ 的 $\\ell_2$ 惩罚项的惩罚似然。\n-   **对数似然函数**：$\\ell(\\boldsymbol{\\beta}) = \\sum_i \\big( y_i \\log p_i + (1-y_i)\\log(1-p_i) \\big)$。\n\n**步骤2：使用提取的已知条件进行验证**\n\n-   **科学依据**：该问题描述了逻辑斯蒂回归，一种标准且基础的统计方法。它在分箱尖峰序列数据上的应用是计算神经科学中的常见实践。所描述的数据配置是一种著名的统计现象，称为“完全分离”或“完美预测”。所要求的分析涉及该条件下估计量的数学性质。该问题在统计学及其在神经科学中的应用方面有坚实的基础。\n-   **适定性**：该问题是适定的。它提出了清晰的数据结构和特定的模型，并询问相应估计量的存在性、唯一性和行为。问题是精确的，并有明确的数学答案。\n-   **客观性**：问题使用客观、精确的数学语言陈述。没有主观或基于观点的断言。\n\n**步骤3：结论与行动**\n\n问题陈述是有效的。它科学合理、适定且客观。它描述了逻辑斯蒂回归中的一个经典场景，并就统计估计量的性质提出了相关问题。我现在将进行解题推导和选项分析。\n\n### 推导与求解\n\n问题描述了一个**完全分离**的场景，其中预测变量的线性组合完美地分开了两个结果类别。在这里，预测变量 $x_i$ 本身就完美地分开了结果 $y_i=0$ 和 $y_i=1$。\n\n**1. 无惩罚 MLE 的分析**\n\n对数似然函数由下式给出：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_i \\left[ y_i \\log p_i + (1-y_i)\\log(1-p_i) \\right] $$\n令 $I_0 = \\{i \\mid x_i=0\\}$ 和 $I_1 = \\{i \\mid x_i=1\\}$。令 $n_0 = |I_0|$ 和 $n_1 = |I_1|$。根据假设，$n_0 \\ge 1$ 且 $n_1 \\ge 1$。\n\n根据数据配置：\n-   对于 $i \\in I_0$，我们有 $x_i=0$ 和 $y_i=0$。线性预测器为 $\\eta_i = \\beta_0$，所以 $p_i = \\sigma(\\beta_0)$。\n-   对于 $i \\in I_1$，我们有 $x_i=1$ 和 $y_i=1$。线性预测器为 $\\eta_i = \\beta_0 + \\beta_1$，所以 $p_i = \\sigma(\\beta_0 + \\beta_1)$。\n\n将此代入对数似然表达式：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i \\in I_0} \\left[ 0 \\cdot \\log p_i + (1-0)\\log(1-p_i) \\right] + \\sum_{i \\in I_1} \\left[ 1 \\cdot \\log p_i + (1-1)\\log(1-p_i) \\right] $$\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i \\in I_0} \\log(1 - \\sigma(\\beta_0)) + \\sum_{i \\in I_1} \\log(\\sigma(\\beta_0 + \\beta_1)) $$\n$$ \\ell(\\boldsymbol{\\beta}) = n_0 \\log(1 - \\sigma(\\beta_0)) + n_1 \\log(\\sigma(\\beta_0 + \\beta_1)) $$\n\n为了最大化 $\\ell(\\boldsymbol{\\beta})$，我们必须同时最大化两项。对于 $u \\in (0,1)$，函数 $\\log(u)$ 在 $u \\to 1$ 时最大化，此时 $\\log(u) \\to 0$。\n-   为了最大化第一项，我们需要 $1 - \\sigma(\\beta_0) \\to 1$，这意味着 $\\sigma(\\beta_0) \\to 0$。这在 $\\beta_0 \\to -\\infty$ 时发生。\n-   为了最大化第二项，我们需要 $\\sigma(\\beta_0 + \\beta_1) \\to 1$。这在 $\\beta_0 + \\beta_1 \\to +\\infty$ 时发生。\n\n两个条件可以同时满足。例如，考虑路径 $\\beta_0 = -c$ 和 $\\beta_1 = 2c$，对于某个常数 $c  0$。那么 $\\beta_0 + \\beta_1 = c$。当 $c \\to \\infty$ 时，我们有 $\\beta_0 \\to -\\infty$ 和 $\\beta_0 + \\beta_1 \\to +\\infty$。沿着这条路径，$\\|\\boldsymbol{\\beta}\\| = \\sqrt{(-c)^2 + (2c)^2} = c\\sqrt{5} \\to \\infty$。\n\n对数似然的值趋近于：\n$$ \\lim_{\\substack{\\beta_0 \\to -\\infty \\\\ \\beta_0+\\beta_1 \\to +\\infty}} \\ell(\\boldsymbol{\\beta}) = n_0 \\log(1) + n_1 \\log(1) = 0 $$\n对数似然的上确界是 $0$。然而，对于任何有限向量 $\\boldsymbol{\\beta}$，这个值都无法达到，因为对于任何有限的 $z$，$\\sigma(z)$ 严格介于 $0$ 和 $1$ 之间。因此，无惩罚的最大似然估计（MLE）不作为有限向量存在。似然仅在参数向量 $\\boldsymbol{\\beta}$ 的范数趋于无穷大的极限情况下才被最大化。\n\nFisher 信息矩阵为 $\\mathcal{I}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$，其中 $\\mathbf{W}$ 是一个对角矩阵，其元素为 $W_{ii} = p_i(1-p_i)$。随着参数发散，概率 $p_i$ 趋近于 $0$ 或 $1$。在任何一种情况下，$p_i(1-p_i) \\to 0$。因此，$\\mathbf{W} \\to \\mathbf{0}$，Fisher 信息矩阵变为奇异矩阵，这与估计量的无限方差相符。\n\n**2. 惩罚 MLE 的分析**\n\n问题要求考虑在负对数似然上添加一个 $\\ell_2$ 惩罚项。要最小化的目标函数是：\n$$ J(\\boldsymbol{\\beta}) = - \\ell(\\boldsymbol{\\beta}) + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2 \\quad \\text{for } \\lambda  0 $$\n等价地，我们可以最大化惩罚对数似然：\n$$ \\ell_p(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) - \\lambda \\|\\boldsymbol{\\beta}\\|_2^2 $$\n让我们分析 $\\ell_p(\\boldsymbol{\\beta})$ 的 Hessian 矩阵。无惩罚对数似然的 Hessian 矩阵是 $\\mathbf{H}(\\boldsymbol{\\beta}) = -\\sum_i p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^\\top$。这个矩阵是负半定的。惩罚项 $-\\lambda \\|\\boldsymbol{\\beta}\\|_2^2 = -\\lambda(\\beta_0^2 + \\beta_1^2)$ 的 Hessian 矩阵是 $-2\\lambda \\mathbf{I}$，其中 $\\mathbf{I}$ 是 $2 \\times 2$ 单位矩阵。对于 $\\lambda  0$，这是一个负定矩阵。\n惩罚对数似然的 Hessian 矩阵是：\n$$ \\mathbf{H}_p(\\boldsymbol{\\beta}) = \\mathbf{H}(\\boldsymbol{\\beta}) - 2\\lambda \\mathbf{I} = -\\left( \\sum_i p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^\\top + 2\\lambda \\mathbf{I} \\right) $$\n矩阵 $\\sum_i p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^\\top$ 是正半定的，而 $2\\lambda \\mathbf{I}$ 是正定的。它们的和是正定的。因此，对于所有 $\\boldsymbol{\\beta}$，$\\mathbf{H}_p(\\boldsymbol{\\beta})$ 都是负定的。这证明了 $\\ell_p(\\boldsymbol{\\beta})$ 是一个严格凹函数。\n\n一个严格凹函数至多有一个最大化点。为了证明存在一个有限的最大化点，我们观察到当 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ 时，惩罚项 $-\\lambda \\|\\boldsymbol{\\beta}\\|_2^2 \\to -\\infty$。由于无惩罚的对数似然 $\\ell(\\boldsymbol{\\beta})$ 的上界为 $0$，因此当 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ 时，惩罚对数似然 $\\ell_p(\\boldsymbol{\\beta}) \\to -\\infty$。这种强制性（coercivity）保证了全局最大值必须在 $\\boldsymbol{\\beta}$ 的一个有限值处存在。\n因为该函数是严格凹的，所以这个有限的最大化点是唯一的。惩罚项将系数朝向 $0$ 收缩，防止了在无惩罚情况下出现发散现象，从而提供了一个唯一、稳定的解。\n\n### 选项评估\n\n**A. 在这些数据下，无惩罚的 MLE 不作为有限向量存在；对数似然仅在 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ 的极限下，沿着使每当 $y_i=1$ 时 $p_i \\to 1$ 以及每当 $y_i=0$ 时 $p_i \\to 0$ 的方向上达到最大化，且对数似然的上确界等于 $0$。**\n这一陈述精确而准确地描述了完全分离对逻辑斯蒂回归的后果。我们的推导证实，MLE 不作为有限向量存在，对数似然是通过将概率推向其观测到的二元结果（$0$ 或 $1$）来最大化的，这需要 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$，并且对数似然的上确界确实是 $0$。\n**结论：正确。**\n\n**B. 无惩罚的 MLE 存在且唯一，但具有非常大的标准误；得分函数的发散是一种数值假象，会随着更多迭代而消失。**\n这是不正确的。无惩罚的 MLE 不作为有限向量存在。参数估计发散的现象是完全分离下似然函数的一个基本数学性质，而不是数值假象。一个有限 MLE 但标准误非常大的情况可能在*准完全分离*时发生，但本题描述的是完全分离。\n**结论：不正确。**\n\n**C. 无惩罚的对数似然有多个不同的有限最大化点，对应于不同的分离超平面，并且在任何最大化点处的 Hessian 矩阵（观测信息）都是负定的，因此标准的渐近正态推断适用。**\n这一陈述在多个方面都是不正确的。不存在*有限的*最大化点。上确界是渐近达到的。因此，没有 Hessian 矩阵可以在“任何最大化点”处进行评估。依赖于有限 MLE 和非奇异 Fisher 信息矩阵的标准渐近推断在此不适用。\n**结论：不正确。**\n\n**D. 对协变量 $x_i$ 进行仿射重缩放（中心化和缩放）可以恢复一个有限的无惩罚 MLE，而不会改变拟合概率，因为似然函数在此类变换下是不变的。**\n这是不正确的。完全分离问题是几何问题；两个类别在预测变量空间中可以被一个超平面完美分离开。对预测变量进行仿射变换（$x'_i = ax_i+b$）是特征空间（如果我们包含截距项）的一个可逆线性变换。它不改变类别的可分性。新的参数将与旧的参数相关，但它们仍然需要发散才能最大化似然函数。重缩放协变量并不能解决 MLE 的不存在问题。\n**结论：不正确。**\n\n**E. 在负对数似然上添加 $\\ell_2$ 惩罚项会产生一个严格凹的惩罚目标函数，该函数具有唯一的有限最大化点；由此产生的估计量将原本发散的系数朝向 $0$ 收缩，并解决了由分离引起的不可辨识性问题。**\n这一陈述是正确的。如上所述，添加 $\\ell_2$ 惩罚（在贝叶斯框架中等同于零均值高斯先验）使惩罚对数似然函数严格凹。这种严格凹性，结合函数在 $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ 时的行为，保证了唯一的、有限的最大化点的存在。这种正则化技术，对于 GLM 来说被称为岭回归，通过惩罚大的参数值，有效地解决了由完全分离引起的不存在问题。\n**结论：正确。**",
            "answer": "$$\\boxed{AE}$$"
        }
    ]
}