{
    "hands_on_practices": [
        {
            "introduction": "The Gaussian distribution is a foundational model for continuous neural data, from Local Field Potentials (LFPs) to firing rates. This first exercise provides practice in deriving the maximum likelihood estimators for the mean and variance of a Gaussian model, a fundamental skill for any data analyst. More importantly, it delves into the properties of these estimators, revealing the inherent bias in the MLE for variance and demonstrating the principle behind Bessel's correction .",
            "id": "4177490",
            "problem": "A laboratory records extracellular Local Field Potential (LFP) signals from a hippocampal region during a sustained behavioral task. After band-pass filtering to isolate a specific oscillatory band, the analyst extracts $n$ short, non-overlapping time windows and summarizes each window by the instantaneous amplitude $y_i$ at a fixed phase of the oscillation, for $i=1,\\dots,n$. Under a standard generative model for aggregated synaptic inputs in this band, assume the amplitudes are independent and identically distributed as $y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$, where $\\mu$ and $\\sigma^2$ denote the unknown mean and variance of the band-limited amplitude.\n\nStarting only from the probability density function of the Gaussian distribution and the independence assumption, use the maximum likelihood principle to derive the estimators of $\\mu$ and $\\sigma^2$. Then, treating the estimators as random variables under the true model, analyze the expectation of the variance estimator to quantify its bias. Finally, contrast this bias with the unbiased variance estimator that uses Bessel’s correction.\n\nYour tasks:\n- Derive the maximum likelihood estimators $\\hat{\\mu}$ and $\\hat{\\sigma}^2$ for $\\mu$ and $\\sigma^2$ based on the sample $\\{y_i\\}_{i=1}^{n}$.\n- Compute $\\mathbb{E}[\\hat{\\sigma}^2]$ under the true model and obtain the bias $\\mathrm{Bias}(\\hat{\\sigma}^2)=\\mathbb{E}[\\hat{\\sigma}^2]-\\sigma^2$ as a closed-form analytic expression in terms of $n$ and $\\sigma^2$.\n- Briefly explain how the unbiased estimator that replaces the denominator $n$ with $n-1$ removes this bias.\n\nProvide as your final answer only the bias $\\mathrm{Bias}(\\hat{\\sigma}^2)$ in closed form. No rounding is required, and no units are required.",
            "solution": "The problem requires the derivation of the maximum likelihood estimators for the parameters of a Gaussian distribution, followed by an analysis of the bias of the variance estimator.\n\nThe data consists of a set of $n$ independent and identically distributed (i.i.d.) observations $\\{y_i\\}_{i=1}^{n}$, where each $y_i$ is drawn from a Gaussian (Normal) distribution with unknown mean $\\mu$ and unknown variance $\\sigma^2$. The probability density function (PDF) for a single observation $y_i$ is given by:\n$$f(y_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)$$\n\n**Step 1: Derivation of the Maximum Likelihood Estimators (MLEs)**\n\nDue to the i.i.d. assumption, the likelihood function $L(\\mu, \\sigma^2)$ for the entire sample is the product of the individual PDFs:\n$$L(\\mu, \\sigma^2 | \\{y_i\\}) = \\prod_{i=1}^{n} f(y_i | \\mu, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right)$$\nTo find the parameters that maximize this function, it is computationally simpler to maximize the log-likelihood function, $\\ell(\\mu, \\sigma^2) = \\ln(L(\\mu, \\sigma^2))$:\n$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\nThe MLEs $\\hat{\\mu}$ and $\\hat{\\sigma}^2$ are the values of $\\mu$ and $\\sigma^2$ that maximize $\\ell$. We find them by setting the partial derivatives of $\\ell$ with respect to $\\mu$ and $\\sigma^2$ to zero.\n\nFirst, we differentiate with respect to $\\mu$:\n$$\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} 2(y_i - \\mu)(-1) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)$$\nSetting the derivative to zero and solving for the estimator $\\hat{\\mu}$:\n$$\\frac{1}{\\hat{\\sigma}^2}\\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0 \\implies \\sum_{i=1}^{n} y_i - n\\hat{\\mu} = 0 \\implies \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$$\nThe MLE for the mean, $\\hat{\\mu}$, is the sample mean.\n\nNext, we differentiate with respect to $\\sigma^2$. Let $\\theta = \\sigma^2$ for convenience.\n$$\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( -\\frac{n}{2}\\ln(\\theta) - \\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) = -\\frac{n}{2\\theta} + \\frac{1}{2\\theta^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\nSetting the derivative to zero and substituting the estimators $\\hat{\\theta} = \\hat{\\sigma}^2$ and $\\hat{\\mu}$:\n$$-\\frac{n}{2\\hat{\\sigma}^2} + \\frac{1}{2(\\hat{\\sigma}^2)^2}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0$$\nMultiplying by $2(\\hat{\\sigma}^2)^2$ yields:\n$$-n\\hat{\\sigma}^2 + \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$$\nThe MLE for the variance is $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$.\n\n**Step 2: Bias of the Variance Estimator $\\hat{\\sigma}^2$**\n\nThe bias of an estimator is the difference between its expected value and the true value of the parameter being estimated. We need to compute $\\mathrm{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2$. First, we compute the expectation $\\mathbb{E}[\\hat{\\sigma}^2]$.\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right]$$\nWe rewrite the term inside the summation by adding and subtracting the true mean $\\mu$:\n$$y_i - \\hat{\\mu} = (y_i - \\mu) - (\\hat{\\mu} - \\mu)$$\nThe sum of squares becomes:\n$$\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = \\sum_{i=1}^{n} [(y_i - \\mu) - (\\hat{\\mu} - \\mu)]^2 = \\sum_{i=1}^{n} \\left[ (y_i - \\mu)^2 - 2(y_i - \\mu)(\\hat{\\mu} - \\mu) + (\\hat{\\mu} - \\mu)^2 \\right]$$\n$$= \\sum_{i=1}^{n}(y_i - \\mu)^2 - 2(\\hat{\\mu} - \\mu)\\sum_{i=1}^{n}(y_i - \\mu) + n(\\hat{\\mu} - \\mu)^2$$\nRecognizing that $\\sum_{i=1}^{n}(y_i - \\mu) = n(\\frac{1}{n}\\sum y_i - \\mu) = n(\\hat{\\mu} - \\mu)$, we substitute this into the expression:\n$$\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = \\sum_{i=1}^{n}(y_i - \\mu)^2 - 2(\\hat{\\mu} - \\mu)n(\\hat{\\mu} - \\mu) + n(\\hat{\\mu} - \\mu)^2 = \\sum_{i=1}^{n}(y_i - \\mu)^2 - n(\\hat{\\mu} - \\mu)^2$$\nNow we take the expectation of this expression using the linearity of expectation:\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n}(y_i - \\mu)^2\\right] - n\\mathbb{E}\\left[(\\hat{\\mu} - \\mu)^2\\right]$$\nThe first term is $\\mathbb{E}\\left[\\sum_{i=1}^{n}(y_i - \\mu)^2\\right] = \\sum_{i=1}^{n}\\mathbb{E}[(y_i - \\mu)^2] = \\sum_{i=1}^{n}\\sigma^2 = n\\sigma^2$, since $\\mathbb{E}[(y_i - \\mu)^2]$ is the definition of the variance $\\sigma^2$.\nThe second term involves the variance of the sample mean, $\\mathrm{Var}(\\hat{\\mu})$. The estimator $\\hat{\\mu}$ is unbiased, so $\\mathbb{E}[\\hat{\\mu}] = \\mu$. Thus, $\\mathbb{E}[(\\hat{\\mu} - \\mu)^2] = \\mathrm{Var}(\\hat{\\mu})$.\n$$\\mathrm{Var}(\\hat{\\mu}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\mathrm{Var}(y_i) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\sigma^2 = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$$\nSubstituting these results back:\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = n\\sigma^2 - n\\left(\\frac{\\sigma^2}{n}\\right) = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2$$\nFinally, we compute $\\mathbb{E}[\\hat{\\sigma}^2]$:\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\frac{1}{n}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n}(n-1)\\sigma^2 = \\frac{n-1}{n}\\sigma^2$$\nThe bias is then:\n$$\\mathrm{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\nThe MLE for the variance, $\\hat{\\sigma}^2$, is a biased estimator, as it systematically underestimates the true variance $\\sigma^2$ by a factor of $\\frac{n-1}{n}$.\n\n**Step 3: Bessel's Correction**\n\nThe bias arises because the sum of squared deviations is calculated around the sample mean $\\hat{\\mu}$, which is itself derived from the data, rather than the true mean $\\mu$. Since the sample mean minimizes the sum of squared deviations for a given sample, $\\sum (y_i - \\hat{\\mu})^2$ is always less than or equal to $\\sum (y_i - c)^2$ for any other constant $c$, including $\\mu$. This leads to the underestimation.\n\nThe unbiased sample variance, commonly denoted as $S^2$, incorporates Bessel's correction by replacing the denominator $n$ with $n-1$:\n$$S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2$$\nThe expectation of this corrected estimator is:\n$$\\mathbb{E}[S^2] = \\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = \\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right]$$\nUsing our previous result that $\\mathbb{E}\\left[\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2\\right] = (n-1)\\sigma^2$:\n$$\\mathbb{E}[S^2] = \\frac{1}{n-1}(n-1)\\sigma^2 = \\sigma^2$$\nThus, $\\mathrm{Bias}(S^2) = \\mathbb{E}[S^2] - \\sigma^2 = 0$. The correction factor $\\frac{n}{n-1}$ applied to the biased MLE $\\hat{\\sigma}^2$ perfectly cancels the bias, resulting in an unbiased estimator. The denominator $n-1$ is interpreted as the degrees of freedom of the sum of squares, accounting for the one degree of freedom lost to the estimation of the sample mean.",
            "answer": "$$\\boxed{-\\frac{\\sigma^2}{n}}$$"
        },
        {
            "introduction": "Beyond modeling the distribution of data, we often want to predict neural activity based on experimental variables, a core goal of systems neuroscience. This exercise moves into the domain of Generalized Linear Models (GLMs), a powerful framework for this task, using a logistic model for spike probability. By working through a hypothetical scenario of perfect prediction, you will investigate a critical practical issue known as complete separation, understand why the standard MLE fails, and see how regularization offers a robust solution .",
            "id": "4177431",
            "problem": "In a spike train analysis, you bin time into small windows and record a binary response $y_i \\in \\{0,1\\}$ indicating whether a spike occurred in bin $i$. You model the conditional spike probability with a Bernoulli Generalized Linear Model (GLM) with a logistic link, that is, $y_i \\mid \\mathbf{x}_i \\sim \\text{Bernoulli}(p_i)$ with $p_i = \\sigma(\\eta_i)$ and $\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$, where $\\sigma(z) = \\dfrac{1}{1+e^{-z}}$. Suppose the design includes an intercept and a single binary covariate $x_i \\in \\{0,1\\}$ indicating whether a stimulus was applied. Empirically, you observe that whenever $x_i = 1$ there is always a spike ($y_i=1$), and whenever $x_i = 0$ there is never a spike ($y_i=0$). Assume the rows with $x_i=1$ and $x_i=0$ both occur at least once. Consider the unpenalized Maximum Likelihood Estimation (MLE) for $\\boldsymbol{\\beta} = (\\beta_0,\\beta_1)^{\\top}$ under this model, and also the penalized likelihood with an $\\ell_2$ penalty of strength $\\lambda > 0$ added to the negative log-likelihood (equivalently, a zero-mean Gaussian prior on $\\boldsymbol{\\beta}$ with variance proportional to $\\lambda^{-1}$).\n\nStarting from the definition of the Bernoulli log-likelihood $\\ell(\\boldsymbol{\\beta}) = \\sum_i \\big( y_i \\log p_i + (1-y_i)\\log(1-p_i) \\big)$ and the logistic link, reason about the identifiability of $\\beta_0$ and $\\beta_1$ under the described data configuration, the existence and behavior of the unpenalized MLE, the behavior of the Fisher information, and the effect of $\\ell_2$ regularization on existence and uniqueness of the estimator.\n\nSelect all statements that are correct.\n\nA. Under these data, the unpenalized MLE does not exist as a finite vector; the log-likelihood is maximized only in the limit $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ along directions that make $p_i \\to 1$ whenever $y_i=1$ and $p_i \\to 0$ whenever $y_i=0$, with the supremum of the log-likelihood equal to $0$.\n\nB. The unpenalized MLE exists and is unique but has very large standard errors; the divergence of the score is a numerical artifact that disappears with more iterations.\n\nC. The unpenalized log-likelihood has multiple distinct finite maximizers corresponding to different separating hyperplanes, and the Hessian (observed information) at any maximizer is negative definite, so standard asymptotic normal inference applies.\n\nD. Affine rescaling (centering and scaling) of the covariate $x_i$ restores a finite unpenalized MLE without changing the fitted probabilities, because the likelihood is invariant to such transformations.\n\nE. Adding an $\\ell_2$ penalty to the negative log-likelihood yields a strictly concave penalized objective with a unique finite maximizer; the resulting estimator shrinks the otherwise divergent coefficient toward $0$ and resolves the separation-induced non-identifiability.",
            "solution": "The user wants a critical validation of the problem statement, followed by a full derivation and an option-by-option analysis.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Model**: Bernoulli Generalized Linear Model (GLM) with a logistic link function.\n-   **Response variable**: $y_i \\in \\{0,1\\}$, indicating spike occurrence in bin $i$.\n-   **Conditional probability**: $y_i \\mid \\mathbf{x}_i \\sim \\text{Bernoulli}(p_i)$.\n-   **Link function**: $p_i = \\sigma(\\eta_i)$, where $\\sigma(z) = \\dfrac{1}{1+e^{-z}}$.\n-   **Linear predictor**: $\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$.\n-   **Design**: The model includes an intercept and a single binary covariate $x_i \\in \\{0,1\\}$. The design vector is $\\mathbf{x}_i = (1, x_i)^{\\top}$ and the parameter vector is $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)^{\\top}$. Thus, $\\eta_i = \\beta_0 + x_i\\beta_1$.\n-   **Data configuration**:\n    -   If $x_i = 1$, then $y_i = 1$ (always).\n    -   If $x_i = 0$, then $y_i = 0$ (always).\n-   **Data assumption**: Both cases ($x_i=1$ and $x_i=0$) occur at least once in the dataset.\n-   **Estimation methods**:\n    1.  Unpenalized Maximum Likelihood Estimation (MLE).\n    2.  Penalized likelihood with an $\\ell_2$ penalty of strength $\\lambda > 0$ added to the negative log-likelihood.\n-   **Log-likelihood function**: $\\ell(\\boldsymbol{\\beta}) = \\sum_i \\big( y_i \\log p_i + (1-y_i)\\log(1-p_i) \\big)$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem describes logistic regression, a standard and fundamental statistical method. Its application to binned spike train data is a common practice in computational neuroscience. The data configuration described is a well-known statistical phenomenon called \"complete separation\" or \"perfect prediction\". The analysis requested pertains to the mathematical properties of estimators under this condition. The problem is firmly grounded in statistics and its application to neuroscience.\n-   **Well-Posed**: The problem is well-posed. It presents a clear data structure and a specific model and asks about the existence, uniqueness, and behavior of the corresponding estimators. The questions are precise and have definite mathematical answers.\n-   **Objective**: The problem is stated using objective and precise mathematical language. There are no subjective or opinion-based claims.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. It describes a classic scenario in logistic regression and asks pertinent questions about the properties of statistical estimators. I will now proceed with the solution derivation and option analysis.\n\n### Derivation and Solution\n\nThe problem describes a scenario of **complete separation**, where a linear combination of the predictors perfectly separates the two outcome classes. Here, the predictor $x_i$ itself perfectly separates the outcomes $y_i=0$ and $y_i=1$.\n\n**1. Analysis of the Unpenalized MLE**\n\nThe log-likelihood function is given by:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_i \\left[ y_i \\log p_i + (1-y_i)\\log(1-p_i) \\right] $$\nLet $I_0 = \\{i \\mid x_i=0\\}$ and $I_1 = \\{i \\mid x_i=1\\}$. Let $n_0 = |I_0|$ and $n_1 = |I_1|$. By assumption, $n_0 \\ge 1$ and $n_1 \\ge 1$.\n\nAccording to the data configuration:\n-   For $i \\in I_0$, we have $x_i=0$ and $y_i=0$. The linear predictor is $\\eta_i = \\beta_0$, so $p_i = \\sigma(\\beta_0)$.\n-   For $i \\in I_1$, we have $x_i=1$ and $y_i=1$. The linear predictor is $\\eta_i = \\beta_0 + \\beta_1$, so $p_i = \\sigma(\\beta_0 + \\beta_1)$.\n\nSubstituting this into the log-likelihood expression:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i \\in I_0} \\left[ 0 \\cdot \\log p_i + (1-0)\\log(1-p_i) \\right] + \\sum_{i \\in I_1} \\left[ 1 \\cdot \\log p_i + (1-1)\\log(1-p_i) \\right] $$\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i \\in I_0} \\log(1 - \\sigma(\\beta_0)) + \\sum_{i \\in I_1} \\log(\\sigma(\\beta_0 + \\beta_1)) $$\n$$ \\ell(\\boldsymbol{\\beta}) = n_0 \\log(1 - \\sigma(\\beta_0)) + n_1 \\log(\\sigma(\\beta_0 + \\beta_1)) $$\n\nTo maximize $\\ell(\\boldsymbol{\\beta})$, we must maximize both terms simultaneously. The function $\\log(u)$ for $u \\in (0,1)$ is maximized as $u \\to 1$, where $\\log(u) \\to 0$.\n-   To maximize the first term, we need $1 - \\sigma(\\beta_0) \\to 1$, which implies $\\sigma(\\beta_0) \\to 0$. This occurs as $\\beta_0 \\to -\\infty$.\n-   To maximize the second term, we need $\\sigma(\\beta_0 + \\beta_1) \\to 1$. This occurs as $\\beta_0 + \\beta_1 \\to +\\infty$.\n\nBoth conditions can be satisfied simultaneously. For example, consider the path $\\beta_0 = -c$ and $\\beta_1 = 2c$ for some constant $c > 0$. Then $\\beta_0 + \\beta_1 = c$. As $c \\to \\infty$, we have $\\beta_0 \\to -\\infty$ and $\\beta_0 + \\beta_1 \\to +\\infty$. Along this path, $\\|\\boldsymbol{\\beta}\\| = \\sqrt{(-c)^2 + (2c)^2} = c\\sqrt{5} \\to \\infty$.\n\nThe value of the log-likelihood approaches:\n$$ \\lim_{\\substack{\\beta_0 \\to -\\infty \\\\ \\beta_0+\\beta_1 \\to +\\infty}} \\ell(\\boldsymbol{\\beta}) = n_0 \\log(1) + n_1 \\log(1) = 0 $$\nThe supremum of the log-likelihood is $0$. However, this value is never achieved for any finite vector $\\boldsymbol{\\beta}$, because $\\sigma(z)$ is strictly between $0$ and $1$ for any finite $z$. Therefore, the unpenalized Maximum Likelihood Estimate (MLE) does not exist as a finite vector. The likelihood is maximized only in the limit as the norm of the parameter vector $\\boldsymbol{\\beta}$ goes to infinity.\n\nThe Fisher information matrix is $\\mathcal{I}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$, where $\\mathbf{W}$ is a diagonal matrix with entries $W_{ii} = p_i(1-p_i)$. As the parameters diverge, the probabilities $p_i$ approach either $0$ or $1$. In either case, $p_i(1-p_i) \\to 0$. Thus, $\\mathbf{W} \\to \\mathbf{0}$, and the Fisher information matrix becomes singular, which is consistent with infinite variance for the estimators.\n\n**2. Analysis of the Penalized MLE**\n\nThe problem asks to consider an $\\ell_2$ penalty added to the negative log-likelihood. The objective function to minimize is:\n$$ J(\\boldsymbol{\\beta}) = - \\ell(\\boldsymbol{\\beta}) + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2 \\quad \\text{for } \\lambda > 0 $$\nEquivalently, we can maximize the penalized log-likelihood:\n$$ \\ell_p(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) - \\lambda \\|\\boldsymbol{\\beta}\\|_2^2 $$\nLet's analyze the Hessian of $\\ell_p(\\boldsymbol{\\beta})$. The Hessian of the unpenalized log-likelihood is $\\mathbf{H}(\\boldsymbol{\\beta}) = -\\sum_i p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^\\top$. This matrix is negative semi-definite. The Hessian of the penalty term $-\\lambda \\|\\boldsymbol{\\beta}\\|_2^2 = -\\lambda(\\beta_0^2 + \\beta_1^2)$ is $-2\\lambda \\mathbf{I}$, where $\\mathbf{I}$ is the $2 \\times 2$ identity matrix. This is a negative definite matrix for $\\lambda > 0$.\nThe Hessian of the penalized log-likelihood is:\n$$ \\mathbf{H}_p(\\boldsymbol{\\beta}) = \\mathbf{H}(\\boldsymbol{\\beta}) - 2\\lambda \\mathbf{I} = -\\left( \\sum_i p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^\\top + 2\\lambda \\mathbf{I} \\right) $$\nThe matrix $\\sum_i p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^\\top$ is positive semi-definite and $2\\lambda \\mathbf{I}$ is positive definite. Their sum is positive definite. Therefore, $\\mathbf{H}_p(\\boldsymbol{\\beta})$ is negative definite for all $\\boldsymbol{\\beta}$. This proves that $\\ell_p(\\boldsymbol{\\beta})$ is a strictly concave function.\n\nA strictly concave function has at most one maximizer. To show that a finite maximizer exists, we observe that as $\\|\\boldsymbol{\\beta}\\| \\to \\infty$, the penalty term $-\\lambda \\|\\boldsymbol{\\beta}\\|_2^2 \\to -\\infty$. Since the unpenalized log-likelihood $\\ell(\\boldsymbol{\\beta})$ is bounded above by $0$, the penalized log-likelihood $\\ell_p(\\boldsymbol{\\beta}) \\to -\\infty$ as $\\|\\boldsymbol{\\beta}\\| \\to \\infty$. This coercivity guarantees that a global maximum must exist at a finite value of $\\boldsymbol{\\beta}$.\nBecause the function is strictly concave, this finite maximizer is unique. The penalty term shrinks the coefficients towards $0$, preventing the divergence seen in the unpenalized case and thus providing a unique, stable solution.\n\n### Evaluation of Options\n\n**A. Under these data, the unpenalized MLE does not exist as a finite vector; the log-likelihood is maximized only in the limit $\\|\\boldsymbol{\\beta}\\| \\to \\infty$ along directions that make $p_i \\to 1$ whenever $y_i=1$ and $p_i \\to 0$ whenever $y_i=0$, with the supremum of the log-likelihood equal to $0$.**\nThis statement is a precise and accurate description of the consequences of complete separation for logistic regression. Our derivation confirms that the MLE does not exist as a finite vector, that the log-likelihood is maximized by pushing probabilities to their observed binary outcomes ($0$ or $1$) which requires $\\|\\boldsymbol{\\beta}\\| \\to \\infty$, and that the supremum of the log-likelihood is indeed $0$.\n**Verdict: Correct.**\n\n**B. The unpenalized MLE exists and is unique but has very large standard errors; the divergence of the score is a numerical artifact that disappears with more iterations.**\nThis is incorrect. The unpenalized MLE does not exist as a finite vector. The phenomenon of diverging parameter estimates is a fundamental mathematical property of the likelihood function under complete separation, not a numerical artifact. A scenario with a finite MLE but very large standard errors can occur with *quasi-complete separation*, but the problem describes complete separation.\n**Verdict: Incorrect.**\n\n**C. The unpenalized log-likelihood has multiple distinct finite maximizers corresponding to different separating hyperplanes, and the Hessian (observed information) at any maximizer is negative definite, so standard asymptotic normal inference applies.**\nThis statement is incorrect on multiple grounds. There are no *finite* maximizers. The supremum is approached asymptotically. Consequently, there is no Hessian to evaluate \"at any maximizer\". Standard asymptotic inference, which relies on a finite MLE and a non-singular Fisher information matrix, is not applicable.\n**Verdict: Incorrect.**\n\n**D. Affine rescaling (centering and scaling) of the covariate $x_i$ restores a finite unpenalized MLE without changing the fitted probabilities, because the likelihood is invariant to such transformations.**\nThis is incorrect. The problem of complete separation is geometric; the two classes are perfectly separable by a hyperplane in the predictor space. An affine transformation of a predictor ($x'_i = ax_i+b$) is an invertible linear transformation of the feature space (if we include the intercept). It does not change the separability of the classes. The new parameters will be related to the old ones, but they will still need to diverge to maximize the likelihood. Rescaling the covariate does not resolve the non-existence of the MLE.\n**Verdict: Incorrect.**\n\n**E. Adding an $\\ell_2$ penalty to the negative log-likelihood yields a strictly concave penalized objective with a unique finite maximizer; the resulting estimator shrinks the otherwise divergent coefficient toward $0$ and resolves the separation-induced non-identifiability.**\nThis statement is correct. As derived above, adding an $\\ell_2$ penalty (equivalent to a zero-mean Gaussian prior in a Bayesian framework) makes the penalized log-likelihood function strictly concave. This strict concavity, combined with the behavior of the function as $\\|\\boldsymbol{\\beta}\\| \\to \\infty$, guarantees the existence of a unique, finite maximizer. This regularization technique, known as ridge regression for GLMs, effectively resolves the non-existence problem caused by complete separation by penalizing large parameter values.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "Neuroscience data often contains hidden structure, such as signals arising from distinct sub-populations of neurons or different underlying brain states. This practice introduces the Expectation-Maximization (EM) algorithm, an essential iterative method for finding maximum likelihood estimates in models with such latent variables. You will derive the core update steps for a Gaussian Mixture Model, learning how to use this powerful technique to simultaneously infer hidden group memberships and estimate model parameters from unlabeled data .",
            "id": "3899853",
            "problem": "A clinical laboratory is developing a probabilistic classifier to distinguish between two physiological states based on a continuous biomarker measured in serum, such as a cardiac troponin concentration. To account for heterogeneity in the patient population, the biomarker value for patient $i$, denoted $x_i$, is modeled as being drawn independently from a $2$-component Gaussian mixture: with probability $\\pi$ the patient belongs to state $1$ (e.g., acute injury) and $x_i$ is generated from a Gaussian distribution with mean $\\mu_1$ and known variance $\\sigma_1^2$, and with probability $1-\\pi$ the patient belongs to state $2$ (e.g., baseline) and $x_i$ is generated from a Gaussian distribution with mean $\\mu_2$ and known variance $\\sigma_2^2$. Formally, for each $i \\in \\{1,\\dots,n\\}$,\n$$\nx_i \\sim \\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2),\n$$\nwhere $\\mathcal{N}(x \\mid \\mu, \\sigma^2)$ denotes the Gaussian probability density function with mean $\\mu$ and variance $\\sigma^2$, and $\\sigma_1^2$ and $\\sigma_2^2$ are known, positive constants determined by prior assay validation studies. The unknown parameters are the mixing proportion $\\pi \\in (0,1)$ and the component means $\\mu_1$ and $\\mu_2$.\n\nUsing only the independence of samples, the definition of the mixture model, Bayes' rule, and the principle of Maximum Likelihood Estimation (MLE), derive the Expectation-Maximization (EM) algorithm’s responsibilities for component $1$ and the maximization-step updates for $\\pi$, $\\mu_1$, and $\\mu_2$. Specifically, let $\\gamma_{i1}$ denote the responsibility that component $1$ takes for datum $x_i$ in the expectation-step, and let $\\pi^{\\text{new}}$, $\\mu_1^{\\text{new}}$, and $\\mu_2^{\\text{new}}$ denote the updated parameter estimates in the maximization-step. Express your final answer as closed-form analytic expressions in terms of $\\{x_i\\}_{i=1}^n$, $\\pi$, $\\mu_1$, $\\mu_2$, $\\sigma_1^2$, and $\\sigma_2^2$.\n\nYour final answer must be a single row matrix containing, in order, the expressions for $\\gamma_{i1}$, $\\pi^{\\text{new}}$, $\\mu_1^{\\text{new}}$, and $\\mu_2^{\\text{new}}$. No numerical evaluation or rounding is required.",
            "solution": "The problem is valid. It presents a standard and well-posed statistical estimation task: deriving the Expectation-Maximization (EM) algorithm update equations for a two-component Gaussian Mixture Model (GMM) with known variances. The problem is scientifically grounded, objective, and contains all necessary information for a unique, meaningful solution.\n\nThe Expectation-Maximization (EM) algorithm is an iterative method for finding maximum likelihood estimates of parameters in statistical models that depend on unobserved latent variables. The problem at hand fits this framework perfectly. The observed data are the biomarker values $\\{x_i\\}_{i=1}^n$. The latent variables are the unknown component memberships for each observation.\n\nLet us define a latent variable $z_i$ for each observation $x_i$. We can represent $z_i$ as a binary indicator, where $z_{i1} = 1$ if $x_i$ is drawn from component $1$ and $z_{i1} = 0$ otherwise. Correspondingly, $z_{i2} = 1 - z_{i1}$, indicating membership in component $2$. The parameters to be estimated are $\\theta = \\{\\pi, \\mu_1, \\mu_2\\}$. The complete data set is $(X, Z) = (\\{x_i\\}_{i=1}^n, \\{z_i\\}_{i=1}^n)$.\n\nThe joint probability of the complete data $(x_i, z_i)$ for a single observation is given by:\n$$\np(x_i, z_i \\mid \\theta) = p(x_i \\mid z_i, \\theta) p(z_i \\mid \\theta)\n$$\nGiven the binary representation of $z_i$, we can write this as:\n$$\np(x_i, z_i \\mid \\theta) = \\left[ \\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right]^{z_{i1}} \\left[ (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right]^{z_{i2}}\n$$\nAssuming the observations are independent and identically distributed (i.i.d.), the complete-data likelihood is the product of the individual probabilities:\n$$\nL_c(\\theta \\mid X, Z) = p(X, Z \\mid \\theta) = \\prod_{i=1}^n p(x_i, z_i \\mid \\theta) = \\prod_{i=1}^n \\left[ \\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right]^{z_{i1}} \\left[ (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right]^{z_{i2}}\n$$\nThe complete-data log-likelihood, $\\mathcal{L}_c(\\theta) = \\ln L_c(\\theta \\mid X, Z)$, is:\n$$\n\\mathcal{L}_c(\\theta) = \\sum_{i=1}^n \\left[ z_{i1} \\left( \\ln\\pi + \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right) + z_{i2} \\left( \\ln(1-\\pi) + \\ln\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right) \\right]\n$$\nThe EM algorithm consists of two steps: the Expectation (E) step and the Maximization (M) step.\n\n**E-Step: Compute the responsibilities**\n\nIn the E-step, we compute the expectation of the complete-data log-likelihood with respect to the posterior distribution of the latent variables $Z$, given the observed data $X$ and the current parameter estimates $\\theta^{\\text{old}} = \\{\\pi^{\\text{old}}, \\mu_1^{\\text{old}}, \\mu_2^{\\text{old}}\\}$. This forms the Q-function:\n$$\nQ(\\theta \\mid \\theta^{\\text{old}}) = E_{Z \\mid X, \\theta^{\\text{old}}}[\\mathcal{L}_c(\\theta)]\n$$\nDue to the linearity of expectation, we only need to compute the expectation of the latent variables $z_{ik}$. This expectation is the posterior probability that observation $x_i$ belongs to component $k$, given $x_i$ and the current parameters. This posterior probability is called the \"responsibility\" of component $k$ for observation $i$. Let $\\gamma_{ik} = E[z_{ik} \\mid x_i, \\theta^{\\text{old}}]$.\n\nThe problem asks for $\\gamma_{i1}$, the responsibility of component $1$ for datum $x_i$. Using Bayes' rule, and dropping the 'old' superscript for notational clarity as per the problem's request for the final expression format:\n$$\n\\gamma_{i1} = p(z_{i1}=1 \\mid x_i, \\theta) = \\frac{p(x_i \\mid z_{i1}=1, \\theta) p(z_{i1}=1 \\mid \\theta)}{p(x_i \\mid \\theta)}\n$$\nThe denominator is the marginal probability of $x_i$, which is the sum over both components:\n$$\np(x_i \\mid \\theta) = \\sum_{k=1}^2 p(x_i \\mid z_{ik}=1, \\theta) p(z_{ik}=1 \\mid \\theta) = \\pi \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)\n$$\nThe numerator is the term for component $1$: $\\pi \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)$.\nThus, the responsibility $\\gamma_{i1}$ is:\n$$\n\\gamma_{i1} = \\frac{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)}{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)}\n$$\nBy symmetry, the responsibility of component $2$ is $\\gamma_{i2} = 1 - \\gamma_{i1}$.\nSubstituting the expectations of $z_{ik}$ with the responsibilities $\\gamma_{ik}$, the Q-function becomes:\n$$\nQ(\\theta \\mid \\theta^{\\text{old}}) = \\sum_{i=1}^n \\left[ \\gamma_{i1} \\left( \\ln\\pi + \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) \\right) + \\gamma_{i2} \\left( \\ln(1-\\pi) + \\ln\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2) \\right) \\right]\n$$\n\n**M-Step: Maximize the Q-function**\n\nIn the M-step, we find the new parameter estimates $\\theta^{\\text{new}}$ that maximize the Q-function. We do this by taking the partial derivatives of $Q(\\theta \\mid \\theta^{\\text{old}})$ with respect to each parameter in $\\theta = \\{\\pi, \\mu_1, \\mu_2\\}$ and setting them to zero.\n\n1.  **Update for $\\pi$:**\n    We maximize the terms in $Q$ that depend on $\\pi$: $\\sum_{i=1}^n \\left[ \\gamma_{i1}\\ln\\pi + \\gamma_{i2}\\ln(1-\\pi) \\right]$. Taking the derivative with respect to $\\pi$ and setting it to $0$:\n    $$\n    \\frac{\\partial Q}{\\partial \\pi} = \\sum_{i=1}^n \\left( \\frac{\\gamma_{i1}}{\\pi} - \\frac{\\gamma_{i2}}{1-\\pi} \\right) = \\frac{1}{\\pi}\\sum_{i=1}^n \\gamma_{i1} - \\frac{1}{1-\\pi}\\sum_{i=1}^n \\gamma_{i2} = 0\n    $$\n    Let $N_1 = \\sum_{i=1}^n \\gamma_{i1}$ and $N_2 = \\sum_{i=1}^n \\gamma_{i2}$. Note that $N_1 + N_2 = \\sum_i(\\gamma_{i1}+\\gamma_{i2}) = \\sum_i 1 = n$.\n    $$\n    \\frac{N_1}{\\pi} = \\frac{N_2}{1-\\pi} \\implies N_1(1-\\pi) = N_2\\pi \\implies N_1 = (N_1+N_2)\\pi = n\\pi\n    $$\n    Solving for $\\pi$ gives the updated estimate $\\pi^{\\text{new}}$:\n    $$\n    \\pi^{\\text{new}} = \\frac{N_1}{n} = \\frac{1}{n}\\sum_{i=1}^n \\gamma_{i1}\n    $$\n\n2.  **Update for $\\mu_1$:**\n    We maximize the terms in $Q$ that depend on $\\mu_1$: $\\sum_{i=1}^n \\gamma_{i1} \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)$.\n    $$\n    \\ln\\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) = -\\frac{1}{2}\\ln(2\\pi\\sigma_1^2) - \\frac{(x_i - \\mu_1)^2}{2\\sigma_1^2}\n    $$\n    Taking the derivative with respect to $\\mu_1$ and setting it to $0$:\n    $$\n    \\frac{\\partial Q}{\\partial \\mu_1} = \\frac{\\partial}{\\partial \\mu_1} \\sum_{i=1}^n \\gamma_{i1} \\left( -\\frac{(x_i - \\mu_1)^2}{2\\sigma_1^2} \\right) = \\sum_{i=1}^n \\gamma_{i1} \\left( \\frac{x_i - \\mu_1}{\\sigma_1^2} \\right) = 0\n    $$\n    Since $\\sigma_1^2 > 0$, we have $\\sum_{i=1}^n \\gamma_{i1} (x_i - \\mu_1) = 0$, which implies:\n    $$\n    \\sum_{i=1}^n \\gamma_{i1}x_i = \\mu_1 \\sum_{i=1}^n \\gamma_{i1}\n    $$\n    Solving for $\\mu_1$ gives the updated estimate $\\mu_1^{\\text{new}}$:\n    $$\n    \\mu_1^{\\text{new}} = \\frac{\\sum_{i=1}^n \\gamma_{i1} x_i}{\\sum_{i=1}^n \\gamma_{i1}}\n    $$\n\n3.  **Update for $\\mu_2$:**\n    The derivation is symmetric to that for $\\mu_1$. We maximize the terms in $Q$ dependent on $\\mu_2$: $\\sum_{i=1}^n \\gamma_{i2} \\ln\\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)$.\n    $$\n    \\frac{\\partial Q}{\\partial \\mu_2} = \\frac{\\partial}{\\partial \\mu_2} \\sum_{i=1}^n \\gamma_{i2} \\left( -\\frac{(x_i - \\mu_2)^2}{2\\sigma_2^2} \\right) = \\sum_{i=1}^n \\gamma_{i2} \\left( \\frac{x_i - \\mu_2}{\\sigma_2^2} \\right) = 0\n    $$\n    This implies $\\sum_{i=1}^n \\gamma_{i2} (x_i - \\mu_2) = 0$, leading to:\n    $$\n    \\mu_2^{\\text{new}} = \\frac{\\sum_{i=1}^n \\gamma_{i2} x_i}{\\sum_{i=1}^n \\gamma_{i2}}\n    $$\n    Substituting $\\gamma_{i2} = 1 - \\gamma_{i1}$, we obtain the expression in terms of $\\gamma_{i1}$:\n    $$\n    \\mu_2^{\\text{new}} = \\frac{\\sum_{i=1}^n (1 - \\gamma_{i1}) x_i}{\\sum_{i=1}^n (1 - \\gamma_{i1})}\n    $$\n\nThese expressions for $\\gamma_{i1}$, $\\pi^{\\text{new}}$, $\\mu_1^{\\text{new}}$, and $\\mu_2^{\\text{new}}$ constitute one full iteration of the EM algorithm. The algorithm proceeds by calculating the responsibilities in the E-step using the current parameters, and then using these responsibilities to compute updated parameters in the M-step. This process is repeated until convergence.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2)}{\\pi \\, \\mathcal{N}(x_i \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) \\, \\mathcal{N}(x_i \\mid \\mu_2, \\sigma_2^2)} & \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{i1} & \\frac{\\sum_{i=1}^{n} \\gamma_{i1} x_i}{\\sum_{i=1}^{n} \\gamma_{i1}} & \\frac{\\sum_{i=1}^{n} (1-\\gamma_{i1}) x_i}{\\sum_{i=1}^{n} (1-\\gamma_{i1})} \\end{pmatrix}}\n$$"
        }
    ]
}