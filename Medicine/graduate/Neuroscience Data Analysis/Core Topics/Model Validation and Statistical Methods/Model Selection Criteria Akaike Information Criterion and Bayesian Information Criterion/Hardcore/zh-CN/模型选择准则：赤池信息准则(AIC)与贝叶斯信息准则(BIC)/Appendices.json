{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的检验方法就是动手计算。这个练习提供了一个在广义线性模型框架下的经典模型比较场景。通过为两个嵌套的逻辑回归模型计算赤池信息准则（AIC）和贝叶斯信息准则（BIC），你将亲身体验这两个准则如何平衡模型拟合度与复杂性，并观察到它们因惩罚项不同而可能导出不同结论的情况。",
            "id": "4595206",
            "problem": "一项在医院进行的横断面研究调查了出现急性呼吸道症状的成年患者当前流感感染的风险因素。二元结果是感染状态，对于患者 $i$ 编码为 $Y_{i} \\in \\{0,1\\}$，使用logit连接函数的逻辑回归广义线性模型进行分析。在包含 $n = 1000$ 名独立患者的同一数据集上，通过最大似然法拟合了两个嵌套模型。\n\n模型 $\\mathcal{M}_{1}$（完整模型）包含一个截距项和 $5$ 个协变量项（总参数数量 $k_{1} = 6$）。模型 $\\mathcal{M}_{0}$（简化模型）包含一个截距项和 $3$ 个协变量项（总参数数量 $k_{0} = 4$）。模型 $\\mathcal{M}_{1}$ 的最大化对数似然为 $\\ell_{1} = -120$，模型 $\\mathcal{M}_{0}$ 的最大化对数似然为 $\\ell_{0} = -125$。使用广义线性模型中用于模型选择的赤池信息准则（AIC）和贝叶斯信息准则（BIC）的定义，计算每个模型的AIC和BIC，并分别根据每个准则确定哪个模型更优。\n\n将你的 $(\\mathrm{AIC}_{1}, \\mathrm{BIC}_{1}, \\mathrm{AIC}_{0}, \\mathrm{BIC}_{0})$ 数值结果以单行矩阵的形式报告，四舍五入到四位有效数字。不需要单位。偏好决策无需包含在报告的矩阵中。",
            "solution": "该问题要求计算两个嵌套逻辑回归模型 $\\mathcal{M}_{1}$（完整模型）和 $\\mathcal{M}_{0}$（简化模型）的赤池信息准则（AIC）和贝叶斯信息准则（BIC），并确定每个准则偏好哪个模型。\n\n问题提供了以下信息：\n样本量为 $n = 1000$ 名患者。\n对于完整模型 $\\mathcal{M}_{1}$：\n参数数量为 $k_{1} = 6$。\n最大化对数似然为 $\\ell_{1} = -120$。\n\n对于简化模型 $\\mathcal{M}_{0}$：\n参数数量为 $k_{0} = 4$。\n最大化对数似然为 $\\ell_{0} = -125$。\n\n赤池信息准则（AIC）定义为：\n$$ \\mathrm{AIC} = -2\\ell + 2k $$\n其中 $k$ 是模型中估计的参数数量，$\\ell$ 是模型对数似然函数的最大化值。\n\n贝叶斯信息准则（BIC）定义为：\n$$ \\mathrm{BIC} = -2\\ell + k \\ln(n) $$\n其中 $n$ 是观测数量，即样本量。\n\n对于这两个准则，通常偏好值较低的模型，因为它表示在模型拟合度（较高的 $\\ell$）和模型简约性（较低的 $k$）之间有更好的平衡。\n\n首先，我们计算完整模型 $\\mathcal{M}_{1}$ 的AIC和BIC。\n使用给定值 $k_{1} = 6$、$\\ell_{1} = -120$ 和 $n = 1000$：\n$$ \\mathrm{AIC}_{1} = -2\\ell_{1} + 2k_{1} = -2(-120) + 2(6) = 240 + 12 = 252 $$\n$$ \\mathrm{BIC}_{1} = -2\\ell_{1} + k_{1} \\ln(n) = -2(-120) + 6 \\ln(1000) = 240 + 6 \\ln(1000) $$\n为了计算 $\\mathrm{BIC}_{1}$ 的数值，我们使用 $\\ln(1000) \\approx 6.907755$：\n$$ \\mathrm{BIC}_{1} \\approx 240 + 6(6.907755) \\approx 240 + 41.44653 = 281.44653 $$\n\n接下来，我们计算简化模型 $\\mathcal{M}_{0}$ 的AIC和BIC。\n使用给定值 $k_{0} = 4$、$\\ell_{0} = -125$ 和 $n = 1000$：\n$$ \\mathrm{AIC}_{0} = -2\\ell_{0} + 2k_{0} = -2(-125) + 2(4) = 250 + 8 = 258 $$\n$$ \\mathrm{BIC}_{0} = -2\\ell_{0} + k_{0} \\ln(n) = -2(-125) + 4 \\ln(1000) = 250 + 4 \\ln(1000) $$\n使用 $\\ln(1000) \\approx 6.907755$：\n$$ \\mathrm{BIC}_{0} \\approx 250 + 4(6.907755) \\approx 250 + 27.63102 = 277.63102 $$\n\n现在，我们通过比较计算出的值，根据每个准则确定偏好的模型。\n对于AIC：\n$\\mathrm{AIC}_{1} = 252$ 且 $\\mathrm{AIC}_{0} = 258$。\n因为 $\\mathrm{AIC}_{1}  \\mathrm{AIC}_{0}$ ($252  258$)，所以赤池信息准则偏好完整模型 $\\mathcal{M}_{1}$。\n\n对于BIC：\n$\\mathrm{BIC}_{1} \\approx 281.4465$ 且 $\\mathrm{BIC}_{0} \\approx 277.6310$。\n因为 $\\mathrm{BIC}_{0}  \\mathrm{BIC}_{1}$ ($277.6310  281.4465$)，所以贝叶斯信息准则偏好简化模型 $\\mathcal{M}_{0}$。\n\n在比较这两个准则时，偏好上的分歧是一个已知的特性。当 $\\ln(n) > 2$ 时，BIC中对模型复杂度的惩罚项 $k \\ln(n)$ 大于AIC中的惩罚项 $2k$，这对于样本量 $n > e^2 \\approx 7.4$ 是成立的。在本例中，当 $n = 1000$ 时，BIC对模型 $\\mathcal{M}_{1}$ 中额外的两个参数施加了更强的惩罚，从而导致选择了更简约的模型 $\\mathcal{M}_{0}$。\n\n问题要求将 $(\\mathrm{AIC}_{1}, \\mathrm{BIC}_{1}, \\mathrm{AIC}_{0}, \\mathrm{BIC}_{0})$ 的数值结果四舍五入到四位有效数字。\n$\\mathrm{AIC}_{1} = 252$，保留四位有效数字为 $252.0$。\n$\\mathrm{BIC}_{1} \\approx 281.44653$，四舍五入为 $281.4$。\n$\\mathrm{AIC}_{0} = 258$，保留四位有效数字为 $258.0$。\n$\\mathrm{BIC}_{0} \\approx 277.63102$，四舍五入为 $277.6$。\n因此，最终的数值向量是 $(252.0, 281.4, 258.0, 277.6)$。",
            "answer": "$$ \\boxed{\\begin{pmatrix} 252.0  281.4  258.0  277.6 \\end{pmatrix}} $$"
        },
        {
            "introduction": "仅仅会计算AIC和BIC值是不够的，理解其背后对复杂度的“惩罚”逻辑至关重要。这个练习将引导你从第一性原理出发，推导出一个更复杂的模型需要比简单模型好多少（以对数似然的提升量来衡量），才能在AIC和BIC评估中胜出。这个推导过程将清晰地揭示AIC和BIC惩罚项的根本区别，特别是BIC对样本量$n$的依赖性。",
            "id": "3919103",
            "problem": "一个合成生物学团队正在对诱导条件下单个启动子的单细胞信使核糖核酸（$\\mathrm{mRNA}$）计数进行建模。他们针对 $n$ 个独立细胞，比较了两个关于转录爆发的嵌套随机模型：一个参数维度为 $k_{S}$ 的较简单模型 $\\mathcal{M}_{S}$，以及一个参数维度为 $k_{C} = k_{S} + 1$ 的较复杂模型 $\\mathcal{M}_{C}$，后者增加了一个具有生物学可解释性的参数，用于捕捉依赖于诱导的爆发规模。令 $\\ln \\hat{L}_{S}$ 和 $\\ln \\hat{L}_{C}$ 分别表示在两个模型下，于其各自的最大似然估计（MLEs）处计算得到的最大化对数似然。将复杂模型相对于简单模型的对数似然改进量定义为 $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$，对于通过最大似然法拟合的嵌套模型，有 $\\Delta \\ln \\hat{L} \\geq 0$。\n\n从赤池信息准则（AIC）和贝叶斯信息准则（BIC）作为结合了拟合优度项和复杂度惩罚项的惩罚性准则的标准定义出发，并仅使用上述假设，推导在给定样本量 $n$ 的情况下，根据每种准则，复杂模型 $\\mathcal{M}_{C}$ 优于简单模型 $\\mathcal{M}_{S}$ 所需的 $\\Delta \\ln \\hat{L}$ 的最小阈值。将最终答案表示为两个关于 $n$ 的精确解析表达式，以单行矩阵 $[\\Delta_{\\mathrm{AIC}} \\quad \\Delta_{\\mathrm{BIC}}]$ 的形式呈现，其中 $\\Delta_{\\mathrm{AIC}}$ 和 $\\Delta_{\\mathrm{BIC}}$ 分别是使得复杂模型在赤池信息准则和贝叶斯信息准则下更受青睐的 $\\Delta \\ln \\hat{L}$ 的最小值。最终表达式无需四舍五入，也不应包含任何单位。",
            "solution": "目标是确定根据赤池信息准则（AIC）和贝叶斯信息准则（BIC），较复杂模型 $\\mathcal{M}_{C}$ 优于较简单模型 $\\mathcal{M}_{S}$ 所需的对数似然改进量 $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$ 的最小阈值。\n\n对于像 AIC 和 BIC 这样的模型选择准则，准则值较低的模型更优。因此，如果 $\\text{AIC}(\\mathcal{M}_{C})  \\text{AIC}(\\mathcal{M}_{S})$，则 $\\mathcal{M}_{C}$ 优于 $\\mathcal{M}_{S}$（对于 AIC）；如果 $\\text{BIC}(\\mathcal{M}_{C})  \\text{BIC}(\\mathcal{M}_{S})$，则 $\\mathcal{M}_{C}$ 优于 $\\mathcal{M}_{S}$（对于 BIC）。阈值对应于等式成立的点。\n\n令 $k$ 为模型中估计参数的数量，$\\hat{L}$ 为模型的似然函数的最大化值， $n$ 为数据点的数量（在本例中为独立细胞的数量）。\n\n**1. 赤池信息准则（AIC）的推导**\n\nAIC 的标准定义是：\n$$ \\text{AIC} = -2\\ln \\hat{L} + 2k $$\n\n对于我们的两个模型 $\\mathcal{M}_{S}$ 和 $\\mathcal{M}_{C}$，AIC 值分别为：\n$$ \\text{AIC}_{S} = -2\\ln \\hat{L}_{S} + 2k_{S} $$\n$$ \\text{AIC}_{C} = -2\\ln \\hat{L}_{C} + 2k_{C} $$\n\n当 $\\text{AIC}_{C}  \\text{AIC}_{S}$ 时，复杂模型 $\\mathcal{M}_{C}$ 优于简单模型 $\\mathcal{M}_{S}$。我们可以将此不等式写为：\n$$ -2\\ln \\hat{L}_{C} + 2k_{C}  -2\\ln \\hat{L}_{S} + 2k_{S} $$\n\n为了找到关于对数似然改进量的条件，我们重排不等式，将似然项和参数数量项分组：\n$$ 2\\ln \\hat{L}_{C} - 2\\ln \\hat{L}_{S} > 2k_{C} - 2k_{S} $$\n$$ 2(\\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}) > 2(k_{C} - k_{S}) $$\n\n根据问题中的定义 $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$，我们有：\n$$ 2\\Delta \\ln \\hat{L} > 2(k_{C} - k_{S}) $$\n$$ \\Delta \\ln \\hat{L} > k_{C} - k_{S} $$\n\n问题陈述，复杂模型的参数维度比简单模型多一，即 $k_{C} = k_{S} + 1$，这意味着 $k_{C} - k_{S} = 1$。将此代入不等式，得到：\n$$ \\Delta \\ln \\hat{L} > 1 $$\n\n偏好 $\\mathcal{M}_{C}$ 所需的 $\\Delta \\ln \\hat{L}$ 的最小值是此不等式的阈值。因此，AIC 的最小阈值为：\n$$ \\Delta_{\\text{AIC}} = 1 $$\n\n**2. 贝叶斯信息准则（BIC）的推导**\n\nBIC 的标准定义是：\n$$ \\text{BIC} = -2\\ln \\hat{L} + k\\ln(n) $$\n其中 $n$ 是样本量。\n\n对于我们的两个模型，BIC 值分别为：\n$$ \\text{BIC}_{S} = -2\\ln \\hat{L}_{S} + k_{S}\\ln(n) $$\n$$ \\text{BIC}_{C} = -2\\ln \\hat{L}_{C} + k_{C}\\ln(n) $$\n\n当 $\\text{BIC}_{C}  \\text{BIC}_{S}$ 时，复杂模型 $\\mathcal{M}_{C}$ 优于简单模型 $\\mathcal{M}_{S}$：\n$$ -2\\ln \\hat{L}_{C} + k_{C}\\ln(n)  -2\\ln \\hat{L}_{S} + k_{S}\\ln(n) $$\n\n我们再次重排以分离出对数似然改进量：\n$$ 2\\ln \\hat{L}_{C} - 2\\ln \\hat{L}_{S} > k_{C}\\ln(n) - k_{S}\\ln(n) $$\n$$ 2(\\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}) > (k_{C} - k_{S})\\ln(n) $$\n\n代入 $\\Delta \\ln \\hat{L} = \\ln \\hat{L}_{C} - \\ln \\hat{L}_{S}$：\n$$ 2\\Delta \\ln \\hat{L} > (k_{C} - k_{S})\\ln(n) $$\n\n同样，我们使用 $k_{C} - k_{S} = 1$ 这一事实：\n$$ 2\\Delta \\ln \\hat{L} > \\ln(n) $$\n$$ \\Delta \\ln \\hat{L} > \\frac{1}{2}\\ln(n) $$\n\n在 BIC 准则下，偏好 $\\mathcal{M}_{C}$ 所需的 $\\Delta \\ln \\hat{L}$ 的最小值是此条件的边界值。因此，BIC 的最小阈值为：\n$$ \\Delta_{\\text{BIC}} = \\frac{1}{2}\\ln(n) $$\n\n最终答案由推导出的两个阈值 $\\Delta_{\\text{AIC}}$ 和 $\\Delta_{\\text{BIC}}$ 组成，以行矩阵的形式呈现。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  \\frac{1}{2}\\ln(n) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "理论和推导最终要服务于实践。最后一个练习是一个综合性的计算任务，要求你将所学知识应用于模拟的真实数据拟合场景。你将为给定的数据集拟合不同复杂度的多项式模型，并运用AIC和BIC来做出最终的模型选择决策。通过编写代码完成这个过程，你将把模型拟合、残差计算和信息准则应用融会贯通，巩固对整个模型选择流程的理解。",
            "id": "2408012",
            "problem": "给定几个由配对值 $(x_i,y_i)$ 组成的独立数据集。假设 $y_i$ 中的观测误差是独立同分布的高斯随机变量，其均值为零，方差未知。对于每个数据集，考虑两个关于 $y$ 作为 $x$ 函数的竞争模型：一个 $2$ 次的二次多项式和一个 $3$ 次的三次多项式。对于每个模型，在所述高斯误差假设下，通过普通最小二乘法拟合系数，然后使用赤池信息量准则 (AIC) 和贝叶斯信息量准则 (BIC) 进行模型选择。自由参数的数量 $k$ 等于多项式次数加一。对于每个数据集，确定 AIC 和 BIC 分别偏好哪个模型，并报告这些决策。\n\n数据集（每个数据集均以一个有序的 $x$ 值列表和一个相应等长的有序 $y$ 值列表的形式呈现）：\n\n- 测试用例 $1$：\n  - $x$: $[-3,-2,-1,0,1,2,3]$\n  - $y$: $[9.45,4.90,2.30,0.95,1.13,3.09,6.17]$\n\n- 测试用例 $2$：\n  - $x$: $[-2,-1,0,1,2,3,4]$\n  - $y$: $[3.35,1.73,0.52,-0.03,0.94,3.75,9.31]$\n\n- 测试用例 $3$：\n  - $x$: $[-1,-0.5,0,1,2]$\n  - $y$: $[1.53,1.605,2.01,3.49,5.96]$\n\n- 测试用例 $4$：\n  - $x$: $[-3,-2,-1,0,1,2,3,4]$\n  - $y$: $[2.035,1.48,1.295,0.98,0.915,0.77,0.985,1.35]$\n\n您的程序必须为每个数据集拟合两个候选模型，并基于拟合残差下的高斯最大似然，计算赤池信息量准则 (AIC) 和贝叶斯信息量准则 (BIC)。对于每个准则，分别选择达到较小值的模型。用整数 $2$ 表示二次模型，用整数 $3$ 表示三次模型。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个项目对应于上述的一个测试用例，并且本身必须是一个双元素列表 $[a,b]$，其中 $a$ 是 AIC 偏好的模型，$b$ 是 BIC 偏好的模型。例如，对于四个测试用例，一个有效的输出是 $[[2,2],[3,3],[2,2],[2,2]]$。\n\n您的答案中不需要物理单位。不涉及角度。请用整数表示所有最终决策。",
            "solution": "这是一个模型选择问题。我们需要决定两个多项式模型（二次或三次）中哪一个能更好地描述给定的数据集。决策将使用赤池信息量准则 (AIC) 和贝叶斯信息量准则 (BIC) 作出。这两个准则都实现了简约原则，即在拟合优度和模型复杂性之间取得平衡。\n\n一个次数为 $d$ 的多项式模型是以下形式的函数：\n$$ f(x; \\mathbf{\\beta}) = \\sum_{j=0}^{d} \\beta_j x^j $$\n其中 $\\mathbf{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_d)$ 是系数向量。\n\n对于一个由 $n$ 个点 $(x_i, y_i)$ 组成的数据集，第一步是为每个模型确定最优系数 $\\hat{\\mathbf{\\beta}}$。这通过普通最小二乘法 (OLS) 实现，该方法最小化残差平方和 (RSS)：\n$$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - f(x_i; \\hat{\\mathbf{\\beta}}))^2 $$\n较低的 RSS 表示对数据的拟合更好。\n\n然而，一个更复杂的模型（更高次数 $d$）几乎总能获得更低的 RSS。为了防止过拟合，我们使用对模型复杂性进行惩罚的信息准则。在独立同分布 (i.i.d.) 高斯误差的假设下，AIC 和 BIC 由以下公式给出：\n$$ \\text{AIC} = n \\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 2k $$\n$$ \\text{BIC} = n \\ln\\left(\\frac{\\text{RSS}}{n}\\right) + k \\ln(n) $$\n这里，$n$ 是数据点的数量，$k$ 是模型中的自由参数数量。问题明确指出 $k$ 等于多项式次数加一 ($k=d+1$)。\n- 对于二次模型 ($d=2$)，我们有 $k=2+1=3$。\n- 对于三次模型 ($d=3$)，我们有 $k=3+1=4$。\n\n项 $n \\ln(\\text{RSS}/n)$ 与模型的最大似然相关，代表拟合优度。项 $2k$ (对于 AIC) 和 $k \\ln(n)$ (对于 BIC) 是对模型复杂性的惩罚项。由于对于 $n \\ge 8$，有 $\\ln(n) > 2$，因此对于这种规模的数据集，BIC 对复杂性的惩罚比 AIC 更强。对于每个准则，值较小的模型是更优选的模型。\n\n对每个数据集的步骤如下：\n1.  对于二次模型 ($d=2, k=3$)，计算最佳拟合系数和相应的 $\\text{RSS}_2$。计算 $\\text{AIC}_2$ 和 $\\text{BIC}_2$。\n2.  对于三次模型 ($d=3, k=4$)，计算最佳拟合系数和相应的 $\\text{RSS}_3$。计算 $\\text{AIC}_3$ 和 $\\text{BIC}_3$。\n3.  比较 $\\text{AIC}_2$ 和 $\\text{AIC}_3$。值较小的模型是 AIC 偏好的模型。\n4.  比较 $\\text{BIC}_2$ 和 $\\text{BIC}_3$。值较小的模型是 BIC 偏好的模型。\n\n该程序将系统地应用于所有提供的测试用例，以得出最终解。计算将使用能够进行稳健多项式拟合的数值库来执行。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for the given datasets.\n\n    For each dataset, it fits a quadratic (degree 2) and a cubic (degree 3)\n    polynomial model. It then calculates the Akaike Information Criterion (AIC)\n    and Bayesian Information Criterion (BIC) for both models and determines\n    which model is preferred by each criterion.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"x\": [-3, -2, -1, 0, 1, 2, 3],\n            \"y\": [9.45, 4.90, 2.30, 0.95, 1.13, 3.09, 6.17]\n        },\n        {\n            \"x\": [-2, -1, 0, 1, 2, 3, 4],\n            \"y\": [3.35, 1.73, 0.52, -0.03, 0.94, 3.75, 9.31]\n        },\n        {\n            \"x\": [-1, -0.5, 0, 1, 2],\n            \"y\": [1.53, 1.605, 2.01, 3.49, 5.96]\n        },\n        {\n            \"x\": [-3, -2, -1, 0, 1, 2, 3, 4],\n            \"y\": [2.035, 1.48, 1.295, 0.98, 0.915, 0.77, 0.985, 1.35]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x_data = np.array(case[\"x\"])\n        y_data = np.array(case[\"y\"])\n        n = len(x_data)\n\n        # --- Model 1: Quadratic Polynomial ---\n        deg2 = 2\n        k2 = deg2 + 1  # Number of free parameters as per problem statement\n        \n        # polyfit with full=True returns extra information, including RSS.\n        # The second returned element is an array containing the RSS.\n        _, residuals2_info, _, _, _ = np.polyfit(x_data, y_data, deg2, full=True)\n        # In case of a perfect fit for n = deg, residuals_info can be empty.\n        rss2 = residuals2_info[0] if residuals2_info.size > 0 else 1e-16\n\n        # Calculate AIC and BIC\n        # Using the formulation for least squares with Gaussian errors.\n        aic2 = n * np.log(rss2 / n) + 2 * k2\n        bic2 = n * np.log(rss2 / n) + k2 * np.log(n)\n\n        # --- Model 2: Cubic Polynomial ---\n        deg3 = 3\n        k3 = deg3 + 1  # Number of free parameters\n        \n        _, residuals3_info, _, _, _ = np.polyfit(x_data, y_data, deg3, full=True)\n        rss3 = residuals3_info[0] if residuals3_info.size > 0 else 1e-16\n        \n        # Calculate AIC and BIC\n        aic3 = n * np.log(rss3 / n) + 2 * k3\n        bic3 = n * np.log(rss3 / n) + k3 * np.log(n)\n\n        # --- Model Selection ---\n        # The model with the lower criterion value is preferred.\n        # Model 2 = Quadratic, Model 3 = Cubic\n        aic_choice = 3 if aic3  aic2 else 2\n        bic_choice = 3 if bic3  bic2 else 2\n\n        results.append([aic_choice, bic_choice])\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, e.g., [[2,2],[3,3],...].\n    # map(str, results) will convert each inner list to its string representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}