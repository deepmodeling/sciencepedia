{
    "hands_on_practices": [
        {
            "introduction": "第一个动手实践旨在将似然比检验的一般理论与经典的统计方法联系起来。通过从第一性原理推导熟悉的双样本$t$检验，你将更深刻地理解这个强大的框架如何统一不同的假设检验，并为其应用提供了根本性的论证。这个练习将巩固你对似然比检验核心逻辑的理解。",
            "id": "4174050",
            "problem": "一个认知电生理学实验室正在分析从单个神经元在两种任务条件下记录的试次平均脉冲计数，这两种条件分别记为条件 A 和条件 B。在每个试次中，脉冲计数在一个固定的 $100$ 毫秒分析窗口内累积。对于条件 A，有 $n_{1}$ 个试次，其计数为 $\\{x_{i}\\}_{i=1}^{n_{1}}$。对于条件 B，有 $n_{2}$ 个试次，其计数为 $\\{y_{j}\\}_{j=1}^{n_{2}}$。根据中心极限定理（CLT）的标准论证，条件内试次间的脉冲计数变异性可以很好地用正态分布来近似。假设两种条件下的样本是独立的，具有特定于条件的均值和一个共同的未知方差。形式上，假设 $x_{i} \\sim \\mathcal{N}(\\mu_{1},\\sigma^{2})$ 对 $i=1,\\dots,n_{1}$ 独立同分布，且 $y_{j} \\sim \\mathcal{N}(\\mu_{2},\\sigma^{2})$ 对 $j=1,\\dots,n_{2}$ 独立同分布，其中 $\\sigma^{2}$ 在两种条件下相同但未知。\n\n你将构建在等方差正态模型下，针对原假设 $H_{0}:\\mu_{1}=\\mu_{2}$ 和双侧备择假设 $H_{1}:\\mu_{1}\\neq\\mu_{2}$ 的似然比（LR）检验。从第一性原理开始：写出联合似然函数，将 LR 统计量定义为在 $H_{0}$ 下最大化的似然与在 $H_{1}$ 下最大化的似然之比，并获得以可观测样本量表示的 LR 的闭式解。然后，通过将 LR 统计量代数地表示为 $t$-统计量和样本大小的显式函数，将 LR 检验与经典的合并双样本 $t$-统计量联系起来，从而显示出使得 LR 检验在 $|t|$ 较大时拒绝原假设的单调关系。\n\n令 $\\bar{x}=\\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}}x_{i}$ 和 $\\bar{y}=\\frac{1}{n_{2}}\\sum_{j=1}^{n_{2}}y_{j}$ 表示样本均值，并令 $s_{1}^{2}=\\frac{1}{n_{1}-1}\\sum_{i=1}^{n_{1}}(x_{i}-\\bar{x})^{2}$ 和 $s_{2}^{2}=\\frac{1}{n_{2}-1}\\sum_{j=1}^{n_{2}}(y_{j}-\\bar{y})^{2}$ 表示无偏样本方差。定义合并方差 $s_{p}^{2}=\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}$ 和双样本合并 $t$-统计量 $t=\\frac{\\bar{x}-\\bar{y}}{s_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$。\n\n你的最终答案必须是关于 $-2\\ln\\Lambda$ 的一个单一的、仅用 $t$、$n_{1}$ 和 $n_{2}$ 表示的闭式解析表达式。不需要进行数值舍入，也不涉及任何单位。将最终答案精确地表示为 $t$、$n_{1}$ 和 $n_{2}$ 的函数。",
            "solution": "用户希望为来自具有共同未知方差的独立正态总体的两个均值相等性构建似然比（LR）检验，然后将 LR 检验统计量用经典的双样本合并 $t$-统计量表示。\n\n### 步骤 1：问题验证\n\n**1.1. 提取已知条件**\n- 数据：两组独立样本，来自条件 A 的 $\\{x_{i}\\}_{i=1}^{n_{1}}$ 和来自条件 B 的 $\\{y_{j}\\}_{j=1}^{n_{2}}$。\n- 模型假设：$x_{i} \\sim \\mathcal{N}(\\mu_{1},\\sigma^{2})$ 对 $i=1,\\dots,n_{1}$ 独立同分布，且 $y_{j} \\sim \\mathcal{N}(\\mu_{2},\\sigma^{2})$ 对 $j=1,\\dots,n_{2}$ 独立同分布。样本是独立的，方差 $\\sigma^{2}$ 是共同的但未知。\n- 假设：原假设 $H_{0}:\\mu_{1}=\\mu_{2}$ 对备择假设 $H_{1}:\\mu_{1}\\neq\\mu_{2}$。\n- 定义：\n    - 样本均值：$\\bar{x}=\\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}}x_{i}$ 和 $\\bar{y}=\\frac{1}{n_{2}}\\sum_{j=1}^{n_{2}}y_{j}$。\n    - 无偏样本方差：$s_{1}^{2}=\\frac{1}{n_{1}-1}\\sum_{i=1}^{n_{1}}(x_{i}-\\bar{x})^{2}$ 和 $s_{2}^{2}=\\frac{1}{n_{2}-1}\\sum_{j=1}^{n_{2}}(y_{j}-\\bar{y})^{2}$。\n    - 合并方差：$s_{p}^{2}=\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}$。\n    - 双样本合并 $t$-统计量：$t=\\frac{\\bar{x}-\\bar{y}}{s_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$。\n\n**1.2. 使用提取的已知条件进行验证**\n- **科学性：**该问题是统计学中假设检验的一个典型例子，其坚实基础是似然理论和正态分布模型。比较脉冲计数的场景是神经科学数据分析中的一个现实应用。\n- **适定性：**问题陈述清晰，提供了所有必要的定义和一个明确的目标：推导一个闭式表达式。存在唯一解。\n- **客观性：**问题是用精确的数学语言表述的，没有主观性。\n\n**1.3. 结论与行动**\n问题有效。将继续进行求解过程。\n\n### 步骤 2：似然比检验的推导\n\n**2.1. 似然函数**\n令总观测数为 $N = n_{1}+n_{2}$。给定数据，参数 $\\mu_{1}, \\mu_{2}, \\sigma^{2}$ 的联合似然函数是各单个概率密度函数的乘积：\n$$L(\\mu_{1}, \\mu_{2}, \\sigma^{2}) = \\prod_{i=1}^{n_{1}} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(x_{i} - \\mu_{1})^{2}}{2\\sigma^{2}}\\right) \\prod_{j=1}^{n_{2}} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(y_{j} - \\mu_{2})^{2}}{2\\sigma^{2}}\\right)$$\n简化为：\n$$L(\\mu_{1}, \\mu_{2}, \\sigma^{2}) = (2\\pi\\sigma^{2})^{-N/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\left[ \\sum_{i=1}^{n_{1}} (x_{i} - \\mu_{1})^{2} + \\sum_{j=1}^{n_{2}} (y_{j} - \\mu_{2})^{2} \\right]\\right)$$\n对应的对数似然为：\n$$\\ln L = -\\frac{N}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}} \\left[ \\sum_{i=1}^{n_{1}} (x_{i} - \\mu_{1})^{2} + \\sum_{j=1}^{n_{2}} (y_{j} - \\mu_{2})^{2} \\right]$$\n\n**2.2. 在备择假设（$H_{1}$）下的最大化**\n在 $H_{1}: \\mu_{1} \\neq \\mu_{2}$ 下，参数 $\\mu_{1}, \\mu_{2}, \\sigma^{2}$ 不受约束。我们通过将 $\\ln L$ 的偏导数设为零来找到最大似然估计（MLEs）。\n- 对于 $\\mu_{1}$：$\\frac{\\partial \\ln L}{\\partial \\mu_{1}} = \\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n_{1}}(x_{i} - \\mu_{1}) = 0 \\implies \\hat{\\mu}_{1} = \\bar{x}$。\n- 对于 $\\mu_{2}$：$\\frac{\\partial \\ln L}{\\partial \\mu_{2}} = \\frac{1}{\\sigma^{2}}\\sum_{j=1}^{n_{2}}(y_{j} - \\mu_{2}) = 0 \\implies \\hat{\\mu}_{2} = \\bar{y}$。\n- 对于 $\\sigma^{2}$：$\\frac{\\partial \\ln L}{\\partial \\sigma^{2}} = -\\frac{N}{2\\sigma^{2}} + \\frac{1}{2(\\sigma^{2})^{2}} \\left[ \\sum(x_{i} - \\hat{\\mu}_{1})^{2} + \\sum(y_{j} - \\hat{\\mu}_{2})^{2} \\right] = 0$。\n这给出了在 $H_{1}$ 下 $\\sigma^{2}$ 的 MLE：\n$$\\hat{\\sigma}^{2}_{H_{1}} = \\frac{1}{N} \\left[ \\sum_{i=1}^{n_{1}}(x_{i} - \\bar{x})^{2} + \\sum_{j=1}^{n_{2}}(y_{j} - \\bar{y})^{2} \\right]$$\n将在 $H_1$ 下的最大化似然，记为 $L_{H_{1}}^{\\max}$，是通过将这些 MLE 代回似然函数得到的：\n$$L_{H_{1}}^{\\max} = (2\\pi\\hat{\\sigma}^{2}_{H_{1}})^{-N/2} \\exp\\left(-\\frac{N\\hat{\\sigma}^{2}_{H_{1}}}{2\\hat{\\sigma}^{2}_{H_{1}}}\\right) = (2\\pi\\hat{\\sigma}^{2}_{H_{1}})^{-N/2} \\exp\\left(-\\frac{N}{2}\\right)$$\n\n**2.3. 在原假设（$H_{0}$）下的最大化**\n在 $H_{0}: \\mu_{1} = \\mu_{2} = \\mu$ 下，模型是受约束的。我们找到 $\\mu$ 和 $\\sigma^{2}$ 的 MLE。\n- 对于 $\\mu$：我们在该约束下最大化 $\\ln L$。关于 $\\mu$ 的导数是：\n$\\frac{\\partial \\ln L}{\\partial \\mu} = \\frac{1}{\\sigma^{2}}\\left[\\sum(x_{i} - \\mu) + \\sum(y_{j} - \\mu)\\right] = 0 \\implies n_{1}\\bar{x} - n_{1}\\mu + n_{2}\\bar{y} - n_{2}\\mu = 0$。\n这给出了共同均值 $\\mu$ 的 MLE：\n$$\\hat{\\mu}_{H_{0}} = \\frac{n_{1}\\bar{x} + n_{2}\\bar{y}}{n_{1} + n_{2}}$$\n这是所有数据点的总均值。\n- 对于 $\\sigma^{2}$：导数的形式相同，得到：\n$$\\hat{\\sigma}^{2}_{H_{0}} = \\frac{1}{N} \\left[ \\sum_{i=1}^{n_{1}}(x_{i} - \\hat{\\mu}_{H_{0}})^{2} + \\sum_{j=1}^{n_{2}}(y_{j} - \\hat{\\mu}_{H_{0}})^{2} \\right]$$\n在 $H_0$ 下的最大化似然，记为 $L_{H_{0}}^{\\max}$，是：\n$$L_{H_{0}}^{\\max} = (2\\pi\\hat{\\sigma}^{2}_{H_{0}})^{-N/2} \\exp\\left(-\\frac{N\\hat{\\sigma}^{2}_{H_{0}}}{2\\hat{\\sigma}^{2}_{H_{0}}}\\right) = (2\\pi\\hat{\\sigma}^{2}_{H_{0}})^{-N/2} \\exp\\left(-\\frac{N}{2}\\right)$$\n\n**2.4. 似然比统计量**\nLR 统计量 $\\Lambda$ 是最大化似然的比值：\n$$\\Lambda = \\frac{L_{H_{0}}^{\\max}}{L_{H_{1}}^{\\max}} = \\frac{(2\\pi\\hat{\\sigma}^{2}_{H_{0}})^{-N/2} \\exp(-N/2)}{(2\\pi\\hat{\\sigma}^{2}_{H_{1}})^{-N/2} \\exp(-N/2)} = \\left(\\frac{\\hat{\\sigma}^{2}_{H_{1}}}{\\hat{\\sigma}^{2}_{H_{0}}}\\right)^{N/2}$$\n似然比检验（LRT）在 $\\Lambda$ 值较小时拒绝 $H_{0}$。这等价于在 $-2\\ln\\Lambda$ 值较大时拒绝：\n$$-2\\ln\\Lambda = -2 \\ln\\left( \\left(\\frac{\\hat{\\sigma}^{2}_{H_{1}}}{\\hat{\\sigma}^{2}_{H_{0}}}\\right)^{N/2} \\right) = -N \\ln\\left(\\frac{\\hat{\\sigma}^{2}_{H_{1}}}{\\hat{\\sigma}^{2}_{H_{0}}}\\right) = N \\ln\\left(\\frac{\\hat{\\sigma}^{2}_{H_{0}}}{\\hat{\\sigma}^{2}_{H_{1}}}\\right)$$\n\n**2.5. 与 $t$-统计量的联系**\n我们引入方差分析（ANOVA）中熟悉的平方和分解。\n$N\\hat{\\sigma}^{2}_{H_{1}}$ 的分子是残差平方和（$SS_{R}$），即与组均值的离差平方和：\n$$SS_{R} = \\sum_{i=1}^{n_{1}}(x_{i} - \\bar{x})^{2} + \\sum_{j=1}^{n_{2}}(y_{j} - \\bar{y})^{2} = (n_{1}-1)s_{1}^{2} + (n_{2}-1)s_{2}^{2}$$\n根据问题定义，$s_{p}^{2} = \\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2} = \\frac{SS_{R}}{n_{1}+n_{2}-2}$。因此，$SS_{R} = (n_{1}+n_{2}-2)s_{p}^{2}$。\n所以，$\\hat{\\sigma}^{2}_{H_{1}} = \\frac{SS_{R}}{N}$。\n\n$N\\hat{\\sigma}^{2}_{H_{0}}$ 的分子是总平方和（$SS_{T}$），即与总均值 $\\hat{\\mu}_{H_0}$ 的离差平方和：\n$$SS_{T} = \\sum_{i=1}^{n_{1}}(x_{i} - \\hat{\\mu}_{H_{0}})^{2} + \\sum_{j=1}^{n_2}(y_{j} - \\hat{\\mu}_{H_{0}})^{2}$$\n一个标准恒等式表明 $SS_{T} = SS_{R} + SS_{B}$，其中 $SS_{B}$ 是组间平方和：\n$$SS_{B} = n_{1}(\\bar{x}-\\hat{\\mu}_{H_{0}})^{2} + n_{2}(\\bar{y}-\\hat{\\mu}_{H_{0}})^{2} = \\frac{n_{1}n_{2}}{n_{1}+n_{2}}(\\bar{x}-\\bar{y})^{2}$$\n现在我们可以写出方差之比：\n$$\\frac{\\hat{\\sigma}^{2}_{H_{0}}}{\\hat{\\sigma}^{2}_{H_{1}}} = \\frac{SS_{T}/N}{SS_{R}/N} = \\frac{SS_{T}}{SS_{R}} = \\frac{SS_{R}+SS_{B}}{SS_{R}} = 1 + \\frac{SS_{B}}{SS_{R}}$$\nLR 统计量变为：\n$$-2\\ln\\Lambda = N \\ln\\left(1 + \\frac{SS_{B}}{SS_{R}}\\right) = (n_{1}+n_{2}) \\ln\\left(1 + \\frac{SS_{B}}{SS_{R}}\\right)$$\n最后，我们将比率 $\\frac{SS_{B}}{SS_{R}}$ 用 $t$-统计量表示。根据定义，$t = \\frac{\\bar{x}-\\bar{y}}{s_{p}\\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}}$。将其平方得到：\n$$t^{2} = \\frac{(\\bar{x}-\\bar{y})^{2}}{s_{p}^{2}\\left(\\frac{1}{n_{1}}+\\frac{1}{n_{2}}\\right)} = \\frac{(\\bar{x}-\\bar{y})^{2}}{s_{p}^{2}\\left(\\frac{n_{1}+n_{2}}{n_{1}n_{2}}\\right)} = \\frac{n_{1}n_{2}(\\bar{x}-\\bar{y})^{2}}{s_{p}^{2}(n_{1}+n_{2})}$$\n从 $SS_{B}$ 的表达式中，我们有 $(\\bar{x}-\\bar{y})^{2} = \\frac{n_{1}+n_{2}}{n_{1}n_{2}}SS_{B}$。将此代入 $t^{2}$ 的方程中：\n$$t^{2} = \\frac{n_{1}n_{2}}{s_{p}^{2}(n_{1}+n_{2})} \\left( \\frac{n_{1}+n_{2}}{n_{1}n_{2}} SS_{B} \\right) = \\frac{SS_{B}}{s_{p}^{2}}$$\n现在我们构建所需比率：\n$$\\frac{SS_{B}}{SS_{R}} = \\frac{t^{2}s_{p}^{2}}{SS_{R}} = \\frac{t^{2}s_{p}^{2}}{(n_{1}+n_{2}-2)s_{p}^{2}} = \\frac{t^{2}}{n_{1}+n_{2}-2}$$\n将此代回 $-2\\ln\\Lambda$ 的表达式中：\n$$-2\\ln\\Lambda = (n_{1}+n_{2}) \\ln\\left(1 + \\frac{t^{2}}{n_{1}+n_{2}-2}\\right)$$\n这个最终表达式显示 $-2\\ln\\Lambda$ 是 $t$、$n_1$ 和 $n_2$ 的函数。由于自然对数是一个严格递增函数，$-2\\ln\\Lambda$ 是 $t^{2}$ 的单调递增函数，因此也是 $|t|$ 的单调递增函数。这表明，对于较大的 $-2\\ln\\Lambda$ 值拒绝 $H_{0}$ 等价于对于较大的 $|t|$ 值拒绝 $H_{0}$，从而确立了 LR 检验与双侧合并 $t$-检验的等价性。",
            "answer": "$$\\boxed{(n_1+n_2)\\ln\\left(1 + \\frac{t^2}{n_1+n_2-2}\\right)}$$"
        },
        {
            "introduction": "从解析推导转向实际应用，这个练习解决了一个计算神经科学中的核心任务：比较嵌套的广义线性模型 (GLM)。你将实现似然比检验，以确定增加一个新的预测变量是否能显著改善神经活动模型，这是检验关于神经元如何编码信息的假设的一项基本技能。该问题将巩固你在真实的多参数建模情境中应用似然比检验的能力。",
            "id": "4174116",
            "problem": "在神经科学数据分析中，考虑由广义线性模型 (GLM) 建模的独立神经观测数据。GLM 框架假设，对于每个观测索引 $i$，响应 $y_i$ 服从指数族中的某个分布，其均值 $E[y_i] = \\mu_i$ 通过线性预测器 $\\eta_i = x_i^\\top \\beta$ 与预测变量 $x_i$ 相关联，其中 $\\beta$ 是参数向量，连接函数则将 $\\mu_i$ 与 $\\eta_i$ 联系起来。假设每种分布都使用其典范连接。您需要实现似然比 (LR) 检验，以比较嵌套模型：一个简化模型和一个完整模型。其中，完整模型包含简化模型的所有参数以及一些额外参数。根据似然原理，LR 检验比较了两个模型下的最大化对数似然值。在标准正则性条件下，LR 检验统计量服从渐近卡方近似，其自由度由完整模型和简化模型之间的参数数量之差给出。\n\n您的程序必须：\n- 对于具有典范对数连接的 Poisson 分布和具有典范 logit 连接的 Bernoulli 分布，通过最大似然法拟合 GLM。\n- 对每个测试用例，构建一个简化设计矩阵和一个完整设计矩阵，两者都包含一个截距项，且完整模型比简化模型多一个额外的协变量列；为两个模型估计最大似然参数；计算 LR 统计量；使用等于完整模型和简化模型参数数量之差的自由度来计算卡方近似的 $p$ 值；并决定是否在显著性水平 $\\alpha = 0.05$ 下拒绝简化模型。\n\n使用以下测试套件，其中每个用例指定了模型类型、样本量、随机种子以及用于模拟的真实数据生成参数。在所有用例中，简化设计矩阵由一个截距和一个协变量 $x_1$ 组成，而完整设计矩阵由一个截距、$x_1$ 和一个额外的协变量 $x_2$ 组成。除非另有说明，在所有用例中，$x_1$ 从 $x_1 \\sim \\mathcal{N}(0,1)$ 中抽取，$x_2$ 独立地从 $x_2 \\sim \\mathcal{N}(0,1)$ 中抽取。截距始终作为一列常数 1 包含在内。\n\n- 用例 A (理想路径，具有真实效应的 Poisson 计数):\n    - 模型类型：Poisson (典范对数连接)。\n    - 样本量：$n = 200$。\n    - 随机种子：$7$。\n    - 真实简化模型参数：$\\beta_{\\text{red}} = (\\beta_0,\\beta_1) = (\\log 8, 0.5)$。\n    - 真实完整模型参数：$\\beta_{\\text{full}} = (\\beta_0,\\beta_1,\\beta_2) = (\\log 8, 0.5, 0.7)$。\n    - 数据生成：模拟 $y_i \\sim \\text{Poisson}(\\mu_i)$，其中 $\\mu_i = \\exp(\\eta_i)$ 且 $\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}$。\n\n- 用例 B (边缘案例，无额外效应的 Poisson 计数):\n    - 模型类型：Poisson (典范对数连接)。\n    - 样本量：$n = 200$。\n    - 随机种子：$11$。\n    - 真实简化模型参数：$\\beta_{\\text{red}} = (\\log 8, 0.5)$。\n    - 真实完整模型参数：$\\beta_{\\text{full}} = (\\log 8, 0.5, 0.0)$。\n    - 数据生成：与用例 A 相同。\n\n- 用例 C (理想路径，具有真实效应的 Bernoulli 脉冲):\n    - 模型类型：Bernoulli (典范 logit 连接)。\n    - 样本量：$n = 300$。\n    - 随机种子：$13$。\n    - 真实简化模型参数：$\\beta_{\\text{red}} = (\\beta_0,\\beta_1) = (-1.0, 0.8)$。\n    - 真实完整模型参数：$\\beta_{\\text{full}} = (\\beta_0,\\beta_1,\\beta_2) = (-1.0, 0.8, -1.0)$。\n    - 数据生成：模拟 $y_i \\sim \\text{Bernoulli}(p_i)$，其中 $p_i = \\mu_i = \\sigma(\\eta_i)$，$\\sigma(z) = 1/(1 + e^{-z})$，且 $\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}$。\n\n- 用例 D (边缘案例，小样本、近似共线性且无额外效应的 Bernoulli):\n    - 模型类型：Bernoulli (典范 logit 连接)。\n    - 样本量：$n = 50$。\n    - 随机种子：$21$。\n    - 真实简化模型参数：$\\beta_{\\text{red}} = (-0.5, 0.3)$。\n    - 真实完整模型参数：$\\beta_{\\text{full}} = (-0.5, 0.3, 0.0)$。\n    - 协变量结构：抽取 $x_1 \\sim \\mathcal{N}(0,1)$；设置 $x_2 = x_1 + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, 0.05^2)$ 以引入近似共线性。\n    - 数据生成：与用例 C 相同。\n\n算法要求：\n- 对于典范连接，使用通过迭代重加权最小二乘法 (IRLS) 对 GLM 特化的牛顿法来实现最大似然估计。对于 Poisson 模型，使用 $\\mu_i = \\exp(\\eta_i)$。对于 Bernoulli 模型，使用 $\\mu_i = \\sigma(\\eta_i)$。\n- 计算每个拟合模型的最大化对数似然。对于 Poisson 模型，对数似然可能包含项 $-\\sum_i \\log(y_i!)$；对于 LR 统计量，对于相同数据，常数项在不同模型间会相互抵消。\n- 计算 LR 检验统计量，其值为完整模型和简化模型最大化对数似然之差的两倍。使用自由度 $\\nu = k_{\\text{full}} - k_{\\text{red}}$ 计算卡方近似的 $p$ 值，其中 $k_{\\text{full}}$ 和 $k_{\\text{red}}$ 分别是完整模型和简化模型中的自由参数数量（包括截距）。\n- 如果 $p$ 值严格小于 $0.05$，则在 $\\alpha = 0.05$ 的水平下决定是否拒绝简化模型。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含四个测试用例的结果，格式为一个逗号分隔的列表的列表。每个内部列表的形式为 $[\\text{LR 统计量}, \\text{$p$ 值}, \\text{自由度}, \\text{决策}]$，其中前两个条目是浮点数，第三个是整数，第四个是布尔值。例如：$[[\\ell_1, p_1, \\nu_1, d_1],[\\ell_2, p_2, \\nu_2, d_2],[\\ell_3, p_3, \\nu_3, d_3],[\\ell_4, p_4, \\nu_4, d_4]]$。",
            "solution": "该问题要求实现似然比 (LR) 检验，用于比较嵌套的广义线性模型 (GLM)，特别是针对 Poisson 分布和 Bernoulli 分布，并将此检验应用于四个不同的模拟数据集。从模型拟合到假设检验的整个过程都必须从基本原理开始开发。\n\n广义线性模型由三个组成部分定义：\n1.  **随机部分**，指定响应变量 $y_i$ 来自指数族的概率分布。在本问题中，我们考虑用于计数的 Poisson 分布和用于二元结果的 Bernoulli 分布。\n2.  **系统部分**，这是一个由一组协变量 $x_i$ 构成的线性预测器 $\\eta_i$：$\\eta_i = x_i^\\top \\beta$。\n3.  **连接函数** $g$，它将响应的期望值 $\\mu_i = E[y_i]$ 与线性预测器相连接：$g(\\mu_i) = \\eta_i$。我们将为每种分布使用典范连接函数，因为它赋予模型一些理想的统计特性，例如对数似然函数的凹性。\n\n对于用于计数数据 $y_i \\in \\{0, 1, 2, ...\\}$ 的 **Poisson 模型**，典范连接是自然对数，$g(\\mu_i) = \\log(\\mu_i) = \\eta_i$。因此，反连接函数为 $\\mu_i = \\exp(\\eta_i)$。响应的方差等于其均值，$\\text{Var}(y_i) = \\mu_i$。\n\n对于用于二元数据 $y_i \\in \\{0, 1\\}$ 的 **Bernoulli 模型**，典范连接是 logit 函数，$g(\\mu_i) = \\text{logit}(\\mu_i) = \\log\\left(\\frac{\\mu_i}{1-\\mu_i}\\right) = \\eta_i$。反连接函数是 sigmoid (或 logistic) 函数，$\\mu_i = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}$。方差由 $\\text{Var}(y_i) = \\mu_i(1-\\mu_i)$ 给出。\n\n模型参数 $\\beta$ 通过最大化对数似然函数 $l(\\beta; y) = \\sum_i l_i(\\beta; y_i)$ 来估计。这种最大化通常通过数值优化算法实现。对于具有典范连接的 GLM，牛顿-拉夫逊方法可简化为一种优雅而高效的算法，称为**迭代重加权最小二乘法 (IRLS)**。IRLS 算法为每次迭代 $t+1$ 提供了参数向量 $\\beta$ 的更新规则：\n$$ \\beta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)} $$\n该方程表示对“调整响应”$z^{(t)}$ 在设计矩阵 $X$ 上的加权最小二乘回归，权重由 $W^{(t)}$ 给出。\n其组成部分为：\n- $X$ 是 $n \\times p$ 的设计矩阵，其中 $n$ 是观测数量，$p$ 是参数数量。\n- $W^{(t)}$ 是一个 $n \\times n$ 的对角权重矩阵。对于典范连接，对角元素是响应的方差，使用前一次迭代的参数计算得出：$W_{ii}^{(t)} = \\text{Var}(y_i | \\beta^{(t)}) = V(\\mu_i^{(t)})$。\n  - 对于 Poisson 模型：$W_{ii}^{(t)} = \\mu_i^{(t)} = \\exp(x_i^\\top \\beta^{(t)})$。\n  - 对于 Bernoulli 模型：$W_{ii}^{(t)} = \\mu_i^{(t)}(1-\\mu_i^{(t)})$。\n- $z^{(t)}$ 是 $n \\times 1$ 的调整响应向量。其元素定义为 $z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)})(\\frac{\\partial \\eta_i}{\\partial \\mu_i})^{(t)}$。对于典范连接，$\\frac{\\partial \\eta_i}{\\partial \\mu_i} = \\frac{1}{V(\\mu_i)}$，因此表达式简化为 $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{V(\\mu_i^{(t)})}$。\n\n该算法使用 $\\beta$ 的一个初始猜测值（例如，零向量）进行初始化，并进行迭代，直到连续迭代之间的 $\\beta$ 变化量低于预定义的容差。\n\n一旦找到最大似然估计 (MLE) $\\hat{\\beta}$，就计算对数似然的最大化值。忽略任何不依赖于 $\\beta$ 的项，对数似然函数的具体形式为：\n- 对于 Poisson 模型：$l(\\hat{\\beta}) = \\sum_{i=1}^n \\left( y_i (x_i^\\top \\hat{\\beta}) - \\exp(x_i^\\top \\hat{\\beta}) \\right)$。\n- 对于 Bernoulli 模型：$l(\\hat{\\beta}) = \\sum_{i=1}^n \\left( y_i (x_i^\\top \\hat{\\beta}) - \\log(1 + \\exp(x_i^\\top \\hat{\\beta})) \\right)$。\n\n**似然比 (LR) 检验**用于比较两个嵌套模型：一个简化模型 ($H_0$) 和一个完整模型 ($H_1$)，其中简化模型是完整模型的一个特例（即其参数空间 $\\Theta_0$ 是完整模型参数空间 $\\Theta_1$ 的一个子集）。LR 检验统计量由下式给出：\n$$ \\Lambda = 2 \\left( l(\\hat{\\beta}_{\\text{full}}) - l(\\hat{\\beta}_{\\text{red}}) \\right) $$\n其中 $l(\\hat{\\beta}_{\\text{full}})$ 和 $l(\\hat{\\beta}_{\\text{red}})$ 分别是完整模型和简化模型的最大化对数似然。\n\n根据 Wilks 定理，在简化模型是真实模型的原假设下，统计量 $\\Lambda$ 渐近服从卡方 ($\\chi^2$) 分布。该分布的自由度 ($\\nu$) 等于两个模型参数数量之差：$\\nu = k_{\\text{full}} - k_{\\text{red}}$。\n\n在本问题中，简化模型包含一个截距和一个协变量 $x_1$，因此 $k_{\\text{red}} = 2$。完整模型增加第二个协变量 $x_2$，因此 $k_{\\text{full}} = 3$。待检验的原假设是 $H_0: \\beta_2 = 0$，LR 检验的自由度为 $\\nu = 3 - 2 = 1$。$p$ 值计算为概率 $P(\\chi^2_1 \\ge \\Lambda)$。如果 $p$ 值小于显著性水平 $\\alpha = 0.05$，我们拒绝原假设（即，我们得出结论，完整模型提供了显著更优的拟合）。\n\n实现过程将首先根据指定参数为每个测试用例模拟数据。然后，对每个用例，我们将使用我们实现的 IRLS 来拟合简化模型和完整 GLM。最后，我们将计算 LR 统计量 $\\Lambda$，其来自 $\\chi^2_1$ 分布的相关 $p$ 值，并根据 $\\alpha=0.05$ 的阈值做出决策。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Performs Likelihood Ratio (LR) tests for nested Generalized Linear Models (GLMs)\n    on four specified test cases.\n    \"\"\"\n\n    def fit_glm(X, y, model_type, tol=1e-8, max_iter=50):\n        \"\"\"\n        Fits a GLM using Iteratively Reweighted Least Squares (IRLS).\n\n        Args:\n            X (np.ndarray): Design matrix (n_samples, n_features).\n            y (np.ndarray): Response vector (n_samples,).\n            model_type (str): 'poisson' or 'bernoulli'.\n            tol (float): Convergence tolerance.\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            tuple: (beta_hat, log_likelihood)\n                   - beta_hat (np.ndarray): Estimated parameters.\n                   - log_likelihood (float): Maximized log-likelihood.\n        \"\"\"\n        if model_type == 'poisson':\n            link_fn = np.exp\n            variance_fn = lambda mu: mu\n            log_likelihood_fn = lambda beta, X, y: np.sum(y * (X @ beta) - np.exp(X @ beta))\n        elif model_type == 'bernoulli':\n            # Use a stable sigmoid implementation\n            def sigmoid(eta):\n                return np.piecewise(eta, [eta > 0], [\n                    lambda x: 1 / (1 + np.exp(-x)),\n                    lambda x: np.exp(x) / (1 + np.exp(x))\n                ])\n            link_fn = sigmoid\n            variance_fn = lambda mu: mu * (1 - mu)\n            log_likelihood_fn = lambda beta, X, y: np.sum(y * (X @ beta) - np.log(1 + np.exp(X @ beta)))\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n\n        beta = np.zeros(X.shape[1])\n        \n        for _ in range(max_iter):\n            eta = X @ beta\n            mu = link_fn(eta)\n            \n            # Numerical stability for Bernoulli\n            if model_type == 'bernoulli':\n                mu = np.clip(mu, 1e-10, 1 - 1e-10)\n\n            var = variance_fn(mu)\n            W = np.diag(var)\n\n            # For canonical link, d(mu)/d(eta) = Var(mu)\n            z = eta + (y - mu) / var\n            \n            # Solve the weighted least squares problem\n            # (X.T @ W @ X) @ beta_new = X.T @ W @ z\n            try:\n                # Add a small regularization term for stability, e.g., in case of collinearity\n                reg = 1e-8 * np.eye(X.shape[1])\n                beta_new = np.linalg.solve(X.T @ W @ X + reg, X.T @ W @ z)\n            except np.linalg.LinAlgError:\n                # Fallback to pseudo-inverse if solve fails\n                beta_new = np.linalg.pinv(X.T @ W @ X) @ (X.T @ W @ z)\n            \n            if np.linalg.norm(beta_new - beta) < tol:\n                beta = beta_new\n                break\n            \n            beta = beta_new\n        \n        log_likelihood = log_likelihood_fn(beta, X, y)\n        return beta, log_likelihood\n\n    test_cases = [\n        {\n            'name': 'A', 'model_type': 'poisson', 'n': 200, 'seed': 7,\n            'beta_true': np.array([np.log(8), 0.5, 0.7]), 'collinear': False\n        },\n        {\n            'name': 'B', 'model_type': 'poisson', 'n': 200, 'seed': 11,\n            'beta_true': np.array([np.log(8), 0.5, 0.0]), 'collinear': False\n        },\n        {\n            'name': 'C', 'model_type': 'bernoulli', 'n': 300, 'seed': 13,\n            'beta_true': np.array([-1.0, 0.8, -1.0]), 'collinear': False\n        },\n        {\n            'name': 'D', 'model_type': 'bernoulli', 'n': 50, 'seed': 21,\n            'beta_true': np.array([-0.5, 0.3, 0.0]), 'collinear': True\n        }\n    ]\n\n    results = []\n    \n    alpha = 0.05\n\n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        n = case['n']\n        \n        # Generate covariates\n        x1 = rng.normal(0, 1, n)\n        if case['collinear']:\n            epsilon = rng.normal(0, 0.05, n)\n            x2 = x1 + epsilon\n        else:\n            x2 = rng.normal(0, 1, n)\n            \n        X_full = np.c_[np.ones(n), x1, x2]\n        \n        # Generate response data y from the full model\n        eta_true = X_full @ case['beta_true']\n        if case['model_type'] == 'poisson':\n            mu_true = np.exp(eta_true)\n            y = rng.poisson(mu_true)\n        else: # bernoulli\n            p_true = 1 / (1 + np.exp(-eta_true))\n            y = rng.binomial(1, p_true)\n            \n        # Define reduced and full design matrices\n        X_red = X_full[:, :2]\n\n        # Fit reduced model (H0)\n        _, ll_red = fit_glm(X_red, y, case['model_type'])\n\n        # Fit full model (H1)\n        _, ll_full = fit_glm(X_full, y, case['model_type'])\n        \n        # Perform Likelihood Ratio Test\n        lr_statistic = 2 * (ll_full - ll_red)\n        if lr_statistic < 0: # Due to numerical precision, can be slightly negative\n            lr_statistic = 0\n\n        dof = X_full.shape[1] - X_red.shape[1]\n        \n        p_value = chi2.sf(lr_statistic, dof)\n        \n        decision = p_value < alpha\n        \n        results.append([lr_statistic, p_value, dof, decision])\n\n    # Format the final output string\n    output_str = '['\n    for i, res in enumerate(results):\n        # Format: [LR_statistic, p-value, degrees of freedom, decision]\n        # Floats, int, bool\n        output_str += f\"[{res[0]:.8f},{res[1]:.8f},{res[2]},{str(res[3]).lower()}]\"\n        if i < len(results) - 1:\n            output_str += ','\n    output_str += ']'\n    \n    # Due to float representation, direct string formatting is safer than map/join\n    # Example format: [[18.84777553,0.00001423,1,true],[0.00001096,0.99735496,1,false], ...\n    # This format is not standard Python list repr. Using this custom builder.\n    \n    # Re-evaluating the output format:\n    # \"a comma-separated list of lists...[[\\ell_1, p_1, \\nu_1, d_1],...]\"\n    # This implies standard list representation printing.\n    print(results)\n\n\nsolve()\n```"
        },
        {
            "introduction": "最后一个练习要求你超越标准应用，直面似然理论中一个重要的细微之处。在我们的模型中，参数通常是受自然约束的，例如方差参数不能为负。本问题探讨了当原假设将一个参数置于其允许空间的边界时会发生什么，在这种情况下，似然比检验统计量的标准卡方 ($\\chi^2$) 近似会失效。通过推导这种边界情况下的正确渐近分布，你将学会如何避免常见的分析错误。",
            "id": "4174137",
            "problem": "考虑从单个神经元在 $n$ 个不重叠的时间窗内记录的脉冲计数数据，得到观测值 $\\{Y_{i}\\}_{i=1}^{n}$，其中每个 $Y_{i} \\in \\{0,1,2,\\dots\\}$。为了捕捉由未观测到的网络状态或不应期导致的过量零值，分析师有时会假设一个零膨胀计数模型。在该模型中，以参数为条件，固定比例的试验是结构性零值。具体来说，假设数据由参数为 $(\\lambda,\\psi)$ 的零膨胀泊松分布建模，其中 $\\lambda>0$ 是泊松率，$\\psi \\in [0,1)$ 是表示结构性零值概率的零膨胀参数。该模型为\n$$\n\\mathbb{P}(Y=y \\mid \\lambda,\\psi)=\n\\begin{cases}\n\\psi+(1-\\psi)\\exp(-\\lambda),  y=0, \\\\\n(1-\\psi)\\exp(-\\lambda)\\frac{\\lambda^{y}}{y!},  y\\in\\{1,2,\\dots\\}.\n\\end{cases}\n$$\n我们感兴趣的是使用似然比检验 (LRT) 来检验是否存在零膨胀，其原假设为 $H_{0}\\!:\\ \\psi=0$，备择假设为 $H_{1}\\!:\\ \\psi>0$。在 $H_{0}$ 下，参数 $\\psi$ 位于其参数空间 $[0,1)$ 的边界上。\n\n设 $\\ell_{n}(\\lambda,\\psi)$ 表示样本 $\\{Y_{i}\\}_{i=1}^{n}$ 在零膨胀泊松模型下的总对数似然，并定义 LRT 统计量\n$$\n\\Lambda_{n}=2\\left\\{\\sup_{\\lambda>0,\\ \\psi\\in[0,1)}\\ \\ell_{n}(\\lambda,\\psi)\\ -\\ \\sup_{\\lambda>0,\\ \\psi=0}\\ \\ell_{n}(\\lambda,0)\\right\\}.\n$$\n假设最大似然估计的标准正则性条件成立，包括 $\\ell_{n}$ 在 $(\\lambda,\\psi)=(\\lambda_{0},0)$ 邻域内的可微性、费雪信息的存在性，以及在 $H_{0}$ 下，经过适当的重参数化以处理 $\\lambda$ 的讨厌效应后，得分函数的渐近正态性。\n\n从得分函数和费雪信息的核心定义出发，并使用对数似然在 $(\\lambda_{0},\\psi=0)$ 附近的二阶泰勒展开，推导在边界情况 $\\psi\\ge 0$ 下 LRT 统计量 $\\Lambda_{n}$ 的大样本 ($n\\to\\infty$) 零分布。将最终结果表示为对于 $t\\ge 0$ 的极限分布的累积分布函数 $F_{\\Lambda}(t)$ 的单个闭式解析表达式，用标准正态累积分布函数 $\\Phi(\\cdot)$ 表示。无需进行数值计算，也不涉及任何单位。最终答案必须是单个解析表达式。",
            "solution": "对用户提供的问题进行验证。\n\n### 步骤 1：提取已知条件\n-   **数据**：$\\{Y_{i}\\}_{i=1}^{n}$ 是来自零膨胀泊松分布的 $n$ 个独立同分布观测值，其中 $Y_{i} \\in \\{0,1,2,\\dots\\}$。\n-   **模型**：概率质量函数由下式给出\n    $$\n    \\mathbb{P}(Y=y \\mid \\lambda,\\psi)=\n    \\begin{cases}\n    \\psi+(1-\\psi)\\exp(-\\lambda),  y=0, \\\\\n    (1-\\psi)\\exp(-\\lambda)\\frac{\\lambda^{y}}{y!},  y\\in\\{1,2,\\dots\\}.\n    \\end{cases}\n    $$\n-   **参数**：泊松率 $\\lambda > 0$ 和零膨胀概率 $\\psi \\in [0,1)$。\n-   **假设检验**：原假设为 $H_{0}: \\psi=0$，备择假设为 $H_1: \\psi>0$。\n-   **检验统计量**：似然比检验 (LRT) 统计量是\n    $$\n    \\Lambda_{n}=2\\left\\{\\sup_{\\lambda>0,\\ \\psi\\in[0,1)}\\ \\ell_{n}(\\lambda,\\psi)\\ -\\ \\sup_{\\lambda>0,\\ \\psi=0}\\ \\ell_{n}(\\lambda,0)\\right\\},\n    $$\n    其中 $\\ell_n(\\lambda, \\psi)$ 是总对数似然。\n-   **假设**：假设最大似然估计的标准正则性条件成立，我们关心的是大样本 ($n\\to\\infty$) 极限。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据。零膨胀泊松模型是包括神经科学在内的许多领域中用于建模具有过量零值的计数数据的标准统计工具。似然比检验是假设检验的一种基本方法。该问题是良定的，要求在特定、明确定义的设置下推导极限分布。问题的核心在于原假设 $H_0: \\psi=0$ 将参数 $\\psi$ 置于其参数空间 $[0,1)$ 的边界上，这是渐近理论中一个已知​​的复杂问题。该问题客观且无歧义，不违反任何无效标准。\n\n### 步骤 3：结论与行动\n该问题被视为**有效**。现在将提供完整的解题推导。\n\n### 极限分布的推导\n该问题要求推导用于检验 $H_0: \\psi=0$ 与 $H_1: \\psi>0$ 的LRT统计量 $\\Lambda_n$ 的渐近分布。关键特征是零值 $\\psi=0$ 位于 $\\psi$ 的参数空间 $[0,1)$ 的边界上。这种情况违反了标准威尔克斯定理的一个关键正则性条件，否则该定理会预测一个 $\\chi_1^2$ 极限分布。因此，必须援引一种不同的渐近理论，该理论由 Chernoff (1954) 和 Self  Liang (1987) 针对边界上的参数而发展。\n\n设 $\\hat{\\theta}_0 = (\\hat{\\lambda}_0, 0)$ 是原假设 $H_0$ 下参数 $(\\lambda, \\psi)$ 的最大似然估计量 (MLE)。此处，$\\hat{\\lambda}_0$ 是标准泊松模型的 MLE，即样本均值 $\\bar{Y}$。设 $\\hat{\\theta}_1 = (\\hat{\\lambda}_1, \\hat{\\psi}_1)$ 是备择假设下的 MLE，其中最大化是针对 $\\lambda>0$ 和 $\\psi \\in [0,1)$ 进行的。\n\n$\\Lambda_n$ 的渐近行为取决于 $H_0$ 下真实参数值邻域内对数似然函数的行为。备择假设下的 MLE $\\hat{\\psi}_1$ 被约束为非负。其值取决于在原假设约束下的 MLE $(\\hat{\\lambda}_0, 0)$ 处对数似然函数关于 $\\psi$ 的梯度。设 $S_{\\psi, a} = \\frac{\\partial \\ell_n}{\\partial \\psi}\\big|_{(\\hat{\\lambda}_0, 0)}$ 为在原假设 MLE 处求值的关于 $\\psi$ 的得分函数。\n\n在 $H_0$ 下，根据得分函数渐近正态性的假设，$S_{\\psi, a}$ (在通过 $\\frac{1}{\\sqrt{n}}$ 进行适当缩放后) 渐近服从均值为 0 的正态分布。因此，对于大的 $n$，$S_{\\psi, a}$ 为正的概率近似为 $1/2$，$S_{\\psi, a}$ 为非正的概率也近似为 $1/2$。\n\n我们分析两种情况：\n\n1.  **情况 1：$S_{\\psi, a} \\le 0$。**\n    如果关于 $\\psi$ 的得分函数为非正，那么当 $\\psi$ 从 0 移动到参数空间 $(0,1)$ 时，对数似然函数不增加。考虑到约束 $\\psi \\ge 0$，备择假设下的似然函数的最大值在边界处达到，即 $\\hat{\\psi}_1 = 0$。在这种情况下，备择假设下的 MLE 与原假设下的 MLE 相同：$(\\hat{\\lambda}_1, \\hat{\\psi}_1) = (\\hat{\\lambda}_0, 0)$。因此，对数似然值相等，LRT 统计量为 $\\Lambda_n=0$。此事件发生的渐近概率为 $1/2$。\n\n2.  **情况 2：$S_{\\psi, a} > 0$。**\n    如果关于 $\\psi$ 的得分函数为正，则对数似然函数在 $\\psi=0$ 处是关于 $\\psi$ 递增的。这意味着备择假设下的 MLE 将位于参数空间的内部，即 $\\hat{\\psi}_1 > 0$。在这种情况下，边界约束不生效。该问题局部上表现得像一个单个参数的无约束检验。对于样本空间的这个子集，LRT 统计量 $\\Lambda_n$ 依分布收敛于一个自由度为 1 的卡方随机变量，即 $\\chi_1^2$。此事件发生的渐近概率为 $1/2$。\n\n结合这两种结果，$\\Lambda_n$ 的极限分布是在 0 处的点质量和 $\\chi_1^2$ 分布的混合，两者权重均为 $1/2$。形式上，如果 $\\Lambda$ 是一个服从该极限分布的随机变量，我们可以写成：\n$$\n\\Lambda \\sim \\frac{1}{2}\\chi_0^2 + \\frac{1}{2}\\chi_1^2\n$$\n其中 $\\chi_0^2$ 表示在 0 处具有点质量的退化分布。\n\n我们现在推导对于 $t \\ge 0$ 的累积分布函数 (CDF)，$F_{\\Lambda}(t) = \\mathbb{P}(\\Lambda \\le t)$。该混合分布的 CDF 是各个 CDF 的加权平均：\n$$\nF_{\\Lambda}(t) = \\frac{1}{2} F_{\\chi_0^2}(t) + \\frac{1}{2} F_{\\chi_1^2}(t)\n$$\n对于 $t \\ge 0$，在 0 处的点质量的 CDF 是 $F_{\\chi_0^2}(t) = \\mathbb{P}(0 \\le t) = 1$。\n$\\chi_1^2$ 分布的 CDF，$F_{\\chi_1^2}(t)$，可以用标准正态 CDF $\\Phi(\\cdot)$ 来表示。一个 $\\chi_1^2$ 随机变量是一个标准正态随机变量 $Z \\sim N(0,1)$ 的平方。因此，对于 $t \\ge 0$：\n$$\nF_{\\chi_1^2}(t) = \\mathbb{P}(Z^2 \\le t) = \\mathbb{P}(-\\sqrt{t} \\le Z \\le \\sqrt{t})\n$$\n$$\nF_{\\chi_1^2}(t) = \\Phi(\\sqrt{t}) - \\Phi(-\\sqrt{t})\n$$\n利用标准正态分布的对称性 $\\Phi(-z) = 1 - \\Phi(z)$，我们得到：\n$$\nF_{\\chi_1^2}(t) = \\Phi(\\sqrt{t}) - (1 - \\Phi(\\sqrt{t})) = 2\\Phi(\\sqrt{t}) - 1\n$$\n将这些结果代回到 $F_{\\Lambda}(t)$ 的表达式中：\n$$\nF_{\\Lambda}(t) = \\frac{1}{2}(1) + \\frac{1}{2}(2\\Phi(\\sqrt{t}) - 1)\n$$\n$$\nF_{\\Lambda}(t) = \\frac{1}{2} + \\Phi(\\sqrt{t}) - \\frac{1}{2}\n$$\n$$\nF_{\\Lambda}(t) = \\Phi(\\sqrt{t})\n$$\n这就是对于 $t \\ge 0$ 的 $\\Lambda_n$ 的极限分布的 CDF 的闭式解析表达式。",
            "answer": "$$\n\\boxed{\\Phi(\\sqrt{t})}\n$$"
        }
    ]
}