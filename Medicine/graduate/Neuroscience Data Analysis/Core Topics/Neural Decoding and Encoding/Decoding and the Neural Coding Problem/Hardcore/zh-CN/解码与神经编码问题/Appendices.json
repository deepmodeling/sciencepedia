{
    "hands_on_practices": [
        {
            "introduction": "评估解码器的性能是神经数据分析中的一项基本任务。一旦我们训练了一个分类解码器，我们就需要客观地衡量其预测的准确性。这个练习将指导你构建一个混淆矩阵——这是评估分类性能的基石——并从中计算出诸如准确率、精确率、召回率和 F1 分数等关键指标，这些都是理解和报告解码器优缺点的核心技能。",
            "id": "4154094",
            "problem": "一个神经科学实验室训练了一个多类别概率解码器，用于从神经元群体的联合发放计数中推断感觉刺激的类别。实验设计包含$4$个不同的刺激类别，记为 $\\{A,B,C,D\\}$，总共进行了$200$次试验。该解码器在留出数据上进行评估。对于每次试验，呈现一个刺激，解码器会输出一个预测的类别。评估记录了对于每个真实类别，解码器在所有类别上的输出计数：\n- 在真实类别为 $A$ 的试验中：$35$ 次预测为 $A$，$5$ 次预测为 $B$，$8$ 次预测为 $C$，$2$ 次预测为 $D$。\n- 在真实类别为 $B$ 的试验中：$4$ 次预测为 $A$，$26$ 次预测为 $B$，$6$ 次预测为 $C$，$4$ 次预测为 $D$。\n- 在真实类别为 $C$ 的试验中：$5$ 次预测为 $A$，$12$ 次预测为 $B$，$38$ 次预测为 $C$，$5$ 次预测为 $D$。\n- 在真实类别为 $D$ 的试验中：$3$ 次预测为 $A$，$7$ 次预测为 $B$，$9$ 次预测为 $C$，$31$ 次预测为 $D$。\n\n从神经科学数据分析中多类别解码所使用的混淆矩阵和基于计数的评估指标的核心定义出发，执行以下操作：\n1. 构建混淆矩阵 $M$，其元素 $M_{ij}$ 等于真实类别为 $i \\in \\{A,B,C,D\\}$ 且预测类别为 $j \\in \\{A,B,C,D\\}$ 的试验次数，行和列均按字母顺序排列。\n2. 根据混淆矩阵，计算总体准确率、每个类别 $\\{A,B,C,D\\}$ 的精确率、召回率和 F1 分数（精确率和召回率的调和平均值）。对每个类别，使用标准的“一对多”（one-versus-rest）方法来解释真阳性、假阳性和假阴性。\n3. 通过计算四个类别的 F1 分数的算术平均值，来计算宏平均 F1 分数。\n\n所有比率均以小数形式报告，并四舍五入至四位有效数字。将宏平均 F1 分数作为您的最终数值答案。这些比率没有适用的物理单位；请将最终答案表示为四舍五入到四位有效数字的小数。",
            "solution": "该任务涉及在多类别设置下的解码评估。推导的基础是混淆矩阵的定义和标准的基于计数的指标。对于一组类别 $\\{A,B,C,D\\}$，混淆矩阵 $M$ 的元素 $M_{ij}$ 记录了真实类别为 $i$ 的实例被预测为类别 $j$ 的数量。对于任意给定的类别 $k$，“一对多”的定义如下：\n- 真阳性 (TP)：$TP_{k}$ 等于对角线元素 $M_{kk}$。\n- 假阳性 (FP)：$FP_{k}$ 等于其他真实类别被预测为类别 $k$ 的总和，即类别 $k$ 的列总和减去 $TP_{k}$。\n- 假阴性 (FN)：$FN_{k}$ 等于真实类别为 $k$ 的实例被预测为其他类别的总和，即类别 $k$ 的行总和减去 $TP_{k}$。\n总体准确率是正确分类实例的比例：对角线元素之和除以实例总数。类别 $k$ 的精确率为 $TP_{k}/(TP_{k}+FP_{k})$，召回率为 $TP_{k}/(TP_{k}+FN_{k})$，而 F1 分数（精确率和召回率的调和平均值）定义为 $F1_{k} = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$。从这些定义出发，通过代入精确率和召回率的表达式并化简，可以推导出\n$$\nF1_{k} \\;=\\; \\frac{2 \\, TP_{k}}{2\\,TP_{k} + FP_{k} + FN_{k}}\n$$\n\n步骤 1：构建混淆矩阵 $M$。\n\n按字母顺序 $(A,B,C,D)$ 排列行和列，给定的计数得出\n$$\nM \\;=\\; \\begin{pmatrix}\n35   5   8   2 \\\\\n4   26   6   4 \\\\\n5   12   38   5 \\\\\n3   7   9   31\n\\end{pmatrix}.\n$$\n行总和（真实类别总数）：$A: 35+5+8+2 = 50$, $B: 4+26+6+4 = 40$, $C: 5+12+38+5 = 60$, $D: 3+7+9+31 = 50$。实例总数为 $N = 50+40+60+50 = 200$。列总和（预测类别总数）：$A: 35+4+5+3 = 47$, $B: 5+26+12+7 = 50$, $C: 8+6+38+9 = 61$, $D: 2+4+5+31 = 42$。\n\n步骤 2：计算准确率、各类别精确率、召回率和 F1 分数。\n\n- 总体准确率：\n对角线元素之和为 $35 + 26 + 38 + 31 = 130$。因此，\n$$\n\\text{accuracy} \\;=\\; \\frac{130}{200} \\;=\\; 0.6500 \\text{ (rounded to four significant figures)}.\n$$\n\n现在计算各类别指标。\n\n类别 $A$：\n- $TP_{A} = 35$。\n- $FP_{A} = (\\text{predicted } A \\text{ total}) - TP_{A} = 47 - 35 = 12$。\n- $FN_{A} = (\\text{true } A \\text{ total}) - TP_{A} = 50 - 35 = 15$。\n- 精确率: $\\frac{35}{47} \\approx 0.74468085 \\rightarrow 0.7447$ (四位有效数字)。\n- 召回率: $\\frac{35}{50} = 0.7000$ (四位有效数字)。\n- F1: 使用 $F1_{A} = \\frac{2 \\times TP_{A}}{2 TP_{A} + FP_{A} + FN_{A}} = \\frac{70}{97} \\approx 0.72164948 \\rightarrow 0.7216$。\n\n类别 $B$：\n- $TP_{B} = 26$。\n- $FP_{B} = 50 - 26 = 24$。\n- $FN_{B} = 40 - 26 = 14$。\n- 精确率: $\\frac{26}{50} = 0.5200$。\n- 召回率: $\\frac{26}{40} = 0.6500$。\n- F1: $F1_{B} = \\frac{52}{52+24+14} = \\frac{52}{90} = \\frac{26}{45} \\approx 0.57777778 \\rightarrow 0.5778$。\n\n类别 $C$：\n- $TP_{C} = 38$。\n- $FP_{C} = 61 - 38 = 23$。\n- $FN_{C} = 60 - 38 = 22$。\n- 精确率: $\\frac{38}{61} \\approx 0.62295082 \\rightarrow 0.6230$。\n- 召回率: $\\frac{38}{60} = \\frac{19}{30} \\approx 0.63333333 \\rightarrow 0.6333$。\n- F1: $F1_{C} = \\frac{76}{76+23+22} = \\frac{76}{121} \\approx 0.62809917 \\rightarrow 0.6281$。\n\n类别 $D$：\n- $TP_{D} = 31$。\n- $FP_{D} = 42 - 31 = 11$。\n- $FN_{D} = 50 - 31 = 19$。\n- 精确率: $\\frac{31}{42} \\approx 0.73809524 \\rightarrow 0.7381$。\n- 召回率: $\\frac{31}{50} = 0.6200$。\n- F1: $F1_{D} = \\frac{62}{62+11+19} = \\frac{62}{92} = \\frac{31}{46} \\approx 0.67391304 \\rightarrow 0.6739$。\n\n步骤 3：宏平均 F1。\n\n宏平均 F1 是四个类别 F1 分数的算术平均值：\n$$\n\\text{macro-F1} \\;=\\; \\frac{1}{4}\\left(F1_{A} + F1_{B} + F1_{C} + F1_{D}\\right)\n\\;=\\; \\frac{1}{4}\\left(\\frac{70}{97} + \\frac{26}{45} + \\frac{76}{121} + \\frac{31}{46}\\right).\n$$\n数值计算如下：\n- $\\frac{70}{97} \\approx 0.72164948$,\n- $\\frac{26}{45} \\approx 0.57777778$,\n- $\\frac{76}{121} \\approx 0.62809917$,\n- $\\frac{31}{46} \\approx 0.67391304$.\n\n和 $\\approx 0.72164948 + 0.57777778 + 0.62809917 + 0.67391304 = 2.60143947$。除以 $4$：\n$$\n\\text{macro-F1} \\;\\approx\\; 0.65035987 \\;\\rightarrow\\; 0.6504 \\text{ (rounded to four significant figures)}.\n$$\n\n因此，所要求的最终数值答案是四舍五入到四位有效数字的宏平均 F1 分数。",
            "answer": "$$\\boxed{0.6504}$$"
        },
        {
            "introduction": "除了评估现有解码器的性能，一个更深层次的问题是：神经编码本身对解码精度设定了怎样的基本限制？这个练习将引导你从第一性原理出发，为一种理想化的神经群体编码模型推导克拉默-拉奥下界（Cramér–Rao bound）。这个下界为任何无偏估计量的方差设定了一个理论上的最小值，从而让我们能够衡量任何实际解码器与理论最优性能的差距。",
            "id": "4154106",
            "problem": "考虑一个一维刺激 $s \\in \\mathbb{R}$，它由一个大的、条件独立的神经元群体编码。神经元 $i$ 的高斯调谐曲线为 $f_{i}(s) = A \\exp\\!\\big(- (s - \\mu_{i})^{2} / (2 \\sigma^{2})\\big)$，其中 $A  0$ 是观测窗口内的峰值平均脉冲计数，$\\sigma  0$ 是调谐宽度，$\\mu_{i} \\in \\mathbb{R}$ 是神经元 $i$ 的偏好刺激。在给定刺激 $s$ 的条件下，每个神经元在观测窗口内根据泊松点过程产生脉冲，其脉冲计数 $n_{i}$ 服从均值为 $f_{i}(s)$ 的泊松随机变量分布。偏好刺激 $\\mu_{i}$ 以均匀密度 $\\rho  0$ （神经元/单位刺激）平铺实数轴（即，在连续极限 $N \\to \\infty$ 下，对神经元的求和可以替换为 $\\rho$ 乘以对 $\\mu$ 的积分）。\n\n要求您使用统计估计的基本原理来分析调谐宽度对解码精度的影响。从泊松似然和费雪信息 (Fisher information) 的定义出发，并且不使用任何预先推导的简化表达式，推导在任意刺激值 $s = s^{\\star}$ 处对 $s$ 的任何无偏估计量的方差的克拉默-拉奥下界 (Cramér–Rao bound, CRB)。在整个过程中，假设一个科学上现实的模型：独立的泊松脉冲计数，其均值由上述调谐曲线给出，以及如前所述的密集平铺的神经元群体。请用 $A$、$\\sigma$ 和 $\\rho$ 将最终的 CRB 表示为单个封闭形式的解析表达式。不需要进行数值近似或四舍五入，并且最终答案中不应包含物理单位。您的最终答案必须是单个表达式。",
            "solution": "用户希望求出刺激 $s$ 的无偏估计量方差的克拉默-拉奥下界 (CRB)。CRB 由费雪信息 $I(s)$ 的倒数给出。我们将从基本原理出发，从观测到的脉冲计数的似然函数开始，推导费雪信息。\n\n问题指明，神经元 $i$ 的脉冲计数 $n_i$ 在给定刺激 $s$ 的条件下，服从均值为 $f_i(s)$ 的泊松分布。观测到脉冲计数 $n_i$ 的概率是：\n$$\nP(n_i | s) = \\frac{(f_i(s))^{n_i} \\exp(-f_i(s))}{n_i!}\n$$\n其中调谐曲线 $f_i(s)$ 由下式给出：\n$$\nf_i(s) = A \\exp\\left(-\\frac{(s - \\mu_i)^2}{2\\sigma^2}\\right)\n$$\n神经元是条件独立的。因此，从整个群体观测到一组脉冲计数 $\\{n_i\\}$ 的似然函数是各个概率的乘积：\n$$\nL(\\{n_i\\} | s) = \\prod_i P(n_i | s) = \\prod_i \\frac{(f_i(s))^{n_i} \\exp(-f_i(s))}{n_i!}\n$$\n为了计算费雪信息，使用对数似然函数 $\\ln L$ 更为方便：\n$$\n\\ln L(\\{n_i\\} | s) = \\ln \\left( \\prod_i \\frac{(f_i(s))^{n_i} \\exp(-f_i(s))}{n_i!} \\right)\n$$\n利用对数的性质，这可以简化为一个和式：\n$$\n\\ln L = \\sum_i \\left( n_i \\ln(f_i(s)) - f_i(s) - \\ln(n_i!) \\right)\n$$\n费雪信息 $I(s)$ 定义为对数似然函数关于参数 $s$ 的二阶导数的期望值的负数：\n$$\nI(s) = -E\\left[\\frac{\\partial^2}{\\partial s^2} \\ln L\\right]\n$$\n另外，对于一个性质良好的似然函数，它也可以表示为一阶导数平方的期望值：\n$$\nI(s) = E\\left[\\left(\\frac{\\partial}{\\partial s} \\ln L\\right)^2\\right]\n$$\n对于条件独立的观测，一个普遍的结论是总费雪信息是来自每个观测的信息之和：$I(s) = \\sum_i I_i(s)$。我们先计算单个神经元的信息 $I_i(s)$，然后对整个群体求和。\n\n对于单个神经元 $i$，对数似然是：\n$$\n\\ln L_i = n_i \\ln(f_i(s)) - f_i(s) - \\ln(n_i!)\n$$\n关于 $s$ 的一阶导数是：\n$$\n\\frac{\\partial}{\\partial s} \\ln L_i = n_i \\frac{f_i'(s)}{f_i(s)} - f_i'(s) = f_i'(s) \\left(\\frac{n_i}{f_i(s)} - 1\\right)\n$$\n其中 $f_i'(s) = \\frac{d f_i(s)}{ds}$。\n二阶导数是：\n$$\n\\frac{\\partial^2}{\\partial s^2} \\ln L_i = \\frac{\\partial}{\\partial s} \\left[ f_i'(s) \\left(\\frac{n_i}{f_i(s)} - 1\\right) \\right] = f_i''(s) \\left(\\frac{n_i}{f_i(s)} - 1\\right) - f_i'(s) \\left(\\frac{n_i f_i'(s)}{(f_i(s))^2}\\right)\n$$\n$$\n\\frac{\\partial^2}{\\partial s^2} \\ln L_i = \\frac{n_i f_i''(s)}{f_i(s)} - f_i''(s) - \\frac{n_i (f_i'(s))^2}{(f_i(s))^2}\n$$\n现在，我们对 $n_i$ 的分布取期望，它是一个均值为 $E[n_i] = f_i(s)$ 的泊松分布。\n$$\nE\\left[\\frac{\\partial^2}{\\partial s^2} \\ln L_i\\right] = \\frac{E[n_i] f_i''(s)}{f_i(s)} - f_i''(s) - \\frac{E[n_i] (f_i'(s))^2}{(f_i(s))^2}\n$$\n$$\nE\\left[\\frac{\\partial^2}{\\partial s^2} \\ln L_i\\right] = \\frac{f_i(s) f_i''(s)}{f_i(s)} - f_i''(s) - \\frac{f_i(s) (f_i'(s))^2}{(f_i(s))^2} = f_i''(s) - f_i''(s) - \\frac{(f_i'(s))^2}{f_i(s)} = -\\frac{(f_i'(s))^2}{f_i(s)}\n$$\n单个神经元的费雪信息是 $I_i(s) = -E\\left[\\frac{\\partial^2}{\\partial s^2} \\ln L_i\\right] = \\frac{(f_i'(s))^2}{f_i(s)}$。\n总费雪信息是所有神经元的总和：\n$$\nI(s) = \\sum_i I_i(s) = \\sum_i \\frac{(f_i'(s))^2}{f_i(s)}\n$$\n接下来，我们计算调谐曲线 $f_i(s)$ 的导数：\n$$\nf_i'(s) = \\frac{d}{ds} \\left[ A \\exp\\left(-\\frac{(s - \\mu_i)^2}{2\\sigma^2}\\right) \\right] = A \\exp\\left(-\\frac{(s - \\mu_i)^2}{2\\sigma^2}\\right) \\cdot \\left(-\\frac{2(s - \\mu_i)}{2\\sigma^2}\\right)\n$$\n$$\nf_i'(s) = - \\frac{s - \\mu_i}{\\sigma^2} f_i(s)\n$$\n将此代入 $I(s)$ 的表达式中：\n$$\nI(s) = \\sum_i \\frac{\\left(-\\frac{s - \\mu_i}{\\sigma^2} f_i(s)\\right)^2}{f_i(s)} = \\sum_i \\frac{\\frac{(s - \\mu_i)^2}{\\sigma^4} (f_i(s))^2}{f_i(s)} = \\sum_i \\frac{(s - \\mu_i)^2}{\\sigma^4} f_i(s)\n$$\n现在，代入 $f_i(s)$ 的表达式：\n$$\nI(s) = \\sum_i \\frac{(s - \\mu_i)^2}{\\sigma^4} A \\exp\\left(-\\frac{(s - \\mu_i)^2}{2\\sigma^2}\\right)\n$$\n问题陈述，在大量神经元的极限情况下，求和可以被一个关于偏好刺激 $\\mu$ 的积分所取代，并由密度 $\\rho$ 加权：\n$$\n\\sum_i g(\\mu_i) \\to \\int_{-\\infty}^{\\infty} \\rho \\, g(\\mu) \\, d\\mu\n$$\n将这个连续极限应用于我们对 $I(s)$ 的表达式：\n$$\nI(s) = \\rho \\int_{-\\infty}^{\\infty} \\frac{(s - \\mu)^2}{\\sigma^4} A \\exp\\left(-\\frac{(s - \\mu)^2}{2\\sigma^2}\\right) d\\mu\n$$\n我们可以将常数从积分中提出：\n$$\nI(s) = \\frac{A\\rho}{\\sigma^4} \\int_{-\\infty}^{\\infty} (s - \\mu)^2 \\exp\\left(-\\frac{(s - \\mu)^2}{2\\sigma^2}\\right) d\\mu\n$$\n为了求解这个积分，我们进行换元，令 $u = s - \\mu$，这意味着 $d\\mu = -du$。积分限从 $\\mu$ 的 $(-\\infty, \\infty)$ 变为 $u$ 的 $(\\infty, -\\infty)$。我们可以翻转积分限并吸收负号：\n$$\nI(s) = \\frac{A\\rho}{\\sigma^4} \\int_{-\\infty}^{\\infty} u^2 \\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right) du\n$$\n这是一个标准的高斯相关积分。它对应于一个均值为零、方差为 $\\sigma^2$ 的高斯分布的未归一化二阶矩。我们知道积分 $\\int_{-\\infty}^{\\infty} \\exp(-ax^2)dx = \\sqrt{\\pi/a}$。我们积分的值可以通过对一个参数求导来找到。令 $J(a) = \\int_{-\\infty}^{\\infty} \\exp(-au^2) du = \\sqrt{\\pi}a^{-1/2}$。那么\n$$\n\\int_{-\\infty}^{\\infty} u^2 \\exp(-au^2) du = -\\frac{d}{da} J(a) = -\\frac{d}{da}(\\sqrt{\\pi}a^{-1/2}) = -\\sqrt{\\pi}(-\\frac{1}{2}a^{-3/2}) = \\frac{\\sqrt{\\pi}}{2}a^{-3/2}\n$$\n在我们的例子中，$a = \\frac{1}{2\\sigma^2}$。代入这个值：\n$$\n\\int_{-\\infty}^{\\infty} u^2 \\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right) du = \\frac{\\sqrt{\\pi}}{2} \\left(\\frac{1}{2\\sigma^2}\\right)^{-3/2} = \\frac{\\sqrt{\\pi}}{2} (2\\sigma^2)^{3/2} = \\frac{\\sqrt{\\pi}}{2} 2^{3/2} (\\sigma^2)^{3/2} = \\frac{\\sqrt{\\pi}}{2} (2\\sqrt{2}) \\sigma^3 = \\sigma^3 \\sqrt{2\\pi}\n$$\n现在将这个结果代回到 $I(s)$ 的表达式中：\n$$\nI(s) = \\frac{A\\rho}{\\sigma^4} (\\sigma^3 \\sqrt{2\\pi}) = \\frac{A\\rho\\sqrt{2\\pi}}{\\sigma}\n$$\n注意，费雪信息 $I(s)$ 与刺激值 $s$ 无关。这是由于问题的平移对称性（偏好刺激在整个实数轴上均匀平铺）。因此，在任意刺激值 $s = s^{\\star}$ 处求值会得到相同的结果：$I(s^{\\star}) = I(s)$。\n\n克拉默-拉奥下界 (CRB) 指出，参数 $s$ 的任何无偏估计量 $\\hat{s}$ 的方差都有一个由费雪信息的倒数给出的下界：\n$$\n\\text{Var}(\\hat{s}) \\ge \\frac{1}{I(s)}\n$$\nCRB 就是这个下界的值。\n$$\n\\text{CRB} = \\frac{1}{I(s^{\\star})} = \\frac{1}{\\frac{A\\rho\\sqrt{2\\pi}}{\\sigma}} = \\frac{\\sigma}{A\\rho\\sqrt{2\\pi}}\n$$\n这就是克拉默-拉奥下界的最终封闭形式表达式。",
            "answer": "$$\\boxed{\\frac{\\sigma}{A \\rho \\sqrt{2 \\pi}}}$$"
        },
        {
            "introduction": "在处理高维神经数据时，降维是一个常见的预处理步骤，但这可能会导致信息损失。本练习探讨了主成分分析（PCA）对解码的实际影响，要求你从信息论的基本定义出发，推导并实现一个算法来量化降维所造成的信息损失。通过这个练习，你将学会如何在简化数据复杂性和保持解码保真度之间做出有根据的权衡。",
            "id": "4154077",
            "problem": "考虑一个由 $n$ 个神经元组成的群体，它们通过一个线性高斯模型编码一个 $d$ 维刺激。设刺激向量为 $\\mathbf{s} \\in \\mathbb{R}^d$，其服从多变量正态先验分布 $\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_s)$，其中 $\\mathbf{\\Sigma}_s \\in \\mathbb{R}^{d \\times d}$ 是一个正定矩阵。设神经响应向量为 $\\mathbf{r} \\in \\mathbb{R}^n$，定义为 $\\mathbf{r} = \\mathbf{A} \\mathbf{s} + \\mathbf{n}$，其中 $\\mathbf{A} \\in \\mathbb{R}^{n \\times d}$ 是一个固定的调谐矩阵，$\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_n)$ 是零均值高斯噪声，其协方差矩阵 $\\mathbf{\\Sigma}_n \\in \\mathbb{R}^{n \\times n}$ 是正定的。总响应协方差为 $\\mathbf{\\Sigma}_r = \\mathbf{A} \\mathbf{\\Sigma}_s \\mathbf{A}^\\top + \\mathbf{\\Sigma}_n$。\n\n对 $\\mathbf{\\Sigma}_r$ 应用主成分分析 (PCA) 以获得其特征分解 $\\mathbf{\\Sigma}_r = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^\\top$，其中 $\\mathbf{U} \\in \\mathbb{R}^{n \\times n}$ 的列是标准正交的特征向量，$\\mathbf{\\Lambda} \\in \\mathbb{R}^{n \\times n}$ 是对角矩阵，其对角线上的特征值按降序排列。投影到前 $k$ 个主成分使用 $\\mathbf{W}_k = \\mathbf{U}_{[:,1:k]} \\in \\mathbb{R}^{n \\times k}$，压缩后的观测值为 $\\mathbf{y}_k = \\mathbf{W}_k^\\top \\mathbf{r} \\in \\mathbb{R}^k$。\n\n使用互信息的基本定义 $I(\\mathbf{X}; \\mathbf{Y}) = H(\\mathbf{Y}) - H(\\mathbf{Y} \\mid \\mathbf{X})$ 和经过充分检验的多变量正态分布熵公式，定义刺激与响应之间的互信息 $I(\\mathbf{s}; \\mathbf{r})$，以及刺激与压缩响应之间的互信息 $I(\\mathbf{s}; \\mathbf{y}_k)$。从第一性原理出发，推导出一个算法，用于计算由降维引起的信息损失，\n$$\nL_k = I(\\mathbf{s}; \\mathbf{r}) - I(\\mathbf{s}; \\mathbf{y}_k),\n$$\n以自然对数单位（奈特）表示，并作为 $\\mathbf{A}$、$\\mathbf{\\Sigma}_s$、$\\mathbf{\\Sigma}_n$ 和 $k$ 的函数。您的推导必须从所述的核心定义和事实出发，不得假定任何专门的快捷公式，并且必须用高斯随机变量和线性变换的基本原理解释每一次变换。\n\n您必须将此算法实现在一个完整的、可运行的程序中，为下面的特定测试套件计算 $L_k$。所有对数都必须是自然对数，最终答案必须以奈特（nats）为单位，表示为实值浮点数。\n\n测试套件：\n$1.$ 场景 $1$ ($n = 5$, $d = 3$)。使用\n$$\n\\mathbf{A}_1 =\n\\begin{bmatrix}\n1.0  0.2  -0.3 \\\\\n0.5  1.1  0.0 \\\\\n0.0  -0.7  0.8 \\\\\n0.3  0.0  0.5 \\\\\n-0.6  0.4  0.2\n\\end{bmatrix},\\quad\n\\mathbf{\\Sigma}_{s,1} =\n\\begin{bmatrix}\n0.9  0.0  0.0 \\\\\n0.0  0.5  0.0 \\\\\n0.0  0.0  0.2\n\\end{bmatrix},\\quad\n\\mathbf{\\Sigma}_{n,1} =\n\\begin{bmatrix}\n0.35  0.02  0.00  0.01  0.00 \\\\\n0.02  0.40  0.03  0.00  0.01 \\\\\n0.00  0.03  0.45  0.02  0.00 \\\\\n0.01  0.00  0.02  0.38  0.02 \\\\\n0.00  0.01  0.00  0.02  0.50\n\\end{bmatrix}.\n$$\n计算 $k \\in \\{0, 1, 3, 5\\}$ 时的 $L_k$。\n\n$2.$ 场景 $2$ ($n = 4$, $d = 2$)。使用\n$$\n\\mathbf{A}_2 =\n\\begin{bmatrix}\n0.9  -0.1 \\\\\n0.4  0.6 \\\\\n-0.2  0.8 \\\\\n0.0  0.5\n\\end{bmatrix},\\quad\n\\mathbf{\\Sigma}_{s,2} =\n\\begin{bmatrix}\n1.5  0.3 \\\\\n0.3  0.7\n\\end{bmatrix},\\quad\n\\mathbf{\\Sigma}_{n,2} =\n\\begin{bmatrix}\n0.6  0.1  0.0  0.0 \\\\\n0.1  0.6  0.1  0.0 \\\\\n0.0  0.1  0.6  0.1 \\\\\n0.0  0.0  0.1  0.6\n\\end{bmatrix}.\n$$\n计算 $k \\in \\{0, 2, 4\\}$ 时的 $L_k$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，$[\\text{result}_1,\\text{result}_2,\\dots]$）。结果必须是上述测试用例的 $L_k$ 值，并严格按照指定的顺序排列：场景 $1$ 的 $k = 0$，场景 $1$ 的 $k = 1$，场景 $1$ 的 $k = 3$，场景 $1$ 的 $k = 5$，场景 $2$ 的 $k = 0$，场景 $2$ 的 $k = 2$，场景 $2$ 的 $k = 4$。",
            "solution": "所述问题在科学上是合理的，在形式上是完整的，并且在数学上是适定的。它基于信息论和线性高斯模型的基本原理，这些都是计算神经科学中的标准内容。所有必要的参数和定义都已提供，不存在内部矛盾或歧义。因此，我将进行完整的推导和求解。\n\n目标是推导一个计算信息损失 $L_k = I(\\mathbf{s}; \\mathbf{r}) - I(\\mathbf{s}; \\mathbf{y}_k)$ 的算法，该损失是由于将神经响应 $\\mathbf{r}$ 投影到其前 $k$ 个主成分而产生的。推导将基于第一性原理。\n\n两个随机向量 $\\mathbf{X}$ 和 $\\mathbf{Y}$ 之间的互信息定义为 $I(\\mathbf{X}; \\mathbf{Y}) = H(\\mathbf{Y}) - H(\\mathbf{Y} \\mid \\mathbf{X})$，其中 $H(\\cdot)$ 是微分熵。对于一个 $D$ 维多变量正态变量 $\\mathbf{Z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})$，其熵由 $H(\\mathbf{Z}) = \\frac{1}{2} \\ln \\det((2\\pi e) \\mathbf{\\Sigma}) = \\frac{D}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\mathbf{\\Sigma})$ 奈特（nats）给出。\n\n**步骤 1：推导 $I(\\mathbf{s}; \\mathbf{r})$**\n\n首先，我们确定 $\\mathbf{r}$ 的分布以及在给定 $\\mathbf{s}$ 条件下 $\\mathbf{r}$ 的分布。\n刺激 $\\mathbf{s}$ 和噪声 $\\mathbf{n}$ 是独立的零均值高斯变量：$\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_s)$ 和 $\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_n)$。\n响应为 $\\mathbf{r} = \\mathbf{A} \\mathbf{s} + \\mathbf{n}$。作为联合高斯变量的线性变换，$\\mathbf{r}$ 也是高斯分布的。\n$\\mathbf{r}$ 的均值为 $\\mathbb{E}[\\mathbf{r}] = \\mathbb{E}[\\mathbf{A} \\mathbf{s} + \\mathbf{n}] = \\mathbf{A}\\mathbb{E}[\\mathbf{s}] + \\mathbb{E}[\\mathbf{n}] = \\mathbf{0}$。\n$\\mathbf{r}$ 的协方差为：\n$$\n\\mathbf{\\Sigma}_r = \\text{Cov}(\\mathbf{r}) = \\mathbb{E}[\\mathbf{r}\\mathbf{r}^\\top] = \\mathbb{E}[(\\mathbf{A} \\mathbf{s} + \\mathbf{n})(\\mathbf{A} \\mathbf{s} + \\mathbf{n})^\\top]\n= \\mathbb{E}[\\mathbf{A}\\mathbf{s}\\mathbf{s}^\\top\\mathbf{A}^\\top + \\mathbf{A}\\mathbf{s}\\mathbf{n}^\\top + \\mathbf{n}\\mathbf{s}^\\top\\mathbf{A}^\\top + \\mathbf{n}\\mathbf{n}^\\top]\n$$\n由于 $\\mathbf{s}$ 和 $\\mathbf{n}$ 的独立性，交叉项的期望为零。因此，\n$$\n\\mathbf{\\Sigma}_r = \\mathbf{A}\\mathbb{E}[\\mathbf{s}\\mathbf{s}^\\top]\\mathbf{A}^\\top + \\mathbb{E}[\\mathbf{n}\\mathbf{n}^\\top] = \\mathbf{A}\\mathbf{\\Sigma}_s\\mathbf{A}^\\top + \\mathbf{\\Sigma}_n\n$$\n因此，响应的无条件分布为 $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_r)$。$\\mathbf{r} \\in \\mathbb{R}^n$ 的熵为：\n$$\nH(\\mathbf{r}) = \\frac{n}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\mathbf{\\Sigma}_r)\n$$\n在给定特定刺激 $\\mathbf{s}$ 的条件下，响应 $\\mathbf{r} = \\mathbf{A} \\mathbf{s} + \\mathbf{n}$ 是一个常数向量 $\\mathbf{A}\\mathbf{s}$ 和一个高斯噪声向量 $\\mathbf{n}$ 的和。\n条件分布为 $(\\mathbf{r} \\mid \\mathbf{s}) \\sim \\mathcal{N}(\\mathbf{A}\\mathbf{s}, \\mathbf{\\Sigma}_n)$。条件熵仅依赖于协方差，其值为：\n$$\nH(\\mathbf{r} \\mid \\mathbf{s}) = \\frac{n}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\mathbf{\\Sigma}_n)\n$$\n互信息是这些熵的差值：\n$$\nI(\\mathbf{s}; \\mathbf{r}) = H(\\mathbf{r}) - H(\\mathbf{r} \\mid \\mathbf{s}) = \\frac{1}{2} \\ln \\det(\\mathbf{\\Sigma}_r) - \\frac{1}{2} \\ln \\det(\\mathbf{\\Sigma}_n) = \\frac{1}{2} \\ln \\left( \\frac{\\det(\\mathbf{\\Sigma}_r)}{\\det(\\mathbf{\\Sigma}_n)} \\right)\n$$\n\n**步骤 2：推导 $I(\\mathbf{s}; \\mathbf{y}_k)$**\n\n压缩响应为 $\\mathbf{y}_k = \\mathbf{W}_k^\\top \\mathbf{r}$，是 $\\mathbf{r}$ 的一个线性投影。\n我们找出 $\\mathbf{y}_k$ 的分布以及在给定 $\\mathbf{s}$ 条件下 $\\mathbf{y}_k$ 的分布。\n由于 $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_r)$，投影向量 $\\mathbf{y}_k \\in \\mathbb{R}^k$ 也是高斯分布的，其均值为 $\\mathbb{E}[\\mathbf{y}_k] = \\mathbf{W}_k^\\top \\mathbb{E}[\\mathbf{r}] = \\mathbf{0}$ 且协方差为 $\\mathbf{\\Sigma}_{y_k} = \\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k$。\n因此，$\\mathbf{y}_k \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k)$。其熵为：\n$$\nH(\\mathbf{y}_k) = \\frac{k}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k)\n$$\n在给定 $\\mathbf{s}$ 的条件下，我们有 $(\\mathbf{r} \\mid \\mathbf{s}) \\sim \\mathcal{N}(\\mathbf{A}\\mathbf{s}, \\mathbf{\\Sigma}_n)$。投影向量 $(\\mathbf{y}_k \\mid \\mathbf{s})$ 是高斯分布的，其均值为 $\\mathbb{E}[\\mathbf{y}_k \\mid \\mathbf{s}] = \\mathbf{W}_k^\\top \\mathbb{E}[\\mathbf{r} \\mid \\mathbf{s}] = \\mathbf{W}_k^\\top \\mathbf{A}\\mathbf{s}$ 且协方差为 $\\mathbf{\\Sigma}_{y_k \\mid s} = \\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k$。\n条件熵为：\n$$\nH(\\mathbf{y}_k \\mid \\mathbf{s}) = \\frac{k}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)\n$$\n压缩响应的互信息为：\n$$\nI(\\mathbf{s}; \\mathbf{y}_k) = H(\\mathbf{y}_k) - H(\\mathbf{y}_k \\mid \\mathbf{s}) = \\frac{1}{2} \\ln \\left( \\frac{\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k)}{\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)} \\right)\n$$\n\n**步骤 3：推导信息损失 $L_k$**\n\n信息损失是两个互信息量之差：\n$$\nL_k = I(\\mathbf{s}; \\mathbf{r}) - I(\\mathbf{s}; \\mathbf{y}_k) = \\frac{1}{2} \\ln \\left( \\frac{\\det(\\mathbf{\\Sigma}_r)}{\\det(\\mathbf{\\Sigma}_n)} \\right) - \\frac{1}{2} \\ln \\left( \\frac{\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k)}{\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)} \\right)\n$$\n$$\nL_k = \\frac{1}{2} \\left[ \\ln(\\det(\\mathbf{\\Sigma}_r)) - \\ln(\\det(\\mathbf{\\Sigma}_n)) - \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k)) + \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)) \\right]\n$$\n这个表达式可以被简化。矩阵 $\\mathbf{W}_k$ 由 $\\mathbf{\\Sigma}_r = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^\\top$ 的前 $k$ 个标准正交特征向量组成。设 $\\mathbf{U} = [\\mathbf{u}_1, \\dots, \\mathbf{u}_n]$，则 $\\mathbf{W}_k = [\\mathbf{u}_1, \\dots, \\mathbf{u}_k]$。\n项 $\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k$ 可简化为由 $\\mathbf{\\Sigma}_r$ 的前 $k$ 个特征值构成的 $k \\times k$ 对角矩阵：$\\mathbf{\\Lambda}_k = \\text{diag}(\\lambda_1, \\dots, \\lambda_k)$。\n因此，$\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_r \\mathbf{W}_k) = \\prod_{i=1}^k \\lambda_i$。\n同样，$\\det(\\mathbf{\\Sigma}_r) = \\det(\\mathbf{\\Lambda}) = \\prod_{i=1}^n \\lambda_i$。\n将这些代入 $L_k$ 的表达式中：\n$$\nL_k = \\frac{1}{2} \\left[ \\ln\\left(\\prod_{i=1}^n \\lambda_i\\right) - \\ln(\\det(\\mathbf{\\Sigma}_n)) - \\ln\\left(\\prod_{i=1}^k \\lambda_i\\right) + \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)) \\right]\n$$\n使用属性 $\\ln(ab) = \\ln(a) + \\ln(b)$，上式变为：\n$$\nL_k = \\frac{1}{2} \\left[ \\sum_{i=1}^n \\ln(\\lambda_i) - \\ln(\\det(\\mathbf{\\Sigma}_n)) - \\sum_{i=1}^k \\ln(\\lambda_i) + \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)) \\right]\n$$\n$$\nL_k = \\frac{1}{2} \\left[ \\sum_{i=k+1}^n \\ln(\\lambda_i) - \\ln(\\det(\\mathbf{\\Sigma}_n)) + \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)) \\right]\n$$\n这就是信息损失的最终公式。\n\n特殊情况：\n- 当 $k=n$ 时，$\\mathbf{W}_n = \\mathbf{U}$。求和项为空（值为 $0$）。$\\det(\\mathbf{U}^\\top \\mathbf{\\Sigma}_n \\mathbf{U}) = \\det(\\mathbf{\\Sigma}_n)$。因此，$L_n = \\frac{1}{2}[0 - \\ln(\\det(\\mathbf{\\Sigma}_n)) + \\ln(\\det(\\mathbf{\\Sigma}_n))] = 0$，符合预期。\n- 当 $k=0$ 时，$\\mathbf{y}_0$ 是一个零维向量，不携带任何信息，所以 $I(\\mathbf{s}; \\mathbf{y}_0)=0$。损失为 $L_0 = I(\\mathbf{s}; \\mathbf{r})$。公式给出 $L_0 = \\frac{1}{2} [\\sum_{i=1}^n \\ln(\\lambda_i) - \\ln(\\det(\\mathbf{\\Sigma}_n))]$。由于 $\\sum_{i=1}^n \\ln(\\lambda_i) = \\ln(\\det(\\mathbf{\\Sigma}_r))$，这正确地恢复了 $L_0 = \\frac{1}{2} [\\ln(\\det(\\mathbf{\\Sigma}_r)) - \\ln(\\det(\\mathbf{\\Sigma}_n))]$。在实现中，对于 $k=0$，项 $\\det(\\mathbf{W}_0^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_0)$ 是一个 $0 \\times 0$ 矩阵的行列式，其值为 $1$，其对数为 $0$。\n\n算法如下：\n1. 构建 $\\mathbf{\\Sigma}_r = \\mathbf{A} \\mathbf{\\Sigma}_s \\mathbf{A}^\\top + \\mathbf{\\Sigma}_n$。\n2. 对 $\\mathbf{\\Sigma}_r$ 进行特征分解，以找到其特征值 $\\lambda_i$ 和特征向量 $\\mathbf{u}_i$，并按 $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_n$ 的顺序排序。\n3. 对于每个给定的 $k$：\n    a. 如果 $k=0$，计算 $L_0 = \\frac{1}{2} [\\sum_{i=1}^n \\ln(\\lambda_i) - \\ln(\\det(\\mathbf{\\Sigma}_n))]$。\n    b. 如果 $k>0$，构建 $\\mathbf{W}_k = [\\mathbf{u}_1, \\dots, \\mathbf{u}_k]$。\n    c. 使用最终公式计算 $L_k$： $L_k = \\frac{1}{2} [ \\sum_{i=k+1}^n \\ln(\\lambda_i) - \\ln(\\det(\\mathbf{\\Sigma}_n)) + \\ln(\\det(\\mathbf{W}_k^\\top \\mathbf{\\Sigma}_n \\mathbf{W}_k)) ]$。\n所有对数均为自然对数。",
            "answer": "```python\nimport numpy as np\n\ndef compute_information_loss(A, Sigma_s, Sigma_n, k_values):\n    \"\"\"\n    Computes the information loss L_k for a set of k values.\n\n    Args:\n        A (np.ndarray): The tuning matrix.\n        Sigma_s (np.ndarray): The stimulus covariance matrix.\n        Sigma_n (np.ndarray): The noise covariance matrix.\n        k_values (list): A list of integers k for which to compute L_k.\n\n    Returns:\n        list: A list of L_k values for each k in k_values.\n    \"\"\"\n    n, d = A.shape\n\n    # Step 1: Construct the total response covariance matrix Sigma_r\n    # Sigma_r = A @ Sigma_s @ A.T + Sigma_n\n    Sigma_r = A.dot(Sigma_s).dot(A.T) + Sigma_n\n\n    # Step 2: Eigendecomposition of Sigma_r\n    # np.linalg.eigh returns eigenvalues in ascending order. We need descending.\n    eigenvalues, eigenvectors = np.linalg.eigh(Sigma_r)\n    \n    # Sort in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    lambdas = eigenvalues[sorted_indices]\n    U = eigenvectors[:, sorted_indices]\n\n    # Pre-compute terms that are constant for all k\n    log_lambdas = np.log(lambdas)\n    log_det_Sigma_n = np.log(np.linalg.det(Sigma_n))\n    \n    results = []\n    \n    # Step 3: Compute L_k for each k\n    for k in k_values:\n        if k  0 or k > n:\n            raise ValueError(f\"k must be between 0 and n (inclusive), but got k={k}\")\n\n        if k == 0:\n            # Case k=0: L_0 = I(s;r) = 0.5 * (log(det(Sigma_r)) - log(det(Sigma_n)))\n            # log(det(Sigma_r)) = log(product of eigenvalues) = sum of log eigenvalues\n            L_k = 0.5 * (np.sum(log_lambdas) - log_det_Sigma_n)\n        else:\n            # Case k>0:\n            # L_k = 0.5 * [ sum_{i=k+1 to n} log(lambda_i) \n            #               - log(det(Sigma_n)) \n            #               + log(det(W_k.T @ Sigma_n @ W_k)) ]\n            \n            # Sum of log-eigenvalues for components being discarded\n            sum_log_lambdas_discarded = np.sum(log_lambdas[k:])\n            \n            # Construct the projection matrix W_k\n            W_k = U[:, :k]\n            \n            # Covariance of the projected noise\n            proj_noise_cov = W_k.T @ Sigma_n @ W_k\n            \n            # Log-determinant of projected noise covariance\n            # For k=n, this term equals log_det_Sigma_n and cancels out.\n            log_det_proj_noise_cov = np.log(np.linalg.det(proj_noise_cov))\n            \n            L_k = 0.5 * (sum_log_lambdas_discarded - log_det_Sigma_n + log_det_proj_noise_cov)\n            \n        results.append(L_k)\n        \n    return results\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite and prints the results.\n    \"\"\"\n    # Test Suite 1\n    A1 = np.array([\n        [1.0, 0.2, -0.3],\n        [0.5, 1.1, 0.0],\n        [0.0, -0.7, 0.8],\n        [0.3, 0.0, 0.5],\n        [-0.6, 0.4, 0.2]\n    ])\n    Sigma_s1 = np.array([\n        [0.9, 0.0, 0.0],\n        [0.0, 0.5, 0.0],\n        [0.0, 0.0, 0.2]\n    ])\n    Sigma_n1 = np.array([\n        [0.35, 0.02, 0.00, 0.01, 0.00],\n        [0.02, 0.40, 0.03, 0.00, 0.01],\n        [0.00, 0.03, 0.45, 0.02, 0.00],\n        [0.01, 0.00, 0.02, 0.38, 0.02],\n        [0.00, 0.01, 0.00, 0.02, 0.50]\n    ])\n    k_values1 = [0, 1, 3, 5]\n    \n    # Test Suite 2\n    A2 = np.array([\n        [0.9, -0.1],\n        [0.4, 0.6],\n        [-0.2, 0.8],\n        [0.0, 0.5]\n    ])\n    Sigma_s2 = np.array([\n        [1.5, 0.3],\n        [0.3, 0.7]\n    ])\n    Sigma_n2 = np.array([\n        [0.6, 0.1, 0.0, 0.0],\n        [0.1, 0.6, 0.1, 0.0],\n        [0.0, 0.1, 0.6, 0.1],\n        [0.0, 0.0, 0.1, 0.6]\n    ])\n    k_values2 = [0, 2, 4]\n\n    # Calculate results\n    results1 = compute_information_loss(A1, Sigma_s1, Sigma_n1, k_values1)\n    results2 = compute_information_loss(A2, Sigma_s2, Sigma_n2, k_values2)\n    \n    all_results = results1 + results2\n    \n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in all_results)}]\")\n\nsolve()\n```"
        }
    ]
}