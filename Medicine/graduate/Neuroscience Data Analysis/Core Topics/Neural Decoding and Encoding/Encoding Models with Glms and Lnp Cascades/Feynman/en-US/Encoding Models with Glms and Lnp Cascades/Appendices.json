{
    "hands_on_practices": [
        {
            "introduction": "The first step in building a powerful encoding model is often designing the structure of its components. This practice focuses on constructing temporal filters, a core element of LNP/GLM models that captures how a neuron's response depends on recent stimulus history. You will implement a widely used technique: approximating a filter using a set of raised-cosine basis functions, which provides a flexible yet low-dimensional representation that is crucial for robust model fitting. ",
            "id": "4158967",
            "problem": "Consider an encoding model in neuroscience where a temporal kernel is used in either a Generalized Linear Model (GLM, Generalized Linear Model) or a Linear-Nonlinear-Poisson (LNP, Linear-Nonlinear-Poisson) cascade. The temporal kernel $k(t)$ is to be approximated by a low-dimensional expansion in temporal basis functions. Your task is to construct a family of raised-cosine temporal basis functions on a logarithmically stretched time axis and compute the least-squares ridge approximation of a known ground-truth kernel, then report the relative squared-error in the continuous-time limit via a discrete approximation.\n\nUse the following definitions and setup, which are standard in encoding models but require explicit implementation details:\n\n1. Time grid and kernel:\n   - Let the total duration be $T = 0.2$ seconds and the time step be $\\Delta t = 0.001$ seconds. Define sample times $t_n = n \\Delta t$ for $n = 0, 1, \\ldots, N$ where $N = T / \\Delta t$.\n   - Define the ground-truth temporal kernel on this grid by $k_{\\text{true}}(t) = t \\exp(-t / \\tau)$ with time constant $\\tau = 0.03$ seconds.\n\n2. Logarithmic time-warp and raised-cosine basis construction:\n   - For a chosen offset $c > 0$, define the stretched coordinate $\\psi(t) = \\log(t + c)$.\n   - For a chosen number of basis functions $M \\geq 2$, define the range $\\psi_{\\min} = \\log(0 + c)$ and $\\psi_{\\max} = \\log(T + c)$. Define $M$ centers $\\{ \\mu_j \\}_{j=1}^M$ that are linearly spaced between $\\psi_{\\min}$ and $\\psi_{\\max}$, inclusive. Let the spacing be $\\Delta_{\\psi} = \\mu_{j+1} - \\mu_j$ (which is constant when $M \\geq 2$).\n   - Define the $j$-th basis function on the original time axis by\n     $$\n     b_j(t) =\n     \\begin{cases}\n     \\dfrac{1}{2} \\left( 1 + \\cos\\left( \\pi \\dfrac{\\psi(t) - \\mu_j}{\\Delta_{\\psi}} \\right) \\right), & \\text{if } \\left| \\psi(t) - \\mu_j \\right| \\le \\Delta_{\\psi}, \\\\\n     0, & \\text{otherwise}.\n     \\end{cases}\n     $$\n   - For each $t_n$ on the grid, form the design matrix $B \\in \\mathbb{R}^{(N+1) \\times M}$ with entries $B_{n j} = b_j(t_n)$ for $j=1,\\ldots,M$.\n\n3. Ridge-regularized least squares projection:\n   - For a given ridge parameter $\\lambda \\ge 0$, define the coefficient vector $w \\in \\mathbb{R}^M$ that minimizes the discretized objective\n     $$\n     J(w) = \\sum_{n=0}^{N} \\left( k_{\\text{true}}(t_n) - \\sum_{j=1}^{M} w_j \\, b_j(t_n) \\right)^2 \\Delta t + \\lambda \\sum_{j=1}^{M} w_j^2.\n     $$\n   - The minimizer satisfies the normal equations\n     $$\n     \\left( B^\\top B \\, \\Delta t + \\lambda I \\right) w = B^\\top k \\, \\Delta t,\n     $$\n     where $k \\in \\mathbb{R}^{N+1}$ has entries $k_n = k_{\\text{true}}(t_n)$ and $I$ is the $M \\times M$ identity matrix.\n\n4. Reconstruction and error metric:\n   - Define the reconstructed kernel $\\hat{k}(t_n) = \\sum_{j=1}^{M} w_j \\, b_j(t_n)$ for all $n$.\n   - Compute the relative $L^2$ error using the Riemann sum approximation of the continuous $L^2$ norm:\n     $$\n     E = \\dfrac{\\left\\| \\hat{k} - k \\right\\|_{2,\\Delta t}}{\\left\\| k \\right\\|_{2,\\Delta t}} = \\dfrac{\\sqrt{ \\sum_{n=0}^{N} \\left( \\hat{k}(t_n) - k_{\\text{true}}(t_n) \\right)^2 \\Delta t }}{\\sqrt{ \\sum_{n=0}^{N} \\left( k_{\\text{true}}(t_n) \\right)^2 \\Delta t }}.\n     $$\n\nImplement a program that performs the above steps for each parameter triple $(M, \\lambda, c)$ in the following test suite and returns the corresponding $E$ values:\n\n- Test case $1$: $M = 8$, $\\lambda = 0.0$, $c = 0.005$.\n- Test case $2$: $M = 3$, $\\lambda = 0.0$, $c = 0.005$.\n- Test case $3$: $M = 8$, $\\lambda = 0.001$, $c = 0.005$.\n- Test case $4$: $M = 8$, $\\lambda = 0.0$, $c = 0.0005$.\n- Test case $5$: $M = 12$, $\\lambda = 0.1$, $c = 0.005$.\n\nYour program must:\n- Use the exact grid specified above with $T = 0.2$ and $\\Delta t = 0.001$ seconds.\n- Construct the raised-cosine basis as defined, without any additional normalization or modification.\n- Solve for $w$ for each test case and compute $E$ exactly as defined.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each $E$ rounded to six decimal places (for example, $[0.123456,0.234567,0.345678,0.456789,0.567890]$).\n\nThere is no randomness in this task, and no physical unit is required for the final error values. The angle unit is not applicable. The outputs for the test suite are real numbers, and the final program output must be a single line in the specified format aggregating all test cases in order.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of definitions and parameters to solve a standard problem in computational neuroscience: approximating a temporal kernel using a basis function expansion. The solution requires implementing the specified numerical procedure.\n\nThe solution proceeds in four main steps: setting up the discrete time grid and the ground-truth kernel, constructing the design matrix from the raised-cosine basis functions, solving the ridge regression problem for the basis weights, and finally, computing the relative approximation error.\n\nFirst, we establish the discrete time domain. The total duration is $T = 0.2$ seconds with a time step of $\\Delta t = 0.001$ seconds. This defines a grid of $N+1$ time points $t_n = n \\Delta t$, where $N = T/\\Delta t = 200$, so $n$ ranges from $0$ to $200$. The time points are thus $t = \\{0, 0.001, 0.002, \\ldots, 0.2\\}$. On this grid, we evaluate the ground-truth kernel, a vector $k \\in \\mathbb{R}^{N+1}$, with entries $k_n = k_{\\text{true}}(t_n) = t_n \\exp(-t_n / \\tau)$ using the given time constant $\\tau = 0.03$ seconds.\n\nSecond, for each test case defined by a parameter triple $(M, \\lambda, c)$, we construct the corresponding design matrix $B \\in \\mathbb{R}^{(N+1) \\times M}$. This involves a logarithmic warping of the time axis. The warped coordinate is defined as $\\psi(t) = \\log(t + c)$, where $c$ is a small positive offset. The range of this warped coordinate over our time domain is $[\\psi_{\\min}, \\psi_{\\max}]$, where $\\psi_{\\min} = \\log(c)$ and $\\psi_{\\max} = \\log(T + c)$. We define $M$ centers, $\\{\\mu_j\\}_{j=1}^M$, which are linearly spaced over this interval, inclusive of the endpoints. The constant spacing between adjacent centers, for $M \\ge 2$, is $\\Delta_{\\psi} = (\\psi_{\\max} - \\psi_{\\min}) / (M - 1)$.\n\nThe $j$-th basis function, $b_j(t)$, is a raised cosine centered at $\\mu_j$ in the warped $\\psi$-space. Its value at time $t$ is given by\n$$\nb_j(t) = \\frac{1}{2} \\left( 1 + \\cos\\left( \\pi \\frac{\\psi(t) - \\mu_j}{\\Delta_{\\psi}} \\right) \\right)\n$$\nif its argument is within its support, specifically when $|\\psi(t) - \\mu_j| \\le \\Delta_{\\psi}$, and $b_j(t) = 0$ otherwise. The design matrix $B$ is populated by evaluating each of the $M$ basis functions at each of the $N+1$ time points, such that $B_{nj} = b_j(t_n)$.\n\nThird, we find the optimal weight vector $w \\in \\mathbb{R}^M$ that approximates the ground-truth kernel as a linear combination of the basis functions, $\\hat{k} = Bw$. The weights are determined by minimizing a ridge-regularized least-squares objective function. This minimization leads to the normal equations, a system of linear equations for $w$:\n$$\n\\left( B^\\top B \\, \\Delta t + \\lambda I \\right) w = B^\\top k \\, \\Delta t\n$$\nHere, $B^\\top$ is the transpose of $B$, $I$ is the $M \\times M$ identity matrix, and $\\lambda$ is the ridge regularization parameter. This linear system is of the form $A w = b_{rhs}$, where $A = (B^\\top B \\, \\Delta t + \\lambda I)$ and $b_{rhs} = (B^\\top k \\, \\Delta t)$. We solve this system for $w$ using standard numerical linear algebra methods.\n\nFourth, with the optimal weights $w$ computed, we form the reconstructed kernel $\\hat{k}$ as a vector of values $\\hat{k}_n = \\hat{k}(t_n) = \\sum_{j=1}^{M} w_j b_j(t_n)$. The error of this approximation is quantified by the relative $L^2$ error, $E$, which is the ratio of the norms of the error vector and the true kernel vector. In its discretized form, this is:\n$$\nE = \\frac{\\left\\| \\hat{k} - k \\right\\|_{2,\\Delta t}}{\\left\\| k \\right\\|_{2,\\Delta t}} = \\frac{\\sqrt{ \\sum_{n=0}^{N} ( \\hat{k}_n - k_n )^2 \\Delta t }}{\\sqrt{ \\sum_{n=0}^{N} k_n^2 \\Delta t }}\n$$\nThe factor $\\sqrt{\\Delta t}$ appears in both the numerator and the denominator, so they cancel. The error can be computed simply as the ratio of the standard Euclidean norms of the vectors $(\\hat{k} - k)$ and $k$. This calculation is performed for each test case, and the resulting error values are collected.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the relative squared-error for ridge-regularized least-squares\n    approximations of a temporal kernel using raised-cosine basis functions.\n    \"\"\"\n\n    # 1. Define global parameters and ground-truth kernel\n    T = 0.2\n    dt = 0.001\n    N = int(T / dt)\n    t = np.linspace(0, T, N + 1)\n    \n    tau = 0.03\n    k_true = t * np.exp(-t / tau)\n\n    def calculate_approximation_error(params):\n        \"\"\"\n        Calculates the error for a single set of parameters (M, lam, c).\n        \"\"\"\n        M, lam, c = params\n        \n        # 2. Construct raised-cosine basis functions\n        psi = lambda time: np.log(time + c)\n        psi_t = psi(t)\n        \n        psi_min = psi(0.0)\n        psi_max = psi(T)\n\n        mu_centers = np.linspace(psi_min, psi_max, M)\n        \n        if M < 2:\n            # The problem states M >= 2. If M=1, delta_psi is ill-defined.\n            # A reasonable value could be psi_max - psi_min, but we follow the spec.\n            # This path will not be taken by the test cases.\n            raise ValueError(\"M must be >= 2 for this basis construction.\")\n        \n        delta_psi = mu_centers[1] - mu_centers[0]\n\n        # Populate the design matrix B\n        B = np.zeros((N + 1, M))\n        for j in range(M):\n            mu_j = mu_centers[j]\n            # Argument for the cosine, normalized by required spacing\n            x = (psi_t - mu_j) / delta_psi\n            \n            # Basis is non-zero only where |x| <= 1\n            valid_indices = np.abs(x) <= 1\n            \n            # Calculate basis function values on its support\n            B[valid_indices, j] = 0.5 * (1 + np.cos(np.pi * x[valid_indices]))\n\n        # 3. Solve for weights using ridge regression normal equations\n        # (B.T @ B * dt + lam * I) @ w = B.T @ k_true * dt\n        identity_M = np.eye(M)\n        A = B.T @ B * dt + lam * identity_M\n        b_rhs = B.T @ k_true * dt\n        \n        w = np.linalg.solve(A, b_rhs)\n        \n        # 4. Reconstruct the kernel and compute the error\n        k_hat = B @ w\n        \n        # The error E is the ratio of L2 norms. The sqrt(dt) term cancels.\n        norm_error_vec = np.linalg.norm(k_hat - k_true)\n        norm_true_vec = np.linalg.norm(k_true)\n        \n        if norm_true_vec == 0:\n            return 0.0 if norm_error_vec == 0 else np.inf\n        \n        error = norm_error_vec / norm_true_vec\n        return error\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (8, 0.0, 0.005),\n        (3, 0.0, 0.005),\n        (8, 0.001, 0.005),\n        (8, 0.0, 0.0005),\n        (12, 0.1, 0.005),\n    ]\n\n    results = []\n    for case in test_cases:\n        error = calculate_approximation_error(case)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once the model's structure is defined, the central task is to estimate its parameters from neural data. This exercise guides you through implementing a complete maximum likelihood estimator for a Poisson GLM from the ground up, using the powerful Newton's method for optimization. By deriving and coding the gradient and Hessian, you will gain a fundamental understanding of the fitting procedure that is at the core of neural encoding models. ",
            "id": "4158982",
            "problem": "You are to implement and test maximum likelihood estimation for a Generalized Linear Model (GLM) with Poisson observations and the canonical logarithm link, in the context of an encoding model that includes a Linear-Nonlinear-Poisson (LNP) cascade. Your implementation must be principled, starting from the definitions of the Poisson distribution and the GLM framework, and it must use Newton's method with a backtracking line search that guarantees monotonic improvement of the objective. You must also verify the convexity property of the objective via the Hessian at the solution. The program must be self-contained and produce deterministic results.\n\nFundamental base:\n- A Poisson random variable $y$ with rate parameter $\\lambda$ has probability mass function $p(y \\mid \\lambda) = \\exp(-\\lambda)\\,\\lambda^{y}/y!$, for $y \\in \\{0,1,2,\\dots\\}$ and $\\lambda > 0$.\n- In a Generalized Linear Model (GLM), the conditional expectation $E[y \\mid x]$ is linked to a linear predictor $x^{\\top}\\beta$ via a link function. For the canonical logarithm link in the Poisson GLM, the mean rate is $\\lambda = \\exp(x^{\\top}\\beta)$.\n- In the Linear-Nonlinear-Poisson (LNP) cascade, the drive is obtained from a linear spatiotemporal filter applied to the stimulus, followed by a pointwise nonlinearity that maps the drive to a firing rate, and a Poisson spike generation process.\n\nTasks to implement:\n1. Given a design matrix $X \\in \\mathbb{R}^{T \\times p}$ and count observations $y \\in \\mathbb{N}^{T}$, derive the negative log-likelihood objective for the Poisson GLM with logarithm link using the independence assumption across observations, and implement Newton's method with backtracking line search to minimize this objective. Your derivation must begin from the Poisson probability mass function and the GLM definition of the canonical log link, without assuming any \"shortcut\" formulas.\n2. Use your estimator to fit $\\hat{\\beta}$ and compute required quantities for a specified test suite. You must ensure numerical robustness by employing a positive damping term for the Hessian in Newton updates and a backtracking line search to guarantee decrease of the objective at each accepted step.\n3. For a collinearity edge case, compute the Hessian of the negative log-likelihood at the fitted solution $\\hat{\\beta}$ and return a boolean indicating whether it is positive semidefinite, using an eigenvalue-based criterion with a small numerical tolerance.\n4. For an LNP cascade case, construct a design matrix from temporal stimulus lags and recover the linear filter coefficients and the bias term through GLM fitting.\n\nData generation and test suite:\nAll random quantities must be generated using a specified seed with independent draws as indicated below, and all matrix constructions must be deterministic given these seeds. There are four test cases:\n\n- Case A (happy path; multivariate covariates with intercept):\n  - Let $T = 80$, number of features $F = 3$, and random seed $42$.\n  - Draw $X_{\\text{base}} \\in \\mathbb{R}^{T \\times F}$ with entries independently from $\\text{Uniform}(-0.5, 0.5)$, add an intercept column of ones to obtain $X \\in \\mathbb{R}^{T \\times (F+1)}$.\n  - Let the true coefficient be $\\beta_{\\text{true}} = [0.4, -0.6, 0.5, 0.2]^{\\top}$ (the last entry is the intercept).\n  - Generate counts $y_t \\sim \\text{Poisson}(\\lambda_t)$ independently with $\\lambda_t = \\exp(x_t^{\\top}\\beta_{\\text{true}})$ for $t = 1,\\dots,T$.\n  - Fit the GLM to obtain $\\hat{\\beta}$ and compute the predicted rate for the query vector $\\tilde{x} = [0.5, -1.0, 0.8, 1]^{\\top}$ as $\\hat{\\lambda} = \\exp(\\tilde{x}^{\\top}\\hat{\\beta})$. Return $\\hat{\\lambda}$ as a float.\n\n- Case B (boundary case; intercept-only model):\n  - Let $T = 60$ and random seed $123$.\n  - Use $X \\in \\mathbb{R}^{T \\times 1}$ consisting of a single intercept column of ones.\n  - Let the true constant rate be $\\lambda = 1.8$; generate $y_t \\sim \\text{Poisson}(\\lambda)$ independently for $t = 1,\\dots,T$.\n  - Fit the GLM to obtain $\\hat{\\beta} \\in \\mathbb{R}$ (intercept-only). Return the scalar intercept estimate $\\hat{\\beta}$ as a float.\n\n- Case C (edge case; near collinearity):\n  - Let $T = 100$, $F = 2$, and random seed $21$.\n  - Draw $x^{(1)}_t \\sim \\text{Uniform}(-0.5, 0.5)$ independently. Set $x^{(2)}_t = x^{(1)}_t + \\epsilon_t$ with $\\epsilon_t \\sim \\text{Normal}(0, 10^{-4})$ independently. Form $X_{\\text{base}}$ with columns $x^{(1)}$ and $x^{(2)}$ and add an intercept column of ones to obtain $X \\in \\mathbb{R}^{T \\times 3}$.\n  - Let $\\beta_{\\text{true}} = [0.7, 0.7, -0.1]^{\\top}$ (two feature weights and one intercept).\n  - Generate counts $y_t \\sim \\text{Poisson}(\\lambda_t)$ independently with $\\lambda_t = \\exp(x_t^{\\top}\\beta_{\\text{true}})$.\n  - Fit the GLM to obtain $\\hat{\\beta}$, then compute the Hessian $H(\\hat{\\beta})$ of the negative log-likelihood at $\\hat{\\beta}$. Return a boolean indicating whether $H(\\hat{\\beta})$ is positive semidefinite, defined as all eigenvalues being greater than or equal to $-10^{-8}$.\n\n- Case D (LNP cascade; temporal filter recovery):\n  - Let $T = 150$, filter length $L = 5$, and random seed $7$.\n  - Draw stimulus samples $s_t \\sim \\text{Uniform}(-1, 1)$ independently for $t = 1,\\dots,T$.\n  - Let the true filter be $k_{\\text{true}} = [0.5, -0.2, 0.15, 0.0, -0.1]^{\\top}$ and bias $b = -0.05$.\n  - For each valid time index $t = L,\\dots,T$, construct a row of the design matrix with stimulus lags $[s_t, s_{t-1}, \\dots, s_{t-L+1}]$ and append an intercept $1$, yielding $X \\in \\mathbb{R}^{(T-L+1) \\times (L+1)}$.\n  - Generate counts $y_t \\sim \\text{Poisson}(\\lambda_t)$ independently with $\\lambda_t = \\exp(k_{\\text{true}}^{\\top}\\ell_t + b)$, where $\\ell_t$ is the vector of lags at time $t$.\n  - Fit the GLM to obtain $\\hat{\\beta} = [\\hat{k}^{\\top}, \\hat{b}]^{\\top}$ and return the list of filter coefficients $\\hat{k}$ as a list of floats of length $L$.\n\nOutput specification:\n- Your program must produce a single line of output containing the results of the four cases, in order A, B, C, D, as a comma-separated list enclosed in square brackets. The first three entries must be floats for Case A and Case B and a boolean for Case C, and the fourth entry must be a list of floats for Case D. For example, the format should be like $[\\hat{\\lambda}, \\hat{\\beta}_0, \\text{boolean}, [\\hat{k}_1, \\dots, \\hat{k}_L]]$.\n- No physical units or angle units are involved; all outputs are dimensionless.",
            "solution": "The problem requires the implementation of a maximum likelihood estimator for a Poisson Generalized Linear Model (GLM) with a canonical log link function. The estimation is to be performed using Newton's method with a backtracking line search to ensure monotonic convergence. The implementation must be validated against a series of test cases, including a standard multivariate case, an intercept-only boundary case, a near-collinear edge case, and an application to a Linear-Nonlinear-Poisson (LNP) encoding model from neuroscience.\n\n### 1. Model Formulation and Objective Function\n\nLet the observed data consist of pairs $\\{(x_t, y_t)\\}_{t=1}^T$, where $x_t \\in \\mathbb{R}^p$ is a vector of covariates (a row from the design matrix $X \\in \\mathbb{R}^{T \\times p}$) and $y_t \\in \\{0, 1, 2, \\dots\\}$ is a count observation.\n\nThe model is defined by two components:\n1.  **Stochastic Component**: The conditional distribution of each observation $y_t$ given the covariates $x_t$ is assumed to be a Poisson distribution with rate parameter $\\lambda_t$. The probability mass function (PMF) is:\n    $$p(y_t | \\lambda_t) = \\frac{\\lambda_t^{y_t} e^{-\\lambda_t}}{y_t!}$$\n2.  **Systematic Component and Link Function**: In a GLM, a linear predictor $\\eta_t = x_t^\\top \\beta$ is related to the mean of the distribution, $E[y_t | x_t] = \\lambda_t$, via a link function $g$. For the Poisson GLM with the canonical logarithm link function, this relationship is $g(\\lambda_t) = \\log(\\lambda_t) = \\eta_t$. The inverse link function is therefore the exponential function:\n    $$\\lambda_t = e^{\\eta_t} = e^{x_t^\\top \\beta}$$\n    where $\\beta \\in \\mathbb{R}^p$ is the vector of model parameters to be estimated.\n\nSubstituting the link function into the PMF gives the probability of an observation $y_t$ as a function of the parameters $\\beta$:\n$$p(y_t | x_t, \\beta) = \\frac{(e^{x_t^\\top \\beta})^{y_t} \\exp(-e^{x_t^\\top \\beta})}{y_t!}$$\n\nAssuming the observations $\\{y_t\\}_{t=1}^T$ are conditionally independent given the covariates, the total likelihood of the data is the product of the individual probabilities:\n$$\\mathcal{L}(\\beta) = \\prod_{t=1}^T p(y_t | x_t, \\beta)$$\nFor analytical and numerical convenience, we work with the log-likelihood $L(\\beta) = \\log \\mathcal{L}(\\beta)$:\n$$L(\\beta) = \\sum_{t=1}^T \\log p(y_t | x_t, \\beta) = \\sum_{t=1}^T \\left( y_t \\log(e^{x_t^\\top \\beta}) - e^{x_t^\\top \\beta} - \\log(y_t!) \\right)$$\n$$L(\\beta) = \\sum_{t=1}^T \\left( y_t (x_t^\\top \\beta) - e^{x_t^\\top \\beta} \\right) - \\sum_{t=1}^T \\log(y_t!)$$\nMaximum likelihood estimation (MLE) seeks to find the parameter vector $\\hat{\\beta}$ that maximizes $L(\\beta)$. This is equivalent to minimizing the negative log-likelihood (NLL). The term $\\sum \\log(y_t!)$ is constant with respect to $\\beta$ and can be dropped from the optimization objective. The NLL to be minimized is:\n$$NLL(\\beta) = -\\sum_{t=1}^T \\left( y_t (x_t^\\top \\beta) - e^{x_t^\\top \\beta} \\right) = \\sum_{t=1}^T \\left( e^{x_t^\\top \\beta} - y_t(x_t^\\top \\beta) \\right)$$\n\n### 2. Gradient and Hessian for Newton's Method\n\nTo apply Newton's method, we must compute the gradient vector and Hessian matrix of the NLL.\n\n**Gradient**: The gradient, $\\nabla NLL(\\beta)$, is the vector of first partial derivatives $\\frac{\\partial NLL}{\\partial \\beta_j}$.\n$$\\frac{\\partial NLL(\\beta)}{\\partial \\beta_j} = \\sum_{t=1}^T \\left( \\frac{\\partial}{\\partial \\beta_j} e^{x_t^\\top \\beta} - \\frac{\\partial}{\\partial \\beta_j} y_t(x_t^\\top \\beta) \\right) = \\sum_{t=1}^T \\left( e^{x_t^\\top \\beta} x_{tj} - y_t x_{tj} \\right)$$\nwhere $x_{tj}$ is the $j$-th component of the vector $x_t$.\nIn vector notation, let $\\lambda$ be the vector of rates with components $\\lambda_t = e^{x_t^\\top \\beta}$, and $y$ be the vector of observations $y_t$. The gradient is:\n$$\\nabla NLL(\\beta) = \\sum_{t=1}^T (e^{x_t^\\top \\beta} - y_t) x_t = X^\\top (\\lambda - y)$$\n\n**Hessian**: The Hessian, $H(\\beta)$, is the matrix of second partial derivatives, $H_{jk}(\\beta) = \\frac{\\partial^2 NLL}{\\partial \\beta_j \\partial \\beta_k}$.\n$$\\frac{\\partial^2 NLL(\\beta)}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{t=1}^T (\\lambda_t - y_t) x_{tj} \\right) = \\sum_{t=1}^T \\frac{\\partial \\lambda_t}{\\partial \\beta_k} x_{tj} = \\sum_{t=1}^T (e^{x_t^\\top \\beta} x_{tk}) x_{tj} = \\sum_{t=1}^T \\lambda_t x_{tj} x_{tk}$$\nIn matrix notation, this is:\n$$H(\\beta) = \\sum_{t=1}^T \\lambda_t x_t x_t^\\top = X^\\top W X$$\nwhere $W$ is a $T \\times T$ diagonal matrix with diagonal entries $W_{tt} = \\lambda_t = e^{x_t^\\top \\beta}$.\n\n**Convexity**: Since $\\lambda_t > 0$ for all $t$, the diagonal matrix $W$ is positive definite. For any non-zero vector $v \\in \\mathbb{R}^p$, the quadratic form is $v^\\top H v = v^\\top X^\\top W X v = (Xv)^\\top W (Xv)$. This quantity is always non-negative. If the design matrix $X$ has full column rank, then $Xv \\neq 0$ for $v \\neq 0$, which makes the Hessian positive definite. In general, the Hessian is positive semidefinite. This proves that the NLL objective function is convex, ensuring that any local minimum found is a global minimum.\n\n### 3. Optimization Algorithm: Damped Newton's Method with Line Search\n\nNewton's method iteratively finds the minimum by approximating the function with a quadratic bowl at each step.\nThe update rule is:\n$$\\beta_{k+1} = \\beta_k - \\alpha_k H(\\beta_k)^{-1} \\nabla NLL(\\beta_k)$$\nwhere $\\alpha_k$ is the step size. The search direction is $\\Delta \\beta_k = -H(\\beta_k)^{-1} \\nabla NLL(\\beta_k)$.\n\n**Damping and Numerical Stability**: The Hessian $H(\\beta_k)$ may be ill-conditioned or singular if the design matrix $X$ has collinear or near-collinear columns (as in Case C). To ensure the matrix is invertible and the step is well-defined, a small positive damping term is added to the diagonal:\n$$H_{\\delta}(\\beta_k) = H(\\beta_k) + \\delta I$$\nwhere $\\delta > 0$ is a small scalar and $I$ is the identity matrix. The search direction is then computed by solving the linear system:\n$$H_{\\delta}(\\beta_k) \\Delta\\beta_k = - \\nabla NLL(\\beta_k)$$\n\n**Backtracking Line Search**: A full Newton step ($\\alpha_k = 1$) does not guarantee a decrease in the NLL. To ensure monotonic convergence, we use a backtracking line search to find an appropriate step size $\\alpha_k$. Starting with $\\alpha=1$, we iteratively reduce it by a factor $\\tau \\in (0, 1)$ until the Armijo condition is met:\n$$NLL(\\beta_k + \\alpha \\Delta\\beta_k) \\le NLL(\\beta_k) + c \\alpha (\\nabla NLL(\\beta_k))^\\top \\Delta\\beta_k$$\nfor some constant $c \\in (0, 1)$. This condition ensures a sufficient decrease in the objective function at each step.\n\n### 4. Implementation for Test Cases\n\nThe described algorithm is implemented in a single function, `fit_poisson_glm`, which is then used to solve the four specified test cases.\n\n-   **Case A & B**: These are standard GLM fitting problems. In Case B, the design matrix is simply a column of ones, which tests the intercept-only model. The analytical solution for Case B, $\\hat{\\beta} = \\log(\\bar{y})$, serves as a sanity check.\n-   **Case C**: This case introduces near-collinearity into the design matrix $X$. The damping in the Newton solver is critical for numerical stability. After finding the solution $\\hat{\\beta}$, the undamped Hessian $H(\\hat{\\beta})$ is computed, and its eigenvalues are checked to verify its positive semidefinite property, a direct consequence of the model's convexity.\n-   **Case D**: This case applies the GLM to an LNP model. A design matrix is constructed from a stimulus time series. Each row of the matrix corresponds to a time point $t$ and contains the stimulus values at $L$ preceding lags $[s_t, s_{t-1}, \\dots, s_{t-L+1}]$ plus an intercept. The response vector $y$ is aligned with these time points. Fitting the GLM to this constructed data allows for the recovery of the linear filter coefficients $\\hat{k}$ and the bias term $\\hat{b}$ as the estimated parameters $\\hat{\\beta}$.\n\nThis principled approach, starting from the statistical definition of the model and deriving the optimization components, leads to a robust and correct implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and tests maximum likelihood estimation for a Poisson GLM\n    using Newton's method with backtracking line search.\n    \"\"\"\n\n    def fit_poisson_glm(X, y, max_iter=100, tol=1e-9, damping=1e-8,\n                        line_search_c=0.25, line_search_tau=0.5):\n        \"\"\"\n        Fits a Poisson GLM with a log link using Newton's method.\n\n        Args:\n            X (np.ndarray): Design matrix of shape (T, p).\n            y (np.ndarray): Observation vector of shape (T,).\n            max_iter (int): Maximum number of iterations.\n            tol (float): Convergence tolerance for the norm of the step.\n            damping (float): Damping factor for the Hessian.\n            line_search_c (float): Armijo condition parameter.\n            line_search_tau (float): Backtracking step size reduction factor.\n\n        Returns:\n            np.ndarray: Estimated parameter vector beta_hat of shape (p,).\n        \"\"\"\n        p = X.shape[1]\n        beta = np.zeros(p)\n\n        def nll(b):\n            eta = np.clip(X @ b, -700, 700) # clip to avoid overflow in exp\n            return np.sum(np.exp(eta) - y * eta)\n\n        for i in range(max_iter):\n            eta = np.clip(X @ beta, -700, 700)\n            lam = np.exp(eta)\n\n            # Gradient\n            grad = X.T @ (lam - y)\n\n            # Hessian\n            W = np.diag(lam)\n            H = X.T @ W @ X\n            \n            # Damped Hessian for numerical stability\n            H_damped = H + damping * np.identity(p)\n\n            # Newton step direction\n            try:\n                delta_beta = np.linalg.solve(H_damped, -grad)\n            except np.linalg.LinAlgError:\n                # Fallback to gradient descent if Hessian is singular despite damping\n                delta_beta = -grad\n            \n            # Backtracking line search\n            alpha = 1.0\n            current_nll = nll(beta)\n            grad_dot_delta = grad.T @ delta_beta\n\n            while nll(beta + alpha * delta_beta) > current_nll + line_search_c * alpha * grad_dot_delta:\n                alpha *= line_search_tau\n                if alpha < 1e-12: # Prevent infinite loop\n                    break\n\n            # Update beta\n            step = alpha * delta_beta\n            beta += step\n\n            if np.linalg.norm(step) < tol:\n                break\n        \n        return beta\n\n    results = []\n\n    # Case A: happy path\n    T_A, F_A, seed_A = 80, 3, 42\n    rng_A = np.random.default_rng(seed_A)\n    X_base_A = rng_A.uniform(-0.5, 0.5, size=(T_A, F_A))\n    X_A = np.c_[X_base_A, np.ones(T_A)]\n    beta_true_A = np.array([0.4, -0.6, 0.5, 0.2])\n    lambda_A = np.exp(X_A @ beta_true_A)\n    y_A = rng_A.poisson(lambda_A)\n    beta_hat_A = fit_poisson_glm(X_A, y_A)\n    x_query_A = np.array([0.5, -1.0, 0.8, 1.0])\n    lambda_hat_A = np.exp(x_query_A @ beta_hat_A)\n    results.append(lambda_hat_A)\n\n    # Case B: intercept-only model\n    T_B, seed_B = 60, 123\n    rng_B = np.random.default_rng(seed_B)\n    X_B = np.ones((T_B, 1))\n    lambda_true_B = 1.8\n    y_B = rng_B.poisson(lambda_true_B, size=T_B)\n    beta_hat_B = fit_poisson_glm(X_B, y_B)\n    results.append(beta_hat_B[0])\n\n    # Case C: near collinearity\n    T_C, F_C, seed_C = 100, 2, 21\n    rng_C = np.random.default_rng(seed_C)\n    x1_C = rng_C.uniform(-0.5, 0.5, size=T_C)\n    eps_C = rng_C.normal(0, 1e-4, size=T_C)\n    x2_C = x1_C + eps_C\n    X_base_C = np.c_[x1_C, x2_C]\n    X_C = np.c_[X_base_C, np.ones(T_C)]\n    beta_true_C = np.array([0.7, 0.7, -0.1])\n    lambda_C = np.exp(X_C @ beta_true_C)\n    y_C = rng_C.poisson(lambda_C)\n    beta_hat_C = fit_poisson_glm(X_C, y_C)\n    \n    eta_hat_C = np.clip(X_C @ beta_hat_C, -700, 700)\n    lambda_hat_C = np.exp(eta_hat_C)\n    W_C = np.diag(lambda_hat_C)\n    H_C = X_C.T @ W_C @ X_C\n    eigenvalues_C = np.linalg.eigvalsh(H_C)\n    is_psd_C = np.all(eigenvalues_C >= -1e-8)\n    results.append(is_psd_C)\n    \n    # Case D: LNP cascade\n    T_D, L_D, seed_D = 150, 5, 7\n    rng_D = np.random.default_rng(seed_D)\n    stimulus_D = rng_D.uniform(-1, 1, size=T_D)\n    k_true_D = np.array([0.5, -0.2, 0.15, 0.0, -0.1])\n    b_true_D = -0.05\n    \n    num_samples_D = T_D - L_D + 1\n    X_D = np.zeros((num_samples_D, L_D + 1))\n    y_D = np.zeros(num_samples_D)\n    \n    for i in range(num_samples_D):\n        t = i + L_D - 1 # Current time index in stimulus array\n        lag_vector = stimulus_D[t-L_D+1 : t+1][::-1]\n        X_D[i, :L_D] = lag_vector\n        X_D[i, L_D] = 1.0\n        \n        rate = np.exp(lag_vector @ k_true_D + b_true_D)\n        y_D[i] = rng_D.poisson(rate)\n        \n    beta_hat_D = fit_poisson_glm(X_D, y_D)\n    k_hat_D = beta_hat_D[:L_D].tolist()\n    results.append(k_hat_D)\n    \n    # Final print statement in the exact required format.\n    # Convert boolean to lowercase string 'true'/'false'\n    results[2] = str(results[2]).lower()\n    print(f\"[{results[0]},{results[1]},{results[2]},{results[3]}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Fitting a model to data is only half the battle; we must then rigorously assess whether the model provides a good description of the neural activity. This final practice equips you with a suite of essential goodness-of-fit metrics, including the Poisson deviance and the Kolmogorov-Smirnov test based on the time-rescaling theorem. Mastering these validation techniques is critical for interpreting your model's parameters and trusting its predictions. ",
            "id": "4158979",
            "problem": "You are given binned spike count data and corresponding predicted conditional means from an encoding model that can be viewed as either a Generalized Linear Model (GLM) with a log link or a Linear-Nonlinear-Poisson (LNP) cascade. In either case, the model specifies that the spike counts per bin, denoted by $y_t$, are conditionally independent Poisson random variables with mean $\\mu_t = \\mathbb{E}[y_t \\mid x_t]$ per bin, where $t$ indexes time bins. You may assume the bin width is sufficiently small that $y_t \\in \\{0,1\\}$ for all bins. The goodness-of-fit must be assessed from first principles of the Poisson model and the time-rescaling theorem.\n\nFundamental base and definitions to be used:\n- The Poisson distribution with mean $\\mu_t$ has probability mass function $p(y_t \\mid \\mu_t) = \\frac{\\mu_t^{y_t} e^{-\\mu_t}}{y_t!}$, and log-likelihood per bin $\\ell_t = y_t \\log \\mu_t - \\mu_t - \\log(y_t!)$.\n- The total log-likelihood over $T$ bins is $\\mathrm{LL} = \\sum_{t=1}^{T} \\ell_t$.\n- The null model is a Poisson model with a constant mean $\\bar{y} = \\frac{1}{T} \\sum_{t=1}^{T} y_t$, so that $\\mu_t^{\\mathrm{null}} = \\bar{y}$ for all $t$.\n- The McFadden pseudo coefficient of determination is defined as $R^2_{\\mathrm{McF}} = 1 - \\frac{\\mathrm{LL}_{\\mathrm{model}}}{\\mathrm{LL}_{\\mathrm{null}}}$.\n- The Poisson deviance for the model relative to the saturated model is $D = 2 \\sum_{t=1}^{T} \\left[ y_t \\log\\left(\\frac{y_t}{\\mu_t}\\right) - (y_t - \\mu_t) \\right]$, using the convention $0 \\cdot \\log(0/\\mu_t) = 0$.\n- The Pearson residual in bin $t$ is $r_t = \\frac{y_t - \\mu_t}{\\sqrt{\\mu_t}}$, and the dispersion estimate is $\\hat{\\phi} = \\frac{1}{T} \\sum_{t=1}^{T} r_t^2$.\n- For time-rescaling in discrete time with at most one spike per bin, define spike bin indices $1 \\le t_1 < t_2 < \\dots < t_n \\le T$ where $y_{t_k} = 1$. Define integrated intensity between spikes as $w_k = \\sum_{t=t_{k-1}+1}^{t_k} \\mu_t$ with $t_0 = 0$. Then define $u_k = 1 - e^{-w_k}$. Under a correct model, the $u_k$ are independent and identically distributed Uniform$(0,1)$ random variables. The Kolmogorov–Smirnov statistic is $D_{\\mathrm{KS}} = \\max_{1 \\le i \\le n} \\max\\left( \\frac{i}{n} - u_{(i)},\\, u_{(i)} - \\frac{i-1}{n} \\right)$, where $u_{(i)}$ denotes the $i$-th order statistic.\n\nYour task is to implement a program that, for each provided test case, computes the following four quantities:\n- $R^2_{\\mathrm{McF}}$,\n- $D$,\n- $D_{\\mathrm{KS}}$ based on the time-rescaling theorem as defined,\n- $\\hat{\\phi}$.\n\nRound all floating-point results to exactly $6$ decimal places.\n\nTest suite:\n- Case $1$ ($T = 20$): $\\mu$ and $y$ are\n  - $\\mu = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.30, 0.20, 0.10, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45]$,\n  - $y = [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]$.\n- Case $2$ ($T = 20$): $\\mu$ is constant and equal to the empirical mean of $y$ from Case $1$, namely $\\mu = [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]$, and $y$ is identical to Case $1$.\n- Case $3$ ($T = 20$): $\\mu$ is the reversal of the $\\mu$ from Case $1$, namely $\\mu = [0.45, 0.40, 0.35, 0.30, 0.25, 0.20, 0.15, 0.10, 0.05, 0.10, 0.20, 0.30, 0.40, 0.35, 0.30, 0.25, 0.20, 0.15, 0.10, 0.05]$, and $y$ is identical to Case $1$.\n\nImplementation requirements:\n- Use the definitions above, derived from the Poisson model and the time-rescaling theorem, to compute the metrics exactly as stated.\n- All logarithms are natural logarithms.\n- For terms of the form $y_t \\log(y_t/\\mu_t)$, apply $0 \\cdot \\log(0/\\mu_t) = 0$ by convention to avoid undefined values.\n- For the Kolmogorov–Smirnov statistic, use the definition in terms of the empirical distribution function of the sorted $u_k$ values, without any approximation or randomization.\n\nFinal output format:\n- Your program should produce a single line of output containing all results concatenated in order of test cases, where each test case contributes the four values $[R^2_{\\mathrm{McF}}, D, D_{\\mathrm{KS}}, \\hat{\\phi}]$, flattened into one list. The line must be a comma-separated list enclosed in square brackets. For example, the output format must be exactly like $[r_1, d_1, k_1, \\phi_1, r_2, d_2, k_2, \\phi_2, r_3, d_3, k_3, \\phi_3]$ with each value rounded to exactly $6$ decimal places.",
            "solution": "The user has provided a well-defined computational problem in the domain of neuroscience data analysis, specifically the evaluation of point process models (GLM/LNP). The problem is scientifically grounded, internally consistent, and contains all necessary information for a unique solution. Therefore, the problem is deemed valid. The solution proceeds by implementing the specified goodness-of-fit metrics for each of the three test cases.\n\nThe core of the problem is to compute four distinct metrics for model performance based on the observed spike counts $y_t$ and the model's predicted conditional mean spike rate $\\mu_t$. The data consists of a series of $T$ time bins, where $y_t \\in \\{0, 1\\}$ indicates the absence or presence of a spike.\n\nThe calculations for each metric are performed as follows:\n\n**1. McFadden's Pseudo Coefficient of Determination ($R^2_{\\mathrm{McF}}$)**\n\nThis metric compares the log-likelihood of the proposed model, $\\mathrm{LL}_{\\mathrm{model}}$, to that of a simpler null model, $\\mathrm{LL}_{\\mathrm{null}}$. The null model assumes a constant firing rate equal to the empirical mean rate, $\\bar{y} = \\frac{1}{T} \\sum_{t=1}^{T} y_t$. The formula is $R^2_{\\mathrm{McF}} = 1 - \\frac{\\mathrm{LL}_{\\mathrm{model}}}{\\mathrm{LL}_{\\mathrm{null}}}$.\n\nThe log-likelihood for a Poisson model over $T$ bins is given by $\\mathrm{LL} = \\sum_{t=1}^{T} \\ell_t = \\sum_{t=1}^{T} [y_t \\log \\mu_t - \\mu_t - \\log(y_t!)]$. Since the problem states that $y_t \\in \\{0, 1\\}$, the term $\\log(y_t!)$ is always $\\log(1) = 0$. The simplified log-likelihood is thus $\\mathrm{LL} = \\sum_{t=1}^{T} (y_t \\log \\mu_t - \\mu_t)$.\n\n-   $\\mathrm{LL}_{\\mathrm{model}}$ is calculated using the provided model-predicted mean rates $\\mu_t$.\n-   $\\mathrm{LL}_{\\mathrm{null}}$ is calculated using a constant rate $\\mu_t^{\\mathrm{null}} = \\bar{y}$ for all $t$. The calculation becomes $\\mathrm{LL}_{\\mathrm{null}} = \\sum_{t=1}^{T} (y_t \\log \\bar{y} - \\bar{y})$.\n\n**2. Poisson Deviance ($D$)**\n\nThe deviance measures the goodness-of-fit of the model relative to a \"saturated\" model that fits the data perfectly (i.e., $\\mu_t^{\\mathrm{sat}} = y_t$). The formula is $D = 2 \\sum_{t=1}^{T} \\left[ y_t \\log\\left(\\frac{y_t}{\\mu_t}\\right) - (y_t - \\mu_t) \\right]$.\n\nThe term $y_t \\log(y_t/\\mu_t)$ requires careful handling due to the potential for $\\log(0)$.\n-   If $y_t = 1$, the term becomes $1 \\cdot \\log(1/\\mu_t) = -\\log(\\mu_t)$.\n-   If $y_t = 0$, the problem specifies the convention $0 \\cdot \\log(0/\\mu_t) = 0$.\nTherefore, the summation for this part of the expression only needs to be performed over time bins where a spike occurred ($y_t=1$). The overall sum is computed as $D = 2 \\left( \\sum_{t: y_t=1} (-\\log \\mu_t) - \\sum_{t=1}^{T}(y_t - \\mu_t) \\right)$.\n\n**3. Kolmogorov–Smirnov Statistic ($D_{\\mathrm{KS}}$)**\n\nThis metric is based on the time-rescaling theorem. For a correct model, the transformed spike times should be uniformly distributed. The procedure is as follows:\n-   First, identify the $0$-indexed time bins where spikes occurred, let these be $s_0, s_1, \\dots, s_{n-1}$. These correspond to the $1$-indexed times $t_1, t_2, \\dots, t_n$ in the problem description.\n-   Next, calculate the integrated intensity between consecutive spikes, $w_k = \\sum_{t=t_{k-1}+1}^{t_k} \\mu_t$, with $t_0=0$. This is efficiently implemented by first computing the cumulative sum of $\\mu_t$, let's call it $C_{\\mu}[i] = \\sum_{j=0}^{i} \\mu_j$. The integrated intensities are then $w_1 = C_{\\mu}[s_0]$ and $w_k = C_{\\mu}[s_{k-1}] - C_{\\mu}[s_{k-2}]$ for $k > 1$.\n-   Transform these integrated intensities into new random variables $u_k = 1 - e^{-w_k}$. If the model is correct, the set $\\{u_k\\}$ will be a sample from a Uniform$(0,1)$ distribution.\n-   Finally, compute the Kolmogorov-Smirnov statistic $D_{\\mathrm{KS}}$ to quantify the deviation of the empirical distribution of $\\{u_k\\}$ from the uniform CDF. Let $u_{(i)}$ be the sorted values of $u_k$. The statistic is $D_{\\mathrm{KS}} = \\max_{1 \\le i \\le n} \\max\\left( \\frac{i}{n} - u_{(i)},\\, u_{(i)} - \\frac{i-1}{n} \\right)$, where $n$ is the total number of spikes.\n\n**4. Dispersion Estimate ($\\hat{\\phi}$)**\n\nThe dispersion provides a measure of whether the variance of the data is consistent with the Poisson model, which assumes that the variance equals the mean. It is calculated from the Pearson residuals, $r_t = \\frac{y_t - \\mu_t}{\\sqrt{\\mu_t}}$. The dispersion estimate is the mean of the squared Pearson residuals: $\\hat{\\phi} = \\frac{1}{T} \\sum_{t=1}^{T} r_t^2 = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{(y_t - \\mu_t)^2}{\\mu_t}$. For a true Poisson process, we expect $\\hat{\\phi} \\approx 1$. A value $\\hat{\\phi} > 1$ suggests overdispersion (variance greater than the mean), while $\\hat{\\phi} < 1$ suggests underdispersion.\n\nThese four calculations are applied to each of the three test cases specified in the problem.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes goodness-of-fit metrics for Poisson encoding models based on\n    provided spike data and predicted means.\n    \"\"\"\n    # Define the data for the three test cases as specified in the problem.\n    mu_1_list = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.30, 0.20, 0.10, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45]\n    y_1_list = [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]\n    \n    y_1_arr = np.array(y_1_list, dtype=float)\n    \n    # Case 1\n    mu_1_arr = np.array(mu_1_list, dtype=float)\n    \n    # Case 2: mu is constant at the empirical mean of y from Case 1\n    y_bar_1 = np.mean(y_1_arr)\n    mu_2_arr = np.full_like(y_1_arr, y_bar_1, dtype=float)\n    \n    # Case 3: mu is the reversal of mu from Case 1\n    mu_3_arr = mu_1_arr[::-1]\n\n    test_cases = [\n        (y_1_arr, mu_1_arr),\n        (y_1_arr, mu_2_arr),\n        (y_1_arr, mu_3_arr)\n    ]\n\n    all_results = []\n    \n    for y, mu in test_cases:\n        T = float(len(y))\n\n        # 1. McFadden's a Pseudo R^2\n        # Log-likelihood for Poisson model with y in {0,1} is sum(y*log(mu) - mu).\n        # np.log(mu) is safe as all provided mu > 0.\n        ll_model = np.sum(y * np.log(mu) - mu)\n        \n        y_bar = np.mean(y)\n        # Check for y_bar=0 is good practice, but not needed for this problem's data.\n        ll_null = np.sum(y * np.log(y_bar) - y_bar) if y_bar > 0 else 0.0\n        \n        # If ll_null is 0, R^2 is undefined or handled by convention.\n        # This occurs if y_bar=0, meaning no spikes. Here y_bar = 0.25.\n        r2_mcf = 1.0 - ll_model / ll_null\n        all_results.append(r2_mcf)\n\n        # 2. Poisson Deviance\n        # D = 2 * sum(y*log(y/mu) - (y-mu))\n        # The term y*log(y/mu) is 0 if y=0, and -log(mu) if y=1.\n        spike_indices = (y == 1)\n        \n        log_term_sum = np.sum(-np.log(mu[spike_indices]))\n        diff_term_sum = -np.sum(y - mu)\n\n        deviance = 2.0 * (log_term_sum + diff_term_sum)\n        all_results.append(deviance)\n\n        # 3. Kolmogorov-Smirnov Statistic from Time-Rescaling\n        t_spike_indices = np.where(y == 1)[0]\n        n_spikes = len(t_spike_indices)\n        \n        if n_spikes == 0:\n            ks_stat = 0.0\n        else:\n            # Calculate integrated intensities w_k between spikes\n            cum_mu = np.cumsum(mu)\n            cum_mu_at_spikes = cum_mu[t_spike_indices]\n            w_k = np.diff(cum_mu_at_spikes, prepend=0)\n            \n            # Rescale to u_k = 1 - exp(-w_k)\n            u_k = 1.0 - np.exp(-w_k)\n            u_sorted = np.sort(u_k)\n            \n            # Calculate KS statistic D_KS\n            i_vec = np.arange(1, n_spikes + 1)\n            term1 = i_vec / n_spikes - u_sorted\n            term2 = u_sorted - (i_vec - 1) / n_spikes\n            ks_stat = np.max(np.maximum(term1, term2))\n        all_results.append(ks_stat)\n\n        # 4. Dispersion Estimate\n        # phi_hat = (1/T) * sum( (y - mu)^2 / mu )\n        pearson_residuals_sq = ((y - mu)**2) / mu\n        phi_hat = np.mean(pearson_residuals_sq)\n        all_results.append(phi_hat)\n\n    # Format the final output string exactly as required, with 6 decimal places.\n    formatted_results = [f'{x:.6f}' for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}