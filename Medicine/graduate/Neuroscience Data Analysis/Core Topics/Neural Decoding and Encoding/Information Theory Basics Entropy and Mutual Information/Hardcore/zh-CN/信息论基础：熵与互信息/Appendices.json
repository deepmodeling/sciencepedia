{
    "hands_on_practices": [
        {
            "introduction": "信息论的基础是熵，它量化了随机变量的不确定性。为了直观地理解这个概念，我们可以从神经科学中最简单的模型之一——单个神经元的尖峰发放与否——入手。通过计算伯努利随机变量的熵 ()，我们不仅能练习熵的定义，还能通过分析揭示一个核心原则：当所有可能结果等概率发生时，系统的不确定性达到最大。",
            "id": "4170225",
            "problem": "在一次感觉刺激范式中，对单个皮层神经元进行胞外记录。时间被离散化为宽度为 $\\Delta t$ 的时间窗，使得在任何一个时间窗内观察到多于一个尖峰的概率可以忽略不计。为每个时间窗定义一个二元随机变量 $X \\in \\{0,1\\}$，其中 $X=1$ 表示存在至少一个尖峰，$X=0$ 表示没有尖峰。假设在此时期内具有平稳性，因此 $X$ 服从伯努利分布 Bernoulli$(\\theta)$，即 $\\mathbb{P}(X=1)=\\theta$ 且 $\\mathbb{P}(X=0)=1-\\theta$，其中 $\\theta \\in (0,1)$。\n\n仅使用离散随机变量的香农熵等于其分布下的期望自信息这一定义，并采用自然对数（信息单位为奈特），完成以下任务：\n\n1. 推导熵 $H(X)$ 关于 $\\theta$ 的闭式表达式。\n2. 通过计算关于 $\\theta$ 的一阶和二阶导数，分析 $H(X)$ 如何依赖于 $\\theta$，并确定使 $H(X)$ 最大化的值 $\\theta^{\\star}$ 以及相应的最大值。您的分析应明确证明在 $(0,\\tfrac{1}{2}]$ 和 $[\\tfrac{1}{2},1)$ 上的单调性以及在 $(0,1)$ 上的凹性。\n3. 在尖峰序列分析的背景下，简要解释为何最大化值 $\\theta^{\\star}$ 与关于时间窗占用情况的最高不确定性相符。\n\n最终的熵表达式以奈特为单位。您最终报告的答案必须是 $H(X)$ 作为 $\\theta$ 函数的单个闭式解析表达式，不含任何附加文本或说明。无需四舍五入。",
            "solution": "问题陈述经核实具有科学依据、提法恰当、客观且自洽。它是在信息论应用于神经数据分析领域的一个标准基础练习。所有前提均合理，并提供了所有必需的信息。\n\n该问题要求分三部分作答：推导熵公式，分析此公式，并解释结果。\n\n### 1. 熵表达式 $H(X)$ 的推导\n\n对于一个概率质量函数为 $p(y) = \\mathbb{P}(Y=y)$ 的离散随机变量 $Y$，香农熵的基本定义是其期望自信息。一个结果 $y$ 的自信息为 $I(y) = -\\ln(p(y))$，其中使用自然对数，信息单位为奈特。熵 $H(Y)$ 是 $I(Y)$ 的期望值：\n$$H(Y) = \\mathbb{E}[I(Y)] = \\sum_{y \\in \\mathcal{Y}} p(y) I(y) = -\\sum_{y \\in \\mathcal{Y}} p(y) \\ln(p(y))$$\n在本问题中，随机变量是 $X$，服从伯努利分布 $X \\sim \\text{Bernoulli}(\\theta)$。其取值集合为 $\\mathcal{X} = \\{0, 1\\}$，两种结果的概率如下：\n$$ \\mathbb{P}(X=1) = \\theta $$\n$$ \\mathbb{P}(X=0) = 1-\\theta $$\n将熵的定义应用于此特定情况，我们对 $X$ 的两种可能结果求和：\n$$ H(X) = - \\big[ \\mathbb{P}(X=0)\\ln(\\mathbb{P}(X=0)) + \\mathbb{P}(X=1)\\ln(\\mathbb{P}(X=1)) \\big] $$\n代入给定的概率，我们得到二元随机变量 $X$ 的熵作为参数 $\\theta$ 的函数表达式：\n$$ H(X) = H(\\theta) = - \\big[ (1-\\theta)\\ln(1-\\theta) + \\theta\\ln(\\theta) \\big] $$\n这可以写成：\n$$ H(\\theta) = -(1-\\theta)\\ln(1-\\theta) - \\theta\\ln(\\theta) $$\n这就是二元熵函数，也是 $H(X)$ 关于 $\\theta$ 的闭式表达式。\n\n### 2. $H(\\theta)$ 的分析\n\n为分析 $H(\\theta)$ 如何依赖于 $\\theta$，我们计算它在 $\\theta \\in (0,1)$ 上关于 $\\theta$ 的一阶和二阶导数。\n\n**一阶导数与单调性：**\n我们对 $H(\\theta)$ 的每一项使用微分乘法法则 $\\frac{d}{dx}(uv) = u\\frac{dv}{dx} + v\\frac{du}{dx}$。\n$$ \\frac{d}{d\\theta} \\big[ -(1-\\theta)\\ln(1-\\theta) \\big] = - \\big[ (-1)\\ln(1-\\theta) + (1-\\theta)\\frac{-1}{1-\\theta} \\big] = \\ln(1-\\theta) + 1 $$\n$$ \\frac{d}{d\\theta} \\big[ -\\theta\\ln(\\theta) \\big] = - \\big[ (1)\\ln(\\theta) + \\theta\\frac{1}{\\theta} \\big] = -\\ln(\\theta) - 1 $$\n将这些结果相加，得到 $H(\\theta)$ 的一阶导数：\n$$ \\frac{dH}{d\\theta} = (\\ln(1-\\theta) + 1) + (-\\ln(\\theta) - 1) = \\ln(1-\\theta) - \\ln(\\theta) = \\ln\\left(\\frac{1-\\theta}{\\theta}\\right) $$\n为求临界点，我们将一阶导数设为零：\n$$ \\frac{dH}{d\\theta} = 0 \\implies \\ln\\left(\\frac{1-\\theta}{\\theta}\\right) = 0 $$\n$$ \\frac{1-\\theta}{\\theta} = \\exp(0) = 1 $$\n$$ 1-\\theta = \\theta \\implies 2\\theta = 1 \\implies \\theta = \\frac{1}{2} $$\n区间 $(0,1)$ 内唯一的临界点是 $\\theta^{\\star} = \\frac{1}{2}$。\n\n现在，我们通过考察 $\\frac{dH}{d\\theta}$ 的符号来证明在指定区间上的单调性：\n- 对于 $\\theta \\in (0, \\frac{1}{2})$：我们有 $1-\\theta > \\theta$，所以比值 $\\frac{1-\\theta}{\\theta} > 1$。因为自然对数是增函数，所以 $\\ln\\left(\\frac{1-\\theta}{\\theta}\\right) > \\ln(1) = 0$。因此 $\\frac{dH}{d\\theta} > 0$，$H(\\theta)$ 在 $(0, \\frac{1}{2}]$ 上是严格单调递增的。\n- 对于 $\\theta \\in (\\frac{1}{2}, 1)$：我们有 $1-\\theta  \\theta$，所以比值 $\\frac{1-\\theta}{\\theta}  1$。这意味着 $\\ln\\left(\\frac{1-\\theta}{\\theta}\\right)  \\ln(1) = 0$。因此 $\\frac{dH}{d\\theta}  0$，$H(\\theta)$ 在 $[\\frac{1}{2}, 1)$ 上是严格单调递减的。\n\n此分析证实 $H(\\theta)$ 在 $\\theta^{\\star} = \\frac{1}{2}$ 处取得局部最大值。\n\n**二阶导数与凹性：**\n为分析凹性，我们计算 $H(\\theta)$ 的二阶导数：\n$$ \\frac{d^2H}{d\\theta^2} = \\frac{d}{d\\theta}\\left[\\ln(1-\\theta) - \\ln(\\theta)\\right] = \\frac{-1}{1-\\theta} - \\frac{1}{\\theta} $$\n$$ \\frac{d^2H}{d\\theta^2} = -\\left(\\frac{1}{1-\\theta} + \\frac{1}{\\theta}\\right) = -\\left(\\frac{\\theta + (1-\\theta)}{\\theta(1-\\theta)}\\right) = -\\frac{1}{\\theta(1-\\theta)} $$\n对于任何 $\\theta \\in (0,1)$，$\\theta$ 和 $1-\\theta$ 均为正数。因此，它们的乘积 $\\theta(1-\\theta)$ 也为正。所以，二阶导数 $\\frac{d^2H}{d\\theta^2} = -\\frac{1}{\\theta(1-\\theta)}$ 对于所有 $\\theta \\in (0,1)$ 都是严格为负的。这证明了函数 $H(\\theta)$ 在整个区间 $(0,1)$ 上是严格凹的。一个严格凹函数至多有一个最大值，因此在 $\\theta^{\\star} = \\frac{1}{2}$ 处的局部最大值是唯一的全局最大值。\n\n**熵的最大值：**\n将 $\\theta^{\\star} = \\frac{1}{2}$ 代入 $H(\\theta)$ 即可得到熵的最大值：\n$$ H\\left(\\frac{1}{2}\\right) = -\\left(1-\\frac{1}{2}\\right)\\ln\\left(1-\\frac{1}{2}\\right) - \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) $$\n$$ H\\left(\\frac{1}{2}\\right) = -\\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) - \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) = -\\ln\\left(\\frac{1}{2}\\right) = \\ln(2) $$\n最大熵为 $\\ln(2)$ 奈特。\n\n### 3. 在尖峰序列分析中的解释\n\n在尖峰序列分析的背景下，熵 $H(X)$ 量化了在一个随机选择的时间窗中观察到尖峰（$X=1$）与没有尖峰（$X=0$）相关的平均不确定性。分析表明，当参数 $\\theta = \\theta^{\\star} = \\frac{1}{2}$ 时，这种不确定性达到最大。\n$\\theta = \\frac{1}{2}$ 的值意味着在一个时间窗内出现尖峰的概率是 $\\frac{1}{2}$，没有尖峰的概率也是 $\\frac{1}{2}$。两种可能的结果是等可能的。这代表了最大的不可预测性状态。如果观察者要预测下一个时间窗是否包含尖峰，在这种条件下，他们的预测将最不准确，不比抛掷一枚公平的硬币更好。\n\n相反，当 $\\theta$ 趋近于 0 或 1 的极端情况时，熵 $H(\\theta)$ 趋近于 0。\n- 如果 $\\theta \\to 0$，尖峰极为罕见。观察者可以高度确定地预测任何给定的时间窗都将是空的（$X=0$）。不确定性非常低。\n- 如果 $\\theta \\to 1$，尖峰几乎总是存在。观察者可以高度确定地预测任何给定的时间窗都将包含一个尖峰（$X=1$）。同样，不确定性非常低。\n\n因此，最大化值 $\\theta^{\\star} = \\frac{1}{2}$ 与“均匀概率分布在所有可能结果上会产生最高不确定性”这一原则是一致的。对于一个二元系统，这对应于两种结果等概率出现。",
            "answer": "$$\n\\boxed{-(1-\\theta)\\ln(1-\\theta) - \\theta\\ln(\\theta)}\n$$"
        },
        {
            "introduction": "除了衡量单个变量的不确定性，我们还经常需要比较不同的统计模型或概率分布。Kullback-Leibler (KL) 散度为此提供了一个非对称的度量，量化了用一个模型近似另一个模型时的信息损失。本练习 () 将 KL 散度的定义应用于神经科学中广泛用于建模尖峰计数的泊松分布，通过推导其解析表达式，可以加深对模型选择和参数估计中信息度量的理解。",
            "id": "4170231",
            "problem": "一位计算神经科学家将固定时长记录窗口内的尖峰计数建模为由泊松分布生成的离散时间计数。假设真实尖峰计数分布的速率参数为 $\\lambda>0$，而一个不匹配的模型使用的速率参数为 $\\mu>0$。仅使用核心定义，推导从真实分布 $\\mathrm{Pois}(\\lambda)$ 到不匹配模型 $\\mathrm{Pois}(\\mu)$ 的 Kullback-Leibler 散度（KL；Kullback-Leibler）的解析表达式。然后，将该散度视为 $\\lambda$ 的函数（$\\mu$ 保持固定），并通过计算其关于 $\\lambda$ 的二阶导数并在 $\\lambda=\\mu$ 处求值，来分析其在等值点 $\\lambda=\\mu$ 处的局部曲率。\n\n请基于以下基本要素进行推导：\n- 离散分布 $P$ 和 $Q$ 在共同支撑集上的 Kullback-Leibler 散度定义：$D_{\\mathrm{KL}}(P\\Vert Q)=\\sum_{k}P(k)\\,\\ln\\!\\big(P(k)/Q(k)\\big)$。\n- 速率为 $\\theta0$ 的泊松分布的概率质量函数：$p(k\\mid \\theta)=\\exp(-\\theta)\\,\\theta^{k}/k!$，其中 $k\\in\\{0,1,2,\\dots\\}$。\n\n请以闭式解析表达式的形式提供散度以及在等值点处的曲率的最终结果。无需进行数值四舍五入。答案不应包含单位，并应使用自然对数表示。",
            "solution": "该问题陈述清晰、具有科学依据且内容自洽。所有必要的定义都已提供，任务是信息论在统计建模中应用的一个标准推导，这是计算神经科学中的常见做法。因此，该问题被认为是有效的，并将提供解答。\n\n任务有两部分：首先，推导从速率参数为 $\\lambda  0$ 的真实泊松分布 $P$ 到速率参数为 $\\mu  0$ 的模型泊松分布 $Q$ 的 Kullback-Leibler (KL) 散度；其次，计算该散度在点 $\\lambda = \\mu$ 处相对于 $\\lambda$ 的局部曲率。\n\n提供的基本定义是：\n1. 离散分布 $P$ 和 $Q$ 的 KL 散度：$D_{\\mathrm{KL}}(P\\Vert Q)=\\sum_{k}P(k)\\,\\ln\\left(\\frac{P(k)}{Q(k)}\\right)$。\n2. 速率为 $\\theta  0$ 的泊松分布的概率质量函数 (PMF)：$p(k\\mid \\theta)=\\frac{\\exp(-\\theta)\\,\\theta^{k}}{k!}$，其中 $k\\in\\{0,1,2,\\dots\\}$。\n\n设 $P(k)$ 为真实分布 $\\mathrm{Pois}(\\lambda)$ 的概率质量函数， $Q(k)$ 为模型分布 $\\mathrm{Pois}(\\mu)$ 的概率质量函数。\n$$ P(k) = \\frac{\\exp(-\\lambda)\\lambda^k}{k!} $$\n$$ Q(k) = \\frac{\\exp(-\\mu)\\mu^k}{k!} $$\n两个分布共享相同的支撑集，即非负整数集合 $k \\in \\{0, 1, 2, \\ldots\\}$。\n\n首先，我们计算比率 $\\frac{P(k)}{Q(k)}$：\n$$ \\frac{P(k)}{Q(k)} = \\frac{\\frac{\\exp(-\\lambda)\\lambda^k}{k!}}{\\frac{\\exp(-\\mu)\\mu^k}{k!}} = \\frac{\\exp(-\\lambda)\\lambda^k}{\\exp(-\\mu)\\mu^k} = \\exp(\\mu - \\lambda) \\left(\\frac{\\lambda}{\\mu}\\right)^k $$\n接下来，我们取这个比率的自然对数：\n$$ \\ln\\left(\\frac{P(k)}{Q(k)}\\right) = \\ln\\left(\\exp(\\mu - \\lambda) \\left(\\frac{\\lambda}{\\mu}\\right)^k\\right) = \\ln(\\exp(\\mu - \\lambda)) + \\ln\\left(\\left(\\frac{\\lambda}{\\mu}\\right)^k\\right) = (\\mu - \\lambda) + k \\ln\\left(\\frac{\\lambda}{\\mu}\\right) $$\n现在，我们将此表达式代入 KL 散度的定义中：\n$$ D_{\\mathrm{KL}}(P\\Vert Q) = \\sum_{k=0}^{\\infty} P(k) \\left[ (\\mu - \\lambda) + k \\ln\\left(\\frac{\\lambda}{\\mu}\\right) \\right] $$\n根据求和的线性性质，我们可以将其分为两部分：\n$$ D_{\\mathrm{KL}}(P\\Vert Q) = \\sum_{k=0}^{\\infty} P(k)(\\mu - \\lambda) + \\sum_{k=0}^{\\infty} P(k) k \\ln\\left(\\frac{\\lambda}{\\mu}\\right) $$\n我们可以将不依赖于求和索引 $k$ 的项提取出来：\n$$ D_{\\mathrm{KL}}(P\\Vert Q) = (\\mu - \\lambda) \\sum_{k=0}^{\\infty} P(k) + \\ln\\left(\\frac{\\lambda}{\\mu}\\right) \\sum_{k=0}^{\\infty} k P(k) $$\n我们现在计算这两个和。第一个和 $\\sum_{k=0}^{\\infty} P(k)$ 是在分布 $P$ 的整个支撑集上的概率之和，其值必须等于 $1$。\n$$ \\sum_{k=0}^{\\infty} P(k) = 1 $$\n第二个和 $\\sum_{k=0}^{\\infty} k P(k)$ 是在分布 $P$ 下随机变量 $k$ 的期望值的定义。对于泊松分布 $\\mathrm{Pois}(\\lambda)$，其期望值等于其速率参数 $\\lambda$。\n$$ \\mathbb{E}_P[k] = \\sum_{k=0}^{\\infty} k P(k) = \\lambda $$\n将这些结果代回 KL 散度的表达式中：\n$$ D_{\\mathrm{KL}}(P\\Vert Q) = (\\mu - \\lambda)(1) + \\ln\\left(\\frac{\\lambda}{\\mu}\\right)(\\lambda) $$\n整理各项，得到 KL 散度的最终解析表达式：\n$$ D_{\\mathrm{KL}}(\\mathrm{Pois}(\\lambda) \\Vert \\mathrm{Pois}(\\mu)) = \\lambda \\ln\\left(\\frac{\\lambda}{\\mu}\\right) - \\lambda + \\mu $$\n这完成了问题的第一部分。\n\n对于第二部分，我们分析此散度的局部曲率。我们将 $D_{\\mathrm{KL}}$ 视为真实速率 $\\lambda$ 的函数，记为 $D(\\lambda)$，同时保持模型速率 $\\mu$ 不变。\n$$ D(\\lambda) = \\lambda \\ln\\left(\\frac{\\lambda}{\\mu}\\right) - \\lambda + \\mu $$\n为便于求导，我们可以重写对数项：\n$$ D(\\lambda) = \\lambda (\\ln\\lambda - \\ln\\mu) - \\lambda + \\mu = \\lambda\\ln\\lambda - \\lambda\\ln\\mu - \\lambda + \\mu $$\n我们计算 $D(\\lambda)$ 关于 $\\lambda$ 的一阶导数：\n$$ \\frac{dD}{d\\lambda} = \\frac{d}{d\\lambda} (\\lambda\\ln\\lambda - \\lambda\\ln\\mu - \\lambda + \\mu) $$\n对 $\\lambda\\ln\\lambda$ 项使用乘法法则，我们得到 $\\frac{d}{d\\lambda}(\\lambda\\ln\\lambda) = 1\\cdot\\ln\\lambda + \\lambda\\cdot\\frac{1}{\\lambda} = \\ln\\lambda + 1$。其他项的导数则很简单：\n$$ \\frac{dD}{d\\lambda} = (\\ln\\lambda + 1) - \\ln\\mu - 1 + 0 = \\ln\\lambda - \\ln\\mu = \\ln\\left(\\frac{\\lambda}{\\mu}\\right) $$\n接下来，我们计算 $D(\\lambda)$ 关于 $\\lambda$ 的二阶导数，它代表了曲率：\n$$ \\frac{d^2D}{d\\lambda^2} = \\frac{d}{d\\lambda} \\left(\\ln\\left(\\frac{\\lambda}{\\mu}\\right)\\right) = \\frac{d}{d\\lambda}(\\ln\\lambda - \\ln\\mu) $$\n由于 $\\mu$ 是关于 $\\lambda$ 的常数，它的对数也是一个常数。\n$$ \\frac{d^2D}{d\\lambda^2} = \\frac{1}{\\lambda} - 0 = \\frac{1}{\\lambda} $$\n问题要求在等值点 $\\lambda=\\mu$ 处计算该曲率。\n$$ \\left. \\frac{d^2D}{d\\lambda^2} \\right|_{\\lambda=\\mu} = \\frac{1}{\\mu} $$\n这个值，即模型与真实分布匹配点处 KL 散度的曲率，等于泊松参数的费雪信息（Fisher information）。\n\n所要求的两个结果是散度的解析表达式 $D_{\\mathrm{KL}}(\\mathrm{Pois}(\\lambda) \\Vert \\mathrm{Pois}(\\mu)) = \\lambda \\ln\\left(\\frac{\\lambda}{\\mu}\\right) - \\lambda + \\mu$，以及在等值点处的曲率 $\\left. \\frac{d^2D}{d\\lambda^2} \\right|_{\\lambda=\\mu} = \\frac{1}{\\mu}$。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\lambda \\ln\\left(\\frac{\\lambda}{\\mu}\\right) - \\lambda + \\mu \\\\ \\frac{1}{\\mu} \\end{pmatrix} } $$"
        },
        {
            "introduction": "从理论转向实践，我们必须面对从有限数据中估计信息论量（如互信息）的挑战，这不可避免地会引入误差。一个常见的问题是，直接使用经验频率的“代入式”估计量通常是有偏的。本练习 () 旨在阐明这一关键问题，它要求推导在信号与响应完全独立的零假设下，互信息估计量的期望值，从而量化其固有的正偏差。理解这种偏差的来源和大小，对于在实际数据分析中准确解释信息论测量结果至关重要。",
            "id": "4170211",
            "problem": "一位系统神经科学家正在分析离散感觉刺激 $S$ 与离散化的单神经元响应 $R$ 之间的统计依赖性。刺激 $S$ 有 $K$ 个类别 $\\{s_{1},\\ldots,s_{K}\\}$，响应 $R$ 有 $M$ 个类别 $\\{r_{1},\\ldots,r_{M}\\}$，这是通过对发放计数进行分箱得到的。实验包含 $N$ 次独立同分布 (i.i.d.) 的试验，产生计数 $\\{n_{ij}\\}_{i=1,\\ldots,K;\\,j=1,\\ldots,M}$，且 $\\sum_{i=1}^{K}\\sum_{j=1}^{M} n_{ij}=N$。经验联合分布为 $\\hat{p}_{SR}(s_{i},r_{j})=n_{ij}/N$，经验边缘分布为 $\\hat{p}_{S}(s_{i})=\\sum_{j=1}^{M} n_{ij}/N$ 和 $\\hat{p}_{R}(r_{j})=\\sum_{i=1}^{K} n_{ij}/N$。该神经科学家使用互信息 (MI) 的朴素“即插即用”估计量，\n$$\n\\hat{I}(S;R)=\\sum_{i=1}^{K}\\sum_{j=1}^{M} \\hat{p}_{SR}(s_{i},r_{j}) \\,\\ln\\!\\left(\\frac{\\hat{p}_{SR}(s_{i},r_{j})}{\\hat{p}_{S}(s_{i})\\,\\hat{p}_{R}(r_{j})}\\right),\n$$\n以自然单位 (nats) 衡量。假设真实的数据生成分布满足独立性零假设，即对所有 $i$ 和 $j$，都有 $p_{SR}(s_{i},r_{j})=p_{S}(s_{i})\\,p_{R}(r_{j})$，并且所有单元格概率都严格为正，以确保渐近分析的有效性。\n\n仅从离散互信息的核心定义和多项式模型的似然出发，并利用一个经过充分检验的大样本结论——即在零假设下，用于检验 $K\\times M$ 列联表中独立性的对数似然比统计量在分布上收敛于自由度为 $(K-1)(M-1)$ 的卡方分布——推导在独立性零假设下，朴素“即插即用”互信息期望值的大样本 $N$ 领先阶表达式。然后利用此结果解释为什么即使在总体中 $S$ 和 $R$ 统计独立，对于有限的 $N$，朴素“即插即用”估计量仍存在正偏差。\n\n将以 nats 为单位的领先阶期望的最终结果表示为包含 $K$、$M$ 和 $N$ 的封闭形式解析表达式。最终答案必须是单个表达式。请勿在最终答案框中包含单位。无需四舍五入。",
            "solution": "问题陈述经评估有效。其科学依据扎根于信息论和统计学的标准原理，问题阐述清晰，为得出唯一解提供了所有必要信息，并且语言客观。任务是推导一个已知的统计结果，这是一项合理的科学实践。\n\n问题要求在刺激 $S$ 和响应 $R$ 之间统计独立的零假设下，朴素“即插即用”互信息估计量 $\\hat{I}(S;R)$ 的期望值的大样本 $N$ 领先阶表达式。\n\n互信息 (MI) 的朴素“即插即用”估计量由下式给出：\n$$\n\\hat{I}(S;R) = \\sum_{i=1}^{K}\\sum_{j=1}^{M} \\hat{p}_{SR}(s_{i},r_{j}) \\,\\ln\\!\\left(\\frac{\\hat{p}_{SR}(s_{i},r_{j})}{\\hat{p}_{S}(s_{i})\\,\\hat{p}_{R}(r_{j})}\\right)\n$$\n其中 $\\hat{p}_{SR}(s_{i},r_{j}) = n_{ij}/N$ 是经验联合概率，$\\hat{p}_{S}(s_{i}) = (\\sum_{j} n_{ij})/N$ 和 $\\hat{p}_{R}(r_{j}) = (\\sum_{i} n_{ij})/N$ 是经验边缘概率。为简化符号，我们分别用 $\\hat{p}_{i\\cdot}$ 和 $\\hat{p}_{\\cdot j}$ 表示边缘概率，用 $\\hat{p}_{ij}$ 表示联合概率。表达式变为：\n$$\n\\hat{I}(S;R) = \\sum_{i=1}^{K}\\sum_{j=1}^{M} \\hat{p}_{ij} \\,\\ln\\!\\left(\\frac{\\hat{p}_{ij}}{\\hat{p}_{i\\cdot}\\,\\hat{p}_{\\cdot j}}\\right)\n$$\n该表达式在形式上等价于经验联合分布 $\\hat{p}_{SR}$ 与其边缘分布乘积 $\\hat{p}_{S}\\hat{p}_{R}$ 之间的 Kullback-Leibler 散度，即 $\\hat{I}(S;R) = D_{KL}(\\hat{p}_{SR} || \\hat{p}_{S}\\hat{p}_{R})$。\n\n问题引导我们使用关于列联表中独立性检验的对数似然比统计量的结果。让我们来构建这个统计量。我们要检验独立性零假设 $H_0$（其中真实单元格概率为 $p_{ij} = p_{i\\cdot} p_{\\cdot j}$）与饱和备择假设 $H_1$（其中 $p_{ij}$ 不受约束，除了总和为 1）的对立情况。\n\n在多项式模型下，观测到计数 $\\{n_{ij}\\}$ 的似然为 $L(\\{p_{ij}\\}) \\propto \\prod_{i,j} (p_{ij})^{n_{ij}}$。\n\n在备择假设 $H_1$ 下，$p_{ij}$ 的最大似然估计 (MLE) 是 $\\hat{p}_{ij} = n_{ij}/N$。最大化的对数似然为：\n$$\n\\ln L_1^* = \\sum_{i,j} n_{ij} \\ln(\\hat{p}_{ij}) = \\sum_{i,j} n_{ij} \\ln(n_{ij}/N)\n$$\n\n在零假设 $H_0$ 下，边缘概率的最大似然估计是 $\\hat{p}_{i\\cdot} = (\\sum_j n_{ij})/N$ 和 $\\hat{p}_{\\cdot j} = (\\sum_i n_{ij})/N$。$H_0$ 下最大化的对数似然为：\n$$\n\\ln L_0^* = \\sum_{i,j} n_{ij} \\ln(\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j})\n$$\n\n对数似然比统计量，通常表示为 $\\Lambda$ 或 $G^2$，定义为最大化对数似然之差的两倍：\n$$\n\\Lambda = 2(\\ln L_1^* - \\ln L_0^*) = 2 \\left( \\sum_{i,j} n_{ij} \\ln(n_{ij}/N) - \\sum_{i,j} n_{ij} \\ln(\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j}) \\right)\n$$\n合并求和内的项：\n$$\n\\Lambda = 2 \\sum_{i,j} n_{ij} \\ln\\left(\\frac{n_{ij}/N}{\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j}}\\right)\n$$\n我们可以提出总样本数 $N$：\n$$\n\\Lambda = 2N \\sum_{i,j} \\frac{n_{ij}}{N} \\ln\\left(\\frac{n_{ij}/N}{\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j}}\\right)\n$$\n认识到 $\\hat{p}_{ij} = n_{ij}/N$，我们可以将其重写为：\n$$\n\\Lambda = 2N \\sum_{i,j} \\hat{p}_{ij} \\ln\\left(\\frac{\\hat{p}_{ij}}{\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j}}\\right)\n$$\n求和项正是朴素 MI 估计量 $\\hat{I}(S;R)$ 的定义。因此，我们建立了一个关键的恒等式：\n$$\n\\Lambda = 2N \\hat{I}(S;R)\n$$\n这表明朴素 MI 估计量乘以 $2N$ 后，与用于独立性检验的 G 检验统计量是相同的。\n\n问题指出，对于大的 $N$，在独立性零假设下，该对数似然比统计量在分布上收敛于卡方 ($\\chi^2$) 分布。在一个 $K \\times M$ 列联表中进行独立性检验的自由度 $\\nu$ 由备择模型中的自由参数数量减去零模型中的自由参数数量给出。\n饱和模型有 $KM-1$ 个自由参数。独立性模型有 $(K-1) + (M-1)$ 个自由参数。\n因此，$\\nu = (KM-1) - ((K-1) + (M-1)) = KM - 1 - K + 1 - M + 1 = KM - K - M + 1 = (K-1)(M-1)$。\n所以，对于大的 $N$，我们有如下渐近分布结果：\n$$\n2N \\hat{I}(S;R) \\quad \\xrightarrow{d} \\quad \\chi^2_{(K-1)(M-1)}\n$$\n为了找到 $\\hat{I}(S;R)$ 期望值的领先阶表达式，我们对该关系式两边取期望。对于大的 $N$，该统计量的期望将接近其极限分布的期望。\n$$\nE[2N \\hat{I}(S;R)] \\approx E[\\chi^2_{(K-1)(M-1)}]\n$$\n由于对于给定的样本量 $N$ 是一个常数，我们可以写成：\n$$\n2N E[\\hat{I}(S;R)] \\approx E[\\chi^2_{(K-1)(M-1)}]\n$$\n一个自由度为 $\\nu$ 的卡方随机变量的期望值就是 $\\nu$。在我们的例子中，$\\nu = (K-1)(M-1)$。\n$$\n2N E[\\hat{I}(S;R)] \\approx (K-1)(M-1)\n$$\n解出 $E[\\hat{I}(S;R)]$，我们得到了在独立性零假设下，朴素 MI 估计量期望值的大样本 $N$ 领先阶表达式：\n$$\nE[\\hat{I}(S;R)] \\approx \\frac{(K-1)(M-1)}{2N}\n$$\n这就是所求的结果。\n\n为了解释为什么该估计量是正偏的，我们将其期望值与它所估计的参数的真实值进行比较。参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的偏差是 $B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$。\n在我们的例子中，估计量是 $\\hat{I}(S;R)$。真实的参数值是真实的互信息 $I(S;R)$。在独立性零假设下，真实的互信息为零，即 $I(S;R) = 0$。\n因此，在零假设下的偏差是：\n$$\nB(\\hat{I}) = E[\\hat{I}(S;R)] - 0 = E[\\hat{I}(S;R)]\n$$\n从我们推导出的结果，我们有：\n$$\nB(\\hat{I}) \\approx \\frac{(K-1)(M-1)}{2N}\n$$\n对于任何具有多于一个刺激类别 ($K  1$) 和多于一个响应类别 ($M  1$) 的非平凡实验设计，以及对于任何有限的试验次数 $N$，表达式 $(K-1)(M-1)/(2N)$ 都是严格为正的。\n这意味着 $E[\\hat{I}(S;R)]  I(S;R) = 0$。一个期望值大于参数真实值的估计量被称为具有正偏差。\n正偏差的产生是由于抽样噪声；即使当底层分布是独立的，有限的抽样也会导致经验计数 $\\{n_{ij}\\}$ 产生波动，从而使得经验联合分布 $\\hat{p}_{SR}$ 偏离经验边缘分布的乘积 $\\hat{p}_{S}\\hat{p}_{R}$。由于互信息（作为一种 Kullback-Leibler 散度）是一个非负量，并且只有当这两个分布完全相同时才为零，因此任何随机偏差都会产生一个正的 $\\hat{I}(S;R)$ 值。我们的计算量化了在大样本量下这种伪正值的平均大小。",
            "answer": "$$\n\\boxed{\\frac{(K-1)(M-1)}{2N}}\n$$"
        }
    ]
}