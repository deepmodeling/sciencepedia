{
    "hands_on_practices": [
        {
            "introduction": "A primary goal in analyzing directionally-tuned neurons is to identify their preferred direction. This practice introduces the fundamental method for this calculation: the circular mean. By representing each directional firing event as a vector and computing their resultant average, we can determine the central tendency of the cell's response, a concept you will implement from first principles in this exercise . This problem adds a layer of neurophysiological realism by incorporating spike-phase information as weights, allowing you to build a more robust estimate of the preferred direction and assess its stability over time.",
            "id": "4202390",
            "problem": "You are given multiple recording sessions for a single neuron. Each session consists of a set of spike-phase pairs, where each pair contains a head direction angle and a local field potential theta phase at the spike time. Let each pair be denoted by $(\\theta_i, \\phi_i)$, with $\\theta_i \\in [0, 2\\pi)$ and $\\phi_i \\in [0, 2\\pi)$, both in radians. The goal is to estimate the preferred head direction for each session using a circular mean that incorporates spike-phase information as reliability weights, and then assess the stability of the preferred direction across sessions using a circular distance metric.\n\nStarting only from the fundamental definition that an angle $\\theta$ corresponds to a unit vector on the plane with Cartesian components $(\\cos\\theta, \\sin\\theta)$, and that a weighted average of vectors is given by the sum of each vector scaled by its nonnegative weight, derive the preferred head direction for a session as the argument of the weighted resultant vector formed from the head direction unit vectors. Use nonnegative weights defined from spike phases by a cosine modulation around a reference phase:\n$$\nw_i = 1 + \\alpha \\cos(\\phi_i - \\phi_{\\text{ref}}),\n$$\nwhere $0 \\le \\alpha < 1$ and $\\phi_{\\text{ref}} \\in [0, 2\\pi)$ are specified parameters. You must express the preferred head direction $\\hat{\\mu}$ as the argument of the weighted sum of unit vectors, and ensure $\\hat{\\mu} \\in [0, 2\\pi)$ in radians.\n\nTo assess stability across sessions, define the circular distance between two preferred directions $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$ as the smallest absolute angular difference on the circle:\n$$\nd = \\left| \\operatorname{wrap}(\\hat{\\mu}_1 - \\hat{\\mu}_2) \\right|,\n$$\nwhere $\\operatorname{wrap}(\\Delta)$ maps any real $\\Delta$ to the unique representative in $(-\\pi, \\pi]$ that is congruent to $\\Delta$ modulo $2\\pi$. A pair of sessions is called stable if $d < \\tau$ for a given threshold $\\tau$ specified in radians.\n\nImplement a program that, for each provided test case, computes:\n- The preferred head direction $\\hat{\\mu}_1$ for session $1$ in radians;\n- The preferred head direction $\\hat{\\mu}_2$ for session $2$ in radians;\n- The circular distance $d$ in radians;\n- A boolean stability indicator given by whether $d < \\tau$.\n\nAll angles must be in radians. All float outputs must be rounded to $6$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is formatted as a list $[\\hat{\\mu}_1, \\hat{\\mu}_2, d, stable]$.\n\nUse the following test suite, which exercises typical, boundary, and edge-case scenarios:\n\nTest Case $1$ (unweighted mean, moderate concentration):\n- Parameters: $\\alpha = 0.0$, $\\phi_{\\text{ref}} = 0.0$, $\\tau = 0.5$.\n- Session $1$ pairs $(\\theta_i, \\phi_i)$:\n  $(0.9, 2.2)$, $(1.1, 0.5)$, $(0.95, 4.7)$, $(1.2, 1.0)$, $(1.05, 3.3)$.\n- Session $2$ pairs $(\\theta_i, \\phi_i)$:\n  $(0.8, 3.1)$, $(1.0, 2.5)$, $(1.15, 3.8)$, $(0.95, 0.2)$, $(1.05, 5.4)$.\n\nTest Case $2$ (phase-weighted mean emphasizing $\\phi \\approx \\phi_{\\text{ref}}$):\n- Parameters: $\\alpha = 0.6$, $\\phi_{\\text{ref}} = 0.0$, $\\tau = 0.5$.\n- Session $1$ pairs $(\\theta_i, \\phi_i)$:\n  $(1.4, 0.1)$, $(1.5, 0.05)$, $(1.6, 0.2)$, $(0.5, 3.1)$, $(0.6, 3.0)$, $(0.4, 2.9)$, $(1.55, 0.0)$, $(0.45, 3.2)$.\n- Session $2$ pairs $(\\theta_i, \\phi_i)$:\n  $(1.65, 0.0)$, $(1.7, 0.1)$, $(1.75, 0.05)$, $(0.6, 3.0)$, $(0.5, 3.1)$, $(0.4, 3.2)$, $(1.68, 0.15)$, $(0.55, 2.9)$.\n\nTest Case $3$ (wrap-around boundary near $0$ and $2\\pi$):\n- Parameters: $\\alpha = 0.3$, $\\phi_{\\text{ref}} = \\pi$, $\\tau = 0.5$.\n- Session $1$ pairs $(\\theta_i, \\phi_i)$:\n  $(0.02, 3.10)$, $(0.04, 3.05)$, $(0.06, 3.20)$, $(6.27, 3.15)$.\n- Session $2$ pairs $(\\theta_i, \\phi_i)$:\n  $(6.22, 3.00)$, $(6.25, 3.05)$, $(0.01, 3.10)$, $(6.20, 3.00)$.\n\nTest Case $4$ (single-spike edge case):\n- Parameters: $\\alpha = 0.9$, $\\phi_{\\text{ref}} = 0.0$, $\\tau = 0.5$.\n- Session $1$ pairs $(\\theta_i, \\phi_i)$: $(2.0, 0.5)$.\n- Session $2$ pairs $(\\theta_i, \\phi_i)$: $(2.3, 1.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$), where each $\\text{result}_k$ is a list $[\\hat{\\mu}_1, \\hat{\\mu}_2, d, stable]$ with all angles in radians and floats rounded to $6$ decimal places.",
            "solution": "The user-provided problem is valid as it is scientifically grounded in circular statistics, well-posed with all necessary information, and free of contradictions or ambiguities. We shall now proceed with a complete solution.\n\nThe problem requires us to perform two main tasks for each test case, which involves two sessions of neuronal recordings: first, to compute the preferred head direction for each session, and second, to assess the stability of these preferred directions across the two sessions.\n\n### Part 1: Estimation of Preferred Head Direction $\\hat{\\mu}$\n\nThe problem states that we must begin from the fundamental definition of an angle $\\theta$ as a unit vector on the Cartesian plane, $(\\cos\\theta, \\sin\\theta)$. The preferred direction $\\hat{\\mu}$ for a session is defined as the argument of the weighted resultant vector formed from the head direction unit vectors.\n\nLet a session consist of $N$ spike-phase pairs $(\\theta_i, \\phi_i)$ for $i=1, \\dots, N$.\n\n**Step 1: Represent Head Directions as Unit Vectors**\nEach head direction angle $\\theta_i$ corresponds to a unit vector $\\vec{v}_i$ in the 2D Cartesian plane:\n$$\n\\vec{v}_i = \\begin{pmatrix} \\cos\\theta_i \\\\ \\sin\\theta_i \\end{pmatrix}\n$$\n\n**Step 2: Define Weights from Spike Phases**\nThe contribution of each vector $\\vec{v}_i$ is weighted by a value $w_i$ derived from the corresponding spike phase $\\phi_i$. The weight $w_i$ is given by a cosine modulation around a reference phase $\\phi_{\\text{ref}}$:\n$$\nw_i = 1 + \\alpha \\cos(\\phi_i - \\phi_{\\text{ref}})\n$$\nThe problem specifies that the parameter $\\alpha$ satisfies $0 \\le \\alpha < 1$. Since the cosine function has a range of $[-1, 1]$, the weights $w_i$ are bounded by:\n$$\n1 + \\alpha(-1) \\le w_i \\le 1 + \\alpha(1) \\implies 1 - \\alpha \\le w_i \\le 1 + \\alpha\n$$\nGiven $\\alpha < 1$, it follows that $1 - \\alpha > 0$, ensuring all weights $w_i$ are strictly positive and thus well-defined for a weighted average.\n\n**Step 3: Compute the Weighted Resultant Vector**\nThe weighted resultant vector, $\\vec{R}$, is the sum of all unit vectors $\\vec{v}_i$ each scaled by its corresponding weight $w_i$:\n$$\n\\vec{R} = \\sum_{i=1}^{N} w_i \\vec{v}_i = \\sum_{i=1}^{N} w_i \\begin{pmatrix} \\cos\\theta_i \\\\ \\sin\\theta_i \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{N} w_i \\cos\\theta_i \\\\ \\sum_{i=1}^{N} w_i \\sin\\theta_i \\end{pmatrix}\n$$\nLet the Cartesian components of $\\vec{R}$ be $R_x$ and $R_y$:\n$$\nR_x = \\sum_{i=1}^{N} w_i \\cos\\theta_i\n$$\n$$\nR_y = \\sum_{i=1}^{N} w_i \\sin\\theta_i\n$$\n\n**Step 4: Determine the Preferred Direction $\\hat{\\mu}$**\nThe preferred direction $\\hat{\\mu}$ is the angle (or argument) of the resultant vector $\\vec{R}$. This angle is determined using the two-argument arctangent function, $\\operatorname{atan2}(y, x)$, which correctly resolves the angle in all four quadrants. The result, $\\hat{\\mu}_{\\text{raw}}$, is typically in the interval $(-\\pi, \\pi]$.\n$$\n\\hat{\\mu}_{\\text{raw}} = \\operatorname{atan2}(R_y, R_x)\n$$\nThe problem requires the final angle $\\hat{\\mu}$ to be in the range $[0, 2\\pi)$. We can map $\\hat{\\mu}_{\\text{raw}}$ to this range by adding $2\\pi$ to any negative result:\n$$\n\\hat{\\mu} = \\begin{cases}\n\\hat{\\mu}_{\\text{raw}} & \\text{if } \\hat{\\mu}_{\\text{raw}} \\ge 0 \\\\\n\\hat{\\mu}_{\\text{raw}} + 2\\pi & \\text{if } \\hat{\\mu}_{\\text{raw}} < 0\n\\end{cases}\n$$\nThis is equivalent to the modulo operation $(\\hat{\\mu}_{\\text{raw}} \\pmod{2\\pi})$ if the language's modulo operator handles negative numbers appropriately to produce a non-negative result. In Python/NumPy, `(mu_raw + 2*np.pi) % (2*np.pi)` achieves this mapping.\n\n### Part 2: Assessment of Stability Across Sessions\n\nStability is evaluated by comparing the preferred directions from two sessions, $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$, using a circular distance metric.\n\n**Step 1: Define Circular Distance**\nThe circular distance $d$ is the smallest absolute angular difference on a circle. Given two angles $\\hat{\\mu}_1, \\hat{\\mu}_2 \\in [0, 2\\pi)$, their difference is $\\Delta = \\hat{\\mu}_1 - \\hat{\\mu}_2$. The circular distance is based on wrapping this difference to the interval $(-\\pi, \\pi]$.\nThe `wrap` function is defined as:\n$$\n\\operatorname{wrap}(\\Delta) = (\\Delta + \\pi) \\pmod{2\\pi} - \\pi\n$$\nThis operation maps any real number $\\Delta$ to its unique congruent value in $(-\\pi, \\pi]$. For example, if $\\Delta = 1.9\\pi$, $\\operatorname{wrap}(\\Delta)$ would be $-0.1\\pi$. If $\\Delta= -1.9\\pi$, $\\operatorname{wrap}(\\Delta)$ would be $0.1\\pi$.\n\nThe circular distance $d$ is the absolute value of this wrapped difference:\n$$\nd = |\\operatorname{wrap}(\\hat{\\mu}_1 - \\hat{\\mu}_2)|\n$$\nThis ensures $d \\in [0, \\pi]$.\n\n**Step 2: Apply Stability Criterion**\nA pair of sessions is deemed stable if their circular distance $d$ is less than a given threshold $\\tau$.\n$$\n\\text{stable} = (d < \\tau)\n$$\nThe result is a boolean value (`True` or `False`).\n\n### Summary of Algorithmic Steps for one Test Case\n1.  For each session (session 1 and session 2):\n    a. Extract the spike-phase pairs $(\\theta_i, \\phi_i)$ and parameters $\\alpha, \\phi_{\\text{ref}}$.\n    b. Compute the weights $w_i = 1 + \\alpha \\cos(\\phi_i - \\phi_{\\text{ref}})$ for all spikes.\n    c. Calculate the components of the resultant vector, $R_x = \\sum w_i \\cos\\theta_i$ and $R_y = \\sum w_i \\sin\\theta_i$.\n    d. Compute the preferred direction $\\hat{\\mu}_{\\text{raw}} = \\operatorname{atan2}(R_y, R_x)$ and map it to $[0, 2\\pi)$ to get $\\hat{\\mu}$.\n2.  Having computed $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$:\n    a. Calculate the circular distance $d = |\\operatorname{wrap}(\\hat{\\mu}_1 - \\hat{\\mu}_2)|$.\n    b. Determine stability by comparing $d$ with the threshold $\\tau$.\n3.  Round all floating-point results ($\\hat{\\mu}_1, \\hat{\\mu}_2, d$) to $6$ decimal places and assemble the final list $[\\hat{\\mu}_1, \\hat{\\mu}_2, d, stable]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the neuroscience tuning analysis problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"alpha\": 0.0,\n            \"phi_ref\": 0.0,\n            \"tau\": 0.5,\n            \"session1_pairs\": np.array([\n                (0.9, 2.2), (1.1, 0.5), (0.95, 4.7), (1.2, 1.0), (1.05, 3.3)\n            ]),\n            \"session2_pairs\": np.array([\n                (0.8, 3.1), (1.0, 2.5), (1.15, 3.8), (0.95, 0.2), (1.05, 5.4)\n            ]),\n        },\n        {\n            \"alpha\": 0.6,\n            \"phi_ref\": 0.0,\n            \"tau\": 0.5,\n            \"session1_pairs\": np.array([\n                (1.4, 0.1), (1.5, 0.05), (1.6, 0.2), (0.5, 3.1), (0.6, 3.0),\n                (0.4, 2.9), (1.55, 0.0), (0.45, 3.2)\n            ]),\n            \"session2_pairs\": np.array([\n                (1.65, 0.0), (1.7, 0.1), (1.75, 0.05), (0.6, 3.0), (0.5, 3.1),\n                (0.4, 3.2), (1.68, 0.15), (0.55, 2.9)\n            ]),\n        },\n        {\n            \"alpha\": 0.3,\n            \"phi_ref\": np.pi,\n            \"tau\": 0.5,\n            \"session1_pairs\": np.array([\n                (0.02, 3.10), (0.04, 3.05), (0.06, 3.20), (6.27, 3.15)\n            ]),\n            \"session2_pairs\": np.array([\n                (6.22, 3.00), (6.25, 3.05), (0.01, 3.10), (6.20, 3.00)\n            ]),\n        },\n        {\n            \"alpha\": 0.9,\n            \"phi_ref\": 0.0,\n            \"tau\": 0.5,\n            \"session1_pairs\": np.array([(2.0, 0.5)]),\n            \"session2_pairs\": np.array([(2.3, 1.0)]),\n        },\n    ]\n\n    results = []\n\n    def compute_preferred_direction(pairs, alpha, phi_ref):\n        \"\"\"\n        Computes the weighted circular mean for a set of spike-phase pairs.\n        \n        Args:\n            pairs (np.ndarray): An array of (theta, phi) pairs.\n            alpha (float): The modulation strength parameter.\n            phi_ref (float): The reference phase.\n\n        Returns:\n            float: The preferred direction mu in [0, 2*pi).\n        \"\"\"\n        if pairs.shape[0] == 0:\n            return 0.0 # Or handle as an error, though not specified for this problem\n\n        thetas = pairs[:, 0]\n        phis = pairs[:, 1]\n\n        # Calculate weights based on phase\n        weights = 1 + alpha * np.cos(phis - phi_ref)\n\n        # Calculate components of the weighted resultant vector\n        Rx = np.sum(weights * np.cos(thetas))\n        Ry = np.sum(weights * np.sin(thetas))\n\n        # Calculate the raw angle using atan2\n        mu_raw = np.arctan2(Ry, Rx)\n        \n        # Map the angle to the range [0, 2*pi)\n        mu = (mu_raw + 2 * np.pi) % (2 * np.pi)\n        \n        return mu\n\n    def compute_circular_distance(mu1, mu2):\n        \"\"\"\n        Computes the circular distance between two angles.\n        \n        Args:\n            mu1 (float): First angle in radians.\n            mu2 (float): Second angle in radians.\n\n        Returns:\n            float: The circular distance d in [0, pi].\n        \"\"\"\n        delta = mu1 - mu2\n        # Wrap the difference to the interval (-pi, pi]\n        wrapped_delta = (delta + np.pi) % (2 * np.pi) - np.pi\n        # The distance is the absolute value of the wrapped difference\n        distance = np.abs(wrapped_delta)\n        return distance\n\n    for case in test_cases:\n        alpha = case[\"alpha\"]\n        phi_ref = case[\"phi_ref\"]\n        tau = case[\"tau\"]\n\n        # Compute preferred direction for session 1\n        mu1 = compute_preferred_direction(case[\"session1_pairs\"], alpha, phi_ref)\n\n        # Compute preferred direction for session 2\n        mu2 = compute_preferred_direction(case[\"session2_pairs\"], alpha, phi_ref)\n        \n        # Compute circular distance\n        d = compute_circular_distance(mu1, mu2)\n\n        # Check for stability\n        stable = d < tau\n\n        # Round float outputs to 6 decimal places\n        mu1_rounded = round(mu1, 6)\n        mu2_rounded = round(mu2, 6)\n        d_rounded = round(d, 6)\n\n        results.append([mu1_rounded, mu2_rounded, d_rounded, stable])\n\n    # Format the output as a single-line string representation of a list of lists.\n    # The map(str, ...) will convert each inner list like [1.2, 3.4, 0.5, True]\n    # into its string representation, e.g., \"'[1.2, 3.4, 0.5, True]'\".\n    # Joining these with commas and enclosing in brackets gives the desired format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "After estimating a neuron's preferred direction, the immediate next question is whether this tuning is statistically significant or simply a result of random fluctuations. This exercise introduces two foundational statistical tools for this purpose: the Rayleigh test and the V test. This practice  will guide you through implementing these tests to distinguish genuine neural tuning from uniform random firing, highlighting the crucial difference in statistical power when a specific direction of tuning is hypothesized beforehand.",
            "id": "4202325",
            "problem": "You are given several finite samples of angles that represent stimulus directions recorded in a tuning analysis context. The objective is to implement a program that, for each sample, applies two classical tests from circular statistics to assess deviation from the uniform distribution of angles on the circle and to compare their relative power against unimodal alternatives when a mean direction is specified. Specifically, you will apply the Rayleigh test and the V test with a specified mean direction, and you will compute their associated one-sided tail probabilities (p-values) under the null hypothesis of uniformity on the circle. Angles must be treated in radians.\n\nStarting from a fundamental base, you must derive the necessary statistical test statistics and tail probabilities for both tests without assuming any shortcut formulas. You may rely on the following core definitions and well-tested facts:\n- Unit circle representation for angles, where each angle corresponds to a point on the unit circle in the plane.\n- The mean resultant vector of angles obtained by summing unit vectors associated with each angle.\n- Under the null hypothesis of uniformity on the circle, the real and imaginary components of the normalized resultant vector are independent and identically distributed with mean $0$, and obey large-sample approximations that enable derivation of the tail probability for the Rayleigh test statistic.\n- The Central Limit Theorem (CLT) and properties of the cosine function under the null hypothesis enable derivation of a one-sided normal approximation for the V test when a mean direction is specified.\n\nYou must compute, for each sample:\n- The Rayleigh test p-value against the null hypothesis of uniformity on the circle.\n- The V test p-value against the null hypothesis of uniformity, for a specified mean direction, as a one-sided test targeting concentration in the given direction.\n- A boolean decision of statistical significance for each test at significance level $\\alpha = 0.05$.\n\nAngle unit specification: all angles are in radians. Your program must assume angles are given in $[0, 2\\pi)$, but you should robustly handle and wrap any provided angles to this range.\n\nTest suite:\n- Case $1$ (boundary uniformity): angles are uniformly spaced on the circle, $n = 16$, $\\theta_k = \\frac{2\\pi k}{16}$ for $k \\in \\{0, 1, \\dots, 15\\}$, specified mean direction $\\mu_0 = \\frac{\\pi}{4}$.\n- Case $2$ (unimodal aligned with specified direction): $n = 12$, angles are $\\theta_i = \\mu_0 + \\delta_i$ with $\\mu_0 = \\frac{\\pi}{3}$ and $\\delta_i \\in \\{-0.10, -0.08, -0.06, -0.04, -0.02, 0.00, 0.02, 0.04, 0.06, 0.08, 0.10, 0.00\\}$, wrapped to $[0, 2\\pi)$.\n- Case $3$ (unimodal misaligned with specified direction): $n = 10$, true concentration around $\\mu_{\\text{true}} = \\frac{\\pi}{2}$ but specified mean direction $\\mu_0 = 0.00$, angles are $\\theta_i = \\mu_{\\text{true}} + \\delta_i$ with $\\delta_i \\in \\{-0.12, -0.08, -0.04, 0.00, 0.04, 0.08, 0.12, 0.00, 0.00, 0.00\\}$, wrapped to $[0, 2\\pi)$.\n- Case $4$ (axial bimodality as an edge case): $n = 6$, two clusters centered at $\\mu_0$ and $\\mu_0 + \\pi$ with $\\mu_0 = \\frac{\\pi}{6}$ and offsets $\\{-0.05, 0.00, 0.05\\}$ around each center, angles are the union of the two clusters, wrapped to $[0, 2\\pi)$.\n- Case $5$ (small sample unimodal aligned): $n = 5$, specified mean direction $\\mu_0 = -\\frac{\\pi}{3}$, offsets $\\{-0.20, -0.10, 0.00, 0.10, 0.20\\}$, angles $\\theta_i = \\mu_0 + \\delta_i$ wrapped to $[0, 2\\pi)$.\n\nDerive, from first principles, the Rayleigh test statistic based on the mean resultant length of unit vectors corresponding to the angles, and its large-sample null distribution to obtain the tail probability. Derive, using the Central Limit Theorem and properties of the cosine projection onto a specified direction, the V test statistic and its one-sided tail probability. Then implement a program that, for each test case, computes:\n- The Rayleigh test p-value as a float,\n- The V test p-value as a float,\n- The boolean decision for Rayleigh test at $\\alpha = 0.05$,\n- The boolean decision for V test at $\\alpha = 0.05$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of four entries in the order described above. P-values must be rounded to six decimal places, and booleans must be printed as either True or False. For example, your program must print something of the form $[\\,[p_1, p_1', d_1, d_1'],\\,[p_2, p_2', d_2, d_2'],\\,\\dots\\,]$, with no spaces.",
            "solution": "We begin by representing each angle on the unit circle as a unit vector. Given angles $\\{\\theta_i\\}_{i=1}^n$, define the sum of unit vectors by the complex representation or equivalently by its Cartesian components. The Cartesian components of the resultant vector are\n$$\nC = \\sum_{i=1}^n \\cos(\\theta_i), \\quad S = \\sum_{i=1}^n \\sin(\\theta_i).\n$$\nThe mean resultant length is\n$$\n\\bar{R} = \\frac{1}{n} \\left\\| \\sum_{i=1}^n (\\cos(\\theta_i), \\sin(\\theta_i)) \\right\\| = \\frac{1}{n} \\sqrt{C^2 + S^2}.\n$$\nUnder the null hypothesis of uniformity on the circle, each unit vector is equally likely to point in any direction, and by symmetry the expected resultant is zero. For large $n$, the Central Limit Theorem implies that the normalized components $C/n$ and $S/n$ behave like averages of independent random variables with mean $0$ and variance $1/2n$. More precisely, the scaled quantities $\\sqrt{n}\\,C/n$ and $\\sqrt{n}\\,S/n$ tend to independent normal distributions with mean $0$ and variance $1/2$. Consequently, the statistic\n$$\nT_R = 2n \\,\\bar{R}^2 = 2n \\left(\\frac{C^2 + S^2}{n^2}\\right)\n$$\nhas, asymptotically under the null, a chi-square distribution with $2$ degrees of freedom. The survival function of a chi-square distribution with $2$ degrees of freedom is $\\exp(-x/2)$. Therefore, the Rayleigh test tail probability (p-value) based on the large-sample approximation is\n$$\np_R = \\Pr\\left(\\chi^2_2 \\ge T_R\\right) = \\exp\\left(-\\frac{T_R}{2}\\right) = \\exp\\left(-n \\bar{R}^2\\right).\n$$\nThis provides a principled way to compute the Rayleigh test p-value from first principles using the mean resultant length and the asymptotic null distribution.\n\nFor the V test, we assume we have a specified mean direction $\\mu_0$ that represents the direction against which power is targeted. The idea is to project each unit vector onto the axis defined by $\\mu_0$, and then assess whether the average projection is positive and large compared to what would be expected under uniformity. Define the projection average\n$$\nu = \\frac{1}{n} \\sum_{i=1}^n \\cos(\\theta_i - \\mu_0).\n$$\nUnder the null hypothesis of uniformity, $\\cos(\\Theta - \\mu_0)$ has mean $0$ by symmetry and variance $1/2$ (because $\\cos(\\Theta)$ over $\\Theta \\sim \\text{Uniform}(0, 2\\pi)$ has mean $0$ and variance $1/2$). By the Central Limit Theorem, the sample mean $u$ is approximately normal with mean $0$ and variance $1/(2n)$. Thus the standardized statistic\n$$\nT_V = \\sqrt{2n}\\,u\n$$\nis approximately standard normal under the null. Since the alternative is one-sided (concentration in the direction $\\mu_0$ implies $u > 0$), the one-sided tail probability (p-value) for the V test is\n$$\np_V = \\Pr\\left(Z \\ge T_V\\right) = 1 - \\Phi(T_V),\n$$\nwhere $\\Phi(\\cdot)$ denotes the cumulative distribution function of the standard normal distribution. Numerically, this can be computed via the complementary error function as\n$$\np_V = \\frac{1}{2}\\,\\operatorname{erfc}\\left(\\frac{T_V}{\\sqrt{2}}\\right).\n$$\n\nAlgorithmic design:\n- Wrap all angles $\\theta_i$ to $[0, 2\\pi)$ to ensure consistent trigonometric evaluations.\n- Compute $C$, $S$, and $\\bar{R}$; then compute $p_R = \\exp(-n \\bar{R}^2)$.\n- Compute $u = (1/n)\\sum \\cos(\\theta_i - \\mu_0)$; then compute $T_V = \\sqrt{2n}\\,u$ and $p_V = \\frac{1}{2}\\,\\operatorname{erfc}\\left(T_V/\\sqrt{2}\\right)$.\n- Compare each p-value to $\\alpha = 0.05$ to obtain boolean decisions for significance.\n- Round p-values to six decimal places for output.\n- Aggregate per-case results into the specified single-line string format with no spaces.\n\nInterpretation of differences in power:\n- The Rayleigh test is powerful against a broad class of unimodal alternatives because it detects any nonzero mean resultant length, regardless of the direction. It is less effective for axial bimodality or distributions where resultant vectors cancel (e.g., two symmetric clusters at opposite directions), leading to small $\\bar{R}$ and large $p_R$.\n- The V test is more powerful when the true mean direction aligns with the specified $\\mu_0$, because averaging $\\cos(\\theta_i - \\mu_0)$ directly targets concentration along that axis. If the true mean direction differs from $\\mu_0$, the V test loses power relative to the Rayleigh test, since the projection $u$ can be near zero and yield larger $p_V$ even when the distribution is unimodal but misaligned.\n\nApplication to the test suite:\n- Case $1$: perfectly uniform spacing yields $\\bar{R} = 0$, so $p_R = 1$, while $u = 0$ also yields $p_V = 0.5$. Neither test is significant at $\\alpha = 0.05$.\n- Case $2$: angles concentrated near $\\mu_0$ yield large $\\bar{R}$ and large $u$, thus both $p_R$ and $p_V$ are small; both tests are significant.\n- Case $3$: angles concentrated near $\\mu_{\\text{true}} = \\pi/2$ while $\\mu_0 = 0$ yields large $\\bar{R}$ (small $p_R$) but $u \\approx 0$ (large $p_V$), so Rayleigh is significant while V test is not, illustrating the difference in targeted power.\n- Case $4$: axial bimodality yields cancellation in the resultant vector ($\\bar{R}$ small) and mean projection ($u$ small), giving large $p_R$ and large $p_V$; neither test is significant, showing a classical limitation of tests targeted at unimodality with a single resultant.\n- Case $5$: small sample concentrated near $\\mu_0$ yields moderately large $\\bar{R}$ and $u$, with $p_V$ often smaller than $p_R$ because it leverages the specified direction, illustrating improved power of V test in aligned small samples.\n\nThe program implements these computations and prints the per-case results as $[\\,[p_R, p_V, d_R, d_V],\\,\\dots\\,]$ with p-values rounded to six decimal places and decisions at $\\alpha = 0.05$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef wrap_angles(theta):\n    \"\"\"\n    Wrap angles to [0, 2*pi).\n    \"\"\"\n    two_pi = 2.0 * math.pi\n    return np.mod(theta, two_pi)\n\ndef rayleigh_p_value(theta):\n    \"\"\"\n    Compute the Rayleigh test p-value using the large-sample approximation:\n    p = exp(-n * Rbar^2), where Rbar is the mean resultant length.\n    \"\"\"\n    n = theta.size\n    C = np.sum(np.cos(theta))\n    S = np.sum(np.sin(theta))\n    Rbar = math.sqrt(C**2 + S**2) / n\n    Z = n * (Rbar**2)\n    p = math.exp(-Z)\n    return p\n\ndef v_test_p_value(theta, mu0):\n    \"\"\"\n    Compute the V test p-value with specified mean direction mu0.\n    Statistic T_V = sqrt(2n) * u, where u = (1/n) sum cos(theta_i - mu0).\n    One-sided p = 0.5 * erfc(T_V / sqrt(2)).\n    \"\"\"\n    n = theta.size\n    # Projection onto specified direction\n    u = np.mean(np.cos(theta - mu0))\n    T_V = math.sqrt(2.0 * n) * u\n    # One-sided tail probability for standard normal\n    p = 0.5 * math.erfc(T_V / math.sqrt(2.0))\n    return p\n\ndef format_nested_list(lst):\n    \"\"\"\n    Format a nested list into a single-line string with no spaces,\n    floats rounded to 6 decimals, and booleans as True/False.\n    \"\"\"\n    def fmt(item):\n        if isinstance(item, list):\n            return \"[\" + \",\".join(fmt(x) for x in item) + \"]\"\n        elif isinstance(item, float):\n            return f\"{item:.6f}\"\n        elif isinstance(item, bool):\n            # Use Python's capitalization for booleans\n            return \"True\" if item else \"False\"\n        else:\n            return str(item)\n    return \"[\" + \",\".join(fmt(x) for x in lst) + \"]\"\n\ndef solve():\n    two_pi = 2.0 * math.pi\n    # Define the test cases from the problem statement.\n    # Each case: (angles array in radians, specified mean direction mu0)\n    # Case 1: Uniform spacing\n    case1_angles = np.array([2.0 * math.pi * k / 16.0 for k in range(16)], dtype=float)\n    case1_mu0 = math.pi / 4.0\n\n    # Case 2: Unimodal aligned with specified direction\n    case2_mu0 = math.pi / 3.0\n    case2_offsets = np.array([-0.10, -0.08, -0.06, -0.04, -0.02, 0.00,\n                               0.02,  0.04,  0.06,  0.08,  0.10, 0.00], dtype=float)\n    case2_angles = case2_mu0 + case2_offsets\n\n    # Case 3: Unimodal misaligned with specified direction\n    case3_mu_true = math.pi / 2.0\n    case3_mu0 = 0.0\n    case3_offsets = np.array([-0.12, -0.08, -0.04, 0.00, 0.04,\n                               0.08,  0.12, 0.00, 0.00, 0.00], dtype=float)\n    case3_angles = case3_mu_true + case3_offsets\n\n    # Case 4: Axial bimodality (two opposite clusters)\n    case4_mu0 = math.pi / 6.0\n    case4_offsets = np.array([-0.05, 0.00, 0.05], dtype=float)\n    case4_angles = np.concatenate([\n        case4_mu0 + case4_offsets,\n        (case4_mu0 + math.pi) + case4_offsets\n    ])\n\n    # Case 5: Small sample unimodal aligned\n    case5_mu0 = -math.pi / 3.0\n    case5_offsets = np.array([-0.20, -0.10, 0.00, 0.10, 0.20], dtype=float)\n    case5_angles = case5_mu0 + case5_offsets\n\n    test_cases = [\n        (case1_angles, case1_mu0),\n        (wrap_angles(case2_angles), case2_mu0),\n        (wrap_angles(case3_angles), case3_mu0),\n        (wrap_angles(case4_angles), case4_mu0),\n        (wrap_angles(case5_angles), case5_mu0),\n    ]\n\n    alpha = 0.05\n    results = []\n    for angles, mu0 in test_cases:\n        angles_wrapped = wrap_angles(angles)\n        p_rayleigh = rayleigh_p_value(angles_wrapped)\n        p_v = v_test_p_value(angles_wrapped, mu0)\n        decision_rayleigh = p_rayleigh < alpha\n        decision_v = p_v < alpha\n        results.append([p_rayleigh, p_v, decision_rayleigh, decision_v])\n\n    # Round p-values to six decimals in the output and remove spaces as specified.\n    print(format_nested_list(results))\n\nsolve()\n```"
        },
        {
            "introduction": "While descriptive statistics and basic tests confirm the presence of tuning, a deeper understanding requires a full parametric model of the tuning curve itself. This advanced practice moves into the powerful framework of Generalized Linear Models (GLMs) to precisely characterize the relationship between a circular variable and neuronal firing rate. In this exercise , you will build a Poisson GLM with harmonic predictors from the ground up, implement the Iteratively Reweighted Least Squares (IRLS) fitting procedure, and use the likelihood ratio test to evaluate the significance of specific tuning components.",
            "id": "4202411",
            "problem": "You are given a task in neuroscience data analysis to assess directional tuning in neuronal firing rates using circular predictors on the unit circle. The fundamental base is the definition of the Poisson Generalized Linear Model (GLM) with the canonical logarithmic link, the construction of harmonic predictors for circular variables, and the likelihood ratio test between nested models grounded in Wilks' theorem.\n\nYou must implement a program that does the following for a set of synthetic datasets:\n\n1. Generate spike count data under a Poisson model. For each dataset, define angles $\\theta_i$ uniformly spaced on $[0,2\\pi)$ in radians, and simulate counts $y_i$ as draws from a Poisson distribution with mean\n$$\n\\lambda_i = \\exp\\left(\\beta_0 + A_1 \\cos(\\theta_i - \\phi_1) + A_2 \\cos\\left(2(\\theta_i - \\phi_2)\\right)\\right).\n$$\nAll angles must be treated in radians. The random number generator must be seeded deterministically to ensure reproducibility.\n\n2. Fit two nested Poisson GLMs with a logarithmic link using Iteratively Reweighted Least Squares (IRLS):\n   - Reduced model (without the first harmonic):\n     $$\n     \\eta_i^{\\text{red}} = \\beta_0 + \\gamma_c \\cos(2\\theta_i) + \\gamma_s \\sin(2\\theta_i),\n     $$\n     which corresponds to predictors $\\{1, \\cos(2\\theta_i), \\sin(2\\theta_i)\\}$.\n   - Full model (with the first harmonic added):\n     $$\n     \\eta_i^{\\text{full}} = \\beta_0 + \\beta_c \\cos(\\theta_i) + \\beta_s \\sin(\\theta_i) + \\gamma_c \\cos(2\\theta_i) + \\gamma_s \\sin(2\\theta_i),\n     $$\n     which corresponds to predictors $\\{1, \\cos(\\theta_i), \\sin(\\theta_i), \\cos(2\\theta_i), \\sin(2\\theta_i)\\}$.\n\n   The log-likelihood (up to an additive constant independent of parameters) is\n   $$\n   \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\eta_i - \\exp(\\eta_i) \\right),\n   $$\n   where $\\eta_i$ is the linear predictor for the model and $\\boldsymbol{\\beta}$ denotes the parameter vector.\n\n3. Compute the likelihood ratio statistic for testing the significance of the first harmonic under the null hypothesis $H_0: \\beta_c = 0, \\beta_s = 0$ versus the alternative that at least one of these is non-zero:\n   $$\n   T = 2\\left(\\ell_{\\text{full}} - \\ell_{\\text{reduced}}\\right).\n   $$\n   Under regularity conditions and $H_0$, $T$ is asymptotically distributed as a chi-square with $k=2$ degrees of freedom. Compute the $p$-value using the chi-square survival function with $k=2$, and decide significance at level $\\alpha = 0.05$.\n\nYour program must implement IRLS from first principles for the Poisson GLM. Specifically, starting from an initial parameter vector, update the parameter estimates using the Fisher scoring step for Poisson with logarithmic link:\n- At each iteration, compute $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$, $\\mu_i = \\exp(\\eta_i)$, weights $w_i = \\mu_i$, working response $z_i = \\eta_i + \\frac{y_i - \\mu_i}{\\mu_i}$, and solve the weighted least squares system\n$$\n\\left(\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\\right) \\boldsymbol{\\beta}^{\\text{new}} = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{z},\n$$\nwhere $\\mathbf{X}$ is the design matrix, $\\mathbf{W} = \\mathrm{diag}(w_i)$, and $\\mathbf{z}$ is the vector of working responses. Iterate until convergence in parameter norm.\n\nAngle units: All angles must be in radians.\n\nTest suite specification:\nImplement the following four datasets, each specified by the tuple $(n, \\beta_0, A_1, \\phi_1, A_2, \\phi_2)$, where $n$ is the number of angles, $\\beta_0$ is the baseline log-rate, $A_1$ and $\\phi_1$ are the amplitude and preferred direction for the first harmonic, and $A_2$ and $\\phi_2$ are the amplitude and preferred direction for the second harmonic. Use the same deterministic random seed across all datasets. The data generation for each dataset must use the stated $\\lambda_i$ formula.\n\n- Case 1 (general case with strong first-harmonic tuning): $(60, 1.2, 0.8, 0.7, 0.3, 2.0)$\n- Case 2 (no first-harmonic tuning but strong second harmonic): $(60, 1.2, 0.0, 1.0, 0.9, 0.5)$\n- Case 3 (small sample with weak first harmonic, no second harmonic): $(24, 0.1, 0.2, 2.5, 0.0, 1.0)$\n- Case 4 (moderate sample with both harmonics present): $(36, 2.0, 0.4, -1.0, 0.6, 1.7)$\n\nFinal output format:\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each element is a boolean indicating whether the first harmonic is significant at level $\\alpha = 0.05$ for that test case (i.e., $p < 0.05$), in the order of the test cases above. For example: \"[True,False,True,False]\". No other text should be printed.",
            "solution": "The task is to implement a statistical procedure to test for the significance of directional tuning in synthetic neuronal spike count data. This involves generating data from a known Poisson process, fitting two nested Generalized Linear Models (GLMs) using a custom implementation of the Iteratively Reweighted Least Squares (IRLS) algorithm, and performing a likelihood ratio test. The entire process is grounded in fundamental principles of statistical modeling and hypothesis testing.\n\n### 1. Data Generation\n\nThe first step is to generate synthetic spike count data. For each dataset, a set of $n$ angles, $\\theta_i$, is uniformly spaced over the interval $[0, 2\\pi)$. The spike count, $y_i$, for each angle is a random draw from a Poisson distribution, $y_i \\sim \\text{Poisson}(\\lambda_i)$. The mean firing rate, $\\lambda_i$, is a function of the angle $\\theta_i$ and is defined by a log-linear model incorporating first and second-order harmonics of the angle:\n$$\n\\lambda_i = \\exp\\left(\\beta_0 + A_1 \\cos(\\theta_i - \\phi_1) + A_2 \\cos\\left(2(\\theta_i - \\phi_2)\\right)\\right)\n$$\nHere, $\\beta_0$ is the baseline log firing rate. $A_1$ and $\\phi_1$ are the amplitude and preferred direction (phase) of the first harmonic component, respectively, which typically models unimodal tuning. $A_2$ and $\\phi_2$ are the amplitude and preferred direction of the second harmonic, which can capture bimodal tuning patterns. All angular quantities are in radians. Data generation requires a deterministic random number generator for reproducibility.\n\n### 2. Model Specification\n\nTo test the significance of the first harmonic component, we fit two nested Poisson GLMs with a logarithmic link function, $\\log(\\mu_i) = \\eta_i$, where $\\mu_i = E[y_i]$ and $\\eta_i$ is the linear predictor.\n\nThe linear predictors are constructed using trigonometric identities. The term $A_k \\cos(k(\\theta_i - \\phi_k))$ can be expanded as $A_k (\\cos(k\\theta_i)\\cos(k\\phi_k) + \\sin(k\\theta_i)\\sin(k\\phi_k))$. This is a linear combination of $\\cos(k\\theta_i)$ and $\\sin(k\\theta_i)$, with coefficients that depend on $A_k$ and $\\phi_k$.\n\n**Full Model ($M_{\\text{full}}$):** This model includes both first and second harmonics. Its linear predictor is:\n$$\n\\eta_i^{\\text{full}} = \\beta_0 + \\beta_c \\cos(\\theta_i) + \\beta_s \\sin(\\theta_i) + \\gamma_c \\cos(2\\theta_i) + \\gamma_s \\sin(2\\theta_i)\n$$\nThis corresponds to a design matrix, $\\mathbf{X}_{\\text{full}}$, with five columns: an intercept ($1$), $\\cos(\\theta_i)$, $\\sin(\\theta_i)$, $\\cos(2\\theta_i)$, and $\\sin(2\\theta_i)$. The parameter vector is $\\boldsymbol{\\beta}_{\\text{full}} = [\\beta_0, \\beta_c, \\beta_s, \\gamma_c, \\gamma_s]^\\top$.\n\n**Reduced Model ($M_{\\text{red}}$):** This model is nested within the full model and represents the null hypothesis that the first harmonic component is absent ($A_1=0$, which implies $\\beta_c = 0$ and $\\beta_s = 0$). The linear predictor omits the first harmonic terms:\n$$\n\\eta_i^{\\text{red}} = \\beta_0 + \\gamma_c \\cos(2\\theta_i) + \\gamma_s \\sin(2\\theta_i)\n$$\nThe corresponding design matrix, $\\mathbf{X}_{\\text{red}}$, has three columns: an intercept, $\\cos(2\\theta_i)$, and $\\sin(2\\theta_i)$. The parameter vector is $\\boldsymbol{\\beta}_{\\text{red}} = [\\beta_0, \\gamma_c, \\gamma_s]^\\top$.\n\n### 3. Model Fitting via Iteratively Reweighted Least Squares (IRLS)\n\nThe parameters for both models are estimated by maximizing the Poisson log-likelihood. For a Poisson GLM, the log-likelihood function, ignoring terms that are constant with respect to the parameters $\\boldsymbol{\\beta}$, is:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\eta_i - \\exp(\\eta_i) \\right) = \\sum_{i=1}^{n} \\left( y_i (\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) - \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) \\right)\n$$\nwhere $\\mathbf{x}_i^\\top$ is the $i$-th row of the design matrix $\\mathbf{X}$. The IRLS algorithm is a numerical method to find the maximum likelihood estimate of $\\boldsymbol{\\beta}$. It iteratively solves a series of weighted least squares problems. For a Poisson model with a canonical log link, the update step for $\\boldsymbol{\\beta}$ at each iteration is given by the Fisher scoring algorithm, which is equivalent to Newton-Raphson in this case.\n\nThe algorithm proceeds as follows:\n1.  Initialize the parameter vector $\\boldsymbol{\\beta}$, for instance, as a vector of zeros.\n2.  Iterate until convergence:\n    a.  Compute the linear predictor for each observation: $\\boldsymbol{\\eta} = \\mathbf{X} \\boldsymbol{\\beta}$.\n    b.  Compute the estimated mean response using the inverse link function: $\\boldsymbol{\\mu} = \\exp(\\boldsymbol{\\eta})$.\n    c.  Define the diagonal weight matrix $\\mathbf{W}$ with diagonal elements $w_i = \\text{Var}(y_i) / (\\text{g}'(\\mu_i))^2$. For the Poisson distribution, $\\text{Var}(y_i)=\\mu_i$, and for the log link, $g(\\mu_i)=\\log(\\mu_i)$, so $g'(\\mu_i)=1/\\mu_i$. This gives $w_i = \\mu_i / (1/\\mu_i)^2 = \\mu_i^2 / \\mu_i = \\mu_i$.\n    d.  Compute the working response (or \"adjusted dependent variable\"): $z_i = \\eta_i + (y_i - \\mu_i) / \\mu_i$.\n    e.  Solve the weighted least squares system for the new parameter estimate $\\boldsymbol{\\beta}^{\\text{new}}$:\n       $$\n       \\left(\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\\right) \\boldsymbol{\\beta}^{\\text{new}} = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{z}\n       $$\n    f.  Check for convergence by examining the norm of the change in parameters, $\\|\\boldsymbol{\\beta}^{\\text{new}} - \\boldsymbol{\\beta}\\|$. If it is below a specified tolerance, terminate. Otherwise, update $\\boldsymbol{\\beta} \\leftarrow \\boldsymbol{\\beta}^{\\text{new}}$ and continue.\n\nThis procedure is applied separately to fit both the full and reduced models, yielding parameter estimates $\\hat{\\boldsymbol{\\beta}}_{\\text{full}}$ and $\\hat{\\boldsymbol{\\beta}}_{\\text{red}}$, respectively.\n\n### 4. Likelihood Ratio Test\n\nWith the maximized log-likelihood values for both models, $\\ell_{\\text{full}} = \\ell(\\hat{\\boldsymbol{\\beta}}_{\\text{full}})$ and $\\ell_{\\text{red}} = \\ell(\\hat{\\boldsymbol{\\beta}}_{\\text{red}})$, we can test the null hypothesis $H_0: \\beta_c = \\beta_s = 0$. The likelihood ratio test (LRT) statistic is:\n$$\nT = 2(\\ell_{\\text{full}} - \\ell_{\\text{reduced}})\n$$\nAccording to Wilks' theorem, under the null hypothesis, $T$ asymptotically follows a chi-square ($\\chi^2$) distribution. The degrees of freedom, $k$, for this distribution is the difference in the number of parameters between the full and reduced models. In this case, $k = 5 - 3 = 2$.\n\nThe $p$-value is the probability of observing a test statistic as extreme as, or more extreme than, the one computed, assuming the null hypothesis is true. This is calculated using the survival function (1 minus the cumulative distribution function) of the $\\chi^2_2$ distribution:\n$$\np = P(\\chi^2_2 \\ge T)\n$$\nFinally, we compare the $p$-value to a pre-defined significance level, $\\alpha = 0.05$. If $p < 0.05$, we reject the null hypothesis and conclude that the first harmonic component is statistically significant. Otherwise, we fail to reject the null hypothesis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef fit_poisson_glm_irls(y, X, max_iter=100, tol=1e-8):\n    \"\"\"\n    Fits a Poisson GLM with a log link using IRLS from first principles.\n\n    Args:\n        y (np.ndarray): The response variable (spike counts), shape (n_samples,).\n        X (np.ndarray): The design matrix, shape (n_samples, n_features).\n        max_iter (int): Maximum number of iterations for the IRLS algorithm.\n        tol (float): Convergence tolerance for the norm of parameter updates.\n\n    Returns:\n        tuple[np.ndarray, float]: Tuple containing the fitted coefficients (beta)\n                                  and the maximized log-likelihood.\n    \"\"\"\n    # Initialize beta coefficients to zero\n    beta = np.zeros(X.shape[1])\n    \n    for _ in range(max_iter):\n        eta = X @ beta\n        # Clamp eta to prevent numerical overflow/underflow in np.exp\n        eta = np.clip(eta, -25, 25)\n        mu = np.exp(eta)\n        \n        # Guard against division by zero if mu is very small\n        mu[mu < 1e-10] = 1e-10\n        \n        # Compute weights and working response for Poisson with log link\n        weights = mu\n        z = eta + (y - mu) / mu\n        \n        # Form and solve the weighted least squares system\n        # X.T @ W @ X * beta_new = X.T @ W @ z\n        # W is a diagonal matrix, so we can use broadcasting for efficiency\n        X_T_W = X.T * weights[np.newaxis, :]  # (p, n)\n        \n        # Left-hand side: Hessian or Fisher Information Matrix\n        hessian = X_T_W @ X  # (p, p)\n        \n        # Right-hand side\n        rhs = X_T_W @ z  # (p,)\n        \n        try:\n            beta_new = np.linalg.solve(hessian, rhs)\n        except np.linalg.LinAlgError:\n            # If solver fails, use pseudo-inverse as a fallback\n            beta_new = np.linalg.pinv(hessian) @ rhs\n\n        # Check for convergence\n        if np.linalg.norm(beta_new - beta) < tol:\n            beta = beta_new\n            break\n        \n        beta = beta_new\n    \n    # Calculate final log-likelihood\n    final_eta = X @ beta\n    log_likelihood = np.sum(y * final_eta - np.exp(final_eta))\n    \n    return beta, log_likelihood\n\n\ndef solve():\n    \"\"\"\n    Main function to run the tuning analysis for all specified test cases.\n    \"\"\"\n    # Test suite specification: (n, beta0, A1, phi1, A2, phi2)\n    test_cases = [\n        (60, 1.2, 0.8, 0.7, 0.3, 2.0),\n        (60, 1.2, 0.0, 1.0, 0.9, 0.5),\n        (24, 0.1, 0.2, 2.5, 0.0, 1.0),\n        (36, 2.0, 0.4, -1.0, 0.6, 1.7)\n    ]\n    \n    # Use a deterministic random seed for reproducibility\n    rng = np.random.default_rng(seed=42)\n    alpha = 0.05\n    results = []\n\n    for n, beta0, A1, phi1, A2, phi2 in test_cases:\n        # 1. Generate synthetic data\n        theta = np.linspace(0, 2 * np.pi, n, endpoint=False)\n        \n        log_lambda = beta0 + A1 * np.cos(theta - phi1) + A2 * np.cos(2 * (theta - phi2))\n        lambda_i = np.exp(log_lambda)\n        \n        y = rng.poisson(lam=lambda_i)\n        \n        # 2. Define design matrices for reduced and full models\n        # Reduced model: Intercept, cos(2*theta), sin(2*theta)\n        X_red = np.c_[np.ones(n), np.cos(2 * theta), np.sin(2 * theta)]\n        \n        # Full model: Intercept, cos/sin(theta), cos/sin(2*theta)\n        X_full = np.c_[\n            np.ones(n), \n            np.cos(theta), \n            np.sin(theta), \n            np.cos(2 * theta), \n            np.sin(2 * theta)\n        ]\n        \n        # 3. Fit both models using IRLS\n        _, ll_red = fit_poisson_glm_irls(y, X_red)\n        _, ll_full = fit_poisson_glm_irls(y, X_full)\n        \n        # 4. Perform the Likelihood Ratio Test\n        # Test statistic\n        lrt_statistic = 2 * (ll_full - ll_red)\n        \n        # Degrees of freedom: difference in number of parameters\n        dof = X_full.shape[1] - X_red.shape[1]\n        \n        # Compute p-value from chi-square survival function\n        p_value = chi2.sf(lrt_statistic, df=dof)\n        \n        # 5. Decide significance\n        is_significant = p_value < alpha\n        results.append(is_significant)\n        \n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}