{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is about building the decoder from its fundamental principles. Starting from Bayes' rule and the Gaussian likelihood model, you will derive the core component of the Linear Discriminant Analysis (LDA) classifier: the weight vector $\\hat{\\mathbf{w}}$. This foundational practice bridges the theory of optimal decision-making and a concrete numerical computation, solidifying your understanding of how a decoder linearly combines neural population responses.",
            "id": "4174380",
            "problem": "In a two-class neural decoding task, a population response vector $\\mathbf{x} \\in \\mathbb{R}^{2}$ is modeled under the classical generative assumptions for Linear Discriminant Analysis (LDA): each class $k \\in \\{0,1\\}$ produces $\\mathbf{x}$ according to a multivariate normal distribution with class-dependent mean $\\boldsymbol{\\mu}_{k}$ and a class-independent covariance $\\boldsymbol{\\Sigma}$. The decision rule is constructed from the log-likelihood ratio and prior probabilities via Bayes decision theory with equal misclassification costs. Starting from the multivariate normal log-density and Bayes decision rule, derive the empirical LDA decoding weight vector $\\hat{\\mathbf{w}}$ that multiplies $\\mathbf{x}$ in the linear discriminant function used to separate the classes. Then, using the following empirically estimated quantities from a neuroscience experiment with two conditions (classes),\n- class sample sizes $n_{0} = 50$ and $n_{1} = 60$,\n- sample means $\\hat{\\boldsymbol{\\mu}}_{0} = (0, 1)$ and $\\hat{\\boldsymbol{\\mu}}_{1} = (1, 2)$,\n- pooled covariance\n$$\n\\hat{\\boldsymbol{\\Sigma}} = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix},\n$$\ncompute the numerical value of the empirical LDA decoding weight vector $\\hat{\\mathbf{w}}$ for this $2$-dimensional feature space. Express the final numerical components of $\\hat{\\mathbf{w}}$ exactly as simplified fractions. No rounding is required.",
            "solution": "The problem asks for the derivation of the empirical Linear Discriminant Analysis (LDA) decoding weight vector, $\\hat{\\mathbf{w}}$, and its numerical computation for a given set of empirical data. The derivation and computation proceed as follows.\n\n**1. Derivation of the LDA Weight Vector**\n\nThe classification is based on the posterior probabilities of the two classes, $k \\in \\{0,1\\}$, for a given observation vector $\\mathbf{x} \\in \\mathbb{R}^{d}$. According to Bayes decision theory with equal misclassification costs, we assign $\\mathbf{x}$ to class $k=1$ if its posterior probability $P(k=1|\\mathbf{x})$ is greater than that of class $k=0$, i.e., $P(k=1|\\mathbf{x})  P(k=0|\\mathbf{x})$.\n\nUsing Bayes' theorem, which states $P(k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|k)P(k)}{p(\\mathbf{x})}$, where $p(\\mathbf{x}|k)$ is the class-conditional density and $P(k)$ is the class prior probability, the decision rule can be rewritten as:\n$$\n\\frac{p(\\mathbf{x}|k=1)P(k=1)}{p(\\mathbf{x})}  \\frac{p(\\mathbf{x}|k=0)P(k=0)}{p(\\mathbf{x})}\n$$\nThis simplifies to $p(\\mathbf{x}|k=1)P(k=1)  p(\\mathbf{x}|k=0)P(k=0)$. To obtain a linear decision function, we take the natural logarithm of both sides:\n$$\n\\ln(p(\\mathbf{x}|k=1)) + \\ln(P(k=1))  \\ln(p(\\mathbf{x}|k=0)) + \\ln(P(k=0))\n$$\nThis inequality defines a discriminant function, $\\delta(\\mathbf{x})$, which we compare to a threshold of $0$:\n$$\n\\delta(\\mathbf{x}) = \\ln\\left(\\frac{p(\\mathbf{x}|k=1)}{p(\\mathbf{x}|k=0)}\\right) + \\ln\\left(\\frac{P(k=1)}{P(k=0)}\\right)  0\n$$\nThe problem states that the class-conditional densities are multivariate normal distributions, $p(\\mathbf{x}|k) = \\mathcal{N}(\\mathbf{x}; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma})$, with a common covariance matrix $\\boldsymbol{\\Sigma}$. The log-density is:\n$$\n\\ln(p(\\mathbf{x}|k)) = -\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) - \\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(\\boldsymbol{\\Sigma}))\n$$\nThe first term of the discriminant function is the log-likelihood ratio, $\\ln(p(\\mathbf{x}|k=1)) - \\ln(p(\\mathbf{x}|k=0))$. The terms $-\\frac{d}{2}\\ln(2\\pi)$ and $-\\frac{1}{2}\\ln(\\det(\\boldsymbol{\\Sigma}))$ are common to both classes and cancel out in the difference:\n$$\n\\ln\\left(\\frac{p(\\mathbf{x}|k=1)}{p(\\mathbf{x}|k=0)}\\right) = -\\frac{1}{2}\\left[ (\\mathbf{x} - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1) - (\\mathbf{x} - \\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_0) \\right]\n$$\nExpanding the quadratic forms $(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) = \\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - 2\\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} + \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_k$, we find that the term quadratic in $\\mathbf{x}$, $\\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x}$, cancels out.\n$$\n\\ln\\left(\\frac{p(\\mathbf{x}|k=1)}{p(\\mathbf{x}|k=0)}\\right) = -\\frac{1}{2}\\left[ (-2\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} + \\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1) - (-2\\boldsymbol{\\mu}_0^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} + \\boldsymbol{\\mu}_0^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0) \\right]\n$$\n$$\n= \\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - \\boldsymbol{\\mu}_0^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}(\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0)\n$$\n$$\n= (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}(\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0^T \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0)\n$$\nThe discriminant function $\\delta(\\mathbf{x})$ is written in the form $\\mathbf{w}^T \\mathbf{x} + w_0$. The weight vector $\\mathbf{w}$ is the part of the expression that multiplies $\\mathbf{x}$. By inspection, $\\mathbf{w}^T = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}^{-1}$. Taking the transpose of both sides and noting that $\\boldsymbol{\\Sigma}^{-1}$ is symmetric, we find the weight vector:\n$$\n\\mathbf{w} = \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0)\n$$\nFor the empirical case, we replace the true parameters with their sample estimates from data, $(\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}})$, to get the empirical weight vector $\\hat{\\mathbf{w}}$:\n$$\n\\hat{\\mathbf{w}} = \\hat{\\boldsymbol{\\Sigma}}^{-1}(\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0)\n$$\n\n**2. Numerical Computation**\n\nThe provided empirical quantities are:\n- Sample means: $\\hat{\\boldsymbol{\\mu}}_{0} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ and $\\hat{\\boldsymbol{\\mu}}_{1} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n- Pooled covariance matrix: $\\hat{\\boldsymbol{\\Sigma}} = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix}$.\n\nFirst, we compute the difference between the sample means:\n$$\n\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nSecond, we find the inverse of the pooled covariance matrix, $\\hat{\\boldsymbol{\\Sigma}}^{-1}$. For a generic $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the inverse is given by $\\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$. The determinant of $\\hat{\\boldsymbol{\\Sigma}}$ is $\\det(\\hat{\\boldsymbol{\\Sigma}}) = (3)(2) - (1)(1) = 5$.\nThus, the inverse is:\n$$\n\\hat{\\boldsymbol{\\Sigma}}^{-1} = \\frac{1}{5} \\begin{pmatrix} 2  -1 \\\\ -1  3 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5}  -\\frac{1}{5} \\\\ -\\frac{1}{5}  \\frac{3}{5} \\end{pmatrix}\n$$\nFinally, we substitute these into the formula for $\\hat{\\mathbf{w}}$:\n$$\n\\hat{\\mathbf{w}} = \\hat{\\boldsymbol{\\Sigma}}^{-1}(\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0) = \\begin{pmatrix} \\frac{2}{5}  -\\frac{1}{5} \\\\ -\\frac{1}{5}  \\frac{3}{5} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nPerforming the matrix-vector multiplication:\n$$\n\\hat{\\mathbf{w}} = \\begin{pmatrix} \\frac{2}{5}(1) - \\frac{1}{5}(1) \\\\ -\\frac{1}{5}(1) + \\frac{3}{5}(1) \\end{pmatrix} = \\begin{pmatrix} \\frac{2-1}{5} \\\\ \\frac{-1+3}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ \\frac{2}{5} \\end{pmatrix}\n$$\nThe components of the empirical LDA decoding weight vector are $\\frac{1}{5}$ and $\\frac{2}{5}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{1}{5} \\\\ \\frac{2}{5} \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Having established the basic computation of the LDA weight vector, we now explore the crucial and often non-intuitive role of neural noise covariance. This practice challenges you to compare the optimal decoding axis for neurons with correlated versus uncorrelated noise. Through this comparison, you will discover how LDA intelligently finds a direction that maximizes the signal-to-noise ratio, which is not necessarily the simple difference between the mean neural responses.",
            "id": "4174421",
            "problem": "A sensory neuroscience lab is decoding which of two stimuli was presented from the joint activity of $2$ simultaneously recorded neurons. Across trials, the feature vector $\\mathbf{x} \\in \\mathbb{R}^{2}$ of trial-averaged firing rates is modeled by class-conditional multivariate Gaussian densities with shared covariance and equal priors: $\\mathbf{x} \\mid y = 0 \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{0}, \\boldsymbol{\\Sigma})$ and $\\mathbf{x} \\mid y = 1 \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma})$, with $P(y=0) = P(y=1)$. Define $\\Delta \\boldsymbol{\\mu} = \\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{0}$. Linear Discriminant Analysis (LDA) seeks a linear decision function derived from the log-likelihood ratio under these assumptions.\n\nStarting from the definition of the log-likelihood ratio for multivariate Gaussian class-conditional densities with equal covariance and equal priors, derive the linear decision function and identify the weight vector $\\mathbf{w}$ of the linear decoder in terms of $\\boldsymbol{\\Sigma}$ and $\\Delta \\boldsymbol{\\mu}$, without invoking any shortcut formulas.\n\nThen, for the empirical values $\\Delta \\boldsymbol{\\mu} = (1, 1)$ and $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 2  1.5 \\\\ 1.5  2 \\end{pmatrix}$, compute the exact entries of $\\mathbf{w}$. Next, compute the weight vector when the covariance is diagonal with the same variances, i.e., $\\boldsymbol{\\Sigma}_{\\mathrm{diag}} = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}$, using the same $\\Delta \\boldsymbol{\\mu}$.\n\nExpress your final answer as a single row matrix containing, in order, the two entries of $\\mathbf{w}$ for the full covariance case followed by the two entries of $\\mathbf{w}$ for the diagonal covariance case. Report all entries as exact rational numbers in simplest form.",
            "solution": "The decision rule for Linear Discriminant Analysis (LDA) is based on the log-ratio of the posterior probabilities. We assign a feature vector $\\mathbf{x}$ to class $y=1$ if $P(y=1|\\mathbf{x})  P(y=0|\\mathbf{x})$. Using Bayes' rule and the given equal priors ($P(y=1)=P(y=0)$), this simplifies to the log-likelihood ratio being positive:\n$$\nd(\\mathbf{x}) = \\ln(p(\\mathbf{x}|y=1)) - \\ln(p(\\mathbf{x}|y=0))  0\n$$\nThe log-density for a multivariate Gaussian class-conditional distribution is:\n$$\n\\ln(p(\\mathbf{x}|y=k)) = C - \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_k)\n$$\nwhere $C$ includes all terms not dependent on $k$. Substituting this into the decision function, the constant terms and the quadratic term in $\\mathbf{x}$ ($\\mathbf{x}^T\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}$) cancel out, leaving a linear function of $\\mathbf{x}$:\n$$\nd(\\mathbf{x}) = \\mathbf{x}^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0) - \\frac{1}{2}(\\boldsymbol{\\mu}_1^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_0)\n$$\nThis is of the form $\\mathbf{w}^T\\mathbf{x} + b$. The weight vector $\\mathbf{w}$ is thus identified as:\n$$\n\\mathbf{w} = \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0) = \\boldsymbol{\\Sigma}^{-1}\\Delta \\boldsymbol{\\mu}\n$$\nNow we compute $\\mathbf{w}$ for the two specified cases using $\\Delta \\boldsymbol{\\mu} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n**Case 1: Full covariance matrix**\nGiven $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 2  1.5 \\\\ 1.5  2 \\end{pmatrix} = \\begin{pmatrix} 2  \\frac{3}{2} \\\\ \\frac{3}{2}  2 \\end{pmatrix}$.\nThe determinant is $\\det(\\boldsymbol{\\Sigma}) = (2)(2) - (\\frac{3}{2})(\\frac{3}{2}) = 4 - \\frac{9}{4} = \\frac{7}{4}$.\nThe inverse is:\n$$\n\\boldsymbol{\\Sigma}^{-1} = \\frac{1}{7/4} \\begin{pmatrix} 2  -\\frac{3}{2} \\\\ -\\frac{3}{2}  2 \\end{pmatrix} = \\frac{4}{7} \\begin{pmatrix} 2  -\\frac{3}{2} \\\\ -\\frac{3}{2}  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{7}  -\\frac{6}{7} \\\\ -\\frac{6}{7}  \\frac{8}{7} \\end{pmatrix}\n$$\nThe weight vector is:\n$$\n\\mathbf{w} = \\boldsymbol{\\Sigma}^{-1}\\Delta \\boldsymbol{\\mu} = \\begin{pmatrix} \\frac{8}{7}  -\\frac{6}{7} \\\\ -\\frac{6}{7}  \\frac{8}{7} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{7} - \\frac{6}{7} \\\\ -\\frac{6}{7} + \\frac{8}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{7} \\\\ \\frac{2}{7} \\end{pmatrix}\n$$\n\n**Case 2: Diagonal covariance matrix**\nGiven $\\boldsymbol{\\Sigma}_{\\mathrm{diag}} = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}$.\nThe inverse is $\\boldsymbol{\\Sigma}_{\\mathrm{diag}}^{-1} = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix}$.\nThe weight vector is:\n$$\n\\mathbf{w} = \\boldsymbol{\\Sigma}_{\\mathrm{diag}}^{-1}\\Delta \\boldsymbol{\\mu} = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}\n$$\n\nThe four requested entries for the final row matrix are the components of these two vectors: $(\\frac{2}{7}, \\frac{2}{7})$ for the full covariance case and $(\\frac{1}{2}, \\frac{1}{2})$ for the diagonal case.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{7}  \\frac{2}{7}  \\frac{1}{2}  \\frac{1}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Our final practice moves from idealized models to a scenario more typical of real-world neuroscience. Neural spike counts are discrete and often better described by Poisson statistics, which violates the Gaussian assumption of LDA. In this simulation exercise, you will investigate the practical consequences of this model mismatch and test a common variance-stabilizing fix—the square-root transformation—to quantify its impact on decoding performance.",
            "id": "4174506",
            "problem": "You are tasked with evaluating the empirical decoding error introduced by modeling spike-count features as approximately Gaussian when using Linear Discriminant Analysis (LDA) for binary classification of neural activity. Consider a synthetic data-generating process where, on each trial, spike counts from $p$ neurons are conditionally independent given a class label $y \\in \\{0,1\\}$, and are distributed as a product of independent Poisson distributions with class-specific rate vectors. Specifically, for class $k \\in \\{0,1\\}$ and neuron $j \\in \\{1,\\dots,p\\}$, the spike count $X_j \\mid y=k$ follows $X_j \\sim \\mathrm{Poisson}(\\lambda^{(k)}_j)$, with the $X_j$ conditionally independent across $j$. Each trial yields a feature vector $x \\in \\mathbb{N}^p$.\n\nYour program must compare two decoders based on Linear Discriminant Analysis (LDA), which assumes Gaussian class-conditional distributions with a shared covariance:\n- Decoder A uses raw counts, that is features $x$.\n- Decoder B uses square-root transformed counts, with features $z = \\sqrt{x + c}$, where $c$ is a fixed constant defined as $c = 3/8$.\n\nBoth decoders should be trained using the standard LDA decision rule under equal class priors, estimating class means and a pooled covariance matrix from training data. To ensure numerical stability for finite-sample covariance estimation, apply ridge regularization by adding $\\epsilon I_p$ to the pooled covariance, where $\\epsilon = 10^{-6}$ and $I_p$ is the $p \\times p$ identity matrix. The regularization parameter $\\epsilon$ is fixed and not to be tuned.\n\nYour goal is to simulate data under the Poisson model for the following test suite, train both decoders, and evaluate their misclassification rates on an independent test set for each case. For each case, you must compute a single scalar defined as the difference between the error rate of Decoder A (raw counts) and the error rate of Decoder B (square-root transformed counts), namely\n$$\n\\Delta = \\mathrm{Err}_{\\text{raw}} - \\mathrm{Err}_{\\sqrt{\\cdot}},\n$$\nwhere each error is the empirical fraction of misclassified trials on the test set, expressed as a decimal in $[0,1]$ with no percentage sign. Positive $\\Delta$ indicates an advantage for the square-root transformed features.\n\nImplement the following test suite. In each case, draw independent training and test sets using the specified random seed, with $n_{\\mathrm{tr}}$ training trials per class and $n_{\\mathrm{te}}$ test trials per class. All spike counts are dimensionless counts.\n\n- Case $1$:\n  - $p = 5$,\n  - $\\lambda^{(0)} = [0.2, 0.5, 0.3, 0.1, 0.4]$,\n  - $\\lambda^{(1)} = [0.8, 0.5, 0.7, 0.1, 0.6]$,\n  - $n_{\\mathrm{tr}} = 400$,\n  - $n_{\\mathrm{te}} = 800$,\n  - $\\text{seed} = 1234$.\n\n- Case $2$:\n  - $p = 8$,\n  - $\\lambda^{(0)} = [3, 5, 2, 6, 1, 4, 3, 2]$,\n  - $\\lambda^{(1)} = [4, 4, 3, 7, 1, 6, 2, 3]$,\n  - $n_{\\mathrm{tr}} = 300$,\n  - $n_{\\mathrm{te}} = 600$,\n  - $\\text{seed} = 2021$.\n\n- Case $3$:\n  - $p = 6$,\n  - $\\lambda^{(0)} = [20, 35, 18, 42, 10, 25]$,\n  - $\\lambda^{(1)} = [24, 32, 22, 40, 12, 28]$,\n  - $n_{\\mathrm{tr}} = 200$,\n  - $n_{\\mathrm{te}} = 400$,\n  - $\\text{seed} = 7$.\n\n- Case $4$:\n  - $p = 12$,\n  - $\\lambda^{(0)} = [2, 4, 1, 3, 5, 2, 1, 4, 3, 2, 5, 1]$,\n  - $\\lambda^{(1)} = [3, 3, 2, 4, 5, 3, 2, 5, 2, 3, 6, 1]$,\n  - $n_{\\mathrm{tr}} = 5$,\n  - $n_{\\mathrm{te}} = 500$,\n  - $\\text{seed} = 99$.\n\nFor each case, perform the following steps:\n- Generate training and test data by sampling independently from the specified Poisson distributions.\n- Train two LDA classifiers: one on raw counts $x$, one on transformed counts $z = \\sqrt{x + 3/8}$.\n- Evaluate the empirical misclassification error for each classifier on its corresponding representation of the test set.\n- Compute $\\Delta$ as defined above.\n\nFinal Output Format:\nYour program should produce a single line of output containing the four values of $\\Delta$ corresponding to Cases $1$ through $4$, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets, for example\n$[0.012345,0.000000,-0.001234,0.056789]$.",
            "solution": "The problem requires a comparison of two Linear Discriminant Analysis (LDA) decoders for classifying neural spike count data. The data for each class are generated from a set of independent Poisson distributions. The core of the problem is to assess whether a variance-stabilizing transformation—specifically, a square-root transform—improves the performance of LDA, which formally assumes that the class-conditional data distributions are Gaussian with a shared covariance matrix.\n\nThe first step is to formalize the data generation process. For each of two classes, $k \\in \\{0, 1\\}$, we generate spike counts for $p$ neurons. The count for neuron $j$, $X_j$, given class $k$, is a random variable following a Poisson distribution with a class-specific rate parameter $\\lambda_j^{(k)}$, denoted as $X_j \\mid y=k \\sim \\mathrm{Poisson}(\\lambda_j^{(k)})$. A single trial produces a feature vector $x = [X_1, \\dots, X_p]^T$. The problem specifies that for each of the four test cases, we must generate a training set with $n_{\\mathrm{tr}}$ trials per class and an independent test set with $n_{\\mathrm{te}}$ trials per class, using a specific random seed for reproducibility.\n\nThe two decoders to be compared operate on different feature representations:\n1.  **Decoder A (raw counts)**: Uses the feature vector $x$ directly.\n2.  **Decoder B (transformed counts)**: Uses the feature vector $z = \\sqrt{x + c}$, where the constant $c = 3/8$. This is the Anscombe transform, designed to stabilize the variance of Poisson-distributed data. For a Poisson variable $X \\sim \\mathrm{Poisson}(\\lambda)$, its mean is $\\mathrm{E}[X] = \\lambda$ and its variance is $\\mathrm{Var}(X) = \\lambda$. The variance depends on the mean. The transformation $Z = \\sqrt{X+c}$ aims to make the variance of $Z$ approximately constant, independent of $\\lambda$, for large $\\lambda$. This transformation can make the data's distribution more amenable to the Gaussian assumption underlying LDA.\n\nBoth decoders employ the LDA classification rule. LDA assumes that the probability density of features $v$ for class $k$ is a multivariate Gaussian, $p(v|y=k) = \\mathcal{N}(v | \\mu_k, \\Sigma)$, with a shared covariance matrix $\\Sigma$ across classes. With equal prior probabilities for the two classes, $P(y=0) = P(y=1) = 0.5$, a new data point $v$ is classified to class $1$ if the log-likelihood ratio is positive:\n$$\n\\log \\frac{p(v|y=1)}{p(v|y=0)}  0\n$$\nSubstituting the Gaussian densities leads to a linear decision rule. The point $v$ is assigned to class $1$ if $w^T v  b$, where:\n-   The weight vector is $w = \\Sigma^{-1}(\\mu_1 - \\mu_0)$.\n-   The decision threshold (or bias) is $b = \\frac{1}{2}(\\mu_1 + \\mu_0)^T w = \\frac{1}{2}(\\mu_1^T \\Sigma^{-1} \\mu_1 - \\mu_0^T \\Sigma^{-1} \\mu_0)$.\n\nIn practice, the true parameters $\\mu_0, \\mu_1, \\Sigma$ are unknown and must be estimated from the training data. For each decoder (operating on features $v=x$ or $v=z$), the training procedure is as follows:\n1.  **Estimate Class Means**: For each class $k \\in \\{0, 1\\}$, the mean vector $\\mu_k$ is estimated by the sample mean of the training data for that class:\n    $$\n    \\hat{\\mu}_k = \\frac{1}{n_{\\mathrm{tr}}} \\sum_{i=1}^{n_{\\mathrm{tr}}} v_i^{(k)}\n    $$\n2.  **Estimate Pooled Covariance**: First, the individual sample covariance matrices are calculated:\n    $$\n    \\hat{\\Sigma}_k = \\frac{1}{n_{\\mathrm{tr}}-1} \\sum_{i=1}^{n_{\\mathrm{tr}}} (v_i^{(k)} - \\hat{\\mu}_k) (v_i^{(k)} - \\hat{\\mu}_k)^T\n    $$\n    Since the number of training samples per class is equal ($n_0 = n_1 = n_{\\mathrm{tr}}$), the pooled covariance estimate is the average of the individual estimates:\n    $$\n    \\hat{\\Sigma}_{\\text{pooled}} = \\frac{(n_{\\mathrm{tr}}-1)\\hat{\\Sigma}_0 + (n_{\\mathrm{tr}}-1)\\hat{\\Sigma}_1}{(n_{\\mathrm{tr}}-1) + (n_{\\mathrm{tr}}-1)} = \\frac{1}{2}(\\hat{\\Sigma}_0 + \\hat{\\Sigma}_1)\n    $$\n3.  **Apply Regularization**: To ensure the covariance matrix is well-conditioned and invertible, especially when the number of features $p$ is large relative to the number of samples $n_{\\mathrm{tr}}$ (as in Case 4), ridge regularization is applied:\n    $$\n    \\hat{\\Sigma}_{\\text{reg}} = \\hat{\\Sigma}_{\\text{pooled}} + \\epsilon I_p\n    $$\n    where $\\epsilon = 10^{-6}$ is a small constant and $I_p$ is the $p \\times p$ identity matrix.\n4.  **Compute Classifier Parameters**: The estimated parameters are plugged into the formulas for $w$ and $b$:\n    $$\n    \\hat{w} = \\hat{\\Sigma}_{\\text{reg}}^{-1}(\\hat{\\mu}_1 - \\hat{\\mu}_0)\n    $$\n    $$\n    \\hat{b} = \\frac{1}{2}(\\hat{\\mu}_1 + \\hat{\\mu}_0)^T \\hat{w}\n    $$\nAfter training, each decoder is evaluated on the independent test set. For a test sample $v_{\\text{test}}$, the predicted class $\\hat{y}$ is 1 if $\\hat{w}^T v_{\\text{test}}  \\hat{b}$, and 0 otherwise. The empirical error rate, $\\mathrm{Err}$, is the fraction of misclassified samples in the test set.\n$$\n\\mathrm{Err} = \\frac{\\text{Number of misclassified test samples}}{\\text{Total number of test samples}} = \\frac{1}{2n_{\\mathrm{te}}} \\sum_{i=1}^{2n_{\\mathrm{te}}} \\mathbb{I}(\\hat{y}_i \\neq y_i)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function.\n\nThis process is performed for both Decoder A (using raw counts $x$) and Decoder B (using transformed counts $z$), yielding error rates $\\mathrm{Err}_{\\text{raw}}$ and $\\mathrm{Err}_{\\sqrt{\\cdot}}$, respectively. The final quantity of interest for each test case is the difference:\n$$\n\\Delta = \\mathrm{Err}_{\\text{raw}} - \\mathrm{Err}_{\\sqrt{\\cdot}}\n$$\nA positive $\\Delta$ indicates that the square-root transformation improved decoding performance by reducing the error rate. This entire simulation is repeated for each of the four specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n\n    test_cases = [\n        # Case 1\n        {\n            \"p\": 5,\n            \"lambda0\": np.array([0.2, 0.5, 0.3, 0.1, 0.4]),\n            \"lambda1\": np.array([0.8, 0.5, 0.7, 0.1, 0.6]),\n            \"n_tr\": 400,\n            \"n_te\": 800,\n            \"seed\": 1234,\n        },\n        # Case 2\n        {\n            \"p\": 8,\n            \"lambda0\": np.array([3, 5, 2, 6, 1, 4, 3, 2]),\n            \"lambda1\": np.array([4, 4, 3, 7, 1, 6, 2, 3]),\n            \"n_tr\": 300,\n            \"n_te\": 600,\n            \"seed\": 2021,\n        },\n        # Case 3\n        {\n            \"p\": 6,\n            \"lambda0\": np.array([20, 35, 18, 42, 10, 25]),\n            \"lambda1\": np.array([24, 32, 22, 40, 12, 28]),\n            \"n_tr\": 200,\n            \"n_te\": 400,\n            \"seed\": 7,\n        },\n        # Case 4\n        {\n            \"p\": 12,\n            \"lambda0\": np.array([2, 4, 1, 3, 5, 2, 1, 4, 3, 2, 5, 1]),\n            \"lambda1\": np.array([3, 3, 2, 4, 5, 3, 2, 5, 2, 3, 6, 1]),\n            \"n_tr\": 5,\n            \"n_te\": 500,\n            \"seed\": 99,\n        },\n    ]\n\n    c = 3.0 / 8.0\n    epsilon = 1e-6\n    delta_results = []\n\n    for case in test_cases:\n        p = case[\"p\"]\n        lambda0 = case[\"lambda0\"]\n        lambda1 = case[\"lambda1\"]\n        n_tr = case[\"n_tr\"]\n        n_te = case[\"n_te\"]\n        seed = case[\"seed\"]\n\n        # Set seed for reproducibility for each case\n        np.random.seed(seed)\n\n        # Generate training data\n        X_tr_0 = np.random.poisson(lambda0, size=(n_tr, p))\n        X_tr_1 = np.random.poisson(lambda1, size=(n_tr, p))\n\n        # Generate test data\n        X_te_0 = np.random.poisson(lambda0, size=(n_te, p))\n        X_te_1 = np.random.poisson(lambda1, size=(n_te, p))\n\n        # --- Decoder A: Raw Counts ---\n        err_raw = train_and_evaluate_lda(X_tr_0, X_tr_1, X_te_0, X_te_1, p, epsilon)\n\n        # --- Decoder B: Square-root Transformed Counts ---\n        Z_tr_0 = np.sqrt(X_tr_0 + c)\n        Z_tr_1 = np.sqrt(X_tr_1 + c)\n        Z_te_0 = np.sqrt(X_te_0 + c)\n        Z_te_1 = np.sqrt(X_te_1 + c)\n        err_sqrt = train_and_evaluate_lda(Z_tr_0, Z_tr_1, Z_te_0, Z_te_1, p, epsilon)\n\n        # --- Calculate Delta ---\n        delta = err_raw - err_sqrt\n        delta_results.append(delta)\n\n    # Format and print the final output\n    formatted_results = [f\"{res:.6f}\" for res in delta_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef train_and_evaluate_lda(V_tr_0, V_tr_1, V_te_0, V_te_1, p, epsilon):\n    \"\"\"\n    Trains an LDA classifier and evaluates its error rate on test data.\n\n    Args:\n        V_tr_0 (np.ndarray): Training data for class 0 (n_tr, p).\n        V_tr_1 (np.ndarray): Training data for class 1 (n_tr, p).\n        V_te_0 (np.ndarray): Test data for class 0 (n_te, p).\n        V_te_1 (np.ndarray): Test data for class 1 (n_te, p).\n        p (int): Number of features.\n        epsilon (float): Regularization parameter.\n\n    Returns:\n        float: The empirical misclassification error rate.\n    \"\"\"\n    n_tr = V_tr_0.shape[0]\n    n_te = V_te_0.shape[0]\n\n    # 1. Estimate class means from training data\n    mu0_hat = np.mean(V_tr_0, axis=0)\n    mu1_hat = np.mean(V_tr_1, axis=0)\n\n    # 2. Estimate class covariance matrices and the pooled covariance\n    # np.cov with ddof=1 uses a denominator of n-1, which is correct\n    if n_tr  1:\n        Sigma0_hat = np.cov(V_tr_0, rowvar=False, ddof=1)\n        Sigma1_hat = np.cov(V_tr_1, rowvar=False, ddof=1)\n        Sigma_pooled = 0.5 * (Sigma0_hat + Sigma1_hat)\n    else: # Handle n_tr=1 case where covariance is undefined\n        Sigma_pooled = np.zeros((p, p))\n\n    # 3. Apply ridge regularization\n    Sigma_reg = Sigma_pooled + epsilon * np.eye(p)\n\n    # 4. Compute classifier parameters\n    Sigma_reg_inv = np.linalg.inv(Sigma_reg)\n    w_hat = Sigma_reg_inv @ (mu1_hat - mu0_hat)\n    b_hat = 0.5 * (mu1_hat + mu0_hat) @ w_hat\n    \n    # 5. Evaluate on test data\n    # Predictions for class 0 data\n    pred_0 = V_te_0 @ w_hat  b_hat\n    misclassified_0 = np.sum(pred_0) # Count how many are incorrectly predicted as 1\n\n    # Predictions for class 1 data\n    pred_1 = V_te_1 @ w_hat = b_hat\n    misclassified_1 = np.sum(pred_1) # Count how many are incorrectly predicted as 0\n\n    # 6. Compute total error rate\n    total_misclassified = misclassified_0 + misclassified_1\n    total_test_samples = 2 * n_te\n    error_rate = total_misclassified / total_test_samples\n\n    return error_rate\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}