{
    "hands_on_practices": [
        {
            "introduction": "在构建一个完整的解码器之前，理解其每个组件如何为最终决策做出贡献是至关重要的。本练习旨在剖析朴素贝叶斯模型，以精确分离出单个神经元对分类结果的影响。通过推导当一个神经元被移除时对数后验几率的变化，您将对单个神经元的证据如何以量化的方式塑造解码器的输出有一个清晰的认识 。",
            "id": "4180807",
            "problem": "考虑一个在持续时间为 $T$ 的固定观测窗口内，使用朴素贝叶斯分类器对刺激 $A$ 和 $B$ 进行二分类解码的任务。您记录了来自 $N$ 个神经元的脉冲计数，在窗口内收集到计数向量 $\\mathbf{x} = (x_1, x_2, \\dots, x_N)$。假设在给定刺激的情况下，神经元之间条件独立，并将每个神经元的脉冲计数建模为泊松随机变量，其在窗口内的均值率取决于特定的刺激：对于神经元 $i$，在刺激 $s \\in \\{A,B\\}$ 下，$x_i \\sim \\mathrm{Poisson}(\\lambda_{i s})$，其中 $\\lambda_{i s} > 0$ 表示在刺激 $s$ 下窗口内的期望脉冲计数。设先验概率为 $\\pi_A = P(S=A)$ 和 $\\pi_B = P(S=B)$，且 $\\pi_A + \\pi_B = 1$。定义对数后验几率为\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\!\\left(\\frac{P(S=A \\mid \\mathbf{x})}{P(S=B \\mid \\mathbf{x})}\\right).\n$$\n现在，您从解码器中移除单个神经元 $j$，形成简化的计数向量 $\\mathbf{x}_{-j} = (x_1, \\dots, x_{j-1}, x_{j+1}, \\dots, x_N)$，并在相同的假设和先验条件下重新计算对数后验几率 $\\mathrm{LPO}(\\mathbf{x}_{-j})$。严格地从贝叶斯定理和条件独立模型出发，推导由下式定义的对数后验几率变化的精确闭式表达式：\n$$\n\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x}),\n$$\n并将其纯粹表示为单个神经元观测值 $x_j$ 和两个特定于刺激的泊松均值 $\\lambda_{jA}$ 和 $\\lambda_{jB}$ 的函数。您的最终答案必须是单一的解析表达式。请勿近似或取整。",
            "solution": "该问题基于计算神经科学中的标准模型，问题设定合理且科学上无误。我们可以开始推导。\n\n目标是推导对数后验几率变化的表达式 $\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x})$。我们首先为完整数据集定义对数后验几率 $\\mathrm{LPO}(\\mathbf{x})$。\n\n根据定义，对数后验几率为：\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{P(S=A \\mid \\mathbf{x})}{P(S=B \\mid \\mathbf{x})}\\right)\n$$\n我们将贝叶斯定理应用于后验概率 $P(S=s \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid S=s)P(S=s)}{P(\\mathbf{x})}$，其中 $s \\in \\{A,B\\}$。将这些代入比率中，证据项 $P(\\mathbf{x})$ 会被消掉：\n$$\n\\frac{P(S=A \\mid \\mathbf{x})}{P(S=B \\mid \\mathbf{x})} = \\frac{P(\\mathbf{x} \\mid S=A)P(S=A)}{P(\\mathbf{x} \\mid S=B)P(S=B)}\n$$\n对两边取自然对数，得到：\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{P(S=A)}{P(S=B)}\\right) + \\ln\\left(\\frac{P(\\mathbf{x} \\mid S=A)}{P(\\mathbf{x} \\mid S=B)}\\right)\n$$\n第一项是对数先验几率，我们可以使用给定的先验概率 $\\pi_A$ 和 $\\pi_B$ 来表示：\n$$\n\\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right)\n$$\n第二项是对数似然比。我们使用条件独立性假设，该假设指出联合似然是每个神经元各自似然的乘积：\n$$\nP(\\mathbf{x} \\mid S=s) = \\prod_{i=1}^{N} P(x_i \\mid S=s)\n$$\n于是，对数似然比变成了各个对数似然比的总和：\n$$\n\\ln\\left(\\frac{P(\\mathbf{x} \\mid S=A)}{P(\\mathbf{x} \\mid S=B)}\\right) = \\ln\\left(\\frac{\\prod_{i=1}^{N} P(x_i \\mid S=A)}{\\prod_{i=1}^{N} P(x_i \\mid S=B)}\\right) = \\sum_{i=1}^{N} \\ln\\left(\\frac{P(x_i \\mid S=A)}{P(x_i \\mid S=B)}\\right)\n$$\n现在，我们将泊松模型用于脉冲计数。泊松随机变量的概率质量函数为 $P(x_i \\mid S=s) = \\frac{\\lambda_{is}^{x_i} \\exp(-\\lambda_{is})}{x_i!}$。将其代入每个神经元的对数似然比中：\n$$\n\\ln\\left(\\frac{P(x_i \\mid S=A)}{P(x_i \\mid S=B)}\\right) = \\ln\\left(\\frac{\\lambda_{iA}^{x_i} \\exp(-\\lambda_{iA}) / x_i!}{\\lambda_{iB}^{x_i} \\exp(-\\lambda_{iB}) / x_i!}\\right) = \\ln\\left(\\frac{\\lambda_{iA}^{x_i} \\exp(-\\lambda_{iA})}{\\lambda_{iB}^{x_i} \\exp(-\\lambda_{iB})}\\right)\n$$\n利用对数的性质 $\\ln(a/b) = \\ln(a) - \\ln(b)$ 和 $\\ln(a^b) = b\\ln(a)$：\n$$\n\\ln\\left(\\frac{P(x_i \\mid S=A)}{P(x_i \\mid S=B)}\\right) = (x_i \\ln(\\lambda_{iA}) - \\lambda_{iA}) - (x_i \\ln(\\lambda_{iB}) - \\lambda_{iB}) = x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB})\n$$\n合并所有项，完整的对数后验几率为：\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right) + \\sum_{i=1}^{N} \\left[ x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB}) \\right]\n$$\n这个表达式表明，总的对数后验几率是一个恒定的先验项与来自 $N$ 个神经元各自贡献的总和。\n\n接下来，我们计算简化数据集的对数后验几率 $\\mathrm{LPO}(\\mathbf{x}_{-j})$。这个计算过程是相同的，但求和是在排除了神经元 $j$ 的神经元集合上进行的：\n$$\n\\mathrm{LPO}(\\mathbf{x}_{-j}) = \\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right) + \\sum_{i \\neq j} \\left[ x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB}) \\right]\n$$\n最后，我们计算所求的变化量 $\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x})$。\n我们可以将 $\\mathrm{LPO}(\\mathbf{x})$ 表达式中的求和项分成两部分：神经元 $j$ 的项和对所有其他神经元的求和。\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\ln\\left(\\frac{\\pi_A}{\\pi_B}\\right) + \\sum_{i \\neq j} \\left[ x_i \\ln\\left(\\frac{\\lambda_{iA}}{\\lambda_{iB}}\\right) - (\\lambda_{iA} - \\lambda_{iB}) \\right] + \\left[ x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right) - (\\lambda_{jA} - \\lambda_{jB}) \\right]\n$$\n注意到该表达式中的前两项与 $\\mathrm{LPO}(\\mathbf{x}_{-j})$ 的表达式完全相同。\n$$\n\\mathrm{LPO}(\\mathbf{x}) = \\mathrm{LPO}(\\mathbf{x}_{-j}) + \\left[ x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right) - (\\lambda_{jA} - \\lambda_{jB}) \\right]\n$$\n重新整理此方程以求解 $\\Delta \\mathrm{LPO}_j$：\n$$\n\\Delta \\mathrm{LPO}_j = \\mathrm{LPO}(\\mathbf{x}_{-j}) - \\mathrm{LPO}(\\mathbf{x}) = - \\left[ x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right) - (\\lambda_{jA} - \\lambda_{jB}) \\right]\n$$\n化简该表达式得到最终结果：\n$$\n\\Delta \\mathrm{LPO}_j = (\\lambda_{jA} - \\lambda_{jB}) - x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right)\n$$\n这表示移除神经元 $j$ 时对数后验几率的变化。按照要求，它仅仅表示为该神经元的观测值 $x_j$ 及其特定于刺激的平均脉冲计数 $\\lambda_{jA}$ 和 $\\lambda_{jB}$ 的函数。",
            "answer": "$$\n\\boxed{(\\lambda_{jA} - \\lambda_{jB}) - x_j \\ln\\left(\\frac{\\lambda_{jA}}{\\lambda_{jB}}\\right)}\n$$"
        },
        {
            "introduction": "从单个神经元扩展到整个神经元群体时，我们必须整合所有来源的证据来计算模型证据 $p(\\mathbf{r})$。在实践中，直接计算这个量常常会因为多个小概率的连乘而导致数值下溢。这个实践性的编程练习将向您介绍 log-sum-exp 技巧，这是一种稳健实现概率模型和计算对数证据的一项基本技术 。",
            "id": "4180768",
            "problem": "您正在使用朴素贝叶斯分类器分析来自神经元群体响应的刺激解码。从以下基础开始：贝叶斯定理和朴素贝叶斯独立性假设。具体来说，对于由 $s$ 索引的离散刺激集和跨越 $I$ 个神经元的神经响应向量 $\\mathbf{r}=(r_1,\\dots,r_I)$，通过贝叶斯定理定义后验概率、证据和朴素贝叶斯分解如下：$p(s\\mid \\mathbf{r})=\\dfrac{p(\\mathbf{r}\\mid s)p(s)}{p(\\mathbf{r})}$，$p(\\mathbf{r})=\\sum_s p(\\mathbf{r}\\mid s)p(s)$，以及 $p(\\mathbf{r}\\mid s)=\\prod_{i=1}^I p(r_i\\mid s)$。计算任务是评估对数证据 $\\log p(\\mathbf{r})$，给定每个类别每个神经元的对数似然和对数先验，同时在神经元数量 $I$ 很大或对数似然为非常大的负数时确保数值稳定性。\n\n您的程序必须通过应用于 $\\sum_s \\exp\\!\\left(\\sum_{i=1}^I \\log p(r_i\\mid s)+\\log p(s)\\right)$ 的 log-sum-exp 变换来实现 $\\log p(\\mathbf{r})$ 的数值稳定计算。具体地，您必须从输入 $L_{s,i}=\\log p(r_i\\mid s)$ 和 $\\ell_s=\\log p(s)$ 计算 $\\log p(\\mathbf{r})$，首先为每个类别 $s$ 形成 $a_s=\\sum_{i=1}^I L_{s,i}+\\ell_s$，然后通过数值稳定的 log-sum-exp 变换聚合这些 $a_s$ 值。您的实现必须设计为能处理 $a_s$ 中的 $-\\infty$ 值（代表一个类别的概率为零），并在所有类别都不可能时返回 $-\\infty$。\n\n为以下测试套件实现计算。在每种情况下，$L$ 是一个列表的列表，包含对于 $s=0,\\dots,S-1$ 和 $i=1,\\dots,I$ 的 $L_{s,i}$，而 $\\ell$ 是对于 $s=0,\\dots,S-1$ 的 $\\ell_s=\\log p(s)$ 的列表。所有对数均为自然对数。对于每种情况，输出是一个表示 $\\log p(\\mathbf{r})$ 的实数。最终输出必须是单行，包含一个方括号内的逗号分隔列表，按所列顺序汇总所有情况的结果。不涉及物理单位；所有输出都是实数。\n\n情况1（正常路径，中等值）：\n- $S=3$， $I=5$。\n- $L$：\n  - 类别0：$[-2.3,-0.7,-1.1,-0.5,-3.0]$，\n  - 类别1：$[-1.9,-1.2,-0.9,-1.5,-2.2]$，\n  - 类别2：$[-3.2,-0.6,-0.8,-0.4,-2.8]$。\n- $\\ell$：$[\\log(0.2),\\log(0.3),\\log(0.5)] = [-1.6094379124341003,-1.2039728043259361,-0.6931471805599453]$。\n\n情况2（极端下溢风险，非常大的负数和）：\n- $S=2$， $I=3$。\n- $L$：\n  - 类别0：$[-1000.0,-800.0,-1200.0]$，\n  - 类别1：$[-1001.0,-799.0,-1199.0]$。\n- $\\ell$：$[\\log(0.5),\\log(0.5)] = [-0.6931471805599453,-0.6931471805599453]$。\n\n情况3（单类别边界）：\n- $S=1$， $I=4$。\n- $L$：\n  - 类别0：$[-0.2,-0.4,-0.1,-0.3]$。\n- $\\ell$：$[\\log(1.0)] = [0.0]$。\n\n情况4（聚合分数相等的平衡类别）：\n- $S=2$， $I=2$。\n- $L$：\n  - 类别0：$[-0.5,-1.0]$，\n  - 类别1：$[-0.5,-1.0]$。\n- $\\ell$：$[\\log(0.5),\\log(0.5)] = [-0.6931471805599453,-0.6931471805599453]$。\n\n情况5（一个类别不可能，一个类别有限）：\n- $S=2$， $I=3$。\n- $L$：\n  - 类别0：$[-\\infty,-0.5,-0.5]$，\n  - 类别1：$[-0.4,-0.6,-0.3]$。\n- $\\ell$：$[\\log(0.6),\\log(0.4)] = [-0.5108256237659907,-0.916290731874155]$。\n\n情况6（所有类别都不可能）：\n- $S=2$， $I=2$。\n- $L$：\n  - 类别0：$[-\\infty,-0.2]$，\n  - 类别1：$[-\\infty,-0.3]$。\n- $\\ell$：$[\\log(0.7),\\log(0.3)] = [-0.35667494393873245,-1.2039728043259361]$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[\\text{result1},\\text{result2},\\dots]$）。结果的顺序必须与上述情况1到6完全一致。每个结果都必须是一个实数（浮点数），当数学上保证时，结果可以为 $-\\infty$。",
            "solution": "该问题是有效的。它提出了一个在计算神经科学和机器学习领域中定义明确、有科学依据的任务，即以数值稳定的方式计算朴素贝叶斯分类器的对数证据。所有需要的数据和定义都已提供，问题没有矛盾或含糊之处。\n\n目标是为给定的神经响应向量 $\\mathbf{r}$ 和一个离散的刺激集 $\\{s\\}$ 计算对数证据 $\\log p(\\mathbf{r})$。证据 $p(\\mathbf{r})$ 是观测到响应 $\\mathbf{r}$ 的边际概率，它通过对所有可能的刺激进行边际化来定义：\n$$p(\\mathbf{r}) = \\sum_s p(\\mathbf{r}, s)$$\n使用联合概率的定义，这可以写成：\n$$p(\\mathbf{r}) = \\sum_s p(\\mathbf{r}\\mid s)p(s)$$\n在这里，$p(s)$ 是刺激 $s$ 的先验概率，而 $p(\\mathbf{r}\\mid s)$ 是在刺激 $s$ 出现的情况下观测到响应 $\\mathbf{r}$ 的似然。\n\n问题指定使用朴素贝叶斯模型。该模型假设，在给定刺激 $s$ 的条件下，单个神经元 $r_i$ 的响应是独立的。对于一个具有响应向量 $\\mathbf{r}=(r_1, \\dots, r_I)$ 的 $I$ 个神经元群体，这个假设被表述为：\n$$p(\\mathbf{r}\\mid s) = \\prod_{i=1}^I p(r_i\\mid s)$$\n将此代入证据的表达式中，得到：\n$$p(\\mathbf{r}) = \\sum_s \\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right)$$\n直接计算这个表达式在数值上是危险的。对于大量的神经元 $I$，乘积项 $\\prod_{i=1}^I p(r_i\\mid s)$ 很容易下溢为零，即使单个概率 $p(r_i\\mid s)$ 并不过小。为了避免这种情况，我们在对数域中进行计算。\n\n所需量是对数证据 $\\log p(\\mathbf{r})$：\n$$\\log p(\\mathbf{r}) = \\log\\left( \\sum_s \\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right) \\right)$$\n为了使用对数和而不是概率积，我们可以使用指数函数来表示求和中的每一项，即 $x = \\exp(\\log x)$：\n$$\\log p(\\mathbf{r}) = \\log\\left( \\sum_s \\exp\\left( \\log\\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right) \\right) \\right)$$\n使用对数的性质 $\\log(ab) = \\log a + \\log b$ 和 $\\log(\\prod_i a_i) = \\sum_i \\log a_i$，指数内的项变为：\n$$\\log\\left( p(s) \\prod_{i=1}^I p(r_i\\mid s) \\right) = \\log p(s) + \\sum_{i=1}^I \\log p(r_i\\mid s)$$\n问题提供了这些对数量作为输入：每个神经元的对数似然 $L_{s,i} = \\log p(r_i\\mid s)$ 和对数先验 $\\ell_s = \\log p(s)$。让我们将 $a_s$ 定义为每个类别 $s$ 的对数联合概率项：\n$$a_s = \\ell_s + \\sum_{i=1}^I L_{s,i}$$\n将其代回，对数证据为：\n$$\\log p(\\mathbf{r}) = \\log\\left( \\sum_s \\exp(a_s) \\right)$$\n这个表达式被称为 log-sum-exp 函数，即 $\\text{LSE}(a_1, a_2, \\dots)$。一个简单的实现，即首先为每个 $s$ 计算 $\\exp(a_s)$ 然后求和，仍然容易出现数值问题。如果任何 $a_s$ 是一个大的正数，$\\exp(a_s)$ 可能会上溢。如果所有的 $a_s$ 都是大的负数，那么对于所有的 $s$，$\\exp(a_s)$ 都可能下溢为零，导致最终结果为 $\\log(0) = -\\infty$ 并丢失所有精度。\n\n为了确保数值稳定性，我们使用“log-sum-exp技巧”。设 $a_{\\max} = \\max_s \\{a_s\\}$。我们可以从和中提出因子 $\\exp(a_{\\max})$：\n$$\\log p(\\mathbf{r}) = \\log\\left( \\exp(a_{\\max}) \\sum_s \\exp(a_s - a_{\\max}) \\right)$$\n$$= a_{\\max} + \\log\\left( \\sum_s \\exp(a_s - a_{\\max}) \\right)$$\n这个公式在数值上是稳健的。指数函数的参数 $(a_s - a_{\\max})$ 现在都小于或等于0。这可以防止上溢，因为当 $x \\le 0$ 时，$\\exp(x)$ 的最大值为1。它还确保了对和贡献最大的项能够以高精度计算。\n\n这种方法还必须正确处理涉及 $-\\infty$ 的特殊情况。输入值为 $L_{s,i} = -\\infty$ 或 $\\ell_s = -\\infty$ 表示一个零概率事件。它们的和 $a_s$ 也将是 $-\\infty$。这样一个类别对证据的贡献是 $\\exp(-\\infty) = 0$，因此这些项正确地从和中消失。如果所有类别都是不可能的，即所有 $a_s = -\\infty$，那么总证据为0，对数证据为 $\\log(0) = -\\infty$。一个稳健的实现，例如 `scipy.special` 模块中可用的 `logsumexp` 函数，可以正确处理这些边界情况。\n\n每个测试用例的算法如下：\n1. 对于从 $0$ 到 $S-1$ 的每个类别 $s$，计算 $a_s = \\ell_s + \\sum_{i=1}^I L_{s,i}$。\n2. 通过将数值稳定的 log-sum-exp 操作应用于包含所有 $a_s$ 值的向量来计算对数证据。\n此过程将应用于所提供的每个测试用例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing the log-evidence for a Naive Bayes classifier\n    for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"L\": [\n                [-2.3, -0.7, -1.1, -0.5, -3.0],\n                [-1.9, -1.2, -0.9, -1.5, -2.2],\n                [-3.2, -0.6, -0.8, -0.4, -2.8]\n            ],\n            \"ell\": [np.log(0.2), np.log(0.3), np.log(0.5)]\n        },\n        {\n            \"L\": [\n                [-1000.0, -800.0, -1200.0],\n                [-1001.0, -799.0, -1199.0]\n            ],\n            \"ell\": [np.log(0.5), np.log(0.5)]\n        },\n        {\n            \"L\": [\n                [-0.2, -0.4, -0.1, -0.3]\n            ],\n            \"ell\": [np.log(1.0)]\n        },\n        {\n            \"L\": [\n                [-0.5, -1.0],\n                [-0.5, -1.0]\n            ],\n            \"ell\": [np.log(0.5), np.log(0.5)]\n        },\n        {\n            \"L\": [\n                [-np.inf, -0.5, -0.5],\n                [-0.4, -0.6, -0.3]\n            ],\n            \"ell\": [np.log(0.6), np.log(0.4)]\n        },\n        {\n            \"L\": [\n                [-np.inf, -0.2],\n                [-np.inf, -0.3]\n            ],\n            \"ell\": [np.log(0.7), np.log(0.3)]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Convert inputs to NumPy arrays for vectorized operations.\n        # L_s,i = log p(r_i|s)\n        # l_s = log p(s)\n        L = np.array(case[\"L\"], dtype=np.float64)\n        ell = np.array(case[\"ell\"], dtype=np.float64)\n\n        # Step 1: Compute a_s for each class s.\n        # a_s = log p(s) + sum_i log p(r_i|s)\n        # This is the log of the joint probability, log p(r, s).\n        # The sum is over neurons (axis=1 of L).\n        a = L.sum(axis=1) + ell\n\n        # Step 2: Compute log-evidence using the log-sum-exp transformation.\n        # log p(r) = log(sum_s exp(a_s))\n        # The scipy.special.logsumexp function provides a numerically stable\n        # implementation that handles -inf correctly.\n        log_evidence = logsumexp(a)\n        \n        results.append(log_evidence)\n\n    # The str() function appropriately formats floating-point numbers, including -inf.\n    # The final output must be a single line containing a comma-separated list\n    # in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "朴素贝叶斯模型依赖于超参数，例如来自伽马 (Gamma) 先验分布的平滑参数 $\\alpha$，这些参数并不能直接从训练数据中学习。本练习探讨如何使用交叉验证来正确地选择这些参数，这是评估模型泛化性能的一项基本技术。通过本练习，您将学会区分正确和不正确的评估目标函数，以避免在模型选择中犯下诸如过拟合和数据泄露等常见错误 。",
            "id": "4180789",
            "problem": "一个实验室正在从 $N$ 个同时测量的神经元记录的群体脉冲计数中解码刺激身份。对于每次试验 $i$，观测到的计数向量为 $x_{i} \\in \\mathbb{N}_{0}^{N}$，刺激标签为 $s_{i} \\in \\{1,\\dots,K\\}$。假设一个朴素贝叶斯模型，其中以刺激 $s$ 为条件，神经元是独立的，并且每个神经元在持续时间为 $T$ 的固定窗口内的脉冲计数被建模为速率参数为 $\\lambda_{n,s}$ 的泊松分布，即对于神经元 $n$，$x_{i,n} \\mid s_{i}=s \\sim \\mathrm{Poisson}(\\lambda_{n,s} T)$，且对所有 $n$ 独立。为了平滑速率估计，为每个 $\\lambda_{n,s}$ 设置一个伽马先验，其形状超参数为 $\\alpha > 0$，固定的速率超参数为 $\\beta > 0$，并且对所有 $n$ 和 $s$ 独立。类别先验 $p(s)$ 是通过每个折叠内训练数据上的经验频率来估计的。\n\n要求您使用 $L$ 折交叉验证来选择平滑超参数 $\\alpha$。设 $\\mathcal{D}=\\{(x_{i},s_{i})\\}_{i=1}^{M}$ 为完整数据集，它被划分为 $L$ 个不相交的测试集 $\\{\\mathcal{D}_{\\mathrm{test}}^{(\\ell)}\\}_{\\ell=1}^{L}$ 和相应的训练集 $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}=\\mathcal{D}\\setminus \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$。对于给定的 $\\alpha$ 和每个折叠 $\\ell$，设 $p(x \\mid s, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$ 表示通过将在由 $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$ 更新的伽马先验上对泊松似然进行积分而得到的后验预测似然，并设 $p(s \\mid x, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$ 表示通过贝叶斯法则得到的相应类别后验。\n\n从第一性原理（针对 $p(s \\mid x)$ 的贝叶斯法则以及交叉验证作为泛化性能的样本外估计的定义）出发，以下哪个候选目标正确地定义了一个用于选择 $\\alpha$ 的交叉验证准则，该准则旨在最大化真实类别的留出对数后验概率或留出分类准确率？选择所有适用项。\n\nA. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$\n\nB. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\mathbf{1}\\!\\left\\{\\;\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) \\;=\\; s_{i}\\;\\right\\}$, 其中 $\\displaystyle \\hat{s}_{\\alpha}^{(\\ell)}(x) \\;=\\; \\arg\\max_{s \\in \\{1,\\dots,K\\}} p\\!\\left(s \\,\\middle|\\, x, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$\n\nC. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{i=1}^{M} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}, \\alpha\\right)$\n\nD. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(x_{i} \\,\\middle|\\, s_{i}, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$\n\nE. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}, \\alpha\\right)$, 使用在 $\\mathcal{D}$ 上拟合的参数为每个折叠计算 $p\\!\\left(s \\mid x, \\mathcal{D}, \\alpha\\right)$\n\n每个候选选项都应严格按照其字面意思进行解释。除了泊松速率上的单个超参数 $\\alpha$ 和经验类别先验外，不应用任何额外的平滑。假设所有定义后验预测量的积分都存在且有限。请提供您的选择，并从贝叶斯解码和交叉验证的第一性原理出发进行论证。",
            "solution": "用户希望我验证问题陈述，如果有效，则根据提供的目标推导出正确的超参数选择。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n- **模型类型：** 用于解码刺激身份的朴素贝叶斯分类器。\n- **数据：** 来自 $N$ 个神经元的群体脉冲计数 $x_{i} \\in \\mathbb{N}_{0}^{N}$，用于试验 $i$，带有刺激标签 $s_{i} \\in \\{1,\\dots,K\\}$。总数据集为 $\\mathcal{D}=\\{(x_{i},s_{i})\\}_{i=1}^{M}$。\n- **似然模型：** 对于给定的刺激 $s$，神经元脉冲计数是独立的。对于神经元 $n$，在持续时间为 $T$ 的窗口内的计数 $x_{n}$ 服从泊松分布：$x_{n} \\mid s \\sim \\mathrm{Poisson}(\\lambda_{n,s} T)$。\n- **速率参数的先验：** 每个速率参数 $\\lambda_{n,s}$ 都有一个独立的伽马先验：$\\lambda_{n,s} \\sim \\mathrm{Gamma}(\\alpha, \\beta)$，其中 $\\alpha > 0$ 是待调整的形状超参数，$\\beta > 0$ 是固定的速率超参数。\n- **类别先验：** 类别先验 $p(s)$ 通过每个折叠内训练数据上的经验频率来估计。\n- **超参数选择方法：** $L$ 折交叉验证。数据集 $\\mathcal{D}$ 被划分为 $L$ 个不相交的测试集 $\\{\\mathcal{D}_{\\mathrm{test}}^{(\\ell)}\\}_{\\ell=1}^{L}$ 和相应的训练集 $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}=\\mathcal{D}\\setminus \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$。\n- **符号表示：**\n    - $p(x \\mid s, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$: 后验预测似然，通过在由训练数据 $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$ 更新的 $\\lambda_{n,s}$ 的先验上积分得到。\n    - $p(s \\mid x, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$: 类的后验概率，通过贝叶斯法则从后验预测似然和类别先验导出。\n- **目标：** 确定用于选择 $\\alpha$ 的正确交叉验证准则，以最大化以下任一指标：\n    1.  真实类别的留出对数后验概率。\n    2.  留出分类准确率。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n- **科学依据：** 该问题在计算神经科学和贝叶斯统计学中有坚实的基础。脉冲计数的泊松模型、伽马先验（它是泊松似然的共轭先验）、朴素贝叶斯假设，以及使用交叉验证进行超参数调整，都是该领域的标准且行之有效的方法。\n- **良态问题（Well-Posed）：** 该问题定义清晰。它提出了一个完整的统计模型和一个具体的任务（确定超参数选择的正确数学公式）。基于交叉验证和概率分类的基本原则，存在唯一的解集。\n- **目标：** 语言正式且无歧义。所有术语都有数学定义。\n\n问题陈述没有科学或事实上的不健全之处，不是不可形式化的，是完整和一致的，是现实的，并且是良态的。这是一个应用于神经科学数据的统计机器学习中的标准、非平凡问题。\n\n**步骤 3：结论和行动**\n\n问题是有效的。我将继续进行推导和分析。\n\n### 推导和选项分析\n\n目标是通过使用 $L$ 折交叉验证估计模型的泛化性能来选择超参数 $\\alpha$。交叉验证的核心原理是在数据的一个子集（训练集）上训练模型，并在一个不相交的、留出的子集（测试集）上评估其性能。对每个折叠重复此过程，并汇总性能指标。选择产生最佳汇总性能的超参数值。\n\n让我们将问题中陈述的两个优化目标形式化。\n\n**1. 最大化真实类别的留出对数后验概率**\n\n对于选定的超参数 $\\alpha$ 和每个折叠 $\\ell \\in \\{1, \\dots, L\\}$，使用 $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$ 训练模型。这包括：\n- 根据 $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$ 中类别的经验频率估计类别先验 $p(s \\mid \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$。\n- 对于每个类别 $s$ 和神经元 $n$，使用 $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$ 中该类别的训练数据更新 $\\lambda_{n,s}$ 上的伽马先验，以获得后验 $p(\\lambda_{n,s} \\mid \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$。\n- 使用这些来定义后验预测似然 $p(x \\mid s, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)$。\n\n然后在测试集 $\\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$ 上评估模型。对于单个测试数据点 $(x_i, s_i) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$，我们感兴趣的量是*真实*类别 $s_i$ 的后验概率，由贝叶斯法则给出：\n$$\np(s_i \\mid x_i, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha) = \\frac{p(x_i \\mid s_i, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha) \\, p(s_i \\mid \\mathcal{D}_{\\mathrm{train}}^{(\\ell)})}{\\sum_{s'=1}^K p(x_i \\mid s', \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha) \\, p(s' \\mid \\mathcal{D}_{\\mathrm{train}}^{(\\ell)})}\n$$\n问题要求最大化留出的*对数后验*概率。这对应于目标函数，通常称为留出标签的对数似然或负对数损失。总分是通过对测试集中的所有数据点求和该量，然后对每个折叠的分数求和来获得的。这给出了交叉验证的目标：\n$$\n\\sum_{\\ell=1}^{L} \\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p(s_{i} \\mid x_{i}, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)\n$$\n最优超参数 $\\hat{\\alpha}$ 是使该值最大化的那个。\n\n**2. 最大化留出分类准确率**\n\n准确率是正确分类样本的比例。对于一个测试数据点 $(x_i, s_i) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$，我们首先确定预测的类别 $\\hat{s}_{\\alpha}^{(\\ell)}(x_i)$。这是通过找到具有最大后验 (MAP) 概率的类别来完成的：\n$$\n\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) = \\arg\\max_{s \\in \\{1,\\dots,K\\}} p(s \\mid x_i, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha)\n$$\n如果 $\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) = s_{i}$，则预测正确。我们可以用一个指示函数 $\\mathbf{1}\\{\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) = s_{i}\\}$ 来表示，如果预测正确，则为 1，否则为 0。\n为了找到所有折叠中正确预测的总数，我们将此指示函数对所有测试点和所有折叠求和。最大化准确率等同于最大化此总数。交叉验证的目标是：\n$$\n\\sum_{\\ell=1}^{L} \\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\mathbf{1}\\{\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) = s_{i}\\}\n$$\n最优超参数 $\\hat{\\alpha}$ 是使该总和最大化的那个。\n\n现在，我们评估每个候选目标。\n\n**A. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$**\n这个表达式计算了对于每个折叠 $\\ell$，在测试集 $\\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$ 中所有点 $(x_i, s_i)$ 的真实类别 $s_i$ 的对数后验概率之和。后验概率是以该折叠的训练数据 $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$ 为条件的。然后它将这些值在所有 $L$ 个折叠上求和。这完全符合最大化真实类别留出对数后验概率的推导。\n**结论：正确**\n\n**B. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\mathbf{1}\\!\\left\\{\\;\\hat{s}_{\\alpha}^{(\\ell)}(x_{i}) \\;=\\; s_{i}\\;\\right\\}$, 其中 $\\displaystyle \\hat{s}_{\\alpha}^{(\\ell)}(x) \\;=\\; \\arg\\max_{s \\in \\{1,\\dots,K\\}} p\\!\\left(s \\,\\middle|\\, x, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$**\n这个表达式将预测类别 $\\hat{s}_{\\alpha}^{(\\ell)}(x_i)$ 定义为在基于 $\\mathcal{D}_{\\mathrm{train}}^{(\\ell)}$ 训练的模型上最大化后验概率的类别。然后它计算在测试集 $\\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$ 中所有点上此预测与真实类别 $s_i$ 匹配的次数，并在所有折叠上对这些计数求和。这是一个完整的交叉验证运行中正确分类的总数。最大化这个值等同于最大化交叉验证的准确率。这完全符合最大化留出分类准确率的推导。\n**结论：正确**\n\n**C. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{i=1}^{M} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}, \\alpha\\right)$**\n这个表达式使用也在完整数据集 $\\mathcal{D}$ 上训练过的模型，计算完整数据集 $\\mathcal{D}$ 中每个点 $(x_i, s_i)$ 的对数后验概率。这是一个*样本内*性能指标。它没有使用留出数据，因此不是一个交叉验证准则。该指标容易过拟合；模型可能在其训练数据上获得高分，但无法泛化到新数据。因此，对于旨在提高泛化能力的超参数选择来说，这是一个不恰当的目标。\n**结论：不正确**\n\n**D. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(x_{i} \\,\\middle|\\, s_{i}, \\mathcal{D}_{\\mathrm{train}}^{(\\ell)}, \\alpha\\right)$**\n这个表达式求的是给定真实类别 $s_i$ 的特征 $x_i$ 的对数*后验预测似然*之和。虽然这一项 $\\log p(x_i|s_i, \\dots)$ 是完整对数后验的一个组成部分，但它不是完整的目标。分类器的目标是从 $x$ 预测 $s$，所以性能指标应基于 $p(s|x)$，而不是 $p(x|s)$。例如，该指标忽略了类别先验 $p(s)$ 和归一化证据项 $p(x_i)$。一个模型可能在为一个稀有类别（高的 $p(x|s_{\\text{rare}})$）建模特征方面非常出色，但总体上却是一个差的分类器，因为它从不预测该类别。这个目标与最大化分类准确率或类别后验概率的既定目标不符。\n**结论：不正确**\n\n**E. $\\displaystyle \\hat{\\alpha} \\;=\\; \\arg\\max_{\\alpha>0} \\;\\sum_{\\ell=1}^{L} \\;\\sum_{(x_{i},s_{i}) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}} \\log p\\!\\left(s_{i} \\,\\middle|\\, x_{i}, \\mathcal{D}, \\alpha\\right)$, 使用在 $\\mathcal{D}$ 上拟合的参数为每个折叠计算 $p\\!\\left(s \\mid x, \\mathcal{D}, \\alpha\\right)$**\n这个表达式在交叉验证测试集上对一个量求和。然而，被评估的量 $\\log p(s_i \\mid x_i, \\mathcal{D}, \\alpha)$ 是使用在整个数据集 $\\mathcal{D}$ 上训练的模型计算的。这意味着对于任何测试点 $(x_i, s_i) \\in \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$，模型已经在 $(x_i, s_i)$ 上训练过，因为 $\\mathcal{D}_{\\mathrm{test}}^{(\\ell)} \\subset \\mathcal{D}$。这是一种数据泄露形式，即测试数据被用于训练过程。它违反了交叉验证的基本原则。该目标在数学上与选项 C 中的目标相同，因为 $\\mathcal{D} = \\bigcup_{\\ell=1}^L \\mathcal{D}_{\\mathrm{test}}^{(\\ell)}$ 且后验项不依赖于 $\\ell$。它是一个以交叉验证形式伪装的样本内评估。\n**结论：不正确**",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}