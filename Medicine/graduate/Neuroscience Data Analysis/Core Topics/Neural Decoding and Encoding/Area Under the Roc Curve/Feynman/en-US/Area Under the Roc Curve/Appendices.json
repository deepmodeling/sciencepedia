{
    "hands_on_practices": [
        {
            "introduction": "To truly master the Area Under the ROC Curve (AUC), it's essential to move beyond software packages and understand how it's constructed from first principles. This exercise guides you through building an empirical ROC curve from a realistic set of decoder outputs, including the common scenario of tied scores . By manually calculating the contribution of each score bin to the total area, you will gain a concrete understanding of how the True and False Positive Rates shape the curve and its summary metric, the AUC.",
            "id": "4138914",
            "problem": "A population decoder trained on multiunit spike features attempts to determine whether a vibrotactile stimulus was present ($+$) or absent ($-$) on each trial. The decoder outputs a discretized score interpreted as a calibrated posterior $p(\\text{stimulus present} \\mid \\text{neural features})$, binned into five levels $\\{0.9, 0.7, 0.5, 0.3, 0.1\\}$. Because of binning, many trials share tied scores. Across a large held-out test set, the following counts are observed at each score level:\n- Score $0.9$: $10$ trials with stimulus present ($+$) and $1$ trial with stimulus absent ($-$).\n- Score $0.7$: $8$ trials with stimulus present ($+$) and $3$ trials with stimulus absent ($-$).\n- Score $0.5$: $5$ trials with stimulus present ($+$) and $6$ trials with stimulus absent ($-$).\n- Score $0.3$: $3$ trials with stimulus present ($+$) and $9$ trials with stimulus absent ($-$).\n- Score $0.1$: $1$ trial with stimulus present ($+$) and $12$ trials with stimulus absent ($-$).\n\nUsing only the foundational definitions of the Receiver Operating Characteristic (ROC) and the Area Under the ROC Curve (AUC), construct the empirical ROC by sweeping a decision threshold from above the highest score downward across the bins, treating all trials within a bin as tied. Let $N_{+}$ denote the total number of stimulus-present trials and $N_{-}$ the total number of stimulus-absent trials. Define the True Positive Rate (TPR) as $\\mathrm{TPR} = \\mathrm{TP}/N_{+}$ and the False Positive Rate (FPR) as $\\mathrm{FPR} = \\mathrm{FP}/N_{-}$, where $\\mathrm{TP}$ and $\\mathrm{FP}$ are the counts of correctly and incorrectly thresholded trials, respectively, at each threshold.\n\nExplain, from first principles, how the step-function ROC accumulates area: why vertical segments contribute no area, why horizontal segments contribute area equal to their width times the appropriate height, and how tied scores within a bin determine that height in a way that yields a unique AUC. Then compute the AUC for this empirical ROC as a single exact value. Provide your final AUC without rounding. The final answer must be a single real number or a single closed-form analytical expression with no units.",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- A population decoder outputs a discretized score binned into five levels: $\\{0.9, 0.7, 0.5, 0.3, 0.1\\}$.\n- Counts of stimulus-present ($+$) and stimulus-absent ($-$) trials for each score:\n    - Score $0.9$: $10$ ($+$), $1$ ($-$).\n    - Score $0.7$: $8$ ($+$), $3$ ($-$).\n    - Score $0.5$: $5$ ($+$), $6$ ($-$).\n    - Score $0.3$: $3$ ($+$), $9$ ($-$).\n    - Score $0.1$: $1$ ($+$), $12$ ($-$).\n- Total number of stimulus-present trials is denoted $N_{+}$.\n- Total number of stimulus-absent trials is denoted $N_{-}$.\n- True Positive Rate (TPR) is defined as $\\mathrm{TPR} = \\mathrm{TP}/N_{+}$.\n- False Positive Rate (FPR) is defined as $\\mathrm{FPR} = \\mathrm{FP}/N_{-}$.\n- The empirical Receiver Operating Characteristic (ROC) curve is to be constructed by sweeping a decision threshold from above the highest score downward.\n- All trials within a bin are to be treated as tied.\n- The task requires an explanation of how the AUC accumulates and a final calculation of the AUC as a single exact value.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded:** The problem is firmly rooted in the standard practices of signal detection theory and machine learning as applied to neuroscience data analysis. ROC analysis is a fundamental tool for evaluating classifier performance. The scenario described is a typical application.\n- **Well-Posed:** The problem provides all necessary data (counts of outcomes for each score bin) and explicit definitions for the quantities to be calculated (TPR, FPR). The instruction to treat trials within a bin as tied, combined with the standard definition of an empirical ROC curve, specifies a clear procedure for obtaining a unique solution.\n- **Objective:** The problem is stated using precise, quantitative, and unbiased language.\n\nThe problem exhibits none of the invalidity flaws:\n- It does not violate any scientific or mathematical principles.\n- It is formalizable and directly relevant to the stated topic area.\n- The setup is complete and consistent.\n- The conditions and data are realistic for a neurophysiological experiment.\n- The structure leads to a unique, meaningful solution.\n- The problem is not trivial, as it requires careful step-by-step construction and calculation, as well as conceptual explanation.\n- The result is mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution\nThe Area Under the Receiver Operating Characteristic Curve (AUC) is a measure of a classifier's ability to distinguish between two classes. The ROC curve itself is a plot of the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. We begin by calculating the total number of positive ($N_{+}$) and negative ($N_{-}$) trials.\n\nFrom the givens:\n$N_{+} = 10 + 8 + 5 + 3 + 1 = 27$\n$N_{-} = 1 + 3 + 6 + 9 + 12 = 31$\n\nThe empirical ROC curve is constructed by sweeping a decision threshold from high to low. A trial is classified as positive if its score is greater than or equal to the threshold. The curve consists of a series of points $(FPR_i, TPR_i)$ corresponding to a set of decreasing thresholds. The curve always starts at $(0, 0)$ (threshold so high that no trials are classified as positive) and ends at $(1, 1)$ (threshold so low that all trials are classified as positive).\n\nBecause the scores are binned and treated as tied, we process the data bin by bin, from the highest score to the lowest. For each bin, we calculate the change in True Positives (TP) and False Positives (FP) and update the cumulative counts. The ROC curve will be a sequence of line segments connecting the points derived from these cumulative counts.\n\nLet $n_{+,s}$ and $n_{-,s}$ be the number of positive and negative trials at score $s$. Let $CTP_i$ and $CFP_i$ be the cumulative TP and FP counts after processing the $i$-th bin (ordered from highest score to lowest). The corresponding point on the ROC curve is $P_i = (FPR_i, TPR_i) = (CFP_i/N_{-}, CTP_i/N_{+})$.\n\n- **Initial state (Threshold $> 0.9$):** $CTP_0 = 0, CFP_0 = 0$. This gives the starting point $P_0 = (0, 0)$.\n\n- **Process Score $s = 0.9$:** We add the trials from this bin.\n  $n_{+,0.9} = 10, n_{-,0.9} = 1$.\n  $CTP_1 = CTP_0 + 10 = 10$.\n  $CFP_1 = CFP_0 + 1 = 1$.\n  $P_1 = (FPR_1, TPR_1) = (\\frac{1}{31}, \\frac{10}{27})$.\n\n- **Process Score $s = 0.7$:**\n  $n_{+,0.7} = 8, n_{-,0.7} = 3$.\n  $CTP_2 = CTP_1 + 8 = 18$.\n  $CFP_2 = CFP_1 + 3 = 4$.\n  $P_2 = (FPR_2, TPR_2) = (\\frac{4}{31}, \\frac{18}{27})$.\n\n- **Process Score $s = 0.5$:**\n  $n_{+,0.5} = 5, n_{-,0.5} = 6$.\n  $CTP_3 = CTP_2 + 5 = 23$.\n  $CFP_3 = CFP_2 + 6 = 10$.\n  $P_3 = (FPR_3, TPR_3) = (\\frac{10}{31}, \\frac{23}{27})$.\n\n- **Process Score $s = 0.3$:**\n  $n_{+,0.3} = 3, n_{-,0.3} = 9$.\n  $CTP_4 = CTP_3 + 3 = 26$.\n  $CFP_4 = CFP_3 + 9 = 19$.\n  $P_4 = (FPR_4, TPR_4) = (\\frac{19}{31}, \\frac{26}{27})$.\n\n- **Process Score $s = 0.1$:**\n  $n_{+,0.1} = 1, n_{-,0.1} = 12$.\n  $CTP_5 = CTP_4 + 1 = 27$.\n  $CFP_5 = CFP_4 + 12 = 31$.\n  $P_5 = (FPR_5, TPR_5) = (\\frac{31}{31}, \\frac{27}{27}) = (1, 1)$.\n\nThe empirical ROC curve is the set of line segments connecting $P_0, P_1, P_2, P_3, P_4,$ and $P_5$.\n\n### First-Principles Explanation of AUC Accumulation\nThe area under the ROC curve (AUC) is the integral of the TPR with respect to the FPR, from $FPR=0$ to $FPR=1$: $AUC = \\int_{0}^{1} TPR(FPR) \\, dFPR$.\nFor an empirical ROC curve composed of line segments, this integral becomes a sum of areas of geometric shapes under each segment.\n\n- **Vertical Segments:** If a threshold crosses scores belonging only to positive examples ($n_+=k, n_-=0$), the curve moves vertically from $(FPR, TPR_{old})$ to $(FPR, TPR_{new})$. The change in FPR is $\\Delta FPR = 0$. The area contributed by this segment is a rectangle of width $0$, so the area is $0$. Vertical segments contribute no area to the AUC.\n\n- **Horizontal Segments:** If a threshold crosses scores belonging only to negative examples ($n_+=0, n_-=k$), the curve moves horizontally from $(FPR_{old}, TPR)$ to $(FPR_{new}, TPR)$. The height (TPR) is constant. This segment forms a rectangle of height $TPR$ and width $\\Delta FPR = FPR_{new} - FPR_{old}$. The area contributed is $TPR \\times \\Delta FPR$.\n\n- **Diagonal Segments (Tied Scores):** In this problem, each score bin contains both positive and negative examples, resulting in tied scores. When the threshold crosses such a bin, both TP and FP counts increase simultaneously. This creates a diagonal line segment on the ROC plot, connecting point $P_{i-1} = (FPR_{i-1}, TPR_{i-1})$ to $P_{i} = (FPR_{i}, TPR_{i})$. The area under this segment is a trapezoid. The area of this trapezoid is given by the formula:\n$$Area_i = \\frac{1}{2}(\\text{height}_1 + \\text{height}_2) \\times \\text{width} = \\frac{1}{2}(TPR_{i-1} + TPR_i) (FPR_i - FPR_{i-1})$$\nThis approach yields a unique AUC because it follows a standard, deterministic convention. While the true ordering of trials within a tied block is unknown, this diagonal line represents the expected path if all possible orderings of the tied positive and negative trials were considered and their ROC paths averaged. The trapezoidal area is therefore the principled, expected area contribution from the block of tied scores, ensuring a single, unique value for the AUC.\n\nThe total AUC is the sum of the areas of these trapezoids:\n$$AUC = \\sum_{i=1}^{5} \\frac{1}{2}(TPR_{i-1} + TPR_i)(FPR_i - FPR_{i-1})$$\n\n### Calculation of AUC\nWe now compute the area for each segment.\n$AUC = \\frac{1}{2}\\sum_{i=1}^{5} (TPR_{i-1} + TPR_i)(FPR_i - FPR_{i-1})$\n\n- Area 1 (segment $P_0P_1$):\n$A_1 = \\frac{1}{2}(0 + \\frac{10}{27})(\\frac{1}{31} - 0) = \\frac{1}{2} \\cdot \\frac{10}{27} \\cdot \\frac{1}{31} = \\frac{5}{27 \\cdot 31}$\n\n- Area 2 (segment $P_1P_2$):\n$A_2 = \\frac{1}{2}(\\frac{10}{27} + \\frac{18}{27})(\\frac{4}{31} - \\frac{1}{31}) = \\frac{1}{2} \\cdot \\frac{28}{27} \\cdot \\frac{3}{31} = \\frac{42}{27 \\cdot 31}$\n\n- Area 3 (segment $P_2P_3$):\n$A_3 = \\frac{1}{2}(\\frac{18}{27} + \\frac{23}{27})(\\frac{10}{31} - \\frac{4}{31}) = \\frac{1}{2} \\cdot \\frac{41}{27} \\cdot \\frac{6}{31} = \\frac{123}{27 \\cdot 31}$\n\n- Area 4 (segment $P_3P_4$):\n$A_4 = \\frac{1}{2}(\\frac{23}{27} + \\frac{26}{27})(\\frac{19}{31} - \\frac{10}{31}) = \\frac{1}{2} \\cdot \\frac{49}{27} \\cdot \\frac{9}{31} = \\frac{441}{2 \\cdot 27 \\cdot 31}$\n\n- Area 5 (segment $P_4P_5$):\n$A_5 = \\frac{1}{2}(\\frac{26}{27} + 1)(\\frac{31}{31} - \\frac{19}{31}) = \\frac{1}{2}(\\frac{53}{27})(\\frac{12}{31}) = \\frac{318}{27 \\cdot 31}$\n\nNow, we sum these areas. Let's use a common denominator of $2 \\cdot 27 \\cdot 31 = 1674$.\n$AUC = \\frac{2 \\cdot 5 + 2 \\cdot 42 + 2 \\cdot 123 + 441 + 2 \\cdot 318}{1674}$\n$AUC = \\frac{10 + 84 + 246 + 441 + 636}{1674}$\n$AUC = \\frac{1417}{1674}$\n\nThis is the exact value for the Area Under the ROC Curve.",
            "answer": "$$\\boxed{\\frac{1417}{1674}}$$"
        },
        {
            "introduction": "With the mechanics of AUC calculation in hand, we now turn to a crucial question: why is AUC often preferred over simpler metrics like accuracy? This practice presents a thought experiment involving a severely imbalanced dataset, a common challenge in fields like medical screening and neuroscience . By analyzing a situation where a classifier can achieve high accuracy while having no actual discriminative ability, you will discover how AUC provides a more reliable and insightful measure of performance.",
            "id": "4138865",
            "problem": "A research team is analyzing calcium imaging signals to detect the presence of a faint odor stimulus in mouse olfactory cortex. Each trial produces a scalar decision variable $S \\in [0,1]$ from a trained decoder that aggregates fluorescence dynamics over $100$ ms windows. Due to experimental constraints, the dataset is severely imbalanced: the probability of stimulus presence is $P(Y=1)=0.01$ and absence is $P(Y=0)=0.99$, where $Y \\in \\{0,1\\}$ denotes the true class. In a particular session, because the decoder was trained on poorly aligned features, its scores are uninformative: for both classes, $S$ is distributed continuously and independently as $\\mathrm{Uniform}(0,1)$. During deployment, the lab used a fixed decision threshold $\\tau^{\\star}=1$ to minimize false alarms; under this threshold, the classifier predicts $Y=0$ for all trials, yielding high accuracy dominated by the majority class.\n\nStarting from the standard definitions of the Receiver Operating Characteristic (ROC) curve, compute the Area Under the Receiver Operating Characteristic Curve (AUC) for this session under the continuous score model described above. Explain briefly, in your reasoning, why ROC analysis is more informative than accuracy in this setting of severe class imbalance. Express the final AUC value as a real number rounded to four significant figures. No units are required.",
            "solution": "The problem is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Decision variable: $S \\in [0,1]$.\n- Data source: Calcium imaging signals from mouse olfactory cortex for detecting odor stimulus.\n- Temporal window: $100$ ms.\n- True class variable: $Y \\in \\{0,1\\}$, where $Y=1$ is stimulus presence and $Y=0$ is stimulus absence.\n- Class probabilities: $P(Y=1)=0.01$ and $P(Y=0)=0.99$.\n- Conditional probability distributions for the score $S$:\n  - For class $Y=1$ (presence): $P(S|Y=1) \\sim \\mathrm{Uniform}(0,1)$.\n  - For class $Y=0$ (absence): $P(S|Y=0) \\sim \\mathrm{Uniform}(0,1)$.\n- The distributions are continuous and independent.\n- A fixed decision threshold $\\tau^{\\star}=1$ was used in deployment.\n- The tasks are to compute the Area Under the Receiver Operating Characteristic Curve (AUC), explain why ROC analysis is more informative than accuracy in this context, and provide the AUC value rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, describing a standard scenario in signal detection theory and machine learning applied to neuroscience data. The concept of an uninformative classifier, modeled by identical score distributions for both classes, is a classic textbook case for illustrating the properties of ROC analysis. The provided information is complete, self-contained, and logically consistent. The distributions are well-defined, and the task—computing the AUC—is well-posed. The problem does not violate any fundamental scientific principles, contain factual errors, or rely on subjective claims. The values for class imbalance are extreme but realistic for many screening applications. Therefore, the problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. It is created by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n\nThe True Positive Rate, also known as sensitivity or recall, is the probability of a positive test result, given that the condition is truly present. For a given decision threshold $\\tau$, a prediction is positive ($\\hat{Y}=1$) if the score $S \\ge \\tau$. Thus:\n$$\n\\text{TPR}(\\tau) = P(\\hat{Y}=1 | Y=1) = P(S \\ge \\tau | Y=1)\n$$\n\nThe False Positive Rate is the probability of a positive test result, given that the condition is truly absent:\n$$\n\\text{FPR}(\\tau) = P(\\hat{Y}=1 | Y=0) = P(S \\ge \\tau | Y=0)\n$$\n\nThe problem states that the score $S$ for both classes is distributed according to a continuous uniform distribution on the interval $[0,1]$. For a random variable $X \\sim \\mathrm{Uniform}(a,b)$, its probability density function (PDF) is $f(x) = \\frac{1}{b-a}$ for $x \\in [a,b]$ and $0$ otherwise. Its cumulative distribution function (CDF) is $F(x) = P(X \\le x) = \\frac{x-a}{b-a}$ for $x \\in [a,b]$.\n\nIn this case, for both $P(S|Y=1)$ and $P(S|Y=0)$, the distribution is $\\mathrm{Uniform}(0,1)$. Let $f_1(s)$ and $f_0(s)$ be the PDFs for the positive and negative classes, respectively. We have $f_1(s) = f_0(s) = 1$ for $s \\in [0,1]$ and $0$ otherwise.\n\nWe can now calculate $\\text{TPR}(\\tau)$ and $\\text{FPR}(\\tau)$ for any threshold $\\tau \\in [0,1]$.\n$$\n\\text{TPR}(\\tau) = P(S \\ge \\tau | Y=1) = \\int_{\\tau}^{1} f_1(s) ds = \\int_{\\tau}^{1} 1 ds = [s]_{\\tau}^{1} = 1 - \\tau\n$$\n$$\n\\text{FPR}(\\tau) = P(S \\ge \\tau | Y=0) = \\int_{\\tau}^{1} f_0(s) ds = \\int_{\\tau}^{1} 1 ds = [s]_{\\tau}^{1} = 1 - \\tau\n$$\n\nThe ROC curve is parameterized by $\\tau$. Let $x = \\text{FPR}(\\tau)$ and $y = \\text{TPR}(\\tau)$. From our calculations, we have:\n$$\nx = 1 - \\tau\n$$\n$$\ny = 1 - \\tau\n$$\nThis implies that the equation for the ROC curve is $y = x$. As the threshold $\\tau$ sweeps from $1$ down to $0$, both $x$ and $y$ sweep from $0$ to $1$. The ROC curve is therefore the diagonal line segment from the point $(0,0)$ to $(1,1)$ in the ROC space.\n\nThe Area Under the Curve (AUC) is the integral of the ROC curve, $y(x)$, from $x=0$ to $x=1$.\n$$\n\\text{AUC} = \\int_{0}^{1} y(x) dx = \\int_{0}^{1} x dx\n$$\nEvaluating the integral:\n$$\n\\text{AUC} = \\left[ \\frac{1}{2}x^2 \\right]_{0}^{1} = \\frac{1}{2}(1)^2 - \\frac{1}{2}(0)^2 = \\frac{1}{2} = 0.5\n$$\nAn AUC of $0.5$ signifies that the classifier has no discriminative ability; it performs no better than random chance. This result is expected, as the score distributions for the positive and negative classes are identical.\n\nRegarding the comparison with accuracy: In the problem, a threshold of $\\tau^{\\star}=1$ is used. With a continuous distribution on $[0,1]$, the probability of $S \\ge 1$ is effectively $P(S=1)$, which is $0$. Therefore, this classifier always predicts the negative class, $\\hat{Y}=0$.\n\nThe accuracy is the overall probability of a correct prediction:\n$$\n\\text{Accuracy} = P(\\hat{Y}=Y) = P(\\hat{Y}=0, Y=0) + P(\\hat{Y}=1, Y=1)\n$$\nSince $\\hat{Y}$ is always $0$, the second term is $0$. The first term is:\n$$\nP(\\hat{Y}=0, Y=0) = P(\\hat{Y}=0 | Y=0) P(Y=0)\n$$\n$P(\\hat{Y}=0 | Y=0)$ is the True Negative Rate (TNR). Since the prediction is always negative, $\\text{TNR}=1$. The prior probability of the negative class is given as $P(Y=0)=0.99$.\nThus, the accuracy is:\n$$\n\\text{Accuracy} = 1 \\times 0.99 = 0.99\n$$\nThis accuracy of $99\\%$ is exceptionally high, yet it is profoundly misleading. It suggests an excellent classifier, whereas the classifier is completely uninformative. This paradox arises because the accuracy metric is dominated by the performance on the majority class ($Y=0$) due to the severe class imbalance ($99\\%$ vs $1\\%$). By always guessing the majority class, the classifier achieves a high accuracy score without any genuine ability to discriminate.\n\nIn contrast, the ROC curve and the AUC are insensitive to class prevalence. Both TPR and FPR are conditioned on the true class, meaning they evaluate performance within the positive and negative populations separately. The AUC of $0.5$ correctly and unambiguously reveals that the classifier has no capacity to distinguish between the two classes, exposing the failure that the high accuracy conceals. Hence, ROC analysis is a far more informative and reliable measure of classifier performance in settings with severe class imbalance.\n\nThe final AUC value, rounded to four significant figures, is $0.5000$.",
            "answer": "$$\\boxed{0.5000}$$"
        },
        {
            "introduction": "Applying AUC analysis in a research setting requires a comprehensive and statistically valid protocol to ensure results are robust and reproducible. This final problem challenges you to identify a gold-standard reporting methodology for a complex neuroscience decoding study with multiple sources of variance . Evaluating the options will test your understanding of critical advanced topics, including nested cross-validation to prevent optimistic bias, handling of clustered data, and the correct construction of confidence intervals.",
            "id": "4138956",
            "problem": "You are analyzing binary trial-level data from a cortical recording study in which multiunit activity from $N$ simultaneously recorded neurons was collected across $M$ sessions. In each session $i \\in \\{1,\\dots,M\\}$, there are $n_i$ trials with labels $Y \\in \\{0,1\\}$ indicating whether a target stimulus was presented ($Y=1$) or not ($Y=0$). You fit a linear decoder with a single regularization hyperparameter $\\lambda$ to produce a real-valued score $\\hat{s}$ per trial. The neuroscience data consist of integer spike counts in fixed windows, so after model scoring, ties in $\\hat{s}$ can occur. You wish to report the discrimination performance using Area Under the Receiver Operating Characteristic curve (AUC), where Receiver Operating Characteristic (ROC) is the function mapping the False Positive Rate (FPR) to the True Positive Rate (TPR), and you will also consider partial AUC where appropriate. Your goal is to ensure a reproducible and statistically valid report, including point estimate, confidence interval, cross-validation, and tie handling, with special attention to the dependence induced by sessions and any class imbalance.\n\nSelect the single option that, if included verbatim in a Methods section, most completely and correctly specifies a reproducible protocol for reporting AUC in this setting, including: the point estimate of $AUC$, the construction of the Confidence Interval (CI), the Cross-Validation (CV) scheme (including how hyperparameters are tuned), explicit handling of ties in $\\hat{s}$, and the definition and standardization of partial $AUC$ if used. The protocol must avoid data leakage, respect session-level dependence, and be unambiguous enough for an independent group to reproduce the reported $AUC$ and its uncertainty.\n\nA. Point estimate: We report $AUC$ computed on pooled out-of-sample predictions from a nested, session-respecting CV. Specifically, the outer CV is leave-one-session-out ($M$ folds). Within each outer fold, features are standardized (mean subtraction and variance scaling) using only the training sessions, and $\\lambda$ is tuned by an inner stratified $K$-fold CV within the training sessions with $K=5$, repeated $R=10$ times with different random seeds; the chosen $\\lambda$ maximizes inner-fold $AUC$. The point estimate of $AUC$ is computed once on the union of all outer-fold test predictions (micro-averaging across sessions), with ties handled by midranks, i.e., treating $\\Pr(\\hat{S}^+ = \\hat{S}^-)$ as contributing $\\frac{1}{2}$ to $AUC$. CI: We compute a $95\\%$ CI by a session-level stratified cluster bootstrap with $B=2000$ replicates, resampling sessions with replacement and rerunning the entire nested CV per replicate; the CI is the percentile interval from the empirical bootstrap distribution of $AUC$. Partial $AUC$: If reported, we define $\\text{pAUC}_{[0,\\alpha]}$ on the FPR range $[0,\\alpha]$ with $\\alpha=0.1$, compute it on the same pooled out-of-sample predictions with midrank tie handling, and standardize by dividing by $\\alpha$ to map to $[0,1]$. All randomization seeds for CV partitioning are fixed and reported (seed $=42$), and class stratification is preserved within folds.\n\nB. Point estimate: We average $AUC$ across $K=10$ trial-level CV folds created by random trial partitioning across all sessions, reporting the mean and standard deviation. CI: We use a $95\\%$ normal approximation CI based on the across-fold mean and variance, assuming independence. Hyperparameter $\\lambda$ is tuned on the same folds used for testing to maximize average $AUC$. Ties are left as-is without special handling. Partial $AUC$ is optionally reported for “low FPR” but without specifying a numeric range or whether it is standardized.\n\nC. Point estimate: We pool all out-of-sample predictions from a single stratified $K=5$ trial-level CV (randomizing trials across sessions) and compute $AUC$ once on the pooled set. Hyperparameter $\\lambda$ is tuned by inner CV but the inner folds include the outer test trials to increase stability. Ties are broken deterministically by trial index when $\\hat{s}$ values are equal. CI: We report a $95\\%$ CI using the DeLong method computed on the pooled predictions, assuming independent observations. Partial $AUC$ is reported on $[0,0.1]$ but is not standardized.\n\nD. Point estimate: We compute $AUC$ on predictions from the training data only, because cross-validation is not necessary for $AUC$. CI: We bootstrap $B=500$ times at the trial level without stratification and report the percentile $95\\%$ CI. Hyperparameter $\\lambda$ is chosen to maximize training $AUC$. Ties are ignored. Partial $AUC$ is reported by restricting to thresholds that yield $FPR \\le 0.1$ after selecting the threshold that maximizes $AUC$ on the test set.\n\nChoose the single best option.",
            "solution": "The user requires the selection of the most complete and correct protocol for reporting the Area Under the Receiver Operating Characteristic curve (AUC) for a binary classification problem in neuroscience, with specific constraints on statistical validity and reproducibility.\n\n### Analysis of the Problem Statement\nThe problem describes a common scenario in neuroscience data analysis: decoding a binary outcome ($Y \\in \\{0,1\\}$) from neural activity (spike counts from $N$ neurons) recorded across multiple sessions ($M$ sessions). The data have a hierarchical or clustered structure, where trials within a session are not independent of each other. Key challenges highlighted are the session-level dependence, the presence of ties in the decoder scores ($\\hat{s}$), the need for hyperparameter ($\\lambda$) tuning, the construction of a valid confidence interval (CI), and the proper reporting of partial AUC (pAUC). A valid protocol must address all these points correctly to avoid common pitfalls like data leakage and underestimation of variance.\n\nThe problem is scientifically grounded, well-posed, and objective. It presents a realistic data analysis challenge and requires the application of established principles from statistics and machine learning for its resolution. The provided details are sufficient and consistent for a rigorous evaluation of the proposed protocols. The problem is therefore deemed valid.\n\n### Principle-Based Derivation of a Correct Protocol\nA correct protocol must adhere to the following principles:\n\n1.  **Estimation of Generalization Performance**: The reported performance metric ($AUC$) must estimate how well the decoder generalizes to new, unseen data. This requires a strict separation of training and testing data. Reporting performance on the training data itself is incorrect as it typically reflects overfitting and is optimistically biased. Cross-validation (CV) is the standard method for estimating generalization performance.\n\n2.  **Handling of Dependent Data**: The data consists of trials clustered within $M$ sessions. Trials from the same session are likely to be more similar to each other than to trials from other sessions due to non-stationarities in neural recordings, animal behavior, or experimental hardware. Standard $K$-fold CV, which randomizes individual trials into folds, violates the independence assumption and leads to data leakage, as the model may learn session-specific artifacts present in both the training and testing portions of a fold. A correct CV scheme must respect the data structure by keeping all trials from a given session together, either in the training set or the test set. Leave-One-Session-Out (LOSO) cross-validation is a natural choice for this structure.\n\n3.  **Unbiased Hyperparameter Tuning**: The regularization hyperparameter $\\lambda$ must be chosen without using information from the test set. If $\\lambda$ is tuned using data that includes the test set, this constitutes data leakage and will lead to an optimistically biased performance estimate. The correct procedure is **nested cross-validation**. The outer loop splits the data into training and test folds (e.g., by session). For each outer fold, an inner cross-validation loop is performed *only on the training data of that a fold* to select the optimal $\\lambda$. The model is then retrained on the entire outer training fold using the selected $\\lambda$ and evaluated on the held-out outer test fold.\n\n4.  **Confidence Interval for Dependent Data**: Standard analytical methods for calculating a CI for the $AUC$, such as the DeLong method, assume that the observations (trials) are independent and identically distributed (IID). This assumption is violated by the session-level clustering. A valid CI must account for the correlation structure. The **cluster bootstrap** is the appropriate method. In this procedure, the clusters (sessions) are the units of resampling, not the individual trials. The entire model fitting and evaluation pipeline, including the nested CV for hyperparameter tuning, must be repeated on each bootstrap sample to generate an empirical distribution of the $AUC$ statistic, from which a percentile CI can be derived.\n\n5.  **Handling of Tied Scores**: The problem states that decoder scores $\\hat{s}$ may have ties, which is common with integer-valued features like spike counts. The $AUC$ can be interpreted as the probability that a randomly chosen positive-class sample has a higher score than a randomly chosen negative-class sample, i.e., $AUC = \\Pr(\\hat{S}^+ > \\hat{S}^-)$. When ties are possible, this definition must be extended. The standard, unbiased approach is to give half-credit for ties: $AUC = \\Pr(\\hat{S}^+ > \\hat{S}^-) + \\frac{1}{2}\\Pr(\\hat{S}^+ = \\hat{S}^-)$. This is equivalent to using midranks for tied values when computing the Wilcoxon-Mann-Whitney U statistic, of which $AUC$ is a linear transformation.\n\n6.  **Partial AUC (pAUC)**: If pAUC is reported, its definition must be unambiguous. This includes specifying the range of the false positive rate (FPR), e.g., $[0, \\alpha]$, and specifying the standardization method. A common standardization is to divide the area by the maximum possible area in that range for a perfect classifier, or more simply, to divide by the range width $\\alpha$ to scale the result to $[0, 1]$.\n\n7.  **Reproducibility**: For a protocol to be reproducible, all sources of randomness, such as the partitioning of data in CV, must be controllable. This is typically achieved by setting and reporting the seed for the pseudo-random number generator.\n\n### Option-by-Option Analysis\n\n**Option A**\nThis option presents a protocol that aligns perfectly with the principles outlined above.\n-   **CV**: It correctly specifies a \"nested, session-respecting CV\" using a \"leave-one-session-out\" outer loop to handle session dependence.\n-   **Hyperparameter Tuning**: It correctly describes the inner CV loop for tuning $\\lambda$ on the training sessions only, avoiding data leakage.\n-   **Point Estimate**: It describes computing a single AUC on pooled out-of-sample predictions (\"micro-averaging\"), which is a robust and standard practice.\n-   **Tie Handling**: It explicitly and correctly states the method for handling ties (\"midranks... contributing $\\frac{1}{2}$ to $AUC$\").\n-   **CI**: It correctly specifies a \"session-level stratified cluster bootstrap\" where the \"entire nested CV\" is rerun per replicate. This is the correct procedure for dependent data.\n-   **pAUC**: It provides a specific definition ($\\text{pAUC}_{[0,0.1]}$) and a clear standardization method (dividing by $\\alpha$).\n-   **Reproducibility**: It mentions fixing and reporting randomization seeds.\n-   **Class Imbalance**: It mentions preserving class stratification in folds.\n**Verdict**: **Correct**. This option describes a complete, correct, and state-of-the-art protocol.\n\n**Option B**\nThis option contains several critical flaws.\n-   **CV**: It proposes \"random trial partitioning across all sessions\". This is incorrect as it ignores the session-level dependence, leading to data leakage and an invalid, optimistic performance estimate.\n-   **Hyperparameter Tuning**: It proposes tuning $\\lambda$ \"on the same folds used for testing\". This is a severe form of data leakage and fundamentally invalidates the performance estimate.\n-   **CI**: It proposes a normal approximation assuming independence across folds, which is violated by the session structure, rendering the CI invalid.\n-   **Tie Handling and pAUC**: The specifications are vague (\"left as-is\", \"low FPR\" without a range), making the protocol non-reproducible and incomplete.\n**Verdict**: **Incorrect**.\n\n**Option C**\nThis option also contains multiple significant errors.\n-   **CV**: It proposes a \"trial-level CV (randomizing trials across sessions)\", which incorrectly ignores the session dependency structure, same as in option B.\n-   **Hyperparameter Tuning**: It suggests that \"inner folds include the outer test trials\". This is an explicit and severe form of data leakage, directly contradicting the principles of cross-validation.\n-   **CI**: It proposes the \"DeLong method... assuming independent observations\". This assumption is explicitly violated by the problem's premise of session-level dependence, making the CI invalid.\n-   **Tie Handling**: It proposes breaking ties \"deterministically by trial index\", which is an arbitrary method without statistical justification and can introduce bias.\n-   **pAUC**: Reporting a non-standardized pAUC is poor practice for interpretability.\n**Verdict**: **Incorrect**.\n\n**Option D**\nThis option is fundamentally flawed and demonstrates a misunderstanding of basic model evaluation concepts.\n-   **CV**: It claims \"cross-validation is not necessary for $AUC$\" and proposes computing $AUC$ \"on predictions from the training data only\". This is incorrect; it measures in-sample fit, not generalization, and will produce a highly inflated, meaningless performance score.\n-   **Hyperparameter Tuning**: It suggests choosing $\\lambda$ \"to maximize training $AUC$\". This promotes overfitting, which is the exact opposite of the purpose of regularization.\n-   **CI**: It proposes bootstrapping \"at the trial level\", which ignores the session dependence and thus produces an invalid CI.\n-   **Tie Handling**: It suggests ties are \"ignored\", which is ambiguous and unscientific.\n-   **pAUC**: The description is conceptually confused, conflating a threshold-independent metric (AUC) with threshold selection and misstating how pAUC is calculated.\n**Verdict**: **Incorrect**.\n\n### Conclusion\nOption A is the only one that describes a statistically sound, complete, and reproducible protocol that correctly handles all the specified challenges of the data analysis problem: session-level dependence, hyperparameter tuning, tie handling, confidence interval construction, and pAUC reporting.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}