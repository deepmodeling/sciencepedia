{
    "hands_on_practices": [
        {
            "introduction": "为了真正掌握支持向量机，我们必须超越其抽象的数学概念，亲手实践其核心机制。这个练习将引导你完成一个基础但至关重要的计算：从给定的对偶系数 (dual coefficients) $\\alpha_i$ 和训练样本出发，推导出决策边界的具体参数。通过这个过程，你将加深对 Karush-Kuhn-Tucker (KKT) 条件在确定支持向量和偏移量 $b$ 中作用的理解，并巩固决策函数是如何构建的。",
            "id": "4197091",
            "problem": "一个实验室正在研究如何从同步记录的神经元群体中解码视觉刺激。在重复试验中，他们通过将试验平均、z-score标准化的脉冲计数投影到先前分析得出的两个正交基分量上，为每次试验提取一个二维潜在特征。目标是使用支持向量机（SVM）从这些特征中解码由 $y \\in \\{-1, +1\\}$ 标记的二元刺激身份。\n\n假设以下是用于SVM训练的仅有的两次试验：\n- 试验 $1$：特征向量 $\\boldsymbol{x}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，标签为 $y_{1} = +1$。\n- 试验 $2$：特征向量 $\\boldsymbol{x}_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，标签为 $y_{2} = -1$。\n\nSVM使用软间隔惩罚常数 $C$ 进行训练，对于得到的对偶系数，满足 $0  \\alpha_{i}  C$。训练使用标准的线性内积作为核函数，因此 $K(\\boldsymbol{x}, \\boldsymbol{z}) = \\boldsymbol{x}^{\\top}\\boldsymbol{z}$。求解优化问题后，学习到的对偶系数为 $\\alpha_{1} = 1$ 和 $\\alpha_{2} = 1$。\n\n从软间隔SVM在其原始变量中的基本约束优化定义出发，推导其对偶表示和决策函数。使用Karush-Kuhn-Tucker（KKT）条件来确定所提供的训练解所隐含的偏移量 $b$。然后，对于一个新的、特征向量为\n$$\n\\boldsymbol{x}^{\\ast} = \\begin{pmatrix} \\tfrac{3}{2} \\\\ \\tfrac{5}{6} \\end{pmatrix}\n$$\n的留出试验，评估决策函数 $f(\\boldsymbol{x}^{\\ast})$ 的值。\n您的最终答案必须是 $f(\\boldsymbol{x}^{\\ast})$ 的单一值，以最简分数表示。不需要四舍五入，也不应报告单位。",
            "solution": "该问题是有效的。它在科学上基于支持向量机的原理，问题陈述良好，信息充分且一致，并且表述客观。我们将进行完整的推导和求解。\n\n软间隔支持向量机（SVM）旨在找到一个能够分离两类数据点的超平面。其原始形式的优化问题定义如下。给定一组 $N$ 个训练数据点 $\\{\\boldsymbol{x}_i, y_i\\}_{i=1}^N$，其中 $\\boldsymbol{x}_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\{-1, +1\\}$，目标是找到权重向量 $\\boldsymbol{w} \\in \\mathbb{R}^d$ 和偏移量 $b \\in \\mathbb{R}$ 来解决以下问题：\n$$\n\\min_{\\boldsymbol{w}, b, \\boldsymbol{\\xi}} \\frac{1}{2}\\boldsymbol{w}^\\top\\boldsymbol{w} + C \\sum_{i=1}^N \\xi_i\n$$\n约束条件为：\n$$\ny_i(\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) \\ge 1 - \\xi_i, \\quad \\text{对于 } i=1, \\dots, N\n$$\n$$\n\\xi_i \\ge 0, \\quad \\text{对于 } i=1, \\dots, N\n$$\n这里，$\\xi_i$ 是允许误分类的松弛变量，$C  0$ 是惩罚此类错误的正则化参数。\n\n为了推导对偶问题，我们引入拉格朗日乘子 $\\alpha_i \\ge 0$ 和 $\\mu_i \\ge 0$。拉格朗日函数为：\n$$\n\\mathcal{L}(\\boldsymbol{w}, b, \\boldsymbol{\\xi}, \\boldsymbol{\\alpha}, \\boldsymbol{\\mu}) = \\frac{1}{2}\\boldsymbol{w}^\\top\\boldsymbol{w} + C \\sum_{i=1}^N \\xi_i - \\sum_{i=1}^N \\alpha_i [y_i(\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) - 1 + \\xi_i] - \\sum_{i=1}^N \\mu_i \\xi_i\n$$\n为了找到对偶问题，我们对原始变量 $\\boldsymbol{w}$、$b$ 和 $\\boldsymbol{\\xi}$ 最小化 $\\mathcal{L}$。我们将相应的偏导数设为零（平稳性条件）：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{w}} = \\boldsymbol{w} - \\sum_{i=1}^N \\alpha_i y_i \\boldsymbol{x}_i = \\boldsymbol{0} \\implies \\boldsymbol{w} = \\sum_{i=1}^N \\alpha_i y_i \\boldsymbol{x}_i\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b} = - \\sum_{i=1}^N \\alpha_i y_i = 0\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0 \\implies C = \\alpha_i + \\mu_i\n$$\n将这些代回拉格朗日函数，得到Wolfe对偶目标函数，该函数需要关于 $\\boldsymbol{\\alpha}$ 进行最大化。对于一个新点 $\\boldsymbol{x}$，决策函数由 $f(\\boldsymbol{x}) = \\boldsymbol{w}^\\top\\boldsymbol{x} + b$ 给出。使用平稳性条件中 $\\boldsymbol{w}$ 的表达式，我们可以用对偶变量 $\\alpha_i$ 和核函数 $K(\\boldsymbol{x}_i, \\boldsymbol{x}) = \\boldsymbol{x}_i^\\top\\boldsymbol{x}$ 来表示决策函数：\n$$\nf(\\boldsymbol{x}) = \\left( \\sum_{i=1}^N \\alpha_i y_i \\boldsymbol{x}_i \\right)^\\top \\boldsymbol{x} + b = \\sum_{i=1}^N \\alpha_i y_i K(\\boldsymbol{x}_i, \\boldsymbol{x}) + b\n$$\n问题为 $N=2$ 提供了以下训练数据：\n- $\\boldsymbol{x}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$， $y_{1} = +1$\n- $\\boldsymbol{x}_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$， $y_{2} = -1$\n\n以及学习到的对偶系数：\n- $\\alpha_{1} = 1$\n- $\\alpha_{2} = 1$\n\n首先，我们来验证对偶约束 $\\sum_i \\alpha_i y_i = 0$：\n$$\n\\sum_{i=1}^2 \\alpha_i y_i = \\alpha_1 y_1 + \\alpha_2 y_2 = (1)(+1) + (1)(-1) = 1 - 1 = 0\n$$\n约束条件得到满足，证实了所提供的对偶系数的一致性。\n\n接下来，我们计算权重向量 $\\boldsymbol{w}$：\n$$\n\\boldsymbol{w} = \\sum_{i=1}^2 \\alpha_i y_i \\boldsymbol{x}_i = \\alpha_1 y_1 \\boldsymbol{x}_1 + \\alpha_2 y_2 \\boldsymbol{x}_2 = (1)(+1)\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + (1)(-1)\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n为了确定偏移量 $b$，我们使用Karush-Kuhn-Tucker（KKT）条件。相关的互补松弛条件是：\n1. $\\alpha_i [y_i(\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) - 1 + \\xi_i] = 0$\n2. $\\mu_i \\xi_i = 0$\n\n从平稳性条件 $C = \\alpha_i + \\mu_i$，我们可以写出 $\\mu_i = C - \\alpha_i$。问题陈述中提到，对于两个支持向量（$i=1, 2$），都有 $0  \\alpha_i  C$。\n由于 $\\alpha_i  C$，因此 $\\mu_i = C - \\alpha_i > 0$。\n根据第二个KKT条件 $\\mu_i \\xi_i = 0$，因为 $\\mu_i > 0$，所以对于 $i=1$ 和 $i=2$ 都必须有 $\\xi_i = 0$。这意味着两个数据点都被正确分类，并且正好位于它们各自的间隔边界上。\n\n现在，我们使用第一个KKT条件。由于 $\\alpha_i > 0$ 且我们已得出 $\\xi_i = 0$，该条件简化为：\n$$\ny_i(\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) - 1 = 0\n$$\n对于任何满足 $0  \\alpha_i  C$ 的支持向量，此式都必须成立。我们可以使用任一数据点来求解 $b$。使用第一个数据点（$i=1$）：\n$$\ny_1(\\boldsymbol{w}^\\top\\boldsymbol{x}_1 + b) = 1\n$$\n$$\n(+1)\\left(\\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + b\\right) = 1\n$$\n$$\n((1)(1) + (-1)(0)) + b = 1\n$$\n$$\n1 + b = 1 \\implies b = 0\n$$\n作为检验，我们可以使用第二个数据点（$i=2$）来验证：\n$$\ny_2(\\boldsymbol{w}^\\top\\boldsymbol{x}_2 + b) = 1\n$$\n$$\n(-1)\\left(\\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + b\\right) = 1\n$$\n$$\n(-1)((1)(0) + (-1)(1) + b) = 1\n$$\n$$\n(-1)(-1 + b) = 1 \\implies 1 - b = 1 \\implies b = 0\n$$\n$b$ 的值是一致的。\n\n最终的决策函数是 $f(\\boldsymbol{x}) = \\boldsymbol{w}^\\top\\boldsymbol{x} + b = \\begin{pmatrix} 1  -1 \\end{pmatrix}\\boldsymbol{x} + 0$。\n如果我们记 $\\boldsymbol{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$，那么 $f(\\boldsymbol{x}) = x_1 - x_2$。\n\n我们被要求对新的特征向量 $\\boldsymbol{x}^{\\ast} = \\begin{pmatrix} \\tfrac{3}{2} \\\\ \\tfrac{5}{6} \\end{pmatrix}$ 评估此函数的值。\n$$\nf(\\boldsymbol{x}^{\\ast}) = \\left(\\boldsymbol{x}^{\\ast}\\right)_1 - \\left(\\boldsymbol{x}^{\\ast}\\right)_2 = \\frac{3}{2} - \\frac{5}{6}\n$$\n为了进行分数减法，我们找到一个公分母，即 $6$：\n$$\nf(\\boldsymbol{x}^{\\ast}) = \\frac{3 \\times 3}{2 \\times 3} - \\frac{5}{6} = \\frac{9}{6} - \\frac{5}{6} = \\frac{9 - 5}{6} = \\frac{4}{6}\n$$\n将分数化为最简形式：\n$$\nf(\\boldsymbol{x}^{\\ast}) = \\frac{2}{3}\n$$",
            "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$"
        },
        {
            "introduction": "在神经解码中，得到一个高精度的分类器只是第一步，更关键的问题是：分类器的权重向量 $\\mathbf{w}$ 在神经科学上意味着什么？这个练习旨在解决一个常见的误区，即直接将权重解释为神经编码的“调谐偏好”。你将从第一性原理出发，推导并实现一个转换，该转换能从解码器的权重（“滤波器”）中恢复出与刺激编码更直接相关的神经活动“模式”（“pattern”），这对于在存在噪声相关性的情况下做出正确的科学推断至关重要。",
            "id": "4197108",
            "problem": "给定一个神经科学数据分析中的解码场景，其中使用支持向量机 (SVM) 对神经元群体的响应进行二元刺激分类。核心问题是如何解释线性SVM的权重向量与神经活动中刺激的基础编码之间的关系。请从第一性原理出发，对该问题进行数学建模，然后实现由此得出的计算。\n\n考虑一个随机标量刺激变量 $s \\in \\mathbb{R}$ 和一个神经元群体响应向量 $\\mathbf{x} \\in \\mathbb{R}^d$。假设神经响应遵循一个带有加性噪声的线性编码模型，\n$$\n\\mathbf{x} = \\boldsymbol{\\beta} s + \\boldsymbol{\\varepsilon},\n$$\n其中 $\\boldsymbol{\\beta} \\in \\mathbb{R}^d$ 是未知的编码向量，$\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_x)$ 是零均值高斯噪声，其协方差矩阵 $\\boldsymbol{\\Sigma}_x \\in \\mathbb{R}^{d \\times d}$ 是对称正定的。假设通过对刺激进行阈值化生成一个平衡的二元标签 $y \\in \\{-1, +1\\}$，$y = \\operatorname{sign}(s)$。\n\n线性解码器 $f$ 使用决策函数进行分类\n$$\nf(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b),\n$$\n其中 $\\mathbf{w} \\in \\mathbb{R}^d$ 和 $b \\in \\mathbb{R}$ 是通过对独立同分布样本 $(\\mathbf{x}_i, y_i)$ 最小化带 $\\ell_2$ 正则化的经验铰链损失来学习的：\n$$\n\\min_{\\mathbf{w}, b} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\max\\left(0, 1 - y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right) + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2\n$$\n其中 $n \\in \\mathbb{N}$ 是样本数量，$\\lambda  0$ 是正则化强度。在大样本极限下，根据上述生成模型，并假设各类别的噪声协方差相等，最优分离法向量的方向同时反映了刺激编码和神经响应的噪声协方差结构。\n\n从高斯模型和线性决策边界的核心定义和经过充分检验的事实出发，您的任务是：\n\n- 使用上述假设，并将类条件分布视为具有相等协方差的高斯分布，推导最优线性决策边界法向量的方向，用编码向量 $\\boldsymbol{\\beta}$ 和噪声协方差 $\\boldsymbol{\\Sigma}_x$ 来表示。\n\n- 基于此推导，定义一个可解释性变换，将线性解码器的权重向量 $\\mathbf{w}$ 映射到与 $\\boldsymbol{\\beta}$ 处于同一特征空间中的编码相关模式向量 $\\mathbf{a}$，使得 $\\mathbf{a}$ 的方向能反映模型下的真实编码方向。\n\n- 实现一个程序，对于给定的几个测试协方差矩阵 $\\boldsymbol{\\Sigma}_x$ 和一个固定的编码向量 $\\boldsymbol{\\beta}$，计算：\n    1. 解码器权重方向与编码方向之间的余弦相似度。\n    2. 经可解释性变换后的模式方向与编码方向之间的余弦相似度。\n\n非零向量 $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^d$ 之间的余弦相似度定义为\n$$\n\\operatorname{cos\\_sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{v} \\rVert_2}.\n$$\n\n您的实现必须遵循从推导中获得的方向和变换，除了指定的环境外，不使用外部数据集或库。所有计算都纯粹是数学上的，并且不得假设任何物理单位。\n\n测试套件：\n- 使用维度 $d = 4$ 和编码向量\n$$\n\\boldsymbol{\\beta} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\\\ -1 \\end{bmatrix}.\n$$\n- 评估以下四个协方差矩阵 $\\boldsymbol{\\Sigma}_x$：\n    1. 单位矩阵 (理想情况)：\n    $$\n    \\boldsymbol{\\Sigma}_x^{(1)} = \\begin{bmatrix}\n    1  0  0  0 \\\\\n    0  1  0  0 \\\\\n    0  0  1  0 \\\\\n    0  0  0  1\n    \\end{bmatrix}.\n    $$\n    2. 对角异方差 (特征独立但方差不等)：\n    $$\n    \\boldsymbol{\\Sigma}_x^{(2)} = \\operatorname{diag}(0.5, 1.0, 2.0, 4.0).\n    $$\n    3. 块相关 (中等相关性)：\n    $$\n    \\boldsymbol{\\Sigma}_x^{(3)} = \\begin{bmatrix}\n    1.0  0.7  0.0  0.0 \\\\\n    0.7  1.0  0.0  0.0 \\\\\n    0.0  0.0  1.5  0.8 \\\\\n    0.0  0.0  0.8  2.0\n    \\end{bmatrix}.\n    $$\n    4. 强相关 (近退化块，边缘情况)：\n    $$\n    \\boldsymbol{\\Sigma}_x^{(4)} = \\begin{bmatrix}\n    1.0  0.99  0.0  0.0 \\\\\n    0.99  1.0  0.0  0.0 \\\\\n    0.0  0.0  1.0  0.99 \\\\\n    0.0  0.0  0.99  1.0\n    \\end{bmatrix}.\n    $$\n\n对于每个 $\\boldsymbol{\\Sigma}_x^{(k)}$，$k \\in \\{1, 2, 3, 4\\}$，计算：\n- 解码器权重方向与 $\\boldsymbol{\\beta}$ 之间的余弦相似度。\n- 经可解释性变换后的模式方向与 $\\boldsymbol{\\beta}$ 之间的余弦相似度。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，数值四舍五入到六位小数，顺序如下：\n$$\n[\\text{raw}^{(1)}, \\text{raw}^{(2)}, \\text{raw}^{(3)}, \\text{raw}^{(4)}, \\text{pattern}^{(1)}, \\text{pattern}^{(2)}, \\text{pattern}^{(3)}, \\text{pattern}^{(4)}],\n$$\n其中 $\\text{raw}^{(k)}$ 是 $\\boldsymbol{\\Sigma}_x^{(k)}$ 的原始解码器与编码的余弦相似度，$\\text{pattern}^{(k)}$ 是 $\\boldsymbol{\\Sigma}_x^{(k)}$ 的模式与编码的余弦相似度。",
            "solution": "该问题要求推导线性解码器权重与底层神经编码模型之间的关系，并基于此推导实现一个计算。验证和解决步骤如下。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- **神经编码模型**：$\\mathbf{x} = \\boldsymbol{\\beta} s + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{x} \\in \\mathbb{R}^d$ 是神经响应， $s \\in \\mathbb{R}$ 是一个标量刺激，$\\boldsymbol{\\beta} \\in \\mathbb{R}^d$ 是编码向量。\n- **噪声模型**：$\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_x)$，其中 $\\boldsymbol{\\Sigma}_x \\in \\mathbb{R}^{d \\times d}$ 是一个对称正定噪声协方差矩阵。\n- **标签生成**：$y = \\operatorname{sign}(s)$，其中 $y \\in \\{-1, +1\\}$。类别是平衡的。\n- **线性解码器**：$f(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)$。\n- **SVM优化**：参数 $\\mathbf{w}$ 和 $b$ 是通过最小化正则化铰链损失获得的：\n$$\n\\min_{\\mathbf{w}, b} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\max\\left(0, 1 - y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right) + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2\n$$\n- **假设**：推导在大样本极限 ($n \\to \\infty$) 下进行。类条件分布被视为具有相等协方差的高斯分布。\n- **任务**：\n    1.  推导最优线性决策边界法向量的方向，用 $\\boldsymbol{\\beta}$ 和 $\\boldsymbol{\\Sigma}_x$ 表示。\n    2.  定义一个可解释性变换，将解码器权重向量 $\\mathbf{w}$ 映射到一个编码相关的模式向量 $\\mathbf{a}$。\n    3.  实现一个程序，为给定的测试用例计算余弦相似度。\n- **余弦相似度**：$\\operatorname{cos\\_sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{v} \\rVert_2}$。\n- **测试套件**：\n    - 维度：$d = 4$。\n    - 编码向量：$\\boldsymbol{\\beta} = [1, 2, 0, -1]^\\top$。\n    - 四个特定的协方差矩阵：$\\boldsymbol{\\Sigma}_x^{(1)}$ (单位矩阵)，$\\boldsymbol{\\Sigma}_x^{(2)}$ (对角矩阵)，$\\boldsymbol{\\Sigma}_x^{(3)}$ (块相关)，$\\boldsymbol{\\Sigma}_x^{(4)}$ (强相关)。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学上合理**：该问题基于标准的神经编码线性高斯模型以及统计分类原理（Bayes最优分类器、线性判别分析和支持向量机）。这是计算神经科学和机器学习中一个经典且成熟的框架。\n- **定义明确**：问题定义清晰。它要求进行数学推导，然后进行具体计算。假设（大样本极限、具有相等协方差的高斯类）为最优分类器的方向导出了一个唯一的理论结果，使得该问题是良构的。\n- **客观性**：问题以精确的数学术语陈述，没有主观或模糊的语言。\n\n该问题不存在任何无效性缺陷：它在科学上是合理的、可形式化的、完整的、在其模型假设下是现实的，并且结构良好。SVM解与大样本极限下的Bayes最优分类器之间的联系是一个标准结论，为推导提供了坚实的基础。\n\n**步骤3：结论与行动**\n问题是**有效的**。将提供一个完整的、有理有据的解决方案。\n\n### 推导与求解\n\n#### 1. 最优线性决策边界\n\n问题的核心在于找到区分 $y = +1$ 和 $y = -1$ 这两个类别的最优线性决策边界的方向。在指定的生成模型下，我们首先刻画神经响应向量 $\\mathbf{x}$ 的类条件分布。\n\n模型为 $\\mathbf{x} = \\boldsymbol{\\beta} s + \\boldsymbol{\\varepsilon}$，其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_x)$。以刺激 $s$ 为条件的 $\\mathbf{x}$ 的分布是高斯分布：\n$$\np(\\mathbf{x}|s) = \\mathcal{N}(\\boldsymbol{\\beta} s, \\boldsymbol{\\Sigma}_x)\n$$\n类别标签由 $y = \\operatorname{sign}(s)$ 决定。我们需要类条件分布 $p(\\mathbf{x} | y = +1)$ 和 $p(\\mathbf{x} | y = -1)$。我们可以通过在 $s$ 的相应范围内对 $\\mathbf{x}$ 取期望来找到它们的均值。\n令 $\\boldsymbol{\\mu}_+$ 和 $\\boldsymbol{\\mu}_-$ 分别为类别 $y=+1$ 和 $y=-1$ 的均值。\n$$\n\\boldsymbol{\\mu}_+ = \\mathbb{E}[\\mathbf{x} | y = +1] = \\mathbb{E}[\\boldsymbol{\\beta}s + \\boldsymbol{\\varepsilon} | s > 0] = \\boldsymbol{\\beta} \\mathbb{E}[s | s > 0] + \\mathbb{E}[\\boldsymbol{\\varepsilon}]\n$$\n由于 $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$，这简化为 $\\boldsymbol{\\mu}_+ = \\boldsymbol{\\beta} \\mathbb{E}[s | s > 0]$。令 $c_+ = \\mathbb{E}[s | s > 0]$。由于 $s$ 产生平衡的二元标签，其分布必须关于0对称。因此，$c_+ > 0$。\n类似地，对于另一个类别：\n$$\n\\boldsymbol{\\mu}_- = \\mathbb{E}[\\mathbf{x} | y = -1] = \\boldsymbol{\\beta} \\mathbb{E}[s | s  0]\n$$\n根据对称性，$\\mathbb{E}[s | s  0] = -c_+$。让我们定义 $c \\equiv c_+$。那么，类别均值为：\n$$\n\\boldsymbol{\\mu}_+ = c\\boldsymbol{\\beta} \\quad \\text{and} \\quad \\boldsymbol{\\mu}_- = -c\\boldsymbol{\\beta}\n$$\n问题陈述要求将类条件分布视为具有相等协方差的高斯分布。在这种情况下，这是一个常见且合理的简化，其中变异性主要由加性噪声 $\\boldsymbol{\\varepsilon}$ 主导。因此，我们假设公共协方差是噪声的协方差 $\\boldsymbol{\\Sigma}_x$。\n因此，这两个类别被建模为：\n$$\np(\\mathbf{x}|y=+1) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_+, \\boldsymbol{\\Sigma}_x) \\quad \\text{and} \\quad p(\\mathbf{x}|y=-1) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_-, \\boldsymbol{\\Sigma}_x)\n$$\n对于两个具有相等协方差的高斯分布，最优线性决策边界（即Bayes最优边界，也是Fisher线性判别分析找到的解）由一个法向量 $\\mathbf{w}^*$ 定义，其方向由下式给出：\n$$\n\\mathbf{w}^* \\propto \\boldsymbol{\\Sigma}_x^{-1} (\\boldsymbol{\\mu}_+ - \\boldsymbol{\\mu}_-)\n$$\n代入均值的表达式：\n$$\n\\mathbf{w}^* \\propto \\boldsymbol{\\Sigma}_x^{-1} (c\\boldsymbol{\\beta} - (-c\\boldsymbol{\\beta})) = \\boldsymbol{\\Sigma}_x^{-1} (2c\\boldsymbol{\\beta})\n$$\n由于 $c > 0$，标量倍数 $2c$ 不影响方向。因此，最优法向量的方向是：\n$$\n\\mathbf{w}^* \\propto \\boldsymbol{\\Sigma}_x^{-1} \\boldsymbol{\\beta}\n$$\n在大样本极限下，SVM学习到的权重向量 $\\mathbf{w}$ 在方向上收敛于这个最优分离向量 $\\mathbf{w}^*$。因此，我们可以说SVM解码器权重向量的方向为：\n$$\n\\mathbf{w} \\propto \\boldsymbol{\\Sigma}_x^{-1} \\boldsymbol{\\beta}\n$$\n\n#### 2. 可解释性变换\n\n上述结果表明，如果噪声是相关的或异方差的（即，如果 $\\boldsymbol{\\Sigma}_x$ 不是单位矩阵的倍数），解码器权重向量 $\\mathbf{w}$ 通常与编码向量 $\\boldsymbol{\\beta}$ *不*对齐。权重向量 $\\mathbf{w}$ 被协方差矩阵的逆“白化”了。\n\n为了根据底层编码来解释解码器权重，我们必须逆转这种白化效应。我们寻求一个将 $\\mathbf{w}$ 映射到一个与 $\\boldsymbol{\\beta}$ 对齐的模式向量 $\\mathbf{a}$ 的变换。基于比例关系 $\\mathbf{w} \\propto \\boldsymbol{\\Sigma}_x^{-1} \\boldsymbol{\\beta}$，我们可以求解 $\\boldsymbol{\\beta}$ 的方向：\n$$\n\\boldsymbol{\\Sigma}_x \\mathbf{w} \\propto \\boldsymbol{\\Sigma}_x (\\boldsymbol{\\Sigma}_x^{-1} \\boldsymbol{\\beta}) = \\mathbf{I} \\boldsymbol{\\beta} = \\boldsymbol{\\beta}\n$$\n这定义了可解释性变换。编码相关模式向量 $\\mathbf{a}$ 通过以下方式获得：\n$$\n\\mathbf{a} = \\boldsymbol{\\Sigma}_x \\mathbf{w}\n$$\n所得向量 $\\mathbf{a}$ 与真实编码向量 $\\boldsymbol{\\beta}$ 具有相同的方向。这个向量 $\\mathbf{a}$ 通常被称为与“滤波器” $\\mathbf{w}$ 对应的“模式”。\n\n#### 3. 为测试套件进行计算\n\n我们现在将这些结果应用于提供的测试用例。对于每个协方差矩阵 $\\boldsymbol{\\Sigma}_x^{(k)}$，我们执行以下计算：\n1.  定义一个表示解码器权重方向的向量：$\\mathbf{w}_{dir}^{(k)} = (\\boldsymbol{\\Sigma}_x^{(k)})^{-1} \\boldsymbol{\\beta}$。\n2.  计算“原始”余弦相似度：$\\operatorname{cos\\_sim}(\\mathbf{w}_{dir}^{(k)}, \\boldsymbol{\\beta})$。\n3.  定义经可解释性变换的模式向量：$\\mathbf{a}_{dir}^{(k)} = \\boldsymbol{\\Sigma}_x^{(k)} \\mathbf{w}_{dir}^{(k)}$。根据我们的推导，$\\mathbf{a}_{dir}^{(k)} = \\boldsymbol{\\Sigma}_x^{(k)} ((\\boldsymbol{\\Sigma}_x^{(k)})^{-1} \\boldsymbol{\\beta}) = \\boldsymbol{\\beta}$。\n4.  计算“模式”余弦相似度：$\\operatorname{cos\\_sim}(\\mathbf{a}_{dir}^{(k)}, \\boldsymbol{\\beta}) = \\operatorname{cos\\_sim}(\\boldsymbol{\\beta}, \\boldsymbol{\\beta}) = 1$。\n\n程序将为四个给定的协方差矩阵中的每一个实现这些步骤。$\\mathbf{w}_{dir}^{(k)}$ 的计算将使用线性求解器以保证数值稳定性，即求解线性系统 $\\boldsymbol{\\Sigma}_x^{(k)} \\mathbf{w}_{dir}^{(k)} = \\boldsymbol{\\beta}$ 来得到 $\\mathbf{w}_{dir}^{(k)}$。\n\n- **案例1**：$\\boldsymbol{\\Sigma}_x^{(1)} = \\mathbf{I}$。此处，$\\mathbf{w}_{dir}^{(1)} = \\mathbf{I}^{-1} \\boldsymbol{\\beta} = \\boldsymbol{\\beta}$。余弦相似度将为 $1$。\n- **案例2**：$\\boldsymbol{\\Sigma}_x^{(2)} = \\operatorname{diag}(0.5, 1.0, 2.0, 4.0)$。$\\mathbf{w}_{dir}^{(2)}$ 将不同于 $\\boldsymbol{\\beta}$，余弦相似度将小于 $1$。\n- **案例3和4**：由于 $\\boldsymbol{\\Sigma}_x$ 中存在非对角元素，$\\mathbf{w}_{dir}$ 和 $\\boldsymbol{\\beta}$ 方向之间的差异预计会更加显著，从而导致更低的余弦相似度。强相关的情况是一个极端的例子，其中方向可能非常不同。\n\n在所有情况下，变换后的模式 $\\mathbf{a}$ 都将恢复 $\\boldsymbol{\\beta}$ 的方向，从而得到1.0的余弦相似度（在浮点精度范围内）。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and computes the relationship between SVM weights and neural encoding.\n    \"\"\"\n\n    # Define the fixed encoding vector beta for d=4\n    beta = np.array([1.0, 2.0, 0.0, -1.0])\n\n    # Define the test suite of covariance matrices\n    sigma_1 = np.array([\n        [1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0]\n    ])\n\n    sigma_2 = np.diag([0.5, 1.0, 2.0, 4.0])\n\n    sigma_3 = np.array([\n        [1.0, 0.7, 0.0, 0.0],\n        [0.7, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.5, 0.8],\n        [0.0, 0.0, 0.8, 2.0]\n    ])\n\n    sigma_4 = np.array([\n        [1.0, 0.99, 0.0, 0.0],\n        [0.99, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.99],\n        [0.0, 0.0, 0.99, 1.0]\n    ])\n\n    # List of test case covariance matrices\n    test_cases = [sigma_1, sigma_2, sigma_3, sigma_4]\n\n    raw_similarities = []\n    pattern_similarities = []\n\n    def cosine_similarity(u, v):\n        \"\"\"Computes cosine similarity between two vectors.\"\"\"\n        # Ensure vectors are not zero to avoid division by zero.\n        norm_u = np.linalg.norm(u)\n        norm_v = np.linalg.norm(v)\n        if norm_u == 0 or norm_v == 0:\n            return 0.0\n        return np.dot(u, v) / (norm_u * norm_v)\n\n    for sigma_x in test_cases:\n        # According to the derivation, the optimal decoder weight vector w has the\n        # orientation of Sigma_x^-1 * beta. We find this direction by solving the\n        # linear system Sigma_x * w_dir = beta, which is numerically more stable\n        # than computing the inverse of Sigma_x.\n        w_dir = np.linalg.solve(sigma_x, beta)\n\n        # 1. Compute cosine similarity between the raw decoder weight direction and beta.\n        raw_sim = cosine_similarity(w_dir, beta)\n        raw_similarities.append(raw_sim)\n\n        # 2. Compute the interpretability-transformed pattern vector 'a'.\n        # The transformation is a = Sigma_x * w.\n        # Based on the derivation, a_dir should be proportional to beta.\n        # a_dir = sigma_x @ w_dir = sigma_x @ (sigma_x^-1 @ beta) = beta\n        a_dir = np.dot(sigma_x, w_dir)\n        \n        # Compute cosine similarity between the transformed pattern and beta.\n        # This is expected to be 1.0 up to floating point precision.\n        pattern_sim = cosine_similarity(a_dir, beta)\n        pattern_similarities.append(pattern_sim)\n\n    # Combine results and format to six decimal places\n    all_results = raw_similarities + pattern_similarities\n    formatted_results = [f\"{res:.6f}\" for res in all_results]\n\n    # Print in the required format\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "建立一个在科学上可信的解码模型，其挑战不仅在于算法本身，更在于严谨的验证流程。这个练习模拟了一个真实的神经数据分析场景，迫使你思考如何设计一个能提供无偏泛化性能估计的超参数优化和模型评估流程。你将权衡各种交叉验证策略，以应对诸如会话间差异 (session-level variability)、时间自相关、类别不平衡等复杂数据结构带来的挑战，最终掌握避免信息泄露和选择偏差的最佳实践。",
            "id": "4197096",
            "problem": "考虑一个二元神经解码任务，该任务使用带有径向基函数 (Radial Basis Function, RBF) 核的支持向量机 (Support Vector Machine, SVM) 从胞外尖峰计数中分类刺激身份。数据集包含在 $s=10$ 个不同的记录会话中收集的 $n=400$ 次试验，每个会话包含按时间排序的试验，这些试验表现出会话内的自相关性。每次试验的特征向量是一个 $d=500$ 维的尖峰计数向量，该向量是在 $d$ 个单一单元上聚合而成的。类别标签不平衡，$\\mathbb{P}(Y=1)\\approx 0.35$ 且 $\\mathbb{P}(Y=0)\\approx 0.65$，其中 $Y\\in\\{0,1\\}$。您需要调整 SVM 的超参数：正则化参数 $C0$ 和 RBF 核尺度参数 $\\gamma0$，并且您将通过 $z$-score 标准化进行特征标准化。目标是在会话级分布偏移和会话内时间依赖性的现实约束下，选择超参数并估计性能，同时避免信息泄露和选择偏差。目标性能指标是平衡准确率，定义为 $\\mathrm{BA}=\\frac{1}{2}\\left(\\mathrm{TPR}+\\mathrm{TNR}\\right)$，其中 $\\mathrm{TPR}=\\mathbb{P}(\\hat{Y}=1\\mid Y=1)$ 和 $\\mathrm{TNR}=\\mathbb{P}(\\hat{Y}=0\\mid Y=0)$，$\\hat{Y}$ 是预测标签。\n\n从第一性原理回顾，解码器 $f$ 的泛化误差是对于一个数据生成分布 $\\mathcal{D}$ 和一个损失 $\\ell$ 的期望损失 $\\mathcal{E}(f)=\\mathbb{E}_{(X,Y)\\sim \\mathcal{D}}\\left[\\ell\\big(f(X),Y\\big)\\right]$，并且交叉验证 (Cross-Validation, CV) 的目的是在构造训练集和测试集划分以尊重独立性结构和泛化的分布单元时，近似 $\\mathcal{E}(f)$。超参数选择必须仅使用训练数据进行，以避免乐观偏差。\n\n在给定上述约束的情况下，哪种策略最能为未见过的会话得出一个近似无偏的泛化性能估计，并实现合理的超参数优化？\n\nA. 对所有 $n$ 次试验进行 $k=10$ 的分层 $k$ 折交叉验证；在划分前在会话间随机打乱试验；在交叉验证前使用在所有 $n$ 次试验上计算的全局均值和方差对特征进行一次 $z$-score 标准化；在每个折内，对 $C$ 和 $\\gamma$ 的线性间隔值进行穷举网格搜索以最大化平均准确率；将各折的平均准确率报告为最终估计。\n\nB. 使用在会话级别分组的嵌套交叉验证：外部 $k$ 折交叉验证，其中 $k=s$，每次留出一个会话；在每个外部训练集内，通过使用不破坏连续性的时间块划分来执行尊重会话分组和会话内时间依赖性的内部交叉验证；在内循环中，使用仅从内部训练折估计的统计数据应用 $z$-score 标准化，并通过对数均匀随机搜索来调整 $C$ 和 $\\gamma$，同时优化平均平衡准确率；应用与内部训练折内估计的类别频率成反比的类别权重；将外循环的平均平衡准确率报告为最终估计。\n\nC. 对所有 $n$ 次试验使用留一试验法交叉验证；在训练试验上拟合 $z$-缩放，然后通过贝叶斯优化调整 $C$ 和 $\\gamma$，该优化在每次候选评估后直接查询并最大化外部交叉验证分数；将优化器实现的最大外部交叉验证分数报告为最终估计。\n\nD. 使用在试验级别分层（忽略会话身份）的嵌套交叉验证：外部 $k$ 折交叉验证，其中 $k=10$，内部交叉验证使用分层折；在任何划分之前，将合成少数类过采样技术 (Synthetic Minority Oversampling Technique, SMOTE) 应用于整个数据集以平衡类别；在内循环中，在线性尺度上对 $C$ 和 $\\gamma$ 进行网格搜索以最大化平均平衡准确率；使用来自内部训练折的统计数据对特征进行 $z$-score 标准化；报告外部平均平衡准确率。\n\nE. 随机留出 20% 的试验作为测试集；使用所有 $n$ 次试验计算 $z$-score；在剩余的 80% 数据上，进行 $k=10$ 的分层 $k$ 折交叉验证，通过对数均匀随机搜索选择 $C$ 和 $\\gamma$ 以优化平均平衡准确率；使用选定的超参数在完整的 80% 数据上重新训练，并在 20% 的留出集上报告平衡准确率。\n\n选择唯一的最佳选项。",
            "solution": "该问题要求为在神经数据上使用支持向量机 (SVM) 的二元分类任务确定最合适的验证策略。该策略必须能为未见过的数据提供一个近似无偏的泛化性能估计，并执行合理的超参数优化。核心挑战在于特定的数据结构和依赖关系：数据被分组成不同的记录会话，它们之间可能存在分布偏移，并且每个会话内的试验在时间上是相关的。该数据集也是类别不平衡的。\n\n我将首先基于统计学习和机器学习方法论的第一性原理来剖析这个问题，然后根据这些原理评估每个选项。\n\n### 模型评估和选择的基本原则\n\n1.  **估计泛化误差**：目标是估计在新的、未见过的数据上的性能。问题明确指出希望泛化到“未见过的会话”。这意味着泛化的单元是会话。因此，用于测试最终模型性能的数据必须来自在模型训练或超参数选择过程中完全未被使用过的会话。这决定了外部交叉验证 (CV) 循环必须按会话来隔离数据。留一会话法交叉验证或分组K折交叉验证（其中每个折由整个会话组成）是正确的方法。\n\n2.  **避免信息泄露**：当来自测试集（或验证集）的数据影响了训练过程时，就会发生信息泄露。这会导致一个过于乐观的性能估计偏差。常见的泄露来源包括：\n    *   **预处理**：在将数据集划分为训练折和测试折之前，使用从整个数据集计算出的统计数据（均值、标准差）应用数据转换（如 $z$-score 标准化）。正确的流程是*仅*在给定折的训练数据上计算这些统计数据，然后将相同的转换应用于相应的测试数据。\n    *   **特征选择/重采样**：在划分数据集之前对整个数据集执行特征选择或数据重采样（例如，用于类别不平衡的 SMOTE）。这些操作必须被视为模型训练流程的一部分，并且仅应用于每个折内的训练数据。\n\n3.  **避免选择偏差**：超参数调整本身就是一种学习形式。如果一个人在某个数据集上调整超参数，并报告在该相同数据集上的性能（即使使用交叉验证），性能估计也会向上偏置。因为超参数是经过专门选择以在该数据上表现良好。为了获得泛化性能的无偏估计，需要采用**嵌套交叉验证**方案。\n    *   **外循环**：将数据划分为训练折和测试折以估计性能。此循环中的测试折被搁置一旁。\n    *   **内循环**：*仅*在外循环的训练数据上操作。它用于选择最佳超参数（例如，$C$ 和 $\\gamma$）。\n    *   然后，使用内循环选择的超参数对整个外循环训练集进行模型训练，并在留出的外循环测试集上进行评估。最终性能是所有外部折的测试集分数的平均值。\n\n4.  **尊重数据依赖性**：问题指出存在“会话内自相关性”。标准的 k 折交叉验证会随机打乱数据，从而破坏这种时间结构。当相关的相邻试验被分到训练集和测试集中时，模型的任务会变得人为地容易，导致性能估计被夸大。验证划分必须尊重这种时间顺序，例如，通过在每个会话内使用时间序列交叉验证方案，如分块或前向链式划分。\n\n5.  **处理类别不平衡**：数据集是不平衡的 ($\\mathbb{P}(Y=1)\\approx 0.35$)。在这种情况下，标准准确率是一个很差的指标。指定的指标，平衡准确率 $\\mathrm{BA}=\\frac{1}{2}(\\mathrm{TPR}+\\mathrm{TNR})$，是合适的。这个指标应该用于超参数优化（在内循环中）和最终性能报告（来自外循环）。此外，可以使用类别加权（例如，设置 SVM 的 `class_weight` 参数）来更重地惩罚对少数类的错误分类。这些权重应该*仅*根据训练数据中的类别频率来计算。\n\n6.  **超参数搜索策略**：SVM 超参数 $C$（正则化）和 $\\gamma$（RBF 核尺度）分别控制模型复杂度和决策边界的长度尺度。它们的最优值通常跨越几个数量级。在对数尺度上搜索（例如，通过对数间隔的网格或对数均匀随机搜索）比在线性尺度上搜索要高效和有效得多。\n\n### 对所提供选项的评估\n\n**A. 对所有 $n$ 次试验进行 $k=10$ 的分层 $k$ 折交叉验证；在划分前在会话间随机打乱试验；在交叉验证前使用在所有 $n$ 次试验上计算的全局均值和方差对特征进行一次 $z$-score 标准化；在每个折内，对 $C$ 和 $\\gamma$ 的线性间隔值进行穷举网格搜索以最大化平均准确率；将各折的平均准确率报告为最终估计。**\n\n*   **分析**：这个选项几乎违反了所有原则。\n    1.  它在会话间随机打乱试验，因此无法估计对*未见过会话*的泛化能力（违反原则#1）。\n    2.  它在交叉验证前对整个数据集应用 $z$-score 标准化，导致严重的*信息泄露*（违反原则#2）。\n    3.  它使用非嵌套交叉验证进行调优和报告，导致*选择偏差*（违反原则#3）。\n    4.  它随机打乱试验，忽略了*会话内的时间依赖性*（违反原则#4）。\n    5.  它优化的是标准 `accuracy` 而不是 `balanced accuracy`，这对于不平衡数据集是不合适的（违反原则#5）。\n    6.  它使用 `linearly spaced`（线性间隔）网格，这对于 $C$ 和 $\\gamma$ 来说效率低下（根据原则#6是次优的）。\n*   **结论**：**错误**。\n\n**B. 使用在会话级别分组的嵌套交叉验证：外部 $k$ 折交叉验证，其中 $k=s$，每次留出一个会话；在每个外部训练集内，通过使用不破坏连续性的时间块划分来执行尊重会话分组和会话内时间依赖性的内部交叉验证；在内循环中，使用仅从内部训练折估计的统计数据应用 $z$-score 标准化，并通过对数均匀随机搜索来调整 $C$ 和 $\\gamma$，同时优化平均平衡准确率；应用与内部训练折内估计的类别频率成反比的类别权重；将外循环的平均平衡准确率报告为最终估计。**\n\n*   **分析**：这个选项正确地实现了一个鲁棒的验证策略。\n    1.  它使用嵌套交叉验证来避免选择偏差（遵循原则#3）。\n    2.  外循环使用留一会话法交叉验证 ($k=s=10$)，正确地设置了问题以估计对未见过会话的泛化能力（遵循原则#1）。\n    3.  内循环使用 `time-block splits`（时间块划分），正确处理了会话内的时序自相关性（遵循原则#4）。\n    4.  $Z$-score 标准化和类别权重仅在折内的训练数据上计算，防止了信息泄露（遵循原则#2和#5）。\n    5.  它优化 `balanced accuracy`，这是该问题的正确指标（遵循原则#5）。\n    6.  它使用 `log-uniform random search`（对数均匀随机搜索），这是一种有效的超参数搜索策略（遵循原则#6）。\n    7.  它正确地将外循环的平均平衡准确率报告为最终估计。\n*   **结论**：**正确**。\n\n**C. 对所有 $n$ 次试验使用留一试验法交叉验证；在训练试验上拟合 $z$-缩放，然后通过贝叶斯优化调整 $C$ 和 $\\gamma$，该优化在每次候选评估后直接查询并最大化外部交叉验证分数；将优化器实现的最大外部交叉验证分数报告为最终估计。**\n\n*   **分析**：这个选项在程序上存在缺陷。\n    1.  在时间相关数据上使用留一试验法交叉验证会提供一个极其乐观且高方差的性能估计（违反了原则#4的精神）。它也完全忽略了会话结构（违反原则#1）。\n    2.  它使用“外部”交叉验证分数直接调整超参数。这不是一个嵌套过程；这是对测试分数的直接优化，构成了*选择偏差*（违反原则#3）。\n    3.  它报告的是 `maximum`（最大）交叉验证分数，而不是平均值。这进一步加剧了乐观偏差。交叉验证的目标是估计期望性能（一个平均值），而不是找到单个最佳情况的结果。\n*   **结论**：**错误**。\n\n**D. 使用在试验级别分层（忽略会话身份）的嵌套交叉验证：外部 $k$ 折交叉验证，其中 $k=10$，内部交叉验证使用分层折；在任何划分之前，将合成少数类过采样技术 (Synthetic Minority Oversampling Technique, SMOTE) 应用于整个数据集以平衡类别；在内循环中，在线性尺度上对 $C$ 和 $\\gamma$ 进行网格搜索以最大化平均平衡准确率；使用来自内部训练折的统计数据对特征进行 $z$-score 标准化；报告外部平均平衡准确率。**\n\n*   **分析**：这个方法包含严重错误。\n    1.  在任何划分*之前*将 SMOTE 应用于整个数据集是一种灾难性的信息泄露形式。它使用将成为测试集的数据信息来创建合成样本，使整个过程无效（违反原则#2）。\n    2.  外部交叉验证划分忽略了会话身份，未能估计对未见过会话的泛化能力（违反原则#1）。\n    3.  在线性尺度上搜索 $C$ 和 $\\gamma$ 效率低下（根据原则#6是次优的）。\n*   **结论**：**错误**。\n\n**E. 随机留出 20% 的试验作为测试集；使用所有 $n$ 次试验计算 $z$-score；在剩余的 80% 数据上，进行 $k=10$ 的分层 $k$ 折交叉验证，通过对数均匀随机搜索选择 $C$ 和 $\\gamma$ 以优化平均平衡准确率；使用选定的超参数在完整的 80% 数据上重新训练，并在 20% 的留出集上报告平衡准确率。**\n\n*   **分析**：这描述了一种训练/验证/测试集划分方法，但实现方式不正确。\n    1.  最初的试验 `random 20%`（随机20%）划分忽略了会话结构。测试集将包含来自同样也存在于训练集中的会话的试验，因此它不能估计对*新会话*的泛化能力（违反原则#1）。正确的划分方法应该是留出整个会话（例如，对于 20% 的划分，留出 2 个会话）。\n    2.  在划分前对所有 $n$ 次试验计算 $z$-score 是一个明显的*信息泄露*案例，即从测试集到训练过程的信息泄露（违反原则#2）。\n*   **结论**：**错误**。\n\n基于对第一性原理的严格应用，选项 B 是唯一一个正确解决了问题所有指定约束的选项：对新会话的泛化、使用嵌套交叉验证以避免选择偏差、正确处理时间依赖性、正确应用预处理步骤以防止信息泄露，以及使用适当的指标和超参数搜索策略。",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}