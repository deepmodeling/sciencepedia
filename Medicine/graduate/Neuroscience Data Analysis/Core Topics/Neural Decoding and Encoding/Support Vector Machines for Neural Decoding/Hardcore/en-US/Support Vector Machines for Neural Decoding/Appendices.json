{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how a Support Vector Machine makes its decisions, it is essential to work through its mathematical core. This exercise guides you through the process of constructing the decision function from the ground up, using learned dual coefficients and the Karush-Kuhn-Tucker (KKT) conditions to determine the hyperplane's offset. Mastering this calculation provides a foundational understanding of how support vectors and their weights define the classification boundary. ",
            "id": "4197091",
            "problem": "A laboratory is studying visual stimulus decoding from simultaneously recorded neural populations. From repeated trials, they extract a two-dimensional latent feature for each trial by projecting trial-averaged, z-scored spike counts onto two orthonormal components derived from prior analysis. The goal is to decode binary stimulus identity, labeled by $y \\in \\{-1, +1\\}$, from these features using a Support Vector Machine (SVM).\n\nAssume the following are the only two training trials used for the SVM:\n- Trial $1$: feature vector $\\boldsymbol{x}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ with label $y_{1} = +1$.\n- Trial $2$: feature vector $\\boldsymbol{x}_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ with label $y_{2} = -1$.\n\nThe SVM is trained with a soft-margin penalty constant $C$ such that $0  \\alpha_{i}  C$ holds for the obtained dual coefficients. The training uses the standard linear inner product as kernel, so $K(\\boldsymbol{x}, \\boldsymbol{z}) = \\boldsymbol{x}^{\\top}\\boldsymbol{z}$. After solving the optimization, the learned dual coefficients are $\\alpha_{1} = 1$ and $\\alpha_{2} = 1$.\n\nStarting from the fundamental constrained optimization definition of the soft-margin SVM in the primal variables, derive the dual representation and the decision function. Use the Karush-Kuhn-Tucker (KKT) conditions to determine the offset $b$ implied by the provided training solution. Then, evaluate the decision function $f(\\boldsymbol{x}^{\\ast})$ for a new held-out trial with feature vector\n$$\n\\boldsymbol{x}^{\\ast} = \\begin{pmatrix} \\tfrac{3}{2} \\\\ \\tfrac{5}{6} \\end{pmatrix}.\n$$\nYour final answer must be the single value of $f(\\boldsymbol{x}^{\\ast})$ expressed as a reduced fraction. No rounding is required and no units should be reported.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of support vector machines, well-posed with sufficient and consistent information, and objectively stated. We will proceed with a full derivation and solution.\n\nThe soft-margin Support Vector Machine (SVM) aims to find a hyperplane that separates two classes of data points. The optimization problem in its primal form is defined as follows. Given a set of $N$ training data points $\\{\\boldsymbol{x}_i, y_i\\}_{i=1}^N$, where $\\boldsymbol{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, +1\\}$, the goal is to find the weight vector $\\boldsymbol{w} \\in \\mathbb{R}^d$ and offset $b \\in \\mathbb{R}$ that solve:\n$$\n\\min_{\\boldsymbol{w}, b, \\boldsymbol{\\xi}} \\frac{1}{2}\\boldsymbol{w}^\\top\\boldsymbol{w} + C \\sum_{i=1}^N \\xi_i\n$$\nsubject to the constraints:\n$$\ny_i(\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) \\ge 1 - \\xi_i, \\quad \\text{for } i=1, \\dots, N\n$$\n$$\n\\xi_i \\ge 0, \\quad \\text{for } i=1, \\dots, N\n$$\nHere, $\\xi_i$ are slack variables that allow for misclassifications, and $C  0$ is a regularization parameter that penalizes such errors.\n\nTo derive the dual problem, we introduce Lagrange multipliers $\\alpha_i \\ge 0$ and $\\mu_i \\ge 0$. The Lagrangian is:\n$$\n\\mathcal{L}(\\boldsymbol{w}, b, \\boldsymbol{\\xi}, \\boldsymbol{\\alpha}, \\boldsymbol{\\mu}) = \\frac{1}{2}\\boldsymbol{w}^\\top\\boldsymbol{w} + C \\sum_{i=1}^N \\xi_i - \\sum_{i=1}^N \\alpha_i [y_i(\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) - 1 + \\xi_i] - \\sum_{i=1}^N \\mu_i \\xi_i\n$$\nTo find the dual, we minimize $\\mathcal{L}$ with respect to the primal variables $\\boldsymbol{w}$, $b$, and $\\boldsymbol{\\xi}$. We set the respective partial derivatives to zero (stationarity conditions):\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{w}} = \\boldsymbol{w} - \\sum_{i=1}^N \\alpha_i y_i \\boldsymbol{x}_i = \\boldsymbol{0} \\implies \\boldsymbol{w} = \\sum_{i=1}^N \\alpha_i y_i \\boldsymbol{x}_i\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b} = - \\sum_{i=1}^N \\alpha_i y_i = 0\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0 \\implies C = \\alpha_i + \\mu_i\n$$\nSubstituting these back into the Lagrangian yields the Wolfe dual objective function, which is to be maximized with respect to $\\boldsymbol{\\alpha}$. The decision function for a new point $\\boldsymbol{x}$ is given by $f(\\boldsymbol{x}) = \\boldsymbol{w}^\\top\\boldsymbol{x} + b$. Using the expression for $\\boldsymbol{w}$ from the stationarity conditions, we can write the decision function in terms of the dual variables $\\alpha_i$ and the kernel function $K(\\boldsymbol{x}_i, \\boldsymbol{x}) = \\boldsymbol{x}_i^\\top\\boldsymbol{x}$:\n$$\nf(\\boldsymbol{x}) = \\left( \\sum_{i=1}^N \\alpha_i y_i \\boldsymbol{x}_i \\right)^\\top \\boldsymbol{x} + b = \\sum_{i=1}^N \\alpha_i y_i K(\\boldsymbol{x}_i, \\boldsymbol{x}) + b\n$$\nThe problem provides the following training data for $N=2$:\n- $\\boldsymbol{x}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $y_{1} = +1$\n- $\\boldsymbol{x}_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $y_{2} = -1$\n\nAnd the learned dual coefficients:\n- $\\alpha_{1} = 1$\n- $\\alpha_{2} = 1$\n\nFirst, let's verify the dual constraint $\\sum_i \\alpha_i y_i = 0$:\n$$\n\\sum_{i=1}^2 \\alpha_i y_i = \\alpha_1 y_1 + \\alpha_2 y_2 = (1)(+1) + (1)(-1) = 1 - 1 = 0\n$$\nThe constraint is satisfied, confirming the consistency of the provided dual coefficients.\n\nNext, we compute the weight vector $\\boldsymbol{w}$:\n$$\n\\boldsymbol{w} = \\sum_{i=1}^2 \\alpha_i y_i \\boldsymbol{x}_i = \\alpha_1 y_1 \\boldsymbol{x}_1 + \\alpha_2 y_2 \\boldsymbol{x}_2 = (1)(+1)\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + (1)(-1)\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nTo determine the offset $b$, we use the Karush-Kuhn-Tucker (KKT) conditions. The relevant complementary slackness conditions are:\n1. $\\alpha_i [y_i(\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) - 1 + \\xi_i] = 0$\n2. $\\mu_i \\xi_i = 0$\n\nFrom the stationarity condition $C = \\alpha_i + \\mu_i$, we can write $\\mu_i = C - \\alpha_i$. The problem states that $0  \\alpha_i  C$ for both support vectors ($i=1, 2$).\nSince $\\alpha_i  C$, it follows that $\\mu_i = C - \\alpha_i  0$.\nFrom the second KKT condition, $\\mu_i \\xi_i = 0$, since $\\mu_i  0$, it must be that $\\xi_i = 0$ for both $i=1$ and $i=2$. This means both data points are correctly classified and lie exactly on their respective margins.\n\nNow, we use the first KKT condition. Since $\\alpha_i  0$ and we have found $\\xi_i = 0$, the condition simplifies to:\n$$\ny_i(\\boldsymbol{w}^\\top\\boldsymbol{x}_i + b) - 1 = 0\n$$\nThis must hold for any support vector with $0  \\alpha_i  C$. We can use either data point to solve for $b$. Using the first data point ($i=1$):\n$$\ny_1(\\boldsymbol{w}^\\top\\boldsymbol{x}_1 + b) = 1\n$$\n$$\n(+1)\\left(\\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + b\\right) = 1\n$$\n$$\n((1)(1) + (-1)(0)) + b = 1\n$$\n$$\n1 + b = 1 \\implies b = 0\n$$\nAs a check, we can verify this using the second data point ($i=2$):\n$$\ny_2(\\boldsymbol{w}^\\top\\boldsymbol{x}_2 + b) = 1\n$$\n$$\n(-1)\\left(\\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + b\\right) = 1\n$$\n$$\n(-1)((1)(0) + (-1)(1) + b) = 1\n$$\n$$\n(-1)(-1 + b) = 1 \\implies 1 - b = 1 \\implies b = 0\n$$\nThe value for $b$ is consistent.\n\nThe final decision function is $f(\\boldsymbol{x}) = \\boldsymbol{w}^\\top\\boldsymbol{x} + b = \\begin{pmatrix} 1  -1 \\end{pmatrix}\\boldsymbol{x} + 0$.\nIf we write $\\boldsymbol{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, then $f(\\boldsymbol{x}) = x_1 - x_2$.\n\nWe are asked to evaluate this function for the new feature vector $\\boldsymbol{x}^{\\ast} = \\begin{pmatrix} \\tfrac{3}{2} \\\\ \\tfrac{5}{6} \\end{pmatrix}$.\n$$\nf(\\boldsymbol{x}^{\\ast}) = \\left(\\boldsymbol{x}^{\\ast}\\right)_1 - \\left(\\boldsymbol{x}^{\\ast}\\right)_2 = \\frac{3}{2} - \\frac{5}{6}\n$$\nTo subtract the fractions, we find a common denominator, which is $6$:\n$$\nf(\\boldsymbol{x}^{\\ast}) = \\frac{3 \\times 3}{2 \\times 3} - \\frac{5}{6} = \\frac{9}{6} - \\frac{5}{6} = \\frac{9 - 5}{6} = \\frac{4}{6}\n$$\nReducing the fraction to its simplest form:\n$$\nf(\\boldsymbol{x}^{\\ast}) = \\frac{2}{3}\n$$",
            "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$"
        },
        {
            "introduction": "A common goal in neural decoding is to understand which neurons contribute to encoding information, but interpreting a decoder's weights is not always straightforward. This practice challenges the naive assumption that large weights imply important neurons by demonstrating how noise correlations can distort the weight vector. By deriving and implementing a transformation to disentangle the underlying signal from the noise structure, you will learn a critical technique for recovering the true \"encoding pattern\" from a trained linear decoder. ",
            "id": "4197108",
            "problem": "You are given a decoding scenario in neuroscience data analysis in which a population of neural responses is used to classify a binary stimulus using Support Vector Machines (SVMs). The central question is how to interpret the linear SVM weight vector in relation to the underlying encoding of the stimulus in the neural activity. Formulate this problem mathematically from first principles and then implement the resulting computation.\n\nConsider a random scalar stimulus variable $s \\in \\mathbb{R}$ and a neural population response vector $\\mathbf{x} \\in \\mathbb{R}^d$. Suppose the neural responses follow a linear encoding model with additive noise,\n$$\n\\mathbf{x} = \\boldsymbol{\\beta} s + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\boldsymbol{\\beta} \\in \\mathbb{R}^d$ is the unknown encoding vector and $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_x)$ is zero-mean Gaussian noise with covariance matrix $\\boldsymbol{\\Sigma}_x \\in \\mathbb{R}^{d \\times d}$ that is symmetric positive definite. Assume a balanced binary label $y \\in \\{-1, +1\\}$ is generated from the stimulus by thresholding, $y = \\operatorname{sign}(s)$.\n\nA linear decoder $f$ classifies using the decision function\n$$\nf(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b),\n$$\nwhere $\\mathbf{w} \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are learned by minimizing the empirical hinge loss with $\\ell_2$ regularization over independent and identically distributed samples $(\\mathbf{x}_i, y_i)$:\n$$\n\\min_{\\mathbf{w}, b} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\max\\left(0, 1 - y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right) + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2,\n$$\nwhere $n \\in \\mathbb{N}$ is the number of samples and $\\lambda  0$ is the regularization strength. In the large-sample limit under the above generative model with equal noise covariance across classes, the orientation of the optimal separating normal vector reflects both the stimulus encoding and the noise covariance structure of the neural responses.\n\nYour tasks, starting from the core definitions and well-tested facts about Gaussian models and linear decision boundaries, are:\n\n- Using the assumptions stated above and treating the class-conditional distributions as Gaussian with equal covariance, derive the orientation of the optimal linear decision boundary normal vector in terms of the encoding vector $\\boldsymbol{\\beta}$ and the noise covariance $\\boldsymbol{\\Sigma}_x$.\n\n- Based on this derivation, define an interpretability transformation that maps a linear decoderâ€™s weight vector $\\mathbf{w}$ to an encoding-relevant pattern vector $\\mathbf{a}$ in the same feature space as $\\boldsymbol{\\beta}$, such that the direction of $\\mathbf{a}$ reflects the true encoding direction under the model.\n\n- Implement a program that, for several test covariance matrices $\\boldsymbol{\\Sigma}_x$ and a fixed encoding vector $\\boldsymbol{\\beta}$, computes:\n    1. The cosine similarity between the decoder weight direction and the encoding direction.\n    2. The cosine similarity between the interpretability-transformed pattern direction and the encoding direction.\n\nCosine similarity between nonzero vectors $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^d$ is defined by\n$$\n\\operatorname{cos\\_sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{v} \\rVert_2}.\n$$\n\nYour implementation must follow the orientation and transformation obtained from your derivation, without using external datasets or libraries beyond the specified environment. All computations are purely mathematical and must not assume any physical units.\n\nTest Suite:\n- Use dimension $d = 4$ and the encoding vector\n$$\n\\boldsymbol{\\beta} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\\\ -1 \\end{bmatrix}.\n$$\n- Evaluate the following four covariance matrices $\\boldsymbol{\\Sigma}_x$:\n    1. Identity (happy path):\n    $$\n    \\boldsymbol{\\Sigma}_x^{(1)} = \\begin{bmatrix}\n    1  0  0  0 \\\\\n    0  1  0  0 \\\\\n    0  0  1  0 \\\\\n    0  0  0  1\n    \\end{bmatrix}.\n    $$\n    2. Diagonal heteroscedastic (independent features with unequal variances):\n    $$\n    \\boldsymbol{\\Sigma}_x^{(2)} = \\operatorname{diag}(0.5, 1.0, 2.0, 4.0).\n    $$\n    3. Block-correlated (moderate correlations):\n    $$\n    \\boldsymbol{\\Sigma}_x^{(3)} = \\begin{bmatrix}\n    1.0  0.7  0.0  0.0 \\\\\n    0.7  1.0  0.0  0.0 \\\\\n    0.0  0.0  1.5  0.8 \\\\\n    0.0  0.0  0.8  2.0\n    \\end{bmatrix}.\n    $$\n    4. Strongly correlated (near-degenerate blocks, edge case):\n    $$\n    \\boldsymbol{\\Sigma}_x^{(4)} = \\begin{bmatrix}\n    1.0  0.99  0.0  0.0 \\\\\n    0.99  1.0  0.0  0.0 \\\\\n    0.0  0.0  1.0  0.99 \\\\\n    0.0  0.0  0.99  1.0\n    \\end{bmatrix}.\n    $$\n\nFor each $\\boldsymbol{\\Sigma}_x^{(k)}$, $k \\in \\{1, 2, 3, 4\\}$, compute:\n- The cosine similarity between the decoder weight direction and $\\boldsymbol{\\beta}$.\n- The cosine similarity between the interpretability-transformed pattern direction and $\\boldsymbol{\\beta}$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with values rounded to six decimals, in the order\n$$\n[\\text{raw}^{(1)}, \\text{raw}^{(2)}, \\text{raw}^{(3)}, \\text{raw}^{(4)}, \\text{pattern}^{(1)}, \\text{pattern}^{(2)}, \\text{pattern}^{(3)}, \\text{pattern}^{(4)}],\n$$\nwhere $\\text{raw}^{(k)}$ is the raw decoder-vs-encoding cosine similarity for $\\boldsymbol{\\Sigma}_x^{(k)}$ and $\\text{pattern}^{(k)}$ is the pattern-vs-encoding cosine similarity for $\\boldsymbol{\\Sigma}_x^{(k)}$.",
            "solution": "The problem asks for a derivation of the relationship between a linear decoder's weights and the underlying neural encoding model, and to implement a calculation based on this derivation. The validation and solution steps are presented below.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Neural Encoding Model**: $\\mathbf{x} = \\boldsymbol{\\beta} s + \\boldsymbol{\\varepsilon}$, where $\\mathbf{x} \\in \\mathbb{R}^d$ is the neural response, $s \\in \\mathbb{R}$ is a scalar stimulus, $\\boldsymbol{\\beta} \\in \\mathbb{R}^d$ is the encoding vector.\n- **Noise Model**: $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_x)$, where $\\boldsymbol{\\Sigma}_x \\in \\mathbb{R}^{d \\times d}$ is a symmetric positive definite noise covariance matrix.\n- **Label Generation**: $y = \\operatorname{sign}(s)$, with $y \\in \\{-1, +1\\}$. The classes are balanced.\n- **Linear Decoder**: $f(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)$.\n- **SVM Optimization**: The parameters $\\mathbf{w}$ and $b$ are obtained by minimizing the regularized hinge loss:\n$$\n\\min_{\\mathbf{w}, b} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\max\\left(0, 1 - y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right) + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2\n$$\n- **Assumptions**: The derivation operates in the large-sample limit ($n \\to \\infty$). The class-conditional distributions are treated as Gaussian with equal covariance.\n- **Tasks**:\n    1.  Derive the orientation of the optimal linear decision boundary normal vector in terms of $\\boldsymbol{\\beta}$ and $\\boldsymbol{\\Sigma}_x$.\n    2.  Define an interpretability transformation mapping the decoder weight vector $\\mathbf{w}$ to an encoding-relevant pattern vector $\\mathbf{a}$.\n    3.  Implement a program to compute cosine similarities for given test cases.\n- **Cosine Similarity**: $\\operatorname{cos\\_sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{v} \\rVert_2}$.\n- **Test Suite**:\n    - Dimension: $d = 4$.\n    - Encoding vector: $\\boldsymbol{\\beta} = [1, 2, 0, -1]^\\top$.\n    - Four specific covariance matrices: $\\boldsymbol{\\Sigma}_x^{(1)}$ (identity), $\\boldsymbol{\\Sigma}_x^{(2)}$ (diagonal), $\\boldsymbol{\\Sigma}_x^{(3)}$ (block-correlated), $\\boldsymbol{\\Sigma}_x^{(4)}$ (strongly correlated).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is based on the standard linear-Gaussian model of neural encoding and the principles of statistical classification (Bayes-optimal classifiers, Linear Discriminant Analysis, and Support Vector Machines). This is a classical and well-established framework in computational neuroscience and machine learning.\n- **Well-Posed**: The problem is clearly defined. It asks for a mathematical derivation followed by a concrete computation. The assumptions (large-sample limit, Gaussian classes with equal covariance) lead to a unique theoretical result for the optimal classifier's orientation, making the problem well-posed.\n- **Objective**: The problem is stated in precise mathematical terms, free of subjective or ambiguous language.\n\nThe problem does not exhibit any of the invalidity flaws: it is scientifically sound, formalizable, complete, realistic within its modeling assumptions, and well-structured. The connection between the SVM solution and the Bayes-optimal classifier in the large-sample limit is a standard result, providing a solid foundation for the derivation.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A full, reasoned solution will be provided.\n\n### Derivation and Solution\n\n#### 1. Optimal Linear Decision Boundary\n\nThe core of the problem lies in finding the orientation of the optimal linear decision boundary for distinguishing between the two classes $y = +1$ and $y = -1$. Under the specified generative model, we first characterize the class-conditional distributions of the neural response vector $\\mathbf{x}$.\n\nThe model is $\\mathbf{x} = \\boldsymbol{\\beta} s + \\boldsymbol{\\varepsilon}$, with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_x)$. The distribution of $\\mathbf{x}$ conditioned on the stimulus $s$ is Gaussian:\n$$\np(\\mathbf{x}|s) = \\mathcal{N}(\\boldsymbol{\\beta} s, \\boldsymbol{\\Sigma}_x)\n$$\nThe class label is determined by $y = \\operatorname{sign}(s)$. We need the class-conditional distributions $p(\\mathbf{x} | y = +1)$ and $p(\\mathbf{x} | y = -1)$. We can find their means by taking the expectation of $\\mathbf{x}$ over the corresponding range of $s$.\nLet $\\boldsymbol{\\mu}_+$ and $\\boldsymbol{\\mu}_-$ be the means for class $y=+1$ and $y=-1$, respectively.\n$$\n\\boldsymbol{\\mu}_+ = \\mathbb{E}[\\mathbf{x} | y = +1] = \\mathbb{E}[\\boldsymbol{\\beta}s + \\boldsymbol{\\varepsilon} | s  0] = \\boldsymbol{\\beta} \\mathbb{E}[s | s  0] + \\mathbb{E}[\\boldsymbol{\\varepsilon}]\n$$\nSince $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$, this simplifies to $\\boldsymbol{\\mu}_+ = \\boldsymbol{\\beta} \\mathbb{E}[s | s  0]$. Let $c_+ = \\mathbb{E}[s | s  0]$. Since $s$ gives rise to a balanced binary label, its distribution must be symmetric about $0$. Thus, $c_+  0$.\nSimilarly, for the other class:\n$$\n\\boldsymbol{\\mu}_- = \\mathbb{E}[\\mathbf{x} | y = -1] = \\boldsymbol{\\beta} \\mathbb{E}[s | s  0]\n$$\nBy symmetry, $\\mathbb{E}[s | s  0] = -c_+$. Let us define $c \\equiv c_+$. Then, the class means are:\n$$\n\\boldsymbol{\\mu}_+ = c\\boldsymbol{\\beta} \\quad \\text{and} \\quad \\boldsymbol{\\mu}_- = -c\\boldsymbol{\\beta}\n$$\nThe problem states to treat the class-conditional distributions as Gaussian with equal covariance. This is a common and reasonable simplification in this context, where the variability is dominated by the additive noise $\\boldsymbol{\\varepsilon}$. Thus, we assume the common covariance is $\\boldsymbol{\\Sigma}_x$, the covariance of the noise.\nThe two classes are therefore modeled as:\n$$\np(\\mathbf{x}|y=+1) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_+, \\boldsymbol{\\Sigma}_x) \\quad \\text{and} \\quad p(\\mathbf{x}|y=-1) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_-, \\boldsymbol{\\Sigma}_x)\n$$\nFor two Gaussian distributions with equal covariance, the optimal linear decision boundary (the Bayes-optimal boundary, which is also the solution found by Fisher's Linear Discriminant Analysis) is defined by a normal vector $\\mathbf{w}^*$ whose orientation is given by:\n$$\n\\mathbf{w}^* \\propto \\boldsymbol{\\Sigma}_x^{-1} (\\boldsymbol{\\mu}_+ - \\boldsymbol{\\mu}_-)\n$$\nSubstituting the expressions for the means:\n$$\n\\mathbf{w}^* \\propto \\boldsymbol{\\Sigma}_x^{-1} (c\\boldsymbol{\\beta} - (-c\\boldsymbol{\\beta})) = \\boldsymbol{\\Sigma}_x^{-1} (2c\\boldsymbol{\\beta})\n$$\nSince $c  0$, the scalar multiple $2c$ does not affect the orientation. Therefore, the orientation of the optimal normal vector is:\n$$\n\\mathbf{w}^* \\propto \\boldsymbol{\\Sigma}_x^{-1} \\boldsymbol{\\beta}\n$$\nIn the large-sample limit, the weight vector $\\mathbf{w}$ learned by an SVM converges in orientation to this optimal separating vector $\\mathbf{w}^*$. Hence, we can state that the SVM decoder weight vector has the orientation:\n$$\n\\mathbf{w} \\propto \\boldsymbol{\\Sigma}_x^{-1} \\boldsymbol{\\beta}\n$$\n\n#### 2. Interpretability Transformation\n\nThe result above shows that the decoder weight vector $\\mathbf{w}$ is generally *not* aligned with the encoding vector $\\boldsymbol{\\beta}$ if the noise is correlated or heteroscedastic (i.e., if $\\boldsymbol{\\Sigma}_x$ is not a multiple of the identity matrix). The weight vector $\\mathbf{w}$ is 'whitened' by the inverse covariance matrix.\n\nTo interpret the decoder weights in terms of the underlying encoding, we must reverse this whitening effect. We seek a transformation that maps $\\mathbf{w}$ to a pattern vector $\\mathbf{a}$ that is aligned with $\\boldsymbol{\\beta}$. Based on the proportionality $\\mathbf{w} \\propto \\boldsymbol{\\Sigma}_x^{-1} \\boldsymbol{\\beta}$, we can solve for $\\boldsymbol{\\beta}$'s direction:\n$$\n\\boldsymbol{\\Sigma}_x \\mathbf{w} \\propto \\boldsymbol{\\Sigma}_x (\\boldsymbol{\\Sigma}_x^{-1} \\boldsymbol{\\beta}) = \\mathbf{I} \\boldsymbol{\\beta} = \\boldsymbol{\\beta}\n$$\nThis defines the interpretability transformation. The encoding-relevant pattern vector $\\mathbf{a}$ is obtained by:\n$$\n\\mathbf{a} = \\boldsymbol{\\Sigma}_x \\mathbf{w}\n$$\nThe resulting vector $\\mathbf{a}$ has the same orientation as the true encoding vector $\\boldsymbol{\\beta}$. This vector $\\mathbf{a}$ is often called the \"pattern\" corresponding to the \"filter\" $\\mathbf{w}$.\n\n#### 3. Computation for the Test Suite\n\nWe will now apply these results to the provided test cases. For each covariance matrix $\\boldsymbol{\\Sigma}_x^{(k)}$, we perform the following computations:\n1.  Define a vector representing the direction of the decoder weights: $\\mathbf{w}_{dir}^{(k)} = (\\boldsymbol{\\Sigma}_x^{(k)})^{-1} \\boldsymbol{\\beta}$.\n2.  Compute the \"raw\" cosine similarity: $\\operatorname{cos\\_sim}(\\mathbf{w}_{dir}^{(k)}, \\boldsymbol{\\beta})$.\n3.  Define the interpretability-transformed pattern vector: $\\mathbf{a}_{dir}^{(k)} = \\boldsymbol{\\Sigma}_x^{(k)} \\mathbf{w}_{dir}^{(k)}$. From our derivation, $\\mathbf{a}_{dir}^{(k)} = \\boldsymbol{\\Sigma}_x^{(k)} ((\\boldsymbol{\\Sigma}_x^{(k)})^{-1} \\boldsymbol{\\beta}) = \\boldsymbol{\\beta}$.\n4.  Compute the \"pattern\" cosine similarity: $\\operatorname{cos\\_sim}(\\mathbf{a}_{dir}^{(k)}, \\boldsymbol{\\beta}) = \\operatorname{cos\\_sim}(\\boldsymbol{\\beta}, \\boldsymbol{\\beta}) = 1$.\n\nThe program will implement these steps for each of the four given covariance matrices. The calculation of $\\mathbf{w}_{dir}^{(k)}$ will be done using a linear solver for numerical stability, i.e., solving the system $\\boldsymbol{\\Sigma}_x^{(k)} \\mathbf{w}_{dir}^{(k)} = \\boldsymbol{\\beta}$ for $\\mathbf{w}_{dir}^{(k)}$.\n\n- **Case 1**: $\\boldsymbol{\\Sigma}_x^{(1)} = \\mathbf{I}$. Here, $\\mathbf{w}_{dir}^{(1)} = \\mathbf{I}^{-1} \\boldsymbol{\\beta} = \\boldsymbol{\\beta}$. The cosine similarity will be $1$.\n- **Case 2**: $\\boldsymbol{\\Sigma}_x^{(2)} = \\operatorname{diag}(0.5, 1.0, 2.0, 4.0)$. $\\mathbf{w}_{dir}^{(2)}$ will differ from $\\boldsymbol{\\beta}$, and the cosine similarity will be less than $1$.\n- **Cases 3  4**: With off-diagonal elements in $\\boldsymbol{\\Sigma}_x$, the discrepancy between the directions of $\\mathbf{w}_{dir}$ and $\\boldsymbol{\\beta}$ is expected to be more pronounced, resulting in lower cosine similarities. The strongly correlated case is an extreme example where the directions can be very different.\n\nIn all cases, the transformed pattern $\\mathbf{a}$ will recover the direction of $\\boldsymbol{\\beta}$, yielding a cosine similarity of $1.0$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and computes the relationship between SVM weights and neural encoding.\n    \"\"\"\n\n    # Define the fixed encoding vector beta for d=4\n    beta = np.array([1.0, 2.0, 0.0, -1.0])\n\n    # Define the test suite of covariance matrices\n    sigma_1 = np.array([\n        [1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0]\n    ])\n\n    sigma_2 = np.diag([0.5, 1.0, 2.0, 4.0])\n\n    sigma_3 = np.array([\n        [1.0, 0.7, 0.0, 0.0],\n        [0.7, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.5, 0.8],\n        [0.0, 0.0, 0.8, 2.0]\n    ])\n\n    sigma_4 = np.array([\n        [1.0, 0.99, 0.0, 0.0],\n        [0.99, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.99],\n        [0.0, 0.0, 0.99, 1.0]\n    ])\n\n    # List of test case covariance matrices\n    test_cases = [sigma_1, sigma_2, sigma_3, sigma_4]\n\n    raw_similarities = []\n    pattern_similarities = []\n\n    def cosine_similarity(u, v):\n        \"\"\"Computes cosine similarity between two vectors.\"\"\"\n        # Ensure vectors are not zero to avoid division by zero.\n        norm_u = np.linalg.norm(u)\n        norm_v = np.linalg.norm(v)\n        if norm_u == 0 or norm_v == 0:\n            return 0.0\n        return np.dot(u, v) / (norm_u * norm_v)\n\n    for sigma_x in test_cases:\n        # According to the derivation, the optimal decoder weight vector w has the\n        # orientation of Sigma_x^-1 * beta. We find this direction by solving the\n        # linear system Sigma_x * w_dir = beta, which is numerically more stable\n        # than computing the inverse of Sigma_x.\n        w_dir = np.linalg.solve(sigma_x, beta)\n\n        # 1. Compute cosine similarity between the raw decoder weight direction and beta.\n        raw_sim = cosine_similarity(w_dir, beta)\n        raw_similarities.append(raw_sim)\n\n        # 2. Compute the interpretability-transformed pattern vector 'a'.\n        # The transformation is a = Sigma_x * w.\n        # Based on the derivation, a_dir should be proportional to beta.\n        # a_dir = sigma_x @ w_dir = sigma_x @ (sigma_x^-1 @ beta) = beta\n        a_dir = np.dot(sigma_x, w_dir)\n        \n        # Compute cosine similarity between the transformed pattern and beta.\n        # This is expected to be 1.0 up to floating point precision.\n        pattern_sim = cosine_similarity(a_dir, beta)\n        pattern_similarities.append(pattern_sim)\n\n    # Combine results and format to six decimal places\n    all_results = raw_similarities + pattern_similarities\n    formatted_results = [f\"{res:.6f}\" for res in all_results]\n\n    # Print in the required format\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building a reliable neural decoder for real-world data involves more than just fitting a model; it requires a rigorous validation strategy to ensure the results are trustworthy and generalizable. This problem presents a realistic scenario with common challenges like session-to-session variability, temporal dependencies, and class imbalance. By critically evaluating different cross-validation pipelines, you will develop the crucial skill of designing a methodologically sound procedure that avoids common pitfalls like information leakage and selection bias. ",
            "id": "4197096",
            "problem": "Consider a binary neural decoding task using a Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel to classify stimulus identity from extracellular spike counts. The dataset consists of $n=400$ trials collected across $s=10$ distinct recording sessions, with each session containing temporally ordered trials exhibiting within-session autocorrelation. The feature vector for each trial is a $d=500$ dimensional spike-count vector aggregated over $d$ single units. Class labels are imbalanced with $\\mathbb{P}(Y=1)\\approx 0.35$ and $\\mathbb{P}(Y=0)\\approx 0.65$, where $Y\\in\\{0,1\\}$. You will tune the SVM hyperparameters: the regularization parameter $C0$ and the RBF kernel scale parameter $\\gamma0$, and you will perform feature standardization by $z$-scoring. The objective is to select hyperparameters and estimate performance while avoiding information leakage and selection bias, under the realistic constraints of session-level distributional shifts and within-session temporal dependence. The targeted performance metric is balanced accuracy, defined as $\\mathrm{BA}=\\frac{1}{2}\\left(\\mathrm{TPR}+\\mathrm{TNR}\\right)$, where $\\mathrm{TPR}=\\mathbb{P}(\\hat{Y}=1\\mid Y=1)$ and $\\mathrm{TNR}=\\mathbb{P}(\\hat{Y}=0\\mid Y=0)$, and $\\hat{Y}$ is the predicted label.\n\nFrom first principles, recall that the generalization error of a decoder $f$ is the expected loss $\\mathcal{E}(f)=\\mathbb{E}_{(X,Y)\\sim \\mathcal{D}}\\left[\\ell\\big(f(X),Y\\big)\\right]$ for a data-generating distribution $\\mathcal{D}$ and a loss $\\ell$, and that cross-validation (CV) aims to approximate $\\mathcal{E}(f)$ when training and test splits are constructed to respect the independence structure and the distributional units of generalization. Hyperparameter selection must be performed using only training data to avoid optimistic bias.\n\nWhich strategy most appropriately yields an approximately unbiased estimate of generalization performance for unseen sessions and sound hyperparameter optimization, given the constraints above?\n\nA. Perform stratified $k$-fold CV across all $n$ trials with $k=10$; shuffle trials across sessions before splitting; $z$-score features once using the global mean and variance computed over all $n$ trials prior to CV; within each fold, run an exhaustive grid search over linearly spaced values of $C$ and $\\gamma$ to maximize mean accuracy; report the mean accuracy across folds as the final estimate.\n\nB. Use nested CV with groups at the session level: outer $k$-fold CV with $k=s$, leaving one session out at a time; within each outer training set, perform inner CV that respects session grouping and within-session temporal dependence by using time-block splits that do not break contiguity; in the inner loop, apply $z$-scoring using statistics estimated only from the inner training folds, and tune $C$ and $\\gamma$ via log-uniform random search while optimizing mean balanced accuracy; apply class weights inversely proportional to class frequencies estimated within the inner training folds; report the outer-loop mean balanced accuracy as the final estimate.\n\nC. Use leave-one-trial-out CV across all $n$ trials; fit $z$-scaling on the training trials, and then tune $C$ and $\\gamma$ via Bayesian optimization that directly queries and maximizes the outer CV score after each candidate evaluation; report the maximum outer CV score achieved by the optimizer as the final estimate.\n\nD. Use nested CV with stratification at the trial level (ignoring session identity): outer $k$-fold CV with $k=10$, inner CV with stratified folds; apply the Synthetic Minority Oversampling Technique (SMOTE) to the full dataset before any splitting to balance classes; within the inner loop, grid search $C$ and $\\gamma$ on a linear scale to maximize mean balanced accuracy; $z$-score features using statistics from the inner training folds; report the outer mean balanced accuracy.\n\nE. Hold out a random $20\\%$ of trials as a test set; compute $z$-scores using all $n$ trials; on the remaining $80\\%$, perform stratified $k$-fold CV with $k=10$ to select $C$ and $\\gamma$ via log-uniform random search optimizing mean balanced accuracy; retrain on the full $80\\%$ with the selected hyperparameters and report balanced accuracy on the $20\\%$ hold-out.\n\nSelect the single best option.",
            "solution": "The problem requires identifying the most appropriate validation strategy for a binary classification task using a Support Vector Machine (SVM) on neural data. The strategy must provide an approximately unbiased estimate of generalization performance on unseen data and perform sound hyperparameter optimization. The core challenges are the specific data structure and dependencies: the data is grouped into distinct recording sessions with potential distributional shifts between them, and trials within each session are temporally correlated. The dataset is also class-imbalanced.\n\nI will begin by dissecting the problem based on first principles of statistical learning and machine learning methodology, then evaluate each option against these principles.\n\n### Fundamental Principles for Model Evaluation and Selection\n\n1.  **Estimating Generalization Error**: The goal is to estimate the performance on new, unseen data. The problem explicitly states the desire to generalize to \"unseen sessions\". This implies that the unit of generalization is the session. Therefore, the data used to test the final model's performance must come from sessions that were in no way used during the training or hyperparameter selection process for that model. This dictates that the outer cross-validation (CV) loop must segregate data by session. A Leave-One-Session-Out or Group K-Fold CV, where folds are composed of entire sessions, is the correct approach.\n\n2.  **Avoiding Information Leakage**: Information leakage occurs when data from the test set (or validation set) influences the training process. This leads to an optimistically biased performance estimate. Common sources of leakage include:\n    *   **Preprocessing**: Applying data transformations (like $z$-scoring) using statistics (mean, standard deviation) computed from the entire dataset before splitting it into training and testing folds. The correct procedure is to compute these statistics *only* on the training data of a given fold and then apply the same transformation to the corresponding test data.\n    *   **Feature Selection/Resampling**: Performing feature selection or data resampling (e.g., SMOTE for class imbalance) on the full dataset before splitting. These operations must be treated as part of the model training pipeline and applied only to the training data within each fold.\n\n3.  **Avoiding Selection Bias**: Hyperparameter tuning itself is a form of learning. If one tunes hyperparameters on a dataset and reports performance on the same dataset (even using CV), the performance estimate will be biased upwards. The hyperparameters have been specifically chosen to perform well on that data. To obtain an unbiased estimate of generalization performance, a **nested cross-validation** scheme is required.\n    *   **Outer Loop**: Splits the data into training and test folds to estimate performance. The test folds in this loop are held aside.\n    *   **Inner Loop**: Operates *only* on the training data from the outer loop. It is used to select the best hyperparameters (e.g., $C$ and $\\gamma$).\n    *   The model with the hyperparameters selected by the inner loop is then trained on the entire outer-loop training set and evaluated on the held-out outer-loop test set. The final performance is the average of the scores from the test sets across all outer folds.\n\n4.  **Respecting Data Dependencies**: The problem states there is \"within-session autocorrelation\". Standard k-fold CV, which shuffles data randomly, breaks this temporal structure. When correlated adjacent trials are split into training and testing sets, the model's task becomes artificially easy, leading to inflated performance estimates. The validation splits must respect this temporal order, for instance, by using time-series CV schemes like blocked or forward-chaining splits within each session.\n\n5.  **Handling Class Imbalance**: The dataset is imbalanced ($\\mathbb{P}(Y=1)\\approx 0.35$). Standard accuracy is a poor metric in this case. The specified metric, balanced accuracy, $\\mathrm{BA}=\\frac{1}{2}(\\mathrm{TPR}+\\mathrm{TNR})$, is appropriate. This metric should be used for both hyperparameter optimization (in the inner loop) and final performance reporting (from the outer loop). Additionally, one can use class weighting (e.g., setting the SVM's `class_weight` parameter) to penalize misclassifications of the minority class more heavily. These weights should be calculated based on the class frequencies in the *training data only*.\n\n6.  **Hyperparameter Search Strategy**: The SVM hyperparameters $C$ (regularization) and $\\gamma$ (RBF kernel scale) control model complexity and the length scale of the decision boundary, respectively. Their optimal values often vary over several orders of magnitude. Searching on a logarithmic scale (e.g., via a log-spaced grid or a log-uniform random search) is significantly more efficient and effective than searching on a linear scale.\n\n### Evaluation of Provided Options\n\n**A. Perform stratified $k$-fold CV across all $n$ trials with $k=10$; shuffle trials across sessions before splitting; $z$-score features once using the global mean and variance computed over all $n$ trials prior to CV; within each fold, run an exhaustive grid search over linearly spaced values of $C$ and $\\gamma$ to maximize mean accuracy; report the mean accuracy across folds as the final estimate.**\n\n*   **Analysis**: This option violates nearly every principle.\n    1.  It shuffles trials across sessions, thus failing to estimate generalization to *unseen sessions* (violates Principle #1).\n    2.  It applies $z$-scoring to the entire dataset before CV, causing significant *information leakage* (violates Principle #2).\n    3.  It uses a non-nested CV for both tuning and reporting, leading to *selection bias* (violates Principle #3).\n    4.  It shuffles trials, ignoring the *within-session temporal dependence* (violates Principle #4).\n    5.  It optimizes standard `accuracy` instead of `balanced accuracy`, which is inappropriate for the imbalanced dataset (violates Principle #5).\n    6.  It uses a `linearly spaced` grid, which is inefficient for $C$ and $\\gamma$ (suboptimal according to Principle #6).\n*   **Verdict**: **Incorrect**.\n\n**B. Use nested CV with groups at the session level: outer $k$-fold CV with $k=s$, leaving one session out at a time; within each outer training set, perform inner CV that respects session grouping and within-session temporal dependence by using time-block splits that do not break contiguity; in the inner loop, apply $z$-scoring using statistics estimated only from the inner training folds, and tune $C$ and $\\gamma$ via log-uniform random search while optimizing mean balanced accuracy; apply class weights inversely proportional to class frequencies estimated within the inner training folds; report the outer-loop mean balanced accuracy as the final estimate.**\n\n*   **Analysis**: This option correctly implements a robust validation strategy.\n    1.  It uses nested CV to avoid selection bias (adheres to Principle #3).\n    2.  The outer loop uses leave-one-session-out CV ($k=s=10$), correctly setting up the problem to estimate generalization to unseen sessions (adheres to Principle #1).\n    3.  The inner CV uses `time-block splits`, correctly handling the within-session temporal autocorrelation (adheres to Principle #4).\n    4.  $Z$-scoring and class weights are computed only on the training data within the folds, preventing information leakage (adheres to Principle #2 and #5).\n    5.  It optimizes `balanced accuracy`, the correct metric for the problem (adheres to Principle #5).\n    6.  It uses `log-uniform random search`, an effective strategy for the hyperparameters (adheres to Principle #6).\n    7.  It correctly reports the mean balanced accuracy from the outer loop as the final estimate.\n*   **Verdict**: **Correct**.\n\n**C. Use leave-one-trial-out CV across all $n$ trials; fit $z$-scaling on the training trials, and then tune $C$ and $\\gamma$ via Bayesian optimization that directly queries and maximizes the outer CV score after each candidate evaluation; report the maximum outer CV score achieved by the optimizer as the final estimate.**\n\n*   **Analysis**: This option is procedurally flawed.\n    1.  Leave-one-trial-out CV on temporally correlated data provides an extremely optimistic and high-variance estimate of performance (violates the spirit of Principle #4). It also completely ignores the session structure (violates Principle #1).\n    2.  It uses the \"outer\" CV score to directly tune hyperparameters. This is not a nested procedure; it is a direct optimization of the test score, which constitutes *selection bias* (violates Principle #3).\n    3.  It reports the `maximum` CV score, not the mean. This further exacerbates the optimistic bias. The goal of CV is to estimate expected performance (an average), not to find a single best-case result.\n*   **Verdict**: **Incorrect**.\n\n**D. Use nested CV with stratification at the trial level (ignoring session identity): outer $k$-fold CV with $k=10$, inner CV with stratified folds; apply the Synthetic Minority Oversampling Technique (SMOTE; Synthetic Minority Oversampling Technique) to the full dataset before any splitting to balance classes; within the inner loop, grid search $C$ and $\\gamma$ on a linear scale to maximize mean balanced accuracy; $z$-score features using statistics from the inner training folds; report the outer mean balanced accuracy.**\n\n*   **Analysis**: This approach contains critical errors.\n    1.  Applying SMOTE to the full dataset *before* any splitting is a catastrophic form of information leakage. It creates synthetic samples using information from what will become the test set, invalidating the entire procedure (violates Principle #2).\n    2.  The outer CV splits ignore session identity, failing to estimate generalization to unseen sessions (violates Principle #1).\n    3.  Searching on a `linear scale` is inefficient for $C$ and $\\gamma$ (suboptimal according to Principle #6).\n*   **Verdict**: **Incorrect**.\n\n**E. Hold out a random $20\\%$ of trials as a test set; compute $z$-scores using all $n$ trials; on the remaining $80\\%$, perform stratified $k$-fold CV with $k=10$ to select $C$ and $\\gamma$ via log-uniform random search optimizing mean balanced accuracy; retrain on the full $80\\%$ with the selected hyperparameters and report balanced accuracy on the $20\\%$ hold-out.**\n\n*   **Analysis**: This describes a train/validation/test split methodology, but implements it incorrectly.\n    1.  The initial `random 20%` split of trials ignores the session structure. The test set will contain trials from sessions that are also in the training set, so it does not estimate generalization to *new sessions* (violates Principle #1). A correct split would hold out entire sessions (e.g., $2$ sessions for a $20\\%$ split).\n    2.  Computing $z$-scores on all $n$ trials before splitting is a clear case of *information leakage* from the test set into the training process (violates Principle #2).\n*   **Verdict**: **Incorrect**.\n\nBased on a rigorous application of first principles, option B is the only one that correctly addresses all the specified constraints of the problem: generalization to new sessions, nested cross-validation to avoid selection bias, proper handling of temporal dependencies, correct application of preprocessing steps to prevent information leakage, and use of the appropriate metric and hyperparameter search strategy.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}