{
    "hands_on_practices": [
        {
            "introduction": "The power of kernel methods lies in the \"kernel trick,\" which allows us to compute decision boundaries in a high-dimensional feature space without ever explicitly mapping the data into that space. This first exercise provides a concrete demonstration of this principle . By manually constructing the quadratic feature map for a simple neuronal firing-rate dataset and comparing the result to the direct kernel computation, you will verify the equivalence and gain a foundational, hands-on understanding of how nonlinear classification is achieved.",
            "id": "4172673",
            "problem": "In a study of neuronal population coding, each trial provides two firing-rate features, forming a two-dimensional input $\\mathbf{x} = (x_{1}, x_{2})$ measured in spikes per second. A binary classification problem is posed to distinguish stimulus classes, using a Support Vector Machine (SVM) with a quadratic kernel. Begin from the following core definitions.\n\n- The linear decision function in a feature space defined by a feature map $\\boldsymbol{\\phi}$ is $f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\boldsymbol{\\phi}(\\mathbf{x}) + b$, with $\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\boldsymbol{\\phi}(\\mathbf{x}_{i})$, where $\\alpha_{i} \\ge 0$ are Lagrange multipliers, $y_{i} \\in \\{-1, +1\\}$ are labels, and $b \\in \\mathbb{R}$ is the bias.\n- A positive semidefinite kernel $k(\\mathbf{x}, \\mathbf{z})$ corresponds to an inner product in a possibly higher-dimensional feature space via $k(\\mathbf{x}, \\mathbf{z}) = \\boldsymbol{\\phi}(\\mathbf{x})^{\\top} \\boldsymbol{\\phi}(\\mathbf{z})$.\n- The decision function can be written in the dual as $f(\\mathbf{x}) = \\sum_{i=1}^{n} \\alpha_{i} y_{i} k(\\mathbf{x}_{i}, \\mathbf{x}) + b$.\n\nConsider the inhomogeneous degree-$2$ polynomial kernel $k(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^{\\top}\\mathbf{z} + c)^{2}$ with offset parameter $c = 1$. Construct an explicit quadratic feature map $\\boldsymbol{\\phi} : \\mathbb{R}^{2} \\to \\mathbb{R}^{m}$ such that $k(\\mathbf{x}, \\mathbf{z}) = \\boldsymbol{\\phi}(\\mathbf{x})^{\\top} \\boldsymbol{\\phi}(\\mathbf{z})$ holds for all $\\mathbf{x}, \\mathbf{z} \\in \\mathbb{R}^{2}$.\n\nA hard-margin SVM is trained on the following neuronal firing-rate dataset (each $\\mathbf{x}_{i}$ is a trial with label $y_{i}$):\n\n- Stimulus $+1$: $\\mathbf{x}_{1} = (2, 1)$ with $y_{1} = +1$, and $\\mathbf{x}_{2} = (1, 2)$ with $y_{2} = +1$.\n- Stimulus $-1$: $\\mathbf{x}_{3} = (1, 3)$ with $y_{3} = -1$, and $\\mathbf{x}_{4} = (0, 1)$ with $y_{4} = -1$.\n\nAfter training, only $\\mathbf{x}_{1}$ and $\\mathbf{x}_{3}$ are support vectors with nonzero Lagrange multipliers, specifically $\\alpha_{1} = \\frac{1}{2}$ and $\\alpha_{3} = \\frac{1}{4}$, and the learned bias is $b = 0$.\n\nTasks:\n\n1. Using your constructed $\\boldsymbol{\\phi}$ for the quadratic kernel with $c = 1$, compute $\\mathbf{w} = \\sum_{i \\in \\{1,3\\}} \\alpha_{i} y_{i} \\boldsymbol{\\phi}(\\mathbf{x}_{i})$.\n2. For the test input $\\mathbf{x}^{\\ast} = (2, 0)$, compute the decision function $f(\\mathbf{x}^{\\ast}) = \\mathbf{w}^{\\top}\\boldsymbol{\\phi}(\\mathbf{x}^{\\ast}) + b$ in the mapped space.\n3. Verify the equivalence by computing $f(\\mathbf{x}^{\\ast})$ again using the kernel representation $f(\\mathbf{x}^{\\ast}) = \\sum_{i \\in \\{1,3\\}} \\alpha_{i} y_{i} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}^{\\ast} + 1)^{2} + b$ and confirming that both computations yield the same value.\n\nExpress your final numerical value for $f(\\mathbf{x}^{\\ast})$ exactly (no rounding).",
            "solution": "The problem will be validated before a solution is attempted.\n\n### Step 1: Extract Givens\n- Input space: $\\mathbf{x} = (x_{1}, x_{2}) \\in \\mathbb{R}^{2}$.\n- Classification labels: $y_{i} \\in \\{-1, +1\\}$.\n- Inhomogeneous degree-$2$ polynomial kernel: $k(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^{\\top}\\mathbf{z} + c)^{2}$ with $c = 1$.\n- Feature map definition: $k(\\mathbf{x}, \\mathbf{z}) = \\boldsymbol{\\phi}(\\mathbf{x})^{\\top} \\boldsymbol{\\phi}(\\mathbf{z})$.\n- Primal decision function: $f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\boldsymbol{\\phi}(\\mathbf{x}) + b$.\n- Primal weight vector: $\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\boldsymbol{\\phi}(\\mathbf{x}_{i})$.\n- Dual decision function: $f(\\mathbf{x}) = \\sum_{i=1}^{n} \\alpha_{i} y_{i} k(\\mathbf{x}_{i}, \\mathbf{x}) + b$.\n- Training data:\n  - $\\mathbf{x}_{1} = (2, 1)$, $y_{1} = +1$.\n  - $\\mathbf{x}_{2} = (1, 2)$, $y_{2} = +1$.\n  - $\\mathbf{x}_{3} = (1, 3)$, $y_{3} = -1$.\n  - $\\mathbf{x}_{4} = (0, 1)$, $y_{4} = -1$.\n- SVM training results (hard-margin):\n  - Support vectors are $\\mathbf{x}_{1}$ and $\\mathbf{x}_{3}$.\n  - Lagrange multipliers: $\\alpha_{1} = \\frac{1}{2}$, $\\alpha_{3} = \\frac{1}{4}$, and $\\alpha_{2} = \\alpha_{4} = 0$.\n  - Bias: $b = 0$.\n- Test input: $\\mathbf{x}^{\\ast} = (2, 0)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem is based on the standard mathematical theory of Support Vector Machines and kernel methods, which are established techniques in machine learning and data analysis. The use case of analyzing neuronal firing rates is a common and valid application domain for these methods.\n- **Well-Posed**: The problem is well-posed. It provides all necessary data points, parameters ($\\alpha_i$, $c$, $b$), and definitions to perform the requested calculations. The tasks are specific and lead to a unique, deterministic answer.\n- **Objective**: The problem is stated using precise mathematical language and objective data. There are no subjective or ambiguous terms.\n- **Completeness and Consistency**: The problem is self-contained. All variables and constants are defined. The provided values for the Lagrange multipliers, support vectors, and bias are consistent with a trained SVM model (though we are not asked to verify the training process itself, only to use its results).\n- **No other invalidity flags are triggered.** The problem is a standard exercise in applying the definitions of kernel methods.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\nThe problem requires a three-part calculation: constructing the feature map $\\boldsymbol{\\phi}(\\mathbf{x})$ and computing the weight vector $\\mathbf{w}$; evaluating the decision function $f(\\mathbf{x}^{\\ast})$ in the feature space; and verifying this result using the kernel trick.\n\n**Task 1: Construct $\\boldsymbol{\\phi}(\\mathbf{x})$ and compute $\\mathbf{w}$**\n\nFirst, we find an explicit feature map $\\boldsymbol{\\phi}(\\mathbf{x})$ for the given kernel $k(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^{\\top}\\mathbf{z} + 1)^{2}$.\nLet $\\mathbf{x} = (x_1, x_2)^{\\top}$ and $\\mathbf{z} = (z_1, z_2)^{\\top}$.\nThe inner product is $\\mathbf{x}^{\\top}\\mathbf{z} = x_1 z_1 + x_2 z_2$.\nThe kernel is $k(\\mathbf{x}, \\mathbf{z}) = (x_1 z_1 + x_2 z_2 + 1)^{2}$.\nExpanding this expression:\n$$k(\\mathbf{x}, \\mathbf{z}) = (x_1 z_1)^2 + (x_2 z_2)^2 + 1^2 + 2(x_1 z_1)(x_2 z_2) + 2(x_1 z_1)(1) + 2(x_2 z_2)(1)$$\n$$k(\\mathbf{x}, \\mathbf{z}) = x_1^2 z_1^2 + x_2^2 z_2^2 + 1 + 2 x_1 x_2 z_1 z_2 + 2 x_1 z_1 + 2 x_2 z_2$$\nTo satisfy $k(\\mathbf{x}, \\mathbf{z}) = \\boldsymbol{\\phi}(\\mathbf{x})^{\\top} \\boldsymbol{\\phi}(\\mathbf{z})$, we group the terms by their dependence on $\\mathbf{x}$ and $\\mathbf{z}$:\n$$k(\\mathbf{x}, \\mathbf{z}) = (x_1^2)(z_1^2) + (x_2^2)(z_2^2) + (\\sqrt{2} x_1 x_2)(\\sqrt{2} z_1 z_2) + (\\sqrt{2} x_1)(\\sqrt{2} z_1) + (\\sqrt{2} x_2)(\\sqrt{2} z_2) + (1)(1)$$\nThis corresponds to an inner product in a $6$-dimensional space, $\\mathbb{R}^{6}$. A valid feature map is:\n$$\\boldsymbol{\\phi}(\\mathbf{x}) = \\begin{pmatrix} x_1^2 \\\\ x_2^2 \\\\ \\sqrt{2} x_1 x_2 \\\\ \\sqrt{2} x_1 \\\\ \\sqrt{2} x_2 \\\\ 1 \\end{pmatrix}$$\nNext, we compute the weight vector $\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\boldsymbol{\\phi}(\\mathbf{x}_{i})$. The sum is only over the support vectors, $\\mathbf{x}_1$ and $\\mathbf{x}_3$.\n$$\\mathbf{w} = \\alpha_1 y_1 \\boldsymbol{\\phi}(\\mathbf{x}_1) + \\alpha_3 y_3 \\boldsymbol{\\phi}(\\mathbf{x}_3)$$\nWe map the support vectors to the feature space:\nFor $\\mathbf{x}_1 = (2, 1)$:\n$$\\boldsymbol{\\phi}(\\mathbf{x}_1) = \\begin{pmatrix} 2^2 \\\\ 1^2 \\\\ \\sqrt{2}(2)(1) \\\\ \\sqrt{2}(2) \\\\ \\sqrt{2}(1) \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\\\ 2\\sqrt{2} \\\\ 2\\sqrt{2} \\\\ \\sqrt{2} \\\\ 1 \\end{pmatrix}$$\nFor $\\mathbf{x}_3 = (1, 3)$:\n$$\\boldsymbol{\\phi}(\\mathbf{x}_3) = \\begin{pmatrix} 1^2 \\\\ 3^2 \\\\ \\sqrt{2}(1)(3) \\\\ \\sqrt{2}(1) \\\\ \\sqrt{2}(3) \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 9 \\\\ 3\\sqrt{2} \\\\ \\sqrt{2} \\\\ 3\\sqrt{2} \\\\ 1 \\end{pmatrix}$$\nSubstituting the values $\\alpha_1=\\frac{1}{2}$, $y_1=+1$, $\\alpha_3=\\frac{1}{4}$, and $y_3=-1$:\n$$\\mathbf{w} = \\frac{1}{2}(+1)\\begin{pmatrix} 4 \\\\ 1 \\\\ 2\\sqrt{2} \\\\ 2\\sqrt{2} \\\\ \\sqrt{2} \\\\ 1 \\end{pmatrix} + \\frac{1}{4}(-1)\\begin{pmatrix} 1 \\\\ 9 \\\\ 3\\sqrt{2} \\\\ \\sqrt{2} \\\\ 3\\sqrt{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1/2 \\\\ \\sqrt{2} \\\\ \\sqrt{2} \\\\ \\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix} - \\begin{pmatrix} 1/4 \\\\ 9/4 \\\\ 3\\sqrt{2}/4 \\\\ \\sqrt{2}/4 \\\\ 3\\sqrt{2}/4 \\\\ 1/4 \\end{pmatrix}$$\n$$\\mathbf{w} = \\begin{pmatrix} 2 - 1/4 \\\\ 1/2 - 9/4 \\\\ \\sqrt{2} - 3\\sqrt{2}/4 \\\\ \\sqrt{2} - \\sqrt{2}/4 \\\\ \\sqrt{2}/2 - 3\\sqrt{2}/4 \\\\ 1/2 - 1/4 \\end{pmatrix} = \\begin{pmatrix} 7/4 \\\\ -7/4 \\\\ \\sqrt{2}/4 \\\\ 3\\sqrt{2}/4 \\\\ -\\sqrt{2}/4 \\\\ 1/4 \\end{pmatrix}$$\n\n**Task 2: Compute $f(\\mathbf{x}^{\\ast})$ in the mapped space**\n\nWe compute the decision function value for the test input $\\mathbf{x}^{\\ast} = (2, 0)$ using $f(\\mathbf{x}^{\\ast}) = \\mathbf{w}^{\\top}\\boldsymbol{\\phi}(\\mathbf{x}^{\\ast}) + b$. The bias $b$ is given as $0$.\nFirst, map $\\mathbf{x}^{\\ast}$ to the feature space:\n$$\\boldsymbol{\\phi}(\\mathbf{x}^{\\ast}) = \\boldsymbol{\\phi}((2, 0)) = \\begin{pmatrix} 2^2 \\\\ 0^2 \\\\ \\sqrt{2}(2)(0) \\\\ \\sqrt{2}(2) \\\\ \\sqrt{2}(0) \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\\\ 2\\sqrt{2} \\\\ 0 \\\\ 1 \\end{pmatrix}$$\nNow, compute the inner product $\\mathbf{w}^{\\top}\\boldsymbol{\\phi}(\\mathbf{x}^{\\ast})$:\n$$f(\\mathbf{x}^{\\ast}) = \\begin{pmatrix} 7/4 & -7/4 & \\sqrt{2}/4 & 3\\sqrt{2}/4 & -\\sqrt{2}/4 & 1/4 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\\\ 2\\sqrt{2} \\\\ 0 \\\\ 1 \\end{pmatrix} + 0$$\n$$f(\\mathbf{x}^{\\ast}) = (\\frac{7}{4})(4) + (-\\frac{7}{4})(0) + (\\frac{\\sqrt{2}}{4})(0) + (\\frac{3\\sqrt{2}}{4})(2\\sqrt{2}) + (-\\frac{\\sqrt{2}}{4})(0) + (\\frac{1}{4})(1)$$\n$$f(\\mathbf{x}^{\\ast}) = 7 + 0 + 0 + \\frac{3 \\cdot 2 \\cdot (\\sqrt{2})^2}{4} + 0 + \\frac{1}{4}$$\n$$f(\\mathbf{x}^{\\ast}) = 7 + \\frac{12}{4} + \\frac{1}{4} = 7 + 3 + \\frac{1}{4} = 10 + \\frac{1}{4} = \\frac{41}{4}$$\n\n**Task 3: Verify using the kernel representation**\n\nWe now re-compute $f(\\mathbf{x}^{\\ast})$ using the dual form, $f(\\mathbf{x}^{\\ast}) = \\sum_{i \\in \\{1,3\\}} \\alpha_{i} y_{i} k(\\mathbf{x}_{i}, \\mathbf{x}^{\\ast}) + b$:\n$$f(\\mathbf{x}^{\\ast}) = \\alpha_1 y_1 k(\\mathbf{x}_1, \\mathbf{x}^{\\ast}) + \\alpha_3 y_3 k(\\mathbf{x}_3, \\mathbf{x}^{\\ast}) + b$$\nWe need to calculate the kernel values $k(\\mathbf{x}_1, \\mathbf{x}^{\\ast})$ and $k(\\mathbf{x}_3, \\mathbf{x}^{\\ast})$.\nFor $\\mathbf{x}_1 = (2, 1)$ and $\\mathbf{x}^{\\ast} = (2, 0)$:\n$$\\mathbf{x}_1^{\\top}\\mathbf{x}^{\\ast} = (2)(2) + (1)(0) = 4$$\n$$k(\\mathbf{x}_1, \\mathbf{x}^{\\ast}) = (\\mathbf{x}_1^{\\top}\\mathbf{x}^{\\ast} + 1)^2 = (4 + 1)^2 = 5^2 = 25$$\nFor $\\mathbf{x}_3 = (1, 3)$ and $\\mathbf{x}^{\\ast} = (2, 0)$:\n$$\\mathbf{x}_3^{\\top}\\mathbf{x}^{\\ast} = (1)(2) + (3)(0) = 2$$\n$$k(\\mathbf{x}_3, \\mathbf{x}^{\\ast}) = (\\mathbf{x}_3^{\\top}\\mathbf{x}^{\\ast} + 1)^2 = (2 + 1)^2 = 3^2 = 9$$\nNow substitute these values into the decision function:\n$$f(\\mathbf{x}^{\\ast}) = (\\frac{1}{2})(+1)(25) + (\\frac{1}{4})(-1)(9) + 0$$\n$$f(\\mathbf{x}^{\\ast}) = \\frac{25}{2} - \\frac{9}{4}$$\nTo combine these terms, we use a common denominator of $4$:\n$$f(\\mathbf{x}^{\\ast}) = \\frac{50}{4} - \\frac{9}{4} = \\frac{41}{4}$$\nThe result from the primal form calculation ($\\frac{41}{4}$) matches the result from the dual (kernel) form calculation ($\\frac{41}{4}$), which verifies the equivalence as required. The final value is $\\frac{41}{4}$.",
            "answer": "$$\n\\boxed{\\frac{41}{4}}\n$$"
        },
        {
            "introduction": "A Support Vector Machine is fundamentally a geometric classifier, defined by its margin-maximizing decision boundary. This practice focuses on quantifying that geometry, which is crucial for interpreting the model's predictions . Although presented with a linear kernel for clarity, the principles of calculating the geometric margin and the distance of new data points to the hyperplane are identical in the high-dimensional feature spaces induced by more complex kernels. This exercise builds intuition for how the distance from the decision boundary serves as a measure of classification confidence.",
            "id": "4172666",
            "problem": "A neuroscience laboratory is decoding motor imagery from electroencephalography (EEG) using a Support Vector Machine (SVM) with a linear kernel, trained on standardized features derived from source-localized band-limited power. Let the trained linear decision function in the input feature space be specified by weight vector $\\mathbf{w} = (2,-1,0)^\\top$ and intercept $b = -0.5$. Assume the SVM solution is presented in canonical scaling, meaning the minimum signed functional margin over the training set equals $1$.\n\nThree held-out standardized EEG feature vectors are given as $\\mathbf{x}^{(1)} = (1,0,0)^\\top$, $\\mathbf{x}^{(2)} = (0,1,0)^\\top$, and $\\mathbf{x}^{(3)} = (0.25,0.5,1)^\\top$. Using only the core geometric definitions of the linear Support Vector Machine and Euclidean projections in the linear kernel setting, derive the geometric margin of the trained hyperplane and the perpendicular distances from each held-out example to the SVM decision boundary. State the sign of the decision function for each example and briefly justify how the distance magnitude relates to the classification confidence under the margin-based interpretation commonly used in kernel methods.\n\nExpress your final numeric answers for the geometric margin and the three distances as exact radicals, not decimals, in a single row vector ordered as $\\big(\\gamma, d^{(1)}, d^{(2)}, d^{(3)}\\big)$. Do not include units. Do not round.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. All necessary information is provided, and the concepts are standard in the field of machine learning and its application to neuroscience data analysis.\n\nThe decision function for a linear Support Vector Machine (SVM) classifies a new data point $\\mathbf{x}$ based on its sign:\n$$f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b$$\nThe decision boundary is the hyperplane defined by the set of points where the decision function is zero: $\\mathbf{w}^\\top \\mathbf{x} + b = 0$.\n\nThe problem provides the weight vector $\\mathbf{w} = (2,-1,0)^\\top$ and the intercept $b = -0.5$. The problem also states that the SVM is formulated in its canonical form, which means the functional margin for the support vectors is equal to $1$. The geometric margin of the hyperplane, denoted $\\gamma$, is the perpendicular distance from the decision boundary to the nearest training examples (the support vectors). In the canonical representation, this geometric margin is given by the reciprocal of the Euclidean norm of the weight vector:\n$$\\gamma = \\frac{1}{\\|\\mathbf{w}\\|}$$\nFirst, we calculate the Euclidean norm of $\\mathbf{w}$:\n$$\\|\\mathbf{w}\\| = \\sqrt{2^2 + (-1)^2 + 0^2} = \\sqrt{4 + 1 + 0} = \\sqrt{5}$$\nTherefore, the geometric margin of the trained hyperplane is:\n$$\\gamma = \\frac{1}{\\sqrt{5}} = \\frac{\\sqrt{5}}{5}$$\n\nNext, we must find the perpendicular distance from each held-out example $\\mathbf{x}^{(i)}$ to the decision boundary. This distance, $d^{(i)}$, is given by the formula:\n$$d^{(i)} = \\frac{|\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b|}{\\|\\mathbf{w}\\|}$$\nThe term in the numerator, $|\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b|$, is the absolute value of the decision function evaluated at the point $\\mathbf{x}^{(i)}$. The sign of the decision function, $\\text{sign}(f(\\mathbf{x}^{(i)}))$, determines the predicted class label ($+1$ or $-1$).\n\nWe will now compute the decision function value and the perpendicular distance for each of the three given feature vectors.\n\nFor the first example, $\\mathbf{x}^{(1)} = (1,0,0)^\\top$:\nThe decision function value is:\n$$f(\\mathbf{x}^{(1)}) = \\mathbf{w}^\\top \\mathbf{x}^{(1)} + b = (2)(1) + (-1)(0) + (0)(0) - 0.5 = 2 - 0.5 = 1.5$$\nThe sign of $f(\\mathbf{x}^{(1)})$ is positive, so this example is classified into the positive class.\nThe perpendicular distance from $\\mathbf{x}^{(1)}$ to the decision boundary is:\n$$d^{(1)} = \\frac{|f(\\mathbf{x}^{(1)})|}{\\|\\mathbf{w}\\|} = \\frac{|1.5|}{\\sqrt{5}} = \\frac{3/2}{\\sqrt{5}} = \\frac{3}{2\\sqrt{5}} = \\frac{3\\sqrt{5}}{10}$$\n\nFor the second example, $\\mathbf{x}^{(2)} = (0,1,0)^\\top$:\nThe decision function value is:\n$$f(\\mathbf{x}^{(2)}) = \\mathbf{w}^\\top \\mathbf{x}^{(2)} + b = (2)(0) + (-1)(1) + (0)(0) - 0.5 = -1 - 0.5 = -1.5$$\nThe sign of $f(\\mathbf{x}^{(2)})$ is negative, so this example is classified into the negative class.\nThe perpendicular distance from $\\mathbf{x}^{(2)}$ to the decision boundary is:\n$$d^{(2)} = \\frac{|f(\\mathbf{x}^{(2)})|}{\\|\\mathbf{w}\\|} = \\frac{|-1.5|}{\\sqrt{5}} = \\frac{1.5}{\\sqrt{5}} = \\frac{3}{2\\sqrt{5}} = \\frac{3\\sqrt{5}}{10}$$\n\nFor the third example, $\\mathbf{x}^{(3)} = (0.25,0.5,1)^\\top$:\nThe decision function value is:\n$$f(\\mathbf{x}^{(3)}) = \\mathbf{w}^\\top \\mathbf{x}^{(3)} + b = (2)(0.25) + (-1)(0.5) + (0)(1) - 0.5 = 0.5 - 0.5 + 0 - 0.5 = -0.5$$\nThe sign of $f(\\mathbf{x}^{(3)})$ is negative, so this example is classified into the negative class.\nThe perpendicular distance from $\\mathbf{x}^{(3)}$ to the decision boundary is:\n$$d^{(3)} = \\frac{|f(\\mathbf{x}^{(3)})|}{\\|\\mathbf{w}\\|} = \\frac{|-0.5|}{\\sqrt{5}} = \\frac{0.5}{\\sqrt{5}} = \\frac{1/2}{\\sqrt{5}} = \\frac{1}{2\\sqrt{5}} = \\frac{\\sqrt{5}}{10}$$\n\nIn the margin-based interpretation of SVMs, the magnitude of the geometric distance from a point to the decision boundary is a measure of classification confidence. A larger distance implies that the point is further from the boundary separating the classes, and thus the classification is considered more confident or robust.\nFor our examples, $d^{(1)} = \\frac{3\\sqrt{5}}{10}$ and $d^{(2)} = \\frac{3\\sqrt{5}}{10}$, while $d^{(3)} = \\frac{\\sqrt{5}}{10}$. The geometric margin of the classifier is $\\gamma = \\frac{\\sqrt{5}}{5} = \\frac{2\\sqrt{5}}{10}$.\nBoth $\\mathbf{x}^{(1)}$ and $\\mathbf{x}^{(2)}$ lie at a distance greater than the margin $\\gamma$ from the decision boundary ($d^{(1)} > \\gamma$ and $d^{(2)} > \\gamma$). Their classification is therefore considered confident.\nIn contrast, $\\mathbf{x}^{(3)}$ lies at a distance smaller than the margin ($d^{(3)} < \\gamma$). Although it is classified (as it does not lie on the boundary), it falls within the margin region. This indicates a low-confidence classification, as the point is close to the decision boundary.\n\nThe final numeric answers for the geometric margin and the three distances, ordered as $(\\gamma, d^{(1)}, d^{(2)}, d^{(3)})$, are $(\\frac{\\sqrt{5}}{5}, \\frac{3\\sqrt{5}}{10}, \\frac{3\\sqrt{5}}{10}, \\frac{\\sqrt{5}}{10})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sqrt{5}}{5} & \\frac{3\\sqrt{5}}{10} & \\frac{3\\sqrt{5}}{10} & \\frac{\\sqrt{5}}{10}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Choosing the right kernel and its parameters is critical for building a successful SVM classifier that generalizes well to unseen data. This advanced practice explores the behavior of the Radial Basis Function (RBF) kernel, one of the most powerful and common choices in neuroscience data analysis . By analytically deriving the geometric margin as a function of the kernel width parameter, $\\gamma$, and examining its behavior at the extremes, you will develop a deep theoretical understanding of the trade-off between model capacity and generalization, linking the mathematics of the kernel to the practical concepts of underfitting and overfitting.",
            "id": "4172674",
            "problem": "In a two-condition neural decoding task, population activity is recorded from $p$ neurons during a single trial of condition $+$ and a single trial of condition $-$. Let the resulting trial-averaged activity vectors be $x_{+} \\in \\mathbb{R}^{p}$ and $x_{-} \\in \\mathbb{R}^{p}$, with labels $y_{+} = +1$ and $y_{-} = -1$. A hard-margin Support Vector Machine (SVM) is trained in a Reproducing Kernel Hilbert Space (RKHS) induced by the Radial Basis Function (RBF) kernel $K_{\\gamma}(x, x') = \\exp(-\\gamma \\|x - x'\\|^{2})$, where $\\gamma > 0$ is the kernel width parameter. Denote the squared Euclidean distance between the two activity vectors by $\\Delta = \\|x_{+} - x_{-}\\|^{2}$.\n\nStarting from the standard hard-margin SVM dual formulation and the definition of the geometric margin in the RKHS, derive the exact expression of the margin $\\rho(\\gamma)$ as a function of $\\gamma$ and $\\Delta$. Then compute the two limits $\\lim_{\\gamma \\to 0} \\rho(\\gamma)$ and $\\lim_{\\gamma \\to \\infty} \\rho(\\gamma)$, and, based on classical margin-based generalization arguments, explain how these limits qualitatively reflect the capacity of the classifier to generalize from this two-sample training set to unseen neural data. Express your final answer as the ordered pair $\\left(\\lim_{\\gamma \\to 0} \\rho(\\gamma), \\lim_{\\gamma \\to \\infty} \\rho(\\gamma)\\right)$ using exact analytic expressions only. No numerical rounding is required.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in machine learning theory, well-posed, objective, and internally consistent. We can proceed with a formal derivation.\n\nThe problem requires the derivation of the geometric margin for a hard-margin Support Vector Machine (SVM) trained on two data points, $(x_+, y_+)$ and $(x_-, y_-)$, where $x_+, x_- \\in \\mathbb{R}^{p}$ are the activity vectors and $y_+=+1, y_-=-1$ are the corresponding labels. The classification is performed in the Reproducing Kernel Hilbert Space (RKHS), denoted $\\mathcal{H}$, induced by the Radial Basis Function (RBF) kernel $K_{\\gamma}(x, x') = \\exp(-\\gamma \\|x - x'\\|^{2})$. The squared Euclidean distance between the input vectors is given as $\\Delta = \\|x_{+} - x_{-}\\|^{2}$.\n\nWe begin with the dual formulation of the hard-margin SVM problem. For a dataset of $N$ points $(x_i, y_i)$, the dual objective is to maximize the Lagrangian $W(\\alpha)$ with respect to the multipliers $\\alpha_i$:\n$$ W(\\alpha) = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) $$\nsubject to the constraints $\\sum_{i=1}^{N} \\alpha_i y_i = 0$ and $\\alpha_i \\ge 0$ for $i=1, \\dots, N$.\n\nIn our specific case, $N=2$. Let $(x_1, y_1) = (x_+, +1)$ and $(x_2, y_2) = (x_-, -1)$. The constraint $\\sum \\alpha_i y_i = 0$ becomes:\n$$ \\alpha_1 y_1 + \\alpha_2 y_2 = 0 $$\n$$ \\alpha_1 (+1) + \\alpha_2 (-1) = 0 \\implies \\alpha_1 = \\alpha_2 $$\nSince the problem is non-trivial (i.e., the points are separable, implying $x_+ \\neq x_-$ and thus $\\Delta > 0$), the Lagrange multipliers $\\alpha_i$ for these two support vectors will be strictly positive. Let us denote the common multiplier by $\\alpha = \\alpha_1 = \\alpha_2 > 0$.\n\nSubstituting $N=2$ and $\\alpha_1 = \\alpha_2 = \\alpha$ into the dual objective $W(\\alpha)$:\n$$ W(\\alpha) = \\alpha_1 + \\alpha_2 - \\frac{1}{2} \\left( \\alpha_1^2 y_1^2 K(x_1, x_1) + \\alpha_2^2 y_2^2 K(x_2, x_2) + 2 \\alpha_1 \\alpha_2 y_1 y_2 K(x_1, x_2) \\right) $$\n$$ W(\\alpha) = 2\\alpha - \\frac{1}{2} \\left( \\alpha^2 (+1)^2 K(x_+, x_+) + \\alpha^2 (-1)^2 K(x_-, x_-) + 2 \\alpha^2 (+1)(-1) K(x_+, x_-) \\right) $$\n$$ W(\\alpha) = 2\\alpha - \\frac{1}{2} \\alpha^2 \\left( K(x_+, x_+) + K(x_-, x_-) - 2K(x_+, x_-) \\right) $$\nNow, we evaluate the kernel terms using $K_{\\gamma}(x, x') = \\exp(-\\gamma \\|x - x'\\|^{2})$ and $\\Delta = \\|x_+ - x_-\\|^2$:\n- $K(x_+, x_+) = \\exp(-\\gamma \\|x_+ - x_+\\|^2) = \\exp(0) = 1$.\n- $K(x_-, x_-) = \\exp(-\\gamma \\|x_- - x_-\\|^2) = \\exp(0) = 1$.\n- $K(x_+, x_-) = \\exp(-\\gamma \\|x_+ - x_-\\|^2) = \\exp(-\\gamma \\Delta)$.\n\nSubstituting these into the expression for $W(\\alpha)$:\n$$ W(\\alpha) = 2\\alpha - \\frac{1}{2} \\alpha^2 (1 + 1 - 2\\exp(-\\gamma \\Delta)) = 2\\alpha - \\alpha^2 (1 - \\exp(-\\gamma \\Delta)) $$\nTo find the optimal $\\alpha$ that maximizes $W(\\alpha)$, we take the derivative with respect to $\\alpha$ and set it to zero:\n$$ \\frac{d W}{d \\alpha} = 2 - 2\\alpha (1 - \\exp(-\\gamma \\Delta)) = 0 $$\nSolving for $\\alpha$:\n$$ \\alpha = \\frac{1}{1 - \\exp(-\\gamma \\Delta)} $$\nThis solution is valid since $\\gamma > 0$ and $\\Delta > 0$ implies $0 < \\exp(-\\gamma \\Delta) < 1$, so $\\alpha$ is positive and finite.\n\nThe geometric margin $\\rho$ is defined as $\\rho = 1/\\|w\\|_{\\mathcal{H}}$, where $\\|w\\|_{\\mathcal{H}}$ is the norm of the weight vector in the RKHS. The norm is related to the optimal Lagrange multipliers by:\n$$ \\|w\\|_{\\mathcal{H}}^2 = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) $$\nFor our case, this simplifies to the term we already found inside $W(\\alpha)$:\n$$ \\|w\\|_{\\mathcal{H}}^2 = \\alpha^2 (K(x_+, x_+) + K(x_-, x_-) - 2K(x_+, x_-)) = 2\\alpha^2 (1 - \\exp(-\\gamma\\Delta)) $$\nSubstituting the optimal value of $\\alpha$:\n$$ \\|w\\|_{\\mathcal{H}}^2 = 2 \\left( \\frac{1}{1 - \\exp(-\\gamma \\Delta)} \\right)^2 (1 - \\exp(-\\gamma \\Delta)) = \\frac{2}{1 - \\exp(-\\gamma \\Delta)} $$\nThe squared geometric margin is $\\rho^2 = 1 / \\|w\\|_{\\mathcal{H}}^2$.\n$$ \\rho(\\gamma)^2 = \\frac{1 - \\exp(-\\gamma \\Delta)}{2} $$\nTherefore, the geometric margin as a function of $\\gamma$ and $\\Delta$ is:\n$$ \\rho(\\gamma) = \\sqrt{\\frac{1 - \\exp(-\\gamma \\Delta)}{2}} $$\n\nNext, we compute the two required limits.\nFirst, the limit as $\\gamma \\to 0$:\n$$ \\lim_{\\gamma \\to 0} \\rho(\\gamma) = \\lim_{\\gamma \\to 0} \\sqrt{\\frac{1 - \\exp(-\\gamma \\Delta)}{2}} $$\nAs $\\gamma \\to 0$, the exponent $-\\gamma\\Delta \\to 0$. Thus, $\\exp(-\\gamma\\Delta) \\to \\exp(0) = 1$.\n$$ \\lim_{\\gamma \\to 0} \\rho(\\gamma) = \\sqrt{\\frac{1 - 1}{2}} = \\sqrt{0} = 0 $$\nSecond, the limit as $\\gamma \\to \\infty$:\n$$ \\lim_{\\gamma \\to \\infty} \\rho(\\gamma) = \\lim_{\\gamma \\to \\infty} \\sqrt{\\frac{1 - \\exp(-\\gamma \\Delta)}{2}} $$\nAs $\\gamma \\to \\infty$, and assuming $\\Delta > 0$, the exponent $-\\gamma\\Delta \\to -\\infty$. Thus, $\\exp(-\\gamma\\Delta) \\to 0$.\n$$ \\lim_{\\gamma \\to \\infty} \\rho(\\gamma) = \\sqrt{\\frac{1 - 0}{2}} = \\sqrt{\\frac{1}{2}} = \\frac{\\sqrt{2}}{2} $$\n\nFinally, we provide a qualitative explanation for these results based on generalization. Classical margin theory suggests that a larger margin corresponds to lower classifier capacity and better generalization.\n- In the limit $\\gamma \\to 0$, the RBF kernel becomes infinitely wide, $K(x, x') \\to 1$ for any pair of points. The feature map $\\phi(x)$ maps all input vectors to nearly the same point in the RKHS, causing the distance between $\\phi(x_+)$ and $\\phi(x_-)$ to vanish. Consequently, the margin $\\rho(\\gamma)$ collapses to $0$. The classifier loses all discriminative power, as it cannot separate the points in the feature space. This corresponds to a state of 'underfitting', where the model is too simple to capture the structure of the data, leading to poor generalization.\n- In the limit $\\gamma \\to \\infty$, the RBF kernel becomes a delta-like function, becoming non-zero only for identical inputs. The kernel matrix approaches the identity matrix, meaning the feature map sends the two distinct points $x_+$ and $x_-$ to two orthogonal unit vectors in the RKHS. They become maximally separated, and the margin $\\rho(\\gamma)$ approaches its maximum possible value of $\\sqrt{2}/2$ (half the distance between two orthogonal unit vectors). While a large margin is often associated with good generalization, in this context it signifies extreme 'overfitting'. The classifier has effectively memorized the two training points, creating a decision function that is highly sensitive around them but fails to generalize to any new, unseen point (unless it is identical to one of the training points). The model capacity has become too high, fitting the specific training samples perfectly at the cost of any broader applicability.\n\nThus, both extremes of $\\gamma$ lead to poor generalization from this minimal training set, albeit for opposite reasons (underfitting vs. overfitting). A practical application would require choosing an intermediate value of $\\gamma$ via a method like cross-validation to balance model complexity and achieve optimal performance on unseen data.\n\nThe final answer is the ordered pair of the two limits.\n$$ \\left( \\lim_{\\gamma \\to 0} \\rho(\\gamma), \\lim_{\\gamma \\to \\infty} \\rho(\\gamma) \\right) = \\left( 0, \\frac{\\sqrt{2}}{2} \\right) $$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & \\frac{\\sqrt{2}}{2}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}