{
    "hands_on_practices": [
        {
            "introduction": "在神经解码中，一个常见挑战是来自神经元群体的记录往往表现出相关的放电模式，即多重共线性。当使用普通最小二乘法等标准线性模型时，这种共线性会导致解码器权重变得极不稳定，从而放大预测误差。本练习将引导你通过理论推导，精确量化这种不稳定性如何体现为预测方差的增加，并将其与数据协方差矩阵的谱特性（特别是其最小特征值）联系起来 。这个分析揭示了为何在解码高度相关的神经活动时，需要更稳健的回归方法。",
            "id": "4189997",
            "problem": "一个神经解码实验室试图使用基于线性回归的解码方法，从一个包含 $p$ 个同时记录的神经元的群体中重建一个一维运动学变量。在每次试验 $t \\in \\{1,\\dots,n\\}$ 中，观测到的运动学标量由线性模型 $y_{t} = x_{t}^{\\top}\\beta^{\\star} + \\varepsilon_{t}$ 建模，其中 $x_{t} \\in \\mathbb{R}^{p}$ 是预处理后的神经协变量向量，$\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是真实的解码权重向量，$\\varepsilon_{t}$ 是满足 $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$ 的独立噪声项。设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其行向量为 $x_{t}^{\\top}$，并且使用权重的普通最小二乘 (OLS) 估计量来构建解码器。考虑该解码器在一个新的单位范数协变量 $x_{\\mathrm{new}} \\in \\mathbb{R}^{p}$（其中 $\\|x_{\\mathrm{new}}\\|=1$）上的预测输出。\n\n从线性模型的基本性质出发，并利用 $X^{\\top}X$ 的谱分解，根据第一性原理推导线性解码器输出 $x_{\\mathrm{new}}^{\\top}\\hat{\\beta}$ 在所有单位范数 $x_{\\mathrm{new}}$ 上的最坏情况下的预测方差。然后，在 $X^{\\top}X$ 的特征值为 $(\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4})=(500,2,1,0.05)$ 且噪声方差为 $\\sigma^{2}=0.2$ 的情况下，计算这个最坏情况下的预测方差。将您的最终数值答案四舍五入到四位有效数字。以无单位的单个实数形式提供最终答案。",
            "solution": "问题陈述已经过验证，是完整且科学严谨的。下面我们进行完整的推导。\n\n问题要求计算线性解码器的最坏情况下的预测方差。让我们首先确定普通最小二乘 (OLS) 估计量 $\\hat{\\beta}$ 的统计特性。线性模型的向量形式为 $y = X\\beta^{\\star} + \\varepsilon$，其中 $y \\in \\mathbb{R}^{n}$ 是观测向量， $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是真实权重向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是噪声项向量。噪声项 $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立同分布的，这意味着噪声向量 $\\varepsilon$ 服从多元正态分布 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\nOLS 估计量 $\\hat{\\beta}$ 由以下公式给出：\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y $$\n代入 $y$ 的模型，我们可以用真实参数 $\\beta^{\\star}$ 和噪声 $\\varepsilon$ 来表示 $\\hat{\\beta}$：\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}(X\\beta^{\\star} + \\varepsilon) = (X^{\\top}X)^{-1}X^{\\top}X\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon = \\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon $$\n设计矩阵 $X$ 被认为是固定的（非随机的）。在 $\\hat{\\beta}$ 的这个表达式中，唯一的随机分量是噪声向量 $\\varepsilon$。\n\n首先，我们确定 $\\hat{\\beta}$ 的期望值：\n$$ \\mathrm{E}[\\hat{\\beta}] = \\mathrm{E}[\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon] = \\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\mathrm{E}[\\varepsilon] $$\n由于 $\\mathrm{E}[\\varepsilon] = 0$，我们有 $\\mathrm{E}[\\hat{\\beta}] = \\beta^{\\star}$。这证实了 OLS 估计量是无偏的。\n\n接下来，我们推导 $\\hat{\\beta}$ 的协方差矩阵。协方差矩阵定义为 $\\mathrm{Cov}(\\hat{\\beta}) = \\mathrm{E}[(\\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}])(\\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}])^{\\top}]$。\n$$ \\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}] = (X^{\\top}X)^{-1}X^{\\top}\\varepsilon $$\n$$ \\mathrm{Cov}(\\hat{\\beta}) = \\mathrm{E}[((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)^{\\top}] = \\mathrm{E}[(X^{\\top}X)^{-1}X^{\\top}\\varepsilon\\varepsilon^{\\top}X((X^{\\top}X)^{-1})^{\\top}] $$\n由于 $X$ 是非随机的，我们可以将其移到期望符号的外面：\n$$ \\mathrm{Cov}(\\hat{\\beta}) = (X^{\\top}X)^{-1}X^{\\top}\\mathrm{E}[\\varepsilon\\varepsilon^{\\top}]X((X^{\\top}X)^{-1})^{\\top} $$\n项 $\\mathrm{E}[\\varepsilon\\varepsilon^{\\top}]$ 是噪声的协方差矩阵，即 $\\mathrm{Cov}(\\varepsilon) = \\sigma^2 I_n$。矩阵 $X^{\\top}X$ 是对称的，所以它的逆矩阵也是对称的。\n$$ \\mathrm{Cov}(\\hat{\\beta}) = (X^{\\top}X)^{-1}X^{\\top}(\\sigma^2 I_n)X(X^{\\top}X)^{-1} = \\sigma^2 (X^{\\top}X)^{-1}X^{\\top}X(X^{\\top}X)^{-1} = \\sigma^2 (X^{\\top}X)^{-1} $$\n这是 OLS 估计量的一个基本结果。\n\n问题要求我们分析解码器对于新协变量向量 $x_{\\mathrm{new}} \\in \\mathbb{R}^p$ 的预测输出。预测值（解码器输出）为 $\\hat{y}_{\\mathrm{new}} = x_{\\mathrm{new}}^{\\top}\\hat{\\beta}$。我们需要求这个预测的方差。利用随机向量线性变换的方差的一般性质 $\\mathrm{Var}(A z) = A \\mathrm{Cov}(z) A^{\\top}$，其中 $A = x_{\\mathrm{new}}^{\\top}$ 且 $z = \\hat{\\beta}$：\n$$ \\mathrm{Var}(x_{\\mathrm{new}}^{\\top}\\hat{\\beta}) = x_{\\mathrm{new}}^{\\top} \\mathrm{Cov}(\\hat{\\beta}) x_{\\mathrm{new}} $$\n代入 $\\mathrm{Cov}(\\hat{\\beta})$ 的表达式：\n$$ \\mathrm{Var}(x_{\\mathrm{new}}^{\\top}\\hat{\\beta}) = x_{\\mathrm{new}}^{\\top} (\\sigma^2 (X^{\\top}X)^{-1}) x_{\\mathrm{new}} = \\sigma^2 x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} $$\n任务是找出在所有可能的单位范数协变量向量 $x_{\\mathrm{new}}$ 上的最坏情况（最大）预测方差。给定的条件是 $\\|x_{\\mathrm{new}}\\|=1$，在此上下文中意味着欧几里得范数为 1，即 $\\|x_{\\mathrm{new}}\\|_2 = 1$。因此，问题就是求解以下优化问题：\n$$ \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( \\sigma^2 x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) = \\sigma^2 \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) $$\n对于一个对称矩阵 $M$ 和一个单位向量 $x$，$x^{\\top} M x$ 表达式被称为瑞利商。瑞利商的最大值是矩阵 $M$ 的最大特征值。在我们的情况中，$M = (X^{\\top}X)^{-1}$。\n所以，我们有：\n$$ \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) = \\lambda_{\\max}((X^{\\top}X)^{-1}) $$\n设矩阵 $X^{\\top}X$ 的特征值为 $\\lambda_i$。由于 $X^{\\top}X$ 是一个对称正定矩阵，其特征值为正实数。逆矩阵 $(X^{\\top}X)^{-1}$ 的特征值是 $X^{\\top}X$ 特征值的倒数，即 $1/\\lambda_i$。\n因此，$(X^{\\top}X)^{-1}$ 的最大特征值是 $X^{\\top}X$ 最小特征值的倒数：\n$$ \\lambda_{\\max}((X^{\\top}X)^{-1}) = \\max_{i} \\left( \\frac{1}{\\lambda_i} \\right) = \\frac{1}{\\min_{i}(\\lambda_i)} = \\frac{1}{\\lambda_{\\min}(X^{\\top}X)} $$\n将此代回最坏情况方差的表达式中：\n$$ \\text{最坏情况方差} = \\sigma^2 \\cdot \\frac{1}{\\lambda_{\\min}(X^{\\top}X)} $$\n$x_{\\mathrm{new}}$ 的最坏情况方向对应于 $X^{\\top}X$ 与其最小特征值 $\\lambda_{\\min}$ 相关联的特征向量。\n\n现在我们可以使用给定的数值来计算这个表达式。\n$X^{\\top}X$ 的特征值给定为 $(\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4) = (500, 2, 1, 0.05)$。\n这些特征值中最小的是：\n$$ \\lambda_{\\min}(X^{\\top}X) = \\min\\{500, 2, 1, 0.05\\} = 0.05 $$\n噪声方差给定为 $\\sigma^2 = 0.2$。\n\n将这些值代入我们推导出的最坏情况预测方差公式中：\n$$ \\text{最坏情况方差} = \\frac{\\sigma^2}{\\lambda_{\\min}(X^{\\top}X)} = \\frac{0.2}{0.05} = 4 $$\n问题要求答案四舍五入到四位有效数字。确切答案是 $4$。用四位有效数字表示，即为 $4.000$。",
            "answer": "$$\n\\boxed{4.000}\n$$"
        },
        {
            "introduction": "针对前一个练习中探讨的方差膨胀问题，我们引入一种强大的解决方案：正则化。本练习聚焦于岭回归，这是一种稳定解码器的基石技术。你将推导岭回归的解析解，并分析其样本外预测误差，以揭示其核心的偏差-方差权衡原理 。通过这个过程，你将理解为何引入少量偏差（通过正则化参数 $\\lambda$）可以显著降低估计量的方差，从而提升解码器的泛化能力，并最终推导出最优的正则化强度。",
            "id": "5002219",
            "problem": "一个实验室正在开发一种脑机接口（BCI）解码器，该解码器在中心向外伸出任务中，将神经元放电率映射到手的标量切向速度。在每个宽度为 $\\Delta t$、由 $t \\in \\{1,\\ldots,N\\}$ 索引的时间窗内，预处理后的放电率向量为 $\\mathbf{r}_{t} \\in \\mathbb{R}^{p}$（已在神经元维度上进行 z-score 标准化和白化，因此样本协方差为单位矩阵），同时测得的手部速度为 $v_{t} \\in \\mathbb{R}$。假设一个线性高斯编码模型 $v_{t} = \\mathbf{r}_{t}^{\\top} \\boldsymbol{\\beta} + \\varepsilon_{t}$，其中 $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$ 在时间 $t$ 上独立，并且与 $\\mathbf{r}_{t}$ 独立。将数据堆叠成设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$（其行向量为 $\\mathbf{r}_{t}^{\\top}$）和目标向量 $\\mathbf{y} \\in \\mathbb{R}^{N}$（其元素为 $v_{t}$）。由于神经元特征是使用训练数据进行白化的，您可以假设经验二阶矩满足 $\\mathbf{X}^{\\top}\\mathbf{X} = N \\mathbf{I}_{p}$。\n\n您通过最小化惩罚最小二乘目标函数来训练一个岭回归解码器\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2},\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数，$\\|\\cdot\\|$ 表示欧几里得范数。\n\n任务：\n1) 从模型 $ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$（其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{N})$）出发，推导最小化 $J(\\mathbf{w};\\lambda)$ 的闭式解 $\\widehat{\\mathbf{w}}$。\n\n2) 对于从同一分布中抽取的一个新的、独立的测试样本 $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$（其中 $\\mathbf{r}_{\\mathrm{new}}$ 与训练数据独立，并满足 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}] = \\mathbf{0}$ 和 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$），推导期望样本外均方预测误差\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]\n$$\n将其表示为 $\\lambda$、$N$、$p$、$\\sigma^{2}$ 和 $\\boldsymbol{\\beta}$ 的显式函数。您的推导过程必须从上述定义以及关于 $\\mathbf{X}$ 和 $\\mathbf{r}_{\\mathrm{new}}$ 的假设出发，并且必须展示出依赖于 $\\lambda$ 的偏差-方差分解。\n\n3) 为了使权衡关系明确且不依赖于某个特定的未知 $\\boldsymbol{\\beta}$，假设一个与神经群体编码一致的层级先验：$\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$，其中 $\\tau^{2} > 0$。在该先验上对您的 $\\mathcal{E}(\\lambda)$ 表达式求平均，并将其简化为 $\\lambda$、$N$、$p$、$\\sigma^{2}$ 和 $\\tau^{2}$ 的标量函数。\n\n4) 使用您求得的平均表达式，确定使期望样本外均方预测误差最小化的 $\\lambda^{\\star}$ 值。然后，使用以下参数对该最优值进行数值计算：\n- $p = 100$,\n- $N = 10000$,\n- $\\sigma^{2} = 0.04$,\n- $\\tau^{2} = 0.01$。\n将 $\\lambda^{\\star}$ 的最终值表示为一个无单位的纯数。如果需要四舍五入，保留四位有效数字。如果不需要，提供精确值。",
            "solution": "该问题要求在脑机接口（BCI）的背景下，对一个岭回归解码器进行多步分析。分析过程涉及在特定的数据模型和先验下，推导解码器、其样本外误差以及最优正则化参数。\n\n### 任务 1：岭回归估计量 $\\widehat{\\mathbf{w}}$ 的推导\n岭回归估计量 $\\widehat{\\mathbf{w}}$ 是通过最小化目标函数得到的\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2}\n$$\n其中 $\\|\\cdot\\|$ 是欧几里得范数。我们可以用向量转置来表示平方范数：\n$$\nJ(\\mathbf{w};\\lambda) = (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^{\\top}(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n展开第一项得到：\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - \\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w} - \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{w} + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n由于 $\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y}$ 是一个标量，它等于其转置 $\\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w}$。因此，我们可以合并交叉项：\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\n为了找到最小值，我们对 $J(\\mathbf{w};\\lambda)$ 求关于 $\\mathbf{w}$ 的梯度，并将其设为零。使用标准的矩阵微积分结果（对于对称矩阵 $\\mathbf{M}$，有 $\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{a} = \\mathbf{a}$ 和 $\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{M}\\mathbf{w} = 2\\mathbf{M}\\mathbf{w}$）：\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w};\\lambda) = -2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\n将梯度设为零向量，得到解 $\\widehat{\\mathbf{w}}$：\n$$\n-2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{0}\n$$\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{X}^{\\top}\\mathbf{y}\n$$\n形式解为 $\\widehat{\\mathbf{w}} = (\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$。问题陈述了假设，即经验二阶矩为 $\\mathbf{X}^{\\top}\\mathbf{X} = N\\mathbf{I}_{p}$。将此代入 $\\widehat{\\mathbf{w}}$ 的表达式中：\n$$\n\\widehat{\\mathbf{w}} = (N\\mathbf{I}_{p} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = ((N+\\lambda)\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{I}_{p}^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n这就是在给定假设下 $\\widehat{\\mathbf{w}}$ 的闭式解。\n\n### 任务 2：期望样本外均方预测误差\n我们需要推导 $\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]$。该期望是关于训练数据的随机性（这使得 $\\widehat{\\mathbf{w}}$ 成为随机的）和新的测试样本 $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$ 计算的。\n新样本的真实模型是 $v_{\\mathrm{new}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}$，其中 $\\varepsilon_{\\mathrm{new}} \\sim \\mathcal{N}(0,\\sigma^{2})$。将此代入误差项中：\n$$\nv_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}) - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}\n$$\n将此表达式平方：\n$$\n(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2} + 2\\varepsilon_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}^{2}\n$$\n现在我们来求期望。新的噪声项 $\\varepsilon_{\\mathrm{new}}$ 与训练数据（因此也与 $\\widehat{\\mathbf{w}}$）以及新的特征 $\\mathbf{r}_{\\mathrm{new}}$ 独立。由于 $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}] = 0$，交叉项消失。我们有 $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}^{2}] = \\sigma^{2}$。\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] + \\sigma^{2}\n$$\n剩余的期望是关于 $\\mathbf{r}_{\\mathrm{new}}$ 和 $\\widehat{\\mathbf{w}}$ 的。我们可以使用迹技巧重写期望内的项：$(\\mathbf{a}^{\\top}\\mathbf{b})^2 = \\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b} = \\mathrm{tr}(\\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}) = \\mathrm{tr}(\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}\\mathbf{b}^{\\top})$。更简单的方法是使用 $\\mathbb{E}[x^2] = \\mathbb{E}[\\mathrm{tr}(x^2)]$，其中 $x$ 是一个标量。\n$$\n\\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] = \\mathbb{E}\\left[\\mathrm{tr}\\left((\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top} (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right)\\right]\n$$\n根据迹和期望的线性性质，并且由于 $\\widehat{\\mathbf{w}}$（来自训练数据）与 $\\mathbf{r}_{\\mathrm{new}}$ 独立：\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right)\n$$\n使用假设 $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$：\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top}\\mathbf{I}_{p}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right) = \\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right]\n$$\n所以，$\\mathcal{E}(\\lambda) = \\mathbb{E}[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}] + \\sigma^2$。现在的期望 $\\mathbb{E}[\\cdot]$ 仅针对训练数据的随机性。我们现在进行偏差-方差分解：\n$$\n\\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right] = \\left\\|\\boldsymbol{\\beta} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\right\\|^{2} + \\mathbb{E}\\left[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^{2}\\right] = \\text{Bias}(\\widehat{\\mathbf{w}})^{2} + \\text{Var}(\\widehat{\\mathbf{w}})\n$$\n我们推导偏差项和方差项。首先，将 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ 代入 $\\widehat{\\mathbf{w}}$ 的表达式中：\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\n$$\n$\\widehat{\\mathbf{w}}$ 的期望（关于 $\\boldsymbol{\\varepsilon}$）是：\n$$\n\\mathbb{E}[\\widehat{\\mathbf{w}}] = \\mathbb{E}\\left[\\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\\right] = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}]) = \\frac{N}{N+\\lambda}\\boldsymbol{\\beta}\n$$\n偏差的平方是：\n$$\n\\text{Bias}(\\widehat{\\mathbf{w}})^{2} = \\left\\|\\mathbb{E}[\\widehat{\\mathbf{w}}] - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|\\frac{N}{N+\\lambda}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|-\\frac{\\lambda}{N+\\lambda}\\boldsymbol{\\beta}\\right\\|^{2} = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2}\n$$\n方差项是 $\\mathbb{E}[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^2] = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}}))$。\n$$\n\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}] = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\n$$\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\mathbb{E}\\left[(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])^{\\top}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbb{E}\\left[\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]\\mathbf{X}\n$$\n使用 $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]=\\sigma^2\\mathbf{I}_N$ 和 $\\mathbf{X}^{\\top}\\mathbf{X}=N\\mathbf{I}_p$：\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{I}_{N}\\mathbf{X} = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{X} = \\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\n$$\n方差是该协方差矩阵的迹：\n$$\n\\text{Var}(\\widehat{\\mathbf{w}}) = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}})) = \\mathrm{tr}\\left(\\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\\right) = \\frac{pN\\sigma^2}{(N+\\lambda)^2}\n$$\n结合所有项，期望样本外误差为：\n$$\n\\mathcal{E}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### 任务 3：在 $\\boldsymbol{\\beta}$ 的先验上求平均\n我们得到了真实权重上的一个先验分布 $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$。我们需要计算 $\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\mathbb{E}_{\\boldsymbol{\\beta}}[\\mathcal{E}(\\lambda)]$。$\\mathcal{E}(\\lambda)$ 中唯一依赖于 $\\boldsymbol{\\beta}$ 的项是 $\\|\\boldsymbol{\\beta}\\|^{2}$。我们在该先验下计算其期望：\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\sum_{i=1}^{p}\\beta_{i}^{2}\\right] = \\sum_{i=1}^{p}\\mathbb{E}_{\\boldsymbol{\\beta}}[\\beta_{i}^{2}]\n$$\n对于每个分量 $\\beta_i \\sim \\mathcal{N}(0, \\tau^{2})$，其二阶矩为 $\\mathbb{E}[\\beta_{i}^{2}] = \\mathrm{Var}(\\beta_i) + (\\mathbb{E}[\\beta_i])^{2} = \\tau^{2} + 0^{2} = \\tau^{2}$。\n因此，期望平方范数为：\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\sum_{i=1}^{p}\\tau^{2} = p\\tau^{2}\n$$\n将此代入 $\\mathcal{E}(\\lambda)$ 的表达式中：\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}p\\tau^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{p\\lambda^{2}\\tau^{2} + pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### 任务 4：最优正则化参数 $\\lambda^{\\star}$\n为了找到最小化 $\\mathcal{E}_{\\mathrm{avg}}(\\lambda)$ 的 $\\lambda$，我们对其关于 $\\lambda$ 求导，并将导数设为零。在最小化过程中，常数项 $\\sigma^{2}$ 可以忽略。\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{d}{d\\lambda}\\left( \\frac{p(\\lambda^{2}\\tau^{2} + N\\sigma^2)}{(N+\\lambda)^2} \\right)\n$$\n使用商法则 $\\frac{d}{dx}(\\frac{u}{v}) = \\frac{u'v - uv'}{v^2}$：\n令 $u(\\lambda) = p(\\lambda^{2}\\tau^{2} + N\\sigma^2)$ 和 $v(\\lambda) = (N+\\lambda)^2$。\n那么 $u'(\\lambda) = 2p\\lambda\\tau^{2}$ 和 $v'(\\lambda) = 2(N+\\lambda)$。\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{(2p\\lambda\\tau^{2})(N+\\lambda)^2 - p(\\lambda^{2}\\tau^{2} + N\\sigma^2) \\cdot 2(N+\\lambda)}{(N+\\lambda)^4}\n$$\n将导数设为零，并假设 $\\lambda \\ge 0, N \\ge 1$，我们可以通过除以非零因子 $2p(N+\\lambda)$ 来简化：\n$$\n(\\lambda\\tau^{2})(N+\\lambda) - (\\lambda^{2}\\tau^{2} + N\\sigma^2) = 0\n$$\n$$\nN\\lambda\\tau^{2} + \\lambda^{2}\\tau^{2} - \\lambda^{2}\\tau^{2} - N\\sigma^2 = 0\n$$\n$$\nN\\lambda\\tau^{2} = N\\sigma^2\n$$\n由于 $N \\ge 1$ 且 $\\tau^{2} > 0$（给定），我们可以除以 $N\\tau^{2}$ 来找到最优的 $\\lambda^{\\star}$：\n$$\n\\lambda^{\\star} = \\frac{\\sigma^2}{\\tau^2}\n$$\n这个结果形式优美简洁，表示测量中的噪声方差与模型参数的先验方差之比。可以检查二阶导数以确认这是一个最小值点。导数的分子简化为 $2pN(\\lambda\\tau^2 - \\sigma^2)(N+\\lambda)$，当 $\\lambda  \\sigma^2/\\tau^2$ 时为负，当 $\\lambda > \\sigma^2/\\tau^2$ 时为正，这确认了该点为最小值点。\n\n最后，我们用给定的值对这个表达式进行数值计算：\n- $\\sigma^{2} = 0.04$\n- $\\tau^{2} = 0.01$\n在这种理想化的情况下，找到 $\\lambda^{\\star}$ 并不需要 $p$ 和 $N$ 的值。\n$$\n\\lambda^{\\star} = \\frac{0.04}{0.01} = 4\n$$\n最优值恰好为 4。",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "最后的这项练习将前面的概念整合到一个仿真的、端到端的综合项目中。你将构建一个完整的编码-解码流水线，从模拟神经元的脉冲序列到评估解码器的性能。核心任务是研究一个关键的设计选择——脉冲分箱的时间窗口——如何影响信号质量、解码精度和计算成本，并最终在给定的计算预算下找到最佳平衡点 。这是一个在实际神经科学研究中普遍存在的挑战，能够锻炼你综合运用理论和编程解决实际问题的能力。",
            "id": "4190004",
            "problem": "考虑一个模拟的神经元群体，其脉冲生成遵循由标量潜在刺激驱动的泊松过程。潜在刺激 $s(t)$ 是一个在总时长 $T_{\\mathrm{total}}$ 秒内，以基础分辨率 $\\delta$ 秒采样的离散时间序列。每个神经元 $i \\in \\{1,\\dots,N\\}$ 的发放率 $r_i(t)$ 取决于 $s(t)$。在给定发放率的条件下，脉冲作为独立的泊松增量生成。您的任务是实现一种基于回归的解码方法，该方法使用普通最小二乘法 (OLS) 将分箱后的脉冲计数映射到潜在刺激，并分析决定系数如何依赖于时间窗宽度。您必须选择在固定的计算预算下，能使解码性能最大化的时间窗宽度 $\\Delta t$。\n\n该问题的基本依据包括：泊松过程的性质、线性回归以及 OLS 的计算复杂度。以下定义适用：\n- 潜在刺激 $s(t)$ 是通过一阶自回归过程生成的：$s_{k+1} = \\phi s_k + \\eta_k$，其中 $k$ 是以分辨率 $\\delta$ 索引的离散时间样本，$\\phi$ 是一个标量自回归系数，$\\eta_k$ 是均值为零的独立高斯噪声。\n- 神经元 $i$ 在时间 $t$ 的发放率由 $r_i(t) = \\exp(b_i + a_i s(t))$ 给出，其中 $a_i$ 和 $b_i$ 是固定的神经元特异性参数，指数非线性确保了发放率的正性。\n- 对于一个时间窗宽度 $\\Delta t$（其为 $\\delta$ 的整数倍），神经元 $i$ 在一个时间窗内的期望脉冲计数等于 $r_i(t)$ 在该时间窗上的积分。在分辨率 $\\delta$ 下，使用黎曼和近似计算，该值约为 $\\lambda_{i,\\mathrm{bin}} \\approx \\sum_{m \\in \\mathrm{bin}} r_i(m\\delta)\\,\\delta$。给定 $\\lambda_{i,\\mathrm{bin}}$，脉冲计数 $x_{i,\\mathrm{bin}}$ 从 $\\mathrm{Poisson}(\\lambda_{i,\\mathrm{bin}})$ 分布中采样，且在神经元和时间窗之间独立。\n- 对于给定的 $\\Delta t$，设计矩阵 $X \\in \\mathbb{R}^{T \\times N}$ 通过将不同时间窗的脉冲计数按行堆叠而成，目标向量 $y \\in \\mathbb{R}^{T}$ 则由每个时间窗内 $s(t)$ 的平均值构成。沿时间维度进行训练集和测试集的划分，第一部分用于训练，其余部分用于测试。\n- 普通最小二乘法 (OLS) 解码拟合 $\\hat{\\beta} \\in \\mathbb{R}^{N}$ 来从 $X$ 预测 $y$，并在留出数据上计算决定系数 $R^2 = 1 - \\frac{\\sum_{j}(y_j - \\hat{y}_j)^2}{\\sum_{j}(y_j - \\bar{y})^2}$，其中 $\\hat{y}_j$ 是在测试集上的预测值，$\\bar{y}$ 是 $y$ 在测试集上的均值。\n- 通过正规方程求解 OLS 的计算复杂度为 $\\mathcal{O}(N^2 T_{\\mathrm{train}} + N T_{\\mathrm{train}} + N^3)$，其中 $N$ 是神经元数量，$T_{\\mathrm{train}}$ 是训练时间窗的数量。为解决此问题，我们将浮点运算成本近似为 $C(\\Delta t) = N^2 T_{\\mathrm{train}} + N T_{\\mathrm{train}} + N^3$，并强制执行预算约束 $C(\\Delta t) \\le B$。\n\n要求：\n1. 以基础分辨率 $\\delta$ 实现一个模拟器，为每个候选的时间窗宽度 $\\Delta t$ 生成潜在刺激 $s(t)$、神经元参数 $\\{a_i,b_i\\}$ 以及分箱后的泊松脉冲计数。使用 $f_{\\mathrm{train}} = 0.8$ 的训练比例（即 $80\\%$ 的时间窗用于训练， $20\\%$ 用于测试）。\n2. 对于每个候选的 $\\Delta t$，在训练集上拟合一个 OLS 解码器，并在测试集上计算 $R^2$。排除任何成本 $C(\\Delta t)$ 超过给定预算 $B$ 的 $\\Delta t$。\n3. 在预算约束下，选择使 $R^2$ 最大化的 $\\Delta t$。如果 $R^2$ 出现平局，选择平局候选中最小的 $\\Delta t$。\n4. 最终程序必须为每个提供的测试用例输出所选的最优时间窗宽度（单位为秒），结果为浮点数并保留三位小数。\n5. 最终输出必须是单行，包含一个由方括号括起来的逗号分隔列表（例如 `[0.050,0.200,0.010]`）。所有输出都必须以秒为单位。\n\n在所有测试用例中，请使用以下固定参数进行模拟，除非在每个用例中另有说明：\n- 自回归系数 $\\phi = 0.99$。\n- 刺激的高斯噪声标准差 $\\sigma_{\\eta} = 0.05$。\n- 神经元增益 $a_i$ 从 $\\mathrm{Uniform}(-0.6,0.6)$ 分布中独立抽取。\n- 神经元对数偏置 $b_i$ 从 $\\mathcal{N}(\\ln(5),0.2^2)$ 分布中独立抽取。\n- 训练比例 $f_{\\mathrm{train}} = 0.8$。\n- 在计算分箱后的发放率时，使用上述分辨率为 $\\delta$ 的黎曼和。\n\n测试套件：\n- 用例 1：$N=40$，$T_{\\mathrm{total}}=100\\,\\mathrm{s}$，$\\delta=0.01\\,\\mathrm{s}$，候选 $\\Delta t \\in \\{0.02,0.05,0.1,0.2,0.5\\}\\,\\mathrm{s}$，预算 $B=7{,}000{,}000$，随机种子 $=12345$。\n- 用例 2：$N=80$，$T_{\\mathrm{total}}=100\\,\\mathrm{s}$，$\\delta=0.01\\,\\mathrm{s}$，候选 $\\Delta t \\in \\{0.02,0.05,0.1,0.2,0.5\\}\\,\\mathrm{s}$，预算 $B=5{,}000{,}000$，随机种子 $=54321$。\n- 用例 3：$N=30$，$T_{\\mathrm{total}}=200\\,\\mathrm{s}$，$\\delta=0.01\\,\\mathrm{s}$，候选 $\\Delta t \\in \\{0.01,0.02,0.05,0.1\\}\\,\\mathrm{s}$，预算 $B=100{,}000{,}000$，随机种子 $=2023$。\n- 用例 4：$N=60$，$T_{\\mathrm{total}}=50\\,\\mathrm{s}$，$\\delta=0.01\\,\\mathrm{s}$，候选 $\\Delta t \\in \\{0.02,0.05,0.1,0.2,0.5\\}\\,\\mathrm{s}$，预算 $B=1{,}000{,}000$，随机种子 $=777$。\n\n答案规格：\n- 对每个用例，程序必须在预算约束下计算最优时间窗宽度（单位为秒），保留三位小数，并将四个结果以 $[b_1,b_2,b_3,b_4]$ 的形式单行输出，其中每个 $b_j$ 是一个以秒为单位的浮点数。\n- 不应打印任何其他文本。",
            "solution": "该问题要求从模拟的神经元脉冲序列中解码一个潜在刺激，并找到最优的时间窗宽度 $\\Delta t$。优化目标是在计算预算 $B$ 的约束下，最大化在留出测试集上的决定系数 $R^2$。解决方案涉及一个多步骤的模拟和分析流程，并对每个测试用例分别执行。\n\n首先，我们建立模拟环境。所有随机过程都设置了种子以保证可复现性。潜在刺激 $s(t)$ 是通过一阶自回归过程 AR(1) 生成的，在总时长 $T_{\\mathrm{total}}$ 秒内，以 $\\delta$ 秒的基础分辨率进行采样。该过程由方程 $s_{k+1} = \\phi s_k + \\eta_k$ 定义，其中 $k$ 是离散时间索引，$\\phi$ 是自回归系数，$\\eta_k$ 从均值为零、标准差为 $\\sigma_{\\eta}$ 的高斯分布中抽取。固定参数为 $\\phi=0.99$ 和 $\\sigma_{\\eta}=0.05$。对于群体中的 $N$ 个神经元，我们为每个神经元生成一个增益参数 $a_i$（从均匀分布 $\\mathrm{Uniform}(-0.6, 0.6)$ 中抽取）和一个对数偏置参数 $b_i$（从正态分布 $\\mathcal{N}(\\ln(5), 0.2^2)$ 中抽取）。\n\n对于每个候选的时间窗宽度 $\\Delta t$，我们仅在它满足计算预算时才继续处理。计算成本由函数 $C(\\Delta t) = N^2 T_{\\mathrm{train}} + N T_{\\mathrm{train}} + N^3 \\le B$ 近似，其中 $T_{\\mathrm{train}}$ 是用于训练的时间窗数量。时间窗总数为 $T = T_{\\mathrm{total}}/\\Delta t$，训练时间窗数量为 $T_{\\mathrm{train}} = \\lfloor f_{\\mathrm{train}} \\cdot T \\rfloor$，训练比例为 $f_{\\mathrm{train}}=0.8$。任何 $C(\\Delta t)$ 超过预算 $B$ 的 $\\Delta t$ 都将被舍弃。\n\n对于每个有效的 $\\Delta t$，我们构建用于回归的数据。神经元 $i$ 在时间 $t_k = k\\delta$ 的瞬时发放率通过非线性函数 $r_i(t_k) = \\exp(b_i + a_i s(t_k))$ 由刺激决定。为了获得分箱后的脉冲计数，我们首先计算每个时间窗内神经元 $i$ 的期望脉冲数。这通过使用黎曼和来近似发放率在时间窗上的积分实现：$\\lambda_{i,\\mathrm{bin}} \\approx \\delta \\sum_{m \\in \\mathrm{bin}} r_i(t_m)$。然后，该时间窗内神经元 $i$ 的实际脉冲计数 $x_{i,\\mathrm{bin}}$ 从均值为 $\\lambda_{i,\\mathrm{bin}}$ 的泊松分布中抽取，即 $x_{i,\\mathrm{bin}} \\sim \\mathrm{Poisson}(\\lambda_{i,\\mathrm{bin}})$。这些计数被组装成一个设计矩阵 $X \\in \\mathbb{R}^{T \\times N}$，其中每一行对应一个时间窗，每一列对应一个神经元。每个时间窗对应的目标变量 $y_{\\mathrm{bin}}$ 是该时间窗内潜在刺激 $s(t)$ 的平均值。这些平均值的集合构成了目标向量 $y \\in \\mathbb{R}^{T}$。\n\n数据集 $(X, y)$ 按时间顺序被划分为训练集和测试集。前 $80\\%$ 的时间窗构成训练数据 $(X_{\\mathrm{train}}, y_{\\mathrm{train}})$，剩下的 $20\\%$ 构成测试数据 $(X_{\\mathrm{test}}, y_{\\mathrm{test}})$。然后，在训练数据上拟合一个普通最小二乘法 (OLS) 模型，以找到回归系数 $\\hat{\\beta} \\in \\mathbb{R}^N$，该系数解决了最小化问题 $\\min_{\\beta} \\|y_{\\mathrm{train}} - X_{\\mathrm{train}}\\beta\\|_2^2$。为保证稳定性和效率，此问题通过标准的线性最小二乘算法进行数值求解。\n\n解码器的性能在未见过的测试数据上进行评估。我们生成预测值 $\\hat{y}_{\\mathrm{test}} = X_{\\mathrm{test}}\\hat{\\beta}$，并计算决定系数 $R^2$，其定义为：\n$$\nR^2 = 1 - \\frac{\\sum_{j \\in \\mathrm{test}} (y_j - \\hat{y}_j)^2}{\\sum_{j \\in \\mathrm{test}} (y_j - \\bar{y}_{\\mathrm{test}})^2}\n$$\n其中 $\\bar{y}_{\\mathrm{test}}$ 是测试集中真实刺激值的均值。\n\n对于每个满足预算约束的候选 $\\Delta t$，重复整个过程。最优时间窗宽度 $\\Delta t^*$ 是产生最高 $R^2$ 值的那个。如果出现 $R^2$ 值相同的情况，则选择并列候选中最小的 $\\Delta t$。该程序对问题陈述中指定的每个测试用例独立应用。最终输出为最优时间窗宽度 $\\Delta t^*$ 的列表，结果保留三位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(params):\n    \"\"\"\n    Runs the full simulation and analysis for a single test case to find the optimal bin width.\n    \"\"\"\n    # Unpack parameters for the current test case\n    N = params['N']\n    T_total = params['T_total']\n    delta = params['delta']\n    dt_candidates = params['dt_candidates']\n    B = params['B']\n    seed = params['seed']\n    \n    # Fixed simulation parameters as per problem description\n    phi = 0.99\n    sigma_eta = 0.05\n    a_dist_params = (-0.6, 0.6)\n    b_dist_params = (np.log(5), 0.2)\n    f_train = 0.8\n    \n    # Initialize a random number generator with the specified seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # Step 1: Simulate the latent stimulus s(t) as an AR(1) process\n    num_samples = int(T_total / delta)\n    s = np.zeros(num_samples)\n    eta = rng.normal(loc=0.0, scale=sigma_eta, size=num_samples)\n    for k in range(num_samples - 1):\n        s[k + 1] = phi * s[k] + eta[k]\n\n    # Step 2: Simulate neuron-specific parameters a_i and b_i\n    a = rng.uniform(a_dist_params[0], a_dist_params[1], size=N)\n    b = rng.normal(loc=b_dist_params[0], scale=b_dist_params[1], size=N)\n\n    # Step 3: Calculate instantaneous firing rates r_i(t)\n    rates_full_res = np.exp(b[:, np.newaxis] + a[:, np.newaxis] * s)\n\n    # Store results for each valid candidate dt\n    valid_results = []\n\n    # Step 4: Iterate over each candidate bin width\n    for dt in dt_candidates:\n        samples_per_bin = int(round(dt / delta))\n        num_bins = int(round(T_total / dt))\n        num_train_bins = int(num_bins * f_train)\n        \n        # Calculate computational cost and check against the budget\n        cost = float(N**2 * num_train_bins) + float(N * num_train_bins) + float(N**3)\n        if cost  B:\n            continue\n\n        # Step 5: Binning process to create design matrix X and target vector y\n        \n        # Target vector y: average stimulus in each bin\n        total_samples_used = num_bins * samples_per_bin\n        s_binned = s[:total_samples_used].reshape(num_bins, samples_per_bin).mean(axis=1)\n\n        # Design matrix X: binned spike counts from Poisson process\n        rates_binnable = rates_full_res[:, :total_samples_used]\n        rates_reshaped = rates_binnable.reshape(N, num_bins, samples_per_bin)\n        \n        # Expected counts (lambda) per bin using Riemann sum approximation\n        lambda_bin = np.sum(rates_reshaped, axis=2) * delta\n        \n        # Sampled spike counts\n        spike_counts = rng.poisson(lambda_bin) # Shape (N, num_bins)\n        \n        X = spike_counts.T # Shape (num_bins, N)\n        y = s_binned       # Shape (num_bins,)\n\n        # Step 6: Split data into training and testing sets\n        X_train, X_test = X[:num_train_bins, :], X[num_train_bins:, :]\n        y_train, y_test = y[:num_train_bins], y[num_train_bins:]\n        \n        r2 = -np.inf # Default to worst possible score\n        if X_train.shape[0]  0 and X_test.shape[0]  0:\n            # Step 7: Fit OLS model on the training data\n            try:\n                beta = np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n                \n                # Step 8: Evaluate model on test data using R^2\n                y_pred = X_test @ beta\n                \n                ss_total = np.sum((y_test - y_test.mean())**2)\n                if np.isclose(ss_total, 0):\n                    ss_res = np.sum((y_test - y_pred)**2)\n                    r2 = 1.0 if np.isclose(ss_res, 0) else -np.inf\n                else:\n                    ss_res = np.sum((y_test - y_pred)**2)\n                    r2 = 1.0 - ss_res / ss_total\n            except np.linalg.LinAlgError:\n                # This should not occur with np.linalg.lstsq, but is included for robustness\n                r2 = -np.inf\n\n        valid_results.append({'dt': dt, 'r2': r2})\n\n    # Step 9: Select the optimal dt\n    if not valid_results:\n        # This case implies no candidate dt satisfied the budget.\n        # This is not expected for the given test cases. Return None if it occurs.\n        return None \n    \n    # Sort results first by R^2 (descending), then by dt (ascending) for tie-breaking\n    best_result = sorted(valid_results, key=lambda x: (-x['r2'], x['dt']))[0]\n    \n    return best_result['dt']\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'N': 40, 'T_total': 100, 'delta': 0.01, 'dt_candidates': [0.02, 0.05, 0.1, 0.2, 0.5], 'B': 7000000, 'seed': 12345},\n        {'N': 80, 'T_total': 100, 'delta': 0.01, 'dt_candidates': [0.02, 0.05, 0.1, 0.2, 0.5], 'B': 5000000, 'seed': 54321},\n        {'N': 30, 'T_total': 200, 'delta': 0.01, 'dt_candidates': [0.01, 0.02, 0.05, 0.1], 'B': 100000000, 'seed': 2023},\n        {'N': 60, 'T_total': 50, 'delta': 0.01, 'dt_candidates': [0.02, 0.05, 0.1, 0.2, 0.5], 'B': 1000000, 'seed': 777}\n    ]\n\n    results = []\n    for params in test_cases:\n        optimal_dt = run_simulation(params)\n        if optimal_dt is not None:\n             results.append(f\"{optimal_dt:.3f}\")\n        else:\n             # Handle the unlikely case where no valid dt is found\n             results.append(\"nan\")\n\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}