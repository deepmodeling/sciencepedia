{
    "hands_on_practices": [
        {
            "introduction": "The first crucial step in many neural decoding pipelines is converting raw, continuous-time spike trains into a set of discrete features suitable for a regression model. This exercise explores the most common approach—binning spike counts over a window of width $\\Delta t$—by treating spiking as a Poisson process. By deriving the mean and variance of these spike count features, you will uncover the fundamental relationship between your choice of bin width and the feature's signal-to-noise ratio, a key determinant of decoding performance.",
            "id": "4190069",
            "problem": "A single cortical neuron is recorded while a subject performs a continuous sensorimotor task. A regression-based decoding model predicts a continuous behavioral variable $y$ from features constructed as binned spike counts. Specifically, for each trial and each neuron, a feature $N$ is the number of spikes observed in a non-overlapping time bin of width $\\Delta t$. Across repeated trials under a stationary condition within the bin, assume the neuron’s spiking obeys a homogeneous Poisson process with constant rate $\\lambda$ (in spikes per second) over the bin.\n\nWorking from first principles for a homogeneous Poisson process, derive the expressions for the expected value and variance of the count feature $N$ as functions of $\\lambda$ and $\\Delta t$. Then, define the feature’s signal-to-noise ratio (SNR) for decoding as the ratio of the feature’s expected value to its standard deviation,\n$$\n\\mathrm{SNR}_{\\text{feat}} \\equiv \\frac{\\mathbb{E}[N]}{\\sqrt{\\mathrm{Var}(N)}}.\n$$\nUsing your derived expressions, produce a closed-form analytic expression for $\\mathrm{SNR}_{\\text{feat}}$ in terms of $\\lambda$ and $\\Delta t$ only. Express your final answer as a symbolic expression. No rounding is required, and do not include units in the final expression.",
            "solution": "The number of spikes $N$ in a time bin of width $\\Delta t$ from a neuron firing with a constant rate $\\lambda$ is described by a Poisson process. The resulting spike count $N$ follows a Poisson distribution with parameter $\\mu = \\lambda \\Delta t$. The probability mass function (PMF) for $N$ is given by:\n$$ P(N=k) = \\frac{\\mu^k e^{-\\mu}}{k!}, \\quad \\text{for } k = 0, 1, 2, \\dots $$\n\n**1. Expected Value $\\mathbb{E}[N]$**\nThe expected value of a Poisson-distributed random variable is its parameter $\\mu$. We can derive this from first principles:\n$$ \\mathbb{E}[N] = \\sum_{k=0}^{\\infty} k \\cdot P(N=k) = \\sum_{k=0}^{\\infty} k \\frac{\\mu^k e^{-\\mu}}{k!} $$\nThe $k=0$ term is zero, so we can start the sum from $k=1$:\n$$ \\mathbb{E}[N] = \\sum_{k=1}^{\\infty} \\frac{\\mu^k e^{-\\mu}}{(k-1)!} = \\mu e^{-\\mu} \\sum_{k=1}^{\\infty} \\frac{\\mu^{k-1}}{(k-1)!} $$\nLet $j = k-1$. The sum becomes the Taylor series for $e^\\mu$:\n$$ \\mathbb{E}[N] = \\mu e^{-\\mu} \\sum_{j=0}^{\\infty} \\frac{\\mu^j}{j!} = \\mu e^{-\\mu} e^{\\mu} = \\mu $$\nSubstituting $\\mu = \\lambda \\Delta t$, we get:\n$$ \\mathbb{E}[N] = \\lambda \\Delta t $$\n\n**2. Variance $\\mathrm{Var}(N)$**\nThe variance of a Poisson-distributed random variable is also its parameter $\\mu$. We can derive this using the relation $\\mathrm{Var}(N) = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2$. We first compute $\\mathbb{E}[N(N-1)]$:\n$$ \\mathbb{E}[N(N-1)] = \\sum_{k=0}^{\\infty} k(k-1) \\frac{\\mu^k e^{-\\mu}}{k!} $$\nThe terms for $k=0$ and $k=1$ are zero, so we start the sum from $k=2$:\n$$ \\mathbb{E}[N(N-1)] = \\sum_{k=2}^{\\infty} \\frac{\\mu^k e^{-\\mu}}{(k-2)!} = \\mu^2 e^{-\\mu} \\sum_{k=2}^{\\infty} \\frac{\\mu^{k-2}}{(k-2)!} $$\nLet $j=k-2$. The sum again becomes the Taylor series for $e^\\mu$:\n$$ \\mathbb{E}[N(N-1)] = \\mu^2 e^{-\\mu} \\sum_{j=0}^{\\infty} \\frac{\\mu^j}{j!} = \\mu^2 e^{-\\mu} e^{\\mu} = \\mu^2 $$\nNow we find $\\mathbb{E}[N^2]$:\n$$ \\mathbb{E}[N^2] = \\mathbb{E}[N(N-1) + N] = \\mathbb{E}[N(N-1)] + \\mathbb{E}[N] = \\mu^2 + \\mu $$\nFinally, we can compute the variance:\n$$ \\mathrm{Var}(N) = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2 = (\\mu^2 + \\mu) - \\mu^2 = \\mu $$\nSubstituting $\\mu = \\lambda \\Delta t$, we get:\n$$ \\mathrm{Var}(N) = \\lambda \\Delta t $$\n\n**3. Signal-to-Noise Ratio (SNR)**\nUsing the derived expressions for the mean and variance, we can find the feature's signal-to-noise ratio:\n$$ \\mathrm{SNR}_{\\text{feat}} = \\frac{\\mathbb{E}[N]}{\\sqrt{\\mathrm{Var}(N)}} = \\frac{\\lambda \\Delta t}{\\sqrt{\\lambda \\Delta t}} = \\sqrt{\\lambda \\Delta t} $$\nThis shows that the SNR of a simple spike count feature scales with the square root of both the firing rate and the bin width.",
            "answer": "$$\n\\boxed{\\sqrt{\\lambda \\Delta t}}\n$$"
        },
        {
            "introduction": "Once features are extracted, a linear decoder can be fit using ordinary least squares (OLS). However, neural populations often exhibit correlated activity, a statistical property known as collinearity, which can destabilize the decoder. This practice delves into the mathematical consequences of this issue , guiding you to derive the worst-case predictive variance and revealing how the geometric structure of the neural features, captured by the eigenvalues of the $X^{\\top}X$ matrix, directly impacts the reliability of the decoder.",
            "id": "4189997",
            "problem": "A neural decoding laboratory seeks to reconstruct a one-dimensional kinematic variable from a population of $p$ simultaneously recorded neurons using linear regression-based decoding. In each trial $t \\in \\{1,\\dots,n\\}$, the observed kinematic scalar is modeled by the linear model $y_{t} = x_{t}^{\\top}\\beta^{\\star} + \\varepsilon_{t}$, where $x_{t} \\in \\mathbb{R}^{p}$ is the vector of preprocessed neural covariates, $\\beta^{\\star} \\in \\mathbb{R}^{p}$ is the true decoding weight vector, and $\\varepsilon_{t}$ are independent noise terms satisfying $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$. The design matrix is $X \\in \\mathbb{R}^{n \\times p}$ with rows $x_{t}^{\\top}$, and the ordinary least squares (OLS) estimator of the weights is used to construct the decoder. Consider the predictive output of this decoder at a new unit-norm covariate $x_{\\mathrm{new}} \\in \\mathbb{R}^{p}$ with $\\|x_{\\mathrm{new}}\\|=1$.\n\nStarting from fundamental properties of the linear model, and using spectral decomposition of $X^{\\top}X$, derive, from first principles, the worst-case predictive variance of the linear decoder output $x_{\\mathrm{new}}^{\\top}\\hat{\\beta}$ over all unit-norm $x_{\\mathrm{new}}$. Then evaluate this worst-case predictive variance when the eigenvalues of $X^{\\top}X$ are given by $(\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4})=(500,2,1,0.05)$ and the noise variance is $\\sigma^{2}=0.2$. Round your final numerical answer to four significant figures. Provide the final answer as a single real number without units.",
            "solution": "The problem statement has been validated and is deemed sound, self-contained, and scientifically grounded. We will proceed with a full derivation.\n\nThe problem asks for the worst-case predictive variance of a linear decoder. Let us begin by establishing the statistical properties of the ordinary least squares (OLS) estimator $\\hat{\\beta}$. The linear model is given in vector form as $y = X\\beta^{\\star} + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the vector of observations, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta^{\\star} \\in \\mathbb{R}^{p}$ is the true weight vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ is the vector of noise terms. The noise terms are independent and identically distributed as $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$, which implies that the noise vector $\\varepsilon$ has a multivariate normal distribution $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix.\n\nThe OLS estimator $\\hat{\\beta}$ is given by the formula:\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y $$\nSubstituting the model for $y$, we can express $\\hat{\\beta}$ in terms of the true parameter $\\beta^{\\star}$ and the noise $\\varepsilon$:\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}(X\\beta^{\\star} + \\varepsilon) = (X^{\\top}X)^{-1}X^{\\top}X\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon = \\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon $$\nThe design matrix $X$ is considered fixed (non-random). The only random component in this expression for $\\hat{\\beta}$ is the noise vector $\\varepsilon$.\n\nFirst, we determine the expected value of $\\hat{\\beta}$:\n$$ \\mathrm{E}[\\hat{\\beta}] = \\mathrm{E}[\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon] = \\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\mathrm{E}[\\varepsilon] $$\nSince $\\mathrm{E}[\\varepsilon] = 0$, we have $\\mathrm{E}[\\hat{\\beta}] = \\beta^{\\star}$. This confirms that the OLS estimator is unbiased.\n\nNext, we derive the covariance matrix of $\\hat{\\beta}$. The covariance matrix is defined as $\\mathrm{Cov}(\\hat{\\beta}) = \\mathrm{E}[(\\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}])(\\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}])^{\\top}]$.\n$$ \\hat{\\beta} - \\mathrm{E}[\\hat{\\beta}] = (X^{\\top}X)^{-1}X^{\\top}\\varepsilon $$\n$$ \\mathrm{Cov}(\\hat{\\beta}) = \\mathrm{E}[((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)^{\\top}] = \\mathrm{E}[(X^{\\top}X)^{-1}X^{\\top}\\varepsilon\\varepsilon^{\\top}X((X^{\\top}X)^{-1})^{\\top}] $$\nSince $X$ is non-random, we can move it outside the expectation:\n$$ \\mathrm{Cov}(\\hat{\\beta}) = (X^{\\top}X)^{-1}X^{\\top}\\mathrm{E}[\\varepsilon\\varepsilon^{\\top}]X((X^{\\top}X)^{-1})^{\\top} $$\nThe term $\\mathrm{E}[\\varepsilon\\varepsilon^{\\top}]$ is the covariance matrix of the noise, which is $\\mathrm{Cov}(\\varepsilon) = \\sigma^2 I_n$. The matrix $X^{\\top}X$ is symmetric, so its inverse is also symmetric.\n$$ \\mathrm{Cov}(\\hat{\\beta}) = (X^{\\top}X)^{-1}X^{\\top}(\\sigma^2 I_n)X(X^{\\top}X)^{-1} = \\sigma^2 (X^{\\top}X)^{-1}X^{\\top}X(X^{\\top}X)^{-1} = \\sigma^2 (X^{\\top}X)^{-1} $$\nThis is a fundamental result for the OLS estimator.\n\nThe problem requires us to analyze the predictive output of the decoder for a new covariate vector $x_{\\mathrm{new}} \\in \\mathbb{R}^p$. The predicted value (decoder output) is $\\hat{y}_{\\mathrm{new}} = x_{\\mathrm{new}}^{\\top}\\hat{\\beta}$. We need to find the variance of this prediction. Using the general property for the variance of a linear transformation of a random vector, $\\mathrm{Var}(A z) = A \\mathrm{Cov}(z) A^{\\top}$, with $A = x_{\\mathrm{new}}^{\\top}$ and $z = \\hat{\\beta}$:\n$$ \\mathrm{Var}(x_{\\mathrm{new}}^{\\top}\\hat{\\beta}) = x_{\\mathrm{new}}^{\\top} \\mathrm{Cov}(\\hat{\\beta}) x_{\\mathrm{new}} $$\nSubstituting the expression for $\\mathrm{Cov}(\\hat{\\beta})$:\n$$ \\mathrm{Var}(x_{\\mathrm{new}}^{\\top}\\hat{\\beta}) = x_{\\mathrm{new}}^{\\top} (\\sigma^2 (X^{\\top}X)^{-1}) x_{\\mathrm{new}} = \\sigma^2 x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} $$\nThe task is to find the worst-case (maximum) predictive variance over all possible unit-norm covariate vectors $x_{\\mathrm{new}}$. The condition is given as $\\|x_{\\mathrm{new}}\\|=1$, which in this context means the Euclidean norm is $1$, i.e., $\\|x_{\\mathrm{new}}\\|_2 = 1$. The problem is thus to solve the following optimization:\n$$ \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( \\sigma^2 x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) = \\sigma^2 \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) $$\nThe expression $x^{\\top} M x$ for a symmetric matrix $M$ and a unit vector $x$ is known as a Rayleigh quotient. The maximum value of the Rayleigh quotient is the largest eigenvalue of the matrix $M$. In our case, $M = (X^{\\top}X)^{-1}$.\nSo, we have:\n$$ \\max_{\\|x_{\\mathrm{new}}\\|_2=1} \\left( x_{\\mathrm{new}}^{\\top} (X^{\\top}X)^{-1} x_{\\mathrm{new}} \\right) = \\lambda_{\\max}((X^{\\top}X)^{-1}) $$\nLet the eigenvalues of the matrix $X^{\\top}X$ be denoted by $\\lambda_i$. Since $X^{\\top}X$ is a symmetric positive definite matrix, its eigenvalues are real and positive. The eigenvalues of the inverse matrix, $(X^{\\top}X)^{-1}$, are the reciprocals of the eigenvalues of $X^{\\top}X$, i.e., $1/\\lambda_i$.\nTherefore, the largest eigenvalue of $(X^{\\top}X)^{-1}$ is the reciprocal of the smallest eigenvalue of $X^{\\top}X$:\n$$ \\lambda_{\\max}((X^{\\top}X)^{-1}) = \\max_{i} \\left( \\frac{1}{\\lambda_i} \\right) = \\frac{1}{\\min_{i}(\\lambda_i)} = \\frac{1}{\\lambda_{\\min}(X^{\\top}X)} $$\nSubstituting this back into the expression for the worst-case variance:\n$$ \\text{Worst-case variance} = \\sigma^2 \\cdot \\frac{1}{\\lambda_{\\min}(X^{\\top}X)} $$\nThe worst-case direction for $x_{\\mathrm{new}}$ corresponds to the eigenvector of $X^{\\top}X$ associated with its smallest eigenvalue, $\\lambda_{\\min}$.\n\nNow we can evaluate this expression using the provided numerical values.\nThe eigenvalues of $X^{\\top}X$ are given as $(\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4) = (500, 2, 1, 0.05)$.\nThe smallest of these eigenvalues is:\n$$ \\lambda_{\\min}(X^{\\top}X) = \\min\\{500, 2, 1, 0.05\\} = 0.05 $$\nThe noise variance is given as $\\sigma^2 = 0.2$.\n\nPlugging these values into our derived formula for the worst-case predictive variance:\n$$ \\text{Worst-case variance} = \\frac{\\sigma^2}{\\lambda_{\\min}(X^{\\top}X)} = \\frac{0.2}{0.05} = 4 $$\nThe problem requires the answer to be rounded to four significant figures. The exact answer is $4$. Expressed with four significant figures, this is $4.000$.",
            "answer": "$$\n\\boxed{4.000}\n$$"
        },
        {
            "introduction": "Having identified the problem of variance inflation caused by collinearity , we now explore a powerful and widely used solution: ridge regression. This comprehensive exercise  guides you through the complete analytical treatment of a regularized decoder, from deriving its mathematical form to decomposing its prediction error into bias and variance components. By analyzing this bias-variance trade-off, you will understand how the regularization parameter $\\lambda$ provides a principled way to build stable and accurate decoders, even with correlated or high-dimensional neural data.",
            "id": "5002219",
            "problem": "A laboratory is developing a Brain-Computer Interface (BCI) decoder that maps neuronal firing rates to the hand’s scalar tangential velocity during a center-out reaching task. In each time bin indexed by $t \\in \\{1,\\ldots,N\\}$ of width $\\Delta t$, the preprocessed firing rate vector is $\\mathbf{r}_{t} \\in \\mathbb{R}^{p}$ (already z-scored and whitened across neurons so that the sample covariance is the identity), and the simultaneously measured hand velocity is $v_{t} \\in \\mathbb{R}$. Assume a linear-Gaussian encoding model $v_{t} = \\mathbf{r}_{t}^{\\top} \\boldsymbol{\\beta} + \\varepsilon_{t}$ where $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$ are independent across $t$ and independent of $\\mathbf{r}_{t}$. Stack the data into the design matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$ with rows $\\mathbf{r}_{t}^{\\top}$ and target vector $\\mathbf{y} \\in \\mathbb{R}^{N}$ with entries $v_{t}$. Because the neuronal features were whitened using the training data, you may assume the empirical second moment satisfies $\\mathbf{X}^{\\top}\\mathbf{X} = N \\mathbf{I}_{p}$.\n\nYou train a ridge regression decoder by minimizing the penalized least-squares objective\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2},\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter and $\\|\\cdot\\|$ denotes the Euclidean norm.\n\nTasks:\n1) Starting from the model $ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{N})$, derive the closed-form solution $\\widehat{\\mathbf{w}}$ that minimizes $J(\\mathbf{w};\\lambda)$.\n\n2) For a new, independent test sample $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$ drawn from the same distribution, with $\\mathbf{r}_{\\mathrm{new}}$ independent of training data and satisfying $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}] = \\mathbf{0}$ and $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$, derive the expected out-of-sample mean squared prediction error\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]\n$$\nas an explicit function of $\\lambda$, $N$, $p$, $\\sigma^{2}$, and $\\boldsymbol{\\beta}$. Your derivation must start from the definitions above and the assumptions on $\\mathbf{X}$ and $\\mathbf{r}_{\\mathrm{new}}$, and it must expose the bias-variance decomposition that depends on $\\lambda$.\n\n3) To make the trade-off explicit and independent of a particular unknown $\\boldsymbol{\\beta}$, assume a hierarchical prior consistent with neural population codes: $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$ with $\\tau^{2} > 0$. Average your expression for $\\mathcal{E}(\\lambda)$ over this prior and simplify to a scalar function of $\\lambda$, $N$, $p$, $\\sigma^{2}$, and $\\tau^{2}$.\n\n4) Using your averaged expression, determine the value $\\lambda^{\\star}$ that minimizes the expected out-of-sample mean squared prediction error. Then, evaluate this optimum numerically for\n- $p = 100$,\n- $N = 10000$,\n- $\\sigma^{2} = 0.04$,\n- $\\tau^{2} = 0.01$.\nExpress the final value of $\\lambda^{\\star}$ as a pure number without units. If rounding is necessary, round to four significant figures. If not, provide the exact value.",
            "solution": "The problem asks for a multi-step analysis of a ridge regression decoder in the context of a Brain-Computer Interface (BCI). The analysis involves deriving the decoder, its out-of-sample error, and the optimal regularization parameter under a specific data model and prior.\n\n### Task 1: Derivation of the Ridge Regression Estimator $\\widehat{\\mathbf{w}}$\nThe ridge regression estimator $\\widehat{\\mathbf{w}}$ is found by minimizing the objective function\n$$\nJ(\\mathbf{w};\\lambda) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^{2} + \\lambda \\|\\mathbf{w}\\|^{2}\n$$\nwhere $\\|\\cdot\\|$ is the Euclidean norm. We can write the squared norms in terms of vector transposes:\n$$\nJ(\\mathbf{w};\\lambda) = (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^{\\top}(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\nExpanding the first term gives:\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - \\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w} - \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{w} + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\nSince $\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y}$ is a scalar, it equals its transpose $\\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{w}$. Thus, we can combine the cross-terms:\n$$\nJ(\\mathbf{w};\\lambda) = \\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf{y} + \\mathbf{w}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\nTo find the minimum, we take the gradient of $J(\\mathbf{w};\\lambda)$ with respect to $\\mathbf{w}$ and set it to zero. Using standard matrix calculus results ($\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{a} = \\mathbf{a}$ and $\\nabla_{\\mathbf{w}} \\mathbf{w}^{\\top}\\mathbf{M}\\mathbf{w} = 2\\mathbf{M}\\mathbf{w}$ for symmetric $\\mathbf{M}$):\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w};\\lambda) = -2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\mathbf{w}\n$$\nSetting the gradient to the zero vector gives the solution $\\widehat{\\mathbf{w}}$:\n$$\n-2\\mathbf{X}^{\\top}\\mathbf{y} + 2(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{0}\n$$\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})\\widehat{\\mathbf{w}} = \\mathbf{X}^{\\top}\\mathbf{y}\n$$\nThe formal solution is $\\widehat{\\mathbf{w}} = (\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$. The problem states the assumption that the empirical second moment is $\\mathbf{X}^{\\top}\\mathbf{X} = N\\mathbf{I}_{p}$. Substituting this into the expression for $\\widehat{\\mathbf{w}}$:\n$$\n\\widehat{\\mathbf{w}} = (N\\mathbf{I}_{p} + \\lambda\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = ((N+\\lambda)\\mathbf{I}_{p})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{I}_{p}^{-1}\\mathbf{X}^{\\top}\\mathbf{y} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\nThis is the closed-form solution for $\\widehat{\\mathbf{w}}$ under the given assumption.\n\n### Task 2: Expected Out-of-Sample Mean Squared Prediction Error\nWe are asked to derive $\\mathcal{E}(\\lambda) = \\mathbb{E}\\big[(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2}\\big]$. The expectation is taken over the randomness in the training data (which makes $\\widehat{\\mathbf{w}}$ random) and the new test sample $(\\mathbf{r}_{\\mathrm{new}}, v_{\\mathrm{new}})$.\nThe true model for the new sample is $v_{\\mathrm{new}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}$, where $\\varepsilon_{\\mathrm{new}} \\sim \\mathcal{N}(0,\\sigma^{2})$. Substituting this into the error term:\n$$\nv_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}\\boldsymbol{\\beta} + \\varepsilon_{\\mathrm{new}}) - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}} = \\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}\n$$\nSquaring this expression:\n$$\n(v_{\\mathrm{new}} - \\mathbf{r}_{\\mathrm{new}}^{\\top}\\widehat{\\mathbf{w}})^{2} = (\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2} + 2\\varepsilon_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}) + \\varepsilon_{\\mathrm{new}}^{2}\n$$\nNow we take the expectation. The new noise term $\\varepsilon_{\\mathrm{new}}$ is independent of the training data (and thus $\\widehat{\\mathbf{w}}$) and the new features $\\mathbf{r}_{\\mathrm{new}}$. Since $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}] = 0$, the cross-term vanishes. We have $\\mathbb{E}[\\varepsilon_{\\mathrm{new}}^{2}] = \\sigma^{2}$.\n$$\n\\mathcal{E}(\\lambda) = \\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] + \\sigma^{2}\n$$\nThe remaining expectation is over $\\mathbf{r}_{\\mathrm{new}}$ and $\\widehat{\\mathbf{w}}$. We can rewrite the term inside the expectation using the trace trick: $(\\mathbf{a}^{\\top}\\mathbf{b})^2 = \\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b} = \\mathrm{tr}(\\mathbf{b}^{\\top}\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}) = \\mathrm{tr}(\\mathbf{a}\\mathbf{a}^{\\top}\\mathbf{b}\\mathbf{b}^{\\top})$. It is simpler to use $\\mathbb{E}[x^2] = \\mathbb{E}[\\mathrm{tr}(x^2)]$ where $x$ is a scalar.\n$$\n\\mathbb{E}\\left[(\\mathbf{r}_{\\mathrm{new}}^{\\top}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}))^{2}\\right] = \\mathbb{E}\\left[\\mathrm{tr}\\left((\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top} (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right)\\right]\n$$\nBy linearity of trace and expectation, and since $\\widehat{\\mathbf{w}}$ (from training data) is independent of $\\mathbf{r}_{\\mathrm{new}}$:\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top} \\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] (\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right)\n$$\nUsing the assumption $\\mathbb{E}[\\mathbf{r}_{\\mathrm{new}}\\mathbf{r}_{\\mathrm{new}}^{\\top}] = \\mathbf{I}_{p}$:\n$$\n= \\mathrm{tr}\\left(\\mathbb{E}\\left[(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})^{\\top}\\mathbf{I}_{p}(\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}})\\right]\\right) = \\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right]\n$$\nSo, $\\mathcal{E}(\\lambda) = \\mathbb{E}[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}] + \\sigma^2$. The expectation $\\mathbb{E}[\\cdot]$ is now only over the training data randomness. We now perform a bias-variance decomposition:\n$$\n\\mathbb{E}\\left[\\|\\boldsymbol{\\beta} - \\widehat{\\mathbf{w}}\\|^{2}\\right] = \\left\\|\\boldsymbol{\\beta} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\right\\|^{2} + \\mathbb{E}\\left[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^{2}\\right] = \\text{Bias}(\\widehat{\\mathbf{w}})^{2} + \\text{Var}(\\widehat{\\mathbf{w}})\n$$\nWe derive the bias and variance terms. First, substitute $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ into the expression for $\\widehat{\\mathbf{w}}$:\n$$\n\\widehat{\\mathbf{w}} = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}) = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\n$$\nThe expectation of $\\widehat{\\mathbf{w}}$ (over $\\boldsymbol{\\varepsilon}$) is:\n$$\n\\mathbb{E}[\\widehat{\\mathbf{w}}] = \\mathbb{E}\\left[\\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon})\\right] = \\frac{1}{N+\\lambda}(N\\boldsymbol{\\beta} + \\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}]) = \\frac{N}{N+\\lambda}\\boldsymbol{\\beta}\n$$\nThe squared bias is:\n$$\n\\text{Bias}(\\widehat{\\mathbf{w}})^{2} = \\left\\|\\mathbb{E}[\\widehat{\\mathbf{w}}] - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|\\frac{N}{N+\\lambda}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}\\right\\|^{2} = \\left\\|-\\frac{\\lambda}{N+\\lambda}\\boldsymbol{\\beta}\\right\\|^{2} = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2}\n$$\nThe variance term is $\\mathbb{E}[\\|\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}]\\|^2] = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}}))$.\n$$\n\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}] = \\frac{1}{N+\\lambda}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\n$$\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\mathbb{E}\\left[(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])(\\widehat{\\mathbf{w}} - \\mathbb{E}[\\widehat{\\mathbf{w}}])^{\\top}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbb{E}\\left[\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{X}\\right] = \\frac{1}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]\\mathbf{X}\n$$\nUsing $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}]=\\sigma^2\\mathbf{I}_N$ and $\\mathbf{X}^{\\top}\\mathbf{X}=N\\mathbf{I}_p$:\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{w}}) = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{I}_{N}\\mathbf{X} = \\frac{\\sigma^2}{(N+\\lambda)^2}\\mathbf{X}^{\\top}\\mathbf{X} = \\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\n$$\nThe variance is the trace of this covariance matrix:\n$$\n\\text{Var}(\\widehat{\\mathbf{w}}) = \\mathrm{tr}(\\mathrm{Cov}(\\widehat{\\mathbf{w}})) = \\mathrm{tr}\\left(\\frac{N\\sigma^2}{(N+\\lambda)^2}\\mathbf{I}_{p}\\right) = \\frac{pN\\sigma^2}{(N+\\lambda)^2}\n$$\nCombining all terms, the expected out-of-sample error is:\n$$\n\\mathcal{E}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}\\|\\boldsymbol{\\beta}\\|^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### Task 3: Averaging Over the Prior for $\\boldsymbol{\\beta}$\nWe are given a prior distribution over the true weights, $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{p})$. We need to compute $\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\mathbb{E}_{\\boldsymbol{\\beta}}[\\mathcal{E}(\\lambda)]$. The only term in $\\mathcal{E}(\\lambda)$ that depends on $\\boldsymbol{\\beta}$ is $\\|\\boldsymbol{\\beta}\\|^{2}$. We compute its expectation under the prior:\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\sum_{i=1}^{p}\\beta_{i}^{2}\\right] = \\sum_{i=1}^{p}\\mathbb{E}_{\\boldsymbol{\\beta}}[\\beta_{i}^{2}]\n$$\nFor each component $\\beta_i \\sim \\mathcal{N}(0, \\tau^{2})$, the second moment is $\\mathbb{E}[\\beta_{i}^{2}] = \\mathrm{Var}(\\beta_i) + (\\mathbb{E}[\\beta_i])^{2} = \\tau^{2} + 0^{2} = \\tau^{2}$.\nTherefore, the expected squared norm is:\n$$\n\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\|\\boldsymbol{\\beta}\\|^{2}\\right] = \\sum_{i=1}^{p}\\tau^{2} = p\\tau^{2}\n$$\nSubstituting this into the expression for $\\mathcal{E}(\\lambda)$:\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\left(\\frac{\\lambda}{N+\\lambda}\\right)^{2}p\\tau^{2} + \\frac{pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n$$\n\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{p\\lambda^{2}\\tau^{2} + pN\\sigma^2}{(N+\\lambda)^2} + \\sigma^{2}\n$$\n\n### Task 4: Optimal Regularization Parameter $\\lambda^{\\star}$\nTo find the $\\lambda$ that minimizes $\\mathcal{E}_{\\mathrm{avg}}(\\lambda)$, we differentiate with respect to $\\lambda$ and set the derivative to zero. The constant term $\\sigma^{2}$ can be ignored during minimization.\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{d}{d\\lambda}\\left( \\frac{p(\\lambda^{2}\\tau^{2} + N\\sigma^2)}{(N+\\lambda)^2} \\right)\n$$\nUsing the quotient rule $\\frac{d}{dx}(\\frac{u}{v}) = \\frac{u'v - uv'}{v^2}$:\nLet $u(\\lambda) = p(\\lambda^{2}\\tau^{2} + N\\sigma^2)$ and $v(\\lambda) = (N+\\lambda)^2$.\nThen $u'(\\lambda) = 2p\\lambda\\tau^{2}$ and $v'(\\lambda) = 2(N+\\lambda)$.\n$$\n\\frac{d}{d\\lambda}\\mathcal{E}_{\\mathrm{avg}}(\\lambda) = \\frac{(2p\\lambda\\tau^{2})(N+\\lambda)^2 - p(\\lambda^{2}\\tau^{2} + N\\sigma^2) \\cdot 2(N+\\lambda)}{(N+\\lambda)^4}\n$$\nSetting the derivative to zero and assuming $\\lambda \\ge 0, N \\ge 1$, we can simplify by dividing by the non-zero factor $2p(N+\\lambda)$:\n$$\n(\\lambda\\tau^{2})(N+\\lambda) - (\\lambda^{2}\\tau^{2} + N\\sigma^2) = 0\n$$\n$$\nN\\lambda\\tau^{2} + \\lambda^{2}\\tau^{2} - \\lambda^{2}\\tau^{2} - N\\sigma^2 = 0\n$$\n$$\nN\\lambda\\tau^{2} = N\\sigma^2\n$$\nSince $N \\ge 1$ and $\\tau^{2} > 0$ (given), we can divide by $N\\tau^{2}$ to find the optimal $\\lambda^{\\star}$:\n$$\n\\lambda^{\\star} = \\frac{\\sigma^2}{\\tau^2}\n$$\nThis result is elegantly simple and represents the ratio of the noise variance in the measurements to the prior variance of the model parameters. The second derivative can be checked to confirm this is a minimum. The numerator of the derivative simplified to $2pN(\\lambda\\tau^2 - \\sigma^2)(N+\\lambda)$, which is negative for $\\lambda < \\sigma^2/\\tau^2$ and positive for $\\lambda > \\sigma^2/\\tau^2$, confirming a minimum.\n\nFinally, we evaluate this expression numerically with the provided values:\n- $\\sigma^{2} = 0.04$\n- $\\tau^{2} = 0.01$\nThe values for $p$ and $N$ are not needed to find $\\lambda^{\\star}$ in this idealized setting.\n$$\n\\lambda^{\\star} = \\frac{0.04}{0.01} = 4\n$$\nThe optimal value is exactly $4$.",
            "answer": "$$\n\\boxed{4}\n$$"
        }
    ]
}