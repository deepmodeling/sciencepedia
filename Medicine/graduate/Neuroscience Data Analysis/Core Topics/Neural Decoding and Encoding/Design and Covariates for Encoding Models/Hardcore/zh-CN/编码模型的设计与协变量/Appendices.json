{
    "hands_on_practices": [
        {
            "introduction": "在我们构建编码模型的旅程中，一个基本任务是估计神经元的感受野。脉冲触发平均（Spike-Triggered Average, STA）是一种直观且计算简单的方法，但其准确性严重依赖于对输入刺激的严格假设。这项练习  将引导你通过数学推导，揭示当刺激并非理想化的白噪声时，STA估计将产生怎样的系统性偏差，从而加深你对模型假设重要性的理解。",
            "id": "4155364",
            "problem": "考虑一个单神经元的离散时间编码模型，其中时间索引为 $t$ 的刺激是一个向量 $x_t \\in \\mathbb{R}^{p}$，代表了过去 $p$ 个时间延迟的刺激历史，而脉冲由一个线性-非线性-泊松 (LNP) 模型产生，其条件强度为 $\\lambda_t = \\exp(\\alpha + k^{\\top} x_t)$，其中 $k \\in \\mathbb{R}^{p}$ 是感受野，$\\alpha \\in \\mathbb{R}$ 是一个基线参数。假设 $\\{x_t\\}$ 是从一个零均值、协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{p \\times p}$（对称正定）的多元高斯分布中独立同分布地抽取的。将脉冲触发平均 (STA) 在一个通用时刻定义为给定该时刻发生一个脉冲的条件下的条件平均刺激，即 $\\mathrm{STA} := \\mathbb{E}[x_t \\mid \\text{在 } t \\text{ 时刻有脉冲}]$。\n\n从点过程的条件期望和多元正态分布的矩生成性质等基本定义出发，推导以 $\\Sigma$ 和 $k$ 表示的 $\\mathbb{E}[\\mathrm{STA}]$。利用这个表达式，从数学上解释为什么只有当刺激是具有单位协方差的高斯白噪声时（即 $\\Sigma = I$），STA 才能产生无偏的感受野估计，并描述当 $\\Sigma \\neq I$ 时，如果直接使用 STA 来估计 $k$ 而不进行任何校正，会产生怎样的偏差。\n\n你的最终答案应该是相关刺激下偏差向量的闭式解析表达式。不需要数值近似，也不涉及单位。不要将最终答案表示为方程式；仅提供偏差的表达式。",
            "solution": "问题陈述被评估为有效。它在神经编码模型理论方面具有科学依据，设定良好，信息充分，可以进行唯一的数学推导，并且表述客观。该问题需要在计算神经科学领域进行一个标准的、尽管不简单的推导。\n\n我们的任务是推导由线性-非线性-泊松 (LNP) 模型描述的神经元的脉冲触发平均 (STA)，并分析其作为神经元感受野估计量的偏差。\n\n首先，我们形式化 STA 的定义。STA 被定义为在给定时刻 $t$ 发生一个脉冲的条件下，刺激 $x_t$ 的条件期望：\n$$\n\\mathrm{STA} := \\mathbb{E}[x_t \\mid \\text{在 } t \\text{ 时刻有脉冲}]\n$$\n使用条件期望的定义，这可以写成关于给定一个脉冲下刺激的后验分布的积分：\n$$\n\\mathrm{STA} = \\int_{\\mathbb{R}^p} x \\, p(x \\mid \\text{脉冲}) \\, dx\n$$\n根据贝叶斯法则，后验密度 $p(x \\mid \\text{脉冲})$ 与刺激的先验 $p(x)$ 以及给定刺激下脉冲的条件概率 $P(\\text{脉冲} \\mid x)$ 相关：\n$$\np(x \\mid \\text{脉冲}) = \\frac{P(\\text{脉冲} \\mid x) p(x)}{P(\\text{脉冲})}\n$$\n在泊松点过程模型中，在一个小的时间间隔内观察到脉冲的概率与瞬时发放率 $\\lambda(x)$ 成正比。因此，我们可以设定 $P(\\text{脉冲} \\mid x) \\propto \\lambda(x)$。无条件的脉冲概率 $P(\\text{脉冲})$ 则与平均发放率 $\\mathbb{E}[\\lambda(x)]$ 成正比。比例常数相互抵消，从而得到广泛使用的 STA 公式，也称为 Stein 公式：\n$$\n\\mathrm{STA} = \\frac{\\int x \\lambda(x) p(x) \\, dx}{\\int \\lambda(x) p(x) \\, dx} = \\frac{\\mathbb{E}[x \\lambda(x)]}{\\mathbb{E}[\\lambda(x)]}\n$$\n问题指明了刺激分布和 LNP 模型的分量：\n1.  刺激 $x_t$ 从一个均值为 $0$、协方差为 $\\Sigma$ 的多元高斯分布中抽取，即 $x \\sim \\mathcal{N}(0, \\Sigma)$。其概率密度函数 (PDF) 为：\n    $$\n    p(x) = \\frac{1}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} x^{\\top} \\Sigma^{-1} x\\right)\n    $$\n2.  条件强度（发放率）由 $\\lambda(x) = \\exp(\\alpha + k^{\\top} x)$ 给出，其中 $k$ 是感受野，$\\alpha$ 是一个基线参数。\n\n我们现在计算 STA 表达式的分子和分母。\n\n分母是期望发放率，$\\mathbb{E}[\\lambda(x)]$：\n$$\n\\mathbb{E}[\\lambda(x)] = \\mathbb{E}[\\exp(\\alpha + k^{\\top} x)] = \\exp(\\alpha) \\mathbb{E}[\\exp(k^{\\top} x)]\n$$\n项 $\\mathbb{E}[\\exp(k^{\\top} x)]$ 是随机变量 $x \\sim \\mathcal{N}(0, \\Sigma)$ 的矩生成函数 (MGF)，在向量 $k$ 处求值。对于多元正态分布 $\\mathcal{N}(\\mu, \\Sigma)$，其 MGF 为 $M_x(t) = \\exp(\\mu^{\\top}t + \\frac{1}{2} t^{\\top}\\Sigma t)$。在我们的例子中，$\\mu=0$，因此 MGF 为 $M_x(t) = \\exp(\\frac{1}{2} t^{\\top}\\Sigma t)$。在 $t=k$ 处求值，我们得到：\n$$\n\\mathbb{E}[\\exp(k^{\\top} x)] = \\exp\\left(\\frac{1}{2} k^{\\top}\\Sigma k\\right)\n$$\n因此，分母是：\n$$\n\\mathbb{E}[\\lambda(x)] = \\exp\\left(\\alpha + \\frac{1}{2} k^{\\top}\\Sigma k\\right)\n$$\n\n分子是刺激与发放率乘积的期望，$\\mathbb{E}[x \\lambda(x)]$：\n$$\n\\mathbb{E}[x \\lambda(x)] = \\int_{\\mathbb{R}^p} x \\, \\exp(\\alpha + k^{\\top} x) \\, p(x) \\, dx = \\exp(\\alpha) \\int_{\\mathbb{R}^p} x \\, \\exp(k^{\\top} x) \\frac{\\exp(-\\frac{1}{2} x^{\\top} \\Sigma^{-1} x)}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\, dx\n$$\n积分内指数的参数可以合并：\n$$\nk^{\\top} x - \\frac{1}{2} x^{\\top} \\Sigma^{-1} x\n$$\n我们对这个二次型进行配方。一个关键的恒等式是 $(x-v)^{\\top}A(x-v) = x^{\\top}Ax - 2v^{\\top}Ax + v^{\\top}Av$。通过设 $A=\\Sigma^{-1}$ 和 $v = \\Sigma k$，我们发现：\n$$\nx^{\\top}\\Sigma^{-1}x - 2k^{\\top}x = (x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) - k^{\\top}\\Sigma\\Sigma^{-1}\\Sigma k = (x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) - k^{\\top}\\Sigma k\n$$\n重新整理这个恒等式，我们有：\n$$\nk^{\\top}x - \\frac{1}{2}x^{\\top}\\Sigma^{-1}x = -\\frac{1}{2}\\left[(x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) - k^{\\top}\\Sigma k\\right]\n$$\n将此代入分子的积分中：\n$$\n\\mathbb{E}[x \\lambda(x)] = \\exp(\\alpha) \\int x \\frac{\\exp\\left(-\\frac{1}{2}(x-\\Sigma k)^{\\top}\\Sigma^{-1}(x-\\Sigma k) + \\frac{1}{2}k^{\\top}\\Sigma k\\right)}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\, dx\n$$\n我们可以将不依赖于 $x$ 的项提出来：\n$$\n\\mathbb{E}[x \\lambda(x)] = \\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right) \\int x \\frac{\\exp\\left(-\\frac{1}{2}(x - \\Sigma k)^{\\top}\\Sigma^{-1}(x - \\Sigma k)\\right)}{\\sqrt{(2\\pi)^p \\det(\\Sigma)}} \\, dx\n$$\n积分内的表达式是 $x$ 与一个新的多元高斯分布 $\\mathcal{N}(\\Sigma k, \\Sigma)$ 的概率密度函数 (PDF) 的乘积。因此，该积分代表从这个新分布中抽取的随机向量的期望值。该期望值就是其均值，即 $\\Sigma k$。\n于是，分子求值为：\n$$\n\\mathbb{E}[x \\lambda(x)] = \\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right) (\\Sigma k)\n$$\n\n现在我们结合分子和分母来求 STA：\n$$\n\\mathrm{STA} = \\frac{\\mathbb{E}[x \\lambda(x)]}{\\mathbb{E}[\\lambda(x)]} = \\frac{\\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right) (\\Sigma k)}{\\exp\\left(\\alpha + \\frac{1}{2}k^{\\top}\\Sigma k\\right)} = \\Sigma k\n$$\n这就得到了用感受野 $k$ 和刺激协方差 $\\Sigma$ 表示的 STA 表达式。这是问题的第一部分。\n\n接下来，我们分析 STA 作为 $k$ 的估计量。如果一个估计量的期望值等于真实参数，则该估计量是无偏的。这里，我们将可从数据计算的量 STA 视为真实感受野 $k$ 的估计量。如果 $\\mathrm{STA} = k$，则该估计量是无偏的。\n根据我们的结果，我们有 $\\mathrm{STA} = \\Sigma k$。因此，无偏估计的条件是 $\\Sigma k = k$。由于这对任意感受野 $k \\in \\mathbb{R}^p$ 都必须成立，该等式要求矩阵 $\\Sigma$ 是单位矩阵，即 $\\Sigma = I$。这对应于高斯白噪声刺激的情况，其中所有刺激维度都不相关且具有单位方差。\n\n当 $\\Sigma \\neq I$ 时，STA 是 $k$ 的一个有偏估计量。估计量的偏差定义为估计量与真实参数值之间的差。偏差向量是：\n$$\n\\text{偏差} = \\mathrm{STA} - k = \\Sigma k - k\n$$\n将 $k$ 提出来，我们得到偏差的闭式表达式：\n$$\n\\text{偏差} = (\\Sigma - I)k\n$$\n这个表达式描述了由刺激相关性引入的失真。STA 实际上是真实感受野 $k$ 被刺激的协方差结构 $\\Sigma$ “着色”后的版本。为了在一般情况下从 STA 中获得 $k$ 的无偏估计，需要对协方差矩阵求逆并计算 $k = \\Sigma^{-1} \\mathrm{STA}$，这个过程被称为 STA 的“白化”。",
            "answer": "$$\n\\boxed{(\\Sigma - I)k}\n$$"
        },
        {
            "introduction": "在掌握了基础模型后，我们转向更强大和灵活的通用线性模型（General Linear Model, GLM）。然而，在设计GLM时，一个常见的陷阱是协变量之间存在相关性，即多重共线性，这会使得模型系数的解释变得模糊不清。此项实践  将让你亲手操作，通过对相关的参数调制器进行格拉姆-施密特（Gram-Schmidt）正交化，直观地观察系数估计如何随着共享方差的重新分配而改变，这是处理和解释复杂GLM的关键实践技能。",
            "id": "4155403",
            "problem": "在神经科学数据分析中，给定一个逐试次编码模型，其中使用相关的参数调制器来解释神经响应。这些调制器被称为振幅和持续时间，它们构成两个通常相关的回归量。您的任务是通过正交化构建和分析一个编码模型，以分离每个回归量的独特贡献，然后比较原始相关设计中的系数与正交化设计中的系数。\n\n从以下基本原理开始：广义线性模型（GLM）将响应向量表示为回归量的线性组合加上噪声。设响应向量为 $y \\in \\mathbb{R}^{N}$，设计矩阵为 $X \\in \\mathbb{R}^{N \\times p}$，系数向量为 $\\beta \\in \\mathbb{R}^{p}$，噪声为 $\\varepsilon \\in \\mathbb{R}^{N}$。GLM 的形式为 $y = X \\beta + \\varepsilon$。普通最小二乘法（OLS）估计量最小化残差平方和，当 $X$ 是满秩时，得出解 $\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y$。当 $X$ 是秩亏时，Moore-Penrose 伪逆 $X^{+}$ 提供了最小范数的最小二乘解 $\\hat{\\beta} = X^{+} y$。\n\n应通过对中心化的调制器的列空间进行 Gram-Schmidt 过程来执行正交化。具体来说，给定一个中心化的振幅回归量 $x_{1} \\in \\mathbb{R}^{N}$ 和一个中心化的持续时间回归量 $x_{2} \\in \\mathbb{R}^{N}$，通过移除 $x_{2}$ 在 $x_{1}$ 上的投影来定义正交化的持续时间回归量：\n$$\nx_{2}^{\\perp} = x_{2} - \\frac{x_{1}^{\\top} x_{2}}{x_{1}^{\\top} x_{1}} x_{1}.\n$$\n正交化设计使用回归量 $[1, x_{1}, x_{2}^{\\perp}]$，其中 $1$ 表示截距列，并且 $x_{1}, x_{2}$ 都已进行均值中心化，以便截距与调制器正交。\n\n为每个测试用例实施以下步骤：\n- 根据给定的公式构建 $x_{1}$ 和 $x_{2}$ 序列，其中索引 $i$ 从 $1$ 到 $N$。\n- 计算中心化的调制器 $x_{1}^{c} = x_{1} - \\bar{x}_{1}$ 和 $x_{2}^{c} = x_{2} - \\bar{x}_{2}$，其中 $\\bar{x}$ 表示样本均值。\n- 使用上述 Gram-Schmidt 公式计算 $x_{2}^{\\perp}$，其中用 $x_{1}^{c}$ 代替 $x_{1}$，用 $x_{2}^{c}$ 代替 $x_{2}$。\n- 构成原始相关设计矩阵 $X_{\\text{orig}} = [\\mathbf{1}, x_{1}^{c}, x_{2}^{c}]$，以及正交化设计矩阵 $X_{\\text{orth}} = [\\mathbf{1}, x_{1}^{c}, x_{2}^{\\perp}]$。\n- 使用确定性线性生成模型 $y_{i} = \\beta_{0} + \\beta_{1} x_{1,i} + \\beta_{2} x_{2,i} + \\varepsilon_{i}$，以及为每种情况指定的系数和确定性噪声序列来生成 $y$。\n- 使用 Moore-Penrose 伪逆估计 $\\hat{\\beta}_{\\text{orig}}$ 和 $\\hat{\\beta}_{\\text{orth}}$，并仅报告调制器的系数（不包括截距），顺序为 $[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$，形式为浮点数。\n\n测试套件规范：\n- 情况 1（正常路径，中度相关）：\n  - 观测数 $N = 12$。\n  - 对于索引 $i \\in \\{1, 2, \\dots, 12\\}$：\n    - 振幅 $x_{1,i} = i$。\n    - 持续时间 $x_{2,i} = 0.8 \\, i + 0.4 \\sin\\!\\left(\\frac{2 \\pi i}{12}\\right)$。\n    - 噪声 $\\varepsilon_{i} = 0.1 \\sin\\!\\left(\\frac{4 \\pi i}{12}\\right)$。\n  - 系数 $\\beta_{0} = 0.2$, $\\beta_{1} = 0.7$, $\\beta_{2} = 0.3$。\n- 情况 2（边界情况，完全共线性）：\n  - 观测数 $N = 10$。\n  - 对于索引 $i \\in \\{1, 2, \\dots, 10\\}$：\n    - 振幅 $x_{1,i} = i$。\n    - 持续时间 $x_{2,i} = 2 \\, x_{1,i}$。\n    - 噪声 $\\varepsilon_{i} = 0.05 \\cos\\!\\left(\\frac{2 \\pi i}{10}\\right)$。\n  - 系数 $\\beta_{0} = 0.0$, $\\beta_{1} = 1.0$, $\\beta_{2} = 0.5$。\n- 情况 3（边缘情况，调制器之间近零相关）：\n  - 观测数 $N = 8$。\n  - 对于索引 $i \\in \\{1, 2, \\dots, 8\\}$：\n    - 振幅 $x_{1,i} = i$。\n    - 持续时间 $x_{2,i} = 3 \\, (-1)^{i}$。\n    - 噪声 $\\varepsilon_{i} = 0.02 \\sin\\!\\left(\\frac{2 \\pi i}{8}\\right)$。\n  - 系数 $\\beta_{0} = -0.1$, $\\beta_{1} = 0.5$, $\\beta_{2} = -0.4$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，并且本身是按 $[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$ 顺序排列的四个浮点数列表。例如，外层列表应类似于 $[[b_{11},b_{12},b_{13},b_{14}],[b_{21},b_{22},b_{23},b_{24}],[b_{31},b_{32},b_{33},b_{34}]]$，其中每个 $b_{jk}$ 都是一个浮点数。",
            "solution": "该问题要求构建和分析一个广义线性模型（GLM），以理解回归量之间的多重共线性影响以及正交化对估计模型系数的作用。我们将首先建立理论基础，然后对每个测试用例进行分步实现。\n\n**1. 理论基础：广义线性模型与多重共线性**\n\n广义线性模型（GLM）假设因变量 $y$ 与一组包含在设计矩阵 $X$ 中的解释变量（回归量）之间存在线性关系。该模型表示为：\n$$\ny = X \\beta + \\varepsilon\n$$\n其中 $y \\in \\mathbb{R}^{N}$ 是 $N$ 个观测值的向量，$X \\in \\mathbb{R}^{N \\times p}$ 是包含 $p$ 个回归量（包括一个截距）的设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是未知系数的向量，而 $\\varepsilon \\in \\mathbb{R}^{N}$ 是误差项的向量。\n\n目标是估计系数 $\\beta$。普通最小二乘法（OLS）找到使残差平方和 $\\|y - X\\beta\\|^2$ 最小化的 $\\hat{\\beta}$。当 $X$ 的列线性无关时（即 $X$ 具有满列秩），唯一的 OLS 解由下式给出：\n$$\n\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y\n$$\n当回归量相关时，会出现一个关键问题，这种情况被称为多重共线性。如果相关性是完美的（即一个回归量是其他回归量的线性组合），则 $X$ 的列变得线性相关，并且 $X$ 是秩亏的。在这种情况下，矩阵 $X^{\\top} X$ 是奇异的，无法求逆。OLS 解不再唯一；存在无限组系数向量可以产生相同的最小平方误差和。\n\n为了即使在秩亏的情况下也能获得单一、明确的解，我们使用 Moore-Penrose 伪逆，表示为 $X^{+}$。解由下式给出：\n$$\n\\hat{\\beta} = X^{+} y\n$$\n在所有可能的最小二乘解中，此解是具有最小欧几里得范数（$\\|\\hat{\\beta}\\|_2$）的解。\n\n**2. 通过 Gram-Schmidt 过程进行正交化**\n\n当回归量相关时，它们估计出的系数难以解释。一个系数 $\\hat{\\beta}_j$ 表示在保持所有其他回归量不变的情况下，回归量 $x_j$ 每变化一个单位，$y$ 的变化量。但是当回归量相关时，改变一个而保持其他不变在统计上和概念上都是有问题的。\n\n正交化是将一组相关的回归量转换为一组不相关（正交）的回归量的过程。这重新归因了回归量之间的共享方差，通常能澄清它们的贡献。我们使用 Gram-Schmidt 过程。给定两个中心化的回归量 $x_1^c$ 和 $x_2^c$，我们可以创建一个新的回归量 $x_2^{\\perp}$，它与 $x_1^c$ 正交。这是通过从 $x_2^c$ 中减去其在 $x_1^c$ 上的投影来实现的：\n$$\nx_{2}^{\\perp} = x_{2}^{c} - \\text{proj}_{x_1^c}(x_2^c) = x_{2}^{c} - \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} x_{1}^{c}\n$$\n根据构造，$(x_{1}^{c})^{\\top} x_{2}^{\\perp} = 0$。然后我们可以用设计矩阵 $X_{\\text{orth}} = [\\mathbf{1}, x_1^c, x_2^{\\perp}]$ 建立一个新的 GLM。因为回归量现在是正交的，它们估计出的系数相互独立。正交化的顺序很重要；在这里，$x_1^c$ 和 $x_2^c$ 之间的共享方差被归因于 $x_1^c$，而 $x_2^{\\perp}$ 仅代表 $x_2^c$ 中独立于 $x_1^c$ 的方差。\n\n**3. 系数之间的关系**\n\n设原始模型和正交化模型为：\n1.  原始模型: $y = \\hat{\\beta}_{0,\\text{orig}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orig}} x_1^c + \\hat{\\beta}_{2,\\text{orig}} x_2^c + e_{\\text{orig}}$\n2.  正交化模型: $y = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orth}} x_1^c + \\hat{\\beta}_{2,\\text{orth}} x_2^{\\perp} + e_{\\text{orth}}$\n\n由于 OLS 拟合提供了最佳的线性预测，两个模型的拟合值和残差必须相同，因为它们张成相同的子空间（除非其中一个是秩亏的，我们稍后会讨论这种情况）。将 $x_2^{\\perp}$ 的定义代入第二个方程，我们得到：\n$$\ny = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orth}} x_1^c + \\hat{\\beta}_{2,\\text{orth}} \\left( x_{2}^{c} - \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} x_{1}^{c} \\right) + e_{\\text{orth}}\n$$\n$$\ny = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\left( \\hat{\\beta}_{1,\\text{orth}} - \\hat{\\beta}_{2,\\text{orth}} \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} \\right) x_1^c + \\hat{\\beta}_{2,\\text{orth}} x_2^c + e_{\\text{orth}}\n$$\n将 $x_1^c$ 和 $x_2^c$ 的系数与原始模型进行比较，我们发现以下关系：\n$$\n\\hat{\\beta}_{2,\\text{orig}} = \\hat{\\beta}_{2,\\text{orth}}\n$$\n$$\n\\hat{\\beta}_{1,\\text{orig}} = \\hat{\\beta}_{1,\\text{orth}} - \\hat{\\beta}_{2,\\text{orth}} \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}}\n$$\n这表明被正交化的回归量（$x_2^c \\to x_2^{\\perp}$）的系数保持不变。然而，第一个回归量的系数 $\\hat{\\beta}_1$ 发生了显著变化。它在正交化模型中的值 $\\hat{\\beta}_{1,\\text{orth}}$ 吸收了 $x_2^c$ 中与 $x_1^c$ 相关部分所具有的解释力。\n\n**4. 实现步骤**\n\n对于每个测试用例，我们将执行以下计算：\n\n1.  **数据生成**：对于给定的观测数 $N$，我们使用提供的公式（索引 $i$ 从 $1$ 到 $N$）生成基础回归量 $x_1$ 和 $x_2$，以及噪声项 $\\varepsilon$。\n2.  **响应生成**：使用真实的生成模型合成响应变量 $y$：$y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\varepsilon_i$。注意，该模型是使用非中心化的回归量指定的。\n3.  **回归量准备**：我们计算中心化的回归量 $x_1^c = x_1 - \\bar{x}_1$ 和 $x_2^c = x_2 - \\bar{x}_2$。GLM 中的截距将解释均值。\n4.  **正交化**：使用 Gram-Schmidt 公式对中心化的回归量计算正交回归量 $x_2^{\\perp}$。\n5.  **设计矩阵构建**：形成两个设计矩阵：\n    -   $X_{\\text{orig}} = [\\mathbf{1}, x_1^c, x_2^c]$ 用于原始相关模型。\n    -   $X_{\\text{orth}} = [\\mathbf{1}, x_1^c, x_2^{\\perp}]$ 用于正交化模型。\n6.  **系数估计**：我们通过应用 Moore-Penrose 伪逆方法来估计系数向量 $\\hat{\\beta}_{\\text{orig}}$ 和 $\\hat{\\beta}_{\\text{orth}}$：$\\hat{\\beta} = X^{+} y$。\n7.  **结果提取**：从估计的系数向量 $\\hat{\\beta}_{\\text{orig}} = [\\hat{\\beta}_{0,\\text{orig}}, \\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}]^{\\top}$ 和 $\\hat{\\beta}_{\\text{orth}} = [\\hat{\\beta}_{0,\\text{orth}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]^{\\top}$ 中，我们提取两个调制器的系数，并按指定顺序报告它们：$[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$。\n\n此过程将应用于所有三个测试用例，这些用例涵盖了中度相关、完全共线性和近零相关的情景，从而展示了在这些不同条件下系数估计的行为。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the neuroscience encoding model problem for three test cases.\n\n    For each case, it constructs correlated and orthogonalized design matrices,\n    simulates neural response data, fits a GLM to both designs using the\n    Moore-Penrose pseudoinverse, and reports the resulting coefficients\n    for the non-intercept regressors.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Moderate correlation\n        {\n            \"N\": 12,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 0.8 * i + 0.4 * np.sin(2 * np.pi * i / 12),\n            \"eps_func\": lambda i: 0.1 * np.sin(4 * np.pi * i / 12),\n            \"betas\": [0.2, 0.7, 0.3]  # beta_0, beta_1, beta_2\n        },\n        # Case 2: Perfect collinearity\n        {\n            \"N\": 10,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 2.0 * i,\n            \"eps_func\": lambda i: 0.05 * np.cos(2 * np.pi * i / 10),\n            \"betas\": [0.0, 1.0, 0.5]\n        },\n        # Case 3: Near-zero correlation\n        {\n            \"N\": 8,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 3.0 * (-1)**i,\n            \"eps_func\": lambda i: 0.02 * np.sin(2 * np.pi * i / 8),\n            \"betas\": [-0.1, 0.5, -0.4]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        beta0, beta1, beta2 = case[\"betas\"]\n        \n        # Step 1: Construct x1, x2, and epsilon sequences.\n        # Index i runs from 1 to N.\n        i = np.arange(1, N + 1)\n        x1 = case[\"x1_func\"](i)\n        x2 = case[\"x2_func\"](i)\n        eps = case[\"eps_func\"](i)\n\n        # Step 2: Compute centered modulators.\n        x1_c = x1 - np.mean(x1)\n        x2_c = x2 - np.mean(x2)\n\n        # Step 3: Compute orthogonalized duration regressor x2_perp.\n        # Handle the case where x1_c is a zero vector to avoid division by zero.\n        dot_x1c_x1c = np.dot(x1_c, x1_c)\n        if np.isclose(dot_x1c_x1c, 0):\n            # If x1_c is zero, its projection is zero, so x2_perp is just x2_c.\n            x2_perp = x2_c\n        else:\n            proj_x2_on_x1 = (np.dot(x1_c, x2_c) / dot_x1c_x1c) * x1_c\n            x2_perp = x2_c - proj_x2_on_x1\n\n        # Step 4: Form the design matrices.\n        intercept = np.ones(N)\n        X_orig = np.column_stack([intercept, x1_c, x2_c])\n        X_orth = np.column_stack([intercept, x1_c, x2_perp])\n\n        # Step 5: Generate y using the deterministic linear generative model.\n        # Note: The generative model uses the original, non-centered regressors.\n        y = beta0 + beta1 * x1 + beta2 * x2 + eps\n\n        # Step 6: Estimate beta_orig and beta_orth using Moore-Penrose pseudoinverse.\n        beta_hat_orig = np.linalg.pinv(X_orig) @ y\n        beta_hat_orth = np.linalg.pinv(X_orth) @ y\n\n        # Report only the coefficients for the modulators (exclude the intercept).\n        # Order: [beta1_orig, beta2_orig, beta1_orth, beta2_orth]\n        results = [\n            beta_hat_orig[1], \n            beta_hat_orig[2], \n            beta_hat_orth[1], \n            beta_hat_orth[2]\n        ]\n        all_results.append(results)\n\n    # Format the final output string.\n    output_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "现代神经科学研究的结论通常需要从单一个体推广到群体层面。分层贝叶斯模型（Hierarchical Bayesian models）为此提供了一个强大的框架，它能够整合来自多个被试的数据，同时尊重个体差异。这项练习  将指导你构建这样一个模型，并量化一个核心概念——“收缩”（shrinkage）效应，即模型如何通过从群体中“借用”统计力量，来改善对单个被试参数的估计，从而得到更稳健的结论。",
            "id": "4155376",
            "problem": "一个实验室记录了在呈现共同刺激期间的多受试者神经时间序列数据。其目标是设计一个带有明确协变量集的、有原则的编码模型，并量化跨受试者的层级共享如何引导受试者特定参数向群体均值的后验收缩。该编码模型使用一个包含刺激回归量和干扰协变量的线性设计矩阵，以及一个将受试者特定系数与共享群体均值联系起来的层级高斯先验。\n\n假设对于每个受试者 $s \\in \\{1,\\dots,S\\}$，在时间样本 $t \\in \\{0,1,\\dots,T-1\\}$ 处，有以下线性高斯编码模型：\n- 观测响应向量为 $y_s \\in \\mathbb{R}^T$。\n- 设计矩阵为 $X \\in \\mathbb{R}^{T \\times p}$，包含 $p$ 个协变量。$X$ 的列代表：\n  - 一个刺激特征 $x_{\\text{stim},t}$，它是一个时间的确定性函数，旨在捕捉与刺激锁定的结构。\n  - 一个干扰漂移协变量 $x_{\\text{drift},t}$，用于捕捉缓慢的基线波动。\n  - 一个截距 $x_{\\text{int},t}$，用于对恒定偏移进行建模。\n\n编码模型和层级先验如下：\n- 似然（线性高斯）：$y_s \\mid \\beta_s \\sim \\mathcal{N}(X \\beta_s, \\sigma^2 I_T)$，其中 $\\beta_s \\in \\mathbb{R}^p$ 是受试者特定的参数向量，$\\sigma^2 > 0$ 是已知的噪声方差，$I_T$ 是 $T \\times T$ 的单位矩阵。\n- 受试者水平先验：$\\beta_s \\mid \\mu \\sim \\mathcal{N}(\\mu, \\Sigma_b)$，其中 $\\mu \\in \\mathbb{R}^p$ 是一个共享的群体均值，$\\Sigma_b \\in \\mathbb{R}^{p \\times p}$ 是一个已知的正定协方差矩阵。\n- 群体均值的超先验：$\\mu \\sim \\mathcal{N}(m_0, \\Sigma_0)$，其中 $m_0 \\in \\mathbb{R}^p$ 和正定矩阵 $\\Sigma_0 \\in \\mathbb{R}^{p \\times p}$ 均已知。\n\n这是一个共轭线性高斯层级结构。从“高斯密度函数的乘积仍为高斯函数”以及“高斯随机变量的线性变换仍为高斯变量”这两个基本事实出发，可以推导出联合后验分布 $p(\\mu, \\{\\beta_s\\}_{s=1}^S \\mid \\{y_s\\}_{s=1}^S)$ 是多元正态分布。后验均值可以通过求解由联合高斯自然参数构成的线性系统得到。根据这些后验均值，后验收缩的量化方式是：相对于独立（非层级）估计，受试者特定的后验均值向群体均值移动了多少。\n\n将受试者 $s$ 的独立估计定义为普通最小二乘解 $\\hat{\\beta}^{\\text{ind}}_s = (X^\\top X)^{-1} X^\\top y_s$。设群体均值的后验均值为 $\\hat{\\mu}^{\\text{post}}$，受试者特定参数的后验均值为 $\\hat{\\beta}^{\\text{post}}_s$。对于每个受试者 $s$，定义归一化收缩幅度为\n$$\n\\text{shr}_s = \\frac{\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2 - \\left\\lVert \\hat{\\beta}^{\\text{post}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2}{\\max\\left(\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2, \\varepsilon\\right)},\n$$\n其中 $\\varepsilon$ 是一个小的正常量，用于在分母为零时避免除以零的错误。所有受试者的平均收缩则为\n$$\n\\overline{\\text{shr}} = \\frac{1}{S} \\sum_{s=1}^S \\text{shr}_s,\n$$\n必须以小数形式报告。任何三角函数中使用的所有角度都必须以弧度为单位进行解释。\n\n你的程序必须实现上述层级编码模型，并为以下每个测试用例计算 $\\overline{\\text{shr}}$。设计矩阵 $X$ 和响应 $y_s$ 必须严格按照规定构建，以确保结果的确定性。\n\n对于所有测试用例，设计矩阵有 $p = 3$ 列：\n- 第1列（刺激协变量）：$x_{\\text{stim},t} = \\cos\\left(\\frac{2\\pi t}{T}\\right)$，其中 $t$ 以弧度为单位。\n- 第2列（干扰漂移）：$x_{\\text{drift},t} = \\frac{t}{T}$。\n- 第3列（截距）：$x_{\\text{int},t} = 1$。\n\n对于受试者 $s \\in \\{1,2,3\\}$，真实的受试者特定参数在所有测试用例中都是固定的：\n- 受试者 1：$\\beta^{\\text{true}}_1 = [1.5, -0.5, 0.2]^\\top$。\n- 受试者 2：$\\beta^{\\text{true}}_2 = [1.0, -0.6, 0.1]^\\top$。\n- 受试者 3：$\\beta^{\\text{true}}_3 = [1.8, -0.4, 0.3]^\\top$。\n\n超参数在所有测试用例中都是恒定的：\n- 群体先验均值：$m_0 = [0, 0, 0]^\\top$。\n- 群体先验协方差：$\\Sigma_0 = \\text{diag}([1.0, 1.0, 1.0])$。\n- 受试者先验协方差：$\\Sigma_b = \\text{diag}([0.4, 0.4, 0.4])$。\n\n对于每个测试用例，使用\n$$\ny_s = X \\beta^{\\text{true}}_s + \\eta_s,\n$$\n来构建 $y_s$，其中 $\\eta_s \\in \\mathbb{R}^T$ 是一个噪声向量，其独立条目从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取，并使用下面指定的固定伪随机生成器种子。\n\n测试套件：\n- 用例1（理想路径）：$S = 3$, $T = 12$, $\\sigma^2 = 0.2$, 噪声种子 $0$。\n- 用例2（高噪声边界）：$S = 3$, $T = 12$, $\\sigma^2 = 3.0$, 噪声种子 $1$。\n- 用例3（大样本边缘）：$S = 3$, $T = 60$, $\\sigma^2 = 0.05$, 噪声种子 $2$。\n\n要求：\n- 通过构建并求解由自然参数形式的多元正态后验所蕴含的联合线性系统来实现层级后验计算。不要依赖任何黑盒贝叶斯采样方法。\n- 为每个用例计算独立估计值 $\\hat{\\beta}^{\\text{ind}}_s$、后验均值 $\\hat{\\mu}^{\\text{post}}$ 和 $\\hat{\\beta}^{\\text{post}}_s$，以及平均收缩值 $\\overline{\\text{shr}}$，其中 $\\varepsilon = 10^{-12}$。\n- 最终输出必须是单行，包含三个测试用例的平均收缩值列表，按所列顺序排列，形式为用方括号括起来的逗号分隔列表（例如，$[\\text{result}_1,\\text{result}_2,\\text{result}_3]$）。每个结果必须是浮点数（小数）。不得打印任何其他文本。",
            "solution": "该问题是有效的。它在科学上植根于贝叶斯统计建模，这是神经科学数据分析中的一种标准技术。该问题是良定的，提供了所有必要的方程、参数和数据生成过程，以得出一个唯一的、有意义的解。它是客观且自包含的，没有任何会妨碍严格形式化的歧义。\n\n### 1. 模型设定与后验推导\n该问题描述了一个三层层级高斯模型。对于每个受试者 $s \\in \\{1, \\dots, S\\}$，模型定义如下：\n- **第一层（似然）：** $p(y_s | \\beta_s) = \\mathcal{N}(y_s; X \\beta_s, \\sigma^2 I_T)$\n- **第二层（受试者水平先验）：** $p(\\beta_s | \\mu) = \\mathcal{N}(\\beta_s; \\mu, \\Sigma_b)$\n- **第三层（群体水平超先验）：** $p(\\mu) = \\mathcal{N}(\\mu; m_0, \\Sigma_0)$\n\n所有未知参数 $\\theta = [\\mu^\\top, \\beta_1^\\top, \\dots, \\beta_S^\\top]^\\top$ 的联合后验分布由贝叶斯定理给出：\n$$\np(\\mu, \\{\\beta_s\\}_{s=1}^S | \\{y_s\\}_{s=1}^S) \\propto p(\\mu) \\prod_{s=1}^S p(y_s | \\beta_s) p(\\beta_s | \\mu)\n$$\n由于似然和先验都是高斯分布，它们的乘积也是一个高斯分布。联合后验的对数（忽略常数项）是参数的二次函数：\n$$\n\\log p(\\theta | \\{y_s\\}) = C - \\frac{1}{2} \\left[ (\\mu - m_0)^\\top \\Sigma_0^{-1}(\\mu - m_0) + \\sum_{s=1}^S (\\beta_s - \\mu)^\\top \\Sigma_b^{-1}(\\beta_s - \\mu) + \\frac{1}{\\sigma^2} \\sum_{s=1}^S (y_s - X\\beta_s)^\\top(y_s - X\\beta_s) \\right]\n$$\n对于多元高斯分布，后验均值与后验众数重合。我们可以通过将对数后验关于 $\\theta$ 的梯度设为零来找到这个值。这等价于求解由联合后验分布的自然参数所定义的线性系统。\n\n### 2. 用于求解后验均值的线性系统\n一个高斯分布 $p(\\theta) \\propto \\exp(-\\frac{1}{2}\\theta^\\top J \\theta + h^\\top\\theta)$ 由其精度矩阵 $J$ 和势向量 $h$ 参数化。其均值可通过求解系统 $J\\theta = h$ 得到。\n通过在对数后验中展开二次型，我们可以识别出联合精度矩阵 $J_{\\text{post}}$ 和联合势向量 $h_{\\text{post}}$ 的块结构。参数向量为 $\\theta = [\\mu^\\top, \\beta_1^\\top, \\dots, \\beta_S^\\top]^\\top$。\n\n联合精度矩阵 $J_{\\text{post}}$ 是一个 $(S+1)p \\times (S+1)p$ 的块矩阵：\n$$\nJ_{\\text{post}} =\n\\begin{pmatrix}\nS\\Sigma_b^{-1} + \\Sigma_0^{-1}  &-\\Sigma_b^{-1}  &\\cdots  &-\\Sigma_b^{-1} \\\\\n-\\Sigma_b^{-1}  &\\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}  &\\cdots  &0 \\\\\n\\vdots  &\\vdots  &\\ddots  &\\vdots \\\\\n-\\Sigma_b^{-1}  &0  &\\cdots  &\\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}\n\\end{pmatrix}\n$$\n联合势向量 $h_{\\text{post}}$ 是一个 $(S+1)p$ 维的向量：\n$$\nh_{\\text{post}} =\n\\begin{pmatrix}\n\\Sigma_0^{-1} m_0 \\\\\n\\frac{1}{\\sigma^2}X^\\top y_1 \\\\\n\\vdots \\\\\n\\frac{1}{\\sigma^2}X^\\top y_S\n\\end{pmatrix}\n$$\n后验均值 $\\hat{\\theta}^{\\text{post}} = [\\hat{\\mu}^{\\text{post}\\top}, \\hat{\\beta}_1^{\\text{post}\\top}, \\dots, \\hat{\\beta}_S^{\\text{post}\\top}]^\\top$ 是线性系统 $J_{\\text{post}} \\hat{\\theta}^{\\text{post}} = h_{\\text{post}}$ 的解。\n\n### 3. 通过块分解求解系统\n直接求解这个大型系统效率低下。我们可以通过块分解更有效地求解它。方程组为：\n$$\n(S\\Sigma_b^{-1} + \\Sigma_0^{-1}) \\hat{\\mu}^{\\text{post}} - \\sum_{s=1}^S \\Sigma_b^{-1} \\hat{\\beta}_s^{\\text{post}} = \\Sigma_0^{-1} m_0 \\quad (1)\n$$\n$$\n-\\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}} + \\left(\\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}\\right) \\hat{\\beta}_s^{\\text{post}} = \\frac{1}{\\sigma^2}X^\\top y_s \\quad \\text{for } s=1, \\dots, S \\quad (2)\n$$\n让我们定义受试者水平的后验精度（假设 $\\mu$ 已知）为 $J_\\beta = \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}$，以及来自数据的受试者特定势贡献为 $h_s = \\frac{1}{\\sigma^2}X^\\top y_s$。方程 $(2)$ 变为：\n$$\n-\\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}} + J_\\beta \\hat{\\beta}_s^{\\text{post}} = h_s\n$$\n由此，我们可以用 $\\hat{\\mu}^{\\text{post}}$ 表示 $\\hat{\\beta}_s^{\\text{post}}$：\n$$\n\\hat{\\beta}_s^{\\text{post}} = J_\\beta^{-1} (h_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}) \\quad (3)\n$$\n将 $(3)$ 代入 $(1)$：\n$$\n(S\\Sigma_b^{-1} + \\Sigma_0^{-1}) \\hat{\\mu}^{\\text{post}} - \\sum_{s=1}^S \\Sigma_b^{-1} J_\\beta^{-1} (h_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}) = \\Sigma_0^{-1} m_0\n$$\n重新整理各项以求解 $\\hat{\\mu}^{\\text{post}}$：\n$$\n\\left(S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}\\right) \\hat{\\mu}^{\\text{post}} = \\Sigma_0^{-1} m_0 + \\sum_{s=1}^S \\Sigma_b^{-1} J_\\beta^{-1} h_s\n$$\n这是一个关于 $\\hat{\\mu}^{\\text{post}}$ 的 $p \\times p$ 线性系统。令 $J_\\mu = S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}$ 和 $h_\\mu = \\Sigma_0^{-1} m_0 + \\Sigma_b^{-1} J_\\beta^{-1} \\sum_{s=1}^S h_s$。那么 $\\hat{\\mu}^{\\text{post}}$ 就是 $J_\\mu \\hat{\\mu}^{\\text{post}} = h_\\mu$ 的解。\n\n### 4. 算法步骤\n计算步骤如下：\n1.  对于给定的测试用例，使用提供的公式和伪随机种子，为 $s=1, \\dots, S$ 构建设计矩阵 $X \\in \\mathbb{R}^{T \\times p}$ 和受试者响应向量 $y_s \\in \\mathbb{R}^T$。\n2.  为每个受试者计算独立的普通最小二乘（OLS）估计：$\\hat{\\beta}^{\\text{ind}}_s = (X^\\top X)^{-1} X^\\top y_s$。\n3.  计算后验计算所需的矩阵：\n    *   $\\Sigma_0^{-1}$ 和 $\\Sigma_b^{-1}$ (由于它们是对角矩阵，计算非常简单)。\n    *   $J_\\beta = \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}$。\n4.  求解后验群体均值 $\\hat{\\mu}^{\\text{post}}$：\n    *   计算 $\\mu$ 的精度：$J_\\mu = S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}$。\n    *   计算 $\\mu$ 的势：$h_\\mu = \\Sigma_0^{-1} m_0 + \\Sigma_b^{-1} J_\\beta^{-1} \\sum_{s=1}^S (\\frac{1}{\\sigma^2}X^\\top y_s)$。\n    *   求解系统 $J_\\mu \\hat{\\mu}^{\\text{post}} = h_\\mu$。\n5.  对每个受试者，使用方程 $(3)$ 求解其后验的受试者特定均值 $\\hat{\\beta}_s^{\\text{post}}$：\n    *   求解系统 $J_\\beta \\hat{\\beta}_s^{\\text{post}} = \\frac{1}{\\sigma^2}X^\\top y_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}$。\n6.  对于每个受试者 $s$，计算归一化收缩幅度：\n    $$\n    \\text{shr}_s = \\frac{\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2 - \\left\\lVert \\hat{\\beta}^{\\text{post}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2}{\\max\\left(\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2, \\varepsilon\\right)}\n    $$\n7.  计算所有受试者的平均收缩：$\\overline{\\text{shr}} = \\frac{1}{S} \\sum_{s=1}^S \\text{shr}_s$。\n对每个测试用例实施此过程以获得最终结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the solution for the hierarchical Bayesian encoding model problem.\n    \"\"\"\n    \n    # Define test cases as per the problem statement.\n    test_cases = [\n        {'S': 3, 'T': 12, 'sigma2': 0.2, 'seed': 0},\n        {'S': 3, 'T': 12, 'sigma2': 3.0, 'seed': 1},\n        {'S': 3, 'T': 60, 'sigma2': 0.05, 'seed': 2}\n    ]\n\n    # Fixed parameters across all test cases\n    beta_true_all = [\n        np.array([1.5, -0.5, 0.2]),\n        np.array([1.0, -0.6, 0.1]),\n        np.array([1.8, -0.4, 0.3])\n    ]\n    m0 = np.array([0.0, 0.0, 0.0])\n    Sigma0 = np.diag([1.0, 1.0, 1.0])\n    Sigma_b = np.diag([0.4, 0.4, 0.4])\n    epsilon = 1e-12\n    \n    # Pre-compute inverse matrices (diagonal, so inversion is trivial)\n    Sigma0_inv = np.linalg.inv(Sigma0)\n    Sigma_b_inv = np.linalg.inv(Sigma_b)\n\n    results = []\n    for case in test_cases:\n        S = case['S']\n        T = case['T']\n        sigma2 = case['sigma2']\n        seed = case['seed']\n        p = 3\n\n        # 1. Construct design matrix X and response vectors y_s\n        t = np.arange(T)\n        x_stim = np.cos(2 * np.pi * t / T)\n        x_drift = t / T\n        x_int = np.ones(T)\n        X = np.stack([x_stim, x_drift, x_int], axis=1)\n\n        rng = np.random.default_rng(seed)\n        y_all = []\n        for s in range(S):\n            noise = rng.normal(0, np.sqrt(sigma2), size=T)\n            y_s = X @ beta_true_all[s] + noise\n            y_all.append(y_s)\n\n        # 2. Compute independent OLS estimates\n        XTX = X.T @ X\n        XTX_inv = np.linalg.inv(XTX)\n        beta_ind_all = []\n        for s in range(S):\n            Xy_s = X.T @ y_all[s]\n            beta_ind_s = XTX_inv @ Xy_s\n            beta_ind_all.append(beta_ind_s)\n\n        # 3. Compute matrices for posterior calculation\n        J_beta = (1 / sigma2) * XTX + Sigma_b_inv\n        J_beta_inv = np.linalg.inv(J_beta)\n\n        # 4. Solve for posterior group mean mu_post\n        J_mu = S * Sigma_b_inv + Sigma0_inv - S * (Sigma_b_inv @ J_beta_inv @ Sigma_b_inv)\n        \n        sum_h_s = np.zeros(p)\n        h_s_all = []\n        for s in range(S):\n            h_s = (1/sigma2) * (X.T @ y_all[s])\n            h_s_all.append(h_s)\n            sum_h_s += h_s\n            \n        h_mu_rhs = Sigma0_inv @ m0 + Sigma_b_inv @ J_beta_inv @ sum_h_s\n        mu_post = np.linalg.solve(J_mu, h_mu_rhs)\n\n        # 5. Solve for posterior subject-specific means beta_post_s\n        beta_post_all = []\n        for s in range(S):\n            beta_post_rhs = h_s_all[s] + Sigma_b_inv @ mu_post\n            beta_post_s = np.linalg.solve(J_beta, beta_post_rhs)\n            beta_post_all.append(beta_post_s)\n\n        # 6. Calculate normalized shrinkage for each subject\n        shr_s_all = []\n        for s in range(S):\n            dist_ind = np.linalg.norm(beta_ind_all[s] - mu_post)\n            dist_post = np.linalg.norm(beta_post_all[s] - mu_post)\n            \n            denominator = max(dist_ind, epsilon)\n            shr_s = (dist_ind - dist_post) / denominator\n            shr_s_all.append(shr_s)\n\n        # 7. Compute average shrinkage\n        avg_shr = np.mean(shr_s_all)\n        results.append(avg_shr)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}