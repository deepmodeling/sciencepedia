{
    "hands_on_practices": [
        {
            "introduction": "In designing encoding models, we often face the challenge of multicollinearity, where two or more predictor variables are correlated. This practice provides a direct, hands-on method for addressing this issue through Gram-Schmidt orthogonalization. By implementing this procedure, you will see firsthand how it re-attributes shared variance and alters model coefficients, offering a clearer, albeit order-dependent, view of each regressor's contribution. This exercise is fundamental for anyone building General Linear Models (GLMs) for fMRI, EEG, or other neurophysiological data. ",
            "id": "4155403",
            "problem": "You are given a trial-wise encoding model in neuroscience data analysis where correlated parametric modulators are used to explain a neural response. The modulators, called amplitude and duration, form two regressors that are often correlated. Your task is to construct and analyze an encoding model in a way that isolates unique contributions of each regressor by orthogonalization, and then to compare coefficients from the original correlated design to the coefficients from the orthogonalized design.\n\nStart from the following fundamental base: A General Linear Model (GLM) represents a response vector as a linear combination of regressors plus noise. Let the response vector be denoted by $y \\in \\mathbb{R}^{N}$, the design matrix by $X \\in \\mathbb{R}^{N \\times p}$, the coefficient vector by $\\beta \\in \\mathbb{R}^{p}$, and the noise by $\\varepsilon \\in \\mathbb{R}^{N}$. The GLM is $y = X \\beta + \\varepsilon$. The Ordinary Least Squares (OLS) estimator minimizes the sum of squared residuals and, when $X$ is full rank, yields the solution $\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y$. When $X$ is rank-deficient, the Moore-Penrose pseudoinverse $X^{+}$ provides the minimal-norm least-squares solution $\\hat{\\beta} = X^{+} y$.\n\nOrthogonalization should be performed via the Gram-Schmidt procedure on the column space of the centered modulators. Specifically, given a centered amplitude regressor $x_{1} \\in \\mathbb{R}^{N}$ and a centered duration regressor $x_{2} \\in \\mathbb{R}^{N}$, define the orthogonalized duration regressor by removing the projection of $x_{2}$ onto $x_{1}$:\n$$\nx_{2}^{\\perp} = x_{2} - \\frac{x_{1}^{\\top} x_{2}}{x_{1}^{\\top} x_{1}} x_{1}.\n$$\nThe orthogonalized design uses regressors $[1, x_{1}, x_{2}^{\\perp}]$, where the $1$ denotes the intercept column and $x_{1}, x_{2}$ are both mean-centered so that the intercept is orthogonal to the modulators.\n\nImplement the following steps for each test case:\n- Construct $x_{1}$ and $x_{2}$ sequences according to the given formulas, with index $i$ running from $1$ to $N$.\n- Compute the centered modulators $x_{1}^{c} = x_{1} - \\bar{x}_{1}$ and $x_{2}^{c} = x_{2} - \\bar{x}_{2}$, where $\\bar{x}$ denotes the sample mean.\n- Compute $x_{2}^{\\perp}$ using the Gram-Schmidt formula above, with $x_{1}^{c}$ in place of $x_{1}$ and $x_{2}^{c}$ in place of $x_{2}$.\n- Form the original correlated design matrix $X_{\\text{orig}} = [\\mathbf{1}, x_{1}^{c}, x_{2}^{c}]$, and the orthogonalized design matrix $X_{\\text{orth}} = [\\mathbf{1}, x_{1}^{c}, x_{2}^{\\perp}]$.\n- Generate $y$ using the deterministic linear generative model $y_{i} = \\beta_{0} + \\beta_{1} x_{1,i} + \\beta_{2} x_{2,i} + \\varepsilon_{i}$ with the specified coefficients and deterministic noise sequence for each case.\n- Estimate $\\hat{\\beta}_{\\text{orig}}$ and $\\hat{\\beta}_{\\text{orth}}$ using the Moore-Penrose pseudoinverse, and report only the coefficients for the modulators (exclude the intercept) in the order $[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$ as floats.\n\nTest suite specification:\n- Case $1$ (happy path, moderate correlation):\n  - Number of observations $N = 12$.\n  - For index $i \\in \\{1, 2, \\dots, 12\\}$:\n    - Amplitude $x_{1,i} = i$.\n    - Duration $x_{2,i} = 0.8 \\, i + 0.4 \\sin\\!\\left(\\frac{2 \\pi i}{12}\\right)$.\n    - Noise $\\varepsilon_{i} = 0.1 \\sin\\!\\left(\\frac{4 \\pi i}{12}\\right)$.\n  - Coefficients $\\beta_{0} = 0.2$, $\\beta_{1} = 0.7$, $\\beta_{2} = 0.3$.\n- Case $2$ (boundary, perfect collinearity):\n  - Number of observations $N = 10$.\n  - For index $i \\in \\{1, 2, \\dots, 10\\}$:\n    - Amplitude $x_{1,i} = i$.\n    - Duration $x_{2,i} = 2 \\, x_{1,i}$.\n    - Noise $\\varepsilon_{i} = 0.05 \\cos\\!\\left(\\frac{2 \\pi i}{10}\\right)$.\n  - Coefficients $\\beta_{0} = 0.0$, $\\beta_{1} = 1.0$, $\\beta_{2} = 0.5$.\n- Case $3$ (edge, near-zero correlation between modulators):\n  - Number of observations $N = 8$.\n  - For index $i \\in \\{1, 2, \\dots, 8\\}$:\n    - Amplitude $x_{1,i} = i$.\n    - Duration $x_{2,i} = 3 \\, (-1)^{i}$.\n    - Noise $\\varepsilon_{i} = 0.02 \\sin\\!\\left(\\frac{2 \\pi i}{8}\\right)$.\n  - Coefficients $\\beta_{0} = -0.1$, $\\beta_{1} = 0.5$, $\\beta_{2} = -0.4$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of four floats in the order $[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$. For example, the outer list should look like $[[b_{11},b_{12},b_{13},b_{14}],[b_{21},b_{22},b_{23},b_{24}],[b_{31},b_{32},b_{33},b_{34}]]$, where each $b_{jk}$ is a float.",
            "solution": "The problem requires the construction and analysis of a General Linear Model (GLM) to understand the impact of multicollinearity among regressors and the effect of orthogonalization on the estimated model coefficients. We will first establish the theoretical foundation, then proceed with the step-by-step implementation for each test case.\n\n**1. Theoretical Foundation: The General Linear Model and Multicollinearity**\n\nA General Linear Model (GLM) posits a linear relationship between a dependent variable $y$ and a set of explanatory variables (regressors) contained in a design matrix $X$. The model is expressed as:\n$$\ny = X \\beta + \\varepsilon\n$$\nwhere $y \\in \\mathbb{R}^{N}$ is the vector of $N$ observations, $X \\in \\mathbb{R}^{N \\times p}$ is the design matrix with $p$ regressors (including an intercept), $\\beta \\in \\mathbb{R}^{p}$ is the vector of unknown coefficients, and $\\varepsilon \\in \\mathbb{R}^{N}$ is a vector of error terms.\n\nThe goal is to estimate the coefficients $\\beta$. The Ordinary Least Squares (OLS) method finds the $\\hat{\\beta}$ that minimizes the sum of squared residuals, $\\|y - X\\beta\\|^2$. When the columns of $X$ are linearly independent (i.e., $X$ has full column rank), the unique OLS solution is given by:\n$$\n\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y\n$$\nA critical issue arises when the regressors are correlated, a condition known as multicollinearity. If the correlation is perfect (i.e., one regressor is a linear combination of others), the columns of $X$ become linearly dependent, and $X$ is rank-deficient. In this case, the matrix $X^{\\top} X$ is singular and cannot be inverted. The OLS solution is no longer unique; there exists an infinite set of coefficient vectors that produce the same minimal sum of squared errors.\n\nTo obtain a single, well-defined solution even in the rank-deficient case, we use the Moore-Penrose pseudoinverse, denoted $X^{+}$. The solution is given by:\n$$\n\\hat{\\beta} = X^{+} y\n$$\nThis solution is the one with the minimum Euclidean norm ($\\|\\hat{\\beta}\\|_2$) among all possible least-squares solutions.\n\n**2. Orthogonalization via Gram-Schmidt**\n\nWhen regressors are correlated, their estimated coefficients are difficult to interpret. A coefficient $\\hat{\\beta}_j$ represents the change in $y$ for a one-unit change in the regressor $x_j$, holding all other regressors constant. But when regressors are correlated, it is statistically and conceptually problematic to change one while holding the others constant.\n\nOrthogonalization is a procedure to transform a set of correlated regressors into a set of uncorrelated (orthogonal) ones. This re-attributes the shared variance between regressors, often clarifying their contributions. We use the Gram-Schmidt process. Given two centered regressors, $x_1^c$ and $x_2^c$, we can create a new regressor, $x_2^{\\perp}$, that is orthogonal to $x_1^c$. This is achieved by subtracting the projection of $x_2^c$ onto $x_1^c$:\n$$\nx_{2}^{\\perp} = x_{2}^{c} - \\text{proj}_{x_1^c}(x_2^c) = x_{2}^{c} - \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} x_{1}^{c}\n$$\nBy construction, $(x_{1}^{c})^{\\top} x_{2}^{\\perp} = 0$. We can then build a new GLM with the design matrix $X_{\\text{orth}} = [\\mathbf{1}, x_1^c, x_2^{\\perp}]$. Because the regressors are now orthogonal, their estimated coefficients are independent of each other. The order of orthogonalization matters; here, variance shared between $x_1^c$ and $x_2^c$ is attributed to $x_1^c$, and $x_2^{\\perp}$ represents only the variance in $x_2^c$ that is unique from $x_1^c$.\n\n**3. Relationship Between Coefficients**\n\nLet the original and orthogonalized models be:\n1.  Original model: $y = \\hat{\\beta}_{0,\\text{orig}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orig}} x_1^c + \\hat{\\beta}_{2,\\text{orig}} x_2^c + e_{\\text{orig}}$\n2.  Orthogonalized model: $y = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orth}} x_1^c + \\hat{\\beta}_{2,\\text{orth}} x_2^{\\perp} + e_{\\text{orth}}$\n\nSince the OLS fit provides the best possible linear prediction, the fitted values and residuals from both models must be identical, as they span the same subspace (unless one is rank-deficient, a case we will address). By substituting the definition of $x_2^{\\perp}$ into the second equation, we get:\n$$\ny = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\hat{\\beta}_{1,\\text{orth}} x_1^c + \\hat{\\beta}_{2,\\text{orth}} \\left( x_{2}^{c} - \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} x_{1}^{c} \\right) + e_{\\text{orth}}\n$$\n$$\ny = \\hat{\\beta}_{0,\\text{orth}}\\mathbf{1} + \\left( \\hat{\\beta}_{1,\\text{orth}} - \\hat{\\beta}_{2,\\text{orth}} \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}} \\right) x_1^c + \\hat{\\beta}_{2,\\text{orth}} x_2^c + e_{\\text{orth}}\n$$\nComparing the coefficients of $x_1^c$ and $x_2^c$ with the original model, we find the following relationships:\n$$\n\\hat{\\beta}_{2,\\text{orig}} = \\hat{\\beta}_{2,\\text{orth}}\n$$\n$$\n\\hat{\\beta}_{1,\\text{orig}} = \\hat{\\beta}_{1,\\text{orth}} - \\hat{\\beta}_{2,\\text{orth}} \\frac{(x_{1}^{c})^{\\top} x_{2}^{c}}{(x_{1}^{c})^{\\top} x_{1}^{c}}\n$$\nThis reveals that the coefficient for the regressor that was orthogonalized ($x_2^c \\to x_2^{\\perp}$) remains unchanged. However, the coefficient for the first regressor, $\\hat{\\beta}_1$, changes significantly. Its value in the orthogonalized model, $\\hat{\\beta}_{1,\\text{orth}}$, absorbs the explanatory power of the part of $x_2^c$ that is correlated with $x_1^c$.\n\n**4. Implementation Steps**\n\nFor each test case, we will perform the following calculations:\n\n1.  **Data Generation**: For a given number of observations $N$, we generate the base regressors $x_1$ and $x_2$, and the noise term $\\varepsilon$, using the provided formulas with an index $i$ from $1$ to $N$.\n2.  **Response Generation**: The response variable $y$ is synthesized using the true generative model: $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\varepsilon_i$. Note that the model is specified with the non-centered regressors.\n3.  **Regressor Preparation**: We compute the centered regressors $x_1^c = x_1 - \\bar{x}_1$ and $x_2^c = x_2 - \\bar{x}_2$. The intercept in the GLM will account for the means.\n4.  **Orthogonalization**: The orthogonal regressor $x_2^{\\perp}$ is calculated using the Gram-Schmidt formula on the centered regressors.\n5.  **Design Matrix Construction**: Two design matrices are formed:\n    -   $X_{\\text{orig}} = [\\mathbf{1}, x_1^c, x_2^c]$ for the original correlated model.\n    -   $X_{\\text{orth}} = [\\mathbf{1}, x_1^c, x_2^{\\perp}]$ for the orthogonalized model.\n6.  **Coefficient Estimation**: We estimate the coefficient vectors $\\hat{\\beta}_{\\text{orig}}$ and $\\hat{\\beta}_{\\text{orth}}$ by applying the Moore-Penrose pseudoinverse method: $\\hat{\\beta} = X^{+} y$.\n7.  **Result Extraction**: From the estimated coefficient vectors $\\hat{\\beta}_{\\text{orig}} = [\\hat{\\beta}_{0,\\text{orig}}, \\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}]^{\\top}$ and $\\hat{\\beta}_{\\text{orth}} = [\\hat{\\beta}_{0,\\text{orth}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]^{\\top}$, we extract the coefficients for the two modulators and report them in the specified order: $[\\hat{\\beta}_{1,\\text{orig}}, \\hat{\\beta}_{2,\\text{orig}}, \\hat{\\beta}_{1,\\text{orth}}, \\hat{\\beta}_{2,\\text{orth}}]$.\n\nThis procedure is applied to all three test cases, which cover scenarios of moderate correlation, perfect collinearity, and near-zero correlation, demonstrating the behavior of the coefficient estimates under these different conditions.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the neuroscience encoding model problem for three test cases.\n\n    For each case, it constructs correlated and orthogonalized design matrices,\n    simulates neural response data, fits a GLM to both designs using the\n    Moore-Penrose pseudoinverse, and reports the resulting coefficients\n    for the non-intercept regressors.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Moderate correlation\n        {\n            \"N\": 12,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 0.8 * i + 0.4 * np.sin(2 * np.pi * i / 12),\n            \"eps_func\": lambda i: 0.1 * np.sin(4 * np.pi * i / 12),\n            \"betas\": [0.2, 0.7, 0.3]  # beta_0, beta_1, beta_2\n        },\n        # Case 2: Perfect collinearity\n        {\n            \"N\": 10,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 2.0 * i,\n            \"eps_func\": lambda i: 0.05 * np.cos(2 * np.pi * i / 10),\n            \"betas\": [0.0, 1.0, 0.5]\n        },\n        # Case 3: Near-zero correlation\n        {\n            \"N\": 8,\n            \"x1_func\": lambda i: i,\n            \"x2_func\": lambda i: 3.0 * (-1)**i,\n            \"eps_func\": lambda i: 0.02 * np.sin(2 * np.pi * i / 8),\n            \"betas\": [-0.1, 0.5, -0.4]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        beta0, beta1, beta2 = case[\"betas\"]\n        \n        # Step 1: Construct x1, x2, and epsilon sequences.\n        # Index i runs from 1 to N.\n        i = np.arange(1, N + 1)\n        x1 = case[\"x1_func\"](i)\n        x2 = case[\"x2_func\"](i)\n        eps = case[\"eps_func\"](i)\n\n        # Step 2: Compute centered modulators.\n        x1_c = x1 - np.mean(x1)\n        x2_c = x2 - np.mean(x2)\n\n        # Step 3: Compute orthogonalized duration regressor x2_perp.\n        # Handle the case where x1_c is a zero vector to avoid division by zero.\n        dot_x1c_x1c = np.dot(x1_c, x1_c)\n        if np.isclose(dot_x1c_x1c, 0):\n            # If x1_c is zero, its projection is zero, so x2_perp is just x2_c.\n            x2_perp = x2_c\n        else:\n            proj_x2_on_x1 = (np.dot(x1_c, x2_c) / dot_x1c_x1c) * x1_c\n            x2_perp = x2_c - proj_x2_on_x1\n\n        # Step 4: Form the design matrices.\n        intercept = np.ones(N)\n        X_orig = np.column_stack([intercept, x1_c, x2_c])\n        X_orth = np.column_stack([intercept, x1_c, x2_perp])\n\n        # Step 5: Generate y using the deterministic linear generative model.\n        # Note: The generative model uses the original, non-centered regressors.\n        y = beta0 + beta1 * x1 + beta2 * x2 + eps\n\n        # Step 6: Estimate beta_orig and beta_orth using Moore-Penrose pseudoinverse.\n        beta_hat_orig = np.linalg.pinv(X_orig) @ y\n        beta_hat_orth = np.linalg.pinv(X_orth) @ y\n\n        # Report only the coefficients for the modulators (exclude the intercept).\n        # Order: [beta1_orig, beta2_orig, beta1_orth, beta2_orth]\n        results = [\n            beta_hat_orig[1], \n            beta_hat_orig[2], \n            beta_hat_orth[1], \n            beta_hat_orth[2]\n        ]\n        all_results.append(results)\n\n    # Format the final output string.\n    output_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the practical skill of orthogonalization, this exercise challenges you to think critically about its implications for interpreting your model. Sequential orthogonalization is not a neutral procedure; the order in which you enter correlated regressors determines how their shared variance is allocated. This practice prompts you to reason about the statistical consequences of this ordering and introduces principled, order-invariant methods for testing a regressor's unique contribution to the model. ",
            "id": "4155359",
            "problem": "Consider an event-related Functional Magnetic Resonance Imaging (fMRI) encoding model specified as a General Linear Model (GLM). Let the observed neural response be a length-$T$ vector $y \\in \\mathbb{R}^T$, modeled as $y = X \\beta + \\varepsilon$, where $X \\in \\mathbb{R}^{T \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^p$ are regression coefficients, and $\\varepsilon \\in \\mathbb{R}^T$ is zero-mean noise with finite variance. Assume $X$ contains the following columns in this order: an intercept $1_T$, a baseline stimulus regressor $s \\in \\mathbb{R}^T$ (e.g., onset indicator convolved with a hemodynamic response), and two parametric modulators $m^{(1)}, m^{(2)} \\in \\mathbb{R}^T$ derived from trial-wise features of the same stimulus events. Suppose all regressors have been mean-centered with respect to the intercept (so that $1_T^\\top s = 0$, $1_T^\\top m^{(1)} = 0$, $1_T^\\top m^{(2)} = 0$), and further that $s$ is uncorrelated with each modulator, i.e., $s^\\top m^{(1)} = 0$ and $s^\\top m^{(2)} = 0$, but the modulators are correlated, i.e., $m^{(1)\\top} m^{(2)} \\neq 0$.\n\nIn many GLM toolchains, parametric modulators are orthogonalized sequentially via Gram–Schmidt with respect to regressors earlier in the design, creating a new design $\\tilde{X}$ whose columns span the same subspace as the original $X$, but where later columns are residualized against earlier ones. Consider two possible orders: placing $m^{(1)}$ before $m^{(2)}$ versus placing $m^{(2)}$ before $m^{(1)}$. Using the definitions of ordinary least squares, linear subspaces, orthogonal projections, and the Frisch–Waugh–Lovell theorem for partitioned regression, reason about the impact of the orthogonalization order on the fitted values and on the unique variance attributed to each modulator, and select all statements that are correct.\n\nA. When parametric modulators are orthogonalized sequentially via Gram–Schmidt while preserving the column span of the original design, the fitted response $\\hat{y}$ is invariant to the order, but the allocation of unique variance to each modulator follows Type $\\mathrm{I}$ sums-of-squares and is order-dependent.\n\nB. If two modulators are centered and have zero correlation with the baseline stimulus, their unique variance under any orthogonalization order will be equal because centering removes all shared structure that could bias order.\n\nC. A principled ordering practice is to place regressors according to theory-driven causal precedence or measurement reliability, so that shared variance is attributed to the regressor with strongest prior justification; and to state the chosen order explicitly in reports.\n\nD. The only way to achieve order-invariant inference is to enforce complete orthogonality among modulators by residualizing each against all others, which necessarily changes the column span and thus alters the fitted response.\n\nE. To obtain order-invariant tests of each modulator’s unique contribution, compare the full model to a nested model that omits that modulator (e.g., via Frisch–Waugh–Lovell or a partial $F$-test), which yields identical results irrespective of orthogonalization and ordering.",
            "solution": "The problem asks us to analyze the effects of sequential orthogonalization of correlated parametric modulators in an fMRI General Linear Model (GLM). We must validate the problem statement before proceeding.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Model:** $y = X \\beta + \\varepsilon$, a GLM where $y \\in \\mathbb{R}^T$ is the response, $X \\in \\mathbb{R}^{T \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^p$ are coefficients, and $\\varepsilon \\in \\mathbb{R}^T$ is zero-mean noise.\n-   **Design Matrix ($X$) columns:** The columns are, in order, an intercept $1_T$, a baseline stimulus regressor $s$, and two parametric modulators $m^{(1)}$ and $m^{(2)}$. Thus, $X = [1_T, s, m^{(1)}, m^{(2)}]$.\n-   **Regressor Properties:**\n    -   Mean-centering: $1_T^\\top s = 0$, $1_T^\\top m^{(1)} = 0$, $1_T^\\top m^{(2)} = 0$. This implies orthogonality to the intercept.\n    -   Stimulus-modulator orthogonality: $s^\\top m^{(1)} = 0$ and $s^\\top m^{(2)} = 0$.\n    -   Modulator correlation: $m^{(1)\\top} m^{(2)} \\neq 0$.\n-   **Procedure:** A new design matrix, $\\tilde{X}$, is created by sequential Gram-Schmidt orthogonalization of the columns of $X$.\n-   **Subspace Invariance:** The column space of the new design matrix is the same as the original: $\\mathrm{span}(\\tilde{X}) = \\mathrm{span}(X)$.\n-   **Core Question:** We are to analyze the effect of changing the orthogonalization order of $m^{(1)}$ and $m^{(2)}$ on the fitted values $\\hat{y}$ and the unique variance attributed to each modulator.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem setup is a standard and realistic representation of a common issue in fMRI encoding models, namely, handling correlated regressors (multicollinearity). The concepts used (GLM, OLS, Gram-Schmidt, FWL theorem) are fundamental to statistics and its application in neuroscience. The scenario is scientifically sound.\n-   **Well-Posed:** The problem is clearly defined with all necessary mathematical conditions. The model, regressor properties, and the transformation process are specified, allowing for a rigorous and unique answer based on the principles of linear algebra and regression analysis.\n-   **Objective:** The problem is presented in precise, technical language, free from subjective or ambiguous terminology.\n\n**Step 3: Verdict and Action**\n-   **Verdict:** The problem statement is valid. It is scientifically grounded, well-posed, and objective.\n-   **Action:** Proceed with the derivation and evaluation of options.\n\n### Derivation and Analysis\n\nThe ordinary least squares (OLS) fitted values are given by the orthogonal projection of the data vector $y$ onto the column space of the design matrix $X$, denoted $\\mathrm{span}(X)$. The projection is $\\hat{y} = P_X y$, where $P_X = X(X^\\top X)^{-1}X^\\top$ is the projection matrix.\n\nA fundamental property of the projection matrix $P_X$ is that it depends only on the subspace $\\mathrm{span}(X)$, not on the particular basis chosen to represent that subspace. The problem states that the sequential orthogonalization process produces a new design matrix $\\tilde{X}$ such that $\\mathrm{span}(\\tilde{X}) = \\mathrm{span}(X)$. Consequently, the projection matrix remains unchanged, $P_{\\tilde{X}} = P_X$. This directly implies that the fitted values are invariant to this transformation: $\\hat{y} = P_{\\tilde{X}} y = P_X y$. This holds true regardless of the order in which the columns are orthogonalized.\n\nNext, we consider the variance attributed to each modulator. The procedure described is sequential orthogonalization. Let's analyze two possible orderings for the modulators. The regressors $1_T$ and $s$ are already orthogonal to each other and are placed first. The modulators $m^{(1)}$ and $m^{(2)}$ are orthogonal to $1_T$ and $s$, but not to each other.\n\n**Order 1: $m^{(1)}$ placed before $m^{(2)}$**\nThe orthogonalization process (Gram-Schmidt) will produce a new set of regressors. Since $1_T$, $s$, and $m^{(1)}$ are already mutually orthogonal with respect to the earlier regressors in this sequence based on the givens, they remain unchanged. The new regressor for $m^{(2)}$ will be its component orthogonal to the preceding regressors. Given the provided orthogonality conditions, this simplifies to residualizing $m^{(2)}$ with respect to $m^{(1)}$:\n$$ \\tilde{m}^{(2.1)} = m^{(2)} - \\mathrm{proj}_{m^{(1)}}(m^{(2)}) = m^{(2)} - \\frac{m^{(1)\\top} m^{(2)}}{m^{(1)\\top} m^{(1)}} m^{(1)} $$\nThe new design matrix has columns that are orthogonal: $[1_T, s, m^{(1)}, \\tilde{m}^{(2.1)}]$. The coefficient for $m^{(1)}$ in this new regression explains variance associated with the full $m^{(1)}$ regressor. The coefficient for $\\tilde{m}^{(2.1)}$ explains the *additional* variance captured by $m^{(2)}$ after accounting for $1_T$, $s$, and $m^{(1)}$. Any variance shared between $m^{(1)}$ and $m^{(2)}$ is attributed to $m^{(1)}$.\n\n**Order 2: $m^{(2)}$ placed before $m^{(1)}$**\nSimilarly, if $m^{(2)}$ is placed first, the orthogonalized regressor for $m^{(1)}$ becomes:\n$$ \\tilde{m}^{(1.2)} = m^{(1)} - \\mathrm{proj}_{m^{(2)}}(m^{(1)}) = m^{(1)} - \\frac{m^{(2)\\top} m^{(1)}}{m^{(2)\\top} m^{(2)}} m^{(2)} $$\nThe design matrix has columns $[1_T, s, m^{(2)}, \\tilde{m}^{(1.2)}]$. Here, the coefficient for $m^{(2)}$ explains variance associated with the full $m^{(2)}$ regressor. The coefficient for $\\tilde{m}^{(1.2)}$ explains the *additional* variance captured by $m^{(1)}$ after accounting for $1_T$, $s$, and $m^{(2)}$. The shared variance is now attributed to $m^{(2)}$.\n\nSince $m^{(1)\\top} m^{(2)} \\neq 0$, the shared variance is non-zero. The allocation of this shared variance depends on the regressor order. This procedure is analogous to a Type I (sequential) sum-of-squares decomposition in ANOVA. The variance attributed to each regressor is its incremental contribution to the model, given the regressors already entered.\n\n### Option-by-Option Analysis\n\n**A. When parametric modulators are orthogonalized sequentially via Gram–Schmidt while preserving the column span of the original design, the fitted response $\\hat{y}$ is invariant to the order, but the allocation of unique variance to each modulator follows Type $\\mathrm{I}$ sums-of-squares and is order-dependent.**\n-   The invariance of $\\hat{y}$ follows from the invariance of the column space, as demonstrated above. This part is correct.\n-   The \"allocation of unique variance\" (more precisely, the variance accounted for by each orthogonalized component) depends on the sequential order, with earlier regressors being credited for any shared variance. This is the definition of Type I sums-of-squares and is indeed order-dependent due to the correlation between modulators. This part is also correct.\n-   **Verdict: Correct.**\n\n**B. If two modulators are centered and have zero correlation with the baseline stimulus, their unique variance under any orthogonalization order will be equal because centering removes all shared structure that could bias order.**\n-   This statement is flawed. While orthogonality to $1_T$ and orthogonality to $s$ are given, the problem explicitly states that the modulators themselves are correlated: $m^{(1)\\top} m^{(2)} \\neq 0$. This correlation represents shared structure between the modulators that is *not* removed by the other orthogonality conditions. It is precisely this shared structure that causes the order-dependence of variance allocation in a sequential scheme.\n-   **Verdict: Incorrect.**\n\n**C. A principled ordering practice is to place regressors according to theory-driven causal precedence or measurement reliability, so that shared variance is attributed to the regressor with strongest prior justification; and to state the chosen order explicitly in reports.**\n-   This describes a methodological guideline for dealing with the statistical ambiguity arising from correlated predictors. When the analysis method, such as sequential orthogonalization, forces a hierarchical attribution of variance, using external scientific theory to determine the hierarchy is a sound and defensible strategy. This allows the researcher to make a deliberate, transparent choice about how to partition shared variance. Explicitly reporting this choice is a cornerstone of reproducible science. This statement correctly represents best practices in the field.\n-   **Verdict: Correct.**\n\n**D. The only way to achieve order-invariant inference is to enforce complete orthogonality among modulators by residualizing each against all others, which necessarily changes the column span and thus alters the fitted response.**\n-   This statement contains two major errors. First, the claim that this is the \"only way\" is false; option E presents a standard alternative. Second, the claim that enforcing orthogonality \"necessarily changes the column span\" is false. Replacing a set of linearly independent vectors (e.g., $m^{(1)}, m^{(2)}$) with an orthogonal basis for the same subspace (e.g., via Gram-Schmidt or PCA) preserves the subspace they span. Therefore, the overall column span of the design matrix, $\\mathrm{span}(X)$, does not change, and the fitted response $\\hat{y}$ is not altered.\n-   **Verdict: Incorrect.**\n\n**E. To obtain order-invariant tests of each modulator’s unique contribution, compare the full model to a nested model that omits that modulator (e.g., via Frisch–Waugh–Lovell or a partial F-test), which yields identical results irrespective of orthogonalization and ordering.**\n-   This statement describes the procedure for obtaining what are known as Type III sums-of-squares. To test the unique contribution of $m^{(1)}$, one compares the full model ($y \\sim 1_T + s + m^{(1)} + m^{(2)}$) to a reduced model lacking $m^{(1)}$ ($y \\sim 1_T + s + m^{(2)}$). The resulting test statistic (from a partial F-test) quantifies the variance explained by $m^{(1)}$ over and above all other regressors. This is, by definition, an order-invariant procedure. The Frisch-Waugh-Lovell theorem confirms that this is equivalent to testing the significance of the coefficient for $m^{(1)}$ in the original multiple regression of $y$ on $X$, where each coefficient's effect is already partialled out from the others. Such tests depend only on the subspaces spanned by the full and reduced models, which are not affected by regressor ordering or the specific choice of basis (orthogonalized or not).\n-   **Verdict: Correct.**",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "While single-subject models are a crucial starting point, neuroscience often seeks to draw conclusions that generalize across a population. This advanced practice introduces hierarchical Bayesian modeling, a powerful framework for combining data from multiple subjects to achieve more robust and reliable estimates. You will implement a hierarchical model from its mathematical foundations and quantify the core concept of \"posterior shrinkage,\" where noisy individual estimates are pulled toward a more stable group average. ",
            "id": "4155376",
            "problem": "A laboratory has recorded multi-subject neural time series during presentation of a common stimulus. The goal is to design a principled encoding model with an explicit set of covariates and to quantify how hierarchical sharing across subjects induces posterior shrinkage of subject-specific parameters toward a population mean. The encoding model uses a linear design matrix with a stimulus regressor and nuisance covariates, and a hierarchical Gaussian prior tying subject-specific coefficients to a shared group mean.\n\nAssume the following linear-Gaussian encoding model for each subject $s \\in \\{1,\\dots,S\\}$ at time samples $t \\in \\{0,1,\\dots,T-1\\}$:\n- The observed response vector is $y_s \\in \\mathbb{R}^T$.\n- The design matrix is $X \\in \\mathbb{R}^{T \\times p}$ with $p$ covariates. Columns of $X$ represent:\n  - A stimulus feature $x_{\\text{stim},t}$, a deterministic function of time designed to capture stimulus-locked structure.\n  - A nuisance drift covariate $x_{\\text{drift},t}$ to capture slow baseline fluctuations.\n  - An intercept $x_{\\text{int},t}$ to model constant offsets.\n\nThe encoding model and hierarchical priors are:\n- Likelihood (linear Gaussian): $y_s \\mid \\beta_s \\sim \\mathcal{N}(X \\beta_s, \\sigma^2 I_T)$, where $\\beta_s \\in \\mathbb{R}^p$ is the subject-specific parameter vector, $\\sigma^2 > 0$ is the known noise variance, and $I_T$ is the $T \\times T$ identity.\n- Subject-level prior: $\\beta_s \\mid \\mu \\sim \\mathcal{N}(\\mu, \\Sigma_b)$, where $\\mu \\in \\mathbb{R}^p$ is a shared group mean and $\\Sigma_b \\in \\mathbb{R}^{p \\times p}$ is a known positive definite covariance.\n- Hyperprior on the group mean: $\\mu \\sim \\mathcal{N}(m_0, \\Sigma_0)$, with known $m_0 \\in \\mathbb{R}^p$ and positive definite $\\Sigma_0 \\in \\mathbb{R}^{p \\times p}$.\n\nThis is a conjugate linear-Gaussian hierarchy. Starting from the fundamental base facts that the product of Gaussian densities yields another Gaussian and that linear transformations of Gaussian random variables remain Gaussian, one can derive that the joint posterior $p(\\mu, \\{\\beta_s\\}_{s=1}^S \\mid \\{y_s\\}_{s=1}^S)$ is multivariate normal. The posterior mean can be obtained by solving a linear system formed by the joint Gaussian natural parameters. From these posterior means, posterior shrinkage is quantified by how much the subject-specific posterior mean moves toward the group mean relative to the independent (non-hierarchical) estimate.\n\nDefine the independent estimate for subject $s$ as the ordinary least squares solution $\\hat{\\beta}^{\\text{ind}}_s = (X^\\top X)^{-1} X^\\top y_s$. Let the posterior mean of the group mean be $\\hat{\\mu}^{\\text{post}}$ and the posterior mean of the subject-specific parameters be $\\hat{\\beta}^{\\text{post}}_s$. For each subject $s$, define the normalized shrinkage magnitude\n$$\n\\text{shr}_s = \\frac{\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2 - \\left\\lVert \\hat{\\beta}^{\\text{post}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2}{\\max\\left(\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2, \\varepsilon\\right)},\n$$\nwhere $\\varepsilon$ is a small positive constant to avoid division by zero if the denominator vanishes. The average shrinkage across subjects is then\n$$\n\\overline{\\text{shr}} = \\frac{1}{S} \\sum_{s=1}^S \\text{shr}_s,\n$$\nwhich must be reported as a decimal number. All angles used in any trigonometric function must be interpreted in radians.\n\nYour program must implement the above hierarchical encoding model and compute $\\overline{\\text{shr}}$ for each of the following test cases. The design matrix $X$ and responses $y_s$ must be constructed exactly as specified to ensure deterministic results.\n\nFor all test cases, the design matrix has $p = 3$ columns:\n- Column $1$ (stimulus covariate): $x_{\\text{stim},t} = \\cos\\left(\\frac{2\\pi t}{T}\\right)$.\n- Column $2$ (nuisance drift): $x_{\\text{drift},t} = \\frac{t}{T}$.\n- Column $3$ (intercept): $x_{\\text{int},t} = 1$.\n\nThe true subject-specific parameters for subjects $s \\in \\{1,2,3\\}$ are fixed across all test cases:\n- Subject $1$: $\\beta^{\\text{true}}_1 = [1.5, -0.5, 0.2]^\\top$.\n- Subject $2$: $\\beta^{\\text{true}}_2 = [1.0, -0.6, 0.1]^\\top$.\n- Subject $3$: $\\beta^{\\text{true}}_3 = [1.8, -0.4, 0.3]^\\top$.\n\nThe hyperparameters are constant across all test cases:\n- Group prior mean: $m_0 = [0, 0, 0]^\\top$.\n- Group prior covariance: $\\Sigma_0 = \\text{diag}([1.0, 1.0, 1.0])$.\n- Subject prior covariance: $\\Sigma_b = \\text{diag}([0.4, 0.4, 0.4])$.\n\nFor each test case, construct $y_s$ using\n$$\ny_s = X \\beta^{\\text{true}}_s + \\eta_s,\n$$\nwhere $\\eta_s \\in \\mathbb{R}^T$ is a noise vector with independent entries drawn from $\\mathcal{N}(0, \\sigma^2)$ using a fixed pseudorandom generator seed as specified below.\n\nTest Suite:\n- Case $1$ (happy path): $S = 3$, $T = 12$, $\\sigma^2 = 0.2$, noise seed $0$.\n- Case $2$ (high-noise boundary): $S = 3$, $T = 12$, $\\sigma^2 = 3.0$, noise seed $1$.\n- Case $3$ (large-sample edge): $S = 3$, $T = 60$, $\\sigma^2 = 0.05$, noise seed $2$.\n\nRequirements:\n- Implement the hierarchical posterior computation by constructing and solving the joint linear system implied by the multivariate normal posterior in natural parameter form. Do not rely on any black-box Bayesian sampling.\n- Compute the independent estimates $\\hat{\\beta}^{\\text{ind}}_s$, the posterior means $\\hat{\\mu}^{\\text{post}}$ and $\\hat{\\beta}^{\\text{post}}_s$, and the average shrinkage $\\overline{\\text{shr}}$ for each case, with $\\varepsilon = 10^{-12}$.\n- The final output must be a single line containing a list of the average shrinkage values for the three test cases, in the order listed, as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$). Each result must be a float (decimal number). No additional text may be printed.",
            "solution": "The problem is valid. It is scientifically grounded in Bayesian statistical modeling, a standard technique in neuroscience data analysis. The problem is well-posed, providing all necessary equations, parameters, and data generation procedures to arrive at a unique, meaningful solution. It is objective and self-contained, with no ambiguities that would prevent a rigorous formalization.\n\n### 1. Model Specification and Posterior Derivation\nThe problem describes a three-level hierarchical Gaussian model. For each subject $s \\in \\{1, \\dots, S\\}$, the model is defined as:\n- **Level 1 (Likelihood):** $p(y_s | \\beta_s) = \\mathcal{N}(y_s; X \\beta_s, \\sigma^2 I_T)$\n- **Level 2 (Subject-level prior):** $p(\\beta_s | \\mu) = \\mathcal{N}(\\beta_s; \\mu, \\Sigma_b)$\n- **Level 3 (Group-level hyperprior):** $p(\\mu) = \\mathcal{N}(\\mu; m_0, \\Sigma_0)$\n\nThe joint posterior distribution over all unknown parameters, $\\theta = [\\mu^\\top, \\beta_1^\\top, \\dots, \\beta_S^\\top]^\\top$, is given by Bayes' theorem:\n$$\np(\\mu, \\{\\beta_s\\}_{s=1}^S | \\{y_s\\}_{s=1}^S) \\propto p(\\mu) \\prod_{s=1}^S p(y_s | \\beta_s) p(\\beta_s | \\mu)\n$$\nSince the likelihood and priors are all Gaussian, their product is also a Gaussian distribution. The log of the joint posterior, up to a constant, is a quadratic function of the parameters:\n$$\n\\log p(\\theta | \\{y_s\\}) = C - \\frac{1}{2} \\left[ (\\mu - m_0)^\\top \\Sigma_0^{-1}(\\mu - m_0) + \\sum_{s=1}^S (\\beta_s - \\mu)^\\top \\Sigma_b^{-1}(\\beta_s - \\mu) + \\frac{1}{\\sigma^2} \\sum_{s=1}^S (y_s - X\\beta_s)^\\top(y_s - X\\beta_s) \\right]\n$$\nFor a multivariate Gaussian distribution, the posterior mean coincides with the posterior mode. We can find this by setting the gradient of the log-posterior with respect to $\\theta$ to zero. This is equivalent to solving the linear system defined by the natural parameters of the joint posterior distribution.\n\n### 2. The Linear System for Posterior Means\nA Gaussian distribution $p(\\theta) \\propto \\exp(-\\frac{1}{2}\\theta^\\top J \\theta + h^\\top\\theta)$ is parameterized by its precision matrix $J$ and potential vector $h$. The mean is found by solving the system $J\\theta = h$.\nBy expanding the quadratic forms in the log-posterior, we can identify the block structure of the joint precision matrix $J_{\\text{post}}$ and the joint potential vector $h_{\\text{post}}$. The parameter vector is $\\theta = [\\mu^\\top, \\beta_1^\\top, \\dots, \\beta_S^\\top]^\\top$.\n\nThe joint precision matrix $J_{\\text{post}}$ is a $(S+1)p \\times (S+1)p$ block matrix:\n$$\nJ_{\\text{post}} =\n\\begin{pmatrix}\nS\\Sigma_b^{-1} + \\Sigma_0^{-1} & -\\Sigma_b^{-1} & \\cdots & -\\Sigma_b^{-1} \\\\\n-\\Sigma_b^{-1} & \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n-\\Sigma_b^{-1} & 0 & \\cdots & \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}\n\\end{pmatrix}\n$$\nThe joint potential vector $h_{\\text{post}}$ is a $(S+1)p$-dimensional vector:\n$$\nh_{\\text{post}} =\n\\begin{pmatrix}\n\\Sigma_0^{-1} m_0 \\\\\n\\frac{1}{\\sigma^2}X^\\top y_1 \\\\\n\\vdots \\\\\n\\frac{1}{\\sigma^2}X^\\top y_S\n\\end{pmatrix}\n$$\nThe posterior means $\\hat{\\theta}^{\\text{post}} = [\\hat{\\mu}^{\\text{post}\\top}, \\hat{\\beta}_1^{\\text{post}\\top}, \\dots, \\hat{\\beta}_S^{\\text{post}\\top}]^\\top$ are the solution to the linear system $J_{\\text{post}} \\hat{\\theta}^{\\text{post}} = h_{\\text{post}}$.\n\n### 3. Solving the System via Block-wise Decomposition\nSolving this large system directly is inefficient. We can solve it more effectively by block-wise decomposition. The system of equations is:\n$$\n(S\\Sigma_b^{-1} + \\Sigma_0^{-1}) \\hat{\\mu}^{\\text{post}} - \\sum_{s=1}^S \\Sigma_b^{-1} \\hat{\\beta}_s^{\\text{post}} = \\Sigma_0^{-1} m_0 \\quad (1)\n$$\n$$\n-\\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}} + \\left(\\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}\\right) \\hat{\\beta}_s^{\\text{post}} = \\frac{1}{\\sigma^2}X^\\top y_s \\quad \\text{for } s=1, \\dots, S \\quad (2)\n$$\nLet's define the subject-level posterior precision, assuming $\\mu$ were known, as $J_\\beta = \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}$, and the subject-specific potential contribution from the data as $h_s = \\frac{1}{\\sigma^2}X^\\top y_s$. Equation $(2)$ becomes:\n$$\n-\\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}} + J_\\beta \\hat{\\beta}_s^{\\text{post}} = h_s\n$$\nFrom this, we can express $\\hat{\\beta}_s^{\\text{post}}$ in terms of $\\hat{\\mu}^{\\text{post}}$:\n$$\n\\hat{\\beta}_s^{\\text{post}} = J_\\beta^{-1} (h_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}) \\quad (3)\n$$\nSubstituting $(3)$ into $(1)$:\n$$\n(S\\Sigma_b^{-1} + \\Sigma_0^{-1}) \\hat{\\mu}^{\\text{post}} - \\sum_{s=1}^S \\Sigma_b^{-1} J_\\beta^{-1} (h_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}) = \\Sigma_0^{-1} m_0\n$$\nRearranging terms to solve for $\\hat{\\mu}^{\\text{post}}$:\n$$\n\\left(S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}\\right) \\hat{\\mu}^{\\text{post}} = \\Sigma_0^{-1} m_0 + \\sum_{s=1}^S \\Sigma_b^{-1} J_\\beta^{-1} h_s\n$$\nThis is a $p \\times p$ linear system for $\\hat{\\mu}^{\\text{post}}$. Let $J_\\mu = S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}$ and $h_\\mu = \\Sigma_0^{-1} m_0 + \\Sigma_b^{-1} J_\\beta^{-1} \\sum_{s=1}^S h_s$. Then $\\hat{\\mu}^{\\text{post}}$ is the solution to $J_\\mu \\hat{\\mu}^{\\text{post}} = h_\\mu$.\n\n### 4. Algorithmic Procedure\nThe computational steps are as follows:\n1.  For a given test case, construct the design matrix $X \\in \\mathbb{R}^{T \\times p}$ and the subject response vectors $y_s \\in \\mathbb{R}^T$ for $s=1, \\dots, S$ using the provided formulas and pseudorandom seed.\n2.  Compute the independent ordinary least squares (OLS) estimate for each subject: $\\hat{\\beta}^{\\text{ind}}_s = (X^\\top X)^{-1} X^\\top y_s$.\n3.  Compute the matrices required for the posterior calculation:\n    *   $\\Sigma_0^{-1}$ and $\\Sigma_b^{-1}$ (which are simple to compute as they are diagonal).\n    *   $J_\\beta = \\frac{1}{\\sigma^2}X^\\top X + \\Sigma_b^{-1}$.\n4.  Solve for the posterior group mean $\\hat{\\mu}^{\\text{post}}$:\n    *   Compute the precision for $\\mu$: $J_\\mu = S\\Sigma_b^{-1} + \\Sigma_0^{-1} - S\\Sigma_b^{-1} J_\\beta^{-1} \\Sigma_b^{-1}$.\n    *   Compute the potential for $\\mu$: $h_\\mu = \\Sigma_0^{-1} m_0 + \\Sigma_b^{-1} J_\\beta^{-1} \\sum_{s=1}^S (\\frac{1}{\\sigma^2}X^\\top y_s)$.\n    *   Solve the system $J_\\mu \\hat{\\mu}^{\\text{post}} = h_\\mu$.\n5.  Solve for the posterior subject-specific means $\\hat{\\beta}_s^{\\text{post}}$ for each subject using equation $(3)$:\n    *   Solve the system $J_\\beta \\hat{\\beta}_s^{\\text{post}} = \\frac{1}{\\sigma^2}X^\\top y_s + \\Sigma_b^{-1} \\hat{\\mu}^{\\text{post}}$.\n6.  For each subject $s$, calculate the normalized shrinkage magnitude:\n    $$\n    \\text{shr}_s = \\frac{\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2 - \\left\\lVert \\hat{\\beta}^{\\text{post}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2}{\\max\\left(\\left\\lVert \\hat{\\beta}^{\\text{ind}}_s - \\hat{\\mu}^{\\text{post}} \\right\\rVert_2, \\varepsilon\\right)}\n    $$\n7.  Compute the average shrinkage across all subjects: $\\overline{\\text{shr}} = \\frac{1}{S} \\sum_{s=1}^S \\text{shr}_s$.\nThis procedure is implemented for each test case to obtain the final results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the solution for the hierarchical Bayesian encoding model problem.\n    \"\"\"\n    \n    # Define test cases as per the problem statement.\n    test_cases = [\n        {'S': 3, 'T': 12, 'sigma2': 0.2, 'seed': 0},\n        {'S': 3, 'T': 12, 'sigma2': 3.0, 'seed': 1},\n        {'S': 3, 'T': 60, 'sigma2': 0.05, 'seed': 2}\n    ]\n\n    # Fixed parameters across all test cases\n    beta_true_all = [\n        np.array([1.5, -0.5, 0.2]),\n        np.array([1.0, -0.6, 0.1]),\n        np.array([1.8, -0.4, 0.3])\n    ]\n    m0 = np.array([0.0, 0.0, 0.0])\n    Sigma0 = np.diag([1.0, 1.0, 1.0])\n    Sigma_b = np.diag([0.4, 0.4, 0.4])\n    epsilon = 1e-12\n    \n    # Pre-compute inverse matrices (diagonal, so inversion is trivial)\n    Sigma0_inv = np.linalg.inv(Sigma0)\n    Sigma_b_inv = np.linalg.inv(Sigma_b)\n\n    results = []\n    for case in test_cases:\n        S = case['S']\n        T = case['T']\n        sigma2 = case['sigma2']\n        seed = case['seed']\n        p = 3\n\n        # 1. Construct design matrix X and response vectors y_s\n        t = np.arange(T)\n        x_stim = np.cos(2 * np.pi * t / T)\n        x_drift = t / T\n        x_int = np.ones(T)\n        X = np.stack([x_stim, x_drift, x_int], axis=1)\n\n        rng = np.random.default_rng(seed)\n        y_all = []\n        for s in range(S):\n            noise = rng.normal(0, np.sqrt(sigma2), size=T)\n            y_s = X @ beta_true_all[s] + noise\n            y_all.append(y_s)\n\n        # 2. Compute independent OLS estimates\n        XTX = X.T @ X\n        XTX_inv = np.linalg.inv(XTX)\n        beta_ind_all = []\n        for s in range(S):\n            Xy_s = X.T @ y_all[s]\n            beta_ind_s = XTX_inv @ Xy_s\n            beta_ind_all.append(beta_ind_s)\n\n        # 3. Compute matrices for posterior calculation\n        J_beta = (1 / sigma2) * XTX + Sigma_b_inv\n        J_beta_inv = np.linalg.inv(J_beta)\n\n        # 4. Solve for posterior group mean mu_post\n        J_mu = S * Sigma_b_inv + Sigma0_inv - S * (Sigma_b_inv @ J_beta_inv @ Sigma_b_inv)\n        \n        sum_h_s = np.zeros(p)\n        h_s_all = []\n        for s in range(S):\n            h_s = (1/sigma2) * (X.T @ y_all[s])\n            h_s_all.append(h_s)\n            sum_h_s += h_s\n            \n        h_mu_rhs = Sigma0_inv @ m0 + Sigma_b_inv @ J_beta_inv @ sum_h_s\n        mu_post = np.linalg.solve(J_mu, h_mu_rhs)\n\n        # 5. Solve for posterior subject-specific means beta_post_s\n        beta_post_all = []\n        for s in range(S):\n            beta_post_rhs = h_s_all[s] + Sigma_b_inv @ mu_post\n            beta_post_s = np.linalg.solve(J_beta, beta_post_rhs)\n            beta_post_all.append(beta_post_s)\n\n        # 6. Calculate normalized shrinkage for each subject\n        shr_s_all = []\n        for s in range(S):\n            dist_ind = np.linalg.norm(beta_ind_all[s] - mu_post)\n            dist_post = np.linalg.norm(beta_post_all[s] - mu_post)\n            \n            denominator = max(dist_ind, epsilon)\n            shr_s = (dist_ind - dist_post) / denominator\n            shr_s_all.append(shr_s)\n\n        # 7. Compute average shrinkage\n        avg_shr = np.mean(shr_s_all)\n        results.append(avg_shr)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}