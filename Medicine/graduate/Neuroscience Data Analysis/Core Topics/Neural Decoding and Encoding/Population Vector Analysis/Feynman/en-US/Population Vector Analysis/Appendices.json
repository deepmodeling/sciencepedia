{
    "hands_on_practices": [
        {
            "introduction": "Before we can perform a population vector analysis, we must first characterize the tuning properties of individual neurons from experimental data. This practice provides hands-on experience in fitting a Generalized Linear Model (GLM) to neural spike counts, a cornerstone of modern computational neuroscience. By completing this exercise , you will learn how to extract key parameters like a neuron's preferred direction vector directly from raw observations, bridging the gap between experimental data and the idealized models used in population decoding.",
            "id": "4186979",
            "problem": "You are given a set of synthetic spike count datasets for single neurons recorded under a set of planar stimulus directions. Each dataset consists of repeated trials at several stimulus angles. The neuronal response is assumed to follow a Generalized Linear Model (GLM), where spike counts are modeled as conditionally independent Poisson random variables with a canonical logarithmic link function. The GLM uses a first-harmonic basis of direction on the circle. Your task is to implement a method to estimate the direction-dependent tuning of each neuron from spike counts and then extract the neuron's preferred direction vector used in population vector analysis.\n\nFundamental base:\n- Empirical evidence supports modeling spike counts $y$ as a Poisson random variable with mean rate $\\lambda$ over a fixed counting window, that is $y \\sim \\mathrm{Poisson}(\\lambda)$.\n- In a Generalized Linear Model (GLM), the conditional mean $\\lambda$ is related to a linear predictor $\\eta$ through a link function $g$, with $g(\\lambda) = \\eta$; the canonical link for the Poisson is the logarithmic function, $g(\\lambda) = \\log \\lambda$.\n- A direction $\\theta$ on the circle can be represented via the first harmonic basis functions $\\phi_1(\\theta) = 1$, $\\phi_2(\\theta) = \\cos \\theta$, and $\\phi_3(\\theta) = \\sin \\theta$.\n\nProblem requirements:\n1. For each dataset, construct a design matrix $X \\in \\mathbb{R}^{N \\times 3}$ with rows $X_n = \\left[1, \\cos \\theta_n, \\sin \\theta_n\\right]$ for $n = 1, \\dots, N$, where $N$ is the total number of trials across all angles. Assume the GLM form $g(\\lambda_n) = X_n \\boldsymbol{\\beta}$ with $g(\\lambda_n) = \\log \\lambda_n$ and parameter vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^3$ to be estimated by maximum likelihood from observed spike counts $y_n$.\n2. Use Newton–Raphson or Iteratively Reweighted Least Squares (IRLS) to compute the maximum likelihood estimate $\\widehat{\\boldsymbol{\\beta}}$. You must derive the update using the Poisson log-likelihood and its derivatives, ensuring numerically stable updates.\n3. From the fitted $\\widehat{\\boldsymbol{\\beta}} = \\left[\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2\\right]^\\top$, extract a neuron's preferred direction and strength on the circle as follows:\n   - Define the preferred angle $\\widehat{\\varphi} = \\mathrm{atan2}\\left(\\widehat{\\beta}_2, \\widehat{\\beta}_1\\right)$ in radians.\n   - Define the first-harmonic magnitude $\\widehat{m} = \\sqrt{\\widehat{\\beta}_1^2 + \\widehat{\\beta}_2^2}$.\n   - Define the preferred direction vector used in population vector analysis as $\\mathbf{p} = \\left[\\widehat{\\beta}_1, \\widehat{\\beta}_2\\right]^\\top$, which equals $\\widehat{m}\\left[\\cos \\widehat{\\varphi}, \\sin \\widehat{\\varphi}\\right]^\\top$ by trigonometric identities.\n4. Boundary handling: if $\\widehat{m}$ equals $0$ within a numerical tolerance, set $\\widehat{\\varphi} = 0$ by convention and $\\mathbf{p} = [0, 0]^\\top$.\n5. Angle unit specification: all angles $\\theta$ and the preferred angle $\\widehat{\\varphi}$ must be expressed in radians.\n6. Data generation for testing: You must generate synthetic datasets using the GLM and Poisson noise model with a fixed pseudorandom number generator seed. For each test case, let angles be uniformly spaced on $[0, 2\\pi)$, specifically $\\theta_k = \\frac{2\\pi k}{M}$ for $k = 0, 1, \\dots, M - 1$ and repeat each angle $R$ times. For each trial at angle $\\theta$, generate a spike count $y$ from $\\mathrm{Poisson}\\left(\\lambda(\\theta)\\right)$, where $\\lambda(\\theta) = \\exp\\left(\\beta_0 + \\beta_1 \\cos \\theta + \\beta_2 \\sin \\theta\\right)$. Use the fixed seed $12345$ for the pseudorandom number generator to ensure reproducibility.\n7. Test suite: Implement your solution for the following parameter sets, each defining one dataset.\n   - Case $1$: $(\\beta_0, \\beta_1, \\beta_2) = (2.0, 0.8, 0.4)$, $M = 16$, $R = 40$.\n   - Case $2$: $(\\beta_0, \\beta_1, \\beta_2) = (1.0, 0.0, 0.0)$, $M = 16$, $R = 40$.\n   - Case $3$: $(\\beta_0, \\beta_1, \\beta_2) = (1.2, -0.6, 0.6)$, $M = 12$, $R = 25$.\n   - Case $4$: $(\\beta_0, \\beta_1, \\beta_2) = (3.0, 0.05, -0.05)$, $M = 8$, $R = 5$.\n8. Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in order and is itself a list of four floats $[\\widehat{\\varphi}, \\widehat{m}, p_x, p_y]$. For example, the output should look like $[[\\widehat{\\varphi}_1, \\widehat{m}_1, p_{x1}, p_{y1}], [\\widehat{\\varphi}_2, \\widehat{m}_2, p_{x2}, p_{y2}], \\dots]$.\n\nYour implementation must be self-contained and runnable, generate the synthetic data internally with the specified seed, estimate the GLM parameters, and report the final preferred angle in radians and preferred direction vector components for each test case.",
            "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the principles of computational neuroscience and statistical modeling, well-posed with a unique and stable solution, and expressed in objective, formal language. It presents a standard but non-trivial task: fitting a Generalized Linear Model (GLM) to neuronal spike count data and extracting parameters of neuroscientific interest. All necessary data, models, and constraints are provided, and there are no contradictions or ambiguities. I will therefore proceed with a complete solution.\n\n### 1. Model Formulation\n\nThe response variable, the neuronal spike count $y_n$ for a given trial $n$, is assumed to follow a Poisson distribution. The probability mass function is given by:\n$$\nP(y_n | \\lambda_n) = \\frac{\\lambda_n^{y_n} e^{-\\lambda_n}}{y_n!}\n$$\nwhere $\\lambda_n$ is the mean spike rate for that trial.\n\nThe GLM framework connects this mean rate $\\lambda_n$ to a set of predictors through a link function. The problem specifies a canonical logarithmic link function, $g(\\lambda) = \\log \\lambda$. The linear predictor, $\\eta_n$, is a linear combination of basis functions of the stimulus direction $\\theta_n$.\nThe relationship is thus:\n$$\ng(\\lambda_n) = \\log \\lambda_n = \\eta_n\n$$\nInverting the link function gives the mean rate as a function of the linear predictor:\n$$\n\\lambda_n = e^{\\eta_n}\n$$\nThe problem uses a first-harmonic basis for direction on the circle, $[1, \\cos \\theta, \\sin \\theta]$. The parameter vector to be estimated is $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^\\top$. The linear predictor for trial $n$ with stimulus direction $\\theta_n$ is:\n$$\n\\eta_n = \\beta_0 \\cdot 1 + \\beta_1 \\cos \\theta_n + \\beta_2 \\sin \\theta_n = X_n \\boldsymbol{\\beta}\n$$\nwhere $X_n = [1, \\cos \\theta_n, \\sin \\theta_n]$ is the $n$-th row of the design matrix $X$. The full model for the spike rate is:\n$$\n\\lambda_n(\\theta_n; \\boldsymbol{\\beta}) = \\exp(\\beta_0 + \\beta_1 \\cos \\theta_n + \\beta_2 \\sin \\theta_n)\n$$\n\n### 2. Maximum Likelihood Estimation\n\nTo find the optimal parameter vector $\\widehat{\\boldsymbol{\\beta}}$, we maximize the log-likelihood of the observed data. Given a dataset of $N$ independent trials with spike counts $\\boldsymbol{y} = [y_1, \\dots, y_N]^\\top$ and corresponding design matrix $X$, the total log-likelihood is the sum of individual log-likelihoods:\n$$\n\\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{n=1}^{N} \\log P(y_n | \\lambda_n) = \\sum_{n=1}^{N} \\left( y_n \\log \\lambda_n - \\lambda_n - \\log(y_n!) \\right)\n$$\nSubstituting $\\lambda_n = \\exp(X_n \\boldsymbol{\\beta})$ and dropping the constant term $\\log(y_n!)$ which does not depend on $\\boldsymbol{\\beta}$:\n$$\n\\mathcal{L}(\\boldsymbol{\\beta}) \\propto \\sum_{n=1}^{N} \\left( y_n (X_n \\boldsymbol{\\beta}) - e^{X_n \\boldsymbol{\\beta}} \\right)\n$$\nTo maximize this function, we use an iterative numerical optimization method. The problem suggests Newton-Raphson or its equivalent for GLMs, Iteratively Reweighted Least Squares (IRLS). This requires the gradient (score vector) and the Hessian matrix of the log-likelihood function.\n\n**Gradient (Score Vector):**\nThe gradient of the log-likelihood with respect to $\\boldsymbol{\\beta}$ is a vector of partial derivatives $\\nabla_{\\boldsymbol{\\beta}} \\mathcal{L} = \\left[ \\frac{\\partial \\mathcal{L}}{\\partial \\beta_0}, \\frac{\\partial \\mathcal{L}}{\\partial \\beta_1}, \\frac{\\partial \\mathcal{L}}{\\partial \\beta_2} \\right]^\\top$. Using the chain rule:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\beta}} = \\sum_{n=1}^N \\frac{\\partial \\mathcal{L}_n}{\\partial \\lambda_n} \\frac{\\partial \\lambda_n}{\\partial \\eta_n} \\frac{\\partial \\eta_n}{\\partial \\boldsymbol{\\beta}}\n$$\nThe components are: $\\frac{\\partial \\mathcal{L}_n}{\\partial \\lambda_n} = \\frac{y_n}{\\lambda_n} - 1$, $\\frac{\\partial \\lambda_n}{\\partial \\eta_n} = e^{\\eta_n} = \\lambda_n$, and $\\frac{\\partial \\eta_n}{\\partial \\boldsymbol{\\beta}} = X_n^\\top$.\nCombining these gives:\n$$\n\\nabla_{\\boldsymbol{\\beta}} \\mathcal{L}_n = \\left(\\frac{y_n}{\\lambda_n} - 1\\right) \\lambda_n X_n^\\top = (y_n - \\lambda_n) X_n^\\top\n$$\nSumming over all $N$ observations yields the full gradient:\n$$\n\\boldsymbol{g}(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} \\mathcal{L} = \\sum_{n=1}^N (y_n - \\lambda_n) X_n^\\top = X^\\top (\\boldsymbol{y} - \\boldsymbol{\\lambda})\n$$\nwhere $\\boldsymbol{\\lambda} = [\\lambda_1, \\dots, \\lambda_N]^\\top$.\n\n**Hessian Matrix:**\nThe Hessian matrix $H$ contains the second partial derivatives, $H_{jk} = \\frac{\\partial^2 \\mathcal{L}}{\\partial \\beta_j \\partial \\beta_k}$.\n$$\n\\frac{\\partial^2 \\mathcal{L}}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^\\top} = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left( \\sum_{n=1}^N (y_n - \\lambda_n) X_n^\\top \\right) = \\sum_{n=1}^N - \\frac{\\partial \\lambda_n}{\\partial \\boldsymbol{\\beta}^\\top} X_n^\\top\n$$\nSince $\\frac{\\partial \\lambda_n}{\\partial \\boldsymbol{\\beta}^\\top} = \\lambda_n X_n$:\n$$\nH(\\boldsymbol{\\beta}) = \\sum_{n=1}^N - \\lambda_n X_n^\\top X_n = - X^\\top W X\n$$\nwhere $W$ is a diagonal matrix with elements $W_{nn} = \\lambda_n$. For a Poisson GLM with a log link, the log-likelihood is globally concave, as the Hessian is negative semidefinite (and typically negative definite), ensuring a unique maximum.\n\n**Newton-Raphson Update:**\nThe Newton-Raphson algorithm iteratively refines an estimate $\\boldsymbol{\\beta}^{(t)}$ by moving in the direction that maximizes the quadratic approximation of the function:\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [H(\\boldsymbol{\\beta}^{(t)})]^{-1} \\boldsymbol{g}(\\boldsymbol{\\beta}^{(t)})\n$$\nSubstituting our gradient and Hessian:\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [-X^\\top W^{(t)} X]^{-1} [X^\\top (\\boldsymbol{y} - \\boldsymbol{\\lambda}^{(t)})]\n$$\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + (X^\\top W^{(t)} X)^{-1} X^\\top (\\boldsymbol{y} - \\boldsymbol{\\lambda}^{(t)})\n$$\nThis is the core update rule. In practice, we solve the linear system $ (X^\\top W^{(t)} X) \\Delta \\boldsymbol{\\beta} = X^\\top (\\boldsymbol{y} - \\boldsymbol{\\lambda}^{(t)}) $ for $\\Delta \\boldsymbol{\\beta}$ and update $\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\Delta \\boldsymbol{\\beta}$. This is numerically more stable than computing the matrix inverse directly. This update is also equivalent to the IRLS algorithm, where at each step one solves a weighted least squares problem.\n\n### 3. Algorithmic Implementation and Parameter Extraction\n\nThe algorithm proceeds as follows:\n1.  **Data Generation**: For each test case, generate stimulus angles $\\theta_k$ and repeat them $R$ times. Construct the design matrix $X$. Using the true $\\boldsymbol{\\beta}$, compute the mean rates $\\lambda_n$ and generate spike counts $y_n$ from $\\mathrm{Poisson}(\\lambda_n)$. The PRNG seed is fixed to $12345$ for reproducibility.\n2.  **Initialization**: Start the iterative estimation with an initial guess, e.g., $\\boldsymbol{\\beta}^{(0)} = \\boldsymbol{0}$.\n3.  **Iteration**: Loop until convergence:\n    a. Calculate the current linear predictor $\\boldsymbol{\\eta}^{(t)} = X \\boldsymbol{\\beta}^{(t)}$.\n    b. Compute the mean rates $\\boldsymbol{\\lambda}^{(t)} = \\exp(\\boldsymbol{\\eta}^{(t)})$.\n    c. Form the weight matrix $W^{(t)} = \\mathrm{diag}(\\boldsymbol{\\lambda}^{(t)})$.\n    d. Calculate the update step $\\Delta \\boldsymbol{\\beta}$ by solving the system $(X^\\top W^{(t)} X) \\Delta \\boldsymbol{\\beta} = X^\\top (\\boldsymbol{y} - \\boldsymbol{\\lambda}^{(t)})$.\n    e. Update the parameters: $\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\Delta \\boldsymbol{\\beta}$.\n    f. Check for convergence, e.g., if the norm of the update step $||\\Delta \\boldsymbol{\\beta}||$ is smaller than a pre-defined tolerance.\n4.  **Parameter Extraction**: Once the algorithm converges to the MLE $\\widehat{\\boldsymbol{\\beta}} = [\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2]^\\top$, extract the quantities of interest:\n    -   The first-harmonic magnitude, a measure of tuning strength: $\\widehat{m} = \\sqrt{\\widehat{\\beta}_1^2 + \\widehat{\\beta}_2^2}$.\n    -   The preferred direction vector used in population vector analysis: $\\mathbf{p} = [\\widehat{\\beta}_1, \\widehat{\\beta}_2]^\\top$.\n    -   The preferred angle in radians: $\\widehat{\\varphi} = \\mathrm{atan2}(\\widehat{\\beta}_2, \\widehat{\\beta}_1)$.\n5.  **Boundary Handling**: A special case is required when the neuron has no directional tuning, meaning $\\widehat{\\beta}_1$ and $\\widehat{\\beta}_2$ are both zero. Due to numerical noise, they will be small but non-zero. If the magnitude $\\widehat{m}$ is below a small numerical tolerance, we set the preferred angle $\\widehat{\\varphi}$ to $0$ by convention and the preferred direction vector $\\mathbf{p}$ to $[0, 0]^\\top$.\n\nThis completes the theoretical framework and algorithmic design for solving the problem. The final Python code will implement these steps.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem. It generates synthetic neural data,\n    fits a Poisson GLM to estimate tuning parameters, and formats the output.\n    \"\"\"\n\n    def generate_data(betas_true, M, R, rng):\n        \"\"\"\n        Generates synthetic spike count data for a single neuron.\n\n        Args:\n            betas_true (np.ndarray): True GLM parameters [beta0, beta1, beta2].\n            M (int): Number of unique stimulus directions.\n            R (int): Number of repeated trials per direction.\n            rng (np.random.Generator): A numpy random number generator.\n\n        Returns:\n            tuple: A tuple containing the design matrix X (np.ndarray) and\n                   the spike counts y (np.ndarray).\n        \"\"\"\n        # Create stimulus angles uniformly spaced on [0, 2*pi)\n        angles_base = np.linspace(0.0, 2.0 * np.pi, M, endpoint=False)\n        all_angles = np.repeat(angles_base, R)\n        N = M * R\n\n        # Construct the design matrix X\n        X = np.c_[np.ones(N), np.cos(all_angles), np.sin(all_angles)]\n\n        # Generate spike counts from the Poisson GLM\n        eta = X @ betas_true\n        lambdas = np.exp(eta)\n        y = rng.poisson(lambdas)\n\n        return X, y\n\n    def fit_poisson_glm(X, y, max_iter=100, tol=1e-8):\n        \"\"\"\n        Fits a Poisson GLM with a log link using Newton-Raphson/IRLS.\n\n        Args:\n            X (np.ndarray): The design matrix.\n            y (np.ndarray): The vector of observed spike counts.\n            max_iter (int): Maximum number of iterations for the optimization.\n            tol (float): Convergence tolerance for the norm of the beta update.\n\n        Returns:\n            np.ndarray: The estimated GLM parameters (beta_hat).\n        \"\"\"\n        # Initialize beta parameters to zero\n        beta = np.zeros(X.shape[1])\n\n        for _ in range(max_iter):\n            # Current estimate of linear predictor and mean rate\n            eta = X @ beta\n            lambdas = np.exp(eta)\n\n            # Gradient (score vector)\n            grad = X.T @ (y - lambdas)\n\n            # Hessian matrix components\n            # W is a diagonal matrix of weights (the lambdas)\n            # The Hessian is H = -X.T @ W @ X\n            W_diag = lambdas\n            \n            # The system to solve is (X.T @ W @ X) * delta_beta = grad\n            # This is the Newton-Raphson update step\n            # delta_beta = inv(H) @ grad = -inv(-X.T @ W @ X) @ grad\n            # To solve for delta_beta, we use np.linalg.solve for stability\n            # A = X.T @ W @ X\n            # b = grad\n            # A @ delta_beta = b -> delta_beta = solve(A, b)\n            \n            # We construct the matrix for the linear system solver.\n            # Using (X.T * W_diag[:, None]) is an efficient way to compute X.T @ W\n            A = X.T @ (W_diag[:, np.newaxis] * X)\n\n            # Solve the system for the update step delta_beta\n            try:\n                delta_beta = np.linalg.solve(A, grad)\n            except np.linalg.LinAlgError:\n                # In case of singular matrix, stop and return current estimate\n                # This may happen with pathological data, though unlikely here\n                break\n\n            # Update beta\n            beta = beta + delta_beta\n\n            # Check for convergence\n            if np.linalg.norm(delta_beta) < tol:\n                break\n        \n        return beta\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # ( (beta0, beta1, beta2), M, R )\n        ((2.0, 0.8, 0.4), 16, 40),\n        ((1.0, 0.0, 0.0), 16, 40),\n        ((1.2, -0.6, 0.6), 12, 25),\n        ((3.0, 0.05, -0.05), 8, 5)\n    ]\n\n    # Fixed seed for reproducibility\n    seed = 12345\n    rng = np.random.default_rng(seed)\n\n    results = []\n    # Machine epsilon based tolerance for checking if magnitude is zero\n    m_tol = np.finfo(float).eps * 10.0\n\n    for case in test_cases:\n        betas_true, M, R = case\n        \n        # 1. Generate synthetic data\n        X, y = generate_data(np.array(betas_true), M, R, rng)\n\n        # 2. Fit GLM to get estimated parameters\n        beta_hat = fit_poisson_glm(X, y)\n\n        # 3. Extract preferred direction and related quantities\n        b0_hat, b1_hat, b2_hat = beta_hat\n        \n        m_hat = np.sqrt(b1_hat**2 + b2_hat**2)\n\n        if m_hat < m_tol:\n            # Boundary handling for untuned neurons\n            phi_hat = 0.0\n            px, py = 0.0, 0.0\n        else:\n            phi_hat = np.arctan2(b2_hat, b1_hat)\n            px, py = b1_hat, b2_hat\n        \n        # Store results for this case\n        results.append([phi_hat, m_hat, px, py])\n\n    # Final print statement in the exact required format.\n    # The string representation of a list is exactly what's needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The classic population vector decoder performs optimally under idealized assumptions, such as pure cosine tuning. However, real neurons exhibit fundamental nonlinearities like half-wave rectification, where firing rates cannot be negative, and firing thresholds. This simulation  allows you to explore how these biologically realistic features systematically distort the population code and lead to decoding errors, providing crucial intuition about the limitations of the standard decoding model when applied to real neural systems.",
            "id": "4186987",
            "problem": "Consider a two-dimensional population of direction-selective neurons with uniformly distributed preferred directions. The preferred direction of neuron $i$ is denoted by $\\phi_i \\in [0,2\\pi)$, and the stimulus direction is denoted by $\\theta \\in [0,2\\pi)$. The neurons follow a directional tuning based on a cosine function before any nonlinearity, where the pre-nonlinearity response is $r_i^{\\text{pre}} = b + g \\cos(\\theta - \\phi_i)$, with $b$ the baseline and $g$ the gain. Two nonlinear distortions are applied to this response: half-wave rectification and thresholding. Under half-wave rectification, the observed response is $r_i^{\\text{rect}} = \\max(0, r_i^{\\text{pre}})$. Under thresholding followed by rectification, the observed response is $r_i^{\\text{thresh}} = \\max(0, r_i^{\\text{pre}} - T)$, where $T$ is a nonnegative threshold.\n\nPopulation vector decoding reconstructs an estimate of the stimulus direction by vectorially summing each neuron’s contribution, which is proportional to its observed response and points along its preferred direction. The resultant vector $\\mathbf{V}$ aggregates these contributions over all neurons, and the decoded direction $\\hat{\\theta}$ is the argument of $\\mathbf{V}$ computed from its Cartesian components. Angles must be expressed in radians throughout. If the resultant vector has zero magnitude for a particular condition (no active neuron), define the decoding error for that condition to be $\\pi$.\n\nStarting from the core definition of cosine directional tuning and the standard population vector decoding construction, design a programmatic simulation to quantify how half-wave rectification and thresholding distort the decoded direction relative to the ideal cosine-tuned case. For a given parameter set $(N,g,b,T,\\theta)$, where $N$ is the number of neurons, compute:\n- the decoding error under rectification, defined as $e_{\\text{rect}} = |\\mathrm{wrap}_{\\pi}(\\hat{\\theta}_{\\text{rect}} - \\theta)|$,\n- the decoding error under thresholding, defined as $e_{\\text{thresh}} = |\\mathrm{wrap}_{\\pi}(\\hat{\\theta}_{\\text{thresh}} - \\theta)|$,\nwhere $\\mathrm{wrap}_{\\pi}(\\alpha)$ returns the unique angle in $(-\\pi,\\pi]$ congruent to $\\alpha$ modulo $2\\pi$. The ideal cosine-tuned case serves as the unbiased reference, where the decode matches $\\theta$ under uniform preferred directions and no nonlinearity.\n\nYour program must implement the following test suite of parameter values, with angles in radians:\n- Test case $1$: $N = 64$, $g = 1.0$, $b = 0.2$, $T = 0.0$, $\\theta = \\pi/4$.\n- Test case $2$: $N = 64$, $g = 1.0$, $b = 0.2$, $T = 1.0$, $\\theta = \\pi/4$.\n- Test case $3$: $N = 4$, $g = 1.0$, $b = 0.0$, $T = 0.0$, $\\theta = \\pi/2$.\n- Test case $4$: $N = 8$, $g = 1.0$, $b = -0.3$, $T = 0.1$, $\\theta = -\\pi/3$.\n- Test case $5$: $N = 360$, $g = 1.0$, $b = 0.0$, $T = 0.5$, $\\theta = 2.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[e_{\\text{rect},1}, e_{\\text{thresh},1}, e_{\\text{rect},2}, e_{\\text{thresh},2}, \\dots, e_{\\text{rect},5}, e_{\\text{thresh},5}]$, where $e_{\\text{rect},k}$ and $e_{\\text{thresh},k}$ denote the errors for test case $k$. All error values must be expressed in radians as real numbers without units in the output.",
            "solution": "### Step 1: Extract Givens\n- **Population Model**: A two-dimensional population of $N$ direction-selective neurons.\n- **Preferred Directions**: The preferred direction of neuron $i$ is $\\phi_i$, which are uniformly distributed in $[0, 2\\pi)$. For a finite population of size $N$ in a simulation, this is interpreted as being evenly spaced, i.e., $\\phi_i = \\frac{2\\pi i}{N}$ for $i=0, 1, \\dots, N-1$.\n- **Stimulus Direction**: $\\theta \\in [0, 2\\pi)$.\n- **Pre-nonlinearity Response**: The response before nonlinearity is $r_i^{\\text{pre}} = b + g \\cos(\\theta - \\phi_i)$, where $b$ is the baseline and $g$ is the gain.\n- **Nonlinear Response Models**:\n    1.  **Half-wave Rectification**: $r_i^{\\text{rect}} = \\max(0, r_i^{\\text{pre}})$.\n    2.  **Thresholding**: $r_i^{\\text{thresh}} = \\max(0, r_i^{\\text{pre}} - T)$, where $T$ is a nonnegative threshold.\n- **Population Vector Decoding**:\n    - The resultant population vector is $\\mathbf{V}$. Its Cartesian components for a generic response $r_i$ are $V_x = \\sum_i r_i \\cos(\\phi_i)$ and $V_y = \\sum_i r_i \\sin(\\phi_i)$.\n    - The decoded direction is $\\hat{\\theta} = \\mathrm{atan2}(V_y, V_x)$.\n- **Zero Magnitude Condition**: If $|\\mathbf{V}|=0$, the decoding error is defined as $\\pi$.\n- **Decoding Error**:\n    - For rectification: $e_{\\text{rect}} = |\\mathrm{wrap}_{\\pi}(\\hat{\\theta}_{\\text{rect}} - \\theta)|$.\n    - For thresholding: $e_{\\text{thresh}} = |\\mathrm{wrap}_{\\pi}(\\hat{\\theta}_{\\text{thresh}} - \\theta)|$.\n- **Wrap Function**: $\\mathrm{wrap}_{\\pi}(\\alpha)$ maps an angle $\\alpha$ to the unique congruent angle in the interval $(-\\pi, \\pi]$.\n- **Test Cases**:\n    - Case $1$: $(N, g, b, T, \\theta) = (64, 1.0, 0.2, 0.0, \\pi/4)$.\n    - Case $2$: $(N, g, b, T, \\theta) = (64, 1.0, 0.2, 1.0, \\pi/4)$.\n    - Case $3$: $(N, g, b, T, \\theta) = (4, 1.0, 0.0, 0.0, \\pi/2)$.\n    - Case $4$: $(N, g, b, T, \\theta) = (8, 1.0, -0.3, 0.1, -\\pi/3)$.\n    - Case $5$: $(N, g, b, T, \\theta) = (360, 1.0, 0.0, 0.5, 2.5)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in computational neuroscience. Cosine tuning curves, half-wave rectification, and thresholding are standard, albeit simplified, models of neural activity. Population vector analysis is a classical method for decoding information from neural ensembles.\n- **Well-Posed**: The problem is well-posed. All parameters and functions are explicitly defined. The instruction to use evenly spaced preferred directions for the simulation provides a deterministic basis for computation. The procedure for calculating the decoded angle and the error is specified unambiguously. The edge case of a zero-magnitude population vector is explicitly handled.\n- **Objective**: The problem statement is objective, quantitative, and free of subjective or speculative content.\n\n**Flaw Checklist**:\n1.  **Scientific or Factual Unsoundness**: The model is a valid scientific simplification. No flaws.\n2.  **Non-Formalizable or Irrelevant**: The problem is formalizable as a computational simulation and is directly relevant to its stated field. No flaws.\n3.  **Incomplete or Contradictory Setup**: The setup is complete and consistent. All necessary parameters are provided for each test case. No flaws.\n4.  **Unrealistic or Infeasible**: The parameters are within a reasonable range for a theoretical simulation. No flaws.\n5.  **Ill-Posed or Poorly Structured**: The problem is clearly structured, and the definitions (e.g., `atan2`, `wrap_pi`, error metric) ensure a unique, meaningful solution for each test case. No flaws.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires a non-trivial simulation and an understanding of how nonlinearities introduce decoding biases, which is a conceptually important topic in the field. No flaws.\n7.  **Outside Scientific Verifiability**: The results of the simulation are fully verifiable by repeating the specified calculations. No flaws.\n\n### Step 3: Verdict and Action\nThe problem is **valid** as it is scientifically grounded, well-posed, and objective. A programmatic solution can be designed based on the provided specifications.\n\n### Solution\nThe problem requires a simulation to compute the decoding error of a population vector decoder under two different nonlinear response functions: half-wave rectification and thresholding. The core of the solution is to implement the population vector decoding algorithm for a given set of neural and stimulus parameters.\n\n**1. Theoretical Framework**\n\nThe population vector $\\mathbf{V}$ is a weighted sum of vectors pointing in the preferred directions $\\phi_i$ of each neuron in the population. The weight for each neuron is its firing rate $r_i$.\n$$\n\\mathbf{V} = \\sum_{i=0}^{N-1} r_i \\begin{pmatrix} \\cos(\\phi_i) \\\\ \\sin(\\phi_i) \\end{pmatrix}\n$$\nThe Cartesian components of the population vector are:\n$$\nV_x = \\sum_{i=0}^{N-1} r_i \\cos(\\phi_i) \\\\\nV_y = \\sum_{i=0}^{N-1} r_i \\sin(\\phi_i)\n$$\nThe decoded stimulus direction, $\\hat{\\theta}$, is the angle of this resultant vector. It is computed using the two-argument arctangent function to resolve quadrant ambiguity:\n$$\n\\hat{\\theta} = \\mathrm{atan2}(V_y, V_x)\n$$\nThe ideal linear model with a pure cosine tuning curve, $r_i = g \\cos(\\theta - \\phi_i)$, and a sufficiently large, uniformly distributed population of neurons, results in a population vector that points exactly in the stimulus direction $\\theta$, thus yielding zero decoding error. The inclusion of a baseline firing rate $b$ does not introduce a bias for a uniform population, as the contributions from $b$ sum to zero vectorially.\n\nNonlinearities such as rectification and thresholding disrupt this symmetry. They selectively silence neurons whose pre-nonlinearity response is below a certain level. This selective removal of contributions breaks the vector summation symmetry, typically causing the population vector to be biased and thus introducing a decoding error.\n\n**2. Simulation Algorithm**\n\nFor each parameter set $(N, g, b, T, \\theta)$, the simulation proceeds as follows:\n\n**Step 2.1: Initialize Neuronal Population**\nAn array of $N$ preferred directions $\\phi_i$ is generated, representing the neuronal population. These directions are spaced uniformly over the interval $[0, 2\\pi)$:\n$$\n\\phi_i = \\frac{2\\pi i}{N} \\quad \\text{for } i = 0, 1, \\dots, N-1\n$$\n\n**Step 2.2: Compute Neuronal Responses**\nFirst, the pre-nonlinearity response $r_i^{\\text{pre}}$ is calculated for all neurons based on the stimulus direction $\\theta$:\n$$\nr_i^{\\text{pre}} = b + g \\cos(\\theta - \\phi_i)\n$$\nNext, the two nonlinear response models are applied:\n-   **Half-wave Rectification**: The response is set to zero if it is negative.\n    $$\n    r_i^{\\text{rect}} = \\max(0, r_i^{\\text{pre}})\n    $$\n-   **Thresholding**: The response is first reduced by the threshold $T$ and then rectified.\n    $$\n    r_i^{\\text{thresh}} = \\max(0, r_i^{\\text{pre}} - T)\n    $$\n\n**Step 2.3: Compute Population Vectors**\nTwo population vectors are computed, one for each response model.\n\n-   For **rectification**:\n    $$\n    V_{x, \\text{rect}} = \\sum_{i=0}^{N-1} r_i^{\\text{rect}} \\cos(\\phi_i) \\\\\n    V_{y, \\text{rect}} = \\sum_{i=0}^{N-1} r_i^{\\text{rect}} \\sin(\\phi_i)\n    $$\n-   For **thresholding**:\n    $$\n    V_{x, \\text{thresh}} = \\sum_{i=0}^{N-1} r_i^{\\text{thresh}} \\cos(\\phi_i) \\\\\n    V_{y, \\text{thresh}} = \\sum_{i=0}^{N-1} r_i^{\\text{thresh}} \\sin(\\phi_i)\n    $$\n\n**Step 2.4: Decode Stimulus Direction**\nThe decoded angle is found for each model. A check for zero active neurons (i.e., zero vector magnitude) is performed. If all $r_i$ are zero for a given model, the total vector magnitude is zero, and the error is defined as $\\pi$. Otherwise, the angle is computed via $\\mathrm{atan2}$:\n$$\n\\hat{\\theta}_{\\text{rect}} = \\mathrm{atan2}(V_{y, \\text{rect}}, V_{x, \\text{rect}}) \\\\\n\\hat{\\theta}_{\\text{thresh}} = \\mathrm{atan2}(V_{y, \\text{thresh}}, V_{x, \\text{thresh}})\n$$\n\n**Step 2.5: Calculate Decoding Error**\nThe decoding error is the absolute value of the wrapped angular difference between the decoded angle $\\hat{\\theta}$ and the true stimulus angle $\\theta$. The wrapping function $\\mathrm{wrap}_{\\pi}(\\alpha)$ ensures the difference lies in $(-\\pi, \\pi]$.\n$$\n\\Delta\\theta = \\hat{\\theta} - \\theta \\\\\ne = |\\mathrm{wrap}_{\\pi}(\\Delta\\theta)|\n$$\nTo implement $\\mathrm{wrap}_{\\pi}(\\alpha)$, we can map $\\alpha$ to $[-\\pi, \\pi)$ using the modulo operator, and then explicitly handle the lower boundary case where the result is $-\\pi$ by mapping it to $\\pi$.\n$$\n\\alpha' = (\\alpha + \\pi) \\pmod{2\\pi} - \\pi\n$$\nIf $\\alpha' = -\\pi$, it is mapped to $\\pi$. This procedure yields the required angle in $(-\\pi, \\pi]$.\n\nThis completes the simulation for one set of parameters. The process is repeated for all test cases provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the population vector decoding problem for a suite of test cases.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    # Format: (N, g, b, T, theta)\n    test_cases = [\n        (64, 1.0, 0.2, 0.0, np.pi / 4),\n        (64, 1.0, 0.2, 1.0, np.pi / 4),\n        (4, 1.0, 0.0, 0.0, np.pi / 2),\n        (8, 1.0, -0.3, 0.1, -np.pi / 3),\n        (360, 1.0, 0.0, 0.5, 2.5),\n    ]\n\n    results = []\n\n    def wrap_to_pi(x):\n        \"\"\"\n        Wraps angle(s) to the interval (-pi, pi].\n        The expression (x + np.pi) % (2 * np.pi) - np.pi maps to [-pi, pi).\n        This function adjusts the lower bound -pi to pi to match the interval (-pi, pi].\n        \"\"\"\n        y = (x + np.pi) % (2 * np.pi) - np.pi\n        # Use np.where for vectorized conditional logic\n        return np.where(y == -np.pi, np.pi, y)\n\n    def calculate_errors(N, g, b, T, theta):\n        \"\"\"\n        Calculates decoding errors for one set of parameters.\n        \"\"\"\n        # Step 1: Generate uniformly spaced preferred directions\n        phi = np.linspace(0, 2 * np.pi, N, endpoint=False)\n\n        # Step 2: Calculate pre-nonlinearity and post-nonlinearity responses\n        r_pre = b + g * np.cos(theta - phi)\n        r_rect = np.maximum(0, r_pre)\n        r_thresh = np.maximum(0, r_pre - T)\n\n        # Calculate error for rectification\n        # Check for zero magnitude vector (no active neurons)\n        if np.isclose(np.sum(r_rect), 0.0):\n            e_rect = np.pi\n        else:\n            # Step 3: Compute population vector components\n            vx_rect = np.sum(r_rect * np.cos(phi))\n            vy_rect = np.sum(r_rect * np.sin(phi))\n            \n            # Step 4: Decode direction\n            theta_hat_rect = np.arctan2(vy_rect, vx_rect)\n            \n            # Step 5: Calculate decoding error\n            delta_theta_rect = theta_hat_rect - theta\n            e_rect = np.abs(wrap_to_pi(delta_theta_rect))\n\n        # Calculate error for thresholding\n        # Check for zero magnitude vector (no active neurons)\n        if np.isclose(np.sum(r_thresh), 0.0):\n            e_thresh = np.pi\n        else:\n            # Step 3: Compute population vector components\n            vx_thresh = np.sum(r_thresh * np.cos(phi))\n            vy_thresh = np.sum(r_thresh * np.sin(phi))\n\n            # Step 4: Decode direction\n            theta_hat_thresh = np.arctan2(vy_thresh, vx_thresh)\n\n            # Step 5: Calculate decoding error\n            delta_theta_thresh = theta_hat_thresh - theta\n            e_thresh = np.abs(wrap_to_pi(delta_theta_thresh))\n            \n        return e_rect, e_thresh\n\n    # Iterate through all test cases\n    for case in test_cases:\n        N, g, b, T, theta = case\n        e_rect, e_thresh = calculate_errors(N, g, b, T, theta)\n        results.extend([e_rect, e_thresh])\n\n    # Format the final output string\n    # Using a high precision format to avoid rounding issues in verification\n    formatted_results = [f\"{r:.15f}\".rstrip('0').rstrip('.') for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Neural recordings are often contaminated by outliers—abnormally high or low spike counts that can arise from measurement artifacts or intermittent biological phenomena. The standard population vector decoder is highly sensitive to such events, which can severely compromise its accuracy. This advanced exercise  introduces the powerful technique of robust M-estimation, implemented via Iteratively Reweighted Least Squares (IRLS), to build a decoder that is resilient to corrupted data, a critical skill for analyzing real-world neural recordings.",
            "id": "4186990",
            "problem": "Consider a population of $N$ neurons indexed by $i \\in \\{1,\\dots,N\\}$, each characterized by a preferred direction angle $\\phi_i \\in [0,2\\pi)$ and a two-dimensional unit vector $\\mathbf{v}_i = [\\cos\\phi_i, \\sin\\phi_i]$. Assume the neurons obey a widely used cosine tuning model for firing rates: for a movement direction $\\theta \\in [0,2\\pi)$, the firing rate of neuron $i$ is $f_i(\\theta) = b_i + k_i \\cos(\\theta - \\phi_i)$, where $b_i \\ge k_i \\ge 0$ and $b_i$ and $k_i$ are neuron-specific baseline and gain parameters, respectively. Observed spike counts $s_i$ are treated deterministically as the nearest integers to $f_i(\\theta)$, i.e., $s_i = \\mathrm{round}(f_i(\\theta))$. Outliers are modeled by modifying selected $s_i$ by adding or subtracting specified amounts. The classic population vector decoder estimates direction by computing $\\hat{\\theta}$ as the angle of the sum vector $\\sum_{i=1}^{N} s_i \\mathbf{v}_i$.\n\nRobust M-estimation reweights observations based on residuals to reduce the influence of outliers. Let the residual for neuron $i$ at a current estimate $\\hat{\\theta}$ be $r_i = s_i - f_i(\\hat{\\theta})$. Two well-tested robust loss functions are:\n- The Huber loss with scale parameter $\\delta > 0$:\n$$\n\\rho_{\\mathrm{Huber}}(r;\\delta) = \\begin{cases}\n\\frac{1}{2} r^2 & \\text{if } |r| \\le \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^2 & \\text{if } |r| > \\delta,\n\\end{cases}\n$$\n- The Tukey bisquare loss with scale parameter $c > 0$:\n$$\n\\rho_{\\mathrm{Tukey}}(r;c) = \\begin{cases}\n\\frac{c^2}{6} \\left[1 - \\left(1 - \\left(\\frac{r}{c}\\right)^2\\right)^3\\right] & \\text{if } |r| < c, \\\\\n\\frac{c^2}{6} & \\text{if } |r| \\ge c.\n\\end{cases}\n$$\n\nYour task is to implement Iteratively Reweighted Least Squares (IRLS) to construct robust weights from these losses and to analyze their effect on angular error in population vector decoding. Start from the standard population vector estimate $\\hat{\\theta}^{(0)}$ computed from raw counts $s_i$. Then perform $T=3$ IRLS iterations for each robust loss: at iteration $t$, compute residuals $r_i^{(t-1)} = s_i - f_i(\\hat{\\theta}^{(t-1)})$, compute robust weights $w_i^{(t)}$ from the derivative of the chosen loss (do not use any pre-derived \"shortcut\" weight formulas), and update the estimate $\\hat{\\theta}^{(t)}$ as the angle of the weighted sum vector $\\sum_{i=1}^{N} w_i^{(t)} s_i \\mathbf{v}_i$. After $T$ iterations, report the final robust estimate. Quantify the angular error for each method as $e = |\\mathrm{wrap}_{[-\\pi,\\pi]}(\\hat{\\theta} - \\theta_{\\mathrm{true}})|$, where $\\mathrm{wrap}_{[-\\pi,\\pi]}(\\alpha)$ wraps the angle difference $\\alpha$ to the interval $[-\\pi, \\pi]$. Express all angles in radians.\n\nImplement a single program that, for the following test suite, computes and outputs the angular errors for three decoders per case: raw population vector (no reweighting), Huber IRLS, and Tukey IRLS. The preferred directions are shared across cases and given by $\\phi_i \\in \\{0, \\frac{\\pi}{4}, \\frac{\\pi}{2}, \\frac{3\\pi}{4}, \\pi, \\frac{5\\pi}{4}, \\frac{3\\pi}{2}, \\frac{7\\pi}{4}\\}$ for $i=1,\\dots,8$. In all cases, use $s_i = \\mathrm{round}(b_i + k_i \\cos(\\theta_{\\mathrm{true}} - \\phi_i))$ before applying outlier modifications.\n\nTest Suite:\n- Case 1 (general case with mild outliers):\n    - $\\theta_{\\mathrm{true}} = 0.7$,\n    - $b = [20,20,20,20,20,20,20,20]$,\n    - $k = [8,12,15,9,11,10,13,7]$,\n    - Outlier modifications: add $+50$ to indices $i \\in \\{2,6\\}$ (one-based indexing) after rounding; ensure nonnegativity by clamping $s_i \\ge 0$,\n    - Huber parameter $\\delta = 10$,\n    - Tukey parameter $c = 20$.\n- Case 2 (heavy outliers):\n    - $\\theta_{\\mathrm{true}} = 3.5$,\n    - $b = [22,21,23,22,22,21,23,22]$,\n    - $k = [14,12,13,15,14,12,13,11]$,\n    - Outlier modifications: add $+100$ to indices $i \\in \\{1,5\\}$ (one-based indexing),\n    - $\\delta = 7$,\n    - $c = 15$.\n- Case 3 (no outliers; symmetric moderate gains):\n    - $\\theta_{\\mathrm{true}} = \\frac{\\pi}{2}$,\n    - $b = [18,18,18,18,18,18,18,18]$,\n    - $k = [6,6,6,6,6,6,6,6]$,\n    - Outlier modifications: none,\n    - $\\delta = 5$,\n    - $c = 10$.\n- Case 4 (boundary case with some zero gains and a negative outlier):\n    - $\\theta_{\\mathrm{true}} = 5.5$,\n    - $b = [20,20,20,20,20,20,20,20]$,\n    - $k = [0,15,0,15,0,15,0,0]$,\n    - Outlier modifications: subtract $15$ from index $i=2$ (one-based indexing), then clamp $s_i \\ge 0$,\n    - $\\delta = 4$,\n    - $c = 8$.\n\nAngle unit specification: all angles and angular errors must be in radians. The final output format: your program should produce a single line containing a list of four lists, one per test case, where each inner list contains three floating-point numbers rounded to six decimal places: $[e_{\\mathrm{raw}}, e_{\\mathrm{Huber}}, e_{\\mathrm{Tukey}}]$. The line must look like:\n$[[e_{1,\\mathrm{raw}},e_{1,\\mathrm{Huber}},e_{1,\\mathrm{Tukey}}],[e_{2,\\mathrm{raw}},e_{2,\\mathrm{Huber}},e_{2,\\mathrm{Tukey}}],[e_{3,\\mathrm{raw}},e_{3,\\mathrm{Huber}},e_{3,\\mathrm{Tukey}}],[e_{4,\\mathrm{raw}},e_{4,\\mathrm{Huber}},e_{4,\\mathrm{Tukey}}]]$,\nwith no extra text.\n\nYour implementation must be self-contained and deterministic, using only the specified libraries. Ensure scientific realism by preserving nonnegative spike counts after outlier modifications. The program must not read input or write files. The only output is the single line specified above.",
            "solution": "The problem requires the implementation and comparison of three population vector-based neural decoders for estimating a movement direction $\\theta$. The first is the classic population vector decoder, and the other two are robust versions based on Iteratively Reweighted Least Squares (IRLS) using Huber and Tukey bisquare loss functions. The performance of each decoder is to be quantified by its angular error on four distinct test cases.\n\nThe foundational model is a population of $N$ neurons, where each neuron $i$ has a preferred direction vector $\\mathbf{v}_i = [\\cos\\phi_i, \\sin\\phi_i]$ and a firing rate described by the cosine tuning model $f_i(\\theta) = b_i + k_i \\cos(\\theta - \\phi_i)$. The observed spike counts, $s_i$, are a deterministic function of the firing rate, $s_i = \\mathrm{round}(f_i(\\theta))$, with modifications to simulate outliers.\n\nThe core of the task is to implement a specific iterative algorithm to find a robust estimate of $\\theta$. This algorithm is a variant of Iteratively Reweighted Least Squares (IRLS) adapted for the population vector context.\n\n**1. Standard Population Vector (PV) Decoder**\n\nThe standard PV decoder provides a baseline estimate of the movement direction. It operates on the principle that each neuron $i$ \"votes\" for its preferred direction $\\mathbf{v}_i$ with a strength proportional to its spike count $s_i$. The estimate $\\hat{\\theta}$ is the angle of the vector sum of all votes:\n$$\n\\mathbf{V} = \\sum_{i=1}^{N} s_i \\mathbf{v}_i\n$$\nThe decoded angle is then $\\hat{\\theta} = \\mathrm{atan2}(V_y, V_x)$, where $\\mathbf{V} = [V_x, V_y]$. This method is simple and effective but is highly sensitive to outliers, as a neuron with an erroneously large spike count $s_i$ can dominate the sum and heavily skew the resulting vector. This estimate will serve as the initial guess, $\\hat{\\theta}^{(0)}$, for the robust methods.\n\n**2. Robust M-Estimation and the IRLS Principle**\n\nRobust M-estimation techniques are designed to reduce the influence of outliers. Instead of minimizing a standard sum-of-squared-errors loss, which heavily penalizes large errors, they use a loss function $\\rho(r)$ that grows more slowly for large residuals $r$. The goal is to find the parameter $\\theta$ that minimizes the total loss, $\\sum_i \\rho(r_i(\\theta))$, where the residual for neuron $i$ is its deviation from the model's prediction: $r_i(\\theta) = s_i - f_i(\\theta)$.\n\nMinimizing this sum leads to an estimating equation involving the derivative of the loss function, $\\psi(r) = \\rho'(r)$, often called the influence function. IRLS is an iterative procedure to solve this equation. At each step, it re-calculates the contribution of each data point by assigning it a weight. This weight is typically derived from the influence function. A common definition for the weight function is $w(r) = \\psi(r)/r$. Data points with large residuals receive smaller weights, thereby reducing their influence.\n\n**3. Derivation of Robust Weights**\n\nThe problem specifies that weights must be derived from the derivatives of the given loss functions.\n\n**Huber Weights:** The Huber loss function is defined as:\n$$\n\\rho_{\\mathrm{Huber}}(r;\\delta) = \\begin{cases}\n\\frac{1}{2} r^2 & \\text{if } |r| \\le \\delta \\\\\n\\delta |r| - \\frac{1}{2} \\delta^2 & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nIts derivative, the influence function $\\psi_{\\mathrm{Huber}}(r) = \\rho'_{\\mathrm{Huber}}(r)$, is:\n$$\n\\psi_{\\mathrm{Huber}}(r;\\delta) = \\begin{cases}\nr & \\text{if } |r| \\le \\delta \\\\\n\\delta \\cdot \\mathrm{sgn}(r) & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nThe corresponding weight function $w(r) = \\psi(r)/r$ is:\n$$\nw_{\\mathrm{Huber}}(r;\\delta) = \\begin{cases}\n1 & \\text{if } |r| \\le \\delta \\\\\n\\delta/|r| & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nThis can be expressed compactly as $w_{\\mathrm{Huber}}(r;\\delta) = \\min(1, \\delta/|r|)$. This weight is $1$ for small residuals (within the quadratic region) and decreases hyperbolically for large residuals.\n\n**Tukey Bisquare Weights:** The Tukey bisquare (or biweight) loss is:\n$$\n\\rho_{\\mathrm{Tukey}}(r;c) = \\begin{cases}\n\\frac{c^2}{6} \\left[1 - \\left(1 - \\left(\\frac{r}{c}\\right)^2\\right)^3\\right] & \\text{if } |r| < c \\\\\n\\frac{c^2}{6} & \\text{if } |r| \\ge c\n\\end{cases}\n$$\nIts derivative is:\n$$\n\\psi_{\\mathrm{Tukey}}(r;c) = \\begin{cases}\nr \\left(1 - \\left(\\frac{r}{c}\\right)^2\\right)^2 & \\text{if } |r| < c \\\\\n0 & \\text{if } |r| \\ge c\n\\end{cases}\n$$\nThe corresponding weight function $w(r) = \\psi(r)/r$ is:\n$$\nw_{\\mathrm{Tukey}}(r;c) = \\begin{cases}\n\\left(1 - \\left(\\frac{r}{c}\\right)^2\\right)^2 & \\text{if } |r| < c \\\\\n0 & \\text{if } |r| \\ge c\n\\end{cases}\n$$\nThis weight function is more aggressive than Huber's: it smoothly decreases to $0$ as $|r|$ approaches $c$, and completely rejects any observation with a residual $|r| \\ge c$ by assigning it a weight of zero.\n\n**4. The Iteratively Reweighted Population Vector Algorithm**\n\nThe problem specifies a particular iterative algorithm, which adapts the IRLS weighting scheme to the population vector decoder. For a total of $T=3$ iterations, the following steps are performed:\n1.  **Initialization**: The initial estimate of the direction, $\\hat{\\theta}^{(0)}$, is obtained from the standard (raw) population vector decoder.\n2.  **Iteration ($t=1, 2, 3$)**:\n    a.  **Compute Residuals**: For each neuron $i$, calculate the residual based on the previous estimate $\\hat{\\theta}^{(t-1)}$: $r_i^{(t-1)} = s_i - f_i(\\hat{\\theta}^{(t-1)})$.\n    b.  **Compute Weights**: Calculate the robust weight $w_i^{(t)}$ for each neuron using either the Huber or Tukey weight function applied to its residual $r_i^{(t-1)}$.\n    c.  **Update Estimate**: Compute a new weighted population vector $\\mathbf{V}^{(t)} = \\sum_{i=1}^{N} w_i^{(t)} s_i \\mathbf{v}_i$. The updated angle estimate, $\\hat{\\theta}^{(t)}$, is the angle of this vector.\n3.  **Final Estimate**: The estimate after $T=3$ iterations, $\\hat{\\theta}^{(3)}$, is the final robust estimate.\n\n**5. Angular Error Calculation**\n\nTo compare the three methods (raw, Huber, Tukey), the angular error is calculated for each. The error $e$ is the absolute difference between the estimated angle $\\hat{\\theta}$ and the true angle $\\theta_{\\mathrm{true}}$, wrapped to the interval $[-\\pi, \\pi]$ to find the shortest angle between them.\n$$\ne = |\\mathrm{wrap}_{[-\\pi,\\pi]}(\\hat{\\theta} - \\theta_{\\mathrm{true}})|\n$$\nwhere the wrapping function is defined as $\\mathrm{wrap}_{[-\\pi,\\pi]}(\\alpha) = (\\alpha + \\pi) \\pmod{2\\pi} - \\pi$. All angles are handled in radians.\n\nThe implementation will systematically apply this entire procedure to each of the four test cases, calculating the spike counts with outliers, running the three decoder algorithms, and computing the final angular errors for comparison.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the population vector decoding problem with robust methods.\n\n    This function implements and compares three decoders:\n    1. Raw Population Vector (PV)\n    2. Robust PV with Iteratively Reweighted Least Squares (IRLS) using Huber loss.\n    3. Robust PV with IRLS using Tukey bisquare loss.\n\n    It processes a suite of four test cases, computes the angular error for each\n    decoder in each case, and formats the results as a single string.\n    \"\"\"\n\n    def wrap_angle(angle_rad):\n        \"\"\"Wraps an angle in radians to the interval [-pi, pi].\"\"\"\n        return (angle_rad + np.pi) % (2 * np.pi) - np.pi\n\n    # Shared parameters for all test cases\n    N = 8\n    phi = np.arange(N) * np.pi / 4  # Preferred direction angles\n    v_pref = np.stack([np.cos(phi), np.sin(phi)], axis=1)  # Preferred direction vectors\n    T_iterations = 3\n\n    # Define the test suite\n    test_cases = [\n        {\n            \"theta_true\": 0.7,\n            \"b\": [20, 20, 20, 20, 20, 20, 20, 20],\n            \"k\": [8, 12, 15, 9, 11, 10, 13, 7],\n            \"outliers\": {2: 50, 6: 50},  # 1-based index: value to add\n            \"delta\": 10,\n            \"c\": 20,\n        },\n        {\n            \"theta_true\": 3.5,\n            \"b\": [22, 21, 23, 22, 22, 21, 23, 22],\n            \"k\": [14, 12, 13, 15, 14, 12, 13, 11],\n            \"outliers\": {1: 100, 5: 100},\n            \"delta\": 7,\n            \"c\": 15,\n        },\n        {\n            \"theta_true\": np.pi / 2,\n            \"b\": [18, 18, 18, 18, 18, 18, 18, 18],\n            \"k\": [6, 6, 6, 6, 6, 6, 6, 6],\n            \"outliers\": {},\n            \"delta\": 5,\n            \"c\": 10,\n        },\n        {\n            \"theta_true\": 5.5,\n            \"b\": [20, 20, 20, 20, 20, 20, 20, 20],\n            \"k\": [0, 15, 0, 15, 0, 15, 0, 0],\n            \"outliers\": {2: -15},\n            \"delta\": 4,\n            \"c\": 8,\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        theta_true = case[\"theta_true\"]\n        b = np.array(case[\"b\"])\n        k = np.array(case[\"k\"])\n        delta_huber = case[\"delta\"]\n        c_tukey = case[\"c\"]\n\n        # Step 1: Generate spike counts with outliers\n        f_i = b + k * np.cos(theta_true - phi)\n        s = np.round(f_i)\n        \n        for idx, val in case[\"outliers\"].items():\n            s[idx - 1] += val  # Convert 1-based index to 0-based\n        \n        s = np.maximum(0, s) # Ensure non-negative spike counts\n\n        # Step 2: Raw Population Vector Decoder\n        V_raw = np.sum(s[:, np.newaxis] * v_pref, axis=0)\n        theta_raw = np.arctan2(V_raw[1], V_raw[0])\n        e_raw = np.abs(wrap_angle(theta_raw - theta_true))\n\n        # Step 3: Huber IRLS Decoder\n        theta_est = theta_raw\n        for _ in range(T_iterations):\n            residuals = s - (b + k * np.cos(theta_est - phi))\n            weights = np.ones_like(residuals, dtype=float)\n            mask = np.abs(residuals) > delta_huber\n            # Only update weights for residuals outside the delta threshold\n            if np.any(mask):\n                weights[mask] = delta_huber / np.abs(residuals[mask])\n            \n            weighted_s = weights * s\n            V_huber = np.sum(weighted_s[:, np.newaxis] * v_pref, axis=0)\n            theta_est = np.arctan2(V_huber[1], V_huber[0])\n        \n        theta_huber = theta_est\n        e_huber = np.abs(wrap_angle(theta_huber - theta_true))\n\n        # Step 4: Tukey IRLS Decoder\n        theta_est = theta_raw\n        for _ in range(T_iterations):\n            residuals = s - (b + k * np.cos(theta_est - phi))\n            weights = np.zeros_like(residuals, dtype=float)\n            mask = np.abs(residuals)  c_tukey\n            \n            # calculate weights only for residuals inside the c threshold\n            if np.any(mask):\n                ratio_sq = (residuals[mask] / c_tukey)**2\n                weights[mask] = (1 - ratio_sq)**2\n            \n            weighted_s = weights * s\n            V_tukey = np.sum(weighted_s[:, np.newaxis] * v_pref, axis=0)\n            \n            # np.arctan2(0, 0) is 0.0, handling the case of a zero vector gracefully.\n            theta_est = np.arctan2(V_tukey[1], V_tukey[0])\n\n        theta_tukey = theta_est\n        e_tukey = np.abs(wrap_angle(theta_tukey - theta_true))\n\n        all_results.append([e_raw, e_huber, e_tukey])\n    \n    # Step 5: Format the final output string\n    outer_parts = []\n    for res_list in all_results:\n        inner_parts = [f\"{val:.6f}\" for val in res_list]\n        outer_parts.append(f\"[{','.join(inner_parts)}]\")\n    \n    final_output = f\"[{','.join(outer_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}