{
    "hands_on_practices": [
        {
            "introduction": "模板匹配看似一个直观的“最佳拟合”过程，但其最强大的形式是从严格的统计学原理推导出来的。本练习将引导你从第一性原理出发，推导两种常见的模板匹配解码器 。通过分别假设神经脉冲计数服从泊松分布或高斯分布，你将看到最大后验概率（MAP）原则如何直接导出具体的算法——这些算法分别基于线性分数或欧几里得距离。这个过程为这些实用方法提供了坚实的理论基础。",
            "id": "4197771",
            "problem": "考虑一个神经科学数据分析中的群体解码问题，其中模板匹配解码器必须根据从训练数据构建的模板，将单次试验的脉冲计数向量分类到两个刺激类别之一。现有 $3$ 个同时记录的神经元。假设在给定刺激类别的条件下，各神经元的脉冲计数是条件独立的。\n\n考虑两种有科学依据的模板构建策略：\n\n1. 一个生成模型，其中每个类别下每个神经元的脉冲发放是独立的泊松过程，且各个类别的先验概率相等。在此设定下，您需要从第一性原理推导最大后验概率（MAP）分类器如何简化为一个带有内积得分和偏置项的线性模板匹配规则。请用依赖于类别的泊松率明确地表示出依赖于类别的模板向量和偏置项。\n\n2. 一个生成模型，其中作用于依赖于类别的平均脉冲计数上的是可加的、独立的、同分布的、方差与类别无关的高斯噪声。在此设定下，您需要从第一性原理推导MAP分类器如何简化为选择其平均模板与单次试验脉冲计数向量之间平方欧几里得距离最小的类别。请将两个类别之间的决策得分差表示为关于脉冲计数的线性形式加上一个依赖于类别的常数。\n\n对于一个具体实例，假设类别 $s \\in \\{1,2\\}$ 具有由下式给出的神经元特定的泊松率向量\n$$\n\\boldsymbol{\\lambda}_{1} = \\begin{pmatrix} 7 \\\\ 3 \\\\ 12 \\end{pmatrix}, \\quad \\boldsymbol{\\lambda}_{2} = \\begin{pmatrix} 4 \\\\ 5 \\\\ 9 \\end{pmatrix},\n$$\n并且一次试验产生了脉冲计数向量\n$$\n\\mathbf{x} = \\begin{pmatrix} 6 \\\\ 7 \\\\ 10 \\end{pmatrix}.\n$$\n在泊松模型下，构建类别1和2之间的MAP线性决策得分差，并在 $\\mathbf{x}$ 处对其进行求值。在高斯模型（单位协方差）下，构建类别1和2之间的平方欧几里得距离决策得分差，并在 $\\mathbf{x}$ 处对其进行求值。令泊松模型的决策得分差记为 $D_{\\mathrm{Pois}}$，高斯模型的决策得分差记为 $D_{\\mathrm{Gauss}}$。\n\n最后，根据提供的泊松率和脉冲计数，计算标量值\n$$\n\\Delta \\equiv D_{\\mathrm{Pois}} - D_{\\mathrm{Gauss}}\n$$\n。将最终数值答案四舍五入到五位有效数字。无需单位。",
            "solution": "该问题被验证为有科学依据、提法恰当且客观。它包含了所有必要信息，没有矛盾或歧义。所提出的模型（用于脉冲计数的泊松和高斯生成模型）是计算神经科学中的标准模型，任务涉及最大后验概率（MAP）分类器的标准推导。\n\n解答分为三个部分：首先，泊松模型的推导；其次，高斯模型的推导；第三，所要求的具体数值计算。\n\n### 第1部分：泊松生成模型的推导\n\n最大后验概率（MAP）分类器选择在给定观测到的脉冲计数向量 $\\mathbf{x}$ 的情况下，使后验概率 $P(s|\\mathbf{x})$ 最大化的类别 $s$。根据 Bayes 定理，后验概率由下式给出：\n$$\nP(s|\\mathbf{x}) = \\frac{P(\\mathbf{x}|s) P(s)}{P(\\mathbf{x})}\n$$\n最大化后验概率等价于最大化分子 $P(\\mathbf{x}|s) P(s)$，因为证据 $P(\\mathbf{x})$ 对于所有类别是常数。问题陈述两个类别的先验概率相等，即 $P(s=1) = P(s=2)$。因此，MAP分类器简化为最大似然（ML）分类器，它选择使似然 $P(\\mathbf{x}|s)$ 最大化的类别 $s$。\n\n在计算上，处理对数似然更为方便。决策规则是选择使 $\\ln P(\\mathbf{x}|s)$ 最大化的类别。该模型假设 $N=3$ 个神经元中的每一个都服从条件独立的泊松脉冲发放。速率为 $\\lambda$ 的泊松分布的概率质量函数是 $P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$。\n\n给定一个脉冲计数向量 $\\mathbf{x} = (x_1, x_2, \\dots, x_N)^T$ 和一个依赖于类别的速率向量 $\\boldsymbol{\\lambda}_s = (\\lambda_{s,1}, \\lambda_{s,2}, \\dots, \\lambda_{s,N})^T$，由于条件独立性，似然函数是单个神经元概率的乘积：\n$$\nP(\\mathbf{x}|s) = \\prod_{i=1}^{N} P(x_i | s) = \\prod_{i=1}^{N} \\frac{(\\lambda_{s,i})^{x_i} \\exp(-\\lambda_{s,i})}{x_i!}\n$$\n对数似然是：\n$$\n\\ln P(\\mathbf{x}|s) = \\ln \\left( \\prod_{i=1}^{N} \\frac{(\\lambda_{s,i})^{x_i} \\exp(-\\lambda_{s,i})}{x_i!} \\right) = \\sum_{i=1}^{N} \\left( x_i \\ln(\\lambda_{s,i}) - \\lambda_{s,i} - \\ln(x_i!) \\right)\n$$\n为了做出决策，我们比较每个类别的对数似然。任何不依赖于类别 $s$ 的项都可以从决策函数中去掉。项 $\\sum_{i=1}^{N} \\ln(x_i!)$ 与 $s$ 无关，可以省略。这就为每个类别留下一个决策得分 $g_s(\\mathbf{x})$：\n$$\ng_s(\\mathbf{x}) = \\sum_{i=1}^{N} x_i \\ln(\\lambda_{s,i}) - \\sum_{i=1}^{N} \\lambda_{s,i}\n$$\n这个表达式可以写成一个模板向量与数据向量的内积，再加上一个偏置项。设类别 $s$ 的模板向量为 $\\mathbf{w}_s$，偏置项为 $b_s$。我们可以确定：\n$$\n\\mathbf{w}_s = \\begin{pmatrix} \\ln(\\lambda_{s,1}) \\\\ \\ln(\\lambda_{s,2}) \\\\ \\vdots \\\\ \\ln(\\lambda_{s,N}) \\end{pmatrix}, \\quad b_s = - \\sum_{i=1}^{N} \\lambda_{s,i}\n$$\n那么决策得分是 $g_s(\\mathbf{x}) = \\mathbf{w}_s^T \\mathbf{x} + b_s$，这正是所要求的线性模板匹配规则。\n\n### 第2部分：高斯生成模型的推导\n\n在此模型中，脉冲计数向量 $\\mathbf{x}$ 是通过将独立同分布的高斯噪声加到一个依赖于类别的均值向量 $\\boldsymbol{\\mu}_s$ 上生成的：$\\mathbf{x} = \\boldsymbol{\\mu}_s + \\boldsymbol{\\epsilon}$。噪声向量 $\\boldsymbol{\\epsilon}$ 来自一个均值为零、协方差矩阵与类别无关（$\\Sigma = \\sigma^2 I$，$I$ 为单位矩阵）的多元正态分布。\n\n在给定类别 $s$ 的情况下观测到 $\\mathbf{x}$ 的似然由多元高斯概率密度函数给出：\n$$\nP(\\mathbf{x}|s) = \\frac{1}{(2\\pi)^{N/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_s)^T \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_s)\\right)\n$$\n代入 $\\Sigma = \\sigma^2 I$，我们得到 $|\\Sigma|^{1/2} = (\\sigma^{2N})^{1/2} = \\sigma^N$ 和 $\\Sigma^{-1} = \\frac{1}{\\sigma^2} I$。\n$$\nP(\\mathbf{x}|s) = \\frac{1}{(2\\pi \\sigma^2)^{N/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (\\mathbf{x} - \\boldsymbol{\\mu}_s)^T (\\mathbf{x} - \\boldsymbol{\\mu}_s)\\right)\n$$\n项 $(\\mathbf{x} - \\boldsymbol{\\mu}_s)^T (\\mathbf{x} - \\boldsymbol{\\mu}_s)$ 是平方欧几里得距离，即 $\\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2$。\n与泊松情况一样，在先验概率相等的情况下，MAP分类器等价于ML分类器。我们最大化对数似然：\n$$\n\\ln P(\\mathbf{x}|s) = -\\frac{N}{2}\\ln(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2\n$$\n由于 $\\sigma^2$ 与类别无关，项 $-\\frac{N}{2}\\ln(2\\pi \\sigma^2)$ 是一个关于 $s$ 的常数。前导因子 $-\\frac{1}{2\\sigma^2}$ 是一个负常数。因此，最大化对数似然等价于最小化平方欧几里得距离 $\\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2$。这证实了问题中关于高斯模型的第一部分。\n\n为了将决策得分差表示成关于 $\\mathbf{x}$ 的线性形式，我们考虑两个类别之间平方距离的差。我们将得分差定义为 $D_{\\mathrm{Gauss}} = \\|\\mathbf{x} - \\boldsymbol{\\mu}_2\\|^2 - \\|\\mathbf{x} - \\boldsymbol{\\mu}_1\\|^2$。决策规则是如果 $D_{\\mathrm{Gauss}} > 0$，则选择类别1。\n展开平方范数：\n$$\n\\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2 = \\mathbf{x}^T\\mathbf{x} - 2\\boldsymbol{\\mu}_s^T\\mathbf{x} + \\boldsymbol{\\mu}_s^T\\boldsymbol{\\mu}_s\n$$\n差值为：\n$$\nD_{\\mathrm{Gauss}} = (\\mathbf{x}^T\\mathbf{x} - 2\\boldsymbol{\\mu}_2^T\\mathbf{x} + \\boldsymbol{\\mu}_2^T\\boldsymbol{\\mu}_2) - (\\mathbf{x}^T\\mathbf{x} - 2\\boldsymbol{\\mu}_1^T\\mathbf{x} + \\boldsymbol{\\mu}_1^T\\boldsymbol{\\mu}_1)\n$$\n$$\nD_{\\mathrm{Gauss}} = 2\\boldsymbol{\\mu}_1^T\\mathbf{x} - 2\\boldsymbol{\\mu}_2^T\\mathbf{x} + \\boldsymbol{\\mu}_2^T\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1^T\\boldsymbol{\\mu}_1\n$$\n$$\nD_{\\mathrm{Gauss}} = 2(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T\\mathbf{x} + (\\|\\boldsymbol{\\mu}_2\\|^2 - \\|\\boldsymbol{\\mu}_1\\|^2)\n$$\n这个表达式是关于脉冲计数 $\\mathbf{x}$ 的线性形式加上一个依赖于类别的常数，符合要求。\n\n### 第3部分：数值计算\n\n给定的速率向量和单次试验脉冲计数向量为：\n$$\n\\boldsymbol{\\lambda}_{1} = \\begin{pmatrix} 7 \\\\ 3 \\\\ 12 \\end{pmatrix}, \\quad \\boldsymbol{\\lambda}_{2} = \\begin{pmatrix} 4 \\\\ 5 \\\\ 9 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} 6 \\\\ 7 \\\\ 10 \\end{pmatrix}\n$$\n泊松模型的“MAP线性决策得分差”$D_{\\mathrm{Pois}}$是决策得分之差 $g_1(\\mathbf{x}) - g_2(\\mathbf{x})$。\n$$\nD_{\\mathrm{Pois}} = \\left( \\sum_{i=1}^{3} x_i \\ln(\\lambda_{1,i}) - \\sum_{i=1}^{3} \\lambda_{1,i} \\right) - \\left( \\sum_{i=1}^{3} x_i \\ln(\\lambda_{2,i}) - \\sum_{i=1}^{3} \\lambda_{2,i} \\right)\n$$\n$$\nD_{\\mathrm{Pois}} = \\sum_{i=1}^{3} x_i (\\ln(\\lambda_{1,i}) - \\ln(\\lambda_{2,i})) + \\left( \\sum_{i=1}^{3} \\lambda_{2,i} - \\sum_{i=1}^{3} \\lambda_{1,i} \\right)\n$$\n我们来计算各项：\n$\\sum \\lambda_{1,i} = 7 + 3 + 12 = 22$。\n$\\sum \\lambda_{2,i} = 4 + 5 + 9 = 18$。\n点积项是：\n$6(\\ln 7 - \\ln 4) + 7(\\ln 3 - \\ln 5) + 10(\\ln 12 - \\ln 9)$\n$= 6\\ln(\\frac{7}{4}) + 7\\ln(\\frac{3}{5}) + 10\\ln(\\frac{12}{9})$\n$= 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3})$\n所以，$D_{\\mathrm{Pois}} = 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) + (18 - 22) = 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) - 4$。\n数值上，这约等于 $3.35769 - 3.57578 + 2.87682 - 4 = -1.34127$。\n\n对于高斯模型，题目要求我们使用单位协方差矩阵，这意味着 $\\sigma^2=1$。我们假设高斯模型的均值对应于泊松模型的速率，因此 $\\boldsymbol{\\mu}_s = \\boldsymbol{\\lambda}_s$。“平方欧几里得距离决策得分差”是 $D_{\\mathrm{Gauss}} = \\|\\mathbf{x} - \\boldsymbol{\\lambda}_2\\|^2 - \\|\\mathbf{x} - \\boldsymbol{\\lambda}_1\\|^2$。\n$$\nD_{\\mathrm{Gauss}} = 2(\\boldsymbol{\\lambda}_1 - \\boldsymbol{\\lambda}_2)^T\\mathbf{x} + (\\|\\boldsymbol{\\lambda}_2\\|^2 - \\|\\boldsymbol{\\lambda}_1\\|^2)\n$$\n我们来计算各项：\n$\\boldsymbol{\\lambda}_1 - \\boldsymbol{\\lambda}_2 = \\begin{pmatrix} 7-4 \\\\ 3-5 \\\\ 12-9 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -2 \\\\ 3 \\end{pmatrix}$。\n内积项：$2(3, -2, 3) \\begin{pmatrix} 6 \\\\ 7 \\\\ 10 \\end{pmatrix} = 2(3 \\times 6 + (-2) \\times 7 + 3 \\times 10) = 2(18 - 14 + 30) = 2(34) = 68$。\n平方范数之差：\n$\\|\\boldsymbol{\\lambda}_2\\|^2 = 4^2 + 5^2 + 9^2 = 16 + 25 + 81 = 122$。\n$\\|\\boldsymbol{\\lambda}_1\\|^2 = 7^2 + 3^2 + 12^2 = 49 + 9 + 144 = 202$。\n$\\|\\boldsymbol{\\lambda}_2\\|^2 - \\|\\boldsymbol{\\lambda}_1\\|^2 = 122 - 202 = -80$。\n所以，$D_{\\mathrm{Gauss}} = 68 - 80 = -12$。\n\n最后，我们计算 $\\Delta = D_{\\mathrm{Pois}} - D_{\\mathrm{Gauss}}$：\n$$\n\\Delta = (6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) - 4) - (-12)\n$$\n$$\n\\Delta = 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) + 8\n$$\n使用高精度值：\n$D_{\\mathrm{Pois}} \\approx -1.3412639143$。\n$\\Delta = -1.3412639143 - (-12) = 10.6587360857$。\n四舍五入到五位有效数字，我们得到 $10.659$。",
            "answer": "$$\\boxed{10.659}$$"
        },
        {
            "introduction": "在概率框架的基础上，我们可以通过融入先验知识来优化解码器。本练习深入探讨了最大似然（ML）解码与最大后验（MAP）解码之间的关键区别 。你将实现一个能够在这两种规则之间切换的解码器，并亲眼观察考虑到类别不平衡（即某些刺激可能比其他刺激更常见这一事实）如何显著改变解码结果。这个练习凸显了贝叶斯推断的核心概念及其对分类器设计的实际影响。",
            "id": "4197763",
            "problem": "给定一个神经科学数据分析问题，重点关注在类别不平衡和先验效应下的模板匹配解码器。假设一个神经元群体在离散时间窗内产生脉冲计数。在独立泊松脉冲模型下，当刺激类别为 $c$ 时，神经元 $i$ 在一个时间窗内的脉冲计数被建模为速率参数为 $\\mu_{c,i}$ 的泊松随机变量。类别对应于不同的刺激、行为或大脑状态。类别 $c$ 的模板是从训练数据中推断出的平均速率向量 $\\mu_{c} = (\\mu_{c,1}, \\ldots, \\mu_{c,N})$。您必须实现一个解码器，该解码器接收单个观测到的脉冲计数向量 $x = (x_1, \\ldots, x_N)$，并使用两种决策规则返回预测的类别：最大似然 (ML) 和最大后验 (MAP)。\n\n基本原理：\n- 所有神经元的独立泊松似然为 $p(x \\mid c) = \\prod_{i=1}^{N} \\left( e^{-\\mu_{c,i}} \\frac{\\mu_{c,i}^{x_i}}{x_i!} \\right)$，这是基于每个神经元在每个时间窗内的脉冲计数遵循独立泊松过程的假设得出的。\n- Bayes 法则指出 $p(c \\mid x) \\propto p(x \\mid c) \\, p(c)$，其中 $p(c)$ 是类别 $c$ 的先验概率。\n- 对于分类问题，ML 规则选择使 $p(x \\mid c)$ 最大化的类别 $c$；而最大后验 (MAP) 规则选择使 $p(c \\mid x)$ 最大化的类别 $c$，这等价于选择使 $\\log p(x \\mid c) + \\log p(c)$ 最大化的类别，因为 $\\log p(x)$ 与 $c$ 无关。\n\n我们将独立泊松模型下的对数似然定义为\n$$\n\\log p(x \\mid c) = \\sum_{i=1}^{N} \\left( -\\mu_{c,i} + x_i \\log \\mu_{c,i} - \\log(x_i!) \\right),\n$$\n并约定，当 $x_i = 0$ 且 $\\mu_{c,i} = 0$ 时，$x_i \\log \\mu_{c,i}$ 的贡献为 $0$；当 $\\mu_{c,i} = 0$ 且 $x_i > 0$ 时，其贡献为 $-\\infty$。MAP 分数为 $\\log p(x \\mid c) + \\log p(c)$，其中先验概率 $p(c)$ 通过经验训练类别计数 $n_c$ 计算得出，即 $p(c) = n_c / \\sum_{j} n_j$。在 ML 规则中，不使用先验；这等价于假设一个均匀先验，因此只需在不同类别间比较 $\\log p(x \\mid c)$。\n\n平局决胜规则：如果多个类别的分数（ML 或 MAP）相等，则选择类别索引最小的那个类别（例如，在类别 $0$ 和 $1$ 之间出现平局时，选择类别 $0$）。\n\n请实现一个程序，为每个测试用例计算 ML 预测的类别索引、MAP 预测的类别索引，以及一个指示 MAP 预测是否与 ML 预测不同的布尔值。每个测试用例的输出必须是一个列表 $[ml\\_class, map\\_class, changed]$，其中 $ml\\_class$ 和 $map\\_class$ 是整数，$changed$ 是一个布尔值。\n\n测试套件：\n用例 1 (基线，均衡先验，三类别，理想路径)：\n- 神经元数量 $N = 4$。\n- 模板：\n  - 类别 $0$: $\\mu^{(0)} = [5.0, 2.0, 1.0, 0.5]$。\n  - 类别 $1$: $\\mu^{(1)} = [1.0, 5.0, 1.0, 0.5]$。\n  - 类别 $2$: $\\mu^{(2)} = [1.5, 1.5, 4.0, 0.5]$。\n- 训练类别计数: $n = [50, 50, 50]$。\n- 观测值: $x = [4, 2, 1, 0]$。\n\n用例 2 (先验中存在强烈的类别不平衡，三类别，先验引起的翻转)：\n- 神经元数量 $N = 4$。\n- 模板：\n  - 类别 $0$: $\\mu^{(0)} = [2.0, 2.0, 2.0, 1.0]$。\n  - 类别 $1$: $\\mu^{(1)} = [1.0, 5.0, 1.0, 0.2]$。\n  - 类别 $2$: $\\mu^{(2)} = [1.5, 1.5, 3.5, 1.2]$。\n- 训练类别计数: $n = [1000, 20, 5]$。\n- 观测值: $x = [1, 1, 4, 1]$。\n\n用例 3 (两个类别似然值相同，不平衡的先验打破平局)：\n- 神经元数量 $N = 2$。\n- 模板：\n  - 类别 $0$: $\\mu^{(0)} = [2.0, 2.0]$。\n  - 类别 $1$: $\\mu^{(1)} = [2.0, 2.0]$。\n  - 类别 $2$: $\\mu^{(2)} = [4.0, 1.0]$。\n- 训练类别计数: $n = [10, 20, 1]$。\n- 观测值: $x = [3, 1]$。\n\n用例 4 (模板速率为零的边界情况，验证 $-\\infty$ 处理和零计数约定)：\n- 神经元数量 $N = 3$。\n- 模板：\n  - 类别 $0$: $\\mu^{(0)} = [0.0, 3.0, 1.0]$。\n  - 类别 $1$: $\\mu^{(1)} = [0.0, 1.0, 2.0]$。\n  - 类别 $2$: $\\mu^{(2)} = [2.0, 3.0, 0.0]$。\n- 训练类别计数: $n = [30, 30, 30]$。\n- 观测值: $x = [1, 3, 0]$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个元素按顺序对应一个测试用例，并且本身是一个形式为 $[ml\\_class, map\\_class, changed]$ 的列表。例如，两个用例的输出可能看起来像 $[[0,0,False],[1,0,True]]$，但您的程序必须使用上面提供的确切测试套件。",
            "solution": "该问题经评估是有效的。它在计算神经科学和统计学的标准原理上具有科学依据，问题定义良好，包含所有必要信息和明确的目标，并且没有矛盾、歧义或事实错误。因此，我们可以着手提供解决方案。\n\n目标是将一个观测到的神经脉冲计数向量 $x = (x_1, \\ldots, x_N)$ 分类到一组预定义的刺激类别 $\\{c\\}$ 中的一个。每个类别 $c$ 由一个平均脉冲速率的模板向量 $\\mu_c = (\\mu_{c,1}, \\ldots, \\mu_{c,N})$ 来表征。分类将使用两种不同但相关的概率决策规则来执行：最大似然 (ML) 和最大后验 (MAP)。\n\n该解码器的基础是独立泊松脉冲模型。对于类别为 $c$ 的刺激，从神经元 $i$ 观测到脉冲计数为 $x_i$ 的概率由泊松概率质量函数给出：\n$$\np(x_i \\mid c) = \\frac{e^{-\\mu_{c,i}} \\mu_{c,i}^{x_i}}{x_i!}\n$$\n由于假设 $N$ 个神经元之间是独立的，因此在给定类别 $c$ 的情况下，观测到向量 $x$ 的总似然是各个概率的乘积：\n$$\np(x \\mid c) = \\prod_{i=1}^{N} p(x_i \\mid c) = \\prod_{i=1}^{N} \\left( e^{-\\mu_{c,i}} \\frac{\\mu_{c,i}^{x_i}}{x_i!} \\right)\n$$\n为了计算稳定性和分析便利性，标准做法是使用似然的对数。对数将乘积转换为求和：\n$$\n\\log p(x \\mid c) = \\sum_{i=1}^{N} \\log \\left( e^{-\\mu_{c,i}} \\frac{\\mu_{c,i}^{x_i}}{x_i!} \\right) = \\sum_{i=1}^{N} \\left( -\\mu_{c,i} + x_i \\log \\mu_{c,i} - \\log(x_i!) \\right)\n$$\n当对一个固定的观测值 $x$ 比较不同类别时，$\\sum_{i=1}^{N} \\log(x_i!)$ 这一项对于所有类别来说是一个恒定的加性因子。因此，可以在计算中将其省略，而不会影响分类结果。我们可以定义一个简化的分数，我们称之为对数似然分数 $S_{LL}(c)$，它与真实的对数似然成正比：\n$$\nS_{LL}(c) = \\sum_{i=1}^{N} \\left( -\\mu_{c,i} + x_i \\log \\mu_{c,i} \\right)\n$$\n\n问题为涉及速率值为零的边界情况指定了约定：\n1.  如果 $\\mu_{c,i} = 0$ 且 $x_i > 0$，则观测到脉冲是不可能的。似然值为 $0$，对数似然为 $-\\infty$。项 $x_i \\log \\mu_{c,i}$ 的计算结果为 $-\\infty$。\n2.  如果 $\\mu_{c,i} = 0$ 且 $x_i = 0$，观测不到脉冲是完全符合的。应用数学极限 $\\lim_{y \\to 0} y \\log y = 0$，因此项 $x_i \\log \\mu_{c,i}$ 对总和的贡献为 $0$。\n\n**最大似然 (ML) 解码**\n\nML 决策规则选择使观测数据最可能的类别。这等价于选择具有最高似然或对数似然的类别。它隐含地假设所有类别在先验上是等可能的（一个均匀先验）。\n$$\n\\hat{c}_{ML} = \\arg\\max_c p(x \\mid c) = \\arg\\max_c \\log p(x \\mid c) = \\arg\\max_c S_{LL}(c)\n$$\n\n**最大后验 (MAP) 解码**\n\nMAP 决策规则选择在给定观测数据下最可能的类别，它融合了关于类别相对频率的先验知识。使用 Bayes 法则，后验概率为：\n$$\np(c \\mid x) = \\frac{p(x \\mid c) p(c)}{p(x)}\n$$\n$p(x)$ 这一项是与类别 $c$ 无关的归一化常数，在最大化过程中可以被忽略。因此，MAP 解码等价于最大化似然与先验的乘积 $p(x \\mid c)p(c)$。在对数空间中，这变为：\n$$\n\\hat{c}_{MAP} = \\arg\\max_c \\left( \\log p(x \\mid c) + \\log p(c) \\right)\n$$\n先验概率 $p(c)$ 是根据训练数据集中类别出现的经验频率估计得出的，由计数 $n_c$ 给出：\n$$\np(c) = \\frac{n_c}{\\sum_{j} n_j}\n$$\n因此，MAP 分数 $S_{MAP}(c)$ 可以用我们的简化对数似然分数表示为：\n$$\nS_{MAP}(c) = S_{LL}(c) + \\log p(c)\n$$\n当先验不均匀时（即存在类别不平衡），MAP 决策可能与 ML 决策不同。一个具有非常高先验概率的类别，即使其似然不是最高的，也可能被选中，前提是先验能够足够大地“提升”其分数。\n\n**算法实现**\n\n对于提供的每个测试用例，执行以下步骤：\n1.  输入是一个观测向量 $x$、一组类别模板 $\\{\\mu_c\\}$ 和一组训练计数 $\\{n_c\\}$。\n2.  从训练计数 $n_c$ 计算对数先验向量 $\\log p(c)$。如果所有 $n_c$ 都相等，则先验是均匀的，$\\hat{c}_{MAP}$ 将等于 $\\hat{c}_{ML}$。\n3.  对每个类别 $c$：\n    a. 计算对数似然分数 $S_{LL}(c)$ 为 $\\sum_{i=1}^{N} (-\\mu_{c,i} + x_i \\log \\mu_{c,i})$，并小心处理 $\\mu_{c,i}=0$ 的情况。\n    b. 计算 MAP 分数 $S_{MAP}(c)$ 为 $S_{LL}(c) + \\log p(c)$。\n4.  ML 预测的类别 $\\hat{c}_{ML}$ 通过在 $S_{LL}$ 分数数组中找到最大值的索引来确定。\n5.  MAP 预测的类别 $\\hat{c}_{MAP}$ 通过在 $S_{MAP}$ 分数数组中找到最大值的索引来确定。\n6.  指定的平局决胜规则（选择最小的类别索引）可以由标准的 `argmax` 函数自然处理，这些函数会返回最大值的第一个索引。\n7.  如果 $\\hat{c}_{ML} \\neq \\hat{c}_{MAP}$，则一个布尔标志被设置为 `True`，否则为 `False`。\n8.  该测试用例的最终结果被汇总到列表 $[\\hat{c}_{ML}, \\hat{c}_{MAP}, \\text{changed}]$ 中。此过程将应用于所有测试用例以生成最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import xlogy\n\ndef solve():\n    \"\"\"\n    Solves the neuroscience decoding problem for a suite of test cases.\n    For each case, it computes the Maximum Likelihood (ML) and Maximum A \n    Posteriori (MAP) predicted classes for a given spike count observation.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"templates\": np.array([\n                [5.0, 2.0, 1.0, 0.5],  # Class 0\n                [1.0, 5.0, 1.0, 0.5],  # Class 1\n                [1.5, 1.5, 4.0, 0.5]   # Class 2\n            ]),\n            \"counts\": np.array([50, 50, 50]),\n            \"observation\": np.array([4, 2, 1, 0])\n        },\n        {\n            \"templates\": np.array([\n                [2.0, 2.0, 2.0, 1.0],  # Class 0\n                [1.0, 5.0, 1.0, 0.2],  # Class 1\n                [1.5, 1.5, 3.5, 1.2]   # Class 2\n            ]),\n            \"counts\": np.array([1000, 20, 5]),\n            \"observation\": np.array([1, 1, 4, 1])\n        },\n        {\n            \"templates\": np.array([\n                [2.0, 2.0],  # Class 0\n                [2.0, 2.0],  # Class 1\n                [4.0, 1.0]   # Class 2\n            ]),\n            \"counts\": np.array([10, 20, 1]),\n            \"observation\": np.array([3, 1])\n        },\n        {\n            \"templates\": np.array([\n                [0.0, 3.0, 1.0],  # Class 0\n                [0.0, 1.0, 2.0],  # Class 1\n                [2.0, 3.0, 0.0]   # Class 2\n            ]),\n            \"counts\": np.array([30, 30, 30]),\n            \"observation\": np.array([1, 3, 0])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        templates = case[\"templates\"]\n        counts = case[\"counts\"]\n        observation = case[\"observation\"]\n\n        # Calculate log-likelihood scores (proportional to the true log-likelihood)\n        # S_LL(c) = sum_i (-mu_{c,i} + x_i * log(mu_{c,i}))\n        # scipy.special.xlogy(x, y) computes x*log(y) and correctly handles\n        # xlogy(0, y) = 0 and xlogy(x>0, 0) = -inf, as required by the problem.\n        # Numpy broadcasting is used for efficient computation across all classes.\n        # `templates` is (C, N), `observation` is (N,).\n        log_likelihood_scores = np.sum(-templates + xlogy(observation, templates), axis=1)\n\n        # ML prediction: find the class with the maximum log-likelihood score.\n        # np.argmax handles the specified tie-breaking rule (smallest index).\n        ml_class = np.argmax(log_likelihood_scores)\n\n        # Calculate log-priors for MAP.\n        # log p(c) = log(n_c / sum(n_j))\n        total_counts = np.sum(counts)\n        log_priors = np.log(counts / total_counts)\n        \n        # Calculate MAP scores.\n        # S_MAP(c) = S_LL(c) + log p(c)\n        map_scores = log_likelihood_scores + log_priors\n\n        # MAP prediction: find the class with the maximum MAP score.\n        map_class = np.argmax(map_scores)\n\n        # Determine if the prediction changed.\n        changed = (ml_class != map_class)\n        \n        results.append([int(ml_class), int(map_class), bool(changed)])\n\n    # Format the final output string exactly as required.\n    # The str() representation of a list is, e.g., '[0, 0, False]'.\n    # Joining these with commas and enclosing in brackets gives the desired format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理论模型虽然强大，但真实的神经数据很少是完美的，常常混杂着可能误导标准解码器的离群点或噪声。本练习旨在解决构建鲁棒解码器这一关键挑战 。你将超越标准的的最大似然解码器，转而实现基于鲁棒统计学的替代方案，例如使用 Huber 损失和修剪离差。这个练习将向你展示如何设计对异常数据点不那么敏感的解码器，这是创造可靠、实用的神经技术的重要一步。",
            "id": "4197761",
            "problem": "给定一个神经科学数据分析中的群体编码问题，该问题侧重于模板匹配解码器以及对离群值和污染的鲁棒性。考虑一个由 $N$ 个神经元组成的群体，它们对 $S$ 个离散刺激中的一个作出响应。对于每个刺激 $s \\in \\{0,1,2\\}$，一个模板将神经元群体的预期平均脉冲数指定为一个非负向量 $\\boldsymbol{\\lambda}_s \\in \\mathbb{R}_{>0}^N$。单次试验产生一个观测到的非负整数脉冲计数向量 $\\mathbf{k} \\in \\mathbb{Z}_{\\ge 0}^N$。目标是通过将 $\\mathbf{k}$ 与模板集 $\\{\\boldsymbol{\\lambda}_s\\}_{s=0}^2$ 进行比较，从 $\\mathbf{k}$ 中解码（推断）出刺激的身份 $s$。\n\n从一个基本假设出发：在给定刺激的条件下，脉冲计数在神经元之间是独立的泊松随机变量：对于神经元 $i$，$K_i \\mid s \\sim \\mathrm{Poisson}(\\lambda_{s,i})$，其中 $\\lambda_{s,i} > 0$ 是在固定的计数窗口内的预期计数值。最大似然估计（MLE）解码器选择使以下对数似然最大化的刺激 $s$：\n$$\n\\log p(\\mathbf{k} \\mid s) = \\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} - \\log(k_i!) \\right),\n$$\n其中 $\\sum_i \\log(k_i!)$ 项不依赖于 $s$，在比较时可以忽略。真实的神经数据由于非平稳性、噪声爆发、脉冲分类错误或罕见事件，常常包含污染或离群值。可以使用广义线性模型（GLM）的偏差残差和鲁棒聚合来构建经典模板匹配的鲁棒替代方案。定义神经元 $i$ 和刺激 $s$ 的泊松偏差：\n$$\nD_{s,i}(\\mathbf{k}) = 2 \\left( k_i \\log\\left( \\frac{k_i}{\\lambda_{s,i}} \\right) - (k_i - \\lambda_{s,i}) \\right),\n$$\n按照惯例，当 $k_i = 0$ 时，$k_i \\log(k_i/\\lambda_{s,i}) = 0$。带符号偏差残差为：\n$$\nr_{s,i}(\\mathbf{k}) = \\operatorname{sign}(k_i - \\lambda_{s,i}) \\sqrt{ D_{s,i}(\\mathbf{k}) }.\n$$\n可以使用带有阈值 $\\delta > 0$ 的Huber损失函数 $\\rho_\\delta$ 构建一个鲁棒的M估计量目标函数：\n$$\n\\rho_\\delta(x) = \\begin{cases}\n\\frac{1}{2} x^2,  &\\text{if } |x| \\le \\delta, \\\\\n\\delta \\left( |x| - \\frac{1}{2} \\delta \\right),  &\\text{if } |x| > \\delta,\n\\end{cases}\n$$\n刺激 $s$ 的鲁棒分数为 $\\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k}))$，解码器选择使该分数最小化的刺激。另一种鲁棒的替代方法是修剪偏差聚合器，它对 $\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$ 进行排序，并仅对其中最小的一部分（比例为 $\\tau \\in (0,1]$）求和，从而减少与离群值相关的最大残差的影响。\n\n您的任务是实现三种解码器：\n- 经典的泊松MLE模板匹配器，选择使 $\\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} \\right)$ 最大化的 $s$。\n- 鲁棒的Huber偏差模板匹配器，选择使 $\\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k}))$ 最小化的 $s$。\n- 修剪偏差模板匹配器，选择使 $\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$ 中最小的 $\\lfloor \\tau N \\rfloor$ 个值的总和最小化的 $s$。\n\n采用以下通过缩放基础速率向量定义的模板。设 $N = 12$，$S = 3$，基础速率向量为\n$$\n\\mathbf{b} = [\\, 5,\\, 6,\\, 4,\\, 8,\\, 2,\\, 9,\\, 3,\\, 7,\\, 5,\\, 6,\\, 4,\\, 2 \\,],\n$$\n特定于刺激的缩放因子为\n$$\n\\mathbf{f} = [\\, 1.0,\\, 1.5,\\, 0.6 \\,].\n$$\n对于 $s \\in \\{0,1,2\\}$，按分量定义 $\\boldsymbol{\\lambda}_s$ 为 $\\lambda_{s,i} = f_s \\cdot b_i$。具体来说，模板为：\n$$\n\\boldsymbol{\\lambda}_0 = [\\, 5.0,\\, 6.0,\\, 4.0,\\, 8.0,\\, 2.0,\\, 9.0,\\, 3.0,\\, 7.0,\\, 5.0,\\, 6.0,\\, 4.0,\\, 2.0 \\,],\n$$\n$$\n\\boldsymbol{\\lambda}_1 = [\\, 7.5,\\, 9.0,\\, 6.0,\\, 12.0,\\, 3.0,\\, 13.5,\\, 4.5,\\, 10.5,\\, 7.5,\\, 9.0,\\, 6.0,\\, 3.0 \\,],\n$$\n$$\n\\boldsymbol{\\lambda}_2 = [\\, 3.0,\\, 3.6,\\, 2.4,\\, 4.8,\\, 1.2,\\, 5.4,\\, 1.8,\\, 4.2,\\, 3.0,\\, 3.6,\\, 2.4,\\, 1.2 \\,].\n$$\n请注意，这些数值是严格为正的，代表每个固定计数窗口的预期脉冲数；由于输出是离散标签，您无需为最终答案表示任何物理单位。\n\n使用以下观测到的脉冲计数向量测试套件（每个长度为12），其中包括正常情况、污染和边缘情况。每个测试用例都需要使用上述三种方法对刺激进行解码。在所有情况下，使用Huber阈值 $\\delta = 2.0$ 和修剪比例 $\\tau = 0.7$。\n\n- 测试用例1（理想路径，无污染，计数接近 $\\boldsymbol{\\lambda}_1$）：\n$$\n\\mathbf{k}^{(1)} = [\\, 8,\\, 9,\\, 6,\\, 12,\\, 3,\\, 14,\\, 5,\\, 11,\\, 8,\\, 9,\\, 6,\\, 3 \\,].\n$$\n- 测试用例2（严重污染：大多数接近 $\\boldsymbol{\\lambda}_2$，但在三个神经元中有大的离群值）：\n$$\n\\mathbf{k}^{(2)} = [\\, 3,\\, 4,\\, 2,\\, 5,\\, 50,\\, 5,\\, 40,\\, 4,\\, 3,\\, 60,\\, 2,\\, 1 \\,].\n$$\n- 测试用例3（边界情况：所有神经元静默）：\n$$\n\\mathbf{k}^{(3)} = [\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0 \\,].\n$$\n- 测试用例4（单个极端离群值，其余接近于零）：\n$$\n\\mathbf{k}^{(4)} = [\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 100,\\, 0,\\, 0 \\,].\n$$\n- 测试用例5（轻度污染：计数接近 $\\boldsymbol{\\lambda}_0$，但有两个中等程度的离群值）：\n$$\n\\mathbf{k}^{(5)} = [\\, 5,\\, 6,\\, 4,\\, 8,\\, 2,\\, 9,\\, 3,\\, 7,\\, 5,\\, 20,\\, 15,\\, 2 \\,].\n$$\n\n如果分数在 $\\epsilon = 10^{-12}$ 的容差范围内完全相等，则通过选择最小的刺激索引 $s$ 来实现确定性的平局打破规则。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例对应一个由三个整数组成的列表 $[s_{\\mathrm{MLE}}, s_{\\mathrm{Huber}}, s_{\\mathrm{Trim}}]$，分别对应上述三种方法按顺序解码出的标签。例如，五个测试用例的最终输出格式为：\n$$\n[\\, [\\, a_1,\\, b_1,\\, c_1 \\,],\\, [\\, a_2,\\, b_2,\\, c_2 \\,],\\, [\\, a_3,\\, b_3,\\, c_3 \\,],\\, [\\, a_4,\\, b_4,\\, c_4 \\,],\\, [\\, a_5,\\, b_5,\\, c_5 \\,] \\,],\n$$\n其中每个 $a_j$、$b_j$ 和 $c_j$ 都是 $\\{0,1,2\\}$ 中的一个整数。\n\n不需要用户输入。程序必须是自包含的，并能复现上面给出的确切测试套件和参数。",
            "solution": "用户提供的问题是计算神经科学中一个明确定义的任务。它要求实现和比较三种不同的解码算法，用于处理基于脉冲计数泊松统计模型的群体编码神经数据。该问题具有科学依据，数学上精确，并包含唯一解所需的所有数据和参数。因此，该问题被认为是有效的。\n\n问题的核心是从一个由 $N=12$ 个神经元组成的群体产生的观测脉冲计数向量 $\\mathbf{k} \\in \\mathbb{Z}_{\\ge 0}^{N}$ 中推断出刺激 $s \\in \\{0, 1, 2\\}$。推断的基础是将观测值 $\\mathbf{k}$ 与一组预定义的模板 $\\{\\boldsymbol{\\lambda}_s\\}_{s=0}^2$进行比较，其中每个模板 $\\boldsymbol{\\lambda}_s \\in \\mathbb{R}_{>0}^N$ 代表给定刺激 $s$ 的预期脉冲计数。模板的分量由 $\\lambda_{s,i} = f_s \\cdot b_i$ 指定，使用提供的基础速率向量 $\\mathbf{b}$ 和特定于刺激的缩放因子 $\\mathbf{f}$。\n\n解决方案涉及在三种不同的解码方案下为每个刺激 $s$ 计算一个分数，并选择优化该分数的刺激。平局通过选择具有最小索引的刺激来解决。\n\n**1. 泊松最大似然（MLE）解码器**\n\n该解码器基于一个基本假设：在给定刺激 $s$ 的条件下，每个神经元 $i$ 的脉冲计数 $k_i$ 是独立的泊松随机变量，其均值为 $\\lambda_{s,i}$。观测到向量 $\\mathbf{k}$ 的似然函数为：\n$$\np(\\mathbf{k} \\mid s) = \\prod_{i=1}^N \\frac{e^{-\\lambda_{s,i}} \\lambda_{s,i}^{k_i}}{k_i!}\n$$\n为了找到 $s$ 的最大似然估计，我们最大化对数似然：\n$$\n\\log p(\\mathbf{k} \\mid s) = \\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} - \\log(k_i!) \\right)\n$$\n由于项 $\\sum_{i=1}^N \\log(k_i!)$ 与刺激 $s$ 无关，因此在优化过程中可以忽略。因此，MLE解码器的目标是找到使以下分数最大化的刺激 $s$：\n$$\n\\mathcal{S}_{\\text{MLE}}(s; \\mathbf{k}) = \\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} \\right)\n$$\n解码出的刺激为 $s_{\\text{MLE}} = \\arg\\max_{s} \\mathcal{S}_{\\text{MLE}}(s; \\mathbf{k})$。该方法在假定模型下是最优的，但众所周知，它对离群值或模型设定错误很敏感，因为大的计数值 $k_i$ 会通过 $k_i \\log \\lambda_{s,i}$ 项对分数产生很大影响。\n\n**2. 鲁棒Huber偏差解码器**\n\n该解码器是一种鲁棒的替代方法，旨在减轻离群值的影响。它基于偏差残差进行操作，偏差残差衡量了观测数据与模型期望之间的差异。在刺激 $s$ 的假设下，神经元 $i$ 的泊松偏差为：\n$$\nD_{s,i}(\\mathbf{k}) = 2 \\left( k_i \\log\\left( \\frac{k_i}{\\lambda_{s,i}} \\right) - (k_i - \\lambda_{s,i}) \\right)\n$$\n对于 $k_i=0$ 的情况，使用了一个特殊约定，即项 $k_i \\log(k_i/\\lambda_{s,i})$ 取值为 $0$，导致 $D_{s,i} = 2\\lambda_{s,i}$。带符号偏差残差则为：\n$$\nr_{s,i}(\\mathbf{k}) = \\operatorname{sign}(k_i - \\lambda_{s,i}) \\sqrt{ D_{s,i}(\\mathbf{k}) }\n$$\n如果模型正确，这些残差近似服从标准正态分布。为了实现鲁棒性，残差平方和（这与总偏差相关）被一个增长较慢的残差函数之和所取代。为此，使用了带有阈值 $\\delta>0$ 的Huber损失函数 $\\rho_\\delta(x)$：\n$$\n\\rho_\\delta(x) = \\begin{cases}\n\\frac{1}{2} x^2,  &\\text{if } |x| \\le \\delta \\\\\n\\delta \\left( |x| - \\frac{1}{2} \\delta \\right),  &\\text{if } |x| > \\delta\n\\end{cases}\n$$\n对于小的残差 ($|r_{s,i}| \\le \\delta$)，这等同于标准最小二乘法，但对于大的残差，其增长变为线性而非二次，从而降低了离群值的影响。目标是找到使总Huber分数最小化的刺激 $s$，使用给定的阈值 $\\delta = 2.0$：\n$$\n\\mathcal{S}_{\\text{Huber}}(s; \\mathbf{k}) = \\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k}))\n$$\n解码出的刺激为 $s_{\\text{Huber}} = \\arg\\min_{s} \\mathcal{S}_{\\text{Huber}}(s; \\mathbf{k})$。\n\n**3. 修剪偏差解码器**\n\n这是另一种通过显式排除离群值来处理它们的鲁棒方法。它为每个神经元计算偏差 $D_{s,i}(\\mathbf{k})$，但不是将它们全部相加，而是丢弃一小部分最大的偏差，这些偏差被认为对应于污染数据点或离群值。\n\n步骤如下：\n1. 对于给定的刺激 $s$ 和观测值 $\\mathbf{k}$，计算偏差集合 $\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$。\n2. 按非递减顺序对这些偏差进行排序。\n3. 仅对最小的 $\\lfloor \\tau N \\rfloor$ 个偏差求和，其中 $\\tau \\in (0, 1]$ 是修剪比例。\n\n对于这个问题，$N=12$ 且 $\\tau=0.7$，所以我们对 $\\lfloor 0.7 \\times 12 \\rfloor = \\lfloor 8.4 \\rfloor = 8$ 个最小的偏差求和。目标是找到使这个修剪和最小化的刺激 $s$：\n$$\n\\mathcal{S}_{\\text{Trim}}(s; \\mathbf{k}) = \\sum_{j=1}^{\\lfloor \\tau N \\rfloor} D_{(j)}(s; \\mathbf{k})\n$$\n其中 $D_{(j)}$ 是偏差集合 $\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$ 中的第 $j$ 小值。解码出的刺激为 $s_{\\text{Trim}} = \\arg\\min_{s} \\mathcal{S}_{\\text{Trim}}(s; \\mathbf{k})$。\n\n实现过程首先计算三个模板 $\\boldsymbol{\\lambda}_0, \\boldsymbol{\\lambda}_1, \\boldsymbol{\\lambda}_2$。然后，对于每个测试用例向量 $\\mathbf{k}^{(j)}$，为三个刺激中的每一个计算三个分数（$\\mathcal{S}_{\\text{MLE}}$, $\\mathcal{S}_{\\text{Huber}}$, $\\mathcal{S}_{\\text{Trim}}$）。选择使相应分数最优的刺激索引作为该方法的解码刺激。在分数相等（容差为 $\\epsilon = 10^{-12}$）的情况下，应用选择最小刺激索引 $s$ 的平局打破规则。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares three decoders (MLE, Huber, Trimmed) for a \n    neuroscience population coding problem.\n    \"\"\"\n\n    # 1. DEFINE PARAMETERS AND TEMPLATES\n    N = 12\n    S = 3\n    stimuli_indices = np.arange(S)\n    \n    b = np.array([5, 6, 4, 8, 2, 9, 3, 7, 5, 6, 4, 2], dtype=float)\n    f = np.array([1.0, 1.5, 0.6], dtype=float)\n    \n    # Stimulus templates (S x N matrix)\n    lambdas = np.outer(f, b)\n    \n    # Robustness parameters\n    delta = 2.0  # Huber threshold\n    tau = 0.7    # Trimming proportion\n    num_to_keep = int(np.floor(tau * N))\n    \n    # Tie-breaking tolerance\n    epsilon = 1e-12\n\n    test_cases = [\n        np.array([8, 9, 6, 12, 3, 14, 5, 11, 8, 9, 6, 3], dtype=float),\n        np.array([3, 4, 2, 5, 50, 5, 40, 4, 3, 60, 2, 1], dtype=float),\n        np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=float),\n        np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 0, 0], dtype=float),\n        np.array([5, 6, 4, 8, 2, 9, 3, 7, 5, 20, 15, 2], dtype=float),\n    ]\n\n    def argmin_tiebreak(scores):\n        min_score = np.min(scores)\n        tied_indices = np.where(scores = min_score + epsilon)[0]\n        return np.min(tied_indices)\n\n    def argmax_tiebreak(scores):\n        max_score = np.max(scores)\n        tied_indices = np.where(scores >= max_score - epsilon)[0]\n        return np.min(tied_indices)\n\n    all_results = []\n    \n    for k in test_cases:\n        mle_scores = np.zeros(S)\n        huber_scores = np.zeros(S)\n        trim_scores = np.zeros(S)\n        \n        for s in stimuli_indices:\n            lambda_s = lambdas[s]\n            \n            # --- MLE Decoder Score (maximize) ---\n            # Score = sum(k_i * log(lambda_{s,i}) - lambda_{s,i})\n            mle_scores[s] = np.sum(k * np.log(lambda_s) - lambda_s)\n            \n            # --- Deviance Calculation (for Huber and Trimmed) ---\n            # Dev = 2 * (k*log(k/lambda) - (k-lambda))\n            # Handle k=0 case for the log term\n            log_term_dev = np.zeros_like(k)\n            nonzero_k_mask = k > 0\n            log_term_dev[nonzero_k_mask] = k[nonzero_k_mask] * np.log(k[nonzero_k_mask] / lambda_s[nonzero_k_mask])\n            deviances = 2 * (log_term_dev - (k - lambda_s))\n\n            # --- Huber Deviance Decoder Score (minimize) ---\n            residuals = np.sign(k - lambda_s) * np.sqrt(deviances)\n            abs_residuals = np.abs(residuals)\n            huber_losses = np.where(abs_residuals = delta,\n                                  0.5 * residuals**2,\n                                  delta * (abs_residuals - 0.5 * delta))\n            huber_scores[s] = np.sum(huber_losses)\n            \n            # --- Trimmed Deviance Decoder Score (minimize) ---\n            sorted_deviances = np.sort(deviances)\n            trim_scores[s] = np.sum(sorted_deviances[:num_to_keep])\n            \n        # 2. DECODE STIMULUS FOR EACH METHOD\n        s_mle = argmax_tiebreak(mle_scores)\n        s_huber = argmin_tiebreak(huber_scores)\n        s_trim = argmin_tiebreak(trim_scores)\n        \n        all_results.append([s_mle, s_huber, s_trim])\n\n    # 3. FORMAT AND PRINT THE FINAL OUTPUT\n    # Manual string construction to avoid spaces after commas.\n    result_str = \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}