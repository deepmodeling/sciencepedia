{
    "hands_on_practices": [
        {
            "introduction": "Building a functional decoder requires more than just a core algorithm; it involves thoughtful pre-processing of the neural data. This first exercise guides you through the implementation of a template matching decoder, focusing on crucial practical steps like baseline subtraction and normalization. By experimenting with different normalization schemes, you will gain hands-on experience in how these choices can make a decoder robust to irrelevant signal variations, such as global changes in firing rates.",
            "id": "4197759",
            "problem": "You are given a decoding task using template matching for neuronal population responses. The objective is to implement a deterministic template matching decoder that handles baseline subtraction and normalization choices rigorously, motivated by the statistics of independent Poisson spiking. The decoder must operate on a finite set of candidate stimulus templates and a single observed response vector, and it must return the predicted stimulus index. All stimuli indices are $1$-based integers. No physical units are involved.\n\nFundamental base and assumptions:\n- Each neuron $i \\in \\{1,\\dots,N\\}$ emits spike counts over a fixed observation window of duration $T$, modeled as an independent Poisson process with rate $\\lambda_{s,i}$ when stimulus $s \\in \\{1,\\dots,S\\}$ is present.\n- For each stimulus $s$, a template vector $\\mathbf{M}_s \\in \\mathbb{R}_{\\ge 0}^N$ represents the expected spike counts over the window $T$. A baseline vector $\\mathbf{b} \\in \\mathbb{R}_{\\ge 0}^N$ represents spontaneous activity (e.g., pre-stimulus counts) measured over the same duration $T$ or transformed to counts for duration $T$.\n- A single observed spike count vector $\\mathbf{r} \\in \\mathbb{R}_{\\ge 0}^N$ is provided for decoding.\n\nDecoder pipeline to implement:\n1. Baseline handling: compute the baseline-subtracted and rectified vectors\n   $$\\mathbf{r}' = \\max(\\mathbf{r} - \\mathbf{b}, \\mathbf{0}), \\quad \\mathbf{M}_s' = \\max(\\mathbf{M}_s - \\mathbf{b}, \\mathbf{0}),$$\n   where the maximum is applied elementwise.\n2. Normalization mode is one of the following:\n   - Mode A (no normalization): use the raw vectors $\\mathbf{r}'$ and $\\mathbf{M}_s'$.\n   - Mode B (Euclidean $L_2$ normalization): normalize by the Euclidean norm, i.e., for any nonzero vector $\\mathbf{x}$, use $\\widehat{\\mathbf{x}} = \\mathbf{x} / \\|\\mathbf{x}\\|_2$.\n   - Mode C (variance normalization under Poisson statistics): let $\\boldsymbol{\\mu} = \\frac{1}{S}\\sum_{s=1}^S \\mathbf{M}_s'$ be the across-stimulus mean of baseline-subtracted templates. Define an elementwise whitening transform $\\mathcal{W}(\\mathbf{x}) = \\mathbf{x} \\oslash \\sqrt{\\boldsymbol{\\mu} + \\varepsilon}$, where $\\oslash$ denotes elementwise division and $\\varepsilon = 10^{-12}$ is a small constant to avoid division by zero. Apply $\\mathcal{W}$ to both $\\mathbf{r}'$ and all $\\mathbf{M}_s'$. Then apply Euclidean $L_2$ normalization as in Mode B to the whitened vectors.\n3. Similarity and decision rule:\n   - For Mode A, compute the similarity score $\\sigma_s = \\langle \\mathbf{r}', \\mathbf{M}_s' \\rangle$, the standard inner product.\n   - For Mode B and Mode C, compute the cosine similarity $\\sigma_s = \\langle \\widehat{\\mathbf{r}'}, \\widehat{\\mathbf{M}_s'} \\rangle$, where the hat denotes $L_2$ normalization applied after any whitening in Mode C.\n4. Degenerate vector handling and tie-breaking:\n   - Define $\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}$. If both processed vectors for Mode B or Mode C are exactly the zero vector (within numeric arithmetic, treat a vector norm less than $\\varepsilon$ as zero), set $\\sigma_s = 0$ for that $s$.\n   - If exactly one of the two processed vectors is zero in Mode B or Mode C, set $\\sigma_s = -\\infty$ for that $s$.\n   - In Mode A, use the inner product as defined without special cases; a zero vector simply yields the standard inner product.\n   - Predict the stimulus index $\\hat{s}$ that maximizes $\\sigma_s$. In the event of a tie in $\\sigma_s$, select the smallest index $s$ among the maximizers.\n\nYour program must implement this decoder and evaluate it on the following test suite. For each case, return the predicted stimulus index $\\hat{s}$ as an integer. Aggregate the results of all cases into a single printed line as a comma-separated list enclosed in square brackets, with no spaces, e.g., $[a,b,c,d]$.\n\nTest suite:\n- Case $1$ (happy path with no normalization):\n  - $N = 4$, $S = 3$.\n  - Baseline $\\mathbf{b} = [2, 5, 1, 0]$.\n  - Templates $\\mathbf{M}_1 = [10, 7, 3, 2]$, $\\mathbf{M}_2 = [6, 10, 2, 1]$, $\\mathbf{M}_3 = [3, 6, 5, 1]$.\n  - Observation $\\mathbf{r} = [9, 10, 2, 3]$.\n  - Normalization mode: Mode A (no normalization).\n- Case $2$ (global gain robustness via $L_2$ normalization):\n  - $N = 4$, $S = 3$.\n  - Baseline $\\mathbf{b} = [2, 5, 1, 0]$.\n  - Templates $\\mathbf{M}_1 = [10, 7, 3, 2]$, $\\mathbf{M}_2 = [6, 10, 2, 1]$, $\\mathbf{M}_3 = [3, 6, 5, 1]$.\n  - Observation $\\mathbf{r} = [26, 11, 7, 6]$.\n  - Normalization mode: Mode B ($L_2$ normalization).\n- Case $3$ (variance normalization under Poisson statistics):\n  - $N = 3$, $S = 2$.\n  - Baseline $\\mathbf{b} = [0, 0, 0]$.\n  - Templates $\\mathbf{M}_1 = [1000, 2, 4]$, $\\mathbf{M}_2 = [1001, 10, 1]$.\n  - Observation $\\mathbf{r} = [1000, 2, 4]$.\n  - Normalization mode: Mode C (variance normalization then $L_2$ normalization with $\\varepsilon = 10^{-12}$).\n- Case $4$ (boundary case with zero vectors after baseline subtraction):\n  - $N = 2$, $S = 2$.\n  - Baseline $\\mathbf{b} = [5, 5]$.\n  - Templates $\\mathbf{M}_1 = [5, 5]$, $\\mathbf{M}_2 = [6, 5]$.\n  - Observation $\\mathbf{r} = [5, 5]$.\n  - Normalization mode: Mode B ($L_2$ normalization).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the predicted indices for Cases $1$ through $4$ in order, enclosed in square brackets, for example $[1,2,1,1]$. Do not print any other text.",
            "solution": "The problem requires the implementation of a deterministic template matching decoder to predict a stimulus index from a neuronal population response vector. The decoding process involves three main stages: baseline subtraction, a choice of normalization, and a similarity computation, followed by a decision rule. The solution will be derived by meticulously applying the specified algorithm to each of the four test cases. All stimuli indices $s$ are $1$-based.\n\nThe decoder pipeline is defined as:\n1.  **Baseline Subtraction**: Compute rectified, baseline-subtracted vectors $\\mathbf{r}' = \\max(\\mathbf{r} - \\mathbf{b}, \\mathbf{0})$ and $\\mathbf{M}_s' = \\max(\\mathbf{M}_s - \\mathbf{b}, \\mathbf{0})$.\n2.  **Normalization**: Apply one of three modes: Mode A (none), Mode B ($L_2$ norm), or Mode C (variance whitening followed by $L_2$ norm).\n3.  **Similarity**: Compute score $\\sigma_s$. For Mode A, this is the inner product $\\langle \\mathbf{r}', \\mathbf{M}_s' \\rangle$. For Modes B and C, it is the cosine similarity of the normalized vectors.\n4.  **Decision**: The predicted stimulus is $\\hat{s} = \\arg\\max_s \\sigma_s$, with ties broken by choosing the smallest index $s$.\n\nSpecial handling for degenerate zero vectors (defined as vectors with an $L_2$-norm less than $\\varepsilon = 10^{-12}$) is specified for Modes B and C. If both vectors in a comparison pair are zero, their similarity $\\sigma_s$ is $0$. If exactly one is zero, $\\sigma_s = -\\infty$.\n\n### Case 1: Mode A (No Normalization)\nGiven:\n- $N = 4$, $S = 3$.\n- Baseline $\\mathbf{b} = [2, 5, 1, 0]$.\n- Templates $\\mathbf{M}_1 = [10, 7, 3, 2]$, $\\mathbf{M}_2 = [6, 10, 2, 1]$, $\\mathbf{M}_3 = [3, 6, 5, 1]$.\n- Observation $\\mathbf{r} = [9, 10, 2, 3]$.\n- Normalization: Mode A.\n\n1.  **Baseline Subtraction**:\n    - $\\mathbf{r}' = \\max(\\mathbf{r} - \\mathbf{b}, \\mathbf{0}) = \\max([9-2, 10-5, 2-1, 3-0], \\mathbf{0}) = [7, 5, 1, 3]$.\n    - $\\mathbf{M}_1' = \\max(\\mathbf{M}_1 - \\mathbf{b}, \\mathbf{0}) = \\max([10-2, 7-5, 3-1, 2-0], \\mathbf{0}) = [8, 2, 2, 2]$.\n    - $\\mathbf{M}_2' = \\max(\\mathbf{M}_2 - \\mathbf{b}, \\mathbf{0}) = \\max([6-2, 10-5, 2-1, 1-0], \\mathbf{0}) = [4, 5, 1, 1]$.\n    - $\\mathbf{M}_3' = \\max(\\mathbf{M}_3 - \\mathbf{b}, \\mathbf{0}) = \\max([3-2, 6-5, 5-1, 1-0], \\mathbf{0}) = [1, 1, 4, 1]$.\n\n2.  **Normalization**: Mode A requires no normalization.\n\n3.  **Similarity (Inner Product)**:\n    - $\\sigma_1 = \\langle \\mathbf{r}', \\mathbf{M}_1' \\rangle = (7)(8) + (5)(2) + (1)(2) + (3)(2) = 56 + 10 + 2 + 6 = 74$.\n    - $\\sigma_2 = \\langle \\mathbf{r}', \\mathbf{M}_2' \\rangle = (7)(4) + (5)(5) + (1)(1) + (3)(1) = 28 + 25 + 1 + 3 = 57$.\n    - $\\sigma_3 = \\langle \\mathbf{r}', \\mathbf{M}_3' \\rangle = (7)(1) + (5)(1) + (1)(4) + (3)(1) = 7 + 5 + 4 + 3 = 19$.\n\n4.  **Decision**:\n    The scores are $(\\sigma_1, \\sigma_2, \\sigma_3) = (74, 57, 19)$. The maximum score is $74$, which corresponds to stimulus $s=1$.\n    Therefore, $\\hat{s} = 1$.\n\n### Case 2: Mode B ($L_2$ Normalization)\nGiven:\n- $N = 4$, $S = 3$.\n- Baseline $\\mathbf{b} = [2, 5, 1, 0]$.\n- Templates $\\mathbf{M}_1$, $\\mathbf{M}_2$, $\\mathbf{M}_3$ are the same as in Case 1.\n- Observation $\\mathbf{r} = [26, 11, 7, 6]$.\n- Normalization: Mode B.\n\n1.  **Baseline Subtraction**:\n    - The templates $\\mathbf{M}_s'$ are identical to Case 1: $\\mathbf{M}_1'=[8,2,2,2]$, $\\mathbf{M}_2'=[4,5,1,1]$, $\\mathbf{M}_3'=[1,1,4,1]$.\n    - $\\mathbf{r}' = \\max(\\mathbf{r} - \\mathbf{b}, \\mathbf{0}) = \\max([26-2, 11-5, 7-1, 6-0], \\mathbf{0}) = [24, 6, 6, 6]$. We observe that $\\mathbf{r}' = 3 \\cdot \\mathbf{M}_1'$.\n\n2.  **Normalization (Mode B)**:\n    The vectors are normalized to unit length using the $L_2$ norm, $\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}$. The normalized vector is $\\widehat{\\mathbf{x}} = \\mathbf{x} / \\|\\mathbf{x}\\|_2$.\n    - For $\\mathbf{r}' = [24, 6, 6, 6]$ and $\\mathbf{M}_1'=[8,2,2,2]$, since $\\mathbf{r}' = 3 \\mathbf{M}_1'$, their directions are identical. Thus, their normalized vectors are equal: $\\widehat{\\mathbf{r}'} = \\widehat{\\mathbf{M}_1'}$.\n    - None of the vectors $\\mathbf{r}', \\mathbf{M}_1', \\mathbf{M}_2', \\mathbf{M}_3'$ are zero vectors, so no degenerate cases apply.\n\n3.  **Similarity (Cosine Similarity)**:\n    - $\\sigma_1 = \\langle \\widehat{\\mathbf{r}'}, \\widehat{\\mathbf{M}_1'} \\rangle$. Since $\\widehat{\\mathbf{r}'} = \\widehat{\\mathbf{M}_1'}$, their inner product is $\\|\\widehat{\\mathbf{M}_1'}\\|_2^2 = 1$.\n    - $\\sigma_2 = \\langle \\widehat{\\mathbf{r}'}, \\widehat{\\mathbf{M}_2'} \\rangle = \\frac{\\langle \\mathbf{r}', \\mathbf{M}_2' \\rangle}{\\|\\mathbf{r}'\\|_2 \\|\\mathbf{M}_2'\\|_2} = \\frac{(24)(4)+(6)(5)+(6)(1)+(6)(1)}{\\sqrt{24^2+3\\cdot 6^2}\\sqrt{4^2+5^2+1^2+1^2}} = \\frac{138}{\\sqrt{684}\\sqrt{43}} \\approx 0.805$.\n    - $\\sigma_3 = \\langle \\widehat{\\mathbf{r}'}, \\widehat{\\mathbf{M}_3'} \\rangle = \\frac{\\langle \\mathbf{r}', \\mathbf{M}_3' \\rangle}{\\|\\mathbf{r}'\\|_2 \\|\\mathbf{M}_3'\\|_2} = \\frac{(24)(1)+(6)(1)+(6)(4)+(6)(1)}{\\sqrt{684}\\sqrt{1^2+1^2+4^2+1^2}} = \\frac{60}{\\sqrt{684}\\sqrt{19}} \\approx 0.526$.\n\n4.  **Decision**:\n    The scores are $(\\sigma_1, \\sigma_2, \\sigma_3) \\approx (1, 0.805, 0.526)$. The maximum score is $1$, which corresponds to stimulus $s=1$.\n    Therefore, $\\hat{s} = 1$.\n\n### Case 3: Mode C (Variance Normalization)\nGiven:\n- $N = 3$, $S = 2$.\n- Baseline $\\mathbf{b} = [0, 0, 0]$.\n- Templates $\\mathbf{M}_1 = [1000, 2, 4]$, $\\mathbf{M}_2 = [1001, 10, 1]$.\n- Observation $\\mathbf{r} = [1000, 2, 4]$.\n- Normalization: Mode C with $\\varepsilon = 10^{-12}$.\n\n1.  **Baseline Subtraction**:\n    - With $\\mathbf{b} = \\mathbf{0}$, we have $\\mathbf{r}'=\\mathbf{r}$, $\\mathbf{M}_1'=\\mathbf{M}_1$, $\\mathbf{M}_2'=\\mathbf{M}_2$.\n    - $\\mathbf{r}' = [1000, 2, 4]$ and $\\mathbf{M}_1'=[1000, 2, 4]$, so $\\mathbf{r}' = \\mathbf{M}_1'$.\n\n2.  **Normalization (Mode C)**:\n    - First, compute the mean of baseline-subtracted templates: $\\boldsymbol{\\mu} = \\frac{1}{S}\\sum_{s=1}^S \\mathbf{M}_s' = \\frac{1}{2}(\\mathbf{M}_1' + \\mathbf{M}_2') = \\frac{1}{2}([1000, 2, 4] + [1001, 10, 1]) = [1000.5, 6, 2.5]$.\n    - Next, apply the elementwise whitening transform $\\mathcal{W}(\\mathbf{x}) = \\mathbf{x} \\oslash \\sqrt{\\boldsymbol{\\mu} + \\varepsilon}$ to $\\mathbf{r}'$ and both $\\mathbf{M}_s'$. Let $\\mathbf{w}(\\cdot)$ denote the whitened vectors.\n    - Since $\\mathbf{r}' = \\mathbf{M}_1'$, their whitened versions will also be identical: $\\mathbf{w}(\\mathbf{r}') = \\mathbf{w}(\\mathbf{M}_1')$.\n    - The subsequent $L_2$ normalization will also produce identical unit vectors: $\\widehat{\\mathbf{w}(\\mathbf{r}')} = \\widehat{\\mathbf{w}(\\mathbf{M}_1')}$.\n    - None of the vectors become zero vectors during this process.\n\n3.  **Similarity (Cosine Similarity)**:\n    - $\\sigma_1 = \\langle \\widehat{\\mathbf{w}(\\mathbf{r}')}, \\widehat{\\mathbf{w}(\\mathbf{M}_1')} \\rangle$. Since the vectors are identical and non-zero, their cosine similarity is $1$.\n    - $\\sigma_2 = \\langle \\widehat{\\mathbf{w}(\\mathbf{r}')}, \\widehat{\\mathbf{w}(\\mathbf{M}_2')} \\rangle$. Since $\\mathbf{M}_1'$ and $\\mathbf{M}_2'$ are not collinear, $\\mathbf{w}(\\mathbf{M}_1')$ and $\\mathbf{w}(\\mathbf{M}_2')$ will not be collinear, so their cosine similarity will be strictly less than $1$. Explicit calculation gives $\\sigma_2 \\approx 0.993$.\n\n4.  **Decision**:\n    The scores are $(\\sigma_1, \\sigma_2) = (1, \\approx 0.993)$. The maximum score is $1$, corresponding to stimulus $s=1$.\n    Therefore, $\\hat{s} = 1$.\n\n### Case 4: Boundary Case with Zero Vectors\nGiven:\n- $N = 2$, $S = 2$.\n- Baseline $\\mathbf{b} = [5, 5]$.\n- Templates $\\mathbf{M}_1 = [5, 5]$, $\\mathbf{M}_2 = [6, 5]$.\n- Observation $\\mathbf{r} = [5, 5]$.\n- Normalization: Mode B.\n\n1.  **Baseline Subtraction**:\n    - $\\mathbf{r}' = \\max(\\mathbf{r} - \\mathbf{b}, \\mathbf{0}) = \\max([5-5, 5-5], \\mathbf{0}) = [0, 0]$.\n    - $\\mathbf{M}_1' = \\max(\\mathbf{M}_1 - \\mathbf{b}, \\mathbf{0}) = \\max([5-5, 5-5], \\mathbf{0}) = [0, 0]$.\n    - $\\mathbf{M}_2' = \\max(\\mathbf{M}_2 - \\mathbf{b}, \\mathbf{0}) = \\max([6-5, 5-5], \\mathbf{0}) = [1, 0]$.\n\n2.  **Normalization (Mode B) and Degenerate Cases**:\n    In Mode B, the \"processed vectors\" that are checked for being zero are $\\mathbf{r}'$ and $\\mathbf{M}_s'$. We test if their norms are less than $\\varepsilon = 10^{-12}$.\n    - $\\|\\mathbf{r}'\\|_2 = \\sqrt{0^2+0^2} = 0$. This is a zero vector.\n    - $\\|\\mathbf{M}_1'\\|_2 = \\sqrt{0^2+0^2} = 0$. This is a zero vector.\n    - $\\|\\mathbf{M}_2'\\|_2 = \\sqrt{1^2+0^2} = 1$. This is not a zero vector.\n\n3.  **Similarity (Cosine Similarity with Degenerate Rules)**:\n    - For $s=1$, we compare $\\mathbf{r}'$ and $\\mathbf{M}_1'$. Both are zero vectors. The rule states: \"If both processed vectors... are exactly the zero vector... set $\\sigma_s = 0$\".\n      Thus, $\\sigma_1 = 0$.\n    - For $s=2$, we compare $\\mathbf{r}'$ and $\\mathbf{M}_2'$. Exactly one vector ($\\mathbf{r}'$) is a zero vector. The rule states: \"If exactly one of the two processed vectors is zero... set $\\sigma_s = -\\infty$\".\n      Thus, $\\sigma_2 = -\\infty$.\n\n4.  **Decision**:\n    The scores are $(\\sigma_1, \\sigma_2) = (0, -\\infty)$. The maximum score is $0$, which corresponds to stimulus $s=1$.\n    Therefore, $\\hat{s} = 1$.\n\nFinal results for all cases are $[1, 1, 1, 1]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef template_matching_decoder(N, S, b, templates, r, mode, epsilon):\n    \"\"\"\n    Implements a deterministic template matching decoder for neuronal responses.\n\n    Args:\n        N (int): Number of neurons.\n        S (int): Number of stimuli.\n        b (list or np.ndarray): Baseline vector of shape (N,).\n        templates (list of lists or np.ndarray): List of S template vectors, each shape (N,).\n        r (list or np.ndarray): Observed response vector of shape (N,).\n        mode (str): Normalization mode ('A', 'B', or 'C').\n        epsilon (float): Small constant for regularization and zero-checking.\n\n    Returns:\n        int: The predicted stimulus index (1-based).\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    b = np.array(b, dtype=float)\n    templates = np.array(templates, dtype=float)\n    r = np.array(r, dtype=float)\n\n    # Step 1: Baseline handling\n    r_prime = np.maximum(r - b, 0)\n    Ms_prime = np.maximum(templates - b, 0)\n\n    scores = []\n\n    if mode == 'A':\n        # Step 2A: No normalization\n        # Step 3A: Similarity is inner product\n        for s in range(S):\n            score = np.dot(r_prime, Ms_prime[s])\n            scores.append(score)\n\n    elif mode == 'B':\n        # Vectors to be normalized are r_prime and Ms_prime\n        norm_r = np.linalg.norm(r_prime)\n        r_is_zero = norm_r  epsilon\n\n        for s in range(S):\n            m_s_prime = Ms_prime[s]\n            norm_m = np.linalg.norm(m_s_prime)\n            m_is_zero = norm_m  epsilon\n\n            # Step 4: Degenerate vector handling\n            if r_is_zero and m_is_zero:\n                score = 0.0\n            elif r_is_zero or m_is_zero:\n                score = -np.inf\n            else:\n                # Step 2B  3B: L2 norm and cosine similarity\n                score = np.dot(r_prime, m_s_prime) / (norm_r * norm_m)\n            scores.append(score)\n\n    elif mode == 'C':\n        # Step 2C, Part 1: Variance normalization (whitening)\n        mu = np.mean(Ms_prime, axis=0)\n        whitener_denom = np.sqrt(mu + epsilon)\n        \n        # Element-wise division, handling potential division by zero in whitener\n        whitened_r = np.divide(r_prime, whitener_denom, \n                               out=np.zeros_like(r_prime), \n                               where=whitener_denom!=0)\n        \n        whitened_Ms = np.divide(Ms_prime, whitener_denom, \n                                out=np.zeros_like(Ms_prime), \n                                where=whitener_denom!=0)\n\n        # Vectors to be L2-normalized are the whitened ones\n        norm_whitened_r = np.linalg.norm(whitened_r)\n        r_is_zero = norm_whitened_r  epsilon\n\n        for s in range(S):\n            current_whitened_m = whitened_Ms[s]\n            norm_whitened_m = np.linalg.norm(current_whitened_m)\n            m_is_zero = norm_whitened_m  epsilon\n\n            # Step 4: Degenerate vector handling\n            if r_is_zero and m_is_zero:\n                score = 0.0\n            elif r_is_zero or m_is_zero:\n                score = -np.inf\n            else:\n                # Step 2C, Part 2  Step 3C: L2 norm and cosine similarity\n                score = np.dot(whitened_r, current_whitened_m) / (norm_whitened_r * norm_whitened_m)\n            scores.append(score)\n\n    # Step 4: Decision rule (argmax breaks ties by smallest index)\n    predicted_index = np.argmax(scores) + 1\n    return predicted_index\n\ndef solve():\n    \"\"\"\n    Runs the decoder on the test suite provided in the problem statement.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\"N\": 4, \"S\": 3, \"b\": [2, 5, 1, 0], \n         \"templates\": [[10, 7, 3, 2], [6, 10, 2, 1], [3, 6, 5, 1]], \n         \"r\": [9, 10, 2, 3], \"mode\": 'A', \"epsilon\": 1e-12},\n        # Case 2\n        {\"N\": 4, \"S\": 3, \"b\": [2, 5, 1, 0], \n         \"templates\": [[10, 7, 3, 2], [6, 10, 2, 1], [3, 6, 5, 1]], \n         \"r\": [26, 11, 7, 6], \"mode\": 'B', \"epsilon\": 1e-12},\n        # Case 3\n        {\"N\": 3, \"S\": 2, \"b\": [0, 0, 0], \n         \"templates\": [[1000, 2, 4], [1001, 10, 1]], \n         \"r\": [1000, 2, 4], \"mode\": 'C', \"epsilon\": 1e-12},\n        # Case 4\n        {\"N\": 2, \"S\": 2, \"b\": [5, 5], \n         \"templates\": [[5, 5], [6, 5]], \n         \"r\": [5, 5], \"mode\": 'B', \"epsilon\": 1e-12},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = template_matching_decoder(\n            case[\"N\"], case[\"S\"], case[\"b\"], case[\"templates\"],\n            case[\"r\"], case[\"mode\"], case[\"epsilon\"]\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While standard statistical models like the Poisson Maximum Likelihood Estimator (MLE) provide an optimal solution under ideal conditions, real-world neural recordings are often corrupted by outliers and noise. This practice challenges you to move beyond the classic MLE decoder and implement robust alternatives that are less sensitive to data contamination. You will compare the standard approach with decoders based on Huber loss and trimmed deviance, learning powerful techniques to build more reliable and accurate decoders for messy, real-world datasets.",
            "id": "4197761",
            "problem": "You are given a population coding problem in neuroscience data analysis focusing on template matching decoders and robustness to outliers and contamination. Consider a population of $N$ neurons responding to one of $S$ discrete stimuli. For each stimulus $s \\in \\{0,1,2\\}$, a template specifies expected mean spike counts across neurons as a nonnegative vector $\\boldsymbol{\\lambda}_s \\in \\mathbb{R}_{0}^N$. A single trial produces an observed nonnegative integer spike count vector $\\mathbf{k} \\in \\mathbb{Z}_{\\ge 0}^N$. The objective is to decode (infer) the stimulus identity $s$ from $\\mathbf{k}$ by comparing $\\mathbf{k}$ against the templates $\\{\\boldsymbol{\\lambda}_s\\}_{s=0}^2$.\n\nStart from the fundamental base that spike counts are modeled as independent Poisson random variables across neurons conditioned on the stimulus: for neuron $i$, $K_i \\mid s \\sim \\mathrm{Poisson}(\\lambda_{s,i})$, where $\\lambda_{s,i}  0$ is the expected count in a fixed counting window. The Maximum Likelihood Estimation (MLE) decoder chooses the stimulus $s$ that maximizes the log-likelihood given by\n$$\n\\log p(\\mathbf{k} \\mid s) = \\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} - \\log(k_i!) \\right),\n$$\nwhere the term $\\sum_i \\log(k_i!)$ does not depend on $s$ and can be ignored for comparison. Real neural data often contain contamination or outliers due to nonstationarity, noise bursts, spike sorting errors, or rare events. Robust alternatives to classical template matching can be constructed using Generalized Linear Model (GLM) deviance residuals and robust aggregation. Define the Poisson deviance for neuron $i$ and stimulus $s$:\n$$\nD_{s,i}(\\mathbf{k}) = 2 \\left( k_i \\log\\left( \\frac{k_i}{\\lambda_{s,i}} \\right) - (k_i - \\lambda_{s,i}) \\right),\n$$\nwith the convention that $k_i \\log(k_i/\\lambda_{s,i}) = 0$ when $k_i = 0$. The signed deviance residual is\n$$\nr_{s,i}(\\mathbf{k}) = \\operatorname{sign}(k_i - \\lambda_{s,i}) \\sqrt{ D_{s,i}(\\mathbf{k}) }.\n$$\nA robust M-estimator objective can be formed using the Huber loss function $\\rho_\\delta$ with threshold $\\delta  0$,\n$$\n\\rho_\\delta(x) = \\begin{cases}\n\\frac{1}{2} x^2,  \\text{if } |x| \\le \\delta, \\\\\n\\delta \\left( |x| - \\frac{1}{2} \\delta \\right),  \\text{if } |x|  \\delta,\n\\end{cases}\n$$\nand the robust score for stimulus $s$ is $\\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k}))$, with the decoder choosing the stimulus that minimizes this score. Another robust alternative is a trimmed deviance aggregator that sorts $\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$ and sums only the smallest fraction $\\tau \\in (0,1]$ of them, thereby reducing the influence of the largest residuals associated with outliers.\n\nYour task is to implement three decoders:\n- The classical Poisson MLE template matcher that chooses $s$ maximizing $\\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} \\right)$.\n- The robust Huber deviance template matcher that chooses $s$ minimizing $\\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k}))$.\n- The trimmed deviance template matcher that chooses $s$ minimizing the sum of the smallest $\\lfloor \\tau N \\rfloor$ values of $\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$.\n\nAdopt the following templates defined by scaling a base rate vector. Let $N = 12$, $S = 3$, the base rate vector be\n$$\n\\mathbf{b} = [\\, 5,\\, 6,\\, 4,\\, 8,\\, 2,\\, 9,\\, 3,\\, 7,\\, 5,\\, 6,\\, 4,\\, 2 \\,],\n$$\nand the stimulus-specific scaling factors be\n$$\n\\mathbf{f} = [\\, 1.0,\\, 1.5,\\, 0.6 \\,].\n$$\nFor $s \\in \\{0,1,2\\}$, define $\\boldsymbol{\\lambda}_s$ component-wise by $\\lambda_{s,i} = f_s \\cdot b_i$. Concretely, the templates are\n$$\n\\boldsymbol{\\lambda}_0 = [\\, 5.0,\\, 6.0,\\, 4.0,\\, 8.0,\\, 2.0,\\, 9.0,\\, 3.0,\\, 7.0,\\, 5.0,\\, 6.0,\\, 4.0,\\, 2.0 \\,],\n$$\n$$\n\\boldsymbol{\\lambda}_1 = [\\, 7.5,\\, 9.0,\\, 6.0,\\, 12.0,\\, 3.0,\\, 13.5,\\, 4.5,\\, 10.5,\\, 7.5,\\, 9.0,\\, 6.0,\\, 3.0 \\,],\n$$\n$$\n\\boldsymbol{\\lambda}_2 = [\\, 3.0,\\, 3.6,\\, 2.4,\\, 4.8,\\, 1.2,\\, 5.4,\\, 1.8,\\, 4.2,\\, 3.0,\\, 3.6,\\, 2.4,\\, 1.2 \\,].\n$$\nNote that the numerical values are strictly positive and represent expected spike counts per fixed counting window; since the outputs are discrete labels, you do not need to express any physical units for the final answer.\n\nUse the following test suite of observed spike count vectors (each of length $12$), which includes normal conditions, contamination, and edge cases. Each test case requires decoding the stimulus by all three methods specified above. In all cases, use Huber threshold $\\delta = 2.0$ and trimming proportion $\\tau = 0.7$.\n\n- Test case $1$ (happy path, no contamination, typical counts near $\\boldsymbol{\\lambda}_1$):\n$$\n\\mathbf{k}^{(1)} = [\\, 8,\\, 9,\\, 6,\\, 12,\\, 3,\\, 14,\\, 5,\\, 11,\\, 8,\\, 9,\\, 6,\\, 3 \\,].\n$$\n- Test case $2$ (heavy contamination: majority near $\\boldsymbol{\\lambda}_2$ with large outliers in three neurons):\n$$\n\\mathbf{k}^{(2)} = [\\, 3,\\, 4,\\, 2,\\, 5,\\, 50,\\, 5,\\, 40,\\, 4,\\, 3,\\, 60,\\, 2,\\, 1 \\,].\n$$\n- Test case $3$ (boundary: all neurons silent):\n$$\n\\mathbf{k}^{(3)} = [\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0 \\,].\n$$\n- Test case $4$ (single extreme outlier, otherwise near zero):\n$$\n\\mathbf{k}^{(4)} = [\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 100,\\, 0,\\, 0 \\,].\n$$\n- Test case $5$ (mild contamination: typical counts near $\\boldsymbol{\\lambda}_0$ with two moderate outliers):\n$$\n\\mathbf{k}^{(5)} = [\\, 5,\\, 6,\\, 4,\\, 8,\\, 2,\\, 9,\\, 3,\\, 7,\\, 5,\\, 20,\\, 15,\\, 2 \\,].\n$$\n\nImplement deterministic tie-breaking by choosing the smallest stimulus index $s$ in case of exact equality of scores up to a tolerance of $\\epsilon = 10^{-12}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of three integers $[s_{\\mathrm{MLE}}, s_{\\mathrm{Huber}}, s_{\\mathrm{Trim}}]$ corresponding to the decoded labels by the three methods in the order specified above. For example, the final output format for five test cases is\n$$\n[\\, [\\, a_1,\\, b_1,\\, c_1 \\,],\\, [\\, a_2,\\, b_2,\\, c_2 \\,],\\, [\\, a_3,\\, b_3,\\, c_3 \\,],\\, [\\, a_4,\\, b_4,\\, c_4 \\,],\\, [\\, a_5,\\, b_5,\\, c_5 \\,] \\,],\n$$\nwith each $a_j$, $b_j$, and $c_j$ an integer in $\\{0,1,2\\}$.\n\nNo user input is required. The program must be self-contained and reproduce the exact test suite and parameters given above.",
            "solution": "The solution requires implementing three distinct decoding algorithms and applying them to five test cases. For each test case and each stimulus template, a score is calculated according to the rules of the three decoders. The predicted stimulus is the one that optimizes its respective score, with ties broken by choosing the smallest stimulus index.\n\n**1. Poisson Maximum Likelihood (MLE) Decoder:**\nThe score for stimulus $s$ is calculated by maximizing the log-likelihood function, ignoring stimulus-independent terms:\n$$ \\mathcal{S}_{\\text{MLE}}(s; \\mathbf{k}) = \\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} \\right) $$\nThe decoder chooses $s_{\\text{MLE}} = \\arg\\max_{s} \\mathcal{S}_{\\text{MLE}}(s; \\mathbf{k})$.\n\n**2. Robust Huber Deviance Decoder:**\nThis decoder aims to minimize a robust cost function. First, the signed deviance residual $r_{s,i}(\\mathbf{k})$ is computed for each neuron $i$ and stimulus $s$. Then, the Huber loss $\\rho_\\delta$ with threshold $\\delta = 2.0$ is applied to each residual. The total score for stimulus $s$ is the sum of these losses:\n$$ \\mathcal{S}_{\\text{Huber}}(s; \\mathbf{k}) = \\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k})) $$\nThe decoder chooses $s_{\\text{Huber}} = \\arg\\min_{s} \\mathcal{S}_{\\text{Huber}}(s; \\mathbf{k})$.\n\n**3. Trimmed Deviance Decoder:**\nThis decoder minimizes a trimmed sum of deviances. For each stimulus $s$, the Poisson deviances $D_{s,i}(\\mathbf{k})$ are calculated for all $N=12$ neurons. These deviances are sorted, and the score is the sum of the smallest $\\lfloor \\tau N \\rfloor = \\lfloor 0.7 \\times 12 \\rfloor = 8$ values.\n$$ \\mathcal{S}_{\\text{Trim}}(s; \\mathbf{k}) = \\sum_{j=1}^{8} D_{(j)}(s; \\mathbf{k}) $$\nwhere $D_{(j)}$ is the $j$-th smallest deviance. The decoder chooses $s_{\\text{Trim}} = \\arg\\min_{s} \\mathcal{S}_{\\text{Trim}}(s; \\mathbf{k})$.\n\nThe final output is generated by running these three decoders on each of the five test vectors $\\mathbf{k}^{(1)}, \\dots, \\mathbf{k}^{(5)}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares three decoders (MLE, Huber, Trimmed) for a \n    neuroscience population coding problem.\n    \"\"\"\n\n    # 1. DEFINE PARAMETERS AND TEMPLATES\n    N = 12\n    S = 3\n    stimuli_indices = np.arange(S)\n    \n    b = np.array([5, 6, 4, 8, 2, 9, 3, 7, 5, 6, 4, 2], dtype=float)\n    f = np.array([1.0, 1.5, 0.6], dtype=float)\n    \n    # Stimulus templates (S x N matrix)\n    lambdas = np.outer(f, b)\n    \n    # Robustness parameters\n    delta = 2.0  # Huber threshold\n    tau = 0.7    # Trimming proportion\n    num_to_keep = int(np.floor(tau * N))\n    \n    # Tie-breaking tolerance\n    epsilon = 1e-12\n\n    test_cases = [\n        np.array([8, 9, 6, 12, 3, 14, 5, 11, 8, 9, 6, 3], dtype=float),\n        np.array([3, 4, 2, 5, 50, 5, 40, 4, 3, 60, 2, 1], dtype=float),\n        np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=float),\n        np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 0, 0], dtype=float),\n        np.array([5, 6, 4, 8, 2, 9, 3, 7, 5, 20, 15, 2], dtype=float),\n    ]\n\n    def argmin_tiebreak(scores):\n        min_score = np.min(scores)\n        tied_indices = np.where(scores  min_score + epsilon)[0]\n        return np.min(tied_indices)\n\n    def argmax_tiebreak(scores):\n        max_score = np.max(scores)\n        tied_indices = np.where(scores > max_score - epsilon)[0]\n        return np.min(tied_indices)\n\n    all_results = []\n    \n    for k in test_cases:\n        mle_scores = np.zeros(S)\n        huber_scores = np.zeros(S)\n        trim_scores = np.zeros(S)\n        \n        for s in stimuli_indices:\n            lambda_s = lambdas[s]\n            \n            # --- MLE Decoder Score (maximize) ---\n            # Score = sum(k_i * log(lambda_{s,i}) - lambda_{s,i})\n            mle_scores[s] = np.sum(k * np.log(lambda_s) - lambda_s)\n            \n            # --- Deviance Calculation (for Huber and Trimmed) ---\n            # Dev = 2 * (k*log(k/lambda) - (k-lambda))\n            # Handle k=0 case for the log term\n            log_term_dev = np.zeros_like(k)\n            nonzero_k_mask = k > 0\n            log_term_dev[nonzero_k_mask] = k[nonzero_k_mask] * np.log(k[nonzero_k_mask] / lambda_s[nonzero_k_mask])\n            deviances = 2 * (log_term_dev - (k - lambda_s))\n\n            # --- Huber Deviance Decoder Score (minimize) ---\n            residuals = np.sign(k - lambda_s) * np.sqrt(deviances)\n            abs_residuals = np.abs(residuals)\n            huber_losses = np.where(abs_residuals = delta,\n                                  0.5 * residuals**2,\n                                  delta * (abs_residuals - 0.5 * delta))\n            huber_scores[s] = np.sum(huber_losses)\n            \n            # --- Trimmed Deviance Decoder Score (minimize) ---\n            sorted_deviances = np.sort(deviances)\n            trim_scores[s] = np.sum(sorted_deviances[:num_to_keep])\n            \n        # 2. DECODE STIMULUS FOR EACH METHOD\n        s_mle = argmax_tiebreak(mle_scores)\n        s_huber = argmin_tiebreak(huber_scores)\n        s_trim = argmin_tiebreak(trim_scores)\n        \n        all_results.append([s_mle, s_huber, s_trim])\n\n    # 3. FORMAT AND PRINT THE FINAL OUTPUT\n    # Manual string construction to avoid spaces after commas.\n    result_str = \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Previous exercises assumed a fixed, predefined metric for comparing neural responses to templates, such as the inner product or cosine similarity. This final practice takes a significant step towards modern machine learning by teaching the decoder to learn the optimal metric directly from data. You will implement a decoder that learns a diagonal Mahalanobis distance by optimizing a set of weights that emphasize informative neurons and down-weight noisy or irrelevant ones, providing a foundation in data-driven decoder design.",
            "id": "4197754",
            "problem": "Design and implement a template matching decoder with a learned diagonal Mahalanobis metric for multi-class neural population decoding. You are given spike count data, where each observation is a nonnegative integer vector representing the number of spikes from a population of neurons during a fixed time window. The goal is to build a decoder that assigns each test observation to the class whose template (class mean vector) is closest under a learned diagonal Mahalanobis metric. The learning rule must be derived from first principles and implemented as a convex optimization over a probability simplex.\n\nStart from the following fundamental bases and core definitions:\n\n1. A spike count vector is represented by $x \\in \\mathbb{R}_{\\ge 0}^{d}$, where $d$ is the number of neurons. A class template (centroid) is the arithmetic mean of its training vectors, denoted $\\mu_{c} \\in \\mathbb{R}^{d}$ for class index $c \\in \\{0,1,\\dots,C-1\\}$.\n\n2. A diagonal Mahalanobis metric is defined by a nonnegative weight vector $w \\in \\mathbb{R}^{d}$, with the matrix $M = \\mathrm{diag}(w)$. The squared distance between $x$ and a template $\\mu_{c}$ under $M$ is\n$$\nD_{M}(x,\\mu_{c}) \\equiv (x - \\mu_{c})^{\\top} M (x - \\mu_{c}) = \\sum_{k=1}^{d} w_{k} (x_{k} - \\mu_{c,k})^{2}.\n$$\nTo make $w$ identifiable and to avoid trivial scalings, constrain $w$ to lie on the probability simplex:\n$$\n\\Delta \\equiv \\left\\{ w \\in \\mathbb{R}^{d} \\,\\middle|\\, \\sum_{k=1}^{d} w_{k} = 1,\\; w_{k} \\ge 0 \\;\\forall k \\right\\}.\n$$\n\n3. The template matching decoder assigns $x$ to the class\n$$\n\\hat{c}(x) = \\arg\\min_{c \\in \\{0,\\dots,C-1\\}} D_{M}(x,\\mu_{c}).\n$$\n\nYou will derive a convex surrogate objective for learning $w$ that trades off within-class compactness against between-class separability per feature. Let the within-class scatter for feature $k$ be\n$$\nS^{\\mathrm{intra}}_{k} \\equiv \\frac{1}{C} \\sum_{c=0}^{C-1} \\left( \\frac{1}{N_{c}} \\sum_{n=1}^{N_{c}} (x^{(n)}_{c,k} - \\mu_{c,k})^{2} \\right),\n$$\nwhere $N_{c}$ is the number of training samples in class $c$, and $x^{(n)}_{c,k}$ is the $k$-th feature of the $n$-th training sample of class $c$. Let the between-class scatter for feature $k$ be the average pairwise squared difference of class means:\n$$\nS^{\\mathrm{inter}}_{k} \\equiv \\frac{2}{C(C-1)} \\sum_{0 \\le c  c' \\le C-1} (\\mu_{c,k} - \\mu_{c',k})^{2}.\n$$\n\nDefine the linear contrast vector\n$$\na_{k} \\equiv S^{\\mathrm{intra}}_{k} - \\lambda \\, S^{\\mathrm{inter}}_{k},\n$$\nwith a user-specified trade-off parameter $\\lambda  0$. To avoid degenerate solutions and to stabilize learning, add an $\\ell_{2}$-regularizer with coefficient $\\gamma  0$. Consider the convex optimization problem\n$$\n\\min_{w \\in \\Delta} \\;\\; J(w) \\equiv \\sum_{k=1}^{d} a_{k} w_{k} + \\frac{\\gamma}{2} \\| w \\|_{2}^{2}.\n$$\nYou must derive how to compute the optimizer $w^{\\star}$ efficiently and exactly, without generic numerical optimizers, using only projection onto the probability simplex. Then, implement the learned-metric template matching decoder and evaluate classification accuracy on the test sets specified below.\n\nYour program must implement the following steps for each test case:\n\n1. Compute class templates $\\mu_{c}$ from the training data.\n2. Compute $S^{\\mathrm{intra}}_{k}$ and $S^{\\mathrm{inter}}_{k}$ for all features $k \\in \\{1,\\dots,d\\}$.\n3. Form $a_{k} = S^{\\mathrm{intra}}_{k} - \\lambda \\, S^{\\mathrm{inter}}_{k}$ and compute\n$$\np_{k} \\equiv -\\frac{a_{k}}{\\gamma} = \\frac{\\lambda \\, S^{\\mathrm{inter}}_{k} - S^{\\mathrm{intra}}_{k}}{\\gamma}.\n$$\n4. Compute $w^{\\star}$ as the Euclidean projection of $p$ onto the simplex $\\Delta$:\n$$\nw^{\\star} = \\operatorname{Proj}_{\\Delta}(p).\n$$\n5. Classify each test vector by $\\hat{c}(x) = \\arg\\min_{c} \\sum_{k=1}^{d} w^{\\star}_{k} (x_{k} - \\mu_{c,k})^{2}$.\n6. Report the accuracy, defined as the fraction of correctly classified test vectors over the total number of test vectors, as a decimal number in $[0,1]$.\n\nTest suite. Implement exactly these three test cases, each with given $(\\lambda,\\gamma)$ and explicit spike count matrices. All counts are in arbitrary consistent spike count units and must be treated as dimensionless integers.\n\n- Test case $1$ (happy path, two classes, clear separation on a subset of neurons):\n  - Dimensions: $C=2$, $d=4$.\n  - Training data:\n    $$\n    X^{\\mathrm{train}}_{0} = \\begin{bmatrix}\n    10  2  5  5\\\\\n    11  1  6  4\\\\\n    9  3  4  6\n    \\end{bmatrix}, \\quad\n    X^{\\mathrm{train}}_{1} = \\begin{bmatrix}\n    2  10  5  5\\\\\n    1  11  6  4\\\\\n    3  9  4  6\n    \\end{bmatrix}.\n    $$\n  - Test data:\n    $$\n    X^{\\mathrm{test}}_{0} = \\begin{bmatrix}\n    10  2  6  5\\\\\n    9  1  5  6\n    \\end{bmatrix}, \\quad\n    X^{\\mathrm{test}}_{1} = \\begin{bmatrix}\n    2  10  4  5\\\\\n    3  9  5  4\n    \\end{bmatrix}.\n    $$\n  - Parameters: $\\lambda = 1$, $\\gamma = 0.2$.\n\n- Test case $2$ (three classes, overlapping activity with a third informative neuron):\n  - Dimensions: $C=3$, $d=5$.\n  - Training data:\n    $$\n    X^{\\mathrm{train}}_{0} = \\begin{bmatrix}\n    8  3  5  5  5\\\\\n    9  2  5  6  4\\\\\n    7  4  6  4  6\n    \\end{bmatrix}, \\quad\n    X^{\\mathrm{train}}_{1} = \\begin{bmatrix}\n    3  8  5  5  5\\\\\n    2  9  4  6  4\\\\\n    4  7  6  4  6\n    \\end{bmatrix}, \\quad\n    X^{\\mathrm{train}}_{2} = \\begin{bmatrix}\n    5  5  9  5  5\\\\\n    6  4  8  6  4\\\\\n    4  6  10  4  6\n    \\end{bmatrix}.\n    $$\n  - Test data:\n    $$\n    X^{\\mathrm{test}}_{0} = \\begin{bmatrix}\n    8  3  5  5  5\\\\\n    9  2  6  5  5\n    \\end{bmatrix}, \\quad\n    X^{\\mathrm{test}}_{1} = \\begin{bmatrix}\n    3  8  5  5  5\\\\\n    4  7  5  6  5\n    \\end{bmatrix}, \\quad\n    X^{\\mathrm{test}}_{2} = \\begin{bmatrix}\n    5  5  9  5  5\\\\\n    6  4  9  5  4\n    \\end{bmatrix}.\n    $$\n  - Parameters: $\\lambda = 1$, $\\gamma = 0.2$.\n\n- Test case $3$ (edge case with large-amplitude nuisance variability, two classes, one highly informative low-count neuron):\n  - Dimensions: $C=2$, $d=3$.\n  - Training data:\n    $$\n    X^{\\mathrm{train}}_{0} = \\begin{bmatrix}\n    50  50  2\\\\\n    60  40  3\\\\\n    55  45  2\n    \\end{bmatrix}, \\quad\n    X^{\\mathrm{train}}_{1} = \\begin{bmatrix}\n    55  45  8\\\\\n    65  35  7\\\\\n    60  40  9\n    \\end{bmatrix}.\n    $$\n  - Test data:\n    $$\n    X^{\\mathrm{test}}_{0} = \\begin{bmatrix}\n    52  48  3\\\\\n    58  42  2\n    \\end{bmatrix}, \\quad\n    X^{\\mathrm{test}}_{1} = \\begin{bmatrix}\n    58  42  8\\\\\n    62  38  7\n    \\end{bmatrix}.\n    $$\n  - Parameters: $\\lambda = 2$, $\\gamma = 0.05$.\n\nFinal output specification. Your program should process the three test cases in order and produce a single line of output containing a list of three floating-point accuracies $[a_{1},a_{2},a_{3}]$, each rounded to exactly three digits after the decimal point, where $a_{i}$ is the accuracy for test case $i$ expressed as a decimal number in $[0,1]$. The output must be printed as a comma-separated list enclosed in square brackets, with no additional text, for example, $[0.750,1.000,0.500]$.",
            "solution": "The solution involves implementing a multi-step algorithm for each test case to learn a metric and classify data. The core of the method is solving a convex optimization problem to find the optimal feature weights.\n\n**1. Data Preprocessing:**\nFirst, for each class $c$, the template (centroid) $\\mu_c$ is calculated by taking the mean of its training vectors.\n\n**2. Scatter Calculation:**\nFor each feature (neuron) $k$, two quantities are computed:\n- **Within-class scatter ($S^{\\mathrm{intra}}_{k}$):** The average variance of feature $k$ across all classes.\n- **Between-class scatter ($S^{\\mathrm{inter}}_{k}$):** The average squared distance between class means for feature $k$.\n\n**3. Learning the Metric Weights ($w^\\star$):**\nThe optimal weight vector $w^\\star$ is found by solving the optimization problem:\n$$ \\min_{w \\in \\Delta} \\;\\; \\sum_{k=1}^{d} a_{k} w_{k} + \\frac{\\gamma}{2} \\| w \\|_{2}^{2} $$\nwhere $a_{k} = S^{\\mathrm{intra}}_{k} - \\lambda S^{\\mathrm{inter}}_{k}$. As derived from the problem statement, this is equivalent to finding the Euclidean projection of the vector $p = (\\lambda S^{\\mathrm{inter}} - S^{\\mathrm{intra}})/\\gamma$ onto the probability simplex $\\Delta$. This projection can be computed efficiently using a standard algorithm that involves sorting the components of $p$ and finding a threshold $\\nu$ such that $w^\\star_k = \\max(0, p_k - \\nu)$.\n\n**4. Classification and Accuracy:**\nOnce $w^\\star$ is learned from the training data, each test vector $x$ is classified by finding the template $\\mu_c$ that minimizes the learned diagonal Mahalanobis distance:\n$$ \\hat{c}(x) = \\arg\\min_{c} \\sum_{k=1}^{d} w^{\\star}_{k} (x_{k} - \\mu_{c,k})^{2} $$\nThe final accuracy is calculated as the fraction of correctly classified test vectors. This entire process is repeated for each of the three test cases.",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to solve the template matching decoder problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"C\": 2, \"d\": 4, \"lambda\": 1.0, \"gamma\": 0.2,\n            \"train_data\": [\n                np.array([\n                    [10, 2, 5, 5],\n                    [11, 1, 6, 4],\n                    [9, 3, 4, 6]\n                ], dtype=float),\n                np.array([\n                    [2, 10, 5, 5],\n                    [1, 11, 6, 4],\n                    [3, 9, 4, 6]\n                ], dtype=float)\n            ],\n            \"test_data\": [\n                np.array([\n                    [10, 2, 6, 5],\n                    [9, 1, 5, 6]\n                ], dtype=float),\n                np.array([\n                    [2, 10, 4, 5],\n                    [3, 9, 5, 4]\n                ], dtype=float)\n            ]\n        },\n        {\n            \"C\": 3, \"d\": 5, \"lambda\": 1.0, \"gamma\": 0.2,\n            \"train_data\": [\n                np.array([\n                    [8, 3, 5, 5, 5],\n                    [9, 2, 5, 6, 4],\n                    [7, 4, 6, 4, 6]\n                ], dtype=float),\n                np.array([\n                    [3, 8, 5, 5, 5],\n                    [2, 9, 4, 6, 4],\n                    [4, 7, 6, 4, 6]\n                ], dtype=float),\n                np.array([\n                    [5, 5, 9, 5, 5],\n                    [6, 4, 8, 6, 4],\n                    [4, 6, 10, 4, 6]\n                ], dtype=float)\n            ],\n            \"test_data\": [\n                np.array([\n                    [8, 3, 5, 5, 5],\n                    [9, 2, 6, 5, 5]\n                ], dtype=float),\n                np.array([\n                    [3, 8, 5, 5, 5],\n                    [4, 7, 5, 6, 5]\n                ], dtype=float),\n                np.array([\n                    [5, 5, 9, 5, 5],\n                    [6, 4, 9, 5, 4]\n                ], dtype=float)\n            ]\n        },\n        {\n            \"C\": 2, \"d\": 3, \"lambda\": 2.0, \"gamma\": 0.05,\n            \"train_data\": [\n                np.array([\n                    [50, 50, 2],\n                    [60, 40, 3],\n                    [55, 45, 2]\n                ], dtype=float),\n                np.array([\n                    [55, 45, 8],\n                    [65, 35, 7],\n                    [60, 40, 9]\n                ], dtype=float)\n            ],\n            \"test_data\": [\n                np.array([\n                    [52, 48, 3],\n                    [58, 42, 2]\n                ], dtype=float),\n                np.array([\n                    [58, 42, 8],\n                    [62, 38, 7]\n                ], dtype=float)\n            ]\n        }\n    ]\n\n    accuracies = []\n\n    for case in test_cases:\n        C = case[\"C\"]\n        d = case[\"d\"]\n        lambda_param = case[\"lambda\"]\n        gamma_param = case[\"gamma\"]\n        train_data = case[\"train_data\"]\n        test_data_by_class = case[\"test_data\"]\n\n        # Step 1: Compute class templates (means)\n        mu = np.array([np.mean(X_c, axis=0) for X_c in train_data])\n\n        # Step 2: Compute S_intra and S_inter\n        S_intra = np.zeros(d)\n        variances_per_class = np.array([np.var(X_c, axis=0, ddof=0) for X_c in train_data])\n        S_intra = np.mean(variances_per_class, axis=0)\n\n        S_inter = np.zeros(d)\n        num_pairs = C * (C - 1) / 2\n        if num_pairs > 0:\n            for c1, c2 in combinations(range(C), 2):\n                S_inter += (mu[c1] - mu[c2])**2\n            S_inter /= num_pairs\n            \n        # Step 3: Compute vector p\n        a = S_intra - lambda_param * S_inter\n        p = -a / gamma_param\n\n        # Step 4: Compute w* by projecting p onto the probability simplex\n        def project_simplex(v):\n            n_features = len(v)\n            v_sorted = np.sort(v)[::-1]\n            cssv = np.cumsum(v_sorted)\n            \n            rho = np.nonzero(v_sorted * np.arange(1, n_features + 1) > cssv - 1)[0][-1]\n            theta = (cssv[rho] - 1) / (rho + 1.0)\n            \n            w = np.maximum(v - theta, 0)\n            return w\n\n        w_star = project_simplex(p)\n\n        # Step 5: Classify each test vector\n        all_test_data = np.vstack(test_data_by_class)\n        true_labels = np.concatenate([np.full(len(X_c), i) for i, X_c in enumerate(test_data_by_class)])\n        \n        predicted_labels = []\n        for x_test in all_test_data:\n            distances = np.zeros(C)\n            for c in range(C):\n                distances[c] = np.sum(w_star * (x_test - mu[c])**2)\n            predicted_labels.append(np.argmin(distances))\n        \n        predicted_labels = np.array(predicted_labels)\n\n        # Step 6: Report accuracy\n        accuracy = np.mean(predicted_labels == true_labels)\n        accuracies.append(accuracy)\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{acc:.3f}' for acc in accuracies)}]\")\n\nsolve()\n```"
        }
    ]
}