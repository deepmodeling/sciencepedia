{
    "hands_on_practices": [
        {
            "introduction": "这个练习将带你回归第一性原理，探索模板匹配解码器的理论基础。它将阐明，我们所熟知的“模板匹配”并非一个随意的启发式方法，而是可以直接从泊松（Poisson）和高斯（Gaussian）等基本统计模型中推导出来的。通过  的练习，你将理解如何根据神经活动的统计特性选择和构建合适的解码器，这对于正确解释解码结果至关重要。",
            "id": "4197771",
            "problem": "考虑一个神经科学数据分析中的群体解码问题，其中模板匹配解码器必须根据从训练数据构建的模板，将单次试验的脉冲计数向量分类到两个刺激类别之一。设有 $3$ 个同时记录的神经元。假设在给定刺激类别的情况下，神经元间的脉冲计数是条件独立的。\n\n考虑两种有科学依据的模板构建策略：\n\n1. 一个生成模型，其中每个类别下每个神经元的脉冲发放都服从独立的泊松分布，且各个类别的先验概率相等。在此设定下，您需要从第一性原理出发，推导最大后验概率分类器 (MAP) 如何简化为一个带有内积得分和偏置项的线性模板匹配规则。请用依赖于类别的泊松率明确表示出依赖于类别的模板向量和偏置项。\n\n2. 一个生成模型，其中存在方差与类别无关的附加、独立、同分布的高斯噪声作用于依赖于类别的平均脉冲计数。在此设定下，您需要从第一性原理出发，推导 MAP 分类器如何简化为选择其平均模板与单次试验脉冲计数向量的欧几里得距离平方最小的类别。请将两个类别之间的决策分差值表示为脉冲计数的线性函数加上一个依赖于类别的常数的形式。\n\n作为一个具体实例，假设类别 $s \\in \\{1,2\\}$ 具有神经元特定的泊松率向量，如下所示\n$$\n\\boldsymbol{\\lambda}_{1} = \\begin{pmatrix} 7 \\\\ 3 \\\\ 12 \\end{pmatrix}, \\quad \\boldsymbol{\\lambda}_{2} = \\begin{pmatrix} 4 \\\\ 5 \\\\ 9 \\end{pmatrix},\n$$\n且一次单次试验产生的脉冲计数向量为\n$$\n\\mathbf{x} = \\begin{pmatrix} 6 \\\\ 7 \\\\ 10 \\end{pmatrix}.\n$$\n在 Poisson 模型下，构建类别1和2之间的 MAP 线性决策分差值，并在 $\\mathbf{x}$ 处进行评估。在具有单位协方差的 Gaussian 模型下，构建类别1和2之间的欧几里得距离平方决策分差值，并在 $\\mathbf{x}$ 处进行评估。令 Poisson 模型的决策分差值为 $D_{\\mathrm{Pois}}$，Gaussian 模型的决策分差值为 $D_{\\mathrm{Gauss}}$。\n\n最后，根据所提供的率和脉冲计数，计算标量\n$$\n\\Delta \\equiv D_{\\mathrm{Pois}} - D_{\\mathrm{Gauss}}\n$$\n将您的最终数值答案四舍五入到五位有效数字。不需要单位。",
            "solution": "该问题被验证为有科学依据、适定且客观。它包含了所有必要信息，并且没有矛盾或歧义。所提出的模型（用于脉冲计数的 Poisson 和 Gaussian 生成模型）是计算神经科学中的标准模型，任务涉及最大后验概率 (MAP) 分类器的标准推导。\n\n解答分为三个部分：第一，Poisson 模型的推导；第二，Gaussian 模型的推导；第三，所要求的具体数值计算。\n\n### 第1部分：Poisson 生成模型的推导\n\n最大后验概率 (MAP) 分类器选择在给定观测到的脉冲计数向量 $\\mathbf{x}$ 的情况下，使后验概率 $P(s|\\mathbf{x})$ 最大化的类别 $s$。根据 Bayes' 定理，后验概率由下式给出：\n$$\nP(s|\\mathbf{x}) = \\frac{P(\\mathbf{x}|s) P(s)}{P(\\mathbf{x})}\n$$\n最大化后验概率等价于最大化分子 $P(\\mathbf{x}|s) P(s)$，因为证据 $P(\\mathbf{x})$ 对所有类别是恒定的。问题陈述两个类别的先验概率相等，即 $P(s=1) = P(s=2)$。因此，MAP 分类器简化为最大似然 (ML) 分类器，它选择使似然 $P(\\mathbf{x}|s)$ 最大化的类别 $s$。\n\n在计算上，处理对数似然更为方便。决策规则是选择使 $\\ln P(\\mathbf{x}|s)$ 最大化的类别。模型假设 $N=3$ 个神经元中的每一个都服从条件独立的 Poisson 脉冲发放。速率为 $\\lambda$ 的 Poisson 分布的概率质量函数是 $P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$。\n\n给定脉冲计数向量 $\\mathbf{x} = (x_1, x_2, \\dots, x_N)^T$ 和依赖于类别的速率向量 $\\boldsymbol{\\lambda}_s = (\\lambda_{s,1}, \\lambda_{s,2}, \\dots, \\lambda_{s,N})^T$，由于条件独立性，似然函数是个体神经元概率的乘积：\n$$\nP(\\mathbf{x}|s) = \\prod_{i=1}^{N} P(x_i | s) = \\prod_{i=1}^{N} \\frac{(\\lambda_{s,i})^{x_i} \\exp(-\\lambda_{s,i})}{x_i!}\n$$\n对数似然为：\n$$\n\\ln P(\\mathbf{x}|s) = \\ln \\left( \\prod_{i=1}^{N} \\frac{(\\lambda_{s,i})^{x_i} \\exp(-\\lambda_{s,i})}{x_i!} \\right) = \\sum_{i=1}^{N} \\left( x_i \\ln(\\lambda_{s,i}) - \\lambda_{s,i} - \\ln(x_i!) \\right)\n$$\n为了做出决策，我们比较每个类别的对数似然。任何不依赖于类别 $s$ 的项都可以从决策函数中去掉。项 $\\sum_{i=1}^{N} \\ln(x_i!)$ 与 $s$ 无关，可以省略。这就为每个类别留下一个决策分 $g_s(\\mathbf{x})$：\n$$\ng_s(\\mathbf{x}) = \\sum_{i=1}^{N} x_i \\ln(\\lambda_{s,i}) - \\sum_{i=1}^{N} \\lambda_{s,i}\n$$\n这个表达式可以写成模板向量与数据向量的内积，再加上一个偏置项。设类别 $s$ 的模板向量为 $\\mathbf{w}_s$，偏置项为 $b_s$。我们可以确定：\n$$\n\\mathbf{w}_s = \\begin{pmatrix} \\ln(\\lambda_{s,1}) \\\\ \\ln(\\lambda_{s,2}) \\\\ \\vdots \\\\ \\ln(\\lambda_{s,N}) \\end{pmatrix}, \\quad b_s = - \\sum_{i=1}^{N} \\lambda_{s,i}\n$$\n于是决策分为 $g_s(\\mathbf{x}) = \\mathbf{w}_s^T \\mathbf{x} + b_s$，这正是所要求的线性模板匹配规则。\n\n### 第2部分：Gaussian 生成模型的推导\n\n在这个模型中，脉冲计数向量 $\\mathbf{x}$ 是通过将独立同分布的 Gaussian 噪声加到一个依赖于类别的均值向量 $\\boldsymbol{\\mu}_s$ 上生成的：$\\mathbf{x} = \\boldsymbol{\\mu}_s + \\boldsymbol{\\epsilon}$。噪声向量 $\\boldsymbol{\\epsilon}$ 从一个均值为零、协方差矩阵与类别无关（$\\Sigma = \\sigma^2 I$，其中 $I$ 是单位矩阵）的多元正态分布中抽取。\n\n在给定类别 $s$ 的情况下，观测到 $\\mathbf{x}$ 的似然由多元高斯概率密度函数给出：\n$$\nP(\\mathbf{x}|s) = \\frac{1}{(2\\pi)^{N/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_s)^T \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_s)\\right)\n$$\n代入 $\\Sigma = \\sigma^2 I$，我们得到 $|\\Sigma|^{1/2} = (\\sigma^{2N})^{1/2} = \\sigma^N$ 和 $\\Sigma^{-1} = \\frac{1}{\\sigma^2} I$。\n$$\nP(\\mathbf{x}|s) = \\frac{1}{(2\\pi \\sigma^2)^{N/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (\\mathbf{x} - \\boldsymbol{\\mu}_s)^T (\\mathbf{x} - \\boldsymbol{\\mu}_s)\\right)\n$$\n项 $(\\mathbf{x} - \\boldsymbol{\\mu}_s)^T (\\mathbf{x} - \\boldsymbol{\\mu}_s)$ 是欧几里得距离的平方，即 $\\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2$。\n与 Poisson 情况一样，在先验概率相等的情况下，MAP 分类器等价于 ML 分类器。我们最大化对数似然：\n$$\n\\ln P(\\mathbf{x}|s) = -\\frac{N}{2}\\ln(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2\n$$\n由于 $\\sigma^2$ 与类别无关，项 $-\\frac{N}{2}\\ln(2\\pi \\sigma^2)$ 是一个关于 $s$ 的常数。前面的因子 $-\\frac{1}{2\\sigma^2}$ 是一个负常数。因此，最大化对数似然等价于最小化欧几里得距离的平方 $\\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2$。这证实了问题中关于 Gaussian 模型的第一部分。\n\n为了将决策分差值表示为 $\\mathbf{x}$ 的线性形式，我们考虑两个类别之间距离平方的差。我们将分差定义为 $D_{\\mathrm{Gauss}} = \\|\\mathbf{x} - \\boldsymbol{\\mu}_2\\|^2 - \\|\\mathbf{x} - \\boldsymbol{\\mu}_1\\|^2$。决策规则是如果 $D_{\\mathrm{Gauss}} > 0$ 就选择类别1。\n展开范数的平方：\n$$\n\\|\\mathbf{x} - \\boldsymbol{\\mu}_s\\|^2 = \\mathbf{x}^T\\mathbf{x} - 2\\boldsymbol{\\mu}_s^T\\mathbf{x} + \\boldsymbol{\\mu}_s^T\\boldsymbol{\\mu}_s\n$$\n差值为：\n$$\nD_{\\mathrm{Gauss}} = (\\mathbf{x}^T\\mathbf{x} - 2\\boldsymbol{\\mu}_2^T\\mathbf{x} + \\boldsymbol{\\mu}_2^T\\boldsymbol{\\mu}_2) - (\\mathbf{x}^T\\mathbf{x} - 2\\boldsymbol{\\mu}_1^T\\mathbf{x} + \\boldsymbol{\\mu}_1^T\\boldsymbol{\\mu}_1)\n$$\n$$\nD_{\\mathrm{Gauss}} = 2\\boldsymbol{\\mu}_1^T\\mathbf{x} - 2\\boldsymbol{\\mu}_2^T\\mathbf{x} + \\boldsymbol{\\mu}_2^T\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1^T\\boldsymbol{\\mu}_1\n$$\n$$\nD_{\\mathrm{Gauss}} = 2(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T\\mathbf{x} + (\\|\\boldsymbol{\\mu}_2\\|^2 - \\|\\boldsymbol{\\mu}_1\\|^2)\n$$\n这个表达式是脉冲计数 $\\mathbf{x}$ 的线性函数加上一个依赖于类别的常数，符合要求。\n\n### 第3部分：数值计算\n\n给定速率向量和单次试验脉冲计数向量：\n$$\n\\boldsymbol{\\lambda}_{1} = \\begin{pmatrix} 7 \\\\ 3 \\\\ 12 \\end{pmatrix}, \\quad \\boldsymbol{\\lambda}_{2} = \\begin{pmatrix} 4 \\\\ 5 \\\\ 9 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} 6 \\\\ 7 \\\\ 10 \\end{pmatrix}\n$$\nPoisson 模型的“MAP 线性决策分差值” $D_{\\mathrm{Pois}}$ 是决策分之差 $g_1(\\mathbf{x}) - g_2(\\mathbf{x})$。\n$$\nD_{\\mathrm{Pois}} = \\left( \\sum_{i=1}^{3} x_i \\ln(\\lambda_{1,i}) - \\sum_{i=1}^{3} \\lambda_{1,i} \\right) - \\left( \\sum_{i=1}^{3} x_i \\ln(\\lambda_{2,i}) - \\sum_{i=1}^{3} \\lambda_{2,i} \\right)\n$$\n$$\nD_{\\mathrm{Pois}} = \\sum_{i=1}^{3} x_i (\\ln(\\lambda_{1,i}) - \\ln(\\lambda_{2,i})) + \\left( \\sum_{i=1}^{3} \\lambda_{2,i} - \\sum_{i=1}^{3} \\lambda_{1,i} \\right)\n$$\n我们来计算各项：\n$\\sum \\lambda_{1,i} = 7 + 3 + 12 = 22$。\n$\\sum \\lambda_{2,i} = 4 + 5 + 9 = 18$。\n点积项为：\n$6(\\ln 7 - \\ln 4) + 7(\\ln 3 - \\ln 5) + 10(\\ln 12 - \\ln 9)$\n$= 6\\ln(\\frac{7}{4}) + 7\\ln(\\frac{3}{5}) + 10\\ln(\\frac{12}{9})$\n$= 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3})$\n所以，$D_{\\mathrm{Pois}} = 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) + (18 - 22) = 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) - 4$。\n数值上，这约等于 $3.35769 - 3.57578 + 2.87682 - 4 = -1.34127$。\n\n对于 Gaussian 模型，我们被指示使用单位协方差矩阵，这意味着 $\\sigma^2=1$。我们假设 Gaussian 模型的均值对应于 Poisson 模型的率，因此 $\\boldsymbol{\\mu}_s = \\boldsymbol{\\lambda}_s$。“欧几里得距离平方决策分差值”是 $D_{\\mathrm{Gauss}} = \\|\\mathbf{x} - \\boldsymbol{\\lambda}_2\\|^2 - \\|\\mathbf{x} - \\boldsymbol{\\lambda}_1\\|^2$。\n$$\nD_{\\mathrm{Gauss}} = 2(\\boldsymbol{\\lambda}_1 - \\boldsymbol{\\lambda}_2)^T\\mathbf{x} + (\\|\\boldsymbol{\\lambda}_2\\|^2 - \\|\\boldsymbol{\\lambda}_1\\|^2)\n$$\n我们来计算各项：\n$\\boldsymbol{\\lambda}_1 - \\boldsymbol{\\lambda}_2 = \\begin{pmatrix} 7-4 \\\\ 3-5 \\\\ 12-9 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -2 \\\\ 3 \\end{pmatrix}$。\n内积项：$2(3, -2, 3) \\begin{pmatrix} 6 \\\\ 7 \\\\ 10 \\end{pmatrix} = 2(3 \\times 6 + (-2) \\times 7 + 3 \\times 10) = 2(18 - 14 + 30) = 2(34) = 68$。\n范数平方差：\n$\\|\\boldsymbol{\\lambda}_2\\|^2 = 4^2 + 5^2 + 9^2 = 16 + 25 + 81 = 122$。\n$\\|\\boldsymbol{\\lambda}_1\\|^2 = 7^2 + 3^2 + 12^2 = 49 + 9 + 144 = 202$。\n$\\|\\boldsymbol{\\lambda}_2\\|^2 - \\|\\boldsymbol{\\lambda}_1\\|^2 = 122 - 202 = -80$。\n所以，$D_{\\mathrm{Gauss}} = 68 - 80 = -12$。\n\n最后，我们计算 $\\Delta = D_{\\mathrm{Pois}} - D_{\\mathrm{Gauss}}$：\n$$\n\\Delta = (6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) - 4) - (-12)\n$$\n$$\n\\Delta = 6\\ln(1.75) + 7\\ln(0.6) + 10\\ln(\\frac{4}{3}) + 8\n$$\n使用高精度值：\n$D_{\\mathrm{Pois}} \\approx 2.6587360857 - 4 = -1.3412639143$。\n$\\Delta = -1.3412639143 - (-12) = 10.6587360857$。\n四舍五入到五位有效数字，我们得到 $10.659$。",
            "answer": "$$\\boxed{10.659}$$"
        },
        {
            "introduction": "从理论转向实践，构建一个有效的解码器需要仔细处理数据预处理步骤。本练习  侧重于实现一个完整的解码流程，特别是基线减除和不同的归一化策略，这些是处理真实神经数据时不可或缺的环节。通过这个编码实践，你将学会如何应对神经元整体发放率变化带来的挑战，从而建立一个更鲁棒的解码器。",
            "id": "4197759",
            "problem": "您将执行一项解码任务，该任务使用模板匹配方法处理神经元集群响应。目标是实现一个确定性的模板匹配解码器，该解码器能够严格地处理基线减除和归一化选择，其原理基于独立 Poisson 尖峰放电的统计特性。该解码器必须作用于一组有限的候选刺激模板和一个观测到的响应向量，并返回预测的刺激索引。所有刺激索引都是从 $1$ 开始的整数。不涉及任何物理单位。\n\n基本原理和假设：\n- 每个神经元 $i \\in \\{1,\\dots,N\\}$ 在一个固定的时长为 $T$ 的观测窗口内发放尖峰，当刺激 $s \\in \\{1,\\dots,S\\}$ 出现时，其尖峰计数被建模为速率为 $\\lambda_{s,i}$ 的独立 Poisson 过程。\n- 对于每个刺激 $s$，模板向量 $\\mathbf{M}_s \\in \\mathbb{R}_{\\ge 0}^N$ 代表在窗口 $T$ 内的预期尖峰计数。基线向量 $\\mathbf{b} \\in \\mathbb{R}_{\\ge 0}^N$ 代表在相同持续时间 $T$ 内测量到的自发活动（例如，刺激前的计数），或已转换为对应时长 $T$ 的计数。\n- 提供一个单一的观测尖峰计数向量 $\\mathbf{r} \\in \\mathbb{R}_{\\ge 0}^N$ 用于解码。\n\n需要实现的解码器流程：\n1. 基线处理：计算减去基线并进行修正（rectified）的向量\n   $$\\mathbf{r}' = \\max(\\mathbf{r} - \\mathbf{b}, \\mathbf{0}), \\quad \\mathbf{M}_s' = \\max(\\mathbf{M}_s - \\mathbf{b}, \\mathbf{0}),$$\n   其中最大值是按元素应用的。\n2. 归一化模式为以下之一：\n   - 模式 A（无归一化）：使用原始向量 $\\mathbf{r}'$ 和 $\\mathbf{M}_s'$。\n   - 模式 B（欧几里得 $L_2$ 归一化）：通过欧几里得范数进行归一化，即对于任何非零向量 $\\mathbf{x}$，使用 $\\widehat{\\mathbf{x}} = \\mathbf{x} / \\|\\mathbf{x}\\|_2$。\n   - 模式 C（基于 Poisson 统计的方差归一化）：令 $\\boldsymbol{\\mu} = \\frac{1}{S}\\sum_{s=1}^S \\mathbf{M}_s'$ 为减去基线后的模板在所有刺激上的平均值。定义一个逐元素的白化变换 $\\mathcal{W}(\\mathbf{x}) = \\mathbf{x} \\oslash \\sqrt{\\boldsymbol{\\mu} + \\varepsilon}$，其中 $\\oslash$ 表示逐元素除法，$\\varepsilon = 10^{-12}$ 是一个用于避免除以零的小常数。将 $\\mathcal{W}$ 应用于 $\\mathbf{r}'$ 和所有的 $\\mathbf{M}_s'$。然后对白化后的向量应用模式 B 中的欧几里得 $L_2$ 归一化。\n3. 相似度与决策规则：\n   - 对于模式 A，计算相似度分数 $\\sigma_s = \\langle \\mathbf{r}', \\mathbf{M}_s' \\rangle$，即标准内积。\n   - 对于模式 B 和模式 C，计算余弦相似度 $\\sigma_s = \\langle \\widehat{\\mathbf{r}'}, \\widehat{\\mathbf{M}_s'} \\rangle$，其中帽子符号表示在模式 C 中任何白化操作之后应用的 $L_2$ 归一化。\n4. 退化向量处理与平局决胜：\n   - 定义 $\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}$。如果在模式 B 或模式 C 中，两个处理后的向量都是精确的零向量（在数值计算中，将范数小于 $\\varepsilon$ 的向量视为零向量），则为该 $s$ 设置 $\\sigma_s = 0$。\n   - 如果在模式 B 或模式 C 中，两个处理后的向量中恰好有一个是零向量，则为该 $s$ 设置 $\\sigma_s = -\\infty$。\n   - 在模式 A 中，直接使用定义的内积，没有特殊情况；一个零向量只是简单地产生标准的内积结果。\n   - 预测使 $\\sigma_s$ 最大化的刺激索引 $\\hat{s}$。如果 $\\sigma_s$ 出现平局，选择最大化者中最小的索引 $s$。\n\n您的程序必须实现这个解码器，并在以下测试集上进行评估。对于每个案例，返回预测的刺激索引 $\\hat{s}$ 作为整数。将所有案例的结果汇总到单行打印输出中，形式为用方括号括起来的逗号分隔列表，不含空格，例如 `[a,b,c,d]`。\n\n测试集：\n- 案例 $1$（无归一化的理想路径）：\n  - $N = 4$, $S = 3$。\n  - 基线 $\\mathbf{b} = [2, 5, 1, 0]$。\n  - 模板 $\\mathbf{M}_1 = [10, 7, 3, 2]$, $\\mathbf{M}_2 = [6, 10, 2, 1]$, $\\mathbf{M}_3 = [3, 6, 5, 1]$。\n  - 观测值 $\\mathbf{r} = [9, 10, 2, 3]$。\n  - 归一化模式：模式 A（无归一化）。\n- 案例 $2$（通过 $L_2$ 归一化实现全局增益鲁棒性）：\n  - $N = 4$, $S = 3$。\n  - 基线 $\\mathbf{b} = [2, 5, 1, 0]$。\n  - 模板 $\\mathbf{M}_1 = [10, 7, 3, 2]$, $\\mathbf{M}_2 = [6, 10, 2, 1]$, $\\mathbf{M}_3 = [3, 6, 5, 1]$。\n  - 观测值 $\\mathbf{r} = [26, 11, 7, 6]$。\n  - 归一化模式：模式 B（$L_2$ 归一化）。\n- 案例 $3$（基于 Poisson 统计的方差归一化）：\n  - $N = 3$, $S = 2$。\n  - 基线 $\\mathbf{b} = [0, 0, 0]$。\n  - 模板 $\\mathbf{M}_1 = [1000, 2, 4]$, $\\mathbf{M}_2 = [1001, 10, 1]$。\n  - 观测值 $\\mathbf{r} = [1000, 2, 4]$。\n  - 归一化模式：模式 C（方差归一化后进行 $L_2$ 归一化，$\\varepsilon = 10^{-12}$）。\n- 案例 $4$（基线减除后出现零向量的边界情况）：\n  - $N = 2$, $S = 2$。\n  - 基线 $\\mathbf{b} = [5, 5]$。\n  - 模板 $\\mathbf{M}_1 = [5, 5]$, $\\mathbf{M}_2 = [6, 5]$。\n  - 观测值 $\\mathbf{r} = [5, 5]$。\n  - 归一化模式：模式 B（$L_2$ 归一化）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含案例 1 到 4 的预测索引，按顺序以逗号分隔，并用方括号括起来，例如 `[1,2,1,1]`。不要打印任何其他文本。",
            "solution": "该问题要求实现一个确定性的模板匹配解码器，用于从神经元集群响应向量中预测刺激索引。解码过程包括三个主要阶段：基线减除、归一化选择和相似度计算，最后是决策规则。解决方案将通过将指定算法细致地应用于四个测试案例中的每一个来得出。所有刺激索引 $s$ 都是从 1 开始的。\n\n解码器流程定义如下：\n1.  **基线减除**：计算经过修正、减去基线的向量 $\\mathbf{r}' = \\max(\\mathbf{r} - \\mathbf{b}, \\mathbf{0})$ 和 $\\mathbf{M}_s' = \\max(\\mathbf{M}_s - \\mathbf{b}, \\mathbf{0})$。\n2.  **归一化**：应用三种模式之一：模式 A（无）、模式 B（$L_2$ 范数）或模式 C（方差白化后接 $L_2$ 范数）。\n3.  **相似度**：计算分数 $\\sigma_s$。对于模式 A，这是内积 $\\langle \\mathbf{r}', \\mathbf{M}_s' \\rangle$。对于模式 B 和 C，这是归一化向量的余弦相似度。\n4.  **决策**：预测的刺激是 $\\hat{s} = \\arg\\max_s \\sigma_s$，平局时选择最小的索引 $s$ 来打破。\n\n对于模式 B 和 C，指定了对退化零向量（定义为 $L_2$ 范数小于 $\\varepsilon = 10^{-12}$ 的向量）的特殊处理。如果比较对中的两个向量都为零，它们的相似度 $\\sigma_s$ 为 $0$。如果只有一个为零，则 $\\sigma_s = -\\infty$。\n\n### 案例 1：模式 A（无归一化）\n给定：\n- $N = 4$, $S = 3$。\n- 基线 $\\mathbf{b} = [2, 5, 1, 0]$。\n- 模板 $\\mathbf{M}_1 = [10, 7, 3, 2]$, $\\mathbf{M}_2 = [6, 10, 2, 1]$, $\\mathbf{M}_3 = [3, 6, 5, 1]$。\n- 观测值 $\\mathbf{r} = [9, 10, 2, 3]$。\n- 归一化：模式 A。\n\n1.  **基线减除**：\n    - $\\mathbf{r}' = \\max(\\mathbf{r} - \\mathbf{b}, \\mathbf{0}) = \\max([9-2, 10-5, 2-1, 3-0], \\mathbf{0}) = [7, 5, 1, 3]$。\n    - $\\mathbf{M}_1' = \\max(\\mathbf{M}_1 - \\mathbf{b}, \\mathbf{0}) = \\max([10-2, 7-5, 3-1, 2-0], \\mathbf{0}) = [8, 2, 2, 2]$。\n    - $\\mathbf{M}_2' = \\max(\\mathbf{M}_2 - \\mathbf{b}, \\mathbf{0}) = \\max([6-2, 10-5, 2-1, 1-0], \\mathbf{0}) = [4, 5, 1, 1]$。\n    - $\\mathbf{M}_3' = \\max(\\mathbf{M}_3 - \\mathbf{b}, \\mathbf{0}) = \\max([3-2, 6-5, 5-1, 1-0], \\mathbf{0}) = [1, 1, 4, 1]$。\n\n2.  **归一化**：模式 A 不需要归一化。\n\n3.  **相似度（内积）**：\n    - $\\sigma_1 = \\langle \\mathbf{r}', \\mathbf{M}_1' \\rangle = (7)(8) + (5)(2) + (1)(2) + (3)(2) = 56 + 10 + 2 + 6 = 74$。\n    - $\\sigma_2 = \\langle \\mathbf{r}', \\mathbf{M}_2' \\rangle = (7)(4) + (5)(5) + (1)(1) + (3)(1) = 28 + 25 + 1 + 3 = 57$。\n    - $\\sigma_3 = \\langle \\mathbf{r}', \\mathbf{M}_3' \\rangle = (7)(1) + (5)(1) + (1)(4) + (3)(1) = 7 + 5 + 4 + 3 = 19$。\n\n4.  **决策**：\n    分数为 $(\\sigma_1, \\sigma_2, \\sigma_3) = (74, 57, 19)$。最大分数为 $74$，对应于刺激 $s=1$。\n    因此，$\\hat{s} = 1$。\n\n### 案例 2：模式 B（$L_2$ 归一化）\n给定：\n- $N = 4$, $S = 3$。\n- 基线 $\\mathbf{b} = [2, 5, 1, 0]$。\n- 模板 $\\mathbf{M}_1$、$\\mathbf{M}_2$、$\\mathbf{M}_3$ 与案例 1 中相同。\n- 观测值 $\\mathbf{r} = [26, 11, 7, 6]$。\n- 归一化：模式 B。\n\n1.  **基线减除**：\n    - 模板 $\\mathbf{M}_s'$ 与案例 1 相同：$\\mathbf{M}_1'=[8,2,2,2]$, $\\mathbf{M}_2'=[4,5,1,1]$, $\\mathbf{M}_3'=[1,1,4,1]$。\n    - $\\mathbf{r}' = \\max(\\mathbf{r} - \\mathbf{b}, \\mathbf{0}) = \\max([26-2, 11-5, 7-1, 6-0], \\mathbf{0}) = [24, 6, 6, 6]$。我们观察到 $\\mathbf{r}' = 3 \\cdot \\mathbf{M}_1'$。\n\n2.  **归一化（模式 B）**：\n    使用 $L_2$ 范数 $\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}$ 将向量归一化为单位长度。归一化后的向量是 $\\widehat{\\mathbf{x}} = \\mathbf{x} / \\|\\mathbf{x}\\|_2$。\n    - 对于 $\\mathbf{r}' = [24, 6, 6, 6]$ 和 $\\mathbf{M}_1'=[8,2,2,2]$，由于 $\\mathbf{r}' = 3 \\mathbf{M}_1'$，它们的方向相同。因此，它们归一化后的向量相等：$\\widehat{\\mathbf{r}'} = \\widehat{\\mathbf{M}_1'}$。\n    - 向量 $\\mathbf{r}', \\mathbf{M}_1', \\mathbf{M}_2', \\mathbf{M}_3'$ 都不是零向量，因此没有退化情况适用。\n\n3.  **相似度（余弦相似度）**：\n    - $\\sigma_1 = \\langle \\widehat{\\mathbf{r}'}, \\widehat{\\mathbf{M}_1'} \\rangle$。由于 $\\widehat{\\mathbf{r}'} = \\widehat{\\mathbf{M}_1'}$，它们的内积是 $\\|\\widehat{\\mathbf{M}_1'}\\|_2^2 = 1$。\n    - $\\sigma_2 = \\langle \\widehat{\\mathbf{r}'}, \\widehat{\\mathbf{M}_2'} \\rangle = \\frac{\\langle \\mathbf{r}', \\mathbf{M}_2' \\rangle}{\\|\\mathbf{r}'\\|_2 \\|\\mathbf{M}_2'\\|_2} = \\frac{(24)(4)+(6)(5)+(6)(1)+(6)(1)}{\\sqrt{24^2+3\\cdot 6^2}\\sqrt{4^2+5^2+1^2+1^2}} = \\frac{138}{\\sqrt{684}\\sqrt{43}} \\approx 0.805$。\n    - $\\sigma_3 = \\langle \\widehat{\\mathbf{r}'}, \\widehat{\\mathbf{M}_3'} \\rangle = \\frac{\\langle \\mathbf{r}', \\mathbf{M}_3' \\rangle}{\\|\\mathbf{r}'\\|_2 \\|\\mathbf{M}_3'\\|_2} = \\frac{(24)(1)+(6)(1)+(6)(4)+(6)(1)}{\\sqrt{684}\\sqrt{1^2+1^2+4^2+1^2}} = \\frac{60}{\\sqrt{684}\\sqrt{19}} \\approx 0.526$。\n\n4.  **决策**：\n    分数为 $(\\sigma_1, \\sigma_2, \\sigma_3) \\approx (1, 0.805, 0.526)$。最大分数为 $1$，对应于刺激 $s=1$。\n    因此，$\\hat{s} = 1$。\n\n### 案例 3：模式 C（方差归一化）\n给定：\n- $N = 3$, $S = 2$。\n- 基线 $\\mathbf{b} = [0, 0, 0]$。\n- 模板 $\\mathbf{M}_1 = [1000, 2, 4]$, $\\mathbf{M}_2 = [1001, 10, 1]$。\n- 观测值 $\\mathbf{r} = [1000, 2, 4]$。\n- 归一化：模式 C，其中 $\\varepsilon = 10^{-12}$。\n\n1.  **基线减除**：\n    - 当 $\\mathbf{b} = \\mathbf{0}$ 时，我们有 $\\mathbf{r}'=\\mathbf{r}$，$\\mathbf{M}_1'=\\mathbf{M}_1$，$\\mathbf{M}_2'=\\mathbf{M}_2$。\n    - $\\mathbf{r}' = [1000, 2, 4]$ 且 $\\mathbf{M}_1'=[1000, 2, 4]$，所以 $\\mathbf{r}' = \\mathbf{M}_1'$。\n\n2.  **归一化（模式 C）**：\n    - 首先，计算减去基线后的模板的平均值：$\\boldsymbol{\\mu} = \\frac{1}{S}\\sum_{s=1}^S \\mathbf{M}_s' = \\frac{1}{2}(\\mathbf{M}_1' + \\mathbf{M}_2') = \\frac{1}{2}([1000, 2, 4] + [1001, 10, 1]) = [1000.5, 6, 2.5]$。\n    - 接下来，将逐元素的白化变换 $\\mathcal{W}(\\mathbf{x}) = \\mathbf{x} \\oslash \\sqrt{\\boldsymbol{\\mu} + \\varepsilon}$ 应用于 $\\mathbf{r}'$ 和所有的 $\\mathbf{M}_s'$。令 $\\mathbf{w}(\\cdot)$ 表示白化后的向量。\n    - 由于 $\\mathbf{r}' = \\mathbf{M}_1'$，它们的白化版本也将相同：$\\mathbf{w}(\\mathbf{r}') = \\mathbf{w}(\\mathbf{M}_1')$。\n    - 后续的 $L_2$ 归一化也将产生相同的单位向量：$\\widehat{\\mathbf{w}(\\mathbf{r}')} = \\widehat{\\mathbf{w}(\\mathbf{M}_1')}$。\n    - 在此过程中，没有向量变成零向量。\n\n3.  **相似度（余弦相似度）**：\n    - $\\sigma_1 = \\langle \\widehat{\\mathbf{w}(\\mathbf{r}')}, \\widehat{\\mathbf{w}(\\mathbf{M}_1')} \\rangle$。由于向量相同且非零，它们的余弦相似度为 $1$。\n    - $\\sigma_2 = \\langle \\widehat{\\mathbf{w}(\\mathbf{r}')}, \\widehat{\\mathbf{w}(\\mathbf{M}_2')} \\rangle$。由于 $\\mathbf{M}_1'$ 和 $\\mathbf{M}_2'$ 不是共线的，$\\mathbf{w}(\\mathbf{M}_1')$ 和 $\\mathbf{w}(\\mathbf{M}_2')$ 也不会是共线的，因此它们的余弦相似度将严格小于 $1$。显式计算得出 $\\sigma_2 \\approx 0.993$。\n\n4.  **决策**：\n    分数为 $(\\sigma_1, \\sigma_2) = (1, \\approx 0.993)$。最大分数为 $1$，对应于刺激 $s=1$。\n    因此，$\\hat{s} = 1$。\n\n### 案例 4：具有零向量的边界情况\n给定：\n- $N = 2$, $S = 2$。\n- 基线 $\\mathbf{b} = [5, 5]$。\n- 模板 $\\mathbf{M}_1 = [5, 5]$, $\\mathbf{M}_2 = [6, 5]$。\n- 观测值 $\\mathbf{r} = [5, 5]$。\n- 归一化：模式 B。\n\n1.  **基线减除**：\n    - $\\mathbf{r}' = \\max(\\mathbf{r} - \\mathbf{b}, \\mathbf{0}) = \\max([5-5, 5-5], \\mathbf{0}) = [0, 0]$。\n    - $\\mathbf{M}_1' = \\max(\\mathbf{M}_1 - \\mathbf{b}, \\mathbf{0}) = \\max([5-5, 5-5], \\mathbf{0}) = [0, 0]$。\n    - $\\mathbf{M}_2' = \\max(\\mathbf{M}_2 - \\mathbf{b}, \\mathbf{0}) = \\max([6-5, 5-5], \\mathbf{0}) = [1, 0]$。\n\n2.  **归一化（模式 B）与退化情况**：\n    在模式 B 中，被检查是否为零的“处理后向量”是 $\\mathbf{r}'$ 和 $\\mathbf{M}_s'$。我们测试它们的范数是否小于 $\\varepsilon = 10^{-12}$。\n    - $\\|\\mathbf{r}'\\|_2 = \\sqrt{0^2+0^2} = 0$。这是一个零向量。\n    - $\\|\\mathbf{M}_1'\\|_2 = \\sqrt{0^2+0^2} = 0$。这是一个零向量。\n    - $\\|\\mathbf{M}_2'\\|_2 = \\sqrt{1^2+0^2} = 1$。这不是一个零向量。\n\n3.  **相似度（带退化规则的余弦相似度）**：\n    - 对于 $s=1$，我们比较 $\\mathbf{r}'$ 和 $\\mathbf{M}_1'$。两者都是零向量。规则规定：“如果两个处理后的向量...都是精确的零向量...则设置 $\\sigma_s = 0$”。\n      因此，$\\sigma_1 = 0$。\n    - 对于 $s=2$，我们比较 $\\mathbf{r}'$ 和 $\\mathbf{M}_2'$。恰好有一个向量（$\\mathbf{r}'$）是零向量。规则规定：“如果两个处理后的向量中恰好有一个是零...则设置 $\\sigma_s = -\\infty$”。\n      因此，$\\sigma_2 = -\\infty$。\n\n4.  **决策**：\n    分数为 $(\\sigma_1, \\sigma_2) = (0, -\\infty)$。最大分数为 $0$，对应于刺激 $s=1$。\n    因此，$\\hat{s} = 1$。\n\n所有案例的最终结果是 $[1, 1, 1, 1]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef template_matching_decoder(N, S, b, templates, r, mode, epsilon):\n    \"\"\"\n    Implements a deterministic template matching decoder for neuronal responses.\n\n    Args:\n        N (int): Number of neurons.\n        S (int): Number of stimuli.\n        b (list or np.ndarray): Baseline vector of shape (N,).\n        templates (list of lists or np.ndarray): List of S template vectors, each shape (N,).\n        r (list or np.ndarray): Observed response vector of shape (N,).\n        mode (str): Normalization mode ('A', 'B', or 'C').\n        epsilon (float): Small constant for regularization and zero-checking.\n\n    Returns:\n        int: The predicted stimulus index (1-based).\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    b = np.array(b, dtype=float)\n    templates = np.array(templates, dtype=float)\n    r = np.array(r, dtype=float)\n\n    # Step 1: Baseline handling\n    r_prime = np.maximum(r - b, 0)\n    Ms_prime = np.maximum(templates - b, 0)\n\n    scores = []\n\n    if mode == 'A':\n        # Step 2A: No normalization\n        # Step 3A: Similarity is inner product\n        for s in range(S):\n            score = np.dot(r_prime, Ms_prime[s])\n            scores.append(score)\n\n    elif mode == 'B':\n        # Vectors to be normalized are r_prime and Ms_prime\n        norm_r = np.linalg.norm(r_prime)\n        r_is_zero = norm_r  epsilon\n\n        for s in range(S):\n            m_s_prime = Ms_prime[s]\n            norm_m = np.linalg.norm(m_s_prime)\n            m_is_zero = norm_m  epsilon\n\n            # Step 4: Degenerate vector handling\n            if r_is_zero and m_is_zero:\n                score = 0.0\n            elif r_is_zero or m_is_zero:\n                score = -np.inf\n            else:\n                # Step 2B  3B: L2 norm and cosine similarity\n                score = np.dot(r_prime, m_s_prime) / (norm_r * norm_m)\n            scores.append(score)\n\n    elif mode == 'C':\n        # Step 2C, Part 1: Variance normalization (whitening)\n        mu = np.mean(Ms_prime, axis=0)\n        whitener_denom = np.sqrt(mu + epsilon)\n        \n        # Element-wise division, handling potential division by zero in whitener\n        whitened_r = np.divide(r_prime, whitener_denom, \n                               out=np.zeros_like(r_prime), \n                               where=whitener_denom!=0)\n        \n        whitened_Ms = np.divide(Ms_prime, whitener_denom, \n                                out=np.zeros_like(Ms_prime), \n                                where=whitener_denom!=0)\n\n        # Vectors to be L2-normalized are the whitened ones\n        norm_whitened_r = np.linalg.norm(whitened_r)\n        r_is_zero = norm_whitened_r  epsilon\n\n        for s in range(S):\n            current_whitened_m = whitened_Ms[s]\n            norm_whitened_m = np.linalg.norm(current_whitened_m)\n            m_is_zero = norm_whitened_m  epsilon\n\n            # Step 4: Degenerate vector handling\n            if r_is_zero and m_is_zero:\n                score = 0.0\n            elif r_is_zero or m_is_zero:\n                score = -np.inf\n            else:\n                # Step 2C, Part 2  Step 3C: L2 norm and cosine similarity\n                score = np.dot(whitened_r, current_whitened_m) / (norm_whitened_r * norm_whitened_m)\n            scores.append(score)\n\n    # Step 4: Decision rule (argmax breaks ties by smallest index)\n    predicted_index = np.argmax(scores) + 1\n    return predicted_index\n\ndef solve():\n    \"\"\"\n    Runs the decoder on the test suite provided in the problem statement.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\"N\": 4, \"S\": 3, \"b\": [2, 5, 1, 0], \n         \"templates\": [[10, 7, 3, 2], [6, 10, 2, 1], [3, 6, 5, 1]], \n         \"r\": [9, 10, 2, 3], \"mode\": 'A', \"epsilon\": 1e-12},\n        # Case 2\n        {\"N\": 4, \"S\": 3, \"b\": [2, 5, 1, 0], \n         \"templates\": [[10, 7, 3, 2], [6, 10, 2, 1], [3, 6, 5, 1]], \n         \"r\": [26, 11, 7, 6], \"mode\": 'B', \"epsilon\": 1e-12},\n        # Case 3\n        {\"N\": 3, \"S\": 2, \"b\": [0, 0, 0], \n         \"templates\": [[1000, 2, 4], [1001, 10, 1]], \n         \"r\": [1000, 2, 4], \"mode\": 'C', \"epsilon\": 1e-12},\n        # Case 4\n        {\"N\": 2, \"S\": 2, \"b\": [5, 5], \n         \"templates\": [[5, 5], [6, 5]], \n         \"r\": [5, 5], \"mode\": 'B', \"epsilon\": 1e-12},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = template_matching_decoder(\n            case[\"N\"], case[\"S\"], case[\"b\"], case[\"templates\"],\n            case[\"r\"], case[\"mode\"], case[\"epsilon\"]\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "真实的神经记录往往充满噪声和离群值，这些异常数据会严重影响标准解码器的性能。这个高级实践  介绍了几种强大的鲁棒统计方法来解决这一问题。通过实现和比较经典的极大似然解码器与基于Huber损失和修剪离差的现代鲁棒解码器，你将掌握如何构建能够有效抵抗数据污染、性能更稳定的解码模型。",
            "id": "4197761",
            "problem": "您面临一个神经科学数据分析中的群体编码问题，重点是模板匹配解码器以及对离群值和污染的鲁棒性。考虑一个由$N$个神经元组成的群体，它们对$S$个离散刺激中的一个作出响应。对于每种刺激$s \\in \\{0,1,2\\}$，一个模板指定了神经元群体的预期平均脉冲计数，表示为一个非负向量$\\boldsymbol{\\lambda}_s \\in \\mathbb{R}_{0}^N$。单次试验产生一个观测到的非负整数脉冲计数向量$\\mathbf{k} \\in \\mathbb{Z}_{\\ge 0}^N$。目标是通过将$\\mathbf{k}$与模板集$\\{\\boldsymbol{\\lambda}_s\\}_{s=0}^2$进行比较，来解码（推断）刺激的身份$s$。\n\n从一个基本假设开始：在给定刺激的条件下，神经元间的脉冲计数被建模为独立的泊松随机变量：对于神经元$i$，$K_i \\mid s \\sim \\mathrm{Poisson}(\\lambda_{s,i})$，其中$\\lambda_{s,i}  0$是在固定计数窗口内的期望计数。最大似然估计（MLE）解码器选择能最大化以下对数似然的刺激$s$：\n$$\n\\log p(\\mathbf{k} \\mid s) = \\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} - \\log(k_i!) \\right),\n$$\n其中，项$\\sum_i \\log(k_i!)$不依赖于$s$，因此在比较时可以忽略。真实的神经数据常因非平稳性、噪声爆发、脉冲分拣错误或罕见事件而包含污染或离群值。可以使用广义线性模型（GLM）的偏差残差和鲁棒聚合方法来构建经典模板匹配的鲁棒替代方案。定义神经元$i$和刺激$s$的泊松偏差：\n$$\nD_{s,i}(\\mathbf{k}) = 2 \\left( k_i \\log\\left( \\frac{k_i}{\\lambda_{s,i}} \\right) - (k_i - \\lambda_{s,i}) \\right),\n$$\n约定当$k_i = 0$时，$k_i \\log(k_i/\\lambda_{s,i}) = 0$。有符号偏差残差为\n$$\nr_{s,i}(\\mathbf{k}) = \\operatorname{sign}(k_i - \\lambda_{s,i}) \\sqrt{ D_{s,i}(\\mathbf{k}) }.\n$$\n一个鲁棒的M估计量目标函数可以使用Huber损失函数$\\rho_\\delta$（阈值$\\delta  0$）来构建，\n$$\n\\rho_\\delta(x) = \\begin{cases}\n\\frac{1}{2} x^2,  \\text{若 } |x| \\le \\delta, \\\\\n\\delta \\left( |x| - \\frac{1}{2} \\delta \\right),  \\text{若 } |x|  \\delta,\n\\end{cases}\n$$\n刺激$s$的鲁棒得分为$\\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k}))$，解码器选择使该得分最小的刺激。另一种鲁棒替代方案是修剪偏差聚合器，它对$\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$进行排序，并仅对其中最小的一部分（比例为$\\tau \\in (0,1]$）求和，从而减少与离群值相关的最大残差的影响。\n\n您的任务是实现三个解码器：\n- 经典的泊松MLE模板匹配器，选择最大化$\\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} \\right)$的$s$。\n- 鲁棒的Huber偏差模板匹配器，选择最小化$\\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k}))$的$s$。\n- 修剪偏差模板匹配器，选择最小化$\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$中最小的$\\lfloor \\tau N \\rfloor$个值的和的$s$。\n\n采用以下通过缩放基准速率向量定义的模板。设$N = 12$, $S = 3$，基准速率向量为\n$$\n\\mathbf{b} = [\\, 5,\\, 6,\\, 4,\\, 8,\\, 2,\\, 9,\\, 3,\\, 7,\\, 5,\\, 6,\\, 4,\\, 2 \\,],\n$$\n刺激特异性缩放因子为\n$$\n\\mathbf{f} = [\\, 1.0,\\, 1.5,\\, 0.6 \\,].\n$$\n对于$s \\in \\{0,1,2\\}$，按分量定义$\\boldsymbol{\\lambda}_s$为$\\lambda_{s,i} = f_s \\cdot b_i$。具体来说，模板是\n$$\n\\boldsymbol{\\lambda}_0 = [\\, 5.0,\\, 6.0,\\, 4.0,\\, 8.0,\\, 2.0,\\, 9.0,\\, 3.0,\\, 7.0,\\, 5.0,\\, 6.0,\\, 4.0,\\, 2.0 \\,],\n$$\n$$\n\\boldsymbol{\\lambda}_1 = [\\, 7.5,\\, 9.0,\\, 6.0,\\, 12.0,\\, 3.0,\\, 13.5,\\, 4.5,\\, 10.5,\\, 7.5,\\, 9.0,\\, 6.0,\\, 3.0 \\,],\n$$\n$$\n\\boldsymbol{\\lambda}_2 = [\\, 3.0,\\, 3.6,\\, 2.4,\\, 4.8,\\, 1.2,\\, 5.4,\\, 1.8,\\, 4.2,\\, 3.0,\\, 3.6,\\, 2.4,\\, 1.2 \\,].\n$$\n请注意，这些数值是严格为正的，代表每个固定计数窗口的预期脉冲计数；由于输出是离散标签，您不需要为最终答案表达任何物理单位。\n\n使用以下观测到的脉冲计数向量（每个长度为12）的测试套件，其中包括正常情况、污染和边界情况。每个测试用例都需要用上述三种方法解码刺激。在所有情况下，使用Huber阈值$\\delta = 2.0$和修剪比例$\\tau = 0.7$。\n\n- 测试用例1（理想情况，无污染，典型计数接近$\\boldsymbol{\\lambda}_1$）：\n$$\n\\mathbf{k}^{(1)} = [\\, 8,\\, 9,\\, 6,\\, 12,\\, 3,\\, 14,\\, 5,\\, 11,\\, 8,\\, 9,\\, 6,\\, 3 \\,].\n$$\n- 测试用例2（重度污染：大部分接近$\\boldsymbol{\\lambda}_2$，但在三个神经元中有大的离群值）：\n$$\n\\mathbf{k}^{(2)} = [\\, 3,\\, 4,\\, 2,\\, 5,\\, 50,\\, 5,\\, 40,\\, 4,\\, 3,\\, 60,\\, 2,\\, 1 \\,].\n$$\n- 测试用例3（边界情况：所有神经元静默）：\n$$\n\\mathbf{k}^{(3)} = [\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0 \\,].\n$$\n- 测试用例4（单个极端离群值，其他接近零）：\n$$\n\\mathbf{k}^{(4)} = [\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 0,\\, 100,\\, 0,\\, 0 \\,].\n$$\n- 测试用例5（轻度污染：典型计数接近$\\boldsymbol{\\lambda}_0$，有两个中等离群值）：\n$$\n\\mathbf{k}^{(5)} = [\\, 5,\\, 6,\\, 4,\\, 8,\\, 2,\\, 9,\\, 3,\\, 7,\\, 5,\\, 20,\\, 15,\\, 2 \\,].\n$$\n\n在得分完全相等（容差为$\\epsilon = 10^{-12}$）的情况下，通过选择最小的刺激索引$s$来实现确定性平局打破规则。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例对应一个包含三个整数的列表$[s_{\\mathrm{MLE}}, s_{\\mathrm{Huber}}, s_{\\mathrm{Trim}}]$，分别表示按上述顺序三种方法解码出的标签。例如，五个测试用例的最终输出格式为\n$$\n[\\, [\\, a_1,\\, b_1,\\, c_1 \\,],\\, [\\, a_2,\\, b_2,\\, c_2 \\,],\\, [\\, a_3,\\, b_3,\\, c_3 \\,],\\, [\\, a_4,\\, b_4,\\, c_4 \\,],\\, [\\, a_5,\\, b_5,\\, c_5 \\,] \\,],\n$$\n其中每个$a_j$、$b_j$和$c_j$是$\\{0,1,2\\}$中的一个整数。\n\n不需要用户输入。程序必须是自包含的，并重现上面给出的确切测试套件和参数。",
            "solution": "用户提供的问题是一个在计算神经科学领域中定义明确的任务。它要求基于脉冲计数的泊松统计模型，实现并比较三种不同的群体编码神经数据解码算法。该问题具有科学依据，数学上精确，并包含了获得唯一解所需的所有数据和参数。因此，该问题被认为是有效的。\n\n问题的核心是从一个由$N=12$个神经元组成的群体产生的观测脉冲计数向量$\\mathbf{k} \\in \\mathbb{Z}_{\\ge 0}^{N}$中推断出刺激$s \\in \\{0, 1, 2\\}$。推断的基础是将观测值$\\mathbf{k}$与一组预定义的模板$\\{\\boldsymbol{\\lambda}_s\\}_{s=0}^2$进行比较，其中每个模板$\\boldsymbol{\\lambda}_s \\in \\mathbb{R}_{0}^N$代表给定刺激$s$的预期脉冲计数。模板的分量由$\\lambda_{s,i} = f_s \\cdot b_i$指定，使用所提供的基准速率向量$\\mathbf{b}$和刺激特异性缩放因子$\\mathbf{f}$。\n\n解决方案涉及为每种刺激$s$在三种不同的解码方案下计算一个分数，并选择优化该分数的刺激。平局通过选择最小索引的刺激来解决。\n\n**1. 泊松最大似然（MLE）解码器**\n\n该解码器基于一个基本假设，即在给定刺激$s$的条件下，每个神经元$i$的脉冲计数$k_i$是独立的泊松随机变量，其均值为$\\lambda_{s,i}$。观测到向量$\\mathbf{k}$的似然为：\n$$\np(\\mathbf{k} \\mid s) = \\prod_{i=1}^N \\frac{e^{-\\lambda_{s,i}} \\lambda_{s,i}^{k_i}}{k_i!}\n$$\n为了找到$s$的最大似然估计，我们最大化对数似然：\n$$\n\\log p(\\mathbf{k} \\mid s) = \\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} - \\log(k_i!) \\right)\n$$\n由于项$\\sum_{i=1}^N \\log(k_i!)$与刺激$s$无关，因此可以从优化中去掉。因此，MLE解码器的目标是找到使以下分数最大化的刺激$s$：\n$$\n\\mathcal{S}_{\\text{MLE}}(s; \\mathbf{k}) = \\sum_{i=1}^N \\left( k_i \\log \\lambda_{s,i} - \\lambda_{s,i} \\right)\n$$\n解码出的刺激是$s_{\\text{MLE}} = \\arg\\max_{s} \\mathcal{S}_{\\text{MLE}}(s; \\mathbf{k})$。该方法在假设的模型下是最优的，但众所周知它对离群值或模型误设敏感，因为大的计数值$k_i$会通过$k_i \\log \\lambda_{s,i}$项对分数产生重大影响。\n\n**2. 鲁棒的Huber偏差解码器**\n\n该解码器是一种旨在减轻离群值影响的鲁棒替代方案。它作用于偏差残差，该残差衡量观测数据与模型期望之间的差异。在刺激$s$的假设下，神经元$i$的泊松偏差为：\n$$\nD_{s,i}(\\mathbf{k}) = 2 \\left( k_i \\log\\left( \\frac{k_i}{\\lambda_{s,i}} \\right) - (k_i - \\lambda_{s,i}) \\right)\n$$\n对于$k_i=0$的情况，使用一个特殊约定，即$k_i \\log(k_i/\\lambda_{s,i})$项取为$0$，导致$D_{s,i} = 2\\lambda_{s,i}$。有符号偏差残差则为：\n$$\nr_{s,i}(\\mathbf{k}) = \\operatorname{sign}(k_i - \\lambda_{s,i}) \\sqrt{ D_{s,i}(\\mathbf{k}) }\n$$\n如果模型正确，这些残差近似服从标准正态分布。为实现鲁棒性，残差平方和（与总偏差相关）被替换为残差的增长较慢的函数之和。为此使用了带有阈值$\\delta0$的Huber损失函数$\\rho_\\delta(x)$：\n$$\n\\rho_\\delta(x) = \\begin{cases}\n\\frac{1}{2} x^2,  \\text{若 } |x| \\le \\delta \\\\\n\\delta \\left( |x| - \\frac{1}{2} \\delta \\right),  \\text{若 } |x|  \\delta\n\\end{cases}\n$$\n对于小残差（$|r_{s,i}| \\le \\delta$），这等同于标准最小二乘法，但对于大残差，其增长变为线性而非二次，从而降低了离群值的影响。目标是找到最小化总Huber分数的刺激$s$，使用给定的阈值$\\delta = 2.0$：\n$$\n\\mathcal{S}_{\\text{Huber}}(s; \\mathbf{k}) = \\sum_{i=1}^N \\rho_\\delta(r_{s,i}(\\mathbf{k}))\n$$\n解码出的刺激是$s_{\\text{Huber}} = \\arg\\min_{s} \\mathcal{S}_{\\text{Huber}}(s; \\mathbf{k})$。\n\n**3. 修剪偏差解码器**\n\n这是另一种通过明确排除离群值来处理它们的鲁棒方法。它为每个神经元计算偏差$D_{s,i}(\\mathbf{k})$，但不是将它们全部相加，而是丢弃一部分最大的偏差，这些偏差被认为对应于污染数据点或离群值。\n\n过程如下：\n1. 对于给定的刺激$s$和观测值$\\mathbf{k}$，计算偏差集合$\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$。\n2. 将这些偏差按非降序排序。\n3. 仅对最小的$\\lfloor \\tau N \\rfloor$个偏差求和，其中$\\tau \\in (0, 1]$是修剪比例。\n\n对于此问题，$N=12$且$\\tau=0.7$，因此我们对$\\lfloor 0.7 \\times 12 \\rfloor = \\lfloor 8.4 \\rfloor = 8$个最小的偏差求和。目标是找到最小化此修剪和的刺激$s$：\n$$\n\\mathcal{S}_{\\text{Trim}}(s; \\mathbf{k}) = \\sum_{j=1}^{\\lfloor \\tau N \\rfloor} D_{(j)}(s; \\mathbf{k})\n$$\n其中$D_{(j)}$是偏差集合$\\{D_{s,i}(\\mathbf{k})\\}_{i=1}^N$中第$j$小的值。解码出的刺激是$s_{\\text{Trim}} = \\arg\\min_{s} \\mathcal{S}_{\\text{Trim}}(s; \\mathbf{k})$。\n\n实现过程首先计算三个模板$\\boldsymbol{\\lambda}_0, \\boldsymbol{\\lambda}_1, \\boldsymbol{\\lambda}_2$。然后，对于每个测试用例向量$\\mathbf{k}^{(j)}$，为三种刺激中的每一种计算三个分数（$\\mathcal{S}_{\\text{MLE}}$, $\\mathcal{S}_{\\text{Huber}}$, $\\mathcal{S}_{\\text{Trim}}$）。选择优化相应分数的刺激索引作为该方法的解码刺激。在分数相等的情况下（容差为$\\epsilon = 10^{-12}$），应用选择最小刺激索引$s$的平局打破规则。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares three decoders (MLE, Huber, Trimmed) for a \n    neuroscience population coding problem.\n    \"\"\"\n\n    # 1. DEFINE PARAMETERS AND TEMPLATES\n    N = 12\n    S = 3\n    stimuli_indices = np.arange(S)\n    \n    b = np.array([5, 6, 4, 8, 2, 9, 3, 7, 5, 6, 4, 2], dtype=float)\n    f = np.array([1.0, 1.5, 0.6], dtype=float)\n    \n    # Stimulus templates (S x N matrix)\n    lambdas = np.outer(f, b)\n    \n    # Robustness parameters\n    delta = 2.0  # Huber threshold\n    tau = 0.7    # Trimming proportion\n    num_to_keep = int(np.floor(tau * N))\n    \n    # Tie-breaking tolerance\n    epsilon = 1e-12\n\n    test_cases = [\n        np.array([8, 9, 6, 12, 3, 14, 5, 11, 8, 9, 6, 3], dtype=float),\n        np.array([3, 4, 2, 5, 50, 5, 40, 4, 3, 60, 2, 1], dtype=float),\n        np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=float),\n        np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 0, 0], dtype=float),\n        np.array([5, 6, 4, 8, 2, 9, 3, 7, 5, 20, 15, 2], dtype=float),\n    ]\n\n    def argmin_tiebreak(scores):\n        min_score = np.min(scores)\n        tied_indices = np.where(scores  min_score + epsilon)[0]\n        return np.min(tied_indices)\n\n    def argmax_tiebreak(scores):\n        max_score = np.max(scores)\n        tied_indices = np.where(scores > max_score - epsilon)[0]\n        return np.min(tied_indices)\n\n    all_results = []\n    \n    for k in test_cases:\n        mle_scores = np.zeros(S)\n        huber_scores = np.zeros(S)\n        trim_scores = np.zeros(S)\n        \n        for s in stimuli_indices:\n            lambda_s = lambdas[s]\n            \n            # --- MLE Decoder Score (maximize) ---\n            # Score = sum(k_i * log(lambda_{s,i}) - lambda_{s,i})\n            mle_scores[s] = np.sum(k * np.log(lambda_s) - lambda_s)\n            \n            # --- Deviance Calculation (for Huber and Trimmed) ---\n            # Dev = 2 * (k*log(k/lambda) - (k-lambda))\n            # Handle k=0 case for the log term\n            log_term_dev = np.zeros_like(k)\n            nonzero_k_mask = k > 0\n            log_term_dev[nonzero_k_mask] = k[nonzero_k_mask] * np.log(k[nonzero_k_mask] / lambda_s[nonzero_k_mask])\n            deviances = 2 * (log_term_dev - (k - lambda_s))\n\n            # --- Huber Deviance Decoder Score (minimize) ---\n            residuals = np.sign(k - lambda_s) * np.sqrt(deviances)\n            abs_residuals = np.abs(residuals)\n            huber_losses = np.where(abs_residuals = delta,\n                                  0.5 * residuals**2,\n                                  delta * (abs_residuals - 0.5 * delta))\n            huber_scores[s] = np.sum(huber_losses)\n            \n            # --- Trimmed Deviance Decoder Score (minimize) ---\n            sorted_deviances = np.sort(deviances)\n            trim_scores[s] = np.sum(sorted_deviances[:num_to_keep])\n            \n        # 2. DECODE STIMULUS FOR EACH METHOD\n        s_mle = argmax_tiebreak(mle_scores)\n        s_huber = argmin_tiebreak(huber_scores)\n        s_trim = argmin_tiebreak(trim_scores)\n        \n        all_results.append([s_mle, s_huber, s_trim])\n\n    # 3. FORMAT AND PRINT THE FINAL OUTPUT\n    # Manual string construction to avoid spaces after commas.\n    result_str = \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}