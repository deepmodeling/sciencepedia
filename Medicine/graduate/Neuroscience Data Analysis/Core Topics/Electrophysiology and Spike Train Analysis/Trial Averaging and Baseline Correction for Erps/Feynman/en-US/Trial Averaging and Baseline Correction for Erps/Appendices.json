{
    "hands_on_practices": [
        {
            "introduction": "The validity of baseline correction hinges on the assumption that pre-stimulus activity provides an unbiased estimate of ongoing, non-evoked brain activity. This exercise challenges that assumption by exploring the impact of non-stationarity, specifically slow linear drifts that differ between experimental conditions. Through a mathematical derivation, you will uncover how such drifts can generate entirely spurious post-stimulus amplitude differences, providing a crucial, first-principles understanding of a common artifact in ERP analysis .",
            "id": "4173065",
            "problem": "Consider a two-condition experiment measuring Event-Related Potentials (ERPs), where each trial for condition $c \\in \\{A,B\\}$ yields a single-channel time series $x_{c,i}(t)$ defined on an epoch $t \\in [-T_0, T_1]$ with stimulus onset at $t = 0$. The ERP for condition $c$ is the across-trial average $\\bar{x}_c(t) = \\frac{1}{N_c} \\sum_{i=1}^{N_c} x_{c,i}(t)$, and the amplitude of a component at latency $\\tau$ is defined as $\\bar{x}_c(\\tau)$ after baseline correction. Baseline correction subtracts the mean in a pre-stimulus window $[-T_b,0)$ from every time point, applied trial-wise: $x'_{c,i}(t) = x_{c,i}(t) - \\frac{1}{T_b}\\int_{-T_b}^{0} x_{c,i}(u)\\,du$, and the baseline-corrected ERP is $\\bar{x}'_c(t) = \\frac{1}{N_c} \\sum_{i=1}^{N_c} x'_{c,i}(t)$. Assume a generative model in which the measured signal comprises an evoked response, a slow non-evoked baseline process, and noise: $x_{c,i}(t) = s_c(t) + d_{c,i}(t) + \\epsilon_{c,i}(t)$, where $s_c(t)$ is the condition-specific evoked response, $d_{c,i}(t)$ is a slow process that is present both before and after $t=0$, and $\\epsilon_{c,i}(t)$ is zero-mean noise independent of $s_c(t)$ and $d_{c,i}(t)$. Suppose that $d_{c,i}(t)$ is adequately approximated on the epoch by a deterministic linear trend $d_{c,i}(t) = \\alpha_c t$, with $\\alpha_c$ potentially differing by condition due to slow drifts or overlap from preceding processes, and that $s_A(t) = s_B(t)$ for all $t \\ge 0$. Under these assumptions and definitions, derive the expected difference $\\Delta(\\tau) = \\mathbb{E}[\\bar{x}'_A(\\tau) - \\bar{x}'_B(\\tau)]$ in terms of $\\alpha_A$, $\\alpha_B$, $T_b$, and $\\tau$, and use your derivation to evaluate how subtracting baselines with different means can induce spurious amplitude differences post-stimulus. Then, evaluate the following statements and select all that are correct about the existence of spurious differences and strategies to mitigate them:\n\nA. If the pre-stimulus drift is condition-specific and continues post-stimulus as $d_c(t) = \\alpha_c t$, baseline subtraction with window $[-T_b,0)$ yields an expected difference at latency $\\tau$ of $(\\alpha_A - \\alpha_B)\\,(\\tau + T_b/2)$ even when the true evoked responses are identical.\n\nB. Using condition-specific baseline subtraction always guarantees unbiased post-stimulus amplitude differences regardless of nonstationary baseline dynamics.\n\nC. Matching baseline distributions across conditions by reweighting trials so that the empirical baseline means are equal removes the constant-offset contribution to the bias but not the time-varying component induced by different slopes.\n\nD. Including the baseline mean as a covariate in a trial-level General Linear Model (GLM), estimating the evoked amplitude as the coefficient of a post-stimulus regressor while controlling for the pre-stimulus baseline, can remove both constant and linear drift-induced bias if the model includes appropriate basis functions (e.g., a linear term in time).\n\nE. High-pass filtering at $2\\,\\mathrm{Hz}$ is always a safe baseline strategy for ERPs because it never distorts component amplitudes.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective. It presents a simplified but standard generative model for Event-Related Potential (ERP) data to explore the artifacts introduced by baseline correction in the presence of condition-specific linear drift. All terms are defined, and the assumptions are explicit, allowing for a unique and meaningful solution. I will therefore proceed with the derivation and evaluation.\n\nThe first step is to derive the expected difference in the baseline-corrected ERP amplitudes between the two conditions, $\\Delta(\\tau) = \\mathbb{E}[\\bar{x}'_A(\\tau) - \\bar{x}'_B(\\tau)]$.\n\nThe baseline-corrected signal for a single trial $i$ in condition $c$ is given by:\n$$x'_{c,i}(t) = x_{c,i}(t) - B_{c,i}$$\nwhere $B_{c,i}$ is the mean of the signal in the pre-stimulus baseline interval $[-T_b, 0)$:\n$$B_{c,i} = \\frac{1}{T_b}\\int_{-T_b}^{0} x_{c,i}(u)\\,du$$\nThe generative model for the observed signal is:\n$$x_{c,i}(t) = s_c(t) + d_{c,i}(t) + \\epsilon_{c,i}(t)$$\nwhere $s_c(t)$ is the evoked signal, $d_{c,i}(t)$ is a slow drift, and $\\epsilon_{c,i}(t)$ is zero-mean noise.\nWe are given the specific form for the drift, $d_{c,i}(t) = \\alpha_c t$. It is standard to assume that the evoked response $s_c(t)$ is zero for pre-stimulus times, i.e., $s_c(t) = 0$ for $t < 0$.\n\nSubstituting the model into the expression for the baseline mean $B_{c,i}$:\n$$B_{c,i} = \\frac{1}{T_b}\\int_{-T_b}^{0} [s_c(u) + \\alpha_c u + \\epsilon_{c,i}(u)]\\,du$$\nGiven $s_c(u) = 0$ for $u<0$, this simplifies to:\n$$B_{c,i} = \\frac{1}{T_b}\\int_{-T_b}^{0} [\\alpha_c u + \\epsilon_{c,i}(u)]\\,du = \\frac{\\alpha_c}{T_b}\\int_{-T_b}^{0} u\\,du + \\frac{1}{T_b}\\int_{-T_b}^{0} \\epsilon_{c,i}(u)\\,du$$\nThe integral of the linear term is $\\int_{-T_b}^{0} u\\,du = \\left[\\frac{u^2}{2}\\right]_{-T_b}^{0} = 0 - \\frac{(-T_b)^2}{2} = -\\frac{T_b^2}{2}$.\nSo, the baseline mean for a single trial is:\n$$B_{c,i} = \\frac{\\alpha_c}{T_b} \\left(-\\frac{T_b^2}{2}\\right) + \\frac{1}{T_b}\\int_{-T_b}^{0} \\epsilon_{c,i}(u)\\,du = -\\frac{\\alpha_c T_b}{2} + \\frac{1}{T_b}\\int_{-T_b}^{0} \\epsilon_{c,i}(u)\\,du$$\nNow, we find the baseline-corrected signal at a post-stimulus latency $\\tau \\ge 0$:\n$$x'_{c,i}(\\tau) = x_{c,i}(\\tau) - B_{c,i} = [s_c(\\tau) + \\alpha_c \\tau + \\epsilon_{c,i}(\\tau)] - \\left[-\\frac{\\alpha_c T_b}{2} + \\frac{1}{T_b}\\int_{-T_b}^{0} \\epsilon_{c,i}(u)\\,du\\right]$$\n$$x'_{c,i}(\\tau) = s_c(\\tau) + \\alpha_c \\tau + \\frac{\\alpha_c T_b}{2} + \\epsilon_{c,i}(\\tau) - \\frac{1}{T_b}\\int_{-T_b}^{0} \\epsilon_{c,i}(u)\\,du$$\nThe condition-specific ERP $\\bar{x}'_c(\\tau)$ is the average of $x'_{c,i}(\\tau)$ across $N_c$ trials. Taking the expectation $\\mathbb{E}[...]$ over all possible noise realizations, and noting that $\\mathbb{E}[\\epsilon_{c,i}(t)] = 0$ for all $t$, we have:\n$$\\mathbb{E}[\\bar{x}'_c(\\tau)] = \\mathbb{E}\\left[\\frac{1}{N_c}\\sum_{i=1}^{N_c} x'_{c,i}(\\tau)\\right] = s_c(\\tau) + \\alpha_c \\tau + \\frac{\\alpha_c T_b}{2} = s_c(\\tau) + \\alpha_c\\left(\\tau + \\frac{T_b}{2}\\right)$$\nThe expected difference between conditions $A$ and $B$ is:\n$$\\Delta(\\tau) = \\mathbb{E}[\\bar{x}'_A(\\tau)] - \\mathbb{E}[\\bar{x}'_B(\\tau)] = \\left[s_A(\\tau) + \\alpha_A\\left(\\tau + \\frac{T_b}{2}\\right)\\right] - \\left[s_B(\\tau) + \\alpha_B\\left(\\tau + \\frac{T_b}{2}\\right)\\right]$$\n$$\\Delta(\\tau) = (s_A(\\tau) - s_B(\\tau)) + (\\alpha_A - \\alpha_B)\\left(\\tau + \\frac{T_b}{2}\\right)$$\nThe problem states that the true evoked responses are identical post-stimulus, $s_A(t) = s_B(t)$ for $t \\ge 0$. Therefore, for $\\tau \\ge 0$, the first term is zero. The resulting expected difference is purely a spurious artifact of the interaction between baseline correction and differential drift:\n$$\\Delta(\\tau) = (\\alpha_A - \\alpha_B)\\left(\\tau + \\frac{T_b}{2}\\right)$$\nThis derivation shows that a difference in pre-stimulus drift slopes $(\\alpha_A \\neq \\alpha_B)$ induces a spurious post-stimulus amplitude difference that is not constant but changes linearly with latency $\\tau$.\n\nNow, I will evaluate each statement.\n\nA. If the pre-stimulus drift is condition-specific and continues post-stimulus as $d_c(t) = \\alpha_c t$, baseline subtraction with window $[-T_b,0)$ yields an expected difference at latency $\\tau$ of $(\\alpha_A - \\alpha_B)\\,(\\tau + T_b/2)$ even when the true evoked responses are identical.\nThis statement is a direct and precise match to the result derived above. The formula is identical to the spurious expected difference $\\Delta(\\tau)$ we calculated under the given assumptions.\nVerdict: **Correct**.\n\nB. Using condition-specific baseline subtraction always guarantees unbiased post-stimulus amplitude differences regardless of nonstationary baseline dynamics.\nThis statement is false. My derivation provides a direct counterexample. For the case of linear nonstationary dynamics ($d_c(t)=\\alpha_c t$), applying condition-specific baseline subtraction results in a biased estimate of the amplitude difference, $\\Delta(\\tau) = (\\alpha_A - \\alpha_B)(\\tau + T_b/2)$, which is non-zero if $\\alpha_A \\neq \\alpha_B$. The term \"always guarantees\" is far too strong, as this common form of baseline correction is well-known to be sensitive to various forms of nonstationarity.\nVerdict: **Incorrect**.\n\nC. Matching baseline distributions across conditions by reweighting trials so that the empirical baseline means are equal removes the constant-offset contribution to the bias but not the time-varying component induced by different slopes.\nThe derived bias is $\\Delta(\\tau) = (\\alpha_A - \\alpha_B)\\tau + (\\alpha_A - \\alpha_B)\\frac{T_b}{2}$. This has two components: a time-varying component $(\\alpha_A - \\alpha_B)\\tau$ and a constant-offset component $(\\alpha_A - \\alpha_B)\\frac{T_b}{2}$. The expected baseline mean for condition $c$ is $\\mathbb{E}[B_c] = -\\frac{\\alpha_c T_b}{2}$. The difference in expected baseline means is thus $\\mathbb{E}[B_A] - \\mathbb{E}[B_B] = -(\\alpha_A - \\alpha_B)\\frac{T_b}{2}$. The constant-offset bias is the negative of this difference. A procedure that matches the empirical baseline means (e.g., by subsampling or reweighting trials) forces the average baseline levels to be equal across conditions, thereby removing the constant-offset contribution to the bias. However, this procedure does not alter the underlying signal dynamics. The time-varying component $(\\alpha_A - \\alpha_B)\\tau$ arises from the fact that the actual signal at time $\\tau$ contains the term $\\alpha_c \\tau$, and this difference in ongoing drift is not corrected by adjusting the pre-stimulus mean. Thus, the statement accurately describes the effect of this corrective strategy.\nVerdict: **Correct**.\n\nD. Including the baseline mean as a covariate in a trial-level General Linear Model (GLM), estimating the evoked amplitude as the coefficient of a post-stimulus regressor while controlling for the pre-stimulus baseline, can remove both constant and linear drift-induced bias if the model includes appropriate basis functions (e.g., a linear term in time).\nThis statement describes a sophisticated and valid approach to mitigate such artifacts. A GLM can model the contribution of multiple factors to the observed data. In a trial-level model of the data point $x_{c,i}(t)$, one could include regressors for the experimental condition, the pre-stimulus baseline mean of that trial, time $t$ itself, and crucially, the interaction between time and condition. A model of the form $x_i(t) \\sim \\text{Condition} + B_i + t + \\text{Condition} \\times t$ explicitly models and estimates the condition-specific linear drift via the main effect of $t$ and the interaction term. By accounting for this differential drift, the model can provide an unbiased estimate of the condition effect on the evoked response. This method removes the root cause of the bias (the differential slopes), thereby correcting for both the time-varying and constant-offset components of the artifact.\nVerdict: **Correct**.\n\nE. High-pass filtering at $2\\,\\mathrm{Hz}$ is always a safe baseline strategy for ERPs because it never distorts component amplitudes.\nThis statement is categorically false. High-pass filtering is a linear time-invariant operation that removes or attenuates frequency components below its cutoff frequency. While it can be effective at removing slow drifts (which are low-frequency phenomena), it will also distort any part of the signal of interest that has energy in the stopband. Many important ERP components, such as the P300, slow cortical potentials, and readiness potentials, are characterized by slow voltage changes and have substantial energy below $2\\,\\mathrm{Hz}$. Applying a $2\\,\\mathrm{Hz}$ high-pass filter would severely attenuate and distort these components, potentially leading to incorrect conclusions about their amplitude and latency. No filtering strategy is \"always safe\" or \"never distorts\"; the choice of filter parameters must be carefully considered based on the specific ERP components of interest and the nature of the noise.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "Having established the theoretical danger of baseline differences, a prudent analyst must test for them empirically. This hands-on practice transitions from theory to a core data quality control procedure. You will write a program to generate synthetic ERP data and then apply the appropriate statistical tool—Welch's $t$-test—to formally assess whether baseline activity differs significantly across conditions, a fundamental step in validating any subsequent comparison of ERP components .",
            "id": "4202094",
            "problem": "You are given a formal task in Event-Related Potential (ERP) analysis for Electroencephalography (EEG) that focuses on trial averaging and baseline correction, framed purely in mathematical and statistical terms. The goal is to compare the baseline across experimental conditions and electrodes, using first principles about statistical estimation and hypothesis testing. You must implement a complete program that generates synthetic, scientifically plausible ERP-like data, applies baseline computations and corrections, and then performs comparative baseline analysis across conditions and electrodes.\n\nUse the following foundational base. An ERP is a stimulus-locked signal defined as an expected value across repeated trials. Let $x_{c,e,i}(t)$ denote the amplitude in microvolts for condition $c$, electrode $e$, trial $i$, at relative time $t$ in milliseconds, where $t = 0$ denotes stimulus onset. The baseline is defined on a pre-stimulus time window $B = [t_1, t_2]$ with $t_1 < 0$ and $t_2 \\le 0$. The per-trial baseline mean is\n$$\nb_{c,e,i} = \\frac{1}{|B|} \\sum_{t \\in B} x_{c,e,i}(t).\n$$\nBaseline correction of each trial is the transformation\n$$\n\\tilde{x}_{c,e,i}(t) = x_{c,e,i}(t) - b_{c,e,i}.\n$$\nThe trial-averaged ERP for a condition and electrode is defined as the arithmetic mean across trials:\n$$\n\\mathrm{ERP}_{c,e}(t) = \\frac{1}{n_c} \\sum_{i=1}^{n_c} \\tilde{x}_{c,e,i}(t),\n$$\nwhere $n_c$ is the number of trials in condition $c$. To assess whether baselines differ across conditions for each electrode, model per-trial baseline means as independent samples and compare the two condition-specific baseline distributions using a two-sided Welch’s $t$-test at significance level $\\alpha = 0.05$, relying on well-tested properties of the sample mean as an unbiased estimator and on large-sample approximations for Gaussian noise.\n\nYour program must generate ERP-like synthetic data for each test case as follows. For each trial, electrode, and condition, construct $x_{c,e,i}(t)$ by summing three components: a condition-specific constant baseline offset $\\Delta_{c,e}$ in microvolts applied over the entire time series, a common stimulus-locked ERP “bump” modeled as a Gaussian pulse of amplitude $A$ microvolts centered at time $\\mu$ milliseconds with width parameter $\\sigma$ milliseconds, and zero-mean Gaussian noise with standard deviation $\\eta$ microvolts independently sampled at each time point. All time points and windows are defined in milliseconds and sampled at integer millisecond resolution according to the sampling rate $f_s$ in Hertz. Amplitudes must be treated and interpreted in microvolts, and time must be treated and interpreted in milliseconds. Angles are not used.\n\nFor each test case, perform the following computations:\n- Compute $b_{c,e,i}$ for all trials and electrodes using the specified baseline window.\n- Apply baseline correction to obtain $\\tilde{x}_{c,e,i}(t)$ and form $\\mathrm{ERP}_{c,e}(t)$ via trial averaging.\n- For each electrode, perform a two-sided Welch’s $t$-test comparing $\\{b_{A,e,i}\\}_{i=1}^{n_A}$ versus $\\{b_{B,e,j}\\}_{j=1}^{n_B}$ at $\\alpha = 0.05$.\n- Count the number of electrodes for which the baseline difference is statistically significant, that is, where the $p$-value is strictly less than $0.05$.\n\nThe test suite consists of three parameter sets designed for coverage and scientific realism. In all cases, the ERP component is identical across conditions to ensure the baseline is the primary differentiator, and random generation must be deterministic per test case via fixed seeds to make the output reproducible.\n\nTest Case $1$ (general “happy path” with multiple electrodes and balanced trials):\n- Sampling rate $f_s = 1000$ Hertz.\n- Time axis from $t_{\\min} = -200$ milliseconds to $t_{\\max} = 600$ milliseconds inclusive.\n- Baseline window $B = [-200, 0]$ milliseconds inclusive.\n- Number of trials $n_A = 120$ and $n_B = 120$.\n- Number of electrodes $E = 3$ with indices $e \\in \\{0,1,2\\}$.\n- Baseline offsets in microvolts: $\\Delta_{A,e} = 0$ for all $e$, and $\\Delta_{B,0} = 0$, $\\Delta_{B,1} = 0.8$, $\\Delta_{B,2} = 1.2$.\n- ERP Gaussian pulse parameters: amplitude $A = 5$, center $\\mu = 300$, width $\\sigma = 50$.\n- Noise standard deviation $\\eta = 0.5$ microvolts.\n- Deterministic random seed $s = 42$.\n\nTest Case $2$ (boundary-like case with a short baseline window and a single electrode):\n- Sampling rate $f_s = 500$ Hertz.\n- Time axis from $t_{\\min} = -20$ milliseconds to $t_{\\max} = 200$ milliseconds inclusive.\n- Baseline window $B = [-20, 0]$ milliseconds inclusive.\n- Number of trials $n_A = 60$ and $n_B = 60$.\n- Number of electrodes $E = 1$ with index $e = 0$.\n- Baseline offsets in microvolts: $\\Delta_{A,0} = 0$ and $\\Delta_{B,0} = 0$.\n- ERP Gaussian pulse parameters: amplitude $A = 3$, center $\\mu = 100$, width $\\sigma = 30$.\n- Noise standard deviation $\\eta = 0.6$ microvolts.\n- Deterministic random seed $s = 43$.\n\nTest Case $3$ (edge case with unequal trials and a strong baseline difference for one electrode):\n- Sampling rate $f_s = 1000$ Hertz.\n- Time axis from $t_{\\min} = -100$ milliseconds to $t_{\\max} = 500$ milliseconds inclusive.\n- Baseline window $B = [-100, 0]$ milliseconds inclusive.\n- Number of trials $n_A = 100$ and $n_B = 70$.\n- Number of electrodes $E = 3$ with indices $e \\in \\{0,1,2\\}$.\n- Baseline offsets in microvolts: $\\Delta_{A,e} = 0$ for all $e$, and $\\Delta_{B,0} = 2.0$, $\\Delta_{B,1} = 0$, $\\Delta_{B,2} = 0$.\n- ERP Gaussian pulse parameters: amplitude $A = 4$, center $\\mu = 200$, width $\\sigma = 40$.\n- Noise standard deviation $\\eta = 0.7$ microvolts.\n- Deterministic random seed $s = 44$.\n\nYour program must output a single line containing the integer counts of significant electrodes for Test Cases $1$, $2$, and $3$, respectively, as a comma-separated list enclosed in square brackets with no spaces, for example, `[x_1,x_2,x_3]`. There are no units associated with this output because it is a count. All intermediate quantities in the computations must properly use microvolts for amplitudes and milliseconds for time as specified in each test case. The hypothesis tests must be two-sided and use $\\alpha = 0.05$.",
            "solution": "The user-provided problem is valid. It is scientifically grounded in the principles of neurophysiological signal processing and statistical inference, specifically concerning Event-Related Potentials (ERPs). The problem is well-posed, with all parameters, models, and procedures explicitly and unambiguously defined, ensuring a unique and reproducible solution. It is objective and free of non-formalizable or subjective elements. Therefore, a solution will be provided.\n\nThe problem requires the implementation of a computational model to analyze synthetic ERP data. The core tasks are to generate the data according to a specified model, compute per-trial baselines, and then perform a statistical comparison of these baselines between two experimental conditions for a set of electrodes.\n\nThe solution proceeds in four main stages: (1) generation of synthetic ERP data, (2) calculation of per-trial baseline means, (3) statistical hypothesis testing on these means, and (4) aggregation of results across specified test cases.\n\n**1. Synthetic Data Generation**\nThe voltage signal for a given condition $c \\in \\{A, B\\}$, electrode $e$, and trial $i$ at time $t$ is modeled as:\n$$\nx_{c,e,i}(t) = \\Delta_{c,e} + G(t; A, \\mu, \\sigma) + \\epsilon_{c,e,i}(t)\n$$\nwhere:\n- $\\Delta_{c,e}$ is a constant baseline offset in microvolts ($\\mu$V) specific to the condition and electrode. This term represents the underlying difference in brain activity we aim to detect.\n- $G(t; A, \\mu, \\sigma)$ is a deterministic, stimulus-locked ERP component common to all trials, modeled as a Gaussian pulse:\n$$\nG(t; A, \\mu, \\sigma) = A \\exp\\left(-\\frac{(t - \\mu)^2}{2\\sigma^2}\\right)\n$$\nHere, $A$ is the amplitude ($\\mu$V), $\\mu$ is the temporal center (ms), and $\\sigma$ is the width (ms) of the pulse.\n- $\\epsilon_{c,e,i}(t)$ is a random noise term, modeled as an independent sample from a zero-mean Gaussian distribution with standard deviation $\\eta$, i.e., $\\epsilon_{c,e,i}(t) \\sim \\mathcal{N}(0, \\eta^2)$.\n\nFor each test case, we first establish a discrete time vector based on the specified sampling rate $f_s$ (in Hz) and time range $[t_{\\min}, t_{\\max}]$ (in ms). The time step is $\\Delta t = 1000/f_s$ ms. The time vector consists of points $t_k = t_{\\min} + k \\cdot \\Delta t$ up to $t_{\\max}$. For each condition, electrode, and trial, a time series is constructed by summing the constant offset, the common Gaussian pulse evaluated at each time point, and a unique realization of the Gaussian noise. The use of a fixed seed for the random number generator ensures reproducibility.\n\n**2. Per-Trial Baseline Calculation**\nThe baseline period is defined as a pre-stimulus time window $B = [t_1, t_2]$, where $t_2 \\le 0$. The per-trial baseline mean, $b_{c,e,i}$, is the sample mean of the signal $x_{c,e,i}(t)$ over the discrete time points falling within this window. It is an estimator for the true mean amplitude during the baseline period.\n$$\nb_{c,e,i} = \\frac{1}{N_B} \\sum_{k \\text{ s.t. } t_k \\in B} x_{c,e,i}(t_k)\n$$\nwhere $N_B$ is the number of time samples in the window $B$. Since the ERP component $G(t)$ is designed to be negligible during the baseline period and the noise $\\epsilon(t)$ has zero mean, the expected value of the baseline mean is approximately the true offset: $\\mathbb{E}[b_{c,e,i}] \\approx \\Delta_{c,e}$.\n\nThe problem also specifies computing the baseline-corrected signal $\\tilde{x}_{c,e,i}(t) = x_{c,e,i}(t) - b_{c,e,i}$ and the trial-averaged $\\mathrm{ERP}_{c,e}(t) = \\frac{1}{n_c} \\sum_{i=1}^{n_c} \\tilde{x}_{c,e,i}(t)$. While these are standard steps in ERP analysis, the core task of comparing baselines relies solely on the $b_{c,e,i}$ values. These quantities are computed as part of the specified procedure.\n\n**3. Statistical Hypothesis Testing**\nTo determine if the baseline activity differs significantly between conditions $A$ and $B_j$ for a given electrode $e$, we compare the distributions of their respective per-trial baseline means, $\\{b_{A,e,i}\\}_{i=1}^{n_A}$ and $\\{b_{B,e,j}\\}_{j=1}^{n_B}$. The null hypothesis ($H_0$) is that the true population means of the baselines are equal ($\\mathbb{E}[b_{A,e}] = \\mathbb{E}[b_{B,e}]$), implying no difference in baseline offset ($\\Delta_{A,e} = \\Delta_{B,e}$). The alternative hypothesis ($H_A$) is that they are not equal.\n\nA two-sided Welch's $t$-test is employed for this comparison. This test is appropriate as it does not assume equal variances or equal sample sizes ($n_A$ vs. $n_B$) between the two groups, making it more robust than the standard Student's $t$-test. The test yields a $p$-value, which represents the probability of observing a difference in sample means as large as or larger than the one computed, assuming the null hypothesis is true. If this $p$-value is less than the pre-defined significance level $\\alpha = 0.05$, we reject the null hypothesis and conclude that there is a statistically significant difference in baselines for that electrode.\n\n**4. Algorithmic Implementation and Aggregation**\nThe overall algorithm is as follows:\n1. Initialize a list to store the results for each test case.\n2. For each of the three test cases defined in the problem:\n    a. Configure all parameters: $f_s, t_{\\min}, t_{\\max}, B, n_A, n_B, E, \\{\\Delta_{c,e}\\}, A, \\mu, \\sigma, \\eta$, and the random seed $s$.\n    b. Create a reproducible random number generator using the specified seed.\n    c. Construct the discrete time vector.\n    d. Generate the common Gaussian ERP pulse.\n    e. For each condition ($A$ and $B$):\n        i. Generate the full data tensor $x_{c,e,i}(t)$ by combining the offsets, ERP pulse, and noise via broadcasting.\n        ii. Identify the time indices corresponding to the baseline window $B$.\n        iii. Calculate the per-trial baseline means $b_{c,e,i}$ by averaging the data over the baseline indices, resulting in a matrix of shape (electrodes, trials).\n    f. Initialize a counter for significant electrodes to $0$.\n    g. For each electrode index $e$ from $0$ to $E-1$:\n        i. Extract the two samples of baseline means: $\\{b_{A,e,i}\\}$ and $\\{b_{B,e,j}\\}$.\n        ii. Perform the two-sided Welch's $t$-test on these two samples.\n        iii. If the resulting $p$-value is strictly less than $0.05$, increment the counter.\n    h. Append the final count to the results list.\n3. Format the results list into the specified output string `\"[count1,count2,count3]\"`.\n\nThis procedure is implemented in the provided Python code, utilizing `numpy` for efficient array computation and `scipy.stats.ttest_ind` for the statistical test.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef run_analysis(params: dict) -> int:\n    \"\"\"\n    Generates synthetic ERP data and performs baseline comparison for a single test case.\n\n    Args:\n        params: A dictionary containing all parameters for the test case.\n\n    Returns:\n        The number of electrodes with a statistically significant baseline difference.\n    \"\"\"\n    # Unpack parameters from the dictionary\n    f_s = params[\"f_s\"]\n    t_min, t_max = params[\"t_range\"]\n    b_win_start, b_win_end = params[\"b_win\"]\n    n_trials = params[\"n_trials\"]\n    E = params[\"E\"]\n    deltas = params[\"deltas\"]\n    erp_params = params[\"erp_params\"]\n    eta = params[\"eta\"]\n    seed = params[\"seed\"]\n\n    # 1. Setup time vector and reproducible random number generator\n    rng = np.random.default_rng(seed)\n    sampling_interval = 1000.0 / f_s\n    # Use np.arange with a small tolerance for the endpoint to be inclusive\n    time_vector = np.arange(t_min, t_max + 1e-9, sampling_interval)\n\n    # 2. Generate common ERP component (Gaussian pulse)\n    A = erp_params['A']\n    mu = erp_params['mu']\n    sigma = erp_params['sigma']\n    erp_bump = A * np.exp(-((time_vector - mu)**2) / (2 * sigma**2))\n\n    baseline_means = {}\n    conditions = ['A', 'B']\n\n    for cond in conditions:\n        n_c = n_trials[cond]\n        delta_c = deltas[cond]\n\n        # 3. Generate synthetic data x_c,e,i(t)\n        # Noise tensor: (electrodes, trials, time points)\n        noise = rng.normal(loc=0.0, scale=eta, size=(E, n_c, len(time_vector)))\n        \n        # Combine components using broadcasting:\n        # delta_c: (E,) -> (E, 1, 1)\n        # erp_bump: (time,) -> (1, 1, time)\n        # noise: (E, n_c, time)\n        x_c = delta_c[:, np.newaxis, np.newaxis] + erp_bump[np.newaxis, np.newaxis, :] + noise\n\n        # 4. Compute per-trial baseline means b_c,e,i\n        baseline_mask = (time_vector >= b_win_start) & (time_vector <= b_win_end)\n        b_c = np.mean(x_c[:, :, baseline_mask], axis=2)\n        baseline_means[cond] = b_c  # Shape: (E, n_c)\n\n        # As required, perform baseline correction and trial averaging, though not used in the final result\n        # _tilde_x_c = x_c - b_c[:, :, np.newaxis]\n        # _erp_c = np.mean(_tilde_x_c, axis=1)\n\n    # 5. Perform Welch's t-test for each electrode\n    significant_electrodes_count = 0\n    alpha = 0.05\n    baselines_A = baseline_means['A']\n    baselines_B = baseline_means['B']\n\n    for e in range(E):\n        sample_A = baselines_A[e, :]\n        sample_B = baselines_B[e, :]\n        \n        # Perform two-sided Welch's t-test\n        _t_stat, p_val = stats.ttest_ind(sample_A, sample_B, equal_var=False, alternative='two-sided')\n        \n        if p_val < alpha:\n            significant_electrodes_count += 1\n            \n    return significant_electrodes_count\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"f_s\": 1000, \"t_range\": (-200, 600), \"b_win\": (-200, 0),\n            \"n_trials\": {'A': 120, 'B': 120}, \"E\": 3,\n            \"deltas\": {'A': np.array([0.0, 0.0, 0.0]), 'B': np.array([0.0, 0.8, 1.2])},\n            \"erp_params\": {'A': 5, 'mu': 300, 'sigma': 50},\n            \"eta\": 0.5, \"seed\": 42\n        },\n        # Test Case 2\n        {\n            \"f_s\": 500, \"t_range\": (-20, 200), \"b_win\": (-20, 0),\n            \"n_trials\": {'A': 60, 'B': 60}, \"E\": 1,\n            \"deltas\": {'A': np.array([0.0]), 'B': np.array([0.0])},\n            \"erp_params\": {'A': 3, 'mu': 100, 'sigma': 30},\n            \"eta\": 0.6, \"seed\": 43\n        },\n        # Test Case 3\n        {\n            \"f_s\": 1000, \"t_range\": (-100, 500), \"b_win\": (-100, 0),\n            \"n_trials\": {'A': 100, 'B': 70}, \"E\": 3,\n            \"deltas\": {'A': np.array([0.0, 0.0, 0.0]), 'B': np.array([2.0, 0.0, 0.0])},\n            \"erp_params\": {'A': 4, 'mu': 200, 'sigma': 40},\n            \"eta\": 0.7, \"seed\": 44\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_analysis(params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "When slow drifts contaminate EEG data, a standard fixed-window baseline may not be sufficient. This exercise introduces an advanced, algorithmic solution: adaptive baseline correction. You will implement a procedure that, on a trial-by-trial basis, intelligently searches the pre-stimulus interval for the most stable segment to use for correction. This practice demonstrates how a sophisticated, data-driven approach can more effectively mitigate drift artifacts and improve the fidelity of the final ERP estimate .",
            "id": "4202160",
            "problem": "You are given a formal model for single-trial electroencephalography (EEG) epochs that contain an Event-Related Potential (ERP), which is the trial-locked, deterministic component of the averaged brain response. Let the sampled time variable be $t \\in \\mathbb{R}$ with stimulus onset at $t = 0$. Each observed trial $i \\in \\{1,\\dots,N\\}$ is modeled as\n$$\nx_i(t) = s(t) + d_i(t) + b_i + \\varepsilon_i(t),\n$$\nwhere $s(t)$ is the unknown deterministic ERP waveform shared across all trials, $d_i(t)$ is a low-frequency drift that may vary across trials, $b_i$ is a trial-specific constant offset, and $\\varepsilon_i(t)$ is zero-mean additive noise. Assume the sampling is uniform at sampling frequency $f_s$ (in $\\mathrm{Hz}$), so the discrete time grid is $t_k = k/f_s + t_0$ for integer $k$, where $t_0$ is chosen to align $t=0$ with stimulus onset. For each trial, the epoch spans a pre-stimulus window $[-T_{\\mathrm{pre}},0)$ and a post-stimulus window $[0,T_{\\mathrm{post}}]$, with $T_{\\mathrm{pre}}, T_{\\mathrm{post}} > 0$ in $\\mathrm{s}$. For baseline correction, one typically subtracts the mean of a pre-stimulus baseline window from the entire trial, to remove $b_i$ and mitigate $d_i(t)$ effects. In this problem, you will implement and evaluate an adaptive baseline window selection procedure that chooses, for each trial, the baseline window within the pre-stimulus interval that exhibits minimal absolute slope (as a proxy for minimal drift contamination).\n\nFoundational assumptions and definitions:\n- The ERP waveform is defined as the expectation $s(t) = \\mathbb{E}[x_i(t)]$ over trials, under the additive model above and stationarity around the stimulus onset.\n- The low-frequency drift within each trial is well approximated by a slowly varying function; in this problem, the synthetic drift will be a sinusoid $d_i(t) = A_{\\mathrm{drift}}\\sin(2\\pi f_{\\mathrm{drift}} t + \\phi_i)$ with trial-specific phase $\\phi_i$.\n- A fixed-window baseline correction uses a contiguous pre-stimulus interval $[t_a,t_b) \\subset [-T_{\\mathrm{pre}},0)$ to compute $\\mu_i = \\frac{1}{M}\\sum_{k \\in \\mathcal{B}} x_i(t_k)$, where $\\mathcal{B} = \\{k: t_k \\in [t_a,t_b)\\}$ and $M = |\\mathcal{B}|$, and subtracts $\\mu_i$ from $x_i(t_k)$ for all $k$.\n- An adaptive baseline correction selects $[t_a,t_b)$ of prescribed duration $T_w$ by scanning candidate windows within $[-T_{\\mathrm{pre}},0)$ and choosing the one with minimal absolute slope magnitude. For a candidate baseline window $\\mathcal{B}$, define the least-squares slope estimator\n$$\n\\hat{m}_i(\\mathcal{B}) = \\frac{\\sum_{k \\in \\mathcal{B}} \\left(t_k - \\bar{t}_{\\mathcal{B}}\\right)\\left(x_i(t_k) - \\bar{x}_{i,\\mathcal{B}}\\right)}{\\sum_{k \\in \\mathcal{B}} \\left(t_k - \\bar{t}_{\\mathcal{B}}\\right)^2},\n$$\nwhere $\\bar{t}_{\\mathcal{B}} = \\frac{1}{M}\\sum_{k \\in \\mathcal{B}} t_k$ and $\\bar{x}_{i,\\mathcal{B}} = \\frac{1}{M}\\sum_{k \\in \\mathcal{B}} x_i(t_k)$. The adaptive window $\\mathcal{B}^\\star$ minimizes $|\\hat{m}_i(\\mathcal{B})|$ over all candidates of duration $T_w$.\n- After trial-wise baseline correction, the ERP estimate is the across-trial average $\\hat{s}(t_k) = \\frac{1}{N}\\sum_{i=1}^N \\left(x_i(t_k) - \\mu_i^\\star\\right)$, where $\\mu_i^\\star$ is the mean over the selected baseline window for trial $i$.\n- To quantify performance, compute the Pearson correlation coefficient between the estimated ERP and the true $s(t)$ over an evaluation interval $[0, T_{\\mathrm{eval}}]$, where $T_{\\mathrm{eval}} = \\min(T_{\\mathrm{post}}, T^\\star)$ with $T^\\star$ specified for the test. Let the discrete evaluation index set be $\\mathcal{E}=\\{k: t_k \\in [0,T_{\\mathrm{eval}}]\\}$ and define\n$$\nr(\\hat{s}, s) = \\frac{\\sum_{k \\in \\mathcal{E}} \\left(\\hat{s}(t_k) - \\overline{\\hat{s}}\\right)\\left(s(t_k) - \\bar{s}\\right)}{\\sqrt{\\sum_{k \\in \\mathcal{E}} \\left(\\hat{s}(t_k) - \\overline{\\hat{s}}\\right)^2}\\sqrt{\\sum_{k \\in \\mathcal{E}} \\left(s(t_k) - \\bar{s}\\right)^2}},\n$$\nwith $\\overline{\\hat{s}} = \\frac{1}{|\\mathcal{E}|}\\sum_{k \\in \\mathcal{E}} \\hat{s}(t_k)$ and $\\bar{s} = \\frac{1}{|\\mathcal{E}|}\\sum_{k \\in \\mathcal{E}} s(t_k)$.\n\nSynthetic data generation for evaluation:\n- Use $s(t)$ as a sum of three Gaussian deflections to emulate canonical ERP components,\n$$\ns(t) = \\sum_{\\ell=1}^{3} A_\\ell \\exp\\!\\left(-\\frac{(t-\\mu_\\ell)^2}{2\\sigma_\\ell^2}\\right),\n$$\nwith $(A_1,\\mu_1,\\sigma_1) = (5.0, 0.100, 0.030)$, $(A_2,\\mu_2,\\sigma_2) = (-3.0, 0.200, 0.040)$, $(A_3,\\mu_3,\\sigma_3) = (3.0, 0.350, 0.060)$, where amplitudes are in arbitrary but consistent units and times in $\\mathrm{s}$.\n- For each trial $i$, draw $\\phi_i$ uniformly on $[0,2\\pi)$, draw $b_i \\sim \\mathcal{N}(0,\\sigma_b^2)$, and draw $\\varepsilon_i(t_k) \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$ independently across $k$ and $i$.\n- The drift is $d_i(t) = A_{\\mathrm{drift}}\\sin(2\\pi f_{\\mathrm{drift}} t + \\phi_i)$.\n\nAdaptive baseline selection:\n- Let the baseline duration be $T_w > 0$, and define a candidate start-grid with step $\\Delta > 0$ so that windows $[t_a,t_b)$ satisfy $t_b - t_a = T_w$ and $[t_a,t_b) \\subset [-T_{\\mathrm{pre}},0)$. Among all such discrete candidates, select the $\\mathcal{B}^\\star$ that minimizes $|\\hat{m}_i(\\mathcal{B})|$, then set $\\mu_i^\\star = \\frac{1}{M}\\sum_{k \\in \\mathcal{B}^\\star} x_i(t_k)$. For the fixed baseline, use the last $T_w$ seconds before $t=0$, i.e., $[t_a,t_b) = [-T_w,0)$, and set $\\mu_i^{\\mathrm{fix}}$ accordingly.\n\nTask:\n- Implement the synthetic data generator, the fixed-window baseline correction, the adaptive baseline correction as specified, and compute the correlation $r$ between the averaged ERP estimate and the true $s(t)$ over $[0,T_{\\mathrm{eval}}]$ for both methods. For each test case, output the scalar difference $\\Delta r = r_{\\mathrm{adaptive}} - r_{\\mathrm{fixed}}$.\n- All outputs are unitless real numbers. No physical units are required in the final outputs.\n- Angles are in radians.\n- The program must produce the results for the following test suite, where each case is a tuple $(\\mathrm{seed}, f_s, T_{\\mathrm{pre}}, T_{\\mathrm{post}}, N, T_w, \\Delta, f_{\\mathrm{drift}}, A_{\\mathrm{drift}}, \\sigma_\\varepsilon, \\sigma_b, T^\\star)$ with all times in $\\mathrm{s}$ and frequencies in $\\mathrm{Hz}$:\n    - Case $1$: $(1,\\, 1000,\\, 0.300,\\, 0.700,\\, 100,\\, 0.200,\\, 0.010,\\, 0.50,\\, 10.0,\\, 3.0,\\, 15.0,\\, 0.600)$\n    - Case $2$: $(2,\\, 1000,\\, 0.300,\\, 0.700,\\, 100,\\, 0.200,\\, 0.010,\\, 0.50,\\, 0.0,\\, 3.0,\\, 15.0,\\, 0.600)$\n    - Case $3$: $(3,\\, 1000,\\, 0.200,\\, 0.700,\\, 80,\\, 0.200,\\, 0.010,\\, 0.50,\\, 8.0,\\, 3.0,\\, 15.0,\\, 0.600)$\n    - Case $4$: $(4,\\, 1000,\\, 0.300,\\, 0.700,\\, 100,\\, 0.200,\\, 0.010,\\, 0.50,\\, 2.0,\\, 10.0,\\, 15.0,\\, 0.600)$\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the four $\\Delta r$ results in the order of the cases above, each rounded to six decimal places, as a comma-separated list enclosed in square brackets (e.g., `[0.123456,-0.000001,0.050000,0.000000]`). No other text should be printed.",
            "solution": "The user-provided problem statement has been meticulously analyzed and is deemed valid. It is scientifically grounded in the principles of neurophysiological signal processing, specifically concerning the estimation of Event-Related Potentials (ERPs). The problem is well-posed, with all models, parameters, and objective functions explicitly defined, forming a self-contained and computationally verifiable task.\n\nThe core of the problem is to implement and compare two methods for baseline correction in synthetic electroencephalography (EEG) data: a standard fixed-window approach and an adaptive-window approach designed to mitigate low-frequency drift. The performance of each method is evaluated by its ability to recover a known, underlying ERP waveform, $s(t)$, from noisy single-trial data.\n\n**1. Synthetic Data Model**\n\nThe model for a single observed EEG trial, indexed by $i$, at time $t$ is given by the linear superposition of several components:\n$$\nx_i(t) = s(t) + d_i(t) + b_i + \\varepsilon_i(t)\n$$\nwhere:\n- $s(t)$ is the deterministic ERP waveform, identical across all trials. It is modeled as a sum of three Gaussian functions:\n  $$\n  s(t) = \\sum_{\\ell=1}^{3} A_\\ell \\exp\\!\\left(-\\frac{(t-\\mu_\\ell)^2}{2\\sigma_\\ell^2}\\right)\n  $$\n  with amplitudes $A_\\ell$, means $\\mu_\\ell$, and standard deviations $\\sigma_\\ell$ provided in the problem statement.\n- $d_i(t)$ represents a trial-variant low-frequency drift, modeled as a sinusoid:\n  $$\n  d_i(t) = A_{\\mathrm{drift}}\\sin(2\\pi f_{\\mathrm{drift}} t + \\phi_i)\n  $$\n  where the phase $\\phi_i$ is drawn uniformly from $[0, 2\\pi)$ for each trial $i \\in \\{1, \\dots, N\\}$.\n- $b_i$ is a constant voltage offset specific to each trial, modeled as a random variable drawn from a normal distribution, $b_i \\sim \\mathcal{N}(0, \\sigma_b^2)$.\n- $\\varepsilon_i(t)$ is additive, zero-mean white Gaussian noise, with $\\varepsilon_i(t_k) \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ for each discrete time point $t_k$.\n\nThe data are sampled at a frequency $f_s$, creating a discrete time grid $t_k$ over a total epoch spanning $[-T_{\\mathrm{pre}}, T_{\\mathrm{post}}]$. The stimulus onset is aligned with $t=0$.\n\n**2. Baseline Correction Methodologies**\n\nBaseline correction aims to remove the unwanted offset $b_i$ and mitigate the effect of the drift $d_i(t)$. This is achieved by estimating a baseline level from the pre-stimulus interval $[-T_{\\mathrm{pre}}, 0)$ and subtracting it from the entire trial's data.\n\n**2.1. Fixed-Window Baseline Correction**\n\nThis standard method uses a fixed, contiguous time interval for all trials to compute the baseline. As specified, this interval is the last $T_w$ seconds of the pre-stimulus period, i.e., $[-T_w, 0)$. For each trial $i$, the baseline value $\\mu_i^{\\mathrm{fix}}$ is the mean of the observed signal $x_i(t_k)$ over all time points $t_k$ within this window. The corrected trial is then $x_i^{\\mathrm{corr}}(t_k) = x_i(t_k) - \\mu_i^{\\mathrm{fix}}$.\n\n**2.2. Adaptive-Window Baseline Correction**\n\nThis method hypothesizes that the drift $d_i(t)$ can be locally approximated as a linear function. The ideal baseline window is one where the drift is minimal, which corresponds to a segment where the slope of the signal is close to zero. The procedure for each trial $i$ is as follows:\n1.  Define a set of candidate baseline windows. Each window has a fixed duration $T_w$ and must lie entirely within the pre-stimulus interval, $[-T_{\\mathrm{pre}}, 0)$. The start times of these candidate windows are generated on a grid with a step size of $\\Delta$, ranging from $-T_{\\mathrm{pre}}$ to $-T_w$.\n2.  For each candidate window, denoted by its set of time indices $\\mathcal{B}$, calculate the least-squares linear regression slope of the signal $x_i(t)$ with respect to time $t$. The formula for the slope estimator is:\n    $$\n    \\hat{m}_i(\\mathcal{B}) = \\frac{\\sum_{k \\in \\mathcal{B}} \\left(t_k - \\bar{t}_{\\mathcal{B}}\\right)\\left(x_i(t_k) - \\bar{x}_{i,\\mathcal{B}}\\right)}{\\sum_{k \\in \\mathcal{B}} \\left(t_k - \\bar{t}_{\\mathcal{B}}\\right)^2}\n    $$\n    where $\\bar{t}_{\\mathcal{B}}$ and $\\bar{x}_{i,\\mathcal{B}}$ are the mean time and mean signal value within the window $\\mathcal{B}$, respectively.\n3.  Select the optimal window $\\mathcal{B}^\\star$ for trial $i$ as the one that minimizes the absolute slope, $|\\hat{m}_i(\\mathcal{B})|$.\n4.  Compute the adaptive baseline value $\\mu_i^\\star$ as the mean of $x_i(t_k)$ over this optimal window $\\mathcal{B}^\\star$. The corrected trial is $x_i^{\\mathrm{corr}}(t_k) = x_i(t_k) - \\mu_i^\\star$.\n\n**3. ERP Estimation and Performance Evaluation**\n\nAfter applying one of the baseline correction methods to all $N$ trials, the estimated ERP, $\\hat{s}(t)$, is computed by averaging the corrected trials:\n$$\n\\hat{s}(t_k) = \\frac{1}{N}\\sum_{i=1}^N x_i^{\\mathrm{corr}}(t_k)\n$$\nThe quality of the estimation is quantified by computing the Pearson correlation coefficient, $r$, between the estimated ERP $\\hat{s}(t)$ and the true ERP $s(t)$. This correlation is calculated over a specified post-stimulus evaluation interval $[0, T_{\\mathrm{eval}}]$, where $T_{\\mathrm{eval}} = \\min(T_{\\mathrm{post}}, T^\\star)$. Let $\\mathcal{E}$ be the set of time indices corresponding to this interval. The correlation is then:\n$$\nr(\\hat{s}, s) = \\frac{\\sum_{k \\in \\mathcal{E}} \\left(\\hat{s}(t_k) - \\overline{\\hat{s}}\\right)\\left(s(t_k) - \\bar{s}\\right)}{\\sqrt{\\sum_{k \\in \\mathcal{E}} \\left(\\hat{s}(t_k) - \\overline{\\hat{s}}\\right)^2}\\sqrt{\\sum_{k \\in \\mathcal{E}} \\left(s(t_k) - \\bar{s}\\right)^2}}\n$$\nwhere $\\overline{\\hat{s}}$ and $\\bar{s}$ are the respective means over the evaluation interval $\\mathcal{E}$.\n\nThe final output for each test case is the difference in performance between the two methods: $\\Delta r = r_{\\mathrm{adaptive}} - r_{\\mathrm{fixed}}$. A positive $\\Delta r$ indicates that the adaptive method yields a more accurate ERP estimate than the fixed method under the given conditions.\n\nThe implementation will proceed by first generating the time grid and the true ERP signal $s(t)$. Then, for each trial, the full signal $x_i(t)$ is synthesized. Both baseline correction algorithms are applied to a copy of the synthetic dataset. Finally, the resulting ERP estimates are correlated with the true ERP, and their difference is computed and stored. The random number generator is seeded for each test case to ensure reproducibility.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are used.\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Each test case is a tuple:\n    # (seed, fs, T_pre, T_post, N, Tw, Delta, f_drift, A_drift, sigma_eps, sigma_b, T_star)\n    test_cases = [\n        (1, 1000, 0.300, 0.700, 100, 0.200, 0.010, 0.50, 10.0, 3.0, 15.0, 0.600),\n        (2, 1000, 0.300, 0.700, 100, 0.200, 0.010, 0.50, 0.0, 3.0, 15.0, 0.600),\n        (3, 1000, 0.200, 0.700, 80, 0.200, 0.010, 0.50, 8.0, 3.0, 15.0, 0.600),\n        (4, 1000, 0.300, 0.700, 100, 0.200, 0.010, 0.50, 2.0, 10.0, 15.0, 0.600)\n    ]\n\n    results = []\n    for params in test_cases:\n        delta_r = run_simulation(*params)\n        results.append(f\"{delta_r:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef pearson_r(x, y):\n    \"\"\"\n    Calculates the Pearson correlation coefficient between two 1D arrays x and y.\n    \"\"\"\n    if x.size == 0 or y.size == 0:\n        return 0.0\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n    \n    cov_xy = np.sum((x - mean_x) * (y - mean_y))\n    \n    var_x = np.sum((x - mean_x) ** 2)\n    var_y = np.sum((y - mean_y) ** 2)\n\n    if var_x == 0 or var_y == 0:\n        return 1.0 if np.all(x == y) else 0.0\n\n    return cov_xy / np.sqrt(var_x * var_y)\n\ndef calculate_slope(t_window, x_window):\n    \"\"\"\n    Calculates the least-squares slope for x(t) over a window.\n    \"\"\"\n    if t_window.size < 2:\n        return 0.0\n    t_mean = np.mean(t_window)\n    x_mean = np.mean(x_window)\n    \n    numerator = np.sum((t_window - t_mean) * (x_window - x_mean))\n    denominator = np.sum((t_window - t_mean) ** 2)\n    \n    return numerator / denominator if denominator != 0 else 0.0\n\ndef run_simulation(seed, fs, T_pre, T_post, N, Tw, Delta, f_drift, A_drift, sigma_eps, sigma_b, T_star):\n    \"\"\"\n    Performs a single simulation run for a given set of parameters.\n    \"\"\"\n    # Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # 1. Time Grid Generation\n    n_pre = int(round(T_pre * fs))\n    n_post = int(round(T_post * fs))\n    n_total = n_pre + n_post + 1\n    t = np.arange(-n_pre, n_post + 1) / fs\n    stim_onset_idx = n_pre\n\n    # 2. True ERP Waveform Generation\n    s_params = [\n        (5.0, 0.100, 0.030),\n        (-3.0, 0.200, 0.040),\n        (3.0, 0.350, 0.060)\n    ]\n    s_true = np.zeros_like(t)\n    for A, mu, sigma in s_params:\n        s_true += A * np.exp(-((t - mu)**2) / (2 * sigma**2))\n\n    # 3. Synthetic Trial Data Generation\n    X = np.zeros((N, n_total))\n    for i in range(N):\n        phi_i = rng.uniform(0, 2 * np.pi)\n        b_i = rng.normal(0, sigma_b)\n        eps_i = rng.normal(0, sigma_eps, size=n_total)\n        d_i = A_drift * np.sin(2 * np.pi * f_drift * t + phi_i)\n        X[i, :] = s_true + d_i + b_i + eps_i\n\n    # --- Fixed-Window Baseline Correction ---\n    X_fixed = np.copy(X)\n    n_win = int(round(Tw * fs))\n    fixed_bl_start_idx = stim_onset_idx - n_win\n    fixed_bl_end_idx = stim_onset_idx\n    \n    baseline_means_fixed = np.mean(X_fixed[:, fixed_bl_start_idx:fixed_bl_end_idx], axis=1, keepdims=True)\n    X_fixed -= baseline_means_fixed\n    s_hat_fixed = np.mean(X_fixed, axis=0)\n\n    # --- Adaptive-Window Baseline Correction ---\n    X_adaptive = np.copy(X)\n    \n    # Define candidate baseline windows\n    win_len_samples = int(round(Tw * fs))\n    max_start_time = -Tw\n    min_start_time = -T_pre\n    \n    # Ensure at least one candidate window exists\n    if min_start_time > max_start_time:\n         # This can happen if T_pre < Tw. The problem spec for Case 3 has T_pre = Tw\n         # which means only one window is possible. We handle this case by ensuring range is not empty\n         start_times = np.array([min_start_time])\n    else:\n         start_times = np.arange(min_start_time, max_start_time + Delta/2, Delta)\n\n    cand_start_indices = np.unique(np.round(start_times * fs).astype(int) + stim_onset_idx)\n\n    for i in range(N):\n        min_abs_slope = np.inf\n        best_bl_start_idx = -1\n\n        for start_idx in cand_start_indices:\n            end_idx = start_idx + win_len_samples\n            if end_idx > stim_onset_idx: # Window cannot go past t=0\n                continue\n            \n            t_window = t[start_idx:end_idx]\n            x_window = X_adaptive[i, start_idx:end_idx]\n            \n            slope = calculate_slope(t_window, x_window)\n            \n            if abs(slope) < min_abs_slope:\n                min_abs_slope = abs(slope)\n                best_bl_start_idx = start_idx\n        \n        # If no valid window was found (edge case), use the fixed one\n        if best_bl_start_idx == -1:\n            best_bl_start_idx = fixed_bl_start_idx\n\n        adaptive_bl_end_idx = best_bl_start_idx + win_len_samples\n        baseline_mean_adaptive = np.mean(X_adaptive[i, best_bl_start_idx:adaptive_bl_end_idx])\n        X_adaptive[i, :] -= baseline_mean_adaptive\n        \n    s_hat_adaptive = np.mean(X_adaptive, axis=0)\n\n    # 4. Performance Evaluation\n    T_eval = min(T_post, T_star)\n    eval_end_idx = stim_onset_idx + int(round(T_eval * fs))\n    eval_indices = slice(stim_onset_idx, eval_end_idx + 1)\n    \n    s_true_eval = s_true[eval_indices]\n    s_hat_fixed_eval = s_hat_fixed[eval_indices]\n    s_hat_adaptive_eval = s_hat_adaptive[eval_indices]\n    \n    r_fixed = pearson_r(s_hat_fixed_eval, s_true_eval)\n    r_adaptive = pearson_r(s_hat_adaptive_eval, s_true_eval)\n    \n    return r_adaptive - r_fixed\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}