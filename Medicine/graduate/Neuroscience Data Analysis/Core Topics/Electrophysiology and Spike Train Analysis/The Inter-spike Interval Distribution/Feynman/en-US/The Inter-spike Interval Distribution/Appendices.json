{
    "hands_on_practices": [
        {
            "introduction": "This problem lays the statistical groundwork for analyzing neural firing rates. By assuming a simple yet powerful model where spike trains follow a homogeneous Poisson process, the resulting inter-spike intervals (ISIs) are exponentially distributed. This exercise guides you through deriving the maximum likelihood estimator for the firing rate, a cornerstone of frequentist inference, and evaluating its precision using Fisher information .",
            "id": "4200038",
            "problem": "A single isolated neuron is observed under stationary conditions so that the spike train over a long recording window can be modeled as a homogeneous Poisson process in time. Under this model, the inter-spike intervals (ISIs), defined as the times between successive spikes, are independent and identically distributed (i.i.d.) exponential random variables with unknown rate parameter $\\lambda$, and probability density function $f(\\tau;\\lambda) = \\lambda \\exp(-\\lambda \\tau)$ for $\\tau \\ge 0$. You collect $n$ observed ISIs $\\tau_1,\\tau_2,\\ldots,\\tau_n$, and assume they follow this i.i.d. exponential model.\n\nStarting from the independence assumption and the exponential probability density, use the maximum likelihood principle to derive the maximum likelihood estimator (MLE) for the rate parameter $\\lambda$ in terms of the sample mean of the ISIs. Then, compute the Fisher information for $\\lambda$ based on $n$ i.i.d. ISIs, and use standard large-sample MLE theory to obtain the asymptotic variance of the MLE as a function of $n$ and $\\lambda$.\n\nExpress your final answer as a single row vector containing, in order: the MLE for $\\lambda$ written purely in terms of the sample mean of the ISIs, the Fisher information for $\\lambda$ based on $n$ samples, and the asymptotic variance of the MLE. No numerical approximation is required. Do not include units in your final boxed answer.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the standard theoretical framework of computational neuroscience and statistical inference, specifically the modeling of neuronal spike trains with Poisson processes. The problem is well-posed, objective, self-contained, and calls for a standard, verifiable mathematical derivation. No inconsistencies, ambiguities, or factual errors are present.\n\nThe solution proceeds in three stages as requested: derivation of the maximum likelihood estimator (MLE), calculation of the Fisher information, and determination of the asymptotic variance of the MLE.\n\nLet the sample of $n$ independent and identically distributed (i.i.d.) inter-spike intervals (ISIs) be $\\mathcal{D} = \\{\\tau_1, \\tau_2, \\ldots, \\tau_n\\}$. The probability density function (PDF) for a single ISI $\\tau$ is given by the exponential distribution:\n$$f(\\tau; \\lambda) = \\lambda \\exp(-\\lambda \\tau), \\quad \\tau \\ge 0$$\nwhere $\\lambda$ is the unknown rate parameter.\n\n**1. Maximum Likelihood Estimator (MLE) for $\\lambda$**\n\nThe likelihood function $L(\\lambda; \\mathcal{D})$ is the joint probability density of observing the given sample. Due to the i.i.d. assumption, it is the product of the individual PDFs:\n$$L(\\lambda; \\mathcal{D}) = \\prod_{i=1}^{n} f(\\tau_i; \\lambda) = \\prod_{i=1}^{n} \\lambda \\exp(-\\lambda \\tau_i)$$\n$$L(\\lambda; \\mathcal{D}) = \\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^{n} \\tau_i\\right)$$\nTo find the MLE, it is mathematically more convenient to maximize the log-likelihood function, $\\ell(\\lambda; \\mathcal{D}) = \\ln(L(\\lambda; \\mathcal{D}))$, since the logarithm is a strictly increasing function.\n$$\\ell(\\lambda; \\mathcal{D}) = \\ln\\left(\\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^{n} \\tau_i\\right)\\right)$$\n$$\\ell(\\lambda; \\mathcal{D}) = \\ln(\\lambda^n) + \\ln\\left(\\exp\\left(-\\lambda \\sum_{i=1}^{n} \\tau_i\\right)\\right)$$\n$$\\ell(\\lambda; \\mathcal{D}) = n \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} \\tau_i$$\nTo find the value of $\\lambda$ that maximizes $\\ell(\\lambda)$, we compute the first derivative with respect to $\\lambda$, set it to zero, and solve for $\\lambda$.\n$$\\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda}\\left(n \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} \\tau_i\\right) = \\frac{n}{\\lambda} - \\sum_{i=1}^{n} \\tau_i$$\nSetting the derivative to zero gives:\n$$\\frac{n}{\\hat{\\lambda}} - \\sum_{i=1}^{n} \\tau_i = 0$$\nwhere $\\hat{\\lambda}$ denotes the MLE. Solving for $\\hat{\\lambda}$:\n$$\\frac{n}{\\hat{\\lambda}} = \\sum_{i=1}^{n} \\tau_i \\implies \\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^{n} \\tau_i}$$\nThe sample mean of the ISIs is defined as $\\bar{\\tau} = \\frac{1}{n} \\sum_{i=1}^{n} \\tau_i$. We can express $\\hat{\\lambda}$ in terms of $\\bar{\\tau}$:\n$$\\hat{\\lambda} = \\frac{n}{n \\bar{\\tau}} = \\frac{1}{\\bar{\\tau}}$$\nTo confirm this is a maximum, we check the second derivative:\n$$\\frac{d^2\\ell}{d\\lambda^2} = \\frac{d}{d\\lambda}\\left(\\frac{n}{\\lambda} - \\sum_{i=1}^{n} \\tau_i\\right) = -\\frac{n}{\\lambda^2}$$\nSince $n  0$ and $\\lambda^2  0$, the second derivative is always negative, which confirms that $\\hat{\\lambda} = 1/\\bar{\\tau}$ is indeed a maximum likelihood estimate.\n\n**2. Fisher Information**\n\nThe Fisher information quantifies the amount of information that an observable random variable carries about an unknown parameter. For $n$ i.i.d. observations, the total Fisher information $I_n(\\lambda)$ is $n$ times the Fisher information for a single observation, $I_1(\\lambda)$.\n$$I_n(\\lambda) = n I_1(\\lambda)$$\nThe Fisher information for a single observation $I_1(\\lambda)$ can be calculated as the negative expectation of the second derivative of the single-observation log-likelihood function:\n$$I_1(\\lambda) = -E\\left[\\frac{\\partial^2}{\\partial\\lambda^2} \\ln f(\\tau; \\lambda)\\right]$$\nThe log-likelihood for a single observation $\\tau$ is:\n$$\\ln f(\\tau; \\lambda) = \\ln(\\lambda) - \\lambda \\tau$$\nThe first derivative with respect to $\\lambda$ is:\n$$\\frac{\\partial}{\\partial\\lambda} \\ln f(\\tau; \\lambda) = \\frac{1}{\\lambda} - \\tau$$\nThe second derivative is:\n$$\\frac{\\partial^2}{\\partial\\lambda^2} \\ln f(\\tau; \\lambda) = -\\frac{1}{\\lambda^2}$$\nThis second derivative is a constant with respect to the random variable $\\tau$. Therefore, its expectation is the constant itself:\n$$E\\left[\\frac{\\partial^2}{\\partial\\lambda^2} \\ln f(\\tau; \\lambda)\\right] = E\\left[-\\frac{1}{\\lambda^2}\\right] = -\\frac{1}{\\lambda^2}$$\nSubstituting this into the formula for $I_1(\\lambda)$:\n$$I_1(\\lambda) = -\\left(-\\frac{1}{\\lambda^2}\\right) = \\frac{1}{\\lambda^2}$$\nThe total Fisher information for $n$ observations is therefore:\n$$I_n(\\lambda) = n I_1(\\lambda) = \\frac{n}{\\lambda^2}$$\n\n**3. Asymptotic Variance of the MLE**\n\nAccording to standard large-sample theory for maximum likelihood estimators, for large $n$, the sampling distribution of the MLE $\\hat{\\lambda}$ is approximately normal with mean $\\lambda$ and variance given by the inverse of the Fisher information. This is also the Cram√©r-Rao lower bound, which the MLE asymptotically achieves.\nThe asymptotic variance is:\n$$\\text{Var}(\\hat{\\lambda}) \\approx [I_n(\\lambda)]^{-1}$$\nUsing the result for $I_n(\\lambda)$ from the previous section:\n$$\\text{Var}(\\hat{\\lambda}) \\approx \\left(\\frac{n}{\\lambda^2}\\right)^{-1} = \\frac{\\lambda^2}{n}$$\n\nThe three requested quantities are thus:\n1. MLE for $\\lambda$: $\\hat{\\lambda} = \\frac{1}{\\bar{\\tau}}$\n2. Fisher information for $n$ samples: $I_n(\\lambda) = \\frac{n}{\\lambda^2}$\n3. Asymptotic variance of the MLE: $\\text{Var}(\\hat{\\lambda}) = \\frac{\\lambda^2}{n}$\nThese are to be presented in a single row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\bar{\\tau}}  \\frac{n}{\\lambda^2}  \\frac{\\lambda^2}{n}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Real-world data collection is rarely perfect, and a common issue is the termination of a recording before the next spike occurs, known as right-censoring. This practice demonstrates how to extend our statistical models to correctly account for such incomplete observations, which is crucial for unbiased parameter estimation . You will derive the modified likelihood function and apply it to a concrete example, highlighting the importance of handling censored data properly.",
            "id": "4199999",
            "problem": "A single neuron is recorded over a finite time window. Spikes are identified and inter-spike intervals (ISIs) are measured until the recording terminates during an ongoing interval, producing a right-censored observation: the time elapsed since the last spike to recording termination is observed but the next spike does not occur within the recording. Assume a renewal process with independent and identically distributed ISIs with probability density function $f(t \\mid \\boldsymbol{\\theta})$ and survival function $S(t \\mid \\boldsymbol{\\theta}) = 1 - F(t \\mid \\boldsymbol{\\theta})$, where $F$ is the cumulative distribution function.\n\nStarting from the definitions of independence of intervals, density for completed ISIs, and survival for right-censored durations, derive the likelihood $L(\\boldsymbol{\\theta})$ for $n$ completed ISIs $t_{1},\\dots,t_{n}$ and one right-censored final interval of length $\\tau_{\\text{cens}}$. Then, specialize to an exponential renewal model with rate parameter $\\lambda$ so that $f(t \\mid \\lambda) = \\lambda \\exp(-\\lambda t)$ and $S(t \\mid \\lambda) = \\exp(-\\lambda t)$. Derive the maximum likelihood estimator (MLE) $\\hat{\\lambda}$ in closed form.\n\nFinally, apply your result to the following dataset of completed ISIs (in seconds): $t_{1} = 0.102$, $t_{2} = 0.087$, $t_{3} = 0.094$, $t_{4} = 0.110$, with a single right-censored final interval of length $\\tau_{\\text{cens}} = 0.060$ seconds. Compute the numerical value of $\\hat{\\lambda}$ for this dataset. Round your answer to four significant figures. Express the answer in hertz (Hz), but do not include units in your final boxed answer.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective. It provides a standard problem in statistical modeling of neural data, specifically the treatment of right-censored data in the context of a renewal process. All necessary information is provided, and the problem is internally consistent. Therefore, the problem is valid and a solution will be derived.\n\nThe derivation proceeds in three stages: first, the general likelihood function for censored and uncensored data is constructed; second, this likelihood is specialized for an exponential distribution; third, the maximum likelihood estimator (MLE) for the exponential rate parameter is derived and calculated for the given data.\n\nLet the set of observations be $n$ fully observed inter-spike intervals (ISIs), $t_1, t_2, \\dots, t_n$, and one right-censored interval of duration $\\tau_{\\text{cens}}$. The process is assumed to be a renewal process, meaning the ISIs are independent and identically distributed (i.i.d.) random variables. The probability density function (PDF) for an ISI of duration $t$ is $f(t \\mid \\boldsymbol{\\theta})$ and the survival function is $S(t \\mid \\boldsymbol{\\theta})$, where $\\boldsymbol{\\theta}$ is the vector of model parameters.\n\nThe likelihood function, $L(\\boldsymbol{\\theta})$, is the joint probability of the observed data, considered as a function of $\\boldsymbol{\\theta}$. Due to the independence of the intervals, the total likelihood is the product of the contributions from each observation.\nFor a fully observed ISI $t_i$, the contribution to the likelihood is its probability density, $f(t_i \\mid \\boldsymbol{\\theta})$.\nFor the right-censored interval $\\tau_{\\text{cens}}$, the observation is that the interval's true duration $T$ is greater than $\\tau_{\\text{cens}}$. The probability of this event is given by the survival function, $P(T  \\tau_{\\text{cens}}) = S(\\tau_{\\text{cens}} \\mid \\boldsymbol{\\theta})$.\n\nTherefore, the general likelihood function is the product of the densities for the $n$ completed intervals and the survival probability for the one censored interval:\n$$L(\\boldsymbol{\\theta}) = \\left( \\prod_{i=1}^{n} f(t_i \\mid \\boldsymbol{\\theta}) \\right) S(\\tau_{\\text{cens}} \\mid \\boldsymbol{\\theta})$$\n\nNext, we specialize this to the exponential renewal model. For this model, the parameter is the rate $\\lambda$, so $\\boldsymbol{\\theta} = \\lambda$. The PDF and survival function are given as:\n$$f(t \\mid \\lambda) = \\lambda \\exp(-\\lambda t)$$\n$$S(t \\mid \\lambda) = \\exp(-\\lambda t)$$\nSubstituting these into the general likelihood function yields:\n$$L(\\lambda) = \\left( \\prod_{i=1}^{n} \\left[ \\lambda \\exp(-\\lambda t_i) \\right] \\right) \\exp(-\\lambda \\tau_{\\text{cens}})$$\nSimplifying the product:\n$$L(\\lambda) = \\lambda^n \\left( \\prod_{i=1}^{n} \\exp(-\\lambda t_i) \\right) \\exp(-\\lambda \\tau_{\\text{cens}})$$\nThe product of exponentials is the exponential of the sum of the arguments:\n$$L(\\lambda) = \\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\exp(-\\lambda \\tau_{\\text{cens}})$$\n$$L(\\lambda) = \\lambda^n \\exp\\left(-\\lambda \\left( \\sum_{i=1}^{n} t_i + \\tau_{\\text{cens}} \\right)\\right)$$\n\nTo find the MLE for $\\lambda$, we maximize $L(\\lambda)$. This is equivalent to maximizing the log-likelihood function, $\\ell(\\lambda) = \\ln(L(\\lambda))$, which is often analytically simpler.\n$$\\ell(\\lambda) = \\ln\\left[ \\lambda^n \\exp\\left(-\\lambda \\left( \\sum_{i=1}^{n} t_i + \\tau_{\\text{cens}} \\right)\\right) \\right]$$\nUsing the properties of logarithms:\n$$\\ell(\\lambda) = \\ln(\\lambda^n) + \\ln\\left(\\exp\\left(-\\lambda \\left( \\sum_{i=1}^{n} t_i + \\tau_{\\text{cens}} \\right)\\right)\\right)$$\n$$\\ell(\\lambda) = n \\ln(\\lambda) - \\lambda \\left( \\sum_{i=1}^{n} t_i + \\tau_{\\text{cens}} \\right)$$\nTo find the maximum, we differentiate $\\ell(\\lambda)$ with respect to $\\lambda$ and set the result to zero.\n$$\\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda} \\left[ n \\ln(\\lambda) - \\lambda \\left( \\sum_{i=1}^{n} t_i + \\tau_{\\text{cens}} \\right) \\right] = \\frac{n}{\\lambda} - \\left( \\sum_{i=1}^{n} t_i + \\tau_{\\text{cens}} \\right)$$\nSetting the derivative to zero for the MLE, $\\hat{\\lambda}$:\n$$\\frac{n}{\\hat{\\lambda}} - \\left( \\sum_{i=1}^{n} t_i + \\tau_{\\text{cens}} \\right) = 0$$\nSolving for $\\hat{\\lambda}$:\n$$\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^{n} t_i + \\tau_{\\text{cens}}}$$\nThis result is intuitively satisfying: the estimated rate is the number of observed events (spikes) divided by the total observation time, which includes both the completed intervals and the final censored one.\n\nFinally, we apply this formula to the given dataset:\nNumber of completed intervals, $n=4$.\nCompleted ISIs (in seconds): $t_1 = 0.102$, $t_2 = 0.087$, $t_3 = 0.094$, $t_4 = 0.110$.\nRight-censored interval (in seconds): $\\tau_{\\text{cens}} = 0.060$.\n\nFirst, we compute the sum of the durations of the completed ISIs:\n$$\\sum_{i=1}^{4} t_i = 0.102 + 0.087 + 0.094 + 0.110 = 0.393$$\nThe total time in the denominator is the sum of all observed durations:\n$$\\sum_{i=1}^{4} t_i + \\tau_{\\text{cens}} = 0.393 + 0.060 = 0.453$$\nNow, we can compute the value of the MLE, $\\hat{\\lambda}$:\n$$\\hat{\\lambda} = \\frac{4}{0.453} \\approx 8.830022...$$\nThe problem asks for the result to be rounded to four significant figures.\n$$\\hat{\\lambda} \\approx 8.830$$\nThe units of $\\hat{\\lambda}$ are inverse seconds ($s^{-1}$), which is Hertz (Hz).",
            "answer": "$$\\boxed{8.830}$$"
        },
        {
            "introduction": "While the exponential distribution provides a useful baseline model for ISIs, real neurons often exhibit more complex firing patterns. This hands-on coding exercise challenges you to implement a likelihood ratio test, a formal statistical method for model selection, to determine if a more flexible gamma distribution offers a significantly better fit to the data . This practice bridges the gap between theoretical models and data-driven decisions, a key skill in computational neuroscience.",
            "id": "4200088",
            "problem": "You are given multiple sets of Inter-Spike Interval (ISI) measurements, each set representing independent and identically distributed time intervals between successive neural spikes for a single neuron under stationary conditions. All ISIs are in seconds and must be treated as strictly positive real numbers. Your task is to programmatically implement a principled hypothesis test that compares an exponential renewal model to a gamma renewal model for ISIs using a Likelihood Ratio Test (LRT). The test should be derived from first principles of likelihood under independence.\n\nStart from the following fundamental base:\n- The definition of a probability density function for a continuous random variable and the independence of observations, implying that the likelihood is the product of densities and the log-likelihood is the sum of log-densities.\n- The exponential model with rate parameter $\\lambda$ is a special case of the gamma model with shape parameter $k=1$.\n- Maximum Likelihood Estimation (MLE) is performed by maximizing the log-likelihood with respect to the model parameters.\n- The Likelihood Ratio Test (LRT) statistic is defined as $T = 2(\\ell_{\\text{alt}} - \\ell_{\\text{null}})$, where $\\ell$ denotes the maximized log-likelihood under the respective model. Under regularity conditions and when the null model is nested within the alternative model with a difference of one free parameter, the LRT statistic is asymptotically distributed as a chi-square random variable with one degree of freedom.\n\nTasks to implement:\n1. For each ISI dataset $\\{x_i\\}_{i=1}^n$ in seconds, derive from first principles the log-likelihood under the exponential model and obtain the MLE for its parameter. Evaluate the maximized log-likelihood $\\ell_{\\text{exp}}$ at the MLE.\n2. For each ISI dataset $\\{x_i\\}_{i=1}^n$ in seconds, derive from first principles the log-likelihood under the gamma model and obtain the MLEs for its parameters. This requires solving the shape parameter equation numerically using a consistent and convergent method based on the derivative of the log-likelihood. Evaluate the maximized log-likelihood $\\ell_{\\text{gamma}}$ at the MLEs.\n3. Compute the LRT statistic $T = 2(\\ell_{\\text{gamma}} - \\ell_{\\text{exp}})$ for each dataset.\n4. Under the null hypothesis that the ISIs follow an exponential distribution (equivalently a gamma distribution with shape $k=1$), assess significance by computing the tail probability under the chi-square distribution with one degree of freedom, i.e., the p-value $p = \\Pr(\\chi^2_1 \\ge T)$.\n5. Using a significance level $\\alpha = 0.05$, return a boolean decision for whether to reject the null hypothesis for each dataset, where rejection occurs if $p  \\alpha$.\n\nPhysical units requirement:\n- All ISIs are in seconds. No unit conversion is needed. Report all results unitless except that the ISIs themselves are seconds-valued inputs.\n\nAngle unit requirement:\n- Not applicable.\n\nPercentages requirement:\n- If any proportion-like quantity arises, it must be expressed as a decimal number.\n\nTest suite:\nProvide results for the following ISI datasets (each value is in seconds):\n- Test case 1 (typical irregular spiking, exponential-like): [0.012, 0.033, 0.078, 0.041, 0.052, 0.090, 0.005, 0.061, 0.045, 0.066, 0.031, 0.084, 0.057, 0.049, 0.022, 0.073, 0.038, 0.059, 0.047, 0.055].\n- Test case 2 (more regular than exponential, gamma-like with $k1$): [0.023, 0.071, 0.041, 0.064, 0.089, 0.055, 0.050, 0.082, 0.035, 0.058, 0.062, 0.046, 0.074, 0.069, 0.039, 0.053, 0.067, 0.101, 0.044, 0.060].\n- Test case 3 (small sample size, irregular): [0.040, 0.061, 0.018, 0.079, 0.052].\n- Test case 4 (highly regular spiking): [0.048, 0.051, 0.050, 0.049, 0.052, 0.047, 0.053, 0.050, 0.051, 0.049].\n\nFinal output format:\n- For each test case, output a sublist $[T, p, \\text{decision}]$, where $T$ is the LRT statistic, $p$ is the p-value under $\\chi^2_1$, and $\\text{decision}$ is a boolean that is `True` if the null is rejected at $\\alpha=0.05$ and `False` otherwise.\n- Round $T$ and $p$ to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example: $[[T_1,p_1,\\text{decision}_1], [T_2,p_2,\\text{decision}_2], \\dots]$.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of computational neuroscience and statistical inference, specifically addressing the modeling of neural spike trains. The problem is well-posed, providing a clear objective, all necessary data and constants (models, test statistic, significance level), and is free from ambiguity or contradiction. It represents a standard and formalizable task in data analysis.\n\nHerein, a step-by-step derivation and solution methodology is presented.\n\n### 1. Model Formulation and Maximum Likelihood Estimation (MLE)\n\nThe core of the task is to compare two nested probabilistic models for Inter-Spike Intervals (ISIs), denoted by the dataset $\\{x_i\\}_{i=1}^n$.\n\n#### 1.1. Null Hypothesis ($H_0$): The Exponential Renewal Model\n\nThe exponential distribution is characterized by a single rate parameter $\\lambda  0$. Its probability density function (PDF) is:\n$$ f(x | \\lambda) = \\lambda e^{-\\lambda x} $$\nFor a set of $n$ independent and identically distributed (i.i.d.) observations $\\{x_i\\}$, the likelihood function $L(\\lambda)$ is the product of the individual densities:\n$$ L(\\lambda | \\{x_i\\}) = \\prod_{i=1}^n \\lambda e^{-\\lambda x_i} = \\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i} $$\nIt is computationally more convenient to work with the log-likelihood, $\\ell(\\lambda) = \\ln L(\\lambda)$:\n$$ \\ell(\\lambda) = n \\ln(\\lambda) - \\lambda \\sum_{i=1}^n x_i $$\nTo find the Maximum Likelihood Estimate (MLE) for $\\lambda$, we differentiate $\\ell(\\lambda)$ with respect to $\\lambda$ and set the result to zero:\n$$ \\frac{d\\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0 $$\nSolving for $\\lambda$ yields the MLE, $\\hat{\\lambda}$:\n$$ \\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^n x_i} = \\frac{1}{\\bar{x}} $$\nwhere $\\bar{x}$ is the sample mean. The maximized log-likelihood under the exponential model, $\\ell_{\\text{exp}}$, is obtained by substituting $\\hat{\\lambda}$ back into the log-likelihood function:\n$$ \\ell_{\\text{exp}} = \\ell(\\hat{\\lambda}) = n \\ln(\\hat{\\lambda}) - \\hat{\\lambda} \\sum_{i=1}^n x_i = n \\ln\\left(\\frac{1}{\\bar{x}}\\right) - \\frac{1}{\\bar{x}} (n\\bar{x}) = -n \\ln(\\bar{x}) - n $$\n\n#### 1.2. Alternative Hypothesis ($H_1$): The Gamma Renewal Model\n\nThe gamma distribution is a more general model with two parameters: a shape parameter $k  0$ and a scale parameter $\\theta  0$. Its PDF is:\n$$ f(x | k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k} x^{k-1} e^{-x/\\theta} $$\nwhere $\\Gamma(k)$ is the gamma function. The log-likelihood for $n$ i.i.d. observations is:\n$$ \\ell(k, \\theta) = \\sum_{i=1}^n \\left[ (k-1)\\ln(x_i) - \\frac{x_i}{\\theta} - k\\ln(\\theta) - \\ln(\\Gamma(k)) \\right] $$\n$$ \\ell(k, \\theta) = (k-1)\\sum_{i=1}^n \\ln(x_i) - \\frac{1}{\\theta}\\sum_{i=1}^n x_i - nk\\ln(\\theta) - n\\ln(\\Gamma(k)) $$\nTo find the MLEs, we take partial derivatives with respect to $\\theta$ and $k$ and set them to zero.\n$$ \\frac{\\partial\\ell}{\\partial\\theta} = \\frac{1}{\\theta^2}\\sum_{i=1}^n x_i - \\frac{nk}{\\theta} = 0 \\implies \\sum_{i=1}^n x_i = nk\\theta $$\nThis yields the MLE for $\\theta$ in terms of $k$ and the data:\n$$ \\hat{\\theta} = \\frac{\\sum x_i}{nk} = \\frac{\\bar{x}}{k} $$\nNext, the partial derivative with respect to $k$:\n$$ \\frac{\\partial\\ell}{\\partial k} = \\sum_{i=1}^n \\ln(x_i) - n\\ln(\\theta) - n\\psi(k) = 0 $$\nwhere $\\psi(k) = \\frac{d}{dk}\\ln(\\Gamma(k))$ is the digamma function. Substituting $\\hat{\\theta} = \\bar{x}/k$ into this equation gives an equation for the MLE of $k$, denoted $\\hat{k}$:\n$$ \\sum_{i=1}^n \\ln(x_i) - n\\ln\\left(\\frac{\\bar{x}}{k}\\right) - n\\psi(k) = 0 $$\n$$ \\frac{1}{n}\\sum_{i=1}^n \\ln(x_i) - \\ln(\\bar{x}) + \\ln(k) - \\psi(k) = 0 $$\n$$ \\ln(k) - \\psi(k) = \\ln(\\bar{x}) - \\overline{\\ln(x)} $$\nwhere $\\overline{\\ln(x)}$ is the sample mean of the log-transformed data. This equation for $\\hat{k}$ has no closed-form solution and must be solved numerically, for instance, using Newton's method. Once $\\hat{k}$ is found, $\\hat{\\theta}$ is computed as $\\hat{\\theta} = \\bar{x}/\\hat{k}$. The maximized log-likelihood, $\\ell_{\\text{gamma}}$, is then:\n$$ \\ell_{\\text{gamma}} = \\ell(\\hat{k}, \\hat{\\theta}) = (\\hat{k}-1)\\sum_{i=1}^n\\ln(x_i) - \\frac{n\\bar{x}}{\\hat{\\theta}} - n\\hat{k}\\ln(\\hat{\\theta}) - n\\ln(\\Gamma(\\hat{k})) $$\n\n### 2. The Likelihood Ratio Test (LRT)\n\nThe exponential distribution is a special case of the gamma distribution where the shape parameter $k=1$. This means the null hypothesis model is nested within the alternative hypothesis model. For such nested models, the Likelihood Ratio Test (LRT) is a powerful tool. The LRT statistic, $T$, is defined as:\n$$ T = 2 (\\ell_{\\text{gamma}} - \\ell_{\\text{exp}}) $$\nBy construction, $\\ell_{\\text{gamma}} \\ge \\ell_{\\text{exp}}$, so $T \\ge 0$. According to Wilks' theorem, under the null hypothesis $H_0$, the statistic $T$ is asymptotically distributed as a chi-square ($\\chi^2$) random variable. The degrees of freedom for this distribution is the difference in the number of free parameters between the alternative and null models. Here, the gamma model has two parameters ($k, \\theta$) and the exponential model has one ($\\lambda$), so the degrees of freedom is $2 - 1 = 1$.\n$$ T \\sim \\chi^2_1 $$\n\n### 3. Hypothesis Test Decision\n\nTo decide whether to reject the null hypothesis, we compute a p-value. The p-value is the probability of observing a test statistic at least as extreme as the one computed from our data, assuming $H_0$ is true.\n$$ p = \\Pr(\\chi^2_1 \\ge T) $$\nThis is calculated as the survival function of the $\\chi^2_1$ distribution evaluated at $T$. We then compare the p-value to a pre-defined significance level, $\\alpha = 0.05$.\n- If $p  \\alpha$: We reject the null hypothesis ($H_0$). The evidence suggests the data are better described by a gamma distribution than an exponential one.\n- If $p \\ge \\alpha$: We fail to reject the null hypothesis ($H_0$). There is insufficient evidence to prefer the more complex gamma model over the simpler exponential model.\n\nThe algorithmic implementation will follow these derived steps for each provided dataset. A numerical root-finding algorithm (Newton-Raphson) will be used to find $\\hat{k}$.",
            "answer": "```python\nimport numpy as np\nfrom scipy import special, stats\n\ndef perform_lrt(isi_data: list[float], alpha: float) - list:\n    \"\"\"\n    Performs a Likelihood Ratio Test to compare an exponential model vs. a gamma model.\n\n    Args:\n        isi_data: A list of inter-spike intervals in seconds.\n        alpha: The significance level for the hypothesis test.\n\n    Returns:\n        A list containing the LRT statistic T, the p-value, and the boolean decision.\n    \"\"\"\n    x = np.array(isi_data, dtype=np.float64)\n    n = len(x)\n    mean_x = np.mean(x)\n\n    # 1. Exponential Model (Null Hypothesis H0)\n    # The maximized log-likelihood is ell_exp = -n * ln(mean_x) - n\n    if mean_x = 0:\n        # Invalid data, cannot proceed. Should not happen with problem constraints.\n        return [np.nan, np.nan, False]\n    log_L_exp = -n * np.log(mean_x) - n\n\n    # 2. Gamma Model (Alternative Hypothesis H1)\n    # The MLE for k requires solving ln(k) - digamma(k) = ln(mean(x)) - mean(ln(x))\n    log_x = np.log(x)\n    mean_log_x = np.mean(log_x)\n    s = np.log(mean_x) - mean_log_x\n    sum_log_x = np.sum(log_x)\n\n    # Numerical solution for k_hat using Newton-Raphson method\n    # Initial guess for k from Minka (2002), stable for s - 0\n    if s  1e-9: # Protection against s being zero or negative\n        k_hat = (3.0 - s + np.sqrt((s - 3.0)**2 + 24.0 * s)) / (12.0 * s)\n        for _ in range(10): # 10 iterations are more than enough for convergence\n            log_k = np.log(k_hat)\n            digamma_k = special.digamma(k_hat)\n            f_k = log_k - digamma_k - s\n            if abs(f_k)  1e-12:\n                break\n            # Derivative f'(k) = 1/k - trigamma(k)\n            f_prime_k = 1.0 / k_hat - special.polygamma(1, k_hat)\n            k_hat -= f_k / f_prime_k\n    else:\n        # If s is effectively zero, data is constant, k - infinity.\n        # The likelihood becomes infinite. We set k to a very large number.\n        # This case is physically implausible and not in the test data.\n        # For LRT, this implies a clear rejection of k=1.\n        k_hat = 1e15\n\n    # Compute maximized log-likelihood for the gamma model\n    # ell_gamma = (k-1)*sum(ln(x)) - sum(x)/theta - n*k*ln(theta) - n*ln(Gamma(k))\n    # where theta = mean_x / k\n    theta_hat = mean_x / k_hat\n    log_L_gamma = (k_hat - 1) * sum_log_x - n * mean_x / theta_hat - n * k_hat * np.log(theta_hat) - n * special.gammaln(k_hat)\n\n    # 3. Compute LRT statistic T\n    T = 2 * (log_L_gamma - log_L_exp)\n    if T  0: # Clamp to zero in case of floating point inaccuracies\n        T = 0.0\n\n    # 4. Compute p-value\n    # Under H0, T is asymptotically chi-square distributed with df=1\n    df = 1\n    p_value = stats.chi2.sf(T, df)\n    \n    # 5. Make decision\n    decision = p_value  alpha\n\n    return [round(T, 6), round(p_value, 6), decision]\n    \ndef solve():\n    \"\"\"\n    Main function to run the analysis on the provided test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        [0.012, 0.033, 0.078, 0.041, 0.052, 0.090, 0.005, 0.061, 0.045, 0.066, 0.031, 0.084, 0.057, 0.049, 0.022, 0.073, 0.038, 0.059, 0.047, 0.055],\n        # Test case 2\n        [0.023, 0.071, 0.041, 0.064, 0.089, 0.055, 0.050, 0.082, 0.035, 0.058, 0.062, 0.046, 0.074, 0.069, 0.039, 0.053, 0.067, 0.101, 0.044, 0.060],\n        # Test case 3\n        [0.040, 0.061, 0.018, 0.079, 0.052],\n        # Test case 4\n        [0.048, 0.051, 0.050, 0.049, 0.052, 0.047, 0.053, 0.050, 0.051, 0.049]\n    ]\n\n    alpha = 0.05\n    results = []\n\n    for case_data in test_cases:\n        result = perform_lrt(case_data, alpha)\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    # str() on a list in Python produces the desired format with spaces.\n    # The user wants comma separated without spaces.\n    results_str = [f\"[{res[0]}, {res[1]}, {res[2]}]\".replace(\"True\", \"True\").replace(\"False\", \"False\") for res in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}