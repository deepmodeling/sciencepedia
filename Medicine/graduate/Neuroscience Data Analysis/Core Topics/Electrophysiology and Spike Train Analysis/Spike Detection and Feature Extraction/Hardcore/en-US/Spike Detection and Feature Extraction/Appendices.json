{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in any spike detection pipeline is setting a voltage threshold to distinguish potential neural events from background noise. While the standard deviation is a common measure of noise scale, it is highly sensitive to large-amplitude artifacts that can corrupt a recording. This exercise  challenges you to derive and apply a more robust noise estimator based on the Median Absolute Deviation (MAD), a technique essential for building reliable detection systems that can operate in non-ideal experimental conditions.",
            "id": "4194221",
            "problem": "In extracellular spike detection, robust thresholding requires an estimate of the background noise scale that is insensitive to occasional large-amplitude artifacts. Consider a voltage trace $x(t)$ recorded at sampling rate $f_{s} = 30\\,\\text{kHz}$ from an extracellular electrode. The background noise in the trace is modeled as a stationary, zero-mean Gaussian process with unknown standard deviation $\\sigma$, and artifacts are sparse and nonstationary. An artifact-free interval $I$ of duration $0.5\\,\\text{ms}$ has been identified, yielding $15$ samples. The samples in $I$ (expressed in microvolts) are\n$$\n[-8,\\; 5,\\; -3,\\; 0,\\; 2,\\; -1,\\; 4,\\; -6,\\; 7,\\; -2,\\; 3,\\; -4,\\; 1,\\; -5,\\; 6]\\,\\mu\\text{V}.\n$$\nStarting from the definition of the median, the symmetry of the zero-mean Gaussian distribution, and the cumulative distribution function (CDF) of the normal distribution, derive the exact constant $k$ such that $\\mathrm{median}(|X|) = k\\,\\sigma$ when $X \\sim \\mathcal{N}(0,\\sigma^{2})$. Using this $k$, define a robust noise scale estimator based on the median absolute deviation (MAD) computed on the artifact-free samples as\n$$\n\\hat{\\sigma} \\equiv \\frac{\\mathrm{median}(|x_{i}|)}{k}.\n$$\nCompute $\\hat{\\sigma}$ numerically from the provided samples. Round your answer to four significant figures and express the final value in microvolts. Your final answer must be a single real number.",
            "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   Voltage trace: $x(t)$\n-   Sampling rate: $f_{s} = 30\\,\\text{kHz}$\n-   Background noise model: Stationary, zero-mean Gaussian process, $X \\sim \\mathcal{N}(0, \\sigma^2)$, with unknown standard deviation $\\sigma$.\n-   Artifact-free interval $I$ of duration $0.5\\,\\text{ms}$.\n-   Number of samples in $I$: $15$.\n-   Samples in $I$: $[-8,\\; 5,\\; -3,\\; 0,\\; 2,\\; -1,\\; 4,\\; -6,\\; 7,\\; -2,\\; 3,\\; -4,\\; 1,\\; -5,\\; 6]\\,\\mu\\text{V}$.\n-   Definition of the constant $k$: $\\mathrm{median}(|X|) = k\\,\\sigma$ for $X \\sim \\mathcal{N}(0,\\sigma^{2})$.\n-   Definition of the estimator: $\\hat{\\sigma} \\equiv \\frac{\\mathrm{median}(|x_{i}|)}{k}$.\n-   Required output: Numerical value of $\\hat{\\sigma}$, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n1.  **Scientific Soundness**: The problem describes a robust statistical estimation method (using the median absolute deviation, MAD) to find the scale of Gaussian noise, a standard technique in signal processing and neuroscience. The model of background noise as a zero-mean Gaussian process is a common and valid approximation.\n2.  **Completeness and Consistency**: All necessary information is provided. The number of samples is consistent with the sampling rate and interval duration: $(0.5 \\times 10^{-3}\\,\\text{s}) \\times (30 \\times 10^{3}\\,\\text{s}^{-1}) = 15$ samples, which matches the count of provided data points. The givens are self-consistent.\n3.  **Well-Posedness**: The problem asks for a derivation of a standard statistical constant and its application to a dataset. The tasks are clearly defined and lead to a unique, stable solution.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Derivation of the constant $k$\nLet $X$ be a random variable following a zero-mean Gaussian distribution, $X \\sim \\mathcal{N}(0, \\sigma^2)$. We are tasked with finding the constant $k$ such that the median of the absolute value of $X$ is $k\\sigma$. Let $Y = |X|$. The median of $Y$, denoted $m_Y$, is defined by the property $P(Y \\le m_Y) = \\frac{1}{2}$.\nFrom the problem statement, we have $m_Y = \\mathrm{median}(|X|) = k\\sigma$. Substituting this into the definition of the median:\n$$\nP(|X| \\le k\\sigma) = \\frac{1}{2}\n$$\nThis inequality can be expanded as:\n$$\nP(-k\\sigma \\le X \\le k\\sigma) = \\frac{1}{2}\n$$\nTo evaluate this probability, we standardize the random variable $X$ by defining $Z = \\frac{X}{\\sigma}$. The variable $Z$ follows the standard normal distribution, $Z \\sim \\mathcal{N}(0, 1)$. The inequality becomes:\n$$\nP(-k \\le Z \\le k) = \\frac{1}{2}\n$$\nLet $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution, defined as $\\Phi(z) = P(Z \\le z)$. The probability can be expressed in terms of the CDF:\n$$\nP(-k \\le Z \\le k) = \\Phi(k) - \\Phi(-k)\n$$\nDue to the symmetry of the standard normal distribution about $0$, we have the identity $\\Phi(-k) = 1 - \\Phi(k)$. Substituting this into the equation gives:\n$$\n\\Phi(k) - (1 - \\Phi(k)) = \\frac{1}{2}\n$$\n$$\n2\\Phi(k) - 1 = \\frac{1}{2}\n$$\nSolving for $\\Phi(k)$:\n$$\n2\\Phi(k) = 1 + \\frac{1}{2} = \\frac{3}{2}\n$$\n$$\n\\Phi(k) = \\frac{3}{4} = 0.75\n$$\nThus, $k$ is the value for which the cumulative probability of the standard normal distribution is $0.75$. This is the $75$th percentile, or third quartile, of the standard normal distribution. The exact analytical expression for $k$ is the inverse of the standard normal CDF evaluated at $0.75$:\n$$\nk = \\Phi^{-1}(0.75)\n$$\nThis completes the derivation of the constant $k$.\n\n### Computation of the noise scale estimator $\\hat{\\sigma}$\nThe estimator for the standard deviation $\\sigma$ is given by:\n$$\n\\hat{\\sigma} = \\frac{\\mathrm{median}(|x_{i}|)}{k}\n$$\nwhere $\\{x_i\\}$ are the provided voltage samples. The procedure is as follows:\n\n1.  **List the samples:**\n    The given samples are $x_i = [-8, 5, -3, 0, 2, -1, 4, -6, 7, -2, 3, -4, 1, -5, 6]$. The units are $\\mu\\text{V}$.\n\n2.  **Compute the absolute values of the samples:**\n    The set of absolute values, $|x_i|$, is $[8, 5, 3, 0, 2, 1, 4, 6, 7, 2, 3, 4, 1, 5, 6]$.\n\n3.  **Find the median of the absolute values:**\n    First, we sort the absolute values in non-decreasing order. The sorted list is:\n    $$\n    [0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 8]\n    $$\n    There are $N=15$ samples, which is an odd number. The median is the middle value, which is located at position $\\frac{N+1}{2} = \\frac{15+1}{2} = 8$.\n    The $8$th value in the sorted list is $4$. Therefore:\n    $$\n    \\mathrm{median}(|x_i|) = 4\\,\\mu\\text{V}\n    $$\n\n4.  **Calculate $\\hat{\\sigma}$:**\n    We now substitute the median and the expression for $k$ into the estimator formula:\n    $$\n    \\hat{\\sigma} = \\frac{4\\,\\mu\\text{V}}{\\Phi^{-1}(0.75)}\n    $$\n    The numerical value of $k = \\Phi^{-1}(0.75)$ is approximately $0.67448975$.\n    $$\n    \\hat{\\sigma} \\approx \\frac{4}{0.67448975}\\,\\mu\\text{V} \\approx 5.930436\\,\\mu\\text{V}\n    $$\n\n5.  **Round the result:**\n    The problem asks for the answer to be rounded to four significant figures. The first four significant figures of $5.930436$ are $5.930$. The fifth digit is $4$, so we round down.\n    $$\n    \\hat{\\sigma} \\approx 5.930\\,\\mu\\text{V}\n    $$\nThis value represents the robustly estimated standard deviation of the background noise.",
            "answer": "$$\n\\boxed{5.930}\n$$"
        },
        {
            "introduction": "Once a spike is detected, its waveform must be captured for feature extraction and subsequent sorting. The choice of the analysis window duration presents a fundamental trade-off: a longer window may capture more of a neuron's unique electrophysiological signature, but it also integrates more background noise and increases feature dimensionality. In this practice , you will mathematically formalize this trade-off by constructing and optimizing an objective function, providing a principled method for determining the ideal window length that maximizes the signal-to-noise content of your extracted features.",
            "id": "4194183",
            "problem": "A neuron is recorded extracellularly and two candidate spike templates, called $\\text{Template 1}$ and $\\text{Template 2}$, differ by a deterministic waveform difference $d(t)$ that, due to afterpotential and conduction variability, decays exponentially: $d(t) = A \\exp(-t/\\tau)$ for $t \\geq 0$, with $A  0$ and $\\tau  0$. The observed voltage is the sum of the underlying spike waveform and additive noise. The noise is modeled in discrete time as an independent, zero-mean process with variance $\\sigma^{2}$ per sample. The feature vector used for downstream sorting is formed by taking the raw samples over a contiguous window of duration $L$ (in seconds), starting at the spike’s peak alignment time, so the feature dimensionality grows approximately linearly with $L$ as $K(L) \\approx L/\\Delta t$, where $\\Delta t$ is the sampling interval.\n\nIn designing the window length $L$, there is a trade-off: longer windows incorporate more of the template-difference energy (which can improve separability of $\\text{Template 1}$ and $\\text{Template 2}$), but also accumulate more noise energy and increase feature dimensionality, which can degrade downstream estimator stability with finite data. To formalize this trade-off, define a net objective $F(L)$ as the difference between the expected integrated squared template-difference captured by the window and a penalty proportional to the expected integrated noise energy over the same window:\n$$\nF(L) \\equiv \\int_{0}^{L} d(t)^{2}\\,dt \\;-\\; \\beta \\,\\mathbb{E}\\!\\left[\\sum_{k=0}^{\\lfloor L/\\Delta t \\rfloor} n[k]^{2} \\,\\Delta t\\right],\n$$\nwhere $n[k]$ denotes the discrete-time noise at sample $k$, $\\beta0$ is a dimensionless regularization weight that encodes the downstream sorter’s sensitivity to feature dimensionality and noise energy, and $\\Delta t$ is the sampling interval. Assume the Riemann sum converges to the integral as $\\Delta t \\to 0$, and that $A^{2}  \\beta \\sigma^{2}$ so that the optimal window length is strictly positive.\n\nStarting only from these definitions and standard properties of independent, zero-mean noise (e.g., linearity of expectation and independence across samples), derive the analytic expression for the window length $L$ (in seconds) that maximizes $F(L)$. Your final answer must be a single closed-form expression. No rounding is required. Express the window length in seconds. Do not include units in your final boxed answer.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of signal processing and statistical estimation as applied to neuroscience data, well-posed as an optimization problem, and objectively stated with sufficient information for a unique solution. The assumptions provided are clear and standard for this type of modeling problem. We may therefore proceed with the derivation.\n\nThe objective is to find the window length $L$ that maximizes the function $F(L)$, defined as:\n$$\nF(L) = \\int_{0}^{L} d(t)^{2}\\,dt \\;-\\; \\beta \\,\\mathbb{E}\\!\\left[\\sum_{k=0}^{\\lfloor L/\\Delta t \\rfloor} n[k]^{2} \\,\\Delta t\\right]\n$$\nWe will analyze the two terms of $F(L)$ separately. The first term represents the integrated signal energy captured within the window, and the second term represents a penalty proportional to the total expected noise energy within the window.\n\nFirst, let us evaluate the signal term, which we denote as $S(L)$:\n$$\nS(L) = \\int_{0}^{L} d(t)^{2}\\,dt\n$$\nThe problem provides the deterministic waveform difference as $d(t) = A \\exp(-t/\\tau)$ for $t \\geq 0$. Squaring this function gives:\n$$\nd(t)^{2} = \\left(A \\exp(-t/\\tau)\\right)^{2} = A^{2} \\exp(-2t/\\tau)\n$$\nSubstituting this into the integral for $S(L)$:\n$$\nS(L) = \\int_{0}^{L} A^{2} \\exp(-2t/\\tau)\\,dt\n$$\nWe can perform this integration:\n$$\nS(L) = A^{2} \\left[ -\\frac{\\tau}{2} \\exp(-2t/\\tau) \\right]_{0}^{L}\n$$\n$$\nS(L) = A^{2} \\left( \\left(-\\frac{\\tau}{2} \\exp(-2L/\\tau)\\right) - \\left(-\\frac{\\tau}{2} \\exp(0)\\right) \\right)\n$$\n$$\nS(L) = \\frac{A^{2}\\tau}{2} \\left( 1 - \\exp(-2L/\\tau) \\right)\n$$\n\nNext, let us evaluate the noise penalty term, which we denote as $N_{p}(L)$:\n$$\nN_{p}(L) = \\beta \\,\\mathbb{E}\\!\\left[\\sum_{k=0}^{\\lfloor L/\\Delta t \\rfloor} n[k]^{2} \\,\\Delta t\\right]\n$$\nThe problem states that the discrete-time noise $n[k]$ is an independent, zero-mean process with variance $\\sigma^{2}$ per sample. The expectation of the square of a zero-mean random variable is its variance:\n$$\n\\mathbb{E}[n[k]^{2}] = \\text{Var}(n[k]) + (\\mathbb{E}[n[k]])^{2} = \\sigma^{2} + 0^{2} = \\sigma^{2}\n$$\nUsing the linearity of the expectation operator, we can move the expectation inside the sum:\n$$\nN_{p}(L) = \\beta \\left(\\sum_{k=0}^{\\lfloor L/\\Delta t \\rfloor} \\mathbb{E}[n[k]^{2}] \\,\\Delta t\\right)\n$$\nSubstituting $\\mathbb{E}[n[k]^{2}] = \\sigma^{2}$:\n$$\nN_{p}(L) = \\beta \\left(\\sum_{k=0}^{\\lfloor L/\\Delta t \\rfloor} \\sigma^{2} \\,\\Delta t\\right)\n$$\nThis sum is a scaled Riemann sum for the constant function $f(t) = \\sigma^2$ over the interval $[0, L]$. The problem explicitly states a simplifying assumption that \"the Riemann sum converges to the integral as $\\Delta t \\to 0$\". Applying this, the sum becomes an integral:\n$$\n\\lim_{\\Delta t \\to 0} \\sum_{k=0}^{\\lfloor L/\\Delta t \\rfloor} \\sigma^{2} \\,\\Delta t = \\int_{0}^{L} \\sigma^{2} \\,dt = \\sigma^{2}L\n$$\nTherefore, the noise penalty term simplifies to:\n$$\nN_{p}(L) = \\beta \\sigma^{2} L\n$$\n\nNow, we can write the full expression for the objective function $F(L)$:\n$$\nF(L) = S(L) - N_{p}(L) = \\frac{A^{2}\\tau}{2} \\left( 1 - \\exp(-2L/\\tau) \\right) - \\beta \\sigma^{2} L\n$$\nTo find the window length $L$ that maximizes $F(L)$, we must find the critical points by taking the derivative of $F(L)$ with respect to $L$ and setting it to zero. Using the Fundamental Theorem of Calculus is equivalent and simpler for the derivative of the integral term. The derivative of the first term is $d(L)^2 = A^2 \\exp(-2L/\\tau)$. The derivative of the second term is $\\beta \\sigma^2$.\n$$\n\\frac{dF}{dL} = \\frac{d}{dL} \\left( \\frac{A^{2}\\tau}{2} \\left( 1 - \\exp(-2L/\\tau) \\right) - \\beta \\sigma^{2} L \\right)\n$$\n$$\n\\frac{dF}{dL} = \\frac{A^{2}\\tau}{2} \\left( - \\exp(-2L/\\tau) \\cdot \\left(-\\frac{2}{\\tau}\\right) \\right) - \\beta \\sigma^{2}\n$$\n$$\n\\frac{dF}{dL} = A^{2} \\exp(-2L/\\tau) - \\beta \\sigma^{2}\n$$\nSetting the derivative to zero to find the optimal $L$:\n$$\nA^{2} \\exp(-2L/\\tau) - \\beta \\sigma^{2} = 0\n$$\n$$\nA^{2} \\exp(-2L/\\tau) = \\beta \\sigma^{2}\n$$\nNow, we solve for $L$. First, we isolate the exponential term:\n$$\n\\exp(-2L/\\tau) = \\frac{\\beta \\sigma^{2}}{A^{2}}\n$$\nTake the natural logarithm of both sides:\n$$\n\\ln\\left(\\exp(-2L/\\tau)\\right) = \\ln\\left(\\frac{\\beta \\sigma^{2}}{A^{2}}\\right)\n$$\n$$\n-\\frac{2L}{\\tau} = \\ln\\left(\\frac{\\beta \\sigma^{2}}{A^{2}}\\right)\n$$\nUsing the property $\\ln(1/x) = -\\ln(x)$:\n$$\n\\frac{2L}{\\tau} = -\\ln\\left(\\frac{\\beta \\sigma^{2}}{A^{2}}\\right) = \\ln\\left(\\left(\\frac{\\beta \\sigma^{2}}{A^{2}}\\right)^{-1}\\right) = \\ln\\left(\\frac{A^{2}}{\\beta \\sigma^{2}}\\right)\n$$\nFinally, solving for $L$:\n$$\nL = \\frac{\\tau}{2} \\ln\\left(\\frac{A^{2}}{\\beta \\sigma^{2}}\\right)\n$$\nTo confirm this is a maximum, we examine the second derivative of $F(L)$:\n$$\n\\frac{d^{2}F}{dL^{2}} = \\frac{d}{dL} \\left( A^{2} \\exp(-2L/\\tau) - \\beta \\sigma^{2} \\right) = A^{2} \\exp(-2L/\\tau) \\left(-\\frac{2}{\\tau}\\right) = -\\frac{2A^{2}}{\\tau} \\exp(-2L/\\tau)\n$$\nSince $A  0$ and $\\tau  0$ are given, $2A^2/\\tau$ is positive. The exponential term $\\exp(-2L/\\tau)$ is always positive. Therefore, $\\frac{d^{2}F}{dL^{2}}  0$ for all $L \\ge 0$, which confirms that the critical point corresponds to a global maximum.\nThe problem provides the condition $A^{2}  \\beta \\sigma^{2}$. This implies $\\frac{A^{2}}{\\beta \\sigma^{2}}  1$, and therefore $\\ln\\left(\\frac{A^{2}}{\\beta \\sigma^{2}}\\right)  0$. Since $\\tau  0$, this ensures that the optimal length $L$ is strictly positive, consistent with the problem's statement.",
            "answer": "$$\n\\boxed{\\frac{\\tau}{2} \\ln\\left(\\frac{A^{2}}{\\beta \\sigma^{2}}\\right)}\n$$"
        },
        {
            "introduction": "After developing a spike detection algorithm, rigorously evaluating its performance is paramount. This hands-on coding practice  guides you through the process of implementing two of the most important evaluation tools from first principles: the Receiver Operating Characteristic (ROC) curve and the Precision-Recall (PR) curve. By generating these curves and their associated area-under-the-curve metrics, you will gain a deeper understanding of how to measure classifier performance and why, in the context of the severe class imbalance often found in spike detection, the PR curve can offer more salient insights than the ROC curve.",
            "id": "4194179",
            "problem": "You are given detection outputs from extracellular spike detection experiments in the form of real-valued scores and binary ground-truth labels indicating whether each timestamped observation corresponds to a true spike event. The goal is to derive Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves from first principles by sweeping a decision threshold across all unique score levels, compute the areas under these curves, and evaluate which curve is more informative under class imbalance. You must construct a complete, runnable program that executes these steps on a fixed test suite and outputs the requested metrics in the specified format.\n\nFundamental base and definitions to be used:\n- Given a set of observations indexed by $i \\in \\{1,\\dots,M\\}$, let the score be $s_i \\in \\mathbb{R}$ and the ground-truth label be $y_i \\in \\{0,1\\}$, where $y_i = 1$ indicates a true spike and $y_i = 0$ indicates a non-spike.\n- For any decision threshold $\\tau \\in \\mathbb{R}$, the predicted class is $\\hat{y}_i(\\tau) = 1$ if $s_i \\ge \\tau$ and $\\hat{y}_i(\\tau) = 0$ otherwise.\n- Let $P = \\sum_{i=1}^{M} \\mathbb{1}\\{y_i = 1\\}$ be the total number of positives and $N = \\sum_{i=1}^{M} \\mathbb{1}\\{y_i = 0\\}$ be the total number of negatives. Assume $P \\ge 1$ and $N \\ge 1$.\n- At threshold $\\tau$, define the confusion-matrix counts: true positives $TP(\\tau)$, false positives $FP(\\tau)$, false negatives $FN(\\tau)$, and true negatives $TN(\\tau)$, following their standard definitions. From these, define:\n  - True Positive Rate (also called sensitivity or recall) $TPR(\\tau) = \\dfrac{TP(\\tau)}{P}$.\n  - False Positive Rate $FPR(\\tau) = \\dfrac{FP(\\tau)}{N}$.\n  - Precision $Prec(\\tau) = \\dfrac{TP(\\tau)}{TP(\\tau) + FP(\\tau)}$ when $TP(\\tau) + FP(\\tau) \\ge 1$.\n\nCurve construction mandate:\n- Construct the ROC curve as the set of points $\\{(FPR(\\tau), TPR(\\tau))\\}$ obtained by sweeping $\\tau$ over the set of unique score values $U = \\{u_1, u_2, \\dots, u_K\\}$ sorted in strictly decreasing order, and include the endpoints $(0,0)$ and $(1,1)$ implied by $\\tau = +\\infty$ and $\\tau = -\\infty$, respectively. At each unique level, all items with equal score must be included simultaneously in the prediction set.\n- Construct the PR curve as the set of points $\\{(Rec(\\tau), Prec(\\tau))\\}$ with $Rec(\\tau) = TPR(\\tau)$ computed at the same sequence of thresholds over $U$ in decreasing order, interpreted as a right-continuous, step-wise function of recall.\n\nArea-under-curve definitions:\n- Define the area under the ROC curve $AUC_{\\mathrm{ROC}}$ as the Riemann integral of $TPR$ with respect to $FPR$ over $[0,1]$, computed numerically by the trapezoidal rule over the piecewise-linear path formed by the ROC points in ascending $FPR$.\n- Define the area under the PR curve $AUC_{\\mathrm{PR}}$ as the Riemann–Stieltjes integral of $Prec$ with respect to $Rec$ over $[0,1]$, computed numerically as a step-wise sum using right-continuous precision: if $(Rec_k, Prec_k)$ are the PR points ordered by non-decreasing recall and $Rec_0 = 0$, then\n  $$AUC_{\\mathrm{PR}} = \\sum_{k=1}^{K} \\left(Rec_k - Rec_{k-1}\\right) \\cdot Prec_k.$$\n\nInformativeness under class imbalance:\n- Let the prevalence be $\\pi = \\dfrac{P}{P+N}$. Consider normalized deviations from chance baselines:\n  - For ROC, the random baseline is $0.5$, so define $I_{\\mathrm{ROC}} = \\dfrac{AUC_{\\mathrm{ROC}} - 0.5}{0.5}$.\n  - For PR, the random baseline is $\\pi$, so define $I_{\\mathrm{PR}} = \\dfrac{AUC_{\\mathrm{PR}} - \\pi}{1 - \\pi}$ for $\\pi \\in [0,1)$.\n- Define the boolean indicator $B$ that PR is more informative than ROC by $B = \\mathbb{1}\\{I_{\\mathrm{PR}}  I_{\\mathrm{ROC}}\\}$.\n\nTask:\n- Implement a program that, for each test case, builds the ROC and PR curves as above by sweeping thresholds over unique score values in strictly decreasing order, computes $AUC_{\\mathrm{ROC}}$ and $AUC_{\\mathrm{PR}}$, computes $\\pi$, $I_{\\mathrm{ROC}}$, $I_{\\mathrm{PR}}$, and then $B$.\n- For numerical reporting, round $AUC_{\\mathrm{ROC}}$ and $AUC_{\\mathrm{PR}}$ to $6$ decimal places. The boolean $B$ must be reported as either the literal $True$ or $False$.\n\nTest suite:\nProvide the following three test cases, each as a pair of lists $(\\mathbf{s}, \\mathbf{y})$, where $\\mathbf{s}$ are scores and $\\mathbf{y}$ are labels:\n- Case $1$ (moderately separable, roughly balanced):\n  - $\\mathbf{s}^{(1)} = [\\,0.95,\\,0.90,\\,0.85,\\,0.80,\\,0.75,\\,0.70,\\,0.65,\\,0.60,\\,0.55,\\,0.50,\\,0.45,\\,0.40\\,]$\n  - $\\mathbf{y}^{(1)} = [\\,1,\\,1,\\,1,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0\\,]$\n- Case $2$ (severely imbalanced with a hard positive near low score):\n  - $\\mathbf{s}^{(2)} = [\\,0.99,\\,0.80,\\,0.75,\\,0.70,\\,0.65,\\,0.60,\\,0.58,\\,0.57,\\,0.56,\\,0.55,\\,0.54,\\,0.53,\\,0.52,\\,0.50,\\,0.49,\\,0.48,\\,0.30,\\,0.25,\\,0.20,\\,0.10\\,]$\n  - $\\mathbf{y}^{(2)} = [\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,0,\\,0,\\,0\\,]$\n- Case $3$ (all scores tied; boundary behavior):\n  - $\\mathbf{s}^{(3)} = [\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50,\\,0.50\\,]$\n  - $\\mathbf{y}^{(3)} = [\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,0\\,]$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a list of lists, one inner list per test case, in the same order as above. Each inner list must be of the form $[AUC\\_{{ROC}}, AUC\\_{{PR}}, B]$, where $AUC\\_{{ROC}}$ and $AUC\\_{{PR}}$ are floats rounded to $6$ decimal places, and $B$ is a boolean. For example, an output with three test cases must look exactly like\n  \"[[0.812345,0.701234,True],[0.923456,0.812345,True],[0.500000,0.300000,False]]\"\nwith no extra spaces and no additional text.",
            "solution": "The problem requires the implementation of an algorithm to compute Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves, along with their respective areas under the curve ($AUC_{\\mathrm{ROC}}$ and $AUC_{\\mathrm{PR}}$), from a given set of detection scores and ground-truth labels. Furthermore, it asks for a comparison of their informativeness under class imbalance. The entire process must be derived from first principles as specified.\n\nThe core of the methodology is to evaluate the classification performance at various decision thresholds. A sample is classified as positive if its score $s_i$ is greater than or equal to a threshold $\\tau$, i.e., $\\hat{y}_i(\\tau) = 1$ if $s_i \\ge \\tau$. By varying $\\tau$ from $+\\infty$ down to $-\\infty$, we can trace the performance of the classifier, which forms the basis for the ROC and PR curves.\n\nFirst, let's establish the necessary quantities. We are given $M$ observations, each with a score $s_i$ and a binary label $y_i \\in \\{0, 1\\}$. The total number of positive instances (true spikes) is $P = \\sum_{i=1}^{M} \\mathbb{1}\\{y_i = 1\\}$, and the total number of negative instances (non-spikes) is $N = \\sum_{i=1}^{M} \\mathbb{1}\\{y_i = 0\\}$. The problem assumes $P \\ge 1$ and $N \\ge 1$.\n\nAt a given threshold $\\tau$, the four cardinal outcomes of binary classification are:\n- True Positives ($TP(\\tau)$): Number of instances where $s_i \\ge \\tau$ and $y_i = 1$.\n- False Positives ($FP(\\tau)$): Number of instances where $s_i \\ge \\tau$ and $y_i = 0$.\n- True Negatives ($TN(\\tau)$): Number of instances where $s_i  \\tau$ and $y_i = 0$.\n- False Negatives ($FN(\\tau)$): Number of instances where $s_i  \\tau$ and $y_i = 1$.\n\nFrom these, we define the key rates:\n- True Positive Rate (TPR), or Recall: $TPR(\\tau) = Rec(\\tau) = \\frac{TP(\\tau)}{P}$. This is the fraction of actual positives that are correctly identified.\n- False Positive Rate (FPR): $FPR(\\tau) = \\frac{FP(\\tau)}{N}$. This is the fraction of actual negatives that are incorrectly identified as positive.\n- Precision: $Prec(\\tau) = \\frac{TP(\\tau)}{TP(\\tau) + FP(\\tau)}$, defined for $TP(\\tau) + FP(\\tau) \\ge 1$. This is the fraction of predicted positives that are actually correct.\n\nThe algorithm proceeds as follows:\n\n**Step 1: Data Preparation and Sorting**\nThe fundamental insight for constructing these curves efficiently is that the values of $TP(\\tau)$, $FP(\\tau)$, and consequently all derived metrics, only change when the threshold $\\tau$ crosses a score value present in the data. Therefore, instead of sweeping $\\tau$ continuously, we only need to consider the unique score values as thresholds.\n\nThe problem mandates that we sort the unique scores in strictly decreasing order. A more robust and standard algorithm, which correctly handles tied scores as specified, is to sort all individual observations. We combine the scores $s_i$ and labels $y_i$ into pairs $(s_i, y_i)$ and sort these pairs in descending order based on the score $s_i$. In case of tied scores, the relative order of labels does not affect the calculation of cumulative $TP$ and $FP$ counts for that threshold level.\n\n**Step 2: Curve Point Generation**\nWe iterate through the sorted list of observations, effectively lowering the decision threshold. The problem specifies that all items with an equal score must be included simultaneously. This is achieved by processing observations in groups with tied scores.\n\nLet's initialize the cumulative counts $TP_{cum} = 0$ and $FP_{cum} = 0$. These correspond to a threshold $\\tau  \\max(s_i)$, where nothing is classified as positive. This gives us the starting point for the ROC curve, $(FPR, TPR) = (0, 0)$.\n\nWe then iterate through the sorted observations, grouping them by score. For each group of observations with a unique score value $u$:\n1. Count the number of positives ($\\Delta TP$) and negatives ($\\Delta FP$) within this group.\n2. Update the cumulative counts: $TP_{cum} \\leftarrow TP_{cum} + \\Delta TP$ and $FP_{cum} \\leftarrow FP_{cum} + \\Delta FP$. These updated counts represent the total true and false positives for a threshold $\\tau=u$.\n3. Calculate the new point on the curves:\n   - $FPR_{new} = \\frac{FP_{cum}}{N}$\n   - $TPR_{new} = \\frac{TP_{cum}}{P}$\n   - $Prec_{new} = \\frac{TP_{cum}}{TP_{cum} + FP_{cum}}$\n4. A new ROC point $(FPR_{new}, TPR_{new})$ and a new PR point $(TPR_{new}, Prec_{new})$ are generated. This process is repeated for all unique score values. The final state after processing all observations will have $TP_{cum} = P$ and $FP_{cum} = N$, yielding the ROC endpoint $(1, 1)$.\n\n**Step 3: Area Under Curve (AUC) Calculation**\n- **$AUC_{\\mathrm{ROC}}$**: The ROC curve is the set of generated $(FPR, TPR)$ points, starting from $(0,0)$ and ending at $(1,1)$. The area is computed using the trapezoidal rule on the piecewise-linear path connecting these points in order of increasing $FPR$. For a sequence of ROC points $(x_k, y_k)$ where $x_k = FPR_k$ and $y_k = TPR_k$, the area is:\n  $$AUC_{\\mathrm{ROC}} = \\sum_{k=1}^{L} \\frac{(y_k + y_{k-1})}{2} (x_k - x_{k-1})$$\n  where $L+1$ is the number of points in the ROC path.\n- **$AUC_{\\mathrm{PR}}$**: The PR curve is given by the points $(Rec_k, Prec_k)$, where $Rec_k = TPR_k$. The area is defined as an integral of a right-continuous step function. The provided formula is a numerical computation of this integral, equivalent to what is often called Average Precision:\n  $$AUC_{\\mathrm{PR}} = \\sum_{k=1}^{K} (Rec_k - Rec_{k-1}) \\cdot Prec_k$$\n  Here, $(Rec_k, Prec_k)$ are the PR points generated in order of non-decreasing recall, and we define $Rec_0 = 0$. Each term in the sum represents the area of a rectangle with height $Prec_k$ and width $(Rec_k - Rec_{k-1})$, which is the increase in recall at step $k$.\n\n**Step 4: Informativeness Comparison**\nThe curves' informativeness is assessed by normalizing their AUC values with respect to the performance of a random classifier.\n- For ROC, a random classifier yields an $AUC_{\\mathrm{ROC}}$ of $0.5$. The normalized information is $I_{\\mathrm{ROC}} = \\frac{AUC_{\\mathrm{ROC}} - 0.5}{0.5}$.\n- For PR, a random classifier yields an $AUC_{\\mathrm{PR}}$ equal to the prevalence of the positive class, $\\pi = \\frac{P}{P+N}$. The normalized information is $I_{\\mathrm{PR}} = \\frac{AUC_{\\mathrm{PR}} - \\pi}{1 - \\pi}$.\nThe boolean indicator $B = \\mathbb{1}\\{I_{\\mathrm{PR}}  I_{\\mathrm{ROC}}\\}$ determines if the PR curve is more informative than the ROC curve under this specific normalization. This comparison is particularly relevant in cases of class imbalance (i.e., when $\\pi$ is far from $0.5$), where the PR curve is often considered a more sensitive measure of performance than the ROC curve. The ROC baseline is constant, while the PR baseline changes with imbalance, making the latter a more challenging benchmark to beat.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the full analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (moderately separable, roughly balanced)\n        (\n            [0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40],\n            [1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n        ),\n        # Case 2 (severely imbalanced with a hard positive near low score)\n        (\n            [0.99, 0.80, 0.75, 0.70, 0.65, 0.60, 0.58, 0.57, 0.56, 0.55, 0.54, 0.53, 0.52, 0.50, 0.49, 0.48, 0.30, 0.25, 0.20, 0.10],\n            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n        ),\n        # Case 3 (all scores tied; boundary behavior)\n        (\n            [0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50],\n            [1, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n        )\n    ]\n\n    all_results = []\n    for scores, labels in test_cases:\n        result = _calculate_metrics(np.array(scores), np.array(labels))\n        all_results.append(result)\n\n    # Format the final output string\n    formatted_results = []\n    for auc_roc, auc_pr, b_flag in all_results:\n        formatted_results.append(f\"[{auc_roc:.6f},{auc_pr:.6f},{str(b_flag)}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _calculate_metrics(scores, labels):\n    \"\"\"\n    Calculates ROC/PR AUC and informativeness for a single test case.\n    \"\"\"\n    # Total number of positive and negative samples\n    pos_total = np.sum(labels == 1)\n    neg_total = len(labels) - pos_total\n\n    # The problem assumes P=1 and N=1, so no need to check for zero division here.\n    \n    # Combine scores and labels, then sort by score descending.\n    # A secondary sort key is not strictly needed for the logic but is good practice.\n    indices = np.argsort(scores, kind='mergesort')[::-1]\n    sorted_scores = scores[indices]\n    sorted_labels = labels[indices]\n    \n    roc_fpr = [0.0]\n    roc_tpr = [0.0]\n    pr_points = []\n\n    tp_cum = 0\n    fp_cum = 0\n    i = 0\n    while i  len(sorted_scores):\n        current_score = sorted_scores[i]\n        \n        # Group all items with the same score\n        j = i\n        while j  len(sorted_scores) and sorted_scores[j] == current_score:\n            j += 1\n        \n        # Count positives and negatives in this tied-score group\n        labels_in_group = sorted_labels[i:j]\n        delta_tp = np.sum(labels_in_group == 1)\n        \n        # Update cumulative counts. This happens after processing the group.\n        tp_cum += delta_tp\n        fp_cum += (j - i) - delta_tp\n\n        # Calculate metrics for this new threshold level\n        tpr = tp_cum / pos_total\n        fpr = fp_cum / neg_total\n        \n        roc_tpr.append(tpr)\n        roc_fpr.append(fpr)\n        \n        # Precision is only defined when TP+FP  0\n        if (tp_cum + fp_cum)  0:\n            precision = tp_cum / (tp_cum + fp_cum)\n            pr_points.append((tpr, precision))\n\n        i = j\n\n    # Compute Area Under the ROC Curve using the trapezoidal rule\n    # np.trapz integrates y (tpr) with respect to x (fpr)\n    auc_roc = np.trapz(roc_tpr, roc_fpr)\n\n    # Compute Area Under the PR Curve (Average Precision)\n    auc_pr = 0.0\n    last_recall = 0.0\n    # pr_points are naturally sorted by non-decreasing recall\n    for recall, precision in pr_points:\n        auc_pr += (recall - last_recall) * precision\n        last_recall = recall\n\n    # Compute informativeness metrics\n    prevalence = pos_total / (pos_total + neg_total)\n    \n    # ROC informativeness\n    i_roc = (auc_roc - 0.5) / 0.5\n    \n    # PR informativeness\n    # Prevent division by zero if prevalence is 1 (all positive samples)\n    if prevalence  1.0:\n        i_pr = (auc_pr - prevalence) / (1 - prevalence)\n    else: # If pi=1, the denominator is 0. By convention, if auc_pr=1, info=1, else can be undefined/0.\n          # For this problem, N=1, so pi  1 is always true.\n        i_pr = 0.0\n\n    b_flag = i_pr  i_roc\n    \n    return auc_roc, auc_pr, b_flag\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}