{
    "hands_on_practices": [
        {
            "introduction": "Before searching for patterns of neural synchrony, it is essential to first understand what the cross-correlogram should look like in their absence. This foundational exercise establishes the theoretical baseline by modeling two neurons as independent homogeneous Poisson processes, a standard representation for random, uncorrelated firing. By deriving the expected value of the cross-correlogram under these null-hypothesis conditions, you will demonstrate that it is flat, providing a crucial benchmark against which to compare correlograms from real data where peaks might signify meaningful interactions .",
            "id": "4151144",
            "problem": "Two simultaneously recorded neurons produce spike trains modeled as independent homogeneous Poisson point processes on the observation interval $[0,T]$, denoted $X$ and $Y$, with constant intensities (rates) $\\lambda_{x}$ and $\\lambda_{y}$, respectively. By homogeneity, for any interval of length $L$, the spike count has expectation $\\lambda L$, and by independence, spike counts in disjoint intervals and across processes are independent. Consider the following binned cross-correlogram estimator of lagged coincidences per unit time at lag $\\tau \\in \\mathbb{R}$ with bin width $\\Delta > 0$:\n$$\n\\hat{C}_{xy}(\\tau) \\equiv \\frac{1}{T} \\sum_{t_{i} \\in X \\cap [0,T]} N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big),\n$$\nwhere $N_{Y}(I)$ denotes the spike count of process $Y$ in interval $I$. Assume $T$ is large and $|\\tau| + \\Delta \\ll T$ so that boundary effects can be neglected for the purpose of computing expectations. Starting only from the definitions of a homogeneous Poisson process and the independence of $X$ and $Y$, derive the expected value $\\mathbb{E}[\\hat{C}_{xy}(\\tau)]$ as an explicit expression in $\\lambda_{x}$, $\\lambda_{y}$, and $\\Delta$, and show that it does not depend on $\\tau$. Provide your final answer as a single closed-form analytic expression. No numerical approximation is required.",
            "solution": "The problem asks for the expected value of the binned cross-correlogram estimator, $\\mathbb{E}[\\hat{C}_{xy}(\\tau)]$, for two independent homogeneous Poisson spike trains, $X$ and $Y$, with constant intensities $\\lambda_x$ and $\\lambda_y$ respectively.\n\nThe estimator is defined as:\n$$\n\\hat{C}_{xy}(\\tau) \\equiv \\frac{1}{T} \\sum_{t_{i} \\in X \\cap [0,T]} N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big)\n$$\nwhere $T$ is the observation interval duration, $\\Delta$ is the bin width, $\\tau$ is the time lag, $t_i$ are the spike times of process $X$, and $N_Y(I)$ is the number of spikes from process $Y$ in a given interval $I$.\n\nWe begin by applying the expectation operator to the definition of $\\hat{C}_{xy}(\\tau)$:\n$$\n\\mathbb{E}[\\hat{C}_{xy}(\\tau)] = \\mathbb{E}\\left[ \\frac{1}{T} \\sum_{t_{i} \\in X \\cap [0,T]} N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big) \\right]\n$$\nBy the linearity of expectation, the constant factor $\\frac{1}{T}$ can be moved outside the expectation:\n$$\n\\mathbb{E}[\\hat{C}_{xy}(\\tau)] = \\frac{1}{T} \\mathbb{E}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big) \\right]\n$$\nThe summation is over the spike times $\\{t_i\\}$ of the point process $X$, which are themselves random variables. To evaluate the expectation of this sum, we apply the law of total expectation (also known as the tower property), by conditioning on the realization of the spike train $X$. Let the specific realization of the process $X$ be the set of spike times $\\{t_i\\}$.\n$$\n\\mathbb{E}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} \\dots \\right] = \\mathbb{E}_{X}\\left[ \\mathbb{E}_{Y}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big) \\bigg| X = \\{t_i\\} \\right] \\right]\n$$\nThe problem states that the processes $X$ and $Y$ are independent. Therefore, conditioning on $X$ does not alter the statistics of $Y$. The inner expectation over $Y$ can be calculated by treating the spike times $\\{t_i\\}$ of $X$ as fixed values.\n$$\n= \\mathbb{E}_{X}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} \\mathbb{E}_{Y}\\left[ N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big) \\right] \\right]\n$$\nNext, we evaluate the inner expectation, $\\mathbb{E}_{Y}[\\dots]$. The process $Y$ is a homogeneous Poisson process with constant intensity $\\lambda_y$. A fundamental property of such a process is that the expected number of events in an interval of length $L$ is $\\lambda_y L$. The interval of interest is $[t_i + \\tau, t_i + \\tau + \\Delta)$, which has a length of $\\Delta$. Thus, the expected count is:\n$$\n\\mathbb{E}_{Y}\\left[ N_{Y}\\big([t_{i} + \\tau,\\, t_{i} + \\tau + \\Delta)\\big) \\right] = \\lambda_y \\Delta\n$$\nThis result is a constant, independent of the specific spike time $t_i$ and the lag $\\tau$. This is a direct consequence of the homogeneity of process $Y$. Substituting this constant value back into our main expression:\n$$\n\\mathbb{E}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} \\dots \\right] = \\mathbb{E}_{X}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} (\\lambda_y \\Delta) \\right]\n$$\nThe constant term $\\lambda_y \\Delta$ can be factored out of the summation:\n$$\n= \\mathbb{E}_{X}\\left[ (\\lambda_y \\Delta) \\sum_{t_{i} \\in X \\cap [0,T]} 1 \\right]\n$$\nThe sum $\\sum_{t_{i} \\in X \\cap [0,T]} 1$ is, by definition, the total number of spikes of process $X$ within the observation interval $[0, T]$. This is denoted as $N_X([0, T])$.\n$$\n= \\mathbb{E}_{X}\\left[ (\\lambda_y \\Delta) N_X([0, T]) \\right]\n$$\nFactoring the constant term $\\lambda_y \\Delta$ out of the expectation gives:\n$$\n= (\\lambda_y \\Delta) \\mathbb{E}_{X}\\left[ N_X([0, T]) \\right]\n$$\nNow, we must evaluate the expected number of spikes for process $X$. Since $X$ is a homogeneous Poisson process with constant intensity $\\lambda_x$, the expected number of spikes in an interval of length $T$ is:\n$$\n\\mathbb{E}_{X}\\left[ N_X([0, T]) \\right] = \\lambda_x T\n$$\nThe problem states that we can neglect boundary effects, which is justified by the assumption $|\\tau| + \\Delta \\ll T$. This ensures our calculation holds for the entire interval.\n\nCombining the pieces, the expectation of the summation term is:\n$$\n\\mathbb{E}\\left[ \\sum_{t_{i} \\in X \\cap [0,T]} \\dots \\right] = (\\lambda_y \\Delta) (\\lambda_x T) = \\lambda_x \\lambda_y T \\Delta\n$$\nFinally, we substitute this result back into the expression for $\\mathbb{E}[\\hat{C}_{xy}(\\tau)]$:\n$$\n\\mathbb{E}[\\hat{C}_{xy}(\\tau)] = \\frac{1}{T} (\\lambda_x \\lambda_y T \\Delta) = \\lambda_x \\lambda_y \\Delta\n$$\nThis expression is the expected value of the cross-correlogram. It depends on the firing rates of the two neurons, $\\lambda_x$ and $\\lambda_y$, and the bin width, $\\Delta$. As required, the result does not depend on the lag $\\tau$, a characteristic feature for independent homogeneous processes, representing the expected rate of chance coincidences.",
            "answer": "$$\n\\boxed{\\lambda_{x} \\lambda_{y} \\Delta}\n$$"
        },
        {
            "introduction": "Moving from theory to practice, the computational methods used to generate a cross-correlogram can introduce significant artifacts if not applied carefully. A common and efficient method involves the Fast Fourier Transform (FFT), but this technique implicitly assumes the data is periodic, which can cause \"wrap-around\" errors for finite, non-periodic neural recordings. This thought experiment  provides a clear and intuitive example of how this artifact can create a spurious peak at a small time lag, potentially leading to a false conclusion of fast neural synchrony.",
            "id": "4151103",
            "problem": "A laboratory records spike trains from two cortical neurons during a single trial of duration $T = 1$ s, binned at width $\\Delta t = 10$ ms, yielding $N = 100$ bins indexed by $n = 0, 1, \\dots, 99$. Define two binary sequences $x[n]$ and $y[n]$ where $x[n] = 1$ for $n \\in \\{0,1,2,3,4\\}$ and $x[n] = 0$ otherwise, and $y[n] = 1$ for $n \\in \\{95,96,97,98,99\\}$ and $y[n] = 0$ otherwise. You wish to assess neural synchrony by computing the cross-correlation function (CCF) between $x[n]$ and $y[n]$.\n\nConsider two computation protocols used in practice:\n- A linear, aperiodic cross-correlation that treats out-of-range indices as zero (often implemented directly in the time domain or via the Fast Fourier Transform (FFT) with appropriate zero-padding).\n- A circular, periodic cross-correlation that implicitly wraps indices modulo $N$ (as occurs when FFT-based correlation is computed without sufficient zero-padding, thereby assuming periodic extension).\n\nAssume the researcher mistakenly applies the circular, periodic interpretation to these finite, non-periodic spike trains. Which option best characterizes the difference in the correlograms and the interpretational consequence for assessing synchrony in this dataset?\n\nA. The circular method produces a prominent peak at lag $k = 5$ bins due to wrap-around, whereas the linear method places the same overlap at lag $k = -95$ bins; the small-lag peak from the circular method is spurious for non-periodic neural data because it aliases end-of-trial activity with start-of-trial activity.\n\nB. Both methods produce identical correlograms with a single peak at lag $k = -95$ bins; wrap-around has no effect when the spike trains are binary.\n\nC. The circular method suppresses all boundary effects and yields no peak because the spikes are separated by almost the entire trial; the linear method also shows no correlation at any lag due to non-overlapping support.\n\nD. Detrending the spike counts before cross-correlation eliminates wrap-around artifacts, so both methods yield only physiologically meaningful peaks near $k = 0$ bins regardless of periodic assumptions.",
            "solution": "To determine the correct option, we must analyze the results of both linear and circular cross-correlation for the given signals.\n\n**1. Linear Cross-Correlation**\n\nThe linear cross-correlation function is computed by sliding one signal past the other and calculating their overlap at each lag, treating any values outside the original signal boundaries as zero. Signal $x[n]$ contains spikes at bins $\\{0,1,2,3,4\\}$, and signal $y[n]$ contains spikes at bins $\\{95,96,97,98,99\\}$.\n\nTo find the lag where these signals align, we need to shift one to match the other. To align the start of the spike burst in $y[n]$ (at bin 95) with the start of the burst in $x[n]$ (at bin 0), we must shift $y[n]$ to the left by 95 bins. A left shift corresponds to a negative lag. Therefore, the maximum correlation will occur at a lag of $k = -95$ bins. At this lag, the five spikes in both signals perfectly overlap, producing a strong peak in the correlogram. At small lags (e.g., $k$ near 0), the signals do not overlap, so the correlation is zero.\n\n**2. Circular Cross-Correlation**\n\nCircular cross-correlation, often an unintended consequence of FFT-based methods without proper zero-padding, assumes the signals are periodic. This means the end of the signal (at bin 99) \"wraps around\" to connect to the beginning (at bin 0).\n\nThe true peak at a lag of $k = -95$ bins still exists. However, due to the circular (periodic) nature of the calculation, this lag is equivalent to a lag of $k = -95 + N = -95 + 100 = +5$ bins. Let's verify this. To compute the correlation at a lag of $k=5$, we are comparing $x[n]$ to a right-shifted version of $y[n]$. The spikes from $y[n]$ at bins $\\{95, 96, 97, 98, 99\\}$ are effectively shifted to bins $\\{(95+5)\\pmod{100}, (96+5)\\pmod{100}, \\dots\\}$, which are $\\{0, 1, 2, 3, 4\\}$. This shifted version of $y[n]$ perfectly aligns with $x[n]$.\n\n**Consequence for Interpretation**\n\nThe circular method incorrectly creates a strong correlation peak at a small, positive lag of $k=5$ bins (which corresponds to $5 \\times 10 \\text{ ms} = 50 \\text{ ms}$). A neuroscientist looking for fast synaptic interactions (which occur at small lags) might mistakenly interpret this \"wrap-around\" artifact as evidence of a fast, functional connection. The linear method correctly shows that the only correlation occurs at a very long lag of $-95$ bins ($-950$ ms), reflecting the actual separation of the spike bursts in time.\n\nTherefore, the circular method creates a spurious small-lag peak by aliasing activity from the end of the trial with activity from the beginning. Option A accurately describes this scenario and its consequences.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once a correlogram is correctly computed and artifacts are avoided, the final step is to determine if any observed peaks are statistically significant or merely the result of random chance. Since we test for correlations across a range of time lags, we face a multiple comparisons problem, where the probability of finding a false positive increases with the number of lags tested. This capstone exercise  guides you through implementing a robust, permutation-based statistical test—the max-statistic approach—to control this Family-Wise Error Rate (FWER) and confidently identify genuine instances of neural synchrony.",
            "id": "4151171",
            "problem": "Consider two discrete-time spike-count sequences $x_{t}$ and $y_{t}$ sampled at a fixed bin width $d t$ over a duration $T$, where $t \\in \\{0,1,2,\\dots,N-1\\}$ and $N = \\lfloor T / d t \\rfloor$. The goal is to assess neural synchrony via the cross-correlogram while controlling for the increased false positive risk arising from multiple comparisons across lags. Begin from the following base:\n\n- The Pearson correlation coefficient between two finite sequences $a_{t}$ and $b_{t}$ of equal length $m$ is defined by\n$$\nr(a,b) = \\frac{\\sum_{t=0}^{m-1} (a_{t} - \\bar{a})(b_{t} - \\bar{b})}{\\sqrt{\\sum_{t=0}^{m-1}(a_{t}-\\bar{a})^{2}} \\sqrt{\\sum_{t=0}^{m-1}(b_{t}-\\bar{b})^{2}}},\n$$\nwhere $\\bar{a}$ and $\\bar{b}$ are the sample means of $a_{t}$ and $b_{t}$, respectively.\n\n- The cross-correlogram between $x_{t}$ and $y_{t}$ is the function of integer lag $\\ell$ defined by computing the Pearson correlation coefficient between the overlapped segments of $x_{t}$ and $y_{t}$ at that lag, i.e., for $\\ell \\geq 0$\n$$\nr(\\ell) = r\\big(x_{0:(N-1-\\ell)}, y_{\\ell:(N-1)}\\big),\n$$\nand for $\\ell < 0$\n$$\nr(\\ell) = r\\big(x_{(-\\ell):(N-1)}, y_{0:(N-1+\\ell)}\\big).\n$$\n\n- The Family-Wise Error Rate (FWER) is the probability of making at least one false rejection among a family of simultaneous hypothesis tests. In this setting, each lag $\\ell \\in \\{-L,\\dots,+L\\}$ defines one test for excess correlation at that lag. To control FWER at level $\\alpha$, you must use a max-statistic approach: under the null hypothesis of no true synchrony, generate null surrogates by circularly shifting $y_{t}$ by a random offset (uniformly distributed over allowed shifts that preserve stationarity) to destroy alignment while preserving marginal properties, compute the cross-correlogram for each surrogate, take the maximum absolute correlation across lags for each surrogate, and use the empirical $(1-\\alpha)$ quantile of these maxima as a threshold $t_{\\alpha}$ that controls FWER over the entire family of lag tests. Then declare significant those observed lags whose absolute observed correlation exceeds $t_{\\alpha}$.\n\nTask. Implement a program that:\n\n1. Simulates binned spike-count sequences $x_{t}$ and $y_{t}$ with realistic neural spiking structure by combining independent Poisson spike-generation with an additive synchronous component:\n   - Independent background: draw $x_{t}$ and $y_{t}$ counts as independent Poisson random variables per bin with means $\\lambda_{x} d t$ and $\\lambda_{y} d t$, respectively.\n   - Synchronous component: draw a Poisson number of synchronous events with rate $\\lambda_{\\mathrm{sync}}$ over $[0,T)$, place each event time $u$ in $x_{t}$ at $u + \\varepsilon_{x}$ and in $y_{t}$ at $u + \\tau + \\varepsilon_{y}$, where $\\tau$ is a fixed synchrony lag and $\\varepsilon_{x}, \\varepsilon_{y}$ are independent Gaussian jitters with standard deviation $\\sigma$ (values are clamped to remain in $[0,T)$). Bin these events into $x_{t}$ and $y_{t}$ by incrementing the appropriate bin count.\n\n2. Computes the observed cross-correlogram $r(\\ell)$ for integer lags $\\ell \\in \\{-L,\\dots,+L\\}$, where $L$ is specified in seconds and converted to bins by $L_{\\mathrm{bins}} = \\lfloor L / d t \\rfloor$.\n\n3. Constructs a null distribution for the max-statistic by generating $K$ circular-shift surrogates of $y_{t}$ (each surrogate $y^{(k)}_{t}$ is $y_{(t+s_{k}) \\bmod N}$ for a randomly chosen shift $s_{k}$), computing the cross-correlogram $r^{(k)}(\\ell)$ for each surrogate, and recording $M_{k} = \\max_{\\ell \\in \\{-L,\\dots,+L\\}} \\left| r^{(k)}(\\ell) \\right|$.\n\n4. Determines the threshold $t_{\\alpha}$ as the empirical $(1-\\alpha)$ quantile of $\\{M_{1},\\dots,M_{K}\\}$ and returns the set of observed lags $\\ell$ such that $\\left| r(\\ell) \\right| \\geq t_{\\alpha}$, expressed as lag values in seconds $\\ell \\cdot d t$ rounded to three decimals.\n\nAssumptions and constraints:\n- You must treat $\\ell$ as an integer number of bins and convert lag values to seconds for the final output.\n- Use circular shifts for surrogates to preserve stationarity of $y_{t}$ while breaking alignment with $x_{t}$ under the null hypothesis of no synchrony.\n- If any overlapped segment has zero variance (all counts identical), define its correlation as $0$ for that lag to avoid undefined values.\n- All time quantities must be expressed in seconds; lag outputs must be rounded to three decimals in seconds.\n- The output per test case must be a list of floats (lag values in seconds), and the aggregated program output must be a single list containing one such list per test case.\n\nTest suite:\nImplement the program for the following parameter sets. Each parameter set is a tuple specifying $(T, d t, \\lambda_{x}, \\lambda_{y}, \\lambda_{\\mathrm{sync}}, \\tau, \\sigma, L, K, \\alpha, \\text{seed})$. Use the seeds as provided to ensure deterministic outputs.\n\n- Case $1$: $(T = \\, 20, d t = \\, 0.005, \\lambda_{x} = \\, 15, \\lambda_{y} = \\, 15, \\lambda_{\\mathrm{sync}} = \\, 5, \\tau = \\, 0, \\sigma = \\, 0.003, L = \\, 0.025, K = \\, 200, \\alpha = \\, 0.05, \\text{seed} = \\, 101)$.\n\n- Case $2$: $(T = \\, 20, d t = \\, 0.005, \\lambda_{x} = \\, 15, \\lambda_{y} = \\, 15, \\lambda_{\\mathrm{sync}} = \\, 5, \\tau = \\, 0.015, \\sigma = \\, 0.003, L = \\, 0.025, K = \\, 200, \\alpha = \\, 0.05, \\text{seed} = \\, 102)$.\n\n- Case $3$: $(T = \\, 20, d t = \\, 0.005, \\lambda_{x} = \\, 15, \\lambda_{y} = \\, 15, \\lambda_{\\mathrm{sync}} = \\, 0, \\tau = \\, 0, \\sigma = \\, 0, L = \\, 0.025, K = \\, 200, \\alpha = \\, 0.05, \\text{seed} = \\, 103)$.\n\n- Case $4$ (boundary condition with zero-lag only): $(T = \\, 10, d t = \\, 0.005, \\lambda_{x} = \\, 20, \\lambda_{y} = \\, 20, \\lambda_{\\mathrm{sync}} = \\, 5, \\tau = \\, 0, \\sigma = \\, 0.002, L = \\, 0, K = \\, 200, \\alpha = \\, 0.05, \\text{seed} = \\, 104)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list of significant lag values (in seconds, rounded to three decimals) for one test case. For example, an output with four test cases must look like $[\\,[0.000,0.015],\\,[0.000],\\,[\\,],\\,[0.000]\\,]$, where an empty list $[\\,]$ indicates no significant lags detected.",
            "solution": "The approach is grounded in statistical testing principles and the definition of the cross-correlogram. We proceed step by step.\n\n1. Fundamental definitions. For discrete-time spike-count sequences $x_{t}$ and $y_{t}$ sampled at bin width $d t$ over duration $T$ with $N = \\lfloor T / d t \\rfloor$ bins, the Pearson correlation coefficient is defined for two equal-length segments by\n$$\nr(a,b) = \\frac{\\sum_{t=0}^{m-1} (a_{t} - \\bar{a})(b_{t} - \\bar{b})}{\\sqrt{\\sum_{t=0}^{m-1}(a_{t}-\\bar{a})^{2}} \\sqrt{\\sum_{t=0}^{m-1}(b_{t}-\\bar{b})^{2}}}.\n$$\nThe cross-correlogram $r(\\ell)$ across integer lags $\\ell$ uses this correlation on overlapped segments. For $\\ell \\geq 0$,\n$$\nr(\\ell) = r\\big(x_{0:(N-1-\\ell)}, y_{\\ell:(N-1)}\\big),\n$$\nand for $\\ell < 0$,\n$$\nr(\\ell) = r\\big(x_{(-\\ell):(N-1)}, y_{0:(N-1+\\ell)}\\big).\n$$\nIf either overlapped segment has zero variance, the correlation is undefined; we set $r(\\ell) = 0$ for robustness.\n\n2. Multiple comparisons across lags and Family-Wise Error Rate (FWER). Testing $H_{0}:\\text{no synchrony}$ at each lag $\\ell \\in \\{-L,\\dots,+L\\}$ introduces a family of tests. The Family-Wise Error Rate (FWER) is $P(\\text{at least one false positive among the tests})$. If each lag test is run at level $\\alpha$ independently, the probability of at least one false positive increases with the number of lags. To control FWER at level $\\alpha$ without overly conservative corrections, we use a max-statistic approach tailored to the dependency structure across lags.\n\n3. Max-statistic approach. Under $H_{0}$ of no true synchrony, a valid null procedure is to circularly shift $y_{t}$ by a random offset $s$ (uniform on $\\{0,1,\\dots,N-1\\}$) to produce $y^{(k)}_{t} = y_{(t+s_{k}) \\bmod N}$. This preserves stationarity and marginal distribution of $y_{t}$ while destroying alignment with $x_{t}$. For each surrogate $k \\in \\{1,\\dots,K\\}$, compute the cross-correlogram $r^{(k)}(\\ell)$ over $\\ell \\in \\{-L_{\\mathrm{bins}},\\dots,+L_{\\mathrm{bins}}\\}$ and record the max-statistic\n$$\nM_{k} = \\max_{\\ell \\in \\{-L_{\\mathrm{bins}},\\dots,+L_{\\mathrm{bins}}\\}} \\left| r^{(k)}(\\ell) \\right|.\n$$\nThe null distribution of $M_{k}$ approximates the sampling distribution of the maximum absolute correlation under $H_{0}$. Let $t_{\\alpha}$ be the empirical $(1-\\alpha)$ quantile of $\\{M_{1},\\dots,M_{K}\\}$. Then\n$$\nP\\left(\\max_{\\ell} \\left| r(\\ell) \\right| \\geq t_{\\alpha} \\,\\big|\\, H_{0}\\right) \\approx \\alpha,\n$$\nwhich implies FWER control at level approximately $\\alpha$ for the family of lag tests. Consequently, declaring those lags $\\ell$ significant where $\\left| r(\\ell) \\right| \\geq t_{\\alpha}$ yields a set of discoveries with controlled FWER.\n\n4. Simulation of spike trains. To generate realistic sequences, we combine independent Poisson background with synchronous events. For each bin, sample independent counts:\n$$\nx_{t}^{\\text{bg}} \\sim \\text{Poisson}(\\lambda_{x} d t), \\quad y_{t}^{\\text{bg}} \\sim \\text{Poisson}(\\lambda_{y} d t).\n$$\nFor the synchronous component, sample a Poisson number of events with mean $\\lambda_{\\mathrm{sync}} T$. For each event with base time $u$, generate perturbed times\n$$\nu_{x} = \\mathrm{clip}(u + \\varepsilon_{x}, 0, T), \\quad u_{y} = \\mathrm{clip}(u + \\tau + \\varepsilon_{y}, 0, T),\n$$\nwith $\\varepsilon_{x}, \\varepsilon_{y} \\sim \\mathcal{N}(0,\\sigma^{2})$. Bin these times into $x_{t}$ and $y_{t}$ by incrementing the corresponding bin counts:\n$$\nt_{x} = \\left\\lfloor \\frac{u_{x}}{d t} \\right\\rfloor, \\quad t_{y} = \\left\\lfloor \\frac{u_{y}}{d t} \\right\\rfloor.\n$$\nThe resulting sequences are $x_{t} = x_{t}^{\\text{bg}} + x_{t}^{\\text{sync}}$ and $y_{t} = y_{t}^{\\text{bg}} + y_{t}^{\\text{sync}}$.\n\n5. Algorithmic steps.\n- Convert $L$ seconds to bins: $L_{\\mathrm{bins}} = \\lfloor L / d t \\rfloor$.\n- Compute observed $r(\\ell)$ for all integer $\\ell \\in \\{-L_{\\mathrm{bins}},\\dots,+L_{\\mathrm{bins}}\\}$.\n- For $k = 1$ to $K$: draw a random shift $s_{k}$ uniformly from $\\{0,1,\\dots,N-1\\}$, set $y^{(k)}_{t} = y_{(t+s_{k}) \\bmod N}$, compute $r^{(k)}(\\ell)$ over lags, record $M_{k} = \\max_{\\ell} |r^{(k)}(\\ell)|$.\n- Compute $t_{\\alpha}$ as the empirical $(1-\\alpha)$ quantile of $\\{M_{k}\\}$.\n- Output the set $\\{\\ell \\cdot d t : |r(\\ell)| \\geq t_{\\alpha}\\}$ in seconds, rounded to three decimals.\n\n6. Edge cases and boundary conditions. If $L = 0$, only the zero-lag test is performed; the max-statistic reduces to the absolute correlation at zero lag. If any overlapped segment has zero variance, set $r(\\ell) = 0$ to avoid divisions by zero. The circular shift surrogates preserve auto-correlation in $y_{t}$ but render alignment with $x_{t}$ effectively random under $H_{0}$, justifying their use for the null max-statistic.\n\n7. Output specification. For each test case, the program returns a list of significant lag values in seconds (rounded to three decimals). The final program output is a single line containing a comma-separated list of these lists, enclosed in square brackets. Units are seconds for all lag values in the output, and $\\alpha$ is specified as a decimal (e.g., $0.05$).\n\nWith these steps, the implementation controls the Family-Wise Error Rate (FWER) using the max-statistic across lags, providing statistically principled detection of synchrony peaks in the cross-correlogram.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate_spike_counts(T, dt, rate_x, rate_y, sync_rate, tau, sigma, rng):\n    \"\"\"\n    Simulate binned spike-count sequences x_t and y_t:\n    - Independent Poisson background per bin.\n    - Add synchronous events with jitter and lag tau.\n    \"\"\"\n    N = int(np.floor(T / dt))\n    # Background Poisson counts\n    mean_x = rate_x * dt\n    mean_y = rate_y * dt\n    x_bg = rng.poisson(mean_x, size=N)\n    y_bg = rng.poisson(mean_y, size=N)\n\n    x = x_bg.copy()\n    y = y_bg.copy()\n\n    # Synchronous component\n    if sync_rate > 0:\n        n_sync = rng.poisson(sync_rate * T)\n        if n_sync > 0:\n            # Draw base event times uniformly over [0, T)\n            base_times = rng.random(n_sync) * T\n            # Jitters\n            jitter_x = rng.normal(0.0, sigma, size=n_sync) if sigma > 0 else np.zeros(n_sync)\n            jitter_y = rng.normal(0.0, sigma, size=n_sync) if sigma > 0 else np.zeros(n_sync)\n            # Compute event times for x and y, apply lag tau to y\n            times_x = base_times + jitter_x\n            times_y = base_times + tau + jitter_y\n            # Clamp to [0, T)\n            times_x = np.clip(times_x, 0.0, T - 1e-12)\n            times_y = np.clip(times_y, 0.0, T - 1e-12)\n            # Bin indices\n            bins_x = np.floor(times_x / dt).astype(int)\n            bins_y = np.floor(times_y / dt).astype(int)\n            # Increment counts\n            # Multiple events can land in the same bin; use add.at for correctness\n            np.add.at(x, bins_x, 1)\n            np.add.at(y, bins_y, 1)\n\n    return x, y\n\ndef pearson_corr(x_seg, y_seg):\n    \"\"\"\n    Compute Pearson correlation for two equal-length 1D arrays.\n    If std dev of either segment is zero, return 0.\n    Uses sample correlation with denominator (n-1)*std_x*std_y.\n    \"\"\"\n    n = x_seg.size\n    if n  2:\n        return 0.0\n    x_mean = x_seg.mean()\n    y_mean = y_seg.mean()\n    x_center = x_seg - x_mean\n    y_center = y_seg - y_mean\n    sx = x_center.std(ddof=1)\n    sy = y_center.std(ddof=1)\n    if sx == 0 or sy == 0:\n        return 0.0\n    cov = np.dot(x_center, y_center) / (n - 1)\n    return cov / (sx * sy)\n\ndef correlogram(x, y, L_bins):\n    \"\"\"\n    Compute Pearson cross-correlogram r(lag) for lags in [-L_bins, ..., +L_bins].\n    \"\"\"\n    N = x.size\n    lags = np.arange(-L_bins, L_bins + 1, dtype=int)\n    r_vals = np.zeros(lags.shape[0], dtype=float)\n    for i, lag in enumerate(lags):\n        if lag >= 0:\n            x_seg = x[:N - lag] if lag != 0 else x\n            y_seg = y[lag:] if lag != 0 else y\n        else:\n            k = -lag\n            x_seg = x[k:]\n            y_seg = y[:N - k]\n        r_vals[i] = pearson_corr(x_seg, y_seg)\n    return lags, r_vals\n\ndef max_stat_threshold(x, y, L_bins, K, alpha, rng):\n    \"\"\"\n    Build null distribution via circular shifts of y and return the (1-alpha) quantile threshold.\n    \"\"\"\n    N = x.size\n    max_vals = np.empty(K, dtype=float)\n    # Precompute shifts\n    shifts = rng.integers(0, N, size=K)\n    for k in range(K):\n        y_shift = np.roll(y, shifts[k])\n        _, r_vals = correlogram(x, y_shift, L_bins)\n        max_vals[k] = np.max(np.abs(r_vals))\n    # Empirical (1 - alpha) quantile; use 'higher' to be conservative in FWER control\n    threshold = np.quantile(max_vals, 1.0 - alpha, method='higher')\n    return threshold\n\ndef run_test_case(params):\n    \"\"\"\n    Execute one test case: simulate data, compute correlogram, compute max-stat threshold,\n    and return list of significant lags (in seconds, rounded to 3 decimals).\n    params: (T, dt, rate_x, rate_y, sync_rate, tau, sigma, L, K, alpha, seed)\n    \"\"\"\n    T, dt, rate_x, rate_y, sync_rate, tau, sigma, L, K, alpha, seed = params\n    rng = np.random.default_rng(seed)\n    x, y = simulate_spike_counts(T, dt, rate_x, rate_y, sync_rate, tau, sigma, rng)\n    L_bins = int(np.floor(L / dt))\n    lags_bins, r_obs = correlogram(x, y, L_bins)\n    thr = max_stat_threshold(x, y, L_bins, K, alpha, rng)\n    sig_mask = np.abs(r_obs) >= thr\n    sig_lags_sec = (lags_bins[sig_mask].astype(float) * dt).tolist()\n    # Round to 3 decimals for output\n    sig_lags_sec_rounded = [round(val, 3) for val in sig_lags_sec]\n    return sig_lags_sec_rounded\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (T, dt, lambda_x, lambda_y, lambda_sync, tau, sigma, L, K, alpha, seed)\n    test_cases = [\n        (20.0, 0.005, 15.0, 15.0, 5.0, 0.0,    0.003, 0.025, 200, 0.05, 101),\n        (20.0, 0.005, 15.0, 15.0, 5.0, 0.015,  0.003, 0.025, 200, 0.05, 102),\n        (20.0, 0.005, 15.0, 15.0, 0.0, 0.0,    0.0,   0.025, 200, 0.05, 103),\n        (10.0, 0.005, 20.0, 20.0, 5.0, 0.0,    0.002, 0.000, 200, 0.05, 104),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Print a single list of lists, each inner list contains floats rounded to 3 decimals.\n    # Build the string manually to ensure formatting.\n    def format_list_of_floats(lst):\n        # Handles the empty list case gracefully\n        if not lst:\n            return \"[]\"\n        # Use a list comprehension to format each float\n        # Special handling for -0.0\n        formatted_nums = [f\"{0.0 if x == 0 and np.signbit(x) else x:.3f}\" for x in lst]\n        return \"[\" + \",\".join(formatted_nums) + \"]\"\n    \n    # Construct the final string for the list of lists\n    output_str = \"[\" + \",\".join(map(format_list_of_floats, results)) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}