{
    "hands_on_practices": [
        {
            "introduction": "Baseline correction is a ubiquitous step in Event-Related Potential (ERP) analysis, intended to remove pre-stimulus DC offsets and slow drifts. However, this procedure rests on the crucial assumption that the pre-stimulus interval contains only noise or a stable baseline. This exercise challenges you to derive analytically the bias introduced into ERP amplitude estimates when this assumption is violated by a simple linear drift, a common form of non-stationarity in electrophysiological recordings. By working through this problem, you will gain a fundamental insight into how even standard processing steps can systematically distort results, emphasizing the need for critical evaluation of analysis choices .",
            "id": "4160321",
            "problem": "Consider a univariate electroencephalography time series recorded around a stimulus onset at time $0$ for an event-related potential (ERP) or event-related field (ERF). Let the observed signal be modeled as the linear superposition $x(t) = s(t) + o(t) + n(t)$, where $s(t)$ is the stimulus-locked ERP/ERF component that satisfies $s(t) = 0$ for $t < 0$, $o(t)$ is an overlapping process that is not strictly stimulus-locked (for example, slow drift or lingering activity from previous events), and $n(t)$ is zero-mean stochastic noise. Baseline correction is defined as subtracting the mean of the observed signal over a pre-stimulus interval $[-$T_b$, 0)$, with $T_b > 0$, so that the baseline-corrected signal is $y(t) = x(t) - \\frac{1}{T_b} \\int_{-T_b}^{0} x(\\tau)\\, d\\tau$. Let the amplitude estimate at a specific post-stimulus time $t_0 > 0$ be the value $y(t_0)$.\n\nStarting from the superposition principle and the definitions above, derive an expression for the expectation $\\mathbb{E}[y(t_0)]$ in terms of $s(t_0)$ and $o(t)$, and define the bias $b(t_0)$ as the difference between $\\mathbb{E}[y(t_0)]$ and the true stimulus-locked amplitude $s(t_0)$. Then, under the specific overlapping-process model $o(t) = \\alpha + \\beta t$ for all $t \\in \\mathbb{R}$, with constants $\\alpha \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$, compute $b(t_0)$ explicitly. Express your final answer as a single closed-form analytic expression for $b(t_0)$. No numerical approximation or rounding is required.",
            "solution": "The user-provided problem statement has been analyzed and validated as scientifically sound, well-posed, and complete. We may therefore proceed with the derivation of a solution.\n\nThe observed signal is modeled as a linear superposition:\n$$x(t) = s(t) + o(t) + n(t)$$\nwhere $s(t)$ is the stimulus-locked signal, $o(t)$ is an overlapping process, and $n(t)$ is zero-mean stochastic noise, implying $\\mathbb{E}[n(t)] = 0$ for all $t$. We are given that $s(t) = 0$ for $t < 0$. The stimulus onset is at $t=0$.\n\nThe baseline-corrected signal, $y(t)$, is defined by subtracting the mean of the signal over the pre-stimulus interval $[-$T_b$, 0)$, where $T_b > 0$:\n$$y(t) = x(t) - \\frac{1}{T_b} \\int_{-T_b}^{0} x(\\tau)\\, d\\tau$$\n\nThe goal is to determine the expectation of the amplitude estimate $y(t_0)$ at a post-stimulus time $t_0 > 0$. We begin by taking the expectation of the expression for $y(t_0)$:\n$$\\mathbb{E}[y(t_0)] = \\mathbb{E}\\left[x(t_0) - \\frac{1}{T_b} \\int_{-T_b}^{0} x(\\tau)\\, d\\tau\\right]$$\n\nBy the linearity of the expectation operator, we can distribute it across the terms:\n$$\\mathbb{E}[y(t_0)] = \\mathbb{E}[x(t_0)] - \\mathbb{E}\\left[\\frac{1}{T_b} \\int_{-T_b}^{0} x(\\tau)\\, d\\tau\\right]$$\n\nSince the integral is a linear operator, and assuming we can exchange the order of expectation and integration (a condition typically met, as per Fubini's theorem for well-behaved stochastic processes), this becomes:\n$$\\mathbb{E}[y(t_0)] = \\mathbb{E}[x(t_0)] - \\frac{1}{T_b} \\int_{-T_b}^{0} \\mathbb{E}[x(\\tau)]\\, d\\tau$$\n\nNext, we substitute the model for $x(t)$ into the expectation $\\mathbb{E}[x(t)]$:\n$$\\mathbb{E}[x(t)] = \\mathbb{E}[s(t) + o(t) + n(t)]$$\nApplying linearity of expectation again:\n$$\\mathbb{E}[x(t)] = \\mathbb{E}[s(t)] + \\mathbb{E}[o(t)] + \\mathbb{E}[n(t)]$$\n\nIn this model, $s(t)$ and $o(t)$ are treated as deterministic signals, thus they are unaffected by the expectation operator (i.e., $\\mathbb{E}[s(t)] = s(t)$ and $\\mathbb{E}[o(t)] = o(t)$). The noise term $n(t)$ is defined as having zero mean, so $\\mathbb{E}[n(t)] = 0$. This simplifies the expectation of the observed signal to:\n$$\\mathbb{E}[x(t)] = s(t) + o(t)$$\n\nSubstituting this result back into the expression for $\\mathbb{E}[y(t_0)]$:\n$$\\mathbb{E}[y(t_0)] = (s(t_0) + o(t_0)) - \\frac{1}{T_b} \\int_{-T_b}^{0} (s(\\tau) + o(\\tau))\\, d\\tau$$\n$$\\mathbb{E}[y(t_0)] = s(t_0) + o(t_0) - \\frac{1}{T_b} \\int_{-T_b}^{0} s(\\tau)\\, d\\tau - \\frac{1}{T_b} \\int_{-T_b}^{0} o(\\tau)\\, d\\tau$$\n\nAccording to the problem statement, the stimulus-locked component $s(t)$ is zero for all negative time, i.e., $s(t) = 0$ for $t < 0$. The integration interval for the baseline is $[-$T_b$, 0)$, which lies entirely within this region. Therefore, the integral of $s(\\tau)$ over this interval is zero:\n$$\\int_{-T_b}^{0} s(\\tau)\\, d\\tau = \\int_{-T_b}^{0} 0\\, d\\tau = 0$$\n\nThe expression for the expected value of the baseline-corrected signal simplifies to:\n$$\\mathbb{E}[y(t_0)] = s(t_0) + o(t_0) - \\frac{1}{T_b} \\int_{-T_b}^{0} o(\\tau)\\, d\\tau$$\n\nThe bias, $b(t_0)$, is defined as the difference between this expectation and the true stimulus-locked amplitude $s(t_0)$:\n$$b(t_0) = \\mathbb{E}[y(t_0)] - s(t_0)$$\nSubstituting our derived expression for $\\mathbb{E}[y(t_0)]$:\n$$b(t_0) = \\left( s(t_0) + o(t_0) - \\frac{1}{T_b} \\int_{-T_b}^{0} o(\\tau)\\, d\\tau \\right) - s(t_0)$$\n$$b(t_0) = o(t_0) - \\frac{1}{T_b} \\int_{-T_b}^{0} o(\\tau)\\, d\\tau$$\nThis general expression shows that the bias is the value of the overlapping process at time $t_0$ minus its average value over the pre-stimulus baseline interval.\n\nWe are now tasked with computing this bias for the specific case where the overlapping process is a linear function of time:\n$$o(t) = \\alpha + \\beta t$$\nwith constants $\\alpha \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$.\n\nFirst, we evaluate $o(t_0)$:\n$$o(t_0) = \\alpha + \\beta t_0$$\n\nSecond, we compute the integral term:\n$$\\frac{1}{T_b} \\int_{-T_b}^{0} o(\\tau)\\, d\\tau = \\frac{1}{T_b} \\int_{-T_b}^{0} (\\alpha + \\beta \\tau)\\, d\\tau$$\nEvaluating the definite integral:\n$$\\int_{-T_b}^{0} (\\alpha + \\beta \\tau)\\, d\\tau = \\left[ \\alpha\\tau + \\frac{\\beta}{2}\\tau^{2} \\right]_{-T_b}^{0}$$\n$$= \\left( \\alpha(0) + \\frac{\\beta}{2}(0)^{2} \\right) - \\left( \\alpha(-T_b) + \\frac{\\beta}{2}(-T_b)^{2} \\right)$$\n$$= 0 - \\left( -\\alpha T_b + \\frac{\\beta}{2}T_b^{2} \\right) = \\alpha T_b - \\frac{\\beta}{2}T_b^{2}$$\nNow, dividing by $T_b$:\n$$\\frac{1}{T_b} \\left( \\alpha T_b - \\frac{\\beta}{2}T_b^{2} \\right) = \\alpha - \\frac{\\beta T_b}{2}$$\n\nFinally, we substitute these results back into the expression for the bias $b(t_0)$:\n$$b(t_0) = (\\alpha + \\beta t_0) - \\left(\\alpha - \\frac{\\beta T_b}{2}\\right)$$\n$$b(t_0) = \\alpha + \\beta t_0 - \\alpha + \\frac{\\beta T_b}{2}$$\n$$b(t_0) = \\beta t_0 + \\frac{\\beta T_b}{2}$$\nThis can be written in factored form as:\n$$b(t_0) = \\beta \\left( t_0 + \\frac{T_b}{2} \\right)$$\nThis is the closed-form analytic expression for the bias introduced by applying pre-stimulus baseline correction in the presence of a linear drift. The bias is directly proportional to the slope $\\beta$ of the drift and depends on both the time of measurement $t_0$ and the duration of the baseline period $T_b$. The constant offset $\\alpha$ is completely removed by the baseline correction procedure.",
            "answer": "$$\\boxed{\\beta \\left(t_0 + \\frac{T_b}{2}\\right)}$$"
        },
        {
            "introduction": "A major challenge in EEG and MEG analysis is removing large non-neural artifacts, such as those from eye blinks or muscle activity, while preserving the much smaller brain signal of interest. This practice moves beyond a qualitative understanding of artifact removal to a quantitative comparison of two powerful methods: Signal Space Projection (SSP) and Independent Component Analysis (ICA). By deriving the precise mathematical effects of these projection-based techniques on data rank and desired signal power, you will develop a rigorous understanding of the trade-offs involved, particularly the critical concept of signal leakage during artifact correction .",
            "id": "4160391",
            "problem": "You are analyzing magnetoencephalography (MEG) event-related fields (ERFs) and wish to remove ocular artifacts. Consider a single condition where the sensor-space data matrix is $X \\in \\mathbb{R}^{M \\times T}$ with $M$ sensors and $T$ time samples. Assume the following setup grounded in linear modeling and basic linear algebra definitions.\n\n1. The MEG sensor data can be represented as a linear instantaneous mixing of latent sources plus noise, $X = A S + E$, where $A \\in \\mathbb{R}^{M \\times Q}$ is a mixing matrix, $S \\in \\mathbb{R}^{Q \\times T}$ contains source time courses, and $E$ is sensor noise. The ocular artifact spans a $k$-dimensional subspace $\\mathcal{N} \\subset \\mathbb{R}^{M}$, and the event-related field of interest at the peak latency has a unit-norm spatial topography $v \\in \\mathbb{R}^{M}$.\n\n2. Signal Space Projection (SSP) models the ocular artifact as confined to $\\mathcal{N}$ and removes it by projecting onto the orthogonal complement $\\mathcal{N}^{\\perp}$. Let $U \\in \\mathbb{R}^{M \\times k}$ have orthonormal columns that span $\\mathcal{N}$, and assume $U^{\\top}U = I_{k}$. The projection is applied to sensor data on the left. Use only the definitions of linear subspace, orthogonal complement, orthogonal projection, and matrix rank to derive how such a projection affects the rank of the data and the fraction of preserved power of the evoked topography $v$.\n\n3. Independent Component Analysis (ICA) identifies an ocular independent component whose mixing vector is $a \\in \\mathbb{R}^{M}$ with $\\|a\\|_{2} = 1$. Removing that ICA component in sensor space corresponds to subtracting the contribution along $a$. Use only the definitions of linear projection and orthogonality to express the fraction of preserved power of $v$ after removing the ICA component.\n\nUse the following scientifically plausible numerical values:\n- Sensor dimensionality: $M = 306$.\n- Original data matrix rank: $\\operatorname{rank}(X) = r_X = 140$.\n- Ocular subspace dimension: $k = 2$.\n- Orthonormal ocular basis: $U = [u_{1}, u_{2}]$ with $u_{1}, u_{2} \\in \\mathbb{R}^{M}$ and $u_{i}^{\\top} u_{j} = \\delta_{ij}$.\n- Evoked topography is unit norm: $\\|v\\|_{2} = 1$.\n- Inner products with the ocular basis: $v^{\\top} u_1 = 0.5$ and $v^{\\top} u_2 = 0.4$.\n- ICA ocular mixing vector unit norm: $\\|a\\|_{2} = 1$ with $v^{\\top} a = 0.3$.\n\nTasks:\n(a) From first principles, derive an expression for $\\operatorname{rank}(P_{\\perp} X)$ in terms of $r_X$, $M$, and $k$, where $P_{\\perp}$ is the orthogonal projector implementing SSP. Evaluate the rank for the numbers given above.\n(b) Using only the definitions of projection and orthogonality, derive the fraction of preserved power of $v$ after SSP in terms of $v^{\\top} u_1$ and $v^{\\top} u_2$, and evaluate it numerically.\n(c) Derive the fraction of preserved power of $v$ after ICA removal of the single ocular component with mixing vector $a$, in terms of $v^{\\top} a$, and evaluate it numerically.\n(d) Finally, compute the ratio $R$ of the SSP-preserved power to the ICA-preserved power for $v$ using the values above.\n\nExpress the final answer as the single real-valued number $R$, rounded to four significant figures. No units are required for the final answer.",
            "solution": "The problem requires an analysis of two common artifact correction techniques in magnetoencephalography (MEG), Signal Space Projection (SSP) and Independent Component Analysis (ICA), based on principles of linear algebra. We will address each of the four tasks in sequence.\n\nFirst, we address task (a), which asks for the rank of the data matrix $X$ after the application of SSP. The SSP method projects the data onto the orthogonal complement of the artifact subspace $\\mathcal{N}$. The artifact subspace $\\mathcal{N}$ is $k$-dimensional and is spanned by the orthonormal columns of the matrix $U \\in \\mathbb{R}^{M \\times k}$. The orthogonal projection operator onto $\\mathcal{N}$ is $P = UU^{\\top}$, where we use the fact that $U^{\\top}U = I_k$. The SSP projection operator, which projects onto the orthogonal complement $\\mathcal{N}^{\\perp}$, is therefore given by $P_{\\perp} = I - P = I - UU^{\\top}$.\n\nThe data matrix after SSP is $P_{\\perp}X$. We need to find its rank, $\\operatorname{rank}(P_{\\perp}X)$. The rank of a matrix is the dimension of its column space (also called its range). The column space of $P_{\\perp}X$ is the set of all vectors of the form $P_{\\perp}y$ where $y$ is in the column space of $X$, denoted $\\operatorname{col}(X)$. Let $\\mathcal{C} = \\operatorname{col}(X)$. The rank of $P_{\\perp}X$ is the dimension of the subspace $P_{\\perp}(\\mathcal{C})$.\n\nUsing the rank-nullity theorem for the linear map $P_{\\perp}$ restricted to the subspace $\\mathcal{C}$, we have:\n$$\n\\dim(\\mathcal{C}) = \\dim(\\ker(P_{\\perp}|_{\\mathcal{C}})) + \\dim(\\operatorname{Im}(P_{\\perp}|_{\\mathcal{C}}))\n$$\nThe term $\\dim(\\mathcal{C})$ is the rank of the original data matrix, given as $r_X = \\operatorname{rank}(X)$. The term $\\dim(\\operatorname{Im}(P_{\\perp}|_{\\mathcal{C}}))$ is the dimension of the image of $\\mathcal{C}$ under $P_{\\perp}$, which is precisely the rank we want to find, $\\operatorname{rank}(P_{\\perp}X)$. The kernel of $P_{\\perp}$ acting on $\\mathcal{C}$ is the set of vectors in $\\mathcal{C}$ that are projected to zero, which are the vectors that lie entirely within the artifact subspace $\\mathcal{N}$. Thus, $\\ker(P_{\\perp}|_{\\mathcal{C}}) = \\mathcal{C} \\cap \\mathcal{N}$.\nThis gives the relation:\n$$\n\\operatorname{rank}(P_{\\perp}X) = \\operatorname{rank}(X) - \\dim(\\operatorname{col}(X) \\cap \\mathcal{N}) = r_X - \\dim(\\mathcal{C} \\cap \\mathcal{N})\n$$\nThe problem does not explicitly state the dimension of the intersection between the data's column space and the artifact subspace. However, for the problem to be well-posed and have a single numerical answer, we must infer a reasonable assumption. The premise of SSP is that the artifacts to be removed are present in the data. This implies that the artifact subspace $\\mathcal{N}$ has a substantial intersection with the data's signal space $\\mathcal{C} = \\operatorname{col}(X)$. The most direct and standard assumption is that the artifact subspace is fully contained within the data's column space, i.e., $\\mathcal{N} \\subset \\mathcal{C}$. This is plausible given that $r_X = 140$ is much larger than $k=2$. Under this assumption, $\\mathcal{C} \\cap \\mathcal{N} = \\mathcal{N}$, and therefore $\\dim(\\mathcal{C} \\cap \\mathcal{N}) = \\dim(\\mathcal{N}) = k$.\nThe expression for the rank becomes:\n$$\n\\operatorname{rank}(P_{\\perp}X) = r_X - k\n$$\nUsing the provided numerical values, $r_X = 140$ and $k=2$, we find the rank:\n$$\n\\operatorname{rank}(P_{\\perp}X) = 140 - 2 = 138\n$$\n\nFor task (b), we derive the fraction of preserved power of the evoked topography vector $v$ after SSP. The original power is proportional to $\\|v\\|_2^2$. After applying the SSP projector $P_{\\perp}$, the new topography is $v' = P_{\\perp}v$. The preserved power is proportional to $\\|v'\\|_2^2$. The fraction of preserved power is $\\|v'\\|_2^2 / \\|v\\|_2^2$.\nGiven that $v$ has unit norm, $\\|v\\|_2 = 1$, this fraction is simply $\\|v'\\|_2^2$.\n$$\nv' = P_{\\perp}v = (I - UU^{\\top})v = v - UU^{\\top}v\n$$\nThe vector $UU^{\\top}v$ is the orthogonal projection of $v$ onto the subspace $\\mathcal{N}$. The vector $v'$ is the orthogonal projection of $v$ onto $\\mathcal{N}^{\\perp}$. By the Pythagorean theorem, $\\|v\\|^2 = \\|P_{\\perp}v\\|^2 + \\|Pv\\|^2$, which means $\\|v'\\|^2 = \\|v\\|^2 - \\|UU^{\\top}v\\|^2$.\nLet's compute the norm of the rejected part:\n$$\n\\|UU^{\\top}v\\|^2 = (UU^{\\top}v)^{\\top}(UU^{\\top}v) = v^{\\top}U(U^{\\top}U)U^{\\top}v\n$$\nSince the columns of $U$ are orthonormal, $U^{\\top}U = I_k$.\n$$\n\\|UU^{\\top}v\\|^2 = v^{\\top}UI_kU^{\\top}v = v^{\\top}UU^{\\top}v\n$$\nThe matrix $U$ is given as $U = [u_1, u_2]$. Thus, $U^{\\top}v = \\begin{pmatrix} u_1^{\\top}v \\\\ u_2^{\\top}v \\end{pmatrix}$.\n$$\n\\|UU^{\\top}v\\|^2 = (v^{\\top}U)(U^{\\top}v) = [v^{\\top}u_1, v^{\\top}u_2] \\begin{pmatrix} v^{\\top}u_1 \\\\ v^{\\top}u_2 \\end{pmatrix} = (v^{\\top}u_1)^2 + (v^{\\top}u_2)^2\n$$\nThe fraction of preserved power is therefore $\\|v'\\|^2 = \\|v\\|^2 - ((v^{\\top}u_1)^2 + (v^{\\top}u_2)^2)$.\nWith $\\|v\\|_2=1$ and the given inner products $v^{\\top}u_1 = 0.5$ and $v^{\\top}u_2 = 0.4$, we get:\n$$\n\\text{Fraction}_{\\text{SSP}} = 1 - (0.5^2 + 0.4^2) = 1 - (0.25 + 0.16) = 1 - 0.41 = 0.59\n$$\n\nFor task (c), we analyze the effect of ICA-based artifact removal. Removing a single component with mixing vector $a$ is equivalent to projecting the data onto the subspace orthogonal to $a$. This is a special case of SSP with $k=1$ and the basis for the artifact subspace being the single vector $a$.\nGiven $\\|a\\|_2 = 1$, the projector onto the line spanned by $a$ is $P_a = aa^{\\top}$. The projector that removes this component is $P_{a^{\\perp}} = I - aa^{\\top}$.\nThe topography after ICA-based cleaning is $v'' = P_{a^{\\perp}}v$.\nThe fraction of preserved power is $\\|v''\\|_2^2 / \\|v\\|_2^2$. Since $\\|v\\|_2=1$, this is $\\|v''\\|_2^2$.\n$$\n\\|v''\\|^2 = \\|(I - aa^{\\top})v\\|^2\n$$\nUsing the same logic as in part (b), this is:\n$$\n\\|v''\\|^2 = \\|v\\|^2 - \\|aa^{\\top}v\\|^2 = 1 - (v^{\\top}a)^2\n$$\nUsing the given value $v^{\\top}a = 0.3$:\n$$\n\\text{Fraction}_{\\text{ICA}} = 1 - (0.3)^2 = 1 - 0.09 = 0.91\n$$\n\nFinally, for task (d), we compute the ratio $R$ of the SSP-preserved power to the ICA-preserved power for the topography $v$.\n$$\nR = \\frac{\\text{Fraction}_{\\text{SSP}}}{\\text{Fraction}_{\\text{ICA}}}\n$$\nUsing the values calculated in parts (b) and (c):\n$$\nR = \\frac{0.59}{0.91} \\approx 0.648351648\n$$\nRounding to four significant figures, we get $R = 0.6484$.",
            "answer": "$$\\boxed{0.6484}$$"
        },
        {
            "introduction": "The ERP waveforms we observe at the scalp are often a composite signal, reflecting the sum of multiple distinct neural processes that overlap in time. This coding exercise guides you through a powerful and modern approach to deconstruct these complex signals: generative modeling. You will implement a model that represents the observed ERP as a linear combination of components, each described by a set of temporal basis functions, and use regularized regression to estimate their individual contributions from noisy data, a technique that enables a principled decomposition of overlapping neural responses .",
            "id": "4160457",
            "problem": "You are given a scenario grounded in the analysis of Event-Related Potentials (ERPs) and Event-Related Fields (ERFs), where a measured sensor waveform is modeled as a linear superposition of overlapping neural components in the presence of additive noise. The aim is to separate the contributions of multiple overlapping components by representing each component’s time course as a linear combination of temporal basis functions and fitting this model to the sensor data. The solution must start from the core principles that the measured signal is a sum of component responses and that the noise is additive and Gaussian, and proceed to a principled estimation method that leverages these assumptions.\n\nA single sensor measurement over time is modeled as follows. Let the time series be sampled uniformly at sampling frequency $f_s$ (in Hz) over an interval $[0, T]$ seconds, producing $N$ samples at times $t_i = i/f_s$ for $i = 0, 1, \\dots, N-1$. The measured signal $y(t)$ is assumed to be the sum of $K$ components and additive noise:\n$$\ny(t) = \\sum_{k=1}^{K} s_k(t) + \\varepsilon(t),\n$$\nwhere for each component $k$, the time course $s_k(t)$ is represented as a linear combination of $M$ temporal basis functions that are anchored to a component-specific latency $\\tau_k$:\n$$\ns_k(t) = \\sum_{m=1}^{M} \\beta_{k,m}\\,\\phi_m\\!\\big((t - \\tau_k)\\big).\n$$\nThe unknown coefficients $\\beta_{k,m}$ are the amplitudes (in microvolts) of basis functions that describe the component shape, and $\\varepsilon(t)$ is zero-mean additive Gaussian noise with variance $\\sigma^2$. The temporal basis functions $\\phi_m(\\cdot)$ are fixed and known; in this task, use Gaussian basis functions parameterized by centers $c_m$ and a common width $w$:\n$$\n\\phi_m(u) = \\exp\\!\\left(-\\tfrac{1}{2}\\,\\frac{(u - c_m)^2}{w^2}\\right).\n$$\nDefine the design matrix $X \\in \\mathbb{R}^{N \\times (K M)}$ whose columns are the basis functions evaluated at the sampled times, with column ordering by component index first and then basis index: for $k \\in \\{1,\\dots,K\\}$ and $m \\in \\{1,\\dots,M\\}$, the column with index $j = (k-1)M + m$ is given entrywise by\n$$\nX_{i, j} = \\phi_m\\!\\big((t_i - \\tau_k)\\big).\n$$\nWith $\\beta \\in \\mathbb{R}^{K M}$ denoting the vector of all coefficients $\\beta_{k,m}$ stacked in the same order, the model is\n$$\ny = X \\beta + \\varepsilon.\n$$\n\nYour task is to:\n- Construct the design matrix $X$ from the given parameters.\n- Generate synthetic data $y$ from a known ground-truth coefficient vector $\\beta^{\\star}$ by $y = X \\beta^{\\star} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n- Estimate $\\hat{\\beta}$ by minimizing a squared-error objective with an $\\ell_2$-penalty (ridge), that is, solve\n$$\n\\hat{\\beta} = \\underset{\\beta}{\\arg\\min}\\ \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2,\n$$\nwhere $\\lambda \\ge 0$ is a given penalty parameter. When $\\lambda = 0$, this reduces to ordinary least squares. The estimator must be implemented for general $\\lambda \\ge 0$.\n- For each test case, compute the maximum absolute coefficient error\n$$\nE_{\\max} = \\max_{j} \\left| \\hat{\\beta}_j - \\beta^{\\star}_j \\right|\n$$\nexpressed in microvolts, as a floating-point value.\n\nUse the following test suite. All numeric values are given explicitly and are scientifically plausible for electrophysiological signals. Use time in seconds, sampling frequency in Hz, and amplitudes in microvolts. Angles are not involved. For random noise generation, use the provided integer random seed for reproducibility. For each case, construct $t_i$ for $i = 0, 1, \\dots, N-1$ with $N = \\lfloor f_s \\cdot T \\rfloor + 1$ (inclusive of both endpoints).\n\nTest Case A (happy path, moderate overlap, high signal-to-noise ratio):\n- Sampling frequency $f_s = 1000$ Hz, duration $T = 0.300$ s.\n- Number of components $K = 2$, number of basis functions per component $M = 3$.\n- Component latencies $\\tau = [0.080,\\ 0.140]$ s.\n- Basis centers $c = [-0.012,\\ 0.000,\\ 0.012]$ s, common width $w = 0.008$ s.\n- Ground-truth coefficients (in microvolts), ordered by component then basis:\n  $\\beta^{\\star} = [4.0,\\ -2.0,\\ 1.0,\\ -3.0,\\ 2.0,\\ -1.5]$.\n- Noise standard deviation $\\sigma = 0.200$ microvolts.\n- Ridge penalty $\\lambda = 0.000$.\n- Random seed $s = 42$.\n\nTest Case B (stronger overlap, lower signal-to-noise ratio, mild regularization):\n- Sampling frequency $f_s = 1000$ Hz, duration $T = 0.300$ s.\n- Number of components $K = 2$, number of basis functions per component $M = 3$.\n- Component latencies $\\tau = [0.100,\\ 0.120]$ s.\n- Basis centers $c = [-0.012,\\ 0.000,\\ 0.012]$ s, common width $w = 0.008$ s.\n- Ground-truth coefficients (in microvolts), ordered by component then basis:\n  $\\beta^{\\star} = [3.5,\\ -1.0,\\ 0.5,\\ 2.0,\\ -2.5,\\ 1.0]$.\n- Noise standard deviation $\\sigma = 0.500$ microvolts.\n- Ridge penalty $\\lambda = 0.100$.\n- Random seed $s = 43$.\n\nTest Case C (three components, severe overlap, stronger regularization):\n- Sampling frequency $f_s = 1000$ Hz, duration $T = 0.300$ s.\n- Number of components $K = 3$, number of basis functions per component $M = 4$.\n- Component latencies $\\tau = [0.090,\\ 0.110,\\ 0.130]$ s.\n- Basis centers $c = [-0.018,\\ -0.006,\\ 0.006,\\ 0.018]$ s, common width $w = 0.010$ s.\n- Ground-truth coefficients (in microvolts), ordered by component then basis:\n  $\\beta^{\\star} = [2.2,\\ -1.2,\\ 0.6,\\ -0.1,\\ -1.5,\\ 2.8,\\ -2.0,\\ 0.7,\\ 0.8,\\ -1.0,\\ 1.2,\\ -0.5]$.\n- Noise standard deviation $\\sigma = 0.700$ microvolts.\n- Ridge penalty $\\lambda = 0.500$.\n- Random seed $s = 44$.\n\nProgram requirements:\n- Implement the model construction, data synthesis, coefficient estimation, and error computation as described above.\n- For each of the three test cases, output the maximum absolute coefficient error $E_{\\max}$ as a floating-point number (in microvolts).\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets (for example, $[r_A,r_B,r_C]$), with $r_A$ corresponding to Test Case A, $r_B$ to Test Case B, and $r_C$ to Test Case C. No additional text should be printed.",
            "solution": "We begin from the principle of linear superposition of time-locked neural responses: the measured signal $y(t)$ at a sensor is modeled as the sum of component waveforms plus additive noise. Specifically, for Event-Related Potentials (ERPs) or Event-Related Fields (ERFs), when component latencies are known or hypothesized, a component’s shape can be flexibly represented by a linear combination of a small set of temporal basis functions. This yields a linear generative model,\n$$\ny(t) = \\sum_{k=1}^{K} \\sum_{m=1}^{M} \\beta_{k,m}\\,\\phi_m\\!\\big((t - \\tau_k)\\big) + \\varepsilon(t), \\quad \\varepsilon(t) \\sim \\mathcal{N}(0, \\sigma^2),\n$$\nwhich, when discretized at sampling instants $t_i = i / f_s$ for $i = 0, 1, \\dots, N-1$, can be written in matrix form as\n$$\ny = X \\beta + \\varepsilon,\n$$\nwhere $y \\in \\mathbb{R}^{N}$ is the stacked sample vector, $X \\in \\mathbb{R}^{N \\times (K M)}$ is the design matrix whose columns are the basis functions evaluated at the sample times and appropriately shifted by the component latencies, and $\\beta \\in \\mathbb{R}^{K M}$ stacks the unknown basis coefficients by component and basis index.\n\nConstruction of the design matrix is a direct application of evaluating the chosen basis functions. With Gaussian basis functions\n$$\n\\phi_m(u) = \\exp\\!\\left(-\\tfrac{1}{2}\\,\\frac{(u - c_m)^2}{w^2}\\right),\n$$\nand component latencies $\\tau_k$, the $(i,j)$-th entry of $X$ for column $j = (k-1)M + m$ is\n$$\nX_{i,j} = \\exp\\!\\left(-\\tfrac{1}{2}\\,\\frac{\\big((t_i - \\tau_k) - c_m\\big)^2}{w^2}\\right).\n$$\nThis ensures each component has its own set of time-local basis functions anchored at its latency, enabling overlapping components to be represented distinctly if the basis is sufficiently expressive.\n\nEstimation derives from a probabilistic principle. Under the assumption that $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$, the likelihood of parameters $\\beta$ is\n$$\np(y \\mid \\beta) \\propto \\exp\\!\\left(-\\tfrac{1}{2 \\sigma^2} \\| y - X \\beta \\|_2^2 \\right).\n$$\nMaximizing this likelihood (equivalently minimizing the negative log-likelihood) leads to minimizing the squared error $\\| y - X \\beta \\|_2^2$, yielding the ordinary least squares estimator when unconstrained. To mitigate instability due to multicollinearity of overlapping basis functions and to control variance under low signal-to-noise ratio, one may incorporate an $\\ell_2$ penalty with weight $\\lambda \\ge 0$, corresponding to a Gaussian prior on $\\beta$ with variance proportional to $1/\\lambda$. The resulting estimator is the ridge estimator:\n$$\n\\hat{\\beta} = \\underset{\\beta}{\\arg\\min}\\ \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2.\n$$\nSetting the gradient of the objective with respect to $\\beta$ to zero yields the normal equations with regularization:\n$$\n(X^\\top X + \\lambda I)\\, \\hat{\\beta} = X^\\top y.\n$$\nWhen $\\lambda = 0$, this reduces to the ordinary least squares normal equations. For $\\lambda > 0$, $(X^\\top X + \\lambda I)$ is strictly positive definite, improving numerical conditioning and handling severe overlap between component bases.\n\nAlgorithmic steps:\n1. For each test case, construct the time vector $t_i = i/f_s$ for $i = 0, \\dots, \\lfloor f_s T \\rfloor$ to cover $[0, T]$ with uniform sampling. This yields $N = \\lfloor f_s T \\rfloor + 1$ samples.\n2. Build the design matrix $X$ by evaluating the Gaussian basis functions with the specified centers $c_m$ and width $w$ at each $t_i$ for each component latency $\\tau_k$, forming $K M$ columns ordered by component then basis.\n3. Form the ground truth coefficient vector $\\beta^{\\star}$ by stacking coefficients for each component in the prescribed order.\n4. Synthesize the observation vector by $y = X \\beta^{\\star} + \\varepsilon$, where $\\varepsilon$ is drawn from a zero-mean Gaussian with standard deviation $\\sigma$. Use the provided integer random seed to initialize a pseudo-random number generator to ensure reproducibility.\n5. Estimate $\\hat{\\beta}$ by solving $(X^\\top X + \\lambda I)\\, \\hat{\\beta} = X^\\top y$ for the specified $\\lambda$. This can be performed using a numerically stable linear solver on the symmetric positive semidefinite (or definite, if $\\lambda > 0$) matrix.\n6. Compute the maximum absolute error across coefficients, $E_{\\max} = \\max_j | \\hat{\\beta}_j - \\beta^{\\star}_j |$. Because the basis functions have unit peak ($\\phi_m(0) = 1$ at their centers) and coefficients are in microvolts, $E_{\\max}$ is naturally expressed in microvolts.\n7. Repeat for all test cases.\n\nDesign considerations:\n- Severe component overlap and closely spaced basis centers can induce multicollinearity, elevating the condition number of $X^\\top X$. The ridge penalty $\\lambda$ regularizes the solution by shrinking coefficients, improving stability and interpretability at the expense of bias.\n- The sampling scheme includes both endpoints to ensure the temporal window $[0, T]$ is fully represented.\n- The use of Gaussian basis functions with specified centers $c_m$ and a common width $w$ provides a compact, smooth, and physiologically plausible parametric family for modeling component shapes.\n\nFinally, the program must output a single line: a list $[r_A, r_B, r_C]$ where each $r$ is the $E_{\\max}$ for the corresponding test case, as floating-point numbers in microvolts and in that order. No additional text should be printed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef gaussian_basis(u, centers, width):\n    \"\"\"\n    Evaluate a bank of Gaussian basis functions at offsets u.\n    centers: array of shape (M,)\n    width: scalar > 0\n    Returns: array of shape (len(u), M)\n    \"\"\"\n    u = u[:, None]  # (N, 1)\n    centers = centers[None, :]  # (1, M)\n    return np.exp(-0.5 * ((u - centers) ** 2) / (width ** 2))\n\ndef build_design_matrix(t, taus, centers, width):\n    \"\"\"\n    Construct design matrix X with columns ordered by component then basis.\n    t: array of shape (N,), absolute time samples\n    taus: array of shape (K,), component latencies\n    centers: array of shape (M,), basis centers (relative to latency)\n    width: scalar > 0, Gaussian width\n    Returns: X of shape (N, K*M)\n    \"\"\"\n    N = t.size\n    K = len(taus)\n    M = len(centers)\n    X = np.zeros((N, K * M), dtype=float)\n    for k, tau in enumerate(taus):\n        # offsets relative to component latency\n        u = t - tau  # (N,)\n        Phi = gaussian_basis(u, centers, width)  # (N, M)\n        # place into design matrix\n        start = k * M\n        X[:, start:start + M] = Phi\n    return X\n\ndef synthesize_data(X, beta_true, sigma, seed):\n    \"\"\"\n    Generate y = X beta_true + epsilon, epsilon ~ N(0, sigma^2 I)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    noise = rng.normal(loc=0.0, scale=sigma, size=X.shape[0])\n    return X @ beta_true + noise\n\ndef ridge_estimate(X, y, lam):\n    \"\"\"\n    Compute ridge estimate: (X^T X + lam I)^{-1} X^T y\n    \"\"\"\n    XT = X.T\n    A = XT @ X\n    if lam > 0:\n        A = A + lam * np.eye(A.shape[0])\n    b = XT @ y\n    # Solve A beta = b\n    beta_hat = np.linalg.solve(A, b)\n    return beta_hat\n\ndef max_abs_error(beta_hat, beta_true):\n    return float(np.max(np.abs(beta_hat - beta_true)))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"fs\": 1000.0,\n            \"T\": 0.300,\n            \"taus\": [0.080, 0.140],\n            \"centers\": [-0.012, 0.000, 0.012],\n            \"width\": 0.008,\n            \"beta_true\": [4.0, -2.0, 1.0, -3.0, 2.0, -1.5],\n            \"sigma\": 0.200,\n            \"lam\": 0.0,\n            \"seed\": 42\n        },\n        {\n            \"fs\": 1000.0,\n            \"T\": 0.300,\n            \"taus\": [0.100, 0.120],\n            \"centers\": [-0.012, 0.000, 0.012],\n            \"width\": 0.008,\n            \"beta_true\": [3.5, -1.0, 0.5, 2.0, -2.5, 1.0],\n            \"sigma\": 0.500,\n            \"lam\": 0.1,\n            \"seed\": 43\n        },\n        {\n            \"fs\": 1000.0,\n            \"T\": 0.300,\n            \"taus\": [0.090, 0.110, 0.130],\n            \"centers\": [-0.018, -0.006, 0.006, 0.018],\n            \"width\": 0.010,\n            \"beta_true\": [2.2, -1.2, 0.6, -0.1, -1.5, 2.8, -2.0, 0.7, 0.8, -1.0, 1.2, -0.5],\n            \"sigma\": 0.700,\n            \"lam\": 0.5,\n            \"seed\": 44\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        fs = case[\"fs\"]\n        T = case[\"T\"]\n        # Time vector inclusive of both endpoints\n        N = int(np.floor(fs * T)) + 1\n        t = np.linspace(0.0, T, N)\n        taus = np.array(case[\"taus\"], dtype=float)\n        centers = np.array(case[\"centers\"], dtype=float)\n        width = float(case[\"width\"])\n        beta_true = np.array(case[\"beta_true\"], dtype=float)\n        sigma = float(case[\"sigma\"])\n        lam = float(case[\"lam\"])\n        seed = int(case[\"seed\"])\n\n        # Build design matrix\n        X = build_design_matrix(t, taus, centers, width)\n\n        # Synthesize data\n        y = synthesize_data(X, beta_true, sigma, seed)\n\n        # Estimate coefficients\n        beta_hat = ridge_estimate(X, y, lam)\n\n        # Compute metric\n        err = max_abs_error(beta_hat, beta_true)\n        results.append(err)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}