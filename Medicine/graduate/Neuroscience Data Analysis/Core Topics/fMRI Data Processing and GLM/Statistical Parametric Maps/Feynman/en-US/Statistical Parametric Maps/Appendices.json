{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of event-related fMRI analysis is the General Linear Model (GLM), which requires a precise prediction of the expected Blood Oxygen Level Dependent (BOLD) signal. This exercise walks you through the fundamental process of creating this prediction, known as a regressor. You will implement the convolution of a stimulus timing function with the canonical Hemodynamic Response Function (HRF), a core computational skill for translating experimental design into a statistical model ().",
            "id": "4196045",
            "problem": "You will implement the construction of regressors for a General Linear Model (GLM) used in Statistical Parametric Mapping (SPM), based on convolving event timings with a Hemodynamic Response Function (HRF). The objective is to start from the definition of convolution in continuous time, build a discretized approximation suitable for computation, and compute a predicted Blood Oxygen Level Dependent (BOLD) time course from a given event train. The fundamental base for this problem is the continuous-time convolution definition and the GLM framework with regressors formed by stimulus functions convolved with an HRF. Specifically, the GLM represents the measured signal $y(t)$ as a linear combination of regressors $x_j(t)$ plus noise, and each regressor is formed by the convolution of a stimulus function $s_j(t)$ with an HRF $h(t)$. You must formalize the stimulus as a sum of Dirac delta functions at event times and derive the discrete-time approximation of the regressor via numerical convolution.\n\nFundamental definitions:\n- Convolution of two functions $s(t)$ and $h(t)$ is defined by\n$$\nx(t) = (s * h)(t) = \\int_{-\\infty}^{+\\infty} s(\\tau)\\, h(t - \\tau)\\, d\\tau.\n$$\n- If the stimulus is a train of events represented by Dirac delta functions $s(t) = \\sum_i w_i\\, \\delta(t - \\tau_i)$ with event times $\\tau_i$ and weights $w_i$, then by properties of the Dirac delta, the continuous-time convolution evaluates to\n$$\nx(t) = \\sum_i w_i\\, h(t - \\tau_i).\n$$\n- A widely used Hemodynamic Response Function (HRF) is the canonical double-gamma model:\n$$\nh(t) = \\frac{t^{a_1 - 1} e^{-t/b}}{b^{a_1}\\, \\Gamma(a_1)} \\;-\\; c\\, \\frac{t^{a_2 - 1} e^{-t/b}}{b^{a_2}\\, \\Gamma(a_2)},\n\\quad t \\ge 0,\n$$\nand $h(t) = 0$ for $t  0$, where $\\Gamma(\\cdot)$ is the gamma function, and $a_1$, $a_2$, $b$, $c$ are positive constants.\n\nDiscrete-time approximation goals:\n- Approximate the continuous-time convolution using a uniform high-resolution time grid with step size $dt$ (in seconds). Let the discrete stimulus be $s[k]$ constructed to approximate the sum of Dirac deltas. To preserve the correct scaling under a Riemann sum, an event at time index $k_i$ with weight $w_i$ should be represented in discrete time as $s[k_i] = w_i/dt$, so that\n$$\nx[n] \\approx dt \\sum_{k} s[k]\\, h[n-k] = \\sum_i w_i\\, h[n - k_i],\n$$\nconsistent with the continuous-time formula.\n- Downsample the high-resolution convolved signal to the acquisition sampling interval $TR$ (in seconds) by taking values at times $t_k = k \\cdot TR$ for integer $k$ such that $t_k \\le T_{\\text{run}}$.\n\nYour program must implement the following steps:\n1. Construct the canonical double-gamma Hemodynamic Response Function $h(t)$ with parameters $a_1 = 6$, $a_2 = 16$, $b = 1$, $c = 1/6$. Sample $h(t)$ on $t \\in [0, t_{\\max}]$ with $t_{\\max} = 32$ seconds using a high-resolution step $dt$ (specified in each test case). Set $h(t) = 0$ for $t  0$.\n2. For each test case, define the stimulus $s(t)$ as a sum of weighted Dirac deltas located at provided event times $\\tau_i$ with weights $w_i$; implement its discrete approximation on the same high-resolution grid by setting $s[k_i] = w_i/dt$ at the index $k_i$ nearest to $\\tau_i$.\n3. Compute the discrete convolution $x_{\\text{hi}} = s * h$ using the Riemann-sum scaling so that $x_{\\text{hi}}[n] = dt \\sum_k s[k]\\, h[n-k]$. Use a discrete convolution routine and multiply the result by $dt$ to approximate the continuous integral.\n4. Downsample $x_{\\text{hi}}$ at the acquisition times $t_k = k \\cdot TR$ for $k = 0, 1, \\dots, \\left\\lfloor T_{\\text{run}} / TR \\right\\rfloor$ to obtain the predicted BOLD time course $x_{\\text{BOLD}}[k]$.\n5. For each predicted BOLD time course, compute the following summary metrics:\n   - The maximum amplitude over the run, $\\max_k x_{\\text{BOLD}}[k]$.\n   - The time-to-peak in seconds, defined as $t_{k^\\star} = k^\\star \\cdot TR$ where $k^\\star = \\arg\\max_k x_{\\text{BOLD}}[k]$ (choose the smallest index in case of ties).\n   - The area under the predicted curve approximated by a Riemann sum at the acquisition resolution: $A = TR \\sum_k x_{\\text{BOLD}}[k]$.\n6. Express all floating-point outputs rounded to six decimal places. All time quantities must be expressed in seconds. Angles are not involved in this task.\n\nTest suite:\n- Test case $1$ (general happy path):\n  - $T_{\\text{run}} = 60$ s, $TR = 2$ s, $dt = 0.1$ s,\n  - event times $\\tau = [10, 30, 50]$ s,\n  - event weights $w = [1, 1, 1]$.\n- Test case $2$ (boundary events and variable sampling):\n  - $T_{\\text{run}} = 48$ s, $TR = 1.5$ s, $dt = 0.1$ s,\n  - event times $\\tau = [0, 46]$ s,\n  - event weights $w = [1, 0.5]$.\n- Test case $3$ (edge case with no events):\n  - $T_{\\text{run}} = 32$ s, $TR = 2$ s, $dt = 0.1$ s,\n  - event times $\\tau = []$,\n  - event weights $w = []$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be an inner list of three floats in the order [maximum amplitude, time-to-peak in seconds, area], each rounded to six decimal places. Thus the final output must be of the form\n$[[m_1,t_1,a_1],[m_2,t_2,a_2],[m_3,t_3,a_3]]$\nwith no spaces anywhere in the line.",
            "solution": "The problem is assessed to be **valid**. It is scientifically grounded in the principles of functional magnetic resonance imaging (fMRI) data analysis using the General Linear Model (GLM). The formulation is mathematically sound, self-contained, and well-posed, providing all necessary definitions, parameters, and data for a unique and verifiable computational solution. The task correctly formalizes a standard procedure in Statistical Parametric Mapping (SPM) for generating regressors from event-based stimuli.\n\nThe solution proceeds by implementing the specified steps to simulate the Blood Oxygen Level Dependent (BOLD) signal and calculate its summary metrics.\n\n### Step 1: Hemodynamic Response Function (HRF) Construction\n\nThe canonical double-gamma Hemodynamic Response Function (HRF), $h(t)$, is defined for $t \\ge 0$ as:\n$$\nh(t) = \\frac{t^{a_1 - 1} e^{-t/b}}{b^{a_1}\\, \\Gamma(a_1)} \\;-\\; c\\, \\frac{t^{a_2 - 1} e^{-t/b}}{b^{a_2}\\, \\Gamma(a_2)}\n$$\nwhere $\\Gamma(\\cdot)$ is the gamma function. The two terms represent two overlapping gamma probability density functions, with the first modeling the primary response and the second, scaled by $c$, modeling the subsequent undershoot. The given parameters are $a_1 = 6$, $a_2 = 16$, $b = 1$, and $c = 1/6$.\n\nThis continuous function is discretized by sampling it over a specified time interval $t \\in [0, t_{\\max}]$, where $t_{\\max} = 32$ seconds. A high-resolution uniform time grid is created with a step size $dt$. The discrete HRF, $h_{\\text{hrf}}[k]$, is a vector of function values at each time point $t_k = k \\cdot dt$ on this grid.\n\n### Step 2: Stimulus Function Discretization\n\nThe stimulus function, $s(t)$, is given as a train of ideal impulses (Dirac delta functions) at specific event times $\\tau_i$ with corresponding weights $w_i$:\n$$\ns(t) = \\sum_i w_i\\, \\delta(t - \\tau_i)\n$$\nTo approximate this continuous function on a discrete grid, we must preserve the integral property of the delta function, $\\int \\delta(t-t_0) dt = 1$. In a discrete domain with step size $dt$, an impulse at time $\\tau_i$ is represented by a single non-zero value at the nearest time index $k_i = \\text{round}(\\tau_i / dt)$. To ensure the area under the impulse is conserved (i.e., its Riemann sum equals the weight $w_i$), the value at this index is set to $w_i/dt$. This is because $\\sum_k (w_i/dt) \\cdot \\delta_{k,k_i} \\cdot dt = w_i$.\n\nA high-resolution stimulus vector, $s_{\\text{hi}}$, is initialized to zeros over the full run duration, $T_{\\text{run}}$. For each event $(\\tau_i, w_i)$, the value at index $k_i = \\text{round}(\\tau_i / dt)$ is incremented by $w_i/dt$.\n\n### Step 3: Convolution\n\nThe predicted BOLD response, $x(t)$, is the convolution of the stimulus function $s(t)$ and the HRF $h(t)$:\n$$\nx(t) = (s * h)(t) = \\int_{-\\infty}^{\\infty} s(\\tau) h(t - \\tau) d\\tau\n$$\nSubstituting the Dirac delta representation of $s(t)$ simplifies this to a sum of shifted and scaled HRFs:\n$$\nx(t) = \\sum_i w_i h(t - \\tau_i)\n$$\nThe discrete approximation of the convolution integral is a scaled summation:\n$$\nx[n] = x(n \\cdot dt) \\approx \\sum_k s(k \\cdot dt) h((n-k) \\cdot dt) \\cdot dt\n$$\nUsing the discretized stimulus $s_{\\text{hi}}[k] = s(k \\cdot dt)$ and HRF $h_{\\text{hrf}}[k] = h(k \\cdot dt)$, the high-resolution predicted BOLD signal, $x_{\\text{hi}}$, is computed as:\n$$\nx_{\\text{hi}}[n] = dt \\cdot \\sum_k s_{\\text{hi}}[k] h_{\\text{hrf}}[n-k]\n$$\nThis operation is performed using a standard discrete convolution algorithm, with the result being multiplied by the time step $dt$. The resulting convolved signal is truncated to match the run duration $T_{\\text{run}}$.\n\n### Step 4: Downsampling\n\nThe high-resolution signal $x_{\\text{hi}}$ must be downsampled to the acquisition time resolution, defined by the repetition time $TR$. The final BOLD regressor, $x_{\\text{BOLD}}$, consists of values of $x_{\\text{hi}}$ at the acquisition times $t_k = k \\cdot TR$ for $k = 0, 1, \\dots, \\lfloor T_{\\text{run}} / TR \\rfloor$. This is achieved by identifying the indices in the high-resolution grid that correspond to each acquisition time point and sampling the signal $x_{\\text{hi}}$ at those indices. The index for a given acquisition time $t_{acq}$ is found by $k_{\\text{hi}} = \\text{round}(t_{acq} / dt)$.\n\n### Step 5: Summary Metrics Calculation\n\nFinally, three summary metrics are computed from the downsampled predicted BOLD time course, $x_{\\text{BOLD}}$:\n1.  **Maximum amplitude**: This is the peak value of the signal over the entire run, calculated as $\\max_k x_{\\text{BOLD}}[k]$.\n2.  **Time-to-peak**: This is the time at which the maximum amplitude occurs. It is calculated as $t_{k^*} = k^* \\cdot TR$, where $k^*$ is the index of the first occurrence of the maximum value, $k^* = \\arg\\max_k x_{\\text{BOLD}}[k]$.\n3.  **Area under the curve**: This metric approximates the total BOLD response over the run. It is computed using a Riemann sum with the acquisition time resolution: $A = TR \\sum_k x_{\\text{BOLD}}[k]$.\n\nFor the edge case where no events occur, the stimulus vector $s_{\\text{hi}}$ is entirely zero. Consequently, the convolution result $x_{\\text{hi}}$ and the downsampled signal $x_{\\text{BOLD}}$ are also zero, leading to all three summary metrics evaluating to $0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma\n\ndef construct_and_run_glm(T_run, TR, dt, event_times, event_weights):\n    \"\"\"\n    Constructs and runs a simulation for a single GLM regressor.\n    \n    Args:\n        T_run (float): Total run duration in seconds.\n        TR (float): Repetition time (acquisition interval) in seconds.\n        dt (float): High-resolution time step for simulation in seconds.\n        event_times (list): List of event onset times in seconds.\n        event_weights (list): List of event weights.\n    \n    Returns:\n        list: A list containing [max_amplitude, time_to_peak, area_under_curve].\n    \"\"\"\n    # 1. Construct the canonical double-gamma Hemodynamic Response Function (HRF)\n    a1, a2, b, c = 6, 16, 1, 1/6\n    t_max = 32.0\n\n    # High-resolution time vector for the HRF\n    # Using linspace for precision and to include endpoint\n    hrf_len = int(t_max / dt) + 1\n    t_hrf = np.linspace(0, t_max, hrf_len)\n\n    # Gamma probability density function term\n    def gamma_pdf(t, shape, scale):\n        # Handle t=0 case where t**(shape-1) can be 0 or undefined\n        # For shape  1, t**(shape-1) is 0 at t=0.\n        # This implementation is safe for shape  1, as is the case here (6 and 16).\n        with np.errstate(divide='ignore', invalid='ignore'):\n            val = (t**(shape - 1) * np.exp(-t / scale)) / (scale**shape * gamma(shape))\n        return np.nan_to_num(val)\n\n    h = gamma_pdf(t_hrf, a1, b) - c * gamma_pdf(t_hrf, a2, b)\n    \n    # 2. Construct the discrete stimulus vector\n    # High-resolution time vector for the full run\n    run_len = int(T_run / dt) + 1\n    t_hi = np.linspace(0, T_run, run_len)\n    \n    s_hi = np.zeros(run_len)\n    \n    for tau, w in zip(event_times, event_weights):\n        # Find the nearest index in the high-resolution grid\n        k_i = int(np.round(tau / dt))\n        if 0 = k_i  run_len:\n            # Add impulse, scaled to preserve area under Riemann sum\n            s_hi[k_i] += w / dt\n    \n    # 3. Compute the discrete convolution\n    if np.all(s_hi == 0):\n        # Optimization for no-event case\n        x_hi = np.zeros(run_len)\n    else:\n        # Perform convolution and scale by dt to approximate integral\n        x_conv = np.convolve(s_hi, h, mode='full')\n        # Truncate to the length of the run\n        x_hi = x_conv[:run_len] * dt\n\n    # 4. Downsample to acquisition resolution (TR)\n    num_scans = int(T_run / TR) + 1\n    acquisition_times = np.arange(num_scans) * TR\n    \n    # Find corresponding indices in the high-resolution signal\n    sample_indices = np.round(acquisition_times / dt).astype(int)\n    \n    # Ensure indices are within bounds\n    sample_indices = np.minimum(sample_indices, run_len - 1)\n    \n    x_bold = x_hi[sample_indices]\n\n    # 5. Compute summary metrics\n    if x_bold.size == 0 or np.all(x_bold == 0):\n        max_amp = 0.0\n        t_peak = 0.0\n        area = 0.0\n    else:\n        max_amp = np.max(x_bold)\n        k_star = np.argmax(x_bold)\n        t_peak = k_star * TR\n        area = TR * np.sum(x_bold)\n        \n    return [max_amp, t_peak, area]\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (T_run, TR, dt, event_times, event_weights)\n        (60, 2, 0.1, [10, 30, 50], [1, 1, 1]),\n        (48, 1.5, 0.1, [0, 46], [1, 0.5]),\n        (32, 2, 0.1, [], []),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        T_run, TR, dt, taus, weights = case\n        metrics = construct_and_run_glm(T_run, TR, dt, taus, weights)\n        all_results.append(metrics)\n\n    # Format the output string precisely as required\n    result_strings = []\n    for res in all_results:\n        # Format each float to 6 decimal places\n        formatted_res = [f\"{val:.6f}\" for val in res]\n        result_strings.append(f\"[{','.join(formatted_res)}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "While the canonical HRF provides a good model, the true hemodynamic response can vary slightly in its timing across the brain. This practice delves into a sophisticated technique used in SPM to account for such variations: adding the temporal derivative of the HRF to the GLM. Through a focused analytical derivation, you will uncover the precise mathematical relationship between the estimated coefficient of this derivative regressor and the underlying latency of the neural response ().",
            "id": "4196024",
            "problem": "Consider an event-related Blood Oxygenation Level Dependent (BOLD) time series modeled in Statistical Parametric Mapping (SPM) using a General Linear Model (GLM). Let the canonical hemodynamic response function (HRF) regressor be a column vector $c \\in \\mathbb{R}^{T}$ formed by convolving a stimulus train with a canonical HRF and sampling at interval $\\Delta > 0$, and let the temporal derivative regressor be a column vector $d \\in \\mathbb{R}^{T}$ formed by sampling the time derivative of the canonical HRF at the same times. Assume a single condition and no confounds for simplicity. Suppose the true noise-free signal is generated by an amplitude $A \\in \\mathbb{R}$ and a small latency shift $\\tau \\in \\mathbb{R}$ (with $|\\tau| \\ll \\Delta$), so that the sampled response obeys the first-order Taylor approximation $c(t - \\tau) \\approx c(t) - \\tau \\, c^{\\prime}(t)$, which in the sampled design translates to the column vector model $y = A \\left(c - \\tau d\\right) \\in \\mathbb{R}^{T}$.\n\nIn the GLM, you construct a design matrix with two columns: the canonical $c$ and the temporal derivative $d$ after orthogonalization to $c$ via Gram–Schmidt, producing $d_{\\perp}$ that satisfies $c^{\\top} d_{\\perp} = 0$. You then estimate the GLM coefficients $\\beta_{\\text{canon}}$ and $\\beta_{\\text{deriv}}$ by ordinary least squares.\n\nStarting only from the first-order Taylor approximation of the shifted response and the standard properties of orthogonal projections and least squares for orthogonal design matrices, derive a closed-form analytic expression for the ordinary least squares estimate of the derivative coefficient $\\hat{\\beta}_{\\text{deriv}}$ in terms of $A$ and $\\tau$ only. Your final answer must be a single analytic expression. No numerical rounding is required, and no units should be included in your final expression.",
            "solution": "The user wants me to solve a problem related to Statistical Parametric Mapping (SPM) and the General Linear Model (GLM). I must first validate the problem statement and then, if it is valid, provide a detailed derivation and a final boxed answer.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Model:** General Linear Model (GLM) for an event-related Blood Oxygenation Level Dependent (BOLD) time series.\n*   **Regressors:**\n    *   $c \\in \\mathbb{R}^{T}$: Canonical hemodynamic response function (HRF) regressor.\n    *   $d \\in \\mathbb{R}^{T}$: Temporal derivative regressor.\n*   **Data Model (True Signal):** The noise-free sampled response is modeled by a first-order Taylor approximation as $y = A(c - \\tau d)$, where:\n    *   $A \\in \\mathbb{R}$ is the signal amplitude.\n    *   $\\tau \\in \\mathbb{R}$ is a small latency shift.\n*   **Design Matrix:** The GLM uses a design matrix $X$ with two columns:\n    *   The first column is the canonical regressor, $c$.\n    *   The second column is $d_{\\perp}$, which is the temporal derivative regressor $d$ after being orthogonalized with respect to $c$ via the Gram-Schmidt process. This implies the property $c^{\\top} d_{\\perp} = 0$.\n*   **Estimation:** The GLM coefficients, $\\beta_{\\text{canon}}$ and $\\beta_{\\text{deriv}}$, are estimated using ordinary least squares (OLS).\n*   **Objective:** Derive a closed-form analytic expression for the OLS estimate of the derivative coefficient, denoted $\\hat{\\beta}_{\\text{deriv}}$, in terms of only $A$ and $\\tau$.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded:** The problem is firmly grounded in the standard methods of fMRI data analysis. The use of a GLM with a canonical HRF and its temporal derivative to model latency shifts is a well-established and fundamental technique in SPM. The first-order Taylor series approximation is the mathematical basis for this method. The setup is scientifically sound.\n*   **Well-Posed:** The problem provides a clear data model, a defined design matrix, a specified estimation method (OLS), and a precise objective. It contains all necessary information to derive a unique solution for the requested parameter.\n*   **Objective:** The language is formal, precise, and free of any subjective or ambiguous terminology. All variables and processes (GLM, OLS, Gram-Schmidt) are standard in statistics and signal processing.\n\nThe problem does not exhibit any of the invalidity flags:\n1.  It is not scientifically or factually unsound.\n2.  It is directly formalizable and relevant to the topic of SPM.\n3.  The setup is complete and consistent.\n4.  The conditions are a standard, accepted simplification used in modeling fMRI signals.\n5.  It is well-posed and admits a unique, meaningful solution.\n6.  The problem is not trivial; it requires a formal derivation using linear algebra.\n7.  The derivation is mathematically verifiable.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. The solution will now be derived.\n\n### Derivation\n\nThe General Linear Model (GLM) is expressed as $y = X\\beta + \\epsilon$, where $y$ is the data vector, $X$ is the design matrix, $\\beta$ is the vector of coefficients, and $\\epsilon$ is the error term. In this problem, we consider the noise-free case, so the data is given by $y = A(c - \\tau d)$.\n\nThe design matrix is $X = \\begin{pmatrix} c  d_{\\perp} \\end{pmatrix}$. The vector of coefficients to be estimated is $\\beta = \\begin{pmatrix} \\beta_{\\text{canon}} \\\\ \\beta_{\\text{deriv}} \\end{pmatrix}$.\n\nThe ordinary least squares (OLS) estimate for $\\beta$ is given by the normal equations:\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y $$\n\nThe problem states that the columns of the design matrix are orthogonal, which means $c^{\\top}d_{\\perp} = 0$. This greatly simplifies the term $X^{\\top}X$:\n$$ X^{\\top}X = \\begin{pmatrix} c^{\\top} \\\\ d_{\\perp}^{\\top} \\end{pmatrix} \\begin{pmatrix} c  d_{\\perp} \\end{pmatrix} = \\begin{pmatrix} c^{\\top}c  c^{\\top}d_{\\perp} \\\\ d_{\\perp}^{\\top}c  d_{\\perp}^{\\top}d_{\\perp} \\end{pmatrix} = \\begin{pmatrix} c^{\\top}c  0 \\\\ 0  d_{\\perp}^{\\top}d_{\\perp} \\end{pmatrix} $$\nThis matrix is diagonal. Its inverse is also diagonal:\n$$ (X^{\\top}X)^{-1} = \\begin{pmatrix} (c^{\\top}c)^{-1}  0 \\\\ 0  (d_{\\perp}^{\\top}d_{\\perp})^{-1} \\end{pmatrix} $$\n\nSubstituting this back into the OLS estimator formula:\n$$ \\hat{\\beta} = \\begin{pmatrix} (c^{\\top}c)^{-1}  0 \\\\ 0  (d_{\\perp}^{\\top}d_{\\perp})^{-1} \\end{pmatrix} \\begin{pmatrix} c^{\\top} \\\\ d_{\\perp}^{\\top} \\end{pmatrix} y = \\begin{pmatrix} (c^{\\top}c)^{-1} c^{\\top}y \\\\ (d_{\\perp}^{\\top}d_{\\perp})^{-1} d_{\\perp}^{\\top}y \\end{pmatrix} $$\n\nWe are interested in the estimate for the derivative coefficient, $\\hat{\\beta}_{\\text{deriv}}$, which corresponds to the second element of the $\\hat{\\beta}$ vector:\n$$ \\hat{\\beta}_{\\text{deriv}} = (d_{\\perp}^{\\top}d_{\\perp})^{-1} d_{\\perp}^{\\top}y = \\frac{d_{\\perp}^{\\top}y}{d_{\\perp}^{\\top}d_{\\perp}} $$\n\nNow, we substitute the given model for the true signal, $y = A(c - \\tau d)$, into this expression:\n$$ \\hat{\\beta}_{\\text{deriv}} = \\frac{d_{\\perp}^{\\top} \\left[ A(c - \\tau d) \\right]}{d_{\\perp}^{\\top}d_{\\perp}} $$\nThe amplitude $A$ is a scalar and can be factored out:\n$$ \\hat{\\beta}_{\\text{deriv}} = \\frac{A}{d_{\\perp}^{\\top}d_{\\perp}} \\left( d_{\\perp}^{\\top}c - \\tau d_{\\perp}^{\\top}d \\right) $$\n\nBy construction, $d_{\\perp}$ is orthogonal to $c$, so their dot product $d_{\\perp}^{\\top}c$ is $0$. The expression simplifies to:\n$$ \\hat{\\beta}_{\\text{deriv}} = \\frac{A}{d_{\\perp}^{\\top}d_{\\perp}} \\left( 0 - \\tau d_{\\perp}^{\\top}d \\right) = -A\\tau \\frac{d_{\\perp}^{\\top}d}{d_{\\perp}^{\\top}d_{\\perp}} $$\n\nThe final step is to analyze the relationship between $d_{\\perp}$, $d$, and $c$. The regressor $d_{\\perp}$ is obtained by orthogonalizing $d$ with respect to $c$ using the Gram-Schmidt process. This means we subtract the projection of $d$ onto $c$ from $d$:\n$$ d_{\\perp} = d - \\text{proj}_{c}(d) = d - \\frac{c^{\\top}d}{c^{\\top}c}c $$\n\nLet us now compute the term $d_{\\perp}^{\\top}d$ found in the numerator of our expression for $\\hat{\\beta}_{\\text{deriv}}$:\n$$ d_{\\perp}^{\\top}d = \\left( d - \\frac{c^{\\top}d}{c^{\\top}c}c \\right)^{\\top} d = d^{\\top}d - \\frac{c^{\\top}d}{c^{\\top}c}c^{\\top}d = \\|d\\|^2 - \\frac{(c^{\\top}d)^2}{\\|c\\|^2} $$\n\nNext, let's compute the term $d_{\\perp}^{\\top}d_{\\perp}$ (i.e., the squared norm of $d_{\\perp}$) from the denominator. A key property of this construction is that $d_{\\perp}$ is orthogonal to the component that was subtracted from $d$. We can write $d = d_{\\perp} + \\text{proj}_{c}(d)$. Since $d_{\\perp}$ and $\\text{proj}_{c}(d)$ are orthogonal, the Pythagorean theorem for vectors applies: $\\|d\\|^2 = \\|d_{\\perp}\\|^2 + \\|\\text{proj}_{c}(d)\\|^2$.\nAlternatively, we can directly compute $d_{\\perp}^{\\top}d_{\\perp}$:\n$$ d_{\\perp}^{\\top}d_{\\perp} = d_{\\perp}^{\\top} \\left( d - \\frac{c^{\\top}d}{c^{\\top}c}c \\right) = d_{\\perp}^{\\top}d - \\frac{c^{\\top}d}{c^{\\top}c}d_{\\perp}^{\\top}c $$\nSince $d_{\\perp}^{\\top}c=0$, the second term vanishes. This leaves us with a crucial identity:\n$$ d_{\\perp}^{\\top}d_{\\perp} = d_{\\perp}^{\\top}d $$\n\nThis identity holds true as long as $d_{\\perp}$ is not the zero vector (which would only happen if $d$ were perfectly collinear with $c$, a case we can exclude as impractical for this model).\n\nSubstituting this identity back into the expression for $\\hat{\\beta}_{\\text{deriv}}$:\n$$ \\hat{\\beta}_{\\text{deriv}} = -A\\tau \\frac{d_{\\perp}^{\\top}d}{d_{\\perp}^{\\top}d_{\\perp}} = -A\\tau \\frac{d_{\\perp}^{\\top}d_{\\perp}}{d_{\\perp}^{\\top}d_{\\perp}} = -A\\tau $$\n\nThe OLS estimate of the derivative coefficient is directly the negative product of the true signal amplitude and the latency shift.",
            "answer": "$$ \\boxed{-A \\tau} $$"
        },
        {
            "introduction": "A major statistical challenge in neuroimaging is the multiple comparisons problem: testing hypotheses in tens of thousands of voxels simultaneously dramatically increases the chance of finding false positives. This practice guides you through the implementation of a powerful non-parametric method to control for these errors, known as the maximum t-statistic permutation test. By simulating the null distribution directly from the data, you will learn how to compute family-wise error (FWE) corrected p-values, a gold standard for ensuring the statistical robustness of brain-wide findings ().",
            "id": "4196015",
            "problem": "You are given voxelwise data matrices that represent Statistical Parametric Maps (SPMs) in a two-group study design. Each matrix row corresponds to a subject and each column corresponds to a voxel. Your task is to implement a nonparametric family-wise error (FWE) control procedure using the maximum absolute t-statistic across voxels (the max-t method). The program must construct, for each test case, an empirical permutation distribution of the maximum absolute t-statistic by exhaustively enumerating all labelings consistent with the provided group sizes. Using this permutation distribution, assign a two-sided FWE-corrected p-value to each voxel.\n\nFundamental base and assumptions to use:\n- Use the classical two-sample design with two groups defined by binary labels, and assume that under the null hypothesis the labels are exchangeable.\n- Use the pooled-variance two-sample t-statistic at each voxel to quantify group mean differences. The statistic must be computed separately at each voxel from the sample means and sample variances of the two groups derived from the current label assignment.\n- Use a two-sided test by considering the absolute value of the t-statistic at each voxel.\n- For each permutation, compute the maximum absolute t-statistic across all voxels; this defines one sample from the null distribution of the max-t statistic.\n- Compute FWE-corrected p-values using an inclusive counting rule over the permutation distribution of the max-t statistic.\n\nYou must implement the following steps for each test case:\n1. Construct the observed labeling set for the two groups and compute the observed voxelwise t-statistics from the data matrix.\n2. Enumerate all unique label assignments that place exactly the specified number of subjects in group $1$ and the remainder in group $2$. For each such labeling, compute the voxelwise t-statistics and retain the maximum absolute t-statistic across all voxels. This yields the empirical permutation distribution of the max-t statistic.\n3. For each voxel, compute the two-sided FWE-corrected p-value as the fraction of permutation max-t values that are greater than or equal to the observed absolute t-statistic at that voxel, using an inclusive adjustment by adding $1$ to both the numerator and denominator.\n4. If a denominator in the t-statistic computation equals $0$ at any voxel under any labeling, treat the corresponding t-statistic as $0$ for that voxel-labeling combination.\n5. Round each reported p-value to exactly six decimal places.\n\nTest suite:\nImplement your program to evaluate the following three test cases. In each case, let $Y \\in \\mathbb{R}^{n \\times v}$ denote the data matrix with $n$ subjects (rows) and $v$ voxels (columns). The observed labeling is given by the set of indices for group $1$; the complement forms group $2$.\n\n- Case $1$ (general multi-voxel):\n  - $n = 8$, $v = 5$.\n  - Observed group $1$ indices: $\\{0, 1, 2, 3\\}$.\n  - Data matrix $Y$ with rows listed in order from subject $0$ to subject $7$:\n    - Subject $0$: $[0.5, 1.2, 2.5, -0.4, 0.1]$\n    - Subject $1$: $[-0.2, 0.9, 2.7, -0.6, -0.3]$\n    - Subject $2$: $[0.3, 1.1, 2.9, -0.7, 0.2]$\n    - Subject $3$: $[0.1, 1.0, 2.6, -0.5, 0.0]$\n    - Subject $4$: $[0.4, 0.8, 1.0, 0.2, -0.1]$\n    - Subject $5$: $[-0.1, 1.1, 0.9, 0.3, 0.1]$\n    - Subject $6$: $[0.2, 0.7, 1.1, 0.4, 0.0]$\n    - Subject $7$: $[0.0, 0.9, 0.8, 0.1, 0.2]$\n\n- Case $2$ (small-sample, potential ties):\n  - $n = 4$, $v = 3$.\n  - Observed group $1$ indices: $\\{0, 1\\}$.\n  - Data matrix $Y$:\n    - Subject $0$: $[1.0, 0.0, -0.2]$\n    - Subject $1$: $[1.1, 0.1, -0.1]$\n    - Subject $2$: $[0.9, -0.1, -0.2]$\n    - Subject $3$: $[1.2, 0.0, -0.1]$\n\n- Case $3$ (single-voxel boundary):\n  - $n = 6$, $v = 1$.\n  - Observed group $1$ indices: $\\{0, 1, 2\\}$.\n  - Data matrix $Y$:\n    - Subject $0$: $[0.3]$\n    - Subject $1$: $[0.4]$\n    - Subject $2$: $[0.2]$\n    - Subject $3$: $[-0.1]$\n    - Subject $4$: $[0.0]$\n    - Subject $5$: $[0.1]$\n\nPermutation policy:\n- For each case, enumerate all unique assignments of exactly the specified number of subjects to group $1$ (that is, all subsets of indices of the required size), and treat each assignment as a distinct permutation.\n\nFWE-corrected p-value policy:\n- For voxel $j$, let $t^{\\mathrm{obs}}_j$ be the observed t-statistic. Let $M$ denote the multiset of maximum absolute t-statistics over all permutations. The FWE-corrected p-value is\n  the inclusive fraction of values in $M$ that are greater than or equal to $\\lvert t^{\\mathrm{obs}}_j \\rvert$, with an addition of $1$ to both numerator and denominator.\n\nOutput specification:\n- The final program output must be a single line containing a list of results for the three cases, in order, where each case’s result is a list of its voxels’ FWE-corrected p-values rounded to six decimal places.\n- The exact output format is a single line with no extra text and no spaces: for example, for three cases, it should look like\n  - $[[p\\_{1,1},p\\_{1,2},\\dots],[p\\_{2,1},p\\_{2,2},\\dots],[p\\_{3,1},\\dots]]$\n  where each $p\\_{i,j}$ is a decimal rounded to six places.\n\nAngles and physical units do not apply. Percentages must not be used; report probabilities as decimals.\n\nYour program must not read any input and must run deterministically as provided. It must rely only on the standard libraries and the specified numerical library.",
            "solution": "The problem requires the implementation of a nonparametric permutation testing procedure to control the family-wise error rate (FWE) when performing statistical tests across multiple voxels in a neuroimaging context. The method specified is the maximum absolute t-statistic (max-t) approach, applied to a two-group study design. The solution involves calculating FWE-corrected p-values for each voxel based on an empirically generated null distribution.\n\nThe fundamental principle underlying this method is the concept of exchangeability under the null hypothesis ($H_0$). $H_0$ posits that there is no true difference between the two experimental groups. If this is true, the assignment of group labels to individual subjects is arbitrary. Consequently, any permutation of these labels is equally likely. By systematically reassigning labels, computing a test statistic for each reassignment, and observing the distribution of these statistics, we can construct an empirical null distribution. This distribution represents the range of outcomes expected purely by chance, under the assumption that $H_0$ is true.\n\nTo control the FWE—the probability of making one or more Type I errors across all voxels—we cannot simply use the distribution of t-statistics from individual voxels. Instead, we must consider the family of tests as a whole. The max-t method achieves this by constructing a null distribution of the *maximum* statistic. For each permutation of labels, we compute the t-statistic at every voxel and identify the maximum of their absolute values. This single value, $T_{\\text{max}}$, is one sample from the null distribution of the most extreme effect one might find anywhere in the volume by chance.\n\nThe step-by-step procedure is as follows:\n\n1.  **Observed Statistic Calculation**: First, we compute the test statistic for the original, unpermuted data. For each voxel $j$, where $j \\in \\{1, \\dots, v\\}$, we use the pooled-variance two-sample t-statistic. Given data from two groups, $G_1$ of size $n_1$ and $G_2$ of size $n_2$, the t-statistic for voxel $j$ is:\n    $$ t_j = \\frac{\\bar{y}_{1j} - \\bar{y}_{2j}}{s_{p,j} \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} $$\n    where $\\bar{y}_{1j}$ and $\\bar{y}_{2j}$ are the sample means of the data in voxel $j$ for groups $G_1$ and $G_2$, respectively. The term $s_{p,j}$ is the pooled standard deviation for voxel $j$, calculated from the sample variances of the two groups, $s_{1j}^2$ and $s_{2j}^2$:\n    $$ s_{p,j} = \\sqrt{\\frac{(n_1-1)s_{1j}^2 + (n_2-1)s_{2j}^2}{n_1+n_2-2}} $$\n    The sample variance for a group with data $\\{y_i\\}$ and mean $\\bar{y}$ is $s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(y_i - \\bar{y})^2$. In the specific case where the denominator of the t-statistic is zero (i.e., $s_{p,j} = 0$), the problem mandates that the t-statistic be treated as $0$. This occurs if and only if the variance within each group is zero for that voxel. We compute these observed statistics, $t^{\\text{obs}}_j$, for all $v$ voxels.\n\n2.  **Permutation Distribution of the Maximum Statistic**: We generate all unique partitions of the $n$ subjects into two groups of sizes $n_1$ and $n_2$. The total number of such partitions (permutations) is $N_{\\text{perm}} = \\binom{n}{n_1}$. For each permutation:\n    a. We re-calculate the t-statistic, $t_j^{\\text{perm}}$, for every voxel $j=1, \\dots, v$ using the new group assignments.\n    b. We find the maximum absolute t-statistic across all voxels for this single permutation:\n       $$ T_{\\text{max}}^{\\text{perm}} = \\max_{j=1,\\dots,v} |t_j^{\\text{perm}}| $$\n    c. This value is stored. The collection of all $N_{\\text{perm}}$ such maximum values forms the empirical null distribution, $M = \\{T_{\\text{max}}^{\\text{perm}_1}, T_{\\text{max}}^{\\text{perm}_2}, \\dots, T_{\\text{max}}^{\\text{perm}_{N_{\\text{perm}}}}\\}$.\n\n3.  **FWE-Corrected p-value Calculation**: The FWE-corrected p-value for a given voxel $j$, $p_{\\text{FWE}, j}$, is the probability of observing a maximum statistic in the null distribution that is at least as extreme as the observed statistic for that voxel, $|t^{\\text{obs}}_j|$. This is estimated from our empirical distribution $M$. We count the number of elements in $M$ that are greater than or equal to $|t^{\\text{obs}}_j|$. Let this count be $k_j$. As per the problem's specification for an inclusive calculation, the p-value is given by:\n    $$ p_{\\text{FWE}, j} = \\frac{k_j + 1}{N_{\\text{perm}} + 1} $$\n    This formula adjusts for the finite number of permutations and ensures that a p-value can never be zero, which is a desirable property for permutation tests. This procedure is repeated for each voxel $j=1, \\dots, v$.\n\n4.  **Final Output**: The resulting p-values for each test case are rounded to six decimal places and formatted as specified. This procedure provides a rigorous, interpretable measure of statistical significance that accounts for the massive multiple comparisons problem inherent in whole-volume brain data analysis.",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to run the FWE correction procedure on all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"Y\": np.array([\n                [0.5, 1.2, 2.5, -0.4, 0.1],\n                [-0.2, 0.9, 2.7, -0.6, -0.3],\n                [0.3, 1.1, 2.9, -0.7, 0.2],\n                [0.1, 1.0, 2.6, -0.5, 0.0],\n                [0.4, 0.8, 1.0, 0.2, -0.1],\n                [-0.1, 1.1, 0.9, 0.3, 0.1],\n                [0.2, 0.7, 1.1, 0.4, 0.0],\n                [0.0, 0.9, 0.8, 0.1, 0.2]\n            ]),\n            \"group1_indices\": {0, 1, 2, 3}\n        },\n        {\n            \"Y\": np.array([\n                [1.0, 0.0, -0.2],\n                [1.1, 0.1, -0.1],\n                [0.9, -0.1, -0.2],\n                [1.2, 0.0, -0.1]\n            ]),\n            \"group1_indices\": {0, 1}\n        },\n        {\n            \"Y\": np.array([\n                [0.3], [0.4], [0.2], [-0.1], [0.0], [0.1]\n            ]),\n            \"group1_indices\": {0, 1, 2}\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        Y = case[\"Y\"]\n        observed_group1_indices = case[\"group1_indices\"]\n        \n        n, v = Y.shape\n        n1 = len(observed_group1_indices)\n        all_indices = set(range(n))\n        \n        # Step 1: Compute observed t-statistics\n        observed_group2_indices = all_indices - observed_group1_indices\n        observed_t_stats = _compute_t_stats_for_labeling(Y, list(observed_group1_indices), list(observed_group2_indices))\n        abs_observed_t_stats = np.abs(observed_t_stats)\n\n        # Step 2: Enumerate permutations and build max-t distribution\n        max_t_distribution = []\n        all_possible_group1s = combinations(range(n), n1)\n        \n        num_permutations = 0\n        for g1_indices_perm_tuple in all_possible_group1s:\n            num_permutations += 1\n            g1_indices_perm = list(g1_indices_perm_tuple)\n            g2_indices_perm = list(all_indices - set(g1_indices_perm))\n            \n            t_stats_perm = _compute_t_stats_for_labeling(Y, g1_indices_perm, g2_indices_perm)\n            max_abs_t = np.max(np.abs(t_stats_perm))\n            max_t_distribution.append(max_abs_t)\n            \n        max_t_distribution = np.array(max_t_distribution)\n\n        # Step 3: Compute FWE-corrected p-values\n        p_values = []\n        for j in range(v):\n            observed_t_val = abs_observed_t_stats[j]\n            # Inclusive counting\n            k = np.sum(max_t_distribution = observed_t_val)\n            # Add 1 to numerator and denominator\n            p_val = (k + 1) / (num_permutations + 1)\n            p_values.append(p_val)\n            \n        all_results.append(p_values)\n\n    # Format output string\n    case_strings = []\n    for p_list in all_results:\n        # Step 5: Round each p-value to exactly six decimal places\n        formatted_p_values = [f\"{p:.6f}\" for p in p_list]\n        case_strings.append(f\"[{','.join(formatted_p_values)}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\ndef _compute_t_stats_for_labeling(Y, g1_indices, g2_indices):\n    \"\"\"\n    Computes pooled-variance two-sample t-statistics for all voxels.\n    \"\"\"\n    n, v = Y.shape\n    n1 = len(g1_indices)\n    n2 = len(g2_indices)\n    \n    # This should not happen in this problem's cases, but is good practice\n    if n1  2 or n2  2:\n        return np.zeros(v)\n\n    g1_data = Y[g1_indices, :]\n    g2_data = Y[g2_indices, :]\n\n    mean1 = np.mean(g1_data, axis=0)\n    mean2 = np.mean(g2_data, axis=0)\n    \n    var1 = np.var(g1_data, axis=0, ddof=1)\n    var2 = np.var(g2_data, axis=0, ddof=1)\n    \n    # Pooled standard deviation\n    s_p_squared_num = (n1 - 1) * var1 + (n2 - 1) * var2\n    s_p_squared_den = n1 + n2 - 2\n    \n    # Handle cases where denominator is zero (i.e. all data points are same)\n    if s_p_squared_den == 0:\n        return np.zeros(v)\n\n    pooled_var = s_p_squared_num / s_p_squared_den\n    \n    # Avoid sqrt of negative due to floating point inaccuracies\n    pooled_var[pooled_var  0] = 0\n    pooled_std = np.sqrt(pooled_var)\n\n    # t-statistic denominator\n    t_stat_den_factor = np.sqrt(1/n1 + 1/n2)\n    t_stat_denominator = pooled_std * t_stat_den_factor\n    \n    # Compute t-statistic, handling zero denominator as per problem spec\n    t_stats = np.zeros(v)\n    non_zero_den_mask = t_stat_denominator != 0\n    t_stats[non_zero_den_mask] = (mean1[non_zero_den_mask] - mean2[non_zero_den_mask]) / t_stat_denominator[non_zero_den_mask]\n    \n    return t_stats\n\nsolve()\n```"
        }
    ]
}