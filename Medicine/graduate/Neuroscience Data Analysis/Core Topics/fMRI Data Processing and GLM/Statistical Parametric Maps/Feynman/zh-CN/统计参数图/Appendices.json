{
    "hands_on_practices": [
        {
            "introduction": "通用线性模型（GLM）是功能磁共振成像（fMRI）分析的基石，但其预测变量——即回归量——究竟是如何构建的呢？本练习将带您体验从刺激时间模型与血流动力学响应函数（HRF）进行卷积，从而生成预测的血氧水平依赖（BOLD）信号的基本过程。掌握这一步骤对于理解事件相关设计如何建模以及构建有效的大脑活动统计模型至关重要。",
            "id": "4196045",
            "problem": "您将为一个用于统计参数图（SPM）的通用线性模型（GLM）实现回归量的构建，该构建基于事件时间点与血流动力学响应函数（HRF）的卷积。目标是从连续时间中的卷积定义出发，建立一个适合计算的离散化近似，并从给定的事件序列中计算出预测的血氧水平依赖（BOLD）时间序列。该问题的根本基础是连续时间卷积定义和以刺激函数与HRF卷积形成的回归量为基础的GLM框架。具体而言，GLM将测量信号$y(t)$表示为回归量$x_j(t)$的线性组合外加噪声，每个回归量由刺激函数$s_j(t)$与HRF $h(t)$的卷积形成。您必须将刺激形式化为在事件时间点上的一系列狄拉克δ函数之和，并通过数值卷积推导出回归量的离散时间近似。\n\n基本定义：\n- 两个函数$s(t)$和$h(t)$的卷积定义为\n$$\nx(t) = (s * h)(t) = \\int_{-\\infty}^{+\\infty} s(\\tau)\\, h(t - \\tau)\\, d\\tau.\n$$\n- 如果刺激是一个由狄拉克δ函数$s(t) = \\sum_i w_i\\, \\delta(t - \\tau_i)$表示的事件序列，其中事件时间为$\\tau_i$，权重为$w_i$，那么根据狄拉克δ函数的性质，连续时间卷积可计算为\n$$\nx(t) = \\sum_i w_i\\, h(t - \\tau_i).\n$$\n- 一种广泛使用的血流动力学响应函数（HRF）是经典的双伽玛模型：\n$$\nh(t) = \\frac{t^{a_1 - 1} e^{-t/b}}{b^{a_1}\\, \\Gamma(a_1)} \\;-\\; c\\, \\frac{t^{a_2 - 1} e^{-t/b}}{b^{a_2}\\, \\Gamma(a_2)},\n\\quad t \\ge 0,\n$$\n且当$t  0$时$h(t) = 0$，其中$\\Gamma(\\cdot)$是伽玛函数，$a_1$、$a_2$、$b$、$c$是正常数。\n\n离散时间近似目标：\n- 使用一个步长为$dt$（单位：秒）的均匀高分辨率时间网格来近似连续时间卷积。令离散刺激为$s[k]$，其构建旨在近似狄拉克δ函数的总和。为在黎曼和下标度保持正确，一个位于时间索引$k_i$且权重为$w_i$的事件，在离散时间中应表示为$s[k_i] = w_i/dt$，从而使得\n$$\nx[n] \\approx dt \\sum_{k} s[k]\\, h[n-k] = \\sum_i w_i\\, h[n - k_i],\n$$\n这与连续时间公式一致。\n- 通过在时间点$t_k = k \\cdot TR$（对于整数$k$且$t_k \\le T_{\\text{run}}$）取值，将高分辨率的卷积信号降采样至采集采样间隔$TR$（单位：秒）。\n\n您的程序必须实现以下步骤：\n1. 使用参数$a_1 = 6$、$a_2 = 16$、$b = 1$、$c = 1/6$构建经典的双伽玛血流动力学响应函数$h(t)$。使用高分辨率步长$dt$（在每个测试用例中指定）在$t \\in [0, t_{\\max}]$（其中$t_{\\max} = 32$秒）上对$h(t)$进行采样。对于$t  0$，设置$h(t) = 0$。\n2. 对于每个测试用例，将刺激$s(t)$定义为位于给定事件时间$\\tau_i$且带有权重$w_i$的加权狄拉克δ函数之和；通过在最接近$\\tau_i$的索引$k_i$处设置$s[k_i] = w_i/dt$，在相同的高分辨率网格上实现其离散近似。\n3. 使用黎曼和缩放计算离散卷积$x_{\\text{hi}} = s * h$，使得$x_{\\text{hi}}[n] = dt \\sum_k s[k]\\, h[n-k]$。使用离散卷积例程，并将结果乘以$dt$来近似连续积分。\n4. 在采集时间$t_k = k \\cdot TR$（对于$k = 0, 1, \\dots, \\left\\lfloor T_{\\text{run}} / TR \\right\\rfloor$）对$x_{\\text{hi}}$进行降采样，以获得预测的BOLD时间序列$x_{\\text{BOLD}}[k]$。\n5. 对于每个预测的BOLD时间序列，计算以下汇总指标：\n   - 整个运行期间的最大振幅，$\\max_k x_{\\text{BOLD}}[k]$。\n   - 峰值时间（单位：秒），定义为$t_{k^\\star} = k^\\star \\cdot TR$，其中$k^\\star = \\arg\\max_k x_{\\text{BOLD}}[k]$（如有多个最大值，选择最小的索引）。\n   - 在采集分辨率下通过黎曼和近似的预测曲线下面积：$A = TR \\sum_k x_{\\text{BOLD}}[k]$。\n6. 所有浮点数输出均四舍五入至六位小数。所有时间量必须以秒为单位表示。此任务不涉及角度。\n\n测试套件：\n- 测试用例1（一般成功路径）：\n  - $T_{\\text{run}} = 60$秒，$TR = 2$秒，$dt = 0.1$秒，\n  - 事件时间$\\tau = [10, 30, 50]$秒，\n  - 事件权重$w = [1, 1, 1]$。\n- 测试用例2（边界事件和可变采样）：\n  - $T_{\\text{run}} = 48$秒，$TR = 1.5$秒，$dt = 0.1$秒，\n  - 事件时间$\\tau = [0, 46]$秒，\n  - 事件权重$w = [1, 0.5]$。\n- 测试用例3（无事件的边缘情况）：\n  - $T_{\\text{run}} = 32$秒，$TR = 2$秒，$dt = 0.1$秒，\n  - 事件时间$\\tau = []$，\n  - 事件权重$w = []$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔结果列表。每个测试用例的结果必须是一个包含三个浮点数的内部列表，顺序为[最大振幅, 峰值时间（秒）, 面积]，每个浮点数四舍五入至六位小数。因此，最终输出必须为以下形式\n$[[m_1,t_1,a_1],[m_2,t_2,a_2],[m_3,t_3,a_3]]$\n行内任何地方都没有空格。",
            "solution": "该问题被评估为**有效**的。它在科学上基于使用通用线性模型（GLM）进行功能性磁共振成像（fMRI）数据分析的原理。其表述在数学上是合理的、自洽的、且适定的，为获得唯一且可验证的计算解提供了所有必要的定义、参数和数据。该任务正确地形式化了统计参数图（SPM）中从基于事件的刺激生成回归量的标准流程。\n\n该解决方案通过实现指定的步骤来模拟血氧水平依赖（BOLD）信号并计算其汇总指标。\n\n### 步骤1：血流动力学响应函数（HRF）的构建\n\n经典的双伽玛血流动力学响应函数（HRF）$h(t)$，在$t \\ge 0$时定义为：\n$$\nh(t) = \\frac{t^{a_1 - 1} e^{-t/b}}{b^{a_1}\\, \\Gamma(a_1)} \\;-\\; c\\, \\frac{t^{a_2 - 1} e^{-t/b}}{b^{a_2}\\, \\Gamma(a_2)}\n$$\n其中$\\Gamma(\\cdot)$是伽玛函数。这两项代表两个重叠的伽玛概率密度函数，第一项模拟主响应，第二项经$c$缩放后模拟随后的下冲。给定的参数为$a_1 = 6$、$a_2 = 16$、$b = 1$和$c = 1/6$。\n\n通过在指定的时间区间$t \\in [0, t_{\\max}]$（其中$t_{\\max} = 32$秒）上对其进行采样，从而离散化此连续函数。我们创建一个具有步长$dt$的高分辨率均匀时间网格。离散的HRF，$h_{\\text{hrf}}[k]$，是在此网格上每个时间点$t_k = k \\cdot dt$的函数值向量。\n\n### 步骤2：刺激函数的离散化\n\n刺激函数$s(t)$以理想脉冲（狄拉克δ函数）序列的形式给出，在特定的事件时间$\\tau_i$具有相应的权重$w_i$：\n$$\ns(t) = \\sum_i w_i\\, \\delta(t - \\tau_i)\n$$\n为了在离散网格上近似这个连续函数，我们必须保持δ函数的积分性质，即$\\int \\delta(t-t_0) dt = 1$。在步长为$dt$的离散域中，时间$\\tau_i$处的脉冲由最接近的时间索引$k_i = \\text{round}(\\tau_i / dt)$处的单个非零值表示。为确保脉冲下的面积得以保持（即其黎曼和等于权重$w_i$），此索引处的值被设置为$w_i/dt$。这是因为$\\sum_k (w_i/dt) \\cdot \\delta_{k,k_i} \\cdot dt = w_i$。\n\n一个高分辨率的刺激向量$s_{\\text{hi}}$在整个运行时间$T_{\\text{run}}$上初始化为零。对于每个事件$(\\tau_i, w_i)$，在索引$k_i = \\text{round}(\\tau_i / dt)$处的值增加$w_i/dt$。\n\n### 步骤3：卷积\n\n预测的BOLD响应$x(t)$是刺激函数$s(t)$和HRF $h(t)$的卷积：\n$$\nx(t) = (s * h)(t) = \\int_{-\\infty}^{\\infty} s(\\tau) h(t - \\tau) d\\tau\n$$\n代入$s(t)$的狄拉克δ函数表示，可将其简化为一系列平移和缩放的HRF之和：\n$$\nx(t) = \\sum_i w_i h(t - \\tau_i)\n$$\n卷积积分的离散近似是一个缩放后的求和：\n$$\nx[n] = x(n \\cdot dt) \\approx \\sum_k s(k \\cdot dt) h((n-k) \\cdot dt) \\cdot dt\n$$\n使用离散化的刺激$s_{\\text{hi}}[k] = s(k \\cdot dt)$和HRF $h_{\\text{hrf}}[k] = h(k \\cdot dt)$，高分辨率的预测BOLD信号$x_{\\text{hi}}$计算如下：\n$$\nx_{\\text{hi}}[n] = dt \\cdot \\sum_k s_{\\text{hi}}[k] h_{\\text{hrf}}[n-k]\n$$\n此操作使用标准的离散卷积算法执行，其结果乘以时间步长$dt$。得到的卷积信号被截断以匹配运行时间$T_{\\text{run}}$。\n\n### 步骤4：降采样\n\n高分辨率信号$x_{\\text{hi}}$必须被降采样到由重复时间$TR$定义的采集时间分辨率。最终的BOLD回归量$x_{\\text{BOLD}}$由$x_{\\text{hi}}$在采集时间点$t_k = k \\cdot TR$（对于$k = 0, 1, \\dots, \\lfloor T_{\\text{run}} / TR \\rfloor$）的值组成。这通过识别高分辨率网格中与每个采集时间点相对应的索引，并在这些索引处对信号$x_{\\text{hi}}$进行采样来实现。对于给定的采集时间$t_{acq}$，其索引通过$k_{\\text{hi}} = \\text{round}(t_{acq} / dt)$找到。\n\n### 步骤5：汇总指标计算\n\n最后，从降采样后的预测BOLD时间序列$x_{\\text{BOLD}}$中计算三个汇总指标：\n1.  **最大振幅**：这是信号在整个运行期间的峰值，计算为$\\max_k x_{\\text{BOLD}}[k]$。\n2.  **峰值时间**：这是最大振幅出现的时间。计算为$t_{k^*} = k^* \\cdot TR$，其中$k^*$是最大值首次出现的索引，$k^* = \\arg\\max_k x_{\\text{BOLD}}[k]$。\n3.  **曲线下面积**：此指标近似了整个运行期间的总BOLD响应。它使用采集时间分辨率的黎曼和计算：$A = TR \\sum_k x_{\\text{BOLD}}[k]$。\n\n对于没有事件发生的边缘情况，刺激向量$s_{\\text{hi}}$完全为零。因此，卷积结果$x_{\\text{hi}}$和降采样后的信号$x_{\\text{BOLD}}$也为零，导致所有三个汇总指标的计算结果都为0。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma\n\ndef construct_and_run_glm(T_run, TR, dt, event_times, event_weights):\n    \"\"\"\n    Constructs and runs a simulation for a single GLM regressor.\n    \n    Args:\n        T_run (float): Total run duration in seconds.\n        TR (float): Repetition time (acquisition interval) in seconds.\n        dt (float): High-resolution time step for simulation in seconds.\n        event_times (list): List of event onset times in seconds.\n        event_weights (list): List of event weights.\n    \n    Returns:\n        list: A list containing [max_amplitude, time_to_peak, area_under_curve].\n    \"\"\"\n    # 1. Construct the canonical double-gamma Hemodynamic Response Function (HRF)\n    a1, a2, b, c = 6, 16, 1, 1/6\n    t_max = 32.0\n\n    # High-resolution time vector for the HRF\n    # Using linspace for precision and to include endpoint\n    hrf_len = int(t_max / dt) + 1\n    t_hrf = np.linspace(0, t_max, hrf_len)\n\n    # Gamma probability density function term\n    def gamma_pdf(t, shape, scale):\n        # Handle t=0 case where t**(shape-1) can be 0 or undefined\n        # For shape > 1, t**(shape-1) is 0 at t=0.\n        # This implementation is safe for shape > 1, as is the case here (6 and 16).\n        with np.errstate(divide='ignore', invalid='ignore'):\n            val = (t**(shape - 1) * np.exp(-t / scale)) / (scale**shape * gamma(shape))\n        return np.nan_to_num(val)\n\n    h = gamma_pdf(t_hrf, a1, b) - c * gamma_pdf(t_hrf, a2, b)\n    \n    # 2. Construct the discrete stimulus vector\n    # High-resolution time vector for the full run\n    run_len = int(T_run / dt) + 1\n    t_hi = np.linspace(0, T_run, run_len)\n    \n    s_hi = np.zeros(run_len)\n    \n    for tau, w in zip(event_times, event_weights):\n        # Find the nearest index in the high-resolution grid\n        k_i = int(np.round(tau / dt))\n        if 0 = k_i  run_len:\n            # Add impulse, scaled to preserve area under Riemann sum\n            s_hi[k_i] += w / dt\n    \n    # 3. Compute the discrete convolution\n    if np.all(s_hi == 0):\n        # Optimization for no-event case\n        x_hi = np.zeros(run_len)\n    else:\n        # Perform convolution and scale by dt to approximate integral\n        x_conv = np.convolve(s_hi, h, mode='full')\n        # Truncate to the length of the run\n        x_hi = x_conv[:run_len] * dt\n\n    # 4. Downsample to acquisition resolution (TR)\n    num_scans = int(T_run / TR) + 1\n    acquisition_times = np.arange(num_scans) * TR\n    \n    # Find corresponding indices in the high-resolution signal\n    sample_indices = np.round(acquisition_times / dt).astype(int)\n    \n    # Ensure indices are within bounds\n    sample_indices = np.minimum(sample_indices, run_len - 1)\n    \n    x_bold = x_hi[sample_indices]\n\n    # 5. Compute summary metrics\n    if x_bold.size == 0 or np.all(x_bold == 0):\n        max_amp = 0.0\n        t_peak = 0.0\n        area = 0.0\n    else:\n        max_amp = np.max(x_bold)\n        k_star = np.argmax(x_bold)\n        t_peak = k_star * TR\n        area = TR * np.sum(x_bold)\n        \n    return [max_amp, t_peak, area]\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (T_run, TR, dt, event_times, event_weights)\n        (60, 2, 0.1, [10, 30, 50], [1, 1, 1]),\n        (48, 1.5, 0.1, [0, 46], [1, 0.5]),\n        (32, 2, 0.1, [], []),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        T_run, TR, dt, taus, weights = case\n        metrics = construct_and_run_glm(T_run, TR, dt, taus, weights)\n        all_results.append(metrics)\n\n    # Format the output string precisely as required\n    result_strings = []\n    for res in all_results:\n        # Format each float to 6 decimal places\n        formatted_res = [f\"{val:.6f}\" for val in res]\n        result_strings.append(f\"[{','.join(formatted_res)}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "当同时分析成千上万个体素时，我们如何才能在不被偶然性欺骗的情况下，自信地宣布一个结果是显著的？本练习将介绍“最大t统计量”（max-t）置换检验，这是一种强大的非参数方法，用于控制全族错误率（FWE），且对数据的假设最少。通过从头开始实施这一程序，您将深刻理解如何对全脑图谱进行严格的统计推断。",
            "id": "4196015",
            "problem": "您将获得代表双组研究设计中统计参数图（SPMs）的体素级数据矩阵。每个矩阵行对应一个受试者，每列对应一个体素。您的任务是使用跨体素的最大t统计量绝对值（max-t方法）实现一个非参数族系误差（FWE）控制程序。程序必须为每个测试用例，通过穷举所有与给定组大小一致的标签分配，构建最大t统计量绝对值的经验置换分布。使用此置换分布，为每个体素分配一个双侧FWE校正p值。\n\n使用的基本原理和假设：\n- 使用经典的双样本设计，其中两组由二元标签定义，并假设在原假设下标签是可交换的。\n- 在每个体素上使用合并方差双样本t统计量来量化组间均值差异。该统计量必须根据当前标签分配得出的两组样本均值和样本方差，在每个体素上独立计算。\n- 通过考虑每个体素上t统计量的绝对值来进行双侧检验。\n- 对于每次置换，计算所有体素中的最大t统计量绝对值；这定义了max-t统计量原分布的一个样本。\n- 使用基于max-t统计量置换分布的包含性计数法则来计算FWE校正p值。\n\n您必须为每个测试用例实现以下步骤：\n1. 为两组构建观察到的标签集，并从数据矩阵中计算观察到的体素级t统计量。\n2. 穷举所有唯一的标签分配，将指定数量的受试者精确地分到组 $1$，其余的则分到组 $2$。对于每种这样的标签分配，计算体素级t统计量，并保留所有体素中的最大t统计量绝对值。这样就得到了max-t统计量的经验置换分布。\n3. 对于每个体素，计算双侧FWE校正p值，其值为置换max-t值中大于或等于该体素观察到的t统计量绝对值的比例，并使用包含性调整，即分子和分母都加 $1$。\n4. 如果在任何标签分配下，任意体素的t统计量计算中分母为 $0$，则将该体素-标签组合对应的t统计量视为 $0$。\n5. 将报告的每个p值四舍五入到恰好六位小数。\n\n测试套件：\n实现您的程序以评估以下三个测试用例。在每个案例中，令 $Y \\in \\mathbb{R}^{n \\times v}$ 表示数据矩阵，其中有 $n$ 个受试者（行）和 $v$ 个体素（列）。观察到的标签由组 $1$ 的索引集给出；其补集构成组 $2$。\n\n- 案例 $1$（通用多体素）：\n  - $n = 8$， $v = 5$。\n  - 观察到的组 $1$ 索引：$\\{0, 1, 2, 3\\}$。\n  - 数据矩阵 $Y$，行按受试者 $0$ 到受试者 $7$ 的顺序列出：\n    - 受试者 $0$: $[0.5, 1.2, 2.5, -0.4, 0.1]$\n    - 受试者 $1$: $[-0.2, 0.9, 2.7, -0.6, -0.3]$\n    - 受试者 $2$: $[0.3, 1.1, 2.9, -0.7, 0.2]$\n    - 受试者 $3$: $[0.1, 1.0, 2.6, -0.5, 0.0]$\n    - 受试者 $4$: $[0.4, 0.8, 1.0, 0.2, -0.1]$\n    - 受试者 $5$: $[-0.1, 1.1, 0.9, 0.3, 0.1]$\n    - 受试者 $6$: $[0.2, 0.7, 1.1, 0.4, 0.0]$\n    - 受试者 $7$: $[0.0, 0.9, 0.8, 0.1, 0.2]$\n\n- 案例 $2$（小样本，可能存在平局）：\n  - $n = 4$， $v = 3$。\n  - 观察到的组 $1$ 索引：$\\{0, 1\\}$。\n  - 数据矩阵 $Y$:\n    - 受试者 $0$: $[1.0, 0.0, -0.2]$\n    - 受试者 $1$: $[1.1, 0.1, -0.1]$\n    - 受试者 $2$: $[0.9, -0.1, -0.2]$\n    - 受试者 $3$: $[1.2, 0.0, -0.1]$\n\n- 案例 $3$（单体素边界情况）：\n  - $n = 6$， $v = 1$。\n  - 观察到的组 $1$ 索引：$\\{0, 1, 2\\}$。\n  - 数据矩阵 $Y$:\n    - 受试者 $0$: $[0.3]$\n    - 受试者 $1$: $[0.4]$\n    - 受试者 $2$: $[0.2]$\n    - 受试者 $3$: $[-0.1]$\n    - 受试者 $4$: $[0.0]$\n    - 受试者 $5$: $[0.1]$\n\n置换策略：\n- 对于每个案例，穷举所有将指定数量的受试者精确分配到组 $1$ 的唯一分配方式（即，所有符合要求大小的索引子集），并将每种分配视为一次不同的置换。\n\nFWE校正p值策略：\n- 对于体素 $j$，令 $t^{\\mathrm{obs}}_j$ 为观察到的t统计量。令 $M$ 表示所有置换中最大t统计量绝对值的多重集。FWE校正p值是 $M$ 中大于或等于 $\\lvert t^{\\mathrm{obs}}_j \\rvert$ 的值的包含性比例，其中分子和分母都加 $1$。\n\n输出规范：\n- 最终程序输出必须是单行，包含三个案例的结果列表（按顺序），每个案例的结果是其体素的FWE校正p值列表（四舍五入到六位小数）。\n- 确切的输出格式是没有任何额外文本和空格的单行：例如，对于三个案例，它应如下所示\n  - $[[p\\_{1,1},p\\_{1,2},\\dots],[p\\_{2,1},p\\_{2,2},\\dots],[p\\_{3,1},\\dots]]$\n  其中每个 $p\\_{i,j}$ 是一个四舍五入到六位小数的小数。\n\n角度和物理单位不适用。不得使用百分比；请以小数形式报告概率。\n\n您的程序不得读取任何输入，并且必须按所提供的方式确定性地运行。它必须仅依赖于标准库和指定的数值库。",
            "solution": "该问题要求实现一个非参数置换检验程序，用于在神经影像学背景下对多个体素进行统计检验时控制族系误差率（FWE）。指定的方法是最大t统计量绝对值（max-t）方法，应用于双组研究设计。解决方案涉及基于经验生成的原分布为每个体素计算FWE校正p值。\n\n该方法的基本原理是原假设（$H_0$）下的可交换性概念。$H_0$假设两个实验组之间没有真实差异。如果这个假设成立，那么将组标签分配给各个受试者就是任意的。因此，这些标签的任何置换都是等可能的。通过系统地重新分配标签，为每次重新分配计算一个检验统计量，并观察这些统计量的分布，我们可以构建一个经验原分布。该分布代表了在$H_0$为真的假设下，纯粹由机遇所期望的结果范围。\n\n为了控制FWE——即在所有体素中犯一个或多个I类错误的概率——我们不能简单地使用来自单个体素的t统计量分布。相反，我们必须将检验族视为一个整体。max-t方法通过构建*最大*统计量的原分布来实现这一点。对于标签的每次置换，我们在每个体素上计算t统计量，并找出其绝对值的最大值。这个单一的值，$T_{\\text{max}}$，是从原分布中抽取的一个样本，代表了在整个数据体中纯粹由机遇可能发现的最极端效应。\n\n循序渐进的步骤如下：\n\n1.  **观察统计量的计算**：首先，我们为原始的、未置换的数据计算检验统计量。对于每个体素 $j$（其中 $j \\in \\{1, \\dots, v\\}$），我们使用合并方差双样本t统计量。给定来自大小为 $n_1$ 的组 $G_1$ 和大小为 $n_2$ 的组 $G_2$ 的数据，体素 $j$ 的t统计量为：\n    $$ t_j = \\frac{\\bar{y}_{1j} - \\bar{y}_{2j}}{s_{p,j} \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} $$\n    其中$\\bar{y}_{1j}$和$\\bar{y}_{2j}$分别是组$G_1$和$G_2$在体素$j$中数据的样本均值。项$s_{p,j}$是体素$j$的合并标准差，由两组的样本方差$s_{1j}^2$和$s_{2j}^2$计算得出：\n    $$ s_{p,j} = \\sqrt{\\frac{(n_1-1)s_{1j}^2 + (n_2-1)s_{2j}^2}{n_1+n_2-2}} $$\n    对于数据为$\\{y_i\\}$且均值为$\\bar{y}$的组，其样本方差为$s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(y_i - \\bar{y})^2$。在t统计量分母为零（即$s_{p,j} = 0$）的特殊情况下，问题要求将t统计量视为$0$。这种情况当且仅当该体素在每个组内的方差都为零时发生。我们为所有$v$个体素计算这些观察统计量$t^{\\text{obs}}_j$。\n\n2.  **最大统计量的置换分布**：我们生成将$n$个受试者划分为大小为$n_1$和$n_2$的两个组的所有唯一分区。这种分区（置换）的总数为$N_{\\text{perm}} = \\binom{n}{n_1}$。对于每次置换：\n    a. 我们使用新的分组为每个体素$j=1, \\dots, v$重新计算t统计量，$t_j^{\\text{perm}}$。\n    b. 我们找到这次置换中所有体素的最大t统计量绝对值：\n       $$ T_{\\text{max}}^{\\text{perm}} = \\max_{j=1,\\dots,v} |t_j^{\\text{perm}}| $$\n    c. 该值被储存起来。所有$N_{\\text{perm}}$个这样的最大值集合构成了经验原分布$M = \\{T_{\\text{max}}^{\\text{perm}_1}, T_{\\text{max}}^{\\text{perm}_2}, \\dots, T_{\\text{max}}^{\\text{perm}_{N_{\\text{perm}}}}\\}$。\n\n3.  **FWE校正p值的计算**：给定体素$j$的FWE校正p值$p_{\\text{FWE}, j}$，是在原分布中观察到至少与该体素的观察统计量$|t^{\\text{obs}}_j|$一样极端的最大统计量的概率。这是从我们的经验分布$M$中估计出来的。我们计算$M$中大于或等于$|t^{\\text{obs}}_j|$的元素数量。令此计数为$k_j$。根据问题对包含性计算的规范，p值由下式给出：\n    $$ p_{\\text{FWE}, j} = \\frac{k_j + 1}{N_{\\text{perm}} + 1} $$\n    这个公式对有限的置换次数进行了调整，并确保p值永远不能为零，这是置换检验的一个理想属性。对每个体素$j=1, \\dots, v$重复此过程。\n\n4.  **最终输出**：每个测试用例得到的p值被四舍五入到六位小数，并按规定格式化。这个程序提供了一种严谨、可解释的统计显著性度量方法，它解决了全脑数据分析中固有的大规模多重比较问题。",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to run the FWE correction procedure on all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"Y\": np.array([\n                [0.5, 1.2, 2.5, -0.4, 0.1],\n                [-0.2, 0.9, 2.7, -0.6, -0.3],\n                [0.3, 1.1, 2.9, -0.7, 0.2],\n                [0.1, 1.0, 2.6, -0.5, 0.0],\n                [0.4, 0.8, 1.0, 0.2, -0.1],\n                [-0.1, 1.1, 0.9, 0.3, 0.1],\n                [0.2, 0.7, 1.1, 0.4, 0.0],\n                [0.0, 0.9, 0.8, 0.1, 0.2]\n            ]),\n            \"group1_indices\": {0, 1, 2, 3}\n        },\n        {\n            \"Y\": np.array([\n                [1.0, 0.0, -0.2],\n                [1.1, 0.1, -0.1],\n                [0.9, -0.1, -0.2],\n                [1.2, 0.0, -0.1]\n            ]),\n            \"group1_indices\": {0, 1}\n        },\n        {\n            \"Y\": np.array([\n                [0.3], [0.4], [0.2], [-0.1], [0.0], [0.1]\n            ]),\n            \"group1_indices\": {0, 1, 2}\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        Y = case[\"Y\"]\n        observed_group1_indices = case[\"group1_indices\"]\n        \n        n, v = Y.shape\n        n1 = len(observed_group1_indices)\n        all_indices = set(range(n))\n        \n        # Step 1: Compute observed t-statistics\n        observed_group2_indices = all_indices - observed_group1_indices\n        observed_t_stats = _compute_t_stats_for_labeling(Y, list(observed_group1_indices), list(observed_group2_indices))\n        abs_observed_t_stats = np.abs(observed_t_stats)\n\n        # Step 2: Enumerate permutations and build max-t distribution\n        max_t_distribution = []\n        all_possible_group1s = combinations(range(n), n1)\n        \n        num_permutations = 0\n        for g1_indices_perm_tuple in all_possible_group1s:\n            num_permutations += 1\n            g1_indices_perm = list(g1_indices_perm_tuple)\n            g2_indices_perm = list(all_indices - set(g1_indices_perm))\n            \n            t_stats_perm = _compute_t_stats_for_labeling(Y, g1_indices_perm, g2_indices_perm)\n            max_abs_t = np.max(np.abs(t_stats_perm))\n            max_t_distribution.append(max_abs_t)\n            \n        max_t_distribution = np.array(max_t_distribution)\n\n        # Step 3: Compute FWE-corrected p-values\n        p_values = []\n        for j in range(v):\n            observed_t_val = abs_observed_t_stats[j]\n            # Inclusive counting\n            k = np.sum(max_t_distribution >= observed_t_val)\n            # Add 1 to numerator and denominator\n            p_val = (k + 1) / (num_permutations + 1)\n            p_values.append(p_val)\n            \n        all_results.append(p_values)\n\n    # Format output string\n    case_strings = []\n    for p_list in all_results:\n        # Step 5: Round each p-value to exactly six decimal places\n        formatted_p_values = [f\"{p:.6f}\" for p in p_list]\n        case_strings.append(f\"[{','.join(formatted_p_values)}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\ndef _compute_t_stats_for_labeling(Y, g1_indices, g2_indices):\n    \"\"\"\n    Computes pooled-variance two-sample t-statistics for all voxels.\n    \"\"\"\n    n, v = Y.shape\n    n1 = len(g1_indices)\n    n2 = len(g2_indices)\n    \n    # This should not happen in this problem's cases, but is good practice\n    if n1  2 or n2  2:\n        return np.zeros(v)\n\n    g1_data = Y[g1_indices, :]\n    g2_data = Y[g2_indices, :]\n\n    mean1 = np.mean(g1_data, axis=0)\n    mean2 = np.mean(g2_data, axis=0)\n    \n    var1 = np.var(g1_data, axis=0, ddof=1)\n    var2 = np.var(g2_data, axis=0, ddof=1)\n    \n    # Pooled standard deviation\n    s_p_squared_num = (n1 - 1) * var1 + (n2 - 1) * var2\n    s_p_squared_den = n1 + n2 - 2\n    \n    # Handle cases where denominator is zero (i.e. all data points are same)\n    if s_p_squared_den == 0:\n        return np.zeros(v)\n\n    pooled_var = s_p_squared_num / s_p_squared_den\n    \n    # Avoid sqrt of negative due to floating point inaccuracies\n    pooled_var[pooled_var  0] = 0\n    pooled_std = np.sqrt(pooled_var)\n\n    # t-statistic denominator\n    t_stat_den_factor = np.sqrt(1/n1 + 1/n2)\n    t_stat_denominator = pooled_std * t_stat_den_factor\n    \n    # Compute t-statistic, handling zero denominator as per problem spec\n    t_stats = np.zeros(v)\n    non_zero_den_mask = t_stat_denominator != 0\n    t_stats[non_zero_den_mask] = (mean1[non_zero_den_mask] - mean2[non_zero_den_mask]) / t_stat_denominator[non_zero_den_mask]\n    \n    return t_stats\n\nsolve()\n```"
        },
        {
            "introduction": "控制全族错误率（FWE）可能过于保守，有时会导致我们错失真实的效应。错误发现率（FDR）控制的是所有声称的“发现”中假阳性的比例，因此提供了一种流行且通常更强大的替代方法。本练习将指导您实现Benjamini-Hochberg程序，让您能够将一组p值转换为经FDR校正的显著性阈值。",
            "id": "4196092",
            "problem": "您正在分析一个由独立的、体素级别的双边 Student $t$ 检验构建的统计参数图 (SPM; Statistical Parametric Map)。对于一组 $m$ 个体素，假设您获得了一个在原假设下有效的经验 $p$ 值列表，以及这些 $t$ 检验的共同自由度 $df$。要求您使用 Benjamini–Hochberg (BH) 程序，在独立性假设下，计算在给定水平 $q \\in (0,1)$ 控制错误发现率 (FDR; False Discovery Rate) 所对应的 $t$ 统计量幅值阈值。\n\n基本定义与约束：\n- FDR 定义为所有拒绝中错误拒绝比例的期望值。Benjamini–Hochberg 程序是一种升阶法，它通过将 $p$ 值按升序排序并选择一个数据驱动的阈值，来在独立 $p$ 值的条件下将 FDR 控制在水平 $q$。\n- 令排序后的 $p$ 值为 $p_{(1)} \\leq p_{(2)} \\leq \\dots \\leq p_{(m)}$。定义集合 $$\\mathcal{K} = \\left\\{ i \\in \\{1,\\dots,m\\} : p_{(i)} \\leq \\frac{i}{m} q \\right\\}$$ 如果 $\\mathcal{K}$ 非空，则 $p$ 值空间中的 BH 拒绝阈值为 $$p_{\\mathrm{BH}} = p_{(k)} \\quad \\text{其中} \\quad k = \\max \\mathcal{K}$$ 否则，在 FDR 水平 $q$ 下不拒绝任何假设。\n- 对于自由度为 $df$ 的双边 $t$ 检验，一个 $t$ 统计量幅值 $|t|$ 对应的双边 $p$ 值为 $$p = 2 \\cdot \\Pr\\left( T_{df} \\ge |t| \\right),$$ 其中 $T_{df}$ 表示一个自由度为 $df$ 的 Student $t$ 随机变量。达到给定双边 $p$ 值 $p$ 所对应的临界 $t$ 阈值是上分位数 $$t_{\\ast} = F^{-1}_{T_{df}} \\left( 1 - \\frac{p}{2} \\right),$$ 其中 $F^{-1}_{T_{df}}$ 是 $T_{df}$ 的逆累积分布函数。等价地，$$t_{\\ast} = \\mathrm{ISF}_{T_{df}} \\left( \\frac{p}{2} \\right),$$ 其中 $\\mathrm{ISF}$ 表示逆生存函数。\n- 如果 $\\mathcal{K}$ 为空（没有 BH 拒绝），您必须返回 $+\\infty$ 的 $t$ 阈值，因为没有有限的 $t$ 值能达到所需的双边 $p$ 值阈值。\n\n任务：\n- 对每个提供的测试用例，根据经验 $p$ 值和 FDR 水平 $q$ 计算 BH $p$ 值阈值 $p_{\\mathrm{BH}}$，然后如上所述，使用自由度 $df$ 将其映射到双边 $t$ 统计量幅值阈值 $t_{\\ast}$。如果没有发生 BH 拒绝，则返回 $+\\infty$。\n\n重要细节：\n- 所有 $p$ 值都是区间 $[0,1]$ 内的实数。\n- FDR 水平 $q$ 以 $(0,1)$ 内的小数形式给出，而不是百分比。\n- 自由度 $df$ 是大于 0 的正实数。\n- 不涉及角度。\n- 输出中没有物理单位。\n\n测试套件：\n为以下五个案例计算 $t$ 统计量幅值阈值。每个案例是一个三元组 $(df, q, \\text{p\\_values})$。\n\n1. $$df = 30,\\quad q = 0.05,\\quad \\text{p\\_values} = [0.0004,\\, 0.0015,\\, 0.006,\\, 0.012,\\, 0.018,\\, 0.021,\\, 0.027,\\, 0.033,\\, 0.041,\\, 0.052,\\, 0.061,\\, 0.074,\\, 0.15,\\, 0.23,\\, 0.31,\\, 0.44,\\, 0.58,\\, 0.71,\\, 0.85,\\, 0.93]$$\n2. $$df = 10,\\quad q = 0.01,\\quad \\text{p\\_values} = [0.12,\\, 0.23,\\, 0.32,\\, 0.41,\\, 0.51,\\, 0.61,\\, 0.7,\\, 0.8,\\, 0.9,\\, 0.99]$$\n3. $$df = 5,\\quad q = 0.1,\\quad \\text{p\\_values} = [0.08]$$\n4. $$df = 120,\\quad q = 0.2,\\quad \\text{p\\_values} = [0.2,\\, 0.18,\\, 0.11,\\, 0.049,\\, 0.051,\\, 0.07,\\, 0.03,\\, 0.025,\\, 0.5,\\, 0.4,\\, 0.33,\\, 0.29,\\, 0.27,\\, 0.23,\\, 0.22,\\, 0.19,\\, 0.17,\\, 0.16,\\, 0.14,\\, 0.13,\\, 0.12,\\, 0.1,\\, 0.08,\\, 0.06,\\, 0.04,\\, 0.035,\\, 0.028,\\, 0.021,\\, 0.015,\\, 0.009,\\, 0.007,\\, 0.004,\\, 0.002]$$\n5. $$df = 40,\\quad q = 0.1,\\quad \\text{p\\_values} = [0.001,\\, 0.005,\\, 0.01,\\, 0.02,\\, 0.031,\\, 0.05,\\, 0.07,\\, 0.09,\\, 0.11,\\, 0.2]$$\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含五个计算出的 $t$ 阈值，格式为方括号括起来的逗号分隔列表（例如，“[x1,x2,x3,x4,x5]”）。每个条目必须是浮点数。如果某个案例没有 BH 拒绝，则相应条目必须是正无穷大的浮点表示，即 \"inf\"。",
            "solution": "该问题是有效的，因为它具有科学依据，定义明确且客观。它提供了一套完整且一致的定义和数据，以解决统计数据分析中一个标准的、非平凡的问题。\n\n任务是计算 Student $t$ 统计量的幅值阈值，该阈值与通过 Benjamini-Hochberg (BH) 程序（用于控制错误发现率 FDR）所确定的显著性水平相对应。该过程可分为两个主要步骤：首先，确定 BH 的 $p$ 值阈值；其次，将此 $p$ 值阈值转换为 $t$ 统计量阈值。\n\n**步骤 1：Benjamini-Hochberg (BH) 程序**\n\nBH 程序提供了一个数据驱动的显著性阈值，在检验之间相互独立的假设下，将 FDR 控制在指定的水平 $q$。\n\n设有一组 $m$ 个独立的 $p$ 值，$\\{p_1, p_2, \\dots, p_m\\}$。\n1.  **对 $p$ 值进行排序**：首先将 $p$ 值按非降序排序，记为 $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$。$p_{(i)}$ 中的索引 $i$ 代表该 $p$ 值的秩次。\n\n2.  **确定阈值**：BH 程序将每个排序后的 $p$ 值 $p_{(i)}$ 与一个临界值进行比较，该临界值取决于其秩次 $i$、总检验数 $m$ 和期望的 FDR 水平 $q$。需要满足的条件是：\n    $$\n    p_{(i)} \\le \\frac{i}{m} q\n    $$\n    我们找出所有满足此不等式的秩次 $i \\in \\{1, \\dots, m\\}$。令这个秩次集合为 $\\mathcal{K}$：\n    $$\n    \\mathcal{K} = \\left\\{ i \\in \\{1, \\dots, m\\} : p_{(i)} \\le \\frac{i}{m} q \\right\\}\n    $$\n\n3.  **找到拒绝阈值**：\n    -   如果集合 $\\mathcal{K}$ 为空（即没有 $p$ 值满足其对应的条件），则不拒绝任何原假设。在这种情况下，问题规定所得的 $t$ 统计量阈值应为正无穷大，$t_{\\ast} = +\\infty$。这是因为拒绝所需的 $p$ 值阈值为 0，这对应于无穷大的检验统计量幅值。\n    -   如果 $\\mathcal{K}$ 非空，我们找到该集合中的最大秩次 $k$, $k = \\max \\mathcal{K}$。BH 发现阈值是此秩次对应的 $p$ 值，$p_{\\mathrm{BH}} = p_{(k)}$。所有对应于小于或等于 $p_{\\mathrm{BH}}$ 的 $p$ 值的假设都被声明为“发现”或被拒绝。\n\n**步骤 2：从 $p$ 值到 $t$ 统计量阈值的转换**\n\n一旦确定了 $p$ 值阈值 $p_{\\mathrm{BH}}$，我们必须将其转换为对应的 $t$ 统计量幅值 $|t|$ 的临界值。问题指明 $p$ 值来自双边 Student $t$ 检验。\n\n1.  **关联 $p$ 值和 $t$ 统计量**：对于给定的 $t$ 统计量幅值 $|t|$ 和自由度 $df$，双边 $p$ 值是观察到至少同样极端的检验统计量的概率。这由 Student $t$ 分布 $T_{df}$ 两尾的面积之和给出：\n    $$\n    p = \\Pr(|T_{df}| \\ge |t|) = 2 \\cdot \\Pr(T_{df} \\ge |t|)\n    $$\n\n2.  **反转关系**：为了找到与我们的 $p$ 值阈值 $p_{\\mathrm{BH}}$ 相对应的 $t$ 统计量阈值 $t_{\\ast}$，我们必须在 $p = p_{\\mathrm{BH}}$ 的条件下解上述方程中的 $|t|$。\n    $$\n    p_{\\mathrm{BH}} = 2 \\cdot \\Pr(T_{df} \\ge t_{\\ast})\n    $$\n    整理以求尾部概率：\n    $$\n    \\Pr(T_{df} \\ge t_{\\ast}) = \\frac{p_{\\mathrm{BH}}}{2}\n    $$\n    这意味着 $t_{\\ast}$ 是这样一个值，它使得 $T_{df}$ 分布右尾的面积恰好为 $p_{\\mathrm{BH}}/2$。\n\n3.  **使用逆生存函数 (ISF)**：值 $t_{\\ast}$ 是通过使用生存函数（即 1 减去累积分布函数 CDF）的逆函数找到的。令 $F_{T_{df}}$ 为自由度为 $df$ 的 Student $t$ 分布的 CDF。则 $\\Pr(T_{df} \\le t_{\\ast}) = F_{T_{df}}(t_{\\ast}) = 1 - p_{\\mathrm{BH}}/2$。阈值 $t_{\\ast}$ 通过应用逆 CDF（也称为百分点函数 PPF）获得：\n    $$\n    t_{\\ast} = F_{T_{df}}^{-1}\\left(1 - \\frac{p_{\\mathrm{BH}}}{2}\\right)\n    $$\n    这等同于使用逆生存函数 (ISF)，如问题中所述：\n    $$\n    t_{\\ast} = \\mathrm{ISF}_{T_{df}}\\left(\\frac{p_{\\mathrm{BH}}}{2}\\right)\n    $$\n    如果没有发现拒绝（即 $p_{\\mathrm{BH}}$ 未定义），则结果为 $t_{\\ast} = +\\infty$。\n\n**示例演练（案例 1）**\n-   给定：$df = 30$, $q = 0.05$，以及一个包含 20 个已排序 $p$ 值的列表。所以，$m = 20$。\n-   BH 条件是 $p_{(i)} \\le \\frac{i}{20} (0.05)$。\n-   对于 $i=1$：$p_{(1)} = 0.0004 \\le \\frac{1}{20}(0.05) = 0.0025$。（真）\n-   对于 $i=2$：$p_{(2)} = 0.0015 \\le \\frac{2}{20}(0.05) = 0.005$。（真）\n-   对于 $i=3$：$p_{(3)} = 0.006 \\le \\frac{3}{20}(0.05) = 0.0075$。（真）\n-   对于 $i=4$：$p_{(4)} = 0.012 \\le \\frac{4}{20}(0.05) = 0.01$。（假）\n-   满足条件的秩次集合是 $\\mathcal{K} = \\{1, 2, 3\\}$。\n-   最大秩次是 $k = \\max \\mathcal{K} = 3$。\n-   $p$ 值阈值是 $p_{\\mathrm{BH}} = p_{(3)} = 0.006$。\n-   对应的 $t$ 统计量阈值是 $t_{\\ast} = \\mathrm{ISF}_{T_{30}}\\left(\\frac{0.006}{2}\\right) = \\mathrm{ISF}_{T_{30}}(0.003) \\approx 2.9467$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Computes the t-statistic magnitude threshold for a series of test cases based on the\n    Benjamini-Hochberg procedure.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (30, 0.05, [0.0004, 0.0015, 0.006, 0.012, 0.018, 0.021, 0.027, 0.033, 0.041, 0.052, 0.061, 0.074, 0.15, 0.23, 0.31, 0.44, 0.58, 0.71, 0.85, 0.93]),\n        (10, 0.01, [0.12, 0.23, 0.32, 0.41, 0.51, 0.61, 0.7, 0.8, 0.9, 0.99]),\n        (5, 0.1, [0.08]),\n        (120, 0.2, [0.2, 0.18, 0.11, 0.049, 0.051, 0.07, 0.03, 0.025, 0.5, 0.4, 0.33, 0.29, 0.27, 0.23, 0.22, 0.19, 0.17, 0.16, 0.14, 0.13, 0.12, 0.1, 0.08, 0.06, 0.04, 0.035, 0.028, 0.021, 0.015, 0.009, 0.007, 0.004, 0.002]),\n        (40, 0.1, [0.001, 0.005, 0.01, 0.02, 0.031, 0.05, 0.07, 0.09, 0.11, 0.2])\n    ]\n\n    results = []\n    for df, q, p_values_list in test_cases:\n        p_values = np.array(p_values_list)\n        m = len(p_values)\n        \n        # Step 1: Sort p-values\n        p_sorted = np.sort(p_values)\n        \n        # Step 2: Apply the Benjamini-Hochberg procedure\n        # Create the BH critical value line: (i/m)*q for i=1,...,m\n        i = np.arange(1, m + 1)\n        bh_critical_values = (i / m) * q\n        \n        # Find all p-values that are less than or equal to their critical value\n        # np.where returns a tuple of arrays, we need the first element\n        significant_indices = np.where(p_sorted = bh_critical_values)[0]\n        \n        if len(significant_indices) == 0:\n            # Case where no hypotheses are rejected\n            t_threshold = np.inf\n        else:\n            # Find the largest index k (0-based) that satisfies the condition\n            k_index = significant_indices.max()\n            \n            # The p-value threshold is the p-value at this largest index\n            p_bh = p_sorted[k_index]\n            \n            # Step 3: Convert the p-value threshold to a t-statistic threshold\n            # For a two-sided test, the corresponding one-sided alpha is p_bh / 2\n            # We use the inverse survival function (ISF), which is equivalent to\n            # the percent-point function (PPF) of (1 - alpha).\n            t_threshold = t.isf(p_bh / 2, df)\n            \n        results.append(t_threshold)\n\n    # Final print statement in the exact required format.\n    # The str() function correctly converts np.inf to \"inf\".\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}