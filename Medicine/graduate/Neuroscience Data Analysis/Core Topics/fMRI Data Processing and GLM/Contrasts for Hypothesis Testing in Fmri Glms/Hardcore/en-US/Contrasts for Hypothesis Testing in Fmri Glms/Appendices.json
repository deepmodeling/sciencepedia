{
    "hands_on_practices": [
        {
            "introduction": "The first step in hypothesis testing is translating a scientific question, such as comparing brain activity between two conditions, into a precise mathematical statement. This practice guides you through this crucial translation by asking you to construct contrast vectors from the first principles of a General Linear Model (GLM). You will also explore the fundamental concept of estimability, verifying geometrically that a valid contrast must lie within the vector space defined by the model's regressors .",
            "id": "4149034",
            "problem": "A single-subject functional Magnetic Resonance Imaging (fMRI) experiment is modeled with a General Linear Model (GLM) having three task conditions $\\mathcal{A}$, $\\mathcal{B}$, and $\\mathcal{C}$. The design matrix $X \\in \\mathbb{R}^{n \\times p}$ uses an intercept and two condition dummy regressors, where the intercept encodes a baseline and the dummies indicate conditions $\\mathcal{B}$ and $\\mathcal{C}$ relative to $\\mathcal{A}$. There are $n=6$ scans, ordered such that scans $1$ and $4$ present condition $\\mathcal{A}$, scans $2$ and $5$ present condition $\\mathcal{B}$, and scans $3$ and $6$ present condition $\\mathcal{C}$. The design matrix rows are\n$$\nx_{\\mathcal{A}}^{\\top} = \\begin{pmatrix} 1  0  0 \\end{pmatrix}, \\quad\nx_{\\mathcal{B}}^{\\top} = \\begin{pmatrix} 1  1  0 \\end{pmatrix}, \\quad\nx_{\\mathcal{C}}^{\\top} = \\begin{pmatrix} 1  0  1 \\end{pmatrix},\n$$\nand $X$ repeats these rows in the order $\\mathcal{A},\\mathcal{B},\\mathcal{C},\\mathcal{A},\\mathcal{B},\\mathcal{C}$. The GLM is $y = X \\beta + \\varepsilon$, with $\\beta \\in \\mathbb{R}^{3}$ and $\\varepsilon$ zero-mean noise with covariance $\\sigma^{2} I$.\n\nStarting from the definitions that (i) the GLM encodes condition-specific predicted means as $x^{\\top}\\beta$ and (ii) a contrast for comparing two condition means is a row vector $c^{\\top}$ such that $c^{\\top}\\beta$ equals the difference of the corresponding predicted means, perform the following:\n\n1. Construct the contrast row vectors $c_{\\mathcal{B}-\\mathcal{A}}^{\\top}$, $c_{\\mathcal{C}-\\mathcal{A}}^{\\top}$, and $c_{\\mathcal{B}-\\mathcal{C}}^{\\top}$ that represent the differences in predicted means between conditions $\\mathcal{B}$ and $\\mathcal{A}$, $\\mathcal{C}$ and $\\mathcal{A}$, and $\\mathcal{B}$ and $\\mathcal{C}$, respectively.\n\n2. Verify from first principles of linear algebra that each constructed contrast lies in the row space of $X$ by explicitly expressing each as a linear combination of rows of $X$.\n\n3. Let $P_{\\mathrm{row}(X)}$ denote the Euclidean orthogonal projector onto the row space of $X$. Define\n$$\nS \\equiv \\left\\| c_{\\mathcal{B}-\\mathcal{A}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2}\n+ \\left\\| c_{\\mathcal{C}-\\mathcal{A}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{C}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2}\n+ \\left\\| c_{\\mathcal{B}-\\mathcal{C}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{C}}^{\\top} \\right\\|_{2}^{2}.\n$$\nCompute the exact value of $S$. No rounding is required. Provide your final answer as a single real number with no units.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of linear models in neuroscience, is well-posed with a unique and meaningful solution, and is stated using objective, formal mathematical language. It contains no scientific flaws, ambiguities, or contradictions.\n\nThe problem requires a three-part analysis of contrasts in a General Linear Model (GLM) for fMRI data. The GLM is given by $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the data, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^{p}$ is the vector of parameters, and $\\varepsilon$ is a noise term. Here, $n=6$ and $p=3$.\n\nThe design of the experiment involves three conditions: $\\mathcal{A}$, $\\mathcal{B}$, and $\\mathcal{C}$. The parameterization uses an intercept representing the baseline activity of condition $\\mathcal{A}$, and two dummy regressors for the additional effects of conditions $\\mathcal{B}$ and $\\mathcal{C}$. The corresponding rows of the design matrix are given as:\n$$\nx_{\\mathcal{A}}^{\\top} = \\begin{pmatrix} 1  0  0 \\end{pmatrix}\n$$\n$$\nx_{\\mathcal{B}}^{\\top} = \\begin{pmatrix} 1  1  0 \\end{pmatrix}\n$$\n$$\nx_{\\mathcal{C}}^{\\top} = \\begin{pmatrix} 1  0  1 \\end{pmatrix}\n$$\nThe full design matrix $X$ consists of these rows repeated in the order $\\mathcal{A},\\mathcal{B},\\mathcal{C},\\mathcal{A},\\mathcal{B},\\mathcal{C}$.\n\nThe parameter vector is $\\beta = \\begin{pmatrix} \\beta_1  \\beta_2  \\beta_3 \\end{pmatrix}^{\\top}$. The predicted mean activity for each condition is given by $x^{\\top}\\beta$.\nFor condition $\\mathcal{A}$, the predicted mean is $\\mu_{\\mathcal{A}} = x_{\\mathcal{A}}^{\\top}\\beta = \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} = \\beta_1$.\nFor condition $\\mathcal{B}$, the predicted mean is $\\mu_{\\mathcal{B}} = x_{\\mathcal{B}}^{\\top}\\beta = \\begin{pmatrix} 1  1  0 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} = \\beta_1 + \\beta_2$.\nFor condition $\\mathcal{C}$, the predicted mean is $\\mu_{\\mathcal{C}} = x_{\\mathcal{C}}^{\\top}\\beta = \\begin{pmatrix} 1  0  1 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} = \\beta_1 + \\beta_3$.\nThis confirms that $\\beta_1$ is the mean of condition $\\mathcal{A}$, $\\beta_2$ is the difference $\\mu_{\\mathcal{B}} - \\mu_{\\mathcal{A}}$, and $\\beta_3$ is the difference $\\mu_{\\mathcal{C}} - \\mu_{\\mathcal{A}}$.\n\n**1. Constructing the contrast vectors**\n\nA contrast is a row vector $c^{\\top}$ such that $c^{\\top}\\beta$ represents a specific linear combination of parameters, typically a comparison between condition means.\n\n- **Contrast for $\\mathcal{B} - \\mathcal{A}$:**\nThe difference in predicted means is $\\mu_{\\mathcal{B}} - \\mu_{\\mathcal{A}} = (\\beta_1 + \\beta_2) - \\beta_1 = \\beta_2$.\nWe need to find $c_{\\mathcal{B}-\\mathcal{A}}^{\\top}$ such that $c_{\\mathcal{B}-\\mathcal{A}}^{\\top}\\beta = \\beta_2$. This is satisfied by:\n$$\nc_{\\mathcal{B}-\\mathcal{A}}^{\\top} = \\begin{pmatrix} 0  1  0 \\end{pmatrix}\n$$\n\n- **Contrast for $\\mathcal{C} - \\mathcal{A}$:**\nThe difference in predicted means is $\\mu_{\\mathcal{C}} - \\mu_{\\mathcal{A}} = (\\beta_1 + \\beta_3) - \\beta_1 = \\beta_3$.\nWe need to find $c_{\\mathcal{C}-\\mathcal{A}}^{\\top}$ such that $c_{\\mathcal{C}-\\mathcal{A}}^{\\top}\\beta = \\beta_3$. This is satisfied by:\n$$\nc_{\\mathcal{C}-\\mathcal{A}}^{\\top} = \\begin{pmatrix} 0  0  1 \\end{pmatrix}\n$$\n\n- **Contrast for $\\mathcal{B} - \\mathcal{C}$:**\nThe difference in predicted means is $\\mu_{\\mathcal{B}} - \\mu_{\\mathcal{C}} = (\\beta_1 + \\beta_2) - (\\beta_1 + \\beta_3) = \\beta_2 - \\beta_3$.\nWe need to find $c_{\\mathcal{B}-\\mathcal{C}}^{\\top}$ such that $c_{\\mathcal{B}-\\mathcal{C}}^{\\top}\\beta = \\beta_2 - \\beta_3$. This is satisfied by:\n$$\nc_{\\mathcal{B}-\\mathcal{C}}^{\\top} = \\begin{pmatrix} 0  1  -1 \\end{pmatrix}\n$$\n\n**2. Verifying the contrasts lie in the row space of $X$**\n\nThe row space of $X$, denoted $\\mathrm{row}(X)$, is the vector space spanned by the rows of $X$. Since the matrix $X$ is formed by repeating the three unique rows $x_{\\mathcal{A}}^{\\top}$, $x_{\\mathcal{B}}^{\\top}$, and $x_{\\mathcal{C}}^{\\top}$, its row space is the span of these three vectors: $\\mathrm{row}(X) = \\mathrm{span}\\{x_{\\mathcal{A}}^{\\top}, x_{\\mathcal{B}}^{\\top}, x_{\\mathcal{C}}^{\\top}\\}$.\nA vector lies in this space if it can be written as a linear combination of these basis vectors.\n\n- For $c_{\\mathcal{B}-\\mathcal{A}}^{\\top} = \\begin{pmatrix} 0  1  0 \\end{pmatrix}$:\nWe observe that $x_{\\mathcal{B}}^{\\top} - x_{\\mathcal{A}}^{\\top} = \\begin{pmatrix} 1  1  0 \\end{pmatrix} - \\begin{pmatrix} 1  0  0 \\end{pmatrix} = \\begin{pmatrix} 0  1  0 \\end{pmatrix}$.\nThus, $c_{\\mathcal{B}-\\mathcal{A}}^{\\top} = (1)x_{\\mathcal{B}}^{\\top} + (-1)x_{\\mathcal{A}}^{\\top}$, which is a linear combination of rows of $X$. Therefore, $c_{\\mathcal{B}-\\mathcal{A}}^{\\top} \\in \\mathrm{row}(X)$.\n\n- For $c_{\\mathcal{C}-\\mathcal{A}}^{\\top} = \\begin{pmatrix} 0  0  1 \\end{pmatrix}$:\nWe observe that $x_{\\mathcal{C}}^{\\top} - x_{\\mathcal{A}}^{\\top} = \\begin{pmatrix} 1  0  1 \\end{pmatrix} - \\begin{pmatrix} 1  0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0  1 \\end{pmatrix}$.\nThus, $c_{\\mathcal{C}-\\mathcal{A}}^{\\top} = (1)x_{\\mathcal{C}}^{\\top} + (-1)x_{\\mathcal{A}}^{\\top}$, which is a linear combination of rows of $X$. Therefore, $c_{\\mathcal{C}-\\mathcal{A}}^{\\top} \\in \\mathrm{row}(X)$.\n\n- For $c_{\\mathcal{B}-\\mathcal{C}}^{\\top} = \\begin{pmatrix} 0  1  -1 \\end{pmatrix}$:\nWe observe that $x_{\\mathcal{B}}^{\\top} - x_{\\mathcal{C}}^{\\top} = \\begin{pmatrix} 1  1  0 \\end{pmatrix} - \\begin{pmatrix} 1  0  1 \\end{pmatrix} = \\begin{pmatrix} 0  1  -1 \\end{pmatrix}$.\nThus, $c_{\\mathcal{B}-\\mathcal{C}}^{\\top} = (1)x_{\\mathcal{B}}^{\\top} + (-1)x_{\\mathcal{C}}^{\\top}$, which is a linear combination of rows of $X$. Therefore, $c_{\\mathcal{B}-\\mathcal{C}}^{\\top} \\in \\mathrm{row}(X)$.\n\nThis explicitly verifies that all three contrast vectors lie in the row space of the design matrix $X$. This is a general requirement for a contrast $c^{\\top}\\beta$ to be estimable.\n\n**3. Computing the value of $S$**\n\nThe quantity to compute is:\n$$\nS \\equiv \\left\\| c_{\\mathcal{B}-\\mathcal{A}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2}\n+ \\left\\| c_{\\mathcal{C}-\\mathcal{A}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{C}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2}\n+ \\left\\| c_{\\mathcal{B}-\\mathcal{C}}^{\\top} - P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{C}}^{\\top} \\right\\|_{2}^{2}\n$$\nwhere $P_{\\mathrm{row}(X)}$ is the Euclidean orthogonal projector onto the subspace $\\mathrm{row}(X)$.\n\nA fundamental property of an orthogonal projector $P_V$ onto a vector subspace $V$ is that for any vector $v \\in V$, the projection of $v$ onto $V$ is the vector $v$ itself. That is, if $v \\in V$, then $P_V v = v$.\n\nIn step 2, we demonstrated that each of the contrast vectors, $c_{\\mathcal{B}-\\mathcal{A}}^{\\top}$, $c_{\\mathcal{C}-\\mathcal{A}}^{\\top}$, and $c_{\\mathcal{B}-\\mathcal{C}}^{\\top}$, is an element of the row space of $X$.\nApplying the property of the orthogonal projector:\n\n- For $c_{\\mathcal{B}-\\mathcal{A}}^{\\top}$: Since $c_{\\mathcal{B}-\\mathcal{A}}^{\\top} \\in \\mathrm{row}(X)$, we have $P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{A}}^{\\top} = c_{\\mathcal{B}-\\mathcal{A}}^{\\top}$.\nThe first term in the sum for $S$ is $\\left\\| c_{\\mathcal{B}-\\mathcal{A}}^{\\top} - c_{\\mathcal{B}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2} = \\left\\| \\mathbf{0}^{\\top} \\right\\|_{2}^{2} = 0$.\n\n- For $c_{\\mathcal{C}-\\mathcal{A}}^{\\top}$: Since $c_{\\mathcal{C}-\\mathcal{A}}^{\\top} \\in \\mathrm{row}(X)$, we have $P_{\\mathrm{row}(X)} c_{\\mathcal{C}-\\mathcal{A}}^{\\top} = c_{\\mathcal{C}-\\mathcal{A}}^{\\top}$.\nThe second term is $\\left\\| c_{\\mathcal{C}-\\mathcal{A}}^{\\top} - c_{\\mathcal{C}-\\mathcal{A}}^{\\top} \\right\\|_{2}^{2} = \\left\\| \\mathbf{0}^{\\top} \\right\\|_{2}^{2} = 0$.\n\n- For $c_{\\mathcal{B}-\\mathcal{C}}^{\\top}$: Since $c_{\\mathcal{B}-\\mathcal{C}}^{\\top} \\in \\mathrm{row}(X)$, we have $P_{\\mathrm{row}(X)} c_{\\mathcal{B}-\\mathcal{C}}^{\\top} = c_{\\mathcal{B}-\\mathcal{C}}^{\\top}$.\nThe third term is $\\left\\| c_{\\mathcal{B}-\\mathcal{C}}^{\\top} - c_{\\mathcal{B}-\\mathcal{C}}^{\\top} \\right\\|_{2}^{2} = \\left\\| \\mathbf{0}^{\\top} \\right\\|_{2}^{2} = 0$.\n\nCombining these results, the value of $S$ is the sum of these three terms:\n$$\nS = 0 + 0 + 0 = 0\n$$\n\nThe expression $\\| v - P_V v \\|_2$ represents the distance from a vector $v$ to the subspace $V$. The calculation confirms that this distance is zero for all three contrast vectors, as they are elements of the subspace $\\mathrm{row}(X)$ by construction.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "While theoretical understanding of estimability is crucial, practical fMRI analysis requires robust computational tools for quality control, especially when dealing with complex or potentially rank-deficient design matrices. This exercise moves from theory to application, challenging you to implement a numerical procedure using QR decomposition to programmatically verify if a contrast is estimable . Mastering this skill allows you to build reliable analysis pipelines that can automatically guard against ill-posed hypothesis tests.",
            "id": "4149008",
            "problem": "You are analyzing contrasts for hypothesis testing in Functional Magnetic Resonance Imaging (fMRI) under the General Linear Model (GLM). The GLM assumes a time series model of the form $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^n$ is the observed data, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix encoding regressors (such as stimulus conditions and nuisance signals), $\\beta \\in \\mathbb{R}^p$ are regression coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is noise with zero mean. A contrast is a linear functional $c^\\top \\beta$ for some $c \\in \\mathbb{R}^p$. A contrast $c^\\top \\beta$ is said to be estimable if and only if $c$ lies in the row space of $X$.\n\nStarting from first principles of linear models and linear algebra, implement a robust numerical decision procedure to verify contrast estimability by checking whether $c$ lies in the row space of $X$. Your program must use the orthogonal-triangular (QR) decomposition numerically to construct an orthonormal basis for the row space of $X$ and decide membership within a user-specified numerical tolerance.\n\nUse the following test suite. For each test case, you are given a matrix $X \\in \\mathbb{R}^{n \\times p}$, a contrast vector $c \\in \\mathbb{R}^p$, and a projection tolerance $\\tau  0$. Your program must return a boolean indicating whether $c$ is estimable given $X$ (that is, whether the distance of $c$ to the row space of $X$ is at most $\\tau$). All matrices and vectors are dimensionally consistent and contain unitless entries. The program must use QR decomposition with column pivoting on $X^\\top$ to obtain an orthonormal basis for the row space of $X$.\n\nTest suite:\n- Case $1$ (full column rank design): \n  $$\n  X_1 =\n  \\begin{bmatrix}\n  1  0  0 \\\\\n  1  1  0 \\\\\n  1  1  1 \\\\\n  1  0  1 \\\\\n  1  0  0 \\\\\n  1  1  0\n  \\end{bmatrix},\n  \\quad\n  c_1 = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix},\n  \\quad\n  \\tau_1 = 10^{-12}.\n  $$\n- Case $2$ (rank deficiency via duplicated regressor, non-estimable contrast):\n  $$\n  X_2 =\n  \\begin{bmatrix}\n  1  0  0 \\\\\n  1  1  1 \\\\\n  1  1  1 \\\\\n  1  0  0 \\\\\n  1  1  1\n  \\end{bmatrix},\n  \\quad\n  c_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix},\n  \\quad\n  \\tau_2 = 10^{-12}.\n  $$\n- Case $3$ (rank deficiency via duplicated regressor, estimable contrast aligned with the row space symmetry):\n  $$\n  X_3 =\n  X_2 \\text{ (as above)},\n  \\quad\n  c_3 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix},\n  \\quad\n  \\tau_3 = 10^{-12}.\n  $$\n- Case $4$ (near collinearity, numerically full rank with tiny perturbation):\n  let $\\delta = 10^{-10}$, and\n  $$\n  X_4 =\n  \\begin{bmatrix}\n  1  0  \\delta \\\\\n  1  1  1 \\\\\n  1  1  1 \\\\\n  1  0  0\n  \\end{bmatrix},\n  \\quad\n  c_4 = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix},\n  \\quad\n  \\tau_4 = 10^{-8}.\n  $$\n- Case $5$ (zero regressor column implies non-estimable contrast on that coefficient):\n  $$\n  X_5 =\n  \\begin{bmatrix}\n  1  0  0 \\\\\n  1  0  1 \\\\\n  1  0  0 \\\\\n  1  0  1\n  \\end{bmatrix},\n  \\quad\n  c_5 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\n  \\quad\n  \\tau_5 = 10^{-12}.\n  $$\n\nImplementation requirements:\n- Use QR decomposition with column pivoting on $X^\\top$ to obtain $Q$ and $R$ factors and a permutation. Determine a numerical rank $r$ by thresholding the absolute diagonal entries of $R$ against a rank tolerance computed internally by your program (you may base this on machine precision and the scale of $R$).\n- Form an orthonormal basis $Q_r$ from the first $r$ columns of $Q$ to represent the row space of $X$. Decide estimability by checking whether the Euclidean norm of $(I - Q_r Q_r^\\top) c$ is at most $\\tau$.\n- For each of the five cases above, compute a boolean indicating whether the provided $c_i$ is estimable from $X_i$ at tolerance $\\tau_i$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,True,False]\") in the order of the cases $1$ through $5$.",
            "solution": "The problem requires the implementation of a robust numerical procedure to determine if a contrast, defined by a vector $c \\in \\mathbb{R}^p$, is estimable within the framework of a General Linear Model (GLM) of the form $y = X\\beta + \\varepsilon$. The condition for estimability is that the contrast vector $c$ must lie in the row space of the design matrix $X \\in \\mathbb{R}^{n \\times p}$.\n\n**Theoretical Foundation**\nA contrast $c^\\top\\beta$ is estimable if and only if the contrast vector $c$ is a member of the row space of the design matrix $X$. The row space of $X$, denoted $\\text{row}(X)$, is the subspace of $\\mathbb{R}^p$ spanned by the row vectors of $X$. An equivalent and more computationally useful perspective is that the row space of $X$ is identical to the column space of its transpose, $X^\\top$. That is, $\\text{row}(X) = \\text{col}(X^\\top)$.\n\nThe problem of checking if $c \\in \\text{row}(X)$ is therefore transformed into checking if $c \\in \\text{col}(X^\\top)$. A vector belongs to a vector subspace if and only if its orthogonal projection onto that subspace is the vector itself. Numerically, this means the distance from the vector to the subspace should be close to zero. The most robust way to verify this is to first construct an orthonormal basis for the subspace $\\text{col}(X^\\top)$. Once such a basis is found, we project the vector $c$ onto the subspace and measure the length (Euclidean norm) of the residual vector. If this norm is smaller than a given tolerance $\\tau$, we conclude that $c$ is numerically within the subspace.\n\n**Numerical Method: Rank-Revealing QR Decomposition**\nTo construct an orthonormal basis for $\\text{col}(X^\\top)$, we employ the QR decomposition with column pivoting. This is a rank-revealing factorization that is numerically stable, especially when $X$ (and thus $X^\\top$) is rank-deficient or ill-conditioned.\n\nWe apply the decomposition to the matrix $A = X^\\top \\in \\mathbb{R}^{p \\times n}$. The decomposition yields $A P = Q R$, which can be rewritten as $X^\\top P = QR$. In this equation:\n- $P$ is a permutation matrix that reorders the columns of $A$ such that the diagonal elements of $R$ are non-increasing in magnitude, i.e., $|R_{00}| \\ge |R_{11}| \\ge \\dots$.\n- $Q \\in \\mathbb{R}^{p \\times p}$ is an orthogonal matrix, meaning its columns form an orthonormal basis for $\\mathbb{R}^p$.\n- $R \\in \\mathbb{R}^{p \\times n}$ is an upper trapezoidal matrix.\n\nThe rank $r$ of the matrix $X$ (which equals the rank of $X^\\top$) is the dimension of its row space. Due to floating-point arithmetic, we must determine a numerical rank. By inspecting the diagonal elements of $R$, we can find the number of linearly independent columns in the permuted matrix $A P$. The numerical rank $r$ is the count of diagonal entries $|R_{ii}|$ that are significantly greater than zero. A standard threshold for this is a tolerance, $\\text{tol}$, typically set relative to the matrix dimensions, machine precision $\\epsilon$, and the scale of the matrix: $\\text{tol} = \\max(p, n) \\cdot \\epsilon \\cdot |R_{00}|$.\n\nThe first $r$ columns of the matrix $Q$, which we denote as $Q_r \\in \\mathbb{R}^{p \\times r}$, form an orthonormal basis for the column space of $X^\\top$, and therefore for the row space of $X$.\n\n**Projection and Verification**\nWith the orthonormal basis $Q_r$, the projection of $c$ onto $\\text{row}(X)$ is given by $c_{\\text{proj}} = Q_r Q_r^\\top c$. The vector component of $c$ that is orthogonal to the row space is the residual $c_{\\perp} = c - c_{\\text{proj}} = (I - Q_r Q_r^\\top)c$. The estimability condition is satisfied if this residual is negligible, i.e., if its Euclidean norm is within the specified tolerance $\\tau$:\n$$ \\| c_{\\perp} \\|_2 = \\|(I - Q_r Q_r^\\top)c\\|_2 \\le \\tau $$\n\nA computationally more efficient and stable method to calculate this norm leverages the properties of the full orthogonal matrix $Q$. Since $Q$ is orthogonal, it preserves norms under transformation ($||Q^\\top v||_2 = ||v||_2$). Let $\\hat{c} = Q^\\top c$ be the representation of $c$ in the basis defined by the columns of $Q$. The vector $c$ can be expressed as a linear combination of the basis vectors: $c = Q\\hat{c}$. The projection $c_{\\text{proj}}$ corresponds to the part of this sum using the first $r$ basis vectors (the columns of $Q_r$), while the residual $c_{\\perp}$ corresponds to the part using the remaining $p-r$ basis vectors. Because these basis vectors are orthonormal, the norm of the residual is simply the norm of the corresponding coefficients in $\\hat{c}$:\n$$ \\|c_{\\perp}\\|_2 = \\left\\| \\hat{c}_{r:p} \\right\\|_2 = \\sqrt{\\sum_{i=r}^{p-1} \\hat{c}_i^2} $$\nwhere $\\hat{c}_{r:p}$ denotes the components of $\\hat{c}$ from index $r$ to the end. This approach avoids forming projection matrices explicitly and is numerically preferred.\n\n**Implementation for Test Cases**\nFor each given test case $(X_i, c_i, \\tau_i)$, the algorithm proceeds as follows:\n1. Form the matrix $A = X_i^\\top$.\n2. Compute the QR decomposition with column pivoting: $A P = Q R$.\n3. Determine the numerical rank $r$ by thresholding the diagonal of $R$ against an internally computed tolerance.\n4. Transform the contrast vector into the new basis: $\\hat{c}_i = Q^\\top c_i$.\n5. Calculate the Euclidean norm of the components of $\\hat{c}_i$ from index $r$ onwards. This is the distance from $c_i$ to the row space of $X_i$.\n6. Compare this norm to the tolerance $\\tau_i$. If the norm is less than or equal to $\\tau_i$, the contrast is estimable (True); otherwise, it is not (False).\n\nThis procedure is applied to all five test cases to generate the final list of boolean results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr\n\ndef is_estimable(X, c, tau):\n    \"\"\"\n    Checks if a contrast vector c is in the row space of a design matrix X\n    using QR decomposition with column pivoting.\n\n    Args:\n        X (np.ndarray): The design matrix of shape (n, p).\n        c (np.ndarray): The contrast vector of shape (p,).\n        tau (float): The numerical tolerance for the distance to the subspace.\n\n    Returns:\n        bool: True if c is estimable within the given tolerance, False otherwise.\n    \"\"\"\n    # Number of regressors (columns of X)\n    p = X.shape[1]\n    if p == 0:\n        # If there are no regressors, the row space is {0} in R^0.\n        # The only valid contrast vector is an empty one.\n        return np.linalg.norm(c) = tau\n\n    # The problem of checking if c is in row(X) is equivalent to\n    # checking if c is in the column space of X.T.\n    A = X.T\n    p, n = A.shape  # p = num_regressors, n = num_timepoints\n\n    # Perform QR decomposition with column pivoting on X.T.\n    # We use mode='full' to ensure Q is a square (p x p) orthogonal matrix.\n    # The decomposition is A[:, P] = Q @ R, where P is the permutation vector.\n    Q, R, _ = qr(A, pivoting=True, mode='full')\n\n    # Determine the numerical rank 'r' of the matrix A.\n    # This is done by thresholding the absolute diagonal entries of R.\n    if R.shape[0] == 0 or R.shape[1] == 0:\n        rank = 0\n    else:\n        # A standard tolerance for rank determination is based on the matrix size,\n        # machine epsilon, and the scale of the matrix (given by abs(R[0, 0])).\n        eps = np.finfo(R.dtype).eps\n        rank_tol = max(p, n) * eps * abs(R[0, 0])\n        diag_R_abs = np.abs(np.diag(R))\n        rank = np.sum(diag_R_abs  rank_tol)\n\n    # Transform the contrast vector c into the basis defined by the columns of Q.\n    c_transformed = Q.T @ c\n\n    # The distance from c to the column space of A (i.e., row space of X) is\n    # the Euclidean norm of the components of c_transformed that correspond to\n    # the basis vectors of the orthogonal complement. These are the components\n    # of the transformed vector from index 'rank' onwards.\n    distance_to_subspace = np.linalg.norm(c_transformed[rank:])\n\n    # The contrast is estimable if this distance is within the specified tolerance.\n    return distance_to_subspace = tau\n\n\ndef solve():\n    \"\"\"\n    Defines the test suite from the problem statement and computes the results.\n    \"\"\"\n    # Case 1: Full column rank design\n    X1 = np.array([\n        [1., 0., 0.], [1., 1., 0.], [1., 1., 1.],\n        [1., 0., 1.], [1., 0., 0.], [1., 1., 0.]\n    ])\n    c1 = np.array([0., 1., -1.])\n    tau1 = 1e-12\n\n    # Case 2: Rank deficiency via duplicated regressor, non-estimable contrast\n    X2 = np.array([\n        [1., 0., 0.], [1., 1., 1.], [1., 1., 1.],\n        [1., 0., 0.], [1., 1., 1.]\n    ])\n    c2 = np.array([0., 1., -1.])\n    tau2 = 1e-12\n\n    # Case 3: Rank deficiency via duplicated regressor, estimable contrast\n    X3 = X2\n    c3 = np.array([0., 1., 1.])\n    tau3 = 1e-12\n\n    # Case 4: Near collinearity, numerically full rank with tiny perturbation\n    delta = 1e-10\n    X4 = np.array([\n        [1., 0., delta], [1., 1., 1.],\n        [1., 1., 1.], [1., 0., 0.]\n    ])\n    c4 = np.array([0., 1., -1.])\n    tau4 = 1e-8\n\n    # Case 5: Zero regressor column implies non-estimable contrast\n    X5 = np.array([\n        [1., 0., 0.], [1., 0., 1.],\n        [1., 0., 0.], [1., 0., 1.]\n    ])\n    c5 = np.array([0., 1., 0.])\n    tau5 = 1e-12\n\n    test_cases = [\n        (X1, c1, tau1),\n        (X2, c2, tau2),\n        (X3, c3, tau3),\n        (X4, c4, tau4),\n        (X5, c5, tau5),\n    ]\n\n    results = []\n    for X, c, tau in test_cases:\n        estimable = is_estimable(X, c, tau)\n        results.append(estimable)\n\n    # Format the final output string as a comma-separated list in brackets.\n    # The map(str, ...) function automatically converts Python's True/False\n    # to the required \"True\"/\"False\" string representation.\n    result_str = f\"[{','.join(map(str, results))}]\"\n    print(result_str)\n\n\nsolve()\n\n```"
        },
        {
            "introduction": "With the ability to correctly define and validate contrasts, how can we be sure our entire statistical inference machinery—from parameter estimation to the final $p$-value—is working correctly? This capstone practice introduces a powerful method for verification: simulation. By generating synthetic data where the 'ground truth' is known, you will cross-validate the behavior of $t$- and $F$-tests, confirming their core properties such as effect detection, null performance, and invariance under transformation . This approach builds deep intuition and confidence in the statistical tools used daily in neuroscience research.",
            "id": "4148954",
            "problem": "Construct a complete, runnable program that implements a principled cross-validation of contrast definitions for hypothesis testing in Functional Magnetic Resonance Imaging (fMRI) General Linear Models (GLMs). The program must start from first principles of the General Linear Model and standard statistical inference assumptions, without relying on any pre-supplied formula snippets. The fundamental base for this task is: the General Linear Model representation of measured data as a linear combination of regressors plus additive noise, Ordinary Least Squares estimation under full rank design, and hypotheses expressed as linear contrasts of model parameters.\n\nThe General Linear Model represents a time series as $y = X \\beta + \\varepsilon$, where $y$ is the observed data vector of length $n$, $X$ is the design matrix of size $n \\times p$ with full column rank $p$, $\\beta$ is the parameter vector of length $p$, and $\\varepsilon$ is zero-mean Gaussian noise with variance $\\sigma^2$ and independent and identically distributed samples. A contrast is defined either as a vector $c \\in \\mathbb{R}^p$ to test a single linear combination of parameters or as a matrix $R \\in \\mathbb{R}^{q \\times p}$ to test $q$-dimensional linear constraints. The program must generate synthetic data under known ground-truth parameter vectors, compute Ordinary Least Squares estimates, form $t$-statistics for given vector contrasts and $F$-statistics for given matrix contrasts, and verify recovery and invariance properties of these statistics.\n\nYour program must:\n- Generate synthetic design matrices $X$ with orthonormal columns via a deterministic procedure using specified pseudorandom seeds, ensuring $X^\\top X$ equals the identity matrix of size $p$, and synthetic observed data $y = X \\beta_{\\text{true}} + \\varepsilon$ where $\\varepsilon$ is sampled from a normal distribution with specified standard deviation.\n- Compute the Ordinary Least Squares estimate $\\hat{\\beta}$, the residual variance estimate, and then form $t$-statistics for given vector contrasts and $F$-statistics for given matrix contrasts. Compute two-sided $t$-test $p$-values and upper-tail (survival function) $F$-test $p$-values using the appropriate degrees of freedom.\n- Cross-validate contrast definitions by checking the following properties on synthetic data with known effects:\n  1. When an effect is present in the regressor targeted by a contrast vector $c$, the recovered $t$-statistic should indicate a significant effect in the correct direction.\n  2. When the effect is absent, the $t$-test should not indicate significance.\n  3. The $t$-statistic is invariant under scaling of $c$ by a nonzero scalar.\n  4. The $F$-statistic for a one-row matrix contrast equals the squared $t$-statistic for the equivalent vector contrast.\n  5. The $F$-statistic is invariant under left multiplication of the contrast matrix $R$ by any invertible matrix that preserves the tested subspace.\n  6. For a multi-row contrast selecting parameters with no true effect, the $F$-test should not indicate significance.\n\nTest Suite Specification:\nUse the following six test cases. All random number generation must be seeded exactly as specified to ensure deterministic results.\n\n- Test case $1$ (effect present, vector contrast detection):\n  - $n = 240$, $p = 5$.\n  - Design seed for $X$: $123$; noise seed: $456$.\n  - Noise standard deviation: $\\sigma = 0.2$.\n  - Ground-truth parameter vector: $\\beta_{\\text{true}} = [2.0, 0.0, 0.0, 0.0, 0.0]$.\n  - Contrast vector: $c = [1, 0, 0, 0, 0]$.\n  - Validation: the recovered two-sided $t$-test $p$-value must be strictly less than $1 \\times 10^{-8}$ and the $t$-statistic must be strictly positive.\n\n- Test case $2$ (effect absent, vector contrast non-significance):\n  - $n = 240$, $p = 5$.\n  - Design seed for $X$: $123$; noise seed: $789$.\n  - Noise standard deviation: $\\sigma = 0.5$.\n  - Ground-truth parameter vector: $\\beta_{\\text{true}} = [0.0, 0.0, 0.0, 0.0, 0.0]$.\n  - Contrast vector: $c = [1, 0, 0, 0, 0]$.\n  - Validation: the recovered two-sided $t$-test $p$-value must be strictly greater than $0.1$.\n\n- Test case $3$ (scaling invariance of $t$):\n  - Reuse the dataset generation specification of Test case $1$.\n  - Contrast vectors: $c_1 = [1, 0, 0, 0, 0]$ and $c_2 = 7.0 \\times c_1$.\n  - Validation: the absolute difference of recovered $t$-statistics for $c_1$ and $c_2$ must be strictly less than $1 \\times 10^{-12}$.\n\n- Test case $4$ ($F$ equals squared $t$ for $q = 1$):\n  - Reuse the dataset generation specification of Test case $1$.\n  - Contrast vector: $c = [1, 0, 0, 0, 0]$; matrix contrast: $R$ is the row vector equal to $c$.\n  - Validation: the absolute difference between the recovered $F$-statistic and the square of the recovered $t$-statistic must be strictly less than $1 \\times 10^{-10}$.\n\n- Test case $5$ (invariance of $F$ under invertible left transformations of $R$):\n  - $n = 300$, $p = 5$.\n  - Design seed for $X$: $321$; noise seed: $654$.\n  - Noise standard deviation: $\\sigma = 0.3$.\n  - Ground-truth parameter vector: $\\beta_{\\text{true}} = [1.5, -1.0, 0.0, 0.0, 0.0]$.\n  - Matrix contrast: $R = \\begin{bmatrix}1  0  0  0  0 \\\\ 0  1  0  0  0\\end{bmatrix}$.\n  - Invertible left transformation: $A = \\begin{bmatrix}2.0  0.5 \\\\ 0.1  1.0\\end{bmatrix}$; transformed contrast $R' = A R$.\n  - Validation: the absolute difference of recovered $F$-statistics for $R$ and $R'$ must be strictly less than $1 \\times 10^{-10}$.\n\n- Test case $6$ (multi-row contrast non-significance under null effects):\n  - $n = 300$, $p = 5$.\n  - Design seed for $X$: $321$; noise seed: $987$.\n  - Noise standard deviation: $\\sigma = 0.4$.\n  - Ground-truth parameter vector: $\\beta_{\\text{true}} = [0.0, 0.0, 0.0, 0.0, 0.0]$.\n  - Matrix contrast: $R = \\begin{bmatrix}1  0  0  0  0 \\\\ 0  1  0  0  0\\end{bmatrix}$.\n  - Validation: the recovered upper-tail $F$-test $p$-value must be strictly greater than $0.1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly six boolean values corresponding to the validations of Test cases $1$ through $6$ in order, for example, $[{\\text{result}_1},{\\text{result}_2},{\\text{result}_3},{\\text{result}_4},{\\text{result}_5},{\\text{result}_6}]$.",
            "solution": "The problem requires the implementation and validation of statistical hypothesis testing within the framework of the General Linear Model (GLM), a cornerstone of statistical analysis in fields such as fMRI. We will proceed by first establishing the theoretical foundations of the GLM, Ordinary Least Squares (OLS) estimation, and contrast-based hypothesis testing for both vector and matrix contrasts. Subsequently, we will implement these principles in a program and verify its correctness against a series of specified test cases that probe fundamental properties of the statistical methods.\n\nThe General Linear Model posits a linear relationship between observed data and a set of explanatory variables or regressors. It is expressed as:\n$$\ny = X \\beta + \\varepsilon\n$$\nwhere $y$ is an $n \\times 1$ vector of observed data points (e.g., a voxel's time series), $X$ is the $n \\times p$ design matrix where each column is a regressor, $\\beta$ is a $p \\times 1$ vector of unknown parameters to be estimated, and $\\varepsilon$ is an $n \\times 1$ vector of random error terms. We assume the errors are independent and identically distributed (i.i.d.) following a Gaussian distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon \\sim N(0, \\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix.\n\nThe parameters $\\beta$ are estimated using Ordinary Least Squares (OLS), which seeks to find the parameter vector $\\hat{\\beta}$ that minimizes the sum of squared residuals, $SSR = \\|y - X\\beta\\|^2_2 = (y - X\\beta)^\\top(y - X\\beta)$. To find the minimum, we differentiate $SSR$ with respect to $\\beta$ and set the gradient to zero:\n$$\n\\frac{\\partial (SSR)}{\\partial \\beta} = -2X^\\top(y - X\\beta) = 0\n$$\nThis leads to the normal equations:\n$$\n(X^\\top X) \\hat{\\beta} = X^\\top y\n$$\nAssuming the design matrix $X$ has full column rank $p$, the matrix $X^\\top X$ is invertible. The OLS estimator for $\\beta$ is then given by:\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n$$\nThe problem specifies that the design matrix $X$ must have orthonormal columns, which means $X^\\top X = I_p$, the $p \\times p$ identity matrix. This greatly simplifies the OLS estimator to:\n$$\n\\hat{\\beta} = X^\\top y\n$$\n\nOnce $\\hat{\\beta}$ is computed, we can find the fitted values $\\hat{y} = X\\hat{\\beta}$ and the residuals $e = y - \\hat{y}$. An unbiased estimator for the error variance $\\sigma^2$ is the residual variance $\\hat{\\sigma}^2$, calculated as the sum of squared residuals divided by the residual degrees of freedom, $df = n-p$:\n$$\n\\hat{\\sigma}^2 = \\frac{e^\\top e}{n-p} = \\frac{\\|y - X\\hat{\\beta}\\|^2}{n-p}\n$$\n\nHypothesis testing in the GLM is performed using contrasts, which are linear combinations of the estimated parameters.\n\nFor a vector contrast, we test a hypothesis about a single linear combination of parameters, specified by a contrast vector $c \\in \\mathbb{R}^p$. The null hypothesis is $H_0: c^\\top \\beta = 0$. The test statistic is the $t$-statistic, calculated as the estimated effect divided by its standard error:\n$$\nt = \\frac{c^\\top \\hat{\\beta}}{\\text{SE}(c^\\top \\hat{\\beta})} = \\frac{c^\\top \\hat{\\beta}}{\\sqrt{\\hat{\\sigma}^2 c^\\top (X^\\top X)^{-1} c}}\n$$\nUnder the orthonormal design assumption ($X^\\top X = I_p$), this simplifies to:\n$$\nt = \\frac{c^\\top \\hat{\\beta}}{\\hat{\\sigma} \\sqrt{c^\\top c}}\n$$\nUnder the null hypothesis $H_0$, this statistic follows a Student's $t$-distribution with $df = n-p$ degrees of freedom. The two-sided $p$-value is computed as $2 \\times P(T_{df} > |t_{obs}|)$, where $T_{df}$ is a random variable following the $t$-distribution with $df$ degrees of freedom.\n\nFor a matrix contrast, we test a hypothesis involving multiple linear combinations simultaneously, specified by a $q \\times p$ contrast matrix $R$ of rank $q$. The null hypothesis is $H_0: R \\beta = 0$. The test statistic is the $F$-statistic, which compares the variance explained by the contrast to the residual variance. It is given by:\n$$\nF = \\frac{(R\\hat{\\beta})^\\top (R(X^\\top X)^{-1}R^\\top)^{-1} (R\\hat{\\beta})}{q \\cdot \\hat{\\sigma}^2}\n$$\nWith the orthonormal design simplification, this becomes:\n$$\nF = \\frac{(R\\hat{\\beta})^\\top (RR^\\top)^{-1} (R\\hat{\\beta})}{q \\cdot \\hat{\\sigma}^2}\n$$\nUnder the null hypothesis $H_0$, this statistic follows an $F$-distribution with $(q, n-p)$ degrees of freedom. The $p$-value is $P(F_{q, n-p} > F_{obs})$.\n\nThe mandatory validation steps are rooted in these fundamental statistical properties:\n1.  **Effect Detection**: When a true effect exists (i.e., $c^\\top \\beta_{\\text{true}} \\neq 0$), a well-powered experiment (large effect size, low noise $\\sigma$, large $n$) will produce an estimate $c^\\top \\hat{\\beta}$ far from zero, yielding a large $|t|$-statistic and a very small $p$-value.\n2.  **Null Effect Non-significance**: When no true effect exists ($c^\\top \\beta_{\\text{true}} = 0$), the $t$-statistic follows its null distribution, centered at $0$. A given realization will produce a $p$-value uniformly distributed on $[0,1]$, thus it is expected to be non-significant (e.g., $0.1$) a high proportion of the time.\n3.  **t-statistic Scaling Invariance**: If we scale a contrast vector $c$ by a positive scalar $k > 0$, the $t$-statistic remains unchanged: $t(kc) = \\frac{(kc)^\\top \\hat{\\beta}}{\\hat{\\sigma} \\sqrt{(kc)^\\top (kc)}} = \\frac{k(c^\\top \\hat{\\beta})}{\\hat{\\sigma} \\sqrt{k^2 c^\\top c}} = \\frac{k(c^\\top \\hat{\\beta})}{k \\hat{\\sigma} \\sqrt{c^\\top c}} = t(c)$.\n4.  **Equivalence of $F$ and $t^2$ for $q=1$**: For a one-dimensional contrast ($q=1$), $R$ is a $1 \\times p$ row vector, equivalent to $c^\\top$. The $F$-statistic becomes $F = \\frac{(c^\\top\\hat{\\beta})^\\top (c^\\top c)^{-1} (c^\\top\\hat{\\beta})}{1 \\cdot \\hat{\\sigma}^2} = \\frac{(c^\\top \\hat{\\beta})^2}{\\hat{\\sigma}^2 (c^\\top c)} = \\left(\\frac{c^\\top \\hat{\\beta}}{\\hat{\\sigma} \\sqrt{c^\\top c}}\\right)^2 = t^2$. This is a fundamental identity relating the two distributions.\n5.  **F-statistic Invariance**: The $F$-test assesses whether $R\\beta$ lies in a particular subspace. The test should be invariant to the choice of basis for that subspace. If $A$ is an invertible $q \\times q$ matrix, then $R$ and $R' = AR$ define the same hypothesis (the null space of $\\beta$ under the mapping is identical). The numerator of the $F$-statistic for $R'$ is $(AR\\hat{\\beta})^\\top (A(RR^\\top)A^\\top)^{-1} (AR\\hat{\\beta}) = (R\\hat{\\beta})^\\top A^\\top (A^\\top)^{-1}(RR^\\top)^{-1}A^{-1}A(R\\hat{\\beta}) = (R\\hat{\\beta})^\\top (RR^\\top)^{-1} (R\\hat{\\beta})$, which is identical to the numerator for $R$. Thus, the $F$-statistic is invariant.\n6.  **Multi-row Null Effect Non-significance**: Similar to point 2, if the null hypothesis $R\\beta_{\\text{true}} = 0$ is true, the $F$-statistic follows its null distribution, and we expect a non-significant $p$-value.\n\nThe program will now implement these calculations and perform the specified validations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Implements and validates GLM hypothesis testing from first principles.\n    \"\"\"\n\n    def generate_data(n, p, beta_true, sigma, design_seed, noise_seed):\n        \"\"\"Generates synthetic data for the GLM.\"\"\"\n        # Generate orthonormal design matrix X\n        rng_X = np.random.default_rng(design_seed)\n        random_matrix = rng_X.standard_normal(size=(n, p))\n        X, _ = np.linalg.qr(random_matrix)\n\n        # Generate noise\n        rng_noise = np.random.default_rng(noise_seed)\n        epsilon = rng_noise.normal(loc=0, scale=sigma, size=n)\n\n        # Generate observed data y\n        y = X @ beta_true + epsilon\n        return y, X\n\n    def compute_statistics(y, X, c=None, R=None):\n        \"\"\"Computes GLM estimates and test statistics.\"\"\"\n        n, p = X.shape\n        df = n - p\n        results = {}\n\n        # OLS parameter estimation (simplified for orthonormal X)\n        beta_hat = X.T @ y\n\n        # Residual variance estimation\n        residuals = y - X @ beta_hat\n        sigma2_hat = (residuals.T @ residuals) / df\n        \n        results['beta_hat'] = beta_hat\n        results['sigma2_hat'] = sigma2_hat\n\n        # t-statistic for vector contrast c\n        if c is not None:\n            c = np.asarray(c, dtype=float)\n            t_num = c.T @ beta_hat\n            # Simplified denominator for orthonormal X, where X.T @ X = I\n            t_den = np.sqrt(sigma2_hat * (c.T @ c))\n            t_stat = t_num / t_den\n            \n            # Two-sided p-value\n            t_pval = 2 * stats.t.sf(np.abs(t_stat), df=df)\n            \n            results['t_stat'] = t_stat\n            results['t_pval'] = t_pval\n\n        # F-statistic for matrix contrast R\n        if R is not None:\n            R = np.asarray(R, dtype=float)\n            q = R.shape[0]\n\n            # Simplified F-statistic for orthonormal X\n            term1 = R @ beta_hat\n            # Inverting RR'\n            term2 = np.linalg.inv(R @ R.T)\n            \n            f_num = term1.T @ term2 @ term1\n            f_stat = f_num / (q * sigma2_hat)\n            \n            # Upper-tail p-value (survival function)\n            f_pval = stats.f.sf(f_stat, dfn=q, dfd=df)\n            \n            results['f_stat'] = f_stat\n            results['f_pval'] = f_pval\n            \n        return results\n\n    validation_results = []\n\n    # Test case 1: effect present, vector contrast detection\n    n1, p1 = 240, 5\n    beta_true1 = np.array([2.0, 0.0, 0.0, 0.0, 0.0])\n    y1, X1 = generate_data(n=n1, p=p1, beta_true=beta_true1, sigma=0.2, design_seed=123, noise_seed=456)\n    c1 = np.array([1, 0, 0, 0, 0])\n    stats1 = compute_statistics(y1, X1, c=c1)\n    is_valid1 = (stats1['t_pval']  1e-8) and (stats1['t_stat']  0)\n    validation_results.append(is_valid1)\n\n    # Test case 2: effect absent, vector contrast non-significance\n    n2, p2 = 240, 5\n    beta_true2 = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n    y2, X2 = generate_data(n=n2, p=p2, beta_true=beta_true2, sigma=0.5, design_seed=123, noise_seed=789)\n    c2 = np.array([1, 0, 0, 0, 0])\n    stats2 = compute_statistics(y2, X2, c=c2)\n    is_valid2 = stats2['t_pval']  0.1\n    validation_results.append(is_valid2)\n\n    # Test case 3: scaling invariance of t\n    c3_1 = np.array([1, 0, 0, 0, 0])\n    c3_2 = 7.0 * c3_1\n    stats3_1 = compute_statistics(y1, X1, c=c3_1) # Reuses data from case 1\n    stats3_2 = compute_statistics(y1, X1, c=c3_2)\n    is_valid3 = np.abs(stats3_1['t_stat'] - stats3_2['t_stat'])  1e-12\n    validation_results.append(is_valid3)\n\n    # Test case 4: F equals squared t for q = 1\n    c4 = np.array([1, 0, 0, 0, 0])\n    R4 = c4.reshape(1, -1)\n    stats4_t = compute_statistics(y1, X1, c=c4) # Reuses data from case 1\n    stats4_f = compute_statistics(y1, X1, R=R4)\n    is_valid4 = np.abs(stats4_f['f_stat'] - (stats4_t['t_stat']**2))  1e-10\n    validation_results.append(is_valid4)\n\n    # Test case 5: invariance of F under invertible left transformations of R\n    n5, p5 = 300, 5\n    beta_true5 = np.array([1.5, -1.0, 0.0, 0.0, 0.0])\n    y5, X5 = generate_data(n=n5, p=p5, beta_true=beta_true5, sigma=0.3, design_seed=321, noise_seed=654)\n    R5 = np.array([[1, 0, 0, 0, 0], [0, 1, 0, 0, 0]])\n    A5 = np.array([[2.0, 0.5], [0.1, 1.0]])\n    R5_prime = A5 @ R5\n    stats5_R = compute_statistics(y5, X5, R=R5)\n    stats5_R_prime = compute_statistics(y5, X5, R=R5_prime)\n    is_valid5 = np.abs(stats5_R['f_stat'] - stats5_R_prime['f_stat'])  1e-10\n    validation_results.append(is_valid5)\n\n    # Test case 6: multi-row contrast non-significance under null effects\n    n6, p6 = 300, 5\n    beta_true6 = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n    y6, X6 = generate_data(n=n6, p=p6, beta_true=beta_true6, sigma=0.4, design_seed=321, noise_seed=987)\n    R6 = np.array([[1, 0, 0, 0, 0], [0, 1, 0, 0, 0]])\n    stats6 = compute_statistics(y6, X6, R=R6)\n    is_valid6 = stats6['f_pval']  0.1\n    validation_results.append(is_valid6)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, validation_results))}]\")\n\nsolve()\n```"
        }
    ]
}