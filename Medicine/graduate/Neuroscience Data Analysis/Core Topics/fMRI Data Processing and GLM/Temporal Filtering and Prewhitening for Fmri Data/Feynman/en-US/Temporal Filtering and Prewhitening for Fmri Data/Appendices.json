{
    "hands_on_practices": [
        {
            "introduction": "High-pass filtering is a ubiquitous preprocessing step in fMRI analysis, essential for removing slow signal drifts unrelated to neural activity. Within the General Linear Model (GLM) framework, this is not done by directly altering the time series but by including low-frequency nuisance regressors in the design matrix. This practice bridges theory and application by demonstrating how to calculate the precise number of Discrete Cosine Transform (DCT) basis functions needed to implement a filter with a specific cutoff period, a common task when setting up analyses in software like SPM.",
            "id": "4197992",
            "problem": "Given a single run of functional Magnetic Resonance Imaging (fMRI) data with Repetition Time (TR) of $2\\ \\mathrm{s}$ and a run length of $N=240$ volumes, consider the implementation of a temporal high-pass filter in Statistical Parametric Mapping (SPM) using Discrete Cosine Transform (DCT) basis functions. In SPM, low-frequency drifts are modeled by including a set of low-frequency cosine components (including the zero-frequency constant term) whose temporal periods are greater than or equal to the specified high-pass cut-off period. The cut-off period for this problem is $128\\ \\mathrm{s}$. Starting from the definition of the DCT-II basis functions and the notion of cycles per sample, derive the number of DCT basis functions required to model all cosine components with periods greater than or equal to the cut-off, explicitly counting the zero-frequency cosine component. Then compute this number for the given parameters. Provide your final answer as the exact integer count, with no units.",
            "solution": "The problem statement is first subjected to validation.\n\n**Step 1: Extract Givens**\n- Repetition Time: $TR = 2\\ \\mathrm{s}$\n- Run length (number of volumes): $N = 240$\n- High-pass cut-off period: $P_{cutoff} = 128\\ \\mathrm{s}$\n- Method: Temporal high-pass filtering is achieved by modeling low-frequency drifts with a set of Discrete Cosine Transform (DCT) basis functions.\n- Inclusion criterion: The model includes all cosine components (including the zero-frequency constant term) whose temporal periods are greater than or equal to the specified high-pass cut-off period.\n- Objective: Derive and compute the total number of such DCT basis functions.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on established methods in functional Magnetic Resonance Imaging (fMRI) data analysis, specifically the use of the General Linear Model (GLM) with DCT basis functions for temporal filtering, as implemented in standard software packages like Statistical Parametric Mapping (SPM). The parameters are realistic for a typical fMRI experiment. The approach is scientifically sound.\n- **Well-Posed:** The problem provides all necessary parameters ($N$, $TR$, $P_{cutoff}$) and a clear, unambiguous rule for determining which basis functions to include. This ensures that a unique, meaningful integer solution exists.\n- **Objective:** The problem is stated in precise, technical language, free from subjectivity or opinion.\n- **Completeness and Consistency:** The problem is self-contained and internally consistent.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be derived.\n\nThe problem asks for the number of DCT basis functions required to model low-frequency signal components in an fMRI time series. The time series consists of $N=240$ samples (volumes), acquired with a sampling interval of $TR=2\\ \\mathrm{s}$. The condition for including a basis function in the model is that its temporal period must be greater than or equal to a cut-off period of $P_{cutoff}=128\\ \\mathrm{s}$.\n\nIn SPM, the set of DCT-II basis functions for a time series of length $N$ is used. For a discrete time index $n \\in \\{0, 1, \\dots, N-1\\}$, the $k$-th basis function (for $k \\in \\{1, 2, \\dots, N-1\\}$) is proportional to:\n$$ \\cos\\left( \\frac{\\pi k (2n+1)}{2N} \\right) $$\nThe frequency of the $k$-th basis function, in cycles per sample, is $f_k^{\\text{cycles/sample}} = \\frac{k}{2N}$. To convert this to Hertz (cycles per second), we divide by the sampling interval, $TR$:\n$$ f_k^{\\text{Hz}} = \\frac{f_k^{\\text{cycles/sample}}}{TR} = \\frac{k}{2N \\cdot TR} $$\nThe period $P_k$ of the $k$-th basis function is the inverse of its frequency in Hertz:\n$$ P_k = \\frac{1}{f_k^\\text{Hz}} = \\frac{2N \\cdot TR}{k} $$\nThis formula is valid for $k = 1, 2, \\dots, N-1$.\n\nThe problem also specifies the inclusion of the \"zero-frequency constant term\". This corresponds to the $k=0$ basis function, which is a constant DC component. The frequency of this component is $f_0 = 0\\ \\mathrm{Hz}$, and its period can be considered infinite, $P_0 = \\infty$.\n\nThe criterion for including a basis function is $P_k \\ge P_{cutoff}$.\nFirst, consider the $k=0$ component. Since its period $P_0 = \\infty$ and the cut-off $P_{cutoff} = 128\\ \\mathrm{s}$ is finite, the condition $\\infty \\ge 128$ is always met. Therefore, the $k=0$ basis function is always included. This accounts for one function.\n\nNext, we consider the basis functions for $k \\ge 1$. We need to find the number of integers $k$ that satisfy the inequality:\n$$ P_k \\ge P_{cutoff} $$\nSubstituting the expression for $P_k$:\n$$ \\frac{2N \\cdot TR}{k} \\ge P_{cutoff} $$\nSince $k$ is a positive integer and $P_{cutoff}$ is positive, we can rearrange the inequality to solve for $k$:\n$$ 2N \\cdot TR \\ge k \\cdot P_{cutoff} $$\n$$ k \\le \\frac{2N \\cdot TR}{P_{cutoff}} $$\nThe values of $k$ that satisfy this condition are $k = 1, 2, \\dots, K_{max}$, where $K_{max}$ is the largest integer less than or equal to the value on the right-hand side. This is given by the floor function:\n$$ K_{max} = \\left\\lfloor \\frac{2N \\cdot TR}{P_{cutoff}} \\right\\rfloor $$\nThe number of these basis functions (for $k \\ge 1$) is simply $K_{max}$.\n\nThe total number of basis functions, $M$, is the sum of the count for $k=0$ and the count for $k \\ge 1$:\n$$ M = 1 + K_{max} = 1 + \\left\\lfloor \\frac{2N \\cdot TR}{P_{cutoff}} \\right\\rfloor $$\nNow, we substitute the given values: $N=240$, $TR=2\\ \\mathrm{s}$, and $P_{cutoff}=128\\ \\mathrm{s}$.\n$$ M = 1 + \\left\\lfloor \\frac{2 \\times 240 \\times 2}{128} \\right\\rfloor $$\nFirst, we evaluate the fraction:\n$$ \\frac{2 \\times 240 \\times 2}{128} = \\frac{960}{128} $$\nTo simplify the fraction, we can divide the numerator and denominator by their greatest common divisor. Both are divisible by $64$:\n$$ \\frac{960}{128} = \\frac{15 \\times 64}{2 \\times 64} = \\frac{15}{2} = 7.5 $$\nSubstituting this value back into the expression for $M$:\n$$ M = 1 + \\lfloor 7.5 \\rfloor $$\nThe floor of $7.5$ is $7$.\n$$ M = 1 + 7 = 8 $$\nTherefore, a total of $8$ DCT basis functions are required to model the low-frequency drifts according to the specified criteria. These correspond to the indices $k = 0, 1, 2, 3, 4, 5, 6, 7$.",
            "answer": "$$\n\\boxed{8}\n$$"
        },
        {
            "introduction": "While temporal filtering removes noise in specific frequency bands, prewhitening offers a more statistically principled approach by transforming the data and model to make the residuals conform to the assumptions of ordinary least squares. This process hinges on first building an accurate model of the temporal autocorrelation present in the noise. This exercise guides you through the fundamental step of estimating the noise structure by deriving and applying the Yule-Walker method to estimate the parameter of a first-order autoregressive, or $\\mathrm{AR}(1)$, model from a series of residuals.",
            "id": "4197953",
            "problem": "You are analyzing noise autocorrelation in a short calibration run of functional Magnetic Resonance Imaging (fMRI) data acquired with repetition time (TR) of $2$ seconds. After fitting a General Linear Model (GLM) with temporally filtered regressors, you obtain residuals $\\hat{\\varepsilon}_{t}$ for $t=1,\\dots,T$. For prewhitening, assume the GLM residuals follow a stationary first-order autoregressive model $\\mathrm{AR}(1)$ with zero mean,\n$$\n\\hat{\\varepsilon}_{t} = \\phi \\hat{\\varepsilon}_{t-1} + w_{t},\n$$\nwhere $w_{t}$ is zero-mean independent and identically distributed white noise with variance $\\sigma_{w}^{2}$ and $|\\phi|<1$. Starting from the definition of the autocovariance function for a strictly stationary process,\n$$\n\\gamma(k) = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-k} \\right],\n$$\nderive an estimator for $\\phi$ by replacing population quantities with their sample analogs,\n$$\n\\hat{\\gamma}(k) = \\frac{1}{T} \\sum_{t=k+1}^{T} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-k},\n$$\nand show that it can be expressed in closed form using the residuals. Then, using the following mean-centered GLM residuals from a short run of length $T=8$,\n$$\n\\left\\{ \\hat{\\varepsilon}_{t} \\right\\}_{t=1}^{8} = \\{ 0.7,\\; 0.4,\\; 0.2,\\; -0.1,\\; -0.3,\\; -0.5,\\; -0.2,\\; -0.2 \\},\n$$\ncompute the numerical value of the estimator. Round your numerical answer to four significant figures. Finally, briefly justify within your derivation why short runs can induce bias in this estimator, and indicate the sign of the bias when $\\phi>0$ under the given sample-autocovariance convention. Express the final answer as a single real number without units.",
            "solution": "The problem requires the derivation of an estimator for the first-order autoregressive parameter $\\phi$ from the residuals of a General Linear Model (GLM), its numerical computation for a given data set, and a justification for its bias in short time series.\n\nFirst, we begin with the provided model for the GLM residuals, a stationary first-order autoregressive process, $\\mathrm{AR}(1)$, with zero mean:\n$$\n\\hat{\\varepsilon}_{t} = \\phi \\hat{\\varepsilon}_{t-1} + w_{t}\n$$\nwhere $w_{t}$ is i.i.d. white noise with $\\mathbb{E}[w_t] = 0$ and variance $\\sigma_{w}^{2}$, and the stationarity condition requires $|\\phi|<1$. The autocovariance function for this strictly stationary process is defined as $\\gamma(k) = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-k} \\right]$.\n\nTo find an expression for $\\phi$ in terms of population autocovariances, we compute $\\gamma(k)$ for $k=0$ and $k=1$.\nFor $k=0$, the autocovariance is the variance of the process:\n$$\n\\gamma(0) = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t} \\right] = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}^{2} \\right] = \\sigma_{\\varepsilon}^{2}\n$$\nFor $k=1$, the autocovariance is:\n$$\n\\gamma(1) = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-1} \\right]\n$$\nSubstitute the $\\mathrm{AR}(1)$ model definition into the expression for $\\gamma(1)$:\n$$\n\\gamma(1) = \\mathbb{E}\\left[ (\\phi \\hat{\\varepsilon}_{t-1} + w_{t}) \\hat{\\varepsilon}_{t-1} \\right]\n$$\nBy linearity of expectation, this becomes:\n$$\n\\gamma(1) = \\mathbb{E}\\left[ \\phi \\hat{\\varepsilon}_{t-1}^{2} \\right] + \\mathbb{E}\\left[ w_{t}\\hat{\\varepsilon}_{t-1} \\right] = \\phi \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t-1}^{2} \\right] + \\mathbb{E}\\left[ w_{t}\\hat{\\varepsilon}_{t-1} \\right]\n$$\nDue to stationarity, the variance is constant over time, so $\\mathbb{E}\\left[ \\hat{\\varepsilon}_{t-1}^{2} \\right] = \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t}^{2} \\right] = \\gamma(0)$. The noise term $w_{t}$ is, by definition, an innovation that is independent of all past values of the process, including $\\hat{\\varepsilon}_{t-1}$. Therefore, the expectation of their product is the product of their expectations:\n$$\n\\mathbb{E}\\left[ w_{t}\\hat{\\varepsilon}_{t-1} \\right] = \\mathbb{E}\\left[ w_{t} \\right] \\mathbb{E}\\left[ \\hat{\\varepsilon}_{t-1} \\right] = 0 \\times 0 = 0\n$$\nSubstituting these results back into the equation for $\\gamma(1)$ yields the Yule-Walker equation for an $\\mathrm{AR}(1)$ process:\n$$\n\\gamma(1) = \\phi \\gamma(0)\n$$\nFrom this, we can express the true parameter $\\phi$ as the ratio of the lag-$1$ autocovariance to the lag-$0$ autocovariance (the variance):\n$$\n\\phi = \\frac{\\gamma(1)}{\\gamma(0)}\n$$\nThe problem specifies that an estimator for $\\phi$, denoted $\\hat{\\phi}$, should be derived by replacing the population quantities $\\gamma(k)$ with their sample analogs $\\hat{\\gamma}(k)$. The sample autocovariance is given as:\n$$\n\\hat{\\gamma}(k) = \\frac{1}{T} \\sum_{t=k+1}^{T} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-k}\n$$\nUsing this definition, we construct the estimator $\\hat{\\phi}$:\n$$\n\\hat{\\phi} = \\frac{\\hat{\\gamma}(1)}{\\hat{\\gamma}(0)} = \\frac{\\frac{1}{T} \\sum_{t=2}^{T} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-1}}{\\frac{1}{T} \\sum_{t=1}^{T} \\hat{\\varepsilon}_{t}^{2}}\n$$\nThe factor of $\\frac{1}{T}$ cancels, leaving the closed-form estimator expressed using the residuals:\n$$\n\\hat{\\phi} = \\frac{\\sum_{t=2}^{T} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-1}}{\\sum_{t=1}^{T} \\hat{\\varepsilon}_{t}^{2}}\n$$\nThis is the ordinary least squares (OLS) estimator of the coefficient in a regression of $\\hat{\\varepsilon}_{t}$ on $\\hat{\\varepsilon}_{t-1}$.\n\nRegarding bias, this estimator is known to be biased in finite samples (i.e., for small $T$). The bias, $\\mathbb{E}[\\hat{\\phi}] - \\phi$, is of order $O(1/T)$ and thus diminishes as the length of the time series $T$ increases, making the estimator asymptotically unbiased (consistent). The bias arises because the regressor, $\\hat{\\varepsilon}_{t-1}$, is not strictly exogenous; it is correlated with the history of the error terms $\\{w_{t-1}, w_{t-2}, \\dots\\}$. This leads to a correlation between the numerator and denominator of the stochastic part of the estimator, $\\sum w_t \\hat{\\varepsilon}_{t-1}$ and $\\sum \\hat{\\varepsilon}_t^2$, respectively. For an $\\mathrm{AR}(1)$ process with $\\phi > 0$, this bias is characteristically negative. That is, $\\mathbb{E}[\\hat{\\phi}] < \\phi$. This means that for short data runs, the estimator tends to underestimate the true magnitude of the temporal autocorrelation. This downward bias is a well-established result in time series analysis (e.g., Hurwicz bias).\n\nNow, we compute the numerical value of $\\hat{\\phi}$ for the given data set:\n$$\n\\left\\{ \\hat{\\varepsilon}_{t} \\right\\}_{t=1}^{8} = \\{ 0.7,\\; 0.4,\\; 0.2,\\; -0.1,\\; -0.3,\\; -0.5,\\; -0.2,\\; -0.2 \\}\n$$\nThe length of the run is $T=8$.\n\nFirst, we calculate the numerator, the sum of lagged products, $\\sum_{t=2}^{8} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-1}$:\n\\begin{align*}\n\\sum_{t=2}^{8} \\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t-1} &= (0.4)(0.7) + (0.2)(0.4) + (-0.1)(0.2) + (-0.3)(-0.1) + (-0.5)(-0.3) + (-0.2)(-0.5) + (-0.2)(-0.2) \\\\\n&= 0.28 + 0.08 - 0.02 + 0.03 + 0.15 + 0.10 + 0.04 \\\\\n&= 0.66\n\\end{align*}\n\nNext, we calculate the denominator, the sum of squares, $\\sum_{t=1}^{8} \\hat{\\varepsilon}_{t}^{2}$:\n\\begin{align*}\n\\sum_{t=1}^{8} \\hat{\\varepsilon}_{t}^{2} &= (0.7)^{2} + (0.4)^{2} + (0.2)^{2} + (-0.1)^{2} + (-0.3)^{2} + (-0.5)^{2} + (-0.2)^{2} + (-0.2)^{2} \\\\\n&= 0.49 + 0.16 + 0.04 + 0.01 + 0.09 + 0.25 + 0.04 + 0.04 \\\\\n&= 1.12\n\\end{align*}\n\nFinally, we compute the estimator $\\hat{\\phi}$:\n$$\n\\hat{\\phi} = \\frac{0.66}{1.12} = \\frac{33}{56} \\approx 0.5892857...\n$$\nRounding the numerical value to four significant figures gives $0.5893$.",
            "answer": "$$\n\\boxed{0.5893}\n$$"
        },
        {
            "introduction": "The ultimate goal of prewhitening is to produce residuals that are serially uncorrelated, or 'white', thereby validating the statistical inferences made from the GLM. It is therefore critical to have diagnostic tools to check whether this goal has been achieved. This practice introduces the Durbin-Watson statistic, a classic test for lag-1 autocorrelation, and challenges you to compute it for a hypothetical set of residuals to see what happens when the whitening procedure is imperfect, in this case leading to residual anti-correlation.",
            "id": "4197966",
            "problem": "Consider a single-voxel time series from a Functional Magnetic Resonance Imaging (fMRI) experiment sampled every $2$ seconds across $8$ scans, modeled by a general linear model (GLM) with regressors capturing task-related effects and nuisance drift terms. Suppose the noise process is well-approximated as first-order autoregressive with true parameter $\\phi_{\\text{true}}$, but the prewhitening stage uses a parameter $\\phi_{\\text{pw}}$ that overestimates $\\phi_{\\text{true}}$, leading to residual anti-correlation after fitting the GLM.\n\nAfter prewhitening and estimation, the residuals for this voxel at times $t=1,\\dots,8$ are observed to be\n$$\ne_1=1,\\quad e_2=-1,\\quad e_3=1,\\quad e_4=-1,\\quad e_5=1,\\quad e_6=-1,\\quad e_7=1,\\quad e_8=-1.\n$$\n\nStarting from the definitions of the general linear model residuals and the sample lag-$1$ covariance structure implied by a first-order autoregression, derive the Durbin–Watson statistic as a function of the residuals $\\{e_t\\}$, and compute its value for the sequence above. Explain how values deviating from $2$ indicate residual autocorrelation and interpret the computed value in terms of remaining correlation after prewhitening. Report the Durbin–Watson statistic as an exact number with no rounding and no units.",
            "solution": "The problem asks for the derivation and computation of the Durbin–Watson statistic for a given sequence of residuals from a general linear model (GLM) applied to fMRI data, and an interpretation of the result in the context of prewhitening.\n\nFirst, we address the derivation and definition of the Durbin–Watson statistic, denoted by $d$. For a time series of residuals $\\{e_t\\}_{t=1}^T$, where $T$ is the number of observations, the statistic is defined as the ratio of the sum of squared differences of successive residuals to the sum of squared residuals:\n$$\nd = \\frac{\\sum_{t=2}^T (e_t - e_{t-1})^2}{\\sum_{t=1}^T e_t^2}\n$$\nThis is the function of the residuals $\\{e_t\\}$ requested by the problem.\n\nTo understand how this statistic relates to autocorrelation, we can expand the numerator:\n$$\n\\sum_{t=2}^T (e_t - e_{t-1})^2 = \\sum_{t=2}^T (e_t^2 - 2e_t e_{t-1} + e_{t-1}^2) = \\sum_{t=2}^T e_t^2 + \\sum_{t=2}^T e_{t-1}^2 - 2\\sum_{t=2}^T e_t e_{t-1}\n$$\nThe second term in the expansion is $\\sum_{t=2}^T e_{t-1}^2 = \\sum_{k=1}^{T-1} e_k^2$. For a sufficiently large sample size $T$, the following approximations hold: $\\sum_{t=2}^T e_t^2 \\approx \\sum_{t=1}^T e_t^2$ and $\\sum_{t=1}^{T-1} e_t^2 \\approx \\sum_{t=1}^T e_t^2$. Using these approximations, the numerator becomes approximately $2 \\sum_{t=1}^T e_t^2 - 2 \\sum_{t=2}^T e_t e_{t-1}$.\n\nSubstituting this approximation back into the definition of $d$:\n$$\nd \\approx \\frac{2 \\sum_{t=1}^T e_t^2 - 2 \\sum_{t=2}^T e_t e_{t-1}}{\\sum_{t=1}^T e_t^2} = 2 \\left( 1 - \\frac{\\sum_{t=2}^T e_t e_{t-1}}{\\sum_{t=1}^T e_t^2} \\right)\n$$\nThe sample lag-$1$ autocorrelation coefficient, $\\hat{\\rho}$, for a series with zero mean is estimated as $\\hat{\\rho} = \\frac{\\sum_{t=2}^T e_t e_{t-1}}{\\sum_{t=1}^T e_t^2}$ (assuming the variance is constant, which allows replacing the denominator $\\sqrt{(\\sum_{t=2}^T e_{t-1}^2)(\\sum_{t=2}^T e_t^2)}$ with $\\sum_{t=1}^T e_t^2$). Thus, we arrive at the well-known approximate relationship:\n$$\nd \\approx 2(1 - \\hat{\\rho})\n$$\n\nNext, we compute the value of $d$ for the given sequence of residuals. The number of scans is $T=8$, and the residuals are:\n$e_1=1$, $e_2=-1$, $e_3=1$, $e_4=-1$, $e_5=1$, $e_6=-1$, $e_7=1$, $e_8=-1$.\n\nFirst, we compute the denominator, the sum of squared residuals:\n$$\n\\sum_{t=1}^8 e_t^2 = (1)^2 + (-1)^2 + (1)^2 + (-1)^2 + (1)^2 + (-1)^2 + (1)^2 + (-1)^2 = 1+1+1+1+1+1+1+1 = 8\n$$\n\nNext, we compute the numerator, the sum of squared differences of successive residuals:\n\\begin{align*}\n\\sum_{t=2}^8 (e_t - e_{t-1})^2 &= (e_2 - e_1)^2 + (e_3 - e_2)^2 + (e_4 - e_3)^2 + (e_5 - e_4)^2 + (e_6 - e_5)^2 + (e_7 - e_6)^2 + (e_8 - e_7)^2 \\\\\n&= (-1 - 1)^2 + (1 - (-1))^2 + (-1 - 1)^2 + (1 - (-1))^2 + (-1 - 1)^2 + (1 - (-1))^2 + (-1 - 1)^2 \\\\\n&= (-2)^2 + (2)^2 + (-2)^2 + (2)^2 + (-2)^2 + (2)^2 + (-2)^2 \\\\\n&= 4 + 4 + 4 + 4 + 4 + 4 + 4 \\\\\n&= 7 \\times 4 = 28\n\\end{align*}\n\nNow, we can compute the Durbin–Watson statistic:\n$$\nd = \\frac{28}{8} = \\frac{7}{2} = 3.5\n$$\n\nFinally, we interpret this result. The Durbin–Watson statistic $d$ ranges approximately from $0$ to $4$.\n- If there is no first-order autocorrelation, $\\hat{\\rho} \\approx 0$, and $d \\approx 2(1-0) = 2$. A value of $d$ near $2$ suggests the residuals are uncorrelated.\n- If there is perfect positive autocorrelation, $\\hat{\\rho} \\approx 1$, and $d \\approx 2(1-1) = 0$. A value of $d$ close to $0$ indicates strong positive autocorrelation.\n- If there is perfect negative autocorrelation (anti-correlation), $\\hat{\\rho} \\approx -1$, and $d \\approx 2(1 - (-1)) = 4$. A value of $d$ close to $4$ indicates strong negative autocorrelation.\n\nThe computed value is $d = 3.5$. This value is much closer to $4$ than to $2$, which indicates the presence of strong negative autocorrelation (anti-correlation) in the residuals. This finding is consistent with the problem's premise. The noise was modeled as an AR($1$) process with true parameter $\\phi_{\\text{true}}$. The prewhitening procedure, intended to remove this correlation, used a parameter $\\phi_{\\text{pw}}$ that was an overestimate, i.e., $\\phi_{\\text{pw}} > \\phi_{\\text{true}} > 0$. This over-correction effectively \"subtracts too much\" of the previous time point's signal, inducing a new, artificial negative correlation in the transformed data. The observed residual sequence $e_t = (-1)^{t+1}$ is a textbook example of perfect anti-correlation, and the resulting Durbin–Watson statistic of $3.5$ provides a quantitative confirmation of this structure. The slight deviation from the theoretical maximum of $4$ is due to the small sample size ($T=8$) and the associated edge effects in the summation, which cause the approximation $d \\approx 2(1-\\hat{\\rho})$ to be inexact.",
            "answer": "$$\\boxed{\\frac{7}{2}}$$"
        }
    ]
}