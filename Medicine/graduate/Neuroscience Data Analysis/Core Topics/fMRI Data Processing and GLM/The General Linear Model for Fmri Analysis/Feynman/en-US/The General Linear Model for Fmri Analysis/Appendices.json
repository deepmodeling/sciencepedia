{
    "hands_on_practices": [
        {
            "introduction": "The design matrix $X$ is the core of the General Linear Model, translating the timing of experimental events into a set of mathematical predictors. While using a single, canonical Hemodynamic Response Function (HRF) is common, a more flexible approach is to estimate the HRF's shape directly from the data. This exercise guides you through the fundamental process of constructing a design matrix for a Finite Impulse Response (FIR) model, which achieves this by representing the HRF as a series of time-lagged impulses. By deriving the structure of the design matrix from the first principles of discrete-time convolution, you will gain a foundational understanding of how a flexible, non-parametric model is implemented in the GLM framework.",
            "id": "4199556",
            "problem": "You are analyzing a single-run event-related functional Magnetic Resonance Imaging (fMRI) experiment using the General Linear Model (GLM). Let $N$ denote the number of acquired volumes and let the Repetition Time (TR) be constant across the run. Consider one task condition $c$ whose onsets are encoded by a discrete-time stimulus indicator vector $\\mathbf{s}^{(c)} \\in \\{0,1\\}^{N}$ defined on the index set $\\{0,1,\\dots,N-1\\}$, where $s^{(c)}_{t} = 1$ if an onset of condition $c$ occurs at time index $t$ and $s^{(c)}_{t} = 0$ otherwise. Assume a linear time-invariant hemodynamic system so that the predicted Blood Oxygenation Level Dependent (BOLD) signal is given by the discrete-time convolution of the stimulus with the Hemodynamic Response Function (HRF). In a nonparametric approach, approximate the HRF as a Finite Impulse Response (FIR) model with $L$ time bins, meaning that the HRF is represented by $L$ unknown coefficients, one per lagged time bin within the window $\\{0,1,\\dots,L-1\\}$.\n\nStarting only from the discrete-time convolution definition and the causal, finite-support FIR assumption described above, define the FIR model for condition $c$ and construct the corresponding design submatrix $X^{(c)} \\in \\mathbb{R}^{N \\times L}$ that estimates the HRF shape nonparametrically. Your construction must be entry-wise, i.e., you must provide a closed-form analytic expression for the $(t,\\ell)$-entry of $X^{(c)}$ expressed solely in terms of the stimulus indicator vector $\\mathbf{s}^{(c)}$ and the indices $t \\in \\{0,1,\\dots,N-1\\}$ and $\\ell \\in \\{0,1,\\dots,L-1\\}$. You may use the discrete-time indicator function $\\mathbf{1}_{A}$, which equals $1$ if the condition $A$ holds and $0$ otherwise. Express your final answer as a single closed-form analytic expression with no units.",
            "solution": "The problem asks for the construction of the design submatrix for a single condition in an event-related fMRI experiment, where the hemodynamic response is modeled nonparametrically using a Finite Impulse Response (FIR) basis set. The construction must be derived from first principles, namely the discrete-time convolution of a stimulus indicator vector with an unknown FIR filter.\n\nLet $\\mathbf{y} \\in \\mathbb{R}^{N}$ be the measured Blood Oxygenation Level Dependent (BOLD) signal for a single voxel over $N$ time points, indexed by $t \\in \\{0, 1, \\dots, N-1\\}$. The General Linear Model (GLM) posits that this signal can be modeled as a linear combination of predictors plus noise:\n$$ \\mathbf{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} $$\nwhere $X$ is the design matrix, $\\boldsymbol{\\beta}$ is a vector of unknown parameters to be estimated, and $\\boldsymbol{\\epsilon}$ is a vector of residual errors.\n\nOur goal is to construct the specific part of the design matrix, $X^{(c)} \\in \\mathbb{R}^{N \\times L}$, that corresponds to a single task condition $c$. The associated parameters, $\\boldsymbol{\\beta}^{(c)} \\in \\mathbb{R}^{L}$, are the unknown coefficients of the FIR model for the Hemodynamic Response Function (HRF). The predicted BOLD signal due to condition $c$ is thus given by $\\mathbf{y}_{\\text{pred}}^{(c)} = X^{(c)} \\boldsymbol{\\beta}^{(c)}$.\n\nThe problem states that the system is linear and time-invariant (LTI). Therefore, the predicted BOLD signal at time $t$, denoted $(y_{\\text{pred}}^{(c)})_t$, is the discrete-time convolution of the stimulus indicator vector $\\mathbf{s}^{(c)}$ with the HRF, which we denote as $h$. The convolution is defined as:\n$$ (y_{\\text{pred}}^{(c)})_t = (\\mathbf{s}^{(c)} * h)_t = \\sum_{\\tau=0}^{N-1} s^{(c)}_{\\tau} h(t-\\tau) $$\nThe problem specifies a nonparametric FIR model for the HRF. This means the HRF is represented as a weighted sum of $L$ time-shifted impulses. Let the unknown weights (the heights of the HRF at different time lags) be the coefficients $\\beta^{(c)}_0, \\beta^{(c)}_1, \\dots, \\beta^{(c)}_{L-1}$. The HRF, $h(k)$, is defined for discrete time lags $k$. The FIR model assumes the HRF is causal and has a finite support of $L$ time bins, so $h(k)$ is non-zero only for $k \\in \\{0, 1, \\dots, L-1\\}$. We can write the HRF as:\n$$ h(k) = \\sum_{\\ell=0}^{L-1} \\beta^{(c)}_{\\ell} \\delta(k - \\ell) $$\nwhere $\\delta(m)$ is the discrete-time impulse or Kronecker delta function, which is $1$ if $m=0$ and $0$ otherwise. The parameter $\\beta^{(c)}_{\\ell}$ represents the value of the HRF at the $\\ell$-th time bin post-stimulus.\n\nWe substitute this FIR definition of $h(k)$ into the convolution equation. The argument to $h$ in the convolution sum is $(t-\\tau)$, so we replace $k$ with $(t-\\tau)$:\n$$ (y_{\\text{pred}}^{(c)})_t = \\sum_{\\tau=0}^{N-1} s^{(c)}_{\\tau} \\left( \\sum_{\\ell=0}^{L-1} \\beta^{(c)}_{\\ell} \\delta((t-\\tau) - \\ell) \\right) $$\nSince the summations are over finite ranges, we can exchange their order:\n$$ (y_{\\text{pred}}^{(c)})_t = \\sum_{\\ell=0}^{L-1} \\beta^{(c)}_{\\ell} \\left( \\sum_{\\tau=0}^{N-1} s^{(c)}_{\\tau} \\delta(t - \\tau - \\ell) \\right) $$\nThis expression is now in the form of a linear combination of the unknown parameters $\\beta^{(c)}_{\\ell}$. This matches the structure of the GLM. The $t$-th row of the matrix equation $\\mathbf{y}_{\\text{pred}}^{(c)} = X^{(c)} \\boldsymbol{\\beta}^{(c)}$ is:\n$$ (y_{\\text{pred}}^{(c)})_t = \\sum_{\\ell=0}^{L-1} X^{(c)}_{t,\\ell} \\beta^{(c)}_{\\ell} $$\nBy comparing this with the rearranged convolution equation, we can identify the entry $X^{(c)}_{t,\\ell}$ of the design submatrix:\n$$ X^{(c)}_{t,\\ell} = \\sum_{\\tau=0}^{N-1} s^{(c)}_{\\tau} \\delta(t - \\tau - \\ell) $$\nThe Kronecker delta term $\\delta(t - \\tau - \\ell)$ is non-zero (equal to $1$) only when its argument is zero, i.e., when $t - \\tau - \\ell = 0$, which implies $\\tau = t - \\ell$. For any other value of $\\tau$, the term is zero. Therefore, the summation over $\\tau$ collapses to a single term, evaluated at $\\tau = t - \\ell$, provided this value of $\\tau$ is within the summation bounds $[0, N-1]$.\n\nThe value of $\\tau$ is $t-\\ell$. The indices for the matrix entries are $t \\in \\{0, 1, \\dots, N-1\\}$ and $\\ell \\in \\{0, 1, \\dots, L-1\\}$.\nLet's check if $\\tau=t-\\ell$ is in the valid range for the index of $s^{(c)}_{\\tau}$.\n1.  Lower bound: $\\tau \\ge 0 \\implies t-\\ell \\ge 0 \\implies t \\ge \\ell$.\n2.  Upper bound: $\\tau \\le N-1 \\implies t-\\ell \\le N-1$. Since the maximum value of $t$ is $N-1$ and the minimum value of $\\ell$ is $0$, the maximum value of $t-\\ell$ is $(N-1)-0 = N-1$. Thus, the condition $t-\\ell \\le N-1$ is always satisfied for the given ranges of $t$ and $\\ell$.\n\nSo, if $t < \\ell$, the value $\\tau = t-\\ell$ is negative and falls outside the summation range. In this case, there is no $\\tau \\in [0, N-1]$ that makes the delta function non-zero, and the sum evaluates to $0$.\nIf $t \\ge \\ell$, the value $\\tau = t-\\ell$ falls within the summation range. The sum collapses to the single term $s^{(c)}_{t-\\ell} \\cdot 1 = s^{(c)}_{t-\\ell}$.\n\nWe can express this case distinction using the discrete-time indicator function $\\mathbf{1}_{A}$, which is $1$ if condition $A$ is true and $0$ otherwise. The condition for a non-zero entry is $t \\ge \\ell$.\nTherefore, the entry at row $t$ and column $\\ell$ of the design submatrix $X^{(c)}$ is given by:\n$$ X^{(c)}_{t,\\ell} = s^{(c)}_{t-\\ell} \\cdot \\mathbf{1}_{t \\ge \\ell} $$\nThis expression is the closed-form representation requested. It constructs each column $\\ell$ of the design matrix by taking the stimulus indicator vector $\\mathbf{s}^{(c)}$ and shifting it down by $\\ell$ time steps, padding with zeros at the top. This is the standard method for constructing an FIR design matrix, and this derivation validates it from first principles.",
            "answer": "$$\\boxed{s^{(c)}_{t-\\ell} \\mathbf{1}_{t \\ge \\ell}}$$"
        },
        {
            "introduction": "Once a model has been specified and its parameters estimated, the primary goal is hypothesis testing. While a $t$-test assesses the contribution of a single regressor, scientific questions often involve the joint influence of multiple factors, such as testing for any task-related activity across several conditions. This requires an F-test, which is a powerful tool for testing such joint hypotheses. This practice demystifies the F-statistic by showing how it arises from comparing the goodness-of-fit of a full model against a simpler, \"restricted\" model where the null hypothesis is enforced, providing a concrete link between the statistical test and the concept of explained variance.",
            "id": "4199521",
            "problem": "Consider a single-voxel functional Magnetic Resonance Imaging (fMRI) time series modeled by the General Linear Model (GLM), where the observed data vector is $y \\in \\mathbb{R}^{T}$ and the design matrix is $X \\in \\mathbb{R}^{T \\times p}$. The GLM assumes $y = X\\beta + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{T})$, independent and identically distributed across time. The voxelâ€™s design matrix $X$ contains $p$ columns: two task regressors convolved with a canonical hemodynamic response function (columns $1$ and $2$), one parametric modulator (column $3$), an intercept (column $4$), and three head-motion nuisance regressors (columns $5$ through $7$). You are interested in testing the joint null hypothesis that the task-evoked responses are absent: $H_{0}: \\beta_{1} = 0$ and $\\beta_{2} = 0$, formulated as a general linear hypothesis $C\\beta = 0$.\n\nYour tasks are:\n- Construct an explicit contrast matrix $C$ that encodes $H_{0}: \\beta_{1} = 0$ and $\\beta_{2} = 0$ in the above GLM.\n- Starting from the assumptions of Gaussian noise and ordinary least squares estimation, derive the F-statistic that compares the restricted model (imposing $C\\beta = 0$) to the full model via the difference of residual sums of squares. Clearly state the degrees of freedom in the numerator and the denominator.\n- Use the following scientifically plausible values obtained from least squares fits at this voxel: number of scans $T = 240$, number of regressors $p = 7$, full-model residual sum of squares $\\mathrm{SSE}_{\\mathrm{full}} = 375.4$, and restricted-model residual sum of squares $\\mathrm{SSE}_{\\mathrm{rest}} = 401.0$. Evaluate the F-statistic implied by your derivation. Round your final numeric value to four significant figures. Express the final answer as a pure number with no units.",
            "solution": "The solution is divided into three parts as requested by the problem statement.\n\n#### 1. Construction of the Contrast Matrix $C$\n\nThe GLM is given by $y = X\\beta + \\varepsilon$, where the parameter vector $\\beta$ is a column vector in $\\mathbb{R}^{p}$. Given $p=7$, the parameter vector is $\\beta = [\\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5, \\beta_6, \\beta_7]^T$. The components of $\\beta$ correspond to the columns of the design matrix $X$ as specified.\n\nThe null hypothesis $H_0$ is that the task-evoked responses are absent, which is stated as $\\beta_1 = 0$ and $\\beta_2 = 0$. This represents a set of two simultaneous linear constraints on the parameter vector $\\beta$. We need to formulate this as a general linear hypothesis of the form $C\\beta=0$.\n\nThe matrix $C$ must be constructed such that the product $C\\beta$ yields a vector representing the expressions in the null hypothesis. Since there are two constraints, the matrix $C$ will have two rows. The number of columns must match the dimension of $\\beta$, which is $p=7$. Let the rank of $C$ be $q$.\n\nThe first constraint is $\\beta_1 = 0$. This can be written as the dot product of a vector with $\\beta$:\n$$1 \\cdot \\beta_1 + 0 \\cdot \\beta_2 + 0 \\cdot \\beta_3 + 0 \\cdot \\beta_4 + 0 \\cdot \\beta_5 + 0 \\cdot \\beta_6 + 0 \\cdot \\beta_7 = 0$$\nThis corresponds to the first row of $C$ being $[1, 0, 0, 0, 0, 0, 0]$.\n\nThe second constraint is $\\beta_2 = 0$. This can be written as:\n$$0 \\cdot \\beta_1 + 1 \\cdot \\beta_2 + 0 \\cdot \\beta_3 + 0 \\cdot \\beta_4 + 0 \\cdot \\beta_5 + 0 \\cdot \\beta_6 + 0 \\cdot \\beta_7 = 0$$\nThis corresponds to the second row of $C$ being $[0, 1, 0, 0, 0, 0, 0]$.\n\nCombining these two rows gives the contrast matrix $C$:\n$$C = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 & 0 \\end{pmatrix}$$\nThe rows of this matrix are linearly independent, so its rank is $q = 2$.\n\n#### 2. Derivation of the F-statistic\n\nThe F-statistic is used to compare a full model with a restricted model that is nested within it. The restricted model is obtained by imposing the null hypothesis $H_0: C\\beta=0$ on the full model $y = X\\beta + \\varepsilon$.\n\nThe full model has $p$ parameters. The Ordinary Least Squares (OLS) estimate of $\\beta$ is $\\hat{\\beta} = (X^T X)^{-1} X^T y$. The residual sum of squares for the full model is:\n$$\\mathrm{SSE}_{\\mathrm{full}} = \\|y - X\\hat{\\beta}\\|^2 = (y - X\\hat{\\beta})^T(y - X\\hat{\\beta})$$\nUnder the assumption that the errors $\\varepsilon$ are normally distributed, the normalized residual sum of squares follows a chi-squared distribution:\n$$\\frac{\\mathrm{SSE}_{\\mathrm{full}}}{\\sigma^2} \\sim \\chi^2_{T-p}$$\nThe degrees of freedom, $df_2 = T-p$, are the number of observations $T$ minus the number of parameters $p$ in the full model. The quantity $\\hat{\\sigma}^2 = \\frac{\\mathrm{SSE}_{\\mathrm{full}}}{T-p}$ is an unbiased estimator of the error variance $\\sigma^2$.\n\nThe restricted model is estimated under the $q$ linear constraints defined by $H_0: C\\beta=0$. The OLS estimation under these constraints yields an estimate $\\hat{\\beta}_{\\mathrm{rest}}$ and a corresponding residual sum of squares, $\\mathrm{SSE}_{\\mathrm{rest}} = \\|y - X\\hat{\\beta}_{\\mathrm{rest}}\\|^2$. By definition, since the restricted model is a special case of the full model, $\\mathrm{SSE}_{\\mathrm{rest}} \\ge \\mathrm{SSE}_{\\mathrm{full}}$.\n\nThe increase in the residual sum of squares due to the constraints is $\\mathrm{SSE}_{\\mathrm{rest}} - \\mathrm{SSE}_{\\mathrm{full}}$. It is a standard result of linear model theory that, under the null hypothesis, this difference, when normalized, also follows a chi-squared distribution, independent of $\\mathrm{SSE}_{\\mathrm{full}}$.\n$$\\frac{\\mathrm{SSE}_{\\mathrm{rest}} - \\mathrm{SSE}_{\\mathrm{full}}}{\\sigma^2} \\sim \\chi^2_q$$\nThe degrees of freedom, $df_1 = q$, are the number of linearly independent constraints imposed, which is the rank of the matrix $C$.\n\nThe F-statistic is defined as the ratio of two independent chi-squared random variables, each divided by its respective degrees of freedom.\n$$F = \\frac{(\\text{Sum of Squares for Hypothesis}) / (\\text{Degrees of Freedom for Hypothesis})}{(\\text{Residual Sum of Squares}) / (\\text{Residual Degrees of Freedom})}$$\nSubstituting the expressions derived above:\n$$F = \\frac{(\\mathrm{SSE}_{\\mathrm{rest}} - \\mathrm{SSE}_{\\mathrm{full}}) / q}{\\mathrm{SSE}_{\\mathrm{full}} / (T-p)}$$\nUnder the null hypothesis $H_0$, this statistic follows an F-distribution with $q$ and $T-p$ degrees of freedom, written as $F \\sim F_{q, T-p}$. The unknown variance term $\\sigma^2$ conveniently cancels from the ratio.\n\nThe degrees of freedom for this F-test are:\n-   Numerator degrees of freedom: $df_1 = q = \\mathrm{rank}(C)$.\n-   Denominator degrees of freedom: $df_2 = T - p$.\n\n#### 3. Evaluation of the F-statistic\n\nWe are given the following values:\n-   Number of scans $T = 240$.\n-   Number of regressors $p = 7$.\n-   Number of constraints $q = 2$ (from the rank of $C$).\n-   Full-model residual sum of squares $\\mathrm{SSE}_{\\mathrm{full}} = 375.4$.\n-   Restricted-model residual sum of squares $\\mathrm{SSE}_{\\mathrm{rest}} = 401.0$.\n\nFirst, we determine the degrees of freedom:\n-   Numerator degrees of freedom: $df_1 = q = 2$.\n-   Denominator degrees of freedom: $df_2 = T - p = 240 - 7 = 233$.\n\nNow, we substitute these values into the F-statistic formula:\n$$F = \\frac{(\\mathrm{SSE}_{\\mathrm{rest}} - \\mathrm{SSE}_{\\mathrm{full}}) / q}{\\mathrm{SSE}_{\\mathrm{full}} / (T-p)}$$\n$$F = \\frac{(401.0 - 375.4) / 2}{375.4 / (240 - 7)}$$\n$$F = \\frac{25.6 / 2}{375.4 / 233}$$\n$$F = \\frac{12.8}{1.611158798...}$$\n$$F \\approx 7.944592435...$$\n\nRounding the result to four significant figures gives $7.945$.",
            "answer": "$$\\boxed{7.945}$$"
        },
        {
            "introduction": "Effective fMRI analysis requires moving beyond the mechanics of the GLM to the art of model selection. A critical decision is the choice of basis functions used to model the HRF, a choice that embodies the fundamental bias-variance trade-off. Using a simple model (like a single canonical HRF) may introduce bias if the true response shape differs, while a complex model (like a large basis set) may have high variance and low predictive precision. This advanced practice challenges you to evaluate these trade-offs, exploring how model complexity, misspecification, and statistical regularization impact our ultimate goal: maximizing the power to detect neural activation.",
            "id": "4199519",
            "problem": "Consider a voxelwise General Linear Model (GLM) for functional Magnetic Resonance Imaging (fMRI) analysis with prewhitened noise, where the observed timeseries $y \\in \\mathbb{R}^{T}$ is modeled as $y = X \\beta + \\varepsilon$, with $X \\in \\mathbb{R}^{T \\times p}$ the design matrix, $\\beta \\in \\mathbb{R}^{p}$ the parameter vector, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{T})$. The regressors in $X$ are constructed by convolving the stimulus timing vector $u(t)$ with a Hemodynamic Response Function (HRF). Two modeling strategies are considered:\n\n- A single-regressor model $M_{c}$ with $X_{c} = [x_{c}]$, where $x_{c}$ is the stimulus convolved with a canonical HRF.\n- A multi-regressor model $M_{b}$ with $X_{b} = [x_{c}, x_{t}, x_{d}, \\dots]$, a larger HRF basis set including the canonical HRF and its temporal and dispersion derivatives (and potentially additional basis functions), all columns scaled to unit norm.\n\nAssume the true signal is $s = \\alpha x_{\\text{true}}$ with amplitude $\\alpha \\in \\mathbb{R}$, where $x_{\\text{true}}$ is the stimulus convolved with the true HRF. For small temporal misalignment $\\delta$, the true HRF is well-approximated locally by a first-order Taylor expansion, so that $x_{\\text{true}} \\approx x_{c} - \\delta x_{t}$ (up to a scaling constant absorbed into $\\alpha$). Detection is performed either by an amplitude-focused $t$-contrast $c$ (e.g., testing alignment to the canonical HRF) or by an omnibus $F$-test across all HRF basis coefficients. Bias is defined as $\\mathrm{bias}(c^{\\top}\\hat{\\beta}) = \\mathbb{E}[c^{\\top}\\hat{\\beta}] - c^{\\top}\\beta$, and the variance of a contrast is $\\mathrm{Var}(c^{\\top}\\hat{\\beta}) = \\sigma^{2} c^{\\top}(X^{\\top} X)^{-1} c$ under Ordinary Least Squares (OLS).\n\nEvaluate the bias-variance trade-off of using larger HRF basis sets versus a single canonical HRF in terms of detection power under model correctness and under misspecification. Select all statements that are true.\n\nA. If $x_{\\text{true}} = x_{c}$ (the canonical HRF is correct), augmenting $X$ to include HRF derivatives typically reduces the power of an amplitude-focused $t$-contrast aligned to the canonical HRF, because multicollinearity inflates $\\sigma^{2} c^{\\top}(X^{\\top}X)^{-1} c$ without reducing bias.\n\nB. If $x_{\\text{true}} \\approx x_{c} - \\delta x_{t}$ with small $\\delta \\neq 0$, using an omnibus $F$-test across the HRF basis in $M_{b}$ can increase detection power relative to $M_{c}$ by reducing misspecification bias via projecting $x_{\\text{true}}$ onto the span of $\\{x_{c}, x_{t}\\}$.\n\nC. Regardless of $x_{\\text{true}}$, a large finite impulse response (FIR) basis with many time-bins always increases power compared to a single canonical HRF regressor, because it spans the entire signal subspace and eliminates bias.\n\nD. As the number of HRF basis functions increases, the variance of estimated coefficients increases and the false-positive rate necessarily increases, because tests become more liberal with more parameters.\n\nE. With appropriately tuned Tikhonov (ridge) regularization applied to HRF basis coefficients, adding a larger HRF basis can improve detection power under misspecification by reducing variance while maintaining acceptable type I error control, relative to unregularized OLS with either $M_{c}$ or $M_{b}$.",
            "solution": "This problem asks us to evaluate five statements about the bias-variance trade-off when choosing an HRF basis set in an fMRI GLM.\n\n*   **Statement A: Correct.** If the true HRF matches the canonical HRF ($x_{\\text{true}} = x_c$), the single-regressor model ($M_c$) is correctly specified and the OLS estimate for its amplitude is unbiased. Augmenting the model with additional regressors (like the temporal derivative $x_t$) which are correlated with $x_c$ introduces multicollinearity. While the estimate for the canonical regressor's amplitude remains unbiased, its variance increases (proportional to $1/(1-\\rho^2)$, where $\\rho$ is the correlation between regressors). This increased variance leads to a smaller t-statistic and thus lower statistical power, without any benefit of bias reduction.\n\n*   **Statement B: Correct.** When the true HRF is misspecified by the canonical model (e.g., due to a time shift, $x_{\\text{true}} \\approx x_{c} - \\delta x_{t}$), the single-regressor model $M_c$ fails to capture the full signal. The uncaptured signal variance is absorbed into the model's residuals, inflating the estimate of the noise variance ($\\hat{\\sigma}^2$) and attenuating the estimated effect size. A more flexible model $M_b$ that includes the temporal derivative can better approximate the true signal shape. An omnibus $F$-test on the coefficients of $M_b$ tests for any signal explained by the basis set. By capturing more of the true signal (reducing misspecification bias) and providing a more accurate (smaller) estimate of the true error variance, this approach can substantially increase the non-centrality parameter of the test, leading to greater detection power.\n\n*   **Statement C: Incorrect.** The claim that a large FIR basis *always* increases power is false. While a flexible FIR basis can reduce or eliminate model misspecification bias, it does so at the cost of high dimensionality and multicollinearity, which significantly increases the variance of the parameter estimates. If the true HRF is simple and well-matched by the canonical HRF, the massive variance inflation and the degrees-of-freedom \"cost\" of the complex FIR model will lead to a substantial *loss* of power compared to the simple, correct model.\n\n*   **Statement D: Incorrect.** The variance of the estimated coefficients does typically increase with more basis functions due to multicollinearity. However, the claim that the false-positive rate *necessarily* increases is wrong. A properly conducted statistical test (like a t-test or F-test) controls the false-positive rate at the specified alpha level (e.g., 5%) by design. The critical value for the test statistic is derived from its null distribution, which accounts for the model's degrees of freedom. Adding more parameters makes the test more conservative in the sense that a larger statistic is required to reach significance, but it does not inherently make the test more \"liberal\" or increase the false-positive rate.\n\n*   **Statement E: Correct.** This statement accurately describes the utility of regularization. Unregularized OLS with a large basis set ($M_b$) can suffer from very high variance, reducing power. Tikhonov (ridge) regularization introduces a small amount of bias into the estimates to achieve a large reduction in their variance. Under model misspecification, this trade-off can be highly beneficial. Compared to the misspecified simple model ($M_c$), the regularized flexible model can better capture the true signal. Compared to the unregularized flexible model (OLS-$M_b$), it controls for variance inflation. With a well-tuned regularization parameter, this can lead to a superior bias-variance trade-off and improved detection power, while statistical methods exist to control the Type I error.",
            "answer": "$$\\boxed{ABE}$$"
        }
    ]
}