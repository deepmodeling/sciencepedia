{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any fMRI analysis using the General Linear Model (GLM) is the design matrix, $\\mathbf{X}$. This exercise provides a hands-on opportunity to construct this matrix from scratch for a powerful and flexible type of model, the Finite Impulse Response (FIR) model . By deriving the structure of the FIR design matrix from the mathematical definition of discrete-time convolution, you will solidify your understanding of how experimental events are translated into a set of predictors for the BOLD signal.",
            "id": "4199556",
            "problem": "You are analyzing a single-run event-related functional Magnetic Resonance Imaging (fMRI) experiment using the General Linear Model (GLM). Let $N$ denote the number of acquired volumes and let the Repetition Time (TR) be constant across the run. Consider one task condition $c$ whose onsets are encoded by a discrete-time stimulus indicator vector $\\mathbf{s}^{(c)} \\in \\{0,1\\}^{N}$ defined on the index set $\\{0,1,\\dots,N-1\\}$, where $s^{(c)}_{t} = 1$ if an onset of condition $c$ occurs at time index $t$ and $s^{(c)}_{t} = 0$ otherwise. Assume a linear time-invariant hemodynamic system so that the predicted Blood Oxygenation Level Dependent (BOLD) signal is given by the discrete-time convolution of the stimulus with the Hemodynamic Response Function (HRF). In a nonparametric approach, approximate the HRF as a Finite Impulse Response (FIR) model with $L$ time bins, meaning that the HRF is represented by $L$ unknown coefficients, one per lagged time bin within the window $\\{0,1,\\dots,L-1\\}$.\n\nStarting only from the discrete-time convolution definition and the causal, finite-support FIR assumption described above, define the FIR model for condition $c$ and construct the corresponding design submatrix $X^{(c)} \\in \\mathbb{R}^{N \\times L}$ that estimates the HRF shape nonparametrically. Your construction must be entry-wise, i.e., you must provide a closed-form analytic expression for the $(t,\\ell)$-entry of $X^{(c)}$ expressed solely in terms of the stimulus indicator vector $\\mathbf{s}^{(c)}$ and the indices $t \\in \\{0,1,\\dots,N-1\\}$ and $\\ell \\in \\{0,1,\\dots,L-1\\}$. You may use the discrete-time indicator function $\\mathbf{1}_{A}$, which equals $1$ if the condition $A$ holds and $0$ otherwise. Express your final answer as a single closed-form analytic expression with no units.",
            "solution": "The problem asks for the construction of the design submatrix for a single condition in an event-related fMRI experiment, where the hemodynamic response is modeled nonparametrically using a Finite Impulse Response (FIR) basis set. The construction must be derived from first principles, namely the discrete-time convolution of a stimulus indicator vector with an unknown FIR filter.\n\nLet $\\mathbf{y} \\in \\mathbb{R}^{N}$ be the measured Blood Oxygenation Level Dependent (BOLD) signal for a single voxel over $N$ time points, indexed by $t \\in \\{0, 1, \\dots, N-1\\}$. The General Linear Model (GLM) posits that this signal can be modeled as a linear combination of predictors plus noise:\n$$ \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} $$\nwhere $\\mathbf{X}$ is the design matrix, $\\boldsymbol{\\beta}$ is a vector of unknown parameters to be estimated, and $\\boldsymbol{\\epsilon}$ is a vector of residual errors.\n\nOur goal is to construct the specific part of the design matrix, $X^{(c)} \\in \\mathbb{R}^{N \\times L}$, that corresponds to a single task condition $c$. The associated parameters, $\\boldsymbol{\\beta}^{(c)} \\in \\mathbb{R}^{L}$, are the unknown coefficients of the FIR model for the Hemodynamic Response Function (HRF). The predicted BOLD signal due to condition $c$ is thus given by $\\mathbf{y}_{\\text{pred}}^{(c)} = X^{(c)} \\boldsymbol{\\beta}^{(c)}$.\n\nThe problem states that the system is linear and time-invariant (LTI). Therefore, the predicted BOLD signal at time $t$, denoted $(y_{\\text{pred}}^{(c)})_t$, is the discrete-time convolution of the stimulus indicator vector $\\mathbf{s}^{(c)}$ with the HRF, which we denote as $h$. The convolution is defined as:\n$$ (y_{\\text{pred}}^{(c)})_t = (\\mathbf{s}^{(c)} * h)_t = \\sum_{\\tau=0}^{N-1} s^{(c)}_{\\tau} h(t-\\tau) $$\nThe problem specifies a nonparametric FIR model for the HRF. This means the HRF is represented as a weighted sum of $L$ time-shifted impulses. Let the unknown weights (the heights of the HRF at different time lags) be the coefficients $\\beta^{(c)}_0, \\beta^{(c)}_1, \\dots, \\beta^{(c)}_{L-1}$. The HRF, $h(k)$, is defined for discrete time lags $k$. The FIR model assumes the HRF is causal and has a finite support of $L$ time bins, so $h(k)$ is non-zero only for $k \\in \\{0, 1, \\dots, L-1\\}$. We can write the HRF as:\n$$ h(k) = \\sum_{\\ell=0}^{L-1} \\beta^{(c)}_{\\ell} \\delta(k - \\ell) $$\nwhere $\\delta(m)$ is the discrete-time impulse or Kronecker delta function, which is $1$ if $m=0$ and $0$ otherwise. The parameter $\\beta^{(c)}_{\\ell}$ represents the value of the HRF at the $\\ell$-th time bin post-stimulus.\n\nWe substitute this FIR definition of $h(k)$ into the convolution equation. The argument to $h$ in the convolution sum is $(t-\\tau)$, so we replace $k$ with $(t-\\tau)$:\n$$ (y_{\\text{pred}}^{(c)})_t = \\sum_{\\tau=0}^{N-1} s^{(c)}_{\\tau} \\left( \\sum_{\\ell=0}^{L-1} \\beta^{(c)}_{\\ell} \\delta((t-\\tau) - \\ell) \\right) $$\nSince the summations are over finite ranges, we can exchange their order:\n$$ (y_{\\text{pred}}^{(c)})_t = \\sum_{\\ell=0}^{L-1} \\beta^{(c)}_{\\ell} \\left( \\sum_{\\tau=0}^{N-1} s^{(c)}_{\\tau} \\delta(t - \\tau - \\ell) \\right) $$\nThis expression is now in the form of a linear combination of the unknown parameters $\\beta^{(c)}_{\\ell}$. This matches the structure of the GLM. The $t$-th row of the matrix equation $\\mathbf{y}_{\\text{pred}}^{(c)} = X^{(c)} \\boldsymbol{\\beta}^{(c)}$ is:\n$$ (y_{\\text{pred}}^{(c)})_t = \\sum_{\\ell=0}^{L-1} X^{(c)}_{t,\\ell} \\beta^{(c)}_{\\ell} $$\nBy comparing this with the rearranged convolution equation, we can identify the entry $X^{(c)}_{t,\\ell}$ of the design submatrix:\n$$ X^{(c)}_{t,\\ell} = \\sum_{\\tau=0}^{N-1} s^{(c)}_{\\tau} \\delta(t - \\tau - \\ell) $$\nThe Kronecker delta term $\\delta(t - \\tau - \\ell)$ is non-zero (equal to $1$) only when its argument is zero, i.e., when $t - \\tau - \\ell = 0$, which implies $\\tau = t - \\ell$. For any other value of $\\tau$, the term is zero. Therefore, the summation over $\\tau$ collapses to a single term, evaluated at $\\tau = t - \\ell$, provided this value of $\\tau$ is within the summation bounds $[0, N-1]$.\n\nThe value of $\\tau$ is $t-\\ell$. The indices for the matrix entries are $t \\in \\{0, 1, \\dots, N-1\\}$ and $\\ell \\in \\{0, 1, \\dots, L-1\\}$.\nLet's check if $\\tau=t-\\ell$ is in the valid range for the index of $s^{(c)}_{\\tau}$.\n1.  Lower bound: $\\tau \\ge 0 \\implies t-\\ell \\ge 0 \\implies t \\ge \\ell$.\n2.  Upper bound: $\\tau \\le N-1 \\implies t-\\ell \\le N-1$. Since the maximum value of $t$ is $N-1$ and the minimum value of $\\ell$ is $0$, the maximum value of $t-\\ell$ is $(N-1)-0 = N-1$. Thus, the condition $t-\\ell \\le N-1$ is always satisfied for the given ranges of $t$ and $\\ell$.\n\nSo, if $t  \\ell$, the value $\\tau = t-\\ell$ is negative and falls outside the summation range. In this case, there is no $\\tau \\in [0, N-1]$ that makes the delta function non-zero, and the sum evaluates to $0$.\nIf $t \\ge \\ell$, the value $\\tau = t-\\ell$ falls within the summation range. The sum collapses to the single term $s^{(c)}_{t-\\ell} \\cdot 1 = s^{(c)}_{t-\\ell}$.\n\nWe can express this case distinction using the discrete-time indicator function $\\mathbf{1}_{A}$, which is $1$ if condition $A$ is true and $0$ otherwise. The condition for a non-zero entry is $t \\ge \\ell$.\nTherefore, the entry at row $t$ and column $\\ell$ of the design submatrix $X^{(c)}$ is given by:\n$$ X^{(c)}_{t,\\ell} = s^{(c)}_{t-\\ell} \\cdot \\mathbf{1}_{t \\ge \\ell} $$\nThis expression is the closed-form representation requested. It constructs each column $\\ell$ of the design matrix by taking the stimulus indicator vector $\\mathbf{s}^{(c)}$ and shifting it down by $\\ell$ time steps, padding with zeros at the top. This is the standard method for constructing an FIR design matrix, and this derivation validates it from first principles.",
            "answer": "$$\\boxed{s^{(c)}_{t-\\ell} \\mathbf{1}_{t \\ge \\ell}}$$"
        },
        {
            "introduction": "After constructing a design matrix, the next critical step is to understand what its associated parameters, $\\beta$, truly represent. This practice explores how a common preprocessing step, mean-centering a regressor, alters the interpretation of the model's coefficients . By deriving the relationship between centered and uncentered models, you will gain crucial insight into the meaning of the intercept term as a measure of baseline brain activity and appreciate the geometric concept of orthogonality in model design.",
            "id": "4199568",
            "problem": "You are analyzing a single-voxel preprocessed blood-oxygenation-level dependent (BOLD) time series in functional magnetic resonance imaging (fMRI), modeled with the general linear model (GLM). Let the observed time series be $\\mathbf{y} \\in \\mathbb{R}^{T}$, and consider a design with an intercept and a single task regressor $\\mathbf{r} \\in \\mathbb{R}^{T}$ constructed by convolving a binary stimulus $u$ with a canonical hemodynamic response function $h$. Assume additive noise $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{T}$ that is mean-zero, with finite variance, and appropriate preprocessing has rendered the errors well-approximated as independent and homoscedastic over time. You fit two GLMs to the same data:\n1. An uncentered model $\\mathbf{y} = \\beta_{0}\\mathbf{1} + \\beta_{1} \\mathbf{r} + \\boldsymbol{\\varepsilon}$.\n2. A mean-centered model $\\mathbf{y} = \\alpha_{0}\\mathbf{1} + \\alpha_{1} \\mathbf{r}_{c} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{r}_{c} = \\mathbf{r} - \\bar{r}\\,\\mathbf{1}$ and $\\bar{r} = \\frac{1}{T}\\mathbf{1}^{\\top} \\mathbf{r}$.\n\nStarting from the definitions of the GLM and the ordinary least squares criterion, derive from first principles how mean-centering $\\mathbf{r}$ changes the interpretation of the coefficients and affects the orthogonality structure with the intercept. In particular, show the orthogonality property between $\\mathbf{r}_{c}$ and the intercept $\\mathbf{1}$ and relate the parameterizations of the two GLMs by equating their fitted values. Your final task is to provide a single closed-form analytic expression for the mean-centered intercept coefficient $\\alpha_{0}$ in terms of the uncentered coefficients $\\beta_{0}$, $\\beta_{1}$, and the mean $\\bar{r}$.\n\nReport only the requested analytic expression for $\\alpha_{0}$ as the final answer. No numerical rounding is required.",
            "solution": "We begin with the general linear model (GLM) representation of the observed time series $\\mathbf{y} \\in \\mathbb{R}^{T}$:\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\mathbf{X}$ is the design matrix and $\\boldsymbol{\\beta}$ are the regression coefficients. In the uncentered model, the design matrix is $\\mathbf{X} = [\\mathbf{1}\\ \\ \\mathbf{r}]$ and the parameter vector is $\\boldsymbol{\\beta} = \\begin{pmatrix}\\beta_{0} \\\\ \\beta_{1}\\end{pmatrix}$, yielding\n$$\n\\mathbf{y} = \\beta_{0}\\mathbf{1} + \\beta_{1} \\mathbf{r} + \\boldsymbol{\\varepsilon}.\n$$\nIn the mean-centered model, we define\n$$\n\\mathbf{r}_{c} = \\mathbf{r} - \\bar{r}\\,\\mathbf{1}, \\quad \\text{with} \\quad \\bar{r} = \\frac{1}{T}\\mathbf{1}^{\\top} \\mathbf{r},\n$$\nand the design matrix becomes $\\mathbf{X}_{c} = [\\mathbf{1}\\ \\ \\mathbf{r}_{c}]$ with parameter vector $\\boldsymbol{\\alpha} = \\begin{pmatrix}\\alpha_{0} \\\\ \\alpha_{1}\\end{pmatrix}$, yielding\n$$\n\\mathbf{y} = \\alpha_{0}\\mathbf{1} + \\alpha_{1} \\mathbf{r}_{c} + \\boldsymbol{\\varepsilon}.\n$$\n\nFirst, we establish the orthogonality of the mean-centered regressor $\\mathbf{r}_{c}$ to the intercept $\\mathbf{1}$. Compute the inner product:\n$$\n\\mathbf{1}^{\\top} \\mathbf{r}_{c} = \\mathbf{1}^{\\top}(\\mathbf{r} - \\bar{r}\\,\\mathbf{1}) = \\mathbf{1}^{\\top} \\mathbf{r} - \\bar{r}\\,\\mathbf{1}^{\\top}\\mathbf{1}.\n$$\nBy definition, $\\bar{r} = \\frac{1}{T}\\mathbf{1}^{\\top} \\mathbf{r}$, so\n$$\n\\mathbf{1}^{\\top} \\mathbf{r}_{c} = \\mathbf{1}^{\\top} \\mathbf{r} - \\left(\\frac{1}{T}\\mathbf{1}^{\\top} \\mathbf{r}\\right) \\cdot T = \\mathbf{1}^{\\top} \\mathbf{r} - \\mathbf{1}^{\\top} \\mathbf{r} = 0.\n$$\nThus, $\\mathbf{r}_{c}$ is orthogonal to the intercept $\\mathbf{1}$ in the Euclidean inner product, a property that directly affects the normal equations in ordinary least squares and simplifies the interpretation of the intercept.\n\nNext, we relate the two parameterizations by expressing $\\mathbf{r}$ in terms of $\\mathbf{r}_{c}$:\n$$\n\\mathbf{r} = \\mathbf{r}_{c} + \\bar{r}\\,\\mathbf{1}.\n$$\nSubstitute this into the uncentered model to obtain the same fitted values expressed in the centered basis:\n$$\n\\beta_{0}\\mathbf{1} + \\beta_{1} \\mathbf{r}\n= \\beta_{0}\\mathbf{1} + \\beta_{1}(\\mathbf{r}_{c} + \\bar{r}\\,\\mathbf{1})\n= (\\beta_{0} + \\beta_{1}\\bar{r})\\mathbf{1} + \\beta_{1} \\mathbf{r}_{c}.\n$$\nSince the two models describe the same data and share the same error process, equality of fitted values requires matching coefficients on the common basis vectors $\\mathbf{1}$ and $\\mathbf{r}_{c}$:\n$$\n\\alpha_{0} = \\beta_{0} + \\beta_{1}\\bar{r}, \\quad \\alpha_{1} = \\beta_{1}.\n$$\n\nThis reparameterization shows the interpretational change induced by mean-centering. In the uncentered model, $\\beta_{0}$ is the predicted value of $\\mathbf{y}$ when $\\mathbf{r} = \\mathbf{0}$ (the baseline at zero regressor amplitude), while $\\beta_{1}$ scales the change in $\\mathbf{y}$ per unit change in $\\mathbf{r}$. After mean-centering, the orthogonality property $\\mathbf{1}^{\\top} \\mathbf{r}_{c} = 0$ implies that the ordinary least squares normal equations decouple the intercept from $\\mathbf{r}_{c}$, yielding an intercept estimate equal to the sample mean of $\\mathbf{y}$:\n$$\n\\alpha_{0} = \\bar{y}, \\quad \\text{where} \\quad \\bar{y} = \\frac{1}{T}\\mathbf{1}^{\\top} \\mathbf{y},\n$$\nunder the standard ordinary least squares solution with a centered design. Consistently, the slope remains unchanged, $\\alpha_{1} = \\beta_{1}$, because centering is a linear reparameterization that does not alter the direction in the column space contributing to $\\mathbf{r}$; it only shifts the allocation of shared variance between the intercept and the regressor.\n\nThe requested closed-form analytic expression for the mean-centered intercept $\\alpha_{0}$ in terms of uncentered parameters and the regressor mean is therefore\n$$\n\\alpha_{0} = \\beta_{0} + \\beta_{1}\\bar{r}.\n$$",
            "answer": "$$\\boxed{\\beta_{0}+\\beta_{1}\\,\\bar{r}}$$"
        },
        {
            "introduction": "The primary goal of the GLM is to test scientific hypotheses, which is accomplished by defining contrasts on the model parameters. This exercise guides you through the formulation and execution of an F-test, a powerful method for asking questions about the joint contribution of multiple regressors . You will translate a conceptual hypothesis into a formal contrast matrix and use the residual sums of squares from full and restricted models to compute the F-statistic, a cornerstone of statistical inference in fMRI.",
            "id": "4199521",
            "problem": "Consider a single-voxel functional Magnetic Resonance Imaging (fMRI) time series modeled by the General Linear Model (GLM), where the observed data vector is $\\mathbf{y} \\in \\mathbb{R}^{T}$ and the design matrix is $\\mathbf{X} \\in \\mathbb{R}^{T \\times p}$. The GLM assumes $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{T})$, independent and identically distributed across time. The voxelâ€™s design matrix $\\mathbf{X}$ contains $p$ columns: two task regressors convolved with a canonical hemodynamic response function (columns $1$ and $2$), one parametric modulator (column $3$), an intercept (column $4$), and three head-motion nuisance regressors (columns $5$ through $7$). You are interested in testing the joint null hypothesis that the task-evoked responses are absent: $H_{0}: \\beta_{1} = 0$ and $\\beta_{2} = 0$, formulated as a general linear hypothesis $C\\boldsymbol{\\beta} = \\mathbf{0}$.\n\nYour tasks are:\n- Construct an explicit contrast matrix $C$ that encodes $H_{0}: \\beta_{1} = 0$ and $\\beta_{2} = 0$ in the above GLM.\n- Starting from the assumptions of Gaussian noise and ordinary least squares estimation, derive the F-statistic that compares the restricted model (imposing $C\\boldsymbol{\\beta} = \\mathbf{0}$) to the full model via the difference of residual sums of squares. Clearly state the degrees of freedom in the numerator and the denominator.\n- Use the following scientifically plausible values obtained from least squares fits at this voxel: number of scans $T = 240$, number of regressors $p = 7$, full-model residual sum of squares $\\mathrm{SSE}_{\\mathrm{full}} = 375.4$, and restricted-model residual sum of squares $\\mathrm{SSE}_{\\mathrm{rest}} = 401.0$. Evaluate the F-statistic implied by your derivation. Round your final numeric value to four significant figures. Express the final answer as a pure number with no units.",
            "solution": "The solution is divided into three parts as requested by the problem statement.\n\n#### 1. Construction of the Contrast Matrix $C$\n\nThe GLM is given by $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where the parameter vector $\\boldsymbol{\\beta}$ is a column vector in $\\mathbb{R}^{p}$. Given $p=7$, the parameter vector is $\\boldsymbol{\\beta} = [\\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5, \\beta_6, \\beta_7]^T$. The components of $\\boldsymbol{\\beta}$ correspond to the columns of the design matrix $\\mathbf{X}$ as specified.\n\nThe null hypothesis $H_0$ is that the task-evoked responses are absent, which is stated as $\\beta_1 = 0$ and $\\beta_2 = 0$. This represents a set of two simultaneous linear constraints on the parameter vector $\\boldsymbol{\\beta}$. We need to formulate this as a general linear hypothesis of the form $C\\boldsymbol{\\beta}=\\mathbf{0}$.\n\nThe matrix $C$ must be constructed such that the product $C\\boldsymbol{\\beta}$ yields a vector representing the expressions in the null hypothesis. Since there are two constraints, the matrix $C$ will have two rows. The number of columns must match the dimension of $\\boldsymbol{\\beta}$, which is $p=7$. Let the rank of $C$ be $q$.\n\nThe first constraint is $\\beta_1 = 0$. This can be written as the dot product of a vector with $\\boldsymbol{\\beta}$:\n$$1 \\cdot \\beta_1 + 0 \\cdot \\beta_2 + 0 \\cdot \\beta_3 + 0 \\cdot \\beta_4 + 0 \\cdot \\beta_5 + 0 \\cdot \\beta_6 + 0 \\cdot \\beta_7 = 0$$\nThis corresponds to the first row of $C$ being $[1, 0, 0, 0, 0, 0, 0]$.\n\nThe second constraint is $\\beta_2 = 0$. This can be written as:\n$$0 \\cdot \\beta_1 + 1 \\cdot \\beta_2 + 0 \\cdot \\beta_3 + 0 \\cdot \\beta_4 + 0 \\cdot \\beta_5 + 0 \\cdot \\beta_6 + 0 \\cdot \\beta_7 = 0$$\nThis corresponds to the second row of $C$ being $[0, 1, 0, 0, 0, 0, 0]$.\n\nCombining these two rows gives the contrast matrix $C$:\n$$C = \\begin{pmatrix} 1  0  0  0  0  0  0 \\\\ 0  1  0  0  0  0  0 \\end{pmatrix}$$\nThe rows of this matrix are linearly independent, so its rank is $q = 2$.\n\n#### 2. Derivation of the F-statistic\n\nThe F-statistic is used to compare a full model with a restricted model that is nested within it. The restricted model is obtained by imposing the null hypothesis $H_0: C\\boldsymbol{\\beta}=\\mathbf{0}$ on the full model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$.\n\nThe full model has $p$ parameters. The Ordinary Least Squares (OLS) estimate of $\\boldsymbol{\\beta}$ is $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$. The residual sum of squares for the full model is:\n$$\\mathrm{SSE}_{\\mathrm{full}} = \\|\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|^2 = (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})$$\nUnder the assumption that the errors $\\boldsymbol{\\varepsilon}$ are normally distributed, the normalized residual sum of squares follows a chi-squared distribution:\n$$\\frac{\\mathrm{SSE}_{\\mathrm{full}}}{\\sigma^2} \\sim \\chi^2_{T-p}$$\nThe degrees of freedom, $df_2 = T-p$, are the number of observations $T$ minus the number of parameters $p$ in the full model. The quantity $\\hat{\\sigma}^2 = \\frac{\\mathrm{SSE}_{\\mathrm{full}}}{T-p}$ is an unbiased estimator of the error variance $\\sigma^2$.\n\nThe restricted model is estimated under the $q$ linear constraints defined by $H_0: C\\boldsymbol{\\beta}=\\mathbf{0}$. The OLS estimation under these constraints yields an estimate $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{rest}}$ and a corresponding residual sum of squares, $\\mathrm{SSE}_{\\mathrm{rest}} = \\|\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\mathrm{rest}}\\|^2$. By definition, since the restricted model is a special case of the full model, $\\mathrm{SSE}_{\\mathrm{rest}} \\ge \\mathrm{SSE}_{\\mathrm{full}}$.\n\nThe increase in the residual sum of squares due to the constraints is $\\mathrm{SSE}_{\\mathrm{rest}} - \\mathrm{SSE}_{\\mathrm{full}}$. It is a standard result of linear model theory that, under the null hypothesis, this difference, when normalized, also follows a chi-squared distribution, independent of $\\mathrm{SSE}_{\\mathrm{full}}$.\n$$\\frac{\\mathrm{SSE}_{\\mathrm{rest}} - \\mathrm{SSE}_{\\mathrm{full}}}{\\sigma^2} \\sim \\chi^2_q$$\nThe degrees of freedom, $df_1 = q$, are the number of linearly independent constraints imposed, which is the rank of the matrix $C$.\n\nThe F-statistic is defined as the ratio of two independent chi-squared random variables, each divided by its respective degrees of freedom.\n$$F = \\frac{(\\text{Sum of Squares for Hypothesis}) / (\\text{Degrees of Freedom for Hypothesis})}{(\\text{Residual Sum of Squares}) / (\\text{Residual Degrees of Freedom})}$$\nSubstituting the expressions derived above:\n$$F = \\frac{(\\mathrm{SSE}_{\\mathrm{rest}} - \\mathrm{SSE}_{\\mathrm{full}}) / q}{\\mathrm{SSE}_{\\mathrm{full}} / (T-p)}$$\nUnder the null hypothesis $H_0$, this statistic follows an F-distribution with $q$ and $T-p$ degrees of freedom, written as $F \\sim F_{q, T-p}$. The unknown variance term $\\sigma^2$ conveniently cancels from the ratio.\n\nThe degrees of freedom for this F-test are:\n-   Numerator degrees of freedom: $df_1 = q = \\mathrm{rank}(C)$.\n-   Denominator degrees of freedom: $df_2 = T - p$.\n\n#### 3. Evaluation of the F-statistic\n\nWe are given the following values:\n-   Number of scans $T = 240$.\n-   Number of regressors $p = 7$.\n-   Number of constraints $q = 2$ (from the rank of $C$).\n-   Full-model residual sum of squares $\\mathrm{SSE}_{\\mathrm{full}} = 375.4$.\n-   Restricted-model residual sum of squares $\\mathrm{SSE}_{\\mathrm{rest}} = 401.0$.\n\nFirst, we determine the degrees of freedom:\n-   Numerator degrees of freedom: $df_1 = q = 2$.\n-   Denominator degrees of freedom: $df_2 = T - p = 240 - 7 = 233$.\n\nNow, we substitute these values into the F-statistic formula:\n$$F = \\frac{(\\mathrm{SSE}_{\\mathrm{rest}} - \\mathrm{SSE}_{\\mathrm{full}}) / q}{\\mathrm{SSE}_{\\mathrm{full}} / (T-p)}$$\n$$F = \\frac{(401.0 - 375.4) / 2}{375.4 / (240 - 7)}$$\n$$F = \\frac{25.6 / 2}{375.4 / 233}$$\n$$F = \\frac{12.8}{1.611158798...}$$\n$$F \\approx 7.944592435...$$\n\nRounding the result to four significant figures gives $7.945$.",
            "answer": "$$\\boxed{7.945}$$"
        }
    ]
}