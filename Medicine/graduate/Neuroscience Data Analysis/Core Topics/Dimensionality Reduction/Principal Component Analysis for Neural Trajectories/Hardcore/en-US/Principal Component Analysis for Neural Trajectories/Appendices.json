{
    "hands_on_practices": [
        {
            "introduction": "Before applying Principal Component Analysis to neural spike counts, the data must be properly prepared. Raw spike counts, often modeled by a Poisson distribution, exhibit a mean-variance relationship that can bias PCA towards high-firing-rate neurons regardless of their dynamic modulation. This exercise guides you through building and justifying an essential preprocessing pipeline that first stabilizes variance with a square-root transform and then centers the data, ensuring that PCA reveals the underlying structure of neural covariation .",
            "id": "4187598",
            "problem": "You are given spike count matrices representing simultaneously recorded neurons across discrete time bins, intended for dimensionality reduction and neural trajectory analysis using Principal Component Analysis (PCA). Construct a deterministic pipeline that first applies a variance stabilization via the square-root transform to the spike counts, then performs per-neuron centering across time, and finally computes summary statistics that justify the order of operations mathematically. The design must start from the following fundamental base: for a Poisson-distributed random variable $X$ with rate parameter $\\lambda$, one has $\\mathbb{E}[X] = \\lambda$ and $\\operatorname{Var}(X) = \\lambda$, and for a smooth function $g$ evaluated at $X$, the Delta Method gives $\\operatorname{Var}(g(X)) \\approx \\left(g'(\\lambda)\\right)^2 \\operatorname{Var}(X)$ for sufficiently large $\\lambda$. Also, PCA expects centered features: given a data matrix $D$ with features as columns and observations as rows, PCA is defined on the covariance of the zero-mean features, and its principal components capture orthogonal directions of maximal variance.\n\nYour program must implement the following pipeline for each test case matrix $X \\in \\mathbb{N}_0^{N \\times T}$:\n\n- Variance stabilization via square-root transform with a nonnegative offset: compute $Y_{i,t} = \\sqrt{X_{i,t} + \\alpha}$ for all neurons $i \\in \\{1,\\dots,N\\}$ and time bins $t \\in \\{1,\\dots,T\\}$, using $\\alpha = \\tfrac{1}{2}$. This step is intended to reduce the dependence of the variance on the mean under the Poisson model.\n- Per-neuron centering across time: compute $\\bar{y}_i = \\tfrac{1}{T} \\sum_{t=1}^T Y_{i,t}$ and define the centered data $Z_{i,t} = Y_{i,t} - \\bar{y}_i$ for all $i$ and $t$.\n- Pre- and post-transform mean-variance correlation: for the raw counts, compute per-neuron means $\\mu_i = \\tfrac{1}{T} \\sum_{t=1}^T X_{i,t}$ and per-neuron sample variances $\\sigma_i^2 = \\tfrac{1}{T-1} \\sum_{t=1}^T (X_{i,t} - \\mu_i)^2$, then compute the Pearson correlation $r_{\\text{pre}}$ between the vectors $(\\mu_i)_{i=1}^N$ and $(\\sigma_i^2)_{i=1}^N$. For the transformed counts, compute $\\tilde{\\mu}_i = \\tfrac{1}{T} \\sum_{t=1}^T Y_{i,t}$ and $\\tilde{\\sigma}_i^2 = \\tfrac{1}{T-1} \\sum_{t=1}^T (Y_{i,t} - \\tilde{\\mu}_i)^2$, then compute the Pearson correlation $r_{\\text{post}}$ between $(\\tilde{\\mu}_i)_{i=1}^N$ and $(\\tilde{\\sigma}_i^2)_{i=1}^N$. If either vector has zero sample standard deviation or if $N  2$, define the correlation value by convention to be $0.0$.\n- Centering verification: verify that per-neuron means of $Z$ are numerically zero within tolerance $\\epsilon = 10^{-12}$; output a boolean indicating whether $\\max_{i} \\left| \\tfrac{1}{T} \\sum_{t=1}^T Z_{i,t} \\right| \\le \\epsilon$.\n- PCA explained variance ratio sum: treat time bins as observations and neurons as features by transposing $Z$ to obtain $Z^\\top \\in \\mathbb{R}^{T \\times N}$, apply Singular Value Decomposition to $Z^\\top$ to get singular values $(s_j)_{j=1}^{m}$ with $m = \\min(N, T)$, define eigenvalues $\\lambda_j = \\tfrac{s_j^2}{T-1}$, and compute the explained variance ratios $\\rho_j = \\tfrac{\\lambda_j}{\\sum_{k=1}^{m} \\lambda_k}$. Return the sum $\\sum_{j=1}^{k} \\rho_j$ for $k = 3$. If $\\sum_{k=1}^{m} \\lambda_k = 0$, define this sum to be $0.0$ by convention. If $m  3$, sum only over the available components.\n\nMathematically justify in your code comments and solution why the square-root transform precedes centering: centering raw counts can yield negative values, making the square-root inapplicable, and it breaks the Poisson structure needed for variance stabilization arguments. Also justify that PCA requires zero-mean features, which is achieved by centering after the variance-stabilizing transform.\n\nTest Suite. Your program must evaluate the following four test cases, each given as an explicit $N \\times T$ integer matrix:\n\n- Test case $1$ ($N = 4$, $T = 6$):\n  $$\n  X^{(1)} = \\begin{pmatrix}\n  0  1  0  2  1  0 \\\\\n  5  7  6  8  7  6 \\\\\n  0  0  0  0  1  0 \\\\\n  2  3  4  3  2  3\n  \\end{pmatrix}.\n  $$\n- Test case $2$ ($N = 3$, $T = 5$):\n  $$\n  X^{(2)} = \\begin{pmatrix}\n  0  0  0  0  0 \\\\\n  0  0  0  0  0 \\\\\n  0  0  0  0  0\n  \\end{pmatrix}.\n  $$\n- Test case $3$ ($N = 5$, $T = 7$):\n  $$\n  X^{(3)} = \\begin{pmatrix}\n  0  0  0  1  0  2  0 \\\\\n  10  12  11  9  13  12  11 \\\\\n  1  3  2  4  3  2  1 \\\\\n  0  5  0  5  0  5  0 \\\\\n  20  19  18  21  22  20  19\n  \\end{pmatrix}.\n  $$\n- Test case $4$ ($N = 6$, $T = 3$):\n  $$\n  X^{(4)} = \\begin{pmatrix}\n  0  1  0 \\\\\n  9  10  11 \\\\\n  0  0  0 \\\\\n  3  3  3 \\\\\n  1  2  3 \\\\\n  5  1  0\n  \\end{pmatrix}.\n  $$\n\nFinal Output Format. Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each element is itself a list of the form $[r_{\\text{pre}}, r_{\\text{post}}, \\text{centered\\_ok}, \\sum_{j=1}^{k} \\rho_j]$ with $k = 3$. For example, the output must look like $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]$, where $a_\\ell$ and $b_\\ell$ are floats, $c_\\ell$ is a boolean, and $d_\\ell$ is a float for test case $\\ell \\in \\{1,2,3,4\\}$.",
            "solution": "The problem requires the construction and justification of a deterministic data preprocessing pipeline for neural spike count matrices, which are intended for analysis using Principal Component Analysis (PCA). The pipeline involves a variance-stabilizing transform followed by data centering. We will first provide a rigorous justification for the design of this pipeline, grounded in statistical principles, and then outline the specific computational steps.\n\nA central premise in analyzing neural spike counts is the Poisson model. For a neuron firing with an average rate $\\lambda$ in a given time bin, the spike count $X$ can be modeled as a Poisson-distributed random variable, $X \\sim \\text{Poisson}(\\lambda)$. A key property of this distribution is that the mean is equal to the variance: $\\mathbb{E}[X] = \\lambda$ and $\\operatorname{Var}(X) = \\lambda$. This coupling of mean and variance is problematic for many standard statistical methods, including PCA, which are sensitive to the relative scaling of different features (neurons). A neuron with a higher mean firing rate will also have a higher variance, potentially dominating the analysis irrespective of its contribution to the underlying neural dynamics.\n\nThe first principle of our pipeline is therefore **variance stabilization**. The goal is to transform the data such that the variance of the transformed variable is approximately independent of its mean. The problem statement introduces the Delta Method for a smooth function $g$ applied to a random variable $X$: $\\operatorname{Var}(g(X)) \\approx (g'(\\mathbb{E}[X]))^2 \\operatorname{Var}(X)$. For our Poisson data $X$ with $\\mathbb{E}[X] = \\lambda$, this becomes $\\operatorname{Var}(g(X)) \\approx (g'(\\lambda))^2 \\lambda$.\nThe specified transformation is $g(x) = \\sqrt{x+\\alpha}$ with the constant offset $\\alpha = \\frac{1}{2}$. The derivative is $g'(x) = \\frac{1}{2\\sqrt{x+\\alpha}}$. Substituting this into the Delta Method approximation gives:\n$$\n\\operatorname{Var}(\\sqrt{X+\\alpha}) \\approx \\left(\\frac{1}{2\\sqrt{\\lambda+\\alpha}}\\right)^2 \\lambda = \\frac{\\lambda}{4(\\lambda+\\alpha)}\n$$\nFor a sufficiently large mean firing rate $\\lambda$, the term $\\lambda+\\alpha \\approx \\lambda$, and the variance of the transformed variable approaches a constant:\n$$\n\\lim_{\\lambda \\to \\infty} \\frac{\\lambda}{4(\\lambda+\\alpha)} = \\frac{1}{4}\n$$\nThus, the square-root transform with offset $\\alpha=\\frac{1}{2}$ (a specific form known as the Anscombe transform, which has favorable properties for small $\\lambda$) decouples the variance from the mean. The effectiveness of this step is empirically measured by comparing the Pearson correlation between per-neuron means and variances before ($r_{\\text{pre}}$) and after ($r_{\\text{post}}$) the transformation. We expect $r_{\\text{pre}}$ to be high and positive, while $r_{\\text{post}}$ should be close to zero.\n\nThe second principle is **data centering for PCA**. PCA aims to find an orthogonal basis for the data that captures directions of maximal variance. These directions are the eigenvectors of the data's covariance matrix. The sample covariance matrix for a set of features is fundamentally defined in terms of deviations from the mean of each feature. If PCA is applied to uncentered data, the first principal component is often dominated by the vector from the origin to the center of mass (the mean) of the data cloud, which is typically uninformative about the structure of variations within the data. Therefore, to ensure that PCA captures the intrinsic variance structure, each feature (neuron) must be centered to have a mean of zero before the analysis.\n\nThe integration of these principles dictates the **order of operations**. The variance-stabilizing transform must be applied before centering. There are two primary reasons for this ordering:\n1.  **Domain constraints**: The raw spike counts, $X_{i,t}$, are non-negative integers. Centering them directly via $X_{i,t} - \\mu_i$ would produce negative values. The square-root transform, $g(x) = \\sqrt{x+\\alpha}$, is not defined over the real numbers for negative arguments (if $x+\\alpha  0$), making this order of operations invalid.\n2.  **Statistical validity**: The justification for the square-root transform is predicated on the data following a Poisson distribution. The operation of centering alters the distribution of the data; the resulting values are no longer Poisson-distributed. Applying the transform after centering would invalidate the statistical reasoning upon which it is based.\nConsequently, the correct and principled pipeline is: $1$) apply the variance-stabilizing square-root transform to the raw counts, and then $2$) center the transformed data on a per-neuron basis.\n\nThe algorithmic pipeline for a given spike count matrix $X \\in \\mathbb{N}_0^{N \\times T}$ is as follows:\n1.  **Pre-transform Analysis**: For each neuron $i \\in \\{1, \\dots, N\\}$, compute the sample mean $\\mu_i = \\frac{1}{T}\\sum_{t=1}^T X_{i,t}$ and sample variance $\\sigma_i^2 = \\frac{1}{T-1}\\sum_{t=1}^T(X_{i,t} - \\mu_i)^2$. Calculate the Pearson correlation coefficient, $r_{\\text{pre}}$, between the vector of means $(\\mu_i)_{i=1}^N$ and the vector of variances $(\\sigma_i^2)_{i=1}^N$.\n2.  **Variance Stabilization**: Compute the transformed matrix $Y$ where $Y_{i,t} = \\sqrt{X_{i,t} + \\alpha}$ for $\\alpha=\\frac{1}{2}$.\n3.  **Post-transform Analysis**: For each neuron $i$, compute the new sample mean $\\tilde{\\mu}_i = \\frac{1}{T}\\sum_{t=1}^T Y_{i,t}$ and variance $\\tilde{\\sigma}_i^2 = \\frac{1}{T-1}\\sum_{t=1}^T(Y_{i,t} - \\tilde{\\mu}_i)^2$. Calculate the correlation $r_{\\text{post}}$ between the vectors $(\\tilde{\\mu}_i)_{i=1}^N$ and $(\\tilde{\\sigma}_i^2)_{i=1}^N$.\n4.  **Centering and Verification**: Compute the centered matrix $Z$ where $Z_{i,t} = Y_{i,t} - \\tilde{\\mu}_i$. Verify that the row means of $Z$ are numerically zero by checking if $\\max_i |\\frac{1}{T}\\sum_{t=1}^T Z_{i,t}| \\le \\epsilon$ for a small tolerance $\\epsilon=10^{-12}$.\n5.  **PCA Explained Variance**: PCA is performed on the data matrix where neurons are features and time points are observations. This corresponds to the transpose of our centered matrix, $Z^\\top \\in \\mathbb{R}^{T \\times N}$. The principal component variances are the eigenvalues of the sample covariance matrix $C = \\frac{1}{T-1}(Z^\\top)^\\top Z^\\top = \\frac{1}{T-1} Z Z^\\top$. A computationally stable way to find these eigenvalues is via Singular Value Decomposition (SVD) of $Z^\\top$. If the singular values of $Z^\\top$ are $(s_j)_{j=1}^m$ where $m=\\min(N,T)$, then the eigenvalues of $C$ are given by $\\lambda_j = \\frac{s_j^2}{T-1}$. The explained variance ratio for the $j$-th component is $\\rho_j = \\lambda_j / \\sum_{k=1}^m \\lambda_k$. The final required statistic is the cumulative sum of these ratios for the top $k=3$ components, $\\sum_{j=1}^{\\min(3, m)} \\rho_j$.\n\nThis sequence of operations constitutes a complete, self-contained, and scientifically justified pipeline for preprocessing neural spike count data for dimensionality reduction.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a deterministic pipeline for preprocessing neural spike count data.\n    The pipeline includes variance stabilization, centering, and PCA-related summary statistics.\n    \"\"\"\n    \n    # Test cases are defined as N x T integer matrices, where N is the number of neurons\n    # and T is the number of time bins.\n    test_cases = [\n        np.array([\n            [0, 1, 0, 2, 1, 0],\n            [5, 7, 6, 8, 7, 6],\n            [0, 0, 0, 0, 1, 0],\n            [2, 3, 4, 3, 2, 3]\n        ], dtype=int),\n        np.array([\n            [0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0]\n        ], dtype=int),\n        np.array([\n            [0, 0, 0, 1, 0, 2, 0],\n            [10, 12, 11, 9, 13, 12, 11],\n            [1, 3, 2, 4, 3, 2, 1],\n            [0, 5, 0, 5, 0, 5, 0],\n            [20, 19, 18, 21, 22, 20, 19]\n        ], dtype=int),\n        np.array([\n            [0, 1, 0],\n            [9, 10, 11],\n            [0, 0, 0],\n            [3, 3, 3],\n            [1, 2, 3],\n            [5, 1, 0]\n        ], dtype=int),\n    ]\n\n    results = []\n    \n    # Define constants from the problem statement.\n    alpha = 0.5\n    epsilon = 1e-12\n    k = 3\n\n    def calculate_correlation(vec1, vec2):\n        \"\"\"\n        Calculates the Pearson correlation between two vectors.\n        Handles edge cases as per problem description (N  2 or constant vectors).\n        \"\"\"\n        # The length of the vectors is N, the number of neurons.\n        if vec1.shape[0]  2:\n            return 0.0\n        \n        # If either vector is constant, its standard deviation is 0.\n        # This makes Pearson correlation undefined. The problem specifies to return 0.0.\n        if np.std(vec1) == 0.0 or np.std(vec2) == 0.0:\n            return 0.0\n        \n        # np.corrcoef is a standard way to compute the correlation coefficient.\n        # It returns a 2x2 matrix, with the correlation at [0, 1] and [1, 0].\n        corr_matrix = np.corrcoef(vec1, vec2)\n        return corr_matrix[0, 1]\n\n    for X in test_cases:\n        N, T = X.shape\n\n        # The order of operations is critical: transform first, then center.\n        # 1. Transform: The square-root transform is based on the data's\n        #    (assumed) Poisson statistics. Applying it to raw counts is correct.\n        # 2. Center: PCA requires zero-mean features. This is done after the\n        #    transform. Centering first would create negative numbers, making the\n        #    square-root transform ill-defined for real-valued outputs and break\n        #    the statistical assumptions for variance stabilization.\n\n        # --- Pre- and post-transform mean-variance correlation ---\n        \n        # Calculate stats for raw counts (pre-transform)\n        # Handle case T=1 for variance calculation to avoid division by zero.\n        if T  1:\n            mu_pre = np.mean(X, axis=1)\n            var_pre = np.var(X, axis=1, ddof=1) # Use sample variance (ddof=1)\n            r_pre = calculate_correlation(mu_pre, var_pre)\n        else: # If T=1, variance is undefined or 0.\n            mu_pre = X.flatten()\n            var_pre = np.zeros_like(mu_pre)\n            r_pre = 0.0\n        \n        # Step 1: Variance stabilization via square-root transform\n        Y = np.sqrt(X + alpha)\n\n        # Calculate stats for transformed counts (post-transform)\n        if T  1:\n            mu_post = np.mean(Y, axis=1)\n            var_post = np.var(Y, axis=1, ddof=1)\n            r_post = calculate_correlation(mu_post, var_post)\n        else:\n            mu_post = Y.flatten()\n            var_post = np.zeros_like(mu_post)\n            r_post = 0.0\n\n        # Step 2: Per-neuron centering across time\n        # keepdims=True ensures that the mean vector (N, 1) correctly broadcasts\n        # for subtraction from the data matrix (N, T).\n        neuron_means_Y = np.mean(Y, axis=1, keepdims=True)\n        Z = Y - neuron_means_Y\n\n        # --- Centering verification ---\n        Z_row_means = np.mean(Z, axis=1)\n        centered_ok = np.max(np.abs(Z_row_means)) = epsilon\n\n        # --- PCA explained variance ratio sum ---\n        # PCA operates on a matrix of (observations x features). Here, that is (time x neurons).\n        # Thus, we use the transpose of Z.\n        D = Z.T\n        m = min(N, T)\n\n        sum_evr = 0.0\n        # SVD and eigenvalue calculation require T  1.\n        if T  1:\n            # Singular Value Decomposition is a stable way to get principal component variances.\n            s = np.linalg.svd(D, compute_uv=False)\n            \n            # Eigenvalues of the covariance matrix are related to the singular values.\n            # lambda_j = s_j^2 / (T-1)\n            lambdas = (s**2) / (T - 1)\n            \n            total_lambda = np.sum(lambdas)\n            \n            # If total variance is zero, the ratio is defined as 0.\n            if total_lambda  0:\n                explained_variance_ratios = lambdas / total_lambda\n                \n                # Sum the ratios for the top k components, or fewer if not available.\n                num_components_to_sum = min(k, m)\n                sum_evr = np.sum(explained_variance_ratios[:num_components_to_sum])\n\n        results.append([r_pre, r_post, centered_ok, sum_evr])\n\n    # Final print statement must match the exact specified format.\n    # The str() of a list gives its representation, e.g., '[1, 2, 3]',\n    # which is what's needed here.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "When analyzing neural data from experiments with repeated trials, a critical decision is how to structure the data for PCA. Should you average across trials to estimate a single, 'clean' trajectory, or stack trials to analyze all the data at once? This choice has profound consequences for the interpretation of the resulting principal components. This practice delves into the statistical trade-offs, exploring how each approach either isolates the shared, task-driven dynamics or additionally captures meaningful trial-to-trial variability in the neural population .",
            "id": "4187767",
            "problem": "Consider a population recording with $N$ simultaneously measured neurons over $T$ aligned time points per trial and $R$ repeated trials of the same condition. For trial $r \\in \\{1,\\dots,R\\}$, let $X^{(r)} \\in \\mathbb{R}^{N \\times T}$ denote the neuron-by-time activity matrix, with column $t$ given by $x_{r,t} \\in \\mathbb{R}^{N}$. Suppose a generative model where, for each trial $r$ and time $t$, the activity decomposes as $x_{r,t} = \\mu_{t} + \\delta_{r} + \\epsilon_{r,t}$, where $\\mu_{t} \\in \\mathbb{R}^{N}$ is a time-varying condition mean trajectory shared across trials, $\\delta_{r} \\in \\mathbb{R}^{N}$ is a trial-specific, time-invariant offset (shared variability across neurons that differs between trials), and $\\epsilon_{r,t} \\in \\mathbb{R}^{N}$ is zero-mean residual noise with finite second moments. Assume $\\{\\delta_{r}\\}$ are independent and identically distributed across $r$, independent of $\\epsilon_{r,t}$ and of $t$ (time-invariant per trial), and $\\{\\epsilon_{r,t}\\}$ are independent across trials and time with $\\mathbb{E}[\\epsilon_{r,t}] = 0$ and covariance $\\Sigma_{\\epsilon}$ that may be neuron-correlated but does not depend on $r$ or $t$.\n\nYou wish to visualize low-dimensional neural trajectories using Principal Component Analysis (PCA) by treating columns as samples and neurons as variables. Consider two preprocessing strategies before applying PCA:\n\n1. Stacking: form $X_{\\mathrm{stack}} = [X^{(1)}, X^{(2)}, \\dots, X^{(R)}] \\in \\mathbb{R}^{N \\times (T \\cdot R)}$ by concatenating trials along time, then mean-center across the $T \\cdot R$ samples and compute the sample covariance across neurons.\n\n2. Averaging: form $X_{\\mathrm{avg}} = \\frac{1}{R} \\sum_{r=1}^{R} X^{(r)} \\in \\mathbb{R}^{N \\times T}$ by averaging across trials at each time point, then mean-center across the $T$ samples and compute the sample covariance across neurons.\n\nStarting from the definition that PCA chooses orthonormal directions in neuron space that maximize projected sample variance (equivalently, eigenvectors of the mean-centered sample covariance matrix), and using the law of total variance and covariance, analyze the impact of these two strategies on the variance structure available to PCA and the ability to capture trial-to-trial variability. Select all statements that are correct.\n\nA. In $X_{\\mathrm{avg}}$, principal components cannot capture the trial-specific variability term $\\delta_{r}$ beyond its contribution to the overall mean, so trial-to-trial variability is largely suppressed; in contrast, $X_{\\mathrm{stack}}$ can reveal low-dimensional shared variability across trials via components spanning the subspace generated by $\\{\\delta_{r}\\}_{r=1}^{R}$.\n\nB. If one mean-centers $X_{\\mathrm{stack}}$ across all $T \\cdot R$ samples, trial-to-trial variability $\\delta_{r}$ is eliminated by construction, making the variance structure equivalent to that of $X_{\\mathrm{avg}}$.\n\nC. Under the assumption that $\\epsilon_{r,t}$ are independent across trials with $\\mathbb{E}[\\epsilon_{r,t}] = 0$, averaging across trials reduces the contribution of $\\epsilon_{r,t}$ to the sample covariance by approximately a factor of $1/R$, thereby stabilizing components that reflect $\\mu_{t}$, but it simultaneously discards between-trial variance from $\\delta_{r}$.\n\nD. If trial-to-trial variability is purely a scalar gain $g_{r} \\in \\mathbb{R}$ multiplying the time course, i.e., $x_{r,t} = g_{r} \\mu_{t} + \\epsilon_{r,t}$, then $X_{\\mathrm{stack}}$ will tend to produce a dominant component aligned with $\\mu_{t}$ and an additional component aligned with a gain direction orthogonal to $\\mu_{t}$, whereas $X_{\\mathrm{avg}}$ will largely eliminate the gain variability.\n\nE. When temporal alignment errors exist across trials (e.g., variable latencies), stacking necessarily mixes time shifts into principal components and degrades trajectory estimation, while averaging perfectly preserves the true $\\mu_{t}$ without distortion.\n\nF. Under the model $x_{r,t} = \\mu_{t} + \\delta_{r} + \\epsilon_{r,t}$ with time-invariant $\\delta_{r}$, the neuron-space sample covariance computed from $X_{\\mathrm{stack}}$ decomposes into a sum of a temporal covariance term from $\\mu_{t}$, a between-trial covariance term from $\\delta_{r}$, and a residual noise term. The between-trial term induces principal components whose projections are constant across time within each trial (up to noise), producing trial-dependent vertical shifts in the projected trajectories.",
            "solution": "The problem statement is a valid exercise in the statistical analysis of neural data. It is scientifically grounded, well-posed, and uses precise, objective language. The generative model is a standard framework for studying trial-structured neural recordings, and the comparison between stacking and averaging trials before applying Principal Component Analysis (PCA) is a fundamental methodological question in computational neuroscience. I will now proceed with a full derivation and analysis.\n\nThe problem asks to analyze two preprocessing strategies for applying PCA to a set of $R$ trials of neural data, where each trial consists of recordings from $N$ neurons over $T$ time points. The generative model for the neural activity vector $x_{r,t} \\in \\mathbb{R}^{N}$ at time $t$ of trial $r$ is given by:\n$$x_{r,t} = \\mu_{t} + \\delta_{r} + \\epsilon_{r,t}$$\nwhere $\\mu_t$ is the shared mean trajectory, $\\delta_r$ is a trial-specific, time-invariant offset, and $\\epsilon_{r,t}$ is zero-mean noise. PCA identifies principal components (PCs) as the eigenvectors of the sample covariance matrix of the data. We analyze the sample covariance for each strategy.\n\n### Strategy 1: Stacking\nIn this strategy, all data points $\\{x_{r,t}\\}_{r \\in \\{1,\\dots,R\\}, t \\in \\{1,\\dots,T\\}}$ are concatenated into a single data matrix $X_{\\mathrm{stack}} \\in \\mathbb{R}^{N \\times (RT)}$. The total number of samples is $RT$. PCA is applied to this set of $RT$ vectors in $\\mathbb{R}^{N}$. The procedure specifies mean-centering the data across all $RT$ samples before computing the sample covariance.\n\nThe sample mean is:\n$$ \\bar{x}_{\\mathrm{stack}} = \\frac{1}{RT} \\sum_{r=1}^{R} \\sum_{t=1}^{T} x_{r,t} = \\frac{1}{RT} \\sum_{r,t} (\\mu_t + \\delta_r + \\epsilon_{r,t}) = \\left(\\frac{1}{T}\\sum_t \\mu_t\\right) + \\left(\\frac{1}{R}\\sum_r \\delta_r\\right) + \\left(\\frac{1}{RT}\\sum_{r,t} \\epsilon_{r,t}\\right) $$\nLet $\\bar{\\mu} = \\frac{1}{T}\\sum_t \\mu_t$, $\\bar{\\delta} = \\frac{1}{R}\\sum_r \\delta_r$, and $\\bar{\\epsilon} = \\frac{1}{RT}\\sum_{r,t} \\epsilon_{r,t}$. A centered data point is:\n$$ x_{r,t} - \\bar{x}_{\\mathrm{stack}} = (\\mu_t - \\bar{\\mu}) + (\\delta_r - \\bar{\\delta}) + (\\epsilon_{r,t} - \\bar{\\epsilon}) $$\nThe sample covariance matrix $C_{\\mathrm{stack}}$ is proportional to the sum of outer products of these centered vectors. A key property, related to the analysis of variance (ANOVA), is that the cross-terms in the sum of squares, such as $\\sum_{r,t} (\\mu_t - \\bar{\\mu})(\\delta_r - \\bar{\\delta})^T$, are zero. For large samples, the sample covariance matrix approximates the true covariance matrix, which, due to the independence of the terms in the generative model, is the sum of the individual covariance matrices:\n$$ C_{\\mathrm{stack}} \\approx \\text{Cov}(\\mu_t - \\bar{\\mu}) + \\text{Cov}(\\delta_r - \\bar{\\delta}) + \\text{Cov}(\\epsilon_{r,t} - \\bar{\\epsilon}) $$\nThis can be written as:\n$$ C_{\\mathrm{stack}} \\approx C_{\\mu} + \\Sigma_{\\delta} + \\Sigma_{\\epsilon} $$\nwhere $C_{\\mu}$ is the covariance of the mean trajectory vectors $\\{\\mu_t\\}$, $\\Sigma_{\\delta}$ is the covariance of the trial-offset vectors $\\{\\delta_r\\}$, and $\\Sigma_{\\epsilon}$ is the covariance of the noise. This shows that the variance available to PCA in the stacking method is a sum of contributions from the mean trajectory, the trial-to-trial variability, and the noise.\n\n### Strategy 2: Averaging\nIn this strategy, we first average the data across trials for each time point:\n$$ \\bar{x}_{t} = \\frac{1}{R} \\sum_{r=1}^{R} x_{r,t} = \\frac{1}{R} \\sum_{r=1}^{R} (\\mu_t + \\delta_r + \\epsilon_{r,t}) = \\mu_t + \\left(\\frac{1}{R}\\sum_r \\delta_r\\right) + \\left(\\frac{1}{R}\\sum_r \\epsilon_{r,t}\\right) $$\n$$ \\bar{x}_{t} = \\mu_t + \\bar{\\delta} + \\bar{\\epsilon}_{t} $$\nwhere $\\bar{\\delta}$ is the same vector defined before, and $\\bar{\\epsilon}_t = \\frac{1}{R}\\sum_r \\epsilon_{r,t}$ is the trial-averaged noise at time $t$. The dataset for PCA is now $\\{ \\bar{x}_t \\}_{t \\in \\{1,\\dots,T\\}}$. The procedure specifies mean-centering these $T$ samples. The sample mean is:\n$$ \\bar{x}_{\\mathrm{avg}} = \\frac{1}{T} \\sum_{t=1}^{T} \\bar{x}_t = \\frac{1}{T} \\sum_t (\\mu_t + \\bar{\\delta} + \\bar{\\epsilon}_{t}) = \\bar{\\mu} + \\bar{\\delta} + \\frac{1}{T}\\sum_t \\bar{\\epsilon}_{t} $$\nA centered averaged data point is:\n$$ \\bar{x}_{t} - \\bar{x}_{\\mathrm{avg}} = (\\mu_t - \\bar{\\mu}) + (\\bar{\\delta} - \\bar{\\delta}) + (\\bar{\\epsilon}_t - \\frac{1}{T}\\sum_t \\bar{\\epsilon}_{t}) = (\\mu_t - \\bar{\\mu}) + (\\bar{\\epsilon}_t - \\bar{\\bar{\\epsilon}}) $$\nCrucially, the time-invariant term $\\bar{\\delta}$ is completely removed by the time-mean-centering step. The trial-to-trial variability encoded by $\\{\\delta_r\\}$ is collapsed into a single vector $\\bar{\\delta}$ which is then subtracted.\nThe sample covariance matrix $C_{\\mathrm{avg}}$ will thus only contain contributions from the mean trajectory and the averaged noise:\n$$ C_{\\mathrm{avg}} \\approx C_{\\mu} + \\text{Cov}(\\bar{\\epsilon}_t) $$\nSince the noise terms $\\epsilon_{r,t}$ are independent across trials $r$ with covariance $\\Sigma_{\\epsilon}$, the covariance of their average $\\bar{\\epsilon}_t$ is:\n$$ \\text{Cov}(\\bar{\\epsilon}_t) = \\text{Cov}\\left(\\frac{1}{R}\\sum_{r=1}^{R} \\epsilon_{r,t}\\right) = \\frac{1}{R^2} \\sum_{r=1}^{R} \\text{Cov}(\\epsilon_{r,t}) = \\frac{R}{R^2}\\Sigma_{\\epsilon} = \\frac{1}{R}\\Sigma_{\\epsilon} $$\nTherefore, the covariance matrix for the averaging strategy is:\n$$ C_{\\mathrm{avg}} \\approx C_{\\mu} + \\frac{1}{R}\\Sigma_{\\epsilon} $$\n\n### Option-by-Option Analysis\n\n**A. In $X_{\\mathrm{avg}}$, principal components cannot capture the trial-specific variability term $\\delta_{r}$ beyond its contribution to the overall mean, so trial-to-trial variability is largely suppressed; in contrast, $X_{\\mathrm{stack}}$ can reveal low-dimensional shared variability across trials via components spanning the subspace generated by $\\{\\delta_{r}\\}_{r=1}^{R}$.**\n- As derived for $X_{\\mathrm{avg}}$, the trial-averaging followed by time-mean-centering removes any contribution from the trial-to-trial variability of $\\delta_r$ to the covariance matrix. The term $\\Sigma_{\\delta}$ is absent in $C_{\\mathrm{avg}}$.\n- As derived for $X_{\\mathrm{stack}}$, the covariance matrix $C_{\\mathrm{stack}}$ contains the term $\\Sigma_{\\delta} = \\text{Cov}(\\delta_r)$, which explicitly represents the variance structure of trial-to-trial variability. PCA applied to $X_{\\mathrm{stack}}$ will therefore find components that capture this variance.\n- **Verdict: Correct.**\n\n**B. If one mean-centers $X_{\\mathrm{stack}}$ across all $T \\cdot R$ samples, trial-to-trial variability $\\delta_{r}$ is eliminated by construction, making the variance structure equivalent to that of $X_{\\mathrm{avg}}$.**\n- Mean-centering $X_{\\mathrm{stack}}$ involves subtracting the global mean $\\bar{x}_{\\mathrm{stack}}$ from each sample $x_{r,t}$. The centered data point is $(\\mu_t - \\bar{\\mu}) + (\\delta_r - \\bar{\\delta}) + (\\epsilon_{r,t} - \\bar{\\epsilon})$. The term $(\\delta_r - \\bar{\\delta})$ represents the deviation of each trial's offset from the average offset. The variance of this term across the $RT$ samples contributes the $\\Sigma_{\\delta}$ term to the total covariance. Centering does not eliminate variability; it only removes the mean. The variance structure remains $C_{\\mathrm{stack}} \\approx C_{\\mu} + \\Sigma_{\\delta} + \\Sigma_{\\epsilon}$, which is not equivalent to $C_{\\mathrm{avg}}$.\n- **Verdict: Incorrect.**\n\n**C. Under the assumption that $\\epsilon_{r,t}$ are independent across trials with $\\mathbb{E}[\\epsilon_{r,t}] = 0$, averaging across trials reduces the contribution of $\\epsilon_{r,t}$ to the sample covariance by approximately a factor of $1/R$, thereby stabilizing components that reflect $\\mu_{t}$, but it simultaneously discards between-trial variance from $\\delta_{r}$.**\n- The noise contribution to $C_{\\mathrm{stack}}$ is $\\Sigma_{\\epsilon}$, while for $C_{\\mathrm{avg}}$ it is $\\frac{1}{R}\\Sigma_{\\epsilon}$. The reduction factor is indeed $1/R$.\n- By reducing the relative magnitude of the noise term, the signal-to-noise ratio for the covariance structure of the mean trajectory, $C_{\\mu}$, is improved. This leads to a more stable and accurate estimation of the PCs of $\\mu_t$.\n- As established in the analysis of option A, the averaging strategy discards the between-trial variance from $\\delta_r$.\n- **Verdict: Correct.**\n\n**D. If trial-to-trial variability is purely a scalar gain $g_{r} \\in \\mathbb{R}$ multiplying the time course, i.e., $x_{r,t} = g_{r} \\mu_{t} + \\epsilon_{r,t}$, then $X_{\\mathrm{stack}}$ will tend to produce a dominant component aligned with $\\mu_{t}$ and an additional component aligned with a gain direction orthogonal to $\\mu_{t}$, whereas $X_{\\mathrm{avg}}$ will largely eliminate the gain variability.**\n- This option proposes an alternative model. For $X_{\\mathrm{avg}}$, $\\bar{x}_t = (\\frac{1}{R}\\sum_r g_r)\\mu_t + \\bar{\\epsilon}_t$. The variability in $g_r$ is averaged out, so this part of the statement is correct.\n- For $X_{\\mathrm{stack}}$, the centered data points (approximating $g_r \\approx 1$ and ignoring noise) are $x_{r,t} - \\bar{x}_{\\mathrm{stack}} \\approx (g_r \\mu_t) - \\bar{\\mu} \\approx (\\mu_t - \\bar{\\mu}) + (g_r-1)\\mu_t$. A more careful decomposition is $g_r \\mu_t - \\bar{g}\\bar{\\mu} \\approx (\\mu_t - \\bar{\\mu}) + (g_r-\\bar{g})\\bar{\\mu}$, for small variations. This shows two primary sources of variance: one related to the shape of the trajectory $(\\mu_t-\\bar{\\mu})$ and one related to the gain fluctuations $(g_r-\\bar{g})$ acting in the direction of the mean state $\\bar{\\mu}$. The statement claims the \"gain direction\" is orthogonal to \"$\\mu_t$\". This is imprecise and generally false. The component capturing gain variance will be aligned with $\\bar{\\mu}$, which is not necessarily orthogonal to the principal axes of the trajectory shape $(\\mu_t - \\bar{\\mu})$. The claim of orthogonality is a critical flaw.\n- **Verdict: Incorrect.**\n\n**E. When temporal alignment errors exist across trials (e.g., variable latencies), stacking necessarily mixes time shifts into principal components and degrades trajectory estimation, while averaging perfectly preserves the true $\\mu_{t}$ without distortion.**\n- With alignment errors (jitter), stacking does treat the jitter as a source of variance, which will be captured by the PCs, thus mixing temporal effects with state-space structure and degrading the estimate of the underlying trajectory shape. This part is correct.\n- However, averaging data with temporal jitter, e.g. $\\bar{x}_t = \\frac{1}{R}\\sum_r \\mu(t+\\tau_r)$, results in a temporally blurred version of the true trajectory. This blurring is a form of distortion; for instance, sharp features in $\\mu_t$ would be smoothed out. Averaging does not \"perfectly preserve\" the trajectory.\n- **Verdict: Incorrect.**\n\n**F. Under the model $x_{r,t} = \\mu_{t} + \\delta_{r} + \\epsilon_{r,t}$ with time-invariant $\\delta_{r}$, the neuron-space sample covariance computed from $X_{\\mathrm{stack}}$ decomposes into a sum of a temporal covariance term from $\\mu_{t}$, a between-trial covariance term from $\\delta_{r}$, and a residual noise term. The between-trial term induces principal components whose projections are constant across time within each trial (up to noise), producing trial-dependent vertical shifts in the projected trajectories.**\n- The first sentence is a correct description of the decomposition $C_{\\mathrm{stack}} \\approx C_{\\mu} + \\Sigma_{\\delta} + \\Sigma_{\\epsilon}$, as derived.\n- The second sentence correctly interprets the effect of the $\\Sigma_{\\delta}$ term. Let $v$ be a PC dominated by variance from $\\Sigma_{\\delta}$. Its projection of a data point is $v^T x_{r,t} = v^T \\mu_t + v^T \\delta_r + v^T \\epsilon_{r,t}$. Within a given trial $r$, the term $v^T \\delta_r$ is a constant value. This creates a constant offset for trial $r$'s trajectory in the direction of $v$. If we project onto several such PCs, the result is a constant offset vector for each trial's trajectory in the low-dimensional space. This is precisely what is meant by \"trial-dependent vertical shifts\".\n- **Verdict: Correct.**\n\nFinal summary: Statements A, C, and F are correct descriptions of the statistical properties of the two methods under the given generative model.",
            "answer": "$$\\boxed{ACF}$$"
        },
        {
            "introduction": "A classic challenge after performing PCA is determining how many components represent genuine signal versus measurement noise. While simple heuristics like the \"elbow plot\" are common, random matrix theory (RMT) offers a more rigorous solution. This practice introduces a powerful technique based on the Marchenko-Pastur law, which describes the expected eigenvalue distribution of a pure noise matrix. By comparing your data's eigenvalues to this theoretical noise ceiling, you can develop a principled method to estimate the true dimensionality of your neural trajectories .",
            "id": "4187623",
            "problem": "You are given a synthetic high-dimensional time series model for neural population activity, designed to reflect low-dimensional neural trajectories embedded in additive measurement noise. The data are represented by a matrix $X \\in \\mathbb{R}^{N \\times T}$, where $N$ denotes the number of neurons and $T$ denotes the number of time points. The Principal Component Analysis (PCA) covariance estimator is the sample covariance $C = \\frac{1}{T} X X^\\top$. For a baseline model with independent zero-mean Gaussian noise of variance $\\sigma^2$ across neurons and time, the noise-only sample covariance is a Wishart matrix. A well-tested fact from random matrix theory states that, in the joint limit where $N$ and $T$ grow with a fixed aspect ratio $q = N/T$, the eigenvalue distribution of the noise-only covariance converges to the Marchenko–Pastur (MP) law, supported on an interval whose upper edge depends on $q$ and $\\sigma^2$. Low-rank signal trajectories produce a finite number of outlier eigenvalues in $C$ that can rise above the upper edge of the MP support, enabling estimation of the signal dimensionality.\n\nImplement a program that:\n- Constructs synthetic datasets according to the model $X = S + \\varepsilon$, with $S = W Z$ and $\\varepsilon$ drawn independently across entries.\n- Uses the sample covariance $C$ and its eigenvalues to estimate the number of signal principal components that lie above the noise bulk predicted by the Marchenko–Pastur law, as determined by the appropriate upper edge for the given $N$, $T$, and $\\sigma^2$.\n- Counts how many eigenvalues of $C$ exceed this upper edge and returns this count for each test case.\n\nSynthetic data specification for each test case:\n- $W \\in \\mathbb{R}^{N \\times r}$ has $r$ orthonormal columns, constructed by orthonormalizing $r$ i.i.d. standard normal columns via the Gram–Schmidt process implemented as a $\\mathrm{QR}$ decomposition.\n- $Z \\in \\mathbb{R}^{r \\times T}$ has independent Gaussian rows, where the $j$-th row has entries distributed as $\\mathcal{N}(0, \\sqrt{s_j^2})$ so that the temporal variance of the $j$-th latent trajectory is $s_j^2$. The list $[s_1^2, s_2^2, \\dots, s_r^2]$ is provided per test case.\n- $\\varepsilon \\in \\mathbb{R}^{N \\times T}$ has entries distributed independently as $\\mathcal{N}(0, \\sqrt{\\sigma^2})$.\n- The sample covariance is $C = \\frac{1}{T} X X^\\top$.\n- The eigenvalues of $C$ are computed and compared to the MP upper edge; any eigenvalue strictly larger than the MP upper edge is counted as a detected signal principal component.\n\nYour program must implement the following test suite, using the exact parameter values and random seeds provided, and must produce the counts for each case in the specified format.\n\nTest suite:\n1. Case A (happy path, multiple strong signals): $N = 50$, $T = 200$, $\\sigma^2 = 1.0$, $r = 3$, $[s_1^2, s_2^2, s_3^2] = [6.0, 4.0, 2.0]$, random seed $42$.\n2. Case B (pure noise, $N  T$ boundary): $N = 200$, $T = 100$, $\\sigma^2 = 1.0$, $r = 0$, empty signal list $[\\ ]$, random seed $123$.\n3. Case C (large $T$, diverse signal strengths): $N = 100$, $T = 500$, $\\sigma^2 = 0.5$, $r = 5$, $[s_1^2, s_2^2, s_3^2, s_4^2, s_5^2] = [1.0, 1.2, 2.5, 0.8, 3.0]$, random seed $2024$.\n4. Case D (very small $T$, signals below detectability): $N = 120$, $T = 20$, $\\sigma^2 = 1.0$, $r = 2$, $[s_1^2, s_2^2] = [3.0, 4.0]$, random seed $7$.\n5. Case E (square case, mixed strengths): $N = 75$, $T = 75$, $\\sigma^2 = 0.8$, $r = 2$, $[s_1^2, s_2^2] = [4.0, 0.5]$, random seed $99$.\n\nAlgorithmic requirements:\n- For each case, construct $W$, $Z$, and $\\varepsilon$ according to the above rules using the specified seed for reproducibility.\n- Form $X = W Z + \\varepsilon$ and compute $C = \\frac{1}{T} X X^\\top$.\n- Compute the eigenvalues of $C$ and determine the Marchenko–Pastur upper edge appropriate to the case’s $N$, $T$, and $\\sigma^2$.\n- Count eigenvalues of $C$ strictly above this upper edge.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[$\\mathrm{result}_1$, $\\mathrm{result}_2$, $\\mathrm{result}_3$, $\\mathrm{result}_4$, $\\mathrm{result}_5$]\".\n- Each $\\mathrm{result}_i$ must be an integer giving the count of detected signal principal components for case $i$.",
            "solution": "The problem requires an estimation of the dimensionality of a low-rank signal embedded in high-dimensional noise. The data follows the model $X = S + \\varepsilon$, where $X \\in \\mathbb{R}^{N \\times T}$ is the data matrix, $S \\in \\mathbb{R}^{N \\times T}$ is the low-rank signal matrix, and $\\varepsilon \\in \\mathbb{R}^{N \\times T}$ is a matrix of additive noise. Here, $N$ is the number of observed variables (e.g., neurons) and $T$ is the number of time samples. The task is to determine the rank of the signal, $r$, by analyzing the eigenvalues of the sample covariance matrix $C = \\frac{1}{T} X X^\\top$.\n\nThe methodology is based on a key result from random matrix theory (RMT). For a data matrix comprised solely of independent and identically distributed (i.i.d.) noise with variance $\\sigma^2$, the empirical spectral distribution of the sample covariance matrix $C_{noise} = \\frac{1}{T} \\varepsilon \\varepsilon^\\top$ converges to the Marchenko-Pastur (MP) distribution in the limit $N, T \\to \\infty$ with their ratio $q = N/T$ held fixed. The support of this distribution is a continuous interval, colloquially known as the \"noise bulk.\" The upper edge of this support, $\\lambda_+$, is given by the formula:\n$$ \\lambda_+ = \\sigma^2 (1 + \\sqrt{q})^2 $$\nWhen a low-rank signal $S$ is added to the noise, under certain conditions, a number of eigenvalues of the full covariance matrix $C$ will detach from the noise bulk and appear as \"outlier\" eigenvalues strictly greater than $\\lambda_+$. The number of such outliers corresponds to the dimensionality (rank) of the signal, $r$. The problem asks us to count these outliers for several synthetic datasets.\n\nThe algorithmic procedure for each test case is as follows:\n\n1.  **Initialize Parameters**: For each case, we are given the dimensions $N$ and $T$, the noise variance $\\sigma^2$, the signal rank $r$ and associated signal component variances $[s_1^2, s_2^2, \\dots, s_r^2]$, and a random seed for reproducibility.\n\n2.  **Generate Synthetic Data**:\n    -   **Signal Matrix Construction**: The signal matrix is formulated as $S = WZ$.\n        -   The matrix $W \\in \\mathbb{R}^{N \\times r}$ provides a set of $r$ orthonormal basis vectors for the signal subspace. It is constructed by first generating an $N \\times r$ matrix with i.i.d. entries from a standard normal distribution, $\\mathcal{N}(0, 1)$, and then orthonormalizing its columns. This orthonormalization is achieved using the QR decomposition, where $W$ is the resulting $Q$ factor.\n        -   The matrix $Z \\in \\mathbb{R}^{r \\times T}$ contains the temporal dynamics of the signal components. Its rows are independent of each other. The entries of the $j$-th row are i.i.d. samples from a Gaussian distribution $\\mathcal{N}(0, s_j)$, where $s_j^2$ is the specified variance for the $j$-th latent trajectory.\n    -   **Noise Matrix Construction**: The noise matrix $\\varepsilon \\in \\mathbb{R}^{N \\times T}$ is generated with each entry drawn independently from a Gaussian distribution with mean $0$ and variance $\\sigma^2$, denoted as $\\mathcal{N}(0, \\sigma)$.\n    -   **Data Matrix Assembly**: The final data matrix is formed by the sum of the signal and noise components: $X = WZ + \\varepsilon$. In the special case where $r=0$, the signal $S$ is simply a zero matrix.\n\n3.  **Covariance and Eigenvalue Calculation**:\n    -   The sample covariance matrix is computed as $C = \\frac{1}{T} X X^\\top$. This is an $N \\times N$ positive semi-definite matrix.\n    -   The eigenvalues of $C$ are then calculated. Since $C$ is symmetric, specialized and efficient numerical algorithms are used. For computational efficiency, if $N  T$, it is advantageous to compute the eigenvalues of the smaller $T \\times T$ matrix $\\frac{1}{T} X^\\top X$. The set of non-zero eigenvalues of $XX^\\top$ and $X^\\top X$ are identical. Since the threshold $\\lambda_+$ is always positive, we do not risk miscounting outliers with this optimization.\n\n4.  **Marchenko-Pastur Edge Calculation**:\n    -   The aspect ratio $q$ is calculated as $q = N/T$.\n    -   The theoretical upper edge of the noise eigenvalue spectrum, $\\lambda_+$, is calculated using the formula $\\lambda_+ = \\sigma^2 (1 + \\sqrt{q})^2$. This formula is valid for any $q  0$.\n\n5.  **Signal Dimensionality Estimation**:\n    -   The computed eigenvalues of the sample covariance matrix are compared against the threshold $\\lambda_+$.\n    -   The estimated signal dimensionality is the total count of eigenvalues that are strictly greater than $\\lambda_+$.\n\nThis procedure is implemented for each of the five test cases defined in the problem statement. The use of specified random seeds ensures that the generated synthetic data is identical for each run, making the results fully reproducible. For example, for Case E with parameters $N=75$, $T=75$, $\\sigma^2 = 0.8$, and signal variances $[4.0, 0.5]$, the aspect ratio is $q=1$ and the MP edge is $\\lambda_+ = 0.8(1+\\sqrt{1})^2 = 3.2$. RMT predicts a signal component with population variance $s_j^2$ forms an outlier if $s_j^2/\\sigma^2  \\sqrt{q}$. Only the first signal component ($4.0/0.8 = 5  1$) satisfies this condition, so we expect to find one outlier.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by iterating through test cases, generating synthetic\n    neural data, and estimating signal dimensionality using random matrix theory.\n    \"\"\"\n\n    def estimate_dimensionality(N, T, sigma2, signal_variances, seed):\n        \"\"\"\n        Constructs synthetic data and estimates signal dimensionality by counting\n        eigenvalues of the sample covariance matrix above the Marchenko-Pastur edge.\n\n        Args:\n            N (int): Number of neurons.\n            T (int): Number of time points.\n            sigma2 (float): Variance of the additive noise.\n            signal_variances (list of float): Variances of the latent signal trajectories.\n            seed (int): Random seed for reproducibility.\n\n        Returns:\n            int: The estimated number of signal dimensions (outlier eigenvalues).\n        \"\"\"\n        # Use a random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n        r = len(signal_variances)\n\n        # 1. Construct Signal Matrix S = WZ\n        if r  0:\n            # Construct W, an orthonormal basis for the signal subspace\n            # by applying QR decomposition to a random Gaussian matrix.\n            A = rng.standard_normal(size=(N, r))\n            W, _ = np.linalg.qr(A)\n\n            # Construct Z, the temporal components of the signal.\n            # Each row j is scaled by the standard deviation sqrt(s_j^2).\n            s = np.sqrt(np.array(signal_variances))\n            Z_std = rng.standard_normal(size=(r, T))\n            Z = s[:, np.newaxis] * Z_std\n\n            S = W @ Z\n        else:\n            # For the pure noise case (r=0), the signal is a zero matrix.\n            S = np.zeros((N, T))\n\n        # 2. Construct Noise Matrix epsilon\n        std_noise = np.sqrt(sigma2)\n        epsilon = std_noise * rng.standard_normal(size=(N, T))\n\n        # 3. Construct Data Matrix X\n        X = S + epsilon\n\n        # 4. Compute Covariance and Eigenvalues\n        # To optimize, we compute eigenvalues from the smaller of the two matrices\n        # XX^T (N x N) and X^T X (T x T), as their non-zero eigenvalues are identical.\n        if N  T:\n            # Form the T x T matrix\n            cov_matrix = (X.T @ X) / T\n        else:\n            # Form the N x N matrix\n            cov_matrix = (X @ X.T) / T\n        \n        # Eigenvalues of the symmetric covariance matrix\n        eigenvalues = np.linalg.eigvalsh(cov_matrix)\n\n        # 5. Compute Marchenko-Pastur Upper Edge\n        q = N / T\n        mp_edge = sigma2 * (1 + np.sqrt(q))**2\n\n        # 6. Count Outlier Eigenvalues\n        # The number of signal components is the count of eigenvalues strictly\n        # greater than the theoretical maximum for a noise-only matrix.\n        num_outliers = np.sum(eigenvalues  mp_edge)\n\n        return int(num_outliers)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Happy path, multiple strong signals\n        {'N': 50, 'T': 200, 'sigma2': 1.0, 'signal_variances': [6.0, 4.0, 2.0], 'seed': 42},\n        # Case B: Pure noise, N  T boundary\n        {'N': 200, 'T': 100, 'sigma2': 1.0, 'signal_variances': [], 'seed': 123},\n        # Case C: Large T, diverse signal strengths\n        {'N': 100, 'T': 500, 'sigma2': 0.5, 'signal_variances': [1.0, 1.2, 2.5, 0.8, 3.0], 'seed': 2024},\n        # Case D: Very small T, signals at detectability limit\n        {'N': 120, 'T': 20, 'sigma2': 1.0, 'signal_variances': [3.0, 4.0], 'seed': 7},\n        # Case E: Square case, mixed strengths (one sub-critical)\n        {'N': 75, 'T': 75, 'sigma2': 0.8, 'signal_variances': [4.0, 0.5], 'seed': 99},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = estimate_dimensionality(\n            N=case['N'],\n            T=case['T'],\n            sigma2=case['sigma2'],\n            signal_variances=case['signal_variances'],\n            seed=case['seed']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}