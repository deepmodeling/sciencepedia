{
    "hands_on_practices": [
        {
            "introduction": "在将主成分分析（PCA）应用于神经脉冲计数数据之前，一个关键的考量是原始数据的统计特性会如何影响分析结果。由于不同神经元的基线放电率（baseline firing rates）差异很大，直接对原始数据进行分析可能会导致高放电率神经元不成比例地主导主成分。这个练习将通过一个生成模型，从理论上推导并比较不同预处理方法（原始数据、平方根变换、Z-score标准化）对神经元方差的影响，从而揭示为何需要对数据进行变换。",
            "id": "4187658",
            "problem": "一个实验室在许多时间点上，沿一条神经轨迹记录了两个同时记录的神经元的脉冲计数。令神经元 $i \\in \\{1,2\\}$ 在时间 $t$ 的脉冲计数为 $Y_{it}$。假设神经轨迹服从一个乘性共享增益模型：以一个潜增益 $s_t$ 为条件，脉冲计数是独立的，并服从如下分布\n$$\nY_{it} \\mid s_t \\sim \\mathrm{Poisson}(\\mu_i s_t),\n$$\n其中 $\\mathbb{E}[s_t] = 1$ 且 $\\mathrm{Var}(s_t) = \\sigma_s^2$，并且 $\\{s_t\\}$ 在时间上是独立的。均值 $\\mu_i$ 包含了异质的基线发放率。考虑对 $Y_{it}$ 随时间变化（将时间点视为样本）应用三种不同预处理选择计算出的协方差矩阵进行主成分分析（PCA）：\n\n- 原始计数：$R_{it} = Y_{it}$。\n- 平方根计数：$Q_{it} = \\sqrt{Y_{it}}$。\n- Z-分数计数：$Z_{it} = \\dfrac{Y_{it} - \\mathbb{E}[Y_{it}]}{\\sqrt{\\mathrm{Var}(Y_{it})}}$，其中期望和方差是根据上述生成模型计算的。\n\n在时间上的大样本极限下，令 $C^{(X)}$ 表示对变换 $X \\in \\{\\mathrm{raw}, \\sqrt{\\cdot}, z\\}$ 的两个神经元之间的期望协方差矩阵，其元素为\n$$\nC^{(X)}_{ij} \\equiv \\mathbb{E}\\Big[\\big(X_{it} - \\mathbb{E}[X_{it}]\\big)\\big(X_{jt} - \\mathbb{E}[X_{jt}]\\big)\\Big].\n$$\n定义变换 $X$ 的异质性偏差指数为其对角线元素之比\n$$\nH^{(X)} \\equiv \\frac{C^{(X)}_{11}}{C^{(X)}_{22}}.\n$$\n假设基线发放率满足 $\\mu_1 = k \\mu_2$ 且 $k > 1$。在适用于大 $\\mu_i$ 和有限 $\\sigma_s^2$ 的波动的主导阶下，符号化地推导 $H^{(\\mathrm{raw})}$、$H^{(\\sqrt{\\cdot})}$ 和 $H^{(z)}$，然后计算比率的完全简化的解析表达式\n$$\n\\mathcal{R}(k) \\equiv \\frac{H^{(\\mathrm{raw})}}{H^{(\\sqrt{\\cdot})}}.\n$$\n请以仅包含 $k$ 的闭式表达式形式给出 $\\mathcal{R}(k)$ 的最终答案。不需要进行数值四舍五入，也不涉及任何单位。",
            "solution": "目标是计算比率 $\\mathcal{R}(k) \\equiv \\frac{H^{(\\mathrm{raw})}}{H^{(\\sqrt{\\cdot})}}$，其中 $H^{(X)}$ 是不同数据变换 $X$ 的异质性偏差指数。该指数定义为方差之比 $H^{(X)} \\equiv C^{(X)}_{11} / C^{(X)}_{22}$，其中 $C^{(X)}_{ii} = \\mathrm{Var}(X_{it})$。我们必须在大的基线发放率 $\\mu_i$ 的主导阶下进行计算。\n\n对每种变换的分析需要变换后变量的无条件方差。一个关键工具是全方差公式。对于一个随机变量 $Y$ 和一个潜变量 $s$，该公式表述为：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_s[\\mathrm{Var}(Y \\mid s)] + \\mathrm{Var}_s(\\mathbb{E}[Y \\mid s])\n$$\n首先，我们计算原始脉冲计数 $Y_{it}$ 的无条件均值和方差。我们已知 $Y_{it} \\mid s_t \\sim \\mathrm{Poisson}(\\mu_i s_t)$。泊松分布的均值和方差等于其率参数。\n条件均值和方差为：\n$$\n\\mathbb{E}[Y_{it} \\mid s_t] = \\mu_i s_t\n$$\n$$\n\\mathrm{Var}(Y_{it} \\mid s_t) = \\mu_i s_t\n$$\n使用全期望公式和已知的 $\\mathbb{E}[s_t] = 1$：\n$$\n\\mathbb{E}[Y_{it}] = \\mathbb{E}_{s_t}[\\mathbb{E}[Y_{it} \\mid s_t]] = \\mathbb{E}_{s_t}[\\mu_i s_t] = \\mu_i \\mathbb{E}[s_t] = \\mu_i\n$$\n使用全方差公式和已知的 $\\mathrm{Var}(s_t) = \\sigma_s^2$：\n$$\n\\mathrm{Var}(Y_{it}) = \\mathbb{E}_{s_t}[\\mathrm{Var}(Y_{it} \\mid s_t)] + \\mathrm{Var}_{s_t}(\\mathbb{E}[Y_{it} \\mid s_t])\n$$\n$$\n\\mathrm{Var}(Y_{it}) = \\mathbb{E}_{s_t}[\\mu_i s_t] + \\mathrm{Var}_{s_t}(\\mu_i s_t)\n$$\n$$\n\\mathrm{Var}(Y_{it}) = \\mu_i \\mathbb{E}[s_t] + \\mu_i^2 \\mathrm{Var}(s_t)\n$$\n$$\n\\mathrm{Var}(Y_{it}) = \\mu_i + \\mu_i^2 \\sigma_s^2\n$$\n这是原始计数方差的精确表达式。\n\n现在我们分析每种预处理选择。\n\n**1. 原始计数 ($R_{it} = Y_{it}$)**\n方差就是我们刚刚计算的结果：\n$$\nC^{(\\mathrm{raw})}_{ii} = \\mathrm{Var}(Y_{it}) = \\mu_i + \\mu_i^2 \\sigma_s^2\n$$\n异质性偏差指数是神经元 1 和神经元 2 的这些方差之比：\n$$\nH^{(\\mathrm{raw})} = \\frac{C^{(\\mathrm{raw})}_{11}}{C^{(\\mathrm{raw})}_{22}} = \\frac{\\mu_1 + \\mu_1^2 \\sigma_s^2}{\\mu_2 + \\mu_2^2 \\sigma_s^2}\n$$\n题目要求我们在大 $\\mu_i$ 的主导阶下进行计算。当 $\\mu_i \\to \\infty$ 时，$\\mu_i^2$ 项主导 $\\mu_i$ 项。代入 $\\mu_1 = k \\mu_2$：\n$$\nH^{(\\mathrm{raw})} = \\frac{k\\mu_2 + (k\\mu_2)^2 \\sigma_s^2}{\\mu_2 + \\mu_2^2 \\sigma_s^2} = \\frac{k\\mu_2 + k^2\\mu_2^2 \\sigma_s^2}{\\mu_2 + \\mu_2^2 \\sigma_s^2} = \\frac{\\mu_2^2 \\sigma_s^2 (k^2 + k/(\\mu_2 \\sigma_s^2))}{\\mu_2^2 \\sigma_s^2 (1 + 1/(\\mu_2 \\sigma_s^2))}\n$$\n在大 $\\mu_2$ 的极限下，这简化为主导项系数的比值：\n$$\nH^{(\\mathrm{raw})} \\approx \\frac{k^2\\mu_2^2 \\sigma_s^2}{\\mu_2^2 \\sigma_s^2} = k^2\n$$\n\n**2. 平方根计数 ($Q_{it} = \\sqrt{Y_{it}}$)**\n我们需要求 $C^{(\\sqrt{\\cdot})}_{ii} = \\mathrm{Var}(\\sqrt{Y_{it}})$。对于大的 $\\mu_i$，$Y_{it}$ 的分布集中在其均值 $\\mu_i$ 附近。我们可以使用 Delta 方法来近似一个随机变量函数的方差。对于一个函数 $g(Y)$，其近似值为 $\\mathrm{Var}(g(Y)) \\approx [g'(\\mathbb{E}[Y])]^2 \\mathrm{Var}(Y)$。\n这里，$g(y) = \\sqrt{y}$，所以其导数为 $g'(y) = \\frac{1}{2\\sqrt{y}}$。随机变量是 $Y_{it}$，其 $\\mathbb{E}[Y_{it}] = \\mu_i$ 且 $\\mathrm{Var}(Y_{it}) = \\mu_i + \\mu_i^2 \\sigma_s^2$。\n应用 Delta 方法：\n$$\nC^{(\\sqrt{\\cdot})}_{ii} = \\mathrm{Var}(\\sqrt{Y_{it}}) \\approx \\left[ g'(\\mathbb{E}[Y_{it}]) \\right]^2 \\mathrm{Var}(Y_{it})\n$$\n$$\nC^{(\\sqrt{\\cdot})}_{ii} \\approx \\left( \\frac{1}{2\\sqrt{\\mu_i}} \\right)^2 (\\mu_i + \\mu_i^2 \\sigma_s^2) = \\frac{1}{4\\mu_i} (\\mu_i + \\mu_i^2 \\sigma_s^2)\n$$\n$$\nC^{(\\sqrt{\\cdot})}_{ii} \\approx \\frac{1}{4} + \\frac{\\mu_i \\sigma_s^2}{4}\n$$\n现在我们计算平方根变换的异质性偏差指数：\n$$\nH^{(\\sqrt{\\cdot})} = \\frac{C^{(\\sqrt{\\cdot})}_{11}}{C^{(\\sqrt{\\cdot})}_{22}} \\approx \\frac{1/4 + \\mu_1 \\sigma_s^2 / 4}{1/4 + \\mu_2 \\sigma_s^2 / 4} = \\frac{1 + \\mu_1 \\sigma_s^2}{1 + \\mu_2 \\sigma_s^2}\n$$\n代入 $\\mu_1 = k \\mu_2$ 并取大 $\\mu_2$ 的主导阶：\n$$\nH^{(\\sqrt{\\cdot})} \\approx \\frac{1 + k\\mu_2 \\sigma_s^2}{1 + \\mu_2 \\sigma_s^2} \\approx \\frac{k\\mu_2 \\sigma_s^2}{\\mu_2 \\sigma_s^2} = k\n$$\n\n**3. Z-分数计数 ($Z_{it}$)**\nZ-分数变量定义为 $Z_{it} = \\frac{Y_{it} - \\mathbb{E}[Y_{it}]}{\\sqrt{\\mathrm{Var}(Y_{it})}}$。\n根据定义，一个 z-分数变量的均值为 0，方差为 1。我们来验证一下 $C^{(z)}_{ii}$：\n$$\nC^{(z)}_{ii} = \\mathrm{Var}(Z_{it}) = \\mathrm{Var}\\left( \\frac{Y_{it} - \\mathbb{E}[Y_{it}]}{\\sqrt{\\mathrm{Var}(Y_{it})}} \\right)\n$$\n由于 $\\mathbb{E}[Y_{it}]$ 和 $\\mathrm{Var}(Y_{it})$ 相对于定义方差的期望而言是常数，我们有：\n$$\nC^{(z)}_{ii} = \\frac{1}{\\left(\\sqrt{\\mathrm{Var}(Y_{it})}\\right)^2} \\mathrm{Var}(Y_{it} - \\mathbb{E}[Y_{it}]) = \\frac{1}{\\mathrm{Var}(Y_{it})} \\mathrm{Var}(Y_{it}) = 1\n$$\n这个结果是精确的，不依赖于任何近似。因此，异质性指数为：\n$$\nH^{(z)} = \\frac{C^{(z)}_{11}}{C^{(z)}_{22}} = \\frac{1}{1} = 1\n$$\n\n**$\\mathcal{R}(k)$ 比率的最终计算**\n问题要求计算比率 $\\mathcal{R}(k) = \\frac{H^{(\\mathrm{raw})}}{H^{(\\sqrt{\\cdot})}}$。使用我们推导出的主导阶表达式：\n$$\nH^{(\\mathrm{raw})} \\approx k^2\n$$\n$$\nH^{(\\sqrt{\\cdot})} \\approx k\n$$\n该比率为：\n$$\n\\mathcal{R}(k) = \\frac{k^2}{k} = k\n$$\n结果是一个仅用 $k$ 表示的闭式表达式。",
            "answer": "$$\\boxed{k}$$"
        },
        {
            "introduction": "基于前一个练习的理论洞察，本实践提供了一个具体的动手编码任务。您将构建一个在神经科学研究中广泛使用的标准预处理流程，该流程首先对脉冲计数数据应用平方根变换以稳定方差，然后对每个神经元的数据进行中心化。完成这些步骤对于确保PCA能够准确捕捉神经活动的潜在动态结构至关重要。",
            "id": "4187598",
            "problem": "给定代表在离散时间区间内同时记录的神经元的脉冲计数矩阵，用于使用主成分分析（PCA; Principal Component Analysis）进行降维和神经轨迹分析。构建一个确定性流程，该流程首先对脉冲计数应用平方根变换进行方差稳定化，然后在时间上对每个神经元进行中心化，最后计算汇总统计量以从数学上证明操作顺序的合理性。该设计必须从以下基本依据出发：对于一个率参数为 $\\lambda$ 的泊松分布随机变量 $X$，有 $\\mathbb{E}[X] = \\lambda$ 和 $\\operatorname{Var}(X) = \\lambda$。对于在 $X$ 处求值的光滑函数 $g$，当 $\\lambda$ 足够大时，Delta方法给出 $\\operatorname{Var}(g(X)) \\approx \\left(g'(\\lambda)\\right)^2 \\operatorname{Var}(X)$。此外，PCA期望中心化的特征：给定一个数据矩阵 $D$，其列为特征，行为观测值，PCA定义在零均值特征的协方差上，其主成分捕捉了最大方差的正交方向。\n\n您的程序必须为每个测试用例矩阵 $X \\in \\mathbb{N}_0^{N \\times T}$ 实现以下流程：\n\n- 通过带非负偏移量的平方根变换进行方差稳定化：对所有神经元 $i \\in \\{1,\\dots,N\\}$ 和时间区间 $t \\in \\{1,\\dots,T\\}$，计算 $Y_{i,t} = \\sqrt{X_{i,t} + \\alpha}$，其中 $\\alpha = \\tfrac{1}{2}$。此步骤旨在减少泊松模型下，方差对均值的依赖性。\n- 按神经元在时间上进行中心化：计算 $\\bar{y}_i = \\tfrac{1}{T} \\sum_{t=1}^T Y_{i,t}$ 并定义中心化数据 $Z_{i,t} = Y_{i,t} - \\bar{y}_i$，对所有 $i$ 和 $t$。\n- 变换前后的均值-方差相关性：对于原始计数，计算每个神经元的均值 $\\mu_i = \\tfrac{1}{T} \\sum_{t=1}^T X_{i,t}$ 和每个神经元的样本方差 $\\sigma_i^2 = \\tfrac{1}{T-1} \\sum_{t=1}^T (X_{i,t} - \\mu_i)^2$，然后计算向量 $(\\mu_i)_{i=1}^N$ 和 $(\\sigma_i^2)_{i=1}^N$ 之间的皮尔逊相关系数 $r_{\\text{pre}}$。对于变换后的计数，计算 $\\tilde{\\mu}_i = \\tfrac{1}{T} \\sum_{t=1}^T Y_{i,t}$ 和 $\\tilde{\\sigma}_i^2 = \\tfrac{1}{T-1} \\sum_{t=1}^T (Y_{i,t} - \\tilde{\\mu}_i)^2$，然后计算向量 $(\\tilde{\\mu}_i)_{i=1}^N$ 和 $(\\tilde{\\sigma}_i^2)_{i=1}^N$ 之间的皮尔逊相关系数 $r_{\\text{post}}$。如果任一向量的样本标准差为零或 $N  2$，则按惯例将相关性值定义为 $0.0$。\n- 中心化验证：验证 $Z$ 的每个神经元均值在容差 $\\epsilon = 10^{-12}$ 内是否为数值零；输出一个布尔值，指示是否 $\\max_{i} \\left| \\tfrac{1}{T} \\sum_{t=1}^T Z_{i,t} \\right| \\le \\epsilon$。\n- PCA方差解释率之和：将时间区间视为观测值，神经元视为特征，通过转置 $Z$ 得到 $Z^\\top \\in \\mathbb{R}^{T \\times N}$，对 $Z^\\top$ 应用奇异值分解（Singular Value Decomposition）得到奇异值 $(s_j)_{j=1}^{m}$，其中 $m = \\min(N, T)$，定义特征值 $\\lambda_j = \\tfrac{s_j^2}{T-1}$，并计算方差解释率 $\\rho_j = \\tfrac{\\lambda_j}{\\sum_{k=1}^{m} \\lambda_k}$。返回 $k=3$ 时的总和 $\\sum_{j=1}^{k} \\rho_j$。如果 $\\sum_{k=1}^{m} \\lambda_k = 0$，则按惯例将此和定义为 $0.0$。如果 $m  3$，则仅对可用分量求和。\n\n在您的代码注释和解决方案中，从数学上论证为什么平方根变换要在中心化之前进行：中心化原始计数会产生负值，使得平方根无法应用，并且它破坏了方差稳定化论证所需的泊松结构。同时论证PCA需要零均值特征，这通过在方差稳定化变换后进行中心化来实现。\n\n测试套件。您的程序必须评估以下四个测试用例，每个用例都以显式的 $N \\times T$ 整数矩阵形式给出：\n\n- 测试用例1 ($N = 4$, $T = 6$):\n  $$\n  X^{(1)} = \\begin{pmatrix}\n  0  1  0  2  1  0 \\\\\n  5  7  6  8  7  6 \\\\\n  0  0  0  0  1  0 \\\\\n  2  3  4  3  2  3\n  \\end{pmatrix}.\n  $$\n- 测试用例2 ($N = 3$, $T = 5$):\n  $$\n  X^{(2)} = \\begin{pmatrix}\n  0  0  0  0  0 \\\\\n  0  0  0  0  0 \\\\\n  0  0  0  0  0\n  \\end{pmatrix}.\n  $$\n- 测试用例3 ($N = 5$, $T = 7$):\n  $$\n  X^{(3)} = \\begin{pmatrix}\n  0  0  0  1  0  2  0 \\\\\n  10  12  11  9  13  12  11 \\\\\n  1  3  2  4  3  2  1 \\\\\n  0  5  0  5  0  5  0 \\\\\n  20  19  18  21  22  20  19\n  \\end{pmatrix}.\n  $$\n- 测试用例4 ($N = 6$, $T = 3$):\n  $$\n  X^{(4)} = \\begin{pmatrix}\n  0  1  0 \\\\\n  9  10  11 \\\\\n  0  0  0 \\\\\n  3  3  3 \\\\\n  1  2  3 \\\\\n  5  1  0\n  \\end{pmatrix}.\n  $$\n\n最终输出格式。您的程序应生成单行输出，其中包含四个测试用例的结果，格式为一个由方括号括起来的逗号分隔列表，其中每个元素本身是形如 $[r_{\\text{pre}}, r_{\\text{post}}, \\text{centered\\_ok}, \\sum_{j=1}^{k} \\rho_j]$（其中 $k=3$）的列表。例如，输出必须看起来像 $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]$，其中对于测试用例 $\\ell \\in \\{1,2,3,4\\}$，$a_\\ell$ 和 $b_\\ell$ 是浮点数，$c_\\ell$ 是布尔值，$d_\\ell$ 是浮点数。",
            "solution": "该问题要求为用于主成分分析（PCA）的神经脉冲计数矩阵构建并论证一个确定性的数据预处理流程。该流程涉及一个方差稳定化变换，后跟数据中心化。我们将首先基于统计学原理，为该流程的设计提供严谨的论证，然后概述具体的计算步骤。\n\n分析神经脉冲计数的一个核心前提是泊松模型。对于一个在给定时间区间内以平均速率 $\\lambda$ 发放脉冲的神经元，其脉冲计数 $X$ 可以被建模为一个泊松分布的随机变量，$X \\sim \\text{Poisson}(\\lambda)$。该分布的一个关键特性是均值等于方差：$\\mathbb{E}[X] = \\lambda$ 和 $\\operatorname{Var}(X) = \\lambda$。均值和方差的这种耦合关系对于许多标准统计方法（包括PCA）来说是有问题的，因为这些方法对不同特征（神经元）的相对尺度很敏感。一个平均发放率较高的神经元也会有较高的方差，这可能会主导分析，而与其对潜在神经动力学的贡献无关。\n\n因此，我们流程的第一个原则是**方差稳定化**。目标是变换数据，使得变换后变量的方差近似独立于其均值。问题陈述介绍了应用于随机变量 $X$ 的光滑函数 $g$ 的Delta方法：$\\operatorname{Var}(g(X)) \\approx (g'(\\mathbb{E}[X]))^2 \\operatorname{Var}(X)$。对于我们的泊松数据 $X$ (其 $\\mathbb{E}[X] = \\lambda$)，这变成 $\\operatorname{Var}(g(X)) \\approx (g'(\\lambda))^2 \\lambda$。指定的变换是 $g(x) = \\sqrt{x+\\alpha}$，其中常数偏移量 $\\alpha = \\frac{1}{2}$。其导数为 $g'(x) = \\frac{1}{2\\sqrt{x+\\alpha}}$。将此代入Delta方法近似式中可得：\n$$\n\\operatorname{Var}(\\sqrt{X+\\alpha}) \\approx \\left(\\frac{1}{2\\sqrt{\\lambda+\\alpha}}\\right)^2 \\lambda = \\frac{\\lambda}{4(\\lambda+\\alpha)}\n$$\n对于足够大的平均发放率 $\\lambda$，项 $\\lambda+\\alpha \\approx \\lambda$，变换后变量的方差趋近于一个常数：\n$$\n\\lim_{\\lambda \\to \\infty} \\frac{\\lambda}{4(\\lambda+\\alpha)} = \\frac{1}{4}\n$$\n因此，带有偏移量 $\\alpha=\\frac{1}{2}$ 的平方根变换（一种称为Anscombe变换的特定形式，对于小的 $\\lambda$ 具有良好性质）将方差与均值解耦。此步骤的有效性通过比较变换前（$r_{\\text{pre}}$）和变换后（$r_{\\text{post}}$）每个神经元的均值与方差之间的皮尔逊相关性来经验性地衡量。我们期望 $r_{\\text{pre}}$ 是一个较高的正值，而 $r_{\\text{post}}$ 应该接近于零。\n\n第二个原则是**为PCA进行数据中心化**。PCA旨在为数据找到一个能够捕捉最大方差方向的正交基。这些方向是数据协方差矩阵的特征向量。一组特征的样本协方差矩阵基本上是根据每个特征与均值的偏差来定义的。如果将PCA应用于未中心化的数据，第一个主成分通常由从原点到数据云质心（均值）的向量主导，而这个向量通常不包含关于数据内部变异结构的信息。因此，为确保PCA能捕捉到内在的方差结构，每个特征（神经元）在分析前都必须被中心化以使其均值为零。\n\n这些原则的整合决定了**操作的顺序**。方差稳定化变换必须在中心化之前应用。这种顺序有两个主要原因：\n1.  **定义域约束**：原始脉冲计数 $X_{i,t}$ 是非负整数。直接通过 $X_{i,t} - \\mu_i$ 对其进行中心化会产生负值。平方根变换 $g(x) = \\sqrt{x+\\alpha}$ 对于负参数（如果 $x+\\alpha  0$）在实数域上没有定义，这使得这种操作顺序无效。\n2.  **统计有效性**：平方根变换的理由是基于数据服从泊松分布。中心化操作改变了数据的分布；结果值不再是泊松分布的。在中心化后应用变换会使其所依据的统计推理无效。\n因此，正确且有原则的流程是：1) 对原始计数应用方差稳定化的平方根变换，然后 2) 对变换后的数据按神经元进行中心化。\n\n对于给定的脉冲计数矩阵 $X \\in \\mathbb{N}_0^{N \\times T}$，算法流程如下：\n1.  **变换前分析**：对每个神经元 $i \\in \\{1, \\dots, N\\}$，计算样本均值 $\\mu_i = \\frac{1}{T}\\sum_{t=1}^T X_{i,t}$ 和样本方差 $\\sigma_i^2 = \\frac{1}{T-1}\\sum_{t=1}^T(X_{i,t} - \\mu_i)^2$。计算均值向量 $(\\mu_i)_{i=1}^N$ 和方差向量 $(\\sigma_i^2)_{i=1}^N$ 之间的皮尔逊相关系数 $r_{\\text{pre}}$。\n2.  **方差稳定化**：计算变换后的矩阵 $Y$，其中 $Y_{i,t} = \\sqrt{X_{i,t} + \\alpha}$，$\\alpha=\\frac{1}{2}$。\n3.  **变换后分析**：对每个神经元 $i$，计算新的样本均值 $\\tilde{\\mu}_i = \\frac{1}{T}\\sum_{t=1}^T Y_{i,t}$ 和方差 $\\tilde{\\sigma}_i^2 = \\frac{1}{T-1}\\sum_{t=1}^T(Y_{i,t} - \\tilde{\\mu}_i)^2$。计算向量 $(\\tilde{\\mu}_i)_{i=1}^N$ 和 $(\\tilde{\\sigma}_i^2)_{i=1}^N$ 之间的相关性 $r_{\\text{post}}$。\n4.  **中心化与验证**：计算中心化矩阵 $Z$，其中 $Z_{i,t} = Y_{i,t} - \\tilde{\\mu}_i$。通过检查是否 $\\max_i |\\frac{1}{T}\\sum_{t=1}^T Z_{i,t}| \\le \\epsilon$（对于一个小的容差 $\\epsilon=10^{-12}$）来验证 $Z$ 的行均值是否在数值上为零。\n5.  **PCA方差解释率**：PCA在神经元为特征、时间点为观测值的数据矩阵上执行。这对应于我们中心化矩阵的转置 $Z^\\top \\in \\mathbb{R}^{T \\times N}$。主成分方差是样本协方差矩阵 $C = \\frac{1}{T-1}(Z^\\top)^\\top Z^\\top = \\frac{1}{T-1} Z Z^\\top$ 的特征值。一种计算上稳定的寻找这些特征值的方法是通过对 $Z^\\top$ 进行奇异值分解（SVD）。如果 $Z^\\top$ 的奇异值为 $(s_j)_{j=1}^m$ (其中 $m=\\min(N,T)$)，那么 $C$ 的特征值由 $\\lambda_j = \\frac{s_j^2}{T-1}$ 给出。第 $j$ 个分量的方差解释率是 $\\rho_j = \\lambda_j / \\sum_{k=1}^m \\lambda_k$。最终要求的统计量是前 $k=3$ 个分量的这些比率的累积和，即 $\\sum_{j=1}^{\\min(3, m)} \\rho_j$。\n\n这一系列操作构成了一个完整、自洽且有科学依据的流程，用于为降维任务预处理神经脉冲计数数据。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a deterministic pipeline for preprocessing neural spike count data.\n    The pipeline includes variance stabilization, centering, and PCA-related summary statistics.\n    \"\"\"\n    \n    # Test cases are defined as N x T integer matrices, where N is the number of neurons\n    # and T is the number of time bins.\n    test_cases = [\n        np.array([\n            [0, 1, 0, 2, 1, 0],\n            [5, 7, 6, 8, 7, 6],\n            [0, 0, 0, 0, 1, 0],\n            [2, 3, 4, 3, 2, 3]\n        ], dtype=int),\n        np.array([\n            [0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0]\n        ], dtype=int),\n        np.array([\n            [0, 0, 0, 1, 0, 2, 0],\n            [10, 12, 11, 9, 13, 12, 11],\n            [1, 3, 2, 4, 3, 2, 1],\n            [0, 5, 0, 5, 0, 5, 0],\n            [20, 19, 18, 21, 22, 20, 19]\n        ], dtype=int),\n        np.array([\n            [0, 1, 0],\n            [9, 10, 11],\n            [0, 0, 0],\n            [3, 3, 3],\n            [1, 2, 3],\n            [5, 1, 0]\n        ], dtype=int),\n    ]\n\n    results = []\n    \n    # Define constants from the problem statement.\n    alpha = 0.5\n    epsilon = 1e-12\n    k = 3\n\n    def calculate_correlation(vec1, vec2):\n        \"\"\"\n        Calculates the Pearson correlation between two vectors.\n        Handles edge cases as per problem description (N  2 or constant vectors).\n        \"\"\"\n        # The length of the vectors is N, the number of neurons.\n        if vec1.shape[0]  2:\n            return 0.0\n        \n        # If either vector is constant, its standard deviation is 0.\n        # This makes Pearson correlation undefined. The problem specifies to return 0.0.\n        if np.std(vec1) == 0.0 or np.std(vec2) == 0.0:\n            return 0.0\n        \n        # np.corrcoef is a standard way to compute the correlation coefficient.\n        # It returns a 2x2 matrix, with the correlation at [0, 1] and [1, 0].\n        corr_matrix = np.corrcoef(vec1, vec2)\n        return corr_matrix[0, 1]\n\n    for X in test_cases:\n        N, T = X.shape\n\n        # The order of operations is critical: transform first, then center.\n        # 1. Transform: The square-root transform is based on the data's\n        #    (assumed) Poisson statistics. Applying it to raw counts is correct.\n        # 2. Center: PCA requires zero-mean features. This is done after the\n        #    transform. Centering first would create negative numbers, making the\n        #    square-root transform ill-defined for real-valued outputs and break\n        #    the statistical assumptions for variance stabilization.\n\n        # --- Pre- and post-transform mean-variance correlation ---\n        \n        # Calculate stats for raw counts (pre-transform)\n        # Handle case T=1 for variance calculation to avoid division by zero.\n        if T > 1:\n            mu_pre = np.mean(X, axis=1)\n            var_pre = np.var(X, axis=1, ddof=1) # Use sample variance (ddof=1)\n            r_pre = calculate_correlation(mu_pre, var_pre)\n        else: # If T=1, variance is undefined or 0.\n            mu_pre = X.flatten()\n            var_pre = np.zeros_like(mu_pre)\n            r_pre = 0.0\n        \n        # Step 1: Variance stabilization via square-root transform\n        Y = np.sqrt(X + alpha)\n\n        # Calculate stats for transformed counts (post-transform)\n        if T > 1:\n            mu_post = np.mean(Y, axis=1)\n            var_post = np.var(Y, axis=1, ddof=1)\n            r_post = calculate_correlation(mu_post, var_post)\n        else:\n            mu_post = Y.flatten()\n            var_post = np.zeros_like(mu_post)\n            r_post = 0.0\n\n        # Step 2: Per-neuron centering across time\n        # keepdims=True ensures that the mean vector (N, 1) correctly broadcasts\n        # for subtraction from the data matrix (N, T).\n        neuron_means_Y = np.mean(Y, axis=1, keepdims=True)\n        Z = Y - neuron_means_Y\n\n        # --- Centering verification ---\n        Z_row_means = np.mean(Z, axis=1)\n        centered_ok = np.max(np.abs(Z_row_means)) = epsilon\n\n        # --- PCA explained variance ratio sum ---\n        # PCA operates on a matrix of (observations x features). Here, that is (time x neurons).\n        # Thus, we use the transpose of Z.\n        D = Z.T\n        m = min(N, T)\n\n        sum_evr = 0.0\n        # SVD and eigenvalue calculation require T > 1.\n        if T > 1:\n            # Singular Value Decomposition is a stable way to get principal component variances.\n            s = np.linalg.svd(D, compute_uv=False)\n            \n            # Eigenvalues of the covariance matrix are related to the singular values.\n            # lambda_j = s_j^2 / (T-1)\n            lambdas = (s**2) / (T - 1)\n            \n            total_lambda = np.sum(lambdas)\n            \n            # If total variance is zero, the ratio is defined as 0.\n            if total_lambda > 0:\n                explained_variance_ratios = lambdas / total_lambda\n                \n                # Sum the ratios for the top k components, or fewer if not available.\n                num_components_to_sum = min(k, m)\n                sum_evr = np.sum(explained_variance_ratios[:num_components_to_sum])\n\n        results.append([r_pre, r_post, centered_ok, sum_evr])\n\n    # Final print statement must match the exact specified format.\n    # The str() of a list gives its representation, e.g., '[1, 2, 3]',\n    # which is what's needed here.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在完成数据预处理并运行PCA之后，一个根本性的挑战依然存在：得到的主成分中，有多少个反映了真实的神经信号，又有多少仅仅是随机噪声？本练习介绍了一种源于随机矩阵理论的强大方法，用于区分信号与噪声维度。通过将样本协方差矩阵的特征值谱与理论预测的噪声谱进行比较，您可以对神经活动中的真实信号维度做出有原则的估计。",
            "id": "4187623",
            "problem": "您将获得一个用于神经群体活动的合成高维时间序列模型，该模型旨在反映嵌入在附加测量噪声中的低维神经轨迹。数据由矩阵 $X \\in \\mathbb{R}^{N \\times T}$ 表示，其中 $N$ 表示神经元的数量，$T$ 表示时间点的数量。主成分分析（PCA）的协方差估计量是样本协方差 $C = \\frac{1}{T} X X^\\top$。对于一个基线模型，其神经元和时间上的噪声是独立的、均值为零、方差为 $\\sigma^2$ 的高斯噪声，此时纯噪声的样本协方差是一个Wishart矩阵。随机矩阵理论中一个经过充分检验的事实指出，在 $N$ 和 $T$ 以固定纵横比 $q = N/T$ 共同增长的联合极限下，纯噪声协方差的特征值分布会收敛于Marchenko–Pastur (MP) 分布，其支撑集为一个区间，该区间的上界取决于 $q$ 和 $\\sigma^2$。低秩信号轨迹会在 $C$ 中产生有限数量的离群特征值，这些特征值可能超过MP支撑集的上界，从而可以估计信号的维度。\n\n实现一个程序，该程序：\n- 根据模型 $X = S + \\varepsilon$ 构建合成数据集，其中 $S = W Z$，$\\varepsilon$ 的各元素独立抽取。\n- 使用样本协方差 $C$ 及其特征值，来估计信号主成分中位于Marchenko–Pastur分布预测的噪声主体之上（由给定的 $N$、$T$ 和 $\\sigma^2$ 所确定的相应上界决定）的数量。\n- 对每个测试用例，计算 $C$ 的特征值中超过此上界的数量，并返回该计数。\n\n每个测试用例的合成数据规范：\n- $W \\in \\mathbb{R}^{N \\times r}$ 具有 $r$ 个标准正交列，通过对 $r$ 个独立同分布的标准正态列进行Gram–Schmidt过程（实现为$\\mathrm{QR}$分解）来构建。\n- $Z \\in \\mathbb{R}^{r \\times T}$ 具有独立的高斯行，其中第 $j$ 行的元素服从 $\\mathcal{N}(0, \\sqrt{s_j^2})$ 分布，因此第 $j$ 个潜在轨迹的时间方差为 $s_j^2$。每个测试用例都会提供列表 $[s_1^2, s_2^2, \\dots, s_r^2]$。\n- $\\varepsilon \\in \\mathbb{R}^{N \\times T}$ 的元素独立地服从 $\\mathcal{N}(0, \\sqrt{\\sigma^2})$ 分布。\n- 样本协方差为 $C = \\frac{1}{T} X X^\\top$。\n- 计算 $C$ 的特征值，并与MP上界进行比较；任何严格大于MP上界的特征值都被计为一个检测到的信号主成分。\n\n您的程序必须实现以下测试套件，使用提供的确切参数值和随机种子，并且必须以指定格式为每个用例生成计数。\n\n测试套件：\n1. 用例 A（理想情况，多个强信号）：$N = 50$，$T = 200$，$\\sigma^2 = 1.0$，$r = 3$，$[s_1^2, s_2^2, s_3^2] = [6.0, 4.0, 2.0]$，随机种子 $42$。\n2. 用例 B（纯噪声，$N > T$ 的边界情况）：$N = 200$，$T = 100$，$\\sigma^2 = 1.0$，$r = 0$，空信号列表 $[\\ ]$，随机种子 $123$。\n3. 用例 C（大 $T$，不同信号强度）：$N = 100$，$T = 500$，$\\sigma^2 = 0.5$，$r = 5$，$[s_1^2, s_2^2, s_3^2, s_4^2, s_5^2] = [1.0, 1.2, 2.5, 0.8, 3.0]$，随机种子 $2024$。\n4. 用例 D（非常小的 $T$，信号低于可检测性）：$N = 120$，$T = 20$，$\\sigma^2 = 1.0$，$r = 2$，$[s_1^2, s_2^2] = [3.0, 4.0]$，随机种子 $7$。\n5. 用例 E（方形情况，混合强度）：$N = 75$，$T = 75$，$\\sigma^2 = 0.8$，$r = 2$，$[s_1^2, s_2^2] = [4.0, 0.5]$，随机种子 $99$。\n\n算法要求：\n- 对每个用例，根据上述规则使用指定的种子构造 $W$、$Z$ 和 $\\varepsilon$，以确保可复现性。\n- 形成 $X = W Z + \\varepsilon$ 并计算 $C = \\frac{1}{T} X X^\\top$。\n- 计算 $C$ 的特征值，并确定适用于该用例的 $N$、$T$ 和 $\\sigma^2$ 的Marchenko–Pastur上界。\n- 计算严格大于此上界的 $C$ 的特征值数量。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如，“[$\\mathrm{result}_1$, $\\mathrm{result}_2$, $\\mathrm{result}_3$, $\\mathrm{result}_4$, $\\mathrm{result}_5$]”。\n- 每个 $\\mathrm{result}_i$ 必须是一个整数，表示用例 $i$ 检测到的信号主成分的计数。",
            "solution": "该问题要求估计嵌入在高维噪声中的低秩信号的维度。数据遵循模型 $X = S + \\varepsilon$，其中 $X \\in \\mathbb{R}^{N \\times T}$ 是数据矩阵，$S \\in \\mathbb{R}^{N \\times T}$ 是低秩信号矩阵，而 $\\varepsilon \\in \\mathbb{R}^{N \\times T}$ 是加性噪声矩阵。在此，$N$ 是观测变量（例如，神经元）的数量，$T$ 是时间样本的数量。任务是通过分析样本协方差矩阵 $C = \\frac{1}{T} X X^\\top$ 的特征值来确定信号的秩 $r$。\n\n该方法基于随机矩阵理论（RMT）的一个关键结果。对于一个仅由独立同分布（i.i.d.）且方差为 $\\sigma^2$ 的噪声组成的数据矩阵，在 $N, T \\to \\infty$ 且其比率 $q = N/T$ 保持固定的极限下，样本协方差矩阵 $C_{noise} = \\frac{1}{T} \\varepsilon \\varepsilon^\\top$ 的经验谱分布收敛于Marchenko-Pastur (MP) 分布。该分布的支撑集是一个连续区间，通常被称为“噪声主体”。该支撑集的上界 $\\lambda_+$ 由以下公式给出：\n$$ \\lambda_+ = \\sigma^2 (1 + \\sqrt{q})^2 $$\n当向噪声中添加低秩信号 $S$ 时，在某些条件下，完整协方差矩阵 $C$ 的一些特征值将从噪声主体中分离出来，成为严格大于 $\\lambda_+$ 的“离群”特征值。这些离群值的数量对应于信号的维度（秩）$r$。问题要求我们为几个合成数据集计算这些离群值的数量。\n\n每个测试用例的算法步骤如下：\n\n1.  **初始化参数**：对于每个用例，我们都给定维度 $N$ 和 $T$、噪声方差 $\\sigma^2$、信号秩 $r$ 及相关的信号分量方差 $[s_1^2, s_2^2, \\dots, s_r^2]$，以及一个用于可复现性的随机种子。\n\n2.  **生成合成数据**：\n    -   **信号矩阵构造**：信号矩阵表示为 $S = WZ$。\n        -   矩阵 $W \\in \\mathbb{R}^{N \\times r}$ 为信号子空间提供了一组 $r$ 个标准正交基向量。它通过首先生成一个 $N \\times r$ 的矩阵（其元素为来自标准正态分布 $\\mathcal{N}(0, 1)$ 的独立同分布样本），然后对其列进行标准正交化来构建。这种标准正交化是通过QR分解实现的，其中 $W$ 是得到的 $Q$ 因子。\n        -   矩阵 $Z \\in \\mathbb{R}^{r \\times T}$ 包含信号分量的时间动态。其各行相互独立。第 $j$ 行的元素是来自高斯分布 $\\mathcal{N}(0, s_j)$ 的独立同分布样本，其中 $s_j^2$ 是第 $j$ 个潜在轨迹的指定方差。\n    -   **噪声矩阵构造**：噪声矩阵 $\\varepsilon \\in \\mathbb{R}^{N \\times T}$ 的每个元素都是从均值为0、方差为 $\\sigma^2$ 的高斯分布（表示为 $\\mathcal{N}(0, \\sigma)$）中独立抽取的。\n    -   **数据矩阵组合**：最终的数据矩阵由信号和噪声分量相加而成：$X = WZ + \\varepsilon$。在 $r=0$ 的特殊情况下，信号 $S$ 就是一个零矩阵。\n\n3.  **协方差和特征值计算**：\n    -   样本协方差矩阵计算为 $C = \\frac{1}{T} X X^\\top$。这是一个 $N \\times N$ 的半正定矩阵。\n    -   $C$ 的特征值随后被计算出来。由于 $C$ 是对称的，因此使用专门且高效的数值算法。为了计算效率，如果 $N > T$，计算较小的 $T \\times T$ 矩阵 $\\frac{1}{T} X^\\top X$ 的特征值会更有优势。$XX^\\top$ 和 $X^\\top X$ 的非零特征值集合是相同的。由于阈值 $\\lambda_+$ 总是正的，我们使用此优化不会有错误计算离群值的风险。\n\n4.  **Marchenko-Pastur边界计算**：\n    -   纵横比 $q$ 计算为 $q = N/T$。\n    -   噪声特征值谱的理论上界 $\\lambda_+$ 使用公式 $\\lambda_+ = \\sigma^2 (1 + \\sqrt{q})^2$ 计算。此公式对任何 $q > 0$ 均有效。\n\n5.  **信号维度估计**：\n    -   将计算出的样本协方差矩阵的特征值与阈值 $\\lambda_+$ 进行比较。\n    -   估计的信号维度是严格大于 $\\lambda_+$ 的特征值的总数。\n\n对问题陈述中定义的五个测试用例中的每一个都实施此过程。使用指定的随机种子可确保每次运行生成的合成数据完全相同，从而使结果完全可复现。例如，对于用例E，参数为 $N=75$，$T=75$，$\\sigma^2 = 0.8$，信号方差为 $[4.0, 0.5]$，纵横比为 $q=1$，MP上界为 $\\lambda_+ = 0.8(1+\\sqrt{1})^2 = 3.2$。RMT预测，如果一个信号分量的总体方差 $s_j^2$ 满足 $s_j^2/\\sigma^2 > \\sqrt{q}$，它将形成一个离群值。只有第一个信号分量（$4.0/0.8 = 5 > 1$）满足此条件，所以我们预期会找到一个离群值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by iterating through test cases, generating synthetic\n    neural data, and estimating signal dimensionality using random matrix theory.\n    \"\"\"\n\n    def estimate_dimensionality(N, T, sigma2, signal_variances, seed):\n        \"\"\"\n        Constructs synthetic data and estimates signal dimensionality by counting\n        eigenvalues of the sample covariance matrix above the Marchenko-Pastur edge.\n\n        Args:\n            N (int): Number of neurons.\n            T (int): Number of time points.\n            sigma2 (float): Variance of the additive noise.\n            signal_variances (list of float): Variances of the latent signal trajectories.\n            seed (int): Random seed for reproducibility.\n\n        Returns:\n            int: The estimated number of signal dimensions (outlier eigenvalues).\n        \"\"\"\n        # Use a random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n        r = len(signal_variances)\n\n        # 1. Construct Signal Matrix S = WZ\n        if r > 0:\n            # Construct W, an orthonormal basis for the signal subspace\n            # by applying QR decomposition to a random Gaussian matrix.\n            A = rng.standard_normal(size=(N, r))\n            W, _ = np.linalg.qr(A)\n\n            # Construct Z, the temporal components of the signal.\n            # Each row j is scaled by the standard deviation sqrt(s_j^2).\n            s = np.sqrt(np.array(signal_variances))\n            Z_std = rng.standard_normal(size=(r, T))\n            Z = s[:, np.newaxis] * Z_std\n\n            S = W @ Z\n        else:\n            # For the pure noise case (r=0), the signal is a zero matrix.\n            S = np.zeros((N, T))\n\n        # 2. Construct Noise Matrix epsilon\n        std_noise = np.sqrt(sigma2)\n        epsilon = std_noise * rng.standard_normal(size=(N, T))\n\n        # 3. Construct Data Matrix X\n        X = S + epsilon\n\n        # 4. Compute Covariance and Eigenvalues\n        # To optimize, we compute eigenvalues from the smaller of the two matrices\n        # XX^T (N x N) and X^T X (T x T), as their non-zero eigenvalues are identical.\n        if N > T:\n            # Form the T x T matrix\n            cov_matrix = (X.T @ X) / T\n        else:\n            # Form the N x N matrix\n            cov_matrix = (X @ X.T) / T\n        \n        # Eigenvalues of the symmetric covariance matrix\n        eigenvalues = np.linalg.eigvalsh(cov_matrix)\n\n        # 5. Compute Marchenko-Pastur Upper Edge\n        q = N / T\n        mp_edge = sigma2 * (1 + np.sqrt(q))**2\n\n        # 6. Count Outlier Eigenvalues\n        # The number of signal components is the count of eigenvalues strictly\n        # greater than the theoretical maximum for a noise-only matrix.\n        num_outliers = np.sum(eigenvalues > mp_edge)\n\n        return int(num_outliers)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Happy path, multiple strong signals\n        {'N': 50, 'T': 200, 'sigma2': 1.0, 'signal_variances': [6.0, 4.0, 2.0], 'seed': 42},\n        # Case B: Pure noise, N > T boundary\n        {'N': 200, 'T': 100, 'sigma2': 1.0, 'signal_variances': [], 'seed': 123},\n        # Case C: Large T, diverse signal strengths\n        {'N': 100, 'T': 500, 'sigma2': 0.5, 'signal_variances': [1.0, 1.2, 2.5, 0.8, 3.0], 'seed': 2024},\n        # Case D: Very small T, signals at detectability limit\n        {'N': 120, 'T': 20, 'sigma2': 1.0, 'signal_variances': [3.0, 4.0], 'seed': 7},\n        # Case E: Square case, mixed strengths (one sub-critical)\n        {'N': 75, 'T': 75, 'sigma2': 0.8, 'signal_variances': [4.0, 0.5], 'seed': 99},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = estimate_dimensionality(\n            N=case['N'],\n            T=case['T'],\n            sigma2=case['sigma2'],\n            signal_variances=case['signal_variances'],\n            seed=case['seed']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}