{
    "hands_on_practices": [
        {
            "introduction": "在将主成分分析（PCA）应用于神经尖峰计数数据之前，正确的预处理是至关重要的一步。原始尖峰计数通常遵循泊松分布，其方差与均值耦合，这会使高发放率的神经元在分析中占据不成比例的主导地位。本练习将指导您构建一个包含方差稳定化和数据中心化的标准预处理流程，并从数学上论证操作顺序的合理性 。",
            "id": "4187598",
            "problem": "给定代表在离散时间区间内同时记录的神经元的脉冲计数矩阵，旨在利用主成分分析（PCA; Principal Component Analysis）进行降维和神经轨迹分析。构建一个确定性的处理流程，该流程首先对脉冲计数应用平方根变换以稳定方差，然后按时间对每个神经元进行中心化，最后计算汇总统计量，以从数学上证明操作顺序的合理性。设计必须基于以下基本原理：对于一个率参数为 $\\lambda$ 的泊松分布随机变量 $X$，有 $\\mathbb{E}[X] = \\lambda$ 和 $\\operatorname{Var}(X) = \\lambda$；对于在 $X$ 处求值的平滑函数 $g$，当 $\\lambda$ 足够大时，Delta 方法给出 $\\operatorname{Var}(g(X)) \\approx \\left(g'(\\lambda)\\right)^2 \\operatorname{Var}(X)$。此外，PCA 要求特征是中心化的：给定一个数据矩阵 $D$，其特征为列，观测为行，PCA 定义在零均值特征的协方差上，其主成分捕捉方差最大的正交方向。\n\n您的程序必须为每个测试用例矩阵 $X \\in \\mathbb{N}_0^{N \\times T}$ 实现以下流程：\n\n- 通过带非负偏移量的平方根变换稳定方差：对所有神经元 $i \\in \\{1,\\dots,N\\}$ 和时间区间 $t \\in \\{1,\\dots,T\\}$，使用 $\\alpha = \\tfrac{1}{2}$ 计算 $Y_{i,t} = \\sqrt{X_{i,t} + \\alpha}$。此步骤旨在减弱泊松模型下，方差对均值的依赖性。\n- 按时间对每个神经元进行中心化：计算 $\\bar{y}_i = \\tfrac{1}{T} \\sum_{t=1}^T Y_{i,t}$ 并定义中心化后的数据 $Z_{i,t} = Y_{i,t} - \\bar{y}_i$，对所有 $i$ 和 $t$。\n- 变换前后的均值-方差相关性：对于原始计数，计算每个神经元的均值 $\\mu_i = \\tfrac{1}{T} \\sum_{t=1}^T X_{i,t}$ 和样本方差 $\\sigma_i^2 = \\tfrac{1}{T-1} \\sum_{t=1}^T (X_{i,t} - \\mu_i)^2$，然后计算向量 $(\\mu_i)_{i=1}^N$ 和 $(\\sigma_i^2)_{i=1}^N$ 之间的皮尔逊相关系数 $r_{\\text{pre}}$。对于变换后的计数，计算 $\\tilde{\\mu}_i = \\tfrac{1}{T} \\sum_{t=1}^T Y_{i,t}$ 和 $\\tilde{\\sigma}_i^2 = \\tfrac{1}{T-1} \\sum_{t=1}^T (Y_{i,t} - \\tilde{\\mu}_i)^2$，然后计算 $(\\tilde{\\mu}_i)_{i=1}^N$ 和 $(\\tilde{\\sigma}_i^2)_{i=1}^N$ 之间的皮尔逊相关系数 $r_{\\text{post}}$。如果任一向量的样本标准差为零，或者如果 $N  2$，则按约定将相关性值定义为 $0.0$。\n- 中心化验证：验证 $Z$ 的逐神经元均值在容差 $\\epsilon = 10^{-12}$ 内是否为数值零；输出一个布尔值，指示是否满足 $\\max_{i} \\left| \\tfrac{1}{T} \\sum_{t=1}^T Z_{i,t} \\right| \\le \\epsilon$。\n- PCA 方差解释比之和：通过转置 $Z$ 得到 $Z^\\top \\in \\mathbb{R}^{T \\times N}$，将时间区间视为观测值，神经元视为特征，对 $Z^\\top$ 应用奇异值分解（Singular Value Decomposition）得到奇异值 $(s_j)_{j=1}^{m}$（其中 $m = \\min(N, T)$），定义特征值 $\\lambda_j = \\tfrac{s_j^2}{T-1}$，并计算方差解释比 $\\rho_j = \\tfrac{\\lambda_j}{\\sum_{k=1}^{m} \\lambda_k}$。返回 $k = 3$ 时的总和 $\\sum_{j=1}^{k} \\rho_j$。如果 $\\sum_{k=1}^{m} \\lambda_k = 0$，则按约定将此总和定义为 $0.0$。如果 $m  3$，则仅对可用成分求和。\n\n在您的代码注释和解决方案中，从数学上论证为何平方根变换先于中心化执行：中心化原始计数会产生负值，使平方根无法应用，并且它会破坏方差稳定化论证所需的泊松结构。同时论证 PCA 需要零均值特征，这通过在方差稳定化变换之后进行中心化来实现。\n\n测试套件。您的程序必须评估以下四个测试用例，每个测试用例均以显式的 $N \\times T$ 整数矩阵形式给出：\n\n- 测试用例 1 ($N = 4$, $T = 6$):\n  $$\n  X^{(1)} = \\begin{pmatrix}\n  0  1  0  2  1  0 \\\\\n  5  7  6  8  7  6 \\\\\n  0  0  0  0  1  0 \\\\\n  2  3  4  3  2  3\n  \\end{pmatrix}.\n  $$\n- 测试用例 2 ($N = 3$, $T = 5$):\n  $$\n  X^{(2)} = \\begin{pmatrix}\n  0  0  0  0  0 \\\\\n  0  0  0  0  0 \\\\\n  0  0  0  0  0\n  \\end{pmatrix}.\n  $$\n- 测试用例 3 ($N = 5$, $T = 7$):\n  $$\n  X^{(3)} = \\begin{pmatrix}\n  0  0  0  1  0  2  0 \\\\\n  10  12  11  9  13  12  11 \\\\\n  1  3  2  4  3  2  1 \\\\\n  0  5  0  5  0  5  0 \\\\\n  20  19  18  21  22  20  19\n  \\end{pmatrix}.\n  $$\n- 测试用例 4 ($N = 6$, $T = 3$):\n  $$\n  X^{(4)} = \\begin{pmatrix}\n  0  1  0 \\\\\n  9  10  11 \\\\\n  0  0  0 \\\\\n  3  3  3 \\\\\n  1  2  3 \\\\\n  5  1  0\n  \\end{pmatrix}.\n  $$\n\n最终输出格式。您的程序应生成单行输出，其中包含四个测试用例的结果，格式为一个逗号分隔的列表，并用方括号括起来，其中每个元素本身是一个形如 $[r_{\\text{pre}}, r_{\\text{post}}, \\text{centered\\_ok}, \\sum_{j=1}^{k} \\rho_j]$ 的列表，其中 $k = 3$。例如，输出必须类似于 $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]$，其中对于测试用例 $\\ell \\in \\{1,2,3,4\\}$，$a_\\ell$ 和 $b_\\ell$ 是浮点数，$c_\\ell$ 是布尔值，$d_\\ell$ 是浮点数。",
            "solution": "该问题要求为用于主成分分析（PCA）的神经脉冲计数矩阵构建并论证一个确定性的数据预处理流程。该流程包括一个方差稳定化变换，随后是数据中心化。我们将首先基于统计学原理，为该流程的设计提供严谨的论证，然后概述具体的计算步骤。\n\n分析神经脉冲计数的一个核心前提是泊松模型。对于在给定时间区间内以平均速率 $\\lambda$ 发放脉冲的神经元，其脉冲计数 $X$ 可以建模为泊松分布的随机变量，$X \\sim \\text{Poisson}(\\lambda)$。该分布的一个关键特性是其均值等于方差：$\\mathbb{E}[X] = \\lambda$ 和 $\\operatorname{Var}(X) = \\lambda$。均值与方差的这种耦合关系对于许多标准统计方法（包括 PCA）是有问题的，因为这些方法对不同特征（神经元）的相对尺度很敏感。平均发放率较高的神经元也将具有较高的方差，这可能导致它在分析中占据主导地位，而与其对潜在神经动力学的贡献无关。\n\n因此，我们流程的第一个原则是**方差稳定化**。目标是变换数据，使得变换后变量的方差近似独立于其均值。问题描述引入了应用于随机变量 $X$ 的平滑函数 $g$ 的 Delta 方法：$\\operatorname{Var}(g(X)) \\approx (g'(\\mathbb{E}[X]))^2 \\mathrm{Var}(X)$。对于我们的泊松数据 $X$（其中 $\\mathbb{E}[X] = \\lambda$），这变为 $\\operatorname{Var}(g(X)) \\approx (g'(\\lambda))^2 \\lambda$。\n指定的变换是 $g(x) = \\sqrt{x+\\alpha}$，其中常数偏移量 $\\alpha = \\frac{1}{2}$。其导数为 $g'(x) = \\frac{1}{2\\sqrt{x+\\alpha}}$。将此代入 Delta 方法的近似公式中，可得：\n$$\n\\operatorname{Var}(\\sqrt{X+\\alpha}) \\approx \\left(\\frac{1}{2\\sqrt{\\lambda+\\alpha}}\\right)^2 \\lambda = \\frac{\\lambda}{4(\\lambda+\\alpha)}\n$$\n对于足够大的平均发放率 $\\lambda$，项 $\\lambda+\\alpha \\approx \\lambda$，变换后变量的方差接近一个常数：\n$$\n\\lim_{\\lambda \\to \\infty} \\frac{\\lambda}{4(\\lambda+\\alpha)} = \\frac{1}{4}\n$$\n因此，带有偏移量 $\\alpha=\\frac{1}{2}$ 的平方根变换（一种被称为 Anscombe 变换的特定形式，对较小的 $\\lambda$ 具有良好性质）将方差与均值解耦。此步骤的有效性通过比较变换前（$r_{\\text{pre}}$）和变换后（$r_{\\text{post}}$）逐神经元均值与方差之间的皮尔逊相关性来凭经验衡量。我们期望 $r_{\\text{pre}}$ 会很高且为正，而 $r_{\\text{post}}$ 应接近于零。\n\n第二个原则是**为 PCA 进行数据中心化**。PCA 旨在为数据找到一个能够捕捉最大方差方向的正交基。这些方向是数据协方差矩阵的特征向量。一组特征的样本协方差矩阵基本上是根据每个特征与其均值的偏差来定义的。如果将 PCA 应用于未中心化的数据，第一个主成分通常由从原点到数据云质心（均值）的向量主导，这通常不包含关于数据内部变异结构的信息。因此，为确保 PCA 能够捕捉内在的方差结构，在分析之前必须将每个特征（神经元）中心化，使其均值为零。\n\n这些原则的整合决定了**操作的顺序**。方差稳定化变换必须在中心化之前应用。此顺序有两个主要原因：\n1.  **定义域约束**：原始脉冲计数 $X_{i,t}$ 是非负整数。直接通过 $X_{i,t} - \\mu_i$ 对其进行中心化会产生负值。平方根变换 $g(x) = \\sqrt{x+\\alpha}$ 对于负参数（如果 $x+\\alpha  0$）在实数范围内没有定义，这使得该操作顺序无效。\n2.  **统计有效性**：平方根变换的理由是基于数据服从泊松分布。中心化操作会改变数据的分布；结果值不再是泊松分布的。在中心化后应用变换将使其所依据的统计推理失效。\n因此，正确且有原则的流程是：1) 对原始计数应用方差稳定的平方根变换，然后 2) 按逐神经元的方式对变换后的数据进行中心化。\n\n对于给定的脉冲计数矩阵 $X \\in \\mathbb{N}_0^{N \\times T}$，算法流程如下：\n1.  **变换前分析**：对于每个神经元 $i \\in \\{1, \\dots, N\\}$，计算样本均值 $\\mu_i = \\frac{1}{T}\\sum_{t=1}^T X_{i,t}$ 和样本方差 $\\sigma_i^2 = \\frac{1}{T-1}\\sum_{t=1}^T(X_{i,t} - \\mu_i)^2$。计算均值向量 $(\\mu_i)_{i=1}^N$ 和方差向量 $(\\sigma_i^2)_{i=1}^N$ 之间的皮尔逊相关系数 $r_{\\text{pre}}$。\n2.  **方差稳定化**：计算变换后的矩阵 $Y$，其中 $Y_{i,t} = \\sqrt{X_{i,t} + \\alpha}$，$\\alpha=\\frac{1}{2}$。\n3.  **变换后分析**：对于每个神经元 $i$，计算新的样本均值 $\\tilde{\\mu}_i = \\frac{1}{T}\\sum_{t=1}^T Y_{i,t}$ 和方差 $\\tilde{\\sigma}_i^2 = \\frac{1}{T-1}\\sum_{t=1}^T(Y_{i,t} - \\tilde{\\mu}_i)^2$。计算向量 $(\\tilde{\\mu}_i)_{i=1}^N$ 和 $(\\tilde{\\sigma}_i^2)_{i=1}^N$ 之间的相关性 $r_{\\text{post}}$。\n4.  **中心化与验证**：计算中心化矩阵 $Z$，其中 $Z_{i,t} = Y_{i,t} - \\tilde{\\mu}_i$。通过检查是否 $\\max_i |\\frac{1}{T}\\sum_{t=1}^T Z_{i,t}| \\le \\epsilon$（其中 $\\epsilon=10^{-12}$ 是一个很小的容差），来验证 $Z$ 的行均值是否在数值上为零。\n5.  **PCA 方差解释比**：PCA 在一个以神经元为特征、时间点为观测值的数据矩阵上执行。这对应于我们中心化矩阵的转置 $Z^\\top \\in \\mathbb{R}^{T \\times N}$。主成分方差是样本协方差矩阵 $C = \\frac{1}{T-1}(Z^\\top)^\\top Z^\\top = \\frac{1}{T-1} Z Z^\\top$ 的特征值。一种计算上稳定的方法是通过对 $Z^\\top$ 进行奇异值分解（SVD）来找到这些特征值。如果 $Z^\\top$ 的奇异值为 $(s_j)_{j=1}^m$（其中 $m=\\min(N,T)$），则 $C$ 的特征值由 $\\lambda_j = \\frac{s_j^2}{T-1}$ 给出。第 $j$ 个成分的方差解释比为 $\\rho_j = \\lambda_j / \\sum_{k=1}^m \\lambda_k$。最终需要的统计量是前 $k=3$ 个成分的这些比率的累积和，即 $\\sum_{j=1}^{\\min(3, m)} \\rho_j$。\n\n这一系列操作构成了一个完整、独立且有科学依据的流程，用于为降维目的预处理神经脉冲计数数据。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a deterministic pipeline for preprocessing neural spike count data.\n    The pipeline includes variance stabilization, centering, and PCA-related summary statistics.\n    \"\"\"\n    \n    # Test cases are defined as N x T integer matrices, where N is the number of neurons\n    # and T is the number of time bins.\n    test_cases = [\n        np.array([\n            [0, 1, 0, 2, 1, 0],\n            [5, 7, 6, 8, 7, 6],\n            [0, 0, 0, 0, 1, 0],\n            [2, 3, 4, 3, 2, 3]\n        ], dtype=int),\n        np.array([\n            [0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0]\n        ], dtype=int),\n        np.array([\n            [0, 0, 0, 1, 0, 2, 0],\n            [10, 12, 11, 9, 13, 12, 11],\n            [1, 3, 2, 4, 3, 2, 1],\n            [0, 5, 0, 5, 0, 5, 0],\n            [20, 19, 18, 21, 22, 20, 19]\n        ], dtype=int),\n        np.array([\n            [0, 1, 0],\n            [9, 10, 11],\n            [0, 0, 0],\n            [3, 3, 3],\n            [1, 2, 3],\n            [5, 1, 0]\n        ], dtype=int),\n    ]\n\n    results = []\n    \n    # Define constants from the problem statement.\n    alpha = 0.5\n    epsilon = 1e-12\n    k = 3\n\n    def calculate_correlation(vec1, vec2):\n        \"\"\"\n        Calculates the Pearson correlation between two vectors.\n        Handles edge cases as per problem description (N  2 or constant vectors).\n        \"\"\"\n        # The length of the vectors is N, the number of neurons.\n        if vec1.shape[0]  2:\n            return 0.0\n        \n        # If either vector is constant, its standard deviation is 0.\n        # This makes Pearson correlation undefined. The problem specifies to return 0.0.\n        if np.std(vec1) == 0.0 or np.std(vec2) == 0.0:\n            return 0.0\n        \n        # np.corrcoef is a standard way to compute the correlation coefficient.\n        # It returns a 2x2 matrix, with the correlation at [0, 1] and [1, 0].\n        corr_matrix = np.corrcoef(vec1, vec2)\n        return corr_matrix[0, 1]\n\n    for X in test_cases:\n        N, T = X.shape\n\n        # The order of operations is critical: transform first, then center.\n        # 1. Transform: The square-root transform is based on the data's\n        #    (assumed) Poisson statistics. Applying it to raw counts is correct.\n        # 2. Center: PCA requires zero-mean features. This is done after the\n        #    transform. Centering first would create negative numbers, making the\n        #    square-root transform ill-defined for real-valued outputs and break\n        #    the statistical assumptions for variance stabilization.\n\n        # --- Pre- and post-transform mean-variance correlation ---\n        \n        # Calculate stats for raw counts (pre-transform)\n        # Handle case T=1 for variance calculation to avoid division by zero.\n        if T  1:\n            mu_pre = np.mean(X, axis=1)\n            var_pre = np.var(X, axis=1, ddof=1) # Use sample variance (ddof=1)\n            r_pre = calculate_correlation(mu_pre, var_pre)\n        else: # If T=1, variance is undefined or 0.\n            mu_pre = X.flatten()\n            var_pre = np.zeros_like(mu_pre)\n            r_pre = 0.0\n        \n        # Step 1: Variance stabilization via square-root transform\n        Y = np.sqrt(X + alpha)\n\n        # Calculate stats for transformed counts (post-transform)\n        if T  1:\n            mu_post = np.mean(Y, axis=1)\n            var_post = np.var(Y, axis=1, ddof=1)\n            r_post = calculate_correlation(mu_post, var_post)\n        else:\n            mu_post = Y.flatten()\n            var_post = np.zeros_like(mu_post)\n            r_post = 0.0\n\n        # Step 2: Per-neuron centering across time\n        # keepdims=True ensures that the mean vector (N, 1) correctly broadcasts\n        # for subtraction from the data matrix (N, T).\n        neuron_means_Y = np.mean(Y, axis=1, keepdims=True)\n        Z = Y - neuron_means_Y\n\n        # --- Centering verification ---\n        Z_row_means = np.mean(Z, axis=1)\n        centered_ok = np.max(np.abs(Z_row_means)) = epsilon\n\n        # --- PCA explained variance ratio sum ---\n        # PCA operates on a matrix of (observations x features). Here, that is (time x neurons).\n        # Thus, we use the transpose of Z.\n        D = Z.T\n        m = min(N, T)\n\n        sum_evr = 0.0\n        # SVD and eigenvalue calculation require T  1.\n        if T  1:\n            # Singular Value Decomposition is a stable way to get principal component variances.\n            s = np.linalg.svd(D, compute_uv=False)\n            \n            # Eigenvalues of the covariance matrix are related to the singular values.\n            # lambda_j = s_j^2 / (T-1)\n            lambdas = (s**2) / (T - 1)\n            \n            total_lambda = np.sum(lambdas)\n            \n            # If total variance is zero, the ratio is defined as 0.\n            if total_lambda  0:\n                explained_variance_ratios = lambdas / total_lambda\n                \n                # Sum the ratios for the top k components, or fewer if not available.\n                num_components_to_sum = min(k, m)\n                sum_evr = np.sum(explained_variance_ratios[:num_components_to_sum])\n\n        results.append([r_pre, r_post, centered_ok, sum_evr])\n\n    # Final print statement must match the exact specified format.\n    # The str() of a list gives its representation, e.g., '[1, 2, 3]',\n    # which is what's needed here.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "上一个练习介绍了通过平方根变换来稳定方差的实用方法，但为什么这种方法优于其他看似合理的选择，例如直接对原始数据或z-score变换后的数据进行PCA？本练习将通过一个理论推导，深入探讨不同预处理策略对异质性发放率神经元的影响 。通过计算和比较不同变换下的方差结构，您将从根本上理解为何需要进行特定的方差稳定化处理。",
            "id": "4187658",
            "problem": "一个实验室记录了在多个时间点上，沿着一条神经轨迹同时记录的两个神经元的尖峰计数。设在时间 $t$ 神经元 $i \\in \\{1,2\\}$ 的尖峰计数为 $Y_{it}$。假设神经轨迹服从一个乘性共享增益模型：以一个潜在增益 $s_t$ 为条件，尖峰计数是独立的，且服从分布\n$$\nY_{it} \\mid s_t \\sim \\mathrm{Poisson}(\\mu_i s_t),\n$$\n其中 $\\mathbb{E}[s_t] = 1$ 且 $\\mathrm{Var}(s_t) = \\sigma_s^2$，并且 $\\{s_t\\}$ 在时间上是独立的。均值 $\\mu_i$ 包含了异质的基线放电率。考虑将主成分分析 (PCA) 应用于根据对 $Y_{it}$ 随时间进行的三种不同预处理选择计算出的协方差矩阵（将时间点视为样本）：\n\n- 原始计数：$R_{it} = Y_{it}$。\n- 平方根计数：$Q_{it} = \\sqrt{Y_{it}}$。\n- Z-分数计数：$Z_{it} = \\dfrac{Y_{it} - \\mathbb{E}[Y_{it}]}{\\sqrt{\\mathrm{Var}(Y_{it})}}$，其中期望和方差是根据上述生成模型计算的。\n\n在时间上的大样本极限下，令 $C^{(X)}$ 表示对于变换 $X \\in \\{\\mathrm{raw}, \\sqrt{\\cdot}, z\\}$，两个神经元间的期望协方差矩阵，其元素为\n$$\nC^{(X)}_{ij} \\equiv \\mathbb{E}\\Big[\\big(X_{it} - \\mathbb{E}[X_{it}]\\big)\\big(X_{jt} - \\mathbb{E}[X_{jt}]\\big)\\Big].\n$$\n将变换 $X$ 的异质性偏差指数定义为对角元素的比率\n$$\nH^{(X)} \\equiv \\frac{C^{(X)}_{11}}{C^{(X)}_{22}}.\n$$\n假设基线放电率满足 $\\mu_1 = k \\mu_2$ 且 $k > 1$。在适用于大 $\\mu_i$ 和有限 $\\sigma_s^2$ 的波动的主导阶下，符号化地推导出 $H^{(\\mathrm{raw})}$, $H^{(\\sqrt{\\cdot})}$ 和 $H^{(z)}$，然后计算比率的完全简化的解析表达式\n$$\n\\mathcal{R}(k) \\equiv \\frac{H^{(\\mathrm{raw})}}{H^{(\\sqrt{\\cdot})}}.\n$$\n请以仅含 $k$ 的闭式表达式形式提供 $\\mathcal{R}(k)$ 的最终答案。不需要进行数值舍入，也不涉及单位。",
            "solution": "目标是计算比率 $\\mathcal{R}(k) \\equiv \\frac{H^{(\\mathrm{raw})}}{H^{(\\sqrt{\\cdot})}}$，其中 $H^{(X)}$ 是不同数据变换 $X$ 的异质性偏差指数。该指数定义为方差的比率 $H^{(X)} \\equiv C^{(X)}_{11} / C^{(X)}_{22}$，其中 $C^{(X)}_{ii} = \\mathrm{Var}(X_{it})$。我们必须在大的基线放电率 $\\mu_i$ 的主导阶下进行计算。\n\n对每种变换的分析都需要变换后变量的无条件方差。一个关键的工具是全方差公式。对于一个随机变量 $Y$ 和一个潜在变量 $s$，该定律表述为：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_s[\\mathrm{Var}(Y \\mid s)] + \\mathrm{Var}_s(\\mathbb{E}[Y \\mid s])\n$$\n首先，我们来计算原始尖峰计数 $Y_{it}$ 的无条件均值和方差。我们已知 $Y_{it} \\mid s_t \\sim \\mathrm{Poisson}(\\mu_i s_t)$。泊松分布的均值和方差等于其率参数。\n条件均值和方差为：\n$$\n\\mathbb{E}[Y_{it} \\mid s_t] = \\mu_i s_t\n$$\n$$\n\\mathrm{Var}(Y_{it} \\mid s_t) = \\mu_i s_t\n$$\n使用全期望公式和给定的 $\\mathbb{E}[s_t] = 1$：\n$$\n\\mathbb{E}[Y_{it}] = \\mathbb{E}_{s_t}[\\mathbb{E}[Y_{it} \\mid s_t]] = \\mathbb{E}_{s_t}[\\mu_i s_t] = \\mu_i \\mathbb{E}[s_t] = \\mu_i\n$$\n使用全方差公式和 $\\mathrm{Var}(s_t) = \\sigma_s^2$：\n$$\n\\mathrm{Var}(Y_{it}) = \\mathbb{E}_{s_t}[\\mathrm{Var}(Y_{it} \\mid s_t)] + \\mathrm{Var}_{s_t}(\\mathbb{E}[Y_{it} \\mid s_t])\n$$\n$$\n\\mathrm{Var}(Y_{it}) = \\mathbb{E}_{s_t}[\\mu_i s_t] + \\mathrm{Var}_{s_t}(\\mu_i s_t)\n$$\n$$\n\\mathrm{Var}(Y_{it}) = \\mu_i \\mathbb{E}[s_t] + \\mu_i^2 \\mathrm{Var}(s_t)\n$$\n$$\n\\mathrm{Var}(Y_{it}) = \\mu_i + \\mu_i^2 \\sigma_s^2\n$$\n这是原始计数的方差的精确表达式。\n\n现在我们分析每一种预处理选择。\n\n**1. 原始计数 ($R_{it} = Y_{it}$)**\n方差就是我们刚才计算的结果：\n$$\nC^{(\\mathrm{raw})}_{ii} = \\mathrm{Var}(Y_{it}) = \\mu_i + \\mu_i^2 \\sigma_s^2\n$$\n异质性偏差指数是神经元 1 和神经元 2 的这些方差的比率：\n$$\nH^{(\\mathrm{raw})} = \\frac{C^{(\\mathrm{raw})}_{11}}{C^{(\\mathrm{raw})}_{22}} = \\frac{\\mu_1 + \\mu_1^2 \\sigma_s^2}{\\mu_2 + \\mu_2^2 \\sigma_s^2}\n$$\n题目要求我们在大 $\\mu_i$ 的主导阶下进行计算。当 $\\mu_i \\to \\infty$ 时，$\\mu_i^2$ 项主导 $\\mu_i$ 项。代入 $\\mu_1 = k \\mu_2$：\n$$\nH^{(\\mathrm{raw})} = \\frac{k\\mu_2 + (k\\mu_2)^2 \\sigma_s^2}{\\mu_2 + \\mu_2^2 \\sigma_s^2} = \\frac{k\\mu_2 + k^2\\mu_2^2 \\sigma_s^2}{\\mu_2 + \\mu_2^2 \\sigma_s^2} = \\frac{\\mu_2^2 \\sigma_s^2 (k^2 + k/(\\mu_2 \\sigma_s^2))}{\\mu_2^2 \\sigma_s^2 (1 + 1/(\\mu_2 \\sigma_s^2))}\n$$\n在大 $\\mu_2$ 的极限下，这简化为主导项系数的比率：\n$$\nH^{(\\mathrm{raw})} \\approx \\frac{k^2\\mu_2^2 \\sigma_s^2}{\\mu_2^2 \\sigma_s^2} = k^2\n$$\n\n**2. 平方根计数 ($Q_{it} = \\sqrt{Y_{it}}$)**\n我们需要求 $C^{(\\sqrt{\\cdot})}_{ii} = \\mathrm{Var}(\\sqrt{Y_{it}})$。对于大的 $\\mu_i$，$Y_{it}$ 的分布集中在其均值 $\\mu_i$ 附近。我们可以使用 Delta 方法来近似随机变量函数的方差。对于函数 $g(Y)$，近似值为 $\\mathrm{Var}(g(Y)) \\approx [g'(\\mathbb{E}[Y])]^2 \\mathrm{Var}(Y)$。\n这里，$g(y) = \\sqrt{y}$，所以其导数为 $g'(y) = \\frac{1}{2\\sqrt{y}}$。随机变量是 $Y_{it}$，其 $\\mathbb{E}[Y_{it}] = \\mu_i$ 且 $\\mathrm{Var}(Y_{it}) = \\mu_i + \\mu_i^2 \\sigma_s^2$。\n应用 Delta 方法：\n$$\nC^{(\\sqrt{\\cdot})}_{ii} = \\mathrm{Var}(\\sqrt{Y_{it}}) \\approx \\left[ g'(\\mathbb{E}[Y_{it}]) \\right]^2 \\mathrm{Var}(Y_{it})\n$$\n$$\nC^{(\\sqrt{\\cdot})}_{ii} \\approx \\left( \\frac{1}{2\\sqrt{\\mu_i}} \\right)^2 (\\mu_i + \\mu_i^2 \\sigma_s^2) = \\frac{1}{4\\mu_i} (\\mu_i + \\mu_i^2 \\sigma_s^2)\n$$\n$$\nC^{(\\sqrt{\\cdot})}_{ii} \\approx \\frac{1}{4} + \\frac{\\mu_i \\sigma_s^2}{4}\n$$\n现在我们计算平方根变换的异质性偏差指数：\n$$\nH^{(\\sqrt{\\cdot})} = \\frac{C^{(\\sqrt{\\cdot})}_{11}}{C^{(\\sqrt{\\cdot})}_{22}} \\approx \\frac{1/4 + \\mu_1 \\sigma_s^2 / 4}{1/4 + \\mu_2 \\sigma_s^2 / 4} = \\frac{1 + \\mu_1 \\sigma_s^2}{1 + \\mu_2 \\sigma_s^2}\n$$\n代入 $\\mu_1 = k \\mu_2$ 并取大 $\\mu_2$ 的主导阶：\n$$\nH^{(\\sqrt{\\cdot})} \\approx \\frac{1 + k\\mu_2 \\sigma_s^2}{1 + \\mu_2 \\sigma_s^2} \\approx \\frac{k\\mu_2 \\sigma_s^2}{\\mu_2 \\sigma_s^2} = k\n$$\n\n**3. Z-分数计数 ($Z_{it}$)**\nZ-分数变量定义为 $Z_{it} = \\frac{Y_{it} - \\mathbb{E}[Y_{it}]}{\\sqrt{\\mathrm{Var}(Y_{it})}}$。\n根据定义，一个 Z-分数变量的均值为 0，方差为 1。我们来验证 $C^{(z)}_{ii}$：\n$$\nC^{(z)}_{ii} = \\mathrm{Var}(Z_{it}) = \\mathrm{Var}\\left( \\frac{Y_{it} - \\mathbb{E}[Y_{it}]}{\\sqrt{\\mathrm{Var}(Y_{it})}} \\right)\n$$\n因为 $\\mathbb{E}[Y_{it}]$ 和 $\\mathrm{Var}(Y_{it})$ 是相对于定义方差的期望而言的常数，我们有：\n$$\nC^{(z)}_{ii} = \\frac{1}{\\left(\\sqrt{\\mathrm{Var}(Y_{it})}\\right)^2} \\mathrm{Var}(Y_{it} - \\mathbb{E}[Y_{it}]) = \\frac{1}{\\mathrm{Var}(Y_{it})} \\mathrm{Var}(Y_{it}) = 1\n$$\n这个结果是精确的，不依赖于任何近似。因此，异质性指数为：\n$$\nH^{(z)} = \\frac{C^{(z)}_{11}}{C^{(z)}_{22}} = \\frac{1}{1} = 1\n$$\n\n**比率 $\\mathcal{R}(k)$ 的最终计算**\n问题要求计算比率 $\\mathcal{R}(k) = \\frac{H^{(\\mathrm{raw})}}{H^{(\\sqrt{\\cdot})}}$。使用我们推导出的主导阶表达式：\n$$\nH^{(\\mathrm{raw})} \\approx k^2\n$$\n$$\nH^{(\\sqrt{\\cdot})} \\approx k\n$$\n该比率为：\n$$\n\\mathcal{R}(k) = \\frac{k^2}{k} = k\n$$\n结果是一个仅含 $k$ 的闭式表达式。",
            "answer": "$$\\boxed{k}$$"
        },
        {
            "introduction": "在成功地对神经数据进行预处理和PCA分解后，一个核心问题随之而来：我们应该保留多少个主成分？选择过多会引入噪声，选择过少则会丢失重要的信号结构。本练习将介绍一种基于随机矩阵理论的先进方法，利用 Marchenko-Pastur 定律来客观地确定信号维度，帮助您将代表真实神经信号的成分与随机噪声区分开来 。",
            "id": "4187623",
            "problem": "您将获得一个用于神经群体活动的合成高维时间序列模型，该模型旨在反映嵌入在加性测量噪声中的低维神经轨迹。数据由一个矩阵 $X \\in \\mathbb{R}^{N \\times T}$ 表示，其中 $N$ 表示神经元的数量， $T$ 表示时间点的数量。主成分分析 (PCA) 协方差估计量是样本协方差 $C = \\frac{1}{T} X X^\\top$。对于一个基准模型，其中神经元和时间上的噪声是方差为 $\\sigma^2$ 的独立零均值高斯噪声，则纯噪声样本协方差是一个 Wishart 矩阵。随机矩阵理论中一个经过充分检验的事实指出，在 $N$ 和 $T$ 以固定长宽比 $q = N/T$ 共同增长的联合极限下，纯噪声协方差的特征值分布收敛于 Marchenko–Pastur (MP) 定律，其支撑集为一个区间，该区间的上界取决于 $q$ 和 $\\sigma^2$。低秩信号轨迹会在 $C$ 中产生有限数量的离群特征值，这些特征值可以超过 MP 支撑集的上界，从而可以估计信号的维度。\n\n实现一个程序，该程序：\n- 根据模型 $X = S + \\varepsilon$ 构建合成数据集，其中 $S = W Z$，且 $\\varepsilon$ 的各项元素独立抽取。\n- 使用样本协方差 $C$ 及其特征值来估计信号主成分的数量，这些主成分位于由 Marchenko–Pastur 定律预测的噪声谱之上，该上界由给定的 $N$ 、 $T$ 和 $\\sigma^2$ 确定。\n- 计算 $C$ 的特征值中有多少超过此上界，并为每个测试用例返回此计数。\n\n每个测试用例的合成数据规范：\n- $W \\in \\mathbb{R}^{N \\times r}$ 具有 $r$ 个标准正交列，通过对 $r$ 个独立同分布的标准正态列进行正交化构建，该过程通过 $\\mathrm{QR}$ 分解实现的 Gram–Schmidt 过程完成。\n- $Z \\in \\mathbb{R}^{r \\times T}$ 具有独立的高斯行，其中第 $j$ 行的元素服从 $\\mathcal{N}(0, \\sqrt{s_j^2})$ 分布，因此第 $j$ 个潜在轨迹的时间方差为 $s_j^2$。列表 $[s_1^2, s_2^2, \\dots, s_r^2]$ 会针对每个测试用例提供。\n- $\\varepsilon \\in \\mathbb{R}^{N \\times T}$ 的元素独立地服从 $\\mathcal{N}(0, \\sqrt{\\sigma^2})$ 分布。\n- 样本协方差为 $C = \\frac{1}{T} X X^\\top$。\n- 计算 $C$ 的特征值并与 MP 上界进行比较；任何严格大于 MP 上界的特征值都被计为一个检测到的信号主成分。\n\n您的程序必须实现以下测试套件，使用提供的确切参数值和随机种子，并且必须以指定格式为每个用例生成计数。\n\n测试套件：\n1. 用例 A (正常路径，多个强信号)：$N = 50$, $T = 200$, $\\sigma^2 = 1.0$, $r = 3$, $[s_1^2, s_2^2, s_3^2] = [6.0, 4.0, 2.0]$, 随机种子 $42$。\n2. 用例 B (纯噪声，$N  T$ 边界情况)：$N = 200$, $T = 100$, $\\sigma^2 = 1.0$, $r = 0$, 空信号列表 $[\\ ]$, 随机种子 $123$。\n3. 用例 C (大 $T$，不同强度的信号)：$N = 100$, $T = 500$, $\\sigma^2 = 0.5$, $r = 5$, $[s_1^2, s_2^2, s_3^2, s_4^2, s_5^2] = [1.0, 1.2, 2.5, 0.8, 3.0]$, 随机种子 $2024$。\n4. 用例 D (极小 $T$，信号低于可检测性)：$N = 120$, $T = 20$, $\\sigma^2 = 1.0$, $r = 2$, $[s_1^2, s_2^2] = [3.0, 4.0]$, 随机种子 $7$。\n5. 用例 E (方形情况，混合强度)：$N = 75$, $T = 75$, $\\sigma^2 = 0.8$, $r = 2$, $[s_1^2, s_2^2] = [4.0, 0.5]$, 随机种子 $99$。\n\n算法要求：\n- 对于每个用例，根据上述规则使用指定的种子构建 $W$ 、 $Z$ 和 $\\varepsilon$，以确保可复现性。\n- 构建 $X = W Z + \\varepsilon$ 并计算 $C = \\frac{1}{T} X X^\\top$。\n- 计算 $C$ 的特征值，并确定适用于该用例的 $N$ 、 $T$ 和 $\\sigma^2$ 的 Marchenko–Pastur 上界。\n- 计算严格高于此上界的 $C$ 的特征值数量。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，例如，`[$\\mathrm{result}_1$, $\\mathrm{result}_2$, $\\mathrm{result}_3$, $\\mathrm{result}_4$, $\\mathrm{result}_5$]`。\n- 每个 $\\mathrm{result}_i$ 必须是一个整数，给出用例 $i$ 检测到的信号主成分的计数。",
            "solution": "该问题要求估计嵌入在高维噪声中的低秩信号的维度。数据遵循模型 $X = S + \\varepsilon$，其中 $X \\in \\mathbb{R}^{N \\times T}$ 是数据矩阵，$S \\in \\mathbb{R}^{N \\times T}$ 是低秩信号矩阵，而 $\\varepsilon \\in \\mathbb{R}^{N \\times T}$ 是加性噪声矩阵。这里，$N$ 是观测变量（例如神经元）的数量， $T$ 是时间样本的数量。任务是通过分析样本协方差矩阵 $C = \\frac{1}{T} X X^\\top$ 的特征值来确定信号的秩 $r$。\n\n该方法基于随机矩阵理论 (RMT) 的一个关键结果。对于一个完全由方差为 $\\sigma^2$ 的独立同分布 (i.i.d.) 噪声构成的数据矩阵，样本协方差矩阵 $C_{noise} = \\frac{1}{T} \\varepsilon \\varepsilon^\\top$ 的经验谱分布在 $N, T \\to \\infty$ 且其比率 $q=N/T$ 保持固定的极限下，收敛于 Marchenko-Pastur (MP) 分布。该分布的支撑集是一个连续区间，通常称为“噪声谱主体 (noise bulk)”。该支撑集的上界 $\\lambda_+$ 由以下公式给出：\n$$ \\lambda_+ = \\sigma^2 (1 + \\sqrt{q})^2 $$\n当向噪声中添加低秩信号 $S$ 时，在特定条件下，完整协方差矩阵 $C$ 的若干特征值将从噪声谱主体中分离出来，并作为严格大于 $\\lambda_+$ 的“离群”特征值出现。这些离群特征值的数量对应于信号的维度（秩）$r$。问题要求我们为几个合成数据集计算这些离群值的数量。\n\n每个测试用例的算法流程如下：\n\n1.  **初始化参数**：对于每个用例，我们都给定维度 $N$ 和 $T$ 、噪声方差 $\\sigma^2$ 、信号秩 $r$ 及相关的信号分量方差 $[s_1^2, s_2^2, \\dots, s_r^2]$，以及一个用于可复现性的随机种子。\n\n2.  **生成合成数据**：\n    -   **信号矩阵构建**：信号矩阵公式化为 $S = WZ$。\n        -   矩阵 $W \\in \\mathbb{R}^{N \\times r}$ 为信号子空间提供了一组 $r$ 个标准正交基向量。它通过首先生成一个具有来自标准正态分布 $\\mathcal{N}(0, 1)$ 的独立同分布条目的 $N \\times r$ 矩阵，然后对其列进行标准正交化来构建。这种标准正交化是通过 QR 分解实现的，其中 $W$ 是得到的 $Q$ 因子。\n        -   矩阵 $Z \\in \\mathbb{R}^{r \\times T}$ 包含信号分量的时间动态。它的行是相互独立的。第 $j$ 行的条目是从高斯分布 $\\mathcal{N}(0, s_j)$ 中抽取的独立同分布样本，其中 $s_j^2$ 是第 $j$ 个潜在轨迹的指定方差。\n    -   **噪声矩阵构建**：噪声矩阵 $\\varepsilon \\in \\mathbb{R}^{N \\times T}$ 的每个条目都是从均值为 $0$ 、方差为 $\\sigma^2$ 的高斯分布（记为 $\\mathcal{N}(0, \\sigma)$）中独立抽取的。\n    -   **数据矩阵组装**：最终的数据矩阵由信号和噪声分量相加而成： $X = WZ + \\varepsilon$。在 $r=0$ 的特殊情况下，信号 $S$ 就是一个零矩阵。\n\n3.  **协方差与特征值计算**：\n    -   样本协方差矩阵计算为 $C = \\frac{1}{T} X X^\\top$。这是一个 $N \\times N$ 的半正定矩阵。\n    -   $C$ 的特征值随后被计算出来。由于 $C$ 是对称的，因此使用专门且高效的数值算法。为了计算效率，如果 $N  T$，计算较小的 $T \\times T$ 矩阵 $\\frac{1}{T} X^\\top X$ 的特征值会更有优势。$XX^\\top$ 和 $X^\\top X$ 的非零特征值集合是相同的。由于阈值 $\\lambda_+$ 总是正的，我们使用这种优化不会有错误计数离群值的风险。\n\n4.  **Marchenko-Pastur 边界计算**：\n    -   长宽比 $q$ 计算为 $q = N/T$。\n    -   噪声特征值谱的理论上界 $\\lambda_+$ 使用公式 $\\lambda_+ = \\sigma^2 (1 + \\sqrt{q})^2$ 计算。该公式对任何 $q  0$ 都有效。\n\n5.  **信号维度估计**：\n    -   将计算出的样本协方差矩阵的特征值与阈值 $\\lambda_+$ 进行比较。\n    -   估计的信号维度是严格大于 $\\lambda_+$ 的特征值的总数。\n\n该过程针对问题陈述中定义的五个测试用例中的每一个实施。使用指定的随机种子可确保每次运行生成的合成数据都相同，从而使结果完全可复现。例如，对于用例 E，参数为 $N=75$, $T=75$, $\\sigma^2 = 0.8$，信号方差为 $[4.0, 0.5]$，长宽比为 $q=1$，MP 边界为 $\\lambda_+ = 0.8(1+\\sqrt{1})^2 = 3.2$。RMT 预测，如果 $s_j^2/\\sigma^2  \\sqrt{q}$，具有总体方差 $s_j^2$ 的信号分量会形成一个离群值。只有第一个信号分量 ($4.0/0.8 = 5  1$) 满足此条件，所以我们期望找到一个离群值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by iterating through test cases, generating synthetic\n    neural data, and estimating signal dimensionality using random matrix theory.\n    \"\"\"\n\n    def estimate_dimensionality(N, T, sigma2, signal_variances, seed):\n        \"\"\"\n        Constructs synthetic data and estimates signal dimensionality by counting\n        eigenvalues of the sample covariance matrix above the Marchenko-Pastur edge.\n\n        Args:\n            N (int): Number of neurons.\n            T (int): Number of time points.\n            sigma2 (float): Variance of the additive noise.\n            signal_variances (list of float): Variances of the latent signal trajectories.\n            seed (int): Random seed for reproducibility.\n\n        Returns:\n            int: The estimated number of signal dimensions (outlier eigenvalues).\n        \"\"\"\n        # Use a random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n        r = len(signal_variances)\n\n        # 1. Construct Signal Matrix S = WZ\n        if r  0:\n            # Construct W, an orthonormal basis for the signal subspace\n            # by applying QR decomposition to a random Gaussian matrix.\n            A = rng.standard_normal(size=(N, r))\n            W, _ = np.linalg.qr(A)\n\n            # Construct Z, the temporal components of the signal.\n            # Each row j is scaled by the standard deviation sqrt(s_j^2).\n            s = np.sqrt(np.array(signal_variances))\n            Z_std = rng.standard_normal(size=(r, T))\n            Z = s[:, np.newaxis] * Z_std\n\n            S = W @ Z\n        else:\n            # For the pure noise case (r=0), the signal is a zero matrix.\n            S = np.zeros((N, T))\n\n        # 2. Construct Noise Matrix epsilon\n        std_noise = np.sqrt(sigma2)\n        epsilon = std_noise * rng.standard_normal(size=(N, T))\n\n        # 3. Construct Data Matrix X\n        X = S + epsilon\n\n        # 4. Compute Covariance and Eigenvalues\n        # To optimize, we compute eigenvalues from the smaller of the two matrices\n        # XX^T (N x N) and X^T X (T x T), as their non-zero eigenvalues are identical.\n        if N  T:\n            # Form the T x T matrix\n            cov_matrix = (X.T @ X) / T\n        else:\n            # Form the N x N matrix\n            cov_matrix = (X @ X.T) / T\n        \n        # Eigenvalues of the symmetric covariance matrix\n        eigenvalues = np.linalg.eigvalsh(cov_matrix)\n\n        # 5. Compute Marchenko-Pastur Upper Edge\n        q = N / T\n        mp_edge = sigma2 * (1 + np.sqrt(q))**2\n\n        # 6. Count Outlier Eigenvalues\n        # The number of signal components is the count of eigenvalues strictly\n        # greater than the theoretical maximum for a noise-only matrix.\n        num_outliers = np.sum(eigenvalues  mp_edge)\n\n        return int(num_outliers)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Happy path, multiple strong signals\n        {'N': 50, 'T': 200, 'sigma2': 1.0, 'signal_variances': [6.0, 4.0, 2.0], 'seed': 42},\n        # Case B: Pure noise, N  T boundary\n        {'N': 200, 'T': 100, 'sigma2': 1.0, 'signal_variances': [], 'seed': 123},\n        # Case C: Large T, diverse signal strengths\n        {'N': 100, 'T': 500, 'sigma2': 0.5, 'signal_variances': [1.0, 1.2, 2.5, 0.8, 3.0], 'seed': 2024},\n        # Case D: Very small T, signals at detectability limit\n        {'N': 120, 'T': 20, 'sigma2': 1.0, 'signal_variances': [3.0, 4.0], 'seed': 7},\n        # Case E: Square case, mixed strengths (one sub-critical)\n        {'N': 75, 'T': 75, 'sigma2': 0.8, 'signal_variances': [4.0, 0.5], 'seed': 99},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = estimate_dimensionality(\n            N=case['N'],\n            T=case['T'],\n            sigma2=case['sigma2'],\n            signal_variances=case['signal_variances'],\n            seed=case['seed']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}