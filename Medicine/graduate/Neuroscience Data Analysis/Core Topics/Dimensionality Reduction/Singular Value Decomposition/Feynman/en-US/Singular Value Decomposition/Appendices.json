{
    "hands_on_practices": [
        {
            "introduction": "Before exploring the powerful applications of Singular Value Decomposition (SVD), it is essential to master its fundamental mechanics. This first exercise provides practice in the core algebraic procedure for finding singular values. By working through the calculation for a simple $2 \\times 2$ matrix, you will solidify your understanding of the relationship between a matrix $A$, its transpose $A^T$, and the singular values that quantify its geometric action.",
            "id": "1071366",
            "problem": "Compute the singular values of the $2 \\times 2$ matrix \n\n$$\nA = \\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\end{bmatrix}.\n$$\n\nExpress the singular values in simplified radical form and list them in decreasing order.",
            "solution": "1. The singular values $\\sigma_i$ of $A$ are the square roots of the eigenvalues of $A^T A$.\n\n2. Compute \n$$A^T A = \\begin{bmatrix}1&1\\\\1&0\\end{bmatrix}\\begin{bmatrix}1&1\\\\1&0\\end{bmatrix}\n=\\begin{bmatrix}2&1\\\\1&1\\end{bmatrix}.$$\n\n3. The characteristic polynomial of $A^T A$ is\n$$\\det\\left(\\begin{bmatrix}2&1\\\\1&1\\end{bmatrix}-\\lambda I\\right)\n=(2-\\lambda)(1-\\lambda)-1\n=\\lambda^2-3\\lambda+1.$$\n\n4. Solve $\\lambda^2-3\\lambda+1=0$:\n$$\\lambda=\\frac{3\\pm\\sqrt{9-4}}{2}=\\frac{3\\pm\\sqrt5}{2}.$$\n\n5. Thus the singular values are\n$$\\sigma_1=\\sqrt{\\frac{3+\\sqrt5}{2}},\\quad\n\\sigma_2=\\sqrt{\\frac{3-\\sqrt5}{2}},$$\nlisted in decreasing order.",
            "answer": "$$\\boxed{\\sqrt{\\frac{3+\\sqrt{5}}{2}},\\ \\sqrt{\\frac{3-\\sqrt{5}}{2}}}$$"
        },
        {
            "introduction": "A deep understanding of SVD requires appreciating its unique geometric meaning, especially in contrast to the more familiar eigendecomposition. This problem moves from pure calculation to conceptual insight, exploring why singular vectors and eigenvectors can be fundamentally different . By analyzing matrices that rotate and stretch space, you will develop a geometric intuition for how SVD identifies the principal axes of a transformation, a concept crucial for interpreting SVD-based analyses of high-dimensional data.",
            "id": "3275104",
            "problem": "A central difference between eigenvectors and singular vectors is geometric: eigenvectors capture directions that are invariant up to scaling under a linear map, whereas singular vectors capture orthogonal directions along which the map stretches or contracts the most. Using only these conceptual bases and the definitions of eigenvectors and singular value decomposition (SVD), decide which of the following options correctly pair a concrete real $2 \\times 2$ matrix $A$ with a valid geometric reason why its singular vectors are dramatically different from its eigenvectors. Select all that apply.\n\nA. $$A = \\begin{bmatrix} 0 & -1 \\\\ 2 & 0 \\end{bmatrix}$$. Reason: $A$ is a composition of a rotation by $\\pi/2$ and anisotropic scaling, so $A$ has no real invariant directions (no real eigenvectors), yet its singular vectors are real orthonormal axes aligned with the principal axes of the ellipse $A$ maps the unit circle onto; therefore, the singular vectors are dramatically different from the eigenvectors.\n\nB. $$A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$$. Reason: Because $A$ has nonzero off-diagonal entries, its singular vectors must be unrelated to its eigenvectors; geometrically, the ellipse $A$ produces from the unit circle has axes unrelated to the eigen-directions, so the two sets of directions diverge.\n\nC. $$A = \\begin{bmatrix} 3 & 1 \\\\ 0 & 1 \\end{bmatrix}$$. Reason: For any upper-triangular matrix, the singular vectors coincide with the eigenvectors; geometrically, the shear does not change the principal axes defined by the eigen-directions, so there is no divergence.\n\nD. $$A = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$$. Reason: $A$ is a pure rotation by $\\pi/2$, so it has no real eigenvectors, whereas its singular vectors form a real orthonormal basis because the image of the unit circle is itself; thus, the singular vectors and eigenvectors are dramatically different.\n\nYour answer should identify all and only the option(s) that provide both a correct example and a correct geometric reason.",
            "solution": "The core of the problem lies in the geometric distinction between eigenvectors and singular vectors of a real $2 \\times 2$ matrix $A$.\nAn eigenvector $v$ of a matrix $A$ is a non-zero vector that satisfies the equation $Av = \\lambda v$ for some scalar eigenvalue $\\lambda$. Geometrically, this means the direction of an eigenvector is unchanged by the transformation $A$. Real eigenvectors correspond to real invariant directions.\nThe Singular Value Decomposition (SVD), $A = U\\Sigma V^T$, provides the geometric picture of $A$ transforming the unit circle into an ellipse. The right singular vectors (columns of $V$) are an orthonormal basis of input directions that get mapped to the principal axes of the output ellipse. The left singular vectors (columns of $U$) are the directions of these principal axes. Singular vectors are always real and orthonormal.\nA key difference arises when a real matrix lacks real eigenvectors (i.e., it has complex eigenvalues), which means it has no real invariant directions. However, its singular vectors must still be real and orthonormal, leading to a dramatic difference. This often happens with matrices that involve a rotation. Another difference occurs for non-normal matrices ($A^T A \\neq A A^T$), where eigenvectors are generally not orthogonal, while singular vectors are. For real symmetric matrices, the eigenvectors are orthonormal and coincide with the singular vectors.\n\nWe now evaluate each option.\n\n**Option A:** $$A = \\begin{bmatrix} 0 & -1 \\\\ 2 & 0 \\end{bmatrix}$$. The characteristic equation is $\\det(A - \\lambda I) = \\lambda^2 + 2 = 0$, giving eigenvalues $\\lambda = \\pm i\\sqrt{2}$. Since the eigenvalues are complex, the matrix has no real eigenvectors. To find singular vectors, we compute $$A^T A = \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix}$$. The eigenvectors of this matrix (the right singular vectors of $A$) are the standard basis vectors, which are real and orthonormal. Thus, the matrix has real singular vectors but no real eigenvectors. The reasoning is correct.\n\nVerdict: **Correct**.\n\n**Option B:** $$A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$$. This matrix is real and symmetric ($A=A^T$). For any real symmetric matrix, its eigenvectors form an orthonormal basis, and they are identical to its singular vectors (both left and right). The reasoning given, that non-zero off-diagonal entries imply a divergence, is false. The symmetric property is what matters.\n\nVerdict: **Incorrect**.\n\n**Option C:** $$A = \\begin{bmatrix} 3 & 1 \\\\ 0 & 1 \\end{bmatrix}$$. This is an upper-triangular matrix, but it is not normal because $$A^T A = \\begin{bmatrix} 9 & 3 \\\\ 3 & 2 \\end{bmatrix} \\neq \\begin{bmatrix} 10 & 1 \\\\ 1 & 1 \\end{bmatrix} = A A^T$$. Its eigenvectors are not orthogonal, whereas its singular vectors must be. Therefore, they cannot coincide. The reasoning that they coincide for any upper-triangular matrix is false.\n\nVerdict: **Incorrect**.\n\n**Option D:** $$A = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$$. This matrix represents a rotation by $\\pi/2$. Its characteristic equation is $\\lambda^2+1=0$, giving eigenvalues $\\lambda = \\pm i$. There are no real eigenvectors. The matrix is orthogonal, so $A^T A = I$. The eigenvalues of $I$ are both $1$, so the singular values of $A$ are both $1$. The eigenvectors of $I$ (the singular vectors of $A$) can be any orthonormal basis in $\\mathbb{R}^2$. We can choose the standard basis, for example. Thus, real singular vectors exist while real eigenvectors do not. The reasoning is correct.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "In neuroscience data analysis, SVD is a cornerstone of dimensionality reduction, but its power depends on a critical decision: selecting the appropriate rank $k$ to separate neural signal from noise. This advanced problem simulates a realistic scenario where you must move beyond simple heuristics like the \"elbow\" of a scree plot. You will apply the principled framework of Minimum Description Length (MDL) to make an informed, data-driven choice, a skill that is directly applicable to analyzing real-world neural recordings .",
            "id": "4193353",
            "problem": "A laboratory records a matrix of neural activity $X \\in \\mathbb{R}^{n \\times t}$, where $n$ denotes the number of neurons and $t$ denotes the number of time points, after mean-centering each row across time so that $\\sum_{j=1}^{t} X_{ij} = 0$ for each $i \\in \\{1,\\dots,n\\}$. Let the singular value decomposition (SVD) of $X$ be $X = U \\Sigma V^\\top$, with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r \\ge 0$, where $r = \\min\\{n,t\\}$. The team considers low-rank modeling of $X$ via a rank-$k$ signal plus noise model $X = S + E$, where $\\mathrm{rank}(S) \\le k$ and $E$ represents noise.\n\nThey wish to (i) specify a scree plot criterion to select $k$ and articulate its limitations for neural data, and (ii) propose a principled information-theoretic selection rule, grounded in the Minimum Description Length (MDL) principle, under a Gaussian noise model. Assume the following fundamental bases:\n\n- The best rank-$k$ approximation in Frobenius norm is given by truncating the SVD, and the residual sum of squares equals the sum of squared discarded singular values.\n- Under an independent and identically distributed Gaussian noise model, $E_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$, the negative log-likelihood of the residuals is proportional to the residual sum of squares scaled by $\\sigma^2$, and at the maximum likelihood estimate the variance is the empirical residual variance.\n- A two-part MDL code length balances goodness of fit (negative log-likelihood at the maximum likelihood estimate) with a model complexity term that grows with the number of free parameters and the sample size.\n\nWhich option correctly specifies a formal scree plot criterion and its substantive limitations in neural data settings, and also gives a consistent MDL objective for selecting $k$ in terms of the SVD and basic parameter counting for rank-$k$ matrices?\n\nA. Scree criterion: choose $\\hat{k}$ as the index of the maximum discrete curvature of the log-singular-value sequence, $\\hat{k}_{\\mathrm{elbow}} = \\arg\\max_{i \\in \\{2,\\dots,r-1\\}} \\left(\\log \\sigma_{i-1} - 2 \\log \\sigma_i + \\log \\sigma_{i+1}\\right)$. Limitations: the elbow can be ambiguous when singular values decay gradually, sensitive to scaling and heteroscedastic or temporally correlated noise common in neural recordings, and unstable in small $n$ or $t$. MDL: under $E_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$, select $k$ by minimizing $L(k) = \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + \\frac{1}{2}\\,k\\,(n+t-k)\\,\\log(nt) + C$, where $\\mathrm{RSS}_k = \\sum_{i=k+1}^{r} \\sigma_i^2$ and $C$ does not depend on $k$.\n\nB. Scree criterion: choose the smallest $k$ such that $\\sigma_k = \\bar{\\sigma}$, where $\\bar{\\sigma} = \\frac{1}{r}\\sum_{i=1}^{r}\\sigma_i$. Limitations: none provided the data are standardized. MDL: select $k$ by minimizing $L(k) = \\frac{nt}{2}\\log(\\mathrm{RSS}_k) + k(n+t)\\log(\\sigma_1)$.\n\nC. Scree criterion: choose the smallest $k$ such that $\\sum_{i=1}^{k}\\sigma_i^2 / \\sum_{i=1}^{r}\\sigma_i^2 \\ge 0.9$. Limitations: the threshold $0.9$ is conventional and may not reflect neural noise properties. MDL: use an Akaike-type score $A(k) = nt \\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + 2k$, treating the number of parameters as $k$.\n\nD. Scree criterion: choose $k$ as the largest index for which $\\sigma_{k+1} \\le \\hat{\\sigma}\\,(\\sqrt{n}+\\sqrt{t})$, where $\\hat{\\sigma}^2 = \\mathrm{RSS}_m/(nt)$ for some fixed $m$. Limitations: none under isotropic noise. MDL: $L(k) = \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + \\frac{k}{2}\\log(nt)$.\n\nSelect the single best option.",
            "solution": "This problem requires evaluating both a heuristic (scree plot) and a formal (MDL) criterion for model selection in the context of SVD.\n\n**Part 1: Scree Plot Criterion and Limitations**\n\nThe scree plot heuristic identifies the \"elbow\" where the singular value spectrum transitions from a steep decline (signal) to a flat floor (noise).\n-   **Criterion:** Option A formalizes this by finding the point of maximum discrete curvature in the log-singular value plot. The expression $\\log \\sigma_{i-1} - 2 \\log \\sigma_i + \\log \\sigma_{i+1}$ is a discrete approximation of the second derivative, and maximizing it finds the sharpest \"bend\" or elbow.\n-   **Limitations:** The limitations listed in Option A are all highly relevant for real neural data. Neural activity often has power-law spectra without a clear elbow. Furthermore, the assumption of simple, i.i.d. noise is often violated; neural noise can be correlated in time and have different variances across neurons (heteroscedastic), distorting the \"noise floor\" and making the elbow heuristic unreliable.\n\n**Part 2: Minimum Description Length (MDL) Criterion**\n\nThe MDL principle seeks to find a model that provides the most compressed description of the data. This involves balancing model fit with model complexity. The MDL cost is $L(k) = -\\log(\\text{Likelihood}) + \\text{Penalty}$.\n\n1.  **Goodness-of-Fit Term:** For a Gaussian noise model $E_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$, the negative log-likelihood at the maximum likelihood estimate of $\\sigma^2$ (which is $\\hat{\\sigma}_k^2 = \\mathrm{RSS}_k / nt$) is, up to constants, $\\frac{nt}{2}\\log(\\hat{\\sigma}_k^2) = \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right)$. Here, the residual sum of squares is $\\mathrm{RSS}_k = \\sum_{i=k+1}^{r} \\sigma_i^2$. This term measures how poorly the rank-$k$ model fits the data.\n\n2.  **Model Complexity Term:** The penalty term is typically $\\frac{D}{2}\\log(N_s)$, where $D$ is the number of free parameters and $N_s$ is the number of samples.\n    -   A rank-$k$ matrix in $\\mathbb{R}^{n \\times t}$ is defined by its SVD factors. The number of degrees of freedom is the dimension of the manifold of rank-$k$ matrices, which is $D = k(n+t-k)$. This accounts for the parameters in the $U_k$ and $V_k$ factors minus the redundancy in their definition.\n    -   The number of data samples is $N_s = nt$.\n    -   Therefore, the penalty term is $\\frac{1}{2} k(n+t-k) \\log(nt)$.\n\nCombining these gives the MDL objective function presented in Option A:\n$$L(k) = \\frac{nt}{2}\\log\\left(\\frac{\\mathrm{RSS}_k}{nt}\\right) + \\frac{1}{2}\\,k\\,(n+t-k)\\,\\log(nt)$$\n\n**Evaluation of Options:**\n\n-   **A:** Correctly specifies the elbow criterion, its limitations, and derives the correct MDL objective function from first principles.\n-   **B:** The scree criterion is ad-hoc, the limitations claim is false, and the MDL formula has incorrect fit and penalty terms.\n-   **C:** The criterion is for \"percent variance explained,\" not a scree plot. The MDL formula is actually an AIC-style criterion (penalty is $2D$) and uses an incorrect parameter count ($D=k$).\n-   **D:** The scree criterion is based on random matrix theory, not the elbow heuristic. The MDL penalty term uses an incorrect parameter count ($D=k$).\n\nTherefore, Option A is the only one that correctly addresses all parts of the question.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}