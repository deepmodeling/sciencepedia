{
    "hands_on_practices": [
        {
            "introduction": "Principal Component Analysis (PCA) is a cornerstone of dimensionality reduction, but its effectiveness hinges on a crucial choice: the number of components to retain. While the traditional scree plot offers a heuristic, this exercise introduces a more rigorous method grounded in Random Matrix Theory (RMT). By modeling the eigenvalues expected from pure noise, you will learn to objectively distinguish true signal from statistical fluctuations, a critical skill for robustly analyzing high-dimensional neural data. ",
            "id": "4156671",
            "problem": "You are given a sample covariance matrix $\\hat{\\Sigma} \\in \\mathbb{R}^{p \\times p}$ for a neural population with $p$ neurons, computed from $n$ samples of zero-mean activity. The goal is to use Principal Component Analysis (PCA) to quantify dimensionality reduction by computing the fraction of variance explained as a function of $k$, and to design an objective scree plot criterion based on Random Matrix Theory (RMT). Start from the following foundational definitions: the sample covariance is $\\hat{\\Sigma} = \\frac{1}{n} X^\\top X$, where $X \\in \\mathbb{R}^{n \\times p}$ is the zero-mean data matrix, and the eigen-decomposition of $\\hat{\\Sigma}$ is $\\hat{\\Sigma} = V \\Lambda V^\\top$ with eigenvalues $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0$. The total variance is $\\mathrm{tr}(\\hat{\\Sigma}) = \\sum_{i=1}^{p} \\lambda_i$, and the fraction of variance explained by the top $k$ principal components is $\\mathrm{FVE}(k) = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{p} \\lambda_i}$. Under an isotropic noise model with variance $\\sigma^2$ and ratio $q = \\frac{p}{n}$, the Marchenko–Pastur (MP) law from Random Matrix Theory (RMT) states that the eigenvalues of a pure-noise sample covariance concentrate in the interval $[\\lambda_{-}, \\lambda_{+}]$ with $\\lambda_{\\pm} = \\sigma^2 (1 \\pm \\sqrt{q})^2$. Use this to define a scree plot selection criterion: choose $k$ as the number of eigenvalues strictly greater than $\\lambda_{+}$.\n\nConstruct $\\hat{\\Sigma}$ deterministically in each test by specifying $p$, $n$, $\\sigma^2$, and a list of nonnegative spike strengths $(s_1, \\dots, s_r)$ with $r \\leq p$. Assume a spiked covariance model aligned with coordinate axes: $\\hat{\\Sigma} = \\sigma^2 I_p + \\sum_{j=1}^{r} s_j e_j e_j^\\top$, where $I_p$ is the $p \\times p$ identity and $e_j$ is the $j$-th standard basis vector, so the eigenvalues are $(\\sigma^2 + s_1, \\dots, \\sigma^2 + s_r, \\sigma^2, \\dots, \\sigma^2)$.\n\nImplement the following tasks from first principles:\n- Compute the sorted eigenvalues $(\\lambda_1, \\dots, \\lambda_p)$ of $\\hat{\\Sigma}$ in descending order.\n- Compute $\\mathrm{FVE}(k)$ for all integers $k$ from $1$ to $p$.\n- Compute the MP upper edge $\\lambda_{+} = \\sigma^2 (1 + \\sqrt{p/n})^2$ and select $k$ as the count of eigenvalues greater than $\\lambda_{+}$.\n\nThere are no physical units involved. All reported fractions must be decimals. Round each $\\mathrm{FVE}(k)$ to $6$ decimal places.\n\nTest suite:\n- Test $1$: $p = 10$, $n = 400$, $\\sigma^2 = 0.5$, spikes $(3.0, 2.0, 1.0)$.\n- Test $2$: $p = 10$, $n = 200$, $\\sigma^2 = 1.0$, spikes $()$ (no spikes).\n- Test $3$: $p = 10$, $n = 50$, $\\sigma^2 = 1.0$, spikes $(0.7)$.\n- Test $4$: $p = 10$, $n = 100$, $\\sigma^2 = 0.8$, spikes $(1.5, 0.9)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test, output one inner list comprising the selected $k$ as an integer followed by the full sequence $[\\mathrm{FVE}(1), \\mathrm{FVE}(2), \\dots, \\mathrm{FVE}(p)]$ as floats rounded to $6$ decimal places. For example, the overall format is $[[k_1, f_{1,1}, \\dots, f_{1,p}], [k_2, f_{2,1}, \\dots, f_{2,p}], [k_3, \\dots], [k_4, \\dots]]$.",
            "solution": "The problem requires an analysis of a specified covariance matrix using Principal Component Analysis (PCA) to quantify dimensionality and determine an effective dimension using a criterion from Random Matrix Theory (RMT). The process involves computing the fraction of variance explained (FVE) by principal components and applying the Marchenko-Pastur (MP) law.\n\n### Principle-Based Design\n\nThe solution is founded on several key principles from linear algebra, statistics, and RMT.\n\n1.  **Principal Component Analysis (PCA)**: PCA is a dimensionality reduction technique that transforms a set of observations of possibly correlated variables into a set of linearly uncorrelated variables called principal components. The components are ordered such that the first few retain most of the variance present in the original data. Mathematically, this is achieved by performing an eigendecomposition of the sample covariance matrix $\\hat{\\Sigma}$. The eigenvectors of $\\hat{\\Sigma}$ are the principal components (directions of maximum variance), and the corresponding eigenvalues $(\\lambda_i)$ quantify the variance along these directions.\n\n2.  **Fraction of Variance Explained (FVE)**: A primary metric for assessing the efficacy of dimensionality reduction is the FVE. The total variance in the data is captured by the trace of the covariance matrix, $\\mathrm{tr}(\\hat{\\Sigma})$, which is equal to the sum of its eigenvalues, $\\sum_{i=1}^{p} \\lambda_i$. The variance captured by the top $k$ principal components is the sum of the top $k$ eigenvalues, $\\sum_{i=1}^{k} \\lambda_i$. The FVE for $k$ components is therefore defined as the ratio:\n    $$\n    \\mathrm{FVE}(k) = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{p} \\lambda_i}\n    $$\n    This provides a cumulative measure of how much information (variance) is retained when projecting the data onto a $k$-dimensional subspace.\n\n3.  **Spiked Covariance Model**: The problem specifies a deterministic construction for the covariance matrix using a \"spiked\" model. This model assumes the true covariance structure consists of a simple noise component (isotropic noise) plus a few \"spikes\" representing strong, low-dimensional signals. The specified matrix is $\\hat{\\Sigma} = \\sigma^2 I_p + \\sum_{j=1}^{r} s_j e_j e_j^\\top$. Here, $\\sigma^2 I_p$ represents the isotropic noise covariance, where $\\sigma^2$ is the noise variance and $I_p$ is the $p \\times p$ identity matrix. The term $\\sum_{j=1}^{r} s_j e_j e_j^\\top$ represents the signal components, where $s_j > 0$ are the \"spike\" strengths and $e_j$ are standard basis vectors. Due to the diagonal structure of this sum, the eigenvalues of $\\hat{\\Sigma}$ are directly given. There are $r$ \"spiked\" eigenvalues of the form $\\sigma^2 + s_j$ and $p-r$ \"bulk\" eigenvalues equal to $\\sigma^2$.\n\n4.  **Random Matrix Theory (RMT) and the Marchenko-Pastur (MP) Law**: RMT provides powerful results about the distribution of eigenvalues of large random matrices. For a sample covariance matrix computed from $n$ samples of $p$-dimensional data containing only isotropic noise (i.e., no signal), the MP law describes the limiting distribution of its eigenvalues as $n, p \\to \\infty$ with their ratio $q = p/n$ held constant. Crucially, the eigenvalues are predicted to lie within a continuous bulk distribution bounded by $\\lambda_{\\pm} = \\sigma^2 (1 \\pm \\sqrt{q})^2$. In the context of the spiked model, eigenvalues that \"pop out\" and lie strictly above the theoretical upper bound $\\lambda_{+}$ are considered to correspond to true signal, while those within the bulk $[0, \\lambda_{+}]$ are attributed to noise. This provides an objective, data-driven criterion for choosing the number of significant components, $k$, thus avoiding the subjectivity of a traditional scree plot inspection. The selected dimension $k$ is the count of eigenvalues $\\lambda_i > \\lambda_{+}$.\n\n### Algorithmic Steps\n\nBased on these principles, the solution proceeds as follows for each test case:\n\n1.  **Determine Eigenvalues**: Given the parameters $p$, $\\sigma^2$, and the list of $r$ spike strengths $(s_1, \\dots, s_r)$, construct the set of $p$ eigenvalues. This set is $\\{\\sigma^2 + s_1, \\dots, \\sigma^2 + s_r\\} \\cup \\{\\sigma^2, \\dots, \\sigma^2\\}$, where the noise eigenvalue $\\sigma^2$ is repeated $p-r$ times.\n\n2.  **Sort Eigenvalues**: Sort the eigenvalues in descending order to obtain the ordered spectrum $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p$.\n\n3.  **Compute Total Variance**: Calculate the total variance by summing all eigenvalues: $V_{\\text{total}} = \\sum_{i=1}^{p} \\lambda_i$.\n\n4.  **Compute FVE Array**: Compute the cumulative sum of the sorted eigenvalues, $C_k = \\sum_{i=1}^{k} \\lambda_i$ for $k=1, \\dots, p$. The FVE array is then computed as $\\mathrm{FVE}(k) = C_k / V_{\\text{total}}$. Each element is rounded to $6$ decimal places.\n\n5.  **Calculate MP Threshold**: Compute the aspect ratio $q = p/n$. Then calculate the Marchenko-Pastur upper edge threshold $\\lambda_{+} = \\sigma^2 (1 + \\sqrt{q})^2$.\n\n6.  **Select Dimension $k$**: Count the number of sorted eigenvalues $\\lambda_i$ that are strictly greater than the threshold $\\lambda_{+}$. This count is the selected intrinsic dimensionality, $k$.\n\n7.  **Format Output**: Combine the selected integer $k$ and the list of $p$ FVE values into a single list for the test case. Repeat for all test cases and format the final output as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the dimensionality reduction problem for a suite of test cases.\n    For each case, it calculates the eigenvalues of a spiked covariance matrix,\n    computes the fraction of variance explained (FVE) array, and determines\n    the number of significant components (k) using a Marchenko-Pastur criterion.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (p, n, sigma_sq, spikes_tuple)\n        (10, 400, 0.5, (3.0, 2.0, 1.0)),\n        (10, 200, 1.0, ()),\n        (10, 50, 1.0, (0.7,)),\n        (10, 100, 0.8, (1.5, 0.9)),\n    ]\n\n    results = []\n    for p, n, sigma_sq, spikes in test_cases:\n        # Main logic to calculate the result for one case goes here.\n\n        # 1. Construct the list of eigenvalues from the spiked covariance model.\n        # The matrix is Sigma = sigma^2 * I + diag(s_1, ..., s_r, 0, ...).\n        # Its eigenvalues are (sigma^2 + s_j) for j=1..r and sigma^2 for the rest.\n        num_spikes = len(spikes)\n        eigenvalues = [sigma_sq + s for s in spikes] + [sigma_sq] * (p - num_spikes)\n        \n        # 2. Sort the eigenvalues in descending order.\n        eigenvalues = np.array(eigenvalues)\n        eigenvalues.sort() # Sorts in-place, ascending\n        sorted_eigs = eigenvalues[::-1] # Reverse to get descending order\n\n        # 3. Compute the total variance.\n        total_variance = np.sum(sorted_eigs)\n\n        # 4. Compute the Fraction of Variance Explained (FVE) array.\n        # Handle the edge case of zero total variance to avoid division by zero.\n        if total_variance > 0:\n            cumulative_variance = np.cumsum(sorted_eigs)\n            fve_array = cumulative_variance / total_variance\n        else:\n            # If total variance is zero, all eigenvalues are zero.\n            # FVE is ill-defined, but we can treat it as an array of zeros.\n            fve_array = np.zeros(p)\n            \n        fve_rounded = np.round(fve_array, 6)\n\n        # 5. Compute the Marchenko-Pastur upper edge.\n        q = p / n\n        lambda_plus = sigma_sq * (1 + np.sqrt(q))**2\n\n        # 6. Select k as the count of eigenvalues strictly greater than lambda_plus.\n        selected_k = np.sum(sorted_eigs > lambda_plus)\n\n        # 7. Format the result for this test case.\n        case_result = [int(selected_k)] + fve_rounded.tolist()\n        results.append(str(case_result))\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, represented as a string.\n    # Example: [[k1, ...], [k2, ...]]\n    # The template `f\"[{','.join(map(str, results))}]` with `results` as a list of lists\n    # will produce `[[k1,...],[k2,...]]`. This matches structural requirements.\n    # Since `results` here is a list of strings, it will also work.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Neural population activity often resides on low-dimensional manifolds that are fundamentally nonlinear, rendering linear methods like PCA insufficient. This practice delves into Kernel PCA (kPCA), a powerful technique that implicitly maps data into a high-dimensional feature space to uncover such curved structures. By deriving the method from first principles and applying it to a simulated dataset, you will gain a deep, practical understanding of how to find and analyze nonlinear geometries in neural data. ",
            "id": "4156661",
            "problem": "You are given a collection of simulated neural recordings represented as high-dimensional vectors. Each vector corresponds to a single trial consisting of spike counts from $n_{\\text{neurons}}$ neurons across $n_{\\text{bins}}$ time bins, flattened into a single vector. The task is to formally derive Kernel Principal Component Analysis (Kernel PCA) from first principles and implement an algorithm to compute embeddings using a Gaussian kernel, including proper centering in feature space. Then, apply this algorithm to the specified neural dataset and produce quantitative outputs for a test suite.\n\nFundamental base and core definitions:\n- Principal Component Analysis (PCA) in feature space starts from the covariance operator defined on centered feature vectors. Consider a mapping $\\phi:\\mathbb{R}^d\\to\\mathcal{H}$ into a Reproducing Kernel Hilbert Space (RKHS). For $n$ training samples $\\{x_i\\}_{i=1}^n$, let $\\Phi_i=\\phi(x_i)-\\bar{\\phi}$ be the centered feature vectors with $\\bar{\\phi}=\\frac{1}{n}\\sum_{i=1}^n\\phi(x_i)$. The feature-space covariance operator is $C=\\frac{1}{n}\\sum_{i=1}^n \\Phi_i\\Phi_i^{\\top}$. PCA seeks eigenpairs $(\\lambda,w)$ satisfying $Cw=\\lambda w$ with unit norm $\\|w\\|_{\\mathcal{H}}=1$.\n- The kernel trick uses the Gram matrix $K$ whose entries are $K_{ij}=k(x_i,x_j)=\\langle \\phi(x_i),\\phi(x_j)\\rangle_{\\mathcal{H}}$. Feature-space centering corresponds to centering $K$ via a centering matrix.\n- For a Gaussian kernel, $k(x,y)=\\exp\\left(-\\frac{\\|x-y\\|^2}{2\\sigma^2}\\right)$ for a bandwidth parameter $\\sigma>0$.\n\nDerivation tasks:\n1. Starting from the PCA definition in feature space, show that any principal direction satisfies $w=\\sum_{i=1}^n\\alpha_i\\Phi_i$ for some coefficients $\\alpha\\in\\mathbb{R}^n$ and derive an eigenvalue problem in terms of the centered Gram matrix $K_c$. Explicitly derive the relationship between the eigenvalues of the covariance operator $C$ and those of the centered Gram matrix $K_c$.\n2. Derive the centering of the Gram matrix in feature space using the centering matrix $H=I-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$, and prove that $K_c=HKH$ corresponds to inner products between centered feature vectors, i.e., $(K_c)_{ij}=\\langle \\Phi_i,\\Phi_j\\rangle_{\\mathcal{H}}$.\n3. For out-of-sample embedding of a new point $x$, derive the formula for the centered kernel vector $k_c(x)\\in\\mathbb{R}^n$ with entries $[k_c(x)]_i=\\langle \\Phi_i,\\phi(x)-\\bar{\\phi}\\rangle_{\\mathcal{H}}$, expressed solely in terms of $K$, the training mean, and $k(x_i,x)$. Derive the projection onto principal directions in terms of the eigenvectors and eigenvalues of $K_c$.\n\nAlgorithmic implementation requirements:\n- Construct a simulated neural dataset with the following specifications to ensure scientific realism and reproducibility:\n  - Set a fixed random seed $s=12345$.\n  - Use $n_{\\text{neurons}}=40$, $n_{\\text{bins}}=20$, and $n_{\\text{trials}}=60$ for training, and generate one additional held-out test trial.\n  - Draw baseline firing rates per neuron independently from a uniform distribution over $[5,15]$ (in arbitrary units per bin).\n  - Define a temporal modulation $g(t)=\\exp\\left(-\\frac{(t-t_0)^2}{2\\sigma_t^2}\\right)$ with $t_0=10$ and $\\sigma_t=3$ for $t=0,1,\\dots,19$.\n  - Define condition-specific amplitudes: for condition $A$ (half the trials), draw amplitudes $a_i^{(A)}$ independently from a normal distribution with mean $3$ and standard deviation $1$; for condition $B$ (other half), draw $a_i^{(B)}$ independently from a normal distribution with mean $-3$ and standard deviation $1$. Add trial-wise amplitude noise by adding independent $\\mathcal{N}(0,0.5)$ to each neuron's amplitude. Clip instantaneous rates at a minimum of $0.1$ to avoid negative rates.\n  - Generate spike counts per bin using independent Poisson draws with the specified rates and flatten each trial into a vector in $\\mathbb{R}^{800}$.\n  - Standardize the training data across trials: for each feature dimension, subtract the across-trial mean and divide by the across-trial standard deviation; replace any zero standard deviation by $1$ to avoid division by zero. Apply the same centering and scaling to the held-out test trial using the training mean and standard deviation.\n\n- Kernel PCA implementation details:\n  - Compute the Gaussian kernel matrix $K\\in\\mathbb{R}^{n\\times n}$ for the training data using the specified $\\sigma$ by $K_{ij}=\\exp\\left(-\\frac{\\|x_i-x_j\\|^2}{2\\sigma^2}\\right)$.\n  - Center the Gram matrix in feature space using $H=I-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ and $K_c=HKH$.\n  - Solve the symmetric eigenvalue problem for $K_c$ to obtain eigenvalues $\\mu_k\\ge 0$ and orthonormal eigenvectors $v_k\\in\\mathbb{R}^n$ satisfying $K_c v_k=\\mu_k v_k$. Ensure numerical stability by sorting eigenvalues in descending order and clipping any tiny negative eigenvalues caused by numerical error to $0$.\n  - Normalize so that the feature-space principal directions have unit norm by setting $w_k=\\frac{1}{\\sqrt{\\mu_k}}\\sum_{i=1}^n (v_k)_i \\Phi_i$ for $\\mu_k>0$.\n  - Compute training embeddings by $y_{k}(x_j)=\\langle w_k,\\Phi_j\\rangle_{\\mathcal{H}}=\\sqrt{\\mu_k}\\,(v_k)_j$.\n  - For a new point $x$, compute the centered kernel vector $k_c(x)$ via\n    $$k_c(x)=k(x)-\\frac{1}{n}K\\mathbf{1}-\\frac{1}{n}\\left(\\sum_{i=1}^n k(x_i,x)\\right)\\mathbf{1}+\\frac{1}{n^2}\\left(\\mathbf{1}^{\\top}K\\mathbf{1}\\right)\\mathbf{1},$$\n    where $k(x)$ is the vector with entries $k(x_i,x)$, and $\\mathbf{1}$ is the all-ones vector. Then compute out-of-sample embeddings by\n    $$y_k(x)=\\frac{v_k^{\\top}k_c(x)}{\\sqrt{\\mu_k}},\\quad \\text{with the convention } y_k(x)=0 \\text{ if }\\mu_k=0.$$\n\nTest suite and outputs:\n- Use the single training dataset and held-out test trial defined above for all test cases. For each case below, compute the required outputs.\n  1. Case $A$ (happy path): $\\sigma=5.0$, retain $m=3$ leading components. Output the list of the first $m$ explained variance fractions, computed as $\\left[\\frac{\\mu_1}{\\sum_{j}\\mu_j},\\frac{\\mu_2}{\\sum_{j}\\mu_j},\\frac{\\mu_3}{\\sum_{j}\\mu_j}\\right]$ as decimal floats.\n  2. Case $B$ (small bandwidth edge case): $\\sigma=0.1$, retain $m=2$ leading components. Output the list of the first two out-of-sample embedding coordinates for the held-out test trial $[y_1(x_{\\text{test}}),y_2(x_{\\text{test}})]$ as floats.\n  3. Case $C$ (large bandwidth boundary case): $\\sigma=1000.0$. Output the Frobenius norm of the centered Gram matrix $\\|K_c\\|_F$ as a single float, which should be near zero for this case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The three elements correspond to the outputs for Cases $A$, $B$, and $C$, in order. Since Cases $A$ and $B$ require lists, the final format will be of the form\n  $$[\\,[f_1,f_2,f_3],\\,[y_1,y_2],\\,\\nu\\,],$$\n  where $f_i$ are explained fraction floats, $y_i$ are embedding floats, and $\\nu$ is the Frobenius norm float.",
            "solution": "The problem statement is well-posed, scientifically grounded, and internally consistent. It provides a complete set of definitions, parameters, and tasks for a formal derivation and implementation of Kernel Principal Component Analysis (Kernel PCA) applied to a simulated neuroscience dataset. All necessary components are specified, allowing for a unique and verifiable solution.\n\nWe will proceed by first providing the requested theoretical derivations and then implementing the algorithm as specified.\n\n### 1. Derivation of the Kernel PCA Eigenvalue Problem\n\nThe objective of PCA in the feature space $\\mathcal{H}$ is to find the eigenvectors $w \\in \\mathcal{H}$ of the covariance operator $C = \\frac{1}{n}\\sum_{i=1}^n \\Phi_i\\Phi_i^{\\top}$, where $\\Phi_i = \\phi(x_i) - \\bar{\\phi}$ are the centered feature vectors. The eigenvalue problem is given by:\n$$Cw = \\lambda w, \\quad \\text{with } \\|w\\|_{\\mathcal{H}}=1$$\nSubstituting the definition of $C$:\n$$ \\frac{1}{n}\\sum_{i=1}^n \\Phi_i\\Phi_i^{\\top} w = \\lambda w $$\nThe term $\\Phi_i^{\\top} w$ is the inner product $\\langle \\Phi_i, w \\rangle_{\\mathcal{H}}$, which is a scalar. This equation demonstrates that any eigenvector $w$ must lie in the span of the centered data points in the feature space, $\\{\\Phi_i\\}_{i=1}^n$. Therefore, $w$ can be expressed as a linear combination of these vectors:\n$$ w = \\sum_{i=1}^n \\alpha_i \\Phi_i $$\nfor some coefficients $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^{\\top} \\in \\mathbb{R}^n$.\n\nSubstituting this expansion of $w$ back into the eigenvalue equation:\n$$ \\frac{1}{n}\\sum_{j=1}^n \\Phi_j \\langle \\Phi_j, \\sum_{i=1}^n \\alpha_i \\Phi_i \\rangle = \\lambda \\sum_{i=1}^n \\alpha_i \\Phi_i $$\nTo solve for the coefficients $\\alpha_i$, we project this entire equation onto a basis vector $\\Phi_k$ for an arbitrary $k \\in \\{1, \\dots, n\\}$:\n$$ \\langle \\Phi_k, \\frac{1}{n}\\sum_{j=1}^n \\Phi_j \\sum_{i=1}^n \\alpha_i \\langle \\Phi_j, \\Phi_i \\rangle \\rangle = \\langle \\Phi_k, \\lambda \\sum_{i=1}^n \\alpha_i \\Phi_i \\rangle $$\nUsing the linearity of the inner product:\n$$ \\frac{1}{n}\\sum_{j=1}^n \\langle \\Phi_k, \\Phi_j \\rangle \\sum_{i=1}^n \\alpha_i \\langle \\Phi_j, \\Phi_i \\rangle = \\lambda \\sum_{i=1}^n \\alpha_i \\langle \\Phi_k, \\Phi_i \\rangle $$\nLet us define the centered Gram matrix $K_c \\in \\mathbb{R}^{n \\times n}$ with entries $(K_c)_{ij} = \\langle \\Phi_i, \\Phi_j \\rangle_{\\mathcal{H}}$. The equation becomes:\n$$ \\frac{1}{n}\\sum_{j=1}^n (K_c)_{kj} \\sum_{i=1}^n (K_c)_{ji} \\alpha_i = \\lambda \\sum_{i=1}^n (K_c)_{ki} \\alpha_i $$\nIn matrix notation, this is:\n$$ \\frac{1}{n} K_c (K_c \\alpha) = \\lambda (K_c \\alpha) $$\nThis implies that $K_c \\alpha$ is an eigenvector of $K_c$ with eigenvalue $n\\lambda$. If $K_c \\alpha \\neq 0$, then $\\alpha$ is also an eigenvector of $K_c$ with the same eigenvalue. Let $\\mu = n\\lambda$. The eigenvalue problem for the coefficients $\\alpha$ is:\n$$ K_c \\alpha = \\mu \\alpha $$\nThis demonstrates that solving the eigenvalue problem for the $n \\times n$ centered Gram matrix $K_c$ is equivalent to solving the PCA problem in the (potentially infinite-dimensional) feature space $\\mathcal{H}$. The eigenvalues $\\mu$ of $K_c$ are related to the eigenvalues $\\lambda$ of the covariance operator $C$ by $\\lambda = \\mu/n$.\n\nThe normalization condition $\\|w\\|_{\\mathcal{H}}=1$ imposes a constraint on the eigenvectors $\\alpha$:\n$$ 1 = \\|w\\|^2 = \\langle \\sum_{i=1}^n \\alpha_i \\Phi_i, \\sum_{j=1}^n \\alpha_j \\Phi_j \\rangle = \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j \\langle \\Phi_i, \\Phi_j \\rangle = \\alpha^{\\top} K_c \\alpha $$\nSubstituting $K_c \\alpha = \\mu \\alpha$:\n$$ 1 = \\alpha^{\\top} (\\mu \\alpha) = \\mu (\\alpha^{\\top} \\alpha) = \\mu \\|\\alpha\\|^2 $$\nThus, $\\|\\alpha\\|^2 = 1/\\mu$. If we find orthonormal eigenvectors $v_k$ of $K_c$ such that $K_c v_k = \\mu_k v_k$ and $v_k^{\\top}v_k=1$, the corresponding coefficient vector $\\alpha_k$ is a scaled version of $v_k$, specifically $\\alpha_k = v_k / \\sqrt{\\mu_k}$. The feature-space principal direction is then $w_k = \\frac{1}{\\sqrt{\\mu_k}}\\sum_{i=1}^n (v_k)_i \\Phi_i$.\n\n### 2. Derivation of the Centered Gram Matrix\n\nThe entries of the centered Gram matrix $K_c$ are $(K_c)_{ij} = \\langle \\Phi_i, \\Phi_j \\rangle_{\\mathcal{H}}$, where $\\Phi_i = \\phi(x_i) - \\bar{\\phi}$ and $\\bar{\\phi} = \\frac{1}{n}\\sum_{l=1}^n \\phi(x_l)$. We expand the inner product:\n$$ (K_c)_{ij} = \\langle \\phi(x_i) - \\bar{\\phi}, \\phi(x_j) - \\bar{\\phi} \\rangle = \\langle \\phi(x_i), \\phi(x_j) \\rangle - \\langle \\phi(x_i), \\bar{\\phi} \\rangle - \\langle \\bar{\\phi}, \\phi(x_j) \\rangle + \\langle \\bar{\\phi}, \\bar{\\phi} \\rangle $$\nWe express each term using the kernel function $k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle$ and the uncentered Gram matrix $K$ with $K_{ij}=k(x_i,x_j)$:\n\\begin{itemize}\n    \\item $\\langle \\phi(x_i), \\phi(x_j) \\rangle = K_{ij}$\n    \\item $\\langle \\phi(x_i), \\bar{\\phi} \\rangle = \\langle \\phi(x_i), \\frac{1}{n}\\sum_{l=1}^n \\phi(x_l) \\rangle = \\frac{1}{n}\\sum_{l=1}^n \\langle \\phi(x_i), \\phi(x_l) \\rangle = \\frac{1}{n}\\sum_{l=1}^n K_{il}$\n    \\item $\\langle \\bar{\\phi}, \\phi(x_j) \\rangle = \\frac{1}{n}\\sum_{l=1}^n K_{lj}$\n    \\item $\\langle \\bar{\\phi}, \\bar{\\phi} \\rangle = \\langle \\frac{1}{n}\\sum_{l=1}^n \\phi(x_l), \\frac{1}{n}\\sum_{m=1}^n \\phi(x_m) \\rangle = \\frac{1}{n^2}\\sum_{l=1}^n \\sum_{m=1}^n \\langle \\phi(x_l), \\phi(x_m) \\rangle = \\frac{1}{n^2}\\sum_{l,m} K_{lm}$\n\\end{itemize}\nCombining these gives:\n$$ (K_c)_{ij} = K_{ij} - \\frac{1}{n}\\sum_{l=1}^n K_{il} - \\frac{1}{n}\\sum_{l=1}^n K_{lj} + \\frac{1}{n^2}\\sum_{l,m} K_{lm} $$\nNow we show this is equivalent to $HKH$, where $H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ is the centering matrix ($I$ is the identity matrix, $\\mathbf{1}$ is the all-ones vector).\n$$ K_c = HKH = (I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}) K (I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}) $$\nExpanding this product:\n$$ K_c = K - \\frac{1}{n}K\\mathbf{1}\\mathbf{1}^{\\top} - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}K + \\frac{1}{n^2}\\mathbf{1}\\mathbf{1}^{\\top}K\\mathbf{1}\\mathbf{1}^{\\top} $$\nLet's analyze the $(i,j)$-th element of each matrix term:\n\\begin{itemize}\n    \\item $(K)_{ij} = K_{ij}$\n    \\item $(K\\mathbf{1}\\mathbf{1}^{\\top})_{ij} = (K\\mathbf{1})_i (\\mathbf{1}^{\\top})_j = (\\sum_{l=1}^n K_{il}) \\cdot 1 = \\sum_{l=1}^n K_{il}$\n    \\item $(\\mathbf{1}\\mathbf{1}^{\\top}K)_{ij} = (\\mathbf{1})_i (\\mathbf{1}^{\\top}K)_j = 1 \\cdot (\\sum_{l=1}^n K_{lj}) = \\sum_{l=1}^n K_{lj}$\n    \\item $(\\mathbf{1}\\mathbf{1}^{\\top}K\\mathbf{1}\\mathbf{1}^{\\top})_{ij} = (\\mathbf{1})_i (\\mathbf{1}^{\\top}K\\mathbf{1}) (\\mathbf{1}^{\\top})_j$. The middle term $\\mathbf{1}^{\\top}K\\mathbf{1} = \\sum_{l,m} K_{lm}$ is a scalar. Thus, the $(i,j)$-th element is $\\sum_{l,m} K_{lm}$.\n\\end{itemize}\nAssembling the terms for $(HKH)_{ij}$:\n$$ (HKH)_{ij} = K_{ij} - \\frac{1}{n}\\sum_{l=1}^n K_{il} - \\frac{1}{n}\\sum_{l=1}^n K_{lj} + \\frac{1}{n^2}\\sum_{l,m} K_{lm} $$\nThis matches the expression for $(K_c)_{ij}$ derived from first principles. Thus, centering the Gram matrix via $K_c=HKH$ is equivalent to computing the inner products of the centered feature vectors.\n\n### 3. Derivation of Out-of-Sample Embedding\n\nFor a new data point $x$, its embedding onto the $k$-th principal component is the projection of its centered feature vector $\\Phi(x) = \\phi(x) - \\bar{\\phi}$ onto the principal direction $w_k$:\n$$ y_k(x) = \\langle w_k, \\Phi(x) \\rangle_{\\mathcal{H}} $$\nUsing the expansion $w_k = \\sum_{i=1}^n (\\alpha_k)_i \\Phi_i$ where $\\alpha_k = v_k/\\sqrt{\\mu_k}$ (for $\\mu_k > 0$):\n$$ y_k(x) = \\langle \\sum_{i=1}^n (\\alpha_k)_i \\Phi_i, \\Phi(x) \\rangle = \\sum_{i=1}^n (\\alpha_k)_i \\langle \\Phi_i, \\Phi(x) \\rangle $$\nLet's define a vector $k_c(x) \\in \\mathbb{R}^n$ whose elements are $[k_c(x)]_i = \\langle \\Phi_i, \\Phi(x) \\rangle$. The projection is then $y_k(x) = \\alpha_k^{\\top} k_c(x)$. Substituting $\\alpha_k = v_k/\\sqrt{\\mu_k}$:\n$$ y_k(x) = \\frac{v_k^{\\top} k_c(x)}{\\sqrt{\\mu_k}} $$\nIf $\\mu_k=0$, the corresponding principal component does not exist (zero variance), and the projection is conventionally set to $0$.\n\nNow we derive the formula for the entries of $k_c(x)$:\n$$ [k_c(x)]_i = \\langle \\Phi_i, \\Phi(x) \\rangle = \\langle \\phi(x_i) - \\bar{\\phi}, \\phi(x) - \\bar{\\phi} \\rangle $$\n$$ = \\langle \\phi(x_i), \\phi(x) \\rangle - \\langle \\phi(x_i), \\bar{\\phi} \\rangle - \\langle \\bar{\\phi}, \\phi(x) \\rangle + \\langle \\bar{\\phi}, \\bar{\\phi} \\rangle $$\nThe terms are evaluated as before:\n\\begin{itemize}\n    \\item $\\langle \\phi(x_i), \\phi(x) \\rangle = k(x_i, x)$\n    \\item $\\langle \\phi(x_i), \\bar{\\phi} \\rangle = \\frac{1}{n}\\sum_{j=1}^n K_{ij}$\n    \\item $\\langle \\bar{\\phi}, \\phi(x) \\rangle = \\frac{1}{n}\\sum_{j=1}^n k(x_j, x)$\n    \\item $\\langle \\bar{\\phi}, \\bar{\\phi} \\rangle = \\frac{1}{n^2}\\sum_{j,l} K_{jl}$\n\\end{itemize}\nSo, for each $i \\in \\{1, \\dots, n\\}$:\n$$ [k_c(x)]_i = k(x_i, x) - \\frac{1}{n}\\sum_{j=1}^n K_{ij} - \\frac{1}{n}\\sum_{j=1}^n k(x_j, x) + \\frac{1}{n^2}\\sum_{j,l} K_{jl} $$\nIn vector form, let $k(x)$ be the vector with entries $k(x_i, x)$ and $\\mathbf{1}$ be the all-ones vector. The equation for the vector $k_c(x)$ is:\n$$ k_c(x) = k(x) - \\frac{1}{n}K\\mathbf{1} - \\frac{1}{n}(\\mathbf{1}^{\\top}k(x))\\mathbf{1} + \\frac{1}{n^2}(\\mathbf{1}^{\\top}K\\mathbf{1})\\mathbf{1} $$\nThis matches the formula provided in the problem statement and allows the out-of-sample embedding to be computed using only kernel evaluations with the training data and the new point.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate data generation, Kernel PCA computation,\n    and evaluation for the specified test cases.\n    \"\"\"\n\n    def generate_data(seed=12345):\n        \"\"\"\n        Generates simulated neural data according to the problem specification.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Parameters\n        n_neurons = 40\n        n_bins = 20\n        n_trials_train = 60\n        n_trials_total = n_trials_train + 1  # +1 for test trial\n        d = n_neurons * n_bins\n        \n        # Baseline firing rates\n        baseline_rates = rng.uniform(5, 15, size=n_neurons)\n        \n        # Temporal modulation\n        t = np.arange(n_bins)\n        t0, sigma_t = 10, 3\n        g = np.exp(-(t - t0)**2 / (2 * sigma_t**2))\n        \n        # Condition-specific amplitudes\n        a_A = rng.normal(3, 1, size=n_neurons)\n        a_B = rng.normal(-3, 1, size=n_neurons)\n        \n        all_trials_data = np.zeros((n_trials_total, d))\n        \n        for trial_idx in range(n_trials_total):\n            # The test trial (last one) is chosen to be from Condition A\n            if trial_idx  n_trials_train / 2 or trial_idx == n_trials_train:\n                base_amplitudes = a_A\n            else: # Condition B\n                base_amplitudes = a_B\n            \n            trial_rates = np.zeros((n_neurons, n_bins))\n            for neuron_idx in range(n_neurons):\n                # Add trial-wise amplitude noise\n                amp_noise = rng.normal(0, 0.5)\n                total_amplitude = base_amplitudes[neuron_idx] + amp_noise\n                \n                # Calculate rate using the additive model\n                rate_profile = baseline_rates[neuron_idx] + total_amplitude * g\n                trial_rates[neuron_idx, :] = rate_profile\n            \n            # Clip rates\n            clipped_rates = np.maximum(trial_rates, 0.1)\n            \n            # Generate spike counts from Poisson distribution\n            spike_counts = rng.poisson(clipped_rates)\n            \n            # Flatten and store\n            all_trials_data[trial_idx, :] = spike_counts.flatten()\n            \n        X_train = all_trials_data[:n_trials_train, :]\n        x_test = all_trials_data[n_trials_train, :]\n        \n        # Standardize data\n        mean_train = np.mean(X_train, axis=0)\n        std_train = np.std(X_train, axis=0)\n        # Avoid division by zero\n        std_train[std_train == 0] = 1.0\n        \n        X_train_std = (X_train - mean_train) / std_train\n        x_test_std = (x_test - mean_train) / std_train\n        \n        return X_train_std, x_test_std\n\n    def run_kernel_pca(X_train, x_test, sigma, case_params):\n        \"\"\"\n        Implements the Kernel PCA algorithm and computes results for a test case.\n        \"\"\"\n        n = X_train.shape[0]\n        \n        # Compute Gaussian kernel matrix\n        sq_dists = squareform(pdist(X_train, 'sqeuclidean'))\n        K = np.exp(-sq_dists / (2 * sigma**2))\n        \n        # Center the Gram matrix\n        H = np.eye(n) - np.ones((n, n)) / n\n        K_c = H @ K @ H\n        \n        # Solve the eigenvalue problem for the centered Gram matrix\n        evals, evecs = np.linalg.eigh(K_c)\n        \n        # Sort eigenvalues and eigenvectors in descending order\n        idx = np.argsort(evals)[::-1]\n        evals = evals[idx]\n        evecs = evecs[:, idx]\n        \n        # Clip numerical noise resulting in small negative eigenvalues\n        evals[evals  0] = 0\n        \n        # --- Compute case-specific outputs ---\n        case_id = case_params['id']\n        if case_id == 'A':\n            m = case_params['m']\n            total_variance = np.sum(evals)\n            if total_variance > 0:\n                explained_variances = evals[:m] / total_variance\n            else:\n                explained_variances = np.zeros(m)\n            return explained_variances.tolist()\n            \n        elif case_id == 'B':\n            m = case_params['m']\n            # Compute kernel vector for the test point\n            k_test = np.exp(-np.sum((X_train - x_test)**2, axis=1) / (2 * sigma**2))\n            \n            # Compute the centered kernel vector k_c(x_test)\n            one_n = np.ones(n)\n            k_c_test = (k_test - (K @ one_n) / n - \n                        (np.sum(k_test) / n) * one_n + \n                        (np.sum(K) / n**2) * one_n)\n            \n            # Compute out-of-sample embeddings\n            embeddings = []\n            for k in range(m):\n                mu_k = evals[k]\n                v_k = evecs[:, k]\n                if mu_k > 1e-15: # Use a small threshold for stability\n                    y_k = (v_k.T @ k_c_test) / np.sqrt(mu_k)\n                else:\n                    y_k = 0.0\n                embeddings.append(y_k)\n            return embeddings\n\n        elif case_id == 'C':\n            # Compute Frobenius norm of the centered Gram matrix\n            norm_kc = np.linalg.norm(K_c, 'fro')\n            return norm_kc\n\n    # Generate data once\n    X_train_std, x_test_std = generate_data()\n\n    # Define test cases\n    test_cases = [\n        {'id': 'A', 'sigma': 5.0, 'm': 3},\n        {'id': 'B', 'sigma': 0.1, 'm': 2},\n        {'id': 'C', 'sigma': 1000.0, 'm': None},\n    ]\n    \n    results = []\n    for case_params in test_cases:\n        result = run_kernel_pca(X_train_std, x_test_std, case_params['sigma'], case_params)\n        results.append(result)\n        \n    # Format final output string\n    res_a_str = f\"[{','.join(f'{x:.7f}' for x in results[0])}]\"\n    res_b_str = f\"[{','.join(f'{x:.7f}' for x in results[1])}]\"\n    res_c_str = f\"{results[2]:.7f}\"\n\n    print(f\"[{res_a_str},{res_b_str},{res_c_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Choosing the right dimensionality reduction tool requires understanding its underlying assumptions and limitations. This exercise moves from applying a single method to the crucial skill of comparative evaluation, challenging you to benchmark linear and nonlinear techniques on datasets with controlled properties. By identifying specific failure cases for PCA, Isomap, and kPCA, you will develop the practical wisdom needed to select the most appropriate algorithm for your research questions and data characteristics. ",
            "id": "4156684",
            "problem": "You are given the task of designing and evaluating dimensionality reduction approaches in the context of neuroscience data analysis under controlled noise and nonlinearity. The overall aim is to identify method-specific failure cases to guide practical choice of dimensionality reduction. The setting is as follows. Let there be a population of neurons with responses recorded across repeated trials, forming a data matrix $X \\in \\mathbb{R}^{T \\times N}$, where $T$ denotes the number of samples and $N$ denotes the number of neurons. The latent structure that drives neural responses is assumed to be low-dimensional, with ground-truth coordinates $Z \\in \\mathbb{R}^{T \\times d}$ known in synthetic tasks.\n\nFundamental base:\n- Distances: For any vectors $a, b \\in \\mathbb{R}^{m}$, the Euclidean distance is $||a - b||_2 = \\sqrt{\\sum_{i=1}^{m} (a_i - b_i)^2}$, and the squared Euclidean distance is $\\sum_{i=1}^{m} (a_i - b_i)^2$.\n- Singular Value Decomposition (SVD): Any centered data matrix $X$ admits $X = U \\Sigma V^\\top$ with orthonormal $U, V$ and nonnegative singular values in $\\Sigma$, which supplies principal directions via Principal Component Analysis (PCA).\n- Kernel methods: Kernel Principal Component Analysis (kPCA) uses a positive semi-definite kernel matrix $K$ to map implicitly into a feature space and finds principal directions by eigen-decomposition of a centered kernel.\n- Graph geodesics and Isometric Mapping (Isomap): A $k$-nearest neighbor graph can be built from $X$ using Euclidean distances; geodesic distances are shortest-path distances in this graph. Classical Multidimensional Scaling (MDS) then finds coordinates matching these distances by double-centering the squared distance matrix.\n- Orthogonal Procrustes alignment: Given embeddings $Y, Z \\in \\mathbb{R}^{T \\times d}$, an optimal rotation and isotropic scaling are found by solving $\\min_{R, s} ||s Y R - Z||_F$ subject to $R^\\top R = I$, which aligns $Y$ to $Z$ up to rigid transform and scale.\n- Stress measure: Given two distance matrices $D_Y, D_Z \\in \\mathbb{R}^{T \\times T}$, define\n$$\\text{stress}(Y,Z) = \\sqrt{\\frac{\\sum_{ij} \\left(D_Y[i,j] - D_Z[i,j]\\right)^2}{\\sum_{ij} D_Z[i,j]^2}}.$$\n\nYour program must:\n1) Generate $X$ for each test case using a specified latent manifold and a specified mapping to high-dimensional neural responses. You must incorporate controlled Gaussian noise with standard deviation $\\sigma$ for each test case. Angles must be treated in radians.\n2) Apply three methods to produce $d = 2$ dimensional embeddings: Principal Component Analysis (PCA), Isometric Mapping (Isomap), and Kernel Principal Component Analysis (kPCA) using a radial basis function (RBF) kernel $K_{ij} = \\exp\\left(-\\gamma ||x_i - x_j||_2^2\\right)$ with a given bandwidth parameter $\\gamma$.\n3) Align each recovered embedding $Y$ to the ground-truth $Z$ using orthogonal Procrustes (rotation and isotropic scaling only). Compute the stress using pairwise Euclidean distances between aligned embeddings and ground-truth coordinates.\n4) For each method in each test case, determine failure as a boolean: failure is $1$ if stress exceeds a specified threshold $\\tau$ or if the method yields a degenerate embedding (e.g., not finite due to a disconnected graph for Isomap); otherwise failure is $0$.\n\nThe test suite consists of the following four parameterized tasks. All numerical values are to be interpreted as real numbers:\n- Test case $1$ (Linear plane, low noise):\n    - Latent manifold: $Z$ sampled uniformly from $[-1,1]^2$.\n    - Mapping: linear mixing into neural responses, $X = Z W^\\top + \\varepsilon$ with $W \\in \\mathbb{R}^{N \\times 2}$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n    - Parameters: $T = 60$, $N = 40$, $\\sigma = 0.05$, $k = 8$ (for Isomap), $\\gamma = 2.0$ (for kPCA), $\\tau = 0.15$.\n\n- Test case $2$ (Swiss roll, moderate noise):\n    - Latent manifold: parameters $t$ sampled uniformly from $[1.5\\pi,4.5\\pi]$, height $h$ from $[0,1]$; ground truth $Z = [t, h]$.\n    - Observed coordinates: $(x, y, z) = (t \\cos t, h, t \\sin t)$ forming a swiss roll.\n    - Mapping: nonlinear basis of observed coordinates into neural responses, including polynomial and trigonometric terms; additive Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n    - Parameters: $T = 80$, $N = 50$, $\\sigma = 0.1$, $k = 10$, $\\gamma = 5.0$, $\\tau = 0.20$.\n\n- Test case $3$ (Ring with high nonlinearity and noise):\n    - Latent manifold: angle $\\theta$ sampled uniformly from $[0,2\\pi]$, in radians. Ground truth $Z = [\\cos \\theta, \\sin \\theta]$.\n    - Distortion: radial factor $r = 1 + 0.3 \\sin(3\\theta)$ produces observed ring coordinates $(u,v) = (r \\cos \\theta, r \\sin \\theta)$.\n    - Mapping: neural responses via nonlinear, orientation-like tuning resembling Von Mises with modulation by $r$; additive Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n    - Parameters: $T = 64$, $N = 60$, $\\sigma = 0.2$, $k = 12$, $\\gamma = 3.0$, $\\tau = 0.22$.\n\n- Test case $4$ (Swiss roll with graph disconnection):\n    - Same latent manifold and mapping design as test case $2$, but choose $k$ small to induce disconnected $k$-nearest neighbor graph.\n    - Parameters: $T = 80$, $N = 50$, $\\sigma = 0.05$, $k = 3$, $\\gamma = 5.0$, $\\tau = 0.20$.\n\nYour program must set a fixed random seed to ensure reproducible results. For each test case, compute the failure booleans for the three methods in the order [PCA, Isomap, kPCA], yielding a list of three integers in $\\{0,1\\}$. Aggregate the results for all four test cases into a single list of lists in the required final output format.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the comma-separated list of the three failure booleans per test case, also enclosed in square brackets. For example, the structure should be of the form $[[a_1,a_2,a_3],[b_1,b_2,b_3],[c_1,c_2,c_3],[d_1,d_2,d_3]]$, where each $a_i, b_i, c_i, d_i \\in \\{0,1\\}$.",
            "solution": "The problem has been validated and is determined to be sound. While the descriptions for generating the high-dimensional data $X$ from the latent manifolds are not given as explicit mathematical formulae, they are sufficiently descriptive to allow for formalization. This task falls within the scope of designing the evaluation, as specified in the problem statement. All other aspects of the problem—the algorithms to be used, the evaluation metrics, and the parameters—are clearly defined and scientifically grounded in the field of machine learning and data analysis.\n\nWe will proceed by first specifying the precise mathematical models for data generation. Then, we will detail the implementation of the three dimensionality reduction algorithms (PCA, Isomap, kPCA), the Procrustes alignment procedure, and the stress calculation. Finally, we apply this entire pipeline to the four specified test cases.\n\n**1. Data Generation Models**\n\nA fixed random seed is used for all random processes to ensure reproducibility. The target embedding dimension is $d=2$ for all cases.\n\n- **Test Case 1: Linear Plane**\n  The latent coordinates $Z \\in \\mathbb{R}^{T \\times d}$ are sampled from a uniform distribution over the square $[-1,1]^2$. The high-dimensional data $X \\in \\mathbb{R}^{T \\times N}$ is a linear projection of $Z$ plus Gaussian noise. The mapping is $X = Z W^\\top + \\varepsilon$, where $W \\in \\mathbb{R}^{N \\times d}$ is a mixing matrix and $\\varepsilon \\in \\mathbb{R}^{T \\times N}$ is a noise matrix. The entries of $W$ are drawn independently from a standard normal distribution $\\mathcal{N}(0,1)$, and the entries of $\\varepsilon$ are from $\\mathcal{N}(0, \\sigma^2)$.\n  The parameters are $T=60$, $N=40$, $\\sigma=0.05$.\n\n- **Test Case 2: Swiss Roll**\n  The latent manifold is parameterized by $t$ and $h$. We sample $t \\in [1.5\\pi, 4.5\\pi]$ and $h \\in [0, 1]$ uniformly. The ground-truth coordinates are $Z = [t, h] \\in \\mathbb{R}^{T \\times 2}$. These are mapped to a 3D Swiss roll structure $(x_i, y_i, z_i) = (t_i \\cos t_i, h_i, t_i \\sin t_i)$. Neural responses are a nonlinear function of these 3D coordinates. We define a basis expansion $\\Phi(x,y,z) \\in \\mathbb{R}^6$ as $\\Phi(x,y,z) = [x, y, z, x^2, z^2, xz]$. The high-dimensional data is then $X = \\Phi(x,y,z) W^\\top + \\varepsilon$, where $W \\in \\mathbb{R}^{N \\times 6}$ is a random mixing matrix with entries from $\\mathcal{N}(0,1)$, and $\\varepsilon$ is Gaussian noise with standard deviation $\\sigma$.\n  The parameters are $T=80$, $N=50$, $\\sigma=0.1$.\n\n- **Test Case 3: Distorted Ring**\n  The latent manifold is a circle, parameterized by an angle $\\theta \\in [0, 2\\pi]$ sampled uniformly. The ground-truth coordinates are $Z=[\\cos \\theta, \\sin \\theta]$. This circle is radially distorted in a 2D observation space by a factor $r = 1 + 0.3 \\sin(3\\theta)$. The neural responses are modeled as Von Mises-like tuning curves. For each neuron $j \\in \\{1, \\dots, N\\}$, we define a preferred angle $\\phi_j$, spaced uniformly in $[0, 2\\pi]$. The response of neuron $j$ for sample $i$ with angle $\\theta_i$ and radial factor $r_i$ is $X_{ij}^{\\text{clean}} = r_i \\exp(\\kappa \\cos(\\theta_i - \\phi_j))$ with concentration $\\kappa=3$. The final data is $X = X^{\\text{clean}} + \\varepsilon$, with noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n  The parameters are $T=64$, $N=60$, $\\sigma=0.2$.\n\n- **Test Case 4: Disconnected Swiss Roll**\n  The data generation follows the exact same procedure as in Test Case 2. The parameters are $T=80$, $N=50$, $\\sigma=0.05$. The key difference for this test case is the Isomap parameter $k=3$, which is chosen to be small enough to likely cause graph disconnection.\n\n**2. Dimensionality Reduction Algorithms**\n\n- **Principal Component Analysis (PCA)**: PCA finds a low-dimensional linear projection of the data that preserves maximal variance.\n  1. Center the data matrix: $X_c = X - \\bar{X}$, where $\\bar{X}$ is the mean of each column (neuron).\n  2. Compute the singular value decomposition (SVD) of the centered data: $X_c = U \\Sigma V^\\top$.\n  3. The principal components (the embedding) are the projections of the data onto the first $d$ principal directions (the first $d$ columns of $V$): $Y_{PCA} = X_c V_{:,:d}$.\n\n- **Isometric Mapping (Isomap)**: Isomap is a manifold learning technique that estimates the intrinsic geometry of the data by approximating geodesic distances.\n  1. For each point, find its $k$ nearest neighbors based on Euclidean distance in the high-dimensional space $\\mathbb{R}^N$.\n  2. Construct a neighborhood graph where nodes are data points and an edge exists between a point and its neighbors. The edge weight is their Euclidean distance.\n  3. Compute the shortest path distances between all pairs of nodes in the graph (e.g., using Dijkstra's or Floyd-Warshall algorithm). This yields the geodesic distance matrix $D_G$. If the graph is disconnected, some distances will be infinite, which constitutes a failure case for the algorithm.\n  4. Apply classical Multidimensional Scaling (MDS) to $D_G$. First, compute the Gram matrix $B = -\\frac{1}{2} J D_G^2 J$, where $D_G^2$ is the element-wise square of $D_G$ and $J=I - \\frac{1}{T}\\mathbf{1}\\mathbf{1}^\\top$ is the centering matrix.\n  5. Compute the top $d$ eigenvalues $\\lambda_1, \\dots, \\lambda_d$ and corresponding eigenvectors $v_1, \\dots, v_d$ of $B$.\n  6. The embedding is $Y_{Isomap} = [ \\sqrt{\\lambda_1}v_1, \\dots, \\sqrt{\\lambda_d}v_d ]$.\n\n- **Kernel Principal Component Analysis (kPCA)**: kPCA generalizes PCA to nonlinear settings by using the \"kernel trick\".\n  1. Compute the kernel matrix $K \\in \\mathbb{R}^{T \\times T}$ using the radial basis function (RBF) kernel: $K_{ij} = \\exp(-\\gamma ||x_i - x_j||_2^2)$, where $x_i, x_j$ are rows of $X$.\n  2. Center the kernel matrix: $K_c = J K J$.\n  3. Compute the top $d$ eigenvalues $\\lambda_1, \\dots, \\lambda_d$ and corresponding eigenvectors $\\alpha_1, \\dots, \\alpha_d$ of $K_c$. The eigenvectors are normalized such that $\\alpha_k^\\top\\alpha_k=1$.\n  4. The embedding is $Y_{kPCA} = [ \\sqrt{\\lambda_1}\\alpha_1, \\dots, \\sqrt{\\lambda_d}\\alpha_d ]$.\n\n**3. Evaluation Procedure**\n\n- **Orthogonal Procrustes Alignment**: To compare the recovered embedding $Y$ with the ground-truth $Z$, we must align them. We solve $\\min_{R, s} ||s Y R - Z||_F$ for a rotation matrix $R$ and a scalar $s$.\n  1. Center both matrices: $Y_c = Y - \\bar{Y}$, $Z_c = Z - \\bar{Z}$.\n  2. Compute the SVD of the cross-covariance matrix: $M = Y_c^\\top Z_c = U \\Sigma V^\\top$.\n  3. The optimal rotation is $R = U V^\\top$.\n  4. The optimal scale is $s = \\text{trace}(\\Sigma) / \\text{trace}(Y_c^\\top Y_c)$.\n  5. The aligned embedding is $Y_{aligned} = s Y_c R + \\bar{Z}$.\n\n- **Stress Calculation**: The quality of the embedding is quantified by the stress metric, which measures the discrepancy between pairwise distances in the aligned embedding $Y_{aligned}$ and the ground-truth $Z$.\n  1. Compute the pairwise Euclidean distance matrices $D_{Y_{aligned}}$ and $D_Z$.\n  2. The stress is calculated as:\n  $$ \\text{stress}(Y_{aligned}, Z) = \\sqrt{\\frac{\\sum_{ij} \\left(D_{Y_{aligned}}[i,j] - D_Z[i,j]\\right)^2}{\\sum_{ij} D_Z[i,j]^2}} $$\n\n- **Failure Determination**: For each method on each test case, a failure is recorded (boolean value $1$) if the stress exceeds a given threshold $\\tau$, or if the algorithm itself fails (e.g., Isomap on a disconnected graph). Otherwise, it is a success (boolean value $0$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd, eigh\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.csgraph import shortest_path\n\ndef generate_data(case_params):\n    \"\"\"Generates synthetic data for a given test case.\"\"\"\n    case_num = case_params['case']\n    T = case_params['T']\n    N = case_params['N']\n    sigma = case_params['sigma']\n    np.random.seed(42) # Ensure reproducibility within data generation\n    \n    if case_num == 1:\n        # Linear plane\n        d = 2\n        Z = np.random.uniform(-1, 1, size=(T, d))\n        W = np.random.randn(N, d)\n        noise = np.random.randn(T, N) * sigma\n        X = Z @ W.T + noise\n        return X, Z\n\n    elif case_num == 2 or case_num == 4:\n        # Swiss roll\n        t = np.random.uniform(1.5 * np.pi, 4.5 * np.pi, size=T)\n        h = np.random.uniform(0, 1, size=T)\n        Z = np.vstack((t, h)).T\n\n        obs_x = t * np.cos(t)\n        obs_y = h\n        obs_z = t * np.sin(t)\n        \n        # Nonlinear basis mapping\n        phi_dim = 6\n        phi_data = np.vstack((obs_x, obs_y, obs_z, obs_x**2, obs_z**2, obs_x*obs_z)).T\n        W = np.random.randn(N, phi_dim)\n        noise = np.random.randn(T, N) * sigma\n        X = phi_data @ W.T + noise\n        return X, Z\n\n    elif case_num == 3:\n        # Distorted ring\n        theta = np.random.uniform(0, 2 * np.pi, size=T)\n        Z = np.vstack((np.cos(theta), np.sin(theta))).T\n        \n        r = 1 + 0.3 * np.sin(3 * theta)\n        \n        # Von Mises-like tuning curves\n        kappa = 3.0\n        pref_angles = np.linspace(0, 2 * np.pi, N, endpoint=False)\n        \n        X_clean = np.zeros((T, N))\n        for i in range(T):\n            for j in range(N):\n                X_clean[i, j] = r[i] * np.exp(kappa * np.cos(theta[i] - pref_angles[j]))\n        \n        noise = np.random.randn(T, N) * sigma\n        X = X_clean + noise\n        return X, Z\n    \n    return None, None\n\ndef do_pca(X, d=2):\n    \"\"\"Performs Principal Component Analysis.\"\"\"\n    X_c = X - X.mean(axis=0)\n    _, _, Vt = svd(X_c, full_matrices=False)\n    V = Vt.T\n    Y = X_c @ V[:, :d]\n    return Y, False\n\ndef do_isomap(X, d=2, k=8):\n    \"\"\"Performs Isometric Mapping.\"\"\"\n    T = X.shape[0]\n    dist_mat = squareform(pdist(X, 'euclidean'))\n    \n    adj = np.zeros((T, T))\n    for i in range(T):\n        neighbors = np.argsort(dist_mat[i, :])[1:k+1]\n        adj[i, neighbors] = dist_mat[i, neighbors]\n        adj[neighbors, i] = dist_mat[neighbors, i]\n\n    graph = csr_matrix(adj)\n    D_geo = shortest_path(csgraph=graph, directed=False, method='auto', unweighted=False)\n\n    if np.any(np.isinf(D_geo)):\n        return None, True # Failure due to disconnected graph\n\n    D2 = D_geo**2\n    J = np.eye(T) - np.ones((T, T)) / T\n    B = -0.5 * J @ D2 @ J\n\n    try:\n        eigvals, eigvecs = eigh(B, subset_by_index=[T - d, T - 1])\n    except np.linalg.LinAlgError:\n        return None, True # Eigendecomposition failed\n\n    eigvals, eigvecs = np.flip(eigvals), np.flip(eigvecs, axis=1)\n    \n    # Handle potential negative eigenvalues from numerical instability\n    valid_eigvals = np.maximum(eigvals[:d], 0)\n    \n    Y = eigvecs[:, :d] @ np.diag(np.sqrt(valid_eigvals))\n    return Y, False\n\ndef do_kpca(X, d=2, gamma=1.0):\n    \"\"\"Performs Kernel Principal Component Analysis.\"\"\"\n    T = X.shape[0]\n    D_euc_sq = squareform(pdist(X, 'sqeuclidean'))\n    K = np.exp(-gamma * D_euc_sq)\n    \n    J = np.eye(T) - np.ones((T, T)) / T\n    K_c = J @ K @ J\n    \n    try:\n        eigvals, eigvecs = eigh(K_c, subset_by_index=[T - d, T - 1])\n    except np.linalg.LinAlgError:\n        return None, True # Eigendecomposition failed\n\n    eigvals, eigvecs = np.flip(eigvals), np.flip(eigvecs, axis=1)\n    \n    valid_eigvals = np.maximum(eigvals[:d], 0)\n    Y = eigvecs[:, :d] * np.sqrt(valid_eigvals)\n    return Y, False\n\ndef procrustes_align(Y, Z):\n    \"\"\"Aligns embedding Y to ground-truth Z using rotation and isotropic scaling.\"\"\"\n    Y_mean = Y.mean(axis=0)\n    Z_mean = Z.mean(axis=0)\n    Yc = Y - Y_mean\n    Zc = Z - Z_mean\n\n    M = Yc.T @ Zc\n    U, s, Vt = svd(M)\n    R = U @ Vt\n    \n    # Avoid division by zero for degenerate embeddings\n    Yc_ss = np.sum(Yc**2)\n    if Yc_ss  1e-12:\n      return Y # Cannot align a zero-variance embedding\n\n    scale = np.sum(s) / Yc_ss\n    Y_aligned = scale * Yc @ R + Z_mean\n    return Y_aligned\n\ndef calculate_stress(Y_aligned, Z):\n    \"\"\"Calculates the stress between aligned embedding and ground-truth.\"\"\"\n    Dy = pdist(Y_aligned)\n    Dz = pdist(Z)\n    \n    # Avoid division by zero if Z has no variance\n    Dz_ss = np.sum(Dz**2)\n    if Dz_ss  1e-12:\n        return 0.0 if np.sum((Dy - Dz)**2)  1e-12 else np.inf\n        \n    stress = np.sqrt(np.sum((Dy - Dz)**2) / Dz_ss)\n    return stress\n\ndef solve():\n    \"\"\"Main function to run the test suite.\"\"\"\n    np.random.seed(42) # Set global seed for reproducibility\n    \n    test_cases = [\n        {'case': 1, 'T': 60, 'N': 40, 'sigma': 0.05, 'k': 8, 'gamma': 2.0, 'tau': 0.15},\n        {'case': 2, 'T': 80, 'N': 50, 'sigma': 0.1, 'k': 10, 'gamma': 5.0, 'tau': 0.20},\n        {'case': 3, 'T': 64, 'N': 60, 'sigma': 0.2, 'k': 12, 'gamma': 3.0, 'tau': 0.22},\n        {'case': 4, 'T': 80, 'N': 50, 'sigma': 0.05, 'k': 3, 'gamma': 5.0, 'tau': 0.20},\n    ]\n\n    all_results = []\n    \n    methods = [\n        ('pca', do_pca),\n        ('isomap', do_isomap),\n        ('kpca', do_kpca)\n    ]\n\n    for params in test_cases:\n        X, Z = generate_data(params)\n        tau = params['tau']\n        case_failures = []\n        \n        for name, func in methods:\n            failure = 0\n            \n            # Select correct parameters for the method\n            method_params = {'d': 2}\n            if name == 'isomap':\n                method_params['k'] = params['k']\n            elif name == 'kpca':\n                method_params['gamma'] = params['gamma']\n\n            Y, algo_failed = func(X, **method_params)\n            \n            if algo_failed or Y is None:\n                failure = 1\n            else:\n                Y_aligned = procrustes_align(Y, Z)\n                stress = calculate_stress(Y_aligned, Z)\n                if stress > tau:\n                    failure = 1\n            \n            case_failures.append(failure)\n        all_results.append(case_failures)\n        \n    # Format the final output\n    output_str = '[' + ','.join(['[' + ','.join(map(str, res)) + ']' for res in all_results]) + ']'\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}