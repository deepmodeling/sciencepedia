{
    "hands_on_practices": [
        {
            "introduction": "要将高斯过程因子分析 (GPFA) 模型应用于真实的神经脉冲计数数据，我们必须通过优化其参数来拟合模型。基于梯度的优化方法是实现这一目标的核心。本练习  将指导你推导泊松观测模型下的对数似然函数梯度，这是训练适用于脉冲数据的 GPFA 模型的关键一步。通过这个推导，你将加深对统计模型和优化算法之间联系的理解。",
            "id": "4166232",
            "problem": "考虑一个用于多神经元脉冲计数的高斯过程因子分析（GPFA）的标准观测模型。有 $p$ 个神经元，$q$ 个潜维度和 $T$ 个离散时间窗。在时间 $t$ 的潜状态为 $x_t \\in \\mathbb{R}^q$，观测到的脉冲计数为 $y_t \\in \\mathbb{N}^p$，其中对于神经元 $i \\in \\{1,\\dots,p\\}$ 和时间 $t \\in \\{1,\\dots,T\\}$，其条目为 $y_{it} \\in \\{0,1,2,\\dots\\}$。在给定潜状态 $\\{x_t\\}_{t=1}^T$ 的条件下，观测值被建模为在神经元和时间上条件独立的泊松分布，并使用对数连接函数：\n$$y_{it} \\mid x_t \\sim \\text{Poisson}\\left(\\lambda_{it}\\right), \\quad \\lambda_{it} = \\exp\\left(c_i^{\\top} x_t + d_i\\right),$$\n其中 $c_i^{\\top}$ 是载荷矩阵 $C \\in \\mathbb{R}^{p \\times q}$ 的第 $i$ 行，而 $d \\in \\mathbb{R}^p$ 是一个偏置向量。令 $\\lambda_t = \\exp\\left(C x_t + d\\right) \\in \\mathbb{R}^p$ 表示时间 $t$ 的强度向量，其中指数函数是逐元素应用的。假设在给定 $\\{x_t\\}$ 的情况下，$y_{it}$ 在所有 $i$ 和 $t$ 上都是条件独立的。\n\n您只能使用基本定义：对于 $y \\in \\{0,1,2,\\dots\\}$，速率为 $\\lambda > 0$ 的泊松概率质量函数为\n$$P(y \\mid \\lambda) = \\frac{\\exp(-\\lambda)\\,\\lambda^{y}}{y!}.$$\n\n将 $\\{x_t\\}_{t=1}^T$ 视为固定且已知的。从第一性原理出发，推导条件对数似然 $\\ell(C,d) = \\ln P\\left(\\{y_t\\}_{t=1}^T \\mid \\{x_t\\}_{t=1}^T, C, d\\right)$ 关于 $C$ 和 $d$ 的梯度，并用 $x_t$、$y_t$、$C$ 和 $d$ 明确表示您的结果。您的最终表达式不得包含任何期望或未指定的常数。使用 $\\exp(\\cdot)$ 表示逐元素的指数函数，并将您的答案写成对 $t \\in \\{1,\\dots,T\\}$ 求和的紧凑矩阵表达式。\n\n不需要数值近似。最终答案必须是单一的闭式解析表达式。如果您提供多个分量，请将它们一起放在一个单行矩阵中。",
            "solution": "目标是推导条件对数似然 $\\ell(C,d) = \\ln P\\left(\\{y_t\\}_{t=1}^T \\mid \\{x_t\\}_{t=1}^T, C, d\\right)$ 关于载荷矩阵 $C \\in \\mathbb{R}^{p \\times q}$ 和偏置向量 $d \\in \\mathbb{R}^p$ 的梯度。\n\n观测模型指出，在给定潜状态 $\\{x_t\\}_{t=1}^T$ 的条件下，脉冲计数 $y_{it}$ 是独立的泊松随机变量，其速率为 $\\lambda_{it} = \\exp(c_i^{\\top} x_t + d_i)$。单个观测值 $y_{it}$ 的概率质量函数由下式给出\n$$P(y_{it} \\mid \\lambda_{it}) = \\frac{\\exp(-\\lambda_{it})\\,\\lambda_{it}^{y_{it}}}{y_{it}!}$$\n由于条件独立性假设，总似然是所有神经元 $i \\in \\{1, \\dots, p\\}$ 和时间窗 $t \\in \\{1, \\dots, T\\}$ 的单个概率的乘积：\n$$P\\left(\\{y_t\\}_{t=1}^T \\mid \\{x_t\\}_{t=1}^T, C, d\\right) = \\prod_{t=1}^T \\prod_{i=1}^p P(y_{it} \\mid \\lambda_{it})$$\n条件对数似然 $\\ell(C,d)$ 是该似然的自然对数：\n$$\\ell(C,d) = \\ln \\left( \\prod_{t=1}^T \\prod_{i=1}^p \\frac{\\exp(-\\lambda_{it})\\,\\lambda_{it}^{y_{it}}}{y_{it}!} \\right) = \\sum_{t=1}^T \\sum_{i=1}^p \\ln \\left( \\frac{\\exp(-\\lambda_{it})\\,\\lambda_{it}^{y_{it}}}{y_{it}!} \\right)$$\n利用对数的性质，上式可简化为：\n$$\\ell(C,d) = \\sum_{t=1}^T \\sum_{i=1}^p \\left( \\ln(\\exp(-\\lambda_{it})) + \\ln(\\lambda_{it}^{y_{it}}) - \\ln(y_{it}!) \\right) = \\sum_{t=1}^T \\sum_{i=1}^p \\left( -\\lambda_{it} + y_{it} \\ln(\\lambda_{it}) - \\ln(y_{it}!) \\right)$$\n我们已知对数连接函数为 $\\lambda_{it} = \\exp(c_i^{\\top} x_t + d_i)$，这意味着 $\\ln(\\lambda_{it}) = c_i^{\\top} x_t + d_i$。将这些代入对数似然表达式中得到：\n$$\\ell(C,d) = \\sum_{t=1}^T \\sum_{i=1}^p \\left( -\\exp(c_i^{\\top} x_t + d_i) + y_{it} (c_i^{\\top} x_t + d_i) - \\ln(y_{it}!) \\right)$$\n为了求梯度，我们对这个表达式关于参数求导。\n\n首先，我们计算关于偏置向量 $d$ 的梯度。梯度 $\\nabla_d \\ell$ 是一个大小为 $p \\times 1$ 的向量，其第 $j$ 个分量是 $\\frac{\\partial \\ell}{\\partial d_j}$。\n$$\\frac{\\partial \\ell}{\\partial d_j} = \\frac{\\partial}{\\partial d_j} \\sum_{t=1}^T \\sum_{i=1}^p \\left( -\\exp(c_i^{\\top} x_t + d_i) + y_{it} (c_i^{\\top} x_t + d_i) - \\ln(y_{it}!) \\right)$$\n仅当 $i=j$ 时，导数才非零。对于所有其他 $i \\neq j$ 的项，关于 $d_j$ 的导数为零。\n$$\\frac{\\partial \\ell}{\\partial d_j} = \\sum_{t=1}^T \\frac{\\partial}{\\partial d_j} \\left( -\\exp(c_j^{\\top} x_t + d_j) + y_{jt} (c_j^{\\top} x_t + d_j) \\right)$$\n使用链式法则，$\\frac{\\partial}{\\partial u} \\exp(u) = \\exp(u)$，我们得到：\n$$\\frac{\\partial \\ell}{\\partial d_j} = \\sum_{t=1}^T \\left( -\\exp(c_j^{\\top} x_t + d_j) \\cdot 1 + y_{jt} \\cdot 1 \\right) = \\sum_{t=1}^T (y_{jt} - \\exp(c_j^{\\top} x_t + d_j))$$\n回想一下，$\\lambda_{jt} = \\exp(c_j^{\\top} x_t + d_j)$，因此我们有 $\\frac{\\partial \\ell}{\\partial d_j} = \\sum_{t=1}^T (y_{jt} - \\lambda_{jt})$。\n将这些偏导数组合成梯度向量 $\\nabla_d \\ell \\in \\mathbb{R}^p$，并使用向量表示法 $y_t \\in \\mathbb{R}^p$ 和 $\\lambda_t = \\exp(Cx_t+d) \\in \\mathbb{R}^p$，我们得到紧凑的矩阵表达式：\n$$\\nabla_d \\ell(C,d) = \\sum_{t=1}^T (y_t - \\lambda_t) = \\sum_{t=1}^T (y_t - \\exp(Cx_t + d))$$\n接下来，我们计算关于载荷矩阵 $C$ 的梯度。梯度 $\\nabla_C \\ell$ 是一个大小为 $p \\times q$ 的矩阵，其 $(j,k)$ 元素是 $\\frac{\\partial \\ell}{\\partial C_{jk}}$。项 $c_i^{\\top}x_t$ 可以写成 $\\sum_{m=1}^q C_{im}x_{mt}$。\n$$\\frac{\\partial \\ell}{\\partial C_{jk}} = \\frac{\\partial}{\\partial C_{jk}} \\sum_{t=1}^T \\sum_{i=1}^p \\left( -\\exp(c_i^{\\top} x_t + d_i) + y_{it} (c_i^{\\top} x_t + d_i) - \\ln(y_{it}!) \\right)$$\n仅当行索引 $i=j$ 时，导数才非零。\n$$\\frac{\\partial \\ell}{\\partial C_{jk}} = \\sum_{t=1}^T \\frac{\\partial}{\\partial C_{jk}} \\left( -\\exp(c_j^{\\top} x_t + d_j) + y_{jt} (c_j^{\\top} x_t + d_j) \\right)$$\n参数 $c_j^{\\top} x_t + d_j = \\sum_{m=1}^q C_{jm}x_{mt} + d_j$ 关于 $C_{jk}$ 的导数是 $x_{kt}$，即向量 $x_t$ 的第 $k$ 个分量。\n应用链式法则：\n$$\\frac{\\partial \\ell}{\\partial C_{jk}} = \\sum_{t=1}^T \\left( (-\\exp(c_j^{\\top} x_t + d_j) + y_{jt}) \\cdot \\frac{\\partial (c_j^{\\top} x_t + d_j)}{\\partial C_{jk}} \\right) = \\sum_{t=1}^T (y_{jt} - \\exp(c_j^{\\top} x_t + d_j)) \\cdot x_{kt}$$\n因此，$(\\nabla_C \\ell)_{jk} = \\sum_{t=1}^T (y_{jt} - \\lambda_{jt}) x_{kt}$。\n这个梯度矩阵的 $(j,k)$ 元素的表达式，对应于在 $t$ 上求和的外积 $(y_t - \\lambda_t)x_t^{\\top}$ 的 $(j,k)$ 元素。项 $(y_t - \\lambda_t)$ 是一个 $p \\times 1$ 的向量，而 $x_t^{\\top}$ 是一个 $1 \\times q$ 的向量，所以它们的外积是一个 $p \\times q$ 的矩阵。\n因此，梯度矩阵 $\\nabla_C \\ell$ 可以紧凑地写为：\n$$\\nabla_C \\ell(C,d) = \\sum_{t=1}^T (y_t - \\lambda_t) x_t^{\\top} = \\sum_{t=1}^T (y_t - \\exp(Cx_t + d)) x_t^{\\top}$$\n这两个梯度，$\\nabla_C \\ell$ 和 $\\nabla_d \\ell$，就是所要求的结果。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sum_{t=1}^{T} \\left(y_t - \\exp(Cx_t + d)\\right)x_t^{\\top} & \\sum_{t=1}^{T} \\left(y_t - \\exp(Cx_t + d)\\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在掌握了如何优化模型之后，一个关键的实际问题随之而来：算法的计算成本如何随数据量的增加而扩展？本练习  将引导你分析精确 GPFA 推断的计算复杂度。理解这种 $O(T^3)$ 的计算缩放行为，揭示了为何精确 GPFA 在处理长时间序列数据时不可行，并为稀疏近似方法的应用提供了理论动机。",
            "id": "4173640",
            "problem": "考虑将高斯过程因子分析 (GPFA) 应用于一个大小为 $N$ 的神经元群体，该群体在 $T$ 个均匀采样的时间点上进行测量。在时间 $t$ 的潜状态为 $x_{t} \\in \\mathbb{R}^{k}$，观测模型为线性高斯模型：$y_{t} = C x_{t} + d + \\varepsilon_{t}$，其中 $\\varepsilon_{t} \\sim \\mathcal{N}(0, R)$，$R \\in \\mathbb{R}^{N \\times N}$ 是对角矩阵，$C \\in \\mathbb{R}^{N \\times k}$ 是一个载荷矩阵。每个潜维度 $x^{(j)} = \\{x^{(j)}_{t}\\}_{t=1}^{T}$ 是一个从零均值高斯过程 (GP) 中抽取的独立样本，其协方差函数为 $\\kappa_{j}(\\cdot, \\cdot)$，因此 $x^{(j)} \\sim \\mathcal{N}(0, K_{j})$，其中 $K_{j} \\in \\mathbb{R}^{T \\times T}$。这 $k$ 个潜过程是先验独立的。\n\n假设观测数据已经过噪声白化，载荷在白化空间中已经正交归一化，因此 $R = I_{N}$ 且 $C^{\\top} C = I_{k}$。同时假设所有的核函数 $\\kappa_{j}$ 都是平稳的，并且其格拉姆矩阵 $K_{j}$ 是稠密且满秩的。你的目标是计算在精确 GPFA 推断下，以数据 $\\{y_{t}\\}_{t=1}^{T}$ 为条件的潜轨迹后验分布，以及在稀疏 GPFA 近似下的后验分布。该稀疏近似为每个潜维度使用 $M$ 个诱导点，其位置为 $\\{u_{m}\\}_{m=1}^{M}$，并采用变分诱导点构造，这导致每个潜过程的计算涉及 $K_{UU} \\in \\mathbb{R}^{M \\times M}$ 和 $K_{TU} \\in \\mathbb{R}^{T \\times M}$。\n\n仅从以下基本事实和定义出发：\n- 对于具有高斯先验的线性高斯模型，其后验分布是高斯的，可以通过求解线性系统或分解正定矩阵来获得。\n- 对于一个大小为 $n \\times n$ 的正定矩阵，Cholesky分解的算术运算成本约为 $n^{3}$ 的数量级。\n- 对于尺寸兼容可乘的稠密矩阵，矩阵-矩阵乘法的成本约为其维度乘积的数量级。\n- 对于 $k$ 个独立的高斯过程，其时间协方差矩阵为 $\\{K_{j}\\}_{j=1}^{k}$，先验精度矩阵在潜维度上是块对角的，其块为 $K_{j}^{-1}$。\n\n在所述假设下，推导计算潜轨迹的精确 GPFA 后验分布的渐近主阶算术成本，用 $k$ 和 $T$ 表示。然后，推导计算相应稀疏 GPFA 后验分布的渐近主阶算术成本，用 $k$、$T$ 和 $M$ 表示。该稀疏 GPFA 每个潜维度使用 $M$ 个诱导点，具有稠密的 $K_{UU}$ 和 $K_{TU}$，并且计算是使用标准的变分稀疏高斯过程条件化、稠密线性代数进行的，没有利用所述之外的额外结构。\n\n最后，将渐近加速因子 $S(T, M)$ 定义为精确成本与稀疏成本的比率，消去任何公共乘法因子，并忽略常数因子和低阶项。将 $S(T, M)$ 表示为仅含 $T$ 和 $M$ 的单个闭式解析表达式。在 $S(T, M)$ 的最终表达式中不要使用大O表示法，也不要包含单位。你的最终答案必须是单个解析表达式。",
            "solution": "问题陈述需经过验证。\n\n### 步骤1：提取已知条件\n- **模型**：高斯过程因子分析 (GPFA)。\n- **群体大小**：$N$。\n- **时间点**：$T$，均匀采样。\n- **潜状态**：在时间 $t$ 为 $x_{t} \\in \\mathbb{R}^{k}$。\n- **观测模型**：$y_{t} = C x_{t} + d + \\varepsilon_{t}$。\n- **观测噪声**：$\\varepsilon_{t} \\sim \\mathcal{N}(0, R)$，其中 $R \\in \\mathbb{R}^{N \\times N}$ 是对角矩阵。\n- **载荷矩阵**：$C \\in \\mathbb{R}^{N \\times k}$。\n- **潜过程**：$x^{(j)} = \\{x^{(j)}_{t}\\}_{t=1}^{T}$，对于 $j \\in \\{1, \\dots, k\\}$。\n- **潜先验**：每个 $x^{(j)}$ 是一个独立的零均值高斯过程 (GP)，$x^{(j)} \\sim \\mathcal{N}(0, K_{j})$，其中 $K_{j} \\in \\mathbb{R}^{T \\times T}$。这 $k$ 个过程是先验独立的。\n- **精确 GPFA 的假设**：$R = I_{N}$，$C^{\\top} C = I_{k}$，核函数 $\\kappa_{j}$ 是平稳的，格拉姆矩阵 $K_{j}$ 是稠密且满秩的。\n- **稀疏 GPFA 的假设**：每个潜维度有 $M$ 个诱导点，采用变分诱导点构造，计算涉及每个潜过程的稠密矩阵 $K_{UU} \\in \\mathbb{R}^{M \\times M}$ 和 $K_{TU} \\in \\mathbb{R}^{T \\times M}$。\n- **成本基元**：对一个 $n \\times n$ 的正定矩阵进行 Cholesky 分解的成本为 $\\mathcal{O}(n^3)$。矩阵-矩阵乘法的成本与其维度乘积同数量级。\n- **先验结构**：完整潜向量 $\\mathbf{x} \\in \\mathbb{R}^{kT}$ 的先验精度矩阵是块对角的，其块为 $K_{j}^{-1}$。\n- **任务**：\n  1. 推导计算精确 GPFA 后验分布的渐近主阶成本，用 $k$ 和 $T$ 表示。\n  2. 推导计算稀疏 GPFA 后验分布的渐近主阶成本，用 $k$、$T$ 和 $M$ 表示。\n  3. 将加速因子 $S(T, M)$ 定义为精确成本与稀疏成本的比率，并以仅含 $T$ 和 $M$ 的单个闭式表达式给出。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题具有科学依据，因为 GPFA 是计算神经科学中的一个标准模型，而变分稀疏高斯过程是一种成熟的近似方法。所提供的假设，如 $R=I_N$ 和 $C^{\\top}C=I_k$，是通过数据预处理（噪声白化和主成分分析）实现的常见简化条件。该问题是适定、客观且自洽的，提供了进行标准计算复杂度分析所需的所有必要信息。算法成本的基元（Cholesky分解、矩阵乘法）是标准的。该问题没有矛盾、模糊之处或伪科学论断。\n\n### 步骤3：结论与行动\n该问题是**有效的**。将提供完整解答。\n\n### 计算成本的推导\n\n目标是确定推断潜轨迹后验分布 $p(\\mathbf{x}|\\mathbf{y})$ 的主阶计算成本，其中 $\\mathbf{x} \\in \\mathbb{R}^{kT}$ 表示所有 $k$ 个维度和 $T$ 个时间点上的所有潜变量，$\\mathbf{y} \\in \\mathbb{R}^{NT}$ 表示所有观测值。由于 $\\mathbf{x}$ 的先验是高斯的，且似然是线性高斯的，因此后验也是高斯的。计算后验等价于计算其均值和协方差。这个过程主要由求解一个涉及后验精度矩阵的大型线性系统所主导。\n\n对数后验与对数先验和对数似然之和成正比：\n$$ \\ln p(\\mathbf{x}|\\mathbf{y}) \\propto \\ln p(\\mathbf{x}) + \\ln p(\\mathbf{y}|\\mathbf{x}) $$\n给定 $k$ 个独立的潜过程 $x^{(j)} \\sim \\mathcal{N}(0, K_j)$，其对数先验为：\n$$ \\ln p(\\mathbf{x}) = -\\frac{1}{2} \\sum_{j=1}^{k} (x^{(j)})^{\\top} K_{j}^{-1} x^{(j)} + \\text{const.} $$\n给定观测模型 $y_{t} = C x_{t} + d + \\varepsilon_{t}$ 且 $\\varepsilon_t \\sim \\mathcal{N}(0, I_N)$，其对数似然为：\n$$ \\ln p(\\mathbf{y}|\\mathbf{x}) = \\sum_{t=1}^{T} \\ln \\mathcal{N}(y_t | C x_t + d, I_N) = -\\frac{1}{2} \\sum_{t=1}^{T} \\|y_t - d - C x_t\\|^2_2 + \\text{const.} $$\n对数似然中关于 $x_t$ 的二次项是 $-\\frac{1}{2} \\sum_{t=1}^{T} x_t^{\\top} C^{\\top} C x_t$。使用给定的假设 $C^{\\top} C = I_k$，该项可简化为：\n$$ -\\frac{1}{2} \\sum_{t=1}^{T} x_t^{\\top} I_k x_t = -\\frac{1}{2} \\sum_{t=1}^{T} x_t^{\\top} x_t $$\n这个和可以根据轨迹 $x^{(j)}$ 重写为：\n$$ \\sum_{t=1}^{T} x_t^{\\top} x_t = \\sum_{t=1}^{T} \\sum_{j=1}^{k} (x_t^{(j)})^2 = \\sum_{j=1}^{k} \\sum_{t=1}^{T} (x_t^{(j)})^2 = \\sum_{j=1}^{k} (x^{(j)})^{\\top} x^{(j)} $$\n负对数后验关于 $\\mathbf{x}$ 的海森矩阵即为后验精度矩阵 $\\Sigma_{\\text{post}}^{-1}$。负对数后验的二次部分为：\n$$ \\frac{1}{2} \\sum_{j=1}^{k} (x^{(j)})^{\\top} K_{j}^{-1} x^{(j)} + \\frac{1}{2} \\sum_{j=1}^{k} (x^{(j)})^{\\top} I_T x^{(j)} = \\frac{1}{2} \\sum_{j=1}^{k} (x^{(j)})^{\\top} (K_{j}^{-1} + I_T) x^{(j)} $$\n这表明后验精度矩阵是块对角的，有 $k$ 个大小为 $T \\times T$ 的块。问题解耦为 $k$ 个独立的后验推断问题，每个潜维度 $j \\in \\{1, \\dots, k\\}$ 对应一个。每个 $x^{(j)}$ 的后验精度为 $K_{j}^{-1} + I_T$。\n\n**1. 精确 GPFA 的成本**\n\n对于每个潜维度 $j$，主要的计算任务是处理 $T \\times T$ 的后验精度矩阵。后验协方差为 $(K_{j}^{-1} + I_T)^{-1}$。使用 Woodbury 矩阵恒等式，这等价于 $K_j(K_j+I_T)^{-1}$。主要的计算步骤是对稠密的 $T \\times T$ 矩阵 $G_j = K_j + I_T$ 进行求逆或分解。\n根据问题陈述，我们对此使用 Cholesky 分解，对于单个 $T \\times T$ 矩阵，其成本为 $\\mathcal{O}(T^3)$。由于必须对 $k$ 个独立的潜维度中的每一个都执行此操作，总成本为：\n$$ C_{\\text{exact}} = k \\times \\mathcal{O}(T^3) = \\mathcal{O}(kT^3) $$\n因此，精确计算成本的主阶项与 $kT^3$ 成正比。\n\n**2. 稀疏 GPFA 的成本**\n\n在每个潜过程使用 $M$ 个诱导点的稀疏 GPFA 近似中，由于 $C^{\\top}C=I_k$ 的假设，计算在 $k$ 个潜维度上也是解耦的。我们分析单个潜维度的成本。变分稀疏 GP 方法通过引入 $M$ 个诱导变量来避免对 $T \\times T$ 矩阵的直接操作。计算的核心在于找到这 $M$ 个变量上的变分后验。\n\n成本主要由涉及诱导点的 $M \\times M$ 协方差矩阵 $K_{UU}$ 和 $T \\times M$ 互协方差矩阵 $K_{TU}$ 的运算决定。在标准的变分公式（例如 Titsias, 2009; Hensman et al., 2013）中，后验更新需要计算一个形式为 $B = K_{UU} + K_{UT} K_{TU}$ 的矩阵（假设单位观测噪声方差，这对应于我们精确情况下的 $I_T$ 项）。我们来分析构造和求逆或分解 $B$ 的成本。\n- 矩阵 $K_{UU}$ 是 $M \\times M$ 的。\n- 矩阵 $K_{TU}$ 是 $T \\times M$ 的，$K_{UT}$ 是 $M \\times T$ 的。\n- 乘积 $P = K_{UT} K_{TU}$ 是一个 $M \\times M$ 矩阵。这个矩阵-矩阵乘法的成本是 $\\mathcal{O}(M \\times T \\times M) = \\mathcal{O}(M^2 T)$。\n- 构造 $B = K_{UU} + P$ 是两个 $M \\times M$ 矩阵的加法，成本为 $\\mathcal{O}(M^2)$。\n- 对 $M \\times M$ 矩阵 $B$ 进行 Cholesky 分解的成本为 $\\mathcal{O}(M^3)$。\n\n单个潜维度的总成本由这些步骤中最昂贵的一步主导。因此，每个潜过程的成本为 $\\mathcal{O}(M^2 T + M^3)$。\n由于此计算是为 $k$ 个潜维度中的每一个独立执行的，稀疏近似的总成本为：\n$$ C_{\\text{sparse}} = k \\times \\mathcal{O}(M^2 T + M^3) = \\mathcal{O}(k M^2 T + k M^3) $$\n稀疏计算成本的主阶项与 $k(M^2 T + M^3)$ 成正比。\n\n**3. 渐近加速因子 $S(T, M)$**\n\n加速因子 $S(T, M)$ 定义为精确成本与稀疏成本的比率，忽略常数并消去公共因子。\n$$ S(T, M) = \\frac{\\text{精确成本的主阶项}}{\\text{稀疏成本的主阶项}} $$\n代入推导出的成本项：\n$$ S(T, M) = \\frac{k T^3}{k (M^2 T + M^3)} $$\n我们消去公共乘法因子 $k$：\n$$ S(T, M) = \\frac{T^3}{M^2 T + M^3} $$\n从分母中提出因子 $M^2$ 得到最终表达式：\n$$ S(T, M) = \\frac{T^3}{M^2(T + M)} $$\n这就是所要求的、用 $T$ 和 $M$ 表示的加速因子的单个闭式解析表达式。",
            "answer": "$$\\boxed{\\frac{T^3}{M^2(T + M)}}$$"
        },
        {
            "introduction": "将抽象的数学公式转化为可工作的代码，是检验理论理解的最终环节。本练习  提供了一个具体的编程任务：实现并验证计算后验分布的两种等价数学路径。这项实践不仅能让你精确地处理模型中的线性代数，还能通过单元测试确保实现的正确性，从而弥合理论与实践之间的鸿沟。",
            "id": "4166080",
            "problem": "考虑一个线性高斯观测模型下的高斯过程因子分析（GPFA）。设潜在轨迹被建模为独立的高斯过程，观测值是潜在状态与加性高斯噪声的线性混合。具体来说，对于时间点 $t_1,\\dots,t_T$，设时间 $t_i$ 的潜在状态为 $x(t_i) \\in \\mathbb{R}^k$，观测值为 $y(t_i) \\in \\mathbb{R}^p$，满足 $y(t_i) = C x(t_i) + d + \\varepsilon(t_i)$，其中 $C \\in \\mathbb{R}^{p \\times k}$ 是一个载荷矩阵，$d \\in \\mathbb{R}^p$ 是一个偏移向量，并且 $\\varepsilon(t_i) \\sim \\mathcal{N}(0, \\sigma_n^2 I_p)$，其中 $\\sigma_n^2 > 0$。\n\n从高斯过程的定义、堆叠的潜在变量和观测值的联合多元正态分布的构建，以及联合高斯向量的条件化规则出发，推导出计算 $p(x \\mid y)$ 的两种数学上等价的后验条件化路径：一种是使用精度算子在潜在空间中进行条件化，另一种是在条件化之前边缘化到观测空间。然后，设计单元测试，通过在几个科学上合理的参数设置下，比较每种路径计算出的后验均值和协方差，来数值验证这两种路径的等价性。\n\n您必须实现一个完整、可运行的程序，该程序能够：\n- 对每个潜在维度，使用平方指数（径向基函数）核函数 $k(t, s) = \\sigma_f^2 \\exp\\!\\big(-\\tfrac{1}{2} (t - s)^2 / \\ell^2\\big)$ 构建堆叠潜在变量的高斯过程先验协方差，每个潜在维度可以有不同的超参数 $(\\sigma_f^2, \\ell)$。\n- 构建线性观测算子，该算子将所有时间点的堆叠潜在向量映射到堆叠观测向量。\n- 从指定的模型中模拟潜在轨迹和观测值，以确保测试的内部一致性。\n- 通过严格遵循高斯条件化所蕴含的两种路径计算后验均值和协方差。\n- 比较两种后验结果，并为每个测试返回一个布尔值，指示后验均值和协方差是否都在指定的绝对容差范围内一致。\n\n使用以下测试套件，其中每个测试指定 $(k, p, T)$、时间点、每个潜在维度的核超参数 $(\\sigma_f^2, \\ell)$、载荷矩阵 $C$、观测噪声方差 $\\sigma_n^2$ 以及用于模拟可复现性的随机种子。所有偏移量 $d$ 都设置为 $0$。\n\n测试 1（一般情况，良态条件）：\n- $k = 2$, $p = 3$, $T = 5$, $t = [0, 1, 2, 3, 4]$.\n- 潜在维度 1：$\\sigma_f^2 = 1.0$，$\\ell = 1.2$；潜在维度 2：$\\sigma_f^2 = 0.5$，$\\ell = 0.7$。\n- $C = \\begin{bmatrix} 1.0  -0.5 \\\\ 0.2  0.8 \\\\ -0.3  0.1 \\end{bmatrix}$.\n- $\\sigma_n^2 = 0.1$，种子 $= 123$。\n\n测试 2（接近无噪声边界）：\n- $k = 3$, $p = 2$, $T = 4$, $t = [0.0, 0.5, 1.5, 2.2]$.\n- 潜在维度 1：$\\sigma_f^2 = 1.0$，$\\ell = 0.9$；潜在维度 2：$\\sigma_f^2 = 1.0$，$\\ell = 1.1$；潜在维度 3：$\\sigma_f^2 = 1.0$，$\\ell = 0.5$。\n- $C$ 由随机种子固定，从标准正态分布中抽取，然后乘以 $0.5$ 以改善条件数，种子为 $7$。\n- $\\sigma_n^2 = 10^{-6}$，种子为 $7$ 用于数据生成。\n\n测试 3（高噪声边界）：\n- $k = 1$, $p = 5$, $T = 6$, $t = [0, 1, 2, 3, 4, 5]$.\n- 潜在维度 1：$\\sigma_f^2 = 2.0$，$\\ell = 1.0$。\n- $C$ 由随机种子固定，从标准正态分布中抽取，种子为 $42$。\n- $\\sigma_n^2 = 100.0$，种子为 $42$ 用于数据生成。\n\n测试 4（标量潜在变量和观测值）：\n- $k = 1$, $p = 1$, $T = 3$, $t = [0.0, 0.3, 0.9]$.\n- 潜在维度 1：$\\sigma_f^2 = 1.5$, $\\ell = 0.4$。\n- $C = [ [1.0] ]$。\n- $\\sigma_n^2 = 0.05$，种子 $= 202$。\n\n测试 5（多潜在维度、不规则时间、良态条件的方阵载荷矩阵）：\n- $k = 4$, $p = 4$, $T = 3$, $t = [0.0, 0.2, 0.7]$.\n- 潜在维度 1：$\\sigma_f^2 = 0.7$，$\\ell = 0.3$；潜在维度 2：$\\sigma_f^2 = 1.1$，$\\ell = 0.6$；潜在维度 3：$\\sigma_f^2 = 0.9$，$\\ell = 0.9$；潜在维度 4：$\\sigma_f^2 = 0.5$，$\\ell = 0.4$。\n- $C$ 的构造方法是：取一个从标准正态分布（种子 $= 99$）中抽取的随机 $4 \\times 4$ 矩阵，计算 $\\mathrm{QR}$ 分解以获得一个标准正交矩阵 $Q$，然后设置 $C = Q \\operatorname{diag}(1.0, 0.5, 0.7, 1.3)$。\n- $\\sigma_n^2 = 0.3$，种子为 $99$ 用于数据生成。\n\n一致性容差：\n- 均值和协方差比较的绝对容差均为 $\\tau = 5 \\times 10^{-8}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，例如 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是一个布尔值，指示测试 $i$ 是否通过（后验均值和协方差都在容差范围内一致）。不涉及物理单位，所有参数都是无量纲的。",
            "solution": "问题陈述是有效的。它描述了一个标准的高斯过程因子分析（GPFA）模型，并要求推导和数值验证潜在变量后验分布的两个等价公式。这是一个基于贝叶斯推断和线性代数原理的良构问题。\n\n### 后验条件化路径的数学推导\n\n**1. 模型设定**\n\n设 $j \\in \\{1, \\dots, k\\}$ 的潜在轨迹是独立的高斯过程（GPs），表示为 $x_j(t) \\sim \\mathcal{GP}(0, k_j(t, s))$。其先验均值为零，协方差由平方指数核函数给出：\n$$ k_j(t, s; \\sigma_{f,j}^2, \\ell_j) = \\sigma_{f,j}^2 \\exp\\left(-\\frac{1}{2} \\frac{(t-s)^2}{\\ell_j^2}\\right) $$\n在一组 $T$ 个离散时间点 $\\{t_1, \\dots, t_T\\}$ 上，潜在状态是一个向量 $x(t_i) = [x_1(t_i), \\dots, x_k(t_i)]^T \\in \\mathbb{R}^k$。相应的观测值 $y(t_i) \\in \\mathbb{R}^p$ 由线性高斯模型生成：\n$$ y(t_i) = C x(t_i) + d + \\varepsilon(t_i) $$\n其中 $C \\in \\mathbb{R}^{p \\times k}$ 是载荷矩阵，$d \\in \\mathbb{R}^p$ 是一个偏移向量（给定为 $d=0$），$\\varepsilon(t_i) \\sim \\mathcal{N}(0, \\sigma_n^2 I_p)$ 是独立同分布的高斯噪声。\n\n**2. 构建联合分布**\n\n为了进行推断，我们将所有 $T$ 个时间点的所有变量堆叠起来：\n-   堆叠潜在向量：$X = [x(t_1)^T, \\dots, x(t_T)^T]^T \\in \\mathbb{R}^{kT}$。\n-   堆叠观测向量：$Y = [y(t_1)^T, \\dots, y(t_T)^T]^T \\in \\mathbb{R}^{pT}$。\n\n堆叠潜在向量 $X$ 的先验分布是一个多元高斯分布，$p(X) = \\mathcal{N}(X \\mid 0, K_x)$，因为高斯过程的定义是任何有限样本集合都服从多元高斯分布。均值是大小为 $kT \\times 1$ 的零向量。先验协方差矩阵 $K_x \\in \\mathbb{R}^{kT \\times kT}$ 是一个由 $T \\times T$ 个块组成的块矩阵，其中每个块的大小为 $k \\times k$。第 $(i, j)$ 个块由 $\\operatorname{Cov}(x(t_i), x(t_j)^T)$ 给出。由于指定的 $k$ 个潜在过程是独立的，这个块是一个对角矩阵：\n$$ [K_x]_{i,j} = \\operatorname{Cov}(x(t_i), x(t_j)^T) = \\operatorname{diag}\\left(k_1(t_i, t_j), \\dots, k_k(t_i, t_j)\\right) $$\n\n堆叠向量的观测模型可以写为 $p(Y \\mid X) = \\mathcal{N}(Y \\mid C_{full}X, R_y)$，其中：\n-   $C_{full} = \\operatorname{blkdiag}(C, \\dots, C)$ 是一个 $(pT) \\times (kT)$ 的块对角矩阵，其主对角线上有 $T$ 个 $C$ 的实例。\n-   $R_y = \\sigma_n^2 I_{pT}$ 是 $(pT) \\times (pT)$ 的观测噪声协方差矩阵，其中 $I_{pT}$ 是大小为 $pT$ 的单位矩阵。\n\n由于先验 $p(X)$ 和似然 $p(Y \\mid X)$ 都是高斯分布，联合分布 $p(X, Y) = p(Y \\mid X) p(X)$ 也是高斯分布。给定 $X \\sim \\mathcal{N}(0, K_x)$ 和 $Y = C_{full}X + \\varepsilon_{full}$，其中 $\\varepsilon_{full} \\sim \\mathcal{N}(0, R_y)$，联合分布的参数为：\n$$\n\\begin{pmatrix} X \\\\ Y \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} K_x & K_x C_{full}^T \\\\ C_{full} K_x & C_{full} K_x C_{full}^T + R_y \\end{pmatrix} \\right)\n$$\n\n**3. 后验 $p(X \\mid Y)$ 的推导**\n\n后验分布 $p(X \\mid Y)$ 通过对联合高斯分布进行条件化得到。我们通过两条等价的路径推导其均值 $\\mu_{post}$ 和协方差 $\\Sigma_{post}$。\n\n**路径 1：通过精度算子在潜在空间中进行条件化**\n\n这种方法对概率密度使用贝叶斯法则，重点关注指数中的二次型（与精度或信息矩阵相关）。后验对数密度是先验对数和似然对数的和，相差一个归一化常数：\n$$ \\ln p(X \\mid Y) = \\ln p(Y \\mid X) + \\ln p(X) + \\text{const.} $$\n忽略常数项，对数似然为：\n$$ \\ln p(Y \\mid X) \\propto -\\frac{1}{2}(Y - C_{full}X)^T R_y^{-1} (Y-C_{full}X) = -\\frac{1}{2}X^T C_{full}^T R_y^{-1} C_{full}X + X^T C_{full}^T R_y^{-1} Y - \\frac{1}{2}Y^T R_y^{-1} Y $$\n对数先验为：\n$$ \\ln p(X) \\propto -\\frac{1}{2} X^T K_x^{-1} X $$\n合并依赖于 $X$ 的项：\n$$ \\ln p(X \\mid Y) \\propto -\\frac{1}{2} X^T (K_x^{-1} + C_{full}^T R_y^{-1} C_{full}) X + X^T (C_{full}^T R_y^{-1} Y) $$\n这个表达式是关于 $X$ 的二次型，这是高斯对数密度的一个特征。通过配方法，我们将二次项的系数确定为后验精度矩阵 $\\Lambda_{post}$（协方差矩阵的逆），并从线性项中确定后验均值 $\\mu_{post}$。\n后验精度矩阵是先验精度与似然贡献的精度的和：\n$$ \\Lambda_{post,1} = K_x^{-1} + C_{full}^T R_y^{-1} C_{full} $$\n后验协方差矩阵是后验精度的逆：\n$$ \\Sigma_{post,1} = (K_x^{-1} + C_{full}^T R_y^{-1} C_{full})^{-1} $$\n后验均值由 $\\mu_{post} = \\Sigma_{post} \\times (\\text{线性项的系数})$ 给出：\n$$ \\mu_{post,1} = \\Sigma_{post,1} (C_{full}^T R_y^{-1} Y) $$\n代入 $R_y = \\sigma_n^2 I_{pT}$，方程变为：\n$$ \\Sigma_{post,1} = \\left(K_x^{-1} + \\frac{1}{\\sigma_n^2} C_{full}^T C_{full}\\right)^{-1} $$\n$$ \\mu_{post,1} = \\Sigma_{post,1} \\left(\\frac{1}{\\sigma_n^2} C_{full}^T Y\\right) $$\n这条路径需要对两个大小为 $(kT) \\times (kT)$ 的矩阵求逆：$K_x$ 和 $\\Lambda_{post,1}$。\n\n**路径 2：边缘化到观测空间**\n\n这种方法使用对分块高斯随机向量进行条件化的标准公式，这些公式是为 $(X,Y)$ 的联合分布推导的。设分块协方差矩阵为：\n$$ \\Sigma = \\begin{pmatrix} \\Sigma_{XX} & \\Sigma_{XY} \\\\ \\Sigma_{YX} & \\Sigma_{YY} \\end{pmatrix} = \\begin{pmatrix} K_x & K_x C_{full}^T \\\\ C_{full} K_x & C_{full} K_x C_{full}^T + R_y \\end{pmatrix} $$\n条件分布 $p(X \\mid Y=y)$ 是高斯分布，其均值和协方差为：\n$$ \\mu_{X \\mid Y=y} = \\mu_X + \\Sigma_{XY} \\Sigma_{YY}^{-1} (y - \\mu_Y) $$\n$$ \\Sigma_{X \\mid Y=y} = \\Sigma_{XX} - \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YX} $$\n由于先验均值 $\\mu_X = 0$ 和 $\\mu_Y = 0$，我们代入相应的块：\n$$ \\mu_{post,2} = (K_x C_{full}^T) (C_{full} K_x C_{full}^T + R_y)^{-1} Y $$\n$$ \\Sigma_{post,2} = K_x - (K_x C_{full}^T) (C_{full} K_x C_{full}^T + R_y)^{-1} (C_{full} K_x) $$\n代入 $R_y = \\sigma_n^2 I_{pT}$：\n$$ \\mu_{post,2} = K_x C_{full}^T (C_{full} K_x C_{full}^T + \\sigma_n^2 I_{pT})^{-1} Y $$\n$$ \\Sigma_{post,2} = K_x - K_x C_{full}^T (C_{full} K_x C_{full}^T + \\sigma_n^2 I_{pT})^{-1} C_{full} K_x $$\n这条路径需要对一个大小为 $(pT) \\times (pT)$ 的矩阵求逆，该矩阵对应于边际观测协方差 $\\Sigma_{YY}$。\n\n路径 1 和路径 2 之间的数学等价性是 Woodbury 矩阵恒等式的直接结果，该恒等式提供了矩阵和的逆的公式。问题中指定的数值单元测试旨在在实际实现中确认这种等价性。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import block_diag\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define test cases as per the problem description.\n    test_cases = [\n        {\n            \"name\": \"Test 1\",\n            \"k\": 2, \"p\": 3, \"T\": 5, \"times\": np.array([0., 1., 2., 3., 4.]),\n            \"kernel_params\": [(1.0, 1.2), (0.5, 0.7)],\n            \"C\": np.array([[1.0, -0.5], [0.2, 0.8], [-0.3, 0.1]]),\n            \"sigma_n2\": 0.1,\n            \"seed\": 123\n        },\n        {\n            \"name\": \"Test 2\",\n            \"k\": 3, \"p\": 2, \"T\": 4, \"times\": np.array([0.0, 0.5, 1.5, 2.2]),\n            \"kernel_params\": [(1.0, 0.9), (1.0, 1.1), (1.0, 0.5)],\n            \"C\": \"random\", \"C_seed\": 7,\n            \"sigma_n2\": 1e-6,\n            \"seed\": 7\n        },\n        {\n            \"name\": \"Test 3\",\n            \"k\": 1, \"p\": 5, \"T\": 6, \"times\": np.array([0., 1., 2., 3., 4., 5.]),\n            \"kernel_params\": [(2.0, 1.0)],\n            \"C\": \"random\", \"C_seed\": 42,\n            \"sigma_n2\": 100.0,\n            \"seed\": 42\n        },\n        {\n            \"name\": \"Test 4\",\n            \"k\": 1, \"p\": 1, \"T\": 3, \"times\": np.array([0.0, 0.3, 0.9]),\n            \"kernel_params\": [(1.5, 0.4)],\n            \"C\": np.array([[1.0]]),\n            \"sigma_n2\": 0.05,\n            \"seed\": 202\n        },\n        {\n            \"name\": \"Test 5\",\n            \"k\": 4, \"p\": 4, \"T\": 3, \"times\": np.array([0.0, 0.2, 0.7]),\n            \"kernel_params\": [(0.7, 0.3), (1.1, 0.6), (0.9, 0.9), (0.5, 0.4)],\n            \"C\": \"qr_random\", \"C_seed\": 99,\n            \"sigma_n2\": 0.3,\n            \"seed\": 99\n        }\n    ]\n\n    tolerance = 5e-8\n    results = []\n\n    for case in test_cases:\n        test_result = run_test(case, tolerance)\n        results.append(test_result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef run_test(case_params, tolerance):\n    \"\"\"\n    Executes a single GPFA test case.\n    \"\"\"\n    k, p, T = case_params[\"k\"], case_params[\"p\"], case_params[\"T\"]\n    times = case_params[\"times\"]\n    kernel_params = case_params[\"kernel_params\"]\n    sigma_n2 = case_params[\"sigma_n2\"]\n    seed = case_params[\"seed\"]\n\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct or generate loading matrix C\n    if isinstance(case_params[\"C\"], str):\n        c_mode = case_params[\"C\"]\n        # The problem statement implies a single reproducible stream for each test.\n        if c_mode == \"random\":\n            if k == 3 and p == 2: # Test 2\n                C = rng.standard_normal(size=(p, k)) * 0.5\n            else: # Test 3\n                C = rng.standard_normal(size=(p, k))\n        elif c_mode == \"qr_random\":\n            A = rng.standard_normal(size=(p, k))\n            Q, _ = np.linalg.qr(A)\n            D_vals = np.array([1.0, 0.5, 0.7, 1.3])\n            C = Q @ np.diag(D_vals)\n    else:\n        C = case_params[\"C\"]\n\n    # 2. Build prior covariance Kx and full loading matrix C_full\n    Kx = build_Kx(k, T, times, kernel_params)\n    C_full = block_diag(*([C] * T))\n\n    # 3. Simulate latent and observed data\n    X_true = rng.multivariate_normal(np.zeros(k * T), Kx)\n    noise = rng.normal(0, np.sqrt(sigma_n2), size=p * T)\n    Y_observed = C_full @ X_true + noise\n\n    # 4. Compute posterior via Route 1 (Latent Space / Precision)\n    Kx_inv = np.linalg.inv(Kx)\n    Lambda_post1 = Kx_inv + (1 / sigma_n2) * C_full.T @ C_full\n    Sigma_post1 = np.linalg.inv(Lambda_post1)\n    mu_post1 = Sigma_post1 @ ((1 / sigma_n2) * C_full.T @ Y_observed)\n\n    # 5. Compute posterior via Route 2 (Observation Space / Covariance)\n    Ky = C_full @ Kx @ C_full.T + sigma_n2 * np.eye(p * T)\n    Ky_inv = np.linalg.inv(Ky)\n    \n    # Pre-calculate to avoid recomputing Kx @ C_full.T\n    Kx_CT = Kx @ C_full.T\n\n    mu_post2 = Kx_CT @ Ky_inv @ Y_observed\n    Sigma_post2 = Kx - Kx_CT @ Ky_inv @ C_full @ Kx\n\n    # 6. Compare results\n    mean_agree = np.allclose(mu_post1, mu_post2, atol=tolerance, rtol=0)\n    cov_agree = np.allclose(Sigma_post1, Sigma_post2, atol=tolerance, rtol=0)\n\n    return mean_agree and cov_agree\n\n\ndef squared_exponential_kernel(t1, t2, var_f, length_scale):\n    \"\"\"\n    Squared exponential (RBF) kernel function.\n    \"\"\"\n    return var_f * np.exp(-0.5 * ((t1 - t2) / length_scale)**2)\n\ndef build_Kx(k, T, times, kernel_params):\n    \"\"\"\n    Builds the prior covariance matrix Kx for the stacked latent vector X.\n    X is stacked time-first: X = [x(t1)', ..., x(tT)']'.\n    \"\"\"\n    Kx = np.zeros((k * T, k * T))\n    for i in range(T):\n        for j in range(T):\n            ti, tj = times[i], times[j]\n            # Create the (i,j)-th block, which is k x k and diagonal\n            cov_diag_block = np.zeros(k)\n            for latent_idx in range(k):\n                var_f, length_scale = kernel_params[latent_idx]\n                cov_diag_block[latent_idx] = squared_exponential_kernel(ti, tj, var_f, length_scale)\n            \n            Kx[i*k:(i+1)*k, j*k:(j+1)*k] = np.diag(cov_diag_block)\n    return Kx\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}