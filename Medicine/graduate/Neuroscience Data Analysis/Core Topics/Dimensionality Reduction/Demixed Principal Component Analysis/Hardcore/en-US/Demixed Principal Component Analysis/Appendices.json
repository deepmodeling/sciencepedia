{
    "hands_on_practices": [
        {
            "introduction": "At the heart of Demixed Principal Component Analysis (dPCA) is the principle of separating variance from different task parameters. This exercise provides a concrete, calculation-based look at the core mechanism behind this separation. By comparing covariance matrices computed after different data centering schemes, you will see firsthand how subtracting condition-specific averages isolates the variance attributable to other parameters, forming the foundation of the \"demixing\" process .",
            "id": "4154819",
            "problem": "You are analyzing a simple neuronal population dataset to illustrate how centering choices influence demixing in demixed principal component analysis (dPCA). Consider a population of $2$ neurons recorded under $2$ stimuli ($s \\in \\{1,2\\}$) and $2$ time points ($t \\in \\{1,2\\}$). The trial-averaged population activity vectors (in spike-rate units, but units are not required in the final answer) for each condition are:\n- $x_{:,1,1} = \\begin{pmatrix}8\\\\3\\end{pmatrix}$,\n- $x_{:,1,2} = \\begin{pmatrix}8\\\\7\\end{pmatrix}$,\n- $x_{:,2,1} = \\begin{pmatrix}2\\\\3\\end{pmatrix}$,\n- $x_{:,2,2} = \\begin{pmatrix}2\\\\7\\end{pmatrix}$.\n\nAssume that demixed principal component analysis (dPCA) seeks to separate stimulus-related and time-related components by appropriate centering schemes before computing covariance matrices across neurons. You will compare two centering schemes:\n- Global mean subtraction: subtract the grand mean across all $(s,t)$ conditions from each observation, then compute the covariance across neurons using the population normalization by the number of observations.\n- Per-condition mean subtraction (within-stimulus centering): for each stimulus $s$, subtract the mean across time $t$ for that stimulus from the corresponding observations, then compute the covariance across neurons using the same population normalization.\n\nStarting from foundational definitions of centering and covariance for multivariate data, compute the two covariance matrices implied by these centering schemes, identify the largest eigenvalue (i.e., the first principal component variance) for each, and report the ratio of the largest eigenvalue under global mean subtraction to the largest eigenvalue under per-condition mean subtraction. Provide your final answer as an exact value. No rounding is required. Do not include units in your final answer.",
            "solution": "We begin by formalizing the data and the two centering schemes. Let $N=2$ denote the number of neurons and let $M=4$ denote the number of observed condition-time combinations. The observations are $x_{m} \\in \\mathbb{R}^{2}$ for $m \\in \\{1,2,3,4\\}$ corresponding to $(s,t) \\in \\{(1,1),(1,2),(2,1),(2,2)\\}$, respectively:\n$$\nx_{1}=\\begin{pmatrix}8\\\\3\\end{pmatrix},\\quad\nx_{2}=\\begin{pmatrix}8\\\\7\\end{pmatrix},\\quad\nx_{3}=\\begin{pmatrix}2\\\\3\\end{pmatrix},\\quad\nx_{4}=\\begin{pmatrix}2\\\\7\\end{pmatrix}.\n$$\nBy definition, the population covariance across neurons for centered data $\\tilde{x}_{m}$ is\n$$\nC \\;=\\; \\frac{1}{M}\\sum_{m=1}^{M} \\tilde{x}_{m}\\,\\tilde{x}_{m}^{\\top}.\n$$\nWe will compute this under two different centering schemes.\n\nGlobal mean subtraction: Let the grand mean across all conditions be\n$$\n\\bar{x} \\;=\\; \\frac{1}{M}\\sum_{m=1}^{M} x_{m}.\n$$\nCompute $\\bar{x}$ componentwise. For neuron $1$,\n$$\n\\bar{x}_{1} \\;=\\; \\frac{8+8+2+2}{4} \\;=\\; \\frac{20}{4} \\;=\\; 5.\n$$\nFor neuron $2$,\n$$\n\\bar{x}_{2} \\;=\\; \\frac{3+7+3+7}{4} \\;=\\; \\frac{20}{4} \\;=\\; 5.\n$$\nHence $\\bar{x}=\\begin{pmatrix}5\\\\5\\end{pmatrix}$. The centered observations are\n$$\n\\tilde{x}_{1}=\\begin{pmatrix}3\\\\-2\\end{pmatrix},\\quad\n\\tilde{x}_{2}=\\begin{pmatrix}3\\\\2\\end{pmatrix},\\quad\n\\tilde{x}_{3}=\\begin{pmatrix}-3\\\\-2\\end{pmatrix},\\quad\n\\tilde{x}_{4}=\\begin{pmatrix}-3\\\\2\\end{pmatrix}.\n$$\nCompute the covariance:\n$$\nC_{\\text{global}} \\;=\\; \\frac{1}{4}\\sum_{m=1}^{4}\\tilde{x}_{m}\\tilde{x}_{m}^{\\top}.\n$$\nNote that for each $\\tilde{x}_{m}=\\begin{pmatrix}\\tilde{x}_{m,1}\\\\\\tilde{x}_{m,2}\\end{pmatrix}$, $\\tilde{x}_{m,1}\\in\\{3,-3\\}$ and $\\tilde{x}_{m,2}\\in\\{-2,2\\}$ occur equally often with independent signs across the $4$ observations, and the cross-term averages to zero. Therefore,\n$$\n\\frac{1}{4}\\sum_{m=1}^{4}\\tilde{x}_{m,1}^{2} \\;=\\; 9,\\qquad\n\\frac{1}{4}\\sum_{m=1}^{4}\\tilde{x}_{m,2}^{2} \\;=\\; 4,\\qquad\n\\frac{1}{4}\\sum_{m=1}^{4}\\tilde{x}_{m,1}\\tilde{x}_{m,2} \\;=\\; 0.\n$$\nThus\n$$\nC_{\\text{global}} \\;=\\; \\begin{pmatrix}9  0 \\\\ 0  4\\end{pmatrix}.\n$$\nThe eigenvalues of a diagonal matrix are the diagonal entries, so the largest eigenvalue of $C_{\\text{global}}$ is\n$$\n\\lambda_{\\max}(C_{\\text{global}}) \\;=\\; 9.\n$$\n\nPer-condition mean subtraction (within-stimulus centering): For each stimulus $s\\in\\{1,2\\}$, compute the mean across the two time points $t\\in\\{1,2\\}$ and subtract it from the corresponding observations with that $s$. For $s=1$,\n$$\n\\bar{x}^{(s=1)} \\;=\\; \\frac{1}{2}\\left(\\begin{pmatrix}8\\\\3\\end{pmatrix}+\\begin{pmatrix}8\\\\7\\end{pmatrix}\\right) \\;=\\; \\begin{pmatrix}8\\\\5\\end{pmatrix}.\n$$\nFor $s=2$,\n$$\n\\bar{x}^{(s=2)} \\;=\\; \\frac{1}{2}\\left(\\begin{pmatrix}2\\\\3\\end{pmatrix}+\\begin{pmatrix}2\\\\7\\end{pmatrix}\\right) \\;=\\; \\begin{pmatrix}2\\\\5\\end{pmatrix}.\n$$\nThe within-stimulus centered observations become\n$$\n\\tilde{x}_{1}^{(\\text{pc})}=\\begin{pmatrix}0\\\\-2\\end{pmatrix},\\quad\n\\tilde{x}_{2}^{(\\text{pc})}=\\begin{pmatrix}0\\\\2\\end{pmatrix},\\quad\n\\tilde{x}_{3}^{(\\text{pc})}=\\begin{pmatrix}0\\\\-2\\end{pmatrix},\\quad\n\\tilde{x}_{4}^{(\\text{pc})}=\\begin{pmatrix}0\\\\2\\end{pmatrix}.\n$$\nCompute the covariance with population normalization:\n$$\nC_{\\text{pc}} \\;=\\; \\frac{1}{4}\\sum_{m=1}^{4}\\tilde{x}_{m}^{(\\text{pc})}\\left(\\tilde{x}_{m}^{(\\text{pc})}\\right)^{\\top}.\n$$\nClearly, the first coordinate is identically $0$ after within-stimulus centering, and the second coordinate takes values $\\pm 2$ equally often. Therefore,\n$$\nC_{\\text{pc}} \\;=\\; \\begin{pmatrix}0  0 \\\\ 0  4\\end{pmatrix},\n$$\nwhose largest eigenvalue is\n$$\n\\lambda_{\\max}(C_{\\text{pc}}) \\;=\\; 4.\n$$\n\nImpact on demixing: Under global mean subtraction, both stimulus offsets (along neuron $1$) and temporal modulations (along neuron $2$) contribute to variance, yielding $C_{\\text{global}}=\\begin{pmatrix}90\\\\04\\end{pmatrix}$ and a dominant component aligned with neuron $1$. Under per-condition centering, the stimulus mean offsets are removed within each stimulus, isolating time-related variance so that $C_{\\text{pc}}=\\begin{pmatrix}00\\\\04\\end{pmatrix}$, concentrating variance along neuron $2$. This illustrates how within-condition centering facilitates demixing by eliminating stimulus main effects from the covariance.\n\nThe requested ratio of the largest eigenvalue under global mean subtraction to that under per-condition mean subtraction is\n$$\n\\frac{\\lambda_{\\max}(C_{\\text{global}})}{\\lambda_{\\max}(C_{\\text{pc}})} \\;=\\; \\frac{9}{4}.\n$$\nNo rounding is required.",
            "answer": "$$\\boxed{\\frac{9}{4}}$$"
        },
        {
            "introduction": "After isolating the variance for a specific task parameter and finding its principal components, it is crucial to assess how well these components capture the signal. This practice guides you in deriving a formal metric for \"reconstructivity,\" analogous to the coefficient of determination ($R^2$) in regression. This skill is essential for evaluating the performance of your dPCA model and answering the key question: how much of a particular parameter's variance is explained by its corresponding demixed components? ",
            "id": "4154825",
            "problem": "Consider neural population activity arranged as a matrix for a single marginalization in demixed principal component analysis (dPCA), where demixed principal component analysis (dPCA) linearly reconstructs condition-specific low-dimensional components to approximate the marginalized signal. Let $X^{(m)} \\in \\mathbb{R}^{n \\times t}$ denote the marginalized data matrix for a task factor $m$, and let $\\hat{X}^{(m)} \\in \\mathbb{R}^{n \\times t}$ denote its reconstruction obtained from a fitted dPCA decoder-encoder pair. Use only the following foundational elements:\n- The Frobenius norm of a matrix $A$ defined by $\\|A\\|_{F}^{2} = \\sum_{i,j} A_{ij}^{2}$.\n- Reconstruction error defined by the difference matrix $E^{(m)} = X^{(m)} - \\hat{X}^{(m)}$.\n- The requirement that a reconstructivity metric should be dimensionless, invariant under uniform scaling of $X^{(m)}$ and $\\hat{X}^{(m)}$, achieve $1$ under perfect reconstruction, be $0$ when $\\hat{X}^{(m)} = 0$, and penalize larger squared errors.\n\n(a) Starting from these principles, construct a mathematically precise expression for a reconstructivity metric $R^{(m)}$ based on the squared reconstruction error and the total signal power of $X^{(m)}$. Justify that your expression satisfies the stated properties and interpret its range and limiting behaviors.\n\n(b) Apply your metric to the following scientifically plausible marginalized dataset (rows index neurons, columns index time bins):\n$$\nX^{(m)} = \\begin{pmatrix}\n2  -1 \\\\\n0  3 \\\\\n1  4\n\\end{pmatrix},\n\\qquad\n\\hat{X}^{(m)} = \\begin{pmatrix}\n1.8  -0.9 \\\\\n0.1  2.7 \\\\\n0.9  3.8\n\\end{pmatrix}.\n$$\nCompute $R^{(m)}$ for this dataset. Express your final numerical answer rounded to four significant figures. No units are required.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of statistical model evaluation, well-posed with all necessary information provided, and objective in its formulation. It requests the derivation of a standard performance metric and its application to a concrete example, which is a standard task in quantitative fields.\n\n**(a) Construction and Justification of the Reconstructivity Metric**\n\nThe objective is to construct a reconstructivity metric, which we denote as $R^{(m)}$, for a marginalized data matrix $X^{(m)} \\in \\mathbb{R}^{n \\times t}$ and its reconstruction $\\hat{X}^{(m)} \\in \\mathbb{R}^{n \\times t}$. The metric must be based on the squared reconstruction error and the total signal power of $X^{(m)}$, and satisfy five specific properties.\n\nFollowing the problem definition, the total signal power of the original data $X^{(m)}$ can be quantified by the squared Frobenius norm, $\\|X^{(m)}\\|_{F}^{2} = \\sum_{i,j} (X^{(m)}_{ij})^{2}$. The reconstruction error is given by the matrix $E^{(m)} = X^{(m)} - \\hat{X}^{(m)}$, and the total squared reconstruction error is $\\|E^{(m)}\\|_{F}^{2} = \\|X^{(m)} - \\hat{X}^{(m)}\\|_{F}^{2}$.\n\nA metric that compares the unexplained variance (error) to the total variance (signal power) is a natural choice, analogous to the coefficient of determination ($R^2$) in linear regression. We propose the following expression for the reconstructivity metric:\n$$\nR^{(m)} = 1 - \\frac{\\|X^{(m)} - \\hat{X}^{(m)}\\|_{F}^{2}}{\\|X^{(m)}\\|_{F}^{2}}\n$$\nWe now verify that this expression satisfies the five required properties.\n\n$1$. **Dimensionless**: The Frobenius norm squared of a matrix has units equal to the square of the units of its elements. The numerator, $\\|X^{(m)} - \\hat{X}^{(m)}\\|_{F}^{2}$, and the denominator, $\\|X^{(m)}\\|_{F}^{2}$, therefore have the same units. Their ratio is a dimensionless quantity. Since $1$ is also dimensionless, $R^{(m)}$ is dimensionless.\n\n$2$. **Invariant under uniform scaling**: Let us scale both matrices by a non-zero scalar $c$. Let the new matrices be $X'^{(m)} = cX^{(m)}$ and $\\hat{X}'^{(m)} = c\\hat{X}^{(m)}$. The new metric $R'^{(m)}$ is:\n$$\nR'^{(m)} = 1 - \\frac{\\|cX^{(m)} - c\\hat{X}^{(m)}\\|_{F}^{2}}{\\|cX^{(m)}\\|_{F}^{2}}\n$$\nUsing the property of the Frobenius norm, $\\|kA\\|_{F}^{2} = k^2 \\|A\\|_{F}^{2}$, we have:\n$$\nR'^{(m)} = 1 - \\frac{c^2 \\|X^{(m)} - \\hat{X}^{(m)}\\|_{F}^{2}}{c^2 \\|X^{(m)}\\|_{F}^{2}} = 1 - \\frac{\\|X^{(m)} - \\hat{X}^{(m)}\\|_{F}^{2}}{\\|X^{(m)}\\|_{F}^{2}} = R^{(m)}\n$$\nThe metric is invariant under uniform scaling for any $c \\neq 0$.\n\n$3$. **Value of $1$ under perfect reconstruction**: Perfect reconstruction implies $\\hat{X}^{(m)} = X^{(m)}$. The error matrix is $E^{(m)} = X^{(m)} - X^{(m)} = 0$. The squared norm of the error is $\\|0\\|_{F}^{2} = 0$. Substituting this into the expression for $R^{(m)}$ yields:\n$$\nR^{(m)} = 1 - \\frac{0}{\\|X^{(m)}\\|_{F}^{2}} = 1\n$$\nThis holds assuming $\\|X^{(m)}\\|_{F}^{2} \\neq 0$, which is true for any non-zero data matrix.\n\n$4$. **Value of $0$ when $\\hat{X}^{(m)} = 0$**: If the reconstruction is the zero matrix, $\\hat{X}^{(m)} = 0$, the error is $E^{(m)} = X^{(m)} - 0 = X^{(m)}$. The squared reconstruction error is $\\|X^{(m)}\\|_{F}^{2}$. Substituting this into the expression:\n$$\nR^{(m)} = 1 - \\frac{\\|X^{(m)}\\|_{F}^{2}}{\\|X^{(m)}\\|_{F}^{2}} = 1 - 1 = 0\n$$\n\n$5$. **Penalizes larger squared errors**: The metric is $R^{(m)} = 1 - \\frac{\\text{error}}{\\text{total power}}$. For a given dataset $X^{(m)}$, the total power $\\|X^{(m)}\\|_{F}^{2}$ is a fixed positive constant. $R^{(m)}$ is thus a monotonically decreasing function of the squared error term $\\|X^{(m)} - \\hat{X}^{(m)}\\|_{F}^{2}$. A larger squared error results in a smaller (more negative) value of $R^{(m)}$, which constitutes a penalty.\n\n**Interpretation of Range and Limiting Behaviors**:\nThe range of $R^{(m)}$ is $(-\\infty, 1]$.\n- A value of $R^{(m)} = 1$ indicates perfect reconstruction.\n- A value of $R^{(m)} = 0$ indicates that the model is no better than a null model that predicts zero activity.\n- A value of $R^{(m)}  0$ indicates that the model's reconstruction is worse than a null model, meaning $\\|X^{(m)} - \\hat{X}^{(m)}\\|_{F}^{2}  \\|X^{(m)}\\|_{F}^{2}$. This signifies a very poor model fit.\n\n**(b) Application to the Dataset**\n\nWe are given the matrices:\n$$\nX^{(m)} = \\begin{pmatrix}\n2  -1 \\\\\n0  3 \\\\\n1  4\n\\end{pmatrix},\n\\qquad\n\\hat{X}^{(m)} = \\begin{pmatrix}\n1.8  -0.9 \\\\\n0.1  2.7 \\\\\n0.9  3.8\n\\end{pmatrix}\n$$\nWe will compute $R^{(m)}$ using the derived formula.\n\nFirst, we calculate the total signal power, $\\|X^{(m)}\\|_{F}^{2}$:\n$$\n\\|X^{(m)}\\|_{F}^{2} = 2^2 + (-1)^2 + 0^2 + 3^2 + 1^2 + 4^2 = 4 + 1 + 0 + 9 + 1 + 16 = 31\n$$\n\nNext, we compute the error matrix $E^{(m)} = X^{(m)} - \\hat{X}^{(m)}$:\n$$\nE^{(m)} = \\begin{pmatrix} 2 - 1.8  -1 - (-0.9) \\\\ 0 - 0.1  3 - 2.7 \\\\ 1 - 0.9  4 - 3.8 \\end{pmatrix} = \\begin{pmatrix} 0.2  -0.1 \\\\ -0.1  0.3 \\\\ 0.1  0.2 \\end{pmatrix}\n$$\n\nNow, we calculate the squared reconstruction error, $\\|E^{(m)}\\|_{F}^{2}$:\n$$\n\\|E^{(m)}\\|_{F}^{2} = (0.2)^2 + (-0.1)^2 + (-0.1)^2 + (0.3)^2 + (0.1)^2 + (0.2)^2\n$$\n$$\n\\|E^{(m)}\\|_{F}^{2} = 0.04 + 0.01 + 0.01 + 0.09 + 0.01 + 0.04 = 0.20\n$$\n\nFinally, we compute the reconstructivity metric $R^{(m)}$:\n$$\nR^{(m)} = 1 - \\frac{\\|E^{(m)}\\|_{F}^{2}}{\\|X^{(m)}\\|_{F}^{2}} = 1 - \\frac{0.20}{31}\n$$\n$$\nR^{(m)} \\approx 1 - 0.00645161... = 0.99354838...\n$$\nRounding the result to four significant figures, we get $0.9935$.",
            "answer": "$$\\boxed{0.9935}$$"
        },
        {
            "introduction": "An ideal dPCA model partitions neural activity into distinct, non-overlapping neural subspaces for each task variable. This advanced practice moves from theory to application by introducing a rigorous method for quantifying the separation, or \"leakage,\" between these demixed subspaces. By implementing a routine to compute the principal angles between subspaces, you will develop a powerful tool to validate the quality of your demixing and test a core hypothesis of dPCA: that the brain may use distinct neural dimensions to encode different aspects of a task .",
            "id": "4154790",
            "problem": "You are analyzing demixed principal component analysis (dPCA) subspaces derived from neuronal population data. For each task variable index $m$, the demixed component directions are assembled as a column-space basis matrix $U^{(m)} \\in \\mathbb{R}^{p \\times k_m}$, whose columns span the demixed subspace $\\mathcal{S}_m \\subset \\mathbb{R}^p$. In practice, the columns of $U^{(m)}$ may not be orthonormal. To scientifically quantify cross-variable leakage, you are asked to define the principal angles $\\theta_i$ between two subspaces $\\mathcal{S}_m$ and $\\mathcal{S}_{m'}$ and compute them using Singular Value Decomposition (SVD), a well-established linear algebra factorization. Use these angles to construct a leakage index that is interpretable in the context of dPCA.\n\nStarting from the fundamental definitions of subspaces, orthonormal bases, and the optimization-based definition of principal angles, design and implement a program that, for each pair $(U^{(m)}, U^{(m')})$, performs the following steps:\n\n- Orthonormalize the columns of $U^{(m)}$ and $U^{(m')}$ to obtain two orthonormal basis matrices $Q^{(m)}$ and $Q^{(m')}$ spanning the same subspaces. Use a numerically stable method based on $QR$ factorization.\n\n- Compute the principal angles $\\theta_i$ between $\\mathcal{S}_m$ and $\\mathcal{S}_{m'}$ via Singular Value Decomposition (SVD), applied to an appropriate matrix derived from $Q^{(m)}$ and $Q^{(m')}$. Derive the connection between the SVD outputs and principal angles from first principles; do not use or state any shortcut formulas directly in the problem statement.\n\n- Define a symmetric leakage index $L_{m,m'}$ as the average of squared cosines of the principal angles, i.e., $$L_{m,m'} = \\frac{1}{r} \\sum_{i=1}^{r} \\cos^2(\\theta_i),$$ where $r = \\min(k_m, k_{m'})$. This index is dimensionless and lies in the closed interval $[0, 1]$, with values closer to $1$ indicating stronger overlap and potential leakage between demixed subspaces.\n\n- Return the principal angles in degrees and the leakage index $L_{m,m'}$. All angles must be expressed in degrees. Round all floating-point outputs to six decimal places.\n\nTest Suite. For scientific coverage of typical and edge cases in dPCA subspace comparisons, use the following parameter sets. In each case, $U^{(m)}$ and $U^{(m')}$ are given explicitly as column-stacked matrices, and all entries are real numbers.\n\n- Case $1$ (happy path, identical subspaces, maximal overlap):\n  Ambient dimension $p = 4$. Let\n  $$U^{(m)} = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  0  0 \\\\\n  0  0\n  \\end{bmatrix}, \\quad\n  U^{(m')} = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  0  0 \\\\\n  0  0\n  \\end{bmatrix}.$$\n\n- Case $2$ (orthogonal subspaces, no overlap):\n  Ambient dimension $p = 4$. Let\n  $$U^{(m)} = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  0  0 \\\\\n  0  0\n  \\end{bmatrix}, \\quad\n  U^{(m')} = \\begin{bmatrix}\n  0  0 \\\\\n  0  0 \\\\\n  1  0 \\\\\n  0  1\n  \\end{bmatrix}.$$\n\n- Case $3$ (partial overlap, unequal subspace dimensions):\n  Ambient dimension $p = 5$. Let the angle parameter be $\\theta = 30^\\circ$ with $\\cos(\\theta) = \\frac{\\sqrt{3}}{2}$ and $\\sin(\\theta) = \\frac{1}{2}$. Define\n  $$U^{(m)} = \\begin{bmatrix}\n  1  0  0 \\\\\n  0  1  0 \\\\\n  0  0  1 \\\\\n  0  0  0 \\\\\n  0  0  0\n  \\end{bmatrix}, \\quad\n  U^{(m')} = \\begin{bmatrix}\n  \\cos(\\theta)  0 \\\\\n  \\sin(\\theta)  0 \\\\\n  0  0 \\\\\n  0  1 \\\\\n  0  0\n  \\end{bmatrix}.$$\n\n- Case $4$ (numerical edge case, nearly collinear vectors and non-orthonormal columns):\n  Ambient dimension $p = 3$. Let $\\varepsilon = 10^{-8}$. Define\n  $$U^{(m)} = \\begin{bmatrix}\n  1  1 \\\\\n  0  \\varepsilon \\\\\n  0  0\n  \\end{bmatrix}, \\quad\n  U^{(m')} = \\begin{bmatrix}\n  1 \\\\\n  -\\varepsilon \\\\\n  0\n  \\end{bmatrix}.$$\n\nImplementation and Output Requirements.\n\n- For each case, compute the list of principal angles $\\theta_i$ (in degrees) sorted in ascending order, then append the leakage index $L_{m,m'}$ as the last element of the list. Round all numerical outputs to six decimal places.\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to one test case and is itself a list of floating-point numbers (the principal angles followed by the leakage index). For example, the output format must be `[case_1,case_2,case_3,case_4]`, where each `case_i` is a list.\n\n- Angle unit: degrees. No physical units are otherwise required. Express all floating-point outputs rounded to six decimal places.\n\n- All computations must be fully self-contained and based strictly on the provided matrices; do not require any external input.",
            "solution": "The problem requires the formulation and implementation of a procedure to quantify the alignment, or \"leakage,\" between two subspaces, $\\mathcal{S}_m$ and $\\mathcal{S}_{m'}$, derived from a demixed Principal Component Analysis (dPCA). This is accomplished by computing the principal angles between the subspaces and then using them to calculate a defined leakage index, $L_{m,m'}$. The subspaces are provided via their basis matrices, $U^{(m)}$ and $U^{(m')}$, whose columns may not be orthonormal.\n\nThe solution proceeds in three main steps: first, constructing orthonormal bases for the subspaces; second, deriving the principal angles from first principles using the Singular Value Decomposition (SVD); and third, calculating the leakage index.\n\n### 1. Orthonormal Basis Construction\nA subspace is uniquely defined by a set of basis vectors, but this representation is not unique. For geometric calculations such as angles, it is computationally and conceptually necessary to work with orthonormal bases. Given a basis matrix $U \\in \\mathbb{R}^{p \\times k}$ whose columns span a $k$-dimensional subspace $\\mathcal{S} \\subset \\mathbb{R}^p$, we must find a matrix $Q \\in \\mathbb{R}^{p \\times k}$ whose columns are orthonormal (i.e., $Q^T Q = I_k$, where $I_k$ is the $k \\times k$ identity matrix) and span the same subspace, $\\text{span}(Q) = \\text{span}(U) = \\mathcal{S}$.\n\nA numerically stable method for this transformation is the QR factorization. For a matrix $A \\in \\mathbb{R}^{p \\times k}$ with $p \\ge k$ and full column rank, the 'thin' or 'reduced' QR factorization yields a unique decomposition $A = QR$, where $Q \\in \\mathbb{R}^{p \\times k}$ is a matrix with orthonormal columns and $R \\in \\mathbb{R}^{k \\times k}$ is an upper triangular, invertible matrix. The columns of $Q$ constitute the desired orthonormal basis.\n\nApplying this to the given matrices $U^{(m)}$ and $U^{(m')}$, we perform QR factorization to obtain the orthonormal basis matrices $Q^{(m)}$ and $Q^{(m')}$:\n$$ U^{(m)} = Q^{(m)} R^{(m)} $$\n$$ U^{(m')} = Q^{(m')} R^{(m')} $$\nThese matrices $Q^{(m)} \\in \\mathbb{R}^{p \\times k_m}$ and $Q^{(m')} \\in \\mathbb{R}^{p \\times k_{m'}}$ will be used for all subsequent calculations.\n\n### 2. Principal Angles from First Principles via SVD\nThe principal angles, $\\theta_i$, between two subspaces, $\\mathcal{S}_m$ and $\\mathcal{S}_{m'}$, provide a hierarchical and comprehensive measure of their relative orientation. They are defined recursively based on an optimization principle.\n\nThe first and smallest principal angle, $\\theta_1$, is the minimum possible angle between a vector $\\mathbf{u} \\in \\mathcal{S}_m$ and a vector $\\mathbf{v} \\in \\mathcal{S}_{m'}$. Maximizing the cosine of the angle is equivalent to minimizing the angle itself. Thus, $\\theta_1$ is defined by:\n$$ \\cos(\\theta_1) = \\max_{\\substack{\\mathbf{u} \\in \\mathcal{S}_m, \\|\\mathbf{u}\\|_2=1 \\\\ \\mathbf{v} \\in \\mathcal{S}_{m'}, \\|\\mathbf{v}\\|_2=1}} \\mathbf{u}^T \\mathbf{v} $$\nThe vectors $\\mathbf{u}_1$ and $\\mathbf{v}_1$ achieving this maximum are the first pair of principal vectors. Subsequent principal angles $\\theta_i$ for $i  1$ are found by repeating this maximization process in the orthogonal complements of the previously found principal vectors.\n\nTo establish a computational method using SVD, we express the vectors $\\mathbf{u}$ and $\\mathbf{v}$ in terms of the orthonormal bases $Q^{(m)}$ and $Q^{(m')}$. Any unit vector $\\mathbf{u} \\in \\mathcal{S}_m$ can be written as $\\mathbf{u} = Q^{(m)} \\mathbf{x}$ for some coordinate vector $\\mathbf{x} \\in \\mathbb{R}^{k_m}$ satisfying $\\|\\mathbf{x}\\|_2 = 1$. Similarly, $\\mathbf{v} = Q^{(m')} \\mathbf{y}$ for some $\\mathbf{y} \\in \\mathbb{R}^{k_{m'}}$ with $\\|\\mathbf{y}\\|_2 = 1$. The dot product $\\mathbf{u}^T \\mathbf{v}$ can then be rewritten as:\n$$ \\mathbf{u}^T \\mathbf{v} = (Q^{(m)} \\mathbf{x})^T (Q^{(m')} \\mathbf{y}) = \\mathbf{x}^T ( (Q^{(m)})^T Q^{(m')} ) \\mathbf{y} $$\nLet $M = (Q^{(m)})^T Q^{(m')}$. The matrix $M \\in \\mathbb{R}^{k_m \\times k_{m'}}$ encapsulates the inner products between the basis vectors of the two subspaces. The optimization problem for $\\cos(\\theta_1)$ is now:\n$$ \\cos(\\theta_1) = \\max_{\\substack{\\|\\mathbf{x}\\|_2=1 \\\\ \\|\\mathbf{y}\\|_2=1}} \\mathbf{x}^T M \\mathbf{y} $$\nThis expression is the definition of the spectral norm (or largest singular value, $\\sigma_1$) of the matrix $M$. The Courant-Fischer theorem establishes that the singular values of $M$, sorted in descending order $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r \\ge 0$ where $r = \\min(k_m, k_{m'})$, are the solutions to the successive maximization problems that define the principal angles.\n\nTherefore, the cosines of the principal angles are precisely the singular values of the matrix $M = (Q^{(m)})^T Q^{(m')}$:\n$$ \\sigma_i = \\cos(\\theta_i) \\quad \\text{for } i = 1, \\dots, r $$\nSince the columns of $Q^{(m)}$ and $Q^{(m')}$ are unit vectors, the entries of $M$ are cosines, and the singular values $\\sigma_i$ are guaranteed to be in the interval $[0, 1]$. To compute the angles, we first find the singular values $\\sigma_i$ of $M$, and then calculate $\\theta_i = \\arccos(\\sigma_i)$. For numerical robustness, it is prudent to clip the computed singular values to the range $[0.0, 1.0]$ before applying the arccosine function to prevent errors from floating-point inaccuracies. The angles obtained will be in radians and must be converted to degrees.\n\n### 3. Leakage Index Calculation\nThe problem defines a symmetric leakage index $L_{m,m'}$ as the average of the squared cosines of the principal angles:\n$$ L_{m,m'} = \\frac{1}{r} \\sum_{i=1}^{r} \\cos^2(\\theta_i), \\quad \\text{where } r = \\min(k_m, k_{m'}) $$\nThis index provides a single scalar metric for subspace overlap. A value of $L_{m,m'} = 1$ indicates that the smaller subspace is entirely contained within the larger one (maximal overlap), while $L_{m,m'} = 0$ indicates that the subspaces are completely orthogonal (no overlap).\nSubstituting $\\cos(\\theta_i) = \\sigma_i$, the formula is directly computable from the singular values:\n$$ L_{m,m'} = \\frac{1}{r} \\sum_{i=1}^{r} \\sigma_i^2 $$\nThe final step is to sort the computed principal angles in ascending order and present them along with the leakage index, with all values rounded to $6$ decimal places as required.",
            "answer": "```python\nimport numpy as np\n\ndef compute_principal_angles_and_leakage(U_m, U_mp):\n    \"\"\"\n    Computes principal angles and leakage index between two subspaces.\n\n    Args:\n        U_m (np.ndarray): A matrix whose columns form a basis for the first subspace.\n        U_mp (np.ndarray): A matrix whose columns form a basis for the second subspace.\n\n    Returns:\n        list: A list containing the sorted principal angles (in degrees) followed\n              by the leakage index, all rounded to 6 decimal places.\n    \"\"\"\n    # Step 1: Orthonormalize the basis matrices using QR factorization.\n    # The 'reduced' mode ensures that the resulting Q matrix has the same number\n    # of columns as the input, forming a basis for the column space.\n    if U_m.shape[1]  0:\n        Q_m, _ = np.linalg.qr(U_m, mode='reduced')\n    else:\n        # Handle zero-dimensional subspace\n        Q_m = np.empty((U_m.shape[0], 0))\n\n    if U_mp.shape[1]  0:\n        Q_mp, _ = np.linalg.qr(U_mp, mode='reduced')\n    else:\n        Q_mp = np.empty((U_mp.shape[0], 0))\n\n    # Determine the number of principal angles to compute.\n    k_m = Q_m.shape[1]\n    k_mp = Q_mp.shape[1]\n    r = min(k_m, k_mp)\n\n    # If one of the subspaces is zero-dimensional, there are no angles to compute.\n    if r == 0:\n        # The subspaces are orthogonal in a trivial sense if one is empty.\n        # Leakage is zero. Principal angles list is empty.\n        return [[], 0.0]\n\n    # Step 2: Compute the SVD of the matrix product of the orthonormal bases.\n    M = Q_m.T @ Q_mp\n    \n    # The singular values of M are the cosines of the principal angles.\n    # We only need the singular values, so compute_uv=False is efficient.\n    singular_values = np.linalg.svd(M, compute_uv=False)\n\n    # Step 3: Calculate the principal angles.\n    # For numerical stability, clip values to the valid domain of arccos, [-1, 1].\n    # Mathematically, these singular values are guaranteed to be in [0, 1].\n    clipped_sv = np.clip(singular_values, 0.0, 1.0)\n    \n    # anp.arccos returns angles in radians.\n    principal_angles_rad = np.arccos(clipped_sv)\n    \n    # Convert radians to degrees.\n    principal_angles_deg = np.rad2deg(principal_angles_rad)\n    \n    # Sort angles in ascending order as per requirement.\n    principal_angles_deg.sort()\n    \n    # Step 4: Calculate the leakage index.\n    # L = (1/r) * sum(cos^2(theta_i)) = (1/r) * sum(sigma_i^2)\n    leakage_index = np.sum(clipped_sv**2) / r\n    \n    # Format the results: round to 6 decimal places.\n    rounded_angles = np.round(principal_angles_deg, 6).tolist()\n    rounded_leakage = np.round(leakage_index, 6)\n    \n    return rounded_angles + [rounded_leakage]\n\ndef solve():\n    \"\"\"\n    Defines test cases and computes the results for each, printing them\n    in the specified format.\n    \"\"\"\n    # Define test cases as pairs of matrices (U^(m), U^(m')).\n    \n    # Case 1: Identical subspaces\n    U_m1 = np.array([[1., 0.], [0., 1.], [0., 0.], [0., 0.]])\n    U_mp1 = np.array([[1., 0.], [0., 1.], [0., 0.], [0., 0.]])\n    \n    # Case 2: Orthogonal subspaces\n    U_m2 = np.array([[1., 0.], [0., 1.], [0., 0.], [0., 0.]])\n    U_mp2 = np.array([[0., 0.], [0., 0.], [1., 0.], [0., 1.]])\n    \n    # Case 3: Partial overlap, unequal dimensions\n    theta3 = np.deg2rad(30)\n    cos_t3, sin_t3 = np.cos(theta3), np.sin(theta3)\n    U_m3 = np.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 0., 0.], [0., 0., 0.]])\n    U_mp3 = np.array([[cos_t3, 0.], [sin_t3, 0.], [0., 0.], [0., 1.], [0., 0.]])\n\n    # Case 4: Numerical edge case\n    eps4 = 1e-8\n    U_m4 = np.array([[1., 1.], [0., eps4], [0., 0.]])\n    U_mp4 = np.array([[1.], [-eps4], [0.]])\n    \n    test_cases = [\n        (U_m1, U_mp1),\n        (U_m2, U_mp2),\n        (U_m3, U_mp3),\n        (U_m4, U_mp4),\n    ]\n\n    results = []\n    for U_m, U_mp in test_cases:\n        result = compute_principal_angles_and_leakage(U_m, U_mp)\n        results.append(result)\n\n    # Print the final output in the exact required format:\n    # [[...],[...],...] without any spaces.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}