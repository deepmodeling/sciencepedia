{
    "hands_on_practices": [
        {
            "introduction": "To build our intuition, we begin with a foundational case: modeling a simple binary outcome. This exercise will guide you through the derivation of the scalar Fisher information for a Bernoulli process from first principles. By working through this problem, you will solidify your understanding of the link between the log-likelihood function's curvature and the information a measurement provides about an unknown parameter .",
            "id": "3886000",
            "problem": "A high-throughput single-cell assay is used to detect the presence of a specific biomarker in circulating tumor cells. Each independently processed cell yields a binary outcome $X_{i} \\in \\{0,1\\}$ indicating detected absence ($X_{i}=0$) or presence ($X_{i}=1$) of the biomarker. Assume the outcomes are independent and identically distributed (i.i.d.) with $X_{i} \\sim \\mathrm{Bernoulli}(p)$ for a fixed but unknown detection probability $p \\in (0,1)$ that characterizes the assay and the underlying biology for a given patient. You process $n \\in \\mathbb{N}$ cells.\n\nUsing only the definition of Fisher information as the expectation, under the sampling distribution at the true parameter value, of the negative second derivative of the log-likelihood with respect to the parameter, derive the Fisher information for $p$ based on the $n$ i.i.d. outcomes. In your derivation, relate this Fisher information to the curvature of the log-likelihood around the true $p$ and comment on how this curvature behaves as $p \\to 0^{+}$ and as $p \\to 1^{-}$ in the context of rare-event detection in biomedical systems.\n\nProvide your final answer as a single closed-form analytic expression in terms of $n$ and $p$. No numerical rounding is required. Express no units.",
            "solution": "The problem statement is deemed valid. It is scientifically grounded in established statistical theory, well-posed with a clear objective and sufficient information, and uses objective, formal language. The problem is a standard derivation in statistical inference with a relevant application context in biomedical systems modeling. We may, therefore, proceed with the derivation.\n\nThe problem asks for the Fisher information for a parameter $p$ of a Bernoulli distribution, based on a sample of $n$ independent and identically distributed (i.i.d.) observations. The observations are $X_1, X_2, \\dots, X_n$, where each $X_i \\in \\{0, 1\\}$ follows a Bernoulli distribution, $X_i \\sim \\mathrm{Bernoulli}(p)$. The parameter $p \\in (0,1)$ represents the probability of success, i.e., $P(X_i=1) = p$.\n\nThe probability mass function (PMF) for a single observation $X_i$ is given by:\n$$P(X_i = x_i | p) = p^{x_i} (1-p)^{1-x_i} \\quad \\text{for } x_i \\in \\{0, 1\\}$$\nSince the observations are i.i.d., the joint PMF for the entire sample $X = (X_1, \\dots, X_n)$ is the product of the individual PMFs. This joint PMF, considered as a function of the parameter $p$, is the likelihood function, $L(p | X)$:\n$$L(p | X) = \\prod_{i=1}^{n} p^{X_i} (1-p)^{1-X_i}$$\nThis can be simplified by defining the total number of successes, $S_n = \\sum_{i=1}^{n} X_i$. The likelihood function becomes:\n$$L(p | S_n) = p^{S_n} (1-p)^{n-S_n}$$\nThe next step is to find the log-likelihood function, $\\ell(p | S_n)$, which is the natural logarithm of the likelihood function:\n$$\\ell(p | S_n) = \\ln(L(p | S_n)) = \\ln\\left(p^{S_n} (1-p)^{n-S_n}\\right)$$\nUsing the properties of logarithms, we get:\n$$\\ell(p | S_n) = S_n \\ln(p) + (n-S_n) \\ln(1-p)$$\nThe problem requires us to use the definition of Fisher information, $I(p)$, as the expectation of the negative second derivative of the log-likelihood with respect to the parameter $p$. We first compute the first derivative of the log-likelihood, which is known as the score function:\n$$\\frac{\\partial \\ell}{\\partial p} = \\frac{\\partial}{\\partial p} \\left[ S_n \\ln(p) + (n-S_n) \\ln(1-p) \\right] = \\frac{S_n}{p} - \\frac{n-S_n}{1-p}$$\nNext, we compute the second derivative of the log-likelihood with respect to $p$:\n$$\\frac{\\partial^2 \\ell}{\\partial p^2} = \\frac{\\partial}{\\partial p} \\left[ \\frac{S_n}{p} - \\frac{n-S_n}{1-p} \\right] = -\\frac{S_n}{p^2} - \\left( -(n-S_n) \\frac{-1}{(1-p)^2} \\right) = -\\frac{S_n}{p^2} - \\frac{n-S_n}{(1-p)^2}$$\nThe Fisher information, $I(p)$, is defined as the negative of the expectation of this second derivative, where the expectation is taken with respect to the sampling distribution of the data, governed by the true parameter value $p$:\n$$I(p) = -E\\left[ \\frac{\\partial^2 \\ell}{\\partial p^2} \\right] = -E\\left[ -\\frac{S_n}{p^2} - \\frac{n-S_n}{(1-p)^2} \\right]$$\nBy the linearity of the expectation operator:\n$$I(p) = E\\left[ \\frac{S_n}{p^2} + \\frac{n-S_n}{(1-p)^2} \\right] = \\frac{1}{p^2} E[S_n] + \\frac{1}{(1-p)^2} E[n-S_n]$$\nThe statistic $S_n = \\sum_{i=1}^{n} X_i$ is the sum of $n$ i.i.d. Bernoulli random variables. The expectation of a single Bernoulli variable $X_i$ is $E[X_i] = p$. Thus, the expectation of $S_n$ is:\n$$E[S_n] = E\\left[ \\sum_{i=1}^{n} X_i \\right] = \\sum_{i=1}^{n} E[X_i] = \\sum_{i=1}^{n} p = np$$\nFurthermore, $E[n-S_n] = n - E[S_n] = n - np = n(1-p)$.\nSubstituting these expectations into the expression for $I(p)$:\n$$I(p) = \\frac{1}{p^2} (np) + \\frac{1}{(1-p)^2} (n(1-p))$$\nSimplifying the terms, we obtain:\n$$I(p) = \\frac{n}{p} + \\frac{n}{1-p}$$\nCombining these terms over a common denominator gives the final expression for the Fisher information:\n$$I(p) = n \\left( \\frac{1-p+p}{p(1-p)} \\right) = \\frac{n}{p(1-p)}$$\nThis is the Fisher information for the parameter $p$ based on $n$ i.i.d. Bernoulli trials.\n\nThe second part of the problem asks for a commentary on the relationship between Fisher information and the curvature of the log-likelihood. The second derivative, $\\frac{\\partial^2 \\ell}{\\partial p^2}$, measures the local curvature of the log-likelihood function at a given point. The Fisher information, $I(p) = E\\left[-\\frac{\\partial^2 \\ell}{\\partial p^2}\\right]$, represents the expected curvature of the log-likelihood function at the true parameter value $p$. A large value of $I(p)$ corresponds to a log-likelihood function that is highly peaked or sharply curved around its maximum. This indicates that the data provide substantial information for estimating the parameter, as small changes in $p$ away from the maximum likelihood estimate lead to a rapid decrease in the log-likelihood. Conversely, a small value of $I(p)$ implies a flat log-likelihood, where the data are less informative for pinning down the precise value of $p$.\n\nFinally, we analyze the behavior of $I(p) = \\frac{n}{p(1-p)}$ as $p \\to 0^{+}$ and $p \\to 1^{-}$.\nThe denominator, $p(1-p)$, approaches $0$ at both extremes of the interval $(0,1)$.\nAs $p \\to 0^{+}$, we have $p(1-p) \\to 0$, and thus $I(p) \\to \\infty$.\nAs $p \\to 1^{-}$, we also have $p(1-p) \\to 0$, and thus $I(p) \\to \\infty$.\nIn the context of rare-event detection in biomedical systems, the presence of a biomarker is often a rare event, which corresponds to the case where $p$ is very close to $0$. The fact that $I(p) \\to \\infty$ as $p \\to 0^{+}$ signifies that when an event is intrinsically rare, observing even a single instance of it (i.e., $X_i=1$) is highly informative. The log-likelihood function becomes extremely peaked, allowing for a very precise estimation of the small parameter $p$. For example, with a large number of cells ($n$ is large) and a true $p$ near $0$, observing zero successes is expected, but observing one or two successes provides strong evidence to distinguish a very small $p$ from $p=0$. The high curvature (high Fisher information) reflects this high information content. Symmetrically, if an event is almost certain ($p \\approx 1$), observing a rare failure (i.e., $X_i=0$) is also highly informative.\nThe information is minimized when the denominator $p(1-p)$ is maximized, which occurs at $p=1/2$, where uncertainty about the outcome of any single trial is greatest.",
            "answer": "$$\\boxed{\\frac{n}{p(1-p)}}$$"
        },
        {
            "introduction": "Real-world models often involve multiple unknown parameters. This practice extends our analysis to a two-parameter Gaussian model, a cornerstone of statistical modeling in biomedical applications. Here, you will derive the full $2 \\times 2$ Fisher Information Matrix, which reveals not only the information about each parameter individually but also the informational coupling between them, introducing the critical concept of parameter orthogonality .",
            "id": "3886010",
            "problem": "A biomarker quantification system produces repeated measurements due to intrinsic biological variability and sensor noise. Let the measurements be modeled as independent and identically distributed (i.i.d.) samples $y_{1},y_{2},\\dots,y_{n}$ from a Gaussian distribution with unknown mean $\\mu$ and unknown variance $\\sigma^{2}$; that is, $y_{i}\\sim \\mathcal{N}(\\mu,\\sigma^{2})$ for $i=1,\\dots,n$. In a parameter estimation study for this biomedical system, you are interested in the local sensitivity of the likelihood to the parameters, summarized by the Fisher information matrix for the parameter vector $(\\mu,\\sigma^{2})$.\n\nStarting from first principles, namely the joint probability density for i.i.d. Gaussian measurements, the log-likelihood for $(\\mu,\\sigma^{2})$, and the definition of the Fisher information matrix in terms of the score and its expected curvature, derive an explicit analytic expression for the Fisher information matrix $I(\\mu,\\sigma^{2})$ for a sample size $n$. Your derivation must explicitly account for expectations with respect to the data-generating model and must not contain any random sample realizations in the final result.\n\nGive your final answer as a single closed-form $2\\times 2$ matrix in terms of $n$ and $\\sigma^{2}$, ordered by the parameter vector $(\\mu,\\sigma^{2})$. No rounding is required. Do not include any units in your final matrix.",
            "solution": "The problem asks for the derivation of the Fisher Information Matrix (FIM) for $n$ i.i.d. samples from a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$, with the parameter vector $\\theta = (\\mu, \\sigma^2)$.\n\nLet $Y = (y_1, y_2, \\dots, y_n)$ be the vector of i.i.d. samples. The log-likelihood function $\\ell(\\mu, \\sigma^2)$ for these samples is:\n$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2$$\nThe FIM $I(\\theta)$ for a parameter vector $\\theta = (\\theta_1, \\theta_2)^T = (\\mu, \\sigma^2)^T$ is a $2 \\times 2$ matrix whose $(j, k)$-th element is $[I(\\theta)]_{jk} = -E\\left[\\frac{\\partial^2 \\ell(\\theta)}{\\partial\\theta_j \\partial\\theta_k}\\right]$. We compute the required second partial derivatives.\n\nFirst, the first partial derivatives (the score vector) are:\n$$\\frac{\\partial\\ell}{\\partial\\mu} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(y_i - \\mu)$$\n$$\\frac{\\partial\\ell}{\\partial\\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)^2$$\nNext, the second partial derivatives (the Hessian matrix) are:\n$$\\frac{\\partial^2\\ell}{\\partial\\mu^2} = -\\frac{n}{\\sigma^2}$$\n$$\\frac{\\partial^2\\ell}{\\partial\\mu \\partial\\sigma^2} = -\\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)$$\n$$\\frac{\\partial^2\\ell}{\\partial(\\sigma^2)^2} = \\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3}\\sum_{i=1}^{n}(y_i - \\mu)^2$$\nNow, we compute the negative expectation of each second derivative to find the elements of the FIM:\n\\begin{enumerate}\n    \\item $I_{\\mu\\mu} = -E\\left[\\frac{\\partial^2\\ell}{\\partial\\mu^2}\\right] = -E\\left[-\\frac{n}{\\sigma^2}\\right] = \\frac{n}{\\sigma^2}$ (since the term is constant with respect to the data).\n    \\item $I_{\\mu, \\sigma^2} = -E\\left[\\frac{\\partial^2\\ell}{\\partial\\mu \\partial\\sigma^2}\\right] = -E\\left[-\\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}(y_i - \\mu)\\right] = \\frac{1}{(\\sigma^2)^2}\\sum_{i=1}^{n}E[y_i - \\mu]$. Since $E[y_i] = \\mu$, we have $E[y_i - \\mu] = 0$, which results in $I_{\\mu, \\sigma^2} = 0$.\n    \\item $I_{\\sigma^2, \\sigma^2} = -E\\left[\\frac{\\partial^2\\ell}{\\partial(\\sigma^2)^2}\\right] = -E\\left[\\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3}\\sum_{i=1}^{n}(y_i - \\mu)^2\\right]$. Using the linearity of expectation and the fact that $E[(y_i - \\mu)^2] = \\sigma^2$:\n    $$I_{\\sigma^2, \\sigma^2} = -\\left(\\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3}\\sum_{i=1}^{n}\\sigma^2\\right) = -\\left(\\frac{n}{2(\\sigma^2)^2} - \\frac{n\\sigma^2}{(\\sigma^2)^3}\\right) = -\\left(\\frac{n}{2(\\sigma^2)^2} - \\frac{n}{(\\sigma^2)^2}\\right) = \\frac{n}{2(\\sigma^2)^2}$$\n\\end{enumerate}\nBy symmetry, $I_{\\sigma^2, \\mu} = I_{\\mu, \\sigma^2} = 0$. Assembling the elements into the FIM for the parameter vector $(\\mu, \\sigma^2)$:\n$$I(\\mu, \\sigma^2) = \\begin{pmatrix} I_{\\mu\\mu} & I_{\\mu, \\sigma^2} \\\\ I_{\\sigma^2, \\mu} & I_{\\sigma^2, \\sigma^2} \\end{pmatrix} = \\begin{pmatrix} \\frac{n}{\\sigma^2} & 0 \\\\ 0 & \\frac{n}{2(\\sigma^2)^2} \\end{pmatrix}$$\nThe off-diagonal terms are zero, which indicates that for a Gaussian distribution, the estimators for the mean $\\mu$ and the variance $\\sigma^2$ are asymptotically orthogonal.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n}{\\sigma^{2}} & 0 \\\\\n0 & \\frac{n}{2 (\\sigma^{2})^{2}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "We now apply the Fisher Information Matrix to one of the most powerful tools in systems modeling: linear regression. This problem demonstrates how the structure of an experiment, encoded in the design matrix $X$, directly impacts the precision with which we can estimate our model parameters, $\\beta$. Deriving the block structure of the Fisher Information Matrix for this model provides a concrete link between statistical theory and the practical principles of optimal experimental design .",
            "id": "3885914",
            "problem": "A widely used linear observation model in biomedical systems modeling posits that repeated measurements of a biomarker response across replicates can be described by a multivariate Gaussian distribution with a mean that is linear in known covariates and independent, homoscedastic noise. Consider an experiment with $n$ independent replicates, where the response vector is $y \\in \\mathbb{R}^{n}$, the design matrix is $X \\in \\mathbb{R}^{n \\times p}$ with full column rank $p$, and the parameter vector is $\\beta \\in \\mathbb{R}^{p}$. Assume the noise is independent across replicates and homoscedastic with variance $\\sigma^{2} > 0$, so that the probabilistic model is $y \\sim \\mathcal{N}(X \\beta, \\sigma^{2} I_{n})$, where $I_{n}$ is the $n \\times n$ identity matrix. Starting only from the fundamental definition of the log-likelihood for the multivariate Gaussian model and the definition of the Fisher information matrix, derive the Fisher information matrix $I(\\beta, \\sigma^{2})$ for the joint parameter $(\\beta, \\sigma^{2})$, and identify the block structure that links the matrix $X^{\\top} X$ to information about $\\beta$. Your final answer must be a single closed-form block matrix expression in terms of $X$, $n$, and $\\sigma^{2}$ only. No numerical rounding is required, and no physical units need be reported.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard problem in mathematical statistics: the derivation of the Fisher information matrix for a linear regression model with Gaussian noise. All necessary components are provided and are mutually consistent. We may therefore proceed with the derivation.\n\nThe parameter vector for this model is $\\theta = (\\beta^{\\top}, \\sigma^{2})^{\\top}$, which is a vector of dimension $p+1$. The Fisher Information Matrix (FIM), denoted $I(\\theta)$, is a $(p+1) \\times (p+1)$ matrix whose elements are given by $I_{ij}(\\theta) = -E\\left[\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta_i \\partial \\theta_j}\\right]$, where $\\ell(\\theta)$ is the log-likelihood function and the expectation is taken with respect to the distribution of the data $y$.\n\nFirst, we establish the log-likelihood function. The model for the response vector $y \\in \\mathbb{R}^{n}$ is a multivariate Gaussian distribution, $y \\sim \\mathcal{N}(\\mu, \\Sigma)$, with mean $\\mu = X\\beta$ and covariance matrix $\\Sigma = \\sigma^{2}I_{n}$. The probability density function (PDF) is:\n$$f(y; \\beta, \\sigma^{2}) = \\frac{1}{(2\\pi)^{n/2} |\\sigma^{2}I_{n}|^{1/2}} \\exp\\left(-\\frac{1}{2}(y - X\\beta)^{\\top}(\\sigma^{2}I_{n})^{-1}(y - X\\beta)\\right)$$\nThe determinant of the covariance matrix is $|\\sigma^{2}I_{n}| = (\\sigma^{2})^{n}|I_{n}| = (\\sigma^{2})^{n}$. The inverse is $(\\sigma^{2}I_{n})^{-1} = \\frac{1}{\\sigma^{2}}I_{n}$. Substituting these into the PDF gives:\n$$f(y; \\beta, \\sigma^{2}) = \\frac{1}{(2\\pi\\sigma^{2})^{n/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}}(y - X\\beta)^{\\top}(y - X\\beta)\\right)$$\nThe log-likelihood function, $\\ell(\\beta, \\sigma^{2}) = \\ln f(y; \\beta, \\sigma^{2})$, is therefore:\n$$\\ell(\\beta, \\sigma^{2}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^{2}) - \\frac{1}{2\\sigma^{2}}(y - X\\beta)^{\\top}(y - X\\beta)$$\n\nNext, we calculate the first and second partial derivatives of the log-likelihood with respect to the parameters $\\beta$ and $\\sigma^{2}$. For clarity, we will let $v = \\sigma^{2}$. The parameter vector is thus $(\\beta^{\\top}, v)^{\\top}$. The FIM will have a $2 \\times 2$ block structure:\n$$I(\\beta, v) = -E\\left[\\begin{pmatrix} \\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^{\\top}} & \\frac{\\partial^2 \\ell}{\\partial \\beta \\partial v} \\\\ \\frac{\\partial^2 \\ell}{\\partial v \\partial \\beta^{\\top}} & \\frac{\\partial^2 \\ell}{\\partial v^2} \\end{pmatrix}\\right] = \\begin{pmatrix} I_{\\beta\\beta} & I_{\\beta v} \\\\ I_{v\\beta} & I_{vv} \\end{pmatrix}$$\n\nWe compute the required derivatives:\n\n1.  **Derivatives with respect to $\\beta$:**\n    The first partial derivative (the score vector for $\\beta$) is:\n    $$\\frac{\\partial \\ell}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta}\\left(-\\frac{1}{2v}(y - X\\beta)^{\\top}(y - X\\beta)\\right) = -\\frac{1}{2v}\\left(-2X^{\\top}(y - X\\beta)\\right) = \\frac{1}{v}X^{\\top}(y - X\\beta)$$\n    The second partial derivative (the $\\beta\\beta$ block of the Hessian matrix) is:\n    $$\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^{\\top}} = \\frac{\\partial}{\\partial \\beta^{\\top}}\\left(\\frac{1}{v}(X^{\\top}y - X^{\\top}X\\beta)\\right) = -\\frac{1}{v}X^{\\top}X = -\\frac{1}{\\sigma^{2}}X^{\\top}X$$\n    This expression is constant with respect to the data $y$, so its expectation is itself.\n    $$I_{\\beta\\beta} = -E\\left[-\\frac{1}{\\sigma^{2}}X^{\\top}X\\right] = \\frac{1}{\\sigma^{2}}X^{\\top}X$$\n    This $p \\times p$ block explicitly links the FIM component for $\\beta$ to the matrix $X^{\\top}X$.\n\n2.  **Derivatives with respect to $v = \\sigma^{2}$:**\n    The first partial derivative (the score for $v$) is:\n    $$\\frac{\\partial \\ell}{\\partial v} = -\\frac{n}{2v} + \\frac{1}{2v^2}(y - X\\beta)^{\\top}(y - X\\beta)$$\n    The second partial derivative is:\n    $$\\frac{\\partial^2 \\ell}{\\partial v^2} = \\frac{n}{2v^2} - \\frac{1}{v^3}(y - X\\beta)^{\\top}(y - X\\beta) = \\frac{n}{2(\\sigma^{2})^2} - \\frac{1}{(\\sigma^{2})^3}(y - X\\beta)^{\\top}(y - X\\beta)$$\n    To find the $I_{vv}$ component, we take the negative expectation. Let $\\epsilon = y - X\\beta$. From the model definition, $E[\\epsilon] = 0$ and the covariance is $\\sigma^2 I_n$. We require $E[\\epsilon^{\\top}\\epsilon]$, which is the trace of the covariance matrix of $\\epsilon$. $E[\\epsilon^{\\top}\\epsilon] = E[\\text{tr}(\\epsilon\\epsilon^{\\top})] = \\text{tr}(E[\\epsilon\\epsilon^{\\top}]) = \\text{tr}(\\sigma^2 I_n) = n\\sigma^2$.\n    $$I_{vv} = -E\\left[\\frac{n}{2\\sigma^{4}} - \\frac{1}{\\sigma^{6}}(y - X\\beta)^{\\top}(y - X\\beta)\\right] = -\\frac{n}{2\\sigma^{4}} + \\frac{1}{\\sigma^{6}}E[(y - X\\beta)^{\\top}(y - X\\beta)]$$\n    $$I_{vv} = -\\frac{n}{2\\sigma^{4}} + \\frac{1}{\\sigma^{6}}(n\\sigma^{2}) = -\\frac{n}{2\\sigma^{4}} + \\frac{n}{\\sigma^{4}} = \\frac{n}{2\\sigma^{4}}$$\n\n3.  **Mixed partial derivatives:**\n    We compute the derivative of the score for $\\beta$ with respect to $v$:\n    $$\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial v} = \\frac{\\partial}{\\partial v}\\left(\\frac{1}{v}X^{\\top}(y - X\\beta)\\right) = -\\frac{1}{v^2}X^{\\top}(y - X\\beta) = -\\frac{1}{\\sigma^{4}}X^{\\top}(y - X\\beta)$$\n    Now we take the negative expectation:\n    $$I_{\\beta v} = -E\\left[-\\frac{1}{\\sigma^{4}}X^{\\top}(y - X\\beta)\\right] = \\frac{1}{\\sigma^{4}}X^{\\top}E[y - X\\beta]$$\n    Since $E[y] = X\\beta$, we have $E[y - X\\beta] = X\\beta - X\\beta = 0_{n \\times 1}$.\n    $$I_{\\beta v} = \\frac{1}{\\sigma^{4}}X^{\\top}0_{n \\times 1} = 0_{p \\times 1}$$\n    The FIM is symmetric, so $I_{v\\beta} = I_{\\beta v}^{\\top} = 0_{1 \\times p}$.\n\nThe off-diagonal blocks are zero, which indicates that the Fisher information for $\\beta$ is decoupled from the Fisher information for $\\sigma^{2}$. This is a key property of this model known as parameter orthogonality.\n\nFinally, we assemble the blocks into the full Fisher Information Matrix for the joint parameter $(\\beta, \\sigma^{2})$:\n$$I(\\beta, \\sigma^{2}) = \\begin{pmatrix} I_{\\beta\\beta} & I_{\\beta, \\sigma^{2}} \\\\ I_{\\sigma^{2}, \\beta} & I_{\\sigma^{2}, \\sigma^{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sigma^{2}}X^{\\top}X & 0_{p \\times 1} \\\\ 0_{1 \\times p} & \\frac{n}{2\\sigma^{4}} \\end{pmatrix}$$\nThis is the final closed-form expression for the Fisher information matrix as required.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sigma^{2}}X^{\\top}X & 0 \\\\\n0 & \\frac{n}{2\\sigma^{4}}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}