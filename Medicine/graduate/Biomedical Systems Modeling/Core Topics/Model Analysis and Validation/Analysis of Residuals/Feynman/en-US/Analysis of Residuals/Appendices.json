{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in mastering residual analysis is to understand the fundamental properties that residuals must satisfy. For any Ordinary Least Squares (OLS) model that includes an intercept term, it is a mathematical necessity that the sum of the residuals equals zero. This exercise  solidifies this theoretical concept through hands-on computation, guiding you to empirically verify this property across various datasets and, importantly, to observe its failure when an intercept is omitted.",
            "id": "4982788",
            "problem": "Consider a clinical modeling scenario in which systolic blood pressure is regressed on age and body mass index using ordinary least squares (OLS). Let $n$ denote the number of patients, let $\\mathbf{y} \\in \\mathbb{R}^{n}$ be the vector of measured systolic blood pressure values in millimeters of mercury (mmHg), and let $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ be the design matrix of predictors. When an intercept is included, the first column of $\\mathbf{X}$ is the constant vector of ones, and the remaining columns are the observed covariates. The OLS estimator $\\hat{\\boldsymbol{\\beta}}$ minimizes the sum of squared residuals $\\sum_{i=1}^{n} \\left(y_{i} - \\hat{y}_{i}\\right)^{2}$, with fitted values defined by $\\hat{\\mathbf{y}} = \\mathbf{X} \\hat{\\boldsymbol{\\beta}}$, and residuals defined by $e_{i} = y_{i} - \\hat{y}_{i}$ for $i=1,\\dots,n$.\n\nStarting from the fundamental OLS setup and definitions, you must empirically verify that the residuals sum to zero when an intercept is included. You will implement computations that, for each test dataset, fit the OLS model (with or without an intercept as specified), construct residuals $e_{i} = y_{i} - \\hat{y}_{i}$, and then compute the scalar $\\sum_{i=1}^{n} e_{i}$. You must express angles used in trigonometric functions in radians. The final results for each test dataset must be reported as booleans, where a result is true if and only if the absolute value of the residual sum is at most $\\tau = 10^{-10}$ (measured in mmHg), and false otherwise. The numerical computations must use the Moore–Penrose pseudoinverse to obtain the OLS solution.\n\nImplement a program that performs the following for each test case:\n- Constructs the design matrix $\\mathbf{X}$ according to whether an intercept is included.\n- Computes $\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^{+} \\mathbf{y}$, where $\\mathbf{X}^{+}$ denotes the Moore–Penrose pseudoinverse.\n- Computes residuals $\\mathbf{e} = \\mathbf{y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}$ and the scalar $S = \\sum_{i=1}^{n} e_{i}$.\n- Returns a boolean indicating whether $|\\S| \\le \\tau$.\n\nUse the following five test cases, all in physically realistic ranges, and ensure all trigonometric arguments are in radians. In each case, age is measured in years, body mass index in kilograms per square meter, and systolic blood pressure in millimeters of mercury.\n\nTest case 1 (general, heteroscedastic noise, intercept included):\n- Sample size $n_{1} = 26$.\n- Ages: $a_{i} = 30 + 2i$ for $i = 0, 1, \\dots, 25$.\n- Body mass index: $b_{i} = 25 + 0.1(a_{i} - 50) + 2 \\sin(a_{i} / 10)$.\n- Noise: $\\eta_{i} = 0.1 \\left(1 + 0.05 b_{i}\\right) \\sin(0.3 a_{i})$.\n- Systolic blood pressure: $y_{i} = 95 + 0.6 a_{i} + 1.1 b_{i} + \\eta_{i}$.\n- Include an intercept.\n\nTest case 2 (perfect linear fit, intercept included):\n- Sample size $n_{2} = 8$.\n- Ages: $a_{i} \\in \\{20, 25, 30, 35, 40, 45, 50, 55\\}$ (in order).\n- Body mass index: $b_{i} = 22 + 0.3 i$ for $i = 0, 1, \\dots, 7$.\n- Systolic blood pressure: $y_{i} = 90 + 0.5 a_{i} + 1.3 b_{i}$.\n- Include an intercept.\n\nTest case 3 (small sample, underdetermined, intercept included):\n- Sample size $n_{3} = 3$.\n- Ages: $(40, 60, 80)$.\n- Body mass index: $(22, 30, 28)$.\n- Noise vector: $(\\nu_{1}, \\nu_{2}, \\nu_{3}) = (0.1 \\times (-0.5), 0.1 \\times 0.3, 0.1 \\times (-0.2))$.\n- Systolic blood pressure: $y_{i} = 100 + 0.4 a_{i} + 1.0 b_{i} + \\nu_{i}$.\n- Include an intercept.\n\nTest case 4 (contrast case, same data as test case 1 but without intercept):\n- Use the same $a_{i}$, $b_{i}$, and $y_{i}$ as in test case 1.\n- Do not include an intercept.\n\nTest case 5 (strong collinearity between predictors, intercept included):\n- Sample size $n_{5} = 12$.\n- Ages: $a_{i} = 30 + 5i$ for $i = 0, 1, \\dots, 11$.\n- Body mass index: $b_{i} = 15 + 0.2 a_{i} + 0.1 \\sin(a_{i})$.\n- Noise: $\\xi_{i} = 0.05 \\cos(0.5 a_{i})$.\n- Systolic blood pressure: $y_{i} = 80 + 0.7 a_{i} + 0.5 b_{i} + \\xi_{i}$.\n- Include an intercept.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the five test cases in the order listed, for example, \"[result1,result2,result3,result4,result5]\". Each element must be a boolean indicating whether the residual sum is within the tolerance $\\tau$, as defined above. No other output should be produced.",
            "solution": "The user wants to empirically verify a fundamental property of ordinary least squares (OLS) regression: that the sum of residuals is zero when the model includes an intercept term. This verification will be performed numerically on several test datasets.\n\n### Step 1: Theoretical Foundation of OLS Residuals\n\nThe OLS method aims to find the coefficient vector $\\hat{\\boldsymbol{\\beta}}$ that minimizes the sum of squared residuals (SSR). Given a response vector $\\mathbf{y} \\in \\mathbb{R}^{n}$ and a design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, the vector of residuals is defined as $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}}$, where $\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\beta}$ are the fitted values.\n\nThe SSR is given by:\n$$SSR(\\boldsymbol{\\beta}) = \\mathbf{e}^\\top\\mathbf{e} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$$\n$$SSR(\\boldsymbol{\\beta}) = \\mathbf{y}^\\top\\mathbf{y} - 2\\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{y} + \\boldsymbol{\\beta}^\\top\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta}$$\n\nTo find the minimum, we differentiate $SSR(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$ and set the gradient to zero:\n$$\\nabla_{\\boldsymbol{\\beta}} SSR = -2\\mathbf{X}^\\top\\mathbf{y} + 2\\mathbf{X}^\\top\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0}$$\n\nThis yields the **normal equations**:\n$$\\mathbf{X}^\\top\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^\\top\\mathbf{y}$$\n\nThe vector of OLS residuals is $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$. A key property of these residuals can be derived by left-multiplying by $\\mathbf{X}^\\top$:\n$$\\mathbf{X}^\\top\\mathbf{e} = \\mathbf{X}^\\top(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{X}^\\top\\mathbf{y} - \\mathbf{X}^\\top\\mathbf{X}\\hat{\\boldsymbol{\\beta}}$$\n\nSubstituting the normal equations into this expression, we find:\n$$\\mathbf{X}^\\top\\mathbf{e} = \\mathbf{X}^\\top\\mathbf{y} - \\mathbf{X}^\\top\\mathbf{y} = \\mathbf{0}$$\n\nThis result, $\\mathbf{X}^\\top\\mathbf{e} = \\mathbf{0}$, signifies that the residual vector $\\mathbf{e}$ is orthogonal to each column vector of the design matrix $\\mathbf{X}$.\n\n### Step 2: The Role of the Intercept Term\n\nWhen a regression model includes an intercept, the design matrix $\\mathbf{X}$ contains a column vector of ones, which we denote as $\\mathbf{1} \\in \\mathbb{R}^{n}$.\n$$\\mathbf{X} = [\\mathbf{1} \\mid \\mathbf{x}_1 \\mid \\dots \\mid \\mathbf{x}_{p-1}]$$\n\nSince the residual vector $\\mathbf{e}$ is orthogonal to every column of $\\mathbf{X}$, it must be orthogonal to the first column, $\\mathbf{1}$:\n$$\\mathbf{1}^\\top\\mathbf{e} = 0$$\n\nExpanding this dot product gives the sum of the individual residuals:\n$$\\mathbf{1}^\\top\\mathbf{e} = \\sum_{i=1}^{n} 1 \\cdot e_i = \\sum_{i=1}^{n} e_i$$\n\nTherefore, for any OLS model that includes an intercept, it is a mathematical necessity that the sum of the residuals is zero:\n$$\\sum_{i=1}^{n} e_i = 0$$\n\nIf the model does not include an intercept (i.e., $\\mathbf{X}$ does not contain a column of ones), the orthogonality condition $\\mathbf{X}^\\top\\mathbf{e} = \\mathbf{0}$ still holds, but we can no longer conclude that $\\mathbf{1}^\\top\\mathbf{e} = 0$. The sum of residuals will generally be non-zero.\n\n### Step 3: Computational Strategy and The Moore-Penrose Pseudoinverse\n\nThe problem requires solving for the OLS coefficients using the Moore-Penrose pseudoinverse, $\\mathbf{X}^{+}$. The OLS coefficient estimator is given by:\n$$\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^{+}\\mathbf{y}$$\n\nThis approach is more general than solving the normal equations via matrix inversion (e.g., $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$), as it is numerically stable and provides a unique solution even when $\\mathbf{X}$ is not of full column rank (e.g., due to multicollinearity) or not a tall matrix.\n\nThe vector of residuals is computed as:\n$$\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{y} - \\mathbf{X}\\mathbf{X}^{+}\\mathbf{y}$$\n\nThe theoretical property that $\\sum e_i = 0$ for models with an intercept remains valid with this computational method. However, due to finite-precision floating-point arithmetic, the computed sum may be a very small non-zero number. We will therefore test the condition by checking if the absolute value of the sum is less than or equal to a small tolerance, $\\tau = 10^{-10}$.\n\nThe procedure for each test case is as follows:\n1.  Generate the response vector $\\mathbf{y}$ and the predictor variables (age and BMI).\n2.  Construct the design matrix $\\mathbf{X}$. If an intercept is included, a column of ones is prepended to the matrix of predictors.\n3.  Compute $\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^{+}\\mathbf{y}$ using `numpy.linalg.pinv`.\n4.  Calculate the residuals $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$.\n5.  Compute the sum $S = \\sum_{i=1}^{n} e_i$.\n6.  Evaluate the boolean condition $|S| \\le \\tau$.\n\nThis procedure is applied to five distinct test cases. Cases 1, 2, 3, and 5 include an intercept and are expected to satisfy the condition. Case 4, which omits the intercept, is expected to fail the condition, serving as a critical control. Cases 2 (perfect fit) and 3 (saturated model, $n=p$) are special instances where residuals should be nearly zero individually, guaranteeing a zero sum. Case 5 tests the robustness of the property under strong multicollinearity.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically verifies the OLS property that residuals sum to zero for models\n    with an intercept.\n    \"\"\"\n\n    def get_test_cases():\n        \"\"\"Generates data for the five specified test cases.\"\"\"\n        cases = []\n        \n        # Test case 1: General case with heteroscedastic noise, intercept included.\n        n1 = 26\n        i1 = np.arange(n1)\n        a1 = 30.0 + 2.0 * i1\n        b1 = 25.0 + 0.1 * (a1 - 50.0) + 2.0 * np.sin(a1 / 10.0)\n        eta1 = 0.1 * (1.0 + 0.05 * b1) * np.sin(0.3 * a1)\n        y1 = 95.0 + 0.6 * a1 + 1.1 * b1 + eta1\n        predictors1 = np.stack([a1, b1], axis=1)\n        cases.append({'y': y1, 'predictors': predictors1, 'intercept': True, 'name': 'Case 1'})\n\n        # Test case 2: Perfect linear fit, intercept included.\n        n2 = 8\n        a2 = np.array([20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0])\n        i2 = np.arange(n2)\n        b2 = 22.0 + 0.3 * i2\n        y2 = 90.0 + 0.5 * a2 + 1.3 * b2\n        predictors2 = np.stack([a2, b2], axis=1)\n        cases.append({'y': y2, 'predictors': predictors2, 'intercept': True, 'name': 'Case 2'})\n\n        # Test case 3: Small sample, saturated model (n=p), intercept included.\n        n3 = 3\n        a3 = np.array([40.0, 60.0, 80.0])\n        b3 = np.array([22.0, 30.0, 28.0])\n        nu3 = 0.1 * np.array([-0.5, 0.3, -0.2])\n        y3 = 100.0 + 0.4 * a3 + 1.0 * b3 + nu3\n        predictors3 = np.stack([a3, b3], axis=1)\n        cases.append({'y': y3, 'predictors': predictors3, 'intercept': True, 'name': 'Case 3'})\n\n        # Test case 4: Contrast case, same data as case 1 but without intercept.\n        cases.append({'y': y1, 'predictors': predictors1, 'intercept': False, 'name': 'Case 4'})\n\n        # Test case 5: Strong collinearity, intercept included.\n        n5 = 12\n        i5 = np.arange(n5)\n        a5 = 30.0 + 5.0 * i5\n        b5 = 15.0 + 0.2 * a5 + 0.1 * np.sin(a5)\n        xi5 = 0.05 * np.cos(0.5 * a5)\n        y5 = 80.0 + 0.7 * a5 + 0.5 * b5 + xi5\n        predictors5 = np.stack([a5, b5], axis=1)\n        cases.append({'y': y5, 'predictors': predictors5, 'intercept': True, 'name': 'Case 5'})\n\n        return cases\n\n    test_cases = get_test_cases()\n    results = []\n    tau = 1.0e-10\n\n    for case in test_cases:\n        y = case['y']\n        predictors = case['predictors']\n        include_intercept = case['intercept']\n\n        # Construct the design matrix X\n        if include_intercept:\n            n_obs = predictors.shape[0]\n            intercept_col = np.ones((n_obs, 1))\n            X = np.hstack([intercept_col, predictors])\n        else:\n            X = predictors\n\n        # Compute OLS solution using Moore-Penrose pseudoinverse\n        X_pinv = np.linalg.pinv(X)\n        beta_hat = X_pinv @ y\n        \n        # Compute residuals and their sum\n        residuals = y - (X @ beta_hat)\n        residual_sum = np.sum(residuals)\n        \n        # Check if the sum is within the tolerance\n        is_zero = abs(residual_sum) = tau\n        results.append(is_zero)\n    \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While raw residuals provide a basic measure of error, their interpretation can be complicated by the fact that observations with high leverage—those with unusual predictor values—tend to have smaller residuals. This practice  introduces standardized residuals, a more refined diagnostic tool that scales each raw residual by an estimate of its standard deviation, properly accounting for leverage. By calculating these standardized values, you will learn to more reliably identify potential outliers whose deviations from the model are statistically significant.",
            "id": "4982808",
            "problem": "Consider a cohort study in clinical epidemiology where the biomarker C-reactive protein (CRP) is measured in milligrams per liter and then log-transformed to stabilize variance. Denote the log-transformed CRP by $y_i$, the age in years by $x_i$, and the smoking status indicator by $z_i$, where $z_i \\in \\{0,1\\}$ with $z_i=1$ for a current smoker and $z_i=0$ otherwise. We consider a linear model for $i=1,\\dots,n$ defined by $y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\varepsilon_i$ where the errors $\\varepsilon_i$ are assumed to be independent and identically distributed as Gaussian with mean $0$ and variance $\\sigma^2$. Using ordinary least squares (OLS), the fitted values and residuals are obtained from the data and the diagonal elements $h_{ii}$ of the hat matrix quantify leverage. Standardized residuals $r_i$ are used in regression diagnostics to assess model adequacy and outliers, and numerical thresholds such as $|r_i|>2$ are commonly interpreted in terms of approximate tail probabilities under a standard normal reference.\n\nFrom first principles of the Gauss–Markov assumptions and the properties of the OLS estimator, implement an algorithm that:\n- Fits the linear model to obtain $\\hat{\\beta}$ using the design matrix $X$ with columns for an intercept, age $x_i$, and smoking indicator $z_i$.\n- Computes residuals $e_i$ and the hat matrix $H$ to obtain $h_{ii}$.\n- Estimates $\\sigma^2$ by the residual mean square $s^2$ using $n-p$ degrees of freedom, where $p$ is the number of regression coefficients including the intercept (here $p=3$).\n- Computes the standardized residuals $r_i$ for all observations.\n- Interprets the threshold $|r_i|>2$ by reporting the approximate two-sided tail probability under a standard normal reference, $2\\{1 - \\Phi(2)\\}$, where $\\Phi(\\cdot)$ is the standard normal cumulative distribution function. Additionally, report the approximate two-sided tail probabilities $p_i = 2\\Phi(-|r_i|)$ for each residual.\n\nAll ages $x_i$ must be treated in units of years. The outputs are unitless because $y_i$ is a logarithm.\n\nYour program must apply the above to the following test suite of three datasets, each specified as triplets $(\\{x_i\\},\\{z_i\\},\\{y_i\\})$:\n\n- Test case $1$ (typical variability, $n=10$, $p=3$):\n  - Ages $\\{x_i\\}$: $[22,34,47,51,36,63,41,55,29,68]$ (years)\n  - Smoking indicators $\\{z_i\\}$: $[0,1,0,1,0,1,0,1,0,1]$\n  - Log-CRP $\\{y_i\\}$: $[0.79,1.35,1.24,1.59,1.15,1.85,1.26,1.80,0.96,1.97]$\n\n- Test case $2$ (one high-leverage outlier, $n=8$, $p=3$):\n  - Ages $\\{x_i\\}$: $[25,40,45,50,55,60,65,95]$ (years)\n  - Smoking indicators $\\{z_i\\}$: $[0,0,1,0,1,0,1,1]$\n  - Log-CRP $\\{y_i\\}$: $[0.93,1.16,1.61,1.41,1.70,1.62,1.95,0.80]$\n\n- Test case $3$ (near-boundary small sample, $n=4$, $p=3$):\n  - Ages $\\{x_i\\}$: $[30,50,70,60]$ (years)\n  - Smoking indicators $\\{z_i\\}$: $[0,1,0,1]$\n  - Log-CRP $\\{y_i\\}$: $[1.01,1.64,1.78,1.86]$\n\nFor each test case, produce:\n- The list of standardized residuals $[r_1,\\dots,r_n]$ rounded to $6$ decimal places.\n- The list of approximate two-sided tail probabilities $[p_1,\\dots,p_n]$ where $p_i = 2\\Phi(-|r_i|)$, rounded to $6$ decimal places.\n- The integer count $c$ of residuals with $|r_i|>2$.\n- The float $\\pi_2 = 2\\{1 - \\Phi(2)\\}$, rounded to $6$ decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the order specified above. For example, the overall format is $[[\\text{case1\\_residuals},\\text{case1\\_pvals},c_1,\\pi_2],[\\text{case2\\_residuals},\\text{case2\\_pvals},c_2,\\pi_2],[\\text{case3\\_residuals},\\text{case3\\_pvals},c_3,\\pi_2]]$. All probabilities must be expressed as decimals, not with a percentage sign.",
            "solution": "The user has provided a problem in the domain of statistical regression diagnostics, which requires the implementation of an algorithm to compute and interpret standardized residuals for a multiple linear regression model. The problem statement has been critically validated and is determined to be valid.\n\n### Validation Verdict:\nThe problem is **valid**.\n- **Scientifically Grounded**: The problem is based on the classical theory of linear models, ordinary least squares (OLS) estimation, and standard regression diagnostics. All concepts, including the Gauss-Markov model, hat matrix, leverage, and standardized residuals, are fundamental to statistics.\n- **Well-Posed**: The problem is clearly defined with all necessary data and mathematical formulas either provided or standard in the field. The OLS solution exists and is unique for the provided datasets as the design matrices have full column rank ($n>p$ and no perfect collinearity).\n- **Objective**: The problem is stated using precise and unambiguous mathematical language, free of subjective claims.\n- **Complete and Consistent**: All required data for the three test cases are provided, and the steps for the algorithm are explicitly outlined. The definition of the variance estimator $s^2$ and its degrees of freedom are consistent with standard OLS theory. The definition of the standardized residual is implicitly the standard one, $r_i = e_i / (s \\sqrt{1-h_{ii}})$, which is appropriate.\n\nThe problem is a straightforward application of established statistical methods and is fully specified, allowing for a unique and verifiable solution.\n\n### Solution Derivations\nThe solution will be implemented by following the algorithmic steps outlined in the problem statement, which are based on the principles of ordinary least squares (OLS) regression.\n\nFor each dataset comprising $n$ observations of $(y_i, x_i, z_i)$, we define the model as:\n$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\varepsilon_i$, with $\\varepsilon_i \\sim \\text{i.i.d. } N(0, \\sigma^2)$.\n\nThis can be expressed in matrix form as $y = X\\beta + \\varepsilon$, where:\n- $y$ is the $n \\times 1$ response vector $\\{y_i\\}$.\n- $X$ is the $n \\times p$ design matrix, with $p=3$. The columns of $X$ are a vector of ones (for the intercept $\\beta_0$), the vector of ages $\\{x_i\\}$, and the vector of smoking indicators $\\{z_i\\}$.\n- $\\beta = (\\beta_0, \\beta_1, \\beta_2)^T$ is the vector of coefficients.\n- $\\varepsilon$ is the vector of error terms.\n\nThe algorithm proceeds as follows:\n\n**1. OLS Estimation of $\\beta$**:\nThe OLS estimator $\\hat{\\beta}$ minimizes the residual sum of squares $RSS = \\sum e_i^2 = (y - X\\beta)^T(y - X\\beta)$. The solution is given by the normal equations:\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\n\n**2. Computation of Residuals and Hat Matrix**:\n- The fitted values are calculated as $\\hat{y} = X\\hat{\\beta}$.\n- The raw residuals are the differences between observed and fitted values: $e = y - \\hat{y}$.\n- The hat matrix, $H$, projects the observed values $y$ onto the space spanned by the columns of $X$ to produce the fitted values: $\\hat{y} = Hy$. It is defined as:\n$$\nH = X(X^T X)^{-1} X^T\n$$\nThe diagonal elements of the hat matrix, $h_{ii}$, are the leverage scores for each observation. They satisfy $0 \\le h_{ii} \\le 1$ and $\\sum_{i=1}^n h_{ii} = p$.\n\n**3. Estimation of Error Variance $\\sigma^2$**:\nThe unknown error variance $\\sigma^2$ is estimated by the residual mean square, $s^2$:\n$$\ns^2 = \\frac{RSS}{n-p} = \\frac{e^T e}{n-p}\n$$\nwhere $n-p$ are the residual degrees of freedom. The estimator for the standard deviation of the error is $s = \\sqrt{s^2}$.\n\n**4. Computation of Standardized Residuals**:\nThe variance of the $i$-th raw residual $e_i$ is $\\text{Var}(e_i) = \\sigma^2(1-h_{ii})$. The standardized residual, $r_i$, is the raw residual scaled by its estimated standard deviation:\n$$\nr_i = \\frac{e_i}{s \\sqrt{1 - h_{ii}}}\n$$\nUnder the model assumptions, each $r_i$ has a mean of approximately $0$ and a variance of approximately $1$. For large $n$, their distribution is well-approximated by the standard normal distribution, $N(0, 1)$.\n\n**5. Calculation of Probabilities**:\n- The reference probability for the threshold $|r_i| > 2$ is computed under the standard normal approximation:\n$$\n\\pi_2 = P(|Z| > 2) = 2 \\cdot P(Z  -2) = 2 \\Phi(-2)\n$$\nwhere $Z \\sim N(0,1)$ and $\\Phi(\\cdot)$ is the standard normal cumulative distribution function (CDF).\n- For each standardized residual $r_i$, the corresponding approximate two-sided tail probability is:\n$$\np_i = P(|Z| > |r_i|) = 2 \\Phi(-|r_i|)\n$$\nThe standard normal CDF is computed using its relation to the error function, $\\text{erf}(x)$, available in `scipy.special`:\n$$\n\\Phi(z) = \\frac{1}{2} \\left(1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right)\n$$\nThis leads to the expression for the two-sided tail probability:\n$$\np_i = 2(1 - \\Phi(|r_i|)) = 2\\left(1 - \\frac{1}{2}\\left(1 + \\text{erf}\\left(\\frac{|r_i|}{\\sqrt{2}}\\right)\\right)\\right) = 1 - \\text{erf}\\left(\\frac{|r_i|}{\\sqrt{2}}\\right)\n$$\nA similar formula holds for $\\pi_2$. The implementation will use this `erf`-based calculation.\n\nThe final step is to count the number of residuals $c$ for which $|r_i| > 2$ and assemble all results into the specified output format. This entire procedure will be applied to each of the three test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (typical variability, n=10, p=3)\n        (\n            [22, 34, 47, 51, 36, 63, 41, 55, 29, 68],  # Ages {x_i}\n            [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],          # Smoking indicators {z_i}\n            [0.79, 1.35, 1.24, 1.59, 1.15, 1.85, 1.26, 1.80, 0.96, 1.97] # Log-CRP {y_i}\n        ),\n        # Test case 2 (one high-leverage outlier, n=8, p=3)\n        (\n            [25, 40, 45, 50, 55, 60, 65, 95],         # Ages {x_i}\n            [0, 0, 1, 0, 1, 0, 1, 1],                 # Smoking indicators {z_i}\n            [0.93, 1.16, 1.61, 1.41, 1.70, 1.62, 1.95, 0.80] # Log-CRP {y_i}\n        ),\n        # Test case 3 (near-boundary small sample, n=4, p=3)\n        (\n            [30, 50, 70, 60],                         # Ages {x_i}\n            [0, 1, 0, 1],                             # Smoking indicators {z_i}\n            [1.01, 1.64, 1.78, 1.86]                  # Log-CRP {y_i}\n        )\n    ]\n\n    all_results = []\n    for case_data in test_cases:\n        result = process_case(case_data)\n        all_results.append(result)\n\n    # Format the final output string exactly as specified.\n    # The str() of a list automatically includes spaces, e.g., '[1, 2]'.\n    # To get a compact representation, we can build the string manually.\n    result_strings = []\n    for res in all_results:\n        r_list_str = f\"[{','.join(map(str, res[0]))}]\"\n        p_list_str = f\"[{','.join(map(str, res[1]))}]\"\n        c_str = str(res[2])\n        pi2_str = str(res[3])\n        result_strings.append(f\"[{r_list_str},{p_list_str},{c_str},{pi2_str}]\")\n    \n    # The problem example implies that str(list) is fine. Let's use the simpler approach.\n    print(str(all_results).replace(\" \", \"\"))\n\n\ndef process_case(case_data):\n    \"\"\"\n    Implements OLS regression and diagnostics for a single dataset.\n\n    Args:\n        case_data (tuple): A tuple containing lists for ages, smoking indicators,\n                           and log-CRP values.\n\n    Returns:\n        list: A list containing [standardized_residuals, tail_probabilities, count, pi_2].\n    \"\"\"\n    x_i, z_i, y_i = case_data\n    \n    # Convert input lists to numpy arrays for vector/matrix operations.\n    x_vec = np.array(x_i)\n    z_vec = np.array(z_i)\n    y_vec = np.array(y_i)\n    \n    n = len(y_vec)  # Number of observations\n    p = 3           # Number of parameters (beta_0, beta_1, beta_2)\n    \n    # Construct the n x p design matrix X.\n    X = np.ones((n, p))\n    X[:, 1] = x_vec\n    X[:, 2] = z_vec\n    \n    # Step 1: Fit the linear model using OLS to get beta_hat.\n    # beta_hat = (X'X)^-1 * X'y\n    XTX = X.T @ X\n    XTX_inv = np.linalg.inv(XTX)\n    XTY = X.T @ y_vec\n    beta_hat = XTX_inv @ XTY\n    \n    # Step 2: Compute residuals and hat matrix.\n    # Fitted values: y_hat = X * beta_hat\n    y_hat = X @ beta_hat\n    # Raw residuals: e = y - y_hat\n    residuals = y_vec - y_hat\n    # Hat matrix: H = X * (X'X)^-1 * X'\n    hat_matrix = X @ XTX_inv @ X.T\n    # Leverage values (diagonal of H): h_ii\n    leverages = np.diag(hat_matrix)\n    \n    # Step 3: Estimate error variance sigma^2.\n    # Residual Sum of Squares (RSS)\n    rss = residuals.T @ residuals\n    # Degrees of freedom for error\n    df = n - p\n    # Residual Mean Square (s^2)\n    s_squared = rss / df\n    # Residual standard error (s)\n    s = np.sqrt(s_squared)\n    \n    # Step 4: Compute standardized residuals.\n    # r_i = e_i / (s * sqrt(1 - h_ii))\n    denom = s * np.sqrt(1 - leverages)\n    # Avoid division by zero in case of h_ii=1 (not expected here)\n    # Adding a small epsilon would be robust, but not necessary for these test cases.\n    standardized_residuals = residuals / denom\n    \n    # Step 5  6: Compute tail probabilities and count.\n    # The two-sided tail probability is P(|Z| > |x|) = 2 * (1 - Phi(|x|))\n    # which simplifies to 1 - erf(|x|/sqrt(2)).\n    \n    # For the threshold |r_i| > 2\n    pi_2 = 1.0 - erf(2.0 / np.sqrt(2.0))\n    \n    # For each standardized residual\n    tail_probabilities = 1.0 - erf(np.abs(standardized_residuals) / np.sqrt(2.0))\n    \n    # Count of residuals with absolute value greater than 2.\n    count_gt_2 = np.sum(np.abs(standardized_residuals) > 2)\n\n    # Prepare the output lists and values, rounded to 6 decimal places.\n    r_list = np.round(standardized_residuals, 6).tolist()\n    p_vals_list = np.round(tail_probabilities, 6).tolist()\n    \n    return [r_list, p_vals_list, int(count_gt_2), round(pi_2, 6)]\n\n# Execute the solver\nsolve()\n```"
        },
        {
            "introduction": "To fully quantify an observation's impact on a model, one must synthesize the concepts of residual magnitude (poor fit) and leverage (unusualness in predictor space). Cook's distance is a quintessential influence metric that accomplishes this by measuring the effect of deleting a given observation on all the model's parameter estimates. This theoretical exercise  guides you through a first-principles derivation of Cook's distance, revealing how it elegantly combines the residual and leverage into a single, interpretable measure of influence.",
            "id": "3871239",
            "problem": "A biomedical systems model is used to relate an exposure vector to a multi-biomarker response in a cohort study. After linearization around an operating point, the model for a single biomarker is represented as a standard linear regression,\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\mathbf{y} \\in \\mathbb{R}^{n}$ is the vector of measured log-transformed responses, $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix encoding exposures and covariates (including an intercept), $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ is the unknown parameter vector, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$ is the noise vector satisfying $\\mathbb{E}[\\boldsymbol{\\varepsilon}]=\\mathbf{0}$ and $\\operatorname{Var}(\\boldsymbol{\\varepsilon})=\\sigma^{2}\\mathbf{I}_{n}$. The ordinary least squares (OLS) estimator is $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$, with fitted values $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$ and residuals $\\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{y}}$. Let the hat matrix be $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}$ and the leverage of the $i$-th observation be $h_{ii} = \\mathbf{x}_{i}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_{i}$, where $\\mathbf{x}_{i}^{\\top}$ is the $i$-th row of $\\mathbf{X}$.\n\nConsider deleting observation $i$, yielding the leave-one-out estimator $\\hat{\\boldsymbol{\\beta}}_{(i)}$ and fitted values $\\hat{\\mathbf{y}}_{(i)}$. Starting only from the linear model, the OLS definitions above, and standard matrix identities, derive the Cook’s distance for observation $i$ as a scalar measure based on the change in fitted values due to case deletion. Explicitly connect this scalar to the residual $r_{i}$, the leverage $h_{ii}$, and the parameter influence quantified by the change in $\\boldsymbol{\\beta}$. Use the number of parameters $p$ and the full-model residual variance estimate $\\hat{\\sigma}^{2} = \\|\\mathbf{r}\\|^{2}/(n-p)$ as needed.\n\nThen, for a specific patient in a pharmacogenomics sub-study with $p=4$, residual $r_{i}=0.85$, leverage $h_{ii}=0.12$, and full-model variance estimate $\\hat{\\sigma}^{2}=0.30$, compute the Cook’s distance for this observation. Round your final numerical answer to four significant figures. Cook’s distance is unitless; express the final value as a unitless quantity.",
            "solution": "The problem requires the derivation of Cook's distance for a single observation $i$ in a linear regression model, and subsequently, its calculation for a specific case.\n\nFirst, we validate the problem.\n\n### Step 1: Extract Givens\n- Model: $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$\n- $\\mathbf{y} \\in \\mathbb{R}^{n}$: response vector\n- $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$: design matrix\n- $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$: parameter vector\n- $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$: noise vector with $\\mathbb{E}[\\boldsymbol{\\varepsilon}]=\\mathbf{0}$ and $\\operatorname{Var}(\\boldsymbol{\\varepsilon})=\\sigma^{2}\\mathbf{I}_{n}$\n- OLS estimator: $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$\n- Fitted values: $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$\n- Residuals: $\\mathbf{r} = \\mathbf{y} - \\hat{\\mathbf{y}}$\n- Hat matrix: $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}$\n- Leverage of observation $i$: $h_{ii} = \\mathbf{x}_{i}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_{i}$, where $\\mathbf{x}_{i}^{\\top}$ is the $i$-th row of $\\mathbf{X}$\n- Leave-one-out estimator: $\\hat{\\boldsymbol{\\beta}}_{(i)}$\n- Residual variance estimate: $\\hat{\\sigma}^{2} = \\|\\mathbf{r}\\|^{2}/(n-p)$\n- Specific case data: $p=4$, $r_{i}=0.85$, $h_{ii}=0.12$, $\\hat{\\sigma}^{2}=0.30$\n- Task: Derive Cook's distance and then compute it for the specific case, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard topic in regression diagnostics within statistics. It is well-posed, providing all necessary definitions and data for a unique solution. The language is objective and mathematically precise. The problem is self-contained and free of contradictions or factual errors. The task is directly related to the analysis of residuals in biomedical modeling.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Derivation of Cook's Distance\nCook's distance, $D_i$, measures the influence of observation $i$ on the model parameters. It can be defined in terms of the squared Euclidean distance between the vector of fitted values calculated with and without the $i$-th observation, scaled by the model's variance and degrees of freedom. An equivalent and insightful definition is based on the displacement of the parameter vector $\\boldsymbol{\\beta}$:\n$$\nD_i = \\frac{(\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)})^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})(\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)})}{p\\hat{\\sigma}^{2}}\n$$\nThis form quantifies the influence of observation $i$ on the parameter estimates $\\hat{\\boldsymbol{\\beta}}$. Our goal is to express this in terms of the residual $r_i$ and leverage $h_{ii}$.\n\nThe key is to find an expression for the change in coefficients, $\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)}$. The leave-one-out estimator $\\hat{\\boldsymbol{\\beta}}_{(i)}$ is computed using the data with the $i$-th observation removed. Let $\\mathbf{X}_{(i)}$ and $\\mathbf{y}_{(i)}$ denote the design matrix and response vector with the $i$-th case deleted. We can write:\n$$\n\\mathbf{X}^{\\top}\\mathbf{X} = \\sum_{j=1}^{n} \\mathbf{x}_j \\mathbf{x}_j^{\\top} = \\mathbf{X}_{(i)}^{\\top}\\mathbf{X}_{(i)} + \\mathbf{x}_i \\mathbf{x}_i^{\\top}\n$$\n$$\n\\mathbf{X}^{\\top}\\mathbf{y} = \\sum_{j=1}^{n} \\mathbf{x}_j y_j = \\mathbf{X}_{(i)}^{\\top}\\mathbf{y}_{(i)} + \\mathbf{x}_i y_i\n$$\nFrom these, we have $\\mathbf{X}_{(i)}^{\\top}\\mathbf{X}_{(i)} = \\mathbf{X}^{\\top}\\mathbf{X} - \\mathbf{x}_i \\mathbf{x}_i^{\\top}$ and $\\mathbf{X}_{(i)}^{\\top}\\mathbf{y}_{(i)} = \\mathbf{X}^{\\top}\\mathbf{y} - \\mathbf{x}_i y_i$.\n\nThe leave-one-out estimator is $\\hat{\\boldsymbol{\\beta}}_{(i)} = (\\mathbf{X}_{(i)}^{\\top}\\mathbf{X}_{(i)})^{-1}\\mathbf{X}_{(i)}^{\\top}\\mathbf{y}_{(i)}$. Substituting the expressions above:\n$$\n\\hat{\\boldsymbol{\\beta}}_{(i)} = (\\mathbf{X}^{\\top}\\mathbf{X} - \\mathbf{x}_i \\mathbf{x}_i^{\\top})^{-1}(\\mathbf{X}^{\\top}\\mathbf{y} - \\mathbf{x}_i y_i)\n$$\nTo handle the inverse of the rank-$1$ updated matrix, we use the Sherman-Morrison-Woodbury formula: $(\\mathbf{A} - \\mathbf{u}\\mathbf{v}^{\\top})^{-1} = \\mathbf{A}^{-1} + \\frac{\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{v}^{\\top}\\mathbf{A}^{-1}}{1 - \\mathbf{v}^{\\top}\\mathbf{A}^{-1}\\mathbf{u}}$.\nLetting $\\mathbf{A} = \\mathbf{X}^{\\top}\\mathbf{X}$ and $\\mathbf{u} = \\mathbf{v} = \\mathbf{x}_i$, we get:\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X} - \\mathbf{x}_i \\mathbf{x}_i^{\\top})^{-1} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} + \\frac{(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i\\mathbf{x}_i^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}}{1 - \\mathbf{x}_i^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i}\n$$\nThe denominator contains the leverage $h_{ii} = \\mathbf{x}_i^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i$. So,\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X} - \\mathbf{x}_i \\mathbf{x}_i^{\\top})^{-1} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} + \\frac{(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i\\mathbf{x}_i^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}}{1 - h_{ii}}\n$$\nNow, substitute this back into the expression for $\\hat{\\boldsymbol{\\beta}}_{(i)}$:\n$$\n\\hat{\\boldsymbol{\\beta}}_{(i)} = \\left[ (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} + \\frac{(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i\\mathbf{x}_i^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}}{1 - h_{ii}} \\right] (\\mathbf{X}^{\\top}\\mathbf{y} - \\mathbf{x}_i y_i)\n$$\nExpanding this product gives four terms:\n$$\n\\hat{\\boldsymbol{\\beta}}_{(i)} = \\underbrace{(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}}_{\\hat{\\boldsymbol{\\beta}}} - (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i y_i + \\frac{(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\overbrace{(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}}^{\\hat{\\boldsymbol{\\beta}}}}{1 - h_{ii}} - \\frac{(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i\\overbrace{\\mathbf{x}_i^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i}^{h_{ii}} y_i}{1 - h_{ii}}\n$$\nRecognizing that the fitted value $\\hat{y}_i = \\mathbf{x}_i^{\\top}\\hat{\\boldsymbol{\\beta}}$, the third term simplifies:\n$$\n\\hat{\\boldsymbol{\\beta}}_{(i)} = \\hat{\\boldsymbol{\\beta}} - (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i y_i + \\frac{(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i \\hat{y}_i}{1 - h_{ii}} - \\frac{(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i h_{ii} y_i}{1 - h_{ii}}\n$$\nFactoring out $(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i$:\n$$\n\\hat{\\boldsymbol{\\beta}}_{(i)} = \\hat{\\boldsymbol{\\beta}} - (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i \\left( y_i - \\frac{\\hat{y}_i}{1 - h_{ii}} + \\frac{h_{ii} y_i}{1 - h_{ii}} \\right)\n$$\n$$\n\\hat{\\boldsymbol{\\beta}}_{(i)} = \\hat{\\boldsymbol{\\beta}} - (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i \\left( \\frac{y_i(1 - h_{ii}) - \\hat{y}_i + h_{ii} y_i}{1 - h_{ii}} \\right) = \\hat{\\boldsymbol{\\beta}} - (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i \\left( \\frac{y_i - y_i h_{ii} - \\hat{y}_i + h_{ii} y_i}{1 - h_{ii}} \\right)\n$$\n$$\n\\hat{\\boldsymbol{\\beta}}_{(i)} = \\hat{\\boldsymbol{\\beta}} - (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i \\left( \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right)\n$$\nUsing the definition of the residual $r_i = y_i - \\hat{y}_i$, we arrive at the expression for the change in coefficients:\n$$\n\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i \\frac{r_i}{1-h_{ii}}\n$$\nThis equation explicitly quantifies the parameter influence of observation $i$. Now we substitute this into the definition of Cook's distance:\n$$\nD_i = \\frac{1}{p\\hat{\\sigma}^{2}} \\left( (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i \\frac{r_i}{1-h_{ii}} \\right)^{\\top} (\\mathbf{X}^{\\top}\\mathbf{X}) \\left( (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i \\frac{r_i}{1-h_{ii}} \\right)\n$$\n$$\nD_i = \\frac{1}{p\\hat{\\sigma}^{2}} \\left( \\frac{r_i}{1-h_{ii}} \\right)^2 \\left( (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i \\right)^{\\top} (\\mathbf{X}^{\\top}\\mathbf{X}) \\left( (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}_i \\right)\n$$\n$$\nD_i = \\frac{r_i^2}{p\\hat{\\sigma}^{2}(1-h_{ii})^2} \\left( \\mathbf{x}_i^{\\top} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} (\\mathbf{X}^{\\top}\\mathbf{X}) (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{x}_i \\right)\n$$\nThe expression in the parenthesis simplifies to $\\mathbf{x}_i^{\\top} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{x}_i = h_{ii}$. Therefore, we obtain the final desired form for Cook's distance:\n$$\nD_i = \\frac{r_i^2}{p\\hat{\\sigma}^{2}} \\frac{h_{ii}}{(1-h_{ii})^2}\n$$\nThis result elegantly shows that the influence of an observation, $D_i$, is a function of its squared residual (how poorly it is fit), its leverage (its distance from the center of the predictor space), the number of parameters, and the overall model error.\n\n### Numerical Calculation\nWe are given the following values for a specific patient:\n- Number of parameters $p = 4$\n- Residual $r_i = 0.85$\n- Leverage $h_{ii} = 0.12$\n- Full-model residual variance estimate $\\hat{\\sigma}^{2} = 0.30$\n\nWe substitute these values into the derived formula for Cook's distance:\n$$\nD_i = \\frac{(0.85)^2}{(4)(0.30)} \\frac{0.12}{(1 - 0.12)^2}\n$$\nFirst, calculate the components of the expression:\n- $r_i^2 = (0.85)^2 = 0.7225$\n- $p\\hat{\\sigma}^2 = 4 \\times 0.30 = 1.2$\n- $1 - h_{ii} = 1 - 0.12 = 0.88$\n- $(1 - h_{ii})^2 = (0.88)^2 = 0.7744$\n\nNow, substitute these back into the expression for $D_i$:\n$$\nD_i = \\frac{0.7225}{1.2} \\times \\frac{0.12}{0.7744}\n$$\n$$\nD_i \\approx (0.6020833...) \\times (0.1549651...)\n$$\n$$\nD_i \\approx 0.09330357...\n$$\nThe problem asks to round the final answer to four significant figures.\n$$\nD_i \\approx 0.09330\n$$\nThe Cook's distance for this observation is approximately $0.09330$.",
            "answer": "$$\n\\boxed{0.09330}\n$$"
        }
    ]
}