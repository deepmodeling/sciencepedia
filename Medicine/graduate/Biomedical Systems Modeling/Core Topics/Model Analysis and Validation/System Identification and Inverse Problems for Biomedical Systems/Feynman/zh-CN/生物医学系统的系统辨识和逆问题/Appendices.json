{
    "hands_on_practices": [
        {
            "introduction": "在处理生物医学系统中的逆问题时，正则化是应对不适定性的关键工具，但其本身也引入了偏差与方差之间的权衡。本练习将引导你对两种经典的正则化方法——Tikhonov正则化和截断奇异值分解（TSVD）——进行理论分析。通过推导它们的期望平方误差，你将深入理解这两种方法如何通过不同的方式来平衡偏差和方差，这对于在实践中选择和调整正则化策略至关重要。",
            "id": "3935371",
            "problem": "考虑一个源于器官灌注动态示踪剂动力学建模的线性反问题，其中一个已知的前向算子将离散化的组织脉冲响应映射到测量的浓度-时间曲线上。在基线工作点附近进行线性化后，模型为 $$y = A x + \\epsilon,$$ 其中 $A \\in \\mathbb{R}^{m \\times n}$ 是病态的，$x \\in \\mathbb{R}^{n}$ 是编码脉冲响应系数的未知参数向量，$y \\in \\mathbb{R}^{m}$ 是测量向量，而 $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2} I_{m})$ 是方差为 $\\sigma^{2}$ 的高斯白噪声。令 $A$ 的奇异值分解（SVD）为 $A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 的对角线元素为 $s_{1} \\ge s_{2} \\ge \\cdots \\ge s_{n} > 0$，即 $A$ 的奇异值。在右奇异向量基中展开真实参数向量为 $$x = \\sum_{i=1}^{n} x_{i} v_{i},$$ 其中 $v_{i}$ 是 $V$ 的第 $i$ 列，$x_{i} \\in \\mathbb{R}$ 是展开系数。\n\n考虑两种正则化估计器：\n- Tikhonov 正则化，参数为 $\\lambda > 0$，定义为 $$\\hat{x}_{\\mathrm{Tik}}(\\lambda) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}.$$\n- 截断奇异值分解（TSVD），截断指数为 $k \\in \\{1,2,\\ldots,n\\}$，其定义为将 $y$ 投影到前 $k$ 个左奇异向量张成的空间上，并仅反演前 $k$ 个奇异分量。\n\n从模型 $y = A x + \\epsilon$、U 和 V 的正交性以及高斯白噪声的性质出发，推导期望平方估计误差 $$\\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{Tik}}(\\lambda) - x\\big\\|_{2}^{2} \\quad \\text{和} \\quad \\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{TSVD}}(k) - x\\big\\|_{2}^{2}$$，用 $\\{s_{i}\\}_{i=1}^{n}$、$\\{x_{i}\\}_{i=1}^{n}$、$\\sigma^{2}$、$\\lambda$ 和 $k$ 表示。使用与通过奇异值分解（SVD）对角化的线性估计器的滤波 SVD 表示相一致的偏差-方差分解。然后，根据每个奇异分量指数 $i$，分析 Tikhonov 正则化和截断奇异值分解（TSVD）之间偏差和方差贡献的差异，并确定在哪些依赖于指数的区间内 TSVD 比 Tikhonov 产生更小的期望误差。\n\n最后，定义差值 $$\\Delta(\\lambda,k) = \\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{Tik}}(\\lambda) - x\\big\\|_{2}^{2} - \\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{TSVD}}(k) - x\\big\\|_{2}^{2}.$$ 为 $\\Delta(\\lambda,k)$ 提供一个单一的闭式解析表达式，明确地表示为 $\\{s_{i}\\}_{i=1}^{n}$、$\\{x_{i}\\}_{i=1}^{n}$、$\\sigma^{2}$、$\\lambda$ 和 $k$ 的函数。最终答案以无单位的符号表达式形式给出。",
            "solution": "所述问题在科学上是合理的、适定的、客观的且自洽的。它提出了逆问题理论和系统辨识领域中一个标准的、非平凡的问题。所有必要的定义、条件和数据都已提供，足以进行严谨的数学推导。因此，该问题是有效的，并且可以推导出解。\n\n该问题要求推导和比较 Tikhonov 和截断奇异值分解（TSVD）这两种正则化估计器在线性反问题中的期望均方误差（MSE）。我们首先将问题转换到由矩阵 A 的奇异值分解（SVD）定义的基中。\n\n模型由 $y = Ax + \\epsilon$ 给出，其中 $A = U \\Sigma V^{\\top}$ 且 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$。我们将模型方程左乘 $U^{\\top}$：\n$$U^{\\top} y = U^{\\top} (U \\Sigma V^{\\top}) x + U^{\\top} \\epsilon = \\Sigma (V^{\\top} x) + U^{\\top} \\epsilon$$\n我们定义变换后的向量：\n- $\\tilde{y} = U^{\\top} y \\in \\mathbb{R}^{m}$\n- $\\tilde{x} = V^{\\top} x \\in \\mathbb{R}^{n}$\n- $\\tilde{\\epsilon} = U^{\\top} \\epsilon \\in \\mathbb{R}^{m}$\n\n$\\tilde{x}$ 的分量由 $\\tilde{x}_i = v_i^{\\top} x$ 给出。问题陈述将这些系数记为 $x_i$，因此我们将采用此记法，即 $\\tilde{x} = (x_1, x_2, \\ldots, x_n)^{\\top}$。变换后的噪声向量 $\\tilde{\\epsilon}$ 的均值为 $\\mathbb{E}[\\tilde{\\epsilon}] = U^{\\top}\\mathbb{E}[\\epsilon] = 0$，协方差为 $\\mathbb{E}[\\tilde{\\epsilon}\\tilde{\\epsilon}^{\\top}] = U^{\\top}\\mathbb{E}[\\epsilon\\epsilon^{\\top}]U = U^{\\top}(\\sigma^2 I_m)U = \\sigma^2 I_m$。因此，分量 $\\tilde{\\epsilon}_i$ 是不相关的，方差为 $\\sigma^2$。\n\n在 SVD 基中，模型分解为一组标量方程：\n$$\\tilde{y}_i = s_i x_i + \\tilde{\\epsilon}_i \\quad \\text{对于 } i=1, \\ldots, n$$\n$$\\tilde{y}_i = \\tilde{\\epsilon}_i \\quad \\text{对于 } i=n+1, \\ldots, m$$\n平方估计误差在 $V$ 的正交变换下是不变的，即 $\\|\\hat{x} - x\\|_2^2 = \\|V^{\\top}(\\hat{x} - x)\\|_2^2 = \\|\\hat{\\tilde{x}} - \\tilde{x}\\|_2^2 = \\sum_{i=1}^n (\\hat{x}_i - x_i)^2$。因此，我们可以通过对每个分量的 MSE 求和来计算总 MSE。\n\n单个分量 $x_i$ 的估计器 $\\hat{x}_i$ 的 MSE 由其偏差的平方和方差之和给出：\n$$\\mathbb{E}[(\\hat{x}_i - x_i)^2] = \\left(\\mathbb{E}[\\hat{x}_i] - x_i\\right)^2 + \\mathbb{E}\\left[(\\hat{x}_i - \\mathbb{E}[\\hat{x}_i])^2\\right]$$\n\n在此背景下，大多数线性估计器可以通过应用于朴素解的滤波因子 $f_i$ 来描述。估计的系数由 $\\hat{x}_i = f_i \\frac{\\tilde{y}_i}{s_i} = f_i(x_i + \\frac{\\tilde{\\epsilon}_i}{s_i})$ 给出。那么，分量 MSE 为：\n$$\\mathbb{E}[(\\hat{x}_i - x_i)^2] = \\mathbb{E}\\left[\\left(f_i x_i + f_i \\frac{\\tilde{\\epsilon}_i}{s_i} - x_i\\right)^2\\right] = \\mathbb{E}\\left[\\left((f_i - 1)x_i + f_i \\frac{\\tilde{\\epsilon}_i}{s_i}\\right)^2\\right]$$\n由于 $\\mathbb{E}[\\tilde{\\epsilon}_i] = 0$，交叉项消失，我们得到每个分量 MSE 的通用表达式：\n$$\\mathrm{MSE}_i = \\underbrace{(f_i - 1)^2 x_i^2}_{\\text{偏差平方}} + \\underbrace{\\frac{f_i^2 \\sigma^2}{s_i^2}}_{\\text{方差}}$$\n\n**Tikhonov 正则化**\nTikhonov 估计器最小化 $\\|Ax - y\\|_2^2 + \\lambda \\|x\\|_2^2$。在 SVD 基中，这等价于最小化 $\\sum_{i=1}^n (s_i x_i - \\tilde{y}_i)^2 + \\lambda \\sum_{i=1}^n x_i^2$。这产生了分量解 $\\hat{x}_{i, \\mathrm{Tik}} = \\frac{s_i}{s_i^2 + \\lambda} \\tilde{y}_i$。相应的滤波因子为 $f_i^{\\mathrm{Tik}} = \\frac{s_i^2}{s_i^2 + \\lambda}$。\n\nTikhonov 正则化的每个分量的 MSE 为：\n- 偏差平方：$\\left(\\frac{s_i^2}{s_i^2 + \\lambda} - 1\\right)^2 x_i^2 = \\left(\\frac{-\\lambda}{s_i^2 + \\lambda}\\right)^2 x_i^2 = \\frac{\\lambda^2 x_i^2}{(s_i^2 + \\lambda)^2}$\n- 方差：$\\left(\\frac{s_i^2}{s_i^2 + \\lambda}\\right)^2 \\frac{\\sigma^2}{s_i^2} = \\frac{s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2}$\n\nTikhonov 估计器的总期望平方估计误差是所有分量之和：\n$$\\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{Tik}}(\\lambda) - x\\big\\|_{2}^{2} = \\sum_{i=1}^{n} \\left[ \\frac{\\lambda^2 x_i^2}{(s_i^2 + \\lambda)^2} + \\frac{s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} \\right] = \\sum_{i=1}^{n} \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2}$$\n\n**截断奇异值分解（TSVD）**\n具有截断指数 $k$ 的 TSVD 估计器有效地将前 $k$ 个分量的滤波因子设置为 $1$，其余分量的滤波因子设置为 $0$。\n$$f_i^{\\mathrm{TSVD}} = \\begin{cases} 1  \\text{如果 } i \\le k \\\\ 0  \\text{如果 } i > k \\end{cases}$$\n我们分两部分分析 MSE：\n\n对于 $i \\in \\{1, \\ldots, k\\}$：$f_i = 1$。\n- 偏差平方：$(1 - 1)^2 x_i^2 = 0$。对于这些分量，估计器是无偏的。\n- 方差：$\\frac{1^2 \\sigma^2}{s_i^2} = \\frac{\\sigma^2}{s_i^2}$。方差被放大，特别是对于小的 $s_i$。\n\n对于 $i \\in \\{k+1, \\ldots, n\\}$：$f_i = 0$。\n- 偏差平方：$(0 - 1)^2 x_i^2 = x_i^2$。误差完全是由于将分量截断为零所产生的偏差。\n- 方差：$\\frac{0^2 \\sigma^2}{s_i^2} = 0$。对于被截断的分量，没有噪声传播。\n\nTSVD 估计器的总期望平方估计误差为：\n$$\\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{TSVD}}(k) - x\\big\\|_{2}^{2} = \\sum_{i=1}^{k} \\frac{\\sigma^2}{s_i^2} + \\sum_{i=k+1}^{n} x_i^2$$\n\n**比较与分析**\n\n对于每个奇异分量 $i$，偏差和方差的贡献如下：\n- **对于 $i \\le k$ (包含的分量):**\n  - Tikhonov：引入了偏差 $\\frac{-\\lambda x_i}{s_i^2 + \\lambda}$，但将方差从 $\\sigma^2/s_i^2$ 减小到 $\\frac{s_i^4 \\sigma^2}{(s_i^2 + \\lambda)^2 s_i^2}$。对于这些分量，Tikhonov 的方差总是小于 TSVD。\n  - TSVD：是无偏的，但有较大的方差贡献 $\\sigma^2/s_i^2$。\n  - 对于分量 $i \\le k$，如果其 MSE 小于 Tikhonov 的 MSE，即 $\\frac{\\sigma^2}{s_i^2}  \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2}$，则 TSVD 产生更小的期望误差。这可以简化为 $s_i^2 \\lambda x_i^2  2\\sigma^2 s_i^2 + \\sigma^2 \\lambda$。这种情况对应于具有高信噪比（$x_i^2/\\sigma^2$）的分量，此时 TSVD 的零偏差优势超过了其较高的方差。\n\n- **对于 $i  k$ (滤波/截断的分量):**\n  - Tikhonov：平滑地过滤分量，产生偏差 $\\frac{-\\lambda x_i}{s_i^2 + \\lambda}$ 和方差 $\\frac{s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2}$。其偏差的绝对值小于 TSVD 的偏差，即 $\\left|\\frac{-\\lambda x_i}{s_i^2+\\lambda}\\right|  |x_i|$。\n  - TSVD：突然将分量截断为零，导致较大的偏差 $-x_i$ 和零方差。\n  - 对于分量 $i  k$，如果 $x_i^2  \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2}$，则 TSVD 产生更小的期望误差。这可以简化为 $x_i^2(s_i^2 + 2\\lambda)  \\sigma^2$。这种情况对应于真实信号功率 $x_i^2$ 相对于噪声水平 $\\sigma^2$ 可以忽略不计的分量。在这种情况下，截断为零（TSVD）是比估计一个小的、含噪声的值（Tikhonov）更好的策略。通常，对于具有不可忽略的 $x_i$ 的分量，由于其偏差误差小得多，Tikhonov 更优。\n\n**$\\Delta(\\lambda, k)$ 的推导**\n最后，我们按照要求计算总期望平方误差的差值。\n$$\\Delta(\\lambda,k) = \\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{Tik}}(\\lambda) - x\\big\\|_{2}^{2} - \\mathbb{E}\\big\\|\\hat{x}_{\\mathrm{TSVD}}(k) - x\\big\\|_{2}^{2}$$\n代入推导出的总 MSE 表达式：\n$$\\Delta(\\lambda,k) = \\left( \\sum_{i=1}^{n} \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} \\right) - \\left( \\sum_{i=1}^{k} \\frac{\\sigma^2}{s_i^2} + \\sum_{i=k+1}^{n} x_i^2 \\right)$$\n该表达式可以根据 TSVD 误差的求和范围进行拆分：\n$$\\Delta(\\lambda,k) = \\left( \\sum_{i=1}^{k} \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} + \\sum_{i=k+1}^{n} \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} \\right) - \\left( \\sum_{i=1}^{k} \\frac{\\sigma^2}{s_i^2} + \\sum_{i=k+1}^{n} x_i^2 \\right)$$\n合并相同指数上的求和，得到最终的解析表达式：\n$$\\Delta(\\lambda,k) = \\sum_{i=1}^{k} \\left[ \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} - \\frac{\\sigma^2}{s_i^2} \\right] + \\sum_{i=k+1}^{n} \\left[ \\frac{\\lambda^2 x_i^2 + s_i^2 \\sigma^2}{(s_i^2 + \\lambda)^2} - x_i^2 \\right]$$\n这就是 Tikhonov 和 TSVD 估计器之间期望平方误差差值的所求闭式表达式。",
            "answer": "$$\\boxed{\\left(\\sum_{i=1}^{n} \\frac{\\lambda^{2} x_{i}^{2} + s_{i}^{2} \\sigma^{2}}{(s_{i}^{2} + \\lambda)^{2}}\\right) - \\left(\\sum_{i=1}^{k} \\frac{\\sigma^{2}}{s_{i}^{2}} + \\sum_{i=k+1}^{n} x_{i}^{2}\\right)}$$"
        },
        {
            "introduction": "在理解了正则化背后的理论权衡之后，下一步便是动手实现一个求解器。本练习聚焦于生物医学建模中的一个常见情景：物理约束（例如，浓度或速率的非负性）必须被强制执行。你将通过实现投影梯度下降（Projected Gradient Descent, PGD）算法来解决一个带正则化的反卷积问题，从而获得处理约束优化的实践经验。",
            "id": "3935338",
            "problem": "考虑一个离散时间线性时不变 (LTI) 系统，它通过与已知脉冲响应 $h \\in \\mathbb{R}^L$ 的因果离散卷积，将一个未知的非负输入 $x \\in \\mathbb{R}^n$ 映射到一个测量输出 $y \\in \\mathbb{R}^n$。$x$与$h$的因果离散卷积定义为 $y[k] = \\sum_{i=0}^{L-1} h[i] x[k-i]$，其中 $k \\in \\{0,1,\\dots,n-1\\}$，并约定当 $j  0$ 时 $x[j] = 0$。该卷积可以表示为一个作用于 $x$ 的线性算子 $H \\in \\mathbb{R}^{n \\times n}$，其中 $y = H x$。在生物医学系统建模中，非负性源于物理浓度或生理速率等本质上是非负的量。\n\n建立以下用于反卷积的约束逆问题：给定 $h$ 和 $y$，通过最小化一个正则化最小二乘目标函数来估计 $x \\ge 0$：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\|H x - y\\|_2^2 \\ + \\ \\frac{\\lambda}{2}\\|x\\|_2^2 \\quad \\text{subject to } x \\ge 0,\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数。实现投影梯度下降 (PGD) 算法，该算法使用到非负象限的欧几里得投影，从 $x^{(0)} = 0$ 开始，并使用一个为满足 Lipschitz 梯度界限而选择的固定步长 $\\alpha$。您必须确定梯度 Lipschitz 常数的一个有效界限，并用它来选择 $\\alpha$。证明当从一个可行点开始时，由 PGD 生成的迭代点保持可行性，即对于所有 $t$，$x^{(t)} \\ge 0$。\n\n你的程序必须：\n- 根据给定的脉冲响应 $h$ 和维度 $n$ 构建因果卷积矩阵 $H$，其中 $y = H x$ 的长度被截断为 $n$。\n- 推导并实现目标函数的梯度。\n- 估计与目标函数光滑部分相关联的梯度 Lipschitz 常数 $L$，并选择 $\\alpha = 1/L$。一种有效的方法是通过幂迭代法估计 $H^\\top H$ 的最大特征值，然后在界限中计入 $\\lambda$。\n- 实现形式为 $x^{(t+1)} = \\Pi_{\\mathbb{R}_+^n}\\big(x^{(t)} - \\alpha \\nabla f(x^{(t)})\\big)$ 的 PGD 更新，其中 $\\Pi_{\\mathbb{R}_+^n}$ 表示到非负象限的欧几里得投影。\n- 对于下面指定的每个测试用例，运行 PGD 达到规定的迭代次数，并计算最终估计值 $\\hat{x}$ 与已知真实值 $x_{\\text{true}}$ 之间的均方根误差 (RMSE)，其定义为 $\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{k=0}^{n-1}(\\hat{x}[k] - x_{\\text{true}}[k])^2}$。\n\n使用以下具有明确参数的测试套件：\n- 情况 1 (正常路径，稀疏非负输入，指数衰减脉冲响应)：\n  - 维度 $n = 64$。\n  - 脉冲响应长度 $L = 15$，其条目为 $h[i] = \\exp\\!\\big(-\\frac{i}{8}\\big)$，对于 $i \\in \\{0,\\dots,14\\}$。\n  - 真实输入 $x_{\\text{true}} \\in \\mathbb{R}^{64}$ 定义为 $x_{\\text{true}}[5]=1.0$，$x_{\\text{true}}[20]=0.6$，$x_{\\text{true}}[40]=1.2$，$x_{\\text{true}}[55]=0.8$，其他情况下 $x_{\\text{true}}[k]=0$。\n  - 正则化参数 $\\lambda = 10^{-2}$。\n  - 观测值 $y = H x_{\\text{true}}$。\n  - 迭代次数 $T = 500$。\n- 情况 2 (边界条件，零信号)：\n  - 维度 $n = 64$。\n  - 脉冲响应与情况 1 相同。\n  - 真实输入 $x_{\\text{true}}[k] = 0$，对于所有 $k \\in \\{0,\\dots,63\\}$。\n  - 正则化参数 $\\lambda = 10^{-2}$。\n  - 观测值 $y = H x_{\\text{true}}$ (零向量)。\n  - 迭代次数 $T = 100$。\n- 情况 3 (病态模糊，阶跃输入，更强正则化)：\n  - 维度 $n = 64$。\n  - 脉冲响应长度 $L = 25$，其条目为 $h[i] = \\exp\\!\\big(-\\frac{1}{2}\\big(\\frac{i}{5}\\big)^2\\big)$，对于 $i \\in \\{0,\\dots,24\\}$，然后进行归一化，使得 $\\sum_{i=0}^{24} h[i] = 1$。\n  - 真实输入 $x_{\\text{true}} \\in \\mathbb{R}^{64}$ 定义为 $x_{\\text{true}}[k]=0.5$ 对于 $k \\in \\{0,\\dots,31\\}$ 且 $x_{\\text{true}}[k]=1.0$ 对于 $k \\in \\{32,\\dots,63\\}$。\n  - 正则化参数 $\\lambda = 10^{-1}$。\n  - 观测值 $y = H x_{\\text{true}}$。\n  - 迭代次数 $T = 800$。\n\n您的程序应生成单行输出，其中包含情况 1、2 和 3 的 RMSE 值，格式为方括号内以逗号分隔的列表（例如，$[r_1,r_2,r_3]$）。所有输出都应是无单位的实数。不应读取任何外部输入，并且计算中不允许存在随机性。",
            "solution": "用户在生物医学系统建模领域提供了一个定义明确的问题，特别关注系统辨识和逆问题。任务是使用投影梯度下降法解决一个约束反卷积问题。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n\n*   **系统模型**：一个离散时间线性时不变 (LTI) 系统，由 $y = Hx$ 描述，其中 $x \\in \\mathbb{R}^n$ 是一个非负输入 ($x \\ge 0$)，$y \\in \\mathbb{R}^n$ 是测量输出，$H \\in \\mathbb{R}^{n \\times n}$ 是从已知脉冲响应 $h \\in \\mathbb{R}^L$ 导出的因果卷积矩阵。卷积定义为 $y[k] = \\sum_{i=0}^{L-1} h[i] x[k-i]$，其中 $k \\in \\{0, \\dots, n-1\\}$，且 $j  0$ 时 $x[j]=0$。\n*   **优化问题**：\n    $$ \\min_{x \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\|H x - y\\|_2^2 \\ + \\ \\frac{\\lambda}{2}\\|x\\|_2^2 \\quad \\text{subject to } x \\ge 0 $$\n*   **算法**：投影梯度下降 (PGD)，初始猜测为 $x^{(0)} = 0$。\n*   **步长**：固定步长 $\\alpha = 1/L_f$，其中 $L_f$ 是目标函数光滑部分梯度的 Lipschitz 常数。$L_f$ 将通过幂迭代法估计 $H^\\top H$ 的最大特征值来确定。\n*   **投影**：到非负象限的欧几里得投影 $\\Pi_{\\mathbb{R}_+^n}$。\n*   **证明要求**：证明迭代点 $x^{(t)}$ 对于所有 $t \\ge 0$ 保持可行。\n*   **输出**：对于每个测试用例，计算最终估计值 $\\hat{x}$ 与真实值 $x_{\\text{true}}$ 之间的均方根误差 (RMSE)。\n*   **测试用例**：\n    *   **情况 1**：$n=64$，$L=15$，$h[i] = \\exp(-i/8)$，$x_{\\text{true}}$ 是稀疏且非负的，$\\lambda=10^{-2}$，$T=500$ 次迭代。\n    *   **情况 2**：$n=64$，$L=15$，$h[i] = \\exp(-i/8)$，$x_{\\text{true}}=0$，$\\lambda=10^{-2}$，$T=100$ 次迭代。\n    *   **情况 3**：$n=64$，$L=25$，$h[i] = \\exp(-\\frac{1}{2}(i/5)^2)$ 归一化使其和为 $1$，$x_{\\text{true}}$ 是一个阶跃函数，$\\lambda=10^{-1}$，$T=800$ 次迭代。\n\n**第 2 步：使用提取的已知条件进行验证**\n\n该问题定义严格，满足所有有效性标准。\n\n*   **科学基础**：该问题建立在线性代数（矩阵运算、特征值）、信号处理（卷积、反卷积）和优化（正则化最小二乘、梯度下降）的基本原理之上。这些都是标准且成熟的理论。\n*   **适定性**：目标函数是一个凸最小二乘项和一个严格凸的二次正则化项（因为所有测试用例中 $\\lambda  0$）的和。所得函数是严格凸的。在一个闭合凸集（非负象限）上最小化一个严格凸函数存在唯一解。\n*   **客观性**：问题以精确的数学语言陈述，没有任何主观性或模糊性。\n*   **完整性和一致性**：所有必要的参数，包括系统维度、模型定义、算法规范和测试数据，都已明确提供。没有矛盾。\n*   **可行性**：指定的模型和参数对于计算信号处理任务是现实的。\n*   **结构**：问题结构良好，引导解决者按逻辑顺序完成任务：模型构建、算法选择、参数推导（步长）和实现。\n\n**第 3 步：结论与行动**\n\n问题有效。将提供完整解决方案。\n\n### 解决方案\n\n该问题要求解决一个非负约束的正则化反卷积问题。我们将使用投影梯度下降 (PGD) 算法。解决方案涉及推导必要的数学组件、证明一个所需的性质以及实现该算法。\n\n**1. 数学公式**\n\n目标是通过解决以下凸优化问题来找到真实输入 $x_{\\text{true}}$ 的估计值 $\\hat{x}$：\n$$\n\\min_{x \\in \\mathbb{R}^n} f(x) \\quad \\text{subject to } x \\ge 0\n$$\n其中 $f(x) = \\frac{1}{2}\\|H x - y\\|_2^2 + \\frac{\\lambda}{2}\\|x\\|_2^2$。矩阵 $H$ 是表示因果卷积的下三角托普利茨矩阵。其元素由 $H_{ij} = h[i-j]$（如果 $0 \\le i-j  L$）和 $H_{ij}=0$（其他情况）给出。\n\n**2. 投影梯度下降 (PGD) 算法**\n\nPGD 是一种用于形如 $\\min_{x \\in \\mathcal{C}} f(x)$ 的约束优化问题的迭代算法，其中 $f$ 是一个光滑函数，$\\mathcal{C}$ 是一个闭合凸集。该迭代是一个两步过程：一个标准的梯度下降步，后跟一个到可行集 $\\mathcal{C}$ 的投影。\n\n更新规则是：\n$$\nx^{(t+1)} = \\Pi_{\\mathcal{C}}\\left(x^{(t)} - \\alpha \\nabla f(x^{(t)})\\right)\n$$\n在我们的问题中，可行集是非负象限 $\\mathcal{C} = \\mathbb{R}_+^n = \\{x \\in \\mathbb{R}^n \\mid x_k \\ge 0 \\text{ for all } k\\}$。投影 $\\Pi_{\\mathbb{R}_+^n}(z)$ 是逐元素计算的，即 $(\\Pi_{\\mathbb{R}_+^n}(z))_k = \\max(0, z_k)$。\n\n**3. 目标函数的梯度**\n\n目标函数是 $f(x) = \\frac{1}{2}(Hx-y)^T(Hx-y) + \\frac{\\lambda}{2}x^T x$。我们计算其关于 x 的梯度：\n$$\n\\nabla f(x) = \\nabla_x \\left( \\frac{1}{2} (x^T H^T H x - 2y^T H x + y^T y) + \\frac{\\lambda}{2} x^T x \\right)\n$$\n使用标准矩阵微积分恒等式，我们得到：\n$$\n\\nabla f(x) = H^T H x - H^T y + \\lambda x = H^T(Hx - y) + \\lambda x\n$$\n\n**4. 步长选择**\n\n如果步长 $\\alpha$ 满足 $0  \\alpha  2/L_f$，梯度下降的收敛性得到保证，其中 $L_f$ 是梯度 $\\nabla f(x)$ 的 Lipschitz 常数。一个常用且安全的选择是 $\\alpha = 1/L_f$。Lipschitz 常数是 $f(x)$ 的 Hessian 矩阵的最大特征值（在量级上）。\n\n$f(x)$ 的 Hessian 矩阵是：\n$$\n\\nabla^2 f(x) = \\frac{d}{dx} \\nabla f(x) = \\frac{d}{dx} \\left( H^T H x - H^T y + \\lambda x \\right) = H^T H + \\lambda I\n$$\nHessian 矩阵是常数。Lipschitz 常数 $L_f$ 是其谱范数，由于该矩阵是对称正定的（对于 $\\lambda > 0$），因此也就是其最大特征值。\n$$\nL_f = \\|\\nabla^2 f(x)\\|_2 = \\|H^T H + \\lambda I\\|_2 = \\lambda_{\\max}(H^T H + \\lambda I)\n$$\n使用性质 $\\lambda_{\\max}(A + cI) = \\lambda_{\\max}(A) + c$，我们发现：\n$$\nL_f = \\lambda_{\\max}(H^T H) + \\lambda\n$$\n项 $\\lambda_{\\max}(H^T H)$ 是 $H$ 的谱范数的平方，即 $\\|H\\|_2^2$。我们使用应用于对称矩阵 $A = H^T H$ 的幂迭代法来估计这个值。从一个非零向量 $b_0$ 开始，该方法迭代 $b_{k+1} = \\frac{A b_k}{\\|A b_k\\|_2}$。经过足够次数的迭代后，最大特征值由瑞利商 $\\lambda_{\\max}(A) \\approx b_k^T A b_k$ 近似。\n\n然后将步长设置为 $\\alpha = \\frac{1}{L_f} = \\frac{1}{\\lambda_{\\max}(H^T H) + \\lambda}$。\n\n**5. PGD 迭代点可行性的证明**\n\n我们必须证明，如果算法从一个可行点 $x^{(0)} \\ge 0$ 开始，那么所有后续的迭代点 $x^{(t)}$ 都保持可行，即对于所有 $t \\ge 0$，$x^{(t)} \\ge 0$。\n\n*   **基本情况**：算法初始化为 $x^{(0)} = 0$。由于所有分量均为 $0$，所以 $x^{(0)} \\ge 0$ 成立，初始点是可行的。\n*   **归纳步骤**：假设在第 $t$ 次迭代时，迭代点 $x^{(t)}$ 是可行的，即 $x^{(t)} \\ge 0$。下一个迭代点计算为 $x^{(t+1)} = \\Pi_{\\mathbb{R}_+^n}(z^{(t)})$，其中 $z^{(t)} = x^{(t)} - \\alpha \\nabla f(x^{(t)})$。向量 $z^{(t)}$ 的分量可以是任意符号。投影算子 $\\Pi_{\\mathbb{R}_+^n}$ 的定义使得对于任何向量 $z \\in \\mathbb{R}^n$，其输出 $p = \\Pi_{\\mathbb{R}_+^n}(z)$ 的分量为 $p_k = \\max(0, z_k)$。根据定义，对于所有 $k$，$p_k \\ge 0$。因此，由投影产生的向量 $x^{(t+1)}$ 必然位于非负象限内，满足 $x^{(t+1)} \\ge 0$。\n\n根据数学归纳法，由 PGD 算法生成的所有迭代点都保持在可行集 $\\mathbb{R}_+^n$ 中。\n\n**6. 实现摘要**\n\n对于每个测试用例，执行以下步骤：\n1.  定义参数 $n, L, \\lambda, T$ 以及向量 $h$ 和 $x_{\\text{true}}$。\n2.  根据 $h$ 构建 $n \\times n$ 的因果卷积矩阵 $H$（使用托普利茨结构）。\n3.  合成观测向量 $y = H x_{\\text{true}}$。\n4.  使用幂迭代法估计 $H^T H$ 的最大特征值，以找到 $\\lambda_{\\max}(H^T H)$。\n5.  计算 Lipschitz 常数 $L_f = \\lambda_{\\max}(H^T H) + \\lambda$ 和步长 $\\alpha = 1/L_f$。\n6.  初始化解的估计值 $x = 0$。\n7.  使用 PGD 更新规则迭代 $T$ 次：$x \\leftarrow \\max(0, x - \\alpha (H^T(Hx-y) + \\lambda x))$。\n8.  计算最终估计值 $\\hat{x}$ 与 $x_{\\text{true}}$ 之间的 RMSE。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef solve():\n    \"\"\"\n    Solves the constrained deconvolution problem for three test cases using\n    Projected Gradient Descent (PGD) and prints the resulting RMSE values.\n    \"\"\"\n\n    def construct_convolution_matrix(h, n):\n        \"\"\"Constructs the n x n causal convolution matrix H from impulse response h.\"\"\"\n        L = len(h)\n        col = np.zeros(n)\n        col[:L] = h\n        row = np.zeros(n)\n        row[0] = h[0]\n        return toeplitz(col, row)\n\n    def estimate_lipschitz_constant(H, lam):\n        \"\"\"\n        Estimates the Lipschitz constant of the gradient of the smooth objective\n        f(x) = 0.5*||Hx-y||^2 + 0.5*lambda*||x||^2.\n        The gradient is grad(f) = H.T @ (H @ x - y) + lam * x.\n        The Hessian is H.T @ H + lam * I.\n        The Lipschitz constant is the max eigenvalue of the Hessian,\n        which is lambda_max(H.T @ H) + lam.\n        lambda_max(H.T @ H) is estimated via power iteration.\n        \"\"\"\n        n = H.shape[0]\n        # Use a deterministic starting vector for reproducibility\n        b_k = np.ones(n)\n        num_iter_power = 100\n\n        for _ in range(num_iter_power):\n            # Calculate the matrix-vector product A*b_k where A = H.T @ H\n            b_k1 = H.T @ (H @ b_k)\n            # Calculate the norm\n            b_k1_norm = np.linalg.norm(b_k1)\n            # Re-normalize the vector\n            if b_k1_norm == 0:\n                b_k = np.zeros(n)\n                break\n            b_k = b_k1 / b_k1_norm\n\n        # The dominant eigenvalue is the Rayleigh quotient b_k.T * A * b_k\n        # Since b_k is unit-norm, this is b_k.T @ (H.T @ H @ b_k)\n        lambda_max_HTH = b_k.T @ (H.T @ (H @ b_k))\n        \n        return lambda_max_HTH + lam\n\n    def run_pgd(n, h, x_true, lam, T):\n        \"\"\"\n        Runs the Projected Gradient Descent algorithm for a single test case.\n        \"\"\"\n        # 1. Construct H and generate y\n        H = construct_convolution_matrix(h, n)\n        y = H @ x_true\n        \n        # 2. Determine step size alpha\n        L_f = estimate_lipschitz_constant(H, lam)\n        alpha = 1.0 / L_f\n\n        # 3. PGD iterations\n        x_hat = np.zeros(n) # Initialize with x^(0) = 0\n        \n        for _ in range(T):\n            # Gradient calculation\n            grad = H.T @ (H @ x_hat - y) + lam * x_hat\n            # Gradient step\n            x_update = x_hat - alpha * grad\n            # Projection step\n            x_hat = np.maximum(0, x_update)\n            \n        # 4. Compute RMSE\n        rmse = np.sqrt(np.mean((x_hat - x_true)**2))\n        return rmse\n\n    # Define test cases\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"n\": 64,\n            \"L\": 15,\n            \"h_func\": lambda i: np.exp(-i / 8.0),\n            \"x_true_def\": {5: 1.0, 20: 0.6, 40: 1.2, 55: 0.8},\n            \"lam\": 1e-2,\n            \"T\": 500,\n            \"h_norm\": False\n        },\n        {\n            \"name\": \"Case 2\",\n            \"n\": 64,\n            \"L\": 15,\n            \"h_func\": lambda i: np.exp(-i / 8.0),\n            \"x_true_def\": {},\n            \"lam\": 1e-2,\n            \"T\": 100,\n            \"h_norm\": False\n        },\n        {\n            \"name\": \"Case 3\",\n            \"n\": 64,\n            \"L\": 25,\n            \"h_func\": lambda i: np.exp(-0.5 * (i / 5.0)**2),\n            \"x_true_def\": {\"range1\": (0, 32, 0.5), \"range2\": (32, 64, 1.0)},\n            \"lam\": 1e-1,\n            \"T\": 800,\n            \"h_norm\": True\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        L = case[\"L\"]\n        \n        # Generate impulse response h\n        h = np.array([case[\"h_func\"](i) for i in range(L)])\n        if case[\"h_norm\"]:\n            h /= np.sum(h)\n            \n        # Generate ground-truth input x_true\n        x_true = np.zeros(n)\n        for key, value in case[\"x_true_def\"].items():\n            if isinstance(key, int):\n                x_true[key] = value\n            elif isinstance(key, str) and key.startswith(\"range\"):\n                start, end, val = value\n                x_true[start:end] = val\n                \n        # Run PGD and store result\n        rmse = run_pgd(n, h, x_true, case[\"lam\"], case[\"T\"])\n        results.append(rmse)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "当一个模型被辨识出来后，一个关键的最后步骤是验证其性能。本练习将介绍残差分析，这是一种用于检测模型设定错误的强大诊断工具。通过实现Ljung-Box检验等统计测试，你将学会如何判断所辨识的模型是否成功捕捉了系统的内在动态，或者其误差中是否仍存在未被解释的结构。",
            "id": "3935335",
            "problem": "给定一项在离散时间系统辨识的模型验证背景下的心血管残差时间序列分析任务。假设一个线性、因果、离散时间模型被拟合到一个测量的心血管信号（例如，以毫米汞柱为单位的动脉压），并将残差序列表示为 $e_t = y_t - \\hat{y}_t$，其中 $y_t$ 是测量输出，而 $\\hat{y}_t$ 是在离散时间 $t$ 的单步向前预测。在线性系统的正确模型设定和标准假设下，残差应该是一个离散时间、宽义平稳、零均值、具有恒定方差的独立白噪声过程的实现。您的任务是实现基于基本定义和推断性检验的残差诊断，以检测与此理想情况的偏离。\n\n从以下基本基础开始：\n- 残差序列 $e_t$ 定义为测量输出与模型预测之差，$e_t = y_t - \\hat{y}_t$。\n- 在模型设定正确的零假设下，残差构成一个零均值、独立同分布的序列，具有恒定方差。平稳性意味着自协方差 $\\gamma_k = \\mathbb{E}[(e_t - \\mu)(e_{t-k} - \\mu)]$ 仅取决于滞后 $k$，其中 $\\mu = \\mathbb{E}[e_t]$。\n- 自相关函数定义为 $\\rho_k = \\gamma_k/\\gamma_0$（对于滞后 $k$），其中 $\\gamma_0$ 是方差。\n- Ljung-Box 综合统计量聚合了跨多个滞后的信息，以检验联合假设 $\\rho_1 = \\rho_2 = \\dots = \\rho_h = 0$。\n\n实现一个程序，对测试套件中的每个残差时间序列执行以下操作：\n1. 使用基于 $e_t$ 的样本均值和样本方差的规范估计量，计算直至指定最大滞后 $h$ 的样本自相关函数。\n2. 在残差是白噪声的零假设下，计算最大滞后 $h$ 处的 Ljung-Box 综合统计量及其相关联的 $p$-值（使用大样本 $\\chi^2$ 近似）。\n3. 通过结合两种诊断方法来判断残差是否指示了模型设定错误：\n   - 基于 Ljung-Box $p$-值与显著性水平 $\\alpha$ 比较的联合检验决策。\n   - 基于任何样本自相关幅值是否超过其从标准正态近似导出的大样本置信界限的边际诊断。\n4. 对每个测试用例，输出一个三元组，包含 Ljung-Box 统计量、相关联的 $p$-值和一个布尔型检测决策，其中如果检测到设定错误，则布尔值为真，否则为假。所有报告的值都是无量纲的。\n\n使用以下科学上合理、自洽的残差生成测试套件，该套件模拟了不同的心血管建模情境。在每种情况下，残差均以毫米汞柱为单位指定，但诊断是无量纲的。设显著性水平为 $\\alpha = 0.05$，并定义最大滞后为 $h = \\min(20, \\lfloor n/4 \\rfloor)$，其中 $n$ 是残差序列的样本大小。对于近似的单滞后置信界限，使用双侧标准正态分位数 $z_{0.975}$。\n\n测试套件（每个案例提供生成残差所需的所有参数）：\n- 案例 1（设定良好，独立噪声）：$n = 512$，残差是独立的零均值高斯噪声，标准差 $\\sigma = 5$ $\\mathrm{mmHg}$，随机种子 $12345$。\n- 案例 2（通过一阶自回归导致的设定错误）：$n = 512$，残差遵循自回归过程 $e_t = \\phi e_{t-1} + \\epsilon_t$，其中 $\\phi = 0.6$，$\\epsilon_t$ 是独立的零均值高斯噪声，标准差 $\\sigma = 5$ $\\mathrm{mmHg}$，随机种子 $54321$。\n- 案例 3（通过振荡分量导致的设定错误，例如呼吸性窦性心律失常）：$n = 500$，采样频率 $f_s = 10$ $\\mathrm{Hz}$，残差 $e_t = A \\sin(2\\pi f_0 t/f_s) + \\epsilon_t$，振幅 $A = 3$ $\\mathrm{mmHg}$，频率 $f_0 = 0.25$ $\\mathrm{Hz}$，$\\epsilon_t$ 是独立的零均值高斯噪声，标准差 $\\sigma = 2$ $\\mathrm{mmHg}$，随机种子 $24680$。\n- 案例 4（小样本设定错误）：$n = 40$，自回归残差 $e_t = \\phi e_{t-1} + \\epsilon_t$，其中 $\\phi = 0.4$，$\\epsilon_t$ 是独立的零均值高斯噪声，标准差 $\\sigma = 5$ $\\mathrm{mmHg}$，随机种子 $11111$。\n- 案例 5（临界弱相关）：$n = 200$，自回归残差 $e_t = \\phi e_{t-1} + \\epsilon_t$，其中 $\\phi = 0.2$，$\\epsilon_t$ 是独立的零均值高斯噪声，标准差 $\\sigma = 5$ $\\mathrm{mmHg}$，随机种子 $22222$。\n\n对于每个测试案例，计算：\n- 样本自相关序列 $\\{r_k\\}_{k=1}^h$。\n- 滞后 $h$ 处的 Ljung-Box 统计量及其在零假设下的 $p$-值。\n- 一个检测决策，如果 Ljung-Box $p$-值小于 $\\alpha$ 或任何 $|r_k|$ 超过近似大样本界限 $z_{0.975}/\\sqrt{n}$，则为真，否则为假。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，每个元素对应一个测试用例，并且本身是按 $[Q, p, \\text{decision}]$ 顺序排列的三个值的列表。例如，输出格式必须类似于 $[[Q_1,p_1,\\mathrm{decision}_1],[Q_2,p_2,\\mathrm{decision}_2],\\dots]$，所有数值以标准十进制表示法打印，布尔值表示为 true 或 false。",
            "solution": "该问题陈述已经过严格验证，并被认为是有效的。它在科学上基于时间序列分析和系统辨识的原理，提法明确，提供了所有必要信息，并且没有歧义或矛盾。\n\n任务是针对五个不同的时间序列进行残差诊断，以评估一个假设的底层模型是否存在设定错误。核心原则是，如果一个线性模型被正确设定，其残差 $e_t = y_t - \\hat{y}_t$ 应构成一个零均值白噪声过程。对这种理想情况的偏离，如显著的自相关，表明模型未能捕捉系统的全部动态。我们将实施两种标准的统计检验来检测这种偏离：用于联合自相关的 Ljung-Box 综合检验和针对单个自相关系数的边际检验。\n\n首先，我们按规定生成五个残差序列。对于每个长度为 $n$ 的序列，分析的最大滞后设置为 $h = \\min(20, \\lfloor n/4 \\rfloor)$。\n\n-   **案例 1（设定良好，独立噪声）**：$n=512, h=20$。残差 $e_t$ 是一个由 512 个来自正态分布 $\\mathcal{N}(0, 5^2)$ 的独立同分布 (i.i.d.) 随机变量组成的序列。这代表了模型被正确设定的零假设 ($H_0$) 情景。\n-   **案例 2（通过自回归导致的设定错误）**：$n=512, h=20$。残差遵循一阶自回归 (AR(1)) 过程 $e_t = 0.6 e_{t-1} + \\epsilon_t$，其中 $\\epsilon_t \\sim \\mathcal{N}(0, 5^2)$。非零系数 $\\phi=0.6$ 引入了序列相关性，这应该被检测为模型设定错误。\n-   **案例 3（通过振荡分量导致的设定错误）**：$n=500, h=20$。残差由 $e_t = 3 \\sin(2\\pi (0.25) t/10) + \\epsilon_t$ 生成，其中 $\\epsilon_t \\sim \\mathcal{N}(0, 2^2)$。这模拟了一个未建模的周期性分量，例如影响心血管信号的呼吸性窦性心律失常，它将在自相关函数中表现为周期性模式。\n-   **案例 4（小样本设定错误）**：$n=40, h=10$。这是另一个 AR(1) 过程，$e_t = 0.4 e_{t-1} + \\epsilon_t$，但样本量非常小。此案例测试了在不理想的小样本条件下诊断方法的功效。\n-   **案例 5（临界弱相关）**：$n=200, h=20$。这个 AR(1) 案例，$e_t = 0.2 e_{t-1} + \\epsilon_t$，具有一个弱相关系数 $\\phi=0.2$。它测试了诊断方法对微妙模型设定错误的灵敏度。\n\n对于每个生成的序列 $\\{e_t\\}_{t=1}^n$，我们执行以下计算：\n\n1.  **样本自相关函数 (ACF)**：分析的基础是样本 ACF，表示为 $\\{r_k\\}$。它是真实自相关函数 $\\rho_k = \\mathrm{Corr}(e_t, e_{t-k})$ 的一个估计。计算过程如下：\n    -   计算样本均值：$\\bar{e} = \\frac{1}{n} \\sum_{t=1}^{n} e_t$。\n    -   计算滞后 $k$ 的样本自协方差：$\\hat{\\gamma}_k = \\frac{1}{n} \\sum_{t=k+1}^{n} (e_t - \\bar{e})(e_{t-k} - \\bar{e})$，对于 $k = 0, 1, \\dots, h$。$\\hat{\\gamma}_0$ 项是样本方差。\n    -   计算滞后 $k$ 的样本自相关：$r_k = \\frac{\\hat{\\gamma}_k}{\\hat{\\gamma}_0}$。\n\n2.  **边际显著性检验**：在残差是白噪声的零假设下，对于滞后 $k>0$ 的样本自相关 $r_k$ 近似服从均值为 $0$、方差为 $1/n$ 的正态分布。因此，我们可以单独检验每个 $r_k$ 的显著性。在显著性水平 $\\alpha=0.05$ 下的双侧检验，如果 $|r_k|$ 超过一个临界值，则拒绝 $H_0$。该值源自标准正态分布分位数 $z_{1-\\alpha/2}$。对于 $\\alpha=0.05$，临界值为 $z_{0.975} \\approx 1.96$。如果对于任何 $k \\in \\{1, \\dots, h\\}$，以下条件成立，则标记为设定错误：\n    $$ |r_k|  \\frac{z_{0.975}}{\\sqrt{n}} $$\n\n3.  **Ljung-Box 综合检验**：该检验对直至滞后 $h$ 的自相关提供单一的联合评估。零假设为 $H_0: \\rho_1 = \\rho_2 = \\dots = \\rho_h = 0$。Ljung-Box 统计量 $Q$ 定义为：\n    $$ Q = n(n+2) \\sum_{k=1}^{h} \\frac{r_k^2}{n-k} $$\n    在 $H_0$ 下，$Q$ 近似服从自由度为 $h$ 的卡方分布，记为 $\\chi^2(h)$。$p$-值是观测到至少与计算出的 $Q$ 一样极端的统计量的概率，即 $p = P(\\chi^2(h) \\geq Q)$。一个小的 $p$-值（小于显著性水平 $\\alpha=0.05$）提供了反对零假设的证据，表明至少有一个 $\\rho_k$ 非零。\n\n4.  **综合决策**：关于模型设定错误的最终决策是一个布尔值（`true`/`false`）。如果 Ljung-Box 检验显著或至少有一个单独的自相关系数显著，则检测到设定错误。也就是说，如果满足以下条件，则决策为 `true`：\n    $$ (p\\text{-值}  0.05) \\quad \\lor \\quad (\\exists k \\in \\{1, \\dots, h\\} \\text{ s.t. } |r_k|  z_{0.975}/\\sqrt{n}) $$\n    否则，决策为 `false`。\n\n此程序被系统地应用于五个测试案例中的每一个，并报告得到的 Ljung-Box 统计量 ($Q$)、其 $p$-值和最终的布尔决策。该实现使用 `numpy` 进行数值运算和时间序列生成，并使用 `scipy.stats` 访问计算 $p$-值和临界值所需的 $\\chi^2$ 分布和正态分布的累积分布函数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef generate_residuals(case_params):\n    \"\"\"Generates a residual time series based on the specified case parameters.\"\"\"\n    n, case_type, params, seed = case_params\n    rng = np.random.default_rng(seed)\n\n    if case_type == 'iid_gaussian':\n        sigma = params['sigma']\n        e = rng.normal(loc=0.0, scale=sigma, size=n)\n    elif case_type == 'ar1':\n        phi, sigma = params['phi'], params['sigma']\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n        e = np.zeros(n)\n        e[0] = epsilon[0]\n        for t in range(1, n):\n            e[t] = phi * e[t-1] + epsilon[t]\n    elif case_type == 'sinusoid':\n        A, f0, fs, sigma = params['A'], params['f0'], params['fs'], params['sigma']\n        t_vec = np.arange(n)\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n        e = A * np.sin(2 * np.pi * f0 * t_vec / fs) + epsilon\n    \n    return e\n\ndef perform_residual_diagnostics(e, alpha=0.05):\n    \"\"\"\n    Performs residual diagnostics on a time series e.\n    Returns the Ljung-Box statistic, its p-value, and a boolean decision.\n    \"\"\"\n    n = len(e)\n    h = min(20, n // 4)\n\n    # Compute sample mean\n    mean_e = np.mean(e)\n\n    # Compute sample autocovariances and autocorrelations\n    # Use a direct, formula-based implementation for clarity\n    centered_e = e - mean_e\n    acov = np.zeros(h + 1)\n    for k in range(h + 1):\n        acov[k] = np.dot(centered_e[k:], centered_e[:n-k]) / n\n    \n    # Avoid division by zero if variance is zero (highly unlikely for these cases)\n    if acov[0] == 0:\n        return [np.nan, np.nan, True]\n\n    acorr = acov / acov[0]\n    r_k = acorr[1:] # We need r_1, ..., r_h\n\n    # --- Ljung-Box Test ---\n    k_vals = np.arange(1, h + 1)\n    lb_stat = n * (n + 2) * np.sum(r_k**2 / (n - k_vals))\n    \n    # p-value from chi-squared distribution with h degrees of freedom\n    p_value = stats.chi2.sf(lb_stat, h)\n    \n    decision_ljung_box = p_value  alpha\n\n    # --- Individual Lag Test ---\n    z_crit = stats.norm.ppf(1 - alpha / 2)\n    conf_bound = z_crit / np.sqrt(n)\n    decision_individual_lags = np.any(np.abs(r_k) > conf_bound)\n\n    # --- Combined Decision ---\n    final_decision = decision_ljung_box or decision_individual_lags\n    \n    return [lb_stat, p_value, final_decision]\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, type, params, seed)\n        (512, 'iid_gaussian', {'sigma': 5.0}, 12345),\n        (512, 'ar1', {'phi': 0.6, 'sigma': 5.0}, 54321),\n        (500, 'sinusoid', {'A': 3.0, 'f0': 0.25, 'fs': 10.0, 'sigma': 2.0}, 24680),\n        (40, 'ar1', {'phi': 0.4, 'sigma': 5.0}, 11111),\n        (200, 'ar1', {'phi': 0.2, 'sigma': 5.0}, 22222)\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        residuals = generate_residuals(case_params)\n        result_triplet = perform_residual_diagnostics(residuals, alpha=0.05)\n        all_results.append(result_triplet)\n\n    # Format the output as specified in the problem statement\n    formatted_results = []\n    for res in all_results:\n        q_val_str = str(res[0])\n        p_val_str = str(res[1])\n        # The problem requires lowercase boolean strings 'true' or 'false'\n        decision_str = str(res[2]).lower()\n        formatted_results.append(f\"[{q_val_str},{p_val_str},{decision_str}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}