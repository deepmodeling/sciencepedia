{
    "hands_on_practices": [
        {
            "introduction": "At the heart of model validation lies the fundamental task of quantifying the discrepancy between model predictions and observed data. The Root Mean Square Error ($RMSE$) and Mean Absolute Error ($MAE$) are two of the most widely used metrics for this purpose. This exercise provides a hands-on exploration of these metrics, demonstrating how their mathematical properties make them respond differently to various error patterns, particularly outliers which are common in biomedical data. By computing and comparing these metrics across carefully designed scenarios, you will gain a principled understanding of when to choose one over the other, linking your choice to the underlying statistical assumptions about the measurement noise .",
            "id": "3904284",
            "problem": "You are validating a pharmacokinetic concentration model within biomedical systems modeling using principled error metrics. Consider a one-compartment intravenous bolus model prediction for plasma concentration given by the baseline formula $C(t) = C_0 e^{-k t}$, where $C_0$ is the initial concentration and $k$ is the elimination rate constant. The model predictions are to be compared against measured concentrations. Define the residuals as $r_i = y_i - \\hat{y}_i$, where $\\hat{y}_i$ is the predicted concentration and $y_i$ is the measured concentration at time $t_i$. Use the standard definitions of root mean square error and mean absolute error to quantify discrepancy. Express all concentration-related quantities in milligrams per liter (mg/L).\n\nStarting from the standard measurement model $y_i = \\hat{y}_i + \\varepsilon_i$ with independent residuals $\\varepsilon_i$, your program must compute, for each test case:\n- The root mean square error (RMSE) in mg/L across the provided time points.\n- The mean absolute error (MAE) in mg/L across the provided time points.\n- The ratio $R$ of sensitivity defined as $R = \\text{RMSE} / \\text{MAE}$, reported as a floating-point number. If $\\text{MAE} = 0$, set $R = 0.0$.\n- An outlier influence indicator interpreted as a boolean computed as follows. Define the robust scale estimate $s_{\\text{MAD}} = \\text{median}(|r_i - \\text{median}(r)|)$ and let $I = \\max_i |r_i| / (s_{\\text{MAD}} + \\epsilon)$ with $\\epsilon = 10^{-12}$. Classify “heavy outliers present” as true if $I > 6$ and false otherwise.\n- A metric selection code as an integer, justified by statistical modeling of residuals: output $0$ if the recommended validation metric is the mean absolute error (MAE) due to evidence of heavy tails or outliers; output $1$ if the recommended validation metric is the root mean square error (RMSE) under conditions consistent with Gaussian residuals. These numeric codes must be produced even though the rationale is based on maximum likelihood consistency and robustness; the rationale will be explained in the solution.\n\nUse the following scientifically realistic test suite. All concentrations are in mg/L and all times are in hours. The parameters for the predictions are $C_0 = 4$ and $k = 0.25$, with time points $t \\in \\{\\,0,1,2,3,4,5,6,7,8,9\\,\\}$. Predictions are computed as $\\hat{y}_i = C_0 e^{-k t_i}$ and measurements are constructed as $y_i = \\hat{y}_i + r_i$ using the specified residuals $r_i$:\n\n- Case A (happy path, small Gaussian-like deviations): residuals $r^{(A)} = [0.05, -0.04, 0.02, -0.03, 0.01, -0.02, 0.00, 0.03, -0.01, 0.02]$.\n- Case B (single large outlier): residuals $r^{(B)} = [0.05, -0.04, 0.02, -0.03, 0.01, 2.50, 0.00, 0.03, -0.01, 0.02]$.\n- Case C (heavy-tailed residual pattern): residuals $r^{(C)} = [0.10, -0.08, 0.06, -0.05, 0.00, 0.00, 0.30, -0.40, 0.50, -0.30]$.\n- Case D (boundary, zero error): residuals $r^{(D)} = [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]$.\n- Case E (edge, single time point, severe deviation): use $t = [2]$, prediction $\\hat{y} = C_0 e^{-k \\cdot 2}$, and residual $r^{(E)} = [4.50]$.\n\nYour program must construct the predictions $\\hat{y}_i$ for each case from the specified $C_0$, $k$, and $t$, add the corresponding residuals to form measurements $y_i$, compute the residuals $r_i$ from the constructed $y_i$ and $\\hat{y}_i$, and then compute the outputs as required above.\n\nFinal output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of five items in the order $[\\,\\text{RMSE}, \\text{MAE}, R, \\text{outlier\\_boolean}, \\text{metric\\_code}\\,]$. Aggregate the case results in the order A, B, C, D, E into one outer list. For example, the overall output format is $[[\\cdots],[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$.",
            "solution": "The problem statement has been critically validated and is deemed sound. It is scientifically grounded, well-posed, objective, and internally consistent. All necessary data, models, and definitions for a unique and meaningful solution are provided. We may therefore proceed with a complete solution.\n\nThe problem asks for the computation of several error metrics to validate a pharmacokinetic model against synthetic measurement data. This is a fundamental task in biomedical systems modeling, where the goal is to quantify the discrepancy between a model's predictions and experimental observations.\n\nThe model under consideration is the one-compartment intravenous (IV) bolus model, which describes the plasma concentration $C(t)$ of a drug at time $t$ following a rapid injection. The governing equation is:\n$$\nC(t) = C_0 e^{-k t}\n$$\nwhere $C_0$ is the initial concentration at $t=0$ and $k$ is the first-order elimination rate constant. For this problem, we are given $C_0 = 4 \\, \\text{mg/L}$ and $k = 0.25 \\, \\text{hr}^{-1}$.\n\nThe model's predictions, denoted $\\hat{y}_i$, are calculated at specific time points $t_i$. The corresponding experimental measurements are denoted $y_i$. The core of model validation lies in analyzing the residuals, $r_i$, defined as the difference between the observed and predicted values:\n$$\nr_i = y_i - \\hat{y}_i\n$$\nThe problem specifies the exact residuals for five different test cases. Although the problem instructs us to first construct predictions $\\hat{y}_i$, then measurements $y_i = \\hat{y}_i + r_i$, and finally re-compute the residuals, this last step simply returns the original given residuals, as $y_i - \\hat{y}_i = (\\hat{y}_i + r_i) - \\hat{y}_i = r_i$. This procedure simply confirms the consistency of the setup. The analysis proceeds based on these residuals.\n\nWe are tasked with computing five quantities for each case:\n\n1.  **Root Mean Square Error (RMSE):** This metric measures the square root of the average of the squared residuals. It is sensitive to large errors (outliers) due to the squaring term.\n    $$\n    \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} r_i^2}\n    $$\n2.  **Mean Absolute Error (MAE):** This metric measures the average of the absolute values of the residuals. It is less sensitive to outliers than RMSE.\n    $$\n    \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |r_i|\n    $$\n3.  **Ratio of Sensitivity ($R$):** Defined as $R = \\text{RMSE} / \\text{MAE}$, this ratio provides insight into the distribution of errors. For a single error, $R=1$. For multiple errors, $R \\ge 1$. A value of $R$ significantly greater than $1$ suggests the presence of large outliers that disproportionately inflate the RMSE. If $\\text{MAE} = 0$, the residuals are all zero, so $\\text{RMSE}$ is also zero, and we define $R=0.0$.\n\n4.  **Outlier Influence Indicator:** This boolean value is determined using a robust statistical approach. First, we compute the Median Absolute Deviation from the median ($s_{\\text{MAD}}$), a robust estimator of statistical dispersion:\n    $$\n    s_{\\text{MAD}} = \\text{median}(|r_i - \\text{median}(r)|)\n    $$\n    Next, we compute an influence score $I$ that compares the maximum absolute residual to this robust scale estimate:\n    $$\n    I = \\frac{\\max_i |r_i|}{s_{\\text{MAD}} + \\epsilon}\n    $$\n    where $\\epsilon=10^{-12}$ is a small constant to prevent division by zero. A large value of $I$ indicates that the largest error is many times greater than the typical error magnitude. The problem sets the threshold for flagging \"heavy outliers present\" as $I > 6$.\n\n5.  **Metric Selection Code:** This integer code ($0$ or $1$) provides a principled recommendation for which error metric (MAE or RMSE) is more appropriate for model validation. The justification stems from the principles of Maximum Likelihood Estimation (MLE). Minimizing the sum of squared errors (and thus RMSE) corresponds to the MLE solution for a model's parameters when the residuals are assumed to follow a Gaussian (normal) distribution. Minimizing the sum of absolute errors (MAE) corresponds to the MLE solution when residuals are assumed to follow a Laplace distribution, which has \"heavier tails\" than the Gaussian distribution and is thus more tolerant of outliers.\n    -   **Code 1 (RMSE):** Recommended when residuals are consistent with a Gaussian distribution (i.e., no significant outliers).\n    -   **Code 0 (MAE):** Recommended when there is evidence of heavy tails or significant outliers, for which the Laplace assumption is more appropriate and MAE is a more robust metric.\n    The decision is based on the outlier influence indicator: if `heavy outliers present` is true, we select code $0$; otherwise, we select code $1$.\n\nWe now apply these calculations to each test case.\n\n**Case A: Small Gaussian-like deviations**\n-   $r^{(A)} = [0.05, -0.04, 0.02, -0.03, 0.01, -0.02, 0.00, 0.03, -0.01, 0.02]$\n-   $\\sum r_i^2 = 0.0073$, $n=10$. $\\text{RMSE} = \\sqrt{0.0073/10} = \\sqrt{0.00073} \\approx 0.0270185$.\n-   $\\sum |r_i| = 0.23$. $\\text{MAE} = 0.23/10 = 0.023$.\n-   $R = 0.0270185 / 0.023 \\approx 1.174718$.\n-   $\\text{median}(r) = 0.005$. $s_{\\text{MAD}} = \\text{median}(\\{|0.045, -0.045, \\dots|\\}) = 0.02$.\n-   $\\max|r_i| = 0.05$. $I = 0.05 / (0.02 + 10^{-12}) = 2.5$.\n-   Outlier Indicator: $I \\le 6$, so `False`.\n-   Metric Code: `1`.\n-   Result: $[0.0270185, 0.023, 1.174718, \\text{False}, 1]$\n\n**Case B: Single large outlier**\n-   $r^{(B)} = [0.05, -0.04, 0.02, -0.03, 0.01, 2.50, 0.00, 0.03, -0.01, 0.02]$\n-   $\\sum r_i^2 = (0.0073 - (-0.02)^2) + 2.5^2 = 6.2569$. $\\text{RMSE} = \\sqrt{6.2569/10} \\approx 0.791005$.\n-   $\\sum |r_i| = (0.23 - |-0.02|) + 2.5 = 2.71$. $\\text{MAE} = 2.71/10 = 0.271$.\n-   $R = 0.791005 / 0.271 \\approx 2.91884$.\n-   $\\text{median}(r) = 0.015$. $s_{\\text{MAD}} = 0.02$.\n-   $\\max|r_i| = 2.50$. $I = 2.50 / (0.02 + 10^{-12}) = 125.0$.\n-   Outlier Indicator: $I > 6$, so `True`.\n-   Metric Code: `0`.\n-   Result: $[0.791005, 0.271, 2.91884, \\text{True}, 0]$\n\n**Case C: Heavy-tailed residual pattern**\n-   $r^{(C)} = [0.10, -0.08, 0.06, -0.05, 0.00, 0.00, 0.30, -0.40, 0.50, -0.30]$\n-   $\\sum r_i^2 = 0.5594$. $\\text{RMSE} = \\sqrt{0.5594/10} \\approx 0.236516$.\n-   $\\sum |r_i| = 1.79$. $\\text{MAE} = 1.79/10 = 0.179$.\n-   $R = 0.236516 / 0.179 \\approx 1.321319$.\n-   $\\text{median}(r) = 0.00$. $s_{\\text{MAD}} = \\text{median}(\\{|r_i|\\}) = \\text{median}([0,0,0.05,0.06,0.08,0.1,0.3,0.3,0.4,0.5]) = (0.08+0.1)/2 = 0.09$.\n-   $\\max|r_i| = 0.50$. $I = 0.50 / (0.09 + 10^{-12}) \\approx 5.555...$.\n-   Outlier Indicator: $I \\le 6$, so `False`.\n-   Metric Code: `1`.\n-   Result: $[0.236516, 0.179, 1.321319, \\text{False}, 1]$\n\n**Case D: Zero error**\n-   $r^{(D)} = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]$\n-   $\\text{RMSE} = 0.0$.\n-   $\\text{MAE} = 0.0$.\n-   $R = 0.0$ (by definition).\n-   $\\text{median}(r) = 0$. $s_{\\text{MAD}} = 0$.\n-   $\\max|r_i| = 0$. $I = 0 / (0 + 10^{-12}) = 0.0$.\n-   Outlier Indicator: $I \\le 6$, so `False`.\n-   Metric Code: `1`.\n-   Result: $[0.0, 0.0, 0.0, \\text{False}, 1]$\n\n**Case E: Single time point, severe deviation**\n-   $r^{(E)} = [4.50]$, $n=1$.\n-   $\\text{RMSE} = \\sqrt{4.5^2} = 4.5$.\n-   $\\text{MAE} = |4.5| = 4.5$.\n-   $R = 4.5 / 4.5 = 1.0$.\n-   $\\text{median}(r) = 4.5$. $s_{\\text{MAD}} = \\text{median}(|4.5 - 4.5|) = 0$.\n-   $\\max|r_i| = 4.5$. $I = 4.5 / (0 + 10^{-12}) = 4.5 \\times 10^{12}$.\n-   Outlier Indicator: $I > 6$, so `True`.\n-   Metric Code: `0`.\n-   Result: $[4.5, 4.5, 1.0, \\text{True}, 0]$\n\nThese results will be generated by the provided Python code, adhering to the specified output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates a pharmacokinetic model by computing error metrics for several test cases.\n    \"\"\"\n\n    # Model parameters and constants\n    C0 = 4.0  # mg/L\n    k = 0.25  # hr^-1\n    epsilon = 1e-12\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of (time_points, residuals).\n    test_cases = [\n        # Case A: Happy path, small Gaussian-like deviations\n        (np.arange(10), \n         np.array([0.05, -0.04, 0.02, -0.03, 0.01, -0.02, 0.00, 0.03, -0.01, 0.02])),\n        \n        # Case B: Single large outlier\n        (np.arange(10), \n         np.array([0.05, -0.04, 0.02, -0.03, 0.01, 2.50, 0.00, 0.03, -0.01, 0.02])),\n        \n        # Case C: Heavy-tailed residual pattern\n        (np.arange(10), \n         np.array([0.10, -0.08, 0.06, -0.05, 0.00, 0.00, 0.30, -0.40, 0.50, -0.30])),\n        \n        # Case D: Boundary, zero error\n        (np.arange(10), \n         np.array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00])),\n        \n        # Case E: Edge, single time point, severe deviation\n        (np.array([2.0]), \n         np.array([4.50]))\n    ]\n\n    all_results = []\n    \n    for t_points, r_given in test_cases:\n        # Per the problem description, construct predictions and measurements to get residuals.\n        # This step is formally required, though it just returns the provided residuals.\n        y_hat = C0 * np.exp(-k * t_points)\n        y_measured = y_hat + r_given\n        r = y_measured - y_hat\n\n        # 1. Compute Root Mean Square Error (RMSE)\n        rmse = np.sqrt(np.mean(r**2))\n\n        # 2. Compute Mean Absolute Error (MAE)\n        mae = np.mean(np.abs(r))\n\n        # 3. Compute Ratio R\n        # Handles the case where MAE is 0 to avoid division by zero.\n        ratio_r = rmse / mae if mae > 0 else 0.0\n\n        # 4. Compute Outlier Influence Indicator\n        median_r = np.median(r)\n        s_mad = np.median(np.abs(r - median_r))\n        max_abs_r = np.max(np.abs(r))\n        \n        # The influence score I\n        influence_score = max_abs_r / (s_mad + epsilon)\n        \n        # Classify as True if I > 6\n        heavy_outliers_present = influence_score > 6\n\n        # 5. Compute Metric Selection Code\n        # Code 0 for MAE (if outliers), Code 1 for RMSE (otherwise)\n        metric_code = 0 if heavy_outliers_present else 1\n\n        case_result = [\n            rmse,\n            mae,\n            ratio_r,\n            heavy_outliers_present,\n            metric_code\n        ]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # Custom formatting to match the example output style exactly.\n    result_str = \"[\"\n    for i, res in enumerate(all_results):\n        # Format: [float, float, float, Boolean, int]\n        # Python's bool __str__ is 'True' or 'False' which is standard.\n        res_list_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]}]\"\n        result_str += res_list_str\n        if i < len(all_results) - 1:\n            result_str += \",\"\n    result_str += \"]\"\n    \n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A model that perfectly describes the data used to build it may fail completely when faced with new information, a phenomenon known as overfitting. Therefore, a crucial validation step is to assess a model's ability to generalize to unseen data. This practice demonstrates the core principle of cross-validation, where a separate dataset is used to compare the predictive performance of competing models. By calculating the one-step-ahead prediction errors for two different models—one simple and one more complex—you will learn how to use validation data to make an objective, evidence-based choice, often favoring the more parsimonious model if it performs sufficiently well .",
            "id": "1592060",
            "problem": "An engineer is tasked with developing a discrete-time model for an unknown dynamic process. After an initial identification experiment, two plausible linear time-invariant models, a simpler first-order model $M_1$ and a more complex second-order model $M_2$, have been proposed.\n\nThe transfer function for the first-order model $M_1$ is given by:\n$$G_1(z) = \\frac{Y(z)}{U(z)} = \\frac{0.5 z^{-1}}{1 - 0.5 z^{-1}}$$\n\nThe transfer function for the second-order model $M_2$ is:\n$$G_2(z) = \\frac{Y(z)}{U(z)} = \\frac{0.2 z^{-1} + 0.31 z^{-2}}{1 - 0.7 z^{-1} + 0.1 z^{-2}}$$\n\nTo validate these models and determine which one provides a better description of the process, a new experiment is conducted, yielding a cross-validation dataset. The one-step-ahead prediction for a model is computed using its corresponding difference equation, based on past inputs and its own past predictions. We define the performance of each model using the Sum of Squared Prediction Errors (SSPE), calculated as $SSPE = \\sum_{k=0}^{4} [y_m(k) - \\hat{y}(k)]^2$, where $y_m(k)$ is the measured output and $\\hat{y}(k)$ is the model's one-step-ahead predicted output.\n\nThe cross-validation dataset for time steps $k=0, 1, 2, 3, 4$ is as follows:\n- Input sequence $u(k)$: $\\{2, -1, 0, 1, 0\\}$\n- Measured output sequence $y_m(k)$: $\\{0, 1.1, -0.1, 0.05, 0.6\\}$\n\nAssume the system is at rest for $t<0$; that is, $u(k)=0$ and all model outputs and their past values are zero for all $k<0$.\n\nCalculate the SSPE for model $M_1$ (denoted $SSPE_1$) and for model $M_2$ (denoted $SSPE_2$). Provide your answer as a row matrix containing the two values $[SSPE_1, SSPE_2]$, with each value rounded to four significant figures.",
            "solution": "We derive the one-step-ahead prediction difference equations from the given transfer functions by multiplying both sides by the denominator and interpreting the shifts in time.\n\nFor model $M_{1}$:\n$$G_{1}(z)=\\frac{0.5 z^{-1}}{1-0.5 z^{-1}} \\implies \\left(1-0.5 z^{-1}\\right)Y(z)=0.5 z^{-1} U(z)$$\nIn the time domain, the one-step-ahead predictor (using its own past predictions and past inputs) satisfies\n$$\\hat{y}_{1}(k)-0.5\\,\\hat{y}_{1}(k-1)=0.5\\,u(k-1), \\quad \\hat{y}_{1}(-1)=0,$$\nwith $u(k)=0$ for $k<0$. Using $u(0)=2,\\,u(1)=-1,\\,u(2)=0,\\,u(3)=1,\\,u(4)=0$:\n- $k=0$: $\\hat{y}_{1}(0)-0.5\\,\\hat{y}_{1}(-1)=0.5\\,u(-1)=0 \\implies \\hat{y}_{1}(0)=0$.\n- $k=1$: $\\hat{y}_{1}(1)-0.5\\,\\hat{y}_{1}(0)=0.5\\,u(0)=1 \\implies \\hat{y}_{1}(1)=1$.\n- $k=2$: $\\hat{y}_{1}(2)-0.5\\,\\hat{y}_{1}(1)=0.5\\,u(1)=-0.5 \\implies \\hat{y}_{1}(2)=0$.\n- $k=3$: $\\hat{y}_{1}(3)-0.5\\,\\hat{y}_{1}(2)=0.5\\,u(2)=0 \\implies \\hat{y}_{1}(3)=0$.\n- $k=4$: $\\hat{y}_{1}(4)-0.5\\,\\hat{y}_{1}(3)=0.5\\,u(3)=0.5 \\implies \\hat{y}_{1}(4)=0.5$.\nWith $y_{m}(k)=\\{0,\\,1.1,\\,-0.1,\\,0.05,\\,0.6\\}$, the errors $e_{1}(k)=y_{m}(k)-\\hat{y}_{1}(k)$ are\n$$e_{1}(0)=0,\\quad e_{1}(1)=0.1,\\quad e_{1}(2)=-0.1,\\quad e_{1}(3)=0.05,\\quad e_{1}(4)=0.1.$$\nThus\n$$SSPE_{1}=\\sum_{k=0}^{4} e_{1}(k)^{2}=0^{2}+0.1^{2}+(-0.1)^{2}+0.05^{2}+0.1^{2}=0.0325.$$\n\nFor model $M_{2}$:\n$$G_{2}(z)=\\frac{0.2 z^{-1}+0.31 z^{-2}}{1-0.7 z^{-1}+0.1 z^{-2}} \\implies \\left(1-0.7 z^{-1}+0.1 z^{-2}\\right)Y(z)=\\left(0.2 z^{-1}+0.31 z^{-2}\\right)U(z)$$\nIn the time domain, the one-step-ahead predictor satisfies\n$$\\hat{y}_{2}(k)-0.7\\,\\hat{y}_{2}(k-1)+0.1\\,\\hat{y}_{2}(k-2)=0.2\\,u(k-1)+0.31\\,u(k-2),$$\nwith $\\hat{y}_{2}(-1)=0$, $\\hat{y}_{2}(-2)=0$ and $u(k)=0$ for $k<0$. Solving sequentially:\n- $k=0$: $\\hat{y}_{2}(0)=0$.\n- $k=1$: $\\hat{y}_{2}(1)=0.7\\,\\hat{y}_{2}(0)-0.1\\,\\hat{y}_{2}(-1)+0.2\\,u(0)+0.31\\,u(-1)=0.4$.\n- $k=2$: $\\hat{y}_{2}(2)=0.7\\cdot 0.4-0.1\\cdot 0+0.2\\cdot(-1)+0.31\\cdot 2=0.70$.\n- $k=3$: $\\hat{y}_{2}(3)=0.7\\cdot 0.70-0.1\\cdot 0.4+0.2\\cdot 0+0.31\\cdot(-1)=0.14$.\n- $k=4$: $\\hat{y}_{2}(4)=0.7\\cdot 0.14-0.1\\cdot 0.70+0.2\\cdot 1+0.31\\cdot 0=0.228$.\nErrors $e_{2}(k)=y_{m}(k)-\\hat{y}_{2}(k)$ are\n$$e_{2}(0)=0,\\quad e_{2}(1)=0.7,\\quad e_{2}(2)=-0.8,\\quad e_{2}(3)=-0.09,\\quad e_{2}(4)=0.372.$$\nThus\n$$SSPE_{2}=\\sum_{k=0}^{4} e_{2}(k)^{2}=0^{2}+0.7^{2}+(-0.8)^{2}+(-0.09)^{2}+0.372^{2}=0.49+0.64+0.0081+0.138384=1.276484.$$\n\nRounded to four significant figures:\n$$SSPE_{1}=0.03250,\\quad SSPE_{2}=1.276.$$\nTherefore, the requested row matrix is\n$$\\begin{pmatrix}0.03250 & 1.276\\end{pmatrix}.$$",
            "answer": "$$\\boxed{\\begin{pmatrix}0.03250 & 1.276\\end{pmatrix}}$$"
        },
        {
            "introduction": "For a clinical prediction model, statistical accuracy alone is insufficient; its ultimate value lies in its ability to improve patient outcomes by guiding better decisions. Decision Curve Analysis (DCA) is an advanced framework for evaluating whether a model offers practical clinical utility compared to default strategies like treating all patients or treating none. This exercise challenges you to build a decision curve from first principles, connecting a model's risk predictions ($p_i$) to the concept of clinical net benefit across a range of decision thresholds ($p_t$). By identifying the specific range of thresholds where the model is superior, you will gain a deep, practical understanding of how to quantify and report a model's real-world value .",
            "id": "3904334",
            "problem": "You are given validation predictions from a cancer screening risk model and corresponding binary outcomes for multiple held-out cohorts. The objective is to construct the decision curve of the model on each cohort and determine the threshold probability ranges under which the model provides strictly higher clinical net benefit than both the treat-all and treat-none strategies. The work must be grounded in decision-theoretic principles and constructed from first principles, not shortcut formulas.\n\nStart from a valid fundamental base appropriate for biomedical systems modeling and model validation, specifically:\n- Define a binary outcome with $y_i \\in \\{0,1\\}$, where $y_i=1$ denotes presence of cancer and $y_i=0$ denotes absence of cancer for individual $i$.\n- Let $p_i \\in [0,1]$ denote a calibrated validation prediction for individual $i$ representing the estimated probability of cancer.\n- Given a threshold probability $p_t \\in (0,1)$ interpreted as the risk level at which clinical intervention becomes justified, define the binary decision rule $d_i(p_t)$ that treats individual $i$ if and only if $p_i \\ge p_t$, otherwise avoids treatment.\n- From first principles in decision analysis, represent clinical net benefit as expected benefit of correctly treated cases minus expected harm of incorrectly treated non-cases, normalized by population size. Derive necessary expressions by combining the decision rule with the confusion matrix counts and the threshold-implied exchange rate between benefits and harms.\n- Define the treat-all strategy as $d_i^{\\text{all}}=1$ for all $i$ and the treat-none strategy as $d_i^{\\text{none}}=0$ for all $i$. Use these as comparators.\n\nYou must implement the following tasks:\n- For each cohort, construct the decision curve by computing the model’s net benefit, the treat-all net benefit, and the treat-none net benefit across a uniform grid of threshold probabilities $p_t \\in \\{0.01,0.02,\\ldots,0.99\\}$, expressed as decimal probabilities (not percentages).\n- Determine the contiguous threshold intervals on this grid where the model’s net benefit is strictly greater than the net benefit of treat-all and strictly greater than the net benefit of treat-none. Report each interval as $[a,b]$ with $a$ and $b$ being the inclusive bounds taken directly from the grid.\n- Express the final interval bounds as decimal probabilities rounded to three decimal places.\n\nThe test suite consists of three held-out cohorts with the following validation predictions and outcomes. Each cohort is represented as a pair of lists $(\\{p_i\\},\\{y_i\\})$:\n\nCohort $1$ ($n=20$):\n- Validation predictions $\\{0.02,0.05,0.10,0.15,0.20,0.25,0.35,0.40,0.45,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.30,0.50,0.95\\}$.\n- Outcomes $\\{0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,1,1,0,1,1\\}$.\n\nCohort $2$ ($n=15$):\n- Validation predictions $\\{0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.30,0.20,0.27,0.23\\}$.\n- Outcomes $\\{0,1,0,0,1,0,0,0,1,0,0,1,0,0,0\\}$.\n\nCohort $3$ ($n=10$):\n- Validation predictions $\\{0.00,0.00,0.00,0.00,1.00,1.00,1.00,1.00,1.00,1.00\\}$.\n- Outcomes $\\{0,0,0,0,1,1,1,1,1,1\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one cohort and is itself a list of $[a,b]$ intervals. For example, the output must look like $[[[a_1,b_1],[a_2,b_2]],[],[[a_3,b_3]]]$ with all numbers rounded to three decimals. If a cohort has no threshold where the model is strictly superior to both comparators, output an empty list for that cohort.\n\nAll probabilities and thresholds must be treated as unitless decimal values. Do not use percentage signs.\n\nThe final output must follow the exact format described and be printed as a single line. No other text should be printed.",
            "solution": "The problem requires the determination of threshold probability ranges for which a clinical risk model provides a greater net benefit than both treating all patients and treating no patients. The solution necessitates a derivation of the net benefit metric from first principles of decision theory, followed by a computational implementation to find the specified ranges for three given cohorts.\n\nFirst, we establish the theoretical foundation for Net Benefit ($NB$). The analysis is based on a binary outcome, $y_i \\in \\{0, 1\\}$, where $y_i=1$ signifies the presence of the condition (e.g., cancer) and $y_i=0$ its absence. A model provides a risk score $p_i \\in [0, 1]$ for each individual $i$. A clinical decision to intervene (treat) is made based on a threshold probability, $p_t \\in (0, 1)$. The decision rule is to treat individual $i$ if and only if their predicted risk $p_i$ is greater than or equal to the threshold, i.e., $p_i \\ge p_t$.\n\nDecision theory posits that a decision should be made if its expected benefit is positive. Let $B$ be the benefit of a true positive (treating a patient who has the condition) and $H$ be the harm of a false positive (treating a patient who does not have the condition). The expected benefit of treating a patient with a true, underlying probability of disease $p$ is $E[\\text{treat}] = p \\cdot B - (1-p) \\cdot H$. The decision to treat is justified if $E[\\text{treat}] > E[\\text{not treat}]$. Assuming the net benefit of not treating is $0$, we treat if $p \\cdot B - (1-p) \\cdot H > 0$. The threshold probability $p_t$ represents the point of indifference, where the expected benefit of treatment equals that of non-treatment: $p_t \\cdot B = (1-p_t) \\cdot H$. This yields the \"exchange rate\" between harm and benefit implied by the threshold:\n$$\n\\frac{H}{B} = \\frac{p_t}{1-p_t}\n$$\nThis relationship signifies that for a given $p_t$, the harm of one false positive is equivalent to the benefit of $\\frac{p_t}{1-p_t}$ true positives.\n\nNet Benefit for the entire cohort is defined as the total benefits minus total harms, averaged over all individuals in the cohort. Let $N$ be the total population size. For a given threshold $p_t$, we can count the number of true positives, $TP(p_t)$, and false positives, $FP(p_t)$. The total benefit is $TP(p_t) \\cdot B$ and total harm is $FP(p_t) \\cdot H$. The population-level net benefit is:\n$$\nNB_{\\text{total}} = TP(p_t) \\cdot B - FP(p_t) \\cdot H\n$$\nTo make this metric independent of the specific units of $B$ and to express it in terms of $p_t$, we can divide by $B$ and substitute the exchange rate. The net benefit per individual is then obtained by dividing by $N$:\n$$\nNB_{\\text{model}}(p_t) = \\frac{TP(p_t) \\cdot B - FP(p_t) \\cdot H}{N \\cdot B} = \\frac{TP(p_t)}{N} - \\frac{FP(p_t)}{N} \\left(\\frac{H}{B}\\right)\n$$\nSubstituting $\\frac{H}{B} = \\frac{p_t}{1-p_t}$, we arrive at the standard formula for the net benefit of the model:\n$$\nNB_{\\text{model}}(p_t) = \\frac{TP(p_t)}{N} - \\frac{FP(p_t)}{N} \\frac{p_t}{1-p_t}\n$$\n\nNext, we define the net benefit for the two comparator strategies.\nThe \"treat-none\" strategy results in $TP=0$ and $FP=0$. Therefore, its net benefit is always zero:\n$$\nNB_{\\text{none}}(p_t) = 0\n$$\nThe \"treat-all\" strategy results in treating every individual in the cohort. The number of true positives equals the total number of individuals with the condition, $N_{pos}$, and the number of false positives equals the total number of individuals without the condition, $N_{neg}$. Let the prevalence of the condition in the cohort be $\\pi = N_{pos} / N$. Then $N_{neg}/N = 1-\\pi$. The net benefit is:\n$$\nNB_{\\text{all}}(p_t) = \\frac{N_{pos}}{N} - \\frac{N_{neg}}{N} \\frac{p_t}{1-p_t} = \\pi - (1-\\pi) \\frac{p_t}{1-p_t}\n$$\n\nThe objective is to find the intervals of $p_t$ where the model is strictly superior to both comparators, i.e., where $NB_{\\text{model}}(p_t) > NB_{\\text{all}}(p_t)$ and $NB_{\\text{model}}(p_t) > NB_{\\text{none}}(p_t)$.\n\nThe algorithm proceeds as follows for each cohort:\n1.  Initialize an empty list to store the threshold values $p_t$ where the model is superior.\n2.  Iterate through each threshold $p_t$ in the specified grid $\\{0.01, 0.02, \\ldots, 0.99\\}$.\n3.  For each $p_t$:\n    a. Calculate $TP(p_t)$ and $FP(p_t)$ by applying the decision rule ($p_i \\ge p_t$) to the cohort's predictions $\\{p_i\\}$ and outcomes $\\{y_i\\}$.\n    b. Calculate $NB_{\\text{model}}(p_t)$, $NB_{\\text{all}}(p_t)$, and $NB_{\\text{none}}(p_t)$ using the derived formulae.\n    c. If $NB_{\\text{model}}(p_t) > NB_{\\text{all}}(p_t)$ and $NB_{\\text{model}}(p_t) > NB_{\\text{none}}(p_t)$, add $p_t$ to the list of superior thresholds.\n4.  After iterating through all thresholds, process the list of superior thresholds to identify contiguous intervals. An interval is contiguous if its constituent thresholds are sequential in the grid (e.g., $0.21, 0.22, 0.23$).\n5.  Format the identified intervals as $[a, b]$, where $a$ and $b$ are the inclusive start and end bounds from the grid, expressed with three decimal places. Collect these intervals for each cohort.\n6.  Finally, aggregate the results from all cohorts into the specified final output format.\nThis procedure is systematically applied to each of the three cohorts provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the decision curve analysis problem for given cohorts.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            [0.02, 0.05, 0.10, 0.15, 0.20, 0.25, 0.35, 0.40, 0.45, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.30, 0.50, 0.95],\n            [0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,1,1,0,1,1]\n        ),\n        (\n            [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.30, 0.20, 0.27, 0.23],\n            [0,1,0,0,1,0,0,0,1,0,0,1,0,0,0]\n        ),\n        (\n            [0.00, 0.00, 0.00, 0.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00],\n            [0,0,0,0,1,1,1,1,1,1]\n        )\n    ]\n\n    pt_grid = np.arange(0.01, 1.00, 0.01)\n    all_cohort_results = []\n\n    for predictions, outcomes in test_cases:\n        p = np.array(predictions)\n        y = np.array(outcomes)\n        n = len(y)\n        \n        if n == 0:\n            all_cohort_results.append([])\n            continue\n\n        prevalence = np.sum(y) / n\n        superior_thresholds = []\n\n        for pt in pt_grid:\n            # The denominator (1-pt) would be zero if pt=1, but the grid excludes it.\n            exchange_rate = pt / (1 - pt)\n\n            # Model Net Benefit\n            treated_mask = p >= pt\n            tp = np.sum(treated_mask & (y == 1))\n            fp = np.sum(treated_mask & (y == 0))\n            nb_model = (tp / n) - (fp / n) * exchange_rate\n\n            # Comparator Net Benefits\n            nb_treat_all = prevalence - (1 - prevalence) * exchange_rate\n            nb_treat_none = 0.0\n\n            if nb_model > nb_treat_all and nb_model > nb_treat_none:\n                superior_thresholds.append(pt)\n\n        # Find contiguous intervals from the list of superior thresholds\n        intervals = []\n        if superior_thresholds:\n            start_interval = superior_thresholds[0]\n            for i in range(1, len(superior_thresholds)):\n                # Check for a gap in the sequence of thresholds.\n                # Use a small tolerance for floating point comparison.\n                if superior_thresholds[i] - superior_thresholds[i-1] > (0.01 + 1e-9):\n                    end_interval = superior_thresholds[i-1]\n                    intervals.append([start_interval, end_interval])\n                    start_interval = superior_thresholds[i]\n            \n            # Add the last interval\n            end_interval = superior_thresholds[-1]\n            intervals.append([start_interval, end_interval])\n\n        all_cohort_results.append(intervals)\n\n    # Format the final output string exactly as specified.\n    cohort_strs = []\n    for cohort_intervals in all_cohort_results:\n        if not cohort_intervals:\n            cohort_strs.append(\"[]\")\n        else:\n            interval_strs = [f\"[{start:.3f},{end:.3f}]\" for start, end in cohort_intervals]\n            cohort_strs.append(f\"[{','.join(interval_strs)}]\")\n    \n    final_output = f\"[{','.join(cohort_strs)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}