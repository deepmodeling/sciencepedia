{
    "hands_on_practices": [
        {
            "introduction": "The first and most fundamental question we must ask in any modeling endeavor is whether the parameters we seek can be uniquely determined from the experiment we plan to perform. This exercise lays the groundwork for parameter estimation by tackling the concept of structural identifiability from first principles . By analyzing a simple linear system, you will connect the formal mathematical definition of identifiability to the practical requirement of designing an informative experiment, revealing why the nature of the input signal is critical for success.",
            "id": "3916249",
            "problem": "Consider a simplified yet scientifically plausible model of an infusion-response subsystem in biomedical systems modeling where the measured output $y(t)$ (for example, a biomarker concentration proportional to an administered input) satisfies $y(t)=\\theta u(t)$ for all $t$ in a finite observation window, with an unknown real scalar parameter $\\theta$ capturing the gain from input $u(t)$ to output $y(t)$. Assume noiseless measurements $y(t_i)$ at distinct times $t_1,\\dots,t_N$, and that the input $u(t)$ is completely known and can be evaluated at these times. Use the definition of structural identifiability as injectivity of the parameter-to-output mapping under the given input to derive, from first principles, necessary and sufficient conditions on the input values $u(t_1),\\dots,u(t_N)$ under which $\\theta$ is structurally identifiable. Then, under those conditions, derive an explicit closed-form inverse mapping that recovers $\\theta$ from the measurement data $\\{(t_i,u(t_i),y(t_i))\\}_{i=1}^{N}$ without appealing to any shortcut formulas. Your final answer must be the single analytic expression for $\\theta$ in terms of $\\{u(t_i),y(t_i)\\}_{i=1}^{N}$ alone. No rounding is required; express your answer exactly, and no physical units are needed in the final expression.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n-   **Model Equation:** The measured output $y(t)$ is related to the input $u(t)$ by the equation $y(t) = \\theta u(t)$, where $\\theta$ is an unknown real scalar parameter.\n-   **Measurement Scheme:** Measurements are noiseless and taken at $N$ distinct times $t_1, \\dots, t_N$ within a finite observation window.\n-   **Data:** The available data set is $\\{(t_i, u(t_i), y(t_i))\\}_{i=1}^{N}$. The input values $u(t_i)$ are known.\n-   **Definition of Identifiability:** Structural identifiability is defined as the injectivity of the parameter-to-output mapping.\n-   **Objective 1:** Derive the necessary and sufficient conditions on the input values $\\{u(t_i)\\}_{i=1}^{N}$ for $\\theta$ to be structurally identifiable.\n-   **Objective 2:** Under these conditions, derive an explicit closed-form inverse mapping to recover $\\theta$ from the data from first principles.\n-   **Final Answer Format:** The final answer must be a single analytic expression for $\\theta$ in terms of $\\{u(t_i), y(t_i)\\}_{i=1}^{N}$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientific Grounding:** The model $y(t) = \\theta u(t)$ represents a simple, static linear gain system. This is a fundamental and scientifically plausible model in many fields, including as a simplification in biomedical systems (e.g., steady-state dose-response). The concept of structural identifiability is a core principle in systems theory and parameter estimation. The problem is scientifically grounded.\n-   **Well-Posedness:** The problem is well-posed. It asks for the conditions under which a unique solution for $\\theta$ exists and then asks for the derivation of that solution. This is a standard structure for an identifiability problem.\n-   **Objectivity:** The problem is stated in precise, objective mathematical language.\n-   **Completeness and Consistency:** The problem is self-contained. It provides the model, data structure, definition of identifiability, and a clear objective. The assumption of \"noiseless measurements\" is a standard and necessary idealization for analyzing *structural* identifiability, which is a property of the model structure itself, independent of noise processes.\n-   **Realism and Feasibility:** The model is an idealization but not scientifically implausible or physically impossible. It forms the basis for more complex models.\n-   **Other Flaws:** The problem is not trivial, as it requires a rigorous derivation from first principles. It is not ill-posed, metaphorical, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\nThe problem asks for two things: first, the conditions for structural identifiability of the parameter $\\theta$, and second, the explicit inverse mapping to find $\\theta$ from the data.\n\nLet us begin by formalizing the parameter-to-output mapping. The parameter is the scalar $\\theta \\in \\mathbb{R}$. The output of the experiment consists of a set of $N$ measurements, which we can arrange into a vector $\\mathbf{y} \\in \\mathbb{R}^N$, where $\\mathbf{y} = \\begin{pmatrix} y(t_1) & y(t_2) & \\dots & y(t_N) \\end{pmatrix}^T$. The model equation $y(t) = \\theta u(t)$ provides the relationship for each measurement point:\n$$y(t_i) = \\theta u(t_i) \\quad \\text{for } i = 1, 2, \\dots, N$$\nThis system of equations can be written in vector form. Let $\\mathbf{u} = \\begin{pmatrix} u(t_1) & u(t_2) & \\dots & u(t_N) \\end{pmatrix}^T$ be the vector of known input values. The system of equations is then:\n$$\\mathbf{y} = \\theta \\mathbf{u}$$\nThe parameter-to-output mapping, which we will denote by $\\mathcal{M}$, is a function that takes the parameter $\\theta$ and, for a given input experiment defined by $\\mathbf{u}$, produces the output vector $\\mathbf{y}$. Thus, $\\mathcal{M}: \\mathbb{R} \\to \\mathbb{R}^N$ is defined by:\n$$\\mathcal{M}(\\theta) = \\theta \\mathbf{u}$$\nThe problem states that $\\theta$ is structurally identifiable if and only if this mapping $\\mathcal{M}$ is injective. A function $f$ is injective if $f(x_1) = f(x_2)$ implies $x_1 = x_2$. Applying this definition to our mapping $\\mathcal{M}$, we must have:\n$$\\mathcal{M}(\\theta_1) = \\mathcal{M}(\\theta_2) \\implies \\theta_1 = \\theta_2$$\nLet's analyze the premise $\\mathcal{M}(\\theta_1) = \\mathcal{M}(\\theta_2)$. Using the definition of our map, this means:\n$$\\theta_1 \\mathbf{u} = \\theta_2 \\mathbf{u}$$\nRearranging this vector equation, we get:\n$$(\\theta_1 - \\theta_2) \\mathbf{u} = \\mathbf{0}$$\nwhere $\\mathbf{0}$ is the zero vector in $\\mathbb{R}^N$. This single vector equation is equivalent to the system of $N$ scalar equations:\n$$(\\theta_1 - \\theta_2) u(t_i) = 0 \\quad \\text{for } i = 1, 2, \\dots, N$$\nFor the mapping to be injective, this system of equations must force the conclusion $\\theta_1 - \\theta_2 = 0$, which is equivalent to $\\theta_1 = \\theta_2$. The equation $(\\theta_1 - \\theta_2) u(t_i) = 0$ is satisfied if either $\\theta_1 - \\theta_2 = 0$ or $u(t_i) = 0$. If $u(t_i) = 0$ for all $i=1, \\dots, N$, then $\\mathbf{u} = \\mathbf{0}$. In this case, the equation becomes $(\\theta_1 - \\theta_2) \\cdot 0 = 0$, which is true for any choice of $\\theta_1$ and $\\theta_2$. We cannot conclude that $\\theta_1 = \\theta_2$, and the parameter is not identifiable.\nConversely, if there exists at least one index $j \\in \\{1, \\dots, N\\}$ such that $u(t_j) \\neq 0$, then for the equation $(\\theta_1 - \\theta_2) u(t_j) = 0$ to hold, we must have $\\theta_1 - \\theta_2 = 0$. This implies $\\theta_1 = \\theta_2$.\nTherefore, the necessary and sufficient condition for the mapping $\\mathcal{M}$ to be injective, and thus for $\\theta$ to be structurally identifiable, is that the input vector $\\mathbf{u}$ is not the zero vector. This means that there must be at least one measurement time $t_i$ at which the input $u(t_i)$ is non-zero.\n\nNext, we must derive the inverse mapping that recovers $\\theta$ from the data $\\{(t_i, u(t_i), y(t_i))\\}_{i=1}^{N}$. This requires solving the system of equations $y(t_i) = \\theta u(t_i)$ for $\\theta$. Since the problem specifies noiseless measurements, all these equations are simultaneously true. We need to find an expression for $\\theta$ that amalgamates the information from all $N$ measurements. This is a classic overdetermined system (if $N>1$), and a robust method to solve it is to find the value of $\\theta$ that minimizes the sum of squared errors between the model predictions and the measurements. This is the principle of least squares. Even for the noiseless case, this method is guaranteed to return the exact true value.\n\nDefine the cost function $J(\\theta)$ as the sum of squared residuals:\n$$J(\\theta) = \\sum_{i=1}^{N} (y(t_i) - \\theta u(t_i))^2$$\nTo find the value of $\\theta$ that minimizes $J(\\theta)$, we take the derivative of $J$ with respect to $\\theta$ and set it to zero.\n$$\\frac{dJ}{d\\theta} = \\sum_{i=1}^{N} \\frac{d}{d\\theta} (y(t_i) - \\theta u(t_i))^2$$\n$$= \\sum_{i=1}^{N} 2(y(t_i) - \\theta u(t_i))(-u(t_i))$$\n$$= -2 \\sum_{i=1}^{N} (y(t_i)u(t_i) - \\theta u(t_i)^2)$$\nSetting the derivative to zero:\n$$-2 \\sum_{i=1}^{N} (y(t_i)u(t_i) - \\theta u(t_i)^2) = 0$$\n$$\\sum_{i=1}^{N} y(t_i)u(t_i) - \\sum_{i=1}^{N} \\theta u(t_i)^2 = 0$$\n$$\\sum_{i=1}^{N} y(t_i)u(t_i) = \\theta \\sum_{i=1}^{N} u(t_i)^2$$\nWe can solve for $\\theta$ by dividing by the sum $\\sum_{i=1}^{N} u(t_i)^2$. This operation is valid if and only if the sum is non-zero. The sum of squares $\\sum_{i=1}^{N} u(t_i)^2$ is zero if and only if each term $u(t_i)^2$ is zero, which means $u(t_i) = 0$ for all $i=1, \\dots, N$. This is precisely the condition under which $\\theta$ was found to be *not* identifiable. Therefore, under the derived condition for identifiability (at least one $u(t_i) \\neq 0$), the denominator is guaranteed to be non-zero, and a unique solution for $\\theta$ exists.\n\nThe explicit closed-form inverse mapping is:\n$$\\theta = \\frac{\\sum_{i=1}^{N} y(t_i) u(t_i)}{\\sum_{i=1}^{N} u(t_i)^2}$$\nThis expression provides the value of $\\theta$ in terms of the known input values $\\{u(t_i)\\}$ and the measured output values $\\{y(t_i)\\}$, as required.\nThis can also be expressed in vector dot product notation as $\\theta = (\\mathbf{y} \\cdot \\mathbf{u}) / (\\mathbf{u} \\cdot \\mathbf{u})$. This is the final expression for the inverse mapping.",
            "answer": "$$\n\\boxed{\\frac{\\sum_{i=1}^{N} y(t_i) u(t_i)}{\\sum_{i=1}^{N} u(t_i)^2}}\n$$"
        },
        {
            "introduction": "Real-world biological systems often possess inherent symmetries that can confound our estimation efforts, leading to structural non-identifiability. This practice moves beyond the basic case to a common scenario in systems biology where two similar molecular species cannot be distinguished by a single measurement, making their individual decay rates ambiguous . You will first demonstrate this non-uniqueness and then discover a powerful strategy to restore identifiability by incorporating a second, independent measurement channel to break the model's symmetry.",
            "id": "3916199",
            "problem": "Consider a minimal two-species biomolecular decay model used in biomedical systems modeling. Two species, with concentrations $x_{1}(t)$ and $x_{2}(t)$, undergo independent first-order decay governed by the Ordinary Differential Equation (ODE) system\n$$\\frac{d x_{1}}{dt} \\;=\\; -\\theta_{1}\\, x_{1}, \\qquad \\frac{d x_{2}}{dt} \\;=\\; -\\theta_{2}\\, x_{2},$$\nwith initial conditions $x_{1}(0) \\;=\\; 1$ and $x_{2}(0) \\;=\\; 1$. A first measurement channel (for example, a spectrophotometric assay) returns the sum signal\n$$y_{1}(t) \\;=\\; x_{1}(t) \\;+\\; x_{2}(t),$$\nwhile a second, independent measurement channel (for example, an assay with different binding affinity) returns a weighted sum\n$$y_{2}(t) \\;=\\; c\\, x_{1}(t) \\;+\\; x_{2}(t),$$\nwhere the known constant $c$ satisfies $c \\neq 1$. You are given $c \\;=\\; 3$ and that time is measured in hours, so the parameters $\\theta_{1}$ and $\\theta_{2}$ have units $\\text{h}^{-1}$.\n\nPart A (single-output non-identifiability): Using only the first output $y_{1}(t)$, demonstrate a parameter non-uniqueness caused by a symmetry in the model. Concretely, exhibit two distinct parameter vectors $(\\theta_{1}, \\theta_{2})$ and $(\\tilde{\\theta}_{1}, \\tilde{\\theta}_{2})$ that produce exactly the same $y_{1}(t)$ for all $t \\geq 0$. Use the provided sample data $y_{1}(1) \\;=\\; 1.1901471847989395$ and $y_{1}(2) \\;=\\; 0.7507081540886818$ (hours are the time unit).\n\nPart B (restoring identifiability with multiple outputs): Show that adding the second output $y_{2}(t)$ at a single time $t^{\\star}$ breaks the symmetry and yields unique parameter identification. Derive a closed-form expression for $(\\theta_{1}, \\theta_{2})$ in terms of $y_{1}(t^{\\star})$, $y_{2}(t^{\\star})$, $c$, and $t^{\\star}$. Your derivation must start from first principles implied by the ODEs and measurement definitions.\n\nPart C (numerical identification): Using the measured values $y_{1}(1) \\;=\\; 1.1901471847989395$ and $y_{2}(1) \\;=\\; 2.6717836261623753$ with $c \\;=\\; 3$ and $t^{\\star} \\;=\\; 1$ hour, compute the unique parameter vector $(\\theta_{1}, \\theta_{2})$. Express the final parameters in $\\text{h}^{-1}$ and round your answer to four significant figures.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in first-order kinetics, mathematically well-posed, objective, and contains a complete and consistent set of information required for its solution. It represents a standard, non-trivial problem in parameter identifiability for systems modeling.\n\nFirst, we solve the given Ordinary Differential Equations (ODEs) with the specified initial conditions. The ODE for species $x_{1}$ is\n$$ \\frac{dx_{1}}{dt} = -\\theta_{1} x_{1} $$\nWith the initial condition $x_{1}(0) = 1$, the solution is\n$$ x_{1}(t) = x_{1}(0) \\exp(-\\theta_{1}t) = \\exp(-\\theta_{1}t) $$\nSimilarly, for species $x_{2}$, the ODE is\n$$ \\frac{dx_{2}}{dt} = -\\theta_{2} x_{2} $$\nWith the initial condition $x_{2}(0) = 1$, the solution is\n$$ x_{2}(t) = x_{2}(0) \\exp(-\\theta_{2}t) = \\exp(-\\theta_{2}t) $$\n\nThe measurement outputs are defined as\n$$ y_{1}(t) = x_{1}(t) + x_{2}(t) = \\exp(-\\theta_{1}t) + \\exp(-\\theta_{2}t) $$\n$$ y_{2}(t) = c\\,x_{1}(t) + x_{2}(t) = c\\,\\exp(-\\theta_{1}t) + \\exp(-\\theta_{2}t) $$\n\n**Part A (single-output non-identifiability)**\n\nUsing only the output $y_{1}(t)$, the model is given by $y_{1}(t) = \\exp(-\\theta_{1}t) + \\exp(-\\theta_{2}t)$. This expression is symmetric with respect to the parameters $\\theta_{1}$ and $\\theta_{2}$. If a parameter vector $(\\theta_{1}, \\theta_{2}) = (a, b)$ is a solution, then swapping the parameters to $(\\tilde{\\theta}_{1}, \\tilde{\\theta}_{2}) = (b, a)$ will yield the exact same output:\n$$ y_{1}(t) = \\exp(-bt) + \\exp(-at) = \\exp(-at) + \\exp(-bt) $$\nThis inherent symmetry means that without further information, we cannot distinguish between $(\\theta_{1}, \\theta_{2})$ and $(\\theta_{2}, \\theta_{1})$. This demonstrates a parameter non-uniqueness.\n\nTo exhibit two distinct parameter vectors, we use the provided data. Let $u = \\exp(-\\theta_{1})$ and $v = \\exp(-\\theta_{2})$. The data at $t=1$ and $t=2$ give a system of equations for $u$ and $v$:\n$$ y_{1}(1) = \\exp(-\\theta_{1}) + \\exp(-\\theta_{2}) = u + v $$\n$$ y_{1}(2) = \\exp(-2\\theta_{1}) + \\exp(-2\\theta_{2}) = u^{2} + v^{2} $$\nWe are given $y_{1}(1) = 1.1901471847989395$ and $y_{1}(2) = 0.7507081540886818$.\nFrom the first equation, $v = y_{1}(1) - u$. Substituting this into the second equation:\n$$ u^{2} + (y_{1}(1) - u)^{2} = y_{1}(2) $$\n$$ u^{2} + y_{1}(1)^{2} - 2y_{1}(1)u + u^{2} = y_{1}(2) $$\n$$ 2u^{2} - 2y_{1}(1)u + (y_{1}(1)^{2} - y_{1}(2)) = 0 $$\nThis is a quadratic equation for $u$. The roots of this quadratic equation will give the values for $u$ and $v$. By the symmetry of the problem, if $u_{A}$ is one root, the other root must be $u_{B} = y_{1}(1) - u_{A}$, which corresponds to $v$.\nSolving the quadratic equation gives two roots, which we can call $u_{\\text{sol}}$ and $v_{\\text{sol}}$:\n$u_{\\text{sol}} = \\exp(-\\theta_{A}) = 0.7408182206817179$\n$v_{\\text{sol}} = \\exp(-\\theta_{B}) = 0.4493289641172216$\nThis gives the two distinct rate constants:\n$\\theta_{A} = -\\ln(0.7408182206817179) = 0.30000000... \\, \\text{h}^{-1}$\n$\\theta_{B} = -\\ln(0.4493289641172216) = 0.80000000... \\, \\text{h}^{-1}$\nDue to the symmetry, we have two possible parameter vectors that produce the same $y_{1}(t)$:\n1. $(\\theta_{1}, \\theta_{2}) = (0.3, 0.8) \\, \\text{h}^{-1}$\n2. $(\\tilde{\\theta}_{1}, \\tilde{\\theta}_{2}) = (0.8, 0.3) \\, \\text{h}^{-1}$\nThese two distinct vectors produce identical outputs $y_{1}(t)$ for all $t \\geq 0$, demonstrating the non-identifiability.\n\n**Part B (restoring identifiability with multiple outputs)**\n\nWith two outputs, $y_{1}(t)$ and $y_{2}(t)$, measured at a single time point $t^{\\star}$, we can uniquely determine the parameters. Let $u = \\exp(-\\theta_{1}t^{\\star})$ and $v = \\exp(-\\theta_{2}t^{\\star})$. The system of equations is:\n$$ y_{1}(t^{\\star}) = u + v $$\n$$ y_{2}(t^{\\star}) = c\\,u + v $$\nThis is a system of two linear equations for $u$ and $v$. The symmetry is broken because the coefficients of $u$ and $v$ are no longer interchangeable in the second equation (since $c \\neq 1$). Subtracting the first equation from the second gives:\n$$ y_{2}(t^{\\star}) - y_{1}(t^{\\star}) = (c\\,u + v) - (u + v) = (c-1)u $$\nSince we are given $c \\neq 1$, we can solve for $u$:\n$$ u = \\frac{y_{2}(t^{\\star}) - y_{1}(t^{\\star})}{c-1} $$\nSubstituting this expression for $u$ back into the first equation, $v = y_{1}(t^{\\star}) - u$:\n$$ v = y_{1}(t^{\\star}) - \\frac{y_{2}(t^{\\star}) - y_{1}(t^{\\star})}{c-1} = \\frac{(c-1)y_{1}(t^{\\star}) - (y_{2}(t^{\\star}) - y_{1}(t^{\\star}))}{c-1} $$\n$$ v = \\frac{cy_{1}(t^{\\star}) - y_{1}(t^{\\star}) - y_{2}(t^{\\star}) + y_{1}(t^{\\star})}{c-1} = \\frac{c\\,y_{1}(t^{\\star}) - y_{2}(t^{\\star})}{c-1} $$\nNow we can solve for $\\theta_{1}$ and $\\theta_{2}$ from the definitions of $u$ and $v$:\n$$ \\exp(-\\theta_{1}t^{\\star}) = u \\implies -\\theta_{1}t^{\\star} = \\ln(u) \\implies \\theta_{1} = -\\frac{1}{t^{\\star}} \\ln(u) $$\n$$ \\theta_{1} = -\\frac{1}{t^{\\star}} \\ln\\left(\\frac{y_{2}(t^{\\star}) - y_{1}(t^{\\star})}{c-1}\\right) $$\nAnd for $\\theta_{2}$:\n$$ \\exp(-\\theta_{2}t^{\\star}) = v \\implies -\\theta_{2}t^{\\star} = \\ln(v) \\implies \\theta_{2} = -\\frac{1}{t^{\\star}} \\ln(v) $$\n$$ \\theta_{2} = -\\frac{1}{t^{\\star}} \\ln\\left(\\frac{c\\,y_{1}(t^{\\star}) - y_{2}(t^{\\star})}{c-1}\\right) $$\nThese expressions provide a unique solution for the ordered pair $(\\theta_{1}, \\theta_{2})$, thereby resolving the non-identifiability.\n\n**Part C (numerical identification)**\n\nWe use the derived formulas with the given data: $t^{\\star} = 1$ hour, $c=3$, $y_{1}(1) = 1.1901471847989395$, and $y_{2}(1) = 2.6717836261623753$.\n\nFirst, calculate the arguments of the logarithms:\nFor $\\theta_{1}$:\n$$ \\frac{y_{2}(1) - y_{1}(1)}{c-1} = \\frac{2.6717836261623753 - 1.1901471847989395}{3-1} = \\frac{1.4816364413634358}{2} = 0.7408182206817179 $$\nFor $\\theta_{2}$:\n$$ \\frac{c\\,y_{1}(1) - y_{2}(1)}{c-1} = \\frac{3 \\times 1.1901471847989395 - 2.6717836261623753}{3-1} = \\frac{3.5704415543968185 - 2.6717836261623753}{2} = \\frac{0.8986579282344432}{2} = 0.4493289641172216 $$\nNow, we compute the parameters $\\theta_{1}$ and $\\theta_{2}$ with $t^{\\star}=1$:\n$$ \\theta_{1} = -\\frac{1}{1} \\ln(0.7408182206817179) = -(-0.30000000000000004) \\approx 0.3000 \\, \\text{h}^{-1} $$\n$$ \\theta_{2} = -\\frac{1}{1} \\ln(0.4493289641172216) = -(-0.8000000000000002) \\approx 0.8000 \\, \\text{h}^{-1} $$\nRounding to four significant figures, the unique parameter vector is $(\\theta_{1}, \\theta_{2}) = (0.3000, 0.8000) \\, \\text{h}^{-1}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.3000 & 0.8000 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Once we've established that a parameter is identifiable, the next challenge is to find the 'best' way to estimate it from noisy data. This exercise shifts our focus from the existence of a solution to the quality of the solution, introducing the Mean Squared Error ($MSE$) as a gold standard for evaluating estimators . You will explore the profound bias-variance tradeoff by comparing a standard unbiased estimator with a biased 'shrinkage' estimator, discovering the conditions under which a biased approach can surprisingly yield more accurate results.",
            "id": "3916253",
            "problem": "A laboratory is quantifying hepatic clearance, denoted by the parameter $\\theta$ measured in liters per hour, within a one-compartment intravenous bolus pharmacokinetic model. For a single subject, repeated microdialysis calibrations yield independent measurements $y_{i}$ that follow the additive noise model $y_{i} = \\theta + \\varepsilon_{i}$, where $\\varepsilon_{i}$ are independent and identically distributed with a Gaussian distribution $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$. A meta-analysis across similar cohorts provides a baseline clearance $\\mu_{0}$ for shrinkage.\n\nConsider two estimators for $\\theta$:\n- The unbiased estimator $\\hat{\\theta}_{U} = \\bar{y}$, where $\\bar{y}$ is the sample mean of $\\{y_{i}\\}_{i=1}^{n}$.\n- A shrinkage estimator $\\hat{\\theta}_{\\alpha} = \\alpha \\bar{y} + (1 - \\alpha)\\mu_{0}$ with a fixed shrinkage weight $\\alpha \\in [0, 1]$.\n\nStarting from the definitions of bias, variance, and Mean Squared Error (MSE), where Mean Squared Error (MSE) is defined as $\\operatorname{MSE}(\\hat{\\theta}) = \\mathbb{E}\\big[(\\hat{\\theta} - \\theta)^{2}\\big]$ under the given measurement model, derive the MSEs of both estimators as functions of $\\alpha$, $n$, $\\sigma^{2}$, $\\mu_{0}$, and $\\theta$. Then determine the value of $\\alpha$ that minimizes the MSE of the shrinkage estimator. Using that minimizer, compute the ratio of the minimized MSE of the biased estimator to the MSE of the unbiased estimator for the following scenario:\n- $n = 12$\n- $\\sigma = 0.9$ liters per hour\n- $\\mu_{0} = 1.2$ liters per hour\n- $\\theta = 0.8$ liters per hour\n\nExpress the final ratio as a dimensionless decimal. Round your final answer to four significant figures.",
            "solution": "The problem asks for the derivation of the Mean Squared Error (MSE) for two estimators of a parameter $\\theta$, the optimization of a shrinkage parameter $\\alpha$, and the calculation of a ratio of MSEs for a specific scenario. The problem is well-posed, scientifically grounded in estimation theory, and provides all necessary information.\n\nFirst, let us establish the statistical properties of the sample mean, $\\bar{y}$. The measurements are given by the model $y_{i} = \\theta + \\varepsilon_{i}$, where $\\varepsilon_{i}$ are independent and identically distributed (i.i.d.) random variables from a Gaussian distribution $\\mathcal{N}(0, \\sigma^{2})$.\nThe expectation of a single measurement is $\\mathbb{E}[y_i] = \\mathbb{E}[\\theta + \\varepsilon_i] = \\theta + \\mathbb{E}[\\varepsilon_i] = \\theta + 0 = \\theta$.\nThe variance of a single measurement is $\\operatorname{Var}(y_i) = \\operatorname{Var}(\\theta + \\varepsilon_i) = \\operatorname{Var}(\\varepsilon_i) = \\sigma^2$.\n\nThe sample mean is $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$.\nIts expectation is $\\mathbb{E}[\\bar{y}] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} y_i\\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[y_i] = \\frac{1}{n} (n\\theta) = \\theta$.\nBecause the measurements $y_i$ are independent, the variance of the sample mean is $\\operatorname{Var}(\\bar{y}) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}(y_i) = \\frac{1}{n^2} (n\\sigma^2) = \\frac{\\sigma^2}{n}$.\n\nThe Mean Squared Error of an estimator $\\hat{\\theta}$ is defined as $\\operatorname{MSE}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^2]$. This can be decomposed into the sum of the squared bias and the variance of the estimator: $\\operatorname{MSE}(\\hat{\\theta}) = (\\operatorname{Bias}(\\hat{\\theta}))^2 + \\operatorname{Var}(\\hat{\\theta})$, where $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$.\n\n**1. MSE of the Unbiased Estimator $\\hat{\\theta}_{U}$**\nThe unbiased estimator is given by $\\hat{\\theta}_{U} = \\bar{y}$.\nThe bias of this estimator is $\\operatorname{Bias}(\\hat{\\theta}_{U}) = \\mathbb{E}[\\hat{\\theta}_{U}] - \\theta = \\mathbb{E}[\\bar{y}] - \\theta = \\theta - \\theta = 0$.\nThe variance of this estimator is $\\operatorname{Var}(\\hat{\\theta}_{U}) = \\operatorname{Var}(\\bar{y}) = \\frac{\\sigma^2}{n}$.\nTherefore, the MSE of the unbiased estimator is:\n$$ \\operatorname{MSE}(\\hat{\\theta}_{U}) = (0)^2 + \\frac{\\sigma^2}{n} = \\frac{\\sigma^2}{n} $$\n\n**2. MSE of the Shrinkage Estimator $\\hat{\\theta}_{\\alpha}$**\nThe shrinkage estimator is given by $\\hat{\\theta}_{\\alpha} = \\alpha \\bar{y} + (1 - \\alpha)\\mu_{0}$, where $\\mu_{0}$ is a fixed constant.\nFirst, we find its bias. The expectation is $\\mathbb{E}[\\hat{\\theta}_{\\alpha}] = \\mathbb{E}[\\alpha \\bar{y} + (1 - \\alpha)\\mu_{0}] = \\alpha \\mathbb{E}[\\bar{y}] + (1 - \\alpha)\\mu_{0} = \\alpha\\theta + (1 - \\alpha)\\mu_{0}$.\nThe bias is $\\operatorname{Bias}(\\hat{\\theta}_{\\alpha}) = \\mathbb{E}[\\hat{\\theta}_{\\alpha}] - \\theta = (\\alpha\\theta + (1 - \\alpha)\\mu_{0}) - \\theta = \\theta(\\alpha - 1) + (1 - \\alpha)\\mu_{0} = (1 - \\alpha)(\\mu_{0} - \\theta)$.\nNext, we find its variance. Since $\\mu_0$ is a constant, $\\operatorname{Var}(\\hat{\\theta}_{\\alpha}) = \\operatorname{Var}(\\alpha \\bar{y} + (1 - \\alpha)\\mu_{0}) = \\operatorname{Var}(\\alpha \\bar{y}) = \\alpha^2 \\operatorname{Var}(\\bar{y}) = \\alpha^2 \\frac{\\sigma^2}{n}$.\nThe MSE of the shrinkage estimator is the sum of its squared bias and variance:\n$$ \\operatorname{MSE}(\\hat{\\theta}_{\\alpha}) = ((1 - \\alpha)(\\mu_{0} - \\theta))^2 + \\left(\\alpha^2 \\frac{\\sigma^2}{n}\\right) = (1 - \\alpha)^2 (\\mu_{0} - \\theta)^2 + \\frac{\\alpha^2 \\sigma^2}{n} $$\n\n**3. Minimization of $\\operatorname{MSE}(\\hat{\\theta}_{\\alpha})$**\nTo find the value of $\\alpha$ that minimizes $\\operatorname{MSE}(\\hat{\\theta}_{\\alpha})$, we take the derivative with respect to $\\alpha$ and set it to zero.\nLet $M(\\alpha) = \\operatorname{MSE}(\\hat{\\theta}_{\\alpha})$.\n$$ \\frac{dM}{d\\alpha} = \\frac{d}{d\\alpha} \\left[ (1 - \\alpha)^2 (\\mu_{0} - \\theta)^2 + \\frac{\\alpha^2 \\sigma^2}{n} \\right] = -2(1 - \\alpha)(\\mu_{0} - \\theta)^2 + \\frac{2\\alpha \\sigma^2}{n} $$\nSetting the derivative to zero to find the optimal $\\alpha$, which we denote $\\alpha_{\\text{opt}}$:\n$$ -2(1 - \\alpha_{\\text{opt}})(\\mu_{0} - \\theta)^2 + \\frac{2\\alpha_{\\text{opt}} \\sigma^2}{n} = 0 $$\n$$ \\alpha_{\\text{opt}} \\frac{\\sigma^2}{n} = (1 - \\alpha_{\\text{opt}})(\\mu_{0} - \\theta)^2 $$\n$$ \\alpha_{\\text{opt}} \\frac{\\sigma^2}{n} = (\\mu_{0} - \\theta)^2 - \\alpha_{\\text{opt}}(\\mu_{0} - \\theta)^2 $$\n$$ \\alpha_{\\text{opt}} \\left( \\frac{\\sigma^2}{n} + (\\mu_{0} - \\theta)^2 \\right) = (\\mu_{0} - \\theta)^2 $$\n$$ \\alpha_{\\text{opt}} = \\frac{(\\mu_{0} - \\theta)^2}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}} $$\nThe second derivative, $\\frac{d^2M}{d\\alpha^2} = 2(\\mu_0 - \\theta)^2 + \\frac{2\\sigma^2}{n}$, is always positive (for $\\sigma^2 > 0$), confirming that this value of $\\alpha$ corresponds to a minimum.\n\n**4. Ratio of Minimized MSEs**\nThe ratio required is $R = \\frac{\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}})}{\\operatorname{MSE}(\\hat{\\theta}_{U})}$.\nLet's first find the expression for the minimized MSE, $\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}})$, by substituting $\\alpha_{\\text{opt}}$ into the MSE formula. A simpler approach is to use the relationship derived an intermediary step during minimization: $\\alpha_{\\text{opt}} \\frac{\\sigma^2}{n} = (1 - \\alpha_{\\text{opt}})(\\mu_{0} - \\theta)^2$.\n$\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}}) = (1 - \\alpha_{\\text{opt}})^2 (\\mu_{0} - \\theta)^2 + \\alpha_{\\text{opt}}^2 \\frac{\\sigma^2}{n}$\n$= (1 - \\alpha_{\\text{opt}}) \\left[ (1 - \\alpha_{\\text{opt}})(\\mu_{0} - \\theta)^2 \\right] + \\alpha_{\\text{opt}} \\left[ \\alpha_{\\text{opt}}\\frac{\\sigma^2}{n}\\right]$\nUsing the relationship, we can substitute for the terms in brackets:\n$= (1 - \\alpha_{\\text{opt}}) \\left[ \\alpha_{\\text{opt}}\\frac{\\sigma^2}{n}\\right] + \\alpha_{\\text{opt}} \\left[ (1 - \\alpha_{\\text{opt}})(\\mu_{0} - \\theta)^2 \\right]$\n$= \\alpha_{\\text{opt}}(1 - \\alpha_{\\text{opt}}) \\left[ \\frac{\\sigma^2}{n} + (\\mu_0 - \\theta)^2 \\right]$\nRecalling that $1 - \\alpha_{\\text{opt}} = \\frac{\\sigma^2/n}{(\\mu_0 - \\theta)^2 + \\sigma^2/n}$:\n$\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}}) = \\frac{(\\mu_{0} - \\theta)^2}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}} \\cdot \\frac{\\frac{\\sigma^2}{n}}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}} \\cdot \\left[ (\\mu_0 - \\theta)^2 + \\frac{\\sigma^2}{n} \\right]$\n$\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}}) = \\frac{\\frac{\\sigma^2}{n} (\\mu_{0} - \\theta)^2}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}}$\n\nNow we can compute the ratio $R$:\n$$ R = \\frac{\\operatorname{MSE}(\\hat{\\theta}_{\\alpha_{\\text{opt}}})}{\\operatorname{MSE}(\\hat{\\theta}_{U})} = \\frac{\\frac{\\frac{\\sigma^2}{n} (\\mu_{0} - \\theta)^2}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}}}{\\frac{\\sigma^2}{n}} $$\n$$ R = \\frac{(\\mu_{0} - \\theta)^2}{(\\mu_{0} - \\theta)^2 + \\frac{\\sigma^2}{n}} $$\nWe observe that the ratio is exactly equal to $\\alpha_{\\text{opt}}$.\n\n**5. Numerical Calculation**\nWe are given the following values:\n$n = 12$\n$\\sigma = 0.9$ L/hr, so $\\sigma^2 = 0.81$ (L/hr)$^2$\n$\\mu_{0} = 1.2$ L/hr\n$\\theta = 0.8$ L/hr\n\nFirst, we compute the components of the expression for $R$:\nThe squared bias term in the denominator is $(\\mu_{0} - \\theta)^2 = (1.2 - 0.8)^2 = (0.4)^2 = 0.16$.\nThe variance of the sample mean is $\\frac{\\sigma^2}{n} = \\frac{0.81}{12} = 0.0675$.\n\nNow, we compute the ratio $R$:\n$$ R = \\frac{0.16}{0.16 + 0.0675} = \\frac{0.16}{0.2275} $$\n$$ R \\approx 0.703296703... $$\nRounding to four significant figures, we get $0.7033$.",
            "answer": "$$\\boxed{0.7033}$$"
        }
    ]
}