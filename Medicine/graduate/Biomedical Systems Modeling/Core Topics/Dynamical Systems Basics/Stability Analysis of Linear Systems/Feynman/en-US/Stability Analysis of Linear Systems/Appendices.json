{
    "hands_on_practices": [
        {
            "introduction": "The Lyapunov equation forms the computational core of stability analysis for linear systems, translating an abstract theorem into a concrete algebraic problem. This first practice provides a fundamental, hands-on exercise in applying Lyapunov's direct method. By solving for the matrix $P$ in the equation $A^{\\top}P + PA = -Q$ and verifying its properties, you will directly certify the stability of a given biomedical model, solidifying the essential link between a system's dynamics and the existence of a valid Lyapunov function .",
            "id": "3930335",
            "problem": "A two-compartment biomedical regulation model is linearized around a nominal operating point, yielding a continuous-time Linear Time-Invariant (LTI) system of the form $\\dot{x}(t)=A\\,x(t)$, where $x(t)\\in\\mathbb{R}^{2}$ represents small perturbations of physiologically meaningful quantities (for example, concentrations or rates), and\n$$\nA=\\begin{bmatrix}-0.7  0.3 \\\\ -0.2  -0.5\\end{bmatrix}.\n$$\nConsider the candidate quadratic Lyapunov function $V(x)=x^{\\top}P\\,x$ with a symmetric matrix $P\\in\\mathbb{R}^{2\\times 2}$. Let $Q=I_{2}$, the identity matrix. From first principles, the continuous-time algebraic Lyapunov equation linking $A$, $P$, and $Q$ is\n$$\nA^{\\top}P + P A = -Q.\n$$\nYour tasks are:\n- Derive and solve for the entries of the symmetric matrix $P$ that satisfy the Lyapunov equation, starting from the definition of $V(x)$ and its time derivative along trajectories of $\\dot{x}(t)=A\\,x(t)$.\n- Using eigenvalue-based stability criteria for linear systems, justify why the existence of such a matrix $P$ with $P0$ certifies asymptotic stability of the origin for the given $A$.\n- As a quantitative certificate of positive definiteness, compute the smallest eigenvalue of $P$ in exact analytical form.\n\nReport as your final answer the smallest eigenvalue of $P$ as a single closed-form expression. No rounding is required, and the quantity is dimensionless.",
            "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n**Step 1: Extract Givens**\n- Continuous-time LTI system: $\\dot{x}(t)=A\\,x(t)$, with $x(t)\\in\\mathbb{R}^{2}$.\n- System matrix: $A=\\begin{bmatrix}-0.7  0.3 \\\\ -0.2  -0.5\\end{bmatrix}$.\n- Candidate quadratic Lyapunov function: $V(x)=x^{\\top}P\\,x$, with $P$ being a symmetric matrix in $\\mathbb{R}^{2\\times 2}$.\n- Auxiliary matrix: $Q=I_{2}$, the $2 \\times 2$ identity matrix.\n- Continuous-time algebraic Lyapunov equation: $A^{\\top}P + P A = -Q$.\n- Tasks:\n    1. Derive and solve for the entries of the symmetric matrix $P$.\n    2. Justify why the existence of $P0$ certifies asymptotic stability of the origin.\n    3. Compute the smallest eigenvalue of $P$ in exact analytical form.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, employing standard principles of linear systems theory and Lyapunov stability analysis. The concept of linearizing a biomedical regulation model is a valid and common practice in systems biology and control engineering.\n\nThe problem is well-posed. The stability of the matrix $A$ determines if a unique, positive definite solution $P$ exists for the Lyapunov equation. The eigenvalues $\\lambda$ of $A$ are given by the characteristic equation $\\det(A - \\lambda I) = 0$:\n$$ \\det\\begin{pmatrix}-0.7 - \\lambda  0.3 \\\\ -0.2  -0.5 - \\lambda\\end{pmatrix} = 0 $$\n$$ (-0.7 - \\lambda)(-0.5 - \\lambda) - (0.3)(-0.2) = 0 $$\n$$ \\lambda^2 + 0.5\\lambda + 0.7\\lambda + 0.35 + 0.06 = 0 $$\n$$ \\lambda^2 + 1.2\\lambda + 0.41 = 0 $$\nFor a second-order characteristic polynomial $\\lambda^2 + a_1\\lambda + a_0 = 0$, the system is asymptotically stable if and only if $a_1  0$ and $a_0  0$. Here, $a_1 = 1.2  0$ and $a_0 = 0.41  0$. Therefore, the matrix $A$ is Hurwitz, meaning all its eigenvalues have negative real parts. According to Lyapunov theory for LTI systems, for any symmetric positive definite matrix $Q$, the equation $A^{\\top}P + P A = -Q$ has a unique, symmetric, positive definite solution $P$. Since $Q=I_2$ is positive definite, the problem is well-posed and a unique solution for $P$ exists and is positive definite.\n\nThe problem is objective, complete, and contains no contradictions or unrealistic data.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\nThe problem requires us to analyze the stability of the system $\\dot{x}(t) = A x(t)$ using the Lyapunov function $V(x) = x^{\\top}P x$.\n\n**Task 1: Derive and Solve for the Matrix P**\n\nFirst, we derive the Lyapunov equation from first principles. The time derivative of the Lyapunov function $V(x)$ along the system's trajectories is:\n$$ \\dot{V}(x) = \\frac{d}{dt}(x^{\\top}P x) = \\dot{x}^{\\top}P x + x^{\\top}P \\dot{x} $$\nSubstituting $\\dot{x} = Ax$:\n$$ \\dot{V}(x) = (Ax)^{\\top}P x + x^{\\top}P (Ax) $$\n$$ \\dot{V}(x) = x^{\\top}A^{\\top}P x + x^{\\top}P A x $$\n$$ \\dot{V}(x) = x^{\\top}(A^{\\top}P + PA)x $$\nThe Lyapunov stability theorem requires $\\dot{V}(x)$ to be negative definite. We enforce this by setting $A^{\\top}P + PA = -Q$, where $Q$ is a positive definite matrix. The problem specifies $Q=I_2$. This yields the continuous-time algebraic Lyapunov equation:\n$$ A^{\\top}P + PA = -I_2 $$\nNow we solve for the symmetric matrix $P = \\begin{pmatrix} p_{11}  p_{12} \\\\ p_{12}  p_{22} \\end{pmatrix}$.\nThe matrix $A$ is $A=\\begin{bmatrix}-0.7  0.3 \\\\ -0.2  -0.5\\end{bmatrix}$ and its transpose is $A^{\\top}=\\begin{bmatrix}-0.7  -0.2 \\\\ 0.3  -0.5\\end{bmatrix}$.\n\nSubstituting $A$ and $P$ into the Lyapunov equation:\n$$ \\begin{bmatrix}-0.7  -0.2 \\\\ 0.3  -0.5\\end{bmatrix}\\begin{pmatrix} p_{11}  p_{12} \\\\ p_{12}  p_{22} \\end{pmatrix} + \\begin{pmatrix} p_{11}  p_{12} \\\\ p_{12}  p_{22} \\end{pmatrix}\\begin{bmatrix}-0.7  0.3 \\\\ -0.2  -0.5\\end{bmatrix} = \\begin{pmatrix}-1  0 \\\\ 0  -1 \\end{pmatrix} $$\nPerforming the matrix multiplications:\n$$ \\begin{pmatrix} -0.7p_{11} - 0.2p_{12}  -0.7p_{12} - 0.2p_{22} \\\\ 0.3p_{11} - 0.5p_{12}  0.3p_{12} - 0.5p_{22} \\end{pmatrix} + \\begin{pmatrix} -0.7p_{11} - 0.2p_{12}  0.3p_{11} - 0.5p_{12} \\\\ -0.7p_{12} - 0.2p_{22}  0.3p_{12} - 0.5p_{22} \\end{pmatrix} = \\begin{pmatrix}-1  0 \\\\ 0  -1 \\end{pmatrix} $$\nSumming the matrices on the left-hand side gives:\n$$ \\begin{pmatrix} -1.4p_{11} - 0.4p_{12}  0.3p_{11} - 1.2p_{12} - 0.2p_{22} \\\\ 0.3p_{11} - 1.2p_{12} - 0.2p_{22}  0.6p_{12} - p_{22} \\end{pmatrix} = \\begin{pmatrix}-1  0 \\\\ 0  -1 \\end{pmatrix} $$\nEquating the corresponding entries yields a system of three linear equations for $p_{11}$, $p_{12}$, and $p_{22}$:\n1. $-1.4p_{11} - 0.4p_{12} = -1 \\implies 1.4p_{11} + 0.4p_{12} = 1 \\implies 7p_{11} + 2p_{12} = 5$\n2. $0.6p_{12} - p_{22} = -1 \\implies p_{22} = 0.6p_{12} + 1$\n3. $0.3p_{11} - 1.2p_{12} - 0.2p_{22} = 0 \\implies 3p_{11} - 12p_{12} - 2p_{22} = 0$\n\nSubstitute (2) into (3):\n$$ 3p_{11} - 12p_{12} - 2(0.6p_{12} + 1) = 0 $$\n$$ 3p_{11} - 12p_{12} - 1.2p_{12} - 2 = 0 $$\n$$ 3p_{11} - 13.2p_{12} = 2 $$\nWe now have a system of two equations for $p_{11}$ and $p_{12}$:\n(I) $7p_{11} + 2p_{12} = 5$\n(II) $3p_{11} - 13.2p_{12} = 2$\n\nFrom (I), $p_{11} = \\frac{5 - 2p_{12}}{7}$. Substitute this into (II):\n$$ 3\\left(\\frac{5 - 2p_{12}}{7}\\right) - 13.2p_{12} = 2 $$\nMultiply by $7$ to clear the denominator:\n$$ 3(5 - 2p_{12}) - 7(13.2)p_{12} = 14 $$\n$$ 15 - 6p_{12} - 92.4p_{12} = 14 $$\n$$ 1 = 98.4p_{12} \\implies p_{12} = \\frac{1}{98.4} = \\frac{10}{984} = \\frac{5}{492} $$\nNow we find $p_{11}$:\n$$ p_{11} = \\frac{5 - 2(\\frac{5}{492})}{7} = \\frac{5 - \\frac{10}{492}}{7} = \\frac{\\frac{5 \\times 492 - 10}{492}}{7} = \\frac{2460 - 10}{492 \\times 7} = \\frac{2450}{3444} = \\frac{350}{492} = \\frac{175}{246} $$\nFinally, we find $p_{22}$:\n$$ p_{22} = 0.6p_{12} + 1 = \\frac{6}{10} \\times \\frac{5}{492} + 1 = \\frac{30}{4920} + 1 = \\frac{3}{492} + 1 = \\frac{1}{164} + 1 = \\frac{165}{164} $$\nThe matrix $P$ is:\n$$ P = \\begin{pmatrix} \\frac{175}{246}  \\frac{5}{492} \\\\ \\frac{5}{492}  \\frac{165}{164} \\end{pmatrix} $$\nTo work with a common denominator, we use $492 = 2 \\times 246 = 3 \\times 164$.\n$$ P = \\begin{pmatrix} \\frac{350}{492}  \\frac{5}{492} \\\\ \\frac{5}{492}  \\frac{495}{492} \\end{pmatrix} = \\frac{1}{492}\\begin{pmatrix} 350  5 \\\\ 5  495 \\end{pmatrix} $$\n\n**Task 2: Justify Asymptotic Stability**\n\nAccording to Lyapunov's direct method, the equilibrium point $x=0$ of the system $\\dot{x}=Ax$ is globally asymptotically stable if there exists a scalar function $V(x)$ such that:\n1. $V(x)$ is positive definite, i.e., $V(0) = 0$ and $V(x)  0$ for all $x \\neq 0$.\n2. $\\dot{V}(x)$ is negative definite, i.e., $\\dot{V}(x)  0$ for all $x \\neq 0$.\n\nFor our chosen Lyapunov function candidate $V(x) = x^{\\top}P x$, the first condition requires the matrix $P$ to be positive definite ($P0$). The second condition requires its time derivative $\\dot{V}(x)$ to be negative definite. As shown previously, $\\dot{V}(x) = x^{\\top}(A^{\\top}P + PA)x$. By solving $A^{\\top}P + PA = -I$, we have enforced that $\\dot{V}(x) = -x^{\\top}I x = -x^{\\top}x = -\\|x\\|_2^2$. Since $\\|x\\|_2^2  0$ for any $x \\neq 0$, $\\dot{V}(x)$ is negative definite.\n\nThus, proving the asymptotic stability of the origin is reduced to certifying that the solution matrix $P$ is positive definite. A symmetric matrix is positive definite if and only if all its eigenvalues are strictly positive. The existence of such a matrix $P$ is a certificate of stability. This connects to the eigenvalue-based stability criterion for $A$: for an LTI system, the matrix $A$ is Hurwitz (all its eigenvalues have negative real parts) if and only if for any given symmetric positive definite matrix $Q$, there exists a unique symmetric positive definite solution $P$ to the Lyapunov equation $A^{\\top}P + PA = -Q$. Since we established that $A$ is Hurwitz, the existence of such a $P0$ is guaranteed. By explicitly finding it, we are confirming the stability predicted by the eigenvalues of $A$.\n\n**Task 3: Compute the Smallest Eigenvalue of P**\n\nTo confirm that $P$ is positive definite, we compute its eigenvalues, denoted by $\\lambda_P$. The eigenvalues are the roots of the characteristic equation $\\det(P - \\lambda_P I) = 0$.\nIt is easier to first find the eigenvalues $\\mu$ of the scaled matrix $P' = 492P = \\begin{pmatrix} 350  5 \\\\ 5  495 \\end{pmatrix}$. The eigenvalues of $P$ will then be $\\lambda_P = \\mu / 492$.\n$$ \\det(P' - \\mu I) = \\det\\begin{pmatrix} 350 - \\mu  5 \\\\ 5  495 - \\mu \\end{pmatrix} = 0 $$\n$$ (350 - \\mu)(495 - \\mu) - 5^2 = 0 $$\n$$ \\mu^2 - (350 + 495)\\mu + (350 \\times 495) - 25 = 0 $$\n$$ \\mu^2 - 845\\mu + 173250 - 25 = 0 $$\n$$ \\mu^2 - 845\\mu + 173225 = 0 $$\nWe solve this quadratic equation for $\\mu$ using the quadratic formula:\n$$ \\mu = \\frac{-(-845) \\pm \\sqrt{(-845)^2 - 4(1)(173225)}}{2(1)} $$\n$$ \\mu = \\frac{845 \\pm \\sqrt{714025 - 692900}}{2} = \\frac{845 \\pm \\sqrt{21125}}{2} $$\nTo simplify the square root: $21125 = 25 \\times 845 = 25 \\times 5 \\times 169 = 5^3 \\times 13^2$.\n$$ \\sqrt{21125} = \\sqrt{5^2 \\times 13^2 \\times 5} = 5 \\times 13 \\sqrt{5} = 65\\sqrt{5} $$\nThe eigenvalues of $P'$ are:\n$$ \\mu = \\frac{845 \\pm 65\\sqrt{5}}{2} $$\nThe two eigenvalues of $P$ are $\\lambda_{P,1} = \\frac{845 + 65\\sqrt{5}}{2 \\times 492} = \\frac{845 + 65\\sqrt{5}}{984}$ and $\\lambda_{P,2} = \\frac{845 - 65\\sqrt{5}}{984}$.\nThe smallest eigenvalue is $\\lambda_{P,min}$:\n$$ \\lambda_{P,min} = \\frac{845 - 65\\sqrt{5}}{984} $$\nTo verify positive definiteness, we must check if this eigenvalue is positive. This is true if $845 - 65\\sqrt{5}  0$, which simplifies to $845  65\\sqrt{5}$, or $13  \\sqrt{5}$. Squaring both sides, $169  5$, which is true. Therefore, both eigenvalues are positive, $P$ is positive definite, and the origin of the system is asymptotically stable. The smallest eigenvalue is the required quantitative certificate.",
            "answer": "$$\n\\boxed{\\frac{845 - 65\\sqrt{5}}{984}}\n$$"
        },
        {
            "introduction": "Biological systems rarely operate in a single, static mode; instead, they often switch between different physiological regimes. This practice moves beyond simple LTI systems to explore the stability of such switched systems, revealing a critical and non-intuitive concept: switching between individually stable dynamics can, under certain conditions, lead to overall instability. By analyzing a periodically switching system, you will learn to use the state-transition matrix over one period to compute Floquet multipliers and determine the stability of the system as a whole, a crucial skill for modeling dynamic biological processes .",
            "id": "3930116",
            "problem": "Consider a linearized two-state biochemical regulation module that alternates between two physiological regimes, for example, two transcriptional modes driven by environmental signals. In regime $i \\in \\{1,2\\}$, the small deviation $x(t) \\in \\mathbb{R}^{2}$ from the corresponding operating point satisfies the linear, time-invariant (LTI) ordinary differential equation (ODE) $\\dot{x}(t) = A_{i} x(t)$. The system switches periodically with dwell time $\\tau  0$: regime $1$ is active on $[k(2\\tau),(k+1)\\tau)$ and regime $2$ is active on $[(k+1)\\tau,(k+2)\\tau)$ for each integer $k \\ge 0$.\n\nSuppose the regime matrices are\n$$\nA_{1} \\;=\\; \\begin{pmatrix} -\\alpha  \\beta \\\\ 0  -\\alpha \\end{pmatrix},\n\\qquad\nA_{2} \\;=\\; \\begin{pmatrix} -\\alpha  0 \\\\ \\beta  -\\alpha \\end{pmatrix},\n$$\nwith $\\alpha  0$ and $\\beta  0$. These arise, for instance, from linearization of two feedback-configurations having the same degradation rate $\\alpha$ but different cross-coupling $\\beta$.\n\nStarting only from the fundamental solution of a linear time-invariant ODE $x(t) = \\exp(A t)\\,x(0)$ and the definition of the state-transition over one switch period, do the following:\n\n- Derive a closed-form expression for the one-period transition matrix\n$$\n\\Phi(\\tau) \\;=\\; \\exp(A_{2}\\,\\tau)\\,\\exp(A_{1}\\,\\tau).\n$$\n- Show that each $A_{i}$ is Hurwitz (all eigenvalues have strictly negative real part).\n- Using your expression for $\\Phi(\\tau)$, derive an exact analytic expression for the dominant Floquet multiplier (the spectral radius) $\\rho\\big(\\Phi(\\tau)\\big)$ as a function of $\\alpha$, $\\beta$, and $\\tau$.\n- For the physiologically plausible parameter values $\\alpha = 1$ (in units of $\\text{h}^{-1}$), $\\beta = 10$ (in units of $\\text{h}^{-1}$), and $\\tau = 0.1$ (in units of $\\text{h}$), compute the numerical value of $\\rho\\big(\\Phi(\\tau)\\big)$. Express your final numerical answer as a dimensionless number, rounded to four significant figures.\n\nFinally, explain why the value you computed implies that periodic switching can be unstable even though each $A_{i}$ is Hurwitz, and conclude what this says about the existence of a common quadratic Lyapunov function for $\\{A_{1},A_{2}\\}$.\n\nYour final reported answer must be the single numerical value of $\\rho\\big(\\Phi(\\tau)\\big)$, rounded to four significant figures, and dimensionless. Do not include any units in the final reported answer.",
            "solution": "The problem requires a stability analysis of a periodically switched linear system. The first step, as per the instructions, is to validate the problem statement. The problem is a standard textbook example from the field of switched and hybrid systems, a sub-discipline of control theory and systems biology. It provides all necessary mathematical definitions, parameters, and constraints. The dynamics are described by linear ordinary differential equations, the matrices are well-defined, and the parameters have specified constraints ($\\alpha  0$, $\\beta  0$). The tasks are clear, mathematically formalizable, and lead to a unique, verifiable solution. The problem statement is scientifically grounded, well-posed, objective, and complete. It is therefore deemed valid.\n\nWe proceed with the solution by addressing each required task in sequence.\n\nFirst, we derive a closed-form expression for the one-period transition matrix $\\Phi(\\tau) = \\exp(A_{2}\\,\\tau)\\,\\exp(A_{1}\\,\\tau)$. This requires computing the matrix exponential for $A_{1}$ and $A_{2}$.\nThe matrices can be written as a sum of a scaled identity matrix and a nilpotent matrix. For $A_{1}$, we have:\n$$\nA_{1} = \\begin{pmatrix} -\\alpha  \\beta \\\\ 0  -\\alpha \\end{pmatrix} = -\\alpha \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 0  \\beta \\\\ 0  0 \\end{pmatrix} = -\\alpha I + N_{1}\n$$\nwhere $N_{1} = \\begin{pmatrix} 0  \\beta \\\\ 0  0 \\end{pmatrix}$. The matrix $N_{1}$ is nilpotent, as $N_{1}^{2} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$. Since the identity matrix $I$ commutes with any matrix, we have $(-\\alpha I)N_{1} = N_{1}(-\\alpha I)$, which allows us to write $\\exp(A_{1}\\tau) = \\exp(-\\alpha I \\tau + N_{1}\\tau) = \\exp(-\\alpha I \\tau)\\exp(N_{1}\\tau)$.\nThe exponential of a scaled identity matrix is $\\exp(-\\alpha I \\tau) = \\exp(-\\alpha \\tau)I$.\nThe exponential of the nilpotent part is found from its series definition:\n$$\n\\exp(N_{1}\\tau) = I + N_{1}\\tau + \\frac{(N_{1}\\tau)^2}{2!} + \\dots = I + N_{1}\\tau = \\begin{pmatrix} 1  \\beta\\tau \\\\ 0  1 \\end{pmatrix}\n$$\nCombining these results, we get:\n$$\n\\exp(A_{1}\\tau) = \\exp(-\\alpha\\tau) I (I + N_{1}\\tau) = \\exp(-\\alpha\\tau) \\begin{pmatrix} 1  \\beta\\tau \\\\ 0  1 \\end{pmatrix}\n$$\nSimilarly, for $A_{2}$:\n$$\nA_{2} = \\begin{pmatrix} -\\alpha  0 \\\\ \\beta  -\\alpha \\end{pmatrix} = -\\alpha I + N_{2}, \\quad \\text{where } N_{2} = \\begin{pmatrix} 0  0 \\\\ \\beta  0 \\end{pmatrix}\n$$\n$N_{2}$ is also nilpotent with $N_{2}^{2} = 0$. The calculation is analogous:\n$$\n\\exp(A_{2}\\tau) = \\exp(-\\alpha\\tau) (I + N_{2}\\tau) = \\exp(-\\alpha\\tau) \\begin{pmatrix} 1  0 \\\\ \\beta\\tau  1 \\end{pmatrix}\n$$\nNow we can compute the one-period transition matrix $\\Phi(\\tau)$:\n$$\n\\Phi(\\tau) = \\exp(A_{2}\\tau)\\exp(A_{1}\\tau) = \\left( \\exp(-\\alpha\\tau)\\begin{pmatrix} 1  0 \\\\ \\beta\\tau  1 \\end{pmatrix} \\right) \\left( \\exp(-\\alpha\\tau)\\begin{pmatrix} 1  \\beta\\tau \\\\ 0  1 \\end{pmatrix} \\right)\n$$\n$$\n\\Phi(\\tau) = \\exp(-2\\alpha\\tau) \\begin{pmatrix} 1  0 \\\\ \\beta\\tau  1 \\end{pmatrix} \\begin{pmatrix} 1  \\beta\\tau \\\\ 0  1 \\end{pmatrix} = \\exp(-2\\alpha\\tau) \\begin{pmatrix} 1  \\beta\\tau \\\\ \\beta\\tau  (\\beta\\tau)^2 + 1 \\end{pmatrix}\n$$\nThis is the closed-form expression for $\\Phi(\\tau)$.\n\nSecond, we show that each matrix $A_{i}$ is Hurwitz. A matrix is Hurwitz if all its eigenvalues have strictly negative real parts. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(A - \\lambda I)=0$.\nFor $A_{1}$, the characteristic equation is:\n$$\n\\det(A_{1}-\\lambda I) = \\det\\begin{pmatrix} -\\alpha-\\lambda  \\beta \\\\ 0  -\\alpha-\\lambda \\end{pmatrix} = (-\\alpha-\\lambda)^2=0\n$$\nThis gives a repeated eigenvalue $\\lambda_{1,2} = -\\alpha$. Since $\\alpha  0$ is given, the real part is strictly negative. Thus, $A_{1}$ is Hurwitz.\nFor $A_{2}$, the characteristic equation is:\n$$\n\\det(A_{2}-\\lambda I) = \\det\\begin{pmatrix} -\\alpha-\\lambda  0 \\\\ \\beta  -\\alpha-\\lambda \\end{pmatrix} = (-\\alpha-\\lambda)^2=0\n$$\nThis also yields a repeated eigenvalue $\\lambda_{1,2} = -\\alpha$. Since $\\alpha  0$, $A_{2}$ is also Hurwitz.\n\nThird, we derive an exact analytic expression for the dominant Floquet multiplier, which is the spectral radius $\\rho\\big(\\Phi(\\tau)\\big)$. This is the maximum of the absolute values of the eigenvalues of $\\Phi(\\tau)$. Let $\\mu$ be an eigenvalue of the matrix $M = \\begin{pmatrix} 1  \\beta\\tau \\\\ \\beta\\tau  (\\beta\\tau)^2 + 1 \\end{pmatrix}$. Then the eigenvalues of $\\Phi(\\tau)$ are $\\lambda = \\exp(-2\\alpha\\tau)\\mu$.\nLet $b = \\beta\\tau$. The characteristic equation for $M$ is $\\det(M - \\mu I) = 0$:\n$$\n\\det\\begin{pmatrix} 1-\\mu  b \\\\ b  b^2+1-\\mu \\end{pmatrix} = (1-\\mu)(b^2+1-\\mu) - b^2 = 0\n$$\n$$\n\\mu^2 - (b^2+1)\\mu - \\mu + b^2+1 - b^2 = 0\n$$\n$$\n\\mu^2 - (b^2+2)\\mu + 1 = 0\n$$\nUsing the quadratic formula to find the eigenvalues $\\mu$:\n$$\n\\mu = \\frac{(b^2+2) \\pm \\sqrt{(b^2+2)^2 - 4}}{2} = \\frac{b^2+2 \\pm \\sqrt{b^4+4b^2+4 - 4}}{2} = \\frac{b^2+2 \\pm \\sqrt{b^2(b^2+4)}}{2}\n$$\nSince $\\beta  0$ and $\\tau  0$, we have $b = \\beta\\tau  0$, so $\\sqrt{b^2}=b$.\n$$\n\\mu = \\frac{b^2+2 \\pm b\\sqrt{b^2+4}}{2}\n$$\nThe two eigenvalues, $\\mu_1$ and $\\mu_2$, are both real and positive. The larger one is:\n$$\n\\mu_{\\max} = \\frac{b^2+2 + b\\sqrt{b^2+4}}{2}\n$$\nThe eigenvalues of $\\Phi(\\tau)$ are $\\lambda_{1,2} = \\exp(-2\\alpha\\tau)\\mu_{1,2}$. Since $\\exp(-2\\alpha\\tau)  0$ and $\\mu_{\\max}  0$, the dominant Floquet multiplier is:\n$$\n\\rho(\\Phi(\\tau)) = \\lambda_{\\max} = \\exp(-2\\alpha\\tau) \\mu_{\\max} = \\exp(-2\\alpha\\tau) \\left( \\frac{(\\beta\\tau)^2+2 + \\beta\\tau\\sqrt{(\\beta\\tau)^2+4}}{2} \\right)\n$$\nThis is the exact analytic expression for the spectral radius.\n\nFourth, we compute the numerical value of $\\rho(\\Phi(\\tau))$ for $\\alpha = 1\\,\\text{h}^{-1}$, $\\beta = 10\\,\\text{h}^{-1}$, and $\\tau = 0.1\\,\\text{h}$.\nThe products $\\alpha\\tau$ and $\\beta\\tau$ are dimensionless:\n$$\n\\alpha\\tau = (1)(0.1) = 0.1\n$$\n$$\n\\beta\\tau = (10)(0.1) = 1\n$$\nSubstituting these values into the expression for $\\rho(\\Phi(\\tau))$:\n$$\n\\rho(\\Phi(\\tau)) = \\exp(-2 \\times 0.1) \\left( \\frac{1^2+2 + 1\\sqrt{1^2+4}}{2} \\right) = \\exp(-0.2) \\left( \\frac{3+\\sqrt{5}}{2} \\right)\n$$\nNumerically evaluating this expression:\n$$\n\\frac{3+\\sqrt{5}}{2} \\approx \\frac{3+2.2360679...}{2} \\approx 2.6180339...\n$$\n$$\n\\exp(-0.2) \\approx 0.8187307...\n$$\n$$\n\\rho(\\Phi(\\tau)) \\approx (0.8187307...)(2.6180339...) \\approx 2.1432549...\n$$\nRounding to four significant figures, the value is $2.143$.\n\nFinally, we explain the implications of this result.\nThe stability of the origin for the discrete-time system $x_{k+1} = \\Phi(\\tau)x_k$ is determined by the spectral radius of the transition matrix $\\Phi(\\tau)$. The system is asymptotically stable if and only if $\\rho(\\Phi(\\tau))  1$.\nWe computed $\\rho(\\Phi(\\tau)) \\approx 2.143$, which is greater than $1$. This implies that the switched linear system is unstable. The state trajectories $x(t)$ will grow unboundedly, even though the system is always operating under one of two stable dynamics (since both $A_1$ and $A_2$ are Hurwitz matrices). This phenomenon, where switching between stable subsystems leads to instability, is a hallmark of switched systems theory.\nRegarding the existence of a common quadratic Lyapunov function (CQLF), a fundamental result states that if there exists a single positive definite matrix $P$ such that $V(x) = x^T P x$ is a Lyapunov function for all subsystems simultaneously (i.e., $A_i^T P + P A_i  0$ for $i=1, 2$), then the switched system is asymptotically stable for *any* arbitrary switching signal.\nSince we have found a specific (periodic) switching signal that renders the system unstable, it proves that the system is not stable for arbitrary switching. By contraposition, this serves as a proof that a common quadratic Lyapunov function for the set of matrices $\\{A_1, A_2\\}$ does not exist.",
            "answer": "$$\\boxed{2.143}$$"
        },
        {
            "introduction": "While analytical methods are foundational, modern biomedical systems modeling often involves complex, large-scale networks where numerical approaches are indispensable. This final practice introduces you to a powerful computational framework using Linear Matrix Inequalities (LMIs) to assess stability, which is especially useful when dealing with interconnected subsystems. You will develop a program to find a \"structured\" Lyapunov function—one that respects the physiological partitioning of the system—thereby bridging the gap between abstract stability theory and its practical application to complex, modular biological models .",
            "id": "3930099",
            "problem": "Consider a continuous-time Linear Time-Invariant (LTI) system representing coupled physiological subsystems in biomedical systems modeling. Let $x(t) \\in \\mathbb{R}^n$ denote the state, and let the system dynamics be given by $\\dot{x}(t) = A x(t)$, where $A \\in \\mathbb{R}^{n \\times n}$ is a constant system matrix. Suppose that $x(t)$ is partitioned into $m$ physiological subsystems of sizes $n_1, n_2, \\dots, n_m$, so that $n = \\sum_{i=1}^m n_i$ and the matrix $A$ is conformally partitioned into $m \\times m$ blocks consistent with this physiology-motivated state decomposition.\n\nThe goal is to guarantee overall asymptotic stability using a Lyapunov function that respects subsystem sparsity. Specifically, consider a quadratic Lyapunov function $V(x) = x^\\top P x$ with a block-diagonal structure $P = \\mathrm{diag}(P_1, P_2, \\dots, P_m)$, where each $P_i \\in \\mathbb{R}^{n_i \\times n_i}$ is symmetric positive definite. Imposing a conservative but physiologically meaningful structure, take each block as a scaled identity: $P_i = \\alpha_i I_{n_i}$ with $\\alpha_i  0$. The Linear Matrix Inequality (LMI) condition for global asymptotic stability is $A^\\top P + P A \\prec 0$ together with $P \\succ 0$, while respecting the block-diagonal sparsity induced by the physiological partition.\n\nStarting from first principles of Lyapunov stability, pose this LMI for the given systems and determine whether there exist positive scalars $\\alpha_1, \\alpha_2, \\dots, \\alpha_m$ such that the LMI holds. Your task is to write a program that, for each provided test case, searches for $\\alpha_1, \\dots, \\alpha_m  0$ and decides whether the structured LMI is feasible. If feasible, output the boolean value $true$; otherwise, output the boolean value $false$.\n\nFoundational base to use:\n- Definition of continuous-time LTI systems $\\dot{x}(t) = A x(t)$.\n- Quadratic Lyapunov functions $V(x) = x^\\top P x$ with $P \\succ 0$.\n- The Lyapunov inequality $A^\\top P + P A \\prec 0$ that implies global asymptotic stability.\n\nYou must not use any pre-derived shortcut formula beyond these foundations. Express all mathematics using LaTeX notation.\n\nTest suite:\nUse the following four test cases with $m = 2$ subsystems, each of dimension $n_1 = 2$ and $n_2 = 2$, so $n = 4$. In each case, define\n$$\nA = \\begin{bmatrix}\nA_1  K_{12} \\\\\nK_{21}  A_2\n\\end{bmatrix},\n$$\nwith the blocks specified below.\n\n- Case 1 (baseline physiological coupling, expected to be stable under structured Lyapunov): \n  $$\n  A_1 = \\begin{bmatrix}\n  -0.9  0.3 \\\\\n  -0.4  -0.7\n  \\end{bmatrix}, \\quad\n  A_2 = \\begin{bmatrix}\n  -1.0  0.2 \\\\\n  0.1  -0.8\n  \\end{bmatrix}, \\quad\n  K_{12} = 0.05 I_2, \\quad\n  K_{21} = 0.02 I_2.\n  $$\n- Case 2 (strong bidirectional coupling, potentially destabilizing):\n  $\n  A_1, A_2 \\text{ as in Case 1}, \\quad\n  K_{12} = 0.9 I_2, \\quad\n  K_{21} = 0.9 I_2.\n  $\n- Case 3 (one subsystem intrinsically unstable):\n  $$\n  A_1 = \\begin{bmatrix}\n  -0.9  0.3 \\\\\n  -0.4  -0.7\n  \\end{bmatrix}, \\quad\n  A_2 = \\begin{bmatrix}\n  0.2  0.0 \\\\\n  0.0  -0.1\n  \\end{bmatrix}, \\quad\n  K_{12} = 0.1 I_2, \\quad\n  K_{21} = 0.1 I_2.\n  $$\n- Case 4 (asymmetric coupling near a boundary condition):\n  $$\n  A_1 = \\begin{bmatrix}\n  -0.9  0.3 \\\\\n  -0.4  -0.7\n  \\end{bmatrix}, \\quad\n  A_2 = \\begin{bmatrix}\n  -1.0  0.2 \\\\\n  0.1  -0.8\n  \\end{bmatrix}, \\quad\n  K_{12} = \\begin{bmatrix}\n  0.2  -0.1 \\\\\n  0.05  0.0\n  \\end{bmatrix}, \\quad\n  K_{21} = \\begin{bmatrix}\n  0.0  0.15 \\\\\n  -0.05  0.2\n  \\end{bmatrix}.\n  $$\n\nYour program must, for each case, determine whether there exist $\\alpha_1  0$ and $\\alpha_2  0$ such that $P = \\mathrm{diag}(\\alpha_1 I_2, \\alpha_2 I_2)$ satisfies $A^\\top P + P A \\prec 0$. Use a numerically sound search procedure over positive $\\alpha_i$ values that does not rely on external data. Use a strict feasibility tolerance: declare feasibility only if the largest eigenvalue of $A^\\top P + P A$ is strictly less than $-10^{-8}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true,false]\"), in the same order as the test cases above. All outputs must be booleans. No physical units, angles, or percentages are involved in this problem.",
            "solution": "The problem requires an analysis of the asymptotic stability of a continuous-time Linear Time-Invariant (LTI) system, $\\dot{x}(t) = A x(t)$, using a structured Lyapunov function.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- **System Dynamics**: $\\dot{x}(t) = A x(t)$, where $x(t) \\in \\mathbb{R}^n$ and $A \\in \\mathbb{R}^{n \\times n}$.\n- **State Partitioning**: The state vector $x(t)$ is partitioned into $m$ subsystems of sizes $n_1, n_2, \\dots, n_m$, with $n = \\sum_{i=1}^m n_i$.\n- **Lyapunov Function Candidate**: $V(x) = x^\\top P x$.\n- **Structure of P**: $P$ is block-diagonal, $P = \\mathrm{diag}(P_1, P_2, \\dots, P_m)$, with $P_i \\in \\mathbb{R}^{n_i \\times n_i}$ being symmetric positive definite.\n- **Specific Structure of $P_i$**: $P_i = \\alpha_i I_{n_i}$ for some positive scalars $\\alpha_i  0$.\n- **Stability Condition**: The Linear Matrix Inequality (LMI) $A^\\top P + P A \\prec 0$ (negative definite).\n- **Task**: For given system matrices $A$, determine if there exist scalars $\\alpha_1, \\dots, \\alpha_m  0$ such that the LMI is satisfied.\n- **Test Cases**: Four specific $4 \\times 4$ matrices $A$ are provided, with $m=2$, $n_1=2$, $n_2=2$.\n- **Numerical Criterion**: Feasibility is declared if the largest eigenvalue of $A^\\top P + P A$ is strictly less than $-10^{-8}$.\n\n**1.2. Validation Using Extracted Givens**\n- **Scientifically Grounded**: The problem is firmly rooted in Lyapunov stability theory, a cornerstone of modern control theory and dynamical systems analysis, particularly relevant to biomedical systems modeling. The concepts of LTI systems, quadratic Lyapunov functions, and LMIs are standard and well-established.\n- **Well-Posed**: The problem is mathematically well-defined. It asks for the feasibility of an LMI with respect to a set of positive parameters. This is a convex feasibility problem, which is known to have a well-defined solution (either feasible or not).\n- **Objective**: The problem is stated using precise mathematical language and definitions. The test cases and the success criterion are specified objectively and quantitatively.\n- **Other checks**: The problem is self-contained, with all necessary information provided. It does not violate any scientific principles, contains no contradictions, and is not trivial.\n\n**1.3. Verdict and Action**\nThe problem is deemed **valid**. It is a standard, well-posed problem in stability analysis. A full solution will be provided.\n\n### Step 2: Derivation of the Solution\n\nThe foundation of the analysis is Lyapunov's second method for stability. For an autonomous system $\\dot{x} = f(x)$, if there exists a continuously differentiable function $V(x)$, called a Lyapunov function, such that:\n1. $V(x)  0$ for all $x \\neq 0$ and $V(0)=0$ (positive definite).\n2. $\\dot{V}(x)  0$ for all $x \\neq 0$ (negative definite time derivative along system trajectories).\nThen, the origin is a globally asymptotically stable equilibrium point.\n\nFor the given LTI system $\\dot{x}(t) = A x(t)$, we consider the quadratic Lyapunov function candidate $V(x) = x^\\top P x$.\n\nThe first condition, $V(x)  0$ for $x \\neq 0$, requires the matrix $P$ to be positive definite, denoted as $P \\succ 0$. The problem specifies the structure of $P$ as $P = \\mathrm{diag}(\\alpha_1 I_{n_1}, \\alpha_2 I_{n_2}, \\dots, \\alpha_m I_{n_m})$, where $I_{n_i}$ is the $n_i \\times n_i$ identity matrix. The eigenvalues of this block-diagonal matrix $P$ are the eigenvalues of its blocks $\\alpha_i I_{n_i}$. The eigenvalues of $\\alpha_i I_{n_i}$ are all equal to $\\alpha_i$. Therefore, for $P$ to be positive definite, all its eigenvalues must be positive, which requires $\\alpha_i  0$ for all $i=1, \\dots, m$. This is given as a constraint in the problem, so the first Lyapunov condition is satisfied by construction.\n\nThe second condition requires analyzing the time derivative of $V(x)$:\n$$\n\\dot{V}(x) = \\frac{d}{dt}(x^\\top P x) = \\dot{x}^\\top P x + x^\\top P \\dot{x}\n$$\nSubstituting the system dynamics $\\dot{x} = A x$:\n$$\n\\dot{V}(x) = (A x)^\\top P x + x^\\top P (A x) = x^\\top A^\\top P x + x^\\top P A x = x^\\top (A^\\top P + P A) x\n$$\nThe condition $\\dot{V}(x)  0$ for all $x \\neq 0$ is equivalent to the matrix $Q(\\alpha_1, \\dots, \\alpha_m) = A^\\top P + P A$ being negative definite, denoted as $A^\\top P + P A \\prec 0$. This is the LMI we must test for feasibility.\n\nLet the system matrix $A$ be partitioned conformally with the state subsystems:\n$$\nA = \\begin{bmatrix}\nA_{11}  A_{12}  \\dots  A_{1m} \\\\\nA_{21}  A_{22}  \\dots  A_{2m} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\nA_{m1}  A_{m2}  \\dots  A_{mm}\n\\end{bmatrix}\n$$\nwhere $A_{ij} \\in \\mathbb{R}^{n_i \\times n_j}$.\nSubstituting the structured $P = \\mathrm{diag}(\\alpha_1 I_{n_1}, \\dots, \\alpha_m I_{n_m})$ into the LMI gives:\n$$\nA^\\top P + PA = \\begin{bmatrix}\nA_{11}^\\top  \\dots  A_{m1}^\\top \\\\\n\\vdots  \\ddots  \\vdots \\\\\nA_{1m}^\\top  \\dots  A_{mm}^\\top\n\\end{bmatrix}\n\\begin{bmatrix}\n\\alpha_1 I_{n_1}   0 \\\\\n \\ddots  \\\\\n0   \\alpha_m I_{n_m}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\alpha_1 I_{n_1}   0 \\\\\n \\ddots  \\\\\n0   \\alpha_m I_{n_m}\n\\end{bmatrix}\n\\begin{bmatrix}\nA_{11}  \\dots  A_{1m} \\\\\n\\vdots  \\ddots  \\vdots \\\\\nA_{m1}  \\dots  A_{mm}\n\\end{bmatrix}\n$$\nThe $(i, j)$-th block of the resulting matrix $Q$ is given by $Q_{ij} = \\alpha_j A_{ji}^\\top + \\alpha_i A_{ij}$. The full matrix is:\n$$\nQ(\\alpha_1, \\dots, \\alpha_m) = \\begin{bmatrix}\n\\alpha_1(A_{11}^\\top + A_{11})  \\alpha_2 A_{21}^\\top + \\alpha_1 A_{12}  \\dots \\\\\n\\alpha_1 A_{12}^\\top + \\alpha_2 A_{21}  \\alpha_2(A_{22}^\\top + A_{22})  \\dots \\\\\n\\vdots  \\vdots  \\ddots\n\\end{bmatrix}\n$$\nThe task is to find if there exists a set of positive scalars $\\{\\alpha_i\\}_{i=1}^m$ that makes this symmetric matrix $Q$ negative definite. A matrix is negative definite if and only if all its eigenvalues are strictly negative.\n\nThe LMI is homogeneous in the variables $\\alpha_i$. That is, if $Q(\\alpha_1, \\dots, \\alpha_m) \\prec 0$, then for any scalar $k  0$, $Q(k\\alpha_1, \\dots, k\\alpha_m) = k \\cdot Q(\\alpha_1, \\dots, \\alpha_m) \\prec 0$ as well. This property allows us to reduce the dimensionality of the search space. We can fix one variable, say $\\alpha_1=1$, and search over the ratios $\\rho_i = \\alpha_{i+1} / \\alpha_1$ for $i=1, \\dots, m-1$.\n\nFor the specific test cases, $m=2$. We need to find if there exist $\\alpha_1  0$ and $\\alpha_2  0$ such that the LMI holds. We set $\\alpha_1=1$ and search for a single positive ratio $\\rho = \\alpha_2  0$. The partitioned matrix $A$ is given as $A = \\begin{bmatrix} A_1  K_{12} \\\\ K_{21}  A_2 \\end{bmatrix}$. The LMI becomes finding $\\rho  0$ such that:\n$$\nQ(1, \\rho) = \\begin{bmatrix}\nA_1^\\top + A_1  \\rho K_{21}^\\top + K_{12} \\\\\nK_{12}^\\top + \\rho K_{21}  \\rho(A_2^\\top + A_2)\n\\end{bmatrix} \\prec 0\n$$\nTo solve this, we can define an objective function $f(\\rho) = \\lambda_{\\max}(Q(1, \\rho))$, where $\\lambda_{\\max}(\\cdot)$ denotes the maximum eigenvalue. Since $Q(1, \\rho)$ is a symmetric matrix whose entries are affine functions of $\\rho$, the function $f(\\rho)$ is convex. We can therefore use a numerical optimization algorithm to find the minimum of $f(\\rho)$ over $\\rho  0$. If the minimum value of $f(\\rho)$ is less than the specified tolerance of $-10^{-8}$, the LMI is feasible, and stability can be guaranteed with this Lyapunov function structure. Otherwise, it is not.\n\nThe algorithm is as follows for a given test case:\n1. Define the objective function $f(\\rho) = \\lambda_{\\max}(Q(1, \\rho))$, where $\\rho  0$.\n2. Use a numerical optimizer (e.g., `scipy.optimize.minimize_scalar`) to find $\\min_{\\rho  0} f(\\rho)$.\n3. If the resulting minimum value is less than $-10^{-8}$, the system is certifiably stable with the structured Lyapunov function, and the result is `true`. Otherwise, the result is `false`.\n\nA necessary (but not sufficient in general) condition for the global matrix $Q$ to be negative definite is that its diagonal blocks must be negative definite. That is, $\\alpha_i(A_{ii}^\\top + A_{ii}) \\prec 0$. Since $\\alpha_i  0$, this means $A_{ii}^\\top + A_{ii}$ must be negative definite. If for any subsystem $i$, the matrix $A_{ii}^\\top + A_{ii}$ is not negative definite, then $Q$ cannot be negative definite, and the LMI is infeasible. This provides a quick check, as observed in Case 3 where $A_2^\\top + A_2$ is not negative definite, immediately implying infeasibility.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Solves for the feasibility of a structured Lyapunov-based stability condition\n    for a set of LTI systems.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (A1, A2, K12, K21).\n    test_cases = [\n        # Case 1: Baseline physiological coupling\n        (\n            np.array([[-0.9, 0.3], [-0.4, -0.7]]),\n            np.array([[-1.0, 0.2], [0.1, -0.8]]),\n            0.05 * np.eye(2),\n            0.02 * np.eye(2),\n        ),\n        # Case 2: Strong bidirectional coupling\n        (\n            np.array([[-0.9, 0.3], [-0.4, -0.7]]),\n            np.array([[-1.0, 0.2], [0.1, -0.8]]),\n            0.9 * np.eye(2),\n            0.9 * np.eye(2),\n        ),\n        # Case 3: One subsystem intrinsically unstable\n        (\n            np.array([[-0.9, 0.3], [-0.4, -0.7]]),\n            np.array([[0.2, 0.0], [0.0, -0.1]]),\n            0.1 * np.eye(2),\n            0.1 * np.eye(2),\n        ),\n        # Case 4: Asymmetric coupling near a boundary condition\n        (\n            np.array([[-0.9, 0.3], [-0.4, -0.7]]),\n            np.array([[-1.0, 0.2], [0.1, -0.8]]),\n            np.array([[0.2, -0.1], [0.05, 0.0]]),\n            np.array([[0.0, 0.15], [-0.05, 0.2]]),\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        is_feasible = check_lmi_feasibility(case)\n        results.append(str(is_feasible).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef check_lmi_feasibility(case_params):\n    \"\"\"\n    Checks if there exists a ratio rho  0 such that the LMI A'P + PA  0 holds.\n    \n    The problem is to find alpha_1, alpha_2  0. Due to homogeneity, we can fix\n    alpha_1 = 1 and search for the ratio rho = alpha_2 / alpha_1  0.\n    \n    The function to minimize is the maximum eigenvalue of the matrix Q(rho),\n    which is a convex function of rho.\n    \"\"\"\n    A1, A2, K12, K21 = case_params\n    \n    # Pre-calculate constant parts of the Q matrix\n    Q11_block = A1.T + A1\n    A2_sym_part = A2.T + A2\n\n    def objective_function(rho):\n        \"\"\"\n        Calculates the maximum eigenvalue of the matrix Q for a given rho.\n        \n        Q(rho) = [ A1'+A1,       rho*K21' + K12 ]\n                 [ K12'+rho*K21,  rho*(A2'+A2)    ]\n        \"\"\"\n        if rho = 0:\n            return np.inf # We are searching for rho  0\n\n        # Construct the matrix Q(rho)\n        Q12_block = rho * K21.T + K12\n        Q21_block = K12.T + rho * K21\n        Q22_block = rho * A2_sym_part\n        \n        Q = np.block([\n            [Q11_block, Q12_block],\n            [Q21_block, Q22_block]\n        ])\n        \n        # We need the maximum eigenvalue of this symmetric matrix.\n        # eigvalsh is efficient for symmetric/Hermitian matrices.\n        max_eigenvalue = np.max(np.linalg.eigvalsh(Q))\n        \n        return max_eigenvalue\n\n    # Search for the optimal rho that minimizes the maximum eigenvalue.\n    # The search range for the ratio rho is chosen to be wide enough\n    # to cover typical physiological parameter ratios.\n    # 'bounded' method is suitable for a single-variable search in a range.\n    res = minimize_scalar(\n        objective_function,\n        bounds=(1e-6, 1e6),\n        method='bounded'\n    )\n    \n    # The minimum value found by the optimizer corresponds to the \"most negative\"\n    # the maximum eigenvalue can be made by choosing the best rho.\n    min_max_eigenvalue = res.fun\n    \n    # Check if this value meets the strict negativity condition.\n    feasibility_tolerance = -1e-8\n    \n    return min_max_eigenvalue  feasibility_tolerance\n\nsolve()\n```"
        }
    ]
}