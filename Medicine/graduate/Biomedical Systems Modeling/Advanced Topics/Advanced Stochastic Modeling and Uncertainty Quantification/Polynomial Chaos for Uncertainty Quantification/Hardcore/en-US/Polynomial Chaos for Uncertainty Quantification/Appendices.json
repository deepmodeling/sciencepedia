{
    "hands_on_practices": [
        {
            "introduction": "Polynomial Chaos Expansions are built upon a foundation of orthogonal polynomials tailored to the probability distribution of the uncertain inputs. This first exercise takes you back to first principles, guiding you through the construction of the essential building blocks for a Gaussian random variable—the orthonormal Hermite polynomials . By applying the Gram-Schmidt process, you will gain a concrete understanding of what it means for a basis to be orthonormal with respect to a probability measure, a concept that is central to the entire PCE methodology.",
            "id": "3920442",
            "problem": "In a stochastic biomedical systems model of a pharmacokinetic compartment, consider a single uncertain standardized physiological parameter $X$ that is modeled as a standard normal random variable with probability density function $w(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$. In the Polynomial Chaos Expansion (PCE) method for uncertainty quantification, one constructs an orthonormal polynomial basis with respect to the weighted inner product defined by the probability density function (PDF), so that basis functions have unit norm and zero pairwise inner products.\n\nStarting from first principles, use the weighted inner product \n$$\\langle f, g \\rangle = \\int_{-\\infty}^{\\infty} f(x)\\,g(x)\\,w(x)\\,dx,$$\nand the Gram–Schmidt orthonormalization applied to the monomials $1$, $x$, and $x^{2}$, to construct the first three orthonormal Hermite polynomials up to quadratic degree under the weight $w(x)$. Then, confirm orthonormality by computing each of the six integrals $\\langle \\psi_{i}, \\psi_{j} \\rangle$ for $i,j \\in \\{0,1,2\\}$ and showing they equal $1$ when $i=j$ and $0$ otherwise, where $\\psi_{0}$, $\\psi_{1}$, and $\\psi_{2}$ denote your orthonormal polynomials.\n\nProvide the three orthonormal polynomials $\\psi_{0}(x)$, $\\psi_{1}(x)$, and $\\psi_{2}(x)$ as your final answer. Your final answer must be a single analytical expression that lists the three polynomials in a single row matrix. No numerical rounding is required, and no units are involved in the final expression.",
            "solution": "The objective is to apply the Gram-Schmidt orthonormalization process to the set of monomials $\\{u_0(x), u_1(x), u_2(x)\\} = \\{1, x, x^2\\}$ using the specified weighted inner product. Let $\\psi_k(x)$ be the resulting orthonormal polynomials.\n\nThe weighted inner product is defined as:\n$$ \\langle f, g \\rangle = \\int_{-\\infty}^{\\infty} f(x)g(x) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right) dx $$\nThis is equivalent to the expectation of the product of the functions, $\\mathbb{E}[f(X)g(X)]$, where $X$ is a standard normal random variable, $X \\sim \\mathcal{N}(0, 1)$. We will need the moments of $X$:\n-   $\\mathbb{E}[X^n] = 0$ for odd integers $n \\geq 1$.\n-   $\\mathbb{E}[X^n] = (n-1)!! = (n-1)(n-3)\\cdots(1)$ for even integers $n \\geq 0$.\nSpecifically, we will use:\n-   $\\mathbb{E}[X^0] = \\mathbb{E}[1] = 1$\n-   $\\mathbb{E}[X^1] = 0$\n-   $\\mathbb{E}[X^2] = 1$\n-   $\\mathbb{E}[X^3] = 0$\n-   $\\mathbb{E}[X^4] = 3 \\cdot 1 = 3$\n\nThe Gram-Schmidt process constructs an orthogonal set $\\{v_k(x)\\}$ first, and then normalizes it to obtain the orthonormal set $\\{\\psi_k(x)\\}$.\n\n**Step I: Construction of $\\psi_0(x)$**\n\nLet $u_0(x) = 1$. The first orthogonal polynomial $v_0(x)$ is simply $u_0(x)$.\n$$ v_0(x) = u_0(x) = 1 $$\nTo find the first orthonormal polynomial $\\psi_0(x)$, we normalize $v_0(x)$. First, we compute its squared norm:\n$$ \\|v_0\\|^2 = \\langle v_0, v_0 \\rangle = \\langle 1, 1 \\rangle = \\int_{-\\infty}^{\\infty} 1 \\cdot 1 \\cdot w(x) dx = \\mathbb{E}[1] = 1 $$\nThe norm is $\\|v_0\\| = \\sqrt{1} = 1$.\nTherefore, the first orthonormal polynomial is:\n$$ \\psi_0(x) = \\frac{v_0(x)}{\\|v_0\\|} = \\frac{1}{1} = 1 $$\n\n**Step II: Construction of $\\psi_1(x)$**\n\nLet $u_1(x) = x$. The second orthogonal polynomial $v_1(x)$ is given by:\n$$ v_1(x) = u_1(x) - \\langle u_1, \\psi_0 \\rangle \\psi_0(x) $$\nWe compute the projection term $\\langle u_1, \\psi_0 \\rangle$:\n$$ \\langle u_1, \\psi_0 \\rangle = \\langle x, 1 \\rangle = \\int_{-\\infty}^{\\infty} x \\cdot 1 \\cdot w(x) dx = \\mathbb{E}[X] = 0 $$\nThus, the second orthogonal polynomial is:\n$$ v_1(x) = x - (0)(1) = x $$\nNext, we normalize $v_1(x)$. We compute its squared norm:\n$$ \\|v_1\\|^2 = \\langle v_1, v_1 \\rangle = \\langle x, x \\rangle = \\int_{-\\infty}^{\\infty} x \\cdot x \\cdot w(x) dx = \\mathbb{E}[X^2] = 1 $$\nThe norm is $\\|v_1\\| = \\sqrt{1} = 1$.\nTherefore, the second orthonormal polynomial is:\n$$ \\psi_1(x) = \\frac{v_1(x)}{\\|v_1\\|} = \\frac{x}{1} = x $$\n\n**Step III: Construction of $\\psi_2(x)$**\n\nLet $u_2(x) = x^2$. The third orthogonal polynomial $v_2(x)$ is given by:\n$$ v_2(x) = u_2(x) - \\langle u_2, \\psi_0 \\rangle \\psi_0(x) - \\langle u_2, \\psi_1 \\rangle \\psi_1(x) $$\nWe compute the two projection terms:\n$$ \\langle u_2, \\psi_0 \\rangle = \\langle x^2, 1 \\rangle = \\int_{-\\infty}^{\\infty} x^2 \\cdot 1 \\cdot w(x) dx = \\mathbb{E}[X^2] = 1 $$\n$$ \\langle u_2, \\psi_1 \\rangle = \\langle x^2, x \\rangle = \\int_{-\\infty}^{\\infty} x^2 \\cdot x \\cdot w(x) dx = \\mathbb{E}[X^3] = 0 $$\nSubstituting these into the expression for $v_2(x)$:\n$$ v_2(x) = x^2 - (1)(1) - (0)(x) = x^2 - 1 $$\nFinally, we normalize $v_2(x)$. We compute its squared norm:\n$$ \\|v_2\\|^2 = \\langle v_2, v_2 \\rangle = \\langle x^2-1, x^2-1 \\rangle = \\int_{-\\infty}^{\\infty} (x^2-1)^2 w(x) dx = \\mathbb{E}[(X^2-1)^2] $$\nExpanding the term inside the expectation:\n$$ \\mathbb{E}[(X^2-1)^2] = \\mathbb{E}[X^4 - 2X^2 + 1] = \\mathbb{E}[X^4] - 2\\mathbb{E}[X^2] + \\mathbb{E}[1] $$\nUsing the moments we calculated earlier:\n$$ \\|v_2\\|^2 = 3 - 2(1) + 1 = 2 $$\nThe norm is $\\|v_2\\| = \\sqrt{2}$.\nTherefore, the third orthonormal polynomial is:\n$$ \\psi_2(x) = \\frac{v_2(x)}{\\|v_2\\|} = \\frac{x^2 - 1}{\\sqrt{2}} $$\n\nThe first three orthonormal Hermite polynomials are:\n-   $\\psi_0(x) = 1$\n-   $\\psi_1(x) = x$\n-   $\\psi_2(x) = \\frac{x^2 - 1}{\\sqrt{2}}$\n\n**Confirmation of Orthonormality**\n\nWe now compute the six inner products $\\langle \\psi_i, \\psi_j \\rangle$ for $i,j \\in \\{0, 1, 2\\}$ to confirm they form an orthonormal set.\n\n1.  $\\langle \\psi_0, \\psi_0 \\rangle = \\langle 1, 1 \\rangle = \\mathbb{E}[1 \\cdot 1] = \\mathbb{E}[1] = 1$.\n2.  $\\langle \\psi_1, \\psi_1 \\rangle = \\langle x, x \\rangle = \\mathbb{E}[x \\cdot x] = \\mathbb{E}[X^2] = 1$.\n3.  $\\langle \\psi_2, \\psi_2 \\rangle = \\left\\langle \\frac{x^2-1}{\\sqrt{2}}, \\frac{x^2-1}{\\sqrt{2}} \\right\\rangle = \\frac{1}{2}\\langle x^2-1, x^2-1 \\rangle = \\frac{1}{2} \\mathbb{E}[(X^2-1)^2] = \\frac{1}{2}(2) = 1$.\n4.  $\\langle \\psi_0, \\psi_1 \\rangle = \\langle 1, x \\rangle = \\mathbb{E}[1 \\cdot x] = \\mathbb{E}[X] = 0$.\n5.  $\\langle \\psi_0, \\psi_2 \\rangle = \\left\\langle 1, \\frac{x^2-1}{\\sqrt{2}} \\right\\rangle = \\frac{1}{\\sqrt{2}}\\langle 1, x^2-1 \\rangle = \\frac{1}{\\sqrt{2}} \\mathbb{E}[1 \\cdot (X^2-1)] = \\frac{1}{\\sqrt{2}}(\\mathbb{E}[X^2] - \\mathbb{E}[1]) = \\frac{1}{\\sqrt{2}}(1 - 1) = 0$.\n6.  $\\langle \\psi_1, \\psi_2 \\rangle = \\left\\langle x, \\frac{x^2-1}{\\sqrt{2}} \\right\\rangle = \\frac{1}{\\sqrt{2}}\\langle x, x^2-1 \\rangle = \\frac{1}{\\sqrt{2}} \\mathbb{E}[X(X^2-1)] = \\frac{1}{\\sqrt{2}}(\\mathbb{E}[X^3] - \\mathbb{E}[X]) = \\frac{1}{\\sqrt{2}}(0 - 0) = 0$.\n\nThe calculations confirm that $\\langle \\psi_i, \\psi_j \\rangle = \\delta_{ij}$ for $i,j \\in \\{0, 1, 2\\}$, where $\\delta_{ij}$ is the Kronecker delta. The set of polynomials is indeed orthonormal.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1  x  \\frac{x^2 - 1}{\\sqrt{2}} \\end{pmatrix}}$$"
        },
        {
            "introduction": "With an understanding of how to construct an orthonormal basis, the next step is to apply it to a model. This practice provides a clear and tangible example of performing a Polynomial Chaos Expansion on a simple linear model, a common starting point in uncertainty analysis . You will compute the PCE coefficients via projection and then use these coefficients to directly recover the model output's mean and variance, illustrating one of the most powerful features of the PCE framework.",
            "id": "3920458",
            "problem": "A linearized biomarker model in a pharmacokinetic-pharmacodynamic setting is used to represent the instantaneous blood biomarker response to a small uncertain input. Let the uncertain input be modeled as a scalar Gaussian random variable $\\xi \\sim \\mathcal{N}(0,1)$ representing standardized fluctuations in an infusion rate, and let the linearized response be $Y(\\xi) = a + b \\xi$, where $a$ is a deterministic baseline and $b$ is a deterministic sensitivity coefficient. Consider a Polynomial Chaos Expansion (PCE) using the Hermite polynomial basis associated with the Gaussian measure. Use the probabilists' Hermite polynomials $H_{n}(\\xi)$ defined by $H_{0}(\\xi) = 1$, $H_{1}(\\xi) = \\xi$, $H_{2}(\\xi) = \\xi^{2} - 1$, and define the orthonormal basis functions $\\psi_{n}(\\xi) = \\frac{H_{n}(\\xi)}{\\sqrt{n!}}$, so that $\\mathbb{E}[\\psi_{m}(\\xi)\\,\\psi_{n}(\\xi)] = \\delta_{mn}$ with respect to the standard normal measure. The inner product is $\\langle f, g \\rangle = \\mathbb{E}[f(\\xi)\\,g(\\xi)]$.\n\nCompute the first two Polynomial Chaos Expansion coefficients $c_{0}$ and $c_{1}$ in the expansion $Y(\\xi) = \\sum_{n=0}^{\\infty} c_{n}\\,\\psi_{n}(\\xi)$, and then use the expansion to verify the expressions for the mean $\\mathbb{E}[Y]$ and the variance $\\mathrm{Var}[Y]$. Express your final answer as a row matrix containing $(c_{0}, c_{1}, \\mathbb{E}[Y], \\mathrm{Var}[Y])$. No rounding is required, and no units are to be used.",
            "solution": "The goal is to compute the first two Polynomial Chaos Expansion (PCE) coefficients for the linear response $Y(\\xi) = a + b \\xi$ under the standard normal input $\\xi \\sim \\mathcal{N}(0,1)$ using an orthonormal Hermite basis, and then verify the mean and variance using the expansion.\n\nWe begin from the foundational definitions of orthogonal expansions with respect to the Gaussian measure. The probabilists' Hermite polynomials $H_{n}(\\xi)$ form an orthogonal basis under the weight associated with the standard normal distribution, and the normalized functions $\\psi_{n}(\\xi) = \\frac{H_{n}(\\xi)}{\\sqrt{n!}}$ constitute an orthonormal basis with\n$$\n\\mathbb{E}[\\psi_{m}(\\xi)\\,\\psi_{n}(\\xi)] = \\delta_{mn}.\n$$\nFor an orthonormal basis, the PCE coefficients are given by the inner products\n$$\nc_{n} = \\mathbb{E}\\big[Y(\\xi)\\,\\psi_{n}(\\xi)\\big].\n$$\n\nWe compute $c_{0}$ and $c_{1}$ explicitly. Using $H_{0}(\\xi) = 1$ and $H_{1}(\\xi) = \\xi$, we have $\\psi_{0}(\\xi) = 1$ and $\\psi_{1}(\\xi) = \\xi$.\n\n1. Coefficient $c_{0}$:\n$$\nc_{0} = \\mathbb{E}\\big[Y(\\xi)\\,\\psi_{0}(\\xi)\\big] = \\mathbb{E}\\big[(a + b \\xi)\\cdot 1\\big] = a + b\\,\\mathbb{E}[\\xi] = a + b \\cdot 0 = a.\n$$\n\n2. Coefficient $c_{1}$:\n$$\nc_{1} = \\mathbb{E}\\big[Y(\\xi)\\,\\psi_{1}(\\xi)\\big] = \\mathbb{E}\\big[(a + b \\xi)\\cdot \\xi\\big] = a\\,\\mathbb{E}[\\xi] + b\\,\\mathbb{E}[\\xi^{2}] = 0 + b \\cdot 1 = b,\n$$\nsince for $\\xi \\sim \\mathcal{N}(0,1)$, $\\mathbb{E}[\\xi] = 0$ and $\\mathbb{E}[\\xi^{2}] = 1$.\n\nTherefore, the PCE for $Y(\\xi)$ in this basis is\n$$\nY(\\xi) = c_{0}\\,\\psi_{0}(\\xi) + c_{1}\\,\\psi_{1}(\\xi) = a \\cdot 1 + b \\cdot \\xi = a + b \\xi,\n$$\nwhich matches the given linear form, as expected.\n\nNext, we verify the mean $\\mathbb{E}[Y]$ and variance $\\mathrm{Var}[Y]$ using the properties of the orthonormal basis. For a PCE in an orthonormal basis,\n$$\n\\mathbb{E}[Y] = c_{0},\n$$\nbecause $\\mathbb{E}[\\psi_{0}(\\xi)] = 1$ and $\\mathbb{E}[\\psi_{n}(\\xi)] = 0$ for all $n \\geq 1$. Thus,\n$$\n\\mathbb{E}[Y] = c_{0} = a.\n$$\n\nThe variance is given by the sum of the squares of the coefficients of the nonconstant modes:\n$$\n\\mathrm{Var}[Y] = \\sum_{n=1}^{\\infty} c_{n}^{2}.\n$$\nIn our case, all coefficients beyond $n=1$ vanish because $Y(\\xi)$ is linear. Hence,\n$$\n\\mathrm{Var}[Y] = c_{1}^{2} = b^{2}.\n$$\n\nWe have verified that the PCE recovers the correct mean and variance of the linear Gaussian model:\n$$\n\\mathbb{E}[Y] = a, \\qquad \\mathrm{Var}[Y] = b^{2}.\n$$\n\nAs requested, we present the final results as a row matrix $(c_{0}, c_{1}, \\mathbb{E}[Y], \\mathrm{Var}[Y])$.",
            "answer": "$$\\boxed{\\begin{pmatrix} a  b  a  b^{2} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Many real-world systems, especially in biomedicine, are dynamic and described by differential equations with uncertain parameters. This advanced practice demonstrates how to apply the PCE framework to such systems using the intrusive stochastic Galerkin method . By projecting the governing stochastic ODE onto the polynomial basis, you will derive a coupled system of deterministic ODEs for the time-varying PCE coefficients, providing a powerful method for propagating uncertainty through time.",
            "id": "3920496",
            "problem": "Consider a one-compartment pharmacokinetic (PK) model for an intravenous bolus dose in which the plasma concentration $C(t,\\xi)$ of a drug evolves according to the linear ordinary differential equation (ODE)\n$$\n\\frac{dC}{dt} = -\\,k(\\xi)\\,C,\n$$\nwhere the first-order elimination rate $k(\\xi)$ is uncertain and modeled as an affine function of a single standard normal random variable $\\xi \\sim \\mathcal{N}(0,1)$, namely $k(\\xi) = k_{0} + k_{1}\\,\\xi$, with $k_{0}  0$ and $k_{1} \\in \\mathbb{R}$ constants. Assume a deterministic initial concentration $C(0,\\xi) = C_{0}$ with $C_{0}  0$.\n\nUse a polynomial chaos expansion (PCE) with an orthonormal Hermite basis adapted to the standard Gaussian measure. Truncate the expansion at total order $2$ so that\n$$\nC(t,\\xi) \\approx \\sum_{n=0}^{2} c_{n}(t)\\,\\psi_{n}(\\xi),\n$$\nwhere $\\{\\psi_{n}\\}_{n=0}^{2}$ are the first three orthonormal probabilists’ Hermite polynomials with respect to the weight of $\\xi \\sim \\mathcal{N}(0,1)$, explicitly\n$$\n\\psi_{0}(\\xi) = 1,\\quad \\psi_{1}(\\xi) = \\xi,\\quad \\psi_{2}(\\xi) = \\frac{\\xi^{2}-1}{\\sqrt{2}}.\n$$\nApply the stochastic Galerkin method: multiply the governing ODE by each test function $\\psi_{m}(\\xi)$ for $m \\in \\{0,1,2\\}$ and take the expectation with respect to the law of $\\xi$, using the orthonormality of $\\{\\psi_{n}\\}_{n=0}^{2}$ under the Gaussian weight.\n\nStarting only from the definitions above and fundamental properties of orthonormal Hermite polynomials and expectations under the Gaussian measure, derive the closed ODE system for the coefficient vector $\\mathbf{c}(t) = \\big(c_{0}(t),c_{1}(t),c_{2}(t)\\big)^{\\top}$ in the form\n$$\n\\dot{\\mathbf{c}}(t) = -\\,\\mathbf{A}\\,\\mathbf{c}(t),\n$$\nand compute the explicit analytic expression for the $3\\times 3$ coupling matrix $\\mathbf{A}$ in terms of $k_{0}$ and $k_{1}$. Your final answer must be the exact analytic form of $\\mathbf{A}$ (no rounding required). Do not solve the ODE system for $\\mathbf{c}(t)$; only provide the explicit matrix $\\mathbf{A}$.",
            "solution": "The starting point is the linear pharmacokinetic ODE\n$$\n\\frac{dC}{dt} = -\\,k(\\xi)\\,C,\\quad k(\\xi) = k_{0} + k_{1}\\,\\xi,\n$$\nwith $\\xi \\sim \\mathcal{N}(0,1)$ and $C(0,\\xi) = C_{0}$. We seek a polynomial chaos expansion (PCE) of $C(t,\\xi)$ in an orthonormal Hermite basis adapted to the Gaussian measure. With truncation at total order $2$, we postulate\n$$\nC(t,\\xi) \\approx \\sum_{n=0}^{2} c_{n}(t)\\,\\psi_{n}(\\xi),\n$$\nwhere the basis functions are\n$$\n\\psi_{0}(\\xi) = 1,\\quad \\psi_{1}(\\xi) = \\xi,\\quad \\psi_{2}(\\xi) = \\frac{\\xi^{2}-1}{\\sqrt{2}}.\n$$\nBy construction of the orthonormal Hermite basis for the standard Gaussian measure,\n$$\n\\mathbb{E}\\!\\left[\\psi_{m}(\\xi)\\,\\psi_{n}(\\xi)\\right] = \\delta_{mn}\\quad \\text{for all } m,n \\in \\{0,1,2\\},\n$$\nwhere $\\delta_{mn}$ is the Kronecker delta and $\\mathbb{E}[\\cdot]$ denotes expectation with respect to $\\xi \\sim \\mathcal{N}(0,1)$.\n\nThe stochastic Galerkin method requires that the residual be orthogonal to each basis function. Multiplying the ODE by $\\psi_{m}(\\xi)$ and taking expectations yields, for each $m \\in \\{0,1,2\\}$,\n$$\n\\mathbb{E}\\!\\left[\\psi_{m}(\\xi)\\,\\frac{d}{dt}\\left(\\sum_{n=0}^{2} c_{n}(t)\\,\\psi_{n}(\\xi)\\right)\\right] = -\\,\\mathbb{E}\\!\\left[\\psi_{m}(\\xi)\\,k(\\xi)\\,\\sum_{n=0}^{2} c_{n}(t)\\,\\psi_{n}(\\xi)\\right].\n$$\nSince $c_{n}(t)$ depends only on $t$, differentiation with respect to $t$ passes through expectation, and orthonormality gives\n$$\n\\frac{d c_{m}}{dt} = -\\,\\sum_{n=0}^{2} c_{n}(t)\\,\\mathbb{E}\\!\\left[k(\\xi)\\,\\psi_{n}(\\xi)\\,\\psi_{m}(\\xi)\\right].\n$$\nSubstituting $k(\\xi) = k_{0} + k_{1}\\,\\xi$,\n$$\n\\frac{d c_{m}}{dt} = -\\,\\sum_{n=0}^{2} c_{n}(t)\\,\\left(k_{0}\\,\\mathbb{E}\\!\\left[\\psi_{n}(\\xi)\\,\\psi_{m}(\\xi)\\right] + k_{1}\\,\\mathbb{E}\\!\\left[\\xi\\,\\psi_{n}(\\xi)\\,\\psi_{m}(\\xi)\\right]\\right).\n$$\nUsing orthonormality, $\\mathbb{E}[\\psi_{n}\\psi_{m}] = \\delta_{mn}$, the system becomes\n$$\n\\frac{d c_{m}}{dt} = -\\,k_{0}\\,c_{m}(t)\\;-\\;k_{1}\\,\\sum_{n=0}^{2} c_{n}(t)\\,G_{mn},\n$$\nwhere the coupling coefficients are the Gaussian triple products\n$$\nG_{mn} := \\mathbb{E}\\!\\left[\\xi\\,\\psi_{n}(\\xi)\\,\\psi_{m}(\\xi)\\right].\n$$\n\nTo compute $G_{mn}$ explicitly, we use a fundamental property of orthonormal probabilists’ Hermite polynomials: the three-term recurrence under multiplication by $\\xi$,\n$$\n\\xi\\,\\psi_{n}(\\xi) = \\sqrt{n+1}\\,\\psi_{n+1}(\\xi) + \\sqrt{n}\\,\\psi_{n-1}(\\xi),\n$$\nwith the convention that $\\psi_{-1} \\equiv 0$. Taking expectations with $\\psi_{m}$ and using orthonormality yields\n$$\n\\mathbb{E}\\!\\left[\\xi\\,\\psi_{n}(\\xi)\\,\\psi_{m}(\\xi)\\right] \\;=\\; \\sqrt{n+1}\\,\\mathbb{E}\\!\\left[\\psi_{n+1}(\\xi)\\,\\psi_{m}(\\xi)\\right] + \\sqrt{n}\\,\\mathbb{E}\\!\\left[\\psi_{n-1}(\\xi)\\,\\psi_{m}(\\xi)\\right],\n$$\nso that\n$$\nG_{mn} = \\sqrt{n+1}\\,\\delta_{m,n+1} + \\sqrt{n}\\,\\delta_{m,n-1}.\n$$\nTherefore, for the truncation $n,m \\in \\{0,1,2\\}$, the only nonzero entries are the sub- and super-diagonal elements:\n$$\nG_{01} = \\sqrt{1} = 1,\\quad G_{10} = \\sqrt{1} = 1,\\quad G_{12} = \\sqrt{2},\\quad G_{21} = \\sqrt{2},\n$$\nand all other $G_{mn}$ vanish. In matrix form,\n$$\n\\mathbf{G} \\;=\\; \\begin{pmatrix}\n0  1  0 \\\\\n1  0  \\sqrt{2} \\\\\n0  \\sqrt{2}  0\n\\end{pmatrix}.\n$$\n\nCollecting terms for the coefficient vector $\\mathbf{c}(t) = \\big(c_{0}(t),c_{1}(t),c_{2}(t)\\big)^{\\top}$, we obtain the closed linear ODE system\n$$\n\\dot{\\mathbf{c}}(t) = -\\,\\mathbf{A}\\,\\mathbf{c}(t),\\quad \\mathbf{A} = k_{0}\\,\\mathbf{I}_{3} + k_{1}\\,\\mathbf{G},\n$$\nwhere $\\mathbf{I}_{3}$ is the $3\\times 3$ identity. Hence, the explicit analytic expression for the $3\\times 3$ coupling matrix is\n$$\n\\mathbf{A} \\;=\\; \\begin{pmatrix}\nk_{0}  k_{1}  0 \\\\\nk_{1}  k_{0}  k_{1}\\,\\sqrt{2} \\\\\n0  k_{1}\\,\\sqrt{2}  k_{0}\n\\end{pmatrix}.\n$$\nThis matrix exhibits the coupling structure induced by the triple products of the Hermite basis functions: the deterministic part $k_{0}$ contributes only to the diagonal, while the stochastic linear part $k_{1}\\,\\xi$ couples adjacent modes through the off-diagonal entries proportional to $1$ and $\\sqrt{2}$ that arise from the Gaussian-weighted products $\\mathbb{E}[\\xi\\,\\psi_{n}\\psi_{m}]$.",
            "answer": "$$\\boxed{\\begin{pmatrix} k_{0}  k_{1}  0 \\\\ k_{1}  k_{0}  k_{1}\\sqrt{2} \\\\ 0  k_{1}\\sqrt{2}  k_{0} \\end{pmatrix}}$$"
        }
    ]
}