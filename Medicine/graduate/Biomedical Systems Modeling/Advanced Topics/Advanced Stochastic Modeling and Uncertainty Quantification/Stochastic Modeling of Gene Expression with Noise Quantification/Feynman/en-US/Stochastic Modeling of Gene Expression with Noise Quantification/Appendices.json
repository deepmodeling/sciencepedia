{
    "hands_on_practices": [
        {
            "introduction": "We begin our exploration with the most fundamental model of gene expression, where a gene is constitutively active. This exercise asks you to analyze a simple birth-death process for messenger RNA (mRNA) molecules, governed by a constant production rate and first-order degradation. By solving the Chemical Master Equation, you will derive the exact stationary distribution of mRNA counts and its key statistical moments, providing a quantitative handle on the concept of intrinsic noise. This foundational calculation  establishes a crucial baseline—the Poisson limit—against which more complex regulatory schemes are often compared.",
            "id": "3932263",
            "problem": "Consider a single gene whose Messenger Ribonucleic Acid (mRNA) molecules are produced constitutively at a constant transcription rate $k_{m}$ and degraded independently at a first-order rate $\\gamma_{m}$ per molecule. Let $m(t)$ denote the random mRNA copy number at time $t$, modeled as a continuous-time Markov jump process on the nonnegative integers with births at rate $k_{m}$ and deaths at rate $\\gamma_{m} m(t)$. Assume the system reaches a unique stationary distribution under this linear birth-death dynamics.\n\nStarting from the Chemical Master Equation (CME), derive the stationary distribution for $m$, and from it compute the stationary mean $\\mathrm{E}[m]$, the variance $\\mathrm{Var}(m)$, and the Fano factor (defined as $\\mathrm{Var}(m)/\\mathrm{E}[m]$) of the mRNA copy number. Express your final answer as closed-form analytic expressions in terms of $k_{m}$ and $\\gamma_{m}$. Report the three quantities in the order $\\mathrm{E}[m]$, $\\mathrm{Var}(m)$, and Fano factor. The mRNA copy number is dimensionless; do not include units. No numerical approximation or rounding is required.",
            "solution": "The problem presents a linear birth-death process for the number of mRNA molecules, $m$. We are asked to derive the stationary distribution and compute its first two moments and the Fano factor.\n\nFirst, we formulate the Chemical Master Equation (CME) for the probability $P(m, t)$ of having $m$ molecules at time $t$. The process involves two reactions:\n1.  Production (birth): $\\emptyset \\xrightarrow{k_m} \\text{mRNA}$, occurring at a constant rate $k_m$.\n2.  Degradation (death): $\\text{mRNA} \\xrightarrow{\\gamma_m} \\emptyset$, occurring at a rate $\\gamma_m m$.\n\nThe CME describes the rate of change of $P(m, t)$ by accounting for the flux of probability into and out of state $m$. For a state with $m$ molecules, where $m \\ge 1$:\n- Flux in from state $m-1$ due to production: $k_m P(m-1, t)$.\n- Flux in from state $m+1$ due to degradation: $\\gamma_m (m+1) P(m+1, t)$.\n- Flux out to state $m+1$ due to production: $k_m P(m, t)$.\n- Flux out to state $m-1$ due to degradation: $\\gamma_m m P(m, t)$.\n\nCombining these terms gives the CME for $m \\ge 1$:\n$$\n\\frac{dP(m, t)}{dt} = k_m P(m-1, t) + \\gamma_m (m+1) P(m+1, t) - (k_m + \\gamma_m m) P(m, t)\n$$\nFor the boundary case $m=0$, there is no degradation, and production can only occur from a state with a negative number of molecules, which is impossible. The equation is:\n$$\n\\frac{dP(0, t)}{dt} = \\gamma_m (1) P(1, t) - k_m P(0, t)\n$$\n\nThe problem states that the system reaches a unique stationary distribution. In the stationary state, the probabilities are constant in time, so $\\frac{dP(m, t)}{dt} = 0$ for all $m$. Let $P_m$ denote the stationary probability of having $m$ molecules. The system of equations becomes:\nFor $m=0$:\n$$\n0 = \\gamma_m P_1 - k_m P_0 \\implies k_m P_0 = \\gamma_m P_1\n$$\nFor $m \\ge 1$:\n$$\n0 = k_m P_{m-1} + \\gamma_m (m+1) P_{m+1} - (k_m + \\gamma_m m) P_m\n$$\nThis set of equations is satisfied by the condition of detailed balance, where the net flux between any two adjacent states is zero at steady state. The flux from state $m-1$ to $m$ (production) must balance the flux from state $m$ to $m-1$ (degradation):\n$$\nk_m P_{m-1} = \\gamma_m m P_m \\quad \\text{for } m \\ge 1\n$$\nThis can be verified by substituting it into the stationary CME. If $k_m P_{m-1} = \\gamma_m m P_m$, then it must also hold that $k_m P_m = \\gamma_m (m+1) P_{m+1}$. Substituting these into the stationary equation for $m \\ge 1$:\n$$\n0 = (\\gamma_m m P_m) + (k_m P_m) - (k_m + \\gamma_m m) P_m = 0\n$$\nThe condition holds. The recurrence relation is therefore $P_m = \\frac{k_m}{\\gamma_m m} P_{m-1}$ for $m \\ge 1$.\n\nWe can solve this recurrence relation by iteration to express $P_m$ in terms of $P_0$:\nFor $m=1$: $P_1 = \\frac{k_m}{\\gamma_m} P_0$.\nFor $m=2$: $P_2 = \\frac{k_m}{2\\gamma_m} P_1 = \\frac{k_m}{2\\gamma_m} \\left( \\frac{k_m}{\\gamma_m} P_0 \\right) = \\frac{1}{2!} \\left( \\frac{k_m}{\\gamma_m} \\right)^2 P_0$.\nFor $m=3$: $P_3 = \\frac{k_m}{3\\gamma_m} P_2 = \\frac{k_m}{3\\gamma_m} \\left( \\frac{1}{2!} \\left( \\frac{k_m}{\\gamma_m} \\right)^2 P_0 \\right) = \\frac{1}{3!} \\left( \\frac{k_m}{\\gamma_m} \\right)^3 P_0$.\nBy induction, the general solution is:\n$$\nP_m = \\frac{1}{m!} \\left( \\frac{k_m}{\\gamma_m} \\right)^m P_0\n$$\nTo find $P_0$, we use the normalization condition that the sum of all probabilities must equal $1$:\n$$\n\\sum_{m=0}^{\\infty} P_m = 1\n$$\nSubstituting the expression for $P_m$:\n$$\n\\sum_{m=0}^{\\infty} \\frac{1}{m!} \\left( \\frac{k_m}{\\gamma_m} \\right)^m P_0 = P_0 \\sum_{m=0}^{\\infty} \\frac{1}{m!} \\left( \\frac{k_m}{\\gamma_m} \\right)^m = 1\n$$\nThe sum is the Taylor series expansion for the exponential function, $\\exp(x) = \\sum_{n=0}^{\\infty} \\frac{x^n}{n!}$. Let the parameter $\\lambda = \\frac{k_m}{\\gamma_m}$.\n$$\nP_0 \\exp(\\lambda) = 1 \\implies P_0 = \\exp(-\\lambda)\n$$\nSubstituting $P_0$ back gives the final stationary distribution:\n$$\nP_m = \\frac{\\lambda^m \\exp(-\\lambda)}{m!} \\quad \\text{where } \\lambda = \\frac{k_m}{\\gamma_m}\n$$\nThis is the probability mass function of a Poisson distribution with parameter $\\lambda$.\n\nNow we compute the required statistical properties from this distribution. For a Poisson distribution with parameter $\\lambda$, the mean and variance are well-known results:\nThe mean (expected value) of $m$ is:\n$$\n\\mathrm{E}[m] = \\lambda = \\frac{k_m}{\\gamma_m}\n$$\nThe variance of $m$ is:\n$$\n\\mathrm{Var}(m) = \\mathrm{E}[(m-\\mathrm{E}[m])^2] = \\lambda = \\frac{k_m}{\\gamma_m}\n$$\nThe Fano factor is defined as the ratio of the variance to the mean:\n$$\n\\text{Fano factor} = \\frac{\\mathrm{Var}(m)}{\\mathrm{E}[m]} = \\frac{\\lambda}{\\lambda} = 1\n$$\nThe Fano factor of $1$ is a characteristic feature of a Poisson process.\n\nThe three quantities requested are the stationary mean $\\mathrm{E}[m]$, the variance $\\mathrm{Var}(m)$, and the Fano factor, expressed in terms of $k_m$ and $\\gamma_m$.\n1. $\\mathrm{E}[m] = \\frac{k_m}{\\gamma_m}$\n2. $\\mathrm{Var}(m) = \\frac{k_m}{\\gamma_m}$\n3. Fano factor $= 1$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{k_m}{\\gamma_m} & \\frac{k_m}{\\gamma_m} & 1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "In reality, gene expression is rarely constant; it is dynamically regulated. This practice introduces the canonical two-state model, where a gene's promoter switches between active and inactive states, a key mechanism behind the 'bursty' nature of transcription. You will use the powerful analytical technique of time-scale separation to deduce the system's behavior in the limits of very fast and very slow promoter switching. This exercise  is crucial for understanding how extrinsic noise, originating from promoter dynamics, combines with intrinsic noise to shape the overall variability in mRNA levels, often leading to super-Poissonian statistics.",
            "id": "3932233",
            "problem": "Consider a canonical two-state promoter model of transcriptional regulation for a single gene. The promoter state $s(t) \\in \\{0,1\\}$ switches as a continuous-time Markov chain with transition rates $k_{\\mathrm{on}}$ from state $0$ to state $1$ and $k_{\\mathrm{off}}$ from state $1$ to state $0$. When in promoter state $i \\in \\{0,1\\}$, messenger ribonucleic acid (mRNA) molecules are synthesized as a zero-order birth process at rate $\\alpha_{i}$ and each mRNA degrades independently at rate $\\gamma$ per molecule. Let $n(t) \\in \\mathbb{N}_{0}$ denote the mRNA copy number at time $t$. The joint process $\\{(s(t),n(t))\\}_{t \\ge 0}$ is a continuous-time Markov process governed by the Chemical Master Equation (CME), where the state transitions are given by births $n \\to n+1$ at rate $\\alpha_{s(t)}$, deaths $n \\to n-1$ at rate $\\gamma n$, and promoter switching $0 \\leftrightarrow 1$ with the given rates. Assume that the process is ergodic and reaches a unique stationary distribution for all parameter values under consideration.\n\nLet $p \\equiv \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}} + k_{\\mathrm{off}}}$ denote the stationary occupancy of promoter state $1$. You will analyze the stationary mRNA count distribution in two distinct time-scale separation limits using only the CME and first principles of time-scale separation in Markov processes:\n\n1. Fast-switching regime: $k_{\\mathrm{on}}, k_{\\mathrm{off}} \\to \\infty$ while $p$ is held fixed, with $\\alpha_{0}, \\alpha_{1}$ and $\\gamma$ fixed and finite.\n2. Slow-switching regime: $k_{\\mathrm{on}}, k_{\\mathrm{off}} \\to 0$ while $p$ is held fixed, with $\\alpha_{0}, \\alpha_{1}$ and $\\gamma$ fixed and finite, and with the degradation dynamics occurring on a faster time scale than promoter switching.\n\nStarting from the CME for the joint probabilities $P_{i}(n,t)$ for $i \\in \\{0,1\\}$, argue carefully which effective reduced stochastic processes govern the stationary mRNA statistics in each regime by invoking appropriate time-scale separation reasoning. From these reduced descriptions, deduce the stationary mRNA count distributions in each limiting regime without invoking any pre-memorized formulas, and then derive the corresponding stationary Fano factor $F \\equiv \\frac{\\mathrm{Var}[n]}{\\mathbb{E}[n]}$ in closed form for each regime. Express your final answers in terms of $\\alpha_{0}$, $\\alpha_{1}$, $\\gamma$, and $p$ only.\n\nProvide your final answer as two expressions for the stationary Fano factor in a single row matrix $\\bigl[F_{\\mathrm{fast}} \\; F_{\\mathrm{slow}}\\bigr]$. No numerical approximation or rounding is required. Do not include units in the final answer.",
            "solution": "The problem statement is validated as scientifically sound, well-posed, and objective. It describes a canonical model in stochastic chemical kinetics and poses a standard, non-trivial problem in the analysis of time-scale separation in Markov processes. All necessary parameters and conditions are provided, and there are no internal contradictions. We may therefore proceed with the solution.\n\nThe state of the system is given by the pair $(s, n)$, where $s \\in \\{0, 1\\}$ is the promoter state and $n \\in \\mathbb{N}_0$ is the number of mRNA molecules. The stationary Chemical Master Equation (CME) describes the probability $P_i(n)$ of finding the system in state $(i, n)$ at steady state. The balance equations are:\n$$\n0 = \\alpha_0 P_0(n-1) + \\gamma (n+1) P_0(n+1) + k_{\\mathrm{off}} P_1(n) - (\\alpha_0 + \\gamma n + k_{\\mathrm{on}}) P_0(n)\n$$\n$$\n0 = \\alpha_1 P_1(n-1) + \\gamma (n+1) P_1(n+1) + k_{\\mathrm{on}} P_0(n) - (\\alpha_1 + \\gamma n + k_{\\mathrm{off}}) P_1(n)\n$$\nwith the understanding that $P_i(n-1)=0$ for $n=0$. We analyze the two specified limiting regimes.\n\n**1. Fast-switching regime: $k_{\\mathrm{on}}, k_{\\mathrm{off}} \\to \\infty$ with $p = \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}} + k_{\\mathrm{off}}}$ held fixed.**\n\nIn this limit, the time scale of promoter switching, which is on the order of $(k_{\\mathrm{on}} + k_{\\mathrm{off}})^{-1}$, is much shorter than the time scales of mRNA synthesis and degradation, which are on the order of $\\alpha_i^{-1}$ and $\\gamma^{-1}$ respectively. Consequently, the promoter state equilibrates almost instantaneously relative to any change in the mRNA copy number. The promoter rapidly samples its stationary distribution, which is given by $P(s=0) = 1-p$ and $P(s=1) = p$.\n\nFrom the perspective of the slower mRNA dynamics, the effective synthesis rate is the average of the two rates $\\alpha_0$ and $\\alpha_1$, weighted by the probability of the promoter being in the corresponding state. This effective rate, $\\langle\\alpha\\rangle$, is constant:\n$$\n\\langle\\alpha\\rangle = \\alpha_0 P(s=0) + \\alpha_1 P(s=1) = \\alpha_0 (1-p) + \\alpha_1 p\n$$\nThe system's dynamics for the mRNA count $n$ are thus reduced to a simple birth-death process with a constant birth rate $\\langle\\alpha\\rangle$ and a state-dependent death rate $\\gamma n$. The stationary master equation for this effective process is:\n$$\n0 = \\langle\\alpha\\rangle P(n-1) + \\gamma(n+1)P(n+1) - (\\langle\\alpha\\rangle + \\gamma n)P(n)\n$$\nThis equation implies a detailed balance condition at steady state: $\\langle\\alpha\\rangle P(n-1) = \\gamma n P(n)$. By recursion, we find:\n$$\nP(n) = \\frac{\\langle\\alpha\\rangle}{\\gamma n} P(n-1) = \\frac{(\\langle\\alpha\\rangle/\\gamma)^n}{n!} P(0)\n$$\nThe normalization condition $\\sum_{n=0}^\\infty P(n) = 1$ requires $P(0) \\sum_{n=0}^\\infty \\frac{(\\langle\\alpha\\rangle/\\gamma)^n}{n!} = P(0)\\exp(\\langle\\alpha\\rangle/\\gamma) = 1$. Thus, the stationary distribution is a Poisson distribution:\n$$\nP(n) = \\frac{(\\langle\\alpha\\rangle/\\gamma)^n}{n!} \\exp\\left(-\\frac{\\langle\\alpha\\rangle}{\\gamma}\\right)\n$$\nFor a Poisson distribution with parameter $\\lambda = \\langle\\alpha\\rangle/\\gamma$, the mean and variance are both equal to $\\lambda$.\n$$\n\\mathbb{E}[n] = \\frac{\\langle\\alpha\\rangle}{\\gamma} = \\frac{\\alpha_0(1-p) + \\alpha_1 p}{\\gamma}\n$$\n$$\n\\mathrm{Var}[n] = \\frac{\\langle\\alpha\\rangle}{\\gamma} = \\frac{\\alpha_0(1-p) + \\alpha_1 p}{\\gamma}\n$$\nThe Fano factor in the fast-switching limit, $F_{\\mathrm{fast}}$, is therefore:\n$$\nF_{\\mathrm{fast}} = \\frac{\\mathrm{Var}[n]}{\\mathbb{E}[n]} = 1\n$$\n\n**2. Slow-switching regime: $k_{\\mathrm{on}}, k_{\\mathrm{off}} \\to 0$ with $p$ held fixed and $\\gamma$ much larger than the switching rates.**\n\nIn this limit, the promoter switching events are rare compared to the lifetime of an mRNA molecule, which is $\\gamma^{-1}$. This means that for a given promoter state (either $0$ or $1$), the mRNA copy number has sufficient time to reach its own conditional stationary distribution before the promoter state changes. The overall stationary mRNA distribution is therefore a mixture of these two conditional distributions.\n\nLet $\\pi_i(n)$ be the stationary mRNA distribution conditioned on the promoter being in state $i \\in \\{0, 1\\}$.\n- If $s=0$, the mRNA dynamics are a birth-death process with birth rate $\\alpha_0$ and death rate $\\gamma n$. As shown previously, the stationary distribution $\\pi_0(n)$ is Poisson with mean $\\lambda_0 = \\alpha_0/\\gamma$.\n- If $s=1$, the mRNA dynamics are a birth-death process with birth rate $\\alpha_1$ and death rate $\\gamma n$. The stationary distribution $\\pi_1(n)$ is Poisson with mean $\\lambda_1 = \\alpha_1/\\gamma$.\n\nThe overall stationary distribution $P(n)$ is a weighted average of these two Poisson distributions, where the weights are the stationary probabilities of the promoter states, $1-p$ and $p$:\n$$\nP(n) = (1-p) \\pi_0(n) + p \\pi_1(n)\n$$\nTo find the Fano factor $F_{\\mathrm{slow}}$, we must compute the mean and variance of this mixture distribution. Let the promoter state $S$ be a random variable taking values $\\{0, 1\\}$ with probabilities $\\{1-p, p\\}$.\n\nThe mean $\\mathbb{E}[n]$ is the expectation of the conditional means:\n$$\n\\mathbb{E}[n] = \\mathbb{E}[\\mathbb{E}[n|S]] = (1-p)\\mathbb{E}[n|S=0] + p\\mathbb{E}[n|S=1] = (1-p) \\lambda_0 + p \\lambda_1\n$$\n$$\n\\mathbb{E}[n] = (1-p)\\frac{\\alpha_0}{\\gamma} + p\\frac{\\alpha_1}{\\gamma} = \\frac{\\alpha_0(1-p)+\\alpha_1 p}{\\gamma}\n$$\nThe variance $\\mathrm{Var}[n]$ is given by the law of total variance:\n$$\n\\mathrm{Var}[n] = \\mathbb{E}[\\mathrm{Var}[n|S]] + \\mathrm{Var}[\\mathbb{E}[n|S]]\n$$\nThe first term is the expectation of the conditional variances. Since the conditional distributions are Poisson, their variances are equal to their means: $\\mathrm{Var}[n|S=i] = \\lambda_i$.\n$$\n\\mathbb{E}[\\mathrm{Var}[n|S]] = (1-p)\\mathrm{Var}[n|S=0] + p\\mathrm{Var}[n|S=1] = (1-p)\\lambda_0 + p\\lambda_1 = \\mathbb{E}[n]\n$$\nThe second term is the variance of the conditional means. The random variable $\\mathbb{E}[n|S]$ takes the value $\\lambda_0$ with probability $1-p$ and $\\lambda_1$ with probability $p$. Its variance is:\n$$\n\\mathrm{Var}[\\mathbb{E}[n|S]] = \\mathbb{E}[(\\mathbb{E}[n|S])^2] - (\\mathbb{E}[\\mathbb{E}[n|S]])^2\n$$\n$$\n= \\left( (1-p)\\lambda_0^2 + p\\lambda_1^2 \\right) - \\left( (1-p)\\lambda_0 + p\\lambda_1 \\right)^2\n$$\n$$\n= (1-p)\\lambda_0^2 + p\\lambda_1^2 - \\left( (1-p)^2\\lambda_0^2 + p^2\\lambda_1^2 + 2p(1-p)\\lambda_0\\lambda_1 \\right)\n$$\n$$\n= ( (1-p)-(1-p)^2 )\\lambda_0^2 + (p-p^2)\\lambda_1^2 - 2p(1-p)\\lambda_0\\lambda_1\n$$\n$$\n= p(1-p)\\lambda_0^2 + p(1-p)\\lambda_1^2 - 2p(1-p)\\lambda_0\\lambda_1\n$$\n$$\n= p(1-p)(\\lambda_1 - \\lambda_0)^2 = p(1-p)\\left(\\frac{\\alpha_1}{\\gamma} - \\frac{\\alpha_0}{\\gamma}\\right)^2 = \\frac{p(1-p)(\\alpha_1 - \\alpha_0)^2}{\\gamma^2}\n$$\nCombining the terms, the total variance is:\n$$\n\\mathrm{Var}[n] = \\mathbb{E}[n] + \\frac{p(1-p)(\\alpha_1 - \\alpha_0)^2}{\\gamma^2}\n$$\nThe Fano factor in the slow-switching limit, $F_{\\mathrm{slow}}$, is:\n$$\nF_{\\mathrm{slow}} = \\frac{\\mathrm{Var}[n]}{\\mathbb{E}[n]} = \\frac{\\mathbb{E}[n] + \\frac{p(1-p)(\\alpha_1 - \\alpha_0)^2}{\\gamma^2}}{\\mathbb{E}[n]} = 1 + \\frac{p(1-p)(\\alpha_1 - \\alpha_0)^2}{\\gamma^2 \\mathbb{E}[n]}\n$$\nSubstituting the expression for $\\mathbb{E}[n]$:\n$$\nF_{\\mathrm{slow}} = 1 + \\frac{p(1-p)(\\alpha_1 - \\alpha_0)^2}{\\gamma^2 \\left(\\frac{\\alpha_0(1-p)+\\alpha_1 p}{\\gamma}\\right)} = 1 + \\frac{p(1-p)(\\alpha_1 - \\alpha_0)^2}{\\gamma(\\alpha_0(1-p)+\\alpha_1 p)}\n$$\nThis expression consists of the intrinsic noise term $1$ (from the Poissonian birth-death process) and an extrinsic noise term arising from the slow switching between production rates.\n\nThe two required expressions are $F_{\\mathrm{fast}} = 1$ and $F_{\\mathrm{slow}} = 1 + \\frac{p(1-p)(\\alpha_1 - \\alpha_0)^2}{\\gamma(\\alpha_0(1-p)+\\alpha_1 p)}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 1 + \\frac{p(1-p)(\\alpha_1 - \\alpha_0)^2}{\\gamma (\\alpha_0(1-p) + \\alpha_1 p)}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A key goal of systems biology is to build models that can be tested and parameterized with experimental data. This final practice bridges the gap between theory and application by tasking you with inferring the parameters of the two-state gene expression model from a simulated time-lapse microscopy dataset. You will frame the problem as a Hidden Markov Model (HMM) and derive the Expectation-Maximization (EM) algorithm to learn the underlying promoter switching and transcription rates from observed transcript counts. This advanced exercise  provides hands-on experience with a powerful statistical inference framework that is widely used to analyze real single-cell data.",
            "id": "3932244",
            "problem": "Consider a single gene whose promoter toggles between an inactive state $s(t)=0$ and an active state $s(t)=1$ as a two-state Continuous-Time Markov Chain (CTMC). The switching rates are $k_{\\text{on}}$ for $0 \\to 1$ and $k_{\\text{off}}$ for $1 \\to 0$. The generator matrix is\n$$\n\\mathbf{Q} \\;=\\; \\begin{pmatrix}\n- k_{\\text{on}} & k_{\\text{on}} \\\\\nk_{\\text{off}} & - k_{\\text{off}}\n\\end{pmatrix}.\n$$\nA time-lapse experiment records a single-cell trajectory of nascent transcription counts $\\{y_i\\}_{i=0}^{T}$ at discrete times $\\{t_i\\}_{i=0}^{T}$ with constant sampling interval $\\Delta t = t_{i+1}-t_i$. Conditional on the promoter state $s(t)$ being piecewise constant on each sampling interval, the number of initiation events $y_i$ accumulated over $[t_i, t_{i+1})$ is modeled as a Poisson random variable with mean $(\\alpha\\, s_i + \\beta)\\, \\Delta t$, where $s_i \\in \\{0,1\\}$ denotes the promoter state at time $t_i$, $\\alpha$ is the transcription initiation rate when the promoter is active, and $\\beta>0$ is a known background rate capturing baseline measurement noise and non-specific events. Assume the initial promoter state distribution equals the stationary distribution induced by the current rates, and use the CTMC transition kernel $\\mathbf{P}(\\Delta t) = \\exp(\\mathbf{Q} \\Delta t)$ between observation times.\n\nStarting from the definitions of a CTMC, the Poisson counting model, and the concept of incomplete-data likelihood in hidden Markov models, perform the following tasks:\n\n1. Write the observed-data likelihood $L(k_{\\text{on}}, k_{\\text{off}}, \\alpha \\mid y_0,\\dots,y_T)$ for the CTMC observed at discrete times under the specified Poisson emission model, expressing it as a sum over latent promoter state paths that factorizes into transition and emission contributions across sampling intervals.\n\n2. Derive an Expectation-Maximization (EM) algorithm to infer $k_{\\text{on}}$, $k_{\\text{off}}$, and $\\alpha$. In the E-step, define the required posterior quantities over latent states and transitions between observation times using forward-backward recursions with $\\mathbf{P}(\\Delta t)$. In the M-step, maximize the expected complete-data log-likelihood with respect to $k_{\\text{on}}$, $k_{\\text{off}}$, and $\\alpha$.\n\n3. Provide explicit closed-form expressions for the M-step updates of $k_{\\text{on}}$, $k_{\\text{off}}$, and $\\alpha$ in terms of the E-step posteriors $\\gamma_i(s)$ and $\\xi_i(a,b)$, where $\\gamma_i(s)$ is the posterior probability of $s_i=s$ and $\\xi_i(a,b)$ is the posterior probability of a transition $s_i=a$ to $s_{i+1}=b$. Express the final answer as analytical expressions. You do not need to round, and no numerical evaluation is required.\n\nYour final answer must be a single analytical expression or a row matrix containing the update formulas for $k_{\\text{on}}$, $k_{\\text{off}}$, and $\\alpha$.",
            "solution": "### Part 1: Observed-Data Likelihood\n\nThe system is a Hidden Markov Model (HMM) where the latent (unobserved) states are the promoter states $S = (s_0, s_1, \\dots, s_T)$ with $s_i \\in \\{0, 1\\}$, and the observations are the transcript counts $Y = (y_0, y_1, \\dots, y_T)$. The parameters to be estimated are $\\theta = (k_{\\text{on}}, k_{\\text{off}}, \\alpha)$.\n\nThe observed-data likelihood $L(\\theta \\mid Y)$ is the probability of the observations given the parameters, which is obtained by marginalizing over all possible latent state sequences:\n$$\nL(\\theta \\mid Y) = P(Y \\mid \\theta) = \\sum_{s_0, \\dots, s_T \\in \\{0,1\\}} P(Y, S \\mid \\theta)\n$$\nBy the properties of an HMM, the joint probability $P(Y, S \\mid \\theta)$ can be factorized into contributions from the initial state, the state transitions, and the emissions:\n$$\nP(Y, S \\mid \\theta) = P(s_0 \\mid \\theta) \\left( \\prod_{i=0}^{T-1} P(s_{i+1} \\mid s_i, \\theta) \\right) \\left( \\prod_{i=0}^{T} P(y_i \\mid s_i, \\theta) \\right)\n$$\nLet's define each term:\n1.  **Initial State Probability**: $P(s_0 \\mid \\theta) = \\pi_{s_0}$. The problem states this is the stationary distribution of the CTMC. The stationary distribution $\\pi = (\\pi_0, \\pi_1)$ satisfies $\\pi \\mathbf{Q} = \\mathbf{0}$, which yields $\\pi_0 = \\frac{k_{\\text{off}}}{k_{\\text{on}} + k_{\\text{off}}}$ and $\\pi_1 = \\frac{k_{\\text{on}}}{k_{\\text{on}} + k_{\\text{off}}}$.\n\n2.  **Transition Probability**: $P(s_{i+1}=b \\mid s_i=a, \\theta) = P_{ab}(\\Delta t)$. These are the elements of the matrix $\\mathbf{P}(\\Delta t) = \\exp(\\mathbf{Q}\\Delta t)$.\n\n3.  **Emission Probability**: $P(y_i \\mid s_i, \\theta) = b_{s_i}(y_i)$. This is given by the Poisson distribution:\n    $$\n    b_s(y_i) = P(y_i \\mid s_i=s) = \\frac{((\\alpha s + \\beta)\\Delta t)^{y_i}}{y_i!} \\exp(-(\\alpha s + \\beta)\\Delta t)\n    $$\n\nSubstituting these components, the observed-data likelihood is:\n$$\nL(k_{\\text{on}}, k_{\\text{off}}, \\alpha \\mid Y) = \\sum_{s_0, \\dots, s_T \\in \\{0,1\\}} \\pi_{s_0} \\left( \\prod_{i=0}^{T-1} P_{s_i, s_{i+1}}(\\Delta t) \\right) \\left( \\prod_{i=0}^{T} \\frac{((\\alpha s_i + \\beta)\\Delta t)^{y_i}}{y_i!} \\exp(-(\\alpha s_i + \\beta)\\Delta t) \\right)\n$$\nThis expression sums over the $2^{T+1}$ possible latent state paths, factoring the contribution of each path into initial, transition, and emission probabilities for each time step.\n\n### Part 2: Expectation-Maximization (EM) Algorithm Derivation\n\nThe EM algorithm is an iterative procedure to find the maximum likelihood estimate of $\\theta$. Each iteration consists of an Expectation (E) step and a Maximization (M) step. Let $\\theta^{(k)}$ be the parameter estimates at iteration $k$.\n\n**E-Step: Compute the $Q$-function**\n\nThe E-step involves computing the expectation of the complete-data log-likelihood, $\\log P(Y, S \\mid \\theta)$, with respect to the posterior distribution of the latent states given the observed data and current parameters, $P(S \\mid Y, \\theta^{(k)})$. This expectation is the $Q$-function:\n$$\nQ(\\theta \\mid \\theta^{(k)}) = E_{S \\mid Y, \\theta^{(k)}}[\\log P(Y, S \\mid \\theta)]\n$$\nThe complete-data log-likelihood is:\n$$\n\\log P(Y, S \\mid \\theta) = \\log \\pi_{s_0} + \\sum_{i=0}^{T-1} \\log P_{s_i, s_{i+1}}(\\Delta t) + \\sum_{i=0}^{T} \\log b_{s_i}(y_i)\n$$\nTaking the expectation, we get:\n$$\nQ(\\theta \\mid \\theta^{(k)}) = \\sum_{s_0, \\dots, s_T} P(S \\mid Y, \\theta^{(k)}) \\left[ \\log \\pi_{s_0} + \\sum_{i=0}^{T-1} \\log P_{s_i, s_{i+1}}(\\Delta t) + \\sum_{i=0}^{T} \\log b_{s_i}(y_i) \\right]\n$$\nThis simplifies by linearity of expectation. We need two types of posterior probabilities:\n-   $\\gamma_i(s) = P(s_i=s \\mid Y, \\theta^{(k)})$: the posterior probability of being in state $s$ at time $t_i$.\n-   $\\xi_i(a,b) = P(s_i=a, s_{i+1}=b \\mid Y, \\theta^{(k)})$: the posterior probability of transitioning from state $a$ at $t_i$ to state $b$ at $t_{i+1}$.\n\nThe $Q$-function can be written in terms of these posteriors:\n$$\nQ(\\theta \\mid \\theta^{(k)}) = \\sum_{s=0,1} \\gamma_0(s) \\log \\pi_s + \\sum_{i=0}^{T-1} \\sum_{a,b \\in \\{0,1\\}} \\xi_i(a,b) \\log P_{ab}(\\Delta t) + \\sum_{i=0}^{T} \\sum_{s=0,1} \\gamma_i(s) \\log b_s(y_i)\n$$\nThe posteriors $\\gamma_i(s)$ and $\\xi_i(a,b)$ are calculated using the **forward-backward algorithm**. Let $\\theta^{(k)}$ be the current parameters. The emission probabilities $b_s(y_i)$ and transition probabilities $P_{ab}(\\Delta t)$ are evaluated at $\\theta^{(k)}$.\n1.  **Forward pass**: computes $\\phi_i(s) = P(y_0, \\dots, y_i, s_i=s \\mid \\theta^{(k)})$.\n    -   Initialization: $\\phi_0(s) = \\pi_s^{(k)} b_s(y_0)$.\n    -   Recursion ($i=0, \\dots, T-1$): $\\phi_{i+1}(s') = \\left( \\sum_{s=0,1} \\phi_i(s) P_{s,s'}^{(k)}(\\Delta t) \\right) b_{s'}(y_{i+1})$.\n\n2.  **Backward pass**: computes $\\psi_i(s) = P(y_{i+1}, \\dots, y_T \\mid s_i=s, \\theta^{(k)})$.\n    -   Initialization: $\\psi_T(s) = 1$.\n    -   Recursion ($i=T-1, \\dots, 0$): $\\psi_i(s) = \\sum_{s'=0,1} P_{s,s'}^{(k)}(\\Delta t) b_{s'}(y_{i+1}) \\psi_{i+1}(s')$.\n\n3.  **Posteriors**:\n    -   The likelihood of the data is $L(Y \\mid \\theta^{(k)}) = \\sum_{s=0,1} \\phi_T(s)$.\n    -   $\\gamma_i(s) = \\frac{\\phi_i(s)\\psi_i(s)}{L(Y \\mid \\theta^{(k)})}$.\n    -   $\\xi_i(a,b) = \\frac{\\phi_i(a) P_{a,b}^{(k)}(\\Delta t) b_b(y_{i+1}) \\psi_{i+1}(b)}{L(Y \\mid \\theta^{(k)})}$.\n\n**M-Step: Maximize the $Q$-function**\n\nThe M-step finds the new parameter estimate $\\theta^{(k+1)}$ by maximizing $Q(\\theta \\mid \\theta^{(k)})$ with respect to $\\theta = (k_{\\text{on}}, k_{\\text{off}}, \\alpha)$. The $Q$-function can be separated into a term for the transition dynamics and a term for the emission process.\n\n### Part 3: M-Step Closed-Form Update Expressions\n\n**Update for $\\alpha$**\n\nWe maximize the part of the $Q$-function that depends on $\\alpha$, which is the emission term:\n$$\nQ_{em}(\\alpha) = \\sum_{i=0}^{T} \\sum_{s=0,1} \\gamma_i(s) \\log b_s(y_i)\n$$\n$$\nQ_{em}(\\alpha) = \\sum_{i=0}^{T} \\left( \\gamma_i(0) \\left[ y_i\\log(\\beta \\Delta t) - \\beta \\Delta t \\right] + \\gamma_i(1) \\left[ y_i\\log((\\alpha+\\beta)\\Delta t) - (\\alpha+\\beta)\\Delta t \\right] \\right) - \\sum_{i=0}^T\\sum_{s=0,1}\\gamma_i(s)\\log(y_i!)\n$$\nTo find the maximum, we differentiate with respect to $\\alpha$ and set the result to zero. Only the terms for $s=1$ depend on $\\alpha$:\n$$\n\\frac{\\partial Q_{em}}{\\partial \\alpha} = \\sum_{i=0}^{T} \\gamma_i(1) \\left[ \\frac{\\partial}{\\partial \\alpha} \\left( y_i \\log(\\alpha+\\beta) + y_i\\log(\\Delta t) - (\\alpha+\\beta)\\Delta t \\right) \\right] = 0\n$$\n$$\n\\sum_{i=0}^{T} \\gamma_i(1) \\left( \\frac{y_i}{\\alpha+\\beta} - \\Delta t \\right) = 0\n$$\nSolving for $\\alpha$:\n$$\n\\frac{1}{\\alpha+\\beta} \\sum_{i=0}^{T} y_i \\gamma_i(1) = \\Delta t \\sum_{i=0}^{T} \\gamma_i(1)\n$$\n$$\n\\alpha+\\beta = \\frac{\\sum_{i=0}^{T} y_i \\gamma_i(1)}{\\Delta t \\sum_{i=0}^{T} \\gamma_i(1)}\n$$\nThe M-step update for $\\alpha$ is:\n$$\n\\alpha^{(k+1)} = \\frac{\\sum_{i=0}^{T} y_i \\gamma_i(1)}{\\Delta t \\sum_{i=0}^{T} \\gamma_i(1)} - \\beta\n$$\n\n**Update for $k_{\\text{on}}$ and $k_{\\text{off}}$**\n\nMaximizing the transition-dependent part of the $Q$-function directly is complicated because $P_{ab}(\\Delta t)$ are complex functions of $k_{\\text{on}}$ and $k_{\\text{off}}$. A standard approach is to first find the M-step update for the discrete-time transition matrix $\\mathbf{P}$, as in a standard HMM, and then determine the CTMC rates $(k_{\\text{on}}, k_{\\text{off}})$ that are consistent with this estimated matrix over the interval $\\Delta t$.\n\n1.  **Estimate the transition matrix $\\mathbf{P}$**: The update for a general HMM transition probability $P_{ab}$ is the expected number of transitions from $a$ to $b$ divided by the expected number of times in state $a$ (from which a transition can occur).\n    $$\n    \\hat{P}_{ab} = \\frac{\\sum_{i=0}^{T-1} \\xi_i(a,b)}{\\sum_{i=0}^{T-1} \\gamma_i(a)}\n    $$\n    This gives us estimates $\\hat{P}_{01}$ and $\\hat{P}_{10}$ for the off-diagonal elements of the updated transition matrix $\\mathbf{P}^{(k+1)}$.\n\n2.  **Relate $\\mathbf{P}$ to $\\mathbf{Q}$**: The transition matrix for a two-state CTMC has eigenvalues $\\lambda_1=1$ and $\\lambda_2 = \\exp(-(k_{\\text{on}}+k_{\\text{off}})\\Delta t)$. The trace of the matrix is $\\text{Tr}(\\mathbf{P}) = \\lambda_1 + \\lambda_2 = 1+\\lambda_2$.\n    Also, $\\text{Tr}(\\mathbf{P}) = P_{00}+P_{11} = (1-P_{01}) + (1-P_{10}) = 2 - P_{01} - P_{10}$.\n    Equating these gives $\\lambda_2 = 1 - P_{01} - P_{10}$. Substituting the expression for $\\lambda_2$:\n    $$\n    \\exp(-(k_{\\text{on}}+k_{\\text{off}})\\Delta t) = 1 - P_{01} - P_{10}\n    $$\n    Taking the natural logarithm and solving for the sum of rates:\n    $$\n    k_{\\text{on}}+k_{\\text{off}} = -\\frac{1}{\\Delta t} \\ln(1 - P_{01} - P_{10})\n    $$\n    Furthermore, the off-diagonal elements of $\\mathbf{P}(\\Delta t) = \\mathbf{\\pi} + e^{\\lambda_2 \\Delta t}(\\mathbf{I}-\\mathbf{\\pi})$ give the relation $\\frac{P_{01}}{P_{10}} = \\frac{\\pi_1}{\\pi_0} = \\frac{k_{\\text{on}}}{k_{\\text{off}}}$.\n\n3.  **Solve for $k_{\\text{on}}$ and $k_{\\text{off}}$**: We have a system of two linear equations for $k_{\\text{on}}$ and $k_{\\text{off}}$:\n    $$\n    \\begin{cases} k_{\\text{on}} + k_{\\text{off}} = S \\\\ k_{\\text{on}}/k_{\\text{off}} = R \\end{cases}\n    $$\n    where $S = -\\frac{1}{\\Delta t} \\ln(1 - \\hat{P}_{01} - \\hat{P}_{10})$ and $R = \\hat{P}_{01}/\\hat{P}_{10}$.\n    Solving this system yields:\n    $$ k_{\\text{on}} = S \\frac{R}{R+1} = S \\frac{\\hat{P}_{01}/\\hat{P}_{10}}{\\hat{P}_{01}/\\hat{P}_{10}+1} = S \\frac{\\hat{P}_{01}}{\\hat{P}_{01}+\\hat{P}_{10}} $$\n    $$ k_{\\text{off}} = S \\frac{1}{R+1} = S \\frac{\\hat{P}_{10}}{\\hat{P}_{01}+\\hat{P}_{10}} $$\n    Substituting back $S$, and using $\\hat{P}_{ab}$ from step 1, we get the M-step updates:\n    $$\n    k_{\\text{on}}^{(k+1)} = \\left( -\\frac{1}{\\Delta t} \\ln\\left(1 - \\frac{\\sum \\xi_i(0,1)}{\\sum \\gamma_i(0)} - \\frac{\\sum \\xi_i(1,0)}{\\sum \\gamma_i(1)}\\right) \\right) \\frac{\\frac{\\sum \\xi_i(0,1)}{\\sum \\gamma_i(0)}}{\\frac{\\sum \\xi_i(0,1)}{\\sum \\gamma_i(0)} + \\frac{\\sum \\xi_i(1,0)}{\\sum \\gamma_i(1)}}\n    $$\n    $$\n    k_{\\text{off}}^{(k+1)} = \\left( -\\frac{1}{\\Delta t} \\ln\\left(1 - \\frac{\\sum \\xi_i(0,1)}{\\sum \\gamma_i(0)} - \\frac{\\sum \\xi_i(1,0)}{\\sum \\gamma_i(1)}\\right) \\right) \\frac{\\frac{\\sum \\xi_i(1,0)}{\\sum \\gamma_i(1)}}{\\frac{\\sum \\xi_i(0,1)}{\\sum \\gamma_i(0)} + \\frac{\\sum \\xi_i(1,0)}{\\sum \\gamma_i(1)}}\n    $$\n    where all sums are from $i=0$ to $T-1$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} -\\frac{\\ln(1 - \\hat{P}_{01} - \\hat{P}_{10})}{\\Delta t} \\frac{\\hat{P}_{01}}{\\hat{P}_{01}+\\hat{P}_{10}} & -\\frac{\\ln(1 - \\hat{P}_{01} - \\hat{P}_{10})}{\\Delta t} \\frac{\\hat{P}_{10}}{\\hat{P}_{01}+\\hat{P}_{10}} & \\frac{\\sum_{i=0}^{T} y_i \\gamma_i(1)}{\\Delta t \\sum_{i=0}^{T} \\gamma_i(1)} - \\beta \\end{pmatrix} \\text{, where } \\hat{P}_{01} = \\frac{\\sum_{i=0}^{T-1} \\xi_i(0,1)}{\\sum_{i=0}^{T-1} \\gamma_i(0)} \\text{ and } \\hat{P}_{10} = \\frac{\\sum_{i=0}^{T-1} \\xi_i(1,0)}{\\sum_{i=0}^{T-1} \\gamma_i(1)} } $$"
        }
    ]
}