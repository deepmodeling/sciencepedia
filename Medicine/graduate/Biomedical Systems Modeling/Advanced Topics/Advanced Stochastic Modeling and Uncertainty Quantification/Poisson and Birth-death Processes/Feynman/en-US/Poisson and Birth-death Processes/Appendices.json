{
    "hands_on_practices": [
        {
            "introduction": "Understanding the dynamics of a stochastic process is the first step towards mastering its application. This exercise takes you back to first principles, asking you to derive the complete time-dependent probability distribution for a pure birth process, also known as the Yule process. By solving the Kolmogorov forward equations directly, you will gain a fundamental appreciation for how the population's state probabilities evolve over time, providing a solid foundation for analyzing more complex birth-death systems. ",
            "id": "3920107",
            "problem": "A clonal population of proliferating cells in a bioreactor is modeled as a continuous-time Markov chain with states given by the cell count $X(t) \\in \\{1,2,3,\\dots\\}$ at time $t \\geq 0$. Each cell divides independently with a constant per-cell division rate $\\lambda  0$, and there is no cell death. Thus, when the population is in state $n$, the total birth rate is $\\lambda n$, and there are no transitions to lower states. Assume the initial condition $X(0)=1$.\n\nStarting only from the Kolmogorov forward equations for birth-death processes and fundamental definitions of continuous-time Markov chains, derive the transient probability distribution of this pure birth process. Specifically, for an arbitrary integer $n \\geq 1$, determine a closed-form expression for the probability $P(X(t)=n \\mid X(0)=1)$ as a function of $\\lambda$ and $t$.\n\nYour final answer must be a single closed-form analytic expression in terms of $\\lambda$, $t$, and $n$. Do not provide an inequality or an equation to be solved. No rounding is required, and no units need be reported because the answer is a probability.",
            "solution": "The problem describes a pure birth process, also known as a Yule process, starting with a single individual. Let $P_n(t)$ be the probability that the population size is $n$ at time $t$, given that it was $1$ at time $t=0$. That is, $P_n(t) = P(X(t)=n \\mid X(0)=1)$. The state space is $\\{1, 2, 3, \\dots\\}$. The per-cell division rate is $\\lambda$, so the total birth rate when the population size is $n$ is $\\lambda_n = n\\lambda$. The death rate is $\\mu_n = 0$ for all $n$.\n\nThe system dynamics are governed by the Kolmogorov forward equations. For a general birth-death process, the equation for the probability of being in state $n$ at time $t$ is:\n$$\n\\frac{dP_n(t)}{dt} = \\lambda_{n-1} P_{n-1}(t) + \\mu_{n+1} P_{n+1}(t) - (\\lambda_n + \\mu_n) P_n(t)\n$$\nIn our specific case, $\\lambda_n = n\\lambda$ and $\\mu_n = 0$. Substituting these rates into the general equation, we obtain the set of differential equations for our pure birth process.\n\nFor $n \\ge 2$, the transitions into state $n$ only come from state $n-1$ (a birth), and transitions out of state $n$ only go to state $n+1$ (another birth). Thus, the equation is:\n$$\n\\frac{dP_n(t)}{dt} = \\lambda(n-1) P_{n-1}(t) - n\\lambda P_n(t)\n$$\nFor the state $n=1$, there are no states below it, so there can be no transitions into state $1$. The only possibility is a transition out of state $1$ to state $2$, which occurs at rate $\\lambda_1 = 1 \\cdot \\lambda = \\lambda$. The equation for $P_1(t)$ is therefore:\n$$\n\\frac{dP_1(t)}{dt} = -\\lambda_1 P_1(t) = -\\lambda P_1(t)\n$$\nThe initial condition is $X(0)=1$, which translates to the following initial probabilities:\n$P_1(0) = 1$\n$P_n(0) = 0$ for all $n \\ge 2$.\n\nWe can solve this system of ordinary differential equations recursively.\n\nFirst, we solve for $P_1(t)$:\n$$\n\\frac{dP_1(t)}{dt} = -\\lambda P_1(t)\n$$\nThis is a separable first-order linear ODE. The solution is of the form $P_1(t) = C \\exp(-\\lambda t)$. Using the initial condition $P_1(0)=1$, we find the constant $C$:\n$1 = C \\exp(0) \\implies C=1$.\nThus, the probability of having exactly one cell at time $t$ is:\n$$\nP_1(t) = \\exp(-\\lambda t)\n$$\nNext, we solve for $P_2(t)$ using the equation for $n=2$:\n$$\n\\frac{dP_2(t)}{dt} = \\lambda(2-1) P_1(t) - 2\\lambda P_2(t) = \\lambda P_1(t) - 2\\lambda P_2(t)\n$$\nSubstitute the expression for $P_1(t)$:\n$$\n\\frac{dP_2(t)}{dt} + 2\\lambda P_2(t) = \\lambda \\exp(-\\lambda t)\n$$\nThis is a first-order linear non-homogeneous ODE. We solve it using an integrating factor, $I(t) = \\exp(\\int 2\\lambda dt) = \\exp(2\\lambda t)$. Multiplying the equation by $I(t)$:\n$$\n\\exp(2\\lambda t) \\frac{dP_2(t)}{dt} + 2\\lambda \\exp(2\\lambda t) P_2(t) = \\lambda \\exp(2\\lambda t) \\exp(-\\lambda t)\n$$\n$$\n\\frac{d}{dt} \\left( P_2(t) \\exp(2\\lambda t) \\right) = \\lambda \\exp(\\lambda t)\n$$\nIntegrating both sides with respect to $t$:\n$$\nP_2(t) \\exp(2\\lambda t) = \\int \\lambda \\exp(\\lambda t) dt = \\exp(\\lambda t) + C\n$$\n$$\nP_2(t) = \\exp(-\\lambda t) + C \\exp(-2\\lambda t)\n$$\nUsing the initial condition $P_2(0)=0$:\n$0 = \\exp(0) + C \\exp(0) \\implies 0 = 1 + C \\implies C = -1$.\nSo, the solution for $P_2(t)$ is:\n$$\nP_2(t) = \\exp(-\\lambda t) - \\exp(-2\\lambda t) = \\exp(-\\lambda t)(1 - \\exp(-\\lambda t))\n$$\nWe observe a pattern emerging:\n$P_1(t) = \\exp(-\\lambda t) (1 - \\exp(-\\lambda t))^{1-1}$\n$P_2(t) = \\exp(-\\lambda t) (1 - \\exp(-\\lambda t))^{2-1}$\nWe hypothesize that the general solution for $n \\ge 1$ is:\n$$\nP_n(t) = \\exp(-\\lambda t) (1 - \\exp(-\\lambda t))^{n-1}\n$$\nWe prove this hypothesis by induction.\nThe base case for $n=1$ is verified.\nAssume the formula holds for some integer $k \\ge 1$: $P_k(t) = \\exp(-\\lambda t) (1 - \\exp(-\\lambda t))^{k-1}$.\nWe must show it holds for $k+1$, using the differential equation:\n$$\n\\frac{dP_{k+1}(t)}{dt} + (k+1)\\lambda P_{k+1}(t) = k\\lambda P_k(t)\n$$\nSubstitute the hypothesized form for $P_{k+1}(t)$:\n$P_{k+1}(t) = \\exp(-\\lambda t)(1-\\exp(-\\lambda t))^k$.\nThe derivative with respect to $t$ is:\n$$\n\\frac{dP_{k+1}(t)}{dt} = -\\lambda \\exp(-\\lambda t) (1-\\exp(-\\lambda t))^k + \\exp(-\\lambda t) \\cdot k(1-\\exp(-\\lambda t))^{k-1} \\cdot (\\lambda \\exp(-\\lambda t))\n$$\n$$\n\\frac{dP_{k+1}(t)}{dt} = -\\lambda \\exp(-\\lambda t) (1-\\exp(-\\lambda t))^k + k\\lambda \\exp(-2\\lambda t) (1-\\exp(-\\lambda t))^{k-1}\n$$\nNow substitute this into the left-hand side (LHS) of the differential equation for $P_{k+1}(t)$:\n$$\nLHS = \\left( -\\lambda \\exp(-\\lambda t) (1-\\exp(-\\lambda t))^k + k\\lambda \\exp(-2\\lambda t) (1-\\exp(-\\lambda t))^{k-1} \\right) + (k+1)\\lambda \\exp(-\\lambda t)(1-\\exp(-\\lambda t))^k\n$$\nCombine the first and third terms:\n$$\nLHS = ( (k+1)\\lambda - \\lambda ) \\exp(-\\lambda t)(1-\\exp(-\\lambda t))^k + k\\lambda \\exp(-2\\lambda t) (1-\\exp(-\\lambda t))^{k-1}\n$$\n$$\nLHS = k\\lambda \\exp(-\\lambda t)(1-\\exp(-\\lambda t))^k + k\\lambda \\exp(-2\\lambda t) (1-\\exp(-\\lambda t))^{k-1}\n$$\nFactor out the common term $k\\lambda \\exp(-\\lambda t)(1-\\exp(-\\lambda t))^{k-1}$:\n$$\nLHS = k\\lambda \\exp(-\\lambda t)(1-\\exp(-\\lambda t))^{k-1} \\left[ (1-\\exp(-\\lambda t)) + \\exp(-\\lambda t) \\right]\n$$\n$$\nLHS = k\\lambda \\exp(-\\lambda t)(1-\\exp(-\\lambda t))^{k-1} [1]\n$$\nThe right-hand side (RHS) of the differential equation is $k\\lambda P_k(t)$. Using our induction hypothesis for $P_k(t)$:\n$$\nRHS = k\\lambda \\left( \\exp(-\\lambda t) (1 - \\exp(-\\lambda t))^{k-1} \\right)\n$$\nWe see that $LHS = RHS$, so our hypothesized solution satisfies the differential equation. We must also check the initial condition:\n$$\nP_{k+1}(0) = \\exp(0) (1 - \\exp(0))^{k} = 1 \\cdot (1-1)^k = 0\n$$\nThis holds for any $k \\ge 1$ (i.e., for any $n \\ge 2$). The induction is complete.\nThe derived transient probability distribution is a geometric distribution with success probability $p=\\exp(-\\lambda t)$. The probability of having $n$ cells at time $t$ is:\n$P(X(t)=n \\mid X(0)=1) = \\exp(-\\lambda t) (1 - \\exp(-\\lambda t))^{n-1}$ for $n=1, 2, 3, \\ldots$.",
            "answer": "$$\\boxed{\\exp(-\\lambda t) (1 - \\exp(-\\lambda t))^{n-1}}$$"
        },
        {
            "introduction": "After mastering the forward dynamics of a process, the next crucial skill is learning to infer its parameters from data. This practice tackles this inverse problem by guiding you through the derivation of the maximum likelihood estimators (MLEs) for the birth rate $\\lambda$ and death rate $\\mu$ of a linear birth-death process. Based on a fully observed trajectory, you will learn to construct the likelihood function for a continuous-time Markov chain and uncover the elegant relationship between the event counts ($B$, $D$) and the total population-time exposure ($S$). ",
            "id": "3920158",
            "problem": "A single-clone mammalian cell population is maintained in a perfusion bioreactor under conditions where cell-cell interactions can be neglected over the observation window. The population size is modeled as a linear birth-death process, which is a Continuous-Time Markov Chain (CTMC) with states $\\{0,1,2,\\dots\\}$ and generator specified by $q_{n,n+1} = \\lambda n$, $q_{n,n-1} = \\mu n$, and $q_{n,n} = -(\\lambda+\\mu)n$ for $n \\in \\mathbb{N}$, where $\\lambda  0$ is the per-capita mitotic birth rate and $\\mu  0$ is the per-capita death rate. The initial population size $n(0) = n_{0}$ is known and strictly positive.\n\nOver a fixed observation horizon $[0,T]$, the trajectory is fully observed: every jump time and its type (birth or death) is recorded, and the piecewise-constant occupancy times in each state are known. Let $B$ denote the total number of recorded birth events, $D$ the total number of recorded death events, and define the cumulative exposure\n$$\nS \\equiv \\int_{0}^{T} n(t)\\, dt,\n$$\nwhich is the total population-size–time accumulated over $[0,T]$. Assume no immigration and no censoring.\n\nStarting only from the CTMC definition, the memoryless property of exponential waiting times, and the product form of path densities for jump processes, derive the likelihood $L(\\lambda,\\mu \\mid \\text{trajectory})$ of the fully observed trajectory as a function of $\\lambda$ and $\\mu$; then, compute the maximum likelihood estimators (MLEs) $\\hat{\\lambda}$ and $\\hat{\\mu}$. Finally, establish the leading-order asymptotic variances of $\\hat{\\lambda}$ and $\\hat{\\mu}$ as the exposure $S \\to \\infty$ (equivalently, as $T \\to \\infty$ under conditions ensuring $S \\to \\infty$), invoking appropriate regularity for MLEs in CTMCs.\n\nIn your final answer, present as a single row matrix in the order $\\hat{\\lambda}$, $\\hat{\\mu}$, $\\mathrm{Var}(\\hat{\\lambda})$ (leading-order), $\\mathrm{Var}(\\hat{\\mu})$ (leading-order). No rounding is required; provide exact analytic expressions. Do not include units in the final boxed expression.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Model**: Linear birth-death process, a Continuous-Time Markov Chain (CTMC).\n- **State Space**: $\\{0, 1, 2, \\dots\\}$.\n- **Generator Matrix Elements**: $q_{n,n+1} = \\lambda n$, $q_{n,n-1} = \\mu n$, and $q_{n,n} = -(\\lambda+\\mu)n$ for $n \\in \\mathbb{N}$.\n- **Parameters**: Per-capita birth rate $\\lambda  0$, per-capita death rate $\\mu  0$.\n- **Initial Condition**: Population size at time $t=0$ is $n(0) = n_0$, where $n_0$ is a known positive integer.\n- **Observation Scheme**: A single trajectory is fully observed over the time interval $[0, T]$. This includes all jump times, the type of each jump (birth or death), and the population size $n(t)$ for all $t \\in [0, T]$.\n- **Sufficient Statistics**:\n    - $B$: The total count of birth events in $[0, T]$.\n    - $D$: The total count of death events in $[0, T]$.\n    - $S \\equiv \\int_{0}^{T} n(t)\\, dt$: The cumulative population exposure over $[0, T]$.\n- **Assumptions**: No immigration into the population and no censoring of data.\n- **Objectives**:\n    1. Derive the likelihood function $L(\\lambda, \\mu \\mid \\text{trajectory})$.\n    2. Compute the maximum likelihood estimators (MLEs) $\\hat{\\lambda}$ and $\\hat{\\mu}$.\n    3. Determine the leading-order asymptotic variances of $\\hat{\\lambda}$ and $\\hat{\\mu}$ in the limit that the cumulative exposure $S \\to \\infty$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in statistical inference for stochastic processes, specifically for Continuous-Time Markov Chains.\n- **Scientifically Grounded**: The linear birth-death process is a fundamental and canonical model in mathematical biology, population genetics, and queuing theory. Its mathematical formulation is rigorous and well-established. The problem as stated is scientifically and mathematically sound.\n- **Well-Posed**: The problem is clearly defined. The data provided ($B$, $D$, and $S$) are the well-known sufficient statistics for the parameters ($\\lambda$, $\\mu$) of this process under full observation. The objectives are unambiguous and lead to a unique, meaningful solution.\n- **Objective**: The problem is stated using precise, standard mathematical and statistical terminology. It is free of any ambiguity, subjectivity, or non-scientific claims.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. The solution process will now proceed.\n\n### Derivation of the Likelihood Function\nThe likelihood of a fully observed trajectory of a general CTMC, $X(t)$, over an interval $[0, T]$ is given by the product of the rates of the observed jumps and the probabilities of the sojourn times between jumps. Let the observed jump times be $0  t_1  t_2  \\dots  t_k  T$, where the state is $n_i$ during the interval $[t_i, t_{i+1})$ (with $t_0=0$ and $t_{k+1}=T$). The likelihood is:\n$$\nL = \\left( \\prod_{i=1}^{k} q_{n_{i-1}, n_i} \\right) \\exp\\left( - \\sum_{i=0}^{k} q_{n_i} (t_{i+1} - t_i) \\right)\n$$\nwhere $q_{n,m}$ is the transition rate from state $n$ to state $m$, and $q_n = \\sum_{m \\neq n} q_{n,m}$ is the total exit rate from state $n$.\n\nFor the given linear birth-death process, the rates are:\n- Birth: $q_{n, n+1} = \\lambda n$\n- Death: $q_{n, n-1} = \\mu n$\n- Total exit rate: $q_n = q_{n, n+1} + q_{n, n-1} = \\lambda n + \\mu n = (\\lambda + \\mu)n$.\n\nThe product term in the likelihood expression can be broken down according to the type of jump. There are $B$ birth events and $D$ death events. Let the state of the system just before the $j$-th birth event be $n_{b,j}$ and just before the $j$-th death event be $n_{d,j}$. The product of jump rates is:\n$$\n\\prod_{\\text{jumps}} \\text{rate} = \\left( \\prod_{j=1}^{B} (\\lambda n_{b,j}) \\right) \\left( \\prod_{j=1}^{D} (\\mu n_{d,j}) \\right) = \\lambda^B \\mu^D \\left( \\prod_{j=1}^{B} n_{b,j} \\prod_{j=1}^{D} n_{d,j} \\right)\n$$\nThe term involving the products of population sizes depends only on the observed path, not on the parameters $\\lambda$ and $\\mu$. For the purpose of finding the MLEs, this term is a constant of proportionality.\n\nThe exponential term involves the sum of the exit rates multiplied by the sojourn times. This sum can be expressed as an integral over the observation horizon:\n$$\n\\sum_{i=0}^{k} q_{n_i} (t_{i+1} - t_i) = \\int_{0}^{T} q_{n(t)} dt = \\int_{0}^{T} (\\lambda + \\mu) n(t) dt\n$$\nUsing the definition of the cumulative exposure $S = \\int_{0}^{T} n(t)\\, dt$, this becomes:\n$$\n(\\lambda + \\mu) \\int_{0}^{T} n(t) dt = (\\lambda + \\mu) S\n$$\nCombining these parts, the likelihood function $L(\\lambda, \\mu)$ for the observed trajectory, viewed as a function of the parameters $\\lambda$ and $\\mu$, is proportional to:\n$$\nL(\\lambda, \\mu \\mid \\text{trajectory}) \\propto \\lambda^B \\mu^D \\exp\\left(-(\\lambda + \\mu)S\\right)\n$$\n\n### Computation of the Maximum Likelihood Estimators (MLEs)\nTo find the MLEs, we maximize $L(\\lambda, \\mu)$ with respect to $\\lambda$ and $\\mu$. It is more convenient to maximize the log-likelihood function, $\\ell(\\lambda, \\mu) = \\ln L(\\lambda, \\mu)$:\n$$\n\\ell(\\lambda, \\mu) = B \\ln \\lambda + D \\ln \\mu - (\\lambda + \\mu)S + C\n$$\nwhere $C$ is a constant that does not depend on $\\lambda$ or $\\mu$.\n\nWe compute the partial derivatives of $\\ell$ with respect to $\\lambda$ and $\\mu$ and set them to zero:\n$$\n\\frac{\\partial \\ell}{\\partial \\lambda} = \\frac{B}{\\lambda} - S = 0\n$$\n$$\n\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{D}{\\mu} - S = 0\n$$\nSolving these equations for $\\lambda$ and $\\mu$ gives the maximum likelihood estimators $\\hat{\\lambda}$ and $\\hat{\\mu}$:\n$$\n\\hat{\\lambda} = \\frac{B}{S}\n$$\n$$\n\\hat{\\mu} = \\frac{D}{S}\n$$\nTo confirm these are maxima, we can check the second partial derivatives. The Hessian matrix of $\\ell$ is:\n$$\nH = \\begin{pmatrix} \\frac{\\partial^2 \\ell}{\\partial \\lambda^2}  \\frac{\\partial^2 \\ell}{\\partial \\lambda \\partial \\mu} \\\\ \\frac{\\partial^2 \\ell}{\\partial \\mu \\partial \\lambda}  \\frac{\\partial^2 \\ell}{\\partial \\mu^2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{B}{\\lambda^2}  0 \\\\ 0  -\\frac{D}{\\mu^2} \\end{pmatrix}\n$$\nSince $\\lambda  0$, $\\mu  0$, $B \\ge 0$, and $D \\ge 0$, the diagonal elements are non-positive. For a non-trivial trajectory with at least one birth and one death, $B  0$ and $D  0$, the Hessian is negative definite, confirming that we have found a unique maximum.\n\n### Derivation of Asymptotic Variances\nThe asymptotic variance-covariance matrix of the MLEs is given by the inverse of the Fisher information matrix, $I(\\theta)$, where $\\theta = (\\lambda, \\mu)$. The Fisher information matrix is defined as the negative of the expectation of the Hessian matrix: $I(\\theta) = -E[H(\\theta)]$.\n$$\nI(\\theta) = -E\\left[ \\begin{pmatrix} -\\frac{B}{\\lambda^2}  0 \\\\ 0  -\\frac{D}{\\mu^2} \\end{pmatrix} \\right] = \\begin{pmatrix} \\frac{E[B]}{\\lambda^2}  0 \\\\ 0  \\frac{E[D]}{\\mu^2} \\end{pmatrix}\n$$\nThe number of births $B$ and deaths $D$ are random variables whose expectations we need to find. The instantaneous rate of birth at time $t$ is $\\lambda n(t)$. The expected number of births over $[0, T]$ is the integral of the expected rate:\n$$\nE[B] = E\\left[ \\int_0^T dN_B(t) \\right] = \\int_0^T E[\\lambda n(t)] dt = \\lambda E\\left[\\int_0^T n(t) dt\\right] = \\lambda E[S]\n$$\nSimilarly, for the number of deaths:\n$$\nE[D] = E\\left[ \\int_0^T dN_D(t) \\right] = \\int_0^T E[\\mu n(t)] dt = \\mu E\\left[\\int_0^T n(t) dt\\right] = \\mu E[S]\n$$\nSubstituting these expectations into the Fisher information matrix:\n$$\nI(\\theta) = \\begin{pmatrix} \\frac{\\lambda E[S]}{\\lambda^2}  0 \\\\ 0  \\frac{\\mu E[S]}{\\mu^2} \\end{pmatrix} = \\begin{pmatrix} \\frac{E[S]}{\\lambda}  0 \\\\ 0  \\frac{E[S]}{\\mu} \\end{pmatrix}\n$$\nThe asymptotic variance-covariance matrix for the estimators $(\\hat{\\lambda}, \\hat{\\mu})$ is the inverse of the Fisher information matrix:\n$$\n\\mathrm{Var}(\\hat{\\theta}) \\approx I(\\theta)^{-1} = \\begin{pmatrix} \\frac{\\lambda}{E[S]}  0 \\\\ 0  \\frac{\\mu}{E[S]} \\end{pmatrix}\n$$\nThe problem specifies the asymptotic limit as the observed exposure $S \\to \\infty$. In this limit, the observed exposure $S$ becomes the natural measure of the amount of information in the sample. For a specific trajectory with a large value of $S$, the expectation $E[S]$ is well-approximated by $S$. Therefore, the leading-order asymptotic variances of the estimators are expressed in terms of the true parameters and the observed exposure $S$.\nThe asymptotic variance of $\\hat{\\lambda}$ is:\n$$\n\\mathrm{Var}(\\hat{\\lambda}) \\approx \\frac{\\lambda}{S}\n$$\nThe asymptotic variance of $\\hat{\\mu}$ is:\n$$\n\\mathrm{Var}(\\hat{\\mu}) \\approx \\frac{\\mu}{S}\n$$\nThe off-diagonal terms are zero, indicating that the estimators $\\hat{\\lambda}$ and $\\hat{\\mu}$ are asymptotically uncorrelated.\n\nThe final results are:\n1.  MLE for birth rate: $\\hat{\\lambda} = \\frac{B}{S}$\n2.  MLE for death rate: $\\hat{\\mu} = \\frac{D}{S}$\n3.  Asymptotic variance of $\\hat{\\lambda}$: $\\mathrm{Var}(\\hat{\\lambda}) \\approx \\frac{\\lambda}{S}$\n4.  Asymptotic variance of $\\hat{\\mu}$: $\\mathrm{Var}(\\hat{\\mu}) \\approx \\frac{\\mu}{S}$\n\nThese expressions will be presented in a single row matrix as requested.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{B}{S}  \\frac{D}{S}  \\frac{\\lambda}{S}  \\frac{\\mu}{S}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Parameter estimation provides the best-fit model, but is the model any good? This final practice addresses the critical step of model validation through a hands-on computational exercise. You will implement a powerful diagnostic based on the time-rescaling theorem to perform a goodness-of-fit test for an inhomogeneous Poisson process, a model often used for phenomena like neuronal spiking. This exercise will equip you with a practical, code-based method to assess whether your model truly captures the statistical signature of the observed data. ",
            "id": "3920131",
            "problem": "You are tasked with building a complete, runnable program that implements a goodness-of-fit diagnostic for biomedical point process data using time-rescaled residuals and a one-sample Kolmogorov–Smirnov test against the uniform distribution. The context is neuronal spike trains modeled as inhomogeneous Poisson processes, though the formulation is purely mathematical. All event times are in seconds (s), and intensities are in inverse seconds ($\\mathrm{s}^{-1}$).\n\nFundamental base:\n- A simple point process with conditional intensity function $\\lambda(t \\mid \\mathcal{H}_t)$, where $\\mathcal{H}_t$ denotes the history up to time $t$, has inter-event intervals whose integrated conditional intensity increments $$W_k = \\int_{t_{k-1}}^{t_k} \\lambda(u \\mid \\mathcal{H}_u)\\,du$$ are independent and identically distributed exponential random variables with mean $1$ when the model is correctly specified for the generating process. Equivalently, the transformed residuals $$U_k = 1 - e^{-W_k}$$ are independent and identically distributed uniform random variables on $[0,1]$.\n- The Kolmogorov–Smirnov (KS) test, formally the Kolmogorov–Smirnov test (KST), compares the empirical cumulative distribution function (CDF) of a sample to a specified continuous CDF, here the uniform CDF on $[0,1]$, and yields a test statistic $D$ and a $p$-value for the null hypothesis that the sample is drawn from the specified distribution.\n\nModel specification:\n- Consider an inhomogeneous Poisson process on $[0,\\infty)$ with intensity\n$$\n\\lambda(t;\\theta) = \\beta + A \\exp\\!\\left(-\\frac{(t - \\mu)^2}{2\\sigma^2}\\right), \\quad t \\ge 0,\n$$\nwhere $\\theta = (\\beta, A, \\mu, \\sigma)$ with $\\beta \\ge 0$, $A \\ge 0$, $\\sigma  0$, and $\\mu \\in \\mathbb{R}$.\n- For any interval $[a,b]$, the integrated intensity under this model is\n$$\n\\int_a^b \\lambda(u;\\theta)\\,du = \\beta(b-a) + A \\sigma \\sqrt{\\frac{\\pi}{2}} \\left[\\operatorname{erf}\\!\\left(\\frac{b - \\mu}{\\sqrt{2}\\,\\sigma}\\right) - \\operatorname{erf}\\!\\left(\\frac{a - \\mu}{\\sqrt{2}\\,\\sigma}\\right)\\right],\n$$\nwhere $\\operatorname{erf}(\\cdot)$ is the Gauss error function.\n\nDiagnostic design (your program must implement):\n1. Synthetic event generation with known ground-truth intensity $\\lambda(t;\\theta_{\\text{true}})$:\n   - Generate $n$ event times $0 = t_0 lt; t_1 lt; \\dots lt; t_n$ deterministically via inversion of the cumulative intensity so that the integrated increments $W_k = \\int_{t_{k-1}}^{t_k} \\lambda(u;\\theta_{\\text{true}})\\,du$ satisfy $U_k = 1 - e^{-W_k}$ with a prescribed deterministic sequence $U_k \\in (0,1)$ for $k=1,\\dots,n$. Use the sequence $U_k = \\frac{k}{n+1}$ so that the integrated increments are $W_k = -\\ln(1 - U_k)$ and solve for each $t_k$ from\n   $$\n   \\int_{t_{k-1}}^{t_k} \\lambda(u;\\theta_{\\text{true}})\\,du = -\\ln(1 - U_k),\n   $$\n   using a monotone root-finding method on $t_k$ (e.g., bisection) that brackets the solution in a finite interval $[t_{k-1}, T_{\\max}]$, with $T_{\\max}$ chosen sufficiently large to accommodate the required total integrated intensity.\n2. Time-rescaled residuals under a candidate model $\\lambda(t;\\theta_{\\text{model}})$:\n   - Compute\n   $$\n   W_k^{\\text{model}} = \\int_{t_{k-1}}^{t_k} \\lambda(u;\\theta_{\\text{model}})\\,du, \\quad U_k^{\\text{model}} = 1 - e^{-W_k^{\\text{model}}}.\n   $$\n3. Apply a one-sample Kolmogorov–Smirnov test to $\\{U_k^{\\text{model}}\\}_{k=1}^n$ against the continuous uniform distribution on $[0,1]$, obtaining the test statistic $D$ and a $p$-value.\n4. For each test case, output a boolean indicating acceptance of the null hypothesis at level $\\alpha$ according to the rule “accept if $p \\ge \\alpha$.”\n\nTest suite:\nImplement the diagnostic for the following four test cases. In each case, generate synthetic event times using $\\theta_{\\text{true}}$ and then compute the residuals using $\\theta_{\\text{model}}$. Use seconds (s) for time and inverse seconds ($\\mathrm{s}^{-1}$) for intensities. Set $T_{\\max} = 300\\,\\mathrm{s}$.\n\n- Case 1 (well-specified, moderate sample size):\n  - $\\theta_{\\text{true}} = (\\beta, A, \\mu, \\sigma) = (0.5, 1.0, 10.0, 1.0)$\n  - $\\theta_{\\text{model}} = (0.5, 1.0, 10.0, 1.0)$\n  - $n = 50$\n  - $\\alpha = 0.05$\n- Case 2 (amplitude misspecified, same sample size):\n  - $\\theta_{\\text{true}} = (0.5, 1.0, 10.0, 1.0)$\n  - $\\theta_{\\text{model}} = (0.5, 2.0, 10.0, 1.0)$\n  - $n = 50$\n  - $\\alpha = 0.05$\n- Case 3 (amplitude misspecified, small sample size):\n  - $\\theta_{\\text{true}} = (0.5, 1.0, 10.0, 1.0)$\n  - $\\theta_{\\text{model}} = (0.5, 2.0, 10.0, 1.0)$\n  - $n = 5$\n  - $\\alpha = 0.05$\n- Case 4 (location misspecified, larger sample size):\n  - $\\theta_{\\text{true}} = (0.3, 1.5, 20.0, 2.0)$\n  - $\\theta_{\\text{model}} = (0.3, 1.5, 25.0, 2.0)$\n  - $n = 60$\n  - $\\alpha = 0.05$\n\nFinal output format:\nYour program should produce a single line of output containing the acceptance decisions for the four cases as a comma-separated list enclosed in square brackets, for example, “[True,False,True,False]”. Each entry is a boolean indicating whether the model’s time-rescaled residuals passed the uniform Kolmogorov–Smirnov test at the specified significance level for that case.",
            "solution": "The user has provided a well-defined computational problem in the domain of biomedical systems modeling, specifically concerning the validation of point process models. I will first confirm the validity of the problem statement and then provide a comprehensive solution and implementation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Process Model**: An inhomogeneous Poisson process on $[0, \\infty)$ with intensity function $\\lambda(t;\\theta) = \\beta + A \\exp(-\\frac{(t - \\mu)^2}{2\\sigma^2})$, where $\\theta = (\\beta, A, \\mu, \\sigma)$.\n*   **Integrated Intensity Formula**: The definite integral of the intensity is given as $\\int_a^b \\lambda(u;\\theta)\\,du = \\beta(b-a) + A \\sigma \\sqrt{\\frac{\\pi}{2}} \\left[\\operatorname{erf}(\\frac{b - \\mu}{\\sqrt{2}\\,\\sigma}) - \\operatorname{erf}(\\frac{a - \\mu}{\\sqrt{2}\\,\\sigma})\\right]$.\n*   **Time-Rescaling Principle**: The integrated intensity increments $W_k = \\int_{t_{k-1}}^{t_k} \\lambda(u \\mid \\mathcal{H}_u)\\,du$ are independent and identically distributed (i.i.d.) exponential random variables with mean $1$ if the model is correct. The transformed residuals $U_k = 1 - e^{-W_k}$ are i.i.d. uniform on $[0,1]$.\n*   **Synthetic Data Generation**: Event times $0 = t_0  t_1  \\dots  t_n$ are generated deterministically by solving $\\int_{t_{k-1}}^{t_k} \\lambda(u;\\theta_{\\text{true}})\\,du = -\\ln(1 - U_k)$ for $t_k$, where $U_k = \\frac{k}{n+1}$. A monotone root-finding method on the interval $[t_{k-1}, T_{\\max}]$ is to be used.\n*   **Goodness-of-Fit Test**: Apply a one-sample Kolmogorov–Smirnov (KS) test to the computed residuals $\\{U_k^{\\text{model}}\\}_{k=1}^n = \\{1 - e^{-\\int_{t_{k-1}}^{t_k} \\lambda(u;\\theta_{\\text{model}})\\,du}\\}_{k=1}^n$ against the standard uniform distribution.\n*   **Decision Rule**: Accept the null hypothesis (that the model fits the data) if the KS test $p$-value is greater than or equal to a significance level $\\alpha$.\n*   **Constants and Parameters**: $T_{\\max} = 300\\,\\mathrm{s}$. Four test cases are specified with distinct values for ground-truth parameters ($\\theta_{\\text{true}}$), model parameters ($\\theta_{\\text{model}}$), sample size ($n$), and significance level ($\\alpha=0.05$).\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem is based on the time-rescaling theorem, a cornerstone of point process theory. The intensity function is a standard model for transient rate changes. The use of the Kolmogorov-Smirnov test for goodness-of-fit against a uniform distribution is a standard statistical procedure. The method for deterministic event generation via inversion is a valid technique for creating synthetic data with specific properties. The problem is scientifically and mathematically sound.\n2.  **Well-Posed**: All necessary functions, parameters, and procedures are explicitly defined. The intensity function $\\lambda(t)$ is strictly positive for the given parameter constraints (as $\\beta \\geq 0$, $A \\geq 0$, and they aren't both zero), which ensures its integral is strictly increasing. This guarantees that the root-finding problem for generating event times has a unique solution. The provided value for $T_{\\max}$ is sufficiently large to contain all events for the specified test cases. The problem is self-contained and unambiguous.\n3.  **Objective**: The problem is stated using precise mathematical language and objective, verifiable instructions. There are no subjective or opinion-based components.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. It is scientifically grounded, well-posed, and objective. I will now proceed with the solution.\n\n### Solution\n\nThe solution involves implementing a computational diagnostic to assess the goodness-of-fit for an inhomogeneous Poisson process model. This will be accomplished by following the prescribed steps: deterministic generation of synthetic event data under a \"true\" model, calculation of time-rescaled residuals under a \"candidate\" model, and statistical testing of these residuals for uniformity.\n\n**1. Theoretical Framework: Time-Rescaling Theorem**\nThe foundation of this method is the time-rescaling theorem. For a point process with event times $t_1, t_2, \\dots, t_n$ and a correctly specified conditional intensity function $\\lambda(t | \\mathcal{H}_t)$, the quantities\n$$\nW_k = \\int_{t_{k-1}}^{t_k} \\lambda(u | \\mathcal{H}_u) \\, du, \\quad k=1, 2, \\dots, n\n$$\nare independent and identically distributed random variables from an exponential distribution with rate $1$. Here, $\\mathcal{H}_t$ represents the history of events up to time $t$. For an inhomogeneous Poisson process, the intensity $\\lambda(t)$ is independent of the history.\n\nBy applying the probability integral transform, we can convert these exponential variables into uniform variables. If $W \\sim \\text{Exponential}(1)$, its cumulative distribution function (CDF) is $F(w) = 1 - e^{-w}$. The transformed variable $U=F(W)=1-e^{-W}$ is uniformly distributed on $[0,1]$. Thus, if our model $\\lambda(t;\\theta)$ is correct, the time-rescaled residuals\n$$\nU_k = 1 - \\exp\\left(-\\int_{t_{k-1}}^{t_k} \\lambda(u;\\theta) \\, du\\right)\n$$\nmust be i.i.d. samples from a Uniform$[0,1]$ distribution. This forms the basis of our null hypothesis for the goodness-of-fit test.\n\n**2. Synthetic Event Generation**\nTo test our diagnostic procedure, we first require a set of event times. We generate these times deterministically from a known ground-truth model, $\\lambda(t; \\theta_{\\text{true}})$. Instead of a stochastic simulation, we enforce a deterministic structure on the residuals $U_k$ by setting them to the quantiles of the uniform distribution:\n$$\nU_k = \\frac{k}{n+1}, \\quad k = 1, \\dots, n.\n$$\nThis choice ensures a \"perfect\" distribution of residuals if analyzed with the correct model. From this, we derive the required integrated intensity for each inter-event interval:\n$$\nW_k = -\\ln(1 - U_k) = -\\ln\\left(1 - \\frac{k}{n+1}\\right) = \\ln\\left(\\frac{n+1}{n+1-k}\\right).\n$$\nThe event times $t_k$ are then found by sequentially solving the integral equation for each $k$:\n$$\n\\int_{t_{k-1}}^{t_k} \\lambda(u; \\theta_{\\text{true}}) \\, du = W_k.\n$$\nLet $\\Lambda(a, b; \\theta) = \\int_a^b \\lambda(u; \\theta) \\, du$. We need to solve the equation $\\Lambda(t_{k-1}, t, \\theta_{\\text{true}}) - W_k = 0$ for $t$. Since $\\lambda(t)  0$, the function $f(t) = \\Lambda(t_{k-1}, t, \\theta_{\\text{true}})$ is strictly increasing in $t$. This guarantees a unique root $t_k  t_{k-1}$, which can be found efficiently using a numerical root-finding algorithm like Brent's method within the search interval $[t_{k-1}, T_{\\max}]$.\n\n**3. Residual Calculation and Statistical Test**\nOnce the event times $\\{t_k\\}_{k=1}^n$ are generated, we test a candidate model $\\lambda(t; \\theta_{\\text{model}})$. Note that $\\theta_{\\text{model}}$ may or may not be equal to $\\theta_{\\text{true}}$. We calculate the residuals under this candidate model:\n$$\nW_k^{\\text{model}} = \\Lambda(t_{k-1}, t_k; \\theta_{\\text{model}}),\n$$\n$$\nU_k^{\\text{model}} = 1 - \\exp(-W_k^{\\text{model}}).\n$$\nIf $\\theta_{\\text{model}}$ is a good representation of the process that generated the event times (i.e., if $\\theta_{\\text{model}} \\approx \\theta_{\\text{true}}$), the sequence $\\{U_k^{\\text{model}}\\}$ should appear to be a sample from the Uniform$[0,1]$ distribution. We formally test this hypothesis using the one-sample Kolmogorov-Smirnov (KS) test. The KS test computes the maximum absolute difference between the empirical CDF of the sample $\\{U_k^{\\text{model}}\\}$ and the theoretical CDF of the standard uniform distribution. This difference is the test statistic $D$. The test yields a $p$-value, which is the probability of observing a statistic at least as extreme as $D$ under the null hypothesis.\n\n**4. Decision**\nWe compare the $p$-value to the significance level $\\alpha$. If $p \\ge \\alpha$, we do not have sufficient evidence to reject the null hypothesis, and we \"accept\" the model (more formally, we fail to reject it). If $p  \\alpha$, we reject the null hypothesis, concluding that the candidate model is a poor fit for the data. The program will execute this entire procedure for each of the four specified test cases and report the boolean outcome of the decision rule.\n\n*   **Case 1 (Well-specified)**: $\\theta_{\\text{model}} = \\theta_{\\text{true}}$. The residuals $\\{U_k^{\\text{model}}\\}$ are generated to be perfectly uniform quantiles. The KS test should yield a large $p$-value, leading to acceptance.\n*   **Case 2 (Misspecified amplitude)**: $\\theta_{\\text{model}}$ has a larger amplitude $A$ than $\\theta_{\\text{true}}$. This will cause the calculated $W_k^{\\text{model}}$ to be systematically larger than the true $W_k$, pushing the $U_k^{\\text{model}}$ values towards $1$. The KS test should detect this deviation from uniformity, yielding a small $p$-value and leading to rejection.\n*   **Case 3 (Misspecified amplitude, small sample)**: Same misspecification as Case 2, but with a much smaller sample size ($n=5$). Statistical tests have lower power with small samples, making it harder to detect deviations. It is possible the test will fail to reject the null hypothesis despite the model being incorrect.\n*   **Case 4 (Misspecified location)**: $\\theta_{\\text{model}}$ has a different peak location $\\mu$ than $\\theta_{\\text{true}}$. This will cause a systematic misfit, where $W_k^{\\text{model}}$ is too small for some intervals and too large for others. With a sufficient sample size ($n=60$), the KS test is expected to detect this and reject the model.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\nfrom scipy.optimize import brentq\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Implements a goodness-of-fit diagnostic for an inhomogeneous Poisson process model\n    using time-rescaled residuals and the Kolmogorov-Smirnov test.\n    \"\"\"\n\n    def integrated_lambda(t_start, t_end, theta):\n        \"\"\"\n        Calculates the integrated intensity over the interval [t_start, t_end].\n\n        The intensity function is lambda(t) = beta + A * exp(-(t - mu)^2 / (2 * sigma^2)).\n        The integral is computed analytically using the error function erf.\n        \"\"\"\n        beta, A, mu, sigma = theta\n        \n        # Linear term from the constant baseline intensity beta\n        integral_beta = beta * (t_end - t_start)\n        \n        # Gaussian term's contribution\n        # Argument for the error function: z = (t - mu) / (sqrt(2) * sigma)\n        z_start = (t_start - mu) / (np.sqrt(2) * sigma)\n        z_end = (t_end - mu) / (np.sqrt(2) * sigma)\n        \n        # Integral of the Gaussian component\n        integral_gauss = A * sigma * np.sqrt(np.pi / 2) * (erf(z_end) - erf(z_start))\n        \n        return integral_beta + integral_gauss\n\n    def generate_events(theta_true, n, t_max):\n        \"\"\"\n        Generates a deterministic sequence of event times by inverting the\n        cumulative intensity function.\n        \"\"\"\n        event_times = np.zeros(n)\n        t_prev = 0.0\n        \n        for k in range(1, n + 1):\n            # Prescribed U_k values are quantiles of the uniform distribution\n            U_k = k / (n + 1)\n            # Corresponding integrated intensity W_k for an exponential(1) variable\n            W_k = -np.log(1 - U_k)\n            \n            # Define the function whose root we need to find.\n            # We are solving integrated_lambda(t_prev, t_k) - W_k = 0 for t_k.\n            def root_func(t_k):\n                return integrated_lambda(t_prev, t_k, theta_true) - W_k\n\n            # Use Brent's method to find the next event time t_k.\n            # The search interval is [t_prev, t_max].\n            # f(t_prev) is -W_k  0. We expect f(t_max)  0 for a root to be bracketed.\n            try:\n                t_k = brentq(root_func, t_prev, t_max, xtol=1e-12, rtol=1e-12)\n                event_times[k-1] = t_k\n                t_prev = t_k\n            except ValueError:\n                # This would happen if t_max is not large enough, so root_func(t_max) is not positive.\n                # The problem setup ensures t_max is sufficient.\n                raise ValueError(f\"Root not bracketed for event {k}. Increase T_max.\")\n                \n        return event_times\n\n    # Test cases defined in the problem statement.\n    # Format: (theta_true, theta_model, n, alpha)\n    # T_max is common for all cases.\n    T_MAX = 300.0\n    test_cases = [\n        # Case 1: Well-specified, moderate sample size\n        {'theta_true': (0.5, 1.0, 10.0, 1.0), 'theta_model': (0.5, 1.0, 10.0, 1.0), 'n': 50, 'alpha': 0.05},\n        # Case 2: Amplitude misspecified, same sample size\n        {'theta_true': (0.5, 1.0, 10.0, 1.0), 'theta_model': (0.5, 2.0, 10.0, 1.0), 'n': 50, 'alpha': 0.05},\n        # Case 3: Amplitude misspecified, small sample size\n        {'theta_true': (0.5, 1.0, 10.0, 1.0), 'theta_model': (0.5, 2.0, 10.0, 1.0), 'n': 5, 'alpha': 0.05},\n        # Case 4: Location misspecified, larger sample size\n        {'theta_true': (0.3, 1.5, 20.0, 2.0), 'theta_model': (0.3, 1.5, 25.0, 2.0), 'n': 60, 'alpha': 0.05},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        theta_true = case['theta_true']\n        theta_model = case['theta_model']\n        n = case['n']\n        alpha = case['alpha']\n\n        # 1. Generate synthetic event times using the ground-truth model\n        event_times = generate_events(theta_true, n, T_MAX)\n        \n        # 2. Compute time-rescaled residuals under the candidate model\n        U_model = np.zeros(n)\n        t_prev = 0.0\n        for i, t_k in enumerate(event_times):\n            W_k_model = integrated_lambda(t_prev, t_k, theta_model)\n            U_model[i] = 1.0 - np.exp(-W_k_model)\n            t_prev = t_k\n            \n        # 3. Apply a one-sample KS test against the uniform distribution\n        # The null hypothesis is that the sample `U_model` is drawn from a uniform distribution.\n        ks_statistic, p_value = kstest(U_model, 'uniform')\n        \n        # 4. Accept the null hypothesis if p = alpha\n        decision = (p_value = alpha)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}