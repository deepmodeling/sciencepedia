{
    "hands_on_practices": [
        {
            "introduction": "A common first step in propagating uncertainty is to use interval arithmetic, which calculates the range of possible outputs given the range of inputs. This exercise is designed to reveal a critical flaw in this naive approach—the dependency problem, which arises when the same uncertain parameter appears multiple times in a calculation. By working through this simple but powerful counterexample , you will gain a deeper appreciation for why correctly accounting for shared dependencies is fundamental to accurate uncertainty analysis.",
            "id": "3941594",
            "problem": "A single-subject linear pharmacokinetic model is used to quantify exposure following two separate intravenous bolus doses of the same drug. In linear pharmacokinetics, the area under the plasma concentration–time curve (AUC) for an intravenous bolus is given by $ \\mathrm{AUC} = \\mathrm{Dose}/\\mathrm{CL} $, where $ \\mathrm{CL} $ is the systemic clearance. Suppose the same subject receives two bolus doses, $ D_{1} = 100\\,\\mathrm{mg} $ and $ D_{2} = 150\\,\\mathrm{mg} $, separated by sufficient time so that the pharmacokinetics are independent across occasions. The subject’s true clearance $ \\mathrm{CL} $ is constant across both occasions but is epistemically uncertain to the modeler, known only to lie in the interval $ \\mathrm{CL} \\in [3.0,\\,5.0]\\,\\mathrm{L\\,h^{-1}} $.\n\nDefine the exposure ratio $ R $ as the ratio of the two AUCs for the two occasions:\n$$\nR = \\frac{\\mathrm{AUC}_{1}}{\\mathrm{AUC}_{2}} = \\frac{D_{1}/\\mathrm{CL}}{D_{2}/\\mathrm{CL}}.\n$$\n\nYou are asked to analyze uncertainty propagation in $ R $ under two treatments of the shared uncertain parameter $ \\mathrm{CL} $:\n- The correct dependent treatment, which acknowledges that the same $ \\mathrm{CL} $ appears in both numerator and denominator.\n- A naive interval-arithmetic treatment that ignores the dependency and treats the $ \\mathrm{CL} $ in the numerator and the $ \\mathrm{CL} $ in the denominator as if they were independent occurrences ranging over $ [3.0,\\,5.0]\\,\\mathrm{L\\,h^{-1}} $.\n\nFrom first principles, determine the exact image set of $ R $ under the dependent treatment, and the naive interval bound obtained by treating the two occurrences of $ \\mathrm{CL} $ independently. Then, compute the width of the naive interval bound for $ R $.\n\nExpress the final width as a pure number (dimensionless) and round your answer to four significant figures.",
            "solution": "The user has provided a problem concerning uncertainty propagation in a pharmacokinetic model. The first step is to validate the problem statement.\n\n### Step 1: Extract Givens\n- **Pharmacokinetic Model**: Single-subject, linear pharmacokinetics.\n- **Dose-AUC relationship**: $ \\mathrm{AUC} = \\mathrm{Dose}/\\mathrm{CL} $ for an intravenous bolus.\n- **Dose 1**: $ D_{1} = 100\\,\\mathrm{mg} $.\n- **Dose 2**: $ D_{2} = 150\\,\\mathrm{mg} $.\n- **Clearance ($ \\mathrm{CL} $)**: Constant for the subject, but epistemically uncertain.\n- **Uncertainty Interval for Clearance**: $ \\mathrm{CL} \\in [3.0,\\,5.0]\\,\\mathrm{L\\,h^{-1}} $.\n- **Definition of Exposure Ratio ($ R $)**:\n$$\nR = \\frac{\\mathrm{AUC}_{1}}{\\mathrm{AUC}_{2}} = \\frac{D_{1}/\\mathrm{CL}}{D_{2}/\\mathrm{CL}}\n$$\n- **Tasks**:\n  1. Determine the exact image set of $ R $ under the correct dependent treatment of $ \\mathrm{CL} $.\n  2. Determine the naive interval bound for $ R $ treating the two occurrences of $ \\mathrm{CL} $ as independent.\n  3. Compute the width of the naive interval bound for $ R $.\n  4. Express the final width as a pure number rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem uses a standard, valid model from pharmacokinetics ($ \\mathrm{AUC} = \\mathrm{Dose}/\\mathrm{CL} $). The concept of analyzing uncertainty propagation for epistemically uncertain parameters is a fundamental topic in biomedical systems modeling and engineering. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly defined with all necessary data ($ D_1 $, $ D_2 $, the interval for $ \\mathrm{CL} $) and a precise objective. The distinction between dependent and independent parameter treatment is a standard concept in uncertainty analysis, and the tasks lead to a unique, meaningful solution.\n- **Objective**: The problem is stated in precise, objective, and quantitative terms.\n- **Other Flaws**: The problem does not exhibit any of the listed flaws. It is not incomplete, contradictory, unrealistic, or ill-posed. It presents a conceptually important, non-trivial problem in uncertainty quantification.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\nThe problem requires an analysis of the uncertainty in the exposure ratio $ R $ under two different assumptions about the shared parameter $ \\mathrm{CL} $.\n\n**1. Dependent Treatment (Correct Analysis)**\n\nThe exposure ratio $ R $ is defined as:\n$$\nR = \\frac{\\mathrm{AUC}_{1}}{\\mathrm{AUC}_{2}}\n$$\nSubstituting the formula for $ \\mathrm{AUC} $, we get:\n$$\nR = \\frac{D_{1}/\\mathrm{CL}}{D_{2}/\\mathrm{CL}}\n$$\nIn this treatment, we recognize that the parameter $ \\mathrm{CL} $ is the same physical quantity in both the numerator and the denominator. The subject's clearance does not change between the two administrations. Since the problem specifies that $ \\mathrm{CL} \\in [3.0, 5.0] $, $ \\mathrm{CL} $ is strictly positive, so we can simplify the expression by multiplication with $ \\frac{\\mathrm{CL}}{\\mathrm{CL}} = 1 $:\n$$\nR = \\left(\\frac{D_{1}}{\\mathrm{CL}}\\right) \\left(\\frac{\\mathrm{CL}}{D_{2}}\\right) = \\frac{D_{1}}{D_{2}}\n$$\nThe parameter $ \\mathrm{CL} $ algebraically cancels out. Therefore, the value of $ R $ is independent of the value of $ \\mathrm{CL} $. The uncertainty in $ \\mathrm{CL} $ does not propagate to $ R $. We can calculate the exact value of $ R $ using the given doses:\n$$\nR = \\frac{D_{1}}{D_{2}} = \\frac{100\\,\\mathrm{mg}}{150\\,\\mathrm{mg}} = \\frac{100}{150} = \\frac{2}{3}\n$$\nThe image set of $ R $ under the correct dependent treatment is a single point, $\\{ \\frac{2}{3} \\}$.\n\n**2. Naive Interval-Arithmetic Treatment (Incorrect Analysis)**\n\nThis approach incorrectly ignores the dependency of the sub-expressions on the same parameter $ \\mathrm{CL} $. It treats the $ \\mathrm{CL} $ in the numerator's expression and the $ \\mathrm{CL} $ in the denominator's expression as if they were two independent variables, let's call them $ \\mathrm{CL}_{num} $ and $ \\mathrm{CL}_{den} $, both varying over the interval $ [3.0,\\,5.0] $.\n\nThe expression for $ R $ under this naive assumption, denoted $ R_{\\text{naive}} $, becomes:\n$$\nR_{\\text{naive}} = \\frac{D_{1}/\\mathrm{CL}_{num}}{D_{2}/\\mathrm{CL}_{den}} = \\frac{D_{1}}{D_{2}} \\cdot \\frac{\\mathrm{CL}_{den}}{\\mathrm{CL}_{num}}\n$$\nTo find the interval bound for $ R_{\\text{naive}} $, we apply the rules of interval arithmetic. The interval for $ R_{\\text{naive}} $ is given by $ [\\min(R_{\\text{naive}}), \\max(R_{\\text{naive}})] $.\n\nThe term $ \\frac{D_{1}}{D_{2}} $ is a constant, $ \\frac{2}{3} $. We need to find the interval for the ratio $ \\frac{\\mathrm{CL}_{den}}{\\mathrm{CL}_{num}} $.\n\nThe minimum value of $ R_{\\text{naive}} $ occurs when the ratio $ \\frac{\\mathrm{CL}_{den}}{\\mathrm{CL}_{num}} $ is minimized. This happens when the numerator, $ \\mathrm{CL}_{den} $, is at its minimum value and the denominator, $ \\mathrm{CL}_{num} $, is at its maximum value.\n$$\n\\min(R_{\\text{naive}}) = \\frac{D_{1}}{D_{2}} \\cdot \\frac{\\min(\\mathrm{CL}_{den})}{\\max(\\mathrm{CL}_{num})} = \\frac{100}{150} \\cdot \\frac{3.0}{5.0} = \\frac{2}{3} \\cdot \\frac{3}{5} = \\frac{2}{5}\n$$\n\nThe maximum value of $ R_{\\text{naive}} $ occurs when the ratio $ \\frac{\\mathrm{CL}_{den}}{\\mathrm{CL}_{num}} $ is maximized. This happens when the numerator, $ \\mathrm{CL}_{den} $, is at its maximum value and the denominator, $ \\mathrm{CL}_{num} $, is at its minimum value.\n$$\n\\max(R_{\\text{naive}}) = \\frac{D_{1}}{D_{2}} \\cdot \\frac{\\max(\\mathrm{CL}_{den})}{\\min(\\mathrm{CL}_{num})} = \\frac{100}{150} \\cdot \\frac{5.0}{3.0} = \\frac{2}{3} \\cdot \\frac{5}{3} = \\frac{10}{9}\n$$\nThus, the naive interval bound for $ R $ is $ [\\frac{2}{5}, \\frac{10}{9}] $.\n\n**3. Compute the Width of the Naive Interval Bound**\n\nThe width of an interval $ [a, b] $ is given by $ b - a $. The width of the naive interval for $ R $ is:\n$$\n\\text{Width} = \\max(R_{\\text{naive}}) - \\min(R_{\\text{naive}}) = \\frac{10}{9} - \\frac{2}{5}\n$$\nTo subtract the fractions, we find a common denominator, which is $ 9 \\times 5 = 45 $.\n$$\n\\text{Width} = \\frac{10 \\cdot 5}{9 \\cdot 5} - \\frac{2 \\cdot 9}{5 \\cdot 9} = \\frac{50}{45} - \\frac{18}{45} = \\frac{50 - 18}{45} = \\frac{32}{45}\n$$\nThe problem asks for this result as a pure number rounded to four significant figures.\n$$\n\\text{Width} = \\frac{32}{45} \\approx 0.711111...\n$$\nRounding to four significant figures, we get $ 0.7111 $. This non-zero width represents the spurious uncertainty introduced by ignoring the dependency of the sub-expressions on the shared parameter $ \\mathrm{CL} $.",
            "answer": "$$\n\\boxed{0.7111}\n$$"
        },
        {
            "introduction": "Having seen the pitfalls of simplistic propagation methods, we now turn to a more rigorous approach for understanding how parameter uncertainty impacts model dynamics. Local sensitivity analysis provides a powerful tool to quantify this influence by calculating the derivative of model outputs with respect to model parameters. This practice requires deriving the fundamental sensitivity equations from first principles and applying them to a common pharmacokinetic model, bridging theoretical understanding with practical application and equipping you to identify which parameters are most critical to estimate accurately .",
            "id": "3941531",
            "problem": "Consider a parametric dynamical system used in biomedical systems modeling, defined by the ordinary differential equation (ODE) $ \\frac{d x}{d t} = f(x,\\theta,t) $ with state $ x(t) \\in \\mathbb{R}^{n} $, parameter $ \\theta \\in \\mathbb{R} $, and a sufficiently smooth right-hand side $ f:\\mathbb{R}^{n} \\times \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}^{n} $. Define the parameter sensitivity $ s(t) = \\frac{\\partial x(t)}{\\partial \\theta} $. Starting from the definitions of differentiability and the chain rule for multivariate functions, derive the ODE governing $ s(t) $ that expresses $ \\frac{d}{dt}\\left( \\frac{\\partial x}{\\partial \\theta} \\right) $ in terms of $ \\frac{\\partial f}{\\partial x} $, $ \\frac{\\partial f}{\\partial \\theta} $, and $ \\frac{\\partial x}{\\partial \\theta} $, together with the sensitivity initial condition expressed via $ \\frac{\\partial x(0)}{\\partial \\theta} $.\n\nThen apply your derived sensitivity ODEs to a one-compartment extravascular pharmacokinetic model with first-order absorption and elimination. The system states are the amount of drug in the gastrointestinal tract $ A_{g}(t) $ and the amount in the central compartment $ A_{c}(t) $. The model is\n$$\n\\frac{d A_{g}}{d t} = - k_{a} A_{g}, \\qquad \\frac{d A_{c}}{d t} = k_{a} A_{g} - k_{e} A_{c},\n$$\nwith initial conditions $ A_{g}(0) = D $ and $ A_{c}(0) = 0 $, where $ D $ is the administered oral dose, $ k_{a} $ is the first-order absorption rate constant, and $ k_{e} $ is the first-order elimination rate constant. The plasma concentration is $ C(t) = \\frac{A_{c}(t)}{V} $, where $ V $ is the apparent volume of distribution.\n\nTreat $ \\theta = k_{e} $ as the uncertain parameter and use the sensitivity ODEs to compute the parameter sensitivity of concentration with respect to $ k_{e} $ at time $ t $, namely $ \\frac{\\partial C(t)}{\\partial k_{e}} $. Evaluate this sensitivity at $ t = 6 $ hours for the parameter values $ D = 100 $ mg, $ k_{a} = 1 $ hour$^{-1}$, $ k_{e} = 0.15 $ hour$^{-1}$, and $ V = 50 $ L. Express your final numerical answer in milligrams-hours per liter (mg·hour/L) and round your answer to four significant figures.",
            "solution": "The problem is divided into two parts: first, a general derivation of the sensitivity ODEs, and second, an application of this result to a specific pharmacokinetic model.\n\n**Part 1: Derivation of the Sensitivity ODE**\n\nWe are given a parametric dynamical system described by the ordinary differential equation (ODE):\n$$\n\\frac{d x}{d t} = f(x, \\theta, t)\n$$\nwhere $x(t) \\in \\mathbb{R}^{n}$ is the state vector, $\\theta \\in \\mathbb{R}$ is a scalar parameter, and $t$ is time. The function $f$ is assumed to be sufficiently smooth.\n\nThe parameter sensitivity of the state with respect to the parameter $\\theta$ is defined as:\n$$\ns(t) = \\frac{\\partial x(t)}{\\partial \\theta}\n$$\nOur goal is to find the ODE that governs the evolution of $s(t)$. We begin by differentiating $s(t)$ with respect to time $t$:\n$$\n\\frac{d s}{d t} = \\frac{d}{dt} \\left( \\frac{\\partial x}{\\partial \\theta} \\right)\n$$\nSince the state $x$ is a function of both $t$ and $\\theta$, and we assume sufficient smoothness (i.e., continuous second partial derivatives), we can apply Clairaut's theorem to interchange the order of differentiation:\n$$\n\\frac{d}{dt} \\left( \\frac{\\partial x}{\\partial \\theta} \\right) = \\frac{\\partial}{\\partial \\theta} \\left( \\frac{d x}{d t} \\right)\n$$\nNow, we substitute the definition of the system's dynamics, $\\frac{d x}{d t} = f(x(t, \\theta), \\theta, t)$:\n$$\n\\frac{d s}{d t} = \\frac{\\partial}{\\partial \\theta} f(x(t, \\theta), \\theta, t)\n$$\nThe function $f$ depends on the parameter $\\theta$ in two ways: explicitly as the second argument, and implicitly through the state vector $x(t, \\theta)$ which is a function of $\\theta$. We must apply the multivariate chain rule to compute the total derivative of $f$ with respect to $\\theta$:\n$$\n\\frac{\\partial f}{\\partial \\theta} (\\text{total}) = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial \\theta} + \\frac{\\partial f}{\\partial \\theta} (\\text{explicit})\n$$\nIn this expression:\n- $\\frac{\\partial f}{\\partial x}$ is the Jacobian matrix of the vector function $f$ with respect to the state vector $x$. Its components are $ (\\frac{\\partial f}{\\partial x})_{ij} = \\frac{\\partial f_i}{\\partial x_j} $.\n- $\\frac{\\partial x}{\\partial \\theta}$ is, by definition, the sensitivity vector $s(t)$.\n- $\\frac{\\partial f}{\\partial \\theta}$ is the partial derivative of the function $f$ with respect to its second argument, holding the other arguments $x$ and $t$ constant.\n\nSubstituting these terms back, we obtain the sensitivity ODE:\n$$\n\\frac{d s}{d t} = \\frac{\\partial f}{\\partial x} s(t) + \\frac{\\partial f}{\\partial \\theta}\n$$\nThe initial condition for this ODE is found by evaluating the definition of sensitivity at $t=0$:\n$$\ns(0) = \\left. \\frac{\\partial x(t)}{\\partial \\theta} \\right|_{t=0} = \\frac{\\partial x(0)}{\\partial \\theta}\n$$\nIf the initial state $x(0)$ is independent of the parameter $\\theta$, then this initial condition is $s(0) = 0$.\n\n**Part 2: Application to a Pharmacokinetic Model**\n\nThe pharmacokinetic model is given by:\n$$\n\\frac{d A_{g}}{d t} = - k_{a} A_{g}\n$$\n$$\n\\frac{d A_{c}}{d t} = k_{a} A_{g} - k_{e} A_{c}\n$$\nThe state vector is $x(t) = \\begin{pmatrix} A_{g}(t) \\\\ A_{c}(t) \\end{pmatrix}$. The uncertain parameter is $\\theta = k_{e}$. The system can be written in the form $\\frac{dx}{dt} = f(x, k_{e})$ where:\n$$\nf(x, k_{e}) = \\begin{pmatrix} -k_{a} A_{g} \\\\ k_{a} A_{g} - k_{e} A_{c} \\end{pmatrix}\n$$\nThe sensitivity vector is $s(t) = \\frac{\\partial x}{\\partial k_{e}} = \\begin{pmatrix} \\frac{\\partial A_{g}}{\\partial k_{e}} \\\\ \\frac{\\partial A_{c}}{\\partial k_{e}} \\end{pmatrix} = \\begin{pmatrix} s_{g}(t) \\\\ s_{c}(t) \\end{pmatrix}$.\n\nTo build the sensitivity ODE, we compute the required partial derivatives:\nThe Jacobian matrix $\\frac{\\partial f}{\\partial x}$:\n$$\n\\frac{\\partial f}{\\partial x} = \\begin{pmatrix} \\frac{\\partial}{\\partial A_{g}}(-k_{a} A_{g}) & \\frac{\\partial}{\\partial A_{c}}(-k_{a} A_{g}) \\\\ \\frac{\\partial}{\\partial A_{g}}(k_{a} A_{g} - k_{e} A_{c}) & \\frac{\\partial}{\\partial A_{c}}(k_{a} A_{g} - k_{e} A_{c}) \\end{pmatrix} = \\begin{pmatrix} -k_{a} & 0 \\\\ k_{a} & -k_{e} \\end{pmatrix}\n$$\nThe partial derivative with respect to the parameter $\\frac{\\partial f}{\\partial k_{e}}$:\n$$\n\\frac{\\partial f}{\\partial k_{e}} = \\begin{pmatrix} \\frac{\\partial}{\\partial k_{e}}(-k_{a} A_{g}) \\\\ \\frac{\\partial}{\\partial k_{e}}(k_{a} A_{g} - k_{e} A_{c}) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -A_{c} \\end{pmatrix}\n$$\nThe system of sensitivity ODEs is $\\frac{ds}{dt} = \\frac{\\partial f}{\\partial x} s + \\frac{\\partial f}{\\partial k_{e}}$:\n$$\n\\begin{pmatrix} \\frac{d s_{g}}{d t} \\\\ \\frac{d s_{c}}{d t} \\end{pmatrix} = \\begin{pmatrix} -k_{a} & 0 \\\\ k_{a} & -k_{e} \\end{pmatrix} \\begin{pmatrix} s_{g} \\\\ s_{c} \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -A_{c}(t) \\end{pmatrix}\n$$\nThis yields two coupled ODEs:\n1. $\\frac{d s_{g}}{d t} = -k_{a} s_{g}$\n2. $\\frac{d s_{c}}{d t} = k_{a} s_{g} - k_{e} s_{c} - A_{c}(t)$\n\nThe initial conditions for the state are $A_{g}(0) = D$ and $A_{c}(0) = 0$. Since these are independent of $k_{e}$, the initial conditions for the sensitivities are:\n$s_{g}(0) = \\frac{\\partial A_{g}(0)}{\\partial k_{e}} = \\frac{\\partial D}{\\partial k_{e}} = 0$\n$s_{c}(0) = \\frac{\\partial A_{c}(0)}{\\partial k_{e}} = \\frac{\\partial 0}{\\partial k_{e}} = 0$\n\nSolving the first sensitivity ODE with $s_{g}(0)=0$ gives $s_{g}(t)=0$ for all $t$. This is expected, as the solution for $A_{g}(t) = D \\exp(-k_a t)$ does not depend on $k_e$.\nThe second sensitivity ODE simplifies to:\n$$\n\\frac{d s_{c}}{d t} = -k_{e} s_{c} - A_{c}(t) \\quad \\text{or} \\quad \\frac{d s_{c}}{d t} + k_{e} s_{c} = -A_{c}(t)\n$$\nTo solve this, we first need the solution for $A_{c}(t)$. The ODE for $A_{c}(t)$ is $\\frac{d A_{c}}{d t} = k_{a} A_{g} - k_{e} A_{c} = k_{a} D \\exp(-k_{a} t) - k_{e} A_{c}$. This is a first-order linear ODE with initial condition $A_c(0)=0$. For $k_a \\neq k_e$, the solution is:\n$$\nA_{c}(t) = \\frac{k_{a} D}{k_{a} - k_{e}} (\\exp(-k_{e} t) - \\exp(-k_{a} t))\n$$\nNow we solve for $s_c(t)$. The ODE is $\\frac{d s_{c}}{d t} + k_{e} s_{c} = -\\frac{k_{a} D}{k_{a} - k_{e}} (\\exp(-k_{e} t) - \\exp(-k_{a} t))$. Using an integrating factor $I(t) = \\exp(\\int k_e dt) = \\exp(k_e t)$, we get:\n$$\n\\frac{d}{dt}(s_c(t) \\exp(k_e t)) = -\\frac{k_{a} D}{k_{a} - k_{e}} (1 - \\exp((k_e - k_a)t))\n$$\nIntegrating from $0$ to $t$ and using $s_c(0)=0$:\n$$\ns_c(t) \\exp(k_e t) = -\\frac{k_{a} D}{k_{a} - k_{e}} \\left[ \\tau - \\frac{\\exp((k_e - k_a)\\tau)}{k_e - k_a} \\right]_0^t\n$$\n$$\ns_c(t) \\exp(k_e t) = -\\frac{k_{a} D}{k_{a} - k_{e}} \\left( t - \\frac{\\exp((k_e - k_a)t)}{k_e - k_a} - \\left( 0 - \\frac{1}{k_e - k_a} \\right) \\right)\n$$\n$$\ns_c(t) \\exp(k_e t) = \\frac{k_{a} D}{(k_{a} - k_{e})^2} (1 - \\exp((k_e - k_a)t)) - \\frac{k_{a} D t}{k_{a} - k_{e}}\n$$\nMultiplying by $\\exp(-k_e t)$:\n$$\ns_c(t) = \\frac{k_{a} D}{(k_{a} - k_{e})^2} (\\exp(-k_{e} t) - \\exp(-k_{a} t)) - \\frac{k_{a} D t}{k_{a} - k_{e}} \\exp(-k_{e} t)\n$$\nThe quantity of interest is the sensitivity of concentration $C(t) = \\frac{A_{c}(t)}{V}$.\n$$\n\\frac{\\partial C(t)}{\\partial k_{e}} = \\frac{1}{V} \\frac{\\partial A_{c}(t)}{\\partial k_{e}} = \\frac{s_c(t)}{V}\n$$\n$$\n\\frac{\\partial C(t)}{\\partial k_{e}} = \\frac{1}{V} \\left[ \\frac{k_{a} D}{(k_{a} - k_{e})^2} (\\exp(-k_{e} t) - \\exp(-k_{a} t)) - \\frac{k_{a} D t}{k_{a} - k_{e}} \\exp(-k_{e} t) \\right]\n$$\nNow we substitute the given numerical values: $D=100$ mg, $k_a=1$ hour$^{-1}$, $k_e=0.15$ hour$^{-1}$, $V=50$ L, and $t=6$ hours.\n$$\nk_{a} - k_{e} = 1 - 0.15 = 0.85 \\text{ hour}^{-1}\n$$\n$$\n\\frac{\\partial C}{\\partial k_{e}}(t=6) = \\frac{1}{50} \\left[ \\frac{1 \\cdot 100}{(0.85)^2} (\\exp(-0.15 \\cdot 6) - \\exp(-1 \\cdot 6)) - \\frac{1 \\cdot 100 \\cdot 6}{0.85} \\exp(-0.15 \\cdot 6) \\right]\n$$\n$$\n\\frac{\\partial C}{\\partial k_{e}}(t=6) \\approx \\frac{1}{50} [ 138.4083 (0.406570 - 0.002479) - 705.8824 (0.406570) ]\n$$\n$$\n\\frac{\\partial C}{\\partial k_{e}}(t=6) \\approx \\frac{1}{50} [ 138.4083 (0.404091) - 287.0003 ]\n$$\n$$\n\\frac{\\partial C}{\\partial k_{e}}(t=6) \\approx \\frac{1}{50} [ 55.9392 - 287.0003 ]\n$$\n$$\n\\frac{\\partial C}{\\partial k_{e}}(t=6) \\approx \\frac{1}{50} [ -231.0611 ]\n$$\n$$\n\\frac{\\partial C}{\\partial k_{e}}(t=6) \\approx -4.621222 \\text{ mg} \\cdot \\text{hour/L}\n$$\nRounding to four significant figures, the result is $-4.621$.",
            "answer": "$$\n\\boxed{-4.621}\n$$"
        },
        {
            "introduction": "We now move from propagating known uncertainty to a complete framework for learning about uncertain parameters from data. Bayesian inference provides a principled way to update our knowledge, expressed as a probability distribution, in light of new observations. This exercise guides you through the entire Bayesian workflow for a foundational biomedical model—the Poisson process for discrete events—by constructing a conjugate prior, deriving the posterior distribution, and generating predictions, allowing you to practice the core mechanics of how prior knowledge is formally combined with data .",
            "id": "3941467",
            "problem": "A research team is modeling spontaneous exocytosis events in single pancreatic beta cells. During each observation window of length $t$ minutes, the number of exocytosis events is recorded. The team assumes a stationary Poisson process with unknown event rate $\\lambda$ (events per minute), so that the count $y_i$ observed in window $i$ is modeled as independent and identically distributed according to a Poisson law with mean $\\lambda t$. The team wishes to perform Bayesian uncertainty quantification for $\\lambda$ and uncertainty propagation to future counts using only fundamental principles: the definition of the Poisson likelihood and Bayes' theorem.\n\nYou are tasked with the following, starting from these base definitions:\n\n- Construct a conjugate prior family for the Poisson rate parameter $\\lambda$ appropriate for this setting and justify its choice in terms of conjugacy and interpretability of hyperparameters in the biomedical context.\n- Derive the posterior distribution of $\\lambda$ after observing $n$ independent windows with counts $\\{y_1,\\dots,y_n\\}$, each of duration $t$ minutes.\n- Derive the one-step-ahead posterior predictive distribution for the count in a future observation window of duration $t^{\\star}$ minutes, and express its probability mass function in closed form.\n- Explain qualitatively, in terms of effective prior information, how the posterior variance of $\\lambda$ depends on prior hyperparameters and the observed data.\n\nAnswer specification:\n- Your final answer must be the exact closed-form analytic expression for the posterior variance of $\\lambda$ given the $n$ observations, expressed in terms of the prior hyperparameters $a_0$ and $b_0$, the common window length $t$, the number of windows $n$, and the total count $\\sum_{i=1}^{n} y_i$. Do not substitute numerical values.\n- Do not include units inside the final answer box; express the variance symbolically.",
            "solution": "The problem statement is parsed and validated according to the specified protocol.\n\n### Step 1: Extract Givens\n-   Observation window length: $t$ minutes.\n-   Number of exocytosis events in window $i$: $y_i$.\n-   Model for counts: $y_i \\sim \\text{Poisson}(\\lambda t)$, independent and identically distributed for $i=1, \\dots, n$.\n-   Unknown parameter: $\\lambda$ (event rate in events per minute).\n-   Task 1: Construct a conjugate prior for $\\lambda$ and justify its choice.\n-   Task 2: Derive the posterior distribution of $\\lambda$ given observations $\\{y_1, \\dots, y_n\\}$.\n-   Task 3: Derive the one-step-ahead posterior predictive distribution for a count in a future window of duration $t^{\\star}$.\n-   Task 4: Qualitatively explain the dependence of the posterior variance of $\\lambda$ on prior hyperparameters and data.\n-   Final Answer Requirement: Provide the closed-form expression for the posterior variance of $\\lambda$, denoted $\\text{Var}(\\lambda | y_1, \\dots, y_n)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n-   **Scientific Grounding**: Modeling event counts with a Poisson process is a standard and fundamental technique in many scientific fields, including biomedical research. The use of Bayesian inference with conjugate priors is a mathematically rigorous and widely accepted method for uncertainty quantification. The problem is based on sound statistical and mathematical principles.\n-   **Well-Posedness**: The problem is clearly specified. It provides a statistical model and asks for standard derivations within the Bayesian framework (prior specification, posterior derivation, predictive distribution derivation, and interpretation of posterior moments). These tasks lead to a unique and meaningful solution.\n-   **Objectivity**: The problem is stated in precise, formal language, free of ambiguity, subjectivity, or non-scientific claims.\n\nThe problem does not exhibit any of the flaws listed in the invalidation criteria. It is a standard, well-defined problem in Bayesian statistics applied to a biomedical context.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivations\n\nThe solution proceeds by addressing the four tasks outlined in the problem statement.\n\n**Task 1: Conjugate Prior for $\\lambda$**\n\nFirst, we write down the likelihood function for the observed data. For a single observation window $i$, the probability mass function (PMF) of the count $y_i$ is given by the Poisson distribution:\n$$P(y_i|\\lambda) = \\frac{(\\lambda t)^{y_i} \\exp(-\\lambda t)}{y_i!}$$\nSince the $n$ observations $\\{y_1, \\dots, y_n\\}$ are independent and identically distributed, the total likelihood function $L(\\lambda | y_1, \\dots, y_n)$ is the product of the individual PMFs:\n$$L(\\lambda | y_1, \\dots, y_n) = \\prod_{i=1}^{n} P(y_i|\\lambda) = \\prod_{i=1}^{n} \\frac{(\\lambda t)^{y_i} \\exp(-\\lambda t)}{y_i!}$$\n$$= \\frac{(t)^{\\sum_{i=1}^{n} y_i}}{\\prod_{i=1}^{n} y_i!} (\\lambda)^{\\sum_{i=1}^{n} y_i} \\exp(-n \\lambda t)$$\nFor the purpose of finding a conjugate prior, we only need the kernel of the likelihood, which is the part that depends on $\\lambda$:\n$$L(\\lambda | y_1, \\dots, y_n) \\propto \\lambda^{\\sum_{i=1}^{n} y_i} \\exp(-n t \\lambda)$$\nThis kernel has the form $\\lambda^{k} \\exp(-c \\lambda)$, which is characteristic of the Gamma distribution. A probability distribution is a conjugate prior for a likelihood function if the resulting posterior distribution is in the same family as the prior. Therefore, the conjugate prior for the Poisson rate parameter $\\lambda$ is the Gamma distribution.\n\nWe define the prior for $\\lambda$ as a Gamma distribution with shape parameter $a_0$ and rate parameter $b_0$:\n$$p(\\lambda) = \\text{Gamma}(\\lambda | a_0, b_0) = \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\lambda^{a_0-1} \\exp(-b_0 \\lambda)$$\nwhere $\\Gamma(\\cdot)$ is the gamma function.\n\n*   **Justification of Choice:**\n    1.  **Conjugacy**: As will be shown in the next step, using a Gamma prior for a Poisson rate parameter results in a Gamma posterior. This property, known as conjugacy, is computationally convenient as it yields a closed-form analytical expression for the posterior, avoiding the need for numerical integration methods like MCMC.\n    2.  **Interpretability**: The hyperparameters $a_0$ and $b_0$ have a clear interpretation in this biomedical context. They can be thought of as representing prior information equivalent to having observed $a_0$ events during a total observation time of $b_0$ minutes. The prior mean rate is $E[\\lambda] = a_0/b_0$, and the prior variance is $\\text{Var}(\\lambda) = a_0/b_0^2$. A large $b_0$ relative to $a_0$ represents strong prior belief (low variance) about the value of $\\lambda$. This allows researchers to formally incorporate knowledge from previous studies or established literature.\n\n**Task 2: Posterior Distribution of $\\lambda$**\n\nAccording to Bayes' theorem, the posterior distribution $p(\\lambda | y_1, \\dots, y_n)$ is proportional to the product of the likelihood and the prior:\n$$p(\\lambda | y_1, \\dots, y_n) \\propto L(\\lambda | y_1, \\dots, y_n) \\times p(\\lambda)$$\n$$p(\\lambda | y_1, \\dots, y_n) \\propto \\left( \\lambda^{\\sum_{i=1}^{n} y_i} \\exp(-n t \\lambda) \\right) \\times \\left( \\lambda^{a_0-1} \\exp(-b_0 \\lambda) \\right)$$\nCombining the terms involving $\\lambda$:\n$$p(\\lambda | y_1, \\dots, y_n) \\propto \\lambda^{(\\sum_{i=1}^{n} y_i) + a_0 - 1} \\exp(-(nt + b_0)\\lambda)$$\nThis is the kernel of a Gamma distribution. We can identify the updated (posterior) hyperparameters, which we will call $a_n$ and $b_n$:\n$$a_n = a_0 + \\sum_{i=1}^{n} y_i$$\n$$b_n = b_0 + nt$$\nThus, the posterior distribution for $\\lambda$ is a Gamma distribution:\n$$p(\\lambda | y_1, \\dots, y_n) = \\text{Gamma}(\\lambda | a_n, b_n) = \\text{Gamma}\\left(\\lambda \\bigg| a_0 + \\sum_{i=1}^{n} y_i, b_0 + nt\\right)$$\nThe posterior mean is $E[\\lambda | y_1, \\dots, y_n] = a_n/b_n$, and the posterior variance is $\\text{Var}(\\lambda | y_1, \\dots, y_n) = a_n/b_n^2$.\n\n**Task 3: Posterior Predictive Distribution**\n\nThe posterior predictive distribution gives the probability of a new observation, $y_{\\text{new}}$, given the previous data. The new observation is from a window of duration $t^{\\star}$, so its likelihood is $p(y_{\\text{new}}|\\lambda) = \\text{Poisson}(y_{\\text{new}}|\\lambda t^{\\star})$. We find the posterior predictive PMF by marginalizing the product of this likelihood and the posterior distribution of $\\lambda$ over all possible values of $\\lambda$:\n$$p(y_{\\text{new}}|y_1, \\dots, y_n) = \\int_{0}^{\\infty} p(y_{\\text{new}}|\\lambda) p(\\lambda|y_1, \\dots, y_n) d\\lambda$$\nSubstituting the respective functions:\n$$p(y_{\\text{new}}|y_1, \\dots, y_n) = \\int_{0}^{\\infty} \\left[ \\frac{(\\lambda t^{\\star})^{y_{\\text{new}}} \\exp(-\\lambda t^{\\star})}{y_{\\text{new}}!} \\right] \\left[ \\frac{b_n^{a_n}}{\\Gamma(a_n)} \\lambda^{a_n-1} \\exp(-b_n \\lambda) \\right] d\\lambda$$\nWe group terms that do not depend on $\\lambda$ outside the integral:\n$$= \\frac{(t^{\\star})^{y_{\\text{new}}} b_n^{a_n}}{y_{\\text{new}}! \\Gamma(a_n)} \\int_{0}^{\\infty} \\lambda^{y_{\\text{new}}} \\exp(-\\lambda t^{\\star}) \\lambda^{a_n-1} \\exp(-b_n \\lambda) d\\lambda$$\n$$= \\frac{(t^{\\star})^{y_{\\text{new}}} b_n^{a_n}}{y_{\\text{new}}! \\Gamma(a_n)} \\int_{0}^{\\infty} \\lambda^{a_n + y_{\\text{new}} - 1} \\exp(-(b_n + t^{\\star})\\lambda) d\\lambda$$\nThe integral is the kernel of a Gamma distribution, $\\text{Gamma}(a_n + y_{\\text{new}}, b_n + t^{\\star})$, integrated over its domain. The value of this integral is $\\frac{\\Gamma(a_n + y_{\\text{new}})}{(b_n + t^{\\star})^{a_n + y_{\\text{new}}}}$.\nSubstituting this back:\n$$p(y_{\\text{new}}|y_1, \\dots, y_n) = \\frac{(t^{\\star})^{y_{\\text{new}}} b_n^{a_n}}{y_{\\text{new}}! \\Gamma(a_n)} \\frac{\\Gamma(a_n + y_{\\text{new}})}{(b_n + t^{\\star})^{a_n + y_{\\text{new}}}}$$\nRearranging the terms:\n$$= \\frac{\\Gamma(a_n + y_{\\text{new}})}{\\Gamma(y_{\\text{new}}+1) \\Gamma(a_n)} \\left(\\frac{b_n}{b_n + t^{\\star}}\\right)^{a_n} \\left(\\frac{t^{\\star}}{b_n + t^{\\star}}\\right)^{y_{\\text{new}}}$$\nwhere we used $y_{\\text{new}}! = \\Gamma(y_{\\text{new}}+1)$. The term $\\frac{\\Gamma(a_n + y_{\\text{new}})}{\\Gamma(y_{\\text{new}}+1) \\Gamma(a_n)}$ is the definition of the binomial coefficient $\\binom{a_n+y_{\\text{new}}-1}{y_{\\text{new}}}$.\nThis is the PMF of a Negative Binomial distribution. Let $r = a_n$ and $p = \\frac{b_n}{b_n + t^{\\star}}$. The PMF is:\n$$p(y_{\\text{new}}|y_1, \\dots, y_n) = \\binom{y_{\\text{new}} + r - 1}{y_{\\text{new}}} p^r (1-p)^{y_{\\text{new}}}$$\nSo, the posterior predictive distribution for $y_{\\text{new}}$ is a Negative Binomial distribution, $y_{\\text{new}}|y_1, \\dots, y_n \\sim \\text{NB}(r, p)$, with parameters:\n$$r = a_n = a_0 + \\sum_{i=1}^{n} y_i$$\n$$p = \\frac{b_n}{b_n + t^{\\star}} = \\frac{b_0 + nt}{b_0 + nt + t^{\\star}}$$\n\n**Task 4: Qualitative Explanation of Posterior Variance**\n\nThe posterior variance of $\\lambda$ is given by the variance of the posterior distribution $\\text{Gamma}(\\lambda | a_n, b_n)$, which is:\n$$\\text{Var}(\\lambda | y_1, \\dots, y_n) = \\frac{a_n}{b_n^2} = \\frac{a_0 + \\sum_{i=1}^{n} y_i}{(b_0 + nt)^2}$$\n-   **Dependence on Prior Hyperparameters ($a_0$, $b_0$)**: The hyperparameters $a_0$ and $b_0$ represent the effective information from a prior experiment. An increase in $a_0$ (prior event count) increases the numerator, tending to increase variance. An increase in $b_0$ (prior observation time) increases the denominator quadratically, strongly decreasing the variance. Therefore, a more informative prior (larger $b_0$) leads to a smaller posterior variance, as the prior beliefs have more weight and constrain the possible values of $\\lambda$.\n-   **Dependence on Observed Data ($n$, $\\sum y_i$)**: The observed data contribute to the posterior through the number of windows, $n$, and the total count, $\\sum_{i=1}^{n} y_i$.\n    -   As the number of observations $n$ increases, the total data observation time $nt$ increases. This term appears in the denominator squared, as $(b_0 + nt)^2$. The total count $\\sum y_i$ is expected to grow linearly with $n$ (approximately as $n \\lambda t$). The denominator grows as $n^2$ while the numerator grows as $n$. The quadratic growth in the denominator dominates, causing the posterior variance to decrease as $n$ increases. This reflects the principle that more data leads to greater certainty (less variance) about the parameter.\n    -   For a fixed number of observations $n$, a larger total count $\\sum y_i$ implies a higher estimate for the rate $\\lambda$. This larger total count increases the numerator $a_n$, thus increasing the posterior variance. This might seem counterintuitive, but it is a property of the Gamma distribution where the variance ($a/b^2$) is proportional to the mean ($a/b$) divided by the rate parameter ($b$). For a fixed exposure time ($b_n$), a process with a higher mean rate naturally has a higher absolute uncertainty (variance).",
            "answer": "$$\\boxed{\\frac{a_0 + \\sum_{i=1}^{n} y_i}{(b_0 + nt)^2}}$$"
        }
    ]
}