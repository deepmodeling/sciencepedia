{
    "hands_on_practices": [
        {
            "introduction": "Understanding the origins of stochasticity in biological systems begins with mastering the fundamental models of molecular production and decay. This exercise guides you through the analysis of a simple birth-death process, often considered the \"hydrogen atom\" of stochastic gene expression. By deriving the stationary distribution from the Chemical Master Equation (CME), you will gain a first-principles understanding of how discrete, random molecular events culminate in a stable, predictable probability distribution for molecule numbers, which is a foundational skill for building and interpreting more complex stochastic models. ",
            "id": "3932684",
            "problem": "Consider a single intracellular molecular species undergoing constitutive synthesis and first-order degradation in a well-mixed single cell. Let $N(t)$ denote the molecular copy number at time $t$. The system is modeled as a continuous-time Markov process (birth-death process) with the following elementary reactions: a synthesis reaction that increases the copy number by one with constant propensity $k$ (units: molecules per minute), and a degradation reaction that decreases the copy number by one with propensity $\\gamma N(t)$ (units: per minute times molecules). Assume the system is ergodic and reaches a unique stationary distribution. Starting from the Chemical Master Equation (CME) for this birth-death process, derive the stationary distribution. Then, compute at stationarity the mean $\\mathbb{E}[N]$, the variance $\\mathrm{Var}[N]$, and the coefficient of variation defined as $\\mathrm{CV} = \\sqrt{\\mathrm{Var}[N]}/\\mathbb{E}[N]$. Use the parameter values $k = 20\\ \\text{molecules/min}$ and $\\gamma = 1\\ \\text{min}^{-1}$. Express the mean in molecules, the variance in molecules squared, and the coefficient of variation as a dimensionless quantity. No rounding is required; provide exact values.",
            "solution": "The problem describes a birth-death process for the number of molecules, $N(t)$, of a single species in a cell. The process consists of two elementary reactions:\n1. Synthesis (birth): $\\emptyset \\xrightarrow{k} \\text{Molecule}$, with a constant propensity (rate) $k$. This reaction increases the number of molecules from $n$ to $n+1$.\n2. Degradation (death): $\\text{Molecule} \\xrightarrow{\\gamma} \\emptyset$, with a propensity $\\gamma n$, where $n$ is the current number of molecules. This reaction decreases the number of molecules from $n$ to $n-1$.\n\nLet $P(n, t)$ be the probability that the system has $n$ molecules at time $t$. The time evolution of this probability distribution is governed by the Chemical Master Equation (CME). For an integer number of molecules $n \\ge 0$, the CME is a set of coupled ordinary differential equations:\n$$\n\\frac{d P(n, t)}{d t} = (\\text{flux into state } n) - (\\text{flux out of state } n)\n$$\nThe flux into state $n$ comes from state $n-1$ via synthesis and from state $n+1$ via degradation. The flux out of state $n$ goes to state $n+1$ via synthesis and to state $n-1$ via degradation.\nFor $n \\ge 1$, the CME is:\n$$\n\\frac{d P(n, t)}{d t} = k P(n-1, t) + \\gamma (n+1) P(n+1, t) - (k + \\gamma n) P(n, t)\n$$\nFor the boundary case $n=0$, synthesis from a non-existent state $n=-1$ is impossible, and degradation from state $n=0$ is also impossible. Thus, the CME for $n=0$ is:\n$$\n\\frac{d P(0, t)}{d t} = \\gamma (1) P(1, t) - k P(0, t)\n$$\nThe problem states that the system reaches a unique stationary distribution. At stationarity, the probability distribution is time-invariant, so $\\frac{d P(n, t)}{d t} = 0$ for all $n$. Let $P_n$ denote the stationary probability $P(n, t \\to \\infty)$. The stationary CME equations are:\n$$\n0 = k P_{n-1} + \\gamma (n+1) P_{n+1} - (k + \\gamma n) P_n \\quad \\text{for } n \\ge 1\n$$\n$$\n0 = \\gamma P_1 - k P_0 \\quad \\text{for } n=0\n$$\nFor a one-dimensional birth-death process, the stationary state is characterized by the principle of detailed balance, where the net probabilistic flux between any two adjacent states is zero. That is, the flux from state $n$ to $n+1$ must equal the flux from state $n+1$ to $n$:\n$$\n\\text{Flux}(n \\to n+1) = \\text{Flux}(n+1 \\to n)\n$$\n$$\nk P_n = \\gamma (n+1) P_{n+1} \\quad \\text{for all } n \\ge 0\n$$\nThis single recurrence relation satisfies both stationary CME equations. From it, we derive a relation for $P_{n+1}$ in terms of $P_n$:\n$$\nP_{n+1} = \\frac{k}{\\gamma(n+1)} P_n\n$$\nLet us define the parameter $\\alpha = k/\\gamma$. The recurrence becomes:\n$$\nP_{n+1} = \\frac{\\alpha}{n+1} P_n\n$$\nWe can solve this recurrence relation by iteration:\nFor $n=1$: $P_1 = \\frac{\\alpha}{1} P_0$\nFor $n=2$: $P_2 = \\frac{\\alpha}{2} P_1 = \\frac{\\alpha}{2} \\left(\\frac{\\alpha}{1} P_0\\right) = \\frac{\\alpha^2}{2 \\cdot 1} P_0 = \\frac{\\alpha^2}{2!} P_0$\nFor $n=3$: $P_3 = \\frac{\\alpha}{3} P_2 = \\frac{\\alpha}{3} \\left(\\frac{\\alpha^2}{2!} P_0\\right) = \\frac{\\alpha^3}{3!} P_0$\nBy induction, the general solution is:\n$$\nP_n = \\frac{\\alpha^n}{n!} P_0\n$$\nTo find the constant $P_0$, we use the normalization condition that the sum of all probabilities must be unity:\n$$\n\\sum_{n=0}^{\\infty} P_n = 1\n$$\n$$\n\\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} P_0 = P_0 \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} = 1\n$$\nThe sum is the Taylor series expansion of the exponential function, $\\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{n!} = \\exp(\\alpha)$.\n$$\nP_0 \\exp(\\alpha) = 1 \\implies P_0 = \\exp(-\\alpha)\n$$\nSubstituting this back into the expression for $P_n$, we obtain the stationary distribution:\n$$\nP_n = \\frac{\\alpha^n \\exp(-\\alpha)}{n!}\n$$\nThis is the probability mass function of a Poisson distribution with parameter $\\alpha = k/\\gamma$.\n\nNow, we compute the required statistical moments at stationarity.\nThe mean (expected value) of the number of molecules, $\\mathbb{E}[N]$, is:\n$$\n\\mathbb{E}[N] = \\sum_{n=0}^{\\infty} n P_n = \\sum_{n=1}^{\\infty} n \\frac{\\alpha^n \\exp(-\\alpha)}{n!} = \\exp(-\\alpha) \\sum_{n=1}^{\\infty} \\frac{\\alpha^n}{(n-1)!}\n$$\nLet $m = n-1$. The sum becomes:\n$$\n\\mathbb{E}[N] = \\exp(-\\alpha) \\sum_{m=0}^{\\infty} \\frac{\\alpha^{m+1}}{m!} = \\alpha \\exp(-\\alpha) \\sum_{m=0}^{\\infty} \\frac{\\alpha^m}{m!} = \\alpha \\exp(-\\alpha) \\exp(\\alpha) = \\alpha\n$$\nThus, the mean is $\\mathbb{E}[N] = \\alpha = k/\\gamma$.\n\nThe variance is $\\mathrm{Var}[N] = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2$. We first compute the second factorial moment, $\\mathbb{E}[N(N-1)]$:\n$$\n\\mathbb{E}[N(N-1)] = \\sum_{n=0}^{\\infty} n(n-1) P_n = \\sum_{n=2}^{\\infty} n(n-1) \\frac{\\alpha^n \\exp(-\\alpha)}{n!} = \\exp(-\\alpha) \\sum_{n=2}^{\\infty} \\frac{\\alpha^n}{(n-2)!}\n$$\nLet $m = n-2$. The sum becomes:\n$$\n\\mathbb{E}[N(N-1)] = \\exp(-\\alpha) \\sum_{m=0}^{\\infty} \\frac{\\alpha^{m+2}}{m!} = \\alpha^2 \\exp(-\\alpha) \\sum_{m=0}^{\\infty} \\frac{\\alpha^m}{m!} = \\alpha^2 \\exp(-\\alpha) \\exp(\\alpha) = \\alpha^2\n$$\nThe second moment $\\mathbb{E}[N^2]$ is related to the factorial moment by $\\mathbb{E}[N^2] = \\mathbb{E}[N(N-1) + N] = \\mathbb{E}[N(N-1)] + \\mathbb{E}[N]$.\n$$\n\\mathbb{E}[N^2] = \\alpha^2 + \\alpha\n$$\nNow, we can compute the variance:\n$$\n\\mathrm{Var}[N] = \\mathbb{E}[N^2] - (\\mathbb{E}[N])^2 = (\\alpha^2 + \\alpha) - (\\alpha)^2 = \\alpha\n$$\nThus, the variance is $\\mathrm{Var}[N] = \\alpha = k/\\gamma$.\n\nThe coefficient of variation, $\\mathrm{CV}$, is defined as the ratio of the standard deviation to the mean:\n$$\n\\mathrm{CV} = \\frac{\\sqrt{\\mathrm{Var}[N]}}{\\mathbb{E}[N]} = \\frac{\\sqrt{\\alpha}}{\\alpha} = \\frac{1}{\\sqrt{\\alpha}}\n$$\n\nFinally, we substitute the given parameter values: $k = 20\\ \\text{molecules/min}$ and $\\gamma = 1\\ \\text{min}^{-1}$.\nThe parameter $\\alpha$ is:\n$$\n\\alpha = \\frac{k}{\\gamma} = \\frac{20}{1} = 20\n$$\nThe mean number of molecules is:\n$$\n\\mathbb{E}[N] = \\alpha = 20\\ \\text{molecules}\n$$\nThe variance of the number of molecules is:\n$$\n\\mathrm{Var}[N] = \\alpha = 20\\ \\text{molecules}^2\n$$\nThe coefficient of variation is:\n$$\n\\mathrm{CV} = \\frac{1}{\\sqrt{\\alpha}} = \\frac{1}{\\sqrt{20}} = \\frac{1}{\\sqrt{4 \\times 5}} = \\frac{1}{2\\sqrt{5}} = \\frac{\\sqrt{5}}{10}\n$$\nThis quantity is dimensionless. The problem requires exact values, which we have provided.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 20 & 20 & \\frac{\\sqrt{5}}{10} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While the Chemical Master Equation provides an exact description, it can be analytically and computationally challenging for complex systems. A powerful alternative is the Chemical Langevin Equation (CLE), a continuous stochastic differential equation that approximates the discrete Markov process when molecule numbers are sufficiently large. This practice introduces the essential tool of Itô's calculus to analyze the CLE for the same birth-death system, allowing you to derive the dynamics of the mean and variance directly.  Comparing the steady-state results from this continuous approach with the exact discrete solution deepens your understanding of the model's validity and the connections between different mathematical frameworks.",
            "id": "3932700",
            "problem": "A single-species birth–death reaction network with constant birth propensity and linear death propensity can be approximated by the Chemical Langevin Equation (CLE), which is a stochastic differential equation driven by a Standard Wiener Process (SWP). Consider a molecular count variable $X(t)$ governed by the CLE\n$$\ndX(t) = \\left(k - \\gamma X(t)\\right)\\,dt + \\sqrt{k + \\gamma X(t)}\\,dW_{t},\n$$\nwhere $k>0$ is the birth rate constant, $\\gamma>0$ is the death rate constant, and $W_{t}$ is the SWP. The diffusion term $\\sqrt{k + \\gamma X(t)}$ reflects the shot noise arising from the sum of the birth and death reaction propensities, consistent with the diffusion approximation for reaction networks.\n\nStarting from the definition of variance $\\mathrm{Var}(X(t)) = \\mathbb{E}[X(t)^{2}] - \\left(\\mathbb{E}[X(t)]\\right)^{2}$ and the Itô calculus for a twice-differentiable test function $f(X)$ applied to the CLE, derive the coupled ordinary differential equations that govern the time evolution of the first moment $\\mathbb{E}[X(t)]$ and the variance $\\mathrm{Var}(X(t))$. Your derivation must begin from the general form of Itô’s lemma and proceed by selecting appropriate functions $f(X)$, without invoking any precomputed moment equations or closure approximations.\n\nExpress your final answer as a single row matrix whose first entry is the expression for $\\frac{d}{dt}\\mathbb{E}[X(t)]$ and whose second entry is the expression for $\\frac{d}{dt}\\mathrm{Var}(X(t))$. No numerical evaluation is required. No units are required. Do not include any equality signs or additional commentary in the final answer.",
            "solution": "The problem requires the derivation of the coupled ordinary differential equations (ODEs) for the time evolution of the first moment, $\\mathbb{E}[X(t)]$, and the variance, $\\mathrm{Var}(X(t))$, of a molecular count variable $X(t)$. The dynamics of $X(t)$ are governed by the Chemical Langevin Equation (CLE), which is a specific type of Itô stochastic differential equation (SDE).\n\nThe given SDE is:\n$$\ndX(t) = \\left(k - \\gamma X(t)\\right)\\,dt + \\sqrt{k + \\gamma X(t)}\\,dW_{t}\n$$\nThis SDE is of the general form $dX(t) = a(X,t)dt + b(X,t)dW_t$, where the drift coefficient is $a(X,t) = k - \\gamma X(t)$ and the diffusion coefficient is $b(X,t) = \\sqrt{k + \\gamma X(t)}$. Note that $b(X,t)^2 = k + \\gamma X(t)$. The constants $k$ and $\\gamma$ are positive.\n\nThe derivation relies on Itô's lemma. For a twice-differentiable test function $f(X)$ that does not have explicit time dependence, Itô's lemma states that the differential $df(X(t))$ is given by:\n$$\ndf(X(t)) = \\left( a(X,t) \\frac{df}{dX} + \\frac{1}{2} b(X,t)^2 \\frac{d^2f}{dX^2} \\right) dt + b(X,t) \\frac{df}{dX} dW_t\n$$\nTo find the ODEs for the moments, we will apply this lemma to specific choices of $f(X)$, and then take the expectation. A key property of the Itô integral is that for a suitable non-anticipating process $\\phi(t)$, the expectation $\\mathbb{E}\\left[\\int_0^t \\phi(s) dW_s\\right] = 0$.\n\n**1. Derivation of the ODE for the First Moment $\\mathbb{E}[X(t)]$**\n\nLet the test function be $f(X) = X$. The derivatives are:\n$$\n\\frac{df}{dX} = 1, \\quad \\frac{d^2f}{dX^2} = 0\n$$\nSubstituting these and the expressions for $a(X,t)$ and $b(X,t)^2$ into Itô's lemma yields:\n$$\ndX(t) = \\left( (k - \\gamma X(t)) \\cdot 1 + \\frac{1}{2} (k + \\gamma X(t)) \\cdot 0 \\right) dt + \\sqrt{k + \\gamma X(t)} \\cdot 1 \\cdot dW_t\n$$\n$$\ndX(t) = (k - \\gamma X(t)) dt + \\sqrt{k + \\gamma X(t)} dW_t\n$$\nThis recovers the original SDE. To obtain the dynamics of the mean, we take the expectation of this equation. In its integral form from $t=0$ to $t$:\n$$\n\\mathbb{E}[X(t) - X(0)] = \\mathbb{E}\\left[ \\int_0^t (k - \\gamma X(s)) ds \\right] + \\mathbb{E}\\left[ \\int_0^t \\sqrt{k + \\gamma X(s)} dW_s \\right]\n$$\nUsing the linearity of expectation and the zero-mean property of the Itô integral, we get:\n$$\n\\mathbb{E}[X(t)] - \\mathbb{E}[X(0)] = \\int_0^t \\mathbb{E}[k - \\gamma X(s)] ds = \\int_0^t (k - \\gamma \\mathbb{E}[X(s)]) ds\n$$\nDifferentiating both sides with respect to $t$ using the Fundamental Theorem of Calculus gives the ODE for the mean:\n$$\n\\frac{d}{dt}\\mathbb{E}[X(t)] = k - \\gamma \\mathbb{E}[X(t)]\n$$\n\n**2. Derivation of the ODE for the Variance $\\mathrm{Var}(X(t))$**\n\nThe variance is defined as $\\mathrm{Var}(X(t)) = \\mathbb{E}[X(t)^2] - (\\mathbb{E}[X(t)])^2$. To find its time evolution, we first need the ODE for the second moment, $\\mathbb{E}[X(t)^2]$.\n\nLet the test function be $f(X) = X^2$. The derivatives are:\n$$\n\\frac{df}{dX} = 2X, \\quad \\frac{d^2f}{dX^2} = 2\n$$\nSubstituting into Itô's lemma:\n$$\nd(X(t)^2) = \\left( (k - \\gamma X(t)) \\cdot (2X(t)) + \\frac{1}{2} (k + \\gamma X(t)) \\cdot 2 \\right) dt + \\sqrt{k + \\gamma X(t)} \\cdot (2X(t)) \\cdot dW_t\n$$\n$$\nd(X(t)^2) = (2kX(t) - 2\\gamma X(t)^2 + k + \\gamma X(t)) dt + 2X(t)\\sqrt{k + \\gamma X(t)} dW_t\n$$\nTaking the expectation of the integral form:\n$$\n\\mathbb{E}[X(t)^2 - X(0)^2] = \\mathbb{E}\\left[ \\int_0^t (2kX(s) - 2\\gamma X(s)^2 + k + \\gamma X(s)) ds \\right] + \\mathbb{E}\\left[ \\int_0^t 2X(s)\\sqrt{k + \\gamma X(s)} dW_s \\right]\n$$\nThe expectation of the stochastic integral is zero. Applying linearity of expectation:\n$$\n\\mathbb{E}[X(t)^2] - \\mathbb{E}[X(0)^2] = \\int_0^t ( (2k+\\gamma)\\mathbb{E}[X(s)] - 2\\gamma \\mathbb{E}[X(s)^2] + k ) ds\n$$\nDifferentiating with respect to $t$ gives the ODE for the second moment:\n$$\n\\frac{d}{dt}\\mathbb{E}[X(t)^2] = k + (2k+\\gamma)\\mathbb{E}[X(t)] - 2\\gamma \\mathbb{E}[X(t)^2]\n$$\nNow we can find the derivative of the variance, $\\mathrm{Var}(X(t))$, using the chain rule:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = \\frac{d}{dt}\\left( \\mathbb{E}[X(t)^2] - (\\mathbb{E}[X(t)])^2 \\right) = \\frac{d}{dt}\\mathbb{E}[X(t)^2] - 2\\mathbb{E}[X(t)] \\frac{d}{dt}\\mathbb{E}[X(t)]\n$$\nSubstitute the derived expressions for the derivatives of the first and second moments:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = \\left( k + (2k+\\gamma)\\mathbb{E}[X(t)] - 2\\gamma \\mathbb{E}[X(t)^2] \\right) - 2\\mathbb{E}[X(t)] \\left( k - \\gamma \\mathbb{E}[X(t)] \\right)\n$$\nExpand the terms:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = k + 2k\\mathbb{E}[X(t)] + \\gamma\\mathbb{E}[X(t)] - 2\\gamma \\mathbb{E}[X(t)^2] - 2k\\mathbb{E}[X(t)] + 2\\gamma (\\mathbb{E}[X(t)])^2\n$$\nThe terms $2k\\mathbb{E}[X(t)]$ cancel, leaving:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = k + \\gamma\\mathbb{E}[X(t)] - 2\\gamma \\mathbb{E}[X(t)^2] + 2\\gamma (\\mathbb{E}[X(t)])^2\n$$\nFactor out $-2\\gamma$ from the last two terms:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = k + \\gamma\\mathbb{E}[X(t)] - 2\\gamma \\left( \\mathbb{E}[X(t)^2] - (\\mathbb{E}[X(t)])^2 \\right)\n$$\nRecognizing that the term in the parentheses is the definition of variance, $\\mathrm{Var}(X(t))$, we arrive at the final ODE for the variance:\n$$\n\\frac{d}{dt}\\mathrm{Var}(X(t)) = k + \\gamma\\mathbb{E}[X(t)] - 2\\gamma \\mathrm{Var}(X(t))\n$$\nThe two resulting ODEs form a closed, coupled linear system for the mean and variance. The requested expressions for the derivatives are:\nFor the first moment: $k - \\gamma \\mathbb{E}[X(t)]$\nFor the variance: $k + \\gamma\\mathbb{E}[X(t)] - 2\\gamma \\mathrm{Var}(X(t))$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} k - \\gamma \\mathbb{E}[X(t)] & k + \\gamma \\mathbb{E}[X(t)] - 2\\gamma \\mathrm{Var}(X(t)) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Noise in biological systems is not only generated by processes like transcription and translation but is also propagated and transformed through events like cell division. This exercise shifts focus to the partitioning of molecules between daughter cells, a critical source of variability in cell populations. Using the powerful laws of total expectation and total variance, you will learn to dissect the total noise in a daughter cell's molecule count into its distinct sources. This practice provides a clear, quantitative demonstration of how \"extrinsic\" noise (inherited from the mother) combines with \"intrinsic\" noise (arising from the partitioning process itself). ",
            "id": "3932643",
            "problem": "A mother cell contains a random number of a certain protein molecules immediately prior to cytokinesis. Let the pre-division molecule count be the random variable $N$ with mean $\\mu = \\mathbb{E}[N]$ and variance $\\sigma_{N}^{2} = \\operatorname{Var}(N)$. During division, each molecule independently chooses daughter A with probability $p$ and daughter B with probability $1-p$. Let $X_{A}$ denote the molecule count in daughter A after division.\n\nStarting from the definitions of conditional expectation and the law of total expectation, and from the law of total variance, derive an analytic expression for the mean $\\mathbb{E}[X_{A}]$ and variance $\\operatorname{Var}(X_{A})$ in terms of $p$, $\\mu$, and $\\sigma_{N}^{2}$. Then, specialize to symmetric partitioning where $p = \\tfrac{1}{2}$ to obtain the corresponding mean and variance for daughter A under symmetry. Finally, quantify the comparison by deriving the ratio $R(p)$ of the asymmetric variance to the symmetric variance, that is $R(p) = \\operatorname{Var}(X_{A}\\,|\\,p)/\\operatorname{Var}(X_{A}\\,|\\,p=\\tfrac{1}{2})$.\n\nExpress the final answer in terms of $p$, $\\mu$, and $\\sigma_{N}^{2}$. There is no rounding required. Counts are dimensionless; do not include any units.\n\nProvide your final answer as a single row matrix whose entries, in order, are $\\mathbb{E}[X_{A}]$, $\\operatorname{Var}(X_{A})$, $\\operatorname{Var}(X_{A}\\,|\\,p=\\tfrac{1}{2})$, and $R(p)$.",
            "solution": "We model partitioning as independent Bernoulli trials at the level of individual molecules. Conditional on the pre-division count $N$, the number of molecules going to daughter A, $X_{A}$, follows a binomial distribution with parameters $N$ and $p$. Formally, $X_{A}\\,|\\,N \\sim \\operatorname{Binomial}(N,p)$. The conditional mean and variance of the binomial distribution are\n$$\n\\mathbb{E}[X_{A}\\,|\\,N] = pN, \\qquad \\operatorname{Var}(X_{A}\\,|\\,N) = p(1-p)N.\n$$\nTo obtain unconditional moments with respect to the randomness in $N$, we use the law of total expectation and the law of total variance. The law of total expectation states\n$$\n\\mathbb{E}[X_{A}] = \\mathbb{E}\\!\\left[\\mathbb{E}[X_{A}\\,|\\,N]\\right].\n$$\nSubstituting the conditional mean yields\n$$\n\\mathbb{E}[X_{A}] = \\mathbb{E}[pN] = p\\,\\mathbb{E}[N] = p\\mu.\n$$\nThe law of total variance states\n$$\n\\operatorname{Var}(X_{A}) = \\mathbb{E}\\!\\left[\\operatorname{Var}(X_{A}\\,|\\,N)\\right] + \\operatorname{Var}\\!\\left(\\mathbb{E}[X_{A}\\,|\\,N]\\right).\n$$\nSubstituting the conditional variance and mean, we obtain\n$$\n\\operatorname{Var}(X_{A}) = \\mathbb{E}[p(1-p)N] + \\operatorname{Var}(pN) = p(1-p)\\,\\mathbb{E}[N] + p^{2}\\operatorname{Var}(N).\n$$\nUsing $\\mathbb{E}[N] = \\mu$ and $\\operatorname{Var}(N) = \\sigma_{N}^{2}$ gives the compact expression\n$$\n\\operatorname{Var}(X_{A}) = p(1-p)\\mu + p^{2}\\sigma_{N}^{2}.\n$$\nThese results decompose the variability in $X_{A}$ into two components: an \"intrinsic\" partitioning noise, $p(1-p)\\mu$, which arises from binomial sampling of the molecules given $N$, and an \"extrinsic\" noise, $p^{2}\\sigma_{N}^{2}$, which reflects fluctuations in the mother cell's inventory $N$ prior to division.\n\nFor symmetric partitioning, set $p = \\tfrac{1}{2}$. The mean becomes\n$$\n\\mathbb{E}[X_{A}\\,|\\,p=\\tfrac{1}{2}] = \\tfrac{1}{2}\\mu,\n$$\nand the variance becomes\n$$\n\\operatorname{Var}(X_{A}\\,|\\,p=\\tfrac{1}{2}) = \\left(\\tfrac{1}{2}\\right)\\left(1 - \\tfrac{1}{2}\\right)\\mu + \\left(\\tfrac{1}{2}\\right)^{2}\\sigma_{N}^{2} = \\tfrac{1}{4}\\mu + \\tfrac{1}{4}\\sigma_{N}^{2}.\n$$\nTo effect a quantitative comparison, define the variance ratio\n$$\nR(p) = \\frac{\\operatorname{Var}(X_{A}\\,|\\,p)}{\\operatorname{Var}(X_{A}\\,|\\,p=\\tfrac{1}{2})} = \\frac{p(1-p)\\mu + p^{2}\\sigma_{N}^{2}}{\\tfrac{1}{4}\\mu + \\tfrac{1}{4}\\sigma_{N}^{2}}.\n$$\nCollecting the required entries in the specified order yields the final expressions:\n- $\\mathbb{E}[X_{A}] = p\\mu$,\n- $\\operatorname{Var}(X_{A}) = p(1-p)\\mu + p^{2}\\sigma_{N}^{2}$,\n- $\\operatorname{Var}(X_{A}\\,|\\,p=\\tfrac{1}{2}) = \\tfrac{1}{4}\\mu + \\tfrac{1}{4}\\sigma_{N}^{2}$,\n- $R(p) = \\dfrac{p(1-p)\\mu + p^{2}\\sigma_{N}^{2}}{\\tfrac{1}{4}\\mu + \\tfrac{1}{4}\\sigma_{N}^{2}}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} p\\mu & p(1-p)\\mu + p^{2}\\sigma_{N}^{2} & \\tfrac{1}{4}\\mu + \\tfrac{1}{4}\\sigma_{N}^{2} & \\dfrac{p(1-p)\\mu + p^{2}\\sigma_{N}^{2}}{\\tfrac{1}{4}\\mu + \\tfrac{1}{4}\\sigma_{N}^{2}} \\end{pmatrix}}$$"
        }
    ]
}