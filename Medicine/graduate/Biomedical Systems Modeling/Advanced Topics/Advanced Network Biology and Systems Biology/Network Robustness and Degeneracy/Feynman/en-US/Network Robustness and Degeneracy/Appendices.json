{
    "hands_on_practices": [
        {
            "introduction": "A fundamental aspect of robustness in any biomedical system is its resilience to inherent, small-scale fluctuations in its underlying parameters. This first practice provides a foundational exercise in quantifying this type of robustness through sensitivity analysis. By deriving and applying the principles of linear error propagation, you will learn how to predict the variance in a system's output based on the known uncertainty in its parameters, a crucial skill for evaluating the stability of models in the face of biological noise .",
            "id": "3909340",
            "problem": "Consider a differentiable mapping from a parameter vector $p \\in \\mathbb{R}^{3}$ to an output vector $y \\in \\mathbb{R}^{2}$, denoted $y = f(p)$, representing the normalized steady-state outputs of a two-readout biomedical signaling module. Let $p^{\\star}$ be a nominal parameter operating point obtained from calibration experiments, and let small parameter fluctuations be modeled as a zero-mean Multivariate Normal distribution (MVN) $p = p^{\\star} + \\delta p$ with covariance matrix $\\Sigma_{p} \\in \\mathbb{R}^{3 \\times 3}$. Assume that the mapping $f$ is sufficiently smooth for a first-order Taylor linearization to be valid in a neighborhood of $p^{\\star}$, and denote the Jacobian at $p^{\\star}$ by $J = \\left.\\frac{\\partial f}{\\partial p}\\right|_{p^{\\star}} \\in \\mathbb{R}^{2 \\times 3}$.\n\nStarting from first principles (Taylor linearization and the definition of covariance), derive an expression for the output covariance matrix $\\Sigma_{y}$ in terms of $J$ and $\\Sigma_{p}$. Then, for the specific case\n$$\nJ = \\begin{pmatrix}\n0.5 & -0.3 & 0.0 \\\\\n0.1 & 0.2 & 0.4\n\\end{pmatrix},\n\\qquad\n\\Sigma_{p} = \\begin{pmatrix}\n0.04 & 0.012 & 0.0 \\\\\n0.012 & 0.09 & -0.006 \\\\\n0.0 & -0.006 & 0.01\n\\end{pmatrix},\n$$\ncompute the output covariance matrix $\\Sigma_{y}$ and use it to quantify robustness to parameter noise via the scalar metric $R = \\operatorname{tr}(\\Sigma_{y})$, the expected squared magnitude of output deviations under the linearized model.\n\nReport the numerical value of $R$ rounded to four significant figures. Because the outputs are normalized, treat $R$ as dimensionless and do not attach units. Your final answer must be a single real number.",
            "solution": "The problem statement is evaluated and found to be valid. It is scientifically grounded in the principles of sensitivity analysis and error propagation for nonlinear systems. The problem is well-posed, providing all necessary data and definitions for a unique solution. The given numerical values are consistent and plausible, as the provided covariance matrix $\\Sigma_p$ is symmetric and positive definite. The problem is objective and free of ambiguity.\n\nThe solution proceeds in two parts. First, we derive the general expression for the output covariance matrix $\\Sigma_y$ under a linear approximation. Second, we apply this formula to the specific matrices provided to compute the robustness metric $R$.\n\n### Part 1: Derivation of the Output Covariance Matrix\n\nWe are given a differentiable mapping $y = f(p)$, where $p \\in \\mathbb{R}^{3}$ is a parameter vector and $y \\in \\mathbb{R}^{2}$ is an output vector. The parameters fluctuate around a nominal operating point $p^{\\star}$ according to $p = p^{\\star} + \\delta p$. The fluctuations $\\delta p$ are modeled as a zero-mean Multivariate Normal distribution with covariance matrix $\\Sigma_{p}$. Thus, we have the statistical properties:\n$$E[\\delta p] = 0$$\n$$\\text{Cov}(\\delta p) = E[(\\delta p - E[\\delta p])(\\delta p - E[\\delta p])^{T}] = E[\\delta p (\\delta p)^{T}] = \\Sigma_{p}$$\n\nThe output $y$ will fluctuate around its nominal value $y^{\\star} = f(p^{\\star})$. We can approximate the output fluctuations $\\delta y = y - y^{\\star}$ using a first-order Taylor series expansion of $f(p)$ around $p^{\\star}$:\n$$y = f(p) = f(p^{\\star} + \\delta p) \\approx f(p^{\\star}) + \\left.\\frac{\\partial f}{\\partial p}\\right|_{p^{\\star}} \\delta p$$\nLet $J = \\left.\\frac{\\partial f}{\\partial p}\\right|_{p^{\\star}}$ be the Jacobian matrix of the mapping evaluated at the nominal point. The approximation becomes:\n$$y \\approx y^{\\star} + J \\delta p$$\nThis implies that the output deviation from the nominal value is approximately linear in the parameter deviation:\n$$\\delta y = y - y^{\\star} \\approx J \\delta p$$\nWe now derive the covariance matrix of the output vector $y$, denoted $\\Sigma_{y} = \\text{Cov}(y)$. The expected value of $y$ is:\n$$E[y] \\approx E[y^{\\star} + J \\delta p] = y^{\\star} + J E[\\delta p] = y^{\\star} + J \\cdot 0 = y^{\\star}$$\nUsing this, the covariance matrix $\\Sigma_{y}$ can be calculated as:\n$$\\Sigma_{y} = E[(y - E[y])(y - E[y])^{T}]$$\nSubstituting the approximations for $E[y]$ and the deviation $y - E[y] \\approx J \\delta p$:\n$$\\Sigma_{y} \\approx E[(J \\delta p)(J \\delta p)^{T}]$$\nUsing the transpose property $(AB)^{T} = B^{T}A^{T}$, we have:\n$$\\Sigma_{y} \\approx E[J (\\delta p) (\\delta p)^{T} J^{T}]$$\nSince the Jacobian matrix $J$ is a constant matrix (evaluated at the fixed point $p^{\\star}$), it can be factored out of the expectation:\n$$\\Sigma_{y} \\approx J E[\\delta p (\\delta p)^{T}] J^{T}$$\nWe recognize the term inside the expectation as the covariance matrix of the parameters, $\\Sigma_{p} = E[\\delta p (\\delta p)^{T}]$.\nTherefore, the expression for the output covariance matrix under this linear approximation is:\n$$\\Sigma_{y} = J \\Sigma_{p} J^{T}$$\n\n### Part 2: Calculation of the Robustness Metric $R$\n\nWe are given the specific matrices for the Jacobian and the parameter covariance:\n$$J = \\begin{pmatrix} 0.5 & -0.3 & 0.0 \\\\ 0.1 & 0.2 & 0.4 \\end{pmatrix}, \\qquad \\Sigma_{p} = \\begin{pmatrix} 0.04 & 0.012 & 0.0 \\\\ 0.012 & 0.09 & -0.006 \\\\ 0.0 & -0.006 & 0.01 \\end{pmatrix}$$\nThe transpose of the Jacobian matrix $J$ is:\n$$J^{T} = \\begin{pmatrix} 0.5 & 0.1 \\\\ -0.3 & 0.2 \\\\ 0.0 & 0.4 \\end{pmatrix}$$\nFirst, we compute the matrix product $J \\Sigma_{p}$:\n$$J \\Sigma_{p} = \\begin{pmatrix} 0.5 & -0.3 & 0.0 \\\\ 0.1 & 0.2 & 0.4 \\end{pmatrix} \\begin{pmatrix} 0.04 & 0.012 & 0.0 \\\\ 0.012 & 0.09 & -0.006 \\\\ 0.0 & -0.006 & 0.01 \\end{pmatrix}$$\n$$J \\Sigma_{p} = \\begin{pmatrix} (0.5)(0.04) + (-0.3)(0.012) + (0.0)(0.0) & (0.5)(0.012) + (-0.3)(0.09) + (0.0)(-0.006) & (0.5)(0.0) + (-0.3)(-0.006) + (0.0)(0.01) \\\\ (0.1)(0.04) + (0.2)(0.012) + (0.4)(0.0) & (0.1)(0.012) + (0.2)(0.09) + (0.4)(-0.006) & (0.1)(0.0) + (0.2)(-0.006) + (0.4)(0.01) \\end{pmatrix}$$\n$$J \\Sigma_{p} = \\begin{pmatrix} 0.02 - 0.0036 & 0.006 - 0.027 & 0.0018 \\\\ 0.004 + 0.0024 & 0.0012 + 0.018 - 0.0024 & -0.0012 + 0.004 \\end{pmatrix}$$\n$$J \\Sigma_{p} = \\begin{pmatrix} 0.0164 & -0.021 & 0.0018 \\\\ 0.0064 & 0.0168 & 0.0028 \\end{pmatrix}$$\nNext, we compute $\\Sigma_{y} = (J \\Sigma_{p}) J^{T}$:\n$$\\Sigma_{y} = \\begin{pmatrix} 0.0164 & -0.021 & 0.0018 \\\\ 0.0064 & 0.0168 & 0.0028 \\end{pmatrix} \\begin{pmatrix} 0.5 & 0.1 \\\\ -0.3 & 0.2 \\\\ 0.0 & 0.4 \\end{pmatrix}$$\n$$\\Sigma_{y} = \\begin{pmatrix} (0.0164)(0.5) + (-0.021)(-0.3) + (0.0018)(0.0) & (0.0164)(0.1) + (-0.021)(0.2) + (0.0018)(0.4) \\\\ (0.0064)(0.5) + (0.0168)(-0.3) + (0.0028)(0.0) & (0.0064)(0.1) + (0.0168)(0.2) + (0.0028)(0.4) \\end{pmatrix}$$\n$$\\Sigma_{y} = \\begin{pmatrix} 0.0082 + 0.0063 & 0.00164 - 0.0042 + 0.00072 \\\\ 0.0032 - 0.00504 & 0.00064 + 0.00336 + 0.00112 \\end{pmatrix}$$\n$$\\Sigma_{y} = \\begin{pmatrix} 0.0145 & -0.00184 \\\\ -0.00184 & 0.00512 \\end{pmatrix}$$\nThe robustness metric $R$ is defined as the trace of the output covariance matrix, $R = \\operatorname{tr}(\\Sigma_{y})$. The trace is the sum of the diagonal elements.\n$$R = 0.0145 + 0.00512 = 0.01962$$\nThe problem requires the answer to be rounded to four significant figures. The number $0.01962$ has four significant figures ($1$, $9$, $6$, $2$). The next digit is an implicit zero. Thus, no rounding is necessary.\nThe value of the robustness metric is $0.01962$. This metric represents the expected squared magnitude of output deviations, $E[||\\delta y||^2]$, and serves as a measure of total output variance. A smaller value of $R$ implies greater robustness to parameter noise.",
            "answer": "$$\\boxed{0.01962}$$"
        },
        {
            "introduction": "Moving beyond passive resilience to random noise, this exercise challenges you to actively probe a network's vulnerabilities. Here, we adopt an adversarial mindset to find the \"worst-case\" small perturbation that maximally disrupts the system's output, a concept critical for understanding fragility and designing robust therapies. You will implement a projected gradient ascent algorithm to identify a network's specific weak points, demonstrating how optimization techniques can be used to uncover the most effective ways to perturb a biological system .",
            "id": "3909253",
            "problem": "Consider a differentiable, redundant pathway model of a biochemical regulatory network that maps an input vector to an aggregate output via parallel, saturating modules. The objective is to evaluate network robustness to adversarial perturbations by computing the worst-case output deviation under a bounded perturbation budget using Projected Gradient Ascent (PGA). Start from the following base definitions: the differentiable composition of functions, the gradient as the direction of steepest ascent, and the chain rule for derivatives.\n\nLet the network input be a baseline vector $x_0 \\in \\mathbb{R}^n$ with $n = 4$ and $x_0 = (0.2,\\,0.5,\\,0.7,\\,0.3)$. The network comprises $M$ parallel, redundant modules indexed by $i \\in \\{1,\\,2,\\,\\dots,\\,M\\}$, each computing a saturating response. For a given $M$, define for each module:\n- Weight vector $a_i \\in \\mathbb{R}^4$ by the rule $v_i = \\left(\\cos(i),\\,\\sin(i),\\,\\cos(2i),\\,\\sin(2i)\\right)$ (angles in radians), and $a_i = \\dfrac{v_i}{\\lVert v_i \\rVert_2}$.\n- Bias $b_i = 0.1 \\cdot \\dfrac{i}{M}$.\n- Output weight $c_i = \\dfrac{1}{M}$.\n- Steepness parameter $\\beta > 0$ (specified per test case).\n\nDefine the module’s response function $\\sigma(u)$ (logistic) as $\\sigma(u) = \\dfrac{1}{1 + e^{-u}}$. The network output is given by\n$$\ny(x) = \\sum_{i=1}^{M} c_i \\, \\sigma\\!\\left(\\beta \\left(a_i^\\top x - b_i\\right)\\right).\n$$\nFor robustness evaluation, constrain perturbations $\\delta \\in \\mathbb{R}^4$ by the Euclidean norm bound $\\lVert \\delta \\rVert_2 \\le \\varepsilon$ and the box constraint $x_0 + \\delta \\in [0,\\,1]^4$ (componentwise). The worst-case output deviation is defined as\n$$\nD^\\star = \\max_{\\substack{\\lVert \\delta \\rVert_2 \\le \\varepsilon \\\\ x_0 + \\delta \\in [0,\\,1]^4}} \\left| y(x_0 + \\delta) - y(x_0) \\right|.\n$$\nApproximate $D^\\star$ using Projected Gradient Ascent (PGA) to maximize the squared deviation objective\n$$\nJ(\\delta) = \\left(y(x_0 + \\delta) - y(x_0)\\right)^2,\n$$\nwith updates\n$$\n\\delta \\leftarrow \\Pi_{\\mathcal{B}_\\varepsilon}\\!\\left(\\operatorname{clip}\\!\\left(x_0 + \\delta + \\alpha \\nabla_\\delta J(\\delta),\\,0,\\,1\\right) - x_0\\right),\n$$\nwhere $\\alpha$ is the step size, $\\Pi_{\\mathcal{B}_\\varepsilon}$ is the projection onto the $\\ell_2$ ball of radius $\\varepsilon$, and $\\operatorname{clip}(\\cdot,\\,0,\\,1)$ enforces the box constraint componentwise.\n\nUse the chain rule to express the gradient:\n$$\n\\nabla_x y(x) = \\sum_{i=1}^{M} c_i \\, \\beta \\, \\sigma\\!\\left(u_i\\right)\\left(1 - \\sigma\\!\\left(u_i\\right)\\right) a_i,\\quad u_i = \\beta \\left(a_i^\\top x - b_i\\right),\n$$\nand\n$$\n\\nabla_\\delta J(\\delta) = 2\\left(y(x_0 + \\delta) - y(x_0)\\right)\\nabla_x y(x_0 + \\delta).\n$$\nAfter $T$ iterations, report $D = \\left|y(x_0 + \\delta_T) - y(x_0)\\right|$ as the approximation of $D^\\star$.\n\nImplement a program that, for each test case defined below, runs PGA from $\\delta_0 = (0,\\,0,\\,0,\\,0)$ for the specified number of iterations $T$ and returns the approximated worst-case deviation $D$ rounded to $6$ decimal places. No physical units are involved; all quantities are dimensionless.\n\nTest suite:\n- Case $1$: $M = 1$, $\\varepsilon = 0.25$, $\\beta = 4.0$, $T = 200$, $\\alpha = 0.1$.\n- Case $2$: $M = 3$, $\\varepsilon = 0.25$, $\\beta = 4.0$, $T = 200$, $\\alpha = 0.1$.\n- Case $3$: $M = 10$, $\\varepsilon = 0.25$, $\\beta = 4.0$, $T = 200$, $\\alpha = 0.1$.\n- Case $4$: $M = 3$, $\\varepsilon = 0.0$, $\\beta = 4.0$, $T = 50$, $\\alpha = 0.1$.\n- Case $5$: $M = 3$, $\\varepsilon = 0.5$, $\\beta = 8.0$, $T = 300$, $\\alpha = 0.05$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1, r_2, r_3, r_4, r_5]$), where each $r_k$ is the rounded float for Case $k$ in the order given above.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of systems biology and network robustness, using a standard mathematical model of a redundant signaling pathway. The objective to find the worst-case output deviation under bounded perturbations is a well-defined problem in robustness analysis. The methodology proposed, Projected Gradient Ascent (PGA), is a standard and appropriate algorithm for this constrained optimization task. All parameters, functions, and constraints are specified completely and consistently, making the problem well-posed and algorithmically solvable. No scientific flaws, contradictions, or ambiguities are present.\n\nThe solution proceeds by implementing the specified Projected Gradient Ascent algorithm.\n\n**1. Model Definition**\nThe core of the model is the network output function $y(x)$, which aggregates the responses of $M$ parallel modules.\nThe input is a vector $x \\in \\mathbb{R}^4$. The baseline is $x_0 = (0.2,\\,0.5,\\,0.7,\\,0.3)$.\nFor each module $i \\in \\{1, \\dots, M\\}$:\n- The weight vector $a_i \\in \\mathbb{R}^4$ is a normalized vector derived from $v_i = (\\cos(i), \\sin(i), \\cos(2i), \\sin(2i))$.\n- The bias is $b_i = 0.1 \\cdot \\frac{i}{M}$.\n- The output weight is $c_i = \\frac{1}{M}$.\n\nThe output of each module is a logistic function $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ applied to a linear combination of inputs:\n$$u_i = \\beta (a_i^\\top x - b_i)$$\nThe total network output is the weighted sum of module outputs:\n$$y(x) = \\sum_{i=1}^{M} c_i \\, \\sigma(u_i)$$\n\n**2. Optimization Problem**\nWe aim to find the perturbation $\\delta \\in \\mathbb{R}^4$ that maximizes the output deviation, subject to two constraints:\n1.  **Box constraint**: The perturbed input $x = x_0 + \\delta$ must lie within the hypercube $[0, 1]^4$. This means $0 \\le (x_0)_j + \\delta_j \\le 1$ for each component $j$.\n2.  **Norm constraint**: The magnitude of the perturbation is bounded by the Euclidean norm: $\\|\\delta\\|_2 \\le \\varepsilon$.\n\nThe optimization problem is to find an approximate solution to:\n$$ D^\\star = \\max_{\\substack{\\lVert \\delta \\rVert_2 \\le \\varepsilon \\\\ x_0 + \\delta \\in [0,\\,1]^4}} \\left| y(x_0 + \\delta) - y(x_0) \\right| $$\nTo simplify the optimization, we maximize the squared deviation, which is equivalent since the square function is monotonic for non-negative values. The objective function is:\n$$ J(\\delta) = \\left(y(x_0 + \\delta) - y(x_0)\\right)^2 $$\n\n**3. Projected Gradient Ascent (PGA)**\nPGA is an iterative method to solve constrained maximization problems. Starting with an initial guess $\\delta_0 = \\mathbf{0}$, each iteration updates the current estimate $\\delta$ by taking a small step in the direction of the gradient of the objective function, $\\nabla_\\delta J(\\delta)$, and then projecting the result back onto the feasible set defined by the constraints.\n\n**3.1. Gradient Calculation**\nUsing the chain rule, the gradient of the objective function $J(\\delta)$ with respect to $\\delta$ is:\n$$ \\nabla_\\delta J(\\delta) = \\frac{\\partial J}{\\partial y} \\cdot \\nabla_\\delta y(x_0+\\delta) $$\nWe have $\\frac{\\partial J}{\\partial y} = 2(y(x_0 + \\delta) - y(x_0))$.\nAnd $\\nabla_\\delta y(x_0+\\delta) = \\nabla_x y(x)|_{x=x_0+\\delta}$.\nThe gradient of the network output $y(x)$ is:\n$$ \\nabla_x y(x) = \\sum_{i=1}^{M} \\frac{\\partial}{\\partial x} \\left[c_i \\, \\sigma\\!\\left(u_i(x)\\right)\\right] = \\sum_{i=1}^{M} c_i \\, \\frac{d\\sigma}{du_i} \\frac{\\partial u_i}{\\partial x} $$\nSince $\\frac{d\\sigma(u)}{du} = \\sigma(u)(1-\\sigma(u))$ and $\\frac{\\partial u_i}{\\partial x} = \\beta a_i$, we get:\n$$ \\nabla_x y(x) = \\sum_{i=1}^{M} c_i \\, \\beta \\, \\sigma(u_i)(1 - \\sigma(u_i)) a_i $$\nCombining these, the full gradient for the PGA update is:\n$$ \\nabla_\\delta J(\\delta) = 2\\left(y(x_0 + \\delta) - y_0\\right) \\left( \\sum_{i=1}^{M} c_i \\, \\beta \\, \\sigma'_i a_i \\right) $$\nwhere $y_0 = y(x_0)$, $u_i$ is evaluated at $x = x_0+\\delta$, and $\\sigma'_i = \\sigma(u_i)(1-\\sigma(u_i))$.\n\n**3.2. Update Rule and Projections**\nThe update rule for $\\delta$ combines the gradient step with projections for the two constraints:\n$$ \\delta_{k+1} = \\Pi_{\\mathcal{B}_\\varepsilon}\\!\\left(\\operatorname{clip}\\!\\left(x_0 + \\delta_k + \\alpha \\nabla_\\delta J(\\delta_k),\\,0,\\,1\\right) - x_0\\right) $$\nThis sequence of operations is decomposed as follows for each iteration $k$:\n1.  **Gradient Ascent Step**: Compute an unconstrained intermediate state in the input space: $x'_{k+1} = (x_0 + \\delta_k) + \\alpha \\nabla_\\delta J(\\delta_k)$.\n2.  **Box Constraint Projection**: Enforce the $[0, 1]^4$ component-wise constraint using the $\\operatorname{clip}$ function: $x''_{k+1} = \\operatorname{clip}(x'_{k+1}, 0, 1)$.\n3.  **Convert to Perturbation**: The result is converted back into a perturbation relative to $x_0$: $\\delta'_{k+1} = x''_{k+1} - x_0$.\n4.  **$\\ell_2$-Norm Constraint Projection**: Project the perturbation onto the Euclidean ball of radius $\\varepsilon$. If $\\|\\delta'_{k+1}\\|_2 > \\varepsilon$, it is scaled down to the boundary of the ball. Otherwise, it remains unchanged. This projection is given by:\n    $$ \\Pi_{\\mathcal{B}_\\varepsilon}(v) = v \\cdot \\min\\left(1, \\frac{\\varepsilon}{\\lVert v \\rVert_2}\\right) $$\n    This yields the final updated perturbation for the next iteration: $\\delta_{k+1} = \\Pi_{\\mathcal{B}_\\varepsilon}(\\delta'_{k+1})$.\n\n**4. Implementation**\nThe algorithm is implemented in Python using the `numpy` library for efficient vector and matrix operations.\n- The network parameters $a_i, b_i, c_i$ are pre-computed for each value of $M$.\n- The network output $y(x)$ and its gradient $\\nabla_x y(x)$ are implemented as vectorized functions for efficiency.\n- A main loop iterates through each test case, initializing $\\delta_0 = \\mathbf{0}$ and performing $T$ iterations of the PGA update.\n- After $T$ iterations, the final deviation $D = |y(x_0+\\delta_T) - y_0|$ is calculated and rounded to $6$ decimal places as required.\n- Case 4, with $\\varepsilon=0$, serves as a sanity check. The only permissible perturbation is $\\delta = \\mathbf{0}$, hence the deviation must be $0$. The algorithm correctly handles this, as the projection $\\Pi_{\\mathcal{B}_0}(\\cdot)$ always returns $\\mathbf{0}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the network robustness problem by implementing Projected Gradient Ascent.\n    \"\"\"\n    \n    # Baseline input vector as specified in the problem\n    X0 = np.array([0.2, 0.5, 0.7, 0.3])\n\n    def sigma(u):\n        \"\"\"Vectorized logistic sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-u))\n\n    def calculate_network_parameters(M):\n        \"\"\"Pre-calculates a, b, c parameter arrays for a given number of modules M.\"\"\"\n        n = 4  # Dimension of the input space\n        a = np.zeros((M, n))\n        b = np.zeros(M)\n        c = np.full(M, 1.0 / M)\n        \n        # Problem uses 1-based indexing for i in formulas\n        i_vals = np.arange(1, M + 1)\n        \n        # Calculate v_i vectors\n        v = np.array([np.cos(i_vals), np.sin(i_vals), np.cos(2 * i_vals), np.sin(2 * i_vals)]).T\n        \n        # Normalize to get a_i vectors\n        norms_v = np.linalg.norm(v, axis=1, keepdims=True)\n        # Avoid division by zero, though not expected for these i_vals\n        a = np.divide(v, norms_v, out=np.zeros_like(v), where=norms_v!=0)\n\n        # Calculate b_i biases\n        b = 0.1 * i_vals / M\n        \n        return a, b, c\n\n    def network_output(x, a, b, c, beta):\n        \"\"\"Computes the vectorized network output y(x).\"\"\"\n        u = beta * (a @ x - b)\n        sigmas = sigma(u)\n        y = c @ sigmas\n        return y\n\n    def network_gradient(x, a, b, c, beta):\n        \"\"\"Computes the vectorized gradient of the network output, grad_x y(x).\"\"\"\n        u = beta * (a @ x - b)\n        sigmas = sigma(u)\n        \n        # scalar_coeffs = c_i * beta * sigma(u_i) * (1 - sigma(u_i))\n        scalar_coeffs = c * beta * sigmas * (1.0 - sigmas)\n        \n        # grad = sum(scalar_coeffs_i * a_i)\n        # This is the dot product of the transposed 'a' matrix with the coefficients vector\n        grad = a.T @ scalar_coeffs\n        return grad\n\n    def project_l2_ball(v, epsilon):\n        \"\"\"Projects vector v onto the L2 ball of radius epsilon.\"\"\"\n        # Handle epsilon=0 case to avoid division by zero if norm > 0\n        if epsilon == 0.0:\n            return np.zeros_like(v)\n        norm = np.linalg.norm(v)\n        if norm > epsilon:\n            return v * epsilon / norm\n        return v\n\n    def run_pga_for_case(M, epsilon, beta, T, alpha):\n        \"\"\"Runs the PGA simulation for one test case.\"\"\"\n        \n        # Step 1: Pre-compute network parameters and baseline output\n        a, b, c = calculate_network_parameters(M)\n        y0 = network_output(X0, a, b, c, beta)\n        \n        # Step 2: Initialize perturbation\n        delta = np.zeros_like(X0)\n        \n        # Step 3: Run Projected Gradient Ascent iterations\n        for _ in range(T):\n            x_current = X0 + delta\n            \n            # Calculate gradient of the objective function J(delta)\n            y_current = network_output(x_current, a, b, c, beta)\n            grad_y_val = network_gradient(x_current, a, b, c, beta)\n            grad_J = 2.0 * (y_current - y0) * grad_y_val\n            \n            # Perform the update according to the specified rule\n            # 1. Take a gradient step in the input space\n            x_unconstrained = x_current + alpha * grad_J\n            \n            # 2. Project onto the box constraint [0, 1]^n\n            x_clipped = np.clip(x_unconstrained, 0.0, 1.0)\n            \n            # 3. Convert the position back to a perturbation\n            delta_clipped = x_clipped - X0\n            \n            # 4. Project the perturbation onto the L2 ball\n            delta = project_l2_ball(delta_clipped, epsilon)\n\n        # Step 4: Calculate final deviation\n        y_final = network_output(X0 + delta, a, b, c, beta)\n        D = np.abs(y_final - y0)\n        \n        return round(D, 6)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (M, epsilon, beta, T, alpha)\n        (1, 0.25, 4.0, 200, 0.1),\n        (3, 0.25, 4.0, 200, 0.1),\n        (10, 0.25, 4.0, 200, 0.1),\n        (3, 0.0, 4.0, 50, 0.1),\n        (3, 0.5, 8.0, 300, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_pga_for_case(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice shifts our focus from small parameter perturbations to large-scale structural damage, simulating the functional consequences of gene knockouts. Using Flux Balance Analysis (FBA), a cornerstone of metabolic engineering, you will assess a network's ability to maintain a critical biological objective—such as biomass production—after entire reactions are deleted. This exercise provides hands-on experience in modeling the functional robustness of metabolic networks, a key factor in understanding disease and identifying synthetic lethal targets .",
            "id": "3909395",
            "problem": "Consider a steady-state metabolic network represented by a stoichiometric matrix $S \\in \\mathbb{R}^{m \\times n}$ with flux vector $v \\in \\mathbb{R}^{n}$, subject to mass balance constraints $S v = 0$ and bound constraints $l \\le v \\le u$, where $l, u \\in \\mathbb{R}^{n}$ are lower and upper bounds for each reaction. Flux Balance Analysis (FBA) is formulated as the linear optimization problem of maximizing a biomass production objective $c^{\\top} v$ subject to $S v = 0$ and $l \\le v \\le u$, where $c \\in \\mathbb{R}^{n}$ selects the biomass-producing reaction(s). Define the robustness to single-reaction knockouts with respect to a minimum biomass flux threshold $f_{\\min} \\in \\mathbb{R}_{\\ge 0}$ as the fraction of reactions in a designated knockout set $\\mathcal{K} \\subseteq \\{0,1,\\dots,n-1\\}$ for which there exists a feasible flux $v$ under the knockout (implemented by fixing the corresponding flux $v_i$ to $0$) that achieves $c^{\\top} v \\ge f_{\\min}$. Formally, for a given test case,\n$$\nR \\equiv \\frac{1}{|\\mathcal{K}|} \\sum_{i \\in \\mathcal{K}} \\mathbf{1}\\left(\\exists v \\text{ such that } S v = 0,\\; l \\le v \\le u,\\; v_i = 0,\\; c^{\\top} v \\ge f_{\\min}\\right),\n$$\nwhere $\\mathbf{1}(\\cdot)$ is $1$ if the condition is true and $0$ otherwise, and $|\\mathcal{K}|$ is the cardinality of $\\mathcal{K}$. All fluxes and thresholds are in arbitrary consistent flux units, so no physical unit conversion is required.\n\nYour task is to write a complete program that computes $R$ for each of the following four toy network test cases. In each case, the program must solve an FBA problem that maximizes $c^{\\top} v$ under the specified constraints for each single-reaction knockout $i \\in \\mathcal{K}$, and then check if the optimal value is at least $f_{\\min}$. Use a numerical tolerance of $10^{-9}$ when comparing $c^{\\top} v$ to $f_{\\min}$ to account for solver precision. The final result for each test case must be returned as a floating-point number in $[0,1]$, equal to the fraction of successful knockouts.\n\nTest Case 1 (Parallel redundant pathways with ample capacities):\n- Internal metabolites are two species, yielding $m = 2$, and there are $n = 4$ reactions with indices $0,1,2,3$. The stoichiometric matrix is\n$$\nS = \\begin{bmatrix}\n1 & -1 & -1 & 0 \\\\\n0 & 0 & 1 & -1\n\\end{bmatrix},\n$$\nwhere columns correspond to reactions $0,1,2,3$ respectively. The bounds are\n$$\nl = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\nu = \\begin{bmatrix} 10 \\\\ 1000 \\\\ 1000 \\\\ 1000 \\end{bmatrix}.\n$$\nThe objective vector is\n$$\nc = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix},\n$$\nso that the biomass objective equals $v_1 + v_3$. The knockout set is\n$$\n\\mathcal{K} = \\{0,1,2,3\\}.\n$$\nThe minimum biomass threshold is\n$$\nf_{\\min} = 5.\n$$\n\nTest Case 2 (Single linear pathway with no redundancy):\n- Internal metabolites are two species, yielding $m = 2$, and there are $n = 3$ reactions with indices $0,1,2$. The stoichiometric matrix is\n$$\nS = \\begin{bmatrix}\n1 & -1 & 0 \\\\\n0 & 1 & -1\n\\end{bmatrix},\n$$\nwith reactions $0,1,2$ respectively. The bounds are\n$$\nl = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\nu = \\begin{bmatrix} 10 \\\\ 1000 \\\\ 1000 \\end{bmatrix}.\n$$\nThe objective vector is\n$$\nc = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix},\n$$\nso that the biomass objective equals $v_2$. The knockout set is\n$$\n\\mathcal{K} = \\{0,1,2\\}.\n$$\nThe minimum biomass threshold is\n$$\nf_{\\min} = 5.\n$$\n\nTest Case 3 (Parallel redundant pathways with threshold equal to the undeleted optimum):\n- Identical to Test Case $1$ in $S$, $l$, $u$, $c$, and $\\mathcal{K}$. The only difference is the minimum biomass threshold\n$$\nf_{\\min} = 10.\n$$\n\nTest Case 4 (Parallel pathways with capacity limits on each branch):\n- Internal metabolites are two species, yielding $m = 2$, and there are $n = 4$ reactions with indices $0,1,2,3$. The stoichiometric matrix is\n$$\nS = \\begin{bmatrix}\n1 & -1 & -1 & 0 \\\\\n0 & 0 & 1 & -1\n\\end{bmatrix}.\n$$\nThe bounds are\n$$\nl = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\nu = \\begin{bmatrix} 10 \\\\ 8 \\\\ 1000 \\\\ 6 \\end{bmatrix},\n$$\nso reaction $1$ has an upper bound of $8$ and reaction $3$ has an upper bound of $6$. The objective vector is\n$$\nc = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix},\n$$\nand the knockout set is\n$$\n\\mathcal{K} = \\{0,1,2,3\\}.\n$$\nThe minimum biomass threshold is\n$$\nf_{\\min} = 9.\n$$\n\nImplement the program to solve each test case exactly as specified. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the above test cases, for example $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the computed robustness value for Test Case $i$ expressed as a decimal in $[0,1]$.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the established principles of Flux Balance Analysis (FBA), a standard method in systems biology for analyzing metabolic networks. The problem is well-posed, providing all necessary mathematical definitions, constraints, and data for four distinct and solvable test cases. The language is objective and precise, and the task is computationally feasible.\n\nThe core of the problem is to compute the robustness, $R$, of a metabolic network to a series of single-reaction knockouts. This metric is defined as the fraction of knockouts in a set $\\mathcal{K}$ that still permit the network to produce a biomass flux greater than or equal to a minimum threshold, $f_{\\min}$. The existence of such a flux distribution is determined by solving an FBA problem for each knockout.\n\nThe fundamental principle is that under the steady-state assumption, the fluxes $v \\in \\mathbb{R}^n$ through the $n$ reactions of a metabolic network with $m$ internal metabolites must satisfy the mass balance equation $S v = 0$, where $S \\in \\mathbb{R}^{m \\times n}$ is the stoichiometric matrix. Additionally, fluxes are constrained by lower and upper bounds, $l \\le v \\le u$. FBA uses linear programming (LP) to find a flux vector $v$ that satisfies these constraints while maximizing a cellular objective function, typically biomass production, represented as $c^{\\top} v$.\n\nThe analytical procedure for each test case is as follows:\n1.  Initialize a counter for successful knockouts, $N_{success}$, to $0$.\n2.  For each reaction index $i$ in the specified knockout set $\\mathcal{K}$:\n    a. Formulate an LP problem to find the maximum possible biomass flux under the knockout condition. A knockout of reaction $i$ is implemented by constraining its flux to zero, i.e., $v_i = 0$.\n    b. The LP problem is thus:\n    $$\n    \\begin{array}{ll}\n    \\underset{v}{\\text{maximize}} & z = c^{\\top} v \\\\\n    \\text{subject to} & S v = 0 \\\\\n    & l_j \\le v_j \\le u_j \\quad \\forall j \\neq i \\\\\n    & v_i = 0\n    \\end{array}\n    $$\n    c. This maximization problem is solved using a standard LP solver. Computationally, this is often posed as a minimization problem. Maximizing $c^{\\top}v$ is equivalent to minimizing $(-c)^{\\top}v$. The constraints are passed to the solver: $S$ as the equality constraint matrix `A_eq`, a zero vector as the equality constraint vector `b_eq`, and the flux bounds $l \\le v \\le u$ as the `bounds` parameter. The knockout constraint $v_i=0$ is efficiently handled by setting the specific bounds for $v_i$ to $(0, 0)$.\n    d. Let the optimal objective value returned by the solver for the minimization be $z^*_{\\min}$. The maximum biomass production is then $z^*_{\\max} = -z^*_{\\min}$.\n    e. This maximum biomass value is compared against the threshold $f_{\\min}$. To account for potential floating-point inaccuracies from the numerical solver, a tolerance $\\epsilon = 10^{-9}$ is used. If the solver finds a feasible solution and $z^*_{\\max} \\ge f_{\\min} - \\epsilon$, the knockout is considered survivable, and the counter $N_{success}$ is incremented.\n3.  After iterating through all knockouts in $\\mathcal{K}$, the robustness $R$ is calculated as the ratio of successful knockouts to the total number of knockouts performed:\n    $$\n    R = \\frac{N_{success}}{|\\mathcal{K}|}\n    $$\nThis procedure is applied to each of the four test cases using their respective parameters ($S$, $l$, $u$, $c$, $\\mathcal{K}$, and $f_{\\min}$) to generate the final results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, compute robustness for each,\n    and print the results in the specified format.\n    \"\"\"\n    # Numerical tolerance for comparing biomass flux to the threshold.\n    TOLERANCE = 1e-9\n\n    # Definition of the four test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Test Case 1\",\n            \"S\": np.array([[1, -1, -1, 0], [0, 0, 1, -1]]),\n            \"l\": np.array([0, 0, 0, 0]),\n            \"u\": np.array([10, 1000, 1000, 1000]),\n            \"c\": np.array([0, 1, 0, 1]),\n            \"K\": {0, 1, 2, 3},\n            \"f_min\": 5.0,\n        },\n        {\n            \"name\": \"Test Case 2\",\n            \"S\": np.array([[1, -1, 0], [0, 1, -1]]),\n            \"l\": np.array([0, 0, 0]),\n            \"u\": np.array([10, 1000, 1000]),\n            \"c\": np.array([0, 0, 1]),\n            \"K\": {0, 1, 2},\n            \"f_min\": 5.0,\n        },\n        {\n            \"name\": \"Test Case 3\",\n            \"S\": np.array([[1, -1, -1, 0], [0, 0, 1, -1]]),\n            \"l\": np.array([0, 0, 0, 0]),\n            \"u\": np.array([10, 1000, 1000, 1000]),\n            \"c\": np.array([0, 1, 0, 1]),\n            \"K\": {0, 1, 2, 3},\n            \"f_min\": 10.0,\n        },\n        {\n            \"name\": \"Test Case 4\",\n            \"S\": np.array([[1, -1, -1, 0], [0, 0, 1, -1]]),\n            \"l\": np.array([0, 0, 0, 0]),\n            \"u\": np.array([10, 8, 1000, 6]),\n            \"c\": np.array([0, 1, 0, 1]),\n            \"K\": {0, 1, 2, 3},\n            \"f_min\": 9.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        robustness = compute_robustness(\n            S=case[\"S\"],\n            l=case[\"l\"],\n            u=case[\"u\"],\n            c=case[\"c\"],\n            K=case[\"K\"],\n            f_min=case[\"f_min\"],\n            tol=TOLERANCE\n        )\n        results.append(robustness)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_robustness(S, l, u, c, K, f_min, tol):\n    \"\"\"\n    Computes the robustness for a single metabolic network configuration.\n\n    Args:\n        S (np.ndarray): Stoichiometric matrix (m x n).\n        l (np.ndarray): Lower bounds for fluxes (n,).\n        u (np.ndarray): Upper bounds for fluxes (n,).\n        c (np.ndarray): Objective vector (n,).\n        K (set): Set of reaction indices to knock out.\n        f_min (float): Minimum biomass flux threshold.\n        tol (float): Numerical tolerance for comparison.\n\n    Returns:\n        float: The computed robustness value R.\n    \"\"\"\n    successful_knockouts = 0\n    num_reactions = S.shape[1]\n    \n    # Objective for minimization: -c to maximize c'v\n    c_solver = -c\n    \n    # Equality constraints: Sv = 0\n    A_eq = S\n    b_eq = np.zeros(S.shape[0])\n\n    for i in K:\n        # Create the bounds for the fluxes for this knockout simulation.\n        # It's a list of (min, max) tuples.\n        bounds = list(zip(l, u))\n        \n        # Apply the knockout by setting the flux bounds to (0, 0)\n        if 0 <= i < num_reactions:\n            bounds[i] = (0, 0)\n        else:\n            # This case should not happen with valid input but is good practice.\n            continue\n            \n        # Solve the linear programming problem\n        res = linprog(c=c_solver, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n\n        # Check if a feasible solution was found that meets the threshold\n        if res.success:\n            # The result from linprog is the minimum. We need the maximum.\n            max_biomass = -res.fun\n            if max_biomass >= f_min - tol:\n                successful_knockouts += 1\n\n    # Calculate robustness R\n    if not K:\n        return 0.0\n    \n    return successful_knockouts / len(K)\n\nsolve()\n```"
        }
    ]
}