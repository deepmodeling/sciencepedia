{
    "hands_on_practices": [
        {
            "introduction": "理论的最佳佐证是实践。本章节提供了一系列动手实践，旨在引导您从变分数据同化的基本原理走向更复杂、更贴近实际的生物医学建模应用。第一个练习是基础中的基础，它在一个简单的标量情境中，揭示了贝叶斯概率与变分优化之间的深刻联系。通过解决这个问题，您将亲手推导出变分方法中无处不在的二次代价函数，从而牢固掌握其理论根源。",
            "id": "3864732",
            "problem": "考虑一个单网格单元的环境状态变量 $x$，它表示遥感和环境建模系统中分析时刻的柱平均示踪剂浓度。您正在使用一次卫星观测执行三维变分（3D-Var）数据同化，这是四维变分（4D-Var）数据同化的时间无关极限。假设观测算子是线性的，且 $H=1$，因此观测模型为 $y=Hx+\\epsilon$，其中 $\\epsilon$ 是附加的零均值高斯仪器噪声。先验（背景）状态被建模为一个均值为 $x_b$、方差为 $B$ 的高斯随机变量。观测误差是高斯的，方差为 $R$。在这些假设下，最大后验（MAP）估计等于从线性高斯模型的贝叶斯定理推导出的二次 3D-Var 代价函数的最小值点。\n\n从高斯先验 $p(x)$ 和高斯似然 $p(y\\mid x)$ 的定义出发，利用贝叶斯定理构建后验 $p(x\\mid y)$，推导当 $H=1$ 时的标量 MAP 估计量 $x_a$ 及相关的分析方差 $\\sigma_a^2$。然后，在 $x_b=2$、$B=4$、$y=5$、$H=1$ 和 $R=1$ 的情况下，计算您推导出的表达式的值。\n\n将最终答案以单行矩阵 $\\begin{pmatrix}x_a  \\sigma_a^2\\end{pmatrix}$ 的形式报告，其中包含两个条目。以精确值表示，无需四舍五入。",
            "solution": "该问题是有效的，因为它具有科学依据、提法明确、客观，并包含得出唯一解所需的所有信息。它代表了贝叶斯推断在数据同化背景下的一个标准应用。\n\n目标是推导最大后验（MAP）估计，记为分析状态 $x_a$，以及相应的分析方差 $\\sigma_a^2$。推导始于贝叶斯定理，该定理将给定观测 $y$ 时状态 $x$ 的后验概率与状态的先验概率以及给定状态时观测的似然联系起来：\n\n$$p(x \\mid y) = \\frac{p(y \\mid x) p(x)}{p(y)}$$\n\n项 $p(y)$ 是一个归一化常数，与 $x$ 无关。因此，为了找到使后验概率最大化的 $x$ 值，我们可以写成：\n\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\n\n问题指定状态变量 $x$ 的先验分布为高斯分布，其均值为 $x_b$（背景状态），方差为 $B$。其概率密度函数（PDF）为：\n\n$$p(x) = \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x - x_b)^2}{B}\\right)$$\n\n观测模型给定为 $y = Hx + \\epsilon$，其中 $H=1$，观测误差 $\\epsilon$ 从均值为零、方差为 $R$ 的高斯分布中抽取。这定义了似然函数 $p(y \\mid x)$，它是一个以 $Hx=x$ 为中心、方差为 $R$ 的关于 $y$ 的高斯分布：\n\n$$p(y \\mid x) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y - x)^2}{R}\\right)$$\n\n将先验和似然的 PDF 代入后验的比例关系式中，得到：\n\n$$p(x \\mid y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y - x)^2}{R}\\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x - x_b)^2}{B}\\right) \\right]$$\n\n合并指数项并忽略常数系数，我们得到：\n\n$$p(x \\mid y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ \\frac{(x - x_b)^2}{B} + \\frac{(y - x)^2}{R} \\right]\\right)$$\n\nMAP 估计 $x_a$ 是使该后验概率最大化的 $x$ 的值。最大化 $p(x \\mid y)$ 等价于最小化其自然对数的负值。这定义了 3D-Var 代价函数 $J(x)$：\n\n$$J(x) = \\frac{1}{2} \\left[ \\frac{(x - x_b)^2}{B} + \\frac{(y - x)^2}{R} \\right]$$\n\n为了找到 $J(x)$ 的最小值，我们计算它关于 $x$ 的一阶导数并令其为零：\n\n$$\\frac{dJ}{dx} = \\frac{1}{2} \\left[ \\frac{2(x - x_b)}{B} + \\frac{2(y - x)(-1)}{R} \\right] = \\frac{x - x_b}{B} - \\frac{y - x}{R}$$\n\n在 $x = x_a$ 处令导数为零：\n\n$$\\frac{x_a - x_b}{B} - \\frac{y - x_a}{R} = 0$$\n\n$$\\frac{x_a - x_b}{B} = \\frac{y - x_a}{R}$$\n\n解出 $x_a$：\n\n$$R(x_a - x_b) = B(y - x_a)$$\n$$Rx_a - Rx_b = By - Bx_a$$\n$$Rx_a + Bx_a = By + Rx_b$$\n$$x_a(R + B) = By + Rx_b$$\n\n这就得出了分析状态 $x_a$ 的 MAP 估计量：\n\n$$x_a = \\frac{By + Rx_b}{B+R}$$\n\n后验分布 $p(x \\mid y)$ 本身也是高斯分布，因为它与两个高斯函数的乘积成正比。该后验分布的方差即为分析方差 $\\sigma_a^2$。在变分数据同化中，分析（后验）协方差是在最小值点处计算的代价函数的海森矩阵的逆。对于这个标量问题，分析方差是 $J(x)$ 的二阶导数的倒数。\n\n代价函数的二阶导数是：\n\n$$\\frac{d^2 J}{dx^2} = \\frac{d}{dx} \\left( \\frac{x - x_b}{B} - \\frac{y - x}{R} \\right) = \\frac{1}{B} + \\frac{1}{R}$$\n\n分析方差 $\\sigma_a^2$ 是该表达式的倒数：\n\n$$\\sigma_a^2 = \\left(\\frac{d^2 J}{dx^2}\\right)^{-1} = \\left(\\frac{1}{B} + \\frac{1}{R}\\right)^{-1} = \\left(\\frac{R + B}{BR}\\right)^{-1} = \\frac{BR}{B+R}$$\n\n现在我们使用给定的数值计算这些表达式：$x_b=2$、$B=4$、$y=5$ 和 $R=1$。\n\n分析状态 $x_a$ 是：\n\n$$x_a = \\frac{(4)(5) + (1)(2)}{4+1} = \\frac{20 + 2}{5} = \\frac{22}{5}$$\n\n分析方差 $\\sigma_a^2$ 是：\n\n$$\\sigma_a^2 = \\frac{(4)(1)}{4+1} = \\frac{4}{5}$$\n\n最终答案是序对 $(x_a, \\sigma_a^2)$，以行矩阵形式呈现。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{22}{5}  \\frac{4}{5} \\end{pmatrix}}$$"
        },
        {
            "introduction": "掌握了标量情况后，下一个自然的步骤是将其扩展到多维状态向量。这个练习将引导您将相同的贝叶斯原理应用于更复杂的场景，其中涉及背景误差协方差矩阵和观测算子。解决此问题的关键在于推导并求解“正规方程组”（Normal Equations），这是解决线性最小二乘问题的核心，也是许多数据同化和机器学习算法的基石。",
            "id": "3864622",
            "problem": "考虑一个二维环境状态向量 $x \\in \\mathbb{R}^{2}$，它代表了在线性设置下从单次卫星观测中要分析的空间聚合量。假设一个基于高斯误差统计和线性观测算子的三维变分（3D-Var）数据同化框架，其中分析场 $x^{a}$ 使由背景场和观测场的不符量（misfit）构成的二次代价函数最小化。设背景场误差协方差为 $B=\\begin{bmatrix}1  0 \\\\ 0  4\\end{bmatrix}$，观测算子为 $H=\\begin{bmatrix}1  1\\end{bmatrix}$，观测误差协方差为 $R=1$，背景场（也称为先验）为 $x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，观测值为 $y=3$。\n\n从结合了线性高斯假设下背景场和观测场不符量的 3D-Var 代价函数定义出发，推导表征最小值的的一阶最优性条件（正规方程）。然后，使用给定的数值，计算分析增量 $\\delta x^{a}=x^{a}-x_{b}$ 和分析状态 $x^{a}$。将您的最终结果表示为顺序为 $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$ 的单行矩阵。\n\n不需要单位。请提供精确值，无需四舍五入。",
            "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤1：提取已知条件\n-   状态向量：$x \\in \\mathbb{R}^{2}$\n-   背景场误差协方差：$B=\\begin{bmatrix}1  0 \\\\ 0  4\\end{bmatrix}$\n-   观测算子：$H=\\begin{bmatrix}1  1\\end{bmatrix}$\n-   观测误差协方差：$R=1$\n-   背景场状态：$x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$\n-   观测值：$y=3$\n-   任务：推导正规方程，然后计算分析增量 $\\delta x^{a}=x^{a}-x_{b}$ 和分析状态 $x^{a}$。\n-   最终输出格式：单行矩阵 $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$。\n\n### 步骤2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n-   **科学依据：** 该问题描述了一个标准的三维变分（3D-Var）数据同化场景。二次代价函数、背景/观测误差协方差以及线性观测算子的使用是遥感和环境建模领域中这一既定科学方法的典型要素。该框架是在线性高斯假设下从贝叶斯定理推导出来的。该问题具有科学合理性。\n-   **适定性：** 该问题旨在寻找一个二次代价函数的最小值。背景场误差协方差矩阵 $B$ 的特征值为1和4，均为正数，因此 $B$ 是正定的，从而可逆。观测误差协方差 $R=1$ 是一个正标量。代价函数的黑塞矩阵（Hessian matrix）$(B^{-1} + H^T R^{-1} H)$ 将被证明是正定的，这保证了唯一稳定最小值的存在。所有必要信息均已提供，且没有矛盾之处。该问题是适定的。\n-   **客观性：** 问题使用精确的数学语言和定义（$x_b, B, H, R, y$）进行陈述。没有主观或含糊的术语。该问题是客观的。\n-   **其他缺陷：** 该问题并非无足轻重，因为它需要推导和矩阵代数。对于一个教科书式的例子来说，它不是隐喻性的、不完整的或不切实际的。\n\n### 步骤3：结论与行动\n该问题是有效的。将提供解答。\n\n3D-Var 分析状态 $x^a$ 是使代价函数 $J(x)$ 最小化的状态向量 $x$。代价函数衡量了与背景场状态和观测值的不符量，并由它们各自的误差协方差加权。对于线性高斯系统，其表达式为：\n$$J(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)$$\n为了找到最小值，我们计算 $J(x)$ 相对于 $x$ 的梯度，并令其为零。梯度 $\\nabla_x J(x)$ 为：\n$$\\nabla_x J(x) = B^{-1}(x - x_b) - H^T R^{-1} (y - Hx)$$\n在 $x=x^a$ 处将梯度设为零，得到一阶最优性条件，也称为正规方程：\n$$B^{-1}(x^a - x_b) - H^T R^{-1} (y - Hx^a) = 0$$\n这就是所要求的正规方程的推导。\n\n我们需要求解分析增量，定义为 $\\delta x^a = x^a - x_b$。我们将 $x^a = x_b + \\delta x^a$ 代入正规方程：\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - H(x_b + \\delta x^a)) = 0$$\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - Hx_b - H\\delta x^a) = 0$$\n重新整理各项以求解 $\\delta x^a$：\n$$B^{-1}(\\delta x^a) + H^T R^{-1} H\\delta x^a = H^T R^{-1} (y - Hx_b)$$\n$$(B^{-1} + H^T R^{-1} H) \\delta x^a = H^T R^{-1} (y - Hx_b)$$\n此方程可以求解分析增量 $\\delta x^a$。\n\n现在，我们代入给定的数值：\n-   $x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n-   $B = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix} \\implies B^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix}$\n-   $H = \\begin{bmatrix} 1  1 \\end{bmatrix} \\implies H^T = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n-   $R = 1 \\implies R^{-1} = 1$\n-   $y = 3$\n\n首先，我们计算方程中求解 $\\delta x^a$ 的各个部分。\n项 $(y - Hx_b)$ 是新息（innovation）：\n$$y - Hx_b = 3 - \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 3 - 0 = 3$$\n方程的右侧是：\n$$H^T R^{-1} (y - Hx_b) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) (3) = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$$\n方程左侧的矩阵是代价函数的黑塞矩阵：\n$$B^{-1} + H^T R^{-1} H = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) \\begin{bmatrix} 1  1 \\end{bmatrix}$$\n$$= \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  1 + \\frac{1}{4} \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}$$\n为了求解 $\\delta x^a$，我们需要对该矩阵求逆。其行列式为：\n$$\\det\\left(\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}\\right) = (2)\\left(\\frac{5}{4}\\right) - (1)(1) = \\frac{5}{2} - 1 = \\frac{3}{2}$$\n逆矩阵为：\n$$\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}^{-1} = \\frac{1}{\\frac{3}{2}} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\frac{2}{3} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\begin{bmatrix} \\frac{10}{12}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix}$$\n现在我们可以求解 $\\delta x^a$：\n$$\\delta x^a = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}(3) - \\frac{2}{3}(3) \\\\ -\\frac{2}{3}(3) + \\frac{4}{3}(3) \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{2} - 2 \\\\ -2 + 4 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\n因此，分析增量的分量为 $\\delta x^a_1 = \\frac{1}{2}$ 和 $\\delta x^a_2 = 2$。\n\n最后，我们计算分析状态 $x^a$：\n$$x^a = x_b + \\delta x^a = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\n分析状态的分量为 $x^a_1 = \\frac{1}{2}$ 和 $x^a_2 = 2$。\n\n最终结果要求以 $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$ 的顺序表示为单行矩阵。这得到：\n$$\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}}$$"
        },
        {
            "introduction": "本练习是一项更高级的实践，旨在展示变分数据同化框架的真正威力与灵活性。我们不再局限于理想化的高斯噪声假设，转而处理生物医学成像（如荧光显微成像）中的一个常见挑战：对低光子计数数据进行建模。通过编程实现并比较高斯和泊松（Poisson）两种观测模型，您将获得根据测量过程的物理特性调整代价函数的实践经验，并深刻体会模型选择对最终估计精度的影响。",
            "id": "3942146",
            "problem": "考虑一个一维动态荧光成像实验，其中潜在的荧光强度状态序列 $\\{x_t\\}_{t=0}^{T-1}$ 根据一个线性时间离散衰减模型演化。数据同化任务是根据观测到的光子计数 $\\{y_t\\}$ 以及给定的模型和先验信息，推断潜在轨迹 $\\{x_t\\}$。\n\n基本原理和定义：\n- 用于后验推断的贝叶斯法则：后验密度与先验密度和似然函数的乘积成正比。\n- 最大后验 (MAP) 估计：使后验密度最大化的参数，等价于使负对数后验最小化的参数。\n- 荧光的离散时间线性衰减模型：$x_{t+1} = a x_t + \\xi_t$，其中 $a = \\exp(-k \\Delta t)$，$k$ 为衰减率，$\\Delta t$ 为时间步长。模型失配 $\\xi_t$ 被建模为均值为零、方差为 $q$ 的高斯分布。\n- 初始状态的高斯先验：$x_0 \\sim \\mathcal{N}(m_0, p_0)$。\n- 两种将期望光子计数与潜在状态关联的观测模型：\n  1. 泊松计数模型：$y_t \\sim \\mathrm{Poisson}(\\lambda_t)$，其中 $\\lambda_t = s x_t + b$，$s$ 是系统灵敏度（单位荧光的计数值），$b$ 是背景计数值。\n  2. 高斯噪声模型：$y_t \\sim \\mathcal{N}(s x_t + b, \\sigma^2)$，其中方差 $\\sigma^2$ 是常数。\n\n任务：\n1. 严格从贝叶斯法则和上述定义出发，推导在泊松和高斯观测模型下的 MAP 估计问题。将每个问题表述为在轨迹 $\\{x_t\\}$ 上最小化一个负对数后验目标函数，并满足对所有 $t$ 的物理约束 $x_t \\ge 0$。\n2. 设计一个有原则的变分数据同化算法，用于计算两种模型下的 MAP 轨迹。该算法应将动力学模型作为二次正则化（软约束）并应用边界约束 $x_t \\ge 0$。推导每个目标函数关于轨迹分量的梯度，以支持基于梯度的数值优化。通过要求 $s x_t + b  0$ 明确确保泊松情况下的定义域有效性。\n3. 对于以下测试套件，计算两种模型的 MAP 轨迹，然后计算每个 MAP 轨迹与真实轨迹之间的均方根误差 (RMSE)。对每个测试用例，报告差值 $\\mathrm{RMSE}_{\\mathrm{Gaussian}} - \\mathrm{RMSE}_{\\mathrm{Poisson}}$ 的浮点数值。\n\n真实值和动力学参数：\n- 帧数：$T = 12$。\n- 时间步长：$\\Delta t = 1$ 秒。\n- 衰减率：$k = 0.2$ 每秒，因此 $a = \\exp(-k \\Delta t)$。\n- 真实初始状态：$x_0^{\\mathrm{true}} = 2$（任意荧光单位）。\n- 真实轨迹：$x_t^{\\mathrm{true}} = x_0^{\\mathrm{true}} a^t$，对于 $t = 0,\\dots,11$。\n- $x_0$ 的先验均值和方差：$m_0 = 1.5$，$p_0 = 0.25$。\n- 模型失配方差：$q = 0.04$。\n\n三个测试用例的观测模型参数和观测光子计数：\n- 用例 1（高计数区）：\n  - 灵敏度：$s = 80$ 计数/单位荧光。\n  - 背景：$b = 20$ 计数。\n  - 高斯噪声标准差：$\\sigma = 5$ 计数。\n  - 观测计数值列表：$[174, 152, 132, 111, 96, 76, 68, 61, 55, 47, 43, 36]$。\n- 用例 2（低计数区）：\n  - 灵敏度：$s = 8$ 计数/单位荧光。\n  - 背景：$b = 1$ 计数。\n  - 高斯噪声标准差：$\\sigma = 3$ 计数。\n  - 观测计数值列表：$[16, 13, 12, 9, 8, 7, 6, 5, 6, 4, 4, 3]$。\n- 用例 3（高背景区）：\n  - 灵敏度：$s = 15$ 计数/单位荧光。\n  - 背景：$b = 40$ 计数。\n  - 高斯噪声标准差：$\\sigma = 5$ 计数。\n  - 观测计数值列表：$[68, 63, 59, 58, 52, 53, 49, 48, 47, 45, 44, 43]$。\n\n算法要求和输出：\n- 针对两种模型，使用支持边界约束 $x_t \\ge 0$ 的基于梯度的凸优化方法。目标函数必须包括来自模型的二次动力学正则化项和相应的观测负对数似然项。\n- 计算每个模型相对于 $x_t^{\\mathrm{true}}$ 的 RMSE：\n  $$\\mathrm{RMSE} = \\sqrt{\\frac{1}{T} \\sum_{t=0}^{T-1} \\left(x_t^{\\mathrm{MAP}} - x_t^{\\mathrm{true}}\\right)^2}.$$\n- 最终输出格式：您的程序应生成一行内容，包含三个结果，形式为方括号括起来的逗号分隔列表，其中每个元素是对应测试用例的 $\\mathrm{RMSE}_{\\mathrm{Gaussian}} - \\mathrm{RMSE}_{\\mathrm{Poisson}}$ 的浮点值，四舍五入到六位小数。例如：$[r_1,r_2,r_3]$。",
            "solution": "该问题具有科学依据，是适定的、客观的，并且包含了推导出唯一解所需的所有信息。这是变分数据同化中的一个标准问题，在地球科学中也称为 4D-Var，或在统计学中称为 MAP 平滑。物理模型和数学框架是标准且一致的。因此，该问题被认为是有效的。\n\n### 第 1 步：MAP 目标函数的推导\n\n目标是给定观测值 $\\mathbf{y} = \\{y_t\\}_{t=0}^{T-1}$，找到状态轨迹 $\\mathbf{x} = \\{x_t\\}_{t=0}^{T-1}$ 的最大后验 (MAP) 估计。根据贝叶斯法则，后验概率密度为：\n$$\np(\\mathbf{x} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\mathbf{x}) p(\\mathbf{x})\n$$\n其中 $p(\\mathbf{y} | \\mathbf{x})$ 是似然， $p(\\mathbf{x})$ 是先验。MAP 估计通过最大化该后验概率找到，这等价于最小化其负对数：\n$$\nJ(\\mathbf{x}) = - \\log p(\\mathbf{x} | \\mathbf{y}) \\propto - \\log p(\\mathbf{y} | \\mathbf{x}) - \\log p(\\mathbf{x})\n$$\n我们将目标函数 $J(\\mathbf{x})$ 定义为负对数先验 $J_{prior}(\\mathbf{x})$ 和负对数似然（观测代价）$J_{obs}(\\mathbf{x})$ 之和。\n\n**1.1. 负对数先验项, $J_{prior}(\\mathbf{x})$**\n\n轨迹的先验 $p(\\mathbf{x})$ 由初始状态先验和动力学模型定义。状态构成马尔可夫链，因此 $p(\\mathbf{x}) = p(x_0) \\prod_{t=0}^{T-2} p(x_{t+1}|x_t)$。\n- 初始状态是高斯的：$x_0 \\sim \\mathcal{N}(m_0, p_0)$，因此 $p(x_0) = \\frac{1}{\\sqrt{2\\pi p_0}} \\exp\\left(-\\frac{(x_0 - m_0)^2}{2p_0}\\right)$。\n- 动力学模型为 $x_{t+1} = a x_t + \\xi_t$，其中 $\\xi_t \\sim \\mathcal{N}(0, q)$。这意味着转移概率为 $p(x_{t+1}|x_t) = \\mathcal{N}(x_{t+1}; a x_t, q) = \\frac{1}{\\sqrt{2\\pi q}} \\exp\\left(-\\frac{(x_{t+1} - a x_t)^2}{2q}\\right)$。\n\n负对数先验为（忽略常数项）：\n$$\nJ_{prior}(\\mathbf{x}) = -\\log p(x_0) - \\sum_{t=0}^{T-2} \\log p(x_{t+1}|x_t) = \\frac{1}{2p_0}(x_0 - m_0)^2 + \\frac{1}{2q}\\sum_{t=0}^{T-2}(x_{t+1} - a x_t)^2\n$$\n该项将动力学模型表示为二次正则化，惩罚对模型动力学的偏离。\n\n**1.2. 负对数似然项, $J_{obs}(\\mathbf{x})$**\n\n假设观测值 $y_t$ 在给定状态 $x_t$ 的条件下是独立的。因此，总似然为 $p(\\mathbf{y} | \\mathbf{x}) = \\prod_{t=0}^{T-1} p(y_t | x_t)$。\n\n**1.2.1. 高斯观测模型**\n模型为 $y_t \\sim \\mathcal{N}(s x_t + b, \\sigma^2)$，其概率密度函数为 $p(y_t | x_t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_t - (s x_t + b))^2}{2\\sigma^2}\\right)$。\n负对数似然（忽略常数项）为：\n$$\nJ_{obs,G}(\\mathbf{x}) = \\sum_{t=0}^{T-1} \\frac{(s x_t + b - y_t)^2}{2\\sigma^2}\n$$\n\n**1.2.2. 泊松观测模型**\n模型为 $y_t \\sim \\mathrm{Poisson}(\\lambda_t)$，其中 $\\lambda_t = s x_t + b$。其概率质量函数为 $p(y_t | x_t) = \\frac{\\lambda_t^{y_t} e^{-\\lambda_t}}{y_t!}$。\n负对数似然（忽略常数项 $\\log(y_t!)$）为：\n$$\nJ_{obs,P}(\\mathbf{x}) = - \\sum_{t=0}^{T-1} (y_t \\log(\\lambda_t) - \\lambda_t) = \\sum_{t=0}^{T-1} (s x_t + b - y_t \\log(s x_t + b))\n$$\n必须强制执行对所有 $t$ 的物理约束 $x_t \\ge 0$。此外，对数的参数必须为正：$s x_t + b  0$。由于所有给定的参数 $s, b$ 均为正，如果 $x_t \\ge 0$，则此条件自动满足。\n\n**1.3. 完整的 MAP 目标函数**\n\n结合先验项和观测项，我们得到两个需要最小化的目标函数，其约束条件为对所有 $t=0, ..., T-1$ 均有 $x_t \\ge 0$。\n\n**高斯模型目标函数：**\n$$\nJ_G(\\mathbf{x}) = \\frac{1}{2p_0}(x_0 - m_0)^2 + \\frac{1}{2q}\\sum_{t=0}^{T-2}(x_{t+1} - a x_t)^2 + \\frac{1}{2\\sigma^2}\\sum_{t=0}^{T-1} (s x_t + b - y_t)^2\n$$\n\n**泊松模型目标函数：**\n$$\nJ_P(\\mathbf{x}) = \\frac{1}{2p_0}(x_0 - m_0)^2 + \\frac{1}{2q}\\sum_{t=0}^{T-2}(x_{t+1} - a x_t)^2 + \\sum_{t=0}^{T-1} (s x_t + b - y_t \\log(s x_t + b))\n$$\n两个目标函数都是凸函数，确保基于梯度的优化器可以找到唯一的全局最小值。\n\n### 第 2 步：用于优化的梯度\n\n为了使用基于梯度的优化器，我们需要每个目标函数相对于轨迹 $\\mathbf{x}$ 的每个分量 $x_k$ 的梯度。梯度 $\\nabla J(\\mathbf{x})$ 的分量为 $(\\nabla J(\\mathbf{x}))_k = \\frac{\\partial J}{\\partial x_k}$。\n\n**2.1. 先验项的梯度**\n先验项对两个模型是共有的。其梯度分量为：\n$$\n\\frac{\\partial J_{prior}}{\\partial x_k} =\n\\begin{cases}\n\\frac{1}{p_0}(x_0 - m_0) - \\frac{a}{q}(x_1 - a x_0)  \\text{若 } k=0 \\\\\n\\frac{1}{q}(x_k - a x_{k-1}) - \\frac{a}{q}(x_{k+1} - a x_k)  \\text{若 } 0  k  T-1 \\\\\n\\frac{1}{q}(x_{T-1} - a x_{T-2})  \\text{若 } k=T-1\n\\end{cases}\n$$\n\n**2.2. 观测项的梯度**\n观测代价项的导数仅影响梯度的第 $k$ 个分量。\n\n**高斯模型：**\n$$\n\\frac{\\partial J_{obs,G}}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( \\frac{(s x_k + b - y_k)^2}{2\\sigma^2} \\right) = \\frac{s(s x_k + b - y_k)}{\\sigma^2}\n$$\n\n**泊松模型：**\n$$\n\\frac{\\partial J_{obs,P}}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} (s x_k + b - y_k \\log(s x_k + b)) = s - \\frac{s y_k}{s x_k + b} = s\\left(1 - \\frac{y_k}{s x_k + b}\\right)\n$$\n\n**2.3. 完整梯度**\n完整梯度是先验梯度和观测梯度的和：$\\nabla J(\\mathbf{x}) = \\nabla J_{prior}(\\mathbf{x}) + \\nabla J_{obs}(\\mathbf{x})$。对于每个分量 $k$：\n$(\\nabla J_G(\\mathbf{x}))_k = \\frac{\\partial J_{prior}}{\\partial x_k} + \\frac{s(s x_k + b - y_k)}{\\sigma^2}$\n$(\\nabla J_P(\\mathbf{x}))_k = \\frac{\\partial J_{prior}}{\\partial x_k} + s\\left(1 - \\frac{y_k}{s x_k + b}\\right)$\n\n这些表达式在数值优化程序中实现，以找到 MAP 轨迹 $x^{\\mathrm{MAP}}$。\n\n### 第 3 步：数值计算和 RMSE\n\n使用支持边界约束（$x_t \\ge 0$）的数值优化器（`scipy.optimize.minimize`，采用 L-BFGS-B 方法），为每个测试用例计算两种模型的 MAP 轨迹。使用上面推导的目标函数及其梯度。然后，对每个估计轨迹与真实轨迹计算均方根误差 (RMSE)：\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{T} \\sum_{t=0}^{T-1} \\left(x_t^{\\mathrm{MAP}} - x_t^{\\mathrm{true}}\\right)^2}\n$$\n每个用例最终报告的值是差值 $\\mathrm{RMSE}_{\\mathrm{Gaussian}} - \\mathrm{RMSE}_{\\mathrm{Poisson}}$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the variational data assimilation problem for all test cases.\n    \"\"\"\n\n    # Ground truth and dynamical parameters (shared across all cases)\n    T = 12\n    delta_t = 1.0\n    k = 0.2\n    a = np.exp(-k * delta_t)\n    x0_true = 2.0\n    x_true = x0_true * (a ** np.arange(T))\n    \n    # Prior and model parameters\n    m0 = 1.5\n    p0 = 0.25\n    q = 0.04\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"s\": 80.0, \"b\": 20.0, \"sigma\": 5.0,\n            \"y\": np.array([174, 152, 132, 111, 96, 76, 68, 61, 55, 47, 43, 36])\n        },\n        {\n            \"s\": 8.0, \"b\": 1.0, \"sigma\": 3.0,\n            \"y\": np.array([16, 13, 12, 9, 8, 7, 6, 5, 6, 4, 4, 3])\n        },\n        {\n            \"s\": 15.0, \"b\": 40.0, \"sigma\": 5.0,\n            \"y\": np.array([68, 63, 59, 58, 52, 53, 49, 48, 47, 45, 44, 43])\n        }\n    ]\n\n    results = []\n    \n    # Common settings for the optimizer\n    bounds = [(0, None)] * T\n\n    for case in test_cases:\n        s, b, sigma, y_obs = case[\"s\"], case[\"b\"], case[\"sigma\"], case[\"y\"]\n\n        # A physically-motivated initial guess for the optimizer\n        x_init = np.maximum(1e-6, (y_obs - b) / s)\n\n        # Solve for Gaussian model\n        res_g = minimize(\n            fun=objective_and_grad,\n            x0=x_init,\n            args=(T, a, m0, p0, q, 'gaussian', y_obs, s, b, sigma),\n            method='L-BFGS-B',\n            jac=True,\n            bounds=bounds\n        )\n        x_map_g = res_g.x\n        \n        # Solve for Poisson model\n        res_p = minimize(\n            fun=objective_and_grad,\n            x0=x_init,\n            args=(T, a, m0, p0, q, 'poisson', y_obs, s, b, sigma),\n            method='L-BFGS-B',\n            jac=True,\n            bounds=bounds\n        )\n        x_map_p = res_p.x\n\n        # Calculate RMSE for both models\n        rmse_g = np.sqrt(np.mean((x_map_g - x_true)**2))\n        rmse_p = np.sqrt(np.mean((x_map_p - x_true)**2))\n\n        # Store the difference\n        results.append(rmse_g - rmse_p)\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef objective_and_grad(x, T, a, m0, p0, q, model_type, y, s, b, sigma):\n    \"\"\"\n    Computes the objective function and its gradient for both models.\n    This function is passed to scipy.optimize.minimize.\n    \"\"\"\n    # 1. Prior term and its gradient (common to both models)\n    \n    # Objective\n    prior_obj = 0.5 * (x[0] - m0)**2 / p0 + \\\n                0.5 * np.sum((x[1:] - a * x[:-1])**2) / q\n    \n    # Gradient\n    grad_prior = np.zeros(T)\n    grad_prior[0] = (x[0] - m0) / p0 - a * (x[1] - a * x[0]) / q\n    \n    # Using array slicing for the middle part\n    dyn_term1 = (x[1:-1] - a * x[:-2]) / q\n    dyn_term2 = -a * (x[2:] - a * x[1:-1]) / q\n    grad_prior[1:-1] = dyn_term1 + dyn_term2\n    \n    grad_prior[T-1] = (x[T-1] - a * x[T-2]) / q\n\n    # 2. Observation term and its gradient (model-specific)\n    if model_type == 'gaussian':\n        # Objective\n        obs_term = s * x + b - y\n        obs_obj = 0.5 * np.sum(obs_term**2) / sigma**2\n        \n        # Gradient\n        grad_obs = s * obs_term / sigma**2\n        \n    elif model_type == 'poisson':\n        # Objective\n        lambda_t = s * x + b\n        # Add a small epsilon for numerical stability if x is at bound 0 and b is 0\n        # although problem statement guarantees b>0.\n        lambda_t = np.maximum(1e-9, lambda_t)\n        obs_obj = np.sum(lambda_t - y * np.log(lambda_t))\n        \n        # Gradient\n        grad_obs = s * (1 - y / lambda_t)\n        \n    else:\n        raise ValueError(\"Unknown model type\")\n\n    # 3. Total objective and gradient\n    total_obj = prior_obj + obs_obj\n    total_grad = grad_prior + grad_obs\n    \n    return total_obj, total_grad\n\nsolve()\n```"
        }
    ]
}