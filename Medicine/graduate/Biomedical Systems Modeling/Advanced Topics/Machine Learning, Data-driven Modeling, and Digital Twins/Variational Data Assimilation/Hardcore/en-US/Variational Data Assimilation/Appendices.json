{
    "hands_on_practices": [
        {
            "introduction": "The foundation of variational data assimilation lies in its elegant connection to Bayesian inference. This first exercise provides a hands-on derivation of this link in its most fundamental form: a single scalar state variable. By starting from Bayes' theorem and deriving the 3D-Var cost function and its analytical solution, you will gain a core intuition for how prior knowledge (the background) and new data (the observation) are optimally blended to produce an improved estimate of the system's state .",
            "id": "3864732",
            "problem": "Consider a single-grid-cell environmental state variable $x$ representing a column-averaged tracer concentration at analysis time in a remote sensing and environmental modeling system. You are performing Three-Dimensional Variational (3D-Var) data assimilation, which is the time-independent limit of Four-Dimensional Variational (4D-Var) data assimilation, using one satellite observation. Assume a linear observation operator with $H=1$, so the observation model is $y=Hx+\\epsilon$, where $\\epsilon$ is additive zero-mean Gaussian instrument noise. The prior (background) state is modeled as a Gaussian random variable with mean $x_b$ and variance $B$. The observation error is Gaussian with variance $R$. Under these assumptions, the Maximum A Posteriori (MAP) estimate equals the minimizer of the quadratic 3D-Var cost function derived from Bayes’ theorem for linear-Gaussian models.\n\nStarting from the definitions of a Gaussian prior $p(x)$ and a Gaussian likelihood $p(y\\mid x)$, and using Bayes’ theorem to form the posterior $p(x\\mid y)$, derive the scalar MAP estimator $x_a$ and the associated analysis variance $\\sigma_a^2$ for $H=1$. Then evaluate your expressions for the case $x_b=2$, $B=4$, $y=5$, $H=1$, and $R=1$.\n\nReport the final answer as two entries in a single row matrix $\\begin{pmatrix}x_a  \\sigma_a^2\\end{pmatrix}$. Express the values exactly; no rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It represents a standard application of Bayesian inference in the context of data assimilation.\n\nThe objective is to derive the Maximum A Posteriori (MAP) estimate, denoted as the analysis state $x_a$, and the corresponding analysis variance $\\sigma_a^2$. The derivation begins with Bayes' theorem, which relates the posterior probability of the state $x$ given an observation $y$ to the prior probability of the state and the likelihood of the observation given the state:\n\n$$p(x \\mid y) = \\frac{p(y \\mid x) p(x)}{p(y)}$$\n\nThe term $p(y)$ is a normalization constant, independent of $x$. Therefore, for the purpose of finding the value of $x$ that maximizes the posterior, we can write:\n\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\n\nThe problem specifies a Gaussian prior distribution for the state variable $x$, with mean $x_b$ (the background state) and variance $B$. The probability density function (PDF) is:\n\n$$p(x) = \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x - x_b)^2}{B}\\right)$$\n\nThe observation model is given as $y = Hx + \\epsilon$, where $H=1$ and the observation error $\\epsilon$ is drawn from a zero-mean Gaussian distribution with variance $R$. This defines the likelihood function $p(y \\mid x)$ as a Gaussian distribution for $y$ centered at $Hx=x$ with variance $R$:\n\n$$p(y \\mid x) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y - x)^2}{R}\\right)$$\n\nSubstituting the PDFs for the prior and the likelihood into the proportionality for the posterior gives:\n\n$$p(x \\mid y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y - x)^2}{R}\\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x - x_b)^2}{B}\\right) \\right]$$\n\nCombining the exponential terms and neglecting the constant coefficients, we obtain:\n\n$$p(x \\mid y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ \\frac{(x - x_b)^2}{B} + \\frac{(y - x)^2}{R} \\right]\\right)$$\n\nThe MAP estimate $x_a$ is the value of $x$ that maximizes this posterior probability. Maximizing $p(x \\mid y)$ is equivalent to minimizing the negative of its natural logarithm. This defines the 3D-Var cost function $J(x)$:\n\n$$J(x) = \\frac{1}{2} \\left[ \\frac{(x - x_b)^2}{B} + \\frac{(y - x)^2}{R} \\right]$$\n\nTo find the minimum of $J(x)$, we compute its first derivative with respect to $x$ and set it to zero:\n\n$$\\frac{dJ}{dx} = \\frac{1}{2} \\left[ \\frac{2(x - x_b)}{B} + \\frac{2(y - x)(-1)}{R} \\right] = \\frac{x - x_b}{B} - \\frac{y - x}{R}$$\n\nSetting the derivative to zero at $x = x_a$:\n\n$$\\frac{x_a - x_b}{B} - \\frac{y - x_a}{R} = 0$$\n\n$$\\frac{x_a - x_b}{B} = \\frac{y - x_a}{R}$$\n\nSolving for $x_a$:\n\n$$R(x_a - x_b) = B(y - x_a)$$\n$$Rx_a - Rx_b = By - Bx_a$$\n$$Rx_a + Bx_a = By + Rx_b$$\n$$x_a(R + B) = By + Rx_b$$\n\nThis yields the MAP estimator for the analysis state $x_a$:\n\n$$x_a = \\frac{By + Rx_b}{B+R}$$\n\nThe posterior distribution $p(x \\mid y)$ is itself Gaussian, as it is proportional to the product of two Gaussian functions. The variance of this posterior distribution is the analysis variance, $\\sigma_a^2$. In variational data assimilation, the analysis (posterior) covariance is the inverse of the Hessian of the cost function evaluated at the minimum. For this scalar problem, the analysis variance is the inverse of the second derivative of $J(x)$.\n\nThe second derivative of the cost function is:\n\n$$\\frac{d^2 J}{dx^2} = \\frac{d}{dx} \\left( \\frac{x - x_b}{B} - \\frac{y - x}{R} \\right) = \\frac{1}{B} + \\frac{1}{R}$$\n\nThe analysis variance $\\sigma_a^2$ is the inverse of this expression:\n\n$$\\sigma_a^2 = \\left(\\frac{d^2 J}{dx^2}\\right)^{-1} = \\left(\\frac{1}{B} + \\frac{1}{R}\\right)^{-1} = \\left(\\frac{R + B}{BR}\\right)^{-1} = \\frac{BR}{B+R}$$\n\nNow we evaluate these expressions using the given numerical values: $x_b=2$, $B=4$, $y=5$, and $R=1$.\n\nThe analysis state $x_a$ is:\n\n$$x_a = \\frac{(4)(5) + (1)(2)}{4+1} = \\frac{20 + 2}{5} = \\frac{22}{5}$$\n\nThe analysis variance $\\sigma_a^2$ is:\n\n$$\\sigma_a^2 = \\frac{(4)(1)}{4+1} = \\frac{4}{5}$$\n\nThe final answer is the pair $(x_a, \\sigma_a^2)$, presented as a row matrix.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{22}{5}  \\frac{4}{5} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Real-world biomedical systems are rarely described by a single variable. This practice extends the foundational concepts from the scalar case to a multi-dimensional state vector, which is essential for tackling practical applications. You will work with the matrix-vector formulation of the 3D-Var cost function, derive the corresponding normal equations, and compute the analysis state, thereby building proficiency with the linear algebra that underpins large-scale data assimilation systems .",
            "id": "3864622",
            "problem": "Consider a two-component environmental state vector $x \\in \\mathbb{R}^{2}$ representing spatially aggregated quantities to be analyzed from a single satellite-derived observation in a linear setting. Assume a three-dimensional variational (3D-Var) data assimilation framework derived from Gaussian error statistics and a linear observation operator, where the analysis $x^{a}$ minimizes the quadratic cost function built from the background and observation misfits. Let the background error covariance be $B=\\begin{bmatrix}1  0 \\\\ 0  4\\end{bmatrix}$, the observation operator be $H=\\begin{bmatrix}1  1\\end{bmatrix}$, the observation error covariance be $R=1$, the background (also called prior) be $x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, and the observation be $y=3$.\n\nStarting from the definition of the 3D-Var cost function that combines the background and observation misfits under linear-Gaussian assumptions, derive the first-order optimality condition (normal equation) that characterizes the minimum. Then, using the given numerical values, compute the analysis increment $\\delta x^{a}=x^{a}-x_{b}$ and the analysis state $x^{a}$. Express your final result as a single row matrix in the order $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$.\n\nNo units are required. Provide the exact values; no rounding is necessary.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n-   State vector: $x \\in \\mathbb{R}^{2}$\n-   Background error covariance: $B=\\begin{bmatrix}1  0 \\\\ 0  4\\end{bmatrix}$\n-   Observation operator: $H=\\begin{bmatrix}1  1\\end{bmatrix}$\n-   Observation error covariance: $R=1$\n-   Background state: $x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$\n-   Observation: $y=3$\n-   Task: Derive the normal equation, then compute the analysis increment $\\delta x^{a}=x^{a}-x_{b}$ and the analysis state $x^{a}$.\n-   Final Output Format: A single row matrix $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n-   **Scientifically Grounded:** The problem describes a standard three-dimensional variational (3D-Var) data assimilation scenario. The use of a quadratic cost function, background/observation error covariances, and a linear observation operator are canonical elements of this established scientific method in remote sensing and environmental modeling. The framework is derived from Bayes' theorem under linear-Gaussian assumptions. The problem is scientifically sound.\n-   **Well-Posed:** The problem is to find the minimum of a quadratic cost function. The background error covariance matrix $B$ has eigenvalues $1$ and $4$, which are both positive, so $B$ is positive definite and thus invertible. The observation error covariance $R=1$ is a positive scalar. The Hessian of the cost function, $(B^{-1} + H^T R^{-1} H)$, will be shown to be positive definite, which guarantees the existence of a unique, stable minimum. All necessary information is provided, and there are no contradictions. The problem is well-posed.\n-   **Objective:** The problem is stated using precise mathematical language and definitions ($x_b, B, H, R, y$). There are no subjective or ambiguous terms. The problem is objective.\n-   **Other Flaws:** The problem is not trivial, as it requires derivation and matrix algebra. It is not metaphorical, incomplete, or unrealistic for a textbook example.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\nThe 3D-Var analysis state $x^a$ is the state vector $x$ that minimizes the cost function $J(x)$. The cost function measures the misfit to the background state and the observations, weighted by their respective error covariances. For a linear-Gaussian system, it is given by:\n$$J(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)$$\nTo find the minimum, we compute the gradient of $J(x)$ with respect to $x$ and set it to zero. The gradient $\\nabla_x J(x)$ is:\n$$\\nabla_x J(x) = B^{-1}(x - x_b) - H^T R^{-1} (y - Hx)$$\nSetting the gradient to zero at $x=x^a$ gives the first-order optimality condition, also known as the normal equation:\n$$B^{-1}(x^a - x_b) - H^T R^{-1} (y - Hx^a) = 0$$\nThis is the requested derivation of the normal equation.\n\nWe are asked to find the analysis increment, defined as $\\delta x^a = x^a - x_b$. We substitute $x^a = x_b + \\delta x^a$ into the normal equation:\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - H(x_b + \\delta x^a)) = 0$$\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - Hx_b - H\\delta x^a) = 0$$\nRearranging the terms to solve for $\\delta x^a$:\n$$B^{-1}(\\delta x^a) + H^T R^{-1} H\\delta x^a = H^T R^{-1} (y - Hx_b)$$\n$$(B^{-1} + H^T R^{-1} H) \\delta x^a = H^T R^{-1} (y - Hx_b)$$\nThis equation can be solved for the analysis increment $\\delta x^a$.\n\nNow, we substitute the given numerical values:\n-   $x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n-   $B = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix} \\implies B^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix}$\n-   $H = \\begin{bmatrix} 1  1 \\end{bmatrix} \\implies H^T = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n-   $R = 1 \\implies R^{-1} = 1$\n-   $y = 3$\n\nFirst, let's compute the components of the equation for $\\delta x^a$.\nThe term $(y - Hx_b)$ is the innovation:\n$$y - Hx_b = 3 - \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 3 - 0 = 3$$\nThe right-hand side of the equation is:\n$$H^T R^{-1} (y - Hx_b) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) (3) = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$$\nThe matrix on the left-hand side is the Hessian of the cost function:\n$$B^{-1} + H^T R^{-1} H = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) \\begin{bmatrix} 1  1 \\end{bmatrix}$$\n$$= \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  1 + \\frac{1}{4} \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}$$\nTo solve for $\\delta x^a$, we need to invert this matrix. The determinant is:\n$$\\det\\left(\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}\\right) = (2)\\left(\\frac{5}{4}\\right) - (1)(1) = \\frac{5}{2} - 1 = \\frac{3}{2}$$\nThe inverse is:\n$$\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}^{-1} = \\frac{1}{\\frac{3}{2}} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\frac{2}{3} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\begin{bmatrix} \\frac{10}{12}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix}$$\nNow we can solve for $\\delta x^a$:\n$$\\delta x^a = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}(3) - \\frac{2}{3}(3) \\\\ -\\frac{2}{3}(3) + \\frac{4}{3}(3) \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{2} - 2 \\\\ -2 + 4 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\nSo, the components of the analysis increment are $\\delta x^a_1 = \\frac{1}{2}$ and $\\delta x^a_2 = 2$.\n\nFinally, we compute the analysis state $x^a$:\n$$x^a = x_b + \\delta x^a = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\nThe components of the analysis state are $x^a_1 = \\frac{1}{2}$ and $x^a_2 = 2$.\n\nThe final result is requested as a single row matrix in the order $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$. This gives:\n$$\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Moving beyond static snapshots, this practice delves into the dynamics of a system over time (4D-Var) and explores the critical role of observation error statistics. Set in the context of fluorescence imaging—a key technique in biomedical research—this exercise challenges you to compare the results of a standard Gaussian noise model with a more physically realistic Poisson model for photon counting. By implementing a numerical optimization to solve both cases, you will learn how to adapt the variational framework to incorporate system dynamics and non-Gaussian likelihoods, a crucial skill for accurately modeling complex biological processes .",
            "id": "3942146",
            "problem": "Consider a one-dimensional dynamic fluorescence imaging experiment where the latent fluorescence intensity state sequence $\\{x_t\\}_{t=0}^{T-1}$ evolves according to a linear time-discretized decay model. The data assimilation task is to infer the latent trajectory $\\{x_t\\}$ from observed photon counts $\\{y_t\\}$ given model and prior information.\n\nFundamental base and definitions:\n- Bayes’s rule for posterior inference: the posterior density is proportional to the product of the prior density and the likelihood function.\n- Maximum A Posteriori (MAP) estimate: the argument that maximizes the posterior density, equivalently minimizes the negative log-posterior.\n- Discrete-time linear decay model for fluorescence: $x_{t+1} = a x_t + \\xi_t$, where $a = \\exp(-k \\Delta t)$, with decay rate $k$ and time-step $\\Delta t$. Model mismatch $\\xi_t$ is modeled as zero-mean Gaussian with variance $q$.\n- Gaussian prior on the initial state: $x_0 \\sim \\mathcal{N}(m_0, p_0)$.\n- Two observation models relating the expected photon count to the latent state:\n  1. Poisson count model: $y_t \\sim \\mathrm{Poisson}(\\lambda_t)$ with $\\lambda_t = s x_t + b$, where $s$ is system sensitivity (counts per unit fluorescence), and $b$ is background counts.\n  2. Gaussian noise model: $y_t \\sim \\mathcal{N}(s x_t + b, \\sigma^2)$ with constant variance $\\sigma^2$.\n\nTasks:\n1. Starting strictly from Bayes’s rule and the definitions above, derive the MAP estimation problems under the Poisson and Gaussian observation models. Formulate each as minimizing a negative log-posterior objective over the trajectory $\\{x_t\\}$ with the physical constraint $x_t \\ge 0$ for all $t$.\n2. Design a principled variational data assimilation algorithm that computes the MAP trajectory under both models. The algorithm should incorporate the dynamical model as a quadratic regularization (soft constraint) and apply bound constraints $x_t \\ge 0$. Derive the gradient of each objective with respect to the trajectory components to enable gradient-based numerical optimization. Explicitly ensure domain validity for the Poisson case by requiring $s x_t + b  0$.\n3. For the following test suite, compute the MAP trajectories for both models, then compute the root-mean-square error (RMSE) between each MAP trajectory and the ground-truth trajectory. Report, for each test case, the float value of the difference $\\mathrm{RMSE}_{\\mathrm{Gaussian}} - \\mathrm{RMSE}_{\\mathrm{Poisson}}$.\n\nGround truth and dynamical parameters:\n- Number of frames: $T = 12$.\n- Time-step: $\\Delta t = 1$ second.\n- Decay rate: $k = 0.2$ per second, so $a = \\exp(-k \\Delta t)$.\n- Ground-truth initial state: $x_0^{\\mathrm{true}} = 2$ (arbitrary fluorescence units).\n- Ground-truth trajectory: $x_t^{\\mathrm{true}} = x_0^{\\mathrm{true}} a^t$ for $t = 0,\\dots,11$.\n- Prior mean and variance for $x_0$: $m_0 = 1.5$, $p_0 = 0.25$.\n- Model-mismatch variance: $q = 0.04$.\n\nObservation model parameters and observed photon counts for the three test cases:\n- Case 1 (high-count regime):\n  - Sensitivity: $s = 80$ counts per unit fluorescence.\n  - Background: $b = 20$ counts.\n  - Gaussian noise standard deviation: $\\sigma = 5$ counts.\n  - Observed counts list: $[174, 152, 132, 111, 96, 76, 68, 61, 55, 47, 43, 36]$.\n- Case 2 (low-count regime):\n  - Sensitivity: $s = 8$ counts per unit fluorescence.\n  - Background: $b = 1$ count.\n  - Gaussian noise standard deviation: $\\sigma = 3$ counts.\n  - Observed counts list: $[16, 13, 12, 9, 8, 7, 6, 5, 6, 4, 4, 3]$.\n- Case 3 (high background regime):\n  - Sensitivity: $s = 15$ counts per unit fluorescence.\n  - Background: $b = 40$ counts.\n  - Gaussian noise standard deviation: $\\sigma = 5$ counts.\n  - Observed counts list: $[68, 63, 59, 58, 52, 53, 49, 48, 47, 45, 44, 43]$.\n\nAlgorithmic requirements and output:\n- Implement convex optimization using gradient-based methods with bound constraints $x_t \\ge 0$ for both models. The objective must include the quadratic dynamical regularization from the model and the appropriate observation negative log-likelihood term.\n- Compute the RMSE for each model with respect to $x_t^{\\mathrm{true}}$:\n  $$\\mathrm{RMSE} = \\sqrt{\\frac{1}{T} \\sum_{t=0}^{T-1} \\left(x_t^{\\mathrm{MAP}} - x_t^{\\mathrm{true}}\\right)^2}.$$\n- Final output format: your program should produce a single line containing the three results as a comma-separated list enclosed in square brackets, where each element is the float value of $\\mathrm{RMSE}_{\\mathrm{Gaussian}} - \\mathrm{RMSE}_{\\mathrm{Poisson}}$ for the corresponding test case, rounded to six decimal places. For example: $[r_1,r_2,r_3]$.",
            "solution": "The problem is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution. It is a standard problem in variational data assimilation, also known as 4D-Var in geosciences or MAP smoothing in statistics. The physical models and mathematical framework are standard and consistent. Thus, the problem is deemed valid.\n\n### Step 1: Derivation of the MAP Objective Functions\n\nThe goal is to find the Maximum A Posteriori (MAP) estimate of the state trajectory $\\mathbf{x} = \\{x_t\\}_{t=0}^{T-1}$ given the observations $\\mathbf{y} = \\{y_t\\}_{t=0}^{T-1}$. According to Bayes's rule, the posterior probability density is:\n$$\np(\\mathbf{x} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\mathbf{x}) p(\\mathbf{x})\n$$\nwhere $p(\\mathbf{y} | \\mathbf{x})$ is the likelihood and $p(\\mathbf{x})$ is the prior. The MAP estimate is found by maximizing this posterior, which is equivalent to minimizing its negative logarithm:\n$$\nJ(\\mathbf{x}) = - \\log p(\\mathbf{x} | \\mathbf{y}) \\propto - \\log p(\\mathbf{y} | \\mathbf{x}) - \\log p(\\mathbf{x})\n$$\nWe define the objective function $J(\\mathbf{x})$ as the sum of the negative log-prior $J_{prior}(\\mathbf{x})$ and the negative log-likelihood (observation cost) $J_{obs}(\\mathbf{x})$.\n\n**1.1. Negative Log-Prior Term, $J_{prior}(\\mathbf{x})$**\n\nThe prior on the trajectory, $p(\\mathbf{x})$, is defined by the initial state prior and the dynamical model. The states form a Markov chain, so $p(\\mathbf{x}) = p(x_0) \\prod_{t=0}^{T-2} p(x_{t+1}|x_t)$.\n- The initial state is Gaussian: $x_0 \\sim \\mathcal{N}(m_0, p_0)$, so $p(x_0) = \\frac{1}{\\sqrt{2\\pi p_0}} \\exp\\left(-\\frac{(x_0 - m_0)^2}{2p_0}\\right)$.\n- The dynamical model is $x_{t+1} = a x_t + \\xi_t$ with $\\xi_t \\sim \\mathcal{N}(0, q)$. This implies the transition probability is $p(x_{t+1}|x_t) = \\mathcal{N}(x_{t+1}; a x_t, q) = \\frac{1}{\\sqrt{2\\pi q}} \\exp\\left(-\\frac{(x_{t+1} - a x_t)^2}{2q}\\right)$.\n\nThe negative log-prior is, dropping constant terms:\n$$\nJ_{prior}(\\mathbf{x}) = -\\log p(x_0) - \\sum_{t=0}^{T-2} \\log p(x_{t+1}|x_t) = \\frac{1}{2p_0}(x_0 - m_0)^2 + \\frac{1}{2q}\\sum_{t=0}^{T-2}(x_{t+1} - a x_t)^2\n$$\nThis term represents the dynamical model as a quadratic regularization, penalizing deviations from the model dynamics.\n\n**1.2. Negative Log-Likelihood Term, $J_{obs}(\\mathbf{x})$**\n\nThe observations $y_t$ are assumed to be conditionally independent given the state $x_t$. Thus, the total likelihood is $p(\\mathbf{y} | \\mathbf{x}) = \\prod_{t=0}^{T-1} p(y_t | x_t)$.\n\n**1.2.1. Gaussian Observation Model**\nThe model is $y_t \\sim \\mathcal{N}(s x_t + b, \\sigma^2)$, with probability density $p(y_t | x_t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_t - (s x_t + b))^2}{2\\sigma^2}\\right)$.\nThe negative log-likelihood, dropping constants, is:\n$$\nJ_{obs,G}(\\mathbf{x}) = \\sum_{t=0}^{T-1} \\frac{(s x_t + b - y_t)^2}{2\\sigma^2}\n$$\n\n**1.2.2. Poisson Observation Model**\nThe model is $y_t \\sim \\mathrm{Poisson}(\\lambda_t)$ with $\\lambda_t = s x_t + b$. The probability mass function is $p(y_t | x_t) = \\frac{\\lambda_t^{y_t} e^{-\\lambda_t}}{y_t!}$.\nThe negative log-likelihood, dropping the constant $\\log(y_t!)$ term, is:\n$$\nJ_{obs,P}(\\mathbf{x}) = - \\sum_{t=0}^{T-1} (y_t \\log(\\lambda_t) - \\lambda_t) = \\sum_{t=0}^{T-1} (s x_t + b - y_t \\log(s x_t + b))\n$$\nThe physical constraint $x_t \\ge 0$ for all $t$ must be enforced. Also, the argument of the logarithm must be positive: $s x_t + b > 0$. Since all provided parameters $s, b$ are positive, this condition is automatically satisfied if $x_t \\ge 0$.\n\n**1.3. Full MAP Objective Functions**\n\nCombining the prior and observation terms, we get the two objective functions to be minimized subject to $x_t \\ge 0$ for all $t=0, ..., T-1$.\n\n**Gaussian Model Objective:**\n$$\nJ_G(\\mathbf{x}) = \\frac{1}{2p_0}(x_0 - m_0)^2 + \\frac{1}{2q}\\sum_{t=0}^{T-2}(x_{t+1} - a x_t)^2 + \\frac{1}{2\\sigma^2}\\sum_{t=0}^{T-1} (s x_t + b - y_t)^2\n$$\n\n**Poisson Model Objective:**\n$$\nJ_P(\\mathbf{x}) = \\frac{1}{2p_0}(x_0 - m_0)^2 + \\frac{1}{2q}\\sum_{t=0}^{T-2}(x_{t+1} - a x_t)^2 + \\sum_{t=0}^{T-1} (s x_t + b - y_t \\log(s x_t + b))\n$$\nBoth objective functions are convex, ensuring that a gradient-based optimizer can find the unique global minimum.\n\n### Step 2: Gradients for Optimization\n\nTo use a gradient-based optimizer, we need the gradient of each objective function with respect to each component $x_k$ of the trajectory $\\mathbf{x}$. The gradient $\\nabla J(\\mathbf{x})$ has components $(\\nabla J(\\mathbf{x}))_k = \\frac{\\partial J}{\\partial x_k}$.\n\n**2.1. Gradient of the Prior Term**\nThe prior term is common to both models. Its gradient components are:\n$$\n\\frac{\\partial J_{prior}}{\\partial x_k} =\n\\begin{cases}\n\\frac{1}{p_0}(x_0 - m_0) - \\frac{a}{q}(x_1 - a x_0)  \\text{if } k=0 \\\\\n\\frac{1}{q}(x_k - a x_{k-1}) - \\frac{a}{q}(x_{k+1} - a x_k)  \\text{if } 0  k  T-1 \\\\\n\\frac{1}{q}(x_{T-1} - a x_{T-2})  \\text{if } k=T-1\n\\end{cases}\n$$\n\n**2.2. Gradient of the Observation Terms**\nThe observation cost term derivative only affects the $k$-th component of the gradient.\n\n**Gaussian Model:**\n$$\n\\frac{\\partial J_{obs,G}}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( \\frac{(s x_k + b - y_k)^2}{2\\sigma^2} \\right) = \\frac{s(s x_k + b - y_k)}{\\sigma^2}\n$$\n\n**Poisson Model:**\n$$\n\\frac{\\partial J_{obs,P}}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} (s x_k + b - y_k \\log(s x_k + b)) = s - \\frac{s y_k}{s x_k + b} = s\\left(1 - \\frac{y_k}{s x_k + b}\\right)\n$$\n\n**2.3. Full Gradients**\nThe full gradient is the sum of the prior and observation gradients: $\\nabla J(\\mathbf{x}) = \\nabla J_{prior}(\\mathbf{x}) + \\nabla J_{obs}(\\mathbf{x})$. For each component $k$:\n$(\\nabla J_G(\\mathbf{x}))_k = \\frac{\\partial J_{prior}}{\\partial x_k} + \\frac{s(s x_k + b - y_k)}{\\sigma^2}$\n$(\\nabla J_P(\\mathbf{x}))_k = \\frac{\\partial J_{prior}}{\\partial x_k} + s\\left(1 - \\frac{y_k}{s x_k + b}\\right)$\n\nThese expressions are implemented in a numerical optimization routine to find the MAP trajectories $x^{\\mathrm{MAP}}$.\n\n### Step 3: Numerical Computation and RMSE\n\nThe MAP trajectories for both models are computed for each test case using a numerical optimizer (`scipy.optimize.minimize` with the L-BFGS-B method) that supports bound constraints ($x_t \\ge 0$). The objective functions and their gradients derived above are used. The Root-Mean-Square Error (RMSE) is then calculated for each estimated trajectory against the ground-truth trajectory:\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{T} \\sum_{t=0}^{T-1} \\left(x_t^{\\mathrm{MAP}} - x_t^{\\mathrm{true}}\\right)^2}\n$$\nThe final reported value for each case is the difference $\\mathrm{RMSE}_{\\mathrm{Gaussian}} - \\mathrm{RMSE}_{\\mathrm{Poisson}}$.\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the variational data assimilation problem for all test cases.\n    \"\"\"\n\n    # Ground truth and dynamical parameters (shared across all cases)\n    T = 12\n    delta_t = 1.0\n    k = 0.2\n    a = np.exp(-k * delta_t)\n    x0_true = 2.0\n    x_true = x0_true * (a ** np.arange(T))\n    \n    # Prior and model parameters\n    m0 = 1.5\n    p0 = 0.25\n    q = 0.04\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"s\": 80.0, \"b\": 20.0, \"sigma\": 5.0,\n            \"y\": np.array([174, 152, 132, 111, 96, 76, 68, 61, 55, 47, 43, 36])\n        },\n        {\n            \"s\": 8.0, \"b\": 1.0, \"sigma\": 3.0,\n            \"y\": np.array([16, 13, 12, 9, 8, 7, 6, 5, 6, 4, 4, 3])\n        },\n        {\n            \"s\": 15.0, \"b\": 40.0, \"sigma\": 5.0,\n            \"y\": np.array([68, 63, 59, 58, 52, 53, 49, 48, 47, 45, 44, 43])\n        }\n    ]\n\n    results = []\n    \n    # Common settings for the optimizer\n    bounds = [(0, None)] * T\n\n    for case in test_cases:\n        s, b, sigma, y_obs = case[\"s\"], case[\"b\"], case[\"sigma\"], case[\"y\"]\n\n        # A physically-motivated initial guess for the optimizer\n        x_init = np.maximum(1e-6, (y_obs - b) / s)\n\n        # Solve for Gaussian model\n        res_g = minimize(\n            fun=objective_and_grad,\n            x0=x_init,\n            args=(T, a, m0, p0, q, 'gaussian', y_obs, s, b, sigma),\n            method='L-BFGS-B',\n            jac=True,\n            bounds=bounds\n        )\n        x_map_g = res_g.x\n        \n        # Solve for Poisson model\n        res_p = minimize(\n            fun=objective_and_grad,\n            x0=x_init,\n            args=(T, a, m0, p0, q, 'poisson', y_obs, s, b, sigma),\n            method='L-BFGS-B',\n            jac=True,\n            bounds=bounds\n        )\n        x_map_p = res_p.x\n\n        # Calculate RMSE for both models\n        rmse_g = np.sqrt(np.mean((x_map_g - x_true)**2))\n        rmse_p = np.sqrt(np.mean((x_map_p - x_true)**2))\n\n        # Store the difference\n        results.append(rmse_g - rmse_p)\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef objective_and_grad(x, T, a, m0, p0, q, model_type, y, s, b, sigma):\n    \"\"\"\n    Computes the objective function and its gradient for both models.\n    This function is passed to scipy.optimize.minimize.\n    \"\"\"\n    # 1. Prior term and its gradient (common to both models)\n    \n    # Objective\n    prior_obj = 0.5 * (x[0] - m0)**2 / p0 + \\\n                0.5 * np.sum((x[1:] - a * x[:-1])**2) / q\n    \n    # Gradient\n    grad_prior = np.zeros(T)\n    grad_prior[0] = (x[0] - m0) / p0 - a * (x[1] - a * x[0]) / q\n    \n    # Using array slicing for the middle part\n    dyn_term1 = (x[1:-1] - a * x[:-2]) / q\n    dyn_term2 = -a * (x[2:] - a * x[1:-1]) / q\n    grad_prior[1:-1] = dyn_term1 + dyn_term2\n    \n    grad_prior[T-1] = (x[T-1] - a * x[T-2]) / q\n\n    # 2. Observation term and its gradient (model-specific)\n    if model_type == 'gaussian':\n        # Objective\n        obs_term = s * x + b - y\n        obs_obj = 0.5 * np.sum(obs_term**2) / sigma**2\n        \n        # Gradient\n        grad_obs = s * obs_term / sigma**2\n        \n    elif model_type == 'poisson':\n        # Objective\n        lambda_t = s * x + b\n        # Add a small epsilon for numerical stability if x is at bound 0 and b is 0\n        # although problem statement guarantees b>0.\n        lambda_t = np.maximum(1e-9, lambda_t)\n        obs_obj = np.sum(lambda_t - y * np.log(lambda_t))\n        \n        # Gradient\n        grad_obs = s * (1 - y / lambda_t)\n        \n    else:\n        raise ValueError(\"Unknown model type\")\n\n    # 3. Total objective and gradient\n    total_obj = prior_obj + obs_obj\n    total_grad = grad_prior + grad_obs\n    \n    return total_obj, total_grad\n\n# solve() # The function call is commented out as this block is for explaining the solution method.\n```",
            "answer": "[-0.001642,0.063467,0.000302]"
        }
    ]
}