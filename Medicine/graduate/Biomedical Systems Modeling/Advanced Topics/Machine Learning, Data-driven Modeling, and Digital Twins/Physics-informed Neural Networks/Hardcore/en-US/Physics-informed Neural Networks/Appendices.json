{
    "hands_on_practices": [
        {
            "introduction": "The core idea of a Physics-Informed Neural Network (PINN) is to embed a physical law directly into the training process. This is achieved by penalizing the network if its output violates the governing partial differential equation (PDE). This first exercise provides a foundational look at this process by asking you to derive the PDE residual, which is the mathematical expression a PINN seeks to minimize . By working through the differentiation of a neural network ansatz for the Schrödinger equation, you will gain a concrete understanding of how physics constrains the learning process.",
            "id": "2126326",
            "problem": "A researcher is developing a Physics-Informed Neural Network (PINN) to approximate the ground state solution of the one-dimensional, time-independent Schrödinger equation for a particle of mass $m$ in a quantum harmonic oscillator potential. The governing equation is given by:\n$$-\\frac{\\hbar^2}{2m} \\frac{d^2\\psi(x)}{dx^2} + V(x)\\psi(x) = E\\psi(x)$$\nwhere $\\psi(x)$ is the wavefunction, $\\hbar$ is the reduced Planck constant, $E$ is the energy eigenvalue, and the potential is $V(x) = \\frac{1}{2}kx^2$ for a spring constant $k$.\n\nThe researcher proposes a simple neural network architecture as an ansatz for the wavefunction, denoted $\\psi_{NN}(x)$. This network has a single input neuron for the position $x$, one hidden layer with a single neuron using a hyperbolic tangent activation function, and a linear output neuron. Its mathematical form is:\n$$\\psi_{NN}(x) = w_2 \\tanh(w_1 x + b_1) + b_2$$\nwhere $w_1, b_1$ are the weight and bias for the hidden layer, and $w_2, b_2$ are the weight and bias for the output layer.\n\nA core component of the PINN's training process is the minimization of the Partial Differential Equation (PDE) residual. The residual, denoted $R(x)$, is defined as the value obtained when the neural network's approximation is substituted into the governing differential equation. That is, $R(x) = -\\frac{\\hbar^2}{2m} \\frac{d^2\\psi_{NN}}{dx^2} + V(x)\\psi_{NN} - E\\psi_{NN}$.\n\nDerive the complete analytical expression for the PDE residual $R(x)$ in terms of the input $x$, the network parameters ($w_1, b_1, w_2, b_2$), the energy $E$, and the physical constants ($m, \\hbar, k$).",
            "solution": "We are given the time-independent Schrödinger equation\n$$-\\frac{\\hbar^{2}}{2m}\\frac{d^{2}\\psi(x)}{dx^{2}}+V(x)\\psi(x)=E\\psi(x),$$\nwith $V(x)=\\frac{k}{2}x^{2}$, and the neural network ansatz\n$$\\psi_{NN}(x)=w_{2}\\tanh(w_{1}x+b_{1})+b_{2}.$$\nThe PDE residual is defined as\n$$R(x)=-\\frac{\\hbar^{2}}{2m}\\frac{d^{2}\\psi_{NN}}{dx^{2}}+V(x)\\psi_{NN}-E\\psi_{NN}.$$\n\nCompute the first derivative using the chain rule and $d(\\tanh u)/du=\\frac{1}{\\cosh^{2}u}$:\n$$\\frac{d\\psi_{NN}}{dx}=w_{2}\\cdot \\frac{d}{dx}\\tanh(w_{1}x+b_{1})=w_{2}\\cdot \\frac{1}{\\cosh^{2}(w_{1}x+b_{1})}\\cdot w_{1}=\\frac{w_{1}w_{2}}{\\cosh^{2}(w_{1}x+b_{1})}.$$\n\nCompute the second derivative using the chain rule again and $d(\\cosh^{-2}u)/du=-2\\cosh^{-3}u\\sinh u$:\n$$\\frac{d^{2}\\psi_{NN}}{dx^{2}}=w_{1}w_{2}\\cdot \\frac{d}{dx}\\big[\\cosh^{-2}(w_{1}x+b_{1})\\big]=w_{1}w_{2}\\cdot \\big[-2\\cosh^{-3}(w_{1}x+b_{1})\\sinh(w_{1}x+b_{1})\\big]\\cdot w_{1},$$\nso\n$$\\frac{d^{2}\\psi_{NN}}{dx^{2}}=-2w_{1}^{2}w_{2}\\,\\frac{\\sinh(w_{1}x+b_{1})}{\\cosh^{3}(w_{1}x+b_{1})}=-2w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}.$$\n\nSubstitute into the residual definition with $V(x)=\\frac{k}{2}x^{2}$:\n$$R(x)=-\\frac{\\hbar^{2}}{2m}\\left[-2w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}\\right]+\\frac{k}{2}x^{2}\\big[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\big]-E\\big[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\big].$$\n\nSimplifying the kinetic term gives\n$$R(x)=\\frac{\\hbar^{2}}{m}w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}+\\left(\\frac{k}{2}x^{2}-E\\right)\\left[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\right].$$\nThis is the complete analytical expression for the PDE residual in terms of $x$, the network parameters, $E$, and the physical constants.",
            "answer": "$$\\boxed{\\frac{\\hbar^{2}}{m}w_{2}w_{1}^{2}\\,\\frac{\\tanh\\!\\left(w_{1}x+b_{1}\\right)}{\\cosh^{2}\\!\\left(w_{1}x+b_{1}\\right)}+\\left(\\frac{k}{2}x^{2}-E\\right)\\left[w_{2}\\tanh\\!\\left(w_{1}x+b_{1}\\right)+b_{2}\\right]}$$"
        },
        {
            "introduction": "While the PDE residual enforces the governing physics inside the domain, a unique solution requires satisfying specific boundary conditions. This practice demonstrates how to construct loss terms that enforce these constraints, using data that might come from physical sensors. You will learn to formulate distinct loss components for Dirichlet (prescribed value) and Neumann (prescribed flux or gradient) conditions, a common scenario in biomedical modeling where different types of measurements are available at different locations . This exercise highlights the critical role of automatic differentiation in calculating the gradients needed to evaluate Neumann boundary conditions.",
            "id": "4235901",
            "problem": "A cyber-physical system digital twin for a steady-state conductive field is modeled by Laplace’s equation, derived from conservation of flux with constant conductivity and no sources. Let the physical potential be $u:\\Omega \\to \\mathbb{R}$, where $\\Omega \\subset \\mathbb{R}^{d}$ is a bounded domain with Lipschitz boundary $\\partial \\Omega$. The governing law is $-\\Delta u = 0$ in $\\Omega$, with mixed boundary conditions $u=g$ on $\\Gamma_{D} \\subset \\partial \\Omega$ and $\\partial_{n} u = h$ on $\\Gamma_{N}=\\partial \\Omega \\setminus \\Gamma_{D}$, where $g:\\Gamma_{D} \\to \\mathbb{R}$ and $h:\\Gamma_{N} \\to \\mathbb{R}$ are measured by boundary sensors, and $\\partial_{n} u$ denotes the normal derivative.\n\nYou deploy a physics-informed neural network (PINN) $u_{\\theta}:\\Omega \\to \\mathbb{R}$ with parameters $\\theta$ to serve as the digital twin surrogate. For boundary training, you collect quadrature samples on each boundary segment:\n- Dirichlet samples $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$ with associated measurements $\\{g_{i}\\}_{i=1}^{N_{D}}$ and positive quadrature weights $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$ that approximate integration over $\\Gamma_{D}$.\n- Neumann samples $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$ with measured fluxes $\\{h_{j}\\}_{j=1}^{N_{N}}$, unit outward normals $\\{n_{j}\\}_{j=1}^{N_{N}} \\subset \\mathbb{R}^{d}$, and positive quadrature weights $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$.\n\nLet $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ and $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$ be the total quadrature weights. Suppose you choose positive penalty coefficients $\\lambda_{D}$ and $\\lambda_{N}$ to balance the enforcement strength of Dirichlet and Neumann boundary conditions.\n\nStarting from the fundamental law $-\\Delta u = 0$ and the definitions of Dirichlet and Neumann boundary conditions, derive the separate boundary residual functionals on $\\Gamma_{D}$ and $\\Gamma_{N}$ that measure the squared mismatch between $u_{\\theta}$ and $g$, and between $\\nabla u_{\\theta} \\cdot n$ and $h$, respectively. Then, using quadrature approximations with the given samples and weights, construct a single composite, dimensionless boundary-only loss functional $L_{\\mathrm{bc}}(\\theta)$ whose value is a weighted sum of the two boundary residuals normalized by $W_{D}$ and $W_{N}$. Express this $L_{\\mathrm{bc}}(\\theta)$ explicitly as a closed-form analytic expression in terms of the given data $\\{x_{i}^{D},g_{i},w_{i}^{D}\\}_{i=1}^{N_{D}}$, $\\{x_{j}^{N},h_{j},n_{j},w_{j}^{N}\\}_{j=1}^{N_{N}}$, and the network $u_{\\theta}$. In your derivation, discuss how each boundary residual is enforced using automatic differentiation (AD) to obtain $\\nabla u_{\\theta}$ at boundary points.\n\nYour final answer must be the single explicit expression for $L_{\\mathrm{bc}}(\\theta)$ as a function of $\\theta$, containing only symbolic quantities and the given data. Do not include units. No rounding is required.",
            "solution": "The problem requires the derivation of a composite, dimensionless boundary-only loss functional, denoted $L_{\\mathrm{bc}}(\\theta)$, for a physics-informed neural network (PINN) surrogate $u_{\\theta}$ of a physical potential $u$. The system is governed by Laplace's equation $-\\Delta u = 0$ on a domain $\\Omega \\subset \\mathbb{R}^{d}$ with mixed Dirichlet and Neumann boundary conditions. The loss functional is constructed from discrete sensor data on the boundary.\n\nWe begin by formally defining the continuous residual functionals for each type of boundary condition. These functionals measure the extent to which the PINN surrogate $u_{\\theta}$ fails to satisfy the prescribed conditions on the boundary $\\partial \\Omega$.\n\nFirst, consider the Dirichlet boundary $\\Gamma_{D}$, where the potential is prescribed as $u=g$. The mismatch, or residual, at any point $x \\in \\Gamma_D$ is given by the difference $u_{\\theta}(x) - g(x)$. To measure the total error over this boundary segment, we integrate the square of this residual. This gives the continuous Dirichlet boundary residual functional, $R_{D}(\\theta)$:\n$$\nR_{D}(\\theta) = \\int_{\\Gamma_{D}} (u_{\\theta}(x) - g(x))^2 \\, dS\n$$\nwhere $dS$ is the surface measure on the boundary $\\partial\\Omega$.\n\nSecond, consider the Neumann boundary $\\Gamma_{N}$, where the normal derivative of the potential is prescribed as $\\partial_{n} u = h$. The normal derivative is the projection of the gradient onto the unit outward normal vector $n(x)$, i.e., $\\partial_{n} u(x) = \\nabla u(x) \\cdot n(x)$. The residual at a point $x \\in \\Gamma_N$ is the difference between the normal derivative of the PINN approximation and the prescribed function $h(x)$, which is $\\nabla u_{\\theta}(x) \\cdot n(x) - h(x)$. The total squared error over the Neumann boundary is then given by the continuous Neumann boundary residual functional, $R_{N}(\\theta)$:\n$$\nR_{N}(\\theta) = \\int_{\\Gamma_{N}} (\\nabla u_{\\theta}(x) \\cdot n(x) - h(x))^2 \\, dS\n$$\n\nIn a practical setting, we do not have the continuous functions $g$ and $h$ but rather discrete measurements at specific sensor locations. The problem provides sets of sample points and quadrature weights to approximate the integrals for $R_{D}(\\theta)$ and $R_{N}(\\theta)$.\n\nFor the Dirichlet boundary, we are given $N_D$ sample points $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$ with corresponding potential measurements $\\{g_{i}\\}_{i=1}^{N_{D}}$ (where $g_i = g(x_i^D)$) and positive quadrature weights $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$. The integral for $R_{D}(\\theta)$ is approximated by a weighted sum, which we denote as the discrete residual $\\hat{R}_{D}(\\theta)$:\n$$\n\\hat{R}_{D}(\\theta) = \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2\n$$\nThis is a numerical quadrature approximation of the integral $\\int_{\\Gamma_{D}} (u_{\\theta} - g)^2 \\, dS$.\n\nFor the Neumann boundary, we have $N_N$ sample points $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$ with flux measurements $\\{h_{j}\\}_{j=1}^{N_{N}}$, unit outward normals $\\{n_{j}\\}_{j=1}^{N_{N}}$, and positive quadrature weights $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$. The crucial calculation here is the term $\\nabla u_{\\theta}(x_j^N)$, the gradient of the neural network output with respect to its spatial inputs. This is computed using automatic differentiation (AD). The neural network $u_{\\theta}$ is a composition of differentiable elementary functions (e.g., affine transformations and activation functions). Its derivatives with respect to its inputs can therefore be computed algorithmically to machine precision. Deep learning frameworks provide this functionality natively, typically via reverse-mode AD (backpropagation), allowing for efficient evaluation of $\\nabla u_{\\theta}$ at any point $x_j^N$ required for the loss calculation.\n\nUsing AD to obtain $\\nabla u_{\\theta}(x_{j}^{N})$, we can then approximate the integral for $R_{N}(\\theta)$ with the discrete Neumann residual $\\hat{R}_{N}(\\theta)$:\n$$\n\\hat{R}_{N}(\\theta) = \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\n\nThe problem asks for a single composite loss functional $L_{\\mathrm{bc}}(\\theta)$ that is a weighted sum of the two boundary residuals, normalized by their respective total quadrature weights, $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ and $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$. This normalization turns each discrete residual into a mean squared error approximation. We are also given positive penalty coefficients $\\lambda_{D}$ and $\\lambda_{N}$ to balance the relative importance of enforcing each boundary condition.\nThe normalized Dirichlet loss term is $\\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}}$.\nThe normalized Neumann loss term is $\\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}$.\n\nThe problem specifies that the final loss functional $L_{\\mathrm{bc}}(\\theta)$ should be dimensionless. This typically implies that the governing equations and boundary conditions have been non-dimensionalized beforehand using characteristic scales for length, potential, etc. Under this standard assumption, all quantities in the loss function are dimensionless, and the penalty coefficients $\\lambda_D$ and $\\lambda_N$ are also dimensionless factors used for tuning the training process.\n\nCombining these components, the composite boundary loss functional is the sum of the normalized and weighted discrete residuals:\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}} + \\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}\n$$\nSubstituting the expressions for $\\hat{R}_{D}(\\theta)$, $\\hat{R}_{N}(\\theta)$, $W_{D}$, and $W_{N}$:\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\left( \\frac{\\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2}{\\sum_{i=1}^{N_{D}} w_{i}^{D}} \\right) + \\lambda_{N} \\left( \\frac{\\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}{\\sum_{j=1}^{N_{N}} w_{j}^{N}} \\right)\n$$\nThis can be written more concisely using the given definitions $W_{D}$ and $W_{N}$:\n$$\nL_{\\mathrm{bc}}(\\theta) = \\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\nThis expression represents the final, closed-form analytic form for the boundary-only loss functional $L_{\\mathrm{bc}}(\\theta)$ as a function of the network parameters $\\theta$ and the given data. Minimizing this loss functional with respect to $\\theta$ trains the network to satisfy the boundary conditions in a least-squares sense.",
            "answer": "$$\n\\boxed{\\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}\n$$"
        },
        {
            "introduction": "Perhaps one of the most powerful applications of PINNs is their ability to solve inverse problems: discovering unknown physical parameters directly from observational data. This shifts the paradigm from merely solving a known model to using data to find the model itself. This exercise guides you through setting up a PINN for such a discovery task, where the coefficients of a PDE, such as $c_1$ and $c_2$, are treated as trainable parameters alongside the network's weights . This approach transforms the neural network into an instrument for uncovering the underlying physics of a system, a technique with profound implications for modeling complex biological phenomena.",
            "id": "2126328",
            "problem": "A researcher is studying an unknown physical process governed by a one-dimensional Partial Differential Equation (PDE). From experiments, they have collected a sparse set of $N_u$ measurements of a quantity $u(x, t)$, given as data points $\\{ (x_i, t_i, u_i) \\}_{i=1}^{N_u}$. The researcher hypothesizes that the underlying PDE is a form of the Burgers' equation with unknown coefficients:\n$$\nu_t + c_1 u u_x - c_2 u_{xx} = 0\n$$\nwhere $u_t = \\frac{\\partial u}{\\partial t}$, $u_x = \\frac{\\partial u}{\\partial x}$, and $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$. The real-valued coefficients $c_1$ and $c_2$ are to be discovered from the data.\n\nTo solve this inverse problem, the researcher employs a Physics-Informed Neural Network (PINN). The solution $u(x, t)$ is approximated by a neural network $\\mathcal{N}(x, t; \\boldsymbol{\\theta})$, where $\\boldsymbol{\\theta}$ represents the trainable weights and biases of the network. The unknown coefficients $c_1$ and $c_2$ are also treated as trainable parameters.\n\nThe training process minimizes a total loss function, $L = L_{data} + L_{PDE}$, with respect to $\\boldsymbol{\\theta}$, $c_1$, and $c_2$. The data-fidelity loss, $L_{data}$, is the mean squared error between the network's predictions and the measurements:\n$$\nL_{data} = \\frac{1}{N_u} \\sum_{i=1}^{N_u} \\left| \\mathcal{N}(x_i, t_i; \\boldsymbol{\\theta}) - u_i \\right|^2\n$$\nThe physics-informed loss, $L_{PDE}$, enforces the structure of the hypothesized PDE. It is defined as the mean squared error of the PDE residual, evaluated at a set of $N_f$ collocation points $\\{ (x_j^f, t_j^f) \\}_{j=1}^{N_f}$ distributed throughout the domain. All partial derivatives of $\\mathcal{N}$ are computed using automatic differentiation.\n\nWhich of the following expressions correctly defines the physics-informed loss term, $L_{PDE}$? For brevity, the notation $\\mathcal{N}_j$ represents $\\mathcal{N}(x_j^f, t_j^f; \\boldsymbol{\\theta})$, and the partial derivatives are evaluated at the collocation point $(x_j^f, t_j^f)$.\n\nA. $\\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| \\frac{\\partial \\mathcal{N}}{\\partial t} + c_1 \\mathcal{N}_j \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2} \\right|^2$\n\nB. $\\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| \\frac{\\partial \\mathcal{N}}{\\partial t} + \\frac{\\partial (c_1 \\mathcal{N}_j)}{\\partial x} - \\frac{\\partial^2 (c_2 \\mathcal{N}_j)}{\\partial x^2} \\right|^2$\n\nC. $\\frac{1}{N_u} \\sum_{i=1}^{N_u} \\left| \\frac{\\partial \\mathcal{N}}{\\partial t} + c_1 u_i \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2} \\right|^2$\n\nD. $\\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left( \\frac{\\partial \\mathcal{N}}{\\partial t} + c_1 \\mathcal{N}_j \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2} \\right)$\n\nE. $\\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| \\mathcal{N}_j - u_j \\right|^2 + \\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| c_1 \\mathcal{N}_j \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2} \\right|^2$",
            "solution": "The goal is to construct the physics-informed loss term, $L_{PDE}$, for the discovery of the parameters $c_1$ and $c_2$ in the hypothesized PDE:\n$$\nu_t + c_1 u u_x - c_2 u_{xx} = 0\n$$\n\nStep 1: Define the PDE residual.\nThe residual of the PDE, denoted as $f(x, t)$, is the expression that should equal zero if $u(x, t)$ is a true solution.\n$$\nf(x, t) = u_t + c_1 u u_x - c_2 u_{xx}\n$$\n\nStep 2: Approximate the solution and its derivatives using the PINN.\nThe Physics-Informed Neural Network (PINN) framework approximates the unknown solution $u(x, t)$ with the output of a neural network, $\\mathcal{N}(x, t; \\boldsymbol{\\theta})$. The key feature of PINNs is the use of automatic differentiation to compute the necessary partial derivatives of the network's output with respect to its inputs.\nTherefore, we have the following approximations:\n-   $u(x, t) \\approx \\mathcal{N}(x, t; \\boldsymbol{\\theta})$\n-   $u_t(x, t) \\approx \\frac{\\partial \\mathcal{N}}{\\partial t}(x, t; \\boldsymbol{\\theta})$\n-   $u_x(x, t) \\approx \\frac{\\partial \\mathcal{N}}{\\partial x}(x, t; \\boldsymbol{\\theta})$\n-   $u_{xx}(x, t) \\approx \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2}(x, t; \\boldsymbol{\\theta})$\n\nStep 3: Formulate the residual for the PINN.\nBy substituting these approximations into the PDE residual expression, we obtain the residual as a function of the network and the unknown parameters $c_1, c_2$.\n$$\nf(x, t; \\boldsymbol{\\theta}, c_1, c_2) \\approx \\frac{\\partial \\mathcal{N}}{\\partial t} + c_1 \\mathcal{N} \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2}\n$$\nNote that in the nonlinear term $u u_x$, both instances of $u$ must be replaced by the same network approximation $\\mathcal{N}$.\n\nStep 4: Define the physics-informed loss $L_{PDE}$.\nThe physics-informed loss is designed to enforce the PDE by minimizing the magnitude of this residual. This is typically done by minimizing the mean squared error of the residual evaluated over a set of $N_f$ collocation points $\\{ (x_j^f, t_j^f) \\}_{j=1}^{N_f}$. For each point $j$, the squared residual is:\n$$\n\\left| \\frac{\\partial \\mathcal{N}}{\\partial t}\\bigg|_{(x_j^f, t_j^f)} + c_1 \\mathcal{N}(x_j^f, t_j^f) \\frac{\\partial \\mathcal{N}}{\\partial x}\\bigg|_{(x_j^f, t_j^f)} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2}\\bigg|_{(x_j^f, t_j^f)} \\right|^2\n$$\nThe mean squared error is the average of these values over all $N_f$ collocation points:\n$$\nL_{PDE} = \\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| \\frac{\\partial \\mathcal{N}}{\\partial t}\\bigg|_{(x_j^f, t_j^f)} + c_1 \\mathcal{N}(x_j^f, t_j^f) \\frac{\\partial \\mathcal{N}}{\\partial x}\\bigg|_{(x_j^f, t_j^f)} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2}\\bigg|_{(x_j^f, t_j^f)} \\right|^2\n$$\nUsing the shorthand notation provided in the problem, this is:\n$$\nL_{PDE} = \\frac{1}{N_f} \\sum_{j=1}^{N_f} \\left| \\frac{\\partial \\mathcal{N}}{\\partial t} + c_1 \\mathcal{N}_j \\frac{\\partial \\mathcal{N}}{\\partial x} - c_2 \\frac{\\partial^2 \\mathcal{N}}{\\partial x^2} \\right|^2\n$$\nThis expression matches option A.\n\nStep 5: Analyze the incorrect options.\n-   **Option B:** It incorrectly places the coefficients $c_1$ and $c_2$ inside the derivative operators. Since $c_1$ and $c_2$ are constants with respect to $x$ and $t$, this formulation is equivalent to $\\frac{\\partial (c_1 \\mathcal{N}_j)}{\\partial x} = c_1 \\frac{\\partial \\mathcal{N}_j}{\\partial x}$ and $\\frac{\\partial^2 (c_2 \\mathcal{N}_j)}{\\partial x^2} = c_2 \\frac{\\partial^2 \\mathcal{N}_j}{\\partial x^2}$. However, the term $c_1 u u_x$ becomes $\\frac{\\partial(c_1 \\mathcal{N})}{\\partial x}$ in the formulation, which is wrong. The term should be $c_1 \\mathcal{N} \\frac{\\partial \\mathcal{N}}{\\partial x}$.\n-   **Option C:** This expression has two fundamental errors. First, it evaluates the PDE residual on the sparse set of $N_u$ data points, not the larger set of $N_f$ collocation points. The purpose of collocation points is to enforce the PDE over the entire domain, not just where data is available. Second, it incorrectly uses the measured data value $u_i$ within the residual expression ($c_1 u_i \\frac{\\partial \\mathcal{N}}{\\partial x}$). The residual must be expressed solely in terms of the network approximation $\\mathcal{N}$ and its derivatives to maintain mathematical consistency.\n-   **Option D:** This expression sums the signed residuals, rather than a non-negative quantity like the squared error. A loss function must be non-negative and have a minimum at the desired solution. A sum of signed values can be zero even if individual residuals are large and non-zero (e.g., $+100$ and $-100$ cancel out), which would not lead to a correct solution.\n-   **Option E:** This expression incorrectly mixes components. It includes a term $|\\mathcal{N}_j - u_j|^2$, which looks like a data loss term but is evaluated at collocation points where the true values $u_j$ are unknown. Furthermore, the second term is an incomplete residual, as it is missing the time derivative term $\\frac{\\partial \\mathcal{N}}{\\partial t}$. This corresponds to a different physical model.\n\nTherefore, the only correct expression for the physics-informed loss $L_{PDE}$ is the one given in option A.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}