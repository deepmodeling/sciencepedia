{
    "hands_on_practices": [
        {
            "introduction": "物理信息神经网络（PINN）的核心在于将控制方程本身编码到损失函数中。第一个练习将带你深入这一核心概念。我们将通过一个源于量子力学的思想实验，手动推导一个简单神经网络的偏微分方程（PDE）残差。这个过程能帮助你具体理解“残差”的含义，以及它如何将神经网络的参数与物理定律直接联系起来，为后续更复杂的应用打下坚实基础。",
            "id": "2126326",
            "problem": "一位研究人员正在开发一个物理信息神经网络 (PINN)，用以近似求解质量为 $m$ 的粒子在量子谐振子势中的一维、不含时薛定谔方程的基态解。其控制方程如下：\n$$-\\frac{\\hbar^2}{2m} \\frac{d^2\\psi(x)}{dx^2} + V(x)\\psi(x) = E\\psi(x)$$\n其中 $\\psi(x)$ 是波函数，$\\hbar$ 是约化普朗克常数，$E$ 是能量本征值，势为 $V(x) = \\frac{1}{2}kx^2$，其中 $k$ 为劲度系数。\n\n该研究人员提出了一个简单的神经网络结构作为波函数的拟设，记为 $\\psi_{NN}(x)$。该网络有一个用于位置 $x$ 的输入神经元，一个使用双曲正切激活函数的单神经元隐藏层，以及一个线性输出神经元。其数学形式为：\n$$\\psi_{NN}(x) = w_2 \\tanh(w_1 x + b_1) + b_2$$\n其中 $w_1, b_1$ 是隐藏层的权重和偏置，$w_2, b_2$ 是输出层的权重和偏置。\n\nPINN 训练过程的一个核心部分是最小化偏微分方程 (PDE) 残差。残差记为 $R(x)$，其定义为将神经网络的近似解代入控制微分方程后得到的值。即 $R(x) = -\\frac{\\hbar^2}{2m} \\frac{d^2\\psi_{NN}}{dx^2} + V(x)\\psi_{NN} - E\\psi_{NN}$。\n\n请推导 PDE 残差 $R(x)$ 的完整解析表达式，该表达式应使用输入 $x$、网络参数 ($w_1, b_1, w_2, b_2$)、能量 $E$ 和物理常数 ($m, \\hbar, k$) 来表示。",
            "solution": "给定不含时的薛定谔方程\n$$-\\frac{\\hbar^{2}}{2m}\\frac{d^{2}\\psi(x)}{dx^{2}}+V(x)\\psi(x)=E\\psi(x),$$\n其中 $V(x)=\\frac{k}{2}x^{2}$，以及神经网络拟设\n$$\\psi_{NN}(x)=w_{2}\\tanh(w_{1}x+b_{1})+b_{2}.$$\nPDE 残差定义为\n$$R(x)=-\\frac{\\hbar^{2}}{2m}\\frac{d^{2}\\psi_{NN}}{dx^{2}}+V(x)\\psi_{NN}-E\\psi_{NN}.$$\n\n使用链式法则和 $d(\\tanh u)/du=\\frac{1}{\\cosh^{2}u}$ 计算一阶导数：\n$$\\frac{d\\psi_{NN}}{dx}=w_{2}\\cdot \\frac{d}{dx}\\tanh(w_{1}x+b_{1})=w_{2}\\cdot \\frac{1}{\\cosh^{2}(w_{1}x+b_{1})}\\cdot w_{1}=\\frac{w_{1}w_{2}}{\\cosh^{2}(w_{1}x+b_{1})}.$$\n\n再次使用链式法则和 $d(\\cosh^{-2}u)/du=-2\\cosh^{-3}u\\sinh u$ 计算二阶导数：\n$$\\frac{d^{2}\\psi_{NN}}{dx^{2}}=w_{1}w_{2}\\cdot \\frac{d}{dx}\\big[\\cosh^{-2}(w_{1}x+b_{1})\\big]=w_{1}w_{2}\\cdot \\big[-2\\cosh^{-3}(w_{1}x+b_{1})\\sinh(w_{1}x+b_{1})\\big]\\cdot w_{1},$$\n所以\n$$\\frac{d^{2}\\psi_{NN}}{dx^{2}}=-2w_{1}^{2}w_{2}\\,\\frac{\\sinh(w_{1}x+b_{1})}{\\cosh^{3}(w_{1}x+b_{1})}=-2w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}.$$\n\n将 $V(x)=\\frac{k}{2}x^{2}$ 代入残差定义中：\n$$R(x)=-\\frac{\\hbar^{2}}{2m}\\left[-2w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}\\right]+\\frac{k}{2}x^{2}\\big[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\big]-E\\big[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\big].$$\n\n化简动能项可得\n$$R(x)=\\frac{\\hbar^{2}}{m}w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}+\\left(\\frac{k}{2}x^{2}-E\\right)\\left[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\right].$$\n这就是用 $x$、网络参数、$E$ 和物理常数表示的 PDE 残差的完整解析表达式。",
            "answer": "$$\\boxed{\\frac{\\hbar^{2}}{m}w_{2}w_{1}^{2}\\,\\frac{\\tanh\\!\\left(w_{1}x+b_{1}\\right)}{\\cosh^{2}\\!\\left(w_{1}x+b_{1}\\right)}+\\left(\\frac{k}{2}x^{2}-E\\right)\\left[w_{2}\\tanh\\!\\left(w_{1}x+b_{1}\\right)+b_{2}\\right]}$$"
        },
        {
            "introduction": "一个定义明确的物理问题不仅包含控制方程，还必须包含边界条件。本练习将重点转向如何将这些至关重要的边界约束整合到PINN的损失函数中。我们将以经典的拉普拉斯方程为例，处理包含狄利克雷（Dirichlet）和诺依曼（Neumann）边界条件的混合边界问题。通过构建相应的损失项，你将掌握处理不同类型边界数据的方法，并深刻体会自动微分在计算诺依曼边界所需的法向导数时的关键作用。",
            "id": "4235901",
            "problem": "一个稳态导电场的信息物理系统数字孪生由拉普拉斯方程建模，该方程由通量守恒、电导率恒定且无源的条件推导得出。设物理势为 $u:\\Omega \\to \\mathbb{R}$，其中 $\\Omega \\subset \\mathbb{R}^{d}$ 是一个具有 Lipschitz 边界 $\\partial \\Omega$ 的有界域。控制方程为在 $\\Omega$ 内 $-\\Delta u = 0$，并带有混合边界条件：在 $\\Gamma_{D} \\subset \\partial \\Omega$ 上 $u=g$，以及在 $\\Gamma_{N}=\\partial \\Omega \\setminus \\Gamma_{D}$ 上 $\\partial_{n} u = h$。其中，$g:\\Gamma_{D} \\to \\mathbb{R}$ 和 $h:\\Gamma_{N} \\to \\mathbb{R}$ 由边界传感器测量，$\\partial_{n} u$ 表示法向导数。\n\n您部署了一个带有参数 $\\theta$ 的物理知识神经网络 (PINN) $u_{\\theta}:\\Omega \\to \\mathbb{R}$，作为数字孪生代理模型。为了进行边界训练，您在每个边界段上收集求积样本：\n- 狄利克雷样本 $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$，及其关联的测量值 $\\{g_{i}\\}_{i=1}^{N_{D}}$ 和用于近似 $\\Gamma_{D}$ 上积分的正求积权重 $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$。\n- 诺伊曼样本 $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$，及其测量的通量 $\\{h_{j}\\}_{j=1}^{N_{N}}$、单位外法向量 $\\{n_{j}\\}_{j=1}^{N_{N}} \\subset \\mathbb{R}^{d}$ 和正求积权重 $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$。\n\n令 $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ 和 $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$ 为总求积权重。假设您选择正惩罚系数 $\\lambda_{D}$ 和 $\\lambda_{N}$ 来平衡狄利克雷和诺伊曼边界条件的施加强度。\n\n从基本定律 $-\\Delta u = 0$ 以及狄利克雷和诺伊曼边界条件的定义出发，推导在 $\\Gamma_{D}$ 和 $\\Gamma_{N}$ 上各自的边界残差泛函。这些泛函分别衡量 $u_{\\theta}$ 与 $g$ 之间以及 $\\nabla u_{\\theta} \\cdot n$ 与 $h$ 之间的平方不匹配程度。然后，利用给定的样本和权重进行求积近似，构建一个单一的、复合的、无量纲的、仅含边界项的损失泛函 $L_{\\mathrm{bc}}(\\theta)$，其值为两个由 $W_{D}$ 和 $W_{N}$ 归一化的边界残差的加权和。将此 $L_{\\mathrm{bc}}(\\theta)$ 明确地表示为一个闭式解析表达式，该表达式使用给定的数据 $\\{x_{i}^{D},g_{i},w_{i}^{D}\\}_{i=1}^{N_{D}}$、$\\{x_{j}^{N},h_{j},n_{j},w_{j}^{N}\\}_{j=1}^{N_{N}}$ 和网络 $u_{\\theta}$ 来表示。在您的推导中，讨论如何使用自动微分 (AD) 来获取边界点上的 $\\nabla u_{\\theta}$，从而施加每个边界残差。\n\n您的最终答案必须是 $L_{\\mathrm{bc}}(\\theta)$ 作为 $\\theta$ 的函数的单一显式表达式，仅包含符号量和给定数据。不要包含单位。无需四舍五入。",
            "solution": "该问题要求为一个物理势 $u$ 的物理知识神经网络 (PINN) 代理模型 $u_{\\theta}$ 推导一个复合的、无量纲的、仅含边界项的损失泛函，记作 $L_{\\mathrm{bc}}(\\theta)$。该系统由定义在域 $\\Omega \\subset \\mathbb{R}^{d}$ 上的拉普拉斯方程 $-\\Delta u = 0$ 控制，并带有混合的狄利克雷和诺伊曼边界条件。该损失泛函是根据边界上的离散传感器数据构建的。\n\n我们首先为每种类型的边界条件正式定义连续残差泛函。这些泛函衡量 PINN 代理模型 $u_{\\theta}$ 未能满足边界 $\\partial \\Omega$ 上规定条件的程度。\n\n首先，考虑狄利克雷边界 $\\Gamma_{D}$，在此边界上势被规定为 $u=g$。在任意点 $x \\in \\Gamma_D$ 的不匹配，或称残差，由差值 $u_{\\theta}(x) - g(x)$ 给出。为了衡量该边界段上的总误差，我们对该残差的平方进行积分。这得到了连续的狄利克雷边界残差泛函 $R_{D}(\\theta)$：\n$$\nR_{D}(\\theta) = \\int_{\\Gamma_{D}} (u_{\\theta}(x) - g(x))^2 \\, dS\n$$\n其中 $dS$ 是边界 $\\partial\\Omega$ 上的表面测度。\n\n其次，考虑诺伊曼边界 $\\Gamma_{N}$，在此边界上势的法向导数被规定为 $\\partial_{n} u = h$。法向导数是梯度在单位外法向量 $n(x)$ 上的投影，即 $\\partial_{n} u(x) = \\nabla u(x) \\cdot n(x)$。在点 $x \\in \\Gamma_N$ 的残差是 PINN 近似的法向导数与规定函数 $h(x)$ 之间的差值，即 $\\nabla u_{\\theta}(x) \\cdot n(x) - h(x)$。诺伊曼边界上的总平方误差则由连续的诺伊曼边界残差泛函 $R_{N}(\\theta)$ 给出：\n$$\nR_{N}(\\theta) = \\int_{\\Gamma_{N}} (\\nabla u_{\\theta}(x) \\cdot n(x) - h(x))^2 \\, dS\n$$\n\n在实际应用中，我们没有连续函数 $g$ 和 $h$，而是在特定传感器位置上的离散测量值。问题提供了样本点集和求积权重，用以近似 $R_{D}(\\theta)$ 和 $R_{N}(\\theta)$ 的积分。\n\n对于狄利克雷边界，我们有 $N_D$ 个样本点 $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$，以及相应的势测量值 $\\{g_{i}\\}_{i=1}^{N_{D}}$ (其中 $g_i = g(x_i^D)$) 和正求积权重 $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$。$R_{D}(\\theta)$ 的积分由一个加权和来近似，我们将其表示为离散残差 $\\hat{R}_{D}(\\theta)$：\n$$\n\\hat{R}_{D}(\\theta) = \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2\n$$\n这是积分 $\\int_{\\Gamma_{D}} (u_{\\theta} - g)^2 \\, dS$ 的一个数值求积近似。\n\n对于诺伊曼边界，我们有 $N_N$ 个样本点 $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$，以及通量测量值 $\\{h_{j}\\}_{j=1}^{N_{N}}$、单位外法向量 $\\{n_{j}\\}_{j=1}^{N_{N}}$ 和正求积权重 $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$。这里的关键计算是项 $\\nabla u_{\\theta}(x_j^N)$，即神经网络输出关于其空间输入的梯度。这通过使用自动微分 (AD) 来计算。神经网络 $u_{\\theta}$ 是可微基本函数（例如，仿射变换和激活函数）的复合。因此，其关于输入的导数可以通过算法计算到机器精度。深度学习框架原生提供此功能，通常通过反向模式 AD（反向传播）实现，从而可以在计算损失时高效地求出任意点 $x_j^N$ 处的 $\\nabla u_{\\theta}$。\n\n使用 AD 获得 $\\nabla u_{\\theta}(x_{j}^{N})$ 后，我们可以用离散诺伊曼残差 $\\hat{R}_{N}(\\theta)$ 来近似 $R_{N}(\\theta)$ 的积分：\n$$\n\\hat{R}_{N}(\\theta) = \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\n\n问题要求一个单一的复合损失泛函 $L_{\\mathrm{bc}}(\\theta)$，它是两个边界残差的加权和，并由它们各自的总求积权重 $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ 和 $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$ 进行归一化。这种归一化将每个离散残差转化为一个均方误差近似。我们还获得了正惩罚系数 $\\lambda_{D}$ 和 $\\lambda_{N}$，以平衡施加每个边界条件的相对重要性。\n归一化的狄利克雷损失项是 $\\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}}$。\n归一化的诺伊曼损失项是 $\\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}$。\n\n问题指明最终的损失泛函 $L_{\\mathrm{bc}}(\\theta)$ 应该是无量纲的。这通常意味着控制方程和边界条件已经事先使用长度、势等的特征尺度进行了无量纲化处理。在这一标准假设下，损失函数中的所有量都是无量纲的，惩罚系数 $\\lambda_D$ 和 $\\lambda_N$ 也是用于调整训练过程的无量纲因子。\n\n结合这些分量，复合边界损失泛函是归一化和加权的离散残差之和：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}} + \\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}\n$$\n代入 $\\hat{R}_{D}(\\theta)$、$\\hat{R}_{N}(\\theta)$、$W_{D}$ 和 $W_{N}$ 的表达式：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\left( \\frac{\\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2}{\\sum_{i=1}^{N_{D}} w_{i}^{D}} \\right) + \\lambda_{N} \\left( \\frac{\\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}{\\sum_{j=1}^{N_{N}} w_{j}^{N}} \\right)\n$$\n使用给定的定义 $W_{D}$ 和 $W_{N}$，这可以更简洁地写成：\n$$\nL_{\\mathrm{bc}}(\\theta) = \\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\n该表达式代表了仅含边界项的损失泛函 $L_{\\mathrm{bc}}(\\theta)$ 的最终闭式解析形式，它是网络参数 $\\theta$ 和给定数据的函数。通过最小化此损失泛函关于 $\\theta$ 的值，可以训练网络在最小二乘意义上满足边界条件。",
            "answer": "$$\n\\boxed{\\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}\n$$"
        },
        {
            "introduction": "这个最终练习展示了PINN框架的真正威力及其在复杂系统中的可扩展性，我们将挑战流体动力学的基石——纳维-斯托克斯（Navier-Stokes）方程。你将为这个复杂的非线性耦合方程组构建物理损失函数。这个过程不仅巩固了前面学到的概念，更凸显了在处理现实世界工程问题时，自动微分框架作为一种不可或缺的工具，其处理复杂导数计算的强大能力。",
            "id": "4235910",
            "problem": "一个信息物理系统中平面流体子系统的数字孪生被建模为不可压缩的牛顿流，由二维 Navier-Stokes 方程控制。一个物理信息神经网络 (PINN) 被用作代理模型，对于时空输入 $(x,y,t)$，它输出对应于速度分量和压力的场 $(u(x,y,t), v(x,y,t), p(x,y,t))$。目标是从第一性原理出发，推导一个强制执行动量方程和不可压缩性约束的损失函数，并实现一个程序，使用自动微分和链式法则来评估指定测试用例的该损失。所有变量都已无量纲化，因此本问题不使用物理单位。\n\n从基本守恒原理出发，不可压缩流满足质量守恒（不可压缩性）和牛顿第二定律的动量平衡。对于具有恒定运动粘度 $\\nu$ 和单位密度的牛顿流体，二维不可压缩 Navier-Stokes 方程为\n$$\n\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y} + \\frac{\\partial p}{\\partial x} - \\nu \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = 0,\n$$\n$$\n\\frac{\\partial v}{\\partial t} + u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y} + \\frac{\\partial p}{\\partial y} - \\nu \\left(\\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2}\\right) = 0,\n$$\n$$\n\\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} = 0.\n$$\n物理信息神经网络用参数化函数 $(\\hat u,\\hat v,\\hat p)$ 来近似 $(u,v,p)$，并通过在配置点上惩罚这些方程的残差来强制执行物理定律。所构建的损失函数必须至少包括动量方程和不可压缩性约束的残差平方。\n\n定义一个具有双曲正切激活函数、宽度为 $H$ 的单隐藏层神经网络，将 $(x,y,t)$ 映射到 $(\\hat u,\\hat v,\\hat p)$：\n- 隐藏层预激活 $z \\in \\mathbb{R}^H$：$z = W_1 s + b_1$，其中 $s = [x,y,t]^\\top$，$W_1 \\in \\mathbb{R}^{H \\times 3}$，$b_1 \\in \\mathbb{R}^H$。\n- 隐藏层激活 $h \\in \\mathbb{R}^H$：$h = \\tanh(z)$。\n- 输出 $o \\in \\mathbb{R}^3$：$o = W_2 h + b_2$，其中 $W_2 \\in \\mathbb{R}^{3 \\times H}$，$b_2 \\in \\mathbb{R}^3$。\n- 将 $(\\hat u,\\hat v,\\hat p)$ 解释为 $(o_1,o_2,o_3)$。\n\n使用自动微分（应用链式法则），计算在配置点处关于 $(x,y,t)$ 所需的一阶和二阶导数。对于 $\\tanh$ 激活函数，使用 $\\tanh'(z) = 1 - \\tanh^2(z)$ 和 $\\tanh''(z) = -2 \\tanh(z) \\left(1 - \\tanh^2(z)\\right)$。\n\n在一组 $N$ 个配置点 $\\{(x_i,y_i,t_i)\\}_{i=1}^N$ 上构建内部物理损失，作为残差平方的均值：\n$$\n\\mathcal{L}_{\\text{int}} = \\frac{1}{N} \\sum_{i=1}^N \\left( r_u(x_i,y_i,t_i)^2 + r_v(x_i,y_i,t_i)^2 + r_c(x_i,y_i,t_i)^2 \\right),\n$$\n其中\n$$\nr_u = \\hat u_t + \\hat u \\hat u_x + \\hat v \\hat u_y + \\hat p_x - \\nu \\left(\\hat u_{xx} + \\hat u_{yy}\\right), \\quad\nr_v = \\hat v_t + \\hat u \\hat v_x + \\hat v \\hat v_y + \\hat p_y - \\nu \\left(\\hat v_{xx} + \\hat v_{yy}\\right),\n$$\n$$\nr_c = \\hat u_x + \\hat v_y.\n$$\n解释如何在该架构内通过自动微分计算 $\\nabla \\cdot \\hat{\\boldsymbol{v}} = \\frac{\\partial \\hat{u}}{\\partial x} + \\frac{\\partial \\hat{v}}{\\partial y}$。\n\n您的程序必须实现上述网络，并为以下每个测试用例计算 $\\mathcal{L}_{\\text{int}}$。对于所有情况，使用隐藏层宽度 $H=3$以及指定的权重、偏置、粘度和配置点。程序应生成单行输出，其中包含三个损失值，格式为用方括号括起来的逗号分隔列表，每个值精确到小数点后六位。\n\n测试用例 1 (一般粘度):\n- $\\nu = 0.01$。\n- $W_1 = \\begin{bmatrix} 0.5  -0.3  0.2 \\\\ -0.4  0.7  -0.1 \\\\ 0.25  0.1  -0.5 \\end{bmatrix}$，$b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$。\n- $W_2 = \\begin{bmatrix} 0.8  -0.5  0.3 \\\\ -0.6  0.4  0.2 \\\\ 0.2  -0.1  0.7 \\end{bmatrix}$，$b_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$。\n- 配置点 $(x,y,t)$:\n  - $(0.2, 0.3, 0.1)$，\n  - $(0.7, 0.5, 0.4)$，\n  - $(0.9, 0.1, 0.9)$，\n  - $(0.0, 0.0, 0.0)$。\n\n测试用例 2 (无粘性极限):\n- $\\nu = 0.0$。\n- 使用与测试用例 1 中相同的 $W_1$、$b_1$、$W_2$、$b_2$ 和配置点。\n\n测试用例 3 (平凡零场):\n- $\\nu = 0.05$。\n- $W_1 = \\mathbf{0}_{3 \\times 3}$，$b_1 = \\mathbf{0}_{3}$。\n- $W_2 = \\mathbf{0}_{3 \\times 3}$，$b_2 = \\mathbf{0}_{3}$。\n- 配置点 $(x,y,t)$:\n  - $(0.1, 0.2, 0.3)$，\n  - $(0.4, 0.4, 0.4)$，\n  - $(0.9, 0.9, 0.9)$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按测试用例 1、测试用例 2 和测试用例 3 的顺序排列的结果，格式为用方括号括起来的逗号分隔列表，每个值精确到小数点后六位（例如，$[0.123456,0.000001,2.500000]$）。",
            "solution": "该问题要求推导并实现一个用于二维不可压缩流体流动的神经网络代理模型的物理信息损失函数。控制方程为 Navier-Stokes 方程。代理模型是一个单隐藏层神经网络。我们将首先验证问题陈述。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n- **控制方程**：针对单位密度和恒定运动粘度 $\\nu$ 的牛顿流体的二维不可压缩 Navier-Stokes 方程。\n  - x-动量: $\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y} + \\frac{\\partial p}{\\partial x} - \\nu \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = 0$\n  - y-动量: $\\frac{\\partial v}{\\partial t} + u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y} + \\frac{\\partial p}{\\partial y} - \\nu \\left(\\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2}\\right) = 0$\n  - 不可压缩性: $\\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} = 0$\n- **神经网络架构**：一个隐藏层宽度为 $H$ 的单隐藏层网络，将输入 $s=[x,y,t]^\\top$ 映射到输出 $o=[\\hat u, \\hat v, \\hat p]^\\top$。\n  - 隐藏层预激活: $z = W_1 s + b_1$，其中 $W_1 \\in \\mathbb{R}^{H \\times 3}$，$b_1 \\in \\mathbb{R}^H$。\n  - 隐藏层激活: $h = \\tanh(z)$。\n  - 输出: $o = W_2 h + b_2$，其中 $W_2 \\in \\mathbb{R}^{3 \\times H}$，$b_2 \\in \\mathbb{R}^3$。\n- **激活函数导数**：$\\tanh'(z) = 1 - \\tanh^2(z)$，$\\tanh''(z) = -2 \\tanh(z) \\left(1 - \\tanh^2(z)\\right)$。\n- **损失函数**：在 $N$ 个配置点 $\\{(x_i,y_i,t_i)\\}_{i=1}^N$ 上方程残差的均方误差。\n  - $\\mathcal{L}_{\\text{int}} = \\frac{1}{N} \\sum_{i=1}^N \\left( r_u^2 + r_v^2 + r_c^2 \\right)$\n  - 残差: $r_u = \\hat u_t + \\hat u \\hat u_x + \\hat v \\hat u_y + \\hat p_x - \\nu \\Delta \\hat u$, $r_v = \\hat v_t + \\hat u \\hat v_x + \\hat v \\hat v_y + \\hat p_y - \\nu \\Delta \\hat v$, $r_c = \\hat u_x + \\hat v_y$。\n- **测试用例参数**：提供了三个不同的测试用例，其中包含 $\\nu$、网络权重 ($W_1, b_1, W_2, b_2$)、隐藏层宽度 ($H=3$) 和配置点的具体值。\n\n**第 2 步：使用提取的已知条件进行验证**\n- **科学依据**：该问题基于流体动力学的基本原理（Navier-Stokes 方程）和计算科学中的标准方法（物理信息神经网络）。所有要素都已得到充分证实。\n- **适定性**：该问题是一项评估任务。所有必要的参数和数据都已提供，以便为每种情况下的损失函数计算出唯一的数值。\n- **客观性**：该问题使用精确的数学定义和客观的计算指令进行陈述。\n所有其他验证标准均已满足。该问题是自包含的、一致的且计算上可行的。\n\n**第 3 步：结论与行动**\n问题有效。详细解答如下。\n\n### 解题推导\n\n问题的核心是在每个配置点计算神经网络输出 $(\\hat u, \\hat v, \\hat p)$ 的残差 $r_u$、$r_v$ 和 $r_c$。这需要计算网络输出相对于其输入 $(x, y, t)$ 的一阶和二阶偏导数。我们使用自动微分，即系统地应用链式法则。\n\n设输入向量为 $s = [s_1, s_2, s_3]^\\top = [x, y, t]^\\top$。网络架构由下式给出：\n$$\no(s) = W_2 h + b_2 = W_2 \\tanh(W_1 s + b_1) + b_2\n$$\n此处，$o(s) = [o_1(s), o_2(s), o_3(s)]^\\top = [\\hat u(s), \\hat v(s), \\hat p(s)]^\\top$。设隐藏层预激活为 $z = W_1 s + b_1$，激活后为 $h = \\tanh(z)$。其分量为 $z_i$ 和 $h_i = \\tanh(z_i)$，其中 $i=1, \\dots, H$。\n\n**1. 一阶偏导数**\n\n为了计算像 $\\hat u_x = \\frac{\\partial \\hat u}{\\partial x}$ 这样的项，我们应用链式法则。第 $k$ 个输出 $o_k$ 对第 $j$ 个输入 $s_j$ 的偏导数的一般表达式为：\n$$\n\\frac{\\partial o_k}{\\partial s_j} = \\sum_{i=1}^H \\frac{\\partial o_k}{\\partial h_i} \\frac{\\partial h_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial s_j}\n$$\n该导数的各个组成部分是：\n- $\\frac{\\partial o_k}{\\partial h_i}$：从 $o_k = \\sum_{l=1}^H (W_2)_{kl} h_l + (b_2)_k$，我们得到 $\\frac{\\partial o_k}{\\partial h_i} = (W_2)_{ki}$。\n- $\\frac{\\partial h_i}{\\partial z_i}$：这是激活函数的导数，$\\frac{d}{dz_i} \\tanh(z_i) = 1 - \\tanh^2(z_i) = 1 - h_i^2$。\n- $\\frac{\\partial z_i}{\\partial s_j}$：从 $z_i = \\sum_{l=1}^3 (W_1)_{il} s_l + (b_1)_i$，我们得到 $\\frac{\\partial z_i}{\\partial s_j} = (W_1)_{ij}$。\n\n结合这些，我们得到一阶偏导数的表达式：\n$$\n\\frac{\\partial o_k}{\\partial s_j} = \\sum_{i=1}^H (W_2)_{ki} (1 - h_i^2) (W_1)_{ij}\n$$\n这个表达式可以紧凑地写成矩阵形式。设 $J_o$ 是输出 $o$ 相对于输入 $s$ 的雅可比矩阵，其中 $(J_o)_{kj} = \\frac{\\partial o_k}{\\partial s_j}$。设 $D(h)$ 是一个对角矩阵，其对角线元素为 $D_{ii} = 1 - h_i^2$。那么，雅可比矩阵为：\n$$\nJ_o = W_2 D(h) W_1\n$$\n残差所需的九个一阶导数可以从这个 $3 \\times 3$ 的雅可比矩阵中提取：\n$$\nJ_o = \\begin{bmatrix} \\partial \\hat u / \\partial x  \\partial \\hat u / \\partial y  \\partial \\hat u / \\partial t \\\\ \\partial \\hat v / \\partial x  \\partial \\hat v / \\partial y  \\partial \\hat v / \\partial t \\\\ \\partial \\hat p / \\partial x  \\partial \\hat p / \\partial y  \\partial \\hat p / \\partial t \\end{bmatrix} = \\begin{bmatrix} \\hat u_x  \\hat u_y  \\hat u_t \\\\ \\hat v_x  \\hat v_y  \\hat v_t \\\\ \\hat p_x  \\hat p_y  \\hat p_t \\end{bmatrix}\n$$\n不可压缩性残差 $r_c$ 直接从雅可比矩阵计算得出：$r_c = \\hat u_x + \\hat v_y = (J_o)_{11} + (J_o)_{22}$。这明确解释了如何通过自动微分计算速度场 $\\nabla \\cdot \\hat{\\boldsymbol{v}}$ 的散度。\n\n**2. 二阶偏导数**\n\n动量残差 $r_u$ 和 $r_v$ 需要速度分量的拉普拉斯算子，即 $\\Delta \\hat u = \\hat u_{xx} + \\hat u_{yy}$ 和 $\\Delta \\hat v = \\hat v_{xx} + \\hat v_{yy}$。我们通过对一阶导数再次求导来计算这些值。让我们找出一般的二阶导数 $\\frac{\\partial^2 o_k}{\\partial s_m \\partial s_j}$：\n$$\n\\frac{\\partial^2 o_k}{\\partial s_m \\partial s_j} = \\frac{\\partial}{\\partial s_m} \\left( \\sum_{i=1}^H (W_2)_{ki} (1 - h_i^2) (W_1)_{ij} \\right) = \\sum_{i=1}^H (W_2)_{ki} (W_1)_{ij} \\frac{\\partial}{\\partial s_m} (1 - h_i^2)\n$$\n导数项是：\n$$\n\\frac{\\partial}{\\partial s_m} (1 - h_i^2) = -2 h_i \\frac{\\partial h_i}{\\partial s_m} = -2 h_i \\left( \\frac{\\partial h_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial s_m} \\right) = -2 h_i (1-h_i^2) (W_1)_{im}\n$$\n这等价于使用激活函数的二阶导数：$\\frac{\\partial}{\\partial s_m} (\\tanh'(z_i)) = \\tanh''(z_i) \\frac{\\partial z_i}{\\partial s_m} = -2 \\tanh(z_i) (1-\\tanh^2(z_i)) (W_1)_{im}$。\n将其代回，我们得到：\n$$\n\\frac{\\partial^2 o_k}{\\partial s_m \\partial s_j} = \\sum_{i=1}^H (W_2)_{ki} (W_1)_{ij} [-2 h_i (1 - h_i^2) (W_1)_{im}]\n$$\n我们需要空间变量的黑塞矩阵对角线项（$j=m=1$ 对应 $x$，$j=m=2$ 对应 $y$）。\n- 对于 $\\hat u_{xx}$ ($k=1, j=m=1$)：\n$$\n\\hat u_{xx} = \\frac{\\partial^2 \\hat u}{\\partial x^2} = \\sum_{i=1}^H (W_2)_{1i} (-2 h_i (1-h_i^2)) ((W_1)_{i1})^2\n$$\n- 对于 $\\hat u_{yy}$ ($k=1, j=m=2$)：\n$$\n\\hat u_{yy} = \\frac{\\partial^2 \\hat u}{\\partial y^2} = \\sum_{i=1}^H (W_2)_{1i} (-2 h_i (1-h_i^2)) ((W_1)_{i2})^2\n$$\n$\\hat u$ 的拉普拉斯算子是这两项的和：\n$$\n\\Delta \\hat u = \\hat u_{xx} + \\hat u_{yy} = \\sum_{i=1}^H (W_2)_{1i} [-2 h_i (1-h_i^2)] \\left( ((W_1)_{i1})^2 + ((W_1)_{i2})^2 \\right)\n$$\n类似地，对于 $\\hat v$ ($k=2$)：\n$$\n\\Delta \\hat v = \\hat v_{xx} + \\hat v_{yy} = \\sum_{i=1}^H (W_2)_{2i} [-2 h_i (1-h_i^2)] \\left( ((W_1)_{i1})^2 + ((W_1)_{i2})^2 \\right)\n$$\n\n**3. 组合损失函数**\n\n对于每个配置点 $(x_k, y_k, t_k)$，算法如下：\n1. 计算前向传播：$s = [x_k, y_k, t_k]^\\top$，$z = W_1 s + b_1$，$h = \\tanh(z)$，$o = W_2 h + b_2$。这给出了 $\\hat u, \\hat v, \\hat p$。\n2. 使用上面推导的公式计算所有需要的导数：$\\hat u_x, \\hat u_y, \\hat u_t, \\hat v_x, \\hat v_y, \\hat v_t, \\hat p_x, \\hat p_y, \\Delta \\hat u, \\Delta \\hat v$。\n3. 评估残差：\n    $$\n    r_u = \\hat u_t + \\hat u \\hat u_x + \\hat v \\hat u_y + \\hat p_x - \\nu \\Delta \\hat u\n    $$\n    $$\n    r_v = \\hat v_t + \\hat u \\hat v_x + \\hat v \\hat v_y + \\hat p_y - \\nu \\Delta \\hat v\n    $$\n    $$\n    r_c = \\hat u_x + \\hat v_y\n    $$\n4. 计算该点的残差平方和：$r_u^2 + r_v^2 + r_c^2$。\n5. 将所有 $N$ 个配置点的这个总和取平均，得到最终损失 $\\mathcal{L}_{\\text{int}}$：\n    $$\n    \\mathcal{L}_{\\text{int}} = \\frac{1}{N} \\sum_{k=1}^N \\left( r_u(s_k)^2 + r_v(s_k)^2 + r_c(s_k)^2 \\right)\n    $$\n该过程为计算物理信息损失提供了一个完整的算法，并在下面的 Python 程序中实现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the physics-informed loss for the 2D Navier-Stokes equations\n    for three specified test cases.\n    \"\"\"\n    \n    # Define test cases as a list of dictionaries.\n    test_cases = [\n        {\n            \"name\": \"Test Case 1 (general viscosity)\",\n            \"nu\": 0.01,\n            \"W1\": np.array([\n                [0.5, -0.3, 0.2],\n                [-0.4, 0.7, -0.1],\n                [0.25, 0.1, -0.5]\n            ]),\n            \"b1\": np.array([0.1, -0.2, 0.05]),\n            \"W2\": np.array([\n                [0.8, -0.5, 0.3],\n                [-0.6, 0.4, 0.2],\n                [0.2, -0.1, 0.7]\n            ]),\n            \"b2\": np.array([0.0, 0.0, 0.0]),\n            \"points\": [\n                (0.2, 0.3, 0.1),\n                (0.7, 0.5, 0.4),\n                (0.9, 0.1, 0.9),\n                (0.0, 0.0, 0.0)\n            ]\n        },\n        {\n            \"name\": \"Test Case 2 (inviscid limit)\",\n            \"nu\": 0.0,\n            \"W1\": np.array([\n                [0.5, -0.3, 0.2],\n                [-0.4, 0.7, -0.1],\n                [0.25, 0.1, -0.5]\n            ]),\n            \"b1\": np.array([0.1, -0.2, 0.05]),\n            \"W2\": np.array([\n                [0.8, -0.5, 0.3],\n                [-0.6, 0.4, 0.2],\n                [0.2, -0.1, 0.7]\n            ]),\n            \"b2\": np.array([0.0, 0.0, 0.0]),\n            \"points\": [\n                (0.2, 0.3, 0.1),\n                (0.7, 0.5, 0.4),\n                (0.9, 0.1, 0.9),\n                (0.0, 0.0, 0.0)\n            ]\n        },\n        {\n            \"name\": \"Test Case 3 (trivial zero field)\",\n            \"nu\": 0.05,\n            \"W1\": np.zeros((3, 3)),\n            \"b1\": np.zeros(3),\n            \"W2\": np.zeros((3, 3)),\n            \"b2\": np.zeros(3),\n            \"points\": [\n                (0.1, 0.2, 0.3),\n                (0.4, 0.4, 0.4),\n                (0.9, 0.9, 0.9)\n            ]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        nu = case[\"nu\"]\n        W1 = case[\"W1\"]\n        b1 = case[\"b1\"]\n        W2 = case[\"W2\"]\n        b2 = case[\"b2\"]\n        points = case[\"points\"]\n        \n        total_squared_residual = 0.0\n        N = len(points)\n        \n        for x, y, t in points:\n            # Input vector\n            s = np.array([x, y, t])\n            \n            # --- Forward Pass ---\n            z = W1 @ s + b1\n            h = np.tanh(z)\n            o = W2 @ h + b2\n            u_hat, v_hat, p_hat = o[0], o[1], o[2]\n            \n            # --- First-Order Derivatives ---\n            # D(h) is diag(1 - tanh^2(z_i))\n            D_h = np.diag(1 - h**2)\n            # Jacobian J_o = W2 * D_h * W1\n            J_o = W2 @ D_h @ W1\n            \n            u_x, u_y, u_t = J_o[0, 0], J_o[0, 1], J_o[0, 2]\n            v_x, v_y, v_t = J_o[1, 0], J_o[1, 1], J_o[1, 2]\n            p_x, p_y     = J_o[2, 0], J_o[2, 1]\n            \n            # --- Second-Order Derivatives (Laplacians) ---\n            # Term for second derivative: -2 * h * (1 - h^2)\n            tanh_double_prime_term = -2 * h * (1 - h**2)\n            \n            # (W1_i1^2 + W1_i2^2) vector\n            W1_spatial_sq_sum = W1[:, 0]**2 + W1[:, 1]**2\n            \n            # Laplacian calculation\n            # lap_u = sum_i( W2_1i * (-2*h_i*(1-h_i^2)) * (W1_i1^2 + W1_i2^2) )\n            lap_u = np.sum(W2[0, :] * tanh_double_prime_term * W1_spatial_sq_sum)\n            lap_v = np.sum(W2[1, :] * tanh_double_prime_term * W1_spatial_sq_sum)\n            \n            # --- Residuals Calculation ---\n            r_c = u_x + v_y\n            r_u = u_t + u_hat * u_x + v_hat * u_y + p_x - nu * lap_u\n            r_v = v_t + u_hat * v_x + v_hat * v_y + p_y - nu * lap_v\n\n            # Sum of squared residuals for this point\n            total_squared_residual += r_u**2 + r_v**2 + r_c**2\n            \n        # Mean squared residual loss\n        loss = total_squared_residual / N\n        results.append(loss)\n\n    # Format the final output string\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}