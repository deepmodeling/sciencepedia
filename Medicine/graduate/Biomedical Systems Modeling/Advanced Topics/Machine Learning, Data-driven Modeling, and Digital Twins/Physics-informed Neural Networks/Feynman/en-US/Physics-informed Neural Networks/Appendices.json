{
    "hands_on_practices": [
        {
            "introduction": "The heart of a Physics-informed Neural Network (PINN) is its ability to embed a physical law directly into the training process. This is achieved by penalizing the network if its output violates the governing differential equation. This first exercise  provides foundational practice in deriving this \"PDE residual\" analytically for a simple neural network architecture, making the connection between network parameters and the physics explicit.",
            "id": "2126326",
            "problem": "A researcher is developing a Physics-Informed Neural Network (PINN) to approximate the ground state solution of the one-dimensional, time-independent Schrödinger equation for a particle of mass $m$ in a quantum harmonic oscillator potential. The governing equation is given by:\n$$-\\frac{\\hbar^2}{2m} \\frac{d^2\\psi(x)}{dx^2} + V(x)\\psi(x) = E\\psi(x)$$\nwhere $\\psi(x)$ is the wavefunction, $\\hbar$ is the reduced Planck constant, $E$ is the energy eigenvalue, and the potential is $V(x) = \\frac{1}{2}kx^2$ for a spring constant $k$.\n\nThe researcher proposes a simple neural network architecture as an ansatz for the wavefunction, denoted $\\psi_{NN}(x)$. This network has a single input neuron for the position $x$, one hidden layer with a single neuron using a hyperbolic tangent activation function, and a linear output neuron. Its mathematical form is:\n$$\\psi_{NN}(x) = w_2 \\tanh(w_1 x + b_1) + b_2$$\nwhere $w_1, b_1$ are the weight and bias for the hidden layer, and $w_2, b_2$ are the weight and bias for the output layer.\n\nA core component of the PINN's training process is the minimization of the Partial Differential Equation (PDE) residual. The residual, denoted $R(x)$, is defined as the value obtained when the neural network's approximation is substituted into the governing differential equation. That is, $R(x) = -\\frac{\\hbar^2}{2m} \\frac{d^2\\psi_{NN}}{dx^2} + V(x)\\psi_{NN} - E\\psi_{NN}$.\n\nDerive the complete analytical expression for the PDE residual $R(x)$ in terms of the input $x$, the network parameters ($w_1, b_1, w_2, b_2$), the energy $E$, and the physical constants ($m, \\hbar, k$).",
            "solution": "We are given the time-independent Schrödinger equation\n$$-\\frac{\\hbar^{2}}{2m}\\frac{d^{2}\\psi(x)}{dx^{2}}+V(x)\\psi(x)=E\\psi(x),$$\nwith $V(x)=\\frac{k}{2}x^{2}$, and the neural network ansatz\n$$\\psi_{NN}(x)=w_{2}\\tanh(w_{1}x+b_{1})+b_{2}.$$\nThe PDE residual is defined as\n$$R(x)=-\\frac{\\hbar^{2}}{2m}\\frac{d^{2}\\psi_{NN}}{dx^{2}}+V(x)\\psi_{NN}-E\\psi_{NN}.$$\n\nCompute the first derivative using the chain rule and $d(\\tanh u)/du=\\frac{1}{\\cosh^{2}u}$:\n$$\\frac{d\\psi_{NN}}{dx}=w_{2}\\cdot \\frac{d}{dx}\\tanh(w_{1}x+b_{1})=w_{2}\\cdot \\frac{1}{\\cosh^{2}(w_{1}x+b_{1})}\\cdot w_{1}=\\frac{w_{1}w_{2}}{\\cosh^{2}(w_{1}x+b_{1})}.$$\n\nCompute the second derivative using the chain rule again and $d(\\cosh^{-2}u)/du=-2\\cosh^{-3}u\\sinh u$:\n$$\\frac{d^{2}\\psi_{NN}}{dx^{2}}=w_{1}w_{2}\\cdot \\frac{d}{dx}\\big[\\cosh^{-2}(w_{1}x+b_{1})\\big]=w_{1}w_{2}\\cdot \\big[-2\\cosh^{-3}(w_{1}x+b_{1})\\sinh(w_{1}x+b_{1})\\big]\\cdot w_{1},$$\nso\n$$\\frac{d^{2}\\psi_{NN}}{dx^{2}}=-2w_{1}^{2}w_{2}\\,\\frac{\\sinh(w_{1}x+b_{1})}{\\cosh^{3}(w_{1}x+b_{1})}=-2w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}.$$\n\nSubstitute into the residual definition with $V(x)=\\frac{k}{2}x^{2}$:\n$$R(x)=-\\frac{\\hbar^{2}}{2m}\\left[-2w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}\\right]+\\frac{k}{2}x^{2}\\big[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\big]-E\\big[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\big].$$\n\nSimplifying the kinetic term gives\n$$R(x)=\\frac{\\hbar^{2}}{m}w_{1}^{2}w_{2}\\,\\frac{\\tanh(w_{1}x+b_{1})}{\\cosh^{2}(w_{1}x+b_{1})}+\\left(\\frac{k}{2}x^{2}-E\\right)\\left[w_{2}\\tanh(w_{1}x+b_{1})+b_{2}\\right].$$\nThis is the complete analytical expression for the PDE residual in terms of $x$, the network parameters, $E$, and the physical constants.",
            "answer": "$$\\boxed{\\frac{\\hbar^{2}}{m}w_{2}w_{1}^{2}\\,\\frac{\\tanh\\!\\left(w_{1}x+b_{1}\\right)}{\\cosh^{2}\\!\\left(w_{1}x+b_{1}\\right)}+\\left(\\frac{k}{2}x^{2}-E\\right)\\left[w_{2}\\tanh\\!\\left(w_{1}x+b_{1}\\right)+b_{2}\\right]}$$"
        },
        {
            "introduction": "While the PDE governs behavior within a domain, the specific solution is determined by conditions at its boundaries. This next practice explores how to incorporate these critical constraints into the PINN's loss function. This problem  demonstrates the formulation of loss terms for both prescribed value (Dirichlet) and prescribed derivative (Neumann) conditions, a common and practical scenario in modeling physical systems.",
            "id": "4235901",
            "problem": "A cyber-physical system digital twin for a steady-state conductive field is modeled by Laplace’s equation, derived from conservation of flux with constant conductivity and no sources. Let the physical potential be $u:\\Omega \\to \\mathbb{R}$, where $\\Omega \\subset \\mathbb{R}^{d}$ is a bounded domain with Lipschitz boundary $\\partial \\Omega$. The governing law is $-\\Delta u = 0$ in $\\Omega$, with mixed boundary conditions $u=g$ on $\\Gamma_{D} \\subset \\partial \\Omega$ and $\\partial_{n} u = h$ on $\\Gamma_{N}=\\partial \\Omega \\setminus \\Gamma_{D}$, where $g:\\Gamma_{D} \\to \\mathbb{R}$ and $h:\\Gamma_{N} \\to \\mathbb{R}$ are measured by boundary sensors, and $\\partial_{n} u$ denotes the normal derivative.\n\nYou deploy a physics-informed neural network (PINN) $u_{\\theta}:\\Omega \\to \\mathbb{R}$ with parameters $\\theta$ to serve as the digital twin surrogate. For boundary training, you collect quadrature samples on each boundary segment:\n- Dirichlet samples $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$ with associated measurements $\\{g_{i}\\}_{i=1}^{N_{D}}$ and positive quadrature weights $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$ that approximate integration over $\\Gamma_{D}$.\n- Neumann samples $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$ with measured fluxes $\\{h_{j}\\}_{j=1}^{N_{N}}$, unit outward normals $\\{n_{j}\\}_{j=1}^{N_{N}} \\subset \\mathbb{R}^{d}$, and positive quadrature weights $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$.\n\nLet $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ and $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$ be the total quadrature weights. Suppose you choose positive penalty coefficients $\\lambda_{D}$ and $\\lambda_{N}$ to balance the enforcement strength of Dirichlet and Neumann boundary conditions.\n\nStarting from the fundamental law $-\\Delta u = 0$ and the definitions of Dirichlet and Neumann boundary conditions, derive the separate boundary residual functionals on $\\Gamma_{D}$ and $\\Gamma_{N}$ that measure the squared mismatch between $u_{\\theta}$ and $g$, and between $\\nabla u_{\\theta} \\cdot n$ and $h$, respectively. Then, using quadrature approximations with the given samples and weights, construct a single composite, dimensionless boundary-only loss functional $L_{\\mathrm{bc}}(\\theta)$ whose value is a weighted sum of the two boundary residuals normalized by $W_{D}$ and $W_{N}$. Express this $L_{\\mathrm{bc}}(\\theta)$ explicitly as a closed-form analytic expression in terms of the given data $\\{x_{i}^{D},g_{i},w_{i}^{D}\\}_{i=1}^{N_{D}}$, $\\{x_{j}^{N},h_{j},n_{j},w_{j}^{N}\\}_{j=1}^{N_{N}}$, and the network $u_{\\theta}$. In your derivation, discuss how each boundary residual is enforced using automatic differentiation (AD) to obtain $\\nabla u_{\\theta}$ at boundary points.\n\nYour final answer must be the single explicit expression for $L_{\\mathrm{bc}}(\\theta)$ as a function of $\\theta$, containing only symbolic quantities and the given data. Do not include units. No rounding is required.",
            "solution": "The problem requires the derivation of a composite, dimensionless boundary-only loss functional, denoted $L_{\\mathrm{bc}}(\\theta)$, for a physics-informed neural network (PINN) surrogate $u_{\\theta}$ of a physical potential $u$. The system is governed by Laplace's equation $-\\Delta u = 0$ on a domain $\\Omega \\subset \\mathbb{R}^{d}$ with mixed Dirichlet and Neumann boundary conditions. The loss functional is constructed from discrete sensor data on the boundary.\n\nWe begin by formally defining the continuous residual functionals for each type of boundary condition. These functionals measure the extent to which the PINN surrogate $u_{\\theta}$ fails to satisfy the prescribed conditions on the boundary $\\partial \\Omega$.\n\nFirst, consider the Dirichlet boundary $\\Gamma_{D}$, where the potential is prescribed as $u=g$. The mismatch, or residual, at any point $x \\in \\Gamma_D$ is given by the difference $u_{\\theta}(x) - g(x)$. To measure the total error over this boundary segment, we integrate the square of this residual. This gives the continuous Dirichlet boundary residual functional, $R_{D}(\\theta)$:\n$$\nR_{D}(\\theta) = \\int_{\\Gamma_{D}} (u_{\\theta}(x) - g(x))^2 \\, dS\n$$\nwhere $dS$ is the surface measure on the boundary $\\partial\\Omega$.\n\nSecond, consider the Neumann boundary $\\Gamma_{N}$, where the normal derivative of the potential is prescribed as $\\partial_{n} u = h$. The normal derivative is the projection of the gradient onto the unit outward normal vector $n(x)$, i.e., $\\partial_{n} u(x) = \\nabla u(x) \\cdot n(x)$. The residual at a point $x \\in \\Gamma_N$ is the difference between the normal derivative of the PINN approximation and the prescribed function $h(x)$, which is $\\nabla u_{\\theta}(x) \\cdot n(x) - h(x)$. The total squared error over the Neumann boundary is then given by the continuous Neumann boundary residual functional, $R_{N}(\\theta)$:\n$$\nR_{N}(\\theta) = \\int_{\\Gamma_{N}} (\\nabla u_{\\theta}(x) \\cdot n(x) - h(x))^2 \\, dS\n$$\n\nIn a practical setting, we do not have the continuous functions $g$ and $h$ but rather discrete measurements at specific sensor locations. The problem provides sets of sample points and quadrature weights to approximate the integrals for $R_{D}(\\theta)$ and $R_{N}(\\theta)$.\n\nFor the Dirichlet boundary, we are given $N_D$ sample points $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$ with corresponding potential measurements $\\{g_{i}\\}_{i=1}^{N_{D}}$ (where $g_i = g(x_i^D)$) and positive quadrature weights $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$. The integral for $R_{D}(\\theta)$ is approximated by a weighted sum, which we denote as the discrete residual $\\hat{R}_{D}(\\theta)$:\n$$\n\\hat{R}_{D}(\\theta) = \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2\n$$\nThis is a numerical quadrature approximation of the integral $\\int_{\\Gamma_{D}} (u_{\\theta} - g)^2 \\, dS$.\n\nFor the Neumann boundary, we have $N_N$ sample points $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$ with flux measurements $\\{h_{j}\\}_{j=1}^{N_{N}}$, unit outward normals $\\{n_{j}\\}_{j=1}^{N_{N}}$, and positive quadrature weights $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$. The crucial calculation here is the term $\\nabla u_{\\theta}(x_j^N)$, the gradient of the neural network output with respect to its spatial inputs. This is computed using automatic differentiation (AD). The neural network $u_{\\theta}$ is a composition of differentiable elementary functions (e.g., affine transformations and activation functions). Its derivatives with respect to its inputs can therefore be computed algorithmically to machine precision. Deep learning frameworks provide this functionality natively, typically via reverse-mode AD (backpropagation), allowing for efficient evaluation of $\\nabla u_{\\theta}$ at any point $x_j^N$ required for the loss calculation.\n\nUsing AD to obtain $\\nabla u_{\\theta}(x_{j}^{N})$, we can then approximate the integral for $R_{N}(\\theta)$ with the discrete Neumann residual $\\hat{R}_{N}(\\theta)$:\n$$\n\\hat{R}_{N}(\\theta) = \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\n\nThe problem asks for a single composite loss functional $L_{\\mathrm{bc}}(\\theta)$ that is a weighted sum of the two boundary residuals, normalized by their respective total quadrature weights, $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ and $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$. This normalization turns each discrete residual into a mean squared error approximation. We are also given positive penalty coefficients $\\lambda_{D}$ and $\\lambda_{N}$ to balance the relative importance of enforcing each boundary condition.\nThe normalized Dirichlet loss term is $\\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}}$.\nThe normalized Neumann loss term is $\\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}$.\n\nThe problem specifies that the final loss functional $L_{\\mathrm{bc}}(\\theta)$ should be dimensionless. This typically implies that the governing equations and boundary conditions have been non-dimensionalized beforehand using characteristic scales for length, potential, etc. Under this standard assumption, all quantities in the loss function are dimensionless, and the penalty coefficients $\\lambda_D$ and $\\lambda_N$ are also dimensionless factors used for tuning the training process.\n\nCombining these components, the composite boundary loss functional is the sum of the normalized and weighted discrete residuals:\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}} + \\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}\n$$\nSubstituting the expressions for $\\hat{R}_{D}(\\theta)$, $\\hat{R}_{N}(\\theta)$, $W_{D}$, and $W_{N}$:\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\left( \\frac{\\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2}{\\sum_{i=1}^{N_{D}} w_{i}^{D}} \\right) + \\lambda_{N} \\left( \\frac{\\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}{\\sum_{j=1}^{N_{N}} w_{j}^{N}} \\right)\n$$\nThis can be written more concisely using the given definitions $W_{D}$ and $W_{N}$:\n$$\nL_{\\mathrm{bc}}(\\theta) = \\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\nThis expression represents the final, closed-form analytic form for the boundary-only loss functional $L_{\\mathrm{bc}}(\\theta)$ as a function of the network parameters $\\theta$ and the given data. Minimizing this loss functional with respect to $\\theta$ trains the network to satisfy the boundary conditions in a least-squares sense.",
            "answer": "$$\n\\boxed{\\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}\n$$"
        },
        {
            "introduction": "Having mastered the fundamentals for single equations, we now scale up to a system of coupled, nonlinear PDEs that model complex phenomena like fluid flow. This advanced practice  tackles the incompressible Navier-Stokes equations, a cornerstone of fluid dynamics. This exercise will transition you from analytical derivations to the logic of computational implementation, showing how automatic differentiation is used to handle the complex system of residuals in a multi-output network.",
            "id": "4235910",
            "problem": "A digital twin of a planar fluid subsystem in a cyber-physical system is modeled as an incompressible Newtonian flow governed by the two-dimensional Navier–Stokes equations. A Physics-Informed Neural Network (PINN) is used as a surrogate that, for space-time input $(x,y,t)$, outputs the fields $(u(x,y,t), v(x,y,t), p(x,y,t))$ corresponding to the velocity components and pressure. The objective is to derive, from first principles, a loss that enforces the momentum equations and the incompressibility constraint, and to implement a program that evaluates this loss for specified test cases using automatic differentiation via the chain rule. All variables are nondimensionalized, so no physical units are used in this problem.\n\nStarting from fundamental conservation principles, the incompressible flow satisfies mass conservation (incompressibility) and Newton’s second law for momentum balance. The two-dimensional incompressible Navier–Stokes equations for a Newtonian fluid with constant kinematic viscosity $\\nu$ and unit density are\n$$\n\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y} + \\frac{\\partial p}{\\partial x} - \\nu \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = 0,\n$$\n$$\n\\frac{\\partial v}{\\partial t} + u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y} + \\frac{\\partial p}{\\partial y} - \\nu \\left(\\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2}\\right) = 0,\n$$\n$$\n\\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} = 0.\n$$\nA Physics-Informed Neural Network approximates $(u,v,p)$ with a parametric function $(\\hat u,\\hat v,\\hat p)$ and enforces the physics by penalizing residuals of these equations at collocation points. The loss to be constructed must include, at a minimum, the squared residuals of both momentum equations and the incompressibility constraint.\n\nDefine a single-hidden-layer neural network with hyperbolic tangent activation of width $H$ that maps $(x,y,t)$ to $(\\hat u,\\hat v,\\hat p)$:\n- Hidden pre-activation $z \\in \\mathbb{R}^H$: $z = W_1 s + b_1$, where $s = [x,y,t]^\\top$, $W_1 \\in \\mathbb{R}^{H \\times 3}$, $b_1 \\in \\mathbb{R}^H$.\n- Hidden activation $h \\in \\mathbb{R}^H$: $h = \\tanh(z)$.\n- Output $o \\in \\mathbb{R}^3$: $o = W_2 h + b_2$, where $W_2 \\in \\mathbb{R}^{3 \\times H}$, $b_2 \\in \\mathbb{R}^3$.\n- Interpret $(\\hat u,\\hat v,\\hat p) = (o_1,o_2,o_3)$.\n\nUsing automatic differentiation (application of the chain rule), compute the required first and second derivatives with respect to $(x,y,t)$ at collocation points. For the $\\tanh$ activation, use $\\tanh'(z) = 1 - \\tanh^2(z)$ and $\\tanh''(z) = -2 \\tanh(z) \\left(1 - \\tanh^2(z)\\right)$.\n\nConstruct the interior physics loss at a set of $N$ collocation points $\\{(x_i,y_i,t_i)\\}_{i=1}^N$ as the mean of squared residuals:\n$$\n\\mathcal{L}_{\\text{int}} = \\frac{1}{N} \\sum_{i=1}^N \\left( r_u(x_i,y_i,t_i)^2 + r_v(x_i,y_i,t_i)^2 + r_c(x_i,y_i,t_i)^2 \\right),\n$$\nwhere\n$$\nr_u = \\hat u_t + \\hat u \\hat u_x + \\hat v \\hat u_y + \\hat p_x - \\nu \\left(\\hat u_{xx} + \\hat u_{yy}\\right), \\quad\nr_v = \\hat v_t + \\hat u \\hat v_x + \\hat v \\hat v_y + \\hat p_y - \\nu \\left(\\hat v_{xx} + \\hat v_{yy}\\right),\n$$\n$$\nr_c = \\hat u_x + \\hat v_y.\n$$\nExplain how to compute $\\nabla \\cdot \\hat{\\boldsymbol{v}} = \\frac{\\partial \\hat u}{\\partial x} + \\frac{\\partial \\hat v}{\\partial y}$ via automatic differentiation within this architecture.\n\nYour program must implement the above network and compute $\\mathcal{L}_{\\text{int}}$ for each of the following test cases. For all cases, use a hidden width $H = 3$ and the specified weights, biases, viscosity, and collocation points. The program should produce a single line of output containing the three loss values as a comma-separated list enclosed in square brackets, with each value formatted to exactly six digits after the decimal point.\n\nTest Case $1$ (general viscosity):\n- $\\nu = 0.01$.\n- $W_1 = \\begin{bmatrix} 0.5 & -0.3 & 0.2 \\\\ -0.4 & 0.7 & -0.1 \\\\ 0.25 & 0.1 & -0.5 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$.\n- $W_2 = \\begin{bmatrix} 0.8 & -0.5 & 0.3 \\\\ -0.6 & 0.4 & 0.2 \\\\ 0.2 & -0.1 & 0.7 \\end{bmatrix}$, $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$.\n- Collocation points $(x,y,t)$:\n  - $(0.2, 0.3, 0.1)$,\n  - $(0.7, 0.5, 0.4)$,\n  - $(0.9, 0.1, 0.9)$,\n  - $(0.0, 0.0, 0.0)$.\n\nTest Case $2$ (inviscid limit):\n- $\\nu = 0.0$.\n- Use the same $W_1$, $b_1$, $W_2$, $b_2$, and collocation points as in Test Case $1$.\n\nTest Case $3$ (trivial zero field):\n- $\\nu = 0.05$.\n- $W_1 = \\mathbf{0}_{3 \\times 3}$, $b_1 = \\mathbf{0}_{3}$.\n- $W_2 = \\mathbf{0}_{3 \\times 3}$, $b_2 = \\mathbf{0}_{3}$.\n- Collocation points $(x,y,t)$:\n  - $(0.1, 0.2, 0.3)$,\n  - $(0.4, 0.4, 0.4)$,\n  - $(0.9, 0.9, 0.9)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Test Case $1$, Test Case $2$, and Test Case $3$, with each value formatted to exactly six digits after the decimal point (for example, $[0.123456,0.000001,2.500000]$).",
            "solution": "The problem requires the derivation and implementation of a physics-informed loss function for a neural network surrogate model of a $2$D incompressible fluid flow. The governing equations are the Navier-Stokes equations. The surrogate model is a single-hidden-layer neural network. We will first validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Governing Equations**: $2$D incompressible Navier-Stokes for a Newtonian fluid with unit density and constant kinematic viscosity $\\nu$.\n  - x-momentum: $\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y} + \\frac{\\partial p}{\\partial x} - \\nu \\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = 0$\n  - y-momentum: $\\frac{\\partial v}{\\partial t} + u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y} + \\frac{\\partial p}{\\partial y} - \\nu \\left(\\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2}\\right) = 0$\n  - Incompressibility: $\\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} = 0$\n- **Neural Network Architecture**: A single-hidden-layer network with hidden width $H$, mapping an input $s=[x,y,t]^\\top$ to an output $o=[\\hat u, \\hat v, \\hat p]^\\top$.\n  - Hidden pre-activation: $z = W_1 s + b_1$, with $W_1 \\in \\mathbb{R}^{H \\times 3}$, $b_1 \\in \\mathbb{R}^H$.\n  - Hidden activation: $h = \\tanh(z)$.\n  - Output: $o = W_2 h + b_2$, with $W_2 \\in \\mathbb{R}^{3 \\times H}$, $b_2 \\in \\mathbb{R}^3$.\n- **Activation Derivatives**: $\\tanh'(z) = 1 - \\tanh^2(z)$, $\\tanh''(z) = -2 \\tanh(z) \\left(1 - \\tanh^2(z)\\right)$.\n- **Loss Function**: Mean squared error of the equation residuals over $N$ collocation points $\\{(x_i,y_i,t_i)\\}_{i=1}^N$.\n  - $\\mathcal{L}_{\\text{int}} = \\frac{1}{N} \\sum_{i=1}^N \\left( r_u^2 + r_v^2 + r_c^2 \\right)$\n  - Residuals: $r_u = \\hat u_t + \\hat u \\hat u_x + \\hat v \\hat u_y + \\hat p_x - \\nu \\Delta \\hat u$, $r_v = \\hat v_t + \\hat u \\hat v_x + \\hat v \\hat v_y + \\hat p_y - \\nu \\Delta \\hat v$, $r_c = \\hat u_x + \\hat v_y$.\n- **Test Case Parameters**: Three distinct test cases are provided with specific values for $\\nu$, network weights ($W_1, b_1, W_2, b_2$), hidden width ($H=3$), and collocation points.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is based on the fundamental principles of fluid dynamics (Navier-Stokes equations) and a standard methodology in computational science (Physics-Informed Neural Networks). All elements are well-established.\n- **Well-Posed**: The problem is an evaluation task. All necessary parameters and data are provided to compute a unique numerical value for the loss function in each case.\n- **Objective**: The problem is stated using precise mathematical definitions and objective computational instructions.\nAll other validation criteria are met. The problem is self-contained, consistent, and computationally feasible.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A detailed solution follows.\n\n### Solution Derivation\n\nThe core of the problem is to compute the residuals $r_u$, $r_v$, and $r_c$ for the neural network output $(\\hat u, \\hat v, \\hat p)$ at each collocation point. This requires calculating the first and second partial derivatives of the network outputs with respect to its inputs $(x, y, t)$. We use automatic differentiation, which is the systematic application of the chain rule.\n\nLet the input vector be $s = [s_1, s_2, s_3]^\\top = [x, y, t]^\\top$. The network architecture is given by:\n$$\no(s) = W_2 h + b_2 = W_2 \\tanh(W_1 s + b_1) + b_2\n$$\nHere, $o(s) = [o_1(s), o_2(s), o_3(s)]^\\top = [\\hat u(s), \\hat v(s), \\hat p(s)]^\\top$. Let the hidden layer pre-activation be $z = W_1 s + b_1$, and the post-activation be $h = \\tanh(z)$. The components are $z_i$ and $h_i = \\tanh(z_i)$ for $i=1, \\dots, H$.\n\n**1. First-Order Partial Derivatives**\n\nTo compute terms like $\\hat u_x = \\frac{\\partial \\hat u}{\\partial x}$, we apply the chain rule. The general expression for the partial derivative of the $k$-th output $o_k$ with respect to the $j$-th input $s_j$ is:\n$$\n\\frac{\\partial o_k}{\\partial s_j} = \\sum_{i=1}^H \\frac{\\partial o_k}{\\partial h_i} \\frac{\\partial h_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial s_j}\n$$\nThe individual components of this derivative are:\n- $\\frac{\\partial o_k}{\\partial h_i}$: From $o_k = \\sum_{l=1}^H (W_2)_{kl} h_l + (b_2)_k$, we get $\\frac{\\partial o_k}{\\partial h_i} = (W_2)_{ki}$.\n- $\\frac{\\partial h_i}{\\partial z_i}$: This is the derivative of the activation function, $\\frac{d}{dz_i} \\tanh(z_i) = 1 - \\tanh^2(z_i) = 1 - h_i^2$.\n- $\\frac{\\partial z_i}{\\partial s_j}$: From $z_i = \\sum_{l=1}^3 (W_1)_{il} s_l + (b_1)_i$, we get $\\frac{\\partial z_i}{\\partial s_j} = (W_1)_{ij}$.\n\nCombining these, we obtain the expression for the first-order partial derivatives:\n$$\n\\frac{\\partial o_k}{\\partial s_j} = \\sum_{i=1}^H (W_2)_{ki} (1 - h_i^2) (W_1)_{ij}\n$$\nThis expression can be written compactly in matrix form. Let $J_o$ be the Jacobian matrix of the output $o$ with respect to the input $s$, where $(J_o)_{kj} = \\frac{\\partial o_k}{\\partial s_j}$. Let $D(h)$ be a diagonal matrix with entries $D_{ii} = 1 - h_i^2$. Then, the Jacobian is:\n$$\nJ_o = W_2 D(h) W_1\n$$\nThe nine first-order derivatives required for the residuals can be extracted from this $3 \\times 3$ Jacobian matrix:\n$$\nJ_o = \\begin{bmatrix} \\partial \\hat u / \\partial x & \\partial \\hat u / \\partial y & \\partial \\hat u / \\partial t \\\\ \\partial \\hat v / \\partial x & \\partial \\hat v / \\partial y & \\partial \\hat v / \\partial t \\\\ \\partial \\hat p / \\partial x & \\partial \\hat p / \\partial y & \\partial \\hat p / \\partial t \\end{bmatrix} = \\begin{bmatrix} \\hat u_x & \\hat u_y & \\hat u_t \\\\ \\hat v_x & \\hat v_y & \\hat v_t \\\\ \\hat p_x & \\hat p_y & \\hat p_t \\end{bmatrix}\n$$\nThe incompressibility residual, $r_c$, is directly computed from the Jacobian: $r_c = \\hat u_x + \\hat v_y = (J_o)_{11} + (J_o)_{22}$. This explicitly explains how to compute the divergence of the velocity field $\\nabla \\cdot \\hat{\\boldsymbol{v}}$ via automatic differentiation.\n\n**2. Second-Order Partial Derivatives**\n\nThe momentum residuals $r_u$ and $r_v$ require the Laplacian of the velocity components, i.e., $\\Delta \\hat u = \\hat u_{xx} + \\hat u_{yy}$ and $\\Delta \\hat v = \\hat v_{xx} + \\hat v_{yy}$. We compute these by differentiating the first-order derivatives again. Let's find the general second-order derivative $\\frac{\\partial^2 o_k}{\\partial s_m \\partial s_j}$:\n$$\n\\frac{\\partial^2 o_k}{\\partial s_m \\partial s_j} = \\frac{\\partial}{\\partial s_m} \\left( \\sum_{i=1}^H (W_2)_{ki} (1 - h_i^2) (W_1)_{ij} \\right) = \\sum_{i=1}^H (W_2)_{ki} (W_1)_{ij} \\frac{\\partial}{\\partial s_m} (1 - h_i^2)\n$$\nThe derivative term is:\n$$\n\\frac{\\partial}{\\partial s_m} (1 - h_i^2) = -2 h_i \\frac{\\partial h_i}{\\partial s_m} = -2 h_i \\left( \\frac{\\partial h_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial s_m} \\right) = -2 h_i (1-h_i^2) (W_1)_{im}\n$$\nThis is equivalent to using the second derivative of the activation function: $\\frac{\\partial}{\\partial s_m} (\\tanh'(z_i)) = \\tanh''(z_i) \\frac{\\partial z_i}{\\partial s_m} = -2 \\tanh(z_i) (1-\\tanh^2(z_i)) (W_1)_{im}$.\nSubstituting this back, we get:\n$$\n\\frac{\\partial^2 o_k}{\\partial s_m \\partial s_j} = \\sum_{i=1}^H (W_2)_{ki} (W_1)_{ij} [-2 h_i (1 - h_i^2) (W_1)_{im}]\n$$\nWe need the diagonal Hessian terms for the spatial variables ($j=m=1$ for $x$ and $j=m=2$ for $y$).\n- For $\\hat u_{xx}$ ($k=1, j=m=1$):\n$$\n\\hat u_{xx} = \\frac{\\partial^2 \\hat u}{\\partial x^2} = \\sum_{i=1}^H (W_2)_{1i} (-2 h_i (1-h_i^2)) ((W_1)_{i1})^2\n$$\n- For $\\hat u_{yy}$ ($k=1, j=m=2$):\n$$\n\\hat u_{yy} = \\frac{\\partial^2 \\hat u}{\\partial y^2} = \\sum_{i=1}^H (W_2)_{1i} (-2 h_i (1-h_i^2)) ((W_1)_{i2})^2\n$$\nThe Laplacian for $\\hat u$ is the sum of these two terms:\n$$\n\\Delta \\hat u = \\hat u_{xx} + \\hat u_{yy} = \\sum_{i=1}^H (W_2)_{1i} [-2 h_i (1-h_i^2)] \\left( ((W_1)_{i1})^2 + ((W_1)_{i2})^2 \\right)\n$$\nSimilarly, for $\\hat v$ ($k=2$):\n$$\n\\Delta \\hat v = \\hat v_{xx} + \\hat v_{yy} = \\sum_{i=1}^H (W_2)_{2i} [-2 h_i (1-h_i^2)] \\left( ((W_1)_{i1})^2 + ((W_1)_{i2})^2 \\right)\n$$\n\n**3. Assembling the Loss Function**\n\nFor each collocation point $(x_k, y_k, t_k)$, the algorithm is as follows:\n1.  Compute the forward pass: $s = [x_k, y_k, t_k]^\\top$, $z = W_1 s + b_1$, $h = \\tanh(z)$, $o = W_2 h + b_2$. This gives $\\hat u, \\hat v, \\hat p$.\n2.  Compute all required derivatives: $\\hat u_x, \\hat u_y, \\hat u_t, \\hat v_x, \\hat v_y, \\hat v_t, \\hat p_x, \\hat p_y, \\Delta \\hat u, \\Delta \\hat v$ using the formulas derived above.\n3.  Evaluate the residuals:\n    $$\n    r_u = \\hat u_t + \\hat u \\hat u_x + \\hat v \\hat u_y + \\hat p_x - \\nu \\Delta \\hat u\n    $$\n    $$\n    r_v = \\hat v_t + \\hat u \\hat v_x + \\hat v \\hat v_y + \\hat p_y - \\nu \\Delta \\hat v\n    $$\n    $$\n    r_c = \\hat u_x + \\hat v_y\n    $$\n4.  Calculate the sum of squared residuals for this point: $r_u^2 + r_v^2 + r_c^2$.\n5.  Average this sum over all $N$ collocation points to get the final loss $\\mathcal{L}_{\\text{int}}$:\n    $$\n    \\mathcal{L}_{\\text{int}} = \\frac{1}{N} \\sum_{k=1}^N \\left( r_u(s_k)^2 + r_v(s_k)^2 + r_c(s_k)^2 \\right)\n    $$\nThis procedure provides a complete algorithm for computing the physics-informed loss, which is implemented in the following Python program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the physics-informed loss for the 2D Navier-Stokes equations\n    for three specified test cases.\n    \"\"\"\n    \n    # Define test cases as a list of dictionaries.\n    test_cases = [\n        {\n            \"name\": \"Test Case 1 (general viscosity)\",\n            \"nu\": 0.01,\n            \"W1\": np.array([\n                [0.5, -0.3, 0.2],\n                [-0.4, 0.7, -0.1],\n                [0.25, 0.1, -0.5]\n            ]),\n            \"b1\": np.array([0.1, -0.2, 0.05]),\n            \"W2\": np.array([\n                [0.8, -0.5, 0.3],\n                [-0.6, 0.4, 0.2],\n                [0.2, -0.1, 0.7]\n            ]),\n            \"b2\": np.array([0.0, 0.0, 0.0]),\n            \"points\": [\n                (0.2, 0.3, 0.1),\n                (0.7, 0.5, 0.4),\n                (0.9, 0.1, 0.9),\n                (0.0, 0.0, 0.0)\n            ]\n        },\n        {\n            \"name\": \"Test Case 2 (inviscid limit)\",\n            \"nu\": 0.0,\n            \"W1\": np.array([\n                [0.5, -0.3, 0.2],\n                [-0.4, 0.7, -0.1],\n                [0.25, 0.1, -0.5]\n            ]),\n            \"b1\": np.array([0.1, -0.2, 0.05]),\n            \"W2\": np.array([\n                [0.8, -0.5, 0.3],\n                [-0.6, 0.4, 0.2],\n                [0.2, -0.1, 0.7]\n            ]),\n            \"b2\": np.array([0.0, 0.0, 0.0]),\n            \"points\": [\n                (0.2, 0.3, 0.1),\n                (0.7, 0.5, 0.4),\n                (0.9, 0.1, 0.9),\n                (0.0, 0.0, 0.0)\n            ]\n        },\n        {\n            \"name\": \"Test Case 3 (trivial zero field)\",\n            \"nu\": 0.05,\n            \"W1\": np.zeros((3, 3)),\n            \"b1\": np.zeros(3),\n            \"W2\": np.zeros((3, 3)),\n            \"b2\": np.zeros(3),\n            \"points\": [\n                (0.1, 0.2, 0.3),\n                (0.4, 0.4, 0.4),\n                (0.9, 0.9, 0.9)\n            ]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        nu = case[\"nu\"]\n        W1 = case[\"W1\"]\n        b1 = case[\"b1\"]\n        W2 = case[\"W2\"]\n        b2 = case[\"b2\"]\n        points = case[\"points\"]\n        \n        total_squared_residual = 0.0\n        N = len(points)\n        \n        for x, y, t in points:\n            # Input vector\n            s = np.array([x, y, t])\n            \n            # --- Forward Pass ---\n            z = W1 @ s + b1\n            h = np.tanh(z)\n            o = W2 @ h + b2\n            u_hat, v_hat, p_hat = o[0], o[1], o[2]\n            \n            # --- First-Order Derivatives ---\n            # D(h) is diag(1 - tanh^2(z_i))\n            D_h = np.diag(1 - h**2)\n            # Jacobian J_o = W2 * D_h * W1\n            J_o = W2 @ D_h @ W1\n            \n            u_x, u_y, u_t = J_o[0, 0], J_o[0, 1], J_o[0, 2]\n            v_x, v_y, v_t = J_o[1, 0], J_o[1, 1], J_o[1, 2]\n            p_x, p_y     = J_o[2, 0], J_o[2, 1]\n            \n            # --- Second-Order Derivatives (Laplacians) ---\n            # Term for second derivative: -2 * h * (1 - h^2)\n            tanh_double_prime_term = -2 * h * (1 - h**2)\n            \n            # (W1_i1^2 + W1_i2^2) vector\n            W1_spatial_sq_sum = W1[:, 0]**2 + W1[:, 1]**2\n            \n            # Laplacian calculation\n            # lap_u = sum_i( W2_1i * (-2*h_i*(1-h_i^2)) * (W1_i1^2 + W1_i2^2) )\n            lap_u = np.sum(W2[0, :] * tanh_double_prime_term * W1_spatial_sq_sum)\n            lap_v = np.sum(W2[1, :] * tanh_double_prime_term * W1_spatial_sq_sum)\n            \n            # --- Residuals Calculation ---\n            r_c = u_x + v_y\n            r_u = u_t + u_hat * u_x + v_hat * u_y + p_x - nu * lap_u\n            r_v = v_t + u_hat * v_x + v_hat * v_y + p_y - nu * lap_v\n\n            # Sum of squared residuals for this point\n            total_squared_residual += r_u**2 + r_v**2 + r_c**2\n            \n        # Mean squared residual loss\n        loss = total_squared_residual / N\n        results.append(loss)\n\n    # Format the final output string\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}