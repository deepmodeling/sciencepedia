{
    "hands_on_practices": [
        {
            "introduction": "Before fitting complex nonlinear models became routine, scientists often transformed them into linear forms to simplify analysis and gain insight. This practice reveals the model's fundamental mathematical structure and its relationship to other statistical frameworks. This exercise  challenges you to normalize the classic Emax model and apply a logit transform, uncovering an elegant underlying linear relationship that is key to understanding dose-response theory.",
            "id": "3936364",
            "problem": "A receptor-mediated pharmacodynamic effect $E(C)$ is measured in an in vitro assay as a function of agonist concentration $C$. At zero concentration, the baseline effect is $E_{0}$, and as $C \\to \\infty$, the effect saturates at $E_{0} + E_{\\max}$. Assume a single class of noncooperative receptors with one-to-one binding and rapid equilibrium, so that the fraction of occupied receptors is given by the law of mass action. Further assume that the transduction from receptor occupancy to effect is linear across the dynamic range, so that the effect scales proportionally with occupancy between $E_{0}$ and $E_{0} + E_{\\max}$.\n\nLet $EC_{50}$ denote the half-maximal effective concentration, defined as the concentration $C$ at which the effect is halfway between $E_{0}$ and $E_{0} + E_{\\max}$. Define the normalized effect $y$ by $y = (E - E_{0})/E_{\\max}$ and the normalized concentration $x$ by $x = C/EC_{50}$.\n\nUsing only the above physical and definitional assumptions:\n- Derive the functional form of $y$ in terms of $x$.\n- Then, under the working assumption that residual errors become homoscedastic after a logit transformation of the mean response, construct an explicit transform that linearizes $y$ in terms of $\\ln C$.\n\nGive your final answer as a single closed-form analytic expression for the logit transform of $y$ in terms of $C$ and $EC_{50}$. Do not include any units in your answer.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It describes a standard derivation in pharmacodynamics, rooted in the law of mass action and linear systems theory, and presents a clear, solvable task. There are no contradictions, ambiguities, or factual inaccuracies.\n\nThe solution proceeds in two parts as requested by the problem statement.\n\nFirst, we derive the functional form of the normalized effect $y$ in terms of the normalized concentration $x$.\n\nThe binding of an agonist (concentration $C$) to a single class of noncooperative receptors is described by the reversible reaction $A + R \\rightleftharpoons AR$. At equilibrium, the law of mass action gives the dissociation constant $K_d$ as:\n$$K_d = \\frac{[A][R]}{[AR]}$$\nIn our notation, $[A] = C$. The total receptor concentration is $[R_T] = [R] + [AR]$. The fraction of occupied receptors, which we denote as $f_{\\text{occ}}$, is given by $f_{\\text{occ}} = \\frac{[AR]}{[R_T]}$.\nWe can express $[R]$ in terms of $[AR]$ and $C$: $[R] = \\frac{K_d [AR]}{C}$.\nSubstituting this into the expression for $[R_T]$:\n$$[R_T] = \\frac{K_d [AR]}{C} + [AR] = [AR] \\left( \\frac{K_d}{C} + 1 \\right) = [AR] \\left( \\frac{K_d + C}{C} \\right)$$\nNow we can write the fraction of occupied receptors as:\n$$f_{\\text{occ}} = \\frac{[AR]}{[R_T]} = \\frac{[AR]}{[AR] \\left( \\frac{K_d + C}{C} \\right)} = \\frac{C}{C + K_d}$$\nThe problem states that the transduction from receptor occupancy to effect is linear. The effect $E$ varies from a baseline of $E_0$ (at $f_{\\text{occ}}=0$) to a maximum of $E_0 + E_{\\max}$ (at $f_{\\text{occ}}=1$). The effect above baseline, $E - E_0$, is therefore directly proportional to the fraction of occupied receptors, with the constant of proportionality being the maximum possible effect above baseline, $E_{\\max}$.\n$$E(C) - E_0 = E_{\\max} \\cdot f_{\\text{occ}}$$\nSubstituting the expression for $f_{\\text{occ}}$ gives the simple $E_{\\max}$ model:\n$$E(C) - E_0 = E_{\\max} \\frac{C}{C + K_d}$$\nNext, we relate $K_d$ to the half-maximal effective concentration, $EC_{50}$. By definition, at $C = EC_{50}$, the effect is halfway between the baseline and maximum, i.e., $E(EC_{50}) = E_0 + \\frac{1}{2} E_{\\max}$.\nFrom our model, at $C = EC_{50}$, the effect above baseline is $E(EC_{50}) - E_0 = \\frac{1}{2} E_{\\max}$.\nSubstituting $C=EC_{50}$ into the model equation:\n$$E(EC_{50}) - E_0 = E_{\\max} \\frac{EC_{50}}{EC_{50} + K_d}$$\nEquating the two expressions for the effect at $EC_{50}$:\n$$\\frac{1}{2} E_{\\max} = E_{\\max} \\frac{EC_{50}}{EC_{50} + K_d}$$\n$$\\frac{1}{2} = \\frac{EC_{50}}{EC_{50} + K_d} \\implies EC_{50} + K_d = 2 EC_{50} \\implies K_d = EC_{50}$$\nThus, for this model, the dissociation constant is equal to the half-maximal effective concentration. We can now write the effect model in terms of $EC_{50}$:\n$$E(C) - E_0 = E_{\\max} \\frac{C}{C + EC_{50}}$$\nThe problem defines the normalized effect $y = \\frac{E - E_0}{E_{\\max}}$. From the equation above, we can directly identify $y$:\n$$y = \\frac{C}{C + EC_{50}}$$\nTo express $y$ in terms of the normalized concentration $x = C/EC_{50}$, we divide the numerator and denominator by $EC_{50}$:\n$$y = \\frac{C/EC_{50}}{(C/EC_{50}) + (EC_{50}/EC_{50})} = \\frac{x}{x + 1}$$\nThis completes the first part of the derivation.\n\nSecond, we construct the transform that linearizes $y$ in terms of $\\ln C$. The problem suggests a logit transformation of the mean response. The logit function applied to a variable $p \\in (0, 1)$ is defined as $\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)$. We apply this transformation to our normalized effect $y$.\n\nWe have the expression for $y$ in terms of $C$ and $EC_{50}$:\n$$y = \\frac{C}{C + EC_{50}}$$\nWe also need to find an expression for $1-y$:\n$$1 - y = 1 - \\frac{C}{C + EC_{50}} = \\frac{C + EC_{50}}{C + EC_{50}} - \\frac{C}{C + EC_{50}} = \\frac{EC_{50}}{C + EC_{50}}$$\nNow we form the ratio $\\frac{y}{1-y}$:\n$$\\frac{y}{1-y} = \\frac{\\left( \\frac{C}{C + EC_{50}} \\right)}{\\left( \\frac{EC_{50}}{C + EC_{50}} \\right)} = \\frac{C}{EC_{50}}$$\nApplying the natural logarithm gives the logit transform of $y$:\n$$\\text{logit}(y) = \\ln\\left(\\frac{y}{1-y}\\right) = \\ln\\left(\\frac{C}{EC_{50}}\\right)$$\nTo verify that this transformation linearizes $y$ with respect to $\\ln C$, we can use the properties of logarithms to rewrite the expression:\n$$\\text{logit}(y) = \\ln(C) - \\ln(EC_{50})$$\nThis is an equation of the form $Y = mX + b$, where the transformed variable is $Y = \\text{logit}(y)$, the independent variable is $X = \\ln(C)$, the slope is $m=1$, and the intercept is $b = -\\ln(EC_{50})$. This is a linear relationship.\n\nThe final answer requested is the single closed-form analytic expression for the logit transform of $y$ in terms of $C$ and $EC_{50}$. This is the expression we have just derived.",
            "answer": "$$\\boxed{\\ln\\left(\\frac{C}{EC_{50}}\\right)}$$"
        },
        {
            "introduction": "Fitting a nonlinear model like the Emax model to experimental data is typically accomplished by minimizing the sum of squared errors. Gradient-based optimization algorithms are the workhorses for this task, and they depend on the analytical derivatives of the objective function with respect to the model parameters. In this practice , you will derive the gradient of the nonlinear least squares objective function for the Emax model from first principles, a foundational skill for understanding how fitting algorithms operate and for developing custom estimation routines.",
            "id": "3936351",
            "problem": "A pharmacodynamic experiment measures effect values $E_{i}$ at administered drug concentrations $C_{i}$ for $i=1,\\dots,n$. Assume a standard saturating exposureâ€“response structure in which the expected effect as a function of concentration is modeled by the maximum effect (Emax) model\n$$\nE(C;\\theta)=E_{0}+E_{\\max}\\,\\frac{C}{EC_{50}+C},\n$$\nwhere $\\theta=(E_{0},E_{\\max},EC_{50})$ represents the baseline effect $E_{0}$, the maximal drug-attributable effect $E_{\\max}$, and the half-maximal effective concentration (EC50), respectively. Suppose the measurement errors are additive, independent, and identically distributed according to a Gaussian distribution with zero mean and common variance $\\sigma^{2}$, that is,\n$$\nE_{i}=E(C_{i};\\theta)+\\varepsilon_{i},\\quad \\varepsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2}),\\quad \\text{independent across }i.\n$$\nStarting from these assumptions and the Gaussian likelihood, derive the nonlinear least squares objective in the form\n$$\nS(\\theta)=\\sum_{i=1}^{n}\\bigl(E_{i}-E(C_{i};\\theta)\\bigr)^{2},\n$$\nand then, using only first principles and symbolic differentiation, derive the gradient $\\nabla S(\\theta)$ with respect to $\\theta=(E_{0},E_{\\max},EC_{50})$. Express your final answer as a closed-form analytical expression for the gradient components in terms of $(E_{i},C_{i})$, $E_{0}$, $E_{\\max}$, and $EC_{50}$. No rounding is required, and no units should be included in the final answer.",
            "solution": "The problem is valid as it is scientifically grounded in standard pharmacodynamic modeling and statistical principles, well-posed with sufficient information for a unique analytical derivation, and objectively stated.\n\nThe solution proceeds in two parts as requested. First, we derive the nonlinear least squares objective function, $S(\\theta)$, from the principle of maximum likelihood under the given Gaussian error model. Second, we derive the gradient of this objective function, $\\nabla S(\\theta)$, with respect to the parameter vector $\\theta=(E_{0},E_{\\max},EC_{50})$.\n\n**Part 1: Derivation of the Nonlinear Least Squares Objective**\n\nThe problem states that the measurement errors, $\\varepsilon_{i}$, are independent and identically distributed (i.i.d.) according to a Gaussian (normal) distribution with a mean of $0$ and a common variance of $\\sigma^{2}$. This is denoted as $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$. The probability density function (PDF) of a single error term $\\varepsilon_i$ is given by:\n$$\nf(\\varepsilon_{i}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{\\varepsilon_{i}^{2}}{2\\sigma^{2}}\\right)\n$$\nThe relationship between the observed effect $E_i$, the model prediction $E(C_i; \\theta)$, and the error is $E_i = E(C_i; \\theta) + \\varepsilon_i$. Thus, the error term can be expressed as $\\varepsilon_i = E_i - E(C_i; \\theta)$. Substituting this into the PDF, we obtain the conditional probability of observing $E_i$ given the concentration $C_i$ and the parameters $\\theta$ and $\\sigma^2$:\n$$\np(E_{i}|C_{i}, \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(E_{i}-E(C_{i};\\theta))^{2}}{2\\sigma^{2}}\\right)\n$$\nThe likelihood function, $L(\\theta, \\sigma^2)$, is the joint probability of observing the entire set of data points $\\{E_i\\}_{i=1}^n$. Since the errors are assumed to be independent, the joint probability is the product of the individual probabilities:\n$$\nL(\\theta, \\sigma^2) = \\prod_{i=1}^{n} p(E_{i}|C_{i}, \\theta, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(E_{i}-E(C_{i};\\theta))^{2}}{2\\sigma^{2}}\\right)\n$$\nThis can be simplified to:\n$$\nL(\\theta, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(E_{i}-E(C_{i};\\theta))^{2}\\right)\n$$\nThe principle of maximum likelihood estimation (MLE) states that we should choose the parameters that maximize this likelihood function. It is often more convenient to work with the natural logarithm of the likelihood function, the log-likelihood $\\ell(\\theta, \\sigma^2)$, because the logarithm is a monotonically increasing function. Maximizing the log-likelihood is equivalent to maximizing the likelihood itself.\n$$\n\\ell(\\theta, \\sigma^2) = \\ln(L(\\theta, \\sigma^2)) = \\ln\\left[\\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(E_{i}-E(C_{i};\\theta))^{2}\\right)\\right]\n$$\nUsing the properties of logarithms, this becomes:\n$$\n\\ell(\\theta, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(E_{i}-E(C_{i};\\theta))^{2}\n$$\nTo find the optimal parameter vector $\\theta$, we need to maximize $\\ell(\\theta, \\sigma^2)$ with respect to $\\theta$. Observe that the first term, $-\\frac{n}{2}\\ln(2\\pi\\sigma^2)$, and the factor $-\\frac{1}{2\\sigma^2}$ in the second term do not depend on $\\theta$ (assuming $\\sigma^2$ is not a function of $\\theta$). Therefore, maximizing $\\ell(\\theta, \\sigma^2)$ with respect to $\\theta$ is equivalent to minimizing the sum of squared differences term:\n$$\nS(\\theta) = \\sum_{i=1}^{n}(E_{i}-E(C_{i};\\theta))^{2}\n$$\nThis is precisely the nonlinear least squares objective function as stated in the problem.\n\n**Part 2: Derivation of the Gradient $\\nabla S(\\theta)$**\n\nThe gradient of $S(\\theta)$ is a vector of its partial derivatives with respect to each component of $\\theta=(E_{0},E_{\\max},EC_{50})$.\n$$\n\\nabla S(\\theta) = \\left( \\frac{\\partial S}{\\partial E_{0}}, \\frac{\\partial S}{\\partial E_{\\max}}, \\frac{\\partial S}{\\partial EC_{50}} \\right)\n$$\nWe compute each component separately using the chain rule. For a generic parameter $p \\in \\{E_0, E_{\\max}, EC_{50}\\}$, the partial derivative is:\n$$\n\\frac{\\partial S}{\\partial p} = \\frac{\\partial}{\\partial p} \\sum_{i=1}^{n}(E_{i}-E(C_{i};\\theta))^{2} = \\sum_{i=1}^{n} 2(E_{i}-E(C_{i};\\theta)) \\cdot \\frac{\\partial}{\\partial p}(-E(C_{i};\\theta))\n$$\n$$\n\\frac{\\partial S}{\\partial p} = -2\\sum_{i=1}^{n}(E_{i}-E(C_{i};\\theta)) \\frac{\\partial E(C_{i};\\theta)}{\\partial p}\n$$\nThe model function is $E(C;\\theta)=E_{0}+E_{\\max}\\,\\frac{C}{EC_{50}+C}$. We now find the partial derivatives of $E(C_i;\\theta)$ with respect to each parameter.\n\n1.  **Partial derivative with respect to $E_0$:**\n    $$\n    \\frac{\\partial E(C_{i};\\theta)}{\\partial E_{0}} = \\frac{\\partial}{\\partial E_{0}}\\left(E_{0}+E_{\\max}\\,\\frac{C_{i}}{EC_{50}+C_{i}}\\right) = 1\n    $$\n    Substituting this into the general formula for the gradient component:\n    $$\n    \\frac{\\partial S}{\\partial E_{0}} = -2\\sum_{i=1}^{n}(E_{i}-E(C_{i};\\theta)) \\cdot 1 = -2\\sum_{i=1}^{n}\\left(E_{i} - \\left(E_{0}+E_{\\max}\\,\\frac{C_{i}}{EC_{50}+C_{i}}\\right)\\right)\n    $$\n\n2.  **Partial derivative with respect to $E_{\\max}$:**\n    $$\n    \\frac{\\partial E(C_{i};\\theta)}{\\partial E_{\\max}} = \\frac{\\partial}{\\partial E_{\\max}}\\left(E_{0}+E_{\\max}\\,\\frac{C_{i}}{EC_{50}+C_{i}}\\right) = \\frac{C_{i}}{EC_{50}+C_{i}}\n    $$\n    Substituting this into the general formula:\n    $$\n    \\frac{\\partial S}{\\partial E_{\\max}} = -2\\sum_{i=1}^{n}(E_{i}-E(C_{i};\\theta)) \\left(\\frac{C_{i}}{EC_{50}+C_{i}}\\right) = -2\\sum_{i=1}^{n}\\left(E_{i} - \\left(E_{0}+E_{\\max}\\,\\frac{C_{i}}{EC_{50}+C_{i}}\\right)\\right)\\left(\\frac{C_{i}}{EC_{50}+C_{i}}\\right)\n    $$\n\n3.  **Partial derivative with respect to $EC_{50}$:**\n    Here we use the quotient rule for differentiation. Let $u = C_i$ and $v = EC_{50}+C_i$. Then $\\frac{\\partial u}{\\partial EC_{50}} = 0$ and $\\frac{\\partial v}{\\partial EC_{50}} = 1$.\n    $$\n    \\frac{\\partial E(C_{i};\\theta)}{\\partial EC_{50}} = \\frac{\\partial}{\\partial EC_{50}}\\left(E_{0}+E_{\\max}\\,\\frac{C_{i}}{EC_{50}+C_{i}}\\right) = E_{\\max} \\frac{\\partial}{\\partial EC_{50}}\\left(\\frac{C_{i}}{EC_{50}+C_{i}}\\right)\n    $$\n    $$\n    \\frac{\\partial}{\\partial EC_{50}}\\left(\\frac{C_{i}}{EC_{50}+C_{i}}\\right) = \\frac{0 \\cdot (EC_{50}+C_{i}) - C_i \\cdot 1}{(EC_{50}+C_{i})^2} = -\\frac{C_{i}}{(EC_{50}+C_{i})^{2}}\n    $$\n    Therefore,\n    $$\n    \\frac{\\partial E(C_{i};\\theta)}{\\partial EC_{50}} = -E_{\\max}\\frac{C_{i}}{(EC_{50}+C_{i})^{2}}\n    $$\n    Substituting this into the general formula:\n    $$\n    \\frac{\\partial S}{\\partial EC_{50}} = -2\\sum_{i=1}^{n}(E_{i}-E(C_{i};\\theta)) \\left(-E_{\\max}\\frac{C_{i}}{(EC_{50}+C_{i})^{2}}\\right)\n    $$\n    $$\n    \\frac{\\partial S}{\\partial EC_{50}} = 2\\sum_{i=1}^{n}(E_{i}-E(C_{i};\\theta)) \\left(E_{\\max}\\frac{C_{i}}{(EC_{50}+C_{i})^{2}}\\right) = 2 E_{\\max}\\sum_{i=1}^{n}\\left(E_{i} - \\left(E_{0}+E_{\\max}\\,\\frac{C_{i}}{EC_{50}+C_{i}}\\right)\\right)\\frac{C_{i}}{(EC_{50}+C_{i})^{2}}\n    $$\nThese three expressions constitute the components of the gradient vector $\\nabla S(\\theta)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -2\\sum_{i=1}^{n}\\left(E_{i} - E_{0} - E_{\\max}\\frac{C_{i}}{EC_{50}+C_{i}}\\right)  -2\\sum_{i=1}^{n}\\left(E_{i} - E_{0} - E_{\\max}\\frac{C_{i}}{EC_{50}+C_{i}}\\right)\\frac{C_{i}}{EC_{50}+C_{i}}  2 E_{\\max}\\sum_{i=1}^{n}\\left(E_{i} - E_{0} - E_{\\max}\\frac{C_{i}}{EC_{50}+C_{i}}\\right)\\frac{C_{i}}{(EC_{50}+C_{i})^{2}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The performance of modern statistical algorithms, such as Hamiltonian Monte Carlo (HMC), is highly sensitive to the model's parameterization, as a poorly-conditioned parameter space can hinder algorithmic efficiency. A clever reparameterization can often resolve these issues by improving the geometry of the likelihood surface. This computational exercise  guides you to implement a logarithmic reparameterization for the sigmoid Emax model and quantify its benefits by analyzing the Fisher Information Matrix, demonstrating how a simple transformation can lead to more robust and efficient parameter estimation.",
            "id": "3936398",
            "problem": "Consider the sigmoidal maximal effect model (often called the sigmoid $E_{\\max}$ model) used in pharmacodynamics to relate drug concentration to effect. Let the observed effect at concentration $C$ be modeled as $E(C) = E_0 + E_{\\max} \\dfrac{C^n}{EC_{50}^n + C^n}$, where $E_0$ is the baseline effect, $E_{\\max}$ is the maximal drug effect, $EC_{50}$ is the concentration that produces half-maximal effect, and $n$ is the Hill coefficient controlling steepness. Assume Independent and Identically Distributed (IID) Gaussian measurement noise: each observation $y_i$ at concentration $C_i$ satisfies $y_i \\sim \\mathcal{N}(E(C_i), \\sigma^2)$, with known variance $\\sigma^2$.\n\nBegin from the fundamental base that, under Gaussian noise, the expected Fisher information matrix for parameters $\\mathbf{p}$ is given by $I(\\mathbf{p}) = \\dfrac{1}{\\sigma^2} \\sum_{i=1}^N \\nabla_{\\mathbf{p}} E(C_i) \\, \\nabla_{\\mathbf{p}} E(C_i)^\\top$, where $\\nabla_{\\mathbf{p}} E(C_i)$ denotes the gradient of the mean function with respect to the parameter vector at concentration $C_i$. For the original parameterization $\\mathbf{p} = (EC_{50}, n)$, this matrix reflects the local curvature of the negative log-likelihood and thus the local geometry relevant to Hamiltonian Monte Carlo (HMC) proposal dynamics. Consider a reparameterization $\\boldsymbol{\\psi} = (\\theta, \\phi)$ defined by $\\theta = \\log EC_{50}$ and $\\phi = \\log n$, which maps the constrained, positive parameter space for $(EC_{50}, n)$ onto the unconstrained real space for $(\\theta, \\phi)$.\n\nYour task is to implement this reparameterization and, using the Fisher information framework, quantify how it alters local geometry. Specifically:\n\n- Derive the gradient $\\nabla_{\\mathbf{p}} E(C)$ for $\\mathbf{p} = (EC_{50}, n)$ starting from the model definition and basic differentiation rules. Then use the chain rule to derive $\\nabla_{\\boldsymbol{\\psi}} E(C)$ for $\\boldsymbol{\\psi} = (\\theta, \\phi)$.\n- For each provided test case, compute two expected Fisher information matrices: $I_{\\text{orig}}$ for the original parameters $(EC_{50}, n)$ and $I_{\\text{repar}}$ for the reparameterized $(\\theta, \\phi)$.\n- For each matrix, compute its spectral condition number $\\kappa = \\dfrac{\\lambda_{\\max}}{\\lambda_{\\min}}$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of the symmetric positive semidefinite matrix. When $\\lambda_{\\min}$ is numerically nonpositive or extremely small (e.g., below $10^{-12}$ due to design degeneracy), treat the condition number as $+\\infty$.\n- For each test case, output the scalar ratio $r = \\dfrac{\\kappa_{\\text{orig}}}{\\kappa_{\\text{repar}}}$, which reflects the improvement in isotropy (values greater than $1$ indicate that the reparameterization yields a smaller condition number and hence more isotropic local geometry).\n\nUse the following constants and units for all computations:\n- Baseline effect $E_0 = 0$ in arbitrary units (a.u.).\n- Maximal effect $E_{\\max} = 100$ in a.u.\n- Noise standard deviation $\\sigma = 5$ in a.u.\n- Concentrations $C_i$ in micromoles per liter ($\\mu\\text{mol}/\\text{L}$); final numerical outputs are unitless floats.\n\nTest suite (each case uses the dose design $[0.1, 0.5, 1.0, 3.0, 10.0, 30.0]$ in $\\mu\\text{mol}/\\text{L}$):\n- Case $1$ (happy path): $EC_{50} = 3.0$, $n = 2.0$.\n- Case $2$ (steep response): $EC_{50} = 3.0$, $n = 5.0$.\n- Case $3$ (large $EC_{50}$ relative to doses): $EC_{50} = 30.0$, $n = 2.0$.\n- Case $4$ (small $EC_{50}$ relative to doses): $EC_{50} = 0.3$, $n = 2.0$.\n\nYour program should compute $r$ for each case in the given order and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1, r_2, r_3, r_4]$.\n\nAll answers must be floats. The final outputs are unitless. Angles are not involved, so no angle unit specification is needed.",
            "solution": "The objective is to analyze the improvement in the local geometric properties of a statistical model's parameter space when undergoing a reparameterization. This is achieved by comparing the spectral condition number of the Fisher Information Matrix (FIM) for the original and reparameterized sigmoid $E_{\\max}$ model. A smaller condition number indicates a more isotropic (spherical) geometry for the negative log-likelihood surface, which is beneficial for sampling algorithms like Hamiltonian Monte Carlo (HMC).\n\nThe sigmoid $E_{\\max}$ model relates drug concentration $C$ to an observable effect $E(C)$ via the equation:\n$$\nE(C) = E_0 + E_{\\max} \\frac{C^n}{EC_{50}^n + C^n}\n$$\nThe parameters for this problem are $E_0 = 0$, $E_{\\max} = 100$, and noise standard deviation $\\sigma = 5$. The parameter vector to be estimated is originally $\\mathbf{p} = (EC_{50}, n)^\\top$. The measurement noise is assumed to be IID Gaussian, $y_i \\sim \\mathcal{N}(E(C_i), \\sigma^2)$.\n\nThe expected Fisher Information Matrix (FIM) for a parameter vector $\\mathbf{p}$ is given by:\n$$\nI(\\mathbf{p}) = \\frac{1}{\\sigma^2} \\sum_{i=1}^N \\nabla_{\\mathbf{p}} E(C_i) \\, \\nabla_{\\mathbf{p}} E(C_i)^\\top\n$$\nwhere $N$ is the number of concentration points $C_i$.\n\nThe reparameterization is defined by $\\theta = \\log EC_{50}$ and $\\phi = \\log n$, yielding the new parameter vector $\\boldsymbol{\\psi} = (\\theta, \\phi)^\\top$. This transforms the parameter space from $(0, \\infty) \\times (0, \\infty)$ for $(EC_{50}, n)$ to $(-\\infty, \\infty) \\times (-\\infty, \\infty)$ for $(\\theta, \\phi)$.\n\nThe solution proceeds in four steps:\n1.  Derivation of the gradient of the mean function $E(C)$ with respect to the original parameters $\\mathbf{p} = (EC_{50}, n)$.\n2.  Derivation of the gradient with respect to the reparameterized parameters $\\boldsymbol{\\psi} = (\\theta, \\phi)$ using the chain rule.\n3.  Construction of the FIM for both parameterizations, $I_{\\text{orig}}$ and $I_{\\text{repar}}$.\n4.  Computation of the condition numbers and their ratio for each test case.\n\n**1. Gradient with respect to Original Parameters $\\mathbf{p} = (EC_{50}, n)$**\nThe gradient vector is $\\nabla_{\\mathbf{p}} E(C) = \\begin{pmatrix} \\partial E / \\partial EC_{50} \\\\ \\partial E / \\partial n \\end{pmatrix}$. With $E_0=0$, we have $E(C) = E_{\\max} \\frac{C^n}{EC_{50}^n + C^n}$.\n\nThe partial derivative with respect to $EC_{50}$ is:\n$$\n\\frac{\\partial E}{\\partial EC_{50}} = E_{\\max} \\cdot C^n \\cdot \\frac{\\partial}{\\partial EC_{50}} (EC_{50}^n + C^n)^{-1} = E_{\\max} C^n (-1) (EC_{50}^n + C^n)^{-2} (n \\cdot EC_{50}^{n-1})\n$$\n$$\n\\frac{\\partial E}{\\partial EC_{50}} = -E_{\\max} \\frac{n C^n EC_{50}^{n-1}}{(EC_{50}^n + C^n)^2}\n$$\n\nThe partial derivative with respect to $n$ is found using the quotient rule on $f(n) = \\frac{C^n}{EC_{50}^n + C^n}$. Note that $\\frac{d}{dn}(a^n) = a^n \\log a$.\n$$\n\\frac{\\partial E}{\\partial n} = E_{\\max} \\frac{(\\frac{\\partial}{\\partial n} C^n)(EC_{50}^n + C^n) - C^n(\\frac{\\partial}{\\partial n}(EC_{50}^n + C^n))}{(EC_{50}^n + C^n)^2}\n$$\n$$\n\\frac{\\partial E}{\\partial n} = E_{\\max} \\frac{(C^n \\log C)(EC_{50}^n + C^n) - C^n(EC_{50}^n \\log EC_{50} + C^n \\log C)}{(EC_{50}^n + C^n)^2}\n$$\n$$\n\\frac{\\partial E}{\\partial n} = E_{\\max} \\frac{C^n EC_{50}^n (\\log C - \\log EC_{50})}{(EC_{50}^n + C^n)^2} = E_{\\max} \\frac{C^n EC_{50}^n \\log(C/EC_{50})}{(EC_{50}^n + C^n)^2}\n$$\n\n**2. Gradient with respect to Reparameterized Parameters $\\boldsymbol{\\psi} = (\\theta, \\phi)$**\nThe gradient with respect to $\\boldsymbol{\\psi} = (\\theta, \\phi)^\\top$ is found using the chain rule, where $EC_{50} = e^\\theta$ and $n = e^\\phi$.\n$$\n\\frac{\\partial E}{\\partial \\theta} = \\frac{\\partial E}{\\partial EC_{50}} \\frac{\\partial EC_{50}}{\\partial \\theta} = \\frac{\\partial E}{\\partial EC_{50}} \\cdot e^\\theta = \\frac{\\partial E}{\\partial EC_{50}} \\cdot EC_{50}\n$$\n$$\n\\frac{\\partial E}{\\partial \\phi} = \\frac{\\partial E}{\\partial n} \\frac{\\partial n}{\\partial \\phi} = \\frac{\\partial E}{\\partial n} \\cdot e^\\phi = \\frac{\\partial E}{\\partial n} \\cdot n\n$$\nSubstituting the expressions from Step 1:\n$$\n\\frac{\\partial E}{\\partial \\theta} = \\left( -E_{\\max} \\frac{n C^n EC_{50}^{n-1}}{(EC_{50}^n + C^n)^2} \\right) \\cdot EC_{50} = -E_{\\max} \\frac{n C^n EC_{50}^{n}}{(EC_{50}^n + C^n)^2}\n$$\n$$\n\\frac{\\partial E}{\\partial \\phi} = \\left( E_{\\max} \\frac{C^n EC_{50}^n \\log(C/EC_{50})}{(EC_{50}^n + C^n)^2} \\right) \\cdot n = E_{\\max} \\frac{n C^n EC_{50}^n \\log(C/EC_{50})}{(EC_{50}^n + C^n)^2}\n$$\nThe gradient vector is $\\nabla_{\\boldsymbol{\\psi}} E(C) = \\begin{pmatrix} \\partial E / \\partial \\theta \\\\ \\partial E / \\partial \\phi \\end{pmatrix}$.\n\n**3. FIM Construction**\nFor each test case, we are given a set of parameter values $(EC_{50}, n)$ and a fixed set of concentrations $C_i \\in \\{0.1, 0.5, 1.0, 3.0, 10.0, 30.0\\}$. For each concentration $C_i$, the gradient vectors $\\nabla_{\\mathbf{p}} E(C_i)$ and $\\nabla_{\\boldsymbol{\\psi}} E(C_i)$ are calculated. The FIMs are then computed by summing the outer products of these vectors over all concentrations and scaling by $1/\\sigma^2$:\n$$\nI_{\\text{orig}} = \\frac{1}{\\sigma^2} \\sum_{i=1}^6 \\nabla_{\\mathbf{p}} E(C_i) \\, (\\nabla_{\\mathbf{p}} E(C_i))^\\top\n$$\n$$\nI_{\\text{repar}} = \\frac{1}{\\sigma^2} \\sum_{i=1}^6 \\nabla_{\\boldsymbol{\\psi}} E(C_i) \\, (\\nabla_{\\boldsymbol{\\psi}} E(C_i))^\\top\n$$\n\n**4. Condition Number and Ratio**\nThe FIMs are symmetric positive semidefinite $2 \\times 2$ matrices. For each FIM, we compute its eigenvalues, $\\lambda_{\\max}$ and $\\lambda_{\\min}$. The spectral condition number $\\kappa$ is the ratio of the largest to the smallest eigenvalue:\n$$\n\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\n$$\nIf $\\lambda_{\\min}$ is numerically indistinguishable from zero (specifically, if $\\lambda_{\\min} \\le 10^{-12}$), the matrix is considered singular for practical purposes, and the condition number is taken to be $+\\infty$. This indicates that the parameters are not locally identifiable from the given experimental design.\n\nFinally, the improvement from reparameterization is quantified by the ratio $r$:\n$$\nr = \\frac{\\kappa_{\\text{orig}}}{\\kappa_{\\text{repar}}}\n$$\nA ratio $r  1$ signifies that the reparameterization has made the local geometry more isotropic, improving the conditioning of the parameter estimation problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the ratio of condition numbers of the Fisher Information Matrix\n    for original and reparameterized sigmoid Emax models.\n    \"\"\"\n    # Define constants and experimental design\n    E_MAX = 100.0\n    SIGMA = 5.0\n    CONCENTRATIONS = np.array([0.1, 0.5, 1.0, 3.0, 10.0, 30.0])\n    \n    # Define test cases: (EC50, n)\n    test_cases = [\n        (3.0, 2.0),\n        (3.0, 5.0),\n        (30.0, 2.0),\n        (0.3, 2.0),\n    ]\n\n    # Threshold for treating an eigenvalue as zero\n    LAMBDA_MIN_TOL = 1e-12\n\n    def gradient_p(C, ec50, n):\n        \"\"\"\n        Computes the gradient of E(C) w.r.t. original parameters p = (EC50, n).\n        \"\"\"\n        C_pow_n = C**n\n        ec50_pow_n = ec50**n\n        denominator = ec50_pow_n + C_pow_n\n        \n        # dE/dEC50\n        dE_dEC50_num = -E_MAX * n * C_pow_n * ec50**(n - 1)\n        dE_dEC50 = dE_dEC50_num / (denominator**2)\n        \n        # dE/dn\n        # Note: np.log is the natural logarithm\n        log_C_div_ec50 = np.log(C / ec50)\n        dE_dn_num = E_MAX * C_pow_n * ec50_pow_n * log_C_div_ec50\n        dE_dn = dE_dn_num / (denominator**2)\n        \n        return np.array([dE_dEC50, dE_dn])\n\n    def gradient_psi(C, ec50, n):\n        \"\"\"\n        Computes the gradient of E(C) w.r.t. reparameterized parameters psi = (log(EC50), log(n)).\n        This is done via the chain rule using the gradient w.r.t. p.\n        \"\"\"\n        grad_p_vec = gradient_p(C, ec50, n)\n        \n        # dE/d(theta) = dE/dEC50 * dEC50/d(theta) = dE/dEC50 * ec50\n        dE_dtheta = grad_p_vec[0] * ec50\n        \n        # dE/d(phi) = dE/dn * dn/d(phi) = dE/dn * n\n        dE_dphi = grad_p_vec[1] * n\n        \n        return np.array([dE_dtheta, dE_dphi])\n\n    def calculate_fim(grad_func, concentrations, params):\n        \"\"\"\n        Calculates the Fisher Information Matrix.\n        \"\"\"\n        ec50, n = params\n        fim = np.zeros((2, 2))\n        for c in concentrations:\n            if c = 0: continue # Should not happen based on problem\n            g = grad_func(c, ec50, n)\n            outer_product = np.outer(g, g)\n            fim += outer_product\n            \n        return fim / (SIGMA**2)\n\n    def get_condition_number(fim):\n        \"\"\"\n        Calculates the spectral condition number of a matrix.\n        \"\"\"\n        # eigvalsh is for symmetric/Hermitian matrices and is more stable\n        eigenvalues = np.linalg.eigvalsh(fim)\n        lambda_min = np.min(eigenvalues)\n        lambda_max = np.max(eigenvalues)\n        \n        if lambda_min = LAMBDA_MIN_TOL:\n            return np.inf\n        \n        return lambda_max / lambda_min\n\n    results = []\n    for ec50, n in test_cases:\n        params = (ec50, n)\n        \n        # Calculate FIM for original parameterization\n        fim_orig = calculate_fim(gradient_p, CONCENTRATIONS, params)\n        kappa_orig = get_condition_number(fim_orig)\n        \n        # Calculate FIM for reparameterization\n        fim_repar = calculate_fim(gradient_psi, CONCENTRATIONS, params)\n        kappa_repar = get_condition_number(fim_repar)\n        \n        # Calculate the improvement ratio\n        # np.inf / number - inf\n        # number / np.inf - 0.0\n        # np.inf / np.inf - nan\n        if kappa_repar == 0:\n            # Avoid division by zero if kappa_repar is unexpectedly zero\n            # while kappa_orig is non-zero\n            ratio = np.inf if kappa_orig > 0 else 0\n        else:\n            ratio = kappa_orig / kappa_repar\n        \n        results.append(ratio)\n    \n    # Use numpy.nan_to_num to handle potential inf/inf cases if needed,\n    # though the problem is structured to likely avoid this. For now,\n    # default string conversion of nan is acceptable.\n    # For a more controlled output, one could replace nan with a specific string.\n    \n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}