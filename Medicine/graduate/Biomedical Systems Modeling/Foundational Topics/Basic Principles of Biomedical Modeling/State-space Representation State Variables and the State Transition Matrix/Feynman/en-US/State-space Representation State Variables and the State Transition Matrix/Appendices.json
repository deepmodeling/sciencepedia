{
    "hands_on_practices": [
        {
            "introduction": "The state transition matrix, $\\Phi(t)$, is the fundamental operator that maps initial states to future states in a linear system. While its computation is straightforward for diagonalizable systems, many real-world models feature defective dynamics. This exercise  provides essential practice in deriving $\\Phi(t)$ for a non-diagonalizable system by constructing its Jordan normal form, a crucial skill for accurately modeling systems with coupled, identical decay rates.",
            "id": "3930758",
            "problem": "Consider a linear time-invariant two-compartment biomedical exchange model, where $x_{1}(t)$ and $x_{2}(t)$ denote the amounts of a substance in two well-mixed compartments. Assume the following physiologically plausible processes governed by first-order kinetics: an irreversible transfer from compartment $1$ to compartment $2$ at rate constant $k$ and first-order elimination from each compartment. Suppose the total first-order outflow rate from each compartment is identical and equal to a fixed positive constant $\\alpha$, with $0 \\le k \\le \\alpha$. This means the elimination rate constant from compartment $1$ equals $\\alpha - k$ and the elimination rate constant from compartment $2$ equals $\\alpha$. Under mass-action kinetics, the state-space model is\n$$\n\\frac{d}{dt}\\begin{pmatrix} x_{1}(t) \\\\ x_{2}(t) \\end{pmatrix}\n= A \\begin{pmatrix} x_{1}(t) \\\\ x_{2}(t) \\end{pmatrix}, \\quad\nA \\equiv \\begin{pmatrix} -\\alpha & 0 \\\\ k & -\\alpha \\end{pmatrix}.\n$$\nLet $\\Phi(t)$ denote the state transition matrix (also called the fundamental solution matrix), defined as the unique matrix function satisfying the homogeneous linear ordinary differential equation (ODE) $\\frac{d}{dt}\\Phi(t) = A \\Phi(t)$ and the initial condition $\\Phi(0)=I$, where $I$ is the identity. The matrix $A$ has a repeated eigenvalue and is non-diagonalizable for $k \\ne 0$.\n\nStarting only from foundational definitions of the state transition matrix and the matrix exponential, and from the Jordan-chain construction for defective eigenvalues, derive $\\Phi(t)$ using a Jordan-form-based argument for the defective case. Express the final answer as a single closed-form symbolic expression for $\\Phi(t)$ in terms of $t$, $\\alpha$, and $k$. Do not approximate any expressions; no rounding is required. Your final answer must be the explicit symbolic expression for $\\Phi(t)$.",
            "solution": "The problem requires the derivation of the state transition matrix $\\Phi(t)$ for the given linear time-invariant (LTI) system defined by the matrix $A = \\begin{pmatrix} -\\alpha & 0 \\\\ k & -\\alpha \\end{pmatrix}$. The derivation must be based on the Jordan decomposition of $A$, as specified for the defective case where $k \\neq 0$.\n\nThe state transition matrix $\\Phi(t)$ for an LTI system $\\frac{d}{dt}\\mathbf{x}(t) = A \\mathbf{x}(t)$ is defined as the unique matrix satisfying $\\frac{d}{dt}\\Phi(t) = A \\Phi(t)$ with the initial condition $\\Phi(0) = I$, where $I$ is the identity matrix. The solution is given by the matrix exponential, $\\Phi(t) = \\exp(At)$.\n\nTo compute $\\exp(At)$ using the Jordan form, we seek a decomposition $A = PJP^{-1}$, where $J$ is the Jordan normal form of $A$ and $P$ is the matrix of eigenvectors and generalized eigenvectors. Then, $\\exp(At) = P \\exp(Jt) P^{-1}$.\n\nFirst, we find the eigenvalues of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix} -\\alpha - \\lambda & 0 \\\\ k & -\\alpha - \\lambda \\end{pmatrix} = (-\\alpha - \\lambda)^2 - (0)(k) = (\\lambda + \\alpha)^2 = 0.\n$$\nThis yields a repeated eigenvalue $\\lambda_{1,2} = -\\alpha$. The algebraic multiplicity of the eigenvalue $\\lambda = -\\alpha$ is $2$.\n\nNext, we find the eigenvectors corresponding to $\\lambda = -\\alpha$ by solving $(A - \\lambda I)\\mathbf{v} = \\mathbf{0}$, which is $(A + \\alpha I)\\mathbf{v} = \\mathbf{0}$.\n$$\n\\begin{pmatrix} -\\alpha+\\alpha & 0 \\\\ k & -\\alpha+\\alpha \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ k & 0 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nThis matrix equation reduces to the single scalar equation $kv_1 = 0$. Since the problem concerns the defective case $k \\neq 0$, it must be that $v_1 = 0$. The component $v_2$ is arbitrary. The eigenspace is spanned by a single eigenvector. Choosing $v_2 = 1$, we obtain the eigenvector $\\mathbf{v} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The geometric multiplicity of the eigenvalue $\\lambda = -\\alpha$ is $1$, which is less than its algebraic multiplicity of $2$. Therefore, the matrix $A$ is defective (non-diagonalizable) for $k \\ne 0$.\n\nTo form a basis for $\\mathbb{R}^2$, we must find a generalized eigenvector $\\mathbf{w}$ that forms a Jordan chain with $\\mathbf{v}$. The generalized eigenvector $\\mathbf{w}$ is found by solving $(A - \\lambda I)\\mathbf{w} = \\mathbf{v}$.\n$$\n(A + \\alpha I)\\mathbf{w} = \\mathbf{v} \\implies \\begin{pmatrix} 0 & 0 \\\\ k & 0 \\end{pmatrix} \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\nThis gives the equation $kw_1 = 1$, so $w_1 = 1/k$. The component $w_2$ can be chosen arbitrarily; for simplicity, we set $w_2 = 0$. This gives the generalized eigenvector $\\mathbf{w} = \\begin{pmatrix} 1/k \\\\ 0 \\end{pmatrix}$.\n\nThe modal matrix $P$ is constructed with the eigenvector $\\mathbf{v}$ and the generalized eigenvector $\\mathbf{w}$ as its columns: $P = [\\mathbf{v}, \\mathbf{w}]$.\n$$\nP = \\begin{pmatrix} 0 & 1/k \\\\ 1 & 0 \\end{pmatrix}.\n$$\nThe Jordan normal form $J$ is structured according to the Jordan chain:\n$$\nJ = \\begin{pmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{pmatrix} = \\begin{pmatrix} -\\alpha & 1 \\\\ 0 & -\\alpha \\end{pmatrix}.\n$$\nNext, we compute the inverse of $P$. For a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant of $P$ is $\\det(P) = (0)(0) - (1/k)(1) = -1/k$.\n$$\nP^{-1} = \\frac{1}{-1/k} \\begin{pmatrix} 0 & -1/k \\\\ -1 & 0 \\end{pmatrix} = -k \\begin{pmatrix} 0 & -1/k \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ k & 0 \\end{pmatrix}.\n$$\nNow, we compute the matrix exponential of $Jt$. The matrix $Jt$ can be written as $Jt = -\\alpha t I + N t$, where $N = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$. Since $-\\alpha t I$ is a scalar matrix, it commutes with any matrix, including $N t$. Therefore, $\\exp(Jt) = \\exp(-\\alpha t I) \\exp(N t)$.\n$$\n\\exp(-\\alpha t I) = \\exp(-\\alpha t) I = \\begin{pmatrix} \\exp(-\\alpha t) & 0 \\\\ 0 & \\exp(-\\alpha t) \\end{pmatrix}.\n$$\nThe matrix $N$ is nilpotent, with $N^2 = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\mathbf{0}$. The Taylor series for $\\exp(Nt)$ thus terminates.\n$$\n\\exp(Nt) = I + Nt + \\frac{(Nt)^2}{2!} + \\dots = I + tN = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + t\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & t \\\\ 0 & 1 \\end{pmatrix}.\n$$\nCombining these results, we get $\\exp(Jt)$:\n$$\n\\exp(Jt) = \\begin{pmatrix} \\exp(-\\alpha t) & 0 \\\\ 0 & \\exp(-\\alpha t) \\end{pmatrix} \\begin{pmatrix} 1 & t \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\exp(-\\alpha t) & t\\exp(-\\alpha t) \\\\ 0 & \\exp(-\\alpha t) \\end{pmatrix}.\n$$\nFinally, we compute $\\Phi(t) = \\exp(At) = P \\exp(Jt) P^{-1}$.\n$$\n\\Phi(t) = \\left[ \\begin{pmatrix} 0 & 1/k \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\exp(-\\alpha t) & t\\exp(-\\alpha t) \\\\ 0 & \\exp(-\\alpha t) \\end{pmatrix} \\right] \\begin{pmatrix} 0 & 1 \\\\ k & 0 \\end{pmatrix}.\n$$\nFirst, we multiply the first two matrices:\n$$\nP \\exp(Jt) = \\begin{pmatrix} (0)\\exp(-\\alpha t) + (1/k)(0) & (0)t\\exp(-\\alpha t) + (1/k)\\exp(-\\alpha t) \\\\ (1)\\exp(-\\alpha t) + (0)(0) & (1)t\\exp(-\\alpha t) + (0)\\exp(-\\alpha t) \\end{pmatrix} = \\begin{pmatrix} 0 & \\frac{1}{k}\\exp(-\\alpha t) \\\\ \\exp(-\\alpha t) & t\\exp(-\\alpha t) \\end{pmatrix}.\n$$\nNow, we perform the final multiplication with $P^{-1}$:\n$$\n\\Phi(t) = \\begin{pmatrix} 0 & \\frac{1}{k}\\exp(-\\alpha t) \\\\ \\exp(-\\alpha t) & t\\exp(-\\alpha t) \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ k & 0 \\end{pmatrix}.\n$$\n$$\n\\Phi(t) = \\begin{pmatrix} (0)(0) + (\\frac{1}{k}\\exp(-\\alpha t))(k) & (0)(1) + (\\frac{1}{k}\\exp(-\\alpha t))(0) \\\\ (\\exp(-\\alpha t))(0) + (t\\exp(-\\alpha t))(k) & (\\exp(-\\alpha t))(1) + (t\\exp(-\\alpha t))(0) \\end{pmatrix}.\n$$\nThis simplifies to the final expression for the state transition matrix:\n$$\n\\Phi(t) = \\begin{pmatrix} \\exp(-\\alpha t) & 0 \\\\ kt\\exp(-\\alpha t) & \\exp(-\\alpha t) \\end{pmatrix}.\n$$\nThis can be written as $\\Phi(t) = \\exp(-\\alpha t) \\begin{pmatrix} 1 & 0 \\\\ kt & 1 \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\exp(-\\alpha t) & 0 \\\\ kt\\exp(-\\alpha t) & \\exp(-\\alpha t) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Beyond predicting a system's trajectory, state-space analysis allows us to probe its fundamental properties, such as controllability—the ability to steer the system to any desired state using an external input. This practice  grounds this abstract concept in a tangible biomedical application. You will first derive the state-space representation for a cardiovascular Windkessel model and then apply the powerful Popov–Belevitch–Hautus (PBH) test to determine if the system's dynamic modes can be influenced by the input.",
            "id": "3930817",
            "problem": "Consider a nondimensionalized three-element Windkessel model of large-artery hemodynamics, consisting of a proximal viscous resistance $r$, a lumped compliance $C$, and a distal peripheral resistance $R$, with a proximal inertance $L$ modeling blood inertia. Let the state vector be $x(t) = \\begin{pmatrix} P(t) \\\\ Q(t) \\end{pmatrix}$, where $P(t)$ is the pressure across the compliant compartment and $Q(t)$ is the inflow through the inertial element. The input is the upstream arterial pressure $u(t)$. Starting from the constitutive relations and conservation laws\n- the compliance law $Q_{C}(t) = C \\,\\frac{dP(t)}{dt}$,\n- the resistive outflow $Q_{R}(t) = \\frac{P(t)}{R}$,\n- the inertial momentum balance $L \\,\\frac{dQ(t)}{dt} = u(t) - r\\,Q(t) - P(t)$,\nand the node continuity $Q(t) = Q_{C}(t) + Q_{R}(t)$,\nderive the state-space representation $\\dot{x}(t) = A\\,x(t) + B\\,u(t)$ for the pair $(A,B)$ under the nondimensional parameter values $C = 1$, $R = 1$, $L = 1$, and $r = 2$. Then, state the Popov–Belevitch–Hautus (PBH) controllability test and apply it to this model by checking the rank condition $\\mathrm{rank}\\!\\left([\\lambda I - A \\;\\; B]\\right) = n$ for each eigenvalue $\\lambda$ of $A$, where $n$ is the state dimension. Define the scalar\n$$\nS \\;=\\; \\sum_{\\lambda \\in \\sigma(A)} \\mathrm{rank}\\!\\left([\\lambda I - A \\;\\; B]\\right),\n$$\nwhere $\\sigma(A)$ denotes the spectrum of $A$. Compute $S$ as a single real number. Express the final result as a pure scalar with no units. No rounding is required.",
            "solution": "We first derive the state-space representation of the system. The state vector is $x(t) = \\begin{pmatrix} P(t) \\\\ Q(t) \\end{pmatrix}$. We need to find expressions for the time derivatives of the state variables, $\\dot{P}(t)$ and $\\dot{Q}(t)$.\n\nFrom the node continuity equation, we have $Q(t) = Q_{C}(t) + Q_{R}(t)$.\nSubstituting the compliance law, $Q_{C}(t) = C \\frac{dP(t)}{dt}$, and the resistive outflow law, $Q_{R}(t) = \\frac{P(t)}{R}$, we get:\n$$Q(t) = C \\frac{dP(t)}{dt} + \\frac{P(t)}{R}$$\nSolving for $\\frac{dP(t)}{dt}$ gives the first state equation:\n$$C \\frac{dP(t)}{dt} = -\\frac{1}{R}P(t) + Q(t)$$\n$$\\frac{dP(t)}{dt} = -\\frac{1}{RC}P(t) + \\frac{1}{C}Q(t)$$\n\nThe second state equation is derived from the inertial momentum balance:\n$$L \\frac{dQ(t)}{dt} = u(t) - r\\,Q(t) - P(t)$$\nSolving for $\\frac{dQ(t)}{dt}$:\n$$\\frac{dQ(t)}{dt} = -\\frac{1}{L}P(t) - \\frac{r}{L}Q(t) + \\frac{1}{L}u(t)$$\n\nWe can now write these two first-order differential equations in matrix form $\\dot{x}(t) = A\\,x(t) + B\\,u(t)$:\n$$\n\\begin{pmatrix} \\dot{P}(t) \\\\ \\dot{Q}(t) \\end{pmatrix} =\n\\begin{pmatrix}\n-\\frac{1}{RC} & \\frac{1}{C} \\\\\n-\\frac{1}{L} & -\\frac{r}{L}\n\\end{pmatrix}\n\\begin{pmatrix} P(t) \\\\ Q(t) \\end{pmatrix} +\n\\begin{pmatrix} 0 \\\\ \\frac{1}{L} \\end{pmatrix}\nu(t)\n$$\nFrom this, we identify the state matrix $A$ and the input matrix $B$:\n$$A = \\begin{pmatrix} -\\frac{1}{RC} & \\frac{1}{C} \\\\ -\\frac{1}{L} & -\\frac{r}{L} \\end{pmatrix}, \\quad B = \\begin{pmatrix} 0 \\\\ \\frac{1}{L} \\end{pmatrix}$$\nSubstituting the given nondimensional parameter values $C = 1$, $R = 1$, $L = 1$, and $r = 2$:\n$$A = \\begin{pmatrix} -\\frac{1}{(1)(1)} & \\frac{1}{1} \\\\ -\\frac{1}{1} & -\\frac{2}{1} \\end{pmatrix} = \\begin{pmatrix} -1 & 1 \\\\ -1 & -2 \\end{pmatrix}$$\n$$B = \\begin{pmatrix} 0 \\\\ \\frac{1}{1} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\n\nNext, we state the Popov–Belevitch–Hautus (PBH) controllability test. A linear time-invariant system $\\dot{x} = Ax + Bu$ with state dimension $n$ is controllable if and only if the matrix $[\\lambda I - A \\;\\; B]$ has full row rank for every eigenvalue $\\lambda$ of $A$. That is, $\\mathrm{rank}\\!\\left([\\lambda I - A \\;\\; B]\\right) = n$ for all $\\lambda \\in \\sigma(A)$, where $\\sigma(A)$ is the spectrum (set of eigenvalues) of $A$.\n\nTo apply the test, we must first find the eigenvalues of the matrix $A$. The eigenvalues are the roots of the characteristic equation $\\det(\\lambda I - A) = 0$.\n$$\n\\det\\left(\\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} -1 & 1 \\\\ -1 & -2 \\end{pmatrix}\\right) = \\det\\begin{pmatrix} \\lambda+1 & -1 \\\\ 1 & \\lambda+2 \\end{pmatrix} = 0\n$$\n$$(\\lambda+1)(\\lambda+2) - (-1)(1) = 0$$\n$$\\lambda^2 + 3\\lambda + 2 + 1 = 0$$\n$$\\lambda^2 + 3\\lambda + 3 = 0$$\nUsing the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\\lambda = \\frac{-3 \\pm \\sqrt{3^2 - 4(1)(3)}}{2(1)} = \\frac{-3 \\pm \\sqrt{9-12}}{2} = \\frac{-3 \\pm i\\sqrt{3}}{2}$$\nThe two eigenvalues are $\\lambda_1 = -\\frac{3}{2} + i\\frac{\\sqrt{3}}{2}$ and $\\lambda_2 = -\\frac{3}{2} - i\\frac{\\sqrt{3}}{2}$.\n\nNow we apply the PBH test. The state dimension is $n=2$. We need to check the rank of the matrix $[\\lambda I - A \\;\\; B]$ for each eigenvalue.\n$$[\\lambda I - A \\;\\; B] = \\begin{pmatrix} \\lambda+1 & -1 & 0 \\\\ 1 & \\lambda+2 & 1 \\end{pmatrix}$$\nThis is a $2 \\times 3$ matrix. Its rank can be at most $2$. The rank is less than $2$ if and only if its two rows are linearly dependent. For the rows to be linearly dependent, one must be a scalar multiple of the other. Let the rows be $\\mathbf{r}_1 = (\\lambda+1, -1, 0)$ and $\\mathbf{r}_2 = (1, \\lambda+2, 1)$. If $\\mathbf{r}_2 = k \\mathbf{r}_1$ for some scalar $k$, we would have:\n$$1 = k(\\lambda+1)$$\n$$\\lambda+2 = k(-1)$$\n$$1 = k(0)$$\nThe last equation, $1 = 0$, is a contradiction. This shows that the rows can never be linearly dependent, regardless of the value of $\\lambda$. Therefore, the rank of the matrix $[\\lambda I - A \\;\\; B]$ is always $2$.\n\nThis holds for both eigenvalues $\\lambda_1$ and $\\lambda_2$.\nFor $\\lambda_1 = -\\frac{3}{2} + i\\frac{\\sqrt{3}}{2}$:\n$$\\mathrm{rank}\\!\\left([\\lambda_1 I - A \\;\\; B]\\right) = 2$$\nFor $\\lambda_2 = -\\frac{3}{2} - i\\frac{\\sqrt{3}}{2}$:\n$$\\mathrm{rank}\\!\\left([\\lambda_2 I - A \\;\\; B]\\right) = 2$$\nSince the rank is equal to the state dimension $n=2$ for all eigenvalues, the system is controllable.\n\nFinally, we compute the scalar $S$:\n$$S = \\sum_{\\lambda \\in \\sigma(A)} \\mathrm{rank}\\!\\left([\\lambda I - A \\;\\; B]\\right)$$\n$$S = \\mathrm{rank}\\!\\left([\\lambda_1 I - A \\;\\; B]\\right) + \\mathrm{rank}\\!\\left([\\lambda_2 I - A \\;\\; B]\\right)$$\n$$S = 2 + 2 = 4$$\nThe value of $S$ is $4$.",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "Understanding the theoretical property of controllability becomes truly insightful when we examine its practical consequences. What limitations does an uncontrollable system impose on therapeutic or experimental interventions? This problem  explores this question by analyzing a stable but uncontrollable system, demonstrating how certain states remain beyond our full control and calculating the precise physical limits this imposes on system regulation.",
            "id": "3930769",
            "problem": "A core task in biomedical systems modeling is to understand when a physiological state can be regulated with bounded actuation, even if not all internal variables are directly actuated. Consider a linear time-invariant (LTI) two-state model that represents a linearized interaction between a measurable analyte concentration and an autonomous biomarker adaptation in response to an intravenous infusion. The state variables are the analyte concentration $x_{1}(t)$ and the biomarker adaptation state $x_{2}(t)$. The system dynamics are defined by the state-space representation\n$$\n\\frac{d}{dt}\\begin{pmatrix} x_{1}(t) \\\\ x_{2}(t) \\end{pmatrix}\n=\n\\underbrace{\\begin{pmatrix} -a & c \\\\ 0 & -d \\end{pmatrix}}_{A}\n\\begin{pmatrix} x_{1}(t) \\\\ x_{2}(t) \\end{pmatrix}\n+\n\\underbrace{\\begin{pmatrix} \\beta \\\\ 0 \\end{pmatrix}}_{B} u(t),\n$$\nwhere $a>0$, $d>0$, $c\\neq 0$, and $\\beta>0$ are constants determined by physiology and linearization. The infusion input $u(t)$ is bounded in magnitude by a hardware and safety constraint, namely $|u(t)| \\leq U_{\\max}$ for all $t \\geq 0$. Assume the initial condition $x_{1}(0) = 0$ and $x_{2}(0) = x_{20}$, with $x_{20} \\in \\mathbb{R}$.\n\nUsing only fundamental definitions and well-tested facts about LTI systems, do the following:\n\n- Starting from the definitions of matrix stability and controllability, show that the matrix $A$ is internally stable and that the pair $(A,B)$ is uncontrollable.\n\n- Derive the state transition matrix $\\Phi(t)$ for the autonomous system governed by $A$.\n\n- Using the state transition representation and the state equations, determine an infusion function $u(t)$ that would make $x_{1}(t) \\equiv 0$ for all $t \\geq 0$, and use this to deduce the exact minimal bound $U_{\\max}$ required so that such regulation is feasible for any sign of $x_{20}$.\n\nYour final answer must be a single closed-form analytic expression for the minimal required bound on the infusion amplitude, denoted $U_{\\max}^{\\mathrm{req}}$, expressed in terms of $a$, $c$, $d$, $\\beta$, and $x_{20}$. No numerical rounding is required, and no physical unit needs to be reported in the final expression. The angle unit is not applicable in this problem.",
            "solution": "**Stability of $A$ and Controllability of $(A,B)$**\n\nFirst, we assess the internal stability of the system matrix $A$. A system is internally stable if all eigenvalues of the matrix $A$ have strictly negative real parts. The matrix $A$ is given as:\n$$\nA = \\begin{pmatrix} -a & c \\\\ 0 & -d \\end{pmatrix}\n$$\nSince $A$ is an upper triangular matrix, its eigenvalues are its diagonal entries. Let the eigenvalues be $\\lambda_1$ and $\\lambda_2$.\n$$\n\\lambda_1 = -a\n$$\n$$\n\\lambda_2 = -d\n$$\nThe problem states that $a>0$ and $d>0$. Therefore, both eigenvalues $\\lambda_1$ and $\\lambda_2$ are strictly negative real numbers. This confirms that the matrix $A$ is internally stable (specifically, Hurwitz stable).\n\nNext, we assess the controllability of the pair $(A,B)$. An LTI system is controllable if and only if its controllability matrix, $\\mathcal{C}$, has full rank. For a second-order system ($n=2$), the rank must be $2$. The controllability matrix is defined as $\\mathcal{C} = [B | AB]$.\nGiven $B = \\begin{pmatrix} \\beta \\\\ 0 \\end{pmatrix}$, we first compute the product $AB$:\n$$\nAB = \\begin{pmatrix} -a & c \\\\ 0 & -d \\end{pmatrix} \\begin{pmatrix} \\beta \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -a\\beta \\\\ 0 \\end{pmatrix}\n$$\nNow, we construct the controllability matrix $\\mathcal{C}$:\n$$\n\\mathcal{C} = \\begin{pmatrix} B & AB \\end{pmatrix} = \\begin{pmatrix} \\beta & -a\\beta \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe rank of a matrix is the number of linearly independent rows or columns. The second row of $\\mathcal{C}$ is a zero vector, which means the rows are linearly dependent. The rank can be formally checked by computing the determinant:\n$$\n\\det(\\mathcal{C}) = (\\beta)(0) - (-a\\beta)(0) = 0\n$$\nSince the determinant is zero, the matrix $\\mathcal{C}$ is singular and does not have full rank. Its rank is $1$, which is less than the system order $n=2$. Therefore, the pair $(A,B)$ is uncontrollable.\n\n**State Transition Matrix $\\Phi(t)$**\n\nThe state transition matrix $\\Phi(t)$ is defined as the matrix exponential $\\Phi(t) = \\exp(At)$. A standard method for its calculation is via the inverse Laplace transform of $(sI-A)^{-1}$.\n$$\nsI-A = \\begin{pmatrix} s & 0 \\\\ 0 & s \\end{pmatrix} - \\begin{pmatrix} -a & c \\\\ 0 & -d \\end{pmatrix} = \\begin{pmatrix} s+a & -c \\\\ 0 & s+d \\end{pmatrix}\n$$\nThe inverse is:\n$$\n(sI-A)^{-1} = \\frac{1}{\\det(sI-A)} \\text{adj}(sI-A) = \\frac{1}{(s+a)(s+d)} \\begin{pmatrix} s+d & c \\\\ 0 & s+a \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{s+a} & \\frac{c}{(s+a)(s+d)} \\\\ 0 & \\frac{1}{s+d} \\end{pmatrix}\n$$\nWe find $\\Phi(t) = \\mathcal{L}^{-1}[(sI-A)^{-1}]$ by taking the inverse Laplace transform of each element.\nThe diagonal elements are straightforward:\n$\\mathcal{L}^{-1}\\left\\{\\frac{1}{s+a}\\right\\} = e^{-at}$ and $\\mathcal{L}^{-1}\\left\\{\\frac{1}{s+d}\\right\\} = e^{-dt}$.\nThe off-diagonal element $\\phi_{12}(t)$ requires partial fraction expansion.\n\nCase 1: $a \\neq d$.\n$$\n\\frac{c}{(s+a)(s+d)} = \\frac{K_1}{s+a} + \\frac{K_2}{s+d}\n$$\n$K_1 = \\lim_{s \\to -a} \\frac{c}{s+d} = \\frac{c}{d-a}$ and $K_2 = \\lim_{s \\to -d} \\frac{c}{s+a} = \\frac{c}{a-d}$.\nThus, $\\phi_{12}(t) = \\mathcal{L}^{-1}\\left\\{\\frac{c}{d-a}\\frac{1}{s+a} + \\frac{c}{a-d}\\frac{1}{s+d}\\right\\} = \\frac{c}{d-a}e^{-at} + \\frac{c}{a-d}e^{-dt} = \\frac{c}{a-d}(e^{-dt} - e^{-at})$.\nThe state transition matrix for $a \\neq d$ is:\n$$\n\\Phi(t) = \\begin{pmatrix} e^{-at} & \\frac{c}{a-d}(e^{-dt} - e^{-at}) \\\\ 0 & e^{-dt} \\end{pmatrix}\n$$\nCase 2: $a=d$.\nThe term becomes $\\frac{c}{(s+a)^2}$. The inverse Laplace transform is $\\mathcal{L}^{-1}\\left\\{\\frac{c}{(s+a)^2}\\right\\} = c t e^{-at}$.\nThe state transition matrix for $a=d$ is:\n$$\n\\Phi(t) = \\begin{pmatrix} e^{-at} & c t e^{-at} \\\\ 0 & e^{-at} \\end{pmatrix}\n$$\nNote that $\\lim_{d \\to a} \\frac{c(e^{-dt} - e^{-at})}{a-d} = c t e^{-at}$ by L'Hopital's rule, so the expression for $a \\neq d$ is general.\n\n**Required Infusion Function and Minimal Bound**\n\nWe need to find an input $u(t)$ that keeps $x_1(t) \\equiv 0$ for all $t \\geq 0$. We can use the system's differential equations directly.\nThe equations are:\n1. $\\frac{dx_1}{dt} = -a x_1(t) + c x_2(t) + \\beta u(t)$\n2. $\\frac{dx_2}{dt} = -d x_2(t)$\n\nThe condition $x_1(t) \\equiv 0$ for $t \\geq 0$ also implies that its derivative $\\frac{dx_1}{dt} \\equiv 0$ for $t \\geq 0$. Substituting these into the first equation:\n$$\n0 = -a(0) + c x_2(t) + \\beta u(t)\n$$\nSolving for $u(t)$ gives:\n$$\nu(t) = -\\frac{c}{\\beta} x_2(t)\n$$\nThis is a state feedback law. To find the explicit function of time, we must solve for $x_2(t)$. The dynamics of $x_2(t)$ are governed by the second equation, which is decoupled from $x_1$ and $u(t)$.\n$$\n\\frac{dx_2}{dt} = -d x_2(t)\n$$\nThis is a standard first-order linear ordinary differential equation with the solution:\n$$\nx_2(t) = x_2(0) \\exp(-dt)\n$$\nGiven the initial condition $x_2(0) = x_{20}$, we have:\n$$\nx_2(t) = x_{20} \\exp(-dt)\n$$\nSubstituting this expression for $x_2(t)$ into the equation for $u(t)$:\n$$\nu(t) = -\\frac{c x_{20}}{\\beta} \\exp(-dt)\n$$\nThis is the required infusion function.\n\nFinally, we determine the minimal bound $U_{\\max}^{\\mathrm{req}}$ such that the hardware constraint $|u(t)| \\leq U_{\\max}$ is always satisfied for $t \\geq 0$. This minimal bound must be equal to the maximum absolute value of $u(t)$ over the time interval $t \\in [0, \\infty)$.\n$$\nU_{\\max}^{\\mathrm{req}} = \\max_{t \\geq 0} |u(t)| = \\max_{t \\geq 0} \\left| -\\frac{c x_{20}}{\\beta} \\exp(-dt) \\right|\n$$\nUsing the properties of absolute value and that $\\beta > 0$:\n$$\nU_{\\max}^{\\mathrm{req}} = \\max_{t \\geq 0} \\left( \\frac{|c x_{20}|}{\\beta} \\exp(-dt) \\right)\n$$\nSince $c$, $x_{20}$, and $\\beta$ are constant with respect to time, we can factor them out of the maximization:\n$$\nU_{\\max}^{\\mathrm{req}} = \\frac{|c x_{20}|}{\\beta} \\max_{t \\geq 0} \\left( \\exp(-dt) \\right)\n$$\nThe problem states that $d>0$. Therefore, the function $\\exp(-dt)$ is a monotonically decreasing function for $t \\geq 0$. Its maximum value occurs at $t=0$, where $\\exp(-d \\cdot 0) = 1$.\n$$\n\\max_{t \\geq 0} \\left( \\exp(-dt) \\right) = 1\n$$\nSubstituting this back, we find the minimal required bound:\n$$\nU_{\\max}^{\\mathrm{req}} = \\frac{|c x_{20}|}{\\beta} \\cdot 1 = \\frac{|c x_{20}|}{\\beta}\n$$\nThis expression holds for any sign of $x_{20}$ due to the absolute value. The result is independent of $a$ and $d$. The independence from $a$ is because the term $-ax_1$ is always zero under the imposed regulation. The independence from $d$ is because the peak input occurs at $t=0$, before the decay term $\\exp(-dt)$ has had any effect.",
            "answer": "$$\\boxed{\\frac{|c x_{20}|}{\\beta}}$$"
        }
    ]
}