{
    "hands_on_practices": [
        {
            "introduction": "Modeling dynamic physiological systems often involves tracking a latent (unobserved) state, such as true blood glucose, from a series of noisy measurements. This practice delves into the Kalman filter, a cornerstone of modern time-series analysis, which provides an optimal recursive method for this exact task . By deriving the filter's update equations from Bayesian first principles, you will gain a deep understanding of how to sequentially combine a predictive model of system dynamics with incoming sensor data to refine your state estimate, a fundamental skill in signal processing and control for biomedical applications.",
            "id": "3921987",
            "problem": "A patient’s interstitial blood glucose concentration, denoted by the latent state $x_k$ at discrete time index $k$, is modeled as a linear Gaussian state-space process appropriate for short-horizon physiology where unmodeled inputs (e.g., insulin action and carbohydrate absorption) are lumped into process noise. Specifically, assume a random-walk state evolution $x_k = x_{k-1} + w_k$ with $w_k \\sim \\mathcal{N}(0,Q)$ and an observation model for a Continuous Glucose Monitor (CGM) measurement $z_k = x_k + v_k$ with $v_k \\sim \\mathcal{N}(0,R)$. Here $\\mathcal{N}$ denotes the normal distribution, $Q \\ge 0$ and $R > 0$ are known scalars, and all noises are mutually independent and independent of the initial state. You are provided at time $k$ with a Gaussian prior (the prediction from the process model) for $x_k$ of the form $x_k \\mid \\mathcal{Z}_{k-1} \\sim \\mathcal{N}(\\hat{x}_{k \\mid k-1}, P_{k \\mid k-1})$, where $\\mathcal{Z}_{k-1}$ denotes all measurements up to time $k-1$, and a new CGM measurement $z_k$.\n\nTask:\n1. Starting from Bayes’ rule and the properties of multivariate normal distributions (without invoking any pre-derived Kalman filter formulas), derive the recursive measurement-update equations for the posterior mean $\\hat{x}_{k \\mid k}$ and posterior variance $P_{k \\mid k}$ in the scalar case described above. Your derivation must explicitly identify the quantity commonly called the Kalman gain in terms of $P_{k \\mid k-1}$ and $R$.\n2. Apply your derived update to the following physiologically plausible scenario: the prior at time $k$ is $\\hat{x}_{k \\mid k-1} = 150 \\text{ mg/dL}$ and $P_{k \\mid k-1} = 400 \\text{ (mg/dL)}^2$, and the CGM reports $z_k = 172 \\text{ mg/dL}$ with a known measurement noise variance $R = 100 \\text{ (mg/dL)}^2$. Report the posterior mean $\\hat{x}_{k \\mid k}$ in $\\text{mg/dL}$.\nExpress the final numeric answer as instructed below.\n\nAnswer specification:\n- The final answer must be the numerical value of the posterior mean $\\hat{x}_{k \\mid k}$, expressed in $\\text{mg/dL}$, rounded to four significant figures.",
            "solution": "**1. Derivation of the Measurement-Update Equations**\n\nThe objective is to find the posterior probability density function (PDF) of the state $x_k$ given all measurements up to time $k$, denoted $\\mathcal{Z}_k = \\{\\mathcal{Z}_{k-1}, z_k\\}$. This PDF is $p(x_k \\mid \\mathcal{Z}_k)$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$p(x_k \\mid \\mathcal{Z}_k) = p(x_k \\mid z_k, \\mathcal{Z}_{k-1}) \\propto p(z_k \\mid x_k, \\mathcal{Z}_{k-1}) p(x_k \\mid \\mathcal{Z}_{k-1})$$\n\nThe terms in this expression are defined as follows:\n*   The prior distribution is given as Gaussian: $p(x_k \\mid \\mathcal{Z}_{k-1}) \\sim \\mathcal{N}(\\hat{x}_{k \\mid k-1}, P_{k \\mid k-1})$. Its PDF is:\n$$p(x_k \\mid \\mathcal{Z}_{k-1}) = \\frac{1}{\\sqrt{2\\pi P_{k \\mid k-1}}} \\exp\\left(-\\frac{1}{2} \\frac{(x_k - \\hat{x}_{k \\mid k-1})^2}{P_{k \\mid k-1}}\\right)$$\n\n*   The likelihood function $p(z_k \\mid x_k, \\mathcal{Z}_{k-1})$ is determined by the measurement model $z_k = x_k + v_k$. Since the measurement noise $v_k$ is independent of the state $x_k$ and past measurements $\\mathcal{Z}_{k-1}$, we have $p(z_k \\mid x_k, \\mathcal{Z}_{k-1}) = p(z_k \\mid x_k)$. The noise $v_k = z_k - x_k$ is distributed as $\\mathcal{N}(0, R)$, which implies that the likelihood, viewed as a function of $x_k$, is a Gaussian centered at $z_k$ with variance $R$:\n$$p(z_k \\mid x_k) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(z_k - x_k)^2}{R}\\right)$$\n\nThe product of two Gaussian PDFs is proportional to another Gaussian PDF. The posterior $p(x_k \\mid \\mathcal{Z}_k)$ is therefore Gaussian, which we can write as $x_k \\mid \\mathcal{Z}_k \\sim \\mathcal{N}(\\hat{x}_{k \\mid k}, P_{k \\mid k})$. To find the posterior mean $\\hat{x}_{k \\mid k}$ and variance $P_{k \\mid k}$, we analyze the exponent of the posterior PDF.\n$$p(x_k \\mid \\mathcal{Z}_k) \\propto \\exp\\left(-\\frac{1}{2} \\frac{(x_k - \\hat{x}_{k \\mid k-1})^2}{P_{k \\mid k-1}}\\right) \\exp\\left(-\\frac{1}{2} \\frac{(x_k - z_k)^2}{R}\\right)$$\n$$p(x_k \\mid \\mathcal{Z}_k) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(x_k - \\hat{x}_{k \\mid k-1})^2}{P_{k \\mid k-1}} + \\frac{(x_k - z_k)^2}{R} \\right] \\right)$$\n\nLet $\\Phi$ be the expression inside the brackets in the exponent. We expand and collect terms in powers of $x_k$:\n$$\\Phi = \\frac{x_k^2 - 2x_k\\hat{x}_{k \\mid k-1} + \\hat{x}_{k \\mid k-1}^2}{P_{k \\mid k-1}} + \\frac{x_k^2 - 2x_k z_k + z_k^2}{R}$$\n$$\\Phi = x_k^2 \\left(\\frac{1}{P_{k \\mid k-1}} + \\frac{1}{R}\\right) - 2x_k \\left(\\frac{\\hat{x}_{k \\mid k-1}}{P_{k \\mid k-1}} + \\frac{z_k}{R}\\right) + C$$\nwhere $C$ contains terms that do not depend on $x_k$.\n\nThe exponent of the posterior PDF $\\mathcal{N}(\\hat{x}_{k \\mid k}, P_{k \\mid k})$ has the form $-\\frac{1}{2} \\frac{(x_k - \\hat{x}_{k \\mid k})^2}{P_{k \\mid k}}$, which expands to:\n$$-\\frac{1}{2} \\left[ \\frac{1}{P_{k \\mid k}}x_k^2 - \\frac{2\\hat{x}_{k \\mid k}}{P_{k \\mid k}}x_k + \\frac{\\hat{x}_{k \\mid k}^2}{P_{k \\mid k}} \\right]$$\n\nBy comparing the coefficients of the $x_k^2$ term in $\\Phi$ and the posterior exponent, we identify the inverse of the posterior variance $P_{k \\mid k}$:\n$$\\frac{1}{P_{k \\mid k}} = \\frac{1}{P_{k \\mid k-1}} + \\frac{1}{R} = \\frac{P_{k \\mid k-1} + R}{P_{k \\mid k-1} R}$$\nSolving for $P_{k \\mid k}$ yields the posterior variance update equation:\n$$P_{k \\mid k} = \\frac{P_{k \\mid k-1} R}{P_{k \\mid k-1} + R}$$\n\nNext, by comparing the coefficients of the linear $x_k$ term, we have:\n$$\\frac{\\hat{x}_{k \\mid k}}{P_{k \\mid k}} = \\frac{\\hat{x}_{k \\mid k-1}}{P_{k \\mid k-1}} + \\frac{z_k}{R}$$\nSolving for $\\hat{x}_{k \\mid k}$ gives the posterior mean:\n$$\\hat{x}_{k \\mid k} = P_{k \\mid k} \\left(\\frac{\\hat{x}_{k \\mid k-1}}{P_{k \\mid k-1}} + \\frac{z_k}{R}\\right)$$\nSubstituting the expression for $P_{k \\mid k}$:\n$$\\hat{x}_{k \\mid k} = \\frac{P_{k \\mid k-1} R}{P_{k \\mid k-1} + R} \\left(\\frac{\\hat{x}_{k \\mid k-1}}{P_{k \\mid k-1}} + \\frac{z_k}{R}\\right) = \\frac{R\\hat{x}_{k \\mid k-1} + P_{k \\mid k-1}z_k}{P_{k \\mid k-1} + R}$$\nThis expression shows the updated mean as a weighted average of the prior mean and the measurement. To express this in the conventional form involving the Kalman gain, we perform an algebraic manipulation:\n$$\\hat{x}_{k \\mid k} = \\frac{(P_{k \\mid k-1} + R)\\hat{x}_{k \\mid k-1} - P_{k \\mid k-1}\\hat{x}_{k \\mid k-1} + P_{k \\mid k-1}z_k}{P_{k \\mid k-1} + R}$$\n$$\\hat{x}_{k \\mid k} = \\frac{(P_{k \\mid k-1} + R)\\hat{x}_{k \\mid k-1}}{P_{k \\mid k-1} + R} + \\frac{P_{k \\mid k-1}(z_k - \\hat{x}_{k \\mid k-1})}{P_{k \\mid k-1} + R}$$\n$$\\hat{x}_{k \\mid k} = \\hat{x}_{k \\mid k-1} + \\left(\\frac{P_{k \\mid k-1}}{P_{k \\mid k-1} + R}\\right)(z_k - \\hat{x}_{k \\mid k-1})$$\nThis is the measurement-update equation for the posterior mean. The term multiplying the innovation $(z_k - \\hat{x}_{k \\mid k-1})$ is defined as the Kalman gain, $K_k$:\n$$K_k = \\frac{P_{k \\mid k-1}}{P_{k \\mid k-1} + R}$$\nThus, the update equation for the mean is $\\hat{x}_{k \\mid k} = \\hat{x}_{k \\mid k-1} + K_k(z_k - \\hat{x}_{k \\mid k-1})$.\n\nThe posterior variance update can also be expressed using $K_k$:\n$$P_{k \\mid k} = \\frac{P_{k \\mid k-1} R}{P_{k \\mid k-1} + R} = \\left(1 - \\frac{P_{k \\mid k-1}}{P_{k \\mid k-1} + R}\\right) P_{k \\mid k-1} = (1 - K_k)P_{k \\mid k-1}$$\nThis completes the derivation of the recursive measurement-update equations.\n\n**2. Application to the Physiological Scenario**\n\nWe are given the following values:\n*   Prior mean: $\\hat{x}_{k \\mid k-1} = 150 \\text{ mg/dL}$\n*   Prior variance: $P_{k \\mid k-1} = 400 \\text{ (mg/dL)}^2$\n*   Measurement: $z_k = 172 \\text{ mg/dL}$\n*   Measurement noise variance: $R = 100 \\text{ (mg/dL)}^2$\n\nFirst, we compute the Kalman gain $K_k$:\n$$K_k = \\frac{P_{k \\mid k-1}}{P_{k \\mid k-1} + R} = \\frac{400}{400 + 100} = \\frac{400}{500} = 0.8$$\nThe gain $K_k=0.8$ is dimensionless. It determines that the correction to the mean will be $80\\%$ of the innovation (the difference between the measurement and the prior estimate).\n\nNext, we apply the measurement-update equation for the posterior mean $\\hat{x}_{k \\mid k}$:\n$$\\hat{x}_{k \\mid k} = \\hat{x}_{k \\mid k-1} + K_k(z_k - \\hat{x}_{k \\mid k-1})$$\nSubstituting the numerical values:\n$$\\hat{x}_{k \\mid k} = 150 + 0.8 \\times (172 - 150)$$\n$$\\hat{x}_{k \\mid k} = 150 + 0.8 \\times (22)$$\n$$\\hat{x}_{k \\mid k} = 150 + 17.6$$\n$$\\hat{x}_{k \\mid k} = 167.6$$\n\nThe posterior mean glucose concentration is $167.6 \\text{ mg/dL}$. This value lies between the prior estimate ($150$) and the new measurement ($172$), and is closer to the measurement because the prior variance ($P_{k \\mid k-1}=400$) is larger than the measurement variance ($R=100$), indicating we have more confidence in the new measurement. The requested rounding to four significant figures is already satisfied by the result $167.6$.",
            "answer": "$$\n\\boxed{167.6}\n$$"
        },
        {
            "introduction": "A frequent challenge in biomedical research is analyzing \"time-to-event\" data, such as the time until a medical device fails or a patient responds to treatment. Studies are often concluded before the event has occurred for all subjects, leading to right-censored observations. This practice demonstrates how to build a statistically sound model in the presence of such incomplete data by constructing a likelihood function that correctly incorporates information from both observed events and censored times . Mastering this technique is essential for performing survival analysis and accurately estimating key parameters like failure rates from real-world cohort data.",
            "id": "3921993",
            "problem": "In a cohort study of implantable ventricular assist devices, early mechanical occlusion is modeled as having a constant instantaneous failure risk over time. Let the time to occlusion for subject $i$ be the continuous random variable $T_i$, assumed independent across subjects and governed by an exponential model with constant hazard rate $\\lambda$. Right-censoring occurs due to study termination or loss to follow-up, yielding the observed time $Y_i = \\min(T_i, C_i)$ and an event indicator $\\Delta_i = \\mathbf{1}\\{T_i \\leq C_i\\}$, where $C_i$ is the independent censoring time. Use the fundamental relationships among the hazard function $h(t)$, survival function $S(t)$, and probability density function $f(t)$, together with independence and the definition of likelihood, to derive the likelihood for $\\lambda$ under independent right-censoring.\n\nThen, using the principle of Maximum Likelihood Estimation (MLE), obtain the closed-form expression for the MLE of $\\lambda$ in terms of observable quantities. Finally, compute its numerical value from the following $n=12$ observed pairs $(Y_i, \\Delta_i)$ (times in days):\n$(5, 1)$, $(6, 1)$, $(7, 1)$, $(8, 1)$, $(9, 1)$, $(10, 1)$, $(12, 1)$, $(13, 1)$, $(1.5, 0)$, $(3.0, 0)$, $(2.5, 0)$, $(3.0, 0)$.\nRound your numerical answer to four significant figures and express the final rate in $\\text{day}^{-1}$.\n\nIn addition, based on the curvature of the log-likelihood, briefly discuss how right-censoring modifies information about $\\lambda$ in this exponential survival model relative to complete (uncensored) data. The final numerical answer must be a single real number.",
            "solution": "**Derivation of the Likelihood Function**\nThe time to event $T$ is modeled by an exponential distribution with a constant hazard rate $\\lambda$. The fundamental functions associated with this model are:\n1.  The hazard function: $h(t) = \\lambda$ for $t \\ge 0$.\n2.  The survival function: $S(t) = P(T > t) = \\exp\\left(-\\int_0^t h(u) \\,du\\right) = \\exp\\left(-\\int_0^t \\lambda \\,du\\right) = \\exp(-\\lambda t)$.\n3.  The probability density function (PDF): $f(t) = -\\frac{dS(t)}{dt} = - \\frac{d}{dt}\\exp(-\\lambda t) = \\lambda \\exp(-\\lambda t)$.\n\nThe likelihood function is constructed from the contributions of each of the $n$ independent subjects. The contribution of subject $i$ depends on whether their event was observed or censored, indicated by $\\Delta_i$.\n\n- If the event is observed for subject $i$ ($\\Delta_i = 1$), it means the failure occurred at time $T_i = Y_i$ and this happened before the censoring time $C_i$. The contribution to the likelihood is the probability density of failure at time $Y_i$, which is $f(Y_i) = \\lambda \\exp(-\\lambda Y_i)$.\n- If the observation is right-censored for subject $i$ ($\\Delta_i = 0$), it means the failure time $T_i$ is greater than the censoring time $C_i$, and the observed time is $Y_i = C_i$. We only know that the subject survived at least until time $Y_i$. The contribution to the likelihood is the probability of this event, which is $P(T_i > Y_i) = S(Y_i) = \\exp(-\\lambda Y_i)$.\n\nA single expression for the likelihood contribution of subject $i$, $L_i(\\lambda)$, can be written using the event indicator $\\Delta_i$:\n$$L_i(\\lambda) = [f(Y_i)]^{\\Delta_i} [S(Y_i)]^{1-\\Delta_i}$$\nSubstituting the expressions for $f(t)$ and $S(t)$:\n$$L_i(\\lambda) = [\\lambda \\exp(-\\lambda Y_i)]^{\\Delta_i} [\\exp(-\\lambda Y_i)]^{1-\\Delta_i}$$\n$$L_i(\\lambda) = \\lambda^{\\Delta_i} \\exp(-\\lambda Y_i \\Delta_i) \\exp(-\\lambda Y_i (1-\\Delta_i))$$\n$$L_i(\\lambda) = \\lambda^{\\Delta_i} \\exp(-\\lambda Y_i (\\Delta_i + 1 - \\Delta_i))$$\n$$L_i(\\lambda) = \\lambda^{\\Delta_i} \\exp(-\\lambda Y_i)$$\nSince all subjects are independent, the total likelihood for the sample of $n$ subjects, $L(\\lambda)$, is the product of the individual contributions:\n$$L(\\lambda) = \\prod_{i=1}^n L_i(\\lambda) = \\prod_{i=1}^n \\left( \\lambda^{\\Delta_i} \\exp(-\\lambda Y_i) \\right)$$\n$$L(\\lambda) = \\left( \\prod_{i=1}^n \\lambda^{\\Delta_i} \\right) \\left( \\prod_{i=1}^n \\exp(-\\lambda Y_i) \\right)$$\n$$L(\\lambda) = \\lambda^{\\sum_{i=1}^n \\Delta_i} \\exp\\left(-\\lambda \\sum_{i=1}^n Y_i\\right)$$\nLet $d = \\sum_{i=1}^n \\Delta_i$ be the total number of observed events, and let $Y_{\\text{total}} = \\sum_{i=1}^n Y_i$ be the total observed time-at-risk. The likelihood function simplifies to:\n$$L(\\lambda) = \\lambda^d \\exp(-\\lambda Y_{\\text{total}})$$\n\n**Derivation of the Maximum Likelihood Estimator (MLE)**\nTo find the MLE for $\\lambda$, we maximize $L(\\lambda)$. It is computationally simpler to maximize the natural logarithm of the likelihood, the log-likelihood function $\\ell(\\lambda)$:\n$$\\ell(\\lambda) = \\ln(L(\\lambda)) = \\ln\\left( \\lambda^d \\exp(-\\lambda Y_{\\text{total}}) \\right)$$\n$$\\ell(\\lambda) = d \\ln(\\lambda) - \\lambda Y_{\\text{total}}$$\nTo find the maximum, we take the first derivative of $\\ell(\\lambda)$ with respect to $\\lambda$ and set it to zero:\n$$\\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda} (d \\ln(\\lambda) - \\lambda Y_{\\text{total}}) = \\frac{d}{\\lambda} - Y_{\\text{total}}$$\nSetting the derivative to zero to find the estimator $\\hat{\\lambda}$:\n$$\\frac{d}{\\hat{\\lambda}} - Y_{\\text{total}} = 0 \\implies \\frac{d}{\\hat{\\lambda}} = Y_{\\text{total}}$$\nSolving for $\\hat{\\lambda}$ gives the MLE:\n$$\\hat{\\lambda}_{\\text{MLE}} = \\frac{d}{Y_{\\text{total}}} = \\frac{\\sum_{i=1}^n \\Delta_i}{\\sum_{i=1}^n Y_i}$$\nThis expression is the total number of events divided by the total person-time of follow-up. To confirm this is a maximum, we check the second derivative:\n$$\\frac{d^2\\ell}{d\\lambda^2} = -\\frac{d}{\\lambda^2}$$\nSince the number of events $d \\ge 0$ and $\\lambda^2 > 0$, the second derivative is non-positive. For $d > 0$, it is strictly negative, which confirms that $\\hat{\\lambda}_{\\text{MLE}}$ corresponds to a local maximum.\n\n**Numerical Calculation**\nThe provided data consists of $n=12$ pairs $(Y_i, \\Delta_i)$:\n$(5, 1)$, $(6, 1)$, $(7, 1)$, $(8, 1)$, $(9, 1)$, $(10, 1)$, $(12, 1)$, $(13, 1)$, $(1.5, 0)$, $(3.0, 0)$, $(2.5, 0)$, $(3.0, 0)$.\n\nFirst, we calculate the total number of events, $d$:\n$$d = \\sum_{i=1}^{12} \\Delta_i = 1+1+1+1+1+1+1+1+0+0+0+0 = 8$$\nNext, we calculate the total time-at-risk, $Y_{\\text{total}}$:\n$$Y_{\\text{total}} = \\sum_{i=1}^{12} Y_i = (5+6+7+8+9+10+12+13) + (1.5+3.0+2.5+3.0)$$\n$$Y_{\\text{total}} = 70 + 10 = 80$$\nThe times are given in days. Now, we compute the MLE for $\\lambda$:\n$$\\hat{\\lambda}_{\\text{MLE}} = \\frac{d}{Y_{\\text{total}}} = \\frac{8}{80} = \\frac{1}{10} = 0.1$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\\hat{\\lambda}_{\\text{MLE}} = 0.1000 \\, \\text{day}^{-1}$$\n\n**Discussion on Information Modification by Censoring**\nThe information about the parameter $\\lambda$ is related to the curvature of the log-likelihood function at the MLE. The observed Fisher information is defined as $I(\\lambda) = -\\frac{d^2\\ell}{d\\lambda^2}$. In this model, we found:\n$$I(\\lambda) = - \\left(-\\frac{d}{\\lambda^2}\\right) = \\frac{d}{\\lambda^2}$$\nA larger value of $I(\\lambda)$ implies a sharper peak in the likelihood function, corresponding to more information about $\\lambda$ and a smaller variance for its estimator (since $\\text{Var}(\\hat{\\lambda}) \\approx 1/I(\\hat{\\lambda})$).\nThe information is directly proportional to $d$, the number of observed events. Right-censoring, by definition, implies that for a given sample of size $n$, the number of events $d$ is less than or equal to $n$ ($d \\le n$). If there were no censoring (a complete dataset of failures), every subject would contribute an event, so we would have $d=n$.\nThe presence of right-censoring reduces the number of observed events from a potential maximum of $n$ to the observed value $d$. This reduction in $d$ directly leads to a smaller value of the information $I(\\lambda)$. A smaller value of information means the log-likelihood function is less peaked (has a smaller curvature), which reflects greater uncertainty in the estimate of $\\lambda$. Therefore, right-censoring modifies the information by reducing it; the censored observations contribute to the total time-at-risk (the denominator of $\\hat{\\lambda}$) but not to the event count (the numerator), which is the sole driver of statistical information for this model.",
            "answer": "$$\\boxed{0.1000}$$"
        },
        {
            "introduction": "Biomedical datasets often contain hidden heterogeneity, where the patient population is composed of distinct phenotypic subgroups that are not immediately apparent. This practice introduces a powerful probabilistic approach to discovering this hidden structure using Gaussian Mixture Models (GMMs) fit via the Expectation-Maximization (EM) algorithm . By working through the derivation and application of the EM algorithm, you will learn an elegant, iterative method for finding model parameters in the presence of latent variables, a foundational technique for unsupervised machine learning and automated phenotyping in systems biology.",
            "id": "3922004",
            "problem": "A biomedical systems modeling team seeks to cluster patient serum biomarker profiles into phenotypic subgroups using a probabilistic model that accounts for inter-patient heterogeneity. Consider a mixture of multivariate Gaussian components for biomarker vectors, where each patient $n$ has a two-dimensional biomarker vector $x_{n} = (x_{n,1}, x_{n,2})$ (biomarker $1$ measured in $\\mathrm{ng/mL}$, biomarker $2$ measured in $\\mathrm{U/L}$). Let the mixture model have $K$ components with mixing proportions $\\pi_{k}$, component means $\\mu_{k} \\in \\mathbb{R}^{2}$, and component covariance matrices $\\Sigma_{k} \\in \\mathbb{R}^{2 \\times 2}$. The observed-data likelihood is $p(x_{n} \\mid \\Theta) = \\sum_{k=1}^{K} \\pi_{k} \\, \\mathcal{N}(x_{n} \\mid \\mu_{k}, \\Sigma_{k})$, where $\\Theta = \\{\\pi_{k}, \\mu_{k}, \\Sigma_{k}\\}_{k=1}^{K}$ and $\\mathcal{N}(\\cdot \\mid \\mu, \\Sigma)$ denotes the multivariate normal density.\n\nStarting from the maximum likelihood principle for incomplete data and introducing latent indicator variables $z_{nk} \\in \\{0, 1\\}$ with $\\sum_{k=1}^{K} z_{nk} = 1$, derive the Expectation-Maximization (EM) algorithm for this Gaussian mixture model (GMM). Your derivation must begin from the complete-data log-likelihood and proceed by formally defining the expectation step and the maximization step, culminating in explicit expressions for:\n- The expectation step responsibilities $\\gamma_{nk} = \\mathbb{E}[z_{nk} \\mid x_{n}, \\Theta^{(t)}]$.\n- The maximization step updates for $\\pi_{k}^{(t+1)}$, $\\mu_{k}^{(t+1)}$, and $\\Sigma_{k}^{(t+1)}$.\n\nThen, apply one EM iteration (one expectation step followed by one maximization step for the mixing proportions only) to the following biomarker dataset and initial parameter values for $K=2$ components. The three patient profiles are\n$$\nx_{1} = (12,\\, 110), \\quad x_{2} = (9,\\, 90), \\quad x_{3} = (21,\\, 210),\n$$\nwith biomarker $1$ in $\\mathrm{ng/mL}$ and biomarker $2$ in $\\mathrm{U/L}$. The initial parameters are\n$$\n\\pi_{1}^{(0)} = 0.6, \\quad \\pi_{2}^{(0)} = 0.4, \\quad\n\\mu_{1}^{(0)} = (10,\\, 100), \\quad \\mu_{2}^{(0)} = (20,\\, 200),\n$$\nand diagonal covariances\n$$\n\\Sigma_{1}^{(0)} = \\mathrm{diag}(4,\\, 400), \\quad \\Sigma_{2}^{(0)} = \\mathrm{diag}(9,\\, 900).\n$$\nCarry out the expectation step to compute the responsibilities $\\gamma_{nk}^{(0)}$ for $n \\in \\{1,2,3\\}$ and $k \\in \\{1,2\\}$ using these initial parameters. Then perform the maximization step for the mixing proportions to obtain $\\pi_{1}^{(1)}$.\n\nExpress the final answer for the updated mixing proportion $\\pi_{1}^{(1)}$ as a decimal number rounded to four significant figures. No units are required for the final answer.",
            "solution": "The derivation of the EM algorithm for a GMM proceeds in two parts: a general derivation of the update equations, followed by a numerical application of one iteration.\n\n**Part 1: General Derivation of the EM Algorithm for GMMs**\n\nLet the observed dataset be $X = \\{x_1, x_2, \\dots, x_N\\}$, where each $x_n$ is a $D$-dimensional real vector. The GMM assumes that each data point is generated from a mixture of $K$ Gaussian components. The likelihood of a single data point $x_n$ is given by:\n$$p(x_n \\mid \\Theta) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)$$\nwhere $\\Theta = \\{\\pi_k, \\mu_k, \\Sigma_k\\}_{k=1}^{K}$ are the model parameters: $\\pi_k$ are the mixing proportions satisfying $0 \\le \\pi_k \\le 1$ and $\\sum_{k=1}^K \\pi_k = 1$, $\\mu_k$ are the mean vectors, and $\\Sigma_k$ are the covariance matrices.\n\nThe EM algorithm is suited for maximum likelihood estimation in models with latent variables. We introduce a set of latent indicator variables $Z = \\{z_1, z_2, \\dots, z_N\\}$, where each $z_n$ is a $K$-dimensional binary vector with elements $z_{nk} \\in \\{0, 1\\}$ and $\\sum_{k=1}^K z_{nk} = 1$. The variable $z_{nk}=1$ indicates that data point $x_n$ was generated by component $k$.\n\nThe complete-data likelihood for the set $\\{X, Z\\}$ is given by:\n$$p(X, Z \\mid \\Theta) = \\prod_{n=1}^N p(x_n, z_n \\mid \\Theta) = \\prod_{n=1}^N \\prod_{k=1}^K \\left[ \\pi_k \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k) \\right]^{z_{nk}}$$\nThe complete-data log-likelihood, $\\mathcal{L}_c(\\Theta) = \\ln p(X, Z \\mid \\Theta)$, is:\n$$\\mathcal{L}_c(\\Theta) = \\sum_{n=1}^N \\sum_{k=1}^K z_{nk} \\left( \\ln \\pi_k + \\ln \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k) \\right)$$\nThe EM algorithm iterates between two steps: the Expectation (E) step and the Maximization (M) step.\n\n**1. Expectation (E) Step**\nIn the E-step, we compute the expectation of the complete-data log-likelihood with respect to the posterior distribution of the latent variables given the observed data $X$ and the current parameter estimates $\\Theta^{(t)}$. This defines the $Q$-function:\n$$Q(\\Theta \\mid \\Theta^{(t)}) = \\mathbb{E}_{Z \\mid X, \\Theta^{(t)}}[\\mathcal{L}_c(\\Theta)]$$\nThe expectation of $z_{nk}$ is the posterior probability that point $x_n$ belongs to component $k$, which is called the responsibility, denoted by $\\gamma_{nk}^{(t)}$:\n$$\\gamma_{nk}^{(t)} = \\mathbb{E}[z_{nk} \\mid x_n, \\Theta^{(t)}] = p(z_{nk}=1 \\mid x_n, \\Theta^{(t)})$$\nUsing Bayes' theorem:\n$$\\gamma_{nk}^{(t)} = \\frac{p(x_n \\mid z_{nk}=1, \\Theta^{(t)}) p(z_{nk}=1 \\mid \\Theta^{(t)})}{\\sum_{j=1}^K p(x_n \\mid z_{nj}=1, \\Theta^{(t)}) p(z_{nj}=1 \\mid \\Theta^{(t)})} = \\frac{\\pi_k^{(t)} \\mathcal{N}(x_n \\mid \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{j=1}^K \\pi_j^{(t)} \\mathcal{N}(x_n \\mid \\mu_j^{(t)}, \\Sigma_j^{(t)})}$$\nThis is the explicit expression for the responsibilities calculated in the E-step.\nThe $Q$-function becomes:\n$$Q(\\Theta \\mid \\Theta^{(t)}) = \\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{nk}^{(t)} \\left( \\ln \\pi_k + \\ln \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k) \\right)$$\n\n**2. Maximization (M) Step**\nIn the M-step, we find the new parameter estimates $\\Theta^{(t+1)}$ by maximizing the $Q$-function with respect to $\\Theta$:\n$$\\Theta^{(t+1)} = \\arg\\max_{\\Theta} Q(\\Theta \\mid \\Theta^{(t)})$$\nWe maximize with respect to $\\pi_k$, $\\mu_k$, and $\\Sigma_k$ separately.\n\nUpdate for $\\pi_k$: We maximize $\\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{nk}^{(t)} \\ln \\pi_k$ subject to the constraint $\\sum_{k=1}^K \\pi_k = 1$. Using a Lagrange multiplier, we find:\n$$\\pi_k^{(t+1)} = \\frac{1}{N} \\sum_{n=1}^N \\gamma_{nk}^{(t)}$$\n\nUpdate for $\\mu_k$: We maximize $\\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{nk}^{(t)} \\ln \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)$ with respect to $\\mu_k$. The term $\\ln \\mathcal{N}$ contains $-\\frac{1}{2}(x_n - \\mu_k)^T \\Sigma_k^{-1} (x_n - \\mu_k)$. Taking the derivative with respect to $\\mu_k$ and setting it to zero yields:\n$$\\mu_k^{(t+1)} = \\frac{\\sum_{n=1}^N \\gamma_{nk}^{(t)} x_n}{\\sum_{n=1}^N \\gamma_{nk}^{(t)}}$$\n\nUpdate for $\\Sigma_k$: We maximize the same term with respect to $\\Sigma_k$. This is more complex but results in:\n$$\\Sigma_k^{(t+1)} = \\frac{\\sum_{n=1}^N \\gamma_{nk}^{(t)} (x_n - \\mu_k^{(t+1)})(x_n - \\mu_k^{(t+1)})^T}{\\sum_{n=1}^N \\gamma_{nk}^{(t)}}$$\nFor convenience, we define the effective number of points in cluster $k$ as $N_k = \\sum_{n=1}^N \\gamma_{nk}^{(t)}$. The update rules are then compactly written as:\n$$\\pi_k^{(t+1)} = \\frac{N_k}{N}, \\quad \\mu_k^{(t+1)} = \\frac{1}{N_k}\\sum_{n=1}^N \\gamma_{nk}^{(t)} x_n, \\quad \\Sigma_k^{(t+1)} = \\frac{1}{N_k}\\sum_{n=1}^N \\gamma_{nk}^{(t)} (x_n - \\mu_k^{(t+1)})(x_n - \\mu_k^{(t+1)})^T$$\n\n**Part 2: Numerical Application for One Iteration**\n\nWe are given $N=3$ data points in $D=2$ dimensions:\n$x_1 = (12, 110)$, $x_2 = (9, 90)$, $x_3 = (21, 210)$.\nThe number of components is $K=2$. The initial parameters at iteration $t=0$ are:\n$\\pi_1^{(0)} = 0.6$, $\\pi_2^{(0)} = 0.4$\n$\\mu_1^{(0)} = (10, 100)$, $\\mu_2^{(0)} = (20, 200)$\n$\\Sigma_1^{(0)} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 400 \\end{pmatrix}$, $\\Sigma_2^{(0)} = \\begin{pmatrix} 9 & 0 \\\\ 0 & 900 \\end{pmatrix}$\n\nThe multivariate normal density for a $D$-dimensional vector $x$ is:\n$$\\mathcal{N}(x \\mid \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{D/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right)$$\nFor a diagonal covariance matrix $\\Sigma = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_D^2)$, this simplifies to:\n$$(x - \\mu)^T \\Sigma^{-1} (x - \\mu) = \\sum_{d=1}^D \\frac{(x_d - \\mu_d)^2}{\\sigma_d^2}$$\n\n**E-Step: Compute Responsibilities $\\gamma_{nk}^{(0)}$**\nWe need to calculate $\\mathcal{N}(x_n \\mid \\mu_k^{(0)}, \\Sigma_k^{(0)})$ for each $n \\in \\{1,2,3\\}$ and $k \\in \\{1,2\\}$. Let's denote the unnormalized probability densities by $p_{nk} = \\mathcal{N}(x_n \\mid \\mu_k^{(0)}, \\Sigma_k^{(0)})$.\n\nFor component $k=1$: $|\\Sigma_1^{(0)}| = 4 \\times 400 = 1600$. The normalization constant is $C_1 = (2\\pi \\sqrt{1600})^{-1} = (80\\pi)^{-1}$.\n- $n=1, x_1=(12, 110)$: Exponent argument is $-\\frac{1}{2}\\left(\\frac{(12-10)^2}{4} + \\frac{(110-100)^2}{400}\\right) = -\\frac{1}{2}(1 + 0.25) = -0.625$. So, $p_{11} = C_1 \\exp(-0.625)$.\n- $n=2, x_2=(9, 90)$: Exponent argument is $-\\frac{1}{2}\\left(\\frac{(9-10)^2}{4} + \\frac{(90-100)^2}{400}\\right) = -\\frac{1}{2}(0.25 + 0.25) = -0.25$. So, $p_{21} = C_1 \\exp(-0.25)$.\n- $n=3, x_3=(21, 210)$: Exponent argument is $-\\frac{1}{2}\\left(\\frac{(21-10)^2}{4} + \\frac{(210-100)^2}{400}\\right) = -\\frac{1}{2}(30.25 + 30.25) = -30.25$. So, $p_{31} = C_1 \\exp(-30.25)$.\n\nFor component $k=2$: $|\\Sigma_2^{(0)}| = 9 \\times 900 = 8100$. The normalization constant is $C_2 = (2\\pi \\sqrt{8100})^{-1} = (180\\pi)^{-1}$.\n- $n=1, x_1=(12, 110)$: Exponent argument is $-\\frac{1}{2}\\left(\\frac{(12-20)^2}{9} + \\frac{(110-200)^2}{900}\\right) = -\\frac{1}{2}(\\frac{64}{9} + \\frac{8100}{900}) = -\\frac{1}{2}(\\frac{64}{9} + 9) = -\\frac{145}{18} \\approx -8.0556$. So, $p_{12} = C_2 \\exp(-145/18)$.\n- $n=2, x_2=(9, 90)$: Exponent argument is $-\\frac{1}{2}\\left(\\frac{(9-20)^2}{9} + \\frac{(90-200)^2}{900}\\right) = -\\frac{1}{2}(\\frac{121}{9} + \\frac{12100}{900}) = -\\frac{1}{2}(\\frac{121}{9} + \\frac{121}{9}) = -\\frac{121}{9} \\approx -13.4444$. So, $p_{22} = C_2 \\exp(-121/9)$.\n- $n=3, x_3=(21, 210)$: Exponent argument is $-\\frac{1}{2}\\left(\\frac{(21-20)^2}{9} + \\frac{(210-200)^2}{900}\\right) = -\\frac{1}{2}(\\frac{1}{9} + \\frac{100}{900}) = -\\frac{1}{2}(\\frac{1}{9} + \\frac{1}{9}) = -\\frac{1}{9} \\approx -0.1111$. So, $p_{32} = C_2 \\exp(-1/9)$.\n\nNow we compute the numerators for the responsibilities, $\\pi_k^{(0)} p_{nk}$. Let's denote them $A_{nk} = \\pi_k^{(0)} p_{nk}$. The factor $(2\\pi)^{-1}$ cancels in $\\gamma_{nk}$.\n- $A_{11} = 0.6 \\frac{1}{40} \\exp(-0.625) \\approx 0.015 \\times 0.53526 = 0.0080289$\n- $A_{12} = 0.4 \\frac{1}{90} \\exp(-145/18) \\approx 0.00444 \\times 3.173 \\times 10^{-4} = 1.410 \\times 10^{-6}$\n- $A_{21} = 0.6 \\frac{1}{40} \\exp(-0.25) \\approx 0.015 \\times 0.77880 = 0.0116820$\n- $A_{22} = 0.4 \\frac{1}{90} \\exp(-121/9) \\approx 0.00444 \\times 1.450 \\times 10^{-6} = 6.442 \\times 10^{-9}$\n- $A_{31} = 0.6 \\frac{1}{40} \\exp(-30.25) \\approx 0.015 \\times 7.271 \\times 10^{-14} = 1.091 \\times 10^{-15}$\n- $A_{32} = 0.4 \\frac{1}{90} \\exp(-1/9) \\approx 0.00444 \\times 0.89484 = 0.0039771$\n\nNow, we compute the responsibilities $\\gamma_{nk}^{(0)} = A_{nk} / (A_{n1} + A_{n2})$:\n- $\\gamma_{11} = \\frac{0.0080289}{0.0080289 + 1.410 \\times 10^{-6}} = 0.999824$\n- $\\gamma_{21} = \\frac{0.0116820}{0.0116820 + 6.442 \\times 10^{-9}} = 0.999999$\n- $\\gamma_{31} = \\frac{1.091 \\times 10^{-15}}{1.091 \\times 10^{-15} + 0.0039771} \\approx 2.743 \\times 10^{-13} \\approx 0$\n\n**M-Step: Update Mixing Proportions (for $\\pi_1^{(1)}$ only)**\nThe problem only requires the update for the mixing proportions. We first compute $N_k = \\sum_{n=1}^3 \\gamma_{nk}^{(0)}$.\n$N_1 = \\gamma_{11} + \\gamma_{21} + \\gamma_{31} \\approx 0.999824 + 0.999999 + 0 = 1.999823$\nThe updated mixing proportion for component $1$ is:\n$$\\pi_1^{(1)} = \\frac{N_1}{N} = \\frac{1.999823}{3} \\approx 0.66660766...$$\nRounding to four significant figures, we get $0.6666$.",
            "answer": "$$\\boxed{0.6666}$$"
        }
    ]
}