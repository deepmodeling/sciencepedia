{
    "hands_on_practices": [
        {
            "introduction": "Understanding how one variable changes when we gain information about another is a cornerstone of predictive modeling. This practice guides you through the fundamental derivation of a conditional probability distribution, a concept that underpins all regression analysis. By applying this to the bivariate normal case—a common model for paired measurements like comparing two different lab assays—you will gain a deeper intuition for the phenomenon of \"regression to the mean\" and how to quantify the relationship between correlated variables .",
            "id": "3922069",
            "problem": "In a paired biomarker assessment for inter-method agreement, a research team compares plasma interleukin-6 concentrations measured by Enzyme-Linked Immunosorbent Assay (ELISA) and Mass Spectrometry (MS). Let $X$ denote the MS reading and $Y$ denote the ELISA reading for the same specimen. The team models the pair $(X,Y)$ as a continuous random vector with a joint probability density function (pdf) $f_{X,Y}(x,y)$ and marginal pdfs $f_{X}(x)$ and $f_{Y}(y)$. \n\nTask 1 (derivation from first principles): Starting only from the definitions of joint and marginal pdfs and conditional probability for continuous random variables, derive the conditional pdf $f_{X \\mid Y}(x \\mid y)$ in terms of $f_{X,Y}(x,y)$ and $f_{Y}(y)$.\n\nTask 2 (bivariate normal specialization and computation): Assume $(X,Y)$ is jointly Gaussian (bivariate normal) with means $\\mu_{X}$ and $\\mu_{Y}$, standard deviations $\\sigma_{X}$ and $\\sigma_{Y}$, and correlation $\\rho$. Using the result of Task 1 together with the well-established form of the bivariate normal pdf, derive the conditional distribution $X \\mid Y=y$ and identify its mean and variance in terms of the parameters $(\\mu_{X},\\mu_{Y},\\sigma_{X},\\sigma_{Y},\\rho)$ and the realized value $y$. Then, in the following scientifically plausible scenario, compute the conditional mean as a demonstration of the characteristic shrinkage of $X$ toward the regression line determined by $Y$:\n- $\\mu_{X} = 5$ $\\mathrm{ng/mL}$, $\\mu_{Y} = 5$ $\\mathrm{ng/mL}$,\n- $\\sigma_{X} = 1.2$ $\\mathrm{ng/mL}$, $\\sigma_{Y} = 1.6$ $\\mathrm{ng/mL}$,\n- $\\rho = 0.65$,\n- observed $Y = y = 8.2$ $\\mathrm{ng/mL}$.\n\nAnswer specification:\n- Report only the conditional mean $\\mathbb{E}[X \\mid Y=y]$ for this scenario as your final answer.\n- Round your answer to four significant figures.\n- Express the final numerical value in $\\mathrm{ng/mL}$.",
            "solution": "The problem is evaluated to be valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The context and parameters are plausible for a biomedical application.\n\nThe problem consists of two tasks. The first is a derivation of the formula for a conditional probability density function (pdf) from first principles. The second is the application of this formula to the specific case of a bivariate normal distribution to derive the conditional mean and variance, followed by a numerical calculation.\n\n**Task 1: Derivation of the Conditional Probability Density Function**\n\nWe begin from the definition of conditional probability for two events, $A$ and $B$, given by $P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$, for $P(B) > 0$. For continuous random variables $X$ and $Y$, the probability that $Y$ takes on a specific value $y$ is zero, i.e., $P(Y=y)=0$. We must therefore use a limiting argument.\n\nLet's define the conditional cumulative distribution function (CDF) of $X$ given $Y=y$, denoted $F_{X \\mid Y}(x \\mid y)$, as the probability $P(X \\le x \\mid Y=y)$. We consider the event that $Y$ falls within a small interval $(y, y+\\Delta y]$, where $\\Delta y > 0$. The conditional probability is:\n$$\nP(X \\le x \\mid y < Y \\le y+\\Delta y) = \\frac{P(X \\le x, y < Y \\le y+\\Delta y)}{P(y < Y \\le y+\\Delta y)}\n$$\nFor a small $\\Delta y$, the probabilities in the numerator and denominator can be approximated using the joint pdf $f_{X,Y}(u,v)$ and the marginal pdf $f_Y(v)$, respectively.\nThe probability in the denominator is:\n$$\nP(y < Y \\le y+\\Delta y) = \\int_y^{y+\\Delta y} f_Y(v) \\, dv \\approx f_Y(y) \\Delta y\n$$\nThe probability in the numerator is:\n$$\nP(X \\le x, y < Y \\le y+\\Delta y) = \\int_{-\\infty}^x \\int_y^{y+\\Delta y} f_{X,Y}(u,v) \\, dv \\, du \\approx \\int_{-\\infty}^x f_{X,Y}(u,y) \\Delta y \\, du\n$$\nSubstituting these approximations into the ratio gives:\n$$\nP(X \\le x \\mid y < Y \\le y+\\Delta y) \\approx \\frac{\\left( \\int_{-\\infty}^x f_{X,Y}(u,y) \\, du \\right) \\Delta y}{f_Y(y) \\Delta y} = \\frac{\\int_{-\\infty}^x f_{X,Y}(u,y) \\, du}{f_Y(y)}\n$$\nTaking the limit as $\\Delta y \\to 0$, the approximation becomes exact, and we obtain the conditional CDF:\n$$\nF_{X \\mid Y}(x \\mid y) = \\lim_{\\Delta y \\to 0} P(X \\le x \\mid y < Y \\le y+\\Delta y) = \\frac{\\int_{-\\infty}^x f_{X,Y}(u,y) \\, du}{f_Y(y)}\n$$\nThe conditional pdf, $f_{X \\mid Y}(x \\mid y)$, is the derivative of the conditional CDF with respect to $x$. Applying the Fundamental Theorem of Calculus:\n$$\nf_{X \\mid Y}(x \\mid y) = \\frac{d}{dx} F_{X \\mid Y}(x \\mid y) = \\frac{d}{dx} \\left[ \\frac{\\int_{-\\infty}^x f_{X,Y}(u,y) \\, du}{f_Y(y)} \\right] = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\n$$\nThis is the desired expression for the conditional pdf, valid for any region where $f_Y(y)>0$.\n\n**Task 2: Bivariate Normal Specialization and Computation**\n\nWe are given that $(X,Y)$ follows a bivariate normal distribution with parameters $(\\mu_X, \\mu_Y, \\sigma_X, \\sigma_Y, \\rho)$. The joint pdf is:\n$$\nf_{X,Y}(x,y) = \\frac{1}{2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}} \\exp\\left( -\\frac{1}{2(1-\\rho^2)} \\left[ \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)^2 - 2\\rho\\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right) + \\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \\right] \\right)\n$$\nThe marginal distribution of $Y$ is also normal, $Y \\sim N(\\mu_Y, \\sigma_Y^2)$, with pdf:\n$$\nf_Y(y) = \\frac{1}{\\sqrt{2\\pi} \\sigma_Y} \\exp\\left( -\\frac{1}{2} \\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \\right)\n$$\nUsing the result from Task 1, we find the conditional pdf $f_{X \\mid Y}(x \\mid y)$ by dividing $f_{X,Y}(x,y)$ by $f_Y(y)$.\nThe ratio of the constant pre-factors is:\n$$\n\\frac{1/(2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2})}{1/(\\sqrt{2\\pi} \\sigma_Y)} = \\frac{1}{\\sqrt{2\\pi} \\sigma_X \\sqrt{1-\\rho^2}}\n$$\nThe exponent of the conditional pdf is the difference of the exponents of the joint and marginal pdfs:\n$$\nE = -\\frac{1}{2(1-\\rho^2)} \\left[ \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)^2 - 2\\rho\\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right) + \\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \\right] - \\left( -\\frac{1}{2} \\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \\right)\n$$\nTo simplify, let $z_x = \\frac{x-\\mu_X}{\\sigma_X}$ and $z_y = \\frac{y-\\mu_Y}{\\sigma_Y}$.\n$$\nE = -\\frac{1}{2(1-\\rho^2)} [ z_x^2 - 2\\rho z_x z_y + z_y^2 ] + \\frac{1-\\rho^2}{2(1-\\rho^2)} z_y^2 = -\\frac{1}{2(1-\\rho^2)} [z_x^2 - 2\\rho z_x z_y + z_y^2 - (1-\\rho^2)z_y^2]\n$$\n$$\nE = -\\frac{1}{2(1-\\rho^2)} [z_x^2 - 2\\rho z_x z_y + z_y^2 - z_y^2 + \\rho^2 z_y^2] = -\\frac{1}{2(1-\\rho^2)} [z_x^2 - 2\\rho z_x z_y + \\rho^2 z_y^2]\n$$\nThe term in the brackets is a perfect square: $[z_x - \\rho z_y]^2$.\n$$\nE = -\\frac{1}{2(1-\\rho^2)} (z_x - \\rho z_y)^2\n$$\nSubstituting back for $z_x$ and $z_y$:\n$$\nE = -\\frac{1}{2(1-\\rho^2)} \\left( \\frac{x-\\mu_X}{\\sigma_X} - \\rho \\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2\n$$\nWe rearrange the term inside the parenthesis to isolate $x$:\n$$\n\\frac{x-\\mu_X}{\\sigma_X} - \\rho \\frac{y-\\mu_Y}{\\sigma_Y} = \\frac{1}{\\sigma_X} \\left( (x-\\mu_X) - \\rho \\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y) \\right) = \\frac{1}{\\sigma_X} \\left( x - \\left[\\mu_X + \\rho\\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y)\\right] \\right)\n$$\nSubstituting this back into the exponent expression:\n$$\nE = -\\frac{1}{2(1-\\rho^2)\\sigma_X^2} \\left( x - \\left[\\mu_X + \\rho\\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y)\\right] \\right)^2\n$$\nThis is the exponent of a normal distribution pdf of the form $-\\frac{(x-\\mu)^2}{2\\sigma^2}$. By inspection, we can identify the parameters of the conditional distribution $X \\mid Y=y$.\nThe conditional mean is:\n$$\n\\mathbb{E}[X \\mid Y=y] = \\mu_{X \\mid Y=y} = \\mu_X + \\rho \\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y)\n$$\nThe conditional variance is:\n$$\n\\mathrm{Var}(X \\mid Y=y) = \\sigma^2_{X \\mid Y=y} = \\sigma_X^2 (1-\\rho^2)\n$$\nSo, the conditional distribution of $X$ given $Y=y$ is normal: $X \\mid Y=y \\sim N(\\mu_X + \\rho\\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y), \\sigma_X^2(1-\\rho^2))$.\n\nNow we compute the conditional mean for the given scenario:\n- $\\mu_{X} = 5$\n- $\\mu_{Y} = 5$\n- $\\sigma_{X} = 1.2$\n- $\\sigma_{Y} = 1.6$\n- $\\rho = 0.65$\n- $y = 8.2$\n\nWe substitute these values into the formula for the conditional mean:\n$$\n\\mathbb{E}[X \\mid Y=8.2] = 5 + (0.65) \\frac{1.2}{1.6} (8.2 - 5)\n$$\nFirst, calculate the terms within the expression:\n$$\n\\frac{1.2}{1.6} = \\frac{3 \\times 0.4}{4 \\times 0.4} = \\frac{3}{4} = 0.75\n$$\n$$\n8.2 - 5 = 3.2\n$$\nNow, substitute these back into the equation for the mean:\n$$\n\\mathbb{E}[X \\mid Y=8.2] = 5 + (0.65)(0.75)(3.2)\n$$\n$$\n\\mathbb{E}[X \\mid Y=8.2] = 5 + (0.4875)(3.2)\n$$\n$$\n\\mathbb{E}[X \\mid Y=8.2] = 5 + 1.56\n$$\n$$\n\\mathbb{E}[X \\mid Y=8.2] = 6.56\n$$\nThe problem requires the answer to be rounded to four significant figures. Thus, the result is $6.560$.",
            "answer": "$$\n\\boxed{6.560}\n$$"
        },
        {
            "introduction": "Biomedical data is often incomplete; for instance, in a clinical study, some subjects may not experience the event of interest by the study's end. This exercise tackles this challenge of \"right-censoring\" by showing you how to construct a valid likelihood function from first principles. By deriving the maximum likelihood estimator (MLE) for an exponential survival model, you will master a powerful and flexible technique for parameter estimation that correctly incorporates information from both observed events and censored data points .",
            "id": "3921993",
            "problem": "In a cohort study of implantable ventricular assist devices, early mechanical occlusion is modeled as having a constant instantaneous failure risk over time. Let the time to occlusion for subject $i$ be the continuous random variable $T_i$, assumed independent across subjects and governed by an exponential model with constant hazard rate $\\lambda$. Right-censoring occurs due to study termination or loss to follow-up, yielding the observed time $Y_i = \\min(T_i, C_i)$ and an event indicator $\\Delta_i = \\mathbf{1}\\{T_i \\leq C_i\\}$, where $C_i$ is the independent censoring time. Use the fundamental relationships among the hazard function $h(t)$, survival function $S(t)$, and probability density function $f(t)$, together with independence and the definition of likelihood, to derive the likelihood for $\\lambda$ under independent right-censoring.\n\nThen, using the principle of Maximum Likelihood Estimation (MLE), obtain the closed-form expression for the MLE of $\\lambda$ in terms of observable quantities. Finally, compute its numerical value from the following $n=12$ observed pairs $(Y_i, \\Delta_i)$ (times in days):\n$(5, 1)$, $(6, 1)$, $(7, 1)$, $(8, 1)$, $(9, 1)$, $(10, 1)$, $(12, 1)$, $(13, 1)$, $(1.5, 0)$, $(3.0, 0)$, $(2.5, 0)$, $(3.0, 0)$.\nRound your numerical answer to four significant figures and express the final rate in $\\text{day}^{-1}$.\n\nIn addition, based on the curvature of the log-likelihood, briefly discuss how right-censoring modifies information about $\\lambda$ in this exponential survival model relative to complete (uncensored) data. The final numerical answer must be a single real number.",
            "solution": "The problem is first validated against the required criteria.\n\n**Step 1: Extract Givens**\n- The time to occlusion for subject $i$, $T_i$, is a continuous random variable.\n- $T_i$ are independent across subjects.\n- $T_i$ follows an exponential model with a constant hazard rate $\\lambda$.\n- Right-censoring occurs, with $C_i$ being the independent censoring time.\n- The observed time is $Y_i = \\min(T_i, C_i)$.\n- The event indicator is $\\Delta_i = \\mathbf{1}\\{T_i \\leq C_i\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- The number of subjects is $n=12$.\n- The observed data pairs $(Y_i, \\Delta_i)$ are: $(5, 1)$, $(6, 1)$, $(7, 1)$, $(8, 1)$, $(9, 1)$, $(10, 1)$, $(12, 1)$, $(13, 1)$, $(1.5, 0)$, $(3.0, 0)$, $(2.5, 0)$, $(3.0, 0)$.\n- The final numerical answer must be rounded to four significant figures and expressed in units of $\\text{day}^{-1}$.\n- The final answer box must contain a single real number.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is well-grounded in the statistical theory of survival analysis, a core topic in biostatistics and biomedical modeling. The exponential model for failure time, the concept of a constant hazard rate, independent right-censoring, and maximum likelihood estimation are all standard, well-established principles.\n- **Well-Posed**: The problem is well-posed. It clearly defines the model, the data structure, and the inferential goal (deriving and calculating the MLE for $\\lambda$). It provides all necessary information for a unique solution.\n- **Objective**: The problem is stated using precise, objective mathematical and statistical terminology.\n- **Completeness and Consistency**: The problem is self-contained and consistent. It provides the model, parameters, and the data required for the solution. There are no contradictions.\n- **Realism**: The scenario of modeling device failure with an exponential distribution and accounting for censoring is a standard and realistic application in biomedical engineering and clinical trials. The data values are plausible.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution is warranted.\n\n**Derivation of the Likelihood Function**\nThe time to event $T$ is modeled by an exponential distribution with a constant hazard rate $\\lambda$. The fundamental functions associated with this model are:\n1.  The hazard function: $h(t) = \\lambda$ for $t \\ge 0$.\n2.  The survival function: $S(t) = P(T > t) = \\exp\\left(-\\int_0^t h(u) \\,du\\right) = \\exp\\left(-\\int_0^t \\lambda \\,du\\right) = \\exp(-\\lambda t)$.\n3.  The probability density function (PDF): $f(t) = -\\frac{dS(t)}{dt} = - \\frac{d}{dt}\\exp(-\\lambda t) = \\lambda \\exp(-\\lambda t)$.\n\nThe likelihood function is constructed from the contributions of each of the $n$ independent subjects. The contribution of subject $i$ depends on whether their event was observed or censored, indicated by $\\Delta_i$.\n\n- If the event is observed for subject $i$ ($\\Delta_i = 1$), it means the failure occurred at time $T_i = Y_i$ and this happened before the censoring time $C_i$. The contribution to the likelihood is the probability density of failure at time $Y_i$, which is $f(Y_i) = \\lambda \\exp(-\\lambda Y_i)$.\n- If the observation is right-censored for subject $i$ ($\\Delta_i = 0$), it means the failure time $T_i$ is greater than the censoring time $C_i$, and the observed time is $Y_i = C_i$. We only know that the subject survived at least until time $Y_i$. The contribution to the likelihood is the probability of this event, which is $P(T_i > Y_i) = S(Y_i) = \\exp(-\\lambda Y_i)$.\n\nA single expression for the likelihood contribution of subject $i$, $L_i(\\lambda)$, can be written using the event indicator $\\Delta_i$:\n$$L_i(\\lambda) = [f(Y_i)]^{\\Delta_i} [S(Y_i)]^{1-\\Delta_i}$$\nSubstituting the expressions for $f(t)$ and $S(t)$:\n$$L_i(\\lambda) = [\\lambda \\exp(-\\lambda Y_i)]^{\\Delta_i} [\\exp(-\\lambda Y_i)]^{1-\\Delta_i}$$\n$$L_i(\\lambda) = \\lambda^{\\Delta_i} \\exp(-\\lambda Y_i \\Delta_i) \\exp(-\\lambda Y_i (1-\\Delta_i))$$\n$$L_i(\\lambda) = \\lambda^{\\Delta_i} \\exp(-\\lambda Y_i (\\Delta_i + 1 - \\Delta_i))$$\n$$L_i(\\lambda) = \\lambda^{\\Delta_i} \\exp(-\\lambda Y_i)$$\nSince all subjects are independent, the total likelihood for the sample of $n$ subjects, $L(\\lambda)$, is the product of the individual contributions:\n$$L(\\lambda) = \\prod_{i=1}^n L_i(\\lambda) = \\prod_{i=1}^n \\left( \\lambda^{\\Delta_i} \\exp(-\\lambda Y_i) \\right)$$\n$$L(\\lambda) = \\left( \\prod_{i=1}^n \\lambda^{\\Delta_i} \\right) \\left( \\prod_{i=1}^n \\exp(-\\lambda Y_i) \\right)$$\n$$L(\\lambda) = \\lambda^{\\sum_{i=1}^n \\Delta_i} \\exp\\left(-\\lambda \\sum_{i=1}^n Y_i\\right)$$\nLet $d = \\sum_{i=1}^n \\Delta_i$ be the total number of observed events, and let $Y_{\\text{total}} = \\sum_{i=1}^n Y_i$ be the total observed time-at-risk. The likelihood function simplifies to:\n$$L(\\lambda) = \\lambda^d \\exp(-\\lambda Y_{\\text{total}})$$\n\n**Derivation of the Maximum Likelihood Estimator (MLE)**\nTo find the MLE for $\\lambda$, we maximize $L(\\lambda)$. It is computationally simpler to maximize the natural logarithm of the likelihood, the log-likelihood function $\\ell(\\lambda)$:\n$$\\ell(\\lambda) = \\ln(L(\\lambda)) = \\ln\\left( \\lambda^d \\exp(-\\lambda Y_{\\text{total}}) \\right)$$\n$$\\ell(\\lambda) = d \\ln(\\lambda) - \\lambda Y_{\\text{total}}$$\nTo find the maximum, we take the first derivative of $\\ell(\\lambda)$ with respect to $\\lambda$ and set it to zero:\n$$\\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda} (d \\ln(\\lambda) - \\lambda Y_{\\text{total}}) = \\frac{d}{\\lambda} - Y_{\\text{total}}$$\nSetting the derivative to zero to find the estimator $\\hat{\\lambda}$:\n$$\\frac{d}{\\hat{\\lambda}} - Y_{\\text{total}} = 0 \\implies \\frac{d}{\\hat{\\lambda}} = Y_{\\text{total}}$$\nSolving for $\\hat{\\lambda}$ gives the MLE:\n$$\\hat{\\lambda}_{\\text{MLE}} = \\frac{d}{Y_{\\text{total}}} = \\frac{\\sum_{i=1}^n \\Delta_i}{\\sum_{i=1}^n Y_i}$$\nThis expression is the total number of events divided by the total person-time of follow-up. To confirm this is a maximum, we check the second derivative:\n$$\\frac{d^2\\ell}{d\\lambda^2} = -\\frac{d}{\\lambda^2}$$\nSince the number of events $d \\ge 0$ and $\\lambda^2 > 0$, the second derivative is non-positive. For $d > 0$, it is strictly negative, which confirms that $\\hat{\\lambda}_{\\text{MLE}}$ corresponds to a local maximum.\n\n**Numerical Calculation**\nThe provided data consists of $n=12$ pairs $(Y_i, \\Delta_i)$:\n$(5, 1)$, $(6, 1)$, $(7, 1)$, $(8, 1)$, $(9, 1)$, $(10, 1)$, $(12, 1)$, $(13, 1)$, $(1.5, 0)$, $(3.0, 0)$, $(2.5, 0)$, $(3.0, 0)$.\n\nFirst, we calculate the total number of events, $d$:\n$$d = \\sum_{i=1}^{12} \\Delta_i = 1+1+1+1+1+1+1+1+0+0+0+0 = 8$$\nNext, we calculate the total time-at-risk, $Y_{\\text{total}}$:\n$$Y_{\\text{total}} = \\sum_{i=1}^{12} Y_i = (5+6+7+8+9+10+12+13) + (1.5+3.0+2.5+3.0)$$\n$$Y_{\\text{total}} = 70 + 10 = 80$$\nThe times are given in days. Now, we compute the MLE for $\\lambda$:\n$$\\hat{\\lambda}_{\\text{MLE}} = \\frac{d}{Y_{\\text{total}}} = \\frac{8}{80} = \\frac{1}{10} = 0.1$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\\hat{\\lambda}_{\\text{MLE}} = 0.1000 \\, \\text{day}^{-1}$$\n\n**Discussion on Information Modification by Censoring**\nThe information about the parameter $\\lambda$ is related to the curvature of the log-likelihood function at the MLE. The observed Fisher information is defined as $I(\\lambda) = -\\frac{d^2\\ell}{d\\lambda^2}$. In this model, we found:\n$$I(\\lambda) = - \\left(-\\frac{d}{\\lambda^2}\\right) = \\frac{d}{\\lambda^2}$$\nA larger value of $I(\\lambda)$ implies a sharper peak in the likelihood function, corresponding to more information about $\\lambda$ and a smaller variance for its estimator (since $\\text{Var}(\\hat{\\lambda}) \\approx 1/I(\\hat{\\lambda})$).\nThe information is directly proportional to $d$, the number of observed events. Right-censoring, by definition, implies that for a given sample of size $n$, the number of events $d$ is less than or equal to $n$ ($d \\le n$). If there were no censoring (a complete dataset of failures), every subject would contribute an event, so we would have $d=n$.\nThe presence of right-censoring reduces the number of observed events from a potential maximum of $n$ to the observed value $d$. This reduction in $d$ directly leads to a smaller value of the information $I(\\lambda)$. A smaller value of information means the log-likelihood function is less peaked (has a smaller curvature), which reflects greater uncertainty in the estimate of $\\lambda$. Therefore, right-censoring modifies the information by reducing it; the censored observations contribute to the total time-at-risk (the denominator of $\\hat{\\lambda}$) but not to the event count (the numerator), which is the sole driver of statistical information for this model.",
            "answer": "$$\\boxed{0.1000}$$"
        },
        {
            "introduction": "Many biomedical systems are dynamic, with underlying states that evolve over time and can only be observed through noisy measurements. This practice introduces a cornerstone of modern time-series analysis: the Kalman filter. Starting from Bayes' rule, you will derive the filter's core \"measurement update\" equations, learning how to optimally blend a model's prediction with new data to produce a refined estimate of a hidden state, such as real-time blood glucose concentration .",
            "id": "3921987",
            "problem": "A patient’s interstitial blood glucose concentration, denoted by the latent state $x_k$ at discrete time index $k$, is modeled as a linear Gaussian state-space process appropriate for short-horizon physiology where unmodeled inputs (e.g., insulin action and carbohydrate absorption) are lumped into process noise. Specifically, assume a random-walk state evolution $x_k = x_{k-1} + w_k$ with $w_k \\sim \\mathcal{N}(0,Q)$ and an observation model for a Continuous Glucose Monitor (CGM) measurement $z_k = x_k + v_k$ with $v_k \\sim \\mathcal{N}(0,R)$. Here $\\mathcal{N}$ denotes the normal distribution, $Q \\ge 0$ and $R > 0$ are known scalars, and all noises are mutually independent and independent of the initial state. You are provided at time $k$ with a Gaussian prior (the prediction from the process model) for $x_k$ of the form $x_k \\mid \\mathcal{Z}_{k-1} \\sim \\mathcal{N}(\\hat{x}_{k \\mid k-1}, P_{k \\mid k-1})$, where $\\mathcal{Z}_{k-1}$ denotes all measurements up to time $k-1$, and a new CGM measurement $z_k$.\n\nTask:\n1. Starting from Bayes’ rule and the properties of multivariate normal distributions (without invoking any pre-derived Kalman filter formulas), derive the recursive measurement-update equations for the posterior mean $\\hat{x}_{k \\mid k}$ and posterior variance $P_{k \\mid k}$ in the scalar case described above. Your derivation must explicitly identify the quantity commonly called the Kalman gain in terms of $P_{k \\mid k-1}$ and $R$.\n2. Apply your derived update to the following physiologically plausible scenario: the prior at time $k$ is $\\hat{x}_{k \\mid k-1} = 150 \\text{ mg/dL}$ and $P_{k \\mid k-1} = 400 \\text{ (mg/dL)}^2$, and the CGM reports $z_k = 172 \\text{ mg/dL}$ with a known measurement noise variance $R = 100 \\text{ (mg/dL)}^2$. Report the posterior mean $\\hat{x}_{k \\mid k}$ in $\\text{mg/dL}$.\nExpress the final numeric answer as instructed below.\n\nAnswer specification:\n- The final answer must be the numerical value of the posterior mean $\\hat{x}_{k \\mid k}$, expressed in $\\text{mg/dL}$, rounded to four significant figures.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in established principles of state estimation and biomedical modeling, is mathematically well-posed with sufficient information for a unique solution, and is expressed in objective, unambiguous language. We may therefore proceed with the derivation and calculation.\n\n**1. Derivation of the Measurement-Update Equations**\n\nThe objective is to find the posterior probability density function (PDF) of the state $x_k$ given all measurements up to time $k$, denoted $\\mathcal{Z}_k = \\{\\mathcal{Z}_{k-1}, z_k\\}$. This PDF is $p(x_k \\mid \\mathcal{Z}_k)$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$p(x_k \\mid \\mathcal{Z}_k) = p(x_k \\mid z_k, \\mathcal{Z}_{k-1}) \\propto p(z_k \\mid x_k, \\mathcal{Z}_{k-1}) p(x_k \\mid \\mathcal{Z}_{k-1})$$\n\nThe terms in this expression are defined as follows:\n*   The prior distribution is given as Gaussian: $p(x_k \\mid \\mathcal{Z}_{k-1}) \\sim \\mathcal{N}(\\hat{x}_{k \\mid k-1}, P_{k \\mid k-1})$. Its PDF is:\n$$p(x_k \\mid \\mathcal{Z}_{k-1}) = \\frac{1}{\\sqrt{2\\pi P_{k \\mid k-1}}} \\exp\\left(-\\frac{1}{2} \\frac{(x_k - \\hat{x}_{k \\mid k-1})^2}{P_{k \\mid k-1}}\\right)$$\n\n*   The likelihood function $p(z_k \\mid x_k, \\mathcal{Z}_{k-1})$ is determined by the measurement model $z_k = x_k + v_k$. Since the measurement noise $v_k$ is independent of the state $x_k$ and past measurements $\\mathcal{Z}_{k-1}$, we have $p(z_k \\mid x_k, \\mathcal{Z}_{k-1}) = p(z_k \\mid x_k)$. The noise $v_k = z_k - x_k$ is distributed as $\\mathcal{N}(0, R)$, which implies that the likelihood, viewed as a function of $x_k$, is a Gaussian centered at $z_k$ with variance $R$:\n$$p(z_k \\mid x_k) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(z_k - x_k)^2}{R}\\right)$$\n\nThe product of two Gaussian PDFs is proportional to another Gaussian PDF. The posterior $p(x_k \\mid \\mathcal{Z}_k)$ is therefore Gaussian, which we can write as $x_k \\mid \\mathcal{Z}_k \\sim \\mathcal{N}(\\hat{x}_{k \\mid k}, P_{k \\mid k})$. To find the posterior mean $\\hat{x}_{k \\mid k}$ and variance $P_{k \\mid k}$, we analyze the exponent of the posterior PDF.\n$$p(x_k \\mid \\mathcal{Z}_k) \\propto \\exp\\left(-\\frac{1}{2} \\frac{(x_k - \\hat{x}_{k \\mid k-1})^2}{P_{k \\mid k-1}}\\right) \\exp\\left(-\\frac{1}{2} \\frac{(x_k - z_k)^2}{R}\\right)$$\n$$p(x_k \\mid \\mathcal{Z}_k) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(x_k - \\hat{x}_{k \\mid k-1})^2}{P_{k \\mid k-1}} + \\frac{(x_k - z_k)^2}{R} \\right] \\right)$$\n\nLet $\\Phi$ be the expression inside the brackets in the exponent. We expand and collect terms in powers of $x_k$:\n$$\\Phi = \\frac{x_k^2 - 2x_k\\hat{x}_{k \\mid k-1} + \\hat{x}_{k \\mid k-1}^2}{P_{k \\mid k-1}} + \\frac{x_k^2 - 2x_k z_k + z_k^2}{R}$$\n$$\\Phi = x_k^2 \\left(\\frac{1}{P_{k \\mid k-1}} + \\frac{1}{R}\\right) - 2x_k \\left(\\frac{\\hat{x}_{k \\mid k-1}}{P_{k \\mid k-1}} + \\frac{z_k}{R}\\right) + C$$\nwhere $C$ contains terms that do not depend on $x_k$.\n\nThe exponent of the posterior PDF $\\mathcal{N}(\\hat{x}_{k \\mid k}, P_{k \\mid k})$ has the form $-\\frac{1}{2} \\frac{(x_k - \\hat{x}_{k \\mid k})^2}{P_{k \\mid k}}$, which expands to:\n$$-\\frac{1}{2} \\left[ \\frac{1}{P_{k \\mid k}}x_k^2 - \\frac{2\\hat{x}_{k \\mid k}}{P_{k \\mid k}}x_k + \\frac{\\hat{x}_{k \\mid k}^2}{P_{k \\mid k}} \\right]$$\n\nBy comparing the coefficients of the $x_k^2$ term in $\\Phi$ and the posterior exponent, we identify the inverse of the posterior variance $P_{k \\mid k}$:\n$$\\frac{1}{P_{k \\mid k}} = \\frac{1}{P_{k \\mid k-1}} + \\frac{1}{R} = \\frac{P_{k \\mid k-1} + R}{P_{k \\mid k-1} R}$$\nSolving for $P_{k \\mid k}$ yields the posterior variance update equation:\n$$P_{k \\mid k} = \\frac{P_{k \\mid k-1} R}{P_{k \\mid k-1} + R}$$\n\nNext, by comparing the coefficients of the linear $x_k$ term, we have:\n$$\\frac{\\hat{x}_{k \\mid k}}{P_{k \\mid k}} = \\frac{\\hat{x}_{k \\mid k-1}}{P_{k \\mid k-1}} + \\frac{z_k}{R}$$\nSolving for $\\hat{x}_{k \\mid k}$ gives the posterior mean:\n$$\\hat{x}_{k \\mid k} = P_{k \\mid k} \\left(\\frac{\\hat{x}_{k \\mid k-1}}{P_{k \\mid k-1}} + \\frac{z_k}{R}\\right)$$\nSubstituting the expression for $P_{k \\mid k}$:\n$$\\hat{x}_{k \\mid k} = \\frac{P_{k \\mid k-1} R}{P_{k \\mid k-1} + R} \\left(\\frac{\\hat{x}_{k \\mid k-1}}{P_{k \\mid k-1}} + \\frac{z_k}{R}\\right) = \\frac{R\\hat{x}_{k \\mid k-1} + P_{k \\mid k-1}z_k}{P_{k \\mid k-1} + R}$$\nThis expression shows the updated mean as a weighted average of the prior mean and the measurement. To express this in the conventional form involving the Kalman gain, we perform an algebraic manipulation:\n$$\\hat{x}_{k \\mid k} = \\frac{(P_{k \\mid k-1} + R)\\hat{x}_{k \\mid k-1} - P_{k \\mid k-1}\\hat{x}_{k \\mid k-1} + P_{k \\mid k-1}z_k}{P_{k \\mid k-1} + R}$$\n$$\\hat{x}_{k \\mid k} = \\frac{(P_{k \\mid k-1} + R)\\hat{x}_{k \\mid k-1}}{P_{k \\mid k-1} + R} + \\frac{P_{k \\mid k-1}(z_k - \\hat{x}_{k \\mid k-1})}{P_{k \\mid k-1} + R}$$\n$$\\hat{x}_{k \\mid k} = \\hat{x}_{k \\mid k-1} + \\left(\\frac{P_{k \\mid k-1}}{P_{k \\mid k-1} + R}\\right)(z_k - \\hat{x}_{k \\mid k-1})$$\nThis is the measurement-update equation for the posterior mean. The term multiplying the innovation $(z_k - \\hat{x}_{k \\mid k-1})$ is defined as the Kalman gain, $K_k$:\n$$K_k = \\frac{P_{k \\mid k-1}}{P_{k \\mid k-1} + R}$$\nThus, the update equation for the mean is $\\hat{x}_{k \\mid k} = \\hat{x}_{k \\mid k-1} + K_k(z_k - \\hat{x}_{k \\mid k-1})$.\n\nThe posterior variance update can also be expressed using $K_k$:\n$$P_{k \\mid k} = \\left(1 - \\frac{P_{k \\mid k-1}}{P_{k \\mid k-1} + R}\\right) P_{k \\mid k-1} = (1 - K_k)P_{k \\mid k-1}$$\nThis completes the derivation of the recursive measurement-update equations.\n\n**2. Application to the Physiological Scenario**\n\nWe are given the following values:\n*   Prior mean: $\\hat{x}_{k \\mid k-1} = 150 \\text{ mg/dL}$\n*   Prior variance: $P_{k \\mid k-1} = 400 \\text{ (mg/dL)}^2$\n*   Measurement: $z_k = 172 \\text{ mg/dL}$\n*   Measurement noise variance: $R = 100 \\text{ (mg/dL)}^2$\n\nFirst, we compute the Kalman gain $K_k$:\n$$K_k = \\frac{P_{k \\mid k-1}}{P_{k \\mid k-1} + R} = \\frac{400}{400 + 100} = \\frac{400}{500} = 0.8$$\nThe gain $K_k=0.8$ is dimensionless. It determines that the correction to the mean will be $80\\%$ of the innovation (the difference between the measurement and the prior estimate).\n\nNext, we apply the measurement-update equation for the posterior mean $\\hat{x}_{k \\mid k}$:\n$$\\hat{x}_{k \\mid k} = \\hat{x}_{k \\mid k-1} + K_k(z_k - \\hat{x}_{k \\mid k-1})$$\nSubstituting the numerical values:\n$$\\hat{x}_{k \\mid k} = 150 + 0.8 \\times (172 - 150)$$\n$$\\hat{x}_{k \\mid k} = 150 + 0.8 \\times (22)$$\n$$\\hat{x}_{k \\mid k} = 150 + 17.6$$\n$$\\hat{x}_{k \\mid k} = 167.6$$\n\nThe posterior mean glucose concentration is $167.6 \\text{ mg/dL}$. This value lies between the prior estimate ($150$) and the new measurement ($172$), and is closer to the measurement because the prior variance ($P_{k \\mid k-1}=400$) is larger than the measurement variance ($R=100$), indicating we have more confidence in the new measurement. The requested rounding to four significant figures is already satisfied by the result $167.6$.",
            "answer": "$$\n\\boxed{167.6}\n$$"
        }
    ]
}