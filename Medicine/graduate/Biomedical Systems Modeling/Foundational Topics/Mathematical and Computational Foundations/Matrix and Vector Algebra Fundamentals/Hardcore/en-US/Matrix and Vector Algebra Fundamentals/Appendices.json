{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of robust data analysis is the ability to construct a numerically stable basis from a set of measurement vectors. This practice explores the Gram-Schmidt process, a fundamental algorithm for creating an orthonormal basis, within the context of diffusion MRI where gradient directions can be nearly collinear. By working through this exercise , you will not only derive a QR factorization but also gain crucial insight into why near-linear dependence poses a significant challenge in finite-precision arithmetic and how to identify its effects.",
            "id": "3899444",
            "problem": "In diffusion Magnetic Resonance Imaging (MRI), a set of diffusion-encoding gradient directions can be represented as vectors in $\\mathbb{R}^{3}$. When these directions are nearly colinear, the corresponding design matrix becomes ill-conditioned, and orthonormalization is required to stabilize estimation in linear models of tissue microstructure. Consider the following three diffusion direction vectors, parameterized by a small positive parameter $\\epsilon$:\n$$\n\\mathbf{a}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{a}_{2} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{a}_{3} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix},\n$$\nwith $\\epsilon  0$ and $\\epsilon \\ll 1$. Let $A = \\begin{pmatrix} \\mathbf{a}_{1}  \\mathbf{a}_{2}  \\mathbf{a}_{3} \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3}$. Your tasks are:\n\n- Using only the definitions of the Euclidean inner product and orthogonal projection as your starting point, apply the Gram–Schmidt process to obtain an orthonormal basis $\\{\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}\\}$ for the column space of $A$, and thereby form the matrices $Q = \\begin{pmatrix} \\mathbf{q}_{1}  \\mathbf{q}_{2}  \\mathbf{q}_{3} \\end{pmatrix}$ and the upper triangular $R$ such that $A = QR$.\n- Identify the diagonal entries $r_{11}$, $r_{22}$, and $r_{33}$ of $R$ explicitly as functions of $\\epsilon$.\n- Explain, from first principles of projection and finite-precision computation, why near linear dependence (small $\\epsilon$) is numerically problematic for classical Gram–Schmidt, and discuss a principled remedy appropriate for biomedical systems modeling workflows.\n- For the final reported result, provide the analytic expression of the third diagonal entry $r_{33}$ of $R$ as a function of $\\epsilon$.\n\nExpress the final answer as a closed-form analytic expression in $\\epsilon$. No rounding is required. The final answer is unitless and should be written as a single mathematical expression.",
            "solution": "We begin from the definitions in a real inner-product space $\\mathbb{R}^{3}$ with the Euclidean inner product. For vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{3}$, the inner product is $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\mathbf{x}^{\\top}\\mathbf{y}$, and the orthogonal projection of $\\mathbf{v}$ onto a nonzero vector $\\mathbf{u}$ is $\\mathrm{proj}_{\\mathbf{u}}(\\mathbf{v}) = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\langle \\mathbf{u}, \\mathbf{u} \\rangle} \\mathbf{u}$. The Gram–Schmidt process constructs orthonormal vectors $\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\mathbf{q}_{3}$ from the input $\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\mathbf{a}_{3}$ by iteratively subtracting projections and normalizing.\n\nStep $1$: Initialize with $\\mathbf{a}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. Set\n$$\n\\mathbf{u}_{1} = \\mathbf{a}_{1}, \\quad r_{11} = \\|\\mathbf{u}_{1}\\|_{2} = \\sqrt{\\langle \\mathbf{u}_{1}, \\mathbf{u}_{1} \\rangle} = \\sqrt{1^{2} + 0^{2} + 0^{2}} = 1, \\quad \\mathbf{q}_{1} = \\frac{\\mathbf{u}_{1}}{r_{11}} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n\nStep $2$: Orthogonalize $\\mathbf{a}_{2} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix}$ against $\\mathbf{q}_{1}$ and normalize. Compute\n$$\nr_{12} = \\langle \\mathbf{q}_{1}, \\mathbf{a}_{2} \\rangle = \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} = 1,\n$$\n$$\n\\mathbf{u}_{2} = \\mathbf{a}_{2} - r_{12} \\mathbf{q}_{1} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ 0 \\end{pmatrix}.\n$$\nThen\n$$\nr_{22} = \\|\\mathbf{u}_{2}\\|_{2} = \\sqrt{0^{2} + \\epsilon^{2} + 0^{2}} = \\epsilon \\quad (\\text{given } \\epsilon  0),\n$$\nand\n$$\n\\mathbf{q}_{2} = \\frac{\\mathbf{u}_{2}}{r_{22}} = \\frac{1}{\\epsilon} \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\n\nStep $3$: Orthogonalize $\\mathbf{a}_{3} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix}$ against $\\mathbf{q}_{1}$ and $\\mathbf{q}_{2}$ and normalize. First,\n$$\nr_{13} = \\langle \\mathbf{q}_{1}, \\mathbf{a}_{3} \\rangle = \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix} = 1,\n$$\n$$\n\\mathbf{w}_{3} = \\mathbf{a}_{3} - r_{13} \\mathbf{q}_{1} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix}.\n$$\nNext,\n$$\nr_{23} = \\langle \\mathbf{q}_{2}, \\mathbf{w}_{3} \\rangle = \\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix} = \\epsilon,\n$$\n$$\n\\mathbf{u}_{3} = \\mathbf{w}_{3} - r_{23} \\mathbf{q}_{2} = \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ \\epsilon^{2} \\end{pmatrix} - \\epsilon \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\epsilon^{2} \\end{pmatrix}.\n$$\nThen\n$$\nr_{33} = \\|\\mathbf{u}_{3}\\|_{2} = \\sqrt{0^{2} + 0^{2} + \\epsilon^{4}} = \\epsilon^{2},\n$$\nand\n$$\n\\mathbf{q}_{3} = \\frac{\\mathbf{u}_{3}}{r_{33}} = \\frac{1}{\\epsilon^{2}} \\begin{pmatrix} 0 \\\\ 0 \\\\ \\epsilon^{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n\nCollecting, the orthonormal basis is\n$$\nQ = \\begin{pmatrix} \\mathbf{q}_{1}  \\mathbf{q}_{2}  \\mathbf{q}_{3} \\end{pmatrix} = \n\\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix},\n$$\nand the upper triangular factor has entries\n$$\nR = \\begin{pmatrix}\nr_{11}  r_{12}  r_{13} \\\\\n0  r_{22}  r_{23} \\\\\n0  0  r_{33}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  1  1 \\\\\n0  \\epsilon  \\epsilon \\\\\n0  0  \\epsilon^{2}\n\\end{pmatrix}.\n$$\nThus, explicitly, $r_{11} = 1$, $r_{22} = \\epsilon$, and $r_{33} = \\epsilon^{2}$.\n\nNumerical issues for small $\\epsilon$: When $\\epsilon \\ll 1$, the columns of $A$ are nearly colinear, so the problem is ill-conditioned. In classical Gram–Schmidt, the orthogonalization step for $\\mathbf{a}_{2}$ performs a subtraction of nearly equal vectors,\n$$\n\\mathbf{u}_{2} = \\mathbf{a}_{2} - \\langle \\mathbf{q}_{1}, \\mathbf{a}_{2} \\rangle \\mathbf{q}_{1} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ 0 \\end{pmatrix},\n$$\nwhich in finite precision arithmetic suffers from catastrophic cancellation in the first component: rounding errors at the scale of machine precision $u$ relative to the magnitude $1$ can leave a spurious residual of order $u$ in the first component of $\\mathbf{u}_{2}$. Subsequent normalization amplifies the relative effect of this roundoff by a factor of approximately $1/\\epsilon$, since $\\|\\mathbf{u}_{2}\\|_{2} = \\epsilon$. Similarly, the computation of $\\mathbf{u}_{3}$ involves subtracting a projection of size $\\epsilon$ from $\\mathbf{w}_{3}$, leaving a vector of norm $\\epsilon^{2}$. Any accumulated roundoff of order $u$ at the scale of the larger components can dominate the true signal when $\\epsilon^{2} \\lesssim u$, leading to loss of orthogonality among the computed $\\mathbf{q}_{j}$ and unreliable diagonals in $R$. In particular, when $\\epsilon \\lesssim \\sqrt{u}$ in double precision (where $u \\approx 2^{-53}$), the magnitude of $r_{33} = \\epsilon^{2}$ approaches machine precision and becomes difficult to resolve accurately.\n\nA principled remedy is to use a numerically stable orthogonalization, such as modified Gram–Schmidt (which reorthogonalizes sequentially against the current orthonormal vectors and reduces cancellation) or Householder reflections (which construct $Q$ via orthogonal transformations with superior stability). In biomedical systems modeling pipelines, especially for diffusion MRI design matrices, combining Householder-based $\\mathrm{QR}$ factorization with column pivoting can further mitigate the impact of near dependence by revealing numerical rank and controlling conditioning.\n\nFinally, from the derivation above, the third diagonal entry of $R$ as a function of $\\epsilon$ is\n$$\nr_{33}(\\epsilon) = \\epsilon^{2}.\n$$",
            "answer": "$$\\boxed{\\epsilon^{2}}$$"
        },
        {
            "introduction": "Building on the concept of nearly dependent vectors, this practice investigates the direct statistical consequences of collinearity in biomedical modeling. When the columns of a design matrix in a linear model are highly correlated, the variance of the estimated parameters can become unacceptably large, a phenomenon known as variance inflation. This exercise  provides a hands-on demonstration of this problem and explores ridge regression (Tikhonov regularization) as a powerful and widely used method to restore stability to the estimation process.",
            "id": "3899461",
            "problem": "A biomedical chemometric sensing system uses three spectral channels to estimate two metabolite concentrations collected in the vector $x \\in \\mathbb{R}^{2}$ from the measurement model $y = A x + \\varepsilon$, where $y \\in \\mathbb{R}^{3}$, $A \\in \\mathbb{R}^{3 \\times 2}$, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$. Due to overlapping spectral signatures, the columns of $A$ are highly collinear. Let the columns be $a_{1}, a_{2} \\in \\mathbb{R}^{3}$ with $\\|a_{1}\\|_{2} = \\|a_{2}\\|_{2} = 1$ and inner product $a_{1}^{\\top} a_{2} = s = 0.999$, so that the Gram matrix is $G = A^{\\top} A = \\begin{pmatrix} 1  s \\\\ s  1 \\end{pmatrix}$. The ordinary least squares (OLS) estimator $\\hat{x}_{\\text{OLS}}$ has covariance $\\operatorname{Cov}(\\hat{x}_{\\text{OLS}}) = \\sigma^{2} (A^{\\top} A)^{-1}$. The ridge (Tikhonov) estimator is $\\hat{x}_{\\lambda} = (A^{\\top} A + \\lambda I_{2})^{-1} A^{\\top} y$, with covariance $\\operatorname{Cov}(\\hat{x}_{\\lambda})$ determined by the noise model and $A$.\n\nStarting from the linear model and these definitions:\n- Derive the closed-form expression for $\\operatorname{Cov}(\\hat{x}_{\\text{OLS}})$ in terms of $s$ and $\\sigma^{2}$, and compute the variances of the two components of $\\hat{x}_{\\text{OLS}}$ for $\\sigma^{2} = 1$ and $s = 0.999$.\n- Using spectral reasoning on $G$, derive the eigenvalues of $\\operatorname{Cov}(\\hat{x}_{\\lambda})$ as functions of the eigenvalues of $G$ and $\\lambda$. Then, determine the smallest $\\lambda  0$ such that the largest eigenvalue of $\\operatorname{Cov}(\\hat{x}_{\\lambda})$ equals the target $v^{\\star} = 0.1$ for $\\sigma^{2} = 1$ and $s = 0.999$.\n\nRound your final numerical value of $\\lambda$ to four significant figures. Express $\\lambda$ as a dimensionless quantity. The final answer must be a single number.",
            "solution": "The linear Gaussian model $y = A x + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$ yields an ordinary least squares (OLS) estimator $\\hat{x}_{\\text{OLS}} = (A^{\\top} A)^{-1} A^{\\top} y$. The covariance of any linear estimator $L y$ under isotropic noise is $\\operatorname{Cov}(L y) = L \\operatorname{Cov}(y) L^{\\top} = \\sigma^{2} L L^{\\top}$. Therefore, for OLS, $\\operatorname{Cov}(\\hat{x}_{\\text{OLS}}) = \\sigma^{2} (A^{\\top} A)^{-1}$.\n\nFirst, we analyze the effect of collinearity in $A$ through the Gram matrix $G = A^{\\top} A$. Given $\\|a_{1}\\|_{2} = \\|a_{2}\\|_{2} = 1$ and $a_{1}^{\\top} a_{2} = s$, the Gram matrix is\n$$\nG = \\begin{pmatrix} 1  s \\\\ s  1 \\end{pmatrix}.\n$$\nFor a $2 \\times 2$ symmetric matrix of the form\n$$\n\\begin{pmatrix} \\alpha  \\beta \\\\ \\beta  \\alpha \\end{pmatrix},\n$$\nthe inverse (when $\\alpha^{2} - \\beta^{2} \\neq 0$) is\n$$\n\\frac{1}{\\alpha^{2} - \\beta^{2}} \\begin{pmatrix} \\alpha  -\\beta \\\\ -\\beta  \\alpha \\end{pmatrix}.\n$$\nApplying this with $\\alpha = 1$ and $\\beta = s$, we obtain\n$$\nG^{-1} = \\frac{1}{1 - s^{2}} \\begin{pmatrix} 1  -s \\\\ -s  1 \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\operatorname{Cov}(\\hat{x}_{\\text{OLS}}) = \\sigma^{2} G^{-1} = \\frac{\\sigma^{2}}{1 - s^{2}} \\begin{pmatrix} 1  -s \\\\ -s  1 \\end{pmatrix}.\n$$\nThe component variances are the diagonal entries:\n$$\n\\operatorname{Var}(\\hat{x}_{\\text{OLS},1}) = \\operatorname{Var}(\\hat{x}_{\\text{OLS},2}) = \\frac{\\sigma^{2}}{1 - s^{2}}.\n$$\nWith $\\sigma^{2} = 1$ and $s = 0.999$, we have $s^{2} = 0.998001$ and $1 - s^{2} = 0.001999$, so\n$$\n\\operatorname{Var}(\\hat{x}_{\\text{OLS},1}) = \\operatorname{Var}(\\hat{x}_{\\text{OLS},2}) = \\frac{1}{0.001999} \\approx 5.002501250625313 \\times 10^{2}.\n$$\nThis demonstrates that near-collinearity ($s \\approx 1$) inflates the variances dramatically because $1 - s^{2}$ becomes small, making $G$ ill-conditioned.\n\nTo propose a remedy via regularization, consider the ridge estimator\n$$\n\\hat{x}_{\\lambda} = (A^{\\top} A + \\lambda I_{2})^{-1} A^{\\top} y,\n$$\nwhich is a linear function of $y$ with $L_{\\lambda} = (A^{\\top} A + \\lambda I_{2})^{-1} A^{\\top}$. The covariance is\n$$\n\\operatorname{Cov}(\\hat{x}_{\\lambda}) = \\sigma^{2} L_{\\lambda} L_{\\lambda}^{\\top} = \\sigma^{2} (A^{\\top} A + \\lambda I_{2})^{-1} A^{\\top} A (A^{\\top} A + \\lambda I_{2})^{-1}.\n$$\nLet $G = A^{\\top} A$. Since $G$ is real symmetric and positive semidefinite, it admits an eigenvalue decomposition\n$$\nG = U \\begin{pmatrix} \\mu_{1}  0 \\\\ 0  \\mu_{2} \\end{pmatrix} U^{\\top},\n$$\nwith orthonormal $U$ and eigenvalues $\\mu_{1}, \\mu_{2} \\geq 0$. Then\n$$\nA^{\\top} A + \\lambda I_{2} = U \\begin{pmatrix} \\mu_{1} + \\lambda  0 \\\\ 0  \\mu_{2} + \\lambda \\end{pmatrix} U^{\\top},\n$$\nand\n$$\n(A^{\\top} A + \\lambda I_{2})^{-1} A^{\\top} A (A^{\\top} A + \\lambda I_{2})^{-1} = U \\begin{pmatrix} \\dfrac{\\mu_{1}}{(\\mu_{1} + \\lambda)^{2}}  0 \\\\ 0  \\dfrac{\\mu_{2}}{(\\mu_{2} + \\lambda)^{2}} \\end{pmatrix} U^{\\top}.\n$$\nTherefore, the eigenvalues of $\\operatorname{Cov}(\\hat{x}_{\\lambda})$ are\n$$\n\\sigma^{2} \\frac{\\mu_{i}}{(\\mu_{i} + \\lambda)^{2}}, \\quad i = 1,2.\n$$\nFor our $G = \\begin{pmatrix} 1  s \\\\ s  1 \\end{pmatrix}$, the eigenvalues are well known:\n$$\n\\mu_{\\max} = 1 + s, \\quad \\mu_{\\min} = 1 - s.\n$$\nWith $s = 0.999$, we have\n$$\n\\mu_{\\max} = 1.999, \\quad \\mu_{\\min} = 0.001.\n$$\nWe are asked to choose the smallest $\\lambda  0$ such that the largest eigenvalue of $\\operatorname{Cov}(\\hat{x}_{\\lambda})$ equals the target $v^{\\star} = 0.1$ for $\\sigma^{2} = 1$. The largest eigenvalue of $\\operatorname{Cov}(\\hat{x}_{\\lambda})$ is\n$$\n\\max \\left\\{ \\sigma^{2} \\frac{\\mu_{\\min}}{(\\mu_{\\min} + \\lambda)^{2}}, \\ \\sigma^{2} \\frac{\\mu_{\\max}}{(\\mu_{\\max} + \\lambda)^{2}} \\right\\}.\n$$\nThe function $f(\\mu) = \\mu / (\\mu + \\lambda)^{2}$ satisfies $\\dfrac{\\partial f}{\\partial \\mu} = \\dfrac{\\lambda - \\mu}{(\\mu + \\lambda)^{3}}$. Thus, for $\\lambda  \\mu_{\\max}$, $f(\\mu)$ is increasing in $\\mu$ on $[0, \\mu_{\\max}]$, and the largest eigenvalue occurs at $\\mu_{\\max}$. We will construct $\\lambda$ so that $\\lambda  \\mu_{\\max}$ and\n$$\n\\sigma^{2} \\frac{\\mu_{\\max}}{(\\mu_{\\max} + \\lambda)^{2}} = v^{\\star}.\n$$\nSolving for $\\lambda$ gives\n$$\n\\mu_{\\max} + \\lambda = \\sqrt{\\frac{\\sigma^{2} \\mu_{\\max}}{v^{\\star}}} \\quad \\Longrightarrow \\quad \\lambda = \\sqrt{\\frac{\\sigma^{2} \\mu_{\\max}}{v^{\\star}}} - \\mu_{\\max}.\n$$\nSubstituting $\\sigma^{2} = 1$, $\\mu_{\\max} = 1.999$, and $v^{\\star} = 0.1$ yields\n$$\n\\lambda = \\sqrt{\\frac{1 \\times 1.999}{0.1}} - 1.999 = \\sqrt{19.99} - 1.999.\n$$\nCompute $\\sqrt{19.99} \\approx 4.471017955$, so\n$$\n\\lambda \\approx 4.471017955 - 1.999 = 2.472017955.\n$$\nWe verify $\\lambda  \\mu_{\\max}$ since $2.472017955  1.999$, consistent with the monotonicity condition used to select $\\mu_{\\max}$ as the maximizer. Rounding to four significant figures,\n$$\n\\lambda \\approx 2.472.\n$$\nThis ridge parameter reduces the largest eigenvalue of the covariance of the ridge estimator to the target $v^{\\star} = 0.1$, thereby mitigating the variance inflation caused by collinearity in the columns of $A$.",
            "answer": "$$\\boxed{2.472}$$"
        },
        {
            "introduction": "Many modern biomedical applications, such as wearable sensors, generate data in a continuous stream, requiring models that can be updated in real time. This practice moves beyond static analysis to demonstrate how matrix factorizations enable efficient online learning. You will explore how to update a model's QR factorization with a newly arriving data point using Givens rotations, a technique that avoids the costly recalculation of the entire factorization . This is a key algorithmic building block for adaptive filtering and real-time systems in biomedical engineering.",
            "id": "3899441",
            "problem": "In an online linear regression for wearable Near-Infrared Spectroscopy (NIRS) monitoring, a linear model $y = X \\beta + \\varepsilon$ is updated as new time samples arrive. Suppose at time $t$ the design matrix $X \\in \\mathbb{R}^{m \\times n}$ with full column rank is maintained via an orthogonal-triangular (QR) factorization $X = Q R$, where $Q \\in \\mathbb{R}^{m \\times n}$ satisfies $Q^{\\top} Q = I$ and $R \\in \\mathbb{R}^{n \\times n}$ is upper triangular. At the next time point $t+1$, a new feature row $x_{\\mathrm{new}}^{\\top} \\in \\mathbb{R}^{n}$ is acquired from the biomedical device. To enable online updates without recomputing a full factorization, one can append $x_{\\mathrm{new}}^{\\top}$ below $R$ and apply a sequence of plane rotations (Givens rotations) on pairs of rows to annihilate subdiagonal entries, preserving upper triangular structure.\n\nStarting from first principles, use the definitions of orthogonality and the invariance of the least squares objective under left-multiplication by an orthogonal matrix to justify why left-multiplication by appropriately chosen plane rotations on the stacked matrix $\\begin{pmatrix} R \\\\ x_{\\mathrm{new}}^{\\top} \\end{pmatrix}$ yields an updated upper triangular factor $R_{+}$ corresponding to the augmented data. Explicitly construct the necessary rotations for the concrete case\n$$\nR = \\begin{pmatrix} 3  4 \\\\ 0  5 \\end{pmatrix}, \\quad x_{\\mathrm{new}}^{\\top} = \\begin{pmatrix} 4  2 \\end{pmatrix}.\n$$\nCarry out the row-operations symbolically to eliminate subdiagonal entries and identify the updated upper triangular factor $R_{+}$. Your target quantity is the updated lower-right entry $r_{22}^{+}$ of $R_{+}$. Provide the value of $r_{22}^{+}$ as an exact closed-form expression. Do not provide a decimal approximation.",
            "solution": "The problem asks for a justification of the online QR update procedure for linear regression and a specific calculation. We will address the justification first, followed by the explicit computation.\n\n**1. Justification of the QR Update Procedure**\n\nThe ordinary least squares (LS) estimate $\\hat{\\beta}$ for the linear model $y = X \\beta + \\varepsilon$ is the vector that minimizes the squared Euclidean norm of the residual, $\\|y - X\\beta\\|^2$. The solution $\\hat{\\beta}$ satisfies the normal equations:\n$$\nX^{\\top}X \\hat{\\beta} = X^{\\top}y\n$$\nGiven the QR factorization of the design matrix $X = QR$, where $X \\in \\mathbb{R}^{m \\times n}$, $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns ($Q^{\\top}Q = I_n$), and $R \\in \\mathbb{R}^{n \\times n}$ is upper triangular and invertible (since $X$ has full column rank), we can substitute this into the normal equations:\n$$\n(QR)^{\\top}(QR) \\hat{\\beta} = (QR)^{\\top}y\n$$\n$$\nR^{\\top}Q^{\\top}Q R \\hat{\\beta} = R^{\\top}Q^{\\top}y\n$$\nUsing the property $Q^{\\top}Q = I_n$, this simplifies to:\n$$\nR^{\\top}R \\hat{\\beta} = R^{\\top}Q^{\\top}y\n$$\nSince $R$ is invertible, $R^{\\top}$ is also invertible. We can left-multiply by $(R^{\\top})^{-1}$ to get:\n$$\nR \\hat{\\beta} = Q^{\\top}y\n$$\nThis upper triangular system can be solved efficiently for $\\hat{\\beta}$ via back substitution.\n\nNow, consider the state at time $t+1$. A new data row $x_{\\mathrm{new}}^{\\top} \\in \\mathbb{R}^n$ and a corresponding observation $y_{\\mathrm{new}}$ are acquired. The augmented design matrix and observation vector are:\n$$\nX_{+} = \\begin{pmatrix} X \\\\ x_{\\mathrm{new}}^{\\top} \\end{pmatrix} \\in \\mathbb{R}^{(m+1) \\times n}, \\quad y_{+} = \\begin{pmatrix} y \\\\ y_{\\mathrm{new}} \\end{pmatrix} \\in \\mathbb{R}^{m+1}\n$$\nThe new LS problem is to find $\\hat{\\beta}_{+}$ that minimizes $\\|y_{+} - X_{+}\\beta\\|^2$. The corresponding normal equations are $X_{+}^{\\top}X_{+} \\hat{\\beta}_{+} = X_{+}^{\\top}y_{+}$. The core of the update is finding the new QR factorization $X_{+} = Q_{+}R_{+}$, from which the updated solution can be found by solving $R_{+}\\hat{\\beta}_{+} = Q_{+}^{\\top}y_{+}$.\n\nLet's focus on the matrix product $X_{+}^{\\top}X_{+}$, often called the information matrix.\n$$\nX_{+}^{\\top}X_{+} = \\begin{pmatrix} X^{\\top}  x_{\\mathrm{new}} \\end{pmatrix} \\begin{pmatrix} X \\\\ x_{\\mathrm{new}}^{\\top} \\end{pmatrix} = X^{\\top}X + x_{\\mathrm{new}}x_{\\mathrm{new}}^{\\top}\n$$\nSubstituting $X^{\\top}X = R^{\\top}R$, we have:\n$$\nX_{+}^{\\top}X_{+} = R^{\\top}R + x_{\\mathrm{new}}x_{\\mathrm{new}}^{\\top}\n$$\nThe proposed update procedure begins by forming the $(n+1) \\times n$ matrix $M = \\begin{pmatrix} R \\\\ x_{\\mathrm{new}}^{\\top} \\end{pmatrix}$. Let us examine the product $M^{\\top}M$:\n$$\nM^{\\top}M = \\begin{pmatrix} R^{\\top}  x_{\\mathrm{new}} \\end{pmatrix} \\begin{pmatrix} R \\\\ x_{\\mathrm{new}}^{\\top} \\end{pmatrix} = R^{\\top}R + x_{\\mathrm{new}}x_{\\mathrm{new}}^{\\top}\n$$\nWe see that $X_{+}^{\\top}X_{+} = M^{\\top}M$. This identity is central to the justification.\n\nThe procedure then applies a sequence of orthogonal transformations (Givens rotations), let's denote their product by a single orthogonal matrix $G^{\\top} \\in \\mathbb{R}^{(n+1) \\times (n+1)}$, to the left of $M$. The purpose of these rotations is to zero out all subdiagonal elements, transforming $M$ into an upper trapezoidal form:\n$$\nG^{\\top}M = \\begin{pmatrix} R_{+} \\\\ 0 \\end{pmatrix}\n$$\nwhere $R_{+} \\in \\mathbb{R}^{n \\times n}$ is upper triangular and $0$ is a $1 \\times n$ zero row.\nBecause $G^{\\top}$ is an orthogonal matrix, we have $G G^{\\top} = I_{n+1}$. Let's consider the product of the transpose of the transformed matrix with itself:\n$$\n(G^{\\top}M)^{\\top}(G^{\\top}M) = M^{\\top}G G^{\\top}M = M^{\\top}I_{n+1}M = M^{\\top}M\n$$\nFurthermore, using the resulting structure:\n$$\n(G^{\\top}M)^{\\top}(G^{\\top}M) = \\begin{pmatrix} R_{+} \\\\ 0 \\end{pmatrix}^{\\top} \\begin{pmatrix} R_{+} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} R_{+}^{\\top}  0^{\\top} \\end{pmatrix} \\begin{pmatrix} R_{+} \\\\ 0 \\end{pmatrix} = R_{+}^{\\top}R_{+}\n$$\nBy equating these results, we establish the crucial link:\n$$\nX_{+}^{\\top}X_{+} = M^{\\top}M = R_{+}^{\\top}R_{+}\n$$\nThis shows that $R_{+}$ is the correct Cholesky factor of the new information matrix $X_{+}^{\\top}X_{+}$, and is therefore the correct updated upper triangular factor in the QR factorization of $X_{+}$. The justification hinges on the fact that left-multiplication by an orthogonal matrix preserves the Gramian matrix $M^{\\top}M$.\n\n**2. Calculation for the Concrete Case**\n\nWe are given:\n$$\nR = \\begin{pmatrix} 3  4 \\\\ 0  5 \\end{pmatrix}, \\quad x_{\\mathrm{new}}^{\\top} = \\begin{pmatrix} 4  2 \\end{pmatrix}\n$$\nFirst, we form the stacked matrix $M$:\n$$\nM = \\begin{pmatrix} R \\\\ x_{\\mathrm{new}}^{\\top} \\end{pmatrix} = \\begin{pmatrix} 3  4 \\\\ 0  5 \\\\ 4  2 \\end{pmatrix}\n$$\nOur goal is to introduce zeros below the main diagonal. We proceed column by column.\n\n**Step 2a: Annihilate the (3,1) element.**\nWe use a Givens rotation to zero out the element $M_{31} = 4$ using the pivot element $M_{11} = 3$. The rotation acts on row 1 and row 3. Let the elements be $a = M_{11} = 3$ and $b = M_{31} = 4$.\nThe rotation parameters $c$ and $s$ are calculated as:\n$r = \\sqrt{a^2 + b^2} = \\sqrt{3^2 + 4^2} = \\sqrt{9+16} = \\sqrt{25} = 5$.\n$c = a/r = 3/5$.\n$s = b/r = 4/5$.\nThe transformation on rows 1 ($R_1$) and 3 ($R_3$) to get the new rows $R_1'$ and $R_3'$ is:\n$$\n\\begin{pmatrix} R_1' \\\\ R_3' \\end{pmatrix} = \\begin{pmatrix} c  s \\\\ -s  c \\end{pmatrix} \\begin{pmatrix} R_1 \\\\ R_3 \\end{pmatrix} = \\begin{pmatrix} 3/5  4/5 \\\\ -4/5  3/5 \\end{pmatrix} \\begin{pmatrix} \\begin{pmatrix} 3  4 \\end{pmatrix} \\\\ \\begin{pmatrix} 4  2 \\end{pmatrix} \\end{pmatrix}\n$$\n$R_1' = \\frac{3}{5}\\begin{pmatrix} 3  4 \\end{pmatrix} + \\frac{4}{5}\\begin{pmatrix} 4  2 \\end{pmatrix} = \\begin{pmatrix} 9/5 + 16/5  12/5 + 8/5 \\end{pmatrix} = \\begin{pmatrix} 25/5  20/5 \\end{pmatrix} = \\begin{pmatrix} 5  4 \\end{pmatrix}$.\n$R_3' = -\\frac{4}{5}\\begin{pmatrix} 3  4 \\end{pmatrix} + \\frac{3}{5}\\begin{pmatrix} 4  2 \\end{pmatrix} = \\begin{pmatrix} -12/5 + 12/5  -16/5 + 6/5 \\end{pmatrix} = \\begin{pmatrix} 0  -10/5 \\end{pmatrix} = \\begin{pmatrix} 0  -2 \\end{pmatrix}$.\nRow 2 remains unchanged. The matrix becomes:\n$$\nM' = \\begin{pmatrix} 5  4 \\\\ 0  5 \\\\ 0  -2 \\end{pmatrix}\n$$\n\n**Step 2b: Annihilate the (3,2) element.**\nThe first column is correct. Now we zero out the element $M'_{32} = -2$ using the pivot element $M'_{22} = 5$. The rotation acts on row 2 and row 3 of $M'$. Let the elements be $a = M'_{22} = 5$ and $b = M'_{32} = -2$.\nThe rotation parameters $c$ and $s$ are calculated as:\n$r = \\sqrt{a^2 + b^2} = \\sqrt{5^2 + (-2)^2} = \\sqrt{25+4} = \\sqrt{29}$.\n$c = a/r = 5/\\sqrt{29}$.\n$s = b/r = -2/\\sqrt{29}$.\nThe transformation on rows 2 ($R_2'$) and 3 ($R_3'$) to get the new rows $R_2''$ and $R_3''$ is:\n$$\n\\begin{pmatrix} R_2'' \\\\ R_3'' \\end{pmatrix} = \\begin{pmatrix} c  s \\\\ -s  c \\end{pmatrix} \\begin{pmatrix} R_2' \\\\ R_3' \\end{pmatrix} = \\begin{pmatrix} 5/\\sqrt{29}  -2/\\sqrt{29} \\\\ 2/\\sqrt{29}  5/\\sqrt{29} \\end{pmatrix} \\begin{pmatrix} \\begin{pmatrix} 0  5 \\end{pmatrix} \\\\ \\begin{pmatrix} 0  -2 \\end{pmatrix} \\end{pmatrix}\n$$\n$R_2'' = \\frac{5}{\\sqrt{29}}\\begin{pmatrix} 0  5 \\end{pmatrix} - \\frac{2}{\\sqrt{29}}\\begin{pmatrix} 0  -2 \\end{pmatrix} = \\begin{pmatrix} 0  25/\\sqrt{29} + 4/\\sqrt{29} \\end{pmatrix} = \\begin{pmatrix} 0  29/\\sqrt{29} \\end{pmatrix} = \\begin{pmatrix} 0  \\sqrt{29} \\end{pmatrix}$.\n$R_3'' = \\frac{2}{\\sqrt{29}}\\begin{pmatrix} 0  5 \\end{pmatrix} + \\frac{5}{\\sqrt{29}}\\begin{pmatrix} 0  -2 \\end{pmatrix} = \\begin{pmatrix} 0  10/\\sqrt{29} - 10/\\sqrt{29} \\end{pmatrix} = \\begin{pmatrix} 0  0 \\end{pmatrix}$.\nRow 1 remains unchanged. The final matrix is:\n$$\nM'' = \\begin{pmatrix} 5  4 \\\\ 0  \\sqrt{29} \\\\ 0  0 \\end{pmatrix}\n$$\nThis matrix has the form $\\begin{pmatrix} R_{+} \\\\ 0 \\end{pmatrix}$, where $R_{+} = \\begin{pmatrix} 5  4 \\\\ 0  \\sqrt{29} \\end{pmatrix}$.\nThe updated lower-right entry of $R_{+}$ is $r_{22}^{+}$.\n$$\nr_{22}^{+} = \\sqrt{29}\n$$\nThis is the required exact closed-form expression.",
            "answer": "$$\\boxed{\\sqrt{29}}$$"
        }
    ]
}