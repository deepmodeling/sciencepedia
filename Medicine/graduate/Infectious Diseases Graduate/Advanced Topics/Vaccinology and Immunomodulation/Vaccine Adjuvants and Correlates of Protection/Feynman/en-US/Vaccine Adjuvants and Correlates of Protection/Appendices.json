{
    "hands_on_practices": [
        {
            "introduction": "A central challenge in developing new vaccine adjuvants is balancing the desired enhancement of immunogenicity with the inevitable increase in local and systemic reactogenicity. This trade-off can be formally modeled to guide dose selection in early-phase clinical trials. This practice provides a quantitative framework for this decision-making process, using standard pharmacological dose-response models to define a rational dosing window that achieves a target level of efficacy while respecting safety constraints. ",
            "id": "4703628",
            "problem": "A Toll-Like Receptor 4 (TLR4) agonist is being evaluated as a vaccine adjuvant in a first-in-human study. The primary immunogenicity correlate of protection is defined by a neutralizing antibody response that reaches or exceeds a prespecified threshold, and the primary safety constraint is to maintain reactogenicity below a tolerability threshold. Consider the following foundational facts and definitions that are widely observed for adjuvant dose–response in humans and relevant animal models:\n\n- The adjuvant dose, denoted by $D$, induces an immunogenicity response $E(D)$ that is monotonic and sigmoidal, characterized by a half-maximal effective dose $EC_{50}$ and a slope parameter $h_E$, with saturation at $E_{\\max}$. This shape reflects cooperative activation and downstream signaling saturation in innate immune pathways.\n- The probability or severity of adverse events (reactogenicity), denoted by $T(D)$, is also monotonic and sigmoidal, characterized by a half-maximal toxic dose $TD_{50}$ and a slope parameter $h_T$, with saturation at $T_{\\max}$. This shape reflects the thresholded activation of inflammatory cascades.\n- It is empirically supported that for optimized TLR4 agonists used in vaccines, $EC_{50} \\ll TD_{50}$ and $h_T$ is often steeper than $h_E$, indicating a wider separation between efficacy onset and toxicity onset and a sharper rise in toxicity near its inflection point than in efficacy near its own inflection point.\n\nIn a candidate formulation of a monophosphoryl lipid A–like TLR4 agonist:\n- The immunogenicity half-max is $EC_{50} = 5\\,\\mu\\mathrm{g}$ with slope $h_E = 2$, and the response saturates at $E_{\\max}$.\n- The adverse events half-max is $TD_{50} = 20\\,\\mu\\mathrm{g}$ with slope $h_T = 3$, and the response saturates at $T_{\\max}$.\n- The correlate of protection threshold is set at $E^\\star = 0.7\\,E_{\\max}$, and the tolerability threshold is set at $T^\\dagger = 0.2\\,T_{\\max}$.\n\nAssume standard Hill-type sigmoidal behavior for both $E(D)$ and $T(D)$, and that $E_{\\max}$ and $T_{\\max}$ are normalized to $1$ for interpretability. Which option most accurately predicts how dose escalation affects immunogenicity and adverse events and proposes a rational dosing window consistent with the efficacy-to-toxicity trade-off described?\n\nA. A rational window is approximately $D \\in [8, 12.6]\\,\\mu\\mathrm{g}$, because increasing $D$ to around $8\\,\\mu\\mathrm{g}$ reaches $E(D) \\ge E^\\star$, and keeping $D$ below about $12.6\\,\\mu\\mathrm{g}$ maintains $T(D) \\le T^\\dagger$. Escalation beyond this window yields diminishing gains in $E(D)$ near the plateau while $T(D)$ rises steeply due to the larger $h_T$ and proximity to $TD_{50}$.\n\nB. Doses at or above $D = 20\\,\\mu\\mathrm{g}$ are optimal; toxicity becomes benign as $D$ increases past $TD_{50}$ due to saturation, and immunogenicity continues to increase linearly with $D$, ensuring robust protection.\n\nC. The safest and most immunogenic approach is to remain at $D \\le 5\\,\\mu\\mathrm{g}$ so that innate activation is minimal; adaptive memory will compensate to meet $E^\\star$ without increasing adverse events.\n\nD. The optimal window is $D \\in [15, 25]\\,\\mu\\mathrm{g}$, aligning dose with regions where both curves are near their half-maxima to balance efficacy and toxicity; this equal trade-off maximizes overall utility despite exceeding tolerability thresholds.",
            "solution": "The user wants me to solve a problem about vaccine adjuvant dose-response.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- Adjuvant dose: $D$\n- Immunogenicity response: $E(D)$, monotonic and sigmoidal\n- Immunogenicity half-maximal effective dose: $EC_{50}$\n- Immunogenicity slope parameter: $h_E$\n- Immunogenicity saturation value: $E_{\\max}$\n- Reactogenicity (adverse events) response: $T(D)$, monotonic and sigmoidal\n- Reactogenicity half-maximal toxic dose: $TD_{50}$\n- Reactogenicity slope parameter: $h_T$\n- Reactogenicity saturation value: $T_{\\max}$\n- Empirical facts: $EC_{50} \\ll TD_{50}$ and $h_T > h_E$\n- Specific immunogenicity parameters: $EC_{50} = 5\\,\\mu\\mathrm{g}$, $h_E = 2$\n- Specific reactogenicity parameters: $TD_{50} = 20\\,\\mu\\mathrm{g}$, $h_T = 3$\n- Correlate of protection threshold: $E^\\star = 0.7\\,E_{\\max}$\n- Tolerability threshold: $T^\\dagger = 0.2\\,T_{\\max}$\n- Assumption 1: Standard Hill-type sigmoidal behavior for $E(D)$ and $T(D)$.\n- Assumption 2: $E_{\\max}$ and $T_{\\max}$ are normalized to $1$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem is based on the Hill equation, a standard and widely used pharmacological model for dose-response relationships. The concepts of $EC_{50}$, $TD_{50}$, Hill coefficients ($h_E$, $h_T$), and response saturation are fundamental to pharmacology and immunology. The use of a Toll-Like Receptor 4 (TLR4) agonist, such as a monophosphoryl lipid A (MPLA) derivative, as a vaccine adjuvant is a well-established and clinically approved methodology. The premise that a useful adjuvant has a therapeutic window where efficacy is achieved at doses lower than those causing significant toxicity ($EC_{50} \\ll TD_{50}$) is a core principle of drug and vaccine development. The problem is firmly rooted in established scientific principles. **(Pass)**\n- **Well-Posed:** The problem provides specific functional forms (Hill equation), all necessary parameters ($EC_{50}$, $TD_{50}$, $h_E$, $h_T$), and clear, quantitative constraints ($E(D) \\ge E^\\star$, $T(D) \\le T^\\dagger$). The task is to determine the dose range $D$ that satisfies these two inequalities. This is a mathematically well-defined problem that admits a unique solution for the dose window. **(Pass)**\n- **Objective:** The problem statement is expressed in precise, technical language common to pharmacology and immunology. It is free of ambiguity, subjective claims, or opinion-based statements. All thresholds and parameters are numerically defined. **(Pass)**\n\n**Step 3: Verdict and Action**\n\nThe problem is scientifically sound, well-posed, objective, and contains all necessary information for a unique solution. It is therefore **valid**. I will proceed with the solution derivation.\n\n### Solution Derivation\n\nThe problem states that the immunogenicity response $E(D)$ and the toxicity response $T(D)$ follow standard Hill-type sigmoidal behavior. With the normalization $E_{\\max}=1$ and $T_{\\max}=1$, and assuming a baseline response of $0$, the Hill equations are:\n\n$$ E(D) = \\frac{D^{h_E}}{EC_{50}^{h_E} + D^{h_E}} $$\n$$ T(D) = \\frac{D^{h_T}}{TD_{50}^{h_T} + D^{h_T}} $$\n\nSubstituting the given parameters:\n- For immunogenicity ($EC_{50} = 5\\,\\mu\\mathrm{g}$, $h_E = 2$):\n$$ E(D) = \\frac{D^2}{5^2 + D^2} = \\frac{D^2}{25 + D^2} $$\n- For toxicity ($TD_{50} = 20\\,\\mu\\mathrm{g}$, $h_T = 3$):\n$$ T(D) = \\frac{D^3}{20^3 + D^3} = \\frac{D^3}{8000 + D^3} $$\nIn these equations, $D$ is in units of $\\mu\\mathrm{g}$.\n\nThe rational dosing window must satisfy two conditions simultaneously:\n1.  The immunogenicity must meet or exceed the correlate of protection threshold: $E(D) \\ge E^\\star$.\n2.  The toxicity must not exceed the tolerability threshold: $T(D) \\le T^\\dagger$.\n\nThe given thresholds are $E^\\star = 0.7\\,E_{\\max} = 0.7$ and $T^\\dagger = 0.2\\,T_{\\max} = 0.2$.\n\n**1. Minimum Dose for Efficacy ($D_{min}$)**\nWe solve for the dose $D$ at which $E(D) \\ge 0.7$:\n$$ \\frac{D^2}{25 + D^2} \\ge 0.7 $$\n$$ D^2 \\ge 0.7 (25 + D^2) $$\n$$ D^2 \\ge 17.5 + 0.7 D^2 $$\n$$ 0.3 D^2 \\ge 17.5 $$\n$$ D^2 \\ge \\frac{17.5}{0.3} = \\frac{175}{3} \\approx 58.333 $$\n$$ D \\ge \\sqrt{\\frac{175}{3}} \\approx 7.638\\,\\mu\\mathrm{g} $$\nThus, a dose of at least $D_{min} \\approx 7.64\\,\\mu\\mathrm{g}$ is required to achieve the desired immunogenicity.\n\n**2. Maximum Dose for Safety ($D_{max}$)**\nWe solve for the dose $D$ at which $T(D) \\le 0.2$:\n$$ \\frac{D^3}{8000 + D^3} \\le 0.2 $$\n$$ D^3 \\le 0.2 (8000 + D^3) $$\n$$ D^3 \\le 1600 + 0.2 D^3 $$\n$$ 0.8 D^3 \\le 1600 $$\n$$ D^3 \\le \\frac{1600}{0.8} = 2000 $$\n$$ D \\le \\sqrt[3]{2000} \\approx 12.599\\,\\mu\\mathrm{g} $$\nThus, the dose must be no more than $D_{max} \\approx 12.60\\,\\mu\\mathrm{g}$ to remain within the tolerability limit.\n\n**3. Rational Dosing Window**\nCombining both conditions, the rational dosing window is the interval $[D_{min}, D_{max}]$, which is approximately $D \\in [7.64, 12.60]\\,\\mu\\mathrm{g}$.\n\n### Option-by-Option Analysis\n\n**A. A rational window is approximately $D \\in [8, 12.6]\\,\\mu\\mathrm{g}$, because increasing $D$ to around $8\\,\\mu\\mathrm{g}$ reaches $E(D) \\ge E^\\star$, and keeping $D$ below about $12.6\\,\\mu\\mathrm{g}$ maintains $T(D) \\le T^\\dagger$. Escalation beyond this window yields diminishing gains in $E(D)$ near the plateau while $T(D)$ rises steeply due to the larger $h_T$ and proximity to $TD_{50}$.**\n\nThe proposed window $D \\in [8, 12.6]\\,\\mu\\mathrm{g}$ is a reasonable practical range that fits entirely within our calculated window of $[7.64, 12.60]\\,\\mu\\mathrm{g}$.\n- At $D = 8\\,\\mu\\mathrm{g}$, $E(8) = \\frac{8^2}{25+8^2} = \\frac{64}{89} \\approx 0.719$, which is $\\ge 0.7$. This satisfies the efficacy criterion.\n- At $D = 12.6\\,\\mu\\mathrm{g}$, $T(12.6) = \\frac{12.6^3}{8000+12.6^3} \\approx \\frac{2000}{8000+2000} = 0.2$. This satisfies the safety criterion.\nThe qualitative description is also correct. The immunogenicity curve has its inflection point at $EC_{50}=5\\,\\mu\\mathrm{g}$, so doses in the $[8, 12.6]\\,\\mu\\mathrm{g}$ range are on the flattening part of the curve, yielding diminishing returns. The toxicity curve rises with a steeper slope ($h_T=3$ vs $h_E=2$) and is approaching its inflection point at $TD_{50}=20\\,\\mu\\mathrm{g}$, meaning it is in a phase of increasingly rapid ascent. The reasoning is sound and the numbers are accurate.\n**Verdict: Correct**\n\n**B. Doses at or above $D = 20\\,\\mu\\mathrm{g}$ are optimal; toxicity becomes benign as $D$ increases past $TD_{50}$ due to saturation, and immunogenicity continues to increase linearly with $D$, ensuring robust protection.**\n\nAt $D=20\\,\\mu\\mathrm{g}$, this is the $TD_{50}$. The toxicity is $T(20) = \\frac{20^3}{20^3+20^3} = 0.5$. This is $2.5$ times the tolerability threshold of $T^\\dagger=0.2$, so any dose in this range is unacceptable. The claim that toxicity becomes \"benign\" past $TD_{50}$ is false; it continues to increase toward its maximum level. The claim that immunogenicity increases linearly is also false; the response is sigmoidal and is already near saturation ($E(20) = \\frac{20^2}{25+20^2} = \\frac{400}{425} \\approx 0.94$).\n**Verdict: Incorrect**\n\n**C. The safest and most immunogenic approach is to remain at $D \\le 5\\,\\mu\\mathrm{g}$ so that innate activation is minimal; adaptive memory will compensate to meet $E^\\star$ without increasing adverse events.**\n\nAt $D=5\\,\\mu\\mathrm{g}$, this is the $EC_{50}$. The immunogenicity is $E(5) = \\frac{5^2}{25+5^2} = 0.5$. This is below the required threshold of $E^\\star=0.7$. Therefore, this dose range is not sufficiently immunogenic. The claim that this is the \"most immunogenic\" approach is false, as higher doses yield higher immunogenicity. The statement about \"adaptive memory will compensate\" is a biological speculation not supported by the quantitative model provided in the problem.\n**Verdict: Incorrect**\n\n**D. The optimal window is $D \\in [15, 25]\\,\\mu\\mathrm{g}$, aligning dose with regions where both curves are near their half-maxima to balance efficacy and toxicity; this equal trade-off maximizes overall utility despite exceeding tolerability thresholds.**\n\nLet's test the lower bound of this window, $D = 15\\,\\mu\\mathrm{g}$. The toxicity is $T(15) = \\frac{15^3}{8000+15^3} = \\frac{3375}{11375} \\approx 0.297$. This value is significantly above the tolerability threshold of $T^\\dagger=0.2$. Proposing a dosing window that explicitly violates the \"primary safety constraint\" is unacceptable and not rational. The premise of maximizing an undefined \"utility\" by violating a hard constraint is flawed.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once a vaccine is developed, establishing a reliable immune correlate of protection is crucial for licensure and deployment. However, the laboratory assays used to measure these correlates, such as antibody titers, are never perfectly precise and are subject to measurement error. This practice delves into the statistical impact of such assay variability, demonstrating how it can lead to an underestimation of a vaccine's true protective effect and introducing a formal method to correct for this bias. ",
            "id": "4703677",
            "problem": "A randomized vaccine efficacy study compares two formulations that are identical except for the adjuvant. Investigators posit that a mechanistic immune correlate of protection, the logarithm of neutralizing antibody titer measured at day $28$, denoted by $X$, is the primary driver of reduced infection risk across recipients regardless of adjuvant. Infection by month $6$ is recorded as a binary outcome $Y \\in \\{0,1\\}$. The protection model is specified as a logistic regression for the true, latent biomarker,\n$$\\Pr(Y=1 \\mid X) = \\operatorname{logit}^{-1}(\\alpha + \\beta X),$$\nwhere $\\beta < 0$ encodes the slope of protection per unit increase in $X$. However, the measured assay value $W$ for each participant is contaminated by classical additive measurement error due to assay variability, such that\n$$W = X + \\epsilon,$$\nwith $\\epsilon$ independent of $X$ and $Y$, mean $0$, and variance $\\sigma_{\\epsilon}^{2}$. The reliability coefficient of the assay is defined by\n$$\\rho = \\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{\\epsilon}^{2}},$$\nwhere $\\sigma_{X}^{2}$ is the between-person variance in $X$.\n\nTo empirically quantify assay reliability, a validation subset of $n_{r} = 50$ participants had $2$ technical replicates of the assay performed on the same sample at day $28$. On the log scale used for analysis, the sample between-person variance was $s_{B}^{2} = 0.50$ and the within-person (repeatability) variance was $s_{W}^{2} = 0.20$. In the full cohort, a naive logistic regression of $Y$ on $W$ (ignoring measurement error) yielded an estimated slope $\\hat{\\beta}^{*} = -0.8$ per $1$-unit increase in $W$. The infection attack rate in the study was approximately $5\\%$.\n\nStarting from core definitions of classical measurement error and the logistic regression model for risk, reason about how measurement error in the correlate attenuates the estimated slope of protection and how reliability-based corrections can be applied. Under appropriate approximations for rare events and small measurement error, use the validation subset data to compute a reliability-corrected slope and its implied odds ratio per $1$-unit increase in $X$.\n\nSelect all statements that are correct.\n\nA. Under the classical additive error model with nondifferential error, the naive logistic slope $\\hat{\\beta}^{*}$ fit on $W$ is approximately attenuated toward $0$ by the factor $\\rho$, that is, $\\hat{\\beta}^{*} \\approx \\rho \\hat{\\beta}$, when the event is rare and effect sizes are modest. A reliability-based correction is $\\hat{\\beta} \\approx \\hat{\\beta}^{*}/\\hat{\\rho}$.\n\nB. With $2$ technical replicates per person, a natural estimator of reliability is $\\hat{\\rho} = s_{B}^{2}/(s_{B}^{2} + s_{W}^{2})$. Using $s_{B}^{2} = 0.50$, $s_{W}^{2} = 0.20$, and $\\hat{\\beta}^{*} = -0.8$, the corrected slope is $\\hat{\\beta} \\approx -1.12$, yielding an odds ratio per $1$-unit increase of $\\exp(-1.12) \\approx 0.326$.\n\nC. Reliability-based division of the naive logistic slope by $\\hat{\\rho}$ exactly removes attenuation bias whenever measurement error is nondifferential, regardless of event prevalence or the magnitude of $\\beta$.\n\nD. If assay error is differential by outcome status (for example, $\\sigma_{\\epsilon}^{2}$ differs between infected and uninfected), then reliability-based correction using replicate data from the cohort remains unbiased for $\\beta$.\n\nE. If $\\hat{\\beta} = \\hat{\\beta}^{*}/\\hat{\\rho}$, then by the delta method an approximate standard error is\n$$\\operatorname{SE}(\\hat{\\beta}) \\approx \\sqrt{\\frac{\\operatorname{SE}(\\hat{\\beta}^{*})^{2}}{\\hat{\\rho}^{2}} + \\frac{(\\hat{\\beta}^{*})^{2}\\operatorname{SE}(\\hat{\\rho})^{2}}{\\hat{\\rho}^{4}}},$$\nwhich demonstrates that uncertainty in $\\hat{\\rho}$ inflates the variance of the corrected slope.",
            "solution": "The problem statement describes a standard scenario in biostatistics and epidemiology, specifically the analysis of correlates of protection in vaccine trials where the purported correlate is subject to measurement error. The problem is scientifically grounded, well-posed, and all provided data and definitions are consistent and sufficient for a rigorous analysis. The models and concepts—logistic regression, classical additive measurement error, the reliability coefficient, and variance component analysis from replicate data—are all standard in the field. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe core task is to analyze the effect of measurement error in the predictor variable $X$ of a logistic regression model. The true model relates the log-odds of infection $Y=1$ to a latent biomarker $X$:\n$$ \\text{logit}\\{\\Pr(Y=1 \\mid X)\\} = \\alpha + \\beta X $$\nHowever, we only observe a noisy version of $X$, denoted by $W$, where $W = X + \\epsilon$. The error $\\epsilon$ has mean $0$ and variance $\\sigma_{\\epsilon}^{2}$, and is independent of $X$ and $Y$ (nondifferential error). When a naive logistic regression of $Y$ on $W$ is performed, the resulting slope estimate, $\\hat{\\beta}^{*}$, is a biased estimate of the true slope $\\beta$.\n\nFor a linear model, the estimated slope is attenuated by the reliability coefficient $\\rho = \\sigma_{X}^{2} / (\\sigma_{X}^{2} + \\sigma_{\\epsilon}^{2})$. For a nonlinear model like logistic regression, the relationship is more complex. However, a widely used approximation, known as regression calibration, suggests a similar attenuation. This approximation posits that the slope of a logistic regression of $Y$ on $W$ is approximately equal to the true slope $\\beta$ multiplied by the reliability $\\rho$. That is, $\\beta^{*} \\approx \\beta\\rho$. This approximation is particularly accurate when the outcome is rare (as stated, the attack rate is $\\approx 5\\%$) and the true effect size $|\\beta|$ is not excessively large.\n\nFrom this approximation, one can derive a corrected estimate for $\\beta$ by inverting the relationship: $\\hat{\\beta} \\approx \\hat{\\beta}^{*} / \\hat{\\rho}$. The reliability $\\rho$ itself must be estimated, typically from a validation substudy with replicate measurements, as provided in the problem.\n\nWith these principles established, we evaluate each statement.\n\n**A. Under the classical additive error model with nondifferential error, the naive logistic slope $\\hat{\\beta}^{*}$ fit on $W$ is approximately attenuated toward $0$ by the factor $\\rho$, that is, $\\hat{\\beta}^{*} \\approx \\rho \\hat{\\beta}$, when the event is rare and effect sizes are modest. A reliability-based correction is $\\hat{\\beta} \\approx \\hat{\\beta}^{*}/\\hat{\\rho}$.**\n\nThis statement accurately summarizes the theoretical background for the regression calibration method. The relationship $\\hat{\\beta}^{*} \\approx \\rho \\hat{\\beta}$ is the key approximation for attenuation in logistic regression under the specified model ($W = X + \\epsilon$, with nondifferential error). It explicitly mentions the conditions under which this approximation is known to be most accurate (rare event, modest effect sizes). The formula for the corrected slope, $\\hat{\\beta} \\approx \\hat{\\beta}^{*}/\\hat{\\rho}$, follows directly by algebraically rearranging the approximation. This is the standard reliability-based correction method.\n\n**Verdict: Correct.**\n\n**B. With $2$ technical replicates per person, a natural estimator of reliability is $\\hat{\\rho} = s_{B}^{2}/(s_{B}^{2} + s_{W}^{2})$. Using $s_{B}^{2} = 0.50$, $s_{W}^{2} = 0.20$, and $\\hat{\\beta}^{*} = -0.8$, the corrected slope is $\\hat{\\beta} \\approx -1.12$, yielding an odds ratio per $1$-unit increase of $\\exp(-1.12) \\approx 0.326$.**\n\nFirst, we must estimate the reliability $\\rho$. The reliability coefficient is the ratio of true between-person variance to total observed variance, $\\rho = \\sigma_{X}^{2} / \\sigma_{W}^{2} = \\sigma_{X}^{2} / (\\sigma_{X}^{2} + \\sigma_{\\epsilon}^{2})$. In a replicate study, the between-person variance component estimates $\\sigma_{X}^{2}$, and the within-person variance component estimates the measurement error variance $\\sigma_{\\epsilon}^{2}$. The problem provides these estimates as $s_{B}^{2} = 0.50$ (estimate for $\\sigma_{X}^{2}$) and $s_{W}^{2} = 0.20$ (estimate for $\\sigma_{\\epsilon}^{2}$). Thus, a natural estimator for the reliability is $\\hat{\\rho} = s_{B}^{2} / (s_{B}^{2} + s_{W}^{2})$.\n\nUsing the given values:\n$$ \\hat{\\rho} = \\frac{0.50}{0.50 + 0.20} = \\frac{0.50}{0.70} = \\frac{5}{7} $$\nNext, we compute the corrected slope estimate using the formula from statement A and the given naive slope $\\hat{\\beta}^{*} = -0.8$:\n$$ \\hat{\\beta} \\approx \\frac{\\hat{\\beta}^{*}}{\\hat{\\rho}} = \\frac{-0.8}{5/7} = -0.8 \\times \\frac{7}{5} = - \\frac{5.6}{5} = -1.12 $$\nThe calculation of the corrected slope is correct. Finally, we compute the odds ratio (OR) per $1$-unit increase in the true correlate $X$. The OR is given by $\\exp(\\beta)$. Using our corrected estimate $\\hat{\\beta}$:\n$$ \\text{OR} = \\exp(\\hat{\\beta}) \\approx \\exp(-1.12) \\approx 0.32627... $$\nThis value is consistent with the approximation $0.326$ given in the statement. All parts of the statement — the formula for $\\hat{\\rho}$ and the subsequent calculations — are correct.\n\n**Verdict: Correct.**\n\n**C. Reliability-based division of the naive logistic slope by $\\hat{\\rho}$ exactly removes attenuation bias whenever measurement error is nondifferential, regardless of event prevalence or the magnitude of $\\beta$.**\n\nThis statement claims the correction is \"exactly\" valid. As discussed in the preamble, the relationship $\\beta^{*} = \\beta\\rho$ for logistic regression is an approximation, not an exact identity (unlike in linear regression). The exact effect of measurement error on the logistic slope depends on the full distribution of the data. The induced model for $Y$ given $W$ is $\\Pr(Y=1|W) = \\int \\operatorname{logit}^{-1}(\\alpha + \\beta x) f(x|w) dx$, which is generally not a logistic function of $W$. The quality of the approximation $\\beta^{*} \\approx \\beta\\rho$ deteriorates as the event prevalence increases away from $0$ or $1$ and as the magnitude of $\\beta$ increases. Therefore, the simple division by $\\hat{\\rho}$ is an approximate correction, not an exact one. The claim that it works \"regardless of event prevalence or the magnitude of $\\beta$\" is false.\n\n**Verdict: Incorrect.**\n\n**D. If assay error is differential by outcome status (for example, $\\sigma_{\\epsilon}^{2}$ differs between infected and uninfected), then reliability-based correction using replicate data from the cohort remains unbiased for $\\beta$.**\n\nThis statement describes differential measurement error, where the measurement error distribution differs for cases ($Y=1$) and controls ($Y=0$). This violates the core assumption of the classical error model presented, which is that $\\epsilon$ is independent of $Y$. When measurement error is differential, the bias in $\\hat{\\beta}^{*}$ is more complex and is no longer a simple attenuation towards zero. The bias can be in either direction (towards or away from the null). Consequently, the simple reliability correction $\\hat{\\beta} \\approx \\hat{\\beta}^{*}/\\hat{\\rho}$, which is derived under the nondifferential error assumption, is no longer valid. Applying it will not yield an unbiased estimate of $\\beta$. More advanced methods that explicitly model the differential error structure are required.\n\n**Verdict: Incorrect.**\n\n**E. If $\\hat{\\beta} = \\hat{\\beta}^{*}/\\hat{\\rho}$, then by the delta method an approximate standard error is\n$$\\operatorname{SE}(\\hat{\\beta}) \\approx \\sqrt{\\frac{\\operatorname{SE}(\\hat{\\beta}^{*})^{2}}{\\hat{\\rho}^{2}} + \\frac{(\\hat{\\beta}^{*})^{2}\\operatorname{SE}(\\hat{\\rho})^{2}}{\\hat{\\rho}^{4}}},$$\nwhich demonstrates that uncertainty in $\\hat{\\rho}$ inflates the variance of the corrected slope.**\n\nThe corrected estimator $\\hat{\\beta}$ is a function of two random variables: the naive slope estimator $\\hat{\\beta}^{*}$ and the reliability estimator $\\hat{\\rho}$. We can approximate the variance of $\\hat{\\beta}$ using the delta method for a ratio. Let $g(\\hat{\\beta}^{*}, \\hat{\\rho}) = \\hat{\\beta}^{*}/\\hat{\\rho}$. The variance is given by:\n$$ \\operatorname{Var}(\\hat{\\beta}) \\approx \\left(\\frac{\\partial g}{\\partial \\hat{\\beta}^{*}}\\right)^2 \\operatorname{Var}(\\hat{\\beta}^{*}) + \\left(\\frac{\\partial g}{\\partial \\hat{\\rho}}\\right)^2 \\operatorname{Var}(\\hat{\\rho}) + 2 \\left(\\frac{\\partial g}{\\partial \\hat{\\beta}^{*}}\\right) \\left(\\frac{\\partial g}{\\partial \\hat{\\rho}}\\right) \\operatorname{Cov}(\\hat{\\beta}^{*}, \\hat{\\rho}) $$\nThe partial derivatives are $\\partial g/\\partial \\hat{\\beta}^{*} = 1/\\hat{\\rho}$ and $\\partial g/\\partial \\hat{\\rho} = -\\hat{\\beta}^{*}/\\hat{\\rho}^2$. Since $\\hat{\\beta}^{*}$ is estimated from the full cohort and $\\hat{\\rho}$ from a validation subset (or from replicates within the full cohort but treated as a separate estimation problem), it is common to assume that $\\hat{\\beta}^{*}$ and $\\hat{\\rho}$ are approximately uncorrelated, so $\\operatorname{Cov}(\\hat{\\beta}^{*}, \\hat{\\rho}) \\approx 0$.\nSubstituting the derivatives and this assumption yields:\n$$ \\operatorname{Var}(\\hat{\\beta}) \\approx \\left(\\frac{1}{\\hat{\\rho}}\\right)^2 \\operatorname{Var}(\\hat{\\beta}^{*}) + \\left(-\\frac{\\hat{\\beta}^{*}}{\\hat{\\rho}^2}\\right)^2 \\operatorname{Var}(\\hat{\\rho}) = \\frac{\\operatorname{Var}(\\hat{\\beta}^{*})}{\\hat{\\rho}^2} + \\frac{(\\hat{\\beta}^{*})^2 \\operatorname{Var}(\\hat{\\rho})}{\\hat{\\rho}^4} $$\nTaking the square root and substituting $\\operatorname{SE}(\\cdot)^2$ for $\\operatorname{Var}(\\cdot)$ gives the exact formula presented in the statement. The second term, $(\\hat{\\beta}^{*})^2 \\operatorname{SE}(\\hat{\\rho})^2 / \\hat{\\rho}^4$, is non-negative and is zero only if there is no uncertainty in the estimate of reliability ($\\operatorname{SE}(\\hat{\\rho}) = 0$). This term explicitly shows that the variance of the corrected estimate is increased due to the uncertainty in estimating $\\rho$. The statement is a correct application of the delta method and provides a correct interpretation.\n\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "Modern vaccinology increasingly leverages high-dimensional 'omics' data to discover novel, early predictors of long-term immunity, a field known as systems vaccinology. Early gene expression signatures measured just days after vaccination can reveal the specific innate immune pathways engaged by an adjuvant and predict the subsequent adaptive response. This computational exercise guides you through building a machine learning model to predict protection status from such high-dimensional data, a core skill in contemporary vaccine research. ",
            "id": "4703630",
            "problem": "You are given a modeling task grounded in infectious diseases immunology: early innate immune responses shaped by vaccine adjuvants can produce measurable gene expression signatures one day after vaccination, which may serve as correlates of protection. Mathematically, suppose an individual’s day-$1$ gene expression signature is represented by a feature vector $x \\in \\mathbb{R}^p$, and the protection status is a binary outcome $y \\in \\{0,1\\}$, where $y=1$ denotes protection. Your task is to implement a penalized logistic regression model for predicting protection status from the gene expression features, and to quantify predictive power using the Receiver Operating Characteristic (ROC) Area Under the Curve (AUC), expressed as a decimal in $[0,1]$.\n\nFundamental base for the derivation and algorithm design:\n- The binary outcome $y$ is modeled as a Bernoulli random variable with parameter $p(y=1 \\mid x) = \\sigma(\\beta_0 + x^\\top \\beta)$, where the logistic function is $\\sigma(z) = \\frac{1}{1 + e^{-z}}$.\n- The likelihood for a dataset $\\{(x_i, y_i)\\}_{i=1}^n$ under the Bernoulli model is $\\prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}$, with $p_i = \\sigma(\\beta_0 + x_i^\\top \\beta)$.\n- The negative log-likelihood is $\\sum_{i=1}^n \\left[ \\log\\left(1 + e^{z_i}\\right) - y_i z_i \\right]$, where $z_i = \\beta_0 + x_i^\\top \\beta$.\n- A standard ridge (also called $\\ell_2$) penalty on coefficients $\\beta$ adds $\\frac{\\lambda}{2}\\lVert \\beta \\rVert_2^2$ to the objective, with $\\lambda \\ge 0$ and no penalty on the intercept $\\beta_0$.\n- The Receiver Operating Characteristic (ROC) curve summarizes binary classifier performance across decision thresholds. The Area Under the Curve (AUC) is the integral of the True Positive Rate over the False Positive Rate and equals the probability that a randomly chosen protected individual has a higher predicted score than a randomly chosen unprotected individual.\n\nImplement the following:\n- Fit penalized logistic regression by minimizing the regularized negative log-likelihood using Maximum Likelihood Estimation (MLE) with an $\\ell_2$ penalty on $\\beta$ only.\n- Standardize features using training data mean and standard deviation before fitting (apply the same transformation to the test set).\n- Compute predicted probabilities on the test set and then compute ROC AUC using a threshold-free definition based on rank statistics.\n- Express each ROC AUC as a decimal rounded to $3$ decimal places.\n\nData generation for evaluation:\n- For each test case, construct a synthetic dataset that is scientifically plausible as a simplified abstraction of day-$1$ gene expression influenced by an adjuvant:\n    - Choose a true coefficient vector $\\beta^\\star \\in \\mathbb{R}^p$ with $k$ nonzero entries among the first $k$ genes, each set to a common magnitude $b$; the remaining $p-k$ entries are $0$.\n    - Choose an intercept $\\beta_0^\\star$ to control class prevalence.\n    - Draw features $x_i \\sim \\mathcal{N}(0, \\Sigma)$, where $\\Sigma$ is a Toeplitz covariance matrix with entries $\\Sigma_{ij} = \\rho^{|i-j|}$ for a given correlation parameter $\\rho \\in [0,1)$.\n    - For each individual $i$, compute $z_i = \\beta_0^\\star + x_i^\\top \\beta^\\star$, then sample $y_i \\sim \\text{Bernoulli}(\\sigma(z_i))$.\n    - Split into a training set of size $n_{\\text{train}}$ and a test set of size $n_{\\text{test}}$.\n\nQuantities to compute:\n- Fit the penalized logistic regression on the training set with regularization strength $\\lambda$.\n- Compute the ROC AUC on the test set as a float in $[0,1]$, rounded to $3$ decimal places.\n\nTest suite:\n- Use the following $5$ parameter sets to generate data and evaluate your model. For each case, $p$ is the number of genes, $k$ is the number of nonzero true coefficients, $b$ is their magnitude, $\\rho$ is the correlation, $\\beta_0^\\star$ is the intercept controlling imbalance, and $\\lambda$ is the regularization strength. Seeds are used for reproducibility.\n\n    $1.$ Happy path: $n_{\\text{train}} = 400$, $n_{\\text{test}} = 200$, $p = 25$, $k = 5$, $b = 0.8$, $\\rho = 0.3$, $\\beta_0^\\star = 0.0$, $\\lambda = 1.0$, $\\text{seed} = 7$.\n\n    $2.$ Near-random signal: $n_{\\text{train}} = 400$, $n_{\\text{test}} = 200$, $p = 25$, $k = 5$, $b = 0.05$, $\\rho = 0.3$, $\\beta_0^\\star = 0.0$, $\\lambda = 1.0$, $\\text{seed} = 11$.\n\n    $3.$ Strong separation: $n_{\\text{train}} = 400$, $n_{\\text{test}} = 200$, $p = 25$, $k = 5$, $b = 3.0$, $\\rho = 0.1$, $\\beta_0^\\star = 0.0$, $\\lambda = 1.0$, $\\text{seed} = 19$.\n\n    $4.$ Class imbalance: $n_{\\text{train}} = 400$, $n_{\\text{test}} = 200$, $p = 25$, $k = 5$, $b = 0.8$, $\\rho = 0.5$, $\\beta_0^\\star = -1.2$, $\\lambda = 1.0$, $\\text{seed} = 23$.\n\n    $5.$ Small-sample, high-dimensional: $n_{\\text{train}} = 40$, $n_{\\text{test}} = 120$, $p = 50$, $k = 8$, $b = 0.6$, $\\rho = 0.4$, $\\beta_0^\\star = 0.2$, $\\lambda = 5.0$, $\\text{seed} = 29$.\n\nFinal output format:\n- Your program should produce a single line of output containing the $5$ ROC AUC results for the above test cases, rounded to $3$ decimal places, as a comma-separated list enclosed in square brackets, for example: $[0.812,0.503,0.997,0.761,0.689]$.",
            "solution": "We model day-$1$ gene expression signatures, influenced by vaccine adjuvants, as a feature matrix $X \\in \\mathbb{R}^{n \\times p}$ and protection status as binary targets $y \\in \\{0,1\\}^n$. The aim is to use a principled probabilistic model to connect early innate transcriptomic signals to a correlate of protection.\n\nPrinciple-based model derivation:\n$1.$ Binary outcome model. For each individual $i$, the protection status $y_i$ is a Bernoulli random variable with parameter $p_i = \\sigma(z_i)$, where $z_i = \\beta_0 + x_i^\\top \\beta$ and $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic function. This stems from the canonical Generalized Linear Model (GLM) construction for binary data using the logit link.\n\n$2.$ Likelihood and negative log-likelihood. The joint likelihood under independence is\n$$\nL(\\beta_0, \\beta) = \\prod_{i=1}^n p_i^{y_i} (1 - p_i)^{1-y_i}.\n$$\nTaking the negative log yields\n$$\n\\ell(\\beta_0, \\beta) = \\sum_{i=1}^n \\left[ \\log\\left(1 + e^{z_i}\\right) - y_i z_i \\right], \\quad z_i = \\beta_0 + x_i^\\top \\beta.\n$$\nThis is convex in $(\\beta_0, \\beta)$ for logistic regression.\n\n$3.$ Regularization for stability and generalization. To guard against overfitting and to handle separation, add an $\\ell_2$ penalty on $\\beta$ only:\n$$\n\\ell_\\lambda(\\beta_0, \\beta) = \\ell(\\beta_0, \\beta) + \\frac{\\lambda}{2} \\lVert \\beta \\rVert_2^2, \\quad \\lambda \\ge 0.\n$$\nNo penalty is applied to the intercept $\\beta_0$.\n\n$4.$ Gradient for optimization. Let $p_i = \\sigma(z_i)$. The gradient of $\\ell_\\lambda$ is\n$$\n\\nabla_{\\beta_0} \\ell_\\lambda = \\sum_{i=1}^n (p_i - y_i),\n\\quad\n\\nabla_{\\beta} \\ell_\\lambda = X^\\top (p - y) + \\lambda \\beta,\n$$\nwhere $p = (p_1,\\ldots,p_n)^\\top$ and $y = (y_1,\\ldots,y_n)^\\top$. These expressions follow from differentiating the logistic log-likelihood and adding the ridge term. Convexity enables the use of quasi-Newton methods such as Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) to find the unique minimizer.\n\n$5.$ Feature standardization. To ensure that the penalty $\\lambda$ affects coefficients comparably across genes and to stabilize numerical optimization, standardize features using the training data: compute $\\mu_j$ and $\\sigma_j$ for each gene $j$ and transform $x_{ij} \\leftarrow \\frac{x_{ij} - \\mu_j}{\\sigma_j}$ for both training and test sets. If any $\\sigma_j = 0$, set $\\sigma_j \\leftarrow 1$ to avoid division by zero.\n\n$6.$ Predictive probabilities. For the fitted parameters $(\\hat{\\beta}_0, \\hat{\\beta})$, compute test-set probabilities $\\hat{p}_i = \\sigma(\\hat{\\beta}_0 + x_i^\\top \\hat{\\beta})$.\n\n$7.$ ROC AUC as a rank statistic. The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate versus the False Positive Rate across thresholds on $\\hat{p}_i$. The Area Under the Curve (AUC) equals the probability that a randomly selected protected individual receives a higher score than a randomly selected unprotected individual. A threshold-free computation uses ranks:\n$$\n\\text{AUC} = \\frac{\\sum_{i: y_i=1} R_i - \\frac{n_+ (n_+ + 1)}{2}}{n_+ n_-},\n$$\nwhere $R_i$ is the rank (with average tie handling) of $\\hat{p}_i$ among all scores, $n_+ = \\sum_i \\mathbb{1}\\{y_i=1\\}$ and $n_- = \\sum_i \\mathbb{1}\\{y_i=0\\}$. This is equivalent to the Wilcoxon–Mann–Whitney statistic, a well-tested nonparametric formulation of ROC AUC.\n\nSynthetic data generation grounded in immunology and statistics:\n$1.$ Choose $p$, $k$, $b$, $\\rho$, $\\beta_0^\\star$, and sample sizes $n_{\\text{train}}$ and $n_{\\text{test}}$.\n$2.$ Build a Toeplitz covariance $\\Sigma$ with entries $\\Sigma_{ij} = \\rho^{|i-j|}$, reflecting decaying correlation across genes, which is plausible for co-regulated innate transcriptional modules affected by adjuvants.\n$3.$ Construct $\\beta^\\star \\in \\mathbb{R}^p$ with the first $k$ entries equal to $b$ and the rest $0$; set intercept $\\beta_0^\\star$.\n$4.$ Sample features $x_i \\sim \\mathcal{N}(0, \\Sigma)$ for $n_{\\text{train}} + n_{\\text{test}}$ individuals, compute $z_i = \\beta_0^\\star + x_i^\\top \\beta^\\star$, and sample $y_i \\sim \\text{Bernoulli}(\\sigma(z_i))$ independently.\n$5.$ Split into training and test sets, standardize by training statistics, fit penalized logistic regression with regularization strength $\\lambda$, and compute ROC AUC on the test set.\n\nTest suite rationale:\n$1.$ The happy path tests moderate signal with balanced classes and correlated features, typical of informative early innate signatures influenced by adjuvants.\n$2.$ The near-random signal tests the boundary where the signature carries little predictive information, expecting ROC AUC near $0.5$.\n$3.$ The strong separation tests the model under high discriminability, where ROC AUC approaches $1.0$.\n$4.$ The class imbalance tests robustness when protection is rarer (negative intercept), a realistic scenario for some vaccines and populations.\n$5.$ The small-sample, high-dimensional case tests regularization and stability when $p$ is large relative to $n_{\\text{train}}$, a common situation in transcriptomics.\n\nImplementation details:\n- Use a quasi-Newton optimizer with the analytic gradient to minimize the convex penalized objective.\n- Leave the intercept unpenalized while penalizing coefficients with $\\lambda$.\n- Compute ROC AUC via average ranks to correctly handle ties.\n- Round each ROC AUC to $3$ decimal places and produce the single-line output $[a_1,a_2,a_3,a_4,a_5]$, where each $a_i \\in [0,1]$ is a float rounded to $3$ decimals.\n\nThis solution adheres to fundamental statistical principles for binary outcomes and leverages a well-tested, threshold-free formulation of ROC AUC to quantify early correlates of protection derived from gene expression signatures modulated by vaccine adjuvants.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import rankdata\n\ndef sigmoid(z):\n    # Numerically stable sigmoid\n    # Clip z to avoid overflow in exp\n    z = np.clip(z, -700, 700)\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef standardize_train_test(X_train, X_test):\n    mean = X_train.mean(axis=0)\n    std = X_train.std(axis=0)\n    # Avoid division by zero: if std == 0, set to 1\n    std_safe = np.where(std == 0.0, 1.0, std)\n    X_train_std = (X_train - mean) / std_safe\n    X_test_std = (X_test - mean) / std_safe\n    return X_train_std, X_test_std\n\ndef fit_logistic_ridge(X, y, lam):\n    # Add intercept column (ones). Intercept is unpenalized.\n    n, p = X.shape\n    X_aug = np.hstack([np.ones((n, 1)), X])\n\n    def loss_and_grad(w):\n        z = X_aug @ w\n        p_hat = sigmoid(z)\n        # Negative log-likelihood\n        nll = np.sum(np.log1p(np.exp(z)) - y * z)\n        # Ridge penalty on coefficients only (excluding intercept at index 0)\n        reg = 0.5 * lam * np.sum(w[1:] ** 2)\n        loss = nll + reg\n        # Gradient\n        grad = X_aug.T @ (p_hat - y)\n        grad[1:] += lam * w[1:]\n        return loss, grad\n\n    w0 = np.zeros(p + 1)\n    res = minimize(\n        fun=lambda w: loss_and_grad(w)[0],\n        x0=w0,\n        jac=lambda w: loss_and_grad(w)[1],\n        method=\"L-BFGS-B\",\n        options={\"maxiter\": 1000}\n    )\n    return res.x  # intercept and coefficients\n\ndef auc_rank(scores, y):\n    # Compute ROC AUC using rank statistic (Mann-Whitney U)\n    y = np.asarray(y)\n    scores = np.asarray(scores)\n    n_pos = int(np.sum(y == 1))\n    n_neg = int(np.sum(y == 0))\n    if n_pos == 0 or n_neg == 0:\n        # Undefined AUC: return NaN to signal degenerate case\n        return np.nan\n    ranks = rankdata(scores, method='average')  # average ties\n    sum_rank_pos = np.sum(ranks[y == 1])\n    auc = (sum_rank_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n    return float(auc)\n\ndef toeplitz_cov(p, rho):\n    # AR(1)-like correlation matrix: Sigma_ij = rho^{|i-j|}\n    # Construct first row then build Toeplitz by broadcasting\n    idx = np.arange(p)\n    first_row = rho ** idx\n    # Toeplitz: Sigma[i,j] = first_row[|i-j|]\n    # Efficient construction using abs difference\n    Sigma = first_row[np.abs(idx[:, None] - idx[None, :])]\n    return Sigma\n\ndef generate_data(n_train, n_test, p, k, b, rho, beta0, seed):\n    rng = np.random.default_rng(seed)\n    # True coefficients: first k entries are b, others 0\n    beta_true = np.zeros(p)\n    beta_true[:k] = b\n    Sigma = toeplitz_cov(p, rho)\n    # Sample features for train+test\n    X_all = rng.multivariate_normal(mean=np.zeros(p), cov=Sigma, size=n_train + n_test)\n    z_all = beta0 + X_all @ beta_true\n    prob_all = sigmoid(z_all)\n    y_all = rng.binomial(1, prob_all, size=n_train + n_test).astype(int)\n    # Split into train and test\n    X_train = X_all[:n_train]\n    y_train = y_all[:n_train]\n    X_test = X_all[n_train:]\n    y_test = y_all[n_train:]\n    return X_train, y_train, X_test, y_test\n\ndef run_case(params):\n    n_train = params[\"n_train\"]\n    n_test = params[\"n_test\"]\n    p = params[\"p\"]\n    k = params[\"k\"]\n    b = params[\"b\"]\n    rho = params[\"rho\"]\n    beta0 = params[\"beta0\"]\n    lam = params[\"lambda\"]\n    seed = params[\"seed\"]\n\n    X_train, y_train, X_test, y_test = generate_data(n_train, n_test, p, k, b, rho, beta0, seed)\n    # Standardize features using training statistics\n    X_train_std, X_test_std = standardize_train_test(X_train, X_test)\n    # Fit penalized logistic regression\n    w_hat = fit_logistic_ridge(X_train_std, y_train, lam)\n    # Predict probabilities on test set\n    intercept = w_hat[0]\n    coef = w_hat[1:]\n    scores = sigmoid(intercept + X_test_std @ coef)\n    # Compute AUC\n    auc = auc_rank(scores, y_test)\n    return auc\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\"n_train\": 400, \"n_test\": 200, \"p\": 25, \"k\": 5, \"b\": 0.8, \"rho\": 0.3, \"beta0\": 0.0, \"lambda\": 1.0, \"seed\": 7},\n        {\"n_train\": 400, \"n_test\": 200, \"p\": 25, \"k\": 5, \"b\": 0.05, \"rho\": 0.3, \"beta0\": 0.0, \"lambda\": 1.0, \"seed\": 11},\n        {\"n_train\": 400, \"n_test\": 200, \"p\": 25, \"k\": 5, \"b\": 3.0, \"rho\": 0.1, \"beta0\": 0.0, \"lambda\": 1.0, \"seed\": 19},\n        {\"n_train\": 400, \"n_test\": 200, \"p\": 25, \"k\": 5, \"b\": 0.8, \"rho\": 0.5, \"beta0\": -1.2, \"lambda\": 1.0, \"seed\": 23},\n        {\"n_train\": 40, \"n_test\": 120, \"p\": 50, \"k\": 8, \"b\": 0.6, \"rho\": 0.4, \"beta0\": 0.2, \"lambda\": 5.0, \"seed\": 29},\n    ]\n\n    results = []\n    for case in test_cases:\n        auc = run_case(case)\n        # Round to 3 decimal places; if NaN (degenerate), keep NaN\n        if np.isnan(auc):\n            results.append(\"nan\")\n        else:\n            results.append(f\"{auc:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}