{
    "hands_on_practices": [
        {
            "introduction": "The journey from raw sequencing reads to a reliable pathogen genome begins with assessing data quality at the most fundamental level: the individual base call. Phred quality scores ($Q$) provide a universal, logarithmic language for expressing the confidence in each nucleotide called by a sequencer. This exercise challenges you to derive this relationship from first principles and apply Bayesian reasoning to combine evidence from multiple independent reads into a single, high-confidence consensus call. By completing this practice, you will gain a deep, quantitative understanding of how consensus sequences are constructed and why high-quality, high-coverage data is the bedrock of robust genomic analysis. ",
            "id": "4667814",
            "problem": "A genomic epidemiology team uses Next-Generation Sequencing (NGS) to investigate a suspected transmission cluster of an RNA virus. At a single locus used for cluster assignment, $k$ independent NGS reads all report the nucleotide $\\texttt{G}$. Each read $i \\in \\{1,\\dots,k\\}$ has an associated base quality score $Q_i$ produced by a standard base caller. The Phred quality score is defined as a logarithmic measure of base-calling error probability such that a reduction of the error probability by a factor of $10$ increases the quality score by $10$ units, and $Q_i$ is monotone decreasing in the error probability.\n\nStarting from this definition and a probabilistic model in which:\n- the nucleotide alphabet is four states $\\{\\texttt{A},\\texttt{C},\\texttt{G},\\texttt{T}\\}$,\n- the prior over true nucleotides is uniform across the four states,\n- per-read base-calling errors are independent across reads,\n- conditional on an error when the true base is not $\\texttt{G}$, the erroneous report is uniformly distributed over the three incorrect nucleotides,\n\nderive from first principles an explicit analytic expression for the posterior error probability of the consensus call $\\texttt{G}$ given the observed data and then its Phred-scaled consensus confidence. Then evaluate this expression for $k=5$ reads with per-base quality scores $Q_1=30$, $Q_2=27$, $Q_3=35$, $Q_4=20$, and $Q_5=40$, all calling $\\texttt{G}$.\n\nReport the final consensus Phred quality score as a single real number. Round your answer to four significant figures. No physical units are required.",
            "solution": "The problem is to derive and evaluate the consensus Phred quality score for a nucleotide base call given a set of Next-Generation Sequencing (NGS) reads and their associated quality scores. This requires a Bayesian approach.\n\nFirst, we formalize the relationship between the Phred quality score $Q_i$ and the base-calling error probability $p_{e,i}$ for a single read $i$. The problem states that a reduction of the error probability by a factor of $10$ increases the quality score by $10$ units. This implies a logarithmic relationship. Let the error probability be $p_e$. The quality score $Q$ is a function of $p_e$, say $Q(p_e)$. The condition is $Q(p_e/10) = Q(p_e) + 10$. This functional equation is solved by the form $Q(p_e) = -10 \\log_{10}(p_e)$. We can verify this: $Q(p_e/10) = -10 \\log_{10}(p_e/10) = -10(\\log_{10}(p_e) - 1) = -10\\log_{10}(p_e) + 10 = Q(p_e) + 10$.\nFrom this definition, the error probability for read $i$ with quality score $Q_i$ is given by:\n$$p_{e,i} = 10^{-Q_i/10}$$\nThe probability of a correct call for read $i$ is $1 - p_{e,i}$.\n\nNext, we establish the Bayesian framework. Let $T$ be the random variable for the true nucleotide at the genomic locus, which can take values in the set $\\{\\texttt{A}, \\texttt{C}, \\texttt{G}, \\texttt{T}\\}$. Let $D$ represent the observed data, which consists of $k$ independent reads, all reporting the nucleotide $\\texttt{G}$. We denote the event that read $i$ reports $\\texttt{G}$ as $D_i$. Thus, $D = \\{D_1, D_2, \\dots, D_k\\}$.\n\nThe goal is to find the posterior error probability of the consensus call $\\texttt{G}$, which is the probability that the true nucleotide is not $\\texttt{G}$, given the data $D$. We denote this as $P_{\\text{err, cons}} = P(T \\neq \\texttt{G} | D)$. This is equal to $1 - P(T = \\texttt{G} | D)$.\n\nUsing Bayes' theorem, the posterior probability of the true base being $\\texttt{G}$ is:\n$$P(T = \\texttt{G} | D) = \\frac{P(D | T = \\texttt{G}) P(T = \\texttt{G})}{P(D)}$$\nThe marginal probability of the data, $P(D)$, is given by the law of total probability:\n$$P(D) = \\sum_{b \\in \\{\\texttt{A},\\texttt{C},\\texttt{G},\\texttt{T}\\}} P(D|T=b)P(T=b)$$\nThe problem specifies a uniform prior over the true nucleotides:\n$$P(T=b) = \\frac{1}{4} \\quad \\text{for any } b \\in \\{\\texttt{A}, \\texttt{C}, \\texttt{G}, \\texttt{T}\\}$$\n\nNow, we derive the likelihood terms, $P(D|T=b)$. Since the reads are independent, the likelihood is the product of the probabilities for each read: $P(D|T=b) = \\prod_{i=1}^{k} P(D_i|T=b)$.\n\nCase 1: The true nucleotide is $\\texttt{G}$ ($T = \\texttt{G}$).\nThe probability of observing read $i$ as $\\texttt{G}$ is the probability of a correct call, which is $1 - p_{e,i}$.\nThe likelihood is:\n$$L_G = P(D | T = \\texttt{G}) = \\prod_{i=1}^{k} (1 - p_{e,i})$$\n\nCase 2: The true nucleotide is not $\\texttt{G}$ (e.g., $T = \\texttt{A}$).\nThe probability of observing read $i$ as $\\texttt{G}$ is the probability of an error. The problem states that conditional on an error, the reported base is uniformly distributed over the three incorrect options. If the true base is $\\texttt{A}$, an error means reading $\\texttt{C}$, $\\texttt{G}$, or $\\texttt{T}$. The probability of reading $\\texttt{G}$ specifically is $1/3$ of the total error probability.\n$$P(D_i | T = \\texttt{A}) = p_{e,i} \\times \\frac{1}{3} = \\frac{p_{e,i}}{3}$$\nBy symmetry, the likelihood is the same for $T = \\texttt{C}$ and $T = \\texttt{T}$. Let's denote this likelihood as $L_{\\neg G}$.\n$$L_{\\neg G} = P(D | T = b \\neq \\texttt{G}) = \\prod_{i=1}^{k} \\frac{p_{e,i}}{3} = \\left(\\frac{1}{3}\\right)^k \\prod_{i=1}^{k} p_{e,i}$$\n\nNow we can write the posterior error probability:\n$$P_{\\text{err, cons}} = P(T \\neq \\texttt{G} | D) = \\frac{P(D | T \\neq \\texttt{G}) P(T \\neq \\texttt{G})}{P(D)}$$\n$$P(D | T \\neq \\texttt{G})P(T \\neq \\texttt{G}) = P(D|T=\\texttt{A})P(T=\\texttt{A}) + P(D|T=\\texttt{C})P(T=\\texttt{C}) + P(D|T=\\texttt{T})P(T=\\texttt{T})$$\n$$= L_{\\neg G} \\left(\\frac{1}{4}\\right) + L_{\\neg G} \\left(\\frac{1}{4}\\right) + L_{\\neg G} \\left(\\frac{1}{4}\\right) = 3 L_{\\neg G} \\left(\\frac{1}{4}\\right)$$\nThe total evidence is $P(D) = P(D|T=\\texttt{G})P(T=\\texttt{G}) + 3 L_{\\neg G} (1/4) = L_G(1/4) + 3L_{\\neg G}(1/4) = \\frac{1}{4}(L_G + 3L_{\\neg G})$.\nSo, the posterior error probability is:\n$$P_{\\text{err, cons}} = \\frac{3 L_{\\neg G} (1/4)}{\\frac{1}{4}(L_G + 3L_{\\neg G})} = \\frac{3 L_{\\neg G}}{L_G + 3 L_{\\neg G}}$$\nThis can be rewritten as:\n$$P_{\\text{err, cons}} = \\frac{1}{1 + \\frac{L_G}{3L_{\\neg G}}}$$\nThe consensus Phred quality score, $Q_{\\text{cons}}$, is defined in the same way as the per-read score:\n$$Q_{\\text{cons}} = -10 \\log_{10}(P_{\\text{err, cons}}) = 10 \\log_{10}\\left(1 + \\frac{L_G}{3L_{\\neg G}}\\right)$$\nLet's substitute the expressions for $L_G$ and $L_{\\neg G}$:\n$$\\frac{L_G}{3L_{\\neg G}} = \\frac{\\prod_{i=1}^{k} (1 - p_{e,i})}{3 \\left(\\frac{1}{3}\\right)^k \\prod_{i=1}^{k} p_{e,i}} = \\frac{\\prod_{i=1}^{k} (1 - p_{e,i})}{3^{1-k} \\prod_{i=1}^{k} p_{e,i}} = 3^{k-1} \\prod_{i=1}^{k} \\frac{1 - p_{e,i}}{p_{e,i}}$$\nUsing the relation $p_{e,i} = 10^{-Q_i/10}$, we have $\\frac{1-p_{e,i}}{p_{e,i}} = \\frac{1 - 10^{-Q_i/10}}{10^{-Q_i/10}} = 10^{Q_i/10} - 1$.\nThe final analytic expression for the consensus Phred score is:\n$$Q_{\\text{cons}} = 10 \\log_{10}\\left(1 + 3^{k-1} \\prod_{i=1}^{k} (10^{Q_i/10} - 1)\\right)$$\n\nNow, we evaluate this expression for the given data: $k=5$ reads with quality scores $Q_1=30$, $Q_2=27$, $Q_3=35$, $Q_4=20$, and $Q_5=40$.\nFirst, we compute the term $(10^{Q_i/10} - 1)$ for each read:\nFor $Q_1=30$: $10^{30/10} - 1 = 10^3 - 1 = 999$\nFor $Q_2=27$: $10^{27/10} - 1 = 10^{2.7} - 1$\nFor $Q_3=35$: $10^{35/10} - 1 = 10^{3.5} - 1$\nFor $Q_4=20$: $10^{20/10} - 1 = 10^2 - 1 = 99$\nFor $Q_5=40$: $10^{40/10} - 1 = 10^4 - 1 = 9999$\n\nThe product term is:\n$$\\prod_{i=1}^{5} (10^{Q_i/10} - 1) = (999) \\times (10^{2.7} - 1) \\times (10^{3.5} - 1) \\times (99) \\times (9999)$$\nWith $k=5$, the factor $3^{k-1}$ is $3^{5-1} = 3^4 = 81$.\nSubstituting these values into the expression for $Q_{\\text{cons}}$:\n$$Q_{\\text{cons}} = 10 \\log_{10}\\left(1 + 81 \\times (999) \\times (10^{2.7} - 1) \\times (10^{3.5} - 1) \\times (99) \\times (9999) \\right)$$\nLet's evaluate the argument of the logarithm.\n$10^{2.7} \\approx 501.1872$\n$10^{3.5} \\approx 3162.2777$\nThe argument is $1 + 81 \\times 999 \\times (501.1872 - 1) \\times (3162.2777 - 1) \\times 99 \\times 9999$.\nThis is approximately $1 + 81 \\times 999 \\times 500.1872 \\times 3161.2777 \\times 99 \\times 9999 \\approx 1.26663 \\times 10^{17}$.\nSince this value is very large, the $1$ is negligible.\n$Q_{\\text{cons}} \\approx 10 \\log_{10}(1.26663 \\times 10^{17})$\n$Q_{\\text{cons}} \\approx 10 \\times ( \\log_{10}(1.26663) + \\log_{10}(10^{17}) )$\n$Q_{\\text{cons}} \\approx 10 \\times (0.10264 + 17) = 10 \\times 17.10264 = 171.0264$\n\nTo be more precise and avoid intermediate rounding:\n$\\log_{10}\\left(1 + 3^{k-1} \\prod (10^{Q_i/10}-1)\\right) \\approx \\log_{10}\\left(3^{k-1} \\prod (10^{Q_i/10}-1)\\right)$\n$= \\log_{10}(3^{k-1}) + \\sum_{i=1}^{k} \\log_{10}(10^{Q_i/10}-1)$\nFor $k=5$:\n$= 4 \\log_{10}(3) + \\log_{10}(999) + \\log_{10}(10^{2.7}-1) + \\log_{10}(10^{3.5}-1) + \\log_{10}(99) + \\log_{10}(9999)$\n$\\approx 4(0.477121) + 2.999565 + 2.699140 + 3.499862 + 1.995635 + 3.999957$\n$\\approx 1.908485 + 15.194159 = 17.102644$\nThe consensus score is $10$ times this value:\n$Q_{\\text{cons}} \\approx 10 \\times 17.102644 = 171.02644$\nRounding to four significant figures gives $171.0$.",
            "answer": "$$\\boxed{171.0}$$"
        },
        {
            "introduction": "Once a set of reliable pathogen genomes is assembled, we can begin to unravel the demographic history of the population from which they were sampled. The coalescent process provides a powerful theoretical framework that works backward in time, modeling how ancestral lineages merge based on population size. This practice guides you through the derivation of a Bayesian skyline plot, a method that uses the timing of these coalescent events to estimate changes in effective population size ($N_e$) over time. This hands-on exercise demystifies a core phylodynamic technique, enabling you to interpret genetic data in the context of epidemic growth, decline, or stability. ",
            "id": "4667746",
            "problem": "You are given inter-coalescent waiting times and corresponding lineage counts from a single rooted, strictly bifurcating phylogeny sampled under the standard coalescent. Assume a piecewise constant effective population size function $N_e(t)$ segmented into groups of consecutive inter-coalescent intervals. Within each group $g$, the effective population size is constant and equal to $N_{e,g}$.\n\nFundamental base and modeling assumptions:\n- Under the standard coalescent with piecewise constant population size, each inter-coalescent waiting time $t_i$ with $k_i$ contemporary lineages has an exponential distribution with hazard rate $h_i = \\binom{k_i}{2}/N_{e,g}$ when interval $i$ belongs to group $g$.\n- Let $\\lambda_g = 1/N_{e,g}$. Then the likelihood contribution of an interval $i$ in group $g$ is exponential with rate $r_i = \\lambda_g \\binom{k_i}{2}$, independent across intervals given the group-wise constants.\n- Adopt the conjugate prior for each group $g$ on $\\lambda_g$: a Gamma distribution with shape $a_g$ and rate $b_g$, denoted $\\lambda_g \\sim \\mathrm{Gamma}(a_g,b_g)$, where the density is proportional to $\\lambda_g^{a_g - 1} \\exp(-b_g \\lambda_g)$.\n- The sufficient statistics for each group $g$ are the number of inter-coalescent intervals $m_g$ and the scaled time sum $S_g = \\sum_{i \\in g} \\binom{k_i}{2} t_i$.\n\nYour task is to compute a Bayesian skyline summary for $N_{e,g}$ for each group $g$ by deriving the posterior distribution of $\\lambda_g$ and then transforming to $N_{e,g} = 1/\\lambda_g$. For each group $g$, compute:\n- The posterior mean of $N_{e,g}$.\n- The posterior median of $N_{e,g}$.\n- The lower and upper endpoints of the central $95$-level credible interval for $N_{e,g}$, that is, the $0.025$ and $0.975$ posterior quantiles.\n\nExpress all outputs for $N_{e,g}$ as real numbers (counts of individuals, a dimensionless quantity), rounded to six decimal places. Time units for $t_i$ are in years; no time unit should be printed in the output.\n\nDerive your method from the fundamental assumptions above. Do not use or assume any formula that is not a direct consequence of these assumptions.\n\nTest suite. For each test case, you are given:\n- A list of inter-coalescent times $\\{t_i\\}$ in years.\n- A list of lineage counts $\\{k_i\\}$ of the same length, where $k_i \\ge 2$ and $t_i \\ge 0$.\n- A list of positive integers specifying the group sizes that partition the sequence of intervals consecutively.\n- Prior hyperparameters shared across groups: shape $a$ and rate $b$.\n\nCompute the Bayesian skyline summary for each group in each test case. The test cases are:\n\n- Test case $1$:\n  - $t = [0.05, 0.12, 0.08, 0.20, 0.30, 0.25]$\n  - $k = [10, 9, 8, 7, 6, 5]$\n  - group sizes $= [2, 2, 2]$\n  - prior $(a, b) = (2.5, 1.0)$\n\n- Test case $2$:\n  - $t = [0.02, 0.02, 0.02, 0.02]$\n  - $k = [50, 49, 48, 47]$\n  - group sizes $= [1, 1, 2]$\n  - prior $(a, b) = (2.1, 0.5)$\n\n- Test case $3$:\n  - $t = [0.30, 0.25, 0.35, 0.40]$\n  - $k = [6, 5, 4, 3]$\n  - group sizes $= [4]$\n  - prior $(a, b) = (1.5, 1.0)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case. Each test case element is a list of groups, and each group element is a list of four numbers in the order [posterior mean, posterior median, $0.025$ quantile, $0.975$ quantile], each rounded to six decimal places. For example, a valid structural shape is:\n  - $[\\,[\\,[x_{11}, x_{12}, x_{13}, x_{14}], [x_{21}, x_{22}, x_{23}, x_{24}], \\ldots], \\; [\\ldots], \\; [\\ldots]\\,]$\n\nIn your written solution, explain the derivation from the base assumptions and discuss identifiability of piecewise constant segments of $N_e(t)$ in terms of how $m_g$ and $S_g$ constrain inference, including edge cases where a segment has very limited or no information.",
            "solution": "The problem requires the computation of posterior summary statistics for a piecewise constant effective population size, $N_e(t)$, within a Bayesian framework. The methodology is derived from the fundamental assumptions of the standard coalescent model, an exponential likelihood for inter-coalescent waiting times, and a conjugate Gamma prior on the precision parameter $\\lambda_g = 1/N_{e,g}$.\n\nThe derivation proceeds in three main stages:\n1.  Derivation of the posterior distribution for the parameter $\\lambda_g$ for each population size group $g$.\n2.  Transformation of the posterior distribution from $\\lambda_g$ to the effective population size $N_{e,g}$.\n3.  Computation of the required summary statistics (mean, median, and $95\\%$ credible interval) from the posterior distribution of $N_{e,g}$.\n\n### 1. Posterior Distribution of $\\lambda_g$\n\nAccording to Bayes' theorem, the posterior probability distribution is proportional to the product of the likelihood and the prior probability distribution.\n$$P(\\lambda_g | \\text{data}) \\propto P(\\text{data} | \\lambda_g) \\times P(\\lambda_g)$$\n\n**Likelihood Function:**\nThe problem states that for an interval $i$ within a group $g$, the inter-coalescent waiting time $t_i$ for $k_i$ lineages is exponentially distributed with a rate $r_i = \\lambda_g \\binom{k_i}{2}$. The probability density function (PDF) for a single interval is:\n$$P(t_i | \\lambda_g, k_i) = \\lambda_g \\binom{k_i}{2} \\exp\\left( -\\lambda_g \\binom{k_i}{2} t_i \\right)$$\nAssuming the waiting times are conditionally independent given $\\lambda_g$, the total likelihood for the data in group $g$ (comprising $m_g$ intervals) is the product of the individual PDFs:\n$$L(\\lambda_g) = P(\\{t_i, k_i\\}_{i \\in g} | \\lambda_g) = \\prod_{i \\in g} P(t_i | \\lambda_g, k_i)$$\n$$L(\\lambda_g) = \\prod_{i \\in g} \\left[ \\lambda_g \\binom{k_i}{2} \\exp\\left( -\\lambda_g \\binom{k_i}{2} t_i \\right) \\right]$$\nWe can group the terms involving $\\lambda_g$:\n$$L(\\lambda_g) = \\left( \\prod_{i \\in g} \\binom{k_i}{2} \\right) \\left( \\prod_{i \\in g} \\lambda_g \\right) \\exp\\left( -\\sum_{i \\in g} \\lambda_g \\binom{k_i}{2} t_i \\right)$$\n$$L(\\lambda_g) = \\left( \\prod_{i \\in g} \\binom{k_i}{2} \\right) \\lambda_g^{m_g} \\exp\\left( -\\lambda_g \\sum_{i \\in g} \\binom{k_i}{2} t_i \\right)$$\nThe term $\\prod_{i \\in g} \\binom{k_i}{2}$ is a constant with respect to $\\lambda_g$. Using the provided sufficient statistic $S_g = \\sum_{i \\in g} \\binom{k_i}{2} t_i$, the likelihood is proportional to:\n$$L(\\lambda_g) \\propto \\lambda_g^{m_g} \\exp(-\\lambda_g S_g)$$\nThis is the kernel of the likelihood function.\n\n**Prior Distribution:**\nThe problem specifies a conjugate Gamma prior for $\\lambda_g$:\n$$\\lambda_g \\sim \\mathrm{Gamma}(a_g, b_g)$$\nThe PDF is $P(\\lambda_g) \\propto \\lambda_g^{a_g - 1} \\exp(-b_g \\lambda_g)$, where $a_g$ is the shape parameter and $b_g$ is the rate parameter.\n\n**Posterior Distribution:**\nMultiplying the likelihood kernel and the prior density kernel gives the posterior density kernel:\n$$P(\\lambda_g | \\text{data}) \\propto \\left( \\lambda_g^{m_g} \\exp(-\\lambda_g S_g) \\right) \\times \\left( \\lambda_g^{a_g - 1} \\exp(-b_g \\lambda_g) \\right)$$\n$$P(\\lambda_g | \\text{data}) \\propto \\lambda_g^{a_g + m_g - 1} \\exp\\left( -(b_g + S_g) \\lambda_g \\right)$$\nThis is the kernel of a Gamma distribution with updated parameters. Let the posterior parameters be $a'_g = a_g + m_g$ and $b'_g = b_g + S_g$.\nThus, the posterior distribution of $\\lambda_g$ is:\n$$\\lambda_g | \\text{data} \\sim \\mathrm{Gamma}(a_g + m_g, b_g + S_g)$$\n\n### 2. Posterior Distribution of $N_{e,g}$\n\nThe effective population size $N_{e,g}$ is the reciprocal of $\\lambda_g$, i.e., $N_{e,g} = 1/\\lambda_g$. If a random variable $X$ follows a Gamma distribution, $X \\sim \\mathrm{Gamma}(\\alpha, \\beta)$, then its reciprocal $Y=1/X$ follows an Inverse-Gamma distribution with the same shape parameter $\\alpha$ and a scale parameter $\\beta$ (the rate of the Gamma becomes the scale of the Inverse-Gamma).\nTherefore, the posterior distribution for $N_{e,g}$ is an Inverse-Gamma distribution:\n$$N_{e,g} | \\text{data} \\sim \\mathrm{InverseGamma}(a'_g, b'_g) = \\mathrm{InverseGamma}(a_g + m_g, b_g + S_g)$$\n\n### 3. Posterior Summary Statistics for $N_{e,g}$\n\nWe can now compute the required statistics from this posterior $\\mathrm{InverseGamma}(a'_g, b'_g)$ distribution.\n\n**Posterior Mean:**\nThe expected value of an Inverse-Gamma distribution, $\\mathrm{InverseGamma}(\\alpha, \\beta)$, is $E[Y] = \\frac{\\beta}{\\alpha - 1}$, defined for $\\alpha > 1$.\nIn our case, the posterior mean of $N_{e,g}$ is:\n$$E[N_{e,g} | \\text{data}] = \\frac{b'_g}{a'_g - 1} = \\frac{b_g + S_g}{a_g + m_g - 1}$$\nThe condition $a'_g > 1$ is satisfied for all test cases, since the given prior shape $a$ is greater than $1$ and the number of intervals per group $m_g$ is at least $1$.\n\n**Posterior Median and Credible Interval:**\nThese statistics are derived from the quantiles of the posterior distribution. There is a direct relationship between the quantiles of a Gamma distribution and its corresponding Inverse-Gamma distribution. Let $Q_Y(p)$ be the $p$-th quantile of $Y \\sim \\mathrm{InverseGamma}(\\alpha, \\beta)$ and $Q_X(p)$ be the $p$-th quantile of $X \\sim \\mathrm{Gamma}(\\alpha, \\beta)$. The relationship is:\n$$Q_Y(p) = \\frac{1}{Q_X(1-p)}$$\nWe apply this to find the median ($50$-th percentile) and the $95\\%$ central credible interval (endpoints at the $2.5$-th and $97.5$-th percentiles) for $N_{e,g}$. Let $Q_{\\lambda_g}(p)$ denote the $p$-th quantile of the posterior distribution $\\lambda_g | \\text{data} \\sim \\mathrm{Gamma}(a'_g, b'_g)$.\n\n-   **Posterior Median ($p=0.5$):**\n    $$\\text{Median}[N_{e,g}] = Q_{N_{e,g}}(0.5) = \\frac{1}{Q_{\\lambda_g}(1-0.5)} = \\frac{1}{Q_{\\lambda_g}(0.5)}$$\n-   **Lower Credible Bound ($p=0.025$):**\n    $$Q_{N_{e,g}}(0.025) = \\frac{1}{Q_{\\lambda_g}(1-0.025)} = \\frac{1}{Q_{\\lambda_g}(0.975)}$$\n-   **Upper Credible Bound ($p=0.975$):**\n    $$Q_{N_{e,g}}(0.975) = \\frac{1}{Q_{\\lambda_g}(1-0.975)} = \\frac{1}{Q_{\\lambda_g}(0.025)}$$\n\nThese quantiles of the Gamma distribution do not have a closed-form expression and must be computed numerically, for which scientific libraries like `scipy.stats` are used.\n\n### Discussion on Identifiability\nThe posterior parameters $a'_g = a_g + m_g$ and $b'_g = b_g + S_g$ explicitly show how the data, summarized by the sufficient statistics $m_g$ and $S_g$, update the prior beliefs encoded in $a_g$ and $b_g$. Identifiability of the population size $N_{e,g}$ for a given segment depends on the information content of the data within that segment.\n-   The statistic $m_g$, the number of coalescent events in group $g$, informs the shape of the posterior distribution. A larger $m_g$ leads to a posterior shape parameter $a'_g$ that is dominated by the data, making the inference less sensitive to the prior shape $a_g$.\n-   The statistic $S_g = \\sum_{i \\in g} \\binom{k_i}{2} t_i$, which is the sum of waiting times each scaled by the number of lineage pairs, informs the rate/scale of the posterior. A large $S_g$ value (indicating long waiting times) will dominate the prior rate $b_g$, pulling the posterior distribution of $\\lambda_g$ towards smaller values, and consequently, the posterior of $N_{e,g}$ towards larger values.\n-   **Edge Cases:** If a group contains very little information (e.g., $m_g$ is small, or all $t_i$ are very short, leading to a small $S_g$), the posterior distribution will be wide (high variance) and heavily influenced by the choice of prior ($a_g, b_g$). In such cases, $N_{e,g}$ is weakly identified. Conversely, when $m_g$ and $S_g$ are large, the likelihood dominates the prior, the posterior distribution becomes sharply peaked, and $N_{e,g}$ is strongly identified by the data. The problem setup ensures $m_g \\ge 1$ for all groups, so the parameters are always technically identifiable, but the precision of the estimate varies with the data content.",
            "answer": "[[[2.162857,1.968254,0.916127,4.869032],[2.125714,1.934898,0.899818,4.789173],[2.285714,2.080517,0.967396,5.132560]],[[0.063158,0.059437,0.029858,0.131109],[0.067347,0.063381,0.031841,0.139773],[0.046465,0.043743,0.024446,0.089855]],[[5.200000,4.642055,2.185257,11.838329]]]"
        },
        {
            "introduction": "A primary goal of pathogen genomics is to link genetic variation to clinically and epidemiologically important phenotypes, such as antibiotic resistance or virulence. When the number of genetic variants vastly exceeds the number of samples, traditional regression methods fail. This exercise introduces the LASSO (Least Absolute Shrinkage and Selection Operator) regression, a penalized method that simultaneously performs variable selection and model fitting, making it ideal for high-dimensional genomic data. By deriving the LASSO objective from a Bayesian perspective and implementing a solver, you will learn how to build sparse, interpretable models that can identify key driver mutations and predict pathogen behavior from sequence alone. ",
            "id": "4667796",
            "problem": "Consider a molecular epidemiology setting in infectious diseases where a continuous phenotype such as the logarithm base two of Minimum Inhibitory Concentration (MIC) for an antibiotic is measured across a cohort of pathogen genomes. Let $n$ denote the number of isolates and $p$ the number of genetic variants, such as Single Nucleotide Polymorphisms (SNPs). The variant presence matrix is $X \\in \\{0,1\\}^{n \\times p}$ with entry $x_{ij}$ indicating the presence ($x_{ij} = 1$) or absence ($x_{ij} = 0$) of variant $j$ in isolate $i$. The phenotype vector is $y \\in \\mathbb{R}^n$. Assume a linear additive genotype-phenotype mapping with an intercept and additive noise.\n\nStarting from foundational principles suitable for pathogen genomics:\n- Use the Central Dogma of molecular biology as the conceptual basis that genotype can influence phenotype through molecular mechanisms.\n- Assume additive contributions of variants to the phenotype grounded in widely accepted quantitative genetics reasoning.\n- Assume Gaussian measurement noise, consistent with the common modeling assumption for continuous traits.\n\nTask 1 (Derivation):\n- Derive the penalized regression objective with an $\\ell_1$-norm penalty known as Least Absolute Shrinkage and Selection Operator (LASSO), without providing any shortcut formulas. Begin from the linear model with Gaussian noise and a sparsity-inducing prior over variant effect sizes that embodies the rationale for favoring simpler models. Explicitly justify the choice of penalty via probabilistic or optimization arguments, and argue why the intercept must not be penalized in this context. Interpret each coefficient in terms of additive effect size on the phenotype scale for binary variant presence variables.\n\nTask 2 (Computation):\n- Implement a program that, given $X$, $y$, and a nonnegative penalty parameter $\\lambda$, computes the intercept and the coefficient vector that minimize the derived penalized objective. Features must be standardized for numerical stability and interpretability: center each column of $X$ and scale it to unit variance when its variance is nonzero; do not penalize the intercept. After optimization in the standardized space, map the solution back to the original feature scale so that coefficients are interpretable as additive changes in the phenotype (on the original scale) when a variant is present versus absent, holding other variants fixed. For any feature column with zero variance, the corresponding coefficient must be set to $0$.\n\nTest Suite:\n- Use the following explicit test cases with specified matrices and vectors. For each test case, the program must output a list of floats containing first the intercept (in the original phenotype scale), followed by the $p$ coefficients (in the original phenotype scale), in the order of variants as given. No angles are involved. No physical units need to be reported in the output; all outputs are pure numbers.\n\nCase $1$ (general case, moderate penalty):\n- $n = 10$, $p = 5$.\n- $X$ rows:\n    - $[1,0,1,0,1]$\n    - $[0,1,1,0,0]$\n    - $[1,1,0,0,0]$\n    - $[0,0,0,1,1]$\n    - $[1,0,0,1,0]$\n    - $[0,1,0,1,1]$\n    - $[1,0,1,1,0]$\n    - $[0,1,1,0,1]$\n    - $[1,1,0,1,0]$\n    - $[0,0,1,0,1]$\n- $y$:\n    - $[2.35,0.98,1.81,1.77,2.60,1.34,2.59,1.02,2.05,1.53]$\n- $\\lambda = 0.2$.\n\nCase $2$ (no penalty baseline, ordinary least squares behavior):\n- $n = 8$, $p = 3$.\n- $X$ rows:\n    - $[1,0,0]$\n    - $[0,1,0]$\n    - $[0,0,1]$\n    - $[1,1,0]$\n    - $[1,0,1]$\n    - $[0,1,1]$\n    - $[1,1,1]$\n    - $[0,0,0]$\n- $y$:\n    - $[1.52,-0.51,1.00,0.53,1.98,0.01,1.00,0.47]$\n- $\\lambda = 0.0$.\n\nCase $3$ (very strong penalty, coefficients shrink to zero, includes a zero-variance feature column):\n- $n = 6$, $p = 4$.\n- $X$ rows:\n    - $[1,0,0,0]$\n    - $[0,0,1,0]$\n    - $[1,0,1,0]$\n    - $[0,0,0,0]$\n    - $[1,0,0,0]$\n    - $[0,0,1,0]$\n- $y$:\n    - $[1.2,0.9,1.1,1.0,1.2,0.9]$\n- $\\lambda = 100.0$.\n\nCase $4$ (perfect collinearity between two features):\n- $n = 7$, $p = 3$.\n- $X$ rows:\n    - $[1,1,0]$\n    - $[0,0,1]$\n    - $[1,1,1]$\n    - $[0,0,0]$\n    - $[1,1,0]$\n    - $[0,0,1]$\n    - $[1,1,1]$\n- $y$:\n    - $[1.9,0.4,1.6,0.7,1.9,0.4,1.6]$\n- $\\lambda = 0.3$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of floats in the order $[b,\\beta_1,\\beta_2,\\dots,\\beta_p]$, where $b$ is the intercept and $\\beta_j$ are the coefficients. The single line must therefore look like a list of lists with no spaces, for example: $[[b_1,\\beta^{(1)}_1,\\dots],[b_2,\\beta^{(2)}_1,\\dots],\\dots]$.",
            "solution": "The problem is valid as it is scientifically grounded in quantitative genetics and statistical learning, well-posed with a convex objective function, and provides a complete and consistent setup for both the theoretical derivation and the computational implementation.\n\n### Task 1: Derivation of the LASSO Objective Function\n\n**1. The Linear Model**\n\nThe foundation of our analysis is the linear additive model, a cornerstone of quantitative genetics. It posits that the continuous phenotype $y_i$ for an isolate $i$ is a linear function of its genetic variants, plus an intercept and some error. The model is expressed as:\n$$y_i = b_0 + \\sum_{j=1}^{p} x_{ij} \\beta_j + \\epsilon_i$$\nwhere:\n- $y_i \\in \\mathbb{R}$ is the phenotype of isolate $i$.\n- $b_0 \\in \\mathbb{R}$ is the intercept, representing the baseline phenotype for an isolate with no variants of interest ($x_{ij}=0$ for all $j$).\n- $X \\in \\{0, 1\\}^{n \\times p}$ is the variant presence matrix, where $x_{ij}=1$ if variant $j$ is present in isolate $i$, and $x_{ij}=0$ otherwise.\n- $\\beta_j \\in \\mathbb{R}$ is the coefficient or effect size for variant $j$. It quantifies the change in phenotype associated with the presence of variant $j$.\n- $\\epsilon_i$ is the random error term for isolate $i$, capturing measurement noise and other unmodeled biological variation.\n\nIn matrix notation, for all $n$ isolates, the model is:\n$$y = b_0\\mathbf{1} + X\\beta + \\epsilon$$\nwhere $y \\in \\mathbb{R}^n$, $\\mathbf{1} \\in \\mathbb{R}^n$ is a vector of ones, $X \\in \\mathbb{R}^{n \\times p}$, $\\beta \\in \\mathbb{R}^p$, and $\\epsilon \\in \\mathbb{R}^n$.\n\n**2. Probabilistic Formulation and Maximum Likelihood**\n\nWe assume that the error terms $\\epsilon_i$ are independent and identically distributed (i.i.d.) following a Gaussian (normal) distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. This is a common and reasonable assumption for continuous biological measurements.\n\nThis assumption implies that the conditional probability of observing phenotype $y_i$ given the predictors is:\n$$P(y_i | x_i, b_0, \\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - b_0 - x_i^T\\beta)^2}{2\\sigma^2}\\right)$$\nThe likelihood of observing the entire phenotype vector $y$ given the matrix $X$ and parameters is the product of individual probabilities, due to the independence assumption:\n$$L(b_0, \\beta, \\sigma^2 | y, X) = \\prod_{i=1}^{n} P(y_i | x_i, b_0, \\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - b_0 - x_i^T\\beta)^2\\right)$$\nTo find the parameters, one can maximize this likelihood. It is equivalent and mathematically more convenient to minimize the negative log-likelihood:\n$$-\\log L = \\frac{n}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - b_0 - x_i^T\\beta)^2$$\nMinimizing this with respect to $b_0$ and $\\beta$ is equivalent to minimizing the Residual Sum of Squares (RSS), which is the principle of Ordinary Least Squares (OLS):\n$$\\text{RSS}(b_0, \\beta) = \\sum_{i=1}^{n} (y_i - b_0 - x_i^T\\beta)^2 = \\|y - b_0\\mathbf{1} - X\\beta\\|_2^2$$\n\n**3. Bayesian Interpretation and the Sparsity Prior**\n\nIn pathogen genomics, it is often the case that the number of potential variants $p$ is very large, possibly much larger than the number of isolates $n$. However, the Central Dogma, which states that genotype influences phenotype, does not imply all variants affect a given phenotype. It is biologically plausible to assume that only a small subset of variants are truly associated with the phenotype. This is the principle of *sparsity*.\n\nWe can formalize this assumption using a Bayesian framework by placing a prior distribution on the coefficients $\\beta_j$ that favors sparse solutions (i.e., many coefficients being exactly zero). A suitable sparsity-inducing prior is the **Laplace (or double-exponential) distribution**:\n$$P(\\beta_j | \\gamma) = \\frac{\\gamma}{2} \\exp(-\\gamma |\\beta_j|)$$\nThis distribution has a sharp peak at $\\beta_j=0$, assigning higher prior probability to coefficients being zero or close to zero compared to a Gaussian prior.\n\nAssuming the coefficients $\\beta_j$ are i.i.d. and placing a non-informative (uniform) prior on the intercept $b_0$, the joint prior on the parameters is $P(b_0, \\beta) \\propto \\prod_{j=1}^p P(\\beta_j)$.\n\n**4. Maximum A Posteriori (MAP) Estimation and the LASSO Objective**\n\nThe goal of MAP estimation is to find the parameter values that maximize the posterior distribution, given by Bayes' theorem:\n$$P(b_0, \\beta | y, X) \\propto L(b_0, \\beta | y, X) P(b_0, \\beta)$$\nMaximizing the posterior is equivalent to minimizing the negative log-posterior:\n$$\\underset{b_0, \\beta}{\\text{argmin}} \\left( -\\log L(b_0, \\beta | y, X) - \\log P(b_0, \\beta) \\right)$$\nSubstituting the expressions for the negative log-likelihood and the negative log-prior:\n$$-\\log L \\propto \\frac{1}{2\\sigma^2} \\|y - b_0\\mathbf{1} - X\\beta\\|_2^2$$\n$$-\\log P(b_0, \\beta) \\propto -\\sum_{j=1}^p \\log\\left(\\frac{\\gamma}{2} \\exp(-\\gamma |\\beta_j|)\\right) = -\\sum_{j=1}^p (\\log(\\gamma/2) - \\gamma|\\beta_j|) = \\text{const} + \\gamma \\sum_{j=1}^p |\\beta_j|$$\nCombining these terms, the MAP optimization problem becomes:\n$$\\underset{b_0, \\beta}{\\text{minimize}} \\left( \\frac{1}{2\\sigma^2} \\|y - b_0\\mathbf{1} - X\\beta\\|_2^2 + \\gamma \\sum_{j=1}^p |\\beta_j| \\right)$$\nBy defining a new penalty parameter $\\lambda = 2\\sigma^2\\gamma$, we arrive at the standard formulation for the **LASSO (Least Absolute Shrinkage and Selection Operator)** objective function:\n$$\\underset{b_0, \\beta}{\\text{minimize}} \\left( \\|y - b_0\\mathbf{1} - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 \\right)$$\nwhere $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$ is the $\\ell_1$-norm of the coefficient vector. The $\\ell_1$-norm penalty, derived from the Laplace prior, forces coefficients of weakly associated variants to become exactly zero, thus performing automated feature selection.\n\n**5. Justification for Not Penalizing the Intercept**\n\nThe intercept $b_0$ is systematically excluded from the $\\ell_1$ penalty for several crucial reasons:\n-   **Differing Roles:** The coefficients $\\beta_j$ represent the *effects* of the predictors (variants). The sparsity assumption applies to these effects, not to the baseline phenotype. The intercept $b_0$ represents the average phenotype when all predictors are zero ($x_{ij}=0$). There is no scientific reason to assume this baseline value should be zero or near zero.\n-   **Invariance to Origin Shift:** A key principle of modeling is that the estimation of effects should not depend on the origin chosen for the response variable. If we were to penalize $b_0$, shifting the phenotype by a constant ($y \\rightarrow y+c$) would result in a different solution for $\\beta$. By leaving $b_0$ unpenalized, the solution for $\\beta$ remains invariant under such shifts, correctly decoupling the estimation of effects from the estimation of the baseline mean.\n\n**6. Interpretation of Coefficients**\n\nIn the specified linear model with binary predictors $x_{ij} \\in \\{0, 1\\}$, each coefficient $\\beta_j$ has a direct and clear interpretation. It represents the estimated **additive effect size** of variant $j$ on the phenotype. Holding all other variants constant, the presence of variant $j$ ($x_{ij}=1$) is associated with a change of $\\beta_j$ in the phenotype value compared to its absence ($x_{ij}=0$).\n\n### Task 2: Computational Algorithm\n\nTo compute the intercept $b_0$ and coefficient vector $\\beta$, we will implement a coordinate descent algorithm to minimize the LASSO objective. It is standard practice to use a scaled version of the objective for numerical stability and to make the penalty parameter $\\lambda$ less dependent on the sample size $n$:\n$$\\underset{b_0, \\beta}{\\text{minimize}} \\left( \\frac{1}{2n} \\|y - b_0\\mathbf{1} - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 \\right)$$\n\nThe algorithm proceeds as follows:\n\n**1. Data Standardization and Intercept Handling**\nThe optimization is simplified by decoupling the intercept. If $b_0$ is unpenalized, its optimal value is given by $\\hat{b}_0 = \\bar{y} - \\bar{x}^T\\hat{\\beta}$, where $\\bar{y}$ is the mean of $y$ and $\\bar{x}$ is the vector of column means of $X$. Substituting this into the objective leads to an equivalent problem on centered and standardized variables.\n\n-   **Centering the Response:** The phenotype vector $y$ is centered by subtracting its mean: $y_c = y - \\bar{y}$.\n-   **Standardizing Features:** Each feature (column) $X_j$ of the matrix $X$ is standardized. First, we compute its mean $\\mu_j$ and population standard deviation $\\sigma_j = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\mu_j)^2}$.\n    -   If $\\sigma_j$ is effectively zero (i.e., the variant is absent or present in all isolates), the feature has zero variance. It provides no information for regression, and its coefficient $\\beta_j$ is set to $0$.\n    -   Otherwise, the feature is standardized to have a mean of $0$ and a standard deviation of $1$: $X'_j = (X_j - \\mu_j) / \\sigma_j$.\n-   Let $X'$ be the matrix of standardized non-constant features, and $\\beta'$ be the corresponding coefficient vector. The optimization problem reduces to:\n    $$\\underset{\\beta'}{\\text{minimize}} \\left( \\frac{1}{2n} \\|y_c - X'\\beta'\\|_2^2 + \\lambda \\|\\beta'\\|_1 \\right)$$\n\n**2. Coordinate Descent**\nThis iterative algorithm optimizes the objective one coefficient at a time, holding all others fixed.\n\n-   Initialize the coefficient vector for standardized features: $\\beta' = \\mathbf{0}$.\n-   Cycle through the coordinates $j = 1, \\dots, p'$ (where $p'$ is the number of non-constant features) repeatedly until convergence:\n    -   For each coordinate $j$, find the value of $\\beta'_j$ that minimizes the objective, assuming all other coefficients $\\beta'_k$ ($k \\neq j$) are fixed.\n    -   This one-dimensional subproblem has a closed-form solution given by the **soft-thresholding operator**, $S$:\n        $$\\beta'_j \\leftarrow S(z_j, \\lambda) = \\text{sign}(z_j) \\cdot \\max(|z_j| - \\lambda, 0)$$\n        where $z_j = \\frac{1}{n} (X'_j)^T r_j$ and $r_j = y_c - \\sum_{k \\neq j} X'_k \\beta'_k$ is the partial residual. A computationally efficient way to calculate $z_j$ within the loop is $z_j = \\frac{1}{n} (X'_j)^T(y_c - X'\\beta') + \\beta'_{j,\\text{old}}$. The loop updates $\\beta'$ in-place.\n-   Convergence is declared when the maximum absolute change in any coefficient between iterations falls below a small tolerance (e.g., $10^{-8}$).\n\n**3. Un-standardization**\nAfter the algorithm converges to the optimal $\\hat{\\beta}'$, the coefficients must be transformed back to the original scale of $X$.\n\n-   The coefficients for the original (unstandardized) features are calculated as: $\\hat{\\beta}_j = \\hat{\\beta}'_j / \\sigma_j$ for non-constant features. For constant features, $\\hat{\\beta}_j=0$.\n-   The intercept is then calculated as: $\\hat{b}_0 = \\bar{y} - \\sum_{j=1}^p \\mu_j \\hat{\\beta}_j = \\bar{y} - \\mu_X^T\\hat{\\beta}$.\n\nThis procedure yields the final intercept and coefficient vector that solve the LASSO problem on the original data, with an unpenalized intercept and handling of zero-variance features as required.",
            "answer": "[[1.87114661,0.58983995,-0.50970002,0.00000000,0.22056976,0.00000000],[0.47000000,1.05000000,-0.98000000,0.53000000],[1.05000000,0.00000000,0.00000000,0.00000000,0.00000000],[1.03333333,0.30000000,0.30000000,-0.30000000]]"
        }
    ]
}