{
    "hands_on_practices": [
        {
            "introduction": "多重测序技术在病原体基因组学中极为普遍，但它也带来了样本间污染的风险，即所谓的“指数错配”（index-cross contamination）。这种污染可能导致错误的系统发育和流行病学推断。本练习将指导您基于基本统计原理，设计一个复合检验来识别被污染的样本，这对于确保下游分析的可靠性至关重要。",
            "id": "4667657",
            "problem": "您将设计并实现一个统计检验，该检验利用重复读段模式和覆盖度均匀性来检测病原体基因组学中多重样本间的索引交叉污染。背景是病原体基因组的多重Illumina测序，其中索引交叉污染（也称为索引跳换）可能导致一个样本中的一部分读段被错误地标记为属于另一个样本。推导的基本依据必须始于以下经过充分检验的事实：(i) 在独立抽样下，读段大致均匀地映射到整个基因组窗口；(ii) 重复读段簇（具有完全相同的起始位点和方向）源于聚合酶链式反应扩增和随机重采样；(iii) 在一个简单的独立抽样零模型下，不相交基因组窗口中的计数可以由泊松过程近似。任务是形式化一个零模型和一个复合检验，该检验联合评估意外的跨样本重复读段重叠和窗口化覆盖度离散度，然后实现此检验以判断哪些样本表现出索引交叉污染。\n\n用作基础的定义和假设：\n- 考虑一组由 $i \\in \\{1,2,\\dots,S\\}$ 索引的样本，每个样本都在 $W$ 个不重叠的基因组窗口上进行测序。设窗口 $w$ 的长度为 $L_w$ 个碱基对。\n- 对于样本 $i$ 和窗口 $w$，设 $D_i(w)$ 为重复读段簇（具有相同映射位置和方向的读段）的数量，设 $U_i(w)$ 为窗口中唯一映射读段的数量。\n- 对于一对样本 $(i,j)$ 和窗口 $w$，设 $O_{ij}(w)$ 表示在窗口 $w$ 内，在样本 $i$ 和 $j$ 中于完全相同的基因组坐标上观察到的重复读段簇的数量。\n- 在窗口 $w$ 尺度上的独立性和均匀性假设下，样本 $i$ 和样本 $j$ 的重复读段簇位置近似均匀分布在 $L_w$ 个可能的起始位置中，这导致零假设下的期望跨样本重叠率非常小。在独立抽样下，窗口化唯一覆盖度遵循一个泊松过程，其均值与测序深度成正比，这意味着如果该过程能被泊松抽样很好地近似，那么跨窗口的方差应接近于均值。\n\n您的任务：\n- 从上述基本原理出发，推导一个针对样本 $i$ 和 $j$ 之间意外跨样本重复读段重叠的单边统计检验，该检验在所有窗口 $w \\in \\{1,\\dots,W\\}$ 上进行聚合。检验必须将观察到的总重叠 $\\sum_w O_{ij}(w)$ 与基于窗口长度 $L_w$ 的独立性和均匀性假设下的零期望进行比较，并产生一个定量的显著性值。\n- 为每个样本 $i$ 的窗口化覆盖度 $U_i(w)$ 推导一个过度离散的单边统计检验，该检验基于在没有污染的情况下窗口计数遵循泊松过程的零模型。\n- 为每个样本 $i$ 指定一种有原则的方法，将两个显著性值组合成一个用于污染检测的单一决策规则，并以显著性阈值 $\\alpha$ 的形式陈述决策标准，其中 $\\alpha$ 必须设置为 $0.05$。\n- 将推导出的检验和决策规则实现为一个完整的程序，用于评估一个给定的测试套件。对于每个测试用例，该程序必须计算一个布尔值列表，为每个样本指示根据您的决策规则是否检测到索引交叉污染。\n\n测试套件：\n- 共有三个测试用例。每个测试用例提供两个样本（$S=2$）和多个窗口，参数如下。所有数字都是计数，必须被视为无量纲整数，所有窗口长度 $L_w$ 的单位都是碱基对。\n\n测试用例1（预期样本A受索引交叉污染影响）：\n- 窗口数 $W=5$，长度 $L_w = [1000,1000,1000,1000,1000]$。\n- 样本A：重复读段计数 $D_A(w) = [40,42,38,41,39]$，唯一覆盖度 $U_A(w) = [820,910,700,880,790]$。\n- 样本B：重复读段计数 $D_B(w) = [60,58,61,59,62]$，唯一覆盖度 $U_B(w) = [800,805,795,810,790]$。\n- 观察到的跨样本重复读段重叠 $O_{AB}(w) = [6,5,7,6,5]$。\n\n测试用例2（预期无污染）：\n- 窗口数 $W=5$，长度 $L_w = [1000,1000,1000,1000,1000]$。\n- 样本A：重复读段计数 $D_A(w) = [30,29,31,30,32]$，唯一覆盖度 $U_A(w) = [500,505,495,510,490]$。\n- 样本B：重复读段计数 $D_B(w) = [45,44,46,47,43]$，唯一覆盖度 $U_B(w) = [520,515,525,510,530]$。\n- 观察到的跨样本重复读段重叠 $O_{AB}(w) = [1,2,1,2,1]$。\n\n测试用例3（小窗口和轻度重叠的边界情况）：\n- 窗口数 $W=4$，长度 $L_w = [200,200,200,200]$。\n- 样本A：重复读段计数 $D_A(w) = [20,19,21,22]$，唯一覆盖度 $U_A(w) = [400,420,380,410]$。\n- 样本B：重复读段计数 $D_B(w) = [25,24,26,23]$，唯一覆盖度 $U_B(w) = [410,405,415,400]$。\n- 观察到的跨样本重复读段重叠 $O_{AB}(w) = [3,3,2,3]$。\n\n实现要求：\n- 您的程序必须硬编码上述测试套件，并使用显著性阈值 $\\alpha = 0.05$ 产生结果。\n- 您的决策规则必须由两个推导出的检验（重复读段重叠升高和覆盖度过度离散）构建，并且必须为每个样本输出一个指示污染检测的布尔值。\n- 最终输出格式：您的程序应生成单行输出，其中包含三个测试用例的结果，形式为一个用方括号括起来的逗号分隔列表，其中每个元素本身是该测试用例中各样本的布尔值列表。例如，输出应类似于“[[true_case1_sampleA,true_case1_sampleB],[...],[...]]”，但使用Python布尔字面量。具体来说，该行必须是“[[b1,b2],[b3,b4],[b5,b6]]”的形式，其中 $b_k$ 是布尔值。\n\n角度单位不适用。答案中没有物理单位。在程序逻辑中将所有显著性值隐式地表示为小数；最终输出仅包含布尔值。",
            "solution": "该问题要求设计并实现一个复合统计检验，用于检测多重测序数据中的索引交叉污染。解决方案根据问题要求分三个阶段展开：首先，推导一个用于检测样本对之间异常重复读段重叠的检验；其次，推导一个用于检测每个样本内覆盖度过度离散的检验；第三，制定一个结合这些检验以识别受污染样本的决策规则。所有数学实体均按要求以LaTeX格式呈现。\n\n### 第1部分：重复读段重叠检验\n\n第一个组成部分是一个统计检验，用于检测两个样本（例如样本 $i$ 和样本 $j$）之间共享的重复读段簇位置数量是否异常高。\n\n**零假设 ($H_{0,overlap}$)**：来自样本 $i$ 和样本 $j$ 的重复读段簇的基因组位置是独立的。在任何给定长度为 $L_w$ 的窗口 $w$ 内，假定这些位置在 $L_w$ 个可能的起始位点中均匀分布。\n\n**推导**：\n考虑一个长度为 $L_w$ 的单个窗口 $w$。设 $D_i(w)$ 为样本 $i$ 的重复读段簇数量，$D_j(w)$ 为样本 $j$ 的重复读段簇数量。在零假设下，这些簇的位置是从 $L_w$ 个位置集合中进行的独立抽取。\n\n来自样本 $i$ 的一个特定重复读段簇与来自样本 $j$ 的一个特定重复读段簇映射到相同起始位置的概率是 $1/L_w$。由于样本 $j$ 中有 $D_j(w)$ 个簇，因此样本 $i$ 的一个特定簇与样本 $j$ 中*任意*一个簇重叠的概率是 $p_{ij}(w) = D_j(w)/L_w$。这个推导假设 $D_j(w) \\ll L_w$ 且来自样本 $j$ 的多个簇不太可能占据同一位点，这是一个合理的近似。\n\n在窗口 $w$ 中观察到的重叠数 $O_{ij}(w)$ 可以建模为一个二项随机变量，代表 $D_i(w)$ 次试验（来自样本 $i$ 的簇），每次试验的成功概率为 $p_{ij}(w)$：\n$$ O_{ij}(w) \\sim \\text{Binomial}(D_i(w), p_{ij}(w)) $$\n窗口 $w$ 中的期望重叠数是 $E[O_{ij}(w)] = D_i(w) p_{ij}(w) = \\frac{D_i(w) D_j(w)}{L_w}$。\n\n由于成功概率 $p_{ij}(w)$ 通常非常小，而 $D_i(w)$ 可能很大，这个二项分布可以很好地用具有相同均值的泊松分布来近似：\n$$ O_{ij}(w) \\sim \\text{Poisson}\\left(\\lambda_{ij}(w)\\right) \\quad \\text{where} \\quad \\lambda_{ij}(w) = \\frac{D_i(w) D_j(w)}{L_w} $$\n\n问题要求一个在所有 $W$ 个窗口上聚合的检验。设 $O_{ij}^{\\text{total}} = \\sum_{w=1}^{W} O_{ij}(w)$ 为观察到的总重叠计数。由于独立泊松随机变量的和也是一个泊松随机变量，因此在零假设下，总重叠计数遵循一个泊松分布，其率参数等于各个窗口率的总和：\n$$ O_{ij}^{\\text{total}} \\sim \\text{Poisson}\\left(\\lambda_{ij}^{\\text{total}}\\right) \\quad \\text{where} \\quad \\lambda_{ij}^{\\text{total}} = \\sum_{w=1}^{W} \\frac{D_i(w) D_j(w)}{L_w} $$\n\n**统计检验**：\n这是一个用于检验重叠是否过多的单边检验。\n- **检验统计量**：观察到的总重叠，$O_{obs} = \\sum_{w=1}^{W} O_{ij}(w)$。\n- **零分布**：率参数为 $\\lambda_{0} = \\sum_{w=1}^{W} \\frac{D_i(w) D_j(w)}{L_w}$ 的泊松分布。\n- **P值 ($p_{overlap}$)**：随机观察到 $O_{obs}$ 或更多重叠的概率是使用泊松分布的生存函数（互补累积分布函数）计算的：\n  $$ p_{overlap, ij} = P(X \\ge O_{obs} | X \\sim \\text{Poisson}(\\lambda_{0})) = \\sum_{k=O_{obs}}^{\\infty} \\frac{e^{-\\lambda_{0}} \\lambda_{0}^k}{k!} $$\n\n### 第2部分：覆盖度离散检验\n\n第二个组成部分是针对每个样本的窗口化唯一读段计数的过度离散检验，这可能是由污染引起的非均匀覆盖的迹象。\n\n**零假设 ($H_{0,dispersion}$)**：对于给定样本 $i$，在 $W$ 个等长窗口中的唯一读段计数 $U_i(w)$ 是来自单个泊松分布 $U_i(w) \\sim \\text{Poisson}(\\mu_i)$ 的独立同分布抽取。这基于问题的前提，即在均匀抽样下，计数遵循泊松过程。\n\n**推导**：\n泊松分布的一个关键性质是其方差等于其均值（$\\sigma^2 = \\mu$）。当观测方差显著大于观测均值时，就会发生过度离散。对此的检验基于泊松离散检验或卡方拟合优度检验。\n\n给定一组计数 $\\{U_i(1), U_i(2), \\dots, U_i(W)\\}$，我们首先用样本均值 $\\hat{\\mu}_i = \\frac{1}{W}\\sum_{w=1}^{W} U_i(w)$ 来估计共同均值 $\\mu_i$。检验统计量是离散指数乘以自由度：\n$$ \\chi^2_i = \\sum_{w=1}^{W} \\frac{(U_i(w) - \\hat{\\mu}_i)^2}{\\hat{\\mu}_i} $$\n这个统计量也可以用样本方差 $\\hat{\\sigma}^2_i = \\frac{1}{W-1}\\sum_{w=1}^{W} (U_i(w) - \\hat{\\mu}_i)^2$ 来表示：\n$$ \\chi^2_i = (W-1)\\frac{\\hat{\\sigma}^2_i}{\\hat{\\mu}_i} $$\n在零假设下，该统计量近似遵循一个具有 $W-1$ 个自由度的卡方分布。\n\n**统计检验**：\n这是一个用于过度离散（方差大于均值）的单边检验。\n- **检验统计量**：$\\chi^2_i = (W-1)\\frac{\\hat{\\sigma}^2_i}{\\hat{\\mu}_i}$。\n- **零分布**：一个自由度为 $df = W-1$ 的卡方分布。\n- **P值 ($p_{dispersion}$)**：随机观察到等于或大于 $\\chi^2_i$ 值的概率是使用 $\\chi^2$ 分布的生存函数计算的：\n  $$ p_{dispersion, i} = P(X \\ge \\chi^2_i | X \\sim \\chi^2(W-1)) $$\n该检验对每个样本 $i$ 独立进行。\n\n### 第3部分：组合决策规则\n\n最后一步是将这两个检验组合成一个单一的决策规则，以判定一个样本是否被污染。显著性水平固定为 $\\alpha = 0.05$。\n\n**基本原理**：\n从来源样本 $j$ 到接收样本 $i$ 的索引交叉污染会以两种方式表现出来：\n1.  $i$ 和 $j$ 之间共享的重复读段簇位置数量异常高，导致重叠检验结果显著（$p_{overlap, ij}  \\alpha$）。\n2.  非均匀的读段流入样本 $i$，破坏其覆盖均匀性并导致过度离散。这导致离散检验结果显著（$p_{dispersion, i}  \\alpha$）。\n\n重叠检验是对称的（$p_{overlap, ij} = p_{overlap, ji}$），它指示了一对样本*之间*存在问题，但没有指明污染的方向。离散检验是样本特异性的，用于识别污染的接收者。表现出过度离散的样本是成为接收者的候选者。\n\n因此，一个有原则的决策规则是：当且仅当样本 $i$ 的覆盖度显著过度离散，并且它与至少一个其他样本共享显著高的重复读段重叠时，才将样本 $i$ 标记为受污染。这个合取规则利用了两种信号来进行高置信度的判断。\n\n**决策规则**：\n对于一组样本中的任何给定样本 $i$，如果满足以下条件，则判定其为受污染：\n$$ p_{dispersion, i}  \\alpha \\quad \\text{and} \\quad (\\exists j \\neq i \\text{ such that } p_{overlap, ij}  \\alpha) $$\n考虑到问题设置为两个样本 A 和 B，规则变为：\n- 样本 A 被污染，如果：($p_{dispersion, A}  0.05$) 且 ($p_{overlap, AB}  0.05$)。\n- 样本 B 被污染，如果：($p_{dispersion, B}  0.05$) 且 ($p_{overlap, AB}  0.05$)。\n\n此框架被实现用于评估所提供的测试用例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson, chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the contamination detection analysis on all test cases\n    and print the results in the specified format.\n    \"\"\"\n    test_cases = [\n        # Test Case 1: Expected contamination in Sample A\n        {\n            \"W\": 5, \"Lw\": [1000] * 5,\n            \"DA\": [40, 42, 38, 41, 39], \"UA\": [820, 910, 700, 880, 790],\n            \"DB\": [60, 58, 61, 59, 62], \"UB\": [800, 805, 795, 810, 790],\n            \"OAB\": [6, 5, 7, 6, 5]\n        },\n        # Test Case 2: Expected no contamination\n        {\n            \"W\": 5, \"Lw\": [1000] * 5,\n            \"DA\": [30, 29, 31, 30, 32], \"UA\": [500, 505, 495, 510, 490],\n            \"DB\": [45, 44, 46, 47, 43], \"UB\": [520, 515, 525, 510, 530],\n            \"OAB\": [1, 2, 1, 2, 1]\n        },\n        # Test Case 3: Boundary case\n        {\n            \"W\": 4, \"Lw\": [200] * 4,\n            \"DA\": [20, 19, 21, 22], \"UA\": [400, 420, 380, 410],\n            \"DB\": [25, 24, 26, 23], \"UB\": [410, 405, 415, 400],\n            \"OAB\": [3, 3, 2, 3]\n        }\n    ]\n\n    alpha = 0.05\n    results = []\n\n    for case in test_cases:\n        # Unpack data for the current test case.\n        W = case[\"W\"]\n        Lw = np.array(case[\"Lw\"])\n        DA = np.array(case[\"DA\"])\n        UA = np.array(case[\"UA\"])\n        DB = np.array(case[\"DB\"])\n        UB = np.array(case[\"UB\"])\n        OAB = np.array(case[\"OAB\"])\n\n        # 1. Duplicate Overlap Test (between Sample A and B)\n        # Calculate expected total overlap under the null hypothesis.\n        lambda_overlap = np.sum(DA * DB / Lw)\n        # Get observed total overlap.\n        observed_overlap = np.sum(OAB)\n        # Calculate one-sided p-value. For a discrete distribution like Poisson,\n        # P(X >= k) = survival_function(k - 1).\n        p_overlap_ab = poisson.sf(observed_overlap - 1, lambda_overlap)\n\n        # 2. Coverage Dispersion Test (for each sample)\n        # --- For Sample A ---\n        # Calculate chi-squared statistic for dispersion.\n        # ddof=1 for sample variance.\n        mu_a = np.mean(UA)\n        var_a = np.var(UA, ddof=1)\n        # Handle case where mean is zero to avoid division by zero.\n        chi_squared_a = (W - 1) * var_a / mu_a if mu_a > 0 else 0\n        degrees_freedom = W - 1\n        # Calculate one-sided p-value.\n        p_dispersion_a = chi2.sf(chi_squared_a, degrees_freedom)\n\n        # --- For Sample B ---\n        # Calculate chi-squared statistic for dispersion.\n        mu_b = np.mean(UB)\n        var_b = np.var(UB, ddof=1)\n        chi_squared_b = (W - 1) * var_b / mu_b if mu_b > 0 else 0\n        # Calculate one-sided p-value.\n        p_dispersion_b = chi2.sf(chi_squared_b, degrees_freedom)\n\n        # 3. Combined Decision Rule\n        # A sample is contaminated if its coverage is overdispersed AND\n        # it has significant duplicate overlap with another sample.\n        is_contaminated_a = (p_dispersion_a  alpha) and (p_overlap_ab  alpha)\n        is_contaminated_b = (p_dispersion_b  alpha) and (p_overlap_ab  alpha)\n\n        results.append([is_contaminated_a, is_contaminated_b])\n\n    # Format the final output string as specified.\n    # Ex: [[True,False],[False,False],[False,False]]\n    inner_parts = []\n    for R_A, R_B in results:\n        inner_parts.append(f\"[{R_A},{R_B}]\")\n    \n    print(f\"[{','.join(inner_parts)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "确定耐药性或毒力等关键表型的遗传基础是分子流行病学的核心任务之一。本练习将介绍如何使用 LASSO（最小绝对收缩和选择算子）这一强大的惩罚性回归方法，从大量的基因变异中筛选出与特定表型相关的候选基因。通过这个练习，您将掌握在基因型-表型关联研究中应用和解释机器学习模型的实践技能。",
            "id": "4667796",
            "problem": "考虑一个传染病分子流行病学场景，其中在一个病原体基因组队列中测量了一个连续表型，例如抗生素的最低抑菌浓度 (MIC) 的以 2 为底的对数。设 $n$ 表示分离株的数量，$p$ 表示遗传变异的数量，例如单核苷酸多态性 (SNPs)。变异存在矩阵为 $X \\in \\{0,1\\}^{n \\times p}$，其中元素 $x_{ij}$ 表示分离株 $i$ 中是否存在变异 $j$（存在为 $x_{ij} = 1$，不存在为 $x_{ij} = 0$）。表型向量为 $y \\in \\mathbb{R}^n$。假设一个带有截距和加性噪声的线性加性基因型-表型映射。\n\n从适用于病原体基因组学的基本原理出发：\n- 使用分子生物学的中心法则作为基因型通过分子机制影响表型的概念基础。\n- 假设变异对表型的贡献是加性的，这基于广泛接受的数量遗传学推理。\n- 假设测量噪声为高斯噪声，这与连续性状的常见建模假设一致。\n\n任务 1 (推导)：\n- 推导带有 $\\ell_1$范数惩罚的惩罚回归目标，即最小绝对收缩和选择算子 (LASSO)，不要提供任何快捷公式。从带有高斯噪声的线性模型和对变异效应大小的稀疏性诱导先验开始，该先验体现了偏好更简单模型的理由。通过概率或优化论证明确证明惩罚项选择的合理性，并论证为什么在这种情况下截距不能被惩罚。解释每个系数在表型尺度上对于二元变异存在变量的加性效应大小。\n\n任务 2 (计算)：\n- 实现一个程序，在给定 $X$、$y$ 和一个非负惩罚参数 $\\lambda$ 的情况下，计算使推导出的惩罚目标最小化的截距和系数向量。为了数值稳定性和可解释性，必须对特征进行标准化：当 $X$ 的每一列方差不为零时，对其进行中心化并缩放到单位方差；不要惩罚截距。在标准化空间中进行优化后，将解映射回原始特征尺度，以便系数可以解释为当一个变异存在与不存在时，在保持其他变异固定的情况下，表型（在原始尺度上）的加性变化。对于任何方差为零的特征列，其对应的系数必须设置为 $0$。\n\n测试套件：\n- 使用以下带有指定矩阵和向量的显式测试用例。对于每个测试用例，程序必须输出一个浮点数列表，首先包含截距（在原始表型尺度上），然后是 $p$ 个系数（在原始表型尺度上），顺序与给定的变异顺序一致。不涉及角度。输出中无需报告物理单位；所有输出均为纯数字。\n\n情况 1（一般情况，中等惩罚）：\n- $n = 10$, $p = 5$。\n- $X$ 的行：\n    - $[1,0,1,0,1]$\n    - $[0,1,1,0,0]$\n    - $[1,1,0,0,0]$\n    - $[0,0,0,1,1]$\n    - $[1,0,0,1,0]$\n    - $[0,1,0,1,1]$\n    - $[1,0,1,1,0]$\n    - $[0,1,1,0,1]$\n    - $[1,1,0,1,0]$\n    - $[0,0,1,0,1]$\n- $y$:\n    - $[2.35,0.98,1.81,1.77,2.60,1.34,2.59,1.02,2.05,1.53]$\n- $\\lambda = 0.2$。\n\n情况 2（无惩罚基线，普通最小二乘行为）：\n- $n = 8$, $p = 3$。\n- $X$ 的行：\n    - $[1,0,0]$\n    - $[0,1,0]$\n    - $[0,0,1]$\n    - $[1,1,0]$\n    - $[1,0,1]$\n    - $[0,1,1]$\n    - $[1,1,1]$\n    - $[0,0,0]$\n- $y$:\n    - $[1.52,-0.51,1.00,0.53,1.98,0.01,1.00,0.47]$\n- $\\lambda = 0.0$。\n\n情况 3（极强惩罚，系数收缩至零，包含一个零方差特征列）：\n- $n = 6$, $p = 4$。\n- $X$ 的行：\n    - $[1,0,0,0]$\n    - $[0,0,1,0]$\n    - $[1,0,1,0]$\n    - $[0,0,0,0]$\n    - $[1,0,0,0]$\n    - $[0,0,1,0]$\n- $y$:\n    - $[1.2,0.9,1.1,1.0,1.2,0.9]$\n- $\\lambda = 100.0$。\n\n情况 4（两个特征之间存在完全共线性）：\n- $n = 7$, $p = 3$。\n- $X$ 的行：\n    - $[1,1,0]$\n    - $[0,0,1]$\n    - $[1,1,1]$\n    - $[0,0,0]$\n    - $[1,1,0]$\n    - $[0,0,1]$\n    - $[1,1,1]$\n- $y$:\n    - $[1.9,0.4,1.6,0.7,1.9,0.4,1.6]$\n- $\\lambda = 0.3$。\n\n要求的最终输出格式：\n- 你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果本身必须是一个浮点数列表，顺序为 $[b,\\beta_1,\\beta_2,\\dots,\\beta_p]$，其中 $b$ 是截距，$\\beta_j$ 是系数。因此，该单行输出必须看起来像一个没有空格的列表的列表，例如：$[[b_1,\\beta^{(1)}_1,\\dots],[b_2,\\beta^{(2)}_1,\\dots],\\dots]$。",
            "solution": "该问题是有效的，因为它在数量遗传学和统计学习方面有科学依据，具有凸目标函数因而问题是适定的，并为理论推导和计算实现提供了完整且一致的设置。\n\n### 任务 1：LASSO 目标函数的推导\n\n**1. 线性模型**\n\n我们分析的基础是线性加性模型，这是数量遗传学的一块基石。该模型假定分离株 $i$ 的连续表型 $y_i$ 是其遗传变异的线性函数，外加一个截距和一些误差。该模型表示为：\n$$y_i = b_0 + \\sum_{j=1}^{p} x_{ij} \\beta_j + \\epsilon_i$$\n其中：\n- $y_i \\in \\mathbb{R}$ 是分离株 $i$ 的表型。\n- $b_0 \\in \\mathbb{R}$ 是截距，表示没有目标变异（对所有 $j$，$x_{ij}=0$）的分离株的基线表型。\n- $X \\in \\{0, 1\\}^{n \\times p}$ 是变异存在矩阵，其中如果变异 $j$ 存在于分离株 $i$ 中，则 $x_{ij}=1$，否则 $x_{ij}=0$。\n- $\\beta_j \\in \\mathbb{R}$ 是变异 $j$ 的系数或效应大小。它量化了与变异 $j$ 存在相关的表型变化。\n- $\\epsilon_i$ 是分离株 $i$ 的随机误差项，捕捉了测量噪声和其他未建模的生物学变异。\n\n对于所有 $n$ 个分离株，用矩阵表示法，模型为：\n$$y = b_0\\mathbf{1} + X\\beta + \\epsilon$$\n其中 $y \\in \\mathbb{R}^n$，$\\mathbf{1} \\in \\mathbb{R}^n$ 是一个全为 1 的向量，$X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^p$ 以及 $\\epsilon \\in \\mathbb{R}^n$。\n\n**2. 概率公式和最大似然**\n\n我们假设误差项 $\\epsilon_i$ 是独立同分布 (i.i.d.) 的，服从均值为 $0$、方差为 $\\sigma^2$ 的高斯（正态）分布，即 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。对于连续的生物学测量，这是一个常见且合理的假设。\n\n这个假设意味着在给定预测变量的情况下，观测到表型 $y_i$ 的条件概率是：\n$$P(y_i | x_i, b_0, \\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - b_0 - x_i^T\\beta)^2}{2\\sigma^2}\\right)$$\n由于独立性假设，在给定矩阵 $X$ 和参数的情况下，观测到整个表型向量 $y$ 的似然是各个概率的乘积：\n$$L(b_0, \\beta, \\sigma^2 | y, X) = \\prod_{i=1}^{n} P(y_i | x_i, b_0, \\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - b_0 - x_i^T\\beta)^2\\right)$$\n为了找到参数，可以最大化这个似然。最小化负对数似然是等价的，并且在数学上更方便：\n$$-\\log L = \\frac{n}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - b_0 - x_i^T\\beta)^2$$\n相对于 $b_0$ 和 $\\beta$ 最小化这个表达式，等同于最小化残差平方和 (RSS)，这就是普通最小二乘法 (OLS) 的原理：\n$$\\text{RSS}(b_0, \\beta) = \\sum_{i=1}^{n} (y_i - b_0 - x_i^T\\beta)^2 = \\|y - b_0\\mathbf{1} - X\\beta\\|_2^2$$\n\n**3. 贝叶斯解释和稀疏性先验**\n\n在病原体基因组学中，潜在变异的数量 $p$ 通常非常大，可能远大于分离株的数量 $n$。然而，阐明基因型影响表型的中心法则并不意味着所有变异都会影响给定的表型。假设只有一个小子集的变异与表型真正相关，这在生物学上是合理的。这就是*稀疏性*原理。\n\n我们可以通过在系数 $\\beta_j$ 上放置一个偏好稀疏解（即许多系数恰好为零）的先验分布，在贝叶斯框架下将这一假设形式化。一个合适的稀疏性诱导先验是**拉普拉斯（或双指数）分布**：\n$$P(\\beta_j | \\gamma) = \\frac{\\gamma}{2} \\exp(-\\gamma |\\beta_j|)$$\n与高斯先验相比，该分布在 $\\beta_j=0$ 处有一个尖峰，为系数为零或接近零赋予了更高的先验概率。\n\n假设系数 $\\beta_j$ 是独立同分布的，并对截距 $b_0$ 放置一个无信息（均匀）先验，则参数的联合先验为 $P(b_0, \\beta) \\propto \\prod_{j=1}^p P(\\beta_j)$。\n\n**4. 最大后验 (MAP) 估计和 LASSO 目标**\n\nMAP 估计的目标是找到使后验分布最大化的参数值，后验分布由贝叶斯定理给出：\n$$P(b_0, \\beta | y, X) \\propto L(b_0, \\beta | y, X) P(b_0, \\beta)$$\n最大化后验等价于最小化负对数后验：\n$$\\underset{b_0, \\beta}{\\text{argmin}} \\left( -\\log L(b_0, \\beta | y, X) - \\log P(b_0, \\beta) \\right)$$\n代入负对数似然和负对数先验的表达式：\n$$-\\log L \\propto \\frac{1}{2\\sigma^2} \\|y - b_0\\mathbf{1} - X\\beta\\|_2^2$$\n$$-\\log P(b_0, \\beta) \\propto -\\sum_{j=1}^p \\log\\left(\\frac{\\gamma}{2} \\exp(-\\gamma |\\beta_j|)\\right) = -\\sum_{j=1}^p (\\log(\\gamma/2) - \\gamma|\\beta_j|) = \\text{const} + \\gamma \\sum_{j=1}^p |\\beta_j|$$\n结合这些项，MAP 优化问题变为：\n$$\\underset{b_0, \\beta}{\\text{minimize}} \\left( \\frac{1}{2\\sigma^2} \\|y - b_0\\mathbf{1} - X\\beta\\|_2^2 + \\gamma \\sum_{j=1}^p |\\beta_j| \\right)$$\n通过定义一个新的惩罚参数 $\\lambda = 2\\sigma^2\\gamma$，我们得到了 **LASSO (最小绝对收缩和选择算子)** 目标函数的标准形式：\n$$\\underset{b_0, \\beta}{\\text{minimize}} \\left( \\|y - b_0\\mathbf{1} - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 \\right)$$\n其中 $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$ 是系数向量的 $\\ell_1$范数。从拉普拉斯先验导出的 $\\ell_1$范数惩罚会迫使弱相关变异的系数变为恰好为零，从而实现自动化特征选择。\n\n**5. 不惩罚截距的理由**\n\n截距 $b_0$ 被系统地从 $\\ell_1$ 惩罚中排除，有几个关键原因：\n-   **角色不同：** 系数 $\\beta_j$ 代表预测变量（变异）的*效应*。稀疏性假设适用于这些效应，而不适用于基线表型。截距 $b_0$ 代表所有预测变量为零（$x_{ij}=0$）时的平均表型。没有科学理由假设这个基线值应该为零或接近零。\n-   **对原点平移的不变性：** 建模的一个关键原则是，效应的估计不应依赖于为响应变量选择的原点。如果我们惩罚 $b_0$，将表型平移一个常数（$y \\rightarrow y+c$）会导致 $\\beta$ 的解不同。通过不惩罚 $b_0$，$\\beta$ 的解在这种平移下保持不变，从而正确地将效应的估计与基线均值的估计解耦。\n\n**6. 系数的解释**\n\n在指定的带有二元预测变量 $x_{ij} \\in \\{0, 1\\}$ 的线性模型中，每个系数 $\\beta_j$ 都有一个直接而清晰的解释。它代表变异 $j$ 对表型的估计**加性效应大小**。在保持所有其他变异恒定的情况下，与不存在变异 $j$ ($x_{ij}=0$) 相比，变异 $j$ 的存在 ($x_{ij}=1$) 与表型值变化 $\\beta_j$ 相关。\n\n### 任务 2：计算算法\n\n为了计算截距 $b_0$ 和系数向量 $\\beta$，我们将实现一个坐标下降算法来最小化 LASSO 目标。标准做法是使用目标的缩放版本以保证数值稳定性，并使惩罚参数 $\\lambda$ 对样本大小 $n$ 的依赖性降低：\n$$\\underset{b_0, \\beta}{\\text{minimize}} \\left( \\frac{1}{2n} \\|y - b_0\\mathbf{1} - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 \\right)$$\n\n该算法按以下步骤进行：\n\n**1. 数据标准化和截距处理**\n通过解耦截距可以简化优化过程。如果 $b_0$ 不被惩罚，其最优值由 $\\hat{b}_0 = \\bar{y} - \\bar{x}^T\\hat{\\beta}$ 给出，其中 $\\bar{y}$ 是 $y$ 的均值，$\\bar{x}$ 是 $X$ 的列均值向量。将此代入目标函数，会得到一个关于中心化和标准化变量的等价问题。\n\n-   **中心化响应变量：** 通过减去其均值来中心化表型向量 $y$：$y_c = y - \\bar{y}$。\n-   **标准化特征：** 对矩阵 $X$ 的每个特征（列）$X_j$ 进行标准化。首先，我们计算其均值 $\\mu_j$ 和总体标准差 $\\sigma_j = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\mu_j)^2}$。\n    -   如果 $\\sigma_j$ 实际上为零（即，该变异在所有分离株中都缺失或存在），则该特征方差为零。它不为回归提供任何信息，其系数 $\\beta_j$ 被设置为 $0$。\n    -   否则，将特征标准化，使其均值为 $0$，标准差为 $1$：$X'_j = (X_j - \\mu_j) / \\sigma_j$。\n-   设 $X'$ 是标准化后的非恒定特征矩阵，$\\beta'$ 是相应的系数向量。优化问题简化为：\n    $$\\underset{\\beta'}{\\text{minimize}} \\left( \\frac{1}{2n} \\|y_c - X'\\beta'\\|_2^2 + \\lambda \\|\\beta'\\|_1 \\right)$$\n\n**2. 坐标下降**\n这个迭代算法一次优化一个系数，同时保持所有其他系数固定。\n\n-   为标准化特征初始化系数向量：$\\beta' = \\mathbf{0}$。\n-   重复循环遍历坐标 $j = 1, \\dots, p'$（其中 $p'$ 是非恒定特征的数量），直到收敛：\n    -   对于每个坐标 $j$，在假设所有其他系数 $\\beta'_k$（$k \\neq j$）固定的情况下，找到使目标最小化的 $\\beta'_j$ 的值。\n    -   这个一维子问题有一个由**软阈值算子** $S$ 给出的闭式解：\n        $$\\beta'_j \\leftarrow S(z_j, \\lambda) = \\text{sign}(z_j) \\cdot \\max(|z_j| - \\lambda, 0)$$\n        其中 $z_j = \\frac{1}{n} (X'_j)^T r_j$ 并且 $r_j = y_c - \\sum_{k \\neq j} X'_k \\beta'_k$ 是偏残差。在循环中计算 $z_j$ 的一种计算上高效的方法是 $z_j = \\frac{1}{n} (X'_j)^T(y_c - X'\\beta') + \\beta'_{j,\\text{old}}$。循环就地更新 $\\beta'$。\n-   当两次迭代之间任何系数的最大绝对变化量低于一个小的容差（例如，$10^{-8}$）时，宣告收敛。\n\n**3. 反标准化**\n算法收敛到最优的 $\\hat{\\beta}'$ 后，必须将系数转换回 $X$ 的原始尺度。\n\n-   原始（未标准化）特征的系数计算如下：对于非恒定特征，$\\hat{\\beta}_j = \\hat{\\beta}'_j / \\sigma_j$。对于恒定特征，$\\hat{\\beta}_j=0$。\n-   然后截距计算如下：$\\hat{b}_0 = \\bar{y} - \\sum_{j=1}^p \\mu_j \\hat{\\beta}_j = \\bar{y} - \\mu_X^T\\hat{\\beta}$。\n\n这个过程产生了最终的截距和系数向量，它们解决了原始数据上的 LASSO 问题，同时满足了截距不被惩罚和处理零方差特征的要求。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"X\": np.array([\n                [1, 0, 1, 0, 1],\n                [0, 1, 1, 0, 0],\n                [1, 1, 0, 0, 0],\n                [0, 0, 0, 1, 1],\n                [1, 0, 0, 1, 0],\n                [0, 1, 0, 1, 1],\n                [1, 0, 1, 1, 0],\n                [0, 1, 1, 0, 1],\n                [1, 1, 0, 1, 0],\n                [0, 0, 1, 0, 1]\n            ]),\n            \"y\": np.array([2.35, 0.98, 1.81, 1.77, 2.60, 1.34, 2.59, 1.02, 2.05, 1.53]),\n            \"lambda_param\": 0.2\n        },\n        {\n            \"X\": np.array([\n                [1, 0, 0],\n                [0, 1, 0],\n                [0, 0, 1],\n                [1, 1, 0],\n                [1, 0, 1],\n                [0, 1, 1],\n                [1, 1, 1],\n                [0, 0, 0]\n            ]),\n            \"y\": np.array([1.52, -0.51, 1.00, 0.53, 1.98, 0.01, 1.00, 0.47]),\n            \"lambda_param\": 0.0\n        },\n        {\n            \"X\": np.array([\n                [1, 0, 0, 0],\n                [0, 0, 1, 0],\n                [1, 0, 1, 0],\n                [0, 0, 0, 0],\n                [1, 0, 0, 0],\n                [0, 0, 1, 0]\n            ]),\n            \"y\": np.array([1.2, 0.9, 1.1, 1.0, 1.2, 0.9]),\n            \"lambda_param\": 100.0\n        },\n        {\n            \"X\": np.array([\n                [1, 1, 0],\n                [0, 0, 1],\n                [1, 1, 1],\n                [0, 0, 0],\n                [1, 1, 0],\n                [0, 0, 1],\n                [1, 1, 1]\n            ]),\n            \"y\": np.array([1.9, 0.4, 1.6, 0.7, 1.9, 0.4, 1.6]),\n            \"lambda_param\": 0.3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = lasso_solver(case[\"X\"], case[\"y\"], case[\"lambda_param\"])\n        results.append(result)\n\n    # Format the output string without spaces in lists\n    inner_strings = [f\"[{','.join(f'{x:.8f}' for x in res)}]\" for res in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\ndef lasso_solver(X, y, lambda_param, max_iter=10000, tol=1e-9):\n    \"\"\"\n    Solves the LASSO regression problem using coordinate descent.\n    \n    Objective: (1/2n) * ||y - Xb - b0||^2_2 + lambda * ||b||_1\n    \n    Args:\n        X (np.ndarray): Feature matrix (n_samples, n_features).\n        y (np.ndarray): Target vector (n_samples,).\n        lambda_param (float): The regularization parameter.\n        max_iter (int): Maximum number of iterations for coordinate descent.\n        tol (float): Tolerance for convergence criterion.\n        \n    Returns:\n        list: A list containing the intercept and coefficients [b0, b1, b2, ...].\n    \"\"\"\n    n, p = X.shape\n\n    # 1. Preprocessing: Standardize features and center target\n    y_mean = np.mean(y)\n    y_c = y - y_mean\n\n    X_mean = np.mean(X, axis=0)\n    X_std = np.std(X, axis=0)\n\n    # Find indices of non-constant features (variance > 0)\n    non_const_indices = np.where(X_std > 1e-10)[0]\n\n    # If all features are constant, all coeffs are 0, intercept is mean of y.\n    if len(non_const_indices) == 0:\n        beta = np.zeros(p)\n        intercept = y_mean\n        return [intercept] + beta.tolist()\n\n    X_std_sub = X_std[non_const_indices]\n    X_mean_sub = X_mean[non_const_indices]\n    X_sub = X[:, non_const_indices]\n    \n    # Standardize the active (non-constant) features\n    X_prime = (X_sub - X_mean_sub) / X_std_sub\n    \n    p_prime = X_prime.shape[1]\n    beta_prime = np.zeros(p_prime)\n\n    # 2. Coordinate Descent\n    for i in range(max_iter):\n        beta_prime_old = np.copy(beta_prime)\n        \n        for j in range(p_prime):\n            # Temporarily set beta_prime[j] to 0 to calculate the partial residual\n            beta_prime[j] = 0.0\n            \n            # Prediction using current coefficients (excluding j)\n            y_pred = X_prime @ beta_prime\n            \n            # Residual for the full model without feature j's contribution\n            residual = y_c - y_pred\n            \n            # Compute z_j = (1/n) * X'_j.T * r_j\n            # This is the covariance of feature j with the partial residual\n            z_j = np.dot(X_prime[:, j], residual) / n\n            \n            # Apply soft-thresholding operator\n            if z_j > lambda_param:\n                beta_prime[j] = z_j - lambda_param\n            elif z_j  -lambda_param:\n                beta_prime[j] = z_j + lambda_param\n            else:\n                beta_prime[j] = 0.0\n        \n        # Check for convergence\n        if np.max(np.abs(beta_prime - beta_prime_old))  tol:\n            break\n\n    # 3. Post-processing: Un-standardize and compute intercept\n    # Initialize full beta vector with zeros\n    final_beta = np.zeros(p)\n    \n    # Compute unstandardized coefficients for non-constant features\n    unstandardized_beta_sub = beta_prime / X_std_sub\n    final_beta[non_const_indices] = unstandardized_beta_sub\n    \n    # Compute intercept for the original scale\n    intercept = y_mean - np.dot(X_mean, final_beta)\n    \n    return [intercept] + final_beta.tolist()\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "重建病原体的种群历史对于理解流行病的动态至关重要，例如，它的有效种群规模 ($N_e$) 如何随时间变化。本练习将引导您实现贝叶斯天际线图（Bayesian skyline plot），这是一种经典的系统发生动力学方法，它利用溯祖理论从基因组序列推断过去的种群动态。您将学习如何从溯祖间隔时间中推导并计算有效种群规模的后验分布，从而揭示病原体的演化历史。",
            "id": "4667746",
            "problem": "给定从标准合并模型下抽样的单个有根、严格二分叉的系统发育树中获得的跨合并等待时间及相应的谱系数量。假设有效种群大小函数 $N_e(t)$ 是分段恒定的，并根据连续的跨合并间隔划分为若干组。在每个组 $g$ 内，有效种群大小是恒定的，等于 $N_{e,g}$。\n\n基本和建模假设：\n- 在具有分段恒定种群大小的标准合并模型下，对于拥有 $k_i$ 个当代谱系的每个跨合并等待时间 $t_i$，当间隔 $i$ 属于组 $g$ 时，其服从指数分布，风险率 $h_i = \\binom{k_i}{2}/N_{e,g}$。\n- 令 $\\lambda_g = 1/N_{e,g}$。那么，组 $g$ 中一个间隔 $i$ 的似然贡献服从速率为 $r_i = \\lambda_g \\binom{k_i}{2}$ 的指数分布，在给定组常数的情况下，各间隔之间相互独立。\n- 对每个组 $g$ 的 $\\lambda_g$ 采用共轭先验：一个形状参数为 $a_g$、速率参数为 $b_g$ 的伽马分布，记为 $\\lambda_g \\sim \\mathrm{Gamma}(a_g,b_g)$，其密度正比于 $\\lambda_g^{a_g - 1} \\exp(-b_g \\lambda_g)$。\n- 每个组 $g$ 的充分统计量是跨合并间隔的数量 $m_g$ 和缩放时间总和 $S_g = \\sum_{i \\in g} \\binom{k_i}{2} t_i$。\n\n您的任务是通过推导 $\\lambda_g$ 的后验分布，然后转换为 $N_{e,g} = 1/\\lambda_g$，来为每个组 $g$ 计算 $N_{e,g}$ 的贝叶斯天际线摘要。对于每个组 $g$，计算：\n- $N_{e,g}$ 的后验均值。\n- $N_{e,g}$ 的后验中位数。\n- $N_{e,g}$ 的中央 $95\\%$ 置信水平可信区间的下限和上限，即 $0.025$ 和 $0.975$ 的后验分位数。\n\n将 $N_{e,g}$ 的所有输出表示为实数（个体数量，一个无量纲的量），四舍五入到六位小数。$t_i$ 的时间单位是年；输出中不应打印任何时间单位。\n\n请从上述基本假设推导您的方法。不要使用或假设任何不是这些假设直接结果的公式。\n\n测试套件。对于每个测试用例，您将获得：\n- 跨合并时间列表 $\\{t_i\\}$，单位为年。\n- 等长的谱系数量列表 $\\{k_i\\}$，其中 $k_i \\ge 2$ 且 $t_i \\ge 0$。\n- 一个正整数列表，指定将间隔序列连续划分的组的大小。\n- 各组共享的先验超参数：形状 $a$ 和速率 $b$。\n\n为每个测试用例中的每个组计算贝叶斯天际线摘要。测试用例如下：\n\n- 测试用例 $1$：\n  - $t = [0.05, 0.12, 0.08, 0.20, 0.30, 0.25]$\n  - $k = [10, 9, 8, 7, 6, 5]$\n  - 组大小 $= [2, 2, 2]$\n  - 先验 $(a, b) = (2.5, 1.0)$\n\n- 测试用例 $2$：\n  - $t = [0.02, 0.02, 0.02, 0.02]$\n  - $k = [50, 49, 48, 47]$\n  - 组大小 $= [1, 1, 2]$\n  - 先验 $(a, b) = (2.1, 0.5)$\n\n- 测试用例 $3$：\n  - $t = [0.30, 0.25, 0.35, 0.40]$\n  - $k = [6, 5, 4, 3]$\n  - 组大小 $= [4]$\n  - 先验 $(a, b) = (1.5, 1.0)$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例。每个测试用例元素是一个组列表，每个组元素是一个包含四个数字的列表，顺序为 [后验均值, 后验中位数, $0.025$ 分位数, $0.975$ 分位数]，每个数字四舍五入到六位小数。例如，一个有效的结构形状是：\n  - $[\\,[\\,[x_{11}, x_{12}, x_{13}, x_{14}], [x_{21}, x_{22}, x_{23}, x_{24}], \\ldots], \\; [\\ldots], \\; [\\ldots]\\,]$\n\n在您的书面解决方案中，解释从基本假设出发的推导过程，并讨论 $N_e(t)$ 的分段恒定片段的可识别性，即 $m_g$ 和 $S_g$ 如何约束推断，包括片段信息非常有限或没有信息的边缘情况。",
            "solution": "该问题要求在贝叶斯框架内，计算分段恒定有效种群大小 $N_e(t)$ 的后验摘要统计量。该方法源于标准合并模型的基本假设、跨合并等待时间的指数似然以及精度参数 $\\lambda_g = 1/N_{e,g}$ 的共轭伽马先验。\n\n推导过程主要分三个阶段进行：\n1.  推导每个种群大小组 $g$ 的参数 $\\lambda_g$ 的后验分布。\n2.  将后验分布从 $\\lambda_g$ 转换为有效种群大小 $N_{e,g}$。\n3.  从 $N_{e,g}$ 的后验分布计算所需的摘要统计量（均值、中位数和 $95\\%$ 可信区间）。\n\n### 1. $\\lambda_g$ 的后验分布\n\n根据贝叶斯定理，后验概率分布正比于似然和先验概率分布的乘积。\n$$P(\\lambda_g | \\text{data}) \\propto P(\\text{data} | \\lambda_g) \\times P(\\lambda_g)$$\n\n**似然函数：**\n问题陈述，对于组 $g$ 内的一个间隔 $i$，拥有 $k_i$ 个谱系的跨合并等待时间 $t_i$ 服从速率为 $r_i = \\lambda_g \\binom{k_i}{2}$ 的指数分布。单个间隔的概率密度函数（PDF）为：\n$$P(t_i | \\lambda_g, k_i) = \\lambda_g \\binom{k_i}{2} \\exp\\left( -\\lambda_g \\binom{k_i}{2} t_i \\right)$$\n假设在给定 $\\lambda_g$ 的条件下，等待时间是条件独立的，那么组 $g$ 中数据（包含 $m_g$ 个间隔）的总似然是各个 PDF 的乘积：\n$$L(\\lambda_g) = P(\\{t_i, k_i\\}_{i \\in g} | \\lambda_g) = \\prod_{i \\in g} P(t_i | \\lambda_g, k_i)$$\n$$L(\\lambda_g) = \\prod_{i \\in g} \\left[ \\lambda_g \\binom{k_i}{2} \\exp\\left( -\\lambda_g \\binom{k_i}{2} t_i \\right) \\right]$$\n我们可以将涉及 $\\lambda_g$ 的项组合起来：\n$$L(\\lambda_g) = \\left( \\prod_{i \\in g} \\binom{k_i}{2} \\right) \\left( \\prod_{i \\in g} \\lambda_g \\right) \\exp\\left( -\\sum_{i \\in g} \\lambda_g \\binom{k_i}{2} t_i \\right)$$\n$$L(\\lambda_g) = \\left( \\prod_{i \\in g} \\binom{k_i}{2} \\right) \\lambda_g^{m_g} \\exp\\left( -\\lambda_g \\sum_{i \\in g} \\binom{k_i}{2} t_i \\right)$$\n项 $\\prod_{i \\in g} \\binom{k_i}{2}$ 相对于 $\\lambda_g$ 是一个常数。使用给定的充分统计量 $S_g = \\sum_{i \\in g} \\binom{k_i}{2} t_i$，似然正比于：\n$$L(\\lambda_g) \\propto \\lambda_g^{m_g} \\exp(-\\lambda_g S_g)$$\n这是似然函数的核心部分。\n\n**先验分布：**\n问题为 $\\lambda_g$ 指定了一个共轭伽马先验：\n$$\\lambda_g \\sim \\mathrm{Gamma}(a_g, b_g)$$\n其 PDF 为 $P(\\lambda_g) \\propto \\lambda_g^{a_g - 1} \\exp(-b_g \\lambda_g)$，其中 $a_g$ 是形状参数，$b_g$ 是速率参数。\n\n**后验分布：**\n将似然核心与先验密度核心相乘，得到后验密度核心：\n$$P(\\lambda_g | \\text{data}) \\propto \\left( \\lambda_g^{m_g} \\exp(-\\lambda_g S_g) \\right) \\times \\left( \\lambda_g^{a_g - 1} \\exp(-b_g \\lambda_g) \\right)$$\n$$P(\\lambda_g | \\text{data}) \\propto \\lambda_g^{a_g + m_g - 1} \\exp\\left( -(b_g + S_g) \\lambda_g \\right)$$\n这是一个具有更新后参数的伽马分布的核心。设后验参数为 $a'_g = a_g + m_g$ 和 $b'_g = b_g + S_g$。\n因此，$\\lambda_g$ 的后验分布为：\n$$\\lambda_g | \\text{data} \\sim \\mathrm{Gamma}(a_g + m_g, b_g + S_g)$$\n\n### 2. $N_{e,g}$ 的后验分布\n\n有效种群大小 $N_{e,g}$ 是 $\\lambda_g$ 的倒数，即 $N_{e,g} = 1/\\lambda_g$。如果一个随机变量 $X$ 服从伽马分布，$X \\sim \\mathrm{Gamma}(\\alpha, \\beta)$，那么它的倒数 $Y=1/X$ 服从逆伽马分布，其形状参数与 $X$ 相同为 $\\alpha$，尺度参数为 $\\beta$（伽马分布的速率参数成为逆伽马分布的尺度参数）。\n因此，$N_{e,g}$ 的后验分布是一个逆伽马分布：\n$$N_{e,g} | \\text{data} \\sim \\mathrm{InverseGamma}(a'_g, b'_g) = \\mathrm{InverseGamma}(a_g + m_g, b_g + S_g)$$\n\n### 3. $N_{e,g}$ 的后验摘要统计量\n\n我们现在可以从这个后验 $\\mathrm{InverseGamma}(a'_g, b'_g)$ 分布中计算所需的统计量。\n\n**后验均值：**\n逆伽马分布 $\\mathrm{InverseGamma}(\\alpha, \\beta)$ 的期望值为 $E[Y] = \\frac{\\beta}{\\alpha - 1}$，定义于 $\\alpha > 1$。\n在我们的情况下，$N_{e,g}$ 的后验均值为：\n$$E[N_{e,g} | \\text{data}] = \\frac{b'_g}{a'_g - 1} = \\frac{b_g + S_g}{a_g + m_g - 1}$$\n条件 $a'_g > 1$ 对所有测试用例都成立，因为给定的先验形状参数 $a$ 大于 $1$，且每组的间隔数 $m_g$ 至少为 $1$。\n\n**后验中位数与可信区间：**\n这些统计量是从后验分布的分位数推导出来的。伽马分布的分位数与其对应的逆伽马分布的分位数之间存在直接关系。设 $Q_Y(p)$ 是 $Y \\sim \\mathrm{InverseGamma}(\\alpha, \\beta)$ 的第 $p$ 个分位数，而 $Q_X(p)$ 是 $X \\sim \\mathrm{Gamma}(\\alpha, \\beta)$ 的第 $p$ 个分位数。其关系为：\n$$Q_Y(p) = \\frac{1}{Q_X(1-p)}$$\n我们应用此关系来找到 $N_{e,g}$ 的中位数（第 $50$ 百分位数）和 $95\\%$ 中央可信区间（端点在第 $2.5$ 和第 $97.5$ 百分位数）。设 $Q_{\\lambda_g}(p)$ 表示后验分布 $\\lambda_g | \\text{data} \\sim \\mathrm{Gamma}(a'_g, b'_g)$ 的第 $p$ 个分位数。\n\n-   **后验中位数 ($p=0.5$):**\n    $$\\text{Median}[N_{e,g}] = Q_{N_{e,g}}(0.5) = \\frac{1}{Q_{\\lambda_g}(1-0.5)} = \\frac{1}{Q_{\\lambda_g}(0.5)}$$\n-   **可信区间下限 ($p=0.025$):**\n    $$Q_{N_{e,g}}(0.025) = \\frac{1}{Q_{\\lambda_g}(1-0.025)} = \\frac{1}{Q_{\\lambda_g}(0.975)}$$\n-   **可信区间上限 ($p=0.975$):**\n    $$Q_{N_{e,g}}(0.975) = \\frac{1}{Q_{\\lambda_g}(1-0.975)} = \\frac{1}{Q_{\\lambda_g}(0.025)}$$\n\n伽马分布的这些分位数没有封闭形式的表达式，必须进行数值计算，为此可使用像 `scipy.stats` 这样的科学计算库。\n\n### 关于可识别性的讨论\n后验参数 $a'_g = a_g + m_g$ 和 $b'_g = b_g + S_g$ 明确显示了由充分统计量 $m_g$ 和 $S_g$ 概括的数据如何更新由 $a_g$ 和 $b_g$ 编码的先验信念。给定片段的种群大小 $N_{e,g}$ 的可识别性取决于该片段内数据的信息含量。\n-   统计量 $m_g$，即组 $g$ 中的合并事件数，为后验分布的形状提供了信息。较大的 $m_g$ 会导致后验形状参数 $a'_g$ 由数据主导，使得推断对先验形状 $a_g$ 的敏感性降低。\n-   统计量 $S_g = \\sum_{i \\in g} \\binom{k_i}{2} t_i$，即每个等待时间乘以谱系对数量后的总和，为后验分布的速率/尺度提供了信息。较大的 $S_g$ 值（表示较长的等待时间）将主导先验速率 $b_g$，将 $\\lambda_g$ 的后验分布拉向较小的值，从而将 $N_{e,g}$ 的后验分布拉向较大的值。\n-   **边缘情况：**如果一个组包含的信息非常少（例如，$m_g$ 很小，或所有 $t_i$ 都很短，导致 $S_g$ 很小），后验分布将会很宽（高方差），并严重受到先验选择 ($a_g, b_g$) 的影响。在这种情况下，$N_{e,g}$ 是弱可识别的。相反，当 $m_g$ 和 $S_g$ 很大时，似然会主导先验，后验分布变得非常尖锐，$N_{e,g}$ 由数据强可识别。问题设置确保所有组的 $m_g \\ge 1$，因此参数在技术上总是可识别的，但估计的精度随数据内容而变化。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import gamma\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian skyline problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"t\": [0.05, 0.12, 0.08, 0.20, 0.30, 0.25],\n            \"k\": [10, 9, 8, 7, 6, 5],\n            \"group_sizes\": [2, 2, 2],\n            \"prior\": (2.5, 1.0)\n        },\n        {\n            \"t\": [0.02, 0.02, 0.02, 0.02],\n            \"k\": [50, 49, 48, 47],\n            \"group_sizes\": [1, 1, 2],\n            \"prior\": (2.1, 0.5)\n        },\n        {\n            \"t\": [0.30, 0.25, 0.35, 0.40],\n            \"k\": [6, 5, 4, 3],\n            \"group_sizes\": [4],\n            \"prior\": (1.5, 1.0)\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_results = bayesian_skyline_summary(\n            times=np.array(case[\"t\"]),\n            lineages=np.array(case[\"k\"]),\n            group_sizes=case[\"group_sizes\"],\n            prior_a=case[\"prior\"][0],\n            prior_b=case[\"prior\"][1]\n        )\n        all_results.append(case_results)\n\n    # Manual string formatting to match the required output format exactly.\n    case_strings = []\n    for case_res in all_results:\n        group_strings = []\n        for group_res in case_res:\n            num_strings = [f\"{x:.6f}\" for x in group_res]\n            group_strings.append(f\"[{','.join(num_strings)}]\")\n        case_strings.append(f\"[{','.join(group_strings)}]\")\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\ndef bayesian_skyline_summary(times, lineages, group_sizes, prior_a, prior_b):\n    \"\"\"\n    Computes the Bayesian skyline summary for a single test case.\n\n    For each group of coalescent intervals, it calculates the posterior mean,\n    median, and 95% central credible interval for the effective population size Ne.\n    \"\"\"\n    case_summary = []\n    \n    # Calculate start and end indices for each group\n    group_indices = np.cumsum([0] + group_sizes)\n    \n    for i in range(len(group_sizes)):\n        start_idx = group_indices[i]\n        end_idx = group_indices[i+1]\n        \n        # Slice data for the current group\n        group_t = times[start_idx:end_idx]\n        group_k = lineages[start_idx:end_idx]\n        \n        # Calculate sufficient statistics for the group\n        # m_g: number of intervals in group g\n        m_g = len(group_t)\n        \n        # S_g: sum of (k_i choose 2) * t_i\n        combs = group_k * (group_k - 1) / 2.0\n        s_g = np.sum(combs * group_t)\n        \n        # Calculate posterior Gamma parameters for lambda_g\n        # lambda_g ~ Gamma(a', b')\n        post_a = prior_a + m_g\n        post_b = prior_b + s_g\n        \n        # Calculate posterior statistics for Ne_g\n        # Ne_g = 1 / lambda_g, so Ne_g ~ InverseGamma(a', b')\n\n        # 1. Posterior Mean of Ne_g\n        # E[Ne_g] = b' / (a' - 1)\n        if post_a = 1:\n            # This case should not occur with the given problem constraints\n            # (a > 1, m_g >= 1), but is included for robustness. Mean is undefined.\n            post_mean_ne = np.nan\n        else:\n            post_mean_ne = post_b / (post_a - 1)\n            \n        # 2. Posterior Median and Credible Interval of Ne_g\n        # These are calculated using quantiles of the posterior Gamma distribution for lambda_g.\n        # Quantile of Ne_g(p) = 1 / Quantile of lambda_g(1-p)\n        \n        # scipy.stats.gamma is parameterized by shape 'a' and 'scale', where scale = 1 / rate.\n        # We want quantiles for Gamma(shape=post_a, rate=post_b).\n        gamma_quantiles = gamma.ppf([0.025, 0.5, 0.975], a=post_a, scale=1/post_b)\n        \n        q_025_lambda, q_50_lambda, q_975_lambda = gamma_quantiles\n        \n        # Median of Ne_g = 1 / median of lambda_g\n        post_median_ne = 1.0 / q_50_lambda\n        \n        # Lower bound of 95% CI for Ne_g = 1 / upper bound of 95% CI for lambda_g\n        lower_ci_ne = 1.0 / q_975_lambda\n        \n        # Upper bound of 95% CI for Ne_g = 1 / lower bound of 95% CI for lambda_g\n        upper_ci_ne = 1.0 / q_025_lambda\n        \n        group_summary = [post_mean_ne, post_median_ne, lower_ci_ne, upper_ci_ne]\n        case_summary.append(group_summary)\n        \n    return case_summary\n\nsolve()\n\n```"
        }
    ]
}