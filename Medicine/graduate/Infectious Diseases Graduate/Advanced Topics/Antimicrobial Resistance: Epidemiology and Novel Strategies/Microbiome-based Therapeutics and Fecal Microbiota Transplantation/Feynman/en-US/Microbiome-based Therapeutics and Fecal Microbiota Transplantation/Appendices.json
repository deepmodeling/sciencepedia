{
    "hands_on_practices": [
        {
            "introduction": "The safety of Fecal Microbiota Transplantation (FMT) hinges on the rigorous screening of donors for transmissible pathogens. However, no diagnostic test is perfect, and understanding the predictive value of a test is paramount for clinical decision-making. This exercise challenges you to apply foundational principles of probability to a realistic clinical scenario . By deriving the posterior probability of pathogen carriage given a positive screening test, you will gain a deeper appreciation for how a test's sensitivity, specificity, and the pathogen's prevalence in the population collectively determine the true meaning of a result.",
            "id": "4666237",
            "problem": "A hospital is evaluating candidates for Fecal Microbiota Transplantation (FMT) to treat recurrent Clostridioides difficile infection. To mitigate the risk of post-procedure bacteremia and transmission of multidrug-resistant organisms, the program screens candidates for rectal colonization with carbapenem-resistant Enterobacterales using a polymerase chain reaction assay. Let the true prevalence of colonization among this FMT candidate cohort be denoted by $p$, the assay sensitivity by $Se$ (the probability of a positive test given true colonization), and the assay specificity by $Sp$ (the probability of a negative test given no colonization). A particular candidate’s screening result is positive. Starting from the core definitions of conditional probability, the law of total probability, and the standard definitions of sensitivity and specificity, derive a closed-form expression for the posterior probability of true colonization given a positive test. Then, define and derive an expression for the residual risk that this positive screening result represents a false positive. Express your final answer as closed-form expressions in terms of $p$, $Se$, and $Sp$. Do not perform any numerical substitutions or rounding, and do not include units.",
            "solution": "The problem requires the derivation of two quantities related to a diagnostic screening test: the posterior probability of true colonization given a positive test and the residual risk of a false positive. We will start from the fundamental principles of probability theory as requested.\n\nLet us define the following events:\n- $C$: The event that a candidate is truly colonized with carbapenem-resistant Enterobacterales.\n- $C^c$: The complementary event that a candidate is not colonized.\n- $T^+$: The event that the screening assay result is positive.\n- $T^-$: The event that the screening assay result is negative.\n\nFrom the problem statement, we are given the following probabilities:\n- The true prevalence of colonization, $P(C) = p$.\n- The assay sensitivity, the probability of a positive test given true colonization, $P(T^+ | C) = Se$.\n- The assay specificity, the probability of a negative test given no colonization, $P(T^- | C^c) = Sp$.\n\nFrom these definitions, we can derive other relevant probabilities. The probability of not being colonized is the complement of the prevalence:\n$$P(C^c) = 1 - P(C) = 1 - p$$\nThe probability of a positive test given no colonization (a false positive event) is the complement of the specificity:\n$$P(T^+ | C^c) = 1 - P(T^- | C^c) = 1 - Sp$$\n\nThe first objective is to derive a closed-form expression for the posterior probability of true colonization given a positive test. This quantity is denoted by $P(C | T^+)$. We will use Bayes' theorem, which is derived from the definition of conditional probability.\n\nThe definition of conditional probability states:\n$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\nApplying this to our problem:\n$$P(C | T^+) = \\frac{P(C \\cap T^+)}{P(T^+)}$$\n\nThe numerator, $P(C \\cap T^+)$, is the joint probability of being truly colonized and testing positive. We can re-express this using the definition of conditional probability for $P(T^+ | C)$:\n$$P(T^+ | C) = \\frac{P(C \\cap T^+)}{P(C)}$$\nRearranging gives:\n$$P(C \\cap T^+) = P(T^+ | C) \\cdot P(C)$$\nSubstituting the given symbols:\n$$P(C \\cap T^+) = Se \\cdot p$$\n\nThe denominator, $P(T^+)$, is the overall probability of a positive test. We find this using the Law of Total Probability. The event space can be partitioned by the events $C$ and $C^c$. Therefore, the probability of a positive test $P(T^+)$ is the sum of the probabilities of a positive test in each partition:\n$$P(T^+) = P(T^+ \\cap C) + P(T^+ \\cap C^c)$$\nUsing the definition of conditional probability for each term:\n$$P(T^+) = P(T^+ | C) \\cdot P(C) + P(T^+ | C^c) \\cdot P(C^c)$$\nSubstituting the known expressions:\n$$P(T^+) = (Se \\cdot p) + ((1 - Sp) \\cdot (1 - p))$$\n\nNow we combine the numerator and the denominator to find the final expression for the posterior probability $P(C | T^+)$, also known as the Positive Predictive Value (PPV):\n$$P(C | T^+) = \\frac{Se \\cdot p}{(Se \\cdot p) + ((1 - Sp) \\cdot (1 - p))}$$\nThis is the closed-form expression for the probability of true colonization given a positive test.\n\nThe second objective is to define and derive an expression for the residual risk that a positive screening result represents a false positive.\nThis risk is defined as the probability that a candidate is not colonized, given that their test result was positive. In our notation, this is the conditional probability $P(C^c | T^+)$.\n\nWe can derive this expression in two ways. First, by recognizing that given a positive test result ($T^+$), a candidate is either truly colonized ($C$) or not colonized ($C^c$). These are mutually exclusive and exhaustive outcomes. Therefore:\n$$P(C | T^+) + P(C^c | T^+) = 1$$\nSolving for the desired quantity:\n$$P(C^c | T^+) = 1 - P(C | T^+)$$\nSubstituting the expression we derived for $P(C | T^+)$:\n$$P(C^c | T^+) = 1 - \\frac{Se \\cdot p}{(Se \\cdot p) + (1 - Sp)(1 - p)}$$\nTo simplify, we find a common denominator:\n$$P(C^c | T^+) = \\frac{((Se \\cdot p) + (1 - Sp)(1 - p)) - (Se \\cdot p)}{(Se \\cdot p) + (1 - Sp)(1 - p)}$$\n$$P(C^c | T^+) = \\frac{(1 - Sp)(1 - p)}{(Se \\cdot p) + (1 - Sp)(1 - p)}$$\nThis is the closed-form expression for the risk of a false positive. This quantity is also known as the False Discovery Rate.\n\nAlternatively, we could have derived $P(C^c | T^+)$ directly using Bayes' theorem:\n$$P(C^c | T^+) = \\frac{P(T^+ | C^c) \\cdot P(C^c)}{P(T^+)}$$\nThe numerator is $P(T^+|C^c) \\cdot P(C^c) = (1-Sp) \\cdot (1-p)$. The denominator $P(T^+)$ is the same as derived before. This yields the identical result, confirming our derivation.\n\nThe problem asks for two expressions: the posterior probability of true colonization given a positive test, and the residual risk of a false positive. These are $P(C|T^+)$ and $P(C^c|T^+)$ respectively.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{p \\cdot Se}{p \\cdot Se + (1-p)(1-Sp)} & \\frac{(1-p)(1-Sp)}{p \\cdot Se + (1-p)(1-Sp)} \\end{pmatrix}}$$"
        },
        {
            "introduction": "To establish the efficacy of a new therapeutic like FMT, the gold standard is the randomized controlled trial (RCT). A fundamental step in designing a compelling RCT is determining the appropriate sample size to ensure the study is adequately powered to detect a clinically meaningful effect. This practice guides you through the derivation of a sample size formula for a trial with a binary outcome, a common scenario in infectious disease research . Engaging with this problem will solidify your understanding of the interplay between Type I error ($\\alpha$), statistical power ($1-\\beta$), and the expected effect size in shaping the scale and feasibility of clinical research.",
            "id": "4666221",
            "problem": "A clinical team is designing a two-arm randomized controlled trial to evaluate Fecal Microbiota Transplantation (FMT) versus standard-of-care in adults with recurrent Clostridioides difficile infection. The primary endpoint is clinical cure by week eight, defined as resolution of diarrhea and no need for further antimicrobial therapy, assessed in each arm as a proportion. Let the baseline cure probability under standard-of-care be $p_{0}$, and suppose FMT is expected to improve cure by an absolute difference $\\Delta$, so the alternative cure probability in the FMT arm is $p_{1} = p_{0} + \\Delta$, with $0 \\leq p_{0} \\leq 1$ and $0 < \\Delta < 1 - p_{0}$. The study will randomize patients $1{:}1$ to FMT or standard-of-care, and will use a two-sided test of equal proportions with Type I error $\\alpha$ and power $1 - \\beta$ for detecting the true difference $\\Delta$.\n\nStarting from the Binomial model for arm-specific cures, the Central Limit Theorem (CLT) for sample proportions, and the definition of Type I and Type II errors for a two-sided Wald test of the difference in proportions, derive the total sample size $N$ required so that the test has two-sided Type I error $\\alpha$ and power $1 - \\beta$ to detect the true difference $\\Delta$. Express your final answer as a single closed-form analytic expression in terms of $\\alpha$, $\\beta$, $p_{0}$, and $\\Delta$, using only the standard normal cumulative distribution function $\\Phi$ and its inverse $\\Phi^{-1}$. Assume equal allocation ($N/2$ per arm) and use the conventional planning approach that employs the pooled variance under the null hypothesis for the test threshold. No numerical evaluation is required.",
            "solution": "The objective is to derive the total sample size $N$ for a two-arm randomized controlled trial comparing Fecal Microbiota Transplantation (FMT) to a standard-of-care therapy. Let $p_1$ and $p_0$ be the true cure proportions in the FMT and standard-of-care arms, respectively. The study is designed to detect an absolute difference $\\Delta = p_1 - p_0$.\n\nFirst, we define the statistical hypotheses. The null hypothesis, $H_0$, posits no difference between the arms, while the alternative hypothesis, $H_A$, posits a difference of $\\Delta$. For a two-sided test, these are:\n$$H_0: p_1 - p_0 = 0 \\quad \\text{or} \\quad p_1 = p_0$$\n$$H_A: p_1 - p_0 \\neq 0$$\nThe study is being powered to detect a specific alternative, $p_1 - p_0 = \\Delta$.\n\nLet $n_1$ and $n_0$ be the sample sizes in the FMT and standard-of-care arms. Given $1:1$ randomization, $n_1 = n_0 = n$. The total sample size is $N = 2n$. Let $X_1$ and $X_0$ be the number of cures observed in each arm. Under the binomial model, $X_1 \\sim \\text{Binomial}(n, p_1)$ and $X_0 \\sim \\text{Binomial}(n, p_0)$. The sample proportions are $\\hat{p}_1 = X_1/n$ and $\\hat{p}_0 = X_0/n$.\n\nAccording to the Central Limit Theorem (CLT), for a sufficiently large sample size $n$, the sampling distributions of the proportions are approximately normal:\n$$\\hat{p}_1 \\approx \\mathcal{N}\\left(p_1, \\frac{p_1(1-p_1)}{n}\\right)$$\n$$\\hat{p}_0 \\approx \\mathcal{N}\\left(p_0, \\frac{p_0(1-p_0)}{n}\\right)$$\nAs the two arms are independent, the difference in sample proportions, $\\hat{p}_1 - \\hat{p}_0$, is also approximately normally distributed:\n$$\\hat{p}_1 - \\hat{p}_0 \\approx \\mathcal{N}\\left(p_1 - p_0, \\frac{p_1(1-p_1)}{n} + \\frac{p_0(1-p_0)}{n}\\right)$$\n\nThe test statistic for the two-sided Wald test of the difference in proportions, using a pooled variance estimate under $H_0$, is:\n$$Z = \\frac{\\hat{p}_1 - \\hat{p}_0}{\\sqrt{\\hat{p}_{pool}(1-\\hat{p}_{pool})\\left(\\frac{1}{n_1} + \\frac{1}{n_0}\\right)}}$$\nwhere $\\hat{p}_{pool} = \\frac{X_1 + X_0}{n_1 + n_0} = \\frac{n\\hat{p}_1 + n\\hat{p}_0}{2n} = \\frac{\\hat{p}_1 + \\hat{p}_0}{2}$. Under $H_0$, $Z$ follows a standard normal distribution, $\\mathcal{N}(0,1)$.\n\nFor a two-sided test with a Type I error rate of $\\alpha$, we reject $H_0$ if $|Z| > \\Phi^{-1}(1-\\alpha/2)$, where $\\Phi^{-1}$ is the quantile function (inverse CDF) of the standard normal distribution. For planning purposes, we cannot use the sample estimate $\\hat{p}_{pool}$. Instead, we use its expected value. The \"conventional planning approach\" specified uses the expected value under the alternative hypothesis to estimate the pooled variance for the test threshold. The expected value of $\\hat{p}_{pool}$ is $E[\\hat{p}_{pool}] = \\frac{p_1+p_0}{2}$, which we denote as $\\bar{p}$.\nSo, the variance part of the test statistic is estimated as:\n$$\\text{Var}_{\\text{pooled}} = \\bar{p}(1-\\bar{p})\\left(\\frac{1}{n} + \\frac{1}{n}\\right) = \\frac{2\\bar{p}(1-\\bar{p})}{n}$$\nWe reject $H_0$ if $|\\hat{p}_1 - \\hat{p}_0| > C$, where the critical value $C$ is:\n$$C = \\Phi^{-1}\\left(1-\\frac{\\alpha}{2}\\right) \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}}$$\n\nNext, we impose the condition for statistical power, $1-\\beta$. Power is the probability of correctly rejecting $H_0$ when $H_A$ is true, i.e., when $p_1 - p_0 = \\Delta$.\n$$1 - \\beta = P(\\text{reject } H_0 | p_1 - p_0 = \\Delta) = P(|\\hat{p}_1 - \\hat{p}_0| > C | p_1 - p_0 = \\Delta)$$\nFor $\\Delta > 0$, the probability of the event $\\hat{p}_1 - \\hat{p}_0 < -C$ is negligible. Thus, we can approximate the power by considering only the upper tail:\n$$1 - \\beta \\approx P(\\hat{p}_1 - \\hat{p}_0 > C | p_1 - p_0 = \\Delta)$$\nTo evaluate this probability, we must standardize $\\hat{p}_1 - \\hat{p}_0$ using its distribution under the alternative hypothesis $H_A$. Under $H_A$, the mean is $\\Delta$ and the variance is:\n$$\\text{Var}_{H_A} = \\frac{p_1(1-p_1)}{n} + \\frac{p_0(1-p_0)}{n}$$\nStandardizing gives:\n$$1 - \\beta \\approx P\\left( \\frac{(\\hat{p}_1 - \\hat{p}_0) - \\Delta}{\\sqrt{\\text{Var}_{H_A}}} > \\frac{C - \\Delta}{\\sqrt{\\text{Var}_{H_A}}} \\right)$$\nThe term on the left is a standard normal variable. Let $Z'$ be this variable.\n$$1 - \\beta \\approx P\\left(Z' > \\frac{C - \\Delta}{\\sqrt{\\text{Var}_{H_A}}}\\right) = 1 - \\Phi\\left(\\frac{C - \\Delta}{\\sqrt{\\text{Var}_{H_A}}}\\right)$$\nThis implies $\\beta \\approx \\Phi\\left(\\frac{C - \\Delta}{\\sqrt{\\text{Var}_{H_A}}}\\right)$. Applying the inverse normal CDF:\n$$\\Phi^{-1}(\\beta) \\approx \\frac{C - \\Delta}{\\sqrt{\\text{Var}_{H_A}}}$$\nUsing the identity $\\Phi^{-1}(\\beta) = -\\Phi^{-1}(1-\\beta)$, we rearrange to solve for $\\Delta$:\n$$\\Delta \\approx C + \\Phi^{-1}(1-\\beta)\\sqrt{\\text{Var}_{H_A}}$$\nNow, substitute the expressions for $C$ and $\\text{Var}_{H_A}$:\n$$\\Delta = \\Phi^{-1}\\left(1-\\frac{\\alpha}{2}\\right) \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}} + \\Phi^{-1}(1-\\beta) \\sqrt{\\frac{p_1(1-p_1) + p_0(1-p_0)}{n}}$$\nFactor out $1/\\sqrt{n}$:\n$$\\Delta = \\frac{1}{\\sqrt{n}} \\left[ \\Phi^{-1}\\left(1-\\frac{\\alpha}{2}\\right) \\sqrt{2\\bar{p}(1-\\bar{p})} + \\Phi^{-1}(1-\\beta) \\sqrt{p_1(1-p_1) + p_0(1-p_0)} \\right]$$\nSolving for $n$, the sample size per arm:\n$$n = \\frac{1}{\\Delta^2} \\left[ \\Phi^{-1}\\left(1-\\frac{\\alpha}{2}\\right) \\sqrt{2\\bar{p}(1-\\bar{p})} + \\Phi^{-1}(1-\\beta) \\sqrt{p_1(1-p_1) + p_0(1-p_0)} \\right]^2$$\nThe total sample size is $N=2n$:\n$$N = \\frac{2}{\\Delta^2} \\left[ \\Phi^{-1}\\left(1-\\frac{\\alpha}{2}\\right) \\sqrt{2\\bar{p}(1-\\bar{p})} + \\Phi^{-1}(1-\\beta) \\sqrt{p_1(1-p_1) + p_0(1-p_0)} \\right]^2$$\nFinally, we express the result solely in terms of the given parameters $\\alpha$, $\\beta$, $p_0$, and $\\Delta$. We substitute $p_1 = p_0 + \\Delta$ and $\\bar{p} = (p_0 + p_1)/2 = p_0 + \\Delta/2$:\n$$N = \\frac{2}{\\Delta^2} \\left[ \\Phi^{-1}\\left(1-\\frac{\\alpha}{2}\\right) \\sqrt{2\\left(p_0 + \\frac{\\Delta}{2}\\right)\\left(1 - p_0 - \\frac{\\Delta}{2}\\right)} + \\Phi^{-1}(1-\\beta) \\sqrt{p_0(1-p_0) + (p_0+\\Delta)(1-p_0-\\Delta)} \\right]^2$$\nThis is the required closed-form analytic expression for the total sample size $N$.",
            "answer": "$$\n\\boxed{\\frac{2}{\\Delta^2} \\left[ \\Phi^{-1}\\left(1-\\frac{\\alpha}{2}\\right) \\sqrt{2\\left(p_0 + \\frac{\\Delta}{2}\\right)\\left(1 - p_0 - \\frac{\\Delta}{2}\\right)} + \\Phi^{-1}(1-\\beta) \\sqrt{p_0(1-p_0) + (p_0+\\Delta)(1-p_0-\\Delta)} \\right]^2}\n$$"
        },
        {
            "introduction": "Beyond establishing *if* an intervention works, a critical goal in microbiome science is to understand *how* it works by identifying the specific microbial taxa that are altered. This task is complicated by the compositional nature of sequencing data, where we only observe relative, not absolute, abundances, necessitating specialized statistical approaches. This practice requires a critical evaluation of two widely used methods for differential abundance analysis, forcing you to dissect their underlying assumptions and limitations . This conceptual exercise is vital for developing the skills to choose appropriate analytical tools and correctly interpret the results of microbiome studies.",
            "id": "4666271",
            "problem": "A randomized, double-blind trial evaluates fecal microbiota transplantation (FMT) as a microbiome-based therapeutic for recurrent Clostridioides difficile infection. Stool samples are collected from participants at baseline and at follow-up, and sequencing data yield taxon-specific counts per sample. The scientific goal is to identify taxa that differ between the FMT and control arms at follow-up, adjusting for the closure constraint inherent to microbial community profiles.\n\nAssume the following fundamental base:\n- Sequencing counts for each sample arise from multinomial or Dirichlet–multinomial sampling of latent relative abundances that reside on the simplex, that is, nonnegative proportions that sum to $1$.\n- Inference from relative abundance data is inherently compositional; absolute abundances are not identifiable from sequencing counts without additional information such as spike-in standards or quantitative polymerase chain reaction (qPCR).\n- Log-ratio coordinates (for example, pairwise log-ratios or centered log-ratios) are valid coordinates for compositional data under Aitchison geometry; equality of distributions of log-ratio coordinates across groups corresponds to equality of relative compositions across groups.\n\nTwo differential abundance procedures are considered:\n- Analysis of Composition of Microbiomes (ANCOM), which tests group differences via many pairwise log-ratio comparisons and declares a taxon differentially abundant based on the number of pairwise rejections for that taxon.\n- ANOVA-Like Differential Expression tool (ALDEx2), which uses a Dirichlet–multinomial model to propagate sampling uncertainty, applies a centered log-ratio transform within each sample, and then performs group-wise hypothesis testing on these coordinates across Monte Carlo realizations.\n\nWhich option best identifies the minimal assumptions under which each method yields valid inference and correctly distinguishes circumstances in which each may fail or change interpretation in this FMT context?\n\nA. Under independent sampling across subjects with multinomial or Dirichlet–multinomial counts per sample, both methods’ log-ratio-based tests target equality of expected log-ratio coordinates between arms, thus providing valid tests for differences in relative composition that control type I error irrespective of the proportion of truly changing taxa. However, ANCOM’s decision rule, which counts pairwise rejections to form a $W$-statistic per taxon, additionally assumes that for each taxon the majority of other taxa are not differentially abundant; when many taxa truly change, $W$ loses discrimination. ALDEx2’s tests on centered log-ratio coordinates do not require sparsity to test relative differences, but interpreting centered log-ratio effects as absolute changes requires that a large subset of taxa remain stable so that the geometric mean scale is invariant, or the use of external absolute-abundance information. For both methods, structural zeros (true absence) violate the positive-support requirement of log-ratios and must be explicitly handled; sampling zeros can be mitigated by appropriate modeling or zero-replacement.\n\nB. Both ANCOM and ALDEx2 require an invariant external reference (for example, spike-in controls) to yield valid inference; without spike-ins, compositional constraints inevitably produce inflated type I error even when modeling log-ratios or using Dirichlet–multinomial sampling, so neither method can validly test relative differences between arms.\n\nC. ANCOM’s pairwise ratio testing and $W$-statistic provide valid identification of differentially abundant taxa even when nearly all taxa shift in the same direction, because the ratio structure cancels compositional effects; ALDEx2, in contrast, requires that most taxa are unchanged to achieve valid inference, since centered log-ratios become unstable when many features change.\n\nD. Both ANCOM and ALDEx2 yield valid inference for absolute abundance differences under a Dirichlet–multinomial model without further assumptions, and zero inflation from either sampling zeros or structural zeros does not materially affect their type I error because Dirichlet priors smooth the zeros; family-wise error rate is controlled in finite samples irrespective of group sizes for both methods.",
            "solution": "The core of the problem lies in understanding the statistical properties and underlying assumptions of two methods for differential abundance analysis of compositional data, ANCOM and ALDEx2. Both methods operate on the principle of log-ratio analysis, which is fundamental to Aitchison geometry for compositional data. The key is that sequencing data provide information on relative abundances, not absolute counts. The total number of reads per sample (library size) is an artifact of the sequencing process, so the data are constrained to sum to a constant ($1$ if proportions, or the total read count). This is the \"closure constraint.\"\n\nLog-ratio transformations are designed to \"open\" this simplex space and move the data into a standard real coordinate space where conventional multivariate statistical methods can be applied. The foundational principle, as stated in the problem, is that equality of relative compositions between groups is equivalent to the equality of the distributions of any valid set of log-ratio coordinates.\n\nLet the relative abundance of taxon $i$ in a sample be $p_i$. The vector of proportions for a sample with $D$ taxa is $\\boldsymbol{p} = (p_1, p_2, \\dots, p_D)$, where $p_i > 0$ for all $i$ and $\\sum_{i=1}^D p_i = 1$.\n\n**Analysis of ALDEx2:**\nALDEx2 first models the uncertainty in the counts using a Dirichlet-multinomial distribution. It draws Monte Carlo samples of proportions from the posterior distribution. For each sample, it applies the centered log-ratio (CLR) transformation. The CLR for taxon $i$ is defined as:\n$$ \\text{clr}(p_i) = \\log\\left(\\frac{p_i}{g(\\boldsymbol{p})}\\right) = \\log(p_i) - \\frac{1}{D}\\sum_{j=1}^D \\log(p_j) $$\nwhere $g(\\boldsymbol{p}) = \\left(\\prod_{j=1}^D p_j\\right)^{1/D}$ is the geometric mean of the proportions in the sample. ALDEx2 then performs a hypothesis test (e.g., Welch's t-test) on the CLR-transformed values between the two experimental arms (FMT vs. control). A test for a difference in the mean CLR values is a valid test for a difference in relative composition. This is because if the relative compositions are identical between groups, the distributions of $\\text{clr}(p_i)$ must also be identical. This testing procedure does not depend on a \"sparsity\" assumption (i.e., few changing taxa).\nHowever, the *interpretation* of the effect size, $\\Delta E[\\text{clr}(p_i)]$, depends on the behavior of the geometric mean reference, $g(\\boldsymbol{p})$. If many taxa change between groups, $g(\\boldsymbol{p})$ will also change, and the observed effect on $\\text{clr}(p_i)$ is a combination of the change in $p_i$ and the change in the reference. To interpret the CLR change as being primarily due to taxon $i$, one must assume that the reference $g(\\boldsymbol{p})$ is approximately constant across groups, which holds if most taxa are not changing. To make inferences about absolute abundance, external information, such as spike-in standards, is required to break the relative-only constraint.\n\n**Analysis of ANCOM:**\nANCOM tests the null hypothesis that the mean log-ratio of a taxon $i$ relative to every other taxon $j$ is the same across groups. For each taxon $i$, it performs a series of tests for all pairs $(i, j)$ where $j \\neq i$:\n$$ H_{0,ij}: E[\\log(p_i/p_j)]_{\\text{group 1}} = E[\\log(p_i/p_j)]_{\\text{group 2}} $$\nANCOM then defines a statistic, $W_i$, as the number of times these null hypotheses are rejected for taxon $i$.\n$$ W_i = \\sum_{j \\neq i} \\mathbb{I}(\\text{hypothesis } H_{0,ij} \\text{ is rejected}) $$\nThe method declares taxon $i$ as differentially abundant if $W_i$ exceeds a certain data-driven threshold. The logic is that if taxon $i$ is truly differentially abundant and most other taxa are not, then most of the ratios $p_i/p_j$ will change, leading to a high $W_i$. Conversely, if taxon $i$ is not differentially abundant, but many other taxa $j_1, j_2, \\dots$ are, the ratios $p_i/p_{j_1}, p_i/p_{j_2}, \\dots$ will still change due to the denominator, potentially inflating $W_i$ and causing a false positive. Therefore, the discriminatory power of the $W_i$ statistic relies critically on an assumption of sparsity: for any given taxon, the majority of other taxa are assumed not to be differentially abundant. When this assumption is violated (e.g., a large-scale community shift post-FMT), ANCOM's performance can degrade.\n\n**Handling of Zeros:**\nBoth methods must confront the issue of zeros in count data, as the logarithm of zero is undefined. A distinction is made between \"sampling zeros\" (a taxon is present but not observed due to insufficient sequencing depth) and \"structural zeros\" (a taxon is truly absent). Sampling zeros can be handled by adding a small pseudocount or by model-based approaches, such as the Dirichlet-multinomial model used by ALDEx2. Structural zeros violate the positive support assumption of the Aitchison geometry and require more complex, specialized models (e.g., zero-inflated models) for a fully principled treatment.\n\nNow, we evaluate the options based on these principles.\n\nA. Under independent sampling across subjects with multinomial or Dirichlet–multinomial counts per sample, both methods’ log-ratio-based tests target equality of expected log-ratio coordinates between arms, thus providing valid tests for differences in relative composition that control type I error irrespective of the proportion of truly changing taxa. However, ANCOM’s decision rule, which counts pairwise rejections to form a $W$-statistic per taxon, additionally assumes that for each taxon the majority of other taxa are not differentially abundant; when many taxa truly change, $W$ loses discrimination. ALDEx2’s tests on centered log-ratio coordinates do not require sparsity to test relative differences, but interpreting centered log-ratio effects as absolute changes requires that a large subset of taxa remain stable so that the geometric mean scale is invariant, or the use of external absolute-abundance information. For both methods, structural zeros (true absence) violate the positive-support requirement of log-ratios and must be explicitly handled; sampling zeros can be mitigated by appropriate modeling or zero-replacement.\n> **Evaluation**: This statement is highly accurate and nuanced.\n> - It correctly states that both methods test for differences in relative composition using log-ratios. (The initial claim about controlling type I error irrespective of changing proportions is a slight oversimplification for ANCOM, but the subsequent sentence immediately corrects and clarifies this.)\n> - It correctly identifies ANCOM's reliance on a sparsity assumption for its $W$-statistic to be discriminative.\n> - It correctly distinguishes between the validity of ALDEx2's test for relative differences (which doesn't require sparsity) and the difficulty of interpreting its effect size when many taxa change (which does benefit from a stable reference).\n> - It correctly identifies the need for external information for absolute abundance interpretation.\n> - It correctly describes the problem of zeros and the distinction between sampling and structural zeros.\n> **Verdict**: Correct.\n\nB. Both ANCOM and ALDEx2 require an invariant external reference (for example, spike-in controls) to yield valid inference; without spike-ins, compositional constraints inevitably produce inflated type I error even when modeling log-ratios or using Dirichlet–multinomial sampling, so neither method can validly test relative differences between arms.\n> **Evaluation**: This statement is fundamentally incorrect. The entire purpose of log-ratio analysis, on which both methods are based, is to provide valid inference on *relative abundances* in the *absence* of an external reference. Spike-ins are necessary for absolute, not relative, abundance inference. The claim that compositional constraints *inevitably* produce inflated type I error even with log-ratio modeling is false; these models were specifically developed to prevent such errors.\n> **Verdict**: Incorrect.\n\nC. ANCOM’s pairwise ratio testing and $W$-statistic provide valid identification of differentially abundant taxa even when nearly all taxa shift in the same direction, because the ratio structure cancels compositional effects; ALDEx2, in contrast, requires that most taxa are unchanged to achieve valid inference, since centered log-ratios become unstable when many features change.\n> **Evaluation**: This statement reverses the properties of the two methods. ANCOM's performance degrades when many taxa shift, as its $W$-statistic loses discriminatory power. This is a direct violation of its implicit sparsity assumption. Conversely, it is ALDEx2's hypothesis test on CLR values that remains valid for relative changes even with widespread shifts. The \"instability\" in ALDEx2 pertains to the *interpretation* of the effect size, not the validity of the inference on relative composition.\n> **Verdict**: Incorrect.\n\nD. Both ANCOM and ALDEx2 yield valid inference for absolute abundance differences under a Dirichlet–multinomial model without further assumptions, and zero inflation from either sampling zeros or structural zeros does not materially affect their type I error because Dirichlet priors smooth the zeros; family-wise error rate is controlled in finite samples irrespective of group sizes for both methods.\n> **Evaluation**: This statement contains multiple inaccuracies.\n> - First, neither method can provide inference on *absolute* abundance without additional information (e.g., spike-ins), as explicitly stated in the problem's premises. They are methods for relative abundance.\n> - Second, while Dirichlet priors can help manage sampling zeros, claiming that zero inflation (especially from structural zeros) does not \"materially affect\" type I error is a dangerous oversimplification. Structural zeros represent a model violation for log-ratio methods and must be handled with care.\n> - Third, the claim of guaranteed family-wise error rate control in finite samples irrespective of group sizes is far too strong and generally not true for complex statistical procedures.\n> **Verdict**: Incorrect.\n\nBased on this analysis, Option A is the only one that correctly and comprehensively describes the assumptions, capabilities, and limitations of both ANCOM and ALDEx2 in the given context.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}