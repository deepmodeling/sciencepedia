{
    "hands_on_practices": [
        {
            "introduction": "A successful nanopore sequencing run begins with meticulous experimental design. A critical parameter that directly impacts both the speed of data acquisition and the longevity of the flow cell is the concentration of the DNA library to be loaded.\n\nThis practice challenges you to model the fundamental trade-off between faster molecule capture at high concentrations and the increased risk of pore clogging and subsequent downtime. By applying principles from renewal theory and mass-action kinetics, you will derive an optimal loading concentration $c^{\\star}$ that maximizes throughput while adhering to a practical reliability constraint, a task that mirrors real-world experimental optimization in a diagnostics lab .",
            "id": "5138899",
            "problem": "A single nanopore in an Oxford Nanopore Technologies (ONT) flow cell alternates between being available for capture and being unavailable due to sequencing or recovery. Consider a library of double-stranded DNA molecules of mean read length $L$ and bulk concentration $c$ in nanomolar (nM). When the pore is available, captures occur as a Poisson process with rate $\\lambda(c) = k c$ (mass-action capture), where $k$ is a concentration-normalized capture rate constant with units $\\mathrm{s}^{-1}\\,\\mathrm{nM}^{-1}$. Following a capture, the pore spends a fixed sequencing time $\\tau_{s}$ translocating the molecule. After sequencing completes, the pore may enter a recovery state due to transient blockage with probability $p(c)$, during which it is unavailable for a fixed time $\\tau_{r}$ before returning to availability. Assume (i) captures occur only when the pore is available, (ii) each capture yields a completed read of duration $\\tau_{s}$ regardless of subsequent recovery, (iii) recovery events occur independently after each read with probability $p(c)$ that increases with $c$, and (iv) waiting times to capture when available are exponentially distributed with mean $1/\\lambda(c)$.\n\nA widely used empirical hazard for recovery is $p(c) = 1 - \\exp(-\\gamma c)$, where $\\gamma$ is a positive parameter encoding the effective concentration-dependence of clogging. Under these assumptions, model the pore as an alternating renewal process with i.i.d. cycles consisting of an availability period (waiting for the next capture), a sequencing period of length $\\tau_{s}$, and, with probability $p(c)$, a recovery period of length $\\tau_{r}$. Let the long-run per-pore read throughput be defined as the reciprocal of the expected cycle length. To ensure robust operation for molecular and immunodiagnostics workflows, the fraction of total time spent in recovery must not exceed a reliability threshold $\\alpha$.\n\nUsing only the fundamental principles stated above (mass-action capture, Poisson arrivals when available, fixed sequencing and recovery times, independence of recovery events, and renewal theory for long-run averages), derive the throughput and the recovery-time fraction as functions of $c$, and then determine the loading concentration $c^{\\star}$ that maximizes throughput subject to the recovery-time fraction being no greater than $\\alpha$. Use the following parameter values: $k = 0.05\\,\\mathrm{s}^{-1}\\,\\mathrm{nM}^{-1}$, $\\tau_{s} = 2\\,\\mathrm{s}$, $\\tau_{r} = 10\\,\\mathrm{s}$, $\\gamma = 0.03\\,\\mathrm{nM}^{-1}$, and $\\alpha = 0.20$. Round your answer for $c^{\\star}$ to $3$ significant figures. Express the final concentration in nanomolar (nM).",
            "solution": "We begin from the stated principles. Let $c$ denote the bulk concentration in nanomolar (nM). When the pore is available, captures occur with rate $\\lambda(c) = k c$. Therefore, the waiting time $W$ to the next capture when the pore is available is exponentially distributed with mean $\\mathbb{E}[W] = 1/\\lambda(c) = \\frac{1}{k c}$. Each cycle consists of the random waiting time $W$, followed by a fixed sequencing time $\\tau_{s}$, and then, with probability $p(c)$, a fixed recovery time $\\tau_{r}$.\n\nBy linearity of expectation and independence, the expected cycle length $E[C(c)]$ is\n$$\nE[C(c)] = \\mathbb{E}[W] + \\tau_{s} + p(c)\\,\\tau_{r} = \\frac{1}{k c} + \\tau_{s} + p(c)\\,\\tau_{r}.\n$$\nUnder renewal theory (the renewal reward theorem), the long-run average number of completed reads per unit time equals the reciprocal of the expected cycle length. Thus the per-pore read throughput $T(c)$ (reads per second) is\n$$\nT(c) = \\frac{1}{E[C(c)]} = \\frac{1}{\\frac{1}{k c} + \\tau_{s} + p(c)\\,\\tau_{r}}.\n$$\nThe fraction of time spent in recovery, denoted $F_{\\mathrm{rec}}(c)$, is the long-run fraction of downtime attributable to recovery within each cycle, i.e.,\n$$\nF_{\\mathrm{rec}}(c) = \\frac{p(c)\\,\\tau_{r}}{E[C(c)]} = \\frac{p(c)\\,\\tau_{r}}{\\frac{1}{k c} + \\tau_{s} + p(c)\\,\\tau_{r}}.\n$$\nThe reliability constraint requires $F_{\\mathrm{rec}}(c) \\leq \\alpha$. Because $p(c)$ is increasing in $c$ and $\\frac{1}{k c}$ is decreasing in $c$, $F_{\\mathrm{rec}}(c)$ is increasing in $c$ for the parameter regime of interest. Meanwhile, $E[C(c)]$ initially decreases with $c$ due to the $\\frac{1}{k c}$ term and increases slowly via $p(c)\\,\\tau_{r}$, yielding a potentially unconstrained optimum at some interior $c$ where $\\frac{\\mathrm{d}}{\\mathrm{d}c}E[C(c)] = 0$. However, if the unconstrained maximizer violates $F_{\\mathrm{rec}}(c) \\leq \\alpha$, then the constrained maximizer is the largest $c$ such that $F_{\\mathrm{rec}}(c) = \\alpha$.\n\nWe now instantiate $p(c)$ as $p(c) = 1 - \\exp(-\\gamma c)$ and use the provided parameters $k = 0.05\\,\\mathrm{s}^{-1}\\,\\mathrm{nM}^{-1}$, $\\tau_{s} = 2\\,\\mathrm{s}$, $\\tau_{r} = 10\\,\\mathrm{s}$, $\\gamma = 0.03\\,\\mathrm{nM}^{-1}$, and $\\alpha = 0.20$.\n\nFirst, we verify whether the unconstrained minimizer of $E[C(c)]$ is feasible. Differentiate $E[C(c)]$:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}c}E[C(c)] = -\\frac{1}{k c^{2}} + \\tau_{r}\\,\\frac{\\mathrm{d}}{\\mathrm{d}c}p(c) = -\\frac{1}{k c^{2}} + \\tau_{r}\\,\\gamma\\,\\exp(-\\gamma c).\n$$\nSetting $\\frac{\\mathrm{d}}{\\mathrm{d}c}E[C(c)] = 0$ gives\n$$\n\\tau_{r}\\,\\gamma\\,\\exp(-\\gamma c) = \\frac{1}{k c^{2}}.\n$$\nThis transcendental equation has a closed-form solution in terms of the Lambert $W$ function. Rearranging,\n$$\n\\exp(\\gamma c) = k \\tau_{r} \\gamma\\, c^{2}.\n$$\nLet $y = \\gamma c$. Then $\\exp(y) = \\frac{k \\tau_{r} \\gamma}{\\gamma^{2}}\\, y^{2} = \\frac{k \\tau_{r}}{\\gamma}\\, y^{2}$, so $y^{2}\\exp(-y) = \\frac{\\gamma}{k \\tau_{r}}$. Setting $y = 2 z$ yields $(z \\exp(-z))^{2} = \\frac{\\gamma}{4 k \\tau_{r}}$, hence $z \\exp(-z) = \\frac{1}{2}\\sqrt{\\frac{\\gamma}{k \\tau_{r}}}$. Let $u = -z$, so $u \\exp(u) = -\\frac{1}{2}\\sqrt{\\frac{\\gamma}{k \\tau_{r}}}$ and $u = W\\!\\left(-\\frac{1}{2}\\sqrt{\\frac{\\gamma}{k \\tau_{r}}}\\right)$. Therefore,\n$$\nc_{\\mathrm{uncon}} = -\\frac{2}{\\gamma}\\,W\\!\\left(-\\frac{1}{2}\\sqrt{\\frac{\\gamma}{k \\tau_{r}}}\\right).\n$$\nSubstituting the parameters gives a real value on the principal branch $W_{0}$. Numerically, this yields $c_{\\mathrm{uncon}} \\approx 9.4$ in nanomolar. Evaluating $F_{\\mathrm{rec}}(c)$ at this $c$ gives a recovery fraction larger than $\\alpha = 0.20$, so the unconstrained optimum is infeasible.\n\nHence, the constrained optimum is attained at the boundary $F_{\\mathrm{rec}}(c) = \\alpha$, i.e.,\n$$\n\\frac{p(c)\\,\\tau_{r}}{\\frac{1}{k c} + \\tau_{s} + p(c)\\,\\tau_{r}} = \\alpha,\n$$\nwith $p(c) = 1 - \\exp(-\\gamma c)$. Solve this equation for $c$:\n$$\n\\alpha\\left(\\frac{1}{k c} + \\tau_{s} + p(c)\\,\\tau_{r}\\right) = p(c)\\,\\tau_{r}\n\\quad\\Rightarrow\\quad\n\\alpha\\left(\\frac{1}{k c} + \\tau_{s}\\right) = p(c)\\,\\tau_{r}(1 - \\alpha).\n$$\nThus,\n$$\np(c) = \\frac{\\alpha\\left(\\frac{1}{k c} + \\tau_{s}\\right)}{\\tau_{r}(1 - \\alpha)}.\n$$\nEquating to $1 - \\exp(-\\gamma c)$ yields the scalar equation\n$$\n1 - \\exp(-\\gamma c) = \\frac{\\alpha\\left(\\frac{1}{k c} + \\tau_{s}\\right)}{\\tau_{r}(1 - \\alpha)}.\n$$\nWith the given parameters, this becomes\n$$\n1 - \\exp(-0.03\\, c) = \\frac{0.20\\left(\\frac{1}{0.05\\, c} + 2\\right)}{10\\,(1 - 0.20)} = \\frac{0.20\\left(\\frac{1}{0.05\\, c} + 2\\right)}{8} = \\frac{\\frac{0.20}{0.05\\, c} + 0.40}{8} = \\frac{\\frac{4}{c} + 0.40}{8} = \\frac{0.5}{c} + 0.05.\n$$\nWe solve\n$$\n1 - \\exp(-0.03\\, c) = 0.05 + \\frac{0.5}{c}.\n$$\nThe left-hand side is increasing in $c$, and the right-hand side is decreasing in $c$, so there is a unique solution. A monotone bracketing and refinement yields:\n\n- At $c = 5$, $\\text{LHS} = 1 - \\exp(-0.15) \\approx 0.139$, $\\text{RHS} = 0.05 + 0.1 = 0.15$ (LHS $<$ RHS).\n- At $c = 5.5$, $\\text{LHS} = 1 - \\exp(-0.165) \\approx 0.152$, $\\text{RHS} \\approx 0.05 + 0.0909 = 0.1409$ (LHS $>$ RHS).\n- Refining, at $c = 5.24$, $\\text{LHS} = 1 - \\exp(-0.1572) \\approx 0.1454$, $\\text{RHS} \\approx 0.05 + 0.09542 = 0.14542$.\n\nThis gives $c^{\\star} \\approx 5.24$ nanomolar. Rounding to $3$ significant figures yields $c^{\\star} = 5.24$ nanomolar. This satisfies $F_{\\mathrm{rec}}(c^{\\star}) = \\alpha$ and maximizes throughput subject to the reliability constraint.",
            "answer": "$$\\boxed{5.24}$$"
        },
        {
            "introduction": "Once a DNA strand is being processed by the pore, the core of nanopore sequencing lies in interpreting the resulting fluctuations in ionic current. This raw electrical signal is inherently noisy and must be decoded to infer the underlying sequence of molecular states corresponding to the translocating nucleotides.\n\nIn this exercise, you will implement a simplified yet conceptually powerful model for this process using a Hidden Markov Model (HMM), where the molecular states are hidden and the current levels are observable. By implementing the Viterbi algorithm, you will gain hands-on experience decoding the most probable sequence of states from a series of observations, providing fundamental insight into the computational challenges at the heart of real-time basecalling .",
            "id": "5138906",
            "problem": "You are given a simplified model of a nanopore strand sequencing readout in which the ionic current is modeled as a Hidden Markov Model (HMM) where hidden states represent discrete step states of the strand (for example, short nucleotide contexts) and emissions are noisy averaged ionic current measurements per step. The physical context is a nanopore system in molecular and immunodiagnostics, where ionic current levels, measured in picoamperes (pA), are characteristic of local strand configurations, and the strand advances in discrete steps controlled by a motor protein. You must construct likelihoods for step states based on a Gaussian emission model and implement a Viterbi decoder to infer the most probable sequence of step states from the noisy averaged current traces.\n\nFundamental base and definitions to use:\n- Hidden Markov Model (HMM): a stochastic model with hidden states and observed emissions. Let the hidden state at time index $t$ be $s_t \\in \\{0,1,\\dots,M-1\\}$ with state transition probabilities $A_{ij} = \\mathbb{P}(s_t = j \\mid s_{t-1} = i)$ and an initial distribution $\\pi_j = \\mathbb{P}(s_0 = j)$.\n- Emission model: for each state $j$, the averaged ionic current per step $\\bar{I}_t$ is modeled as a Gaussian (Normal) random variable with mean $\\mu_j$ and variance $\\sigma^2/K$, i.e., $\\bar{I}_t \\sim \\mathcal{N}(\\mu_j, \\sigma^2/K)$, where $\\sigma$ is the per-sample standard deviation and $K$ is the number of samples averaged per step. The likelihood is \n$$\np(\\bar{I}_t \\mid s_t=j) = \\frac{1}{\\sqrt{2\\pi \\sigma^2/K}}\\exp\\left(-\\frac{(\\bar{I}_t - \\mu_j)^2}{2\\sigma^2/K}\\right).\n$$\n- Maximum a posteriori decoding via the Viterbi algorithm: compute the most probable state path $s_0,s_1,\\dots,s_{T-1}$ given observations $\\bar{I}_0,\\bar{I}_1,\\dots,\\bar{I}_{T-1}$ by dynamic programming in the log domain using natural logarithms.\n\nYour task:\n1. For each test case, construct emission log-likelihoods $\\log p(\\bar{I}_t \\mid s_t=j)$ using the Gaussian model above, with variance $\\sigma^2/K$. Then implement the Viterbi dynamic programming recurrences:\n$$\n\\delta_t(j) = \\max_i \\left[\\delta_{t-1}(i) + \\log A_{ij}\\right] + \\log p(\\bar{I}_t \\mid s_t=j),\n$$\nwith initialization\n$$\n\\delta_0(j) = \\log \\pi_j + \\log p(\\bar{I}_0 \\mid s_0=j),\n$$\nand maintain backpointers to reconstruct the maximizing path. Use natural logarithms for all log operations.\n2. Decode the most probable state sequence for each test case independently.\n3. Express the final decoded sequences as lists of integers (state indices), with no physical units in the output. All physical units mentioned below are for the model specification only; the output is unitless.\n4. Your program should produce a single line of output containing the decoded sequences for all test cases as a comma-separated list enclosed in square brackets, where each decoded sequence is represented as a list of integers, e.g., $[[0,1,2],[1,1,0]]$.\n\nTest suite:\n- Test Case 1 (general \"happy path\"):\n  - Number of states: $M = 4$.\n  - State means (picoamperes): $\\mu = [64.8, 67.6, 62.9, 70.5]$.\n  - Per-sample standard deviation (picoamperes): $\\sigma = 2.0$.\n  - Samples per step: $K = 5$.\n  - Transition matrix:\n    $$\n    A = \\begin{bmatrix}\n    0.70 & 0.25 & 0.03 & 0.02 \\\\\n    0.15 & 0.70 & 0.13 & 0.02 \\\\\n    0.02 & 0.15 & 0.70 & 0.13 \\\\\n    0.02 & 0.03 & 0.25 & 0.70\n    \\end{bmatrix}.\n    $$\n  - Initial distribution: $\\pi = [0.25, 0.25, 0.25, 0.25]$.\n  - Number of steps: $T = 8$.\n  - Averaged observations (picoamperes): $\\bar{I} = [65.1, 67.3, 67.8, 62.2, 70.3, 69.8, 63.1, 66.9]$.\n\n- Test Case 2 (overlapping state means, challenging emissions):\n  - Number of states: $M = 3$.\n  - State means (picoamperes): $\\mu = [66.0, 66.6, 66.9]$.\n  - Per-sample standard deviation (picoamperes): $\\sigma = 1.2$.\n  - Samples per step: $K = 10$.\n  - Transition matrix:\n    $$\n    A = \\begin{bmatrix}\n    0.60 & 0.35 & 0.05 \\\\\n    0.20 & 0.60 & 0.20 \\\\\n    0.05 & 0.35 & 0.60\n    \\end{bmatrix}.\n    $$\n  - Initial distribution: $\\pi = [\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}]$.\n  - Number of steps: $T = 6$.\n  - Averaged observations (picoamperes): $\\bar{I} = [65.8, 66.9, 66.6, 66.5, 66.7, 66.1]$.\n\n- Test Case 3 (boundary and short sequence):\n  - Number of states: $M = 2$.\n  - State means (picoamperes): $\\mu = [63.0, 69.0]$.\n  - Per-sample standard deviation (picoamperes): $\\sigma = 3.0$.\n  - Samples per step: $K = 2$.\n  - Transition matrix:\n    $$\n    A = \\begin{bmatrix}\n    0.80 & 0.20 \\\\\n    0.20 & 0.80\n    \\end{bmatrix}.\n    $$\n  - Initial distribution: $\\pi = [0.5, 0.5]$.\n  - Number of steps: $T = 1$.\n  - Averaged observations (picoamperes): $\\bar{I} = [68.2]$.\n\nAngle units are not applicable. Percentages do not appear in the output. The ionic current values used in the model are given in picoamperes (pA), but your programâ€™s output must be the decoded state index sequences without units. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[[0,1,2],[2,1,0],[1]]$.",
            "solution": "The posed problem requires the decoding of the most probable sequence of hidden states from a series of noisy observations, a canonical task in the analysis of sequential data. The system is modeled as a Hidden Markov Model (HMM), a framework well-suited for describing the stochastic process underlying nanopore sequencing where a biopolymer, such as DNA, translocates through a nanopore, giving rise to a sequence of characteristic ionic current levels. The Viterbi algorithm provides an exact and computationally efficient solution to this decoding problem via dynamic programming.\n\nThe solution proceeds by first formalizing the HMM components and then implementing the Viterbi algorithm in the numerically stable logarithmic domain.\n\nA Hidden Markov Model is defined by a set of parameters $(\\mathcal{S}, \\mathcal{V}, A, B, \\pi)$, where:\n- $\\mathcal{S} = \\{0, 1, \\dots, M-1\\}$ is the finite set of $M$ hidden states, representing local strand configurations.\n- $\\mathcal{V}$ is the set of possible observations. In this problem, observations are continuous, representing the averaged ionic current $\\bar{I}_t \\in \\mathbb{R}$.\n- $A$ is the state transition probability matrix, where $A_{ij} = \\mathbb{P}(s_t = j \\mid s_{t-1} = i)$ is the probability of transitioning from state $i$ to state $j$.\n- $B$ represents the emission probabilities, $p(\\bar{I}_t \\mid s_t=j)$, which is the probability of observing current $\\bar{I}_t$ given the system is in state $j$.\n- $\\pi$ is the initial state distribution, where $\\pi_j = \\mathbb{P}(s_0 = j)$.\n\nThe problem specifies a Gaussian emission model for the averaged ionic current $\\bar{I}_t$ at time step $t$. Given the hidden state is $s_t=j$, the observation $\\bar{I}_t$ is drawn from a Normal distribution $\\mathcal{N}(\\mu_j, \\sigma_{avg}^2)$, where $\\mu_j$ is the mean current for state $j$ and $\\sigma_{avg}^2 = \\sigma^2/K$ is the variance of the averaged measurement. Here, $\\sigma$ is the per-sample standard deviation and $K$ is the number of samples averaged per step.\n\nThe probability density function for an observation $\\bar{I}_t$ given state $j$ is:\n$$\np(\\bar{I}_t \\mid s_t=j) = \\frac{1}{\\sqrt{2\\pi \\sigma^2/K}}\\exp\\left(-\\frac{(\\bar{I}_t - \\mu_j)^2}{2\\sigma^2/K}\\right)\n$$\nDirectly multiplying these probabilities for a long sequence can lead to numerical underflow. Therefore, all computations are performed using natural logarithms. The log-likelihood of an emission is:\n$$\n\\log p(\\bar{I}_t \\mid s_t=j) = -\\frac{1}{2}\\log(2\\pi \\sigma^2/K) - \\frac{(\\bar{I}_t - \\mu_j)^2}{2\\sigma^2/K}\n$$\nThese log-likelihoods are computed for each time step $t \\in \\{0, \\dots, T-1\\}$ and each state $j \\in \\{0, \\dots, M-1\\}$.\n\nThe Viterbi algorithm finds the optimal state sequence $S^* = (s_0^*, s_1^*, \\dots, s_{T-1}^*)$ by maximizing the joint log-probability of the state sequence and the observations. This is achieved through dynamic programming. We define two tables:\n$1$. $\\delta_t(j)$: The maximum log-probability of any path of length $t+1$ that ends in state $j$ and generates observations $\\bar{I}_0, \\dots, \\bar{I}_t$.\n$2$. $\\psi_t(j)$: A backpointer table to store the state at time $t-1$ that maximizes the probability of reaching state $j$ at time $t$.\n\nThe algorithm proceeds in three stages:\n\n**$1$. Initialization ($t=0$):**\nFor each state $j \\in \\{0, \\dots, M-1\\}$, the initial log-probability is the sum of the log initial probability and the log-emission probability for the first observation $\\bar{I}_0$.\n$$\n\\delta_0(j) = \\log \\pi_j + \\log p(\\bar{I}_0 \\mid s_0=j)\n$$\n\n**$2$. Recursion ($t=1, \\dots, T-1$):**\nFor each subsequent time step $t$, we extend the optimal paths from time $t-1$. For each state $j$, we find the previous state $i$ that maximizes the probability of transitioning to $j$ and generating observation $\\bar{I}_t$.\n$$\n\\delta_t(j) = \\max_{i \\in \\{0, \\dots, M-1\\}} \\left[ \\delta_{t-1}(i) + \\log A_{ij} \\right] + \\log p(\\bar{I}_t \\mid s_t=j)\n$$\nThe backpointer for state $j$ at time $t$ stores the index of this maximizing previous state $i$:\n$$\n\\psi_t(j) = \\arg\\max_{i \\in \\{0, \\dots, M-1\\}} \\left[ \\delta_{t-1}(i) + \\log A_{ij} \\right]\n$$\n\n**$3$. Termination and Path Reconstruction:**\nAfter computing $\\delta_{T-1}(j)$ for all states $j$, the most probable final state is the one with the highest log-probability:\n$$\ns^*_{T-1} = \\arg\\max_{j \\in \\{0, \\dots, M-1\\}} \\delta_{T-1}(j)\n$$\nThe remainder of the most probable path is found by backtracking from this final state using the stored backpointers:\n$$\ns^*_{t-1} = \\psi_t(s^*_t) \\quad \\text{for } t = T-1, \\dots, 1\n$$\nThis procedure yields the complete optimal state sequence $S^*=(s_0^*, s_1^*, \\dots, s_{T-1}^*)$, which is the solution for a given test case. The implementation will process each test case provided in the problem statement independently using this algorithm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef viterbi_decode(M, mu, sigma, K, A, pi, T, I_bar):\n    \"\"\"\n    Decodes the most probable sequence of hidden states in an HMM using the Viterbi algorithm.\n    All calculations are performed in the log domain for numerical stability.\n\n    Args:\n        M (int): Number of hidden states.\n        mu (list or np.ndarray): Mean ionic current for each state.\n        sigma (float): Per-sample standard deviation of the ionic current.\n        K (int): Number of samples averaged per step.\n        A (list of lists or np.ndarray): State transition probability matrix.\n        pi (list or np.ndarray): Initial state probability distribution.\n        T (int): Number of observations (time steps).\n        I_bar (list or np.ndarray): Sequence of averaged ionic current observations.\n\n    Returns:\n        list: The most probable sequence of state indices.\n    \"\"\"\n    num_states = M\n    num_observations = T\n\n    # Convert inputs to numpy arrays for vectorized operations\n    mu = np.array(mu)\n    # Ensure A and pi are numpy arrays for log operations\n    A = np.array(A)\n    pi = np.array(pi)\n    I_bar = np.array(I_bar)\n\n    # 1. Preparation: Compute log probabilities\n    # Use np.errstate to suppress warnings for log(0), which is correctly handled as -np.inf\n    with np.errstate(divide='ignore'):\n        log_A = np.log(A)\n        log_pi = np.log(pi)\n\n    # Compute emission log-likelihoods for all observations and states.\n    # The matrix emission_log_lik will have shape (T, M), where B[t, j] = log p(I_bar[t] | s_t = j).\n    var_avg = (sigma**2) / K\n    std_avg = np.sqrt(var_avg)\n    \n    emission_log_lik = np.zeros((num_observations, num_states))\n    for t in range(num_observations):\n        # norm.logpdf calculates the log probability density for I_bar[t] under\n        # a normal distribution for each state's mean mu[j] and the common scale std_avg.\n        emission_log_lik[t, :] = norm.logpdf(I_bar[t], loc=mu, scale=std_avg)\n\n    # 2. Dynamic Programming tables\n    # delta[t, j]: max log-prob of a path ending in state j at time t\n    delta = np.zeros((num_observations, num_states))\n    # psi[t, j]: backpointer to the previous state on the optimal path to j at t\n    psi = np.zeros((num_observations, num_states), dtype=int)\n\n    # 3. Initialization step (t=0)\n    delta[0, :] = log_pi + emission_log_lik[0, :]\n\n    # 4. Recursion step (t=1 to T-1)\n    if num_observations > 1:\n        for t in range(1, num_observations):\n            for j in range(num_states):\n                # For each current state j, find the best previous state i.\n                # The term inside the max is delta[t-1, i] + log_A[i, j].\n                temp_log_prob = delta[t-1, :] + log_A[:, j]\n                \n                # Find the maximum log probability and the corresponding state index\n                max_log_prob = np.max(temp_log_prob)\n                argmax_index = np.argmax(temp_log_prob)\n                \n                # Update the DP tables\n                delta[t, j] = max_log_prob + emission_log_lik[t, j]\n                psi[t, j] = argmax_index\n\n    # 5. Termination and Path Reconstruction (Backtracking)\n    path = np.zeros(num_observations, dtype=int)\n    \n    # Find the most probable final state\n    path[num_observations - 1] = np.argmax(delta[num_observations - 1, :])\n    \n    # Backtrack to find the rest of the optimal path\n    for t in range(num_observations - 2, -1, -1):\n        path[t] = psi[t + 1, path[t + 1]]\n\n    return path.tolist()\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        (\n            4, # M\n            [64.8, 67.6, 62.9, 70.5], # mu\n            2.0, # sigma\n            5, # K\n            [[0.70, 0.25, 0.03, 0.02],\n             [0.15, 0.70, 0.13, 0.02],\n             [0.02, 0.15, 0.70, 0.13],\n             [0.02, 0.03, 0.25, 0.70]], # A\n            [0.25, 0.25, 0.25, 0.25], # pi\n            8, # T\n            [65.1, 67.3, 67.8, 62.2, 70.3, 69.8, 63.1, 66.9] # I_bar\n        ),\n        # Test Case 2\n        (\n            3, # M\n            [66.0, 66.6, 66.9], # mu\n            1.2, # sigma\n            10, # K\n            [[0.60, 0.35, 0.05],\n             [0.20, 0.60, 0.20],\n             [0.05, 0.35, 0.60]], # A\n            [1/3, 1/3, 1/3], # pi\n            6, # T\n            [65.8, 66.9, 66.6, 66.5, 66.7, 66.1] # I_bar\n        ),\n        # Test Case 3\n        (\n            2, # M\n            [63.0, 69.0], # mu\n            3.0, # sigma\n            2, # K\n            [[0.80, 0.20],\n             [0.20, 0.80]], # A\n            [0.5, 0.5], # pi\n            1, # T\n            [68.2] # I_bar\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        M, mu, sigma, K, A, pi, T, I_bar = case\n        decoded_path = viterbi_decode(M, mu, sigma, K, A, pi, T, I_bar)\n        results.append(decoded_path)\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list of lists contains spaces.\n    # The required format is compact, without spaces.\n    print(str(results).replace(' ', ''))\n\nsolve()\n\n```"
        },
        {
            "introduction": "Individual nanopore reads, while impressively long, contain inherent errors. For high-stakes applications like clinical immunodiagnostics and variant calling, it is essential to combine information from multiple independent reads to produce a single, highly accurate consensus sequence.\n\nThis problem explores the statistical foundation that underpins the reliability of sequencing data. Using the binomial distribution as a model for random errors, you will see how increasing sequencing depth, or coverage, powerfully suppresses errors through a majority vote. You will calculate the minimum coverage depth $N$ required to achieve a desired Phred quality score, a critical skill for designing experiments and validating that the final results meet the stringent accuracy standards for diagnostic confidence .",
            "id": "5138841",
            "problem": "A molecular and immunodiagnostics laboratory uses nanopore strand sequencing to assemble a per-base consensus across independent single-strand reads to detect single-nucleotide variants in a clinically relevant locus. Assume that each independent read covering a given base has identical and independent base-call performance: with probability $a$ it reports the true base, and with probability $1-a$ it reports some incorrect base. To suppress bias from error mode heterogeneity, assume errors are symmetric among the three incorrect bases. The laboratory constructs a consensus base at coverage depth $N$ (an odd integer to avoid ties) by majority vote over the $N$ base calls. The Phred quality score (PQS) for a consensus call is defined such that if the probability of a consensus error is $e$, then the consensus quality is $Q=-10\\log_{10}(e)$.\n\nStarting from the definitions of independence, the binomial distribution, and the Phred quality score, derive an analytic expression for the consensus accuracy $A(N)$ as a function of coverage depth $N$ and per-read correctness $a$. Then, specializing to $a=0.95$, determine the minimal odd coverage depth $N$ required so that the consensus Phred quality is at least $Q=30$ (that is, the consensus error probability $e$ is no greater than $10^{-3}$). Report the minimal odd $N$ as your final answer. No rounding is required.",
            "solution": "Let $a$ be the probability that a single, independent read correctly identifies a base. The probability of an incorrect base call is then $1-a$. The coverage depth, $N$, is the number of independent reads covering the base, and it is given to be an odd integer.\n\nA consensus call is formed by a majority vote. Let $k$ be the number of reads that correctly identify the true base out of $N$ total reads. These $N$ reads are independent Bernoulli trials, so the number of correct reads $k$ follows a binomial distribution:\n$$P(k; N, a) = \\binom{N}{k} a^k (1-a)^{N-k}$$\nFor the consensus call to be correct, the true base must be called more frequently than any other base. The problem states that errors are symmetric, meaning the $1-a$ probability of an error is distributed equally among the three other possible bases. If the number of correct reads $k$ is greater than half the total reads, $N/2$, then the number of incorrect reads, $N-k$, must be less than $N/2$. Since these $N-k$ reads are distributed among three incorrect bases, no single incorrect base can possibly receive more votes than $k$. Therefore, the condition for a correct consensus call is that the number of correct reads, $k$, constitutes a majority.\n\nSince $N$ is an odd integer, a majority is achieved if $k > N/2$. The smallest integer $k$ satisfying this is $k = (N+1)/2$. The consensus is correct if $k$ is any integer from $(N+1)/2$ to $N$.\n\nThe consensus accuracy, $A(N)$, is the probability that the consensus call is correct. This is the sum of probabilities for all values of $k$ that result in a correct consensus:\n$$A(N) = P\\left(k \\ge \\frac{N+1}{2}\\right) = \\sum_{k=(N+1)/2}^{N} P(k; N, a)$$\nSubstituting the binomial probability mass function, we arrive at the analytic expression for the consensus accuracy as a function of $N$ and $a$:\n$$A(N) = \\sum_{k=(N+1)/2}^{N} \\binom{N}{k} a^k (1-a)^{N-k}$$\nThis is the first part of the required derivation.\n\nThe probability of a consensus error, $e$, is the complement of the accuracy: $e(N) = 1 - A(N)$. This corresponds to the case where the number of correct reads is in the minority, i.e., $k < (N+1)/2$. Since $k$ is an integer, this is equivalent to $k \\le (N-1)/2$.\n$$e(N) = P\\left(k \\le \\frac{N-1}{2}\\right) = \\sum_{k=0}^{(N-1)/2} \\binom{N}{k} a^k (1-a)^{N-k}$$\nThe Phred quality score, $Q$, is defined as $Q = -10\\log_{10}(e)$. The problem requires finding the minimal odd $N$ such that $Q \\ge 30$ for a per-read accuracy of $a=0.95$.\nThe condition $Q \\ge 30$ translates to:\n$$-10\\log_{10}(e) \\ge 30$$\n$$\\log_{10}(e) \\le -3$$\n$$e \\le 10^{-3}$$\nWe need to find the smallest odd integer $N$ for which $e(N) \\le 0.001$, given $a=0.95$ and $1-a=0.05$. We will test successive odd values of $N$.\n\nFor $N=1$:\nThe error condition is $k \\le (1-1)/2 = 0$, so $k=0$.\n$e(1) = \\binom{1}{0} a^0 (1-a)^1 = 1-a = 0.05$.\n$0.05 > 0.001$, so $N=1$ is insufficient.\n\nFor $N=3$:\nThe error condition is $k \\le (3-1)/2 = 1$, so $k=0$ or $k=1$.\n$e(3) = \\sum_{k=0}^{1} \\binom{3}{k} a^k (1-a)^{3-k} = \\binom{3}{0}a^0(1-a)^3 + \\binom{3}{1}a^1(1-a)^2$\n$e(3) = (1-a)^3 + 3a(1-a)^2 = (0.05)^3 + 3(0.95)(0.05)^2$\n$e(3) = 0.000125 + 3(0.95)(0.0025) = 0.000125 + 0.007125 = 0.00725$.\n$0.00725 > 0.001$, so $N=3$ is insufficient.\n\nFor $N=5$:\nThe error condition is $k \\le (5-1)/2 = 2$, so $k=0, 1, 2$.\n$e(5) = \\sum_{k=0}^{2} \\binom{5}{k} a^k (1-a)^{5-k} = \\binom{5}{0}(1-a)^5 + \\binom{5}{1}a(1-a)^4 + \\binom{5}{2}a^2(1-a)^3$\n$e(5) = (0.05)^5 + 5(0.95)(0.05)^4 + 10(0.95)^2(0.05)^3$\n$e(5) = 3.125 \\times 10^{-7} + 5(0.95)(6.25 \\times 10^{-6}) + 10(0.9025)(1.25 \\times 10^{-4})$\n$e(5) = 3.125 \\times 10^{-7} + 2.96875 \\times 10^{-5} + 1.128125 \\times 10^{-3}$\n$e(5) = 0.0000003125 + 0.0000296875 + 0.001128125 = 0.001158125$.\n$0.001158125 > 0.001$, so $N=5$ is insufficient.\n\nFor $N=7$:\nThe error condition is $k \\le (7-1)/2 = 3$, so $k=0, 1, 2, 3$.\n$e(7) = \\sum_{k=0}^{3} \\binom{7}{k} a^k (1-a)^{7-k}$\n$e(7) = \\binom{7}{0}(1-a)^7 + \\binom{7}{1}a(1-a)^6 + \\binom{7}{2}a^2(1-a)^5 + \\binom{7}{3}a^3(1-a)^4$\n$e(7) = (0.05)^7 + 7(0.95)(0.05)^6 + 21(0.95)^2(0.05)^5 + 35(0.95)^3(0.05)^4$\n$e(7) = (7.8125 \\times 10^{-10}) + 7(0.95)(1.5625 \\times 10^{-8}) + 21(0.9025)(3.125 \\times 10^{-7}) + 35(0.857375)(6.25 \\times 10^{-6})$\n$e(7) = 7.8125 \\times 10^{-10} + 1.0390625 \\times 10^{-7} + 5.9228515625 \\times 10^{-6} + 1.8754296875 \\times 10^{-4}$\n$e(7) \\approx 1.9357 \\times 10^{-4}$.\n$1.9357 \\times 10^{-4} = 0.00019357$, which is less than $0.001$.\nThus, $N=7$ is sufficient to achieve the desired quality score.\n\nSince coverage depth $N=5$ is insufficient and $N=7$ is sufficient, the minimal odd coverage depth required is $N=7$.",
            "answer": "$$\\boxed{7}$$"
        }
    ]
}