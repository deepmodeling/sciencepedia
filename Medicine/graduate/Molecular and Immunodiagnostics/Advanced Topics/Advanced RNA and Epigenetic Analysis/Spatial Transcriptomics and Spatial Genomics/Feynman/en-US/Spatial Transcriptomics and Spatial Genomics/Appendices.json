{
    "hands_on_practices": [
        {
            "introduction": "In array-based spatial transcriptomics, each data point, or 'spot,' captures the genetic material from a mixture of cells. Understanding this cell multiplicity is fundamental to correctly interpreting the resulting expression data. This practice  guides you through building a model from first principles, applying concepts from stochastic geometry to estimate the expected number of cells contributing to a single spot's signal.",
            "id": "5163993",
            "problem": "In a spatially resolved gene expression assay such as Spatial Transcriptomics (ST), a tissue section is placed on an array of circular capture spots of diameter $d_{s}$ that collect messenger ribonucleic acid (mRNA) from any cell body that geometrically intersects a spot. Consider a two-dimensional tissue layer where cells are modeled as non-overlapping circular disks with random diameters $D$. Assume:\n- Cell centers follow a homogeneous Poisson point process (PPP) in two dimensions with intensity $n$ (cells per unit area).\n- The cell body diameter $D$ is an independent random variable with probability distribution having finite first and second moments.\n- The cellular area fraction (areal packing fraction) of the tissue is $\\phi \\in (0,1)$, defined as the fraction of area occupied by cells.\n- A cell contributes to a spot’s multiplicity if and only if its disk intersects the spot’s disk.\n\nUsing only foundational definitions for homogeneous PPPs, linearity of expectation, and standard results on the area of Minkowski dilations (Steiner’s formula for convex sets), perform the following:\n\n1. Derive an analytic expression for the expected cell multiplicity per spot, $\\mathbb{E}[M]$, as a function of $d_{s}$, $\\phi$, and the first two moments of $D$. Your derivation should start from:\n   - The PPP definition that the expected count in a region equals intensity times area.\n   - The relationship between intensity $n$ and the packing fraction $\\phi$ under a non-overlap approximation.\n   - The area of the dilation of a disk by another disk, extended by linearity to a random cell radius.\n   Express your final symbolic result in terms of $d_{s}$, $\\phi$, $\\mathbb{E}[D]$, and $\\mathbb{E}[D^{2}]$.\n\n2. Specialize your expression to the case where $D$ follows a Gamma distribution with mean $\\bar{d}$ and coefficient of variation $c_{v}$. Express $\\mathbb{E}[M]$ purely in terms of $d_{s}$, $\\phi$, $\\bar{d}$, and $c_{v}$.\n\n3. Evaluate the expected multiplicity numerically for a tissue with packing fraction $\\phi = $ $0.7$, spot diameter $d_{s} = $ $55$ micrometers, and a Gamma diameter distribution with mean $\\bar{d} = $ $12$ micrometers and coefficient of variation $c_{v} = $ $0.25$. Express the final result as a dimensionless expected count (cells per spot) and round your answer to four significant figures.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of stochastic geometry and probability theory, well-posed with a clear objective, and provides a complete and consistent set of parameters typical for the field of spatial transcriptomics.\n\nThe solution is derived in three parts as requested.\n\nPart 1: Derivation of the expected cell multiplicity per spot, $\\mathbb{E}[M]$.\n\nLet $M$ be the random variable representing the multiplicity, which is the number of cells captured by a single spot. The problem models cell centers as a homogeneous Poisson Point Process (PPP) in $\\mathbb{R}^2$ with intensity $n$. The cells themselves are circular disks with a random diameter $D$, and the capture spots are circular disks of fixed diameter $d_s$.\n\nA cell with center $\\mathbf{x}_c \\in \\mathbb{R}^2$ and radius $R = D/2$ intersects a spot centered at the origin with radius $r_s = d_s/2$ if and only if the distance between their centers is less than or equal to the sum of their radii. This condition is expressed as $\\|\\mathbf{x}_c\\| \\le R + r_s$.\n\nFor a specific cell with a given radius $R$, its center $\\mathbf{x}_c$ must fall within a disk of radius $R + r_s$ centered at the origin for it to be counted. The area of this \"capture region\" is the area of the Minkowski sum of the spot disk and the cell disk, given by $A(D) = \\pi (r_s + R)^2 = \\pi (d_s/2 + D/2)^2$.\n\nAccording to the properties of a homogeneous PPP, the expected number of points that fall into a given region is the intensity $n$ multiplied by the area of that region. Since the area of our capture region depends on the random variable $D$, we must take the expectation over the distribution of $D$. Using the law of total expectation (or Campbell's Theorem for marked point processes), the expected multiplicity $\\mathbb{E}[M]$ is the product of the intensity $n$ and the expected area of the capture region:\n$$\n\\mathbb{E}[M] = n \\cdot \\mathbb{E}[A(D)]\n$$\nWe first compute the expected area $\\mathbb{E}[A(D)]$ using the linearity of expectation:\n$$\n\\mathbb{E}[A(D)] = \\mathbb{E}\\left[\\pi \\left(\\frac{d_s}{2} + \\frac{D}{2}\\right)^2\\right] = \\frac{\\pi}{4} \\mathbb{E}[(d_s + D)^2] = \\frac{\\pi}{4} \\mathbb{E}[d_s^2 + 2d_s D + D^2]\n$$\n$$\n\\mathbb{E}[A(D)] = \\frac{\\pi}{4} (d_s^2 + 2d_s \\mathbb{E}[D] + \\mathbb{E}[D^2])\n$$\nNext, we relate the intensity $n$ to the cellular area fraction $\\phi$. The area fraction $\\phi$ is defined as the total area occupied by cells per unit area. For a PPP, this is the product of the expected number of cells per unit area, $n$, and the expected area of a single cell. The area of a cell with diameter $D$ is $\\pi (D/2)^2 = \\frac{\\pi}{4}D^2$. Its expected area is $\\mathbb{E}[\\frac{\\pi}{4}D^2] = \\frac{\\pi}{4}\\mathbb{E}[D^2]$.\nTherefore, the area fraction is:\n$$\n\\phi = n \\cdot \\frac{\\pi}{4}\\mathbb{E}[D^2]\n$$\nThis allows us to express the intensity $n$ in terms of $\\phi$ and the second moment of the cell diameter:\n$$\nn = \\frac{4\\phi}{\\pi \\mathbb{E}[D^2]}\n$$\nNow, we substitute this expression for $n$ into the equation for $\\mathbb{E}[M]$:\n$$\n\\mathbb{E}[M] = \\left(\\frac{4\\phi}{\\pi \\mathbb{E}[D^2]}\\right) \\cdot \\left(\\frac{\\pi}{4} (d_s^2 + 2d_s \\mathbb{E}[D] + \\mathbb{E}[D^2])\\right)\n$$\nThe factors of $\\pi/4$ cancel, yielding:\n$$\n\\mathbb{E}[M] = \\phi \\cdot \\frac{d_s^2 + 2d_s \\mathbb{E}[D] + \\mathbb{E}[D^2]}{\\mathbb{E}[D^2]}\n$$\nThis expression can be written as:\n$$\n\\mathbb{E}[M] = \\phi \\left(1 + \\frac{2d_s \\mathbb{E}[D]}{\\mathbb{E}[D^2]} + \\frac{d_s^2}{\\mathbb{E}[D^2]}\\right)\n$$\nThis is the required analytic expression for the expected multiplicity as a function of $d_s$, $\\phi$, and the first two moments of $D$, $\\mathbb{E}[D]$ and $\\mathbb{E}[D^2]$.\n\nPart 2: Specialization for a Gamma-distributed diameter.\n\nThe problem specifies that the cell diameter $D$ follows a Gamma distribution with mean $\\mathbb{E}[D] = \\bar{d}$ and coefficient of variation $c_v$. The coefficient of variation is defined as $c_v = \\frac{\\sqrt{\\text{Var}(D)}}{\\mathbb{E}[D]}$.\n\nWe need to express the first and second moments of $D$ in terms of $\\bar{d}$ and $c_v$.\nThe first moment is given directly:\n$$\n\\mathbb{E}[D] = \\bar{d}\n$$\nThe second moment $\\mathbb{E}[D^2]$ is related to the variance and the mean by the identity $\\text{Var}(D) = \\mathbb{E}[D^2] - (\\mathbb{E}[D])^2$.\nFrom the definition of $c_v$, the variance is $\\text{Var}(D) = (c_v \\cdot \\mathbb{E}[D])^2 = c_v^2 \\bar{d}^2$.\nTherefore, we can solve for the second moment:\n$$\n\\mathbb{E}[D^2] = \\text{Var}(D) + (\\mathbb{E}[D])^2 = c_v^2 \\bar{d}^2 + \\bar{d}^2 = \\bar{d}^2(1 + c_v^2)\n$$\nSubstituting these expressions for $\\mathbb{E}[D]$ and $\\mathbb{E}[D^2]$ into the result from Part 1:\n$$\n\\mathbb{E}[M] = \\phi \\left(1 + \\frac{2d_s \\bar{d}}{\\bar{d}^2(1 + c_v^2)} + \\frac{d_s^2}{\\bar{d}^2(1 + c_v^2)}\\right)\n$$\nSimplifying this expression yields:\n$$\n\\mathbb{E}[M] = \\phi \\left(1 + \\frac{2d_s/\\bar{d} + (d_s/\\bar{d})^2}{1 + c_v^2}\\right)\n$$\nThis is the specialized expression for $\\mathbb{E}[M]$ in terms of $d_s$, $\\phi$, $\\bar{d}$, and $c_v$.\n\nPart 3: Numerical evaluation.\n\nWe are given the following parameter values:\n- Packing fraction: $\\phi = 0.7$\n- Spot diameter: $d_s = 55$ micrometers\n- Mean cell diameter: $\\bar{d} = 12$ micrometers\n- Coefficient of variation of cell diameter: $c_v = 0.25$\n\nFirst, we calculate the values of the moments:\n$$\n\\mathbb{E}[D] = \\bar{d} = 12\n$$\n$$\n\\mathbb{E}[D^2] = \\bar{d}^2(1 + c_v^2) = 12^2(1 + 0.25^2) = 144(1 + 0.0625) = 144(1.0625) = 153\n$$\nNow, we substitute these moments and the given parameters into the general expression from Part 1:\n$$\n\\mathbb{E}[M] = \\phi \\left(1 + \\frac{2d_s \\mathbb{E}[D]}{\\mathbb{E}[D^2]} + \\frac{d_s^2}{\\mathbb{E}[D^2]}\\right)\n$$\n$$\n\\mathbb{E}[M] = 0.7 \\left(1 + \\frac{2(55)(12)}{153} + \\frac{55^2}{153}\\right)\n$$\n$$\n\\mathbb{E}[M] = 0.7 \\left(1 + \\frac{1320}{153} + \\frac{3025}{153}\\right)\n$$\nCombining the terms inside the parentheses over the common denominator $153$:\n$$\n\\mathbb{E}[M] = 0.7 \\left(\\frac{153 + 1320 + 3025}{153}\\right) = 0.7 \\left(\\frac{4498}{153}\\right)\n$$\nPerforming the division and multiplication:\n$$\n\\mathbb{E}[M] \\approx 0.7 \\times 29.39869...\n$$\n$$\n\\mathbb{E}[M] \\approx 20.57908...\n$$\nThe problem requires rounding the result to four significant figures.\n$$\n\\mathbb{E}[M] \\approx 20.58\n$$\nThe expected multiplicity is approximately $20.58$ cells per spot.",
            "answer": "$$\\boxed{20.58}$$"
        },
        {
            "introduction": "Unique Molecular Identifiers (UMIs) are vital for accurate digital gene quantification, but their effectiveness is limited by the size of the UMI library. For highly expressed genes, distinct mRNA molecules may be tagged with the same UMI—an event known as a 'collision'—which leads to undercounting. This exercise  explores this phenomenon by applying a classic probability model to derive the expected quantification bias, a critical skill for assessing data quality in sequencing experiments.",
            "id": "5164035",
            "problem": "In a spatial transcriptomics assay used in molecular and immunodiagnostics, messenger ribonucleic acid (mRNA) molecules from a single gene within a single spatial spot are captured and labeled with Unique Molecular Identifiers (UMIs), where a UMI is a short oligonucleotide tag used to deduplicate polymerase chain reaction (PCR) amplifications. Consider a single gene for which $n$ transcript molecules are captured at a given spot. Assume each captured molecule independently receives a UMI uniformly at random from a fixed UMI space of size $U$ (that is, there are $U$ possible distinct UMI sequences), and that deduplication collapses molecules sharing the same UMI into a single observed UMI count. Let $X$ denote the number of unique UMIs observed for this gene after deduplication.\n\nStarting from first principles of occupancy models in probability (independent placement of $n$ balls into $U$ bins with equal probability for each bin) and the standard large-system Poissonization known from the coupon collector problem, derive a closed-form analytical expression for the expected number of unique UMIs, $\\mathbb{E}[X]$, in terms of $n$ and $U$. Define the absolute bias of deduplication as $B = \\mathbb{E}[X] - n$, which quantifies the deviation of the expected observed UMI count from the true number of captured transcripts. Using the same approximation framework, derive a closed-form analytical expression for $B$ and interpret it in the low-complexity regime where $n/U$ is large.\n\nExpress your final answer as a row matrix containing, in order, the expression for $\\mathbb{E}[X]$ and the expression for $B$ as functions of $n$ and $U$. No numerical rounding is required, and no units are necessary since $X$ and $B$ are counts.",
            "solution": "The problem describes a scenario that can be modeled as a classic \"balls and bins\" problem in probability theory. Here, the $n$ transcript molecules are the \"balls,\" and the $U$ possible UMI sequences are the \"bins.\" Each transcript is independently assigned a UMI with uniform probability $1/U$, which corresponds to throwing $n$ balls into $U$ bins, with each bin being equally likely for each ball. The number of unique UMIs observed, $X$, is equivalent to the number of occupied bins.\n\nThe problem directs us to use the \"large-system Poissonization\" approximation. In this framework, the fixed number of transcripts, $n$, is replaced by a random variable, let's call it $N_{total}$, which follows a Poisson distribution with a mean equal to $n$. Thus, $N_{total} \\sim \\text{Poisson}(n)$.\n\nA fundamental property of this approximation is that the number of transcripts assigned to each individual UMI bin becomes an independent Poisson random variable. If the total number of incoming transcripts is Poisson distributed with mean $n$, and these are distributed uniformly among $U$ bins, then the number of transcripts landing in any specific bin $j$, let's call this $N_j$, follows a Poisson distribution with mean $\\lambda = n/U$. That is, for each $j \\in \\{1, 2, \\dots, U\\}$:\n$$N_j \\sim \\text{Poisson}\\left(\\frac{n}{U}\\right)$$\n\nThe number of unique UMIs, $X$, is the number of bins that are not empty. To find its expectation, $\\mathbb{E}[X]$, we can use the linearity of expectation. Let us define an indicator variable $I_j$ for each UMI bin $j$:\n$$I_j = \\begin{cases} 1 & \\text{if bin } j \\text{ is occupied (i.e., } N_j > 0\\text{)} \\\\ 0 & \\text{if bin } j \\text{ is empty (i.e., } N_j = 0\\text{)} \\end{cases}$$\nThe total number of unique UMIs is the sum of these indicators: $X = \\sum_{j=1}^{U} I_j$.\nThe expected value of $X$ is then:\n$$\\mathbb{E}[X] = \\mathbb{E}\\left[\\sum_{j=1}^{U} I_j\\right] = \\sum_{j=1}^{U} \\mathbb{E}[I_j]$$\nThe expectation of an indicator variable is simply the probability of the event it indicates. Thus, $\\mathbb{E}[I_j] = P(I_j = 1) = P(N_j > 0)$.\n\nIt is easier to first calculate the probability of the complementary event, i.e., that bin $j$ is empty ($N_j = 0$). For a Poisson-distributed random variable $N_j$ with mean $\\lambda = n/U$, the probability mass function is $P(N_j = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$. For $k=0$, we have:\n$$P(N_j = 0) = \\frac{(n/U)^0 \\exp(-n/U)}{0!} = \\exp\\left(-\\frac{n}{U}\\right)$$\nThe probability that bin $j$ is occupied is therefore:\n$$P(N_j > 0) = 1 - P(N_j = 0) = 1 - \\exp\\left(-\\frac{n}{U}\\right)$$\nThis gives the expectation of the indicator variable: $\\mathbb{E}[I_j] = 1 - \\exp\\left(-\\frac{n}{U}\\right)$.\n\nSince this probability is the same for all $U$ bins, we can substitute it into the sum for $\\mathbb{E}[X]$:\n$$\\mathbb{E}[X] = \\sum_{j=1}^{U} \\left(1 - \\exp\\left(-\\frac{n}{U}\\right)\\right) = U \\left(1 - \\exp\\left(-\\frac{n}{U}\\right)\\right)$$\nThis is the closed-form analytical expression for the expected number of unique UMIs under the Poissonization approximation.\n\nNext, we derive the expression for the absolute bias, $B$, defined as $B = \\mathbb{E}[X] - n$. Using the result for $\\mathbb{E}[X]$:\n$$B = U \\left(1 - \\exp\\left(-\\frac{n}{U}\\right)\\right) - n$$\nThis is the closed-form analytical expression for the bias.\n\nFinally, we are asked to interpret $B$ in the low-complexity regime, where the ratio $n/U$ is large. In this regime, we examine the behavior as $n/U \\to \\infty$.\nAs the ratio $n/U$ becomes very large, the term $\\exp(-n/U)$ approaches zero:\n$$\\lim_{n/U \\to \\infty} \\exp\\left(-\\frac{n}{U}\\right) = 0$$\nIn this limit, the expected number of unique UMIs becomes:\n$$\\mathbb{E}[X] \\approx U(1 - 0) = U$$\nThis result is intuitive: when the number of transcripts $n$ vastly exceeds the number of available UMIs $U$, the UMI space becomes saturated. Consequently, the expected number of observed unique UMIs, $\\mathbb{E}[X]$, approaches its maximum possible value, which is the total size of the UMI library, $U$.\n\nThe bias $B$ in this regime is approximately:\n$$B \\approx U - n$$\nSince $n/U$ is large, we have $n \\gg U$, which implies that $B$ is a large negative number. This signifies a severe underestimation of the true transcript count. The observed count, $\\mathbb{E}[X]$, is capped at $U$, whereas the true count, $n$, can be much larger. The bias $B$ quantifies this discrepancy, and its magnitude $|B| \\approx n-U$ represents the expected number of transcripts that are effectively \"lost\" to observation due to being assigned a UMI that was already used by another transcript. The deduplication process thus introduces a systematic negative bias that grows in magnitude with the true number of molecules, $n$, once the UMI space is saturated.",
            "answer": "$$\\boxed{\\begin{pmatrix} U \\left(1 - \\exp\\left(-\\frac{n}{U}\\right)\\right) & U \\left(1 - \\exp\\left(-\\frac{n}{U}\\right)\\right) - n \\end{pmatrix}}$$"
        },
        {
            "introduction": "A central goal in spatial transcriptomics is to map the cellular composition of tissues. Since many technologies generate data from spots containing multiple cell types, computational 'deconvolution' is necessary to infer the underlying cellular makeup. This practice  delves into a powerful method for this task, using constrained optimization with a spatial regularization term to improve accuracy. By deriving the model's optimality conditions, you will gain a deep understanding of the mathematical principles that power state-of-the-art spatial analysis tools.",
            "id": "5163986",
            "problem": "In a spatial transcriptomics deconvolution setting, a spot-level observed gene expression vector $y_{s} \\in \\mathbb{R}^{m}$ is modeled as a linear combination of $k$ reference expression signatures collected in a matrix $G \\in \\mathbb{R}^{m \\times k}$ with non-negative cell type proportions $p_{s} \\in \\mathbb{R}^{k}$, so that $y_{s} \\approx G p_{s}$. Ignoring measurement noise correlations and assuming least squares fidelity, a standard estimator per spot solves a non-negative least squares problem. Spatial context suggests that neighboring spots should have similar cell type compositions; this can be captured by a quadratic penalty induced by the unnormalized graph Laplacian of the spot adjacency graph.\n\nStarting from the foundational facts that least squares objectives are convex quadratics in their arguments and that the Karush–Kuhn–Tucker (KKT) conditions characterize optimality for convex problems with inequality constraints, proceed as follows.\n\n- First, for a single spot $s$, formulate the Lagrangian for the non-negative least squares problem\n$$\n\\min_{p_{s} \\in \\mathbb{R}^{k}} \\ \\frac{1}{2} \\| y_{s} - G p_{s} \\|_{2}^{2} \\quad \\text{subject to} \\quad p_{s} \\succeq 0,\n$$\nand derive the KKT conditions in terms of the primal variable $p_{s}$ and the dual variable $\\mu_{s} \\in \\mathbb{R}^{k}$ associated with the non-negativity constraints.\n\n- Next, consider $n$ spots indexed by $s \\in \\{1,\\dots,n\\}$ with an undirected weighted adjacency graph having weights $w_{st}=w_{ts} \\ge 0$ and unnormalized graph Laplacian $L \\in \\mathbb{R}^{n \\times n}$ defined by $L_{ss} = \\sum_{t \\ne s} w_{st}$ and $L_{st} = -w_{st}$ for $s \\ne t$. Argue how the stationarity condition is modified when augmenting the objective with the spatial smoothness regularizer\n$$\n\\frac{\\lambda}{2} \\sum_{(s,t)} w_{st} \\| p_{s} - p_{t} \\|_{2}^{2},\n$$\nwith $\\lambda \\ge 0$. You may express the coupled stationarity conditions using the graph Laplacian.\n\n- Finally, specialize to a two-spot, two-cell-type toy model that captures spatial smoothing while remaining analytically tractable. Let $k=2$, $n=2$, let $G = I_{2}$ (the $2 \\times 2$ identity), let the only nonzero edge weight be $w_{AB}=1$ between spots $A$ and $B$, and impose per-spot simplex constraints $1^{\\top} p_{s} = 1$ and $p_{s} \\succeq 0$ so that $p_{s}$ is a vector of cell type fractions. Take $y_{A} = \\begin{pmatrix} a \\\\ 1-a \\end{pmatrix}$ and $y_{B} = \\begin{pmatrix} b \\\\ 1-b \\end{pmatrix}$ with $a \\in (0,1)$ and $b \\in (0,1)$. Under the assumption that the non-negativity constraints are inactive at the optimum for all $\\lambda \\ge 0$ (which is satisfied when $a \\in (0,1)$ and $b \\in (0,1)$), reduce the problem to two scalar variables $x = p_{A,1}$ and $z = p_{B,1}$ and minimize the resulting quadratic objective. Derive the exact closed-form analytic expression for the optimal $x^{\\star}$ as a function of $a$, $b$, and $\\lambda$. Report this expression as your final answer. Do not round; provide a closed-form expression.",
            "solution": "The problem is valid as it is scientifically grounded in computational biology and optimization theory, mathematically well-posed, objective, and self-contained. The provided data and assumptions are consistent and sufficient for deriving a unique, meaningful solution.\n\nThe solution is developed in three parts as requested by the problem statement.\n\n**Part 1: KKT Conditions for Single-Spot Non-Negative Least Squares**\n\nThe problem for a single spot $s$ is a non-negative least squares (NNLS) problem, which is a convex optimization problem. The objective function is $\\mathcal{J}(p_s) = \\frac{1}{2} \\| y_s - G p_s \\|_2^2$, and the constraints are $p_s \\succeq 0$. This corresponds to $k$ individual constraints $p_{s,j} \\ge 0$ for $j=1, \\dots, k$. We can write these constraints in the standard form $g_j(p_s) \\le 0$ as $-p_{s,j} \\le 0$.\n\nThe Lagrangian function $\\mathcal{L}$ is formed by adding the constraints to the objective function, weighted by Lagrange multipliers (also known as dual variables). Let $\\mu_s \\in \\mathbb{R}^k$ be the vector of Lagrange multipliers for the non-negativity constraints on $p_s$. The Lagrangian is:\n$$\n\\mathcal{L}(p_s, \\mu_s) = \\mathcal{J}(p_s) + \\sum_{j=1}^{k} \\mu_{s,j} (-p_{s,j}) = \\frac{1}{2} \\| y_s - G p_s \\|_2^2 - \\mu_s^\\top p_s\n$$\nThe Karush-Kuhn-Tucker (KKT) conditions provide necessary and sufficient conditions for optimality for a convex problem with inequality constraints. They are:\n\n1.  **Stationarity**: The gradient of the Lagrangian with respect to the primal variables $p_s$ must be zero.\n    First, we compute the gradient of the objective function $\\mathcal{J}(p_s)$:\n    $$\n    \\mathcal{J}(p_s) = \\frac{1}{2} (y_s - G p_s)^\\top (y_s - G p_s) = \\frac{1}{2} (y_s^\\top y_s - 2 y_s^\\top G p_s + p_s^\\top G^\\top G p_s)\n    $$\n    The gradient with respect to $p_s$ is:\n    $$\n    \\nabla_{p_s} \\mathcal{J}(p_s) = \\frac{1}{2} (-2 G^\\top y_s + 2 G^\\top G p_s) = G^\\top G p_s - G^\\top y_s\n    $$\n    The gradient of the full Lagrangian is:\n    $$\n    \\nabla_{p_s} \\mathcal{L}(p_s, \\mu_s) = (G^\\top G p_s - G^\\top y_s) - \\mu_s\n    $$\n    Setting this to zero gives the stationarity condition:\n    $$\n    G^\\top G p_s - G^\\top y_s - \\mu_s = 0\n    $$\n\n2.  **Primal Feasibility**: The primal variables must satisfy the original constraints.\n    $$\n    p_s \\succeq 0\n    $$\n\n3.  **Dual Feasibility**: The Lagrange multipliers for inequality constraints of the form $g(x) \\le 0$ must be non-negative.\n    $$\n    \\mu_s \\succeq 0\n    $$\n\n4.  **Complementary Slackness**: The product of each Lagrange multiplier and its corresponding constraint must be zero.\n    $$\n    \\mu_{s,j} p_{s,j} = 0 \\quad \\text{for all} \\quad j = 1, \\dots, k\n    $$\n    In vector form, this is written using the element-wise (Hadamard) product as $\\mu_s \\odot p_s = 0$.\n\nIn summary, the KKT conditions for the single-spot NNLS problem are:\n-   $G^\\top G p_s - G^\\top y_s - \\mu_s = 0$\n-   $p_s \\succeq 0$\n-   $\\mu_s \\succeq 0$\n-   $\\mu_s \\odot p_s = 0$\n\n**Part 2: Modified Stationarity with Spatial Regularization**\n\nThe objective function is augmented with a spatial regularization term for a system of $n$ spots. The total objective function is:\n$$\nJ(p_1, \\dots, p_n) = \\sum_{s=1}^n \\frac{1}{2} \\| y_s - G p_s \\|_2^2 + \\frac{\\lambda}{2} \\sum_{(s,t)} w_{st} \\| p_s - p_t \\|_2^2\n$$\nThe first part is the sum of the individual least squares errors. The second part is the spatial regularizer, which penalizes differences in cell type proportions $p_s$ and $p_t$ between adjacent spots $s$ and $t$, weighted by $w_{st}$.\n\nTo find the modified stationarity condition, we need to find the gradient of the total objective with respect to a single proportion vector $p_s$. The Lagrangian for the full problem is $\\mathcal{L} = J(p_1, \\dots, p_n) - \\sum_{s=1}^n \\mu_s^\\top p_s$. The stationarity condition for each spot $s$ is $\\nabla_{p_s} \\mathcal{L} = 0$.\n\nThe gradient of the data-fitting term $\\sum_{i=1}^n \\frac{1}{2} \\| y_i - G p_i \\|_2^2$ with respect to $p_s$ is simply $G^\\top G p_s - G^\\top y_s$, as derived in Part 1.\n\nNow, consider the regularization term $R = \\frac{\\lambda}{2} \\sum_{(u,v)} w_{uv} \\| p_u - p_v \\|_2^2$. The sum is over unique undirected edges $(u,v)$. This can be rewritten as a sum over all pairs $(u,v)$ with a factor of $1/2$: $R = \\frac{\\lambda}{4} \\sum_{u=1}^n \\sum_{v=1}^n w_{uv} \\| p_u - p_v \\|_2^2$, where we define $w_{uu}=0$. The gradient with respect to $p_s$ is:\n$$\n\\nabla_{p_s} R = \\frac{\\lambda}{4} \\left( \\sum_{v \\ne s} w_{sv} \\nabla_{p_s} \\| p_s - p_v \\|_2^2 + \\sum_{u \\ne s} w_{us} \\nabla_{p_s} \\| p_u - p_s \\|_2^2 \\right)\n$$\nUsing $\\nabla_x \\|x-c\\|_2^2 = 2(x-c)$ and the symmetry $w_{us}=w_{su}$, we get:\n$$\n\\nabla_{p_s} R = \\frac{\\lambda}{4} \\left( \\sum_{t \\ne s} w_{st} 2(p_s - p_t) + \\sum_{t \\ne s} w_{st} 2(p_s - p_t) \\right) = \\lambda \\sum_{t \\ne s} w_{st} (p_s - p_t)\n$$\nThis expression can be related to the graph Laplacian $L$.\n$$\n\\nabla_{p_s} R = \\lambda \\left( \\left(\\sum_{t \\ne s} w_{st}\\right) p_s - \\sum_{t \\ne s} w_{st} p_t \\right)\n$$\nUsing the definition of the unnormalized graph Laplacian, $L_{ss} = \\sum_{t \\ne s} w_{st}$ and $L_{st} = -w_{st}$ for $s \\ne t$:\n$$\n\\nabla_{p_s} R = \\lambda \\left( L_{ss} p_s + \\sum_{t \\ne s} L_{st} p_t \\right) = \\lambda \\sum_{t=1}^n L_{st} p_t\n$$\nThis is the $s$-th block row of the product of a block matrix involving $L$ and the stacked vector of all $p_t$.\n\nThe stationarity condition for each spot $s$ is obtained by setting $\\nabla_{p_s} \\mathcal{L} = 0$:\n$$\nG^\\top G p_s - G^\\top y_s + \\lambda \\sum_{t=1}^n L_{st} p_t - \\mu_s = 0\n$$\nCompared to the single-spot case, the stationarity condition for spot $s$ is modified by the addition of the term $\\lambda \\sum_{t=1}^n L_{st} p_t$. This term couples the solution for $p_s$ with the solutions for its neighbors in the graph, enforcing spatial smoothness. The full system consists of $n$ such vector equations, which must be solved simultaneously.\n\n**Part 3: Two-Spot, Two-Cell-Type Toy Model**\n\nWe are given a simplified model with $n=2$ spots ($A, B$) and $k=2$ cell types.\n-   $G = I_2$, the $2 \\times 2$ identity matrix.\n-   $y_A = \\begin{pmatrix} a \\\\ 1-a \\end{pmatrix}$, $y_B = \\begin{pmatrix} b \\\\ 1-b \\end{pmatrix}$ with $a,b \\in (0,1)$.\n-   $w_{AB}=1$, so the graph Laplacian is $L = \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}$.\n-   Constraints: $p_s \\succeq 0$ and $1^\\top p_s = 1$ for $s \\in \\{A, B\\}$.\n-   Assumption: The non-negativity constraints $p_s \\succeq 0$ are inactive.\n\nThe sum-to-one constraint means we can parameterize the proportion vectors. Let $x = p_{A,1}$ and $z = p_{B,1}$. Then $p_{A,2} = 1-x$ and $p_{B,2} = 1-z$. So, $p_A = \\begin{pmatrix} x \\\\ 1-x \\end{pmatrix}$ and $p_B = \\begin{pmatrix} z \\\\ 1-z \\end{pmatrix}$. The assumption that non-negativity is inactive means we can ignore these constraints in the optimization (and verify post-hoc that the solution satisfies them). The simplex constraints become box constraints $x \\in [0,1]$ and $z \\in [0,1]$.\n\nThe total objective function is $J(p_A, p_B) = \\frac{1}{2} \\| y_A - p_A \\|_2^2 + \\frac{1}{2} \\| y_B - p_B \\|_2^2 + \\frac{\\lambda}{2} \\| p_A - p_B \\|_2^2$, since $G=I_2$ and $w_{AB}=1$.\n\nLet's express the terms in $J$ using $x$ and $z$:\n-   $\\| y_A - p_A \\|_2^2 = \\left\\| \\begin{pmatrix} a-x \\\\ (1-a)-(1-x) \\end{pmatrix} \\right\\|_2^2 = (a-x)^2 + (x-a)^2 = 2(a-x)^2$.\n-   $\\| y_B - p_B \\|_2^2 = \\left\\| \\begin{pmatrix} b-z \\\\ (1-b)-(1-z) \\end{pmatrix} \\right\\|_2^2 = (b-z)^2 + (z-b)^2 = 2(b-z)^2$.\n-   $\\| p_A - p_B \\|_2^2 = \\left\\| \\begin{pmatrix} x-z \\\\ (1-x)-(1-z) \\end{pmatrix} \\right\\|_2^2 = (x-z)^2 + (z-x)^2 = 2(x-z)^2$.\n\nSubstituting these into the objective function:\n$$\nJ(x, z) = \\frac{1}{2} \\left[ 2(a-x)^2 + 2(b-z)^2 + \\lambda \\cdot 2(x-z)^2 \\right] = (a-x)^2 + (b-z)^2 + \\lambda(x-z)^2\n$$\nTo find the optimal $(x^\\star, z^\\star)$, we minimize this unconstrained quadratic function by setting its partial derivatives with respect to $x$ and $z$ to zero.\n$$\n\\frac{\\partial J}{\\partial x} = -2(a-x) + 2\\lambda(x-z) = 0 \\implies -a+x+\\lambda x - \\lambda z = 0 \\implies (1+\\lambda)x - \\lambda z = a\n$$\n$$\n\\frac{\\partial J}{\\partial z} = -2(b-z) - 2\\lambda(x-z) = 0 \\implies -b+z-\\lambda x + \\lambda z = 0 \\implies -\\lambda x + (1+\\lambda)z = b\n$$\nThis gives a system of two linear equations in $x$ and $z$:\n$$\n\\begin{pmatrix} 1+\\lambda & -\\lambda \\\\ -\\lambda & 1+\\lambda \\end{pmatrix} \\begin{pmatrix} x \\\\ z \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\end{pmatrix}\n$$\nWe need to solve for $x$. We can use Cramer's rule. The determinant of the coefficient matrix is:\n$$\n\\Delta = (1+\\lambda)^2 - (-\\lambda)^2 = 1 + 2\\lambda + \\lambda^2 - \\lambda^2 = 1+2\\lambda\n$$\nThe determinant for the numerator of $x$ is:\n$$\n\\Delta_x = \\det \\begin{pmatrix} a & -\\lambda \\\\ b & 1+\\lambda \\end{pmatrix} = a(1+\\lambda) - b(-\\lambda) = a + a\\lambda + b\\lambda = a + (a+b)\\lambda\n$$\nThe solution for $x^{\\star}$ is:\n$$\nx^{\\star} = \\frac{\\Delta_x}{\\Delta} = \\frac{a + (a+b)\\lambda}{1+2\\lambda}\n$$\nAs a check, since $a,b \\in (0,1)$ and $\\lambda \\ge 0$, we have $x^\\star > 0$. Also, $x^\\star < 1$ because $a + (a+b)\\lambda < 1+2\\lambda \\iff a-1 + (a+b-2)\\lambda < 0$, which is true since $a-1 < 0$ and $a+b-2 < 0$. Thus the solution lies in $(0,1)$ and the non-negativity constraints are indeed inactive.",
            "answer": "$$\\boxed{\\frac{a + (a+b)\\lambda}{1+2\\lambda}}$$"
        }
    ]
}