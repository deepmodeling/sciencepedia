{
    "hands_on_practices": [
        {
            "introduction": "At the limits of assay sensitivity, distinguishing a true but weak positive signal from low-level background contamination represents a critical diagnostic challenge. This exercise  tackles this fundamental problem by modeling the discrete, low-copy number of template molecules with the Poisson distribution. You will apply the powerful likelihood ratio test, a cornerstone of statistical hypothesis testing, to derive a statistically optimal decision boundary for classifying ambiguous results.",
            "id": "5146147",
            "problem": "A quantitative Polymerase Chain Reaction (qPCR) assay for pathogen detection is operated near its limit of detection. The fundamental facts for analysis are as follows: Polymerase Chain Reaction (PCR) exponentially amplifies starting template molecules, so after $c$ cycles the amplicon count is $N(c) = N_0 (1+E)^{c}$, where $N_0$ denotes the number of amplifiable template molecules that seeded the reaction and $E$ is the per-cycle efficiency. The quantification cycle $C_{q}$ is defined as the cycle at which the amplicon count crosses a fixed fluorescence threshold $N_{\\text{th}}$, which implies $C_{q} = \\log_{1+E}\\!\\big(N_{\\text{th}}/N_0\\big)$. Hence $C_{q}$ is a strictly decreasing function of $N_0$, making $N_0$ a sufficient statistic for discriminating hypotheses that differ only in the expected seeding count. Near the limit of detection, $N_0$ is dominated by stochastic sampling of molecules into the reaction volume and is well modeled by a Poisson random variable due to random partitioning in a well-mixed solution.\n\nConsider two hypotheses about the origin of $N_0$ in a single reaction:\n- $H_{c}$ (contamination-only): $N_0 \\sim \\text{Poisson}(\\lambda_{c})$, representing aerosol or carryover contamination with mean $\\lambda_{c} > 0$ molecules per reaction.\n- $H_{t}$ (true low-titer positive): $N_0 \\sim \\text{Poisson}(\\lambda_{t})$, representing a genuine low-titer sample with mean $\\lambda_{t} > \\lambda_{c}$ molecules per reaction.\n\nAssume $N_0$ is directly inferable as the sufficient statistic from the measured $C_{q}$ via the monotonic mapping above and that measurement noise is negligible relative to the Poisson seeding noise at this operating regime. Using the Poisson model as the foundational base and the likelihood ratio principle, derive the log-likelihood ratio $\\Lambda(n)$ for an observed integer $n$ and determine the real-valued boundary $n^{\\star}$ at which the contamination hypothesis and the true low-titer hypothesis are indistinguishable in the sense that $\\Lambda(n^{\\star}) = 0$. Then, propose a decision rule in terms of $\\Lambda(n)$ and a tunable threshold that accounts for prior odds and misclassification costs. Your final answer must be the closed-form expression for $n^{\\star}$ in terms of $\\lambda_{c}$ and $\\lambda_{t}$. No numerical evaluation is required, and no rounding is needed. Express all mathematical relationships using standard natural logarithms $\\ln(\\cdot)$.",
            "solution": "The problem is first evaluated for its validity.\n\n### Step 1: Extract Givens\n- Amplicon count after $c$ cycles: $N(c) = N_0 (1+E)^{c}$\n- $N_0$: number of amplifiable template molecules.\n- $E$: per-cycle efficiency.\n- Quantification cycle: $C_{q} = \\log_{1+E}(N_{\\text{th}}/N_0)$, where $N_{\\text{th}}$ is a fixed fluorescence threshold.\n- $N_0$ is the sufficient statistic.\n- $N_0$ is modeled as a Poisson random variable.\n- Hypothesis $H_{c}$ (contamination-only): $N_0 \\sim \\text{Poisson}(\\lambda_{c})$, with mean $\\lambda_{c} > 0$.\n- Hypothesis $H_{t}$ (true low-titer positive): $N_0 \\sim \\text{Poisson}(\\lambda_{t})$, with mean $\\lambda_{t} > \\lambda_{c}$.\n- $N_0$ is assumed to be an integer that is directly inferable from the measured $C_{q}$.\n- The task is to derive the log-likelihood ratio $\\Lambda(n)$ for an observed integer count $n$, find the boundary $n^{\\star}$ where $\\Lambda(n^{\\star}) = 0$, and propose a decision rule.\n- The final expression for $n^{\\star}$ should be in terms of $\\lambda_{c}$ and $\\lambda_{t}$ using natural logarithms, $\\ln(\\cdot)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on well-established principles of Polymerase Chain Reaction (PCR) kinetics and statistical hypothesis testing. The exponential amplification model is a standard first-order approximation. Modeling low-copy number template molecules with a Poisson distribution is a cornerstone of digital PCR and quantitative analysis near the limit of detection. The setup represents a classic and critical problem in molecular diagnostics: distinguishing a weak positive signal from background contamination.\n- **Well-Posed:** The problem provides two distinct, well-defined probability distributions for the two hypotheses ($H_c$ and $H_t$). The condition $\\lambda_t > \\lambda_c$ ensures the hypotheses are not identical. The objective is clearly stated: derive the log-likelihood ratio and find the specific value $n^{\\star}$ where the log-likelihood ratio is zero. This structure leads to a unique mathematical solution.\n- **Objective:** The language is formal, precise, and devoid of subjective or ambiguous terminology.\n- **Complete and Consistent:** All necessary information, including the probability models (Poisson) and the parameters for each hypothesis, is provided. The premises are internally consistent. The simplification that $N_0$ is directly inferable is explicitly stated, making the problem self-contained and tractable.\n- **Realistic and Feasible:** The scenario is highly realistic in the context of high-sensitivity diagnostic assays. The mathematical derivation required is standard and feasible.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. A rigorous solution can be derived from the provided information.\n\n### Solution Derivation\nThe problem requires the application of the likelihood ratio principle to decide between two competing hypotheses, $H_{c}$ and $H_{t}$, based on an observed integer count $n$ of template molecules, where $n$ is assumed to be an observation of the random variable $N_0$.\n\nThe probability mass function (PMF) for a Poisson-distributed random variable with mean $\\lambda$ is given by:\n$$\nP(N_0 = n | \\lambda) = \\frac{\\lambda^n \\exp(-\\lambda)}{n!}\n$$\nfor $n \\in \\{0, 1, 2, \\dots\\}$.\n\nUnder the true low-titer positive hypothesis, $H_{t}$, the parameter of the Poisson distribution is $\\lambda_t$. The likelihood of observing the count $n$ is:\n$$\nL(H_{t} | n) = P(N_0 = n | \\lambda_t) = \\frac{\\lambda_t^n \\exp(-\\lambda_t)}{n!}\n$$\n\nUnder the contamination-only hypothesis, $H_{c}$, the parameter is $\\lambda_c$. The likelihood of observing the count $n$ is:\n$$\nL(H_{c} | n) = P(N_0 = n | \\lambda_c) = \\frac{\\lambda_c^n \\exp(-\\lambda_c)}{n!}\n$$\n\nThe likelihood ratio, $LR(n)$, is the ratio of these two likelihoods. We define it as $L(H_{t} | n) / L(H_{c} | n)$:\n$$\nLR(n) = \\frac{\\frac{\\lambda_t^n \\exp(-\\lambda_t)}{n!}}{\\frac{\\lambda_c^n \\exp(-\\lambda_c)}{n!}}\n$$\nThe factorial term $n!$ cancels out, simplifying the expression to:\n$$\nLR(n) = \\frac{\\lambda_t^n \\exp(-\\lambda_t)}{\\lambda_c^n \\exp(-\\lambda_c)}\n$$\nThis can be rearranged by grouping terms with like bases:\n$$\nLR(n) = \\left(\\frac{\\lambda_t}{\\lambda_c}\\right)^n \\exp(-\\lambda_t - (-\\lambda_c)) = \\left(\\frac{\\lambda_t}{\\lambda_c}\\right)^n \\exp(\\lambda_c - \\lambda_t)\n$$\nThe problem asks for the log-likelihood ratio, $\\Lambda(n)$, which is the natural logarithm of $LR(n)$:\n$$\n\\Lambda(n) = \\ln(LR(n)) = \\ln\\left[\\left(\\frac{\\lambda_t}{\\lambda_c}\\right)^n \\exp(\\lambda_c - \\lambda_t)\\right]\n$$\nUsing the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(a^b) = b\\ln(a)$, we get:\n$$\n\\Lambda(n) = \\ln\\left(\\left(\\frac{\\lambda_t}{\\lambda_c}\\right)^n\\right) + \\ln(\\exp(\\lambda_c - \\lambda_t))\n$$\n$$\n\\Lambda(n) = n \\ln\\left(\\frac{\\lambda_t}{\\lambda_c}\\right) + (\\lambda_c - \\lambda_t)\n$$\nThis is the derived log-likelihood ratio as a function of the observed count $n$.\n\nNext, we must find the boundary $n^{\\star}$, which is the real-valued count at which the two hypotheses are indistinguishable, defined by the condition $\\Lambda(n^{\\star}) = 0$.\nSetting the expression for $\\Lambda(n)$ to zero:\n$$\nn^{\\star} \\ln\\left(\\frac{\\lambda_t}{\\lambda_c}\\right) + (\\lambda_c - \\lambda_t) = 0\n$$\nNow, we solve for $n^{\\star}$:\n$$\nn^{\\star} \\ln\\left(\\frac{\\lambda_t}{\\lambda_c}\\right) = \\lambda_t - \\lambda_c\n$$\n$$\nn^{\\star} = \\frac{\\lambda_t - \\lambda_c}{\\ln\\left(\\frac{\\lambda_t}{\\lambda_c}\\right)}\n$$\nThis expression can also be written using the property $\\ln(a/b) = \\ln(a) - \\ln(b)$:\n$$\nn^{\\star} = \\frac{\\lambda_t - \\lambda_c}{\\ln(\\lambda_t) - \\ln(\\lambda_c)}\n$$\nThis is the closed-form expression for the decision boundary $n^{\\star}$. Notably, this expression defines the logarithmic mean of $\\lambda_t$ and $\\lambda_c$.\n\nFinally, a decision rule is proposed. The Neyman-Pearson framework for hypothesis testing dictates that the optimal test compares the likelihood ratio to a threshold. This is equivalent to comparing the log-likelihood ratio $\\Lambda(n)$ to a log-threshold $\\tau$.\nThe decision rule is:\n- Decide $H_t$ (true positive) if $\\Lambda(n) > \\tau$.\n- Decide $H_c$ (contamination) if $\\Lambda(n) < \\tau$.\nThe threshold $\\tau$ is tunable and can be chosen to balance the probabilities of Type I (false positive) and Type II (false negative) errors. In a Bayesian framework, $\\tau$ incorporates prior probabilities of the hypotheses and the costs of misclassification:\n$$\n\\tau = \\ln\\left(\\frac{P(H_c) \\cdot C_{FP}}{P(H_t) \\cdot C_{FN}}\\right)\n$$\nwhere $P(H_c)$ and $P(H_t)$ are the prior probabilities and $C_{FP}$ and $C_{FN}$ are the costs of a false positive and a false negative, respectively. The boundary $n^{\\star}$ corresponds to the special case where the threshold $\\tau=0$, which occurs when the prior odds and misclassification costs are balanced such that $\\frac{P(H_c) C_{FP}}{P(H_t) C_{FN}} = 1$. At this boundary, the likelihoods of the two hypotheses are equal.\nThe decision rule in terms of the observed count $n$ and the boundary $n^{\\star}$ (for the $\\tau=0$ case) is:\n- Decide $H_t$ if $n > n^{\\star}$.\n- Decide $H_c$ if $n < n^{\\star}$.\nThis follows because $\\ln(\\lambda_t/\\lambda_c) > 0$ since $\\lambda_t > \\lambda_c$, making $\\Lambda(n)$ a strictly increasing function of $n$.",
            "answer": "$$\n\\boxed{\\frac{\\lambda_t - \\lambda_c}{\\ln\\left(\\frac{\\lambda_t}{\\lambda_c}\\right)}}\n$$"
        },
        {
            "introduction": "When a qPCR assay deviates from expectations, it is crucial to diagnose the root cause, which could be contamination or the presence of amplification inhibitors in the sample. An internal control provides the necessary additional information to distinguish between these common failure modes. This practice  guides you through building a statistical classifier based on the joint quantification cycle ($Cq$) shifts of both the target and the internal control, which produce distinct data signatures. By applying linear discriminant analysis to a multivariate normal model, you will learn how to translate these signatures into a quantitative decision rule for automated diagnostics.",
            "id": "5146234",
            "problem": "A quantitative Polymerase Chain Reaction (qPCR) assay is used to detect a pathogen target in patient samples. To monitor the integrity of amplification and detect polymerase inhibition, a synthetic spike-in internal control of known copy number is added to every reaction. In qPCR, the quantification cycle ($Cq$) is the cycle number at which fluorescence crosses a set threshold. Under the assumption of exponential amplification with efficiency $E$ per cycle, the relationship between the initial template amount $N_{0}$ and the $Cq$ can be modeled as $Cq = C^{\\ast} - \\log_{1+E}(N_{0})$, where $C^{\\ast}$ is a constant determined by the threshold and instrument settings. Inhibition reduces $E$ across all amplicons, causing both the pathogen target and spike-in control to exhibit positive shifts in $Cq$ relative to a clean reference. In contrast, contamination corresponds to an increase in the apparent initial target quantity $N_{0}$ due to carryover template, causing a negative $Cq$ shift for the target while leaving the spike-in control approximately unchanged.\n\nLet the joint shifts $\\boldsymbol{\\Delta} = (\\Delta Cq_{t}, \\Delta Cq_{c})^{\\top}$, where $\\Delta Cq_{t}$ is the target shift and $\\Delta Cq_{c}$ is the control shift relative to a clean reference, be modeled as bivariate normal under two competing hypotheses: inhibition ($\\mathcal{H}_{i}$) and contamination ($\\mathcal{H}_{c}$). Empirical calibration yields the following class-conditional parameters, with equal prior probabilities and equal misclassification costs:\n\nInhibition ($\\mathcal{H}_{i}$):\n- Mean vector $\\boldsymbol{\\mu}_{i} = \\begin{pmatrix} 12/5 \\\\ 21/10 \\end{pmatrix}$,\n- Covariance matrix $\\Sigma_{i} = \\begin{pmatrix} 1/4 & 3/20 \\\\ 3/20 & 1/5 \\end{pmatrix}$.\n\nContamination ($\\mathcal{H}_{c}$):\n- Mean vector $\\boldsymbol{\\mu}_{c} = \\begin{pmatrix} -3 \\\\ 1/10 \\end{pmatrix}$,\n- Covariance matrix $\\Sigma_{c} = \\begin{pmatrix} 4/25 & 1/50 \\\\ 1/50 & 1/10 \\end{pmatrix}$.\n\nAssume the equal-covariance model with pooled covariance $\\Sigma_{p} = (\\Sigma_{i} + \\Sigma_{c})/2$, and the linear discriminant decision rule derived from the Gaussian log-likelihood ratio with equal priors and costs. Define the discriminant function $g(\\Delta Cq_{t}, \\Delta Cq_{c})$ such that a sample is classified as inhibition if $g(\\Delta Cq_{t}, \\Delta Cq_{c}) > 0$ and contamination otherwise. Starting from first principles (exponential amplification in qPCR, how inhibition and contamination shift $Cq$, and the multivariate normal model with equal covariance and equal priors), derive the explicit closed-form analytic expression for $g(\\Delta Cq_{t}, \\Delta Cq_{c})$ in terms of $\\Delta Cq_{t}$ and $\\Delta Cq_{c}$, using the provided numerical parameters. Provide your final expression for $g(\\Delta Cq_{t}, \\Delta Cq_{c})$ in exact fractional form. No rounding is required.",
            "solution": "The problem requires the derivation of a linear discriminant function for classifying a qPCR sample as exhibiting either inhibition ($\\mathcal{H}_i$) or contamination ($\\mathcal{H}_c$), based on the observed cycle quantification ($Cq$) shifts of a target gene ($\\Delta Cq_t$) and an internal control ($\\Delta Cq_c$).\n\nThe problem is grounded in the principles of qPCR, where the $Cq$ value is inversely related to the logarithm of the initial template quantity, $N_0$, and the amplification efficiency, $E$. The provided model is $Cq = C^{\\ast} - \\log_{1+E}(N_{0})$. Inhibition reduces $E$, increasing the $Cq$ value (positive $\\Delta Cq$). Contamination increases the target's $N_0$, decreasing its $Cq$ value (negative $\\Delta Cq_t$), while leaving the internal control's $Cq$ largely unaffected. The given mean vectors, $\\boldsymbol{\\mu}_i$ for inhibition and $\\boldsymbol{\\mu}_c$ for contamination, are consistent with these biophysical effects.\n\nThe classification task is to be solved using a decision rule based on the bivariate normal model of the shifts $\\boldsymbol{\\Delta} = (\\Delta Cq_{t}, \\Delta Cq_{c})^{\\top}$. We are given that the prior probabilities for the two classes are equal, $P(\\mathcal{H}_i) = P(\\mathcal{H}_c)$, and the costs of misclassification are also equal. This leads to a Bayes-optimal decision rule that chooses the class with the higher likelihood. We classify a sample as inhibition ($\\mathcal{H}_i$) if the likelihood of observing $\\boldsymbol{\\Delta}$ under $\\mathcal{H}_i$ is greater than under $\\mathcal{H}_c$.\n$$p(\\boldsymbol{\\Delta} | \\mathcal{H}_i) > p(\\boldsymbol{\\Delta} | \\mathcal{H}_c)$$\nIt is more convenient to work with the log-likelihood ratio. The discriminant function $g(\\boldsymbol{\\Delta})$ is defined as this ratio, with the decision rule being to classify as inhibition if $g(\\boldsymbol{\\Delta}) > 0$.\n$$g(\\boldsymbol{\\Delta}) = \\ln\\left(\\frac{p(\\boldsymbol{\\Delta} | \\mathcal{H}_i)}{p(\\boldsymbol{\\Delta} | \\mathcal{H}_c)}\\right) = \\ln(p(\\boldsymbol{\\Delta} | \\mathcal{H}_i)) - \\ln(p(\\boldsymbol{\\Delta} | \\mathcal{H}_c))$$\nThe problem specifies that the data is modeled as a $2$-dimensional multivariate normal distribution, and mandates the use of a pooled covariance matrix $\\Sigma_p = (\\Sigma_i + \\Sigma_c)/2$. The probability density function for a class $k \\in \\{i, c\\}$ is:\n$$p(\\boldsymbol{\\Delta} | \\mathcal{H}_k) = \\frac{1}{2\\pi |\\Sigma_p|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\Delta} - \\boldsymbol{\\mu}_k)^{\\top} \\Sigma_p^{-1} (\\boldsymbol{\\Delta} - \\boldsymbol{\\mu}_k)\\right)$$\nThe natural logarithm of this density is:\n$$\\ln(p(\\boldsymbol{\\Delta} | \\mathcal{H}_k)) = -\\ln(2\\pi) - \\frac{1}{2}\\ln|\\Sigma_p| - \\frac{1}{2}(\\boldsymbol{\\Delta} - \\boldsymbol{\\mu}_k)^{\\top} \\Sigma_p^{-1} (\\boldsymbol{\\Delta} - \\boldsymbol{\\mu}_k)$$\nSubstituting this into the expression for $g(\\boldsymbol{\\Delta})$, the terms $-\\ln(2\\pi) - \\frac{1}{2}\\ln|\\Sigma_p|$ cancel out:\n$$g(\\boldsymbol{\\Delta}) = -\\frac{1}{2}(\\boldsymbol{\\Delta} - \\boldsymbol{\\mu}_i)^{\\top} \\Sigma_p^{-1} (\\boldsymbol{\\Delta} - \\boldsymbol{\\mu}_i) + \\frac{1}{2}(\\boldsymbol{\\Delta} - \\boldsymbol{\\mu}_c)^{\\top} \\Sigma_p^{-1} (\\boldsymbol{\\Delta} - \\boldsymbol{\\mu}_c)$$\nExpanding the quadratic forms $(\\boldsymbol{\\Delta} - \\boldsymbol{\\mu})^{\\top} \\Sigma_p^{-1} (\\boldsymbol{\\Delta} - \\boldsymbol{\\mu}) = \\boldsymbol{\\Delta}^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\Delta} - 2\\boldsymbol{\\mu}^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\Delta} + \\boldsymbol{\\mu}^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\mu}$:\n$$g(\\boldsymbol{\\Delta}) = -\\frac{1}{2}(\\boldsymbol{\\Delta}^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\Delta} - 2\\boldsymbol{\\mu}_i^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\Delta} + \\boldsymbol{\\mu}_i^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\mu}_i) + \\frac{1}{2}(\\boldsymbol{\\Delta}^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\Delta} - 2\\boldsymbol{\\mu}_c^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\Delta} + \\boldsymbol{\\mu}_c^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\mu}_c)$$\nThe quadratic term $\\boldsymbol{\\Delta}^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\Delta}$ cancels, resulting in a linear function of $\\boldsymbol{\\Delta}$:\n$$g(\\boldsymbol{\\Delta}) = \\boldsymbol{\\mu}_i^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\Delta} - \\frac{1}{2}\\boldsymbol{\\mu}_i^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_c^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\Delta} + \\frac{1}{2}\\boldsymbol{\\mu}_c^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\mu}_c$$\n$$g(\\boldsymbol{\\Delta}) = (\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_c)^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\Delta} - \\frac{1}{2}(\\boldsymbol{\\mu}_i^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_c^{\\top}\\Sigma_p^{-1}\\boldsymbol{\\mu}_c)$$\nThis can be written as $g(\\boldsymbol{\\Delta}) = \\boldsymbol{w}^{\\top}\\boldsymbol{\\Delta} + w_0$, where $\\boldsymbol{w} = \\Sigma_p^{-1}(\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_c)$ and $w_0 = -\\frac{1}{2}(\\boldsymbol{\\mu}_i + \\boldsymbol{\\mu}_c)^{\\top}\\Sigma_p^{-1}(\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_c)$.\n\nWe now substitute the given numerical values. Let $\\Delta Cq_t = x$ and $\\Delta Cq_c = y$. So $\\boldsymbol{\\Delta} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$.\nThe class-conditional parameters are:\n$\\boldsymbol{\\mu}_{i} = \\begin{pmatrix} 12/5 \\\\ 21/10 \\end{pmatrix}$, $\\Sigma_{i} = \\begin{pmatrix} 1/4 & 3/20 \\\\ 3/20 & 1/5 \\end{pmatrix}$\n$\\boldsymbol{\\mu}_{c} = \\begin{pmatrix} -3 \\\\ 1/10 \\end{pmatrix}$, $\\Sigma_{c} = \\begin{pmatrix} 4/25 & 1/50 \\\\ 1/50 & 1/10 \\end{pmatrix}$\n\nFirst, we calculate the pooled covariance matrix $\\Sigma_p$:\n$$\\Sigma_p = \\frac{1}{2}(\\Sigma_i + \\Sigma_c) = \\frac{1}{2}\\left(\\begin{pmatrix} \\frac{1}{4} & \\frac{3}{20} \\\\ \\frac{3}{20} & \\frac{1}{5} \\end{pmatrix} + \\begin{pmatrix} \\frac{4}{25} & \\frac{1}{50} \\\\ \\frac{1}{50} & \\frac{1}{10} \\end{pmatrix}\\right)$$\n$$\\Sigma_p = \\frac{1}{2}\\begin{pmatrix} \\frac{1}{4}+\\frac{4}{25} & \\frac{3}{20}+\\frac{1}{50} \\\\ \\frac{3}{20}+\\frac{1}{50} & \\frac{1}{5}+\\frac{1}{10} \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} \\frac{25+16}{100} & \\frac{15+2}{100} \\\\ \\frac{15+2}{100} & \\frac{2+1}{10} \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} \\frac{41}{100} & \\frac{17}{100} \\\\ \\frac{17}{100} & \\frac{30}{100} \\end{pmatrix} = \\begin{pmatrix} \\frac{41}{200} & \\frac{17}{200} \\\\ \\frac{17}{200} & \\frac{30}{200} \\end{pmatrix}$$\nNext, we find the inverse, $\\Sigma_p^{-1}$:\n$$\\det(\\Sigma_p) = \\left(\\frac{41}{200}\\right)\\left(\\frac{30}{200}\\right) - \\left(\\frac{17}{200}\\right)^2 = \\frac{1230 - 289}{40000} = \\frac{941}{40000}$$\n$$\\Sigma_p^{-1} = \\frac{1}{\\det(\\Sigma_p)}\\begin{pmatrix} \\frac{30}{200} & -\\frac{17}{200} \\\\ -\\frac{17}{200} & \\frac{41}{200} \\end{pmatrix} = \\frac{40000}{941} \\cdot \\frac{1}{200}\\begin{pmatrix} 30 & -17 \\\\ -17 & 41 \\end{pmatrix} = \\frac{200}{941}\\begin{pmatrix} 30 & -17 \\\\ -17 & 41 \\end{pmatrix}$$\nNow, we compute the difference and sum of the mean vectors:\n$$\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_c = \\begin{pmatrix} \\frac{12}{5} - (-3) \\\\ \\frac{21}{10} - \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{12+15}{5} \\\\ \\frac{20}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{27}{5} \\\\ 2 \\end{pmatrix}$$\n$$\\boldsymbol{\\mu}_i + \\boldsymbol{\\mu}_c = \\begin{pmatrix} \\frac{12}{5} + (-3) \\\\ \\frac{21}{10} + \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{12-15}{5} \\\\ \\frac{22}{10} \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{5} \\\\ \\frac{11}{5} \\end{pmatrix}$$\nWe calculate the weight vector $\\boldsymbol{w} = \\Sigma_p^{-1}(\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_c)$:\n$$\\boldsymbol{w} = \\frac{200}{941}\\begin{pmatrix} 30 & -17 \\\\ -17 & 41 \\end{pmatrix} \\begin{pmatrix} \\frac{27}{5} \\\\ 2 \\end{pmatrix} = \\frac{200}{941}\\begin{pmatrix} 30(\\frac{27}{5}) - 17(2) \\\\ -17(\\frac{27}{5}) + 41(2) \\end{pmatrix} = \\frac{200}{941}\\begin{pmatrix} 162 - 34 \\\\ -\\frac{459}{5} + \\frac{410}{5} \\end{pmatrix} = \\frac{200}{941}\\begin{pmatrix} 128 \\\\ -\\frac{49}{5} \\end{pmatrix}$$\n$$\\boldsymbol{w} = \\begin{pmatrix} \\frac{200 \\times 128}{941} \\\\ \\frac{200 \\times (-49)}{941 \\times 5} \\end{pmatrix} = \\begin{pmatrix} \\frac{25600}{941} \\\\ \\frac{40 \\times (-49)}{941} \\end{pmatrix} = \\begin{pmatrix} \\frac{25600}{941} \\\\ -\\frac{1960}{941} \\end{pmatrix}$$\nFinally, we calculate the intercept term $w_0 = -\\frac{1}{2}(\\boldsymbol{\\mu}_i + \\boldsymbol{\\mu}_c)^{\\top}\\boldsymbol{w}$:\n$$w_0 = -\\frac{1}{2} \\begin{pmatrix} -\\frac{3}{5} & \\frac{11}{5} \\end{pmatrix} \\begin{pmatrix} \\frac{25600}{941} \\\\ -\\frac{1960}{941} \\end{pmatrix} = -\\frac{1}{2 \\cdot 5 \\cdot 941} \\left( (-3)(25600) + (11)(-1960) \\right)$$\n$$w_0 = -\\frac{1}{9410} (-76800 - 21560) = -\\frac{1}{9410}(-98360) = \\frac{98360}{9410} = \\frac{9836}{941}$$\nCombining the terms, we get the discriminant function $g(\\Delta Cq_{t}, \\Delta Cq_{c}) = \\boldsymbol{w}^{\\top}\\boldsymbol{\\Delta} + w_0$:\n$$g(\\Delta Cq_{t}, \\Delta Cq_{c}) = \\frac{25600}{941}\\Delta Cq_{t} - \\frac{1960}{941}\\Delta Cq_{c} + \\frac{9836}{941}$$\nThis is the explicit closed-form expression for the discriminant function.",
            "answer": "$$\n\\boxed{\\frac{25600}{941}\\Delta Cq_{t} - \\frac{1960}{941}\\Delta Cq_{c} + \\frac{9836}{941}}\n$$"
        },
        {
            "introduction": "Effective laboratory quality control involves not just detecting contamination but also pinpointing its source to enable swift and effective corrective action. Analyzing the pattern of positive signals among a set of negative controls is key to this diagnostic process. In this exercise , you will use Bayesian inference to determine the most probable origin of contamination—distinguishing between a sporadic, well-specific event and a failure stemming from a shared, contaminated reagent. This practice will demonstrate how to update prior beliefs with observed evidence to calculate the posterior probability of a batch-wide contamination event, a core skill in modern laboratory quality management.",
            "id": "5146144",
            "problem": "A molecular diagnostics laboratory runs a real-time quantitative polymerase chain reaction (qPCR) assay for pathogen detection. To monitor contamination, it includes negative controls distributed across reagent preparation steps. Three separate reagent aliquots of qPCR master mix are used to set up the negative controls on the same plate. Each aliquot $i \\in \\{1,2,3\\}$ is used to prepare $n_i = 8$ negative-control wells. After amplification, the following numbers of negatives in each aliquot show amplification signal consistent with the positive control: $x_1 = 3$, $x_2 = 1$, $x_3 = 0$.\n\nAssume the following, grounded in standard contamination-control models for Polymerase Chain Reaction (PCR):\n1. Under purely sporadic, well-specific contamination (no aliquot-level contamination), each well independently amplifies with probability $p$ due to stochastic carryover. Validation history for this assay establishes $p = 0.005$ per well.\n2. If a reagent aliquot is contaminated (shared-aliquot contamination), then each well prepared from that aliquot independently amplifies with probability $q$ due to shared template carryover. Based on prior performance in contamination incident investigations with this assay and target, take $q = 0.6$ per well.\n3. Each aliquot $i$ has an independent prior probability $\\theta$ of being contaminated, with $\\theta = 0.02$, reflecting routine handling risk and historical incident rates.\n4. Conditional on an aliquot’s contamination status, wells within that aliquot are independent and identically distributed.\n\nStarting from the definitions of independent Bernoulli trials, the binomial sampling model, and Bayes’ theorem, derive the posterior probability that at least one aliquot is contaminated given the observed counts $\\{(n_i,x_i)\\}$. Then, using that posterior, infer which aliquot is the most likely source of batch-wide contamination.\n\nCompute the posterior probability that at least one aliquot is contaminated given the data above, and report this single value. Round your final answer to four significant figures. Express the probability as a decimal with no units.",
            "solution": "The problem requires the computation of the posterior probability that at least one of three reagent aliquots is contaminated, given the observed number of positive amplification signals in negative-control wells prepared from each aliquot. This problem is a classic application of Bayesian inference, involving binomial likelihoods and updating of prior beliefs based on evidence.\n\n### Step 1: Problem Formulation and Variable Definition\nLet $C_i$ be the event that aliquot $i$ is contaminated, for $i \\in \\{1, 2, 3\\}$.\nLet $\\neg C_i$ be the event that aliquot $i$ is not contaminated.\nThe prior probability for an aliquot to be contaminated is given as $P(C_i) = \\theta = 0.02$.\nThe prior probability for an aliquot to be free of contamination is $P(\\neg C_i) = 1 - \\theta = 1 - 0.02 = 0.98$.\nThe problem states that these events are independent for each aliquot.\n\nThe data for aliquot $i$ is denoted by $D_i = (n_i, x_i)$, where $n_i$ is the number of wells and $x_i$ is the number of wells showing amplification. We are given:\n- Aliquot 1: $n_1=8$, $x_1=3$.\n- Aliquot 2: $n_2=8$, $x_2=1$.\n- Aliquot 3: $n_3=8$, $x_3=0$.\nThe total dataset is $D = \\{D_1, D_2, D_3\\}$.\n\nThe amplification of wells is modeled as a series of independent Bernoulli trials. The number of positive wells $x_i$ out of $n_i$ thus follows a binomial distribution. The probability of amplification per well depends on the contamination status of the aliquot:\n- If aliquot $i$ is not contaminated ($\\neg C_i$), amplification occurs with probability $p = 0.005$ per well (sporadic contamination).\n- If aliquot $i$ is contaminated ($C_i$), amplification occurs with probability $q = 0.6$ per well (shared-aliquot contamination).\n\nThe likelihood of observing data $D_i$ given the contamination status of aliquot $i$ is given by the binomial probability mass function:\n- Likelihood given contamination: $P(D_i | C_i) = \\binom{n_i}{x_i} q^{x_i} (1-q)^{n_i - x_i}$.\n- Likelihood given no contamination: $P(D_i | \\neg C_i) = \\binom{n_i}{x_i} p^{x_i} (1-p)^{n_i - x_i}$.\n\n### Step 2: Strategy for Calculating the Posterior Probability\nWe want to find the posterior probability that at least one aliquot is contaminated, given the data $D$. This can be written as $P(C_1 \\cup C_2 \\cup C_3 | D)$. It is computationally simpler to calculate the probability of the complementary event: that no aliquots are contaminated, $P(\\neg C_1 \\cap \\neg C_2 \\cap \\neg C_3 | D)$, and subtract it from $1$.\n$$ P(\\text{at least one } C_i | D) = 1 - P(\\neg C_1 \\cap \\neg C_2 \\cap \\neg C_3 | D) $$\nThe contamination statuses $C_i$ are independent a priori. Furthermore, the data $D_i$ for each aliquot is conditionally independent of the data from other aliquots, given the contamination status of aliquot $i$. This means $P(D_i|C_i, C_j, \\dots) = P(D_i|C_i)$. This structure implies that the posterior probabilities of contamination for each aliquot are also independent. That is, $P(C_i, C_j | D_i, D_j) = P(C_i|D_i)P(C_j|D_j)$.\nTherefore, we can write:\n$$ P(\\neg C_1 \\cap \\neg C_2 \\cap \\neg C_3 | D) = P(\\neg C_1 | D_1) \\times P(\\neg C_2 | D_2) \\times P(\\neg C_3 | D_3) $$\nOur strategy will be to calculate the posterior probability of non-contamination, $P(\\neg C_i | D_i)$, for each aliquot separately using Bayes' theorem, and then combine the results.\n\n### Step 3: Application of Bayes' Theorem for a Single Aliquot\nFor each aliquot $i$, the posterior probability of non-contamination is:\n$$ P(\\neg C_i | D_i) = \\frac{P(D_i | \\neg C_i) P(\\neg C_i)}{P(D_i)} $$\nThe denominator, $P(D_i)$, is the marginal likelihood or evidence, calculated by the law of total probability:\n$$ P(D_i) = P(D_i | C_i)P(C_i) + P(D_i | \\neg C_i)P(\\neg C_i) $$\nSubstituting this into the Bayes' formula:\n$$ P(\\neg C_i | D_i) = \\frac{P(D_i | \\neg C_i) (1-\\theta)}{P(D_i | C_i)\\theta + P(D_i | \\neg C_i) (1-\\theta)} $$\n\n### Step 4: Numerical Calculation for Each Aliquot\nLet's compute the likelihoods for each aliquot's data.\nWe have $n=8, p=0.005, q=0.6, \\theta=0.02$.\n\n**Aliquot 1 ($n_1=8, x_1=3$):**\n$\\binom{8}{3} = \\frac{8 \\cdot 7 \\cdot 6}{3 \\cdot 2 \\cdot 1} = 56$.\n$P(D_1|C_1) = \\binom{8}{3} q^3 (1-q)^{5} = 56 \\times (0.6)^3 \\times (0.4)^5 = 56 \\times 0.216 \\times 0.01024 = 0.12386304$.\n$P(D_1|\\neg C_1) = \\binom{8}{3} p^3 (1-p)^{5} = 56 \\times (0.005)^3 \\times (0.995)^5 \\approx 6.8267 \\times 10^{-6}$.\n\nPosterior for Aliquot 1:\n$$ P(\\neg C_1 | D_1) = \\frac{(6.8267 \\times 10^{-6})(0.98)}{(0.12386304)(0.02) + (6.8267 \\times 10^{-6})(0.98)} = \\frac{6.6902 \\times 10^{-6}}{0.00247726 + 6.6902 \\times 10^{-6}} \\approx 0.0026932 $$\n\n**Aliquot 2 ($n_2=8, x_2=1$):**\n$\\binom{8}{1} = 8$.\n$P(D_2|C_2) = \\binom{8}{1} q^1 (1-q)^{7} = 8 \\times (0.6)^1 \\times (0.4)^7 = 8 \\times 0.6 \\times 0.0016384 = 0.00786432$.\n$P(D_2|\\neg C_2) = \\binom{8}{1} p^1 (1-p)^{7} = 8 \\times 0.005 \\times (0.995)^7 \\approx 0.03862268$.\n\nPosterior for Aliquot 2:\n$$ P(\\neg C_2 | D_2) = \\frac{(0.03862268)(0.98)}{(0.00786432)(0.02) + (0.03862268)(0.98)} = \\frac{0.03785023}{0.00015729 + 0.03785023} \\approx 0.99586 $$\n\n**Aliquot 3 ($n_3=8, x_3=0$):**\n$\\binom{8}{0} = 1$.\n$P(D_3|C_3) = \\binom{8}{0} q^0 (1-q)^{8} = (0.4)^8 = 0.00065536$.\n$P(D_3|\\neg C_3) = \\binom{8}{0} p^0 (1-p)^{8} = (0.995)^8 \\approx 0.96078805$.\n\nPosterior for Aliquot 3:\n$$ P(\\neg C_3 | D_3) = \\frac{(0.96078805)(0.98)}{(0.00065536)(0.02) + (0.96078805)(0.98)} = \\frac{0.94157229}{0.00001311 + 0.94157229} \\approx 0.999986 $$\n\nAs a side note for the inference task, we can calculate the posterior probability of contamination for each aliquot:\n$P(C_1|D_1) = 1 - P(\\neg C_1|D_1) \\approx 1 - 0.0026932 = 0.9973068$.\n$P(C_2|D_2) = 1 - P(\\neg C_2|D_2) \\approx 1 - 0.99586 = 0.00414$.\n$P(C_3|D_3) = 1 - P(\\neg C_3|D_3) \\approx 1 - 0.999986 = 0.000014$.\nThe data strongly indicate that aliquot 1 is contaminated (posterior probability $\\approx 99.7 \\%$), while aliquots 2 and 3 are very likely not contaminated. Thus, aliquot 1 is the most likely source.\n\n### Step 5: Final Calculation\nNow, we compute the posterior probability that none of the aliquots are contaminated:\n$$ P(\\text{none contaminated} | D) = P(\\neg C_1 | D_1) \\times P(\\neg C_2 | D_2) \\times P(\\neg C_3 | D_3) $$\n$$ P(\\text{none contaminated} | D) \\approx 0.0026932 \\times 0.99586 \\times 0.999986 \\approx 0.0026820 $$\nFinally, the probability of at least one aliquot being contaminated is:\n$$ P(\\text{at least one contaminated} | D) = 1 - P(\\text{none contaminated} | D) $$\n$$ P(\\text{at least one contaminated} | D) \\approx 1 - 0.0026820 = 0.997318 $$\nRounding the result to four significant figures, we get $0.9973$.",
            "answer": "$$\n\\boxed{0.9973}\n$$"
        }
    ]
}