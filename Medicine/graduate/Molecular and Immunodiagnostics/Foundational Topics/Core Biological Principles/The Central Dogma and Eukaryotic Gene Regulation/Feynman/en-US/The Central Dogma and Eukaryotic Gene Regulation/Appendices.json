{
    "hands_on_practices": [
        {
            "introduction": "The control of gene expression often begins with the binding of transcription factors (TFs) to specific DNA sites. To build predictive models of regulation, we must first understand the physical chemistry of these interactions. This practice challenges you to derive the fundamental equations for TF binding from first principles, including the crucial concept of cooperativity, which allows for sophisticated, switch-like regulatory control .",
            "id": "5167750",
            "problem": "In a chromatin immunoprecipitation-based transcription factor occupancy assay for a eukaryotic promoter, you are modeling binding of a single Transcription Factor (TF) species to one or two nearby, identical DNA sites. Assume rapid equilibrium, a well-mixed nucleus, and that the free TF concentration $[TF]$ is effectively constant relative to the number of DNA sites assayed. The dissociation constant $K_d$ for a single TF binding site is defined by the equilibrium relation $K_d = \\frac{[TF][S]}{[TF{:}S]}$, where $[S]$ denotes free site concentration and $[TF{:}S]$ denotes concentration of the TF-bound site complex. For two sites, assume each site is characterized by the same $K_d$ when bound independently. To capture cooperativity between the two sites upon simultaneous occupancy, introduce a dimensionless cooperativity factor $\\omega$, defined such that the statistical weight of the double-occupied state is multiplied by $\\omega$ relative to the product of independent single-site weights; $\\omega > 1$ represents positive cooperativity, $\\omega = 1$ independent binding, and $\\omega < 1$ negative cooperativity. Using equilibrium statistical mechanics and mass-action binding, first derive the equilibrium occupancy probability for a single site. Then extend the derivation to two identical sites to obtain the equilibrium probability that both sites are simultaneously occupied as a function of $[TF]$, $K_d$, and $\\omega$. Provide your final answer as a single closed-form analytic expression for the probability of double occupancy. No numerical approximation or rounding is required, and do not include units.",
            "solution": "The problem requires the derivation of the equilibrium probability of simultaneous occupancy of two identical, cooperative transcription factor (TF) binding sites. We will approach this using the principles of equilibrium statistical mechanics.\n\nFirst, let us validate the problem statement.\n**Step 1: Extract Givens**\n- **System:** A single Transcription Factor (TF) species binding to one or two nearby, identical DNA sites.\n- **Assumptions:** Rapid equilibrium, a well-mixed nucleus, and the free TF concentration, denoted as $[TF]$, is constant.\n- **Definition 1 (Dissociation Constant):** For a single site `S`, the dissociation constant $K_d$ is given by the equilibrium relation $K_d = \\frac{[TF][S]}{[TF{:}S]}$, where $[S]$ is the free site concentration and $[TF{:}S]$ is the concentration of the TF-bound site complex.\n- **Definition 2 (Cooperativity Factor):** For a two-site system, a dimensionless cooperativity factor $\\omega$ is introduced. The statistical weight of the double-occupied state is multiplied by $\\omega$ relative to the product of the independent single-site weights.\n- **Task:** 1) Derive the equilibrium occupancy probability for a single site. 2) Extend the derivation to two identical sites to find the equilibrium probability that both sites are simultaneously occupied, expressed as a function of $[TF]$, $K_d$, and $\\omega$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, utilizing standard models of biomolecular binding (mass action law, statistical mechanics, cooperativity) found in physical chemistry and molecular biology. The terms are well-defined ($K_d$, $\\omega$), and the assumptions (constant $[TF]$, equilibrium) are standard for simplifying such models. The problem is self-contained, objective, and well-posed, asking for a specific quantity based on the provided parameters. There are no contradictions, factual errors, or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will proceed with the solution.\n\n**Derivation for a Single Binding Site**\nFor a single binding site, there are two possible states: unbound ($S$) and bound ($TF{:}S$). The probability of occupancy, $P_{\\text{occ,1}}$, is the fraction of total sites in the bound state:\n$$P_{\\text{occ,1}} = \\frac{[TF{:}S]}{[S] + [TF{:}S]}$$\nFrom the definition of the dissociation constant, $K_d = \\frac{[TF][S]}{[TF{:}S]}$, we can express the concentration of free sites $[S]$ in terms of the bound sites $[TF{:}S]$:\n$$[S] = K_d \\frac{[TF{:}S]}{[TF]}$$\nSubstituting this expression for $[S]$ into the equation for $P_{\\text{occ,1}}$:\n$$P_{\\text{occ,1}} = \\frac{[TF{:}S]}{K_d \\frac{[TF{:}S]}{[TF]} + [TF{:}S]}$$\nDividing the numerator and denominator by $[TF{:}S]$ (which must be non-zero for the question to be meaningful):\n$$P_{\\text{occ,1}} = \\frac{1}{\\frac{K_d}{[TF]} + 1}$$\nSimplifying this expression gives the occupancy probability for a single site, a result formally known as the Langmuir isotherm:\n$$P_{\\text{occ,1}} = \\frac{[TF]}{K_d + [TF]}$$\nThis completes the first part of the task.\n\n**Derivation for Two Cooperative Binding Sites**\nFor the system with two identical sites, we use the statistical mechanics framework. The probability of the system being in a specific state is the ratio of that state's statistical weight to the sum of all possible statistical weights (the partition function, $Z$).\n\nLet the two sites be $S_1$ and $S_2$. There are four possible microstates for the system:\n1.  Both sites unbound ($S_1, S_2$).\n2.  Site $1$ bound, site $2$ unbound ($TF{:}S_1, S_2$).\n3.  Site $1$ unbound, site $2$ bound ($S_1, TF{:}S_2$).\n4.  Both sites bound ($TF{:}S_1, TF{:}S_2$).\n\nWe define the statistical weight of the fully unbound state as unity.\n-   Statistical weight $W_{00}$ for state ($S_1, S_2$): $W_{00} = 1$.\n\nThe statistical weight of a state with one TF molecule bound is proportional to the concentration of the ligand, $[TF]$, and inversely proportional to its dissociation constant, $K_d$.\n-   Statistical weight $W_{10}$ for state ($TF{:}S_1, S_2$): $W_{10} = \\frac{[TF]}{K_d}$.\n-   Statistical weight $W_{01}$ for state ($S_1, TF{:}S_2$): $W_{01} = \\frac{[TF]}{K_d}$, since the sites are identical.\n\nThe statistical weight for the double-occupancy state accounts for two binding events. If the binding events were independent ($\\omega=1$), the weight would be the product of the individual weights, i.e., $(\\frac{[TF]}{K_d}) \\times (\\frac{[TF]}{K_d})$. The problem introduces the cooperativity factor $\\omega$ which multiplies this independent weight.\n-   Statistical weight $W_{11}$ for state ($TF{:}S_1, TF{:}S_2$): $W_{11} = \\omega \\left(\\frac{[TF]}{K_d}\\right)^2 = \\omega \\frac{[TF]^2}{K_d^2}$.\n\nThe partition function, $Z$, is the sum of the statistical weights of all possible states:\n$$Z = W_{00} + W_{10} + W_{01} + W_{11}$$\n$$Z = 1 + \\frac{[TF]}{K_d} + \\frac{[TF]}{K_d} + \\omega \\frac{[TF]^2}{K_d^2}$$\n$$Z = 1 + 2 \\frac{[TF]}{K_d} + \\omega \\frac{[TF]^2}{K_d^2}$$\n\nThe problem asks for the equilibrium probability that both sites are simultaneously occupied. This corresponds to the probability of being in state ($TF{:}S_1, TF{:}S_2$), which we denote as $P_{11}$. This probability is the statistical weight of the double-occupied state, $W_{11}$, divided by the partition function, $Z$.\n$$P_{11} = \\frac{W_{11}}{Z} = \\frac{\\omega \\frac{[TF]^2}{K_d^2}}{1 + 2 \\frac{[TF]}{K_d} + \\omega \\frac{[TF]^2}{K_d^2}}$$\nTo obtain a simplified closed-form expression, we can multiply the numerator and the denominator by $K_d^2$:\n$$P_{11} = \\frac{K_d^2 \\left(\\omega \\frac{[TF]^2}{K_d^2}\\right)}{K_d^2 \\left(1 + 2 \\frac{[TF]}{K_d} + \\omega \\frac{[TF]^2}{K_d^2}\\right)}$$\n$$P_{11} = \\frac{\\omega [TF]^2}{K_d^2 + 2 K_d [TF] + \\omega [TF]^2}$$\nThis is the final expression for the probability of double occupancy as a function of $[TF]$, $K_d$, and $\\omega$.",
            "answer": "$$\\boxed{\\frac{\\omega [TF]^2}{K_d^2 + 2 K_d [TF] + \\omega [TF]^2}}$$"
        },
        {
            "introduction": "With a model for transcription factor binding in hand, a key challenge is to identify their target sites across the vast expanse of the genome. Techniques like ChIP-seq reveal regions bound by a specific TF, and this exercise guides you through the essential bioinformatic task of statistically testing whether these regions are significantly enriched for the TF's known DNA binding motif. This practice is fundamental to linking TFs to their regulatory targets on a genomic scale .",
            "id": "5167759",
            "problem": "You are investigating transcription factor binding in eukaryotic gene regulation within the framework of the Central Dogma of Molecular Biology, which states that deoxyribonucleic acid (DNA) is transcribed into ribonucleic acid (RNA) and translated into protein. Transcription factors bind deoxyribonucleic acid (DNA) at specific short sequence motifs to regulate gene expression; Chromatin Immunoprecipitation sequencing (ChIP-seq) identifies genomic regions enriched for such binding. In a diagnostic context, a motif is considered enriched in a ChIP-seq peak set if its frequency in peaks exceeds what would be expected from a defined genomic background.\n\nFrom first principles, model motif incidence in peaks relative to background using either sampling without replacement or independent Bernoulli trials. Formulate a right-tailed hypothesis test of enrichment with the following null hypothesis: the motif frequency in the peak set equals the background frequency. For sampling without replacement from a finite population, use a model consistent with the hypergeometric distribution; for independent Bernoulli trials with success probability equal to the background proportion, use a model consistent with the binomial distribution. For each motif, compute the right-tailed p-value $\\Pr(X \\ge k)$, where $X$ is the random variable counting motif occurrences in the peak set and $k$ is the observed count. Across motifs within each test case, correct for multiple testing using the Benjamini–Hochberg (BH) False Discovery Rate (FDR) procedure and return the Benjamini–Hochberg adjusted p-values. Express the FDR control level as a decimal (e.g., $0.05$), but you do not need to output significance decisions; only output the adjusted p-values.\n\nDefine the parameters for each motif as follows. Let $M$ denote the total number of background regions examined, $K$ denote the number of background regions containing the motif, $n$ denote the number of ChIP-seq peaks, and $k$ denote the number of peaks containing the motif. For the binomial model, the null success probability is $p_0 = K / M$.\n\nImplement a program that, for each test case below, computes the right-tailed p-values for all listed motifs using the specified model, applies the Benjamini–Hochberg procedure across those p-values, and outputs the Benjamini–Hochberg adjusted p-values. Round each adjusted p-value to $6$ decimal places.\n\nTest suite (three distinct test cases that together cover typical, boundary, and edge scenarios):\n\n- Test Case $1$ (sampling without replacement; hypergeometric model):\n  - Shared parameters: $M = 10000$, $n = 500$.\n  - Motif $\\mathrm{A}$: $K = 800$, $k = 70$.\n  - Motif $\\mathrm{B}$: $K = 300$, $k = 10$.\n  - Motif $\\mathrm{C}$: $K = 1500$, $k = 30$.\n\n- Test Case $2$ (independent Bernoulli trials; binomial model):\n  - Shared parameters: $M = 20000$, $n = 40$; the null probability is $p_0 = K/M$ for each motif.\n  - Motif $\\mathrm{D}$: $K = 400$, $k = 0$ (tests zero successes).\n  - Motif $\\mathrm{E}$: $K = 1200$, $k = 40$ (tests all successes).\n  - Motif $\\mathrm{F}$: $K = 900$, $k = 3$ (moderate case).\n\n- Test Case $3$ (edge conditions; hypergeometric model):\n  - Shared parameters: $M = 1000$, $n = 50$.\n  - Motif $\\mathrm{G}$: $K = 0$, $k = 0$ (no successes possible).\n  - Motif $\\mathrm{H}$: $K = 1000$, $k = 50$ (all draws are successes).\n  - Motif $\\mathrm{I}$: $K = 80$, $k = 2$ (near-background scenario).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should be the concatenation of the Benjamini–Hochberg adjusted p-values from Test Case $1$ in the order $\\mathrm{A}, \\mathrm{B}, \\mathrm{C}$, then from Test Case $2$ in the order $\\mathrm{D}, \\mathrm{E}, \\mathrm{F}$, then from Test Case $3$ in the order $\\mathrm{G}, \\mathrm{H}, \\mathrm{I}$. For example, the output format should be of the form $[\\text{adjA},\\text{adjB},\\text{adjC},\\text{adjD},\\text{adjE},\\text{adjF},\\text{adjG},\\text{adjH},\\text{adjI}]$, where each element is a float rounded to $6$ decimal places.",
            "solution": "The problem statement has been evaluated and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution. The described scenario—testing for transcription factor motif enrichment in ChIP-seq data using hypergeometric or binomial models, followed by multiple testing correction—is a standard and fundamental procedure in computational biology and bioinformatics. The provided parameters and test cases, including edge conditions, are appropriate for validating a correct implementation.\n\nThe solution will be developed by first principles, as requested.\n\n### Principles of Motif Enrichment Analysis\n\nThe core task is to determine if a short DNA sequence motif occurs more frequently within a set of genomic regions of interest (ChIP-seq peaks) than would be expected by chance, based on its frequency in a larger genomic background. This is framed as a statistical hypothesis test.\n\n**1. Hypothesis Testing Framework**\n\nFor each motif, we test a null hypothesis ($H_0$) against an alternative hypothesis ($H_1$).\n- **Null Hypothesis ($H_0$)**: The frequency of the motif in the peak set is the same as its frequency in the genomic background. Any observed excess is due to random sampling variation.\n- **Alternative Hypothesis ($H_1$)**: The frequency of the motif in the peak set is greater than its frequency in the background, suggesting biological enrichment.\n\nThis is a **right-tailed test**. We calculate a p-value, which is the probability of observing a motif count in the peaks at least as extreme as the one actually observed ($k$), assuming the null hypothesis is true. A small p-value suggests that the observation is unlikely under the null hypothesis, providing evidence for enrichment.\n\nThe p-value is formally $\\Pr(X \\ge k)$, where $X$ is the random variable representing the number of motifs found in the peak set under the null model, and $k$ is the observed count.\n\n**2. Statistical Models for the Null Distribution**\n\nThe problem specifies two distinct sampling models for the null distribution of motif counts.\n\n**a) Hypergeometric Model (Sampling Without Replacement)**\n\nThis model is appropriate when the peak set is considered a sample drawn *without replacement* from a finite background population.\n\n- Let $M$ be the total number of regions in the genomic background.\n- Let $K$ be the number of those background regions that contain the motif.\n- Let $n$ be the number of regions in the peak set (the sample size).\n- Let $X$ be the random variable for the number of peaks in the sample that contain the motif.\n\nUnder the null hypothesis, $X$ follows a hypergeometric distribution, $X \\sim \\text{Hypergeometric}(M, K, n)$. The probability mass function (PMF) is given by:\n$$P(X=i) = \\frac{\\binom{K}{i} \\binom{M-K}{n-i}}{\\binom{M}{n}}$$\nThe p-value is the probability of observing $k$ or more motifs, calculated using the survival function (SF):\n$$p = \\Pr(X \\ge k) = \\sum_{i=k}^{\\min(n, K)} P(X=i)$$\n\nThis model is used for Test Case $1$ and Test Case $3$.\n\n**b) Binomial Model (Sampling With Replacement / Independent Bernoulli Trials)**\n\nThis model is appropriate when the background population $M$ is very large relative to the sample size $n$, or when the peaks are considered a series of independent trials.\n\n- Let $n$ be the number of peaks (trials).\n- Let $p_0$ be the probability of a single peak containing the motif under the null hypothesis. This is estimated from the background as $p_0 = K/M$.\n- Let $Y$ be the random variable for the number of peaks containing the motif.\n\nUnder the null hypothesis, $Y$ follows a binomial distribution, $Y \\sim \\text{Binomial}(n, p_0)$. The PMF is:\n$$P(Y=i) = \\binom{n}{i} p_0^i (1-p_0)^{n-i}$$\nThe p-value is the probability of observing $k$ or more motifs:\n$$p = \\Pr(Y \\ge k) = \\sum_{i=k}^{n} P(Y=i)$$\n\nThis model is used for Test Case $2$.\n\n**3. Multiple Testing Correction: The Benjamini–Hochberg (BH) Procedure**\n\nWhen performing multiple hypothesis tests simultaneously (e.g., one for each motif), the probability of making at least one false discovery (a Type I error) increases. The Benjamini–Hochberg procedure is a standard method to control the False Discovery Rate (FDR), which is the expected proportion of rejected null hypotheses that are actually true.\n\nThe procedure to compute BH-adjusted p-values for a set of $m$ raw p-values ($p_1, p_2, \\ldots, p_m$) is as follows:\n1.  Rank the raw p-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\ldots \\le p_{(m)}$.\n2.  For each ranked p-value $p_{(i)}$, calculate an intermediate value: $q_{(i)} = \\frac{m}{i} p_{(i)}$.\n3.  To ensure that the adjusted p-values are monotonically non-decreasing with the raw p-values, the final adjusted p-value for rank $i$ is calculated as the cumulative minimum of the intermediate values from that rank upwards: $p_{adj, (i)} = \\min_{j=i}^{m} \\{q_{(j)}\\}$.\n4.  Finally, cap any adjusted p-value that exceeds $1.0$ at $1.0$: $p_{adj, (i)}' = \\min(p_{adj, (i)}, 1.0)$.\n5.  The adjusted p-values are then returned to their original, unranked order.\n\n### Computational Strategy and Execution\n\nThe implementation will proceed by calculating the raw p-values for each motif within a test case, and then applying the BH procedure to that set of p-values.\n\nFor calculating the raw p-values $\\Pr(X \\ge k)$, we use the survival function (SF) provided by `scipy.stats`. For a discrete distribution, `dist.sf(x)` computes $\\Pr(X > x)$. Therefore, to compute $\\Pr(X \\ge k)$, we must evaluate `dist.sf(k-1)`.\n\n**Test Case $1$: Hypergeometric Model**\nShared parameters: $M = 10000$, $n = 500$.\n- Motif $\\mathrm{A}$: $K=800, k=70$. The expected count is $n \\times (K/M) = 500 \\times (800/10000) = 40$. The observed count of $70$ suggests strong enrichment.\n- Motif $\\mathrm{B}$: $K=300, k=10$. The expected count is $500 \\times (300/10000) = 15$. The observed count of $10$ is less than expected, suggesting no enrichment.\n- Motif $\\mathrm{C}$: $K=1500, k=30$. The expected count is $500 \\times (1500/10000) = 75$. The observed count of $30$ is much less than expected.\nThe raw p-values for motifs $\\mathrm{A}$, $\\mathrm{B}$, and $\\mathrm{C}$ are computed using the hypergeometric survival function, and the BH procedure is applied to this set of three p-values.\n\n**Test Case $2$: Binomial Model**\nShared parameters: $M = 20000$, $n = 40$.\n- Motif $\\mathrm{D}$: $K=400, k=0$. The null probability is $p_0 = 400/20000 = 0.02$. Observing $k=0$ is not evidence against the null (i.e., not evidence for enrichment), so we expect a high p-value. $\\Pr(Y \\ge 0) = 1$.\n- Motif $\\mathrm{E}$: $K=1200, k=40$. The null probability is $p_0 = 1200/20000 = 0.06$. Observing the motif in all $40$ peaks when the background probability is only $0.06$ is extremely unlikely under the null, suggesting powerful enrichment.\n- Motif $\\mathrm{F}$: $K=900, k=3$. The null probability is $p_0 = 900/20000 = 0.045$. The expected count is $n \\times p_0 = 40 \\times 0.045 = 1.8$. Observing $3$ is slightly above expectation.\nThe raw p-values for motifs $\\mathrm{D}$, $\\mathrm{E}$, and $\\mathrm{F}$ are computed using the binomial survival function, and the BH procedure is applied.\n\n**Test Case $3$: Hypergeometric Edge Conditions**\nShared parameters: $M=1000, n=50$.\n- Motif $\\mathrm{G}$: $K=0, k=0$. If the motif does not exist in the background ($K=0$), it is impossible to observe it in the sample. The only possible outcome is $k=0$, so $\\Pr(X=0)=1$ and $\\Pr(X \\ge 0) = 1$. A p-value of $1.0$ is expected.\n- Motif $\\mathrm{H}$: $K=1000, k=50$. If the motif is in every background region ($K=M$), every sampled peak must contain it. The only possible outcome is $k=n=50$, so $\\Pr(X=50)=1$ and $\\Pr(X \\ge 50)=1$. A p-value of $1.0$ is expected.\n- Motif $\\mathrm{I}$: $K=80, k=2$. The expected count is $50 \\times (80/1000) = 4$. Observing $2$ is below expectation.\nThe raw p-values for motifs $\\mathrm{G}$, $\\mathrm{H}$, and $\\mathrm{I}$ are computed, and the BH procedure is applied.\n\nThe final output is the concatenation of all $9$ adjusted p-values, rounded to $6$ decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Computes motif enrichment p-values, applies Benjamini-Hochberg correction,\n    and returns the adjusted p-values for all test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"model\": \"hypergeometric\",\n            \"params\": {\"M\": 10000, \"n\": 500},\n            \"motifs\": [\n                {\"name\": \"A\", \"K\": 800, \"k\": 70},\n                {\"name\": \"B\", \"K\": 300, \"k\": 10},\n                {\"name\": \"C\", \"K\": 1500, \"k\": 30},\n            ],\n        },\n        {\n            \"model\": \"binomial\",\n            \"params\": {\"M\": 20000, \"n\": 40},\n            \"motifs\": [\n                {\"name\": \"D\", \"K\": 400, \"k\": 0},\n                {\"name\": \"E\", \"K\": 1200, \"k\": 40},\n                {\"name\": \"F\", \"K\": 900, \"k\": 3},\n            ],\n        },\n        {\n            \"model\": \"hypergeometric\",\n            \"params\": {\"M\": 1000, \"n\": 50},\n            \"motifs\": [\n                {\"name\": \"G\", \"K\": 0, \"k\": 0},\n                {\"name\": \"H\", \"K\": 1000, \"k\": 50},\n                {\"name\": \"I\", \"K\": 80, \"k\": 2},\n            ],\n        },\n    ]\n\n    def benjamini_hochberg(p_values):\n        \"\"\"\n        Applies the Benjamini-Hochberg FDR correction to a list of p-values.\n        \"\"\"\n        p_values = np.asarray(p_values)\n        n_tests = len(p_values)\n        \n        # Original indices to restore order later\n        original_indices = np.arange(n_tests)\n        \n        # Sort p-values and their original indices\n        sorted_indices = np.argsort(p_values)\n        sorted_p_values = p_values[sorted_indices]\n        \n        # Calculate ranks (1-based) for sorted p-values\n        ranks = np.arange(1, n_tests + 1)\n        \n        # Calculate BH adjusted p-values for the sorted list\n        adj_p_values_sorted = (n_tests / ranks) * sorted_p_values\n        \n        # Enforce monotonicity: p_adj(i) = min(p_adj(i), p_adj(i+1), ...)\n        # This is achieved by a cumulative minimum on the reversed array\n        monotonic_adj_p = np.minimum.accumulate(adj_p_values_sorted[::-1])[::-1]\n        \n        # Cap values at 1.0\n        final_adj_p_sorted = np.minimum(monotonic_adj_p, 1.0)\n        \n        # Restore original order\n        # Create an empty array and fill it using the sorted indices\n        restored_adj_p = np.empty(n_tests)\n        restored_adj_p[sorted_indices] = final_adj_p_sorted\n        \n        return restored_adj_p.tolist()\n\n    all_results = []\n    \n    for case in test_cases:\n        raw_p_values = []\n        if case[\"model\"] == \"hypergeometric\":\n            M = case[\"params\"][\"M\"]\n            n = case[\"params\"][\"n\"]\n            for motif in case[\"motifs\"]:\n                K = motif[\"K\"]\n                k = motif[\"k\"]\n                # Pr(X >= k) is the survival function at k-1 for discrete distributions\n                p_val = stats.hypergeom.sf(k - 1, M, K, n)\n                raw_p_values.append(p_val)\n        \n        elif case[\"model\"] == \"binomial\":\n            M = case[\"params\"][\"M\"]\n            n = case[\"params\"][\"n\"]\n            for motif in case[\"motifs\"]:\n                K = motif[\"K\"]\n                k = motif[\"k\"]\n                p0 = K / M\n                # Pr(Y >= k) is the survival function at k-1\n                p_val = stats.binom.sf(k - 1, n, p0)\n                raw_p_values.append(p_val)\n\n        # Apply Benjamini-Hochberg correction\n        adjusted_p_values = benjamini_hochberg(raw_p_values)\n        \n        # Round results to 6 decimal places and add to the final list\n        all_results.extend([round(p, 6) for p in adjusted_p_values])\n\n    # Format the final output as a comma-separated list in brackets\n    # Using .6f ensures trailing zeros for consistent formatting\n    formatted_results = [f\"{res:.6f}\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Eukaryotic gene regulation is not confined to the linear sequence of DNA; it unfolds in three dimensions. Distal enhancers can regulate promoters hundreds of kilobases away by looping through nuclear space to make physical contact. This practice simulates the analysis of Hi-C data, a powerful technique for mapping the 3D genome, challenging you to develop a normalization pipeline that can distinguish true, statistically significant chromatin loops from background noise and experimental biases .",
            "id": "5167804",
            "problem": "You are given synthetic genome-wide contact data representing High-throughput Chromosome Conformation Capture (Hi-C) assays. In Eukaryotic gene regulation, distal regulatory elements (enhancers) physically contact promoters to modulate transcription, consistent with the Central Dogma from deoxyribonucleic acid (DNA) to ribonucleic acid (RNA) to protein via regulated transcription. In Hi-C, contacts decay with genomic distance due to polymer physics, and raw counts are confounded by locus-specific coverage and sampling biases. Your task is to design a program that constructs an expected contact model using contact decay with genomic distance, normalizes for coverage, and calls statistically significant loops.\n\nFundamental base and assumptions:\n- Distal regulation is mediated by three-dimensional chromatin folding; contact probability decays with genomic distance. Thus, the expected contact function depends on genomic distance and local coverage. Observed ligation counts are discrete events, which for moderate coverage can be treated as random variables obeying count statistics, where the variance scales with the mean under a Poisson approximation. This provides a principled, testable normalization to detect outliers with respect to the expected.\n- Observed contacts are represented by a symmetric, nonnegative integer matrix $C \\in \\mathbb{N}^{n \\times n}$ with $C_{ij} = C_{ji}$ for bin indices $i$ and $j$ along a single chromosome. Each bin has a coverage bias $b_i > 0$ that scales its expected contact rate with other loci. The genomic distance between bins is $d_{ij} = |i-j| \\cdot \\Delta$, where $\\Delta$ is the bin size in base pairs.\n\nDefinitions and required normalization:\n- For each pair $(i,j)$ with $i \\neq j$, define the bias-corrected contact $x_{ij} = C_{ij} / (b_i b_j)$.\n- Partition the set of pairs $(i,j)$ by discrete genomic separation $s = |i-j| \\in \\{1,2,\\dots,n-1\\}$, which corresponds to distance $d_{ij} = s \\cdot \\Delta$. For each separation $s$, collect the multiset $X_s = \\{ x_{ij} : |i-j| = s, i < j \\}$.\n- Estimate the expected baseline contact as a function of separation via a robust aggregation. Let the trimming fraction be $\\tau \\in (0, 0.5)$. For each $s$, define the trimmed mean $\\hat{\\mu}(s)$ by sorting $X_s$ and removing the lowest $\\lfloor \\tau |X_s| \\rfloor$ and highest $\\lfloor \\tau |X_s| \\rfloor$ elements, then averaging the remaining values. If $|X_s|$ is too small to trim (for example $|X_s| < 3$), use the plain average.\n- The expected contact for a pair $(i,j)$ is $E_{ij} = \\hat{\\mu}(|i-j|) \\cdot b_i b_j$.\n\nSignificance metric and calling rule:\n- Under a Poisson approximation, use the standardized residual $z_{ij} = (C_{ij} - E_{ij}) / \\sqrt{E_{ij} + \\varepsilon}$, where $\\varepsilon$ is a small positive constant to avoid division by zero.\n- A candidate loop at $(i,j)$ is called “significant” if two conditions hold simultaneously:\n  1. $z_{ij} \\ge z^\\star$ for a specified threshold $z^\\star$.\n  2. $z_{ij}$ is a strict local maximum relative to all neighbors $(p,q)$ within a Chebyshev radius $R$ in the contact map, that is $|p-i| \\le R$ and $|q-j| \\le R$, excluding $(p,q)=(i,j)$ itself.\n\nYour program must implement this normalization and calling rule and evaluate a provided test suite. For each test case, synthetically generate $C$ from a distance-decay baseline, multiplicative coverage $b_i$, and optionally add loop increments. Specifically, for $i < j$ define a baseline mean $\\lambda_{ij}^{\\mathrm{base}} = \\lambda_0 / (1 + |i-j|)$, then set a mean $\\Lambda_{ij} = \\lambda_{ij}^{\\mathrm{base}} \\cdot b_i b_j + \\sum_{\\ell} \\delta_{\\ell} \\mathbf{1}\\{(i,j) = (p_{\\ell}, q_{\\ell})\\}$, where $(p_{\\ell}, q_{\\ell})$ are specified loop pairs and $\\delta_{\\ell}$ are positive increments. Sample $C_{ij} \\sim \\mathrm{Poisson}(\\Lambda_{ij})$, set $C_{ji} = C_{ij}$, and set $C_{ii} = 0$. Use specified pseudorandom seeds for reproducibility.\n\nParameters to use in the test suite:\n- Global calling parameters: trimming fraction $\\tau = 0.1$, neighborhood radius $R = 1$, threshold $z^\\star = 3$, small constant $\\varepsilon = 10^{-9}$. All distances use base pairs with bin size $\\Delta$ as specified in each case.\n- Case $1$ (happy path):\n  - Matrix size $n = 28$, bin size $\\Delta = 10000$ base pairs, baseline $\\lambda_0 = 60$, coverage $b_i = 1$ for all $i$, pseudorandom seed $2023$.\n  - Insert a loop at $(6,21)$ with increment $\\delta = 180$.\n  - Evaluate the candidate list $\\{(6,21), (10,20), (12,13)\\}$.\n- Case $2$ (boundary: near-diagonal loop):\n  - Matrix size $n = 32$, bin size $\\Delta = 5000$ base pairs, baseline $\\lambda_0 = 50$, coverage $b_i = 1$ for all $i$, pseudorandom seed $2024$.\n  - Insert a loop at $(7,8)$ with increment $\\delta = 140$.\n  - Evaluate the candidate list $\\{(7,8), (7,9), (15,16)\\}$.\n- Case $3$ (bias confounding control):\n  - Matrix size $n = 30$, bin size $\\Delta = 10000$ base pairs, baseline $\\lambda_0 = 55$, coverage initialized to $b_i = 1$ and then set $b_3 = 4.0$, $b_{22} = 4.0$, $b_{12} = 1.5$, $b_{25} = 1.8$, pseudorandom seed $2025$.\n  - Insert a real loop at $(12,25)$ with increment $\\delta = 180$.\n  - Evaluate the candidate list $\\{(3,22), (12,25), (10,20)\\}$.\n- Case $4$ (edge case: no loops):\n  - Matrix size $n = 24$, bin size $\\Delta = 10000$ base pairs, baseline $\\lambda_0 = 50$, coverage initialized to $b_i = 1$ and then set $b_5 = 1.3$, $b_{15} = 0.7$, pseudorandom seed $2026$.\n  - No loop insertions.\n  - Evaluate the candidate list $\\{(5,15), (8,12)\\}$.\n\nProgram requirements:\n- Implement the expected-contact normalization and loop calling rule exactly as described above.\n- For each test case, compute a boolean list indicating whether each candidate pair in the specified order is called significant.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list of booleans for one case (for example, $\\left[\\left[ \\mathrm{True}, \\mathrm{False}\\right], \\left[\\mathrm{False}, \\mathrm{True}\\right]\\right]$). No physical units or angles are required in the output; all boolean values must be expressed as plain boolean literals.",
            "solution": "The user-provided problem has been analyzed and is deemed valid. It is scientifically grounded in the principles of chromatin conformation analysis, is well-posed with a clear algorithmic path to a unique solution, and is expressed in objective, formal language. The provided parameters and test cases are self-contained and consistent.\n\nThe solution involves a multi-step bioinformatic pipeline to identify statistically significant chromatin loops from synthetic Hi-C data. The process can be broken down into three main phases: (1) synthetic data generation, (2) construction of a normalized expected contact model, and (3) statistical significance testing to call loops.\n\n**1. Synthetic Hi-C Data Generation**\n\nThe foundation of the analysis is a synthetic contact matrix $C \\in \\mathbb{N}^{n \\times n}$, representing observed ligation counts between $n$ genomic bins. The generation of this matrix follows a defined statistical model based on a Poisson process, which is appropriate for modeling discrete count events like DNA ligations.\n\nThe mean of the Poisson distribution, $\\Lambda_{ij}$, for a pair of loci $(i, j)$ is constructed from three components:\n- **Baseline Contact Probability**: Chromatin is a polymer, and the probability of contact between two loci decreases as their linear genomic distance increases. This is modeled by a power-law decay function $\\lambda_{ij}^{\\mathrm{base}} = \\lambda_0 / (1 + |i-j|)$, where $|i-j| = s$ is the separation in bins and $\\lambda_0$ is a baseline contact frequency parameter.\n- **Locus-Specific Biases**: Hi-C data is subject to experimental biases, including differences in DNA fragmentability, GC content, and sequencing efficiency. These are modeled as a multiplicative factor for each bin, $b_i > 0$. The combined effect on a contact $(i,j)$ is $b_i b_j$.\n- **Chromatin Loops**: Biologically significant loops, such as enhancer-promoter interactions, create contacts that are much more frequent than expected from genomic distance alone. These are modeled by adding a large positive increment $\\delta_{\\ell}$ to the mean count for specific pairs $(p_{\\ell}, q_{\\ell})$.\n\nCombining these, the mean contact frequency $\\Lambda_{ij}$ for $i < j$ is:\n$$\n\\Lambda_{ij} = \\left( \\frac{\\lambda_0}{1 + |i-j|} \\right) \\cdot b_i b_j + \\sum_{\\ell} \\delta_{\\ell} \\mathbf{1}\\{(i,j) = (p_{\\ell}, q_{\\ell})\\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The observed contact count $C_{ij}$ is then drawn from a Poisson distribution, $C_{ij} \\sim \\mathrm{Poisson}(\\Lambda_{ij})$. The matrix is symmetric ($C_{ij} = C_{ji}$) and has a zero diagonal ($C_{ii} = 0$), as contacts are defined between distinct loci.\n\n**2. Expected Contact Model Construction**\n\nTo identify true loops, which are outliers, we must first accurately model the expected contact frequency based on the background of polymer folding and experimental biases. This is achieved through a robust normalization procedure.\n\n- **Bias Correction**: The first step is to correct for the known locus-specific biases. The raw counts $C_{ij}$ are divided by the product of the bias factors for the interacting loci, yielding a bias-corrected value $x_{ij} = C_{ij} / (b_i b_j)$. This places all contacts on a comparable scale, assuming the bias factors $b_i$ are known or have been accurately estimated.\n\n- **Distance-Stratified Expectation**: The primary determinant of expected contacts is genomic distance. Therefore, the bias-corrected contacts $x_{ij}$ are stratified into multisets $X_s$ based on their genomic separation $s = |i-j| \\in \\{1, 2, \\dots, n-1\\}$.\n\n- **Robust Aggregation**: Within each stratum $X_s$, we need to estimate the central tendency of contact frequency. A simple average would be sensitive to the very outliers (the loops) we wish to detect. Therefore, a robust estimator, the trimmed mean, is employed. For each multiset $X_s$, the lowest and highest $\\lfloor \\tau |X_s| \\rfloor$ values are removed, where $\\tau$ is the trimming fraction (given as $\\tau = 0.1$). The mean of the remaining values gives the robust estimate of the expected bias-corrected contact frequency at separation $s$, denoted $\\hat{\\mu}(s)$. If the number of data points, $|X_s|$, is too small to be trimmed (specified as $|X_s| < 3$), the standard mean is used instead to avoid instability.\n\n- **Final Expected Matrix**: The expected count for any pair $(i, j)$ is then reconstructed by taking the estimated mean for its separation class and reapplying the bias factors:\n$$\nE_{ij} = \\hat{\\mu}(|i-j|) \\cdot b_i b_j\n$$\nThis matrix $E$ represents the expected contact frequencies under the null hypothesis of no specific loop interaction.\n\n**3. Significance Testing and Loop Calling**\n\nThe final step is to compare the observed counts $C_{ij}$ to the expected counts $E_{ij}$ to determine which, if any, are statistically significant.\n\n- **Standardized Residuals**: Under the Poisson approximation for the count data, the variance of $C_{ij}$ is approximately equal to its mean, $E_{ij}$. A standardized residual (or Z-score) can be calculated to measure the deviation of the observed count from the expected value in units of standard deviation:\n$$\nz_{ij} = \\frac{C_{ij} - E_{ij}}{\\sqrt{E_{ij} + \\varepsilon}}\n$$\nA small constant $\\varepsilon = 10^{-9}$ is added to the denominator to ensure numerical stability if $E_{ij}$ is close to zero.\n\n- **Significance Conditions**: A candidate pair $(i, j)$ is called a \"significant loop\" if it satisfies two criteria simultaneously:\n  1. **Magnitude Threshold**: The Z-score must exceed a predefined significance threshold, $z_{ij} \\ge z^\\star$. Here, $z^\\star = 3$, corresponding to an event approximately $3$ standard deviations above the mean.\n  2. **Local Maximum**: The interaction must not just be strong, but locally dominant. The value $z_{ij}$ must be a strict local maximum within a square neighborhood of a given Chebyshev radius $R$. This means $z_{ij}$ must be strictly greater than $z_{pq}$ for all neighboring pairs $(p,q)$ where $|p-i| \\le R$ and $|q-j| \\le R$, excluding the center point $(i,j)$ itself. For this problem, $R=1$, defining a $3 \\times 3$ neighborhood. This condition prevents calling an entire \"plaid\" region of elevated scores caused by a single strong bias or a large, diffuse contact domain; it isolates punctate, peak-like features characteristic of loops.\n\nBy implementing this entire pipeline for each test case, we can systematically evaluate the provided candidate loops against the specified statistical criteria.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import trim_mean\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Global calling parameters\n    tau = 0.1\n    R = 1\n    z_star = 3.0\n    eps = 1e-9\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"n\": 28, \"delta_bp\": 10000, \"lambda0\": 60, \"b_init\": 1.0,\n            \"b_updates\": {}, \"seed\": 2023,\n            \"loops\": [((6, 21), 180)],\n            \"candidates\": [(6, 21), (10, 20), (12, 13)]\n        },\n        # Case 2 (boundary: near-diagonal loop)\n        {\n            \"n\": 32, \"delta_bp\": 5000, \"lambda0\": 50, \"b_init\": 1.0,\n            \"b_updates\": {}, \"seed\": 2024,\n            \"loops\": [((7, 8), 140)],\n            \"candidates\": [(7, 8), (7, 9), (15, 16)]\n        },\n        # Case 3 (bias confounding control)\n        {\n            \"n\": 30, \"delta_bp\": 10000, \"lambda0\": 55, \"b_init\": 1.0,\n            \"b_updates\": {3: 4.0, 22: 4.0, 12: 1.5, 25: 1.8}, \"seed\": 2025,\n            \"loops\": [((12, 25), 180)],\n            \"candidates\": [(3, 22), (12, 25), (10, 20)]\n        },\n        # Case 4 (edge case: no loops)\n        {\n            \"n\": 24, \"delta_bp\": 10000, \"lambda0\": 50, \"b_init\": 1.0,\n            \"b_updates\": {5: 1.3, 15: 0.7}, \"seed\": 2026,\n            \"loops\": [],\n            \"candidates\": [(5, 15), (8, 12)]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(\n            n=case[\"n\"],\n            lambda0=case[\"lambda0\"],\n            b_init=case[\"b_init\"],\n            b_updates=case[\"b_updates\"],\n            seed=case[\"seed\"],\n            loops=case[\"loops\"],\n            candidates=case[\"candidates\"],\n            tau=tau,\n            R=R,\n            z_star=z_star,\n            eps=eps\n        )\n        all_results.append(result)\n\n    # Format the final output as a string representation of a list of lists.\n    # e.g., [[True, False], [False, True]]\n    print(str(all_results).replace(\" \", \"\"))\n\n\ndef process_case(n, lambda0, b_init, b_updates, seed, loops, candidates, tau, R, z_star, eps):\n    \"\"\"\n    Processes a single test case from data generation to loop calling.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate synthetic data\n    # Set up bias vector\n    b = np.full(n, b_init, dtype=float)\n    for idx, val in b_updates.items():\n        b[idx] = val\n\n    # Calculate mean matrix Lambda\n    Lambda = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        for j in range(i + 1, n):\n            s = j - i\n            lambda_base = lambda0 / (1 + s)\n            Lambda[i, j] = lambda_base * b[i] * b[j]\n\n    for (p, q), delta_loop in loops:\n        Lambda[p, q] += delta_loop\n    \n    # Sample from Poisson to get contact matrix C\n    C = np.zeros((n, n), dtype=int)\n    for i in range(n):\n        for j in range(i + 1, n):\n            C[i, j] = rng.poisson(Lambda[i, j])\n            C[j, i] = C[i, j]\n\n    # 2. Construct expected model\n    # Bias correction\n    b_matrix = np.outer(b, b)\n    # Avoid division by zero, although b_i > 0 is stated.\n    b_matrix[b_matrix == 0] = 1.0\n    X_corrected = C / b_matrix\n\n    # Distance-stratified trimmed mean\n    mu_hat = np.zeros(n)\n    for s in range(1, n):\n        Xs = np.diagonal(X_corrected, offset=s)\n        if len(Xs) < 3:\n            mu_hat[s] = np.mean(Xs)\n        else:\n            mu_hat[s] = trim_mean(Xs, proportiontocut=tau)\n\n    # Construct expected matrix E\n    E = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        for j in range(i + 1, n):\n            s = j - i\n            E[i, j] = mu_hat[s] * b[i] * b[j]\n            E[j, i] = E[i, j]\n\n    # 3. Significance testing\n    # Calculate Z-score matrix\n    # Add a small epsilon to E in the denominator to avoid division by zero\n    # or sqrt of negative, E is non-negative by construction\n    # We only care about z_ij where i < j, but compute for the whole matrix for simplicity.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        Z = (C - E) / np.sqrt(E + eps)\n    Z = np.nan_to_num(Z, nan=0.0)\n    \n    # Call loops for specified candidates\n    case_results = []\n    for i, j in candidates:\n        # Condition 1: Z-score threshold\n        z_val = Z[i, j]\n        if z_val < z_star:\n            case_results.append(False)\n            continue\n        \n        # Condition 2: Strict local maximum\n        i_min, i_max = max(0, i - R), min(n, i + R + 1)\n        j_min, j_max = max(0, j - R), min(n, j + R + 1)\n        \n        neighborhood = Z[i_min:i_max, j_min:j_max]\n        \n        is_strict_local_max = (np.max(neighborhood) == z_val) and (np.sum(neighborhood == z_val) == 1)\n        \n        if is_strict_local_max:\n            case_results.append(True)\n        else:\n            case_results.append(False)\n            \n    return case_results\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}