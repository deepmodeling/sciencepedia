{
    "hands_on_practices": [
        {
            "introduction": "掌握基于似然的推断，首先要从其最核心的应用开始。此练习将引导您完成对一个基本但极其重要的模型——伯努利分布的极大似然估计（MLE）的推导。通过这个实践，您将亲手应用极大似然法的基本流程，并计算估计量的渐近方差，为理解更复杂的统计模型打下坚实的理论基础。",
            "id": "4578114",
            "problem": "一项使用下一代测序 (NGS) 技术的肿瘤测序研究旨在估计某个特定基因中具有临床可操作性变异的队列水平流行率。在经过严格的质量控制过滤以去除低置信度的读数后，每个肿瘤的变异检测结果被建模为一个独立的 Bernoulli 随机变量，其共同参数为 $p \\in (0,1)$，其中 $p$ 表示从研究人群中随机抽样的肿瘤中该变异真实存在的概率。设 $X_{1}, X_{2}, \\dots, X_{n}$ 为 $n$ 个独立的检测指标，其中 $X_{i} \\in \\{0,1\\}$，并定义充分统计量 $S = \\sum_{i=1}^{n} X_{i}$。假设 $0  S  n$ 以避免边界解。\n\n使用独立同分布 Bernoulli 观测值的似然函数和标量参数的 Fisher 信息的基本定义，推导 $p$ 的最大似然估计量以及该估计量基于 Fisher 信息的渐近方差。将最大似然估计量表示为 $S$ 和 $n$ 的封闭形式解析表达式，并将其渐近方差表示为 $p$ 和 $n$ 的表达式。无需数值舍入，也不涉及物理单位。将这两个量作为单一最终答案一起提供。",
            "solution": "在尝试任何解答之前，需对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n-   每个肿瘤的检测结果 $X_i$ 是一个独立的 Bernoulli 随机变量。\n-   Bernoulli 分布的共同参数为 $p \\in (0,1)$。\n-   观测值为 $X_{1}, X_{2}, \\dots, X_{n}$，它们是 $n$ 个独立同分布 (i.i.d.) 的指标，其中 $X_{i} \\in \\{0,1\\}$。\n-   充分统计量为 $S = \\sum_{i=1}^{n} X_{i}$。\n-   明确做出假设：$0  S  n$。\n-   任务是推导 $p$ 的最大似然估计量 (MLE)，并用 $S$ 和 $n$ 表示。\n-   任务还要求基于 Fisher 信息推导该估计量的渐近方差，并用 $p$ 和 $n$ 表示。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题定义明确且具有科学依据。\n-   **科学依据：** 使用 Bernoulli 分布对二元结果（变异存在/不存在）进行建模是统计学和生物信息学中的一种基本且标准的做法。最大似然估计和 Fisher 信息的使用是统计推断的核心原则。\n-   **定义明确：** 该问题提供了一个清晰的统计模型（独立同分布的 Bernoulli 试验）和一个精确定义的目标（推导MLE及其渐近方差）。假设 $0  S  n$ 是一个简化条件，以确保估计量不会落在参数空间的边界上，这是许多理论处理中的标准程序。预期会有一个唯一且有意义的解。\n-   **客观性：** 问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n\n### 步骤3：结论与行动\n该问题有效。将提供一个完整的、有理有据的解答。\n\n### 解题推导\n问题要求推导 Bernoulli 分布参数 $p$ 的最大似然估计量 (MLE) 以及该估计量的渐近方差。\n\n#### 第一部分：$p$ 的最大似然估计量\n\n设 $X_1, X_2, \\dots, X_n$ 为一组来自参数为 $p$ 的 Bernoulli 分布的 $n$ 个独立同分布的随机变量。对于单个观测值 $X_i=x_i$（其中 $x_i \\in \\{0,1\\}$），其概率质量函数 (PMF) 为：\n$$\nP(X_i=x_i | p) = p^{x_i}(1-p)^{1-x_i}\n$$\n由于观测值是独立的，观测到整个数据集 $x = (x_1, x_2, \\dots, x_n)$ 的联合概率是各个概率的乘积。这个联合概率，在给定数据集的情况下，被看作是参数 $p$ 的函数，即为似然函数 $L(p|\\boldsymbol{x})$。\n$$\nL(p | x_1, \\dots, x_n) = \\prod_{i=1}^{n} P(X_i=x_i | p) = \\prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i}\n$$\n该表达式可以使用充分统计量 $S = \\sum_{i=1}^{n} x_i$ 来简化，它代表成功的总次数（即检测到的变异数）。\n$$\nL(p) = p^S (1-p)^{n-S}\n$$\n为了找到使 $L(p)$ 最大化的 $p$ 值，分析上更方便的是最大化似然函数的自然对数，即对数似然函数 $\\ell(p)$，因为对数是一个单调递增函数。\n$$\n\\ell(p) = \\ln(L(p)) = \\ln(p^S (1-p)^{n-S}) = S \\ln(p) + (n-S) \\ln(1-p)\n$$\n为求最大值，我们计算 $\\ell(p)$ 对 $p$ 的一阶导数，并令其为零。这个导数被称为得分函数。\n$$\n\\frac{d\\ell}{dp} = \\frac{d}{dp} \\left( S \\ln(p) + (n-S) \\ln(1-p) \\right) = \\frac{S}{p} - \\frac{n-S}{1-p}\n$$\n将得分函数设为零以找到临界点，我们求解 $p$。设 $\\hat{p}$ 表示估计量。\n$$\n\\frac{S}{\\hat{p}} - \\frac{n-S}{1-\\hat{p}} = 0 \\implies \\frac{S}{\\hat{p}} = \\frac{n-S}{1-\\hat{p}}\n$$\n$$\nS(1-\\hat{p}) = (n-S)\\hat{p}\n$$\n$$\nS - S\\hat{p} = n\\hat{p} - S\\hat{p}\n$$\n$$\nS = n\\hat{p}\n$$\n求解 $\\hat{p}$ 得到最大似然估计量：\n$$\n\\hat{p} = \\frac{S}{n}\n$$\n条件 $0  S  n$ 确保了 $\\hat{p}$ 位于开区间 $(0,1)$ 内，与参数空间 $p \\in (0,1)$ 一致，并避免了得分函数中出现除以零的情况。为了确认这是一个最大值，我们检查对数似然函数的二阶导数：\n$$\n\\frac{d^2\\ell}{dp^2} = \\frac{d}{dp} \\left( \\frac{S}{p} - \\frac{n-S}{1-p} \\right) = -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2}\n$$\n由于 $S>0$，$n-S>0$，$p^2 > 0$ 且 $(1-p)^2 > 0$，对于所有 $p \\in (0,1)$，二阶导数 $\\frac{d^2\\ell}{dp^2}$ 严格为负。这证实了对数似然函数是凹函数，临界点 $\\hat{p} = S/n$ 确实是一个唯一的最大值点。\n\n#### 第二部分：MLE的渐近方差\n\nMLE的渐近方差由 Fisher 信息 $I(p)$ 的倒数给出。对于单个标量参数，Fisher 信息的定义为：\n$$\nI(p) = -E\\left[ \\frac{d^2\\ell}{dp^2} \\right]\n$$\n其中期望是关于数据的分布来计算的。我们已经得到二阶导数：\n$$\n\\frac{d^2\\ell}{dp^2} = -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2}\n$$\n现在，我们求期望。此表达式中的随机变量是 $S = \\sum_{i=1}^n X_i$。和的期望等于期望的和。对于单个 Bernoulli 试验 $X_i$，$E[X_i] = 1 \\cdot p + 0 \\cdot (1-p) = p$。因此，$S$ 的期望是：\n$$\nE[S] = E\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n E[X_i] = \\sum_{i=1}^n p = np\n$$\n将 $E[S]$ 代入二阶导数期望的表达式中：\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = E\\left[ -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2} \\right] = -\\frac{E[S]}{p^2} - \\frac{n-E[S]}{(1-p)^2}\n$$\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = -\\frac{np}{p^2} - \\frac{n-np}{(1-p)^2} = -\\frac{n}{p} - \\frac{n(1-p)}{(1-p)^2}\n$$\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = -\\frac{n}{p} - \\frac{n}{1-p} = -n \\left( \\frac{1}{p} + \\frac{1}{1-p} \\right) = -n \\left( \\frac{1-p+p}{p(1-p)} \\right) = -\\frac{n}{p(1-p)}\n$$\nFisher 信息是这个量的负值：\n$$\nI(p) = - \\left( -\\frac{n}{p(1-p)} \\right) = \\frac{n}{p(1-p)}\n$$\n根据 Cramer-Rao Lower Bound 和关于MLE的大样本理论，$\\hat{p}$ 的渐近方差是 Fisher 信息的倒数：\n$$\n\\text{Asymptotic Variance}(\\hat{p}) = [I(p)]^{-1} = \\left( \\frac{n}{p(1-p)} \\right)^{-1} = \\frac{p(1-p)}{n}\n$$\n这个方差是独立同分布 Bernoulli 随机变量样本均值的方差，这是一个众所周知的结果，并且MLE的渐近方差收敛于这个值。\n\n最终答案需要给出MLE $\\hat{p}$ 及其渐近方差的表达式。\n-   最大似然估计量：$\\hat{p} = \\frac{S}{n}$\n-   渐近方差：$\\frac{p(1-p)}{n}$\n\n这两个量将以单行矩阵的形式呈现。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{S}{n}  \\frac{p(1-p)}{n} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在生物信息学中，我们处理的数据往往比简单的伯努利试验更复杂，例如基因表达的读数计数。此练习将模型扩展到带有偏置项（exposure）的泊松分布，这在处理如RNA测序等高通量数据时至关重要。更重要的是，本练习将引导您推导充分统计量，揭示似然原理如何为大规模数据集的无损压缩提供理论依据，这是高效处理海量生物数据的关键概念。",
            "id": "4578097",
            "problem": "在一项核糖核酸测序 (RNA-seq) 实验中，来自 $n$ 个单细胞的单个基因的信使核糖核酸 (mRNA) 分子计数被建模为独立的泊松随机变量，它们共享一个率参数 $\\lambda$，并具有已知的细胞特异性暴露度 $t_{i} \\in (0,\\infty)$，该暴露度用于解释文库大小的归一化。形式上，以 $\\lambda$ 为条件，计数 $X_{1},\\dots,X_{n}$ 满足 $X_{i} \\sim \\mathrm{Pois}(\\lambda t_{i})$，对于 $i=1,\\dots,n$ 独立。泊松分布的概率质量函数由 $p(x;\\mu)=\\frac{\\mu^{x}\\exp(-\\mu)}{x!}$ 给出，其中 $x \\in \\{0,1,2,\\dots\\}$ 且均值 $\\mu>0$。\n\n仅从这些定义和 Fisher–Neyman 分解定理出发，推导出一个关于 $\\lambda$ 的充分且最小充分的统计量，并将似然函数表达为一种能明确展示数据如何能在不损失任何关于 $\\lambda$ 的似然推断信息的情况下被压缩的形式。从第一性原理出发，仔细证明推导的每一步，并阐明你的结果在高通量生物信息学背景下对基于似然的计数数据压缩的意义。\n\n然后，推导 $\\lambda$ 的最大似然估计 (MLE)，并用观测计数 $x_{1},\\dots,x_{n}$ 和已知暴露度 $t_{1},\\dots,t_{n}$ 表示其闭式解。\n\n请以 $x_{i}$ 和 $t_{i}$ 表示的 $\\lambda$ 的 MLE 的单一解析表达式作为你的最终答案。不需要数值四舍五入，也不需要单位。",
            "solution": "问题陈述的严格验证如下。\n\n**第1步：提取已知条件**\n- 数据：$n$ 个单细胞 mRNA 分子计数，$X_{1}, \\dots, X_{n}$。\n- 模型：计数被建模为独立的随机变量。\n- 分布：对于每个 $i \\in \\{1, \\dots, n\\}$，以 $\\lambda$ 为条件，$X_{i} \\sim \\mathrm{Pois}(\\lambda t_{i})$。\n- 参数：$\\lambda$ 是一个共享的率参数，$\\lambda \\in (0, \\infty)$。\n- 已知常数：$t_{1}, \\dots, t_{n}$ 是已知的细胞特异性正暴露度，$t_{i} \\in (0, \\infty)$。\n- 概率质量函数 (PMF)：泊松分布的 PMF 为 $p(x;\\mu) = \\frac{\\mu^{x}\\exp(-\\mu)}{x!}$，其中 $x \\in \\{0, 1, 2, \\dots\\}$ 且均值 $\\mu > 0$。\n- 要求任务：\n    1.  使用 Fisher–Neyman 分解定理推导一个关于 $\\lambda$ 的充分且最小充分的统计量。\n    2.  表达似然函数以显示数据如何能被压缩。\n    3.  阐明在生物信息学中数据压缩的意义。\n    4.  推导 $\\lambda$ 的最大似然估计 (MLE)。\n\n**第2步：使用提取的已知条件进行验证**\n- **科学依据充分：** 该问题牢固地建立在成熟的统计理论及其在生物信息学中的应用之上。泊松模型是计数数据（包括 RNA-seq 数据）的标准且被广泛接受的模型，使用暴露度/偏移量项 ($t_i$) 是一种常见的归一化技术。\n- **定义明确：** 该问题提供了推导所要求的统计量所需的所有必要信息和定义。目标清晰，并能通过标准统计方法得出一个唯一的、明确定义的解。\n- **客观：** 问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n\n**第3步：结论与行动**\n该问题是有效的。这是一个在数理统计及其在生物信息学应用中定义良好的问题。将开始求解。\n\n我们首先构建观测数据的似然函数。观测值 $X_{1}, \\dots, X_{n}$ 是独立的。对于一个观测到的计数向量 $\\mathbf{x} = (x_1, \\dots, x_n)$，其似然函数 $L(\\lambda; \\mathbf{x})$ 是联合概率质量函数，由各个 PMF 的乘积给出：\n$$\nL(\\lambda; \\mathbf{x}) = \\prod_{i=1}^{n} p(x_i; \\lambda t_i)\n$$\n代入泊松 PMF $p(x;\\mu) = \\frac{\\mu^{x}\\exp(-\\mu)}{x!}$，其中均值 $\\mu_i = \\lambda t_i$：\n$$\nL(\\lambda; \\mathbf{x}) = \\prod_{i=1}^{n} \\frac{(\\lambda t_i)^{x_i} \\exp(-\\lambda t_i)}{x_i!}\n$$\n我们可以通过分离涉及 $\\lambda$ 的项和不涉及 $\\lambda$ 的项来重新整理这个表达式：\n$$\nL(\\lambda; \\mathbf{x}) = \\left( \\prod_{i=1}^{n} \\lambda^{x_i} \\right) \\left( \\prod_{i=1}^{n} t_i^{x_i} \\right) \\left( \\prod_{i=1}^{n} \\exp(-\\lambda t_i) \\right) \\left( \\prod_{i=1}^{n} \\frac{1}{x_i!} \\right)\n$$\n利用乘积和指数的性质，我们得到：\n$$\nL(\\lambda; \\mathbf{x}) = \\lambda^{\\sum_{i=1}^{n} x_i} \\cdot \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\cdot \\left( \\frac{\\prod_{i=1}^{n} t_i^{x_i}}{\\prod_{i=1}^{n} x_i!} \\right)\n$$\nFisher–Neyman 分解定理指出，一个统计量 $T(\\mathbf{X})$ 是参数 $\\theta$ 的充分统计量，当且仅当似然函数可以分解为两个非负函数的乘积：\n$$\nL(\\theta; \\mathbf{x}) = g(T(\\mathbf{x}), \\theta) \\cdot h(\\mathbf{x})\n$$\n其中 $g$ 仅通过统计量 $T(\\mathbf{x})$ 依赖于数据 $\\mathbf{x}$，而 $h$ 不依赖于参数 $\\theta$。\n\n在我们的例子中，我们定义统计量 $T(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$。那么我们的似然函数可以写成：\n$$\nL(\\lambda; \\mathbf{x}) = \\underbrace{\\left[ \\lambda^{\\sum_{i=1}^{n} x_i} \\cdot \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\right]}_{g(T(\\mathbf{x}), \\lambda)} \\cdot \\underbrace{\\left( \\frac{\\prod_{i=1}^{n} t_i^{x_i}}{\\prod_{i=1}^{n} x_i!} \\right)}_{h(\\mathbf{x})}\n$$\n在这里，函数 $g$ 仅通过总和 $T(\\mathbf{x}) = \\sum_{i=1}^{n} x_i$ 依赖于数据 $\\mathbf{x}$。已知常数 $\\sum_{i=1}^{n} t_i$ 是模型结构的一部分，而不是数据实现的一部分。函数 $h(\\mathbf{x})$ 依赖于 $\\mathbf{x}$ 和已知暴露度 $t_i$，但不依赖于参数 $\\lambda$。因此，根据 Fisher–Neyman 分解定理，统计量 $T(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$ 是 $\\lambda$ 的一个充分统计量。\n\n为了证明 $T(\\mathbf{X})$ 是最小充分的，我们可以考察两个不同数据向量 $\\mathbf{x}$ 和 $\\mathbf{y}$ 的似然比。一个统计量 $T$ 是最小充分的，如果对于任意两个数据点 $\\mathbf{x}$ 和 $\\mathbf{y}$，比率 $L(\\lambda; \\mathbf{x})/L(\\lambda; \\mathbf{y})$ 作为 $\\lambda$ 的函数是常数，当且仅当 $T(\\mathbf{x}) = T(\\mathbf{y})$。\n让我们构建这个比率：\n$$\n\\frac{L(\\lambda; \\mathbf{x})}{L(\\lambda; \\mathbf{y})} = \\frac{\\lambda^{\\sum x_i} \\exp(-\\lambda \\sum t_i) \\left( \\frac{\\prod t_i^{x_i}}{\\prod x_i!} \\right)}{\\lambda^{\\sum y_i} \\exp(-\\lambda \\sum t_i) \\left( \\frac{\\prod t_i^{y_i}}{\\prod y_i!} \\right)}\n$$\n$$\n\\frac{L(\\lambda; \\mathbf{x})}{L(\\lambda; \\mathbf{y})} = \\lambda^{(\\sum x_i - \\sum y_i)} \\cdot \\frac{\\left( \\frac{\\prod t_i^{x_i}}{\\prod x_i!} \\right)}{\\left( \\frac{\\prod t_i^{y_i}}{\\prod y_i!} \\right)}\n$$\n要使这个比率对于所有 $\\lambda > 0$ 都与 $\\lambda$ 无关，项 $\\lambda^{(\\sum x_i - \\sum y_i)}$ 必须是常数。这要求指数为零，即 $\\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} y_i = 0$，这意味着 $\\sum_{i=1}^{n} x_i = \\sum_{i=1}^{n} y_i$。这正是条件 $T(\\mathbf{x}) = T(\\mathbf{y})$。因此，$T(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$ 是 $\\lambda$ 的一个最小充分统计量。\n\n显式分解 $L(\\lambda; \\mathbf{x}) = g(T(\\mathbf{x}), \\lambda) \\cdot h(\\mathbf{x})$ 展示了数据如何在不损失任何关于似然推断信息的情况下被压缩。所有关于 $\\lambda$ 的信息都包含在函数 $g$ 中，而 $g$ 仅通过最小充分统计量 $T(\\mathbf{x}) = \\sum_{i=1}^{n} x_i$ 的值来依赖于数据。项 $h(\\mathbf{x})$ 不包含关于 $\\lambda$ 的信息，因此在对 $\\lambda$ 进行推断时可以忽略。\n\n这对高通量生物信息学的意义是深远的。一项 RNA-seq 实验可能会为单个基因生成跨越数千或数百万个细胞的计数（即 $n$ 非常大）。单个基因的原始数据集是一个维度为 $n$ 的计数向量 $(x_1, x_2, \\dots, x_n)$。最小充分统计量 $T(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$ 的存在意味着，为了估计潜在的表达率 $\\lambda$ 或进行任何其他基于似然的推断，这整个包含 $n$ 个数字的向量可以被一个单一的数字——它的总和——所取代。数据集可以从 $n$ 个值压缩为 1 个值 $T(\\mathbf{x})$，以及预先确定的暴露度总和 $\\sum t_i$，而完全不损失任何关于 $\\lambda$ 的信息。这会极大地提高数据存储、传输和计算的效率。\n\n接下来，我们推导 $\\lambda$ 的最大似然估计 (MLE)。MLE，记作 $\\hat{\\lambda}$，是使似然函数 $L(\\lambda; \\mathbf{x})$ 最大化的 $\\lambda$ 的值。最大化似然函数的自然对数，即对数似然函数 $\\ell(\\lambda; \\mathbf{x}) = \\ln(L(\\lambda; \\mathbf{x}))$，通常更容易。\n$$\n\\ell(\\lambda; \\mathbf{x}) = \\ln\\left[ \\lambda^{\\sum x_i} \\exp(-\\lambda \\sum t_i) \\left( \\frac{\\prod t_i^{x_i}}{\\prod x_i!} \\right) \\right]\n$$\n$$\n\\ell(\\lambda; \\mathbf{x}) = \\left(\\sum_{i=1}^{n} x_i\\right) \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} t_i + \\sum_{i=1}^{n} x_i \\ln(t_i) - \\sum_{i=1}^{n} \\ln(x_i!)\n$$\n为了找到最大值，我们计算 $\\ell(\\lambda; \\mathbf{x})$ 关于 $\\lambda$ 的一阶导数，并将其设为零。\n$$\n\\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda} \\left[ \\left(\\sum_{i=1}^{n} x_i\\right) \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} t_i + \\text{const.} \\right]\n$$\n$$\n\\frac{d\\ell}{d\\lambda} = \\frac{\\sum_{i=1}^{n} x_i}{\\lambda} - \\sum_{i=1}^{n} t_i\n$$\n将导数设为零，得到 MLE $\\hat{\\lambda}$ 的方程：\n$$\n\\frac{\\sum_{i=1}^{n} x_i}{\\hat{\\lambda}} - \\sum_{i=1}^{n} t_i = 0\n$$\n解出 $\\hat{\\lambda}$：\n$$\n\\frac{\\sum_{i=1}^{n} x_i}{\\hat{\\lambda}} = \\sum_{i=1}^{n} t_i\n$$\n$$\n\\hat{\\lambda} = \\frac{\\sum_{i=1}^{n} x_i}{\\sum_{i=1}^{n} t_i}\n$$\n为了确认这是一个最大值，我们考察二阶导数：\n$$\n\\frac{d^2\\ell}{d\\lambda^2} = \\frac{d}{d\\lambda} \\left[ \\left(\\sum_{i=1}^{n} x_i\\right) \\lambda^{-1} - \\sum_{i=1}^{n} t_i \\right] = - \\left(\\sum_{i=1}^{n} x_i\\right) \\lambda^{-2} = -\\frac{\\sum_{i=1}^{n} x_i}{\\lambda^2}\n$$\n因为计数 $x_i \\ge 0$，它们的和 $\\sum_{i=1}^{n} x_i \\ge 0$。暴露度 $t_i > 0$，所以它们的和是正的。如果所有的 $x_i = 0$，那么 $\\hat{\\lambda}=0$，这位于参数空间 $(0, \\infty)$ 的边界上。如果至少有一个 $x_i > 0$，那么 $\\sum x_i > 0$，并且由于 $\\lambda \\in (0, \\infty)$，$\\lambda^2 > 0$。在这种情况下，$\\frac{d^2\\ell}{d\\lambda^2}  0$，这表明对数似然函数是严格凹的，因此该临界点是一个唯一的全局最大值。推导出的表达式就是 MLE。\n\nMLE $\\hat{\\lambda}$ 的最终表达式是观测到的总计数除以总暴露度。这是一个直观上很有吸引力的结果：估计的率是每单位总“时间”或暴露度的总观测事件数。",
            "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n} x_{i}}{\\sum_{i=1}^{n} t_{i}}}$$"
        },
        {
            "introduction": "当模型中包含无法直接观测的潜变量时，直接最大化似然函数往往变得非常困难。本练习介绍了一种强大而通用的迭代算法——期望最大化（EM）算法，来解决这一挑战。通过在一个真实的生物信息学场景（基于测序数据进行基因分型）中从头开始实施EM算法，您将学会如何通过巧妙地处理“完整数据”的期望来解决潜变量模型的极大似然估计问题，这是现代计算统计学和遗传学中的一项核心技能。",
            "id": "4578121",
            "problem": "给定一个通过下一代测序（Next-Generation Sequencing, NGS）技术测序的队列中的一个双等位基因座。每个个体 $i \\in \\{1,\\dots,N\\}$ 都有一个潜在的基因型 $g_i \\in \\{0,1,2\\}$，表示参考等位基因的数量。您观察到的是不完全数据：支持参考等位基因的读数（read）数量 $r_i$ 和支持备择等位基因的读数数量 $a_i$，其中 $n_i = r_i + a_i$。假设在给定 $g_i$ 和已知的单位点对称测序错误率 $\\varepsilon \\in (0,1/2)$ 的条件下，各读数是独立的。在此模型下，任何单个读数报告为参考碱基的概率为：如果 $g_i=0$，概率是 $\\varepsilon$；如果 $g_i=1$，概率是 $1/2$；如果 $g_i=2$，概率是 $1-\\varepsilon$。参考等位基因的群体等位基因频率为 $p \\in (0,1)$，且基因型遵循哈代-温伯格平衡（Hardy–Weinberg Equilibrium, HWE），其先验概率为 $\\Pr(g_i=0)=(1-p)^2$、$\\Pr(g_i=1)=2p(1-p)$ 和 $\\Pr(g_i=2)=p^2$。假设个体之间相互独立。\n\n您的任务是使用基于似然的推断，通过期望最大化（Expectation–Maximization, EM）算法，将基因型视为潜在变量，来对 $p$ 进行最大似然估计（Maximum Likelihood Estimation, MLE）。请从似然和条件独立性的核心定义出发；不要假设任何快捷公式。请通过明确写出完整数据似然，计算其在给定当前参数下关于潜在变量后验分布的期望，并对 $p$ 进行最大化，从第一性原理推导出 EM 算法所需的完整数据充分统计量的期望以及 $p$ 的更新公式。\n\n实现要求：\n- 输入是固定的并嵌入在您的程序中。对于每个测试用例，使用提供的对称错误率 $\\varepsilon$、观测到的读数计数列表 $\\{(r_i,a_i)\\}_{i=1}^N$（其中 $r_i,a_i \\in \\mathbb{N}_0$）以及一个初始值 $p^{(0)} \\in (0,1)$。\n- 使用一个 EM 算法，该算法迭代 E-步和 M-步，直到绝对差 $|p^{(t+1)} - p^{(t)}|$ 小于容差 $\\tau = 10^{-9}$ 或迭代次数达到最大值 $T_{\\max} = 1000$（以先到者为准）。\n- 为保证数值稳定性，请在对数域中实现 E-步，使用稳定的归一化方法（例如，log-sum-exp）。对于零深度样本（其中 $n_i = 0$），处理方式需与概率模型保持一致。\n- 每个测试用例的最终估计值 $\\hat{p}$ 必须以小数形式打印，并四舍五入到小数点后六位。不要打印百分号。\n\n测试套件：\n- 案例1（一般混合）：$\\varepsilon = 0.01$，读数 $\\{(28,2),(2,28),(15,15),(18,2),(12,13),(0,10)\\}$，$p^{(0)} = 0.5$。\n- 案例2（趋于固定的边界情况）：$\\varepsilon = 0.001$，读数 $\\{(40,0),(30,0),(25,0),(10,0),(50,0)\\}$，$p^{(0)} = 0.4$。\n- 案例3（整个队列呈杂合子状）：$\\varepsilon = 0.02$，读数 $\\{(4,4),(5,5),(6,6),(3,3),(7,7),(10,10),(2,2),(15,15)\\}$，$p^{(0)} = 0.1$。\n- 案例4（包含零深度个体）：$\\varepsilon = 0.01$，读数 $\\{(0,0),(12,12),(0,30),(0,0),(25,25),(49,1)\\}$，$p^{(0)} = 0.6$。\n- 案例5（高测序错误率）：$\\varepsilon = 0.2$，读数 $\\{(18,12),(2,18),(10,10),(16,4)\\}$，$p^{(0)} = 0.3$。\n\n您的程序应生成单行输出，其中包含五个案例的估计等位基因频率，形式为用方括号括起来的逗号分隔列表，每个值都四舍五入到小数点后六位（例如，$[0.123456,0.654321,0.500000,0.750000,0.333333]$）。不应打印任何其他文本。",
            "solution": "该问题要求推导并实现一个期望最大化（Expectation-Maximization, EM）算法，以找到群体中参考等位基因频率 $p$ 的最大似然估计（Maximum Likelihood Estimate, MLE）。该估计基于下一代测序（Next-Generation Sequencing, NGS）的读数计数，其中个体基因型是未观测到的（潜在的）。\n\n### 1. 模型设定\n\n我们来正式定义概率模型。\n\n-   **观测数据**：对于 $N$ 个个体中的每一个，我们观测到读数计数 $D_i = (r_i, a_i)$，其中 $r_i$ 是支持参考等位基因的读数数量，$a_i$ 是支持备择等位基因的读数数量。个体 $i$ 的总读数数量为 $n_i = r_i + a_i$。完整的观测数据集为 $D = \\{D_i\\}_{i=1}^N$。\n-   **潜在变量**：每个个体的基因型 $g_i$ 是一个潜在变量。由于该基因座是双等位基因的，$g_i \\in \\{0, 1, 2\\}$ 表示个体 $i$ 拥有的参考等位基因的数量。所有基因型的集合为 $G = \\{g_i\\}_{i=1}^N$。\n-   **参数**：待估计的参数是群体中的参考等位基因频率 $p \\in (0,1)$。测序错误率 $\\varepsilon \\in (0, 1/2)$ 是一个已知的常数。\n-   **测序错误模型**：在给定基因型 $g_i$ 的条件下，总共 $n_i$ 个读数中参考读数的数量 $r_i$ 假定遵循二项分布：$r_i | g_i, n_i \\sim \\text{Binomial}(n_i, \\theta_{g_i})$。单个读数为参考等位基因的概率 $\\theta_{g_i}$ 取决于真实基因型和对称错误率 $\\varepsilon$：\n    -   如果 $g_i=0$（纯合备择型），$\\theta_0 = \\varepsilon$。\n    -   如果 $g_i=1$（杂合型），$\\theta_1 = 1/2$。\n    -   如果 $g_i=2$（纯合参考型），$\\theta_2 = 1-\\varepsilon$。\n    因此，在给定基因型 $g_i$ 的情况下观测到读数 $D_i$ 的似然为 $\\Pr(D_i|g_i, \\varepsilon) = \\binom{n_i}{r_i}\\theta_{g_i}^{r_i}(1-\\theta_{g_i})^{a_i}$。\n-   **群体遗传学模型**：假定基因型处于哈代-温伯格平衡（Hardy–Weinberg Equilibrium, HWE）状态。每个基因型的先验概率取决于等位基因频率 $p$：\n    -   $\\Pr(g_i=0|p) = (1-p)^2$\n    -   $\\Pr(g_i=1|p) = 2p(1-p)$\n    -   $\\Pr(g_i=2|p) = p^2$\n\n### 2. EM 算法框架\n\n我们的目标是通过最大化观测数据的对数似然 $\\ell(p; D) = \\log \\Pr(D | p, \\varepsilon)$ 来找到 $p$ 的 MLE。这个边际似然是通过对所有可能的潜在基因型配置求和得到的：\n$$ \\ell(p; D) = \\log \\left( \\sum_{G} \\Pr(D, G | p, \\varepsilon) \\right) = \\sum_{i=1}^N \\log \\left( \\sum_{j=0}^2 \\Pr(D_i | g_i=j, \\varepsilon) \\Pr(g_i=j | p) \\right) $$\n由于对数内部存在求和，直接最大化该表达式很复杂。EM 算法提供了一种迭代方法来解决这个问题。它作用于**完整数据对数似然** $\\ell_c(p; D, G) = \\log \\Pr(D, G | p, \\varepsilon)$，该似然函数更易于最大化。\n\n假设个体之间相互独立，完整数据对数似然为：\n$$ \\ell_c(p; D, G) = \\sum_{i=1}^N \\log \\Pr(D_i, g_i | p, \\varepsilon) = \\sum_{i=1}^N \\left( \\log \\Pr(D_i|g_i, \\varepsilon) + \\log \\Pr(g_i|p) \\right) $$\n我们引入指示变量 $z_{ij} = \\mathbb{I}(g_i=j)$，其中 $j \\in \\{0, 1, 2\\}$ 且 $\\sum_{j=0}^2 z_{ij} = 1$。与估计 $p$ 相关的项是 $\\log \\Pr(g_i|p)$：\n$$ \\log \\Pr(g_i|p) = z_{i0} \\log((1-p)^2) + z_{i1} \\log(2p(1-p)) + z_{i2} \\log(p^2) $$\n$$ = (2z_{i0} + z_{i1})\\log(1-p) + (z_{i1} + 2z_{i2})\\log p + z_{i1}\\log 2 $$\n完整数据对数似然可以表示为：\n$$ \\ell_c(p; D, G) = \\sum_{i=1}^N \\left[ (z_{i1} + 2z_{i2})\\log p + (2z_{i0} + z_{i1})\\log(1-p) \\right] + C $$\n其中 $C$ 包含不依赖于 $p$ 的项。项 $(z_{i1} + 2z_{i2})$ 和 $(2z_{i0} + z_{i1})$ 分别表示个体 $i$ 的参考等位基因和备择等位基因的数量。\n\nEM 算法包含两个步骤，迭代进行直至收敛：\n1.  **E-步**：计算完整数据对数似然关于在给定观测数据 $D$ 和当前参数估计 $p^{(t)}$ 条件下潜在变量 $G$ 的后验分布的期望。该函数记为 $Q(p|p^{(t)})$。\n2.  **M-步**：对 $p$ 最大化 $Q(p|p^{(t)})$ 以获得更新后的参数估计 $p^{(t+1)}$。\n\n### 3. E-步：求期望\n\nE-步涉及计算 $Q(p|p^{(t)}) = E_{G|D, p^{(t)}}[\\ell_c(p; D, G)]$。根据期望的线性性质，我们只需要计算指示变量 $z_{ij}$ 的期望：\n$$ E[z_{ij} | D, p^{(t)}] = \\Pr(g_i=j | D_i, p^{(t)}) $$\n我们将这个后验概率（或称“责任”，responsibility）记为 $\\omega_{ij}^{(t)}$。根据贝叶斯定理：\n$$ \\omega_{ij}^{(t)} = \\frac{\\Pr(D_i|g_i=j, \\varepsilon) \\Pr(g_i=j|p^{(t)})}{\\sum_{k=0}^2 \\Pr(D_i|g_i=k, \\varepsilon) \\Pr(g_i=k|p^{(t)})} $$\n为保证计算稳定性，我们在对数域中执行此计算。令：\n-   对数基因型似然：$\\log L'_{ij} = \\log \\Pr(D_i|g_i=j, \\varepsilon)_{\\text{unnormalized}} = r_i \\log \\theta_j + a_i \\log(1-\\theta_j)$。\n    -   $\\log L'_{i0} = r_i \\log \\varepsilon + a_i \\log(1-\\varepsilon)$\n    -   $\\log L'_{i1} = (r_i+a_i) \\log(0.5)$\n    -   $\\log L'_{i2} = r_i \\log(1-\\varepsilon) + a_i \\log \\varepsilon$\n-   对数基因型先验：$\\log\\pi_j^{(t)} = \\log \\Pr(g_i=j|p^{(t)})$。\n    -   $\\log\\pi_0^{(t)} = 2\\log(1-p^{(t)})$\n    -   $\\log\\pi_1^{(t)} = \\log 2 + \\log p^{(t)} + \\log(1-p^{(t)})$\n    -   $\\log\\pi_2^{(t)} = 2\\log p^{(t)}$\n\n联合概率的对数为 $J_{ij}^{(t)} = \\log L'_{ij} + \\log\\pi_j^{(t)}$。如果直接计算，$\\omega_{ij}^{(t)}$ 表达式中的分母可能会数值不稳定。我们使用 log-sum-exp 技巧：\n-   令 $M_i = \\max_k J_{ik}^{(t)}$。\n-   则 $\\omega_{ij}^{(t)} = \\frac{\\exp(J_{ij}^{(t)} - M_i)}{\\sum_{k=0}^2 \\exp(J_{ik}^{(t)} - M_i)}$。\nE-步到此结束，得出了所有个体 $i$ 和基因型 $j$ 的后验概率 $\\omega_{ij}^{(t)}$。\n\n对于零深度样本（$n_i=0$，因此 $r_i=0, a_i=0$），基因型似然均为 $1$，因此对于 $j=0,1,2$，$\\log L'_{ij}=0$。数据没有提供任何信息，后验概率正确地简化为先验概率：$\\omega_{ij}^{(t)} = \\Pr(g_i=j|p^{(t)})$。\n\n### 4. M-步：最大化\n\n在 M-步中，我们最大化 $Q(p|p^{(t)})$ 以求得 $p^{(t+1)}$。\n$$ Q(p|p^{(t)}) = E\\left[ \\sum_{i=1}^N \\left( (z_{i1} + 2z_{i2})\\log p + (2z_{i0} + z_{i1})\\log(1-p) \\right) \\middle| D, p^{(t)} \\right] + C' $$\n$$ Q(p|p^{(t)}) = \\sum_{i=1}^N \\left( (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})\\log p + (2\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)})\\log(1-p) \\right) + C' $$\n此函数具有二项对数似然的形式。为将其最大化，我们将其关于 $p$ 的导数设为零：\n$$ \\frac{\\partial Q}{\\partial p} = \\frac{\\sum_i (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})}{p} - \\frac{\\sum_i (2\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)})}{1-p} = 0 $$\n对 $p$ 求解得到 $p^{(t+1)}$ 的更新规则：\n$$ p^{(t+1)} = \\frac{\\sum_{i=1}^N (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})}{\\sum_{i=1}^N \\left( (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)}) + (2\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)}) \\right)} $$\n由于 $\\sum_{j=0}^2 \\omega_{ij}^{(t)} = 1$，分母简化为 $\\sum_{i=1}^N 2(\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)} + \\omega_{i2}^{(t)}) = 2N$。\n因此，M-步的更新公式为：\n$$ p^{(t+1)} = \\frac{\\sum_{i=1}^N (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})}{2N} $$\n这个表达式是直观的：等位基因频率的新估计值是参考等位基因的期望总数除以样本中的等位基因总数。\n\n### 5. 算法总结\n\n1.  **初始化**：设置初始等位基因频率 $p^{(0)}$ 和迭代计数器 $t=0$。\n2.  **迭代**：对 $t=0, 1, 2, \\dots$ 重复以下步骤，直至收敛：\n    a. **E-步**：对每个个体 $i=1, \\dots, N$，使用当前估计值 $p^{(t)}$ 和观测数据 $D_i$ 计算后验基因型概率 $\\omega_{i0}^{(t)}, \\omega_{i1}^{(t)}, \\omega_{i2}^{(t)}$。\n    b. **M-步**：使用上面推导的公式计算更新后的等位基因频率 $p^{(t+1)}$。\n    c. **检查收敛**：如果 $|p^{(t+1)} - p^{(t)}|$ 小于容差 $\\tau$（例如 $10^{-9}$），或者达到最大迭代次数 $T_{\\max}$，则终止。\n    d. 设置 $p^{(t)} \\leftarrow p^{(t+1)}$ 并继续下一次迭代。\n3.  **结果**：最终值 $p^{(t+1)}$ 即为 MLE $\\hat{p}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef em_mle(epsilon, reads, p_init, tol=1e-9, max_iter=1000):\n    \"\"\"\n    Estimates the reference allele frequency p using the Expectation-Maximization (EM) algorithm.\n\n    Args:\n        epsilon (float): The symmetric sequencing error rate.\n        reads (list of tuples): A list of (reference_reads, a_lternative_reads) for each individual.\n        p_init (float): The initial value for the allele frequency p.\n        tol (float): The convergence tolerance.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        float: The maximum likelihood estimate of p.\n    \"\"\"\n    reads_np = np.array(reads, dtype=np.float64)\n    r = reads_np[:, 0]\n    a = reads_np[:, 1]\n    n_reads = r + a\n    \n    num_individuals = len(reads_np)\n    if num_individuals == 0:\n        return p_init\n\n    p_current = p_init\n    \n    # Pre-compute log probabilities related to epsilon, which are constant across iterations.\n    log_eps = np.log(epsilon)\n    log_1_minus_eps = np.log(1 - epsilon)\n    log_0_5 = np.log(0.5)\n\n    # Calculate the log genotype likelihoods, log P(D_i | g_i=j), for each individual.\n    # This is also constant across iterations.\n    # Shape: (num_individuals, 3), where columns correspond to genotypes g=0, 1, 2.\n    log_gls = np.zeros((num_individuals, 3))\n    # g=0 (0 ref alleles): probability of a reference read is epsilon\n    log_gls[:, 0] = r * log_eps + a * log_1_minus_eps\n    # g=1 (1 ref allele): probability of a reference read is 0.5\n    log_gls[:, 1] = n_reads * log_0_5\n    # g=2 (2 ref alleles): probability of a reference read is 1-epsilon\n    log_gls[:, 2] = r * log_1_minus_eps + a * log_eps\n\n    for i in range(max_iter):\n        # Clamp p_current to avoid log(0) issues if p approaches 0 or 1.\n        p_clipped = np.clip(p_current, 1e-12, 1 - 1e-12)\n\n        # --- E-step: Calculate posterior probabilities of genotypes (responsibilities) ---\n        \n        # Calculate log genotype priors, log P(g_i=j | p), based on current p.\n        log_priors = np.array([\n            2 * np.log(1 - p_clipped),                             # g=0: (1-p)^2\n            np.log(2) + np.log(p_clipped) + np.log(1 - p_clipped), # g=1: 2p(1-p)\n            2 * np.log(p_clipped)                                  # g=2: p^2\n        ])\n\n        # Calculate log joint probability, log P(D_i, g_i=j | p), by adding log-likelihoods and log-priors.\n        # NumPy's broadcasting adds the (3,) log_priors array to each row of the (N,3) log_gls array.\n        log_joints = log_gls + log_priors\n        \n        # Normalize in log-space using logsumexp to get log posteriors, log P(g_i=j | D_i, p).\n        log_marginal_likelihoods = logsumexp(log_joints, axis=1, keepdims=True)\n        log_posteriors = log_joints - log_marginal_likelihoods\n        \n        # Convert to linear space for the M-step.\n        posteriors = np.exp(log_posteriors)\n\n        # --- M-step: Update p by maximizing the expected complete-data log-likelihood ---\n        \n        # The expected number of reference alleles for genotype j is j.\n        # Total expected number of ref alleles is sum over individuals of E[#ref_alleles_i]\n        # E[#ref_alleles_i] = 0 * posterior(g=0) + 1 * posterior(g=1) + 2 * posterior(g=2)\n        expected_ref_alleles_sum = np.sum(posteriors[:, 1] + 2 * posteriors[:, 2])\n\n        p_next = expected_ref_alleles_sum / (2 * num_individuals)\n        \n        # Check for convergence.\n        if abs(p_next - p_current)  tol:\n            p_current = p_next\n            break\n            \n        p_current = p_next\n\n    return p_current\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: General mixture\n        {'epsilon': 0.01, 'reads': [(28,2),(2,28),(15,15),(18,2),(12,13),(0,10)], 'p_init': 0.5},\n        # Case 2: Boundary leaning to fixation\n        {'epsilon': 0.001, 'reads': [(40,0),(30,0),(25,0),(10,0),(50,0)], 'p_init': 0.4},\n        # Case 3: Heterozygote-like across cohort\n        {'epsilon': 0.02, 'reads': [(4,4),(5,5),(6,6),(3,3),(7,7),(10,10),(2,2),(15,15)], 'p_init': 0.1},\n        # Case 4: Includes zero-depth individuals\n        {'epsilon': 0.01, 'reads': [(0,0),(12,12),(0,30),(0,0),(25,25),(49,1)], 'p_init': 0.6},\n        # Case 5: High sequencing error\n        {'epsilon': 0.2, 'reads': [(18,12),(2,18),(10,10),(16,4)], 'p_init': 0.3},\n    ]\n\n    results = []\n    for case in test_cases:\n        p_hat = em_mle(case['epsilon'], case['reads'], case['p_init'])\n        results.append(p_hat)\n\n    # Format results to six decimal places for the final output.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}