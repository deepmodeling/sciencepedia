{
    "hands_on_practices": [
        {
            "introduction": "Understanding Maximum Likelihood Estimation (MLE) begins with the fundamentals. This first exercise walks you through the derivation of the MLE for a Bernoulli parameter, a model that underpins the analysis of binary outcomes common in bioinformatics, such as the presence or absence of a genetic variant. By working from first principles, you will not only find the intuitive estimator for a proportion but also compute its asymptotic variance using Fisher information, linking the practical estimate to its theoretical properties .",
            "id": "4578114",
            "problem": "A tumor sequencing study using Next-Generation Sequencing (NGS) aims to estimate the cohort-level prevalence of a clinically actionable variant in a specific gene. After stringent quality control filtering to remove low-confidence reads, each tumor’s variant detection outcome is modeled as an independent Bernoulli random variable with common parameter $p \\in (0,1)$, where $p$ represents the probability that the variant is truly present in a randomly sampled tumor from the study population. Let $X_{1}, X_{2}, \\dots, X_{n}$ be the $n$ independent detection indicators, with $X_{i} \\in \\{0,1\\}$, and define the sufficient statistic $S = \\sum_{i=1}^{n} X_{i}$. Assume $0  S  n$ to avoid boundary solutions.\n\nUsing the fundamental definitions of likelihood for independent and identically distributed Bernoulli observations and Fisher information for a scalar parameter, derive the maximum likelihood estimator of $p$ and the asymptotic variance of this estimator based on the Fisher information. Express your final answer as a closed-form analytic expression in terms of $S$ and $n$ for the maximum likelihood estimator, and in terms of $p$ and $n$ for its asymptotic variance. No numerical rounding is required and no physical units apply. Provide both quantities together as a single final answer.",
            "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n-   The detection outcome for each tumor, $X_i$, is an independent Bernoulli random variable.\n-   The common parameter for the Bernoulli distribution is $p \\in (0,1)$.\n-   The observations are $X_{1}, X_{2}, \\dots, X_{n}$, which are $n$ independent and identically distributed (i.i.d.) indicators, with $X_{i} \\in \\{0,1\\}$.\n-   The sufficient statistic is $S = \\sum_{i=1}^{n} X_{i}$.\n-   An explicit assumption is made: $0  S  n$.\n-   The task is to derive the maximum likelihood estimator (MLE) of $p$ in terms of $S$ and $n$.\n-   The task is also to derive the asymptotic variance of this estimator based on the Fisher information, expressed in terms of $p$ and $n$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-posed and scientifically grounded.\n-   **Scientifically Grounded:** The modeling of binary outcomes (variant present/absent) with a Bernoulli distribution is a fundamental and standard practice in statistics and bioinformatics. The use of maximum likelihood estimation and Fisher information are core principles of statistical inference.\n-   **Well-Posed:** The problem provides a clear statistical model (i.i.d. Bernoulli trials) and a precisely defined objective (derive the MLE and its asymptotic variance). The assumption $0  S  n$ is a simplifying condition to ensure the estimator does not fall on the boundary of the parameter space, which is a standard procedure in many theoretical treatments. A unique and meaningful solution is expected.\n-   **Objective:** The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution Derivation\nThe problem requires the derivation of the maximum likelihood estimator (MLE) for the parameter $p$ of a Bernoulli distribution and the asymptotic variance of this estimator.\n\n#### Part 1: Maximum Likelihood Estimator of $p$\n\nLet $X_1, X_2, \\dots, X_n$ be a set of $n$ independent and identically distributed random variables from a Bernoulli distribution with parameter $p$. The probability mass function (PMF) for a single observation $X_i=x_i$, where $x_i \\in \\{0,1\\}$, is given by:\n$$\nP(X_i=x_i | p) = p^{x_i}(1-p)^{1-x_i}\n$$\nDue to the independence of the observations, the joint probability of observing the entire dataset $x = (x_1, x_2, \\dots, x_n)$ is the product of the individual probabilities. This joint probability, viewed as a function of the parameter $p$ for a fixed set of data, is the likelihood function, $L(p|\\boldsymbol{x})$.\n$$\nL(p | x_1, \\dots, x_n) = \\prod_{i=1}^{n} P(X_i=x_i | p) = \\prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i}\n$$\nThis expression can be simplified by using the sufficient statistic $S = \\sum_{i=1}^{n} x_i$, which represents the total number of successes (variants detected).\n$$\nL(p) = p^{\\sum_{i=1}^{n} x_i} (1-p)^{\\sum_{i=1}^{n} (1-x_i)} = p^S (1-p)^{n-S}\n$$\nTo find the value of $p$ that maximizes $L(p)$, it is analytically more convenient to maximize the natural logarithm of the likelihood function, the log-likelihood $\\ell(p)$, since the logarithm is a monotonically increasing function.\n$$\n\\ell(p) = \\ln(L(p)) = \\ln(p^S (1-p)^{n-S}) = S \\ln(p) + (n-S) \\ln(1-p)\n$$\nTo find the maximum, we compute the first derivative of $\\ell(p)$ with respect to $p$ and set it to zero. This derivative is known as the score function.\n$$\n\\frac{d\\ell}{dp} = \\frac{d}{dp} \\left( S \\ln(p) + (n-S) \\ln(1-p) \\right) = \\frac{S}{p} - \\frac{n-S}{1-p}\n$$\nSetting the score to zero to find the critical point, we solve for $p$. Let $\\hat{p}$ denote the estimator.\n$$\n\\frac{S}{\\hat{p}} - \\frac{n-S}{1-\\hat{p}} = 0 \\implies \\frac{S}{\\hat{p}} = \\frac{n-S}{1-\\hat{p}}\n$$\n$$\nS(1-\\hat{p}) = (n-S)\\hat{p}\n$$\n$$\nS - S\\hat{p} = n\\hat{p} - S\\hat{p}\n$$\n$$\nS = n\\hat{p}\n$$\nSolving for $\\hat{p}$ yields the maximum likelihood estimator:\n$$\n\\hat{p} = \\frac{S}{n}\n$$\nThe condition $0  S  n$ ensures that $\\hat{p}$ is in the open interval $(0,1)$, consistent with the parameter space $p \\in (0,1)$, and avoids division by zero in the score function. To confirm that this is a maximum, we examine the second derivative of the log-likelihood:\n$$\n\\frac{d^2\\ell}{dp^2} = \\frac{d}{dp} \\left( \\frac{S}{p} - \\frac{n-S}{1-p} \\right) = -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2}\n$$\nSince $S0$, $n-S0$, and $p^2  0$, $(1-p)^2  0$, the second derivative $\\frac{d^2\\ell}{dp^2}$ is strictly negative for all $p \\in (0,1)$. This confirms that the log-likelihood function is concave and the critical point $\\hat{p} = S/n$ is indeed a unique maximum.\n\n#### Part 2: Asymptotic Variance of the MLE\n\nThe asymptotic variance of an MLE is given by the inverse of the Fisher information, $I(p)$. For a single scalar parameter, the Fisher information is defined as:\n$$\nI(p) = -E\\left[ \\frac{d^2\\ell}{dp^2} \\right]\n$$\nwhere the expectation is taken with respect to the distribution of the data. We have the second derivative:\n$$\n\\frac{d^2\\ell}{dp^2} = -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2}\n$$\nNow, we take the expectation. The random variable in this expression is $S = \\sum_{i=1}^n X_i$. The expectation of a sum is the sum of expectations. For a single Bernoulli trial $X_i$, $E[X_i] = 1 \\cdot p + 0 \\cdot (1-p) = p$. Therefore, the expectation of $S$ is:\n$$\nE[S] = E\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n E[X_i] = \\sum_{i=1}^n p = np\n$$\nSubstituting $E[S]$ into the expression for the expected second derivative:\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = E\\left[ -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2} \\right] = -\\frac{E[S]}{p^2} - \\frac{n-E[S]}{(1-p)^2}\n$$\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = -\\frac{np}{p^2} - \\frac{n-np}{(1-p)^2} = -\\frac{n}{p} - \\frac{n(1-p)}{(1-p)^2}\n$$\n$$\nE\\left[ \\frac{d^2\\ell}{dp^2} \\right] = -\\frac{n}{p} - \\frac{n}{1-p} = -n \\left( \\frac{1}{p} + \\frac{1}{1-p} \\right) = -n \\left( \\frac{1-p+p}{p(1-p)} \\right) = -\\frac{n}{p(1-p)}\n$$\nThe Fisher information is the negative of this quantity:\n$$\nI(p) = - \\left( -\\frac{n}{p(1-p)} \\right) = \\frac{n}{p(1-p)}\n$$\nAccording to the Cramer-Rao Lower Bound and large-sample theory for MLEs, the asymptotic variance of $\\hat{p}$ is the inverse of the Fisher information:\n$$\n\\text{Asymptotic Variance}(\\hat{p}) = [I(p)]^{-1} = \\left( \\frac{n}{p(1-p)} \\right)^{-1} = \\frac{p(1-p)}{n}\n$$\nThis variance is the variance of the sample mean of i.i.d. Bernoulli random variables, which is a well-known result, and the MLE's asymptotic variance converges to this value.\n\nThe final answer requires the expression for the MLE, $\\hat{p}$, and its asymptotic variance.\n-   Maximum Likelihood Estimator: $\\hat{p} = \\frac{S}{n}$\n-   Asymptotic Variance: $\\frac{p(1-p)}{n}$\n\nThese two quantities will be presented as a single row matrix.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{S}{n}  \\frac{p(1-p)}{n} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Building on the basic principles of likelihood, this practice shifts our focus to count data, which is central to fields like transcriptomics. Using the Poisson distribution to model RNA-seq counts, you will derive the MLE for an expression rate parameter while accounting for experimental normalization factors. More importantly, this exercise introduces the powerful concept of sufficiency, revealing how the likelihood function itself justifies compressing a large dataset into a single summary statistic without any loss of information for inference .",
            "id": "4578097",
            "problem": "A single gene’s messenger ribonucleic acid (mRNA) molecule counts from $n$ single cells in a ribonucleic acid sequencing (RNA-seq) experiment are modeled as independent Poisson random variables with a shared rate parameter $\\lambda$ and known cell-specific exposures $t_{i} \\in (0,\\infty)$ that account for library size normalization. Formally, conditional on $\\lambda$, the counts $X_{1},\\dots,X_{n}$ satisfy $X_{i} \\sim \\mathrm{Pois}(\\lambda t_{i})$ independently for $i=1,\\dots,n$. The Poisson probability mass function is given by $p(x;\\mu)=\\frac{\\mu^{x}\\exp(-\\mu)}{x!}$ for $x \\in \\{0,1,2,\\dots\\}$ and mean $\\mu0$.\n\nStarting only from these definitions and the Fisher–Neyman factorization theorem, derive a statistic that is sufficient and minimal sufficient for $\\lambda$, and express the likelihood in a form that makes explicit how the data can be compressed without any loss of information for likelihood-based inference about $\\lambda$. Carefully justify each step of the derivation from first principles, and articulate the implications of your result for likelihood-based compression of count data in high-throughput bioinformatics settings.\n\nThen, derive the maximum likelihood estimator (MLE) of $\\lambda$ and express it in closed form in terms of the observed counts $x_{1},\\dots,x_{n}$ and the known exposures $t_{1},\\dots,t_{n}$.\n\nProvide your final answer as the single analytic expression for the MLE of $\\lambda$ in terms of $x_{i}$ and $t_{i}$. No numerical rounding is required, and no units are necessary.",
            "solution": "The problem statement is critically validated as follows.\n\n**Step 1: Extract Givens**\n- Data: $n$ single-cell mRNA molecule counts, $X_{1}, \\dots, X_{n}$.\n- Model: The counts are modeled as independent random variables.\n- Distribution: For each $i \\in \\{1, \\dots, n\\}$, $X_{i} \\sim \\mathrm{Pois}(\\lambda t_{i})$, conditional on $\\lambda$.\n- Parameter: $\\lambda$ is a shared rate parameter, $\\lambda \\in (0, \\infty)$.\n- Known Constants: $t_{1}, \\dots, t_{n}$ are known cell-specific positive exposures, $t_{i} \\in (0, \\infty)$.\n- Probability Mass Function (PMF) of Poisson distribution: $p(x;\\mu) = \\frac{\\mu^{x}\\exp(-\\mu)}{x!}$ for $x \\in \\{0, 1, 2, \\dots\\}$ and mean $\\mu  0$.\n- Required tasks:\n    1.  Derive a statistic that is sufficient and minimal sufficient for $\\lambda$ using the Fisher–Neyman factorization theorem.\n    2.  Express the likelihood to show how data can be compressed.\n    3.  Articulate the implications for data compression in bioinformatics.\n    4.  Derive the maximum likelihood estimator (MLE) of $\\lambda$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly grounded in established statistical theory and its application in bioinformatics. The Poisson model is a standard and widely accepted model for count data, including RNA-seq data, and the use of exposure/offset terms ($t_i$) is a common technique for normalization.\n- **Well-Posed:** The problem provides all necessary information and definitions to derive the requested statistical quantities. The objectives are clear and lead to a unique, well-defined solution based on standard statistical methods.\n- **Objective:** The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-formulated problem in mathematical statistics and its application to bioinformatics. The solution will proceed.\n\nWe begin by constructing the likelihood function for the observed data. The observations $X_{1}, \\dots, X_{n}$ are independent. The likelihood function $L(\\lambda; \\mathbf{x})$ for a vector of observed counts $\\mathbf{x} = (x_1, \\dots, x_n)$ is the joint probability mass function, given by the product of the individual PMFs:\n$$\nL(\\lambda; \\mathbf{x}) = \\prod_{i=1}^{n} p(x_i; \\lambda t_i)\n$$\nSubstituting the Poisson PMF $p(x;\\mu) = \\frac{\\mu^{x}\\exp(-\\mu)}{x!}$ with mean $\\mu_i = \\lambda t_i$:\n$$\nL(\\lambda; \\mathbf{x}) = \\prod_{i=1}^{n} \\frac{(\\lambda t_i)^{x_i} \\exp(-\\lambda t_i)}{x_i!}\n$$\nWe can rearrange this expression by separating terms that involve $\\lambda$ and terms that do not:\n$$\nL(\\lambda; \\mathbf{x}) = \\left( \\prod_{i=1}^{n} \\lambda^{x_i} \\right) \\left( \\prod_{i=1}^{n} t_i^{x_i} \\right) \\left( \\prod_{i=1}^{n} \\exp(-\\lambda t_i) \\right) \\left( \\prod_{i=1}^{n} \\frac{1}{x_i!} \\right)\n$$\nUsing the properties of products and exponents, we get:\n$$\nL(\\lambda; \\mathbf{x}) = \\lambda^{\\sum_{i=1}^{n} x_i} \\cdot \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\cdot \\left( \\frac{\\prod_{i=1}^{n} t_i^{x_i}}{\\prod_{i=1}^{n} x_i!} \\right)\n$$\nThe Fisher–Neyman factorization theorem states that a statistic $T(\\mathbf{X})$ is sufficient for a parameter $\\theta$ if and only if the likelihood function can be factorized into two non-negative functions:\n$$\nL(\\theta; \\mathbf{x}) = g(T(\\mathbf{x}), \\theta) \\cdot h(\\mathbf{x})\n$$\nwhere $g$ depends on the data $\\mathbf{x}$ only through the statistic $T(\\mathbf{x})$, and $h$ does not depend on the parameter $\\theta$.\n\nIn our case, let's define the statistic $T(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$. Then our likelihood function can be written as:\n$$\nL(\\lambda; \\mathbf{x}) = \\underbrace{\\left[ \\lambda^{\\sum_{i=1}^{n} x_i} \\cdot \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\right]}_{g(T(\\mathbf{x}), \\lambda)} \\cdot \\underbrace{\\left( \\frac{\\prod_{i=1}^{n} t_i^{x_i}}{\\prod_{i=1}^{n} x_i!} \\right)}_{h(\\mathbf{x})}\n$$\nHere, the function $g$ depends on the data $\\mathbf{x}$ only through the sum $T(\\mathbf{x}) = \\sum_{i=1}^{n} x_i$. The known constants $\\sum_{i=1}^{n} t_i$ are part of the model structure, not the data realization. The function $h(\\mathbf{x})$ depends on $\\mathbf{x}$ and the known exposures $t_i$, but not on the parameter $\\lambda$. Therefore, by the Fisher–Neyman factorization theorem, the statistic $T(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$ is a sufficient statistic for $\\lambda$.\n\nTo show that $T(\\mathbf{X})$ is minimal sufficient, we can examine the ratio of likelihoods for two different data vectors, $\\mathbf{x}$ and $\\mathbf{y}$. A statistic $T$ is minimal sufficient if, for any two data points $\\mathbf{x}$ and $\\mathbf{y}$, the ratio $L(\\lambda; \\mathbf{x})/L(\\lambda; \\mathbf{y})$ is constant as a function of $\\lambda$ if and only if $T(\\mathbf{x}) = T(\\mathbf{y})$.\nLet's form the ratio:\n$$\n\\frac{L(\\lambda; \\mathbf{x})}{L(\\lambda; \\mathbf{y})} = \\frac{\\lambda^{\\sum x_i} \\exp(-\\lambda \\sum t_i) \\left( \\frac{\\prod t_i^{x_i}}{\\prod x_i!} \\right)}{\\lambda^{\\sum y_i} \\exp(-\\lambda \\sum t_i) \\left( \\frac{\\prod t_i^{y_i}}{\\prod y_i!} \\right)}\n$$\n$$\n\\frac{L(\\lambda; \\mathbf{x})}{L(\\lambda; \\mathbf{y})} = \\lambda^{(\\sum x_i - \\sum y_i)} \\cdot \\frac{\\left( \\frac{\\prod t_i^{x_i}}{\\prod x_i!} \\right)}{\\left( \\frac{\\prod t_i^{y_i}}{\\prod y_i!} \\right)}\n$$\nFor this ratio to be independent of $\\lambda$ for all $\\lambda  0$, the term $\\lambda^{(\\sum x_i - \\sum y_i)}$ must be constant. This requires the exponent to be zero, i.e., $\\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} y_i = 0$, which implies $\\sum_{i=1}^{n} x_i = \\sum_{i=1}^{n} y_i$. This is exactly the condition $T(\\mathbf{x}) = T(\\mathbf{y})$. Thus, $T(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$ is a minimal sufficient statistic for $\\lambda$.\n\nThe explicit factorization $L(\\lambda; \\mathbf{x}) = g(T(\\mathbf{x}), \\lambda) \\cdot h(\\mathbf{x})$ demonstrates how data can be compressed without loss of information for likelihood-based inference. All information about $\\lambda$ is contained in the function $g$, which depends on the data solely through the value of the minimal sufficient statistic $T(\\mathbf{x}) = \\sum_{i=1}^{n} x_i$. The term $h(\\mathbf{x})$ contains no information about $\\lambda$ and can be disregarded for the purpose of inference on $\\lambda$.\n\nThe implication for high-throughput bioinformatics is profound. An RNA-seq experiment might generate counts for a single gene across thousands or millions of cells (i.e., $n$ is very large). The original dataset for a single gene is a vector of counts $(x_1, x_2, \\dots, x_n)$ of dimension $n$. The existence of the minimal sufficient statistic $T(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$ implies that, for the purposes of estimating the underlying expression rate $\\lambda$ or performing any other likelihood-based inference, this entire vector of $n$ numbers can be replaced by a single number: its sum. The dataset can be compressed from $n$ values to $1$ value, $T(\\mathbf{x})$, along with the pre-determined sum of exposures $\\sum t_i$, with absolutely no loss of information about $\\lambda$. This leads to massive efficiencies in data storage, transmission, and computation.\n\nNext, we derive the maximum likelihood estimator (MLE) of $\\lambda$. The MLE, denoted $\\hat{\\lambda}$, is the value of $\\lambda$ that maximizes the likelihood function $L(\\lambda; \\mathbf{x})$. It is often easier to maximize the natural logarithm of the likelihood, the log-likelihood function $\\ell(\\lambda; \\mathbf{x}) = \\ln(L(\\lambda; \\mathbf{x}))$.\n$$\n\\ell(\\lambda; \\mathbf{x}) = \\ln\\left[ \\lambda^{\\sum x_i} \\exp(-\\lambda \\sum t_i) \\left( \\frac{\\prod t_i^{x_i}}{\\prod x_i!} \\right) \\right]\n$$\n$$\n\\ell(\\lambda; \\mathbf{x}) = \\left(\\sum_{i=1}^{n} x_i\\right) \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} t_i + \\sum_{i=1}^{n} x_i \\ln(t_i) - \\sum_{i=1}^{n} \\ln(x_i!)\n$$\nTo find the maximum, we compute the first derivative of $\\ell(\\lambda; \\mathbf{x})$ with respect to $\\lambda$ and set it to zero.\n$$\n\\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda} \\left[ \\left(\\sum_{i=1}^{n} x_i\\right) \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} t_i + \\text{const.} \\right]\n$$\n$$\n\\frac{d\\ell}{d\\lambda} = \\frac{\\sum_{i=1}^{n} x_i}{\\lambda} - \\sum_{i=1}^{n} t_i\n$$\nSetting the derivative to zero gives the equation for the MLE $\\hat{\\lambda}$:\n$$\n\\frac{\\sum_{i=1}^{n} x_i}{\\hat{\\lambda}} - \\sum_{i=1}^{n} t_i = 0\n$$\nSolving for $\\hat{\\lambda}$:\n$$\n\\frac{\\sum_{i=1}^{n} x_i}{\\hat{\\lambda}} = \\sum_{i=1}^{n} t_i\n$$\n$$\n\\hat{\\lambda} = \\frac{\\sum_{i=1}^{n} x_i}{\\sum_{i=1}^{n} t_i}\n$$\nTo confirm this is a maximum, we examine the second derivative:\n$$\n\\frac{d^2\\ell}{d\\lambda^2} = \\frac{d}{d\\lambda} \\left[ \\left(\\sum_{i=1}^{n} x_i\\right) \\lambda^{-1} - \\sum_{i=1}^{n} t_i \\right] = - \\left(\\sum_{i=1}^{n} x_i\\right) \\lambda^{-2} = -\\frac{\\sum_{i=1}^{n} x_i}{\\lambda^2}\n$$\nSince the counts $x_i \\ge 0$, their sum $\\sum_{i=1}^{n} x_i \\ge 0$. The exposures $t_i  0$, so their sum is positive. If all $x_i = 0$, then $\\hat{\\lambda}=0$, which lies on the boundary of the parameter space $(0, \\infty)$. If at least one $x_i  0$, then $\\sum x_i  0$, and since $\\lambda \\in (0, \\infty)$, $\\lambda^2  0$. In this case, $\\frac{d^2\\ell}{d\\lambda^2}  0$, which indicates that the log-likelihood function is strictly concave, and thus the critical point is a unique global maximum. The derived expression is the MLE.\n\nThe final expression for the MLE, $\\hat{\\lambda}$, is the total number of observed counts divided by the total exposure. This is an intuitively appealing result: the estimated rate is the total observed events per total \"time\" or exposure.",
            "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n} x_{i}}{\\sum_{i=1}^{n} t_{i}}}$$"
        },
        {
            "introduction": "Many real-world bioinformatics problems involve incomplete data or latent (unobserved) variables, making direct likelihood maximization difficult. This advanced practice tackles such a scenario by guiding you through the implementation of the Expectation-Maximization (EM) algorithm to estimate population allele frequencies from sequencing data where the true genotypes are unknown. This exercise demonstrates how to iteratively solve for the MLE in complex models by cycling between estimating the latent variables and optimizing the parameters, a cornerstone technique in modern statistical genomics .",
            "id": "4578121",
            "problem": "You are given a biallelic locus in a cohort sequenced by Next-Generation Sequencing (NGS). Each individual $i \\in \\{1,\\dots,N\\}$ has a latent genotype $g_i \\in \\{0,1,2\\}$ denoting the number of reference alleles. You observe incomplete data: the number of reads supporting the reference allele $r_i$ and the number supporting the alternative allele $a_i$, with $n_i = r_i + a_i$. Reads are assumed independent conditional on $g_i$ and a known per-base symmetric sequencing error rate $\\varepsilon \\in (0,1/2)$. Under this model, the probability that any single read reports the reference base is: if $g_i=0$, the probability is $\\varepsilon$; if $g_i=1$, the probability is $1/2$; if $g_i=2$, the probability is $1-\\varepsilon$. The population allele frequency of the reference allele is $p \\in (0,1)$, and genotypes follow Hardy–Weinberg equilibrium (HWE) with prior probabilities $\\Pr(g_i=0)=(1-p)^2$, $\\Pr(g_i=1)=2p(1-p)$, $\\Pr(g_i=2)=p^2$. Assume independence across individuals.\n\nYour task is to use likelihood-based inference to estimate $p$ by Maximum Likelihood Estimation (MLE) via the Expectation–Maximization (EM) algorithm, treating genotypes as latent. Start from core definitions of likelihoods and conditional independence; do not assume any shortcut formulas. Derive the expected complete-data sufficient statistics required by EM and the update for $p$ from first principles by explicitly writing the complete-data likelihood, taking expectations with respect to the posterior of latent variables given current parameters, and maximizing with respect to $p$.\n\nImplementation requirements:\n- Input is fixed and embedded in your program. For each test case, use the provided symmetric error rate $\\varepsilon$, the list of observed read counts $\\{(r_i,a_i)\\}_{i=1}^N$ with $r_i,a_i \\in \\mathbb{N}_0$, and an initial value $p^{(0)} \\in (0,1)$.\n- Use an EM algorithm that iterates the E-step and M-step until the absolute difference $|p^{(t+1)} - p^{(t)}|$ is less than a tolerance $\\tau = 10^{-9}$ or the number of iterations reaches a maximum $T_{\\max} = 1000$, whichever occurs first.\n- For numerical stability, implement the E-step in the log-domain using a stable normalization (e.g., log-sum-exp). Treat zero-depth samples (where $n_i = 0$) consistently with the probabilistic model.\n- The final estimate $\\hat p$ for each test case must be printed as a decimal rounded to six digits after the decimal point. Do not print a percentage sign.\n\nTest suite:\n- Case $1$ (general mixture): $\\varepsilon = 0.01$, reads $\\{(28,2),(2,28),(15,15),(18,2),(12,13),(0,10)\\}$, $p^{(0)} = 0.5$.\n- Case $2$ (boundary leaning to fixation): $\\varepsilon = 0.001$, reads $\\{(40,0),(30,0),(25,0),(10,0),(50,0)\\}$, $p^{(0)} = 0.4$.\n- Case $3$ (heterozygote-like across cohort): $\\varepsilon = 0.02$, reads $\\{(4,4),(5,5),(6,6),(3,3),(7,7),(10,10),(2,2),(15,15)\\}$, $p^{(0)} = 0.1$.\n- Case $4$ (includes zero-depth individuals): $\\varepsilon = 0.01$, reads $\\{(0,0),(12,12),(0,30),(0,0),(25,25),(49,1)\\}$, $p^{(0)} = 0.6$.\n- Case $5$ (high sequencing error): $\\varepsilon = 0.2$, reads $\\{(18,12),(2,18),(10,10),(16,4)\\}$, $p^{(0)} = 0.3$.\n\nYour program should produce a single line of output containing the estimated allele frequencies for the five cases as a comma-separated list enclosed in square brackets, each rounded to six digits after the decimal point (for example, $[0.123456,0.654321,0.500000,0.750000,0.333333]$). No other text should be printed.",
            "solution": "The problem requires the derivation and implementation of an Expectation-Maximization (EM) algorithm to find the Maximum Likelihood Estimate (MLE) of the reference allele frequency, $p$, in a population. The estimation is based on Next-Generation Sequencing (NGS) read counts, where individual genotypes are unobserved (latent).\n\n### 1. Model Specification\n\nLet us formally define the probabilistic model.\n\n-   **Observed Data**: For each of $N$ individuals, we observe the read counts $D_i = (r_i, a_i)$, where $r_i$ is the number of reads supporting the reference allele and $a_i$ is the number of reads supporting the alternative allele. The total number of reads for individual $i$ is $n_i = r_i + a_i$. The complete set of observed data is $D = \\{D_i\\}_{i=1}^N$.\n-   **Latent Variables**: The genotype $g_i$ for each individual is a latent variable. As the locus is biallelic, $g_i \\in \\{0, 1, 2\\}$ represents the number of reference alleles possessed by individual $i$. The set of all genotypes is $G = \\{g_i\\}_{i=1}^N$.\n-   **Parameter**: The parameter to be estimated is the reference allele frequency in the population, $p \\in (0,1)$. The sequencing error rate $\\varepsilon \\in (0, 1/2)$ is a known constant.\n-   **Sequencing Error Model**: Conditional on the genotype $g_i$, the number of reference reads $r_i$ out of $n_i$ total reads is assumed to follow a binomial distribution: $r_i | g_i, n_i \\sim \\text{Binomial}(n_i, \\theta_{g_i})$. The probability $\\theta_{g_i}$ that a single read is a reference allele depends on the true genotype and the symmetric error rate $\\varepsilon$:\n    -   If $g_i=0$ (homozygous alternative), $\\theta_0 = \\varepsilon$.\n    -   If $g_i=1$ (heterozygous), $\\theta_1 = 1/2$.\n    -   If $g_i=2$ (homozygous reference), $\\theta_2 = 1-\\varepsilon$.\n    The likelihood of observing reads $D_i$ given genotype $g_i$ is therefore $\\Pr(D_i|g_i, \\varepsilon) = \\binom{n_i}{r_i}\\theta_{g_i}^{r_i}(1-\\theta_{g_i})^{a_i}$.\n-   **Population Genetic Model**: Genotypes are assumed to be in Hardy–Weinberg Equilibrium (HWE). The prior probability of each genotype depends on the allele frequency $p$:\n    -   $\\Pr(g_i=0|p) = (1-p)^2$\n    -   $\\Pr(g_i=1|p) = 2p(1-p)$\n    -   $\\Pr(g_i=2|p) = p^2$\n\n### 2. The EM Algorithm Framework\n\nOur goal is to find the MLE of $p$ by maximizing the log-likelihood of the observed data, $\\ell(p; D) = \\log \\Pr(D | p, \\varepsilon)$. This marginal likelihood is obtained by summing over all possible latent genotype configurations:\n$$ \\ell(p; D) = \\log \\left( \\sum_{G} \\Pr(D, G | p, \\varepsilon) \\right) = \\sum_{i=1}^N \\log \\left( \\sum_{j=0}^2 \\Pr(D_i | g_i=j, \\varepsilon) \\Pr(g_i=j | p) \\right) $$\nDirect maximization of this expression is complicated due to the sum inside the logarithm. The EM algorithm provides an iterative approach to solve this. It operates on the **complete-data log-likelihood**, $\\ell_c(p; D, G) = \\log \\Pr(D, G | p, \\varepsilon)$, which is more amenable to maximization.\n\nAssuming independence across individuals, the complete-data log-likelihood is:\n$$ \\ell_c(p; D, G) = \\sum_{i=1}^N \\log \\Pr(D_i, g_i | p, \\varepsilon) = \\sum_{i=1}^N \\left( \\log \\Pr(D_i|g_i, \\varepsilon) + \\log \\Pr(g_i|p) \\right) $$\nWe introduce indicator variables $z_{ij} = \\mathbb{I}(g_i=j)$ for $j \\in \\{0, 1, 2\\}$, where $\\sum_{j=0}^2 z_{ij} = 1$. The term relevant for estimating $p$ is $\\log \\Pr(g_i|p)$:\n$$ \\log \\Pr(g_i|p) = z_{i0} \\log((1-p)^2) + z_{i1} \\log(2p(1-p)) + z_{i2} \\log(p^2) $$\n$$ = (2z_{i0} + z_{i1})\\log(1-p) + (z_{i1} + 2z_{i2})\\log p + z_{i1}\\log 2 $$\nThe complete-data log-likelihood can be expressed as:\n$$ \\ell_c(p; D, G) = \\sum_{i=1}^N \\left[ (z_{i1} + 2z_{i2})\\log p + (2z_{i0} + z_{i1})\\log(1-p) \\right] + C $$\nwhere $C$ contains terms that do not depend on $p$. The terms $(z_{i1} + 2z_{i2})$ and $(2z_{i0} + z_{i1})$ represent the counts of reference and alternative alleles, respectively, for individual $i$.\n\nThe EM algorithm consists of two steps, iterated until convergence:\n1.  **E-Step**: Compute the expectation of the complete-data log-likelihood with respect to the posterior distribution of the latent variables $G$ given the observed data $D$ and the current parameter estimate $p^{(t)}$. This function is denoted $Q(p|p^{(t)})$.\n2.  **M-Step**: Maximize $Q(p|p^{(t)})$ with respect to $p$ to obtain the updated parameter estimate $p^{(t+1)}$.\n\n### 3. E-Step: Expectation\n\nThe E-step involves calculating $Q(p|p^{(t)}) = E_{G|D, p^{(t)}}[\\ell_c(p; D, G)]$. Due to the linearity of expectation, we only need the expectation of the indicator variables $z_{ij}$:\n$$ E[z_{ij} | D, p^{(t)}] = \\Pr(g_i=j | D_i, p^{(t)}) $$\nLet's denote this posterior probability, or responsibility, as $\\omega_{ij}^{(t)}$. Using Bayes' theorem:\n$$ \\omega_{ij}^{(t)} = \\frac{\\Pr(D_i|g_i=j, \\varepsilon) \\Pr(g_i=j|p^{(t)})}{\\sum_{k=0}^2 \\Pr(D_i|g_i=k, \\varepsilon) \\Pr(g_i=k|p^{(t)})} $$\nFor computational stability, we perform this calculation in the log domain. Let:\n-   Log Genotype Likelihood: $\\log L'_{ij} = \\log \\Pr(D_i|g_i=j, \\varepsilon)_{\\text{unnormalized}} = r_i \\log \\theta_j + a_i \\log(1-\\theta_j)$.\n    -   $\\log L'_{i0} = r_i \\log \\varepsilon + a_i \\log(1-\\varepsilon)$\n    -   $\\log L'_{i1} = (r_i+a_i) \\log(0.5)$\n    -   $\\log L'_{i2} = r_i \\log(1-\\varepsilon) + a_i \\log \\varepsilon$\n-   Log Genotype Prior: $\\log\\pi_j^{(t)} = \\log \\Pr(g_i=j|p^{(t)})$.\n    -   $\\log\\pi_0^{(t)} = 2\\log(1-p^{(t)})$\n    -   $\\log\\pi_1^{(t)} = \\log 2 + \\log p^{(t)} + \\log(1-p^{(t)})$\n    -   $\\log\\pi_2^{(t)} = 2\\log p^{(t)}$\n\nThe log of the joint probability is $J_{ij}^{(t)} = \\log L'_{ij} + \\log\\pi_j^{(t)}$. The denominator in the expression for $\\omega_{ij}^{(t)}$ can be unstable if computed directly. We use the log-sum-exp trick:\n-   Let $M_i = \\max_k J_{ik}^{(t)}$.\n-   Then $\\omega_{ij}^{(t)} = \\frac{\\exp(J_{ij}^{(t)} - M_i)}{\\sum_{k=0}^2 \\exp(J_{ik}^{(t)} - M_i)}$.\nThis concludes the E-step, yielding the posterior probabilities $\\omega_{ij}^{(t)}$ for all individuals $i$ and genotypes $j$.\n\nFor a zero-depth sample ($n_i=0$, so $r_i=0, a_i=0$), the genotype likelihoods are all $1$, so $\\log L'_{ij}=0$ for $j=0,1,2$. The data provides no information, and the posterior probabilities correctly reduce to the prior probabilities: $\\omega_{ij}^{(t)} = \\Pr(g_i=j|p^{(t)})$.\n\n### 4. M-Step: Maximization\n\nIn the M-step, we maximize $Q(p|p^{(t)})$ to find $p^{(t+1)}$.\n$$ Q(p|p^{(t)}) = E\\left[ \\sum_{i=1}^N \\left( (z_{i1} + 2z_{i2})\\log p + (2z_{i0} + z_{i1})\\log(1-p) \\right) \\middle| D, p^{(t)} \\right] + C' $$\n$$ Q(p|p^{(t)}) = \\sum_{i=1}^N \\left( (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})\\log p + (2\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)})\\log(1-p) \\right) + C' $$\nThis function has the form of a binomial log-likelihood. To maximize it, we set its derivative with respect to $p$ to zero:\n$$ \\frac{\\partial Q}{\\partial p} = \\frac{\\sum_i (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})}{p} - \\frac{\\sum_i (2\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)})}{1-p} = 0 $$\nSolving for $p$ yields the update rule for $p^{(t+1)}$:\n$$ p^{(t+1)} = \\frac{\\sum_{i=1}^N (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})}{\\sum_{i=1}^N \\left( (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)}) + (2\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)}) \\right)} $$\nSince $\\sum_{j=0}^2 \\omega_{ij}^{(t)} = 1$, the denominator simplifies to $\\sum_{i=1}^N 2(\\omega_{i0}^{(t)} + \\omega_{i1}^{(t)} + \\omega_{i2}^{(t)}) = 2N$.\nThus, the M-step update is:\n$$ p^{(t+1)} = \\frac{\\sum_{i=1}^N (\\omega_{i1}^{(t)} + 2\\omega_{i2}^{(t)})}{2N} $$\nThis expression is intuitive: the new estimate for the allele frequency is the expected total number of reference alleles divided by the total number of alleles in the sample.\n\n### 5. Algorithmic Summary\n\n1.  **Initialize**: Set an initial allele frequency $p^{(0)}$ and iteration counter $t=0$.\n2.  **Iterate**: Repeat for $t=0, 1, 2, \\dots$ until convergence:\n    a. **E-Step**: For each individual $i=1, \\dots, N$, compute the posterior genotype probabilities $\\omega_{i0}^{(t)}, \\omega_{i1}^{(t)}, \\omega_{i2}^{(t)}$ using the current estimate $p^{(t)}$ and the observed data $D_i$.\n    b. **M-Step**: Compute the updated allele frequency $p^{(t+1)}$ using the formula derived above.\n    c. **Check Convergence**: If $|p^{(t+1)} - p^{(t)}|$ falls below a tolerance $\\tau$ (e.g., $10^{-9}$), or if the maximum number of iterations $T_{\\max}$ is reached, terminate.\n    d. Set $p^{(t)} \\leftarrow p^{(t+1)}$ and continue to the next iteration.\n3.  **Result**: The final value $p^{(t+1)}$ is the MLE $\\hat{p}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef em_mle(epsilon, reads, p_init, tol=1e-9, max_iter=1000):\n    \"\"\"\n    Estimates the reference allele frequency p using the Expectation-Maximization (EM) algorithm.\n\n    Args:\n        epsilon (float): The symmetric sequencing error rate.\n        reads (list of tuples): A list of (reference_reads, a_lternative_reads) for each individual.\n        p_init (float): The initial value for the allele frequency p.\n        tol (float): The convergence tolerance.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        float: The maximum likelihood estimate of p.\n    \"\"\"\n    reads_np = np.array(reads, dtype=np.float64)\n    r = reads_np[:, 0]\n    a = reads_np[:, 1]\n    n_reads = r + a\n    \n    num_individuals = len(reads_np)\n    if num_individuals == 0:\n        return p_init\n\n    p_current = p_init\n    \n    # Pre-compute log probabilities related to epsilon, which are constant across iterations.\n    log_eps = np.log(epsilon)\n    log_1_minus_eps = np.log(1 - epsilon)\n    log_0_5 = np.log(0.5)\n\n    # Calculate the log genotype likelihoods, log P(D_i | g_i=j), for each individual.\n    # This is also constant across iterations.\n    # Shape: (num_individuals, 3), where columns correspond to genotypes g=0, 1, 2.\n    log_gls = np.zeros((num_individuals, 3))\n    # g=0 (0 ref alleles): probability of a reference read is epsilon\n    log_gls[:, 0] = r * log_eps + a * log_1_minus_eps\n    # g=1 (1 ref allele): probability of a reference read is 0.5\n    log_gls[:, 1] = n_reads * log_0_5\n    # g=2 (2 ref alleles): probability of a reference read is 1-epsilon\n    log_gls[:, 2] = r * log_1_minus_eps + a * log_eps\n\n    for i in range(max_iter):\n        # Clamp p_current to avoid log(0) issues if p approaches 0 or 1.\n        p_clipped = np.clip(p_current, 1e-12, 1 - 1e-12)\n\n        # --- E-step: Calculate posterior probabilities of genotypes (responsibilities) ---\n        \n        # Calculate log genotype priors, log P(g_i=j | p), based on current p.\n        log_priors = np.array([\n            2 * np.log(1 - p_clipped),                             # g=0: (1-p)^2\n            np.log(2) + np.log(p_clipped) + np.log(1 - p_clipped), # g=1: 2p(1-p)\n            2 * np.log(p_clipped)                                  # g=2: p^2\n        ])\n\n        # Calculate log joint probability, log P(D_i, g_i=j | p), by adding log-likelihoods and log-priors.\n        # NumPy's broadcasting adds the (3,) log_priors array to each row of the (N,3) log_gls array.\n        log_joints = log_gls + log_priors\n        \n        # Normalize in log-space using logsumexp to get log posteriors, log P(g_i=j | D_i, p).\n        log_marginal_likelihoods = logsumexp(log_joints, axis=1, keepdims=True)\n        log_posteriors = log_joints - log_marginal_likelihoods\n        \n        # Convert to linear space for the M-step.\n        posteriors = np.exp(log_posteriors)\n\n        # --- M-step: Update p by maximizing the expected complete-data log-likelihood ---\n        \n        # The expected number of reference alleles for genotype j is j.\n        # Total expected number of ref alleles is sum over individuals of E[#ref_alleles_i]\n        # E[#ref_alleles_i] = 0 * posterior(g=0) + 1 * posterior(g=1) + 2 * posterior(g=2)\n        expected_ref_alleles_sum = np.sum(posteriors[:, 1] + 2 * posteriors[:, 2])\n\n        p_next = expected_ref_alleles_sum / (2 * num_individuals)\n        \n        # Check for convergence.\n        if abs(p_next - p_current)  tol:\n            p_current = p_next\n            break\n            \n        p_current = p_next\n\n    return p_current\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: General mixture\n        {'epsilon': 0.01, 'reads': [(28,2),(2,28),(15,15),(18,2),(12,13),(0,10)], 'p_init': 0.5},\n        # Case 2: Boundary leaning to fixation\n        {'epsilon': 0.001, 'reads': [(40,0),(30,0),(25,0),(10,0),(50,0)], 'p_init': 0.4},\n        # Case 3: Heterozygote-like across cohort\n        {'epsilon': 0.02, 'reads': [(4,4),(5,5),(6,6),(3,3),(7,7),(10,10),(2,2),(15,15)], 'p_init': 0.1},\n        # Case 4: Includes zero-depth individuals\n        {'epsilon': 0.01, 'reads': [(0,0),(12,12),(0,30),(0,0),(25,25),(49,1)], 'p_init': 0.6},\n        # Case 5: High sequencing error\n        {'epsilon': 0.2, 'reads': [(18,12),(2,18),(10,10),(16,4)], 'p_init': 0.3},\n    ]\n\n    results = []\n    for case in test_cases:\n        p_hat = em_mle(case['epsilon'], case['reads'], case['p_init'])\n        results.append(p_hat)\n\n    # Format results to six decimal places for the final output.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}