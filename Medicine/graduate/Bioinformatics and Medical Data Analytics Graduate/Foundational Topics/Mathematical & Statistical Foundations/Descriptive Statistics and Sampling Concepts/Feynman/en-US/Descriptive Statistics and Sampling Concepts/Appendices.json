{
    "hands_on_practices": [
        {
            "introduction": "Before any analysis can begin, we must first obtain a representative sample of data. In the age of massive Electronic Health Record (EHR) systems, it is tempting to use computationally simple methods like systematic sampling. This exercise  serves as a critical cautionary tale, forcing us to reason about how hidden periodic patterns in time-ordered data, such as diurnal variations in clinical activity, can interact with a fixed sampling interval to produce a deceptively biased sample. Understanding this risk is a foundational skill for anyone working with large-scale longitudinal health data.",
            "id": "4555621",
            "problem": "A hospital system maintains Electronic Health Records (EHR) for $N$ inpatient admissions over $1$ calendar year, time-ordered by admission timestamp. For each admission $i \\in \\{1,\\dots,N\\}$, let $Y_i \\in \\{0,1\\}$ indicate whether an early sepsis panel was ordered within $6$ hours of admission. The goal is to estimate the finite-population mean $\\bar{Y}_N = N^{-1}\\sum_{i=1}^N Y_i$.\n\nA data team proposes a systematic sampling design to reduce data extraction costs: pick a fixed start position $r \\in \\{1,\\dots,k\\}$ anchored to the first admission after midnight on day $1$, and then extract every $k$-th record thereafter, yielding the sample index set $S(r,k) = \\{r, r+k, r+2k, \\dots\\} \\cap \\{1,\\dots,N\\}$ and estimator $\\hat{\\mu}_{\\text{sys}}(r,k) = |S(r,k)|^{-1} \\sum_{i \\in S(r,k)} Y_i$. They justify anchoring $r$ to a calendar time for operational reasons and do not randomize $r$.\n\nIt is scientifically plausible that $Y_i$ is not exchangeable over admission time: (i) sepsis suspicion and laboratory throughput often exhibit diurnal variation with period $T = 24$ hours, (ii) admission volume is higher during certain hours and days, and (iii) case-mix may vary systematically by time-of-day. Let $p$ denote the average number of admissions per $T$-hour cycle so that an index increment of approximately $p$ records corresponds to advancing by one diurnal cycle in calendar time.\n\nUsing only fundamental definitions of sampling designs and design-based bias, and without resorting to any canned formula, reason about when such a systematic design can produce a biased estimator of $\\bar{Y}_N$ due to periodicity in $\\{Y_i\\}$ or in covariates that determine $\\{Y_i\\}$. Then propose an empirically checkable diagnostic that could reveal or quantify this bias risk using the time-ordered data before committing to the design, or retrospectively using a pilot extraction.\n\nSelect all options that correctly specify both: (a) conditions under which periodicity induces bias in $\\hat{\\mu}_{\\text{sys}}(r,k)$ with a fixed start, and (b) a principled diagnostic that would reliably indicate the threat in realistic EHR data.\n\nA. If the conditional mean exhibits a nonconstant periodic component with period $p$ in record index, for example $E[Y_i]$ depends on $i \\bmod p$ due to $T=24$-hour cycles, and if the step $k$ shares a nontrivial common divisor with $p$ while the start $r$ is fixed to a calendar phase, then the sample traverses only a subset of the $p$ phases. Whenever the average of $E[Y_i]$ over that subset differs from the overall average, $\\hat{\\mu}_{\\text{sys}}(r,k)$ is biased. A diagnostic is to compute a periodogram or discrete Fourier transform of $\\{Y_i\\}$ (or of a pilot proxy such as hourly ordering rates) to test for a spectral peak near frequency $1/p$, and to evaluate the dispersion of $\\hat{\\mu}_{\\text{sys}}(r',k)$ across all $k$ possible starts $r' \\in \\{1,\\dots,k\\}$; marked variation across starts indicates aliasing with the periodic component.\n\nB. Bias is primarily driven by large step sizes $k$ regardless of periodic structure; if $k$ is large, $\\hat{\\mu}_{\\text{sys}}(r,k)$ is biased, and if $k$ is small, it is unbiased. A diagnostic is unnecessary if $k$ is chosen below a fixed threshold.\n\nC. Choosing a step $k$ that is coprime to the period $p$ guarantees that, even with a fixed start anchored to a specific calendar time and nonuniform admission intensities over the year, the estimator $\\hat{\\mu}_{\\text{sys}}(r,k)$ is unbiased for $\\bar{Y}_N$. Therefore, no additional diagnostic is required.\n\nD. A necessary and sufficient condition for bias is that the sample autocorrelation of $\\{Y_i\\}$ at lag $k$ is nonzero; thus computing the lag-$k$ autocorrelation and its significance test fully diagnoses the bias risk. If that autocorrelation is zero, no other periodic structure can induce bias.\n\nE. If there is time-of-day or day-of-week periodicity in either the outcome mechanism or case-mix, combined with a fixed-start systematic sample that induces unequal representation of calendar phases, bias occurs whenever the induced distribution of phase $\\phi(i)$ among sampled indices differs from its population distribution. This includes but is not limited to the case where $k$ and the effective period in records share a common divisor under approximately stationary arrivals; with nonstationary arrivals, unequal phase coverage can still arise even if $k$ and $p$ are coprime. A diagnostic is to model $E[Y_i]$ as a function of calendar phase via harmonic regression with $\\sin(2\\pi \\phi(i))$ and $\\cos(2\\pi \\phi(i))$, test for periodic effects, and then emulate the design by computing $\\hat{\\mu}_{\\text{sys}}(r',k)$ over multiple candidate starts $r'$ to assess sensitivity; additionally, compare the systematic estimate to a phase-stratified estimator that weights phase-specific means by their population frequencies.",
            "solution": "The user wants a critical validation of the provided problem statement, followed by a detailed solution and evaluation of the options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Population:** $N$ inpatient admissions over $1$ calendar year, time-ordered by admission timestamp.\n-   **Population units:** $i \\in \\{1, \\dots, N\\}$.\n-   **Outcome variable:** $Y_i \\in \\{0,1\\}$, indicating if an early sepsis panel was ordered within $6$ hours of admission.\n-   **Parameter of interest (Estimand):** The finite-population mean, $\\bar{Y}_N = N^{-1}\\sum_{i=1}^N Y_i$.\n-   **Sampling design:** Systematic sampling with a fixed, non-randomized start.\n    -   Start position: $r \\in \\{1, \\dots, k\\}$, anchored to the first admission after midnight on day $1$.\n    -   Sampling interval (step): $k$.\n    -   Sample index set: $S(r,k) = \\{r, r+k, r+2k, \\dots\\} \\cap \\{1, \\dots, N\\}$.\n-   **Estimator:** $\\hat{\\mu}_{\\text{sys}}(r,k) = |S(r,k)|^{-1} \\sum_{i \\in S(r,k)} Y_i$.\n-   **Domain knowledge:**\n    -   The sequence $\\{Y_i\\}$ is not exchangeable.\n    -   Diurnal variation (periodicity) with period $T = 24$ hours is plausible for the process generating $Y_i$.\n    -   Admission volume is non-uniform (non-stationary arrivals).\n    -   Case-mix varies by time-of-day.\n-   **Auxiliary definition:** $p$ is the average number of admissions per $T$-hour cycle. An index increment of $\\approx p$ corresponds to a calendar time advance of one $T=24$-hour cycle.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded:** The problem is firmly rooted in applied statistics, specifically sampling theory, and is set in a highly realistic clinical informatics context. The concepts of EHR data, systematic sampling for cost reduction, and diurnal physiological/operational patterns are all scientifically sound.\n-   **Well-Posed:** The problem defines a population, a parameter, a sampling design, and an estimator. It asks for the conditions under which the estimator is biased and for a method to diagnose this risk. This is a standard and well-defined question in sampling theory, particularly when considering superpopulation models to formalize concepts like periodicity.\n-   **Objective:** The language is formal, precise, and objective. It uses standard terminology from statistics and medicine.\n\n**Flaw Checklist:**\n1.  **Scientific or Factual Unsoundness:** None. The scenario is a classic case study for the risks of systematic sampling.\n2.  **Non-Formalizable or Irrelevant:** None. The problem is directly formalizable within sampling theory and is highly relevant to its stated topic.\n3.  **Incomplete or Contradictory Setup:** None. The problem provides all necessary information to reason about the nature of bias. The fact that the start $r$ is fixed and not randomized is the central constraint that makes the problem interesting.\n4.  **Unrealistic or Infeasible:** None. The scenario is highly realistic.\n5.  **Ill-Posed or Poorly Structured:** None. The question about bias can be rigorously answered by considering a superpopulation model, which is implicitly invited by the mention of periodicity in the data-generating process ($E[Y_i]$). This is the standard way to analyze the properties of a fixed-start systematic sample.\n6.  **Pseudo-Profound, Trivial, or Tautological:** None. The problem addresses a subtle but practically critical issue that requires substantive reasoning.\n7.  **Outside Scientific Verifiability:** None. The claims can be verified mathematically and through simulation.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. I will proceed with deriving a solution and evaluating the options.\n\n### Solution Derivation\n\nThe central issue is the potential for bias when using a fixed-start systematic sampling design on a population that has a periodic structure.\n\n**Formalizing Bias:**\nThe design-based bias of an estimator is $E_{\\text{design}}[\\hat{\\theta}] - \\theta$. In this problem, the sampling design has a fixed start $r$, meaning there is only one possible sample. The expectation over the design is thus just the value for that one sample: $E_{\\text{design}}[\\hat{\\mu}_{\\text{sys}}(r,k)] = \\hat{\\mu}_{\\text{sys}}(r,k)$. The design-based bias is $\\hat{\\mu}_{\\text{sys}}(r,k) - \\bar{Y}_N$. The estimator is biased unless, by chance, the sample mean equals the population mean.\n\nTo have a more useful concept of bias that relates to the underlying structure, we adopt a model-based perspective, as suggested by the problem's reference to periodicity in the process generating the $Y_i$. Let the finite population $\\{Y_i\\}_{i=1}^N$ be a realization of a stochastic process where $\\mu_i = E[Y_i]$. The model-bias of the estimator $\\hat{\\mu}_{\\text{sys}}(r,k)$ for the finite-population mean $\\bar{Y}_N$ is:\n$$ B = E_{\\text{model}}[\\hat{\\mu}_{\\text{sys}}(r,k) - \\bar{Y}_N] = E_{\\text{model}}[\\hat{\\mu}_{\\text{sys}}(r,k)] - E_{\\text{model}}[\\bar{Y}_N] $$\n$$ B = \\left( |S(r,k)|^{-1} \\sum_{i \\in S(r,k)} \\mu_i \\right) - \\left( N^{-1} \\sum_{i=1}^N \\mu_i \\right) $$\nBias arises if the average of the expected outcome values, $\\{\\mu_i\\}$, over the sample is different from the average of the expected outcome values over the population.\n\n**Conditions for Bias due to Periodicity:**\nThe problem posits that $\\mu_i$ may be periodic with a time period of $T=24$ hours, which corresponds to approximately $p$ records. This means $\\mu_i$ varies systematically depending on the \"phase\" of the record (e.g., time of day).\nIf the sampling interval $k$ and the period $p$ have a harmonic relationship, the sample may be unrepresentative. The most severe case is when $k$ is a multiple of $p$ or shares a common divisor with $p$. For example, if $p \\approx 100$ admissions/day and we sample every $k=100$ records, with a start time in the morning, we will only ever sample records from the morning. The sample will systematically miss all other phases (afternoon, evening, night). If the probability of a sepsis panel being ordered, $\\mu_i$, is different in the morning than at other times, then the sample average of the $\\mu_i$ will not equal the population average, and the estimator will be biased.\n\nMore generally, if $\\text{gcd}(k, p) = d > 1$, the systematic sample will only visit $p/d$ of the $p$ possible phases. With a fixed start $r$, we are locked into a specific subset of phases. If the average of $\\mu_i$ over this subset differs from the overall average of $\\mu_i$, bias will result.\n\nThe problem further complicates this by noting non-uniform admission volume (non-stationarity). This means the number of records per $T=24$ hours is not a constant $p$. Even if we choose $k$ to be coprime with the *average* period $\\bar{p}$, the varying density of records can cause the sample to over- or under-represent certain phases over the long run, leading to bias. The fundamental condition for bias is therefore an unequal distribution of phases in the sample compared to the population, regardless of the precise arithmetic cause.\n\n**Principled Diagnostics:**\nA robust diagnostic strategy should confirm the risk factors and quantify their potential impact.\n1.  **Detect Periodicity:** Before worrying about its interaction with sampling, one must confirm periodicity exists.\n    -   **Spectral Analysis:** Computing a periodogram or Discrete Fourier Transform (DFT) of the sequence $\\{Y_i\\}$ (or a time-aggregated version, like hourly means) can reveal strong periodic components. A peak at a frequency corresponding to a $24$-hour cycle (i.e., at frequency $f=1/p$) is a clear signal of risk.\n    -   **Regression Modeling:** One can model $\\mu_i = E[Y_i]$ as a function of the calendar phase $\\phi(i)$ (e.g., hour of day from $0$ to $23$). A harmonic regression model like $E[Y_i | \\phi(i)] = \\beta_0 + \\sum_j (\\beta_{cj} \\cos(j \\omega \\phi(i)) + \\beta_{sj} \\sin(j \\omega \\phi(i)))$, where $\\omega$ is the fundamental frequency, can be fitted. Statistical significance of the harmonic coefficients $(\\beta_{cj}, \\beta_{sj})$ confirms periodicity.\n2.  **Assess Design-Periodicity Interaction:**\n    -   **Emulation over Multiple Starts:** The core problem is the fixed start. We can assess its impact by simulating what would have happened with other start points. By calculating the estimate $\\hat{\\mu}_{\\text{sys}}(r',k)$ for all possible starts $r' \\in \\{1, \\dots, k\\}$, we generate $k$ different possible estimates. If these estimates show wide variation, it means the choice of start is highly influential, which happens precisely when the sampling interval $k$ is aliased with a periodic component. Large dispersion is a major red flag for bias.\n    -   **Comparison with a Robust Estimator:** A stratified sampling design, with strata defined by the calendar phases (e.g., $24$ strata for each hour of the day), is the textbook solution for handling such periodicity. One can compute the phase-stratified estimate $\\hat{\\mu}_{\\text{strat}} = \\sum_{h} W_h \\bar{Y}_h$, where $W_h$ is the population proportion of records in phase $h$ and $\\bar{Y}_h$ is the mean outcome in that phase. A significant difference between the systematic sample estimate $\\hat{\\mu}_{\\text{sys}}(r,k)$ and the stratified estimate $\\hat{\\mu}_{\\text{strat}}$ is a direct indication of bias.\n\n### Option-by-Option Analysis\n\n**A. If the conditional mean exhibits a nonconstant periodic component with period $p$ in record index, for example $E[Y_i]$ depends on $i \\bmod p$ due to $T=24$-hour cycles, and if the step $k$ shares a nontrivial common divisor with $p$ while the start $r$ is fixed to a calendar phase, then the sample traverses only a subset of the $p$ phases. Whenever the average of $E[Y_i]$ over that subset differs from the overall average, $\\hat{\\mu}_{\\text{sys}}(r,k)$ is biased. A diagnostic is to compute a periodogram or discrete Fourier transform of $\\{Y_i\\}$ (or of a pilot proxy such as hourly ordering rates) to test for a spectral peak near frequency $1/p$, and to evaluate the dispersion of $\\hat{\\mu}_{\\text{sys}}(r',k)$ across all $k$ possible starts $r' \\in \\{1,\\dots,k\\}$; marked variation across starts indicates aliasing with the periodic component.**\n-   **Analysis:** The description of the condition for bias is correct; it accurately describes the aliasing mechanism in the case of stationary periodicity ($\\text{gcd}(k,p)>1$). The proposed diagnostics are both principled and standard for this problem: spectral analysis (periodogram/DFT) to detect periodicity and assessing the dispersion of estimates over all possible starts to evaluate the impact of the fixed-start design.\n-   **Verdict:** Correct.\n\n**B. Bias is primarily driven by large step sizes $k$ regardless of periodic structure; if $k$ is large, $\\hat{\\mu}_{\\text{sys}}(r,k)$ is biased, and if $k$ is small, it is unbiased. A diagnostic is unnecessary if $k$ is chosen below a fixed threshold.**\n-   **Analysis:** This statement is fundamentally incorrect. Bias is driven by the arithmetic relationship between $k$ and any periodicities, not by the magnitude of $k$ alone. A small $k$ can be highly biasing (e.g., if period $p=24$ and $k=12$), while a very large $k$ might not be if it's not in phase with any periodicity. The claim that a diagnostic is unnecessary is false and reckless.\n-   **Verdict:** Incorrect.\n\n**C. Choosing a step $k$ that is coprime to the period $p$ guarantees that, even with a fixed start anchored to a specific calendar time and nonuniform admission intensities over the year, the estimator $\\hat{\\mu}_{\\text{sys}}(r,k)$ is unbiased for $\\bar{Y}_N$. Therefore, no additional diagnostic is required.**\n-   **Analysis:** The claim of a \"guarantee\" is too strong and incorrect. While choosing $k$ to be coprime to $p$ is a good heuristic in stationary cases, the problem explicitly mentions non-uniform admission intensities (non-stationarity). This variation in the number of records per cycle means that even a coprime $k$ does not guarantee proportional representation of all calendar phases. Bias can still occur. Consequently, a diagnostic is still necessary.\n-   **Verdict:** Incorrect.\n\n**D. A necessary and sufficient condition for bias is that the sample autocorrelation of $\\{Y_i\\}$ at lag $k$ is nonzero; thus computing the lag-$k$ autocorrelation and its significance test fully diagnoses the bias risk. If that autocorrelation is zero, no other periodic structure can induce bias.**\n-   **Analysis:** This is an oversimplification. The potential for bias in a systematic sample is related to the entire pattern of autocorrelations at lags that are multiples of $k$ (i.e., $\\text{lags } k, 2k, 3k, \\dots$), not just at lag $k$. A zero autocorrelation at lag $k$ does not rule out a strong positive autocorrelation at lag $2k$, which would also create a highly unrepresentative sample. Therefore, checking only the lag-$k$ autocorrelation is an insufficient diagnostic. The condition is neither necessary nor sufficient.\n-   **Verdict:** Incorrect.\n\n**E. If there is time-of-day or day-of-week periodicity in either the outcome mechanism or case-mix, combined with a fixed-start systematic sample that induces unequal representation of calendar phases, bias occurs whenever the induced distribution of phase $\\phi(i)$ among sampled indices differs from its population distribution. This includes but is not limited to the case where $k$ and the effective period in records share a common divisor under approximately stationary arrivals; with nonstationary arrivals, unequal phase coverage can still arise even if $k$ and $p$ are coprime. A diagnostic is to model $E[Y_i]$ as a function of calendar phase via harmonic regression with $\\sin(2\\pi \\phi(i))$ and $\\cos(2\\pi \\phi(i))$, test for periodic effects, and then emulate the design by computing $\\hat{\\mu}_{\\text{sys}}(r',k)$ over multiple candidate starts $r'$ to assess sensitivity; additionally, compare the systematic estimate to a phase-stratified estimator that weights phase-specific means by their population frequencies.**\n-   **Analysis:** This option provides the most complete and accurate analysis. Its description of the bias condition is fundamentalâ€”it correctly identifies \"unequal representation of calendar phases\" as the root cause. It correctly points out that the simpler arithmetic rule involving $\\text{gcd}(k,p)$ is a special case and that non-stationarity invalidates simple guarantees. The proposed diagnostics are comprehensive and robust: harmonic regression to model periodicity, emulation over multiple starts to assess sensitivity (same as in A), and comparison to a phase-stratified estimator (a textbook robust alternative). This combination represents a thorough approach to diagnosing the problem.\n-   **Verdict:** Correct.\n\nBoth options A and E are correct. They both identify a valid condition for bias and propose principled diagnostics. Option E is more general and comprehensive in its description and diagnostics, but option A is also correct in its statements. The question asks to select all correct options.",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "Once a sample is collected, the next step is to describe its characteristics. However, biomedical data are notoriously prone to noise and extreme values, or outliers, which can dramatically skew common summary statistics like the sample mean. This practice  provides a hands-on opportunity to explore the concept of robust statistics by calculating a trimmed mean and comparing its stability against the sample mean in the presence of a plausible outlier. By quantifying the sensitivity of each estimator, you will gain a concrete understanding of why robust methods are essential for reliable data analysis.",
            "id": "4555567",
            "problem": "A bioinformatics team is analyzing interleukin-6 (IL-6) cytokine concentrations measured in picograms per milliliter using Enzyme-Linked Immunosorbent Assay (ELISA). To assess robustness of summary statistics to outliers in medical data analytics, consider the following simulated IL-6 concentrations from $20$ patient samples:\n$\\{4, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 400\\}$.\n\nYour tasks are:\n- Using only first principles of descriptive statistics, compute the sample mean and the $20$ percent trimmed mean of the given dataset. Interpret trimming as discarding the lowest and highest $k$ ordered values, where $k = \\lfloor 0.2 n \\rfloor$ for a dataset of size $n$, and averaging the remaining values.\n- To quantify sensitivity to an outlier, define the outlier sensitivity index for a statistic $T$ on dataset $X$ as follows. Let $X'$ be the dataset obtained by deleting the single largest observation from $X$. Define $\\Delta_{T} = |T(X) - T(X')|$. Compute the ratio\n$$S = \\frac{\\Delta_{\\bar{X}}}{\\Delta_{T_{0.2}}},$$\nwhere $\\bar{X}$ is the sample mean and $T_{0.2}$ is the $20$ percent trimmed mean, each computed independently on its respective dataset size with its own trimming parameter $k = \\lfloor 0.2 n \\rfloor$.\n\nProvide your final numeric answer as an exact rational number (no rounding). The final answer to submit is the single value of $S$ (unitless).",
            "solution": "The user-provided problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Dataset $X$**: $\\{4, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 400\\}$\n- **Dataset size $n$**: $20$\n- **Task 1**: Compute the sample mean ($\\bar{X}$) and the $20\\%$ trimmed mean ($T_{0.2}$) of $X$.\n- **Definition of Trimming**: Discard the lowest and highest $k$ ordered values, where $k = \\lfloor 0.2 n \\rfloor$ for a dataset of size $n$, and average the remaining values.\n- **Task 2**: Compute the outlier sensitivity ratio $S$.\n- **Definition of $X'$**: The dataset obtained by deleting the single largest observation from $X$.\n- **Definition of Sensitivity Index $\\Delta_T$**: $\\Delta_{T} = |T(X) - T(X')|$, where $T$ is a statistic.\n- **Definition of Ratio $S$**: $S = \\frac{\\Delta_{\\bar{X}}}{\\Delta_{T_{0.2}}}$, where $\\bar{X}$ is the sample mean and $T_{0.2}$ is the $20\\%$ trimmed mean.\n- **Calculation Constraint**: For calculations on $X'$, the statistics ($\\bar{X}$ and $T_{0.2}$) are computed independently on the new dataset size $n'$ and its corresponding trimming parameter $k' = \\lfloor 0.2 n' \\rfloor$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is grounded in the established principles of descriptive statistics. The scenario of analyzing cytokine concentrations with outliers is a standard and realistic problem in medical data analytics and bioinformatics. The concepts of sample mean, trimmed mean, and sensitivity analysis are fundamental to robust statistics.\n2.  **Well-Posed**: All terms are explicitly and unambiguously defined. The dataset is provided, and the computational steps are clearly specified. The rule for re-calculating the trimming parameter for the reduced dataset $X'$ ensures there is no ambiguity, leading to a unique solution.\n3.  **Objective**: The problem is stated using precise, quantitative language, free from subjective or opinion-based claims.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is scientifically sound, well-posed, objective, and contains all necessary information for a unique solution. The solution process may now proceed.\n\n### Solution Derivation\n\nThe initial dataset is $X = \\{4, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 400\\}$. The dataset is already sorted in ascending order. The size of the dataset is $n=20$.\n\n**1. Calculate Statistics for the Full Dataset $X$**\n\nFirst, we compute the sample mean of $X$, denoted $\\bar{X}(X)$.\nThe sum of all observations in $X$ is:\n$$ \\sum_{i=1}^{n} x_i = (4+5+7+9+10+11+12+13+14+15+16+17+18+19+20+21+22+23+24) + 400 $$\nThe sum of the first $19$ observations is $280$.\n$$ \\sum_{i=1}^{n} x_i = 280 + 400 = 680 $$\nThe sample mean is:\n$$ \\bar{X}(X) = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{680}{20} = 34 $$\n\nNext, we compute the $20\\%$ trimmed mean of $X$, denoted $T_{0.2}(X)$.\nThe number of observations to trim from each end is $k = \\lfloor 0.2 \\times n \\rfloor = \\lfloor 0.2 \\times 20 \\rfloor = \\lfloor 4 \\rfloor = 4$.\nWe discard the $4$ smallest values $\\{4, 5, 7, 9\\}$ and the $4$ largest values $\\{22, 23, 24, 400\\}$.\nThe remaining dataset for the trimmed mean consists of $n - 2k = 20 - 2(4) = 12$ observations:\n$$ X_{T} = \\{10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21\\} $$\nThe sum of these values, which form an arithmetic progression, is:\n$$ \\sum_{x \\in X_T} x = \\frac{12}{2} (10 + 21) = 6 \\times 31 = 186 $$\nThe $20\\%$ trimmed mean is:\n$$ T_{0.2}(X) = \\frac{186}{12} = \\frac{31}{2} $$\n\n**2. Calculate Statistics for the Reduced Dataset $X'$**\n\nThe dataset $X'$ is formed by removing the single largest observation ($400$) from $X$.\n$$ X' = \\{4, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24\\} $$\nThe size of this new dataset is $n' = 19$.\n\nFirst, we compute the sample mean of $X'$, denoted $\\bar{X}(X')$.\nThe sum of observations in $X'$ is the sum of $X$ minus the removed outlier:\n$$ \\sum_{x \\in X'} x = 680 - 400 = 280 $$\nThe sample mean of $X'$ is:\n$$ \\bar{X}(X') = \\frac{280}{19} $$\n\nNext, we compute the $20\\%$ trimmed mean of $X'$, denoted $T_{0.2}(X')$.\nThe number of observations to trim is recalculated based on $n'=19$:\n$$ k' = \\lfloor 0.2 \\times n' \\rfloor = \\lfloor 0.2 \\times 19 \\rfloor = \\lfloor 3.8 \\rfloor = 3 $$\nWe discard the $3$ smallest values $\\{4, 5, 7\\}$ and the $3$ largest values $\\{22, 23, 24\\}$ from $X'$.\nThe remaining dataset for the trimmed mean consists of $n' - 2k' = 19 - 2(3) = 13$ observations:\n$$ X'_{T} = \\{9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21\\} $$\nThe sum of these values, which form an arithmetic progression, is:\n$$ \\sum_{x \\in X'_{T}} x = \\frac{13}{2} (9 + 21) = \\frac{13}{2} (30) = 13 \\times 15 = 195 $$\nThe $20\\%$ trimmed mean of $X'$ is:\n$$ T_{0.2}(X') = \\frac{195}{13} = 15 $$\n\n**3. Compute the Sensitivity Indices and the Final Ratio $S$**\n\nThe sensitivity index for the sample mean, $\\Delta_{\\bar{X}}$, is:\n$$ \\Delta_{\\bar{X}} = |\\bar{X}(X) - \\bar{X}(X')| = \\left|34 - \\frac{280}{19}\\right| = \\left|\\frac{34 \\times 19}{19} - \\frac{280}{19}\\right| = \\left|\\frac{646 - 280}{19}\\right| = \\frac{366}{19} $$\n\nThe sensitivity index for the trimmed mean, $\\Delta_{T_{0.2}}$, is:\n$$ \\Delta_{T_{0.2}} = |T_{0.2}(X) - T_{0.2}(X')| = \\left|\\frac{31}{2} - 15\\right| = \\left|\\frac{31}{2} - \\frac{30}{2}\\right| = \\frac{1}{2} $$\n\nFinally, we compute the ratio $S$:\n$$ S = \\frac{\\Delta_{\\bar{X}}}{\\Delta_{T_{0.2}}} = \\frac{\\frac{366}{19}}{\\frac{1}{2}} = \\frac{366}{19} \\times 2 = \\frac{732}{19} $$\nThe number $19$ is prime, and $732$ is not a multiple of $19$ ($732 = 38 \\times 19 + 10$). Thus, the fraction is irreducible.",
            "answer": "$$\n\\boxed{\\frac{732}{19}}\n$$"
        },
        {
            "introduction": "Moving from simple description to formal inference is a key step in translating data into knowledge. A common task is to estimate a population proportion, such as the prevalence of a biomarker, and quantify our uncertainty using a confidence interval. This exercise  challenges us to look inside the black box of statistical formulas, deriving and comparing the widely used Wald interval with the more theoretically sound Wilson score interval. By working through the mechanics, especially in a realistic small-sample scenario, you will appreciate why the choice of statistical method matters profoundly for generating reliable scientific conclusions.",
            "id": "4555572",
            "problem": "A translational oncology team is piloting a binary immunohistochemistry assay to quantify the proportion of tumor biopsies that are positive for a cell-surface biomarker linked to targeted therapy response. In the pilot, there are $n = 12$ independent biopsies, among which $x = 2$ are positive. Let $p$ denote the true probability of biomarker positivity for the target population.\n\nStarting from the binomial sampling model and the definition of a two-sided confidence interval (CI) as an inversion of a test, derive the Wilson score CI for $p$ at a two-sided confidence level of $95$ using the standard normal quantile $z_{0.975}$, and also derive the Wald CI for $p$ based on the asymptotic normal approximation. Assume no continuity correction and use $z_{0.975} = 1.96$.\n\nDefine the length of an interval as $L = U - L$, where $U$ and $L$ are the upper and lower endpoints, respectively. Compute the ratio\n$$\nR \\;=\\; \\frac{L_{\\text{Wald}}}{L_{\\text{Wilson}}}\n$$\nfor the observed data ($n = 12$, $x = 2$). Round your final numerical answer for $R$ to four significant figures.",
            "solution": "The problem requires the derivation of two types of confidence intervals (CIs) for a binomial proportion, $p$, and the subsequent calculation of the ratio of their lengths for a given dataset. The two CIs are the Wald interval and the Wilson score interval.\n\nLet $n$ be the number of independent Bernoulli trials and $x$ be the number of successes. The point estimate for the true proportion $p$ is the sample proportion $\\hat{p} = x/n$. The number of successes $X$ is assumed to follow a binomial distribution, $X \\sim \\text{Binomial}(n,p)$. For a sufficiently large sample size $n$, the sampling distribution of $\\hat{p}$ can be approximated by a normal distribution according to the Central Limit Theorem: $\\hat{p} \\approx \\mathcal{N}\\left(p, \\frac{p(1-p)}{n}\\right)$.\n\nThe confidence level is specified as $95\\%$, which corresponds to a significance level of $\\alpha = 0.05$. The quantile from the standard normal distribution is given as $z_{1-\\alpha/2} = z_{0.975} = 1.96$.\n\n**1. Derivation and Formula for the Wald Confidence Interval**\n\nThe Wald CI is derived from the normal approximation of the distribution of $\\hat{p}$. It approximates the standard error of $\\hat{p}$, which is $SE = \\sqrt{p(1-p)/n}$, by substituting the sample estimate $\\hat{p}$ for the unknown parameter $p$. This gives the estimated standard error, $\\widehat{SE}(\\hat{p}) = \\sqrt{\\hat{p}(1-\\hat{p})/n}$.\n\nThe CI is constructed from the pivotal quantity $Z = (\\hat{p}-p)/\\widehat{SE}(\\hat{p})$, which is assumed to follow a standard normal distribution. The $100(1-\\alpha)\\%$ CI is the range of $p$ values satisfying:\n$$\n-z_{1-\\alpha/2} \\le \\frac{\\hat{p}-p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} \\le z_{1-\\alpha/2}\n$$\nSolving for $p$ yields the interval:\n$$\n\\hat{p} \\pm z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n$$\nThe lower ($L$) and upper ($U$) endpoints are $L_{\\text{Wald}} = \\hat{p} - z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$ and $U_{\\text{Wald}} = \\hat{p} + z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$. The length of the Wald interval is the difference between the upper and lower endpoints:\n$$\nL_{\\text{Wald}} = U_{\\text{Wald}} - L_{\\text{Wald}} = 2 z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n$$\n\n**2. Derivation and Formula for the Wilson Score Confidence Interval**\n\nThe Wilson score CI is derived by inverting the score test for the proportion $p$. The score test statistic uses the null-hypothesized value $p_0$ to calculate the standard error, rather than the sample estimate $\\hat{p}$. The test statistic is:\n$$\nZ = \\frac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\n$$\nThe confidence interval for $p$ includes all values $p_0$ for which the null hypothesis $H_0: p = p_0$ is not rejected, i.e., for which $|Z| \\le z_{1-\\alpha/2}$.\n$$\n\\left| \\frac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}} \\right| \\le z_{1-\\alpha/2}\n$$\nSquaring both sides and rearranging leads to a quadratic inequality in $p_0$:\n$$\n(\\hat{p}-p_0)^2 \\le z_{1-\\alpha/2}^2 \\left( \\frac{p_0(1-p_0)}{n} \\right)\n$$\n$$\nn(\\hat{p}^2 - 2\\hat{p}p_0 + p_0^2) \\le z_{1-\\alpha/2}^2 (p_0 - p_0^2)\n$$\n$$\n(n + z_{1-\\alpha/2}^2)p_0^2 - (2n\\hat{p} + z_{1-\\alpha/2}^2)p_0 + n\\hat{p}^2 \\le 0\n$$\nThe endpoints of the CI are the roots of the quadratic equation $(n + z^2)p^2 - (2n\\hat{p} + z^2)p + n\\hat{p}^2 = 0$, where $z = z_{1-\\alpha/2}$. Using the quadratic formula $p = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$, the roots are:\n$$\np = \\frac{(2n\\hat{p} + z^2) \\pm \\sqrt{(2n\\hat{p} + z^2)^2 - 4(n + z^2)(n\\hat{p}^2)}}{2(n + z^2)}\n$$\nSimplifying the discriminant:\n$$\n\\sqrt{4n^2\\hat{p}^2 + 4n\\hat{p}z^2 + z^4 - 4n^2\\hat{p}^2 - 4n\\hat{p}^2z^2} = \\sqrt{z^2(4n\\hat{p}(1-\\hat{p}) + z^2)} = z\\sqrt{4n\\hat{p}(1-\\hat{p}) + z^2}\n$$\nThe endpoints of the Wilson score interval are:\n$$\np = \\frac{2n\\hat{p} + z^2 \\pm z\\sqrt{4n\\hat{p}(1-\\hat{p}) + z^2}}{2(n + z^2)}\n$$\nThe length of the Wilson interval is the difference between these endpoints:\n$$\nL_{\\text{Wilson}} = \\frac{2z\\sqrt{4n\\hat{p}(1-\\hat{p}) + z^2}}{2(n + z^2)} = \\frac{z\\sqrt{4n\\hat{p}(1-\\hat{p}) + z^2}}{n + z^2}\n$$\n\n**3. Calculation of the Ratio of Lengths**\n\nGiven data: $n = 12$, $x = 2$, and $z_{0.975} = 1.96$.\nThe sample proportion is $\\hat{p} = \\frac{x}{n} = \\frac{2}{12} = \\frac{1}{6}$.\n\nFirst, compute the length of the Wald interval, $L_{\\text{Wald}}$:\n$$\nL_{\\text{Wald}} = 2(1.96)\\sqrt{\\frac{(1/6)(1 - 1/6)}{12}} = 3.92\\sqrt{\\frac{(1/6)(5/6)}{12}} = 3.92\\sqrt{\\frac{5/36}{12}} = 3.92\\sqrt{\\frac{5}{432}}\n$$\nNumerically, $L_{\\text{Wald}} \\approx 3.92 \\times 0.10758286 = 0.4217255...$\n\nNext, compute the length of the Wilson score interval, $L_{\\text{Wilson}}$. Let $z = 1.96$:\n$$\nL_{\\text{Wilson}} = \\frac{z\\sqrt{4n\\hat{p}(1-\\hat{p}) + z^2}}{n + z^2}\n$$\nCalculate the components:\n$z^2 = 1.96^2 = 3.8416$.\n$4n\\hat{p}(1-\\hat{p}) = 4(12)(\\frac{1}{6})(\\frac{5}{6}) = 48(\\frac{5}{36}) = \\frac{4}{3} \\times 5 = \\frac{20}{3}$.\nThe denominator is $n + z^2 = 12 + 3.8416 = 15.8416$.\nThe numerator is $z\\sqrt{\\frac{20}{3} + 3.8416} = 1.96\\sqrt{6.6666... + 3.8416} = 1.96\\sqrt{10.508266...}$.\nSo, $L_{\\text{Wilson}} = \\frac{1.96 \\times 3.2416456...}{15.8416} = \\frac{6.353625...}{15.8416} \\approx 0.4010728...$\n\nFinally, compute the ratio $R$:\n$$\nR = \\frac{L_{\\text{Wald}}}{L_{\\text{Wilson}}} = \\frac{0.4217255...}{0.4010728...} \\approx 1.051493...\n$$\nRounding this result to four significant figures yields $1.051$.",
            "answer": "$$\\boxed{1.051}$$"
        }
    ]
}