{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Receiver Operating Characteristic (ROC) analysis, it is essential to build a curve from the ground up. This first exercise provides a direct, hands-on opportunity to do just that. By working with a small, concrete set of classifier scores, you will manually calculate the True Positive Rate ($ \\mathrm{TPR} $) and False Positive Rate ($ \\mathrm{FPR} $) at each potential decision threshold, plot the resulting points, and compute the Area Under the Curve (AUC) . This foundational process demystifies the ROC curve, revealing it as a straightforward summary of a classifier's performance across its entire operating range.",
            "id": "4918235",
            "problem": "A binary diagnostic task in medical imaging is evaluated using Receiver Operating Characteristic (ROC) analysis. A classifier produces continuous confidence scores in the interval $[0,1]$ that reflect the likelihood of disease. For a threshold $\\tau$, the decision rule is: declare an image positive (diseased) if its score is $\\ge \\tau$ and negative (nondiseased) otherwise. The True Positive Rate ($\\mathrm{TPR}$, sensitivity) at threshold $\\tau$ is defined as the fraction of diseased cases declared positive, and the False Positive Rate ($\\mathrm{FPR}$, one minus specificity) is the fraction of nondiseased cases declared positive.\n\nYou are given three diseased-case scores $\\{0.3,\\,0.6,\\,0.9\\}$ and four nondiseased-case scores $\\{0.1,\\,0.2,\\,0.4,\\,0.8\\}$. Using the empirical definitions above:\n\n1) Construct the empirical ROC curve by enumerating the unique thresholds $\\tau$ drawn from the union of the given scores, augmented with endpoint conventions $\\tau>\\max$ (no cases positive) and $\\tau\\le\\min$ (all cases positive). For each threshold, compute the corresponding point $(\\mathrm{FPR},\\,\\mathrm{TPR})$.\n\n2) From the empirical ROC curve, compute the Area Under the Curve (AUC) by summing the areas of the trapezoids formed between successive points along the $\\mathrm{FPR}$ axis.\n\nExpress the final AUC as a single exact fraction. No units are required.",
            "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It represents a standard application of Receiver Operating Characteristic (ROC) analysis in evaluating a binary classifier. All necessary data and definitions are provided and are internally consistent. We may therefore proceed with the solution.\n\nThe problem requires the construction of an empirical ROC curve and the calculation of the Area Under this Curve (AUC) for a given set of classifier scores.\n\nFirst, we establish the given information. The set of scores for diseased cases is $S_D = \\{0.3, 0.6, 0.9\\}$, and the number of diseased cases is $N_D = 3$. The set of scores for nondiseased cases is $S_{ND} = \\{0.1, 0.2, 0.4, 0.8\\}$, and the number of nondiseased cases is $N_{ND} = 4$.\n\nA decision threshold $\\tau$ is applied to the scores. A case is classified as positive if its score is greater than or equal to $\\tau$.\nThe True Positive Rate, $\\mathrm{TPR}(\\tau)$, is the fraction of diseased cases with scores $\\ge \\tau$:\n$$ \\mathrm{TPR}(\\tau) = \\frac{|\\{s \\in S_D \\mid s \\ge \\tau\\}|}{N_D} $$\nThe False Positive Rate, $\\mathrm{FPR}(\\tau)$, is the fraction of nondiseased cases with scores $\\ge \\tau$:\n$$ \\mathrm{FPR}(\\tau) = \\frac{|\\{s \\in S_{ND} \\mid s \\ge \\tau\\}|}{N_{ND}} $$\nThe ROC curve is a plot of $\\mathrm{TPR}$ versus $\\mathrm{FPR}$ for all possible thresholds $\\tau$.\n\nPart 1: Construct the empirical ROC curve.\nThe vertices of the empirical ROC curve are generated by considering thresholds at the values of the given scores. A standard method is to sort all scores in descending order and move the threshold down past each score. This generates a path on the $\\mathrm{FPR}$-$\\mathrm{TPR}$ plane.\nThe combined set of scores, sorted in descending order with their class (D for diseased, ND for nondiseased), is:\n$0.9(\\text{D}), 0.8(\\text{ND}), 0.6(\\text{D}), 0.4(\\text{ND}), 0.3(\\text{D}), 0.2(\\text{ND}), 0.1(\\text{ND})$\n\nThe curve starts at the point $(0, 0)$, which corresponds to a threshold $\\tau > 0.9$ where no cases are classified as positive. As the threshold is lowered, for each diseased case encountered, the curve moves up by a step of size $\\frac{1}{N_D} = \\frac{1}{3}$. For each nondiseased case encountered, the curve moves to the right by a step of size $\\frac{1}{N_{ND}} = \\frac{1}{4}$.\n\nLet the ordered points on the ROC curve be $(x_i, y_i)$, where $x_i = \\mathrm{FPR}$ and $y_i = \\mathrm{TPR}$.\n1.  Start at point $p_0 = (x_0, y_0) = (0, 0)$.\n2.  The highest score is $0.9$, which is from a diseased case. The curve moves up. $p_1 = (0, 0 + \\frac{1}{3}) = (0, \\frac{1}{3})$.\n3.  The next score is $0.8$, from a nondiseased case. The curve moves right. $p_2 = (0 + \\frac{1}{4}, \\frac{1}{3}) = (\\frac{1}{4}, \\frac{1}{3})$.\n4.  The next score is $0.6$, from a diseased case. The curve moves up. $p_3 = (\\frac{1}{4}, \\frac{1}{3} + \\frac{1}{3}) = (\\frac{1}{4}, \\frac{2}{3})$.\n5.  The next score is $0.4$, from a nondiseased case. The curve moves right. $p_4 = (\\frac{1}{4} + \\frac{1}{4}, \\frac{2}{3}) = (\\frac{1}{2}, \\frac{2}{3})$.\n6.  The next score is $0.3$, from a diseased case. The curve moves up. $p_5 = (\\frac{1}{2}, \\frac{2}{3} + \\frac{1}{3}) = (\\frac{1}{2}, 1)$.\n7.  The next score is $0.2$, from a nondiseased case. The curve moves right. $p_6 = (\\frac{1}{2} + \\frac{1}{4}, 1) = (\\frac{3}{4}, 1)$.\n8.  The final score is $0.1$, from a nondiseased case. The curve moves right. $p_7 = (\\frac{3}{4} + \\frac{1}{4}, 1) = (1, 1)$.\n\nThe sequence of vertices for the empirical ROC curve is:\n$p_0 = (0, 0)$\n$p_1 = (0, \\frac{1}{3})$\n$p_2 = (\\frac{1}{4}, \\frac{1}{3})$\n$p_3 = (\\frac{1}{4}, \\frac{2}{3})$\n$p_4 = (\\frac{1}{2}, \\frac{2}{3})$\n$p_5 = (\\frac{1}{2}, 1)$\n$p_6 = (\\frac{3}{4}, 1)$\n$p_7 = (1, 1)$\n\nPart 2: Compute the Area Under the Curve (AUC).\nThe AUC is calculated by summing the areas of the trapezoids formed by consecutive points $(x_{i-1}, y_{i-1})$ and $(x_i, y_i)$ and the $\\mathrm{FPR}$ axis. The area of each trapezoid is given by the formula:\n$$ A_i = (x_i - x_{i-1}) \\times \\frac{y_i + y_{i-1}}{2} $$\nThe total AUC is the sum of these areas:\n$$ \\mathrm{AUC} = \\sum_{i=1}^{7} A_i = \\sum_{i=1}^{7} (x_i - x_{i-1}) \\times \\frac{y_i + y_{i-1}}{2} $$\nWe compute the area for each segment of the curve:\n-   Area between $p_0$ and $p_1$: $A_1 = (0 - 0) \\times \\frac{\\frac{1}{3} + 0}{2} = 0$.\n-   Area between $p_1$ and $p_2$: $A_2 = (\\frac{1}{4} - 0) \\times \\frac{\\frac{1}{3} + \\frac{1}{3}}{2} = \\frac{1}{4} \\times \\frac{1}{3} = \\frac{1}{12}$.\n-   Area between $p_2$ and $p_3$: $A_3 = (\\frac{1}{4} - \\frac{1}{4}) \\times \\frac{\\frac{2}{3} + \\frac{1}{3}}{2} = 0$.\n-   Area between $p_3$ and $p_4$: $A_4 = (\\frac{1}{2} - \\frac{1}{4}) \\times \\frac{\\frac{2}{3} + \\frac{2}{3}}{2} = \\frac{1}{4} \\times \\frac{2}{3} = \\frac{2}{12} = \\frac{1}{6}$.\n-   Area between $p_4$ and $p_5$: $A_5 = (\\frac{1}{2} - \\frac{1}{2}) \\times \\frac{1 + \\frac{2}{3}}{2} = 0$.\n-   Area between $p_5$ and $p_6$: $A_6 = (\\frac{3}{4} - \\frac{1}{2}) \\times \\frac{1 + 1}{2} = \\frac{1}{4} \\times 1 = \\frac{1}{4}$.\n-   Area between $p_6$ and $p_7$: $A_7 = (1 - \\frac{3}{4}) \\times \\frac{1 + 1}{2} = \\frac{1}{4} \\times 1 = \\frac{1}{4}$.\n\nThe total AUC is the sum of these individual areas:\n$$ \\mathrm{AUC} = 0 + \\frac{1}{12} + 0 + \\frac{1}{6} + 0 + \\frac{1}{4} + \\frac{1}{4} $$\nTo sum these fractions, we find a common denominator, which is $12$:\n$$ \\mathrm{AUC} = \\frac{1}{12} + \\frac{2}{12} + \\frac{3}{12} + \\frac{3}{12} = \\frac{1+2+3+3}{12} = \\frac{9}{12} $$\nSimplifying the fraction gives the final result:\n$$ \\mathrm{AUC} = \\frac{3}{4} $$\nThis result can be verified using the Wilcoxon-Mann-Whitney U statistic, which states that the AUC is the probability that a randomly selected diseased case will have a higher score than a randomly selected nondiseased case. The number of pairs of one diseased and one nondiseased case is $N_D \\times N_{ND} = 3 \\times 4 = 12$. The number of pairs where the diseased score is higher is found to be $9$. Thus, the AUC is $\\frac{9}{12} = \\frac{3}{4}$, confirming our calculation.",
            "answer": "$$\\boxed{\\frac{3}{4}}$$"
        },
        {
            "introduction": "The Area Under the Curve (AUC) is more than just a single number; it holds two powerful and equivalent interpretations that are crucial for a deep understanding of classifier performance. This practice challenges you to explore this duality by calculating the AUC in two distinct ways: first, as the geometric area under the empirically constructed ROC curve, and second, as the probabilistic measure of rank-order correctness . By implementing both methods and verifying their identical output, you will solidify your grasp of the AUC as both a graphical summary and the probability that the classifier correctly ranks a random positive-negative pair.",
            "id": "4604304",
            "problem": "You are given several binary-labeled datasets consisting of real-valued scores from a classifier applied to biological samples. For each dataset, denote the labels by a vector $y \\in \\{0,1\\}^n$ with $y_i = 1$ for a positive sample and $y_i = 0$ for a negative sample, and denote the corresponding real-valued scores by a vector $s \\in \\mathbb{R}^n$. Let $P$ be the number of positives and $N$ be the number of negatives, with $P \\ge 1$ and $N \\ge 1$. Your task is to compute the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) in two ways and compare them.\n\nFundamental definitions to use:\n- Receiver Operating Characteristic (ROC) curve plots the False Positive Rate (FPR) on the horizontal axis versus the True Positive Rate (TPR) on the vertical axis as a discrimination threshold moves over the score range.\n- True Positive Rate (TPR) is defined as $\\mathrm{TPR} = \\mathrm{TP}/P$, where $\\mathrm{TP}$ is the number of true positives at a given threshold.\n- False Positive Rate (FPR) is defined as $\\mathrm{FPR} = \\mathrm{FP}/N$, where $\\mathrm{FP}$ is the number of false positives at a given threshold.\n- Area Under the Curve (AUC) is the area under the ROC curve.\n\nCompute the AUC as follows for each dataset:\n1) Empirical ROC with trapezoidal rule:\n   - Sweep a threshold from $+\\infty$ down to $-\\infty$. At each threshold, predict positive if $s_i \\ge \\tau$. Construct the empirical ROC by:\n     - Initializing at $(\\mathrm{FPR}, \\mathrm{TPR}) = (0,0)$.\n     - Processing distinct score levels in strictly decreasing order. At each distinct score value, simultaneously include all samples having that score (this correctly handles ties). Update $\\mathrm{TP}$ and $\\mathrm{FP}$, then compute the new $(\\mathrm{FPR}, \\mathrm{TPR})$ point and append it to the ROC polyline.\n     - The ROC ends at $(1,1)$ once all samples are included.\n   - Compute the AUC by applying the trapezoidal rule over the piecewise-linear ROC in the $\\mathrm{FPR}$–$\\mathrm{TPR}$ plane:\n     $$\\mathrm{AUC}_{\\text{trap}} = \\sum_{k=1}^{K} \\frac{\\mathrm{TPR}_{k} + \\mathrm{TPR}_{k-1}}{2}\\left(\\mathrm{FPR}_{k} - \\mathrm{FPR}_{k-1}\\right),$$\n     where the ROC points are indexed in order of increasing $\\mathrm{FPR}$, starting from $(\\mathrm{FPR}_0,\\mathrm{TPR}_0)=(0,0)$.\n2) Pairwise rank formulation:\n   - For every positive–negative pair $(i,j)$ with $y_i = 1$ and $y_j = 0$, compare their scores $s_i$ and $s_j$ and define a contribution of $1$ if $s_i > s_j$, a contribution of $0$ if $s_i < s_j$, and a contribution of $1/2$ if $s_i = s_j$.\n   - Define the pairwise AUC as the average contribution over all $P \\cdot N$ such pairs. Denote this by $\\mathrm{AUC}_{\\text{rank}}$.\n\nYour program must, for each dataset in the test suite below, compute:\n- $\\mathrm{AUC}_{\\text{trap}}$ via the empirical ROC and trapezoidal rule as described,\n- $\\mathrm{AUC}_{\\text{rank}}$ via the pairwise definition above,\n- The absolute difference $\\lvert \\mathrm{AUC}_{\\text{trap}} - \\mathrm{AUC}_{\\text{rank}} \\rvert$.\n\nAll three values for each dataset must be returned as floating-point numbers rounded to $6$ decimal places.\n\nTest suite:\n- Dataset $1$: $y = [\\,1,0,1,0,1,0,0,1,0,1\\,]$, $s = [\\,0.92,0.85,0.85,0.85,0.60,0.60,0.55,0.55,0.40,0.40\\,]$.\n- Dataset $2$: $y = [\\,1,1,1,0,0,0\\,]$, $s = [\\,0.90,0.80,0.70,0.60,0.20,0.10\\,]$.\n- Dataset $3$: $y = [\\,1,1,0,0\\,]$, $s = [\\,0.10,0.20,0.80,0.90\\,]$.\n- Dataset $4$: $y = [\\,1,1,0,0,0\\,]$, $s = [\\,0.50,0.50,0.50,0.50,0.50\\,]$.\n- Dataset $5$: $y = [\\,1,0,0,0\\,]$, $s = [\\,0.50,0.10,0.50,0.90\\,]$.\n- Dataset $6$: $y = [\\,1,1,1,0\\,]$, $s = [\\,0.20,0.50,0.80,0.50\\,]$.\n\nFinal output format:\n- Your program should produce a single line of output containing a single list that aggregates the results for all datasets in order. For each dataset, append three values in this order: $\\mathrm{AUC}_{\\text{trap}}$, $\\mathrm{AUC}_{\\text{rank}}$, and the absolute difference. Concatenate all datasets’ triples into one flat list. The final line must be a single comma-separated list enclosed in square brackets, with no spaces, for example $[\\text{v}_1,\\text{v}_2,\\ldots,\\text{v}_{3M}]$ for $M$ datasets.\n- All floats must be rounded to $6$ decimal places before printing.",
            "solution": "The problem requires the computation of the Area Under the Receiver Operating Characteristic Curve (AUC) for several datasets using two distinct, prescribed methods. The core scientific principle underlying this problem is the mathematical equivalence between the geometric interpretation of the AUC (as the area under a curve) and its probabilistic interpretation (related to the Mann-Whitney U statistic). We will first validate this principle by implementing both algorithms as specified and then comparing their outputs.\n\nA Receiver Operating Characteristic (ROC) curve is a graphical plot illustrating the diagnostic ability of a binary classifier system as its discrimination threshold is varied. It is created by plotting the True Positive Rate ($\\mathrm{TPR}$) against the False Positive Rate ($\\mathrm{FPR}$) at various threshold settings.\n- The True Positive Rate is given by $\\mathrm{TPR} = \\mathrm{TP}/P$, where $\\mathrm{TP}$ is the number of true positives and $P$ is the total number of positive samples.\n- The False Positive Rate is given by $\\mathrm{FPR} = \\mathrm{FP}/N$, where $\\mathrm{FP}$ is the number of false positives and $N$ is the total number of negative samples.\n\nThe AUC provides an aggregate measure of performance across all possible classification thresholds. An AUC of $1.0$ represents a perfect classifier, while an AUC of $0.5$ represents a classifier with no discriminative ability, equivalent to random guessing.\n\n### Method 1: Empirical ROC with Trapezoidal Rule ($\\mathrm{AUC}_{\\text{trap}}$)\n\nThis method constructs the ROC curve geometrically and then calculates the area underneath it. The algorithm proceeds as follows:\n\n1.  **Preparation**: Given the label vector $y$ and score vector $s$, we first determine the total counts of positive ($P$) and negative ($N$) samples.\n2.  **Thresholding and Point Generation**: The ROC curve is constructed by sweeping a decision threshold $\\tau$ from $+\\infty$ to $-\\infty$. A sample $i$ is classified as positive if its score $s_i \\ge \\tau$.\n    - We begin at the point $(\\mathrm{FPR}, \\mathrm{TPR}) = (0,0)$, which corresponds to a threshold $\\tau = +\\infty$ where no samples are classified as positive.\n    - The vertices of the ROC curve are generated by considering each unique score value present in the data, processed in strictly decreasing order.\n    - At each unique score value, we consider all samples that have this score. The change in the number of true positives ($\\Delta \\mathrm{TP}$) and false positives ($\\Delta \\mathrm{FP}$) is determined by the labels of these samples.\n    - The cumulative counts of true positives ($\\mathrm{TP}$) and false positives ($\\mathrm{FP}$) are updated.\n    - A new point on the ROC curve, $(\\mathrm{FPR}, \\mathrm{TPR}) = (\\mathrm{FP}/N, \\mathrm{TP}/P)$, is then recorded. This handling of ties ensures that a block of samples with the same score corresponds to a single diagonal segment on the ROC curve.\n    - The process continues until all samples have been included, at which point the curve terminates at $(\\mathrm{FPR}, \\mathrm{TPR}) = (1,1)$.\n3.  **Area Calculation**: The sequence of generated points defines a piecewise-linear path from $(0,0)$ to $(1,1)$. The area under this path, $\\mathrm{AUC}_{\\text{trap}}$, is calculated by applying the trapezoidal rule. For a sequence of ROC points $(\\mathrm{FPR}_k, \\mathrm{TPR}_k)$ indexed by increasing $\\mathrm{FPR}$:\n    $$\n    \\mathrm{AUC}_{\\text{trap}} = \\sum_{k=1}^{K} \\frac{\\mathrm{TPR}_{k} + \\mathrm{TPR}_{k-1}}{2}\\left(\\mathrm{FPR}_{k} - \\mathrm{FPR}_{k-1}\\right)\n    $$\n    where $(\\mathrm{FPR}_0, \\mathrm{TPR}_0) = (0,0)$. Each term in the sum represents the area of a single trapezoid formed by two consecutive points on the ROC curve and their projections onto the FPR axis.\n\n### Method 2: Pairwise Rank Formulation ($\\mathrm{AUC}_{\\text{rank}}$)\n\nThis method is based on the probabilistic interpretation of AUC. The AUC is equivalent to the probability that a randomly chosen positive sample is ranked higher than a randomly chosen negative sample. This can be calculated directly by examining all positive-negative pairs.\n\n1.  **Pairwise Comparison**: We form all possible pairs of samples $(i, j)$ such that sample $i$ is positive ($y_i=1$) and sample $j$ is negative ($y_j=0$). There are $P \\cdot N$ such pairs.\n2.  **Scoring**: Each pair is scored based on the classifier's output scores, $s_i$ and $s_j$:\n    - The pair contributes $1$ if the positive sample has a higher score ($s_i > s_j$).\n    - The pair contributes $0.5$ if the scores are tied ($s_i = s_j$).\n    - The pair contributes $0$ if the positive sample has a lower score ($s_i < s_j$).\n3.  **Averaging**: The value of $\\mathrm{AUC}_{\\text{rank}}$ is the sum of all these contributions, divided by the total number of pairs, $P \\cdot N$. This gives the average contribution, which is the empirical probability of correct ranking.\n    $$\n    \\mathrm{AUC}_{\\text{rank}} = \\frac{1}{P \\cdot N} \\sum_{i: y_i=1} \\sum_{j: y_j=0} \\mathbf{I}(s_i, s_j)\n    $$\n    where $\\mathbf{I}(s_i, s_j)$ is the scoring function defined above.\n\n### Equivalence and Implementation\n\nThe two formulations, $\\mathrm{AUC}_{\\text{trap}}$ and $\\mathrm{AUC}_{\\text{rank}}$, are mathematically equivalent. The problem requires implementing both distinct algorithms to demonstrate this equivalence numerically. The absolute difference $|\\mathrm{AUC}_{\\text{trap}} - \\mathrm{AUC}_{\\text{rank}}|$ is expected to be zero, or a very small number attributable to floating-point precision errors. Our implementation will compute these two values, along with their absolute difference, for each test dataset. All final numerical results will be rounded to six decimal places as required.",
            "answer": "```python\nimport numpy as np\n\ndef calculate_auc_trap(y, s):\n    \"\"\"\n    Computes AUC using the empirical ROC and trapezoidal rule.\n    \"\"\"\n    y = np.array(y)\n    s = np.array(s)\n\n    # Ensure there are both positive and negative samples\n    pos_mask = (y == 1)\n    neg_mask = (y == 0)\n    P = np.sum(pos_mask)\n    N = np.sum(neg_mask)\n    \n    if P == 0 or N == 0:\n        # According to the problem statement, P>=1 and N>=1.\n        # This case should not be reached with valid inputs.\n        return np.nan\n\n    # Get unique scores and sort them in descending order\n    unique_scores = np.unique(s)[::-1]\n    \n    # Initialize ROC curve points and counters\n    roc_points = [(0.0, 0.0)]\n    tp, fp = 0, 0\n    \n    # Process each distinct score level\n    for thresh in unique_scores:\n        # Find all samples with the current score\n        mask = (s == thresh)\n        \n        # Count positives and negatives at this score\n        delta_tp = np.sum(pos_mask[mask])\n        delta_fp = np.sum(neg_mask[mask])\n        \n        # Update cumulative counts\n        tp += delta_tp\n        fp += delta_fp\n        \n        # Calculate TPR and FPR and add a new point to the ROC curve\n        tpr = tp / P\n        fpr = fp / N\n        roc_points.append((fpr, tpr))\n        \n    # Calculate AUC using the trapezoidal rule\n    auc = 0.0\n    for i in range(1, len(roc_points)):\n        fpr_i, tpr_i = roc_points[i]\n        fpr_im1, tpr_im1 = roc_points[i-1]\n        \n        # Area of the trapezoid between point i-1 and i\n        auc += (tpr_i + tpr_im1) * (fpr_i - fpr_im1) / 2.0\n            \n    return auc\n\ndef calculate_auc_rank(y, s):\n    \"\"\"\n    Computes AUC using the pairwise rank formulation.\n    \"\"\"\n    y = np.array(y)\n    s = np.array(s)\n\n    # Separate scores for positive and negative samples\n    pos_scores = s[y == 1]\n    neg_scores = s[y == 0]\n    \n    P = len(pos_scores)\n    N = len(neg_scores)\n    \n    if P == 0 or N == 0:\n        # According to the problem statement, P>=1 and N>=1.\n        # This case should not be reached with valid inputs.\n        return np.nan\n        \n    # Sum contributions from all positive-negative pairs\n    rank_sum = 0.0\n    for s_pos in pos_scores:\n        for s_neg in neg_scores:\n            if s_pos > s_neg:\n                rank_sum += 1.0\n            elif s_pos == s_neg:\n                rank_sum += 0.5\n    \n    # The AUC is the average contribution\n    auc = rank_sum / (P * N)\n    return auc\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        (\n            [1, 0, 1, 0, 1, 0, 0, 1, 0, 1],\n            [0.92, 0.85, 0.85, 0.85, 0.60, 0.60, 0.55, 0.55, 0.40, 0.40]\n        ),\n        (\n            [1, 1, 1, 0, 0, 0],\n            [0.90, 0.80, 0.70, 0.60, 0.20, 0.10]\n        ),\n        (\n            [1, 1, 0, 0],\n            [0.10, 0.20, 0.80, 0.90]\n        ),\n        (\n            [1, 1, 0, 0, 0],\n            [0.50, 0.50, 0.50, 0.50, 0.50]\n        ),\n        (\n            [1, 0, 0, 0],\n            [0.50, 0.10, 0.50, 0.90]\n        ),\n        (\n            [1, 1, 1, 0],\n            [0.20, 0.50, 0.80, 0.50]\n        )\n    ]\n\n    all_results = []\n    \n    for y_data, s_data in test_cases:\n        # Compute AUC using both methods\n        auc_trap = calculate_auc_trap(y_data, s_data)\n        auc_rank = calculate_auc_rank(y_data, s_data)\n        \n        # Compute the absolute difference\n        diff = abs(auc_trap - auc_rank)\n        \n        # Append the formatted results to the list\n        all_results.append(\"{:.6f}\".format(auc_trap))\n        all_results.append(\"{:.6f}\".format(auc_rank))\n        all_results.append(\"{:.6f}\".format(diff))\n\n    # Print the final list in the specified format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While empirical ROC curves describe the performance on a specific dataset, theoretical models allow us to understand how a classifier might behave under idealized conditions. This exercise introduces the binormal model, a common framework where classifier scores for each class are assumed to follow a normal distribution. You will derive the analytical expression for the AUC and discover how it is a function of the means and variances of these distributions . This practice bridges the gap between empirical data and statistical theory, offering profound insights into how class separation ($ \\mu_{1} - \\mu_{0} $) and data variability ($ \\sigma_{0}, \\sigma_{1} $) fundamentally determine a classifier's discriminatory power.",
            "id": "4604288",
            "problem": "A proteomics-based diagnostic classifier outputs a continuous risk score for each patient. In a cohort study, the risk scores for patients with the disease and without the disease are modeled as independent Gaussian random variables: the diseased group score $S_{1}$ follows a normal distribution with mean $\\mu_{1}$ and variance $\\sigma_{1}^{2}$, and the non-diseased group score $S_{0}$ follows a normal distribution with mean $\\mu_{0}$ and variance $\\sigma_{0}^{2}$. The Receiver Operating Characteristic (ROC) curve is constructed by varying a decision threshold on the score, and the Area Under the Curve (AUC) is used to summarize discrimination. Starting only from the fundamental definitions of the ROC and the AUC, derive the general expression for the AUC under the heteroscedastic binormal model with $\\sigma_{1}\\neq\\sigma_{0}$. Then, using $\\mu_{0}=0$, $\\mu_{1}=1.7$, $\\sigma_{0}=0.8$, and $\\sigma_{1}=1.5$, compute the AUC. Round your final numerical answer to four significant figures and express it as a decimal. Finally, compare the derived expression to the equal-variance case in which both classes have the same variance chosen so that the total variance $\\sigma_{1}^{2}+\\sigma_{0}^{2}$ is preserved, and explain qualitatively how variance disparity influences discrimination in terms of the curve shape and the AUC. The final answer must be the computed AUC only.",
            "solution": "The problem asks for the derivation of the Area Under the Curve (AUC) for a heteroscedastic binormal model, a specific numerical calculation of the AUC, and a qualitative comparison with an equal-variance model.\n\nFirst, we validate the problem.\n**Step 1: Extract Givens**\n- The score for the diseased group, $S_{1}$, is a random variable following a normal distribution $N(\\mu_{1}, \\sigma_{1}^{2})$.\n- The score for the non-diseased group, $S_{0}$, is a random variable following a normal distribution $N(\\mu_{0}, \\sigma_{0}^{2})$.\n- $S_{1}$ and $S_{0}$ are independent.\n- The model is heteroscedastic, meaning $\\sigma_{1} \\neq \\sigma_{0}$.\n- For the numerical calculation, the parameters are: $\\mu_{0}=0$, $\\mu_{1}=1.7$, $\\sigma_{0}=0.8$, and $\\sigma_{1}=1.5$.\n- The final numerical answer must be rounded to four significant figures.\n- A qualitative comparison is required with an equal-variance case where the total variance $\\sigma_{1}^{2}+\\sigma_{0}^{2}$ is preserved.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as the binormal model is a standard theoretical framework for Receiver Operating Characteristic (ROC) analysis. The problem is well-posed, providing all necessary information for a unique solution. The language is objective and precise. The problem statement does not violate any fundamental scientific principles, is not based on false premises, and contains no contradictions or ambiguities. It is a formalizable statistical problem.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\n**Derivation of the AUC Expression**\n\nThe ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) for a continuum of decision thresholds. Let the decision threshold be $c$. A subject is classified as diseased if their score $S \\ge c$.\n\nThe TPR, or sensitivity, is the probability of correctly classifying a diseased subject:\n$$ TPR(c) = P(S_{1} \\ge c) $$\nThe FPR, or $1 - \\text{specificity}$, is the probability of incorrectly classifying a non-diseased subject:\n$$ FPR(c) = P(S_{0} \\ge c) $$\nThe Area Under the ROC Curve (AUC) is given by the integral of the ROC curve, $TPR(FPR)$, over the range of $FPR$ from $0$ to $1$:\n$$ AUC = \\int_{0}^{1} TPR(FPR) \\, d(FPR) $$\nThis integral can be evaluated by parameterizing both $TPR$ and $FPR$ by the threshold $c$. Note that as $c$ goes from $-\\infty$ to $+\\infty$, $FPR(c)$ goes from $1$ to $0$. Let $F_{0}(c) = P(S_0 \\le c)$ be the cumulative distribution function (CDF) of $S_0$, and $f_{0}(c)$ be its probability density function (PDF). Then $FPR(c) = 1 - F_{0}(c)$, and $d(FPR) = -f_{0}(c) dc$. Similarly, $TPR(c) = 1 - F_{1}(c)$, where $F_{1}(c)$ is the CDF of $S_{1}$.\nSubstituting these into the integral for AUC:\n$$ AUC = \\int_{c=+\\infty}^{c=-\\infty} [1 - F_{1}(c)] (-f_{0}(c) \\, dc) = \\int_{-\\infty}^{+\\infty} [1 - F_{1}(c)] f_{0}(c) \\, dc $$\nThe term $[1 - F_{1}(c)]$ is $P(S_{1} > c)$. The integral represents the expectation of $P(S_{1} > S_{0})$ where the outer expectation is over the distribution of $S_{0}$. This confirms the well-known, equivalent definition of the AUC as the probability that a randomly selected diseased subject will have a score greater than a randomly selected non-diseased subject.\n$$ AUC = P(S_{1} > S_{0}) $$\nTo compute this probability, we define a new random variable $D = S_{1} - S_{0}$. Since $S_{1}$ and $S_{0}$ are independent normal random variables, their difference $D$ is also a normal random variable.\nThe mean of $D$ is:\n$$ E[D] = E[S_{1} - S_{0}] = E[S_{1}] - E[S_{0}] = \\mu_{1} - \\mu_{0} $$\nThe variance of $D$ is, due to independence:\n$$ Var(D) = Var(S_{1} - S_{0}) = Var(S_{1}) + (-1)^{2}Var(S_{0}) = \\sigma_{1}^{2} + \\sigma_{0}^{2} $$\nSo, the distribution of $D$ is $N(\\mu_{1} - \\mu_{0}, \\sigma_{1}^{2} + \\sigma_{0}^{2})$.\nThe AUC is the probability $P(D > 0)$. To calculate this, we standardize the variable $D$:\n$$ P(D > 0) = P\\left(\\frac{D - (\\mu_{1} - \\mu_{0})}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{0}^{2}}} > \\frac{0 - (\\mu_{1} - \\mu_{0})}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{0}^{2}}}\\right) $$\nLet $Z = \\frac{D - (\\mu_{1} - \\mu_{0})}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{0}^{2}}}$, which is a standard normal variable, $Z \\sim N(0, 1)$.\nThe inequality becomes:\n$$ P(D > 0) = P\\left(Z > -\\frac{\\mu_{1} - \\mu_{0}}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{0}^{2}}}\\right) $$\nUsing the symmetry of the standard normal distribution, $P(Z > -z) = P(Z < z) = \\Phi(z)$, where $\\Phi(z)$ is the CDF of the standard normal distribution.\nTherefore, the general expression for the AUC under the heteroscedastic binormal model is:\n$$ AUC = \\Phi\\left(\\frac{\\mu_{1} - \\mu_{0}}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{0}^{2}}}\\right) $$\n\n**Numerical Calculation**\n\nWe are given the parameters: $\\mu_{0}=0$, $\\mu_{1}=1.7$, $\\sigma_{0}=0.8$, and $\\sigma_{1}=1.5$.\nWe substitute these values into the derived expression.\nFirst, calculate the argument of the function $\\Phi$:\n$$ z = \\frac{\\mu_{1} - \\mu_{0}}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{0}^{2}}} = \\frac{1.7 - 0}{\\sqrt{(1.5)^{2} + (0.8)^{2}}} $$\nThe variances are $\\sigma_{1}^{2} = (1.5)^{2} = 2.25$ and $\\sigma_{0}^{2} = (0.8)^{2} = 0.64$.\nThe sum of variances is $\\sigma_{1}^{2} + \\sigma_{0}^{2} = 2.25 + 0.64 = 2.89$.\nThe standard deviation of the difference is $\\sqrt{2.89} = 1.7$.\nSubstituting this back into the expression for $z$:\n$$ z = \\frac{1.7}{1.7} = 1 $$\nSo, the AUC is given by $\\Phi(1)$.\nThe value of $\\Phi(1)$ is the area under the standard normal curve to the left of $z=1$. This is a standard value, approximately $0.8413447...$.\nThe problem requires rounding to four significant figures.\n$$ AUC \\approx 0.8413 $$\n\n**Qualitative Comparison**\n\nThe derived expression for the AUC is $AUC = \\Phi\\left(\\frac{\\mu_{1} - \\mu_{0}}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{0}^{2}}}\\right)$.\nLet's consider the equal-variance (homoscedastic) case where the total variance $\\sigma_{1}^{2}+\\sigma_{0}^{2}$ is preserved. Let the new common variance be $\\sigma^{2}$. The condition is $2\\sigma^{2} = \\sigma_{1}^{2} + \\sigma_{0}^{2}$.\nThe AUC for this homoscedastic model, $AUC_{eq}$, would be:\n$$ AUC_{eq} = \\Phi\\left(\\frac{\\mu_{1} - \\mu_{0}}{\\sqrt{\\sigma^{2} + \\sigma^{2}}}\\right) = \\Phi\\left(\\frac{\\mu_{1} - \\mu_{0}}{\\sqrt{2\\sigma^{2}}}\\right) $$\nSince $2\\sigma^{2} = \\sigma_{1}^{2} + \\sigma_{0}^{2}$, the argument of $\\Phi$ is identical to the heteroscedastic case.\n$$ AUC_{eq} = \\Phi\\left(\\frac{\\mu_{1} - \\mu_{0}}{\\sqrt{\\sigma_{1}^{2} + \\sigma_{0}^{2}}}\\right) = AUC $$\nThis demonstrates that for a fixed mean separation $(\\mu_{1} - \\mu_{0})$ and a fixed total variance $(\\sigma_{1}^{2} + \\sigma_{0}^{2})$, the AUC is independent of how that variance is distributed between the two classes (i.e., the degree of heteroscedasticity).\n\nHowever, the variance disparity significantly influences the *shape* of the ROC curve. The ROC curve is defined by $y(x) = TPR(FPR^{-1}(x))$. For the binormal model, this is $y(x) = \\Phi\\left(\\frac{\\mu_1 - \\mu_0}{\\sigma_1} + \\frac{\\sigma_0}{\\sigma_1} \\Phi^{-1}(x)\\right)$.\n- **Equal-variance case ($\\sigma_{1}=\\sigma_{0}$):** The term $\\frac{\\sigma_{0}}{\\sigma_{1}}$ is $1$. The resulting ROC curve is symmetric with respect to the anti-diagonal line from $(0,1)$ to $(1,0)$. This means that the trade-off between gaining sensitivity and losing specificity is symmetric across the curve.\n- **Unequal-variance case ($\\sigma_{1} \\neq \\sigma_{0}$):** The curve is asymmetric. In our problem, $\\sigma_{1} = 1.5 > \\sigma_{0} = 0.8$. The diseased distribution is wider than the non-diseased distribution. This causes the ROC curve to be asymmetric, bowing more towards the top-left corner near the point $(0,1)$ compared to the symmetric curve. This shape indicates that at high thresholds (low FPR), it is difficult to capture the long tail of the wide diseased distribution, so the TPR rises relatively slowly. At lower thresholds, the bulk of the diseased distribution is captured more rapidly. The choice of an optimal operating point on the curve, which depends on the relative costs of misclassification, is therefore influenced by the variance disparity, even though the overall AUC is not.\n\nIn summary, variance disparity does not alter the total discriminative power as measured by the AUC (given fixed mean separation and total variance), but it changes the local trade-offs between sensitivity and specificity at different decision thresholds, which is reflected in the asymmetric shape of the ROC curve.",
            "answer": "$$\\boxed{0.8413}$$"
        }
    ]
}