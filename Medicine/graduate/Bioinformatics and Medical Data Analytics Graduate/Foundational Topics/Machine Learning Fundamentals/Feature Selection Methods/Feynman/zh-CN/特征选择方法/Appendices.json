{
    "hands_on_practices": [
        {
            "introduction": "在典型的基因组学研究中，我们可能会同时检验数以万计的特征（例如基因表达量）。在这种大规模筛选中，传统的显著性阈值（如 $p \\lt 0.05$）会因多重检验问题而导致大量假阳性发现。本练习将引导您从第一性原理出发，推导并应用两种核心的错误率控制方法——族系错误率（FWER）和错误发现率（FDR），从而深刻理解在探索性分析中精确度与发现能力之间的权衡。",
            "id": "4563535",
            "problem": "一个研究团队正在一个基因组学流程中，在训练疾病分类器之前，执行一个基于过滤的特征选择阶段。他们针对一个二元疾病表型，测试了 $m = 20{,}000$ 个转录本特征的关联性，每个特征获得一个独立的 $p$-值。他们希望在 $\\alpha = 0.05$ 的水平下，根据两种不同的误差控制机制设定一个固定的 $p$-值截断值：\n\n1. 族系误差率（FWER），定义为在所有 $m$ 个检验中，至少犯一个假阳性错误的概率。\n\n2. 错误发现率（FDR），定义为在所有被拒绝的假设中，假阳性所占的期望比例。\n\n从第一性原理出发，仅使用用于控制FWER的联合界、FDR的定义，以及已知在原假设为真时，$p$-值在随机性上不小于 $[0,1]$ 上的均匀分布这一性质，推导：\n\n- 将FWER控制在水平 $\\alpha$ 的 Bonferroni 单特征显著性阈值。\n\n- 在 $p$-值相互独立的条件下，将FDR控制在水平 $\\alpha$ 的 Benjamini–Hochberg (BH) 升阶临界值函数 $t(k)$，其中 $k \\in \\{1,2,\\ldots,m\\}$ 是按升序排列的 $p$-值的秩。\n\n计算当 $m = 20{,}000$ 和 $\\alpha = 0.05$ 时这些量的值。将你的最终答案表示为一个单行矩阵，其第一个元素是 Bonferroni 单特征阈值，第二个元素是 BH 临界值函数 $t(k)$ 作为秩指数 $k$ 的函数的封闭形式表达式。无需四舍五入。\n\n最后，请用文字论证，在规模化组学特征选择的背景下，控制FWER和控制FDR之间的权衡，以及当后续目标是预测性能而非验证性推断时，为什么从业者可能更倾向于选择其中一种。你的论证必须基于上述定义和相应阈值的行为，不得引用未经证实的启发式方法。",
            "solution": "该问题已被验证为具有科学依据、提法明确且客观。它基于统计学和生物信息学中多重假设检验的既定原则。所有必要的定义和数据均已提供，问题没有矛盾或含糊之处。\n\n令 $m = 20{,}000$ 为特征总数，对应于 $m$ 个独立的假设检验。令 $\\alpha = 0.05$ 为误差控制的期望显著性水平。对于每个假设 $H_i$，$i \\in \\{1, 2, \\ldots, m\\}$，我们有一个对应的 $p$-值，$p_i$。如果一个假设 $H_i$ 的 $p$-值 $p_i$ 小于或等于某个阈值，则该假设被拒绝。\n\n### 1. 族系误差率 (FWER) 与 Bonferroni 校正\n\n族系误差率（FWER）定义为在所有 $m$ 个检验中，犯下至少一个 I 型错误（假阳性）的概率。令 $I_0$ 为对应于真原假设的索引集合，令 $m_0 = |I_0|$ 为真原假设的数量。如果假设 $H_i$ 为真（$i \\in I_0$）但被拒绝，则发生了 I 型错误。令此事件为 $E_i$。\n\n那么FWER为 $P(\\cup_{i \\in I_0} E_i)$。该问题要求使用联合界（也称为布尔不等式）来控制FWER。联合界指出，对于任何事件集合，其并集的概率不大于它们各自概率的总和。\n$$\n\\text{FWER} = P(\\cup_{i \\in I_0} E_i) \\le \\sum_{i \\in I_0} P(E_i)\n$$\n我们采用一个单一、固定的 $p$-值截断值，称之为 $t_{bonf}$。如果 $p_i \\le t_{bonf}$，则假设 $H_i$ 被拒绝。因此，事件 $E_i$ 是 $\\{p_i \\le t_{bonf}\\}$。在原假设为真的情况下，$p$-值 $p_i$ 在随机性上不小于 $[0,1]$ 上的一个均匀随机变量。对于标准的连续检验统计量，$p_i \\sim U(0,1)$。因此，对于任何 $i \\in I_0$，发生 I 型错误的概率是 $P(p_i \\le t_{bonf}) = t_{bonf}$。\n\n将此代入联合界不等式：\n$$\n\\text{FWER} \\le \\sum_{i \\in I_0} t_{bonf} = m_0 t_{bonf}\n$$\n由于真原假设的数量 $m_0$ 是未知的，我们采用最保守的情况，即所有原假设都可能为真，即 $m_0 \\le m$。这给出：\n$$\n\\text{FWER} \\le m_0 t_{bonf} \\le m t_{bonf}\n$$\n为了将FWER控制在水平 $\\alpha$，我们强制要求这个上界不大于 $\\alpha$：\n$$\nm t_{bonf} \\le \\alpha\n$$\n求解单特征显著性阈值 $t_{bonf}$，得到 Bonferroni 校正：\n$$\nt_{bonf} = \\frac{\\alpha}{m}\n$$\n对于给定的值 $m = 20{,}000$ 和 $\\alpha = 0.05$：\n$$\nt_{bonf} = \\frac{0.05}{20{,}000} = \\frac{5 \\times 10^{-2}}{2 \\times 10^4} = 2.5 \\times 10^{-6}\n$$\n\n### 2. 错误发现率 (FDR) 与 Benjamini-Hochberg (BH) 程序\n\n错误发现率（FDR）定义为在所有被拒绝的假设（发现）中，假阳性所占的期望比例。令 $R$ 为被拒绝的假设总数，$V$ 为假阳性的数量（被拒绝的真原假设）。FDR 为 $E\\left[\\frac{V}{R}\\right]$（如果 $R=0$，则该分数为 $0$）。\n\nBenjamini-Hochberg (BH) 程序提供了一种比 FWER 不那么严格的控制，这对于探索性分析通常更为有效。我们被要求从第一性原理推导其临界值函数 $t(k)$。\n\n核心思想是找到一个自适应阈值。我们寻找一个阈值 $t$，使得我们对FDR的估计被控制在水平 $\\alpha$。对于给定的阈值 $t$，发现的数量为 $R(t) = \\sum_{i=1}^m \\mathbb{I}(p_i \\le t)$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数。假阳性的期望数量为 $E[V(t)] = E[\\sum_{i \\in I_0} \\mathbb{I}(p_i \\le t)] = \\sum_{i \\in I_0} P(p_i \\le t) = m_0 t$。\n\n对于给定的阈值 $t$，一个简单但非形式严谨的 FDR 插入式估计量是 $\\widehat{\\text{FDR}}(t) = \\frac{E[V(t)]}{R(t)} = \\frac{m_0 t}{R(t)}$。由于 $m_0$ 未知，我们可以使用保守上界 $m_0 \\le m$，这产生了条件：\n$$\n\\frac{m t}{R(t)} \\le \\alpha \\implies t \\le \\frac{R(t)}{m} \\alpha\n$$\n这个不等式必须对我们的拒绝阈值 $t$ 成立。然而，阈值 $t$ 本身决定了拒绝的数量 $R(t)$。这种循环性表明需要一个搜索过程。BH 程序提供了一种系统化的方式来执行此搜索。\n\n让我们将 $p$-值按升序排列：$p_{(1)} \\le p_{(2)} \\le \\ldots \\le p_{(m)}$。\n假设我们考虑将第 $k$ 个有序 $p$-值 $p_{(k)}$ 作为我们的潜在阈值。如果我们将阈值设为 $t = p_{(k)}$，那么我们将恰好做出 $k$ 次拒绝。所以，$R(t) = R(p_{(k)}) = k$。\n将 $t=p_{(k)}$ 和 $R(t)=k$ 代入上述条件，得到：\n$$\np_{(k)} \\le \\frac{k}{m} \\alpha\n$$\n这为每个排好序的 $p$-值提供了一个条件。BH 程序对所有 $k \\in \\{1, 2, \\ldots, m\\}$ 检查此条件。为了最大化发现的数量（即获得最高的功效），我们找到满足此条件的最大 $k$。令其为 $k_{max} = \\max\\{k : p_{(k)} \\le \\frac{k\\alpha}{m}\\}$。然后，所有 $p$-值小于等于 $p_{(k_{max})}$ 的假设都被拒绝。\n\n从这个推导中，用于与第 $k$ 个排好序的 $p$-值 $p_{(k)}$ 进行比较的临界值为：\n$$\nt(k) = \\frac{k \\alpha}{m}\n$$\n这就是 Benjamini-Hochberg 升阶临界值函数。对于给定的值，它变为：\n$$\nt(k) = \\frac{k \\times 0.05}{20{,}000} = (2.5 \\times 10^{-6}) k\n$$\n\n### 关于预测性能权衡的论证\n\n控制 FWER 还是 FDR 的选择，关键取决于科学目标。FWER 适用于验证性研究，在这种研究中，单个错误声明的代价很高。FDR 适用于探索性研究，例如用于预测的特征选择，其目标是为下游任务生成一组有希望的候选特征。\n\n用于控制 FWER 的 Bonferroni 阈值是 $t_{bonf} = \\alpha/m = 2.5 \\times 10^{-6}$。这是一个单一的、极其严格的、不随数据自适应的阈值。在典型的大规模组学研究中，真实效应可能不大，这种高显著性门槛通常导致很少或没有特征被选中。这对应于高假阴性率（真正具有预测性的特征被错过），这对于构建一个强大的预测模型是有害的。建立在贫乏特征集上的模型很可能会欠拟合，并且预测准确性差。\n\n用于控制 FDR 的 BH 临界值是 $t(k) = k\\alpha/m$。这些值形成一个递增的斜坡：$t(1) = \\alpha/m$，$t(2) = 2\\alpha/m$，以此类推。任何在 Bonferroni 校正下被选中的特征（即 $p_{(1)} \\le \\alpha/m$）也同样会被 BH 方法选中。然而，BH 允许选择更多的特征，因为对于不那么显著的 $p$-值，阈值变得更加宽松。例如，排名第 100 的特征是根据阈值 $100\\alpha/m$ 进行检验的，这个阈值比 Bonferroni 阈值大 100 倍。\n\n这种更宽松的 FDR 控制方法允许将更多的真阳性包含在所选特征集中。虽然这也增加了假阳性的数量，但 FDR 框架保证了在所有选定特征中，这些“噪声”特征的*期望比例*被控制在水平 $\\alpha$。对于预测任务而言，这是一个非常理想的权衡。许多现代机器学习算法（例如，L1 正则化回归、随机森林）对于包含有限数量的非信息性特征具有鲁棒性；它们有内部机制可以为这些特征分配较低的重要性或零权重。包含少数伪特征所造成的损害，通常远小于排除大量真正具有预测性特征所造成的损害。\n\n因此，当目标是预测性能而非验证性推断时，从业者会更倾向于控制 FDR 而非 FWER。FDR 优先考虑更高的功效（灵敏度）来发现潜在的预测性特征，同时接受一个受控比例的错误发现，这种权衡与构建鲁棒且准确的预测模型的目标非常吻合。而 FWER 的极端保守性（优先考虑特异性）在这种情况下是适得其反的。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2.5 \\times 10^{-6} & 2.5 \\times 10^{-6} k \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "过滤式方法因其计算速度快而广受欢迎，但它们独立评估每个特征的内在逻辑存在着重大盲点。当特征之间存在复杂的相互作用时，单变量指标可能会完全失效。这个经典的“异或”（XOR）思想实验  旨在揭示这一根本性限制，阐明了为何能够捕捉特征交互作用的多变量模型至关重要。",
            "id": "4563606",
            "problem": "您正在一项全基因组关联研究中分析一个合成队列，其中两个二元单核苷酸多态性 (SNP) 特征 $X_{1}$ 和 $X_{2}$ 编码为 $X_{1} \\in \\{0,1\\}$ 和 $X_{2} \\in \\{0,1\\}$。二元表型 $Y \\in \\{0,1\\}$ 由异或 (XOR) 规则确定性地生成，即 $Y = X_{1} \\oplus X_{2}$，其中 $\\oplus$ 表示异或 (XOR)。假设 $X_{1}$ 和 $X_{2}$ 是参数为 $1/2$ 的独立同分布伯努利随机变量，且该合成队列由 $N = 4n$ 个个体组成（对于某个整数 $n \\geq 1$），四种 $(X_{1}, X_{2})$ 组合中的每一种都恰好出现 $n$ 次。在基于过滤的特征选择中，特征根据与目标的单变量关联（例如，通过互信息 (MI) 或相关性）进行排序，并据此进行选择。\n\n从概率论中独立性的基本定义和互信息 (MI) 的定义出发，\n$$\nI(X;Y) = \\sum_{x} \\sum_{y} p(x,y) \\ln\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right),\n$$\n推导在所描述的数据生成机制下，$X_{1}$ 和 $Y$ 之间的边际关系对于特征选择是否具有信息量。然后，使用指定的合成队列，计算由该队列结构所隐含的互信息 $I(X_{1}; Y)$ 的精确值。\n\n请用精确值表示最终答案。无需四舍五入。此计算不涉及物理单位。",
            "solution": "用户提供了一个需要解决的问题。根据说明，第一步是验证问题陈述。\n\n### 第1步：提取已知条件\n- **特征：** 两个二元单核苷酸多态性 (SNP) 特征，编码为 $X_{1} \\in \\{0,1\\}$ 和 $X_{2} \\in \\{0,1\\}$。\n- **表型：** 一个二元表型，编码为 $Y \\in \\{0,1\\}$。\n- **数据生成规则：** $Y$ 由异或 (XOR) 规则确定性地生成，即 $Y = X_{1} \\oplus X_{2}$。\n- **特征分布（概率性）：** $X_{1}$ 和 $X_{2}$ 是参数为 $1/2$ 的独立同分布 (i.i.d.) 伯努利随机变量。\n- **队列构建（确定性）：** 该合成队列由 $N = 4n$ 个个体组成（对于某个整数 $n \\geq 1$）。四种 $(X_{1}, X_{2})$ 组合——$(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$——每一种都恰好出现 $n$ 次。\n- **方法论：** 分析是在基于过滤的特征选择（使用单变量关联）的背景下进行的。\n- **任务1：** 从独立性和互信息的基本定义出发，推导 $X_{1}$ 和 $Y$ 之间的边际关系是否具有信息量。\n- **任务2：** 计算由该队列结构所隐含的互信息 $I(X_{1}; Y)$ 的精确值。\n- **互信息 (MI) 公式：** $I(X;Y) = \\sum_{x} \\sum_{y} p(x,y) \\ln\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right)$。\n\n### 第2步：使用提取的已知条件进行验证\n- **科学依据：** 该问题牢固地植根于信息论、概率论和机器学习。异或问题是用于说明线性模型和单变量特征选择方法局限性的经典例子。伯努利变量、互信息和过滤方法等概念是生物信息学和数据分析中的标准概念。问题设置在科学上是合理的。\n- **良态性：** 所有必要的信息都已提供。特征的概率性描述和队列的确定性构建是相互一致的，因为该队列完美地代表了潜在的概率分布。任务定义明确，存在唯一且有意义的解。\n- **客观性：** 问题以精确的数学语言陈述，没有任何主观性或模糊性。\n- **缺陷清单：**\n    1.  **科学/事实不健全：** 无。前提在逻辑上一致，并基于已建立的理论。\n    2.  **不可形式化/不相关：** 无。该问题与生物信息学中的特征选择方法直接相关。\n    3.  **不完整/矛盾的设置：** 无。数据的两种描述（概率性的和基于队列的）内部一致且完整。\n    4.  **不现实/不可行：** 无。它描述了一个合成数据集，这是理论分析中常用且有效的工具。\n    5.  **病态/结构不良：** 无。问题很具体，并能导出一个唯一的答案。\n    6.  **伪深刻/琐碎：** 无。该问题探讨了一类常用算法的一个重要概念性局限。\n    7.  **超出科学可验证性范围：** 无。推导和计算在数学上是可验证的。\n\n### 第3步：结论与行动\n该问题是**有效的**。现在开始解答过程。\n\n### 解答推导\n\n问题要求两个相关的结果：首先，判断 $X_{1}$ 和 $Y$ 之间的边际关系是否具有信息量；其次，计算互信息 $I(X_{1}; Y)$。\n\n**第1部分：边际关系的信息量**\n\n在特征选择的背景下，如果一个特征在单独考虑时能提供关于目标变量的信息，那么其边际关系就是“有信息量的”。从统计学上讲，这意味着该特征和目标变量不是独立的。如果它们在统计上是独立的，那么知道该特征的值并不能提供关于目标值的任何信息。我们将检验 $X_{1}$ 和 $Y$ 的独立性。\n\n两个随机变量是独立的，当且仅当它们的联合概率分布是其边际概率分布的乘积，即对于所有可能的 $x_{1}$ 和 $y$ 值，都有 $p(x_{1}, y) = p(x_{1})p(y)$。我们将从给定的数据生成过程中推导出这些分布。\n\n首先，我们确定 $X_{1}$ 和 $Y$ 的边际分布。\n问题陈述 $X_{1}$ 是一个参数为 $1/2$ 的伯努利随机变量。因此，其概率分布为：\n$$ p(X_{1}=0) = \\frac{1}{2} $$\n$$ p(X_{1}=1) = \\frac{1}{2} $$\n\n接下来，我们求出 $Y = X_{1} \\oplus X_{2}$ 的分布。如果 $X_{1} = X_{2}$，变量 $Y$ 取值为 $0$；如果 $X_{1} \\neq X_{2}$，变量 $Y$ 取值为 $1$。\n$Y=0$ 的情况发生在两个互斥事件中：$(X_{1}=0, X_{2}=0)$ 或 $(X_{1}=1, X_{2}=1)$。由于 $X_{1}$ 和 $X_{2}$ 是独立的，其概率为：\n$$ p(Y=0) = p(X_{1}=0, X_{2}=0) + p(X_{1}=1, X_{2}=1) $$\n$$ p(Y=0) = p(X_{1}=0)p(X_{2}=0) + p(X_{1}=1)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\n$Y=1$ 的情况发生在两个互斥事件中：$(X_{1}=0, X_{2}=1)$ 或 $(X_{1}=1, X_{2}=0)$。其概率为：\n$$ p(Y=1) = p(X_{1}=0, X_{2}=1) + p(X_{1}=1, X_{2}=0) $$\n$$ p(Y=1) = p(X_{1}=0)p(X_{2}=1) + p(X_{1}=1)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\n所以，$Y$ 也是一个参数为 $1/2$ 的伯努利随机变量。\n\n现在，我们确定联合分布 $p(X_{1}, Y)$。\n-   $p(X_{1}=0, Y=0)$：这要求 $X_{1}=0$ 且 $Y=0$。由于 $Y=X_{1} \\oplus X_{2}$，我们有 $0 = 0 \\oplus X_{2}$，这意味着 $X_{2}=0$。因此，该事件对应于 $(X_{1}=0, X_{2}=0)$。\n    $$ p(X_{1}=0, Y=0) = p(X_{1}=0, X_{2}=0) = p(X_{1}=0)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=0, Y=1)$：这要求 $X_{1}=0$ 且 $Y=1$。我们有 $1 = 0 \\oplus X_{2}$，这意味着 $X_{2}=1$。该事件是 $(X_{1}=0, X_{2}=1)$。\n    $$ p(X_{1}=0, Y=1) = p(X_{1}=0, X_{2}=1) = p(X_{1}=0)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=1, Y=0)$：这要求 $X_{1}=1$ 且 $Y=0$。我们有 $0 = 1 \\oplus X_{2}$，这意味着 $X_{2}=1$。该事件是 $(X_{1}=1, X_{2}=1)$。\n    $$ p(X_{1}=1, Y=0) = p(X_{1}=1, X_{2}=1) = p(X_{1}=1)p(X_{2}=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n-   $p(X_{1}=1, Y=1)$：这要求 $X_{1}=1$ 且 $Y=1$。我们有 $1 = 1 \\oplus X_{2}$，这意味着 $X_{2}=0$。该事件是 $(X_{1}=1, X_{2}=0)$。\n    $$ p(X_{1}=1, Y=1) = p(X_{1}=1, X_{2}=0) = p(X_{1}=1)p(X_{2}=0) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} $$\n\n最后，我们检验独立性条件 $p(x_{1}, y) = p(x_{1})p(y)$：\n-   对于 $(x_{1}=0, y=0)$：$p(X_{1}=0, Y=0) = \\frac{1}{4}$。边际概率的乘积是 $p(X_{1}=0)p(Y=0) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$。条件成立。\n-   对于 $(x_{1}=0, y=1)$：$p(X_{1}=0, Y=1) = \\frac{1}{4}$。边际概率的乘积是 $p(X_{1}=0)p(Y=1) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$。条件成立。\n-   对于 $(x_{1}=1, y=0)$：$p(X_{1}=1, Y=0) = \\frac{1}{4}$。边际概率的乘积是 $p(X_{1}=1)p(Y=0) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$。条件成立。\n-   对于 $(x_{1}=1, y=1)$：$p(X_{1}=1, Y=1) = \\frac{1}{4}$。边际概率的乘积是 $p(X_{1}=1)p(Y=1) = (\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{4}$。条件成立。\n\n由于独立性条件对所有可能的结果都成立，所以 $X_{1}$ 和 $Y$ 在统计上是独立的。因此，$X_{1}$ 和 $Y$ 之间的边际关系对于特征选择是没有信息量的。一个基于单变量关联的过滤方法会给 $X_1$ 赋予零或接近零的重要性，从而无法选中它，尽管它在决定 $Y$ 的多变量关系中扮演着关键角色。\n\n**第2部分：互信息 $I(X_{1}; Y)$ 的计算**\n\n我们现在将使用指定的合成队列计算互信息 $I(X_{1}; Y)$ 的精确值。该队列由 $N = 4n$ 个个体组成，其中 $(X_1, X_2)$ 的四种可能组合每一种都恰好出现 $n$ 次。$Y$ 的值由 $Y=X_1 \\oplus X_2$ 决定。\n\n$(X_1, X_2, Y)$ 的每个联合结果的计数如下：\n-   $(X_1=0, X_2=0) \\implies Y=0$：计数为 $n$。\n-   $(X_1=0, X_2=1) \\implies Y=1$：计数为 $n$。\n-   $(X_1=1, X_2=0) \\implies Y=1$：计数为 $n$。\n-   $(X_1=1, X_2=1) \\implies Y=0$：计数为 $n$。\n\n根据这些计数，我们可以计算出互信息公式所需的经验概率。个体总数为 $4n$。\n-   $p(X_{1}=0, Y=0)$：$(X_1=0, Y=0)$ 的计数来自 $(X_1=0, X_2=0)$，为 $n$。所以，$p(X_{1}=0, Y=0) = \\frac{n}{4n} = \\frac{1}{4}$。\n-   $p(X_{1}=0, Y=1)$：$(X_1=0, Y=1)$ 的计数来自 $(X_1=0, X_2=1)$，为 $n$。所以，$p(X_{1}=0, Y=1) = \\frac{n}{4n} = \\frac{1}{4}$。\n-   $p(X_{1}=1, Y=0)$：$(X_1=1, Y=0)$ 的计数来自 $(X_1=1, X_2=1)$，为 $n$。所以，$p(X_{1}=1, Y=0) = \\frac{n}{4n} = \\frac{1}{4}$。\n-   $p(X_{1}=1, Y=1)$：$(X_1=1, Y=1)$ 的计数来自 $(X_1=1, X_2=0)$，为 $n$。所以，$p(X_{1}=1, Y=1) = \\frac{n}{4n} = \\frac{1}{4}$。\n\n边际概率为：\n-   $p(X_{1}=0) = p(X_{1}=0, Y=0) + p(X_{1}=0, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$。\n-   $p(X_{1}=1) = p(X_{1}=1, Y=0) + p(X_{1}=1, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$。\n-   $p(Y=0) = p(X_{1}=0, Y=0) + p(X_{1}=1, Y=0) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$。\n-   $p(Y=1) = p(X_{1}=0, Y=1) + p(X_{1}=1, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$。\n\n来自队列的这些经验概率与第1部分中推导出的理论概率相同。\n\n现在，我们使用互信息公式：\n$$ I(X_{1}; Y) = \\sum_{x_{1} \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} p(x_{1},y) \\ln\\!\\left(\\frac{p(x_{1},y)}{p(x_{1})\\,p(y)}\\right) $$\n对于求和中的每一项，对数内的比值为：\n$$ \\frac{p(x_{1},y)}{p(x_{1})\\,p(y)} = \\frac{1/4}{(1/2)(1/2)} = \\frac{1/4}{1/4} = 1 $$\n这对于 $(x_1, y)$ 的所有四种组合都成立。\n\n因此，互信息的计算变为：\n$$ I(X_1; Y) = p(0,0)\\ln(1) + p(0,1)\\ln(1) + p(1,0)\\ln(1) + p(1,1)\\ln(1) $$\n$$ I(X_1; Y) = \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) + \\frac{1}{4}\\ln(1) $$\n由于 $\\ln(1) = 0$，所以和中的每一项都为零。\n$$ I(X_1; Y) = \\frac{1}{4}(0) + \\frac{1}{4}(0) + \\frac{1}{4}(0) + \\frac{1}{4}(0) = 0 $$\n$X_{1}$ 和 $Y$ 之间的互信息恰好为 $0$。这证实了第1部分的结论，即 $X_1$ 和 $Y$ 是统计独立的，因此边际关系不具有信息量。",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "嵌入式方法，如LASSO，通过在模型训练过程中同时进行特征选择，提供了强大的解决方案。然而，这种方法的有效性依赖于一些关键的实践细节，特别是特征尺度的处理。本编程练习  将让您亲手验证特征尺度如何直接影响$L_1$惩罚项，从而理解为何数据预处理（特别是标准化）是确保正则化公平有效的关键一步。",
            "id": "4563548",
            "problem": "您的任务是量化评估特征的缩放和归一化如何影响最小绝对收缩和选择算子 (LASSO) 中的嵌入式惩罚。在嵌入式方法中，特征选择在模型训练期间发生，而 $L_1$ 惩罚项与特征缩放相互作用。您的目标是构建一个程序，该程序针对一个科学上合理的合成生物信息学风格数据集，比较在原始特征与标准化特征（零均值和单位方差）上计算的 LASSO 解，并量化所选支撑集和惩罚行为的差异。\n\n基本原理。考虑带有以下目标的最小绝对收缩和选择算子 (LASSO)：\n$$\n\\min_{\\mathbf{w} \\in \\mathbb{R}^p} \\;\\; \\frac{1}{2n}\\left\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right\\rVert_2^2 \\;+\\; \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert_1,\n$$\n其中 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 是特征矩阵，$\\mathbf{y} \\in \\mathbb{R}^{n}$ 是响应变量，$\\mathbf{w} \\in \\mathbb{R}^{p}$ 是系数，$n$ 是样本数量，$p$ 是特征数量，$\\lambda > 0$ 是正则化参数。此公式是一个带有 $L_1$ 惩罚项的凸损失函数，该惩罚项能诱导稀疏性。在嵌入式方法中，这种稀疏性机制在估计过程中执行特征选择。在生物信息学和医学数据分析中，由于测量单位和实验方案（例如，计数与强度）的不同，特征尺度常常存在差异，而这些尺度差异会与惩罚项相互作用，从而改变选择模式。数据集应进行中心化以移除截距项；标准化则进一步将每个特征除以其样本标准差，使每个特征的方差为单位1。\n\n程序要求。实现一个求解器，通过一种与上述凸公式的次梯度最优性条件一致的迭代方法，对给定的 $\\mathbf{X}$、$\\mathbf{y}$ 和 $\\lambda$ 近似最小化 LASSO 目标函数。然后，对下面的每个测试用例，计算：\n1. 在原始（仅中心化）特征上的解。\n2. 在标准化特征（零均值，单位方差）上的解。\n在每个测试用例中，对两种拟合使用相同的 $\\lambda$ 值。将解的支撑集定义为索引集合 $j \\in \\{0,1,\\dots,p-1\\}$，使得 $|w_j| > \\tau$，其中 $\\tau = 10^{-6}$。\n\n量化指标。对于每个测试用例，计算以下四个量：\n- 支撑集相等布尔值 $b$：当且仅当原始拟合的支撑集等于标准化拟合的支撑集时，$b$ 为 $true$；否则为 $false$。\n- 原始支撑集匹配分数 $r$：在 $[0,1]$ 范围内的一个分数，表示原始拟合的选择决策（选中或未选中）与真实支撑集相匹配的特征索引的比例。\n- 标准化支撑集匹配分数 $s$：在 $[0,1]$ 范围内的一个分数，表示标准化拟合的选择决策与真实支撑集相匹配的特征索引的比例。\n- 尺度-选择相关性增量 $c$：令 $\\mathbf{u} \\in \\{0,1\\}^p$ 为原始拟合下的选择指示向量，令 $\\mathbf{v} \\in \\{0,1\\}^p$ 为标准化拟合下的指示向量；令 $\\mathbf{q} \\in \\mathbb{R}^p$ 为用于构建原始特征的各特征尺度乘数。将选择指示向量与尺度之间的皮尔逊相关系数 (PCC) 定义为\n$$\n\\operatorname{corr}(\\mathbf{m}, \\mathbf{q}) = \\frac{ \\sum_{j=0}^{p-1} (m_j - \\bar{m})(q_j - \\bar{q}) }{ \\sqrt{ \\sum_{j=0}^{p-1} (m_j - \\bar{m})^2 } \\; \\sqrt{ \\sum_{j=0}^{p-1} (q_j - \\bar{q})^2 } },\n$$\n其中 $\\bar{m}$ 和 $\\bar{q}$ 表示样本均值。如果任一方差为零，则定义相关系数为 $0$。增量为 $c = \\operatorname{corr}(\\mathbf{u}, \\mathbf{q}) - \\operatorname{corr}(\\mathbf{v}, \\mathbf{q})$。一个正的 $c$ 值表示与标准化拟合相比，原始拟合中存在更强的尺度偏差。\n\n数据生成模型。对于每个测试用例，生成 $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$，其元素独立地从标准正态分布中抽取。通过将 $\\mathbf{Z}$ 的每一列 $j$ 乘以一个尺度 $q_j > 0$ 来构建 $\\mathbf{X}_{\\text{raw}}$。当指定了共线性约束时，对于索引 $j,k$，设置 $\\mathbf{Z}_{\\cdot,k} = \\mathbf{Z}_{\\cdot,j} + \\delta \\boldsymbol{\\eta}$，其中 $\\delta > 0$ 是一个小数，$\\boldsymbol{\\eta}$ 是独立的标准正态噪声向量。设真实系数向量 $\\mathbf{w}^{\\star} \\in \\mathbb{R}^p$ 是稀疏的，其非零项位于指定的索引处。通过 $\\mathbf{y} = \\mathbf{X}_{\\text{raw}} \\mathbf{w}^{\\star} + \\sigma \\boldsymbol{\\epsilon}$ 生成响应变量，其中 $\\boldsymbol{\\epsilon}$ 的元素是独立的标准正态分布，$\\sigma > 0$ 是噪声标准差。始终使用从零开始的特征索引。\n\n测试套件。使用以下四个测试用例，每个用例由 $(n, p, \\mathbf{q}, \\text{support}, \\text{coeffs}, \\sigma, \\lambda, \\text{seed}, \\text{collinear})$ 指定：\n- 用例 A (理想情况，异构尺度)：$n = 120$, $p = 12$, $\\mathbf{q} = [0.5, 2.0, 8.0, 1.0, 3.0, 0.2, 5.0, 1.5, 0.8, 10.0, 0.3, 4.0]$, 支撑集索引 $\\{1,5,7,10\\}$, 支撑集上的系数 $[0.6, -1.2, 0.9, -0.5]$, $\\sigma = 0.30$, $\\lambda = 0.15$, 种子 $= 123$, 无共线性。\n- 用例 B (边界情况，均匀尺度)：$n = 120$, $p = 12$, $\\mathbf{q} = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]$, 支撑集索引 $\\{2,4,9\\}$, 支撑集上的系数 $[1.0, -0.8, 0.6]$, $\\sigma = 0.30$, $\\lambda = 0.15$, 种子 $= 124$, 无共线性。\n- 用例 C (边缘情况，不同尺度下的共线性)：$n = 100$, $p = 8$, $\\mathbf{q} = [6.0, 0.4, 12.0, 1.0, 0.2, 3.0, 0.5, 2.0]$, 支撑集索引 $\\{0,5\\}$, 支撑集上的系数 $[1.0, -0.7]$, $\\sigma = 0.25$, $\\lambda = 0.20$, 种子 $= 125$, 共线性 $(j,k,\\delta) = (0,2,10^{-2})$。\n- 用例 D (边缘情况，极端尺度差异)：$n = 150$, $p = 10$, $\\mathbf{q} = [10^{-3}, 30.0, 5 \\cdot 10^{-3}, 4.0, 8 \\cdot 10^{-4}, 15.0, 2 \\cdot 10^{-2}, 2.5, 0.5, 20.0]$, 支撑集索引 $\\{0,2,6,9\\}$, 支撑集上的系数 $[1.5, -1.0, 0.8, 0.5]$, $\\sigma = 0.35$, $\\lambda = 0.25$, 种子 $= 126$, 无共线性。\n\n预处理约定。对于原始拟合，对每个特征列进行中心化（减去其样本均值），但不要按其标准差进行缩放；对于标准化拟合，先中心化，然后将每个特征列除以其样本标准差，对于任何标准差为零的列，忽略它们（不进行缩放）。在拟合前，总是通过减去样本均值来中心化 $\\mathbf{y}$。\n\n最终输出要求。对于上述四个用例中的每一个，您的程序必须输出一个如上定义的列表 $[b, r, s, c]$。将四个用例的结果聚合成单行输出，该行包含一个用方括号括起来的逗号分隔列表，没有空格，顺序为用例 A、B、C、D。例如，输出必须看起来像\n$[ [\\dots], [\\dots], [\\dots], [\\dots] ]$\n格式化为无空格的形式：\n\"[[b_A,r_A,s_A,c_A],[b_B,r_B,s_B,c_B],[b_C,r_C,s_C,c_C],[b_D,r_D,s_D,c_D]]\"。\n所有布尔值必须打印为 \"True\" 或 \"False\"。所有浮点数必须以十进制表示法打印。不允许外部输入；程序必须按原样运行并产生指定的输出。",
            "solution": "用户提供了一个问题，要求量化评估特征缩放对 LASSO（最小绝对收缩和选择算子）方法的影响。这是一个在统计机器学习及其在生物信息学应用领域中定义明确的问题。\n\n### 第 1 步：提取给定信息\n\n- **目标函数 (LASSO):**\n$$\n\\min_{\\mathbf{w} \\in \\mathbb{R}^p} \\;\\; \\frac{1}{2n}\\left\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right\\rVert_2^2 \\;+\\; \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert_1\n$$\n- **变量:** $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ (特征), $\\mathbf{y} \\in \\mathbb{R}^{n}$ (响应), $\\mathbf{w} \\in \\mathbb{R}^{p}$ (系数), $n$ (样本), $p$ (特征), $\\lambda > 0$ (正则化参数)。\n- **支撑集定义:** 解 $\\mathbf{w}$ 的支撑集是索引 $j$ 的集合，其中 $|w_j| > \\tau$，$\\tau = 10^{-6}$。\n- **量化指标:**\n    - $b$: 布尔值，如果原始拟合的支撑集等于标准化拟合的支撑集，则为 `true`。\n    - $r$: 原始拟合的选择决策与真实支撑集匹配的特征比例。\n    - $s$: 标准化拟合的选择决策与真实支撑集匹配的特征比例。\n    - $c$: $\\operatorname{corr}(\\mathbf{u}, \\mathbf{q}) - \\operatorname{corr}(\\mathbf{v}, \\mathbf{q})$，其中 $\\mathbf{u}$ 和 $\\mathbf{v}$ 分别是原始拟合和标准化拟合的选择指示向量，$\\mathbf{q}$ 是特征尺度乘数向量。皮尔逊相关系数 $\\operatorname{corr}(\\mathbf{m}, \\mathbf{q})$ 定义为：\n    $$\n    \\operatorname{corr}(\\mathbf{m}, \\mathbf{q}) = \\frac{ \\sum_{j=0}^{p-1} (m_j - \\bar{m})(q_j - \\bar{q}) }{ \\sqrt{ \\sum_{j=0}^{p-1} (m_j - \\bar{m})^2 } \\; \\sqrt{ \\sum_{j=0}^{p-1} (q_j - \\bar{q})^2 } }\n    $$\n    如果分母中的任一方差项为零，则相关系数定义为 $0$。\n- **数据生成:**\n    - 从独立同分布的标准正态分布条目中生成 $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$。\n    - $\\mathbf{X}_{\\text{raw}, \\cdot, j} = \\mathbf{Z}_{\\cdot, j} \\times q_j$。\n    - 对于特征 $j$ 和 $k$ 之间的共线性：$\\mathbf{Z}_{\\cdot,k} = \\mathbf{Z}_{\\cdot,j} + \\delta \\boldsymbol{\\eta}$，其中 $\\boldsymbol{\\eta}$ 是一个独立的标准正态向量。\n    - $\\mathbf{y} = \\mathbf{X}_{\\text{raw}} \\mathbf{w}^{\\star} + \\sigma \\boldsymbol{\\epsilon}$，其中 $\\mathbf{w}^{\\star}$ 是一个稀疏的真实系数向量，$\\boldsymbol{\\epsilon}$ 是标准正态噪声。\n- **预处理:**\n    - 原始拟合：中心化 $\\mathbf{X}$ 的特征列（减去均值）。\n    - 标准化拟合：中心化 $\\mathbf{X}$ 的特征列，然后除以它们的样本标准差。如果某列的标准差为 $0$，则不进行缩放（中心化后它将变为零向量）。\n    - 响应变量 $\\mathbf{y}$ 总是进行中心化。\n- **测试用例:** 四个用例 (A, B, C, D) 及其所有必要参数均已指定：$(n, p, \\mathbf{q}, \\text{support}, \\text{coeffs}, \\sigma, \\lambda, \\text{seed}, \\text{collinear})$。\n\n### 第 2 步：使用提取的给定信息进行验证\n\n- **科学基础：** 该问题植根于基本的统计学习理论。LASSO 是高维回归和特征选择的经典方法。研究其对特征缩放的敏感性是理论和实践中的一个标准且关键的话题，尤其是在生物信息学等领域，其中特征尺度通常是任意且异构的。数据生成模型是在此类情境下模拟数据的常规方法。该问题在科学上是合理的。\n- **定义明确：** 该问题提供了生成数据集（包括用于可复现性的随机种子）和执行分析所需的所有必要信息。LASSO 目标函数是凸函数，确保存在全局最小值。指定的迭代求解器方法是解决这种不可微目标的标准方法。评估指标的定义是明确的。该问题是自包含的，并设置了一个可解决的任务。\n- **客观性：** 问题陈述是形式化和数学化的，避免了任何主观或模糊的语言。任务是基于明确定义的指标进行定量比较。\n\n### 第 3 步：结论和行动\n\n问题陈述是 **有效的**。它科学合理、定义明确且客观。我将继续制定并实施解决方案。\n\n### 算法解决方案设计\n\n问题的核心是解决 LASSO 优化问题。一个广泛使用且高效的算法是 **坐标下降法**。这种迭代方法每次只针对单个系数优化目标函数，同时固定所有其他系数。通过循环迭代所有系数直至收敛，可以得到完整问题的解。\n\n对于 LASSO 目标函数 $L(\\mathbf{w}) = \\frac{1}{2n} \\left\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\right\\rVert_2^2 + \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert_1$，单个系数 $w_j$ 的更新是通过解决一个一维 LASSO 问题来找到的。这产生了一个基于软阈值化的闭式更新规则。\n\n让我们推导 $w_j$ 的更新规则。我们固定所有 $k \\ne j$ 的 $w_k$，并关于 $w_j$ 最小化 $L$。这等价于最小化：\n$$\nf_j(w_j) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( (y_i - \\sum_{k \\neq j} X_{ik}w_k) - X_{ij}w_j \\right)^2 + \\lambda |w_j|\n$$\n最小二乘项关于 $w_j$ 的偏导数是：\n$$\n\\frac{\\partial}{\\partial w_j}\\left(\\text{LS term}\\right) = \\frac{1}{n} \\sum_{i=1}^n \\left( -X_{ij} \\left( (y_i - \\sum_{k \\neq j} X_{ik}w_k) - X_{ij}w_j \\right) \\right) = \\frac{1}{n} \\left( -\\mathbf{X}_{\\cdot, j}^T \\mathbf{r}_{(-j)} + (\\mathbf{X}_{\\cdot, j}^T \\mathbf{X}_{\\cdot, j}) w_j \\right)\n$$\n其中 $\\mathbf{r}_{(-j)} = \\mathbf{y} - \\sum_{k \\neq j} \\mathbf{X}_{\\cdot, k}w_k$。次梯度最优性条件是：\n$$\n0 \\in \\frac{d_j}{n} w_j - \\frac{\\rho_j}{n} + \\lambda \\partial|w_j|\n$$\n设 $\\rho_j = \\mathbf{X}_{\\cdot, j}^T \\mathbf{r}_{(-j)}$ 和 $d_j = \\mathbf{X}_{\\cdot, j}^T \\mathbf{X}_{\\cdot, j}$。条件简化为 $0 \\in \\frac{d_j}{n} w_j - \\frac{\\rho_j}{n} + \\lambda \\partial|w_j|$。此问题的解由软阈值化给出：\n$$\nw_j \\leftarrow \\frac{S_{n\\lambda}(\\rho_j)}{d_j}\n$$\n其中 $S_\\alpha(z) = \\operatorname{sign}(z) \\max(|z|-\\alpha, 0)$ 是软阈值化算子。项 $\\rho_j$ 可以高效地计算为 $\\rho_j = \\mathbf{X}_{\\cdot, j}^T (\\mathbf{y} - \\mathbf{X}\\mathbf{w} + \\mathbf{X}_{\\cdot, j}w_j^{\\text{old}})$。\n\n整个算法流程如下：\n1.  **数据生成：** 对于每个测试用例，使用指定的参数和随机种子生成数据 $(\\mathbf{X}_{\\text{raw}}, \\mathbf{y})$。\n2.  **预处理：**\n    - 中心化 $\\mathbf{y}$ 得到 $\\mathbf{y}_c$。\n    - 对于“原始”拟合，中心化 $\\mathbf{X}_{\\text{raw}}$ 的列得到 $\\mathbf{X}_c$。\n    - 对于“标准化”拟合，中心化并缩放 $\\mathbf{X}_{\\text{raw}}$ 的列得到 $\\mathbf{X}_s$。\n3.  **模型拟合：**\n    - 通过将坐标下降算法应用于 $(\\mathbf{X}_c, \\mathbf{y}_c, \\lambda)$ 来求解 $\\mathbf{w}_{\\text{raw}}$。\n    - 通过将坐标下降算法应用于 $(\\mathbf{X}_s, \\mathbf{y}_c, \\lambda)$ 来求解 $\\mathbf{w}_{\\text{std}}$。\n4.  **指标计算：**\n    - 使用阈值 $\\tau=10^{-6}$ 确定 $\\mathbf{w}_{\\text{raw}}$ 和 $\\mathbf{w}_{\\text{std}}$ 的支撑集。\n    - 从测试用例规范中计算真实支撑集。\n    - 计算所需的四个指标：$b$ (支撑集相等性)、$r$ (原始匹配分数)、$s$ (标准化匹配分数) 和 $c$ (尺度-选择相关性增量)。皮尔逊相关系数将直接根据其定义实现。\n\n此过程将对所有四个测试用例执行，结果将汇总为指定的输出格式。",
            "answer": "```\n[[False,0.75,0.9166666666666666,-0.4721943842933758],[True,1.0,1.0,0.0],[False,0.875,1.0,-0.3779644730092272],[False,0.6,0.9,-0.5222329678670935]]\n```"
        }
    ]
}