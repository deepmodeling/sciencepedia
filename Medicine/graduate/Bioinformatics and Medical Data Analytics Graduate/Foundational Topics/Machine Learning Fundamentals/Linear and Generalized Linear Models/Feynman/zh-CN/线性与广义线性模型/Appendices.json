{
    "hands_on_practices": [
        {
            "introduction": "掌握广义线性模型 (GLM) 的第一步是理解其核心的数学引擎。逻辑回归是 GLM 家族中最常用的模型之一，其参数估计通常通过最大化对数似然函数来实现。本练习将引导你从伯努利分布的概率质量函数这一基本出发点，推导出逻辑回归模型的对数似然函数、梯度（得分向量）和海森矩阵，这三者是模型拟合算法（如牛顿-拉夫逊法）的基石。",
            "id": "4578844",
            "problem": "在一个用于二元疾病结局的病例-对照生物标志物研究中，假设我们观测了 $n$ 个独立个体，索引为 $i = 1, \\dots, n$。对于每个个体 $i$，令 $\\mathbf{x}_{i} \\in \\mathbb{R}^{p}$ 表示一个固定的 $p$ 维生物信息学特征协变量向量（例如，$p$ 个基因的归一化表达量测量值），令 $y_{i} \\in \\{0,1\\}$ 表示其疾病状态（其中 $y_{i} = 1$ 表示病例，$y_{i} = 0$ 表示对照）。假设一个广义线性模型 (GLM)，其响应为伯努利分布，并采用标准对数几率连接函数，即在给定 $\\mathbf{x}_{i}$ 和参数 $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ 的条件下，$Y_{i}$ 的条件分布是均值为 $\\mu_{i} = \\Pr(Y_{i} = 1 \\mid \\mathbf{x}_{i}, \\boldsymbol{\\beta})$ 的伯努利分布，并且线性预测器为 $\\eta_{i} = \\mathbf{x}_{i}^{\\top}\\boldsymbol{\\beta}$，逻辑均值函数为 $\\mu_{i} = \\frac{1}{1 + \\exp(-\\eta_{i})}$。\n\n仅从以下基本出发点开始：\n- 伯努利概率质量函数 $p(y \\mid \\mu) = \\mu^{y}(1 - \\mu)^{1 - y}$，其中 $y \\in \\{0,1\\}$。\n- 个体间观测值的独立性。\n- 标准对数几率连接函数 $\\eta = \\ln\\left(\\frac{\\mu}{1-\\mu}\\right)$ 及其逆函数 $\\mu = \\frac{1}{1 + \\exp(-\\eta)}$。\n\n推导此逻辑回归模型的全样本对数似然函数 $\\ell(\\boldsymbol{\\beta})$，用 $\\{\\mathbf{x}_{i}, y_{i}\\}_{i=1}^{n}$ 和 $\\boldsymbol{\\beta}$ 表示，然后明确计算：\n1. 得分向量（对数似然函数关于 $\\boldsymbol{\\beta}$ 的梯度）。\n2. 海森矩阵（对数似然函数关于 $\\boldsymbol{\\beta}$ 的二阶导数矩阵）。\n\n你的推导应基于上述基本事实，不得引用现成的逻辑回归公式。通过引入 $n \\times p$ 的设计矩阵 $\\mathbf{X}$（其第 $i$ 行为 $\\mathbf{x}_{i}^{\\top}$）、响应向量 $\\mathbf{y} = (y_{1}, \\dots, y_{n})^{\\top}$、均值向量 $\\boldsymbol{\\mu}(\\boldsymbol{\\beta}) = (\\mu_{1}, \\dots, \\mu_{n})^{\\top}$ 以及对角权重矩阵 $\\mathbf{W}(\\boldsymbol{\\beta}) = \\mathrm{diag}\\big(\\mu_{i}(1 - \\mu_{i})\\big)$，使用标准矩阵表示法来表达你的最终结果。以闭合形式提供这三个表达式。最终答案应为单个闭合形式的解析表达式。无需四舍五入。",
            "solution": "该问题陈述构成了广义线性模型 (GLM) 理论中的一个标准的、适定的推导。它在科学上是合理的、客观的，并包含了唯一解所需的所有必要信息。因此，我们可以开始推导。\n\n目标是为逻辑回归模型推导对数似然函数、其梯度（得分向量）及其海森矩阵。我们从所提供的基本原理出发。\n\n**1. 对数似然函数 $\\ell(\\boldsymbol{\\beta})$ 的推导**\n\n对于单个观测值 $i$，响应变量 $Y_i$ 服从参数为 $\\mu_i = \\Pr(Y_i=1 \\mid \\mathbf{x}_i, \\boldsymbol{\\beta})$ 的伯努利分布。其概率质量函数 (PMF) 给出为 $p(y_i \\mid \\mu_i) = \\mu_i^{y_i} (1 - \\mu_i)^{1 - y_i}$。由于 $n$ 个观测值是独立的，参数 $\\boldsymbol{\\beta}$ 的总似然函数是各个 PMF 的乘积：\n$$ L(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} p(y_i \\mid \\mu_i(\\boldsymbol{\\beta})) = \\prod_{i=1}^{n} \\mu_i^{y_i} (1 - \\mu_i)^{1-y_i} $$\n对数似然函数 $\\ell(\\boldsymbol{\\beta})$ 是似然函数的自然对数：\n$$ \\ell(\\boldsymbol{\\beta}) = \\ln(L(\\boldsymbol{\\beta})) = \\ln\\left(\\prod_{i=1}^{n} \\mu_i^{y_i} (1 - \\mu_i)^{1-y_i}\\right) = \\sum_{i=1}^{n} \\ln\\left(\\mu_i^{y_i} (1 - \\mu_i)^{1-y_i}\\right) $$\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i \\ln(\\mu_i) + (1-y_i)\\ln(1-\\mu_i) \\right] $$\n该表达式可以重新整理为：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i \\ln(\\mu_i) - y_i \\ln(1-\\mu_i) + \\ln(1-\\mu_i) \\right] = \\sum_{i=1}^{n} \\left[ y_i \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right) + \\ln(1-\\mu_i) \\right] $$\n该模型使用标准对数几率连接函数 $\\eta_i = \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)$，其中线性预测器为 $\\eta_i = \\mathbf{x}_i^\\top\\boldsymbol{\\beta}$。将 $\\eta_i$ 代入 $\\ell(\\boldsymbol{\\beta})$ 的表达式中得到：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} [y_i \\eta_i + \\ln(1-\\mu_i)] $$\n为了用 $\\eta_i$ 表示 $\\ln(1-\\mu_i)$，我们使用逆连接函数 $\\mu_i = \\frac{1}{1 + \\exp(-\\eta_i)}$。\n$$ 1 - \\mu_i = 1 - \\frac{1}{1 + \\exp(-\\eta_i)} = \\frac{1 + \\exp(-\\eta_i) - 1}{1 + \\exp(-\\eta_i)} = \\frac{\\exp(-\\eta_i)}{1 + \\exp(-\\eta_i)} $$\n一个等价的表达式是通过将分子和分母同乘以 $\\exp(\\eta_i)$ 得到：\n$$ 1 - \\mu_i = \\frac{1}{ \\exp(\\eta_i) + 1 } $$\n取自然对数，我们得到：\n$$ \\ln(1-\\mu_i) = \\ln\\left(\\frac{1}{1+\\exp(\\eta_i)}\\right) = -\\ln(1+\\exp(\\eta_i)) $$\n将此结果代回 $\\ell(\\boldsymbol{\\beta})$ 的表达式中，得到以线性预测器 $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ 表示的对数似然函数的最终形式：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i (\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right] $$\n\n**2. 得分向量 $\\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta})$ 的推导**\n\n得分向量是对数似然函数关于参数向量 $\\boldsymbol{\\beta}$ 的梯度。它是一个 $p \\times 1$ 的向量。\n$$ \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\left[ y_i (\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right] $$\n我们对和中的每一项使用链式法则。令 $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$。则 $\\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}} = \\mathbf{x}_i$。\n第 $i$ 项的导数为：\n$$ \\frac{\\partial \\ell_i(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = y_i \\frac{\\partial(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}}[\\ln(1 + \\exp(\\eta_i))] $$\n$$ \\frac{\\partial \\ell_i(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = y_i \\mathbf{x}_i - \\frac{1}{1 + \\exp(\\eta_i)} \\cdot \\exp(\\eta_i) \\cdot \\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}} $$\n注意到 $\\mu_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{1}{1 + \\exp(-\\eta_i)}$，我们有：\n$$ \\frac{\\partial \\ell_i(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = y_i \\mathbf{x}_i - \\mu_i \\mathbf{x}_i = (y_i - \\mu_i) \\mathbf{x}_i $$\n对所有观测值求和，得到得分向量：\n$$ \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\mu_i(\\boldsymbol{\\beta})) \\mathbf{x}_i $$\n为了用矩阵表示法表示，令 $\\mathbf{X}$ 为 $n \\times p$ 的设计矩阵，其行为 $\\mathbf{x}_i^\\top$，$\\mathbf{y}$ 为 $n \\times 1$ 的结果向量，$\\boldsymbol{\\mu}(\\boldsymbol{\\beta})$ 为 $n \\times 1$ 的均值向量。该和等价于矩阵乘积：\n$$ \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}(\\boldsymbol{\\beta})) $$\n\n**3. 海森矩阵 $\\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta})$ 的推导**\n\n海森矩阵是对数似然函数的二阶偏导数 $p \\times p$ 矩阵。它通过对得分向量关于 $\\boldsymbol{\\beta}^\\top$ 求导得到。\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left( \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) \\right) = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left( \\sum_{i=1}^{n} (y_i - \\mu_i) \\mathbf{x}_i \\right) $$\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top} \\left( y_i \\mathbf{x}_i - \\mu_i \\mathbf{x}_i \\right) $$\n由于 $y_i$ 和 $\\mathbf{x}_i$ 是固定的，第一项 $y_i\\mathbf{x}_i$ 的导数为零。对于第二项，我们应用向量微积分的乘积法则，注意到 $\\mathbf{x}_i$ 不是 $\\boldsymbol{\\beta}$ 的函数：\n$$ \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top}(-\\mu_i \\mathbf{x}_i) = - \\mathbf{x}_i \\frac{\\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} $$\n我们需要求 $\\mu_i$ 关于 $\\boldsymbol{\\beta}^\\top$ 的导数。我们再次使用链式法则：$\\frac{\\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}^\\top}$。我们有 $\\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}^\\top} = \\mathbf{x}_i^\\top$。均值函数 $\\mu_i$ 关于线性预测器 $\\eta_i$ 的导数为：\n$$ \\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i}\\left(\\frac{1}{1+\\exp(-\\eta_i)}\\right) = - (1+\\exp(-\\eta_i))^{-2} \\cdot (\\exp(-\\eta_i)) \\cdot (-1) = \\frac{\\exp(-\\eta_i)}{(1+\\exp(-\\eta_i))^2} $$\n这可以因式分解为：\n$$ \\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\left(\\frac{1}{1+\\exp(-\\eta_i)}\\right) \\left(\\frac{\\exp(-\\eta_i)}{1+\\exp(-\\eta_i)}\\right) = \\mu_i (1-\\mu_i) $$\n因此，$\\mu_i$ 关于 $\\boldsymbol{\\beta}^\\top$ 的导数为：\n$$ \\frac{\\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} = \\mu_i(1-\\mu_i) \\mathbf{x}_i^\\top $$\n将此代入海森矩阵第 $i$ 项的表达式中，得到：\n$$ \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^\\top}(-\\mu_i \\mathbf{x}_i) = - \\mathbf{x}_i (\\mu_i(1-\\mu_i)\\mathbf{x}_i^\\top) = - \\mu_i(1-\\mu_i) \\mathbf{x}_i \\mathbf{x}_i^\\top $$\n这是一个 $p \\times p$ 矩阵。对所有观测值求和，得到海森矩阵：\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} - \\mu_i(1-\\mu_i) \\mathbf{x}_i \\mathbf{x}_i^\\top = - \\sum_{i=1}^{n} \\mu_i(1-\\mu_i) \\mathbf{x}_i \\mathbf{x}_i^\\top $$\n使用定义的权重矩阵 $\\mathbf{W}(\\boldsymbol{\\beta}) = \\mathrm{diag}(\\mu_i(1-\\mu_i))$，这个和是矩阵乘积 $-\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}$ 的定义。\n$$ \\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = - \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X} $$\n该矩阵在用于拟合 GLM 的牛顿-拉弗森算法中至关重要，它与费雪信息矩阵相关。\n\n所要求的三个表达式是：\n1.  对数似然函数：$\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\mathbf{x}_i^\\top \\boldsymbol{\\beta} - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right)$\n2.  得分向量：$\\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}(\\boldsymbol{\\beta}))$\n3.  海森矩阵：$\\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta}) = - \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\ell(\\boldsymbol{\\beta}) \\\\\n\\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) \\\\\n\\nabla_{\\boldsymbol{\\beta}}^2 \\ell(\\boldsymbol{\\beta})\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum_{i=1}^{n} \\left( y_i \\mathbf{x}_i^\\top \\boldsymbol{\\beta} - \\ln(1 + \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})) \\right) \\\\\n\\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}(\\boldsymbol{\\beta})) \\\\\n- \\mathbf{X}^\\top \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "模型拟合完成后，下一个关键步骤是结果的解释。本练习聚焦于逻辑回归最常见的应用：从模型系数中计算和解释优势比 (Odds Ratio, OR)。通过一个典型的流行病学研究场景，你将学习如何将一个从模型中得到的数值系数  转化为关于风险和关联的有意义的陈述，从而搭建起理论与实践之间的桥梁。",
            "id": "4595172",
            "problem": "一项队列研究旨在调查二元暴露 $X$（编码为 $X=1$ 表示暴露，$X=0$ 表示未暴露）是否与一年内二元疾病结局 $Y$ 的发生相关，同时调整了连续协变量 $Z$（中心化年龄）。研究人员拟合了一个广义线性模型 (GLM)，其结局为二项分布，并使用典则 logit 连接函数，使得条件均值满足 $g(\\mathbb{E}[Y \\mid X,Z]) = \\ln\\!\\left(\\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\\right)$，且线性预测器为 $\\eta = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z$。暴露的拟合系数为 $\\hat{\\beta}_{1} = 1.1$。\n\n仅使用在 logit 连接函数和上述模型结构下优势（odds）和优势比（OR）的核心定义，推导在固定 $Z$ 值的情况下，比较 $X=1$ 与 $X=0$ 的暴露优势比（由该拟合模型隐含），然后使用 $\\hat{\\beta}_{1} = 1.1$ 计算其数值。请将最终数值答案以无单位比率的形式报告，并四舍五入到 $3$ 位有效数字。",
            "solution": "在进行解答之前，需对问题陈述进行验证。\n\n### 第 1 步：提取已知条件\n-   **模型类型**: 广义线性模型 (GLM)。\n-   **结局**: 二元疾病结局 $Y$。$Y=1$ 表示疾病发生，$Y=0$ 表示无疾病。结局服从二项分布。\n-   **暴露**: 二元暴露 $X$，$X=1$ 表示暴露，$X=0$ 表示未暴露。\n-   **协变量**: 连续协变量 $Z$（中心化年龄）。\n-   **连接函数**: 典则 logit 连接函数，$g(\\mu) = \\ln\\!\\left(\\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\\right)$。\n-   **线性预测器**: $\\eta = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z$。\n-   **拟合系数**: 暴露的估计系数为 $\\hat{\\beta}_{1} = 1.1$。\n-   **任务**:\n    1. 在 $Z$ 值固定的情况下，推导比较 $X=1$ 与 $X=0$ 的暴露优势比 (OR)。\n    2. 使用给定的拟合系数计算该 OR 的数值。\n    3. 将结果四舍五入至 $3$ 位有效数字并报告。\n\n### 第 2 步：使用提取的已知条件进行验证\n-   **科学依据**：该问题描述了一个逻辑斯谛回归模型，这是流行病学和生物统计学中分析二元结局的标准且基础的工具。优势、优势比、logit 连接函数和线性预测器等概念都是标准且科学合理的。\n-   **定义明确**：问题定义清晰。它提供了具体的模型结构和推导计算优势比所需的系数。问题明确无歧义，且有唯一、稳定的解。\n-   **客观性**：问题以精确、正式且无偏见的数学和统计语言陈述。\n-   **完整性和一致性**：所提供的信息是充分且自洽的。为了推导所要求的优势比，模型结构已完全指定。没有缺失必要信息，也没有矛盾之处。\n-   **现实性**：一个队列研究检验暴露-疾病关系，同时调整像年龄这样的混杂变量，这是 GLM 在流行病学中一个经典且非常现实的应用场景。系数值 $\\hat{\\beta}_{1} = 1.1$ 对于此类研究中的效应估计来说是一个合理的量级。\n\n### 第 3 步：结论与行动\n该问题被认为是有效的，因为它具有科学依据、定义明确、客观、完整、一致且现实。我将继续提供一个完整且有理有据的解答。\n\n问题的核心是从给定的逻辑斯谛回归模型中推导暴露 $X$ 的优势比 (OR)。该模型通过 logit 连接函数指定了预测变量（$X$, $Z$）与结局概率（$Y=1$）之间的关系。\n\n模型方程表明，疾病的对数优势是预测变量的线性函数：\n$$\n\\ln\\!\\left(\\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\\right) = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z\n$$\n\n事件的优势定义为事件发生的概率除以事件不发生的概率。在此背景下，对于暴露状态为 $X$、年龄为 $Z$ 的个体，其患病优势为：\n$$\n\\text{Odds}(Y=1 \\mid X, Z) = \\frac{\\Pr(Y=1 \\mid X,Z)}{1-\\Pr(Y=1 \\mid X,Z)}\n$$\n\n对模型方程两边取指数，我们可以直接表示优势：\n$$\n\\text{Odds}(Y=1 \\mid X, Z) = \\exp(\\beta_{0} + \\beta_{1} X + \\beta_{2} Z)\n$$\n\n暴露的优势比 (OR) 比较了暴露组 ($X=1$) 的患病优势与未暴露组 ($X=0$) 的患病优势，同时保持所有其他协变量不变。这里，我们将协变量 $Z$ 保持在一个固定的任意值。\nOR 定义为以下比率：\n$$\n\\text{OR} = \\frac{\\text{Odds}(Y=1 \\mid X=1, Z)}{\\text{Odds}(Y=1 \\mid X=0, Z)}\n$$\n\n现在我们将基于模型的优势表达式代入此比率中。\n\n对于暴露组 ($X=1$)，其优势为：\n$$\n\\text{Odds}(Y=1 \\mid X=1, Z) = \\exp(\\beta_{0} + \\beta_{1}(1) + \\beta_{2} Z) = \\exp(\\beta_{0} + \\beta_{1} + \\beta_{2} Z)\n$$\n\n对于未暴露组 ($X=0$)，其优势为：\n$$\n\\text{Odds}(Y=1 \\mid X=0, Z) = \\exp(\\beta_{0} + \\beta_{1}(0) + \\beta_{2} Z) = \\exp(\\beta_{0} + \\beta_{2} Z)\n$$\n\n现在，我们构建比率来求优势比：\n$$\n\\text{OR} = \\frac{\\exp(\\beta_{0} + \\beta_{1} + \\beta_{2} Z)}{\\exp(\\beta_{0} + \\beta_{2} Z)}\n$$\n\n使用指数的性质 $\\frac{\\exp(a)}{\\exp(b)} = \\exp(a-b)$，我们简化该表达式：\n$$\n\\text{OR} = \\exp\\left((\\beta_{0} + \\beta_{1} + \\beta_{2} Z) - (\\beta_{0} + \\beta_{2} Z)\\right)\n$$\n$$\n\\text{OR} = \\exp(\\beta_{0} + \\beta_{1} + \\beta_{2} Z - \\beta_{0} - \\beta_{2} Z)\n$$\n$$\n\\text{OR} = \\exp(\\beta_{1})\n$$\n\n这个推导表明，在没有交互项的逻辑斯谛回归模型中，预测变量单位变化（在本例中，即 $X$ 从 $0$ 变为 $1$）的优势比就是该变量系数的指数，即 $\\exp(\\beta_{1})$。该 OR 在模型中其他协变量（本例中为 $Z$）的所有水平上都是恒定的。\n\n问题提供了暴露的拟合系数 $\\hat{\\beta}_{1} = 1.1$。因此，估计的优势比为：\n$$\n\\widehat{\\text{OR}} = \\exp(\\hat{\\beta}_{1}) = \\exp(1.1)\n$$\n\n我们需要计算这个值并将其四舍五入到 $3$ 位有效数字。\n$$\n\\exp(1.1) \\approx 3.0041660239\n$$\n将此值四舍五入到 $3$ 位有效数字得到 $3.00$。前三位有效数字是 $3$、$0$ 和 $0$。第四位数字是 $4$，小于 $5$，所以我们向下舍入（即保持第三位有效数字不变）。\n\n因此，比较暴露个体与未暴露个体的估计优势比为 $3.00$。",
            "answer": "$$\n\\boxed{3.00}\n$$"
        },
        {
            "introduction": "现代生物信息学和医学数据分析通常需要处理复杂的数据并构建完整的建模流程。本练习将挑战你整合理论知识与编程实践，完成一个端到端的分析任务。你将使用另一种重要的 GLM——用于计数数据的泊松回归，并结合 $\\ell_2$ 正则化（岭回归）和交叉验证等现代机器学习技术，来解决一个贴近现实的生物信息学问题。",
            "id": "5197921",
            "problem": "您的任务是设计、推导和实现一个完整的程序，通过最小化正则化经验风险来估计一个惩罚泊松广义线性模型 (GLM)，并使用交叉验证偏差来评估此程序，同时使用单标准误规则选择正则化参数。您的实现必须是一个单一的可运行程序，不接受任何输入，并打印所需的输出。该设定在医学人工智能和数据科学领域中很常见，其中计数结果（例如，临床事件的计数）使用泊松回归进行建模。\n\n从泊松分布和广义线性模型框架的基本基础出发：\n- 观测值 $i$ 的响应 $y_i$ 服从均值参数为 $\\mu_i$ 的泊松分布，并使用典则对数连接函数，使得 $\\ln(\\mu_i) = \\eta_i$，其中 $\\eta_i = \\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ 且 $\\mathbf{x}_i \\in \\mathbb{R}^p$。\n- 目标是通过最小化一个正则化经验损失来估计截距 $\\alpha$ 和系数 $\\boldsymbol{\\beta}$，该损失函数结合了负对数似然和仅对 $\\boldsymbol{\\beta}$ 施加的 $\\ell_2$ (岭) 惩罚（截距不被惩罚）。\n\n您的任务如下：\n1. 从泊松概率质量函数和 GLM 典则连接的定义出发，推导模型 $y_i \\sim \\mathrm{Poisson}(\\mu_i)$（其中 $\\ln(\\mu_i) = \\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$）的负对数似然，并由此推导出偏差，其定义为饱和模型对数似然与拟合模型对数似然之差的两倍。使用适定极限（在需要时）陈述泊松响应的单个观测值偏差贡献的闭式解。\n2. 推导带有典则连接的无惩罚泊松 GLM 的迭代重加权最小二乘法 (IRLS) 更新步骤，然后修改这些步骤以包含一个排除截距项的 $\\ell_2$ 惩罚 $\\lambda \\lVert \\boldsymbol{\\beta} \\rVert_2^2/2$。清晰地阐明在每个 IRLS 迭代中，对于固定的 $\\lambda$，必须根据工作响应和权重求解的线性系统。\n3. 设计一个用于估计每个观测值的预期样本外偏差的 $K$-折交叉验证估计器。对于给定的 $\\lambda$，交叉验证偏差应为各折中留出集偏差的平均值，标准误应为各折样本标准差除以 $\\sqrt{K}$。定义“最小偏差”选择 $\\lambda_{\\mathrm{min}}$ 和“单标准误”选择 $\\lambda_{\\mathrm{1SE}}$，其中 $\\lambda_{\\mathrm{1SE}}$ 是其平均偏差在最小平均偏差的一个标准误范围内的最大 $\\lambda$。\n4. 实现一个完整的算法，该算法：\n   - 对每一折，使用训练折的均值和标准差对预测变量进行标准化（将零标准差替换为 $1$ 以避免除以零）。不要对响应进行标准化。\n   - 使用 IRLS 和路径上的热启动，在一系列 $\\lambda$ 值网格上拟合惩罚模型。使用不被惩罚的截距。\n   - 计算每个 $\\lambda$ 的交叉验证偏差及其标准误。\n   - 选择 $\\lambda_{\\mathrm{min}}$ 和 $\\lambda_{\\mathrm{1SE}}$，并为下面定义的每个测试案例输出它们。\n   - 对偏差使用数值稳定的计算，尤其是在 $y_i = 0$ 时，并在需要时裁剪线性预测器以避免指数映射中的溢出。\n5. 简洁而精确地讨论该模型中由 $\\ell_2$ 惩罚引起的偏差-方差权衡，并解释为什么单标准误规则是一种偏好更简约（正则化更强）模型的策略，它以可能略微增加的偏差为代价来换取模型方差的降低。\n\n不涉及物理单位。在计算中不要使用百分比；如果需要比例，必须使用小数形式，但这里只需要实值浮点数。\n\n测试套件：\n实现您的程序，根据以下规范生成合成数据，然后为每个案例运行您的交叉验证和选择程序。对于每个测试案例，程序必须输出配对 $[\\lambda_{\\mathrm{min}}, \\lambda_{\\mathrm{1SE}}]$。\n\n- 案例 A (理想路径，中等信号):\n  - $n=200$， $p=5$， 种子 $42$。\n  - 真实截距 $\\alpha = 1.0$，真实系数 $\\boldsymbol{\\beta} = [0.2, -0.5, 0.0, 0.3, 0.7]$。\n  - 预测变量 $\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_p)$ 独立同分布，响应 $y_i \\sim \\mathrm{Poisson}(\\exp(\\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}))$ 独立同分布。\n  - 交叉验证: $K=5$ 折，确定性的折分配通过使用给定种子进行洗牌，然后划分为连续的块来定义。\n  - 正则化网格: $\\lambda \\in \\{\\lambda_j\\}_{j=1}^{20}$ 在 $10^{-3}$ 到 $10^{1}$ 之间对数间隔取值。\n\n- 案例 B (边界情况，含大量零值，均值较小，轻度稀疏):\n  - $n=120$， $p=8$， 种子 $123$。\n  - 真实截距 $\\alpha = 0.3$，真实系数 $\\boldsymbol{\\beta} = [-0.8, 0.0, 0.0, 0.5, 0.0, 0.2, 0.0, 0.0]$。\n  - 预测变量和响应生成方式同案例 A。\n  - 交叉验证: $K=6$ 折，同上。\n  - 正则化网格: $\\lambda \\in \\{\\lambda_j\\}_{j=1}^{30}$ 在 $10^{-4}$ 到 $10^{2}$ 之间对数间隔取值。\n\n- 案例 C (高维, $p > n$, 需要强正则化):\n  - $n=60$， $p=80$， 种子 $777$。\n  - 真实截距 $\\alpha = 1.5$，真实系数 $\\boldsymbol{\\beta}$ 均为零，除了在固定索引 $[0, 5, 10, 20, 50]$ 处的 5 个非零项，其值为 $[0.7, -0.6, 0.5, 0.4, -0.3]$。\n  - 预测变量和响应生成方式同案例 A。\n  - 交叉验证: $K=5$ 折，同上。\n  - 正则化网格: $\\lambda \\in \\{\\lambda_j\\}_{j=1}^{25}$ 在 $10^{-3}$ 到 $10^{3}$ 之间对数间隔取值。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。具体来说，它必须打印一个包含三个列表的列表，每个内部列表包含两个浮点数值，四舍五入到六位小数，并按测试案例的顺序排列。所需格式为：\n[[lambda_min_A,lambda_1se_A],[lambda_min_B,lambda_1se_B],[lambda_min_C,lambda_1se_C]]",
            "solution": "所提出的问题要求推导、设计和实现一个用于拟合惩罚泊松广义线性模型 (GLM) 并通过交叉验证选择正则化参数的程序。这是统计机器学习中一个标准且定义明确的问题。\n\n假设观测值 $i$ 的响应变量 $y_i$ 服从泊松分布，$y_i \\sim \\mathrm{Poisson}(\\mu_i)$。均值 $\\mu_i$ 通过典则对数连接函数与一组预测变量 $\\mathbf{x}_i \\in \\mathbb{R}^p$ 相关联，使得 $\\ln(\\mu_i) = \\eta_i = \\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$。这里，$\\alpha$ 是截距，$\\boldsymbol{\\beta}$ 是一个包含 $p$ 个系数的向量。\n\n参数 $\\alpha$ 和 $\\boldsymbol{\\beta}$ 通过最小化一个惩罚负对数似然函数来估计。惩罚项是施加在系数 $\\boldsymbol{\\beta}$ 上的 $\\ell_2$ (岭) 项，而不施加在截距 $\\alpha$ 上。要最小化的目标函数是 $J(\\alpha, \\boldsymbol{\\beta}) = -\\mathcal{L}(\\alpha, \\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\lVert \\boldsymbol{\\beta} \\rVert_2^2$，其中 $\\mathcal{L}$ 是对数似然，$\\lambda$ 是正则化参数。\n\n**1. 负对数似然和偏差的推导**\n\n来自均值为 $\\mu_i$ 的泊松分布的单个观测值 $y_i$ 的概率质量函数 (PMF) 为 $P(y_i ; \\mu_i) = \\frac{e^{-\\mu_i} \\mu_i^{y_i}}{y_i!}$。该观测值的对数似然为 $\\ell_i(\\mu_i; y_i) = \\ln P(y_i ; \\mu_i) = y_i \\ln(\\mu_i) - \\mu_i - \\ln(y_i!)$。使用连接函数 $\\mu_i = e^{\\eta_i} = e^{\\alpha + \\mathbf{x}_i^\\top \\boldsymbol{\\beta}}$，对数似然变为 $\\ell_i(\\alpha, \\boldsymbol{\\beta}; y_i, \\mathbf{x}_i) = y_i \\eta_i - e^{\\eta_i} - \\ln(y_i!)$。\n\n对于一个包含 $n$ 个独立观测值的数据集，总对数似然是 $\\mathcal{L}(\\alpha, \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\ell_i = \\sum_{i=1}^n (y_i \\eta_i - e^{\\eta_i} - \\ln(y_i!))$ 的总和。负对数似然 (NLL) 为 $-\\mathcal{L}(\\alpha, \\boldsymbol{\\beta}) = \\sum_{i=1}^n (e^{\\eta_i} - y_i \\eta_i + \\ln(y_i!))$。由于 $\\ln(y_i!)$ 相对于参数是常数，最小化 NLL 等价于最小化 $\\sum_{i=1}^n (e^{\\eta_i} - y_i \\eta_i)$。\n\nGLM 的偏差定义为 $D = 2(\\mathcal{L}_{\\text{sat}} - \\mathcal{L}_{\\text{fit}})$，其中 $\\mathcal{L}_{\\text{sat}}$ 是饱和模型的对数似然，$\\mathcal{L}_{\\text{fit}}$ 是拟合模型的对数似然。在饱和模型中，每个观测值的均值参数 $\\tilde{\\mu}_i$ 被设置为与数据完美匹配，对于泊松分布，即为 $\\tilde{\\mu}_i = y_i$。饱和模型的对数似然是 $\\mathcal{L}_{\\text{sat}} = \\sum_{i=1}^n (y_i \\ln(y_i) - y_i - \\ln(y_i!))$。拟合模型的均值为 $\\hat{\\mu}_i$，由 GLM 估计得出，其对数似然为 $\\mathcal{L}_{\\text{fit}} = \\sum_{i=1}^n (y_i \\ln(\\hat{\\mu}_i) - \\hat{\\mu}_i - \\ln(y_i!))$。\n\n因此，总偏差为：\n$$ D = 2 \\left[ \\sum_{i=1}^n (y_i \\ln(y_i) - y_i) - \\sum_{i=1}^n (y_i \\ln(\\hat{\\mu}_i) - \\hat{\\mu}_i) \\right] = 2 \\sum_{i=1}^n \\left( y_i \\ln\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - (y_i - \\hat{\\mu}_i) \\right) $$\n单个观测值的偏差贡献是 $d_i = 2 ( y_i \\ln(y_i/\\hat{\\mu}_i) - (y_i - \\hat{\\mu}_i) )$。对于 $y_i=0$ 的情况，我们使用适定极限 $\\lim_{y \\to 0} y \\ln(y) = 0$。$y_i=0$ 的观测值的偏差贡献简化为 $d_i = 2\\hat{\\mu}_i$。\n\n**2. 惩罚迭代重加权最小二乘法 (IRLS) 的推导**\n\nIRLS 算法是用于寻找 GLM 最大似然估计的一种程序，等价于 Newton-Raphson 方法。我们将其扩展到惩罚情况。设 $\\boldsymbol{\\theta} = [\\alpha, \\boldsymbol{\\beta}^\\top]^\\top$ 为 $(p+1)$ 维参数向量，$\\mathbf{X}^*$ 为 $n \\times (p+1)$ 的设计矩阵，其第一列为全1，因此 $\\boldsymbol{\\eta} = \\mathbf{X}^* \\boldsymbol{\\theta}$。\n\n要最小化的目标函数是 $J(\\boldsymbol{\\theta}) = \\sum_{i=1}^n (e^{\\eta_i} - y_i \\eta_i) + \\frac{\\lambda}{2} \\boldsymbol{\\beta}^\\top \\boldsymbol{\\beta}$。\n在第 $(t)$ 次迭代中，$\\boldsymbol{\\theta}$ 的 Newton-Raphson 更新为 $\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - (\\mathbf{H}_J^{(t)})^{-1} \\nabla J^{(t)}$，其中 $\\nabla J$ 是目标函数的梯度，$\\mathbf{H}_J$ 是其 Hessian 矩阵。\n\nNLL 部分的梯度是 $\\nabla(-\\mathcal{L}) = (\\mathbf{X}^*)^\\top(\\boldsymbol{\\mu} - \\mathbf{y})$。NLL 部分的 Hessian 矩阵是 $\\mathbf{H}_{-\\mathcal{L}} = (\\mathbf{X}^*)^\\top \\mathbf{W} \\mathbf{X}^*$，其中 $\\mathbf{W}$ 是一个对角矩阵，其元素为 $W_{ii} = \\mu_i$。\n设 $\\mathbf{\\Lambda}$ 是一个 $(p+1) \\times (p+1)$ 的对角矩阵，其中 $\\Lambda_{00}=0$ 且对于 $j=1, \\dots, p$，$\\Lambda_{jj}=\\lambda$。惩罚项的梯度是 $\\mathbf{\\Lambda}\\boldsymbol{\\theta}$，其 Hessian 矩阵是 $\\mathbf{\\Lambda}$。\n总梯度是 $\\nabla J(\\boldsymbol{\\theta}) = (\\mathbf{X}^*)^\\top(\\boldsymbol{\\mu} - \\mathbf{y}) + \\mathbf{\\Lambda}\\boldsymbol{\\theta}$。\n总 Hessian 矩阵是 $\\mathbf{H}_J(\\boldsymbol{\\theta}) = (\\mathbf{X}^*)^\\top \\mathbf{W} \\mathbf{X}^* + \\mathbf{\\Lambda}$。\n\n将这些代入 Newton-Raphson 公式并重新排列，得到 IRLS 更新。在每一步中，我们在以下线性系统中求解 $\\boldsymbol{\\theta}^{(t+1)}$：\n$$ (\\mathbf{H}_J^{(t)}) \\boldsymbol{\\theta}^{(t+1)} = (\\mathbf{H}_J^{(t)}) \\boldsymbol{\\theta}^{(t)} - \\nabla J^{(t)} $$\n$$ \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t+1)} = \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t)} - \\left((\\mathbf{X}^*)^{\\top}(\\boldsymbol{\\mu}^{(t)} - \\mathbf{y}) + \\mathbf{\\Lambda}\\boldsymbol{\\theta}^{(t)}\\right) $$\n$$ \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t+1)} = (\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* \\boldsymbol{\\theta}^{(t)} + (\\mathbf{X}^*)^{\\top}(\\mathbf{y} - \\boldsymbol{\\mu}^{(t)}) $$\n这可以表示为一个惩罚加权最小二乘问题，通过定义一个“工作响应”向量 $\\mathbf{z}^{(t)}$，其分量为 $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}}$。更新方程的右侧等价于 $(\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{z}^{(t)}$。因此，每次迭代需要求解的线性系统是：\n$$ \\left((\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{X}^* + \\mathbf{\\Lambda}\\right) \\boldsymbol{\\theta}^{(t+1)} = (\\mathbf{X}^*)^{\\top} \\mathbf{W}^{(t)} \\mathbf{z}^{(t)} $$\n\n**3. 交叉验证设计和 Lambda 选择**\n\n采用 $K$-折交叉验证程序来估计使用不同 $\\lambda$ 值拟合的模型的样本外偏差。\n1. 数据被划分为 $K$ 个不相交的折。\n2. 对每一折 $k=1, \\dots, K$，模型在剩余的 $K-1$ 折上进行训练，并在第 $k$ 折上进行验证。\n3. 对于每次训练/验证集划分，训练集中的预测变量被标准化为零均值和单位方差。计算出的均值和标准差随后用于标准化验证集的预测变量。\n4. 对于预定义网格中的每个 $\\lambda$，在训练数据上拟合一个模型，并在验证数据上计算平均每个观测值的偏差。记为 $d_k(\\lambda)$。\n5. 遍历所有折后，每个 $\\lambda$ 的交叉验证偏差是各折特定偏差的平均值：$\\mathrm{CV}(\\lambda) = \\frac{1}{K}\\sum_{k=1}^K d_k(\\lambda)$。\n6. 标准误也一并计算：$\\mathrm{SE}(\\lambda) = \\sqrt{\\frac{1}{K(K-1)} \\sum_{k=1}^K (d_k(\\lambda) - \\mathrm{CV}(\\lambda))^2}$。\n7. 最优的 $\\lambda$ 值根据两条规则选择：\n   - $\\lambda_{\\mathrm{min}}$：产生最小交叉验证偏差的 $\\lambda$ 值，$\\mathrm{CV}_{\\min} = \\mathrm{CV}(\\lambda_{\\mathrm{min}})$。\n   - $\\lambda_{\\mathrm{1SE}}$：使得交叉验证偏差在最小值的一个标准误范围内的最大 $\\lambda$ 值（正则化最强），即 $\\mathrm{CV}(\\lambda) \\le \\mathrm{CV}_{\\min} + \\mathrm{SE}(\\lambda_{\\min})$。\n\n**4. 实现说明**\n\n实现需要仔细处理数值细节。线性预测器 $\\eta_i$ 在应用指数函数之前应进行裁剪，以防止溢出。热启动，即使用给定 $\\lambda$ 的解作为网格中下一个 $\\lambda$ 的初始猜测，可以显著加速 IRLS 算法的收敛。\n\n**5. 偏差-方差权衡与单标准误规则**\n\n$\\ell_2$ 惩罚引入了偏差-方差权衡。通过将系数估计 $\\boldsymbol{\\beta}$ 向零收缩，惩罚为模型增加了偏差，因为真实的系数不太可能为零。然而，这种收缩降低了估计的方差，使模型对特定的训练数据样本不那么敏感。对于一个最优的 $\\lambda$，方差的减少超过了偏差的增加，从而提高了在未见数据上的预测性能。单标准误规则是一种启发式方法，它偏爱更简单、正则化更强的模型。它选择具有统计上与交叉验证所识别的最佳模型性能无异的最简约模型（$\\lambda_{\\mathrm{1SE}} \\ge \\lambda_{\\min}$）。这是一种旨在降低模型方差的策略，因为它接受了预测误差（偏差）的微小、统计上不显著的增加，以换取一个可能更稳定和泛化能力更强的模型（方差更低）。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import xlogy\n\ndef make_beta_c():\n    \"\"\"Creates the specific beta vector for Test Case C.\"\"\"\n    beta = np.zeros(80)\n    indices = [0, 5, 10, 20, 50]\n    values = [0.7, -0.6, 0.5, 0.4, -0.3]\n    beta[indices] = values\n    return beta\n\ndef generate_data(spec):\n    \"\"\"Generates synthetic data for a given test case specification.\"\"\"\n    rng = np.random.default_rng(spec['seed'])\n    X = rng.normal(size=(spec['n'], spec['p']))\n    eta = spec['alpha'] + X @ spec['beta']\n    mu = np.exp(np.clip(eta, -30, 30)) # Clip for numerical stability\n    y = rng.poisson(mu)\n    return X, y\n\ndef create_folds(n, K, seed):\n    \"\"\"Creates deterministic K-fold cross-validation indices.\"\"\"\n    rng = np.random.default_rng(seed)\n    indices = np.arange(n)\n    rng.shuffle(indices)\n    fold_indices = np.array_split(indices, K)\n    folds = []\n    for k in range(K):\n        val_idx = fold_indices[k]\n        train_idx = np.concatenate([fold_indices[i] for i in range(K) if i != k])\n        folds.append((train_idx, val_idx))\n    return folds\n\ndef standardize(X_train, X_val):\n    \"\"\"Standardizes training and validation sets based on training set statistics.\"\"\"\n    means = np.mean(X_train, axis=0)\n    stds = np.std(X_train, axis=0)\n    stds[stds == 0] = 1.0  # As per problem, avoid division by zero\n    X_train_std = (X_train - means) / stds\n    X_val_std = (X_val - means) / stds\n    return X_train_std, X_val_std\n\ndef poisson_deviance(y, mu):\n    \"\"\"Computes the per-observation Poisson deviance.\"\"\"\n    mu = np.maximum(mu, 1e-15)  # Avoid log(0)\n    # Deviance = 2 * (y*log(y/mu) - (y-mu))\n    term1 = xlogy(y, y) - xlogy(y, mu)\n    term2 = mu - y\n    return 2 * (term1 + term2)\n\ndef fit_penalized_poisson(X_aug, y, lam, initial_theta, max_iter=100, tol=1e-7):\n    \"\"\"\n    Fits a penalized Poisson GLM using Iteratively Reweighted Least Squares (IRLS).\n    \"\"\"\n    p_plus_1 = X_aug.shape[1]\n    theta = np.copy(initial_theta)\n    \n    # Penalty matrix (intercept is not penalized)\n    Lambda = np.diag([0.0] + [lam] * (p_plus_1 - 1))\n    \n    for _ in range(max_iter):\n        eta = X_aug @ theta\n        eta = np.clip(eta, -30, 30) # Prevent overflow/underflow\n        mu = np.exp(eta)\n        \n        # Ensure mu is positive to avoid division by zero in working response\n        mu = np.maximum(mu, 1e-8)\n        \n        W_diag = mu  # Diagonal of the weight matrix W\n        z = eta + (y - mu) / mu  # Working response\n        \n        # Efficiently compute (X_aug.T @ W @ X_aug + Lambda) and (X_aug.T @ W @ z)\n        X_t_W = X_aug.T * W_diag  # Broadcasting for X_aug.T @ W\n        \n        A = X_t_W @ X_aug + Lambda\n        b = X_t_W @ z\n        \n        try:\n            theta_new = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Fallback for ill-conditioned matrices\n            theta_new = np.linalg.pinv(A) @ b\n            \n        # Check for convergence using relative change\n        if np.linalg.norm(theta_new - theta) / (np.linalg.norm(theta) + 1e-8)  tol:\n            theta = theta_new\n            break\n            \n        theta = theta_new\n        \n    return theta\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {\n            'n': 200, 'p': 5, 'seed': 42, 'alpha': 1.0, \n            'beta': np.array([0.2, -0.5, 0.0, 0.3, 0.7]), \n            'K': 5, 'lambda_grid': np.logspace(-3, 1, 20)\n        },\n        {\n            'n': 120, 'p': 8, 'seed': 123, 'alpha': 0.3, \n            'beta': np.array([-0.8, 0.0, 0.0, 0.5, 0.0, 0.2, 0.0, 0.0]), \n            'K': 6, 'lambda_grid': np.logspace(-4, 2, 30)\n        },\n        {\n            'n': 60, 'p': 80, 'seed': 777, 'alpha': 1.5, \n            'beta': make_beta_c(), \n            'K': 5, 'lambda_grid': np.logspace(-3, 3, 25)\n        }\n    ]\n    \n    all_final_results = []\n\n    for case in test_cases:\n        X, y = generate_data(case)\n        folds = create_folds(case['n'], case['K'], case['seed'])\n        \n        num_lambdas = len(case['lambda_grid'])\n        fold_deviances = np.zeros((case['K'], num_lambdas))\n\n        for k in range(case['K']):\n            train_idx, val_idx = folds[k]\n            X_train, y_train = X[train_idx], y[train_idx]\n            X_val, y_val = X[val_idx], y[val_idx]\n            \n            X_train_std, X_val_std = standardize(X_train, X_val)\n            \n            X_train_aug = np.c_[np.ones(X_train_std.shape[0]), X_train_std]\n            X_val_aug = np.c_[np.ones(X_val_std.shape[0]), X_val_std]\n            \n            # Use warm starts for coefficients across the lambda path\n            theta = np.zeros(case['p'] + 1)\n            \n            for j, lam in enumerate(case['lambda_grid']):\n                theta = fit_penalized_poisson(X_train_aug, y_train, lam, initial_theta=theta)\n                \n                eta_val = X_val_aug @ theta\n                mu_val = np.exp(np.clip(eta_val, -30, 30))\n                \n                dev = poisson_deviance(y_val, mu_val)\n                fold_deviances[k, j] = np.mean(dev)\n\n        mean_cv_deviances = np.mean(fold_deviances, axis=0)\n        std_err_cv_deviances = np.std(fold_deviances, axis=0, ddof=1) / np.sqrt(case['K'])\n        \n        # Select lambda_min\n        lambda_min_idx = np.argmin(mean_cv_deviances)\n        lambda_min = case['lambda_grid'][lambda_min_idx]\n        \n        # Select lambda_1se\n        min_dev_val = mean_cv_deviances[lambda_min_idx]\n        min_dev_se = std_err_cv_deviances[lambda_min_idx]\n        threshold = min_dev_val + min_dev_se\n        \n        eligible_indices = np.where(mean_cv_deviances = threshold)[0]\n        # The 1SE rule takes the largest lambda (most regularized) from this set\n        lambda_1se_idx = np.max(eligible_indices)\n        lambda_1se = case['lambda_grid'][lambda_1se_idx]\n        \n        all_final_results.append([lambda_min, lambda_1se])\n\n    case_results_str = []\n    for res_pair in all_final_results:\n        case_results_str.append(f\"[{res_pair[0]:.6f},{res_pair[1]:.6f}]\")\n    \n    # Print in the exact required format\n    print(f\"[{','.join(case_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}