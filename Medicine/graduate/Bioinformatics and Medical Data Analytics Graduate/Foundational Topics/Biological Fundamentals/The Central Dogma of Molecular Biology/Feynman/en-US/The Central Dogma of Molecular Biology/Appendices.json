{
    "hands_on_practices": [
        {
            "introduction": "The Central Dogma begins with the faithful replication of DNA, a process that ensures the stable inheritance of genetic information across cell generations. While remarkably accurate, this process is not perfect. This exercise  invites you to build a quantitative model from first principles to estimate the overall mutation rate, integrating the successive layers of error-correction, from polymerase proofreading to mismatch repair. By calculating the expected number of mutations per cell division, you will develop a concrete appreciation for the balance between the immense scale of the genome and the high fidelity of its maintenance.",
            "id": "2965554",
            "problem": "A eukaryotic cell replicates a linear genome comprising $3.2 \\times 10^{9}$ nucleotide positions during each cell division. At each position, the deoxyribonucleic acid (DNA) polymerase misincorporates a nucleotide with marginal probability $\\epsilon_{\\text{pol}} = 10^{-5}$. Conditional on a misincorporation event having occurred at the 3-prime terminus, the exonuclease proofreading mechanism attempts to remove the mismatch; the conditional probability that proofreading fails to correct the mismatch is $\\epsilon_{\\text{proof}} = 10^{-2}$. Conditional on a post-replicative mismatch persisting, the mismatch repair (MMR) system attempts to correct it; the conditional probability that mismatch repair fails is $\\epsilon_{\\text{MMR}} = 10^{-1}$. Assume independence of error events across positions and conditional independence of correction failures given the presence of a mismatch. A base substitution becomes a fixed mutation only if a misincorporation occurs and both correction systems fail before the next replication cycle.\n\nWithin the framework of the Central Dogma of Molecular Biology, where high-fidelity replication preserves information flow from DNA to DNA, use only the definitions above and first principles of probability and expectation to derive an expression for the expected number of replication-induced base substitutions per cell division across the $3.2 \\times 10^{9}$ positions. Then evaluate this expectation numerically using the given parameter values. Report the final number of substitutions per division as an exact number (no rounding, no units).\n\nFinally, based on your result and established qualitative knowledge of replication fidelity in normal human somatic cells with intact proofreading and mismatch repair, briefly assess in words whether the magnitude you computed is biologically plausible, and justify your reasoning qualitatively (your qualitative assessment will not affect the numerical answer requirement).",
            "solution": "The problem requires the derivation and calculation of the expected number of base substitutions that become fixed in a genome after one cycle of replication. This quantity is determined by the interplay of error introduction by DNA polymerase and subsequent correction by two major repair systems: proofreading and mismatch repair. The foundation of the solution lies in probability theory, specifically the calculation of the probability of a sequence of dependent events, and the application of the linearity of expectation.\n\nLet $N$ be the total number of nucleotide positions in the genome, which is given as $N = 3.2 \\times 10^{9}$.\nA base substitution becomes a fixed mutation at a single position if and only if a sequence of three events occurs: an initial misincorporation, a failure of proofreading, and a subsequent failure of mismatch repair. We must calculate the probability of this compound event.\n\nLet us define the events for a single nucleotide position:\n- $M$: The event that DNA polymerase misincorporates a nucleotide. The probability is given as $P(M) = \\epsilon_{\\text{pol}} = 10^{-5}$.\n- $F_P$: The event that the exonuclease proofreading mechanism fails to correct the mismatch. This is a conditional event, dependent on the occurrence of a misincorporation. The problem provides the conditional probability $P(F_P | M) = \\epsilon_{\\text{proof}} = 10^{-2}$.\n- $F_{MMR}$: The event that the mismatch repair (MMR) system fails to correct the mismatch. This event is conditional on a mismatch having persisted through both replication and proofreading, i.e., conditional on the event $M \\cap F_P$. The problem provides the conditional probability $P(F_{MMR} | M \\cap F_P) = \\epsilon_{\\text{MMR}} = 10^{-1}$.\n\nThe probability of a single fixed substitution, which we denote as $\\mu$, is the probability of the intersection of these three events: $\\mu = P(M \\cap F_P \\cap F_{MMR})$. Using the chain rule for probabilities, we can express $\\mu$ as follows:\n$$ \\mu = P(M) \\times P(F_P | M) \\times P(F_{MMR} | M \\cap F_P) $$\nThe problem statement provides all the necessary probabilities on the right-hand side. Substituting the given symbolic parameters:\n$$ \\mu = \\epsilon_{\\text{pol}} \\cdot \\epsilon_{\\text{proof}} \\cdot \\epsilon_{\\text{MMR}} $$\nThis expression represents the probability of a fixed mutation arising at any single nucleotide position during one cell division.\n\nNow, we must find the expected total number of such mutations across the entire genome of $N$ positions. Let $X$ be a random variable representing the total number of substitutions per cell division. We can define $N$ indicator random variables, $X_1, X_2, \\dots, X_N$, where $X_i = 1$ if a fixed substitution occurs at position $i$, and $X_i = 0$ otherwise. The probability that $X_i = 1$ is precisely $\\mu$, as calculated above.\nThe total number of substitutions is the sum of these indicator variables: $X = \\sum_{i=1}^{N} X_i$.\n\nBy the linearity of expectation, the expected value of a sum of random variables is the sum of their individual expected values:\n$$ E[X] = E\\left[\\sum_{i=1}^{N} X_i\\right] = \\sum_{i=1}^{N} E[X_i] $$\nThe expectation of an indicator variable $X_i$ is equal to the probability of the event it indicates: $E[X_i] = 1 \\cdot P(X_i=1) + 0 \\cdot P(X_i=0) = P(X_i=1) = \\mu$.\nSince the probability of mutation $\\mu$ is assumed to be identical for all positions, the sum simplifies to:\n$$ E[X] = \\sum_{i=1}^{N} \\mu = N \\cdot \\mu $$\nThus, the final expression for the expected number of substitutions is:\n$$ E[X] = N \\cdot \\epsilon_{\\text{pol}} \\cdot \\epsilon_{\\text{proof}} \\cdot \\epsilon_{\\text{MMR}} $$\nWe now proceed to the numerical evaluation using the given parameter values:\n$N = 3.2 \\times 10^{9}$\n$\\epsilon_{\\text{pol}} = 10^{-5}$\n$\\epsilon_{\\text{proof}} = 10^{-2}$\n$\\epsilon_{\\text{MMR}} = 10^{-1}$\n\nFirst, we calculate the single-site mutation probability, $\\mu$:\n$$ \\mu = (10^{-5}) \\times (10^{-2}) \\times (10^{-1}) = 10^{-5-2-1} = 10^{-8} $$\nThis is the per-site, per-replication mutation rate.\n\nNext, we calculate the expected total number of substitutions, $E[X]$:\n$$ E[X] = (3.2 \\times 10^{9}) \\times (10^{-8}) = 3.2 \\times 10^{9-8} = 3.2 \\times 10^{1} = 32 $$\nThe expected number of replication-induced base substitutions per cell division is therefore $32$.\n\nFinally, we assess the biological plausibility of this result. The calculated overall mutation rate of $\\mu = 10^{-8}$ mutations per base pair per replication cycle is a textbook value, highly consistent with measurements of the germline mutation rate in humans and other mammals. The resulting expectation of $32$ new mutations for a genome of size $3.2 \\times 10^{9}$ base pairs is also well within the range of established biological estimates. Empirical studies of *de novo* mutations in humans, which accumulate over cell divisions, report tens of new mutations per generation. An expectation of $32$ mutations arising from a single round of replication of the genome is therefore a scientifically sound and plausible magnitude. It correctly quantifies the outcome of a process where an immense number of nucleotides are copied with extraordinarily high, yet imperfect, fidelity. The Central Dogma's tenet of information preservation from DNA to DNA is maintained with high fidelity, but this calculation demonstrates that it is not absolute.",
            "answer": "$$\n\\boxed{32}\n$$"
        },
        {
            "introduction": "The translation of messenger RNA into protein is governed by the genetic code, often presented as a universal dictionary. However, the Central Dogma is implemented with biological machinery that exhibits evolutionary variation. This practice  explores the crucial implications of this fact by examining a case of horizontal gene transfer between organisms with different genetic codes. You will reason through the consequences at the molecular level, reinforcing the principle that genetic information is not absolute but is interpreted in the context of a specific cellular environment.",
            "id": "1779314",
            "problem": "A researcher investigates horizontal gene transfer between prokaryotes and eukaryotes by studying the expression of a bacterial antibiotic resistance gene in a ciliate protist. The chosen gene is the *tet(A)* gene from *Escherichia coli*, which confers resistance to tetracycline. The protein product, TET(A), is a membrane-bound efflux pump consisting of 399 amino acids. In *Escherichia coli*, the translation of the *tet(A)* messenger Ribonucleic Acid (mRNA) is terminated by a UAG stop codon. The coding sequence of the gene, excluding the stop codon, is known to be free of any UAA or UAG codons.\n\nThis *tet(A)* gene is successfully transferred and integrated into the genome of the ciliate *Stentor coeruleus*. This particular ciliate possesses an alternative genetic code where the codons UAA and UAG both code for the amino acid glutamine (Gln). The sole stop codon in *Stentor coeruleus* is UGA. Assume the transferred gene is correctly transcribed and its mRNA is available for translation by the ciliate's ribosomes. The mRNA transcript produced in the ciliate contains an in-frame UGA codon located 25 codons downstream from the original UAG codon position.\n\nWhich of the following statements best describes the protein product synthesized from the *tet(A)* gene in *Stentor* and the resulting phenotype?\n\nA. A functional TET(A) protein of 399 amino acids is produced, conferring tetracycline resistance to *Stentor*.\nB. The *tet(A)* mRNA is rapidly degraded by the ciliate's nonsense-mediated decay pathway because it contains what the machinery perceives as a premature termination codon.\nC. A protein of 400 amino acids is produced, with a C-terminal glutamine. This protein is non-functional because it cannot be properly released from the ribosome, but it does not affect the ciliate's viability.\nD. An elongated protein of 424 amino acids is produced due to translational read-through of the original stop codon. This protein is non-functional, and the ciliate does not gain resistance.\nE. Translation terminates prematurely at an unexpected internal site, resulting in a short, non-functional peptide, as the ciliate ribosome is incompatible with the bacterial mRNA structure.",
            "solution": "The starting point is the bacterial gene *tet(A)* from *Escherichia coli*, which encodes a 399-amino-acid membrane efflux pump, TET(A). In *E. coli*, translation terminates at a UAG stop codon immediately after the 399th amino acid. The coding region prior to that stop is specified to be free of any UAA or UAG codons.\n\nIn the ciliate *Stentor coeruleus*, the nuclear genetic code is different: UAA and UAG encode glutamine (Gln), and UGA is the only stop codon. Therefore, when the same mRNA sequence is translated by *Stentor* ribosomes, the codon that is UAG in the *E. coli* context will not terminate translation; instead, it will be read as glutamine. Consequently, translation does not stop at the position where *E. coli* would stop; it continues in-frame beyond that point.\n\nThe problem further specifies that the mRNA in *Stentor* contains an in-frame UGA codon located 25 codons downstream from the original UAG position. Because UGA is the sole stop codon in *Stentor*, translation will continue from the recoded UAG (now Gln) until encountering this UGA and then terminate properly.\n\nLet $L_{E}$ denote the length of the native *E. coli* TET(A) protein and $L_{S}$ the length of the protein synthesized in *Stentor*. We are told $L_{E} = 399$. The recoding of the original UAG as Gln adds one amino acid at that position, and the presence of an in-frame UGA 25 codons downstream means that a total of 25 additional amino acids will be appended before termination. Thus,\n$$\nL_{S} = 399 + 25 = 424.\n$$\nTherefore, the product in *Stentor* is an elongated protein of 424 amino acids.\n\nEvaluating the answer choices:\n- A is incorrect because the protein will not be 399 amino acids; the UAG is read as Gln and translation continues to the downstream UGA.\n- B is unlikely because there is no premature termination codon relative to *Stentor’s* translational code; termination occurs at a bona fide stop (UGA) after translation has extended the open reading frame. Moreover, the prompt assumes proper transcription and translation initiation, and does not introduce exon–exon junction contexts that would favor nonsense-mediated decay.\n- C is incorrect because translation will not terminate at 400 amino acids; a downstream UGA is present 25 codons further in-frame, allowing normal release at that point.\n- D correctly states that an elongated protein of 424 amino acids will be produced due to continued translation past the former stop (UAG now coding Gln) until the UGA stop. Given TET(A) is a tightly structured membrane transporter, a C-terminal extension of 25 residues is very likely to disrupt folding, membrane topology, or function, so the protein is expected to be non-functional and not confer tetracycline resistance.\n- E is incorrect; the prompt explicitly assumes correct transcription and availability for translation, and there is no basis for an unexpected premature internal termination due to incompatibility with bacterial mRNA structure.\n\nThus, the best description is that an elongated, non-functional 424-amino-acid protein is produced, and the ciliate does not gain resistance.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "Modern biology applies the principles of the Central Dogma at a genomic scale, using high-throughput data to understand the regulation of thousands of genes simultaneously. This hands-on exercise  moves into this domain of quantitative systems biology and data analytics. You will implement a regression model to dissect the relationship between transcription factor binding and transcription rates, but with a critical twist—you must also diagnose and quantify the impact of a confounding variable, a ubiquitous challenge in the analysis of real-world 'omics data.",
            "id": "4613289",
            "problem": "Consider the Central Dogma of Molecular Biology, which states that deoxyribonucleic acid (DNA) is transcribed into ribonucleic acid (RNA) and then translated into protein. At the level of transcription, for a gene indexed by $i$, let $r_i$ denote its transcription initiation rate (molecules per cell per unit time), $m_i$ its steady-state messenger ribonucleic acid (mRNA) abundance (molecules per cell), and $d_i$ its first-order degradation rate (per unit time). A well-tested steady-state relationship for mRNA kinetics is given by the differential equation\n$$\\frac{dm_i}{dt} = r_i - d_i m_i,$$\nwhich at steady state ($\\frac{dm_i}{dt}=0$) implies\n$$r_i = d_i m_i.$$\nAssume that properly normalized ribonucleic acid sequencing (RNA-seq) measurements $E_i$ are proportional to $m_i$. For this problem, take $E_i$ to be in the same arbitrary units as $m_i$ so that $E_i = m_i$. Assume Chromatin Immunoprecipitation followed by sequencing (ChIP-seq) provides a scalar occupancy feature $O_i$ that summarizes transcription factor binding near the gene promoter, and chromatin accessibility $A_i$ (for example from Assay for Transposase-Accessible Chromatin using sequencing, ATAC-seq) may act as a confounder that is associated with both $O_i$ and $r_i$. The latent transcription rate is modeled as\n$$\\log r_i = \\alpha + \\beta_O O_i + \\beta_A A_i + \\varepsilon_i,$$\nwhere $\\varepsilon_i$ are independent and identically distributed with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nYou are to implement a program that, given simulated arrays $(O_i,A_i,E_i,d_i)$ for $i=1,\\dots,N$, will:\n- Use the steady-state relationship to infer $\\hat{r}_i$ from $E_i$ and $d_i$ via $\\hat{r}_i = d_i E_i$.\n- Fit two linear models by ordinary least squares (OLS):\n  1. Model $\\mathcal{M}_1$: regress $y_i = \\log \\hat{r}_i$ on $O_i$ with an intercept.\n  2. Model $\\mathcal{M}_2$: regress $y_i = \\log \\hat{r}_i$ on $O_i$ and $A_i$ with an intercept.\n- Use the first $\\lfloor 0.7N \\rfloor$ genes as the training set and the remaining genes as the test set for prediction evaluation. Angles are not used, so no angle units are required. No physical unit conversions are required in the output; all requested outputs are dimensionless.\n- Compute the following quantities for each test case:\n  1. The root mean squared error (RMSE) on the test set for $\\mathcal{M}_1$:\n     $$\\mathrm{RMSE}_1 = \\sqrt{\\frac{1}{n_{\\text{test}}} \\sum_{i \\in \\text{test}} \\left(y_i - \\hat{y}_{1,i}\\right)^2},$$\n     where $\\hat{y}_{1,i}$ are the predictions from $\\mathcal{M}_1$ and $n_{\\text{test}}$ is the number of test genes.\n  2. The RMSE on the test set for $\\mathcal{M}_2$:\n     $$\\mathrm{RMSE}_2 = \\sqrt{\\frac{1}{n_{\\text{test}}} \\sum_{i \\in \\text{test}} \\left(y_i - \\hat{y}_{2,i}\\right)^2},$$\n     where $\\hat{y}_{2,i}$ are the predictions from $\\mathcal{M}_2$.\n  3. The absolute change-in-estimate for the occupancy coefficient between the two models, computed on the training set:\n     $$\\Delta_{\\beta_O} = \\left|\\hat{\\beta}_O^{(2)} - \\hat{\\beta}_O^{(1)}\\right|,$$\n     where $\\hat{\\beta}_O^{(1)}$ is the coefficient of $O$ in $\\mathcal{M}_1$ and $\\hat{\\beta}_O^{(2)}$ is the coefficient of $O$ in $\\mathcal{M}_2$.\n  4. A boolean confounding flag defined as $\\mathrm{True}$ if $\\Delta_{\\beta_O} > \\tau$ and $\\mathrm{False}$ otherwise, where $\\tau$ is a specified threshold for that test case.\n  5. The partial coefficient of determination for $O$ given $A$ on the training set,\n     $$R^2_{O\\mid A} = \\frac{R^2_{\\text{full}} - R^2_{\\text{reduced}}}{1 - R^2_{\\text{reduced}}},$$\n     where $R^2_{\\text{full}}$ is from $\\mathcal{M}_2$ and $R^2_{\\text{reduced}}$ is from the reduced model regressing $y$ on $A$ with an intercept only. If numerical rounding yields values slightly outside $[0,1]$, clip to $[0,1]$.\n\nImplement OLS by minimizing the sum of squared residuals; do not rely on any prepackaged model-fitting utilities beyond basic linear algebra. The training/test split must be deterministic as specified. All logarithms are natural logarithms.\n\nTest suite. Your program must generate data internally for the following test cases, using independent Gaussian bases and the specified random seeds to ensure determinism. Let $Z_i \\sim \\mathcal{N}(0,1)$ and $O_i \\sim \\mathcal{N}(0,1)$ be independent, and construct $A_i = \\rho O_i + \\sqrt{1-\\rho^2} Z_i$. Let $\\log r_i = \\alpha + \\beta_O O_i + \\beta_A A_i + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$, and set $r_i = \\exp(\\log r_i)$. Draw $\\log d_i \\sim \\mathcal{N}(\\mu_d,\\sigma_d^2)$ and set $d_i = \\exp(\\log d_i)$. Finally, set $E_i = r_i / d_i$. For each case, provide $(N,\\text{seed},\\alpha,\\beta_O,\\beta_A,\\sigma,\\rho,\\mu_d,\\sigma_d,\\tau)$:\n\n- Case $1$ (independent accessibility, moderate effects): $(N=300, \\text{seed}=13, \\alpha=1.0, \\beta_O=0.8, \\beta_A=0.5, \\sigma=0.2, \\rho=0.0, \\mu_d=-2.0, \\sigma_d=0.5, \\tau=0.1)$.\n- Case $2$ (confounding via correlation): $(N=300, \\text{seed}=17, \\alpha=1.0, \\beta_O=0.8, \\beta_A=0.8, \\sigma=0.2, \\rho=0.8, \\mu_d=-2.0, \\sigma_d=0.5, \\tau=0.1)$.\n- Case $3$ (stronger confounding, low noise): $(N=200, \\text{seed}=19, \\alpha=1.0, \\beta_O=0.8, \\beta_A=0.8, \\sigma=0.05, \\rho=0.95, \\mu_d=-2.0, \\sigma_d=0.7, \\tau=0.1)$.\n- Case $4$ (no true occupancy effect, spurious association risk): $(N=300, \\text{seed}=23, \\alpha=1.0, \\beta_O=0.0, \\beta_A=0.8, \\sigma=0.2, \\rho=0.9, \\mu_d=-2.0, \\sigma_d=0.5, \\tau=0.1)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case in the order above, output the quadruple $[\\mathrm{RMSE}_1,\\mathrm{RMSE}_2,\\text{flag},R^2_{O\\mid A}]$ and concatenate these quadruples into one flat list across all cases. For example, the output should look like\n$$[\\mathrm{RMSE}_{1,1},\\mathrm{RMSE}_{2,1},\\text{flag}_1,R^2_{O\\mid A,1},\\mathrm{RMSE}_{1,2},\\mathrm{RMSE}_{2,2},\\text{flag}_2,R^2_{O\\mid A,2},\\mathrm{RMSE}_{1,3},\\mathrm{RMSE}_{2,3},\\text{flag}_3,R^2_{O\\mid A,3},\\mathrm{RMSE}_{1,4},\\mathrm{RMSE}_{2,4},\\text{flag}_4,R^2_{O\\mid A,4}],$$\nwhere commas separate all entries and booleans appear as either True or False. No other text should be printed.",
            "solution": "The problem is scientifically and mathematically well-posed, providing a clear protocol to simulate and analyze a common statistical issue in genomics known as confounding. The problem is valid and a solution can be formulated.\n\nThe core of this problem lies in understanding and quantifying the effect of a confounding variable in a linear regression model, framed within the context of transcriptional regulation. The central dogma provides the biological foundation: deoxyribonucleic acid ($DNA$) is transcribed into messenger ribonucleic acid ($mRNA$), which is then translated into protein. The abundance of a specific $mRNA$ molecule, denoted $m_i$, is governed by its rates of synthesis (transcription initiation, $r_i$) and degradation ($d_i$). A common kinetic model is given by the ordinary differential equation $\\frac{dm_i}{dt} = r_i - d_i m_i$. At steady state, where the concentration of $mRNA$ is stable, $\\frac{dm_i}{dt} = 0$, leading to the fundamental relationship $r_i = d_i m_i$.\n\nThe problem states that we have measurements of $mRNA$ abundance ($E_i$, assumed equal to $m_i$), and the $mRNA$ degradation rate ($d_i$). From these, we can infer the transcription rate for each gene $i$ as $\\hat{r}_i = d_i E_i$. The problem posits that this transcription rate is influenced by regulatory factors. Specifically, the logarithm of the transcription rate is modeled as a linear function of transcription factor occupancy ($O_i$) and chromatin accessibility ($A_i$):\n$$ \\log r_i = \\alpha + \\beta_O O_i + \\beta_A A_i + \\varepsilon_i $$\nHere, $\\alpha$ is a basal transcription rate (as an intercept), $\\beta_O$ and $\\beta_A$ are the effects of occupancy and accessibility, respectively, and $\\varepsilon_i$ is a random noise term assumed to be normally distributed, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. Our task is to estimate these effects from simulated data.\n\nThe key statistical challenge is confounding. A confounder is a variable that is associated with both the predictor of interest (here, $O_i$) and the outcome (here, $y_i = \\log \\hat{r}_i$). In our simulation setup, chromatin accessibility $A_i$ is constructed to be correlated with occupancy $O_i$ through the parameter $\\rho$, expressed as $A_i = \\rho O_i + \\sqrt{1-\\rho^2} Z_i$, where $Z_i$ is an independent random variable. Since $A_i$ also directly influences $\\log r_i$ via the term $\\beta_A A_i$, it is a potential confounder.\n\nTo investigate this, we fit two linear models using Ordinary Least Squares (OLS). For any linear model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, the OLS estimate of the coefficient vector $\\boldsymbol{\\beta}$ is the one that minimizes the sum of squared residuals, given by the solution to the normal equations:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} $$\nwhere $\\mathbf{X}$ is the design matrix (including a column of ones for the intercept) and $\\mathbf{y}$ is the vector of outcomes.\n\nModel $\\mathcal{M}_1$ is a simple regression that omits the potential confounder:\n$$ y_i = \\log \\hat{r}_i = \\alpha^{(1)} + \\beta_O^{(1)} O_i + \\text{error}_1 $$\nIf $A_i$ is indeed a confounder (i.e., $\\rho \\neq 0$ and $\\beta_A \\neq 0$), the estimate $\\hat{\\beta}_O^{(1)}$ will be biased. It will incorrectly absorb some of the effect of the omitted variable $A_i$.\n\nModel $\\mathcal{M}_2$ is a multiple regression that includes the potential confounder:\n$$ y_i = \\log \\hat{r}_i = \\alpha^{(2)} + \\beta_O^{(2)} O_i + \\beta_A^{(2)} A_i + \\text{error}_2 $$\nBy including $A_i$ in the model, we can obtain an estimate, $\\hat{\\beta}_O^{(2)}$, that is adjusted for the effect of $A_i$. This estimate reflects the association of $O_i$ with the outcome, holding $A_i$ statistically constant.\n\nThe process for each test case is as follows:\n1.  **Data Simulation**: Generate vectors $(O_i, A_i, E_i, d_i)$ of length $N$ according to the specified stochastic process, ensuring reproducibility via a random seed.\n2.  **Data Preparation**: Calculate the outcome variable $y_i = \\log(d_i E_i)$. Split the data deterministically into a training set (the first $\\lfloor 0.7N \\rfloor$ samples) and a test set (the remainder).\n3.  **Model Fitting**: On the training set, fit models $\\mathcal{M}_1$ and $\\mathcal{M}_2$ using OLS to obtain coefficient estimates $(\\hat{\\alpha}^{(1)}, \\hat{\\beta}_O^{(1)})$ and $(\\hat{\\alpha}^{(2)}, \\hat{\\beta}_O^{(2)}, \\hat{\\beta}_A^{(2)})$.\n4.  **Performance Evaluation**:\n    - Use the fitted models to predict outcomes on the test set, yielding $\\hat{y}_{1,i}$ and $\\hat{y}_{2,i}$. Compute the Root Mean Squared Error (RMSE) for both models, $\\mathrm{RMSE}_1$ and $\\mathrm{RMSE}_2$, to assess their predictive accuracy on unseen data. A lower RMSE indicates better performance.\n5.  **Confounding Assessment**:\n    - Compute the change-in-estimate, $\\Delta_{\\beta_O} = \\left|\\hat{\\beta}_O^{(2)} - \\hat{\\beta}_O^{(1)}\\right|$. A large value suggests that $A_i$ was a significant confounder, as its inclusion in the model substantially changed the estimated effect of $O_i$. A boolean flag is set based on a threshold $\\tau$.\n    - Compute the partial coefficient of determination, $R^2_{O\\mid A}$. This metric quantifies the proportion of variance in $y_i$ that is explained by $O_i$ after the variance explained by $A_i$ has already been accounted for. It is calculated on the training data via the formula $R^2_{O\\mid A} = (R^2_{\\text{full}} - R^2_{\\text{reduced}}) / (1 - R^2_{\\text{reduced}})$, where $R^2_{\\text{full}}$ is the R-squared from model $\\mathcal{M}_2$ (regressing $y$ on $O$ and $A$) and $R^2_{\\text{reduced}}$ is from a model regressing $y$ on $A$ alone. This isolates the unique contribution of $O_i$.\n\nThis comprehensive analysis allows us to not only detect the presence of confounding but also to quantify its impact on both model coefficients and predictive power.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        # (N, seed, alpha, beta_O, beta_A, sigma, rho, mu_d, sigma_d, tau)\n        (300, 13, 1.0, 0.8, 0.5, 0.2, 0.0, -2.0, 0.5, 0.1),\n        (300, 17, 1.0, 0.8, 0.8, 0.2, 0.8, -2.0, 0.5, 0.1),\n        (200, 19, 1.0, 0.8, 0.8, 0.05, 0.95, -2.0, 0.7, 0.1),\n        (300, 23, 1.0, 0.0, 0.8, 0.2, 0.9, -2.0, 0.5, 0.1),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        N, seed, alpha, beta_O, beta_A, sigma, rho, mu_d, sigma_d, tau = case\n        \n        # 1. Data Generation\n        rng = np.random.default_rng(seed)\n        O = rng.normal(0, 1, N)\n        Z = rng.normal(0, 1, N)\n        A = rho * O + np.sqrt(1 - rho**2) * Z\n        epsilon = rng.normal(0, sigma, N)\n        \n        log_r = alpha + beta_O * O + beta_A * A + epsilon\n        r = np.exp(log_r)\n        \n        log_d = rng.normal(mu_d, sigma_d, N)\n        d = np.exp(log_d)\n        \n        E = r / d\n        \n        # 2. Data Preparation\n        r_hat = d * E\n        y = np.log(r_hat)\n        \n        n_train = int(np.floor(0.7 * N))\n        n_test = N - n_train\n        \n        # Training set\n        O_train, A_train, y_train = O[:n_train], A[:n_train], y[:n_train]\n        # Test set\n        O_test, A_test, y_test = O[n_train:], A[n_train:], y[n_train:]\n\n        def perform_ols(X, y_data):\n            \"\"\"Performs OLS regression using np.linalg.solve for stability.\"\"\"\n            # beta_hat = (X.T @ X)^-1 @ X.T @ y\n            try:\n                # More stable than inv()\n                beta_hat = np.linalg.solve(X.T @ X, X.T @ y_data)\n            except np.linalg.LinAlgError:\n                # Fallback for singular matrix\n                beta_hat = np.linalg.pinv(X.T @ X) @ X.T @ y_data\n            return beta_hat\n\n        # 3. Model Fitting on Training Data\n        \n        # Model 1: y ~ 1 + O\n        X1_train = np.c_[np.ones(n_train), O_train]\n        beta1_hat = perform_ols(X1_train, y_train)\n        \n        # Model 2: y ~ 1 + O + A (Full model)\n        X2_train = np.c_[np.ones(n_train), O_train, A_train]\n        beta2_hat = perform_ols(X2_train, y_train)\n\n        # 4. RMSE Calculation on Test Data\n        \n        # Predictions for Model 1\n        X1_test = np.c_[np.ones(n_test), O_test]\n        y1_pred = X1_test @ beta1_hat\n        rmse1 = np.sqrt(np.mean((y_test - y1_pred)**2))\n        \n        # Predictions for Model 2\n        X2_test = np.c_[np.ones(n_test), O_test, A_test]\n        y2_pred = X2_test @ beta2_hat\n        rmse2 = np.sqrt(np.mean((y_test - y2_pred)**2))\n        \n        # 5. Confounding Assessment (on training coefficients)\n        beta_O1 = beta1_hat[1]\n        beta_O2 = beta2_hat[1]\n        delta_beta_O = np.abs(beta_O2 - beta_O1)\n        confounding_flag = delta_beta_O > tau\n\n        # 6. Partial R^2 Calculation (on training data)\n        \n        # Reduced model for partial R^2: y ~ 1 + A\n        XA_train = np.c_[np.ones(n_train), A_train]\n        betaA_hat = perform_ols(XA_train, y_train)\n        \n        SST = np.sum((y_train - np.mean(y_train))**2)\n        \n        # R^2 for full model (M2)\n        y2_pred_train = X2_train @ beta2_hat\n        SSR_full = np.sum((y_train - y2_pred_train)**2)\n        R2_full = 1 - SSR_full / SST if SST > 0 else 1.0\n\n        # R^2 for reduced model\n        yA_pred_train = XA_train @ betaA_hat\n        SSR_reduced = np.sum((y_train - yA_pred_train)**2)\n        R2_reduced = 1 - SSR_reduced / SST if SST > 0 else 1.0\n        \n        # Partial R^2\n        denominator = 1 - R2_reduced\n        if np.isclose(denominator, 0):\n             # If reduced model explains all variance, O can't explain more\n            R2_partial = 0.0\n        else:\n            R2_partial = (R2_full - R2_reduced) / denominator\n        \n        R2_partial_clipped = np.clip(R2_partial, 0, 1)\n\n        all_results.extend([rmse1, rmse2, confounding_flag, R2_partial_clipped])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}