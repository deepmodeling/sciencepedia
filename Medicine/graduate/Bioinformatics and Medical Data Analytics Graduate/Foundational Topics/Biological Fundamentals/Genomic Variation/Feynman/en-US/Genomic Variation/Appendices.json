{
    "hands_on_practices": [
        {
            "introduction": "The study of genomic variation begins at the population level. A fundamental concept is the Hardy-Weinberg Equilibrium (HWE), which provides a mathematical null model for how allele and genotype frequencies are maintained over generations in the absence of evolutionary influences. This exercise will ground you in the core principles of population genetics by tasking you with deriving allele frequency estimates directly from observed genotype counts and testing for deviations from HWE, a critical first step in many genetic association studies. ",
            "id": "4569027",
            "problem": "A large clinical biobank assembled for precision medicine studies has genotyped an autosomal, biallelic single-nucleotide polymorphism (SNP) using high-coverage whole-genome sequencing in a cohort of unrelated individuals. Let the two alleles be denoted by the nucleotide letters $T$ (reference) and $C$ (alternate). The observed genotype counts in the cohort are: $n_{TT} = 548$, $n_{TC} = 362$, and $n_{CC} = 90$, for a total of $n = 1000$ diploid individuals.\n\nStarting from the fundamental definition that, in a randomly mating population without selection, mutation, migration, or assortative mating, each gamete carries allele $C$ independently with probability $p$ and allele $T$ with probability $1 - p$, and that the $2n$ alleles in the cohort constitute a sample from the population gene pool, perform the following:\n\n1. Using only first principles of likelihood for independent Bernoulli trials on the $2n$ alleles, derive the maximum likelihood estimate $\\hat{p}$ of the population frequency $p$ of allele $C$ in terms of the observed genotype counts.\n\n2. Using the definition of Hardy–Weinberg equilibrium (HWE), that is, random union of gametes producing genotype classes as products of the allele probabilities, derive the expected genotype frequencies for $TT$, $TC$, and $CC$ in terms of $p$.\n\n3. Compute the expected number of heterozygotes $TC$ in this cohort by substituting your $\\hat{p}$ into the HWE expression obtained in part 2. Express the final result as a single number. Round your answer to four significant figures. No units are required.",
            "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, objective, and self-contained. It represents a standard exercise in population genetics. We may therefore proceed with a formal solution.\n\nThe problem is structured into three parts. We will address each in sequence.\n\n**Part 1: Derivation of the Maximum Likelihood Estimate (MLE) of the allele frequency $p$**\n\nWe are asked to derive the maximum likelihood estimate, $\\hat{p}$, for the frequency of the alternate allele $C$. The problem specifies that this derivation must be based on the principle of independent Bernoulli trials on the $2n$ alleles present in the cohort of $n$ diploid individuals.\n\nThe total number of individuals is $n = n_{TT} + n_{TC} + n_{CC} = 548 + 362 + 90 = 1000$.\nThe total number of alleles in the sample is therefore $2n = 2000$.\n\nLet $p$ be the population frequency of allele $C$.\nLet $1-p$ be the population frequency of allele $T$.\n\nEach of the $2n$ alleles in the sample can be considered an independent draw from the population's gene pool. The event of drawing a $C$ allele is a \"success\" with probability $p$, and drawing a $T$ allele is a \"failure\" with probability $1-p$. This constitutes a series of $2n$ independent Bernoulli trials.\n\nFirst, we count the total number of $C$ alleles, denoted $N_C$, and $T$ alleles, denoted $N_T$, in our sample.\nAn individual with genotype $TT$ contributes $0$ $C$ alleles.\nAn individual with genotype $TC$ contributes $1$ $C$ allele.\nAn individual with genotype $CC$ contributes $2$ $C$ alleles.\nThus, the total count of $C$ alleles is:\n$$N_C = 2 \\cdot n_{CC} + 1 \\cdot n_{TC} + 0 \\cdot n_{TT} = 2n_{CC} + n_{TC}$$\nThe total count of $T$ alleles is:\n$$N_T = 2 \\cdot n_{TT} + 1 \\cdot n_{TC} + 0 \\cdot n_{CC} = 2n_{TT} + n_{TC}$$\nThe total number of alleles is $N_C + N_T = (2n_{CC} + n_{TC}) + (2n_{TT} + n_{TC}) = 2(n_{TT} + n_{TC} + n_{CC}) = 2n$, which is consistent.\n\nThe probability of observing exactly $N_C$ successes in $2n$ independent trials follows the binomial distribution. The likelihood function $L(p)$, which is the probability of the observed data given the parameter $p$, is:\n$$L(p | N_C, n) = \\binom{2n}{N_C} p^{N_C} (1-p)^{2n - N_C}$$\nTo find the value of $p$ that maximizes this function, it is computationally more convenient to maximize the natural logarithm of the likelihood, the log-likelihood function $\\ell(p) = \\ln(L(p))$:\n$$\\ell(p) = \\ln\\left(\\binom{2n}{N_C}\\right) + N_C \\ln(p) + (2n - N_C) \\ln(1-p)$$\nWe find the maximum by taking the first derivative of $\\ell(p)$ with respect to $p$ and setting it to zero. The term $\\ln\\left(\\binom{2n}{N_C}\\right)$ is a constant with respect to $p$, so its derivative is zero.\n$$\\frac{d\\ell}{dp} = \\frac{N_C}{p} - \\frac{2n - N_C}{1-p}$$\nSetting the derivative to zero gives the MLE, denoted $\\hat{p}$:\n$$\\frac{N_C}{\\hat{p}} - \\frac{2n - N_C}{1-\\hat{p}} = 0$$\n$$\\frac{N_C}{\\hat{p}} = \\frac{2n - N_C}{1-\\hat{p}}$$\n$$N_C (1-\\hat{p}) = \\hat{p} (2n - N_C)$$\n$$N_C - N_C \\hat{p} = 2n \\hat{p} - N_C \\hat{p}$$\n$$N_C = 2n \\hat{p}$$\nSolving for $\\hat{p}$, we obtain the maximum likelihood estimate:\n$$\\hat{p} = \\frac{N_C}{2n}$$\nSubstituting the expression for $N_C$ in terms of the genotype counts, we arrive at the final expression for the MLE:\n$$\\hat{p} = \\frac{2n_{CC} + n_{TC}}{2n}$$\n\n**Part 2: Derivation of Expected Genotype Frequencies under Hardy–Weinberg Equilibrium (HWE)**\n\nThe problem defines HWE as the state resulting from the random union of gametes. In the population's gamete pool, the frequency of allele $C$ is $p$ and the frequency of allele $T$ is $1-p$. The formation of a diploid zygote involves the random combination of two such gametes.\n\nThe probability of forming a $CC$ genotype is the probability that a gamete carrying $C$ unites with another gamete carrying $C$. Due to the random union (independence), this probability is:\n$$\\text{Freq}(CC) = p \\times p = p^2$$\nThe probability of forming a $TT$ genotype is the probability that a gamete carrying $T$ unites with another gamete carrying $T$:\n$$\\text{Freq}(TT) = (1-p) \\times (1-p) = (1-p)^2$$\nThe probability of forming a $TC$ heterozygote can occur in two mutually exclusive ways:\n1. A sperm with allele $T$ fertilizes an egg with allele $C$. The probability is $(1-p) \\times p$.\n2. A sperm with allele $C$ fertilizes an egg with allele $T$. The probability is $p \\times (1-p)$.\nThe total probability for the heterozygous genotype is the sum of these probabilities:\n$$\\text{Freq}(TC) = p(1-p) + (1-p)p = 2p(1-p)$$\nThus, the expected genotype frequencies under HWE are:\n$$(1-p)^2 \\quad \\text{for } TT$$\n$$2p(1-p) \\quad \\text{for } TC$$\n$$p^2 \\quad \\text{for } CC$$\n\n**Part 3: Calculation of the Expected Number of Heterozygotes**\n\nFirst, we calculate the numerical value of the MLE, $\\hat{p}$, using the provided genotype counts: $n_{TT} = 548$, $n_{TC} = 362$, $n_{CC} = 90$, and $n = 1000$.\nFrom Part 1, we have:\n$$\\hat{p} = \\frac{2n_{CC} + n_{TC}}{2n} = \\frac{2(90) + 362}{2(1000)} = \\frac{180 + 362}{2000} = \\frac{542}{2000} = 0.271$$\nNext, we use this estimate $\\hat{p}$ to calculate the expected frequency of heterozygotes ($TC$) under HWE, based on the formula derived in Part 2:\n$$\\text{Expected Freq}(TC) = 2\\hat{p}(1-\\hat{p})$$\n$$\\text{Expected Freq}(TC) = 2(0.271)(1 - 0.271) = 2(0.271)(0.729) = 0.395118$$\nFinally, the expected number of heterozygotes, $E[n_{TC}]$, in a cohort of size $n=1000$ is the total number of individuals multiplied by the expected frequency of that genotype:\n$$E[n_{TC}] = n \\times \\text{Expected Freq}(TC)$$\n$$E[n_{TC}] = 1000 \\times 0.395118 = 395.118$$\nThe problem requires the answer to be rounded to four significant figures.\n$$395.118 \\approx 395.1$$\nTherefore, the expected number of heterozygotes in this cohort, under the assumption of HWE, is $395.1$.",
            "answer": "$$\n\\boxed{395.1}\n$$"
        },
        {
            "introduction": "Detecting genomic variants from sequencing data is not a simple act of counting; it is an exercise in probabilistic inference that must account for numerous technical artifacts. This practice moves from population-level theory to the analysis of an individual's sequencing reads, focusing on the common challenge of reference bias and base-calling errors at a heterozygous SNP. By building a generative model, you will learn to precisely quantify how these biases distort the observed allele balance, a key skill for developing robust variant callers. ",
            "id": "4568960",
            "problem": "A single nucleotide polymorphism (SNP) is a biallelic heterozygous site in a diploid genome with one reference allele (REF) and one alternative allele (ALT). In high-throughput sequencing, reference bias can cause reads carrying the REF allele to map more successfully than reads carrying the ALT allele. Consider the following generative model for reads overlapping a heterozygous SNP:\n\n1. Each originating fragment is equally likely to come from the REF or ALT haplotype, with probabilities $0.5$ and $0.5$, respectively.\n2. Conditional on its haplotype of origin, a fragment maps and passes all filters with probability $m_{R}$ if it carries the REF haplotype and with probability $m_{A}$ if it carries the ALT haplotype, where $m_{R} \\neq m_{A}$ due to alignment bias.\n3. Conditional on mapping, base calling at the SNP position is subject to a symmetric substitution error rate $\\epsilon$: a mapped REF-originating read reports ALT with probability $\\epsilon$ and REF with probability $1-\\epsilon$; a mapped ALT-originating read reports ALT with probability $1-\\epsilon$ and REF with probability $\\epsilon$.\n\nDefine the allele balance as the expected fraction of mapped reads supporting the ALT allele among all mapped reads overlapping the SNP. Using only the core definitions of conditional probability, the law of total expectation, and the generative model above, derive the expected allele balance as a function of $m_{R}$, $m_{A}$, and $\\epsilon$, and then evaluate it numerically for $m_{R}=0.98$, $m_{A}=0.84$, and $\\epsilon=0.005$.\n\nExpress your final numerical result as a decimal rounded to four significant figures. Do not include a percent sign or any units in your final answer.",
            "solution": "The user wants to derive and evaluate an expression for the allele balance at a heterozygous single nucleotide polymorphism (SNP) site, given a generative model for sequencing reads that includes reference bias and base-calling errors.\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n- A SNP is a biallelic heterozygous site with a reference (REF) and an alternative (ALT) allele.\n- An originating fragment is from the REF haplotype with probability $P(H_R) = 0.5$.\n- An originating fragment is from the ALT haplotype with probability $P(H_A) = 0.5$.\n- The probability that a REF-originating fragment maps is $m_{R} = P(M|H_R)$.\n- The probability that an ALT-originating fragment maps is $m_{A} = P(M|H_A)$.\n- There is alignment bias, meaning $m_{R} \\neq m_{A}$.\n- A symmetric substitution error rate, $\\epsilon$, governs base calling for mapped reads.\n- Probability of a mapped REF-originating read reporting ALT: $P(A_{obs}|M, H_R) = \\epsilon$.\n- Probability of a mapped REF-originating read reporting REF: $P(R_{obs}|M, H_R) = 1-\\epsilon$.\n- Probability of a mapped ALT-originating read reporting ALT: $P(A_{obs}|M, H_A) = 1-\\epsilon$.\n- Probability of a mapped ALT-originating read reporting REF: $P(R_{obs}|M, H_A) = \\epsilon$.\n- The allele balance is defined as the expected fraction of mapped reads supporting the ALT allele.\n- Numerical values for evaluation: $m_{R}=0.98$, $m_{A}=0.84$, and $\\epsilon=0.005$.\n- The final numerical result should be rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It presents a simplified but standard and valid probabilistic model used in bioinformatics to account for sequencing artifacts like mapping bias and base call errors. The provided parameters are physically realistic. The problem is self-contained and free of contradictions or ambiguities. The terminology is standard within the field of genomics. Therefore, the problem is valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the solution.\n\n**Derivation of the Allele Balance**\n\nThe allele balance ($AB$) is defined as the expected fraction of mapped reads supporting the ALT allele. This is equivalent to finding the probability that a single, randomly selected mapped read reports the ALT allele. Let's denote the event that a read maps as $M$ and the event that a mapped read reports the ALT allele as $A_{obs}$. The quantity we wish to find is the conditional probability $P(A_{obs}|M)$.\n\nUsing the definition of conditional probability:\n$$ AB = P(A_{obs}|M) = \\frac{P(A_{obs} \\cap M)}{P(M)} $$\nWe can calculate the numerator and the denominator separately using the law of total probability, partitioning the sample space based on the read's haplotype of origin: $H_R$ (REF haplotype) or $H_A$ (ALT haplotype).\n\nFirst, let's calculate the denominator, $P(M)$, which is the overall probability that a read maps, regardless of its origin.\n$$ P(M) = P(M|H_R)P(H_R) + P(M|H_A)P(H_A) $$\nSubstituting the given probabilities:\n$$ P(M) = m_{R}(0.5) + m_{A}(0.5) = 0.5(m_{R} + m_{A}) $$\n\nNext, let's calculate the numerator, $P(A_{obs} \\cap M)$, which is the joint probability that a read maps and reports the ALT allele.\n$$ P(A_{obs} \\cap M) = P(A_{obs} \\cap M | H_R)P(H_R) + P(A_{obs} \\cap M | H_A)P(H_A) $$\nUsing the chain rule for conditional probabilities, $P(A_{obs} \\cap M | H_i) = P(A_{obs}|M, H_i)P(M|H_i)$, we can rewrite the expression as:\n$$ P(A_{obs} \\cap M) = P(A_{obs}|M, H_R)P(M|H_R)P(H_R) + P(A_{obs}|M, H_A)P(M|H_A)P(H_A) $$\nNow, we substitute the parameters from the generative model:\n- For a read from the REF haplotype ($H_R$): it maps with probability $m_R$ and is then miscalled as ALT with probability $\\epsilon$.\n- For a read from the ALT haplotype ($H_A$): it maps with probability $m_A$ and is then correctly called as ALT with probability $1-\\epsilon$.\n\nSubstituting these into the equation for $P(A_{obs} \\cap M)$:\n$$ P(A_{obs} \\cap M) = (\\epsilon)(m_{R})(0.5) + (1-\\epsilon)(m_{A})(0.5) $$\n$$ P(A_{obs} \\cap M) = 0.5(\\epsilon m_{R} + (1-\\epsilon)m_{A}) $$\n\nFinally, we can compute the allele balance by taking the ratio of the numerator and the denominator:\n$$ AB = \\frac{P(A_{obs} \\cap M)}{P(M)} = \\frac{0.5(\\epsilon m_{R} + (1-\\epsilon)m_{A})}{0.5(m_{R} + m_{A})} $$\nThe factors of $0.5$ cancel, yielding the general expression for the allele balance:\n$$ AB = \\frac{\\epsilon m_{R} + (1-\\epsilon)m_{A}}{m_{R} + m_{A}} $$\n\n**Numerical Evaluation**\n\nNow, we substitute the given numerical values into the derived expression: $m_{R}=0.98$, $m_{A}=0.84$, and $\\epsilon=0.005$.\n$$ AB = \\frac{(0.005)(0.98) + (1-0.005)(0.84)}{0.98 + 0.84} $$\n$$ AB = \\frac{(0.005)(0.98) + (0.995)(0.84)}{1.82} $$\nCalculate the terms in the numerator:\n$$ (0.005)(0.98) = 0.0049 $$\n$$ (0.995)(0.84) = 0.8358 $$\nSum the terms in the numerator:\n$$ 0.0049 + 0.8358 = 0.8407 $$\nNow, perform the final division:\n$$ AB = \\frac{0.8407}{1.82} \\approx 0.4619230769... $$\nThe problem requires the result to be rounded to four significant figures. The first four significant figures are $4$, $6$, $1$, and $9$. The fifth digit is $2$, so we round down.\n$$ AB \\approx 0.4619 $$\nThis is the expected fraction of mapped reads that support the ALT allele under the given conditions.",
            "answer": "$$\n\\boxed{0.4619}\n$$"
        },
        {
            "introduction": "Beyond single nucleotide changes, genomes harbor large-scale structural variations like copy number variations (CNVs), which are often detected by analyzing read depth patterns. However, raw read coverage is heavily influenced by local sequence characteristics, particularly mappability, creating a major source of potential false positives. This hands-on coding problem challenges you to develop and implement a statistically principled method for CNV detection that explicitly normalizes for mappability and applies a rigorous filtering and testing framework, mirroring the logic of production-grade bioinformatics pipelines. ",
            "id": "4568999",
            "problem": "Consider short-read whole-genome sequencing used to detect genomic variation, including Single Nucleotide Polymorphism (SNP), insertions and deletions (indels), and Structural Variation (SV). Reads are aligned to a reference genome, and the read coverage at each genomic position is used to infer Copy Number Variation (CNV). Two foundational facts motivate the statistical treatment of coverage: (i) under uniform sampling and independent read starts, per-base read counts follow a Poisson process, and (ii) alignment mappability varies across the genome because of repetitive DNA, including segmental duplications. A mappability track assigns to each genomic position a score between $0$ and $1$ representing the probability that a read originating at that position can be uniquely aligned; low mappability positions yield reduced observed coverage even without true CNV, and highly repetitive regions can also exhibit aberrant pile-ups depending on alignment heuristics. The goal is to formalize how mappability and segmental duplications distort coverage and to design a principled filtering and normalization scheme that mitigates false CNV positives using mappability tracks.\n\nFundamental base and definitions:\n- The Central Dogma of Molecular Biology states that deoxyribonucleic acid (DNA) is transcribed to ribonucleic acid (RNA) and translated into protein; while not directly governing read coverage, this sets the context for sequencing-based inference of genomic variation.\n- In short-read sequencing, the observed per-base coverage $C_i$ at position $i$ can be modeled as a Poisson random variable with mean $\\lambda_i$, denoted $C_i \\sim \\mathrm{Poisson}(\\lambda_i)$, where $\\lambda_i$ depends on total sequencing depth, copy number, and mappability.\n- Let $m_i \\in [0,1]$ denote the mappability at base $i$. Let the baseline expected coverage for a diploid copy number of $2$ in a uniquely mappable region be $\\mu$ (reads per base). Then, for absolute copy number $\\mathrm{CN}$ at base $i$, the expected coverage satisfies $\\mathbb{E}[C_i] = \\lambda_i = \\mu \\cdot m_i \\cdot \\frac{\\mathrm{CN}}{2}$ under a model where non-uniquely mappable reads are effectively unavailable for confident alignment.\n- A Copy Number Variation (CNV) call aims to detect $\\mathrm{CN} \\neq 2$ by aggregating evidence over a genomic window. Segmental duplications and other low-mappability features distort naive coverage-based CNV calling because $\\{m_i\\}$ varies substantially, often biasing $C_i$ downward independent of true $\\mathrm{CN}$.\n\nTask:\n1. Derive a normalization and testing scheme, starting from the Poisson model with mappability, to estimate $\\widehat{\\mathrm{CN}}$ for a genomic window and to compute a test statistic for deviation from the null $\\mathrm{CN}_0 = 2$. Your derivation must not rely on shortcut formulas; it should begin from $C_i \\sim \\mathrm{Poisson}(\\lambda_i)$ with $\\lambda_i = \\mu \\cdot m_i \\cdot \\frac{\\mathrm{CN}}{2}$.\n2. Propose and implement a filtering procedure using mappability tracks that mitigates false positives in low-mappability or highly repetitive windows. The filter must include:\n   - Exclusion of bases with $m_i  t_{\\text{map}}$.\n   - A minimum fraction $f_{\\text{min}}$ of retained bases with $m_i \\ge t_{\\text{map}}$ required to attempt a CNV call.\n   - A minimum retained length $n_{\\text{min}}$ (in bases) below which the window is considered inconclusive.\n3. Implement the resulting estimator and test statistic in code. For each test window, classify the CNV state using a symmetric significance threshold $z_{\\text{thr}}$: output $-1$ for deletion, $0$ for neutral (no CNV call or inconclusive), and $+1$ for duplication.\n\nMathematical specification to implement:\n- Given a window with per-base coverage $\\{C_i\\}_{i=1}^n$, mappability $\\{m_i\\}_{i=1}^n$, and parameters $\\mu$, $t_{\\text{map}}$, $f_{\\text{min}}$, $n_{\\text{min}}$, and $z_{\\text{thr}}$, apply the filter to retain indices $\\mathcal{I} = \\{ i \\in \\{1,\\dots,n\\} : m_i \\ge t_{\\text{map}} \\}$. If $|\\mathcal{I}|  n_{\\text{min}}$ or $\\frac{|\\mathcal{I}|}{n}  f_{\\text{min}}$, output $0$.\n- Define normalized per-base quantities $X_i = \\frac{C_i}{\\mu \\cdot m_i}$ for $i \\in \\mathcal{I}$. Let $\\widehat{\\mathrm{CN}} = \\frac{2}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} X_i$. Under the null $\\mathrm{CN}_0 = 2$, approximate the variance of $X_i$ by $\\mathrm{Var}(X_i \\mid \\mathrm{CN}_0) = \\frac{\\mathrm{CN}_0}{2 \\cdot \\mu \\cdot m_i}$ by Poisson-to-normal approximation. Then the null variance of $\\widehat{\\mathrm{CN}}$ is\n$$\n\\mathrm{Var}_0(\\widehat{\\mathrm{CN}}) = \\left( \\frac{2}{|\\mathcal{I}|} \\right)^2 \\sum_{i \\in \\mathcal{I}} \\mathrm{Var}(X_i \\mid \\mathrm{CN}_0) = \\frac{4}{|\\mathcal{I}|^2} \\sum_{i \\in \\mathcal{I}} \\frac{1}{\\mu \\cdot m_i}.\n$$\n- Compute the $z$-score under the null,\n$$\nz = \\frac{\\widehat{\\mathrm{CN}} - 2}{\\sqrt{\\mathrm{Var}_0(\\widehat{\\mathrm{CN}})}}.\n$$\n- Classify by $z$: output $-1$ if $z \\le -z_{\\text{thr}}$, $+1$ if $z \\ge z_{\\text{thr}}$, else $0$.\n\nTest suite:\nUse the following five windows that probe diverse scenarios, with arrays expressed as sequences of integers for $\\{C_i\\}$ and decimals in $[0,1]$ for $\\{m_i\\}$:\n\n- Test $1$ (high mappability, neutral diploid): $\\mu = 30$, $t_{\\text{map}} = 0.5$, $f_{\\text{min}} = 0.6$, $n_{\\text{min}} = 5$, $z_{\\text{thr}} = 3.0$, $\\{C_i\\} = \\{31,29,33,30,28,32,31,30,29,34\\}$, $\\{m_i\\} = \\{0.99,0.98,1.0,0.97,0.96,0.99,0.98,1.0,0.95,0.97\\}$.\n- Test $2$ (segmental duplication with low mappability, neutral diploid but coverage dropout): $\\mu = 30$, $t_{\\text{map}} = 0.5$, $f_{\\text{min}} = 0.6$, $n_{\\text{min}} = 5$, $z_{\\text{thr}} = 3.0$, $\\{C_i\\} = \\{9,11,8,12,10,9,10,9,8,10\\}$, $\\{m_i\\} = \\{0.30,0.35,0.25,0.40,0.33,0.28,0.31,0.29,0.27,0.34\\}$.\n- Test $3$ (true heterozygous deletion in high mappability): $\\mu = 30$, $t_{\\text{map}} = 0.5$, $f_{\\text{min}} = 0.6$, $n_{\\text{min}} = 5$, $z_{\\text{thr}} = 3.0$, $\\{C_i\\} = \\{15,14,16,13,17,15,14,16\\}$, $\\{m_i\\} = \\{0.98,0.97,0.99,0.96,0.97,0.98,0.99,0.97\\}$.\n- Test $4$ (duplication in mixed mappability; sufficient unique fraction to pass filter): $\\mu = 30$, $t_{\\text{map}} = 0.5$, $f_{\\text{min}} = 0.6$, $n_{\\text{min}} = 5$, $z_{\\text{thr}} = 3.0$, $\\{C_i\\} = \\{36,31,27,18,25,41,38,23,20,31,29,27\\}$, $\\{m_i\\} = \\{0.8,0.7,0.6,0.4,0.55,0.9,0.85,0.5,0.45,0.7,0.65,0.6\\}$.\n- Test $5$ (short window at boundary; inconclusive by minimum length): $\\mu = 30$, $t_{\\text{map}} = 0.5$, $f_{\\text{min}} = 0.6$, $n_{\\text{min}} = 5$, $z_{\\text{thr}} = 3.0$, $\\{C_i\\} = \\{30,31,29\\}$, $\\{m_i\\} = \\{0.95,0.96,0.97\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the CNV classifications for the five tests as a comma-separated list enclosed in square brackets, for example $[\\dots]$. Use integers to encode calls: $-1$ for deletion, $0$ for neutral or filtered/inconclusive, and $+1$ for duplication. No units are required in the output. The program must be self-contained and require no user input.",
            "solution": "The problem requires the formulation and implementation of a statistical method to detect Copy Number Variations (CNVs) from short-read sequencing data, while accounting for confounding effects of genome mappability. The solution involves three principal components: a statistical model for read coverage, a filtering scheme to handle regions of low data quality, and a hypothesis testing framework to assess the significance of observed coverage deviations.\n\nThe derivation and procedure are as follows:\n\nFirst, we establish the statistical model for observed read counts. At any given genomic position $i$, the number of reads covering that position, $C_i$, is modeled as a Poisson-distributed random variable, denoted $C_i \\sim \\mathrm{Poisson}(\\lambda_i)$. The parameter $\\lambda_i$ represents the expected (or mean) coverage at that position.\n\nSecond, we model the expected coverage $\\lambda_i$ by incorporating biological and technical factors. The problem posits that $\\lambda_i$ is a function of three main components:\n1.  The overall sequencing depth, represented by a baseline mean coverage $\\mu$ in a \"normal\" genomic region. A normal region is defined as being diploid (copy number $2$) and perfectly mappable.\n2.  The true, underlying absolute copy number at position $i$, denoted $\\mathrm{CN}$. The coverage is proportional to the copy number. We normalize this by the diploid state, leading to a factor of $\\frac{\\mathrm{CN}}{2}$.\n3.  The alignment mappability at position $i$, $m_i \\in [0, 1]$. This score represents the probability that a read originating at position $i$ can be uniquely and correctly mapped back to the reference genome. Non-uniquely mappable reads are typically discarded or have their mapping quality scores reduced, leading to an effective reduction in observed coverage.\nCombining these factors, the model for the expected coverage is given by:\n$$\n\\lambda_i = \\mathbb{E}[C_i] = \\mu \\cdot m_i \\cdot \\frac{\\mathrm{CN}}{2}\n$$\n\nThe goal is to estimate $\\mathrm{CN}$ for a genomic window containing bases $\\{1, \\dots, n\\}$ and test whether it deviates from the diploid null hypothesis, $H_0: \\mathrm{CN} = 2$.\n\nTo derive a copy number estimator, we first define a normalized quantity for each base that corrects for the known effects of sequencing depth ($\\mu$) and mappability ($m_i$). Let us define $X_i$:\n$$\nX_i = \\frac{C_i}{\\mu \\cdot m_i}\n$$\nSubstituting the expectation of $C_i$, the expectation of $X_i$ is:\n$$\n\\mathbb{E}[X_i] = \\frac{\\mathbb{E}[C_i]}{\\mu \\cdot m_i} = \\frac{\\mu \\cdot m_i \\cdot \\frac{\\mathrm{CN}}{2}}{\\mu \\cdot m_i} = \\frac{\\mathrm{CN}}{2}\n$$\nThis shows that $X_i$ is an unbiased estimator for $\\frac{\\mathrm{CN}}{2}$. To get a more robust estimate of $\\mathrm{CN}$ over a genomic window, we can average these single-position estimates. The problem specifies a filtering step before aggregation. We consider only the set of indices $\\mathcal{I} = \\{ i \\in \\{1,\\dots,n\\} : m_i \\ge t_{\\mathrm{map}} \\}$, where $t_{\\mathrm{map}}$ is a minimum mappability threshold. The copy number estimator for the window, $\\widehat{\\mathrm{CN}}$, is then defined as the average of the estimators for $2 \\cdot X_i$ over the set $\\mathcal{I}$:\n$$\n\\widehat{\\mathrm{CN}} = \\frac{1}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} (2 \\cdot X_i) = \\frac{2}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} \\frac{C_i}{\\mu \\cdot m_i}\n$$\nThis is a Method of Moments estimator for $\\mathrm{CN}$. By linearity of expectation, it is unbiased: $\\mathbb{E}[\\widehat{\\mathrm{CN}}] = \\mathrm{CN}$.\n\nTo perform a statistical test, we must determine the distribution of $\\widehat{\\mathrm{CN}}$ under the null hypothesis, $H_0: \\mathrm{CN} = 2$. By the Central Limit Theorem, the sum (and thus the mean) of a sufficiently large number of independent random variables will be approximately normally distributed. We assume the read counts $C_i$ are independent across positions.\nThe mean of $\\widehat{\\mathrm{CN}}$ under $H_0$ is $\\mathbb{E}[\\widehat{\\mathrm{CN}} \\mid H_0] = 2$.\nNext, we derive the variance of $\\widehat{\\mathrm{CN}}$ under $H_0$, denoted $\\mathrm{Var}_0(\\widehat{\\mathrm{CN}})$. First, we find the variance of $X_i$:\n$$\n\\mathrm{Var}(X_i) = \\mathrm{Var}\\left(\\frac{C_i}{\\mu \\cdot m_i}\\right) = \\frac{1}{(\\mu \\cdot m_i)^2} \\mathrm{Var}(C_i)\n$$\nA key property of the Poisson distribution is that its variance equals its mean, $\\mathrm{Var}(C_i) = \\lambda_i$. Under $H_0: \\mathrm{CN} = 2$, the mean coverage is $\\lambda_{i,0} = \\mu \\cdot m_i \\cdot \\frac{2}{2} = \\mu \\cdot m_i$. Therefore, the variance of $X_i$ under the null is:\n$$\n\\mathrm{Var}(X_i \\mid H_0) = \\frac{\\lambda_{i,0}}{(\\mu \\cdot m_i)^2} = \\frac{\\mu \\cdot m_i}{(\\mu \\cdot m_i)^2} = \\frac{1}{\\mu \\cdot m_i}\n$$\nThis corresponds to the provided formula $\\frac{\\mathrm{CN}_0}{2 \\cdot \\mu \\cdot m_i}$ with $\\mathrm{CN}_0 = 2$.\nAssuming independence of the $X_i$ variables, the variance of their sum is the sum of their variances. Thus, the variance of $\\widehat{\\mathrm{CN}}$ under $H_0$ is:\n$$\n\\mathrm{Var}_0(\\widehat{\\mathrm{CN}}) = \\mathrm{Var}\\left(\\frac{2}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} X_i \\mid H_0 \\right) = \\left(\\frac{2}{|\\mathcal{I}|}\\right)^2 \\sum_{i \\in \\mathcal{I}} \\mathrm{Var}(X_i \\mid H_0) = \\frac{4}{|\\mathcal{I}|^2} \\sum_{i \\in \\mathcal{I}} \\frac{1}{\\mu \\cdot m_i}\n$$\nWith the mean and variance defined, we can construct a $z$-score test statistic, which measures the deviation of the estimate from its null-hypothesized value in units of standard deviations:\n$$\nz = \\frac{\\widehat{\\mathrm{CN}} - \\mathbb{E}[\\widehat{\\mathrm{CN}} \\mid H_0]}{\\sqrt{\\mathrm{Var}_0(\\widehat{\\mathrm{CN}})}} = \\frac{\\widehat{\\mathrm{CN}} - 2}{\\sqrt{\\mathrm{Var}_0(\\widehat{\\mathrm{CN}})}}\n$$\nUnder the null hypothesis, this $z$-score approximately follows a standard normal distribution, $\\mathcal{N}(0, 1)$.\n\nBefore conducting the test, a filtering procedure is applied to ensure the reliability of the call. This is crucial as low-mappability regions can produce noisy and biased coverage data, while very short windows lack statistical power. The specified filters are:\n1.  Per-base mappability filter: Only bases $i$ with mappability $m_i \\ge t_{\\mathrm{map}}$ are included in the analysis. This defines the set $\\mathcal{I}$.\n2.  Window quality filters:\n    -   The number of retained bases must meet a minimum count: $|\\mathcal{I}| \\ge n_{\\min}$.\n    -   The fraction of retained bases must meet a minimum threshold: $\\frac{|\\mathcal{I}|}{n} \\ge f_{\\min}$, where $n$ is the total number of bases in the original window.\nIf a window fails any of these quality filters, it is deemed inconclusive, and the output is $0$ (neutral).\n\nFinally, for windows that pass the filters, the CNV state is classified based on the calculated $z$-score and a pre-defined significance threshold $z_{\\mathrm{thr}}$. A symmetric threshold is used to call deletions or duplications:\n- If $z \\ge z_{\\mathrm{thr}}$, the coverage is significantly higher than expected under the null, implying a duplication. The output is $+1$.\n- If $z \\le -z_{\\mathrm{thr}}$, the coverage is significantly lower than expected, implying a deletion. The output is $-1$.\n- If $-z_{\\mathrm{thr}}  z  z_{\\mathrm{thr}}$, the deviation is not statistically significant. The output is $0$ (neutral).\n\nThis complete scheme provides a principled, statistically grounded method for CNV detection that correctly normalizes for mappability and filters for data quality, thereby mitigating common sources of false positives.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CNV detection problem for the given test suite.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"id\": 1,\n            \"mu\": 30.0, \"t_map\": 0.5, \"f_min\": 0.6, \"n_min\": 5, \"z_thr\": 3.0,\n            \"C\": np.array([31, 29, 33, 30, 28, 32, 31, 30, 29, 34]),\n            \"m\": np.array([0.99, 0.98, 1.0, 0.97, 0.96, 0.99, 0.98, 1.0, 0.95, 0.97])\n        },\n        {\n            \"id\": 2,\n            \"mu\": 30.0, \"t_map\": 0.5, \"f_min\": 0.6, \"n_min\": 5, \"z_thr\": 3.0,\n            \"C\": np.array([9, 11, 8, 12, 10, 9, 10, 9, 8, 10]),\n            \"m\": np.array([0.30, 0.35, 0.25, 0.40, 0.33, 0.28, 0.31, 0.29, 0.27, 0.34])\n        },\n        {\n            \"id\": 3,\n            \"mu\": 30.0, \"t_map\": 0.5, \"f_min\": 0.6, \"n_min\": 5, \"z_thr\": 3.0,\n            \"C\": np.array([15, 14, 16, 13, 17, 15, 14, 16]),\n            \"m\": np.array([0.98, 0.97, 0.99, 0.96, 0.97, 0.98, 0.99, 0.97])\n        },\n        {\n            \"id\": 4,\n            \"mu\": 30.0, \"t_map\": 0.5, \"f_min\": 0.6, \"n_min\": 5, \"z_thr\": 3.0,\n            \"C\": np.array([36, 31, 27, 18, 25, 41, 38, 23, 20, 31, 29, 27]),\n            \"m\": np.array([0.8, 0.7, 0.6, 0.4, 0.55, 0.9, 0.85, 0.5, 0.45, 0.7, 0.65, 0.6])\n        },\n        {\n            \"id\": 5,\n            \"mu\": 30.0, \"t_map\": 0.5, \"f_min\": 0.6, \"n_min\": 5, \"z_thr\": 3.0,\n            \"C\": np.array([30, 31, 29]),\n            \"m\": np.array([0.95, 0.96, 0.97])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        C = case[\"C\"]\n        m = case[\"m\"]\n        mu = case[\"mu\"]\n        t_map = case[\"t_map\"]\n        f_min = case[\"f_min\"]\n        n_min = case[\"n_min\"]\n        z_thr = case[\"z_thr\"]\n        \n        n = len(C) # Original window size\n\n        # Step 1: Apply mappability filter to get the set of indices I\n        mappable_indices = np.where(m = t_map)[0]\n        I_size = len(mappable_indices)\n        \n        # Step 2: Apply window quality filters\n        if I_size  n_min or (I_size / n)  f_min:\n            results.append(0)\n            continue\n            \n        C_I = C[mappable_indices]\n        m_I = m[mappable_indices]\n\n        # Step 3: Calculate the CNV estimate\n        # X_i = C_i / (mu * m_i) for i in I\n        X_I = C_I / (mu * m_I)\n        \n        # CN_hat = (2 / |I|) * sum(X_i) = 2 * mean(X_I)\n        CN_hat = 2.0 * np.mean(X_I)\n        \n        # Step 4: Calculate the variance of the estimator under the null (CN=2)\n        # Var_0(CN_hat) = (4 / |I|^2) * sum(1 / (mu * m_i)) for i in I\n        var_sum_term = np.sum(1.0 / (mu * m_I))\n        var_CN_hat_null = (4.0 / (I_size**2)) * var_sum_term\n        \n        # Step 5: Compute the z-score\n        # Prevent division by zero if variance is somehow zero\n        if var_CN_hat_null = 0:\n            results.append(0) # Inconclusive if variance is not positive\n            continue\n\n        std_dev_CN_hat_null = np.sqrt(var_CN_hat_null)\n        z_score = (CN_hat - 2.0) / std_dev_CN_hat_null\n\n        # Step 6: Classify the CNV state\n        if z_score = -z_thr:\n            results.append(-1) # Deletion\n        elif z_score = z_thr:\n            results.append(1) # Duplication\n        else:\n            results.append(0) # Neutral\n            \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}