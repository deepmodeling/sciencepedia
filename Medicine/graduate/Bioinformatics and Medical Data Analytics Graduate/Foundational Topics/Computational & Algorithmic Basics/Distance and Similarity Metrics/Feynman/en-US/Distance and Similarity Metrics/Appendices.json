{
    "hands_on_practices": [
        {
            "introduction": "In many bioinformatics applications, individuals or samples are represented as points in a high-dimensional feature space. This exercise explores how the choice of distance metric can profoundly affect our interpretation of similarity, particularly in the presence of outliers common in clinical lab data. By calculating both the Manhattan ($L_1$) and Euclidean ($L_2$) distances for two patient profiles, you will gain a concrete understanding of why the $L_2$ metric is more sensitive to extreme values, a critical consideration for robust data analysis .",
            "id": "4558075",
            "problem": "A research team models each patient’s laboratory profile as a vector in a feature space where each coordinate is a standardized score obtained by z-scoring that laboratory measure across a large reference cohort. Consider the following six features in the order: fasting plasma glucose (FPG), hemoglobin A1c (HbA1c), systolic blood pressure (SBP), low-density lipoprotein (LDL), C-reactive protein (CRP), and body mass index (BMI). Two patients are represented by standardized vectors:\n$$\nx_A = (0.5,\\; 0.1,\\; -0.6,\\; -0.2,\\; 0.3,\\; 0.4), \\quad\nx_B = (9.5,\\; 0.2,\\; 0.8,\\; 0.8,\\; -0.1,\\; 0.4).\n$$\nThe difference between the two patients is dominated by a single extreme fasting plasma glucose measurement in patient $B$, while the other features differ modestly.\n\nStarting from the definitions of the Manhattan metric (also called the $L_1$ metric) and the Euclidean metric (also called the $L_2$ metric) on $\\mathbb{R}^d$, and treating the laboratory vectors as points in $\\mathbb{R}^6$, compute the $L_1$ and $L_2$ distances between $x_A$ and $x_B$. Then, briefly justify which metric is more influenced by the single extreme coordinate in this setting and why, using only properties that follow from the definitions.\n\nProvide your final numerical distances as exact values without rounding, and report them in the order $L_1$ distance, then $L_2$ distance, as a pair. Do not include any units in your final answer.",
            "solution": "The context is a finite-dimensional real vector space where each patient is a point in $\\mathbb{R}^6$ consisting of z-scored laboratory values. The Manhattan metric ($L_1$) and the Euclidean metric ($L_2$) are induced by the corresponding norms on $\\mathbb{R}^d$, defined for a vector $v = (v_1,\\dots,v_d)$ by\n$$\n\\|v\\|_1 = \\sum_{i=1}^{d} |v_i|, \\quad\n\\|v\\|_2 = \\left(\\sum_{i=1}^{d} v_i^2\\right)^{1/2}.\n$$\nGiven two points $x, y \\in \\mathbb{R}^d$, the induced metric distances are $d_1(x,y) = \\|x-y\\|_1$ and $d_2(x,y) = \\|x-y\\|_2$.\n\nWe first compute the coordinate-wise difference vector $x_B - x_A$:\n- Fasting plasma glucose (FPG): $9.5 - 0.5 = 9.0$.\n- Hemoglobin A1c (HbA1c): $0.2 - 0.1 = 0.1$.\n- Systolic blood pressure (SBP): $0.8 - (-0.6) = 1.4$.\n- Low-density lipoprotein (LDL): $0.8 - (-0.2) = 1.0$.\n- C-reactive protein (CRP): $-0.1 - 0.3 = -0.4$.\n- Body mass index (BMI): $0.4 - 0.4 = 0.0$.\n\nThus,\n$$\nx_B - x_A = (9.0,\\; 0.1,\\; 1.4,\\; 1.0,\\; -0.4,\\; 0.0).\n$$\n\nCompute the $L_1$ distance:\n$$\nd_1(x_A, x_B) \\;=\\; |9.0| + |0.1| + |1.4| + |1.0| + |{-0.4}| + |0.0|\n= 9.0 + 0.1 + 1.4 + 1.0 + 0.4 + 0\n= 11.9.\n$$\nAs an exact rational number, $11.9 = \\frac{119}{10}$.\n\nCompute the $L_2$ distance by summing squared differences:\n$$\n\\begin{aligned}\nd_2(x_A, x_B)\n&= \\left( (9.0)^2 + (0.1)^2 + (1.4)^2 + (1.0)^2 + (-0.4)^2 + (0.0)^2 \\right)^{1/2} \\\\\n&= \\left( 81 + 0.01 + 1.96 + 1 + 0.16 + 0 \\right)^{1/2}\n= \\left( 84.13 \\right)^{1/2}.\n\\end{aligned}\n$$\nIn exact fractional form, $84.13 = \\frac{8413}{100}$, so\n$$\nd_2(x_A, x_B) = \\frac{\\sqrt{8413}}{10}.\n$$\n\nJustification of outlier influence based on definitions: In the $L_1$ distance, each coordinate contributes linearly via $|v_i|$, so a single large coordinate contributes in proportion to its magnitude. In the $L_2$ distance, each coordinate contributes quadratically via $v_i^2$ before taking a square root, which amplifies the relative influence of larger coordinates. In this example, the squared contribution of the extreme glucose coordinate is $81$, while the sum of squared contributions from all other coordinates is $0.01 + 1.96 + 1 + 0.16 + 0 = 3.13$. The fraction of the total squared sum attributable to glucose is therefore\n$$\n\\frac{81}{81 + 3.13} \\;=\\; \\frac{81}{84.13} \\;=\\; \\frac{8100}{8413},\n$$\nwhich is substantially larger than the fraction of the total $L_1$ distance attributable to glucose,\n$$\n\\frac{9.0}{11.9} \\;=\\; \\frac{90}{119}.\n$$\nThese ratios, derived solely from the metric definitions, show that the Euclidean ($L_2$) metric is more influenced by a single extreme coordinate than the Manhattan ($L_1$) metric in this setting.\n\nTherefore, the exact distances are $d_1(x_A,x_B) = \\frac{119}{10}$ and $d_2(x_A,x_B) = \\frac{\\sqrt{8413}}{10}$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{119}{10} & \\frac{\\sqrt{8413}}{10}\\end{pmatrix}}$$"
        },
        {
            "introduction": "We now shift from continuous vector spaces to the discrete world of biological sequences. Comparing DNA or protein sequences is a foundational task in bioinformatics, and simple identity checks are insufficient to capture evolutionary relationships. This problem introduces the concept of edit distance, specifically a biologically-informed weighted Levenshtein distance, which quantifies sequence dissimilarity based on the minimum cost to transform one sequence into another using operations like insertions, deletions, and substitutions with varying penalties .",
            "id": "4558085",
            "problem": "In Deoxyribonucleic Acid (DNA) sequence analysis, distances between strings over the nucleotide alphabet $\\Sigma = \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$ are often defined by counting weighted edits required to transform one string into another. Begin from the core definition of a metric and the concept of an edit script composed of insertions, deletions, and substitutions with nonnegative costs. Using that foundation, formally define a weighted Levenshtein edit distance $d:\\Sigma^{\\ast} \\times \\Sigma^{\\ast} \\to \\mathbb{R}_{\\ge 0}$ for DNA that assigns a cost $c_{\\mathrm{ins}}$ to insertions, a cost $c_{\\mathrm{del}}$ to deletions, and a substitution cost $c_{\\mathrm{sub}}(a,b)$ that depends on whether the substitution is a transition or a transversion: a transition is a substitution between purines $\\{\\text{A}, \\text{G}\\}$ or between pyrimidines $\\{\\text{C}, \\text{T}\\}$, and a transversion is any other mismatch. Then, with the specific costs $c_{\\mathrm{ins}} = 2$, $c_{\\mathrm{del}} = 2$, $c_{\\mathrm{ts}} = 1$ for transitions, $c_{\\mathrm{tv}} = 3$ for transversions, and $c_{\\mathrm{sub}}(a,b) = 0$ if $a=b$, compute the distance between the two short gene sequences\n$$\nx = \\text{ACGTCGTA}, \\quad y = \\text{AGTTTGCA}.\n$$\nYour final answer must be a single real-valued number. No rounding is required.",
            "solution": "The problem requires the formal definition of a weighted Levenshtein distance for DNA sequences and the computation of this distance between two specific sequences, $x = \\text{ACGTCGTA}$ and $y = \\text{AGTTTGCA}$. The solution will proceed in two parts: first, a formal definition of the distance and its properties, and second, the explicit computation using a dynamic programming algorithm.\n\nA distance function $d$ on a set of objects is a metric if it satisfies four axioms for any objects $x, y, z$:\n1.  Non-negativity: $d(x,y) \\ge 0$.\n2.  Identity of indiscernibles: $d(x,y) = 0$ if and only if $x=y$.\n3.  Symmetry: $d(x,y) = d(y,x)$.\n4.  Triangle inequality: $d(x,z) \\le d(x,y) + d(y,z)$.\n\nA weighted Levenshtein distance is defined as the minimum cost to transform one string into another using a set of weighted edit operations: insertion, deletion, and substitution. Let $\\Sigma$ be our alphabet, which is $\\Sigma = \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$. For any two strings $x, y \\in \\Sigma^{\\ast}$, the distance $d(x,y)$ is the minimum total cost of an edit script (a sequence of operations) that converts $x$ to $y$. The costs for the operations are given as:\n-   Insertion cost: $c_{\\mathrm{ins}} = 2$.\n-   Deletion cost: $c_{\\mathrm{del}} = 2$.\n-   Substitution cost $c_{\\mathrm{sub}}(a,b)$ for $a,b \\in \\Sigma$.\n\nThe substitution cost depends on whether the substitution is a transition or a transversion. A transition is a substitution within the same class of nucleotide: either between purines $\\{\\text{A}, \\text{G}\\}$ or between pyrimidines $\\{\\text{C}, \\text{T}\\}$. A transversion is a substitution between different classes (a purine for a pyrimidine or vice versa). The costs are:\n-   $c_{\\mathrm{sub}}(a,a) = 0$ (match).\n-   $c_{\\mathrm{sub}}(a,b) = c_{\\mathrm{ts}} = 1$ for a transition.\n-   $c_{\\mathrm{sub}}(a,b) = c_{\\mathrm{tv}} = 3$ for a transversion.\n\nThe given costs ($c_{\\mathrm{ins}} = c_{\\mathrm{del}} = 2 > 0$, and substitution costs $0, 1, 3 \\ge 0$) ensure non-negativity and the identity of indiscernibles. Symmetry is satisfied because $c_{\\mathrm{ins}} = c_{\\mathrm{del}}$ and the substitution cost matrix is symmetric, i.e., $c_{\\mathrm{sub}}(a,b) = c_{\\mathrm{sub}}(b,a)$. The triangle inequality also holds for this cost structure. Thus, the defined distance is a valid metric.\n\nThe substitution cost matrix $S(a,b) = c_{\\mathrm{sub}}(a,b)$ can be explicitly written as:\n$$\n\\begin{array}{c|cccc}\nc_{\\mathrm{sub}} & \\text{A} & \\text{C} & \\text{G} & \\text{T} \\\\\n\\hline\n\\text{A} & 0 & 3 & 1 & 3 \\\\\n\\text{C} & 3 & 0 & 3 & 1 \\\\\n\\text{G} & 1 & 3 & 0 & 3 \\\\\n\\text{T} & 3 & 1 & 3 & 0 \\\\\n\\end{array}\n$$\n\nTo compute the distance $d(x,y)$ for $x = \\text{ACGTCGTA}$ and $y = \\text{AGTTTGCA}$, we use the Wagner-Fischer algorithm. This dynamic programming approach constructs a matrix $D$ of size $(|x|+1) \\times (|y|+1)$, where $|x|=8$ and $|y|=8$. The entry $D_{i,j}$ stores the minimum cost to transform the prefix $x[1..i]$ to $y[1..j]$.\n\nThe matrix is initialized as follows:\n$D_{0,0} = 0$\n$D_{i,0} = i \\times c_{\\mathrm{del}} = 2i$ for $i \\in \\{1, \\dots, 8\\}$.\n$D_{0,j} = j \\times c_{\\mathrm{ins}} = 2j$ for $j \\in \\{1, \\dots, 8\\}$.\n\nThe remaining entries are filled using the recurrence relation:\n$$\nD_{i,j} = \\min \\begin{cases} D_{i-1,j} + c_{\\mathrm{del}} \\\\ D_{i,j-1} + c_{\\mathrm{ins}} \\\\ D_{i-1,j-1} + c_{\\mathrm{sub}}(x_i, y_j) \\end{cases}\n= \\min( D_{i-1,j} + 2, D_{i,j-1} + 2, D_{i-1,j-1} + c_{\\mathrm{sub}}(x_i, y_j) )\n$$\nThe calculation proceeds row by row. For example, to compute $D_{1,1}$:\n$x_1 = \\text{A}$, $y_1 = \\text{A}$. $c_{\\mathrm{sub}}(\\text{A},\\text{A}) = 0$.\n$D_{1,1} = \\min(D_{0,1}+2, D_{1,0}+2, D_{0,0}+0) = \\min(2+2, 2+2, 0+0) = 0$.\n\nTo compute $D_{1,2}$:\n$x_1 = \\text{A}$, $y_2 = \\text{G}$. $c_{\\mathrm{sub}}(\\text{A},\\text{G}) = 1$ (transition).\n$D_{1,2} = \\min(D_{0,2}+2, D_{1,1}+2, D_{0,1}+1) = \\min(4+2, 0+2, 2+1) = \\min(6, 2, 3) = 2$.\n\nBy completing this process for the entire matrix, we obtain the following table, where rows correspond to prefixes of $x$ (indexed by $i$) and columns to prefixes of $y$ (indexed by $j$):\n\n$$\n\\begin{array}{c|c|cccccccc}\n  & j & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\\\\ni & & \\text{ } & \\text{A} & \\text{G} & \\text{T} & \\text{T} & \\text{T} & \\text{G} & \\text{C} & \\text{A} \\\\\n\\hline\n0 & \\text{ } & 0 & 2 & 4 & 6 & 8 & 10 & 12 & 14 & 16 \\\\\n1 & \\text{A} & 2 & 0 & 2 & 4 & 6 & 8 & 10 & 12 & 14 \\\\\n2 & \\text{C} & 4 & 2 & 3 & 3 & 5 & 7 & 9 & 10 & 12 \\\\\n3 & \\text{G} & 6 & 4 & 2 & 4 & 6 & 8 & 7 & 9 & 11 \\\\\n4 & \\text{T} & 8 & 6 & 4 & 2 & 4 & 6 & 8 & 8 & 10 \\\\\n5 & \\text{C} & 10 & 8 & 6 & 4 & 3 & 5 & 7 & 8 & 10 \\\\\n6 & \\text{G} & 12 & 10 & 8 & 6 & 5 & 6 & 5 & 7 & 9 \\\\\n7 & \\text{T} & 14 & 12 & 10 & 8 & 6 & 5 & 7 & 6 & 8 \\\\\n8 & \\text{A} & 16 & 14 & 12 & 10 & 8 & 7 & 6 & 8 & \\mathbf{6} \\\\\n\\end{array}\n$$\nThe distance $d(x,y)$ is the value in the bottom-right cell of the matrix, $D_{|x|,|y|} = D_{8,8}$.\nLet us verify the final entry, $D_{8,8}$, which corresponds to aligning $x_8=\\text{A}$ and $y_8=\\text{A}$. The substitution cost is $c_{\\mathrm{sub}}(\\text{A},\\text{A})=0$.\n$$\nD_{8,8} = \\min(D_{7,8}+2, D_{8,7}+2, D_{7,7}+0) = \\min(8+2, 8+2, 6+0) = \\min(10, 10, 6)=6.\n$$\nThe minimum cost to transform string $x$ into string $y$ is $6$.\nA possible optimal alignment derived from tracing back the calculations is:\n$$\n\\begin{array}{cccccccccc}\nx: & \\text{A} & \\text{C} & \\text{G} & - & \\text{T} & \\text{C} & \\text{G} & \\text{T} & \\text{A} \\\\\ny: & \\text{A} & - & \\text{G} & \\text{T} & \\text{T} & \\text{T} & \\text{G} & \\text{C} & \\text{A} \\\\\n\\text{Cost}: & 0 & 2 & 0 & 2 & 0 & 1 & 0 & 1 & 0 \\\\\n\\end{array}\n$$\nThe total cost of this sequence of operations (match, deletion, match, insertion, match, substitution, match, substitution, match) is $0+2+0+2+0+1+0+1+0 = 6$.",
            "answer": "$$\n\\boxed{6}\n$$"
        },
        {
            "introduction": "Patient monitoring generates vast streams of physiological time-series data, which require specialized metrics that are robust to temporal shifts and warping. This advanced exercise delves into Dynamic Time Warping (DTW), a powerful algorithm for measuring similarity between two temporal sequences that may vary in speed. You will not only analyze its computational complexity but also address a critical real-world challenge: designing and implementing a provably correct lower-bounding technique to make similarity searches in large time-series databases computationally feasible .",
            "id": "4558111",
            "problem": "A health system stores many patient physiological time series, such as heart rate and glucose, with each time series represented as a real-valued sequence. The similarity between a query time series and a candidate is computed using Dynamic Time Warping (DTW), which, by design, allows local time distortions subject to monotonicity and continuity constraints. In this setting, computing DTW for every candidate can be computationally expensive. Your task is twofold: analyze the computational complexity of DTW and design a lower-bounding pruning strategy using the Keogh Lower Bound (LB_Keogh) to make retrieval scalable. The program you produce must implement these concepts and demonstrate their effect on pruning.\n\nBegin from fundamental definitions:\n- A time series is a sequence $Q = (q_1, q_2, \\dots, q_n)$ and $C = (c_1, c_2, \\dots, c_m)$ with $q_i, c_j \\in \\mathbb{R}$ for all indices.\n- The local alignment cost uses the squared Euclidean difference $d(q_i, c_j) = (q_i - c_j)^2$.\n- Dynamic Time Warping (DTW) aligns $Q$ and $C$ under monotone, continuous warping paths and aggregates the local alignment costs along the optimal path. Constrain the admissible alignments using the Sakoe–Chiba band of half-width $w$, which restricts the allowed pairs $(i,j)$ to those satisfying $|i - j| \\leq w$.\n- The lower-bounding pruning strategy must use the Keogh Lower Bound (LB_Keogh) principle to produce a provable lower bound on the DTW distance. The bound must be computable in time sublinear in the full alignment grid and must be safe, in the sense that pruning based on the bound does not remove any sequence whose true DTW distance is less than or equal to a given threshold.\n\nYou must:\n1. Derive the time and space complexity of DTW in terms of $n$, $m$, and $w$ under the Sakoe–Chiba constraint, starting from the above definitions and reasoning from first principles (do not assume shortcut formulas).\n2. Derive LB_Keogh as a valid lower bound for DTW using only the envelope concept created by the candidate time series within the Sakoe–Chiba band and the properties of the local cost metric $d(\\cdot,\\cdot)$, and prove it is a lower bound. Do not assume or state the target formula a priori; instead derive it carefully from the bounding principle.\n3. Implement a program that:\n   - Computes the LB_Keogh bound between $Q$ and $C$ under a given $w$ and uses it to prune candidates whose lower bound exceeds a threshold $\\tau$.\n   - Computes the exact DTW distance under the Sakoe–Chiba band $w$ only for candidates not pruned by LB_Keogh.\n   - Returns the indices (zero-based) of candidates whose DTW distance is less than or equal to $\\tau$. Also report the number of candidates pruned and the number of exact DTW computations performed.\n   - Uses early abandonment both in LB_Keogh summation and in DTW dynamic programming when the partial evidence guarantees that exceeding $\\tau$ is unavoidable.\n\nUse the following test suite. Each test case specifies $(Q, \\text{candidates}, w, \\tau)$, where the candidates are a list of time series $C^{(k)}$ indexed by $k = 0, 1, 2, 3, 4$, and the query $Q$ is as given:\n\n- Test case $1$ (general case, modest warping, moderate threshold):\n  - $Q = [$ $71$, $73$, $72$, $74$, $76$, $75$, $77$, $79$ $]$\n  - candidates:\n    - $C^{(0)} = [$ $70$, $72$, $71$, $73$, $75$, $74$, $76$, $78$ $]$\n    - $C^{(1)} = [$ $71$, $74$, $72$, $75$, $78$, $74$, $77$, $80$ $]$\n    - $C^{(2)} = [$ $90$, $91$, $92$, $93$, $94$, $95$, $96$, $97$ $]$\n    - $C^{(3)} = [$ $71$, $73$, $72$, $74$, $76$, $75$, $77$, $79$ $]$\n    - $C^{(4)} = [$ $68$, $70$, $69$, $71$, $73$, $72$, $74$, $76$ $]$\n  - $w = $ $1$, $\\ \\tau = $ $20$.\n\n- Test case $2$ (boundary case $w = 0$ ensuring no warping):\n  - $Q = [$ $71$, $73$, $72$, $74$, $76$, $75$, $77$, $79$ $]$\n  - candidates are as in Test case $1$\n  - $w = $ $0$, $\\ \\tau = $ $8$.\n\n- Test case $3$ (length mismatch, larger band, tighter threshold):\n  - $Q = [$ $72$, $74$, $76$, $75$, $77$, $79$ $]$\n  - candidates are as in Test case $1$\n  - $w = $ $2$, $\\ \\tau = $ $15$.\n\n- Test case $4$ (large band and high threshold to admit moderately similar sequences):\n  - $Q = [$ $71$, $73$, $72$, $74$, $76$, $75$, $77$, $79$ $]$\n  - candidates are as in Test case $1$\n  - $w = $ $4$, $\\ \\tau = $ $150$.\n\n- Test case $5$ (zero threshold to return only exact matches):\n  - $Q = [$ $71$, $73$, $72$, $74$, $76$, $75$, $77$, $79$ $]$\n  - candidates are as in Test case $1$\n  - $w = $ $1$, $\\ \\tau = $ $0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the form $[$pruned\\_count,computed\\_count,returned\\_indices$]$, and returned\\_indices is a list of integers in increasing order. For example, for two cases the format would be $[[p_1,c_1,[i_{1,1},i_{1,2}]], [p_2,c_2,[i_{2,1}]]]$. There are no physical units or angles involved; all outputs are integers and lists.",
            "solution": "The problem requires a two-part theoretical analysis followed by a practical implementation of Dynamic Time Warping (DTW) with a Sakoe-Chiba band constraint and a pruning strategy using the Keogh Lower Bound (LB_Keogh).\n\n### Part 1: Complexity Analysis of Dynamic Time Warping\n\nLet the two time series be $Q = (q_1, q_2, \\dots, q_n)$ and $C = (c_1, c_2, \\dots, c_m)$. The local alignment cost between points $q_i$ and $c_j$ is given by $d(q_i, c_j) = (q_i - c_j)^2$.\n\nDynamic Time Warping finds the optimal alignment path between $Q$ and $C$ that minimizes the total accumulated cost. This is typically solved using dynamic programming. Let $D(i, j)$ be the minimum accumulated cost of an alignment path from the start $(q_1, c_1)$ to the point $(q_i, c_j)$. The standard recurrence relation for $D(i, j)$ is:\n$$D(i, j) = d(q_i, c_j) + \\min\\{D(i-1, j), D(i, j-1), D(i-1, j-1)\\}$$\nThis recurrence is computed for $i$ from $1$ to $n$ and $j$ from $1$ to $m$. The boundary conditions are typically set as $D(0, 0) = 0$ and $D(i, 0) = D(0, j) = \\infty$ for $i>0, j>0$. The final DTW distance is given by $D(n, m)$.\n\n**Time Complexity (Unconstrained):** To compute the final value $D(n, m)$, we must fill an $n \\times m$ cost matrix. The computation of each cell $D(i, j)$ involves a constant number of arithmetic operations (one squaring, one subtraction, one addition, and one minimum of three values). Therefore, the total time complexity is proportional to the number of cells, which is $n \\times m$. The complexity is $O(nm)$.\n\n**Space Complexity (Unconstrained):** A naive implementation would store the entire $n \\times m$ matrix $D$, leading to a space complexity of $O(nm)$. However, the computation of row $i$ only requires values from row $i-1$ and potentially the current row $i$. By storing only the previous row and the current row being computed, the space can be reduced to $O(m)$. If we iterate column-wise, it becomes $O(n)$. Thus, space complexity can be optimized to $O(\\min(n, m))$.\n\n**Effect of the Sakoe-Chiba Band:**\nThe Sakoe-Chiba band of half-width $w$ is a global constraint that restricts the set of allowed alignments. A pair $(i, j)$ is only considered if it satisfies $|i - j| \\leq w$. This means the optimal path is confined to a diagonal band in the cost matrix.\n\n**Time Complexity (with Sakoe-Chiba Band):**\nWith this constraint, we no longer need to compute the entire $n \\times m$ matrix. For each row $i$ (from $1$ to $n$), we only need to compute $D(i, j)$ for values of $j$ such that $|i - j| \\leq w$. This means $j$ must be in the range $[i-w, i+w]$. The number of relevant $j$ values for each $i$ is at most $(i+w) - (i-w) + 1 = 2w+1$.\nSince we do this for each of the $n$ rows (assuming $n \\approx m$ for simplicity), the total number of cells to compute is approximately $n \\times (2w+1)$. Therefore, the time complexity is reduced to $O(nw)$.\n\n**Space Complexity (with Sakoe-Chiba Band):**\nSimilar to the unconstrained case, we can optimize space. To compute the values in row $i$ within the band, we only need the values from row $i-1$ that are also within or near the band. Specifically, to compute $D(i,j)$, we need $D(i-1, j-1)$, $D(i-1, j)$, and $D(i-1, j+1)$ is not needed, but for $D(i, j+1)$ one would need $D(i-1, j)$. The required portion of the previous row has a width of approximately $2w+1$. Thus, by storing only the relevant part of the previous row, the space complexity can be reduced to $O(w)$.\n\n### Part 2: Derivation of the Keogh Lower Bound (LB_Keogh)\n\nThe goal is to find a function $LB_{Keogh}(Q, C)$ that is computationally cheaper than $DTW(Q, C)$ and satisfies $LB_{Keogh}(Q, C) \\leq DTW(Q, C)$ for any $Q$ and $C$.\n\nLet the time series $Q$ be of length $n$. The essence of the LB_Keogh bound is to find, for each point $q_i$ in the query $Q$, the minimum possible contribution it can make to the total DTW cost, irrespective of the final warping path.\n\n**1. Constructing the Envelope:**\nFor the candidate time series $C$ of length $m$ and a warping window $w$, we define an \"envelope\" consisting of an upper bound sequence $U$ and a lower bound sequence $L$. For each time index $i$ corresponding to an index in $Q$ (from $1$ to $n$), $U_i$ and $L_i$ are defined as:\n$$U_i = \\max\\{c_j \\mid \\max(1, i-w) \\leq j \\leq \\min(m, i+w)\\}$$\n$$L_i = \\min\\{c_j \\mid \\max(1, i-w) \\leq j \\leq \\min(m, i+w)\\}$$\nThe sequences $U$ and $L$ form a bounding envelope around the time series $C$ within the allowed warping flexibility.\n\n**2. Local Cost Lower Bound:**\nConsider a point $q_i$ from the query $Q$. In any valid DTW alignment path constrained by the Sakoe-Chiba band, $q_i$ must be aligned with some point $c_j$ from $C$ where $|i-j| \\leq w$. By the definition of the envelope, any such $c_j$ must lie within the interval $[L_i, U_i]$.\n\nThe contribution of this single alignment $(q_i, c_j)$ to the total cost is $(q_i - c_j)^2$. We seek a lower bound for this value. The value of $(q_i - y)^2$ is minimized when $y$ is the point in the interval $[L_i, U_i]$ closest to $q_i$.\n- If $q_i > U_i$, the closest point is $U_i$. The minimum squared distance is $(q_i - U_i)^2$.\n- If $q_i < L_i$, the closest point is $L_i$. The minimum squared distance is $(q_i - L_i)^2$.\n- If $L_i \\leq q_i \\leq U_i$, $q_i$ is inside the envelope. The closest point is $q_i$ itself, so the minimum squared distance is $0$.\n\nCombining these, the lower bound on the cost contribution for a single point $q_i$ is:\n$$ \\text{cost}_i = \\begin{cases} (q_i - U_i)^2 & \\text{if } q_i > U_i \\\\ (q_i - L_i)^2 & \\text{if } q_i < L_i \\\\ 0 & \\text{if } L_i \\leq q_i \\leq U_i \\end{cases} $$\nThis $\\text{cost}_i$ is a lower bound for any term $(q_i - c_j)^2$ where $(i,j)$ is an allowable alignment.\n\n**3. Summation to Form the Global Lower Bound:**\nThe Keogh lower bound, $LB_{Keogh}(Q,C)$, is defined as the sum of these individual minimum costs over all points in the query $Q$:\n$$LB_{Keogh}(Q, C) = \\sum_{i=1}^{n} \\text{cost}_i$$\n\n**4. Proof of Lower Bound Validity:**\nLet $\\pi^* = ((i_1, j_1), (i_2, j_2), \\dots, (i_K, j_K))$ be the optimal warping path from $(1,1)$ to $(n,m)$ for the standard DTW recurrence $D(i, j) = d(q_i, c_j) + \\min(\\dots)$. The total cost $DTW(Q,C) = D(n,m)$ can be shown to be the sum of local costs $d(q_{i_k}, c_{j_k})$ for all cells $(i_k, j_k)$ on this path.\n$$DTW(Q,C) = \\sum_{k=1}^{K} (q_{i_k} - c_{j_k})^2$$\nFor each point $(i_k, j_k)$ on the path, the Sakoe-Chiba constraint $|i_k - j_k| \\leq w$ holds. This implies that $c_{j_k}$ is within the range used to construct the envelope at index $i_k$, i.e., $L_{i_k} \\leq c_{j_k} \\leq U_{i_k}$.\nFrom our derivation of $\\text{cost}_i$, we know that for any validly aligned pair $(q_{i_k}, c_{j_k})$, the cost term $(q_{i_k} - c_{j_k})^2$ is greater than or equal to the minimal possible cost for $q_{i_k}$, which is $\\text{cost}_{i_k}$.\n$$(q_{i_k} - c_{j_k})^2 \\geq \\text{cost}_{i_k}$$\nSumming over all points in the optimal path:\n$$DTW(Q,C) = \\sum_{k=1}^{K} (q_{i_k} - c_{j_k})^2 \\geq \\sum_{k=1}^{K} \\text{cost}_{i_k}$$\nThe sequence of first indices of the path, $i_1, i_2, \\dots, i_K$, is a non-decreasing sequence that starts at $1$ and ends at $n$. By the properties of a valid warping path, this sequence must contain every integer from $1$ to $n$ at least once. Since each $\\text{cost}_i \\geq 0$:\n$$\\sum_{k=1}^{K} \\text{cost}_{i_k} \\geq \\sum_{i=1}^{n} \\text{cost}_i$$\nCombining the inequalities:\n$$DTW(Q,C) \\geq \\sum_{i=1}^{n} \\text{cost}_i = LB_{Keogh}(Q, C)$$\nThis proves that $LB_{Keogh}$ is a valid lower bound for DTW. Its computation requires one pass to build the envelope ($O(m \\cdot w)$) and one pass to sum the costs ($O(n)$), making it significantly faster than the $O(nm)$ or $O(nw)$ DTW computation.\n\n### Part 3: Implementation Strategy\n\nThe program will be structured as follows:\n\n1.  **`lb_keogh(Q, C, w, tau)` function:**\n    *   Takes a query `Q`, a candidate `C`, a window `w`, and a threshold `tau`.\n    *   Computes the envelope sequences `L` and `U` for `C` with respect to the indices of `Q`. For each `i` from `0` to `len(Q)-1`, define the window on `C`'s indices and find the min/max value of `C` in that window.\n    *   Initializes a running sum `lb_sum = 0.0`.\n    *   Iterates `i` from `0` to `len(Q)-1`. In each step, it calculates `cost_i` and adds it to `lb_sum`.\n    *   **Early Abandonment:** After each addition, it checks if `lb_sum > tau`. If so, it immediately returns `lb_sum` (or infinity), as the bound has already exceeded the threshold.\n    *   If the loop completes, it returns the final `lb_sum`.\n\n2.  **`dtw(Q, C, w, tau)` function:**\n    *   Takes `Q`, `C`, `w`, and `tau`.\n    *   Implements the DTW dynamic programming algorithm with space optimization. Two rows of the DP table are sufficient: `prev_row` and `current_row`. Their size will be `len(C) + 1`.\n    *   The outer loop iterates `i` from `1` to `len(Q)`. The inner loop iterates `j` over the Sakoe-Chiba band: from `max(1, i-w)` to `min(len(C), i+w)`.\n    *   **Early Abandonment:** After completing each row `i`, it finds the minimum value in `current_row` within the band. If this minimum is greater than `tau`, it means no path through this row can result in a final cost less than or equal to `tau`, so the function can terminate early and return a value greater than `tau` (e.g., infinity).\n\n3.  **Main processing loop:**\n    *   For each test case, it iterates through the provided `candidates`.\n    *   For each candidate `C^{(k)}`, it first computes `lb = lb_keogh(Q, C^{(k)}, w, tau)`.\n    *   If `lb > tau`, the candidate is pruned. `pruned_count` is incremented.\n    *   If `lb <= tau`, the candidate is not pruned. `computed_count` is incremented, and the exact `dist = dtw(Q, C^{(k)}, w, tau)` is calculated.\n    *   If `dist <= tau`, the index `k` is added to the `returned_indices` list.\n    *   Finally, it assembles the results `[pruned_count, computed_count, returned_indices]` for the test case and adds it to a list of all results. The final output is printed in the specified format.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    \n    # Define the common candidates for several test cases\n    common_candidates = [\n        np.array([70, 72, 71, 73, 75, 74, 76, 78], dtype=float),\n        np.array([71, 74, 72, 75, 78, 74, 77, 80], dtype=float),\n        np.array([90, 91, 92, 93, 94, 95, 96, 97], dtype=float),\n        np.array([71, 73, 72, 74, 76, 75, 77, 79], dtype=float),\n        np.array([68, 70, 69, 71, 73, 72, 74, 76], dtype=float)\n    ]\n\n    test_cases = [\n        {\n            \"Q\": np.array([71, 73, 72, 74, 76, 75, 77, 79], dtype=float),\n            \"candidates\": common_candidates,\n            \"w\": 1,\n            \"tau\": 20.0,\n        },\n        {\n            \"Q\": np.array([71, 73, 72, 74, 76, 75, 77, 79], dtype=float),\n            \"candidates\": common_candidates,\n            \"w\": 0,\n            \"tau\": 8.0,\n        },\n        {\n            \"Q\": np.array([72, 74, 76, 75, 77, 79], dtype=float),\n            \"candidates\": common_candidates,\n            \"w\": 2,\n            \"tau\": 15.0,\n        },\n        {\n            \"Q\": np.array([71, 73, 72, 74, 76, 75, 77, 79], dtype=float),\n            \"candidates\": common_candidates,\n            \"w\": 4,\n            \"tau\": 150.0,\n        },\n        {\n            \"Q\": np.array([71, 73, 72, 74, 76, 75, 77, 79], dtype=float),\n            \"candidates\": common_candidates,\n            \"w\": 1,\n            \"tau\": 0.0,\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        all_results.append(\n            process_case(case[\"Q\"], case[\"candidates\"], case[\"w\"], case[\"tau\"])\n        )\n\n    # Convert the Python list of lists into the required string format\n    print(str(all_results).replace(\" \", \"\"))\n\n\ndef lb_keogh(Q, C, w, tau):\n    \"\"\"\n    Computes the Keogh Lower Bound (LB_Keogh) with early abandonment.\n    \"\"\"\n    n = len(Q)\n    m = len(C)\n    \n    # Create envelope for C\n    L = np.zeros(n)\n    U = np.zeros(n)\n    for i in range(n):\n        start = max(0, i - w)\n        end = min(m, i + w + 1)\n        if start >= end:\n             # This can happen if Q is much longer than C and w is small\n             return float('inf')\n        window = C[start:end]\n        L[i] = np.min(window)\n        U[i] = np.max(window)\n\n    lb_sum = 0.0\n    for i in range(n):\n        q_i = Q[i]\n        if q_i > U[i]:\n            lb_sum += (q_i - U[i])**2\n        elif q_i  L[i]:\n            lb_sum += (q_i - L[i])**2\n        \n        # Early abandonment for LB_Keogh\n        if lb_sum > tau:\n            return lb_sum\n    \n    return lb_sum\n\ndef dtw(Q, C, w, tau):\n    \"\"\"\n    Computes Dynamic Time Warping distance with Sakoe-Chiba band and early abandonment.\n    \"\"\"\n    n, m = len(Q), len(C)\n    \n    # Using two rows for space optimization\n    prev_row = np.full(m + 1, float('inf'))\n    curr_row = np.full(m + 1, float('inf'))\n    prev_row[0] = 0.0\n\n    for i in range(1, n + 1):\n        # Reset current row for this iteration of i\n        # curr_row's first element corresponds to D(i, 0), which is infinity\n        curr_row.fill(float('inf'))\n        \n        # Determine the band for the current row\n        j_start = max(1, i - w)\n        j_end = min(m, i + w) + 1\n\n        for j in range(j_start, j_end):\n            cost = (Q[i - 1] - C[j - 1])**2\n            min_prev_cost = min(prev_row[j], prev_row[j - 1], curr_row[j - 1])\n            curr_row[j] = cost + min_prev_cost\n        \n        # Early abandonment for DTW\n        min_in_row = np.min(curr_row[j_start:j_end]) if j_start  j_end else float('inf')\n\n        if min_in_row > tau:\n            return float('inf')\n\n        prev_row, curr_row = curr_row, prev_row\n\n    return prev_row[m]\n\n\ndef process_case(Q, candidates, w, tau):\n    \"\"\"\n    Processes one test case: prunes candidates and computes DTW for the rest.\n    \"\"\"\n    pruned_count = 0\n    computed_count = 0\n    returned_indices = []\n\n    for k, C in enumerate(candidates):\n        # Step 1: Lower-bounding\n        lb = lb_keogh(Q, C, w, tau)\n\n        if lb > tau:\n            pruned_count += 1\n        else:\n            # Step 2: Exact DTW computation\n            computed_count += 1\n            dist = dtw(Q, C, w, tau)\n            if dist = tau:\n                returned_indices.append(k)\n    \n    return [pruned_count, computed_count, returned_indices]\n\nsolve()\n```"
        }
    ]
}