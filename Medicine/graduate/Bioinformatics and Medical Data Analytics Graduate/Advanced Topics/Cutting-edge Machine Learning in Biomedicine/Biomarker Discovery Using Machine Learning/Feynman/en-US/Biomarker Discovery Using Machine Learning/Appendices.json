{
    "hands_on_practices": [
        {
            "introduction": "The validity of any predictive model rests on the integrity of its training and validation data. A common and critical pitfall is 'label leakage,' where the model is inadvertently exposed to information it would not have at the time of a real-world decision. This practice challenges you to identify these subtle but fatal flaws in a prognostic biomarker study, reinforcing the foundational principles of building a trustworthy model. ",
            "id": "4543002",
            "problem": "You are developing a prognostic classifier for cancer progression using omics biomarkers. For each patient $i \\in \\{1,\\dots,n\\}$, you have an index (baseline) time $t_{0,i}$ at diagnosis, a baseline omics vector $X_{0,i} \\in \\mathbb{R}^p$ measured at time $t_{0,i}$, treatment $T_i$ initiated after $t_{0,i}$, an early treatment response score $R_i$ measured at $t_{0,i} + \\Delta$ for some fixed $\\Delta \\approx 3$ months, a last contact (follow-up) time $F_i \\ge t_{0,i}$, and a binary label $Y_i \\in \\{0,1\\}$ indicating whether progression occurred within $5$ years from $t_{0,i}$ (coded as $Y_i=1$ if the event occurred before or at $t_{0,i}+5$ years, $Y_i=0$ otherwise). Assume some patients are right-censored at $C_i$ with $C_i \\le t_{0,i}+5$ years, but labels $Y_i$ are consistently defined using standard time-to-event processing. Batches of assays are run on calendar days, yielding a batch identifier $B_i$ determined by assay date. You intend to learn a predictor $f:\\mathbb{R}^p \\to [0,1]$ to approximate the conditional probability $P(Y=1 \\mid X_0)$ at decision time $t_0$.\n\nThe foundational base for this task is as follows:\n- In supervised learning, the target is to approximate the Bayes decision rule using only information available at decision time $t_0$. Formally, letting $\\mathcal{F}_{t}$ denote the sigma-algebra generated by all measurements available up to and including time $t$, a valid predictor at baseline must be $\\mathcal{F}_{t_0}$-measurable. The population risk under a loss $\\ell$ is $\\mathcal{R}(f) = \\mathbb{E}[\\ell(Y, f(X_0))]$, where $X_0$ denotes the baseline feature vector.\n- A validation estimator is unbiased for deployment risk only if the training and validation sets are conditionally independent given the data-generating process and the validation pipeline does not use validation information during training or preprocessing. In particular, data transformations must be fit without access to the validation fold.\n- If a feature $X_j$ is a function of future information or of the outcome $Y$ or its downstream consequences, then conditioning on $X_j$ at $t_0$ violates the information constraint and can induce optimistic bias in estimated performance.\n\nYou are asked to define the risk of label leakage and provide principles to prevent using post-outcome variables or future information as features during training and validation in this biomarker discovery setting. Consider the following candidate practices.\n\nSelect all options that correctly define the risk and state valid principles to prevent leakage in both training and validation.\n\nA. The risk of label leakage arises when the feature set contains any variable not measurable with respect to $\\mathcal{F}_{t_0}$, including variables $X_j$ with timestamps $t_j > t_0$ or variables that are deterministic or stochastic functions of $Y$. A principled prevention is to restrict the predictor to $X_0$ and to enforce a time-aware split such that all training patients satisfy $t_{0,i} \\le t_{\\mathrm{cut}}$ and all validation patients satisfy $t_{0,i} > t_{\\mathrm{cut}}$, ensuring that no future-derived information at $t>t_0$ is used in model fitting or validation.\n\nB. It is acceptable to include $R_i$ (the early response at $t_{0,i}+\\Delta$) and $T_i$ as features because they are predictive; to avoid leakage, one can use stratified $K$-fold Cross-Validation (CV) with shuffling, since stratification on $Y$ controls for label imbalance and therefore also controls for leakage.\n\nC. Performing $z$-score normalization of each gene and univariate feature selection using two-sample $t$-tests on $X_0$ with respect to $Y$ on the entire dataset prior to any train-validation split is acceptable because the normalization is unsupervised and the $t$-test feature selection will be repeated inside cross-validation; this improves stability and does not introduce leakage.\n\nD. In $K$-fold Cross-Validation (CV), every data-dependent transformation, including imputation, scaling, empirical Bayes batch correction such as Combining Batches (ComBat), and feature selection, must be fit using only the training indices in each fold and then applied to the held-out fold; batch indicators $B_i$ should not be used as predictive features if they are functions of calendar time correlated with $Y$. This prevents both direct and indirect leakage from validation into training and from future information into features.\n\nE. In the presence of right-censoring, including the derived feature $F_i - t_{0,i}$ (“days from baseline to last contact”) helps address informative censoring and is therefore a principled way to reduce leakage while improving discrimination.\n\nF. To prevent patient-level leakage when multiple samples per subject exist (for example, technical replicates or longitudinal draws), split data at the subject level so that no subject’s samples appear in both training and validation. If measurements occur across time, impose $t_{\\max}^{\\text{train}} < t_{\\min}^{\\text{test}}$ to respect chronology. This protects against both subject leakage and inadvertent use of future information.\n\nChoose all that apply. Your answer should be the set of correct option letters.",
            "solution": "The problem statement describes a standard task in prognostic biomarker discovery using machine learning. It asks for the identification of correct principles and practices to define and prevent label leakage. The problem setup is detailed, internally consistent, and scientifically grounded in the fields of biostatistics, bioinformatics, and machine learning. The provided foundational principles are correct statements of statistical learning theory. The problem is therefore valid.\n\nThe core principle for preventing label leakage in this context is to ensure that the predictor $f$ is a function of only information that is available at the decision time $t_0$. This is formally stated by requiring the predictor to be $\\mathcal{F}_{t_0}$-measurable. Furthermore, any validation procedure must yield an unbiased estimate of the model's performance on new patients, which requires strict separation of training and validation data at all stages of model development, including preprocessing.\n\nLet us evaluate each option based on these principles.\n\n**A. The risk of label leakage arises when the feature set contains any variable not measurable with respect to $\\mathcal{F}_{t_0}$, including variables $X_j$ with timestamps $t_j > t_0$ or variables that are deterministic or stochastic functions of $Y$. A principled prevention is to restrict the predictor to $X_0$ and to enforce a time-aware split such that all training patients satisfy $t_{0,i} \\le t_{\\mathrm{cut}}$ and all validation patients satisfy $t_{0,i} > t_{\\mathrm{cut}}$, ensuring that no future-derived information at $t>t_0$ is used in model fitting or validation.**\n\nThis option provides a correct and formal definition of label leakage in a temporal setting. The requirement that all information must be measurable with respect to the sigma-algebra at baseline, $\\mathcal{F}_{t_0}$, is the precise mathematical formulation of the \"no future information\" rule. Variables measured after $t_0$ (e.g., $R_i$) or variables constructed from the outcome $Y$ (which itself depends on events up to $5$ years post-$t_0$) are not $\\mathcal{F}_{t_0}$-measurable and constitute leakage.\n\nThe proposed prevention strategy is also sound.\n1.  Restricting the predictor to features $X_0$ is correct because $X_0$ is, by definition, measured at $t_{0,i}$ and thus is $\\mathcal{F}_{t_{0}}$-measurable.\n2.  Enforcing a chronological or time-aware split (e.g., training on patients diagnosed before a cutoff date $t_{\\mathrm{cut}}$ and validating on patients diagnosed after) is a robust method to prevent the model from learning spurious temporal trends. This split structure ensures that the validation mimics a real-world deployment scenario where a model trained on past data is applied to future patients.\n\nTherefore, this statement correctly defines the risk and proposes valid and principled prevention methods.\n\nVerdict: **Correct**.\n\n**B. It is acceptable to include $R_i$ (the early response at $t_{0,i}+\\Delta$) and $T_i$ as features because they are predictive; to avoid leakage, one can use stratified $K$-fold Cross-Validation (CV) with shuffling, since stratification on $Y$ controls for label imbalance and therefore also controls for leakage.**\n\nThis statement is fundamentally flawed.\n1.  The feature $R_i$ is the early treatment response measured at time $t_{0,i} + \\Delta$, which is after the decision time $t_0$. It is not $\\mathcal{F}_{t_0}$-measurable. Including it in a model intended for prognosis at baseline ($t_0$) is a direct violation of the information constraint and a classic example of label leakage. While $R_i$ might be highly predictive, its value is not known at the time of prediction.\n2.  The treatment $T_i$ is also a post-baseline event. Including it changes the quantity being estimated from $P(Y=1 \\mid X_0)$ to $P(Y=1 \\mid X_0, T_i, \\dots)$. This is a different, and often easier, prediction task that does not answer the prognostic question posed at baseline.\n3.  The proposed prevention method is incorrect and illogical. Stratified $K$-fold CV ensures that class proportions are maintained across folds. Shuffling randomizes the order of the data. Neither of these actions can remedy the inclusion of an invalid, post-baseline feature. In fact, for data with a temporal structure, shuffling is contraindicated as it can introduce leakage by allowing the model to be trained on \"future\" data points to predict \"past\" ones. Stratification does not \"control for leakage\".\n\nVerdict: **Incorrect**.\n\n**C. Performing $z$-score normalization of each gene and univariate feature selection using two-sample $t$-tests on $X_0$ with respect to $Y$ on the entire dataset prior to any train-validation split is acceptable because the normalization is unsupervised and the $t$-test feature selection will be repeated inside cross-validation; this improves stability and does not introduce leakage.**\n\nThis statement describes two canonical examples of data leakage during the preprocessing phase.\n1.  **Normalization on the entire dataset**: $Z$-score normalization requires computing the mean and standard deviation of each feature. If these parameters are computed using the *entire* dataset (including training and validation folds), the transformation of the training data is influenced by the validation data. This is a subtle but clear violation of the principle that the validation set must be held out entirely.\n2.  **Feature selection on the entire dataset**: Performing feature selection using the outcome label $Y$ (e.g., via $t$-tests) on the entire dataset before splitting is a severe form of label leakage. It means the features were selected with knowledge of the validation set's labels. This leads to an information leak that makes the chosen features inherently biased to perform well on the validation set, yielding a deceptively optimistic estimate of the model's true performance.\n\nThe justifications provided are specious. The fact that normalization is \"unsupervised\" is irrelevant; the leak comes from using the validation data to fit the normalizer. The claim that the $t$-test \"will be repeated inside cross-validation\" is nonsensical if the selection has already been performed on the whole dataset; the damage is already done.\n\nVerdict: **Incorrect**.\n\n**D. In $K$-fold Cross-Validation (CV), every data-dependent transformation, including imputation, scaling, empirical Bayes batch correction such as Combining Batches (ComBat), and feature selection, must be fit using only the training indices in each fold and then applied to the held-out fold; batch indicators $B_i$ should not be used as predictive features if they are functions of calendar time correlated with $Y$.**\n\nThis option correctly describes the gold standard for implementing preprocessing within a cross-validation framework.\n1.  The first part of the statement is a critical principle for obtaining an unbiased performance estimate. Any step that \"learns\" from data—be it calculating means for scaling, medians for imputation, parameters for batch correction, or feature-ranking criteria for selection—must be executed *only* on the training portion of the data for that specific fold. The fitted transformation is then applied to both the training and validation portions. This methodology is encapsulated in tools like `sklearn.pipeline.Pipeline` and is essential to prevent leakage from the validation set into the training process.\n2.  The second part addresses a subtle but important issue. The batch identifier $B_i$ is a function of the assay's calendar date. If there are secular trends in the population, treatment, or even survival outcomes over time, $B_i$ may become spuriously correlated with the outcome $Y$. Including such a feature can cause the model to learn a time-based artifact rather than the underlying biological signal, compromising its generalizability. It is correct to use $B_i$ to *correct* for batch effects but risky to use it as a predictive feature.\n\nVerdict: **Correct**.\n\n**E. In the presence of right-censoring, including the derived feature $F_i - t_{0,i}$ (“days from baseline to last contact”) helps address informative censoring and is therefore a principled way to reduce leakage while improving discrimination.**\n\nThis statement is dangerously incorrect. The feature $F_i - t_{0,i}$ represents the observed follow-up time. The outcome label $Y_i$ (progression within $5$ years) is itself determined by the event time and censoring time. The observed follow-up time is $F_i - t_{0,i} = \\min(\\text{EventTime}_i, \\text{CensoringTime}_i)$.\nFor a patient who experiences the event (i.e., $Y_i=1$), the follow-up time *is* the event time. Providing this as a feature is equivalent to providing a significant part of the answer to the model during training. For example, the model would learn the trivial and useless rule: if $F_i - t_{0,i} < 5$ years, the patient likely has $Y_i=1$. This is circular reasoning. The model would show spectacularly high performance in cross-validation but would be useless in practice, as for a new patient, the follow-up time is the very thing we wish to predict. This is a severe form of label leakage, and the claim that it \"reduce[s] leakage\" is precisely the opposite of the truth.\n\nVerdict: **Incorrect**.\n\n**F. To prevent patient-level leakage when multiple samples per subject exist (for example, technical replicates or longitudinal draws), split data at the subject level so that no subject’s samples appear in both training and validation. If measurements occur across time, impose $t_{\\max}^{\\text{train}} < t_{\\min}^{\\text{test}}$ to respect chronology. This protects against both subject leakage and inadvertent use of future information.**\n\nThis option describes two essential principles for robust validation, especially in biomedical settings.\n1.  **Subject-level splitting**: When the data are not independent and identically distributed (i.i.d.), such as when there are multiple samples from the same patient, a simple random split is invalid. Samples from the same patient are highly correlated. If they appear in both training and validation sets, the model can \"memorize\" patient-specific characteristics, leading to an overly optimistic performance evaluation. The correct procedure is to perform splits at the patient level (e.g., using `GroupKFold`), ensuring that all data from a given patient belongs to only one fold.\n2.  **Chronological splitting**: As also noted in option A, when data has a temporal component, splits must respect the arrow of time. The condition $t_{\\max}^{\\text{train}} < t_{\\min}^{\\text{test}}$ (where $t$ is a relevant timestamp like diagnosis or sample date) ensures that the model is always trained on the past to predict the future. This prevents leakage from future data and provides a more realistic estimate of deployment performance.\n\nBoth principles are correct and fundamental for preventing leakage in complex data structures.\n\nVerdict: **Correct**.\n\nIn summary, options A, D, and F correctly state principles for defining and preventing data and label leakage in the given context.",
            "answer": "$$\\boxed{ADF}$$"
        },
        {
            "introduction": "Biomedical datasets are often characterized by significant class imbalance, and the clinical consequences of prediction errors are rarely symmetrical. A model that ignores these realities may achieve high accuracy but have little clinical value. This exercise explores how to align your model with clinical priorities by properly handling asymmetric error costs and imbalanced data through methods like stratified validation and cost-sensitive learning. ",
            "id": "4542961",
            "problem": "A clinical biomarker discovery study uses gene expression features to predict a binary disease phenotype, encoded as $Y \\in \\{0,1\\}$ where $Y=1$ denotes the presence of a condition requiring an immediate intervention, and $Y=0$ denotes absence. In the target population, the disease prevalence is low, with $P(Y=1)\\ll P(Y=0)$. The study aims to train a classifier $f:\\mathcal{X}\\to\\{0,1\\}$ that minimizes clinical decision cost under asymmetric error costs: a false negative (predicting $0$ when $Y=1$) incurs cost $c_{FN}>0$, and a false positive (predicting $1$ when $Y=0$) incurs cost $c_{FP}>0$, with $c_{FN}$ typically larger than $c_{FP}$ due to missed treatment risk. The team will estimate generalization performance via $k$-fold Cross-Validation (CV), and contemplates stratified CV, cost-sensitive learning, and class resampling.\n\nUsing the fundamental definition of expected loss (risk) $R(f)=\\mathbb{E}[L(Y,f(X))]$ with task-appropriate $0\\text{-}1$ loss weighted by clinical costs, and basic sampling considerations under class-prior imbalance, select all statements that are correct.\n\nA. Class imbalance in this setting means $P(Y=1)\\neq P(Y=0)$, and specifically $P(Y=1)\\ll P(Y=0)$. Unweighted accuracy misaligns with clinical utility when error costs are asymmetric, and stratified $k$-fold CV, which preserves the empirical class ratio within each fold, reduces the variance of risk estimates under unequal class priors.\n\nB. Under asymmetric error costs with $c_{FN}\\gg c_{FP}$, the Bayes-optimal decision rule that minimizes expected clinical cost predicts $Y=1$ when the posterior odds $\\frac{P(Y=1\\mid x)}{P(Y=0\\mid x)}$ exceed the ratio $\\frac{c_{FP}}{c_{FN}}$, and equivalently, cost-sensitive empirical risk minimization can be implemented by weighting the positive-class loss by $c_{FN}$ and the negative-class loss by $c_{FP}$.\n\nC. Random oversampling of the minority class or undersampling of the majority class changes the conditional likelihoods $p(x\\mid y)$ learned by the model and therefore the Bayes-optimal classifier; because of this, resampling is generally inconsistent and should never be used to address asymmetric error costs.\n\nD. Stratified $k$-fold CV is required to achieve unbiased estimation of sensitivity and specificity under class imbalance, because non-stratified splits necessarily inflate the minority-class frequency in some folds, leading to upward bias in sensitivity and downward bias in specificity.\n\nE. When $P(Y=1)=0.05$, $c_{FN}=10$, and $c_{FP}=1$, a calibrated probabilistic classifier should set a fixed decision threshold at $t^{\\star}=\\frac{c_{FP}}{c_{FN}}=\\frac{1}{10}$ for $P(Y=1\\mid x)$; in parallel, cost-sensitive learning that weights the positive class by $10$ and the negative class by $1$ optimizes the same expected-cost objective.",
            "solution": "The problem statement describes a binary classification task in a clinical setting characterized by class imbalance and asymmetric error costs. The objective is to evaluate several statements regarding classification strategy and performance evaluation under these conditions.\n\nFirst, let's formalize the expected cost (risk) and the optimal decision rule. The problem defines a binary phenotype $Y \\in \\{0, 1\\}$, where $Y=1$ is the positive class (disease) and $Y=0$ is the negative class (absence). A classifier $f(X)$ maps features $X$ to a prediction $\\hat{Y} \\in \\{0, 1\\}$.\n\nThe cost-weighted loss function $L(Y, \\hat{Y})$ is defined as:\n- $L(1, 0) = c_{FN}$ (False Negative cost)\n- $L(0, 1) = c_{FP}$ (False Positive cost)\n- $L(1, 1) = 0$ (True Positive, zero cost)\n- $L(0, 0) = 0$ (True Negative, zero cost)\n\nThe expected risk of a classifier $f$ is $R(f) = \\mathbb{E}_{X,Y}[L(Y, f(X))]$. The Bayes-optimal classifier $f^*$ is the one that minimizes this risk. For any given input $x$, it does so by minimizing the conditional expected cost:\n$$\n\\mathbb{E}_{Y|X=x}[L(Y, f(x))]\n$$\nTo make a decision for a given $x$, we compare the expected cost of predicting $\\hat{Y}=1$ versus $\\hat{Y}=0$. Let $p(x) = P(Y=1 \\mid X=x)$ be the true posterior probability of the positive class. Then $P(Y=0 \\mid X=x) = 1 - p(x)$.\n\nThe conditional expected cost of predicting $\\hat{Y}=1$ is:\n$$\nC(\\text{predict } 1 \\mid x) = L(1,1)P(Y=1 \\mid x) + L(0,1)P(Y=0 \\mid x) = 0 \\cdot p(x) + c_{FP} \\cdot (1-p(x)) = c_{FP}(1-p(x))\n$$\nThe conditional expected cost of predicting $\\hat{Y}=0$ is:\n$$\nC(\\text{predict } 0 \\mid x) = L(1,0)P(Y=1 \\mid x) + L(0,0)P(Y=0 \\mid x) = c_{FN} \\cdot p(x) + 0 \\cdot (1-p(x)) = c_{FN}p(x)\n$$\nThe Bayes-optimal decision is to predict $\\hat{Y}=1$ if its cost is lower, i.e., $C(\\text{predict } 1 \\mid x) < C(\\text{predict } 0 \\mid x)$:\n$$\nc_{FP}(1-p(x)) < c_{FN}p(x)\n$$\nThis inequality can be rearranged in two useful ways:\n1. In terms of posterior odds: $\\frac{p(x)}{1-p(x)} = \\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)} > \\frac{c_{FP}}{c_{FN}}$.\n2. In terms of posterior probability: $c_{FP} < c_{FP}p(x) + c_{FN}p(x) \\implies c_{FP} < (c_{FP} + c_{FN})p(x) \\implies p(x) > \\frac{c_{FP}}{c_{FP} + c_{FN}}$.\nThe term $\\frac{c_{FP}}{c_{FP} + c_{FN}}$ is the optimal decision threshold for the posterior probability $p(x)$.\n\nWith these principles established, we evaluate each option.\n\nA. Class imbalance in this setting means $P(Y=1)\\neq P(Y=0)$, and specifically $P(Y=1)\\ll P(Y=0)$. Unweighted accuracy misaligns with clinical utility when error costs are asymmetric, and stratified $k$-fold CV, which preserves the empirical class ratio within each fold, reduces the variance of risk estimates under unequal class priors.\n- The first clause correctly defines class imbalance as described in the problem, where the disease prevalence is low ($P(Y=1)\\ll P(Y=0)$).\n- The second clause is also correct. Unweighted accuracy, defined as $\\frac{TP+TN}{N}$, assigns equal weight to all classification errors. With low prevalence and $c_{FN} > c_{FP}$, a classifier that always predicts the majority class ($Y=0$) can achieve very high accuracy (approaching $P(Y=0)$), but it would incur a massive clinical cost by missing every single positive case. Therefore, accuracy is a poor measure of clinical utility here.\n- The third clause is a key principle of cross-validation. In standard $k$-fold CV with low prevalence, random assignment of samples to folds can result in some test folds having zero or very few positive instances. This makes metrics like sensitivity ($TP/(TP+FN)$) or precision ($TP/(TP+FP)$) highly unstable (high variance) or even undefined across folds. Stratified $k$-fold CV ensures that the class distribution in each fold mirrors that of the overall dataset, leading to more stable and reliable (lower variance) estimates of performance metrics.\nAll three parts of this statement are correct.\n**Verdict: Correct**\n\nB. Under asymmetric error costs with $c_{FN}\\gg c_{FP}$, the Bayes-optimal decision rule that minimizes expected clinical cost predicts $Y=1$ when the posterior odds $\\frac{P(Y=1\\mid x)}{P(Y=0\\mid x)}$ exceed the ratio $\\frac{c_{FP}}{c_{FN}}$, and equivalently, cost-sensitive empirical risk minimization can be implemented by weighting the positive-class loss by $c_{FN}$ and the negative-class loss by $c_{FP}$.\n- The first part of the statement describes the optimal decision rule in terms of posterior odds. As derived above, the rule to predict $Y=1$ is $\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)} > \\frac{c_{FP}}{c_{FN}}$. This is a direct consequence of minimizing the expected cost and is mathematically correct.\n- The second part discusses an implementation strategy. Cost-sensitive learning aims to train a model that implicitly learns this optimal rule. A standard method is to modify the objective function (empirical risk) that is minimized during training. Instead of minimizing $\\sum_i L(y_i, f(x_i))$, one minimizes a weighted sum: $\\sum_{i: y_i=1} c_{FN} \\cdot \\ell(y_i, f(x_i)) + \\sum_{i: y_i=0} c_{FP} \\cdot \\ell(y_i, f(x_i))$, where $\\ell$ is a base loss like log-loss. This forces the learning algorithm to pay more attention to avoiding errors on the positive (high-cost) class. This is a standard and correct formulation of cost-sensitive learning that is equivalent to minimizing the target clinical cost.\nBoth parts of the statement are correct.\n**Verdict: Correct**\n\nC. Random oversampling of the minority class or undersampling of the majority class changes the conditional likelihoods $p(x\\mid y)$ learned by the model and therefore the Bayes-optimal classifier; because of this, resampling is generally inconsistent and should never be used to address asymmetric error costs.\n- The statement claims that resampling changes the conditional likelihoods $p(x \\mid y)$. This is fundamentally incorrect. Resampling techniques (like duplicating samples from the minority class or discarding samples from the majority class) alter the class priors $P(Y)$ in the training set. They do not change the underlying distribution of features for a given class, $p(x \\mid y)$. A generative model trained on resampled data would still estimate the correct $p(x \\mid y)$ from the available samples, but it would estimate the wrong class prior $P'(Y)$ from the now-balanced dataset. A discriminative model would learn a biased posterior $P'(Y \\mid x)$ because of the changed priors.\n- The conclusion that resampling \"should never be used\" is an overly strong and false generalization. While resampling can introduce bias into posterior probability estimates if not corrected for, it is a widely used and often effective heuristic to combat the practical problems that severe class imbalance poses to many learning algorithms (e.g., preventing the model from simply ignoring the minority class). Undersampling can be very efficient, and oversampling variants like SMOTE are standard practice. The assertion is wrong.\n**Verdict: Incorrect**\n\nD. Stratified $k$-fold CV is required to achieve unbiased estimation of sensitivity and specificity under class imbalance, because non-stratified splits necessarily inflate the minority-class frequency in some folds, leading to upward bias in sensitivity and downward bias in specificity.\n- The claim that stratification is \"required to achieve unbiased estimation\" is incorrect. The primary purpose of stratification is to reduce the variance of the cross-validation estimator, not to correct for bias. The cross-validation estimator itself is generally considered to be approximately unbiased, and any bias is typically small and a subject of complex debate; it is not a settled issue that non-stratified CV is biased and stratified CV is not.\n- The causal explanation is also flawed. First, non-stratified splits do not \"necessarily\" inflate the minority-class frequency; it is a matter of random chance, and some folds may have deflated frequencies. Second, it is not clear that an inflated minority frequency would lead to a systematic \"upward bias in sensitivity\". The estimated sensitivity in a given fold depends on the classifier's performance on the specific instances in that fold. While fold composition affects the estimate, there is no simple, direct line to systematic bias in this manner. The main problem is instability (high variance) from folds with too few minority samples.\n**Verdict: Incorrect**\n\nE. When $P(Y=1)=0.05$, $c_{FN}=10$, and $c_{FP}=1$, a calibrated probabilistic classifier should set a fixed decision threshold at $t^{\\star}=\\frac{c_{FP}}{c_{FN}}=\\frac{1}{10}$ for $P(Y=1\\mid x)$; in parallel, cost-sensitive learning that weights the positive class by $10$ and the negative class by $1$ optimizes the same expected-cost objective.\n- The first part of the statement provides a specific threshold for the posterior probability $P(Y=1 \\mid x)$. As derived from first principles, the optimal decision threshold for the posterior probability is $t^{\\star} = \\frac{c_{FP}}{c_{FP} + c_{FN}}$. With the given values $c_{FN}=10$ and $c_{FP}=1$, the correct threshold is $t^{\\star} = \\frac{1}{1 + 10} = \\frac{1}{11} \\approx 0.0909$. The statement incorrectly claims the threshold is $t^{\\star} = \\frac{c_{FP}}{c_{FN}} = \\frac{1}{10} = 0.1$. The ratio $\\frac{c_{FP}}{c_{FN}}$ is the threshold for the posterior odds, not the posterior probability. Therefore, this part of the statement is mathematically incorrect. The prior probability $P(Y=1)=0.05$ is irrelevant for determining the optimal decision threshold, which depends only on the costs.\n- The second part of the statement, regarding cost-sensitive learning by weighting the positive class by $c_{FN}=10$ and the negative class by $c_{FP}=1$, is a correct description of the methodology, as explained in the analysis of option B.\n- However, since the first part of the statement contains a clear mathematical error, the statement as a whole is incorrect.\n**Verdict: Incorrect**\n\nSummary of verdicts:\nA: Correct\nB: Correct\nC: Incorrect\nD: Incorrect\nE: Incorrect\n\nFinal correct statements are A and B.",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "Beyond standard performance metrics, the ultimate test of a biomarker is its ability to improve clinical decisions and patient outcomes. Decision Curve Analysis (DCA) provides a framework for evaluating this real-world utility by quantifying the net benefit of using a model to guide treatment choices. This hands-on problem will guide you through the derivation and calculation of net benefit, translating a model's probabilistic predictions into a clear measure of clinical value. ",
            "id": "4542960",
            "problem": "A research team has developed a multi-omic biomarker panel, integrated into a logistic regression classifier, to predict a patient’s risk of developing aggressive disease within $12$ months in a high-risk clinical cohort. To assess whether the model’s probabilistic predictions can improve clinical decision-making, the team applies Decision Curve Analysis (DCA). In DCA, clinical decisions are operationalized by a threshold risk probability $p_{t}$ at which a clinician would choose to initiate treatment. Let $B$ denote the expected clinical benefit from appropriately treating a truly diseased patient, and let $H$ denote the expected clinical harm from unnecessarily treating a non-diseased patient. Under expected utility principles, a rational treatment decision treats a patient when $p B - (1-p) H > 0$, and the threshold $p_{t}$ satisfies equality at indifference. \n\nStarting from fundamental expected utility and the definitions of sensitivity and specificity:\n- Sensitivity is the probability that a diseased patient is classified as positive.\n- Specificity is the probability that a non-diseased patient is classified as negative.\n\nDerive, from first principles, a net benefit metric used in DCA that measures the clinical utility of using the classifier at threshold $p_{t}$, expressed as a function of the counts of true positives $TP$, false positives $FP$, total cohort size $N$, and the threshold probability $p_{t}$.\n\nThen, consider an independent validation cohort with $N = 1200$ patients, disease prevalence $\\pi = 0.30$, threshold probability $p_{t} = 0.20$, and the classifier operating at that threshold exhibits sensitivity $0.80$ and specificity $0.85$. Compute the net benefit at $p_{t}$ for this cohort using the derived expression. Express the final net benefit as a reduced fraction. If you choose to report a decimal value instead, round your answer to four significant figures. The net benefit is dimensionless.",
            "solution": "The user has provided a problem that requires two tasks: first, to derive the formula for the net benefit metric used in Decision Curve Analysis (DCA) from first principles, and second, to apply this formula to calculate the net benefit for a specific validation cohort. The problem validation will be performed first.\n\n### Step 1: Extract Givens\n- A logistic regression classifier predicts disease risk with probability $p$.\n- A clinician treats a patient if $p$ is greater than a threshold probability $p_t$.\n- $B$ is the expected clinical benefit from treating a truly diseased patient.\n- $H$ is the expected clinical harm from unnecessarily treating a non-diseased patient.\n- The decision to treat is made when the expected utility is positive: $p B - (1-p) H > 0$.\n- The threshold $p_t$ is defined by the indifference point: $p_t B - (1-p_t) H = 0$.\n- Sensitivity is the probability that a diseased patient is classified as positive.\n- Specificity is the probability that a non-diseased patient is classified as negative.\n- The net benefit metric is to be expressed as a function of true positives ($TP$), false positives ($FP$), total cohort size ($N$), and the threshold probability ($p_t$).\n- For the calculation part:\n    - Total cohort size, $N = 1200$.\n    - Disease prevalence, $\\pi = 0.30$.\n    - Threshold probability, $p_t = 0.20$.\n    - Sensitivity at $p_t$, $\\text{Sens} = 0.80$.\n    - Specificity at $p_t$, $\\text{Spec} = 0.85$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. Decision Curve Analysis is a standard, well-validated statistical method for evaluating prediction models and diagnostic tests, first described by Vickers and Elkin. The derivation from expected utility theory is the formal basis of this method. The definitions of sensitivity, specificity, prevalence, $TP$, and $FP$ are fundamental concepts in epidemiology and biostatistics. The problem is well-posed; it provides all necessary information for both the derivation and the subsequent calculation. The language is objective and precise. The numerical values provided for the validation cohort are realistic. Therefore, the problem is deemed valid.\n\n### Part 1: Derivation of the Net Benefit Metric\n\nThe net benefit of a predictive model at a given risk threshold $p_t$ measures the clinical utility of using the model to make decisions, compared to a default strategy of treating all or no patients. The derivation begins from the concept of expected utility provided.\n\nA clinician using a model will decide to treat any patient whose predicted risk $p$ is at or above the threshold $p_t$. For a cohort of $N$ patients, this decision-making process results in a certain number of true positive decisions ($TP$, diseased patients correctly identified for treatment) and false positive decisions ($FP$, non-diseased patients incorrectly identified for treatment).\n\nThe total utility or benefit gained from the $TP$ decisions is $TP \\times B$.\nThe total disutility or harm incurred from the $FP$ decisions is $FP \\times H$.\n\nThe overall net benefit in raw utility terms is the sum of benefits minus the sum of harms:\n$$ \\text{Net Benefit}_{\\text{raw}} = (TP \\times B) - (FP \\times H) $$\nThis expression depends on the specific, and often unknown, values of $B$ and $H$. The core insight of DCA is to re-express this benefit in a standardized way. We use the relationship defined by the threshold probability $p_t$. At this threshold, a rational decision-maker is indifferent between treating and not treating, meaning the expected benefit equals the expected harm:\n$$ p_t B = (1 - p_t) H $$\nFrom this equality, we can derive the \"exchange rate\" between harm and benefit:\n$$ \\frac{H}{B} = \\frac{p_t}{1 - p_t} $$\nThis ratio represents the number of units of benefit ($B$) a decision-maker is willing to forgo to avoid one unit of harm ($H$).\n\nTo standardize the net benefit, it is conventional to express it in units of true positive equivalents. This is achieved by dividing the raw net benefit by $B$:\n$$ \\text{Net Benefit}_{\\text{scaled}} = \\frac{(TP \\times B) - (FP \\times H)}{B} = TP - FP \\left( \\frac{H}{B} \\right) $$\nNow, we substitute the exchange rate into this equation:\n$$ \\text{Net Benefit}_{\\text{scaled}} = TP - FP \\left( \\frac{p_t}{1 - p_t} \\right) $$\nThis expression gives the absolute net number of patients who benefit from the model's use in the cohort. To create a standardized metric that is comparable across studies of different sizes, this quantity is averaged over the entire cohort of size $N$:\n$$ \\text{Net Benefit} = \\frac{\\text{Net Benefit}_{\\text{scaled}}}{N} = \\frac{TP - FP \\left( \\frac{p_t}{1 - p_t} \\right)}{N} $$\nThis can be written as:\n$$ NB(p_t) = \\frac{TP}{N} - \\frac{FP}{N} \\left( \\frac{p_t}{1 - p_t} \\right) $$\nThis is the final expression for the net benefit metric used in DCA, as a function of $TP$, $FP$, $N$, and $p_t$. It represents the net gain in true positives per patient, with false positives being penalized according to the odds of the chosen risk threshold.\n\n### Part 2: Calculation for the Validation Cohort\n\nWe are given the following parameters for the validation cohort:\n- Total cohort size: $N = 1200$\n- Disease prevalence: $\\pi = 0.30$\n- Threshold probability: $p_t = 0.20$\n- Sensitivity at this threshold: $\\text{Sens} = 0.80$\n- Specificity at this threshold: $\\text{Spec} = 0.85$\n\nFirst, we calculate the number of diseased ($D$) and non-diseased ($ND$) individuals in the cohort.\n$$ D = N \\times \\pi = 1200 \\times 0.30 = 360 $$\n$$ ND = N \\times (1 - \\pi) = 1200 \\times (1 - 0.30) = 1200 \\times 0.70 = 840 $$\n\nNext, we calculate the number of true positives ($TP$) and false positives ($FP$) using the definitions of sensitivity and specificity.\n- The number of true positives is the number of diseased individuals who are correctly classified as positive.\n$$ TP = D \\times \\text{Sens} = 360 \\times 0.80 = 288 $$\n- The number of false positives is the number of non-diseased individuals who are incorrectly classified as positive. The rate of this error is $(1 - \\text{Specificity})$.\n$$ FP = ND \\times (1 - \\text{Spec}) = 840 \\times (1 - 0.85) = 840 \\times 0.15 = 126 $$\n\nNow, we calculate the weighting factor based on the threshold probability $p_t$:\n$$ \\frac{p_t}{1 - p_t} = \\frac{0.20}{1 - 0.20} = \\frac{0.20}{0.80} = \\frac{1}{4} $$\n\nFinally, we substitute $TP=288$, $FP=126$, $N=1200$, and the weighting factor into the derived net benefit formula:\n$$ NB(p_t=0.20) = \\frac{288}{1200} - \\frac{126}{1200} \\left( \\frac{1}{4} \\right) $$\nWe can factor out $\\frac{1}{1200}$:\n$$ NB(p_t=0.20) = \\frac{1}{1200} \\left( 288 - \\frac{126}{4} \\right) $$\n$$ NB(p_t=0.20) = \\frac{1}{1200} (288 - 31.5) $$\n$$ NB(p_t=0.20) = \\frac{256.5}{1200} $$\nTo express this as a reduced fraction, we first eliminate the decimal:\n$$ NB(p_t=0.20) = \\frac{2565}{12000} $$\nWe can divide the numerator and the denominator by their greatest common divisor.\nDivide by $5$:\n$$ \\frac{2565 \\div 5}{12000 \\div 5} = \\frac{513}{2400} $$\nThe sum of the digits of $513$ is $5+1+3=9$, so it is divisible by $9$ (and thus by $3$). The sum of the digits of $2400$ is $2+4+0+0=6$, so it is divisible by $3$. Divide by $3$:\n$$ \\frac{513 \\div 3}{2400 \\div 3} = \\frac{171}{800} $$\nThe prime factorization of $171$ is $3^2 \\times 19$. The prime factorization of $800$ is $8 \\times 100 = 2^3 \\times 10^2 = 2^3 \\times (2 \\times 5)^2 = 2^5 \\times 5^2$. There are no common prime factors. Thus, the fraction is in its simplest form.\n\nThe net benefit of using the classifier at a risk threshold of $p_t = 0.20$ is $\\frac{171}{800}$.\nAs a decimal, this is $0.21375$.\nThe problem requests a reduced fraction or a decimal to four significant figures ($0.2138$). The exact fraction is preferred.",
            "answer": "$$\n\\boxed{\\frac{171}{800}}\n$$"
        }
    ]
}