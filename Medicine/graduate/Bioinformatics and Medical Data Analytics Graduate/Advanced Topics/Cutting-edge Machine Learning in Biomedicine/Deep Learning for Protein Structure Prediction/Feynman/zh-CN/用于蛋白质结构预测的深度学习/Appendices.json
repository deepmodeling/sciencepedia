{
    "hands_on_practices": [
        {
            "introduction": "现代蛋白质结构预测模型通常将蛋白质表示为图。本练习旨在通过模拟单步“信息传递”（message passing）过程，揭示这些模型如何对蛋白质结构进行推理，即如何整合来自相邻残基的信息以更新特定残基的特征。通过这项动手计算，您将清晰地理解图神经网络（GNNs）的核心机制，例如在 AlphaFold2 等模型中使用的机制 。",
            "id": "4554878",
            "problem": "考虑一个包含四个残基的蛋白质接触图，残基索引为 $i \\in \\{1,2,3,4\\}$。残基是节点，无向边代表空间接触。每个节点 $i$ 带有描述学习到的生化属性的二维特征向量 $h_{i} \\in \\mathbb{R}^{2}$，每条边 $(i,j)$ 带有编码几何形状和序列分离的二维特征 $e_{ij} \\in \\mathbb{R}^{2}$。这种基于图的表示方法与物理原理一致，即残基-残基相互作用沿接触网络传播，并且任何学习到的更新都必须对节点索引的排列保持不变。\n\n初始节点特征为\n$$\nh_{1}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix},\\quad\nh_{2}=\\begin{pmatrix}0 \\\\ 2\\end{pmatrix},\\quad\nh_{3}=\\begin{pmatrix}1 \\\\ -1\\end{pmatrix},\\quad\nh_{4}=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}.\n$$\n边存在于点对 $(1,2)$, $(2,3)$, $(3,4)$ 和 $(1,4)$ 之间。对于每条边 $(i,j)$，定义 $e_{ij}=\\begin{pmatrix}1/d_{ij} \\\\ |i-j|\\end{pmatrix}$，其中 $d_{ij}$ 是以埃（Angstroms）为单位的残基间距离（一个对残基接触具有物理意义的尺度）。距离为\n$$\nd_{12}=4,\\quad d_{23}=5,\\quad d_{34}=4,\\quad d_{14}=8,\n$$\n因此\n$$\ne_{12}=\\begin{pmatrix}\\frac{1}{4} \\\\ 1\\end{pmatrix},\\quad\ne_{23}=\\begin{pmatrix}\\frac{1}{5} \\\\ 1\\end{pmatrix},\\quad\ne_{34}=\\begin{pmatrix}\\frac{1}{4} \\\\ 1\\end{pmatrix},\\quad\ne_{14}=\\begin{pmatrix}\\frac{1}{8} \\\\ 3\\end{pmatrix}.\n$$\n\n使用图神经网络（GNN）执行一次消息传递迭代，其基本原理是残基 $j$ 从其邻居 $i \\in \\mathcal{N}(j)$ 接收消息，每条有向消息 $m_{i\\to j}$ 由一个从发送者特征和边特征派生的门控进行调制。使用以下组件：\n- 逐元素应用的修正线性单元（ReLU）非线性函数 $\\phi(x)=\\max(0,x)$。\n- 门控得分 $s_{ij}=a^{\\top}h_{i}+c^{\\top}e_{ij}$，其中 $a=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$ 且 $c=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$，以及标量门控 $g_{ij}=\\phi(s_{ij})$。\n- 消息函数 $m_{i\\to j}=g_{ij}\\cdot \\phi\\!\\big(W_{m}h_{i}+U_{m}e_{ij}+b_{m}\\big)$，其中\n$$\nW_{m}=\\begin{pmatrix}1 & -1 \\\\ 0 & 2\\end{pmatrix},\\quad\nU_{m}=\\begin{pmatrix}2 & 0 \\\\ 1 & 1\\end{pmatrix},\\quad\nb_{m}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}.\n$$\n- 通过求和进行邻居聚合：$s_{j}=\\sum_{i\\in\\mathcal{N}(j)} m_{i\\to j}$。\n- 使用 ReLU 对节点进行残差更新：$h_{j}^{\\text{new}}=\\phi\\!\\big(W_{s}h_{j}+V s_{j}+b_{s}\\big)$，其中\n$$\nW_{s}=\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix},\\quad\nV=\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix},\\quad\nb_{s}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}.\n$$\n\n计算残基 $3$ 在这次单次消息传递迭代后更新的特征 $h_{3}^{\\text{new}}$，然后计算其欧几里得范数的平方 $\\|h_{3}^{\\text{new}}\\|_{2}^{2}$。将最终的标量答案表示为一个精确的有理数。不需要四舍五入，也不需要报告单位。",
            "solution": "用户要求计算单次消息传递迭代后更新的特征向量 $h_{3}^{\\text{new}}$ 及其欧几里得范数的平方 $\\|h_{3}^{\\text{new}}\\|_{2}^{2}$。该问题定义明确，在用于蛋白质建模的图神经网络领域具有科学依据，并为唯一解提供了所有必要的数据和定义。\n\n该过程涉及计算来自残基 $3$ 的邻居的消息，对它们进行聚合，然后使用聚合后的消息更新残基 $3$ 的特征向量。\n\n残基 $j=3$ 的邻居集合是 $\\mathcal{N}(3)=\\{2, 4\\}$，这由给定的边 $(2,3)$ 和 $(3,4)$ 确定。\n\n节点特征向量 $h_j$ 的更新规则由下式给出：\n$$\nh_{j}^{\\text{new}}=\\phi\\!\\big(W_{s}h_{j}+V s_{j}+b_{s}\\big)\n$$\n其中 $s_j$ 是来自 $j$ 的邻居的聚合消息。更新步骤的参数给定为 $W_s$ 和 $V$ 的单位矩阵，以及 $b_s$ 的零向量：\n$$\nW_{s}=\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}, \\quad V=\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}, \\quad b_{s}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\n$$\n更新规则简化为：\n$$\nh_{j}^{\\text{new}}=\\phi(h_{j} + s_{j})\n$$\n聚合消息 $s_j$ 是来自其邻居的传入消息的总和：\n$$\ns_{j}=\\sum_{i\\in\\mathcal{N}(j)} m_{i\\to j}\n$$\n对于残基 $j=3$，此为 $s_{3}=m_{2\\to 3}+m_{4\\to 3}$。\n\n消息函数 $m_{i\\to j}$ 定义为：\n$$\nm_{i\\to j}=g_{ij}\\cdot \\phi\\!\\big(W_{m}h_{i}+U_{m}e_{ij}+b_{m}\\big)\n$$\n其中门控 $g_{ij}$ 是 $g_{ij}=\\phi(s_{ij})$，得分 $s_{ij}$ 是 $s_{ij}=a^{\\top}h_{i}+c^{\\top}e_{ij}$。\n\n首先，我们计算从残基 $2$ 到残基 $3$ 的消息，记为 $m_{2\\to 3}$。\n所需的特征是 $h_{2}=\\begin{pmatrix}0 \\\\ 2\\end{pmatrix}$ 和 $e_{23}=\\begin{pmatrix}1/5 \\\\ 1\\end{pmatrix}$。门控参数是 $a=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$ 和 $c=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$。\n门控得分是：\n$$\ns_{23} = a^{\\top}h_{2}+c^{\\top}e_{23} = \\begin{pmatrix}1 & 1\\end{pmatrix}\\begin{pmatrix}0 \\\\ 2\\end{pmatrix} + \\begin{pmatrix}0 & 1\\end{pmatrix}\\begin{pmatrix}\\frac{1}{5} \\\\ 1\\end{pmatrix} = (1 \\cdot 0 + 1 \\cdot 2) + (0 \\cdot \\frac{1}{5} + 1 \\cdot 1) = 2 + 1 = 3\n$$\n门控是 $g_{23}=\\phi(s_{23})=\\phi(3)=3$。\n使用 $W_{m}=\\begin{pmatrix}1 & -1 \\\\ 0 & 2\\end{pmatrix}$、$U_{m}=\\begin{pmatrix}2 & 0 \\\\ 1 & 1\\end{pmatrix}$ 和 $b_{m}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ 计算消息函数中 ReLU 的参数：\n$$\nW_{m}h_{2} = \\begin{pmatrix}1 & -1 \\\\ 0 & 2\\end{pmatrix}\\begin{pmatrix}0 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}-2 \\\\ 4\\end{pmatrix}\n$$\n$$\nU_{m}e_{23} = \\begin{pmatrix}2 & 0 \\\\ 1 & 1\\end{pmatrix}\\begin{pmatrix}\\frac{1}{5} \\\\ 1\\end{pmatrix} = \\begin{pmatrix}\\frac{2}{5} \\\\ \\frac{1}{5}+1\\end{pmatrix} = \\begin{pmatrix}\\frac{2}{5} \\\\ \\frac{6}{5}\\end{pmatrix}\n$$\n预激活向量是：\n$$\nW_{m}h_{2}+U_{m}e_{23}+b_{m} = \\begin{pmatrix}-2 \\\\ 4\\end{pmatrix} + \\begin{pmatrix}\\frac{2}{5} \\\\ \\frac{6}{5}\\end{pmatrix} + \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}-\\frac{10}{5}+\\frac{2}{5} \\\\ \\frac{20}{5}+\\frac{6}{5}\\end{pmatrix} = \\begin{pmatrix}-\\frac{8}{5} \\\\ \\frac{26}{5}\\end{pmatrix}\n$$\n应用 ReLU 并乘以门控得到消息：\n$$\nm_{2\\to 3} = g_{23} \\cdot \\phi\\left(\\begin{pmatrix}-\\frac{8}{5} \\\\ \\frac{26}{5}\\end{pmatrix}\\right) = 3 \\cdot \\begin{pmatrix}0 \\\\ \\frac{26}{5}\\end{pmatrix} = \\begin{pmatrix}0 \\\\ \\frac{78}{5}\\end{pmatrix}\n$$\n接下来，我们计算从残基 $4$ 到残基 $3$ 的消息，记为 $m_{4\\to 3}$。\n所需的特征是 $h_{4}=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}$ 和 $e_{43}=e_{34}=\\begin{pmatrix}1/4 \\\\ 1\\end{pmatrix}$。\n门控得分是：\n$$\ns_{43} = a^{\\top}h_{4}+c^{\\top}e_{43} = \\begin{pmatrix}1 & 1\\end{pmatrix}\\begin{pmatrix}2 \\\\ 1\\end{pmatrix} + \\begin{pmatrix}0 & 1\\end{pmatrix}\\begin{pmatrix}\\frac{1}{4} \\\\ 1\\end{pmatrix} = (1 \\cdot 2 + 1 \\cdot 1) + (0 \\cdot \\frac{1}{4} + 1 \\cdot 1) = 3 + 1 = 4\n$$\n门控是 $g_{43}=\\phi(s_{43})=\\phi(4)=4$。\n预激活向量是：\n$$\nW_{m}h_{4} = \\begin{pmatrix}1 & -1 \\\\ 0 & 2\\end{pmatrix}\\begin{pmatrix}2 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}\n$$\n$$\nU_{m}e_{43} = \\begin{pmatrix}2 & 0 \\\\ 1 & 1\\end{pmatrix}\\begin{pmatrix}\\frac{1}{4} \\\\ 1\\end{pmatrix} = \\begin{pmatrix}\\frac{2}{4} \\\\ \\frac{1}{4}+1\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{5}{4}\\end{pmatrix}\n$$\n$$\nW_{m}h_{4}+U_{m}e_{43}+b_{m} = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix} + \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{5}{4}\\end{pmatrix} + \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}\\frac{3}{2} \\\\ \\frac{8}{4}+\\frac{5}{4}\\end{pmatrix} = \\begin{pmatrix}\\frac{3}{2} \\\\ \\frac{13}{4}\\end{pmatrix}\n$$\n应用 ReLU 并乘以门控得到消息。由于两个分量都为正，ReLU 不起作用。\n$$\nm_{4\\to 3} = g_{43} \\cdot \\phi\\left(\\begin{pmatrix}\\frac{3}{2} \\\\ \\frac{13}{4}\\end{pmatrix}\\right) = 4 \\cdot \\begin{pmatrix}\\frac{3}{2} \\\\ \\frac{13}{4}\\end{pmatrix} = \\begin{pmatrix}6 \\\\ 13\\end{pmatrix}\n$$\n现在，我们聚合消息以获得 $s_3$：\n$$\ns_{3} = m_{2\\to 3} + m_{4\\to 3} = \\begin{pmatrix}0 \\\\ \\frac{78}{5}\\end{pmatrix} + \\begin{pmatrix}6 \\\\ 13\\end{pmatrix} = \\begin{pmatrix}6 \\\\ \\frac{78}{5} + \\frac{65}{5}\\end{pmatrix} = \\begin{pmatrix}6 \\\\ \\frac{143}{5}\\end{pmatrix}\n$$\n我们使用这个聚合消息来更新残基 $3$ 的特征向量，使用其初始状态 $h_{3}=\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$。\n$$\nh_{3}^{\\text{new}} = \\phi(h_{3} + s_{3}) = \\phi\\left(\\begin{pmatrix}1 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}6 \\\\ \\frac{143}{5}\\end{pmatrix}\\right) = \\phi\\left(\\begin{pmatrix}7 \\\\ -\\frac{5}{5} + \\frac{143}{5}\\end{pmatrix}\\right) = \\phi\\left(\\begin{pmatrix}7 \\\\ \\frac{138}{5}\\end{pmatrix}\\right)\n$$\n由于参数的两个分量都为正，ReLU 非线性函数不起作用。\n$$\nh_{3}^{\\text{new}} = \\begin{pmatrix}7 \\\\ \\frac{138}{5}\\end{pmatrix}\n$$\n最后，我们计算 $h_{3}^{\\text{new}}$ 的欧几里得范数的平方：\n$$\n\\|h_{3}^{\\text{new}}\\|_{2}^{2} = (7)^2 + \\left(\\frac{138}{5}\\right)^2 = 49 + \\frac{138^2}{5^2} = 49 + \\frac{19044}{25}\n$$\n为了将其表示为单个有理数，我们找到一个公分母：\n$$\n\\|h_{3}^{\\text{new}}\\|_{2}^{2} = \\frac{49 \\times 25}{25} + \\frac{19044}{25} = \\frac{1225}{25} + \\frac{19044}{25} = \\frac{1225 + 19044}{25} = \\frac{20269}{25}\n$$\n这个分数是不可约分的，因为分子不以 $0$ 或 $5$ 结尾，因此不能被 $5$ 整除。\n最终答案是 $\\frac{20269}{25}$。",
            "answer": "$$\n\\boxed{\\frac{20269}{25}}\n$$"
        },
        {
            "introduction": "深度学习模型通过最小化一个衡量其预测与真实值之间差异的损失函数来进行学习。本实践将深入探讨如何构建一个复杂的多分量损失函数，它融合了几何精度（FAPE）、距离分布概率（distograms）和物理真实性。通过实现这个复合损失函数，您将理解顶尖模型如何被训练以同时满足多种结构标准 。",
            "id": "3842267",
            "problem": "您的任務是為基於深度學習的蛋白質結構預測構建一個複合訓練損失，該損失混合了四個分量：距離圖交叉熵、框架對齊點誤差 (FAPE)、扭轉角損失和違規懲罰。目標是推導、實現並論證一個維度一致且能平衡異構項貢獻的有原則的加權方案。所有角度必須以弧度處理，所有距離以埃（寫作 $\\mathrm{\\AA}$）為單位，所有對數必須是自然對數。您必須產出一個完整的、可運行的程序，該程序為多個測試案例計算複合損失，並將結果作為單行列表輸出。\n\n從以下基礎和核心定義開始：\n\n- 概率公理與交叉熵的定義：對於離散分佈 $p(k)$ 和在區間 $k^\\star$ 上的 one-hot 目標，交叉熵為 $-\\log p(k^\\star)$。\n- 由一個旋轉矩陣 $\\mathbf{R} \\in \\mathbb{R}^{3 \\times 3}$ 和一個平移向量 $\\mathbf{t} \\in \\mathbb{R}^{3}$ 定義的三維剛體框架，以及歐幾里得範數 $\\|\\cdot\\|_2$。\n- 圓上週期性角度差的定義，使用 $2\\pi$ 週期性環繞來處理模 $2\\pi$ 的角度等價性。\n- 由閾值距離和對低於該閾值的距離的校正懲罰定義的空間位阻衝突違規懲罰。\n\n您的程序必須為每個測試案例實現以下四個損失分量：\n\n1. 距離圖交叉熵。距離圖是成對殘基間距離的離散分佈。給定 $K$ 個區間邊界 $\\{b_0, b_1, \\dots, b_K\\}$ 且 $b_0  b_1  \\dots  b_K$，以及一個預測的概率向量 $\\mathbf{p} \\in \\mathbb{R}^K$ 且 $\\sum_{k=0}^{K-1} p_k = 1$，通過 $k^\\star = \\max\\{k \\in \\{0,\\dots,K-1\\}: b_k \\le d\\}$ 將真實距離 $d$ 分配給區間索引 $k^\\star$，對於 $d \\ge b_K$ 的情況進行上界截斷。每對的交叉熵為 $L_{\\mathrm{dist}} = -\\log p_{k^\\star}$，距離圖損失是這些值在所有對上的平均值。\n\n2. 框架對齊點誤差 (FAPE)。對於一組由 $i$ 索引的點，每個點都有預測的全局座標 $\\mathbf{x}_i^{(p)} \\in \\mathbb{R}^3$、真實的全局座標 $\\mathbf{x}_i^{(t)} \\in \\mathbb{R}^3$、一個預測的框架 $(\\mathbf{R}^{(p)}, \\mathbf{t}^{(p)})$ 和一個真實的框架 $(\\mathbf{R}^{(t)}, \\mathbf{t}^{(t)})$，通過 $\\mathbf{y}_i^{(p)} = \\mathbf{R}^{(p)\\top}(\\mathbf{x}_i^{(p)} - \\mathbf{t}^{(p)})$ 和 $\\mathbf{y}_i^{(t)} = \\mathbf{R}^{(t)\\top}(\\mathbf{x}_i^{(t)} - \\mathbf{t}^{(t)})$ 將點轉換到各自的局部框架中。每个點的 FAPE 為 $e_i = \\min\\{\\tau, \\|\\mathbf{y}_i^{(p)} - \\mathbf{y}_i^{(t)}\\|_2\\}$，其中截斷值 $\\tau > 0$ 的單位是 $\\mathrm{\\AA}$。FAPE 損失是所有點的平均值 $\\overline{e}$。使用繞 $z$ 軸旋轉角度 $\\phi$ 的旋轉，即 $\\mathbf{R}_z(\\phi) = \\begin{bmatrix} \\cos\\phi  -\\sin\\phi  0 \\\\ \\sin\\phi  \\cos\\phi  0 \\\\ 0  0  1 \\end{bmatrix}$。\n\n3. 扭轉角損失。對於預測的扭轉角 $\\theta_j^{(p)}$ 和真實的扭轉角 $\\theta_j^{(t)}$（均以弧度為單位），計算環繞差 $\\Delta\\theta_j = \\mathrm{wrap}(\\theta_j^{(p)} - \\theta_j^{(t)})$，其中 $\\mathrm{wrap}(\\alpha) = ((\\alpha + \\pi) \\bmod 2\\pi) - \\pi$。每個角度的損失是 $1 - \\cos(\\Delta\\theta_j)$，扭轉角損失是所有角度的平均值。\n\n4. 違規懲罰。對於預測的非鍵配對距離 $d_{ij} \\in \\mathbb{R}_{\\ge 0}$ 和一個閾值 $r_{\\mathrm{th}} > 0$（單位為 $\\mathrm{\\AA}$），定義每對的懲罰為 $v_{ij} = \\max\\{0, r_{\\mathrm{th}} - d_{ij}\\}^2$。對所有唯一的無序對 $(i,j)$（其中 $i  j$）上的懲罰求平均值，得到 $L_{\\mathrm{viol}}$。\n\n您的實現必須將這四個分量組合成一個單一的複合損失 $L_{\\mathrm{comp}}$。使用一個特徵長度尺度 $\\ell = 10\\,\\mathrm{\\AA}$ 來無量綱化 FAPE 和違規損失：$L_2 = \\overline{e}/\\ell$ 和 $L_4 = L_{\\mathrm{viol}}/\\ell^2$。然後，使用自適應加權方案 $w_i \\propto 1/(\\varepsilon + L_i)$ 來組合四個無量綱分量 $\\{L_1, L_2, L_3, L_4\\}$，其中 $\\varepsilon = 10^{-8}$ 是一個小的穩定性常數。\n\n為以下三個測試案例計算複合損失，並將結果作為一個包含三個浮點數的單行列表返回，例如 `[loss1,loss2,loss3]`。\n\n**案例 1**: 典型情況\n- 距離圖: 區間邊界 $[0, 4, 8, 12, 16]$，真實距離 $[5.2, 8.7, 12.0]$，預測概率 `[[0.05, 0.8, 0.1, 0.05], [0.1, 0.1, 0.7, 0.1], [0.05, 0.15, 0.2, 0.6]]`\n- FAPE: $\\phi_p=0.1, \\mathbf{t}_p=[0,0,0], \\phi_t=0.2, \\mathbf{t}_t=[0.1,-0.1,0], \\mathbf{x}_p=[[1,0,0],[0,1,0]], \\mathbf{x}_t=[[1.1,-0.1,0],[0.1,0.9,0]]$\n- 扭轉角: $\\theta_p=[-2.9, 1.5, 3.1], \\theta_t=[-3.0, 1.4, -3.05]$\n- 違規: 距離矩陣 `[[0, 3.8, 2.9], [3.8, 0, 5.2], [2.9, 5.2, 0]]`, $r_{\\mathrm{th}}=3.2$\n\n**案例 2**: 邊界條件，包括角度環繞和 FAPE 截斷\n- 距離圖: 區間邊界 $[0, 4, 8, 12, 16]$，真實距離 $[15.9, 0.5, 20.0]$，預測概率 `[[0.25,0.25,0.25,0.25],[0.9,0.05,0.03,0.02],[0.05,0.05,0.05,0.85]]`\n- FAPE: $\\phi_p=0, \\mathbf{t}_p=[0,0,0], \\phi_t=0, \\mathbf{t}_t=[20,0,0], \\mathbf{x}_p=[[0,0,0],[1,0,0]], \\mathbf{x}_t=[[20,0,0],[21,0,0]]$\n- 扭轉角: $\\theta_p=[-\\pi+0.01, \\pi-0.02, 0], \\theta_t=[\\pi-0.01, -\\pi+0.02, 0]$\n- 違規: 距離矩陣 `[[0, 3.2, 3.2], [3.2, 0, 3.2], [3.2, 3.2, 0]]`, $r_{\\mathrm{th}}=3.2$\n\n**案例 3**: 均勻距離圖，無 FAPE/扭轉角誤差，嚴重碰撞\n- 距離圖: 區間邊界 $[0, 4, 8, 12, 16]$，真實距離 $[7.0, 7.0, 7.0]$，預測概率 `[[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25],[0.25,0.25,0.25,0.25]]`\n- FAPE: $\\phi_p=0.5, \\mathbf{t}_p=[1,-1,0], \\phi_t=0.5, \\mathbf{t}_t=[1,-1,0], \\mathbf{x}_p=[[2,0,0],[0,2,0]], \\mathbf{x}_t=[[2,0,0],[0,2,0]]$\n- 扭轉角: $\\theta_p=[0.5, -1.0, 2.0], \\theta_t=[0.5, -1.0, 2.0]$\n- 違規: 距離矩陣 `[[0, 2.0, 2.0], [2.0, 0, 2.0], [2.0, 2.0, 0]]`, $r_{\\mathrm{th}}=3.5$",
            "solution": "問題要求為蛋白質結構預測構建並實現一個複合損失函數，該函數包含四個不同的分量：距離圖交叉熵損失、框架對齊點誤差 (FAPE)、扭轉角損失和空間位阻違規懲罰。解決方案涉及仔細實現指定好的每个分量，通過有原則的無量綱化方案確保維度一致性，最後使用自適應加權機制將它們組合起來。\n\n首先，我們定義並實現四個損失分量中的每一個。所有距離單位均為埃（$\\mathrm{\\AA}$），角度單位為弧度，對數為自然對數。\n\n1.  **距離圖交叉熵 ($L_{\\mathrm{dist}}$)**：此分量衡量預測的殘基間距離概率分佈的誤差。距離圖是距離區間集合上的一個離散概率分佈。給定由 $K+1$ 個邊界 $\\{b_0, b_1, \\dots, b_K\\}$ 定義的 $K$ 個區間，一個真實距離 $d$ 會被分配到一個特定的區間 $k^\\star$。問題將區間索引定義為 $k^\\star = \\max\\{k \\in \\{0, \\dots, K-1\\} : b_k \\le d\\}$。該規則能正確處理所有情況，包括將大於等於 $b_K$ 的距離 $d$ 截斷到最後一個區間 $k^\\star = K-1$，因為對於這樣的 $d$，所有 $k \\le K-1$ 都滿足 $b_k \\le d$。對於一個預測的概率向量 $\\mathbf{p} = (p_0, \\dots, p_{K-1})$，單個蛋白質殘基對的交叉熵損失由真實區間的負對數概率給出：\n    $$L_{\\mathrm{dist}}^{\\mathrm{(pair)}} = -\\log p_{k^\\star}$$\n    該損失本身是無量綱的。總距離圖損失 $L_{\\mathrm{dist}}$ 是這些單獨配對損失在蛋白質中所有相關殘基對上的算術平均值。\n\n2.  **框架對齊點誤差 (FAPE)**：FAPE 通過測量原子在局部參考系中與其真實座標的偏差，來量化預測的3D原子座標的誤差。對於每個殘基，都定義了一個由旋轉矩陣 $\\mathbf{R} \\in \\mathbb{R}^{3 \\times 3}$ 和平移向量 $\\mathbf{t} \\in \\mathbb{R}^{3}$ 組成的剛體框架。給定一組由 $i$ 索引的原子，設其預測和真實的全局座標為 $\\mathbf{x}_i^{(p)}$ 和 $\\mathbf{x}_i^{(t)}$，對應的預測和真實局部框架為 $(\\mathbf{R}^{(p)}, \\mathbf{t}^{(p)})$ 和 $(\\mathbf{R}^{(t)}, \\mathbf{t}^{(t)})$。首先將座標轉換到它們各自的局部框架中：\n    $$ \\mathbf{y}_i^{(p)} = \\mathbf{R}^{(p)\\top}(\\mathbf{x}_i^{(p)} - \\mathbf{t}^{(p)}) $$\n    $$ \\mathbf{y}_i^{(t)} = \\mathbf{R}^{(t)\\top}(\\mathbf{x}_i^{(t)} - \\mathbf{t}^{(t)}) $$\n    點 $i$ 的誤差是這些局部座標之間的歐幾里得距離，並在最大值 $\\tau$ 處進行截斷，以防止單個大誤差產生過大的梯度。\n    $$ e_i = \\min\\left\\{\\tau, \\|\\mathbf{y}_i^{(p)} - \\mathbf{y}_i^{(t)}\\|_2\\right\\} $$\n    總 FAPE 損失，記作 $\\overline{e}$，是這些誤差 $e_i$ 在所有考慮的點上的平均值。該損失具有距離單位（$\\mathrm{\\AA}$）。旋轉矩陣由給定的繞 $z$ 軸的旋轉角 $\\phi$ 生成，即 $\\mathbf{R}_z(\\phi)$。\n\n3.  **扭轉角損失 ($L_{\\mathrm{tors}}$)**：該損失衡量預測的骨架和側鏈扭轉角與其真實值之間的偏差。由於角度是週期性的，簡單的差值是不夠的。我們計算每個角度 $j$ 在圓上的週期性差 $\\Delta\\theta_j$，即預測角 $\\theta_j^{(p)}$ 和真實角 $\\theta_j^{(t)}$ 之間的差：\n    $$ \\Delta\\theta_j = \\mathrm{wrap}(\\theta_j^{(p)} - \\theta_j^{(t)}) $$\n    其中 wrap 函數通過 $\\mathrm{wrap}(\\alpha) = ((\\alpha + \\pi) \\bmod 2\\pi) - \\pi$ 將其參數映射到區間 $[-\\pi, \\pi]$。一個合適的損失函數是 $1 - \\cos(\\Delta\\theta_j)$，它在 $\\Delta\\theta_j=0$ 時為最小值，在 $\\Delta\\theta_j=\\pm\\pi$ 時為最大值，並且是平滑的。總扭轉角損失 $L_{\\mathrm{tors}}$ 是這些值在所有角度 $j$ 上的平均值：\n    $$ L_{\\mathrm{tors}} = \\frac{1}{J} \\sum_{j=1}^{J} \\left(1 - \\cos(\\Delta\\theta_j)\\right) $$\n    這個量是無量綱的。\n\n4.  **違規懲罰 ($L_{\\mathrm{viol}}$)**：此項懲罰物理上不切實際的結構，特別是非鍵合原子之間距離過近的空間位阻衝突。對於每對非鍵合原子 $(i, j)$，如果它們的預測距離 $d_{ij}$ 小於指定的閾值 $r_{\\mathrm{th}}$，則會產生懲罰。懲罰被定義為一個校正後的二次函數：\n    $$ v_{ij} = \\max\\{0, r_{\\mathrm{th}} - d_{ij}\\}^2 $$\n    這會對更小的距離施加更嚴厲的懲罰，並提供平滑的梯度。總違規損失 $L_{\\mathrm{viol}}$ 是這些懲罰在所有唯一的非鍵合對 $(i, j)$（其中 $i  j$）上的平均值。該損失的單位是距離的平方（$\\mathrm{\\AA}^2$）。\n\n為了將這些異構的損失分量組合成單一的標量值，我們首先確保它們在維度上是一致的。FAPE 損失 $\\overline{e}$ 和違規損失 $L_{\\mathrm{viol}}$ 分別通過除以一個特徵長度尺度 $\\ell = 10\\,\\mathrm{\\AA}$ 及其平方來進行無量綱化。四個無量綱的損失分量是：\n$$ L_1 = L_{\\mathrm{dist}} \\quad (\\text{無量綱}) $$\n$$ L_2 = \\overline{e} / \\ell \\quad (\\text{無量綱}) $$\n$$ L_3 = L_{\\mathrm{tors}} \\quad (\\text{無量綱}) $$\n$$ L_4 = L_{\\mathrm{viol}} / \\ell^2 \\quad (\\text{無量綱}) $$\n接著採用一種自適應加權方案來平衡它們的貢獻。權重 $w_i$ 被設計成與其對應的損失分量 $L_i$ 的大小成反比，以確保損失值較小（表示該方面的預測較好）的分量不會被忽略。權重計算如下：\n$$ \\tilde{w}_i = \\frac{1}{\\varepsilon + L_i}, \\quad w_i = \\frac{\\tilde{w}_i}{\\sum_{j=1}^{4} \\tilde{w}_j} $$\n在這裡，$L_i$ 代表當前數據樣本中相應分量的平均值，$\\varepsilon = 10^{-8}$ 是一個小常數，用於確保當某個損失分量為零或非常接近零時的數值穩定性。最終的複合損失 $L_{\\mathrm{comp}}$ 是無量綱分量的加權和：\n$$ L_{\\mathrm{comp}} = \\sum_{i=1}^{4} w_i L_i $$\n這種形式為訓練蛋白質結構預測模型創建了一個平衡且維度合理的目標函數。所提供的程序為三個不同的測試案例實現了這些計算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a composite loss for protein structure prediction based on the problem statement.\n    The composite loss is a weighted sum of four components: distogram cross-entropy,\n    Frame Aligned Point Error (FAPE), torsion angle loss, and a violation penalty.\n    \"\"\"\n\n    # --- Constants specified in the problem ---\n    L_SCALE = 10.0  # Characteristic length scale l in Angstroms\n    TAU = 10.0      # FAPE clamp value in Angstroms\n    EPSILON = 1e-8  # Stability constant for weighting\n\n    # --- Test Cases ---\n    test_cases = [\n        {\n            # Case 1 (typical)\n            \"distogram\": {\n                \"bin_edges\": np.array([0, 4, 8, 12, 16], dtype=float),\n                \"true_dists\": np.array([5.2, 8.7, 12.0]),\n                \"pred_probs\": np.array([\n                    [0.05, 0.80, 0.10, 0.05],\n                    [0.10, 0.10, 0.70, 0.10],\n                    [0.05, 0.15, 0.20, 0.60]\n                ])\n            },\n            \"fape\": {\n                \"phi_p\": 0.1, \"t_p\": np.array([0.0, 0.0, 0.0]),\n                \"phi_t\": 0.2, \"t_t\": np.array([0.1, -0.1, 0.0]),\n                \"x_p\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n                \"x_t\": np.array([[1.1, -0.1, 0.0], [0.1, 0.9, 0.0]])\n            },\n            \"torsion\": {\n                \"theta_p\": np.array([-2.9, 1.5, 3.1]),\n                \"theta_t\": np.array([-3.0, 1.4, -3.05])\n            },\n            \"violation\": {\n                \"dist_matrix\": np.array([[0, 3.8, 2.9], [3.8, 0, 5.2], [2.9, 5.2, 0]]),\n                \"r_th\": 3.2\n            }\n        },\n        {\n            # Case 2 (boundary wrapping and clamp)\n            \"distogram\": {\n                \"bin_edges\": np.array([0, 4, 8, 12, 16], dtype=float),\n                \"true_dists\": np.array([15.9, 0.5, 20.0]),\n                \"pred_probs\": np.array([\n                    [0.25, 0.25, 0.25, 0.25],\n                    [0.90, 0.05, 0.03, 0.02],\n                    [0.05, 0.05, 0.05, 0.85]\n                ])\n            },\n            \"fape\": {\n                \"phi_p\": 0.0, \"t_p\": np.array([0.0, 0.0, 0.0]),\n                \"phi_t\": 0.0, \"t_t\": np.array([20.0, 0.0, 0.0]),\n                \"x_p\": np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0]]),\n                \"x_t\": np.array([[20.0, 0.0, 0.0], [21.0, 0.0, 0.0]])\n            },\n            \"torsion\": {\n                \"theta_p\": np.array([-np.pi + 0.01, np.pi - 0.02, 0.0]),\n                \"theta_t\": np.array([np.pi - 0.01, -np.pi + 0.02, 0.0])\n            },\n            \"violation\": {\n                \"dist_matrix\": np.array([[0, 3.2, 3.2], [3.2, 0, 3.2], [3.2, 3.2, 0]]),\n                \"r_th\": 3.2\n            }\n        },\n        {\n            # Case 3 (uniform distogram, no FAPE/torsion error, severe clashes)\n            \"distogram\": {\n                \"bin_edges\": np.array([0, 4, 8, 12, 16], dtype=float),\n                \"true_dists\": np.array([7.0, 7.0, 7.0]),\n                \"pred_probs\": np.array([\n                    [0.25, 0.25, 0.25, 0.25],\n                    [0.25, 0.25, 0.25, 0.25],\n                    [0.25, 0.25, 0.25, 0.25]\n                ])\n            },\n            \"fape\": {\n                \"phi_p\": 0.5, \"t_p\": np.array([1.0, -1.0, 0.0]),\n                \"phi_t\": 0.5, \"t_t\": np.array([1.0, -1.0, 0.0]),\n                \"x_p\": np.array([[2.0, 0.0, 0.0], [0.0, 2.0, 0.0]]),\n                \"x_t\": np.array([[2.0, 0.0, 0.0], [0.0, 2.0, 0.0]])\n            },\n            \"torsion\": {\n                \"theta_p\": np.array([0.5, -1.0, 2.0]),\n                \"theta_t\": np.array([0.5, -1.0, 2.0])\n            },\n            \"violation\": {\n                \"dist_matrix\": np.array([[0, 2.0, 2.0], [2.0, 0, 2.0], [2.0, 2.0, 0]]),\n                \"r_th\": 3.5\n            }\n        }\n    ]\n\n    results = []\n    \n    # --- Helper functions for loss components ---\n    \n    def _calculate_distogram_loss(bin_edges, true_dists, pred_probs):\n        losses = []\n        num_bins = len(pred_probs[0])\n        bin_indices = bin_edges[:num_bins]\n        for d, p in zip(true_dists, pred_probs):\n            # Find bin index k_star = max{k in {0..K-1}: b_k = d}\n            valid_indices = np.where(bin_indices = d)[0]\n            k_star = np.max(valid_indices)\n            loss = -np.log(p[k_star])\n            losses.append(loss)\n        return np.mean(losses) if losses else 0.0\n\n    def _calculate_fape_loss(phi_p, t_p, phi_t, t_t, x_p, x_t):\n        # Rotation matrix for rotation about z-axis\n        def get_rot_mat(phi):\n            c_phi, s_phi = np.cos(phi), np.sin(phi)\n            return np.array([\n                [c_phi, -s_phi, 0],\n                [s_phi,  c_phi, 0],\n                [0,          0, 1]\n            ])\n\n        R_p = get_rot_mat(phi_p)\n        R_t = get_rot_mat(phi_t)\n\n        errors = []\n        for i in range(len(x_p)):\n            y_p = R_p.T @ (x_p[i] - t_p)\n            y_t = R_t.T @ (x_t[i] - t_t)\n            dist = np.linalg.norm(y_p - y_t)\n            error = min(TAU, dist)\n            errors.append(error)\n        return np.mean(errors) if errors else 0.0\n\n    def _calculate_torsion_loss(theta_p, theta_t):\n        diff = theta_p - theta_t\n        # Wrap difference to [-pi, pi]\n        wrapped_diff = (diff + np.pi) % (2 * np.pi) - np.pi\n        losses = 1 - np.cos(wrapped_diff)\n        return np.mean(losses) if len(losses) > 0 else 0.0\n\n    def _calculate_violation_loss(dist_matrix, r_th):\n        penalties = []\n        num_atoms = dist_matrix.shape[0]\n        for i in range(num_atoms):\n            for j in range(i + 1, num_atoms):\n                d_ij = dist_matrix[i, j]\n                penalty = max(0, r_th - d_ij)**2\n                penalties.append(penalty)\n        return np.mean(penalties) if penalties else 0.0\n\n    # --- Main computation loop ---\n    for case in test_cases:\n        # Calculate individual loss components\n        l_dist = _calculate_distogram_loss(**case[\"distogram\"])\n        e_bar = _calculate_fape_loss(**case[\"fape\"])\n        l_tors = _calculate_torsion_loss(**case[\"torsion\"])\n        l_viol = _calculate_violation_loss(**case[\"violation\"])\n        \n        # Nondimensionalize\n        L1 = l_dist\n        L2 = e_bar / L_SCALE\n        L3 = l_tors\n        L4 = l_viol / (L_SCALE**2)\n        \n        losses = np.array([L1, L2, L3, L4])\n        \n        # Calculate adaptive weights\n        unnorm_weights = 1.0 / (EPSILON + losses)\n        sum_unnorm_weights = np.sum(unnorm_weights)\n        weights = unnorm_weights / sum_unnorm_weights\n        \n        # Calculate composite loss\n        composite_loss = np.sum(weights * losses)\n        results.append(composite_loss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个模型的真正性能体现在其对全新、未见过数据的泛化能力上。本练习旨在解决生物信息学中的一个关键挑战：通过实施严格的、基于时间的训练/验证数据划分来防止信息泄露。这项实践对于开发稳健的模型和获得可信的性能评估至关重要，是该领域优秀科研实践的基石 。",
            "id": "4554918",
            "problem": "您正在使用蛋白质数据库 (Protein Data Bank, PDB) 评估深度学习模型在蛋白质结构预测中的时间泛化能力。为了防止未来结构的信息泄漏，评估必须遵守发布时间的因果关系。从以下基本原则出发：模型的泛化能力必须在训练时无法获得的数据上进行评估，这意味着需要时间排序。形式上，对于一个由元组 $\\left(x_i, t_i, c_i\\right)$ 构成的条目数据集（由 $i$ 索引），其中 $x_i$ 是目标标识符，$t_i$ 是其发布日期，$c_i$ 是指示近似序列相似性分组的簇标签，一个围绕截止时间 $\\tau$ 和非负缓冲区 $b$（以天为单位）的有效时间划分必须满足：\n- 训练集仅使用满足 $t_i \\le \\tau - b$ 的条目。\n- 评估集仅使用满足 $t_i  \\tau$ 的条目。\n- 为避免同源体泄漏，排除评估集中任何其簇标签 $c_i$ 出现在训练簇中的条目。\n\n定义以下集合和计数：\n- 训练索引集 $I_{\\mathrm{train}}(\\tau, b) = \\{ i \\mid t_i \\le \\tau - b \\}$。\n- 候选评估集 $I_{\\mathrm{post}}(\\tau) = \\{ i \\mid t_i  \\tau \\}$。\n- 训练簇集 $C_{\\mathrm{train}}(\\tau, b) = \\{ c_i \\mid i \\in I_{\\mathrm{train}}(\\tau, b) \\}$。\n- 纯净评估集 $I_{\\mathrm{clean}}(\\tau, b) = \\{ i \\in I_{\\mathrm{post}}(\\tau) \\mid c_i \\notin C_{\\mathrm{train}}(\\tau, b) \\}$。\n- 朴素评估计数 $N_{\\mathrm{naive}}(\\tau, b) = \\left| I_{\\mathrm{post}}(\\tau) \\right|$。\n- 纯净评估计数 $N_{\\mathrm{clean}}(\\tau, b) = \\left| I_{\\mathrm{clean}}(\\tau, b) \\right|$。\n- 泄漏计数 $N_{\\mathrm{leak}}(\\tau, b) = N_{\\mathrm{naive}}(\\tau, b) - N_{\\mathrm{clean}}(\\tau, b)$。\n\n实现一个完整的程序，为固定的 PDB 条目数据集计算这些计数。每个条目是一个元组，包含一个 PDB 标识符字符串、一个表示发布日期的 ISO 日期字符串 \"YYYY-MM-DD\" 和一个簇标识符字符串。数据集如下：\n- (\"1A01\", \"2016-12-15\", \"C1\")\n- (\"2B02\", \"2017-06-10\", \"C2\")\n- (\"13M13\", \"2017-08-01\", \"C2\")\n- (\"3C03\", \"2017-12-31\", \"C3\")\n- (\"4D04\", \"2018-01-01\", \"C1\")\n- (\"14N14\", \"2018-01-31\", \"C11\")\n- (\"5E05\", \"2018-03-01\", \"C4\")\n- (\"6F06\", \"2018-05-01\", \"C5\")\n- (\"7G07\", \"2018-07-01\", \"C2\")\n- (\"8H08\", \"2019-01-15\", \"C6\")\n- (\"9I09\", \"2020-06-01\", \"C7\")\n- (\"10J10\", \"2021-01-01\", \"C8\")\n- (\"11K11\", \"2021-03-02\", \"C9\")\n- (\"12L12\", \"2022-01-01\", \"C10\")\n- (\"15O15\", \"2019-12-31\", \"C1\")\n\n您的程序必须根据上述定义，为每个指定的 $(\\tau, b)$ 对计算三元组 $\\left[N_{\\mathrm{naive}}(\\tau, b), N_{\\mathrm{clean}}(\\tau, b), N_{\\mathrm{leak}}(\\tau, b)\\right]$。日期应解释为日历日，$b$ 以天为单位。所有时间都是不含时区的朴素日期。最终答案中除了天之外不需要其他物理单位，因为只要求整数计数。\n\n使用以下 $(\\tau, b)$ 对的测试套件：\n- $\\tau =$ \"2018-01-01\", $b = 0$。\n- $\\tau =$ \"2018-01-01\", $b = 60$。\n- $\\tau =$ \"2019-01-01\", $b = 30$。\n- $\\tau =$ \"2016-12-15\", $b = 0$。\n- $\\tau =$ \"2022-01-01\", $b = 0$。\n\n您的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个由方括号括起来的三元组的逗号分隔列表，不含空格，例如： \"[[a,b,c],[d,e,f],...]\"。输出元素必须是整数。结果的顺序必须与上述测试用例的顺序相匹配。",
            "solution": "该问题要求实现一种时间划分方法，以分割蛋白质结构数据集，用于评估机器学习模型。这是生物信息学中的一个关键步骤，旨在确保模型在测试集上的表现能够代表其泛化到新的、未见过数据的能力，而不是识别或插值训练集中存在的结构或进化相关的样本的能力。评估基于蛋白质数据库 (PDB) 的发布日期，确保了因果顺序。\n\n该方法通过一组基于截止日期 $\\tau$ 和时间缓冲区 $b$ 的定义进行形式化。缓冲区确保训练集和评估集之间有最小的时间间隔，进一步减少了信息泄漏的可能性。设数据集中的每个条目为一个元组 $(x_i, t_i, c_i)$，其中 $i$ 是索引，$x_i$ 是目标标识符，$t_i$ 是其发布日期，$c_i$ 是代表序列相似性（进化相关性或同源性的代理）的簇标签。对于日期比较，所有日期字符串都解释为日历日期。\n\n核心定义如下：\n1.  训练集 $I_{\\mathrm{train}}(\\tau, b)$ 包含所有发布日期在指定截止时间减去缓冲期之前（含）的条目。训练截止日期为 $\\tau - b$。\n    $$I_{\\mathrm{train}}(\\tau, b) = \\{ i \\mid t_i \\le \\tau - b \\}$$\n2.  收集训练集中存在的簇集合 $C_{\\mathrm{train}}(\\tau, b)$，以识别潜在的同源体。\n    $$C_{\\mathrm{train}}(\\tau, b) = \\{ c_i \\mid i \\in I_{\\mathrm{train}}(\\tau, b) \\}$$\n3.  候选评估集 $I_{\\mathrm{post}}(\\tau)$ 由所有在截止时间 $\\tau$ 之后发布的条目组成。\n    $$I_{\\mathrm{post}}(\\tau) = \\{ i \\mid t_i  \\tau \\}$$\n4.  “纯净”评估集 $I_{\\mathrm{clean}}(\\tau, b)$ 是候选评估集的一个子集，其中已移除了任何属于训练集中已出现簇的条目。这明确地过滤掉了同源结构。\n    $$I_{\\mathrm{clean}}(\\tau, b) = \\{ i \\in I_{\\mathrm{post}}(\\tau) \\mid c_i \\notin C_{\\mathrm{train}}(\\tau, b) \\}$$\n5.  根据这些集合，我们定义要计算的计数：\n    -   $N_{\\mathrm{naive}}(\\tau, b) = \\left| I_{\\mathrm{post}}(\\tau) \\right|$：候选评估条目的总数。\n    -   $N_{\\mathrm{clean}}(\\tau, b) = \\left| I_{\\mathrm{clean}}(\\tau, b) \\right|$：移除同源体后的评估条目数。\n    -   $N_{\\mathrm{leak}}(\\tau, b) = N_{\\mathrm{naive}}(\\tau, b) - N_{\\mathrm{clean}}(\\tau, b)$：被移除的条目数，代表“泄漏”的集合。\n\n现在，我们将对每个测试用例，将这些定义应用于所提供的数据集。\n\n数据集包含 $15$ 个条目：\n- $d_0$: (\"1A01\", \"2016-12-15\", \"C1\")\n- $d_1$: (\"2B02\", \"2017-06-10\", \"C2\")\n- $d_2$: (\"13M13\", \"2017-08-01\", \"C2\")\n- $d_3$: (\"3C03\", \"2017-12-31\", \"C3\")\n- $d_4$: (\"4D04\", \"2018-01-01\", \"C1\")\n- $d_5$: (\"14N14\", \"2018-01-31\", \"C11\")\n- $d_6$: (\"5E05\", \"2018-03-01\", \"C4\")\n- $d_7$: (\"6F06\", \"2018-05-01\", \"C5\")\n- $d_8$: (\"7G07\", \"2018-07-01\", \"C2\")\n- $d_9$: (\"8H08\", \"2019-01-15\", \"C6\")\n- $d_{10}$: (\"9I09\", \"2020-06-01\", \"C7\")\n- $d_{11}$: (\"10J10\", \"2021-01-01\", \"C8\")\n- $d_{12}$: (\"11K11\", \"2021-03-02\", \"C9\")\n- $d_{13}$: (\"12L12\", \"2022-01-01\", \"C10\")\n- $d_{14}$: (\"15O15\", \"2019-12-31\", \"C1\")\n\n**情况 1：$\\tau = \\text{\"2018-01-01\"}$，$b = 0$ 天**\n- 训练截止日期：$t_{\\mathrm{cutoff}} = \\text{\"2018-01-01\"} - 0 \\text{ 天} = \\text{\"2018-01-01\"}$。\n- $I_{\\mathrm{train}}$：$t_i \\le \\text{\"2018-01-01\"}$ 的条目。它们是 $d_0, d_1, d_2, d_3, d_4$（5 个条目）。\n- $C_{\\mathrm{train}}$：这些条目的簇是 $\\{c_0, c_1, c_2, c_3, c_4\\} = \\{\\text{\"C1\"}, \\text{\"C2\"}, \\text{\"C3\"}\\}$。\n- $I_{\\mathrm{post}}$：$t_i  \\text{\"2018-01-01\"}$ 的条目。它们是 $d_5, d_6, d_7, d_8, d_9, d_{10}, d_{11}, d_{12}, d_{13}, d_{14}$（10 个条目）。\n- $N_{\\mathrm{naive}} = 10$。\n- $I_{\\mathrm{clean}}$：根据 $C_{\\mathrm{train}}$ 过滤 $I_{\\mathrm{post}}$。\n  - $d_8$（簇 \"C2\"）被移除，因为 \"C2\" $\\in C_{\\mathrm{train}}$。\n  - $d_{14}$（簇 \"C1\"）被移除，因为 \"C1\" $\\in C_{\\mathrm{train}}$。\n  - 剩下的 $8$ 个条目构成 $I_{\\mathrm{clean}}$。\n- $N_{\\mathrm{clean}} = 8$。\n- $N_{\\mathrm{leak}} = 10 - 8 = 2$。\n- 结果：$[10, 8, 2]$。\n\n**情况 2：$\\tau = \\text{\"2018-01-01\"}$，$b = 60$ 天**\n- 训练截止日期：$t_{\\mathrm{cutoff}} = \\text{\"2018-01-01\"} - 60 \\text{ 天} = \\text{\"2017-11-02\"}$。\n- $I_{\\mathrm{train}}$：$t_i \\le \\text{\"2017-11-02\"}$ 的条目。它们是 $d_0, d_1, d_2$（3 个条目）。\n- $C_{\\mathrm{train}}$：这些条目的簇是 $\\{c_0, c_1, c_2\\} = \\{\\text{\"C1\"}, \\text{\"C2\"}\\}$。\n- $I_{\\mathrm{post}}$：$t_i  \\text{\"2018-01-01\"}$ 的条目。该集合与情况 1 相同。\n- $N_{\\mathrm{naive}} = 10$。\n- $I_{\\mathrm{clean}}$：根据新的 $C_{\\mathrm{train}}$ 过滤 $I_{\\mathrm{post}}$。\n  - $d_8$（簇 \"C2\"）被移除，因为 \"C2\" $\\in C_{\\mathrm{train}}$。\n  - $d_{14}$（簇 \"C1\"）被移除，因为 \"C1\" $\\in C_{\\mathrm{train}}$。\n  - 剩下的 $8$ 个条目构成 $I_{\\mathrm{clean}}$。\n- $N_{\\mathrm{clean}} = 8$。\n- $N_{\\mathrm{leak}} = 10 - 8 = 2$。\n- 结果：$[10, 8, 2]$。\n\n**情况 3：$\\tau = \\text{\"2019-01-01\"}$，$b = 30$ 天**\n- 训练截止日期：$t_{\\mathrm{cutoff}} = \\text{\"2019-01-01\"} - 30 \\text{ 天} = \\text{\"2018-12-02\"}$。\n- $I_{\\mathrm{train}}$：$t_i \\le \\text{\"2018-12-02\"}$ 的条目。它们是 $d_0, d_1, d_2, d_3, d_4, d_5, d_6, d_7, d_8$（9 个条目）。\n- $C_{\\mathrm{train}}$：这些条目的簇是 $\\{\\text{\"C1\"}, \\text{\"C2\"}, \\text{\"C3\"}, \\text{\"C11\"}, \\text{\"C4\"}, \\text{\"C5\"}\\}$。\n- $I_{\\mathrm{post}}$：$t_i  \\text{\"2019-01-01\"}$ 的条目。它们是 $d_9, d_{10}, d_{11}, d_{12}, d_{13}, d_{14}$（6 个条目）。\n- $N_{\\mathrm{naive}} = 6$。\n- $I_{\\mathrm{clean}}$：根据 $C_{\\mathrm{train}}$ 过滤 $I_{\\mathrm{post}}$。\n  - $d_{14}$（簇 \"C1\"）被移除，因为 \"C1\" $\\in C_{\\mathrm{train}}$。\n  - 剩下的 $5$ 个条目构成 $I_{\\mathrm{clean}}$。\n- $N_{\\mathrm{clean}} = 5$。\n- $N_{\\mathrm{leak}} = 6 - 5 = 1$。\n- 结果：$[6, 5, 1]$。\n\n**情况 4：$\\tau = \\text{\"2016-12-15\"}$，$b = 0$ 天**\n- 训练截止日期：$t_{\\mathrm{cutoff}} = \\text{\"2016-12-15\"} - 0 \\text{ 天} = \\text{\"2016-12-15\"}$。\n- $I_{\\mathrm{train}}$：$t_i \\le \\text{\"2016-12-15\"}$ 的条目。只有 $d_0$ 符合条件（1 个条目）。\n- $C_{\\mathrm{train}} = \\{\\text{\"C1\"}\\}$。\n- $I_{\\mathrm{post}}$：$t_i  \\text{\"2016-12-15\"}$ 的条目。这包括除 $d_0$ 之外的所有条目（14 个条目）。\n- $N_{\\mathrm{naive}} = 14$。\n- $I_{\\mathrm{clean}}$：根据 $C_{\\mathrm{train}}$ 过滤 $I_{\\mathrm{post}}$。\n  - $d_4$（簇 \"C1\"）被移除。\n  - $d_{14}$（簇 \"C1\"）被移除。\n  - 剩下的 $12$ 个条目构成 $I_{\\mathrm{clean}}$。\n- $N_{\\mathrm{clean}} = 12$。\n- $N_{\\mathrm{leak}} = 14 - 12 = 2$。\n- 结果：$[14, 12, 2]$。\n\n**情况 5：$\\tau = \\text{\"2022-01-01\"}$，$b = 0$ 天**\n- 训练截止日期：$t_{\\mathrm{cutoff}} = \\text{\"2022-01-01\"} - 0 \\text{ 天} = \\text{\"2022-01-01\"}$。\n- $I_{\\mathrm{train}}$：$t_i \\le \\text{\"2022-01-01\"}$ 的条目。数据集中的所有 $15$ 个条目都符合条件。\n- $C_{\\mathrm{train}} = \\{\\text{\"C1\"}, \\text{\"C2\"}, \\text{\"C3\"}, \\text{\"C4\"}, \\text{\"C5\"}, \\text{\"C6\"}, \\text{\"C7\"}, \\text{\"C8\"}, \\text{\"C9\"}, \\text{\"C10\"}, \\text{\"C11\"}\\}$。\n- $I_{\\mathrm{post}}$：$t_i  \\text{\"2022-01-01\"}$ 的条目。数据集中没有这样的条目（0 个条目）。\n- $N_{\\mathrm{naive}} = 0$。\n- $I_{\\mathrm{clean}}$：由于 $I_{\\mathrm{post}}$ 为空，因此 $I_{\\mathrm{clean}}$ 也为空。\n- $N_{\\mathrm{clean}} = 0$。\n- $N_{\\mathrm{leak}} = 0 - 0 = 0$。\n- 结果：$[0, 0, 0]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom datetime import date, timedelta\n\ndef solve():\n    \"\"\"\n    Computes naive, clean, and leaked evaluation counts for a temporal\n    splitting methodology on a fixed dataset of PDB entries.\n    \"\"\"\n    \n    # Dataset of PDB entries: (PDB ID, Release Date, Cluster ID)\n    dataset = [\n        (\"1A01\", \"2016-12-15\", \"C1\"),\n        (\"2B02\", \"2017-06-10\", \"C2\"),\n        (\"13M13\", \"2017-08-01\", \"C2\"),\n        (\"3C03\", \"2017-12-31\", \"C3\"),\n        (\"4D04\", \"2018-01-01\", \"C1\"),\n        (\"14N14\", \"2018-01-31\", \"C11\"),\n        (\"5E05\", \"2018-03-01\", \"C4\"),\n        (\"6F06\", \"2018-05-01\", \"C5\"),\n        (\"7G07\", \"2018-07-01\", \"C2\"),\n        (\"8H08\", \"2019-01-15\", \"C6\"),\n        (\"9I09\", \"2020-06-01\", \"C7\"),\n        (\"10J10\", \"2021-01-01\", \"C8\"),\n        (\"11K11\", \"2021-03-02\", \"C9\"),\n        (\"12L12\", \"2022-01-01\", \"C10\"),\n        (\"15O15\", \"2019-12-31\", \"C1\"),\n    ]\n\n    # Parse date strings into date objects for easier comparison.\n    parsed_dataset = [\n        (pdb_id, date.fromisoformat(date_str), cluster_id)\n        for pdb_id, date_str, cluster_id in dataset\n    ]\n\n    # Test suite of (tau, b) pairs.\n    test_cases = [\n        (\"2018-01-01\", 0),\n        (\"2018-01-01\", 60),\n        (\"2019-01-01\", 30),\n        (\"2016-12-15\", 0),\n        (\"2022-01-01\", 0),\n    ]\n\n    results = []\n    for tau_str, b in test_cases:\n        tau_date = date.fromisoformat(tau_str)\n        buffer_delta = timedelta(days=b)\n        train_cutoff_date = tau_date - buffer_delta\n\n        # 1. Determine the training cluster set C_train(tau, b)\n        training_clusters = set()\n        for _, t_i, c_i in parsed_dataset:\n            if t_i = train_cutoff_date:\n                training_clusters.add(c_i)\n\n        # 2. Determine the candidate evaluation set I_post(tau)\n        post_tau_entries = []\n        for entry in parsed_dataset:\n            _, t_i, _ = entry\n            if t_i > tau_date:\n                post_tau_entries.append(entry)\n        \n        n_naive = len(post_tau_entries)\n\n        # 3. Determine the clean evaluation set I_clean(tau, b)\n        clean_entries = []\n        for entry in post_tau_entries:\n            _, _, c_i = entry\n            if c_i not in training_clusters:\n                clean_entries.append(entry)\n        \n        n_clean = len(clean_entries)\n\n        # 4. Calculate the leaked count N_leak\n        n_leak = n_naive - n_clean\n\n        results.append([n_naive, n_clean, n_leak])\n\n    # Format the results into the exact required output string.\n    # e.g., [[1,2,3],[4,5,6]]\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}