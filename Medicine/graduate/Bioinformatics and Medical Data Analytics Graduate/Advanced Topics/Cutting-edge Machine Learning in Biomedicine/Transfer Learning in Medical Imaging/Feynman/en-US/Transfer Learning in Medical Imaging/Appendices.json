{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we first explore the theoretical risks of transfer learning. Naively applying a model from a source domain (e.g., pediatric images) to a target domain (e.g., adult images) can sometimes hurt performance, a phenomenon known as negative transfer. This exercise guides you through deriving the exact analytical form of this performance drop, revealing its fundamental connection to the Kullback-Leibler (KL) divergence between the true data distributions of the two domains .",
            "id": "4615246",
            "problem": "Consider a transfer learning scenario in chest X-ray diagnostics involving two age groups: pediatric and adult. Let $a \\in \\{0,1\\}$ denote age group, with $a=0$ for pediatric and $a=1$ for adult. The input is an image feature vector $x \\in \\mathcal{X}$ and the binary disease label is $y \\in \\{0,1\\}$. Assume a data-generating process in which the adult-domain joint distribution factorizes as $P(x,y \\mid a=1) = P(x \\mid a=1) P(y \\mid x, a=1)$, and define the demographic-conditional posteriors $\\eta_{a}(x) := P(y=1 \\mid x, a)$.\n\nA classifier in this setting is a probabilistic predictor that maps $x$ to a probability $q(x) \\in [0,1]$, interpreted as the predicted $P(y=1 \\mid x)$. Its target-domain performance is measured under the Negative Log-Likelihood (NLL), also known as Cross-Entropy (CE) loss, defined pointwise by $\\ell(q(x), y) = -\\ln\\left( q(x)^{y} (1-q(x))^{1-y} \\right)$, with target-domain risk $R_{1}(q) := \\mathbb{E}_{x \\sim P(x \\mid a=1)} \\, \\mathbb{E}_{y \\sim P(y \\mid x, a=1)} \\left[ \\ell(q(x), y) \\right]$. It is a fundamental and widely used fact in statistical learning that the Bayes-optimal predictor for CE minimizes $R_{1}(q)$ and is given by $q_{1}^{\\star}(x) = \\eta_{1}(x)$.\n\nSuppose a transfer procedure initializes a target-domain model by adopting a source-domain pediatric posterior, i.e., $q_{0}(x) := \\eta_{0}(x)$, and then evaluates it directly on the adult domain without adaptation. Define the excess adult risk incurred by this transferred predictor as\n$$\n\\Delta := R_{1}(q_{0}) - R_{1}(q_{1}^{\\star}).\n$$\nThis excess quantifies the degree of negative transfer under CE: when $\\Delta$ is positive and sufficiently large, the transferred model performs worse than a target-domain model that is Bayes-optimal under CE.\n\nUsing only the factorization of the adult-domain joint distribution, the definition of CE risk, and the Bayes optimality of $q_{1}^{\\star}(x) = \\eta_{1}(x)$ for CE, derive the exact analytic expression of $\\Delta$ as a functional of the demographic-conditional posteriors $\\eta_{0}(x)$ and $\\eta_{1}(x)$ and the adult marginal $P(x \\mid a=1)$. Your final answer must be a single closed-form expression. No inequalities are permitted in the final answer. If you introduce any acronym, you must write its full name on first use. No numerical rounding is required for this problem.",
            "solution": "The problem asks for the derivation of the excess adult risk $\\Delta$ incurred by using a pediatric-domain predictor on an adult population. The excess risk is defined as $\\Delta := R_{1}(q_{0}) - R_{1}(q_{1}^{\\star})$, where $q_{0}(x) = \\eta_{0}(x)$ is the transferred predictor and $q_{1}^{\\star}(x) = \\eta_{1}(x)$ is the Bayes-optimal predictor for the adult target domain.\n\nThe derivation begins with the definition of the target-domain risk, $R_{1}(q)$, for an arbitrary predictor $q(x)$.\n$$\nR_{1}(q) := \\mathbb{E}_{x \\sim P(x \\mid a=1)} \\, \\mathbb{E}_{y \\sim P(y \\mid x, a=1)} \\left[ \\ell(q(x), y) \\right]\n$$\nThe loss function is the Negative Log-Likelihood (NLL), also known as Cross-Entropy (CE), given by $\\ell(q(x), y) = -\\ln\\left( q(x)^{y} (1-q(x))^{1-y} \\right)$. Using logarithm properties, this can be written as:\n$$\n\\ell(q(x), y) = - \\big( y \\ln(q(x)) + (1-y) \\ln(1-q(x)) \\big)\n$$\nSubstituting this into the risk definition:\n$$\nR_{1}(q) = \\mathbb{E}_{x \\sim P(x \\mid a=1)} \\left[ \\mathbb{E}_{y \\sim P(y \\mid x, a=1)} \\left[ - \\big( y \\ln(q(x)) + (1-y) \\ln(1-q(x)) \\big) \\right] \\right]\n$$\nWe first evaluate the inner expectation with respect to the conditional distribution of the label $y$, $P(y \\mid x, a=1)$. Since $y$ is a binary variable, the expectation is a sum over its two possible values, $y=0$ and $y=1$. The probabilities are given by the true demographic-conditional posterior for the adult domain ($a=1$): $P(y=1 \\mid x, a=1) = \\eta_{1}(x)$ and $P(y=0 \\mid x, a=1) = 1 - \\eta_{1}(x)$.\n\nThe inner expectation becomes:\n\\begin{align*}\n\\mathbb{E}_{y \\sim P(y \\mid x, a=1)} [\\ell(q(x),y)] &= P(y=1 \\mid x, a=1) \\cdot \\ell(q(x), 1) + P(y=0 \\mid x, a=1) \\cdot \\ell(q(x), 0) \\\\\n&= \\eta_{1}(x) \\cdot \\big( - \\ln(q(x)) \\big) + (1-\\eta_{1}(x)) \\cdot \\big( - \\ln(1-q(x)) \\big) \\\\\n&= - \\big[ \\eta_{1}(x) \\ln(q(x)) + (1-\\eta_{1}(x)) \\ln(1-q(x)) \\big]\n\\end{align*}\nThis expression is the cross-entropy between two Bernoulli distributions with parameters $\\eta_{1}(x)$ and $q(x)$, respectively.\n\nSubstituting this back into the expression for $R_{1}(q)$ gives the risk as an expectation over the feature distribution $P(x \\mid a=1)$:\n$$\nR_{1}(q) = \\mathbb{E}_{x \\sim P(x \\mid a=1)} \\left[ - \\big[ \\eta_{1}(x) \\ln(q(x)) + (1-\\eta_{1}(x)) \\ln(1-q(x)) \\big] \\right]\n$$\nNow we can compute the two specific risks required for $\\Delta$.\n\nFirst, the risk of the transferred predictor $q_{0}(x) = \\eta_{0}(x)$:\n$$\nR_{1}(q_{0}) = \\mathbb{E}_{x \\sim P(x \\mid a=1)} \\left[ - \\big[ \\eta_{1}(x) \\ln(\\eta_{0}(x)) + (1-\\eta_{1}(x)) \\ln(1-\\eta_{0}(x)) \\big] \\right]\n$$\nSecond, the risk of the Bayes-optimal predictor $q_{1}^{\\star}(x) = \\eta_{1}(x)$:\n$$\nR_{1}(q_{1}^{\\star}) = \\mathbb{E}_{x \\sim P(x \\mid a=1)} \\left[ - \\big[ \\eta_{1}(x) \\ln(\\eta_{1}(x)) + (1-\\eta_{1}(x)) \\ln(1-\\eta_{1}(x)) \\big] \\right]\n$$\nThe term inside this expectation, $-[\\eta_{1}(x) \\ln(\\eta_{1}(x)) + (1-\\eta_{1}(x)) \\ln(1-\\eta_{1}(x))]$, is the entropy of a Bernoulli distribution with parameter $\\eta_{1}(x)$.\n\nThe excess risk $\\Delta$ is the difference $R_{1}(q_{0}) - R_{1}(q_{1}^{\\star})$. Using the linearity of the expectation operator, we can combine the terms:\n\\begin{align*}\n\\Delta &= R_{1}(q_{0}) - R_{1}(q_{1}^{\\star}) \\\\\n&= \\mathbb{E}_{x \\sim P(x \\mid a=1)} \\left[ - \\big[ \\eta_{1}(x) \\ln(\\eta_{0}(x)) + (1-\\eta_{1}(x)) \\ln(1-\\eta_{0}(x)) \\big] - \\left( - \\big[ \\eta_{1}(x) \\ln(\\eta_{1}(x)) + (1-\\eta_{1}(x)) \\ln(1-\\eta_{1}(x)) \\big] \\right) \\right] \\\\\n&= \\mathbb{E}_{x \\sim P(x \\mid a=1)} \\left[ \\eta_{1}(x) \\ln(\\eta_{1}(x)) - \\eta_{1}(x) \\ln(\\eta_{0}(x)) + (1-\\eta_{1}(x)) \\ln(1-\\eta_{1}(x)) - (1-\\eta_{1}(x)) \\ln(1-\\eta_{0}(x)) \\right]\n\\end{align*}\nWe can group the terms and use the logarithm property $\\ln(a) - \\ln(b) = \\ln(a/b)$:\n\\begin{align*}\n\\Delta &= \\mathbb{E}_{x \\sim P(x \\mid a=1)} \\left[ \\eta_{1}(x) (\\ln(\\eta_{1}(x)) - \\ln(\\eta_{0}(x))) + (1-\\eta_{1}(x)) (\\ln(1-\\eta_{1}(x)) - \\ln(1-\\eta_{0}(x))) \\right] \\\\\n&= \\mathbb{E}_{x \\sim P(x \\mid a=1)} \\left[ \\eta_{1}(x) \\ln\\left(\\frac{\\eta_{1}(x)}{\\eta_{0}(x)}\\right) + (1-\\eta_{1}(x)) \\ln\\left(\\frac{1-\\eta_{1}(x)}{1-\\eta_{0}(x)}\\right) \\right]\n\\end{align*}\nThe expression inside the expectation is the Kullback-Leibler (KL) divergence, a measure of how one probability distribution diverges from a second, expected probability distribution. Specifically, it is the KL divergence between a Bernoulli distribution with parameter $\\eta_{1}(x)$ and another with parameter $\\eta_{0}(x)$, often denoted as $D_{KL}(\\text{Bernoulli}(\\eta_1(x)) \\,\\|\\, \\text{Bernoulli}(\\eta_0(x)))$.\n\nThus, the excess risk $\\Delta$ is the expected KL divergence between the true posterior label distribution in the target domain and the posterior label distribution from the source domain, averaged over the distribution of features in the target domain. This is the final analytical expression.",
            "answer": "$$\n\\boxed{\\mathbb{E}_{x \\sim P(x \\mid a=1)} \\left[ \\eta_{1}(x) \\ln\\left(\\frac{\\eta_{1}(x)}{\\eta_{0}(x)}\\right) + (1-\\eta_{1}(x)) \\ln\\left(\\frac{1-\\eta_{1}(x)}{1-\\eta_{0}(x)}\\right) \\right]}\n$$"
        },
        {
            "introduction": "After understanding the theoretical risk of domain shift, it is crucial to have tools to measure it in practice. This problem introduces Maximum Mean Discrepancy (MMD), a powerful non-parametric statistic for quantifying the distance between the feature distributions of source and target datasets. By working through a concrete calculation, you will gain hands-on experience with MMD and see how it provides a tangible bound on the anticipated error in the target domain .",
            "id": "4615251",
            "problem": "In a transfer learning study for chest radiograph classification, a feature extractor trained on a large source dataset outputs two-dimensional feature embeddings for both source and target domains. Let the source feature set be $\\{\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\mathbf{x}_{3}\\}$ with $\\mathbf{x}_{1} = (0, 0)$, $\\mathbf{x}_{2} = (0.5, 0)$, $\\mathbf{x}_{3} = (0, 0.5)$, and the target feature set be $\\{\\mathbf{y}_{1}, \\mathbf{y}_{2}, \\mathbf{y}_{3}\\}$ with $\\mathbf{y}_{1} = (1, 1)$, $\\mathbf{y}_{2} = (1.5, 1)$, $\\mathbf{y}_{3} = (1, 1.5)$. Consider the Gaussian radial basis function kernel $k(\\mathbf{u}, \\mathbf{v}) = \\exp\\!\\left(-\\|\\mathbf{u} - \\mathbf{v}\\|^{2} / (2 \\sigma^{2})\\right)$ with bandwidth $\\sigma = 0.5$, and let the Reproducing Kernel Hilbert Space (RKHS) be the space associated with $k$. \n\nUsing first principles from kernel mean embeddings and the definition of Maximum Mean Discrepancy (MMD) as the difference between RKHS mean embeddings of distributions, compute the unbiased empirical estimate of the squared MMD between the source and target feature sets and then its square root to obtain the MMD. Next, assume the composed loss function $\\ell \\circ h$ belongs to the RKHS with norm bounded by $c = 0.5$ and that the empirical source-domain expected loss is $0.12$. Using only foundational RKHS inequalities, derive a numerical upper bound on the anticipated target-domain expected loss in terms of the computed MMD. \n\nRound your final numerical bound to four significant figures and report it as a pure number (no units).",
            "solution": "### Solution\n\nThe solution is divided into two parts as requested: first, the computation of the Maximum Mean Discrepancy (MMD) between the source and target feature sets, and second, the derivation of an upper bound on the target-domain expected loss.\n\n**Part 1: Computation of the Maximum Mean Discrepancy (MMD)**\n\nThe Maximum Mean Discrepancy is defined as the distance between the mean embeddings of two probability distributions, $P_S$ (source) and $P_T$ (target), in a Reproducing Kernel Hilbert Space (RKHS) $\\mathcal{H}$. The squared MMD is given by:\n$$ \\text{MMD}^2(P_S, P_T) = \\|\\mu_{P_S} - \\mu_{P_T}\\|_{\\mathcal{H}}^2 $$\nwhere $\\mu_P$ is the mean embedding of a distribution $P$. For finite samples $X = \\{\\mathbf{x}_i\\}_{i=1}^m$ from $P_S$ and $Y = \\{\\mathbf{y}_j\\}_{j=1}^n$ from $P_T$, an unbiased empirical estimator of the squared MMD is given by:\n$$ \\widehat{\\text{MMD}}_u^2(X, Y) = \\frac{1}{m(m-1)} \\sum_{i \\neq j} k(\\mathbf{x}_i, \\mathbf{x}_j) + \\frac{1}{n(n-1)} \\sum_{i \\neq j} k(\\mathbf{y}_i, \\mathbf{y}_j) - \\frac{2}{mn} \\sum_{i=1}^m \\sum_{j=1}^n k(\\mathbf{x}_i, \\mathbf{y}_j) $$\nIn this problem, we have $m=3$ source samples and $n=3$ target samples. The kernel is the Gaussian RBF with $\\sigma = 0.5$. The kernel function is:\n$$ k(\\mathbf{u}, \\mathbf{v}) = \\exp\\left(-\\frac{\\|\\mathbf{u} - \\mathbf{v}\\|^{2}}{2 \\sigma^2}\\right) = \\exp\\left(-\\frac{\\|\\mathbf{u} - \\mathbf{v}\\|^{2}}{2 (0.5)^2}\\right) = \\exp\\left(-\\frac{\\|\\mathbf{u} - \\mathbf{v}\\|^{2}}{0.5}\\right) = \\exp(-2\\|\\mathbf{u} - \\mathbf{v}\\|^{2}) $$\n\nWe now compute the individual kernel terms.\n\n**1. Within-Source Terms:** $S_1 = \\sum_{i \\neq j} k(\\mathbf{x}_i, \\mathbf{x}_j)$\nThe source points are $\\mathbf{x}_{1} = (0, 0)$, $\\mathbf{x}_{2} = (0.5, 0)$, $\\mathbf{x}_{3} = (0, 0.5)$.\n-   $\\|\\mathbf{x}_1 - \\mathbf{x}_2\\|^2 = \\|(0,0) - (0.5,0)\\|^2 = (-0.5)^2 = 0.25$.\n    $k(\\mathbf{x}_1, \\mathbf{x}_2) = \\exp(-2 \\cdot 0.25) = \\exp(-0.5)$.\n-   $\\|\\mathbf{x}_1 - \\mathbf{x}_3\\|^2 = \\|(0,0) - (0,0.5)\\|^2 = (-0.5)^2 = 0.25$.\n    $k(\\mathbf{x}_1, \\mathbf{x}_3) = \\exp(-2 \\cdot 0.25) = \\exp(-0.5)$.\n-   $\\|\\mathbf{x}_2 - \\mathbf{x}_3\\|^2 = \\|(0.5,0) - (0,0.5)\\|^2 = (0.5)^2 + (-0.5)^2 = 0.25 + 0.25 = 0.5$.\n    $k(\\mathbf{x}_2, \\mathbf{x}_3) = \\exp(-2 \\cdot 0.5) = \\exp(-1)$.\nThe sum $\\sum_{i \\neq j}$ has $m(m-1)=6$ terms.\n$S_1 = 2 \\cdot \\left( \\exp(-0.5) + \\exp(-0.5) + \\exp(-1) \\right) = 2 \\cdot (2\\exp(-0.5) + \\exp(-1))$.\nThe first term of $\\widehat{\\text{MMD}}_u^2$ is $\\frac{1}{3(2)} S_1 = \\frac{1}{3}(2\\exp(-0.5) + \\exp(-1))$.\n\n**2. Within-Target Terms:** $S_2 = \\sum_{i \\neq j} k(\\mathbf{y}_i, \\mathbf{y}_j)$\nThe target points are $\\mathbf{y}_{1} = (1, 1)$, $\\mathbf{y}_{2} = (1.5, 1)$, $\\mathbf{y}_{3} = (1, 1.5)$.\nThe geometry of the target points is identical to the source points (a right-angled isosceles triangle with shorter side length $0.5$). Thus, the kernel evaluations are the same.\n-   $k(\\mathbf{y}_1, \\mathbf{y}_2) = \\exp(-0.5)$.\n-   $k(\\mathbf{y}_1, \\mathbf{y}_3) = \\exp(-0.5)$.\n-   $k(\\mathbf{y}_2, \\mathbf{y}_3) = \\exp(-1)$.\nThe second term of $\\widehat{\\text{MMD}}_u^2$ is $\\frac{1}{3(2)} \\sum_{i \\neq j} k(\\mathbf{y}_i, \\mathbf{y}_j) = \\frac{1}{3}(2\\exp(-0.5) + \\exp(-1))$.\n\n**3. Cross-Domain Terms:** $S_3 = \\sum_{i=1}^3 \\sum_{j=1}^3 k(\\mathbf{x}_i, \\mathbf{y}_j)$\n-   $k(\\mathbf{x}_1, \\mathbf{y}_1) = \\exp(-2\\|(0,0)-(1,1)\\|^2) = \\exp(-2(1^2+1^2)) = \\exp(-4)$.\n-   $k(\\mathbf{x}_1, \\mathbf{y}_2) = \\exp(-2\\|(0,0)-(1.5,1)\\|^2) = \\exp(-2(1.5^2+1^2)) = \\exp(-2(3.25)) = \\exp(-6.5)$.\n-   $k(\\mathbf{x}_1, \\mathbf{y}_3) = \\exp(-2\\|(0,0)-(1,1.5)\\|^2) = \\exp(-2(1^2+1.5^2)) = \\exp(-2(3.25)) = \\exp(-6.5)$.\n-   $k(\\mathbf{x}_2, \\mathbf{y}_1) = \\exp(-2\\|(0.5,0)-(1,1)\\|^2) = \\exp(-2((-0.5)^2+(-1)^2)) = \\exp(-2(1.25)) = \\exp(-2.5)$.\n-   $k(\\mathbf{x}_2, \\mathbf{y}_2) = \\exp(-2\\|(0.5,0)-(1.5,1)\\|^2) = \\exp(-2((-1)^2+(-1)^2)) = \\exp(-4)$.\n-   $k(\\mathbf{x}_2, \\mathbf{y}_3) = \\exp(-2\\|(0.5,0)-(1,1.5)\\|^2) = \\exp(-2((-0.5)^2+(-1.5)^2)) = \\exp(-2(2.5)) = \\exp(-5)$.\n-   $k(\\mathbf{x}_3, \\mathbf{y}_1) = \\exp(-2\\|(0,0.5)-(1,1)\\|^2) = \\exp(-2((-1)^2+(-0.5)^2)) = \\exp(-2(1.25)) = \\exp(-2.5)$.\n-   $k(\\mathbf{x}_3, \\mathbf{y}_2) = \\exp(-2\\|(0,0.5)-(1.5,1)\\|^2) = \\exp(-2((-1.5)^2+(-0.5)^2)) = \\exp(-2(2.5)) = \\exp(-5)$.\n-   $k(\\mathbf{x}_3, \\mathbf{y}_3) = \\exp(-2\\|(0,0.5)-(1,1.5)\\|^2) = \\exp(-2((-1)^2+(-1)^2)) = \\exp(-4)$.\nSumming these $9$ terms:\n$S_3 = 3\\exp(-4) + 2\\exp(-6.5) + 2\\exp(-2.5) + 2\\exp(-5)$.\n\n**4. Final MMD Calculation:**\n$$ \\widehat{\\text{MMD}}_u^2(X, Y) = \\frac{1}{3}(2\\exp(-0.5) + \\exp(-1)) + \\frac{1}{3}(2\\exp(-0.5) + \\exp(-1)) - \\frac{2}{9} S_3 $$\n$$ \\widehat{\\text{MMD}}_u^2(X, Y) = \\frac{2}{3}(2\\exp(-0.5) + \\exp(-1)) - \\frac{2}{9}(3\\exp(-4) + 2\\exp(-2.5) + 2\\exp(-5) + 2\\exp(-6.5)) $$\n$$ \\widehat{\\text{MMD}}_u^2(X, Y) = \\frac{4}{3}\\exp(-0.5) + \\frac{2}{3}\\exp(-1) - \\frac{2}{3}\\exp(-4) - \\frac{4}{9}\\exp(-2.5) - \\frac{4}{9}\\exp(-5) - \\frac{4}{9}\\exp(-6.5) $$\nNow, we substitute the numerical values:\n$\\exp(-0.5) \\approx 0.606531$\n$\\exp(-1) \\approx 0.367879$\n$\\exp(-2.5) \\approx 0.082085$\n$\\exp(-4) \\approx 0.018316$\n$\\exp(-5) \\approx 0.006738$\n$\\exp(-6.5) \\approx 0.001503$\n$$ \\widehat{\\text{MMD}}_u^2 \\approx \\frac{4}{3}(0.606531) + \\frac{2}{3}(0.367879) - \\frac{2}{3}(0.018316) - \\frac{4}{9}(0.082085) - \\frac{4}{9}(0.006738) - \\frac{4}{9}(0.001503) $$\n$$ \\widehat{\\text{MMD}}_u^2 \\approx 0.808708 + 0.245253 - 0.012211 - 0.036482 - 0.002995 - 0.000668 $$\n$$ \\widehat{\\text{MMD}}_u^2 \\approx 1.053961 - 0.052356 = 1.001605 $$\nThe problem asks for the MMD, which is the square root of this value:\n$$ \\widehat{\\text{MMD}}_u = \\sqrt{1.001605} \\approx 1.000802 $$\n\n**Part 2: Upper Bound on Target-Domain Expected Loss**\n\nThe foundational RKHS property linking expectations and inner products is $E_{P}[g(\\mathbf{x})] = \\langle g, \\mu_P \\rangle_{\\mathcal{H}}$ for any function $g \\in \\mathcal{H}$. The difference in expectations between the source ($P_S$) and target ($P_T$) distributions for a function $g$ can be written as:\n$$ |E_{P_S}[g] - E_{P_T}[g]| = |\\langle g, \\mu_{P_S} \\rangle_{\\mathcal{H}} - \\langle g, \\mu_{P_T} \\rangle_{\\mathcal{H}}| = |\\langle g, \\mu_{P_S} - \\mu_{P_T} \\rangle_{\\mathcal{H}}| $$\nBy the Cauchy-Schwarz inequality in the RKHS, we have:\n$$ |\\langle g, \\mu_{P_S} - \\mu_{P_T} \\rangle_{\\mathcal{H}}| \\le \\|g\\|_{\\mathcal{H}} \\|\\mu_{P_S} - \\mu_{P_T}\\|_{\\mathcal{H}} $$\nBy definition, $\\|\\mu_{P_S} - \\mu_{P_T}\\|_{\\mathcal{H}} = \\text{MMD}(P_S, P_T)$. This yields the key inequality:\n$$ |E_{P_S}[g] - E_{P_T}[g]| \\le \\|g\\|_{\\mathcal{H}} \\text{MMD}(P_S, P_T) $$\nFrom this, we can bound the target-domain expectation:\n$$ E_{P_T}[g] \\le E_{P_S}[g] + \\|g\\|_{\\mathcal{H}} \\text{MMD}(P_S, P_T) $$\nIn our problem, the function $g$ is the composed loss function, $g = \\ell \\circ h$. We are given:\n-   An upper bound on its norm: $\\|g\\|_{\\mathcal{H}} = \\|\\ell \\circ h\\|_{\\mathcal{H}} \\le c = 0.5$.\n-   The empirical source-domain expected loss: $\\hat{E}_S[g] = \\frac{1}{3}\\sum_{i=1}^3 \\ell(h(\\mathbf{x}_i)) = 0.12$.\n-   We have computed the empirical MMD: $\\widehat{\\text{MMD}}_u(X, Y) \\approx 1.000802$.\n\nThe problem asks for a numerical upper bound on the *anticipated* target-domain expected loss. This directs us to substitute the available empirical estimates into the theoretical inequality, which provides a practical, albeit not statistically rigorous, bound.\n$$ \\text{Anticipated } E_{P_T}[\\ell \\circ h] \\le \\hat{E}_S[\\ell \\circ h] + c \\cdot \\widehat{\\text{MMD}}_u(X, Y) $$\nSubstituting the given and computed values:\n$$ \\text{Upper Bound} \\le 0.12 + (0.5) \\cdot (1.000802) $$\n$$ \\text{Upper Bound} \\le 0.12 + 0.500401 $$\n$$ \\text{Upper Bound} \\le 0.620401 $$\nThe problem requires the final numerical bound to be rounded to four significant figures.\n$$ \\text{Upper Bound} \\approx 0.6204 $$",
            "answer": "$$\\boxed{0.6204}$$"
        },
        {
            "introduction": "Ultimately, the success of a transferred model is judged by its clinical utility, which depends not just on its ability to rank cases correctly but on the reliability of its probability estimates. This final exercise addresses the critical issue of model calibration, a common victim of domain shift, by having you compute essential metrics like Expected Calibration Error (ECE) and the Brier score. This practice will solidify your understanding of why a miscalibrated model, even one with a high AUC, can lead to suboptimal clinical decisions .",
            "id": "4615254",
            "problem": "A convolutional neural network pre-trained on a large natural image corpus and then fine-tuned on a source hospitalâ€™s chest radiographs is deployed at a target hospital to detect pneumonia. On a held-out target-domain audit set of size $N = 12$, the transferred model outputs predicted probabilities $\\{p_i\\}_{i=1}^{N}$ for pneumonia and the corresponding ground-truth labels $\\{y_i\\}_{i=1}^{N}$ with $y_i \\in \\{0,1\\}$ are recorded as the following pairs $(p_i, y_i)$:\n$(0.92, 0)$, $(0.88, 1)$, $(0.83, 1)$, $(0.77, 0)$, $(0.65, 1)$, $(0.58, 0)$, $(0.41, 1)$, $(0.36, 0)$, $(0.24, 0)$, $(0.18, 1)$, $(0.12, 0)$, $(0.05, 0)$.\nYou are to evaluate calibration of the transferred model on this target-domain set using $B = 5$ equal-width bins partitioning $[0,1)$ as $[0,0.2)$, $[0.2,0.4)$, $[0.4,0.6)$, $[0.6,0.8)$, and $[0.8,1.0]$.\n\nUsing only fundamental probability concepts and the definition of proper scoring rules, do the following:\n1. Starting from the notion of true conditional risk $r(x) = \\mathbb{P}(Y=1 \\mid X=x)$ and the definition of calibration as agreement between predicted probabilities and empirical event frequencies, derive a finite-sample, bin-based estimator for the expected calibration error and define the Brier score as a strictly proper scoring rule for probabilistic classification. Do not assume any pre-given formula; derive these from first principles (law of total expectation and empirical approximation).\n2. Compute the empirical expected calibration error on the given data using the specified bins, and compute the empirical Brier score for the same data.\n3. Using Bayes decision theory with asymmetric misclassification costs $c_{01}$ for a false positive and $c_{10}$ for a false negative, explain why a model can maintain a high Area Under the Receiver Operating Characteristic Curve (AUC-ROC) yet produce worse expected clinical decision utility when miscalibrated after transfer, even if its ranking of cases is preserved. Your argument must start from first principles of threshold decision rules on estimated risks and show how miscalibration perturbs the cost-optimal operating point.\n\nExpress your final computed calibration metrics as a row vector $\\big(\\mathrm{ECE}, \\mathrm{Brier}\\big)$, rounded to four significant figures, with no units. The final answer must contain only these two rounded numbers in that order.",
            "solution": "### Part 1: Derivation of Estimators\n\n#### Expected Calibration Error (ECE)\n\nA model is perfectly calibrated if its predicted probability for an event equals the true long-run frequency of that event. Let $X$ be the input features and $Y \\in \\{0, 1\\}$ be the binary outcome. A model produces a prediction $\\hat{P} = f(X)$, which is an estimate of the true conditional probability of the event, $r(X) = \\mathbb{P}(Y=1 \\mid X)$. Perfect calibration requires that, for any probability value $p \\in [0, 1]$, the following holds:\n$$\n\\mathbb{P}(Y=1 \\mid \\hat{P}=p) = p\n$$\nThis condition states that among all instances for which the model predicts a probability $p$, the actual fraction of positive instances is precisely $p$.\n\nThe calibration error for a specific prediction level $p$ is the absolute difference $|\\mathbb{P}(Y=1 \\mid \\hat{P}=p) - p|$. To obtain a single summary statistic for the model's calibration, we compute the expectation of this error over the distribution of the model's predictions $\\hat{P}$. This is the Expected Calibration Error (ECE):\n$$\n\\mathrm{ECE} = \\mathbb{E}_{\\hat{P}} \\big[ |\\mathbb{P}(Y=1 \\mid \\hat{P}) - \\hat{P}| \\big]\n$$\nThe outer expectation is taken over the random variable $\\hat{P}$. Using the law of total expectation, this can be written as an integral over all possible predicted probability values $p$:\n$$\n\\mathrm{ECE} = \\int_{0}^{1} |\\mathbb{P}(Y=1 \\mid \\hat{P}=p) - p| \\, f_{\\hat{P}}(p) \\, dp\n$$\nwhere $f_{\\hat{P}}(p)$ is the probability density function of the predicted scores.\n\nIn practice, with a finite dataset of size $N$, we cannot evaluate this integral directly. We resort to a bin-based estimation. The interval $[0, 1]$ is partitioned into $B$ disjoint bins, $I_m$ for $m=1, \\dots, B$. The ECE is then approximated as a weighted sum of the calibration errors within each bin:\n$$\n\\mathrm{ECE} = \\sum_{m=1}^{B} \\mathbb{P}(\\hat{P} \\in I_m) \\cdot \\mathbb{E}\\big[ |\\mathbb{P}(Y=1 \\mid \\hat{P}) - \\hat{P}| \\mid \\hat{P} \\in I_m \\big]\n$$\nWithin each bin $I_m$, we make two approximations. We approximate the true conditional probability $\\mathbb{P}(Y=1 \\mid \\hat{P}=p)$ for $p \\in I_m$ by the average accuracy of the predictions in that bin, $\\mathrm{acc}(I_m) = \\mathbb{E}[Y \\mid \\hat{P} \\in I_m]$. We approximate the prediction value $p$ by the average confidence of the predictions in that bin, $\\mathrm{conf}(I_m) = \\mathbb{E}[\\hat{P} \\mid \\hat{P} \\in I_m]$. This yields:\n$$\n\\mathrm{ECE} \\approx \\sum_{m=1}^{B} \\mathbb{P}(\\hat{P} \\in I_m) \\cdot |\\mathrm{acc}(I_m) - \\mathrm{conf}(I_m)|\n$$\nGiven a finite sample of $N$ predictions $\\{p_i\\}_{i=1}^{N}$ and labels $\\{y_i\\}_{i=1}^{N}$, we can compute empirical estimates for each term. Let $S_m$ be the set of indices of samples for which the prediction $p_i$ falls into bin $I_m$, and let $N_m = |S_m|$ be the number of samples in that bin.\nThe empirical estimates are:\n\\begin{itemize}\n    \\item Proportion of samples in bin $m$: $\\hat{\\mathbb{P}}(\\hat{P} \\in I_m) = \\frac{N_m}{N}$\n    \\item Empirical accuracy in bin $m$: $\\widehat{\\mathrm{acc}}(I_m) = \\frac{1}{N_m} \\sum_{i \\in S_m} y_i$\n    \\item Empirical confidence in bin $m$: $\\widehat{\\mathrm{conf}}(I_m) = \\frac{1}{N_m} \\sum_{i \\in S_m} p_i$\n\\end{itemize}\nSubstituting these into the approximation gives the finite-sample bin-based estimator for ECE:\n$$\n\\widehat{\\mathrm{ECE}} = \\sum_{m=1}^{B} \\frac{N_m}{N} \\left| \\left( \\frac{1}{N_m} \\sum_{i \\in S_m} y_i \\right) - \\left( \\frac{1}{N_m} \\sum_{i \\in S_m} p_i \\right) \\right| = \\frac{1}{N} \\sum_{m=1}^{B} \\left| \\sum_{i \\in S_m} y_i - \\sum_{i \\in S_m} p_i \\right|\n$$\n\n#### Brier Score\n\nA scoring rule $S(p, y)$ assigns a score to a probabilistic prediction $p$ for a binary outcome $y \\in \\{0, 1\\}$. A scoring rule is *proper* if the expected score is minimized by reporting the true probability. Let the true probability of the event be $q = \\mathbb{P}(Y=1)$. The expected score for a prediction $p$ is:\n$$\n\\mathbb{E}[S(p, Y)] = q \\cdot S(p, 1) + (1-q) \\cdot S(p, 0)\n$$\nThe rule is proper if $p=q$ minimizes this expectation. It is *strictly proper* if the minimum is unique.\n\nThe Brier score is defined as the mean squared error between the prediction and the outcome:\n$$\nS(p, y) = (p - y)^2\n$$\nThe expected Brier score for a prediction $p$ given a true probability $q$ is:\n$$\n\\mathbb{E}[S(p, Y)] = q(p-1)^2 + (1-q)(p-0)^2 = q(p^2 - 2p + 1) + (1-q)p^2\n$$\nTo find the value of $p$ that minimizes this expectation, we differentiate with respect to $p$ and set the result to zero:\n$$\n\\frac{d}{dp}\\mathbb{E}[S(p, Y)] = q(2p - 2) + (1-q)(2p) = 2qp - 2q + 2p - 2qp = 2p - 2q\n$$\nSetting the derivative to zero gives $2p - 2q = 0$, which implies $p=q$. The second derivative is $\\frac{d^2}{dp^2}\\mathbb{E}[S(p, Y)] = 2$, which is positive, confirming a unique minimum. Therefore, the Brier score is a strictly proper scoring rule, as it is uniquely minimized when the predicted probability equals the true probability.\n\nFor a dataset of $N$ pairs $(p_i, y_i)$, the empirical Brier score is the average of the individual squared errors:\n$$\n\\mathrm{Brier} = \\frac{1}{N} \\sum_{i=1}^{N} (p_i - y_i)^2\n$$\n\n### Part 2: Calculation of Metrics\n\nThe dataset consists of $N=12$ pairs $(p_i, y_i)$:\n$(0.92, 0)$, $(0.88, 1)$, $(0.83, 1)$, $(0.77, 0)$, $(0.65, 1)$, $(0.58, 0)$, $(0.41, 1)$, $(0.36, 0)$, $(0.24, 0)$, $(0.18, 1)$, $(0.12, 0)$, $(0.05, 0)$.\nThe bins are $I_1 = [0, 0.2)$, $I_2 = [0.2, 0.4)$, $I_3 = [0.4, 0.6)$, $I_4 = [0.6, 0.8)$, and $I_5 = [0.8, 1.0]$.\n\n**Binning the Data:**\n*   **Bin 1: $I_1 = [0, 0.2)$**\n    Samples: $(0.18, 1), (0.12, 0), (0.05, 0)$.\n    $N_1 = 3$.\n    $\\widehat{\\mathrm{acc}}(I_1) = \\frac{1+0+0}{3} = \\frac{1}{3}$.\n    $\\widehat{\\mathrm{conf}}(I_1) = \\frac{0.18+0.12+0.05}{3} = \\frac{0.35}{3}$.\n*   **Bin 2: $I_2 = [0.2, 0.4)$**\n    Samples: $(0.36, 0), (0.24, 0)$.\n    $N_2 = 2$.\n    $\\widehat{\\mathrm{acc}}(I_2) = \\frac{0+0}{2} = 0$.\n    $\\widehat{\\mathrm{conf}}(I_2) = \\frac{0.36+0.24}{2} = \\frac{0.60}{2} = 0.3$.\n*   **Bin 3: $I_3 = [0.4, 0.6)$**\n    Samples: $(0.58, 0), (0.41, 1)$.\n    $N_3 = 2$.\n    $\\widehat{\\mathrm{acc}}(I_3) = \\frac{0+1}{2} = 0.5$.\n    $\\widehat{\\mathrm{conf}}(I_3) = \\frac{0.58+0.41}{2} = \\frac{0.99}{2} = 0.495$.\n*   **Bin 4: $I_4 = [0.6, 0.8)$**\n    Samples: $(0.77, 0), (0.65, 1)$.\n    $N_4 = 2$.\n    $\\widehat{\\mathrm{acc}}(I_4) = \\frac{0+1}{2} = 0.5$.\n    $\\widehat{\\mathrm{conf}}(I_4) = \\frac{0.77+0.65}{2} = \\frac{1.42}{2} = 0.71$.\n*   **Bin 5: $I_5 = [0.8, 1.0]$**\n    Samples: $(0.92, 0), (0.88, 1), (0.83, 1)$.\n    $N_5 = 3$.\n    $\\widehat{\\mathrm{acc}}(I_5) = \\frac{0+1+1}{3} = \\frac{2}{3}$.\n    $\\widehat{\\mathrm{conf}}(I_5) = \\frac{0.92+0.88+0.83}{3} = \\frac{2.63}{3}$.\n\n**Expected Calibration Error (ECE):**\n$$\n\\widehat{\\mathrm{ECE}} = \\sum_{m=1}^{5} \\frac{N_m}{N} |\\widehat{\\mathrm{acc}}(I_m) - \\widehat{\\mathrm{conf}}(I_m)|\n$$\n$$\n\\widehat{\\mathrm{ECE}} = \\frac{3}{12}\\left|\\frac{1}{3} - \\frac{0.35}{3}\\right| + \\frac{2}{12}|0 - 0.3| + \\frac{2}{12}|0.5 - 0.495| + \\frac{2}{12}|0.5 - 0.71| + \\frac{3}{12}\\left|\\frac{2}{3} - \\frac{2.63}{3}\\right|\n$$\n$$\n\\widehat{\\mathrm{ECE}} = \\frac{1}{12} \\left( 3 \\cdot \\frac{|1-0.35|}{3} + 2 \\cdot 0.3 + 2 \\cdot 0.005 + 2 \\cdot 0.21 + 3 \\cdot \\frac{|2-2.63|}{3} \\right)\n$$\n$$\n\\widehat{\\mathrm{ECE}} = \\frac{1}{12} \\left( 0.65 + 0.6 + 0.01 + 0.42 + 0.63 \\right) = \\frac{2.31}{12} = 0.1925\n$$\n\n**Brier Score:**\n$$\n\\mathrm{Brier} = \\frac{1}{12} \\sum_{i=1}^{12} (p_i - y_i)^2\n$$\n$$\n\\mathrm{Brier} = \\frac{1}{12} \\Big[ (0.92-0)^2 + (0.88-1)^2 + (0.83-1)^2 + (0.77-0)^2 + (0.65-1)^2 + (0.58-0)^2 + (0.41-1)^2 + (0.36-0)^2 + (0.24-0)^2 + (0.18-1)^2 + (0.12-0)^2 + (0.05-0)^2 \\Big]\n$$\n$$\n\\mathrm{Brier} = \\frac{1}{12} \\Big[ 0.8464 + 0.0144 + 0.0289 + 0.5929 + 0.1225 + 0.3364 + 0.3481 + 0.1296 + 0.0576 + 0.6724 + 0.0144 + 0.0025 \\Big]\n$$\n$$\n\\mathrm{Brier} = \\frac{3.1661}{12} \\approx 0.26384166...\n$$\nRounded to four significant figures, the Brier score is $0.2638$.\n\n### Part 3: Miscalibration and Clinical Utility\n\nThe Area Under the Receiver Operating Characteristic Curve (AUC-ROC) is a measure of a model's ability to discriminate between classes. It is calculated by integrating the True Positive Rate (TPR) over all False Positive Rate (FPR) values, which is equivalent to the probability that a randomly chosen positive sample is ranked higher than a randomly chosen negative sample. Consequently, AUC-ROC is invariant to any strictly increasing monotonic transformation of the model's output scores, as such a transformation preserves the rank ordering of samples.\n\nClinical decision utility, however, is not solely dependent on ranking. It depends on minimizing the expected cost of decisions. Following Bayes decision theory, we define costs for misclassification: $c_{01}$ for a false positive (predicting 1 when the truth is 0) and $c_{10}$ for a false negative (predicting 0 when the truth is 1). For a given case with features $X$, the decision to predict \"pneumonia\" ($Y=1$) or \"no pneumonia\" ($Y=0$) is based on minimizing the expected cost.\n\nLet $r(X) = \\mathbb{P}(Y=1 \\mid X)$ be the true conditional risk.\nThe expected cost of predicting $Y=1$ is $C_1(X) = c_{01} \\cdot \\mathbb{P}(Y=0 \\mid X) = c_{01}(1-r(X))$.\nThe expected cost of predicting $Y=0$ is $C_0(X) = c_{10} \\cdot \\mathbb{P}(Y=1 \\mid X) = c_{10} r(X)$.\n\nThe Bayes-optimal decision rule is to predict $Y=1$ if $C_1(X) < C_0(X)$, which is:\n$$\nc_{01}(1-r(X)) < c_{10} r(X) \\implies c_{01} < (c_{10} + c_{01}) r(X) \\implies r(X) > \\frac{c_{01}}{c_{01} + c_{10}}\n$$\nThis defines an optimal decision threshold $\\tau^* = \\frac{c_{01}}{c_{01} + c_{10}}$ on the true risk $r(X)$. Since we do not know $r(X)$, we use the model's output $p(X)$ as an estimate. The practical decision rule becomes: predict $Y=1$ if $p(X) > \\tau^*$. This rule is optimal only if the model is perfectly calibrated, i.e., $p(X) = r(X)$.\n\nWhen a model is transferred to a new domain, it may become miscalibrated due to domain shift. This means the relationship between the predicted probability and the true risk is distorted, e.g., $p(X) = f(r(X))$ where $f(z) \\ne z$. For instance, the model might become systematically overconfident, so $p(X) > r(X)$. Even if this distortion $f$ is monotonic (preserving the ranking of cases and thus the AUC-ROC), the decision utility is compromised.\n\nBy applying the threshold $\\tau^*$ to the miscalibrated scores $p(X)$, the decision rule is $p(X) > \\tau^*$, which is equivalent to $f(r(X)) > \\tau^*$. Assuming $f$ is invertible, this becomes $r(X) > f^{-1}(\\tau^*)$. The effective threshold being applied to the true risk is $\\tau_{eff} = f^{-1}(\\tau^*)$.\n\nIf the model is miscalibrated, $f(z) \\ne z$, which implies $f^{-1}(z) \\ne z$, and therefore $\\tau_{eff} \\ne \\tau^*$. We are now operating at a suboptimal point on the ROC curve, meaning our decision rule is no longer minimizing the expected cost. For example, if the model is overconfident ($f(z) > z$), then $f^{-1}(z) < z$, leading to $\\tau_{eff} < \\tau^*$. This means we are being too aggressive in predicting pneumonia, leading to more false positives and a higher total expected cost than is optimal.\n\nIn conclusion, even if a model maintains a high AUC-ROC after transfer (good ranking), its miscalibration causes a mismatch between the cost-optimal decision threshold and the model's output scale. Applying the theoretically optimal threshold to miscalibrated probabilities leads to a suboptimal decision policy, thereby reducing the model's expected clinical utility.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1925 & 0.2638\n\\end{pmatrix}\n}\n$$"
        }
    ]
}