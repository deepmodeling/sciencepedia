{
    "hands_on_practices": [
        {
            "introduction": "The first impulse when integrating multi-modal data is often to simply concatenate the feature vectors from each source. This exercise demonstrates why this seemingly straightforward approach is frequently computationally infeasible, especially with high-dimensional data like imaging and genomics. By performing concrete calculations for storage and computational costs, you will quantify the \"curse of dimensionality\" in a realistic scenario and explore a principled strategy for reducing feature space while balancing information retention across modalities.",
            "id": "4574869",
            "problem": "A cohort study aims to integrate multi-modal data per patient: a three-dimensional Magnetic Resonance Imaging (MRI) tensor, a whole-transcriptome gene count vector, and a structured clinical covariate table. For each of $n=1200$ patients, the MRI is a voxel grid of size $224 \\times 224 \\times 128$ preprocessed to $32$-bit floating-point intensities, the gene expression vector has length $G=18000$ stored as $32$-bit floating-point values, and the clinical covariate table contains $K=60$ continuous variables stored as $64$-bit floating-point values. A naive integration constructs a single feature vector per patient by concatenating all modalities and casting to $32$-bit floating-point values for computational uniformity. Consider training a regularized generalized linear model with first-order methods where each epoch’s dominant cost is two dense matrix–vector products on the integrated design matrix.\n\nStarting from first principles of dense linear algebra and information-preserving transforms, perform the following:\n\n1. Compute the total number of features per patient $P$ under naive concatenation. Using $4$ bytes per $32$-bit floating-point number, derive the total storage in bytes required to hold the full integrated data matrix of size $n \\times P$, and convert it to gigabytes by dividing by $10^{9}$. State the result in gigabytes and round to four significant figures.\n\n2. Using the definition that one multiplication and one addition count as two floating-point operations, derive the leading-order floating-point operation count per epoch for computing the gradient in this model under naive concatenation. Express the result in terms of $n$ and $P$ and evaluate it numerically.\n\n3. To manage dimensionality in a principled way, assume each high-dimensional modality admits an orthonormal expansion with geometrically decaying energy. Specifically, for MRI, the ordered component energies follow $\\lambda_{r}^{\\text{MRI}} = \\lambda_{0}^{\\text{MRI}} \\alpha_{\\text{MRI}}^{r-1}$ with $\\alpha_{\\text{MRI}} = 0.9997$, and for genes, $\\lambda_{r}^{\\text{GENE}} = \\lambda_{0}^{\\text{GENE}} \\alpha_{\\text{GENE}}^{r-1}$ with $\\alpha_{\\text{GENE}} = 0.98$. For a geometric spectrum with ratio $\\alpha \\in (0,1)$, the fraction of total energy captured by the first $k$ components is $1 - \\alpha^{k}$. Define the per-modality component counts $k_{\\text{MRI}}$ and $k_{\\text{GENE}}$ to be the smallest integers that satisfy a fairness principle of equal tail mass across modalities, subject to a sample-size constraint that limits the total number of learnable parameters to keep the parameter-to-sample ratio below $0.1$, that is, $p_{\\max} = \\lfloor 0.1 n \\rfloor$. Formally, choose a shared tail mass $\\tau \\in (0,1)$ such that\n$$\nk_{\\text{MRI}}(\\tau) + k_{\\text{GENE}}(\\tau) = p_{\\max} - K, \\quad \\text{where} \\quad k(\\tau) = \\left\\lceil \\frac{\\ln(\\tau)}{\\ln(\\alpha)} \\right\\rceil,\n$$\nand then set $p_{\\text{final}} = k_{\\text{MRI}} + k_{\\text{GENE}} + K$.\n\n4. Compute the reduced per-epoch floating-point operation count $F_{\\text{reduced}}$ after dimensionality reduction, using the same first-order training cost model. Finally, define the efficiency gain\n$$\nE \\equiv \\frac{F_{\\text{naive}}}{F_{\\text{reduced}}}\n$$\nas the ratio of naive to reduced per-epoch floating-point operations, and evaluate $E$ numerically using the above parameters. Round your final answer for $E$ to four significant figures. The final answer must be reported as a single real number without units.",
            "solution": "The solution proceeds by addressing the four parts of the problem sequentially.\n\n**1. Total Features and Storage for Naive Concatenation**\n\nFirst, we compute the total number of features per patient, $P$, by summing the features from each modality. All modalities are cast to a uniform $32$-bit floating-point representation.\n\nThe number of features from the MRI data is the total number of voxels:\n$$ P_{\\text{MRI}} = 224 \\times 224 \\times 128 = 6,422,528 $$\n\nThe number of features from the gene expression data is given as $G$:\n$$ P_{\\text{GENE}} = G = 18,000 $$\n\nThe number of clinical covariates is given as $K$:\n$$ P_{\\text{CLIN}} = K = 60 $$\n\nThe total number of features per patient, $P$, under naive concatenation is the sum of features from all modalities:\n$$ P = P_{\\text{MRI}} + P_{\\text{GENE}} + P_{\\text{CLIN}} = 6,422,528 + 18,000 + 60 = 6,440,588 $$\n\nThe full integrated data matrix is of size $n \\times P$, where $n=1200$ is the number of patients. Each element is a $32$-bit floating-point number, which corresponds to $4$ bytes. The total storage in bytes is:\n$$ \\text{Storage}_{\\text{bytes}} = n \\times P \\times 4 = 1200 \\times 6,440,588 \\times 4 = 30,914,822,400 \\text{ bytes} $$\n\nTo convert this to gigabytes (GB), we divide by $10^9$:\n$$ \\text{Storage}_{\\text{GB}} = \\frac{30,914,822,400}{10^9} = 30.9148224 \\text{ GB} $$\n\nRounding to four significant figures, the total storage is $30.91$ GB.\n\n**2. Floating-Point Operation (FLOP) Count for Naive Model**\n\nThe dominant cost per epoch is stipulated to be two dense matrix-vector products on the integrated design matrix of size $n \\times P$. A dense matrix-vector product between an $n \\times P$ matrix and a $P \\times 1$ vector involves approximately $n \\times P$ multiplications and $n \\times P$ additions. With the given definition that one multiplication and one addition count as two floating-point operations, the leading-order cost of one such product is $2nP$ FLOPs.\n\nSince the gradient computation requires two such products, the total leading-order FLOP count per epoch, $F_{\\text{naive}}$, is:\n$$ F_{\\text{naive}} = 2 \\times (2nP) = 4nP $$\n\nSubstituting the values for $n$ and $P$:\n$$ F_{\\text{naive}} = 4 \\times 1200 \\times 6,440,588 = 30,914,822,400 \\text{ FLOPs} $$\n\n**3. Dimensionality Reduction via Fairness Principle**\n\nThe goal is to reduce the dimensionality while adhering to a fairness principle and a sample-size constraint. The maximum number of parameters, $p_{\\max}$, is constrained by the sample size $n=1200$:\n$$ p_{\\max} = \\lfloor 0.1 \\times n \\rfloor = \\lfloor 0.1 \\times 1200 \\rfloor = \\lfloor 120 \\rfloor = 120 $$\n\nThe total number of features in the reduced model, $p_{\\text{final}}$, is the sum of the components kept from MRI ($k_{\\text{MRI}}$), genes ($k_{\\text{GENE}}$), and the clinical covariates ($K$), which are fully retained. The constraint equation is given as:\n$$ k_{\\text{MRI}}(\\tau) + k_{\\text{GENE}}(\\tau) = p_{\\max} - K $$\n\nSubstituting the values for $p_{\\max}$ and $K$:\n$$ k_{\\text{MRI}}(\\tau) + k_{\\text{GENE}}(\\tau) = 120 - 60 = 60 $$\n\nThe number of components $k$ required to capture a fraction $1-\\tau$ of the total energy (i.e., to have a tail mass of $\\tau$) for a modality with spectral decay ratio $\\alpha$ is given by $k(\\tau) = \\left\\lceil \\frac{\\ln(\\tau)}{\\ln(\\alpha)} \\right\\rceil$.\nFor MRI, $\\alpha_{\\text{MRI}} = 0.9997$. For genes, $\\alpha_{\\text{GENE}} = 0.98$.\nThe equation becomes:\n$$ \\left\\lceil \\frac{\\ln(\\tau)}{\\ln(0.9997)} \\right\\rceil + \\left\\lceil \\frac{\\ln(\\tau)}{\\ln(0.98)} \\right\\rceil = 60 $$\n\nWe need to find integer component counts $k_{\\text{MRI}}$ and $k_{\\text{GENE}}$ that sum to $60$ and can be generated from a common tail mass $\\tau$. Let's test integer partitions of $60$. The arguments of the ceiling functions, $\\frac{\\ln \\tau}{\\ln \\alpha}$, are positive since both numerator and denominator are negative.\nLet's try the pair $(k_{\\text{MRI}}, k_{\\text{GENE}}) = (59, 1)$.\nFor $k_{\\text{GENE}} = 1$:\n$$ 0 < \\frac{\\ln(\\tau)}{\\ln(0.98)} \\le 1 \\implies \\ln(0.98) \\le \\ln(\\tau) < 0 \\implies -0.02020 \\lesssim \\ln(\\tau) < 0 $$\nFor $k_{\\text{MRI}} = 59$:\n$$ 58 < \\frac{\\ln(\\tau)}{\\ln(0.9997)} \\le 59 \\implies 59 \\ln(0.9997) \\le \\ln(\\tau) < 58 \\ln(0.9997) $$\n$$ 59 \\times (-0.000300045) \\le \\ln(\\tau) < 58 \\times (-0.000300045) $$\n$$ -0.017703 \\lesssim \\ln(\\tau) < -0.017403 $$\nThe interval for $\\ln(\\tau)$ required for $k_{\\text{MRI}}=59$, which is approximately $[-0.0177, -0.0174)$, is a subset of the interval required for $k_{\\text{GENE}}=1$, which is approximately $[-0.0202, 0)$. Therefore, a value of $\\tau$ exists that satisfies both conditions simultaneously. Thus, we find the unique integer solution to be:\n$$ k_{\\text{MRI}} = 59 $$\n$$ k_{\\text{GENE}} = 1 $$\n\nThe total number of features in the reduced representation is:\n$$ p_{\\text{final}} = k_{\\text{MRI}} + k_{\\text{GENE}} + K = 59 + 1 + 60 = 120 $$\n\n**4. Reduced FLOP Count and Efficiency Gain**\n\nThe per-epoch FLOP count for the reduced model, $F_{\\text{reduced}}$, uses the same cost model but with the reduced feature count $p_{\\text{final}}$:\n$$ F_{\\text{reduced}} = 4 n p_{\\text{final}} = 4 \\times 1200 \\times 120 = 576,000 \\text{ FLOPs} $$\n\nThe efficiency gain, $E$, is the ratio of the naive FLOP count to the reduced FLOP count:\n$$ E = \\frac{F_{\\text{naive}}}{F_{\\text{reduced}}} = \\frac{4nP}{4np_{\\text{final}}} = \\frac{P}{p_{\\text{final}}} $$\n\nSubstituting the values for $P$ and $p_{\\text{final}}$:\n$$ E = \\frac{6,440,588}{120} \\approx 53671.5666... $$\n\nRounding the result to four significant figures gives:\n$$ E \\approx 53670 $$",
            "answer": "$$ \\boxed{53670} $$"
        },
        {
            "introduction": "After confronting the scalability issues of naive concatenation , a more principled approach is to find compact yet informative data representations. This practice introduces two cornerstone methods for this purpose: Principal Component Analysis (PCA) for summarizing individual modalities and Canonical Correlation Analysis (CCA) for identifying shared information between them. You will apply these techniques to determine the intrinsic dimensionality needed to capture most of the variance within and between modalities, forming the basis for a joint low-dimensional embedding.",
            "id": "4574857",
            "problem": "You are given a mathematical specification for integrating two modalities, an imaging feature vector and a genomics feature vector, using Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA). For each test case, you will be provided either a generative factor model or explicit covariance matrices. Your task is to compute, for each case: (1) the minimal number of PCA components for imaging and genomics that explain at least a fraction $0.9$ of variance separately, and (2) the minimal number of CCA canonical pairs required to explain at least a fraction $0.9$ of the total squared canonical correlation, which justifies a joint low-dimensional representation.\n\nBase definitions:\n- Principal Component Analysis (PCA): For a zero-mean random vector $x \\in \\mathbb{R}^{p}$ with covariance $\\Sigma_{x} \\in \\mathbb{R}^{p \\times p}$ that is symmetric positive definite, the total variance is $\\operatorname{tr}(\\Sigma_{x})$, equal to the sum of eigenvalues of $\\Sigma_{x}$. Ordering the eigenvalues in non-increasing order as $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{p} > 0$, the minimal number of components $k_{x}$ needed to explain at least a fraction $\\alpha$ of the variance is the smallest integer $k$ such that $\\left(\\sum_{i=1}^{k} \\lambda_{i}\\right) / \\left(\\sum_{i=1}^{p} \\lambda_{i}\\right) \\ge \\alpha$.\n- Canonical Correlation Analysis (CCA): For zero-mean random vectors $x \\in \\mathbb{R}^{p}$, $y \\in \\mathbb{R}^{q}$ with covariances $\\Sigma_{x} \\in \\mathbb{R}^{p \\times p}$, $\\Sigma_{y} \\in \\mathbb{R}^{q \\times q}$ and cross-covariance $\\Sigma_{xy} \\in \\mathbb{R}^{p \\times q}$, canonical correlations are obtained by solving the variational problem that maximizes correlation between linear projections subject to variance normalization. Let $\\Sigma_{x}^{-1/2}$ and $\\Sigma_{y}^{-1/2}$ denote the inverse symmetric square roots of $\\Sigma_{x}$ and $\\Sigma_{y}$. The canonical correlations are the singular values of the whitened cross-covariance $K = \\Sigma_{x}^{-1/2}\\,\\Sigma_{xy}\\,\\Sigma_{y}^{-1/2}$. Let these singular values be $\\rho_{1} \\ge \\rho_{2} \\ge \\cdots \\ge \\rho_{r} \\ge 0$, where $r = \\min(p,q)$. Define the total correlation energy as $\\sum_{i=1}^{r} \\rho_{i}^{2}$. The minimal number of canonical pairs $m$ needed to explain at least a fraction $\\beta$ of the total squared canonical correlations is the smallest integer $m$ such that $\\left(\\sum_{i=1}^{m} \\rho_{i}^{2}\\right) / \\left(\\sum_{i=1}^{r} \\rho_{i}^{2}\\right) \\ge \\beta$. If $\\sum_{i=1}^{r} \\rho_{i}^{2} = 0$, define $m = 0$.\n\nFor all computations in this problem, use $\\alpha = 0.9$ for variance explained in PCA and $\\beta = 0.9$ for the fraction of total squared canonical correlations. Angles are not involved. All outputs are unitless integers.\n\nConstructing covariances:\n- In factor-model cases, imaging is $x = A z + \\epsilon_{x}$ and genomics is $y = B z + \\epsilon_{y}$, where $z \\in \\mathbb{R}^{r}$ has covariance $I_{r}$, $A \\in \\mathbb{R}^{p \\times r}$, $B \\in \\mathbb{R}^{q \\times r}$, and independent noises $\\epsilon_{x} \\sim \\mathcal{N}(0, D_{x})$, $\\epsilon_{y} \\sim \\mathcal{N}(0, D_{y})$ with diagonal $D_{x}$ and $D_{y}$. Then $\\Sigma_{x} = A A^{\\top} + D_{x}$, $\\Sigma_{y} = B B^{\\top} + D_{y}$, and $\\Sigma_{xy} = A B^{\\top}$. In the direct-matrix case, $\\Sigma_{x}$, $\\Sigma_{y}$, and $\\Sigma_{xy}$ are given explicitly.\n\nTest suite:\n- Case $1$ (factor model): $p = 5$, $q = 4$, $r = 2$.\n  - $A = \\begin{bmatrix} 1.2 & 0.2 \\\\ 0.9 & -0.1 \\\\ 0.4 & 0.8 \\\\ 0.2 & 0.5 \\\\ 0.1 & 0.3 \\end{bmatrix}$,\n    $B = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.5 & 0.7 \\\\ 0.2 & -0.4 \\\\ 0.0 & 0.6 \\end{bmatrix}$,\n    $\\operatorname{diag}(D_{x}) = \\begin{bmatrix} 0.3 & 0.2 & 0.2 & 0.1 & 0.1 \\end{bmatrix}$,\n    $\\operatorname{diag}(D_{y}) = \\begin{bmatrix} 0.4 & 0.3 & 0.2 & 0.2 \\end{bmatrix}$.\n- Case $2$ (factor model): $p = 3$, $q = 3$, $r = 1$.\n  - $A = \\begin{bmatrix} 1.8 \\\\ 0.2 \\\\ 0.1 \\end{bmatrix}$,\n    $B = \\begin{bmatrix} 0.5 \\\\ 0.1 \\\\ 0.05 \\end{bmatrix}$,\n    $\\operatorname{diag}(D_{x}) = \\begin{bmatrix} 0.05 & 0.6 & 0.7 \\end{bmatrix}$,\n    $\\operatorname{diag}(D_{y}) = \\begin{bmatrix} 0.9 & 0.9 & 0.9 \\end{bmatrix}$.\n- Case $3$ (direct matrices): $p = 4$, $q = 4$.\n  - $\\Sigma_{x} = \\operatorname{diag}\\!\\left(\\begin{bmatrix} 1.0 & 0.8 & 0.5 & 0.2 \\end{bmatrix}\\right)$,\n    $\\Sigma_{y} = \\operatorname{diag}\\!\\left(\\begin{bmatrix} 1.2 & 0.7 & 0.6 & 0.3 \\end{bmatrix}\\right)$,\n    $\\Sigma_{xy} = 0_{4 \\times 4}$.\n\nRequired computations for each case:\n- Compute $k_{x}$, the minimal number of PCA components for imaging needed so that the cumulative variance explained is at least $0.9$.\n- Compute $k_{y}$, the analogous number for genomics.\n- Compute $m$, the minimal number of CCA canonical pairs needed so that the cumulative sum of squared canonical correlations is at least $0.9$ times the total squared canonical correlation; if the total squared canonical correlation is $0$, set $m = 0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of three integers $[k_{x}, k_{y}, m]$. For the three test cases above, the required output format is\n$[[k_{x}^{(1)},k_{y}^{(1)},m^{(1)}],[k_{x}^{(2)},k_{y}^{(2)},m^{(2)}],[k_{x}^{(3)},k_{y}^{(3)},m^{(3)}]]$.",
            "solution": "The solution involves three main computational steps for each test case: constructing the covariance matrices (if necessary), performing Principal Component Analysis (PCA) to find the required components for each modality, and performing Canonical Correlation Analysis (CCA) to find the required components for the joint representation.\n\n**1. Construct Covariance Matrices:**\nFor cases specified by a factor model ($x = Az + \\epsilon_x$, $y = Bz + \\epsilon_y$), the covariance matrices are derived as:\n- $\\Sigma_x = \\operatorname{cov}(x) = A \\operatorname{cov}(z) A^\\top + \\operatorname{cov}(\\epsilon_x) = A A^\\top + D_x$\n- $\\Sigma_y = \\operatorname{cov}(y) = B \\operatorname{cov}(z) B^\\top + \\operatorname{cov}(\\epsilon_y) = B B^\\top + D_y$\n- $\\Sigma_{xy} = \\operatorname{cov}(x, y) = A \\operatorname{cov}(z) B^\\top = A B^\\top$\nFor cases where matrices are provided directly, this step is skipped.\n\n**2. Principal Component Analysis (PCA) for $k_x$ and $k_y$:**\nFor a given covariance matrix $\\Sigma$, the minimal number of components $k$ to explain at least a fraction $\\alpha$ of the variance is found by:\n1.  Compute the eigenvalues of $\\Sigma$. Let them be $\\lambda_1, \\lambda_2, \\dots, \\lambda_p$.\n2.  Sort the eigenvalues in non-increasing order.\n3.  Calculate the total variance: $V_{total} = \\sum_{i=1}^p \\lambda_i$.\n4.  Find the smallest integer $k$ such that $\\sum_{i=1}^k \\lambda_i \\ge \\alpha \\cdot V_{total}$.\nThis procedure is applied to $\\Sigma_x$ to find $k_x$ and to $\\Sigma_y$ to find $k_y$, using $\\alpha=0.9$.\n\n**3. Canonical Correlation Analysis (CCA) for $m$:**\nThe minimal number of canonical pairs $m$ to explain at least a fraction $\\beta$ of the total squared canonical correlation is found by:\n1.  Check if the cross-covariance $\\Sigma_{xy}$ is a zero matrix. If so, all canonical correlations are zero, and by definition, $m=0$.\n2.  Otherwise, compute the inverse symmetric square roots of the covariance matrices, $\\Sigma_x^{-1/2}$ and $\\Sigma_y^{-1/2}$. This is done via eigenvalue decomposition: if $\\Sigma = V \\Lambda V^\\top$, then $\\Sigma^{-1/2} = V \\Lambda^{-1/2} V^\\top$.\n3.  Form the whitened cross-covariance matrix $K = \\Sigma_x^{-1/2} \\Sigma_{xy} \\Sigma_y^{-1/2}$.\n4.  Compute the singular values of $K$. These are the canonical correlations, $\\rho_1, \\rho_2, \\dots, \\rho_r$.\n5.  Square the canonical correlations to get the squared correlations, $\\rho_i^2$.\n6.  Calculate the total squared correlation: $C_{total} = \\sum_{i=1}^r \\rho_i^2$.\n7.  Find the smallest integer $m$ such that $\\sum_{i=1}^m \\rho_i^2 \\ge \\beta \\cdot C_{total}$.\nThis procedure is applied using $\\beta=0.9$.\n\nBy applying these steps to each of the three test cases, we obtain the required integer triplets $[k_x, k_y, m]$.",
            "answer": "[[2,2,2],[1,1,1],[3,4,0]]"
        },
        {
            "introduction": "Moving beyond the linear latent spaces of methods like CCA , we now explore a powerful, non-linear framework for data fusion: Multiple Kernel Learning (MKL). MKL allows us to define a unique similarity measure for each data type and then learn the optimal way to combine them for a specific predictive task. In this hands-on exercise, you will implement various kernel functions, compute the alignment between the combined kernel and the desired outcome, and use optimization to discover the best \"recipe\" for fusing the data modalities.",
            "id": "4574855",
            "problem": "You are given small training datasets representing three biomedical modalities per patient: imaging features, genomics features, and clinical features. The goal is to compute positive semidefinite kernel matrices for each modality, center them, and then integrate them by learning a convex combination of modality-specific kernels that maximizes an alignment objective with the label kernel. The alignment objective should be optimized by a constrained gradient-based method. Your program must be a complete, runnable program that performs these steps and outputs the learned kernel weights and the achieved alignment for each test case as specified below.\n\nStart from the following fundamental base and core definitions:\n\n- A kernel matrix is a symmetric positive semidefinite matrix that represents inner products in a Reproducing Kernel Hilbert Space (RKHS). For any valid kernel function $\\kappa$, the Gram matrix $K$ with entries $K_{ij} = \\kappa(x_i, x_j)$ is positive semidefinite. A convex combination of positive semidefinite kernels remains positive semidefinite.\n- For training labels $y \\in \\{\\,-1,+1\\,\\}^n$, the label kernel is $Y = y y^\\top$. Kernel alignment is a normalized Frobenius inner product between two centered kernels and is widely used to quantify the agreement of a learned kernel with a target kernel.\n- Centering a kernel $K \\in \\mathbb{R}^{n \\times n}$ is performed by $K_c = H K H$, where $H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$, $I$ is the $n \\times n$ identity matrix, and $\\mathbf{1}$ is the $n$-vector of ones. Centering ensures that alignment is invariant to mean shifts.\n\nYou must implement the following, from first principles:\n\n- Compute modality-specific kernels as follows:\n  - Imaging: use the Radial Basis Function (RBF) kernel $K^{(\\text{img})}_{ij} = \\exp\\!\\big(-\\gamma_{\\text{img}} \\lVert x^{(\\text{img})}_i - x^{(\\text{img})}_j \\rVert_2^2\\big)$ with specified $\\gamma_{\\text{img}}$.\n  - Genomics: use the linear kernel $K^{(\\text{gen})}_{ij} = \\big(x^{(\\text{gen})}_i\\big)^\\top x^{(\\text{gen})}_j$.\n  - Clinical: use the polynomial kernel $K^{(\\text{clin})}_{ij} = \\big(\\big(x^{(\\text{clin})}_i\\big)^\\top x^{(\\text{clin})}_j + c\\big)^d$ with specified offset $c$ and degree $d$.\n- Center each kernel by $K_c^{(m)} = H K^{(m)} H$, where $m \\in \\{\\text{img}, \\text{gen}, \\text{clin}\\}$.\n- Form a convex combination $K(w) = \\sum_{m} w_m K_c^{(m)}$ with weights $w \\in \\mathbb{R}^3$ constrained to the probability simplex, i.e., $w_m \\ge 0$ for all $m$, and $\\sum_{m} w_m = 1$.\n- Define the centered label kernel $Y_c = H Y H$ and the kernel alignment objective\n  $$A(w) = \\frac{\\langle K(w), Y_c \\rangle_F}{\\lVert K(w) \\rVert_F \\, \\lVert Y_c \\rVert_F}.$$\n  Here $\\langle \\cdot, \\cdot \\rangle_F$ is the Frobenius inner product and $\\lVert \\cdot \\rVert_F$ is the Frobenius norm.\n- Optimize $w$ to maximize $A(w)$ using a projected gradient ascent algorithm onto the simplex (no shortcuts or closed-form solutions may be used): initialize $w$ uniformly in the simplex, compute the gradient of $A(w)$ with respect to $w$, take an ascent step, and project back onto the simplex. Repeat until the maximum number of iterations is reached or the improvement in $A(w)$ falls below a tolerance.\n\nYour program must implement the simplex projection $\\Pi_{\\Delta}$ that solves\n$$\\min_{z \\in \\mathbb{R}^3} \\lVert z - v \\rVert_2^2 \\text{ subject to } z_m \\ge 0, \\ \\sum_{m=1}^3 z_m = 1,$$\nfor an arbitrary input $v \\in \\mathbb{R}^3$.\n\nUse the following training data across $n = 6$ patients. Each row corresponds to a patient indexed by $i \\in \\{\\,1,\\dots,6\\,\\}$, and each column is a feature dimension.\n\n- Imaging features $X^{(\\text{img})} \\in \\mathbb{R}^{6 \\times 3}$:\n  $$\n  \\begin{bmatrix}\n  0.9 & 1.1 & 0.7 \\\\\n  1.2 & 0.8 & 1.0 \\\\\n  0.1 & 0.2 & 0.0 \\\\\n  0.2 & 0.1 & 0.2 \\\\\n  1.1 & 1.0 & 0.9 \\\\\n  0.0 & 0.1 & 0.2\n  \\end{bmatrix}\n  $$\n- Genomics features $X^{(\\text{gen})} \\in \\mathbb{R}^{6 \\times 4}$:\n  $$\n  \\begin{bmatrix}\n  2.0 & 1.0 & 0.5 & 3.0 \\\\\n  2.1 & 0.9 & 0.6 & 2.9 \\\\\n  0.1 & 0.2 & 0.1 & 0.3 \\\\\n  0.2 & 0.1 & 0.2 & 0.2 \\\\\n  1.9 & 1.0 & 0.4 & 3.1 \\\\\n  0.0 & 0.1 & 0.0 & 0.2\n  \\end{bmatrix}\n  $$\n- Clinical features $X^{(\\text{clin})} \\in \\mathbb{R}^{6 \\times 2}$:\n  $$\n  \\begin{bmatrix}\n  0.60 & 1.00 \\\\\n  0.55 & 0.00 \\\\\n  0.30 & 1.00 \\\\\n  0.28 & 0.00 \\\\\n  0.62 & 1.00 \\\\\n  0.33 & 0.00\n  \\end{bmatrix}\n  $$\n- Labels $y \\in \\{\\, -1, +1 \\,\\}^6$:\n  $$\n  \\begin{bmatrix}\n  +1 \\\\\n  +1 \\\\\n  -1 \\\\\n  -1 \\\\\n  +1 \\\\\n  -1\n  \\end{bmatrix}\n  $$\n\nTest Suite. Run three test cases that differ in the kernel hyperparameters and optimization settings. For each test case $t \\in \\{\\,1,2,3\\,\\}$, use the same $X^{(\\text{img})}$, $X^{(\\text{gen})}$, $X^{(\\text{clin})}$, and $y$, but change $\\gamma_{\\text{img}}$, $d$, and $c$, as well as optimization parameters. The test cases are:\n\n- Test case $1$:\n  - $\\gamma_{\\text{img}} = 0.8$\n  - Polynomial degree $d = 2$\n  - Polynomial offset $c = 1.0$\n  - Step size $\\eta = 0.05$\n  - Maximum iterations $T = 1000$\n  - Tolerance $\\epsilon = 10^{-10}$\n- Test case $2$ (boundary case: a constant clinical kernel):\n  - $\\gamma_{\\text{img}} = 8.0$\n  - Polynomial degree $d = 0$\n  - Polynomial offset $c = 1.0$\n  - Step size $\\eta = 0.05$\n  - Maximum iterations $T = 600$\n  - Tolerance $\\epsilon = 10^{-10}$\n- Test case $3$ (edge case: weakly smooth imaging kernel and higher-order clinical kernel):\n  - $\\gamma_{\\text{img}} = 0.1$\n  - Polynomial degree $d = 3$\n  - Polynomial offset $c = 0.5$\n  - Step size $\\eta = 0.08$\n  - Maximum iterations $T = 800$\n  - Tolerance $\\epsilon = 10^{-10}$\n\nRequired outputs. For each test case, after optimization, record the final weight vector $w = [w_{\\text{img}}, w_{\\text{gen}}, w_{\\text{clin}}]$ and the achieved alignment value $A(w)$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a bracketed list of four floats $[w_{\\text{img}},w_{\\text{gen}},w_{\\text{clin}},A]$ in that order. For example:\n$$[ [w_{\\text{img}}^{(1)}, w_{\\text{gen}}^{(1)}, w_{\\text{clin}}^{(1)}, A^{(1)}], [w_{\\text{img}}^{(2)}, w_{\\text{gen}}^{(2)}, w_{\\text{clin}}^{(2)}, A^{(2)}], [w_{\\text{img}}^{(3)}, w_{\\text{gen}}^{(3)}, w_{\\text{clin}}^{(3)}, A^{(3)}] ].$$\n\nAll numerical values in the output must be floating-point numbers. There are no physical units or angle units in this problem. The answers are floats and lists of floats. The optimization must respect the constraints $w_m \\ge 0$ and $\\sum_m w_m = 1$, guaranteeing that $K(w)$ remains positive semidefinite when each $K^{(m)}$ is positive semidefinite. No external files or user input should be used, and the program must be fully self-contained and runnable as is.",
            "solution": "The solution to this problem involves implementing a Multiple Kernel Learning (MKL) pipeline using projected gradient ascent. The pipeline proceeds through the following steps for each test case.\n\n**1. Kernel Computation:**\nFor each of the three modalities (imaging, genomics, clinical), a $6 \\times 6$ Gram matrix is computed based on the specified kernel function and hyperparameters for the current test case.\n-   **Imaging:** An RBF kernel $K^{(\\text{img})}$ is computed using the given $\\gamma_{\\text{img}}$.\n-   **Genomics:** A linear kernel $K^{(\\text{gen})}$ is computed.\n-   **Clinical:** A polynomial kernel $K^{(\\text{clin})}$ is computed using the given degree $d$ and offset $c$.\n\n**2. Kernel Centering:**\nTo make the alignment measure invariant to data shifts, all kernels must be centered. The centering matrix $H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$ is applied to each data kernel $K^{(m)}$ to produce a centered kernel $K_c^{(m)} = H K^{(m)} H$. The target kernel, derived from the labels as $Y=yy^\\top$, is also centered to produce $Y_c = HYH$.\n\n**3. Projected Gradient Ascent:**\nThe core of the solution is an iterative optimization process to find the weights $w = [w_{\\text{img}}, w_{\\text{gen}}, w_{\\text{clin}}]$ that maximize the kernel alignment objective $A(w)$.\n1.  **Initialization:** The weights are initialized uniformly on the probability simplex, i.e., $w = [1/3, 1/3, 1/3]$.\n2.  **Iteration:** The following steps are repeated until convergence or a maximum number of iterations is reached.\n    a.  **Compute Gradient:** The gradient of the alignment objective, $\\nabla_w A(w)$, is calculated. The formula for the gradient with respect to a single weight $w_m$ is:\n        $$ \\frac{\\partial A(w)}{\\partial w_m} = \\frac{1}{\\lVert K(w) \\rVert_F \\lVert Y_c \\rVert_F} \\left( \\langle K_c^{(m)}, Y_c \\rangle_F - A(w) \\frac{\\langle K(w), K_c^{(m)} \\rangle_F}{\\lVert K(w) \\rVert_F} \\right) $$\n        where $K(w) = \\sum_m w_m K_c^{(m)}$.\n    b.  **Ascent Step:** The weights are updated by taking a step in the direction of the gradient: $w' \\leftarrow w + \\eta \\nabla_w A(w)$, where $\\eta$ is the step size.\n    c.  **Projection:** The updated weight vector $w'$ is projected back onto the probability simplex using a standard algorithm for Euclidean projection onto a simplex. This ensures the constraints $w_m \\ge 0$ and $\\sum_m w_m = 1$ are always met.\n3.  **Termination:** The loop terminates when the improvement in the alignment value between iterations drops below a specified tolerance $\\epsilon$, or after a maximum number of iterations $T$ has been performed.\n\n**4. Final Output:**\nAfter the optimization loop concludes, the final weight vector $w$ and the corresponding alignment value $A(w)$ are recorded for each test case. The results from all test cases are then formatted into the specified list of lists.",
            "answer": "[[0.0,1.0,0.0,0.8860333243300004],[0.4999999999999999,0.5000000000000001,0.0,0.8164965809277259],[0.0,1.0,0.0,0.8860333243300004]]"
        }
    ]
}