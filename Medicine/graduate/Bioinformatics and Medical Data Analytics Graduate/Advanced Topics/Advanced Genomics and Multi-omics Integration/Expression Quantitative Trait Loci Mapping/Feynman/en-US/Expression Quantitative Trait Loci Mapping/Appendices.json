{
    "hands_on_practices": [
        {
            "introduction": "A primary goal of eQTL mapping is not just to identify associations, but to quantify their strength. This practice delves into the fundamental concept of variance explained ($R^2$), which measures the proportion of a gene's expression variation that can be attributed to a specific genetic variant. By working through a standard additive genetic model, you will derive and calculate $R^2$, providing a crucial link between the statistical effect size ($\\beta$) from a regression and its biological impact.",
            "id": "4562167",
            "problem": "Consider a cis expression Quantitative Trait Locus (cis-eQTL) mapping analysis in which messenger ribonucleic acid (mRNA) expression for a single gene is modeled with an additive genotype effect. Let the individual-level expression be denoted by $y$, measured on a log-transformed, quantile-normalized scale after removing known covariates, and let the genotype dosage be denoted by $g \\in \\{0,1,2\\}$ for the minor allele count. Assume the following:\n- The linear model $y = \\alpha + \\beta g + \\varepsilon$ holds, where $\\varepsilon$ is a random residual independent of $g$, and $\\alpha$ and $\\beta$ are constants.\n- The minor allele frequency is $p = 0.25$.\n- The population is in Hardy–Weinberg Equilibrium (HWE), so $g$ may be treated as a binomial random variable with two trials and success probability $p$.\n- The marginal variance of the expression phenotype (after covariate adjustment) is $\\mathrm{Var}(y) = 0.8$.\n- The ordinary least squares (OLS) estimate of the per-allele effect on expression is $\\beta = 0.3$.\n\nStarting from the law of total variance and the definition of the linear model with independent residuals, derive an analytic expression for the fraction of expression variance explained by genotype, denoted by $R^{2}$, in terms of $\\beta$, $p$, and $\\mathrm{Var}(y)$. Then, using the provided values, compute the numerical value of $R^{2}$. Round your final numeric answer to three significant figures and express it as a decimal number with no units. As part of your reasoning, situate the computed magnitude of $R^{2}$ within typical ranges observed for cis-eQTL effect sizes, but ensure that your final reported answer is only the requested number.",
            "solution": "The problem statement will first be validated against the required criteria.\n\n### Step 1: Extract Givens\n- The model for gene expression $y$ is a linear model with an additive genotype effect: $y = \\alpha + \\beta g + \\varepsilon$.\n- $y$ is the individual-level expression on a log-transformed, quantile-normalized scale after removing known covariates.\n- $g$ is the genotype dosage for the minor allele count, with $g \\in \\{0, 1, 2\\}$.\n- $\\varepsilon$ is a random residual independent of $g$.\n- $\\alpha$ and $\\beta$ are constants.\n- The minor allele frequency is $p = 0.25$.\n- The population is in Hardy–Weinberg Equilibrium (HWE).\n- $g$ is treated as a binomial random variable with two trials and success probability $p$.\n- The marginal variance of the expression phenotype is $\\mathrm{Var}(y) = 0.8$.\n- The ordinary least squares (OLS) estimate of the per-allele effect on expression is $\\beta = 0.3$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated for validity:\n- **Scientifically Grounded:** The problem is firmly grounded in the principles of statistical genetics and bioinformatics, specifically in the context of expression Quantitative Trait Locus (eQTL) analysis. The additive linear model, Hardy-Weinberg Equilibrium, and the concept of variance explained are standard components of this field. The given values for minor allele frequency ($p=0.25$), effect size ($\\beta=0.3$), and total expression variance ($\\mathrm{Var}(y)=0.8$) are realistic for cis-eQTLs.\n- **Well-Posed:** The problem is well-posed. It provides all necessary information and clear definitions to derive a unique, meaningful solution. The question is unambiguous.\n- **Objective:** The problem is stated in precise, objective language, free of subjective claims or bias.\n- **Completeness and Consistency:** The setup is self-contained and internally consistent. No information is missing or contradictory.\n- **Feasibility:** The scenario described is not only scientifically plausible but represents a standard, simplified model used in real-world genetic data analysis.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe objective is to derive an expression for the fraction of expression variance explained by genotype, $R^2$, and then to compute its numerical value.\n\nThe fraction of variance explained, $R^2$, is defined as the ratio of the variance explained by the model's predictor variable (genotype $g$) to the total variance of the outcome variable (expression $y$).\n$$\nR^2 = \\frac{\\mathrm{Var}_{\\text{explained}}}{\\mathrm{Var}_{\\text{total}}} = \\frac{\\mathrm{Var}(\\text{E}[y|g])}{\\mathrm{Var}(y)}\n$$\n\nAs stipulated, we begin with the law of total variance:\n$$\n\\mathrm{Var}(y) = \\mathrm{E}[\\mathrm{Var}(y|g)] + \\mathrm{Var}(\\mathrm{E}[y|g])\n$$\nThis partitions the total variance of $y$ into two components: the expected value of the conditional variance (residual variance) and the variance of the conditional expectation (explained variance).\n\nWe use the given linear model, $y = \\alpha + \\beta g + \\varepsilon$, to determine these components.\nFirst, we find the conditional expectation of $y$ given $g$:\n$$\n\\mathrm{E}[y|g] = \\mathrm{E}[\\alpha + \\beta g + \\varepsilon | g]\n$$\nSince $\\alpha$ and $\\beta$ are constants, and $g$ is known in the conditioning, we have:\n$$\n\\mathrm{E}[y|g] = \\alpha + \\beta g + \\mathrm{E}[\\varepsilon|g]\n$$\nThe problem states that the residual $\\varepsilon$ is independent of $g$. Therefore, $\\mathrm{E}[\\varepsilon|g] = \\mathrm{E}[\\varepsilon]$. In an OLS framework, it is assumed that $\\mathrm{E}[\\varepsilon]=0$. Thus, the conditional expectation is:\n$$\n\\mathrm{E}[y|g] = \\alpha + \\beta g\n$$\nThis term represents the part of the expression level that is predicted by the genotype.\n\nNext, we find the variance of this conditional expectation, which is the variance explained by the genotype:\n$$\n\\mathrm{Var}(\\mathrm{E}[y|g]) = \\mathrm{Var}(\\alpha + \\beta g)\n$$\nSince $\\alpha$ is a constant, it does not contribute to the variance. The variance property $\\mathrm{Var}(cX) = c^2\\mathrm{Var}(X)$ gives:\n$$\n\\mathrm{Var}(\\mathrm{E}[y|g]) = \\mathrm{Var}(\\beta g) = \\beta^2 \\mathrm{Var}(g)\n$$\n\nThe problem states that the population is in Hardy-Weinberg Equilibrium (HWE) and the genotype dosage $g$ can be treated as a binomial random variable with $n=2$ trials (for a diploid organism) and success probability $p$ (the minor allele frequency). The variance of a binomial random variable is given by $np(1-p)$.\nTherefore, the variance of the genotype dosage is:\n$$\n\\mathrm{Var}(g) = 2p(1-p)\n$$\nSubstituting this into the expression for the explained variance yields:\n$$\n\\mathrm{Var}(\\mathrm{E}[y|g]) = \\beta^2 [2p(1-p)] = 2p(1-p)\\beta^2\n$$\n\nNow we return to the definition of $R^2$. We have the expression for the explained variance, and the total variance $\\mathrm{Var}(y)$ is given.\nThe analytical expression for the fraction of variance explained by genotype is:\n$$\nR^2 = \\frac{\\mathrm{Var}(\\mathrm{E}[y|g])}{\\mathrm{Var}(y)} = \\frac{2p(1-p)\\beta^2}{\\mathrm{Var}(y)}\n$$\nThis is the required general formula in terms of $\\beta$, $p$, and $\\mathrm{Var}(y)$.\n\nNow, we substitute the provided numerical values into this expression:\n- $\\beta = 0.3$\n- $p = 0.25$\n- $\\mathrm{Var}(y) = 0.8$\n\nFirst, calculate the variance of the genotype, $\\mathrm{Var}(g)$:\n$$\n\\mathrm{Var}(g) = 2p(1-p) = 2(0.25)(1-0.25) = 2(0.25)(0.75) = 0.5(0.75) = 0.375\n$$\nNext, calculate the variance explained by the genotype:\n$$\n\\mathrm{Var}(\\mathrm{E}[y|g]) = \\beta^2 \\mathrm{Var}(g) = (0.3)^2 \\times 0.375 = 0.09 \\times 0.375 = 0.03375\n$$\nFinally, calculate $R^2$:\n$$\nR^2 = \\frac{0.03375}{\\mathrm{Var}(y)} = \\frac{0.03375}{0.8} = 0.0421875\n$$\nThe problem asks to round the answer to three significant figures. The first significant figure is $4$, the second is $2$, and the third is $1$. The following digit is $8$, which is greater than or equal to $5$, so we round up the third significant digit.\n$$\nR^2 \\approx 0.0422\n$$\n\nThis value, representing $4.22\\%$ of the variance in gene expression explained by a single genetic variant, is a plausible and commonly observed effect size for a strong cis-eQTL. In large-scale studies like the Genotype-Tissue Expression (GTEx) project, it is typical for the most significant cis-eQTL for a given gene (the \"eGene\") to explain between $1\\%$ and $20\\%$ of the expression variance, with some exceptional cases exceeding $50\\%$. An $R^2$ of approximately $4\\%$ is therefore a substantial and realistic effect size within this biological context.",
            "answer": "$$\n\\boxed{0.0422}\n$$"
        },
        {
            "introduction": "Before embarking on an expensive eQTL study, it is essential to ensure the experiment is adequately designed to find the effects you are looking for. This exercise explores the critical concept of statistical power—the probability of detecting a true association. You will derive the key formula that connects sample size ($n$), allele frequency ($p$), and effect size ($\\beta$) to the power of an eQTL analysis, providing the theoretical foundation for study design.",
            "id": "4562175",
            "problem": "Consider a cis-expression quantitative trait loci (cis-eQTL) mapping study where normalized gene expression $Y$ for a single gene is modeled in a cohort of $n$ unrelated individuals using a simple linear regression with an intercept and an additive genotype predictor. Let the genotype $G$ be coded as the minor-allele count $0$, $1$, or $2$. Assume the following data-generating model and design conditions:\n- $Y = \\mu + \\beta G + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ are independent and identically distributed residuals, independent of $G$.\n- The minor allele frequency (MAF) is $p \\in (0,1)$, and the genotype $G$ follows Hardy–Weinberg Equilibrium (HWE).\n- Individuals are independent and identically distributed.\n\nYou will test the null hypothesis $H_{0}: \\beta = 0$ using the usual $t$-statistic for the slope in this two-parameter linear model (intercept and $G$), with a two-sided test at nominal significance level $\\alpha \\in (0,1)$. Starting only from core definitions of linear regression, normal-error models, and HWE genotype distributions, do the following:\n\n1. Derive the noncentrality parameter $\\delta$ of the $t$-statistic under the alternative $H_{1}: \\beta \\neq 0$ in terms of $n$, $p$, $\\sigma^{2}$, and $\\beta$.\n2. Express the statistical power as a closed-form analytic function of $n$, $p$, $\\sigma^{2}$, $\\beta$, and $\\alpha$, using the cumulative distribution function of the noncentral $t$ distribution.\n\nYour final answer must be a single closed-form analytic expression or a single row matrix of analytic expressions. No numerical evaluation is required, and no rounding is needed. Do not include any units in the final answer.",
            "solution": "The user wants to solve a problem in the field of bioinformatics and medical data analytics, specifically on the topic of expression quantitative trait loci mapping.\n\n### Step 1: Extract Givens\n- **Model Equation**: $Y = \\mu + \\beta G + \\varepsilon$.\n- **Response Variable**: $Y$ is the normalized gene expression.\n- **Predictor Variable**: $G$ is the genotype, coded as the minor-allele count $\\{0, 1, 2\\}$.\n- **Error Term**: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ are independent and identically distributed (i.i.d.) residuals, independent of $G$.\n- **Model Parameters**: $\\mu$ is the intercept, $\\beta$ is the effect size of the genotype.\n- **Genetic Model**: Genotype $G$ for $n$ unrelated individuals follows Hardy–Weinberg Equilibrium (HWE) with a minor allele frequency (MAF) of $p \\in (0,1)$. The individuals are i.i.d.\n- **Hypothesis Test**: Null hypothesis $H_0: \\beta = 0$ versus the alternative $H_1: \\beta \\neq 0$.\n- **Test Statistic**: The standard $t$-statistic for the slope in a two-parameter linear model.\n- **Significance Level**: The test is two-sided with a nominal significance level $\\alpha \\in (0,1)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem is well-grounded in statistical genetics. The model ($Y = \\mu + \\beta G + \\varepsilon$), the assumption of HWE, the additive coding of genotypes, and the use of a $t$-test to assess the association are all standard, fundamental concepts in eQTL mapping.\n- **Well-Posed**: The problem is well-posed. It asks for the derivation of two specific, well-defined statistical quantities: the noncentrality parameter of a test statistic and the statistical power. The provided information is sufficient to derive these quantities.\n- **Objective**: The problem is stated using precise, objective, and standard terminology from statistics and genetics. There are no subjective or ambiguous statements.\n- **Completeness**: The problem is self-contained and provides all necessary information (the model, distributional assumptions, genetic parameters, and hypothesis testing framework).\n- **Consistency and Feasibility**: The assumptions are consistent and scientifically plausible within the context of a theoretical statistical genetics problem.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is a standard derivation in statistical genetics that requires a rigorous application of principles from linear regression theory and population genetics. I will proceed with the solution.\n\n***\n\nThe solution requires two main derivations: first, the noncentrality parameter ($\\delta$) of the $t$-statistic, and second, the statistical power of the test.\n\n**1. Derivation of the Noncentrality Parameter ($\\delta$)**\n\nThe model is a simple linear regression: $Y_i = \\mu + \\beta G_i + \\varepsilon_i$ for $i=1, \\dots, n$.\nThe ordinary least squares estimator for the slope $\\beta$ is given by:\n$$ \\hat{\\beta} = \\frac{\\sum_{i=1}^{n} (G_i - \\bar{G})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (G_i - \\bar{G})^2} $$\nwhere $\\bar{G} = \\frac{1}{n} \\sum_{i=1}^{n} G_i$ and $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$.\n\nThe $t$-statistic for testing $H_0: \\beta=0$ is:\n$$ t = \\frac{\\hat{\\beta}}{\\mathrm{se}(\\hat{\\beta})} = \\frac{\\hat{\\beta}}{\\hat{\\sigma} / \\sqrt{\\sum_{i=1}^{n} (G_i - \\bar{G})^2}} $$\nwhere $\\hat{\\sigma}^2$ is the unbiased estimator of the error variance $\\sigma^2$, given by $\\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2$.\n\nUnder the alternative hypothesis $H_1: \\beta \\neq 0$, and conditional on the genotypes $\\mathbf{G} = (G_1, \\dots, G_n)$, the statistic $t$ follows a noncentral $t$-distribution with $n-2$ degrees of freedom. The noncentrality parameter (NCP), $\\delta$, is given by the expected value of the numerator of the standardized test statistic divided by its standard deviation.\nSpecifically, we can write the $t$-statistic as the ratio of a standard normal variable (with a non-zero mean under $H_1$) and the square root of an independent chi-squared variable divided by its degrees of freedom.\nConditional on $\\mathbf{G}$, $\\hat{\\beta} \\sim \\mathcal{N}\\left(\\beta, \\frac{\\sigma^2}{S_{GG}}\\right)$, where $S_{GG} = \\sum_{i=1}^{n} (G_i - \\bar{G})^2$.\nThus, the variable $Z = \\frac{\\hat{\\beta} \\sqrt{S_{GG}}}{\\sigma}$ follows a normal distribution $\\mathcal{N}\\left(\\frac{\\beta \\sqrt{S_{GG}}}{\\sigma}, 1\\right)$.\nThe term $\\frac{(n-2)\\hat{\\sigma}^2}{\\sigma^2}$ follows a $\\chi^2_{n-2}$ distribution, independent of $\\hat{\\beta}$.\nThe $t$-statistic is $t = \\frac{\\hat{\\beta}\\sqrt{S_{GG}}/\\sigma}{\\hat{\\sigma}/\\sigma} = \\frac{Z}{\\sqrt{\\chi^2_{n-2}/(n-2)}}$.\nThe conditional NCP is the mean of $Z$, which is $\\delta(\\mathbf{G}) = \\frac{\\beta \\sqrt{S_{GG}}}{\\sigma}$.\n\nFor the purpose of study design and power calculation, we require a single NCP value that is not conditional on a specific realization of the random genotypes $\\mathbf{G}$. It is standard practice to approximate the random term $S_{GG}$ with its expected value, $E[S_{GG}]$.\n$$ E[S_{GG}] = E\\left[\\sum_{i=1}^{n} (G_i - \\bar{G})^2\\right] = (n-1)\\mathrm{Var}(G) $$\nTo find $\\mathrm{Var}(G)$, we first need the probability distribution, expectation, and variance of the genotype variable $G$ under Hardy-Weinberg Equilibrium (HWE). Given a minor allele frequency $p$:\n- $P(G=0) = (1-p)^2$\n- $P(G=1) = 2p(1-p)$\n- $P(G=2) = p^2$\n\nThe expectation of $G$ is:\n$$ E[G] = 0 \\cdot P(G=0) + 1 \\cdot P(G=1) + 2 \\cdot P(G=2) = 2p(1-p) + 2p^2 = 2p $$\nThe expectation of $G^2$ is:\n$$ E[G^2] = 0^2 \\cdot P(G=0) + 1^2 \\cdot P(G=1) + 2^2 \\cdot P(G=2) = 2p(1-p) + 4p^2 = 2p+2p^2 $$\nThe variance of $G$ is:\n$$ \\mathrm{Var}(G) = E[G^2] - (E[G])^2 = (2p+2p^2) - (2p)^2 = 2p+2p^2-4p^2 = 2p(1-p) $$\nNow, we substitute this variance into the expression for $E[S_{GG}]$:\n$$ E[S_{GG}] = (n-1) \\cdot 2p(1-p) $$\nUsing this expected value in place of $S_{GG}$, we obtain the noncentrality parameter:\n$$ \\delta = \\frac{\\beta \\sqrt{E[S_{GG}]}}{\\sigma} = \\frac{\\beta \\sqrt{2(n-1)p(1-p)}}{\\sigma} $$\nGiven that the problem provides $\\sigma^2$, we can write $\\sigma = \\sqrt{\\sigma^2}$. The sign of $\\delta$ is determined by the sign of $\\beta$.\n\n**2. Derivation of Statistical Power**\n\nStatistical power is the probability of rejecting the null hypothesis $H_0$ when the alternative hypothesis $H_1$ is true.\nFor a two-sided test at significance level $\\alpha$, we reject $H_0$ if $|t| > t_{n-2, 1-\\alpha/2}$, where $t_{n-2, 1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the central Student's $t$-distribution with $n-2$ degrees of freedom.\n\nThe power is the probability of this event, calculated under the distribution of the $t$-statistic when $H_1$ is true. Under $H_1$, the statistic $T = \\frac{\\hat{\\beta}}{\\mathrm{se}(\\hat{\\beta})}$ follows a noncentral $t$-distribution with $n-2$ degrees of freedom and NCP $\\delta$, denoted $T \\sim t_{n-2, \\delta}$.\n\nLet $c = t_{n-2, 1-\\alpha/2}$. The power is:\n$$ \\text{Power} = P(T > c \\text{ or } T < -c | H_1) = P(T > c | H_1) + P(T < -c | H_1) $$\nLet $F_{t_{\\nu, \\delta}}$ denote the cumulative distribution function (CDF) of the noncentral $t$-distribution with $\\nu$ degrees of freedom and NCP $\\delta$.\nThe power can be expressed using this CDF:\n$$ P(T > c | H_1) = 1 - F_{t_{n-2, \\delta}}(c) $$\n$$ P(T < -c | H_1) = F_{t_{n-2, \\delta}}(-c) $$\nCombining these gives the final expression for power:\n$$ \\text{Power} = 1 - F_{t_{n-2, \\delta}}(t_{n-2, 1-\\alpha/2}) + F_{t_{n-2, \\delta}}(-t_{n-2, 1-\\alpha/2}) $$\nThis expression is a function of $n$, $p$, $\\sigma^2$, $\\beta$, and $\\alpha$ as required, since $\\delta$ depends on $n, p, \\beta, \\sigma$ and the critical value $t_{n-2, 1-\\alpha/2}$ depends on $n, \\alpha$. The power formula is an even function of $\\delta$, which is expected for a two-sided test.\n\nThe two requested results are the expression for the noncentrality parameter $\\delta$ and the expression for the statistical power.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{\\beta \\sqrt{2(n-1)p(1-p)}}{\\sigma} & 1 - F_{t_{n-2, \\delta}}(t_{n-2, 1-\\alpha/2}) + F_{t_{n-2, \\delta}}(-t_{n-2, 1-\\alpha/2}) \\end{pmatrix} } $$"
        },
        {
            "introduction": "Modern genetic studies frequently use imputed genotypes to analyze variants not directly measured, introducing a layer of uncertainty. This practice addresses the statistical consequences of this common scenario, exploring how using imputed genotype dosages instead of true genotypes affects eQTL discovery. Through this analysis, you will uncover the relationship between imputation quality, statistical power, and the important concept of an 'effective sample size'.",
            "id": "4562182",
            "problem": "An investigator performs expression quantitative trait loci (eQTL) mapping in a cohort of $n$ individuals. For a single variant with minor allele frequency $p$ under Hardy–Weinberg Equilibrium (HWE), the true genotype count $G_i \\in \\{0,1,2\\}$ is not directly observed. Instead, genotype imputation provides a dosage $D_i = \\mathbb{E}[G_i \\mid \\mathcal{I}_i]$, where $\\mathcal{I}_i$ denotes the imputation information for individual $i$, and a per-individual posterior variance $v_i = \\operatorname{Var}(G_i \\mid \\mathcal{I}_i)$. Gene expression is modeled by a linear regression $Y_i = \\alpha + \\beta G_i + \\varepsilon_i$ with independent and identically distributed errors $\\varepsilon_i$ satisfying $\\mathbb{E}[\\varepsilon_i] = 0$ and $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2$. The analyst, however, fits $Y_i = \\alpha + \\beta D_i + \\varepsilon_i$ using Ordinary Least Squares (OLS).\n\nUsing only first principles such as the law of total variance, the law of total covariance, and the properties of OLS estimators, analyze how imputation uncertainty enters association testing through the dosage variance and its consequences for the $Z$- or Wald statistic. Let $\\operatorname{Var}(G) = 2p(1-p)$ under HWE, and define the imputation quality metric $R^2$ for this variant in terms of population-level quantities and the imputation posterior variance. Based on your derivations, select all statements that are correct:\n\nA. In OLS of $Y$ on $D$, the slope estimator for $\\beta$ is unbiased. Relative to using true genotypes, the expected Wald or $Z$-statistic for testing $H_0:\\beta=0$ is attenuated by a factor $\\sqrt{R^2}$, where $R^2 = 1 - \\mathbb{E}[v_i]/(2p(1-p))$.\n\nB. Modeling dosage uncertainty as $D_i = G_i + \\epsilon_i$ with $\\operatorname{Var}(\\epsilon_i) = v_i$ implies $\\operatorname{Var}(D) = \\operatorname{Var}(G) + \\mathbb{E}[v_i]$, so imputation uncertainty inflates the predictor variance and therefore increases power.\n\nC. The imputation quality metric $R^2$ equals $\\operatorname{Corr}(D,G)$ (not squared), and the association $Z$-statistic is reduced by a factor $R^2$.\n\nD. Filtering variants by an $R^2$ threshold necessarily biases effect estimates toward zero because retained variants have $\\operatorname{Var}(D) < \\operatorname{Var}(G)$.\n\nE. By the law of total variance, $\\operatorname{Var}(G) = \\operatorname{Var}(D) + \\mathbb{E}[v_i]$, which implies $R^2 = \\operatorname{Var}(D)/\\operatorname{Var}(G)$; therefore, tests using dosages have an effective sample size $n_{\\text{eff}} = R^2 n$.\n\nF. Because $R^2$ quantifies the fraction of genotype variance captured by dosages, a principled practice is to filter variants with low $R^2$ (for example, below $0.3$ in large samples or below $0.8$ for rare variants), since they contribute $n_{\\text{eff}} \\approx R^2 n$ and have limited power without improving inference quality.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective before a solution is attempted.\n\n### Step 1: Extract Givens\n-   **Cohort size**: $n$ individuals.\n-   **Variant**: A single variant with minor allele frequency $p$.\n-   **Genetic Model**: Hardy–Weinberg Equilibrium (HWE) is assumed.\n-   **True Genotype**: $G_i \\in \\{0, 1, 2\\}$, which is the unobserved count of the minor allele for individual $i$. From HWE, $\\mathbb{E}[G] = 2p$ and $\\operatorname{Var}(G) = 2p(1-p)$.\n-   **Imputed Dosage**: $D_i = \\mathbb{E}[G_i \\mid \\mathcal{I}_i]$, where $\\mathcal{I}_i$ is the individual-specific imputation information.\n-   **Posterior Variance**: $v_i = \\operatorname{Var}(G_i \\mid \\mathcal{I}_i)$, the uncertainty in the imputed genotype for individual $i$.\n-   **True Phenotype Model**: $Y_i = \\alpha + \\beta G_i + \\varepsilon_i$.\n-   **Error Term**: The errors $\\varepsilon_i$ are independent and identically distributed (i.i.d.) with $\\mathbb{E}[\\varepsilon_i] = 0$ and $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2$. The error $\\varepsilon_i$ is independent of the genetic information $G_i$ and $\\mathcal{I}_i$.\n-   **Fitted Model**: An analyst fits the model $Y_i = \\alpha + \\beta D_i + \\varepsilon_i$ using Ordinary Least Squares (OLS).\n-   **Objective**: Analyze the properties of the OLS estimator for $\\beta$ and the associated Wald ($Z$) statistic from the fitted model, and define an imputation quality metric $R^2$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem describes a standard scenario in statistical genetics and eQTL mapping. Genotype imputation, the use of dosages ($D_i$), posterior variances ($v_i$), and linear regression are all fundamental components of modern genetic association studies. The statistical formulations, including the definitions of $D_i$ and $v_i$ as conditional expectation and variance, are correct. The application of first principles like the laws of total variance and covariance is appropriate. The problem is scientifically and mathematically sound.\n-   **Well-Posed**: The problem is clearly stated with all necessary definitions and assumptions to derive the consequences of using imputed dosages in place of true genotypes. The question is specific and leads to a unique analytical solution.\n-   **Objective**: The language is technical, precise, and free from subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full derivation and evaluation of the options may proceed.\n\n### Derivation of Core Relationships\n\nWe will use population-level quantities, which correspond to the large-sample behavior of their sample-based estimators. The analysis relies on the laws of total expectation, variance, and covariance.\n\n1.  **Relationship between $\\operatorname{Var}(G)$, $\\operatorname{Var}(D)$, and $\\mathbb{E}[v_i]$**:\n    The law of total variance states that for random variables $X$ and $Z$, $\\operatorname{Var}(X) = \\operatorname{Var}(\\mathbb{E}[X \\mid Z]) + \\mathbb{E}[\\operatorname{Var}(X \\mid Z)]$.\n    Let $X=G$ and $Z=\\mathcal{I}$. The quantities in the problem are defined based on conditioning on $\\mathcal{I}_i$ for each individual $i$. We can think of these as population-level expectations over the distribution of individuals and their imputation information $\\mathcal{I}$.\n    -   By definition, $\\mathbb{E}[G \\mid \\mathcal{I}] = D$. Thus, $\\operatorname{Var}(\\mathbb{E}[G \\mid \\mathcal{I}]) = \\operatorname{Var}(D)$.\n    -   By definition, $\\operatorname{Var}(G \\mid \\mathcal{I}) = v$. Thus, $\\mathbb{E}[\\operatorname{Var}(G \\mid \\mathcal{I})] = \\mathbb{E}[v_i]$ (taking the expectation over all individuals).\n    -   Substituting these into the law of total variance gives the fundamental equation:\n        $$ \\operatorname{Var}(G) = \\operatorname{Var}(D) + \\mathbb{E}[v_i] $$\n    This shows that the variance of the imputed dosages is always less than or equal to the variance of the true genotypes: $\\operatorname{Var}(D) = \\operatorname{Var}(G) - \\mathbb{E}[v_i] \\le \\operatorname{Var}(G)$.\n\n2.  **OLS Estimator for $\\beta$**:\n    The analyst regresses $Y$ on $D$. The population OLS slope for this regression is $\\beta_D = \\frac{\\operatorname{Cov}(Y, D)}{\\operatorname{Var}(D)}$. This is the value to which the OLS estimator $\\hat{\\beta}_D$ converges in large samples.\n    -   Let's analyze the covariance term: $\\operatorname{Cov}(Y, D) = \\operatorname{Cov}(\\alpha + \\beta G + \\varepsilon, D) = \\beta \\operatorname{Cov}(G, D) + \\operatorname{Cov}(\\varepsilon, D)$.\n    -   The non-genetic residual $\\varepsilon$ is independent of the genetic imputation process, so $\\operatorname{Cov}(\\varepsilon, D) = 0$.\n    -   Now, we analyze $\\operatorname{Cov}(G, D)$ using the law of total covariance: $\\operatorname{Cov}(X,Y) = \\mathbb{E}[\\operatorname{Cov}(X,Y|Z)] + \\operatorname{Cov}(\\mathbb{E}[X|Z], \\mathbb{E}[Y|Z])$.\n    -   Let $X=G, Y=D, Z=\\mathcal{I}$.\n    -   $\\mathbb{E}[G \\mid \\mathcal{I}]=D$. Since $D$ is a function of $\\mathcal{I}$, it is \"constant\" when conditioned on $\\mathcal{I}$, so $\\mathbb{E}[D \\mid \\mathcal{I}] = D$ and $\\operatorname{Cov}(G, D \\mid \\mathcal{I}) = 0$.\n    -   Therefore, $\\operatorname{Cov}(G, D) = \\operatorname{Cov}(\\mathbb{E}[G \\mid \\mathcal{I}], \\mathbb{E}[D \\mid \\mathcal{I}]) = \\operatorname{Cov}(D, D) = \\operatorname{Var}(D)$.\n    -   Substituting this back into the expression for $\\beta_D$:\n        $$ \\beta_D = \\frac{\\beta \\operatorname{Cov}(G, D)}{\\operatorname{Var}(D)} = \\frac{\\beta \\operatorname{Var}(D)}{\\operatorname{Var}(D)} = \\beta $$\n    This demonstrates that the OLS estimator of the slope, when regressing on imputed dosages defined as conditional expectations, is **unbiased** for the true effect size $\\beta$. This type of model, where $D_i = \\mathbb{E}[G_i | \\mathcal{I}_i]$, is known as a Berkson error model.\n\n3.  **Imputation Quality $R^2$**:\n    The standard measure of imputation quality, denoted $R^2$, is the squared correlation between the true genotype $G$ and the imputed dosage $D$.\n    $$ R^2 = (\\operatorname{Corr}(G, D))^2 = \\frac{(\\operatorname{Cov}(G, D))^2}{\\operatorname{Var}(G) \\operatorname{Var}(D)} $$\n    -   Using our previous result that $\\operatorname{Cov}(G, D) = \\operatorname{Var}(D)$:\n        $$ R^2 = \\frac{(\\operatorname{Var}(D))^2}{\\operatorname{Var}(G) \\operatorname{Var}(D)} = \\frac{\\operatorname{Var}(D)}{\\operatorname{Var}(G)} $$\n    -   This confirms that $R^2$ is the fraction of true genotype variance captured by the imputed dosages.\n    -   Using $\\operatorname{Var}(D) = \\operatorname{Var}(G) - \\mathbb{E}[v_i]$ and the problem's given $\\operatorname{Var}(G) = 2p(1-p)$:\n        $$ R^2 = \\frac{\\operatorname{Var}(G) - \\mathbb{E}[v_i]}{\\operatorname{Var}(G)} = 1 - \\frac{\\mathbb{E}[v_i]}{\\operatorname{Var}(G)} = 1 - \\frac{\\mathbb{E}[v_i]}{2p(1-p)} $$\n\n4.  **Wald ($Z$) Statistic and Power**:\n    The Wald statistic for testing $H_0: \\beta=0$ is $Z = \\hat{\\beta} / \\operatorname{SE}(\\hat{\\beta})$. Power is determined by the non-centrality parameter of the test, which for large $n$ is $\\lambda = \\mathbb{E}[\\hat{\\beta}] / \\operatorname{SE}(\\hat{\\beta})$.\n    -   **With true genotypes ($G$)**: The estimator $\\hat{\\beta}_G$ is unbiased for $\\beta$. The standard error is $\\operatorname{SE}(\\hat{\\beta}_G) \\approx \\sqrt{\\frac{\\sigma^2}{n \\operatorname{Var}(G)}}$. The squared non-centrality parameter is:\n        $$ \\lambda_G^2 = \\frac{\\beta^2}{\\operatorname{Var}(\\hat{\\beta}_G)} \\approx \\frac{n \\beta^2 \\operatorname{Var}(G)}{\\sigma^2} $$\n    -   **With imputed dosages ($D$)**: The estimator $\\hat{\\beta}_D$ is also unbiased for $\\beta$. The standard error used for testing $H_0$ is estimated under the null hypothesis $\\beta=0$. In the fitted model $Y_i = \\alpha + \\beta D_i + u_i$, the error variance under the null is simply $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2$. The predictor variance is $\\operatorname{Var}(D)$. So, the standard error is $\\operatorname{SE}(\\hat{\\beta}_D) \\approx \\sqrt{\\frac{\\sigma^2}{n \\operatorname{Var}(D)}}$. The squared non-centrality parameter is:\n        $$ \\lambda_D^2 = \\frac{\\beta^2}{\\operatorname{Var}(\\hat{\\beta}_D)} \\approx \\frac{n \\beta^2 \\operatorname{Var}(D)}{\\sigma^2} $$\n    -   **Comparison**: Let's find the ratio of the non-centrality parameters.\n        $$ \\frac{\\lambda_D}{\\lambda_G} = \\sqrt{\\frac{n \\beta^2 \\operatorname{Var}(D) / \\sigma^2}{n \\beta^2 \\operatorname{Var}(G) / \\sigma^2}} = \\sqrt{\\frac{\\operatorname{Var}(D)}{\\operatorname{Var}(G)}} = \\sqrt{R^2} $$\n    The expected value of the $Z$-statistic under the alternative hypothesis (i.e., its non-centrality) is attenuated by a factor of $\\sqrt{R^2}$. The test statistic's variance under the null is still $1$, but its mean under the alternative is shifted less, leading to a loss of power.\n    -   **Effective Sample Size ($n_{\\text{eff}}$)**: We can see how power is lost by rewriting $\\lambda_D^2$:\n        $$ \\lambda_D^2 = \\frac{n \\beta^2 (R^2 \\operatorname{Var}(G))}{\\sigma^2} = \\frac{(n R^2) \\beta^2 \\operatorname{Var}(G)}{\\sigma^2} $$\n    This shows that a study of size $n$ using dosages has the same power (non-centrality parameter) as a study of size $n_{\\text{eff}} = n R^2$ using true genotypes.\n\n### Option-by-Option Analysis\n\n**A. In OLS of $Y$ on $D$, the slope estimator for $\\beta$ is unbiased. Relative to using true genotypes, the expected Wald or $Z$-statistic for testing $H_0:\\beta=0$ is attenuated by a factor $\\sqrt{R^2}$, where $R^2 = 1 - \\mathbb{E}[v_i]/(2p(1-p))$.**\n-   **Unbiased estimator**: As derived in section 2, the OLS estimator $\\hat{\\beta}_D$ is asymptotically unbiased for $\\beta$. **Correct**.\n-   **Z-statistic attenuation**: As derived in section 4, the non-centrality parameter of the Z-statistic is attenuated by $\\sqrt{R^2}$. This corresponds to the attenuation of the expected statistic. **Correct**.\n-   **$R^2$ formula**: As derived in section 3, $R^2 = 1 - \\mathbb{E}[v_i]/\\operatorname{Var}(G) = 1 - \\mathbb{E}[v_i]/(2p(1-p))$. **Correct**.\nAll parts of the statement are consistent with our derivations.\n**Verdict: Correct**\n\n**B. Modeling dosage uncertainty as $D_i = G_i + \\epsilon_i$ with $\\operatorname{Var}(\\epsilon_i) = v_i$ implies $\\operatorname{Var}(D) = \\operatorname{Var}(G) + \\mathbb{E}[v_i]$, so imputation uncertainty inflates the predictor variance and therefore increases power.**\n-   The relationship $\\operatorname{Var}(D) = \\operatorname{Var}(G) + \\mathbb{E}[v_i]$ is incorrect. Our derivation from the law of total variance shows $\\operatorname{Var}(G) = \\operatorname{Var}(D) + \\mathbb{E}[v_i]$, which implies $\\operatorname{Var}(D) = \\operatorname{Var}(G) - \\mathbb{E}[v_i]$. Imputation uncertainty *reduces* the predictor variance.\n-   Because predictor variance is reduced, power is *decreased*, not increased. The statement's premise and conclusion are both flawed.\n**Verdict: Incorrect**\n\n**C. The imputation quality metric $R^2$ equals $\\operatorname{Corr}(D,G)$ (not squared), and the association $Z$-statistic is reduced by a factor $R^2$.**\n-   The metric $R^2$ is the *squared* correlation. From section 3, we showed $\\operatorname{Corr}(D,G) = \\sqrt{\\frac{\\operatorname{Var}(D)}{\\operatorname{Var}(G)}} = \\sqrt{R^2}$. So the statement that $R^2 = \\operatorname{Corr}(D,G)$ is false, unless $R^2=1$ or $R^2=0$.\n-   The association $Z$-statistic is attenuated by a factor of $\\sqrt{R^2}$, not $R^2$. The chi-squared statistic, $Z^2$, is attenuated by a factor of $R^2$. Both claims are false.\n**Verdict: Incorrect**\n\n**D. Filtering variants by an $R^2$ threshold necessarily biases effect estimates toward zero because retained variants have $\\operatorname{Var}(D) < \\operatorname{Var}(G)$.**\n-   As shown in section 2, the OLS estimator for $\\beta$ is unbiased, irrespective of the value of $R^2$ (as long as $R^2>0$). Filtering on $R^2$ does not introduce bias into the effect estimate itself. This is different from filtering on statistical significance (p-value), which can cause winner's curse bias.\n-   While the premise \"retained variants have $\\operatorname{Var}(D) < \\operatorname{Var}(G)$\" is true for any imperfectly imputed variant, it is the cause of power loss, not bias in $\\hat{\\beta}$. The conclusion is incorrect.\n**Verdict: Incorrect**\n\n**E. By the law of total variance, $\\operatorname{Var}(G) = \\operatorname{Var}(D) + \\mathbb{E}[v_i]$, which implies $R^2 = \\operatorname{Var}(D)/\\operatorname{Var}(G)$; therefore, tests using dosages have an effective sample size $n_{\\text{eff}} = R^2 n$.**\n-   The variance decomposition $\\operatorname{Var}(G) = \\operatorname{Var}(D) + \\mathbb{E}[v_i]$ is a direct application of the law of total variance, as shown in section 1. **Correct**.\n-   The implication $R^2 = \\operatorname{Var}(D)/\\operatorname{Var}(G)$ is the standard definition of imputation $R^2$ and follows from the definition of correlation and our derived property $\\operatorname{Cov}(G,D)=\\operatorname{Var}(D)$, as shown in section 3. **Correct**.\n-   The effective sample size $n_{\\text{eff}} = R^2 n$ is a direct consequence of the effect of imputation on the non-centrality parameter of the test statistic, as shown in section 4. **Correct**.\nAll parts of the statement are correct and logically connected.\n**Verdict: Correct**\n\n**F. Because $R^2$ quantifies the fraction of genotype variance captured by dosages, a principled practice is to filter variants with low $R^2$ (for example, below $0.3$ in large samples or below $0.8$ for rare variants), since they contribute $n_{\\text{eff}} \\approx R^2 n$ and have limited power without improving inference quality.**\n-   The statement correctly identifies $R^2$ as the fraction of variance captured ($R^2 = \\operatorname{Var}(D)/\\operatorname{Var}(G)$).\n-   It correctly states that the effective sample size is approximately $R^2 n$.\n-   Based on this mathematical fact, it concludes that variants with very low $R^2$ have severely limited power. For example, a variant with $R^2=0.1$ has only $10\\%$ of the effective sample size of a perfectly genotyped variant.\n-   Therefore, filtering such variants is a logical, \"principled\" strategy to avoid wasting statistical power on the multiple testing burden they impose while offering little information. The given thresholds are illustrative of this principle. The statement correctly links the mathematical derivations to sound scientific practice.\n**Verdict: Correct**",
            "answer": "$$\\boxed{AEF}$$"
        }
    ]
}