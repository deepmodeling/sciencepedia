{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of instrumental variable analysis, we begin with its most fundamental form. In many epidemiological and Mendelian Randomization studies, the instrumental variable is binary, such as the presence or absence of a specific genetic allele. This exercise challenges you to derive the classic IV estimator for this scenario, known as the Wald estimator, directly from the core assumptions of instrument relevance and exogeneity. By working from first principles, you will see precisely why the causal effect is identified as a ratio of two differences, providing a foundational insight that extends to more complex IV models.",
            "id": "4574229",
            "problem": "A large-scale Mendelian randomization study in cardiovascular epidemiology investigates the causal effect of a continuous inflammatory biomarker $X$ (C-reactive protein, measured in mg/L) on a continuous outcome $Y$ (systolic blood pressure, measured in mmHg). A single-nucleotide polymorphism is used as an instrumental variable (IV), and it is coded as a binary instrument $Z \\in \\{0,1\\}$ indicating the absence ($Z=0$) or presence ($Z=1$) of the effect allele. Assume a linear structural causal model in which the average causal effect of $X$ on $Y$ is a constant slope parameter $ \\beta $, and that $Z$ may affect $X$ but affects $Y$ only through $X$. Work in a superpopulation framework, and assume that all relevant expectations and variances exist and are finite.\n\nUsing only core definitions of instrumental variable relevance and exogeneity, and without invoking any pre-specified estimating formulas, derive the population ratio form of the just-identified IV estimator based on $Z$ for the causal effect of $X$ on $Y$ when $Z$ is binary. Then, using the sample summaries below, form the sample-analog estimator and compute its numerical value.\n\nA cohort of $n = 100{,}000$ unrelated individuals of homogeneous ancestry is genotyped. The following subgroup means are estimated from the data:\n- Among individuals with $Z=1$: the sample mean of $X$ is $\\bar{X}_{1} = 2.91$ mg/L and the sample mean of $Y$ is $\\bar{Y}_{1} = 129.80$ mmHg.\n- Among individuals with $Z=0$: the sample mean of $X$ is $\\bar{X}_{0} = 2.28$ mg/L and the sample mean of $Y$ is $\\bar{Y}_{0} = 127.43$ mmHg.\n\nCompute the instrument-based ratio estimate of the average causal effect of $X$ on $Y$ as a scalar, using the above sample means. Round your answer to four significant figures. Express the causal effect in units of mmHg per mg/L.\n\nFinally, clearly articulate the minimal high-level conditions, in terms of instrument relevance, exogeneity, structural assumptions, and regularity conditions, under which this estimator is consistent for the causal effect parameter in the described epidemiological setting with a binary instrument and continuous exposure.",
            "solution": "The problem asks for the derivation of the instrumental variable (IV) estimator for a binary instrument, its calculation from sample data, and an articulation of the conditions for its consistency. The context is a Mendelian randomization study where a genetic variant $Z$ is used as an instrument to estimate the causal effect of a biomarker $X$ on a health outcome $Y$.\n\nFirst, we derive the population form of the IV estimator. Let the structural causal model relating the outcome $Y$ to the exposure $X$ be linear with a constant causal effect parameter $\\beta$:\n$$\nY = \\alpha_Y + \\beta X + U\n$$\nwhere $U$ represents all other causes of $Y$, including unmeasured confounders that may be correlated with $X$. The goal is to estimate $\\beta$.\n\nThe instrumental variable $Z$ must satisfy two core conditions:\n1.  **Relevance**: $Z$ must be associated with the exposure $X$. Formally, $\\text{Cov}(Z, X) \\neq 0$.\n2.  **Exogeneity**: $Z$ must be independent of any unmeasured confounders of the $X-Y$ relationship. This is typically operationalized as two sub-conditions: (a) $Z$ affects $Y$ only through $X$ (the exclusion restriction), and (b) $Z$ is independent of $U$. Together, these imply $\\text{Cov}(Z, U) = 0$.\n\nWe take the covariance of the structural equation with the instrument $Z$:\n$$\n\\text{Cov}(Y, Z) = \\text{Cov}(\\alpha_Y + \\beta X + U, Z)\n$$\nUsing the linearity property of covariance, this becomes:\n$$\n\\text{Cov}(Y, Z) = \\text{Cov}(\\alpha_Y, Z) + \\beta \\text{Cov}(X, Z) + \\text{Cov}(U, Z)\n$$\nSince $\\alpha_Y$ is a constant, $\\text{Cov}(\\alpha_Y, Z) = 0$. From the exogeneity assumption, we have $\\text{Cov}(U, Z) = 0$. This simplifies the equation to:\n$$\n\\text{Cov}(Y, Z) = \\beta \\text{Cov}(X, Z)\n$$\nFrom the relevance assumption, $\\text{Cov}(X, Z) \\neq 0$, so we can solve for $\\beta$:\n$$\n\\beta = \\frac{\\text{Cov}(Y, Z)}{\\text{Cov}(X, Z)}\n$$\nThis is the general form of the population IV estimand. For a binary instrument $Z \\in \\{0,1\\}$, the covariance formula can be simplified. For any random variable $V$, the covariance with a binary $Z$ is $\\text{Cov}(V, Z) = (E[V|Z=1] - E[V|Z=0])P(Z=1)(1-P(Z=1))$. Applying this to the numerator and denominator:\n$$\n\\beta = \\frac{(E[Y|Z=1] - E[Y|Z=0])P(Z=1)(1-P(Z=1))}{(E[X|Z=1] - E[X|Z=0])P(Z=1)(1-P(Z=1))}\n$$\nAssuming that $Z$ is not a constant, i.e., $0 < P(Z=1) < 1$, the term $P(Z=1)(1-P(Z=1))$ is non-zero and can be cancelled. This yields the specific population ratio form for a binary instrument:\n$$\n\\beta = \\frac{E[Y|Z=1] - E[Y|Z=0]}{E[X|Z=1] - E[X|Z=0]}\n$$\nThis formula defines the estimand as the ratio of the difference in mean outcome to the difference in mean exposure between the two groups defined by the instrument.\n\nSecond, we form the sample-analog estimator by replacing the population conditional expectations with their sample counterparts—the conditional sample means. The estimator, denoted $\\hat{\\beta}_{IV}$, is:\n$$\n\\hat{\\beta}_{IV} = \\frac{\\bar{Y}_{1} - \\bar{Y}_{0}}{\\bar{X}_{1} - \\bar{X}_{0}}\n$$\nwhere $\\bar{Y}_{1}$ and $\\bar{X}_{1}$ are the sample means of $Y$ and $X$ for the subgroup with $Z=1$, and $\\bar{Y}_{0}$ and $\\bar{X}_{0}$ are the sample means for the subgroup with $Z=0$.\n\nUsing the provided data:\n- $\\bar{Y}_{1} = 129.80$\n- $\\bar{Y}_{0} = 127.43$\n- $\\bar{X}_{1} = 2.91$\n- $\\bar{X}_{0} = 2.28$\n\nWe substitute these values into the estimator formula:\n$$\n\\hat{\\beta}_{IV} = \\frac{129.80 - 127.43}{2.91 - 2.28} = \\frac{2.37}{0.63} \\approx 3.7619047...\n$$\nRounding to four significant figures as requested, we get:\n$$\n\\hat{\\beta}_{IV} \\approx 3.762\n$$\nThe units are mmHg per mg/L.\n\nThird, the minimal high-level conditions for $\\hat{\\beta}_{IV}$ to be a consistent estimator for the causal effect $\\beta$ in this setting are:\n1.  **Instrument Relevance**: The SNP ($Z$) must have a causal effect on C-reactive protein levels ($X$). In the population, this means $E[X|Z=1] \\neq E[X|Z=0]$. A violation of this, or a near-violation (a \"weak instrument\"), would make the estimator biased and highly variable.\n2.  **Instrument Exogeneity (and Exclusion Restriction)**: The SNP ($Z$) must be independent of all unmeasured confounding factors that influence both C-reactive protein ($X$) and systolic blood pressure ($Y$). This implies that (a) there is no horizontal pleiotropy, where the SNP affects blood pressure through a pathway independent of C-reactive protein, and that (b) the presence of the SNP is not correlated with other lifestyle or environmental factors that cause high blood pressure. This second part is made plausible by the random assortment of alleles at meiosis.\n3.  **Linearity and Homogeneity**: The problem's premise of a linear structural model with a constant causal effect $\\beta$ is a strong structural assumption. It assumes that the causal effect of $X$ on $Y$ is the same for every individual. The IV estimator is consistent for this constant effect $\\beta$ under this assumption. Without this assumption (i.e., with effect heterogeneity), the IV ratio converges to the Local Average Treatment Effect (LATE), which is the average effect only among those individuals whose exposure level is affected by the instrument.\n\nThese three conditions, along with standard regularity assumptions (e.g., a random sample from the population and finite moments, as given) and a large sample size, ensure that the sample estimator $\\hat{\\beta}_{IV}$ converges in probability to the true causal parameter $\\beta$.",
            "answer": "$$\\boxed{3.762}$$"
        },
        {
            "introduction": "The IV estimator provides a single value for the causal effect, but what does this value truly represent if the effect of the exposure on the outcome varies among individuals? This practice moves beyond the simple assumption of a constant effect to explore the concept of effect heterogeneity. Using the potential outcomes framework, you will derive the Local Average Treatment Effect (LATE), a cornerstone of modern causal inference . This will reveal that the IV estimand identifies the average causal effect only for the specific subpopulation of \"compliers\"—those whose exposure status is actually changed by the instrument—a critical nuance for correctly interpreting the results of an IV analysis.",
            "id": "4574225",
            "problem": "A genomics-enabled health system conducts an encouragement design to evaluate the causal effect of initiating a pharmacogenomically guided statin therapy on low-density lipoprotein cholesterol reduction. A random subset of eligible patients receives an electronic encouragement to attend a pharmacogenomic consultation, which increases the chance of initiating the statin regimen. Let the binary instrument be $Z \\in \\{0,1\\}$, where $Z=1$ indicates encouragement. Let the binary treatment be $X \\in \\{0,1\\}$, where $X=1$ indicates initiation of therapy. Let the outcome be $Y \\in \\mathbb{R}$, the follow-up low-density lipoprotein cholesterol level (lower is better). Use the potential outcomes framework with the following definitions and assumptions:\n\n- Stable Unit Treatment Value Assumption (SUTVA): Each unit has well-defined potential treatment $X(z) \\in \\{0,1\\}$ for $z \\in \\{0,1\\}$ and potential outcome $Y(x) \\in \\mathbb{R}$ for $x \\in \\{0,1\\}$, and there is no interference between units.\n- Independence: The instrument is randomly assigned, so $Z \\perp \\big(Y(1),Y(0),X(1),X(0)\\big)$.\n- Exclusion restriction: The instrument affects the outcome only through the treatment, so the observed outcome is $Y=Y\\big(X\\big)$ and there is no direct effect of $Z$ on $Y$ beyond its effect on $X$.\n- Monotonicity: There are no defiers, so $X(1) \\geq X(0)$ for all units.\n- Relevance: The instrument shifts treatment uptake, so $\\mathbb{P}\\big(X(1) \\neq X(0)\\big) > 0$.\n\nDefine the Local Average Treatment Effect (LATE) as the average causal effect of treatment on the subpopulation of compliers, that is\n$$\n\\text{LATE} \\equiv \\mathbb{E}\\big[\\,Y(1)-Y(0)\\,\\big|\\,X(1)=1,\\,X(0)=0\\,\\big].\n$$\n\nStarting from these definitions and assumptions alone, derive a closed-form expression for $\\text{LATE}$ in terms of observed differences in conditional expectations across levels of the instrument, using only quantities of the form $\\mathbb{E}[\\,\\cdot \\mid Z=z\\,]$. In your derivation, explicitly state and justify each step from the assumptions to the resulting identification formula, including the role of each compliance type (always-takers, never-takers, compliers, and defiers) under the monotonicity assumption. Explain the interpretation of the numerator and denominator in the final expression within the epidemiologic context of this encouragement design. Provide the final expression for $\\text{LATE}$ as a single simplified symbolic fraction in terms of observable expectations. Do not include any numerical computation. The final answer must be a single analytic expression.",
            "solution": "We begin by formalizing the causal structure using the potential outcomes framework. Each unit has potential treatments $X(1)$ and $X(0)$ corresponding to instrument levels $Z=1$ and $Z=0$, and potential outcomes $Y(1)$ and $Y(0)$ corresponding to treatment levels $X=1$ and $X=0$. The observed treatment is $X = X(Z)$ and, by the exclusion restriction, the observed outcome is $Y = Y\\big(X(Z)\\big) = Y\\big(X\\big)$.\n\nDefine the principal strata by the joint values of $\\big(X(1),X(0)\\big)$:\n- Always-takers: $X(1)=1$, $X(0)=1$.\n- Never-takers: $X(1)=0$, $X(0)=0$.\n- Compliers: $X(1)=1$, $X(0)=0$.\n- Defiers: $X(1)=0$, $X(0)=1$.\n\nBy monotonicity, $X(1) \\geq X(0)$ for all units, which rules out defiers. Let $\\mathcal{A}$, $\\mathcal{N}$, and $\\mathcal{C}$ denote the sets of always-takers, never-takers, and compliers, respectively.\n\nWe first express the observed mean outcome conditional on $Z=z$:\n$$\n\\mathbb{E}[\\,Y \\mid Z=z\\,] \\;=\\; \\mathbb{E}\\Big[\\,Y\\big(X(z)\\big) \\,\\big|\\, Z=z\\,\\Big].\n$$\nBy independence, $Z \\perp \\big(Y(1),Y(0),X(1),X(0)\\big)$, so conditioning on $Z=z$ does not change the distribution of the potential outcomes and potential treatments. Therefore,\n$$\n\\mathbb{E}[\\,Y \\mid Z=z\\,] \\;=\\; \\mathbb{E}\\Big[\\,Y\\big(X(z)\\big)\\,\\Big].\n$$\nDecompose this expectation across principal strata:\n$$\n\\mathbb{E}[\\,Y \\mid Z=z\\,] \\;=\\; \\mathbb{E}\\big[\\,Y\\big(X(z)\\big)\\,\\big|\\,\\mathcal{A}\\,\\big]\\mathbb{P}(\\mathcal{A})\n\\;+\\; \\mathbb{E}\\big[\\,Y\\big(X(z)\\big)\\,\\big|\\,\\mathcal{N}\\,\\big]\\mathbb{P}(\\mathcal{N})\n\\;+\\; \\mathbb{E}\\big[\\,Y\\big(X(z)\\big)\\,\\big|\\,\\mathcal{C}\\,\\big]\\mathbb{P}(\\mathcal{C}).\n$$\nFor always-takers, $X(1)=X(0)=1$, so $X(z)=1$ for both $z=0$ and $z=1$, hence\n$$\n\\mathbb{E}\\big[\\,Y\\big(X(1)\\big)\\,\\big|\\,\\mathcal{A}\\,\\big] \\;=\\; \\mathbb{E}\\big[\\,Y(1)\\,\\big|\\,\\mathcal{A}\\,\\big], \n\\quad\n\\mathbb{E}\\big[\\,Y\\big(X(0)\\big)\\,\\big|\\,\\mathcal{A}\\,\\big] \\;=\\; \\mathbb{E}\\big[\\,Y(1)\\,\\big|\\,\\mathcal{A}\\,\\big].\n$$\nThus the contribution of always-takers cancels when taking the difference across $Z$. Similarly, for never-takers, $X(1)=X(0)=0$, so $X(z)=0$ for both $z=0$ and $z=1$, implying\n$$\n\\mathbb{E}\\big[\\,Y\\big(X(1)\\big)\\,\\big|\\,\\mathcal{N}\\,\\big] \\;=\\; \\mathbb{E}\\big[\\,Y(0)\\,\\big|\\,\\mathcal{N}\\,\\big], \n\\quad\n\\mathbb{E}\\big[\\,Y\\big(X(0)\\big)\\,\\big|\\,\\mathcal{N}\\,\\big] \\;=\\; \\mathbb{E}\\big[\\,Y(0)\\,\\big|\\,\\mathcal{N}\\,\\big],\n$$\nand their contribution also cancels in the difference across $Z$.\n\nFor compliers, $X(1)=1$ and $X(0)=0$, so $X(1)-X(0)=1$. Their contribution to the outcome difference is\n$$\n\\mathbb{E}\\big[\\,Y\\big(X(1)\\big)\\,\\big|\\,\\mathcal{C}\\,\\big] - \\mathbb{E}\\big[\\,Y\\big(X(0)\\big)\\,\\big|\\,\\mathcal{C}\\,\\big]\n\\;=\\; \\mathbb{E}\\big[\\,Y(1)-Y(0)\\,\\big|\\,\\mathcal{C}\\,\\big]\n\\;=\\; \\text{LATE}.\n$$\nTherefore, taking the difference in observed mean outcomes across instrument levels and using linearity of expectation, we obtain\n$$\n\\mathbb{E}[\\,Y \\mid Z=1\\,] - \\mathbb{E}[\\,Y \\mid Z=0\\,]\n\\;=\\; \\mathbb{P}(\\mathcal{C}) \\cdot \\text{LATE}.\n$$\nWe now analyze the difference in mean treatment across instrument levels:\n$$\n\\mathbb{E}[\\,X \\mid Z=z\\,] \\;=\\; \\mathbb{E}\\big[\\,X(z)\\,\\big].\n$$\nDecomposing by principal strata and using the same logic:\n- Always-takers contribute $1$ to $\\mathbb{E}[\\,X \\mid Z=z\\,]$ for both $z=0$ and $z=1$.\n- Never-takers contribute $0$ for both $z=0$ and $z=1$.\n- Compliers contribute $1$ when $z=1$ and $0$ when $z=0$.\n\nThus,\n$$\n\\mathbb{E}[\\,X \\mid Z=1\\,] - \\mathbb{E}[\\,X \\mid Z=0\\,] \\;=\\; \\mathbb{P}(\\mathcal{C}).\n$$\nCombining the two identities and using the instrument relevance assumption $\\mathbb{P}(\\mathcal{C})>0$, we can solve for $\\text{LATE}$ as\n$$\n\\text{LATE} \\;=\\; \\frac{\\mathbb{E}[\\,Y \\mid Z=1\\,] - \\mathbb{E}[\\,Y \\mid Z=0\\,]}{\\mathbb{E}[\\,X \\mid Z=1\\,] - \\mathbb{E}[\\,X \\mid Z=0\\,]}.\n$$\n\nInterpretation of the terms in the epidemiologic encouragement design:\n- The numerator $\\mathbb{E}[\\,Y \\mid Z=1\\,] - \\mathbb{E}[\\,Y \\mid Z=0\\,]$ is the Intention-To-Treat effect of the encouragement on the outcome. It captures how the randomized encouragement shifts the average low-density lipoprotein cholesterol level, aggregating across all compliance types.\n- The denominator $\\mathbb{E}[\\,X \\mid Z=1\\,] - \\mathbb{E}[\\,X \\mid Z=0\\,]$ is the Intention-To-Treat effect of the encouragement on treatment uptake, that is, the increase in the probability of initiating pharmacogenomically guided statin therapy due to encouragement.\n- Their ratio is the average causal effect of initiating treatment among compliers, the subpopulation whose treatment status changes in response to the encouragement. This is the Local Average Treatment Effect, identified under the stated assumptions.\n\nThis expression is a closed-form estimand in terms of observable conditional expectations and does not require knowledge of compliance types at the individual level. It is valid in this bioinformatics-driven epidemiologic setting provided the randomization of $Z$, exclusion restriction, monotonicity, and SUTVA hold, and the instrument is relevant so that the denominator is nonzero.",
            "answer": "$$\\boxed{\\frac{\\mathbb{E}[Y \\mid Z=1]-\\mathbb{E}[Y \\mid Z=0]}{\\mathbb{E}[X \\mid Z=1]-\\mathbb{E}[X \\mid Z=0]}}$$"
        },
        {
            "introduction": "Moving from theory to practice, this final exercise requires you to build a complete two-sample Mendelian Randomization (MR) pipeline, simulating a real-world bioinformatics workflow from start to finish. You will confront the practical challenges of using summary statistics from large-scale genome-wide association studies (GWAS) to estimate a causal effect. This involves implementing a sequence of critical steps: selecting valid instruments based on statistical significance, pruning correlated instruments through linkage disequilibrium (LD) clumping, resolving allele mismatches via harmonization, and finally, combining evidence from multiple instruments using the Inverse-Variance Weighted (IVW) method . This capstone practice synthesizes the principles from the previous exercises into a powerful, practical application.",
            "id": "4574240",
            "problem": "You are tasked with implementing a complete two-sample Mendelian Randomization pipeline as a runnable program that estimates the causal effect of low-density lipoprotein cholesterol on coronary artery disease using instrumental variable analysis. Your program must implement variant selection, linkage disequilibrium clumping, allele harmonization, and palindromic allele handling, and then compute the Inverse-Variance Weighted (IVW) estimator for the causal effect. The design must reflect standard instrumental variable analysis in epidemiology. The implementation must be fully deterministic using the provided synthetic summary data and must not require any user input.\n\nStart from the following foundational base:\n- Instrumental variables are defined by three assumptions: relevance, independence, and exclusion restriction. In two-sample Mendelian Randomization, single nucleotide polymorphisms serve as instruments for the exposure.\n- For each instrument indexed by $i$, the exposure association is denoted by $\\hat{\\beta}_{X i}$ with standard error $s_{X i}$ and $p$-value $p_{X i}$, and the outcome association is denoted by $\\hat{\\beta}_{Y i}$ with standard error $s_{Y i}$.\n- In a summary-data two-sample setting under the no-measurement-error approximation for $\\hat{\\beta}_{X i}$, the IVW estimator is obtained by weighted least squares regression of $\\hat{\\beta}_{Y i}$ on $\\hat{\\beta}_{X i}$ with zero intercept and weights $w_i = 1 / s_{Y i}^2$. The IVW slope estimate is\n$$\n\\hat{\\theta}_{\\mathrm{IVW}} \\;=\\; \\frac{\\sum_i w_i \\,\\hat{\\beta}_{X i}\\,\\hat{\\beta}_{Y i}}{\\sum_i w_i \\,\\hat{\\beta}_{X i}^2},\n$$\nwith fixed-effects standard error\n$$\n\\mathrm{se}(\\hat{\\theta}_{\\mathrm{IVW}}) \\;=\\; \\sqrt{\\frac{1}{\\sum_i w_i \\,\\hat{\\beta}_{X i}^2}},\n$$\nand Cochran’s heterogeneity statistic\n$$\nQ \\;=\\; \\sum_i w_i \\,\\big(\\hat{\\beta}_{Y i} - \\hat{\\theta}_{\\mathrm{IVW}} \\,\\hat{\\beta}_{X i}\\big)^2.\n$$\nYour program must compute $\\hat{\\theta}_{\\mathrm{IVW}}$ for each test case; secondary quantities such as $\\mathrm{se}(\\hat{\\theta}_{\\mathrm{IVW}})$ and $Q$ may be computed internally for validation, but the required output is only the $\\hat{\\theta}_{\\mathrm{IVW}}$ values.\n\nData and rules to implement:\n1) Variant filtering by exposure association:\n   - Keep only variants with $p_{X i} \\leq p_{\\mathrm{thr}}$.\n\n2) Linkage disequilibrium clumping:\n   - After filtering, sort variants by $p_{X i}$ ascending. Iterate through this order and retain a candidate only if there is no already-retained variant within a specified physical window whose pairwise linkage disequilibrium exceeds a threshold.\n   - Specifically, define a window of $W_{\\mathrm{kb}}$ kilobases and an $r^2$ threshold $r^2_{\\mathrm{thr}}$. For a new candidate $j$, if any retained variant $k$ satisfies both $| \\mathrm{pos}_j - \\mathrm{pos}_k | \\leq 1000 \\times W_{\\mathrm{kb}}$ and $r^2_{jk} \\geq r^2_{\\mathrm{thr}}$, then discard $j$; otherwise retain $j$.\n   - If a pair $r^2$ is not explicitly provided, assume $r^2 = 0$.\n\n3) Harmonization of alleles:\n   - Each variant has exposure alleles $(A_{E}, A_{O})$ and outcome alleles $(A'_{E}, A'_{O})$. Also provided are exposure effect allele frequencies $\\mathrm{EAF}_X$ and outcome effect allele frequencies $\\mathrm{EAF}_Y$.\n   - Non-palindromic alleles: If $(A'_{E}, A'_{O}) = (A_{E}, A_{O})$, keep $\\hat{\\beta}_{Y i}$ as is. If $(A'_{E}, A'_{O}) = (A_{O}, A_{E})$, flip the sign of $\\hat{\\beta}_{Y i}$. If neither matches, attempt strand-complementing the outcome alleles by applying the complement map $\\mathcal{C}$ with $\\mathcal{C}(A) = T$, $\\mathcal{C}(T) = A$, $\\mathcal{C}(C) = G$, $\\mathcal{C}(G) = C$ to both outcome alleles, and then re-check the two matching rules; if still no match, drop the variant.\n   - Palindromic alleles: A palindromic set is one where $\\{A_{E}, A_{O}\\} \\in \\big\\{ \\{A,T\\}, \\{C,G\\} \\big\\}$. For palindromic variants, do not use letter-matching or strand complement to decide orientation. Instead, use $\\mathrm{EAF}_X$ relative to an ambiguity band. If $\\mathrm{EAF}_X \\in [L_{\\mathrm{amb}}, U_{\\mathrm{amb}}]$, where $L_{\\mathrm{amb}}$ and $U_{\\mathrm{amb}}$ are ambiguity bounds, drop the variant as unresolvable. Otherwise, determine orientation by comparing $|\\mathrm{EAF}_Y - \\mathrm{EAF}_X|$ versus $| (1 - \\mathrm{EAF}_Y) - \\mathrm{EAF}_X |$. If the former is smaller or equal, keep $\\hat{\\beta}_{Y i}$; if the latter is smaller, flip the sign of $\\hat{\\beta}_{Y i}$.\n\n4) IVW estimation:\n   - After clumping and harmonization, compute $\\hat{\\theta}_{\\mathrm{IVW}}$ using the formulas above with $w_i = 1 / s_{Y i}^2$. If only a single instrument remains, the estimator reduces to $\\hat{\\theta}_{\\mathrm{IVW}} = \\hat{\\beta}_{Y 1} / \\hat{\\beta}_{X 1}$.\n\nSynthetic summary data:\n- Single nucleotide polymorphisms: $\\mathrm{SNP} \\in \\{\\mathrm{rs1}, \\mathrm{rs2}, \\mathrm{rs3}, \\mathrm{rs4}, \\mathrm{rs5}, \\mathrm{rs6}\\}$, all on chromosome $1$ with base-pair positions:\n  - $\\mathrm{rs1}$ at $100{,}000$,\n  - $\\mathrm{rs2}$ at $120{,}000$,\n  - $\\mathrm{rs3}$ at $160{,}000$,\n  - $\\mathrm{rs4}$ at $220{,}000$,\n  - $\\mathrm{rs5}$ at $500{,}000$,\n  - $\\mathrm{rs6}$ at $620{,}000$.\n- Exposure associations $(\\hat{\\beta}_{X}, s_{X}, p_{X}, \\mathrm{EAF}_X, A_{E}, A_{O})$:\n  - $\\mathrm{rs1}$: $(0.08, 0.01, 1\\times 10^{-12}, 0.30, A, G)$,\n  - $\\mathrm{rs2}$: $(0.07, 0.015, 2\\times 10^{-9}, 0.55, T, A)$,\n  - $\\mathrm{rs3}$: $(0.05, 0.02, 7\\times 10^{-8}, 0.40, C, T)$,\n  - $\\mathrm{rs4}$: $(0.02, 0.02, 1\\times 10^{-6}, 0.20, G, C)$,\n  - $\\mathrm{rs5}$: $(0.06, 0.018, 3\\times 10^{-9}, 0.10, G, T)$,\n  - $\\mathrm{rs6}$: $(0.09, 0.017, 1\\times 10^{-10}, 0.49, C, G)$.\n- Outcome associations $(\\hat{\\beta}_{Y}, s_{Y}, \\mathrm{EAF}_Y, A'_{E}, A'_{O})$:\n  - $\\mathrm{rs1}$: $(0.035, 0.01, 0.31, A, G)$,\n  - $\\mathrm{rs2}$: $(-0.028, 0.012, 0.45, A, T)$,\n  - $\\mathrm{rs3}$: $(0.018, 0.012, 0.60, G, A)$,\n  - $\\mathrm{rs4}$: $(0.006, 0.015, 0.20, G, C)$,\n  - $\\mathrm{rs5}$: $(-0.030, 0.011, 0.90, T, G)$,\n  - $\\mathrm{rs6}$: $(-0.041, 0.013, 0.51, G, C)$.\n- Linkage disequilibrium pairs are given by $(r^2)$ values for pairs within the physical window; if a pair is not listed, use $0$:\n  - $\\mathrm{rs1}$–$\\mathrm{rs2}$: $0.60$,\n  - $\\mathrm{rs2}$–$\\mathrm{rs3}$: $0.25$,\n  - $\\mathrm{rs1}$–$\\mathrm{rs3}$: $0.19$,\n  - all other listed pairs: $0$.\n\nTest suite and parameterization:\nYour program must run the following four test cases using the dataset above and output the IVW estimate $\\hat{\\theta}_{\\mathrm{IVW}}$ for each case in order.\n\n- Test case $1$ (happy path): $p_{\\mathrm{thr}} = 5\\times 10^{-8}$, $r^2_{\\mathrm{thr}} = 0.20$, $W_{\\mathrm{kb}} = 100$, $L_{\\mathrm{amb}} = 0.42$, $U_{\\mathrm{amb}} = 0.58$. Use the exposure and outcome data exactly as specified above.\n\n- Test case $2$ (boundary on $r^2$ equality): $p_{\\mathrm{thr}} = 5\\times 10^{-8}$, $r^2_{\\mathrm{thr}} = 0.19$, $W_{\\mathrm{kb}} = 100$, $L_{\\mathrm{amb}} = 0.42$, $U_{\\mathrm{amb}} = 0.58$. Use the same data; note that equality at $r^2 = 0.19$ should trigger clumping according to the rule $r^2 \\ge r^2_{\\mathrm{thr}}$.\n\n- Test case $3$ (resolvable palindromic using effect allele frequency): $p_{\\mathrm{thr}} = 5\\times 10^{-8}$, $r^2_{\\mathrm{thr}} = 0.20$, $W_{\\mathrm{kb}} = 100$, $L_{\\mathrm{amb}} = 0.42$, $U_{\\mathrm{amb}} = 0.58$. Modify only the palindromic variant $\\mathrm{rs6}$ to have exposure $\\mathrm{EAF}_X = 0.30$ and outcome $\\mathrm{EAF}_Y = 0.70$; all other values remain as specified.\n\n- Test case $4$ (single-instrument edge case): $p_{\\mathrm{thr}} = 1\\times 10^{-11}$, $r^2_{\\mathrm{thr}} = 0.20$, $W_{\\mathrm{kb}} = 100$, $L_{\\mathrm{amb}} = 0.42$, $U_{\\mathrm{amb}} = 0.58$. Use the original data; this threshold ensures only the strongest instrument is retained.\n\nFinal output format:\n- Your program must produce a single line of output containing the four IVW estimates as a comma-separated list enclosed in square brackets, in the order of test cases $1$ through $4$, for example, $[\\hat{\\theta}_1,\\hat{\\theta}_2,\\hat{\\theta}_3,\\hat{\\theta}_4]$.\n- Each element must be a floating-point number. No additional text should be printed.",
            "solution": "The problem requires the implementation of a two-sample Mendelian Randomization (MR) pipeline to estimate the causal effect of an exposure on an outcome. The pipeline adheres to standard methodologies in epidemiological genetics, including instrument selection, linkage disequilibrium (LD) clumping, allele harmonization, and finally, causal effect estimation using the Inverse-Variance Weighted (IVW) method. The entire process will be applied to a provided synthetic dataset for four distinct test cases.\n\nFirst, we will represent the provided genomic data in a structured manner to facilitate processing. Each single nucleotide polymorphism (SNP) is an object with attributes for its identifier, chromosomal position, and summary statistics for both the exposure and outcome studies. These statistics include the effect estimate ($\\hat{\\beta}$), standard error ($s$), p-value ($p$), effect allele frequency (EAF), and the effect and other alleles.\n\nThe MR pipeline consists of four main sequential steps:\n\n**1. Variant Filtering by Exposure Association Strength**\nThe first step in selecting valid instrumental variables (IVs) is to ensure they satisfy the *relevance assumption*. This assumption states that the instrument must be associated with the exposure. In practice, this is implemented by filtering variants based on the statistical significance of their association with the exposure. We will retain only those SNPs for which the exposure p-value, $p_{X i}$, is less than or equal to a specified threshold, $p_{\\mathrm{thr}}$.\n$$\n\\text{Retain SNP } i \\text{ if } p_{X i} \\leq p_{\\mathrm{thr}}\n$$\n\n**2. Linkage Disequilibrium (LD) Clumping**\nThe second step addresses the *independence assumption*, which requires the instruments to be independent of each other. SNPs that are physically close on a chromosome are often correlated, a phenomenon known as linkage disequilibrium. Including correlated instruments can bias the MR estimate. Clumping is a greedy algorithm to select a set of approximately independent SNPs. The procedure is as follows:\na) The SNPs that passed the p-value filter are sorted in ascending order of their exposure p-values, $p_{X i}$. This prioritizes SNPs with stronger evidence of association with the exposure.\nb) We iterate through the sorted list, considering each SNP as a potential instrument. The first SNP (with the lowest $p_{X i}$) is designated as the first \"index SNP\" and is retained.\nc) For each subsequent candidate SNP $j$, we check if it is in LD with any of the already retained index SNPs $k$. A candidate SNP $j$ is \"clumped out\" (i.e., discarded) if there exists any already retained SNP $k$ that satisfies two conditions simultaneously:\n   i) Physical Proximity: The distance between the SNPs is within a specified window, $|\\mathrm{pos}_j - \\mathrm{pos}_k| \\leq 1000 \\times W_{\\mathrm{kb}}$, where $W_{\\mathrm{kb}}$ is the window size in kilobases.\n   ii) Correlation: The squared correlation coefficient (LD), $r^2_{jk}$, is greater than or equal to a specified threshold, $r^2_{jk} \\geq r^2_{\\mathrm{thr}}$.\nd) If a candidate SNP is not clumped out by any of the already retained SNPs, it is itself retained and added to the set of index SNPs.\n\n**3. Allele Harmonization**\nTwo-sample MR uses summary statistics from two different studies (one for the exposure, one for the outcome). It is crucial to ensure that the effect estimate $\\hat{\\beta}$ for each SNP corresponds to the same effect allele in both studies. Harmonization is the process of aligning these alleles. For each SNP $i$, we have exposure alleles $(A_E, A_O)$ and outcome alleles $(A'_E, A'_O)$.\n\n- **Non-palindromic SNPs**: A SNP is non-palindromic if its alleles are not A/T or C/G pairs (e.g., A/G). For these, we can determine the correct orientation by matching allele letters.\n  a) If the alleles match perfectly, $(A'_E, A'_O) = (A_E, A_O)$, the outcome effect $\\hat{\\beta}_{Y i}$ is used as is.\n  b) If the alleles are flipped, $(A'_E, A'_O) = (A_O, A_E)$, the sign of the outcome effect is inverted ($\\hat{\\beta}_{Y i} \\to -\\hat{\\beta}_{Y i}$).\n  c) If neither matches, it might be a strand-ambiguity issue. We apply the complement mapping ($\\mathcal{C}(A)=T, \\mathcal{C}(T)=A, \\mathcal{C}(C)=G, \\mathcal{C}(G)=C$) to the outcome alleles and re-attempt matching. If a match is found, the effect (or its flip) is used.\n  d) If no match can be found, the SNP is discarded.\n\n- **Palindromic SNPs**: A SNP is palindromic if its alleles are complementary (A/T or C/G). For these, allele-letter matching is ambiguous. We must rely on allele frequencies.\n  a) If the effect allele frequency for the exposure, $\\mathrm{EAF}_X$, is near $0.5$, it is difficult to infer the correct strand. If $\\mathrm{EAF}_X$ falls within a specified ambiguity band, i.e., $L_{\\mathrm{amb}} \\leq \\mathrm{EAF}_X \\leq U_{\\mathrm{amb}}$, the SNP is discarded as unresolvable.\n  b) Otherwise, we infer the orientation by comparing the effect allele frequencies. We calculate two distances: $d_1 = |\\mathrm{EAF}_Y - \\mathrm{EAF}_X|$ and $d_2 = |(1-\\mathrm{EAF}_Y) - \\mathrm{EAF}_X|$. The first assumes the alleles are aligned, while the second assumes they are flipped (since the frequency of the other allele in the outcome study is $1 - \\mathrm{EAF}_Y$). If $d_1 \\leq d_2$, the alignment is likely correct. If $d_2 < d_1$, the alignment is likely flipped, and the sign of $\\hat{\\beta}_{Y i}$ is inverted.\n\n**4. Inverse-Variance Weighted (IVW) Estimation**\nAfter filtering, clumping, and harmonization, we have a set of valid and independent instruments. The final step is to estimate the causal effect, $\\theta$. The IVW method is a meta-analysis approach that combines the ratio estimates from each instrument ($\\hat{\\theta}_i = \\hat{\\beta}_{Y i} / \\hat{\\beta}_{X i}$) by weighting them by the inverse of their variance. This is equivalent to a weighted linear regression of the outcome effects on the exposure effects, with the intercept constrained to zero. The weights are $w_i = 1 / s_{Y i}^2$, where $s_{Y i}$ is the standard error of the outcome effect.\n\nThe IVW estimate of the causal effect is given by:\n$$\n\\hat{\\theta}_{\\mathrm{IVW}} = \\frac{\\sum_i \\hat{\\beta}_{X i} \\hat{\\beta}_{Y i} / s_{Y i}^2}{\\sum_i \\hat{\\beta}_{X i}^2 / s_{Y i}^2} = \\frac{\\sum_i w_i \\hat{\\beta}_{X i} \\hat{\\beta}_{Y i}}{\\sum_i w_i \\hat{\\beta}_{X i}^2}\n$$\nIn the specific edge case where only a single instrument ($i=1$) remains after the pipeline, the IVW formula simplifies to the Wald ratio estimator:\n$$\n\\hat{\\theta}_{\\mathrm{IVW}} = \\frac{\\hat{\\beta}_{Y 1}}{\\hat{\\beta}_{X 1}}\n$$\nThe program will execute this entire pipeline for each of the four specified test cases, each with its own set of parameters, and report the resulting $\\hat{\\theta}_{\\mathrm{IVW}}$ estimate.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a two-sample Mendelian Randomization pipeline and runs it on four test cases.\n    \"\"\"\n\n    # --- Data Definition ---\n    # Synthetic summary data for 6 SNPs\n    snp_database = [\n        {'id': 'rs1', 'pos': 100000, 'beta_x': 0.08, 'se_x': 0.01, 'p_x': 1e-12, 'eaf_x': 0.30, 'a_e': 'A', 'a_o': 'G', 'beta_y': 0.035, 'se_y': 0.01, 'eaf_y': 0.31, 'ap_e': 'A', 'ap_o': 'G'},\n        {'id': 'rs2', 'pos': 120000, 'beta_x': 0.07, 'se_x': 0.015, 'p_x': 2e-9, 'eaf_x': 0.55, 'a_e': 'T', 'a_o': 'A', 'beta_y': -0.028, 'se_y': 0.012, 'eaf_y': 0.45, 'ap_e': 'A', 'ap_o': 'T'},\n        {'id': 'rs3', 'pos': 160000, 'beta_x': 0.05, 'se_x': 0.02, 'p_x': 7e-8, 'eaf_x': 0.40, 'a_e': 'C', 'a_o': 'T', 'beta_y': 0.018, 'se_y': 0.012, 'eaf_y': 0.60, 'ap_e': 'G', 'ap_o': 'A'},\n        {'id': 'rs4', 'pos': 220000, 'beta_x': 0.02, 'se_x': 0.02, 'p_x': 1e-6, 'eaf_x': 0.20, 'a_e': 'G', 'a_o': 'C', 'beta_y': 0.006, 'se_y': 0.015, 'eaf_y': 0.20, 'ap_e': 'G', 'ap_o': 'C'},\n        {'id': 'rs5', 'pos': 500000, 'beta_x': 0.06, 'se_x': 0.018, 'p_x': 3e-9, 'eaf_x': 0.10, 'a_e': 'G', 'a_o': 'T', 'beta_y': -0.030, 'se_y': 0.011, 'eaf_y': 0.90, 'ap_e': 'T', 'ap_o': 'G'},\n        {'id': 'rs6', 'pos': 620000, 'beta_x': 0.09, 'se_x': 0.017, 'p_x': 1e-10, 'eaf_x': 0.49, 'a_e': 'C', 'a_o': 'G', 'beta_y': -0.041, 'se_y': 0.013, 'eaf_y': 0.51, 'ap_e': 'G', 'ap_o': 'C'},\n    ]\n    \n    # Linkage disequilibrium (LD) data\n    # Using frozenset for unordered pair keys\n    ld_matrix = {\n        frozenset(['rs1', 'rs2']): 0.60,\n        frozenset(['rs2', 'rs3']): 0.25,\n        frozenset(['rs1', 'rs3']): 0.19,\n    }\n\n    # Test suite parameters\n    test_cases = [\n        {'p_thr': 5e-8, 'r2_thr': 0.20, 'W_kb': 100, 'L_amb': 0.42, 'U_amb': 0.58, 'mods': {}},\n        {'p_thr': 5e-8, 'r2_thr': 0.19, 'W_kb': 100, 'L_amb': 0.42, 'U_amb': 0.58, 'mods': {}},\n        {'p_thr': 5e-8, 'r2_thr': 0.20, 'W_kb': 100, 'L_amb': 0.42, 'U_amb': 0.58, 'mods': {'rs6': {'eaf_x': 0.30, 'eaf_y': 0.70}}},\n        {'p_thr': 1e-11, 'r2_thr': 0.20, 'W_kb': 100, 'L_amb': 0.42, 'U_amb': 0.58, 'mods': {}},\n    ]\n\n    results = []\n\n    def run_pipeline(snps, ld, params):\n        p_thr, r2_thr, W_kb, L_amb, U_amb = params['p_thr'], params['r2_thr'], params['W_kb'], params['L_amb'], params['U_amb']\n        window_bp = 1000 * W_kb\n        \n        # 1. Variant Filtering by exposure p-value\n        filtered_snps = [snp for snp in snps if snp['p_x'] = p_thr]\n\n        # 2. Linkage Disequilibrium (LD) Clumping\n        filtered_snps.sort(key=lambda s: s['p_x'])\n        retained_snps = []\n        for candidate_snp in filtered_snps:\n            is_clumped = False\n            for retained_snp in retained_snps:\n                # Check physical distance\n                if abs(candidate_snp['pos'] - retained_snp['pos']) = window_bp:\n                    # Check LD\n                    pair = frozenset([candidate_snp['id'], retained_snp['id']])\n                    r2 = ld.get(pair, 0.0)\n                    if r2 >= r2_thr:\n                        is_clumped = True\n                        break\n            if not is_clumped:\n                retained_snps.append(candidate_snp)\n\n        # 3. Allele Harmonization\n        harmonized_snps = []\n        complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n        for snp in retained_snps:\n            exp_alleles = (snp['a_e'], snp['a_o'])\n            out_alleles = (snp['ap_e'], snp['ap_o'])\n            beta_y = snp['beta_y']\n            \n            is_palindromic = {snp['a_e'], snp['a_o']} in [{'A', 'T'}, {'C', 'G'}]\n            \n            if is_palindromic:\n                if L_amb = snp['eaf_x'] = U_amb:\n                    continue  # Drop ambiguous palindromic SNP\n                \n                dist1 = abs(snp['eaf_y'] - snp['eaf_x'])\n                dist2 = abs((1 - snp['eaf_y']) - snp['eaf_x'])\n\n                if dist2  dist1:\n                    beta_y = -beta_y\n                \n                harmonized_snp = snp.copy()\n                harmonized_snp['beta_y'] = beta_y\n                harmonized_snps.append(harmonized_snp)\n\n            else: # Non-palindromic\n                harmonized = False\n                # Direct and flipped match\n                if out_alleles == exp_alleles:\n                    harmonized = True\n                elif out_alleles == (exp_alleles[1], exp_alleles[0]):\n                    beta_y = -beta_y\n                    harmonized = True\n                else: # Strand complement check\n                    comp_out_alleles = (complement.get(out_alleles[0]), complement.get(out_alleles[1]))\n                    if comp_out_alleles[0] is None or comp_out_alleles[1] is None:\n                        continue # Invalid allele, drop\n                    if comp_out_alleles == exp_alleles:\n                        harmonized = True\n                    elif comp_out_alleles == (exp_alleles[1], exp_alleles[0]):\n                        beta_y = -beta_y\n                        harmonized = True\n                \n                if harmonized:\n                    harmonized_snp = snp.copy()\n                    harmonized_snp['beta_y'] = beta_y\n                    harmonized_snps.append(harmonized_snp)\n\n        # 4. IVW Estimation\n        if not harmonized_snps:\n            return np.nan\n        \n        if len(harmonized_snps) == 1:\n            snp = harmonized_snps[0]\n            if snp['beta_x'] == 0: return np.nan\n            return snp['beta_y'] / snp['beta_x']\n\n        numerator = 0.0\n        denominator = 0.0\n        for snp in harmonized_snps:\n            w = 1.0 / (snp['se_y'] ** 2)\n            numerator += w * snp['beta_x'] * snp['beta_y']\n            denominator += w * (snp['beta_x'] ** 2)\n        \n        if denominator == 0: return np.nan\n        return numerator / denominator\n\n    # --- Main Loop ---\n    for case_params in test_cases:\n        # Create a deep copy of the database for this run\n        current_data = [{k: v for k, v in snp.items()} for snp in snp_database]\n        \n        # Apply test-case specific modifications\n        for snp_id, modifications in case_params['mods'].items():\n            for snp in current_data:\n                if snp['id'] == snp_id:\n                    snp.update(modifications)\n                    break\n        \n        result = run_pipeline(current_data, ld_matrix, case_params)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}