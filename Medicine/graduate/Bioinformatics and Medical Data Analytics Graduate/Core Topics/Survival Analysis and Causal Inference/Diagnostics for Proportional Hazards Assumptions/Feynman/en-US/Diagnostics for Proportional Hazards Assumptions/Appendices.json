{
    "hands_on_practices": [
        {
            "introduction": "Assessing the proportional hazards assumption can begin with intuitive graphical checks. While a direct plot of Kaplan-Meier survival curves can be misleading, a carefully chosen transformation can linearize the relationship between groups under the proportional hazards assumption. This exercise  challenges you to derive and apply the complementary log-log transformation, turning the multiplicative nature of the hazard ratio into an additive, constant separation between curves, providing a powerful visual diagnostic tool.",
            "id": "4555957",
            "problem": "In a clinical genomics study in oncology, investigators analyze time-to-progression in patients stratified by a binary biomarker group defined by messenger ribonucleic acid (mRNA) expression (high versus low). The time-to-event variable is denoted by $T$, right-censoring is present, and the scientific question concerns assessing the Proportional Hazards (PH) assumption as it would apply in a Cox proportional hazards model. The investigators plan to use the Kaplan–Meier (KM) method to estimate group-specific survival functions $\\hat S_g(t)$ for each biomarker group $g$. Starting from the core definitions of survival analysis, namely the survival function $S(t) = \\Pr(T > t)$, the hazard function $h(t)$, and the cumulative hazard function $H(t)$, and using only nonparametric estimation together with monotone transformations, the goal is to translate the group-specific survival estimates into a graphical diagnostic for the PH assumption that yields a pattern interpretable as a group effect that does not vary with $t$. Which option best describes the correct construction of this diagnostic using transformed scales and how to interpret it under the PH assumption, while remaining faithful to KM-based estimation and right-censoring?\n\nA. For each group $g$, compute the KM estimate $\\hat S_g(t)$ on a common event-time grid. Then plot the complementary log-log transformation $y_g(t) = \\log\\!\\big(-\\log \\hat S_g(t)\\big)$ against $\\log t$ for all groups. Under PH, the curves $y_g(t)$ are expected to be approximately parallel across groups because the group effect enters as a constant vertical shift with respect to $t$. The constancy of the vertical gap across $t$ is interpretable as evidence of a time-invariant hazard ratio. Use the same $t$ axis for all groups, apply standard right-censoring handling intrinsic to KM, and avoid over-interpretation in regions with sparse risk sets.\n\nB. For each group $g$, compute $\\hat S_g(t)$ and plot $\\hat S_g(t)$ against $t$. Under PH, the curves should display a constant vertical gap across $t$; this gap can be read as the hazard ratio. Because KM already accounts for right-censoring, no further transformation is needed.\n\nC. For each group $g$, estimate the group-specific hazard function as $-\\frac{d}{dt}\\log \\hat S_g(t)$ and plot this against $t$. Under PH, the hazards should align after rescaling the time axis, so coincident curves confirm PH.\n\nD. For each group $g$, estimate the Nelson–Aalen cumulative hazard $\\hat H_g(t)$ and then plot $\\log \\hat H_g(t)$ against $t$. Under PH, the curves should be parallel with a constant vertical gap across $t$, which directly estimates the log hazard ratio; this approach is preferred to KM because it targets hazards.\n\nE. For each group $g$, compute $\\hat S_g(t)$ and plot $\\log \\hat S_g(t)$ against $\\log t$. Under PH, curves should be parallel because the proportionality of hazards implies proportionality of survival on the log scale, yielding a constant vertical gap interpretable as the log hazard ratio.",
            "solution": "### Derivation from First Principles\nThe objective is to find a transformation of the Kaplan-Meier (KM) survival function estimate, $\\hat{S}_g(t)$, that results in a graphical representation where parallel lines indicate that the proportional hazards (PH) assumption holds. We begin with the core definitions.\n\nLet $h_g(t)$ be the hazard function for group $g$, $H_g(t) = \\int_0^t h_g(u)du$ be the cumulative hazard function, and $S_g(t) = \\Pr(T > t)$ be the survival function. The fundamental relationship between these is $S_g(t) = \\exp(-H_g(t))$.\n\nThe Proportional Hazards (PH) assumption states that the hazard function for one group is a constant multiple of the hazard function for a reference group (let's denote it by subscript $0$). For any other group $g$, we have:\n$$h_g(t) = \\lambda_g \\cdot h_0(t)$$\nwhere $\\lambda_g$ is the hazard ratio, a constant that does not depend on time $t$.\n\nTo see how this assumption translates to other functions, we first integrate the hazard function to get the cumulative hazard:\n$$H_g(t) = \\int_0^t h_g(u)du = \\int_0^t \\lambda_g \\cdot h_0(u)du = \\lambda_g \\int_0^t h_0(u)du = \\lambda_g \\cdot H_0(t)$$\nThis shows that under the PH assumption, the cumulative hazard functions are also proportional.\n\nTo create a graphical diagnostic with parallel lines, we need an additive relationship. Taking the natural logarithm of both sides of the cumulative hazard proportionality equation yields:\n$$\\log H_g(t) = \\log(\\lambda_g \\cdot H_0(t)) = \\log H_0(t) + \\log \\lambda_g$$\nThis equation shows that if we plot $\\log H_g(t)$ against some function of time (e.g., $t$ or $\\log t$), the curves for different groups $g$ will be parallel, separated by a constant vertical distance of $\\log \\lambda_g$ (the log-hazard ratio).\n\nThe problem specifies starting with the Kaplan-Meier estimate of the survival function, $\\hat{S}_g(t)$. We must now relate the derived $\\log H_g(t)$ form to $S_g(t)$. From the definition $S_g(t) = \\exp(-H_g(t))$, we can solve for $H_g(t)$:\n$$ \\log S_g(t) = -H_g(t) \\implies H_g(t) = -\\log S_g(t) $$\nNote that since $S_g(t) \\in [0, 1]$, its logarithm $\\log S_g(t)$ is non-positive.\n\nNow, substitute this expression for $H_g(t)$ into our additive relationship:\n$$ \\log(-\\log S_g(t)) = \\log(-\\log S_0(t)) + \\log \\lambda_g $$\nThis transformation, $y(t) = \\log(-\\log S(t))$, is known as the complementary log-log (cloglog) transformation. This derivation proves that if the PH assumption holds, plotting the cloglog-transformed survival functions for different groups against a time-axis (like $t$ or $\\log t$) should produce approximately parallel curves. The constant vertical separation between the curve for group $g$ and the reference group $0$ is an estimate of the log-hazard ratio, $\\log \\lambda_g$.\n\nThe procedure is thus:\n1. For each group $g$, compute the nonparametric Kaplan-Meier estimate $\\hat{S}_g(t)$. This method correctly handles right-censored data.\n2. Apply the complementary log-log transformation to each estimate: $\\hat{y}_g(t) = \\log(-\\log \\hat{S}_g(t))$.\n3. Plot $\\hat{y}_g(t)$ versus $t$ or $\\log t$ for all groups on the same axes. The choice of $\\log t$ for the x-axis is common as it can help visualize behavior at early time points more clearly.\n4. Assess if the resulting curves are reasonably parallel. Parallelism supports the PH assumption, while crossing or systematically diverging/converging curves suggest a violation.\n\n### Evaluation of Options\n\nLet's evaluate each option based on this derivation.\n\n**A. For each group $g$, compute the KM estimate $\\hat S_g(t)$ on a common event-time grid. Then plot the complementary log-log transformation $y_g(t) = \\log\\!\\big(-\\log \\hat S_g(t)\\big)$ against $\\log t$ for all groups. Under PH, the curves $y_g(t)$ are expected to be approximately parallel across groups because the group effect enters as a constant vertical shift with respect to $t$. The constancy of the vertical gap across $t$ is interpretable as evidence of a time-invariant hazard ratio. Use the same $t$ axis for all groups, apply standard right-censoring handling intrinsic to KM, and avoid over-interpretation in regions with sparse risk sets.**\nThis option perfectly matches our derivation. It specifies the correct transformation, the correct plot, the correct interpretation of parallel lines as a constant vertical shift representing a time-invariant hazard ratio, and includes appropriate methodological cautions.\n**Verdict: Correct.**\n\n**B. For each group $g$, compute $\\hat S_g(t)$ and plot $\\hat S_g(t)$ against $t$. Under PH, the curves should display a constant vertical gap across $t$; this gap can be read as the hazard ratio. Because KM already accounts for right-censoring, no further transformation is needed.**\nThis describes a standard plot of KM curves. Under PH, $S_g(t) = [S_0(t)]^{\\lambda_g}$. The vertical gap is $S_0(t) - S_g(t) = S_0(t) - [S_0(t)]^{\\lambda_g}$, which is a function of $t$ and is not constant. Therefore, the curves are not parallel. Furthermore, the gap does not represent the hazard ratio.\n**Verdict: Incorrect.**\n\n**C. For each group $g$, estimate the group-specific hazard function as $-\\frac{d}{dt}\\log \\hat S_g(t)$ and plot this against $t$. Under PH, the hazards should align after rescaling the time axis, so coincident curves confirm PH.**\nWhile $h(t) = -\\frac{d}{dt}\\log S(t)$ is formally correct, estimating the hazard function $h(t)$ (a derivative) from a non-parametric step function $\\hat{S}_g(t)$ is highly unstable and not typically done directly. More critically, the PH assumption $h_g(t) = \\lambda_g \\cdot h_0(t)$ implies that the hazard curves are *proportional*, not that they align after a time-axis rescaling. Time-axis rescaling corresponds to an Accelerated Failure Time (AFT) model, which is a different class of model.\n**Verdict: Incorrect.**\n\n**D. For each group $g$, estimate the Nelson–Aalen cumulative hazard $\\hat H_g(t)$ and then plot $\\log \\hat H_g(t)$ against $t$. Under PH, the curves should be parallel with a constant vertical gap across $t$, which directly estimates the log hazard ratio; this approach is preferred to KM because it targets hazards.**\nThis describes a valid alternative diagnostic plot. As shown in the derivation, plotting $\\log H_g(t)$ versus time should yield parallel lines under PH. The Nelson-Aalen estimator is the proper tool for estimating $H_g(t)$ nonparametrically. However, the problem explicitly constrains the method to \"using the Kaplan–Meier (KM) method to estimate ... $\\hat{S}_g(t)$\" and \"translat[ing] the group-specific survival estimates\". Option A follows this constraint by transforming $\\hat{S}_g(t)$ itself. Option D deviates by proposing the use of a different primary estimator, the Nelson-Aalen estimator.\n**Verdict: Incorrect.**\n\n**E. For each group $g$, compute $\\hat S_g(t)$ and plot $\\log \\hat S_g(t)$ against $\\log t$. Under PH, curves should be parallel because the proportionality of hazards implies proportionality of survival on the log scale, yielding a constant vertical gap interpretable as the log hazard ratio.**\nUnder PH, we have $\\log S_g(t) = \\lambda_g \\cdot \\log S_0(t)$. This is a relationship of proportionality, not a constant additive difference (parallelism). The vertical gap on a plot of $\\log S_g(t)$ vs. $\\log t$ would be $\\log S_g(t) - \\log S_0(t) = (\\lambda_g - 1)\\log S_0(t)$, which is dependent on time through $S_0(t)$ and therefore not constant. The curves will not be parallel unless $\\lambda_g=1$.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Real-world data analysis rarely involves checking a single assumption in isolation; misspecification of one aspect of a model can create spurious violations in another. This is particularly true for the interplay between a covariate's functional form and the proportional hazards (PH) assumption. This practice  guides you through a robust, two-step diagnostic workflow, demonstrating how to first use martingale residuals to ensure covariates are modeled correctly before proceeding to assess the PH assumption with Schoenfeld residuals.",
            "id": "4555951",
            "problem": "A translational genomics team analyzes time-to-event outcomes in a cohort where the endpoint is time from enrollment to onset of a cardiovascular event. The working model is a Cox proportional hazards model with a continuous messenger ribonucleic acid (mRNA) expression score, denoted by $x_{1}$, a continuous inflammation biomarker, denoted by $x_{2}$, and standard clinical covariates (e.g., age, sex, comorbidity index). The hazard for an individual with covariate vector $x$ at time $t$ is modeled as $h(t \\mid x) = h_{0}(t)\\exp\\{x^{\\top}\\beta\\}$ under the proportional hazards assumption that the log hazard ratio $\\log\\{h(t \\mid x) / h(t \\mid x^{\\prime})\\}$ is constant in $t$ for any two covariate vectors $x$ and $x^{\\prime}$. The team is concerned that the continuous covariates $x_{1}$ and $x_{2}$ may not enter linearly on the log hazard scale and that misspecification of their functional forms could confound diagnostics for the proportional hazards assumption.\n\nBased only on the core definitions of hazard functions, partial likelihood estimation in the Cox model, and residuals as observed-minus-expected summaries under a fitted model, which of the following $2$-step diagnostic procedures most appropriately (i) addresses functional form misspecification for $x_{1}$ and $x_{2}$ using martingale residuals and (ii) reassesses the proportional hazards assumption using Schoenfeld residuals after refitting the model?\n\nA. Fit the Cox model with $x_{1}$ and $x_{2}$ entered linearly. For each continuous covariate, plot subject-level martingale residuals against the covariate and add a smooth. If a systematic pattern indicates nonlinearity, refit the model replacing the linear term with a flexible function (e.g., a restricted cubic spline) until the martingale residuals versus that covariate fluctuate randomly around zero without trend. Then, compute scaled Schoenfeld residuals for each covariate from the refitted model and test their independence from time (e.g., via a rank-based regression of residuals on a function of $t$) and inspect residual-vs-time smooths. If time dependence remains, consider a time-by-covariate interaction or stratification to address non-proportional hazards.\n\nB. Fit the Cox model with $x_{1}$ and $x_{2}$ entered linearly. First test proportional hazards by regressing scaled Schoenfeld residuals on time; only if a covariate shows time dependence, transform that covariate (e.g., logarithm for $x_{2}$) to reduce correlation with time. Because Schoenfeld residuals are invariant to functional misspecification, it is unnecessary to revisit martingale residuals after transforming.\n\nC. Fit the Cox model and compute deviance residuals; use their symmetry around zero to assess non-proportional hazards. Then, for covariates with skewed deviance residuals, check Schoenfeld residuals versus the covariate value to diagnose functional form and add quadratic terms if needed.\n\nD. Use a parametric accelerated failure time model to diagnose functional form by plotting score residuals against $x_{1}$ and $x_{2}$ and adjust their forms in that model. Then switch back to the Cox proportional hazards model and compute Schoenfeld residuals to assess proportional hazards without refitting the Cox model with the adjusted functional forms.",
            "solution": "### Principle-Based Derivation\nThe core of this problem lies in understanding the distinct roles of different types of residuals in diagnosing potential misspecifications in a Cox proportional hazards model. A critical principle in model building and diagnostics is to address more fundamental assumptions before proceeding to more specific ones. The functional form of covariates is a more fundamental aspect of the model specification, $x^{\\top}\\beta$, than the proportional hazards assumption, which relates the specified model to time. Misspecification of the functional form can induce spurious evidence of non-proportional hazards. Therefore, the logical diagnostic sequence is:\n1.  Verify the functional form of the covariates.\n2.  After ensuring the functional form is reasonably correct (and refitting the model if necessary), verify the proportional hazards assumption.\n\nWe must now evaluate the tools for each step.\n-   **Martingale Residuals**: The martingale residual for subject $i$ is defined as $M_i = \\delta_i - \\hat{E}_i$, where $\\delta_i$ is the event indicator ($1$ for an event, $0$ for censoring) and $\\hat{E}_i$ is the expected number of events for that subject under the fitted model. These residuals represent the \"excess\" number of events experienced by a subject compared to the model's expectation. A plot of martingale residuals against a covariate $x_k$ is the standard tool to check for nonlinearity. A systematic pattern (e.g., a curve) in this plot suggests that the assumed linear relationship between $x_k$ and the log-hazard is incorrect. The correct response is to adjust the functional form, for instance by using polynomial terms, transformations, or splines.\n-   **Schoenfeld Residuals**: These are defined for each covariate at each event time. The fundamental property is that, under the PH assumption, the expected value of the Schoenfeld residuals for a given covariate is constant over time (specifically, $0$). Therefore, a plot of Schoenfeld residuals against time should show a random scatter around a horizontal line at zero. A non-zero slope or other systematic trend indicates a violation of the PH assumption for that covariate.\n\nBased on this, the correct procedure is to first use martingale residuals to check and fix the functional form of continuous covariates like $x_1$ and $x_2$. After refitting the model with the corrected functional forms, one should then use Schoenfeld residuals from this new, improved model to assess the proportional hazards assumption.\n\n### Option-by-Option Analysis\n\n**A.** This option proposes a sequence:\n1.  Fit an initial linear model. Plot martingale residuals against each continuous covariate ($x_{1}$, $x_{2}$) to check for nonlinearity.\n2.  If nonlinearity is detected, refit the model with a more flexible functional form (like a restricted cubic spline).\n3.  Using this refitted model, compute scaled Schoenfeld residuals and test their independence from time to assess the proportional hazards assumption.\n4.  If non-proportionality is found, consider appropriate remedies like time-by-covariate interactions.\n\nThis sequence is perfectly aligned with the established statistical principles. It addresses functional form first using the correct diagnostic (martingale residuals) and then assesses proportional hazards on the improved model using its specific diagnostic (Schoenfeld residuals).\n**Verdict: Correct.**\n\n**B.** This option proposes:\n1.  Test for proportional hazards first using Schoenfeld residuals.\n2.  It claims that transforming a covariate is the remedy for time-dependence in Schoenfeld residuals. This is a confusion of purpose; transformations address functional form, while time-interactions address non-proportionality.\n3.  It makes the factually incorrect statement that \"Schoenfeld residuals are invariant to functional misspecification.\" As explained, functional form misspecification can induce apparent non-proportionality, meaning Schoenfeld residuals are indeed affected.\n\nThis procedure reverses the correct diagnostic order and is based on a false premise.\n**Verdict: Incorrect.**\n\n**C.** This option proposes:\n1.  Use deviance residuals to assess non-proportional hazards. This is incorrect; while deviance residuals are useful for overall goodness-of-fit and outlier detection, Schoenfeld residuals are the specific tool for assessing the PH assumption.\n2.  Use Schoenfeld residuals plotted against the *covariate value* to diagnose functional form. This is also incorrect. Martingale residuals are plotted against the covariate to check functional form. Schoenfeld residuals are plotted against *time* to check the PH assumption.\n\nThis option fundamentally confuses the application of different residual types.\n**Verdict: Incorrect.**\n\n**D.** This option proposes a convoluted workflow:\n1.  Switch to a different model family, the accelerated failure time (AFT) model, to diagnose functional form. The optimal functional form for an AFT model (where covariates scale time) is not necessarily the same as for a Cox model (where covariates scale hazard). This is not a standard or direct way to diagnose the Cox model.\n2.  Compute Schoenfeld residuals from the original Cox model \"without refitting the Cox model with the adjusted functional forms.\" This is logically flawed. If a model is believed to be misspecified in its functional form, any subsequent diagnostic (like a PH test) performed on that same flawed model is unreliable. One must always refit the model after making a correction before proceeding to the next diagnostic step.\n\nThis procedure is illogical, mixes model families inappropriately, and omits the crucial step of refitting the model.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "When diagnostics reveal a violation of the proportional hazards assumption, a powerful strategy is to explicitly model the time-varying nature of a covariate's effect. This practice  takes you to the core of survival analysis by tasking you with implementing a Cox model with a time-dependent covariate from first principles. By simulating data and building your own partial likelihood optimizer, you will gain a profound understanding of how a term like $x \\times \\log t$ both diagnoses and quantifies departures from proportional hazards.",
            "id": "4555902",
            "problem": "You are given the task of implementing, from first principles, a maximum partial-likelihood estimator for a Cox proportional hazards (PH) model extended with a time-varying component to diagnose violations of the proportional hazards assumption. The scientific base you must strictly use is: (i) the definition of the Cox proportional hazards (PH) model with possibly time-dependent covariates, namely that the hazard is modeled as $\\lambda(t \\mid Z(t)) = \\lambda_0(t) \\exp\\{\\beta^\\top Z(t)\\}$ for a nonparametric baseline hazard $\\lambda_0(t)$ and a finite-dimensional parameter $\\beta$, and (ii) the Cox partial likelihood constructed from risk sets at observed event times. Assume independent right-censoring is absent so that every observation is an event time, and assume there is no left-truncation.\n\nYou must implement a self-contained program that:\n- Simulates event-time data under a known data-generating mechanism where the hazard depends on a binary covariate $x \\in \\{0,1\\}$ and a time-varying interaction term $x \\log t$ in the linear predictor. Specifically, when $x=0$, the hazard is $\\lambda(t \\mid x=0) = c$ (a constant baseline hazard). When $x=1$, the hazard is $\\lambda(t \\mid x=1) = c \\exp(\\beta_x) t^{\\beta_{x \\log t}}$, where $c$ is a positive constant, $\\beta_x$ is the coefficient for $x$, and $\\beta_{x \\log t}$ is the coefficient for the interaction term $x \\log t$. The covariate $x$ is independently drawn as $\\mathrm{Bernoulli}(0.5)$. You must generate event times via valid inverse-transform sampling using the cumulative hazard implied by the specified hazard.\n- Fits a Cox model with a time-dependent covariate vector $Z(t) = (x,\\; x \\log t)^\\top$ by maximizing the Cox partial log-likelihood over $\\beta = (\\beta_x,\\; \\beta_{x \\log t})^\\top$, using Breslow handling of ties and Newton–Raphson optimization. For each unique observed event time $t_j$, use the risk set $R_j = \\{i: T_i \\ge t_j\\}$ and evaluate $Z_i(t_j)$ at the event time $t_j$ for all $i \\in R_j$.\n- Returns, for each dataset, the sign of the estimated interaction coefficient $\\widehat{\\beta}_{x \\log t}$ as an integer in $\\{-1, 0, +1\\}$ according to the following rule: return $0$ if $\\lvert \\widehat{\\beta}_{x \\log t} \\rvert < \\tau$, return $+1$ if $\\widehat{\\beta}_{x \\log t} \\ge \\tau$, and return $-1$ if $\\widehat{\\beta}_{x \\log t} \\le -\\tau$, where $\\tau$ is a small positive tolerance given below. This integer encodes the inferred direction of deviation from proportional hazards: $+1$ indicates that the effect of $x$ strengthens over time, $-1$ indicates that it weakens over time, and $0$ indicates no detectable deviation within tolerance.\n\nYour implementation must rely only on the model definition and partial likelihood construction. You must derive and implement the score and observed information for Newton–Raphson without using pre-built survival analysis libraries. Use Breslow’s approximation for ties. Use natural logarithms for $\\log t$.\n\nUse the following fixed test suite of parameter settings. For each case, simulate the data set and then fit the model as specified above. In every case, you must set the random seed to the given value before simulating in order to make the result deterministic.\n\n- Case A (proportional hazards holds):\n  - Sample size $n = 4000$\n  - Baseline hazard $c = 0.05$\n  - Coefficient for $x$: $\\beta_x = 0.4$\n  - Coefficient for $x \\log t$: $\\beta_{x \\log t} = 0$\n  - Random seed $= 123$\n- Case B (effect of $x$ increases over time):\n  - Sample size $n = 2000$\n  - Baseline hazard $c = 0.05$\n  - Coefficient for $x$: $\\beta_x = 0.2$\n  - Coefficient for $x \\log t$: $\\beta_{x \\log t} = 0.6$\n  - Random seed $= 456$\n- Case C (effect of $x$ decreases over time):\n  - Sample size $n = 2000$\n  - Baseline hazard $c = 0.05$\n  - Coefficient for $x$: $\\beta_x = 0.2$\n  - Coefficient for $x \\log t$: $\\beta_{x \\log t} = -0.6$\n  - Random seed $= 789$\n\nImplementation details and constraints:\n- Use inverse-transform sampling for event times implied by the specified hazards. For $x \\in \\{0,1\\}$ with $x=0$ or $x=1$, the cumulative hazard must be integrated exactly and inverted to obtain a valid generator of positive event times.\n- Implement Newton–Raphson with a reasonable convergence stopping rule based on either the parameter increment norm or the score norm, and a modest maximum number of iterations to guarantee termination. If needed, apply simple step halving when the partial log-likelihood decreases.\n- Use Breslow’s approximation to handle tied event times in both the partial log-likelihood and its first and second derivatives.\n- Use natural logarithms for all occurrences of $\\log$.\n\nTolerance and output:\n- Use the classification tolerance $\\tau = 0.05$.\n- Your program should produce a single line of output containing the three integer results corresponding to Cases A, B, and C, in that order, as a comma-separated list enclosed in square brackets (for example, $\\left[\\text{result}_A,\\text{result}_B,\\text{result}_C\\right]$). The integers must be exactly $-1$, $0$, or $+1$ as defined above.\n\nAll answers are unitless real numbers where applicable; no physical units are involved. Angles are not involved. The final outputs are integers as specified, so no percentages are required or permitted. The problem is designed to be universally applicable and solvable using any modern programming language that supports basic linear algebra and numerical optimization, strictly following the algorithmic specifications above.",
            "solution": "### 1. Theoretical Foundation\n\nThe Cox proportional hazards model describes the hazard rate $\\lambda(t)$ for an individual at time $t$ with a covariate vector $Z(t)$ as:\n$$\n\\lambda(t \\mid Z(t)) = \\lambda_0(t) \\exp\\{\\beta^\\top Z(t)\\}\n$$\nHere, $\\lambda_0(t)$ is an arbitrary, non-negative baseline hazard function, and $\\beta$ is a vector of regression coefficients. The PH assumption posits that the ratio of hazards for any two individuals is constant over time. This assumption is violated if the covariates $Z(t)$ explicitly depend on time.\n\nTo test the PH assumption for a binary covariate $x \\in \\{0, 1\\}$, we can introduce an interaction term with a function of time, such as $\\log t$. The covariate vector becomes $Z(t) = (x, x \\log t)^\\top$ with a corresponding parameter vector $\\beta = (\\beta_x, \\beta_{x \\log t})^\\top$. The linear predictor is $\\eta(t) = \\beta_x x + \\beta_{x \\log t} (x \\log t)$. The hazard ratio between a subject with $x=1$ and a subject with $x=0$ is:\n$$\n\\frac{\\lambda(t \\mid x=1)}{\\lambda(t \\mid x=0)} = \\frac{\\lambda_0(t) \\exp(\\beta_x + \\beta_{x \\log t} \\log t)}{\\lambda_0(t) \\exp(0)} = \\exp(\\beta_x) t^{\\beta_{x \\log t}}\n$$\nIf $\\beta_{x \\log t} = 0$, the hazard ratio is constant ($\\exp(\\beta_x)$), and the PH assumption holds. If $\\beta_{x \\log t} \\neq 0$, the assumption is violated. A positive $\\beta_{x \\log t}$ indicates the effect of the covariate strengthens over time, while a negative value indicates it weakens. The sign of the estimated coefficient $\\widehat{\\beta}_{x \\log t}$ thus diagnoses the nature of the deviation from proportional hazards.\n\n### 2. Data Simulation via Inverse-Transform Sampling\n\nWe must generate event-time data from the specified data-generating mechanism. This is achieved via inverse-transform sampling, which relates a uniformly distributed random variable $U \\sim \\mathrm{Uniform}(0,1)$ to an event time $T$ via the cumulative hazard function $H(t) = \\int_0^t \\lambda(u) du$. The relationship is $T = H^{-1}(-\\log U)$. As $-\\log U$ follows an exponential distribution with rate $1$, we can write $T = H^{-1}(E)$ where $E \\sim \\mathrm{Exponential}(1)$.\n\nThe hazard functions are given for two strata based on the binary covariate $x \\sim \\mathrm{Bernoulli}(0.5)$:\n-   For $x=0$: The hazard is $\\lambda(t \\mid x=0) = c$.\n    The cumulative hazard is $H(t \\mid x=0) = \\int_0^t c \\,du = ct$.\n    Inverting this gives $H^{-1}(y) = y/c$. Thus, event times are sampled as $T = E/c$.\n-   For $x=1$: The hazard is $\\lambda(t \\mid x=1) = c \\exp(\\beta_x) t^{\\beta_{x \\log t}}$. Let $A = c \\exp(\\beta_x)$ and $p = \\beta_{x \\log t}$.\n    -   If $p \\neq -1$, the cumulative hazard is $H(t \\mid x=1) = \\int_0^t A u^p \\,du = \\frac{A}{p+1} t^{p+1}$.\n        Inverting this gives $H^{-1}(y) = \\left( \\frac{y(p+1)}{A} \\right)^{1/(p+1)}$. Thus, event times are sampled as $T = \\left( \\frac{E(p+1)}{A} \\right)^{1/(p+1)}$.\n    -   The special case $p=0$ (Case A) corresponds to a constant hazard $\\lambda(t \\mid x=1) = A$. The generator simplifies to $T = E/A$. This is consistent with the general formula as $p \\to 0$.\n\n### 3. Parameter Estimation via Newton-Raphson\n\nThe parameter vector $\\beta = (\\beta_x, \\beta_{x \\log t})^\\top$ is estimated by maximizing the Cox partial log-likelihood. With observed event times, no censoring, and allowing for ties, the data consists of pairs $(T_i, x_i)$ for $i=1, \\dots, n$. Let the $k$ distinct ordered event times be $T_{(1)} < T_{(2)} < \\dots < T_{(k)}$. Let $D_j$ be the set of individuals with an event at $T_{(j)}$, with size $d_j = |D_j|$. Let $R_j = \\{i: T_i \\ge T_{(j)}\\}$ be the risk set at time $T_{(j)}$.\n\nThe partial log-likelihood, using the Breslow approximation for ties, is:\n$$\n\\ell(\\beta) = \\sum_{j=1}^{k} \\left( \\sum_{l \\in D_j} \\beta^\\top Z_l(T_{(j)}) - d_j \\log \\left( \\sum_{i \\in R_j} \\exp\\{\\beta^\\top Z_i(T_{(j)})\\} \\right) \\right)\n$$\nwhere $Z_i(t) = (x_i, x_i \\log t)^\\top$.\n\nTo maximize $\\ell(\\beta)$, we use the Newton-Raphson algorithm, which iteratively updates an estimate $\\beta_m$ via:\n$$\n\\beta_{m+1} = \\beta_m + I(\\beta_m)^{-1} U(\\beta_m)\n$$\nwhere $U(\\beta) = \\frac{\\partial \\ell}{\\partial \\beta}$ is the score vector (gradient) and $I(\\beta) = -\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^\\top}$ is the observed information matrix (negative Hessian).\n\nLet us define the following sums over the risk set $R_j$ at time $T_{(j)}$:\n-   $S_j^{(0)}(\\beta) = \\sum_{i \\in R_j} \\exp\\{\\beta^\\top Z_i(T_{(j)})\\}$\n-   $S_j^{(1)}(\\beta) = \\sum_{i \\in R_j} Z_i(T_{(j)}) \\exp\\{\\beta^\\top Z_i(T_{(j)})\\}$\n-   $S_j^{(2)}(\\beta) = \\sum_{i \\in R_j} Z_i(T_{(j)}) Z_i(T_{(j)})^\\top \\exp\\{\\beta^\\top Z_i(T_{(j)})\\}$\n\nThe score vector is the first derivative of the log-likelihood:\n$$\nU(\\beta) = \\frac{\\partial \\ell}{\\partial \\beta} = \\sum_{j=1}^{k} \\left( \\sum_{l \\in D_j} Z_l(T_{(j)}) - d_j \\frac{S_j^{(1)}(\\beta)}{S_j^{(0)}(\\beta)} \\right)\n$$\nThe observed information matrix is the negative of the second derivative (Hessian):\n$$\nI(\\beta) = -\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^\\top} = \\sum_{j=1}^{k} d_j \\left[ \\frac{S_j^{(2)}(\\beta)}{S_j^{(0)}(\\beta)} - \\left(\\frac{S_j^{(1)}(\\beta)}{S_j^{(0)}(\\beta)}\\right) \\left(\\frac{S_j^{(1)}(\\beta)}{S_j^{(0)}(\\beta)}\\right)^\\top \\right]\n$$\n\n### 4. Implementation Algorithm\n\nThe overall algorithm proceeds as follows for each test case:\n1.  **Data Generation**: For the given parameters ($n, c, \\beta_x, \\beta_{x \\log t}$) and random seed, generate $n$ covariate values $x_i$ and corresponding event times $T_i$ using the inverse-transform sampling method described above.\n2.  **Data Preparation**: Sort the data pairs $(T_i, x_i)$ by event time $T_i$. Identify the unique event times $T_{(j)}$ and their frequencies (tie counts) $d_j$.\n3.  **Newton-Raphson Iteration**:\n    a. Initialize the parameter vector $\\beta = (0, 0)^\\top$.\n    b. Iterate for a maximum number of steps:\n        i. Compute the log-likelihood $\\ell(\\beta)$, score vector $U(\\beta)$, and information matrix $I(\\beta)$ by summing contributions from each unique event time $T_{(j)}$. This involves calculating $S_j^{(0)}, S_j^{(1)}, S_j^{(2)}$ for each risk set $R_j$.\n        ii. Check for convergence by testing if the norm of the score vector, $\\|U(\\beta)\\|$, is below a small tolerance (e.g., $10^{-6}$). If so, terminate and return the current $\\beta$.\n        iii. Calculate the Newton-Raphson step: $\\Delta\\beta = I(\\beta)^{-1} U(\\beta)$. This is solved as a linear system $I(\\beta) \\Delta\\beta = U(\\beta)$.\n        iv. **Step Halving**: To ensure the likelihood increases at each step, apply a backtracking line search. Start with a step size $\\alpha=1$. If $\\ell(\\beta + \\alpha \\Delta\\beta) \\le \\ell(\\beta)$, halve $\\alpha$ and repeat. Update $\\beta \\leftarrow \\beta + \\alpha \\Delta\\beta$ once an improvement is found. If $\\alpha$ becomes too small, terminate to avoid non-convergence.\n\n### 5. Output Classification\n\nAfter the Newton-Raphson algorithm converges to an estimate $\\widehat{\\beta} = (\\widehat{\\beta}_x, \\widehat{\\beta}_{x \\log t})^\\top$, we examine the sign of the interaction coefficient $\\widehat{\\beta}_{x \\log t}$ to diagnose the violation of the PH assumption. Using the given tolerance $\\tau = 0.05$, the result is classified as:\n-   $+1$ if $\\widehat{\\beta}_{x \\log t} \\ge \\tau$ (effect strengthens over time)\n-   $-1$ if $\\widehat{\\beta}_{x \\log t} \\le -\\tau$ (effect weakens over time)\n-   $0$ if $|\\widehat{\\beta}_{x \\log t}| < \\tau$ (no significant violation detected)\n\nThis integer result is computed for each of the three test cases, and the final output is a list of these three integers.",
            "answer": "```python\nimport numpy as np\n\ndef _calculate_loglik_derivatives(beta, T_sorted, x_sorted, unique_times, tie_first_indices, tie_counts, compute_derivatives=True):\n    \"\"\"\n    Calculates the partial log-likelihood, score vector, and information matrix.\n    \"\"\"\n    log_lik = 0.0\n    score = np.zeros(2) if compute_derivatives else None\n    information = np.zeros((2, 2)) if compute_derivatives else None\n\n    # Iterate through unique event times\n    for j in range(len(unique_times)):\n        t_j = unique_times[j]\n        d_j = tie_counts[j]\n        risk_set_start_idx = tie_first_indices[j]\n        \n        risk_set_x = x_sorted[risk_set_start_idx:]\n        death_set_x = x_sorted[risk_set_start_idx : risk_set_start_idx + d_j]\n        \n        log_t_j = np.log(t_j)\n        \n        # Covariate Z(t_j) for the entire risk set\n        Z_risk = np.zeros((len(risk_set_x), 2))\n        Z_risk[:, 0] = risk_set_x\n        Z_risk[:, 1] = risk_set_x * log_t_j\n        \n        # Linear predictor eta = beta^T * Z for risk set\n        eta_risk = Z_risk @ beta\n        \n        # Risk scores exp(eta)\n        exp_eta_risk = np.exp(eta_risk)\n        \n        S0 = np.sum(exp_eta_risk)\n        if S0 == 0:\n            continue\n        \n        # Covariate Z(t_j) for the death set\n        Z_deaths = np.zeros((d_j, 2))\n        Z_deaths[:, 0] = death_set_x\n        Z_deaths[:, 1] = death_set_x * log_t_j\n        \n        # Log-likelihood contribution\n        eta_deaths = Z_deaths @ beta\n        log_lik += np.sum(eta_deaths) - d_j * np.log(S0)\n        \n        if compute_derivatives:\n            # S1 = sum(Z * exp(eta)) over risk set\n            S1 = (Z_risk.T @ exp_eta_risk)\n            # S2 = sum(Z * Z.T * exp(eta)) over risk set\n            S2 = (Z_risk.T * exp_eta_risk) @ Z_risk\n            \n            E_j = S1 / S0\n            \n            # Score vector contribution\n            sum_Z_deaths = np.sum(Z_deaths, axis=0)\n            score += sum_Z_deaths - d_j * E_j\n            \n            # Information matrix contribution\n            V_j = (S2 / S0) - np.outer(E_j, E_j)\n            information += d_j * V_j\n\n    return log_lik, score, information\n\ndef _fit_cox_model(T, x, max_iter=50, tol=1e-6, min_alpha=1e-5):\n    \"\"\"\n    Fits the Cox model with time-dependent covariates using Newton-Raphson.\n    \"\"\"\n    # Sort data by event time\n    sort_indices = np.argsort(T)\n    T_sorted = T[sort_indices]\n    x_sorted = x[sort_indices]\n\n    # Get unique times and tie structure\n    unique_times, tie_first_indices, tie_counts = np.unique(T_sorted, return_index=True, return_counts=True)\n    \n    beta = np.zeros(2)\n\n    for _ in range(max_iter):\n        log_lik, score, information = _calculate_loglik_derivatives(beta, T_sorted, x_sorted, unique_times, tie_first_indices, tie_counts)\n\n        if np.linalg.norm(score)  tol:\n            break\n\n        try:\n            delta_beta = np.linalg.solve(information, score)\n        except np.linalg.LinAlgError:\n            return beta \n\n        alpha = 1.0\n        while alpha > min_alpha:\n            beta_new = beta + alpha * delta_beta\n            new_log_lik, _, _ = _calculate_loglik_derivatives(beta_new, T_sorted, x_sorted, unique_times, tie_first_indices, tie_counts, compute_derivatives=False)\n            \n            if np.isfinite(new_log_lik) and new_log_lik > log_lik:\n                beta = beta_new\n                break\n            alpha /= 2\n        \n        if alpha = min_alpha:\n            break\n    \n    return beta\n\ndef _simulate_data(n, c, beta_x, beta_xt, seed):\n    \"\"\"\n    Simulates event time data based on the specified hazard functions.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    x = rng.binomial(1, 0.5, n)\n    T = np.zeros(n)\n    \n    # Generate standard exponential random variables\n    E = rng.exponential(1, n)\n\n    # Stratum x=0\n    mask0 = (x == 0)\n    T[mask0] = E[mask0] / c\n\n    # Stratum x=1\n    mask1 = (x == 1)\n    A = c * np.exp(beta_x)\n    p = beta_xt\n\n    # Handle the two sub-cases for x=1\n    if np.abs(p)  1e-9: # Proportional hazards subcase\n        T[mask1] = E[mask1] / A\n    else: # Non-proportional hazards subcase\n        T[mask1] = (E[mask1] * (p + 1) / A) ** (1 / (p + 1))\n    \n    return T, x\n\ndef _process_case(n, c, beta_x, beta_xt, seed, tau):\n    \"\"\"\n    Runs one full cycle: simulation, fitting, and classification.\n    \"\"\"\n    T, x = _simulate_data(n, c, beta_x, beta_xt, seed)\n    beta_hat = _fit_cox_model(T, x)\n    beta_xt_hat = beta_hat[1]\n    \n    if beta_xt_hat >= tau:\n        return 1\n    elif beta_xt_hat = -tau:\n        return -1\n    else:\n        return 0\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Case A: Proportional hazards\n        {'n': 4000, 'c': 0.05, 'beta_x': 0.4, 'beta_xt': 0.0, 'seed': 123},\n        # Case B: Increasing effect\n        {'n': 2000, 'c': 0.05, 'beta_x': 0.2, 'beta_xt': 0.6, 'seed': 456},\n        # Case C: Decreasing effect\n        {'n': 2000, 'c': 0.05, 'beta_x': 0.2, 'beta_xt': -0.6, 'seed': 789},\n    ]\n    \n    tau = 0.05\n    results = []\n    \n    for case in test_cases:\n        result = _process_case(case['n'], case['c'], case['beta_x'], case['beta_xt'], case['seed'], tau)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}