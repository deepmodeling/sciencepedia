{
    "hands_on_practices": [
        {
            "introduction": "The Expect-value ($E$-value) is a cornerstone of sequence alignment statistics, yet its magnitude is not an absolute property of an alignment; it is critically dependent on the size of the search space. This exercise explores the direct, linear relationship between the database size and the $E$-value for a fixed alignment bit score. By deriving the $E$-value formula from first principles and calculating its change across databases of different scales , you will gain a fundamental intuition for why an alignment's significance can change dramatically depending on the search context, such as comparing a search in a targeted pathogen database versus a vast metagenomic one.",
            "id": "4538924",
            "problem": "A research group is performing protein homology searches in two practical settings: a targeted clinical pathogen database and a comprehensive metagenomic database. The same protein query of length $m = 1200$ amino acids produces a reported Basic Local Alignment Search Tool (BLAST) bit score $S' = 65$ for its top-scoring alignment to a specific subject sequence present in both databases. The pathogen database has total length $n_{A} = 5.0 \\times 10^{7}$ amino acids, and the metagenomic database has total length $n_{B} = 3.0 \\times 10^{9}$ amino acids. Assume the Karlin–Altschul model for local alignment scores applies with fixed statistical parameters for the scoring system used, and that effective lengths can be taken equal to the actual lengths for this setup.\n\nStarting from the foundational definitions associated with the Karlin–Altschul statistics and the BLAST bit score, derive the expression for the expected number of random high-scoring alignments (E-value) in terms of the query length $m$, database length $n$, and the bit score $S'$. Then, using that expression, quantify how the E-value changes when moving from Database $A$ to Database $B$ by computing the fold-change $\\frac{E_{B}}{E_{A}}$.\n\nProvide the final numeric value for $\\frac{E_{B}}{E_{A}}$ and round your answer to four significant figures. No units are required in the final answer.",
            "solution": "The problem requires the derivation of the formula for the BLAST E-value in terms of query length, database length, and bit score, and then using this formula to calculate the fold-change in E-value when moving from a smaller database to a larger one.\n\nThe analysis begins with the foundational Karlin–Altschul statistical model. The expected number of random alignments (the E-value, $E$) with a raw score $S$ greater than or equal to a threshold score $x$ is given by:\n$$E = K m' n' \\exp(-\\lambda x)$$\nIn this equation, $K$ and $\\lambda$ are statistical parameters determined by the scoring matrix and the background amino acid frequencies. The terms $m'$ and $n'$ represent the effective lengths of the query and the database, respectively. The problem statement allows for the simplification that the effective lengths are equal to the actual sequence lengths, thus $m' = m$ and $n' = n$. For a specific alignment with an observed raw score $S$, the E-value is:\n$$E = K m n \\exp(-\\lambda S)$$\n\nThe bit score, denoted as $S'$, is a normalized score that is independent of the parameters $K$ and $\\lambda$. The relationship between the raw score $S$ and the bit score $S'$ is defined as:\n$$S' = \\frac{\\lambda S - \\ln(K)}{\\ln(2)}$$\nTo derive the E-value expression in terms of the bit score $S'$, we first need to express the term $\\exp(-\\lambda S)$ using $S'$. We rearrange the bit score equation to solve for $\\lambda S$:\n$$S' \\ln(2) = \\lambda S - \\ln(K)$$\n$$\\lambda S = S' \\ln(2) + \\ln(K)$$\nNow, we can substitute this expression into the exponent of the E-value equation:\n$$E = K m n \\exp(-(S' \\ln(2) + \\ln(K)))$$\nUsing the property of exponents $\\exp(a+b) = \\exp(a)\\exp(b)$, we can separate the terms:\n$$E = K m n \\exp(-S' \\ln(2)) \\exp(-\\ln(K))$$\nThe term $\\exp(-\\ln(K))$ simplifies to $\\exp(\\ln(K^{-1}))$, which is equal to $K^{-1}$ or $\\frac{1}{K}$. Substituting this back into the equation gives:\n$$E = K m n \\exp(-S' \\ln(2)) \\frac{1}{K}$$\nThe parameter $K$ cancels out, leaving:\n$$E = m n \\exp(-S' \\ln(2))$$\nUsing another property of logarithms and exponents, $a \\ln(b) = \\ln(b^a)$ and $\\exp(\\ln(x)) = x$, we can simplify the exponential term:\n$$\\exp(-S' \\ln(2)) = \\exp(\\ln(2^{-S'})) = 2^{-S'}$$\nThis leads to the final derived expression for the E-value in terms of the query length $m$, the database length $n$, and the bit score $S'$:\n$$E = m n 2^{-S'}$$\nThis completes the first part of the problem.\n\nFor the second part, we must compute the fold-change $\\frac{E_{B}}{E_{A}}$. The problem provides the following values:\nQuery length: $m = 1200$\nBit score: $S' = 65$\nDatabase A length: $n_{A} = 5.0 \\times 10^{7}$\nDatabase B length: $n_{B} = 3.0 \\times 10^{9}$\n\nUsing the derived formula, we can write the E-values for the alignment in each database. The query length $m$ and the bit score $S'$ are constant for both cases.\nFor Database A:\n$$E_{A} = m n_{A} 2^{-S'}$$\nFor Database B:\n$$E_{B} = m n_{B} 2^{-S'}$$\n\nThe fold-change is the ratio of $E_{B}$ to $E_{A}$:\n$$\\frac{E_{B}}{E_{A}} = \\frac{m n_{B} 2^{-S'}}{m n_{A} 2^{-S'}}$$\nThe terms $m$ and $2^{-S'}$ cancel out, demonstrating that for a given query and alignment score, the E-value scales linearly with the database size. The ratio simplifies to:\n$$\\frac{E_{B}}{E_{A}} = \\frac{n_{B}}{n_{A}}$$\nSubstituting the given numerical values for the database lengths:\n$$\\frac{E_{B}}{E_{A}} = \\frac{3.0 \\times 10^{9}}{5.0 \\times 10^{7}}$$\n$$\\frac{E_{B}}{E_{A}} = \\frac{3.0}{5.0} \\times \\frac{10^{9}}{10^{7}} = 0.6 \\times 10^{2} = 60$$\nThe problem requires the final answer to be rounded to four significant figures. Therefore, the value $60$ must be expressed as $60.00$.",
            "answer": "$$\\boxed{60.00}$$"
        },
        {
            "introduction": "Modern bioinformatics pipelines often involve hundreds or thousands of simultaneous sequence searches, which introduces a significant multiple testing problem that must be managed to avoid a deluge of false positives. This practice applies the principle of linearity of expectation to control the total number of chance findings across an entire high-throughput experiment. By determining a uniform $E$-value cutoff based on a predefined run-wide budget for false discoveries , you will learn an essential and practical method for maintaining statistical rigor in large-scale data analysis, a technique analogous to the classic Bonferroni correction.",
            "id": "4538915",
            "problem": "A translational bioinformatics pipeline in medical data analytics executes two high-throughput sequence alignment components in a single run: (i) a set of $N_{1} = 35$ query-versus-database searches using the Basic Local Alignment Search Tool (BLAST), and (ii) a set of $N_{2} = 120$ profile-based searches using Hidden Markov Model (HMM) scoring for domain detection. Every search computes alignment significance using the expected number of random high-scoring alignments (E-value). The team wishes to enforce a run-level false positive budget of at most $B = 5$ expected random hits across the entire run by applying a single uniform per-search E-value cutoff $E^{\\ast}$ across both components.\n\nAssuming searches are calibrated using standard local alignment statistics and can be treated as independent for the purpose of controlling expected counts, derive from first principles the per-search E-value cutoff $E^{\\ast}$ that satisfies the stated budget when applied uniformly to all $N_{1} + N_{2}$ searches in the run. Report the numerical value of $E^{\\ast}$, rounded to four significant figures. No physical units are required.",
            "solution": "The problem requires us to derive a uniform per-search E-value cutoff, $E^{\\ast}$, that keeps the total expected number of false positives across all searches within a specified budget.\n\nThe central concept is the definition of the E-value. For a single search, an E-value represents the expected number of hits with a similar or better score that would occur by chance. Therefore, if we set a significance cutoff at $E^{\\ast}$, the expected number of false positives for that single search is $E^{\\ast}$.\n\nThe pipeline executes a total of $N_{total}$ searches, where $N_{total} = N_{1} + N_{2}$.\nThe total number of BLAST searches is $N_{1} = 35$.\nThe total number of HMM-based searches is $N_{2} = 120$.\nSo, the total number of searches is:\n$$N_{total} = 35 + 120 = 155$$\n\nLet $E_{run}$ be the total expected number of false positives for the entire run. Due to the linearity of expectation, the total expected value is the sum of the individual expected values.\n$$E_{run} = \\sum_{i=1}^{N_{total}} (\\text{Expected false positives for search } i)$$\nSince a uniform cutoff $E^{\\ast}$ is used for all searches, the expected number of false positives for each search is $E^{\\ast}$.\n$$E_{run} = \\sum_{i=1}^{N_{total}} E^{\\ast} = N_{total} \\times E^{\\ast}$$\n\nThe problem states that the run-level false positive budget is at most $B = 5$. To find the least stringent (i.e., largest) cutoff that satisfies this condition, we set the total expected number of false positives equal to the budget:\n$$N_{total} \\times E^{\\ast} = B$$\n$$(N_{1} + N_{2}) \\times E^{\\ast} = B$$\n\nWe can now solve for the per-search E-value cutoff, $E^{\\ast}$:\n$$E^{\\ast} = \\frac{B}{N_{1} + N_{2}}$$\n\nSubstituting the given numerical values:\n$$E^{\\ast} = \\frac{5}{35 + 120} = \\frac{5}{155} = \\frac{1}{31}$$\n\nTo provide the numerical answer, we calculate the decimal value:\n$$E^{\\ast} \\approx 0.0322580645...$$\n\nThe problem requires this value to be rounded to four significant figures. The first four significant figures are 3, 2, 2, and 5. The next digit is 8, so we round up the last significant digit.\nThus, the value of $E^{\\ast}$ rounded to four significant figures is $0.03226$.",
            "answer": "$$\\boxed{0.03226}$$"
        },
        {
            "introduction": "As sequence databases grow at an exponential rate, the statistical significance of an alignment with a fixed score can \"drift\" or degrade over time, a crucial consideration for the reproducibility and long-term validity of bioinformatics findings. This exercise challenges you to translate theory into practice by implementing the core Karlin-Altschul formulas to build a computational tool that precisely quantifies this significance drift. By designing and coding an algorithm to track $E$-value changes across database releases , you will not only solidify your theoretical understanding but also develop practical skills in handling numerical stability and automating the re-evaluation of historical results.",
            "id": "4538965",
            "problem": "You are tasked with constructing a principled computational experiment to quantify how successive increases in database size alter the statistical significance assigned to a fixed local alignment raw score in sequence search, within the framework of extremal statistics used in bioinformatics and medical data analytics. You must begin from the following base assumptions and facts, which are widely accepted in the theory of local sequence alignment:\n\n1. Under suitable scoring schemes and independence assumptions for residues, the distribution of maximal local alignment scores between a query and a random database belongs to the Type I Extreme Value family (Gumbel). Specifically, for high thresholds, exceedances behave like a Poisson process whose rate is determined by the contact opportunities between a query of effective length $m$ and a database of effective length $n$, and by scoring-system-dependent constants.\n2. The expected number of high-scoring segment pairs (HSPs) above a given raw score threshold $S$ between a query of effective length $m$ and a database of effective length $n$ is the standard definition of the expected value (E-value) of that threshold.\n3. There exist two positive constants, customarily denoted by $\\lambda$ and $K$, that depend on the scoring scheme and background residue frequencies, which parameterize the distribution of local alignment scores and the intensity of the Poisson process for exceedances.\n4. A normalized score, colloquially referred to as the bit score, is routinely used to remove dependencies on the scoring scale and the constants; it is defined from $S$, $K$, and $\\lambda$ so that database size affects the expected value linearly through the factor $m n$ while the normalized score remains invariant to $m$ and $n$.\n\nStarting only from these base facts, derive the relationship between the expected count of HSPs above threshold $S$ and the parameters $(m,n,\\lambda,K)$, and then derive the transformation from the raw score $S$ to a normalized bit score. Use these derivations to design and implement an algorithm that, for a fixed raw score $S$ and fixed $(\\lambda,K)$ across two releases of a database with sizes $n_{\\text{old}}$ and $n_{\\text{new}}$, computes:\n\n- The E-value $E_{\\text{old}}$ at $n_{\\text{old}}$,\n- The E-value $E_{\\text{new}}$ at $n_{\\text{new}}$,\n- The drift ratio $r = E_{\\text{new}} / E_{\\text{old}}$,\n- The bit score $S_{\\text{bit}}$ associated with the raw score $S$.\n\nYour program must implement numerically stable computations for these quantities, taking care with extreme values. You may use $\\log$-space transformations where appropriate.\n\nTest Suite:\nImplement your algorithm on the following five parameter sets, representing a diversity of regimes encountered in practical work. Each test case is a tuple $(m,n_{\\text{old}},n_{\\text{new}},\\lambda,K,S)$, where $m$ and $n$ are effective lengths measured in letters (no physical units), and $S$ is dimensionless in raw score units from the scoring system.\n\n- Case 1 (typical protein regime, moderate significance growth): $(m,n_{\\text{old}},n_{\\text{new}},\\lambda,K,S) = (1200,\\ 5\\times 10^{8},\\ 7.5\\times 10^{8},\\ 0.318,\\ 0.134,\\ 200)$\n- Case 2 (tiny database boundary case): $(m,n_{\\text{old}},n_{\\text{new}},\\lambda,K,S) = (100,\\ 10^{3},\\ 2\\times 10^{3},\\ 0.318,\\ 0.1,\\ 50)$\n- Case 3 (extremely high raw score in a large database): $(m,n_{\\text{old}},n_{\\text{new}},\\lambda,K,S) = (2000,\\ 10^{9},\\ 4\\times 10^{9},\\ 0.318,\\ 0.1,\\ 600)$\n- Case 4 (nucleotide-like regime with larger $\\lambda$): $(m,n_{\\text{old}},n_{\\text{new}},\\lambda,K,S) = (1000,\\ 5\\times 10^{10},\\ 10^{11},\\ 1.37,\\ 0.711,\\ 40)$\n- Case 5 (low raw score with huge database yielding large E-values): $(m,n_{\\text{old}},n_{\\text{new}},\\lambda,K,S) = (1000,\\ 10^{9},\\ 2\\times 10^{9},\\ 0.318,\\ 0.1,\\ 30)$\n\nOutput Specification:\n- For each test case, output a list of four floating-point numbers $[E_{\\text{old}}, E_{\\text{new}}, r, S_{\\text{bit}}]$ in that order.\n- All floating-point numbers must be printed in lower-case scientific notation with exactly six digits after the decimal point.\n- Your program should produce a single line of output containing the results for all five test cases as a comma-separated list of these per-case lists, enclosed in square brackets. For example: \"[[e11,e12,e13,e14],[e21,e22,e23,e24],...]\" where each \"eij\" adheres to the formatting rule above.\n\nNo physical units are involved in this problem. Angles are not used. Express any ratios as decimal numbers, not percentages. The program must be self-contained, must not read any input, and must use only the specified runtime environment.",
            "solution": "The problem requires the derivation and implementation of fundamental formulas from the statistical theory of local sequence alignment to analyze the effect of database growth on alignment significance. The theoretical foundation is Karlin-Altschul statistics, which models the distribution of maximal alignment scores using extreme value theory.\n\n**Derivation of the Expected Value (E-value)**\n\nThe problem statement establishes that for a high raw score threshold $S$, the number of High-scoring Segment Pairs (HSPs) exceeding this score follows a Poisson process. The E-value, denoted $E$, is defined as the expected number of such HSPs. For a Poisson process, the expected value is equal to its rate parameter. This rate is the product of the number of opportunities for an alignment to occur and the probability of a single alignment achieving a score of at least $S$.\n\nThe number of opportunities, or the search space size, is proportional to the product of the effective lengths of the query sequence, $m$, and the database, $n$. The effective lengths are used to correct for edge effects in finite sequences. The search space is therefore proportional to the product $m \\cdot n$.\n\nThe probability $P(\\text{score} \\ge S)$ for a single random alignment to achieve a score of at least $S$ is given by the tail of the score distribution. The theory of extreme value distributions, as applied to local sequence alignment, gives this probability as:\n$$ P(\\text{score} \\ge S) \\approx K e^{-\\lambda S} $$\nHere, $\\lambda$ and $K$ are positive statistical parameters that depend on the residue frequencies and the scoring matrix used. The parameter $\\lambda$ is the scale parameter of the Gumbel distribution, and $K$ is a pre-factor related to the entropy of the scoring system.\n\nCombining the search space size and the probability of an HSP, the expected number of alignments with a score of at least $S$ is:\n$$ E = K \\cdot m \\cdot n \\cdot e^{-\\lambda S} $$\nThis is the central formula for the E-value, directly derived from the axiomatic statements provided.\n\n**Derivation of the Normalized Bit Score**\n\nThe second task is to derive the transformation from the raw score $S$ to a normalized bit score, $S_{\\text{bit}}$. The guiding principle is that the bit score should be independent of the scoring system parameters ($\\lambda, K$) and the database size ($n$). A standard convention, which re-scales the E-value formula, is to express it in terms of a power of $2$:\n$$ E = m \\cdot n \\cdot 2^{-S_{\\text{bit}}} $$\nEquating this conventional form with the previously derived formula for $E$:\n$$ m n 2^{-S_{\\text{bit}}} = K m n e^{-\\lambda S} $$\nThe search space term $mn$ cancels from both sides, confirming that $S_{\\text{bit}}$ is independent of the query and database lengths, as required:\n$$ 2^{-S_{\\text{bit}}} = K e^{-\\lambda S} $$\nTo solve for $S_{\\text{bit}}$, we take the natural logarithm ($\\ln$) of both sides:\n$$ \\ln(2^{-S_{\\text{bit}}}) = \\ln(K e^{-\\lambda S}) $$\nUsing the properties of logarithms, $\\ln(a^b) = b \\ln(a)$ and $\\ln(ab) = \\ln(a) + \\ln(b)$:\n$$ -S_{\\text{bit}} \\ln(2) = \\ln(K) + \\ln(e^{-\\lambda S}) $$\n$$ -S_{\\text{bit}} \\ln(2) = \\ln(K) - \\lambda S $$\nMultiplying by $-1$ and rearranging to solve for $S_{\\text{bit}}$ yields the transformation:\n$$ S_{\\text{bit}} = \\frac{\\lambda S - \\ln(K)}{\\ln(2)} $$\nThis formula allows the conversion of any raw score $S$ into a standardized, unitless bit score, which reflects the information content of the alignment normalized by the statistical properties of the scoring system.\n\n**Analysis of Database Growth (Drift Ratio)**\n\nThe problem asks for the computation of the E-values for an old database of size $n_{\\text{old}}$ and a new, larger database of size $n_{\\text{new}}$, along with their ratio, $r$. For a fixed raw score $S$, query length $m$, and parameters $\\lambda$ and $K$:\n$$ E_{\\text{old}} = K m n_{\\text{old}} e^{-\\lambda S} $$\n$$ E_{\\text{new}} = K m n_{\\text{new}} e^{-\\lambda S} $$\nThe drift ratio, $r$, is defined as $E_{\\text{new}} / E_{\\text{old}}$:\n$$ r = \\frac{E_{\\text{new}}}{E_{\\text{old}}} = \\frac{K m n_{\\text{new}} e^{-\\lambda S}}{K m n_{\\text{old}} e^{-\\lambda S}} $$\nAll common terms cancel, yielding a simple and direct relationship:\n$$ r = \\frac{n_{\\text{new}}}{n_{\\text{old}}} $$\nThis result demonstrates that, for a fixed raw score, the E-value scales linearly with the size of the database. Consequently, the statistical significance of an alignment (where a lower E-value implies higher significance) degrades as the database grows.\n\n**Computational Algorithm and Numerical Stability**\n\nThe quantities to be computed for each test case $(m, n_{\\text{old}}, n_{\\text{new}}, \\lambda, K, S)$ are:\n1.  $E_{\\text{old}} = K m n_{\\text{old}} e^{-\\lambda S}$\n2.  $E_{\\text{new}} = K m n_{\\text{new}} e^{-\\lambda S}$\n3.  $r = n_{\\text{new}} / n_{\\text{old}}$\n4.  $S_{\\text{bit}} = (\\lambda S - \\ln(K)) / \\ln(2)$\n\nThe input parameters involve very large numbers (e.g., $n$ up to $10^{11}$) and potentially large values for the exponent $\\lambda S$. Direct computation of $e^{-\\lambda S}$ can lead to numerical underflow if $\\lambda S$ is large. Multiplying a very large number ($Kmn$) by a very small number ($e^{-\\lambda S}$) can result in a loss of precision or an erroneous zero result.\n\nTo ensure computational stability, calculations for the E-value are performed in logarithmic space. We compute the natural logarithm of the E-value first:\n$$ \\ln(E) = \\ln(K m n e^{-\\lambda S}) = \\ln(K) + \\ln(m) + \\ln(n) - \\lambda S $$\nThis transformation converts the product of numbers spanning many orders of magnitude into a sum of numbers of a more manageable scale. The final E-value is then recovered by exponentiation: $E = \\exp(\\ln(E))$.\n\nThe algorithm for each test case is as follows:\n1.  Given the input tuple $(m, n_{\\text{old}}, n_{\\text{new}}, \\lambda, K, S)$.\n2.  Calculate the bit score $S_{\\text{bit}}$ using its direct formula.\n3.  Calculate the drift ratio $r$ as the simple fraction $n_{\\text{new}} / n_{\\text{old}}$.\n4.  Calculate the logarithm of the old E-value: $\\ln(E_{\\text{old}}) = \\ln(K) + \\ln(m) + \\ln(n_{\\text{old}}) - \\lambda S$.\n5.  Exponentiate to find $E_{\\text{old}} = \\exp(\\ln(E_{\\text{old}}))$.\n6.  Calculate the new E-value. This can be done efficiently as $E_{\\text{new}} = E_{\\text{old}} \\cdot r$.\n7.  Collect the four resulting floating-point numbers $[E_{\\text{old}}, E_{\\text{new}}, r, S_{\\text{bit}}]$ and format them into the specified output string format.\n\nThis procedure is robust against the extreme values present in the test suite and accurately implements the derived theoretical relationships.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes E-values, drift ratio, and bit score for sequence alignments\n    given different database sizes and statistical parameters.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (m, n_old, n_new, lam, K, S)\n    test_cases = [\n        (1200, 5e8, 7.5e8, 0.318, 0.134, 200),\n        (100, 1e3, 2e3, 0.318, 0.1, 50),\n        (2000, 1e9, 4e9, 0.318, 0.1, 600),\n        (1000, 5e10, 1e11, 1.37, 0.711, 40),\n        (1000, 1e9, 2e9, 0.318, 0.1, 30),\n    ]\n\n    all_results_formatted = []\n\n    for case in test_cases:\n        m, n_old, n_new, lam, K, S = case\n\n        # The Karlin-Altschul E-value formula is E = K * m * n * exp(-lambda * S).\n        # The bit score is defined by S_bit = (lambda*S - ln(K)) / ln(2).\n        # The E-value can also be expressed as E = m * n * 2**(-S_bit).\n\n        # 1. Calculate the drift ratio r = E_new / E_old.\n        # This simplifies to n_new / n_old as all other terms cancel.\n        r = n_new / n_old\n\n        # 2. Calculate the normalized bit score S_bit.\n        # This score is independent of query and database lengths.\n        s_bit = (lam * S - np.log(K)) / np.log(2)\n\n        # 3. Calculate the E-value for the old database size, E_old.\n        # To maintain numerical stability with very large or small numbers,\n        # we compute in log-space first.\n        # ln(E_old) = ln(K) + ln(m) + ln(n_old) - lambda * S\n        log_e_old = np.log(K) + np.log(m) + np.log(n_old) - lam * S\n        e_old = np.exp(log_e_old)\n\n        # 4. Calculate the E-value for the new database size, E_new.\n        # E_new is simply E_old scaled by the drift ratio r.\n        e_new = e_old * r\n\n        # Store the computed results for the current case.\n        result_list = [e_old, e_new, r, s_bit]\n        \n        # Format the list of results into the required string format.\n        # Each number must be in scientific notation with 6 decimal places.\n        formatted_case = f\"[{','.join(['{:.6e}'.format(x) for x in result_list])}]\"\n        all_results_formatted.append(formatted_case)\n\n    # Final print statement in the exact required format: a list of lists.\n    print(f\"[{','.join(all_results_formatted)}]\")\n\nsolve()\n```"
        }
    ]
}