{
    "hands_on_practices": [
        {
            "introduction": "Before any assembly can begin, we must generate sufficient sequencing data. This exercise explores the fundamental relationship between sequencing coverage and the completeness of the genome assembly. By deriving the classic Lander–Waterman model result , you will gain a first-principles understanding of why gaps are an inevitable consequence of random sampling and how nominal sequencing coverage, $C$, directly mitigates this problem.",
            "id": "4552682",
            "problem": "A contiguous genome of length $G$ is sequenced by $N$ independent reads, each of fixed length $L$. Assume read start positions are independent and identically distributed over the $G$ possible start sites with equal probability, and ignore edge effects by taking $G \\gg L$. In the de novo assembly context of Overlap–Layout–Consensus (OLC) and de Bruijn graph models, a fundamental quantity is the nominal sequencing coverage $C$, defined as $C = \\frac{N L}{G}$. Using only core probability definitions and the limit connecting the binomial distribution to the Poisson distribution, derive from first principles the expression for the probability that an arbitrary base in the genome is uncovered by all reads. Express your final result purely in terms of $C$. Based on the result, briefly explain the meaning of this probability for assembly completeness in Overlap–Layout–Consensus (OLC) and de Bruijn graph frameworks, under the stated assumptions. Your final answer must be a single closed-form analytic expression and requires no rounding.",
            "solution": "The problem is found to be valid as it is scientifically grounded, well-posed, objective, and internally consistent. It represents a classic problem in sequencing theory. We may, therefore, proceed with the solution.\n\nLet $G$ be the length of the genome, $N$ be the number of reads, and $L$ be the length of each read. The nominal sequencing coverage is defined as $C = \\frac{NL}{G}$. We seek to find the probability that an arbitrary base in the genome is not covered by any of the $N$ reads, under the assumption that read start positions are independent and uniformly distributed across the $G$ possible start sites. The condition $G \\gg L$ allows us to neglect edge effects.\n\nConsider an arbitrary base at a specific position within the genome. For a single read to cover this base, its starting position must fall within a specific window. A read of length $L$ starting at position $s$ covers the genomic interval $[s, s+L-1]$. For this read to cover our arbitrary base, located at position $k$, the condition $s \\le k \\le s+L-1$ must hold. This is equivalent to the starting position $s$ being in the range $k-L+1 \\le s \\le k$. The number of integer starting positions that satisfy this condition is $k - (k-L+1) + 1 = L$.\n\nThe problem states that there are $G$ possible starting sites for any given read, and each site is chosen with equal probability. Therefore, the probability $p$ that a single, randomly placed read covers our specific base is the ratio of the number of favorable starting positions to the total number of possible starting positions:\n$$p = \\frac{L}{G}$$\n\nConsequently, the probability that a single read does *not* cover this specific base is $1-p$:\n$$1-p = 1 - \\frac{L}{G}$$\n\nThe problem states that the $N$ reads are placed independently. Thus, the event of a base being covered by one read is independent of it being covered by another. The probability that the base is not covered by any of the $N$ reads is the product of the individual probabilities that each read does not cover it. Let $P_0$ be the probability of being uncovered (i.e., having zero coverage). This corresponds to $N$ independent Bernoulli trials, each with a \"failure\" probability of $1-p$.\n$$P_0 = (1 - p)^N = \\left(1 - \\frac{L}{G}\\right)^N$$\n\nThe problem requires us to use the limit connecting the binomial distribution to the Poisson distribution. The number of reads covering a specific base follows a binomial distribution $B(N, p)$, where $N$ is the number of trials and $p = L/G$ is the probability of success (coverage) in a single trial. In typical sequencing experiments, the number of reads $N$ is very large (millions to billions), and since $G \\gg L$, the probability $p$ is very small. In this limit, where $N \\to \\infty$ and $p \\to 0$ such that the mean $\\lambda = Np$ remains constant, the binomial distribution can be accurately approximated by a Poisson distribution with mean $\\lambda$.\n\nThe mean number of times our arbitrary base is covered is:\n$$\\lambda = Np = N \\left(\\frac{L}{G}\\right) = \\frac{NL}{G}$$\nBy definition, this is the nominal sequencing coverage, $C$. So, $\\lambda = C$.\n\nThe Poisson distribution gives the probability of observing exactly $k$ events (in this case, $k$ reads covering the base) as:\n$$P(k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$$\nWe are interested in the probability that the base is uncovered, which corresponds to $k=0$. Substituting $k=0$ and $\\lambda=C$ into the Poisson probability mass function, we get:\n$$P_0 = P(k=0) = \\frac{C^0 \\exp(-C)}{0!}$$\nSince $C^0 = 1$ and $0! = 1$, the expression simplifies to:\n$$P_0 = \\exp(-C)$$\nThis is the probability that an arbitrary base in the genome is uncovered, expressed purely in terms of the nominal coverage $C$.\n\nThis result has a direct and significant meaning for genome assembly completeness. The value $\\exp(-C)$ represents the expected fraction of the genome that has zero coverage.\n\nIn the Overlap-Layout-Consensus (OLC) framework, assembly proceeds by finding overlaps between reads to construct longer contiguous sequences (contigs). If a segment of the genome has zero coverage, no reads originating from this region will be present in the sequencing data. Consequently, there will be no reads to bridge the gap between contigs generated from the flanking regions. The assembly graph will be disconnected at these points, resulting in a fragmented assembly. The quantity $\\exp(-C)$ thus provides a direct estimate of the proportion of the genome that will be missing from the final assembly due to coverage gaps.\n\nIn the de Bruijn graph (DBG) framework, all reads are decomposed into smaller overlapping subsequences of length $k$ (k-mers). These k-mers form the nodes of the graph, and edges connect k-mers that are adjacent in the reads. A complete and contiguous assembly corresponds to finding a path (or paths) that traverses all the edges corresponding to the true genome sequence. If a region of the genome has zero coverage, all k-mers unique to that region will be absent from the read set. These missing k-mers create breaks in the de Bruijn graph, preventing the reconstruction of a single, continuous path. The assembly algorithm will therefore output multiple disjoint contigs, corresponding to the connected components of the graph. The probability $\\exp(-C)$ serves as a powerful proxy for the likelihood of encountering such assembly-breaking gaps, again relating directly to the expected completeness of the assembly.",
            "answer": "$$\\boxed{\\exp(-C)}$$"
        },
        {
            "introduction": "De Bruijn graphs are powerful but often contain ambiguous structures called \"bubbles,\" which can arise from either biological variation or sequencing errors. This practice demonstrates how rigorous statistical methods can be used to resolve such ambiguities. You will formulate a Likelihood Ratio Test  to distinguish a bubble caused by a rare sequencing error ($\\mathcal{H}_{0}$) from one representing true heterozygosity ($\\mathcal{H}_{1}$), a critical task in modern assemblers.",
            "id": "4552729",
            "problem": "A bubble in a de Bruijn graph (DBG) during de novo assembly arises when two alternative directed paths diverge from a node and reconverge, typically reflecting either true diploid heterozygosity or sequencing errors. In a diploid genome, a heterozygous Single Nucleotide Polymorphism (SNP) induces two allelic paths whose coverage is proportional to allelic fractions, whereas a sequencing error tends to produce a low-coverage spurious path. Assume Next-Generation Sequencing (NGS) read coverage across $k$-mers is well modeled by independent Poisson variables whose means are proportional to path-specific exposure weights that aggregate per-$k$-mer contributions along the path (for example, lengths or quality-weighted multiplicities). Consider a single bubble with two branch paths $A$ and $B$, with observed aggregate $k$-mer counts $S_{A}$ and $S_{B}$ over exposure weights $E_{A}$ and $E_{B}$, respectively. Let the base-calling error rate be $\\epsilon$, and assume a simple substitution-error model where, conditional on an error occurring at the distinguishing site, the erroneous base is uniformly one of the three possible alternative nucleotides, so the probability of the specific erroneous allele is $\\epsilon/3$.\n\nFormulate a Likelihood Ratio Test (LRT) to decide between the hypotheses:\n- $\\mathcal{H}_{0}$ (error bubble): branch $A$ is the true allele with per-unit exposure rate proportional to $(1-\\epsilon)$ and branch $B$ is the spurious error allele with per-unit exposure rate proportional to $\\epsilon/3$, with an unknown shared scaling factor $\\lambda$.\n- $\\mathcal{H}_{1}$ (heterozygous bubble): both branches are true alleles with per-unit exposure rates proportional to $\\lambda q$ for branch $A$ and $\\lambda(1-q)$ for branch $B$, where $\\lambda>0$ and $q\\in(0,1)$ are unknown.\n\nStarting from the Poisson likelihood and without invoking any shortcut formulas, derive the coverage-weighted LRT statistic $-2\\ln\\Lambda$ in closed form in terms of $S_{A}$, $S_{B}$, $E_{A}$, $E_{B}$, and $\\epsilon$. Then, using the derived expression, compute the numerical value of $-2\\ln\\Lambda$ for the following bubble:\n- $S_{A} = 240$, $S_{B} = 230$,\n- $E_{A} = 12$, $E_{B} = 12$,\n- $\\epsilon = 0.01$.\n\nRound your final numerical value to four significant figures. Express the final statistic as a dimensionless number. Finally, explain, in words, how this decision rule justifies preserving bubbles that reflect true heterozygosity rather than collapsing bubbles attributable to sequencing errors, but only report the numerical value for the final answer.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of statistical genomics, is well-posed, objective, and contains all necessary information to derive a solution.\n\nThe task is to formulate a Likelihood Ratio Test (LRT) to distinguish between an error-induced bubble and a heterozygous bubble in a de Bruijn graph. We are given the observed aggregate $k$-mer counts $S_A$ and $S_B$ for two alternative paths, A and B, with corresponding exposure weights $E_A$ and $E_B$. The counts are assumed to follow independent Poisson distributions.\n\nThe likelihood function for two independent Poisson-distributed random variables $S_A$ and $S_B$ with means $\\mu_A$ and $\\mu_B$ is given by:\n$$L(\\mu_A, \\mu_B; S_A, S_B) = \\frac{\\mu_A^{S_A} \\exp(-\\mu_A)}{S_A!} \\cdot \\frac{\\mu_B^{S_B} \\exp(-\\mu_B)}{S_B!}$$\nThe log-likelihood, ignoring constant terms in $S_A!$ and $S_B!$ which will cancel in the LRT, is:\n$$\\ln L(\\mu_A, \\mu_B) = S_A \\ln(\\mu_A) - \\mu_A + S_B \\ln(\\mu_B) - \\mu_B$$\n\nWe will find the maximized log-likelihood under each hypothesis.\n\n**Null Hypothesis $\\mathcal{H}_0$: Error Bubble**\nUnder $\\mathcal{H}_0$, the means are parameterized by a single unknown scaling factor $\\lambda > 0$:\n$$\\mu_A = \\lambda (1-\\epsilon) E_A$$\n$$\\mu_B = \\lambda \\frac{\\epsilon}{3} E_B$$\nThe log-likelihood under $\\mathcal{H}_0$ is a function of $\\lambda$:\n$$\\ln L_0(\\lambda) = S_A \\ln(\\lambda (1-\\epsilon) E_A) - \\lambda (1-\\epsilon) E_A + S_B \\ln(\\lambda \\frac{\\epsilon}{3} E_B) - \\lambda \\frac{\\epsilon}{3} E_B$$\n$$\\ln L_0(\\lambda) = (S_A + S_B) \\ln(\\lambda) - \\lambda \\left((1-\\epsilon)E_A + \\frac{\\epsilon}{3}E_B\\right) + \\text{constant terms}$$\nTo find the Maximum Likelihood Estimate (MLE) for $\\lambda$, we differentiate with respect to $\\lambda$ and set the result to zero:\n$$\\frac{\\partial \\ln L_0}{\\partial \\lambda} = \\frac{S_A + S_B}{\\lambda} - \\left((1-\\epsilon)E_A + \\frac{\\epsilon}{3}E_B\\right) = 0$$\nSolving for $\\lambda$ gives the MLE $\\hat{\\lambda}_0$:\n$$\\hat{\\lambda}_0 = \\frac{S_A + S_B}{(1-\\epsilon)E_A + \\frac{\\epsilon}{3}E_B}$$\nThe MLEs for the means under $\\mathcal{H}_0$ are:\n$$\\hat{\\mu}_{A,0} = \\hat{\\lambda}_0 (1-\\epsilon) E_A = (S_A + S_B) \\frac{(1-\\epsilon)E_A}{(1-\\epsilon)E_A + \\frac{\\epsilon}{3}E_B}$$\n$$\\hat{\\mu}_{B,0} = \\hat{\\lambda}_0 \\frac{\\epsilon}{3} E_B = (S_A + S_B) \\frac{\\frac{\\epsilon}{3}E_B}{(1-\\epsilon)E_A + \\frac{\\epsilon}{3}E_B}$$\nNote that $\\hat{\\mu}_{A,0} + \\hat{\\mu}_{B,0} = S_A + S_B$.\nThe maximized log-likelihood under $\\mathcal{H}_0$ is $\\ln L_0(\\hat{\\lambda}_0) = S_A \\ln(\\hat{\\mu}_{A,0}) - \\hat{\\mu}_{A,0} + S_B \\ln(\\hat{\\mu}_{B,0}) - \\hat{\\mu}_{B,0}$.\n\n**Alternative Hypothesis $\\mathcal{H}_1$: Heterozygous Bubble**\nUnder $\\mathcal{H}_1$, the means are parameterized by two unknown parameters, $\\lambda > 0$ and $q \\in (0,1)$:\n$$\\mu_A = \\lambda q E_A$$\n$$\\mu_B = \\lambda (1-q) E_B$$\nThis model has two free parameters. For any pair of observed positive counts $(S_A, S_B)$, we can find a unique pair $(\\lambda, q)$ within the specified ranges such that the model means equal the observations. Solving $\\hat{\\mu}_{A,1} = S_A$ and $\\hat{\\mu}_{B,1} = S_B$ for $\\lambda$ and $q$ gives:\n$$\\hat{q}_1 = \\frac{S_A/E_A}{S_A/E_A + S_B/E_B} = \\frac{S_A E_B}{S_A E_B + S_B E_A}$$\n$$\\hat{\\lambda}_1 = \\frac{S_A}{E_A} + \\frac{S_B}{E_B}$$\nSince the given counts $S_A, S_B$ are positive, $\\hat{q}_1 \\in (0,1)$ and $\\hat{\\lambda}_1 > 0$. Therefore, the parameter space under $\\mathcal{H}_1$ is rich enough to perfectly fit any positive data. This means $\\mathcal{H}_1$ is a reparameterization of the saturated model. The MLEs for the means under this hypothesis are simply the observed counts:\n$$\\hat{\\mu}_{A,1} = S_A$$\n$$\\hat{\\mu}_{B,1} = S_B$$\nThe maximized log-likelihood under $\\mathcal{H}_1$ is $\\ln L_1 = S_A \\ln(S_A) - S_A + S_B \\ln(S_B) - S_B$.\n\n**Likelihood Ratio Test Statistic**\nThe LRT statistic is given by $-2\\ln\\Lambda$, where $\\Lambda = \\frac{L_0(\\hat{\\lambda}_0)}{L_1(\\hat{\\lambda}_1, \\hat{q}_1)}$.\n$$-2\\ln\\Lambda = -2 (\\ln L_0 - \\ln L_1)$$\n$$-2\\ln\\Lambda = -2 \\left[ (S_A \\ln(\\hat{\\mu}_{A,0}) - \\hat{\\mu}_{A,0} + S_B \\ln(\\hat{\\mu}_{B,0}) - \\hat{\\mu}_{B,0}) - (S_A \\ln(S_A) - S_A + S_B \\ln(S_B) - S_B) \\right]$$\nUsing the fact that $\\hat{\\mu}_{A,0} + \\hat{\\mu}_{B,0} = S_A+S_B$, the terms $-\\hat{\\mu}_{A,0} - \\hat{\\mu}_{B,0}$ and $-S_A - S_B$ cancel.\n$$-2\\ln\\Lambda = -2 \\left[ S_A \\ln(\\hat{\\mu}_{A,0}) + S_B \\ln(\\hat{\\mu}_{B,0}) - S_A \\ln(S_A) - S_B \\ln(S_B) \\right]$$\n$$-2\\ln\\Lambda = 2 \\left[ S_A (\\ln(S_A) - \\ln(\\hat{\\mu}_{A,0})) + S_B (\\ln(S_B) - \\ln(\\hat{\\mu}_{B,0})) \\right]$$\n$$-2\\ln\\Lambda = 2 \\left[ S_A \\ln\\left(\\frac{S_A}{\\hat{\\mu}_{A,0}}\\right) + S_B \\ln\\left(\\frac{S_B}{\\hat{\\mu}_{B,0}}\\right) \\right]$$\nSubstituting the expressions for $\\hat{\\mu}_{A,0}$ and $\\hat{\\mu}_{B,0}$, we obtain the closed-form expression:\n$$ -2\\ln\\Lambda = 2 \\left[ S_A \\ln\\left(\\frac{S_A \\left((1-\\epsilon)E_A + \\frac{\\epsilon}{3}E_B\\right)}{(S_A+S_B)(1-\\epsilon)E_A}\\right) + S_B \\ln\\left(\\frac{S_B \\left((1-\\epsilon)E_A + \\frac{\\epsilon}{3}E_B\\right)}{(S_A+S_B)\\frac{\\epsilon}{3}E_B}\\right) \\right] $$\n\n**Numerical Calculation**\nWe are given the values: $S_A = 240$, $S_B = 230$, $E_A = 12$, $E_B = 12$, and $\\epsilon=0.01$.\nFirst, calculate the expected counts under $\\mathcal{H}_0$:\n$$S_A + S_B = 240 + 230 = 470$$\nSince $E_A=E_B=12$, the expressions for the means simplify:\n$$\\hat{\\mu}_{A,0} = (470) \\frac{(1-0.01) \\cdot 12}{(1-0.01) \\cdot 12 + \\frac{0.01}{3} \\cdot 12} = 470 \\frac{0.99}{0.99 + 0.01/3} = 470 \\frac{0.99}{2.98/3} = 470 \\frac{2.97}{2.98} \\approx 468.4228$$\n$$\\hat{\\mu}_{B,0} = 470 - \\hat{\\mu}_{A,0} = 470 - 470 \\frac{2.97}{2.98} = 470 \\left(1 - \\frac{2.97}{2.98}\\right) = 470 \\frac{0.01}{2.98} \\approx 1.5772$$\nNow we compute the statistic:\n$$-2\\ln\\Lambda = 2 \\left[ 240 \\ln\\left(\\frac{240}{468.4228}\\right) + 230 \\ln\\left(\\frac{230}{1.5772}\\right) \\right]$$\n$$-2\\ln\\Lambda = 2 \\left[ 240 \\ln(0.512358) + 230 \\ln(145.8298) \\right]$$\n$$-2\\ln\\Lambda = 2 \\left[ 240(-0.668735) + 230(4.982410) \\right]$$\n$$-2\\ln\\Lambda = 2 \\left[ -160.4964 + 1145.9543 \\right]$$\n$$-2\\ln\\Lambda = 2 [985.4579] = 1970.9158$$\nRounding to four significant figures, the value is $1971$.\n\n**Explanation of the Decision Rule**\nThe LRT statistic $-2\\ln\\Lambda$ quantifies how much better the alternative hypothesis $\\mathcal{H}_1$ (heterozygous alleles) explains the observed data compared to the null hypothesis $\\mathcal{H}_0$ (sequencing error). A larger value of this statistic indicates stronger evidence against the null hypothesis.\nUnder Wilks' theorem, for nested or correctly specified models, this statistic asymptotically follows a chi-squared distribution with degrees of freedom equal to the difference in the number of free parameters between the two hypotheses ($df = 2-1=1$ in this case). A researcher would compare the calculated value of $-2\\ln\\Lambda$ to a critical value from the $\\chi^2_1$ distribution (e.g., $3.84$ for a significance level of $\\alpha=0.05$).\nIn our case, the observed counts $S_A=240$ and $S_B=230$ are very similar. The heterozygous model ($\\mathcal{H}_1$) can readily account for these similar counts with an estimated allele fraction $\\hat{q}_1 = 240/470 \\approx 0.51$. In contrast, the error model ($\\mathcal{H}_0$) assumes one path is a rare artifact, so it predicts count proportions close to $(1-\\epsilon) : \\epsilon/3$, which is approximately $0.99 : 0.0033$. The expected counts under this model ($\\hat{\\mu}_{A,0} \\approx 468, \\hat{\\mu}_{B,0} \\approx 2$) are drastically different from the observed counts. This poor fit of the error model results in a very small likelihood $L_0$ compared to $L_1$, leading to a very large positive value for $-2\\ln\\Lambda$.\nThe extremely large statistic ($1971$) provides overwhelming evidence to reject the error hypothesis in favor of the heterozygous hypothesis. Therefore, the decision rule justifies preserving the bubble as it almost certainly represents true biological variation (heterozygosity) rather than a technical artifact.",
            "answer": "$$\\boxed{1971}$$"
        },
        {
            "introduction": "After running an assembly algorithm, we are left with a set of contiguous sequences, or \"contigs.\" A crucial final step is to assess the quality of this assembly, particularly its continuity. This exercise walks you through the calculation of N50 , one of the most widely used metrics for quantifying assembly contiguity, providing a tangible method to evaluate the success of an assembly project.",
            "id": "4552686",
            "problem": "A de novo genome assembly was generated using a de Bruijn graph method with $k$-mer size $k=55$ from a diploid sample, and contigs were computed after resolving bubbles and removing low-coverage tips. Consider the assembled contig lengths (in base pairs) collected from the final contig set: $830{,}124$, $710{,}552$, $690{,}121$, $540{,}600$, $439{,}820$, $415{,}000$, $398{,}657$, $372{,}000$, $250{,}300$, $240{,}000$, $120{,}000$, $95{,}000$. In de novo genome assembly (overlap–layout–consensus and de Bruijn graphs), the $N50$ statistic is defined as the contig length $L$ such that at least one half of the total assembly size is contained in contigs of length greater than or equal to $L$. Using only standard, widely accepted definitions of contigs and the $N50$ statistic, derive the algorithmic procedure from first principles to compute $N50$ for a multiset of contig lengths, and then apply it to the given lengths. Explicitly describe the sorting and accumulation steps, define the threshold mathematically, and identify the smallest contig length crossing the threshold. Report the final $N50$ length as an integer number of base pairs. Do not round; provide the exact integer. Express the final result in base pairs.",
            "solution": "The problem requires the computation of the N50 statistic for a given set of contig lengths from a de novo genome assembly. First, we must formalize the definition of the N50 statistic and derive the algorithm for its calculation from first principles, as requested.\n\nThe N50 statistic is a measure of assembly continuity. It is defined as the length $L_{50}$ such that at least $50\\%$ of the total assembly size is contained within contigs of length greater than or equal to $L_{50}$. By convention in bioinformatics, $L_{50}$ is one of the contig lengths from the assembly itself. Specifically, it is the smallest contig length in the minimal set of the largest contigs whose combined length meets or exceeds $50\\%$ of the total assembly size.\n\nFrom this definition, we can derive a precise, step-by-step algorithm:\n\n1.  **Calculate the Total Assembly Size**: Let the given multiset of contig lengths be $\\mathcal{L} = \\{l_1, l_2, \\dots, l_n\\}$, where $n$ is the total number of contigs. The total assembly size, $S_{\\text{total}}$, is the sum of the lengths of all contigs.\n    $$S_{\\text{total}} = \\sum_{i=1}^{n} l_i$$\n\n2.  **Define the N50 Threshold**: The threshold is defined as one half of the total assembly size.\n    $$T_{50} = \\frac{S_{\\text{total}}}{2}$$\n\n3.  **Sort the Contig Lengths**: The contig lengths must be sorted in descending order, from longest to shortest. Let the sorted list of lengths be $\\mathcal{L}' = (l'_1, l'_2, \\dots, l'_n)$, where $l'_1 \\ge l'_2 \\ge \\dots \\ge l'_n$.\n\n4.  **Find the N50 Value**: Iterate through the sorted list $\\mathcal{L}'$, calculating the cumulative sum of the contig lengths. The N50 value is the length $l'_j$ of the first contig in the sequence that causes the cumulative sum to meet or exceed the threshold $T_{50}$. Mathematically, we find the smallest index $j$ such that:\n    $$\\sum_{i=1}^{j} l'_i \\ge T_{50}$$\n    The N50 value is then the contig length at this index, $L_{50} = l'_j$. The previous cumulative sum must be less than the threshold: $\\sum_{i=1}^{j-1} l'_i < T_{50}$ (with the sum being $0$ for $j=1$).\n\nNow, we apply this algorithm to the provided data.\n\nThe given contig lengths, in base pairs, are:\n$\\mathcal{L} = \\{830{,}124, 710{,}552, 690{,}121, 540{,}600, 439{,}820, 415{,}000, 398{,}657, 372{,}000, 250{,}300, 240{,}000, 120{,}000, 95{,}000\\}$.\nThere are $n=12$ contigs.\n\n**Step 1: Calculate the Total Assembly Size**\nWe sum all the lengths in the set $\\mathcal{L}$:\n$$S_{\\text{total}} = 830{,}124 + 710{,}552 + 690{,}121 + 540{,}600 + 439{,}820 + 415{,}000 + 398{,}657 + 372{,}000 + 250{,}300 + 240{,}000 + 120{,}000 + 95{,}000$$\n$$S_{\\text{total}} = 5{,}092{,}174 \\text{ base pairs}$$\n\n**Step 2: Define the N50 Threshold**\nThe threshold is half of the total assembly size:\n$$T_{50} = \\frac{5{,}092{,}174}{2} = 2{,}546{,}087$$\n\n**Step 3: Sort the Contig Lengths**\nThe provided list of contig lengths is already sorted in descending order. Let's denote this sorted list as $\\mathcal{L}'$:\n$\\mathcal{L}' = (l'_1, l'_2, \\dots, l'_{12}) = (830{,}124, 710{,}552, 690{,}121, 540{,}600, 439{,}820, 415{,}000, 398{,}657, 372{,}000, 250{,}300, 240{,}000, 120{,}000, 95{,}000)$.\n\n**Step 4: Find the N50 Value by Accumulation**\nWe compute the cumulative sum of contig lengths from the sorted list $\\mathcal{L}'$ and compare it to the threshold $T_{50} = 2{,}546{,}087$.\n\n-   Add $l'_1 = 830{,}124$:\n    Cumulative sum = $830{,}124$. This is less than $2{,}546{,}087$.\n\n-   Add $l'_2 = 710{,}552$:\n    Cumulative sum = $830{,}124 + 710{,}552 = 1{,}540{,}676$. This is less than $2{,}546{,}087$.\n\n-   Add $l'_3 = 690{,}121$:\n    Cumulative sum = $1{,}540{,}676 + 690{,}121 = 2{,}230{,}797$. This is less than $2{,}546{,}087$.\n\n-   Add $l'_4 = 540{,}600$:\n    Cumulative sum = $2{,}230{,}797 + 540{,}600 = 2{,}771{,}397$. This value is greater than or equal to the threshold $T_{50} = 2{,}546{,}087$.\n\nThis is the first point at which the cumulative sum crosses the threshold. The index is $j=4$. The contig length that caused this crossing is $l'_4$. Therefore, the N50 value is the length of this contig.\n\n$L_{50} = l'_4 = 540{,}600$.\n\nThe set of contigs with length greater than or equal to $540{,}600$ is $\\{830{,}124, 710{,}552, 690{,}121, 540{,}600\\}$. The sum of their lengths is $2{,}771{,}397$, which constitutes $\\frac{2{,}771{,}397}{5{,}092{,}174} \\approx 54.4\\%$ of the total assembly, thus satisfying the N50 definition. The next largest contig length in the sorted list is $l'_3 = 690{,}121$. The sum of lengths of contigs $\\ge 690{,}121$ is $2{,}230{,}797$, which is less than the threshold. Thus, $l'_4 = 540{,}600$ is indeed the correct N50 value according to the standard definition.",
            "answer": "$$\\boxed{540600}$$"
        }
    ]
}