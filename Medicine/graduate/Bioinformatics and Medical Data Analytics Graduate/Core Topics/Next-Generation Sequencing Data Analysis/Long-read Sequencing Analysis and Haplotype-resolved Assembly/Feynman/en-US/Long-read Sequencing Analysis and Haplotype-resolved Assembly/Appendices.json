{
    "hands_on_practices": [
        {
            "introduction": "The ultimate quality of a genome assembly is deeply connected to the characteristics of the input sequencing reads. For long-read technologies, the distribution of read lengths is paramount, and the read N50 is a critical statistic used to summarize this distribution. This exercise provides hands-on practice in both the practical calculation and theoretical understanding of this metric, connecting the raw data quality to the final assembly outcome. You will first compute the read N50 for a sample dataset and then explore, from first principles, how a higher read N50 directly enables assemblers to resolve the repetitive sequences that fragment assemblies .",
            "id": "4579363",
            "problem": "In long-read sequencing, one summary statistic of the read length distribution is the read N50. You are given a sequencing dataset consisting of $14$ single-molecule reads with lengths (in kilobases): $25$, $20$, $18$, $15$, $12$, $12$, $10$, $9$, $8$, $7$, $6$, $5$, $4$, $3$. Using the formal definition that the read N50 is the length $N$ such that reads of length at least $N$ collectively contribute at least one half of the total sequenced bases, compute the read N50 for this dataset. Express the read N50 in $\\mathrm{kb}$. No rounding is necessary.\n\nThen, under an idealized Overlap–Layout–Consensus (OLC) assembler (Overlap–Layout–Consensus (OLC) defines an assembly paradigm that constructs a graph of read-to-read overlaps, orders reads into layouts, and derives consensus sequences), explain from first principles how the read N50 influences the expected contig N50. In your explanation, you may assume the following widely used foundations without proof: read start positions along a genome of length $G$ are well approximated by a homogeneous Poisson process at coverage depth $c$, repeats of length $R$ require at least one read to span the repeat plus flanking unique sequence of total length $2u$ to be resolved, and contigs tend to break at unresolved repeats and at coverage gaps. Do not compute any numerical quantity for this part; instead, derive and state a symbolic expression that connects the probability that a repeat is resolved to the read length distribution, and use it to argue how increasing read N50 changes the expected contig N50.\n\nYour final reported answer must be the computed read N50 in $\\mathrm{kb}$ only. If any rounding were necessary, you would round to four significant figures, but for this dataset it is not required.",
            "solution": "The problem presents two distinct tasks. The first is a numerical calculation of the read N50 for a given dataset. The second is a theoretical explanation of the relationship between read N50 and contig N50 based on first principles of genome assembly.\n\nFirst, we address the calculation of the read N50. The formal definition states that the read N50 is the length $N$ such that sequencing reads of length at least $N$ collectively contribute at least one half of the total number of sequenced bases.\n\nThe provided dataset consists of $14$ reads with the following lengths in kilobases (kb): $25$, $20$, $18$, $15$, $12$, $12$, $10$, $9$, $8$, $7$, $6$, $5$, $4$, $3$.\n\nThe procedure to compute the read N50 is as follows:\n1.  Calculate the total number of sequenced bases by summing the lengths of all reads.\n2.  Determine the target value, which is one half of the total number of bases.\n3.  Sort the reads in descending order of their length.\n4.  Iterate through the sorted list, calculating the cumulative sum of read lengths.\n5.  The N50 is the length of the read at which the cumulative sum first meets or exceeds the target value from step 2.\n\nLet's execute these steps.\nStep 1: Calculate the total length of all reads.\nTotal Length $= 25 + 20 + 18 + 15 + 12 + 12 + 10 + 9 + 8 + 7 + 6 + 5 + 4 + 3 = 154$ kb.\n\nStep 2: Determine the halfway point of the total length.\nTarget Length $= \\frac{1}{2} \\times 154 = 77$ kb.\n\nStep 3: Sort the reads by length in descending order. The provided list is already sorted:\n$L_1 = 25$, $L_2 = 20$, $L_3 = 18$, $L_4 = 15$, $L_5 = 12$, $L_6 = 12$, $L_7 = 10$, $L_8 = 9$, $L_9 = 8$, $L_{10} = 7$, $L_{11} = 6$, $L_{12} = 5$, $L_{13} = 4$, $L_{14} = 3$.\n\nStep 4: Compute the cumulative sum of lengths and compare it to the target length of $77$ kb.\n-   Adding the longest read ($L_1 = 25$): Cumulative sum = $25$ kb. This is less than $77$ kb.\n-   Adding the second longest read ($L_2 = 20$): Cumulative sum = $25 + 20 = 45$ kb. This is less than $77$ kb.\n-   Adding the third longest read ($L_3 = 18$): Cumulative sum = $45 + 18 = 63$ kb. This is less than $77$ kb.\n-   Adding the fourth longest read ($L_4 = 15$): Cumulative sum = $63 + 15 = 78$ kb. This sum is greater than or equal to the target of $77$ kb.\n\nStep 5: Identify the N50 value. The cumulative sum first meets the threshold with the inclusion of the read of length $15$ kb. Therefore, by definition, the read N50 for this dataset is $15$ kb.\n\nNext, we address the second part of the problem: explaining from first principles how the read N50 influences the expected contig N50 under an idealized OLC assembler.\n\nThe contig N50 is a metric for assembly quality, where a higher value indicates a more contiguous assembly (i.e., the genome is assembled into fewer, longer pieces). The problem states that contigs break at unresolved repeats and at coverage gaps. A primary challenge in genome assembly is the correct resolution of repetitive sequences. An increase in read N50 has a direct, positive influence on the ability to resolve repeats, which in turn increases the contig N50.\n\nLet's formalize this relationship based on the provided assumptions. A repeat of length $R$ requires at least one read to span it and its flanking unique sequences of total length $2u$. Thus, a resolving read must have a length $L$ such that $L \\geq R + 2u$.\n\nLet the read length distribution be described by a probability density function $f(L)$. The probability that a randomly selected read is long enough to resolve the repeat is:\n$$\nP(L \\geq R + 2u) = \\int_{R+2u}^{\\infty} f(L) \\,dL\n$$\n\nThe problem states that read start positions are modeled by a homogeneous Poisson process. The rate of read starts per unit length of the genome is $\\lambda_{\\text{start}}$. This rate can be expressed in terms of the total number of reads, $N_{\\text{reads}}$, and the genome length, $G$, as $\\lambda_{\\text{start}} = N_{\\text{reads}}/G$. Since the coverage depth $c$ is defined as $c = (N_{\\text{reads}} \\cdot E[L]) / G$, where $E[L]$ is the mean read length, we can write $\\lambda_{\\text{start}} = c / E[L]$.\n\nFor a read to span the repeat of length $R$ and its unique flank of length $u$ on the upstream side, it must start within this upstream flank. The number of reads starting in this region of length $u$, which we denote as $N_u$, follows a Poisson distribution with mean $\\mu_u = \\lambda_{\\text{start}} \\cdot u = \\frac{cu}{E[L]}$.\n\nHowever, not all of these reads will be long enough to span the entire construct. We can consider a \"thinned\" Poisson process for only the reads that are long enough, i.e., those with $L \\geq R + 2u$. The rate of such \"spanning\" reads starting per unit length is $\\lambda_{\\text{span\\_start}} = \\lambda_{\\text{start}} \\cdot P(L \\geq R + 2u)$.\n\nThe number of such spanning reads that start in the upstream flanking region of length $u$, let's call it $N_{\\text{span}}$, will follow a Poisson distribution with mean:\n$$\n\\lambda = \\lambda_{\\text{span\\_start}} \\cdot u = \\left( \\frac{c}{E[L]} P(L \\geq R + 2u) \\right) \\cdot u = \\frac{cu}{E[L]} \\int_{R+2u}^{\\infty} f(L) \\,dL\n$$\nThe repeat is considered unresolved if no such spanning read exists, i.e., if $N_{\\text{span}} = 0$. The probability of this event is given by the Poisson probability mass function for $k=0$:\n$$\nP(\\text{unresolved}) = P(N_{\\text{span}} = 0) = \\frac{\\lambda^0 \\exp(-\\lambda)}{0!} = \\exp(-\\lambda)\n$$\nTherefore, the probability that the repeat is successfully resolved is:\n$$\nP(\\text{resolved}) = 1 - P(\\text{unresolved}) = 1 - \\exp\\left(-\\frac{cu}{E[L]} \\int_{R+2u}^{\\infty} f(L) \\,dL\\right)\n$$\nThis is the required symbolic expression connecting the probability of repeat resolution to the read length distribution.\n\nNow, we connect this to the read N50. The read N50 is a summary statistic of the read length distribution $f(L)$. A higher read N50 implies that the distribution is skewed towards longer reads, meaning a larger fraction of the total sequence data is contained within the longest reads. An increase in read N50 generally leads to an increase in the value of the integral $\\int_{R+2u}^{\\infty} f(L) \\,dL$ for any given repeat length $R$. This integral represents the fraction of reads that are theoretically capable of spanning the repeat.\n\nAs this integral term, $P(L \\geq R + 2u)$, increases, the argument of the exponential function in our expression for $P(\\text{resolved})$ becomes more negative. Consequently, the term $\\exp(-\\lambda)$ becomes smaller, and the overall probability of resolution, $P(\\text{resolved}) = 1 - \\exp(-\\lambda)$, increases, approaching $1$.\n\nIn summary, the chain of reasoning is as follows:\n1.  An increased read N50 signifies a read length distribution $f(L)$ with a heavier tail, i.e., more and/or longer long reads.\n2.  This leads to a higher probability, $P(L \\geq R + 2u)$, that a randomly chosen read is long enough to span a repeat of length $R$.\n3.  A higher $P(L \\geq R + 2u)$ increases the probability of repeat resolution, $P(\\text{resolved})$.\n4.  Since contigs break at unresolved repeats, higher resolution probability means fewer contig breaks across the genome.\n5.  Fewer breaks result in a more contiguous assembly, which is measured by a higher expected contig N50.\n\nThus, a higher read N50 is a strong predictor of a higher contig N50 because it directly enhances the ability of an assembler to resolve repeats, which are a primary cause of assembly fragmentation.",
            "answer": "$$\n\\boxed{15}\n$$"
        },
        {
            "introduction": "After generating an assembly, the crucial next step is to evaluate its quality, particularly its contiguity. Building on the idea that longer reads produce longer contigs, this practice focuses on the standard metrics used to quantify assembly contiguity: N50 and NG50. While their names are similar, they measure contiguity against different yardsticks—the total assembly size versus the estimated genome size. This exercise will solidify your understanding of these fundamental metrics by guiding you through their calculation for a set of contigs from a haplotype-resolved assembly .",
            "id": "4579430",
            "problem": "In haplotype-resolved deoxyribonucleic acid (DNA) assembly evaluation for long-read sequencing, contiguity metrics such as $N50$ and $NG50$ are widely reported. Build these metrics from first principles using the following base: a genome assembly is a multiset of contigs with lengths $\\{L_{i}\\}$, which can be sorted in nonincreasing order as $L_{1} \\geq L_{2} \\geq \\dots \\geq L_{n}$. The cumulative sum after $k$ contigs is $S_{k} = \\sum_{i=1}^{k} L_{i}$, and the total assembly length is $S_{n} = \\sum_{i=1}^{n} L_{i}$. The estimated haploid genome size is a known external quantity $G$ that may exceed $S_{n}$ when the assembly is incomplete.\n\nStarting from these definitions and facts, derive formal definitions and stepwise procedures for $N50$ and $NG50$ that rely only on sorted contig lengths and, for $NG50$, the known or estimated genome size. Then apply your procedures to the following maternal haplotype contig set from a long-read assembly, given in megabases (Mb), already sorted in nonincreasing order:\n$$\n25.0,\\;22.0,\\;20.0,\\;18.0,\\;16.0,\\;14.0,\\;12.0,\\;10.0,\\;9.0,\\;8.0,\\;7.5,\\;6.5,\\;5.0,\\;4.5,\\;4.0,\\;3.5,\\;3.0,\\;2.5,\\;2.0,\\;1.5.\n$$\nAssume the estimated haploid genome size is $G = 230$ megabases. Compute $N50$ and $NG50$ in megabases following your derived procedures, and then compute the ratio $N50/NG50$. Round your final ratio to six significant figures and report it as a pure number without units.",
            "solution": "The problem statement is evaluated and found to be valid. It is scientifically grounded in the established principles of genome assembly evaluation, well-posed with all necessary information provided, and objective in its formulation. We can therefore proceed with a formal solution.\n\nThe problem requires the derivation of formal definitions and procedures for the assembly contiguity metrics $N50$ and $NG50$, followed by their application to a provided dataset of sorted contig lengths.\n\nLet the multiset of contig lengths be $\\{L_{i}\\}_{i=1}^{n}$, sorted in nonincreasing order such that $L_{1} \\geq L_{2} \\geq \\dots \\geq L_{n}$.\nThe total length of the assembly is given by $S_{n} = \\sum_{i=1}^{n} L_{i}$.\nThe estimated haploid genome size is an external parameter, denoted by $G$.\n\n**Procedure for N50**\n\nThe $N50$ metric is defined as the length of the shortest contig in the smallest set of contigs whose cumulative length constitutes at least $50\\%$ of the total assembly length, $S_n$.\n\n1.  Calculate the total assembly length, $S_{n} = \\sum_{i=1}^{n} L_{i}$.\n2.  Determine the target length, which is half of the total assembly length: $T_{N50} = 0.5 \\times S_{n}$.\n3.  Find the smallest integer index $k$ such that the cumulative sum of the lengths of the first $k$ contigs (in descending order of length) is greater than or equal to $T_{N50}$. Mathematically, find the minimum $k \\in \\{1, 2, \\dots, n\\}$ that satisfies the inequality:\n    $$S_{k} = \\sum_{i=1}^{k} L_{i} \\geq T_{N50}$$\n4.  The $N50$ value is the length of the $k$-th contig, $L_k$.\n    $$N50 = L_{k}$$\n\n**Procedure for NG50**\n\nThe $NG50$ metric is analogous to $N50$, but it uses the estimated genome size, $G$, as the reference instead of the total assembly length, $S_n$. This provides a measure of contiguity relative to an external, more objective standard of genome completeness.\n\n1.  Use the given estimated haploid genome size, $G$.\n2.  Determine the target length, which is half of the estimated genome size: $T_{NG50} = 0.5 \\times G$.\n3.  Find the smallest integer index $m$ such that the cumulative sum of the lengths of the first $m$ contigs is greater than or equal to $T_{NG50}$. Mathematically, find the minimum $m \\in \\{1, 2, \\dots, n\\}$ that satisfies the inequality:\n    $$S_{m} = \\sum_{i=1}^{m} L_{i} \\geq T_{NG50}$$\n4.  The $NG50$ value is the length of the $m$-th contig, $L_m$.\n    $$NG50 = L_{m}$$\n\n**Application to the Provided Data**\n\nThe sorted list of contig lengths (in megabases, Mb) is:\n$\\{L_i\\} = \\{25.0, 22.0, 20.0, 18.0, 16.0, 14.0, 12.0, 10.0, 9.0, 8.0, 7.5, 6.5, 5.0, 4.5, 4.0, 3.5, 3.0, 2.5, 2.0, 1.5\\}$.\nThe number of contigs is $n = 20$.\nThe estimated haploid genome size is $G = 230$ Mb.\n\nFirst, we calculate the total assembly length, $S_{n}$:\n$$S_{20} = 25.0 + 22.0 + 20.0 + 18.0 + 16.0 + 14.0 + 12.0 + 10.0 + 9.0 + 8.0 + 7.5 + 6.5 + 5.0 + 4.5 + 4.0 + 3.5 + 3.0 + 2.5 + 2.0 + 1.5$$\n$$S_{20} = 194.0 \\text{ Mb}$$\n\n**Calculation of N50**\n\n1.  The target length is $T_{N50} = 0.5 \\times S_{20} = 0.5 \\times 194.0 = 97.0$ Mb.\n2.  We compute the cumulative sums $S_k$ until the sum is at least $97.0$:\n    $S_{1} = L_{1} = 25.0$\n    $S_{2} = S_{1} + L_{2} = 25.0 + 22.0 = 47.0$\n    $S_{3} = S_{2} + L_{3} = 47.0 + 20.0 = 67.0$\n    $S_{4} = S_{3} + L_{4} = 67.0 + 18.0 = 85.0$\n    $S_{5} = S_{4} + L_{5} = 85.0 + 16.0 = 101.0$\n3.  The cumulative sum $S_5 = 101.0$ is the first to exceed the target of $97.0$. Thus, the index is $k=5$.\n4.  The $N50$ value is the length of the $5$-th contig, $L_5$.\n    $$N50 = L_{5} = 16.0 \\text{ Mb}$$\n\n**Calculation of NG50**\n\n1.  The target length is $T_{NG50} = 0.5 \\times G = 0.5 \\times 230.0 = 115.0$ Mb.\n2.  We continue computing the cumulative sums $S_m$ until the sum is at least $115.0$:\n    We already have $S_5 = 101.0$.\n    $S_{6} = S_{5} + L_{6} = 101.0 + 14.0 = 115.0$\n3.  The cumulative sum $S_6 = 115.0$ is the first to be greater than or equal to the target of $115.0$. Thus, the index is $m=6$.\n4.  The $NG50$ value is the length of the $6$-th contig, $L_6$.\n    $$NG50 = L_{6} = 14.0 \\text{ Mb}$$\n\n**Calculation of the Ratio N50/NG50**\n\nThe final step is to compute the ratio of $N50$ to $NG50$ and round to six significant figures.\n$$\\frac{N50}{NG50} = \\frac{16.0}{14.0} = \\frac{8}{7}$$\nAs a decimal, this is:\n$$\\frac{8}{7} \\approx 1.142857142...$$\nRounding to six significant figures involves examining the seventh significant figure. The first six are $1, 1, 4, 2, 8, 5$. The seventh is $7$. Since $7 \\geq 5$, we round up the sixth digit.\n$$1.142857... \\approx 1.14286$$\nThe final ratio is a pure number.",
            "answer": "$$\\boxed{1.14286}$$"
        },
        {
            "introduction": "A defining advantage of long-read sequencing is its ability to produce haplotype-resolved, or 'phased,' assemblies that separate maternal and paternal chromosomes. Evaluating such an assembly requires more than just measuring contiguity; we must also assess the accuracy of the phasing itself. The primary metric for this is the switch error rate, which quantifies how often the assembly incorrectly switches between paternal and maternal variants along a chromosome. This exercise provides a focused, practical application of calculating this essential quality score, giving you hands-on experience with a key benchmark in modern genomics .",
            "id": "4579441",
            "problem": "A haplotype-resolved assembly generated from long-read sequencing is being evaluated against a high-confidence truth set in a $5$ megabase window of a human chromosome. Consider the following setup grounded in standard phasing definitions. A heterozygous variant is defined as any site where the two homologous chromosomes carry different alleles. A phasing assigns each heterozygous variant to one of two haplotypes, and a phase transition between two adjacent heterozygous variants indicates whether the haplotype assignment stays the same or flips when moving from the first to the second site in coordinate order. The orientation of each phase block is arbitrary, so only relative phase transitions between adjacent heterozygous variants are meaningful for truth evaluation.\n\nYou are given a truth set of heterozygous variants and a corresponding phased call set from the assembly. To compute the switch error rate relative to truth, only adjacent heterozygous variant pairs that are present and phased in both sets and do not cross a phase break in either set are considered. Because missing or unphased variants break adjacency, the comparable adjacent pairs decompose into disjoint runs where consecutive truth heterozygous variants are all present and phased in the call set. In this region, the intersection of truth and call heterozygous variants forms $6$ runs of lengths $50$, $63$, $37$, $42$, $30$, and $58$ respectively. Within these runs, the number of adjacent heterozygous variant pairs whose phase transitions disagree with the truth (i.e., the call’s relative transition differs from the truth’s relative transition) are $5$, $7$, $3$, $2$, $1$, and $4$ respectively.\n\nUsing foundational definitions of haplotype phasing and phase transitions, compute the switch error rate as the fraction of comparable adjacent heterozygous variant pairs with incorrect phase transitions. Express your final answer as a decimal and round to four significant figures.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of bioinformatics and genomics, specifically haplotype phasing and assembly evaluation. The problem is well-posed, objective, and contains all necessary information for a unique solution.\n\nThe primary task is to compute the switch error rate for a haplotype-resolved assembly. According to the problem's definition, the switch error rate is the fraction of comparable adjacent heterozygous variant pairs with incorrect phase transitions. This can be expressed mathematically as:\n$$\n\\text{Switch Error Rate} = \\frac{\\text{Total number of switch errors}}{\\text{Total number of comparable adjacent pairs}}\n$$\n\nFirst, we must determine the total number of comparable adjacent heterozygous variant pairs. The problem states that the analysis is restricted to disjoint runs of variants that are present and phased in both the truth and call sets. A run of length $L$, defined as containing $L$ consecutive heterozygous variants, will have $L-1$ adjacent pairs of variants.\n\nThe lengths of the $6$ disjoint runs are given as $L_1 = 50$, $L_2 = 63$, $L_3 = 37$, $L_4 = 42$, $L_5 = 30$, and $L_6 = 58$.\n\nThe number of adjacent pairs in each run, denoted $P_i$, is calculated as $P_i = L_i - 1$.\nFor the first run: $P_1 = L_1 - 1 = 50 - 1 = 49$\nFor the second run: $P_2 = L_2 - 1 = 63 - 1 = 62$\nFor the third run: $P_3 = L_3 - 1 = 37 - 1 = 36$\nFor the fourth run: $P_4 = L_4 - 1 = 42 - 1 = 41$\nFor the fifth run: $P_5 = L_5 - 1 = 30 - 1 = 29$\nFor the sixth run: $P_6 = L_6 - 1 = 58 - 1 = 57$\n\nThe total number of comparable adjacent pairs, $P_{total}$, is the sum of the pairs in all runs:\n$$\nP_{total} = \\sum_{i=1}^{6} P_i = P_1 + P_2 + P_3 + P_4 + P_5 + P_6\n$$\n$$\nP_{total} = 49 + 62 + 36 + 41 + 29 + 57 = 274\n$$\nAlternatively, one could calculate this as the sum of all run lengths minus the number of runs:\n$$\nP_{total} = \\left(\\sum_{i=1}^{6} L_i\\right) - 6 = (50 + 63 + 37 + 42 + 30 + 58) - 6 = 280 - 6 = 274\n$$\n\nNext, we must determine the total number of switch errors. A switch error is defined as an adjacent heterozygous variant pair where the phase transition in the call set disagrees with the truth set. The problem provides the number of such errors for each run: $E_1 = 5$, $E_2 = 7$, $E_3 = 3$, $E_4 = 2$, $E_5 = 1$, and $E_6 = 4$.\n\nThe total number of switch errors, $E_{total}$, is the sum of the errors in all runs:\n$$\nE_{total} = \\sum_{i=1}^{6} E_i = E_1 + E_2 + E_3 + E_4 + E_5 + E_6\n$$\n$$\nE_{total} = 5 + 7 + 3 + 2 + 1 + 4 = 22\n$$\n\nNow, we can compute the switch error rate by dividing the total number of switch errors by the total number of comparable adjacent pairs:\n$$\n\\text{Switch Error Rate} = \\frac{E_{total}}{P_{total}} = \\frac{22}{274}\n$$\nThe fraction simplifies to:\n$$\n\\frac{22}{274} = \\frac{11}{137}\n$$\nTo express this as a decimal, we perform the division:\n$$\n\\frac{11}{137} \\approx 0.08029197...\n$$\nThe problem requires the answer to be rounded to four significant figures. The first significant figure is $8$. The fourth significant figure is $9$ (in the ten-thousandths place). The digit following the $9$ is $1$, which is less than $5$, so we round down (i.e., keep the $9$ as it is).\n\nTherefore, the switch error rate, rounded to four significant figures, is $0.08029$.",
            "answer": "$$\\boxed{0.08029}$$"
        }
    ]
}