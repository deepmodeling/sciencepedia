{
    "hands_on_practices": [
        {
            "introduction": "A foundational task in medical image registration is aligning two images based on a set of corresponding anatomical landmarks. This exercise guides you through the derivation of the classic closed-form solution for optimal rigid alignment, known as the Procrustes problem . Mastering this derivation provides deep insight into how Singular Value Decomposition (SVD) can be used to solve geometric optimization problems and highlights the critical importance of ensuring a physically valid rotation.",
            "id": "4582035",
            "problem": "Consider two sets of corresponding anatomical landmark points in three-dimensional space, $\\{x_{i}\\}_{i=1}^{n}$ and $\\{y_{i}\\}_{i=1}^{n}$, extracted from preoperative and intraoperative medical images of the same patient. A rigid transform in three dimensions consists of a rotation $R \\in \\mathbb{R}^{3 \\times 3}$ with $R^{\\top}R = I$ and $\\det(R) = 1$, and a translation $t \\in \\mathbb{R}^{3}$. The registration objective is to minimize the sum of squared Euclidean distances\n$$\nE(R,t) = \\sum_{i=1}^{n} \\| R x_{i} + t - y_{i} \\|^{2}.\n$$\nStarting from first principles—namely, the definition of a rigid body transform, properties of orthogonal matrices, the decomposition of variance by centering data, and the invariance properties of the trace under cyclic permutations—derive the closed-form solution for the optimal rotation $R$ and translation $t$ that minimize $E(R,t)$ using the Singular Value Decomposition (SVD) of the $3 \\times 3$ cross-covariance matrix, and explain how to handle reflections to enforce $\\det(R) = 1$.\n\nLet the sample means be $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}$ and $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$, and define centered points $x_{i}' = x_{i} - \\bar{x}$ and $y_{i}' = y_{i} - \\bar{y}$. Define the cross-covariance matrix $H = \\sum_{i=1}^{n} x_{i}' (y_{i}')^{\\top}$, and let its Singular Value Decomposition (SVD) be $H = U \\Sigma V^{\\top}$, where $U, V \\in \\mathbb{R}^{3 \\times 3}$ are orthogonal matrices and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\sigma_{3})$ with $\\sigma_{1} \\geq \\sigma_{2} \\geq \\sigma_{3} \\geq 0$. Let $s = \\operatorname{sign}(\\det(U V^{\\top}))$.\n\nYour derivation must be self-contained and must explain, from the listed principles, why the reflection case arises and how to correct it to ensure $\\det(R) = 1$.\n\nAnswer specification:\n- Provide the minimal value of the objective $E(R,t)$ at the optimal $(R,t)$ as a single closed-form analytic expression, expressed in terms of $\\{x_{i}\\}$, $\\{y_{i}\\}$, $\\bar{x}$, $\\bar{y}$, and the singular values $\\sigma_{1}, \\sigma_{2}, \\sigma_{3}$ and the sign $s$ defined above.\n- The final answer must be a single analytic expression. Do not include units.",
            "solution": "The problem is to find the rotation $R \\in \\mathbb{R}^{3 \\times 3}$ (with $R^{\\top}R=I$ and $\\det(R)=1$) and translation $t \\in \\mathbb{R}^{3}$ that minimize the sum of squared Euclidean distances between two sets of corresponding points, $\\{x_i\\}_{i=1}^n$ and $\\{y_i\\}_{i=1}^n$. The objective function is:\n$$\nE(R,t) = \\sum_{i=1}^{n} \\| R x_{i} + t - y_{i} \\|^{2}\n$$\nThe solution is found by first optimizing for the translation $t$ and then for the rotation $R$.\n\nFirst, we find the optimal translation $t$ for a given rotation $R$. The function $E(R,t)$ is a quadratic function of $t$, and its minimum can be found by setting its gradient with respect to $t$ to zero.\n$$\n\\frac{\\partial E}{\\partial t} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial t} \\| R x_{i} + t - y_{i} \\|^{2} = \\sum_{i=1}^{n} 2(R x_{i} + t - y_{i}) = 0\n$$\n$$\n\\sum_{i=1}^{n} (R x_{i} + t - y_{i}) = 0\n$$\n$$\nR \\left(\\sum_{i=1}^{n} x_{i} \\right) + \\sum_{i=1}^{n} t - \\sum_{i=1}^{n} y_{i} = 0\n$$\n$$\nR (n \\bar{x}) + n t - n \\bar{y} = 0\n$$\nwhere $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}$ and $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$ are the centroids of the point sets. Solving for $t$, we get the optimal translation $t_{opt}$:\n$$\nt_{opt} = \\bar{y} - R \\bar{x}\n$$\nThis result shows that the optimal translation aligns the centroid of the transformed source points, $R\\bar{x}$, with the centroid of the target points, $\\bar{y}$.\n\nNext, we substitute $t_{opt}$ back into the objective function $E(R,t)$ to obtain a new objective function that depends only on $R$.\n$$\nR x_{i} + t_{opt} - y_{i} = R x_{i} + (\\bar{y} - R \\bar{x}) - y_{i} = R(x_{i} - \\bar{x}) - (y_{i} - \\bar{y})\n$$\nUsing the centered points $x'_{i} = x_{i} - \\bar{x}$ and $y'_{i} = y_{i} - \\bar{y}$, the expression becomes $R x'_{i} - y'_{i}$. The objective function is now:\n$$\nE(R) = \\sum_{i=1}^{n} \\| R x'_{i} - y'_{i} \\|^{2}\n$$\nWe expand the squared norm:\n$$\n\\| R x'_{i} - y'_{i} \\|^{2} = (R x'_{i} - y'_{i})^{\\top}(R x'_{i} - y'_{i}) = (x_{i}'^{\\top}R^{\\top} - y_{i}'^{\\top})(R x'_{i} - y'_{i})\n$$\n$$\n= x_{i}'^{\\top}R^{\\top}R x'_{i} - x_{i}'^{\\top}R^{\\top}y'_{i} - y_{i}'^{\\top}R x'_{i} + y_{i}'^{\\top}y'_{i}\n$$\nSince $R$ is an orthogonal matrix, $R^{\\top}R = I$. The two cross-terms are scalars and are transposes of each other, so they are equal: $x_{i}'^{\\top}R^{\\top}y'_{i} = (y_{i}'^{\\top}R x'_{i})^{\\top} = y_{i}'^{\\top}R x'_{i}$. Thus, the expression simplifies to:\n$$\n\\| R x'_{i} - y'_{i} \\|^{2} = \\|x'_{i}\\|^{2} + \\|y'_{i}\\|^{2} - 2 y_{i}'^{\\top}R x'_{i}\n$$\nSumming over all points $i=1, \\dots, n$:\n$$\nE(R) = \\sum_{i=1}^{n} \\|x'_{i}\\|^{2} + \\sum_{i=1}^{n} \\|y'_{i}\\|^{2} - 2 \\sum_{i=1}^{n} y_{i}'^{\\top}R x'_{i}\n$$\nThe first two terms, $\\sum \\|x'_{i}\\|^{2}$ and $\\sum \\|y'_{i}\\|^{2}$, represent the variance of the centered point sets and are independent of $R$. Therefore, minimizing $E(R)$ is equivalent to maximizing the term $\\sum_{i=1}^{n} y_{i}'^{\\top}R x'_{i}$.\n\nWe can express this term using the trace operator. For a scalar $a$, $a = \\operatorname{Tr}(a)$.\n$$\n\\sum_{i=1}^{n} y_{i}'^{\\top}R x'_{i} = \\sum_{i=1}^{n} \\operatorname{Tr}( y_{i}'^{\\top}R x'_{i} )\n$$\nUsing the cyclic property of the trace, $\\operatorname{Tr}(ABC) = \\operatorname{Tr}(CAB)$:\n$$\n\\sum_{i=1}^{n} \\operatorname{Tr}(y_{i}'^{\\top}R x'_{i}) = \\sum_{i=1}^{n} \\operatorname{Tr}(R x'_{i} y_{i}'^{\\top}) = \\operatorname{Tr}\\left(R \\sum_{i=1}^{n} x'_{i} y_{i}'^{\\top}\\right)\n$$\nWe recognize the sum as the cross-covariance matrix $H = \\sum_{i=1}^{n} x'_{i} y_{i}'^{\\top}$. The optimization problem for $R$ is now:\n$$\n\\underset{R}{\\operatorname{maximize}} \\quad \\operatorname{Tr}(RH) \\quad \\text{subject to} \\quad R^{\\top}R = I, \\det(R) = 1\n$$\nLet the Singular Value Decomposition (SVD) of $H$ be $H = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal matrices and $\\Sigma = \\operatorname{diag}(\\sigma_1, \\sigma_2, \\sigma_3)$ is a diagonal matrix of singular values.\n$$\n\\operatorname{Tr}(RH) = \\operatorname{Tr}(R U \\Sigma V^{\\top})\n$$\nAgain, using the cyclic property of the trace, $\\operatorname{Tr}(ABCD) = \\operatorname{Tr}(DABC)$:\n$$\n\\operatorname{Tr}(R U \\Sigma V^{\\top}) = \\operatorname{Tr}(V^{\\top}R U \\Sigma)\n$$\nLet's define a new matrix $M = V^{\\top}R U$. Since $V^{\\top}$, $R$, and $U$ are all orthogonal matrices, their product $M$ is also an orthogonal matrix. We must maximize $\\operatorname{Tr}(M \\Sigma)$. The constraint $\\det(R)=1$ imposes a constraint on $M$:\n$$\n\\det(M) = \\det(V^{\\top}R U) = \\det(V^{\\top})\\det(R)\\det(U) = \\det(V)^{-1}(1)\\det(U)\n$$\nSince $V$ is orthogonal, $\\det(V) = \\pm 1$, so $\\det(V)^{-1} = \\det(V)$. Therefore, $\\det(M) = \\det(V)\\det(U)$. The problem statement defines $s = \\operatorname{sign}(\\det(U V^{\\top})) = \\det(U V^{\\top}) = \\det(U)\\det(V^{\\top}) = \\det(U)\\det(V)$. Thus, the constraint on $M$ is $\\det(M) = s$.\n\nOur task is to maximize $\\operatorname{Tr}(M\\Sigma) = \\sum_{j=1}^{3} M_{jj}\\sigma_j$ subject to $M$ being orthogonal and $\\det(M)=s$. Since $\\sigma_1 \\geq \\sigma_2 \\geq \\sigma_3 \\geq 0$ and the diagonal elements of an orthogonal matrix satisfy $|M_{jj}| \\leq 1$, we analyze two cases based on the value of $s$.\n\nCase 1: $s=1$. We need to find an orthogonal matrix $M$ with $\\det(M)=1$ (a proper rotation) that maximizes the sum. The maximum is achieved when $M_{jj}$ values are as large as possible, which is $M_{jj}=1$. The matrix $M = I = \\operatorname{diag}(1,1,1)$ satisfies the conditions, as $\\det(I)=1$. This gives the maximum possible trace value: $\\operatorname{Tr}(I\\Sigma) = \\sigma_1 + \\sigma_2 + \\sigma_3$.\n\nCase 2: $s=-1$. We need to find an orthogonal matrix $M$ with $\\det(M)=-1$ (a reflection) that maximizes the sum. To maximize the sum while having a determinant of $-1$, we should make one diagonal element $-1$ and the others $1$. To minimize the reduction in the sum, the $-1$ should be paired with the smallest singular value, $\\sigma_3$. Thus, the optimal matrix is $M = \\operatorname{diag}(1,1,-1)$, which has $\\det(M)=-1$. The maximized trace is $\\operatorname{Tr}(\\operatorname{diag}(1,1,-1)\\Sigma) = \\sigma_1 + \\sigma_2 - \\sigma_3$. This is the reflection case. The SVD procedure provides the best orthogonal mapping, which may not be a proper rotation. The correction involves inverting the direction associated with the smallest singular value, which corresponds to the least \"correlated\" axis between the two point sets.\n\nCombining both cases, the maximum value of $\\operatorname{Tr}(RH)$ is $\\sigma_1 + \\sigma_2 + s \\sigma_3$.\nThe optimal rotation $R_{opt}$ is found by solving $M_{opt} = V^{\\top}R_{opt}U$ for $R_{opt}$:\n$$\nR_{opt} = V M_{opt} U^{\\top}\n$$\nIf $s=1$, $M_{opt}=I$, and $R_{opt} = V U^{\\top}$.\nIf $s=-1$, $M_{opt}=\\operatorname{diag}(1,1,-1)$, and $R_{opt} = V \\operatorname{diag}(1,1,-1) U^{\\top}$.\n\nFinally, we compute the minimal value of the objective function, $E_{min}$, by substituting the maximized trace value back into the expression for $E(R)$.\n$$\nE_{min} = \\sum_{i=1}^{n} \\|x'_{i}\\|^{2} + \\sum_{i=1}^{n} \\|y'_{i}\\|^{2} - 2 \\max_{R}\\operatorname{Tr}(RH)\n$$\nSubstituting the maximized trace value $\\sigma_1 + \\sigma_2 + s \\sigma_3$ and the definitions of the centered points:\n$$\nE_{min} = \\sum_{i=1}^{n} \\|x_{i} - \\bar{x}\\|^{2} + \\sum_{i=1}^{n} \\|y_{i} - \\bar{y}\\|^{2} - 2(\\sigma_1 + \\sigma_2 + s \\sigma_3)\n$$\nThis is the final closed-form expression for the minimum sum of squared errors.",
            "answer": "$$\n\\boxed{\\sum_{i=1}^{n} \\|x_{i} - \\bar{x}\\|^{2} + \\sum_{i=1}^{n} \\|y_{i} - \\bar{y}\\|^{2} - 2(\\sigma_1 + \\sigma_2 + s \\sigma_3)}\n$$"
        },
        {
            "introduction": "Moving beyond rigid motion, affine transformations model global shearing and scaling, which can account for more complex anatomical changes. This practice explores the quantitative analysis of such transformations by calculating the Jacobian determinant . Understanding this concept is crucial for interpreting how a registration algorithm deforms tissue, specifically whether it locally expands or compresses volume.",
            "id": "4582032",
            "problem": "A three-dimensional synthetic grid of tissue landmarks sampled in millimeter coordinates is used to evaluate an affine registration between baseline and follow-up Magnetic Resonance Imaging (MRI). The registration map is an affine transformation $T : \\mathbb{R}^{3} \\to \\mathbb{R}^{3}$ given by $T(\\mathbf{x}) = A \\mathbf{x} + \\mathbf{b}$, where the $3 \\times 3$ matrix $A$ and translation vector $\\mathbf{b}$ are\n$$\nA = \\begin{pmatrix}\n1.08 & 0.03 & -0.02 \\\\\n0.01 & 0.92 & 0.05 \\\\\n0.04 & -0.02 & 1.15\n\\end{pmatrix}, \\quad\n\\mathbf{b} = \\begin{pmatrix}\n0.6 \\\\\n-0.4 \\\\\n1.2\n\\end{pmatrix}.\n$$\nFrom the perspective of medical image registration, the determinant of the Jacobian matrix of $T$ at a point quantifies the local volumetric change of an infinitesimal tissue element under the registration. Starting from first principles in multivariable calculus and continuum mechanics, determine the determinant of the Jacobian of $T$ at any point in the grid and interpret its implication for local volume change. Report the determinant as a unitless real number. No rounding is necessary; give an exact decimal value.",
            "solution": "The affine transformation is given by $T(\\mathbf{x}) = A\\mathbf{x} + \\mathbf{b}$. Let the vector $\\mathbf{x}$ be represented by its components $(x_1, x_2, x_3)^T$. The transformation $T$ maps $\\mathbf{x}$ to a new point $\\mathbf{y} = (y_1, y_2, y_3)^T$, where:\n$$\n\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix}\n1.08 & 0.03 & -0.02 \\\\\n0.01 & 0.92 & 0.05 \\\\\n0.04 & -0.02 & 1.15\n\\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} +\n\\begin{pmatrix} 0.6 \\\\ -0.4 \\\\ 1.2 \\end{pmatrix}\n$$\nThe components of the transformed vector $\\mathbf{y} = T(\\mathbf{x})$ are:\n$$\ny_1(x_1, x_2, x_3) = 1.08 x_1 + 0.03 x_2 - 0.02 x_3 + 0.6\n$$\n$$\ny_2(x_1, x_2, x_3) = 0.01 x_1 + 0.92 x_2 + 0.05 x_3 - 0.4\n$$\n$$\ny_3(x_1, x_2, x_3) = 0.04 x_1 - 0.02 x_2 + 1.15 x_3 + 1.2\n$$\nFrom first principles, the Jacobian matrix of the transformation $T$ at a point $\\mathbf{x}$, denoted $J_T(\\mathbf{x})$, is the matrix of all first-order partial derivatives of the component functions of $T$. For a transformation from $\\mathbb{R}^3$ to $\\mathbb{R}^3$, it is defined as:\n$$\nJ_T(\\mathbf{x}) = \\begin{pmatrix}\n\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_1}{\\partial x_3} \\\\\n\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_3} \\\\\n\\frac{\\partial y_3}{\\partial x_1} & \\frac{\\partial y_3}{\\partial x_2} & \\frac{\\partial y_3}{\\partial x_3}\n\\end{pmatrix}\n$$\nWe compute each partial derivative:\n$$\n\\frac{\\partial y_1}{\\partial x_1} = 1.08, \\quad \\frac{\\partial y_1}{\\partial x_2} = 0.03, \\quad \\frac{\\partial y_1}{\\partial x_3} = -0.02\n$$\n$$\n\\frac{\\partial y_2}{\\partial x_1} = 0.01, \\quad \\frac{\\partial y_2}{\\partial x_2} = 0.92, \\quad \\frac{\\partial y_2}{\\partial x_3} = 0.05\n$$\n$$\n\\frac{\\partial y_3}{\\partial x_1} = 0.04, \\quad \\frac{\\partial y_3}{\\partial x_2} = -0.02, \\quad \\frac{\\partial y_3}{\\partial x_3} = 1.15\n$$\nAssembling these derivatives into the Jacobian matrix, we find:\n$$\nJ_T(\\mathbf{x}) = \\begin{pmatrix}\n1.08 & 0.03 & -0.02 \\\\\n0.01 & 0.92 & 0.05 \\\\\n0.04 & -0.02 & 1.15\n\\end{pmatrix}\n$$\nThis demonstrates a fundamental property of affine transformations: the Jacobian matrix is constant for all points $\\mathbf{x}$ in the domain and is equal to the linear part of the transformation, the matrix $A$. The translation vector $\\mathbf{b}$ does not affect the derivatives.\n\nThe determinant of the Jacobian, $\\det(J_T)$, quantifies the local volume change factor. An infinitesimal volume element $dV$ at a point $\\mathbf{x}$ is transformed into an infinitesimal volume element whose volume is $|\\det(J_T(\\mathbf{x}))| dV$. Since $J_T = A$ is constant, the volume change factor is uniform throughout the space. We now compute the determinant of $A$:\n$$\n\\det(A) = \\det \\begin{pmatrix}\n1.08 & 0.03 & -0.02 \\\\\n0.01 & 0.92 & 0.05 \\\\\n0.04 & -0.02 & 1.15\n\\end{pmatrix}\n$$\nUsing cofactor expansion along the first row:\n$$\n\\det(A) = 1.08 \\begin{vmatrix} 0.92 & 0.05 \\\\ -0.02 & 1.15 \\end{vmatrix} - 0.03 \\begin{vmatrix} 0.01 & 0.05 \\\\ 0.04 & 1.15 \\end{vmatrix} + (-0.02) \\begin{vmatrix} 0.01 & 0.92 \\\\ 0.04 & -0.02 \\end{vmatrix}\n$$\n$$\n\\det(A) = 1.08 \\times ((0.92 \\times 1.15) - (0.05 \\times -0.02)) - 0.03 \\times ((0.01 \\times 1.15) - (0.05 \\times 0.04)) - 0.02 \\times ((0.01 \\times -0.02) - (0.92 \\times 0.04))\n$$\n$$\n\\det(A) = 1.08 \\times (1.058 - (-0.001)) - 0.03 \\times (0.0115 - 0.002) - 0.02 \\times (-0.0002 - 0.0368)\n$$\n$$\n\\det(A) = 1.08 \\times (1.059) - 0.03 \\times (0.0095) - 0.02 \\times (-0.037)\n$$\n$$\n\\det(A) = 1.14372 - 0.000285 + 0.00074\n$$\n$$\n\\det(A) = 1.144175\n$$\nThe determinant of the Jacobian is $1.144175$.\n\nInterpretation:\nThe determinant of the Jacobian, $\\det(J_T) = 1.144175$, is the factor by which volume is scaled locally under the transformation $T$.\n1. Since $\\det(J_T) > 0$, the transformation is orientation-preserving. This is physically expected for tissue deformation, as it means local regions are not turned \"inside-out\".\n2. Since $|\\det(J_T)| = 1.144175 > 1$, the transformation causes a local expansion of volume. Specifically, any infinitesimal volume of tissue is enlarged by a factor of $1.144175$. This corresponds to a volumetric increase of $(1.144175 - 1) \\times 100\\% = 14.4175\\%$.\n3. Because the transformation is affine, this volumetric expansion is uniform across the entire grid; it is the same at every point.",
            "answer": "$$\n\\boxed{1.144175}\n$$"
        },
        {
            "introduction": "Many advanced registration methods operate directly on image intensities rather than predefined landmarks. This exercise introduces the fundamental principle behind many of these techniques: the optical flow constraint, derived from the brightness constancy assumption . You will not only derive this core equation but also apply it to estimate a local displacement field, revealing how an ill-posed problem can be solved by introducing local structural assumptions.",
            "id": "4582048",
            "problem": "A two-dimensional (2D) Magnetic Resonance Imaging (MRI) slice sequence is acquired at times $t$ and $t+\\delta t$. Let the image intensity be $I(x,y,t)$, where $(x,y)$ denotes spatial coordinates, and let the unknown in-plane tissue velocity field at time $t$ be $u(x,y) = (u_{x}(x,y), u_{y}(x,y))$. Assume small deformations between frames so that first-order expansions are valid and that the brightness constancy principle holds, meaning that the intensity of a material point of tissue does not change as it moves. Starting from the definition of brightness constancy and basic kinematics, derive the first-order linear relation that links the temporal variation of intensity, the spatial gradient of intensity, and the velocity field under the small-deformation assumption.\n\nThen, suppose within a local patch the velocity is approximately constant, $u(x,y) \\approx (u_{x}, u_{y})$, and the following spatial gradients and temporal intensity derivatives (per frame) have been measured at three pixels:\n- Pixel $1$: $\\nabla I = (3, 1)$, $I_{t} = -7$,\n- Pixel $2$: $\\nabla I = (1, 2)$, $I_{t} = -5$,\n- Pixel $3$: $\\nabla I = (2, 2)$, $I_{t} = -8$.\n\nFormulate the least-squares problem implied by the derived linear relation to estimate the constant velocity $u = (u_{x}, u_{y})$ from these measurements, and solve it exactly. Express your final answer as exact fractions in voxels per frame, using a row matrix with entries $u_{x}$ and $u_{y}$. No rounding is required.\n\nIn addition, state succinctly how the derived linear relation is exploited in small-deformation registration algorithms to estimate incremental displacements.\n\nThe final answer must be the estimated velocity components in a single row matrix, expressed in voxels per frame.",
            "solution": "**Part 1: Derivation of the Linear Relation**\n\nThe brightness constancy principle states that the intensity of a specific tissue point remains constant over time as it moves. Let a point be at position $(x, y)$ at time $t$. At a short time $\\delta t$ later, it will have moved to a new position $(x + \\delta x, y + \\delta y)$. The velocity components are $u_x = \\frac{dx}{dt}$ and $u_y = \\frac{dy}{dt}$. For a small time interval $\\delta t$, the displacements are $\\delta x \\approx u_x \\delta t$ and $\\delta y \\approx u_y \\delta t$.\n\nThe brightness constancy can be expressed mathematically as:\n$$I(x + \\delta x, y + \\delta y, t + \\delta t) = I(x, y, t)$$\nSubstituting the expressions for the displacements:\n$$I(x + u_x \\delta t, y + u_y \\delta t, t + \\delta t) = I(x, y, t)$$\nThe problem states that we can assume small deformations, which justifies a first-order Taylor series expansion of the left-hand side around the point $(x, y, t)$:\n$$I(x, y, t) + \\frac{\\partial I}{\\partial x}(u_x \\delta t) + \\frac{\\partial I}{\\partial y}(u_y \\delta t) + \\frac{\\partial I}{\\partial t}(\\delta t) + O((\\delta t)^2) \\approx I(x, y, t)$$\nSubtracting $I(x, y, t)$ from both sides and neglecting higher-order terms gives:\n$$\\frac{\\partial I}{\\partial x} u_x \\delta t + \\frac{\\partial I}{\\partial y} u_y \\delta t + \\frac{\\partial I}{\\partial t} \\delta t \\approx 0$$\nDividing by the non-zero time interval $\\delta t$, we obtain the desired first-order linear relation, known as the optical flow constraint equation:\n$$\\frac{\\partial I}{\\partial x} u_x + \\frac{\\partial I}{\\partial y} u_y + \\frac{\\partial I}{\\partial t} = 0$$\nUsing vector notation, let the spatial intensity gradient be $\\nabla I = \\left(\\frac{\\partial I}{\\partial x}, \\frac{\\partial I}{\\partial y}\\right)$, the velocity vector be $u = (u_x, u_y)$, and the temporal derivative be $I_t = \\frac{\\partial I}{\\partial t}$. The equation becomes:\n$$\\nabla I \\cdot u + I_t = 0$$\n\n**Part 2: Least-Squares Estimation of Velocity**\n\nWe are given measurements from three pixels and are asked to find a single constant velocity vector $u = (u_x, u_y)$ that best fits all three measurements in a least-squares sense. The equation for each pixel $i$ is $(\\nabla I)_i \\cdot u = -(I_t)_i$. This gives us a system of linear equations:\n$$\n\\begin{cases}\n(\\frac{\\partial I}{\\partial x})_1 u_x + (\\frac{\\partial I}{\\partial y})_1 u_y = -(I_t)_1 \\\\\n(\\frac{\\partial I}{\\partial x})_2 u_x + (\\frac{\\partial I}{\\partial y})_2 u_y = -(I_t)_2 \\\\\n(\\frac{\\partial I}{\\partial x})_3 u_x + (\\frac{\\partial I}{\\partial y})_3 u_y = -(I_t)_3\n\\end{cases}\n$$\nSubstituting the given values:\n- Pixel $1$: $\\nabla I = (3, 1)$, $I_{t} = -7 \\implies 3u_x + 1u_y = 7$\n- Pixel $2$: $\\nabla I = (1, 2)$, $I_{t} = -5 \\implies 1u_x + 2u_y = 5$\n- Pixel $3$: $\\nabla I = (2, 2)$, $I_{t} = -8 \\implies 2u_x + 2u_y = 8$\n\nThis is an overdetermined system of the form $A\\mathbf{x} = \\mathbf{b}$, where $\\mathbf{x} = \\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix}$. The matrix $A$ and vector $\\mathbf{b}$ are:\n$$ A = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\\\ 2 & 2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 7 \\\\ 5 \\\\ 8 \\end{pmatrix} $$\nThe least-squares problem is to find the vector $\\mathbf{x}$ that minimizes the squared Euclidean norm of the residual, $\\|A\\mathbf{x} - \\mathbf{b}\\|^2$. The solution is given by the normal equations:\n$$ A^T A \\mathbf{x} = A^T \\mathbf{b} $$\nFirst, we compute $A^T A$:\n$$ A^T A = \\begin{pmatrix} 3 & 1 & 2 \\\\ 1 & 2 & 2 \\end{pmatrix} \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\\\ 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 3(3)+1(1)+2(2) & 3(1)+1(2)+2(2) \\\\ 1(3)+2(1)+2(2) & 1(1)+2(2)+2(2) \\end{pmatrix} = \\begin{pmatrix} 9+1+4 & 3+2+4 \\\\ 3+2+4 & 1+4+4 \\end{pmatrix} = \\begin{pmatrix} 14 & 9 \\\\ 9 & 9 \\end{pmatrix} $$\nNext, we compute $A^T \\mathbf{b}$:\n$$ A^T \\mathbf{b} = \\begin{pmatrix} 3 & 1 & 2 \\\\ 1 & 2 & 2 \\end{pmatrix} \\begin{pmatrix} 7 \\\\ 5 \\\\ 8 \\end{pmatrix} = \\begin{pmatrix} 3(7)+1(5)+2(8) \\\\ 1(7)+2(5)+2(8) \\end{pmatrix} = \\begin{pmatrix} 21+5+16 \\\\ 7+10+16 \\end{pmatrix} = \\begin{pmatrix} 42 \\\\ 33 \\end{pmatrix} $$\nThe normal equations are:\n$$ \\begin{pmatrix} 14 & 9 \\\\ 9 & 9 \\end{pmatrix} \\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix} = \\begin{pmatrix} 42 \\\\ 33 \\end{pmatrix} $$\nThis is a system of two linear equations:\n1. $14u_x + 9u_y = 42$\n2. $9u_x + 9u_y = 33$\n\nFrom the second equation, we can simplify by dividing by $3$:\n$$ 3u_x + 3u_y = 11 \\implies u_y = \\frac{11 - 3u_x}{3} $$\nSubstitute this into the first equation:\n$$ 14u_x + 9\\left(\\frac{11 - 3u_x}{3}\\right) = 42 $$\n$$ 14u_x + 3(11 - 3u_x) = 42 $$\n$$ 14u_x + 33 - 9u_x = 42 $$\n$$ 5u_x = 42 - 33 $$\n$$ 5u_x = 9 $$\n$$ u_x = \\frac{9}{5} $$\nNow, we find $u_y$:\n$$ u_y = \\frac{11 - 3(\\frac{9}{5})}{3} = \\frac{\\frac{55}{5} - \\frac{27}{5}}{3} = \\frac{\\frac{28}{5}}{3} = \\frac{28}{15} $$\nThe estimated constant velocity is $u = \\left(\\frac{9}{5}, \\frac{28}{15}\\right)$ voxels per frame.\n\n**Part 3: Use in Small-Deformation Registration**\n\nIn small-deformation image registration, the goal is to find a displacement field that maps one image to another. The derived linear relation, $\\nabla I \\cdot u + I_t = 0$, provides the fundamental constraint for this task. Here, $u$ is the velocity which, when multiplied by the time step, gives the incremental displacement field, and $I_t$ represents the intensity difference between the images to be registered. This single equation is ill-posed as it provides only one constraint for two unknown displacement components $(u_x, u_y)$ at each pixel (the aperture problem). Registration algorithms overcome this by introducing additional constraints. For example, the Lucas-Kanade method, as demonstrated in this problem, assumes the displacement is constant over a local image patch, creating an overdetermined system that can be solved via least squares. For larger deformations, registration is often performed iteratively: the algorithm computes a small, incremental displacement field using this linear relation, warps the image with this field, and repeats the process until the images are aligned.",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{9}{5} & \\frac{28}{15} \\end{pmatrix}} $$"
        }
    ]
}