{
    "hands_on_practices": [
        {
            "introduction": "To unlock the full potential of richly annotated biomedical databases, we must be able to formulate precise questions. This practice demonstrates how to translate a specific biological inquiry—finding gene annotations supported by experimental evidence within a particular functional category—into a formal SPARQL query. By constructing and justifying each part of the query, you will gain hands-on experience with the logic of querying RDF graphs, a foundational skill for navigating and integrating semantic data resources .",
            "id": "4543585",
            "problem": "You are given a simplified, in-memory representation of Resource Description Framework (RDF) data and asked to construct and evaluate a Structured Query Language for RDF (SPARQL) query that retrieves Gene Ontology (GO) annotations meeting specific biological and ontological constraints. The goal is to formalize the query using the semantics of conjunctive query evaluation over RDF graphs and to compute the number of distinct matching annotations in each of several test datasets.\n\nFoundational base and definitions to use:\n- Resource Description Framework (RDF): An RDF graph is a set of triples, each triple being an ordered tuple of subject, predicate, and object. Denote a graph by a set $G \\subseteq U \\times U \\times (U \\cup L \\cup B)$, where $U$ is the set of Uniform Resource Identifiers (URIs), $L$ is the set of literals, and $B$ is the set of blank nodes.\n- SPARQL basic graph patterns: A basic graph pattern is a finite set of triple patterns with variables. A solution mapping is a partial function $\\mu: V \\to (U \\cup L \\cup B)$, where $V$ is the set of variables. A triple pattern $(s,p,o)$ matches a triple $(s',p',o')$ under $\\mu$ if, for each component, either it is a constant equal to the constant in the triple or it is a variable $v \\in V$ and $\\mu(v)$ equals the corresponding component of the triple; variables may be unbound before matching and become bound during evaluation. A basic graph pattern evaluates to the set of mappings that satisfy all triple patterns conjunctively. Filters apply a boolean predicate on mappings. Projection selects specified variables, and DISTINCT yields set semantics over projected tuples.\n- Gene Ontology (GO): GO terms are categorized into three aspects: Molecular Function, Biological Process, and Cellular Component. The task focuses on filtering annotations by the Molecular Function aspect.\n- Evidence code: In GO, the code \"EXP\" denotes \"Inferred from Experiment\". Treat \"EXP\" as a specific constant in the data model.\n\nYour task:\n- Construct a SPARQL query that retrieves all distinct annotation nodes for which the evidence code is \"EXP\" and the associated GO term is in the Molecular Function namespace. Use the following simplified vocabulary (prefix \"ex:\" is implied for domain-specific terms):\n  - ex:Annotation (class for annotation nodes)\n  - rdf:type (type predicate)\n  - ex:hasEvidence (annotation to evidence code)\n  - ex:EXP (evidence code for \"Inferred from Experiment\")\n  - ex:hasGO (annotation to GO term)\n  - ex:hasAspect (GO term to aspect)\n  - ex:MolecularFunction, ex:BiologicalProcess, ex:CellularComponent (GO aspects)\n- You must justify each triple pattern and each filter by mapping it to a necessary biological or semantic constraint grounded in the above definitions.\n- Implement a program that evaluates your query over multiple test graphs under the formal semantics of basic graph patterns as homomorphisms. For this problem, you must treat FILTER isIRI on the annotation variable as a necessary filter to exclude blank node annotations, thereby ensuring reportable identifiers. Use projection on the annotation variable and DISTINCT to avoid duplicate reporting of the same annotation.\n\nFormal evaluation requirements:\n- Let $G$ be an RDF graph and let $P$ be the set of triple patterns in your query. Evaluation must proceed by constructing the set of solution mappings starting from the empty mapping and iteratively joining with matches of each triple pattern. Apply the FILTER condition $isIRI(?ann)$ to exclude mappings where the value bound to $?ann$ is not a URI. Project the variable $?ann$ and apply DISTINCT to form a set of unique annotation identifiers. The final result for a graph $G$ is the cardinality of this set, which is a nonnegative integer.\n\nData model for tests:\n- Each RDF triple is represented as a tuple of three strings. URIs are strings like \"ex:annA\" or \"rdf:type\". Blank nodes are strings starting with \"_:\". Literals are strings enclosed in double quotes (not used in this test suite). The graphs below are expressed in terms of these triples.\n\nTest suite:\n- There are $4$ test graphs. For each, compute the integer equal to the number of distinct annotation URIs returned by your query evaluation.\n\nTest graph $G_1$ (general case with mixed qualifying and non-qualifying annotations):\n- (\"ex:annA\",\"rdf:type\",\"ex:Annotation\")\n- (\"ex:annA\",\"ex:hasEvidence\",\"ex:EXP\")\n- (\"ex:annA\",\"ex:hasGO\",\"ex:GO_MF\")\n- (\"ex:GO_MF\",\"ex:hasAspect\",\"ex:MolecularFunction\")\n- (\"ex:annB\",\"rdf:type\",\"ex:Annotation\")\n- (\"ex:annB\",\"ex:hasEvidence\",\"ex:IDA\")\n- (\"ex:annB\",\"ex:hasGO\",\"ex:GO_MF\")\n- (\"ex:annC\",\"rdf:type\",\"ex:Annotation\")\n- (\"ex:annC\",\"ex:hasEvidence\",\"ex:EXP\")\n- (\"ex:annC\",\"ex:hasGO\",\"ex:GO_BP\")\n- (\"ex:GO_BP\",\"ex:hasAspect\",\"ex:BiologicalProcess\")\n- (\"ex:annD\",\"rdf:type\",\"ex:Annotation\")\n- (\"ex:annD\",\"ex:hasEvidence\",\"ex:EXP\")\n- (\"ex:annD\",\"ex:hasGO\",\"ex:GO_MF2\")\n- (\"ex:GO_MF2\",\"ex:hasAspect\",\"ex:MolecularFunction\")\n\nTest graph $G_2$ (boundary case with zero matches):\n- (\"ex:annE\",\"rdf:type\",\"ex:Annotation\")\n- (\"ex:annE\",\"ex:hasEvidence\",\"ex:IDA\")\n- (\"ex:annE\",\"ex:hasGO\",\"ex:GO_MF\")\n- (\"ex:GO_MF\",\"ex:hasAspect\",\"ex:MolecularFunction\")\n- (\"ex:annF\",\"rdf:type\",\"ex:Annotation\")\n- (\"ex:annF\",\"ex:hasEvidence\",\"ex:EXP\")\n\nTest graph $G_3$ (edge case with a blank node annotation and a GO term with multiple aspects):\n- (\"ex:annG\",\"rdf:type\",\"ex:Annotation\")\n- (\"ex:annG\",\"ex:hasEvidence\",\"ex:EXP\")\n- (\"ex:annG\",\"ex:hasGO\",\"ex:GO_MFCC\")\n- (\"_:b1\",\"rdf:type\",\"ex:Annotation\")\n- (\"_:b1\",\"ex:hasEvidence\",\"ex:EXP\")\n- (\"_:b1\",\"ex:hasGO\",\"ex:GO_MFCC\")\n- (\"ex:GO_MFCC\",\"ex:hasAspect\",\"ex:MolecularFunction\")\n- (\"ex:GO_MFCC\",\"ex:hasAspect\",\"ex:CellularComponent\")\n\nTest graph $G_4$ (edge case with multiple evidence statements and mixed aspects):\n- (\"ex:annH\",\"rdf:type\",\"ex:Annotation\")\n- (\"ex:annH\",\"ex:hasEvidence\",\"ex:EXP\")\n- (\"ex:annH\",\"ex:hasEvidence\",\"ex:IPI\")\n- (\"ex:annH\",\"ex:hasGO\",\"ex:GO_POISED\")\n- (\"ex:GO_POISED\",\"ex:hasAspect\",\"ex:MolecularFunction\")\n- (\"ex:annI\",\"rdf:type\",\"ex:Annotation\")\n- (\"ex:annI\",\"ex:hasEvidence\",\"ex:EXP\")\n- (\"ex:annI\",\"ex:hasGO\",\"ex:GO_BPONLY\")\n- (\"ex:GO_BPONLY\",\"ex:hasAspect\",\"ex:BiologicalProcess\")\n\nProgramming task and output specification:\n- Implement a complete program that:\n  - Constructs the SPARQL query string with appropriate PREFIX declarations, a SELECT DISTINCT clause on the annotation variable, the necessary triple patterns, and a FILTER ensuring $isIRI(?ann)$.\n  - Implements a basic graph pattern evaluator as described above to evaluate the query against each $G_i$.\n  - For each $G_i$, computes the integer equal to the number of distinct annotation URIs satisfying the query.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[|Q(G_1)|,|Q(G_2)|,|Q(G_3)|,|Q(G_4)|]$, where $|Q(G_i)|$ denotes the cardinality of the distinct projected solutions for graph $G_i$.",
            "solution": "We begin from first principles of Resource Description Framework (RDF) graphs and Structured Query Language for RDF (SPARQL) evaluation. An RDF graph is a set $G \\subseteq U \\times U \\times (U \\cup L \\cup B)$, where $U$ is the set of Uniform Resource Identifiers (URIs), $L$ is the set of literals, and $B$ is the set of blank nodes. A SPARQL basic graph pattern is a finite set $P = \\{t_1,\\dots,t_k\\}$ of triple patterns, each $t_j$ being a triple over $(U \\cup V) \\times (U \\cup V) \\times (U \\cup L \\cup B \\cup V)$, where $V$ is the set of variables such that $V \\cap (U \\cup L \\cup B) = \\emptyset$. A solution mapping is a partial function $\\mu: V \\to (U \\cup L \\cup B)$.\n\nA triple pattern $t=(s,p,o)$ matches a triple $(s',p',o') \\in G$ under a mapping $\\mu$ if for each position we have either a constant equality (if the component is in $U \\cup L \\cup B$) or a variable $v \\in V$ such that either $v \\notin \\mathrm{dom}(\\mu)$ and we extend $\\mu$ by $\\{v \\mapsto s'\\}$ (or $p'$ or $o'$ as appropriate), or $v \\in \\mathrm{dom}(\\mu)$ and $\\mu(v)$ equals the corresponding component of the triple. The evaluation of a basic graph pattern $P$ over $G$ yields the set\n$$\n\\llbracket P \\rrbracket_G = \\{\\mu \\mid \\mu \\text{ is a mapping that makes all } t \\in P \\text{ simultaneously match some triples in } G\\}.\n$$\nA filter condition is a boolean predicate $\\phi$ over variables; applying FILTER keeps only those $\\mu \\in \\llbracket P \\rrbracket_G$ for which $\\phi(\\mu)$ evaluates to true. Projection on variables $W \\subseteq V$ yields the multiset or set (with DISTINCT) of tuples $\\mu|_W$. We will use DISTINCT to ensure set semantics over the projected solutions. The specific filter used is $isIRI(?ann)$, which enforces that $\\mu(?ann) \\in U$ and excludes $\\mu(?ann) \\in B$ or $\\mu(?ann) \\in L$.\n\nMapping the biological problem to triple patterns:\n- We require that the result is an annotation node. This is enforced by the pattern $t_1 = (?ann, rdf:type, ex:Annotation)$, which ensures that the subject bound to the variable $?ann$ is of type ex:Annotation.\n- We require that the annotation has experimental evidence code \"EXP\". This is enforced by the pattern $t_2 = (?ann, ex:hasEvidence, ex:EXP)$, using a constant object ex:EXP to match the \"Inferred from Experiment\" evidence code.\n- We require that the annotation refers to a Gene Ontology (GO) term. This is enforced by the pattern $t_3 = (?ann, ex:hasGO, ?go)$, which binds the variable $?go$ to the referenced GO term.\n- We require that the GO term is within the Molecular Function namespace (aspect). This is enforced by the pattern $t_4 = (?go, ex:hasAspect, ex:MolecularFunction)$.\n- We must ensure that the reported identifier is an Internationalized Resource Identifier (IRI), not a blank node, for downstream interoperability. This is enforced by the filter $\\mathrm{isIRI}(?ann)$, a predicate that accepts only $\\mu$ satisfying $\\mu(?ann) \\in U$.\n- We must avoid duplicates if multiple supporting triples could produce the same $?ann$ binding (for example, if a GO term had multiple aspects or the annotation had multiple statements that do not change $?ann$). Therefore, we use SELECT DISTINCT over $?ann$ so that the projection yields a set of unique annotation identifiers.\n\nPutting these elements together yields the SPARQL query structure:\n- PREFIX declarations for ex: and rdf:.\n- SELECT DISTINCT ?ann\n- WHERE with the conjunction of $t_1, t_2, t_3, t_4$ and FILTER isIRI(?ann).\n\nAlgorithmic evaluation design:\n- Let $P = [t_1, t_2, t_3, t_4]$ be the ordered list of triple patterns. We evaluate the basic graph pattern by iterative join:\n  1. Initialize the set of partial mappings $M_0 = \\{\\emptyset\\}$.\n  2. For $j = 1$ to $4$, compute\n     $$\n     M_j = \\{\\mu' \\mid \\mu \\in M_{j-1},\\ \\exists (s',p',o') \\in G: \\text{$t_j$ matches $(s',p',o')$ under $\\mu$ and $\\mu'$ is $\\mu$ extended consistently}\\}.\n     $$\n  3. Apply the filter: $M_f = \\{\\mu \\in M_4 \\mid \\mathrm{isIRI}(\\mu(?ann))\\}$.\n  4. Project on $?ann$ and apply DISTINCT to obtain $S = \\{\\mu(?ann) \\mid \\mu \\in M_f\\}$.\n  5. The result is the integer $\\lvert S \\rvert$.\n- The function $\\mathrm{isIRI}(x)$ is implemented as true if and only if $x$ is a string denoting a URI, which in our representation means it does not start with the blank node prefix \"_:\" and is not a quoted literal.\n\nCorrectness justification:\n- By RDF typing, $t_1$ restricts candidates to annotation resources, which is necessary to avoid conflating other nodes (such as GO terms or general resources) with annotations.\n- By evidence modeling, ex:hasEvidence with object ex:EXP precisely picks out annotations supported by experimental evidence, aligning with the intended Evidence and Conclusion Ontology (ECO) code \"EXP\".\n- By the structure of GO annotation, an annotation must refer to a GO term; $t_3$ binds this term to $?go$.\n- By the definition of GO aspects, ex:hasAspect ensures that only GO terms in the Molecular Function aspect are accepted; $t_4$ enforces the biological namespace constraint.\n- The filter $\\mathrm{isIRI}(?ann)$ enforces that results are in $U$, excluding blank nodes that cannot be dereferenced or globally referenced.\n- DISTINCT on $?ann$ ensures that the final set $S$ contains each annotation at most once even if multiple supporting triples would otherwise yield multiple solution mappings with the same $?ann$ binding; thus, the output reflects the count of unique annotations.\n\nApplying to the test suite:\n- For $G_1$, annotations ex:annA and ex:annD satisfy all patterns and the filter; ex:annB fails $t_2$ (evidence not ex:EXP) and ex:annC fails $t_4$ (aspect is BiologicalProcess). Thus $\\lvert S \\rvert = 2$.\n- For $G_2$, ex:annE fails $t_2$ (evidence not ex:EXP), and ex:annF fails $t_3$ (no GO term bound), so no mapping satisfies all patterns; thus $\\lvert S \\rvert = 0$.\n- For $G_3$, both ex:annG and the blank node \"_:b1\" satisfy the basic graph pattern, but the filter $\\mathrm{isIRI}(?ann)$ excludes the blank node, leaving only ex:annG; thus $\\lvert S \\rvert = 1$.\n- For $G_4$, ex:annH satisfies all patterns and the filter (multiple evidence statements include ex:EXP, and the GO term has aspect MolecularFunction), while ex:annI fails $t_4$ (aspect is BiologicalProcess). Thus $\\lvert S \\rvert = 1$.\n\nTherefore, the computed results in order $[|Q(G_1)|,|Q(G_2)|,|Q(G_3)|,|Q(G_4)|]$ are $[2, 0, 1, 1]$.\n\nThe program to be implemented constructs the SPARQL query string, evaluates the basic graph pattern with the filter and DISTINCT projection over each $G_i$, and prints the single-line output containing the four integers as a comma-separated list enclosed in square brackets.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nfrom typing import List, Tuple, Dict\n\nTriple = Tuple[str, str, str]\nMapping = Dict[str, str]\n\ndef is_variable(term: str) -> bool:\n    return term.startswith(\"?\")\n\ndef substitute(pattern: Tuple[str, str, str], mapping: Mapping) -> Tuple[str, str, str]:\n    s, p, o = pattern\n    s_sub = mapping.get(s, s) if is_variable(s) else s\n    p_sub = mapping.get(p, p) if is_variable(p) else p\n    o_sub = mapping.get(o, o) if is_variable(o) else o\n    return (s_sub, p_sub, o_sub)\n\ndef match_and_extend(pattern: Tuple[str, str, str], triple: Triple, mapping: Mapping) -> Mapping | None:\n    # Attempt to match pattern to triple under existing mapping; return extended mapping if consistent.\n    s_pat, p_pat, o_pat = pattern\n    s_tr, p_tr, o_tr = triple\n    new_mapping = dict(mapping)\n\n    # Helper to unify one component\n    def unify(pat: str, trm: str) -> bool:\n        if is_variable(pat):\n            if pat in new_mapping:\n                return new_mapping[pat] == trm\n            else:\n                new_mapping[pat] = trm\n                return True\n        else:\n            return pat == trm\n\n    if not unify(s_pat, s_tr):\n        return None\n    if not unify(p_pat, p_tr):\n        return None\n    if not unify(o_pat, o_tr):\n        return None\n    return new_mapping\n\ndef evaluate_bgp(graph: List[Triple], patterns: List[Tuple[str, str, str]]) -> List[Mapping]:\n    # Iterative join over triples for each pattern\n    mappings: List[Mapping] = [dict()]\n    for pat in patterns:\n        next_mappings: List[Mapping] = []\n        for m in mappings:\n            # Apply current mapping to pattern (for bound variables)\n            s, p, o = pat\n            s_eff = m.get(s, s) if is_variable(s) else s\n            p_eff = m.get(p, p) if is_variable(p) else p\n            o_eff = m.get(o, o) if is_variable(o) else o\n            eff_pat = (s_eff, p_eff, o_eff)\n            for triple in graph:\n                ext = match_and_extend(eff_pat, triple, m)\n                if ext is not None:\n                    next_mappings.append(ext)\n        mappings = next_mappings\n        if not mappings:\n            break\n    return mappings\n\ndef is_iri(value: str) -> bool:\n    # In our representation, IRIs are non-blank, non-literal strings.\n    if value.startswith(\"_:\"):\n        return False\n    if len(value) >= 2 and value.startswith('\"') and value.endswith('\"'):\n        return False\n    return True\n\ndef project_distinct(mappings: List[Mapping], vars_to_project: List[str]) -> List[Tuple[str, ...]]:\n    seen = set()\n    results: List[Tuple[str, ...]] = []\n    for m in mappings:\n        tup = tuple(m[v] for v in vars_to_project if v in m)\n        if len(tup) != len(vars_to_project):\n            # Skip mappings that do not bind all projected variables\n            continue\n        if tup not in seen:\n            seen.add(tup)\n            results.append(tup)\n    return results\n\ndef build_query_string() -> str:\n    # Construct the SPARQL query string as specified\n    prefixes = [\n        \"PREFIX ex: <http://example.org/ex#>\",\n        \"PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\"\n    ]\n    select = \"SELECT DISTINCT ?ann WHERE {\"\n    patterns = [\n        \"  ?ann rdf:type ex:Annotation .\",\n        \"  ?ann ex:hasEvidence ex:EXP .\",\n        \"  ?ann ex:hasGO ?go .\",\n        \"  ?go ex:hasAspect ex:MolecularFunction .\",\n        \"  FILTER isIRI(?ann)\"\n    ]\n    closing = \"}\"\n    return \"\\n\".join(prefixes + [select] + patterns + [closing])\n\ndef solve():\n    # Define the test cases from the problem statement.\n    G1: List[Triple] = [\n        (\"ex:annA\",\"rdf:type\",\"ex:Annotation\"),\n        (\"ex:annA\",\"ex:hasEvidence\",\"ex:EXP\"),\n        (\"ex:annA\",\"ex:hasGO\",\"ex:GO_MF\"),\n        (\"ex:GO_MF\",\"ex:hasAspect\",\"ex:MolecularFunction\"),\n        (\"ex:annB\",\"rdf:type\",\"ex:Annotation\"),\n        (\"ex:annB\",\"ex:hasEvidence\",\"ex:IDA\"),\n        (\"ex:annB\",\"ex:hasGO\",\"ex:GO_MF\"),\n        (\"ex:annC\",\"rdf:type\",\"ex:Annotation\"),\n        (\"ex:annC\",\"ex:hasEvidence\",\"ex:EXP\"),\n        (\"ex:annC\",\"ex:hasGO\",\"ex:GO_BP\"),\n        (\"ex:GO_BP\",\"ex:hasAspect\",\"ex:BiologicalProcess\"),\n        (\"ex:annD\",\"rdf:type\",\"ex:Annotation\"),\n        (\"ex:annD\",\"ex:hasEvidence\",\"ex:EXP\"),\n        (\"ex:annD\",\"ex:hasGO\",\"ex:GO_MF2\"),\n        (\"ex:GO_MF2\",\"ex:hasAspect\",\"ex:MolecularFunction\"),\n    ]\n\n    G2: List[Triple] = [\n        (\"ex:annE\",\"rdf:type\",\"ex:Annotation\"),\n        (\"ex:annE\",\"ex:hasEvidence\",\"ex:IDA\"),\n        (\"ex:annE\",\"ex:hasGO\",\"ex:GO_MF\"),\n        (\"ex:GO_MF\",\"ex:hasAspect\",\"ex:MolecularFunction\"),\n        (\"ex:annF\",\"rdf:type\",\"ex:Annotation\"),\n        (\"ex:annF\",\"ex:hasEvidence\",\"ex:EXP\"),\n    ]\n\n    G3: List[Triple] = [\n        (\"ex:annG\",\"rdf:type\",\"ex:Annotation\"),\n        (\"ex:annG\",\"ex:hasEvidence\",\"ex:EXP\"),\n        (\"ex:annG\",\"ex:hasGO\",\"ex:GO_MFCC\"),\n        (\"_:b1\",\"rdf:type\",\"ex:Annotation\"),\n        (\"_:b1\",\"ex:hasEvidence\",\"ex:EXP\"),\n        (\"_:b1\",\"ex:hasGO\",\"ex:GO_MFCC\"),\n        (\"ex:GO_MFCC\",\"ex:hasAspect\",\"ex:MolecularFunction\"),\n        (\"ex:GO_MFCC\",\"ex:hasAspect\",\"ex:CellularComponent\"),\n    ]\n\n    G4: List[Triple] = [\n        (\"ex:annH\",\"rdf:type\",\"ex:Annotation\"),\n        (\"ex:annH\",\"ex:hasEvidence\",\"ex:EXP\"),\n        (\"ex:annH\",\"ex:hasEvidence\",\"ex:IPI\"),\n        (\"ex:annH\",\"ex:hasGO\",\"ex:GO_POISED\"),\n        (\"ex:GO_POISED\",\"ex:hasAspect\",\"ex:MolecularFunction\"),\n        (\"ex:annI\",\"rdf:type\",\"ex:Annotation\"),\n        (\"ex:annI\",\"ex:hasEvidence\",\"ex:EXP\"),\n        (\"ex:annI\",\"ex:hasGO\",\"ex:GO_BPONLY\"),\n        (\"ex:GO_BPONLY\",\"ex:hasAspect\",\"ex:BiologicalProcess\"),\n    ]\n\n    test_cases = [G1, G2, G3, G4]\n\n    # Define the query as triple patterns and a filter\n    triple_patterns: List[Tuple[str, str, str]] = [\n        (\"?ann\", \"rdf:type\", \"ex:Annotation\"),\n        (\"?ann\", \"ex:hasEvidence\", \"ex:EXP\"),\n        (\"?ann\", \"ex:hasGO\", \"?go\"),\n        (\"?go\", \"ex:hasAspect\", \"ex:MolecularFunction\"),\n    ]\n\n    # Construct the SPARQL query string (not used for evaluation but built per problem)\n    _query_string = build_query_string()\n\n    results: List[int] = []\n    for G in test_cases:\n        mappings = evaluate_bgp(G, triple_patterns)\n        # Apply FILTER isIRI(?ann)\n        mappings = [m for m in mappings if (\"?ann\" in m and is_iri(m[\"?ann\"]))]\n        # Project and apply DISTINCT on ?ann\n        projected = project_distinct(mappings, [\"?ann\"])\n        results.append(len(projected))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "After identifying a set of interesting genes, a key analytical step is to determine which biological functions or processes are statistically over-represented. This exercise walks you through the core mechanics of a Gene Ontology (GO) enrichment analysis, starting from the fundamental hypergeometric test for over-representation. You will learn to calculate enrichment $p$-values and, crucially, apply the Benjamini-Hochberg procedure to control the false discovery rate, a non-negotiable step when performing many simultaneous tests .",
            "id": "4543495",
            "problem": "A research team is studying differential expression results and seeks to evaluate biological process enrichment using the Gene Ontology (GO; Gene Ontology Consortium) within a curated experiment-specific universe. In this experiment, the analysis universe contains $N=20$ protein-coding genes, and the query gene set (significantly upregulated) contains $M=6$ genes. Six GO biological process terms are preselected based on prior knowledge, with the following annotation counts in the universe and observed overlaps with the query set: term $T_{1}$ has $K_{1}=8$ annotated genes with an observed overlap $x_{1}=4$; term $T_{2}$ has $K_{2}=7$ with $x_{2}=4$; term $T_{3}$ has $K_{3}=5$ with $x_{3}=3$; term $T_{4}$ has $K_{4}=10$ with $x_{4}=5$; term $T_{5}$ has $K_{5}=4$ with $x_{5}=2$; term $T_{6}$ has $K_{6}=9$ with $x_{6}=1$.\n\nStarting from the combinatorial model of sampling without replacement for annotation enrichment and the principle that the False Discovery Rate (FDR) is controlled by the Benjamini–Hochberg (BH) procedure (Benjamini and Hochberg), derive the one-sided enrichment $p$-values for each GO term as the probability of observing at least the reported overlap under the null hypothesis that the $M$ genes are drawn uniformly at random from the $N$-gene universe. Then, apply the BH procedure across the $m=6$ tests to obtain $q$-values, and determine which terms are significant at $q \\le 0.05$.\n\nReport as your final answer the total number of significant GO terms at $q \\le 0.05$. Express your final answer as a single real number. No rounding instruction is necessary for this count.",
            "solution": "The appropriate null model for over-representation testing of Gene Ontology (GO) terms is sampling without replacement from a finite universe. Under the null, drawing $M$ genes from a universe of $N$ genes with $K$ of them annotated to a given term is described by the hypergeometric distribution. The number of annotated genes $X$ observed among the $M$ draws has probability mass function\n$$\n\\Pr(X=t) \\;=\\; \\frac{\\binom{K}{t}\\,\\binom{N-K}{M-t}}{\\binom{N}{M}},\n$$\nfor integer $t$ with $0 \\le t \\le \\min\\{M,K\\}$ and $M-t \\le N-K$. A one-sided enrichment test considers the right tail,\n$$\np\\text{-value} \\;=\\; \\Pr(X \\ge x) \\;=\\; \\sum_{t=x}^{\\min\\{M,K\\}} \\frac{\\binom{K}{t}\\,\\binom{N-K}{M-t}}{\\binom{N}{M}},\n$$\nwhere $x$ is the observed overlap.\n\nWe are given $N=20$ and $M=6$, so the denominator for all terms is\n$$\n\\binom{N}{M} \\;=\\; \\binom{20}{6} \\;=\\; 38{,}760.\n$$\nWe now compute each term’s tail probability.\n\nTerm $T_{1}$ with $K_{1}=8$ and $x_{1}=4$ requires $t=4,5,6$:\n\\begin{align*}\n\\Pr(X \\ge 4) &= \\frac{\\binom{8}{4}\\binom{12}{2} + \\binom{8}{5}\\binom{12}{1} + \\binom{8}{6}\\binom{12}{0}}{\\binom{20}{6}} \\\\\n&= \\frac{70 \\cdot 66 + 56 \\cdot 12 + 28 \\cdot 1}{38{,}760} \\\\\n&= \\frac{4{,}620 + 672 + 28}{38{,}760} \\\\\n&= \\frac{5{,}320}{38{,}760} \\;=\\; \\frac{133}{969} \\approx 0.1372.\n\\end{align*}\n\nTerm $T_{2}$ with $K_{2}=7$ and $x_{2}=4$ uses $t=4,5,6$:\n\\begin{align*}\n\\Pr(X \\ge 4) &= \\frac{\\binom{7}{4}\\binom{13}{2} + \\binom{7}{5}\\binom{13}{1} + \\binom{7}{6}\\binom{13}{0}}{\\binom{20}{6}} \\\\\n&= \\frac{35 \\cdot 78 + 21 \\cdot 13 + 7 \\cdot 1}{38{,}760} \\\\\n&= \\frac{2{,}730 + 273 + 7}{38{,}760} \\\\\n&= \\frac{3{,}010}{38{,}760} \\;=\\; \\frac{301}{3{,}876} \\approx 0.0776.\n\\end{align*}\n\nTerm $T_{3}$ with $K_{3}=5$ and $x_{3}=3$ uses $t=3,4,5$:\n\\begin{align*}\n\\Pr(X \\ge 3) &= \\frac{\\binom{5}{3}\\binom{15}{3} + \\binom{5}{4}\\binom{15}{2} + \\binom{5}{5}\\binom{15}{1}}{\\binom{20}{6}} \\\\\n&= \\frac{10 \\cdot 455 + 5 \\cdot 105 + 1 \\cdot 15}{38{,}760} \\\\\n&= \\frac{4{,}550 + 525 + 15}{38{,}760} \\\\\n&= \\frac{5{,}090}{38{,}760} \\;=\\; \\frac{509}{3{,}876} \\approx 0.1313.\n\\end{align*}\n\nTerm $T_{4}$ with $K_{4}=10$ and $x_{4}=5$ uses $t=5,6$:\n\\begin{align*}\n\\Pr(X \\ge 5) &= \\frac{\\binom{10}{5}\\binom{10}{1} + \\binom{10}{6}\\binom{10}{0}}{\\binom{20}{6}} \\\\\n&= \\frac{252 \\cdot 10 + 210 \\cdot 1}{38{,}760} \\\\\n&= \\frac{2{,}520 + 210}{38{,}760} \\\\\n&= \\frac{2{,}730}{38{,}760} \\;=\\; \\frac{91}{1{,}292} \\approx 0.0704.\n\\end{align*}\n\nTerm $T_{5}$ with $K_{5}=4$ and $x_{5}=2$ uses $t=2,3,4$:\n\\begin{align*}\n\\Pr(X \\ge 2) &= \\frac{\\binom{4}{2}\\binom{16}{4} + \\binom{4}{3}\\binom{16}{3} + \\binom{4}{4}\\binom{16}{2}}{\\binom{20}{6}} \\\\\n&= \\frac{6 \\cdot 1{,}820 + 4 \\cdot 560 + 1 \\cdot 120}{38{,}760} \\\\\n&= \\frac{10{,}920 + 2{,}240 + 120}{38{,}760} \\\\\n&= \\frac{13{,}280}{38{,}760} \\;=\\; \\frac{332}{969} \\approx 0.3426.\n\\end{align*}\n\nTerm $T_{6}$ with $K_{6}=9$ and $x_{6}=1$ has a simpler complement form $\\Pr(X \\ge 1) = 1 - \\Pr(X=0)$:\n\\begin{align*}\n\\Pr(X \\ge 1) &= 1 - \\frac{\\binom{9}{0}\\binom{11}{6}}{\\binom{20}{6}} \\\\\n&= 1 - \\frac{462}{38{,}760} \\\\\n&= 1 - \\frac{77}{6{,}460} \\approx 1 - 0.0119 \\approx 0.9881.\n\\end{align*}\n\nCollect the six $p$-values (approximations shown for ordering): \n$$\np_{1} \\approx 0.1372,\\quad p_{2} \\approx 0.0776,\\quad p_{3} \\approx 0.1313,\\quad p_{4} \\approx 0.0704,\\quad p_{5} \\approx 0.3426,\\quad p_{6} \\approx 0.9881.\n$$\n\nApply the Benjamini–Hochberg (BH) procedure across $m=6$ hypotheses. First sort the $p$-values in ascending order, indexing them as $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(6)}$:\n$$\np_{(1)} = p_{4} \\approx 0.0704,\\quad\np_{(2)} = p_{2} \\approx 0.0776,\\quad\np_{(3)} = p_{3} \\approx 0.1313,\\quad\np_{(4)} = p_{1} \\approx 0.1372,\\quad\np_{(5)} = p_{5} \\approx 0.3426,\\quad\np_{(6)} = p_{6} \\approx 0.9881.\n$$\nDefine the initial BH transforms $r_{i} = \\frac{m}{i} p_{(i)}$ for $i=1,\\dots,6$:\n\\begin{align*}\nr_{1} &\\approx \\frac{6}{1} \\cdot 0.0704 \\approx 0.4224, \\\\\nr_{2} &\\approx \\frac{6}{2} \\cdot 0.0776 \\approx 0.2328, \\\\\nr_{3} &\\approx \\frac{6}{3} \\cdot 0.1313 \\approx 0.2626, \\\\\nr_{4} &\\approx \\frac{6}{4} \\cdot 0.1372 \\approx 0.2058, \\\\\nr_{5} &\\approx \\frac{6}{5} \\cdot 0.3426 \\approx 0.4111, \\\\\nr_{6} &\\approx \\frac{6}{6} \\cdot 0.9881 \\approx 0.9881.\n\\end{align*}\nNext, enforce monotonicity of the adjusted values by setting\n$$\nq_{(6)} = \\min\\{r_{6}, 1\\},\\quad q_{(i)} = \\min\\{r_{i}, q_{(i+1)}\\}\\ \\text{for}\\ i=5,4,3,2,1.\n$$\nThis yields\n\\begin{align*}\nq_{(6)} &\\approx 0.9881, \\\\\nq_{(5)} &\\approx \\min\\{0.4111,\\,0.9881\\} = 0.4111, \\\\\nq_{(4)} &\\approx \\min\\{0.2058,\\,0.4111\\} = 0.2058, \\\\\nq_{(3)} &\\approx \\min\\{0.2626,\\,0.2058\\} = 0.2058, \\\\\nq_{(2)} &\\approx \\min\\{0.2328,\\,0.2058\\} = 0.2058, \\\\\nq_{(1)} &\\approx \\min\\{0.4224,\\,0.2058\\} = 0.2058.\n\\end{align*}\nThus, all BH $q$-values are greater than $0.05$. Therefore, the number of significant GO terms at $q \\le 0.05$ is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "A successful enrichment analysis often yields a long, redundant list of statistically significant GO terms, making direct interpretation difficult. This advanced practice addresses this challenge by showing you how to reduce redundancy through semantic similarity clustering. You will implement an algorithm that uses information content and the ontology structure to group related terms, ultimately distilling a complex list into a concise set of representative biological themes for easier interpretation .",
            "id": "4543494",
            "problem": "You are given a simplified Gene Ontology (GO) fragment represented as a Directed Acyclic Graph (DAG), a corpus of direct annotation counts per GO term, and sets of enriched terms. The aim is to construct a principled algorithm to remove redundancy among enriched terms by clustering them according to semantic similarity and selecting a single representative per cluster. The approach must be grounded in core definitions from ontology-based annotation and information theory, and the outcome must be fully specified, deterministic, and testable.\n\nFundamental base definitions to be used:\n- The Gene Ontology (GO) is a Directed Acyclic Graph (DAG) represented as $G=(V,E)$ with vertices $V$ being term identifiers and edges $E$ expressing parent relations. A directed edge $(u,v)\\in E$ indicates that $u$ is a child of $v$ (that is, $v$ is a parent of $u$).\n- For any term $t\\in V$, let $\\mathcal{A}(t)$ denote the set of ancestors including $t$ itself, and let $\\mathcal{D}(t)$ denote the set of descendants excluding $t$.\n- Let $c(t)$ denote the number of direct annotations for term $t$, and $c^\\downarrow(t)=c(t)+\\sum_{x\\in \\mathcal{D}(t)} c(x)$ the descendant-aggregated count for $t$. Let $N=\\sum_{v\\in V} c(v)$ be the total number of direct annotations in the corpus. Define the probability $p(t)=\\frac{c^\\downarrow(t)}{N}$ and the Information Content (IC) $IC(t)=-\\ln p(t)$.\n- The Most Informative Common Ancestor (MICA) of terms $a$ and $b$ is $\\operatorname{MICA}(a,b)=\\arg\\max_{x\\in \\mathcal{A}(a)\\cap \\mathcal{A}(b)} IC(x)$, breaking ties by the smallest term identifier.\n- Define the Jiang–Conrath (JC) semantic distance between $a$ and $b$ by $d_{\\mathrm{JC}}(a,b)=IC(a)+IC(b)-2\\,IC\\!\\left(\\operatorname{MICA}(a,b)\\right)$.\n- For clustering, given two clusters $A$ and $B$ of terms, define complete linkage by $L_{\\mathrm{complete}}(A,B)=\\max_{a\\in A,\\,b\\in B} d_{\\mathrm{JC}}(a,b)$ and average linkage by $L_{\\mathrm{average}}(A,B)=\\frac{1}{|A|\\,|B|}\\sum_{a\\in A,\\,b\\in B} d_{\\mathrm{JC}}(a,b)$.\n- Given a set of enriched terms $E\\subseteq V$, define the multiset of pairwise distances $D=\\{d_{\\mathrm{JC}}(a,b)\\,|\\,a,b\\in E,\\,a<b\\}$. Compute the median $\\tilde{m}=\\operatorname{median}(D)$ and the Median Absolute Deviation $\\operatorname{MAD}=\\operatorname{median}(\\{|d-\\tilde{m}|\\,|\\,d\\in D\\})$. The agglomeration cutoff is $c=\\max\\!\\left(0,\\tilde{m}-k\\cdot \\operatorname{MAD}\\right)$ for a specified real parameter $k$. At each step, merge the pair of clusters with the smallest linkage distance not exceeding $c$; break ties by the lexicographically smallest pair of cluster labels (defined by the smallest term identifiers in each cluster). Continue until no inter-cluster linkage distance is $\\le c$.\n- After clustering, select one representative term per cluster: $r(A)=\\arg\\max_{t\\in A} IC(t)$; break ties by the smallest term identifier.\n\nGO fragment (vertex set and parent edges) and direct annotation counts:\n- Vertex set $V=\\{0,1,2,3,4,5,6,7,8,9,10,11\\}$ where $0$ is the root.\n- Parent edges $E=\\{(1,0),(2,0),(3,1),(4,1),(5,2),(6,2),(7,3),(7,5),(8,4),(9,6),(10,7),(11,8),(11,9)\\}$.\n- Direct counts $c(0)=0$, $c(1)=400$, $c(2)=350$, $c(3)=200$, $c(4)=180$, $c(5)=150$, $c(6)=170$, $c(7)=80$, $c(8)=60$, $c(9)=70$, $c(10)=30$, $c(11)=25$. Thus $N=1715$.\n\nImplement the following steps deterministically:\n1. Construct $\\mathcal{A}(t)$ and $\\mathcal{D}(t)$ for all $t\\in V$.\n2. Compute $c^\\downarrow(t)$, then $p(t)$ and $IC(t)$ for all $t\\in V$.\n3. For a given enriched set $E$, compute all pairwise $d_{\\mathrm{JC}}(a,b)$ for $a,b\\in E$, $a<b$.\n4. Compute $\\tilde{m}$ and $\\operatorname{MAD}$, then $c=\\max(0,\\tilde{m}-k\\cdot \\operatorname{MAD})$ for the specified $k$.\n5. Perform agglomerative clustering over $E$ using the specified linkage criterion $L\\in\\{L_{\\mathrm{complete}},L_{\\mathrm{average}}\\}$ and cutoff $c$.\n6. For each cluster, choose the representative $r(A)$ using IC maximization; in case of ties, select the smallest identifier.\n7. Return the sorted list (ascending) of representatives for the enriched set.\n\nTest suite to exercise the algorithm across different facets, including typical behavior, robustness of the cutoff, and the effect of linkage choice:\n- Case $1$: $E=\\{3,4,7,8,10\\}$, linkage $L_{\\mathrm{complete}}$, parameter $k=1.0$.\n- Case $2$: $E=\\{5,6,7,9,11\\}$, linkage $L_{\\mathrm{average}}$, parameter $k=1.5$.\n- Case $3$: $E=\\{1,2,10,11\\}$, linkage $L_{\\mathrm{complete}}$, parameter $k=-0.5$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result in that list must itself be a list of integers corresponding to the sorted representatives for the respective test case. For example, the output must look like $[[a_1,a_2],[b_1,b_2,b_3],[c_1]]$ with actual integer identifiers in place of $a_i$, $b_i$, $c_i$.",
            "solution": "The problem statement is assessed as valid. It presents a well-defined, scientifically grounded task in bioinformatics, specifically the analysis of Gene Ontology (GO) enrichment results. All necessary data, definitions, and deterministic rules for computation are provided, making the problem self-contained and verifiable. The definitions for Information Content ($IC$), Most Informative Common Ancestor ($\\operatorname{MICA}$), Jiang-Conrath distance ($d_{\\mathrm{JC}}$), and agglomerative clustering are standard in the field. The inclusion of explicit tie-breaking rules at every stage ensures that a unique solution can be derived.\n\nThe solution is implemented by following the sequence of steps specified in the problem statement. The overall algorithm can be divided into a pre-computation phase and a case-processing phase.\n\n**1. Pre-computation: Graph and Information Content Initialization**\n\nFirst, the GO fragment's topology and annotation data are processed to compute foundational metrics for all terms.\n\n*   **Graph Representation**: The DAG $G=(V, E)$ is represented using data structures that allow for efficient traversal. Specifically, for each term $t \\in V$, we maintain a list of its parents and a list of its children based on the given edge set $E$.\n*   **Ancestor and Descendant Sets**: For each term $t$, the set of all its ancestors, $\\mathcal{A}(t)$, is computed by traversing upwards from $t$ to the root ($0$), collecting all visited nodes including $t$. The set of its descendants, $\\mathcal{D}(t)$, is computed by a downward traversal from $t$, collecting all reachable nodes excluding $t$.\n*   **Descendant-Aggregated Counts ($c^\\downarrow(t)$)**: The direct annotation counts $c(t)$ are given. The descendant-aggregated count $c^\\downarrow(t)$ for a term $t$ is the sum of its own direct count and the direct counts of all its descendants: $c^\\downarrow(t) = c(t) + \\sum_{x \\in \\mathcal{D}(t)} c(x)$. These are computed for all $t \\in V$.\n*   **Information Content ($IC(t)$)**: The total number of direct annotations is $N = \\sum_{v \\in V} c(v) = 1715$. The probability of a term $t$ is $p(t) = \\frac{c^\\downarrow(t)}{N}$. The Information Content ($IC$) is then calculated as $IC(t) = -\\ln p(t)$. A higher $IC$ value indicates a more specific, less frequent term. The pre-computed $IC$ values form the basis for all subsequent semantic similarity calculations.\n\nThe pre-computed $IC$ values for the terms are approximately:\n$IC(0)=0.0$, $IC(1) \\approx 0.5647$, $IC(2) \\approx 0.6730$, $IC(3) \\approx 1.7093$, $IC(4) \\approx 1.8680$, $IC(5) \\approx 1.8863$, $IC(6) \\approx 1.8680$, $IC(7) \\approx 2.7483$, $IC(8) \\approx 3.0039$, $IC(9) \\approx 2.8948$, $IC(10) \\approx 3.7594$, $IC(11) \\approx 4.2259$.\n\n**2. Case Processing**\n\nFor each test case, defined by an enriched set $E$, a linkage method $L$, and a parameter $k$, the following sequence is executed.\n\n*   **Pairwise Semantic Distance Calculation**: The Jiang-Conrath distance, $d_{\\mathrm{JC}}(a, b) = IC(a) + IC(b) - 2 \\cdot IC(\\operatorname{MICA}(a, b))$, is calculated for every pair of distinct terms $(a, b)$ in the enriched set $E$ where $a<b$.\n    *   This requires finding the Most Informative Common Ancestor ($\\operatorname{MICA}$) for each pair. The $\\operatorname{MICA}$ is the common ancestor with the highest $IC$ value. Ties are broken by selecting the ancestor with the smallest term identifier.\n    *   The resulting distances form a multiset $D = \\{d_{\\mathrm{JC}}(a,b) \\,|\\, a,b \\in E, a<b\\}$.\n\n*   **Clustering Cutoff Determination**: A dynamic distance threshold for clustering, $c$, is computed based on the statistical properties of the pairwise distances in $D$.\n    *   The median of the distances, $\\tilde{m} = \\operatorname{median}(D)$, is calculated.\n    *   The Median Absolute Deviation, $\\operatorname{MAD} = \\operatorname{median}(\\{|d - \\tilde{m}| \\,|\\, d \\in D\\})$, is calculated as a robust measure of the variability of the distances.\n    *   The cutoff is then defined as $c = \\max(0, \\tilde{m} - k \\cdot \\operatorname{MAD})$. The parameter $k$ controls the stringency of this cutoff. A larger $k$ leads to a smaller cutoff, resulting in less clustering.\n\n*   **Agglomerative Hierarchical Clustering**: The enriched terms are clustered using a bottom-up agglomerative approach.\n    *   **Initialization**: Each term $t \\in E$ starts in its own cluster, $\\{t\\}$.\n    *   **Iteration**: The algorithm repeatedly merges the two \"closest\" clusters until no two clusters can be merged.\n        1.  In each step, the inter-cluster distance is calculated for all pairs of current clusters using the specified linkage method ($L_{\\mathrm{complete}}$ or $L_{\\mathrm{average}}$).\n        2.  The pair of clusters $(A, B)$ with the minimum linkage distance, $d_{\\min}$, is identified. Ties are resolved by selecting the pair whose labels (the smallest term identifiers in each cluster) form a lexicographically smaller pair.\n        3.  If $d_{\\min} \\le c$, clusters $A$ and $B$ are merged into a new cluster $A \\cup B$.\n        4.  If $d_{\\min} > c$, no more clusters can be merged, and the clustering process terminates.\n\n*   **Representative Selection**: After the final clusters are formed, a single representative term is selected from each cluster.\n    *   For each cluster $A$, the representative $r(A)$ is the term within that cluster having the maximum Information Content: $r(A) = \\arg\\max_{t \\in A} IC(t)$.\n    *   Ties are broken by choosing the term with the smallest identifier.\n\n*   **Final Output**: The list of representative term identifiers for the test case is sorted in ascending order and stored. This process is repeated for all test cases, and the final results are formatted into a single string as specified.\n\nThe application of this deterministic procedure to the three test cases yields distinct sets of representatives, demonstrating how the choice of linkage, the set of enriched terms, and the parameter $k$ collectively influence the outcome of redundancy removal.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the Gene Ontology term clustering problem based on the provided\n    definitions and test cases.\n    \"\"\"\n    # 1. Extract Givens and Pre-computation\n    V = set(range(12))\n    # Parent edges (u, v) -> v is parent of u\n    edges = [(1, 0), (2, 0), (3, 1), (4, 1), (5, 2), (6, 2), (7, 3), \n             (7, 5), (8, 4), (9, 6), (10, 7), (11, 8), (11, 9)]\n    \n    # Direct counts c(t)\n    c = np.array([0, 400, 350, 200, 180, 150, 170, 80, 60, 70, 30, 25])\n    N = np.sum(c)\n\n    # Build graph structures: parents and children adjacency lists\n    parents = {i: [] for i in V}\n    children = {i: [] for i in V}\n    for u, v in edges:\n        parents[u].append(v)\n        children[v].append(u)\n\n    # Compute ancestor sets A(t) including t\n    ancestors = {i: set() for i in V}\n    for i in V:\n        q = [i]\n        visited = {i}\n        head = 0\n        while head < len(q):\n            curr = q[head]\n            head += 1\n            ancestors[i].add(curr)\n            for p in parents[curr]:\n                if p not in visited:\n                    visited.add(p)\n                    q.append(p)\n\n    # Compute descendant sets D(t) excluding t\n    descendants = {i: set() for i in V}\n    for i in V:\n        q = list(children[i])\n        visited = set(children[i])\n        head = 0\n        while head < len(q):\n            curr = q[head]\n            head += 1\n            descendants[i].add(curr)\n            for child in children[curr]:\n                if child not in visited:\n                    visited.add(child)\n                    q.append(child)\n\n    # Compute descendant-aggregated counts c_down(t)\n    c_down = np.zeros(len(V))\n    for i in V:\n        desc_sum = np.sum(c[list(descendants[i])]) if descendants[i] else 0\n        c_down[i] = c[i] + desc_sum\n\n    # Compute Information Content IC(t)\n    # Handle c_down[i] == 0 case to avoid log(0)\n    ic = np.array([-math.log(count / N) if count > 0 else float('inf') for count in c_down])\n\n    # Helper functions for case processing\n    def get_mica(a, b):\n        common_ancestors = list(ancestors[a].intersection(ancestors[b]))\n        if not common_ancestors:\n            return None # Should not happen in a connected GO fragment with a root\n        \n        max_ic = -1.0\n        mica_term = -1\n        \n        # Sort to ensure tie-breaking is deterministic (smallest term ID)\n        common_ancestors.sort()\n\n        for term in common_ancestors:\n            if ic[term] > max_ic:\n                max_ic = ic[term]\n                mica_term = term\n        \n        return mica_term\n\n    def get_jc_distance(a, b):\n        mica = get_mica(a,b)\n        if mica is None:\n            return float('inf')\n        return ic[a] + ic[b] - 2 * ic[mica]\n\n    # Test cases from the problem statement\n    test_cases = [\n        ({'E': {3, 4, 7, 8, 10}, 'linkage': 'complete', 'k': 1.0}),\n        ({'E': {5, 6, 7, 9, 11}, 'linkage': 'average', 'k': 1.5}),\n        ({'E': {1, 2, 10, 11}, 'linkage': 'complete', 'k': -0.5})\n    ]\n\n    final_results = []\n    \n    for case in test_cases:\n        E = sorted(list(case['E']))\n        linkage_type = case['linkage']\n        k = case['k']\n        \n        # 3. Compute pairwise JC distances\n        distances = []\n        term_pairs = []\n        for i in range(len(E)):\n            for j in range(i + 1, len(E)):\n                term_pairs.append((E[i], E[j]))\n                distances.append(get_jc_distance(E[i], E[j]))\n        \n        # 4. Compute clustering cutoff c\n        if not distances:\n            c = 0\n        else:\n            m_tilde = np.median(distances)\n            abs_devs = [abs(d - m_tilde) for d in distances]\n            mad = np.median(abs_devs)\n            c = max(0, m_tilde - k * mad)\n\n        # 5. Perform agglomerative clustering\n        clusters = [frozenset([term]) for term in E]\n        \n        # Pre-calculate all elementary distances\n        base_distances = {}\n        for i in range(len(E)):\n            for j in range(i + 1, len(E)):\n                dist = get_jc_distance(E[i], E[j])\n                base_distances[frozenset([E[i], E[j]])] = dist\n        \n        while True:\n            min_dist = float('inf')\n            best_pair_to_merge = None\n            \n            if len(clusters) <= 1:\n                break\n            \n            # Find the closest pair of clusters\n            cluster_pairs = []\n            for i in range(len(clusters)):\n                for j in range(i + 1, len(clusters)):\n                    cluster_pairs.append((clusters[i], clusters[j]))\n            \n            # Sort pairs for deterministic tie-breaking\n            cluster_pairs.sort(key=lambda p: (min(p[0]), min(p[1])))\n\n            for c1, c2 in cluster_pairs:\n                dist = 0.0\n                if linkage_type == 'complete':\n                    dist = -1.0\n                    for t1 in c1:\n                        for t2 in c2:\n                            pair = frozenset([t1, t2])\n                            dist = max(dist, base_distances[pair])\n                elif linkage_type == 'average':\n                    total_dist = 0.0\n                    for t1 in c1:\n                        for t2 in c2:\n                            pair = frozenset([t1, t2])\n                            total_dist += base_distances[pair]\n                    dist = total_dist / (len(c1) * len(c2))\n                \n                if dist < min_dist:\n                    min_dist = dist\n                    best_pair_to_merge = (c1, c2)\n\n            if min_dist <= c:\n                c1, c2 = best_pair_to_merge\n                clusters.remove(c1)\n                clusters.remove(c2)\n                clusters.append(c1.union(c2))\n            else:\n                break # No more merges possible below cutoff\n\n        # 6. Select representatives\n        representatives = []\n        for cluster in clusters:\n            best_rep = -1\n            max_ic = -1\n            # Sort cluster elements for deterministic tie-breaking\n            sorted_cluster = sorted(list(cluster))\n            for term in sorted_cluster:\n                if ic[term] > max_ic:\n                    max_ic = ic[term]\n                    best_rep = term\n            representatives.append(best_rep)\n\n        # 7. Sort and store result\n        representatives.sort()\n        final_results.append(representatives)\n\n    # Format the final output string\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}