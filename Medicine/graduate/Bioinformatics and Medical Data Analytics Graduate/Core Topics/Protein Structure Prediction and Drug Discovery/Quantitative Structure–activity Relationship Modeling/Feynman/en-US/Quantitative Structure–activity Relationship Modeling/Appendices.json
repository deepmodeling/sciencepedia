{
    "hands_on_practices": [
        {
            "introduction": "Before a structure-activity relationship can be modeled, the 'structure' must be quantified. This practice guides you through a foundational exercise in chemoinformatics: deriving a similarity metric from first principles. By starting with a set of logical axioms, you will construct the Tanimoto coefficient, the most widely used similarity measure for binary molecular fingerprints, and understand precisely why it is defined as the ratio of the intersection to the union of feature sets .",
            "id": "4602706",
            "problem": "In quantitative structure–activity relationship (QSAR) modeling, small molecules are often represented by binary fingerprints that indicate the presence ($1$) or absence ($0$) of predefined substructural features. Each molecule can be identified with the set of indices of its present features. Consider two molecules with binary fingerprints encoded over a shared feature universe, and let $A$ and $B$ denote the sets of feature indices present in the first and second molecule, respectively. Define $a := |A|$, $b := |B|$, and $c := |A \\cap B|$.\n\nStarting from the following fundamental base in set theory and measurement:\n- Finite sets admit cardinalities, and $|A \\cup B| = |A| + |B| - |A \\cap B|$.\n- A similarity between two objects is a real-valued functional $S(A,B)$ satisfying, at minimum, normalization $S(A,A) = 1$, null-similarity on disjoint presence $S(A,B) = 0$ whenever $A \\cap B = \\varnothing$ with $A \\cup B \\neq \\varnothing$, symmetry $S(A,B) = S(B,A)$, and invariance under embedding into a larger feature universe by appending features that are absent in both objects (that is, adding a common set of zeros to both fingerprints must not change the similarity).\n\nUsing only these foundational facts and invariances, and without assuming any particular pre-existing formula for $S(A,B)$, derive a closed-form expression $S(a,b,c)$ depending only on $a$, $b$, and $c$ that satisfies:\n- Dependence only on $a$, $b$, $c$ (no dependence on the total fingerprint length or counts of jointly absent features).\n- The above normalization, symmetry, and embedding invariance.\n- Monotonicity in $c$ for fixed $a$ and $b$.\n- Consistency under disjoint concatenation of independent feature blocks in the sense that if the feature universe is partitioned into two disjoint blocks and $A,B$ decompose accordingly, then the overall similarity is a union-size-weighted average of blockwise similarities.\n\nProve that your expression satisfies these properties, and justify why the denominator must count only features present in at least one of the two molecules.\n\nFinally, for two fingerprints with $a = 287$, $b = 355$, and $c = 132$, compute the resulting similarity as a real number and round your final numerical answer to five significant figures. No units are required.",
            "solution": "The problem requires the derivation of a similarity measure $S(A,B)$ between two sets, $A$ and $B$, based on a set of axioms. The similarity must be a function of the cardinalities $a = |A|$, $b = |B|$, and $c = |A \\cap B|$.\n\nFirst, we validate the problem statement.\n### Step 1: Extract Givens\n-   $A, B$ are sets of feature indices present in two molecules.\n-   $a := |A|$, $b := |B|$, $c := |A \\cap B|$.\n-   Axiom $1$ (Principle of Inclusion-Exclusion): $|A \\cup B| = |A| + |B| - |A \\cap B| = a+b-c$.\n-   Axiom $2$ (Normalization): $S(A,A) = 1$.\n-   Axiom $3$ (Null-similarity): $S(A,B) = 0$ if $A \\cap B = \\varnothing$ and $A \\cup B \\neq \\varnothing$.\n-   Axiom $4$ (Symmetry): $S(A,B) = S(B,A)$.\n-   Axiom $5$ (Invariance under embedding): $S(A,B)$ is invariant to the size of the total feature universe, implying dependence only on features present in at least one object. This is reinforced by the requirement that $S$ is a function $S(a,b,c)$.\n-   Axiom $6$ (Monotonicity): $S(a,b,c)$ is non-decreasing in $c$ for fixed $a$ and $b$.\n-   Axiom $7$ (Consistency under disjoint concatenation): If the feature universe is a disjoint union of two blocks ($1$ and $2$), such that $A = A_1 \\cup A_2$ and $B = B_1 \\cup B_2$, then the overall similarity is a union-size-weighted average of the blockwise similarities:\n    $$S(A,B) = \\frac{|A_1 \\cup B_1| S(A_1, B_1) + |A_2 \\cup B_2| S(A_2, B_2)}{|A \\cup B|}$$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a foundational derivation of the Tanimoto coefficient (or Jaccard index) which is a standard tool in chemoinformatics and QSAR. The problem is well-posed, providing a set of consistent and sufficient axioms to derive a unique functional form. It is objective and uses precise mathematical language. The problem is free of any specified flaws.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the derivation.\n\nLet the similarity function be denoted by $f(a,b,c)$. The cardinalities for the disjoint concatenation are:\n$a = |A| = |A_1| + |A_2| = a_1 + a_2$\n$b = |B| = |B_1| + |B_2| = b_1 + b_2$\n$c = |A \\cap B| = |A_1 \\cap B_1| + |A_2 \\cap B_2| = c_1 + c_2$\n$|A \\cup B| = a+b-c = (a_1+a_2) + (b_1+b_2) - (c_1+c_2)$\n$|A_i \\cup B_i| = a_i+b_i-c_i$ for $i \\in \\{1, 2\\}$.\n\nSubstituting these into the disjoint concatenation axiom (Axiom $7$):\n$$f(a_1+a_2, b_1+b_2, c_1+c_2) = \\frac{(a_1+b_1-c_1)f(a_1,b_1,c_1) + (a_2+b_2-c_2)f(a_2,b_2,c_2)}{(a_1+b_1-c_1)+(a_2+b_2-c_2)}$$\nLet us define an auxiliary function $g(a,b,c) = (a+b-c)f(a,b,c)$. Substituting $f(a,b,c) = \\frac{g(a,b,c)}{a+b-c}$ into the equation above:\n$$\\frac{g(a_1+a_2, b_1+b_2, c_1+c_2)}{(a_1+a_2)+(b_1+b_2)-(c_1+c_2)} = \\frac{(a_1+b_1-c_1)\\frac{g(a_1,b_1,c_1)}{a_1+b_1-c_1} + (a_2+b_2-c_2)\\frac{g(a_2,b_2,c_2)}{a_2+b_2-c_2}}{(a_1+a_2)+(b_1+b_2)-(c_1+c_2)}$$\nThis simplifies to:\n$$g(a_1+a_2, b_1+b_2, c_1+c_2) = g(a_1,b_1,c_1) + g(a_2,b_2,c_2)$$\nThis is a multidimensional Cauchy functional equation. Since the arguments $a,b,c$ are discrete counts (cardinalities), the solution must be a linear function:\n$$g(a,b,c) = k_1 a + k_2 b + k_3 c$$\nwhere $k_1, k_2, k_3$ are constants. Therefore, the similarity function must have the form:\n$$S(a,b,c) = f(a,b,c) = \\frac{k_1 a + k_2 b + k_3 c}{a+b-c}$$\nWe now use the other axioms to determine the constants $k_1, k_2, k_3$.\n\n1.  **Symmetry (Axiom 4)**: $f(a,b,c) = f(b,a,c)$.\n    $$\\frac{k_1 a + k_2 b + k_3 c}{a+b-c} = \\frac{k_1 b + k_2 a + k_3 c}{b+a-c}$$\n    This implies $k_1 a + k_2 b = k_1 b + k_2 a$, which can be rewritten as $(k_1-k_2)a = (k_1-k_2)b$. For this to hold for arbitrary $a$ and $b$, we must have $k_1 = k_2$.\n    The function simplifies to $f(a,b,c) = \\frac{k_1(a+b) + k_3 c}{a+b-c}$.\n\n2.  **Null-similarity (Axiom 3)**: $f(a,b,0) = 0$ for $a+b > 0$.\n    When $c=0$, we have $A \\cap B = \\varnothing$.\n    $$f(a,b,0) = \\frac{k_1(a+b) + k_3(0)}{a+b-0} = \\frac{k_1(a+b)}{a+b} = k_1$$\n    For this to be $0$, we must have $k_1=0$. Since $k_1=k_2$, it follows that $k_2=0$.\n    The function further simplifies to $f(a,b,c) = \\frac{k_3 c}{a+b-c}$.\n\n3.  **Normalization (Axiom 2)**: $f(a,a,a) = 1$ for $a > 0$.\n    When $B=A$, we have $a=b=c$.\n    $$f(a,a,a) = \\frac{k_3 a}{a+a-a} = \\frac{k_3 a}{a} = k_3$$\n    For this to be $1$, we must have $k_3=1$.\n\nCombining these results, the unique functional form consistent with the axioms is:\n$$S(a,b,c) = \\frac{c}{a+b-c}$$\nIn terms of the sets, this is $S(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$, which is the Jaccard index or Tanimoto coefficient.\n\n**Proof of Properties:**\nWe verify that this expression satisfies all stated properties.\n-   **Dependence and Symmetry:** The expression depends only on $a,b,c$ and is symmetric in $a$ and $b$, as shown in the derivation.\n-   **Normalization:** $S(a,a,a) = \\frac{a}{a+a-a} = \\frac{a}{a} = 1$ for $a>0$. Correct.\n-   **Null-similarity:** $S(a,b,0) = \\frac{0}{a+b-0} = 0$ for $a+b > 0$. Correct.\n-   **Monotonicity (Axiom 6):** We examine the partial derivative with respect to $c$, holding $a$ and $b$ constant.\n    $$\\frac{\\partial S}{\\partial c} = \\frac{\\partial}{\\partial c} \\left( \\frac{c}{a+b-c} \\right) = \\frac{(1)(a+b-c) - (c)(-1)}{(a+b-c)^2} = \\frac{a+b}{(a+b-c)^2}$$\n    Since $a=|A| \\ge 0$ and $b=|B| \\ge 0$, their sum $a+b \\ge 0$. The denominator is a square and thus non-negative. Therefore, $\\frac{\\partial S}{\\partial c} \\ge 0$, and the function is monotonically non-decreasing in $c$.\n\n**Justification for the Denominator:**\nThe denominator is $a+b-c$, which is equal to $|A \\cup B|$. This is the total number of features present in at least one of the two molecules. The requirement that the denominator counts only these features stems directly from Axiom $5$ (Invariance under embedding). This axiom demands that the similarity score does not change if we embed the molecules into a larger feature universe by adding features that are absent in both fingerprints (i.e., appending common zeros). Let the total number of features be $N$. The number of features absent in both $A$ and $B$ is $N - |A \\cup B|$. If the similarity formula depended on $N$ (e.g., through this count of dually-absent features), then changing $N$ by adding new features would change the similarity score. To be invariant to such changes, the formula must depend only on quantities derivable from $A$ and $B$ alone, such as $|A|$, $|B|$, $|A \\cap B|$, and $|A \\cup B|$. The derived form $S(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$ respects this axiom perfectly, as both numerator and denominator are independent of features outside of $A \\cup B$. The denominator represents the size of the set of relevant features for the comparison, and the numerator represents the size of the subset of shared features within that relevant set.\n\n**Final Calculation:**\nGiven $a = 287$, $b = 355$, and $c = 132$.\n$$S = \\frac{c}{a+b-c} = \\frac{132}{287 + 355 - 132}$$\nThe denominator is $287 + 355 - 132 = 642 - 132 = 510$.\n$$S = \\frac{132}{510}$$\nNow, we compute the numerical value and round to five significant figures.\n$$S = 0.258823529...$$\nRounding to five significant figures gives $0.25882$.",
            "answer": "$$ \\boxed{0.25882} $$"
        },
        {
            "introduction": "Effective QSAR modeling requires not only a robust representation of chemical structure but also a statistically appropriate scale for biological activity. Raw bioactivity data, such as the half-maximal inhibitory concentration ($\\text{IC}_{50}$), often spans several orders of magnitude and exhibits non-constant variance, violating key assumptions of linear regression. This exercise  demonstrates the crucial transformation of $\\text{IC}_{50}$ values to the logarithmic $\\text{pIC}_{50}$ scale, challenging you to justify this common practice by explaining how it stabilizes variance and improves the numerical conditioning of the modeling problem.",
            "id": "4602674",
            "problem": "In quantitative structure–activity relationship (QSAR) modeling within bioinformatics and medical data analytics, inhibitory potency is frequently summarized by the half-maximal inhibitory concentration in molar units, denoted by $\\text{IC}_{50}$, and a logarithmic potency scale, denoted by $\\text{pIC}_{50}$. Consider an assay that reports an $\\text{IC}_{50}$ value for a small-molecule inhibitor as $35\\,\\text{nM}$. Starting from the following fundamental bases:\n- The half-maximal inhibitory concentration $\\text{IC}_{50}$ is the concentration (in $\\text{M}$) at which the biological response is reduced by $50\\%$ relative to a control.\n- The decadic logarithm compresses multiplicative scales of concentration into additive scales according to the rule $\\log_{10}(ab)=\\log_{10}(a)+\\log_{10}(b)$.\n- The $p$-scale used for molar concentrations in chemistry (for example, $\\text{pH}$) is constructed from the base-$10$ logarithm to encode orders of magnitude in a linearized form.\n\nPart A. Derive an explicit analytic expression for $\\text{pIC}_{50}$ in terms of $\\text{IC}_{50}$ expressed in $\\text{M}$ that is dimensionally consistent and uses only the stated bases.\n\nPart B. Using your derived expression, compute $\\text{pIC}_{50}$ for the given $\\text{IC}_{50}$ value of $35\\,\\text{nM}$. Round your final numeric answer to four significant figures. Express the answer as a pure number without units.\n\nPart C. In a regression context where the response variability in $\\text{IC}_{50}$ is well described by a multiplicative (log-normal) error model, provide a concise mathematical argument, starting from the log-normal definition and the properties of the decadic logarithm, explaining how transforming $\\text{IC}_{50}$ to $\\text{pIC}_{50}$ can improve numerical conditioning during model training by stabilizing the response scale and variance. Your explanation should refer to the effect on the distribution of errors and the scale of the response but should not rely on any specific algorithmic implementation details or software settings.\n\nOnly one final numerical value is required as the answer: the value from Part B, rounded as instructed.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains sufficient information for a complete solution.\n\nThis problem is divided into three parts. We will address each part systematically.\n\nPart A. Derive an explicit analytic expression for $\\text{pIC}_{50}$.\n\nThe problem requires the derivation of an expression for $\\text{pIC}_{50}$ from fundamental bases. The key premise is that the $p$-scale for molar concentrations is constructed analogously to other $p$-scales in chemistry, such as $\\text{pH}$. The definition of $\\text{pH}$ is based on the negative base-$10$ logarithm of the hydrogen ion activity. For dilute solutions, this is approximated by the molar concentration.\nA logarithm can only be taken of a dimensionless quantity. Therefore, any concentration must be normalized by a standard state concentration, which is by convention $c^\\circ = 1\\,\\text{M}$ (moles per liter).\nLet the concentration value of the inhibitor be denoted by its symbol $\\text{IC}_{50}$. The dimensionless quantity required for the logarithm is the ratio $\\frac{\\text{IC}_{50}}{c^\\circ}$. Following the construction of $\\text{pH}$, the $\\text{pIC}_{50}$ is defined as the negative decadic logarithm of this dimensionless quantity:\n$$ \\text{pIC}_{50} = -\\log_{10}\\left(\\frac{\\text{IC}_{50}}{c^\\circ}\\right) $$\nGiven that $c^\\circ = 1\\,\\text{M}$, the expression becomes:\n$$ \\text{pIC}_{50} = -\\log_{10}\\left(\\frac{\\text{IC}_{50}}{1\\,\\text{M}}\\right) $$\nThe problem asks for an expression in terms of $\\text{IC}_{50}$ expressed in units of $\\text{M}$. If we let the variable $\\text{IC}_{50}$ represent the numerical value of the concentration in units of moles per liter, the division by $1\\,\\text{M}$ is implicitly handled, and the expression simplifies to:\n$$ \\text{pIC}_{50} = -\\log_{10}(\\text{IC}_{50, \\text{in M}}) $$\nThis is the standard definition used in medicinal chemistry and pharmacology.\n\nPart B. Compute $\\text{pIC}_{50}$ for the given $\\text{IC}_{50}$ value.\n\nThe given value is $\\text{IC}_{50} = 35\\,\\text{nM}$. To use the derived formula, this value must first be converted to molar units ($\\text{M}$). The prefix 'nano' (n) corresponds to a factor of $10^{-9}$.\n$$ \\text{IC}_{50} = 35\\,\\text{nM} = 35 \\times 10^{-9}\\,\\text{M} $$\nNow, we substitute the numerical value of the concentration in $\\text{M}$ into the expression derived in Part A:\n$$ \\text{pIC}_{50} = -\\log_{10}(35 \\times 10^{-9}) $$\nUsing the property of logarithms that $\\log_{10}(ab) = \\log_{10}(a) + \\log_{10}(b)$:\n$$ \\text{pIC}_{50} = -(\\log_{10}(35) + \\log_{10}(10^{-9})) $$\nSince $\\log_{10}(10^{-9}) = -9$, the expression becomes:\n$$ \\text{pIC}_{50} = -(\\log_{10}(35) - 9) $$\n$$ \\text{pIC}_{50} = 9 - \\log_{10}(35) $$\nNow, we compute the numerical value.\n$$ \\log_{10}(35) \\approx 1.544068 $$\nSubstituting this into the expression for $\\text{pIC}_{50}$:\n$$ \\text{pIC}_{50} \\approx 9 - 1.544068 = 7.455932 $$\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $7.455932...$, which, when rounded to four significant figures, is $7.456$.\n\nPart C. Explain the statistical benefit of the $\\text{pIC}_{50}$ transformation.\n\nIn a regression context, the goal is often to model a relationship of the form $Y = f(\\mathbf{X})$, where $Y$ is the response variable and $\\mathbf{X}$ is a vector of predictor variables (molecular descriptors in QSAR). Many standard regression algorithms, such as ordinary least squares, perform optimally when the errors are additive, normally distributed, and have constant variance (homoscedasticity).\n\nThe problem states that the variability in $\\text{IC}_{50}$ is described by a multiplicative (log-normal) error model. Let $Y = \\text{IC}_{50}$ be the response variable. The model can be written as:\n$$ Y_i = f(\\mathbf{x}_i) \\cdot \\epsilon_i $$\nwhere $f(\\mathbf{x}_i)$ is the true predicted value for the $i$-th compound with descriptors $\\mathbf{x}_i$, and $\\epsilon_i$ is a multiplicative error term. If $Y_i$ is log-normally distributed, its logarithm is normally distributed. The error term $\\epsilon_i$ is assumed to be drawn from a log-normal distribution, typically with a geometric mean of $1$. An important feature of the log-normal distribution is that its variance is dependent on its mean, leading to heteroscedasticity (non-constant variance of errors). The distribution is also positively skewed.\n\nNow, consider the transformation to the $\\text{pIC}_{50}$ scale. Let the transformed response be $Z = \\text{pIC}_{50}$.\n$$ Z_i = \\text{pIC}_{50,i} = -\\log_{10}(Y_i) $$\nApplying this transformation to the multiplicative error model:\n$$ Z_i = -\\log_{10}(f(\\mathbf{x}_i) \\cdot \\epsilon_i) $$\nUsing the logarithm rule $\\log_{10}(ab) = \\log_{10}(a) + \\log_{10}(b)$:\n$$ Z_i = -(\\log_{10}(f(\\mathbf{x}_i)) + \\log_{10}(\\epsilon_i)) $$\n$$ Z_i = -\\log_{10}(f(\\mathbf{x}_i)) - \\log_{10}(\\epsilon_i) $$\nThis equation represents an additive error model:\n$$ Z_i = g(\\mathbf{x}_i) + \\delta_i $$\nwhere $g(\\mathbf{x}_i) = -\\log_{10}(f(\\mathbf{x}_i))$ is the transformed model function and $\\delta_i = -\\log_{10}(\\epsilon_i)$ is the additive error term.\n\nThe transformation yields two critical improvements for numerical conditioning and model fitting:\n1.  **Stabilization of Variance**: By definition, if $\\epsilon_i$ follows a log-normal distribution, then $\\log_{10}(\\epsilon_i)$ follows a normal distribution. Consequently, the new error term $\\delta_i = -\\log_{10}(\\epsilon_i)$ is also normally distributed. A key property of the normal distribution is that its variance is constant and independent of its mean. The transformation thus converts heteroscedastic multiplicative errors in the $\\text{IC}_{50}$ scale to homoscedastic (constant variance) additive errors in the $\\text{pIC}_{50}$ scale. This satisfies a fundamental assumption of many regression algorithms.\n2.  **Symmetrization of the Response and Error Distribution**: The $\\text{IC}_{50}$ values, being log-normally distributed, are skewed. The logarithmic transformation makes the distribution of the transformed response variable $Z$ and its errors $\\delta_i$ symmetric and normal. This improves the reliability of parameter estimates and the validity of statistical significance tests.\n\nIn summary, transforming $\\text{IC}_{50}$ to $\\text{pIC}_{50}$ converts a multiplicative, heteroscedastic, and skewed error structure into an additive, homoscedastic, and normal error structure. This change makes the data conform better to the underlying assumptions of many regression models, thereby improving the numerical stability and statistical validity of the resulting QSAR model.",
            "answer": "$$ \\boxed{7.456} $$"
        },
        {
            "introduction": "A predictive model is only as reliable as the data it is built upon, and in QSAR, it is critical to identify individual compounds that may be unduly influencing the results. This exercise introduces a cornerstone of regression diagnostics, Cook's distance, which measures the effect of removing a single observation on the entire model. By deriving the formula for Cook's distance from its fundamental components—leverage and residual error—you will gain a deep, practical understanding of how to detect and interpret influential data points in your QSAR models .",
            "id": "4602664",
            "problem": "A researcher is building a linear quantitative structure–activity relationship (QSAR) model, where the biological response $y$ (for example, $\\log_{10}(\\text{IC}_{50})$) is modeled as a linear combination of $p$ molecular descriptors (including an intercept) collected in the design matrix $X \\in \\mathbb{R}^{n \\times p}$ for $n$ compounds. The ordinary least squares (OLS) estimator is defined by $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$, the fitted responses are $\\hat{y} = X\\hat{\\beta}$, the residuals are $e = y - \\hat{y}$ with components $e_i$, and the hat matrix is $H = X(X^{\\top}X)^{-1}X^{\\top}$ with diagonal entries $h_{ii}$ called leverages. Let $\\hat{\\sigma}^{2}$ denote the OLS residual variance estimate and define the standardized residual of observation $i$ by $r_i = \\dfrac{e_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}}$.\n\nStarting from these fundamental definitions, derive an expression for the aggregate change in the fitted response vector caused by deleting observation $i$, and use it to obtain an explicit analytic formula for Cook’s distance $D_i$ that depends only on the leverage $h_{ii}$, the standardized residual $r_i$, and the number of parameters $p$. Then, in the context of linear QSAR diagnostics, explain how the derived form of $D_i$ guides the detection of influential observations, including the roles of leverage and residual size and scientifically accepted, model-size-aware decision heuristics for influence screening. Your final answer must be a single, closed-form analytic expression for $D_i$ in terms of $h_{ii}$, $r_i$, and $p$.",
            "solution": "The problem requires the derivation of an explicit formula for Cook’s distance, $D_i$, in terms of model leverage $h_{ii}$, standardized residual $r_i$, and the number of parameters $p$. It also requests an explanation of its use in identifying influential observations in a quantitative structure–activity relationship (QSAR) context.\n\nFirst, we validate the problem statement.\nThe problem provides a standard setup for ordinary least squares (OLS) linear regression.\n**Givens:**\n- Model: Linear, $y = X\\beta + \\epsilon$.\n- OLS estimator: $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$.\n- Fitted values: $\\hat{y} = X\\hat{\\beta}$.\n- Residuals: $e = y - \\hat{y}$.\n- Hat matrix: $H = X(X^{\\top}X)^{-1}X^{\\top}$.\n- Leverages: $h_{ii}$ are the diagonal elements of $H$.\n- Residual variance estimate: $\\hat{\\sigma}^{2}$.\n- Standardized residual: $r_i = \\dfrac{e_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}}$.\nThe problem is scientifically grounded, well-posed, objective, and self-contained. It is a standard derivation in regression diagnostics. Therefore, the problem is valid.\n\nWe begin the derivation of Cook's distance, $D_i$. Cook's distance measures the influence of observation $i$ by quantifying the aggregate change in the fitted values when observation $i$ is deleted from the dataset. It is defined as:\n$$ D_i = \\frac{\\sum_{j=1}^{n} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{p \\hat{\\sigma}^2} $$\nwhere $\\hat{y}_{j(i)}$ is the $j$-th fitted value obtained from a regression where observation $i$ has been removed. The vector of these fitted values is $\\hat{y}_{(i)} = X \\hat{\\beta}_{(i)}$, where $\\hat{\\beta}_{(i)}$ is the coefficient vector estimated without observation $i$. The numerator can be written in matrix form:\n$$ D_i = \\frac{(\\hat{y} - \\hat{y}_{(i)})^{\\top}(\\hat{y} - \\hat{y}_{(i)})}{p \\hat{\\sigma}^2} $$\nSubstituting $\\hat{y} = X\\hat{\\beta}$ and $\\hat{y}_{(i)} = X\\hat{\\beta}_{(i)}$, we get:\n$$ D_i = \\frac{(X\\hat{\\beta} - X\\hat{\\beta}_{(i)})^{\\top}(X\\hat{\\beta} - X\\hat{\\beta}_{(i)})}{p \\hat{\\sigma}^2} = \\frac{(\\hat{\\beta} - \\hat{\\beta}_{(i)})^{\\top}X^{\\top}X(\\hat{\\beta} - \\hat{\\beta}_{(i)})}{p \\hat{\\sigma}^2} $$\nThe central task is to find a computationally efficient expression for the change in the coefficient vector, $\\hat{\\beta} - \\hat{\\beta}_{(i)}$. A key result in regression diagnostics, derivable from the Sherman-Morrison-Woodbury formula for a rank-$1$ update of an inverse matrix, connects this change to quantities from the full-data regression:\n$$ \\hat{\\beta} - \\hat{\\beta}_{(i)} = \\frac{(X^{\\top}X)^{-1}x_i e_i}{1 - h_{ii}} $$\nwhere $x_i^{\\top}$ is the $i$-th row of the design matrix $X$, $e_i = y_i - \\hat{y}_i$ is the $i$-th ordinary residual, and $h_{ii} = x_i^{\\top}(X^{\\top}X)^{-1}x_i$ is the $i$-th leverage value.\n\nNow, we substitute this expression back into the numerator of $D_i$:\n$$ (\\hat{\\beta} - \\hat{\\beta}_{(i)})^{\\top}X^{\\top}X(\\hat{\\beta} - \\hat{\\beta}_{(i)}) = \\left( \\frac{(X^{\\top}X)^{-1}x_i e_i}{1 - h_{ii}} \\right)^{\\top} X^{\\top}X \\left( \\frac{(X^{\\top}X)^{-1}x_i e_i}{1 - h_{ii}} \\right) $$\nThe term $e_i / (1 - h_{ii})$ is a scalar, so we can factor it out:\n$$ = \\frac{e_i^2}{(1 - h_{ii})^2} \\left( (X^{\\top}X)^{-1}x_i \\right)^{\\top} X^{\\top}X \\left( (X^{\\top}X)^{-1}x_i \\right) $$\nApplying the transpose rule $(AB)^{\\top} = B^{\\top}A^{\\top}$:\n$$ = \\frac{e_i^2}{(1 - h_{ii})^2} x_i^{\\top} \\left((X^{\\top}X)^{-1}\\right)^{\\top} X^{\\top}X (X^{\\top}X)^{-1}x_i $$\nSince $X^{\\top}X$ is a symmetric matrix, its inverse $(X^{\\top}X)^{-1}$ is also symmetric. Thus, $\\left((X^{\\top}X)^{-1}\\right)^{\\top} = (X^{\\top}X)^{-1}$. The expression simplifies:\n$$ = \\frac{e_i^2}{(1 - h_{ii})^2} x_i^{\\top} (X^{\\top}X)^{-1} X^{\\top}X (X^{\\top}X)^{-1}x_i $$\n$$ = \\frac{e_i^2}{(1 - h_{ii})^2} x_i^{\\top} (X^{\\top}X)^{-1}x_i $$\nWe recognize the term $x_i^{\\top}(X^{\\top}X)^{-1}x_i$ as the definition of the leverage $h_{ii}$. Therefore, the numerator is:\n$$ \\text{Numerator} = \\frac{e_i^2 h_{ii}}{(1 - h_{ii})^2} $$\nSubstituting this back into the formula for $D_i$:\n$$ D_i = \\frac{1}{p\\hat{\\sigma}^2} \\frac{e_i^2 h_{ii}}{(1 - h_{ii})^2} $$\nWe are asked to express this in terms of the standardized residual $r_i$. The problem defines $r_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}}$. Squaring both sides gives $r_i^2 = \\frac{e_i^2}{\\hat{\\sigma}^2(1 - h_{ii})}$, which implies $e_i^2 = r_i^2 \\hat{\\sigma}^2 (1 - h_{ii})$.\nNow, substitute this expression for $e_i^2$ into the formula for $D_i$:\n$$ D_i = \\frac{1}{p\\hat{\\sigma}^2} \\frac{r_i^2 \\hat{\\sigma}^2 (1 - h_{ii}) h_{ii}}{(1 - h_{ii})^2} $$\nThe $\\hat{\\sigma}^2$ terms cancel, as does one factor of $(1 - h_{ii})$:\n$$ D_i = \\frac{r_i^2 h_{ii}}{p(1 - h_{ii})} $$\nThis expression can be rearranged to highlight the constituent parts:\n$$ D_i = \\frac{r_i^2}{p} \\left( \\frac{h_{ii}}{1 - h_{ii}} \\right) $$\nThis final form is the required analytic formula for Cook's distance.\n\nIn the context of linear QSAR diagnostics, this formula provides critical insight into the detection of influential observations. An influential observation is a data point that, if removed, would cause a substantial change in the estimated model parameters $\\hat{\\beta}$ and consequently the fitted values $\\hat{y}$. The formula demonstrates that influence, as measured by $D_i$, is a function of two distinct components:\n$1$. **Residual Size**: The term $r_i^2/p$ reflects how poorly the observation is fit by the model. The standardized residual $r_i$ measures the distance between the observed response $y_i$ and the fitted response $\\hat{y}_i$, scaled by an estimate of its standard deviation. A large $|r_i|$ indicates that the point is an outlier in the response variable direction.\n$2$. **Leverage**: The term $h_{ii}/(1-h_{ii})$ reflects the potential for influence due to the observation's position in the predictor space (the space of molecular descriptors). The leverage $h_{ii}$ ranges from $1/n$ to $1$ and measures how far the descriptor vector $x_i$ is from the center of the descriptor data. The term $h_{ii}/(1-h_{ii})$ is a monotonically increasing function of $h_{ii}$ that grows nonlinearly and approaches infinity as $h_{ii}$ approaches $1$. A point with high leverage is said to be at a \"high-leverage\" position.\n\nCrucially, an observation must have both a large residual and high leverage to be influential. A point with high leverage but a small residual (a \"good\" leverage point) lies far out in the descriptor space but is consistent with the trend of the other data, often stabilizing the regression model. Conversely, a point with a large residual but low leverage (a typical outlier) has predictor values close to the average and thus has limited ability to pull the regression line towards it. An influential point is one that is both an outlier in the $y$-direction (large $r_i$) and a remote point in the $X$-space (high $h_{ii}$).\n\nFor practical influence screening in QSAR, one cannot simply inspect residuals or leverages in isolation. Cook's distance combines both into a single, interpretable metric. Scientifically accepted, model-size-aware decision heuristics are used to flag potentially influential compounds for further investigation. A common and simple rule of thumb is to consider observations with $D_i > 1$ as highly influential. However, for larger sample sizes, this threshold can be too conservative. A more sensitive, size-adjusted heuristic is to flag observations where $D_i > 4/n$, where $n$ is the number of compounds. This criterion automatically adjusts the threshold for influence based on the size of the dataset, which is a critical consideration in QSAR studies that may range from a few dozen to thousands of compounds. Identifying such points is the first step in a careful diagnostic process, which may involve checking for data entry errors, re-evaluating the molecular structure, or considering if the compound represents a different mechanism of action not captured by the current model.",
            "answer": "$$\\boxed{\\frac{r_i^2}{p} \\left( \\frac{h_{ii}}{1 - h_{ii}} \\right)}$$"
        }
    ]
}