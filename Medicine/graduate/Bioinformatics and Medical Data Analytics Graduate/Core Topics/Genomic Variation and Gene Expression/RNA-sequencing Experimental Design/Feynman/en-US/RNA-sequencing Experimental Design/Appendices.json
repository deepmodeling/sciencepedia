{
    "hands_on_practices": [
        {
            "introduction": "A successful RNA-sequencing experiment hinges on a design that can distinguish true biological signals from technical noise and random variation. At the heart of this is the principle of biological replication, which allows us to quantify the natural variability within a population. This first exercise  challenges you to critique a common but fundamentally flawed experimental design that substitutes deep sequencing of pooled samples for true biological replicates, forcing you to articulate why such a design invalidates statistical inference.",
            "id": "2385533",
            "problem": "An investigator wants to assess Differential Gene Expression (DGE) between a control condition and a treatment. They collect tissue from $n_c$ control donors and $n_t$ treatment donors, extract total ribonucleic acid (RNA), and create two pools: one by combining equal RNA mass from all control donors and one by combining equal RNA mass from all treatment donors. They construct one sequencing library from each pool on different days and perform high-depth RNA sequencing (RNA-seq), obtaining gene-level count data for $G$ genes. They plan to identify differentially expressed genes by applying a standard DGE pipeline that outputs $p$-values and False Discovery Rate (FDR) $q$-values.\n\nWhich statement is the most accurate critique of this design, together with an appropriate remedy?\n\nA. The fundamental flaw is the absence of independent biological replicates, which makes the between-sample variance unidentifiable and renders inferential $p$-values for differential expression invalid; reads within a library are not independent experimental units. Pooling also masks donor-to-donor heterogeneity. The appropriate remedy is to include at least $k\\geq 3$ independent biological replicates per condition, randomized and balanced across library-preparation batches, so that variance can be estimated and batch effects can be separated from condition effects.\n\nB. The design is acceptable if the sequencing depth is extremely high (for example, $\\geq 10^8$ reads per library), because the law of large numbers ensures that per-gene means and variances are estimated with sufficient precision to yield valid hypothesis tests without biological replicates.\n\nC. Biological replication is unnecessary if multiple technical replicates are generated by sequencing the same two pooled libraries across $r\\geq 3$ lanes each; lane-to-lane variation provides the variance needed for valid differential expression testing between conditions.\n\nD. External spike-in controls allow complete normalization and hypothesis testing without biological replicates; with appropriate spike-ins, one can compute valid $p$-values for each gene despite having only one pooled sample per condition.\n\nE. Even with one pooled library per condition, a permutation test can be constructed by shuffling reads between the two libraries to create an empirical null distribution for each gene, enabling valid $p$-values without biological replicates.",
            "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- An investigator aims to perform Differential Gene Expression (DGE) analysis between a control and a treatment condition.\n- Tissue samples are from $n_c$ control donors and $n_t$ treatment donors.\n- Total ribonucleic acid (RNA) is extracted from all samples.\n- Two pools of RNA are created: one by combining equal RNA mass from all $n_c$ control donors, and one by combining equal RNA mass from all $n_t$ treatment donors.\n- One sequencing library is constructed from each of the two pools.\n- The two libraries are prepared on different days.\n- High-depth RNA sequencing (RNA-seq) is performed on the two libraries.\n- Gene-level count data are obtained for $G$ genes.\n- The investigator's plan is to identify differentially expressed genes using a standard DGE pipeline which outputs $p$-values and False Discovery Rate (FDR) $q$-values.\n- The question asks for the most accurate critique of this experimental design and the appropriate remedy.\n\nStep 2: Validate Using Extracted Givens\nThe problem describes a specific experimental design for an RNA-seq experiment and asks for a critical evaluation based on principles of biostatistics and bioinformatics.\n\n- **Scientifically Grounded:** The concepts presented—DGE, RNA-seq, pooling of samples, biological replicates, batch effects, $p$-values, and FDR—are all standard and fundamental to the field of computational biology and genomics. The described experimental design, while flawed, is a realistic scenario that investigators might propose. The problem is based on established scientific principles.\n- **Well-Posed:** The problem is structured as a multiple-choice question requiring the identification of the most accurate critique among a set of options. This is a standard and well-posed format for assessing understanding of experimental design principles. A unique, best answer is expected to exist based on established best practices in the field.\n- **Objective:** The language is technical and precise. Terms like \"control condition,\" \"treatment,\" \"RNA-seq,\" and \"biological replicates\" have clear, unambiguous definitions in this scientific context. The description of the experimental procedure is objective and free of subjective claims.\n\nThe problem statement itself does not contain scientific fallacies; rather, it describes an experimental design that is scientifically flawed and asks the student to identify those flaws. This is a valid pedagogical approach. The problem is self-contained, consistent, and directly relevant to its topic. It does not violate any of the criteria for invalidity.\n\nStep 3: Verdict and Action\nThe problem statement is valid. I will proceed with a full derivation and analysis.\n\nThe central task in differential gene expression analysis is to test the hypothesis that the mean expression of a gene differs between two or more populations (e.g., control population vs. treatment population). A statistical test requires an estimate of the effect size (the difference in means) and an estimate of the variance or uncertainty associated with this effect size.\n\nIn biological experiments, the total variance has two major components: biological variance and technical variance.\n1.  **Biological Variance** ($\\sigma^2_{\\text{bio}}$): This represents the true, natural variation in gene expression levels among different individuals within the same population or condition. It is the donor-to-donor variability in this problem. Inference about the populations requires quantifying this variance.\n2.  **Technical Variance** ($\\sigma^2_{\\text{tech}}$): This is the variation introduced during the experimental measurement process, including RNA extraction, library preparation, and sequencing. For count data like RNA-seq, a major component of technical noise is Poisson shot noise.\n\nThe fundamental flaw in the proposed experimental design is the complete absence of **independent biological replicates**. By pooling all RNA from the $n_c$ control donors into a single sample and all RNA from the $n_t$ treatment donors into another single sample, the investigator is left with an effective sample size of $n=1$ for the control group and $n=1$ for the treatment group.\n\nWith a sample size of $n=1$ per group, it is mathematically impossible to estimate the biological variance ($\\sigma^2_{\\text{bio}}$). Any observed difference in gene expression between the two pooled libraries is hopelessly confounded. The difference could be due to:\n- A true biological effect of the treatment.\n- Random chance, where the specific group of $n_c$ donors happened to have, on average, different expression levels from the specific group of $n_t$ donors, even without any treatment.\n- A systematic technical artifact, or **batch effect**, since the two libraries were prepared on different days. The day of preparation is perfectly confounded with the condition (control vs. treatment).\n\nThe reads obtained from sequencing a single library are **technical replicates**, not biological replicates. Increasing sequencing depth provides a more precise measurement of the mean expression level *in that specific pool of RNA*, thereby reducing the uncertainty due to technical (sampling) variance. However, it provides zero information about the biological variance of the population from which the donors were sampled. Inferential statistics for DGE are concerned with whether an observed effect is large relative to the biological variation in the population, not the technical variation in measuring one sample. Standard DGE software packages like DESeq2 or edgeR are explicitly designed to model biological variance using data from biological replicates. Without them, the models cannot be properly fitted, and the resulting $p$-values are invalid.\n\nThe appropriate remedy must correct these flaws. It requires:\n1.  Creating independent libraries for multiple individuals from each condition. This constitutes true biological replication and allows for the estimation of biological variance. A minimum of $k=3$ replicates per condition is a widely accepted standard for achieving reasonable statistical power and robust variance estimation.\n2.  Addressing the batch effect. Experimental units (libraries from controls and treatments) must be processed in a randomized and balanced manner across batches (e.g., days of library preparation) to ensure that any batch-related technical variation does not confound the biological signal of interest.\n\nNow, I will evaluate each option.\n\n**A. The fundamental flaw is the absence of independent biological replicates, which makes the between-sample variance unidentifiable and renders inferential $p$-values for differential expression invalid; reads within a library are not independent experimental units. Pooling also masks donor-to-donor heterogeneity. The appropriate remedy is to include at least $k\\geq 3$ independent biological replicates per condition, randomized and balanced across library-preparation batches, so that variance can be estimated and batch effects can be separated from condition effects.**\nThis statement is a precise and comprehensive critique. It correctly identifies the lack of biological replicates as the core issue, explains the consequence (unidentifiable biological variance and invalid $p$-values), correctly distinguishes between technical and biological units, and notes the loss of information from pooling. The proposed remedy—using at least $3$ biological replicates per condition and employing a randomized, balanced design to mitigate batch effects—is the standard and correct solution in modern genomics. The original design had a batch effect (different days), which this remedy also explicitly addresses.\nVerdict: **Correct**.\n\n**B. The design is acceptable if the sequencing depth is extremely high (for example, $\\geq 10^8$ reads per library), because the law of large numbers ensures that per-gene means and variances are estimated with sufficient precision to yield valid hypothesis tests without biological replicates.**\nThis is incorrect. High sequencing depth only reduces technical variance, providing a very precise measurement of the specific pooled sample. It does not and cannot provide information about the biological variance within the population of donors. The Law of Large Numbers applies to the technical sampling of RNA molecules from the library, not the biological sampling of subjects from the population. The hypothesis test is about the population, so an estimate of population variance is required, which necessitates biological replicates.\nVerdict: **Incorrect**.\n\n**C. Biological replication is unnecessary if multiple technical replicates are generated by sequencing the same two pooled libraries across $r\\geq 3$ lanes each; lane-to-lane variation provides the variance needed for valid differential expression testing between conditions.**\nThis is incorrect. Technical replicates (e.g., multiple sequencing lanes for the same library) only allow for the estimation of technical variance. Using technical variance as a proxy for biological variance in a hypothesis test is a grave statistical error. Biological variance is almost always substantially larger than technical variance. Doing so would lead to an extremely small denominator in the test statistic, resulting in a massive inflation of false positives. One would detect thousands of \"significant\" genes that are merely reflections of small random fluctuations between the two specific pools, not true biological effects generalizable to the populations.\nVerdict: **Incorrect**.\n\n**D. External spike-in controls allow complete normalization and hypothesis testing without biological replicates; with appropriate spike-ins, one can compute valid $p$-values for each gene despite having only one pooled sample per condition.**\nThis is incorrect. Spike-in controls are exogenous RNA molecules of known concentration added to each sample. Their primary use is for technical normalization and to assess technical performance. They can help in estimating technical variance and modeling technical biases. However, since they are not subject to biological regulation within the organism, they provide no information whatsoever about the biological variance of the endogenous genes of interest. Thus, they cannot substitute for biological replicates in a DGE analysis.\nVerdict: **Incorrect**.\n\n**E. Even with one pooled library per condition, a permutation test can be constructed by shuffling reads between the two libraries to create an empirical null distribution for each gene, enabling valid $p$-values without biological replicates.**\nThis is incorrect. A permutation test relies on the exchangeability of sample labels under the null hypothesis. The units that are exchangeable are the independent biological replicates. Here, there is only one unit per condition (the pooled library). There is nothing to permute. Shuffling individual reads between libraries is not a valid statistical procedure. Reads within a library are not independent; their counts are correlated and depend on the composition of that specific library. Shuffling them would destroy the data's structure and would not generate a valid null distribution for the test statistic.\nVerdict: **Incorrect**.\n\nIn conclusion, Option A is the only one that correctly diagnoses the critical flaws of the experimental design and proposes the correct, standard remedy.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Proper experimental design involves not only including replicates but also preventing systematic biases from confounding your results. Confounding occurs when a technical factor, like the processing batch, is correlated with the biological factor of interest, such as the disease condition. This practice  moves from concept to calculation, asking you to derive and quantify the bias introduced into a differential expression estimate when a batch effect is ignored, providing a stark mathematical illustration of why randomization and balanced designs are critical for valid conclusions.",
            "id": "4605715",
            "problem": "A Ribonucleic Acid sequencing (RNA-seq) study aims to detect differential expression between a disease condition and a control condition. For a single gene, assume the normalized log-count for sample $i$ is modeled by a linear mean-shift generative model\n$$\ny_i = \\mu + \\beta c_i + \\gamma b_i + \\varepsilon_i,\n$$\nwhere $y_i$ is the log-count, $c_i \\in \\{0,1\\}$ indicates condition ($c_i=1$ for disease, $c_i=0$ for control), $b_i \\in \\{0,1\\}$ indicates batch membership ($b_i=1$ for batch A, $b_i=0$ for batch B), $\\mu$ is a baseline mean, $\\beta$ is the true condition effect (log-scale), $\\gamma$ is the batch effect (log-scale), and $\\varepsilon_i$ are independent errors with $\\mathbb{E}[\\varepsilon_i]=0$ and finite variance. Suppose the experimental design induces correlation between batch and condition (e.g., more disease samples are processed in batch A than in batch B), but batch is not adjusted for in the analysis.\n\nAn analyst erroneously fits the reduced Ordinary Least Squares (OLS) model that omits batch,\n$$\ny_i = \\alpha + \\tau c_i + u_i,\n$$\nand reports $\\hat{\\tau}$ as the estimated log-scale differential expression between disease and control.\n\nStarting from fundamental definitions of linear models and the OLS estimator as a projection of the response onto the span of the regressors, derive an analytic expression for the expectation $\\mathbb{E}[\\hat{\\tau}]$ under the generative model above, and from it derive the bias $\\mathbb{E}[\\hat{\\tau}] - \\beta$ as a function of the omitted batch effect and the joint distribution of $c_i$ and $b_i$. Then, for the following concrete design, compute the expected value $\\mathbb{E}[\\hat{\\tau}]$:\n\n- Total samples $n=100$, with $n_1=50$ disease samples ($c_i=1$) and $n_0=50$ control samples ($c_i=0$).\n- Batch composition: among disease samples, $m_1=45$ are in batch A ($b_i=1$) and $5$ are in batch B ($b_i=0$); among control samples, $m_0=5$ are in batch A and $45$ are in batch B.\n- True condition effect $\\beta=1.2$ and batch effect $\\gamma=0.7$.\n\nExpress the final numerical answer as a pure number (dimensionless) and round your answer to four significant figures.",
            "solution": "The user wants me to critically validate and then solve a problem concerning omitted variable bias in a linear model for RNA-sequencing data analysis.\n\n### Step 1: Extract Givens\n\n-   **True Generative Model:** For sample $i$, the normalized log-count $y_i$ is given by $y_i = \\mu + \\beta c_i + \\gamma b_i + \\varepsilon_i$.\n    -   $y_i$: log-count for sample $i$.\n    -   $c_i \\in \\{0,1\\}$: condition indicator ($1$ for disease, $0$ for control).\n    -   $b_i \\in \\{0,1\\}$: batch indicator ($1$ for batch A, $0$ for batch B).\n    -   $\\mu$: baseline mean.\n    -   $\\beta$: true condition effect.\n    -   $\\gamma$: true batch effect.\n    -   $\\varepsilon_i$: independent errors with $\\mathbb{E}[\\varepsilon_i]=0$ and finite variance.\n-   **Fitted (Misspecified) Model:** An analyst fits the model $y_i = \\alpha + \\tau c_i + u_i$.\n    -   $\\hat{\\tau}$: the OLS estimate of the condition effect, reported as the result.\n-   **Experimental Design Data:**\n    -   Total samples: $n=100$.\n    -   Disease samples ($c_i=1$): $n_1=50$.\n    -   Control samples ($c_i=0$): $n_0=50$.\n    -   Batch composition for disease samples: $m_1=45$ in batch A ($b_i=1$), $5$ in batch B ($b_i=0$).\n    -   Batch composition for control samples: $m_0=5$ in batch A ($b_i=1$), $45$ in batch B ($b_i=0$).\n-   **True Parameter Values:**\n    -   $\\beta = 1.2$.\n    -   $\\gamma = 0.7$.\n-   **Task:**\n    1.  Derive the expectation $\\mathbb{E}[\\hat{\\tau}]$ and the bias $\\mathbb{E}[\\hat{\\tau}] - \\beta$ as a function of $\\gamma$ and the joint distribution of $c_i$ and $b_i$. The derivation should start from the OLS estimator's definition as a projection.\n    2.  Compute the numerical value of $\\mathbb{E}[\\hat{\\tau}]$ for the given design.\n    3.  Report the final numerical answer rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded:** The problem is firmly grounded in statistical theory (omitted variable bias in linear regression) and its application to a realistic scenario in bioinformatics (confounding of a biological condition with a technical batch effect in an RNA-seq experiment). This is a canonical problem in applied statistics.\n-   **Well-Posed:** The problem is well-posed. The models are clearly defined, all necessary parameters and data are provided, and a specific, unique analytical and numerical answer is requested.\n-   **Objective:** The problem is stated in objective, mathematical language and asks for a quantitative, verifiable result.\n-   **Completeness and Consistency:** The problem is self-contained and provides all necessary information for a solution. There are no contradictions in the given data.\n-   **Realism and Feasibility:** The experimental design described, where a condition is confounded with a batch, is a common and serious issue in real-world studies. The specified sample sizes and effect sizes are plausible.\n-   **Other Flaws:** The problem does not exhibit any other flaws such as being ill-posed, trivial, or pseudo-profound.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. The reasoning process will now proceed to derive the solution.\n\n### Derivation of the Estimator's Expectation and Bias\n\nThe analyst fits the simple linear regression model $y_i = \\alpha + \\tau c_i + u_i$. The Ordinary Least Squares (OLS) estimator for the slope parameter $\\tau$ is given by $\\hat{\\tau}$. Fundamentally, $\\hat{\\tau}$ is derived from the projection of the response vector $\\mathbf{y}$ onto the subspace spanned by the regressors, after accounting for the intercept. This is equivalent to regressing the centered response on the centered regressor.\n\nLet $\\mathbf{y} = [y_1, \\dots, y_n]^T$ be the vector of responses and $\\mathbf{c} = [c_1, \\dots, c_n]^T$ be the vector of the condition indicator. Let $\\mathbf{1}$ be a vector of ones of length $n$. The OLS estimator $\\hat{\\tau}$ is obtained by projecting $\\mathbf{y}$ onto the part of $\\mathbf{c}$ that is orthogonal to $\\mathbf{1}$. Let $\\mathbf{c}^* = \\mathbf{c} - \\bar{c}\\mathbf{1}$ be the centered condition vector, where $\\bar{c} = \\frac{1}{n}\\sum_{i=1}^n c_i$. The estimator $\\hat{\\tau}$ is then:\n$$\n\\hat{\\tau} = \\frac{\\langle \\mathbf{c}^*, \\mathbf{y} \\rangle}{\\langle \\mathbf{c}^*, \\mathbf{c}^* \\rangle} = \\frac{\\sum_{i=1}^n (c_i - \\bar{c})y_i}{\\sum_{i=1}^n (c_i - \\bar{c})^2}\n$$\nThe true generative model is $\\mathbf{y} = \\mu\\mathbf{1} + \\beta\\mathbf{c} + \\gamma\\mathbf{b} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{b}=[b_1, \\dots, b_n]^T$ and $\\boldsymbol{\\varepsilon}=[\\varepsilon_1, \\dots, \\varepsilon_n]^T$. To find the expectation of $\\hat{\\tau}$, we substitute this true model into the estimator's formula.\nThe numerator becomes:\n$$\n\\sum_{i=1}^n (c_i - \\bar{c})y_i = \\sum_{i=1}^n (c_i - \\bar{c})(\\mu + \\beta c_i + \\gamma b_i + \\varepsilon_i)\n$$\nWe expand this sum term by term:\n1.  $\\sum_{i=1}^n (c_i - \\bar{c})\\mu = \\mu \\sum_{i=1}^n (c_i - \\bar{c}) = \\mu(n\\bar{c} - n\\bar{c}) = 0$.\n2.  $\\sum_{i=1}^n (c_i - \\bar{c})(\\beta c_i) = \\beta \\sum_{i=1}^n (c_i - \\bar{c})c_i = \\beta \\sum_{i=1}^n (c_i - \\bar{c})(c_i - \\bar{c}) = \\beta \\sum_{i=1}^n (c_i - \\bar{c})^2$.\n3.  $\\sum_{i=1}^n (c_i - \\bar{c})(\\gamma b_i) = \\gamma \\sum_{i=1}^n (c_i - \\bar{c})b_i$.\n4.  $\\sum_{i=1}^n (c_i - \\bar{c})\\varepsilon_i$.\n\nSubstituting these back into the expression for $\\hat{\\tau}$:\n$$\n\\hat{\\tau} = \\frac{\\beta \\sum_{i=1}^n (c_i - \\bar{c})^2 + \\gamma \\sum_{i=1}^n (c_i - \\bar{c})b_i + \\sum_{i=1}^n (c_i - \\bar{c})\\varepsilon_i}{\\sum_{i=1}^n (c_i - \\bar{c})^2}\n$$\n$$\n\\hat{\\tau} = \\beta + \\gamma \\frac{\\sum_{i=1}^n (c_i - \\bar{c})b_i}{\\sum_{i=1}^n (c_i - \\bar{c})^2} + \\frac{\\sum_{i=1}^n (c_i - \\bar{c})\\varepsilon_i}{\\sum_{i=1}^n (c_i - \\bar{c})^2}\n$$\nWe now take the expectation with respect to the distribution of the errors $\\varepsilon_i$. Since $\\mathbb{E}[\\varepsilon_i]=0$ and the design variables $c_i, b_i$ are fixed, the expectation of the last term is zero.\n$$\n\\mathbb{E}[\\hat{\\tau}] = \\beta + \\gamma \\frac{\\sum_{i=1}^n (c_i - \\bar{c})b_i}{\\sum_{i=1}^n (c_i - \\bar{c})^2}\n$$\nThe bias of the estimator is $\\mathbb{E}[\\hat{\\tau}] - \\beta$:\n$$\n\\text{Bias}(\\hat{\\tau}) = \\mathbb{E}[\\hat{\\tau}] - \\beta = \\gamma \\frac{\\sum_{i=1}^n (c_i - \\bar{c})b_i}{\\sum_{i=1}^n (c_i - \\bar{c})^2}\n$$\nThis is the general formula for the omitted variable bias. The term $\\frac{\\sum(c_i-\\bar{c})b_i}{\\sum(c_i-\\bar{c})^2}$ is the coefficient of a regression of the omitted variable $b$ on the included variable $c$.\n\nTo express the bias in terms of the joint distribution of $c_i$ and $b_i$ (in this case, their sample counts), we can rewrite the terms. Let $n_{jk}$ be the count of samples with $c_i=j$ and $b_i=k$.\nThe sample mean of $c$ is $\\bar{c} = (n_{11}+n_{10})/n = n_1/n$.\nThe denominator is the sum of squared deviations for $c_i$:\n$\\sum(c_i - \\bar{c})^2 = n_1(1-\\bar{c})^2 + n_0(0-\\bar{c})^2 = n_1(n_0/n)^2 + n_0(-n_1/n)^2 = \\frac{n_1 n_0^2 + n_0 n_1^2}{n^2} = \\frac{n_1 n_0 (n_0+n_1)}{n^2} = \\frac{n_1 n_0}{n}$.\nThe numerator of the bias term is $\\sum(c_i - \\bar{c})b_i$, which is the sample covariance term:\n$\\sum(c_i - \\bar{c})b_i = n_{11}(1-\\bar{c}) + n_{01}(0-\\bar{c}) = n_{11}(1-n_1/n) - n_{01}(n_1/n) = n_{11}(n_0/n) - n_{01}(n_1/n) = \\frac{n_{11}n_0 - n_{01}n_1}{n}$.\nSo, the bias is:\n$$\n\\text{Bias}(\\hat{\\tau}) = \\gamma \\frac{(n_{11}n_0 - n_{01}n_1)/n}{n_1 n_0/n} = \\gamma \\frac{n_{11}n_0 - n_{01}n_1}{n_1 n_0} = \\gamma \\left( \\frac{n_{11}}{n_1} - \\frac{n_{01}}{n_0} \\right)\n$$\nHere, $n_{11}/n_1$ is the proportion of batch A samples within the disease group (sample $P(b=1|c=1)$), and $n_{01}/n_0$ is the proportion of batch A samples within the control group (sample $P(b=1|c=0)$).\n\n### Numerical Calculation\n\nNow we apply this formula to the given experimental design.\n-   Total samples $n = 100$.\n-   Disease samples ($c=1$): $n_1 = 50$.\n-   Control samples ($c=0$): $n_0 = 50$.\n-   Disease samples in batch A ($b=1, c=1$): $n_{11} = 45$.\n-   Control samples in batch A ($b=1, c=0$): $n_{01} = 5$.\n-   True effects: $\\beta = 1.2$, $\\gamma = 0.7$.\n\nFirst, we compute the proportions:\n-   Proportion of batch A in disease group: $P(b=1|c=1) = \\frac{n_{11}}{n_1} = \\frac{45}{50} = 0.9$.\n-   Proportion of batch A in control group: $P(b=1|c=0) = \\frac{n_{01}}{n_0} = \\frac{5}{50} = 0.1$.\n\nNext, we calculate the bias:\n$$\n\\text{Bias}(\\hat{\\tau}) = \\gamma \\left( \\frac{n_{11}}{n_1} - \\frac{n_{01}}{n_0} \\right) = 0.7 \\times (0.9 - 0.1) = 0.7 \\times 0.8 = 0.56\n$$\nThe expected value of the estimated condition effect $\\hat{\\tau}$ is the true effect $\\beta$ plus this bias:\n$$\n\\mathbb{E}[\\hat{\\tau}] = \\beta + \\text{Bias}(\\hat{\\tau}) = 1.2 + 0.56 = 1.76\n$$\nThe problem requires the answer to be rounded to four significant figures. Thus, $1.76$ becomes $1.760$. This value is dimensionless as it represents a log-count difference.",
            "answer": "$$\\boxed{1.760}$$"
        },
        {
            "introduction": "Even the most carefully designed experiment can encounter technical failures during execution. Therefore, rigorous quality control (QC) is an indispensable step that bridges the gap between raw data and reliable analysis. This final practice  simulates a real-world scenario where a single sample presents as a severe outlier, requiring you to synthesize multiple lines of evidence—from Principal Component Analysis to specific QC metrics—to diagnose the root cause of the issue and determine the most scientifically sound course of action.",
            "id": "4605754",
            "problem": "A study of messenger Ribonucleic Acid sequencing (RNA-seq) aims to estimate gene expression differences between a disease condition and controls. There are $n=12$ libraries: $6$ disease cases and $6$ controls. Library preparation was performed in two batches $\\mathcal{B}_1$ and $\\mathcal{B}_2$, each containing $3$ cases and $3$ controls, using the same kit and protocol. For each sample $i$, the observed count for gene $g$, denoted $Y_{gi}$, is modeled as coming from a sampling process in which fragments from expressed transcripts are stochastically captured and sequenced. A well-tested base model for count data assumes a Negative Binomial distribution with mean $\\mu_{gi}$ and variance $\\mu_{gi} + \\alpha_g \\mu_{gi}^2$, where $\\mu_{gi} = s_i q_{gi}$, $s_i$ is a sample-specific library size (size factor), $q_{gi}$ is the underlying expression level for gene $g$ in sample $i$, and $\\alpha_g$ is a gene-specific dispersion. Technical failures can distort $s_i$ and induce gene- and transcript-length-dependent biases in $q_{gi}$ (for example, due to degradation causing $3'$ end enrichment), thereby violating the assumption that differences are captured by fixed design covariates.\n\nQuality control (QC) summaries are available for all samples and include total reads $R_i$, uniquely mapped fraction $\\hat{p}_{u,i}$, duplication rate $d_i$, ribosomal fraction $r_i$, $3'$ to $5'$ end coverage ratio $b_i$, RNA Integrity Number $q_i$ (RIN), and estimated size factor $s_i$ by the median ratio method. Principal Component Analysis (PCA) was performed on log-transformed, normalized counts across genes, computing the eigendecomposition of the sample covariance matrix to obtain principal components with eigenvalues $\\lambda_1, \\lambda_2, \\ldots$; the first two components explain $\\lambda_1 / \\sum_j \\lambda_j$ and $\\lambda_2 / \\sum_j \\lambda_j$ of the variance, respectively.\n\nOne case sample, labeled $S7$, is a distinct outlier in the PCA. The first principal component explains $35\\%$ of variance (that is, $\\lambda_1 / \\sum_j \\lambda_j = 0.35$) and the second explains $18\\%$ of variance (that is, $\\lambda_2 / \\sum_j \\lambda_j = 0.18$). The PCA coordinates for $S7$ are $z_{1,7} = 5.2$ on principal component $1$ and $z_{2,7} = -0.4$ on principal component $2$, while all other samples satisfy $|z_{1,i}| < 1.0$ and $|z_{2,i}| < 1.2$. Coloring by batch shows overlapping clusters for $\\mathcal{B}_1$ and $\\mathcal{B}_2$, with $S7$ separating from both batches.\n\nQC metrics for $S7$ versus the ranges for other samples are:\n- $R_7 = 2.8 \\times 10^7$ reads (others: $R_i \\in [3.0 \\times 10^7, 4.1 \\times 10^7]$),\n- $\\hat{p}_{u,7} = 0.52$ (others: $\\hat{p}_{u,i} \\in [0.76, 0.88]$),\n- $d_7 = 0.65$ (others: $d_i \\in [0.22, 0.44]$),\n- $r_7 = 0.23$ (others: $r_i \\in [0.04, 0.09]$),\n- $b_7 = 2.3$ (others: $b_i \\in [0.93, 1.15]$),\n- $q_7 = 5.1$ (others: $q_i \\in [7.8, 9.2]$),\n- $s_7 = 0.42$ (others: $s_i \\in [0.84, 1.15]$).\n\nIn addition, External RNA Controls Consortium (ERCC) spike-in molecules were added at known quantities. For $S7$, the Spearman correlation between observed ERCC counts and expected concentrations is $\\rho_7 = 0.31$ (others: $\\rho_i \\in [0.78, 0.92]$). The remaining input RNA material available for $S7$ is $M_7 = 0.8 \\,\\mu\\text{g}$, and the library kit’s specification requires $\\geq 1.0 \\,\\mu\\text{g}$ of high-quality input RNA.\n\nYou plan to fit a generalized linear model to the counts $Y_{gi}$ using a design matrix $X$ with an intercept and condition indicator, and potentially batch covariates or additional nuisance factors. Based on the above, choose the most appropriate course of action for $S7$ to ensure scientifically valid inference and justify your choice from first principles of the data-generating and analysis models.\n\nWhich action is most appropriate?\n\nA. Exclude $S7$ from downstream differential expression analysis and document exclusion due to technical failure.\n\nB. Rerun library preparation for $S7$ using the remaining RNA material to recover a usable library.\n\nC. Retain $S7$ and include a batch indicator in the design matrix $X$ to account for the outlier.\n\nD. Retain $S7$ and apply surrogate variable analysis to capture hidden factors while keeping the sample.\n\nE. Retain $S7$ and filter out ribosomal RNA-aligning reads; proceed otherwise unchanged.",
            "solution": "The problem requires determining the most appropriate course of action for handling a sample, $S7$, which appears to be a significant outlier in an RNA-sequencing experiment. The decision must be grounded in the principles of experimental design, data quality control, and statistical modeling to ensure the scientific validity of the final conclusions.\n\nFirst, I will summarize and interpret the extensive evidence provided regarding sample $S7$. The goal is to determine if the sample's outlier status reflects a genuine biological phenomenon or a technical failure.\n\n**Analysis of Evidence Pertaining to Sample $S7$**\n\nThe data present a compelling and multi-faceted case for a severe technical failure in the processing of sample $S7$.\n\n1.  **Principal Component Analysis (PCA):** The sample $S7$ is a dramatic outlier on the first principal component (PC1), with a coordinate $z_{1,7} = 5.2$ while all other samples have $|z_{1,i}| < 1.0$. Since PC1 explains $35\\%$ of the total variance in the dataset ($\\lambda_1 / \\sum_j \\lambda_j = 0.35$), this single sample is responsible for driving the dominant pattern of variation. A primary source of variation driven by a single sample, rather than by the experimental condition of interest, is a strong indicator of a technical artifact, not a biologically extreme phenotype. The fact that $S7$ separates from both batches $\\mathcal{B}_1$ and $\\mathcal{B}_2$ confirms that this is not a simple batch effect.\n\n2.  **RNA Quality and Degradation:** The RNA Integrity Number (RIN) for $S7$ is $q_7 = 5.1$, which is substantially lower than the range for other samples ($q_i \\in [7.8, 9.2]$). A RIN score below $6.0$ indicates severely degraded RNA. This is corroborated by the high $3'$-to-$5'$ end coverage ratio of $b_7 = 2.3$ (others: $b_i \\in [0.93, 1.15]$). RNA degradation in conjunction with poly-A-tailed library preparation methods leads to an accumulation of reads at the $3'$ end of transcripts. This creates a gene-length-dependent bias in quantification that cannot be corrected by a single, sample-specific size factor $s_i$.\n\n3.  **Library Quality:** The library generated from $S7$ is of poor quality. The duplication rate is extremely high ($d_7 = 0.65$ vs. others in $[0.22, 0.44]$), suggesting a low-complexity library where PCR amplification was excessive to achieve sufficient yield. This reduces the effective sequencing depth and the precision of expression estimates. The uniquely mapped fraction is very low ($\\hat{p}_{u,7} = 0.52$ vs. others in $[0.76, 0.88]$), which could indicate contamination or other library preparation failures like adapter-dimer formation.\n\n4.  **Sequencing Efficiency and Contamination:** The ribosomal RNA fraction is markedly elevated ($r_7 = 0.23$ vs. others in $[0.04, 0.09]$), indicating that the poly-A selection or rRNA depletion step was inefficient. A large portion of sequencing resources was thus wasted on non-target molecules. This, combined with a slightly lower total read count ($R_7 = 2.8 \\times 10^7$), contributes to a very low estimated size factor ($s_7 = 0.42$), meaning the effective library size for coding transcripts is a fraction of that of the other samples.\n\n5.  **Quantitative Accuracy (Ground Truth):** The most definitive evidence of failure comes from the External RNA Controls Consortium (ERCC) spike-in data. The Spearman correlation between observed counts and expected concentrations for $S7$ is only $\\rho_7 = 0.31$, while for all other samples it is high ($\\rho_i \\in [0.78, 0.92]$). This demonstrates that the quantitative measurements from sample $S7$ do not faithfully reflect the true abundance of molecules in the original sample. The entire quantitative basis of the measurement is compromised.\n\n**Implications for the Statistical Model**\n\nThe intended analysis uses a Negative Binomial model where the mean count $\\mu_{gi}$ is modeled as $\\mu_{gi} = s_i q_{gi}$. This model assumes that all sample-specific technical effects can be captured by a single scaling factor $s_i$, leaving $q_{gi}$ to represent the biological quantity of interest. The evidence for $S7$ demonstrates that this assumption is grossly violated. The RNA degradation has introduced complex, gene-dependent biases (e.g., related to transcript length) that a single factor $s_7$ cannot possibly correct. Including this sample in the analysis would inflate variance estimates (the dispersion $\\alpha_g$), drastically reduce statistical power, and introduce a high risk of false positive and false negative differential expression calls due to the massive, unmodeled technical noise.\n\nBased on this analysis, we can evaluate the proposed actions.\n\n**Option-by-Option Analysis**\n\n**A. Exclude $S7$ from downstream differential expression analysis and document exclusion due to technical failure.**\nThe evidence is conclusive: the data from $S7$ are not a valid representation of the sample's biology due to a cascade of technical failures beginning with poor quality input RNA. The fundamental assumptions of the downstream statistical analysis are violated in a way that is not computationally correctable. In such cases, the most scientifically rigorous and defensible action is to exclude the sample. This prioritizes the validity and integrity of the study's conclusions over a marginal loss in sample size. The final analysis would proceed with $5$ cases versus $6$ controls. The exclusion and the detailed reasons for it must be documented transparently.\n**Verdict: Correct.**\n\n**B. Rerun library preparation for $S7$ using the remaining RNA material to recover a usable library.**\nThe problem states that the remaining RNA material is $M_7 = 0.8 \\,\\mu\\text{g}$, while the kit requires $\\geq 1.0 \\,\\mu\\text{g}$. Furthermore, this remaining RNA is the same stock that was established to be of very poor quality ($q_7 = 5.1$). Attempting to generate a new library from insufficient and degraded starting material is a violation of the protocol and is almost certain to result in another failed library. This action would be a waste of time and resources.\n**Verdict: Incorrect.**\n\n**C. Retain $S7$ and include a batch indicator in the design matrix $X$ to account for the outlier.**\nA batch effect is a systematic technical variation that affects a group of samples processed together. The PCA plot shows that $S7$ is an outlier with respect to *all* other samples, including those from its own batch. Therefore, its variation is not a batch effect. Including a batch covariate in the model would not account for the unique deviation of $S7$. Modeling $S7$ as its own batch is an ad-hoc fix that does not address the underlying non-linear, gene-dependent biases (like $3'$ bias) and the fundamental lack of quantitative accuracy confirmed by the ERCC data.\n**Verdict: Incorrect.**\n\n**D. Retain $S7$ and apply surrogate variable analysis to capture hidden factors while keeping the sample.**\nSurrogate Variable Analysis (SVA) is a method to estimate and account for unmeasured sources of systematic variation. While SVA is powerful, it is not intended to salvage data from a single, catastrophic sample failure. The variation from $S7$ is so dominant ($\\lambda_1 / \\sum_j \\lambda_j = 0.35$) and its technical cause is so well-documented (degradation, low complexity, etc.) that it is not a \"hidden\" factor. More importantly, SVA and other linear model adjustments cannot fix the fundamental problem that the data are not quantitative, as shown by the low ERCC correlation ($\\rho_7 = 0.31$). Best practice is to remove samples with known, severe technical artifacts *before* applying methods like SVA to find more subtle, unknown confounders.\n**Verdict: Incorrect.**\n\n**E. Retain $S7$ and filter out ribosomal RNA-aligning reads; proceed otherwise unchanged.**\nFiltering rRNA reads is a standard data processing step. The high rRNA fraction ($r_7 = 0.23$) is a symptom of a failed library preparation, not the root cause. Removing these reads would not fix the underlying problems of RNA degradation ($q_7 = 5.1$, $b_7 = 2.3$), low library complexity ($d_7 = 0.65$), or the resulting loss of quantitative accuracy ($\\rho_7 = 0.31$). The remaining non-rRNA reads would still carry all these technical artifacts, rendering them unsuitable for differential expression analysis.\n**Verdict: Incorrect.**\n\nIn conclusion, the overwhelming and consistent evidence from multiple, independent QC metrics points to an unrecoverable technical failure for sample $S7$. The only scientifically sound action is to exclude the sample to maintain the integrity of the analysis.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}