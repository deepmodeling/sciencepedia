{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in evaluating any new variant callset is to compute its transition/transversion (Ti/Tv) ratio. This simple statistic serves as a powerful, high-level sanity check on the data's quality. This practice will guide you through calculating the Ti/Tv ratio from raw variant counts and, more importantly, understanding how the result reflects underlying biological mutational patterns and can expose potential systemic errors in your dataset. ",
            "id": "4617272",
            "problem": "A variant callset was generated from human whole-genome sequencing (average coverage $30\\times$) and filtered to include only high-confidence biallelic single-nucleotide variants (SNVs). Insertions and deletions (indels) and multi-allelic sites were excluded. In single-stranded, directed notation, the observed counts of reference-to-alternate base changes are:\n\n- $A \\rightarrow G$: $850{,}250$\n- $G \\rightarrow A$: $845{,}700$\n- $C \\rightarrow T$: $832{,}400$\n- $T \\rightarrow C$: $838{,}100$\n- $A \\rightarrow C$: $220{,}500$\n- $C \\rightarrow A$: $222{,}300$\n- $A \\rightarrow T$: $180{,}200$\n- $T \\rightarrow A$: $179{,}800$\n- $C \\rightarrow G$: $215{,}400$\n- $G \\rightarrow C$: $216{,}100$\n- $G \\rightarrow T$: $230{,}600$\n- $T \\rightarrow G$: $231{,}300$\n\nUse the foundational definitions that transitions are purine-to-purine or pyrimidine-to-pyrimidine substitutions ($A \\leftrightarrow G$ and $C \\leftrightarrow T$) and transversions are purine-to-pyrimidine or pyrimidine-to-purine substitutions (the remaining four base-pair classes). Treat each directed count as one observed SNV, and assume strand symmetry does not change the classification.\n\n- Compute the transition-to-transversion ratio (Ti/Tv) for this callset as the ratio of the total number of transitions to the total number of transversions. Express the final answer as a dimensionless ratio and round your answer to four significant figures.\n\n- Then, starting from well-tested observations in human genetics that typical whole-genome Ti/Tv is approximately $2.0$ and coding-region Ti/Tv is approximately $3.0$ due to purifying selection against transversional nonsynonymous changes, explain how deviations of an observed Ti/Tv from these expectations can indicate quality issues (for example, excess sequencing or mapping errors) or enrichment for coding regions.",
            "solution": "The problem requires the calculation of the transition-to-transversion (Ti/Tv) ratio from a given set of single-nucleotide variant (SNV) counts, followed by an explanation of the significance of this ratio as a quality control metric in genomics.\n\nFirst, we must categorize the given SNV types as either transitions or transversions. The four nucleotide bases are classified into two chemical groups: purines, which have a double-ring structure ($A$, $G$), and pyrimidines, which have a single-ring structure ($C$, $T$).\nA **transition** is a substitution within the same chemical group:\n- Purine to purine: $A \\leftrightarrow G$\n- Pyrimidine to pyrimidine: $C \\leftrightarrow T$\n\nA **transversion** is a substitution between different chemical groups:\n- Purine to pyrimidine: $A \\leftrightarrow C$, $A \\leftrightarrow T$, $G \\leftrightarrow C$, $G \\leftrightarrow T$\n- Pyrimidine to purine: $C \\leftrightarrow A$, $C \\leftrightarrow G$, $T \\leftrightarrow A$, $T \\leftrightarrow G$\n\nThe problem provides the following directed counts for each SNV type:\n- $N_{A \\rightarrow G} = 850,250$ (Transition)\n- $N_{G \\rightarrow A} = 845,700$ (Transition)\n- $N_{C \\rightarrow T} = 832,400$ (Transition)\n- $N_{T \\rightarrow C} = 838,100$ (Transition)\n- $N_{A \\rightarrow C} = 220,500$ (Transversion)\n- $N_{C \\rightarrow A} = 222,300$ (Transversion)\n- $N_{A \\rightarrow T} = 180,200$ (Transversion)\n- $N_{T \\rightarrow A} = 179,800$ (Transversion)\n- $N_{C \\rightarrow G} = 215,400$ (Transversion)\n- $N_{G \\rightarrow C} = 216,100$ (Transversion)\n- $N_{G \\rightarrow T} = 230,600$ (Transversion)\n- $N_{T \\rightarrow G} = 231,300$ (Transversion)\n\nThe total number of transitions, denoted by $N_{Ti}$, is the sum of the counts of all transition-type substitutions.\n$$N_{Ti} = N_{A \\rightarrow G} + N_{G \\rightarrow A} + N_{C \\rightarrow T} + N_{T \\rightarrow C}$$\n$$N_{Ti} = 850,250 + 845,700 + 832,400 + 838,100 = 3,366,450$$\n\nThe total number of transversions, denoted by $N_{Tv}$, is the sum of the counts of all transversion-type substitutions.\n$$N_{Tv} = N_{A \\rightarrow C} + N_{C \\rightarrow A} + N_{A \\rightarrow T} + N_{T \\rightarrow A} + N_{C \\rightarrow G} + N_{G \\rightarrow C} + N_{G \\rightarrow T} + N_{T \\rightarrow G}$$\n$$N_{Tv} = 220,500 + 222,300 + 180,200 + 179,800 + 215,400 + 216,100 + 230,600 + 231,300$$\n$$N_{Tv} = 1,696,200$$\n\nThe transition-to-transversion ratio, $R_{Ti/Tv}$, is the ratio of the total number of transitions to the total number of transversions.\n$$R_{Ti/Tv} = \\frac{N_{Ti}}{N_{Tv}} = \\frac{3,366,450}{1,696,200}$$\n$$R_{Ti/Tv} \\approx 1.984700035...$$\n\nRounding the result to four significant figures as required by the problem statement, we obtain:\n$$R_{Ti/Tv} \\approx 1.985$$\n\nThe second part of the problem asks for an explanation of how deviations in the observed Ti/Tv ratio from expected values can indicate quality issues or biological context. The calculated ratio of approximately $1.985$ is very close to the established benchmark for human whole-genome sequencing (WGS), which is typically around $2.0$. This proximity suggests that the provided variant callset is of high quality and represents a non-enriched, genome-wide sample.\n\nThe significance of the Ti/Tv ratio stems from both biological and technical factors:\n- **Biological Basis**: Due to the chemical properties of nucleotide bases and the nature of spontaneous mutations (e.g., deamination of methylated cytosine to thymine, a common $C \\rightarrow T$ transition), transitions arise more frequently in the genome than transversions. Transversions are structurally more disruptive (substituting a single-ring base for a double-ring one, or vice-versa), which makes them more likely to be deleterious if they fall within a functional genomic element.\n- **Quality Control**: Deviations from the expected Ti/Tv ratio are a critical quality control metric for variant callsets.\n    1.  **A low Ti/Tv ratio (e.g., significantly less than $2.0$ for WGS)** is a strong indicator of random errors. If base substitutions were to occur purely at random, there are two possible transversion outcomes for each purine or pyrimidine, but only one possible transition outcome. This leads to a $1:2$ ratio of transitions to transversions, or a theoretical Ti/Tv ratio of $0.5$ for random noise. Therefore, an excess of false-positive variant calls, which are often random in nature (due to sequencing errors, mapping artifacts, etc.), will disproportionately increase the number of transversions and drive the overall Ti/Tv ratio down towards $0.5$. A low Ti/Tv ratio is a red flag that suggests the callset may have a high error rate and that variant filtering thresholds may be too lenient.\n    2.  **A high Ti/Tv ratio (e.g., approaching $3.0$)** often indicates enrichment for specific genomic regions, particularly coding sequences (exons). Within protein-coding regions, natural selection acts to preserve function. Transversions are more likely than transitions to result in a non-synonymous amino acid substitution, and these substitutions are more likely to be functionally radical (e.g., changing the chemical class of the amino acid). Consequently, purifying selection acts more strongly against transversions in coding regions. This differential selective pressure leads to a higher observed Ti/Tv ratio in exomes, typically around $3.0$. An unexpectedly high Ti/Tv ratio in a dataset purported to be from WGS could suggest that the sequencing library was unintentionally biased towards capturing exonic regions.\n\nIn summary, the Ti/Tv ratio is a powerful summary statistic. The value calculated here ($1.985$) reflects a high-quality WGS dataset. A lower value would suggest contamination with random errors, while a significantly higher value would suggest enrichment for coding regions where purifying selection has elevated the ratio.",
            "answer": "$$\\boxed{1.985}$$"
        },
        {
            "introduction": "While global metrics like the Ti/Tv ratio are valuable, robust quality control requires assessing the confidence of each individual variant call. The QUAL score is a cornerstone of this assessment, yet its statistical underpinnings can be opaque. This exercise demystifies the QUAL score by having you derive it from first principles using Bayes' rule, demonstrating how it quantitatively combines the likelihood of the observed sequencing data with prior beliefs about genetic variation. ",
            "id": "4617239",
            "problem": "A single diploid sample is sequenced at a biallelic locus potentially harboring a Single Nucleotide Polymorphism (SNP). Let $g \\in \\{0/0, 0/1, 1/1\\}$ denote the sample’s genotype and let $D$ denote the observed read data at the locus. The genotype likelihood $L(D \\mid g)$ is the probability of observing $D$ given genotype $g$. The widely used Phred-scaled likelihoods $PL_g$ are defined relative to the most likely genotype as $PL_g = -10 \\log_{10}\\left(L(D \\mid g)/\\max_{h} L(D \\mid h)\\right)$. Assume the following $PL$ values are reported for the locus: $PL_{0/0} = 80$, $PL_{0/1} = 0$, and $PL_{1/1} = 40$.\n\nA site-level prior expresses the prior odds $O$ of “variant” versus “no variant,” where “no variant” means genotype $0/0$ for the single sample. Let $O = 10^{-3}$. Conditional on the event “variant,” assume a prior allocation across the genotypes $0/1$ and $1/1$ given by a heterozygote fraction $\\alpha = 0.9$, so that $P(0/1) = \\alpha \\frac{O}{1+O}$ and $P(1/1) = (1-\\alpha)\\frac{O}{1+O}$, while $P(0/0) = \\frac{1}{1+O}$.\n\nUsing only the definitions above and Bayes’ rule, derive an expression for the posterior probability $P(\\text{no variant} \\mid D)$ in terms of $L(D \\mid g)$ and $P(g)$, and then obtain the site quality score $Q$ by applying the standard Phred transformation to this posterior probability. Finally, compute $Q$ for the locus using the provided $PL$ values and prior parameters. Round your final numerical result to four significant figures.",
            "solution": "The primary objective is to calculate the site quality score $Q$, which requires first determining the posterior probability of the site being non-variant, $P(\\text{no variant} \\mid D)$.\n\n**1. Derive the expression for the posterior probability $P(\\text{no variant} \\mid D)$**\n\nThe event \"no variant\" is equivalent to the sample having the homozygous reference genotype, $g=0/0$. We are asked to find the posterior probability of this event given the observed data $D$, which is $P(g=0/0 \\mid D)$.\n\nAccording to Bayes' rule, the posterior probability of any given genotype $g$ is:\n$$P(g \\mid D) = \\frac{P(D \\mid g) P(g)}{P(D)}$$\nwhere $P(D \\mid g)$ is the likelihood of the data given the genotype (denoted as $L(D \\mid g)$), $P(g)$ is the prior probability of the genotype, and $P(D)$ is the marginal probability of the data, which serves as a normalization constant. The marginal probability is calculated by summing over all possible genotypes $h$:\n$$P(D) = \\sum_{h \\in \\{0/0, 0/1, 1/1\\}} P(D \\mid h) P(h)$$\n\nSubstituting the given notation for likelihoods and priors, the denominator becomes:\n$$P(D) = L(D \\mid 0/0) P(0/0) + L(D \\mid 0/1) P(0/1) + L(D \\mid 1/1) P(1/1)$$\n\nThe posterior probability for the \"no variant\" case ($g=0/0$) is therefore:\n$$P(\\text{no variant} \\mid D) = P(g=0/0 \\mid D) = \\frac{L(D \\mid 0/0) P(0/0)}{L(D \\mid 0/0) P(0/0) + L(D \\mid 0/1) P(0/1) + L(D \\mid 1/1) P(1/1)}$$\nThis is the requested expression for the posterior probability in terms of $L(D \\mid g)$ and $P(g)$.\n\n**2. Define the site quality score $Q$**\n\nThe problem states that the site quality score $Q$ is obtained by applying the standard Phred transformation to the posterior probability of the \"no variant\" event. The Phred transformation of a probability $p$ is given by $-10 \\log_{10}(p)$. Thus, the definition of $Q$ is:\n$$Q = -10 \\log_{10}(P(\\text{no variant} \\mid D)) = -10 \\log_{10}(P(g=0/0 \\mid D))$$\n\n**3. Compute the value of $Q$**\n\nTo compute $Q$, we must first calculate the numerical value of $P(g=0/0 \\mid D)$ using the provided parameters. This involves calculating the likelihood ratios from the $PL$ values and the prior probabilities from $O$ and $\\alpha$.\n\nFirst, let's determine the relative likelihoods from the given $PL$ values. The definition is $PL_g = -10 \\log_{10}\\left(L(D \\mid g)/ L_{\\max}\\right)$, where $L_{\\max} = \\max_{h} L(D \\mid h)$. Rearranging this gives:\n$$\\frac{L(D \\mid g)}{L_{\\max}} = 10^{-PL_g/10}$$\nThe provided $PL$ values are $PL_{0/0} = 80$, $PL_{0/1} = 0$, and $PL_{1/1} = 40$. The genotype with $PL=0$ is the one with the maximum likelihood, so $L_{\\max} = L(D \\mid 0/1)$. The relative likelihoods are:\n-   For $g=0/0$: $\\frac{L(D \\mid 0/0)}{L_{\\max}} = 10^{-80/10} = 10^{-8}$\n-   For $g=0/1$: $\\frac{L(D \\mid 0/1)}{L_{\\max}} = 10^{-0/10} = 10^0 = 1$\n-   For $g=1/1$: $\\frac{L(D \\mid 1/1)}{L_{\\max}} = 10^{-40/10} = 10^{-4}$\n\nNext, we calculate the prior probabilities using $O = 10^{-3}$ and $\\alpha = 0.9$:\n-   $P(0/0) = \\frac{1}{1+O} = \\frac{1}{1+10^{-3}}$\n-   $P(0/1) = \\alpha \\frac{O}{1+O} = 0.9 \\times \\frac{10^{-3}}{1+10^{-3}}$\n-   $P(1/1) = (1-\\alpha) \\frac{O}{1+O} = (1-0.9) \\frac{10^{-3}}{1+10^{-3}} = 0.1 \\times \\frac{10^{-3}}{1+10^{-3}}$\n\nNow we substitute these into the expression for $P(g=0/0 \\mid D)$. Note that the terms $L_{\\max}$ in the likelihoods and $\\frac{1}{1+O}$ in the priors will cancel from the numerator and denominator. We can work with terms proportional to the likelihoods, $L'_g = L(D \\mid g)/L_{max}$, and terms proportional to the priors, $P'(g) = P(g)(1+O)$.\n-   $L'_{0/0} = 10^{-8}$, $L'_{0/1} = 1$, $L'_{1/1} = 10^{-4}$\n-   $P'(0/0) = 1$, $P'(0/1) = \\alpha O = 0.9 \\times 10^{-3}$, $P'(1/1) = (1-\\alpha)O = 0.1 \\times 10^{-3}$\n\nThe posterior probability is:\n$$P(g=0/0 \\mid D) = \\frac{L'_{0/0} P'(0/0)}{L'_{0/0} P'(0/0) + L'_{0/1} P'(0/1) + L'_{1/1} P'(1/1)}$$\nSubstituting the values:\n$$P(g=0/0 \\mid D) = \\frac{10^{-8} \\times 1}{(10^{-8} \\times 1) + (1 \\times 0.9 \\times 10^{-3}) + (10^{-4} \\times 0.1 \\times 10^{-3})}$$\n$$P(g=0/0 \\mid D) = \\frac{10^{-8}}{10^{-8} + 9 \\times 10^{-4} + 10^{-8}}$$\n$$P(g=0/0 \\mid D) = \\frac{10^{-8}}{9 \\times 10^{-4} + 2 \\times 10^{-8}}$$\nTo simplify, factor out $10^{-4}$ from the denominator:\n$$P(g=0/0 \\mid D) = \\frac{10^{-8}}{10^{-4} (9 + 2 \\times 10^{-4})} = \\frac{10^{-4}}{9.0002}$$\n\nFinally, we compute $Q$:\n$$Q = -10 \\log_{10} \\left( \\frac{10^{-4}}{9.0002} \\right)$$\nUsing the properties of logarithms, $\\log_{10}(a/b) = \\log_{10}(a) - \\log_{10}(b)$:\n$$Q = -10 \\left( \\log_{10}(10^{-4}) - \\log_{10}(9.0002) \\right)$$\n$$Q = -10 \\left( -4 - \\log_{10}(9.0002) \\right)$$\n$$Q = 40 + 10 \\log_{10}(9.0002)$$\nUsing a calculator, $\\log_{10}(9.0002) \\approx 0.95425143$.\n$$Q \\approx 40 + 10 \\times (0.95425143)$$\n$$Q \\approx 40 + 9.5425143$$\n$$Q \\approx 49.5425143$$\n\nThe problem requires the result to be rounded to four significant figures.\n$$Q \\approx 49.54$$",
            "answer": "$$\\boxed{49.54}$$"
        },
        {
            "introduction": "After assigning a quality score to each variant, the final challenge is to decide which calls to keep and which to discard. Applying a simple, arbitrary quality threshold is not ideal; a more statistically rigorous approach is needed to balance sensitivity and precision. This practice introduces the concept of the False Discovery Rate (FDR) and walks you through an algorithm to determine an optimal QUAL score threshold that controls the FDR at a desired level, a fundamental task in producing a high-confidence final callset. ",
            "id": "4617237",
            "problem": "You are given a set of variant calls from whole-genome analysis of Single-Nucleotide Polymorphism (SNP) and insertion and deletion (indel) candidates. Each call has an associated Phred-like variant quality score $QUAL$, and a boolean indicator of whether the call lies within independently verified truth regions used for benchmarking. The objective is to derive, from first principles, a thresholding rule on $QUAL$ that controls the False Discovery Rate (FDR) at or below a target value while maximizing retained calls, then implement it as a program that computes results for a specified test suite.\n\nFundamental base and definitions:\n- The Phred scale interprets a quality value $Q$ as a monotone transformation of error likelihood, and is widely used in genomics to rank variant calls by confidence. No specific calibration is assumed; only that higher $Q$ should not decrease the plausibility of correctness relative to lower $Q$.\n- In statistical hypothesis testing, the False Discovery Rate (FDR) is defined as the expected fraction of false discoveries among all discoveries. In a finite counting approximation appropriate for benchmarking against a truth set, define for a given $QUAL$ threshold $\\tau$:\n  - Let the dataset consist of $n$ calls indexed by $i \\in \\{1,\\dots,n\\}$.\n  - Let $Q_i$ be the $QUAL$ of call $i$.\n  - Let $B_i \\in \\{0,1\\}$ indicate membership in truth regions, with $B_i=1$ meaning the call lies inside truth regions (proxy for a true positive candidate), and $B_i=0$ meaning the call lies outside truth regions (proxy for a false positive candidate).\n  - Retain calls by the inclusive rule: a call $i$ is retained if and only if $Q_i \\ge \\tau$.\n  - Let $S(\\tau)=\\{i \\mid Q_i \\ge \\tau\\}$ be the set of retained calls.\n  - Define the retained false positives $FP(\\tau) = \\sum_{i \\in S(\\tau)} (1 - B_i)$.\n  - Define the retained true positives $TP(\\tau) = \\sum_{i \\in S(\\tau)} B_i$.\n  - Define the finite-sample FDR at threshold $\\tau$ as\n    $$\\mathrm{FDR}(\\tau) = \\begin{cases}\n    \\dfrac{FP(\\tau)}{FP(\\tau) + TP(\\tau)}  \\text{if } FP(\\tau) + TP(\\tau)  0, \\\\\n    0  \\text{if } FP(\\tau) + TP(\\tau) = 0,\n    \\end{cases}$$\n    which sets the FDR to $0$ when there are no retained calls (no discoveries implies no false discoveries).\n- The optimal threshold under a target FDR $t$ is the minimal $\\tau$ that satisfies $\\mathrm{FDR}(\\tau) \\le t$, using the inclusive $QUAL$ filter $Q_i \\ge \\tau$. Minimality promotes sensitivity by retaining as many calls as possible subject to the FDR constraint.\n\nAlgorithmic requirements:\n- Consider candidate thresholds given by the sorted unique values of $\\{Q_i\\}$, denoted $\\tau \\in \\mathcal{T} = \\{\\text{sorted unique }Q_i\\}$. Under the inclusive rule $Q_i \\ge \\tau$, these candidates enumerate all distinct retained sets $S(\\tau)$.\n- If no $\\tau \\in \\mathcal{T}$ satisfies $\\mathrm{FDR}(\\tau) \\le t$, define a fallback threshold\n  $$\\tau^\\mathrm{fallback} = \\max_i(Q_i) + \\epsilon,$$\n  where $\\epsilon = 10^{-6}$, which yields $S(\\tau^\\mathrm{fallback}) = \\emptyset$ and thus $\\mathrm{FDR}(\\tau^\\mathrm{fallback}) = 0$ with $0$ retained calls.\n- The optimal threshold is\n  $$\\tau^\\ast = \\begin{cases}\n  \\min\\{\\tau \\in \\mathcal{T} \\mid \\mathrm{FDR}(\\tau) \\le t\\}  \\text{if the set is nonempty}, \\\\\n  \\tau^\\mathrm{fallback}  \\text{otherwise}.\n  \\end{cases}$$\n\nTask:\n- Implement a program that, for each test case, computes $\\tau^\\ast$, $\\mathrm{FDR}(\\tau^\\ast)$, and the retained call count $\\lvert S(\\tau^\\ast) \\rvert$.\n- Use the inclusive filter $Q_i \\ge \\tau$ exactly as defined above.\n- Round $\\tau^\\ast$ and $\\mathrm{FDR}(\\tau^\\ast)$ to $6$ decimal places; the retained call count must be an integer.\n\nTest suite:\nEvaluate your program on the following test cases. Each test case is specified by a list of $QUAL$ values, a corresponding list of truth-region indicators $B_i$, and a target FDR $t$ (expressed as a decimal fraction).\n\n- Case $1$ (general nonmonotone scenario):\n  - $QUAL$: $\\{12,18,35,42,60,75,90,25,55,40\\}$\n  - $B$: $\\{0,0,1,1,1,1,0,1,1,0\\}$\n  - Target $t$: $0.3$\n- Case $2$ (all retained calls are inside truth regions):\n  - $QUAL$: $\\{5,10,20,40\\}$\n  - $B$: $\\{1,1,1,1\\}$\n  - Target $t$: $0.0$\n- Case $3$ (no acceptable threshold except excluding all calls):\n  - $QUAL$: $\\{30,60,90\\}$\n  - $B$: $\\{0,0,0\\}$\n  - Target $t$: $0.0$\n- Case $4$ (ties at the threshold, inclusive retention):\n  - $QUAL$: $\\{20,20,50,50,50\\}$\n  - $B$: $\\{0,1,0,1,1\\}$\n  - Target $t$: $0.35$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of the form $[\\tau^\\ast,\\mathrm{FDR}(\\tau^\\ast),\\lvert S(\\tau^\\ast) \\rvert]$, with $\\tau^\\ast$ and $\\mathrm{FDR}(\\tau^\\ast)$ rounded to $6$ decimal places and $\\lvert S(\\tau^\\ast) \\rvert$ as an integer.\n- For example, the overall output should look like\n  $$[[\\tau^\\ast_1,\\mathrm{FDR}(\\tau^\\ast_1),\\lvert S(\\tau^\\ast_1)\\rvert],[\\tau^\\ast_2,\\mathrm{FDR}(\\tau^\\ast_2),\\lvert S(\\tau^\\ast_2)\\rvert],\\dots].$$",
            "solution": "The problem requires the derivation and implementation of an algorithm to determine an optimal quality score threshold for filtering genomic variant calls. The optimization objective is to control the False Discovery Rate (FDR) at or below a specified target level, $t$, while maximizing the number of retained calls, which corresponds to maximizing sensitivity.\n\nThe provided framework is based on a finite-sample approximation of FDR, calculated using a benchmark set. Each variant call $i$ from a total of $n$ calls is characterized by a quality score $Q_i$ and a binary indicator $B_i$. $B_i=1$ signifies that the call is within a trusted \"truth\" region (a proxy for a true positive), and $B_i=0$ signifies it is outside (a proxy for a false positive). The core assumption is that $Q_i$ is a Phred-like score, meaning it is monotonically related to the confidence in the call; a higher $Q_i$ implies a more reliable call.\n\nA call $i$ is retained if its quality score $Q_i$ meets or exceeds a given threshold $\\tau$, i.e., $Q_i \\ge \\tau$. The set of retained calls is denoted by $S(\\tau) = \\{i \\mid Q_i \\ge \\tau\\}$. For this set, we can count the number of retained true positives, $TP(\\tau) = \\sum_{i \\in S(\\tau)} B_i$, and retained false positives, $FP(\\tau) = \\sum_{i \\in S(\\tau)} (1 - B_i)$. The total number of retained calls, or discoveries, is $|S(\\tau)| = TP(\\tau) + FP(\\tau)$.\n\nThe finite-sample FDR for a threshold $\\tau$ is defined as the fraction of false positives among all retained calls:\n$$\n\\mathrm{FDR}(\\tau) = \\begin{cases}\n\\dfrac{FP(\\tau)}{FP(\\tau) + TP(\\tau)}  \\text{if } FP(\\tau) + TP(\\tau)  0, \\\\\n0  \\text{if } FP(\\tau) + TP(\\tau) = 0.\n\\end{cases}\n$$\nThe case where the denominator is $0$ implies no calls are retained, hence no discoveries are made, and consequently, no false discoveries.\n\nThe goal is to find an optimal threshold $\\tau^\\ast$. To maximize the number of retained calls (sensitivity) subject to the constraint $\\mathrm{FDR}(\\tau) \\le t$, we should choose the least stringent (i.e., minimal) threshold that satisfies the condition. A lower threshold retains more calls.\n\nThe set of retained calls $S(\\tau)$ only changes when the threshold $\\tau$ crosses one of the quality score values $Q_i$. Therefore, we only need to consider the unique values of $Q_i$ as candidate thresholds. Let $\\mathcal{T}$ be the set of unique quality scores, sorted in ascending order: $\\mathcal{T} = \\{\\tau_1, \\tau_2, \\dots, \\tau_k\\}$ where $\\tau_1  \\tau_2  \\dots  \\tau_k$.\n\nThe algorithm to find the optimal threshold $\\tau^\\ast$ proceeds as follows:\n\n1.  **Identify Candidate Thresholds**: Extract all unique quality scores $\\{Q_i\\}$ from the dataset and sort them in ascending order to form the set of candidate thresholds, $\\mathcal{T}$.\n\n2.  **Iterative Search**: Iterate through the candidate thresholds $\\tau_j \\in \\mathcal{T}$ starting from the smallest, $\\tau_1$. For each $\\tau_j$:\n    a.  Define the set of retained calls $S(\\tau_j) = \\{i \\mid Q_i \\ge \\tau_j\\}$. Note that due to the inclusive inequality, all calls with quality equal to $\\tau_j$ are included.\n    b.  Calculate the number of retained false positives, $FP(\\tau_j) = \\sum_{i \\in S(\\tau_j)} (1-B_i)$, and the total number of retained calls, $|S(\\tau_j)|$.\n    c.  Compute the FDR, $\\mathrm{FDR}(\\tau_j)$, using the definition above.\n    d.  Check if the FDR constraint is met: $\\mathrm{FDR}(\\tau_j) \\le t$.\n    e.  If the constraint is satisfied, we have found a valid threshold. Since we are iterating from the smallest $\\tau_j$ upwards, this first valid threshold is the minimal one. We set $\\tau^\\ast = \\tau_j$ and the search is complete. The corresponding values $\\mathrm{FDR}(\\tau^\\ast)$ and $|S(\\tau^\\ast)|$ are recorded.\n\n3.  **Fallback Mechanism**: If the iteration completes and no threshold $\\tau_j \\in \\mathcal{T}$ satisfies the condition $\\mathrm{FDR}(\\tau_j) \\le t$, it implies that no non-trivial filtering can meet the target FDR. In this case, the problem specifies a fallback rule to ensure the FDR constraint is strictly met, albeit at the complete loss of sensitivity. The threshold is set to a value guaranteed to reject all calls:\n    $$ \\tau^\\ast = \\tau^\\mathrm{fallback} = \\max_i(Q_i) + \\epsilon $$\n    where $\\epsilon$ is a small positive constant, given as $10^{-6}$. For this threshold, the set of retained calls $S(\\tau^\\ast)$ is empty. Consequently, $|S(\\tau^\\ast)| = 0$ and, by definition, $\\mathrm{FDR}(\\tau^\\ast) = 0$.\n\nThis procedure is guaranteed to find a unique optimal threshold $\\tau^\\ast$ for any given dataset and target FDR $t$. The implementation will involve sorting the unique quality scores and then iterating through them to perform the calculations as described. For computational efficiency, especially with large datasets, pre-sorting the calls by quality score and using cumulative sums of $B_i$ and $(1-B_i)$ would be more performant than re-scanning the entire dataset for each threshold. However, for the specified test cases, a direct implementation of the above logic is sufficient.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal quality threshold based on FDR control for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"QUAL\": [12, 18, 35, 42, 60, 75, 90, 25, 55, 40],\n            \"B\": [0, 0, 1, 1, 1, 1, 0, 1, 1, 0],\n            \"t\": 0.3\n        },\n        {\n            \"QUAL\": [5, 10, 20, 40],\n            \"B\": [1, 1, 1, 1],\n            \"t\": 0.0\n        },\n        {\n            \"QUAL\": [30, 60, 90],\n            \"B\": [0, 0, 0],\n            \"t\": 0.0\n        },\n        {\n            \"QUAL\": [20, 20, 50, 50, 50],\n            \"B\": [0, 1, 0, 1, 1],\n            \"t\": 0.35\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        quals = np.array(case[\"QUAL\"], dtype=float)\n        bs = np.array(case[\"B\"], dtype=int)\n        target_fdr = case[\"t\"]\n        \n        # Handle cases with no calls\n        if quals.size == 0:\n            # tau_fallback = 1e-6 as max would fail\n            # By definition, |S|=0, FDR=0.\n            # The exact tau doesn't matter much as long as it's positive.\n            # Using 1e-6 aligns with the epsilon definition.\n            all_results.append([1.0e-6, 0.0, 0])\n            continue\n\n        candidate_thresholds = np.unique(quals) # Already sorted in ascending order\n\n        found_solution = False\n        for tau in candidate_thresholds:\n            # Inclusive filter: retain if quality is greater than or equal to tau\n            retained_mask = quals >= tau\n            \n            retained_count = np.sum(retained_mask)\n\n            if retained_count == 0:\n                fdr = 0.0\n            else:\n                # B=0 are false positives, B=1 are true positives\n                # fp_count = sum of (1-B) for retained calls which is sum of (B==0)\n                retained_bs = bs[retained_mask]\n                fp_count = np.sum(retained_bs == 0)\n                fdr = fp_count / retained_count\n\n            if fdr = target_fdr:\n                opt_tau = tau\n                opt_fdr = fdr\n                opt_count = int(retained_count)\n                found_solution = True\n                break\n\n        if not found_solution:\n            # Fallback case: no threshold met the criteria\n            epsilon = 1e-6\n            opt_tau = np.max(quals) + epsilon\n            opt_fdr = 0.0\n            opt_count = 0\n\n        all_results.append([opt_tau, opt_fdr, opt_count])\n\n    # Format the final output string exactly as required\n    formatted_results = [\n        f\"[{r[0]:.6f},{r[1]:.6f},{r[2]}]\" for r in all_results\n    ]\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}