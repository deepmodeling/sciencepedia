{
    "hands_on_practices": [
        {
            "introduction": "Accurate quantification begins with accurate counting. Unique Molecular Identifiers (UMIs) are crucial for removing amplification bias in RNA-seq experiments, but they are not immune to sequencing errors, which can falsely inflate the number of unique molecules observed. This practice guides you through implementing a widely-used, graph-based deduplication algorithm to correct these errors, providing a hands-on understanding of how a practical heuristic is justified by a statistical error model .",
            "id": "4614673",
            "problem": "You are given observed counts of Unique Molecular Identifier (UMI) sequences associated with Deoxyribonucleic Acid (DNA) or Ribonucleic Acid (RNA) fragments in a transcript abundance quantification setting. The sequences are strings over the alphabet $\\{A,C,G,T\\}$ of fixed length $L$. Assume the standard independent per-base substitution error model: each base is sequenced independently with error probability $\\epsilon$, and when an error occurs, it is uniformly miscalled to one of the other three nucleotides. Let the Hamming distance between two sequences be the count of positions at which they differ.\n\nStarting from the Central Dogma of Molecular Biology and the standard experimental design in which each original molecule is tagged with a UMI prior to amplification and sequencing, it is known that synthetic UMI sequences are intended to be unique per original molecule; however, polymerase and sequencing introduce errors. You must construct and implement an adjacency-based UMI deduplication algorithm that uses the following rules:\n\n- Build an undirected adjacency graph $G$ whose nodes are the observed UMI sequences. Add an edge between two nodes if their Hamming distance is exactly $1$.\n- Process nodes in descending order of observed count (breaking ties deterministically by lexicographic order on the sequence). When visiting a node $u$, if $u$ has not already been removed, declare $u$ to be a representative of a unique original molecule. Then, for each neighbor $v$ of $u$, if the count of $u$ is strictly greater than the count of $v$, remove $v$ (merge $v$ into $u$). If counts are equal, do not remove either on the basis of that adjacency during this step.\n- Continue until all nodes have been visited. The deduplicated count for the set is the number of representatives declared by this procedure.\n\nJustify, from the independent per-base error model, why merging on Hamming distance $1$ with directed adjacency from higher count to lower count is statistically appropriate when $\\epsilon$ is small, and explain why ties should not be merged.\n\nYour program must implement this algorithm and apply it to the following test suite. Each test case specifies $(L,\\epsilon,\\text{UMI\\_counts})$, where $\\text{UMI\\_counts}$ is a list of $(\\text{sequence},\\text{count})$ pairs. For each case, compute the deduplicated count as an integer. There are no physical units involved. The error probability values are to be interpreted as decimals, not percentages.\n\nTest suite:\n- Case $1$: $L=8$, $\\epsilon=0.01$, $\\text{UMI\\_counts}=[(\\text{\"ACGTACGT\"},10),(\\text{\"ACGTACGA\"},2),(\\text{\"TCGTACGT\"},1),(\\text{\"GGGGGGGG\"},5)]$.\n- Case $2$: $L=4$, $\\epsilon=0.01$, $\\text{UMI\\_counts}=[(\\text{\"ACGT\"},5),(\\text{\"ACGG\"},5),(\\text{\"ACGA\"},1)]$.\n- Case $3$: $L=4$, $\\epsilon=0.01$, $\\text{UMI\\_counts}=[(\\text{\"AAAA\"},8),(\\text{\"AAAT\"},3),(\\text{\"AATT\"},1)]$.\n- Case $4$: $L=1$, $\\epsilon=0.01$, $\\text{UMI\\_counts}=[(\\text{\"A\"},4),(\\text{\"C\"},1),(\\text{\"G\"},1)]$.\n- Case $5$: $L=4$, $\\epsilon=0.01$, $\\text{UMI\\_counts}=[(\\text{\"TTTT\"},20),(\\text{\"TTTA\"},3),(\\text{\"TTAT\"},2),(\\text{\"TATT\"},1),(\\text{\"ATTT\"},1)]$.\n- Case $6$: $L=4$, $\\epsilon=0.01$, $\\text{UMI\\_counts}=[(\\text{\"CATC\"},6),(\\text{\"CATT\"},6),(\\text{\"CCTC\"},1),(\\text{\"GATC\"},1)]$.\n\nYour program should produce a single line of output containing the deduplicated counts for cases $1$ through $6$ as a comma-separated list enclosed in square brackets (e.g., $\\text{\"[r_1,r_2,r_3,r_4,r_5,r_6]\"}$), where each $r_i$ is an integer result corresponding to case $i$ in order.",
            "solution": "The problem requires the implementation and justification of a specific adjacency-based algorithm for Unique Molecular Identifier (UMI) deduplication. This process is crucial in transcript abundance quantification to correct for artifacts introduced during laboratory procedures, particularly polymerase chain reaction (PCR) amplification and high-throughput sequencing.\n\nThe core principle is to distinguish true, distinct UMI sequences, each representing an original molecule, from erroneous copies generated by sequencing errors. The provided algorithm formalizes a common heuristic: an observed UMI with a low count that is a small (Hamming distance $1$) variation of a UMI with a much higher count is likely an error derived from the high-count UMI.\n\n**Statistical Justification of the Algorithm**\n\nThe problem defines an independent per-base substitution error model. Let $L$ be the length of a UMI sequence over the alphabet $\\{\\text{A,C,G,T}\\}$. Each base is sequenced with an error probability of $\\epsilon$. When an error occurs at a position, the base is uniformly miscalled as one of the other $3$ nucleotides.\n\nLet $S_{true}$ be the sequence of a true UMI molecule. The probability of sequencing any single base correctly is $1-\\epsilon$. The probability of it being substituted by a specific one of the other $3$ bases is $\\epsilon/3$.\nThe probability of observing the sequence $S_{true}$ perfectly, $P(S_{true} | S_{true})$, is the probability of $L$ independent correct base calls:\n$$P(S_{true} | S_{true}) = (1-\\epsilon)^L$$\nNow, consider another sequence $S_{error}$ that differs from $S_{true}$ by a Hamming distance of $h=1$. The probability of observing this specific sequence $S_{error}$ originating from $S_{true}$, $P(S_{error} | S_{true})$, requires one specific position to be miscalled to one specific base (probability $\\epsilon/3$) and the other $L-1$ positions to be correct (probability $1-\\epsilon$ each):\n$$P(S_{error} | S_{true}) \\text{ for } d(S_{true}, S_{error})=1 \\text{ is } (1-\\epsilon)^{L-1} \\left(\\frac{\\epsilon}{3}\\right)^1$$\nTo justify the algorithm's main rule, we compare the likelihood of observing the true sequence versus a single-error variant. The ratio of these probabilities is:\n$$\\frac{P(S_{error} | S_{true})}{P(S_{true} | S_{true})} = \\frac{(1-\\epsilon)^{L-1} (\\epsilon/3)}{(1-\\epsilon)^L} = \\frac{\\epsilon}{3(1-\\epsilon)}$$\nFor a small error probability such as the given $\\epsilon=0.01$, this ratio is approximately $\\epsilon/3$:\n$$\\frac{0.01}{3(1-0.01)} = \\frac{0.01}{3 \\times 0.99} = \\frac{0.01}{2.97} \\approx 0.003367$$\nThis very small ratio indicates that for each copy of an original molecule, a read corresponding to a single-error variant is more than $290$ times less likely to be generated than a read corresponding to the correct sequence. Consequently, if a true UMI is amplified and sequenced many times, the expected count of the true sequence will be substantially greater than the count of any of its Hamming-distance-$1$ neighbors that arise from sequencing errors. Therefore, the rule to merge a node $v$ into a neighbor $u$ if $\\text{count}(u) > \\text{count}(v)$ is statistically sound. It identifies $v$ as a probable error derived from the more abundant sequence $u$.\n\nThe rule to not merge when counts are equal, i.e., $\\text{count}(u) = \\text{count}(v)$, is also justified. Given the large disparity in probabilities, observing equal counts for two sequences at Hamming distance $1$ is statistically improbable if one is an error artifact of the other, especially for moderate to high counts. A more parsimonious explanation is that $u$ and $v$ represent two distinct original molecules that happened to be present at similar initial concentrations. Merging them would risk collapsing two true biological signals into one. Thus, the conservative action of not merging ties prevents false positives in deduplication at the cost of potentially allowing some rare error-derived UMIs to be counted as real.\n\n**Algorithmic Procedure**\n\nThe algorithm proceeds as follows:\n$1$. **Initialization**: The input is a list of (sequence, count) pairs. A dictionary is used to store the count for each unique UMI, and a set is initialized to keep track of nodes (UMIs) that have been removed.\n\n$2$. **Graph Construction**: An undirected graph $G$ is constructed. The nodes of $G$ are the unique UMI sequences. An edge is created between any two nodes $u$ and $v$ if and only if their Hamming distance, $d(u,v)$, is exactly $1$. An adjacency list representation is suitable for this purpose.\n\n$3$. **Node Ordering**: All UMI nodes are sorted to create a deterministic processing order. The primary sorting key is the UMI's observed count in descending order. The secondary sorting key is the UMI sequence itself in lexicographical (alphabetical) order, which serves to break ties in counts.\n\n$4$. **Iterative Deduplication**: The sorted nodes are processed one by one. For each node $u$:\n    a. If $u$ is already in the set of removed nodes, it is skipped.\n    b. If $u$ has not been removed, it is declared a representative of a true molecule, and the total count of representatives is incremented.\n    c. Subsequently, all neighbors $v$ of $u$ in the graph are examined. If the count of the current node $u$ is strictly greater than the count of a neighbor $v$ (i.e., $\\text{count}(u) > \\text{count}(v)$), then $v$ is added to the set of removed nodes.\n\n$5$. **Final Count**: After iterating through all nodes, the final count of declared representatives is the result of the deduplication. This count represents the estimated number of unique original molecules.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np # numpy is permitted but not strictly needed for this implementation\n\ndef solve():\n    \"\"\"\n    Solves the UMI deduplication problem for the given test suite.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        (8, 0.01, [(\"ACGTACGT\", 10), (\"ACGTACGA\", 2), (\"TCGTACGT\", 1), (\"GGGGGGGG\", 5)]),\n        # Case 2\n        (4, 0.01, [(\"ACGT\", 5), (\"ACGG\", 5), (\"ACGA\", 1)]),\n        # Case 3\n        (4, 0.01, [(\"AAAA\", 8), (\"AAAT\", 3), (\"AATT\", 1)]),\n        # Case 4\n        (1, 0.01, [(\"A\", 4), (\"C\", 1), (\"G\", 1)]),\n        # Case 5\n        (4, 0.01, [(\"TTTT\", 20), (\"TTTA\", 3), (\"TTAT\", 2), (\"TATT\", 1), (\"ATTT\", 1)]),\n        # Case 6\n        (4, 0.01, [(\"CATC\", 6), (\"CATT\", 6), (\"CCTC\", 1), (\"GATC\", 1)]),\n    ]\n\n    def hamming_distance(s1, s2):\n        \"\"\"Calculates the Hamming distance between two strings of equal length.\"\"\"\n        return sum(c1 != c2 for c1, c2 in zip(s1, s2))\n\n    results = []\n    for case_num, (L, epsilon, umi_counts_list) in enumerate(test_cases):\n        \n        # Step 1: Parse input and get unique UMIs\n        counts = {seq: count for seq, count in umi_counts_list}\n        unique_umis = list(counts.keys())\n        \n        # Step 2: Build the adjacency graph\n        adj_list = {umi: [] for umi in unique_umis}\n        for i in range(len(unique_umis)):\n            for j in range(i + 1, len(unique_umis)):\n                umi1 = unique_umis[i]\n                umi2 = unique_umis[j]\n                if hamming_distance(umi1, umi2) == 1:\n                    adj_list[umi1].append(umi2)\n                    adj_list[umi2].append(umi1)\n\n        # Step 3: Process nodes in descending order of count, with lexicographic tie-breaking\n        # The key for sorting is a tuple: (-count, sequence).\n        # The negative count achieves descending order for counts, and the sequence achieves\n        # ascending (lexicographical) order for tie-breaking.\n        sorted_umis = sorted(unique_umis, key=lambda umi: (-counts[umi], umi))\n        \n        # Step 4: Iterative deduplication\n        removed_nodes = set()\n        representative_count = 0\n        \n        for u in sorted_umis:\n            # If a node has been removed, it cannot be a representative\n            if u in removed_nodes:\n                continue\n            \n            # This node is a representative\n            representative_count += 1\n            \n            # Remove neighbors with lower counts\n            for v in adj_list[u]:\n                if counts[u] > counts[v]:\n                    removed_nodes.add(v)\n        \n        results.append(representative_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "With reliable counts of molecular fragments, the next challenge is to assign them to their parent transcripts, especially when a single fragment is compatible with multiple isoforms. This exercise breaks the problem down to its core statistical foundation by tasking you with deriving transcript abundances directly using Maximum Likelihood Estimation (MLE) for a simple, idealized transcriptome. By working from first principles, you will build a solid conceptual understanding of the link between observed fragment counts in equivalence classes and the underlying transcript proportions we aim to estimate .",
            "id": "4614663",
            "problem": "A researcher is quantifying transcript abundance from Ribonucleic Acid Sequencing (RNA-Seq) data in a small toy transcriptome comprising three transcripts, labeled $T_{1}$, $T_{2}$, and $T_{3}$. The goal is to compute the maximum likelihood estimates of the transcript fractions $\\pi = (\\pi_{1}, \\pi_{2}, \\pi_{3})$, where $\\pi_{i}$ denotes the probability that a randomly sampled fragment originated from transcript $T_{i}$, subject to $\\sum_{i=1}^{3} \\pi_{i} = 1$ and $\\pi_{i} \\geq 0$.\n\nAssume the standard RNA-Seq sampling model with uniform fragment start positions across each transcriptâ€™s effective length, and that observed fragments are partitioned into equivalence classes by a pseudoalignment procedure. The three equivalence classes are:\n- $E_{1}$: fragments that pseudoalign uniquely to $T_{1}$,\n- $E_{2}$: fragments that pseudoalign ambiguously to $\\{T_{2}, T_{3}\\}$,\n- $E_{3}$: fragments that pseudoalign uniquely to $T_{2}$.\n\nThe effective lengths (in nucleotides) of the transcripts are provided as:\n- $\\tilde{\\ell}_{1} = 900$,\n- $\\tilde{\\ell}_{2} = 600$,\n- $\\tilde{\\ell}_{3} = 300$.\n\nThe compatibility footprints (in nucleotides) defining how much of each transcript contributes reads to each equivalence class are:\n- From $T_{1}$: $900$ to $E_{1}$, $0$ to $E_{2}$, $0$ to $E_{3}$,\n- From $T_{2}$: $0$ to $E_{1}$, $180$ to $E_{2}$, $420$ to $E_{3}$,\n- From $T_{3}$: $0$ to $E_{1}$, $300$ to $E_{2}$, $0$ to $E_{3}$.\n\nA library of $N = 1000$ fragments is observed, with equivalence class counts:\n- $c_{E_{1}} = 400$,\n- $c_{E_{2}} = 500$,\n- $c_{E_{3}} = 100$.\n\nUsing only first-principles reasoning from the stated sampling assumptions and data, compute the maximum likelihood estimates of the transcript fractions $\\pi = (\\pi_{1}, \\pi_{2}, \\pi_{3})$. Round your final numerical values to four significant figures. Express the result as dimensionless probabilities.",
            "solution": "The problem is to determine the maximum likelihood estimates (MLE) for the transcript fractions $\\pi = (\\pi_{1}, \\pi_{2}, \\pi_{3})$, which represent the probabilities that a randomly sampled fragment originates from transcripts $T_{1}$, $T_{2}$, and $T_{3}$, respectively.\n\nThe problem is first validated.\n**Step 1: Extract Givens**\n- Transcript set: $\\{T_{1}, T_{2}, T_{3}\\}$\n- Parameters to estimate: $\\pi = (\\pi_{1}, \\pi_{2}, \\pi_{3})$ with constraints $\\sum_{i=1}^{3} \\pi_{i} = 1$ and $\\pi_{i} \\geq 0$.\n- Equivalence classes: $E_{1}$ (unique to $T_{1}$), $E_{2}$ (ambiguous to $\\{T_{2}, T_{3}\\}$), $E_{3}$ (unique to $T_{2}$).\n- Effective transcript lengths: $\\tilde{\\ell}_{1} = 900$, $\\tilde{\\ell}_{2} = 600$, $\\tilde{\\ell}_{3} = 300$.\n- Compatibility footprints $f_{ij}$ (length of transcript $i$ compatible with class $j$):\n  - $T_1$: $f_{11}=900$, $f_{12}=0$, $f_{13}=0$.\n  - $T_2$: $f_{21}=0$, $f_{22}=180$, $f_{23}=420$.\n  - $T_3$: $f_{31}=0$, $f_{32}=300$, $f_{33}=0$.\n- Total observed fragments: $N = 1000$.\n- Equivalence class counts: $c_{E_{1}} = 400$, $c_{E_{2}} = 500$, $c_{E_{3}} = 100$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, situated within the standard framework of transcript abundance quantification in bioinformatics. All terms are well-defined in this context. The provided data is self-contained and internally consistent; for each transcript $T_i$, the sum of its compatibility footprints equals its effective length ($\\sum_{j} f_{ij} = \\tilde{\\ell}_i$). The problem is a well-posed maximum likelihood estimation task. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation of the Maximum Likelihood Estimates**\n\nThe core of the problem is to construct the likelihood function for the observed equivalence class counts and maximize it with respect to the transcript fractions $\\pi$.\n\nFirst, we determine the conditional probability, $p(E_{j} | T_{i})$, that a fragment originating from transcript $T_{i}$ is classified into equivalence class $E_{j}$. This probability is the ratio of the compatibility footprint $f_{ij}$ to the transcript's effective length $\\tilde{\\ell}_{i}$:\n$$p(E_{j} | T_{i}) = \\frac{f_{ij}}{\\tilde{\\ell}_{i}}$$\nUsing the provided data:\n- For $T_{1}$: $p(E_{1} | T_{1}) = \\frac{900}{900} = 1$; $p(E_{2} | T_{1}) = 0$; $p(E_{3} | T_{1}) = 0$.\n- For $T_{2}$: $p(E_{1} | T_{2}) = 0$; $p(E_{2} | T_{2}) = \\frac{180}{600} = 0.3$; $p(E_{3} | T_{2}) = \\frac{420}{600} = 0.7$.\n- For $T_{3}$: $p(E_{1} | T_{3}) = 0$; $p(E_{2} | T_{3}) = \\frac{300}{300} = 1$; $p(E_{3} | T_{3}) = 0$.\n\nNext, we express the overall probability of a random fragment belonging to equivalence class $E_{j}$, $p(E_{j})$, using the law of total probability, summing over all possible originating transcripts:\n$$p(E_{j}) = \\sum_{i=1}^{3} p(E_{j} | T_{i}) \\pi_{i}$$\nThis yields:\n- $p(E_{1}) = p(E_{1} | T_{1}) \\pi_{1} + p(E_{1} | T_{2}) \\pi_{2} + p(E_{1} | T_{3}) \\pi_{3} = (1) \\pi_{1} + (0) \\pi_{2} + (0) \\pi_{3} = \\pi_{1}$\n- $p(E_{2}) = p(E_{2} | T_{1}) \\pi_{1} + p(E_{2} | T_{2}) \\pi_{2} + p(E_{2} | T_{3}) \\pi_{3} = (0) \\pi_{1} + (0.3) \\pi_{2} + (1) \\pi_{3} = 0.3 \\pi_{2} + \\pi_{3}$\n- $p(E_{3}) = p(E_{3} | T_{1}) \\pi_{1} + p(E_{3} | T_{2}) \\pi_{2} + p(E_{3} | T_{3}) \\pi_{3} = (0) \\pi_{1} + (0.7) \\pi_{2} + (0) \\pi_{3} = 0.7 \\pi_{2}$\n\nThe observed counts $(c_{E_{1}}, c_{E_{2}}, c_{E_{3}})$ follow a multinomial distribution with parameters $N=1000$ and probabilities $(p(E_{1}), p(E_{2}), p(E_{3}))$. The log-likelihood function $\\mathcal{L}(\\pi)$ of the observed data is, up to a constant term:\n$$\\mathcal{L}(\\pi) = \\sum_{j=1}^{3} c_{E_{j}} \\ln(p(E_{j}))$$\nSubstituting the expressions for $p(E_{j})$ and the given counts:\n$$\\mathcal{L}(\\pi_{1}, \\pi_{2}, \\pi_{3}) = 400 \\ln(\\pi_{1}) + 500 \\ln(0.3 \\pi_{2} + \\pi_{3}) + 100 \\ln(0.7 \\pi_{2})$$\nWe must maximize this function subject to the constraints $\\pi_{1} + \\pi_{2} + \\pi_{3} = 1$ and $\\pi_{i} \\geq 0$ for $i \\in \\{1, 2, 3\\}$.\n\nFrom the structure of $p(E_1)$, the term $400 \\ln(\\pi_1)$ is decoupled from the others. For a multinomial model, the MLE for a category probability is its observed frequency. The probability of a fragment landing in class $E_1$ is $\\pi_1$, and its observed frequency is $c_{E_1}/N$. Thus, the MLE for $\\pi_{1}$ is:\n$$\\hat{\\pi}_{1} = \\frac{c_{E_{1}}}{N} = \\frac{400}{1000} = 0.4$$\nThis simplifies the problem. The constraint on the remaining fractions becomes $\\pi_{2} + \\pi_{3} = 1 - \\hat{\\pi}_{1} = 1 - 0.4 = 0.6$. We can express $\\pi_{3}$ in terms of $\\pi_{2}$:\n$$\\pi_{3} = 0.6 - \\pi_{2}$$\nThe domain for $\\pi_{2}$ is $[0, 0.6]$. We substitute this into the part of the log-likelihood function involving $\\pi_{2}$ and $\\pi_{3}$, which we denote $\\mathcal{L}_{23}$:\n$$\\mathcal{L}_{23}(\\pi_{2}) = 500 \\ln(0.3 \\pi_{2} + (0.6 - \\pi_{2})) + 100 \\ln(0.7 \\pi_{2})$$\n$$\\mathcal{L}_{23}(\\pi_{2}) = 500 \\ln(0.6 - 0.7 \\pi_{2}) + 100 \\ln(0.7 \\pi_{2})$$\nTo find the maximum, we compute the derivative with respect to $\\pi_{2}$ and set it to zero:\n$$\\frac{d\\mathcal{L}_{23}}{d\\pi_{2}} = 500 \\cdot \\frac{-0.7}{0.6 - 0.7 \\pi_{2}} + 100 \\cdot \\frac{0.7}{0.7 \\pi_{2}} = \\frac{-350}{0.6 - 0.7 \\pi_{2}} + \\frac{100}{\\pi_{2}}$$\nSetting the derivative to zero:\n$$\\frac{100}{\\pi_{2}} = \\frac{350}{0.6 - 0.7 \\pi_{2}}$$\n$$100(0.6 - 0.7 \\pi_{2}) = 350 \\pi_{2}$$\n$$60 - 70 \\pi_{2} = 350 \\pi_{2}$$\n$$60 = 420 \\pi_{2}$$\n$$\\hat{\\pi}_{2} = \\frac{60}{420} = \\frac{1}{7}$$\nThis value is in the valid range $[0, 0.6]$. The second derivative is negative, confirming this is a maximum. Now we find the estimate for $\\pi_{3}$:\n$$\\hat{\\pi}_{3} = 0.6 - \\hat{\\pi}_{2} = \\frac{3}{5} - \\frac{1}{7} = \\frac{21 - 5}{35} = \\frac{16}{35}$$\nThe maximum likelihood estimates for the transcript fractions are $\\hat{\\pi} = (\\frac{2}{5}, \\frac{1}{7}, \\frac{16}{35})$.\n\nFinally, we convert these exact fractions to decimal values rounded to four significant figures as requested:\n- $\\hat{\\pi}_{1} = 0.4 = 0.4000$\n- $\\hat{\\pi}_{2} = \\frac{1}{7} \\approx 0.142857 \\dots \\rightarrow 0.1429$\n- $\\hat{\\pi}_{3} = \\frac{16}{35} \\approx 0.457142 \\dots \\rightarrow 0.4571$\nThe sum of the rounded values is $0.4000 + 0.1429 + 0.4571 = 1.0000$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.4000 & 0.1429 & 0.4571\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "For the complex scenarios found in real transcriptomes, direct maximization of the likelihood function is often intractable. The Expectation-Maximization (EM) algorithm is the computational workhorse that iteratively solves this problem by finding a maximum likelihood solution in the presence of latent variables. In this exercise, you will implement a single iteration of the EM algorithm, demystifying its two-step process by translating the abstract E-step (probabilistically assigning fragments) and M-step (re-estimating abundances) into functional code .",
            "id": "4614687",
            "problem": "You are given a probabilistic mixture-model setup for transcript abundance quantification in ribonucleic acid sequencing (RNA-seq). In this setting, a set of transcripts acts as mixture components with mixture proportions $\\pi_t$ that satisfy $\\sum_{t=1}^{m} \\pi_t = 1$ and $\\pi_t \\ge 0$ for all $t$. Each sequencing fragment (read) $i$ contributes nonnegative alignment weights $w_{it}$ that are proportional to the likelihood that fragment $i$ originates from transcript $t$. The Expectation-Maximization (EM) algorithm iteratively updates the mixture proportions $\\pi_t$ by alternating between computing posterior transcript-membership probabilities for each read (Expectation step) and averaging those probabilities to obtain new mixture proportions (Maximization step).\n\nFundamental base to use:\n- Probability mixture model: mixture proportions $\\pi_t$ with $\\sum_{t} \\pi_t = 1$ and $\\pi_t \\ge 0$.\n- Bayes rule: posterior membership for read $i$ in transcript $t$ is proportional to the product of prior and likelihood, namely $p(t|i) \\propto \\pi_t \\cdot p(i|t)$; here, $p(i|t)$ is represented by $w_{it}$ up to a proportional constant.\n- The updated mixture proportions are the average of the posterior membership probabilities across all included reads.\n\nImplementation requirements:\n- Implement a single EM iteration numerically using the above principles without introducing any additional heuristic corrections.\n- For each read $i$, compute the posterior membership probabilities $p(t|i)$ using $\\pi_t$ and $w_{it}$. If the normalizing denominator $\\sum_{s=1}^{m} \\pi_s \\cdot w_{is}$ equals $0$ for read $i$, treat that read as uninformative and exclude it from both the numerator and the denominator of the update (do not assign fractional membership for that read).\n- Let $n_{\\mathrm{incl}}$ be the count of included reads (those with strictly positive normalizing denominator). Update the mixture proportions to new values $\\pi_t^{(\\mathrm{new})}$ by averaging the posterior membership probabilities over the included reads.\n- Verify that the updated mixture proportions satisfy $\\sum_{t=1}^{m} \\pi_t^{(\\mathrm{new})} = 1$ within a numerical tolerance of $10^{-12}$, and report a boolean for each test indicating whether this normalization holds.\n- If $n_{\\mathrm{incl}} = 0$ (all reads are excluded), define $\\pi_t^{(\\mathrm{new})} := \\pi_t$ and perform the normalization check on the unchanged values.\n\nYour program must implement the above for the following test suite and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this top-level list corresponds to one test case and must itself be a two-element list of the form $[\\text{updated\\_pi}, \\text{normalization\\_boolean}]$, where $\\text{updated\\_pi}$ is a list of floats (the updated $\\pi_t^{(\\mathrm{new})}$ values in transcript index order) and $\\text{normalization\\_boolean}$ is a boolean.\n\nTest suite:\n- Test case $1$ (happy path, mixed ambiguity):\n    - Transcripts $m = 3$.\n    - Reads $n = 4$.\n    - Initial mixture proportions $\\pi = [\\,0.5,\\,0.3,\\,0.2\\,]$.\n    - Alignment weights $w$ as rows by reads:\n        - Read $1$: $[\\,0.9,\\,0.1,\\,0.0\\,]$.\n        - Read $2$: $[\\,0.4,\\,0.4,\\,0.2\\,]$.\n        - Read $3$: $[\\,0.0,\\,0.5,\\,0.5\\,]$.\n        - Read $4$: $[\\,0.0,\\,0.0,\\,1.0\\,]$.\n- Test case $2$ (boundary: one read unmapped, deterministic reads):\n    - Transcripts $m = 2$.\n    - Reads $n = 3$.\n    - Initial mixture proportions $\\pi = [\\,0.6,\\,0.4\\,]$.\n    - Alignment weights $w$:\n        - Read $1$: $[\\,0.0,\\,0.0\\,]$.\n        - Read $2$: $[\\,1.0,\\,0.0\\,]$.\n        - Read $3$: $[\\,0.0,\\,1.0\\,]$.\n- Test case $3$ (edge: zero initial mixture weight for one transcript):\n    - Transcripts $m = 3$.\n    - Reads $n = 3$.\n    - Initial mixture proportions $\\pi = [\\,0.0,\\,0.7,\\,0.3\\,]$.\n    - Alignment weights $w$:\n        - Read $1$: $[\\,1.0,\\,0.0,\\,0.0\\,]$.\n        - Read $2$: $[\\,0.4,\\,0.6,\\,0.0\\,]$.\n        - Read $3$: $[\\,0.4,\\,0.0,\\,0.6\\,]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[\\,\\text{updated\\_pi},\\,\\text{normalization\\_boolean}\\,]$ for the three test cases in order. For example, the printed structure must look like $[[\\ldots],\\ldots]$ with exactly one top-level list containing three inner two-element lists.",
            "solution": "The user has provided a problem statement that requires the implementation of a single iteration of the Expectation-Maximization (EM) algorithm for a probabilistic mixture model used in transcript abundance quantification.\n\n### Step 1: Extract Givens\n\nThe provided information consists of:\n- **Model**: A probabilistic mixture model with $m$ transcripts as components.\n- **Mixture Proportions**: $\\pi_t$ for transcript $t$, where $\\sum_{t=1}^{m} \\pi_t = 1$ and $\\pi_t \\ge 0$.\n- **Data**: A set of $n$ sequencing fragments (reads).\n- **Alignment Weights**: Nonnegative weights $w_{it}$ proportional to the likelihood that read $i$ originates from transcript $t$.\n- **EM Algorithm Step (E-step)**: The posterior membership probability $p(t|i)$ is derived from Bayes' rule: $p(t|i) \\propto \\pi_t \\cdot p(i|t)$, where $p(i|t)$ is proportional to $w_{it}$.\n- **EM Algorithm Step (M-step)**: The updated mixture proportions $\\pi_t^{(\\mathrm{new})}$ are the average of the posterior membership probabilities over a set of included reads.\n- **Inclusion Criterion**: A read $i$ is included if its normalizing denominator, $\\sum_{s=1}^{m} \\pi_s \\cdot w_{is}$, is strictly greater than $0$. Let $n_{\\mathrm{incl}}$ be the number of included reads.\n- **Update Rule**: If $n_{\\mathrm{incl}} > 0$, $\\pi_t^{(\\mathrm{new})} = \\frac{1}{n_{\\mathrm{incl}}} \\sum_{i \\in \\text{included reads}} p(t|i)$.\n- **Edge Case (No Included Reads)**: If $n_{\\mathrm{incl}} = 0$, the updated proportions are set to the initial proportions: $\\pi_t^{(\\mathrm{new})} := \\pi_t$.\n- **Verification**: Check if the updated proportions sum to $1$ within a numerical tolerance of $10^{-12}$: $|\\sum_{t=1}^{m} \\pi_t^{(\\mathrm{new})} - 1| \\le 10^{-12}$.\n- **Test Suite**:\n    - **Test case 1**: $m = 3$, $n = 4$, $\\pi = [\\,0.5,\\,0.3,\\,0.2\\,]$, $w = [[\\,0.9,\\,0.1,\\,0.0\\,], [\\,0.4,\\,0.4,\\,0.2\\,], [\\,0.0,\\,0.5,\\,0.5\\,], [\\,0.0,\\,0.0,\\,1.0\\,]]$.\n    - **Test case 2**: $m = 2$, $n = 3$, $\\pi = [\\,0.6,\\,0.4\\,]$, $w = [[\\,0.0,\\,0.0\\,], [\\,1.0,\\,0.0\\,], [\\,0.0,\\,1.0\\,]]$.\n    - **Test case 3**: $m = 3$, $n = 3$, $\\pi = [\\,0.0,\\,0.7,\\,0.3\\,]$, $w = [[\\,1.0,\\,0.0,\\,0.0\\,], [\\,0.4,\\,0.6,\\,0.0\\,], [\\,0.4,\\,0.0,\\,0.6\\,]]$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem describes a simplified but standard application of the Expectation-Maximization algorithm to a finite mixture model. This technique is fundamental in statistics and is widely used in bioinformatics for problems like transcript quantification. The formulation using Bayes' rule for the E-step and averaging posteriors for the M-step is correct for this model.\n- **Well-Posed**: The problem is well-posed. It provides all necessary inputs (initial proportions $\\pi$, alignment weights $w$) and a clear, deterministic procedure for calculating the output. The rules for handling edge cases (denominator of $0$, no includable reads) are explicitly defined, ensuring a unique solution exists for any valid input.\n- **Objective**: The problem is stated in precise, objective mathematical and computational terms. There are no subjective or ambiguous statements.\n\nThe problem statement does not violate any of the specified invalidity criteria. It is scientifically sound, formally specified, complete, and computationally feasible.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Design and Solution\n\nThe task is to implement a single iteration of the Expectation-Maximization (EM) algorithm to update transcript abundance estimates, represented by mixture proportions $\\pi_t$. The algorithm consists of two steps: the Expectation (E) step and the Maximization (M) step.\n\nLet $\\pi = (\\pi_1, \\pi_2, \\dots, \\pi_m)$ be the vector of initial mixture proportions for $m$ transcripts. Let $W$ be the $n \\times m$ matrix of alignment weights, where $W_{it} = w_{it}$ is proportional to the likelihood of observing read $i$ given it originated from transcript $t$.\n\n**Expectation (E) Step: Compute Posterior Probabilities**\n\nFor each read $i$ (from $i=1$ to $n$), we compute the posterior probability that it originated from transcript $t$ (for $t=1$ to $m$), given the current estimates of $\\pi_t$. Using Bayes' rule, the posterior $p(t|i)$ is proportional to the prior $\\pi_t$ times the likelihood $p(i|t)$, which is represented by $w_{it}$.\n\n$p(t|i) = \\frac{p(t) p(i|t)}{\\sum_{s=1}^{m} p(s) p(i|s)} \\propto \\frac{\\pi_t \\cdot w_{it}}{\\sum_{s=1}^{m} \\pi_s \\cdot w_{is}}$\n\nLet $Z_i = \\sum_{s=1}^{m} \\pi_s \\cdot w_{is}$ be the normalizing constant for read $i$. This term represents the marginal likelihood of observing read $i$, summed over all possible transcript origins.\n\nA crucial part of the procedure is handling reads that cannot be probabilistically assigned. If $Z_i=0$, it implies that for every transcript $s$, either its initial proportion $\\pi_s$ is $0$ or its alignment weight $w_{is}$ is $0$. Such a read provides no information for updating the proportions and must be excluded from the M-step calculation. Reads for which $Z_i > 0$ are considered \"included\". Let the set of included reads be denoted by $\\mathcal{I}$. The number of included reads is $n_{\\mathrm{incl}} = |\\mathcal{I}|$.\n\nFor each included read $i \\in \\mathcal{I}$, the posterior probability is:\n$$ p(t|i) = \\frac{\\pi_t \\cdot w_{it}}{Z_i} $$\nFor excluded reads ($i \\notin \\mathcal{I}$), these posteriors are undefined.\n\n**Maximization (M) Step: Update Mixture Proportions**\n\nThe M-step updates the mixture proportions $\\pi_t$ to new values $\\pi_t^{(\\mathrm{new})}$ by maximizing the expected log-likelihood. For this mixture model, this simplifies to averaging the posterior probabilities (responsibilities) over the included reads.\n\nIf $n_{\\mathrm{incl}} > 0$, the update rule is:\n$$ \\pi_t^{(\\mathrm{new})} = \\frac{1}{n_{\\mathrm{incl}}} \\sum_{i \\in \\mathcal{I}} p(t|i) $$\nThis calculation intuitively re-estimates the proportion of a transcript as the average fraction of reads assigned to it.\n\nIf $n_{\\mathrm{incl}} = 0$, no reads can inform an update. The problem specifies that in this case, the proportions remain unchanged:\n$$ \\pi_t^{(\\mathrm{new})} = \\pi_t $$\n\n**Normalization Check**\n\nA fundamental property of the EM algorithm for mixture models is that if the initial proportions $\\pi_t$ sum to $1$, the updated proportions $\\pi_t^{(\\mathrm{new})}$ will also automatically sum to $1$ (barring floating-point inaccuracies). We can prove this:\n$$ \\sum_{t=1}^{m} \\pi_t^{(\\mathrm{new})} = \\sum_{t=1}^{m} \\frac{1}{n_{\\mathrm{incl}}} \\sum_{i \\in \\mathcal{I}} p(t|i) = \\frac{1}{n_{\\mathrm{incl}}} \\sum_{i \\in \\mathcal{I}} \\sum_{t=1}^{m} p(t|i) $$\nSince $\\sum_{t=1}^{m} p(t|i) = \\sum_{t=1}^{m} \\frac{\\pi_t w_{it}}{Z_i} = \\frac{1}{Z_i} \\sum_{t=1}^{m} \\pi_t w_{it} = \\frac{Z_i}{Z_i} = 1$ for any included read $i$, the sum becomes:\n$$ \\sum_{t=1}^{m} \\pi_t^{(\\mathrm{new})} = \\frac{1}{n_{\\mathrm{incl}}} \\sum_{i \\in \\mathcal{I}} 1 = \\frac{n_{\\mathrm{incl}}}{n_{\\mathrm{incl}}} = 1 $$\nWe must verify this property numerically by checking if $|\\sum_{t=1}^{m} \\pi_t^{(\\mathrm{new})} - 1| \\le 10^{-12}$.\n\n**Implementation Using Matrix Operations**\n\nThe process can be efficiently implemented using vector and matrix operations.\n1.  Represent initial proportions as a $1 \\times m$ vector $\\boldsymbol{\\pi}$ and weights as an $n \\times m$ matrix $W$.\n2.  Compute the numerators for all reads and transcripts simultaneously: $N = \\boldsymbol{\\pi} \\odot W$, where $\\odot$ is element-wise multiplication (broadcasting $\\boldsymbol{\\pi}$ across the rows of $W$). This results in an $n \\times m$ matrix $N$ where $N_{it} = \\pi_t \\cdot w_{it}$.\n3.  Compute the denominators for each read by summing the rows of $N$: $\\mathbf{z} = \\sum_{t=1}^{m} N_{it}$ for each $i$. This gives an $n \\times 1$ vector of denominators.\n4.  Identify the included reads where $z_i > 0$. Let this be a boolean mask.\n5.  If no reads are included, $\\boldsymbol{\\pi}^{(\\mathrm{new})} = \\boldsymbol{\\pi}$.\n6.  If reads are included, filter the rows of $N$ and the elements of $\\mathbf{z}$ corresponding to included reads. Let these be $N_{\\mathrm{incl}}$ and $\\mathbf{z}_{\\mathrm{incl}}$.\n7.  Calculate the posterior probability matrix for included reads: $P_{\\mathrm{incl}} = N_{\\mathrm{incl}} \\oslash \\mathbf{z}_{\\mathrm{incl}}$, where $\\oslash$ denotes element-wise division with broadcasting of the column vector $\\mathbf{z}_{\\mathrm{incl}}$.\n8.  Calculate the new proportions $\\boldsymbol{\\pi}^{(\\mathrm{new})}$ by taking the column-wise mean of $P_{\\mathrm{incl}}$.\n9.  Perform the normalization check on $\\boldsymbol{\\pi}^{(\\mathrm{new})}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the transcript abundance quantification problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path, mixed ambiguity)\n        {\n            \"pi_initial\": [0.5, 0.3, 0.2],\n            \"weights\": [\n                [0.9, 0.1, 0.0],\n                [0.4, 0.4, 0.2],\n                [0.0, 0.5, 0.5],\n                [0.0, 0.0, 1.0],\n            ],\n        },\n        # Test case 2 (boundary: one read unmapped, deterministic reads)\n        {\n            \"pi_initial\": [0.6, 0.4],\n            \"weights\": [\n                [0.0, 0.0],\n                [1.0, 0.0],\n                [0.0, 1.0],\n            ],\n        },\n        # Test case 3 (edge: zero initial mixture weight for one transcript)\n        {\n            \"pi_initial\": [0.0, 0.7, 0.3],\n            \"weights\": [\n                [1.0, 0.0, 0.0],\n                [0.4, 0.6, 0.0],\n                [0.4, 0.0, 0.6],\n            ],\n        },\n    ]\n\n    results = []\n    \n    # Numerical tolerance for normalization check\n    TOLERANCE = 1e-12\n\n    for case in test_cases:\n        pi = np.array(case[\"pi_initial\"], dtype=np.float64)\n        W = np.array(case[\"weights\"], dtype=np.float64)\n\n        # E-step: Calculate unnormalized posterior probabilities (numerators)\n        # This uses broadcasting: (1, m) * (n, m) -> (n, m)\n        numerators = pi * W\n        \n        # Calculate normalizing denominators for each read\n        denominators = np.sum(numerators, axis=1)\n\n        # Identify included reads (those with a positive denominator)\n        included_mask = denominators > 0\n        n_incl = np.sum(included_mask)\n\n        if n_incl == 0:\n            # If no reads are included, proportions remain unchanged\n            pi_new = pi\n        else:\n            # Filter to include only informative reads\n            included_numerators = numerators[included_mask]\n            included_denominators = denominators[included_mask]\n            \n            # E-step completion: Calculate posterior probabilities (responsibilities)\n            # for included reads. Shape of included_denominators is (n_incl,),\n            # we reshape to (n_incl, 1) for broadcasting.\n            posteriors = included_numerators / included_denominators[:, np.newaxis]\n\n            # M-step: Update mixture proportions by averaging posteriors\n            pi_new = np.sum(posteriors, axis=0) / n_incl\n\n        # Verify that the new proportions sum to 1 within the specified tolerance\n        normalization_check = np.isclose(np.sum(pi_new), 1.0, atol=TOLERANCE, rtol=0)\n\n        # Format the result as required: [list_of_floats, boolean]\n        result_pair = [pi_new.tolist(), bool(normalization_check)]\n        results.append(result_pair)\n\n    # Custom function to format the final list to avoid extra spaces\n    def format_list(data):\n        if isinstance(data, list):\n            return f\"[{','.join(map(format_list, data))}]\"\n        elif isinstance(data, float):\n            return f\"{data:.16g}\"  # Use general format for floats\n        elif isinstance(data, bool):\n            return \"true\" if data else \"false\"\n        else:\n            return str(data)\n            \n    # The problem asks for standard list string representation, which `str` produces\n    # with spaces. The example output format `[[...],...]` implies a compact\n    # representation.\n    final_output_str = str(results).replace(\" \", \"\").replace(\"True\", \"true\").replace(\"False\", \"false\")\n\n    # Final print statement in the exact required format.\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}