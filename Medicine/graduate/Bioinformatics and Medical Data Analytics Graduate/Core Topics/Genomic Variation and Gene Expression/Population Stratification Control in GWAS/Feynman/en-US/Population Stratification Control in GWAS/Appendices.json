{
    "hands_on_practices": [
        {
            "introduction": "Population stratification is a major source of confounding in genome-wide association studies, potentially leading to a high rate of false-positive findings. To effectively control for it, we must first understand its mechanism at a fundamental level. This practice guides you through a first-principles derivation to show how a spurious association for a non-causal variant arises when allele frequencies and disease prevalence both differ across unobserved subpopulations . By working through the mathematics, you will gain a clear, quantitative intuition for how population structure can mimic a genuine genetic effect.",
            "id": "4596444",
            "problem": "Consider a Genome-Wide Association Study (GWAS) conducted under a case-control design in a source population composed of two subpopulations. Let the subpopulations be indexed by $i \\in \\{1,2\\}$. The mixing proportions of the subpopulations in the source population are $r_1$ and $r_2$, with $r_1 + r_2 = 1$ and $0 < r_i < 1$. The binary phenotype $Y \\in \\{0,1\\}$ has subpopulation-specific mean (prevalence) $\\mu_i = \\Pr(Y = 1 \\mid i)$, which may differ across subpopulations. A single nucleotide polymorphism (SNP) has a minor allele $A$ with subpopulation-specific allele frequencies $p_i = \\Pr(A \\mid i)$, which may also differ across subpopulations.\n\nAssume the SNP is non-causal for the phenotype, in the sense that within each subpopulation $i$, the SNP genotype is independent of the phenotype given the subpopulation label. Specifically, for $i \\in \\{1,2\\}$, assume $\\Pr(A \\mid Y, i) = \\Pr(A \\mid i) = p_i$. A case-control sample is drawn by sampling $n_1$ cases ($Y=1$) uniformly from all cases in the source population and $n_0$ controls ($Y=0$) uniformly from all controls in the source population, with $n_1, n_0$ large so that expectations coincide with population probabilities. In the subsequent analysis, population labels are ignored.\n\nDefine the expected spurious association for the non-causal SNP as the expected log-odds ratio comparing the allele $A$ frequency in cases versus controls that would be estimated by a simple allele-based logistic regression model ignoring subpopulation labels. Starting only from the laws of conditional probability, the law of total probability, and the case-control sampling scheme described above, derive a closed-form analytic expression for this expected log-odds ratio in terms of $r_1, r_2, \\mu_1, \\mu_2, p_1, p_2$.\n\nYour final answer must be a single closed-form analytic expression. No numerical approximation is required and no units are involved.",
            "solution": "The problem asks for the expected log-odds ratio for a non-causal SNP in a case-control study where population structure is ignored. The population is a mixture of two subpopulations, $i \\in \\{1,2\\}$, with mixing proportions $r_1, r_2$, subpopulation-specific disease prevalences $\\mu_i = \\Pr(Y=1 \\mid i)$, and subpopulation-specific allele frequencies $p_i = \\Pr(A \\mid i)$. The SNP is non-causal, meaning $\\Pr(A \\mid Y, i) = \\Pr(A \\mid i) = p_i$.\n\nThe expected log-odds ratio, which we denote as $\\text{log-OR}$, is defined based on the allele frequencies in the case population ($Y=1$) and the control population ($Y=0$). Let $P_{\\text{case}} = \\Pr(A \\mid Y=1)$ be the frequency of allele $A$ among all cases in the source population, and $P_{\\text{control}} = \\Pr(A \\mid Y=0)$ be the frequency of allele $A$ among all controls.\n\nThe odds of allele $A$ in cases is $\\frac{P_{\\text{case}}}{1 - P_{\\text{case}}}$.\nThe odds of allele $A$ in controls is $\\frac{P_{\\text{control}}}{1 - P_{\\text{control}}}$.\n\nThe odds ratio (OR) is the ratio of these two odds:\n$$\n\\text{OR} = \\frac{P_{\\text{case}} / (1 - P_{\\text{case}})}{P_{\\text{control}} / (1 - P_{\\text{control}})}\n$$\nThe quantity to be derived is the natural logarithm of this expression, $\\text{log-OR} = \\ln(\\text{OR})$. The derivation proceeds by first finding expressions for $P_{\\text{case}}$ and $P_{\\text{control}}$.\n\nFirst, we calculate the overall disease prevalence, $\\mu_{\\text{total}} = \\Pr(Y=1)$, in the source population using the law of total probability, summing over the subpopulations:\n$$\n\\mu_{\\text{total}} = \\Pr(Y=1) = \\sum_{i=1}^{2} \\Pr(Y=1 \\mid i) \\Pr(i) = \\mu_1 r_1 + \\mu_2 r_2\n$$\nSimilarly, the overall probability of being a control is:\n$$\n\\Pr(Y=0) = 1 - \\Pr(Y=1) = 1 - (\\mu_1 r_1 + \\mu_2 r_2)\n$$\n\nNow, we derive the allele frequency in cases, $P_{\\text{case}} = \\Pr(A \\mid Y=1)$. Using the law of total probability, conditioning on the subpopulation label $i$:\n$$\nP_{\\text{case}} = \\Pr(A \\mid Y=1) = \\sum_{i=1}^{2} \\Pr(A \\mid Y=1, i) \\Pr(i \\mid Y=1)\n$$\nThe problem states that the SNP is non-causal within each subpopulation, which is formally given as $\\Pr(A \\mid Y, i) = \\Pr(A \\mid i) = p_i$. Therefore, $\\Pr(A \\mid Y=1, i) = p_i$.\n\nNext, we find the probability that a case comes from subpopulation $i$, $\\Pr(i \\mid Y=1)$, using Bayes' theorem:\n$$\n\\Pr(i \\mid Y=1) = \\frac{\\Pr(Y=1 \\mid i) \\Pr(i)}{\\Pr(Y=1)} = \\frac{\\mu_i r_i}{\\mu_1 r_1 + \\mu_2 r_2}\n$$\nSubstituting these into the expression for $P_{\\text{case}}$:\n$$\nP_{\\text{case}} = \\sum_{i=1}^{2} p_i \\left( \\frac{\\mu_i r_i}{\\mu_1 r_1 + \\mu_2 r_2} \\right) = \\frac{p_1 \\mu_1 r_1 + p_2 \\mu_2 r_2}{\\mu_1 r_1 + \\mu_2 r_2}\n$$\n\nNext, we derive the allele frequency in controls, $P_{\\text{control}} = \\Pr(A \\mid Y=0)$. The logic is analogous.\n$$\nP_{\\text{control}} = \\Pr(A \\mid Y=0) = \\sum_{i=1}^{2} \\Pr(A \\mid Y=0, i) \\Pr(i \\mid Y=0)\n$$\nAgain, due to the non-causal assumption, $\\Pr(A \\mid Y=0, i) = p_i$. We find the probability that a control comes from subpopulation $i$, $\\Pr(i \\mid Y=0)$, using Bayes' theorem:\n$$\n\\Pr(i \\mid Y=0) = \\frac{\\Pr(Y=0 \\mid i) \\Pr(i)}{\\Pr(Y=0)}\n$$\nSince $\\Pr(Y=0 \\mid i) = 1 - \\Pr(Y=1 \\mid i) = 1 - \\mu_i$, we have:\n$$\n\\Pr(i \\mid Y=0) = \\frac{(1 - \\mu_i) r_i}{1 - (\\mu_1 r_1 + \\mu_2 r_2)}\n$$\nSubstituting these into the expression for $P_{\\text{control}}$:\n$$\nP_{\\text{control}} = \\sum_{i=1}^{2} p_i \\left( \\frac{(1 - \\mu_i) r_i}{1 - (\\mu_1 r_1 + \\mu_2 r_2)} \\right) = \\frac{p_1 (1 - \\mu_1) r_1 + p_2 (1 - \\mu_2) r_2}{1 - (\\mu_1 r_1 + \\mu_2 r_2)}\n$$\n\nNow we assemble the odds ratio. Let's first compute the odds for cases:\n$$\n\\frac{P_{\\text{case}}}{1 - P_{\\text{case}}} = \\frac{\\frac{p_1 \\mu_1 r_1 + p_2 \\mu_2 r_2}{\\mu_1 r_1 + \\mu_2 r_2}}{1 - \\frac{p_1 \\mu_1 r_1 + p_2 \\mu_2 r_2}{\\mu_1 r_1 + \\mu_2 r_2}} = \\frac{p_1 \\mu_1 r_1 + p_2 \\mu_2 r_2}{(\\mu_1 r_1 + \\mu_2 r_2) - (p_1 \\mu_1 r_1 + p_2 \\mu_2 r_2)} = \\frac{p_1 \\mu_1 r_1 + p_2 \\mu_2 r_2}{(1-p_1)\\mu_1 r_1 + (1-p_2)\\mu_2 r_2}\n$$\nAnd the odds for controls:\n$$\n\\frac{P_{\\text{control}}}{1 - P_{\\text{control}}} = \\frac{\\frac{p_1 (1 - \\mu_1) r_1 + p_2 (1 - \\mu_2) r_2}{1 - (\\mu_1 r_1 + \\mu_2 r_2)}}{1 - \\frac{p_1 (1 - \\mu_1) r_1 + p_2 (1 - \\mu_2) r_2}{1 - (\\mu_1 r_1 + \\mu_2 r_2)}} = \\frac{p_1 (1 - \\mu_1) r_1 + p_2 (1 - \\mu_2) r_2}{(1 - (\\mu_1 r_1 + \\mu_2 r_2)) - (p_1 (1 - \\mu_1) r_1 + p_2 (1 - \\mu_2) r_2)}\n$$\nThe denominator simplifies to $(1-p_1)(1 - \\mu_1) r_1 + (1-p_2)(1 - \\mu_2) r_2$. So the odds for controls are:\n$$\n\\frac{P_{\\text{control}}}{1 - P_{\\text{control}}} = \\frac{p_1 (1 - \\mu_1) r_1 + p_2 (1 - \\mu_2) r_2}{(1-p_1)(1 - \\mu_1) r_1 + (1-p_2)(1 - \\mu_2) r_2}\n$$\nThe odds ratio $\\text{OR}$ is the ratio of the odds in cases to the odds in controls:\n$$\n\\text{OR} = \\frac{\\frac{p_1 \\mu_1 r_1 + p_2 \\mu_2 r_2}{(1-p_1)\\mu_1 r_1 + (1-p_2)\\mu_2 r_2}}{\\frac{p_1 (1 - \\mu_1) r_1 + p_2 (1 - \\mu_2) r_2}{(1-p_1)(1 - \\mu_1) r_1 + (1-p_2)(1 - \\mu_2) r_2}}\n$$\n$$\n\\text{OR} = \\left( \\frac{p_1 \\mu_1 r_1 + p_2 \\mu_2 r_2}{(1-p_1)\\mu_1 r_1 + (1-p_2)\\mu_2 r_2} \\right) \\left( \\frac{(1-p_1)(1 - \\mu_1) r_1 + (1-p_2)(1 - \\mu_2) r_2}{p_1 (1 - \\mu_1) r_1 + p_2 (1 - \\mu_2) r_2} \\right)\n$$\nThe expected log-odds ratio is the natural logarithm of this expression. This spurious association arises when both allele frequencies ($p_1 \\neq p_2$) and disease prevalences ($\\mu_1 \\neq \\mu_2$) differ across subpopulations. If either $p_1 = p_2$ or $\\mu_1 = \\mu_2$, the expression for the OR simplifies to $1$, and the log-OR becomes $0$.\n\nThe final closed-form expression for the log-odds ratio is:\n$$\n\\text{log-OR} = \\ln \\left[ \\left( \\frac{p_1 \\mu_1 r_1 + p_2 \\mu_2 r_2}{(1-p_1)\\mu_1 r_1 + (1-p_2)\\mu_2 r_2} \\right) \\left( \\frac{(1-p_1)(1 - \\mu_1) r_1 + (1-p_2)(1 - \\mu_2) r_2}{p_1 (1 - \\mu_1) r_1 + p_2 (1 - \\mu_2) r_2} \\right) \\right]\n$$\nThis expression can also be written as a sum of logarithms:\n$$\n\\text{log-OR} = \\ln(p_1 \\mu_1 r_1 + p_2 \\mu_2 r_2) + \\ln((1-p_1)(1 - \\mu_1) r_1 + (1-p_2)(1 - \\mu_2) r_2) - \\ln((1-p_1)\\mu_1 r_1 + (1-p_2)\\mu_2 r_2) - \\ln(p_1 (1 - \\mu_1) r_1 + p_2 (1 - \\mu_2) r_2)\n$$\nBoth forms are equivalent. The first form is more compact.",
            "answer": "$$\n\\boxed{\\ln\\left(\\frac{\\left(p_1 \\mu_1 r_1 + p_2 \\mu_2 r_2\\right)\\left((1-p_1)(1 - \\mu_1) r_1 + (1-p_2)(1 - \\mu_2) r_2\\right)}{\\left((1-p_1)\\mu_1 r_1 + (1-p_2)\\mu_2 r_2\\right)\\left(p_1 (1 - \\mu_1) r_1 + p_2 (1 - \\mu_2) r_2\\right)}\\right)}\n$$"
        },
        {
            "introduction": "Having seen how population structure can create confounding, we now turn to a primary method for its correction: the Linear Mixed Model (LMM). The core of an LMM in GWAS is the Genetic Relationship Matrix (GRM), which quantifies the shared ancestry among all pairs of individuals. This exercise provides a tangible, hands-on experience in building a GRM from a small genotype dataset and justifying the critical step of variance-standardization, which correctly weights variants based on their allele frequencies . Completing this practice will demystify the GRM and clarify its role in modeling background genetic effects.",
            "id": "4596615",
            "problem": "A Genome-Wide Association Study (GWAS) seeks to control population stratification by modeling phenotypes with a Linear Mixed Model (LMM), where the random genetic effect has covariance proportional to a Genetic Relationship Matrix (GRM). The GRM should reflect realized genomic relatedness in a way that is comparable across genetic markers with different allele frequencies, under a scientifically reasonable generative model for genotypes.\n\nAssume the following setting. There are $N = 3$ individuals and $M = 3$ independent biallelic single-nucleotide polymorphisms (SNPs), coded as minor-allele counts $x_{im} \\in \\{0,1,2\\}$ for individual $i \\in \\{1,2,3\\}$ and marker $m \\in \\{1,2,3\\}$. For each marker $m$, the sample minor-allele frequency is defined as $p_m = \\frac{1}{2N}\\sum_{i=1}^{N} x_{im}$. Under Hardy–Weinberg Equilibrium (HWE), the genotype $X_m$ at marker $m$ has mean $2p_m$ and variance $2p_m(1 - p_m)$. The GRM is constructed as the empirical average of outer products of variance-standardized genotype vectors across markers.\n\nYou are given the genotype data for the $M=3$ SNPs:\n- For marker $m=1$: $\\mathbf{x}_1 = \\begin{pmatrix}2 \\\\ 1 \\\\ 0\\end{pmatrix}$.\n- For marker $m=2$: $\\mathbf{x}_2 = \\begin{pmatrix}0 \\\\ 1 \\\\ 1\\end{pmatrix}$.\n- For marker $m=3$: $\\mathbf{x}_3 = \\begin{pmatrix}1 \\\\ 0 \\\\ 2\\end{pmatrix}$.\n\nTasks:\n1. Using the variance-standardized genotype construction justified by HWE, compute the full $3 \\times 3$ Genetic Relationship Matrix $\\mathbf{K}$ for these $N=3$ individuals as an exact expression with rational entries. Do not round; provide the exact rational values.\n2. Starting from the Hardy–Weinberg Equilibrium model and the requirement that each marker contribute equally in expectation to realized relatedness, justify mathematically why allele-frequency-based variance standardization is the appropriate weighting to use when constructing $\\mathbf{K}$ in the LMM for GWAS.\n\nYour final answer must be the single matrix $\\mathbf{K}$, expressed with exact rational entries, and no rounding. No units are required.",
            "solution": "The solution is divided into two parts, corresponding to the two tasks. Task 2, the theoretical justification, is presented first as it establishes the foundation for the calculation in Task 1.\n\n### Task 2: Justification for Variance Standardization\n\nThe objective is to construct a Genetic Relationship Matrix, $\\mathbf{K}$, such that each genetic marker $m$ contributes equally in expectation to the estimate of realized relatedness. The GRM is constructed as the average of contributions from $M$ markers. The contribution of a single marker $m$ to the relationship between individuals $i$ and $j$ is based on their genotypes, $x_{im}$ and $x_{jm}$.\n\nLet $\\mathbf{x}_m$ be the column vector of genotype counts for marker $m$. These counts are coded as $x_{im} \\in \\{0, 1, 2\\}$ representing the number of minor alleles for individual $i$ at marker $m$. The problem states that under Hardy–Weinberg Equilibrium (HWE), and using the sample minor-allele frequency $p_m$ as an estimate for the population frequency, the genotype variable $X_m$ has a mean $E[X_m] = 2p_m$ and a variance $Var(X_m) = 2p_m(1-p_m)$.\n\nThe variance of a marker's genotypes, $2p_m(1-p_m)$, is a function of its allele frequency $p_m$. Markers with intermediate allele frequencies (i.e., $p_m$ close to $0.5$) have higher variance than markers with rare alleles (i.e., $p_m$ close to $0$ or $1$). If we were to use the raw, unstandardized genotypes to compute relatedness, markers with higher variance would disproportionately influence the final GRM. This is undesirable as the relatedness should be an aggregate measure over the entire genome, not dominated by a subset of high-variance markers.\n\nTo ensure each marker contributes equally, we must standardize the genotype values. A standard approach is to transform the genotype variable for each marker to have a mean of $0$ and a variance of $1$. Let $Z_{im}$ be the standardized genotype for individual $i$ at marker $m$:\n$$\nZ_{im} = \\frac{x_{im} - E[X_m]}{\\sqrt{Var(X_m)}} = \\frac{x_{im} - 2p_m}{\\sqrt{2p_m(1-p_m)}}\n$$\nBy construction, for any marker $m$, the standardized variable $Z_m$ has an expected value of $E[Z_m] = 0$ and an expected variance of $Var(Z_m) = 1$. The second moment is therefore $E[Z_m^2] = Var(Z_m) + (E[Z_m])^2 = 1 + 0^2 = 1$.\n\nThe GRM, $\\mathbf{K}$, is defined as the empirical average of the outer products of these standardized genotype vectors. Let $\\mathbf{z}_m$ be the column vector of standardized genotypes for marker $m$. The entry $K_{ij}$ of the GRM is:\n$$\nK_{ij} = \\frac{1}{M} \\sum_{m=1}^{M} z_{im} z_{jm}\n$$\nThe contribution of each marker to the diagonal elements of $\\mathbf{K}$, which represent self-relatedness or inbreeding, is $z_{im}^2$. The expected contribution is $E[z_{im}^2]$. As shown above, treating the genotypes as random variables, $E[Z_{im}^2]=1$ for all markers $m$. Therefore, the expected contribution of any marker to the diagonal of the GRM is identical and independent of its allele frequency $p_m$.\n\nThis standardization places all markers on an equal footing, fulfilling the requirement that \"each marker contribute equally in expectation to realized relatedness\". This prevents common variants from dominating the kinship estimate and ensures that shared rare variants, which can be highly informative about recent co-ancestry, are appropriately weighted. This method of constructing $\\mathbf{K}$ from variance-standardized genotypes is a cornerstone of modern LMM-based GWAS analysis.\n\n### Task 1: Computation of the Genetic Relationship Matrix $\\mathbf{K}$\n\nThe GRM $\\mathbf{K}$ is given by the formula:\n$$\n\\mathbf{K} = \\frac{1}{M} \\sum_{m=1}^{M} \\frac{(\\mathbf{x}_m - 2p_m\\mathbf{1})(\\mathbf{x}_m - 2p_m\\mathbf{1})^T}{2p_m(1-p_m)}\n$$\nwhere $M=3$ is the number of markers, $\\mathbf{x}_m$ is the genotype vector for marker $m$, $p_m$ is the sample minor-allele frequency, and $\\mathbf{1}$ is a vector of ones. We compute the contribution for each marker individually.\n\n**Marker $m=1$**:\nThe genotype vector is $\\mathbf{x}_1 = \\begin{pmatrix}2 \\\\ 1 \\\\ 0\\end{pmatrix}$.\nThe number of individuals is $N=3$.\nThe sample minor-allele frequency is $p_1 = \\frac{1}{2N}\\sum_{i=1}^{N} x_{i1} = \\frac{1}{2 \\cdot 3}(2+1+0) = \\frac{3}{6} = \\frac{1}{2}$.\nThe mean is $\\mu_1 = 2p_1 = 2 \\left(\\frac{1}{2}\\right) = 1$.\nThe variance is $\\sigma_1^2 = 2p_1(1-p_1) = 2 \\left(\\frac{1}{2}\\right) \\left(1-\\frac{1}{2}\\right) = \\frac{1}{2}$.\nThe centered genotype vector is $\\mathbf{y}_1 = \\mathbf{x}_1 - \\mu_1\\mathbf{1} = \\begin{pmatrix}2-1 \\\\ 1-1 \\\\ 0-1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\\\ -1\\end{pmatrix}$.\nThe contribution from marker $1$ is $\\mathbf{K}_1 = \\frac{\\mathbf{y}_1 \\mathbf{y}_1^T}{\\sigma_1^2} = \\frac{1}{1/2} \\begin{pmatrix}1 \\\\ 0 \\\\ -1\\end{pmatrix} \\begin{pmatrix}1 & 0 & -1\\end{pmatrix} = 2 \\begin{pmatrix}1 & 0 & -1 \\\\ 0 & 0 & 0 \\\\ -1 & 0 & 1\\end{pmatrix} = \\begin{pmatrix}2 & 0 & -2 \\\\ 0 & 0 & 0 \\\\ -2 & 0 & 2\\end{pmatrix}$.\n\n**Marker $m=2$**:\nThe genotype vector is $\\mathbf{x}_2 = \\begin{pmatrix}0 \\\\ 1 \\\\ 1\\end{pmatrix}$.\nThe sample minor-allele frequency is $p_2 = \\frac{1}{2 \\cdot 3}(0+1+1) = \\frac{2}{6} = \\frac{1}{3}$.\nThe mean is $\\mu_2 = 2p_2 = 2 \\left(\\frac{1}{3}\\right) = \\frac{2}{3}$.\nThe variance is $\\sigma_2^2 = 2p_2(1-p_2) = 2 \\left(\\frac{1}{3}\\right) \\left(1-\\frac{1}{3}\\right) = 2 \\left(\\frac{1}{3}\\right) \\left(\\frac{2}{3}\\right) = \\frac{4}{9}$.\nThe centered genotype vector is $\\mathbf{y}_2 = \\mathbf{x}_2 - \\mu_2\\mathbf{1} = \\begin{pmatrix}0 - 2/3 \\\\ 1 - 2/3 \\\\ 1 - 2/3\\end{pmatrix} = \\begin{pmatrix}-2/3 \\\\ 1/3 \\\\ 1/3\\end{pmatrix}$.\nThe contribution from marker $2$ is $\\mathbf{K}_2 = \\frac{\\mathbf{y}_2 \\mathbf{y}_2^T}{\\sigma_2^2} = \\frac{1}{4/9} \\begin{pmatrix}-2/3 \\\\ 1/3 \\\\ 1/3\\end{pmatrix} \\begin{pmatrix}-2/3 & 1/3 & 1/3\\end{pmatrix} = \\frac{9}{4} \\begin{pmatrix}4/9 & -2/9 & -2/9 \\\\ -2/9 & 1/9 & 1/9 \\\\ -2/9 & 1/9 & 1/9\\end{pmatrix} = \\begin{pmatrix}1 & -1/2 & -1/2 \\\\ -1/2 & 1/4 & 1/4 \\\\ -1/2 & 1/4 & 1/4\\end{pmatrix}$.\n\n**Marker $m=3$**:\nThe genotype vector is $\\mathbf{x}_3 = \\begin{pmatrix}1 \\\\ 0 \\\\ 2\\end{pmatrix}$.\nThe sample minor-allele frequency is $p_3 = \\frac{1}{2 \\cdot 3}(1+0+2) = \\frac{3}{6} = \\frac{1}{2}$.\nThe mean is $\\mu_3 = 2p_3 = 2 \\left(\\frac{1}{2}\\right) = 1$.\nThe variance is $\\sigma_3^2 = 2p_3(1-p_3) = 2 \\left(\\frac{1}{2}\\right) \\left(1-\\frac{1}{2}\\right) = \\frac{1}{2}$.\nThe centered genotype vector is $\\mathbf{y}_3 = \\mathbf{x}_3 - \\mu_3\\mathbf{1} = \\begin{pmatrix}1-1 \\\\ 0-1 \\\\ 2-1\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1 \\\\ 1\\end{pmatrix}$.\nThe contribution from marker $3$ is $\\mathbf{K}_3 = \\frac{\\mathbf{y}_3 \\mathbf{y}_3^T}{\\sigma_3^2} = \\frac{1}{1/2} \\begin{pmatrix}0 \\\\ -1 \\\\ 1\\end{pmatrix} \\begin{pmatrix}0 & -1 & 1\\end{pmatrix} = 2 \\begin{pmatrix}0 & 0 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & -1 & 1\\end{pmatrix} = \\begin{pmatrix}0 & 0 & 0 \\\\ 0 & 2 & -2 \\\\ 0 & -2 & 2\\end{pmatrix}$.\n\n**Final GRM Calculation**:\nNow, we sum the contributions and divide by $M=3$.\n$$\n\\mathbf{K} = \\frac{1}{3} (\\mathbf{K}_1 + \\mathbf{K}_2 + \\mathbf{K}_3)\n$$\n$$\n\\mathbf{K} = \\frac{1}{3} \\left( \\begin{pmatrix}2 & 0 & -2 \\\\ 0 & 0 & 0 \\\\ -2 & 0 & 2\\end{pmatrix} + \\begin{pmatrix}1 & -1/2 & -1/2 \\\\ -1/2 & 1/4 & 1/4 \\\\ -1/2 & 1/4 & 1/4\\end{pmatrix} + \\begin{pmatrix}0 & 0 & 0 \\\\ 0 & 2 & -2 \\\\ 0 & -2 & 2\\end{pmatrix} \\right)\n$$\nSumming the matrices inside the parentheses:\n$$\n\\mathbf{K}_1 + \\mathbf{K}_2 + \\mathbf{K}_3 = \\begin{pmatrix} 2+1+0 & 0-1/2+0 & -2-1/2+0 \\\\ 0-1/2+0 & 0+1/4+2 & 0+1/4-2 \\\\ -2-1/2+0 & 0+1/4-2 & 2+1/4+2 \\end{pmatrix} = \\begin{pmatrix} 3 & -1/2 & -5/2 \\\\ -1/2 & 9/4 & -7/4 \\\\ -5/2 & -7/4 & 17/4 \\end{pmatrix}\n$$\nFinally, multiplying by $\\frac{1}{3}$:\n$$\n\\mathbf{K} = \\frac{1}{3} \\begin{pmatrix} 3 & -1/2 & -5/2 \\\\ -1/2 & 9/4 & -7/4 \\\\ -5/2 & -7/4 & 17/4 \\end{pmatrix} = \\begin{pmatrix} 1 & -1/6 & -5/6 \\\\ -1/6 & 3/4 & -7/12 \\\\ -5/6 & -7/12 & 17/12 \\end{pmatrix}\n$$\nThis is the final Genetic Relationship Matrix $\\mathbf{K}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix}\n1 & -\\frac{1}{6} & -\\frac{5}{6} \\\\\n-\\frac{1}{6} & \\frac{3}{4} & -\\frac{7}{12} \\\\\n-\\frac{5}{6} & -\\frac{7}{12} & \\frac{17}{12}\n\\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While methods like LMMs are powerful, they require access to individual-level genotype data, which is often not publicly available. Linkage Disequilibrium (LD) Score Regression (LDSC) provides an ingenious solution by operating directly on GWAS summary statistics. In this practical coding exercise, you will implement a simplified LDSC analysis to dissect the sources of inflation in GWAS test statistics . This will allow you to distinguish true polygenic architecture from confounding due to stratification or other technical artifacts, a crucial skill for the critical interpretation of GWAS results.",
            "id": "4596512",
            "problem": "You are tasked with implementing a self-contained program that applies Linkage Disequilibrium Score Regression (LDSC) to single-trait Genome-Wide Association Study (GWAS) summary statistics to quantify and interpret inflation in test statistics while considering population stratification control. Your program must fit a linear model to squared standard normal GWAS test statistics across single nucleotide polymorphisms (SNPs) and assess whether the intercept is significantly above $1$, then compute an attenuation ratio, and finally, infer plausible non-stratification explanations using a rule-based classifier.\n\nBackground and fundamental base: Consider the additive polygenic model for a standardized phenotype vector $y \\in \\mathbb{R}^{n}$ under independent, small effects and many variants, $y = G \\beta + \\varepsilon$, where $G \\in \\mathbb{R}^{n \\times M}$ is a standardized genotype matrix across $M$ single nucleotide polymorphisms, $\\beta \\in \\mathbb{R}^{M}$ are small additive effects with $\\mathbb{E}[\\beta] = 0$ and variance summing to narrow-sense heritability $h^{2}$, and $\\varepsilon$ is uncorrelated noise with variance $1 - h^{2}$. For a marginal association test at variant $j$, let $z_{j}$ denote the standard normal test statistic under large-sample regression theory (squared statistic $z_{j}^{2}$ approximately follows a scaled chi-square with one degree of freedom under general alternatives). The linkage disequilibrium (LD) score at variant $j$ is $l_{j} = \\sum_{k} r_{jk}^{2}$, where $r_{jk}^{2}$ are squared correlations in a reference panel. The LDSC framework leverages the fact that, under the polygenic model and in the absence of confounding, the expected value of $z_{j}^{2}$ increases linearly with $l_{j}$ due to the aggregation of many small effects correlated through LD. In practice, additional uniform inflation sources can shift the baseline level of $z_{j}^{2}$ upward across all variants. You will implement a regression-based estimator to capture these two effects: an intercept and a slope, and then assess the intercept for evidence of inflation beyond $1$.\n\nYour tasks:\n1) For each test case, given arrays $\\{l_{j}\\}_{j=1}^{M}$ and $\\{z_{j}^{2}\\}_{j=1}^{M}$, fit the linear model $z_{j}^{2} = \\alpha + \\beta \\, l_{j} + \\text{residual}_{j}$ by ordinary least squares with an intercept.\n\n2) Compute heteroskedasticity-robust (HC1) standard errors for $(\\alpha,\\beta)$ via the sandwich estimator using $n = M$ observations and $p = 2$ parameters. Let $\\widehat{\\alpha}$ and $\\widehat{\\sigma}_{\\widehat{\\alpha}}$ denote the intercept estimate and its robust standard error. Let $\\widehat{\\beta}$ and $\\widehat{\\sigma}_{\\widehat{\\beta}}$ denote the slope estimate and its robust standard error. Use the usual $t$-statistic with degrees of freedom $n - p$ to test $H_{0}: \\alpha = 1$; compute a two-sided $p$-value and define significance at level $\\delta = 0.01$. Also compute a two-sided test for $H_{0}: \\beta = 0$.\n\n3) Compute the mean chi-square $\\overline{\\chi^{2}} = \\frac{1}{M} \\sum_{j=1}^{M} z_{j}^{2}$. Define the attenuation ratio $R = \\frac{\\widehat{\\alpha} - 1}{\\overline{\\chi^{2}} - 1}$ when $\\overline{\\chi^{2}} - 1 > 0$, and define $R = 0$ when $\\overline{\\chi^{2}} \\leq 1$. Compute the coefficient of determination $R^{2}$ from the fitted linear model and the Spearman correlation $\\rho$ between $|\\text{residual}_{j}|$ and $l_{j}$.\n\n4) Using only $(\\widehat{\\alpha}, \\widehat{\\sigma}_{\\widehat{\\alpha}}, \\widehat{\\beta}, \\widehat{\\sigma}_{\\widehat{\\beta}}, \\overline{\\chi^{2}}, R, R^{2}, \\rho)$, infer plausible non-stratification explanations for an intercept substantially above $1$ using the rule-based classifier below. Encode explanations as a bitmask integer $B$:\n   - Bit $1$ (value $1$): Uniform, non-LD-correlated inflation is plausible (for example, cryptic relatedness, sample overlap, extreme case-control imbalance, or technical confounding) if the intercept test is significant at level $\\delta$ and the slope is not significant at level $\\eta = 0.05$, and $R \\geq 0.7$.\n   - Bit $2$ (value $2$): LD score mismatch or measurement error attenuation is plausible if the intercept test is significant at level $\\delta$ and $R^{2}  0.3$ or $\\rho \\geq 0.3$, and the slope is significant at level $\\eta$.\n\n5) For each test case, produce a list $[\\widehat{\\alpha}, \\widehat{\\sigma}_{\\widehat{\\alpha}}, R, B]$, where $\\widehat{\\alpha}$, $\\widehat{\\sigma}_{\\widehat{\\alpha}}$, and $R$ are floating-point values, and $B$ is an integer bitmask computed as above.\n\n6) Final output format: Your program should produce a single line of output containing the results for all test cases as a comma-separated list of the four per-case lists, enclosed in square brackets. For example, a shape like $[[a_{1},s_{1},r_{1},b_{1}],[a_{2},s_{2},r_{2},b_{2}],[a_{3},s_{3},r_{3},b_{3}],[a_{4},s_{4},r_{4},b_{4}]]$ must be printed with no extra text. There is no unit requirement for any outputs.\n\nTest suite (common construction and four cases):\n- For all cases, let $M = 200$ and define for $j \\in \\{1,\\dots,M\\}$:\n  - $l^{\\text{true}}_{j} = 1 + 50 \\cdot \\frac{j - 0.5}{M}$.\n  - $q_{j}^{2} = 1 + 0.2 \\cdot \\sin\\left(2\\pi \\cdot \\frac{j}{M}\\right)$.\n- The observed squared statistics are constructed deterministically as $z_{j}^{2} = \\left(1 + \\alpha \\cdot l^{\\text{gen}}_{j} + c\\right) \\cdot q_{j}^{2}$, where $l^{\\text{gen}}_{j}$ is the generator-side LD input specific to each case. The regression uses the provided observed $l_{j}$ as predictor (which equals $l^{\\text{obs}}_{j}$ below). For each case, you are given the observed $l_{j}$ and $z_{j}^{2}$ constructed as follows:\n  - Case $1$ (polygenic with mild baseline inflation): $\\alpha = 0.001$, $c = 0.02$, $l^{\\text{gen}}_{j} = l^{\\text{true}}_{j}$, $l^{\\text{obs}}_{j} = l^{\\text{true}}_{j}$.\n  - Case $2$ (uniform inflation with negligible polygenicity): $\\alpha = 0$, $c = 0.20$, $l^{\\text{gen}}_{j} = l^{\\text{true}}_{j}$, $l^{\\text{obs}}_{j} = l^{\\text{true}}_{j}$.\n  - Case $3$ (polygenic with LD score mismatch): $\\alpha = 0.002$, $c = 0$, $l^{\\text{gen}}_{j} = l^{\\text{true}}_{j}$, $l^{\\text{obs}}_{j} = l^{\\text{true}}_{j} \\cdot \\left(1 + 0.3 \\cdot \\cos\\left(2\\pi \\cdot \\frac{j}{13}\\right)\\right)$.\n  - Case $4$ (null baseline): $\\alpha = 0$, $c = 0$, $l^{\\text{gen}}_{j} = l^{\\text{true}}_{j}$, $l^{\\text{obs}}_{j} = l^{\\text{true}}_{j}$.\n\nYou must:\n- Construct $l_{j}$ as $l^{\\text{obs}}_{j}$ for each case, construct $z_{j}^{2}$ via the formula above, and then apply the same pipeline to obtain $[\\widehat{\\alpha}, \\widehat{\\sigma}_{\\widehat{\\alpha}}, R, B]$ for each case.\n- Use the Spearman correlation between $|\\text{residual}_{j}|$ and $l_{j}$ in each case.\n- Use two-sided tests for both intercept and slope using $t$-statistics with degrees of freedom $M - 2$.\n- The final output must be a single line containing the four lists in order, enclosed in square brackets, with no extra text or spaces beyond those naturally produced by list stringification.\n\nAll variables, symbols, and numbers are specified in mathematics as required. No physical units are involved. Your program must be fully deterministic and must not rely on any random number generation.",
            "solution": "The problem is valid as it is scientifically grounded in the established statistical genetics methodology of Linkage Disequilibrium Score Regression (LDSC), is well-posed with a unique and verifiable solution path, and all parameters and conditions are specified without ambiguity or contradiction. We proceed to the solution.\n\nThe overall task is to implement a simplified LDSC analysis pipeline, apply it to four simulated test cases, and report specific diagnostic metrics. The pipeline involves data generation, ordinary least squares (OLS) regression, calculation of robust standard errors, hypothesis testing, and a final rule-based classification.\n\n**1. Data Generation**\n\nFor each of the four test cases, we first generate the input data: the observed LD scores $\\{l_j\\}_{j=1}^M$ (the predictor variable) and the squared GWAS z-scores $\\{z_j^2\\}_{j=1}^M$ (the response variable). The number of single nucleotide polymorphisms (SNPs) is fixed at $M=200$. The index $j$ runs from $1$ to $200$.\n\nThe base components are defined as:\n- True LD scores: $l^{\\text{true}}_{j} = 1 + 50 \\cdot \\frac{j - 0.5}{M}$\n- A deterministic noise term: $q_{j}^{2} = 1 + 0.2 \\cdot \\sin\\left(2\\pi \\cdot \\frac{j}{M}\\right)$\n\nThe observed squared z-scores are generated using the formula:\n$$z_{j}^{2} = \\left(1 + \\alpha_{\\text{case}} \\cdot l^{\\text{gen}}_{j} + c_{\\text{case}}\\right) \\cdot q_{j}^{2}$$\nwhere $\\alpha_{\\text{case}}$ and $c_{\\text{case}}$ are case-specific parameters for polygenic effect size and baseline inflation, respectively, and $l^{\\text{gen}}_{j}$ is the LD score used in the data generation process. The predictor variable used in our regression, $l_j$, is specified as $l^{\\text{obs}}_j$ for each case.\n\n**2. Ordinary Least Squares (OLS) Regression**\n\nWe fit the linear model $y_j = \\alpha + \\beta x_j + e_j$, where $y_j = z_j^2$ and $x_j = l_j$. In matrix form, this is $Y = X\\boldsymbol{\\theta} + \\boldsymbol{e}$, where $Y$ is the $M \\times 1$ vector of $z_j^2$ values, $X$ is the $M \\times 2$ design matrix with a column of ones and a column of $l_j$ values, $\\boldsymbol{\\theta} = [\\alpha, \\beta]^T$ is the vector of parameters to be estimated, and $\\boldsymbol{e}$ is the vector of residuals.\n\nThe OLS estimator for $\\boldsymbol{\\theta}$ is given by:\n$$ \\widehat{\\boldsymbol{\\theta}} = (X^T X)^{-1} X^T Y $$\nThis yields the estimates for the intercept, $\\widehat{\\alpha}$, and the slope, $\\widehat{\\beta}$.\n\n**3. Heteroskedasticity-Consistent (HC1) Standard Errors**\n\nGWAS statistics are inherently heteroskedastic; the variance of $z_j^2$ depends on its expected value. Standard OLS standard errors would be biased. We therefore compute heteroskedasticity-robust standard errors using the HC1 sandwich estimator. The variance-covariance matrix of the estimated parameters is given by:\n$$ \\widehat{\\text{Cov}}_{\\text{HC1}}(\\widehat{\\boldsymbol{\\theta}}) = \\frac{M}{M-p} (X^T X)^{-1} (X^T \\hat{\\Omega} X) (X^T X)^{-1} $$\nwhere $M=200$ is the number of observations, $p=2$ is the number of parameters ($\\alpha$ and $\\beta$), and $\\hat{\\Omega}$ is a diagonal matrix with the squared OLS residuals, $\\hat{e}_j^2 = (z_j^2 - (\\widehat{\\alpha} + \\widehat{\\beta} l_j))^2$, on its diagonal.\n\nThe standard errors for the intercept and slope, $\\widehat{\\sigma}_{\\widehat{\\alpha}}$ and $\\widehat{\\sigma}_{\\widehat{\\beta}}$, are the square roots of the diagonal elements of this covariance matrix:\n$$ \\widehat{\\sigma}_{\\widehat{\\alpha}} = \\sqrt{\\widehat{\\text{Cov}}_{\\text{HC1}}(\\widehat{\\boldsymbol{\\theta}})_{1,1}} $$\n$$ \\widehat{\\sigma}_{\\widehat{\\beta}} = \\sqrt{\\widehat{\\text{Cov}}_{\\text{HC1}}(\\widehat{\\boldsymbol{\\theta}})_{2,2}} $$\n\n**4. Statistical Hypothesis Testing**\n\nWe perform two separate two-sided $t$-tests using the estimates and their robust standard errors. The tests use a $t$-distribution with $df = M-p = 200-2 = 198$ degrees of freedom.\n\n- **Intercept Test**: To test for inflation beyond what is expected under the null polygenic model, we test $H_0: \\alpha = 1$ against $H_1: \\alpha \\neq 1$. The $t$-statistic is:\n  $$ t_{\\alpha} = \\frac{\\widehat{\\alpha} - 1}{\\widehat{\\sigma}_{\\widehat{\\alpha}}} $$\n  The corresponding $p$-value, $p_{\\alpha}$, is calculated, and the null hypothesis is rejected if $p_{\\alpha}  \\delta = 0.01$.\n\n- **Slope Test**: To test for a significant polygenic signal (i.e., correlation between heritability and LD), we test $H_0: \\beta = 0$ against $H_1: \\beta \\neq 0$. The $t$-statistic is:\n  $$ t_{\\beta} = \\frac{\\widehat{\\beta} - 0}{\\widehat{\\sigma}_{\\widehat{\\beta}}} $$\n  The $p$-value, $p_{\\beta}$, is calculated, and the null hypothesis is rejected if $p_{\\beta}  \\eta = 0.05$.\n\n**5. Diagnostic Metrics Calculation**\n\nWe compute three additional metrics for each case:\n\n- **Mean Chi-Square Statistic**: This is the average of the squared z-scores, a measure of overall inflation in the test statistics.\n  $$ \\overline{\\chi^2} = \\frac{1}{M} \\sum_{j=1}^M z_j^2 $$\n\n- **Attenuation Ratio ($R$)**: This ratio quantifies what proportion of the observed inflation (beyond the null expectation of $1$) is attributable to the baseline inflation captured by the intercept. It is defined as:\n  $$ R = \\begin{cases} \\frac{\\widehat{\\alpha} - 1}{\\overline{\\chi^2} - 1}  \\text{if } \\overline{\\chi^2} > 1 \\\\ 0  \\text{if } \\overline{\\chi^2} \\le 1 \\end{cases} $$\n\n- **Coefficient of Determination ($R^2$)**: This measures the proportion of the variance in $z_j^2$ that is predictable from $l_j$.\n  $$ R^2 = 1 - \\frac{\\sum_j \\hat{e}_j^2}{\\sum_j (z_j^2 - \\overline{z_j^2})^2} $$\n  where $\\overline{z_j^2}$ is the mean of the $z_j^2$ values.\n\n- **Spearman Correlation ($\\rho$)**: We compute the Spearman rank correlation $\\rho$ between the absolute values of the residuals, $|\\hat{e}_j|$, and the LD scores, $l_j$. This assesses if the magnitude of model error is monotonically related to LD score, a potential sign of model misspecification.\n\n**6. Rule-Based Classifier**\n\nFinally, we use the computed statistics to infer plausible explanations for any significant inflation in the intercept, encoded as an integer bitmask $B$.\n\n- **Bit 1 (value $1$) is set if**: The intercept test is significant ($p_{\\alpha}  0.01$), the slope test is not significant ($p_{\\beta} \\ge 0.05$), and the attenuation ratio is high ($R \\ge 0.7$). This pattern suggests uniform inflation (e.g., from cryptic relatedness) dominating any polygenic signal.\n\n- **Bit 2 (value $2$) is set if**: The intercept test is significant ($p_{\\alpha}  0.01$), the slope test is also significant ($p_{\\beta}  0.05$), and there is evidence of model misspecification, indicated by a low coefficient of determination ($R^2  0.3$) or a substantial correlation between residual magnitude and LD score ($\\rho \\ge 0.3$). This can point to issues like LD score mismatch between the reference panel and study sample.\n\nThe final bitmask $B$ is the sum of the values of the triggered bits. The output for each case is the list $[\\widehat{\\alpha}, \\widehat{\\sigma}_{\\widehat{\\alpha}}, R, B]$.",
            "answer": "[[1.020084534720937, 0.005393457591668478, 0.4497645003503831, 0],[1.1993413997637848, 0.005404172464731032, 0.9998054694931349, 1],[1.025732168931165, 0.03845942702787836, 0.22223849734324523, 2],[0.9994508498818961, 0.005403043842603417, 0.0, 0]]"
        }
    ]
}