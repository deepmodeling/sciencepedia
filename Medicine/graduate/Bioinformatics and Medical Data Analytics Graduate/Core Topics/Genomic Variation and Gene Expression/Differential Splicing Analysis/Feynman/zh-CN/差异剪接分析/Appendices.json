{
    "hands_on_practices": [
        {
            "introduction": "任何可变剪接分析的第一步都是根据原始测序数据准确定量剪接事件。本练习将探讨“剪接包含百分比”（Percent Spliced In, PSI 或 $\\Psi$）这一核心指标的计算，同时利用两种不同类型的证据：跨越剪接点的读段（junction reads）和转录本丰度。本练习强调了对多重比对不明确性和检测偏差等常见技术伪影进行校正的必要性，为理解剪接定量的来源奠定了坚实的基础。",
            "id": "4556822",
            "problem": "单个人类基因 $G$ 发生了一个外显子跳跃（SE）事件：一个盒式外显子 $E$ 位于组成性外显子 $A$（上游）和 $B$（下游）之间。有四个已注释的转录本：$T_1$ 和 $T_2$ 包含 $E$（包含型异构体），而 $T_3$ 和 $T_4$ 跳过 $E$（跳跃型异构体）。您获得了同一样本的两个独立数据视图：跨剪接点读段证据和转录本丰度估计。\n\n您必须使用的基本定义：\n- 对于一个SE事件，剪接包含百分比（Percent Spliced In, PSI），记为 $\\Psi$，定义为在所有包含或跳过该盒式外显子的成熟信使核糖核酸（mRNA）分子中，包含该外显子的分子所占的比例。\n- 基于剪接点的证据源于跨越外显子-外显子剪接点的读段。对于一个SE事件，包含型分子可以为上游到外显子的剪接点和外显子到下游的剪接点生成跨剪接点的片段，而跳跃型分子则为上游到下游的跳跃剪接点生成片段。假设文库插入片段大小和读段长度使得包含型分子对两个包含型剪接点的可及性相同。\n- 模糊比对的读段根据其后验权重（即源自相关位点/剪接点的概率）进行按比例分配。每个剪接点的序列特异性检测偏好由一个乘法因子 $b$ 表示，该因子会扭曲观测到的计数值；无偏校正是将计数值除以 $b$。\n\n剪接点证据数据：\n- 上游到外显子的剪接点 ($A \\rightarrow E$)：唯一比对的读段 $C_{UE}^{\\mathrm{uniq}} = 120$，模糊读段 $C_{UE}^{\\mathrm{amb}} = 20$，其到该基因的后验权重为 $p_{UE} = 0.6$，偏好因子 $b_{UE} = 0.85$。\n- 外显子到下游的剪接点 ($E \\rightarrow B$)：$C_{ED}^{\\mathrm{uniq}} = 100$， $C_{ED}^{\\mathrm{amb}} = 10$，后验权重 $p_{ED} = 0.5$，偏好因子 $b_{ED} = 1.10$。\n- 上游到下游的跳跃剪接点 ($A \\rightarrow B$)：$C_{UD}^{\\mathrm{uniq}} = 80$， $C_{UD}^{\\mathrm{amb}} = 40$，后验权重 $p_{UD} = 0.25$，偏好因子 $b_{UD} = 0.95$。\n\n转录本丰度数据（来自一个报告每百万转录本数（TPM）和总结序列组成效应的每个转录本偏好因子的标准转录本定量器）。对于 $i \\in \\{1,2,3,4\\}$，令 $A_i$ 表示未校正的TPM， $g_i$ 表示转录本特异性偏好因子；无偏校正是除以 $g_i$。\n- $T_1$ (包含型): $A_1 = 18$, $g_1 = 0.90$.\n- $T_2$ (包含型): $A_2 = 12$, $g_2 = 1.10$.\n- $T_3$ (跳跃型): $A_3 = 15$, $g_3 = 0.95$.\n- $T_4$ (跳跃型): $A_4 = 5$, $g_4 = 0.85$.\n\n任务：\n- 仅使用上述基本定义，首先从第一性原理出发，通过恰当地组合来自两个支持包含事件的剪接点和一个支持跳跃事件的剪接点的、经过偏好校正和按比例分配的剪接点计数，为该SE事件推导出一个无偏的基于剪接点的包含比例估计量 $\\Psi_{\\mathrm{J}}$。\n- 独立地，通过恰当地组合所有包含型异构体和所有相关异构体的、经过偏好校正的TPM值，推导出一个无偏的基于转录本的$\\Psi$估计量 $\\Psi_{\\mathrm{T}}$。\n- 根据提供的数据和您推导出的估计量，计算 $\\Psi_{\\mathrm{J}}$ 和 $\\Psi_{\\mathrm{T}}$，然后报告绝对差异 $D = |\\Psi_{\\mathrm{T}} - \\Psi_{\\mathrm{J}}|$ 作为您的最终数值答案。\n\n将您的最终答案四舍五入到四位有效数字。将最终答案表示为无单位的小数（不要使用百分号）。",
            "solution": "所述问题在科学上是合理的、定义明确且客观的。它基于可变剪接的RNA测序数据分析的既定原则，特别是从两种不同的数据模态——跨剪接点读段和转录本丰度估计——计算剪接包含百分比（PSI）指标。推导和计算所需的所有数据和定义均已提供。因此，该问题是有效的，可以构建解决方案。\n\n总体目标是计算PSI（记为$\\Psi$）的两个估计量，并找出它们之间的绝对差异。$\\Psi$ 定义为包含特定盒式外显子 $E$ 的mRNA分子所占的比例。\n$$\n\\Psi = \\frac{\\text{包含型异构体的丰度}}{\\text{包含型异构体的丰度} + \\text{跳跃型异构体的丰度}}\n$$\n\n**第1部分：基于剪接点的估计量 $\\Psi_{\\mathrm{J}}$ 的推导与计算**\n\n基于剪接点的估计量使用跨越外显子-外显子剪接点的读段计数作为异构体丰度的代理。所提供的数据必须针对比对模糊性和检测偏好进行校正。\n\n首先，对于每个剪接点 $j$，我们通过组合唯一比对的读段 $C_{j}^{\\mathrm{uniq}}$ 和使用其后验权重 $p_{j}$ 按比例分配的模糊读段 $C_{j}^{\\mathrm{amb}}$，来计算有效读段计数 $C_{j}^{\\mathrm{eff}}$：\n$$\nC_{j}^{\\mathrm{eff}} = C_{j}^{\\mathrm{uniq}} + C_{j}^{\\mathrm{amb}} p_{j}\n$$\n接下来，我们通过用给定的乘法偏好因子 $b_{j}$ 校正有效计数来获得无偏强度度量 $I_{j}$。根据问题定义，此校正是通过除法进行的：\n$$\nI_{j} = \\frac{C_{j}^{\\mathrm{eff}}}{b_{j}}\n$$\n\n跳跃事件由单个剪接点支持，即上游到下游的跳跃剪接点（$A \\rightarrow B$），用下标 $UD$ 表示。其无偏强度 $I_{\\mathrm{skip}}$ 为：\n$$\nI_{\\mathrm{skip}} = \\frac{C_{UD}^{\\mathrm{uniq}} + C_{UD}^{\\mathrm{amb}} p_{UD}}{b_{UD}}\n$$\n代入给定值：\n$$\nI_{\\mathrm{skip}} = \\frac{80 + 40 \\times 0.25}{0.95} = \\frac{80 + 10}{0.95} = \\frac{90}{0.95} \\approx 94.73684\n$$\n\n包含事件由两个剪接点支持：上游外显子到盒式外显子的剪接点（$A \\rightarrow E$，下标 $UE$）和盒式外显子到下游外显子的剪接点（$E \\rightarrow B$，下标 $ED$）。我们首先计算它们各自的无偏强度 $I_{UE}$ 和 $I_{ED}$：\n$$\nI_{UE} = \\frac{C_{UE}^{\\mathrm{uniq}} + C_{UE}^{\\mathrm{amb}} p_{UE}}{b_{UE}} = \\frac{120 + 20 \\times 0.6}{0.85} = \\frac{120 + 12}{0.85} = \\frac{132}{0.85} \\approx 155.29412\n$$\n$$\nI_{ED} = \\frac{C_{ED}^{\\mathrm{uniq}} + C_{ED}^{\\mathrm{amb}} p_{ED}}{b_{ED}} = \\frac{100 + 10 \\times 0.5}{1.10} = \\frac{100 + 5}{1.10} = \\frac{105}{1.10} \\approx 95.45455\n$$\n问题陈述指出这两个包含型剪接点是“同等可及的”。将这两个来源的证据合并为单一的包含强度度量 $I_{\\mathrm{incl}}$ 的一种稳健方法是取其平均值。这可以缓和特定于某个剪接点的任何测量噪声或未建模偏好的影响。\n$$\nI_{\\mathrm{incl}} = \\frac{1}{2} (I_{UE} + I_{ED}) = \\frac{1}{2} \\left( \\frac{132}{0.85} + \\frac{105}{1.10} \\right) \\approx \\frac{1}{2} (155.29412 + 95.45455) \\approx 125.37433\n$$\n基于剪接点的PSI估计量 $\\Psi_{\\mathrm{J}}$ 是包含强度与总强度（包含强度加跳跃强度）的比率：\n$$\n\\Psi_{\\mathrm{J}} = \\frac{I_{\\mathrm{incl}}}{I_{\\mathrm{incl}} + I_{\\mathrm{skip}}} \\approx \\frac{125.37433}{125.37433 + 94.73684} \\approx \\frac{125.37433}{220.11117} \\approx 0.5695936\n$$\n\n**第2部分：基于转录本的估计量 $\\Psi_{\\mathrm{T}}$ 的推导与计算**\n\n基于转录本的估计量使用以每百万转录本数（$TPM$）为单位的定量转录本丰度。这些丰度值（转录本 $T_i$ 的丰度为 $A_i$）必须根据其各自的偏好因子 $g_i$ 进行校正。通过将 $A_i$ 除以 $g_i$ 来获得无偏的相对丰度 $A'_i$：\n$$\nA'_i = \\frac{A_i}{g_i}\n$$\n包含型异构体（$T_1$ 和 $T_2$）的总校正丰度是它们各自校正丰度的总和：\n$$\nA'_{\\mathrm{incl}} = A'_{1} + A'_{2} = \\frac{A_1}{g_1} + \\frac{A_2}{g_2}\n$$\n代入给定值：\n$$\nA'_{\\mathrm{incl}} = \\frac{18}{0.90} + \\frac{12}{1.10} = 20 + 10.90909... = 30.90909...\n$$\n类似地，跳跃型异构体（$T_3$ 和 $T_4$）的总校正丰度是：\n$$\nA'_{\\mathrm{skip}} = A'_{3} + A'_{4} = \\frac{A_3}{g_3} + \\frac{A_4}{g_4}\n$$\n代入给定值：\n$$\nA'_{\\mathrm{skip}} = \\frac{15}{0.95} + \\frac{5}{0.85} \\approx 15.78947... + 5.88235... = 21.67182...\n$$\n基于转录本的PSI估计量 $\\Psi_{\\mathrm{T}}$ 是总校正包含丰度与所有参与该事件的异构体的总校正丰度之比：\n$$\n\\Psi_{\\mathrm{T}} = \\frac{A'_{\\mathrm{incl}}}{A'_{\\mathrm{incl}} + A'_{\\mathrm{skip}}} \\approx \\frac{30.90909...}{30.90909... + 21.67182...} \\approx \\frac{30.90909...}{52.58091...} \\approx 0.5878345\n$$\n\n**第3部分：绝对差异的最终计算**\n\n最后的任务是计算两个估计量之间的绝对差异 $D$：\n$$\nD = |\\Psi_{\\mathrm{T}} - \\Psi_{\\mathrm{J}}|\n$$\n使用计算出的值：\n$$\nD \\approx |0.5878345 - 0.5695936| \\approx 0.0182409\n$$\n按要求将结果四舍五入到四位有效数字，得到 $0.01824$。",
            "answer": "$$\n\\boxed{0.01824}\n$$"
        },
        {
            "introduction": "在量化剪接之后，下一步是检验不同实验条件之间是否存在显著差异。一个常见的陷阱是将基因总表达量的变化与剪接模式的真实变化相混淆。本练习将演示如何使用带有适当偏移量（offset）的广义线性模型（Generalized Linear Model, GLM），从而正确地将差异剪接效应与基因丰度变化的混淆因素分离开来，这是进行稳健统计推断的关键一步。",
            "id": "4556820",
            "problem": "您正在使用核糖核酸测序 (RNA-Seq) 技术分析单个基因中的一个盒式外显子事件。该基因有两个亚型：一个包含该外显子（包含型亚型），另一个则跳过它（跳跃型亚型）。在每个样本中，记录了外显子包含和跳跃的计数，基因水平的总计数是包含计数和跳跃计数的总和。存在两种生物学条件：对照组和处理组，每种条件下有两个样本。四个样本记录的计数如下：\n- 对照组样本1：包含数 $y_{1} = 120$，跳跃数 $k_{1} = 80$，总数 $n_{1} = 200$。\n- 对照组样本2：包含数 $y_{2} = 180$，跳跃数 $k_{2} = 120$，总数 $n_{2} = 300$。\n- 处理组样本1：包含数 $y_{3} = 240$，跳跃数 $k_{3} = 160$，总数 $n_{3} = 400$。\n- 处理组样本2：包含数 $y_{4} = 360$，跳跃数 $k_{4} = 240$，总数 $n_{4} = 600$。\n\n生物学上的真实情况是，亚型的相对使用率（包含比例）在不同条件下保持不变；与对照组相比，处理组中所有转录本的计数都成比例地增加了。然而，如果进行事件水平的分析，仅使用条件指示变量（而不控制基因水平的总数）对包含数进行泊松回归拟合，将会错误地推断出存在条件效应，因为处理组中的原始包含数更大。\n\n为了在事件水平上正确地检验差异性剪接，使用带有对数连接函数和考虑了基因水平暴露度的样本特异性偏移量的泊松广义线性模型 (GLM, Generalized Linear Model) 对包含数进行建模。设 $y_{i}$ 表示样本 $i$ 中的包含数，$n_{i}$ 表示样本 $i$ 中基因水平的总数，$x_{i} \\in \\{0,1\\}$ 表示条件指示变量（$0$ 代表对照组，$1$ 代表处理组）。拟合模型\n$$\n\\ln\\!\\big(\\mu_{i}\\big) \\;=\\; \\alpha \\;+\\; \\beta\\, x_{i} \\;+\\; \\ln\\!\\big(n_{i}\\big),\n$$\n其中 $\\mu_{i}$ 是期望包含数，$\\alpha$ 是一个截距，捕捉在给定暴露度下的基线包含水平，$\\beta$ 是超出暴露度影响之外的条件对包含数的影响，而 $\\ln(n_{i})$ 是偏移量。\n\n假设泊松均值-方差关系精确成立，并将偏移量视为已知。在此模型和所提供的数据下，计算条件效应参数 $\\hat{\\beta}$ 的最大似然估计。请将最终答案表示为一个纯数（无量纲）。无需四舍五入；请提供精确值。",
            "solution": "用户要求计算指定泊松广义线性模型 (GLM) 中条件效应参数 $\\beta$ 的最大似然估计 (MLE)。\n\n首先，对问题进行验证。\n\n### 第1步：提取已知条件\n- **4个样本 ($i=1, 2, 3, 4$) 的数据：**\n    - 对照组样本1 ($i=1$)：包含数 $y_{1} = 120$，总数 $n_{1} = 200$。条件指示变量 $x_1 = 0$。\n    - 对照组样本2 ($i=2$)：包含数 $y_{2} = 180$，总数 $n_{2} = 300$。条件指示变量 $x_2 = 0$。\n    - 处理组样本1 ($i=3$)：包含数 $y_{3} = 240$，总数 $n_{3} = 400$。条件指示变量 $x_3 = 1$。\n    - 处理组样本2 ($i=4$)：包含数 $y_{4} = 360$，总数 $n_{4} = 600$。条件指示变量 $x_4 = 1$。\n- **统计模型：**\n    - 包含数 $y_{i}$ 被建模为来自泊松分布的抽样，$y_{i} \\sim \\text{Poisson}(\\mu_{i})$。\n    - 期望包含数 $\\mu_{i}$ 通过对数连接函数与协变量相关联：\n    $$\n    \\ln(\\mu_{i}) = \\alpha + \\beta x_{i} + \\ln(n_{i})\n    $$\n    其中 $\\alpha$ 是截距，$\\beta$ 是条件效应，$x_{i}$ 是条件指示变量，$\\ln(n_{i})$ 是样本特异性偏移量。\n\n### 第2步：使用提取的已知条件进行验证\n- **科学依据**：该问题具有科学依据。使用带有偏移项的泊松GLM来建模RNA测序的计数数据，是生物信息学中分析差异表达和剪接的标准且成熟的方法。\n- **适定性**：该问题是适定的。它提供了估计目标参数所需的所有数据和完全指定的模型。任务是找到最大似然估计，这是一个唯一定义的目标。\n- **客观性**：问题以客观、技术性的语言陈述，没有偏见或主观论断。\n- 验证结论是，该问题是健全、完整和一致的。\n\n### 第3步：结论与行动\n问题有效。将提供解答。\n\n### 解答推导\n\n期望包含数 $\\mu_{i}$ 的模型由下式给出：\n$$\n\\ln(\\mu_{i}) = \\alpha + \\beta x_{i} + \\ln(n_{i})\n$$\n这可以重写为：\n$$\n\\mu_{i} = \\exp(\\alpha + \\beta x_{i} + \\ln(n_{i})) = n_{i} \\exp(\\alpha + \\beta x_{i})\n$$\n均值为 $\\mu_{i}$ 的泊松分布随机变量 $y_{i}$ 的概率质量函数为：\n$$\nP(Y_{i} = y_{i}) = \\frac{\\mu_{i}^{y_{i}} \\exp(-\\mu_{i})}{y_{i}!}\n$$\n单个观测值 $y_{i}$ 的对数似然函数为：\n$$\n\\ell_{i}(\\alpha, \\beta) = y_{i} \\ln(\\mu_{i}) - \\mu_{i} - \\ln(y_{i}!)\n$$\n代入 $\\ln(\\mu_{i})$ 的表达式：\n$$\n\\ell_{i}(\\alpha, \\beta) = y_{i}(\\alpha + \\beta x_{i} + \\ln(n_{i})) - n_{i} \\exp(\\alpha + \\beta x_{i}) - \\ln(y_{i}!)\n$$\n所有 $N=4$ 个观测值的总对数似然函数是各个对数似然函数之和：\n$$\n\\ell(\\alpha, \\beta) = \\sum_{i=1}^{4} \\ell_{i}(\\alpha, \\beta) = \\sum_{i=1}^{4} \\left[ y_{i}(\\alpha + \\beta x_{i} + \\ln(n_{i})) - n_{i} \\exp(\\alpha + \\beta x_{i}) \\right] - \\sum_{i=1}^{4} \\ln(y_{i}!)\n$$\n为了找到最大似然估计值（$\\hat{\\alpha}$，$\\hat{\\beta}$），我们对对数似然函数求关于 $\\alpha$ 和 $\\beta$ 的偏导数，并令它们为零。这些就是得分方程。\n\n关于 $\\alpha$ 的偏导数为：\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{i=1}^{4} \\left[ y_{i} - n_{i} \\exp(\\alpha + \\beta x_{i}) \\right] = \\sum_{i=1}^{4} (y_{i} - \\mu_{i})\n$$\n令其为零，得到第一个得分方程：\n$$\n\\sum_{i=1}^{4} y_{i} = \\sum_{i=1}^{4} \\hat{\\mu}_{i} = \\sum_{i=1}^{4} n_{i} \\exp(\\hat{\\alpha} + \\hat{\\beta} x_{i})\n$$\n关于 $\\beta$ 的偏导数为：\n$$\n\\frac{\\partial \\ell}{\\partial \\beta} = \\sum_{i=1}^{4} \\left[ y_{i}x_{i} - x_{i}n_{i} \\exp(\\alpha + \\beta x_{i}) \\right] = \\sum_{i=1}^{4} x_{i}(y_{i} - \\mu_{i})\n$$\n令其为零，得到第二个得分方程：\n$$\n\\sum_{i=1}^{4} x_{i}y_{i} = \\sum_{i=1}^{4} x_{i}\\hat{\\mu}_{i} = \\sum_{i=1}^{4} x_{i}n_{i} \\exp(\\hat{\\alpha} + \\hat{\\beta} x_{i})\n$$\n现在，我们将提供的数据代入得分方程。\n数据是：\n$y = (120, 180, 240, 360)$\n$n = (200, 300, 400, 600)$\n$x = (0, 0, 1, 1)$\n\n让我们先解第二个得分方程，因为它更简单：\n$$\n\\sum_{i=1}^{4} x_{i}y_{i} = (0)(120) + (0)(180) + (1)(240) + (1)(360) = 600\n$$\n$$\n\\sum_{i=1}^{4} x_{i}n_{i} \\exp(\\hat{\\alpha} + \\hat{\\beta} x_{i}) = (0)n_{1}\\exp(\\hat{\\alpha}) + (0)n_{2}\\exp(\\hat{\\alpha}) + (1)n_{3}\\exp(\\hat{\\alpha}+\\hat{\\beta}) + (1)n_{4}\\exp(\\hat{\\alpha}+\\hat{\\beta})\n$$\n$$\n= (n_{3} + n_{4}) \\exp(\\hat{\\alpha}+\\hat{\\beta}) = (400 + 600) \\exp(\\hat{\\alpha}+\\hat{\\beta}) = 1000 \\exp(\\hat{\\alpha}+\\hat{\\beta})\n$$\n将两部分相等：\n$$\n600 = 1000 \\exp(\\hat{\\alpha}+\\hat{\\beta}) \\implies \\exp(\\hat{\\alpha}+\\hat{\\beta}) = \\frac{600}{1000} = \\frac{3}{5}\n$$\n现在，让我们使用第一个得分方程：\n$$\n\\sum_{i=1}^{4} y_{i} = 120 + 180 + 240 + 360 = 900\n$$\n$$\n\\sum_{i=1}^{4} n_{i} \\exp(\\hat{\\alpha} + \\hat{\\beta} x_{i}) = n_{1}\\exp(\\hat{\\alpha}) + n_{2}\\exp(\\hat{\\alpha}) + n_{3}\\exp(\\hat{\\alpha}+\\hat{\\beta}) + n_{4}\\exp(\\hat{\\alpha}+\\hat{\\beta})\n$$\n$$\n= (n_{1}+n_{2})\\exp(\\hat{\\alpha}) + (n_{3}+n_{4})\\exp(\\hat{\\alpha}+\\hat{\\beta})\n$$\n$$\n= (200+300)\\exp(\\hat{\\alpha}) + (400+600)\\exp(\\hat{\\alpha}+\\hat{\\beta}) = 500\\exp(\\hat{\\alpha}) + 1000\\exp(\\hat{\\alpha}+\\hat{\\beta})\n$$\n将两部分相等，并代入第二个方程的结果：\n$$\n900 = 500\\exp(\\hat{\\alpha}) + 1000\\left(\\frac{3}{5}\\right)\n$$\n$$\n900 = 500\\exp(\\hat{\\alpha}) + 600\n$$\n$$\n300 = 500\\exp(\\hat{\\alpha}) \\implies \\exp(\\hat{\\alpha}) = \\frac{300}{500} = \\frac{3}{5}\n$$\n我们现在得到了 $\\exp(\\hat{\\alpha})$ 和 $\\exp(\\hat{\\alpha}+\\hat{\\beta})$ 的估计值：\n$$\n\\exp(\\hat{\\alpha}) = \\frac{3}{5}\n$$\n$$\n\\exp(\\hat{\\alpha}+\\hat{\\beta}) = \\frac{3}{5}\n$$\n为了求出 $\\hat{\\beta}$，我们可以使用指数的性质 $\\exp(a+b) = \\exp(a)\\exp(b)$：\n$$\n\\exp(\\hat{\\alpha}+\\hat{\\beta}) = \\exp(\\hat{\\alpha})\\exp(\\hat{\\beta})\n$$\n$$\n\\frac{3}{5} = \\left(\\frac{3}{5}\\right)\\exp(\\hat{\\beta})\n$$\n两边同除以 $3/5$ 得：\n$$\n\\exp(\\hat{\\beta}) = 1\n$$\n对两边取自然对数，得到 $\\beta$ 的最大似然估计：\n$$\n\\hat{\\beta} = \\ln(1) = 0\n$$\n这个结果与数据是一致的，在数据中，包含比例 $y_{i}/n_{i}$ 在所有样本中都是恒定的：$120/200 = 180/300 = 240/400 = 360/600 = 0.6$。条件效应 $\\beta=0$ 表明对照组和处理组之间的包含数对数比例没有变化，这与数据所显示的情况完全相符。",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "差异剪接分析不仅可以用于检验单个事件，还可以用于发现样本间的全局模式。本练习将进入无监督学习的领域，展示如何根据样本的整体剪接图谱对其进行聚类。练习强调了使用基于统计学原理的距离度量的重要性，该度量需要考虑 $\\Psi$ 值所特有的统计属性，如其固有的异方差性，从而实现有意义的生物学发现。",
            "id": "4556866",
            "problem": "给定多个样本队列，每个队列都由一组固定的外显子跳跃事件的剪接百分比 (Percent Spliced In, PSI) 值向量来表征。对于索引为 $i$ 的样本和索引为 $j$ 的外显子事件，PSI 估计值是闭区间 $[0,1]$ 内的一个分数，可以解释为源于包含（inclusion）和排除（exclusion）读数计数的二项分布比例。您的任务是根据样本的 PSI 向量，定义、论证并实现一种有原则的距离度量，然后执行层次聚类以识别基于剪接的亚型。\n\n从一个基本前提开始：用于包含和排除的 RNA 测序读数计数可以建模为二项试验，并且 PSI 是一个比例估计量，由于因子 $\\psi(1-\\psi)$ 的存在，其在单位区间上的方差具有异方差性。您必须推导出一个满足以下条件的距离度量：\n- 经过方差稳定化处理，使得在边界 $0$ 和 $1$ 附近的差异相对于中间范围的值不会被过分降低或增加权重，\n- 通过整合基于跨样本计算的稳健统计量的逐外显子离散度权重，使其对外显子异常值和无信息外显子具有稳健性，\n- 能以一种科学合理且不引入偏差的方式处理 PSI 中的缺失值（在此问题中表示为非数字），\n- 与一种在欧几里得空间中具有明确定义的目标函数、并能产生紧凑且方差同质的簇的层次聚类连接方法兼容。\n\n您必须将这些推理整合到一个算法中，该算法为每个队列生成一个划分为预定数量簇的划分。\n\n为实现自动化评估，请实现以下具体、完全指定的实例：\n1. 对于每个外显子 $j$，使用在该队列中观测到的值计算的外显子样本中位数来填补缺失的 PSI 值。\n2. 对每个 PSI 条目应用适用于二项分布比例的方差稳定化变换，以获得变换后的值 $y_{i,j}$；使用标准的反正弦平方根变换（此处未提供明确公式，以鼓励从第一性原理进行推理）。\n3. 计算每个外显子 $j$ 的稳健离散度权重，即 $\\{y_{i,j}\\}_{i}$ 在样本间的中位数绝对偏差 (MAD)，加上一个小的正常数 $\\epsilon$ 以避免权重为零，并归一化权重使其总和等于外显子数量 $p$。使用 $\\epsilon = 10^{-3}$。\n4. 通过将每个特征 $j$ 按其权重的平方根进行缩放，构建一个加权欧几里得几何。在此几何中，使用 Ward 连接准则对样本进行层次聚类。\n5. 给定队列所需簇的数量 $K$，切割树状图以精确生成 $K$ 个簇。根据簇在样本索引顺序 $i=0,1,\\dots,n-1$ 中首次出现的次序，将簇标签映射到整数 $\\{0,1,\\dots,K-1\\}$。\n\n给定三个队列作为测试套件。在每个队列中，PSI 值是无单位的分数。缺失值表示为非数字（not-a-number）。对于每个队列 $c \\in \\{1,2,3\\}$，给定一个 $n_c \\times p_c$ 的 PSI 矩阵 $P^{(c)}$ 和一个目标簇数 $K^{(c)}$。您的程序必须独立处理每个队列，并按顺序输出每个队列的簇标签列表。\n\n测试套件：\n- 队列 $1$ 有 $n_1=6$ 个样本和 $p_1=8$ 个外显子。PSI 矩阵 $P^{(1)}$ 的行（样本 $0$ 到 $5$）如下：\n  - 样本 $0$：$[0.82,0.71,0.58,0.93,0.18,0.27,0.39,0.12]$\n  - 样本 $1$：$[0.78,0.69,0.62,0.88,0.22,0.33,0.41,0.09]$\n  - 样本 $2$：$[0.81,0.73,0.59,0.91,0.21,0.31,0.38,0.15]$\n  - 样本 $3$：$[0.19,0.29,0.43,0.11,0.81,0.72,0.64,0.88]$\n  - 样本 $4$：$[0.23,0.34,0.39,0.09,0.77,0.68,0.58,0.92]$\n  - 样本 $5$：$[0.17,0.27,0.41,0.13,0.83,0.73,0.61,0.87]$\n  使用 $K^{(1)}=2$。\n- 队列 $2$ 有 $n_2=4$ 个样本和 $p_2=5$ 个外显子。PSI 矩阵 $P^{(2)}$ 的行如下：\n  - 样本 $0$：$[0.5,0.5,0.5,0.5,0.5]$\n  - 样本 $1$：$[0.5,0.5,0.5,0.5,0.5]$\n  - 样本 $2$：$[0.5,0.5,0.5,0.5,0.5]$\n  - 样本 $3$：$[0.5,0.5,0.5,0.5,0.5]$\n  使用 $K^{(2)}=1$。\n- 队列 $3$ 有 $n_3=5$ 个样本和 $p_3=7$ 个外显子。PSI 矩阵 $P^{(3)}$ 的行如下：\n  - 样本 $0$：$[0.85,0.78,\\mathrm{NaN},0.52,0.18,0.22,0.60]$\n  - 样本 $1$：$[0.80,0.75,\\mathrm{NaN},0.49,0.20,0.25,0.58]$\n  - 样本 $2$：$[0.25,0.30,0.55,\\mathrm{NaN},0.82,0.75,0.40]$\n  - 样本 $3$：$[0.28,0.33,0.58,\\mathrm{NaN},0.78,0.70,0.42]$\n  - 样本 $4$：$[0.55,\\mathrm{NaN},0.20,0.85,0.45,\\mathrm{NaN},0.15]$\n  使用 $K^{(3)}=3$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个簇标签列表，格式为方括号括起来的逗号分隔列表，每个队列的标签按样本索引的顺序列出。例如，输出必须类似于 $[[\\dots],[\\dots],[\\dots]]$，不含空格。\n- 每个标签列表必须包含整数 $\\{0,1,\\dots,K^{(c)}-1\\}$，并按上述指定的首次出现顺序重新映射。\n\n此问题中的所有数值输入均为无单位分数，由方差稳定化变换引入的所有角度均隐式地以弧度为单位。最终输出仅包含整数标签，因此无需为输出指定单位。",
            "solution": "该问题陈述被认为是有效的，因为它提出了一个基于生物信息学和统计学既定原则的、科学上合理、定义明确且客观的任务。它为生物样本聚类算法提供了完整、自洽的规范，并包含了执行所需的所有数据。所描述的方法是分析可变剪接数据的标准工作流程。因此，我们可以着手提供完整的解决方案。\n\n总体目标是根据外显子剪接模式将生物样本队列划分为不同的亚型，这些模式通过剪接百分比 (PSI 或 $\\psi$) 值进行量化。PSI 值是比例值，其统计特性要求一个精心设计的分析流程。规定的算法包含五个主要步骤，每个步骤都有其基本原理论证。\n\n### 步骤 1：缺失值的填补\n\n生物数据集由于技术性假象或某些样本中特定事件的读数覆盖不足，常常包含缺失值。问题规定，缺失的 PSI 值（表示为非数字 $\\mathrm{NaN}$）应使用该特定外显子在队列中所有其他样本中的 PSI 中位数进行填补。设 $P^{(c)}$ 为队列 $c$ 的 PSI 矩阵，其条目为 $\\psi_{i,j}$，代表样本 $i$ 和外显子 $j$。如果 $\\psi_{i,j}$ 缺失，则替换为 $\\hat{\\psi}_{i,j} = \\mathrm{median}(\\{\\psi_{k,j} \\mid \\psi_{k,j} \\text{ 是已观测值，对于队列 } c \\text{ 中的样本 } k\\})。\n\n选择中位数是经过深思熟虑的。作为一种稳健的集中趋势度量，中位数对极端异常值不敏感，而剪接数据中可能存在这类异常值。这可以防止单个异常样本扭曲填补值，从而比均值填补引入更少的偏差。\n\n### 步骤 2：方差稳定化变换 (VST)\n\nPSI 值 $\\psi$ 是二项分布比例的一个估计量。这类估计量的方差依赖于比例本身：$\\mathrm{Var}(\\hat{\\psi}) \\propto \\psi(1-\\psi)$。这一特性被称为异方差性，意味着方差在 $\\psi=0.5$ 时达到最大，而在边界 $\\psi=0$ 和 $\\psi=1$ 附近接近于零。像 Ward 法这样基于欧几里得距离的标准聚类方法，在数据范围内的方差恒定（方差齐性）时表现最佳。若不进行变换，PSI 值在边界附近的差异将相对于在范围中心处的相同差异被不适当地降低权重。\n\n为解决此问题，我们应用方差稳定化变换。问题建议使用反正弦平方根变换，这是针对二项分布比例的标准 VST。对于每个已填补的 PSI 值 $\\psi_{i,j}$，变换后的值 $y_{i,j}$ 计算如下：\n$$y_{i,j} = \\arcsin(\\sqrt{\\psi_{i,j}})$$\n角度以弧度为单位。统计学中的德尔塔方法 (delta method) 的一个结果表明，对于一个随机变量 $X \\sim \\mathrm{Binomial}(N, p)$，变换后的变量 $Y = \\arcsin(\\sqrt{X/N})$ 的方差近似恒定且独立于 $p$：\n$$\\mathrm{Var}(Y) \\approx \\frac{1}{4N}$$\n其中 $N$ 是试验总次数（在此背景下，与测序读数深度相关）。通过稳定方差，此变换确保了后续聚类步骤中使用的距离度量能够可比地处理剪接水平的差异，无论其在 $[0,1]$ 区间内的位置如何。\n\n### 步骤 3：稳健的逐外显子加权\n\n并非所有外显子在区分生物亚型方面都具有同等的信息量。一些外显子在所有样本中可能几乎没有变异，因此对聚类无用。另一些外显子则可能表现出高变异性，而这是不同样本组的特征。该算法采用一种加权方案来增加这些信息丰富的外显子的权重。\n\n每个外显子 $j$ 的权重源自其在样本间的离散度。为确保对异常样本的稳健性，离散度通过中位数绝对偏差 (MAD) 来衡量，这是一个稳健的统计量，定义为：\n$$\\mathrm{MAD}_j = \\mathrm{median}_i( | y_{i,j} - \\mathrm{median}_k(y_{k,j}) | )$$\n其中中位数是在队列中所有样本 $i,k$ 上计算的。向 MAD 添加一个小的正常数 $\\epsilon = 10^{-3}$，以防止零离散度（即在所有样本中恒定）的外显子权重为零，从而被完全从分析中移除。外显子 $j$ 的原始权重为 $w'_j = \\mathrm{MAD}_j + \\epsilon$。\n\n然后对这些原始权重进行归一化，使其总和等于外显子总数 $p$：\n$$w_j = w'_j \\cdot \\frac{p}{\\sum_{k=1}^p w'_k}$$\n这种归一化在保持总“信息预算”的同时，根据外显子经稳健方法测得的信息量，在它们之间重新分配权重。\n\n### 步骤 4：在加权欧几里得空间中进行层次聚类\n\n有了经过变换和加权的数据，我们现在可以对样本进行聚类。该过程可概念化为在加权欧几里得空间中执行聚类。两个索引为 $a$ 和 $b$ 的样本之间的平方距离定义为：\n$$d^2(a, b) = \\sum_{j=1}^{p} w_j (y_{a,j} - y_{b,j})^2$$\n这等效于首先将每个变换后的特征列 $j$ 按其权重的平方根 $\\sqrt{w_j}$ 进行缩放，然后在该缩放空间中计算标准欧几里得距离。设缩放后的数据矩阵为 $Z$，其中 $Z_{i,j} = y_{i,j} \\sqrt{w_j}$。\n\n问题指定使用 Ward 连接准则进行凝聚型层次聚类。Ward 方法旨在合并那些能导致总簇内平方和增量最小的簇对。其目标函数是最小化簇内的总方差。这种方法倾向于产生紧凑的球形簇，这在识别不同样本组时通常是一个理想的属性。聚类过程会产生一个树状图，这是一种表示样本和簇嵌套合并过程的树形结构。\n\n### 步骤 5：树状图分割与标签分配\n\n最后一步是将每个队列 $c$ 的样本划分为预定数量的簇 $K^{(c)}$。这是通过在能产生恰好 $K^{(c)}$ 个不同簇的水平上“切割”树状图来实现的。\n\n聚类算法为每个簇分配一个任意的整数标签。为确保输出的确定性和标准化，这些标签会重新映射到集合 $\\{0, 1, \\dots, K^{(c)}-1\\}$。重映射规则基于每个簇的成员在原始样本顺序（$i=0, 1, \\dots, n-1$）中首次出现的次序。第一个遇到的簇被分配标签 $0$，第二个遇到的新簇被分配标签 $1$，依此类推。这样可以为每个队列生成一个唯一且可复现的簇分配列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.cluster.hierarchy import ward, fcluster\n\ndef solve():\n    \"\"\"\n    Main function to run the splicing analysis on all test cohorts.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"P\": np.array([\n                [0.82, 0.71, 0.58, 0.93, 0.18, 0.27, 0.39, 0.12],\n                [0.78, 0.69, 0.62, 0.88, 0.22, 0.33, 0.41, 0.09],\n                [0.81, 0.73, 0.59, 0.91, 0.21, 0.31, 0.38, 0.15],\n                [0.19, 0.29, 0.43, 0.11, 0.81, 0.72, 0.64, 0.88],\n                [0.23, 0.34, 0.39, 0.09, 0.77, 0.68, 0.58, 0.92],\n                [0.17, 0.27, 0.41, 0.13, 0.83, 0.73, 0.61, 0.87]\n            ]),\n            \"K\": 2\n        },\n        {\n            \"P\": np.array([\n                [0.5, 0.5, 0.5, 0.5, 0.5],\n                [0.5, 0.5, 0.5, 0.5, 0.5],\n                [0.5, 0.5, 0.5, 0.5, 0.5],\n                [0.5, 0.5, 0.5, 0.5, 0.5]\n            ]),\n            \"K\": 1\n        },\n        {\n            \"P\": np.array([\n                [0.85, 0.78, np.nan, 0.52, 0.18, 0.22, 0.60],\n                [0.80, 0.75, np.nan, 0.49, 0.20, 0.25, 0.58],\n                [0.25, 0.30, 0.55, np.nan, 0.82, 0.75, 0.40],\n                [0.28, 0.33, 0.58, np.nan, 0.78, 0.70, 0.42],\n                [0.55, np.nan, 0.20, 0.85, 0.45, np.nan, 0.15]\n            ]),\n            \"K\": 3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        P, K = case[\"P\"], case[\"K\"]\n        result = process_cohort(P, K)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef process_cohort(P, K):\n    \"\"\"\n    Processes a single cohort according to the specified algorithm.\n    \"\"\"\n    n, p = P.shape\n    epsilon = 1e-3\n\n    # Step 1: Impute missing PSI values with the exon-wise sample median.\n    P_imputed = np.copy(P)\n    # np.nanmedian ignores NaNs by default.\n    col_medians = np.nanmedian(P_imputed, axis=0)\n    \n    # Find indices of NaNs and replace them with corresponding column medians\n    nan_inds = np.where(np.isnan(P_imputed))\n    P_imputed[nan_inds] = np.take(col_medians, nan_inds[1])\n\n    # Step 2: Apply the arcsine-square-root variance-stabilizing transformation.\n    # The domain of arcsin is [-1, 1], and sqrt of PSI [0,1] is in [0,1], so this is safe.\n    Y = np.arcsin(np.sqrt(P_imputed))\n\n    # Step 3: Compute robust dispersion weights per exon.\n    # We implement MAD manually to ensure it's unscaled, as specified.\n    # MAD_j = median_i( |y_ij - median_k(y_kj)| )\n    y_medians = np.median(Y, axis=0, keepdims=True)\n    abs_deviations = np.abs(Y - y_medians)\n    mad_weights = np.median(abs_deviations, axis=0)\n\n    # Add epsilon and normalize weights to sum to p.\n    raw_weights = mad_weights + epsilon\n    normalized_weights = raw_weights * (p / np.sum(raw_weights))\n\n    # Step 4: Form a weighted geometry and perform hierarchical clustering.\n    # Scale each feature j by the square root of its weight.\n    Z = Y * np.sqrt(normalized_weights)\n    \n    # Perform hierarchical clustering using Ward's linkage.\n    # Ward's linkage operates on the condensed distance matrix or the original data.\n    # scipy.cluster.hierarchy.ward takes the n x m observation matrix.\n    linkage_matrix = ward(Z)\n\n    # Step 5: Cut the dendrogram and remap cluster labels.\n    # fcluster returns labels from 1 to K.\n    raw_labels = fcluster(linkage_matrix, t=K, criterion='maxclust')\n\n    # Remap labels to {0, 1, ..., K-1} by order of first occurrence.\n    final_labels = np.zeros(n, dtype=int)\n    mapping = {}\n    next_label = 0\n    for i in range(n):\n        raw_label = raw_labels[i]\n        if raw_label not in mapping:\n            mapping[raw_label] = next_label\n            next_label += 1\n        final_labels[i] = mapping[raw_label]\n        \n    return final_labels.tolist()\n\n\nsolve()\n\n```"
        }
    ]
}