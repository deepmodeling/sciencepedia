{
    "hands_on_practices": [
        {
            "introduction": "Creating an accurate electromyography (EMG) envelope is a cornerstone of biomechanical analysis, but the choice of filter can introduce subtle yet critical timing errors. This practice explores how different digital filters—and how they are applied—affect the signal's phase, directly impacting the measurement of muscle activation onset . By comparing Infinite Impulse Response (IIR) and Finite Impulse Response (FIR) filters and the effect of zero-phase filtering, you will develop the skills to preserve the temporal integrity of your data.",
            "id": "4170148",
            "problem": "A biomechanics laboratory studies Electromyography (EMG) signals and computes EMG envelopes by full-wave rectification followed by low-pass filtering to suppress carrier fluctuations and retain motor-unit recruitment dynamics. Consider discrete-time, Linear Time-Invariant (LTI) filtering of the rectified EMG sampled at rate $f_s$ to form the envelope, and onset detection by threshold crossing on the filtered envelope. Two families of filters are used in practice: Infinite Impulse Response (IIR) low-pass designs (for example, Butterworth) and Finite Impulse Response (FIR) low-pass designs with symmetric coefficients. The laboratory is considering four implementations for the envelope stage: (i) single-pass causal IIR, (ii) single-pass causal linear-phase FIR, (iii) forward–backward application of the IIR, and (iv) forward–backward application of the FIR. In forward–backward filtering, the signal is filtered forward in time and then filtered backward in time by the same filter, producing a noncausal operation often termed zero-phase filtering.\n\nUsing only foundational definitions for LTI systems—namely, the discrete-time frequency response $H(e^{j\\omega})$, its phase $\\phi(\\omega) = \\arg H(e^{j\\omega})$, the group delay $\\tau_g(\\omega) = -\\frac{d\\phi(\\omega)}{d\\omega}$, and the impulse response symmetry condition for linear-phase FIR filters—reason about phase response and onset timing bias. Select all statements that are correct.\n\nA. A causal IIR low-pass filter with real coefficients generally has nonlinear phase, so $\\tau_g(\\omega)$ depends on $\\omega$; forward–backward application cancels phase because the net frequency response is $H(e^{j\\omega})H(e^{-j\\omega})$, which has zero net phase and magnitude equal to $\\lvert H(e^{j\\omega}) \\rvert^2$, thus removing onset timing bias due to phase delay.\n\nB. A symmetric FIR low-pass filter has zero group delay for all $\\omega$, hence single-pass causal application does not shift EMG onset times.\n\nC. Applying forward–backward filtering to a linear-phase FIR yields a noncausal, zero-phase filter whose magnitude response equals the original magnitude response, not its square.\n\nD. For any stable causal low-pass filter, the group delay is constant and equals half the filter order, so onset timing bias is independent of frequency content.\n\nE. In threshold-based onset detection using a single-pass causal linear-phase FIR of length $N$ samples at sampling rate $f_s$, the detected onset time is shifted by a constant $\\Delta t = \\frac{N-1}{2 f_s}$ relative to the same detection applied to the noncausal zero-phase version of the same filtering, independent of the waveform shape of the envelope, because linear phase implies a pure constant delay in addition to magnitude shaping.\n\nSelect all that apply.",
            "solution": "The problem statement has been validated and found to be scientifically sound, well-posed, and objective. It provides a correct and standard description of digital signal processing techniques used in biomechanics for EMG signal analysis. We may proceed to evaluate the given statements.\n\nThe analysis will be based on the fundamental principles of discrete-time Linear Time-Invariant (LTI) systems. Let an LTI filter be characterized by its impulse response $h[n]$ and its Discrete-Time Fourier Transform (DTFT), the frequency response $H(e^{j\\omega}) = \\sum_{n=-\\infty}^{\\infty} h[n]e^{-j\\omega n}$. The frequency response can be written in polar form as $H(e^{j\\omega}) = \\lvert H(e^{j\\omega}) \\rvert e^{j\\phi(\\omega)}$, where $\\lvert H(e^{j\\omega}) \\rvert$ is the magnitude response and $\\phi(\\omega) = \\arg H(e^{j\\omega})$ is the phase response. The group delay is defined as $\\tau_g(\\omega) = -\\frac{d\\phi(\\omega)}{d\\omega}$ and represents the delay experienced by a narrow band of frequencies centered at $\\omega$.\n\nFor any filter with a real-valued impulse response $h[n]$, the frequency response exhibits conjugate symmetry: $H(e^{j\\omega}) = H^*(e^{-j\\omega})$. This implies an even magnitude response, $\\lvert H(e^{j\\omega}) \\rvert = \\lvert H(e^{-j\\omega}) \\rvert$, and an odd phase response, $\\phi(\\omega) = -\\phi(-\\omega)$.\n\nForward-backward filtering consists of a forward pass followed by a time-reversed backward pass with the same filter $h[n]$. If the input is $x[n]$, the output of the forward pass is $y_f[n] = x[n]*h[n]$. In the frequency domain, $Y_f(e^{j\\omega}) = X(e^{j\\omega})H(e^{j\\omega})$. The backward pass filters the time-reversed signal $y_f[-n]$. The final output $y_{fb}[n]$ has a frequency-domain representation $Y_{fb}(e^{j\\omega}) = X(e^{j\\omega}) H_{fb}(e^{j\\omega})$, where the effective frequency response is $H_{fb}(e^{j\\omega}) = H(e^{j\\omega})H(e^{-j\\omega})$. For a filter with real $h[n]$, this becomes $H_{fb}(e^{j\\omega}) = H(e^{j\\omega})H^*(e^{j\\omega}) = \\lvert H(e^{j\\omega}) \\rvert^2$. This response is purely real and non-negative, meaning its phase is zero for all frequencies ($\\phi_{fb}(\\omega) = 0$).\n\nNow, we evaluate each statement.\n\n**A. A causal IIR low-pass filter with real coefficients generally has nonlinear phase, so $\\tau_g(\\omega)$ depends on $\\omega$; forward–backward application cancels phase because the net frequency response is $H(e^{j\\omega})H(e^{-j\\omega})$, which has zero net phase and magnitude equal to $\\lvert H(e^{j\\omega}) \\rvert^2$, thus removing onset timing bias due to phase delay.**\n\n- A stable, causal IIR filter with real coefficients cannot have exact linear phase. Its rational transfer function structure with poles not at the origin or infinity leads to a nonlinear phase response $\\phi(\\omega)$. Consequently, the group delay $\\tau_g(\\omega) = -\\frac{d\\phi(\\omega)}{d\\omega}$ is a function of frequency $\\omega$. This causes phase distortion or dispersion, where different frequency components of the signal are delayed by different amounts.\n- As derived above, forward-backward filtering results in an effective frequency response of $H_{fb}(e^{j\\omega}) = H(e^{j\\omega})H(e^{-j\\omega})$. For a real IIR filter, this simplifies to $H_{fb}(e^{j\\omega}) = \\lvert H(e^{j\\omega}) \\rvert^2$. This new transfer function is purely real, meaning its phase is zero for all $\\omega$.\n- A zero-phase filter has zero group delay for all frequencies. It does not introduce any time delay or phase distortion. Therefore, it completely eliminates the timing bias (phase-induced delay) that would have been introduced by the original IIR filter, including the frequency-dependent part. The statement accurately describes the properties of IIR filters and the outcome of the forward-backward filtering technique.\n- Verdict: **Correct**.\n\n**B. A symmetric FIR low-pass filter has zero group delay for all $\\omega$, hence single-pass causal application does not shift EMG onset times.**\n\n- This statement refers to a causal linear-phase FIR filter with symmetric coefficients (implementation (ii)). Let the filter have length $N$ and a symmetric impulse response $h[n] = h[N-1-n]$ for $n=0, 1, ..., N-1$. Such a filter has a generalized linear phase response given by $\\phi(\\omega) = -\\alpha \\omega$, where $\\alpha = \\frac{N-1}{2}$ is a constant.\n- The group delay is $\\tau_g(\\omega) = -\\frac{d}{d\\omega}(-\\alpha \\omega) = \\alpha = \\frac{N-1}{2}$ samples. This delay is constant for all frequencies, not zero (unless $N=1$, a trivial filter).\n- Because the group delay is a non-zero constant, the filter introduces a pure time shift of $\\frac{N-1}{2}$ samples to the signal. This shift will affect the detected onset time. The statement that the group delay is zero is false.\n- Verdict: **Incorrect**.\n\n**C. Applying forward–backward filtering to a linear-phase FIR yields a noncausal, zero-phase filter whose magnitude response equals the original magnitude response, not its square.**\n\n- The forward-backward filtering process is inherently noncausal because the backward pass requires access to future values of the forward-filtered signal. As shown in the general derivation, the resulting effective filter has zero phase.\n- The effective magnitude response is $\\lvert H_{fb}(e^{j\\omega}) \\rvert = \\lvert \\lvert H(e^{j\\omega}) \\rvert^2 \\rvert = \\lvert H(e^{j\\omega}) \\rvert^2$. This is the square of the original filter's magnitude response. The statement claims the magnitude response is equal to the original, not its square. This is incorrect. The resulting passband will be flatter and the stopband will have greater attenuation (if the original attenuation was, say, $-40$ dB, the new attenuation will be $-80$ dB), but the magnitude response function itself is squared.\n- Verdict: **Incorrect**.\n\n**D. For any stable causal low-pass filter, the group delay is constant and equals half the filter order, so onset timing bias is independent of frequency content.**\n\n- This statement makes a false generalization. The property of constant group delay is characteristic of linear-phase filters, which are typically implemented as FIR filters. As established in the analysis of option A, stable causal IIR filters generally do not have linear phase, and therefore have a group delay that varies with frequency.\n- The statement claims this property holds for \"any stable causal low-pass filter,\" which is false. For example, a Butterworth IIR filter is a stable causal low-pass filter, but its group delay is not constant. Therefore, the premise is invalid, and the conclusion that timing bias is independent of frequency content does not hold true for all such filters.\n- Verdict: **Incorrect**.\n\n**E. In threshold-based onset detection using a single-pass causal linear-phase FIR of length $N$ samples at sampling rate $f_s$, the detected onset time is shifted by a constant $\\Delta t = \\frac{N-1}{2 f_s}$ relative to the same detection applied to the noncausal zero-phase version of the same filtering, independent of the waveform shape of the envelope, because linear phase implies a pure constant delay in addition to magnitude shaping.**\n\n- A single-pass causal linear-phase FIR filter of length $N$ possesses a constant group delay of $\\tau_g = \\frac{N-1}{2}$ samples.\n- A constant group delay implies that all frequency components of the signal are delayed by the same amount of time. This results in a perfect time-shift of the entire waveform without any shape distortion (phase dispersion). The magnitude response still shapes the signal's spectrum, but this is decoupled from the time shift.\n- The total time delay in seconds is the group delay in samples divided by the sampling rate $f_s$, which is $\\Delta t = \\frac{\\tau_g}{f_s} = \\frac{(N-1)/2}{f_s} = \\frac{N-1}{2f_s}$.\n- A \"noncausal zero-phase version\" of the filtering implies an operation with zero group delay. The relative shift between the causal linear-phase output and the zero-phase output is therefore exactly this constant delay $\\Delta t$.\n- Because the delay is uniform for all frequencies, the shape of the EMG envelope is preserved, just shifted in time. Thus, a threshold crossing on the filtered signal will occur at a time that is delayed by $\\Delta t$ compared to the zero-phase filtered signal, regardless of the envelope's specific shape.\n- Verdict: **Correct**.",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "Quantifying the level of muscle activity is fundamental, but how do we choose the best metric? This exercise delves into the statistical underpinnings of two common amplitude estimators, the Root Mean Square (RMS) and the Average Rectified Value (ARV) . By deriving their properties from first principles and examining their behavior in the presence of signal bursts, you will gain insight into their relative robustness and the importance of matching your processing tools to the statistical nature of the EMG signal.",
            "id": "4170133",
            "problem": "An intramuscular electromyography (EMG) signal, after appropriate high-pass filtering and baseline correction, can be modeled over an analysis window as a zero-mean, wide-sense stationary random process. Consider a scalar sample $x$ drawn from the stationary distribution. Define two amplitude functionals commonly used in EMG signal processing: the Root Mean Square (RMS) and the Average Rectified Value (ARV). Assume that during steady activity (no bursts), $x$ is distributed as a zero-mean Gaussian with variance $\\sigma^{2}$.\n\nStarting from fundamental probability definitions, including the definitions of expectation, variance, and the Gaussian probability density, and without assuming any pre-derived special-case expectations, derive the relationship between the RMS and the variance for a zero-mean process, and compute the ARV for a zero-mean Gaussian in terms of $\\sigma$.\n\nIn EMG recordings, transient bursts can occur, leading to intermittent increases in amplitude. Model this deviation as a mixture in which, independently for each sample, with probability $1-p$ the sample is drawn from $\\mathcal{N}(0,\\sigma^{2})$ (baseline) and with probability $p$ it is drawn from $\\mathcal{N}(0,b^{2}\\sigma^{2})$ (burst), where $b>1$ and $0<p<1$. Consider the infinite-window limit so that time averages equal ensemble expectations.\n\nDefine a Gaussian-calibrated ARV-based amplitude estimator $\\hat{\\sigma}_{\\mathrm{ARV}}=c_{\\mathrm{ARV}}\\ \\mathbb{E}[|x|]$ whose calibration constant $c_{\\mathrm{ARV}}$ makes it unbiased for $\\sigma$ under the baseline Gaussian model. Use the RMS functional $\\hat{\\sigma}_{\\mathrm{RMS}}=\\sqrt{\\mathbb{E}[x^{2}]}$ as the RMS-based amplitude estimator. Under the burst-mixture model, derive the biases of $\\hat{\\sigma}_{\\mathrm{ARV}}$ and $\\hat{\\sigma}_{\\mathrm{RMS}}$ relative to $\\sigma$, and then provide a single closed-form analytic expression for the ratio of their absolute biases as a function of $p$ and $b$.\n\nReport only this ratio as your final answer as a simplified analytic expression in terms of $p$ and $b$. No numerical approximation is required, and no units are to be reported.",
            "solution": "We begin from foundational definitions. For any real-valued random variable $x$ with finite second moment, the variance is defined by\n$$\n\\mathrm{Var}[x] \\equiv \\mathbb{E}\\big[(x-\\mathbb{E}[x])^{2}\\big] = \\mathbb{E}[x^{2}] - \\big(\\mathbb{E}[x]\\big)^{2}.\n$$\nThe Root Mean Square (RMS) functional is defined as\n$$\n\\mathrm{RMS}(x) \\equiv \\sqrt{\\mathbb{E}[x^{2}]}.\n$$\nFor a zero-mean process, $\\mathbb{E}[x]=0$, so $\\mathrm{Var}[x]=\\mathbb{E}[x^{2}]$. Therefore, for zero-mean,\n$$\n\\mathrm{RMS}(x) = \\sqrt{\\mathbb{E}[x^{2}]} = \\sqrt{\\mathrm{Var}[x]}.\n$$\n\nNext, we compute the Average Rectified Value (ARV) for a zero-mean Gaussian. Let $x \\sim \\mathcal{N}(0,\\sigma^{2})$ with density\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right).\n$$\nBy definition,\n$$\n\\mathrm{ARV} = \\mathbb{E}[|x|] = \\int_{-\\infty}^{\\infty} |x|\\,f(x)\\,dx.\n$$\nBy symmetry,\n$$\n\\mathbb{E}[|x|] = 2 \\int_{0}^{\\infty} x \\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)\\,dx.\n$$\nEvaluate the integral via the substitution $u = \\frac{x^{2}}{2\\sigma^{2}}$, so $du = \\frac{x}{\\sigma^{2}}\\,dx$ and $x\\,dx = \\sigma^{2}\\,du$, yielding\n$$\n\\mathbb{E}[|x|] = \\frac{2}{\\sqrt{2\\pi}\\,\\sigma} \\int_{0}^{\\infty} x \\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right) dx\n= \\frac{2}{\\sqrt{2\\pi}\\,\\sigma} \\int_{0}^{\\infty} \\sigma^{2} \\exp(-u)\\,du\n= \\frac{2\\sigma^{2}}{\\sqrt{2\\pi}\\,\\sigma} \\cdot 1\n= \\sigma \\sqrt{\\frac{2}{\\pi}}.\n$$\n\nWe now define the Gaussian-calibrated ARV-based amplitude estimator as $\\hat{\\sigma}_{\\mathrm{ARV}} = c_{\\mathrm{ARV}}\\,\\mathbb{E}[|x|]$ and choose $c_{\\mathrm{ARV}}$ such that it is unbiased under the baseline Gaussian model. Under $x \\sim \\mathcal{N}(0,\\sigma^{2})$,\n$$\n\\mathbb{E}\\big[\\hat{\\sigma}_{\\mathrm{ARV}}\\big] = c_{\\mathrm{ARV}}\\,\\mathbb{E}[|x|] = c_{\\mathrm{ARV}}\\,\\sigma \\sqrt{\\frac{2}{\\pi}}.\n$$\nUnbiasedness for $\\sigma$ requires $c_{\\mathrm{ARV}}\\,\\sigma \\sqrt{\\frac{2}{\\pi}} = \\sigma$, hence\n$$\nc_{\\mathrm{ARV}} = \\sqrt{\\frac{\\pi}{2}}.\n$$\nThe RMS-based estimator is $\\hat{\\sigma}_{\\mathrm{RMS}} = \\sqrt{\\mathbb{E}[x^{2}]}$. For zero-mean Gaussian, $\\mathbb{E}[x^{2}] = \\sigma^{2}$ so $\\hat{\\sigma}_{\\mathrm{RMS}} = \\sigma$.\n\nWe now introduce burst contamination. Let $x$ be drawn from the mixture\n$$\nx \\sim (1-p)\\,\\mathcal{N}(0,\\sigma^{2}) + p\\,\\mathcal{N}(0,b^{2}\\sigma^{2}),\n$$\nwith $0<p<1$ and $b>1$. In the infinite-window limit, time averages equal expectations with respect to this mixture. We compute the RMS and ARV functionals under the mixture, then apply the Gaussian calibration.\n\nFirst, the RMS functional under the mixture:\n$$\n\\hat{\\sigma}_{\\mathrm{RMS}} = \\sqrt{\\mathbb{E}[x^{2}]} = \\sqrt{(1-p)\\,\\mathbb{E}[x^{2}\\mid \\mathcal{N}(0,\\sigma^{2})] + p\\,\\mathbb{E}[x^{2}\\mid \\mathcal{N}(0,b^{2}\\sigma^{2})]}.\n$$\nFor a zero-mean Gaussian with variance $\\tau^{2}$, $\\mathbb{E}[x^{2}]=\\tau^{2}$. Hence\n$$\n\\hat{\\sigma}_{\\mathrm{RMS}} = \\sqrt{(1-p)\\,\\sigma^{2} + p\\,b^{2}\\sigma^{2}} = \\sigma\\,\\sqrt{(1-p) + p\\,b^{2}}.\n$$\nThe bias of $\\hat{\\sigma}_{\\mathrm{RMS}}$ relative to $\\sigma$ is\n$$\n\\mathrm{Bias}_{\\mathrm{RMS}} = \\hat{\\sigma}_{\\mathrm{RMS}} - \\sigma = \\sigma\\left(\\sqrt{(1-p)+p\\,b^{2}} - 1\\right).\n$$\n\nSecond, the ARV functional under the mixture:\n$$\n\\mathbb{E}[|x|] = (1-p)\\,\\mathbb{E}[|x|\\mid \\mathcal{N}(0,\\sigma^{2})] + p\\,\\mathbb{E}[|x|\\mid \\mathcal{N}(0,b^{2}\\sigma^{2})].\n$$\nFor $x \\sim \\mathcal{N}(0,\\tau^{2})$, we have shown $\\mathbb{E}[|x|] = \\tau \\sqrt{\\frac{2}{\\pi}}$. Thus\n$$\n\\mathbb{E}[|x|] = (1-p)\\,\\sigma \\sqrt{\\frac{2}{\\pi}} + p\\,(b\\sigma) \\sqrt{\\frac{2}{\\pi}} = \\sigma \\sqrt{\\frac{2}{\\pi}}\\,\\big((1-p) + p\\,b\\big).\n$$\nApplying the Gaussian calibration $c_{\\mathrm{ARV}}=\\sqrt{\\frac{\\pi}{2}}$, the ARV-based estimator under the mixture is\n$$\n\\hat{\\sigma}_{\\mathrm{ARV}} = \\sqrt{\\frac{\\pi}{2}}\\ \\mathbb{E}[|x|] = \\sqrt{\\frac{\\pi}{2}}\\ \\sigma \\sqrt{\\frac{2}{\\pi}}\\,\\big((1-p) + p\\,b\\big) = \\sigma\\,\\big((1-p) + p\\,b\\big).\n$$\nTherefore, the bias of $\\hat{\\sigma}_{\\mathrm{ARV}}$ relative to $\\sigma$ is\n$$\n\\mathrm{Bias}_{\\mathrm{ARV}} = \\hat{\\sigma}_{\\mathrm{ARV}} - \\sigma = \\sigma\\left((1-p) + p\\,b - 1\\right) = \\sigma\\,p\\,(b-1).\n$$\n\nWe are asked for the ratio of the absolute biases. Since $b>1$ and $0<p<1$, both biases are nonnegative, hence their absolute values equal their values. The ratio is\n$$\n\\frac{|\\mathrm{Bias}_{\\mathrm{ARV}}|}{|\\mathrm{Bias}_{\\mathrm{RMS}}|} = \\frac{\\sigma\\,p\\,(b-1)}{\\sigma\\left(\\sqrt{(1-p)+p\\,b^{2}} - 1\\right)} = \\frac{p\\,(b-1)}{\\sqrt{(1-p)+p\\,b^{2}} - 1}.\n$$\nThis is a closed-form analytic expression in terms of $p$ and $b$ as required.",
            "answer": "$$\\boxed{\\frac{p\\,(b-1)}{\\sqrt{(1-p)+p\\,b^{2}}-1}}$$"
        },
        {
            "introduction": "Moving beyond the aggregate EMG envelope, advanced analysis seeks to decompose the signal into the firing patterns of individual motor units. This practice introduces the powerful framework of convolutional sparse coding, showing how a physiologically-inspired generative model can be translated into a solvable optimization problem . By deriving the Maximum A Posteriori (MAP) objective, you will understand how the principles of sparsity and statistical inference enable us to \"unmix\" superimposed neural commands.",
            "id": "4170100",
            "problem": "An Electromyography (EMG) time series $x[n]$ recorded from a single differential electrode over a muscle is modeled as the linear superposition of Motor Unit Action Potential (MUAP) template responses convolved with latent, unit-specific spike trains, corrupted by additive noise. Let $\\{h_i[n]\\}_{i=1}^{K}$ denote fixed MUAP templates (finite-length impulse responses) associated with $K$ distinct motor units, and let $\\{s_i[n]\\}_{i=1}^{K}$ denote their corresponding nonnegative latent spike trains. Assume the following generative model in discrete time:\n$$\nx[n] = \\sum_{i=1}^{K} (h_i * s_i)[n] + w[n],\n$$\nwhere $*$ denotes linear convolution and $w[n]$ is additive noise. Assume $w[n]$ is a realization of a zero-mean, independent and identically distributed Gaussian process with variance $\\sigma^{2}$, that is, $w[n] \\sim \\mathcal{N}(0,\\sigma^{2})$ independently across $n$. Further assume that the spike trains $\\{s_i[n]\\}$ are independent across $i$ and have a Laplace (double-exponential) prior promoting sparsity, that is,\n$$\np(s_i) \\propto \\exp\\!\\big(-\\alpha \\lVert s_i \\rVert_{1}\\big),\n$$\nwith $\\alpha>0$ a fixed hyperparameter and $\\lVert s_i \\rVert_{1} \\triangleq \\sum_{n} |s_i[n]|$.\n\nStarting only from these modeling assumptions and the core definitions of linear convolution, Gaussian likelihood, Laplace prior, and Maximum A Posteriori (MAP) estimation, perform the following:\n\n- Using the independence assumptions, write the posterior $p(\\{s_i\\}\\,|\\,x)$ up to a proportionality constant, then take the negative logarithm to obtain the MAP objective for $\\{s_i\\}$ in terms of $x$, the templates $\\{h_i\\}$, and the spike trains $\\{s_i\\}$. Eliminate additive and multiplicative constants that do not depend on $\\{s_i\\}$, and express the objective as a sum of a data fidelity term and a sparsity-inducing regularizer. Introduce a single regularization parameter $\\lambda>0$ that collects the dependence on $\\sigma^{2}$ and $\\alpha$ into one scalar weight.\n- Explain, using first principles from sparse representation theory and properties of the $\\ell_{1}$ norm, how sparsity in $\\{s_i\\}$ promotes separation of temporally overlapping MUAPs when the templates $\\{h_i\\}$ are not identical and exhibit limited mutual coherence. Your explanation should reference the role of the triangle inequality for $\\ell_{1}$, the reduced support of sparse solutions, and the qualitative effect of mutual coherence on identifiability, without relying on unproven shortcut results.\n\nAs your final answer, provide the analytic expression for the resulting convolutional sparse coding MAP objective in its simplest form, written as a function of $\\{s_i\\}$, $x$, $\\{h_i\\}$, and $\\lambda$. No numerical computation is required, and no units are to be reported in the final answer.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- EMG time series: $x[n]$\n- Generative model: $x[n] = \\sum_{i=1}^{K} (h_i * s_i)[n] + w[n]$\n- MUAP templates: $\\{h_i[n]\\}_{i=1}^{K}$, fixed, finite-length impulse responses\n- Latent spike trains: $\\{s_i[n]\\}_{i=1}^{K}$, nonnegative\n- Convolution operator: $*$\n- Additive noise: $w[n] \\sim \\mathcal{N}(0,\\sigma^{2})$, zero-mean, i.i.d. Gaussian with variance $\\sigma^2$\n- Independence of spike trains: $\\{s_i[n]\\}$ are independent across index $i$\n- Prior on spike trains: $p(s_i) \\propto \\exp(-\\alpha \\lVert s_i \\rVert_{1})$ with hyperparameter $\\alpha > 0$\n- Definition of $\\ell_1$ norm: $\\lVert s_i \\rVert_{1} \\triangleq \\sum_{n} |s_i[n]|$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard one in the field of biomedical signal processing, specifically in the domain of EMG decomposition. The generative model described is a well-established convolutional sparse coding (CSC) framework. The assumptions are physically and mathematically sound:\n- **Scientifically Grounded**: The model of an EMG signal as a superposition of convolved spike trains is a cornerstone of EMG analysis. The use of a Gaussian model for noise and a Laplace prior (which leads to $\\ell_1$ regularization) for promoting sparsity in the underlying neural drive is a standard and powerful technique in modern statistical signal processing and machine learning.\n- **Well-Posed**: The task is to derive an objective function and provide a conceptual explanation. The premises provided (likelihood and prior distributions) are sufficient and consistent for deriving the Maximum A Posteriori (MAP) objective.\n- **Objective**: The problem is stated using precise mathematical and technical language, with no subjective or ambiguous terminology.\n\nAll validation criteria are met. The problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Derivation of the MAP Objective Function\n\nThe goal is to find the Maximum A Posteriori (MAP) estimate of the latent spike trains $\\{s_i\\}_{i=1}^{K}$ given the observed EMG signal $x$. The MAP estimate, denoted $\\{\\hat{s}_i\\}$, maximizes the posterior probability $p(\\{s_i\\} | x)$. Using Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(\\{s_i\\}_{i=1}^{K} \\,|\\, x) \\propto p(x \\,|\\, \\{s_i\\}_{i=1}^{K}) \\, p(\\{s_i\\}_{i=1}^{K})\n$$\n\nFirst, we define the likelihood term $p(x \\,|\\, \\{s_i\\})$. The generative model is $x[n] = \\sum_{i=1}^{K} (h_i * s_i)[n] + w[n]$. This can be rewritten for the noise as $w[n] = x[n] - \\sum_{i=1}^{K} (h_i * s_i)[n]$. The problem states that $w[n]$ is a realization of an independent and identically distributed (i.i.d.) Gaussian process with zero mean and variance $\\sigma^2$, i.e., $w[n] \\sim \\mathcal{N}(0, \\sigma^2)$. The probability density function for a single noise sample is $p(w[n]) = (2\\pi\\sigma^2)^{-1/2} \\exp\\left(-\\frac{w[n]^2}{2\\sigma^2}\\right)$. Since the noise samples are independent across time $n$, the joint probability of the entire noise sequence $w = \\{w[n]\\}$ is the product of the individual probabilities. This gives the likelihood of observing data $x$ given the spike trains $\\{s_i\\}$:\n$$\np(x \\,|\\, \\{s_i\\}) = \\prod_{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{\\left(x[n] - \\sum_{i=1}^{K} (h_i * s_i)[n]\\right)^2}{2\\sigma^2} \\right)\n$$\nThis expression can be written more compactly. Taking the product of exponentials is equivalent to summing their arguments:\n$$\np(x \\,|\\, \\{s_i\\}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{n} \\left(x[n] - \\sum_{i=1}^{K} (h_i * s_i)[n]\\right)^2 \\right)\n$$\nThe sum is the squared Euclidean ($\\ell_2$) norm of the residual vector. Let the full signals be represented as vectors $x$ and $s_i$. The expression is:\n$$\np(x \\,|\\, \\{s_i\\}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} \\right)\n$$\nNext, we define the prior term $p(\\{s_i\\})$. The problem states that the spike trains $\\{s_i\\}$ are independent across motor units $i$, and each follows a Laplace prior $p(s_i) \\propto \\exp(-\\alpha \\lVert s_i \\rVert_1)$. The joint prior is the product of the individual priors:\n$$\np(\\{s_i\\}_{i=1}^{K}) = \\prod_{i=1}^{K} p(s_i) \\propto \\prod_{i=1}^{K} \\exp(-\\alpha \\lVert s_i \\rVert_1) = \\exp\\left(-\\alpha \\sum_{i=1}^{K} \\lVert s_i \\rVert_1\\right)\n$$\nCombining the likelihood and the prior, the posterior probability is:\n$$\np(\\{s_i\\} \\,|\\, x) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} \\right) \\exp\\left(-\\alpha \\sum_{i=1}^{K} \\lVert s_i \\rVert_1\\right)\n$$\n$$\np(\\{s_i\\} \\,|\\, x) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} - \\alpha \\sum_{i=1}^{K} \\lVert s_i \\rVert_1 \\right)\n$$\nMaximizing the posterior probability is equivalent to minimizing its negative logarithm. Let $J(\\{s_i\\})$ be this negative log-posterior objective function:\n$$\nJ(\\{s_i\\}) = -\\ln(p(\\{s_i\\} \\,|\\, x)) = C + \\frac{1}{2\\sigma^2} \\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} + \\alpha \\sum_{i=1}^{K} \\lVert s_i \\rVert_1\n$$\nwhere $C$ is a constant that does not depend on $\\{s_i\\}$. To find the MAP estimate $\\{\\hat{s}_i\\}$, we minimize this objective with respect to $\\{s_i\\}$. We can discard the constant $C$ and multiply the entire objective by a positive constant, $2\\sigma^2$, without changing the location of the minimum. This yields the final form of the MAP optimization problem:\n$$\n\\{\\hat{s}_i\\} = \\arg\\min_{\\{s_i\\}, s_i \\ge 0} \\left\\{ \\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} + 2\\sigma^2\\alpha \\sum_{i=1}^{K} \\lVert s_i \\rVert_1 \\right\\}\n$$\nThe problem also asks to introduce a single regularization parameter $\\lambda = 2\\sigma^2\\alpha$. Since $\\sigma^2 > 0$ and $\\alpha > 0$, we have $\\lambda > 0$. The resulting objective function is a sum of two terms: a data fidelity term and a sparsity-inducing regularizer. The fidelity term, $\\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2}$, penalizes deviations of the model from the observed data and arises from the Gaussian noise assumption. The regularizer, $\\lambda \\sum_{i=1}^{K} \\lVert s_i \\rVert_1$, penalizes non-sparse solutions and arises from the Laplace prior assumption. The nonnegativity constraint $s_i[n] \\ge 0$ is also imposed, as given.\n\n### Explanation of Sparsity for MUAP Separation\n\nThe MAP objective derived above is a form of convolutional sparse coding. The effectiveness of this framework in separating temporally overlapping MUAPs relies on the interplay between the properties of the $\\ell_1$-norm regularizer and the characteristics of the MUAP template dictionary $\\{h_i\\}$.\n\n1.  **Sparsity and Reduced Support**: The $\\ell_1$-norm, $\\lVert s_i \\rVert_1 = \\sum_n |s_i[n]|$, is a convex function that promotes sparsity. This means that minimizing an objective containing an $\\ell_1$ penalty encourages solutions for $\\{s_i\\}$ where many of the coefficients $s_i[n]$ are exactly zero. In the context of EMG, an event $s_i[n] > 0$ represents a firing of motor unit $i$ at time $n$. A sparse solution implies that the model explains the observed signal $x[n]$ using the fewest possible motor unit firings, which is consistent with the underlying physiology of neural control. The set of non-zero entries in a signal is its \"support\". The $\\ell_1$ penalty seeks solutions with reduced support.\n\n2.  **Role of Mutual Coherence**: The ability to distinguish between different MUAPs depends on how \"different\" their templates $\\{h_i\\}$ are. This is formalized by the concept of mutual coherence of the dictionary of templates. Low mutual coherence implies that any one template $h_i$ cannot be well-represented as a linear combination of other templates $\\{h_j\\}_{j\\neq i}$. When two or more MUAPs from different units (e.g., $i$ and $j$) fire nearly simultaneously, the observed signal is a superposition of their templates, e.g., $s_i[n_1]h_i[n-n_1] + s_j[n_2]h_j[n-n_2]$. If the templates $h_i$ and $h_j$ have low coherence with all other templates $h_k$, it is impossible to accurately represent this superposition using a single scaled template $s_k[n_k]h_k[n-n_k]$ without incurring a large error in the data fidelity term $\\lVert \\cdot \\rVert_2^2$. Therefore, the optimization process is forced to activate both $s_i$ and $s_j$ to achieve a good fit.\n\n3.  **Identifiability via $\\ell_1$ Minimization and Triangle Inequality**: Once the optimizer is forced to use a combination of templates to explain an overlapping event, the $\\ell_1$ penalty guides the selection towards the sparsest possible combination. Consider the true explanation of an overlapping segment $x_{seg}$ requires two spikes, represented by spike trains $s_i$ and $s_j$. The contribution to the regularization term is $\\lambda (\\lVert s_i \\rVert_1 + \\lVert s_j \\rVert_1)$. An incorrect, non-sparse explanation might involve small contributions from many different motor units, $\\{s'_k\\}_{k=1}^K$, to reconstruct the same segment $x_{seg}$. A fundamental property related to the $\\ell_1$ norm and the triangle inequality is that representing a signal with a few \"correct\" atomic elements is generally more efficient (i.e., has a smaller $\\ell_1$ norm on the coefficients) than representing it with a dense combination of many elements. The minimizer of the $\\ell_1$ norm is preferentially a sparse vector. Sparse representation theory guarantees that if the true underlying solution is sufficiently sparse and the template dictionary has sufficiently low mutual coherence, then solving the $\\ell_1$-regularized minimization problem is guaranteed to recover the correct sparse solution.\n\nIn summary, when MUAPs overlap, low template coherence creates a large reconstruction error for any overly simple (e.g., single-spike) explanation. The optimizer must therefore use a superposition of multiple templates. The $\\ell_1$ regularizer then ensures that this superposition is the sparsest one possible, favoring the true, physiologically meaningful set of distinct motor unit firings over a dense, un-interpretable alternative. This allows the algorithm to \"unmix\" the superimposed signals into their constituent spike trains.",
            "answer": "$$\n\\boxed{\\left\\lVert x - \\sum_{i=1}^{K} h_i * s_i \\right\\rVert_{2}^{2} + \\lambda \\sum_{i=1}^{K} \\left\\lVert s_i \\right\\rVert_{1}}\n$$"
        }
    ]
}