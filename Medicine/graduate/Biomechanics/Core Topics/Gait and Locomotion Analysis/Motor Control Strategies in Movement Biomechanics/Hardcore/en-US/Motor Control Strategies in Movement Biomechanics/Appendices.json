{
    "hands_on_practices": [
        {
            "introduction": "To control movement effectively, the central nervous system (CNS) must first form an accurate estimate of the body's state, such as the position of a limb. This is challenging because individual sensory inputs like vision and proprioception are inherently noisy and sometimes conflicting. This exercise explores how the brain can optimally solve this problem by applying Bayesian principles to fuse multiple sensory signals, weighting each input according to its reliability . By working through this problem, you will derive the mathematical foundation of sensory reweighting and gain insight into how the CNS achieves perceptual estimates that are more precise than any single sense could provide alone.",
            "id": "4192578",
            "problem": "A participant performs a one-dimensional reaching task in which the Central Nervous System (CNS) must estimate hand position along a single axis. Two independent sensory modalities provide unbiased measurements of the same underlying true position, each with additive Gaussian noise. Let the true position be denoted by $x$, and the observed measurements be $x_{1}$ (modality $1$) and $x_{2}$ (modality $2$). Assume $x_{1} \\sim \\mathcal{N}(x, \\sigma_{1}^{2})$ and $x_{2} \\sim \\mathcal{N}(x, \\sigma_{2}^{2})$, with independence between modalities and no systematic bias. Assume an uninformative prior on $x$.\n\nIn a particular trial, modality $1$ (e.g., proprioception) reports $x_{1} = 12$ mm and modality $2$ (e.g., vision) reports $x_{2} = 8$ mm. Their noise variances are $\\sigma_{1}^{2} = 25$ mm$^{2}$ and $\\sigma_{2}^{2} = 9$ mm$^{2}$, respectively.\n\nStarting from Bayesâ€™ rule and the Gaussian likelihood model, derive the posterior distribution $p(x \\mid x_{1}, x_{2})$ and identify its mean (the Bayesian fused estimate) and variance. Then compute the fused estimate and its variance for the given numerical values. Finally, explain how sensory reweighting arises from relative reliability within this framework and how it manifests in the fused estimate.\n\nExpress the fused estimate in millimeters and the fused variance in square millimeters. Provide the final numerical values as exact simplified fractions; do not round.",
            "solution": "The problem requires the derivation of the posterior distribution for the true position $x$ given two independent sensory measurements, $x_{1}$ and $x_{2}$, and then the computation of its mean and variance for a specific trial. Finally, an explanation of sensory reweighting is required.\n\n**Step 1: Problem Validation**\nThe problem statement is scientifically grounded, well-posed, and objective. It describes a standard Bayesian Maximum Likelihood Estimation (MLE) model for sensory integration, a fundamental concept in computational neuroscience and motor control. All givens are clearly defined, consistent, and scientifically plausible. The problem is valid.\n\n**Step 2: Derivation of the Posterior Distribution**\nAccording to Bayes' rule, the posterior probability of the true position $x$ given the measurements $x_{1}$ and $x_{2}$ is:\n$$p(x \\mid x_{1}, x_{2}) = \\frac{p(x_{1}, x_{2} \\mid x) p(x)}{p(x_{1}, x_{2})}$$\nThe term $p(x_{1}, x_{2})$ is a normalization constant, so we can express this as a proportionality:\n$$p(x \\mid x_{1}, x_{2}) \\propto p(x_{1}, x_{2} \\mid x) p(x)$$\nThe problem states that the sensory modalities are independent. Therefore, the joint likelihood $p(x_{1}, x_{2} \\mid x)$ is the product of the individual likelihoods:\n$$p(x_{1}, x_{2} \\mid x) = p(x_{1} \\mid x) p(x_{2} \\mid x)$$\nThe prior on $x$, $p(x)$, is given as \"uninformative\". In this context, this is treated as a uniform prior, $p(x) \\propto \\text{constant}$. This improper prior is acceptable here because the posterior will be proper. Thus, the posterior is proportional to the product of the likelihoods:\n$$p(x \\mid x_{1}, x_{2}) \\propto p(x_{1} \\mid x) p(x_{2} \\mid x)$$\nThe likelihoods are given by the Gaussian probability density functions for $x_{1} \\sim \\mathcal{N}(x, \\sigma_{1}^{2})$ and $x_{2} \\sim \\mathcal{N}(x, \\sigma_{2}^{2})$:\n$$p(x_{1} \\mid x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{1}^{2}}} \\exp\\left(-\\frac{(x_{1} - x)^{2}}{2\\sigma_{1}^{2}}\\right)$$\n$$p(x_{2} \\mid x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{2}^{2}}} \\exp\\left(-\\frac{(x_{2} - x)^{2}}{2\\sigma_{2}^{2}}\\right)$$\nSubstituting these into the proportionality for the posterior and dropping the constant normalization factors:\n$$p(x \\mid x_{1}, x_{2}) \\propto \\exp\\left(-\\frac{(x_{1} - x)^{2}}{2\\sigma_{1}^{2}}\\right) \\exp\\left(-\\frac{(x_{2} - x)^{2}}{2\\sigma_{2}^{2}}\\right)$$\n$$p(x \\mid x_{1}, x_{2}) \\propto \\exp\\left(-\\frac{1}{2}\\left[\\frac{(x - x_{1})^{2}}{\\sigma_{1}^{2}} + \\frac{(x - x_{2})^{2}}{\\sigma_{2}^{2}}\\right]\\right)$$\nTo identify the form of this distribution, we analyze the exponent by expanding the quadratic terms in $x$:\n$$-\\frac{1}{2}\\left[ \\frac{x^{2} - 2xx_{1} + x_{1}^{2}}{\\sigma_{1}^{2}} + \\frac{x^{2} - 2xx_{2} + x_{2}^{2}}{\\sigma_{2}^{2}} \\right]$$\n$$= -\\frac{1}{2}\\left[ x^{2}\\left(\\frac{1}{\\sigma_{1}^{2}} + \\frac{1}{\\sigma_{2}^{2}}\\right) - 2x\\left(\\frac{x_{1}}{\\sigma_{1}^{2}} + \\frac{x_{2}}{\\sigma_{2}^{2}}\\right) + \\left(\\frac{x_{1}^{2}}{\\sigma_{1}^{2}} + \\frac{x_{2}^{2}}{\\sigma_{2}^{2}}\\right) \\right]$$\nThis is a quadratic function of $x$. The resulting posterior distribution is therefore also a Gaussian, $\\mathcal{N}(\\hat{x}, \\sigma_{\\text{fused}}^{2})$, whose exponent is of the form $-\\frac{(x - \\hat{x})^{2}}{2\\sigma_{\\text{fused}}^{2}} = -\\frac{1}{2\\sigma_{\\text{fused}}^{2}}(x^{2} - 2x\\hat{x} + \\hat{x}^{2})$.\n\nBy comparing the coefficients of the $x^{2}$ term, we find the inverse variance of the posterior:\n$$\\frac{1}{\\sigma_{\\text{fused}}^{2}} = \\frac{1}{\\sigma_{1}^{2}} + \\frac{1}{\\sigma_{2}^{2}}$$\nThis shows that the precision (inverse variance) of the fused estimate is the sum of the precisions of the individual sensory modalities. The variance of the fused estimate is:\n$$\\sigma_{\\text{fused}}^{2} = \\left(\\frac{1}{\\sigma_{1}^{2}} + \\frac{1}{\\sigma_{2}^{2}}\\right)^{-1} = \\frac{\\sigma_{1}^{2}\\sigma_{2}^{2}}{\\sigma_{1}^{2} + \\sigma_{2}^{2}}$$\nBy comparing the coefficients of the $x$ term, we find the mean of the posterior:\n$$\\frac{2\\hat{x}}{2\\sigma_{\\text{fused}}^{2}} = \\frac{x_{1}}{\\sigma_{1}^{2}} + \\frac{x_{2}}{\\sigma_{2}^{2}}$$\n$$\\hat{x} = \\sigma_{\\text{fused}}^{2} \\left(\\frac{x_{1}}{\\sigma_{1}^{2}} + \\frac{x_{2}}{\\sigma_{2}^{2}}\\right) = \\frac{\\frac{x_{1}}{\\sigma_{1}^{2}} + \\frac{x_{2}}{\\sigma_{2}^{2}}}{\\frac{1}{\\sigma_{1}^{2}} + \\frac{1}{\\sigma_{2}^{2}}}$$\nThis mean $\\hat{x}$ is the Bayesian fused estimate.\n\n**Step 3: Numerical Computation**\nThe given values are:\n$x_{1} = 12$ mm, $\\sigma_{1}^{2} = 25$ mm$^{2}$\n$x_{2} = 8$ mm, $\\sigma_{2}^{2} = 9$ mm$^{2}$\n\nWe compute the fused variance, $\\sigma_{\\text{fused}}^{2}$:\n$$\\frac{1}{\\sigma_{\\text{fused}}^{2}} = \\frac{1}{25} + \\frac{1}{9} = \\frac{9 + 25}{225} = \\frac{34}{225}$$\n$$\\sigma_{\\text{fused}}^{2} = \\frac{225}{34} \\text{ mm}^{2}$$\nNext, we compute the fused estimate, $\\hat{x}$:\n$$\\hat{x} = \\frac{\\frac{12}{25} + \\frac{8}{9}}{\\frac{1}{25} + \\frac{1}{9}} = \\frac{\\frac{12 \\times 9 + 8 \\times 25}{225}}{\\frac{34}{225}}$$\n$$\\hat{x} = \\frac{\\frac{108 + 200}{225}}{\\frac{34}{225}} = \\frac{308/225}{34/225} = \\frac{308}{34}$$\nSimplifying the fraction gives:\n$$\\hat{x} = \\frac{154}{17} \\text{ mm}$$\n\n**Step 4: Explanation of Sensory Reweighting**\nThe formula for the fused estimate $\\hat{x}$ can be expressed as a weighted average of the individual measurements $x_{1}$ and $x_{2}$:\n$$\\hat{x} = w_{1}x_{1} + w_{2}x_{2}$$\nwhere the weights $w_{1}$ and $w_{2}$ are given by:\n$$w_{1} = \\frac{1/\\sigma_{1}^{2}}{1/\\sigma_{1}^{2} + 1/\\sigma_{2}^{2}} \\quad \\text{and} \\quad w_{2} = \\frac{1/\\sigma_{2}^{2}}{1/\\sigma_{1}^{2} + 1/\\sigma_{2}^{2}}$$\nNote that $w_{1} + w_{2} = 1$. The weight $w_{i}$ assigned to a measurement $x_{i}$ is proportional to its precision, $1/\\sigma_{i}^{2}$. Precision is a measure of a sensory modality's reliability; a smaller variance (less noise) corresponds to higher precision and thus greater reliability.\n\n**Sensory reweighting** is the principle, derived from this Bayesian framework, that the central nervous system combines sensory signals by assigning more weight to more reliable (i.e., less noisy or more precise) sources of information.\n\nThis principle manifests in the fused estimate by pulling it closer to the measurement from the more reliable modality. For the given values:\n- Precision of modality $1$: $1/\\sigma_{1}^{2} = 1/25$\n- Precision of modality $2$: $1/\\sigma_{2}^{2} = 1/9$\nSince $1/9 > 1/25$, modality $2$ is more reliable.\n\nLet's compute the numerical weights:\n- Total precision: $1/25 + 1/9 = 34/225$\n- Weight for modality $1$: $w_{1} = \\frac{1/25}{34/225} = \\frac{1}{25} \\times \\frac{225}{34} = \\frac{9}{34}$\n- Weight for modality $2$: $w_{2} = \\frac{1/9}{34/225} = \\frac{1}{9} \\times \\frac{225}{34} = \\frac{25}{34}$\n\nThe weight for the more reliable modality $2$ ($w_{2} = 25/34 \\approx 0.735$) is significantly larger than the weight for modality $1$ ($w_{1} = 9/34 \\approx 0.265$). The individual measurements are $x_{1} = 12$ and $x_{2} = 8$. The fused estimate is $\\hat{x} = 154/17 \\approx 9.06$, which is much closer to $x_{2}=8$ than to $x_{1}=12$. This demonstrates how the brain optimally \"reweights\" its trust in sensory inputs based on their relative quality, a core strategy for robust motor control. Furthermore, the resulting estimate is more precise than either input alone: $\\sigma_{\\text{fused}}^{2} = 225/34 \\approx 6.62$, which is less than both $\\sigma_{1}^{2}=25$ and $\\sigma_{2}^{2}=9$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{154}{17} & \\frac{225}{34}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once the nervous system knows the starting and ending points for a movement, it must plan the entire trajectory of the limb through space and time. A remarkable feature of human reaching movements is their stereotypical smoothness, a quality that motor control theories aim to explain. This practice delves into the influential minimum-jerk hypothesis, which posits that trajectories are planned to be as smooth as possible by minimizing the time integral of the squared jerk . Completing this exercise will give you first-hand experience in using calculus of variations to derive a biologically plausible movement plan from an optimization principle.",
            "id": "4192584",
            "problem": "A central hypothesis in human motor control is that voluntary reaching movements are planned to minimize the time integral of squared kinematic jerk. Consider a planar point-mass end-effector undergoing a reach in the horizontal plane from initial position $(0,0)$ in meters to final position $(0.25,0.15)$ in meters, completed in duration $T=0.8 \\ \\mathrm{s}$. The movement is constrained to have zero initial and final velocity and acceleration in both coordinates. Let the movement in each Cartesian coordinate be represented by a time-polynomial $x(t)=\\sum_{k=0}^{5} a_{k}^{x} t^{k}$ and $y(t)=\\sum_{k=0}^{5} a_{k}^{y} t^{k}$ for $t \\in [0,T]$, where the coefficients $a_{k}^{x}$ and $a_{k}^{y}$ are constant and to be determined.\n\nStarting from the definition of the minimum-jerk objective as the cost functional\n$$\n\\mathcal{J} = \\int_{0}^{T} \\left( \\left( \\frac{d^{3} x}{d t^{3}} \\right)^{2} + \\left( \\frac{d^{3} y}{d t^{3}} \\right)^{2} \\right) dt,\n$$\nand using only fundamental principles of calculus of variations and the stated kinematic boundary conditions at $t=0$ and $t=T$, derive the form of the optimal coordinate trajectories and compute the polynomial coefficients $a_{0}^{x}, a_{1}^{x}, a_{2}^{x}, a_{3}^{x}, a_{4}^{x}, a_{5}^{x}$ and $a_{0}^{y}, a_{1}^{y}, a_{2}^{y}, a_{3}^{y}, a_{4}^{y}, a_{5}^{y}$ for the specified reach. Report the coefficients for $x(t)$ and $y(t)$ in the order $(a_{0}^{x}, a_{1}^{x}, a_{2}^{x}, a_{3}^{x}, a_{4}^{x}, a_{5}^{x}, a_{0}^{y}, a_{1}^{y}, a_{2}^{y}, a_{3}^{y}, a_{4}^{y}, a_{5}^{y})$.\n\nExpress each coefficient in International System of Units (SI), where $a_{0}$ is in meters, $a_{1}$ is in meters per second, $a_{2}$ is in meters per second squared, $a_{3}$ is in meters per second cubed, $a_{4}$ is in meters per second to the fourth power, and $a_{5}$ is in meters per second to the fifth power. Round each nonzero coefficient to eight significant figures. In your final boxed answer, provide the twelve numbers as a single row vector and do not include units inside the box. Angle units are not applicable in this problem.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Hypothesis**: Voluntary reaching movements are planned to minimize the time integral of squared kinematic jerk.\n- **System**: A planar point-mass end-effector.\n- **Initial Position**: $(x(0), y(0)) = (0,0)$ meters.\n- **Final Position**: $(x(T), y(T)) = (0.25, 0.15)$ meters.\n- **Movement Duration**: $T=0.8 \\ \\mathrm{s}$.\n- **Boundary Conditions at $t=0$**: Zero velocity and acceleration.\n  - $\\dot{x}(0) = 0$, $\\dot{y}(0) = 0$.\n  - $\\ddot{x}(0) = 0$, $\\ddot{y}(0) = 0$.\n- **Boundary Conditions at $t=T$**: Zero velocity and acceleration.\n  - $\\dot{x}(T) = 0$, $\\dot{y}(T) = 0$.\n  - $\\ddot{x}(T) = 0$, $\\ddot{y}(T) = 0$.\n- **Trajectory Model**:\n  - $x(t) = \\sum_{k=0}^{5} a_{k}^{x} t^{k}$ for $t \\in [0,T]$.\n  - $y(t) = \\sum_{k=0}^{5} a_{k}^{y} t^{k}$ for $t \\in [0,T]$.\n- **Cost Functional**: $\\mathcal{J} = \\int_{0}^{T} \\left( \\left( \\frac{d^{3} x}{d t^{3}} \\right)^{2} + \\left( \\frac{d^{3} y}{d t^{3}} \\right)^{2} \\right) dt$.\n- **Objective**: Derive the optimal trajectories and compute the twelve polynomial coefficients $(a_{0}^{x}, a_{1}^{x}, a_{2}^{x}, a_{3}^{x}, a_{4}^{x}, a_{5}^{x})$ and $(a_{0}^{y}, a_{1}^{y}, a_{2}^{y}, a_{3}^{y}, a_{4}^{y}, a_{5}^{y})$.\n- **Output Specification**: Report the twelve coefficients as a single row vector, with each nonzero coefficient rounded to eight significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is based on the minimum-jerk hypothesis, a seminal and well-established principle in computational motor control, originally formulated by Flash and Hogan (1985). This is scientifically sound.\n- **Well-Posedness**: The problem seeks to find the coefficients of two $5$th-degree polynomials. Each polynomial has $6$ unknown coefficients. For each coordinate ($x$ and $y$), there are $6$ boundary conditions provided: position at $t=0$ and $t=T$, velocity at $t=0$ and $t=T$, and acceleration at $t=0$ and $t=T$. This constitutes a well-posed problem with a sufficient number of conditions to determine a unique solution for the coefficients.\n- **Objectivity**: The problem is stated using precise mathematical and physical language, with all quantities clearly defined. It is free of ambiguity or subjective claims.\n- **Conclusion**: The problem is scientifically grounded, well-posed, complete, and stated objectively. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution process will now commence.\n\nThe cost functional $\\mathcal{J}$ is separable for the $x$ and $y$ coordinates:\n$$\n\\mathcal{J} = \\int_{0}^{T} \\left( \\frac{d^{3} x}{d t^{3}} \\right)^{2} dt + \\int_{0}^{T} \\left( \\frac{d^{3} y}{d t^{3}} \\right)^{2} dt = \\mathcal{J}_x + \\mathcal{J}_y\n$$\nMinimizing $\\mathcal{J}$ is equivalent to independently minimizing $\\mathcal{J}_x$ and $\\mathcal{J}_y$. Let us consider the general problem for a coordinate $p(t)$, which can represent either $x(t)$ or $y(t)$. The functional to be minimized is $\\mathcal{J}_p = \\int_{0}^{T} (\\dddot{p}(t))^2 dt$. The integrand is $L(t, p, \\dot{p}, \\ddot{p}, \\dddot{p}) = (\\dddot{p})^2$.\n\nAccording to the fundamental principles of the calculus of variations, the function $p(t)$ that minimizes this functional must satisfy the Euler-Lagrange equation. For a functional with derivatives up to the $n$-th order, this equation is:\n$$\n\\frac{\\partial L}{\\partial p} - \\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{p}} + \\frac{d^2}{dt^2}\\frac{\\partial L}{\\partial \\ddot{p}} - \\dots + (-1)^n \\frac{d^n}{dt^n}\\frac{\\partial L}{\\partial p^{(n)}} = 0\n$$\nIn our case, $n=3$. The partial derivatives of $L$ with respect to $p$, $\\dot{p}$, and $\\ddot{p}$ are all zero. The only non-zero contribution comes from the term involving $\\dddot{p}$:\n$$\n\\frac{\\partial L}{\\partial \\dddot{p}} = 2\\dddot{p}\n$$\nThe Euler-Lagrange equation thus simplifies to:\n$$\n(-1)^3 \\frac{d^3}{dt^3} \\left( \\frac{\\partial L}{\\partial \\dddot{p}} \\right) = 0 \\implies -\\frac{d^3}{dt^3} (2\\dddot{p}) = 0 \\implies \\frac{d^6 p}{dt^6} = 0\n$$\nThe general solution to this $6$th-order ordinary differential equation is a $5$th-order polynomial, as given in the problem statement:\n$$\np(t) = a_5 t^5 + a_4 t^4 + a_3 t^3 + a_2 t^2 + a_1 t + a_0\n$$\nThe coefficients $a_k$ are determined by the six boundary conditions. We will now solve for these coefficients for $x(t)$ and $y(t)$ separately.\n\n**Derivation for $x$-coordinate coefficients:**\nThe trajectory is $x(t) = a_{5}^{x} t^5 + a_{4}^{x} t^4 + a_{3}^{x} t^3 + a_{2}^{x} t^2 + a_{1}^{x} t + a_{0}^{x}$.\nThe derivatives are:\n$\\dot{x}(t) = 5 a_{5}^{x} t^4 + 4 a_{4}^{x} t^3 + 3 a_{3}^{x} t^2 + 2 a_{2}^{x} t + a_{1}^{x}$\n$\\ddot{x}(t) = 20 a_{5}^{x} t^3 + 12 a_{4}^{x} t^2 + 6 a_{3}^{x} t + 2 a_{2}^{x}$\n\nThe boundary conditions are:\n$x(0)=0.0$, $x(T)=0.25$\n$\\dot{x}(0)=0$, $\\dot{x}(T)=0$\n$\\ddot{x}(0)=0$, $\\ddot{x}(T)=0$\nwhere $T=0.8$.\n\nApplying the conditions at $t=0$:\n$x(0) = a_{0}^{x} = 0$\n$\\dot{x}(0) = a_{1}^{x} = 0$\n$\\ddot{x}(0) = 2a_{2}^{x} = 0 \\implies a_{2}^{x} = 0$\n\nThus, the first three coefficients ($a_{0}^{x}, a_{1}^{x}, a_{2}^{x}$) are zero. The polynomial reduces to $x(t) = a_{5}^{x} t^5 + a_{4}^{x} t^4 + a_{3}^{x} t^3$.\nNow, applying the conditions at $t=T$:\n1. $x(T) = a_{5}^{x} T^5 + a_{4}^{x} T^4 + a_{3}^{x} T^3 = 0.25$\n2. $\\dot{x}(T) = 5 a_{5}^{x} T^4 + 4 a_{4}^{x} T^3 + 3 a_{3}^{x} T^2 = 0$\n3. $\\ddot{x}(T) = 20 a_{5}^{x} T^3 + 12 a_{4}^{x} T^2 + 6 a_{3}^{x} T = 0$\n\nThis is a system of three linear equations for $a_{3}^{x}, a_{4}^{x}, a_{5}^{x}$. We can simplify by dividing by powers of $T \\neq 0$:\n1. $a_{5}^{x} T^2 + a_{4}^{x} T + a_{3}^{x} = \\frac{0.25}{T^3}$\n2. $5 a_{5}^{x} T^2 + 4 a_{4}^{x} T + 3 a_{3}^{x} = 0$\n3. $10 a_{5}^{x} T^2 + 6 a_{4}^{x} T + 3 a_{3}^{x} = 0$\n\nSolving this system symbolically for a general displacement $\\Delta x = x(T) - x(0)$ yields:\n$a_{3}^{x} = \\frac{10 \\Delta x}{T^3}$\n$a_{4}^{x} = -\\frac{15 \\Delta x}{T^4}$\n$a_{5}^{x} = \\frac{6 \\Delta x}{T^5}$\n\nSubstituting $\\Delta x = 0.25$ and $T=0.8$:\n$a_{3}^{x} = \\frac{10(0.25)}{(0.8)^3} = \\frac{2.5}{0.512} = 4.8828125$\n$a_{4}^{x} = -\\frac{15(0.25)}{(0.8)^4} = -\\frac{3.75}{0.4096} = -9.1552734375$\n$a_{5}^{x} = \\frac{6(0.25)}{(0.8)^5} = \\frac{1.5}{0.32768} = 4.57763671875$\n\n**Derivation for $y$-coordinate coefficients:**\nThe procedure is identical. The boundary conditions are:\n$y(0)=0.0$, $y(T)=0.15$\n$\\dot{y}(0)=0$, $\\dot{y}(T)=0$\n$\\ddot{y}(0)=0$, $\\ddot{y}(T)=0$\n\nThe coefficients $a_{0}^{y}, a_{1}^{y}, a_{2}^{y}$ are zero for the same reasons as for the $x$-coordinate. For the remaining coefficients, we use the same symbolic solutions with $\\Delta y = y(T) - y(0) = 0.15$:\n$a_{3}^{y} = \\frac{10 \\Delta y}{T^3} = \\frac{10(0.15)}{(0.8)^3} = \\frac{1.5}{0.512} = 2.9296875$\n$a_{4}^{y} = -\\frac{15 \\Delta y}{T^4} = -\\frac{15(0.15)}{(0.8)^4} = -\\frac{2.25}{0.4096} = -5.4931640625$\n$a_{5}^{y} = \\frac{6 \\Delta y}{T^5} = \\frac{6(0.15)}{(0.8)^5} = \\frac{0.9}{0.32768} = 2.74658203125$\n\nFinally, we round the nonzero coefficients to eight significant figures as specified.\nFor $x(t)$:\n$a_{0}^{x} = 0$\n$a_{1}^{x} = 0$\n$a_{2}^{x} = 0$\n$a_{3}^{x} = 4.8828125$ (This number has exactly $8$ significant figures)\n$a_{4}^{x} = -9.1552734375 \\approx -9.1552734$\n$a_{5}^{x} = 4.57763671875 \\approx 4.5776367$\n\nFor $y(t)$:\n$a_{0}^{y} = 0$\n$a_{1}^{y} = 0$\n$a_{2}^{y} = 0$\n$a_{3}^{y} = 2.9296875$ (This number has exactly $8$ significant figures)\n$a_{4}^{y} = -5.4931640625 \\approx -5.4931641$\n$a_{5}^{y} = 2.74658203125 \\approx 2.7465820$\n\nThe final set of coefficients is assembled into a single row vector in the specified order $(a_{0}^{x}, \\dots, a_{5}^{x}, a_{0}^{y}, \\dots, a_{5}^{y})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 0 & 0 & 4.8828125 & -9.1552734 & 4.5776367 & 0 & 0 & 0 & 2.9296875 & -5.4931641 & 2.7465820\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Motor plans are not infallible; unexpected perturbations or changes in the environment can lead to movement errors. A key feature of the motor system is its ability to learn from these errors and adapt future performance. This exercise introduces a powerful yet simple framework for modeling this process: the state-space model of motor adaptation . By analyzing hypothetical experimental data, you will practice a core skill of motor control research: estimating the parameters that govern learning, such as the retention factor and learning rate, from trial-by-trial performance.",
            "id": "4192601",
            "problem": "In a visuomotor rotation task with error-clamped feedback, the central nervous system (CNS) updates an internal adaptation state according to a single-state linear model. Let the produced compensatory rotation on trial $k$ be the state $x_k$ (in degrees), and let the experienced sensory error be $e_k$ (in degrees). Under error clamp, the error is held constant across trials, so $e_k = e_0$ for all $k$. The state update is modeled as a discrete-time linear dynamical system\n$$\nx_{k+1} = a\\,x_k + b\\,e_k,\n$$\nwhere $a$ is the retention factor and $b$ is the learning rate. Assume baseline $x_0 = 0$ and constant error $e_0 = 15^\\circ$. In an experiment, the observed adaptation states are $x_1 = 6^\\circ$ and $x_2 = 2.4^\\circ$.\n\nUsing only the stated model and assumptions, set up and solve the linear system implied by the observations to estimate $a$ and $b$, and then compute the predicted state $x_3$ for the next trial. Express the final predicted state in degrees and round your answer to four significant figures. The final answer must be a single real-valued number.",
            "solution": "The problem specifies a single-state linear update law for sensorimotor adaptation, which is a well-tested model for error-based motor learning near equilibrium under small perturbations. The discrete-time model\n$$\nx_{k+1} = a\\,x_k + b\\,e_k\n$$\ncombines retention of the current state (scaled by $a$) and a proportional update driven by the experienced error (scaled by $b$). Under error clamp, $e_k = e_0$ is constant, which yields\n$$\nx_{k+1} = a\\,x_k + b\\,e_0.\n$$\nWith baseline $x_0 = 0$, we can write the first two observed trials as linear equations in $a$ and $b$:\n1. For $k = 0$,\n$$\nx_1 = a\\,x_0 + b\\,e_0 = a \\cdot 0 + b\\,e_0 = b\\,e_0.\n$$\n2. For $k = 1$,\n$$\nx_2 = a\\,x_1 + b\\,e_0.\n$$\nThese two equations form a linear system in the unknown parameters $a$ and $b$. From the first equation,\n$$\nb = \\frac{x_1}{e_0}.\n$$\nSubstitute this into the second equation to solve for $a$:\n$$\nx_2 = a\\,x_1 + b\\,e_0 = a\\,x_1 + \\left(\\frac{x_1}{e_0}\\right)e_0 = a\\,x_1 + x_1,\n$$\nwhich implies\n$$\na = \\frac{x_2}{x_1} - 1.\n$$\nNow insert the given numerical values $e_0 = 15^\\circ$, $x_1 = 6^\\circ$, and $x_2 = 2.4^\\circ$:\n$$\nb = \\frac{6}{15} = 0.4,\n\\quad\na = \\frac{2.4}{6} - 1 = 0.4 - 1 = -0.6.\n$$\nTo predict the next state $x_3$, use the update with $k = 2$:\n$$\nx_3 = a\\,x_2 + b\\,e_0.\n$$\nSubstitute the estimated parameters and observed $x_2$:\n$$\nx_3 = (-0.6)\\cdot(2.4) + (0.4)\\cdot(15) = -1.44 + 6 = 4.56.\n$$\nRounding to four significant figures yields $4.560$. The predicted state is therefore $4.560$ degrees.",
            "answer": "$$\\boxed{4.560}$$"
        }
    ]
}