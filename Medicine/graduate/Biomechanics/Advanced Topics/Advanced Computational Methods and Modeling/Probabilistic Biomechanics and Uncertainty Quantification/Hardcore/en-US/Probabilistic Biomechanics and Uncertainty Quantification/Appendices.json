{
    "hands_on_practices": [
        {
            "introduction": "Quantifying the probability of an event is the foundation of probabilistic analysis. This first exercise provides a direct, hands-on approach to this task by using experimental data to build an empirical estimate of a cumulative distribution function (CDF), a fundamental non-parametric tool for assessing failure risk. ",
            "id": "4200159",
            "problem": "A ligament is tested under quasi-static tensile loading to failure in a controlled biomechanics laboratory. Each specimen produces one failure load measurement. Model the sample space as $\\Omega = \\mathbb{R}_{>0}$, representing all possible positive failure loads in Newtons, with the sigma-algebra $\\mathcal{F}$ taken to be the Borel sigma-algebra on $\\mathbb{R}_{>0}$ generated by intervals. An unknown probability measure $P$ on $(\\Omega,\\mathcal{F})$ describes the variability in failure loads across nominally identical specimens under fixed testing conditions. The measure is estimated from repeated tests as the empirical probability measure based on independent and identically distributed samples.\n\nYou conduct $n = 10$ repeated destructive tests and record the failure loads (in Newtons) as\n$$\nx_1 = 1500,\\quad x_2 = 1700,\\quad x_3 = 1850,\\quad x_4 = 1950,\\quad x_5 = 2050,\\quad x_6 = 2200,\\quad x_7 = 2350,\\quad x_8 = 2500,\\quad x_9 = 2600,\\quad x_{10} = 2800.\n$$\nStarting from the definitions of a sigma-algebra, a probability measure, and the empirical probability measure derived from repeated independent tests, formulate how to compute the probability $P(\\text{failure load} \\leq x)$ for a given threshold $x$ using only the observed data. Then evaluate this probability for the threshold $x = 2000$ Newtons. Explain the physical interpretation of this probability in the context of ligament failure under the specified testing conditions. Express the final probability as a decimal and round your answer to four significant figures. Do not include units with the final numerical value.",
            "solution": "The problem is validated as self-contained, scientifically grounded in probability theory and biomechanics, and well-posed. We proceed with the solution.\n\nThe problem asks for the probability of a ligament's failure load being less than or equal to a specified threshold, estimated from a set of experimental observations. This requires an understanding of fundamental concepts in measure-theoretic probability and their application in statistics, specifically the empirical probability measure.\n\nFirst, let us formally define the components of the probability space $(\\Omega, \\mathcal{F}, P)$.\nThe sample space $\\Omega$ is the set of all possible outcomes. In this problem, an outcome is a failure load, which is a positive real number. Thus, the sample space is given as $\\Omega = \\mathbb{R}_{>0}$, the set of all positive real numbers.\n\nThe sigma-algebra $\\mathcal{F}$ is a collection of subsets of $\\Omega$ for which we can assign a probability. It must satisfy three properties:\n$1.$ $\\Omega \\in \\mathcal{F}$.\n$2.$ If $A \\in \\mathcal{F}$, then its complement $A^c = \\Omega \\setminus A$ is also in $\\mathcal{F}$.\n$3.$ If $A_1, A_2, \\dots$ is a countable sequence of sets in $\\mathcal{F}$, then their union $\\cup_{i=1}^{\\infty} A_i$ is also in $\\mathcal{F}$.\nThe problem states that $\\mathcal{F}$ is the Borel sigma-algebra on $\\mathbb{R}_{>0}$, which is the standard choice for continuous sample spaces. It is the smallest sigma-algebra containing all open intervals in $\\mathbb{R}_{>0}$.\n\nThe probability measure $P$ is a function $P: \\mathcal{F} \\to [0, 1]$ that assigns a probability to each event (a set in $\\mathcal{F}$). It must satisfy:\n$1.$ $P(\\Omega) = 1$.\n$2.$ For any countable sequence of pairwise disjoint events $A_1, A_2, \\dots$ in $\\mathcal{F}$, $P(\\cup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty} P(A_i)$.\nThis measure $P$ describes the true, but unknown, distribution of ligament failure loads.\n\nWe are given a set of $n = 10$ independent and identically distributed (i.i.d.) observations $\\{x_1, x_2, \\dots, x_{10}\\}$ drawn from the distribution $P$.\nThe data are:\n$$\nx_1 = 1500, \\quad x_2 = 1700, \\quad x_3 = 1850, \\quad x_4 = 1950, \\quad x_5 = 2050, \\\\\nx_6 = 2200, \\quad x_7 = 2350, \\quad x_8 = 2500, \\quad x_9 = 2600, \\quad x_{10} = 2800.\n$$\nSince the true measure $P$ is unknown, we estimate it using the empirical probability measure $\\hat{P}_n$. The empirical probability measure is constructed from the observed data. For any event $A \\in \\mathcal{F}$, the empirical probability $\\hat{P}_n(A)$ is defined as the fraction of the $n$ observations that fall into the set $A$.\n\nMathematically, this is expressed using the indicator function $I_A(x)$, which is $1$ if $x \\in A$ and $0$ otherwise. The empirical probability of an event $A$ is:\n$$\n\\hat{P}_n(A) = \\frac{1}{n} \\sum_{i=1}^{n} I_A(x_i)\n$$\nThis formula provides a systematic way to compute the probability of any event based solely on the observed data. By the Glivenko-Cantelli theorem, as the number of samples $n$ increases, the empirical distribution function converges uniformly to the true cumulative distribution function.\n\nThe problem asks for the probability $P(\\text{failure load} \\leq x)$, which corresponds to the event $A = \\{\\omega \\in \\Omega \\mid \\omega \\leq x\\}$. For the given threshold $x = 2000$ Newtons, the event is $A = \\{\\omega \\in \\mathbb{R}_{>0} \\mid \\omega \\leq 2000\\}$, or more simply, the interval $(0, 2000]$. We estimate the probability of this event using the empirical measure $\\hat{P}_{10}$:\n$$\n\\hat{P}_{10}(\\text{failure load} \\leq 2000) = \\frac{1}{10} \\sum_{i=1}^{10} I_{(0, 2000]}(x_i)\n$$\nThe term $\\sum_{i=1}^{10} I_{(0, 2000]}(x_i)$ is simply the count of the number of data points $x_i$ such that $x_i \\leq 2000$. Let us examine the given data:\n- $x_1 = 1500 \\leq 2000$ (True)\n- $x_2 = 1700 \\leq 2000$ (True)\n- $x_3 = 1850 \\leq 2000$ (True)\n- $x_4 = 1950 \\leq 2000$ (True)\n- $x_5 = 2050 > 2000$ (False)\n- $x_6 = 2200 > 2000$ (False)\n- $x_7 = 2350 > 2000$ (False)\n- $x_8 = 2500 > 2000$ (False)\n- $x_9 = 2600 > 2000$ (False)\n- $x_{10} = 2800 > 2000$ (False)\n\nThe number of observations less than or equal to $2000$ is $4$. Let this count be $k$. So, $k=4$.\nThe total number of observations is $n=10$.\nThe estimated probability is therefore:\n$$\n\\hat{P}_{10}(\\text{failure load} \\leq 2000) = \\frac{k}{n} = \\frac{4}{10} = 0.4\n$$\nThe problem requires the answer to be rounded to four significant figures. Thus, $0.4$ becomes $0.4000$.\n\nThe physical interpretation of this probability is that, based on the collected experimental data, there is an estimated $40\\%$ chance that a ligament specimen, chosen randomly from the same population and tested under the identical quasi-static tensile loading conditions, will fail at or below a load of $2000$ Newtons. This value, $\\hat{P}_{10}(\\text{failure load} \\leq 2000)$, is an estimate of the value of the true cumulative distribution function (CDF) of the failure load, $F(x) = P(\\text{failure load} \\leq x)$, at the point $x=2000$. In biomechanical engineering and clinical practice, such a probability is crucial for risk assessment. For example, if an in-vivo load on a ligament is expected to reach $2000$ N, this result provides a preliminary, data-driven estimate of the risk of tissue failure.",
            "answer": "$$\n\\boxed{0.4000}\n$$"
        },
        {
            "introduction": "Often, we aim to infer the parameters of a physics-based model from noisy data, a core task in scientific computing. This practice introduces the powerful framework of Bayesian inference by guiding you through the derivation of the core component of a Markov Chain Monte Carlo (MCMC) algorithm. You will develop the acceptance criterion needed to sample from the posterior probability distribution for a material's stiffness, effectively combining experimental evidence with prior knowledge. ",
            "id": "4200122",
            "problem": "A single-degree-of-freedom tendon segment is modeled as a linear elastic element with stiffness $k>0$ relating force $F$ to elongation $x$ through the constitutive relation $F = k x$. In an experiment, $n$ independent measurements $\\{(x_i, F_i)\\}_{i=1}^{n}$ are collected under quasi-static conditions, where the elongations $x_i$ are controlled and the forces $F_i$ are observed. The measurement noise in force is modeled as independent and identically distributed Gaussian noise: for each $i \\in \\{1,\\dots,n\\}$,\n$$\nF_i = k x_i + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith known noise standard deviation $\\sigma>0$. The prior for stiffness is lognormal: $\\ln k \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$ with known hyperparameters $\\mu_0 \\in \\mathbb{R}$ and $\\sigma_0>0$. Consider a Metropolis–Hastings algorithm that updates the transformed parameter $\\theta = \\ln k$ by a Gaussian random-walk proposal,\n$$\n\\theta' = \\theta + \\eta,\\quad \\eta \\sim \\mathcal{N}(0, s^2),\n$$\nwith proposal scale $s>0$ and proposal density symmetric in $\\theta$.\n\nStarting from Bayes’ theorem and the definitions above, derive the exact closed-form expression for the Metropolis–Hastings acceptance ratio (that is, the argument inside the $\\min(1,\\cdot)$) for moving from the current state $\\theta$ to the proposed state $\\theta'$. Your derivation must properly account for the parameter transformation and any Jacobian factors. Express your final answer in terms of $(x_i,F_i)_{i=1}^{n}$, $\\sigma$, $\\mu_0$, $\\sigma_0$, $\\theta$, and $\\theta'$ only, using elementary functions.\n\nThen, discuss proposal scaling for efficiency: under the assumption that the posterior in $\\theta$ is well-approximated locally by a Gaussian distribution around its maximizer $\\theta^\\ast$, derive the Laplace approximation to the posterior variance $V$ in $\\theta$ by computing the negative second derivative of the log-posterior at $\\theta^\\ast$. Using this approximation and the one-dimensional random-walk Metropolis theory for a Gaussian target, state a principled scaling rule of the form $s \\approx c^\\ast \\sqrt{V}$, where $c^\\ast$ is a constant chosen to balance exploration and acceptance. Express $c^\\ast$ in terms of an optimization condition that characterizes the maximizer of the expected squared jumping distance for a one-dimensional Gaussian target. Do not use any numerical values for $c^\\ast$.\n\nYour final reported answer must be the single, exact analytical expression for the Metropolis–Hastings acceptance ratio derived in the first part. No units are required for this expression. Do not provide any numerical approximation.",
            "solution": "The problem is scientifically grounded, well-posed, and all necessary information for its solution is provided. It represents a standard application of Bayesian inference and Markov chain Monte Carlo (MCMC) methods to a common problem in biomechanics. We can therefore proceed with the derivation.\n\nThe first task is to derive the acceptance ratio for the Metropolis-Hastings algorithm. The general form for the acceptance probability of a move from a state $\\theta$ to a proposed state $\\theta'$ is\n$$\n\\alpha(\\theta, \\theta') = \\min\\left(1, \\frac{\\pi(\\theta'|D) q(\\theta|\\theta')}{\\pi(\\theta|D) q(\\theta'|\\theta)}\\right)\n$$\nwhere $\\pi(\\cdot|D)$ is the posterior distribution given the data $D = \\{(x_i, F_i)\\}_{i=1}^{n}$, and $q(\\theta'|\\theta)$ is the proposal distribution. The problem specifies a Gaussian random-walk proposal, $\\theta' = \\theta + \\eta$ where $\\eta \\sim \\mathcal{N}(0, s^2)$. The density of $\\eta$ is symmetric around $0$, which implies the proposal is symmetric, i.e., $q(\\theta'|\\theta) = q(\\theta|\\theta')$. The acceptance probability then simplifies to\n$$\n\\alpha(\\theta, \\theta') = \\min\\left(1, \\frac{\\pi(\\theta'|D)}{\\pi(\\theta|D)}\\right)\n$$\nThe term inside the $\\min(1, \\cdot)$ is the acceptance ratio, which we are tasked to derive.\n\nAccording to Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior: $\\pi(\\theta|D) \\propto p(D|\\theta)p(\\theta)$. The acceptance ratio is therefore\n$$\n\\frac{\\pi(\\theta'|D)}{\\pi(\\theta|D)} = \\frac{p(D|\\theta')p(\\theta')}{p(D|\\theta)p(\\theta)}\n$$\nWe derive the expressions for the likelihood $p(D|\\theta)$ and the prior $p(\\theta)$.\n\nThe likelihood model is given by $F_i = k x_i + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that each force measurement $F_i$ is drawn from a normal distribution centered at $k x_i$ with variance $\\sigma^2$: $F_i \\sim \\mathcal{N}(k x_i, \\sigma^2)$. Since the measurements are independent, the total likelihood for the data $D$ is the product of the individual likelihoods:\n$$\np(D|k) = \\prod_{i=1}^n p(F_i|x_i, k) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(F_i - k x_i)^2}{2\\sigma^2}\\right)\n$$\nThe MCMC algorithm samples the transformed parameter $\\theta = \\ln k$, so we must express the likelihood as a function of $\\theta$. By substituting $k = e^\\theta$, we obtain\n$$\np(D|\\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(F_i - e^\\theta x_i)^2}{2\\sigma^2}\\right)\n$$\nThe prior distribution is specified directly for the transformed parameter $\\theta = \\ln k$, which follows a normal distribution: $\\theta \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$. The prior probability density function for $\\theta$ is\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2}\\right)\n$$\nThe problem statement requires accounting for parameter transformations and Jacobians. Let's be explicit. The posterior for $k$ is $\\pi_K(k|D) \\propto p(D|k)\\pi_K(k)$. The prior for $k$ is lognormal, so its density is $\\pi_K(k) = p(\\ln k)|\\frac{d(\\ln k)}{dk}| = p(\\ln k) \\frac{1}{k}$. The posterior density for $\\theta$ is found by the change of variables rule: $\\pi_\\Theta(\\theta|D) = \\pi_K(k(\\theta)|D) |\\frac{dk}{d\\theta}|$. Since $k=e^\\theta$, the Jacobian is $|\\frac{dk}{d\\theta}| = e^\\theta = k$. Thus, $\\pi_\\Theta(\\theta|D) \\propto p(D|k=e^\\theta) \\pi_K(k=e^\\theta) \\cdot k = p(D|e^\\theta) (p(\\theta)\\frac{1}{k}) \\cdot k = p(D|e^\\theta)p(\\theta)$. The Jacobians cancel, confirming that our posterior for $\\theta$ is simply proportional to the likelihood re-expressed in $\\theta$ times the prior on $\\theta$.\n\nNow we form the acceptance ratio by taking the ratio of posteriors evaluated at $\\theta'$ and $\\theta$:\n$$\n\\frac{p(D|\\theta')p(\\theta')}{p(D|\\theta)p(\\theta)} = \\frac{\\left(\\prod_{i=1}^n \\exp\\left(-\\frac{(F_i - e^{\\theta'} x_i)^2}{2\\sigma^2}\\right)\\right) \\exp\\left(-\\frac{(\\theta' - \\mu_0)^2}{2\\sigma_0^2}\\right)}{\\left(\\prod_{i=1}^n \\exp\\left(-\\frac{(F_i - e^\\theta x_i)^2}{2\\sigma^2}\\right)\\right) \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2}\\right)}\n$$\nThe constant factors cancel. It is easier to work with the exponents by taking the logarithm. Let $R$ be the acceptance ratio.\n$$\n\\ln(R) = \\sum_{i=1}^n \\left(-\\frac{(F_i - e^{\\theta'} x_i)^2}{2\\sigma^2}\\right) - \\frac{(\\theta' - \\mu_0)^2}{2\\sigma_0^2} - \\left[ \\sum_{i=1}^n \\left(-\\frac{(F_i - e^\\theta x_i)^2}{2\\sigma^2}\\right) - \\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2} \\right]\n$$\nRearranging the terms gives\n$$\n\\ln(R) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left( (F_i - e^\\theta x_i)^2 - (F_i - e^{\\theta'} x_i)^2 \\right) + \\frac{1}{2\\sigma_0^2} \\left( (\\theta - \\mu_0)^2 - (\\theta' - \\mu_0)^2 \\right)\n$$\nExponentiating both sides yields the final expression for the acceptance ratio:\n$$\nR = \\exp\\left[ \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left( (F_i - e^\\theta x_i)^2 - (F_i - e^{\\theta'} x_i)^2 \\right) + \\frac{1}{2\\sigma_0^2} \\left( (\\theta - \\mu_0)^2 - (\\theta' - \\mu_0)^2 \\right) \\right]\n$$\nThis expression depends only on the specified variables and uses elementary functions, as required.\n\nThe second part of the task involves discussing proposal scaling. We use the Laplace approximation for the posterior variance of $\\theta$. The log-posterior, up to an additive constant, is\n$$\nL(\\theta) = \\ln(\\pi(\\theta|D)) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (F_i - e^\\theta x_i)^2 - \\frac{1}{2\\sigma_0^2} (\\theta - \\mu_0)^2\n$$\nThe variance $V$ of the Gaussian approximation is the negative reciprocal of the second derivative of $L(\\theta)$ at the posterior mode $\\theta^\\ast$. The first derivative is\n$$\n\\frac{dL}{d\\theta} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n 2(F_i - e^\\theta x_i)(-e^\\theta x_i) - \\frac{1}{2\\sigma_0^2} 2(\\theta - \\mu_0) = \\frac{e^\\theta}{\\sigma^2}\\sum_{i=1}^n (F_i x_i - e^\\theta x_i^2) - \\frac{\\theta - \\mu_0}{\\sigma_0^2}\n$$\nThe second derivative is\n$$\n\\frac{d^2L}{d\\theta^2} = \\frac{d}{d\\theta} \\left[ \\frac{1}{\\sigma^2}\\sum_{i=1}^n (F_i x_i e^\\theta - x_i^2 e^{2\\theta}) - \\frac{\\theta - \\mu_0}{\\sigma_0^2} \\right] = \\frac{1}{\\sigma^2}\\sum_{i=1}^n (F_i x_i e^\\theta - 2x_i^2 e^{2\\theta}) - \\frac{1}{\\sigma_0^2}\n$$\nThe mode $\\theta^\\ast$ is where $dL/d\\theta = 0$. The inverse of the posterior variance, or precision, is $V^{-1} = -d^2L/d\\theta^2$ evaluated at $\\theta^\\ast$:\n$$\nV^{-1} = -\\frac{d^2L}{d\\theta^2}\\bigg|_{\\theta=\\theta^\\ast} = \\frac{1}{\\sigma_0^2} - \\frac{1}{\\sigma^2}\\sum_{i=1}^n (F_i x_i e^{\\theta^\\ast} - 2x_i^2 e^{2\\theta^\\ast}) = \\frac{1}{\\sigma_0^2} + \\frac{e^{\\theta^\\ast}}{\\sigma^2}\\sum_{i=1}^n (2x_i^2 e^{\\theta^\\ast} - F_i x_i)\n$$\nThe Laplace approximation to the posterior variance is $V = (V^{-1})^{-1}$.\n\nFor efficient sampling, the proposal scale $s$ should be tuned. For a one-dimensional random-walk Metropolis algorithm targeting a Gaussian distribution, theory suggests a scaling rule of the form $s \\approx c^\\ast \\sqrt{V}$, where $V$ is the target variance. The constant $c^\\ast$ is chosen to optimize a measure of sampler efficiency, typically the expected squared jumping distance (ESJD). For a standard Gaussian target $\\Phi \\sim \\mathcal{N}(0,1)$ and a proposal step $\\lambda Z$ with $Z \\sim \\mathcal{N}(0,1)$ independent of $\\Phi$, the ESJD is $E[\\alpha(\\Phi, \\lambda Z) (\\lambda Z)^2]$. The constant $c^\\ast$ is the value of $\\lambda=s/\\sqrt{V}$ that maximizes this quantity. The optimization condition that defines $c^\\ast$ is thus:\n$$\nc^\\ast = \\underset{\\lambda > 0}{\\arg\\max} \\left( \\lambda^2 E_{\\Phi,Z \\sim i.i.d.\\mathcal{N}(0,1)}\\left[\\min\\left(1, \\exp\\left(-\\lambda \\Phi Z - \\frac{(\\lambda Z)^2}{2}\\right)\\right)\\right] \\right)\n$$\nThis provides a principled rule for setting the proposal scale $s$ based on the Laplace-approximated posterior variance $V$.",
            "answer": "$$\n\\boxed{\\exp\\left[ \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left( (F_i - e^\\theta x_i)^2 - (F_i - e^{\\theta'} x_i)^2 \\right) + \\frac{1}{2\\sigma_0^2} \\left( (\\theta - \\mu_0)^2 - (\\theta' - \\mu_0)^2 \\right) \\right]}\n$$"
        },
        {
            "introduction": "Beyond individual parameters, we must assess the reliability of entire biomechanical systems where multiple components contribute to uncertainty. This practice challenges you to implement the First-Order Reliability Method (FORM), an efficient and widely-used engineering algorithm for approximating a system's overall probability of failure. This exercise bridges the gap between component-level uncertainty and system-level risk assessment. ",
            "id": "4200112",
            "problem": "You are to implement the First-Order Reliability Method (FORM) to approximate the probability that a biomechanical structure fails, given uncertain inputs. The fundamental base is the definition of failure as the event where the limit-state function $g(\\mathbf{X}) \\le 0$, where $\\mathbf{X}$ are independent random variables representing biomechanical quantities. Use the probability integral transform to map $\\mathbf{X}$ into a standard normal space $\\mathbf{U}$, where $U_i = \\Phi^{-1}(F_{X_i}(X_i))$, with $\\Phi$ the standard normal cumulative distribution function and $F_{X_i}$ the cumulative distribution function of $X_i$. In $\\mathbf{U}$-space, the reliability index $\\beta$ is defined by minimizing the Euclidean norm of $\\mathbf{u}$ subject to the failure surface constraint, and the associated failure probability approximation is $P_f \\approx \\Phi(-\\beta)$.\n\nStarting from the definitions of failure probability $P_f = \\mathbb{P}[g(\\mathbf{X}) \\le 0]$, the probability integral transform $U_i = \\Phi^{-1}(F_{X_i}(X_i))$, and the Euclidean norm $\\|\\mathbf{u}\\|_2 = \\sqrt{\\sum_i u_i^2}$, implement FORM by:\n- Linearizing the limit state at the current iterate using a first-order Taylor expansion in $\\mathbf{U}$-space.\n- Updating the iterate using the minimum-norm solution of the linearized constraint.\n- Iterating until convergence to the design point $\\mathbf{u}^*$.\n- Returning the reliability index $\\beta = \\|\\mathbf{u}^*\\|_2$ and the approximation $P_f \\approx \\Phi(-\\beta)$.\n\nAngles, where present, must be treated in radians. The final answers for the reliability index and failure probability must be expressed as dimensionless floats. The failure probability must be expressed as a decimal (not a percentage).\n\nImplement the above for the following test suite of independent input variables and biomechanical limit-state functions $g(\\mathbf{X})$. All physical quantities are specified in scientifically plausible ranges. Each test case defines independent distributions for its variables. All random variable distributions are independent.\n\nTest Case 1 (linear strength-demand, analytical reference):\n- Variables:\n  - Strength $R \\sim \\mathcal{N}(\\mu_R = 1500, \\sigma_R = 150)$ in $\\mathrm{N}$.\n  - Demand $S \\sim \\mathcal{N}(\\mu_S = 1300, \\sigma_S = 200)$ in $\\mathrm{N}$.\n- Limit-state function: $g(\\mathbf{X}) = R - S$.\n\nTest Case 2 (nonlinear anterior cruciate ligament loading, knee flexion in radians):\n- Variables:\n  - Strength $R \\sim \\mathcal{N}(\\mu_R = 2000, \\sigma_R = 300)$ in $\\mathrm{N}$.\n  - Quadriceps force $F_q \\sim \\log\\mathcal{N}(\\mu_{\\ln} = \\ln(1000), \\sigma_{\\ln} = 0.2)$ in $\\mathrm{N}$, where $\\mu_{\\ln}$ and $\\sigma_{\\ln}$ are the mean and standard deviation of the underlying normal distribution of $\\ln(F_q)$.\n  - Knee flexion angle $\\theta \\sim \\mathcal{N}(\\mu_\\theta = 0.5, \\sigma_\\theta = 0.1)$ in radians.\n- Limit-state function: $g(\\mathbf{X}) = R - c \\, F_q \\, \\sin(\\theta)$ with $c = 0.3$ (dimensionless).\n\nTest Case 3 (nonlinear Achilles tendon loading near the failure boundary, angle in radians):\n- Variables:\n  - Strength $R \\sim \\mathcal{N}(\\mu_R = 2500, \\sigma_R = 250)$ in $\\mathrm{N}$.\n  - Muscle force $F_m \\sim \\mathcal{N}(\\mu_{F_m} = 2400, \\sigma_{F_m} = 250)$ in $\\mathrm{N}$.\n  - Joint angle $\\theta \\sim \\mathcal{N}(\\mu_\\theta = 0.1, \\sigma_\\theta = 0.05)$ in radians.\n- Limit-state function: $g(\\mathbf{X}) = R - F_m \\, \\cos(\\theta)$.\n\nAlgorithmic requirements:\n- Use the probability integral transform to map each $X_i$ to $U_i = \\Phi^{-1}(F_{X_i}(X_i))$ and its inverse $X_i = F_{X_i}^{-1}(\\Phi(U_i))$.\n- Compute the gradient of $g$ with respect to $\\mathbf{U}$ numerically via central differences by perturbing $U_i$ and mapping back to $\\mathbf{X}$ through the inverse transform.\n- At iteration $k$, denote the current point by $\\mathbf{u}_k$, the mapped physical point by $\\mathbf{x}_k$, the function value by $g_k = g(\\mathbf{x}_k)$, and the gradient in $\\mathbf{U}$-space by $\\nabla_{\\mathbf{u}} g(\\mathbf{u}_k)$. With the unit vector $\\boldsymbol{\\alpha}_k = \\nabla_{\\mathbf{u}} g(\\mathbf{u}_k) / \\|\\nabla_{\\mathbf{u}} g(\\mathbf{u}_k)\\|_2$, update using the minimum-norm solution of the linearized constraint:\n  $$\\mathbf{u}_{k+1} = \\boldsymbol{\\alpha}_k \\left( \\boldsymbol{\\alpha}_k^\\top \\mathbf{u}_k - \\frac{g_k}{\\|\\nabla_{\\mathbf{u}} g(\\mathbf{u}_k)\\|_2} \\right).$$\n- Stop when successive iterates change by less than a small tolerance and the linearized constraint is approximately satisfied.\n- Set $\\beta = \\|\\mathbf{u}^*\\|_2$ and $P_f \\approx \\Phi(-\\beta)$.\n\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, with each test case reported as a two-element list $[\\beta, P_f]$ and each float rounded to six decimal places. For example: \n\"[[\\beta_1,P_f^{(1)}],[\\beta_2,P_f^{(2)}],[\\beta_3,P_f^{(3)}]]\".",
            "solution": "The problem requires the implementation of the First-Order Reliability Method (FORM) to estimate the failure probability of three distinct biomechanical systems. The problem statement is scientifically sound, mathematically well-posed, and provides all necessary information for a unique solution. The specified methodology, known as the Hasofer-Lind-Rackwitz-Fiessler (HLRF) algorithm, is a standard and appropriate technique for this class of problem. The physical parameters and limit-state functions are consistent with established principles in biomechanics. Therefore, the problem is deemed valid and a full solution is warranted.\n\nThe core of the problem lies in determining the probability of failure, $P_f$, defined as the probability that a limit-state function $g(\\mathbf{X})$ is less than or equal to zero, $P_f = \\mathbb{P}[g(\\mathbf{X}) \\le 0]$. The vector $\\mathbf{X} = [X_1, X_2, \\dots, X_n]^\\top$ contains the system's basic random variables, which are assumed to be independent.\n\nA direct computation of the multi-dimensional integral for $P_f$ is generally intractable. FORM circumvents this by transforming the problem from the original physical space of variables $\\mathbf{X}$ to an uncorrelated standard normal space of variables $\\mathbf{U} = [U_1, U_2, \\dots, U_n]^\\top$. This is accomplished using the probability integral transform for each component:\n$$U_i = \\Phi^{-1}(F_{X_i}(X_i))$$\nwhere $F_{X_i}$ is the cumulative distribution function (CDF) of the variable $X_i$, and $\\Phi^{-1}$ is the inverse CDF (or quantile function) of the standard normal distribution. The corresponding inverse transformation is:\n$$X_i = F_{X_i}^{-1}(\\Phi(U_i))$$\nThe limit-state function is then re-expressed in the standard normal space as $g(\\mathbf{X}(\\mathbf{U})) = G(\\mathbf{U})$. The failure domain in this space is defined by $G(\\mathbf{U}) \\le 0$.\n\nIn the standard normal space, the joint probability density function is spherically symmetric. The most probable point (MPP) of failure, denoted $\\mathbf{u}^*$, is the point on the failure surface $G(\\mathbf{U})=0$ that is closest to the origin. This is found by solving the constrained optimization problem:\n$$\\text{minimize } \\|\\mathbf{u}\\|_2 \\quad \\text{subject to } G(\\mathbf{u}) = 0$$\nThe solution to this problem gives the reliability index, $\\beta$, which is the geometric distance from the origin to the MPP: $\\beta = \\|\\mathbf{u}^*\\|_2$. Because FORM approximates the failure surface $G(\\mathbf{U})=0$ with a hyperplane tangent at $\\mathbf{u}^*$, the failure probability is approximated as:\n$$P_f \\approx \\Phi(-\\beta)$$\n\nThe iterative HLRF algorithm is employed to find $\\mathbf{u}^*$. Starting from an initial point $\\mathbf{u}_k$ (typically the origin $\\mathbf{u}_0 = \\mathbf{0}$, corresponding to the mean values in $\\mathbf{X}$-space), the algorithm proceeds as follows for iteration $k$:\n\n1.  Transform the current iterate $\\mathbf{u}_k$ from standard normal space to physical space: $\\mathbf{x}_k = T^{-1}(\\mathbf{u}_k)$, where $T^{-1}$ represents the component-wise inverse mapping $X_i = F_{X_i}^{-1}(\\Phi(U_i))$.\n\n2.  Evaluate the limit-state function at the physical point: $g_k = g(\\mathbf{x}_k)$.\n\n3.  Compute the gradient of the limit-state function with respect to the standard normal variables, $\\nabla_{\\mathbf{u}} G(\\mathbf{u}_k)$. This is done numerically using a central difference scheme, which avoids analytical differentiation:\n    $$\\frac{\\partial G}{\\partial u_i}(\\mathbf{u}_k) \\approx \\frac{G(\\mathbf{u}_k + h\\mathbf{e}_i) - G(\\mathbf{u}_k - h\\mathbf{e}_i)}{2h}$$\n    where $\\mathbf{e}_i$ is the $i$-th standard basis vector and $h$ is a small perturbation (e.g., $10^{-6}$). The function $G$ is evaluated by mapping the perturbed $\\mathbf{u}$ vectors back to $\\mathbf{x}$ space.\n\n4.  Linearize the failure surface at $\\mathbf{u}_k$ with a tangent hyperplane. The minimum-norm point on this hyperplane provides the next iterate, $\\mathbf{u}_{k+1}$. The update is given by the formula:\n    $$\\mathbf{u}_{k+1} = \\boldsymbol{\\alpha}_k \\left( \\boldsymbol{\\alpha}_k^\\top \\mathbf{u}_k - \\frac{g_k}{\\|\\nabla_{\\mathbf{u}} G(\\mathbf{u}_k)\\|_2} \\right)$$\n    where $\\boldsymbol{\\alpha}_k = \\frac{\\nabla_{\\mathbf{u}} G(\\mathbf{u}_k)}{\\|\\nabla_{\\mathbf{u}} G(\\mathbf{u}_k)\\|_2}$ is the unit vector normal to the failure surface at $\\mathbf{u}_k$.\n\nThe process is iterated until the change between successive iterates, $\\|\\mathbf{u}_{k+1} - \\mathbf{u}_k\\|_2$, falls below a prescribed tolerance. The final converged point is the MPP, $\\mathbf{u}^*$.\n\nThe specific transformations for the distributions in this problem are:\n- For a **Normal** variable $X \\sim \\mathcal{N}(\\mu, \\sigma)$:\n  The transformation from $u$ to $x$ is $x = \\mu + \\sigma u$.\n- For a **Lognormal** variable $X \\sim \\log\\mathcal{N}(\\mu_{\\ln}, \\sigma_{\\ln})$, where $\\ln(X) \\sim \\mathcal{N}(\\mu_{\\ln}, \\sigma_{\\ln})$:\n  The transformation from $u$ to $x$ is $x = \\exp(\\mu_{\\ln} + \\sigma_{\\ln} u)$.\n\nThe algorithm will be applied to each of the three test cases.\n\n**Test Case 1**: $g(R, S) = R - S$ with $R \\sim \\mathcal{N}(1500, 150)$ and $S \\sim \\mathcal{N}(1300, 200)$. This is a linear limit-state function with normal variables, for which FORM yields an exact solution in a single iteration. The analytical solution is $\\beta = (\\mu_R - \\mu_S) / \\sqrt{\\sigma_R^2 + \\sigma_S^2} = (1500 - 1300) / \\sqrt{150^2 + 200^2} = 200 / 250 = 0.8$.\n\n**Test Case 2**: $g(R, F_q, \\theta) = R - 0.3 F_q \\sin(\\theta)$ with $R \\sim \\mathcal{N}(2000, 300)$, $F_q \\sim \\log\\mathcal{N}(\\ln(1000), 0.2)$, and $\\theta \\sim \\mathcal{N}(0.5, 0.1)$. Due to the nonlinear terms $F_q$ (lognormal) and $\\sin(\\theta)$, the iterative procedure is necessary.\n\n**Test Case 3**: $g(R, F_m, \\theta) = R - F_m \\cos(\\theta)$ with $R \\sim \\mathcal{N}(2500, 250)$, $F_m \\sim \\mathcal{N}(2400, 250)$, and $\\theta \\sim \\mathcal{N}(0.1, 0.05)$. The nonlinearity from $\\cos(\\theta)$ necessitates the iterative solution.\n\nThe implementation will follow these steps, providing the reliability index $\\beta$ and failure probability $P_f$ for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nimport json\n\ndef solve():\n    \"\"\"\n    Implements the First-Order Reliability Method (FORM) to approximate\n    the probability of failure for three biomechanical test cases.\n    \"\"\"\n\n    def run_form(limit_state_func, variables, constants, max_iter=100, tol=1e-7, h=1e-6):\n        \"\"\"\n        Executes the HLRF algorithm for a given problem setup.\n\n        Args:\n            limit_state_func (callable): The limit-state function g(x, **consts).\n            variables (list): A list of dictionaries, one for each random variable.\n            constants (dict): A dictionary of constants for the limit-state function.\n            max_iter (int): Maximum number of iterations.\n            tol (float): Convergence tolerance.\n            h (float): Step size for central difference gradient calculation.\n\n        Returns:\n            tuple: A tuple containing the reliability index (beta) and failure probability (Pf).\n        \"\"\"\n        n_vars = len(variables)\n        u_k = np.zeros(n_vars)\n\n        def transform_u_to_x(u_vec):\n            \"\"\"Transforms a vector from standard normal space to physical space.\"\"\"\n            x_vec = np.zeros_like(u_vec)\n            for i, var_info in enumerate(variables):\n                if var_info['dist'] == 'normal':\n                    x_vec[i] = var_info['mu'] + var_info['sigma'] * u_vec[i]\n                elif var_info['dist'] == 'lognormal':\n                    x_vec[i] = np.exp(var_info['mu_ln'] + var_info['sigma_ln'] * u_vec[i])\n            return x_vec\n\n        def evaluate_g_in_u_space(u_vec):\n            \"\"\"Evaluates the limit-state function for a given u_vector.\"\"\"\n            x_vec = transform_u_to_x(u_vec)\n            return limit_state_func(x_vec, **constants)\n\n        for _ in range(max_iter):\n            x_k = transform_u_to_x(u_k)\n            g_k = limit_state_func(x_k, **constants)\n\n            # Numerically compute gradient of g w.r.t. u using central differences\n            grad_g_u = np.zeros(n_vars)\n            for i in range(n_vars):\n                u_plus = u_k.copy()\n                u_minus = u_k.copy()\n                u_plus[i] += h\n                u_minus[i] -= h\n                g_plus = evaluate_g_in_u_space(u_plus)\n                g_minus = evaluate_g_in_u_space(u_minus)\n                grad_g_u[i] = (g_plus - g_minus) / (2 * h)\n            \n            norm_grad = np.linalg.norm(grad_g_u)\n            if np.isclose(norm_grad, 0):\n                # Gradient is zero, might be at an extremum. Or the failure\n                # probability is very high/low. Break gracefully.\n                # For these problems, this is not expected away from solution.\n                break\n\n            alpha_k = grad_g_u / norm_grad\n            \n            # HLRF update formula\n            u_k_plus_1 = alpha_k * (np.dot(alpha_k, u_k) - g_k / norm_grad)\n\n            if np.linalg.norm(u_k_plus_1 - u_k) < tol:\n                u_k = u_k_plus_1\n                break\n            \n            u_k = u_k_plus_1\n        \n        # The converged point u_k is the design point u*\n        u_star = u_k\n        beta = np.linalg.norm(u_star)\n        pf = norm.cdf(-beta)\n        \n        return beta, pf\n\n    # Test Case 1: Linear strength-demand\n    case1 = {\n        'limit_state_func': lambda x, **kwargs: x[0] - x[1], # R - S\n        'variables': [\n            {'dist': 'normal', 'mu': 1500, 'sigma': 150}, # R\n            {'dist': 'normal', 'mu': 1300, 'sigma': 200}, # S\n        ],\n        'constants': {}\n    }\n\n    # Test Case 2: Nonlinear ACL loading\n    case2 = {\n        'limit_state_func': lambda x, c: x[0] - c * x[1] * np.sin(x[2]), # R - c*Fq*sin(theta)\n        'variables': [\n            {'dist': 'normal', 'mu': 2000, 'sigma': 300}, # R\n            {'dist': 'lognormal', 'mu_ln': np.log(1000), 'sigma_ln': 0.2}, # Fq\n            {'dist': 'normal', 'mu': 0.5, 'sigma': 0.1}, # theta\n        ],\n        'constants': {'c': 0.3}\n    }\n\n    # Test Case 3: Nonlinear Achilles tendon loading\n    case3 = {\n        'limit_state_func': lambda x, **kwargs: x[0] - x[1] * np.cos(x[2]), # R - Fm*cos(theta)\n        'variables': [\n            {'dist': 'normal', 'mu': 2500, 'sigma': 250}, # R\n            {'dist': 'normal', 'mu': 2400, 'sigma': 250}, # Fm\n            {'dist': 'normal', 'mu': 0.1, 'sigma': 0.05}, # theta\n        ],\n        'constants': {}\n    }\n\n    test_cases = [case1, case2, case3]\n    results = []\n\n    for case in test_cases:\n        beta, pf = run_form(case['limit_state_func'], case['variables'], case['constants'])\n        results.append([round(beta, 6), round(pf, 6)])\n\n    # Use json.dumps for precise list-of-lists formatting without spaces\n    print(json.dumps(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}