{
    "hands_on_practices": [
        {
            "introduction": "Disproportionality analysis is a cornerstone of sifting through vast spontaneous reporting databases to identify potential safety signals. More advanced than simple frequency counts, methods like the Bayesian Confidence Propagation Neural Network (BCPNN) provide statistically robust measures of association, such as the Information Component ($IC$). This practice  will guide you through the application of this method from first principles, illustrating how its underlying Bayesian framework can stabilize estimates and improve the reliability of signal detection.",
            "id": "4520104",
            "problem": "A spontaneous reporting database is summarized by a $2 \\times 2$ contingency table for drug $D$ and adverse event $E$ with total reports $N$, joint count $n_{DE}$, and marginals $n_{D \\cdot}$ and $n_{\\cdot E}$. Consider a Bayesian shrinkage model for the four cell probabilities $(p_{11}, p_{10}, p_{01}, p_{00})$ with a symmetric Dirichlet prior with pseudocount $\\alpha$ on each cell, where the data are modeled as multinomially distributed. For the Bayesian Confidence Propagation Neural Network (BCPNN) disproportionality metric, the Information Component (IC) is defined as $IC = \\log_{2}\\!\\left(\\frac{p_{11}}{(p_{11}+p_{10})(p_{11}+p_{01})}\\right)$.\n\nStarting from the definitions of the multinomial likelihood and the Dirichlet conjugate prior, use the posterior distribution of $(p_{11}, p_{10}, p_{01}, p_{00})$ to:\n- compute a point estimate of $IC$ by evaluating the defining function at the posterior mean of $(p_{11}, p_{10}, p_{01}, p_{00})$, and\n- compute the delta-method standard error of $IC$ by propagating the posterior covariance of the Dirichlet distribution through the above transformation.\n\nYou are given $n_{DE} = 20$, $n_{D \\cdot} = 200$, $n_{\\cdot E} = 150$, $N = 5000$, and a symmetric Dirichlet prior with $\\alpha = 1$ on each of the four cells. Report the pair consisting of the point estimate of $IC$ and its delta-method standard error. Round both values to four significant figures. No units are required.",
            "solution": "The problem requires the computation of a point estimate and a standard error for the Information Component ($IC$) within a Bayesian framework.\n\nFirst, we establish the $2 \\times 2$ contingency table notation and calculate the counts for all four cells. Let cell $(i, j)$ correspond to drug status $i$ and event status $j$, where $i=1$ for drug $D$ present and $i=0$ for drug $D$ absent, and similarly $j=1$ for event $E$ present and $j=0$ for event $E$ absent.\n\nThe given counts are:\nTotal reports: $N=5000$\nDrug $D$ and Event $E$: $n_{11} = n_{DE} = 20$\nDrug $D$ marginal: $n_{D \\cdot} = n_{11} + n_{10} = 200$\nEvent $E$ marginal: $n_{\\cdot E} = n_{11} + n_{01} = 150$\n\nFrom these, we derive the other cell counts:\nDrug $D$, no Event $E$: $n_{10} = n_{D \\cdot} - n_{11} = 200 - 20 = 180$\nNo Drug $D$, Event $E$: $n_{01} = n_{\\cdot E} - n_{11} = 150 - 20 = 130$\nNo Drug $D$, no Event $E$: $n_{00} = N - n_{11} - n_{10} - n_{01} = 5000 - 20 - 180 - 130 = 4670$\nThe full set of counts is $(n_{11}, n_{10}, n_{01}, n_{00}) = (20, 180, 130, 4670)$.\n\nThe model assumes the data $\\mathbf{n} = (n_{11}, n_{10}, n_{01}, n_{00})$ follows a Multinomial distribution with parameters $N$ and cell probabilities $\\mathbf{p} = (p_{11}, p_{10}, p_{01}, p_{00})$. The prior on $\\mathbf{p}$ is a symmetric Dirichlet distribution, $\\mathbf{p} \\sim \\text{Dir}(\\alpha, \\alpha, \\alpha, \\alpha)$, with $\\alpha=1$.\n\nThe Dirichlet distribution is the conjugate prior for the Multinomial likelihood. Therefore, the posterior distribution of $\\mathbf{p}$ given the data $\\mathbf{n}$ is also a Dirichlet distribution:\n$$ \\mathbf{p} \\,|\\, \\mathbf{n} \\sim \\text{Dir}(\\alpha'_{11}, \\alpha'_{10}, \\alpha'_{01}, \\alpha'_{00}) $$\nwhere the posterior parameters are $\\alpha'_{ij} = n_{ij} + \\alpha$.\nWith $\\alpha=1$, the posterior parameters are:\n$\\alpha'_{11} = 20 + 1 = 21$\n$\\alpha'_{10} = 180 + 1 = 181$\n$\\alpha'_{01} = 130 + 1 = 131$\n$\\alpha'_{00} = 4670 + 1 = 4671$\n\nThe sum of the posterior parameters is $\\alpha'_0 = \\sum_{i,j} \\alpha'_{ij} = 21 + 181 + 131 + 4671 = 5004$. Note that this is also $N + 4\\alpha = 5000 + 4(1) = 5004$.\n\n**1. Point Estimate of IC**\n\nThe point estimate is calculated by substituting the posterior mean of the probabilities into the definition of $IC$. The posterior mean of $p_{ij}$ is $E[p_{ij} | \\mathbf{n}] = \\hat{p}_{ij} = \\frac{\\alpha'_{ij}}{\\alpha'_0}$.\nThe IC is defined as $g(\\mathbf{p}) = IC = \\log_{2}\\!\\left(\\frac{p_{11}}{p_{D\\cdot} p_{\\cdot E}}\\right)$, where $p_{D\\cdot} = p_{11}+p_{10}$ and $p_{\\cdot E} = p_{11}+p_{01}$.\nThe point estimate $\\widehat{IC}$ is:\n$$ \\widehat{IC} = g(\\hat{\\mathbf{p}}) = \\log_{2}\\!\\left(\\frac{\\hat{p}_{11}}{\\hat{p}_{D\\cdot}\\hat{p}_{\\cdot E}}\\right) $$\nThe posterior means of the marginal probabilities are $\\hat{p}_{D\\cdot} = \\hat{p}_{11}+\\hat{p}_{10} = \\frac{\\alpha'_{11}+\\alpha'_{10}}{\\alpha'_0}$ and $\\hat{p}_{\\cdot E} = \\hat{p}_{11}+\\hat{p}_{01} = \\frac{\\alpha'_{11}+\\alpha'_{01}}{\\alpha'_0}$.\nLet $\\alpha'_{D\\cdot} = \\alpha'_{11}+\\alpha'_{10} = 21+181 = 202$ and $\\alpha'_{\\cdot E} = \\alpha'_{11}+\\alpha'_{01} = 21+131 = 152$.\nThen,\n$$ \\widehat{IC} = \\log_{2}\\!\\left(\\frac{\\alpha'_{11}/\\alpha'_{0}}{(\\alpha'_{D\\cdot}/\\alpha'_{0})(\\alpha'_{\\cdot E}/\\alpha'_{0})}\\right) = \\log_{2}\\!\\left(\\frac{\\alpha'_{11}\\alpha'_0}{\\alpha'_{D\\cdot}\\alpha'_{\\cdot E}}\\right) $$\nSubstituting the values:\n$$ \\widehat{IC} = \\log_{2}\\!\\left(\\frac{21 \\times 5004}{202 \\times 152}\\right) = \\log_{2}\\!\\left(\\frac{105084}{30704}\\right) \\approx \\log_{2}(3.42248567) $$\n$$ \\widehat{IC} = \\frac{\\ln(3.42248567)}{\\ln(2)} \\approx 1.775193 $$\nRounding to four significant figures, the point estimate is $1.775$.\n\n**2. Delta-Method Standard Error of IC**\n\nThe delta method approximates the variance of a function $g(\\mathbf{p})$ as $\\text{Var}(g(\\mathbf{p})) \\approx (\\nabla g)^T \\Sigma_{\\mathbf{p}} (\\nabla g)$, evaluated at the posterior mean $\\hat{\\mathbf{p}}$. For a posterior Dirichlet distribution, this simplifies to:\n$$ \\text{Var}(g(\\mathbf{p})) \\approx \\frac{1}{\\alpha'_0+1} \\left[ \\sum_{i,j} \\hat{p}_{ij} \\left(\\frac{\\partial g}{\\partial p_{ij}}\\right)^2 - \\left(\\sum_{i,j} \\hat{p}_{ij} \\frac{\\partial g}{\\partial p_{ij}}\\right)^2 \\right] $$\nLet's first express $g(\\mathbf{p})$ using the natural logarithm: $g(\\mathbf{p}) = \\frac{1}{\\ln(2)}[\\ln(p_{11}) - \\ln(p_{11}+p_{10}) - \\ln(p_{11}+p_{01})]$. Let $C = 1/\\ln(2)$.\nThe partial derivatives, evaluated at the posterior mean $\\hat{\\mathbf{p}}$, are:\n$\\frac{\\partial g}{\\partial p_{11}} = C \\left(\\frac{1}{p_{11}} - \\frac{1}{p_{D\\cdot}} - \\frac{1}{p_{\\cdot E}}\\right)$\n$\\frac{\\partial g}{\\partial p_{10}} = C \\left(-\\frac{1}{p_{D\\cdot}}\\right)$\n$\\frac{\\partial g}{\\partial p_{01}} = C \\left(-\\frac{1}{p_{\\cdot E}}\\right)$\n$\\frac{\\partial g}{\\partial p_{00}} = 0$\n\nLet's compute the two terms in the variance formula. The first term is $E_{\\hat{\\mathbf{p}}}[\\nabla g] = \\sum \\hat{p}_{ij}\\frac{\\partial g}{\\partial p_{ij}}$:\n$$ \\sum_{i,j} \\hat{p}_{ij}\\frac{\\partial g}{\\partial p_{ij}} = C \\left[ \\hat{p}_{11}\\left(\\frac{1}{\\hat{p}_{11}} - \\frac{1}{\\hat{p}_{D\\cdot}} - \\frac{1}{\\hat{p}_{\\cdot E}}\\right) - \\hat{p}_{10}\\frac{1}{\\hat{p}_{D\\cdot}} - \\hat{p}_{01}\\frac{1}{\\hat{p}_{\\cdot E}} \\right] $$\n$$ = C \\left[ 1 - \\frac{\\hat{p}_{11}}{\\hat{p}_{D\\cdot}} - \\frac{\\hat{p}_{11}}{\\hat{p}_{\\cdot E}} - \\frac{\\hat{p}_{10}}{\\hat{p}_{D\\cdot}} - \\frac{\\hat{p}_{01}}{\\hat{p}_{\\cdot E}} \\right] = C \\left[ 1 - \\frac{\\hat{p}_{11}+\\hat{p}_{10}}{\\hat{p}_{D\\cdot}} - \\frac{\\hat{p}_{11}+\\hat{p}_{01}}{\\hat{p}_{\\cdot E}} \\right] $$\n$$ = C [1 - 1 - 1] = -C $$\nThe second term is $E_{\\hat{\\mathbf{p}}}[(\\nabla g)^2] = \\sum \\hat{p}_{ij}(\\frac{\\partial g}{\\partial p_{ij}})^2$:\n$$ \\sum_{i,j} \\hat{p}_{ij}\\left(\\frac{\\partial g}{\\partial p_{ij}}\\right)^2 = C^2 \\left[ \\hat{p}_{11}\\left(\\frac{1}{\\hat{p}_{11}}-\\frac{1}{\\hat{p}_{D\\cdot}}-\\frac{1}{\\hat{p}_{\\cdot E}}\\right)^2 + \\frac{\\hat{p}_{10}}{\\hat{p}_{D\\cdot}^2} + \\frac{\\hat{p}_{01}}{\\hat{p}_{\\cdot E}^2} \\right] $$\nExpanding and simplifying this expression yields:\n$$ \\sum_{i,j} \\hat{p}_{ij}\\left(\\frac{\\partial g}{\\partial p_{ij}}\\right)^2 = C^2 \\left[ \\frac{1}{\\hat{p}_{11}} - \\frac{1}{\\hat{p}_{D\\cdot}} - \\frac{1}{\\hat{p}_{\\cdot E}} + \\frac{2\\hat{p}_{11}}{\\hat{p}_{D\\cdot}\\hat{p}_{\\cdot E}} \\right] $$\nNow, substituting these simplified forms into the variance formula:\n$$ \\text{Var}(IC) \\approx \\frac{1}{\\alpha'_0+1} \\left[ C^2 \\left( \\frac{1}{\\hat{p}_{11}} - \\frac{1}{\\hat{p}_{D\\cdot}} - \\frac{1}{\\hat{p}_{\\cdot E}} + \\frac{2\\hat{p}_{11}}{\\hat{p}_{D\\cdot}\\hat{p}_{\\cdot E}} \\right) - (-C)^2 \\right] $$\n$$ \\text{Var}(IC) \\approx \\frac{C^2}{\\alpha'_0+1} \\left[ \\frac{1}{\\hat{p}_{11}} - \\frac{1}{\\hat{p}_{D\\cdot}} - \\frac{1}{\\hat{p}_{\\cdot E}} + \\frac{2\\hat{p}_{11}}{\\hat{p}_{D\\cdot}\\hat{p}_{\\cdot E}} - 1 \\right] $$\nLet's substitute values into the term in the brackets.\n$\\frac{1}{\\hat{p}_{11}} = \\frac{\\alpha'_0}{\\alpha'_{11}} = \\frac{5004}{21} \\approx 238.2857$\n$\\frac{1}{\\hat{p}_{D\\cdot}} = \\frac{\\alpha'_0}{\\alpha'_{D\\cdot}} = \\frac{5004}{202} \\approx 24.7723$\n$\\frac{1}{\\hat{p}_{\\cdot E}} = \\frac{\\alpha'_0}{\\alpha'_{\\cdot E}} = \\frac{5004}{152} \\approx 32.9211$\n$\\frac{2\\hat{p}_{11}}{\\hat{p}_{D\\cdot}\\hat{p}_{\\cdot E}} = \\frac{2\\alpha'_{11}\\alpha'_0}{\\alpha'_{D\\cdot}\\alpha'_{\\cdot E}} = \\frac{2 \\times 21 \\times 5004}{202 \\times 152} \\approx 6.8450$\nThe term in brackets is $238.2857 - 24.7723 - 32.9211 + 6.8450 - 1 = 186.4373$\nSo, the variance is:\n$$ \\text{Var}(IC) \\approx \\frac{1}{(\\ln(2))^2} \\frac{186.4373}{5004+1} \\approx \\frac{1}{0.480453} \\frac{186.4373}{5005} \\approx 2.081368 \\times 0.0372502 \\approx 0.0775317 $$\nThe standard error is the square root of the variance:\n$$ SE(IC) = \\sqrt{\\text{Var}(IC)} \\approx \\sqrt{0.0775317} \\approx 0.278445 $$\nRounding to four significant figures, the standard error is $0.2784$.\n\nThe pair consisting of the point estimate and its delta-method standard error is $(1.775, 0.2784)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.775  0.2784\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once a signal is flagged from a spontaneous reporting system, it often requires confirmation using more rigorous pharmacoepidemiological methods. Cohort studies, which track defined groups of patients over time, are a primary tool for such investigations. This exercise  focuses on calculating the Incidence Rate Ratio ($IRR$) and its confidence interval from person-time data, which is the standard measure for comparing event rates in these studies and forms the basis of many post-authorization safety assessments.",
            "id": "4520180",
            "problem": "A multinational post-authorization safety study evaluates a newly approved oral anticoagulant for its association with gastrointestinal bleeding relative to a well-established comparator. In a cohort assembled from routine care electronic health records and claims, the exposed group (new anticoagulant) contributed $t_{E} = 23{,}500$ person-years and experienced $k_{E} = 85$ first gastrointestinal bleeding events. The comparator group contributed $t_{C} = 31{,}200$ person-years and experienced $k_{C} = 96$ first gastrointestinal bleeding events. Assume event counts follow a Poisson process with rate parameters that are constant within each group and person-time is accurately measured.\n\nUsing a log-rate model with a Poisson variance assumption, derive from first principles a large-sample Wald confidence interval for the logarithm of the incidence rate ratio, then back-transform to obtain the incidence rate ratio. Compute the point estimate of the incidence rate ratio and its $95\\%$ confidence interval.\n\nExpress the incidence rate ratio and its confidence interval bounds as dimensionless quantities. Round the incidence rate ratio and the confidence interval bounds to four significant figures. The final answer must be a single row vector containing the incidence rate ratio, the lower confidence bound, and the upper confidence bound, in that order.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective, providing all necessary data and assumptions for a valid analysis. We can thus proceed with the solution.\n\nLet $\\lambda_E$ and $\\lambda_C$ be the true, unknown incidence rates of gastrointestinal bleeding in the exposed (new anticoagulant) and comparator groups, respectively. The units of these rates are events per person-year. The problem states that the number of events in each group follows a Poisson process. The number of events $K$ observed over a period of person-time $t$ is a random variable $K \\sim \\text{Poisson}(\\lambda t)$. The given data are the observed event counts $k_E = 85$ over $t_E = 23{,}500$ person-years and $k_C = 96$ over $t_C = 31{,}200$ person-years.\n\nThe maximum likelihood estimator for a Poisson rate $\\lambda$ is given by the observed number of events divided by the person-time, $\\hat{\\lambda} = k/t$. Therefore, the estimated rates for the two groups are:\n$$\n\\hat{\\lambda}_E = \\frac{k_E}{t_E} = \\frac{85}{23{,}500} \\text{ person-years}^{-1}\n$$\n$$\n\\hat{\\lambda}_C = \\frac{k_C}{t_C} = \\frac{96}{31{,}200} \\text{ person-years}^{-1}\n$$\nThe parameter of interest is the Incidence Rate Ratio (IRR), defined as the ratio of the two rates:\n$$\nIRR = \\frac{\\lambda_E}{\\lambda_C}\n$$\nThe point estimate of the IRR, denoted $\\widehat{IRR}$, is calculated from the estimated rates:\n$$\n\\widehat{IRR} = \\frac{\\hat{\\lambda}_E}{\\hat{\\lambda}_C} = \\frac{k_E/t_E}{k_C/t_C} = \\frac{k_E t_C}{k_C t_E}\n$$\nTo construct a confidence interval, it is standard practice to work with the natural logarithm of the IRR. The log-transformation yields a sampling distribution that is more closely approximated by a normal distribution. Let $\\theta = \\ln(IRR)$. The estimator for $\\theta$ is $\\hat{\\theta} = \\ln(\\widehat{IRR})$.\n$$\n\\hat{\\theta} = \\ln\\left(\\frac{\\hat{\\lambda}_E}{\\hat{\\lambda}_C}\\right) = \\ln(\\hat{\\lambda}_E) - \\ln(\\hat{\\lambda}_C)\n$$\nFor a large-sample Wald confidence interval, we require the variance of $\\hat{\\theta}$. Since the exposed and comparator groups are independent, the variance of the difference is the sum of the variances:\n$$\n\\text{Var}(\\hat{\\theta}) = \\text{Var}(\\ln(\\hat{\\lambda}_E)) + \\text{Var}(\\ln(\\hat{\\lambda}_C))\n$$\nWe derive the variance of a single log-rate estimator, $\\text{Var}(\\ln(\\hat{\\lambda}))$, using the Delta method. For a function $g(X)$, the variance is approximated by $\\text{Var}(g(X)) \\approx (g'(E[X]))^2 \\text{Var}(X)$. Here, our random variable is the estimator $\\hat{\\lambda} = K/t$, and the function is $g(\\hat{\\lambda}) = \\ln(\\hat{\\lambda})$.\n\nThe estimator $\\hat{\\lambda}$ is unbiased, so its expected value is $E[\\hat{\\lambda}] = E[K/t] = (\\lambda t)/t = \\lambda$.\nThe variance of $\\hat{\\lambda}$ is $\\text{Var}(\\hat{\\lambda}) = \\text{Var}(K/t) = \\frac{1}{t^2}\\text{Var}(K)$. Since $K \\sim \\text{Poisson}(\\lambda t)$, its variance is $\\text{Var}(K) = \\lambda t$. Thus,\n$$\n\\text{Var}(\\hat{\\lambda}) = \\frac{1}{t^2}(\\lambda t) = \\frac{\\lambda}{t}\n$$\nThe derivative of the function $g(\\lambda) = \\ln(\\lambda)$ is $g'(\\lambda) = 1/\\lambda$. Applying the Delta method:\n$$\n\\text{Var}(\\ln(\\hat{\\lambda})) \\approx \\left(\\frac{1}{E[\\hat{\\lambda}]}\\right)^2 \\text{Var}(\\hat{\\lambda}) = \\left(\\frac{1}{\\lambda}\\right)^2 \\left(\\frac{\\lambda}{t}\\right) = \\frac{1}{\\lambda^2} \\frac{\\lambda}{t} = \\frac{1}{\\lambda t}\n$$\nThe expression $\\lambda t$ represents the expected number of events. In practice, this is unknown and is estimated by substituting the MLE $\\hat{\\lambda}$ for $\\lambda$, yielding an estimated variance of $1/(\\hat{\\lambda}t) = 1/((k/t)t) = 1/k$. Thus, the estimated variance of the log-rate is simply the reciprocal of the observed event count.\n\nApplying this to our problem, the estimated variance of $\\hat{\\theta}$ is:\n$$\n\\widehat{\\text{Var}}(\\hat{\\theta}) = \\widehat{\\text{Var}}(\\ln(\\hat{\\lambda}_E)) + \\widehat{\\text{Var}}(\\ln(\\hat{\\lambda}_C)) = \\frac{1}{k_E} + \\frac{1}{k_C}\n$$\nThe standard error (SE) of $\\hat{\\theta}$ is the square root of this variance:\n$$\nSE(\\hat{\\theta}) = \\sqrt{\\frac{1}{k_E} + \\frac{1}{k_C}}\n$$\nA $(1-\\alpha)100\\%$ Wald confidence interval for $\\theta = \\ln(IRR)$ is given by $\\hat{\\theta} \\pm z_{1-\\alpha/2} SE(\\hat{\\theta})$, where $z_{1-\\alpha/2}$ is the critical value from the standard normal distribution. For a $95\\%$ confidence interval, $\\alpha = 0.05$, and $z_{0.975} \\approx 1.959964$. The confidence interval for $\\ln(IRR)$ is:\n$$\n\\ln(\\widehat{IRR}) \\pm z_{0.975} \\sqrt{\\frac{1}{k_E} + \\frac{1}{k_C}}\n$$\nTo obtain the confidence interval for the IRR, we exponentiate the lower and upper bounds of the interval for $\\ln(IRR)$:\n$$\nCI_{IRR} = \\exp\\left( \\ln(\\widehat{IRR}) \\pm z_{0.975} \\sqrt{\\frac{1}{k_E} + \\frac{1}{k_C}} \\right) = \\widehat{IRR} \\cdot \\exp\\left(\\pm z_{0.975} \\sqrt{\\frac{1}{k_E} + \\frac{1}{k_C}}\\right)\n$$\nNow, we substitute the given values: $k_E = 85$, $t_E = 23{,}500$, $k_C = 96$, $t_C = 31{,}200$.\n\nFirst, the point estimate of the IRR:\n$$\n\\widehat{IRR} = \\frac{85 \\times 31{,}200}{96 \\times 23{,}500} = \\frac{2{,}652{,}000}{2{,}256{,}000} \\approx 1.1755319\n$$\nNext, the standard error of the log-IRR:\n$$\nSE(\\ln(\\widehat{IRR})) = \\sqrt{\\frac{1}{85} + \\frac{1}{96}} \\approx \\sqrt{0.0117647 + 0.0104167} = \\sqrt{0.0221814} \\approx 0.1489342\n$$\nThe $95\\%$ confidence interval for the log-IRR is:\n$$\n\\ln(1.1755319) \\pm 1.959964 \\times 0.1489342\n$$\n$$\n0.161744 \\pm 0.291907\n$$\nThis gives the interval $[-0.130163, 0.453651]$ for $\\ln(IRR)$.\n\nFinally, we back-transform by exponentiating the bounds to get the $95\\%$ confidence interval for the IRR:\nLower bound: $L = \\exp(-0.130163) \\approx 0.877942$\nUpper bound: $U = \\exp(0.453651) \\approx 1.574044$\n\nRounding the point estimate and the confidence bounds to four significant figures:\nPoint estimate $\\widehat{IRR} = 1.176$\nLower bound $L = 0.8779$\nUpper bound $U = 1.574$\n\nThe point estimate of $1.176$ suggests a $17.6\\%$ increased rate of gastrointestinal bleeding with the new anticoagulant compared to the comparator. However, the $95\\%$ confidence interval, which spans from $0.8779$ to $1.574$, includes the null value of $1.0$. This indicates that the observed increase is not statistically significant at the $\\alpha=0.05$ level.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.176  0.8779  1.574 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Pharmacovigilance is not a one-time analysis but a dynamic process of continuous surveillance, requiring methods that monitor data streams as they accumulate. The Exponentially Weighted Moving Average (EWMA) chart is a powerful statistical process control tool adapted for this purpose, designed to detect small but persistent shifts in reporting rates by giving greater weight to recent data. In this computational practice , you will implement an EWMA chart from first principles to gain a practical understanding of its response to various temporal patterns, such as sudden spikes or changes in baseline counts.",
            "id": "4520144",
            "problem": "You are asked to construct and apply an Exponentially Weighted Moving Average (EWMA) chart for pharmacovigilance signal detection, grounded in first principles of probability and statistics. Consider a single drugâ€“event pair monitored monthly. Let the observed monthly count at month $t$ be $X_t$, and let the expected count under the null (no true change) be $\\mu_t$. Assume that, under the null, $X_t$ are independent and follow a Poisson distribution with mean $\\mu_t$, i.e., $X_t \\sim \\text{Poisson}(\\mu_t)$ and the sequence $\\{X_t\\}$ is independent over time.\n\nStarting from the fundamental definitions of linearity of expectation and variance of independent random variables, and the definition of an EWMA statistic:\n- Define the EWMA statistic $Z_t$ recursively via a smoothing parameter $\\lambda \\in (0,1)$ as $Z_t = \\lambda X_t + (1-\\lambda) Z_{t-1}$, with a head-start $Z_0 = \\mu_1$.\n- Derive the expected value $m_t = \\mathbb{E}[Z_t]$ recursively from first principles using the linearity of expectation and the independence assumption.\n- Derive the variance $v_t = \\text{Var}(Z_t)$ recursively from first principles using $\\text{Var}(aY + bZ) = a^2 \\text{Var}(Y) + b^2 \\text{Var}(Z) + 2ab\\,\\text{Cov}(Y,Z)$, the independence of $X_t$ from past observations, and the fact that for a Poisson random variable $Y \\sim \\text{Poisson}(\\theta)$, $\\text{Var}(Y) = \\theta$.\n- Using these derived recursions, compute time-varying two-sided control limits at each month $t$ as $m_t \\pm L \\sqrt{v_t}$, with width multiplier $L = 3$. A potential upper signal is flagged at month $t$ if $Z_t  m_t + L \\sqrt{v_t}$, and a potential lower signal is flagged if $Z_t  m_t - L \\sqrt{v_t}$.\n\nTake the smoothing parameter as $\\lambda = 0.2$. All quantities are dimensionless monthly counts; no physical units are required. Angles are not involved. Percentages are not involved.\n\nImplement this logic for each of the following three test cases (each spans $24$ months). For each case, treat the sequence $\\{\\mu_t\\}$ as known and fixed, and the observed counts $\\{x_t\\}$ as given data to be plugged into the EWMA recursion.\n\nTest Suite:\n- Case 1 (constant baseline, single extreme spike):\n    - $\\mu_t = 5$ for $t=1,\\dots,24$.\n    - $x_t = [5,5,5,5,5,5,5,5,5,5,5,5,50,5,5,5,5,5,5,5,5,5,5,5]$.\n- Case 2 (very low baseline, single spike amid zeros):\n    - $\\mu_t = 0.2$ for $t=1,\\dots,24$.\n    - $x_t = [0,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0]$.\n- Case 3 (stepwise-varying baseline, single extreme spike after a step change):\n    - $\\mu_t = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,8,8,8,8,8,8]$.\n    - $x_t$ equals $\\mu_t$ at all months except a spike at month $19$, i.e., $x_t = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,40,8,8,8,8,8]$.\n\nRequired computational steps:\n1. For each case, initialize $Z_0 = \\mu_1$, set $m_0 = \\mu_1$, and $v_0 = 0$.\n2. For $t = 1,\\dots,24$, update $Z_t$, $m_t$, and $v_t$ using only fundamental laws (linearity of expectation and variance of independent sums) together with $\\text{Var}(X_t) = \\mu_t$ for Poisson counts.\n3. At each $t$, compute the upper and lower control limits and record the month index $t$ if a potential signal occurs as defined above.\n\nFinal Output Format:\n- Your program should produce a single line of output containing a JSON-like representation of a list of three elements (one per test case).\n- Each element must itself be a list of two lists: the first is the list of months (using $1$-based indexing) with upper signals, and the second is the list of months with lower signals.\n- For example, the output format must be of the form: \"[[ [u1_1,u1_2,...], [l1_1,l1_2,...] ], [ [u2_1,...], [l2_1,...] ], [ [u3_1,...], [l3_1,...] ]]\" with no spaces.",
            "solution": "We construct the Exponentially Weighted Moving Average (EWMA) statistic and its control limits by starting from the definition of $Z_t$ and applying fundamental properties of expectation and variance.\n\nDefinition of the Exponentially Weighted Moving Average (EWMA):\nFor smoothing parameter $\\lambda \\in (0,1)$ and a head-start $Z_0 = \\mu_1$, define the recursive statistic\n$$\nZ_t = \\lambda X_t + (1 - \\lambda) Z_{t-1}, \\quad t = 1,2,\\dots,24.\n$$\nThis is the standard definition of the Exponentially Weighted Moving Average (EWMA), which emphasizes recent observations while retaining information from the past.\n\nAssumptions under the null (no true change in the underlying process beyond the specified $\\mu_t$):\n- The observed count $X_t$ at time $t$ is a Poisson random variable with mean $\\mu_t$, i.e., $X_t \\sim \\text{Poisson}(\\mu_t)$.\n- The sequence $\\{X_t\\}$ is independent over time.\n\nWe now derive the expected value $m_t = \\mathbb{E}[Z_t]$ and variance $v_t = \\text{Var}(Z_t)$ from first principles.\n\nExpected value recursion:\nUsing linearity of expectation,\n$$\nm_t = \\mathbb{E}[Z_t] = \\mathbb{E}[\\lambda X_t + (1-\\lambda) Z_{t-1}]\n= \\lambda \\mathbb{E}[X_t] + (1-\\lambda) \\mathbb{E}[Z_{t-1}]\n= \\lambda \\mu_t + (1-\\lambda) m_{t-1},\n$$\nwith initialization $m_0 = \\mathbb{E}[Z_0] = \\mu_1$ because $Z_0$ is fixed to $\\mu_1$.\n\nVariance recursion:\nUsing the variance of a linear combination and the independence assumption, note that $Z_{t-1}$ depends only on $X_1,\\dots,X_{t-1}$, while $X_t$ is independent of the past. Thus $\\text{Cov}(Z_{t-1}, X_t) = 0$. Using $\\text{Var}(aY + bZ) = a^2 \\text{Var}(Y) + b^2 \\text{Var}(Z) + 2ab \\,\\text{Cov}(Y,Z)$, we obtain\n$$\nv_t = \\text{Var}(Z_t) = \\text{Var}(\\lambda X_t + (1-\\lambda) Z_{t-1})\n= \\lambda^2 \\text{Var}(X_t) + (1-\\lambda)^2 \\text{Var}(Z_{t-1}) + 2 \\lambda (1-\\lambda) \\text{Cov}(X_t, Z_{t-1}).\n$$\nBy independence, $\\text{Cov}(X_t, Z_{t-1}) = 0$. For Poisson $X_t$, $\\text{Var}(X_t) = \\mu_t$. Therefore,\n$$\nv_t = (1-\\lambda)^2 v_{t-1} + \\lambda^2 \\mu_t,\n$$\nwith initialization $v_0 = \\text{Var}(Z_0) = 0$ because $Z_0$ is fixed.\n\nControl limits at time $t$:\nAt each time $t$, we compute the time-varying center and limits as\n$$\n\\text{Center: } m_t, \\quad\n\\text{Standard deviation: } s_t = \\sqrt{v_t}, \\quad\n\\text{Upper control limit: } \\text{UCL}_t = m_t + L s_t, \\quad\n\\text{Lower control limit: } \\text{LCL}_t = m_t - L s_t,\n$$\nwhere $L = 3$ is the width multiplier.\n\nSignal logic:\n- Upper signal at month $t$ if $Z_t  \\text{UCL}_t$.\n- Lower signal at month $t$ if $Z_t  \\text{LCL}_t$.\n\nWe now apply this to the specified test cases with $\\lambda = 0.2$.\n\nInitialization common to all cases:\n- $Z_0 = \\mu_1$,\n- $m_0 = \\mu_1$,\n- $v_0 = 0$.\n\nCase 1:\n- $\\mu_t = 5$ for $t=1,\\dots,24$.\n- $x_t = [5,5,5,5,5,5,5,5,5,5,5,5,50,5,5,5,5,5,5,5,5,5,5,5]$.\n\nBecause $\\mu_t$ is constant and $Z_0 = \\mu_1$, it follows from the recursion that $m_t = 5$ for all $t$. The variance $v_t$ converges quickly to its steady-state value $v_\\infty = \\frac{\\lambda}{2-\\lambda} \\mu \\approx \\frac{0.2}{1.8}\\cdot 5 \\approx 0.555\\ldots$; numerically, after the initial transient, $s_t \\approx \\sqrt{0.555\\ldots} \\approx 0.745$, so $\\text{UCL}_t \\approx 5 + 3\\times 0.745 \\approx 7.236$ and $\\text{LCL}_t \\approx 2.764$. The long sequence of $x_t=5$ keeps $Z_t$ at $5$ until month $13$, at which $x_{13}=50$ causes a jump:\n$$\nZ_{13} = (1-\\lambda) Z_{12} + \\lambda x_{13} \\approx 0.8 \\cdot 5 + 0.2 \\cdot 50 = 14,\n$$\nwhich is well above $\\text{UCL}_{13}$. Thereafter, with $x_t = 5$, the difference $d_t = Z_t - m_t$ decays by a factor of $(1-\\lambda) = 0.8$ each month. Solving $9 \\cdot 0.8^k  3 s_t \\approx 2.236$ gives $k = 0,1,2,3,4,5,6$, corresponding to months $13$ through $19$. No lower signals occur.\n\nResult for Case 1:\n- Upper signals at months $[13,14,15,16,17,18,19]$.\n- Lower signals at months $[]$.\n\nCase 2:\n- $\\mu_t = 0.2$ for $t=1,\\dots,24$.\n- $x_t = [0,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0]$.\n\nWith constant $\\mu_t$ and $Z_0 = \\mu_1$, $m_t = 0.2$ for all $t$. The steady-state variance is $v_\\infty = \\frac{\\lambda}{2-\\lambda} \\mu \\approx \\frac{0.2}{1.8}\\cdot 0.2 \\approx 0.02222\\ldots$, so $s_t \\approx 0.149$ and $\\text{UCL}_t \\approx 0.2 + 3 \\cdot 0.149 \\approx 0.647$. The initial run of zeros drives $Z_t$ down but never below the lower control limit because it remains positive. At month $11$, the spike to $x_{11} = 5$ yields\n$$\nZ_{11} \\approx 0.8 Z_{10} + 0.2 \\cdot 5 \\approx 0.8 \\cdot 0.0215 + 1 \\approx 1.017,\n$$\nwhich is greater than $\\text{UCL}_{11}$. With subsequent zeros, $d_t = Z_t - m_t$ decays by $0.8$ per month, so $d_{11} \\approx 0.817$. Solving $0.817 \\cdot 0.8^k  3 s_t \\approx 0.447$ gives $k = 0,1,2$, corresponding to months $11,12,13$. No lower signals occur.\n\nResult for Case 2:\n- Upper signals at months $[11,12,13]$.\n- Lower signals at months $[]$.\n\nCase 3:\n- $\\mu_t = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,8,8,8,8,8,8]$ (stepwise increases).\n- $x_t = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,40,8,8,8,8,8]$ (equals baseline except a spike at month $19$).\n\nWe compute $m_t$ and $v_t$ recursively. Because $Z_0 = \\mu_1$, $m_0 = \\mu_1$. The expected value follows $m_t = (1-\\lambda) m_{t-1} + \\lambda \\mu_t$ and adapts to step changes gradually. The variance follows $v_t = (1-\\lambda)^2 v_{t-1} + \\lambda^2 \\mu_t$; when $\\mu_t$ is higher, the variance increases slightly, and the standard deviation $s_t = \\sqrt{v_t}$ approaches $\\sqrt{\\frac{\\lambda}{2-\\lambda} \\mu_t}$ during periods of stability.\n\nAt month $19$, the spike $x_{19} = 40$ with $\\mu_{19} = 8$ yields\n$$\nZ_{19} \\approx 0.8 Z_{18} + 0.2 \\cdot 40, \\quad m_{19} = 0.8 m_{18} + 0.2 \\cdot 8.\n$$\nSince $m_{18}$ is close to $6$ and $Z_{18}$ is close to $6$ from the prior stable period, we get $Z_{19} \\approx 12.8$ and $m_{19} \\approx 6.4$. The standard deviation $s_{19}$ is approximately $0.86$, so $\\text{UCL}_{19} \\approx 6.4 + 3 \\cdot 0.86 \\approx 8.99$. Thus month $19$ is an upper signal. Thereafter, with $x_t = \\mu_t = 8$, the deviation $d_t = Z_t - m_t$ obeys $d_t = (1-\\lambda) d_{t-1}$ and decays by a factor of $0.8$ each month while $s_t$ approaches $\\sqrt{\\frac{0.2}{1.8}\\cdot 8} \\approx 0.943$. Solving $d_{19} \\cdot 0.8^k  3 s_t$ with $d_{19} \\approx 6.4$ and $3 s_t \\approx 2.83$ yields $k = 0,1,2,3$, corresponding to months $19,20,21,22$. No lower signals occur.\n\nResult for Case 3:\n- Upper signals at months $[19,20,21,22]$.\n- Lower signals at months $[]$.\n\nTherefore, the final aggregated results to be printed are:\n- Case 1: $[ [13,14,15,16,17,18,19], [] ]$,\n- Case 2: $[ [11,12,13], [] ]$,\n- Case 3: $[ [19,20,21,22], [] ]$.\n\nThe program must output a single line with no spaces: \"[[[13,14,15,16,17,18,19],[]],[[11,12,13],[]],[[19,20,21,22],[]]]\".",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport json\nimport math\n\ndef ewma_signals(x, mu, lam=0.2, L=3.0):\n    n = len(x)\n    # Initialize\n    z_prev = mu[0]\n    m_prev = mu[0]\n    v_prev = 0.0\n\n    upper_signal_months = []\n    lower_signal_months = []\n\n    for t in range(1, n + 1):\n        xt = x[t - 1]\n        mut = mu[t - 1]\n\n        # Update EWMA\n        z_t = lam * xt + (1.0 - lam) * z_prev\n\n        # Update mean and variance under null\n        m_t = (1.0 - lam) * m_prev + lam * mut\n        v_t = (1.0 - lam) ** 2 * v_prev + (lam ** 2) * mut\n\n        s_t = math.sqrt(v_t)\n        ucl = m_t + L * s_t\n        lcl = m_t - L * s_t\n\n        # Check signals (1-based month index)\n        if z_t > ucl:\n            upper_signal_months.append(t)\n        if z_t  lcl:\n            lower_signal_months.append(t)\n\n        # Prepare for next iteration\n        z_prev = z_t\n        m_prev = m_t\n        v_prev = v_t\n\n    return upper_signal_months, lower_signal_months\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case 1\n    mu1 = [5.0] * 24\n    x1 = [5,5,5,5,5,5,5,5,5,5,5,5,50,5,5,5,5,5,5,5,5,5,5,5]\n\n    # Case 2\n    mu2 = [0.2] * 24\n    x2 = [0,0,0,0,0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0]\n\n    # Case 3\n    mu3 = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,8,8,8,8,8,8]\n    x3 = [2,2,2,2,2,2,4,4,4,4,4,4,6,6,6,6,6,6,40,8,8,8,8,8]\n\n    lam = 0.2\n    L = 3.0\n\n    results = []\n    for x, mu in [(x1, mu1), (x2, mu2), (x3, mu3)]:\n        upper, lower = ewma_signals(x, mu, lam=lam, L=L)\n        results.append([upper, lower])\n\n    # Final print statement in the exact required format (no spaces).\n    print(json.dumps(results, separators=(',',':')))\n\nsolve()\n```"
        }
    ]
}