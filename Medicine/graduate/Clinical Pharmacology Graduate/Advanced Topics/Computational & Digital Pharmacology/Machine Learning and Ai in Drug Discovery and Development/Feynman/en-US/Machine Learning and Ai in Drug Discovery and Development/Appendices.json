{
    "hands_on_practices": [
        {
            "introduction": "Machine learning models build quantitative relationships between a molecule's features and its biological properties. However, these features—such as partition coefficients, molecular weights, and surface areas—often have vastly different units and numerical ranges. This exercise () demonstrates a critical preprocessing step, standardization, which transforms descriptors onto a common scale to ensure the model learns from the underlying chemical information rather than arbitrary numerical magnitudes.",
            "id": "4563981",
            "problem": "A quantitative structure-property modeling task in Clinical Pharmacology uses Machine Learning (ML) and Artificial Intelligence (AI) to predict the fraction unbound in plasma from physicochemical descriptors. Three descriptors are considered: acid dissociation constant ($pK_{a}$), octanol-water partition coefficient ($\\log_{10} P$) and polar surface area (PSA). The training dataset for the model comprises molecules with descriptor components that are approximately unimodal and symmetric after routine curation. You are asked to construct a normalization scheme that enables a downstream linear model to treat each descriptor on a comparable scale, avoiding implicit weighting due to differing units and ranges.\n\nBegin from core definitions of sample mean and sample standard deviation, and general principles of affine transformations of random variables. Explain, by examining the physical units and typical ranges of $pK_{a}$ (dimensionless logarithmic constant), $\\log_{10} P$ (dimensionless logarithmic ratio), and PSA (an area measured in square angstroms, $\\text{\\AA}^{2}$), why a per-descriptor transformation that centers and scales each component to unitless zero mean and unit variance is appropriate for many ML algorithms in drug discovery and development. Then derive, from first principles, the affine transformation that maps any descriptor component to have zero training-set mean and unit training-set variance.\n\nYou are provided training-set summary statistics and a new compound’s descriptor vector:\n- Training-set means: $\\mu_{pK_{a}} = 6.80$, $\\mu_{\\log_{10} P} = 2.50$, $\\mu_{\\mathrm{PSA}} = 85.0$ (PSA in $\\text{\\AA}^{2}$).\n- Training-set standard deviations: $\\sigma_{pK_{a}} = 1.20$, $\\sigma_{\\log_{10} P} = 1.10$, $\\sigma_{\\mathrm{PSA}} = 30.0$ (PSA in $\\text{\\AA}^{2}$).\n- New compound descriptors: $x_{pK_{a}} = 7.40$, $x_{\\log_{10} P} = 3.20$, $x_{\\mathrm{PSA}} = 78.0$ (PSA in $\\text{\\AA}^{2}$).\n\nApply your derived transformation componentwise to the new compound’s descriptor vector to obtain a standardized row vector. Round each component to four significant figures. Express the final standardized vector as a row matrix. The standardized components are unitless; do not include units in your final expression.",
            "solution": "The problem statement is rigorously validated and found to be valid. It is scientifically grounded in the fields of quantitative structure-property relationship (QSPR) modeling, clinical pharmacology, and machine learning. The problem is well-posed, objective, self-contained, and describes a standard, realistic task in computational drug discovery. All necessary data are provided, and there are no internal contradictions or scientifically unsound premises.\n\nThe task is to justify and derive a normalization scheme for physicochemical descriptors and apply it to a new data point. The descriptors are the acid dissociation constant ($pK_{a}$), the octanol-water partition coefficient ($\\log_{10} P$), and the polar surface area (PSA). These are used to predict the fraction of a drug unbound in plasma. The necessity for such a scheme arises from the use of a downstream linear model.\n\nA linear model's prediction, $\\hat{y}$, is a weighted sum of the input features $x_i$:\n$$\n\\hat{y} = \\beta_0 + \\sum_{i=1}^{d} \\beta_i x_i\n$$\nwhere $d$ is the number of descriptors, and the coefficients $\\beta_i$ are learned from the training data. The magnitude of a coefficient $\\beta_i$ is contingent on the scale of its corresponding feature $x_i$. If features have disparate scales, the coefficients cannot be directly compared to gauge feature importance. Furthermore, many algorithms used to find the optimal coefficients, such as those based on gradient descent, converge faster when features are on a similar scale.\n\nLet us examine the provided descriptors and their training-set statistics:\n1.  $pK_a$: A dimensionless logarithmic constant with mean $\\mu_{pK_{a}} = 6.80$ and standard deviation $\\sigma_{pK_{a}} = 1.20$.\n2.  $\\log_{10} P$: A dimensionless logarithmic ratio with mean $\\mu_{\\log_{10} P} = 2.50$ and standard deviation $\\sigma_{\\log_{10} P} = 1.10$.\n3.  PSA: An area measured in square angstroms ($\\text{\\AA}^{2}$) with mean $\\mu_{\\mathrm{PSA}} = 85.0 \\, \\text{\\AA}^{2}$ and standard deviation $\\sigma_{\\mathrm{PSA}} = 30.0 \\, \\text{\\AA}^{2}$.\n\nThe numerical range of PSA is an order of magnitude larger than that of $pK_{a}$ and $\\log_{10} P$. For instance, the mean of PSA ($85.0$) is vastly different from the means of the other two descriptors ($6.80$ and $2.50$). A change of $1.0$ unit in $pK_{a}$ represents a significant change in a molecule's ionization state, whereas a change of $1.0 \\, \\text{\\AA}^{2}$ in PSA is a very small change in its surface properties. A linear model would be disproportionately sensitive to variations in PSA simply because of its larger numerical values, leading to an implicit and undesirable weighting.\n\nTo mitigate this, we must transform the features onto a common scale. The standard method, known as standardization or Z-score normalization, is to transform each feature to have a mean of $0$ and a variance of $1$. This transformation is particularly appropriate here as the problem states the descriptor distributions are \"approximately unimodal and symmetric,\" which implies that the mean and standard deviation are meaningful statistics for describing the center and spread of the data. The resulting standardized features are dimensionless, making them directly comparable.\n\nWe shall now derive the required affine transformation from first principles. An affine transformation of a variable $x$ is of the form $z = ax + b$, where $a$ and $b$ are constants. Let $X$ be a random variable representing a descriptor, with mean $E[X] = \\mu$ and standard deviation $\\mathrm{StdDev}[X] = \\sigma$. We seek a transformation that maps $X$ to a new variable $Z = aX + b$ such that $E[Z] = 0$ and $\\mathrm{StdDev}[Z] = 1$.\n\nUsing the properties of expectation, the mean of the transformed variable is:\n$$\nE[Z] = E[aX + b] = aE[X] + b = a\\mu + b\n$$\nFor the new mean to be zero, we must have:\n$$\na\\mu + b = 0 \\implies b = -a\\mu\n$$\nUsing the properties of variance, the variance of the transformed variable is:\n$$\n\\mathrm{Var}(Z) = \\mathrm{Var}(aX + b) = a^2\\mathrm{Var}(X) = a^2\\sigma^2\n$$\nThe standard deviation is the square root of the variance, $\\sigma_Z = \\sqrt{\\mathrm{Var}(Z)}$. Since we require the new standard deviation to be $1$, we have:\n$$\n\\sigma_Z = \\sqrt{a^2\\sigma^2} = |a|\\sigma = 1\n$$\nAssuming the descriptor is not constant ($\\sigma > 0$), we can solve for $a$. By convention, we choose the positive solution, $a = \\frac{1}{\\sigma}$.\n\nSubstituting this expression for $a$ into the equation for $b$:\n$$\nb = -a\\mu = -\\frac{1}{\\sigma}\\mu = -\\frac{\\mu}{\\sigma}\n$$\nThus, the affine transformation that maps a data point $x$ from the original distribution to the standardized value $z$ is:\n$$\nz = ax + b = \\left(\\frac{1}{\\sigma}\\right)x - \\frac{\\mu}{\\sigma} = \\frac{x - \\mu}{\\sigma}\n$$\nThis transformation, when applied to a specific descriptor, uses the mean $\\mu$ and standard deviation $\\sigma$ calculated from the training set.\n\nWe now apply this transformation component-wise to the new compound's descriptor vector $(x_{pK_{a}}, x_{\\log_{10} P}, x_{\\mathrm{PSA}}) = (7.40, 3.20, 78.0)$. The training-set statistics are given as:\n$\\mu_{pK_{a}} = 6.80$, $\\sigma_{pK_{a}} = 1.20$\n$\\mu_{\\log_{10} P} = 2.50$, $\\sigma_{\\log_{10} P} = 1.10$\n$\\mu_{\\mathrm{PSA}} = 85.0$, $\\sigma_{\\mathrm{PSA}} = 30.0$\n\nLet the standardized vector be $(z_{pK_{a}}, z_{\\log_{10} P}, z_{\\mathrm{PSA}})$.\n\n1.  For $pK_a$:\n    $$\n    z_{pK_{a}} = \\frac{x_{pK_{a}} - \\mu_{pK_{a}}}{\\sigma_{pK_{a}}} = \\frac{7.40 - 6.80}{1.20} = \\frac{0.60}{1.20} = 0.5\n    $$\n    Rounding to four significant figures, this is $0.5000$.\n\n2.  For $\\log_{10} P$:\n    $$\n    z_{\\log_{10} P} = \\frac{x_{\\log_{10} P} - \\mu_{\\log_{10} P}}{\\sigma_{\\log_{10} P}} = \\frac{3.20 - 2.50}{1.10} = \\frac{0.70}{1.10} \\approx 0.636363...\n    $$\n    Rounding to four significant figures, this is $0.6364$.\n\n3.  For PSA:\n    $$\n    z_{\\mathrm{PSA}} = \\frac{x_{\\mathrm{PSA}} - \\mu_{\\mathrm{PSA}}}{\\sigma_{\\mathrm{PSA}}} = \\frac{78.0 - 85.0}{30.0} = \\frac{-7.0}{30.0} \\approx -0.233333...\n    $$\n    Rounding to four significant figures, this is $-0.2333$.\n\nThe final standardized descriptor vector for the new compound is $(0.5000, 0.6364, -0.2333)$. This is expressed as a row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5000 & 0.6364 & -0.2333\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Before we can build models to predict a drug's behavior, we must first be able to quantify that behavior from experimental or simulated data. A cornerstone of clinical pharmacology is the characterization of systemic drug exposure, most commonly measured by the Area Under the Curve ($AUC$) of the concentration-time profile. This practice () walks through the fundamental numerical integration technique—the trapezoidal rule—used in noncompartmental analysis (NCA) to compute $AUC$, a critical endpoint for subsequent exposure-response modeling.",
            "id": "4563937",
            "problem": "An Artificial Intelligence (AI) driven in silico clinical trial simulator has produced predicted plasma concentration-time data for a single oral dose of a new small-molecule candidate. In Noncompartmental Analysis (NCA), the Area Under the Curve (AUC) from time zero to the last observation, and the extrapolated AUC to infinity based on the terminal elimination slope, are core exposure metrics used in downstream machine learning (ML) exposure-response modeling in clinical pharmacology.\n\nYou are given the following time-concentration pairs for one subject, where time is in hours (h) and concentration is in milligrams per liter (mg/L):\n- $t = 0$, $C = 0$\n- $t = 0.5$, $C = 6.2$\n- $t = 1$, $C = 9.8$\n- $t = 2$, $C = 12.0$\n- $t = 4$, $C = 5.488116361$\n- $t = 6$, $C = 4.065696597$\n- $t = 8$, $C = 3.011942119$\n- $t = 12$, $C = 1.653297706$\n\nStarting from the fundamental definition of the AUC as the integral $\\int C(t)\\,dt$, compute the noncompartmental $AUC_{0-t_{\\text{last}}}$ using the trapezoidal rule between successive observations. Then, under the assumption that the terminal elimination is first-order (so the natural logarithm of concentration is linear in time in the terminal phase), estimate the terminal elimination rate constant $\\lambda_{z}$ by an ordinary least squares (OLS) linear regression of $\\ln(C)$ on $t$ using the last four points ($t = 4, 6, 8, 12$). Use this $\\lambda_{z}$ to extrapolate the residual area from $t_{\\text{last}}$ to infinity and thereby derive $AUC_{0-\\infty}$.\n\nRound your final numeric result to four significant figures. Express the final AUC in mg·h/L. Use the natural logarithm $\\ln(\\cdot)$ wherever needed.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in standard pharmacokinetic principles, well-posed with all necessary data and methods provided, and objective in its language and requirements. The task is to calculate the total Area Under the Curve from time zero to infinity, $AUC_{0-\\infty}$. This quantity is the sum of two components: the AUC from time zero to the last observation time, $AUC_{0-t_{\\text{last}}}$, and the extrapolated AUC from the last observation time to infinity, $AUC_{t_{\\text{last}}-\\infty}$.\n\nFirst, we compute $AUC_{0-t_{\\text{last}}}$ using the linear trapezoidal rule. The formula for the area of a single trapezoid between two time points, $t_i$ and $t_{i+1}$, with corresponding concentrations $C_i$ and $C_{i+1}$, is:\n$$AUC_{i \\to i+1} = \\frac{C_i + C_{i+1}}{2} (t_{i+1} - t_i)$$\nThe total $AUC_{0-t_{\\text{last}}}$ is the sum of the areas of the trapezoids formed by successive data points. The given data points are $(t, C)$: $(0, 0)$, $(0.5, 6.2)$, $(1, 9.8)$, $(2, 12.0)$, $(4, 5.488116361)$, $(6, 4.065696597)$, $(8, 3.011942119)$, and $(12, 1.653297706)$. The last observation time is $t_{\\text{last}} = 12$ h.\n\nThe individual trapezoidal areas are calculated as follows:\n- For the interval $[0, 0.5]$:\n$AUC_{0-0.5} = \\frac{0 + 6.2}{2} (0.5 - 0) = 3.1 \\times 0.5 = 1.55$\n- For the interval $[0.5, 1]$:\n$AUC_{0.5-1} = \\frac{6.2 + 9.8}{2} (1 - 0.5) = 8.0 \\times 0.5 = 4.0$\n- For the interval $[1, 2]$:\n$AUC_{1-2} = \\frac{9.8 + 12.0}{2} (2 - 1) = 10.9 \\times 1 = 10.9$\n- For the interval $[2, 4]$:\n$AUC_{2-4} = \\frac{12.0 + 5.488116361}{2} (4 - 2) = 8.7440581805 \\times 2 = 17.488116361$\n- For the interval $[4, 6]$:\n$AUC_{4-6} = \\frac{5.488116361 + 4.065696597}{2} (6 - 4) = 4.776906479 \\times 2 = 9.553812958$\n- For the interval $[6, 8]$:\n$AUC_{6-8} = \\frac{4.065696597 + 3.011942119}{2} (8 - 6) = 3.538819358 \\times 2 = 7.077638716$\n- For the interval $[8, 12]$:\n$AUC_{8-12} = \\frac{3.011942119 + 1.653297706}{2} (12 - 8) = 2.3326199125 \\times 4 = 9.33047965$\n\nSumming these individual areas gives $AUC_{0-t_{\\text{last}}}$:\n$$AUC_{0-t_{\\text{last}}} = 1.55 + 4.0 + 10.9 + 17.488116361 + 9.553812958 + 7.077638716 + 9.33047965$$\n$$AUC_{0-t_{\\text{last}}} = 59.900047685 \\, \\text{mg} \\cdot \\text{h/L}$$\n\nNext, we estimate the terminal elimination rate constant, $\\lambda_z$. We are instructed to use ordinary least squares (OLS) linear regression of the natural logarithm of concentration, $\\ln(C)$, on time, $t$, for the last four data points. The model is $\\ln(C) = \\ln(C_{\\text{intercept}}) - \\lambda_z t$. The slope of this line is $m = -\\lambda_z$.\n\nThe data points for the regression are:\n- $t_4 = 4$, $C_4 = 5.488116361 \\implies \\ln(C_4) \\approx 1.70265$\n- $t_5 = 6$, $C_5 = 4.065696597 \\implies \\ln(C_5) \\approx 1.40265$\n- $t_6 = 8$, $C_6 = 3.011942119 \\implies \\ln(C_6) \\approx 1.10265$\n- $t_7 = 12$, $C_7 = 1.653297706 \\implies \\ln(C_7) \\approx 0.50265$\nLet the regression points be $(x_j, y_j)$, where $x_j=t_j$ and $y_j=\\ln(C_j)$. The points are $(4, 1.70265)$, $(6, 1.40265)$, $(8, 1.10265)$, and $(12, 0.50265)$. A quick check reveals that these points are perfectly collinear. The slope between any two points is constant:\n$$m = \\frac{1.40265 - 1.70265}{6 - 4} = \\frac{-0.3}{2} = -0.15$$\n$$m = \\frac{0.50265 - 1.10265}{12 - 8} = \\frac{-0.6}{4} = -0.15$$\nSince the points lie on a perfect line, the OLS regression will yield this exact slope. Thus, the slope $m = -0.15$. The terminal elimination rate constant is $\\lambda_z = -m = 0.15$ h$^{-1}$.\n\nNow, we calculate the extrapolated area from $t_{\\text{last}}$ to infinity, $AUC_{t_{\\text{last}}-\\infty}$. The standard formula for this extrapolation is:\n$$AUC_{t_{\\text{last}}-\\infty} = \\frac{C_{\\text{last}}}{\\lambda_z}$$\nwhere $C_{\\text{last}}$ is the last observed concentration. Here, $t_{\\text{last}} = 12$ and $C_{\\text{last}} = C(12) = 1.653297706$ mg/L.\n$$AUC_{12-\\infty} = \\frac{1.653297706}{0.15} \\approx 11.02198471 \\, \\text{mg} \\cdot \\text{h/L}$$\nSince the terminal data points are perfectly collinear, the last observed concentration $C_{\\text{last}}$ is identical to the concentration at $t_{\\text{last}}$ predicted by the regression line, which avoids any ambiguity in the formula.\n\nFinally, we compute the total area under the curve, $AUC_{0-\\infty}$, by summing the two components:\n$$AUC_{0-\\infty} = AUC_{0-t_{\\text{last}}} + AUC_{t_{\\text{last}}-\\infty}$$\n$$AUC_{0-\\infty} = 59.900047685 + 11.02198471 = 70.922032395 \\, \\text{mg} \\cdot \\text{h/L}$$\n\nThe problem requires rounding the final result to four significant figures. The value is $70.922032395$. The fifth significant digit is $2$, so we round down.\nThe final result is $70.92$.",
            "answer": "$$\n\\boxed{70.92}\n$$"
        },
        {
            "introduction": "When an AI model predicts the probability of an event, such as drug-induced toxicity, we need to trust that a predicted 0.8 risk truly corresponds to a high event rate. This property, known as calibration, is essential for making reliable, risk-based decisions in drug development. This exercise () introduces the Expected Calibration Error (ECE), a key metric for assessing how well a model's predicted probabilities align with real-world outcomes.",
            "id": "4563969",
            "problem": "A clinical pharmacology group is using Machine Learning (ML) within Artificial Intelligence (AI) to support early drug development decisions by predicting the probability that a candidate compound will cause drug-induced liver injury. A probabilistic classifier outputs for each compound $i$ a predicted probability $p_i \\in [0,1]$ for the event $Y_i=1$ indicating clinically significant hepatotoxicity. In a held-out evaluation cohort of size $N$, the team assesses calibration of the predicted probabilities via a reliability diagram that partitions the probability range into $M$ disjoint bins $\\{B_m\\}_{m=1}^{M}$.\n\nAs a foundational principle, perfect calibration is defined by the condition that for any probability level $p \\in [0,1]$, the true conditional event probability equals the predicted probability, that is $\\mathbb{P}(Y=1 \\mid p_i = p) = p$. In practice, one approximates this condition by binning predictions and comparing, within each bin $B_m$, the empirical event rate to the mean predicted probability in that bin. Let $n_m$ denote the number of predictions falling into bin $B_m$, $\\operatorname{acc}(m)$ denote the empirical event rate in $B_m$ (that is, $\\operatorname{acc}(m) = \\frac{1}{n_m}\\sum_{i \\in B_m} y_i$), and $\\operatorname{conf}(m)$ denote the mean predicted probability in $B_m$ (that is, $\\operatorname{conf}(m) = \\frac{1}{n_m}\\sum_{i \\in B_m} p_i$).\n\nStarting from the above calibration principle and the concept of measuring the expected absolute deviation between the empirical event rate and the mean predicted probability, aggregated across bins and weighted by the empirical bin frequencies, do the following:\n- Derive a closed-form expression for the Expected Calibration Error (ECE) as a function of $M$, $\\{n_m\\}_{m=1}^{M}$, $\\{\\operatorname{acc}(m)\\}_{m=1}^{M}$, $\\{\\operatorname{conf}(m)\\}_{m=1}^{M}$, and $N$.\n- Using your expression, compute the numerical value of ECE for the following calibration histogram obtained from $N = 1000$ predictions with $M = 8$ bins:\n\nBin summaries (for $m = 1,\\dots,8$):\n- Bin $1$: $n_1 = 90$, $\\operatorname{conf}(1) = 0.07$, $\\operatorname{acc}(1) = 0.05$.\n- Bin $2$: $n_2 = 110$, $\\operatorname{conf}(2) = 0.18$, $\\operatorname{acc}(2) = 0.14$.\n- Bin $3$: $n_3 = 130$, $\\operatorname{conf}(3) = 0.31$, $\\operatorname{acc}(3) = 0.28$.\n- Bin $4$: $n_4 = 170$, $\\operatorname{conf}(4) = 0.43$, $\\operatorname{acc}(4) = 0.47$.\n- Bin $5$: $n_5 = 180$, $\\operatorname{conf}(5) = 0.57$, $\\operatorname{acc}(5) = 0.52$.\n- Bin $6$: $n_6 = 150$, $\\operatorname{conf}(6) = 0.69$, $\\operatorname{acc}(6) = 0.62$.\n- Bin $7$: $n_7 = 110$, $\\operatorname{conf}(7) = 0.80$, $\\operatorname{acc}(7) = 0.74$.\n- Bin $8$: $n_8 = 60$, $\\operatorname{conf}(8) = 0.92$, $\\operatorname{acc}(8) = 0.86$.\n\nExpress the final ECE as a dimensionless number rounded to three significant figures.",
            "solution": "The problem as stated is scientifically sound, self-contained, and well-posed. The data provided is internally consistent, as the sum of the number of predictions in each bin, $\\sum_{m=1}^{M} n_m$, equals the total cohort size $N$. Specifically, $\\sum_{m=1}^{8} n_m = 90 + 110 + 130 + 170 + 180 + 150 + 110 + 60 = 1000 = N$. All provided values are within their valid ranges. Therefore, a solution can be derived.\n\nThe first part of the task is to derive a closed-form expression for the Expected Calibration Error (ECE) from its definition. The problem describes ECE as \"the expected absolute deviation between the empirical event rate and the mean predicted probability, aggregated across bins and weighted by the empirical bin frequencies.\" We will formalize this description step-by-step.\n\nLet there be $M$ disjoint bins, denoted by $\\{B_m\\}_{m=1}^{M}$.\nFor each bin $B_m$, we are given:\n- The number of predictions in the bin: $n_m$.\n- The empirical event rate, or accuracy: $\\operatorname{acc}(m) = \\frac{1}{n_m}\\sum_{i \\in B_m} y_i$, where $y_i \\in \\{0, 1\\}$ is the true outcome for prediction $i$.\n- The mean predicted probability, or confidence: $\\operatorname{conf}(m) = \\frac{1}{n_m}\\sum_{i \\in B_m} p_i$, where $p_i \\in [0,1]$ is the predicted probability for prediction $i$.\n\nThe total number of predictions is $N = \\sum_{m=1}^{M} n_m$.\n\nLet's translate the definition of ECE:\n1.  The \"absolute deviation between the empirical event rate and the mean predicted probability\" for a single bin $B_m$ is given by the absolute difference $|\\operatorname{acc}(m) - \\operatorname{conf}(m)|$.\n2.  The quantity must be \"aggregated across bins\" and \"weighted by the empirical bin frequencies\". The empirical frequency of bin $B_m$ is the fraction of total predictions that fall into this bin, which is $\\frac{n_m}{N}$.\n3.  The aggregation is a weighted sum over all bins. The term \"Expected\" in ECE, in this binned estimator context, refers to this weighted average over the finite sample.\n\nCombining these components yields the expression for ECE:\n$$\n\\text{ECE} = \\sum_{m=1}^{M} \\frac{n_m}{N} |\\operatorname{acc}(m) - \\operatorname{conf}(m)|\n$$\nThis is the required closed-form expression for the Expected Calibration Error.\n\nThe second part of the task is to compute the numerical value of ECE using the provided data for $N = 1000$ predictions and $M = 8$ bins. We will calculate the contribution of each bin to the total ECE and then sum them. The contribution for bin $m$ is $\\frac{n_m}{N} |\\operatorname{acc}(m) - \\operatorname{conf}(m)|$.\n\n- For bin $m=1$: $\\frac{90}{1000} |0.05 - 0.07| = 0.09 \\times |-0.02| = 0.09 \\times 0.02 = 0.0018$.\n- For bin $m=2$: $\\frac{110}{1000} |0.14 - 0.18| = 0.11 \\times |-0.04| = 0.11 \\times 0.04 = 0.0044$.\n- For bin $m=3$: $\\frac{130}{1000} |0.28 - 0.31| = 0.13 \\times |-0.03| = 0.13 \\times 0.03 = 0.0039$.\n- For bin $m=4$: $\\frac{170}{1000} |0.47 - 0.43| = 0.17 \\times |0.04| = 0.17 \\times 0.04 = 0.0068$.\n- For bin $m=5$: $\\frac{180}{1000} |0.52 - 0.57| = 0.18 \\times |-0.05| = 0.18 \\times 0.05 = 0.0090$.\n- For bin $m=6$: $\\frac{150}{1000} |0.62 - 0.69| = 0.15 \\times |-0.07| = 0.15 \\times 0.07 = 0.0105$.\n- For bin $m=7$: $\\frac{110}{1000} |0.74 - 0.80| = 0.11 \\times |-0.06| = 0.11 \\times 0.06 = 0.0066$.\n- For bin $m=8$: $\\frac{60}{1000} |0.86 - 0.92| = 0.06 \\times |-0.06| = 0.06 \\times 0.06 = 0.0036$.\n\nNow, we sum these individual contributions to find the total ECE:\n$$\n\\text{ECE} = 0.0018 + 0.0044 + 0.0039 + 0.0068 + 0.0090 + 0.0105 + 0.0066 + 0.0036\n$$\n$$\n\\text{ECE} = 0.0466\n$$\nThe problem requires the final answer to be rounded to three significant figures. The calculated value is $0.0466$. The first significant figure is $4$, the second is $6$, and the third is $6$. The value is presented with three significant figures.",
            "answer": "$$\\boxed{0.0466}$$"
        }
    ]
}