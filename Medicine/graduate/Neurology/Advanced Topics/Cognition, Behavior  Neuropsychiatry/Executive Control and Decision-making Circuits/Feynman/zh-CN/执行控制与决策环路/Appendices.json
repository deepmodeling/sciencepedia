{
    "hands_on_practices": [
        {
            "introduction": "执行控制的一个基本功能是根据行动的后果来调整未来的行为。强化学习理论为这一过程提供了一个强大的计算框架，尤其是在模拟基底节功能方面。这个练习将带你亲手实践Q学习算法的核心机制，通过计算一个奖励预测误差（RPE）如何更新一个特定状态-动作对的价值，从而具体地理解多巴胺信号在学习过程中的作用。",
            "id": "4479810",
            "problem": "一个关于执行控制的皮质-基底神经节-丘脑-皮质环路模型假定，背侧纹状体中的皮质纹状体突触编码状态-动作价值 $Q_{t}(s,a)$，这些价值通过基底神经节输出核指导动作选择。来自腹侧被盖区 (Ventral Tegmental Area, VTA) 和黑质致密部 (Substantia Nigra pars compacta, SNc) 的相位性多巴胺释放传达了奖励预测误差 (RPE)，该误差调节突触可塑性并更新这些价值。在标准的强化学习解释下，时间 $t$ 的RPE比较了实际经历的回报和预期的回报，纹状体价值的更新与此误差成正比，比例常数等于学习率 $\\alpha$。预期的回报包括即时奖励 $r_{t}$ 和使用折扣因子 $\\gamma$ 及下一状态 $s'$ 中所有可用动作的最大估计价值对未来回报的折扣估计。\n\n考虑单个试验，其中前额叶皮质状态 $s$ 导致选择动作 $a$，产生即时奖励 $r_{t}$ 并转换到状态 $s'$。当前对已执行的状态-动作对的纹状体估计值为 $Q_{t}(s,a)$。下一状态的最大价值为 $\\max_{a'} Q_{t}(s',a')$。假设参数 $\\alpha=0.1$，$\\gamma=0.9$，$Q_{t}(s,a)=0.5$，$r_{t}=1$，以及 $\\max_{a'} Q_{t}(s',a')=0.8$。使用奖励预测误差的定义以及对 $Q_{t}(s,a)$ 的更新与此误差成正比的原理，计算更新后的价值 $Q_{t+1}(s,a)$。将您的最终答案表示为单个实数，不带任何额外的单位或符号。无需四舍五入。",
            "solution": "该问题描述了状态-动作价值函数 $Q(s,a)$ 的更新机制，这是强化学习中的一个核心概念，尤其是在Q学习算法中。该更新规则形式化了智能体如何通过根据新信息调整其价值估计来从经验中学习。\n\n问题指出，对价值 $Q_{t}(s,a)$ 的更新与奖励预测误差（RPE）成正比，比例常数为学习率 $\\alpha$。RPE，我们可以表示为 $\\delta_{t}$，是实际经历的回报与预期回报之间的差值。\n\n预期回报就是当前对在状态 $s$ 下采取动作 $a$ 的价值的估计，即 $Q_{t}(s,a)$。\n\n实际经历的回报，通常称为TD（时间差分）目标，是收到的即时奖励 $r_{t}$ 和下一状态 $s'$ 中最佳可能动作的折扣价值之和。这由表达式 $r_{t} + \\gamma \\max_{a'} Q_{t}(s',a')$ 给出。折扣因子 $\\gamma$ 决定了未来奖励的现值。\n\n因此，RPE在数学上定义为：\n$$\n\\delta_{t} = \\left( r_{t} + \\gamma \\max_{a'} Q_{t}(s',a') \\right) - Q_{t}(s,a)\n$$\nQ值的更新量 $\\Delta Q_{t}(s,a)$ 是这个误差乘以学习率 $\\alpha$：\n$$\n\\Delta Q_{t}(s,a) = \\alpha \\cdot \\delta_{t}\n$$\n在时间 $t+1$ 的新Q值 $Q_{t+1}(s,a)$ 是旧值加上这个更新量：\n$$\nQ_{t+1}(s,a) = Q_{t}(s,a) + \\Delta Q_{t}(s,a)\n$$\n将更新量和误差的表达式代入，我们得到完整的Q学习更新规则：\n$$\nQ_{t+1}(s,a) = Q_{t}(s,a) + \\alpha \\left[ \\left( r_{t} + \\gamma \\max_{a'} Q_{t}(s',a') \\right) - Q_{t}(s,a) \\right]\n$$\n我们已知以下数值：\n-   学习率 $\\alpha = 0.1$\n-   折扣因子 $\\gamma = 0.9$\n-   当前Q值 $Q_{t}(s,a) = 0.5$\n-   即时奖励 $r_{t} = 1$\n-   下一状态最大Q值 $\\max_{a'} Q_{t}(s',a') = 0.8$\n\n首先，我们计算TD目标，即括号内的项，代表对 $(s,a)$ 价值的新估计：\n$$\n\\text{TD Target} = r_{t} + \\gamma \\max_{a'} Q_{t}(s',a') = 1 + (0.9)(0.8) = 1 + 0.72 = 1.72\n$$\n接下来，我们计算RPE，$\\delta_{t}$：\n$$\n\\delta_{t} = \\text{TD Target} - Q_{t}(s,a) = 1.72 - 0.5 = 1.22\n$$\n最后，我们通过将RPE的一部分（由 $\\alpha$ 决定）加到旧值上来计算更新后的价值 $Q_{t+1}(s,a)$：\n$$\nQ_{t+1}(s,a) = Q_{t}(s,a) + \\alpha \\cdot \\delta_{t} = 0.5 + 0.1 \\cdot (1.22)\n$$\n$$\nQ_{t+1}(s,a) = 0.5 + 0.122\n$$\n$$\nQ_{t+1}(s,a) = 0.622\n$$\n因此，状态-动作对 $(s,a)$ 的更新后的纹状体估计值为 $0.622$。",
            "answer": "$$\n\\boxed{0.622}\n$$"
        },
        {
            "introduction": "一旦我们的大脑为不同的潜在行动赋予了价值，它必须根据这些价值来选择一个行动。这个选择过程并非总是确定性地选择最高价值的选项，而常常是在“利用”已知最佳选择和“探索”其他可能性之间取得平衡。这个练习将引导你从信息论的第一性原理出发，推导出在决策中广泛应用的Softmax策略，并计算在特定价值和神经增益（即精确度$\\beta$）下的选择概率。",
            "id": "4479852",
            "problem": "考虑人脑中的一个皮质-纹状体决策回路，该回路基于背外侧前额叶皮层 (dlPFC) 中神经元集群编码的价值，并通过基底节 (BG) 进行传递，在前扣带皮层 (ACC) 的冲突监控下，在两个动作 $a_{1}$ 和 $a_{2}$ 之间进行仲裁。神经病学和强化学习 (RL) 模型的实证研究支持这样的观点：此类回路实现受神经变异性约束的随机选择策略。假设该回路选择一个关于动作的概率分布 $P(a_{i})$，以最大化一个由期望值和熵正则化项组成的、由精确度控制的目标函数。具体而言，该策略求解以下约束优化问题：\n$$\n\\max_{\\{P(a_{i})\\}}\\ \\sum_{i} P(a_{i})\\, v_{i} + \\frac{1}{\\beta}\\, H(P)\\quad\\text{subject to}\\quad \\sum_{i} P(a_{i}) = 1,\\ \\ P(a_{i}) \\ge 0,\n$$\n其中 $v_{i}$ 表示回路为动作 $a_{i}$ 编码的归一化价值，$\\beta>0$ 是一个反映神经调质增益的逆噪声（精确度）参数，而 $H(P) = -\\sum_{i} P(a_{i}) \\ln P(a_{i})$ 是香农熵。\n\n任务：\n1. 使用上述目标和约束，从第一性原理出发，推导出最优选择策略 $P(a_{i})$ 作为 $v_{i}$ 和 $\\beta$ 的函数。\n2. 将问题具体化到两个动作，其归一化价值为 $v = [1, 2]$（即 $v_{1} = 1$ 和 $v_{2} = 2$），且 $\\beta = 1$，计算 $a_{1}$ 和 $a_{2}$ 的精确选择概率。\n3. 定义相对价值差异 $\\Delta v = v_{2} - v_{1}$，并且对于两个动作的情况，将 $P(a_{2})$ 表示为 $\\Delta v$ 和 $\\beta$ 的函数。计算在 $\\beta = 1$ 和 $\\Delta v = 1$ 时的精确局部敏感度 $\\frac{dP(a_{2})}{d\\Delta v}$。简要解释在固定的神经噪声下，该敏感度对于执行控制回路如何响应相对价值变化以调整选择概率意味着什么。\n\n以精确形式表示所有最终数值结果（不要四舍五入或近似）。将您的最终答案表示为一个单行矩阵，按顺序包含 $P(a_{1})$、$P(a_{2})$ 和 $\\left.\\frac{dP(a_{2})}{d\\Delta v}\\right|_{\\beta=1,\\ \\Delta v=1}$。不需要物理单位。",
            "solution": "该问题被认为是有效的，因为它在计算神经科学方面有科学依据，数学上是适定的、客观的，并包含了唯一解所需的所有信息。\n\n任务是分析一个通过最大化熵正则化目标函数推导出的随机选择策略。目标是找到最大化以下函数的概率分布 $\\{P(a_i)\\}$：\n$$\n\\mathcal{F}(P) = \\sum_{i} P(a_{i})\\, v_{i} + \\frac{1}{\\beta}\\, H(P) = \\sum_{i} P(a_{i})\\, v_{i} - \\frac{1}{\\beta}\\, \\sum_{i} P(a_{i}) \\ln P(a_{i})\n$$\n受限于 $\\{P(a_i)\\}$ 构成一个有效概率分布的约束：\n$$\n\\sum_{i} P(a_{i}) = 1 \\quad \\text{and} \\quad P(a_{i}) \\ge 0 \\quad \\text{for all } i\n$$\n这是一个约束优化问题，可以使用拉格朗日乘数法解决。\n\n### 1. 最优选择策略的推导\n\n我们引入一个拉格朗日乘子 $\\lambda$ 来整合归一化约束 $\\sum_{i} P(a_{i}) = 1$。拉格朗日函数 $\\mathcal{L}$ 是：\n$$\n\\mathcal{L}(\\{P(a_i)\\}, \\lambda) = \\sum_{i} P(a_{i})\\, v_{i} - \\frac{1}{\\beta}\\, \\sum_{i} P(a_{i}) \\ln P(a_{i}) + \\lambda \\left(1 - \\sum_{i} P(a_{i})\\right)\n$$\n为了找到最大值，我们将 $\\mathcal{L}$ 对每个 $P(a_k)$ 求偏导数，并令其为零。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial P(a_k)} = \\frac{\\partial}{\\partial P(a_k)} \\left( P(a_k) v_k - \\frac{1}{\\beta} P(a_k) \\ln P(a_k) - \\lambda P(a_k) \\right) = 0\n$$\n$$\nv_k - \\frac{1}{\\beta} \\left(1 \\cdot \\ln P(a_k) + P(a_k) \\cdot \\frac{1}{P(a_k)}\\right) - \\lambda = 0\n$$\n$$\nv_k - \\frac{1}{\\beta} (\\ln P(a_k) + 1) - \\lambda = 0\n$$\n现在，我们求解 $P(a_k)$：\n$$\n\\ln P(a_k) + 1 = \\beta(v_k - \\lambda)\n$$\n$$\n\\ln P(a_k) = \\beta v_k - \\beta \\lambda - 1\n$$\n$$\nP(a_k) = \\exp(\\beta v_k - \\beta \\lambda - 1) = \\exp(\\beta v_k) \\exp(-\\beta \\lambda - 1)\n$$\n对于所有的 $k$，项 $\\exp(-\\beta \\lambda - 1)$ 是一个常数。我们定义一个归一化常数 $Z = \\exp(\\beta \\lambda + 1)$。那么 $P(a_k) = \\frac{\\exp(\\beta v_k)}{Z}$。\n为了求出 $Z$，我们应用约束 $\\sum_{i} P(a_{i}) = 1$：\n$$\n\\sum_{k} P(a_k) = \\sum_{k} \\frac{\\exp(\\beta v_k)}{Z} = 1\n$$\n$$\n\\frac{1}{Z} \\sum_{k} \\exp(\\beta v_k) = 1 \\implies Z = \\sum_{k} \\exp(\\beta v_k)\n$$\n将 $Z$ 代回 $P(a_k)$ 的表达式中，得到最优策略，即吉布斯-玻尔兹曼分布或 softmax 分布：\n$$\nP(a_k) = \\frac{\\exp(\\beta v_k)}{\\sum_{j} \\exp(\\beta v_j)}\n$$\n该策略为具有较高关联价值 $v_k$ 的动作分配更高的概率。参数 $\\beta$ 控制选择的精确度或确定性：当 $\\beta \\to \\infty$ 时，策略趋向于确定性地选择最佳动作；当 $\\beta \\to 0$ 时，策略趋向于在所有动作上进行均匀随机选择。\n\n### 2. 选择概率的计算\n\n给定两个动作 $a_1$ 和 $a_2$，其价值为 $v_1 = 1, v_2 = 2$，精确度参数为 $\\beta = 1$。使用推导出的策略：\n分母（配分函数）为 $Z = \\sum_{j=1}^{2} \\exp(\\beta v_j) = \\exp(1 \\cdot 1) + \\exp(1 \\cdot 2) = \\exp(1) + \\exp(2)$。\n动作 $a_1$ 的概率是：\n$$\nP(a_1) = \\frac{\\exp(\\beta v_1)}{\\exp(\\beta v_1) + \\exp(\\beta v_2)} = \\frac{\\exp(1)}{\\exp(1) + \\exp(2)}\n$$\n为了简化，我们将分子和分母同时除以 $\\exp(1)$：\n$$\nP(a_1) = \\frac{\\exp(1)/\\exp(1)}{(\\exp(1) + \\exp(2))/\\exp(1)} = \\frac{1}{1 + \\exp(1)} = \\frac{1}{1+e}\n$$\n类似地，动作 $a_2$ 的概率是：\n$$\nP(a_2) = \\frac{\\exp(\\beta v_2)}{\\exp(\\beta v_1) + \\exp(\\beta v_2)} = \\frac{\\exp(2)}{\\exp(1) + \\exp(2)}\n$$\n将分子和分母同时除以 $\\exp(1)$：\n$$\nP(a_2) = \\frac{\\exp(2)/\\exp(1)}{(\\exp(1) + \\exp(2))/\\exp(1)} = \\frac{\\exp(1)}{1 + \\exp(1)} = \\frac{e}{1+e}\n$$\n作为检验，$P(a_1) + P(a_2) = \\frac{1}{1+e} + \\frac{e}{1+e} = \\frac{1+e}{1+e} = 1$。\n\n### 3. 敏感度分析\n\n对于两个动作，我们定义相对价值差异 $\\Delta v = v_2 - v_1$。我们将 $P(a_2)$ 表示为 $\\Delta v$ 和 $\\beta$ 的函数。\n$$\nP(a_2) = \\frac{\\exp(\\beta v_2)}{\\exp(\\beta v_1) + \\exp(\\beta v_2)}\n$$\n将分子和分母同时除以 $\\exp(\\beta v_1)$：\n$$\nP(a_2) = \\frac{\\exp(\\beta v_2 - \\beta v_1)}{1 + \\exp(\\beta v_2 - \\beta v_1)} = \\frac{\\exp(\\beta (v_2 - v_1))}{1 + \\exp(\\beta (v_2 - v_1))}\n$$\n代入 $\\Delta v$：\n$$\nP(a_2) = \\frac{\\exp(\\beta \\Delta v)}{1 + \\exp(\\beta \\Delta v)}\n$$\n这是逻辑S型函数（logistic sigmoid function）。为了找到局部敏感度，我们计算 $P(a_2)$ 对 $\\Delta v$ 的导数。使用商法则求导，其中 $u(\\Delta v) = \\exp(\\beta \\Delta v)$ 且 $w(\\Delta v) = 1 + \\exp(\\beta \\Delta v)$：\n$$\n\\frac{dP(a_2)}{d\\Delta v} = \\frac{\\frac{du}{d\\Delta v} w - u \\frac{dw}{d\\Delta v}}{w^2} = \\frac{\\beta \\exp(\\beta \\Delta v)(1 + \\exp(\\beta \\Delta v)) - \\exp(\\beta \\Delta v)(\\beta \\exp(\\beta \\Delta v))}{(1 + \\exp(\\beta \\Delta v))^2}\n$$\n$$\n= \\frac{\\beta \\exp(\\beta \\Delta v) + \\beta \\exp(2\\beta \\Delta v) - \\beta \\exp(2\\beta \\Delta v)}{(1 + \\exp(\\beta \\Delta v))^2} = \\frac{\\beta \\exp(\\beta \\Delta v)}{(1 + \\exp(\\beta \\Delta v))^2}\n$$\n我们在 $\\beta = 1$ 和 $\\Delta v = 1$ 处评估该敏感度（注意，对于 $v_1=1, v_2=2$，我们有 $\\Delta v = 2-1 = 1$）。\n$$\n\\left.\\frac{dP(a_2)}{d\\Delta v}\\right|_{\\beta=1, \\Delta v=1} = \\frac{1 \\cdot \\exp(1 \\cdot 1)}{(1 + \\exp(1 \\cdot 1))^2} = \\frac{\\exp(1)}{(1 + \\exp(1))^2} = \\frac{e}{(1+e)^2}\n$$\n这个敏感度值表示，随着更有价值的动作（$a_2$）相对于备选动作（$a_1$）的价值优势增大，选择 $a_2$ 的概率增加的速率。对于固定的神经精确度水平 $\\beta$，该项量化了系统响应支持某个选项的证据变化以调整其选择概率的能力。更高的敏感度意味着更灵活和高效的决策调整。敏感度为正证实了随着 $v_2$ 相对于 $v_1$ 的价值增加，回路更可能选择 $a_2$，这是自适应决策的一个基本属性。\n\n所要求的三个数值结果是 $P(a_1) = \\frac{1}{1+e}$，$P(a_2) = \\frac{e}{1+e}$，以及 $\\left.\\frac{dP(a_2)}{d\\Delta v}\\right|_{\\beta=1,\\ \\Delta v=1} = \\frac{e}{(1+e)^2}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{1+e}  \\frac{e}{1+e}  \\frac{e}{(1+e)^2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "高级决策通常不是由单一系统完成的，而是多个神经回路协同作用或相互竞争的结果，例如前额叶皮层中深思熟虑的“基于模型”的系统与纹状体中习惯性的“无模型”的系统。执行控制的一个关键作用是裁决这些不同控制信号之间的冲突。这个练习将探讨大脑如何根据每个系统的历史表现或不确定性，来优化地权衡和整合来自不同决策顾问的建议，从而实现更精确的适应性行为。",
            "id": "4479773",
            "problem": "考虑一个皮质-基底节决策架构，其中两个子系统贡献的动作价值被线性组合以形成单一的控制信号。具体来说，一个与前额叶皮层（PFC）相关的规划子系统实施基于模型（Model-Based, MB）的策略，而一个与背外侧纹状体（DLS）相关的习惯子系统实施无模型（Model-Free, MF）的策略。设这两个子系统分别对一个潜在的动作价值 $x$ 产生无偏估计，记为 $\\hat{x}_{\\mathrm{MB}}$ 和 $\\hat{x}_{\\mathrm{MF}}$。假设在给定 $x$ 的条件下，每个估计都受到独立的、零均值的高斯噪声的干扰，其方差分别为 $\\sigma_{\\mathrm{MB}}^{2}$ 和 $\\sigma_{\\mathrm{MF}}^{2}$。\n\n仲裁机制构建了一个混合策略价值\n$$\nQ_{\\mathrm{mix}} \\;=\\; w\\,Q_{\\mathrm{MB}} \\;+\\; \\left(1-w\\right)\\,Q_{\\mathrm{MF}},\n$$\n其中 $Q_{\\mathrm{MB}}$ 和 $Q_{\\mathrm{MF}}$ 与 $\\hat{x}_{\\mathrm{MB}}$ 和 $\\hat{x}_{\\mathrm{MF}}$ 成正比，并且在上述假设下，选择 $w \\in [0,1]$ 以在保持无偏性的同时最小化混合的方差。\n\n从所述的概率模型以及混合是无偏且方差最小的要求出发，推导出 $w$ 作为 $\\sigma_{\\mathrm{MB}}^{2}$ 和 $\\sigma_{\\mathrm{MF}}^{2}$ 函数的闭式表达式。\n\n然后，取 $\\sigma_{\\mathrm{MB}}^{2} = 0.25$ 和 $\\sigma_{\\mathrm{MF}}^{2} = 1$，计算 $w$ 的数值。简要解释最终的混合策略，说明MB和MF通路在动作选择中的相对影响。\n\n将最终的仲裁权重表示为精确分数或小数；无需四舍五入。最终数字不应包含单位。",
            "solution": "### 步骤 1：提取已知条件\n- 两个子系统提供对一个潜在动作价值 $x$ 的估计：一个基于模型（MB）的子系统，其估计为 $\\hat{x}_{\\mathrm{MB}}$，以及一个无模型（MF）的子系统，其估计为 $\\hat{x}_{\\mathrm{MF}}$。\n- 在给定 $x$ 的条件下，估计是无偏的：$E[\\hat{x}_{\\mathrm{MB}} | x] = x$ 且 $E[\\hat{x}_{\\mathrm{MF}} | x] = x$。\n- 估计受到独立的、零均值的高斯噪声的干扰。\n- 估计量的方差为 $\\mathrm{Var}(\\hat{x}_{\\mathrm{MB}}) = \\sigma_{\\mathrm{MB}}^{2}$ 和 $\\mathrm{Var}(\\hat{x}_{\\mathrm{MF}}) = \\sigma_{\\mathrm{MF}}^{2}$。\n- 估计量是独立的，这意味着它们的协方差为零：$\\mathrm{Cov}(\\hat{x}_{\\mathrm{MB}}, \\hat{x}_{\\mathrm{MF}}) = 0$。\n- 形成一个混合策略价值：$Q_{\\mathrm{mix}} = w\\,Q_{\\mathrm{MB}} + (1-w)\\,Q_{\\mathrm{MF}}$，其中 $w \\in [0,1]$。\n- 动作价值与估计成正比：$Q_{\\mathrm{MB}} \\propto \\hat{x}_{\\mathrm{MB}}$ 且 $Q_{\\mathrm{MF}} \\propto \\hat{x}_{\\mathrm{MF}}$。设比例常数为 $k > 0$，因此 $Q_{\\mathrm{MB}} = k\\hat{x}_{\\mathrm{MB}}$ 且 $Q_{\\mathrm{MF}} = k\\hat{x}_{\\mathrm{MF}}$。\n- 目标：选择 $w$ 以在保持无偏性的同时最小化混合的方差。\n- 用于计算的数值：$\\sigma_{\\mathrm{MB}}^{2} = 0.25$ 和 $\\sigma_{\\mathrm{MF}}^{2} = 1$。\n\n### 步骤 2：使用提取的已知条件进行验证\n问题陈述具有科学依据，提出了一个简化但标准的模型，该模型用于计算神经科学和强化学习中，以研究不同决策策略之间的仲裁。该模型使用已建立的统计学原理，例如组合无偏估计量。语言精确客观。该问题是适定的，因为它要求最小化权重 $w$ 的二次函数，该函数有唯一解。设置是自洽和一致的。没有矛盾、歧义或物理上不可能的条件。因此，该问题被认为是**有效的**。\n\n### 步骤 3：进行求解\n\n该问题要求我们找到最优权重 $w$，它在估计量保持无偏的约束下，最小化组合估计量的方差。\n\n让我们将潜在价值 $x$ 的组合估计量定义为 $\\hat{x}_{\\mathrm{mix}}$。由于 $Q_{\\mathrm{mix}}$ 与 $\\hat{x}_{\\mathrm{mix}}$ 成正比，最小化 $Q_{\\mathrm{mix}}$ 的方差等同于最小化 $\\hat{x}_{\\mathrm{mix}}$ 的方差。\n混合价值 $Q_{\\mathrm{mix}}$ 由下式给出：\n$$\nQ_{\\mathrm{mix}} = w\\,Q_{\\mathrm{MB}} + (1-w)\\,Q_{\\mathrm{MF}} = w (k\\hat{x}_{\\mathrm{MB}}) + (1-w) (k\\hat{x}_{\\mathrm{MF}}) = k(w\\hat{x}_{\\mathrm{MB}} + (1-w)\\hat{x}_{\\mathrm{MF}})\n$$\n设组合估计量为 $\\hat{x}_{\\mathrm{mix}} = w\\hat{x}_{\\mathrm{MB}} + (1-w)\\hat{x}_{\\mathrm{MF}}$。\n\n首先，我们验证这个组合估计量的无偏性。$\\hat{x}_{\\mathrm{mix}}$ 的期望值为：\n$$\nE[\\hat{x}_{\\mathrm{mix}}] = E[w\\hat{x}_{\\mathrm{MB}} + (1-w)\\hat{x}_{\\mathrm{MF}}]\n$$\n根据期望的线性性质：\n$$\nE[\\hat{x}_{\\mathrm{mix}}] = w E[\\hat{x}_{\\mathrm{MB}}] + (1-w) E[\\hat{x}_{\\mathrm{MF}}]\n$$\n鉴于两个估计量都是无偏的，我们有 $E[\\hat{x}_{\\mathrm{MB}}] = x$ 和 $E[\\hat{x}_{\\mathrm{MF}}] = x$。将这些代入方程：\n$$\nE[\\hat{x}_{\\mathrm{mix}}] = wx + (1-w)x = wx + x - wx = x\n$$\n组合估计量 $\\hat{x}_{\\mathrm{mix}}$ 对于任何 $w$ 的选择都是无偏的，因为其权重之和为1。无偏性约束被混合的形式自动满足。\n\n接下来，我们推导 $\\hat{x}_{\\mathrm{mix}}$ 方差的表达式。方差由下式给出：\n$$\n\\mathrm{Var}(\\hat{x}_{\\mathrm{mix}}) = \\mathrm{Var}(w\\hat{x}_{\\mathrm{MB}} + (1-w)\\hat{x}_{\\mathrm{MF}})\n$$\n使用随机变量和的方差性质 $\\mathrm{Var}(aX + bY) = a^2\\mathrm{Var}(X) + b^2\\mathrm{Var}(Y) + 2ab\\mathrm{Cov}(X,Y)$，我们得到：\n$$\n\\mathrm{Var}(\\hat{x}_{\\mathrm{mix}}) = w^2\\mathrm{Var}(\\hat{x}_{\\mathrm{MB}}) + (1-w)^2\\mathrm{Var}(\\hat{x}_{\\mathrm{MF}}) + 2w(1-w)\\mathrm{Cov}(\\hat{x}_{\\mathrm{MB}}, \\hat{x}_{\\mathrm{MF}})\n$$\n问题陈述噪声源是独立的，这意味着估计量 $\\hat{x}_{\\mathrm{MB}}$ 和 $\\hat{x}_{\\mathrm{MF}}$ 是独立的。因此，它们的协方差为零，即 $\\mathrm{Cov}(\\hat{x}_{\\mathrm{MB}}, \\hat{x}_{\\mathrm{MF}}) = 0$。方差表达式简化为：\n$$\n\\mathrm{Var}(\\hat{x}_{\\mathrm{mix}}) = w^2\\mathrm{Var}(\\hat{x}_{\\mathrm{MB}}) + (1-w)^2\\mathrm{Var}(\\hat{x}_{\\mathrm{MF}})\n$$\n代入给定的方差 $\\sigma_{\\mathrm{MB}}^{2}$ 和 $\\sigma_{\\mathrm{MF}}^{2}$：\n$$\nV(w) = w^2\\sigma_{\\mathrm{MB}}^{2} + (1-w)^2\\sigma_{\\mathrm{MF}}^{2}\n$$\n为了找到最小化该方差的 $w$ 值，我们对 $V(w)$ 关于 $w$ 求一阶导数，并令其为零：\n$$\n\\frac{dV}{dw} = \\frac{d}{dw} \\left( w^2\\sigma_{\\mathrm{MB}}^{2} + (1-w)^2\\sigma_{\\mathrm{MF}}^{2} \\right) = 2w\\sigma_{\\mathrm{MB}}^{2} + 2(1-w)(-1)\\sigma_{\\mathrm{MF}}^{2} = 2w\\sigma_{\\mathrm{MB}}^{2} - 2(1-w)\\sigma_{\\mathrm{MF}}^{2}\n$$\n为求最小值，令导数为零：\n$$\n2w\\sigma_{\\mathrm{MB}}^{2} - 2(1-w)\\sigma_{\\mathrm{MF}}^{2} = 0\n$$\n$$\nw\\sigma_{\\mathrm{MB}}^{2} = (1-w)\\sigma_{\\mathrm{MF}}^{2}\n$$\n$$\nw\\sigma_{\\mathrm{MB}}^{2} = \\sigma_{\\mathrm{MF}}^{2} - w\\sigma_{\\mathrm{MF}}^{2}\n$$\n$$\nw\\sigma_{\\mathrm{MB}}^{2} + w\\sigma_{\\mathrm{MF}}^{2} = \\sigma_{\\mathrm{MF}}^{2}\n$$\n$$\nw(\\sigma_{\\mathrm{MB}}^{2} + \\sigma_{\\mathrm{MF}}^{2}) = \\sigma_{\\mathrm{MF}}^{2}\n$$\n求解 $w$，我们得到闭式表达式：\n$$\nw = \\frac{\\sigma_{\\mathrm{MF}}^{2}}{\\sigma_{\\mathrm{MB}}^{2} + \\sigma_{\\mathrm{MF}}^{2}}\n$$\n为了确认这是一个最小值，我们检查二阶导数：\n$$\n\\frac{d^2V}{dw^2} = \\frac{d}{dw} \\left( 2w\\sigma_{\\mathrm{MB}}^{2} - 2\\sigma_{\\mathrm{MF}}^{2} + 2w\\sigma_{\\mathrm{MF}}^{2} \\right) = 2\\sigma_{\\mathrm{MB}}^{2} + 2\\sigma_{\\mathrm{MF}}^{2}\n$$\n由于方差 $\\sigma^2$ 是非负的（在任何有意义的情况下都是严格正的），所以 $\\frac{d^2V}{dw^2} > 0$，这证实了我们找到的 $w$ 值对应于一个最小值。\n\n现在，我们使用给定的方差计算 $w$ 的数值：$\\sigma_{\\mathrm{MB}}^{2} = 0.25$ 和 $\\sigma_{\\mathrm{MF}}^{2} = 1$。\n$$\nw = \\frac{1}{0.25 + 1} = \\frac{1}{1.25} = \\frac{1}{5/4} = \\frac{4}{5} = 0.8\n$$\n最优仲裁权重为 $w = 0.8$。\n\n简要解释：权重 $w$ 对应于基于模型（MB）的子系统。推导出的公式 $w = \\sigma_{\\mathrm{MF}}^2 / (\\sigma_{\\mathrm{MB}}^2 + \\sigma_{\\mathrm{MF}}^2)$ 表明，一个估计量的最优权重与另一个估计量的不确定性（方差）成正比。这是一种反方差加权的形式。不确定性较低的系统被赋予更高的信任度。\n在这种情况下，$\\sigma_{\\mathrm{MB}}^2 = 0.25$ 远小于 $\\sigma_{\\mathrm{MF}}^2 = 1.0$，表明MB系统比MF系统更精确，或噪声更小。得到的最优权重 $w=0.8$ 反映了这一点。混合策略为 $Q_{\\mathrm{mix}} = 0.8\\,Q_{\\mathrm{MB}} + 0.2\\,Q_{\\mathrm{MF}}$。这意味着仲裁机制将80%的影响力分配给更可靠的MB通路，而只将20%分配给可靠性较低的MF通路。这种策略产生的组合动作价值估计比任何单个估计本身都更精确（即具有更小的方差）。",
            "answer": "$$\n\\boxed{0.8}\n$$"
        }
    ]
}