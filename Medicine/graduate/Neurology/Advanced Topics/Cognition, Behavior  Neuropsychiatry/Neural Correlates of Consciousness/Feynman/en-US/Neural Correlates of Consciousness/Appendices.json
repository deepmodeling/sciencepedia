{
    "hands_on_practices": [
        {
            "introduction": "A foundational skill in the study of consciousness is the ability to quantify changes in neural signals that track with shifts in arousal and awareness. This exercise provides a concrete starting point by focusing on electroencephalography (EEG) spectral power, a widely used neural correlate, in the context of anesthesia-induced loss of consciousness. You will practice calculating and interpreting changes in specific frequency bands, linking raw data transformation to profound changes in conscious states. ",
            "id": "4501010",
            "problem": "Electroencephalography (EEG) band-limited spectral power is a widely used, quantitative neural signal linked to level of arousal. The alpha band ($8$–$12$ Hz) and delta band ($0.5$–$4$ Hz) are known to change with gamma-aminobutyric acid type A ($\\text{GABA}_{\\text{A}}$)-ergic anesthetic induction such as propofol, and these changes have been consistently associated with transitions in consciousness. Suppose an adult subject at rest exhibits baseline alpha-band power of $5$ arbitrary units and delta-band power of $3$ arbitrary units immediately prior to the onset of clinically confirmed propofol-induced unresponsiveness. Within $5$ minutes after induction, the measured alpha-band power increases to $12$ arbitrary units and the delta-band power increases to $10$ arbitrary units, using the same preprocessing pipeline and spatial montage. Using the fundamental definition of proportional increase in a quantity, defined as $\\frac{P_{\\text{after}}-P_{\\text{before}}}{P_{\\text{before}}}$, compute the proportional increase for the alpha and delta bands. Express each increase as a decimal (that is, a fraction of the initial value, not a number with a percent sign), and round each to four significant figures. Report the two numbers in the order alpha, delta. In a sentence, interpret how the pattern of band-specific changes relates to level of consciousness in terms of neural correlates of consciousness. The final numeric answer must be given with no units (dimensionless).",
            "solution": "The problem is first subjected to a validation process.\n\n### Step 1: Extract Givens\n-   Alpha band frequency range: $8$–$12$ Hz\n-   Delta band frequency range: $0.5$–$4$ Hz\n-   Baseline alpha-band power, $P_{\\alpha, \\text{before}}$: $5$ arbitrary units\n-   Baseline delta-band power, $P_{\\delta, \\text{before}}$: $3$ arbitrary units\n-   Post-induction alpha-band power, $P_{\\alpha, \\text{after}}$: $12$ arbitrary units\n-   Post-induction delta-band power, $P_{\\delta, \\text{after}}$: $10$ arbitrary units\n-   Definition of proportional increase: $\\frac{P_{\\text{after}}-P_{\\text{before}}}{P_{\\text{before}}}$\n-   Rounding requirement: four significant figures\n-   Reporting order: alpha, delta\n-   Interpretive task: Relate the pattern of changes to the level of consciousness.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. The stated changes in alpha and delta band power following the administration of a GABAergic anesthetic like propofol are consistent with established findings in clinical neuroscience and anesthesiology. The phenomenon of a paradoxical increase in frontal alpha power (\"alpha-surge\") and a simultaneous increase in slow-wave (delta) power are well-documented neural correlates of the loss of consciousness. The problem is well-posed, providing all necessary numerical values and a precise mathematical definition for the calculation. It is objective and free of ambiguity. The interpretive component of the question does not render the problem invalid, as it asks for a conclusion based on the calculated data within the provided scientific context, a standard practice in scientific problem-solving.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid and a full solution will be provided.\n\nThe problem requires the computation of the proportional increase for the alpha and delta EEG frequency bands using the provided formula and data. Let the proportional increase be denoted by $\\Delta_P$. The formula is:\n$$ \\Delta_P = \\frac{P_{\\text{after}}-P_{\\text{before}}}{P_{\\text{before}}} $$\nwhere $P_{\\text{before}}$ is the power before anesthetic induction and $P_{\\text{after}}$ is the power after induction.\n\nFirst, we compute the proportional increase for the alpha band, $\\Delta_{P, \\alpha}$.\nThe given values are $P_{\\alpha, \\text{before}} = 5$ and $P_{\\alpha, \\text{after}} = 12$.\nSubstituting these into the formula:\n$$ \\Delta_{P, \\alpha} = \\frac{12 - 5}{5} = \\frac{7}{5} = 1.4 $$\nThe problem requires the answer to be rounded to four significant figures. The exact value $1.4$ is written as $1.400$ to meet this requirement.\n\nNext, we compute the proportional increase for the delta band, $\\Delta_{P, \\delta}$.\nThe given values are $P_{\\delta, \\text{before}} = 3$ and $P_{\\delta, \\text{after}} = 10$.\nSubstituting these into the formula:\n$$ \\Delta_{P, \\delta} = \\frac{10 - 3}{3} = \\frac{7}{3} $$\nAs a decimal, this is a repeating fraction: $2.3333...$.\nRounding to four significant figures, we get $2.333$.\n\nThe final numeric answers are the proportional increases for the alpha and delta bands, in that order: $1.400$ and $2.333$.\n\nFinally, the problem asks for an interpretation of the pattern of band-specific changes. The concurrent large proportional increases in both alpha ($1.400$) and delta ($2.333$) band power, particularly the greater relative increase in slow-wave delta activity, represent a characteristic electrophysiological signature or neural correlate of the transition from wakefulness to propofol-induced unresponsiveness.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.400 & 2.333\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from tonic brain states to the fast neural dynamics that underlie specific conscious percepts requires more temporally precise methods. This practice delves into the analysis of Event-Related Potentials (ERPs), which are ideal for pinpointing the timing of neural processes related to conscious versus non-conscious perception. Here, you will apply rigorous statistical techniques to identify significant divergences in brain activity, including the critical step of correcting for multiple comparisons—an essential skill for analyzing any neurophysiological time-series data. ",
            "id": "4501015",
            "problem": "An Electroencephalography (EEG) study examines the neural correlates of consciousness by comparing Event-Related Potential (ERP) waveforms elicited by the same visual stimuli under two conditions: consciously perceived (\"seen\") and not consciously perceived (\"unseen\") due to backward masking. Measurements are taken at a posterior occipital electrode. For $n=24$ participants, the grand-average ERP amplitude (in microvolts) at eight post-stimulus latencies $t$ and the within-subject standard deviation of the paired differences are as follows. For each latency $t$, you are given $\\mu_{\\text{seen}}(t)$, $\\mu_{\\text{unseen}}(t)$, and $s_{d}(t)$, where $s_{d}(t)$ is the standard deviation across participants of the within-subject difference $d_{i}(t)=\\text{ERP}_{\\text{seen},i}(t)-\\text{ERP}_{\\text{unseen},i}(t)$.\n\n- $t=100~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=2.1~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=2.0~\\mu\\mathrm{V}$, $s_{d}(t)=1.2~\\mu\\mathrm{V}$\n- $t=150~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=3.0~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=2.7~\\mu\\mathrm{V}$, $s_{d}(t)=1.1~\\mu\\mathrm{V}$\n- $t=180~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=-1.6~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=-0.8~\\mu\\mathrm{V}$, $s_{d}(t)=1.0~\\mu\\mathrm{V}$\n- $t=200~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=-2.4~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=-1.2~\\mu\\mathrm{V}$, $s_{d}(t)=1.1~\\mu\\mathrm{V}$\n- $t=220~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=-2.7~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=-1.2~\\mu\\mathrm{V}$, $s_{d}(t)=1.0~\\mu\\mathrm{V}$\n- $t=250~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=-1.9~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=-0.9~\\mu\\mathrm{V}$, $s_{d}(t)=1.2~\\mu\\mathrm{V}$\n- $t=300~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=2.4~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=1.9~\\mu\\mathrm{V}$, $s_{d}(t)=1.3~\\mu\\mathrm{V}$\n- $t=350~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=5.1~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=3.1~\\mu\\mathrm{V}$, $s_{d}(t)=1.5~\\mu\\mathrm{V}$\n\nTask:\n1. Compute the difference wave $D(t)=\\mu_{\\text{seen}}(t)-\\mu_{\\text{unseen}}(t)$ at each latency.\n2. For each latency, perform a paired $t$-test across participants using the statistic $t(t)=\\bar{d}(t)/\\left(s_{d}(t)/\\sqrt{n}\\right)$, where $\\bar{d}(t)=D(t)$ and degrees of freedom $\\mathrm{df}=n-1$. Obtain two-sided $p$-values for each $t$ using the Student $t$ distribution.\n3. To control for multiple comparisons across $m=8$ time points, apply the Benjamini–Hochberg procedure for False Discovery Rate (FDR) control at $q=0.05$. Identify the earliest latency at which a statistically significant divergence between seen and unseen ERPs first occurs.\n4. Briefly justify, based on the statistical structure of the tests and the temporal dependence of ERP data, why the chosen multiple-comparisons method is appropriate, and contrast it with at least one alternative approach (e.g., Bonferroni Family-Wise Error Rate (FWER) control or cluster-based permutation).\n\nWhich option best identifies the earliest significant divergence and an appropriate method to control for multiple comparisons in this context?\n\nA. $180~\\mathrm{ms}$; compute $D(t)$ as seen minus unseen, use paired $t$-tests at each time point, and control across $m=8$ tests with Benjamini–Hochberg False Discovery Rate at $q=0.05$; the earliest significant latency is $180~\\mathrm{ms}$.\n\nB. $150~\\mathrm{ms}$; compute $D(t)$ and use uncorrected paired $t$-tests with $\\alpha=0.05$, then apply Bonferroni to confirm that $150~\\mathrm{ms}$ remains significant as the earliest divergence.\n\nC. $220~\\mathrm{ms}$; use paired $t$-tests and Bonferroni Family-Wise Error Rate control with $\\alpha_{\\mathrm{FWER}}=0.05/8$, because earlier latencies do not survive this correction, making $220~\\mathrm{ms}$ the first significant divergence.\n\nD. $300~\\mathrm{ms}$; use cluster-based permutation across time points to control multiple comparisons, because only the late positive complex around $300~\\mathrm{ms}$ yields a significant cluster onset and earlier negative deflections do not form significant clusters.",
            "solution": "The user has provided a problem from the field of cognitive neuroscience, requiring statistical analysis of simulated Event-Related Potential (ERP) data. The problem will be solved by following the four specified tasks.\n\n### Step 1: Validation of the Problem Statement\n\nFirst, the givens of the problem are extracted.\n- **Study Type**: Electroencephalography (EEG) study on neural correlates of consciousness.\n- **Comparison**: \"seen\" vs. \"unseen\" conditions for identical visual stimuli.\n- **Measurement**: ERP amplitude at a posterior occipital electrode.\n- **Sample Size**: $n=24$ participants.\n- **Data Points**: $m=8$ latencies $t$ (in $\\mathrm{ms}$).\n- **Provided Statistics for each latency $t$**:\n  - Grand-average ERP amplitude for the \"seen\" condition: $\\mu_{\\text{seen}}(t)$.\n  - Grand-average ERP amplitude for the \"unseen\" condition: $\\mu_{\\text{unseen}}(t)$.\n  - Standard deviation of the within-subject difference: $s_{d}(t)$, where $d_{i}(t)=\\text{ERP}_{\\text{seen},i}(t)-\\text{ERP}_{\\text{unseen},i}(t)$.\n- **Specific Data**:\n  - $t=100~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=2.1~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=2.0~\\mu\\mathrm{V}$, $s_{d}(t)=1.2~\\mu\\mathrm{V}$\n  - $t=150~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=3.0~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=2.7~\\mu\\mathrm{V}$, $s_{d}(t)=1.1~\\mu\\mathrm{V}$\n  - $t=180~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=-1.6~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=-0.8~\\mu\\mathrm{V}$, $s_{d}(t)=1.0~\\mu\\mathrm{V}$\n  - $t=200~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=-2.4~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=-1.2~\\mu\\mathrm{V}$, $s_{d}(t)=1.1~\\mu\\mathrm{V}$\n  - $t=220~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=-2.7~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=-1.2~\\mu\\mathrm{V}$, $s_{d}(t)=1.0~\\mu\\mathrm{V}$\n  - $t=250~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=-1.9~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=-0.9~\\mu\\mathrm{V}$, $s_{d}(t)=1.2~\\mu\\mathrm{V}$\n  - $t=300~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=2.4~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=1.9~\\mu\\mathrm{V}$, $s_{d}(t)=1.3~\\mu\\mathrm{V}$\n  - $t=350~\\mathrm{ms}$: $\\mu_{\\text{seen}}(t)=5.1~\\mu\\mathrm{V}$, $\\mu_{\\text{unseen}}(t)=3.1~\\mu\\mathrm{V}$, $s_{d}(t)=1.5~\\mu\\mathrm{V}$\n- **Task 1**: Compute the difference wave $D(t)=\\mu_{\\text{seen}}(t)-\\mu_{\\text{unseen}}(t)$.\n- **Task 2**: Perform a paired $t$-test at each latency. The test statistic is given as $t(t)=\\bar{d}(t)/\\left(s_{d}(t)/\\sqrt{n}\\right)$, with $\\bar{d}(t)=D(t)$ and degrees of freedom $\\mathrm{df}=n-1$. Obtain two-sided $p$-values.\n- **Task 3**: Apply the Benjamini–Hochberg procedure for False Discovery Rate (FDR) control at $q=0.05$ across $m=8$ tests. Identify the earliest significant latency.\n- **Task 4**: Justify the choice of multiple-comparisons method.\n\nThe problem statement is scientifically grounded, describing a standard experimental design and data analysis pipeline in cognitive neuroscience. The provided data are realistic, and the tasks are well-defined and formalizable. The use of grand-average amplitudes $\\mu_{\\text{seen}}$ and $\\mu_{\\text{unseen}}$ to calculate the mean difference $\\bar{d}(t)$ is correct, as the mean of the differences is equal to the difference of the means: $\\bar{d}(t) = \\frac{1}{n}\\sum_i d_i(t) = \\frac{1}{n}\\sum_i(\\text{ERP}_{\\text{seen},i}(t) - \\text{ERP}_{\\text{unseen},i}(t)) = \\mu_{\\text{seen}}(t) - \\mu_{\\text{unseen}}(t) = D(t)$. The statistical formulas for the paired $t$-test and degrees of freedom are correct. The problem is self-contained and does not contain any contradictions or ambiguities.\n\n**Verdict**: The problem is valid.\n\n### Step 2: Derivation of the Solution\n\nThe solution proceeds by executing the four tasks laid out in the problem statement.\n\n**Task 1: Compute the difference wave $D(t)$.**\nThe difference wave $D(t) = \\bar{d}(t) = \\mu_{\\text{seen}}(t)-\\mu_{\\text{unseen}}(t)$ is calculated for each latency:\n- $t=100~\\mathrm{ms}$: $D(100) = 2.1 - 2.0 = 0.1~\\mu\\mathrm{V}$\n- $t=150~\\mathrm{ms}$: $D(150) = 3.0 - 2.7 = 0.3~\\mu\\mathrm{V}$\n- $t=180~\\mathrm{ms}$: $D(180) = -1.6 - (-0.8) = -0.8~\\mu\\mathrm{V}$\n- $t=200~\\mathrm{ms}$: $D(200) = -2.4 - (-1.2) = -1.2~\\mu\\mathrm{V}$\n- $t=220~\\mathrm{ms}$: $D(220) = -2.7 - (-1.2) = -1.5~\\mu\\mathrm{V}$\n- $t=250~\\mathrm{ms}$: $D(250) = -1.9 - (-0.9) = -1.0~\\mu\\mathrm{V}$\n- $t=300~\\mathrm{ms}$: $D(300) = 2.4 - 1.9 = 0.5~\\mu\\mathrm{V}$\n- $t=350~\\mathrm{ms}$: $D(350) = 5.1 - 3.1 = 2.0~\\mu\\mathrm{V}$\n\n**Task 2: Perform paired $t$-tests and obtain $p$-values.**\nThe sample size is $n=24$, so the number of degrees of freedom is $\\mathrm{df}=n-1=23$. The standard error of the mean difference is $SE_d(t) = s_d(t)/\\sqrt{n} = s_d(t)/\\sqrt{24}$. The $t$-statistic is $t_{stat}(t) = \\bar{d}(t)/SE_d(t)$. We calculate these values and the corresponding two-sided $p$-values from the Student's $t$-distribution.\n\n- $t=100~\\mathrm{ms}$: $SE_d = 1.2/\\sqrt{24} \\approx 0.2449~\\mu\\mathrm{V}$. $t_{stat} = 0.1 / 0.2449 \\approx 0.408$. $p \\approx 0.6871$.\n- $t=150~\\mathrm{ms}$: $SE_d = 1.1/\\sqrt{24} \\approx 0.2245~\\mu\\mathrm{V}$. $t_{stat} = 0.3 / 0.2245 \\approx 1.336$. $p \\approx 0.1942$.\n- $t=180~\\mathrm{ms}$: $SE_d = 1.0/\\sqrt{24} \\approx 0.2041~\\mu\\mathrm{V}$. $t_{stat} = -0.8 / 0.2041 \\approx -3.919$. $p \\approx 0.00069$.\n- $t=200~\\mathrm{ms}$: $SE_d = 1.1/\\sqrt{24} \\approx 0.2245~\\mu\\mathrm{V}$. $t_{stat} = -1.2 / 0.2245 \\approx -5.345$. $p \\approx 0.000018$.\n- $t=220~\\mathrm{ms}$: $SE_d = 1.0/\\sqrt{24} \\approx 0.2041~\\mu\\mathrm{V}$. $t_{stat} = -1.5 / 0.2041 \\approx -7.349$. $p \\approx 0.0000003$.\n- $t=250~\\mathrm{ms}$: $SE_d = 1.2/\\sqrt{24} \\approx 0.2449~\\mu\\mathrm{V}$. $t_{stat} = -1.0 / 0.2449 \\approx -4.083$. $p \\approx 0.00049$.\n- $t=300~\\mathrm{ms}$: $SE_d = 1.3/\\sqrt{24} \\approx 0.2654~\\mu\\mathrm{V}$. $t_{stat} = 0.5 / 0.2654 \\approx 1.884$. $p \\approx 0.0720$.\n- $t=350~\\mathrm{ms}$: $SE_d = 1.5/\\sqrt{24} \\approx 0.3062~\\mu\\mathrm{V}$. $t_{stat} = 2.0 / 0.3062 \\approx 6.532$. $p \\approx 0.0000015$.\n\n**Task 3: Apply the Benjamini–Hochberg (BH) procedure.**\nWe control the False Discovery Rate (FDR) at $q=0.05$ for $m=8$ comparisons.\n1.  Rank the $p$-values, $p_{(i)}$, in ascending order:\n    - $p_{(1)} = 0.0000003$ (from $t=220~\\mathrm{ms}$)\n    - $p_{(2)} = 0.0000015$ (from $t=350~\\mathrm{ms}$)\n    - $p_{(3)} = 0.000018$ (from $t=200~\\mathrm{ms}$)\n    - $p_{(4)} = 0.00049$ (from $t=250~\\mathrm{ms}$)\n    - $p_{(5)} = 0.00069$ (from $t=180~\\mathrm{ms}$)\n    - $p_{(6)} = 0.0720$ (from $t=300~\\mathrm{ms}$)\n    - $p_{(7)} = 0.1942$ (from $t=150~\\mathrm{ms}$)\n    - $p_{(8)} = 0.6871$ (from $t=100~\\mathrm{ms}$)\n2.  Find the largest rank $k$ for which $p_{(k)} \\leq \\frac{k}{m}q$. Here, $m=8$ and $q=0.05$.\n    - $k=1$: $p_{(1)} \\approx 3 \\times 10^{-7} \\leq (1/8)\\times 0.05 = 0.00625$. Condition met.\n    - $k=2$: $p_{(2)} \\approx 1.5 \\times 10^{-6} \\leq (2/8)\\times 0.05 = 0.0125$. Condition met.\n    - $k=3$: $p_{(3)} \\approx 1.8 \\times 10^{-5} \\leq (3/8)\\times 0.05 = 0.01875$. Condition met.\n    - $k=4$: $p_{(4)} \\approx 4.9 \\times 10^{-4} \\leq (4/8)\\times 0.05 = 0.025$. Condition met.\n    - $k=5$: $p_{(5)} \\approx 6.9 \\times 10^{-4} \\leq (5/8)\\times 0.05 = 0.03125$. Condition met.\n    - $k=6$: $p_{(6)} \\approx 0.0720 > (6/8)\\times 0.05 = 0.0375$. Condition NOT met.\n3.  The largest rank is $k=5$. Therefore, the null hypotheses corresponding to $p$-values with ranks $1$ through $5$ are rejected.\n4.  The significant latencies are those corresponding to these $p$-values: $220$, $350$, $200$, $250$, and $180~\\mathrm{ms}$.\n5.  The earliest latency among these significant results is $180~\\mathrm{ms}$.\n\n**Task 4: Justify the method and contrast with alternatives.**\n- **Appropriateness of Benjamini-Hochberg (FDR)**: In neurophysiological time-series data like ERPs, adjacent time points are highly correlated. The standard BH procedure is robust under certain forms of dependency (positive regression dependency, PRDS), which is a reasonable assumption for this type of data. FDR control is less conservative than Family-Wise Error Rate (FWER) control, offering greater statistical power to detect true effects, which is often desirable in exploratory analyses common in neuroscience. It controls the expected proportion of false discoveries among all rejected hypotheses, a pragmatic trade-off between finding true effects and making false claims.\n- **Contrast with Bonferroni (FWER)**: The Bonferroni correction controls the FWER by dividing the significance level $\\alpha$ by the number of tests $m$. Here, the adjusted significance threshold would be $p < 0.05/8 = 0.00625$. The latencies at $180, 200, 220, 250, 350~\\mathrm{ms}$ have $p$-values smaller than this threshold and would also be significant. The earliest significant latency would still be $180~\\mathrm{ms}$. While simpler to apply, Bonferroni is known to be overly conservative, especially with correlated tests, thus increasing the chance of Type II errors (failing to detect true effects).\n- **Contrast with Cluster-Based Permutation**: This non-parametric method is specifically designed for data with spatio-temporal structure. It identifies clusters of adjacent tests that exceed a certain threshold and evaluates the significance of these clusters via permutation testing. This approach intrinsically accounts for the temporal correlation of the ERP data and is often more sensitive than point-wise corrections like BH or Bonferroni. However, it tests for the significance of clusters, not individual time points. Based on our strong, contiguous $t$-statistics from $180$ to $250~\\mathrm{ms}$, this method would likely identify a significant negative cluster starting at or before $180~\\mathrm{ms}$. BH is a valid and widely used point-wise alternative.\n\n### Step 3: Evaluation of the Options\n\n- **A. $180~\\mathrm{ms}$; compute $D(t)$ as seen minus unseen, use paired $t$-tests at each time point, and control across $m=8$ tests with Benjamini–Hochberg False Discovery Rate at $q=0.05$; the earliest significant latency is $180~\\mathrm{ms}$.**\nThis option accurately describes the methodology requested in the problem statement and correctly identifies the outcome of the analysis. My calculations confirmed that after BH-FDR correction, $180~\\mathrm{ms}$ is the earliest latency at which a statistically significant difference is observed.\n**Verdict: Correct.**\n\n- **B. $150~\\mathrm{ms}$; compute $D(t)$ and use uncorrected paired $t$-tests with $\\alpha=0.05$, then apply Bonferroni to confirm that $150~\\mathrm{ms}$ remains significant as the earliest divergence.**\nThe uncorrected $p$-value at $150~\\mathrm{ms}$ is $p \\approx 0.194$, which is not significant at $\\alpha=0.05$. Therefore, the premise that $150~\\mathrm{ms}$ is the earliest significant uncorrected latency is false. The methodological approach described is also unsound.\n**Verdict: Incorrect.**\n\n- **C. $220~\\mathrm{ms}$; use paired $t$-tests and Bonferroni Family-Wise Error Rate control with $\\alpha_{\\mathrm{FWER}}=0.05/8$, because earlier latencies do not survive this correction, making $220~\\mathrm{ms}$ the first significant divergence.**\nThe Bonferroni-corrected threshold is $p < 0.05/8 = 0.00625$. The $p$-value at $180~\\mathrm{ms}$ is $p \\approx 0.00069$, which is well below this threshold. Thus, the statement that earlier latencies (specifically $180~\\mathrm{ms}$) do not survive the correction is false. The earliest significant latency with Bonferroni correction is $180~\\mathrm{ms}$, not $220~\\mathrm{ms}$.\n**Verdict: Incorrect.**\n\n- **D. $300~\\mathrm{ms}$; use cluster-based permutation across time points to control multiple comparisons, because only the late positive complex around $300~\\mathrm{ms}$ yields a significant cluster onset and earlier negative deflections do not form significant clusters.**\nThis option proposes an alternative method. However, the uncorrected $p$-value at $300~\\mathrm{ms}$ is $p \\approx 0.072$, which is not statistically significant. In contrast, the latencies from $180~\\mathrm{ms}$ to $250~\\mathrm{ms}$ show a strong, contiguous negative deflection with very small $p$-values. It is extremely unlikely that a cluster-based analysis would fail to find a significant cluster in this range while finding one at $300~\\mathrm{ms}$. The conclusion is inconsistent with the provided data.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The most advanced practice involves moving beyond analyzing empirical data to building and testing computational theories of consciousness. This exercise challenges you to implement a hierarchical predictive coding model, a leading theoretical framework that posits consciousness emerges from the brain's process of inferring the causes of its sensory inputs. By formalizing concepts like prediction error and posterior precision in code, you will gain hands-on experience with how theoretical ideas about perceptual inference can be simulated and tested, bridging the gap between abstract theory and concrete implementation. ",
            "id": "4501106",
            "problem": "You are asked to formalize and implement a two-level hierarchical predictive coding model to study the neural correlates of consciousness through the interplay between feedback predictions and feedforward prediction errors. The foundation must be built from the following base: Bayes’ rule for linear Gaussian models, the concept of precision as the inverse of variance, and variational inference under the Laplace assumption, which yields gradient descent updates to minimize Variational Free Energy (VFE). The task is to derive and implement a mathematically explicit model and a decision criterion for when conscious contents emerge, framed as a fully specified computational problem.\n\nConsider a hierarchical linear Gaussian generative model with two latent levels and an observation:\n- The top latent state $s_t \\in \\mathbb{R}^n$ generates the middle latent state $x_t \\in \\mathbb{R}^n$ through the linear mapping $A \\in \\mathbb{R}^{n \\times n}$.\n- The middle latent state $x_t$ generates the observation $y_t \\in \\mathbb{R}^n$ through the linear mapping $C \\in \\mathbb{R}^{n \\times n}$.\n- Noise is additive and Gaussian at each level.\n\nFormally, the generative process is:\n$$\nx_t = A s_t + \\epsilon_x, \\quad \\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x),\n$$\n$$\ny_t = C x_t + \\epsilon_y, \\quad \\epsilon_y \\sim \\mathcal{N}(0, \\Sigma_y),\n$$\nand the prior over $s_t$ is\n$$\ns_t \\sim \\mathcal{N}(\\mu_s^0, \\Sigma_s).\n$$\nThe inverse covariances (precisions) are $\\Pi_x = \\Sigma_x^{-1}$, $\\Pi_y = \\Sigma_y^{-1}$, and $\\Pi_s = \\Sigma_s^{-1}$. Assume a constant latent cause $s_t \\equiv s^\\star$ over time and time-independent $\\Sigma_x$, $\\Sigma_y$, and $\\Sigma_s$. Let an attention-like gain $\\gamma > 0$ modulate the effective sensory precision via $\\Pi_y^{\\mathrm{eff}} = \\gamma \\Pi_y$.\n\nDefine internal estimates $\\hat{s}_t$ and $\\hat{x}_t$ and assume predictive coding updates arise from gradient descent on Variational Free Energy (VFE) under a Laplace approximation. Feedback signals carry predictions $\\hat{x}_t^{\\mathrm{pred}} = A \\hat{s}_t$ and $\\hat{y}_t^{\\mathrm{pred}} = C \\hat{x}_t$. Feedforward signals carry precision-weighted prediction errors, with sensory prediction error $e_y = y_t - \\hat{y}_t^{\\mathrm{pred}}$ and middle-level prediction error $e_x = \\hat{x}_t - \\hat{x}_t^{\\mathrm{pred}}$. The prior deviation is $e_s = \\hat{s}_t - \\mu_s^0$.\n\nYour tasks:\n- Derive, from the above base, discrete-time gradient descent updates for $\\hat{x}_t$ and $\\hat{s}_t$ that minimize VFE and are consistent with the rule that feedback conveys predictions and feedforward conveys precision-weighted prediction errors.\n- Implement a simulator that evolves the internal estimates over $T$ steps for fixed $A$, $C$, and $s^\\star$, while generating observations from the generative model with independent Gaussian noise at each time step.\n- Define a Conscious Content Emergence Criterion (CCEC) in terms of the interplay between predictions and prediction errors:\n    - Let the precision-weighted sensory error norm be $\\| \\Pi_y^{\\mathrm{eff}} e_y \\|_2$ and the precision-weighted middle-level error norm be $\\| \\Pi_x e_x \\|_2$.\n    - Approximate the posterior precision at the top level using the Laplace approximation for linear Gaussian models:\n      $$\n      \\Lambda_s = \\Pi_s + A^\\top \\Pi_x A, \\quad \\Sigma_s^{\\mathrm{post}} = \\Lambda_s^{-1}.\n      $$\n      The differential entropy for a Gaussian top-level posterior is\n      $$\n      H_s = \\frac{1}{2} \\ln\\left((2\\pi e)^n \\det(\\Sigma_s^{\\mathrm{post}})\\right).\n      $$\n    - Conscious content is said to emerge at time $t$ if all of the following are simultaneously satisfied:\n        1. $\\| \\Pi_y^{\\mathrm{eff}} e_{y,t} \\|_2 \\le \\varepsilon_{\\mathrm{low}}$,\n        2. $\\| \\Pi_x e_{x,t} \\|_2 \\le \\varepsilon_{\\mathrm{low}}$,\n        3. $\\| \\hat{s}_t - s^\\star \\|_2 \\le \\delta_s$,\n        4. $H_s \\le H_{\\mathrm{thresh}}$.\n      Conscious content is declared for the trial if this set of inequalities holds for at least $L$ consecutive time steps within the $T$-step simulation.\n\nYour program must:\n- Use $n=2$, $A = I_2$, and $C = I_2$, where $I_2$ is the $2\\times 2$ identity matrix.\n- Use a fixed $s^\\star = [1.0, -1.0]^\\top$ and initial internal estimates $\\hat{s}_0 = [0.0, 0.0]^\\top$, $\\hat{x}_0 = [0.0, 0.0]^\\top$.\n- Draw independent Gaussian noise at each time step: $\\epsilon_x \\sim \\mathcal{N}(0, \\Sigma_x)$, $\\epsilon_y \\sim \\mathcal{N}(0, \\Sigma_y)$, and set the random seed to ensure reproducibility so that your answers are deterministic.\n- Implement the derived discrete-time updates with step sizes $\\alpha_x$ and $\\alpha_s$.\n- For each test case, return a boolean indicating whether conscious content emerges under the CCEC.\n\nTest suite:\n- Common fixed quantities for all tests: dimension $n=2$, $A=I_2$, $C=I_2$, $s^\\star = [1.0,-1.0]^\\top$, initial $\\hat{s}_0=[0,0]^\\top$, $\\hat{x}_0=[0,0]^\\top$, $T=80$, $L=5$, $\\varepsilon_{\\mathrm{low}}=0.2$, $\\delta_s=0.25$, $H_{\\mathrm{thresh}}=0.5$.\n- The four test cases differ in noise scales, prior, attention gain, and update rates, specified as tuples $(\\mu_s^0, \\sigma_y, \\sigma_x, \\sigma_s, \\gamma, \\alpha_x, \\alpha_s)$:\n    1. Case 1 (happy path): $( [0.0,0.0], 0.2, 0.3, 1.0, 1.0, 0.35, 0.25 )$.\n    2. Case 2 (low effective sensory precision): $( [0.0,0.0], 1.5, 1.0, 1.0, 0.05, 0.35, 0.25 )$.\n    3. Case 3 (weak feedback updating): $( [0.0,0.0], 0.2, 0.3, 1.0, 1.0, 0.35, 0.02 )$.\n    4. Case 4 (dominating incorrect prior): $( [3.0,3.0], 0.2, 0.3, 0.1, 1.0, 0.35, 0.25 )$.\n\nImplementation details:\n- Set $\\Sigma_y = \\sigma_y^2 I_2$, $\\Sigma_x = \\sigma_x^2 I_2$, and $\\Sigma_s = \\sigma_s^2 I_2$ for each test case, with corresponding precisions $\\Pi_y = \\Sigma_y^{-1}$, $\\Pi_x = \\Sigma_x^{-1}$, $\\Pi_s = \\Sigma_s^{-1}$, and $\\Pi_y^{\\mathrm{eff}} = \\gamma \\Pi_y$.\n- At each time step, generate $x_t = A s^\\star + \\epsilon_x$ and $y_t = C x_t + \\epsilon_y$ and update $\\hat{x}_t$ and $\\hat{s}_t$ by the derived predictive coding rules.\n\nRequired final program output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False]\"), corresponding to the four test cases in the order listed above.",
            "solution": "The user has provided a well-defined problem in computational neuroscience, grounded in the theory of predictive coding. The task is to derive the update rules for a hierarchical generative model, implement a simulation, and apply a specific criterion for the emergence of conscious content. The problem is scientifically sound, self-contained, and algorithmically specified.\n\n### 1. Derivation of Update Rules from Variational Free Energy\n\nThe core of the predictive coding framework is the minimization of Variational Free Energy (VFE), which, for linear Gaussian models under the Laplace approximation, is equivalent to minimizing an energy function defined by the sum of squared, precision-weighted prediction errors.\n\nThe generative model is given by:\n- Prior: $p(s_t) = \\mathcal{N}(s_t | \\mu_s^0, \\Sigma_s)$\n- Likelihood (Level 1): $p(x_t|s_t) = \\mathcal{N}(x_t | A s_t, \\Sigma_x)$\n- Likelihood (Level 2): $p(y_t|x_t) = \\mathcal{N}(y_t | C x_t, \\Sigma_y)$\n\nThe VFE, denoted by $F$, can be expressed as a function of the internal estimates $\\hat{s}_t$ and $\\hat{x}_t$. The objective is to find the estimates that minimize this quantity. The VFE is proportional to the sum of squared errors, weighted by their respective precisions. The sensory precision $\\Pi_y = \\Sigma_y^{-1}$ is modulated by an attentional gain $\\gamma$, yielding an effective precision $\\Pi_y^{\\mathrm{eff}} = \\gamma \\Pi_y$.\n\nThe VFE to be minimized is:\n$$\nF(\\hat{s}_t, \\hat{x}_t) = \\frac{1}{2} (y_t - C\\hat{x}_t)^\\top \\Pi_y^{\\mathrm{eff}} (y_t - C\\hat{x}_t) + \\frac{1}{2} (\\hat{x}_t - A\\hat{s}_t)^\\top \\Pi_x (\\hat{x}_t - A\\hat{s}_t) + \\frac{1}{2} (\\hat{s}_t - \\mu_s^0)^\\top \\Pi_s (\\hat{s}_t - \\mu_s^0)\n$$\nWe derive the update rules for $\\hat{x}_t$ and $\\hat{s}_t$ using gradient descent. The updates are in the negative direction of the gradient of $F$:\n$$\n\\hat{x}_{t+1} = \\hat{x}_t - \\alpha_x \\frac{\\partial F}{\\partial \\hat{x}_t}\n$$\n$$\n\\hat{s}_{t+1} = \\hat{s}_t - \\alpha_s \\frac{\\partial F}{\\partial \\hat{s}_t}\n$$\nwhere $\\alpha_x$ and $\\alpha_s$ are the step sizes (learning rates).\n\nFirst, we compute the partial derivative of $F$ with respect to $\\hat{x}_t$:\n$$\n\\frac{\\partial F}{\\partial \\hat{x}_t} = \\frac{\\partial}{\\partial \\hat{x}_t} \\left[ \\frac{1}{2} (y_t - C\\hat{x}_t)^\\top \\Pi_y^{\\mathrm{eff}} (y_t - C\\hat{x}_t) + \\frac{1}{2} (\\hat{x}_t - A\\hat{s}_t)^\\top \\Pi_x (\\hat{x}_t - A\\hat{s}_t) \\right]\n$$\nUsing the identity for quadratic forms $\\frac{\\partial}{\\partial v} (z - Mv)^\\top W (z-Mv) = -2 M^\\top W (z-Mv)$, we get:\n$$\n\\frac{\\partial F}{\\partial \\hat{x}_t} = -C^\\top \\Pi_y^{\\mathrm{eff}} (y_t - C\\hat{x}_t) + \\Pi_x (\\hat{x}_t - A\\hat{s}_t)\n$$\nDefining the prediction errors as $e_{y,t} = y_t - C\\hat{x}_t$ and $e_{x,t} = \\hat{x}_t - A\\hat{s}_t$, the gradient is:\n$$\n\\frac{\\partial F}{\\partial \\hat{x}_t} = \\Pi_x e_{x,t} - C^\\top \\Pi_y^{\\mathrm{eff}} e_{y,t}\n$$\nThe update rule for $\\hat{x}_t$ is therefore:\n$$\n\\hat{x}_{t+1} = \\hat{x}_t - \\alpha_x (\\Pi_x e_{x,t} - C^\\top \\Pi_y^{\\mathrm{eff}} e_{y,t}) = \\hat{x}_t + \\alpha_x (C^\\top \\Pi_y^{\\mathrm{eff}} e_{y,t} - \\Pi_x e_{x,t})\n$$\nThis update adjusts $\\hat{x}_t$ to reduce the error it causes at the sensory level (term with $e_{y,t}$) and the error it represents with respect to the prediction from the higher level (term with $e_{x,t}$).\n\nNext, we compute the partial derivative with respect to $\\hat{s}_t$:\n$$\n\\frac{\\partial F}{\\partial \\hat{s}_t} = \\frac{\\partial}{\\partial \\hat{s}_t} \\left[ \\frac{1}{2} (\\hat{x}_t - A\\hat{s}_t)^\\top \\Pi_x (\\hat{x}_t - A\\hat{s}_t) + \\frac{1}{2} (\\hat{s}_t - \\mu_s^0)^\\top \\Pi_s (\\hat{s}_t - \\mu_s^0) \\right]\n$$\nThis yields:\n$$\n\\frac{\\partial F}{\\partial \\hat{s}_t} = -A^\\top \\Pi_x (\\hat{x}_t - A\\hat{s}_t) + \\Pi_s (\\hat{s}_t - \\mu_s^0)\n$$\nDefining the prior deviation as $e_{s,t} = \\hat{s}_t - \\mu_s^0$, the gradient is:\n$$\n\\frac{\\partial F}{\\partial \\hat{s}_t} = \\Pi_s e_{s,t} - A^\\top \\Pi_x e_{x,t}\n$$\nThe update rule for $\\hat{s}_t$ is:\n$$\n\\hat{s}_{t+1} = \\hat{s}_t - \\alpha_s (\\Pi_s e_{s,t} - A^\\top \\Pi_x e_{x,t}) = \\hat{s}_t + \\alpha_s (A^\\top \\Pi_x e_{x,t} - \\Pi_s e_{s,t})\n$$\nThis update adjusts $\\hat{s}_t$ to better predict the state below (term with $e_{x,t}$) while being regularized by its prior (term with $e_{s,t}$). These derived rules align with the canonical form of predictive coding updates.\n\n### 2. Simulation and Conscious Content Emergence Criterion (CCEC)\n\nThe simulation evolves the internal estimates $\\hat{x}_t$ and $\\hat{s}_t$ over $T=80$ time steps. At each step, a new observation $y_t$ is generated from the model with a fixed underlying cause $s^\\star$. The CCEC is then evaluated.\n\nThe CCEC requires four conditions to be met simultaneously for at least $L=5$ consecutive time steps. The conditions for a given time step $t$ are:\n1.  **Low Sensory Prediction Error:** $\\| \\Pi_y^{\\mathrm{eff}} e_{y,t} \\|_2 \\le \\varepsilon_{\\mathrm{low}}$. This measures if the model's sensory predictions are accurate, weighted by the effective sensory precision. Here, $\\varepsilon_{\\mathrm{low}}=0.2$.\n2.  **Low Hierarchical Prediction Error:** $\\| \\Pi_x e_{x,t} \\|_2 \\le \\varepsilon_{\\mathrm{low}}$. This measures if the mid-level representation is well-explained by the top-level cause, weighted by the mid-level precision.\n3.  **Accurate Top-Level Estimate:** $\\| \\hat{s}_t - s^\\star \\|_2 \\le \\delta_s$. This checks if the model's highest-level estimate has converged to the true underlying cause of the sensations. Here, $\\delta_s=0.25$.\n4.  **High Posterior Precision (Low Entropy):** $H_s \\le H_{\\mathrm{thresh}}$. Posterior precision reflects the confidence in the top-level estimate. A more precise posterior corresponds to lower entropy. The differential entropy $H_s$ of the Gaussian posterior over $s$ is calculated. Here, $H_{\\mathrm{thresh}}=0.5$.\n\nThe posterior precision, under the Laplace approximation, is $\\Lambda_s = \\Pi_s + A^\\top \\Pi_x A$. The posterior covariance is $\\Sigma_s^{\\mathrm{post}} = \\Lambda_s^{-1}$. For this problem, where $n=2$, $A=I_2$, and precisions are scalar multiples of the identity matrix ($\\Pi_s = \\frac{1}{\\sigma_s^2}I_2$, $\\Pi_x = \\frac{1}{\\sigma_x^2}I_2$), the entropy is constant for each test case:\n$$\n\\Lambda_s = \\left(\\frac{1}{\\sigma_s^2} + \\frac{1}{\\sigma_x^2}\\right) I_2\n$$\n$$\n\\det(\\Sigma_s^{\\mathrm{post}}) = \\det(\\Lambda_s^{-1}) = \\left(\\left(\\frac{1}{\\sigma_s^2} + \\frac{1}{\\sigma_x^2}\\right)^{-1}\\right)^2\n$$\n$$\nH_s = \\frac{1}{2} \\ln\\left((2\\pi e)^n \\det(\\Sigma_s^{\\mathrm{post}})\\right) = \\ln\\left(2\\pi e \\left(\\frac{1}{\\sigma_s^2} + \\frac{1}{\\sigma_x^2}\\right)^{-1}\\right) = 1 + \\ln(2\\pi) - \\ln\\left(\\frac{1}{\\sigma_s^2} + \\frac{1}{\\sigma_x^2}\\right)\n$$\nThis condition acts as a static filter. For Case 2, with $\\sigma_x=1.0$ and $\\sigma_s=1.0$, $H_s = 1 + \\ln(2\\pi) - \\ln(2) \\approx 2.14 > 0.5$. Thus, the CCEC can never be met for Case 2. For all other cases, this condition is satisfied. The simulation then proceeds by applying the derived update rules and checking the remaining three conditions dynamically.",
            "answer": "```python\n# The final, self-contained, and runnable Python code.\nimport numpy as np\n\ndef run_simulation(params):\n    \"\"\"\n    Runs a single simulation for a given set of parameters.\n\n    Args:\n        params (tuple): A tuple containing the parameters for the simulation:\n                        (mu_s0, sigma_y, sigma_x, sigma_s, gamma, alpha_x, alpha_s).\n\n    Returns:\n        bool: True if conscious content emerges, False otherwise.\n    \"\"\"\n    # Unpack parameters\n    mu_s0, sigma_y, sigma_x, sigma_s, gamma, alpha_x, alpha_s = params\n    mu_s0 = np.array(mu_s0, dtype=np.float64)\n\n    # Common fixed parameters\n    n = 2\n    T = 80\n    L = 5\n    eps_low = 0.2\n    delta_s = 0.25\n    H_thresh = 0.5\n    s_star = np.array([1.0, -1.0], dtype=np.float64)\n    A = np.identity(n, dtype=np.float64)\n    C = np.identity(n, dtype=np.float64)\n\n    # Initial internal estimates\n    s_hat = np.array([0.0, 0.0], dtype=np.float64)\n    x_hat = np.array([0.0, 0.0], dtype=np.float64)\n\n    # Precision matrices (using np.float64 for stability)\n    sigma_y, sigma_x, sigma_s = np.float64(sigma_y), np.float64(sigma_x), np.float64(sigma_s)\n    \n    # Sigmas are squared, so they must be non-zero.\n    if sigma_y == 0.0 or sigma_x == 0.0 or sigma_s == 0.0:\n        return False\n        \n    Pi_y = (1.0 / sigma_y**2) * np.identity(n)\n    Pi_x = (1.0 / sigma_x**2) * np.identity(n)\n    Pi_s = (1.0 / sigma_s**2) * np.identity(n)\n    Pi_y_eff = gamma * Pi_y\n\n    # --- CCEC Condition 4: Entropy ---\n    # This is constant for a given simulation. It can be pre-calculated.\n    pi_s_scalar = 1.0 / sigma_s**2\n    pi_x_scalar = 1.0 / sigma_x**2\n    H_s = (n / 2.0) * (np.log(2.0 * np.pi) + 1.0 - np.log(pi_s_scalar + pi_x_scalar))\n    \n    if H_s > H_thresh:\n        return False\n\n    consecutive_success_count = 0\n\n    # Simulation loop over T time steps\n    for _ in range(T):\n        # 1. Generate new observation y_t from the generative model\n        eps_x = np.random.normal(0, sigma_x, size=n)\n        x_t = A @ s_star + eps_x\n        eps_y = np.random.normal(0, sigma_y, size=n)\n        y_t = C @ x_t + eps_y\n\n        # 2. Compute prediction errors based on current estimates (from previous step)\n        e_y = y_t - C @ x_hat\n        e_x = x_hat - A @ s_hat\n        e_s = s_hat - mu_s0\n\n        # 3. Update internal estimates via gradient descent\n        # Gradients are grad_F_x = Pi_x*e_x - C.T*Pi_y_eff*e_y\n        # and grad_F_s = Pi_s*e_s - A.T*Pi_x*e_x.\n        # Updates are in the negative gradient direction.\n        x_hat_next = x_hat + alpha_x * (C.T @ Pi_y_eff @ e_y - Pi_x @ e_x)\n        s_hat_next = s_hat + alpha_s * (A.T @ Pi_x @ e_x - Pi_s @ e_s)\n        \n        # 4. Check CCEC conditions.\n        # The error criteria use errors computed before the update.\n        # The state criterion uses the state after the update.\n        cond1 = np.linalg.norm(Pi_y_eff @ e_y) = eps_low\n        cond2 = np.linalg.norm(Pi_x @ e_x) = eps_low\n        cond3 = np.linalg.norm(s_hat_next - s_star) = delta_s\n        # cond4 (H_s) is static and was checked before the loop.\n\n        if cond1 and cond2 and cond3:\n            consecutive_success_count += 1\n        else:\n            consecutive_success_count = 0\n        \n        if consecutive_success_count >= L:\n            return True\n\n        # 5. Advance the state for the next time step\n        x_hat = x_hat_next\n        s_hat = s_hat_next\n\n    # If the loop completes without meeting the criterion for L steps\n    return False\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of the results.\n    np.random.seed(0)\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (mu_s^0, sigma_y, sigma_x, sigma_s, gamma, alpha_x, alpha_s).\n    test_cases = [\n        # Case 1 (happy path)\n        ([0.0, 0.0], 0.2, 0.3, 1.0, 1.0, 0.35, 0.25),\n        # Case 2 (low effective sensory precision)\n        ([0.0, 0.0], 1.5, 1.0, 1.0, 0.05, 0.35, 0.25),\n        # Case 3 (weak feedback updating)\n        ([0.0, 0.0], 0.2, 0.3, 1.0, 1.0, 0.35, 0.02),\n        # Case 4 (dominating incorrect prior)\n        ([3.0, 3.0], 0.2, 0.3, 0.1, 1.0, 0.35, 0.25),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}