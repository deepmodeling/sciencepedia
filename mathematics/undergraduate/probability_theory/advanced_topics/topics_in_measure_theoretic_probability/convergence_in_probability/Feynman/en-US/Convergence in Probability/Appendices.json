{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a foundational example of convergence in probability. We will explore a scenario where successive measurements of a physical constant become increasingly precise. This practice will allow you to apply Chebyshev's inequality, a powerful tool for connecting a random variable's variance to the probability of it deviating from its mean, and directly demonstrate how decreasing variance leads to convergence in probability .",
            "id": "1910709",
            "problem": "An engineer is developing a new sensor to measure a specific physical property, whose true value is an unknown constant $c$. The engineer takes a series of measurements. Let $X_n$ be the random variable representing the outcome of the $n$-th measurement for $n=1, 2, 3, \\ldots$. Due to refinements in the experimental setup, the measurements are unbiased, meaning the expected value of any measurement is the true value, i.e., $E[X_n] = c$ for all $n \\geq 1$. The precision of the measurements improves with each attempt, and the variance of the $n$-th measurement is found to be $Var(X_n) = \\frac{\\sigma^2}{n^2}$, where $\\sigma$ is a known positive constant representing a baseline measurement uncertainty.\n\nThe sequence of measurements $\\{X_n\\}$ converges in probability to a specific value. Determine this value.",
            "solution": "We want the limit in probability of $X_{n}$ as $n \\to \\infty$. By definition, $X_{n} \\to c$ in probability if for every $\\varepsilon > 0$,\n$$\n\\lim_{n \\to \\infty} \\Pr\\left(|X_{n} - c| > \\varepsilon\\right) = 0.\n$$\nSince the measurements are unbiased, $E[X_{n}] = c$ for all $n \\geq 1$. By Chebyshev's inequality, for any $\\varepsilon > 0$,\n$$\n\\Pr\\left(|X_{n} - E[X_{n}]| \\geq \\varepsilon\\right) \\leq \\frac{\\operatorname{Var}(X_{n})}{\\varepsilon^{2}}.\n$$\nUsing $\\operatorname{Var}(X_{n}) = \\frac{\\sigma^{2}}{n^{2}}$ and $E[X_{n}] = c$, we get\n$$\n\\Pr\\left(|X_{n} - c| \\geq \\varepsilon\\right) \\leq \\frac{\\sigma^{2}}{n^{2}\\varepsilon^{2}} \\xrightarrow[n \\to \\infty]{} 0.\n$$\nTherefore, by the definition of convergence in probability, $X_{n} \\xrightarrow{P} c$. Equivalently, one may observe that\n$$\nE\\left[(X_{n} - c)^{2}\\right] = \\operatorname{Var}(X_{n}) = \\frac{\\sigma^{2}}{n^{2}} \\to 0,\n$$\nso $X_{n} \\to c$ in mean square, which implies convergence in probability to $c$.",
            "answer": "$$\\boxed{c}$$"
        },
        {
            "introduction": "Having seen a case where an estimator converges, it is equally important to understand when one does not. This problem presents a thought experiment with an intentionally flawed estimator that, despite an increasing amount of available data, fails to improve . By analyzing why this estimator is not consistent, you will gain a deeper appreciation for how an estimator must properly utilize all available information from a sample to converge to the true parameter value.",
            "id": "1910737",
            "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables from a population with a finite mean $E[X_i] = \\mu$ and a finite, non-zero variance $Var(X_i) = \\sigma^2$.\n\nAn investigator proposes an estimator for the population mean $\\mu$, defined as $\\hat{\\mu}_n = X_1$. This estimator uses only the first observation, regardless of the total sample size $n$. We wish to analyze the consistency of this estimator.\n\nWhich of the following statements correctly describes the convergence property of the estimator $\\hat{\\mu}_n$ as the sample size $n$ approaches infinity?\n\nA. $\\hat{\\mu}_n$ converges in probability to $\\mu$ because it is an unbiased estimator of $\\mu$.\n\nB. $\\hat{\\mu}_n$ converges in probability to $\\mu$ since the Law of Large Numbers guarantees convergence for any estimator as the sample size $n$ grows infinitely large.\n\nC. $\\hat{\\mu}_n$ does not converge in probability to $\\mu$ because the probability of the estimator being more than a small distance away from $\\mu$ remains a fixed positive number as $n$ increases.\n\nD. $\\hat{\\mu}_n$ does not converge in probability to $\\mu$ because the estimator is biased, and biased estimators can never be consistent.\n\nE. Whether $\\hat{\\mu}_n$ converges in probability to $\\mu$ cannot be determined without knowing if the underlying distribution of the $X_i$ variables is normal.",
            "solution": "Let $\\{X_{i}\\}_{i=1}^{n}$ be i.i.d. with $E[X_{i}] = \\mu$ and $\\operatorname{Var}(X_{i}) = \\sigma^{2}$ where $\\sigma^{2} \\in (0,\\infty)$. The proposed estimator is $\\hat{\\mu}_{n} = X_{1}$ for all $n$.\n\nBy definition, $\\hat{\\mu}_{n}$ is consistent for $\\mu$ if and only if for every $\\varepsilon > 0$,\n$$\n\\lim_{n \\to \\infty} \\Pr\\!\\left(|\\hat{\\mu}_{n} - \\mu| > \\varepsilon\\right) = 0.\n$$\nSince $\\hat{\\mu}_{n} = X_{1}$ for all $n$, we have for every $\\varepsilon > 0$ and every $n$,\n$$\n\\Pr\\!\\left(|\\hat{\\mu}_{n} - \\mu| > \\varepsilon\\right) = \\Pr\\!\\left(|X_{1} - \\mu| > \\varepsilon\\right) =: p(\\varepsilon),\n$$\nwhich does not depend on $n$.\n\nWe now show $p(\\varepsilon)$ is strictly positive for at least one $\\varepsilon > 0$. Suppose, for contradiction, that $p(\\varepsilon) = 0$ for all $\\varepsilon > 0$. Then for each rational sequence $\\{\\varepsilon_{k}\\}$ with $\\varepsilon_{k} \\downarrow 0$,\n$$\n\\Pr\\!\\left(|X_{1} - \\mu| \\le \\varepsilon_{k}\\right) = 1 \\quad \\text{for all } k,\n$$\nhence\n$$\n\\Pr\\!\\left(\\bigcap_{k=1}^{\\infty} \\{|X_{1} - \\mu| \\le \\varepsilon_{k}\\}\\right) = 1,\n$$\nwhich implies $\\Pr(X_{1} = \\mu) = 1$, and therefore $\\operatorname{Var}(X_{1}) = 0$, contradicting $\\sigma^{2} > 0$. Thus there exists $\\varepsilon_{0} > 0$ such that\n$$\np(\\varepsilon_{0}) = \\Pr\\!\\left(|X_{1} - \\mu| > \\varepsilon_{0}\\right) > 0.\n$$\nConsequently,\n$$\n\\lim_{n \\to \\infty} \\Pr\\!\\left(|\\hat{\\mu}_{n} - \\mu| > \\varepsilon_{0}\\right) = \\lim_{n \\to \\infty} p(\\varepsilon_{0}) = p(\\varepsilon_{0}) > 0,\n$$\nso the consistency condition fails. Therefore $\\hat{\\mu}_{n}$ does not converge in probability to $\\mu$.\n\nOption-by-option assessment:\n- A is false: unbiasedness alone does not imply consistency.\n- B is false: the Law of Large Numbers applies to the sample mean that depends on $n$, not to $X_{1}$ which ignores $n$.\n- C is true: the error probability is a fixed positive constant for some small $\\varepsilon$, independent of $n$.\n- D is false: the estimator is unbiased, and in any case, bias does not preclude consistency in general.\n- E is false: normality is unnecessary; the argument relies only on $\\sigma^{2} > 0$.\n\nThus the correct choice is C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Our final practice explores a more nuanced type of limiting behavior. Not all sequences of random variables that stabilize converge in probability to a constant; some converge to a new, non-degenerate random variable. This exercise  examines the lifetime of a system composed of many components, revealing how a properly scaled sequence can converge *in distribution* to a well-known probability distribution, even when it does not converge in probability. This distinction is fundamental in many areas of applied probability and statistics.",
            "id": "1910714",
            "problem": "A large-scale computing system is constructed using $n$ identical, independent processors. The reliability of the system is a critical concern, and the system is considered to have failed as soon as the first processor fails. The lifetime of each individual processor, measured in thousands of hours, is modeled by a random variable uniformly distributed on the interval $(0, 1)$. Let $T_n$ be the lifetime of the entire system with $n$ processors. To understand the failure characteristics of systems with a very large number of processors, an engineer studies a scaled lifetime, defined as $S_n = n T_n$.\n\nWhich of the following statements provides the most accurate description of the behavior of the scaled lifetime $S_n$ as the number of processors $n$ approaches infinity?\n\nA. The scaled lifetime $S_n$ converges in probability to the constant 0.\n\nB. The scaled lifetime $S_n$ converges in probability to the constant 1.\n\nC. The scaled lifetime $S_n$ converges in probability to a random variable that follows an Exponential distribution with a rate parameter of 1.\n\nD. The scaled lifetime $S_n$ does not converge in probability to any constant, but its distribution converges to an Exponential distribution with a rate parameter of 1.\n\nE. The scaled lifetime $S_n$ does not converge to any well-defined limiting distribution.",
            "solution": "Let $X_{1},\\dots,X_{n}$ be independent and identically distributed random variables with $X_{i}\\sim \\text{Uniform}(0,1)$. The system fails at the first processor failure, so the system lifetime is the minimum $T_{n}=\\min\\{X_{1},\\dots,X_{n}\\}$. For $t\\in(0,1)$, use independence to compute the survival function of $T_{n}$:\n$$\n\\mathbb{P}(T_{n}>t)=\\mathbb{P}(X_{1}>t,\\dots,X_{n}>t)=\\prod_{i=1}^{n}\\mathbb{P}(X_{i}>t)=(1-t)^{n},\n$$\nand $\\mathbb{P}(T_{n}>t)=1$ for $t\\leq 0$, $\\mathbb{P}(T_{n}>t)=0$ for $t\\geq 1$. Define the scaled lifetime $S_{n}=nT_{n}$. For $s\\geq 0$ and $s\\leq n$, the survival function of $S_{n}$ is\n$$\n\\mathbb{P}(S_{n}>s)=\\mathbb{P}(T_{n}>s/n)=(1-s/n)^{n}.\n$$\nFor any fixed $s\\geq 0$, take the limit as $n\\to\\infty$ and use the standard limit $\\lim_{n\\to\\infty}(1-x/n)^{n}=\\exp(-x)$ to obtain\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}(S_{n}>s)=\\exp(-s).\n$$\nEquivalently, the distribution functions satisfy $\\lim_{n\\to\\infty}\\mathbb{P}(S_{n}\\leq s)=1-\\exp(-s)$ for all $s\\geq 0$, which is the cumulative distribution function of an Exponential distribution with rate parameter $1$. Therefore $S_{n}$ converges in distribution to $\\text{Exp}(1)$. Since the limiting distribution is non-degenerate, $S_{n}$ does not converge in probability to any constant; if it did, it would converge in distribution to a degenerate distribution at that constant, which contradicts the established exponential limit. Hence the most accurate description is that $S_{n}$ does not converge in probability to any constant, but its distribution converges to $\\text{Exp}(1)$.",
            "answer": "$$\\boxed{D}$$"
        }
    ]
}