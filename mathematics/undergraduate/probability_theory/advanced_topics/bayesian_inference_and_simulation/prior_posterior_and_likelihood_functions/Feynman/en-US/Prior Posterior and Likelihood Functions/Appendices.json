{
    "hands_on_practices": [
        {
            "introduction": "The core of Bayesian inference is updating our beliefs in light of new evidence. This first practice provides a foundational exercise in this process, demonstrating how to combine a prior belief about a rate parameter with observed data. By working through this epidemiological scenario , you will see how a Gamma prior distribution for a Poisson rate $\\lambda$ elegantly combines with Poisson-distributed data to yield a new, updated Gamma posterior distribution, a property known as conjugacy.",
            "id": "1379681",
            "problem": "An epidemiologist is studying the transmission of a newly identified virus in a specific city. The number of new cases reported each week is assumed to follow a Poisson distribution with an unknown average rate parameter $\\lambda$. Based on preliminary studies of similar viruses in other regions, the epidemiologist's prior belief about $\\lambda$ is modeled by a Gamma distribution, $\\text{Gamma}(\\alpha, \\beta)$. The probability density function for this prior distribution is given by:\n$$p(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda)$$\nfor $\\lambda > 0$, where $\\Gamma(\\alpha)$ is the Gamma function. For this particular virus, the epidemiologist sets the prior parameters to be $\\alpha=3$ and $\\beta=2$.\n\nOver a period of four consecutive weeks, the following numbers of new cases are observed: $k_1=7, k_2=4, k_3=10, k_4=5$. Assuming that the number of cases in each week is an independent event, determine the full posterior probability density function for the rate parameter $\\lambda$ given the observed data. Express your answer as a function of $\\lambda$.",
            "solution": "Let the weekly counts be modeled as independent Poisson random variables given the rate parameter $\\lambda$, so for a single observation $k$ the likelihood is\n$$\np(k \\mid \\lambda)=\\frac{\\lambda^{k}\\exp(-\\lambda)}{k!}.\n$$\nFor $n$ independent weeks with observations $k_{1},\\dots,k_{n}$, the joint likelihood is\n$$\nL(\\lambda \\mid k_{1},\\dots,k_{n})=\\prod_{i=1}^{n}\\frac{\\lambda^{k_{i}}\\exp(-\\lambda)}{k_{i}!}=\\left(\\prod_{i=1}^{n}\\frac{1}{k_{i}!}\\right)\\lambda^{\\sum_{i=1}^{n}k_{i}}\\exp(-n\\lambda).\n$$\nThe prior for $\\lambda$ is $\\text{Gamma}(\\alpha,\\beta)$ with density\n$$\np(\\lambda)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\beta\\lambda),\\quad \\lambda>0.\n$$\nBy Bayes' theorem, the posterior density is proportional to the product of the likelihood and the prior:\n$$\np(\\lambda \\mid k_{1},\\dots,k_{n})\\propto \\lambda^{\\sum_{i=1}^{n}k_{i}}\\exp(-n\\lambda)\\cdot \\lambda^{\\alpha-1}\\exp(-\\beta\\lambda)=\\lambda^{\\alpha-1+\\sum_{i=1}^{n}k_{i}}\\exp\\bigl(-(\\beta+n)\\lambda\\bigr).\n$$\nThis is the kernel of a Gamma distribution with updated parameters\n$$\n\\alpha^{\\prime}=\\alpha+\\sum_{i=1}^{n}k_{i},\\qquad \\beta^{\\prime}=\\beta+n.\n$$\nTherefore, the normalized posterior is\n$$\np(\\lambda \\mid k_{1},\\dots,k_{n})=\\frac{{\\beta^{\\prime}}^{\\alpha^{\\prime}}}{\\Gamma(\\alpha^{\\prime})}\\lambda^{\\alpha^{\\prime}-1}\\exp(-\\beta^{\\prime}\\lambda),\\quad \\lambda>0.\n$$\nFor the given data, $\\alpha=3$, $\\beta=2$, $n=4$, and $\\sum_{i=1}^{4}k_{i}=7+4+10+5=26$, hence\n$$\n\\alpha^{\\prime}=3+26=29,\\qquad \\beta^{\\prime}=2+4=6,\n$$\nso the full posterior density is\n$$\np(\\lambda \\mid k_{1}=7,k_{2}=4,k_{3}=10,k_{4}=5)=\\frac{6^{29}}{\\Gamma(29)}\\lambda^{28}\\exp(-6\\lambda),\\quad \\lambda>0.\n$$",
            "answer": "$$\\boxed{\\frac{6^{29}}{\\Gamma(29)}\\,\\lambda^{28}\\,\\exp(-6\\lambda)}$$"
        },
        {
            "introduction": "Once we have a posterior distribution, we often need a single number to summarize our findings. This practice delves into the comparison between two different ways of generating such a point estimate: the maximum likelihood estimate (MLE) and the Bayesian posterior mean. This exercise  will help clarify the conceptual and quantitative differences between the frequentist approach of maximizing the likelihood and the Bayesian approach of averaging over the posterior distribution, highlighting the subtle but important influence of the prior.",
            "id": "1379693",
            "problem": "A materials scientist is studying a newly synthesized semiconductor material. The probability, denoted by $\\theta$, that an electron injected into the material successfully passes through a potential barrier is unknown. Based on the material's theoretical design, the scientist has no prior preference for any particular value of $\\theta$, and thus assumes a uniform prior probability distribution for $\\theta$ over the interval $[0, 1]$.\n\nIn an experiment, $N$ electrons are independently injected into the material, and it is observed that exactly $k$ of them successfully pass through the barrier, where $0 < k < N$.\n\nTwo methods are proposed to estimate the value of $\\theta$ based on this experimental outcome.\n\n1.  **Method A**: Find the value of $\\theta$ that maximizes the probability of observing the experimental outcome of $k$ successes in $N$ trials. Let this estimate be $\\hat{\\theta}_{A}$.\n2.  **Method B**: First, determine the updated (posterior) probability distribution of $\\theta$ given the experimental data. Then, calculate the expected value of $\\theta$ with respect to this new distribution. Let this estimate be $\\hat{\\theta}_{B}$.\n\nDetermine the difference between these two estimates, $\\hat{\\theta}_{B} - \\hat{\\theta}_{A}$. Express your final answer as a single closed-form expression in terms of $N$ and $k$.",
            "solution": "We model the number of successes $k$ in $N$ independent trials with success probability $\\theta$ as a binomial outcome. The likelihood of observing exactly $k$ successes is\n$$\nL(\\theta)\\propto \\binom{N}{k}\\,\\theta^{k}\\left(1-\\theta\\right)^{N-k}.\n$$\nMethod A asks for the value that maximizes this likelihood. Since $\\binom{N}{k}$ does not depend on $\\theta$, we maximize $f(\\theta)=\\theta^{k}(1-\\theta)^{N-k}$ on $(0,1)$. Taking the derivative of $\\ln f(\\theta)=k\\ln\\theta+(N-k)\\ln(1-\\theta)$ and setting it to zero gives\n$$\n\\frac{d}{d\\theta}\\ln f(\\theta)=\\frac{k}{\\theta}-\\frac{N-k}{1-\\theta}=0\n\\;\\;\\Longrightarrow\\;\\;\nk(1-\\theta)=(N-k)\\theta\n\\;\\;\\Longrightarrow\\;\\;\n\\hat{\\theta}_{A}=\\frac{k}{N},\n$$\nwhich is an interior maximum for $0<k<N$.\n\nFor Method B, with a uniform prior on $[0,1]$, the prior is $\\operatorname{Beta}(1,1)$. Given $k$ successes and $N-k$ failures, the posterior is\n$$\n\\theta\\mid k,N \\sim \\operatorname{Beta}(k+1,\\,N-k+1),\n$$\nsince the posterior density is proportional to $\\theta^{k}(1-\\theta)^{N-k}$. The posterior mean is\n$$\n\\hat{\\theta}_{B}=\\mathbb{E}[\\theta\\mid k,N]=\\frac{k+1}{(k+1)+(N-k+1)}=\\frac{k+1}{N+2}.\n$$\nTherefore, the difference is\n$$\n\\hat{\\theta}_{B}-\\hat{\\theta}_{A}\n=\\frac{k+1}{N+2}-\\frac{k}{N}\n=\\frac{N(k+1)-k(N+2)}{N(N+2)}\n=\\frac{Nk+N-kN-2k}{N(N+2)}\n=\\frac{N-2k}{N(N+2)}.\n$$",
            "answer": "$$\\boxed{\\frac{N-2k}{N(N+2)}}$$"
        },
        {
            "introduction": "Statistical inference is particularly powerful when data is sparse or outcomes are extreme. This final practice explores the challenging scenario where zero successes are observed in a series of trials, a situation that requires a careful and nuanced approach to estimation. You will learn how Bayesian methods, by incorporating a prior belief, can provide a reasonable and non-trivial estimate for the underlying success probability , and you will practice calculating the posterior median, a robust summary of our updated belief that is particularly useful for skewed posterior distributions.",
            "id": "1923968",
            "problem": "A new, high-risk surgical procedure is being developed for a condition that was previously considered untreatable. The true probability of success for this procedure, denoted by $p$, is unknown. Before conducting any clinical trials, medical researchers hold no preconceived bias about this probability, and thus consider every possible value of $p$ in the interval $[0, 1]$ to be equally likely.\n\nThe procedure is then attempted on $n$ different patients. In every one of these $n$ independent trials, the procedure fails to achieve a successful outcome.\n\nBased on this observed data of zero successes in $n$ trials, an updated belief about the success probability $p$ is formed. A medical statistician decides to summarize this updated belief by calculating a central value, $p_{med}$. This value is defined such that, given the observed data, it is equally probable for the true success rate $p$ to be less than or equal to $p_{med}$ as it is for it to be greater than $p_{med}$.\n\nFind a closed-form analytic expression for $p_{med}$ in terms of $n$.",
            "solution": "Let $p$ denote the success probability. The prior belief is uniform on $[0,1]$, which is the $\\operatorname{Beta}(1,1)$ prior with density $\\pi(p)=1$ for $p\\in[0,1]$.\n\nWe observe $n$ independent Bernoulli trials with $k=0$ successes. The likelihood is\n$$\nL(p\\mid \\text{data}) \\propto p^{k}(1-p)^{n-k}=(1-p)^{n}.\n$$\nBy Bayesâ€™ theorem, the posterior is proportional to prior times likelihood:\n$$\n\\pi(p\\mid \\text{data}) \\propto \\pi(p)L(p\\mid \\text{data}) \\propto (1-p)^{n}, \\quad 0\\leq p\\leq 1.\n$$\nThis is a $\\operatorname{Beta}(1,n+1)$ distribution. Normalizing explicitly,\n$$\n\\int_{0}^{1} (1-p)^{n}\\,dp=\\frac{1}{n+1},\n$$\nso the posterior density is\n$$\nf(p\\mid \\text{data})=(n+1)(1-p)^{n}, \\quad 0\\leq p\\leq 1.\n$$\nThe posterior cumulative distribution function is\n$$\nF(p)=\\int_{0}^{p} (n+1)(1-t)^{n}\\,dt.\n$$\nCompute the integral via $u=1-t$, $du=-dt$:\n$$\nF(p)=(n+1)\\int_{0}^{p} (1-t)^{n}\\,dt=(n+1)\\int_{1-p}^{1} u^{n}\\,du\n= (n+1)\\left[\\frac{u^{n+1}}{n+1}\\right]_{1-p}^{1}\n=1-(1-p)^{n+1}.\n$$\nThe posterior median $p_{med}$ satisfies $F(p_{med})=\\frac{1}{2}$, hence\n$$\n1-(1-p_{med})^{n+1}=\\frac{1}{2}\n\\;\\;\\Longrightarrow\\;\\;\n(1-p_{med})^{n+1}=\\frac{1}{2}.\n$$\nTaking the $(n+1)$-th root gives\n$$\n1-p_{med}=2^{-\\frac{1}{n+1}},\n$$\nso\n$$\np_{med}=1-2^{-\\frac{1}{n+1}}.\n$$\nThis expression is valid for all $n\\geq 0$ and, in particular, for $n\\geq 1$ as in the problem context.",
            "answer": "$$\\boxed{1-2^{-\\frac{1}{n+1}}}$$"
        }
    ]
}