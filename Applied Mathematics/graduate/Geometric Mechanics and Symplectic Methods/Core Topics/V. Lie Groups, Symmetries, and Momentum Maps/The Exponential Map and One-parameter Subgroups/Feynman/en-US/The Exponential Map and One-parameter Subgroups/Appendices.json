{
    "hands_on_practices": [
        {
            "introduction": "The exponential map provides a canonical bridge from a Lie algebra to its corresponding Lie group. This first practice demonstrates the most fundamental method for computing the matrix exponential: diagonalization. By leveraging the spectral properties of a matrix, you will see that exponentiating a diagonalizable matrix is equivalent to exponentiating its eigenvalues, a process that dramatically simplifies calculation and deepens your understanding of the link between the spectra of algebra and group elements .",
            "id": "3775878",
            "problem": "Let $G$ be a real matrix Lie group and let its Lie algebra be realized as a subalgebra of the general linear Lie algebra $\\mathfrak{gl}(n,\\mathbb{R})$. The exponential map $\\exp:\\mathfrak{gl}(n,\\mathbb{R})\\to \\mathrm{GL}(n,\\mathbb{R})$ is defined by the convergent power series $\\exp(A)=\\sum_{k=0}^{\\infty}\\frac{A^{k}}{k!}$. A one-parameter subgroup of $G$ is a smooth homomorphism $\\gamma:\\mathbb{R}\\to G$ satisfying $\\gamma(t+s)=\\gamma(t)\\gamma(s)$ and $\\gamma(0)=I$, and any $A\\in\\mathfrak{gl}(n,\\mathbb{R})$ generates a one-parameter subgroup via $t\\mapsto \\exp(tA)$.\n\nStarting only from these definitions and standard spectral properties of diagonalizable matrices, do the following:\n\n1. Prove that if $A\\in\\mathfrak{gl}(n,\\mathbb{R})$ is diagonalizable over $\\mathbb{C}$ with eigenvalues $\\lambda_{1},\\dots,\\lambda_{n}$, then $\\exp(A)$ is diagonalizable over $\\mathbb{C}$ with eigenvalues $\\exp(\\lambda_{1}),\\dots,\\exp(\\lambda_{n})$.\n\n2. Consider the special linear Lie algebra $\\mathfrak{sl}(2,\\mathbb{R})=\\{A\\in\\mathfrak{gl}(2,\\mathbb{R})\\mid \\mathrm{tr}(A)=0\\}$. Let\n$$\nA=\\begin{pmatrix}\n3 & 4 \\\\\n2 & -3\n\\end{pmatrix}\\in \\mathfrak{sl}(2,\\mathbb{R}).\n$$\nUsing only the above foundational definitions and properties, compute the matrix $\\exp(A)$ explicitly, simplifying your final expression as much as possible. Express the answer as a single $2\\times 2$ matrix whose entries are closed-form analytic expressions. No numerical approximation is required.",
            "solution": "This problem consists of two parts. First, a proof regarding the eigenvalues of the exponential of a diagonalizable matrix, and second, an explicit calculation of the exponential of a specific $2 \\times 2$ matrix.\n\n**Part 1: Proof**\n\nWe are asked to prove that if a matrix $A \\in \\mathfrak{gl}(n,\\mathbb{R})$ is diagonalizable over $\\mathbb{C}$ with eigenvalues $\\lambda_{1}, \\dots, \\lambda_{n}$, then $\\exp(A)$ is also diagonalizable over $\\mathbb{C}$ with eigenvalues $\\exp(\\lambda_{1}), \\dots, \\exp(\\lambda_{n})$.\n\nThe hypothesis that $A$ is diagonalizable over $\\mathbb{C}$ means there exists an invertible matrix $P \\in \\mathrm{GL}(n,\\mathbb{C})$ and a diagonal matrix $D \\in \\mathfrak{gl}(n,\\mathbb{C})$ such that\n$$A = PDP^{-1}$$\nThe columns of $P$ are the eigenvectors of $A$, and the diagonal entries of $D$ are the corresponding eigenvalues $\\lambda_{1}, \\dots, \\lambda_{n}$. So, $D = \\mathrm{diag}(\\lambda_{1}, \\dots, \\lambda_{n})$.\n\nWe use the definition of the matrix exponential, given as the power series:\n$$\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{A^{k}}{k!}$$\nFirst, let us examine the powers of $A$. For any integer $k \\ge 0$, we have:\n$$A^k = (PDP^{-1})^k = \\underbrace{(PDP^{-1})(PDP^{-1})\\dots(PDP^{-1})}_{k \\text{ times}}$$\nThe intermediate $P^{-1}P$ terms cancel out, leaving:\n$$A^k = PD^kP^{-1}$$\nThis can be formally proven by induction. The base case $k=1$ is trivial. Assuming $A^m = PD^mP^{-1}$, we have $A^{m+1} = A^m A = (PD^mP^{-1})(PDP^{-1}) = PD^m(P^{-1}P)DP^{-1} = PD^mIDP^{-1} = PD^{m+1}P^{-1}$.\n\nSubstituting this expression for $A^k$ into the power series for $\\exp(A)$:\n$$\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{PD^kP^{-1}}{k!}$$\nSince matrix multiplication is distributive over addition, and $P$ and $P^{-1}$ are constant with respect to the summation index $k$, we can factor them out of the sum:\n$$\\exp(A) = P \\left( \\sum_{k=0}^{\\infty} \\frac{D^k}{k!} \\right) P^{-1}$$\nThe expression in the parentheses is, by definition, the exponential of the matrix $D$, so we have:\n$$\\exp(A) = P \\exp(D) P^{-1}$$\nNow, we must compute $\\exp(D)$. Since $D$ is a diagonal matrix, $D = \\mathrm{diag}(\\lambda_{1}, \\dots, \\lambda_{n})$, its powers are simply the diagonal matrices of the powers of its entries:\n$$D^k = \\mathrm{diag}(\\lambda_{1}^k, \\dots, \\lambda_{n}^k)$$\nSubstituting this into the series for $\\exp(D)$:\n$$\n\\exp(D) = \\sum_{k=0}^{\\infty} \\frac{1}{k!} \\mathrm{diag}(\\lambda_{1}^k, \\dots, \\lambda_{n}^k)\n= \\mathrm{diag}\\left(\\sum_{k=0}^{\\infty} \\frac{\\lambda_{1}^k}{k!}, \\dots, \\sum_{k=0}^{\\infty} \\frac{\\lambda_{n}^k}{k!}\\right)\n$$\nEach sum on the diagonal is the Taylor series for the scalar exponential function. Therefore,\n$$\\exp(D) = \\mathrm{diag}(\\exp(\\lambda_{1}), \\dots, \\exp(\\lambda_{n}))$$\nWe have shown that $\\exp(A) = P \\exp(D) P^{-1}$, where $\\exp(D)$ is a diagonal matrix. This is precisely the definition of a diagonalizable matrix. Thus, $\\exp(A)$ is diagonalizable with the same change-of-basis matrix $P$ as $A$.\n\nThe eigenvalues of a matrix that has been diagonalized are the diagonal entries of the resulting diagonal matrix. In this case, the eigenvalues of $\\exp(A)$ are the diagonal entries of $\\exp(D)$, which are $\\exp(\\lambda_{1}), \\dots, \\exp(\\lambda_{n})$. This completes the proof.\n\n**Part 2: Calculation**\n\nWe are asked to compute $\\exp(A)$ for the matrix $A = \\begin{pmatrix} 3 & 4 \\\\ 2 & -3 \\end{pmatrix}$.\nThe trace of $A$ is $\\mathrm{tr}(A) = 3 + (-3) = 0$, so $A \\in \\mathfrak{sl}(2,\\mathbb{R})$ as stated.\nFollowing the result from Part 1, we will diagonalize the matrix $A$. First, we find the eigenvalues by solving the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix} 3-\\lambda & 4 \\\\ 2 & -3-\\lambda \\end{pmatrix} = (3-\\lambda)(-3-\\lambda) - (4)(2) = 0\n$$\n$$\n-(\\lambda-3)(\\lambda+3) - 8 = 0\n$$\n$$\n-(\\lambda^2 - 9) - 8 = 0\n$$\n$$\n\\lambda^2 - 9 - 8 = 0\n$$\n$$\n\\lambda^2 - 17 = 0\n$$\nThe eigenvalues are $\\lambda_1 = \\sqrt{17}$ and $\\lambda_2 = -\\sqrt{17}$. Since they are distinct, $A$ is diagonalizable.\n\nNext, we find the corresponding eigenvectors.\nFor $\\lambda_1 = \\sqrt{17}$:\n$$\n(A-\\sqrt{17}I)v_1 = \\begin{pmatrix} 3-\\sqrt{17} & 4 \\\\ 2 & -3-\\sqrt{17} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, we have $(3-\\sqrt{17})x + 4y = 0$. A possible non-zero solution is $x=4$, which gives $y = -(3-\\sqrt{17}) = \\sqrt{17}-3$.\nSo, an eigenvector is $v_1 = \\begin{pmatrix} 4 \\\\ \\sqrt{17}-3 \\end{pmatrix}$.\n\nFor $\\lambda_2 = -\\sqrt{17}$:\n$$\n(A+\\sqrt{17}I)v_2 = \\begin{pmatrix} 3+\\sqrt{17} & 4 \\\\ 2 & -3+\\sqrt{17} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, $(3+\\sqrt{17})x + 4y = 0$. A possible non-zero solution is $x=4$, which gives $y = -(3+\\sqrt{17})$.\nSo, an eigenvector is $v_2 = \\begin{pmatrix} 4 \\\\ -3-\\sqrt{17} \\end{pmatrix}$.\n\nWe construct the change-of-basis matrix $P$ from the eigenvectors and the diagonal matrix $D$ from the eigenvalues:\n$$\nP = \\begin{pmatrix} 4 & 4 \\\\ \\sqrt{17}-3 & -3-\\sqrt{17} \\end{pmatrix}, \\quad D = \\begin{pmatrix} \\sqrt{17} & 0 \\\\ 0 & -\\sqrt{17} \\end{pmatrix}\n$$\nWe need the inverse of $P$:\n$$\n\\det(P) = 4(-3-\\sqrt{17}) - 4(\\sqrt{17}-3) = -12 - 4\\sqrt{17} - 4\\sqrt{17} + 12 = -8\\sqrt{17}\n$$\n$$\nP^{-1} = \\frac{1}{-8\\sqrt{17}}\\begin{pmatrix} -3-\\sqrt{17} & -4 \\\\ -(\\sqrt{17}-3) & 4 \\end{pmatrix} = \\frac{1}{8\\sqrt{17}}\\begin{pmatrix} 3+\\sqrt{17} & 4 \\\\ \\sqrt{17}-3 & -4 \\end{pmatrix}\n$$\nAccording to Part 1, $\\exp(A) = P\\exp(D)P^{-1}$. The matrix $\\exp(D)$ is:\n$$\n\\exp(D) = \\begin{pmatrix} \\exp(\\sqrt{17}) & 0 \\\\ 0 & \\exp(-\\sqrt{17}) \\end{pmatrix}\n$$\nNow we perform the matrix multiplication:\n$$\n\\exp(A) = \\frac{1}{8\\sqrt{17}} \\begin{pmatrix} 4 & 4 \\\\ \\sqrt{17}-3 & -3-\\sqrt{17} \\end{pmatrix} \\begin{pmatrix} \\exp(\\sqrt{17}) & 0 \\\\ 0 & \\exp(-\\sqrt{17}) \\end{pmatrix} \\begin{pmatrix} 3+\\sqrt{17} & 4 \\\\ \\sqrt{17}-3 & -4 \\end{pmatrix}\n$$\nFirst, $P\\exp(D)$:\n$$\nP\\exp(D) = \\begin{pmatrix} 4\\exp(\\sqrt{17}) & 4\\exp(-\\sqrt{17}) \\\\ (\\sqrt{17}-3)\\exp(\\sqrt{17}) & (-3-\\sqrt{17})\\exp(-\\sqrt{17}) \\end{pmatrix}\n$$\nNow we multiply this by $8\\sqrt{17} P^{-1}$:\n$$ \\begin{pmatrix} 4e^{\\sqrt{17}} & 4e^{-\\sqrt{17}} \\\\ (\\sqrt{17}-3)e^{\\sqrt{17}} & (-3-\\sqrt{17})e^{-\\sqrt{17}} \\end{pmatrix} \\begin{pmatrix} 3+\\sqrt{17} & 4 \\\\ \\sqrt{17}-3 & -4 \\end{pmatrix} $$\nThe (1,1) entry is:\n$4\\exp(\\sqrt{17})(3+\\sqrt{17}) + 4\\exp(-\\sqrt{17})(\\sqrt{17}-3) = 12(\\exp(\\sqrt{17})-\\exp(-\\sqrt{17})) + 4\\sqrt{17}(\\exp(\\sqrt{17})+\\exp(-\\sqrt{17}))$.\nThe (1,2) entry is:\n$4\\exp(\\sqrt{17})(4) + 4\\exp(-\\sqrt{17})(-4) = 16(\\exp(\\sqrt{17})-\\exp(-\\sqrt{17}))$.\nThe (2,1) entry is:\n$(\\sqrt{17}-3)\\exp(\\sqrt{17})(3+\\sqrt{17}) + (-3-\\sqrt{17})\\exp(-\\sqrt{17})(\\sqrt{17}-3) = (17-9)\\exp(\\sqrt{17}) - (17-9)\\exp(-\\sqrt{17}) = 8(\\exp(\\sqrt{17})-\\exp(-\\sqrt{17}))$.\nThe (2,2) entry is:\n$(\\sqrt{17}-3)\\exp(\\sqrt{17})(4) + (-3-\\sqrt{17})\\exp(-\\sqrt{17})(-4) = -12(\\exp(\\sqrt{17})-\\exp(-\\sqrt{17})) + 4\\sqrt{17}(\\exp(\\sqrt{17})+\\exp(-\\sqrt{17}))$.\n\nTo simplify, we use the definitions of hyperbolic functions: $\\cosh(x) = \\frac{\\exp(x)+\\exp(-x)}{2}$ and $\\sinh(x) = \\frac{\\exp(x)-\\exp(-x)}{2}$.\nLet $\\theta = \\sqrt{17}$. Then $\\exp(\\theta)+\\exp(-\\theta) = 2\\cosh(\\theta)$ and $\\exp(\\theta)-\\exp(-\\theta) = 2\\sinh(\\theta)$.\n\nDividing each entry by $8\\sqrt{17}$ and simplifying:\nThe entries of $\\exp(A)$ are:\n(1,1): $\\frac{24\\sinh\\theta+8\\theta\\cosh\\theta}{8\\theta} = \\frac{3}{\\theta}\\sinh\\theta + \\cosh\\theta$.\n(1,2): $\\frac{32\\sinh\\theta}{8\\theta} = \\frac{4}{\\theta}\\sinh\\theta$.\n(2,1): $\\frac{16\\sinh\\theta}{8\\theta} = \\frac{2}{\\theta}\\sinh\\theta$.\n(2,2): $\\frac{-24\\sinh\\theta+8\\theta\\cosh\\theta}{8\\theta} = -\\frac{3}{\\theta}\\sinh\\theta + \\cosh\\theta$.\n\nSubstituting $\\theta = \\sqrt{17}$, we get the final matrix:\n$$\n\\exp(A) = \\begin{pmatrix}\n\\cosh(\\sqrt{17}) + \\frac{3}{\\sqrt{17}}\\sinh(\\sqrt{17}) & \\frac{4}{\\sqrt{17}}\\sinh(\\sqrt{17}) \\\\\n\\frac{2}{\\sqrt{17}}\\sinh(\\sqrt{17}) & \\cosh(\\sqrt{17}) - \\frac{3}{\\sqrt{17}}\\sinh(\\sqrt{17})\n\\end{pmatrix}\n$$\nThis can also be expressed as $\\exp(A) = \\cosh(\\sqrt{17})I + \\frac{\\sinh(\\sqrt{17})}{\\sqrt{17}}A$, which is a general result for the exponential of a traceless $2 \\times 2$ matrix $A$ where $-\\det(A) > 0$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix}\n\\cosh(\\sqrt{17}) + \\frac{3}{\\sqrt{17}}\\sinh(\\sqrt{17}) & \\frac{4}{\\sqrt{17}}\\sinh(\\sqrt{17}) \\\\\n\\frac{2}{\\sqrt{17}}\\sinh(\\sqrt{17}) & \\cosh(\\sqrt{17}) - \\frac{3}{\\sqrt{17}}\\sinh(\\sqrt{17})\n\\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While diagonalization is powerful, many important Lie algebras contain elements that are not diagonalizable, such as nilpotent matrices. This exercise explores this crucial alternative scenario, where the infinite series defining the exponential map conveniently truncates to a finite polynomial. You will compute the exponential of a nilpotent element within the symplectic Lie algebra $\\mathfrak{sp}(6,\\mathbb{R})$, a setting of direct relevance to Hamiltonian mechanics, and see how nilpotency makes the calculation exact and elegant .",
            "id": "3775875",
            "problem": "Consider the real symplectic vector space $\\mathbb{R}^{6}$ equipped with the standard symplectic form whose matrix is $J \\in \\mathbb{R}^{6 \\times 6}$ given by\n$$\nJ \\;=\\; \\begin{pmatrix}\n0 & I_{3} \\\\\n- I_{3} & 0\n\\end{pmatrix},\n$$\nwhere $I_{3}$ is the $3 \\times 3$ identity matrix. The symplectic Lie algebra $\\mathfrak{sp}(6,\\mathbb{R})$ consists of all matrices $N \\in \\mathbb{R}^{6 \\times 6}$ satisfying $N^{\\top} J + J N = 0$. Define\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3},\n\\qquad\nN \\;=\\; \\begin{pmatrix}\nA & 0 \\\\\n0 & -A^{\\top}\n\\end{pmatrix} \\in \\mathbb{R}^{6 \\times 6}.\n$$\nIt is known in geometric mechanics that such $N$ is Hamiltonian, i.e., $N = J S$ for a symmetric $S \\in \\mathbb{R}^{6 \\times 6}$, and therefore generates a linear Hamiltonian flow and a one-parameter subgroup in the Symplectic Group (Sp). Starting from first principles—namely, the definition of the matrix exponential\n$$\n\\exp(X) \\;=\\; \\sum_{k=0}^{\\infty} \\frac{1}{k!} X^{k},\n$$\nand the property that the flow of a linear Hamiltonian system preserves the symplectic form—derive a closed-form expression for $\\exp(t N)$ as a finite polynomial in $N$. Your derivation must establish why and how the infinite series truncates using the nilpotency of $A$, without appealing to any memorized shortcut formulas. Express your final result in terms of $t$ and $N$ only, in exact symbolic form. No rounding is required, and no units are involved. Provide the single final expression for $\\exp(t N)$ as your answer.",
            "solution": "The problem asks for a closed-form expression for the matrix exponential $\\exp(tN)$ for a given matrix $N \\in \\mathbb{R}^{6 \\times 6}$. The derivation must proceed from the first principle, which is the definition of the matrix exponential as an infinite series, and explicitly demonstrate how the series truncates.\n\nThe definition of the matrix exponential for a matrix $X$ is given by the power series:\n$$\n\\exp(X) = \\sum_{k=0}^{\\infty} \\frac{1}{k!} X^{k} = I + X + \\frac{1}{2!}X^2 + \\frac{1}{3!}X^3 + \\dots\n$$\nIn our case, we need to compute $\\exp(tN)$, where $t$ is a scalar parameter. Applying the definition, we have:\n$$\n\\exp(tN) = \\sum_{k=0}^{\\infty} \\frac{1}{k!} (tN)^{k} = \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} N^{k}\n$$\nThis is an infinite series. To find a closed-form expression as a finite polynomial, we must investigate the powers of the matrix $N$.\n\nThe matrix $N$ is given as a block-diagonal matrix:\n$$\nN = \\begin{pmatrix} A & 0 \\\\ 0 & -A^{\\top} \\end{pmatrix}\n$$\nwhere $A \\in \\mathbb{R}^{3 \\times 3}$, $A = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}$, and $0$ represents the $3 \\times 3$ zero matrix.\n\nThe powers of a block-diagonal matrix are computed by taking the powers of the individual blocks. For any integer $k \\ge 0$, the $k$-th power of $N$ is:\n$$\nN^k = \\begin{pmatrix} A & 0 \\\\ 0 & -A^{\\top} \\end{pmatrix}^k = \\begin{pmatrix} A^k & 0 \\\\ 0 & (-A^{\\top})^k \\end{pmatrix} = \\begin{pmatrix} A^k & 0 \\\\ 0 & (-1)^k (A^{\\top})^k \\end{pmatrix}\n$$\nThe problem statement directs us to use the nilpotency of $A$. A matrix $M$ is nilpotent if there exists a positive integer $p$ such that $M^p = 0$. Let us compute the powers of $A$:\n$$\nA^1 = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\nA^2 = A \\cdot A = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\nA^3 = A^2 \\cdot A = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = 0_{3 \\times 3}\n$$\nSince $A^3$ is the zero matrix, $A$ is nilpotent with an index of $3$. Consequently, all higher powers of $A$ are also zero: $A^k = 0_{3 \\times 3}$ for all integers $k \\ge 3$.\n\nNow we can determine the powers of $N$. For $k \\ge 3$, the block $A^k$ in the expression for $N^k$ becomes the zero matrix. Since $(A^{\\top})^k = (A^k)^{\\top}$, we have $(A^{\\top})^k = 0_{3 \\times 3}$ for $k \\ge 3$ as well.\nLet's compute the first few powers of $N$:\n$N^0 = I_6$, the $6 \\times 6$ identity matrix.\n$N^1 = N$.\n$N^2 = \\begin{pmatrix} A^2 & 0 \\\\ 0 & (A^{\\top})^2 \\end{pmatrix}$.\nFor $k=3$, we have:\n$$\nN^3 = \\begin{pmatrix} A^3 & 0 \\\\ 0 & (-1)^3 (A^{\\top})^3 \\end{pmatrix} = \\begin{pmatrix} 0_{3 \\times 3} & 0 \\\\ 0 & - (A^3)^{\\top} \\end{pmatrix} = \\begin{pmatrix} 0_{3 \\times 3} & 0 \\\\ 0 & 0_{3 \\times 3} \\end{pmatrix} = 0_{6 \\times 6}\n$$\nThus, $N$ is also a nilpotent matrix with an index of $3$. This means that $N^k = 0_{6 \\times 6}$ for all integers $k \\ge 3$.\n\nThis nilpotency property causes the infinite series for $\\exp(tN)$ to truncate. We can now write out the series expansion:\n$$\n\\exp(tN) = \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} N^{k} = \\frac{t^0}{0!}N^0 + \\frac{t^1}{1!}N^1 + \\frac{t^2}{2!}N^2 + \\frac{t^3}{3!}N^3 + \\frac{t^4}{4!}N^4 + \\dots\n$$\nSince $N^3=0$, all subsequent terms $N^k$ for $k > 3$ are also zero matrices, because $N^k = N^{k-3}N^3 = N^{k-3} \\cdot 0 = 0$. Therefore, all terms in the series from $k=3$ onwards are zero.\nThe expression simplifies to a finite sum:\n$$\n\\exp(tN) = \\frac{1}{1}I_6 + \\frac{t}{1}N + \\frac{t^2}{2}N^2 + 0 + 0 + \\dots\n$$\nHere, we have used the conventions $0!=1$, $t^0=1$, and $N^0=I_6$.\n\nThe final closed-form expression for $\\exp(tN)$ is a finite polynomial in $N$ with coefficients depending on $t$:\n$$\n\\exp(tN) = I_6 + tN + \\frac{t^2}{2}N^2\n$$\nThis derivation fulfills all requirements of the problem, starting from first principles and showing the truncation due to nilpotency. The result is expressed solely in terms of $t$ and $N$ (and the identity matrix $I_6$).",
            "answer": "$$\n\\boxed{I_{6} + tN + \\frac{t^2}{2} N^2}\n$$"
        },
        {
            "introduction": "Beyond the algebraic power series, the exponential map has a profound geometric interpretation as the time-one flow of a left-invariant vector field. This exercise solidifies that connection by focusing on the Heisenberg group, a cornerstone example in non-abelian Lie theory. You will first derive the exponential map by integrating the differential equations of motion on the group manifold and then verify your result by directly computing the matrix power series, which relies on the nilpotency of the algebra .",
            "id": "3775848",
            "problem": "Consider the real Heisenberg group $\\mathsf{H}$ realized as the set of $3 \\times 3$ upper triangular matrices with real entries and ones on the diagonal,\n$$\n\\mathsf{H} = \\left\\{ \\begin{pmatrix} 1 & u & w \\\\ 0 & 1 & v \\\\ 0 & 0 & 1 \\end{pmatrix} : u,v,w \\in \\mathbb{R} \\right\\},\n$$\nwith matrix multiplication as the group operation. Its Lie algebra $\\mathfrak{h}$ consists of strictly upper triangular $3 \\times 3$ real matrices,\n$$\n\\mathfrak{h} = \\left\\{ \\begin{pmatrix} 0 & x & z \\\\ 0 & 0 & y \\\\ 0 & 0 & 0 \\end{pmatrix} : x,y,z \\in \\mathbb{R} \\right\\}.\n$$\nLet the basis $\\{E_{1},E_{2},E_{3}\\} \\subset \\mathfrak{h}$ be given by\n$$\nE_{1} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}, \\quad\nE_{2} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}, \\quad\nE_{3} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix},\n$$\nso that the only nontrivial Lie bracket is $[E_{1},E_{2}] = E_{3}$.\n\nLet $\\xi \\in \\mathfrak{h}$ be the Lie algebra element\n$$\n\\xi = x E_{1} + y E_{2} + z E_{3},\n$$\nwith $x,y,z \\in \\mathbb{R}$. Denote by $\\exp: \\mathfrak{h} \\to \\mathsf{H}$ the Lie group exponential map. In geometric mechanics, one-parameter subgroups are obtained by integrating left-invariant vector fields generated by Lie algebra elements. Use the following foundational facts:\n- For a Lie group, the left translation by $g \\in \\mathsf{H}$ is $L_{g}: \\mathsf{H} \\to \\mathsf{H}$, $L_{g}(h) = g h$.\n- A one-parameter subgroup $g(t)$ generated by $\\xi$ satisfies the left-invariant Ordinary Differential Equation (ODE) $\\dot{g}(t) = (L_{g(t)})_{*} \\xi$ with initial condition $g(0) = e$, where $e$ is the identity.\n- In a matrix Lie group, $(L_{g})_{*} \\xi = g \\xi$.\n\nYour tasks:\n1. Parameterize $g(t) \\in \\mathsf{H}$ by coordinates $(u(t), v(t), w(t)) \\in \\mathbb{R}^{3}$ via\n$$\ng(t) = \\begin{pmatrix} 1 & u(t) & w(t) \\\\ 0 & 1 & v(t) \\\\ 0 & 0 & 1 \\end{pmatrix},\n$$\nand derive the left-invariant ODE for $(u(t), v(t), w(t))$ corresponding to the generator $\\xi$.\n2. Solve this ODE with $g(0) = e$ to obtain $g(1) = \\exp(\\xi)$ expressed as a $3 \\times 3$ matrix in terms of $x,y,z$.\n3. Independently, compute the matrix exponential $\\exp(\\xi)$ using the power series definition and the algebraic properties of $\\mathfrak{h}$, and verify that it matches your result from solving the left-invariant ODE.\n\nProvide the final explicit $3 \\times 3$ matrix expression for $\\exp(\\xi)$. The final answer must be a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem asks for the computation of the exponential map for the Heisenberg group $\\mathsf{H}$ in three ways: by solving a left-invariant ODE, by direct power series expansion of the matrix exponential, and to verify their consistency. The final result should be the explicit matrix form of $\\exp(\\xi)$.\n\nFirst, we address Task 1: Deriving the left-invariant ODE.\nLet $g(t) \\in \\mathsf{H}$ be a one-parameter subgroup generated by $\\xi \\in \\mathfrak{h}$. It is parameterized as\n$$\ng(t) = \\begin{pmatrix} 1 & u(t) & w(t) \\\\ 0 & 1 & v(t) \\\\ 0 & 0 & 1 \\end{pmatrix}.\n$$\nThe tangent vector to the curve at $t$ is its time derivative, $\\dot{g}(t)$:\n$$\n\\dot{g}(t) = \\frac{d}{dt} \\begin{pmatrix} 1 & u(t) & w(t) \\\\ 0 & 1 & v(t) \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & \\dot{u}(t) & \\dot{w}(t) \\\\ 0 & 0 & \\dot{v}(t) \\\\ 0 & 0 & 0 \\end{pmatrix}.\n$$\nThe generator of the flow is the Lie algebra element $\\xi$, which is given by\n$$\n\\xi = x E_{1} + y E_{2} + z E_{3} = x \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} + y \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} + z \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & x & z \\\\ 0 & 0 & y \\\\ 0 & 0 & 0 \\end{pmatrix}.\n$$\nThe left-invariant ODE is given by $\\dot{g}(t) = (L_{g(t)})_{*} \\xi$. For a matrix Lie group, this simplifies to $\\dot{g}(t) = g(t) \\xi$. We compute the right-hand side of this equation:\n$$\ng(t)\\xi = \\begin{pmatrix} 1 & u(t) & w(t) \\\\ 0 & 1 & v(t) \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & x & z \\\\ 0 & 0 & y \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & x & z + u(t)y \\\\ 0 & 0 & y \\\\ 0 & 0 & 0 \\end{pmatrix}.\n$$\nEquating the components of $\\dot{g}(t)$ and $g(t)\\xi$, we obtain the system of first-order ordinary differential equations for the coordinates $(u(t), v(t), w(t))$:\n$$\n\\begin{cases}\n\\dot{u}(t) = x \\\\\n\\dot{v}(t) = y \\\\\n\\dot{w}(t) = z + u(t)y\n\\end{cases}\n$$\n\nNext, we address Task 2: Solving the ODE.\nThe initial condition for a one-parameter subgroup is $g(0) = e$, where $e$ is the identity element of the group $\\mathsf{H}$. The identity matrix is\n$$\ne = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}.\n$$\nThis implies the initial conditions for our coordinates are $u(0)=0$, $v(0)=0$, and $w(0)=0$.\nWe solve the ODE system by direct integration.\nThe first equation is $\\dot{u}(t) = x$. Integrating with respect to $t$ gives $u(t) = xt + C_1$. Applying the initial condition $u(0)=0$, we find $0 = x(0) + C_1$, so $C_1=0$. Thus, $u(t) = xt$.\nThe second equation is $\\dot{v}(t) = y$. Integrating gives $v(t) = yt + C_2$. Applying $v(0)=0$, we find $C_2=0$. Thus, $v(t) = yt$.\nThe third equation is $\\dot{w}(t) = z + u(t)y$. Substituting the solution for $u(t)$, we get $\\dot{w}(t) = z + (xt)y = z + xyt$. Integrating with respect to $t$ yields $w(t) = zt + \\frac{1}{2}xyt^2 + C_3$. Applying $w(0)=0$, we find $C_3=0$. Thus, $w(t) = zt + \\frac{1}{2}xyt^2$.\nThe one-parameter subgroup is therefore\n$$\ng(t) = \\begin{pmatrix} 1 & xt & zt + \\frac{1}{2}xyt^2 \\\\ 0 & 1 & yt \\\\ 0 & 0 & 1 \\end{pmatrix}.\n$$\nThe exponential map is defined by $\\exp(\\xi) = g(1)$. Setting $t=1$, we obtain:\n$$\n\\exp(\\xi) = g(1) = \\begin{pmatrix} 1 & x & z + \\frac{1}{2}xy \\\\ 0 & 1 & y \\\\ 0 & 0 & 1 \\end{pmatrix}.\n$$\n\nFinally, we address Task 3: Independent verification using the power series definition.\nThe exponential of a matrix $A$ is defined by the power series $\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{A^k}{k!} = I + A + \\frac{A^2}{2!} + \\frac{A^3}{3!} + \\dots$, where $I$ is the identity matrix. In our case, $A = \\xi$.\nWe need to compute the powers of $\\xi$:\n$$\n\\xi = \\begin{pmatrix} 0 & x & z \\\\ 0 & 0 & y \\\\ 0 & 0 & 0 \\end{pmatrix}.\n$$\nThe square of $\\xi$ is:\n$$\n\\xi^2 = \\xi \\cdot \\xi = \\begin{pmatrix} 0 & x & z \\\\ 0 & 0 & y \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & x & z \\\\ 0 & 0 & y \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & xy \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}.\n$$\nIn terms of the basis elements, $\\xi^2 = xy E_3$. This indicates the nilpotent nature of the algebra.\nThe cube of $\\xi$ is:\n$$\n\\xi^3 = \\xi^2 \\cdot \\xi = \\begin{pmatrix} 0 & 0 & xy \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & x & z \\\\ 0 & 0 & y \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}.\n$$\nSince $\\xi^3$ is the zero matrix, all higher powers $\\xi^k$ for $k \\ge 3$ are also zero. The exponential series truncates:\n$$\n\\exp(\\xi) = I + \\xi + \\frac{\\xi^2}{2}.\n$$\nSubstituting the matrices for $I$, $\\xi$, and $\\xi^2$:\n$$\n\\exp(\\xi) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 0 & x & z \\\\ 0 & 0 & y \\\\ 0 & 0 & 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 0 & 0 & xy \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}.\n$$\nAdding the matrices component-wise gives:\n$$\n\\exp(\\xi) = \\begin{pmatrix} 1 & x & z + \\frac{1}{2}xy \\\\ 0 & 1 & y \\\\ 0 & 0 & 1 \\end{pmatrix}.\n$$\nThis result matches the expression for $g(1)$ obtained by solving the left-invariant ODE, thus verifying the result. The explicit matrix expression for $\\exp(\\xi)$ is determined.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & x & z + \\frac{1}{2}xy \\\\\n0 & 1 & y \\\\\n0 & 0 & 1\n\\end{pmatrix}\n}\n$$"
        }
    ]
}