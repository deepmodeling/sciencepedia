{
    "hands_on_practices": [
        {
            "introduction": "在训练自编码器以发现序参量后，一个关键的验证步骤是评估学到的潜变量是否与物理现实相符。如果系统存在一个已知的序参量，我们可以通过统计检验来评估潜变量与该已知参量之间的相关性。 这个练习将指导你建立一个严谨的定量标准，使用皮尔逊相关性和假设检验来判断自编码器是否成功捕捉到了已知的宏观特征，从而将抽象的潜空间与既有物理知识联系起来。",
            "id": "3828675",
            "problem": "考虑一个包含 $N$ 个样本的数据集，这些样本索引了一个多尺度系统的微观状态 $\\{x_i\\}_{i=1}^N$。一个在这些状态上训练的自编码器 (Autoencoder, AE) 为每个样本 $i$ 生成一个一维潜变量 $z_i \\in \\mathbb{R}$，并且对于相同的样本，一个已知的宏观序参量 (Order Parameter, OP) $m_i \\in \\mathbb{R}$ 也是可用的。典型相关分析 (Canonical Correlation Analysis, CCA) 旨在寻找能够最大化两组变量之间相关性的线性组合。通常，对于集合 $X \\in \\mathbb{R}^p$ 和 $Y \\in \\mathbb{R}^q$，CCA 解决的问题是 $\\max_{\\alpha \\in \\mathbb{R}^p, \\beta \\in \\mathbb{R}^q} \\mathrm{corr}(\\alpha^\\top X, \\beta^\\top Y)$。当两组变量都是一维时，即 $p=q=1$，CCA 简化为标准化标量 $z$ 和 $m$ 之间的绝对皮尔逊相关性。您的任务是形式化一个判定准则，以测试 AE 的潜变量 $z$ 是否对应于已知的序参量 $m$，并将其实现为一个程序。\n\n您形式化的判定准则必须基于：计算整个数据集上 $z$ 和 $m$ 之间的样本皮尔逊相关系数 $r$，在真实相关性为零的原假设下评估其统计显著性，然后对典型相关性的大小应用一个阈值。具体来说，定义样本皮尔逊相关系数\n$$\nr = \\frac{\\sum_{i=1}^N (z_i - \\bar{z})(m_i - \\bar{m})}{\\sqrt{\\sum_{i=1}^N (z_i - \\bar{z})^2} \\sqrt{\\sum_{i=1}^N (m_i - \\bar{m})^2}},\n$$\n其中 $\\bar{z} = \\frac{1}{N}\\sum_{i=1}^N z_i$ 且 $\\bar{m} = \\frac{1}{N}\\sum_{i=1}^N m_i$。在原假设 $H_0: \\rho = 0$ (其中 $\\rho$ 是总体相关性) 下，并假设样本是具有有限方差的独立同分布样本，统计量\n$$\nt = r \\sqrt{\\frac{N - 2}{1 - r^2}}\n$$\n服从自由度为 $N - 2$ 的学生 $t$ 分布。双边 $p$ 值为\n$$\np = 2 \\left(1 - F_{t_{N-2}}(|t|)\\right),\n$$\n其中 $F_{t_{N-2}}$ 是自由度为 $N - 2$ 的学生 $t$ 分布的累积分布函数。设判定阈值为相关性大小 $\\tau \\in [0,1]$ 和显著性水平 $\\alpha \\in (0,1)$。您的判定规则必须是：\n- 如果 $\\mathrm{Var}(z) = 0$ 或 $\\mathrm{Var}(m) = 0$，则判定 $z$ 不对应于 $m$。\n- 否则，按上述方法计算 $r$ 和 $p$，并且当且仅当 $|r| \\ge \\tau$ 且 $p \\le \\alpha$ 时，判定 $z$ 对应于 $m$。\n\n对于任何三角函数，角度都必须以弧度为单位处理。程序必须实现此准则，并评估以下由索引 $i$ 的显式公式构造的 $(z, m)$ 对定义的确定性案例测试套件：\n\n- 案例1（一般成功路径）：$N = 200$, $m_i = \\sin\\left(\\frac{2\\pi i}{N}\\right)$，$z_i = 1.1 \\, m_i$。使用 $\\tau = 0.8$ 和 $\\alpha = 0.01$。角度单位为弧度。预期行为：强正典型相关性。\n- 案例2（正交性边界情况）：$N = 200$, $m_i = \\sin\\left(\\frac{2\\pi i}{N}\\right)$，$z_i = \\cos\\left(\\frac{2\\pi i}{N}\\right)$。使用 $\\tau = 0.8$ 和 $\\alpha = 0.01$。角度单位为弧度。预期行为：可忽略的典型相关性。\n- 案例3（负相关一般情况）：$N = 150$， $m_i$ 线性间隔为 $m_i = -1 + \\frac{2(i-1)}{N-1}$，$z_i = - m_i + 0.05 \\, \\sin\\left(\\frac{4\\pi i}{N}\\right)$。使用 $\\tau = 0.8$ 和 $\\alpha = 0.01$。角度单位为弧度。预期行为：强负典型相关性。\n- 案例4（退化方差边界）：$N = 100$，$m_i = 0$ 对所有 $i$ 成立，$z_i = \\cos\\left(\\frac{2\\pi i}{N}\\right)$。使用 $\\tau = 0.8$ 和 $\\alpha = 0.01$。角度单位为弧度。预期行为：由于 $m$ 的方差为零导致相关性未定义，必须视为拒绝。\n- 案例5（小样本中等相关性，显著性边缘）：$N = 20$, $m_i = -1 + \\frac{2(i-1)}{N-1}$，$z_i = 0.4 \\, m_i + 0.9 \\, \\sin\\left(\\frac{2\\pi i}{N}\\right)$。使用 $\\tau = 0.8$ 和 $\\alpha = 0.01$。角度单位为弧度。预期行为：相关性大小低于阈值，并且在给定的 $\\alpha$ 水平下可能不具有统计显著性。\n\n您的程序必须生成单行输出，其中包含五个案例的判定规则结果，格式为方括号内以逗号分隔的布尔值列表，顺序与上面列出的案例顺序相同（例如，`[`result$_1$,result$_2$,result$_3$,result$_4$,result$_5`]$）。输出值必须是布尔类型，并且不得打印任何额外文本。",
            "solution": "该问题要求形式化并实现一个判定准则，以评估由自编码器从一组微观状态中学习到的一维潜变量 $z$ 是否对应于一个已知的宏观序参量 $m$。这种对应关系是根据两个变量 $z$ 和 $m$ 之间线性关系的强度和统计显著性来评估的。\n\n该判定准则的核心在于样本皮尔逊相关系数 $r$ 和相关的假设检验。对于两组成对数据 $\\{z_i\\}_{i=1}^N$ 和 $\\{m_i\\}_{i=1}^N$，样本皮尔逊相关系数由下式给出：\n$$\nr = \\frac{\\sum_{i=1}^N (z_i - \\bar{z})(m_i - \\bar{m})}{\\sqrt{\\sum_{i=1}^N (z_i - \\bar{z})^2} \\sqrt{\\sum_{i=1}^N (m_i - \\bar{m})^2}}\n$$\n其中 $\\bar{z} = \\frac{1}{N}\\sum_{i=1}^N z_i$ 和 $\\bar{m} = \\frac{1}{N}\\sum_{i=1}^N m_i$ 是样本均值。在两个一维变量之间的典型相关分析 (CCA) 的背景下，最大化的相关性就是皮尔逊系数的绝对值 $|r|$。\n\n该判定准则是一个包含指定阈值的两部分检验：最小相关性大小 $\\tau$ 和统计显著性水平 $\\alpha$。\n\n步骤 1：先决条件验证。\n如果任一变量的方差为零，皮尔逊相关系数就未定义。方差为零的变量是一个常数，不能与另一个变量协变。因此，该准则的第一步是检查样本方差 $\\mathrm{Var}(z)$ 和 $\\mathrm{Var}(m)$。如果 $\\mathrm{Var}(z) = 0$ 或 $\\mathrm{Var}(m) = 0$，则变量之间无法进行有意义的相关。在这种情况下，我们判定 $z$ 不对应于 $m$。在数值实现中，必须使用一个小的容差来执行此检查，以考虑浮点运算的限制。\n\n步骤 2：相关性大小检验。\n如果两个变量的方差都非零，我们继续计算 $r$。判定规则的第一部分评估相关性的强度。只有当潜变量 $z$ 与序参量 $m$ 之间线性关系的大小足够大时，才认为 $z$ 是 $m$ 的一个潜在表示。这被形式化为条件：\n$$\n|r| \\ge \\tau\n$$\n如果不满足此条件，我们判定对应关系太弱，准则不通过。\n\n步骤 3：统计显著性检验。\n较大的样本相关系数 $r$ 可能偶然出现，尤其是在小数据集中。为防止这种情况，我们检验观测到的相关性的统计显著性。我们构建一个原假设 $H_0$，即样本所来自的潜在总体中没有相关性，即总体相关系数 $\\rho$ 为零 ($H_0: \\rho = 0$)。\n\n在样本 $(z_i, m_i)$ 独立同分布于一个二元正态分布（或对于大样本 $N$ 可通过中心极限定理）的假设下，检验统计量\n$$\nt = r \\sqrt{\\frac{N - 2}{1 - r^2}}\n$$\n服从自由度为 $N-2$ 的学生 $t$ 分布。$t$ 的较大绝对值提供了反对原假设的证据。\n\n根据计算出的 $t$ 统计量，我们确定双边 $p$ 值，即在原假设为真的情况下，观测到至少与 $r$ 一样极端的相关性的概率。$p$ 值由下式给出：\n$$\np_{\\text{value}} = 2 \\left(1 - F_{t_{N-2}}(|t|)\\right)\n$$\n其中 $F_{t_{N-2}}$ 是自由度为 $N-2$ 的学生 $t$ 分布的累积分布函数 (CDF)。如果该 $p$ 值小于或等于指定的显著性水平 $\\alpha$，则认为相关性具有统计显著性：\n$$\np_{\\text{value}} \\le \\alpha\n$$\n\n当 $|r|=1$ 时会出现一个特殊情况。$t$ 统计量的公式会涉及除以零。然而，完全相关 ($|r|=1$) 是可能的最大效应大小，这意味着观测到更极端结果的概率为零。因此，对于 $|r|=1$， $p$ 值为 $0$，对于任何 $\\alpha > 0$，显著性检验总是通过。\n\n最终判定规则：\n综合这些步骤，当且仅当所有三个条件都满足时，潜变量 $z$ 才被判定为对应于序参量 $m$：\n1. $\\mathrm{Var}(z) > 0$ 且 $\\mathrm{Var}(m) > 0$。\n2. 相关性大小足够：$|r| \\ge \\tau$。\n3. 相关性具有统计显著性：$p_{\\text{value}} \\le \\alpha$。\n\n该程序将被实现并应用于五个指定的测试案例。为了实现，我们将利用数值库来确保方差、相关系数和学生 $t$ 分布累积分布函数 (CDF) 的稳健计算。\n- 案例1：$z_i = 1.1 m_i$，所以 $r=1$。$|r| = 1 \\ge 0.8$ 且 $p_{\\text{value}} = 0 \\le 0.01$。准则通过。\n- 案例2：$m_i$ 和 $z_i$ 是一个完整周期上的正交正弦和余弦函数。它们的样本相关性 $r$ 将接近于 $0$。条件 $|r| \\ge 0.8$ 将不满足。\n- 案例3：$z_i$ 是 $m_i$ 带有微小噪声的强反相关版本。我们预期 $r$ 接近 $-1$。因此， $|r| \\approx 1 \\ge 0.8$。当 $N=150$ 时，这种强相关性将是高度显著的，所以 $p_{\\text{value}} \\ll 0.01$。准则通过。\n- 案例4：$m_i$ 是一个常数向量，所以 $\\mathrm{Var}(m) = 0$。先决条件不满足，准则立即返回否定结果。\n- 案例5：预期存在中等相关性，但样本量 $N=20$ 很小，且噪声项较大。所得的相关性大小 $|r|$ 不太可能达到 $\\tau = 0.8$ 的高阈值。此外，即使达到了，小样本量也可能导致结果在严格的 $\\alpha=0.01$ 水平下不具有统计显著性。预期准则不通过。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as student_t\n\ndef solve():\n    \"\"\"\n    Main function to evaluate the decision criterion for all test cases.\n    \"\"\"\n\n    def decision_criterion(z, m, N, tau, alpha):\n        \"\"\"\n        Applies the formal decision criterion to determine if latent variable z\n        corresponds to order parameter m.\n\n        Args:\n            z (np.ndarray): The latent variable vector.\n            m (np.ndarray): The order parameter vector.\n            N (int): The number of samples.\n            tau (float): The correlation magnitude threshold.\n            alpha (float): The significance level.\n\n        Returns:\n            bool: True if z corresponds to m, False otherwise.\n        \"\"\"\n        # Step 1: Pre-condition Validation (non-zero variance)\n        # Using a small tolerance for floating-point comparison.\n        if np.var(m) < 1e-15 or np.var(z) < 1e-15:\n            return False\n\n        # Compute sample Pearson correlation coefficient r\n        # np.corrcoef returns a 2x2 matrix, the value is at [0, 1] or [1, 0]\n        r = np.corrcoef(z, m)[0, 1]\n\n        # Step 2: Correlation Magnitude Test\n        if abs(r) < tau:\n            return False\n\n        # Step 3: Statistical Significance Test\n        # Handle the edge case of perfect correlation to avoid division by zero.\n        # If |r| is extremely close to 1, the p-value is effectively 0.\n        if abs(r) > 1.0 - 1e-9:\n            p_val = 0.0\n        else:\n            # Calculate the t-statistic\n            t_stat = r * np.sqrt((N - 2) / (1 - r**2))\n            \n            # Degrees of freedom\n            df = N - 2\n            \n            # Calculate the two-sided p-value using the Student's t-distribution CDF\n            p_val = 2 * (1 - student_t.cdf(abs(t_stat), df))\n\n        # Final decision: True if p-value is below significance level\n        return p_val <= alpha\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (N, m_func, z_func, tau, alpha)\n    test_cases = [\n        (200, lambda i, N: np.sin(2 * np.pi * i / N), lambda i, N: 1.1 * np.sin(2 * np.pi * i / N), 0.8, 0.01),\n        (200, lambda i, N: np.sin(2 * np.pi * i / N), lambda i, N: np.cos(2 * np.pi * i / N), 0.8, 0.01),\n        (150, lambda i, N: -1 + 2 * (i - 1) / (N - 1), lambda i, N: -(-1 + 2 * (i - 1) / (N - 1)) + 0.05 * np.sin(4 * np.pi * i / N), 0.8, 0.01),\n        (100, lambda i, N: np.zeros_like(i, dtype=float), lambda i, N: np.cos(2 * np.pi * i / N), 0.8, 0.01),\n        (20, lambda i, N: -1 + 2 * (i - 1) / (N - 1), lambda i, N: 0.4 * (-1 + 2 * (i - 1) / (N - 1)) + 0.9 * np.sin(2 * np.pi * i / N), 0.8, 0.01),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, m_func, z_func, tau, alpha = case\n        \n        # The problem states index i from 1 to N\n        i_indices = np.arange(1, N + 1)\n        \n        m_vec = m_func(i_indices, N)\n        z_vec = z_func(i_indices, N)\n        \n        result = decision_criterion(z_vec, m_vec, N, tau, alpha)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Python's str() for booleans is 'True'/'False' (capitalized).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "通常，我们使用自编码器正是因为我们事先并不知道序参量。在这种情况下，一个有物理意义的模型应该尊重它所代表的系统的基本定律，例如质量或能量守恒。 这个动手练习将教你如何通过量化模型重构结果对物理不变量（如总质量和能量）的违背程度，来创建一个“物理性分数”，即使在没有已知序参量的情况下，这也为你提供了一个强大的模型验证工具。",
            "id": "3828693",
            "problem": "给定一个在一维区间 $[0,2\\pi)$ 上的周期性标量场 $u(x)$，该场被均匀离散化为 $N$ 个网格点，网格间距为 $\\Delta x = \\frac{2\\pi}{N}$。假设使用一个自编码器将 $u(x)$ 压缩为一个潜变量，然后再将其解码回一个重构场 $\\hat{u}(x)$。在多尺度建模与分析中，有意义的潜变量（序参量）应该编码那些遵循全局守恒定律的物理相关特征。为了评估这一点，请实现一个一致性检查，以量化解码器的重构结果是否保持预设的全局不变量。使用以下基本依据：\n\n- 对于以类对流或哈密顿动力学为主导且耗散可忽略不计的周期性系统，总质量和二次能量等全局不变量是守恒的。将质量定义为 $I_1[u] = \\int_0^{2\\pi} u(x)\\,dx$，二次能量定义为 $I_2[u] = \\int_0^{2\\pi} u(x)^2\\,dx$。\n- 在间距为 $\\Delta x$ 的均匀网格上，使用黎曼和来近似这些积分，即 $I_1[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j$ 和 $I_2[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j^2$，其中 $u_j = u(x_j)$ 且 $x_j = j\\Delta x$。\n- 守恒性检查比较原始场 $u$ 和重构场 $\\hat{u}$ 的不变量。设 $\\Delta_k = I_k[\\hat{u}] - I_k[u]$，其中 $k \\in \\{1,2\\}$。\n\n通过使用具有物理意义的尺度对绝对差值进行归一化，为每个不变量定义一个无量纲偏差：\n- 对于质量，使用 $d_1 = \\frac{|\\Delta_1|}{\\max\\left(|I_1[u]|, \\|u\\|_{L^1}\\right)}$，其中 $\\|u\\|_{L^1} \\approx \\Delta x \\sum_{j=0}^{N-1} |u_j|$。\n- 对于能量，使用 $d_2 = \\frac{|\\Delta_2|}{\\max\\left(|I_2[u]|, \\epsilon\\right)}$，其中 $\\epsilon$ 是为避免当 $I_2[u]=0$ 时出现除以零的任何正下界。在本问题中，测试套件会避免 $I_2[u]=0$ 的情况，因此您可以将 $\\epsilon$ 设为 $0$。\n\n将这些偏差汇总成一个标量潜物理性分数\n$$\nS = \\exp\\left(-\\frac{d_1 + d_2}{2}\\right),\n$$\n使得 $S \\in (0,1]$，当精确守恒时 $S=1$，且 $S$ 会随着偏差的增大而衰减。\n\n实现一个程序，该程序：\n- 使用 $N=512$ 个均匀间隔的点，在 $[0,2\\pi)$ 上构建一个基础场 $u_{\\text{base}}(x) = \\sin(x) + 0.5\\cos(2x)$，其中角度以弧度为单位。\n- 使用以下四个测试用例，从 $u_{\\text{base}}$ 定义重构场 $\\hat{u}$：\n    1. 用例 A（理想路径）：精确重构 $\\hat{u}(x) = u_{\\text{base}}(x)$。\n    2. 用例 B（谱高斯模糊）：在傅里叶空间中，对 $u_{\\text{base}}$ 应用一个标准差为 $\\sigma = 0.2$（在 $x$ 坐标系中）的高斯滤波器，然后通过逆变换回到实空间。这应该会保持零波数模式，从而保持质量，同时减少高频能量。\n    3. 用例 C（幅度缩放）：$\\hat{u}(x) = s \\, u_{\\text{base}}(x)$，其中 $s = 1.1$。\n    4. 用例 D（加性偏置）：$\\hat{u}(x) = u_{\\text{base}}(x) + b$，其中 $b = 0.05$。\n\n对于每个用例，计算 $I_1[u]$、$I_2[u]$、$I_1[\\hat{u}]$、$I_2[\\hat{u}]$、如上定义的偏差 $d_1$ 和 $d_2$ 以及标量分数 $S$。角度以弧度为单位。在此问题中，所有量都是无量纲的。\n\n您的程序应生成单行输出，其中包含用例 A、B、C 和 D 的四个分数 $S$，每个分数四舍五入到六位小数，以逗号分隔的列表形式并用方括号括起来（例如：\"[0.999999,0.912345,0.812345,0.456789]\"）。\n\n需要实现的测试套件细节：\n- 域长度 $L = 2\\pi$。\n- 网格点数 $N = 512$。\n- 高斯模糊参数 $\\sigma = 0.2$（以 $x$ 为单位）。\n- 缩放因子 $s = 1.1$。\n- 加性偏置 $b = 0.05$。\n\n最终输出是按规定格式化的包含四个浮点数的列表。无需用户输入。",
            "solution": "该问题要求实现一个物理性分数 $S$，以评估一个重构场 $\\hat{u}(x)$ 是否保持原始场 $u(x)$ 的关键全局不变量。该分数基于在一维周期性标量场在域 $[0, 2\\pi)$ 上的总质量 $I_1[u]$ 和总二次能量 $I_2[u]$ 的守恒性。解决方案遵循以下步骤：定义离散系统，计算基础场及其不变量，生成四个不同的重构场，并为每个重构场计算基于偏差的分数。\n\n首先，我们建立计算域。场 $u(x)$ 在一个包含 $N=512$ 个点的均匀网格上进行离散化，记作 $x_j = j \\Delta x$，其中 $j \\in \\{0, 1, \\dots, N-1\\}$。网格间距为 $\\Delta x = \\frac{2\\pi}{N}$。基础场由函数 $u_{\\text{base}}(x) = \\sin(x) + 0.5\\cos(2x)$ 给出。我们在我们的离散网格上计算该函数的值，以获得向量 $u_{\\text{base},j} = u_{\\text{base}}(x_j)$。\n\n接下来，我们为全局不变量定义数值近似。总质量 $I_1[u] = \\int_0^{2\\pi} u(x)\\,dx$ 和总二次能量 $I_2[u] = \\int_0^{2\\pi} u(x)^2\\,dx$ 使用黎曼和进行近似：\n$$I_1[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j$$\n$$I_2[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j^2$$\n同样，归一化所需的 $L^1$范数 $\\|u\\|_{L^1} = \\int_0^{2\\pi} |u(x)|\\,dx$ 近似为：\n$$\\|u\\|_{L^1} \\approx \\Delta x \\sum_{j=0}^{N-1} |u_j|$$\n使用这些公式，我们为离散的基础场计算不变量 $I_1[u_{\\text{base}}]$、$I_2[u_{\\text{base}}]$ 以及范数 $\\|u_{\\text{base}}\\|_{L^1}$。解析上，$I_1[u_{\\text{base}}] = 0$，而 $I_2[u_{\\text{base}}] = 1.25\\pi$。我们的数值计算将得出非常接近这些值的结果。\n\n问题的核心在于为从 $u_{\\text{base}}(x)$ 派生出的四个不同的重构场 $\\hat{u}(x)$ 评估物理性分数。\n\n用例 A（精确重构）：$\\hat{u}(x) = u_{\\text{base}}(x)$。在这个平凡的情况下，重构是完美的。\n\n用例 B（谱高斯模糊）：这种重构模拟了一种常见的信息损失形式，其中高频细节被抑制。这是在傅里叶空间中实现的。首先，我们计算基础场的离散傅里叶变换 (DFT)，$\\tilde{u}_{\\text{base}} = \\mathcal{F}\\{u_{\\text{base}}\\}$。相应的角波数由向量 $k$ 给出，其中 $k_m = 2\\pi f_m$，$f_m$ 是由标准 FFT 频率函数为长度为 $N$、采样间距为 $\\Delta x$ 的信号提供的离散频率。然后我们在傅里叶域中定义一个高斯滤波器：\n$$G(k) = \\exp\\left(-\\frac{k^2 \\sigma^2}{2}\\right)$$\n其中给定的实空间标准差为 $\\sigma = 0.2$。傅里叶空间中滤波后的场是 $\\tilde{\\hat{u}} = \\tilde{u}_{\\text{base}} \\odot G$，其中 $\\odot$ 表示逐元素乘法。然后通过应用逆 DFT 获得重构场 $\\hat{u}(x)$，即 $\\hat{u} = \\text{Re}(\\mathcal{F}^{-1}\\{\\tilde{\\hat{u}}\\})$。取实部是为了舍弃由数值精度误差产生的可忽略的虚部。\n\n用例 C（幅度缩放）：该用例模拟一个简单的增益误差，其中重构场被均匀缩放：$\\hat{u}(x) = s \\cdot u_{\\text{base}}(x)$，缩放因子为 $s = 1.1$。\n\n用例 D（加性偏置）：该用例模拟一个恒定的偏移误差：$\\hat{u}(x) = u_{\\text{base}}(x) + b$，偏置为 $b = 0.05$。\n\n对于这四个用例中的每一个，我们都计算物理性分数 $S$。首先，我们计算重构场的不变量 $I_1[\\hat{u}]$ 和 $I_2[\\hat{u}]$。然后，我们求出与基础场不变量的差值：$\\Delta_1 = I_1[\\hat{u}] - I_1[u_{\\text{base}}]$ 和 $\\Delta_2 = I_2[\\hat{u}] - I_2[u_{\\text{base}}]$。\n\n这些差值通过归一化被转换为无量纲偏差 $d_1$ 和 $d_2$：\n$$d_1 = \\frac{|\\Delta_1|}{\\max\\left(|I_1[u_{\\text{base}}]|, \\|u_{\\text{base}}\\|_{L^1}\\right)}$$\n$$d_2 = \\frac{|\\Delta_2|}{|I_2[u_{\\text{base}}]|}$$\n$d_1$ 的分母能稳健地处理场均值为零的情况，这对于 $u_{\\text{base}}$ 来说是成立的。$d_2$ 的分母是安全的，因为 $I_2[u_{\\text{base}}]$ 严格为正，测试套件避免了 $I_2[u] = 0$ 的情况。\n\n最后，通过汇总偏差来计算标量物理性分数 $S$：\n$$S = \\exp\\left(-\\frac{d_1 + d_2}{2}\\right)$$\n这个分数将两个偏差映射到区间 $(0, 1]$ 上，其中 $S=1$ 表示两个不变量都完美守恒，而分数会随着守恒误差的增大而减小。对四个用例中的每一个重复此过程，并将得到的分数格式化为一个列表。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a physicality score for four test cases of field reconstructions\n    based on the conservation of mass and energy invariants.\n    \"\"\"\n    # Define problem parameters\n    N = 512\n    L = 2.0 * np.pi\n    dx = L / N\n    sigma = 0.2\n    s = 1.1\n    b = 0.05\n\n    # Construct the grid and base field\n    x = np.linspace(0.0, L, N, endpoint=False)\n    u_base = np.sin(x) + 0.5 * np.cos(2.0 * x)\n\n    # --- Define test cases for reconstructions ---\n    test_cases = []\n\n    # Case A: Exact reconstruction\n    u_hat_A = u_base\n    test_cases.append(u_hat_A)\n\n    # Case B: Spectral Gaussian blur\n    # Compute Fourier transform and corresponding wavenumbers\n    u_base_tilde = np.fft.fft(u_base)\n    # Frequencies in cycles per unit of sample spacing\n    # `d=dx` means the unit is physical length\n    freq = np.fft.fftfreq(N, d=dx)\n    # Convert to angular wavenumbers (rad/length)\n    k = 2.0 * np.pi * freq\n    # Define Gaussian filter in Fourier space\n    gaussian_filter = np.exp(-(k**2 * sigma**2) / 2.0)\n    # Apply filter and inverse transform, taking real part to remove numerical noise\n    u_hat_tilde_B = u_base_tilde * gaussian_filter\n    u_hat_B = np.fft.ifft(u_hat_tilde_B).real\n    test_cases.append(u_hat_B)\n\n    # Case C: Amplitude scaling\n    u_hat_C = s * u_base\n    test_cases.append(u_hat_C)\n\n    # Case D: Additive bias\n    u_hat_D = u_base + b\n    test_cases.append(u_hat_D)\n\n    # --- Calculate invariants for the base field ---\n    # Riemann sum approximation of the integral\n    I1_base = np.sum(u_base) * dx\n    I2_base = np.sum(u_base**2) * dx\n    L1_norm_base = np.sum(np.abs(u_base)) * dx\n\n    scores = []\n    # --- Process each test case to calculate its score ---\n    for u_hat in test_cases:\n        # Calculate invariants for the reconstructed field\n        I1_hat = np.sum(u_hat) * dx\n        I2_hat = np.sum(u_hat**2) * dx\n\n        # Calculate differences and dimensionless deviations\n        delta_1 = I1_hat - I1_base\n        delta_2 = I2_hat - I2_base\n\n        # Normalization for d1, robust to I1_base being near zero\n        d1_denom = np.maximum(np.abs(I1_base), L1_norm_base)\n        d1 = np.abs(delta_1) / d1_denom if d1_denom != 0 else 0.0\n\n        # Normalization for d2\n        # Problem statement guarantees I2_base is not zero for the given test suite\n        d2 = np.abs(delta_2) / np.abs(I2_base)\n        \n        # Calculate the final physicality score\n        score = np.exp(-(d1 + d2) / 2.0)\n        scores.append(score)\n\n    # Format the output as a comma-separated list with 6 decimal places\n    formatted_scores = [f\"{score:.6f}\" for score in scores]\n    print(f\"[{','.join(formatted_scores)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "除了简单的数值相关性或物理守恒性，潜空间的整体几何或拓扑结构可以揭示关于系统相的更深刻见解。对于具有非平凡拓扑结构（例如，周期性自由度）的系统，一个理想的自编码器应该能学习到一个反映这种拓扑的潜空间。 这项高级练习介绍了一种前沿的数据拓扑分析（TDA）方法——持续同调，它使你能够量化数据中的环状或连通分量等“形状”特征，从而验证自编码器是否捕捉到了序的抽象拓扑标记。",
            "id": "3828633",
            "problem": "您将获得表示由自动编码器生成的低维编码的合成潜空间点云，该自动编码器经过训练，旨在从微观构型中捕捉宏观序参量。您的目标是通过计算潜空间点云上的持续同调，来验证与不同相相关的非平凡拓扑特征的存在，并在一个多尺度框架内解释这些特征对宏观行为的意义。\n\n从以下基本定义和事实出发：\n- 序参量是一个函数，它将微观状态映射到一个能够区分不同相的宏观描述符。当自动编码器学习到一个保留了显著宏观结构的潜表示时，其潜编码可能携带了相的拓扑印记。\n- 点云 $X \\subset \\mathbb{R}^d$ 上尺度为 $r$ 的 Vietoris–Rips 复形是一个抽象单纯复形，其顶点是 $X$ 中的点，其 $k$-单纯形是由 $k+1$ 个点组成的集合，这些点之间的两两欧几里得距离小于或等于 $r$。\n- Betti 数 $\\beta_k$ 计算拓扑空间中 $k$ 维孔洞的数量：$\\beta_0$ 计算连通分支的数量，$\\beta_1$ 计算一维环（圈）的数量。\n- 持续同调追踪同调特征在一个滤波参数 $r$ 上的“诞生”和“消亡”，通过量化它们的生命周期来区分信号和噪声。\n\n您的任务是，从基本原理出发，实现一个算法，该算法能够：\n- 使用欧几里得度量在潜空间点云上构建 Vietoris–Rips 滤波。\n- 随着 $r$ 的增加，使用并查集结构来计算出现的边，从而计算连通分支的数量 $\\beta_0$。\n- 在每个尺度 $r$ 上计算一维环的数量 $\\beta_1$。此计算需正确考虑由两两邻近性构建的团复形中的边和填充的三角形，并使用一种从计数数据到 $\\beta_1$ 的原则性转换，该转换在所考虑的潜数据中高维同调可以忽略不计时，与标志复形的欧拉示性数一致。\n- 通过从 $\\beta_1$ 在离散滤波尺度上的变化中提取其生死区间，来近似计算维度 1 的持续同调，并通过任何 $\\beta_1$ 特征的最大生命周期来总结每个数据集。\n\n任何参数化构造的角度单位必须是弧度。不涉及物理单位。所有数值输出必须是实数。最终结果必须表示为浮点数。\n\n实现一个程序，生成以下潜空间点云（每个都在 $\\mathbb{R}^2$ 中）：\n- Case R (单个环): 在以 $(0,0)$ 为中心、半径为 $1$ 的圆上采样的 $N = 50$ 个点，每个坐标上都添加了标准差为 $\\sigma = 0.02$ 的独立高斯噪声。角度在 $[0,2\\pi)$ 弧度范围内均匀采样。\n- Case B (团块): 从均值为零、每个坐标标准差为 $0.4$ 的各向同性高斯分布中采样的 $N = 50$ 个点。\n- Case 2R (两个不相交的环): $N = 80$ 个点，其中 $40$ 个点在以 $(-2, 0)$ 为中心、半径为 $1$ 的圆上，另外 $40$ 个点在以 $(2, 0)$ 为中心、半径为 $1$ 的圆上；每个坐标上都有独立的 $\\sigma = 0.02$ 的高斯噪声；角度在 $[0,2\\pi)$ 弧度范围内均匀采样。\n- Case NR (噪声环): 在以 $(0,0)$ 为中心、半径为 $1$ 的圆上采样的 $N = 50$ 个点，每个坐标上都添加了标准差为 $\\sigma = 0.12$ 的独立高斯噪声；角度在 $[0,2\\pi)$ 弧度范围内均匀采样。\n- Case SR (稀疏环): 在以 $(0,0)$ 为中心、半径为 $1$ 的圆上采样的 $N = 20$ 个点，每个坐标上都添加了标准差为 $\\sigma = 0.02$ 的独立高斯噪声；角度在 $[0,2\\pi)$ 弧度范围内均匀采样。\n\n在每个数据集上，计算 $r \\in \\{ r_1, r_2, \\dots, r_M \\}$ 上的 Vietoris–Rips 滤波，其中 $M = 40$ 且 $r_m$ 在 $[0.02, 0.6]$ 范围内线性间隔。在每个尺度 $r_m$ 上，计算与由两两距离小于或等于 $r_m$ 的边以及当所有三条相应边都存在时填充的 $2$-单纯形（三角形）所形成的团复形一致的 $\\beta_0(r_m)$ 和 $\\beta_1(r_m)$。从序列 $\\{ \\beta_1(r_m) \\}_{m=1}^M$ 中，通过将 $\\beta_1$ 的每次增加视为在该 $r_m$ 处的诞生，每次减少视为在该 $r_m$ 处的消亡，并以尊重滤波顺序的方式将它们配对，来提取一维特征的近似生死区间。对于在最大尺度上任何剩余的诞生，记录其消亡于 $r = 0.6$。\n\n对于每个数据集，报告一个等于所提取的 $\\beta_1$ 区间中最大生命周期的浮点数。您的程序应生成单行输出，包含这五个浮点数，以逗号分隔并用方括号括起（例如，\"[x_R,x_B,x_2R,x_NR,x_SR]\"）。\n\n测试套件设计：\n- 不同相的覆盖：Case R 应表现出一个具有显著生命周期的非平凡环（$\\beta_1$ 特征），Case B 不应有，Case 2R 应表现出两个环，因此至少有一个长生命周期，Case NR 应保留一个环，但其生命周期相对于 Case R 有所减少，Case SR 由于欠采样，应难以检测并产生短生命周期。\n- 边界条件：最小尺度 $r_1 = 0.02$ 应产生孤立点（大的 $\\beta_0$，零 $\\beta_1$），而最大尺度 $r_M = 0.6$ 应产生高度连接的复形，其中环被填充。\n- 可量化的答案：输出是五个案例的最大生命周期（浮点数）列表，格式需与指定的最终输出格式完全一致。",
            "solution": "该问题要求从合成的潜空间点云中计算和解释拓扑特征，具体来说是一维环（圈）。这项任务属于拓扑数据分析（TDA）的范畴，该领域使用代数拓扑学的工具来分析数据的“形状”。解决方案的核心在于实现一个算法来计算持续同调，它量化了拓扑特征在一系列尺度上的诞生和消亡。\n\n其基本原理是，一个本身在拓扑上是平凡的（一组离散的点）点云，可以通过构建一系列称为滤波（filtration）的单纯复形来赋予其更丰富的结构。我们将使用 Vietoris–Rips (VR) 复形，它由一个邻近参数 $r$ 决定。对于给定的点集 $X$，VR 复形 $VR(X, r)$ 对 $X$ 中每一组两两之间欧几里得距离在 $r$ 以内的 $k+1$ 个点，都包含一个 $k$-单纯形。\n\n我们的主要目标是跟踪这些复形的 Betti 数 $\\beta_k$ 如何随着 $r$ 的增加而变化。$\\beta_0$ 计算连通分支的数量，$\\beta_1$ 计算一维环或圈的数量。持续同调捕捉这些特征的生命周期，表示为生死区间 $[r_{birth}, r_{death})$。生命周期长的特征被认为是显著的拓扑信号，而生命周期短的特征通常归因于噪声。\n\n算法按以下步骤进行：\n\n1.  **数据生成**：对于五个案例中的每一个，根据指定的参数生成一个 $\\mathbb{R}^2$ 中的点云。使用固定的随机种子以确保可复现性。这些案例旨在代表不同的拓扑情景：一个完美的单环（Case R）、一个可收缩的团块（Case B）、两个不相交的环（Case 2R）、一个带噪声的环（Case NR）以及一个稀疏的环（Case SR）。\n\n2.  **Vietoris-Rips 滤波**：我们不采用在 $M=40$ 个尺度 $r_m \\in [0.02, 0.6]$ 的每一个上重新构建 VR 复形的方法，而是采用一种更高效的增量方法。\n    a. 计算所有 $N$ 个点之间的两两欧几里得距离，形成一个潜在边的列表。\n    b. 按距离升序对这些边进行排序。这个排序后的列表决定了向复形中添加单纯形的顺序。\n\n3.  **Betti 数的计算**：我们遍历离散尺度 $r_m$，并在每个尺度上，通过处理所有长度小于或等于当前 $r_m$ 的边来更新单纯形和连通分支的计数。\n    a. **$\\beta_0(r)$**：使用并查集（Union-Find 或 Disjoint Set Union, DSU）数据结构来跟踪连通分支的数量。最初，有 $N$ 个点，$\\beta_0 = N$。当一条边 $(u, v)$ 被添加到复形中时，如果 $u$ 和 $v$ 属于不同的分支，则将它们合并，并将 $\\beta_0$ 减 1。\n    b. **$\\beta_1(r)$**：第一个 Betti 数是使用从单纯复形 $K$ 的欧拉-庞加莱公式推导出的近似公式计算的：$\\chi(K) = \\sum_{k=0}^{\\infty} (-1)^k c_k = \\sum_{k=0}^{\\infty} (-1)^k \\beta_k$，其中 $c_k$ 是 $k$-单纯形的数量。对于我们在 $\\mathbb{R}^2$ 中的点云，我们可以假设高维同调可以忽略不计（即对于 $k \\ge 2$，$\\beta_k \\approx 0$）。VR 复形是一个标志复形，这意味着我们将图中的所有团（clique）都视作填充的单纯形。因此，公式简化为 $c_0 - c_1 + c_2 \\approx \\beta_0 - \\beta_1$。整理得到 $\\beta_1$，我们获得每个尺度 $r$ 的计算公式：\n    $$\n    \\beta_1(r) \\approx \\beta_0(r) - c_0(r) + c_1(r) - c_2(r)\n    $$\n    这里，$c_0(r)=N$ 是顶点的数量，$c_1(r)$ 是长度 $\\le r$ 的边的数量，$c_2(r)$ 是所有三条边的长度都 $\\le r$ 的 $2$-单纯形（三角形）的数量，而 $\\beta_0(r)$ 是连通分支的数量。算法在遍历排序后的边直到当前尺度 $r_m$ 的过程中，增量地更新 $c_1$、$c_2$ 和 $\\beta_0$。当添加一条边 $(u, v)$ 时，对于 $u$ 和 $v$ 的每一个已存在的共同邻居，都会形成一个新的三角形。\n\n4.  **持续同调区间的提取**：在计算出序列 $\\{\\beta_1(r_m)\\}_{m=1}^M$ 后，我们提取一维特征的生死区间。采用一个简单的贪心配对算法：\n    a. 维护一个活跃特征的诞生尺度列表。\n    b. 当 $\\beta_1(r_m) > \\beta_1(r_{m-1})$ 时，差值 $\\Delta \\beta_1 = \\beta_1(r_m) - \\beta_1(r_{m-1})$ 对应于在尺度 $r_m$ 处诞生的 $\\Delta \\beta_1$ 个新特征。它们的诞生尺度被添加到活跃列表中。\n    c. 当 $\\beta_1(r_m)  \\beta_1(r_{m-1})$ 时，差值 $|\\Delta \\beta_1|$ 对应于特征的消亡。这些消亡与活跃列表中诞生尺度最早的特征进行配对（遵循先进先出原则）。对于每一个这样的配对 $(r_{birth}, r_m)$，记录一个持续同调区间。\n    d. 在最终尺度 $r_M = 0.6$ 时仍留在活跃列表中的任何特征，被认为在观测范围内无限持续，并被赋予一个为 $r_M$ 的消亡时间。\n\n5.  **最大生命周期**：对于每个数据集，一个区间 $[r_{birth}, r_{death})$ 的生命周期是 $r_{death} - r_{birth}$。最终报告的值是在所有已识别的 $\\beta_1$ 区间中找到的最大生命周期。该值可以作为数据中最显著的环状特征的稳健指标。\n\n这种结构化的方法使我们能够系统地量化潜数据的拓扑“形状”，并由此推断出自动编码器学会区分的宏观相的属性。",
            "answer": "```python\nimport numpy as np\n\nclass UnionFind:\n    \"\"\"A simple Union-Find data structure.\"\"\"\n    def __init__(self, n):\n        self.parent = list(range(n))\n        self.num_components = n\n\n    def find(self, i):\n        if self.parent[i] == i:\n            return i\n        self.parent[i] = self.find(self.parent[i])\n        return self.parent[i]\n\n    def union(self, i, j):\n        root_i = self.find(i)\n        root_j = self.find(j)\n        if root_i != root_j:\n            self.parent[root_j] = root_i\n            self.num_components -= 1\n            return True\n        return False\n\ndef generate_points(case_name):\n    \"\"\"Generates point clouds based on the case name.\"\"\"\n    if case_name == 'R':\n        N, radius, center, sigma = 50, 1.0, (0.0, 0.0), 0.02\n        angles = np.random.uniform(0, 2 * np.pi, N)\n        points = np.zeros((N, 2))\n        points[:, 0] = center[0] + radius * np.cos(angles)\n        points[:, 1] = center[1] + radius * np.sin(angles)\n        points += np.random.normal(0, sigma, size=points.shape)\n        return points\n    elif case_name == 'B':\n        N, sigma = 50, 0.4\n        points = np.random.normal(0, sigma, size=(N, 2))\n        return points\n    elif case_name == '2R':\n        N_half, radius, centers, sigma = 40, 1.0, [(-2.0, 0.0), (2.0, 0.0)], 0.02\n        points = []\n        for center in centers:\n            angles = np.random.uniform(0, 2 * np.pi, N_half)\n            ring_points = np.zeros((N_half, 2))\n            ring_points[:, 0] = center[0] + radius * np.cos(angles)\n            ring_points[:, 1] = center[1] + radius * np.sin(angles)\n            ring_points += np.random.normal(0, sigma, size=ring_points.shape)\n            points.append(ring_points)\n        return np.vstack(points)\n    elif case_name == 'NR':\n        N, radius, center, sigma = 50, 1.0, (0.0, 0.0), 0.12\n        angles = np.random.uniform(0, 2 * np.pi, N)\n        points = np.zeros((N, 2))\n        points[:, 0] = center[0] + radius * np.cos(angles)\n        points[:, 1] = center[1] + radius * np.sin(angles)\n        points += np.random.normal(0, sigma, size=points.shape)\n        return points\n    elif case_name == 'SR':\n        N, radius, center, sigma = 20, 1.0, (0.0, 0.0), 0.02\n        angles = np.random.uniform(0, 2 * np.pi, N)\n        points = np.zeros((N, 2))\n        points[:, 0] = center[0] + radius * np.cos(angles)\n        points[:, 1] = center[1] + radius * np.sin(angles)\n        points += np.random.normal(0, sigma, size=points.shape)\n        return points\n    return None\n\ndef calculate_max_lifetime(points, r_min, r_max, M):\n    \"\"\"\n    Computes VR filtration and max Betti 1 lifetime.\n    \"\"\"\n    N = points.shape[0]\n    if N == 0:\n        return 0.0\n\n    # 1. Compute pairwise distances and sort edges\n    dists = np.sqrt(np.sum((points[:, np.newaxis, :] - points[np.newaxis, :, :])**2, axis=-1))\n    edges = []\n    for i in range(N):\n        for j in range(i + 1, N):\n            edges.append((dists[i, j], i, j))\n    edges.sort()\n\n    scales = np.linspace(r_min, r_max, M)\n    betti1_sequence = []\n    \n    # 2. Iterate through filtration scales\n    edge_idx = 0\n    uf = UnionFind(N)\n    adj = [set() for _ in range(N)]\n    num_edges = 0\n    num_triangles = 0\n    c0 = N\n\n    for r_m in scales:\n        while edge_idx  len(edges) and edges[edge_idx][0] = r_m:\n            dist, u, v = edges[edge_idx]\n            \n            # Update c1\n            num_edges += 1\n            \n            # Update c2 (count new triangles formed by adding edge (u,v))\n            num_new_triangles = len(adj[u].intersection(adj[v]))\n            num_triangles += num_new_triangles\n            \n            # Update graph and beta0\n            adj[u].add(v)\n            adj[v].add(u)\n            uf.union(u, v)\n\n            edge_idx += 1\n        \n        beta0 = uf.num_components\n        c1 = num_edges\n        c2 = num_triangles\n        \n        # Calculate beta_1 using Euler-Poincare formula approximation\n        beta1 = beta0 - c0 + c1 - c2\n        betti1_sequence.append(beta1)\n    \n    # 3. Extract birth-death intervals\n    births = []\n    intervals = []\n    prev_b1 = 0\n    for i, current_b1 in enumerate(betti1_sequence):\n        r_m = scales[i]\n        delta_b1 = current_b1 - prev_b1\n        if delta_b1  0:\n            for _ in range(delta_b1):\n                births.append(r_m)\n            births.sort()\n        elif delta_b1  0:\n            num_deaths = -delta_b1\n            for _ in range(min(num_deaths, len(births))):\n                birth_time = births.pop(0)\n                intervals.append((birth_time, r_m))\n        prev_b1 = current_b1\n        \n    # 4. Handle remaining births\n    for birth_time in births:\n        intervals.append((birth_time, r_max))\n        \n    # 5. Calculate max lifetime\n    if not intervals:\n        return 0.0\n        \n    max_lifetime = max(death - birth for birth, death in intervals)\n    return max_lifetime\n\ndef solve():\n    np.random.seed(0)\n    \n    cases = ['R', 'B', '2R', 'NR', 'SR']\n    \n    # Filtration parameters\n    r_min, r_max, M = 0.02, 0.6, 40\n    \n    results = []\n    for case_name in cases:\n        points = generate_points(case_name)\n        max_lifetime = calculate_max_lifetime(points, r_min, r_max, M)\n        results.append(max_lifetime)\n    \n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}