{
    "hands_on_practices": [
        {
            "introduction": "Before deploying an autoencoder to discover unknown order parameters, it is crucial to verify its ability to capture known ones in well-understood systems. This exercise provides a quantitative framework for this validation, using statistical correlation to measure the correspondence between the autoencoder's learned latent variable, $z$, and a known order parameter, $m$. By implementing the formal decision criterion detailed in this problem , you will develop a fundamental tool for benchmarking and validating your data-driven models in controlled scientific settings.",
            "id": "3828675",
            "problem": "Consider a dataset with $N$ samples indexing microscopic states $\\{x_i\\}_{i=1}^N$ of a multiscale system, where an Autoencoder (AE) trained on these states produces a one-dimensional latent variable $z_i \\in \\mathbb{R}$ for each sample $i$, and a known macroscopic Order Parameter (OP) $m_i \\in \\mathbb{R}$ is available for the same samples. Canonical Correlation Analysis (CCA) seeks linear combinations that maximize the correlation between two sets of variables. In general, for sets $X \\in \\mathbb{R}^p$ and $Y \\in \\mathbb{R}^q$, CCA solves $\\max_{\\alpha \\in \\mathbb{R}^p, \\beta \\in \\mathbb{R}^q} \\mathrm{corr}(\\alpha^\\top X, \\beta^\\top Y)$. When both sets are one-dimensional, i.e., $p=q=1$, CCA reduces to the absolute Pearson correlation between the standardized scalars $z$ and $m$. You are asked to formalize a decision criterion that tests whether the AE latent $z$ corresponds to the known OP $m$, and implement it as a program.\n\nYour formal decision criterion must be based on computing the sample Pearson correlation coefficient $r$ between $z$ and $m$ across the dataset, evaluating its statistical significance under the null hypothesis that the true correlation is zero, and then applying a threshold on the canonical correlation magnitude. Specifically, define the sample Pearson correlation coefficient\n$$\nr = \\frac{\\sum_{i=1}^N (z_i - \\bar{z})(m_i - \\bar{m})}{\\sqrt{\\sum_{i=1}^N (z_i - \\bar{z})^2} \\sqrt{\\sum_{i=1}^N (m_i - \\bar{m})^2}},\n$$\nwhere $\\bar{z} = \\frac{1}{N}\\sum_{i=1}^N z_i$ and $\\bar{m} = \\frac{1}{N}\\sum_{i=1}^N m_i$. Under the null hypothesis $H_0: \\rho = 0$, where $\\rho$ is the population correlation, and assuming samples are independent and identically distributed with finite variance, the statistic\n$$\nt = r \\sqrt{\\frac{N - 2}{1 - r^2}}\n$$\nfollows a Student $t$ distribution with $N - 2$ degrees of freedom. The two-sided $p$-value is\n$$\np = 2 \\left(1 - F_{t_{N-2}}(|t|)\\right),\n$$\nwhere $F_{t_{N-2}}$ is the cumulative distribution function of the Student $t$ distribution with $N - 2$ degrees of freedom. Let the decision threshold be a correlation magnitude $\\tau \\in [0,1]$ and significance level $\\alpha \\in (0,1)$. Your decision rule must be:\n- If either $\\mathrm{Var}(z) = 0$ or $\\mathrm{Var}(m) = 0$, declare that $z$ does not correspond to $m$.\n- Otherwise, compute $r$ and $p$ as above, and declare that $z$ corresponds to $m$ if and only if $|r| \\ge \\tau$ and $p \\le \\alpha$.\n\nAngles must be treated in radians for any trigonometric functions. The program must implement this criterion and evaluate the following test suite of deterministic cases, each defined by $(z, m)$ pairs constructed from explicit formulas over index $i$:\n\n- Case $1$ (general happy path): $N = 200$, $m_i = \\sin\\left(\\frac{2\\pi i}{N}\\right)$, $z_i = 1.1 \\, m_i$. Use $\\tau = 0.8$ and $\\alpha = 0.01$. Angle unit is radians. Expected behavior: strong positive canonical correlation.\n- Case $2$ (orthogonality edge case): $N = 200$, $m_i = \\sin\\left(\\frac{2\\pi i}{N}\\right)$, $z_i = \\cos\\left(\\frac{2\\pi i}{N}\\right)$. Use $\\tau = 0.8$ and $\\alpha = 0.01$. Angle unit is radians. Expected behavior: negligible canonical correlation.\n- Case $3$ (negative correlation general case): $N = 150$, $m_i$ linearly spaced as $m_i = -1 + \\frac{2(i-1)}{N-1}$, $z_i = - m_i + 0.05 \\, \\sin\\left(\\frac{4\\pi i}{N}\\right)$. Use $\\tau = 0.8$ and $\\alpha = 0.01$. Angle unit is radians. Expected behavior: strong negative canonical correlation.\n- Case $4$ (degenerate variance boundary): $N = 100$, $m_i = 0$ for all $i$, $z_i = \\cos\\left(\\frac{2\\pi i}{N}\\right)$. Use $\\tau = 0.8$ and $\\alpha = 0.01$. Angle unit is radians. Expected behavior: undefined correlation due to zero variance in $m$ must be treated as rejection.\n- Case $5$ (small-sample moderate correlation, significance edge): $N = 20$, $m_i = -1 + \\frac{2(i-1)}{N-1}$, $z_i = 0.4 \\, m_i + 0.9 \\, \\sin\\left(\\frac{2\\pi i}{N}\\right)$. Use $\\tau = 0.8$ and $\\alpha = 0.01$. Angle unit is radians. Expected behavior: correlation magnitude below the threshold and likely not statistically significant at the given $\\alpha$.\n\nYour program must produce a single line of output containing the results of the decision rule for the five cases, as a comma-separated list of boolean values enclosed in square brackets, in the order of the cases listed above (e.g., `[result_1,result_2,result_3,result_4,result_5]`). The output values must be of type boolean, and there must be no additional text printed.",
            "solution": "The problem requires the formalization and implementation of a decision criterion to assess whether a one-dimensional latent variable $z$, learned by an autoencoder from a set of microscopic states, corresponds to a known macroscopic order parameter $m$. The correspondence is evaluated based on the strength and statistical significance of the linear relationship between the two variables, $z$ and $m$.\n\nThe core of the decision criterion rests upon the sample Pearson correlation coefficient, $r$, and an associated hypothesis test. For two sets of paired data $\\{z_i\\}_{i=1}^N$ and $\\{m_i\\}_{i=1}^N$, the sample Pearson correlation coefficient is given by:\n$$\nr = \\frac{\\sum_{i=1}^N (z_i - \\bar{z})(m_i - \\bar{m})}{\\sqrt{\\sum_{i=1}^N (z_i - \\bar{z})^2} \\sqrt{\\sum_{i=1}^N (m_i - \\bar{m})^2}}\n$$\nwhere $\\bar{z} = \\frac{1}{N}\\sum_{i=1}^N z_i$ and $\\bar{m} = \\frac{1}{N}\\sum_{i=1}^N m_i$ are the sample means. In the context of Canonical Correlation Analysis (CCA) between two one-dimensional variables, the maximized correlation is simply the absolute value of the Pearson coefficient, $|r|$.\n\nThe decision criterion is a two-part test with specified thresholds: a minimum correlation magnitude $\\tau$ and a statistical significance level $\\alpha$.\n\nStep 1: Pre-condition Validation.\nThe Pearson correlation coefficient is undefined if either of the variables has zero variance. A variable with zero variance is a constant and cannot covary with another variable. Therefore, the first step of the criterion is to check the sample variances, $\\mathrm{Var}(z)$ and $\\mathrm{Var}(m)$. If either $\\mathrm{Var}(z) = 0$ or $\\mathrm{Var}(m) = 0$, the variables cannot be meaningfully correlated. In this case, we conclude that $z$ does not correspond to $m$. In a numerical implementation, this check must be performed with a small tolerance to account for floating-point arithmetic limitations.\n\nStep 2: Correlation Magnitude Test.\nIf both variables have non-zero variance, we proceed to compute $r$. The first part of the decision rule assesses the strength of the correlation. The latent variable $z$ is considered a potential representation of the order parameter $m$ only if the magnitude of their linear relationship is sufficiently large. This is formalized by the condition:\n$$\n|r| \\ge \\tau\n$$\nIf this condition is not met, we conclude that the correspondence is too weak, and the criterion fails.\n\nStep 3: Statistical Significance Test.\nA large sample correlation $r$ might occur by chance, especially in small datasets. To guard against this, we test the statistical significance of the observed correlation. We formulate a null hypothesis, $H_0$, that there is no correlation in the underlying population from which the samples are drawn, i.e., the population correlation coefficient $\\rho$ is zero ($H_0: \\rho = 0$).\n\nUnder the assumption that the samples $(z_i, m_i)$ are independent and identically distributed from a bivariate normal distribution (or for large $N$ via the Central Limit Theorem), the test statistic\n$$\nt = r \\sqrt{\\frac{N - 2}{1 - r^2}}\n$$\nfollows a Student's $t$-distribution with $N - 2$ degrees of freedom. A large absolute value of $t$ provides evidence against the null hypothesis.\n\nFrom the calculated $t$-statistic, we determine the two-sided $p$-value, which is the probability of observing a correlation at least as extreme as $r$ if the null hypothesis were true. The $p$-value is given by:\n$$\np_{\\text{value}} = 2 \\left(1 - F_{t_{N-2}}(|t|)\\right)\n$$\nwhere $F_{t_{N-2}}$ is the cumulative distribution function (CDF) of the Student's $t$-distribution with $N - 2$ degrees of freedom. The correlation is deemed statistically significant if this $p$-value is less than or equal to the specified significance level $\\alpha$:\n$$\np_{\\text{value}} \\le \\alpha\n$$\n\nA special case arises when $|r|=1$. The formula for the $t$-statistic would involve division by zero. However, a perfect correlation ($|r|=1$) is the maximum possible effect size, implying that the probability of observing a more extreme result is zero. Thus, for $|r|=1$, the $p$-value is $0$, and the significance test is always passed for any $\\alpha > 0$.\n\nFinal Decision Rule:\nCombining these steps, the latent variable $z$ is declared to correspond to the order parameter $m$ if and only if all three conditions are met:\n1. $\\mathrm{Var}(z) > 0$ and $\\mathrm{Var}(m) > 0$.\n2. The correlation magnitude is sufficient: $|r| \\ge \\tau$.\n3. The correlation is statistically significant: $p_{\\text{value}} \\le \\alpha$.\n\nThis procedure will be implemented and applied to the five specified test cases. For implementation, we will utilize numerical libraries to ensure robust calculation of the variance, correlation coefficient, and the Student's $t$-distribution CDF.\n- Case $1$: $z_i = 1.1 m_i$, so $r=1$. $|r| = 1 \\ge 0.8$ and $p_{\\text{value}} = 0 \\le 0.01$. The criterion passes.\n- Case $2$: $m_i$ and $z_i$ are orthogonal sine and cosine functions over a full period. Their sample correlation $r$ will be close to $0$. The condition $|r| \\ge 0.8$ will fail.\n- Case $3$: $z_i$ is a strongly anti-correlated version of $m_i$ with minor noise. We expect $r$ to be close to $-1$. Therefore, $|r| \\approx 1 \\ge 0.8$. With $N=150$, this strong correlation will be highly significant, so $p_{\\text{value}} \\ll 0.01$. The criterion passes.\n- Case $4$: $m_i$ is a constant vector, so $\\mathrm{Var}(m) = 0$. The pre-condition fails, and the criterion immediately returns a negative result.\n- Case $5$: A moderate correlation is expected, but the sample size $N=20$ is small, and the noise term is large. The resulting correlation magnitude $|r|$ is unlikely to meet the high threshold of $\\tau = 0.8$. Furthermore, even if it did, the small sample size may render the result not statistically significant at the strict $\\alpha=0.01$ level. The criterion is expected to fail.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as student_t\n\ndef solve():\n    \"\"\"\n    Main function to evaluate the decision criterion for all test cases.\n    \"\"\"\n\n    def decision_criterion(z, m, N, tau, alpha):\n        \"\"\"\n        Applies the formal decision criterion to determine if latent variable z\n        corresponds to order parameter m.\n\n        Args:\n            z (np.ndarray): The latent variable vector.\n            m (np.ndarray): The order parameter vector.\n            N (int): The number of samples.\n            tau (float): The correlation magnitude threshold.\n            alpha (float): The significance level.\n\n        Returns:\n            bool: True if z corresponds to m, False otherwise.\n        \"\"\"\n        # Step 1: Pre-condition Validation (non-zero variance)\n        # Using a small tolerance for floating-point comparison.\n        if np.var(m) < 1e-15 or np.var(z) < 1e-15:\n            return False\n\n        # Compute sample Pearson correlation coefficient r\n        # np.corrcoef returns a 2x2 matrix, the value is at [0, 1] or [1, 0]\n        r = np.corrcoef(z, m)[0, 1]\n\n        # Step 2: Correlation Magnitude Test\n        if abs(r) < tau:\n            return False\n\n        # Step 3: Statistical Significance Test\n        # Handle the edge case of perfect correlation to avoid division by zero.\n        # If |r| is extremely close to 1, the p-value is effectively 0.\n        if abs(r) > 1.0 - 1e-9:\n            p_val = 0.0\n        else:\n            # Calculate the t-statistic\n            t_stat = r * np.sqrt((N - 2) / (1 - r**2))\n            \n            # Degrees of freedom\n            df = N - 2\n            \n            # Calculate the two-sided p-value using the Student's t-distribution CDF\n            p_val = 2 * (1 - student_t.cdf(abs(t_stat), df))\n\n        # Final decision: True if p-value is below significance level\n        return p_val <= alpha\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (N, m_func, z_func, tau, alpha)\n    test_cases = [\n        (200, lambda i, N: np.sin(2 * np.pi * i / N), lambda i, N: 1.1 * np.sin(2 * np.pi * i / N), 0.8, 0.01),\n        (200, lambda i, N: np.sin(2 * np.pi * i / N), lambda i, N: np.cos(2 * np.pi * i / N), 0.8, 0.01),\n        (150, lambda i, N: -1 + 2 * (i - 1) / (N - 1), lambda i, N: -(-1 + 2 * (i - 1) / (N - 1)) + 0.05 * np.sin(4 * np.pi * i / N), 0.8, 0.01),\n        (100, lambda i, N: np.zeros_like(i, dtype=float), lambda i, N: np.cos(2 * np.pi * i / N), 0.8, 0.01),\n        (20, lambda i, N: -1 + 2 * (i - 1) / (N - 1), lambda i, N: 0.4 * (-1 + 2 * (i - 1) / (N - 1)) + 0.9 * np.sin(2 * np.pi * i / N), 0.8, 0.01),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, m_func, z_func, tau, alpha = case\n        \n        # The problem states index i from 1 to N\n        i_indices = np.arange(1, N + 1)\n        \n        m_vec = m_func(i_indices, N)\n        z_vec = z_func(i_indices, N)\n        \n        result = decision_criterion(z_vec, m_vec, N, tau, alpha)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Python's str() for booleans is 'True'/'False' (capitalized).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A low reconstruction error does not guarantee that an autoencoder has learned a physically meaningful representation; the model might achieve accuracy without capturing the underlying structure of the physical system. A more robust evaluation involves checking whether the model's reconstructions adhere to fundamental conservation laws, such as the conservation of mass or energy. This practice  guides you in implementing a \"physicality score\" that quantifies how well the autoencoder respects these global invariants, moving your evaluation beyond simple pixel-wise accuracy to true physical consistency.",
            "id": "3828693",
            "problem": "You are given a one-dimensional periodic scalar field $u(x)$ on the interval $[0,2\\pi)$, discretized uniformly with $N$ grid points and grid spacing $\\Delta x = \\frac{2\\pi}{N}$. Suppose an autoencoder is used to compress $u(x)$ to a latent variable and decode it back to a reconstruction $\\hat{u}(x)$. In multiscale modeling and analysis, meaningful latent variables (order parameters) should encode physically relevant features that respect global conservation laws. To assess this, implement a consistency check that quantifies whether decoder reconstructions conserve prescribed global invariants. Use the following fundamental base:\n\n- For periodic systems where advection-like or Hamiltonian dynamics are dominant and dissipation is negligible, global invariants such as total mass and quadratic energy are conserved. Define the mass as $I_1[u] = \\int_0^{2\\pi} u(x)\\,dx$ and the quadratic energy as $I_2[u] = \\int_0^{2\\pi} u(x)^2\\,dx$.\n- On a uniform grid with spacing $\\Delta x$, approximate these integrals by Riemann sums, i.e., $I_1[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j$ and $I_2[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j^2$, where $u_j = u(x_j)$ and $x_j = j\\Delta x$.\n- The conservation check compares invariants of the original field $u$ and the reconstruction $\\hat{u}$. Let $\\Delta_k = I_k[\\hat{u}] - I_k[u]$ for $k \\in \\{1,2\\}$.\n\nDefine a dimensionless deviation for each invariant by normalizing absolute differences with physically meaningful scales:\n- For mass, use $d_1 = \\frac{|\\Delta_1|}{\\max\\left(|I_1[u]|, \\|u\\|_{L^1}\\right)}$, where $\\|u\\|_{L^1} \\approx \\Delta x \\sum_{j=0}^{N-1} |u_j|$.\n- For energy, use $d_2 = \\frac{|\\Delta_2|}{\\max\\left(|I_2[u]|, \\epsilon\\right)}$, where $\\epsilon$ is any positive lower bound to avoid division by zero if $I_2[u]=0$. In this problem, the test suite avoids $I_2[u]=0$, so you may set $\\epsilon$ to $0$.\n\nAggregate these deviations into a scalar latent physicality score\n$$\nS = \\exp\\left(-\\frac{d_1 + d_2}{2}\\right),\n$$\nso that $S \\in (0,1]$, $S=1$ for exact conservation, and $S$ decays as deviations increase.\n\nImplement a program that:\n- Constructs a base field $u_{\\text{base}}(x) = \\sin(x) + 0.5\\cos(2x)$ on $[0,2\\pi)$ using $N=512$ uniformly spaced points with angles in radians.\n- Uses the following four test cases to define reconstructions $\\hat{u}$ from $u_{\\text{base}}$:\n    1. Case A (happy path): exact reconstruction $\\hat{u}(x) = u_{\\text{base}}(x)$.\n    2. Case B (spectral Gaussian blur): in Fourier space, apply a Gaussian filter with standard deviation $\\sigma = 0.2$ (in the $x$ coordinate) to $u_{\\text{base}}$ and inverse transform to real space. This should preserve the zero-wavenumber mode and thus mass, while reducing high-frequency energy.\n    3. Case C (amplitude scaling): $\\hat{u}(x) = s \\, u_{\\text{base}}(x)$ with $s = 1.1$.\n    4. Case D (additive bias): $\\hat{u}(x) = u_{\\text{base}}(x) + b$ with $b = 0.05$.\n\nFor each case, compute $I_1[u]$, $I_2[u]$, $I_1[\\hat{u}]$, $I_2[\\hat{u}]$, the deviations $d_1$ and $d_2$ as defined above, and the scalar score $S$. Angles are in radians. All quantities are dimensionless in this problem.\n\nYour program should produce a single line of output containing the four scores $S$ for cases A, B, C, and D, respectively, rounded to six decimal places, as a comma-separated list enclosed in square brackets (e.g., \"[0.999999,0.912345,0.812345,0.456789]\").\n\nTest suite details to implement:\n- Domain length $L = 2\\pi$.\n- Number of grid points $N = 512$.\n- Gaussian blur parameter $\\sigma = 0.2$ (in $x$ units).\n- Scaling factor $s = 1.1$.\n- Additive bias $b = 0.05$.\n\nThe final output is a list of four floats as specified. No user input is required.",
            "solution": "The problem requires the implementation of a physicality score $S$ to assess whether a reconstructed field $\\hat{u}(x)$ conserves key global invariants of an original field $u(x)$. The score is based on the conservation of total mass, $I_1[u]$, and total quadratic energy, $I_2[u]$, for a one-dimensional periodic scalar field on the domain $[0, 2\\pi)$. The solution proceeds through the following steps: defining the discrete system, calculating the base field and its invariants, generating four distinct reconstructed fields, and for each, computing the deviation-based score.\n\nFirst, we establish the computational domain. The field $u(x)$ is discretized on a uniform grid of $N=512$ points, denoted $x_j = j \\Delta x$ for $j \\in \\{0, 1, \\dots, N-1\\}$. The grid spacing is $\\Delta x = \\frac{2\\pi}{N}$. The base field is given by the function $u_{\\text{base}}(x) = \\sin(x) + 0.5\\cos(2x)$. We evaluate this function on our discrete grid to obtain the vector $u_{\\text{base},j} = u_{\\text{base}}(x_j)$.\n\nNext, we define the numerical approximations for the global invariants. The total mass, $I_1[u] = \\int_0^{2\\pi} u(x)\\,dx$, and the total quadratic energy, $I_2[u] = \\int_0^{2\\pi} u(x)^2\\,dx$, are approximated using a Riemann sum:\n$$I_1[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j$$\n$$I_2[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j^2$$\nSimilarly, the $L^1$-norm, $\\|u\\|_{L^1} = \\int_0^{2\\pi} |u(x)|\\,dx$, required for normalization, is approximated as:\n$$\\|u\\|_{L^1} \\approx \\Delta x \\sum_{j=0}^{N-1} |u_j|$$\nUsing these formulas, we calculate the invariants $I_1[u_{\\text{base}}]$, $I_2[u_{\\text{base}}]$, and the norm $\\|u_{\\text{base}}\\|_{L^1}$ for the discrete base field. Analytically, $I_1[u_{\\text{base}}] = 0$, while $I_2[u_{\\text{base}}] = 1.25\\pi$. Our numerical calculation will yield values very close to these.\n\nThe core of the problem involves evaluating the physicality score for four different reconstructed fields, $\\hat{u}(x)$, derived from $u_{\\text{base}}(x)$.\n\nCase A (Exact Reconstruction): $\\hat{u}(x) = u_{\\text{base}}(x)$. In this trivial case, the reconstruction is perfect.\n\nCase B (Spectral Gaussian Blur): This reconstruction simulates a common form of information loss where high-frequency details are dampened. This is achieved in Fourier space. First, we compute the discrete Fourier transform (DFT) of the base field, $\\tilde{u}_{\\text{base}} = \\mathcal{F}\\{u_{\\text{base}}\\}$. The corresponding angular wavenumbers are given by the vector $k$, where $k_m = 2\\pi f_m$ and $f_m$ are the discrete frequencies provided by a standard FFT frequency function for a signal of length $N$ with sample spacing $\\Delta x$. We then define a Gaussian filter in the Fourier domain:\n$$G(k) = \\exp\\left(-\\frac{k^2 \\sigma^2}{2}\\right)$$\nwith the given standard deviation in real space, $\\sigma = 0.2$. The filtered field in Fourier space is $\\tilde{\\hat{u}} = \\tilde{u}_{\\text{base}} \\odot G$, where $\\odot$ denotes element-wise multiplication. The reconstructed field $\\hat{u}(x)$ is then obtained by applying the inverse DFT, $\\hat{u} = \\text{Re}(\\mathcal{F}^{-1}\\{\\tilde{\\hat{u}}\\})$. The real part is taken to discard negligible imaginary components arising from numerical precision errors.\n\nCase C (Amplitude Scaling): This case models a simple gain error, where the reconstruction is uniformly scaled: $\\hat{u}(x) = s \\cdot u_{\\text{base}}(x)$ with a scaling factor of $s = 1.1$.\n\nCase D (Additive Bias): This models a constant offset error: $\\hat{u}(x) = u_{\\text{base}}(x) + b$, with a bias of $b = 0.05$.\n\nFor each of these four cases, we compute the physicality score $S$. First, we calculate the invariants for the reconstructed field, $I_1[\\hat{u}]$ and $I_2[\\hat{u}]$. Then, we find the differences from the base field's invariants: $\\Delta_1 = I_1[\\hat{u}] - I_1[u_{\\text{base}}]$ and $\\Delta_2 = I_2[\\hat{u}] - I_2[u_{\\text{base}}]$.\n\nThese differences are converted into dimensionless deviations, $d_1$ and $d_2$, through normalization:\n$$d_1 = \\frac{|\\Delta_1|}{\\max\\left(|I_1[u_{\\text{base}}]|, \\|u_{\\text{base}}\\|_{L^1}\\right)}$$\n$$d_2 = \\frac{|\\Delta_2|}{|I_2[u_{\\text{base}}]|}$$\nThe denominator for $d_1$ robustly handles cases where the mean of the field is zero, which is true for $u_{\\text{base}}$. The denominator for $d_2$ is safe as $I_2[u_{\\text{base}}]$ is strictly positive since the test suite avoids $I_2[u] = 0$.\n\nFinally, the scalar physicality score $S$ is calculated by aggregating the deviations:\n$$S = \\exp\\left(-\\frac{d_1 + d_2}{2}\\right)$$\nThis score maps the two deviations onto the interval $(0, 1]$, where $S=1$ indicates perfect conservation of both invariants, and the score decreases as the conservation errors grow. The procedure is repeated for each of the four cases, and the resulting scores are formatted as a list.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a physicality score for four test cases of field reconstructions\n    based on the conservation of mass and energy invariants.\n    \"\"\"\n    # Define problem parameters\n    N = 512\n    L = 2.0 * np.pi\n    dx = L / N\n    sigma = 0.2\n    s = 1.1\n    b = 0.05\n\n    # Construct the grid and base field\n    x = np.linspace(0.0, L, N, endpoint=False)\n    u_base = np.sin(x) + 0.5 * np.cos(2.0 * x)\n\n    # --- Define test cases for reconstructions ---\n    test_cases = []\n\n    # Case A: Exact reconstruction\n    u_hat_A = u_base\n    test_cases.append(u_hat_A)\n\n    # Case B: Spectral Gaussian blur\n    # Compute Fourier transform and corresponding wavenumbers\n    u_base_tilde = np.fft.fft(u_base)\n    # Frequencies in cycles per unit of sample spacing\n    # `d=dx` means the unit is physical length\n    freq = np.fft.fftfreq(N, d=dx)\n    # Convert to angular wavenumbers (rad/length)\n    k = 2.0 * np.pi * freq\n    # Define Gaussian filter in Fourier space\n    gaussian_filter = np.exp(-(k**2 * sigma**2) / 2.0)\n    # Apply filter and inverse transform, taking real part to remove numerical noise\n    u_hat_tilde_B = u_base_tilde * gaussian_filter\n    u_hat_B = np.fft.ifft(u_hat_tilde_B).real\n    test_cases.append(u_hat_B)\n\n    # Case C: Amplitude scaling\n    u_hat_C = s * u_base\n    test_cases.append(u_hat_C)\n\n    # Case D: Additive bias\n    u_hat_D = u_base + b\n    test_cases.append(u_hat_D)\n\n    # --- Calculate invariants for the base field ---\n    # Riemann sum approximation of the integral\n    I1_base = np.sum(u_base) * dx\n    I2_base = np.sum(u_base**2) * dx\n    L1_norm_base = np.sum(np.abs(u_base)) * dx\n\n    scores = []\n    # --- Process each test case to calculate its score ---\n    for u_hat in test_cases:\n        # Calculate invariants for the reconstructed field\n        I1_hat = np.sum(u_hat) * dx\n        I2_hat = np.sum(u_hat**2) * dx\n\n        # Calculate differences and dimensionless deviations\n        delta_1 = I1_hat - I1_base\n        delta_2 = I2_hat - I2_base\n\n        # Normalization for d1, robust to I1_base being near zero\n        d1_denom = np.maximum(np.abs(I1_base), L1_norm_base)\n        d1 = np.abs(delta_1) / d1_denom if d1_denom != 0 else 0.0\n\n        # Normalization for d2\n        # Problem statement guarantees I2_base is not zero for the given test suite\n        d2 = np.abs(delta_2) / np.abs(I2_base)\n        \n        # Calculate the final physicality score\n        score = np.exp(-(d1 + d2) / 2.0)\n        scores.append(score)\n\n    # Format the output as a comma-separated list with 6 decimal places\n    formatted_scores = [f\"{score:.6f}\" for score in scores]\n    print(f\"[{','.join(formatted_scores)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal of this data-driven approach is to discover novel order parameters in complex systems where they are not known a priori. Such parameters may not be simple scalar quantities but could possess a non-trivial topology, such as the periodic angle describing a spin system's collective orientation. This advanced exercise  introduces the powerful methods of Topological Data Analysis (TDA), enabling you to probe the very shape of the learned latent space for features like loops ($\\beta_1 \\gt 0$) that signal the presence of these complex, topologically significant order parameters.",
            "id": "3828633",
            "problem": "You are given synthetic latent point clouds that represent low-dimensional codes produced by an autoencoder trained to capture macroscopic order parameters from microscopic configurations. The goal is to verify the presence of nontrivial topological features associated with distinct phases by computing persistent homology on the latent point clouds and to interpret the implications of these features for macroscopic behavior within a multiscale framework.\n\nStart from the following base definitions and facts:\n- An order parameter is a function that maps a microscopic state to a macroscopic descriptor that distinguishes phases. When an autoencoder learns a latent representation that preserves the salient macroscopic structure, its latent codes may carry topological signatures of phases.\n- The Vietoris–Rips complex at scale $r$ on a point cloud $X \\subset \\mathbb{R}^d$ is the abstract simplicial complex whose vertices are points in $X$ and whose $k$-simplices are sets of $k+1$ points with pairwise Euclidean distances less than or equal to $r$.\n- Betti numbers $\\beta_k$ count $k$-dimensional holes in a topological space: $\\beta_0$ counts connected components, $\\beta_1$ counts one-dimensional cycles (loops).\n- Persistent homology tracks the birth and death of homological features across a filtration parameter $r$, quantifying their lifetimes to distinguish signal from noise.\n\nYour task is to implement, from first principles, an algorithm that:\n- Constructs the Vietoris–Rips filtration on latent point clouds using the Euclidean metric.\n- Computes the number of connected components $\\beta_0$ using a union–find structure across the edges that appear as $r$ increases.\n- Computes the number of one-dimensional cycles $\\beta_1$ at each scale $r$ by correctly accounting for edges and filled triangles in the clique complex built from pairwise proximity, and uses a principled transformation from count data to $\\beta_1$ that is consistent with the Euler characteristic for a flag complex when higher-dimensional homology is negligible in the latent data considered.\n- Approximates persistent homology in dimension $1$ by extracting birth–death intervals for $\\beta_1$ from its changes across the discrete filtration scales, and summarizes each dataset by the maximum lifetime of any $\\beta_1$ feature.\n\nAngle units must be in radians for any parametric constructions. No physical units are involved. All numerical outputs must be real numbers. The final results must be expressed as floating-point numbers.\n\nImplement a program that generates the following latent point clouds (each in $\\mathbb{R}^2$):\n- Case R (single ring): $N = 50$ points sampled on a circle of radius $1$ centered at $(0,0)$, with independent Gaussian noise of standard deviation $\\sigma = 0.02$ added to each coordinate. Angles are sampled uniformly in $[0,2\\pi)$ in radians.\n- Case B (blob): $N = 50$ points sampled from a zero-mean isotropic Gaussian with standard deviation $0.4$ per coordinate.\n- Case 2R (two disjoint rings): $N = 80$ points, with $40$ points on a circle of radius $1$ centered at $(-2, 0)$ and $40$ points on a circle of radius $1$ centered at $(2, 0)$; independent Gaussian noise $\\sigma = 0.02$ per coordinate; angles uniform in $[0,2\\pi)$ in radians.\n- Case NR (noisy ring): $N = 50$ points sampled on a circle of radius $1$ centered at $(0,0)$, with independent Gaussian noise $\\sigma = 0.12$ per coordinate; angles uniform in $[0,2\\pi)$ in radians.\n- Case SR (sparse ring): $N = 20$ points sampled on a circle of radius $1$ centered at $(0,0)$, with independent Gaussian noise $\\sigma = 0.02$ per coordinate; angles uniform in $[0,2\\pi)$ in radians.\n\nOn each dataset, compute the Vietoris–Rips filtration on $r \\in \\{ r_1, r_2, \\dots, r_M \\}$ where $M = 40$ and $r_m$ are linearly spaced in $[0.02, 0.6]$. At each scale $r_m$, compute $\\beta_0(r_m)$ and $\\beta_1(r_m)$ consistent with the clique complex formed by edges with pairwise distances less than or equal to $r_m$ and filled $2$-simplices (triangles) when all three corresponding edges are present. From the sequence $\\{ \\beta_1(r_m) \\}_{m=1}^M$, extract approximate birth–death intervals of one-dimensional features by treating each increase in $\\beta_1$ as births at that $r_m$ and each decrease as deaths at that $r_m$, pairing them in a way that respects the filtration order. For any remaining births at the largest scale, record deaths at $r = 0.6$.\n\nFor each dataset, report a single float equal to the maximum lifetime among the extracted $\\beta_1$ intervals. Your program should produce a single line of output containing these five floats as a comma-separated list enclosed in square brackets (e.g., \"[x_R,x_B,x_2R,x_NR,x_SR]\").\n\nTest Suite Design:\n- Coverage of distinct phases: Case R should exhibit a nontrivial loop ($\\beta_1$ feature with a substantial lifetime), Case B should not, Case 2R should exhibit two loops and therefore at least one long lifetime, Case NR should retain a loop but with reduced lifetime relative to Case R, Case SR should challenge detection due to undersampling and produce short lifetimes.\n- Boundary conditions: The minimal scale $r_1 = 0.02$ should yield isolated points (large $\\beta_0$, zero $\\beta_1$), while the maximal scale $r_M = 0.6$ should yield highly connected complexes where loops are filled.\n- Quantifiable answers: The output is the list of maximum lifetimes (floats) for the five cases in the exact final output format specified.",
            "solution": "The problem requires the computation and interpretation of topological features, specifically one-dimensional cycles (loops), from synthetic latent point clouds. This task falls under the purview of Topological Data Analysis (TDA), a field that uses tools from algebraic topology to analyze the shape of data. The core of the solution lies in implementing an algorithm to compute persistent homology, which quantifies the birth and death of topological features across a sequence of scales.\n\nThe underlying principle is that a point cloud, which is itself topologically trivial (a discrete set of points), can be endowed with a richer structure by constructing a sequence of simplicial complexes, known as a filtration. We will use the Vietoris–Rips (VR) complex, which is determined by a proximity parameter $r$. For a given set of points $X$, the VR complex $VR(X, r)$ contains a $k$-simplex for every subset of $k+1$ points in $X$ that are pairwise within a Euclidean distance of $r$.\n\nOur primary goal is to track the Betti numbers, $\\beta_k$, of these complexes as $r$ increases. $\\beta_0$ counts the number of connected components, and $\\beta_1$ counts the number of one-dimensional cycles or loops. Persistent homology captures the lifespan of these features, represented as birth-death intervals $[r_{birth}, r_{death})$. Features with long lifetimes are considered significant topological signals, while those with short lifetimes are often attributed to noise.\n\nThe algorithm proceeds in the following steps:\n\n1.  **Data Generation**: For each of the five cases, a point cloud in $\\mathbb{R}^2$ is generated according to the specified parameters. A fixed random seed is used to ensure reproducibility. The cases are designed to represent distinct topological scenarios: a single perfect loop (Case R), a contractible blob (Case B), two disjoint loops (Case 2R), a noisy loop (Case NR), and a sparse loop (Case SR).\n\n2.  **Vietoris-Rips Filtration**: Instead of rebuilding the VR complex at each of the $M=40$ scales $r_m \\in [0.02, 0.6]$, a more efficient incremental approach is taken.\n    a. All pairwise Euclidean distances between the $N$ points are computed, forming a list of potential edges.\n    b. These edges are sorted by distance in ascending order. This sorted list dictates the sequence in which simplices are added to the complex.\n\n3.  **Calculation of Betti Numbers**: We iterate through the discrete scales $r_m$ and, for each scale, update the counts of simplices and components by processing all edges shorter than or equal to the current $r_m$.\n    a. **$\\beta_0(r)$**: The number of connected components is tracked using a Union-Find or Disjoint Set Union (DSU) data structure. Initially, with $N$ points, $\\beta_0 = N$. As an edge $(u, v)$ is added to the complex, if $u$ and $v$ are in different components, they are merged, and $\\beta_0$ is decremented by $1$.\n    b. **$\\beta_1(r)$**: The first Betti number is calculated using an approximation derived from the Euler-Poincaré formula for a simplicial complex $K$: $\\chi(K) = \\sum_{k=0}^{\\infty} (-1)^k c_k = \\sum_{k=0}^{\\infty} (-1)^k \\beta_k$, where $c_k$ is the number of $k$-simplices. For our point clouds in $\\mathbb{R}^2$, we can assume higher-dimensional homology is negligible (i.e., $\\beta_k \\approx 0$ for $k \\ge 2$). The VR complex is a flag complex, meaning we consider all cliques in the graph as filled simplices. The formula thus simplifies to $c_0 - c_1 + c_2 \\approx \\beta_0 - \\beta_1$. Rearranging for $\\beta_1$, we get the calculation formula for each scale $r$:\n    $$\n    \\beta_1(r) \\approx \\beta_0(r) - c_0(r) + c_1(r) - c_2(r)\n    $$\n    Here, $c_0(r)=N$ is the number of vertices, $c_1(r)$ is the number of edges with length $\\le r$, $c_2(r)$ is the number of $2$-simplices (triangles) where all three edges have length $\\le r$, and $\\beta_0(r)$ is the number of connected components. The algorithm incrementally updates $c_1$, $c_2$, and $\\beta_0$ as it sweeps through the sorted edges up to the current scale $r_m$. A new triangle is formed by adding an edge $(u, v)$ for each existing common neighbor of $u$ and $v$.\n\n4.  **Persistence Interval Extraction**: After computing the sequence $\\{\\beta_1(r_m)\\}_{m=1}^M$, we extract the birth-death intervals for the one-dimensional features. A simple greedy pairing algorithm is employed:\n    a. A list of active feature birth scales is maintained.\n    b. When $\\beta_1(r_m) > \\beta_1(r_{m-1})$, the difference $\\Delta \\beta_1 = \\beta_1(r_m) - \\beta_1(r_{m-1})$ corresponds to $\\Delta \\beta_1$ new features born at scale $r_m$. Their birth scales are added to the active list.\n    c. When $\\beta_1(r_m) < \\beta_1(r_{m-1})$, the difference $|\\Delta \\beta_1|$ corresponds to feature deaths. These deaths are paired with the features that have the earliest birth scales in the active list (a First-In, First-Out discipline). For each such pair $(r_{birth}, r_m)$, a persistence interval is recorded.\n    d. Any features remaining in the active list at the final scale $r_M = 0.6$ are considered to persist indefinitely within the observed range and are assigned a death time of $r_M$.\n\n5.  **Maximum Lifetime**: For each dataset, the lifetime of an interval $[r_{birth}, r_{death})$ is $r_{death} - r_{birth}$. The final reported value is the maximum lifetime found among all identified $\\beta_1$ intervals. This value serves as a robust indicator of the most prominent loop-like feature in the data.\n\nThis structured approach allows us to systematically quantify the topological \"shape\" of the latent data and, by extension, infer properties about the macroscopic phases the autoencoder has learned to distinguish.",
            "answer": "```python\nimport numpy as np\n\nclass UnionFind:\n    \"\"\"A simple Union-Find data structure.\"\"\"\n    def __init__(self, n):\n        self.parent = list(range(n))\n        self.num_components = n\n\n    def find(self, i):\n        if self.parent[i] == i:\n            return i\n        self.parent[i] = self.find(self.parent[i])\n        return self.parent[i]\n\n    def union(self, i, j):\n        root_i = self.find(i)\n        root_j = self.find(j)\n        if root_i != root_j:\n            self.parent[root_j] = root_i\n            self.num_components -= 1\n            return True\n        return False\n\ndef generate_points(case_name):\n    \"\"\"Generates point clouds based on the case name.\"\"\"\n    if case_name == 'R':\n        N, radius, center, sigma = 50, 1.0, (0.0, 0.0), 0.02\n        angles = np.random.uniform(0, 2 * np.pi, N)\n        points = np.zeros((N, 2))\n        points[:, 0] = center[0] + radius * np.cos(angles)\n        points[:, 1] = center[1] + radius * np.sin(angles)\n        points += np.random.normal(0, sigma, size=points.shape)\n        return points\n    elif case_name == 'B':\n        N, sigma = 50, 0.4\n        points = np.random.normal(0, sigma, size=(N, 2))\n        return points\n    elif case_name == '2R':\n        N_half, radius, centers, sigma = 40, 1.0, [(-2.0, 0.0), (2.0, 0.0)], 0.02\n        points = []\n        for center in centers:\n            angles = np.random.uniform(0, 2 * np.pi, N_half)\n            ring_points = np.zeros((N_half, 2))\n            ring_points[:, 0] = center[0] + radius * np.cos(angles)\n            ring_points[:, 1] = center[1] + radius * np.sin(angles)\n            ring_points += np.random.normal(0, sigma, size=ring_points.shape)\n            points.append(ring_points)\n        return np.vstack(points)\n    elif case_name == 'NR':\n        N, radius, center, sigma = 50, 1.0, (0.0, 0.0), 0.12\n        angles = np.random.uniform(0, 2 * np.pi, N)\n        points = np.zeros((N, 2))\n        points[:, 0] = center[0] + radius * np.cos(angles)\n        points[:, 1] = center[1] + radius * np.sin(angles)\n        points += np.random.normal(0, sigma, size=points.shape)\n        return points\n    elif case_name == 'SR':\n        N, radius, center, sigma = 20, 1.0, (0.0, 0.0), 0.02\n        angles = np.random.uniform(0, 2 * np.pi, N)\n        points = np.zeros((N, 2))\n        points[:, 0] = center[0] + radius * np.cos(angles)\n        points[:, 1] = center[1] + radius * np.sin(angles)\n        points += np.random.normal(0, sigma, size=points.shape)\n        return points\n    return None\n\ndef calculate_max_lifetime(points, r_min, r_max, M):\n    \"\"\"\n    Computes VR filtration and max Betti 1 lifetime.\n    \"\"\"\n    N = points.shape[0]\n    if N == 0:\n        return 0.0\n\n    # 1. Compute pairwise distances and sort edges\n    dists = np.sqrt(np.sum((points[:, np.newaxis, :] - points[np.newaxis, :, :])**2, axis=-1))\n    edges = []\n    for i in range(N):\n        for j in range(i + 1, N):\n            edges.append((dists[i, j], i, j))\n    edges.sort()\n\n    scales = np.linspace(r_min, r_max, M)\n    betti1_sequence = []\n    \n    # 2. Iterate through filtration scales\n    edge_idx = 0\n    uf = UnionFind(N)\n    adj = [set() for _ in range(N)]\n    num_edges = 0\n    num_triangles = 0\n    c0 = N\n\n    for r_m in scales:\n        while edge_idx < len(edges) and edges[edge_idx][0] <= r_m:\n            dist, u, v = edges[edge_idx]\n            \n            # Update c1\n            num_edges += 1\n            \n            # Update c2 (count new triangles formed by adding edge (u,v))\n            num_new_triangles = len(adj[u].intersection(adj[v]))\n            num_triangles += num_new_triangles\n            \n            # Update graph and beta0\n            adj[u].add(v)\n            adj[v].add(u)\n            uf.union(u, v)\n\n            edge_idx += 1\n        \n        beta0 = uf.num_components\n        c1 = num_edges\n        c2 = num_triangles\n        \n        # Calculate beta_1 using Euler-Poincare formula approximation\n        beta1 = beta0 - c0 + c1 - c2\n        betti1_sequence.append(beta1)\n    \n    # 3. Extract birth-death intervals\n    births = []\n    intervals = []\n    prev_b1 = 0\n    for i, current_b1 in enumerate(betti1_sequence):\n        r_m = scales[i]\n        delta_b1 = current_b1 - prev_b1\n        if delta_b1 > 0:\n            for _ in range(delta_b1):\n                births.append(r_m)\n            births.sort()\n        elif delta_b1 < 0:\n            num_deaths = -delta_b1\n            for _ in range(min(num_deaths, len(births))):\n                birth_time = births.pop(0)\n                intervals.append((birth_time, r_m))\n        prev_b1 = current_b1\n        \n    # 4. Handle remaining births\n    for birth_time in births:\n        intervals.append((birth_time, r_max))\n        \n    # 5. Calculate max lifetime\n    if not intervals:\n        return 0.0\n        \n    max_lifetime = max(death - birth for birth, death in intervals)\n    return max_lifetime\n\ndef solve():\n    np.random.seed(0)\n    \n    cases = ['R', 'B', '2R', 'NR', 'SR']\n    \n    # Filtration parameters\n    r_min, r_max, M = 0.02, 0.6, 40\n    \n    results = []\n    for case_name in cases:\n        points = generate_points(case_name)\n        max_lifetime = calculate_max_lifetime(points, r_min, r_max, M)\n        results.append(max_lifetime)\n    \n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}