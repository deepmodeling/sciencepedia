{
    "hands_on_practices": [
        {
            "introduction": "In scientific machine learning, a model's utility is measured not only by its accuracy but also by its adherence to fundamental physical principles. An autoencoder that yields low reconstruction error but produces physically implausible configurations is of limited use. This practice introduces a crucial validation step beyond standard loss metrics: quantifying the model's respect for conservation laws. By developing a \"physicality score\" based on how well reconstructions conserve invariants like mass and energy, you will learn to evaluate whether a latent representation is physically meaningful .",
            "id": "3828693",
            "problem": "You are given a one-dimensional periodic scalar field $u(x)$ on the interval $[0,2\\pi)$, discretized uniformly with $N$ grid points and grid spacing $\\Delta x = \\frac{2\\pi}{N}$. Suppose an autoencoder is used to compress $u(x)$ to a latent variable and decode it back to a reconstruction $\\hat{u}(x)$. In multiscale modeling and analysis, meaningful latent variables (order parameters) should encode physically relevant features that respect global conservation laws. To assess this, implement a consistency check that quantifies whether decoder reconstructions conserve prescribed global invariants. Use the following fundamental base:\n\n- For periodic systems where advection-like or Hamiltonian dynamics are dominant and dissipation is negligible, global invariants such as total mass and quadratic energy are conserved. Define the mass as $I_1[u] = \\int_0^{2\\pi} u(x)\\,dx$ and the quadratic energy as $I_2[u] = \\int_0^{2\\pi} u(x)^2\\,dx$.\n- On a uniform grid with spacing $\\Delta x$, approximate these integrals by Riemann sums, i.e., $I_1[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j$ and $I_2[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j^2$, where $u_j = u(x_j)$ and $x_j = j\\Delta x$.\n- The conservation check compares invariants of the original field $u$ and the reconstruction $\\hat{u}$. Let $\\Delta_k = I_k[\\hat{u}] - I_k[u]$ for $k \\in \\{1,2\\}$.\n\nDefine a dimensionless deviation for each invariant by normalizing absolute differences with physically meaningful scales:\n- For mass, use $d_1 = \\frac{|\\Delta_1|}{\\max\\left(|I_1[u]|, \\|u\\|_{L^1}\\right)}$, where $\\|u\\|_{L^1} \\approx \\Delta x \\sum_{j=0}^{N-1} |u_j|$.\n- For energy, use $d_2 = \\frac{|\\Delta_2|}{\\max\\left(|I_2[u]|, \\epsilon\\right)}$, where $\\epsilon$ is any positive lower bound to avoid division by zero if $I_2[u]=0$. In this problem, the test suite avoids $I_2[u]=0$, so you may set $\\epsilon$ to $0$.\n\nAggregate these deviations into a scalar latent physicality score\n$$\nS = \\exp\\left(-\\frac{d_1 + d_2}{2}\\right),\n$$\nso that $S \\in (0,1]$, $S=1$ for exact conservation, and $S$ decays as deviations increase.\n\nImplement a program that:\n- Constructs a base field $u_{\\text{base}}(x) = \\sin(x) + 0.5\\cos(2x)$ on $[0,2\\pi)$ using $N=512$ uniformly spaced points with angles in radians.\n- Uses the following four test cases to define reconstructions $\\hat{u}$ from $u_{\\text{base}}$:\n    1. Case A (happy path): exact reconstruction $\\hat{u}(x) = u_{\\text{base}}(x)$.\n    2. Case B (spectral Gaussian blur): in Fourier space, apply a Gaussian filter with standard deviation $\\sigma = 0.2$ (in the $x$ coordinate) to $u_{\\text{base}}$ and inverse transform to real space. This should preserve the zero-wavenumber mode and thus mass, while reducing high-frequency energy.\n    3. Case C (amplitude scaling): $\\hat{u}(x) = s \\, u_{\\text{base}}(x)$ with $s = 1.1$.\n    4. Case D (additive bias): $\\hat{u}(x) = u_{\\text{base}}(x) + b$ with $b = 0.05$.\n\nFor each case, compute $I_1[u]$, $I_2[u]$, $I_1[\\hat{u}]$, $I_2[\\hat{u}]$, the deviations $d_1$ and $d_2$ as defined above, and the scalar score $S$. Angles are in radians. All quantities are dimensionless in this problem.\n\nYour program should produce a single line of output containing the four scores $S$ for cases A, B, C, and D, respectively, rounded to six decimal places, as a comma-separated list enclosed in square brackets (e.g., \"[0.999999,0.912345,0.812345,0.456789]\").\n\nTest suite details to implement:\n- Domain length $L = 2\\pi$.\n- Number of grid points $N = 512$.\n- Gaussian blur parameter $\\sigma = 0.2$ (in $x$ units).\n- Scaling factor $s = 1.1$.\n- Additive bias $b = 0.05$.\n\nThe final output is a list of four floats as specified. No user input is required.",
            "solution": "The problem requires the implementation of a physicality score $S$ to assess whether a reconstructed field $\\hat{u}(x)$ conserves key global invariants of an original field $u(x)$. The score is based on the conservation of total mass, $I_1[u]$, and total quadratic energy, $I_2[u]$, for a one-dimensional periodic scalar field on the domain $[0, 2\\pi)$. The solution proceeds through the following steps: defining the discrete system, calculating the base field and its invariants, generating four distinct reconstructed fields, and for each, computing the deviation-based score.\n\nFirst, we establish the computational domain. The field $u(x)$ is discretized on a uniform grid of $N=512$ points, denoted $x_j = j \\Delta x$ for $j \\in \\{0, 1, \\dots, N-1\\}$. The grid spacing is $\\Delta x = \\frac{2\\pi}{N}$. The base field is given by the function $u_{\\text{base}}(x) = \\sin(x) + 0.5\\cos(2x)$. We evaluate this function on our discrete grid to obtain the vector $u_{\\text{base},j} = u_{\\text{base}}(x_j)$.\n\nNext, we define the numerical approximations for the global invariants. The total mass, $I_1[u] = \\int_0^{2\\pi} u(x)\\,dx$, and the total quadratic energy, $I_2[u] = \\int_0^{2\\pi} u(x)^2\\,dx$, are approximated using a Riemann sum:\n$$I_1[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j$$\n$$I_2[u] \\approx \\Delta x \\sum_{j=0}^{N-1} u_j^2$$\nSimilarly, the $L^1$-norm, $\\|u\\|_{L^1} = \\int_0^{2\\pi} |u(x)|\\,dx$, required for normalization, is approximated as:\n$$\\|u\\|_{L^1} \\approx \\Delta x \\sum_{j=0}^{N-1} |u_j|$$\nUsing these formulas, we calculate the invariants $I_1[u_{\\text{base}}]$, $I_2[u_{\\text{base}}]$, and the norm $\\|u_{\\text{base}}\\|_{L^1}$ for the discrete base field. Analytically, $I_1[u_{\\text{base}}] = 0$, while $I_2[u_{\\text{base}}] = 1.25\\pi$. Our numerical calculation will yield values very close to these.\n\nThe core of the problem involves evaluating the physicality score for four different reconstructed fields, $\\hat{u}(x)$, derived from $u_{\\text{base}}(x)$.\n\nCase A (Exact Reconstruction): $\\hat{u}(x) = u_{\\text{base}}(x)$. In this trivial case, the reconstruction is perfect.\n\nCase B (Spectral Gaussian Blur): This reconstruction simulates a common form of information loss where high-frequency details are dampened. This is achieved in Fourier space. First, we compute the discrete Fourier transform (DFT) of the base field, $\\tilde{u}_{\\text{base}} = \\mathcal{F}\\{u_{\\text{base}}\\}$. The corresponding angular wavenumbers are given by the vector $k$, where $k_m = 2\\pi f_m$ and $f_m$ are the discrete frequencies provided by a standard FFT frequency function for a signal of length $N$ with sample spacing $\\Delta x$. We then define a Gaussian filter in the Fourier domain:\n$$G(k) = \\exp\\left(-\\frac{k^2 \\sigma^2}{2}\\right)$$\nwith the given standard deviation in real space, $\\sigma = 0.2$. The filtered field in Fourier space is $\\tilde{\\hat{u}} = \\tilde{u}_{\\text{base}} \\odot G$, where $\\odot$ denotes element-wise multiplication. The reconstructed field $\\hat{u}(x)$ is then obtained by applying the inverse DFT, $\\hat{u} = \\text{Re}(\\mathcal{F}^{-1}\\{\\tilde{\\hat{u}}\\})$. The real part is taken to discard negligible imaginary components arising from numerical precision errors.\n\nCase C (Amplitude Scaling): This case models a simple gain error, where the reconstruction is uniformly scaled: $\\hat{u}(x) = s \\cdot u_{\\text{base}}(x)$ with a scaling factor of $s = 1.1$.\n\nCase D (Additive Bias): This models a constant offset error: $\\hat{u}(x) = u_{\\text{base}}(x) + b$, with a bias of $b = 0.05$.\n\nFor each of these four cases, we compute the physicality score $S$. First, we calculate the invariants for the reconstructed field, $I_1[\\hat{u}]$ and $I_2[\\hat{u}]$. Then, we find the differences from the base field's invariants: $\\Delta_1 = I_1[\\hat{u}] - I_1[u_{\\text{base}}]$ and $\\Delta_2 = I_2[\\hat{u}] - I_2[u_{\\text{base}}]$.\n\nThese differences are converted into dimensionless deviations, $d_1$ and $d_2$, through normalization:\n$$d_1 = \\frac{|\\Delta_1|}{\\max\\left(|I_1[u_{\\text{base}}]|, \\|u_{\\text{base}}\\|_{L^1}\\right)}$$\n$$d_2 = \\frac{|\\Delta_2|}{|I_2[u_{\\text{base}}]|}$$\nThe denominator for $d_1$ robustly handles cases where the mean of the field is zero, which is true for $u_{\\text{base}}$. The denominator for $d_2$ is safe as $I_2[u_{\\text{base}}]$ is strictly positive since the test suite avoids $I_2[u] = 0$.\n\nFinally, the scalar physicality score $S$ is calculated by aggregating the deviations:\n$$S = \\exp\\left(-\\frac{d_1 + d_2}{2}\\right)$$\nThis score maps the two deviations onto the interval $(0, 1]$, where $S=1$ indicates perfect conservation of both invariants, and the score decreases as the conservation errors grow. The procedure is repeated for each of the four cases, and the resulting scores are formatted as a list.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a physicality score for four test cases of field reconstructions\n    based on the conservation of mass and energy invariants.\n    \"\"\"\n    # Define problem parameters\n    N = 512\n    L = 2.0 * np.pi\n    dx = L / N\n    sigma = 0.2\n    s = 1.1\n    b = 0.05\n\n    # Construct the grid and base field\n    x = np.linspace(0.0, L, N, endpoint=False)\n    u_base = np.sin(x) + 0.5 * np.cos(2.0 * x)\n\n    # --- Define test cases for reconstructions ---\n    test_cases = []\n\n    # Case A: Exact reconstruction\n    u_hat_A = u_base\n    test_cases.append(u_hat_A)\n\n    # Case B: Spectral Gaussian blur\n    # Compute Fourier transform and corresponding wavenumbers\n    u_base_tilde = np.fft.fft(u_base)\n    # Frequencies in cycles per unit of sample spacing\n    # `d=dx` means the unit is physical length\n    freq = np.fft.fftfreq(N, d=dx)\n    # Convert to angular wavenumbers (rad/length)\n    k = 2.0 * np.pi * freq\n    # Define Gaussian filter in Fourier space\n    gaussian_filter = np.exp(-(k**2 * sigma**2) / 2.0)\n    # Apply filter and inverse transform, taking real part to remove numerical noise\n    u_hat_tilde_B = u_base_tilde * gaussian_filter\n    u_hat_B = np.fft.ifft(u_hat_tilde_B).real\n    test_cases.append(u_hat_B)\n\n    # Case C: Amplitude scaling\n    u_hat_C = s * u_base\n    test_cases.append(u_hat_C)\n\n    # Case D: Additive bias\n    u_hat_D = u_base + b\n    test_cases.append(u_hat_D)\n\n    # --- Calculate invariants for the base field ---\n    # Riemann sum approximation of the integral\n    I1_base = np.sum(u_base) * dx\n    I2_base = np.sum(u_base**2) * dx\n    L1_norm_base = np.sum(np.abs(u_base)) * dx\n\n    scores = []\n    # --- Process each test case to calculate its score ---\n    for u_hat in test_cases:\n        # Calculate invariants for the reconstructed field\n        I1_hat = np.sum(u_hat) * dx\n        I2_hat = np.sum(u_hat**2) * dx\n\n        # Calculate differences and dimensionless deviations\n        delta_1 = I1_hat - I1_base\n        delta_2 = I2_hat - I2_base\n\n        # Normalization for d1, robust to I1_base being near zero\n        d1_denom = np.maximum(np.abs(I1_base), L1_norm_base)\n        d1 = np.abs(delta_1) / d1_denom if d1_denom != 0 else 0.0\n\n        # Normalization for d2\n        # Problem statement guarantees I2_base is not zero for the given test suite\n        d2 = np.abs(delta_2) / np.abs(I2_base)\n        \n        # Calculate the final physicality score\n        score = np.exp(-(d1 + d2) / 2.0)\n        scores.append(score)\n\n    # Format the output as a comma-separated list with 6 decimal places\n    formatted_scores = [f\"{score:.6f}\" for score in scores]\n    print(f\"[{','.join(formatted_scores)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After training an autoencoder, the central challenge becomes interpreting its latent space. If the goal is to identify an order parameter, how can we be certain that a learned latent variable, $z$, truly corresponds to a known macroscopic quantity, $m$? This hands-on exercise moves beyond qualitative visual inspection to a rigorous quantitative assessment. You will implement a formal statistical criterion to test the correspondence between $z$ and $m$ by combining correlation analysis with hypothesis testing, providing a robust method for validating the physical interpretation of a learned representation .",
            "id": "3828675",
            "problem": "Consider a dataset with $N$ samples indexing microscopic states $\\{x_i\\}_{i=1}^N$ of a multiscale system, where an Autoencoder (AE) trained on these states produces a one-dimensional latent variable $z_i \\in \\mathbb{R}$ for each sample $i$, and a known macroscopic Order Parameter (OP) $m_i \\in \\mathbb{R}$ is available for the same samples. Canonical Correlation Analysis (CCA) seeks linear combinations that maximize the correlation between two sets of variables. In general, for sets $X \\in \\mathbb{R}^p$ and $Y \\in \\mathbb{R}^q$, CCA solves $\\max_{\\alpha \\in \\mathbb{R}^p, \\beta \\in \\mathbb{R}^q} \\mathrm{corr}(\\alpha^\\top X, \\beta^\\top Y)$. When both sets are one-dimensional, i.e., $p=q=1$, CCA reduces to the absolute Pearson correlation between the standardized scalars $z$ and $m$. You are asked to formalize a decision criterion that tests whether the AE latent $z$ corresponds to the known OP $m$, and implement it as a program.\n\nYour formal decision criterion must be based on computing the sample Pearson correlation coefficient $r$ between $z$ and $m$ across the dataset, evaluating its statistical significance under the null hypothesis that the true correlation is zero, and then applying a threshold on the canonical correlation magnitude. Specifically, define the sample Pearson correlation coefficient\n$$\nr = \\frac{\\sum_{i=1}^N (z_i - \\bar{z})(m_i - \\bar{m})}{\\sqrt{\\sum_{i=1}^N (z_i - \\bar{z})^2} \\sqrt{\\sum_{i=1}^N (m_i - \\bar{m})^2}},\n$$\nwhere $\\bar{z} = \\frac{1}{N}\\sum_{i=1}^N z_i$ and $\\bar{m} = \\frac{1}{N}\\sum_{i=1}^N m_i$. Under the null hypothesis $H_0: \\rho = 0$, where $\\rho$ is the population correlation, and assuming samples are independent and identically distributed with finite variance, the statistic\n$$\nt = r \\sqrt{\\frac{N - 2}{1 - r^2}}\n$$\nfollows a Student $t$ distribution with $N - 2$ degrees of freedom. The two-sided $p$-value is\n$$\np = 2 \\left(1 - F_{t_{N-2}}(|t|)\\right),\n$$\nwhere $F_{t_{N-2}}$ is the cumulative distribution function of the Student $t$ distribution with $N - 2$ degrees of freedom. Let the decision threshold be a correlation magnitude $\\tau \\in [0,1]$ and significance level $\\alpha \\in (0,1)$. Your decision rule must be:\n- If either $\\mathrm{Var}(z) = 0$ or $\\mathrm{Var}(m) = 0$, declare that $z$ does not correspond to $m$.\n- Otherwise, compute $r$ and $p$ as above, and declare that $z$ corresponds to $m$ if and only if $|r| \\ge \\tau$ and $p \\le \\alpha$.\n\nAngles must be treated in radians for any trigonometric functions. The program must implement this criterion and evaluate the following test suite of deterministic cases, each defined by $(z, m)$ pairs constructed from explicit formulas over index $i$:\n\n- Case $1$ (general happy path): $N = 200$, $m_i = \\sin\\left(\\frac{2\\pi i}{N}\\right)$, $z_i = 1.1 \\, m_i$. Use $\\tau = 0.8$ and $\\alpha = 0.01$. Angle unit is radians. Expected behavior: strong positive canonical correlation.\n- Case $2$ (orthogonality edge case): $N = 200$, $m_i = \\sin\\left(\\frac{2\\pi i}{N}\\right)$, $z_i = \\cos\\left(\\frac{2\\pi i}{N}\\right)$. Use $\\tau = 0.8$ and $\\alpha = 0.01$. Angle unit is radians. Expected behavior: negligible canonical correlation.\n- Case $3$ (negative correlation general case): $N = 150$, $m_i$ linearly spaced as $m_i = -1 + \\frac{2(i-1)}{N-1}$, $z_i = - m_i + 0.05 \\, \\sin\\left(\\frac{4\\pi i}{N}\\right)$. Use $\\tau = 0.8$ and $\\alpha = 0.01$. Angle unit is radians. Expected behavior: strong negative canonical correlation.\n- Case $4$ (degenerate variance boundary): $N = 100$, $m_i = 0$ for all $i$, $z_i = \\cos\\left(\\frac{2\\pi i}{N}\\right)$. Use $\\tau = 0.8$ and $\\alpha = 0.01$. Angle unit is radians. Expected behavior: undefined correlation due to zero variance in $m$ must be treated as rejection.\n- Case $5$ (small-sample moderate correlation, significance edge): $N = 20$, $m_i = -1 + \\frac{2(i-1)}{N-1}$, $z_i = 0.4 \\, m_i + 0.9 \\, \\sin\\left(\\frac{2\\pi i}{N}\\right)$. Use $\\tau = 0.8$ and $\\alpha = 0.01$. Angle unit is radians. Expected behavior: correlation magnitude below the threshold and likely not statistically significant at the given $\\alpha$.\n\nYour program must produce a single line of output containing the results of the decision rule for the five cases, as a comma-separated list of boolean values enclosed in square brackets, in the order of the cases listed above (e.g., $[$result$_1$,result$_2$,result$_3$,result$_4$,result$_5]$). The output values must be of type boolean, and there must be no additional text printed.",
            "solution": "The problem requires the formalization and implementation of a decision criterion to assess whether a one-dimensional latent variable $z$, learned by an autoencoder from a set of microscopic states, corresponds to a known macroscopic order parameter $m$. The correspondence is evaluated based on the strength and statistical significance of the linear relationship between the two variables, $z$ and $m$.\n\nThe core of the decision criterion rests upon the sample Pearson correlation coefficient, $r$, and an associated hypothesis test. For two sets of paired data $\\{z_i\\}_{i=1}^N$ and $\\{m_i\\}_{i=1}^N$, the sample Pearson correlation coefficient is given by:\n$$\nr = \\frac{\\sum_{i=1}^N (z_i - \\bar{z})(m_i - \\bar{m})}{\\sqrt{\\sum_{i=1}^N (z_i - \\bar{z})^2} \\sqrt{\\sum_{i=1}^N (m_i - \\bar{m})^2}}\n$$\nwhere $\\bar{z} = \\frac{1}{N}\\sum_{i=1}^N z_i$ and $\\bar{m} = \\frac{1}{N}\\sum_{i=1}^N m_i$ are the sample means. In the context of Canonical Correlation Analysis (CCA) between two one-dimensional variables, the maximized correlation is simply the absolute value of the Pearson coefficient, $|r|$.\n\nThe decision criterion is a two-part test with specified thresholds: a minimum correlation magnitude $\\tau$ and a statistical significance level $\\alpha$.\n\nStep 1: Pre-condition Validation.\nThe Pearson correlation coefficient is undefined if either of the variables has zero variance. A variable with zero variance is a constant and cannot covary with another variable. Therefore, the first step of the criterion is to check the sample variances, $\\mathrm{Var}(z)$ and $\\mathrm{Var}(m)$. If either $\\mathrm{Var}(z) = 0$ or $\\mathrm{Var}(m) = 0$, the variables cannot be meaningfully correlated. In this case, we conclude that $z$ does not correspond to $m$. In a numerical implementation, this check must be performed with a small tolerance to account for floating-point arithmetic limitations.\n\nStep 2: Correlation Magnitude Test.\nIf both variables have non-zero variance, we proceed to compute $r$. The first part of the decision rule assesses the strength of the correlation. The latent variable $z$ is considered a potential representation of the order parameter $m$ only if the magnitude of their linear relationship is sufficiently large. This is formalized by the condition:\n$$\n|r| \\ge \\tau\n$$\nIf this condition is not met, we conclude that the correspondence is too weak, and the criterion fails.\n\nStep 3: Statistical Significance Test.\nA large sample correlation $r$ might occur by chance, especially in small datasets. To guard against this, we test the statistical significance of the observed correlation. We formulate a null hypothesis, $H_0$, that there is no correlation in the underlying population from which the samples are drawn, i.e., the population correlation coefficient $\\rho$ is zero ($H_0: \\rho = 0$).\n\nUnder the assumption that the samples $(z_i, m_i)$ are independent and identically distributed from a bivariate normal distribution (or for large $N$ via the Central Limit Theorem), the test statistic\n$$\nt = r \\sqrt{\\frac{N - 2}{1 - r^2}}\n$$\nfollows a Student's $t$-distribution with $N - 2$ degrees of freedom. A large absolute value of $t$ provides evidence against the null hypothesis.\n\nFrom the calculated $t$-statistic, we determine the two-sided $p$-value, which is the probability of observing a correlation at least as extreme as $r$ if the null hypothesis were true. The $p$-value is given by:\n$$\np_{\\text{value}} = 2 \\left(1 - F_{t_{N-2}}(|t|)\\right)\n$$\nwhere $F_{t_{N-2}}$ is the cumulative distribution function (CDF) of the Student's $t$-distribution with $N - 2$ degrees of freedom. The correlation is deemed statistically significant if this $p$-value is less than or equal to the specified significance level $\\alpha$:\n$$\np_{\\text{value}} \\le \\alpha\n$$\n\nA special case arises when $|r|=1$. The formula for the $t$-statistic would involve division by zero. However, a perfect correlation ($|r|=1$) is the maximum possible effect size, implying that the probability of observing a more extreme result is zero. Thus, for $|r|=1$, the $p$-value is $0$, and the significance test is always passed for any $\\alpha > 0$.\n\nFinal Decision Rule:\nCombining these steps, the latent variable $z$ is declared to correspond to the order parameter $m$ if and only if all three conditions are met:\n1. $\\mathrm{Var}(z) > 0$ and $\\mathrm{Var}(m) > 0$.\n2. The correlation magnitude is sufficient: $|r| \\ge \\tau$.\n3. The correlation is statistically significant: $p_{\\text{value}} \\le \\alpha$.\n\nThis procedure will be implemented and applied to the five specified test cases. For implementation, we will utilize numerical libraries to ensure robust calculation of the variance, correlation coefficient, and the Student's $t$-distribution CDF.\n- Case $1$: $z_i = 1.1 m_i$, so $r=1$. $|r| = 1 \\ge 0.8$ and $p_{\\text{value}} = 0 \\le 0.01$. The criterion passes.\n- Case $2$: $m_i$ and $z_i$ are orthogonal sine and cosine functions over a full period. Their sample correlation $r$ will be close to $0$. The condition $|r| \\ge 0.8$ will fail.\n- Case $3$: $z_i$ is a strongly anti-correlated version of $m_i$ with minor noise. We expect $r$ to be close to $-1$. Therefore, $|r| \\approx 1 \\ge 0.8$. With $N=150$, this strong correlation will be highly significant, so $p_{\\text{value}} \\ll 0.01$. The criterion passes.\n- Case $4$: $m_i$ is a constant vector, so $\\mathrm{Var}(m) = 0$. The pre-condition fails, and the criterion immediately returns a negative result.\n- Case $5$: A moderate correlation is expected, but the sample size $N=20$ is small, and the noise term is large. The resulting correlation magnitude $|r|$ is unlikely to meet the high threshold of $\\tau = 0.8$. Furthermore, even if it did, the small sample size may render the result not statistically significant at the strict $\\alpha=0.01$ level. The criterion is expected to fail.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as student_t\n\ndef solve():\n    \"\"\"\n    Main function to evaluate the decision criterion for all test cases.\n    \"\"\"\n\n    def decision_criterion(z, m, N, tau, alpha):\n        \"\"\"\n        Applies the formal decision criterion to determine if latent variable z\n        corresponds to order parameter m.\n\n        Args:\n            z (np.ndarray): The latent variable vector.\n            m (np.ndarray): The order parameter vector.\n            N (int): The number of samples.\n            tau (float): The correlation magnitude threshold.\n            alpha (float): The significance level.\n\n        Returns:\n            bool: True if z corresponds to m, False otherwise.\n        \"\"\"\n        # Step 1: Pre-condition Validation (non-zero variance)\n        # Using a small tolerance for floating-point comparison.\n        if np.var(m)  1e-15 or np.var(z)  1e-15:\n            return False\n\n        # Compute sample Pearson correlation coefficient r\n        # np.corrcoef returns a 2x2 matrix, the value is at [0, 1] or [1, 0]\n        r = np.corrcoef(z, m)[0, 1]\n\n        # Step 2: Correlation Magnitude Test\n        if abs(r)  tau:\n            return False\n\n        # Step 3: Statistical Significance Test\n        # Handle the edge case of perfect correlation to avoid division by zero.\n        # If |r| is extremely close to 1, the p-value is effectively 0.\n        if abs(r) > 1.0 - 1e-9:\n            p_val = 0.0\n        else:\n            # Calculate the t-statistic\n            t_stat = r * np.sqrt((N - 2) / (1 - r**2))\n            \n            # Degrees of freedom\n            df = N - 2\n            \n            # Calculate the two-sided p-value using the Student's t-distribution CDF\n            p_val = 2 * (1 - student_t.cdf(abs(t_stat), df))\n\n        # Final decision: True if p-value is below significance level\n        return p_val = alpha\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (N, m_func, z_func, tau, alpha)\n    test_cases = [\n        (200, lambda i, N: np.sin(2 * np.pi * i / N), lambda i, N: 1.1 * np.sin(2 * np.pi * i / N), 0.8, 0.01),\n        (200, lambda i, N: np.sin(2 * np.pi * i / N), lambda i, N: np.cos(2 * np.pi * i / N), 0.8, 0.01),\n        (150, lambda i, N: -1 + 2 * (i - 1) / (N - 1), lambda i, N: -(-1 + 2 * (i - 1) / (N - 1)) + 0.05 * np.sin(4 * np.pi * i / N), 0.8, 0.01),\n        (100, lambda i, N: np.zeros_like(i, dtype=float), lambda i, N: np.cos(2 * np.pi * i / N), 0.8, 0.01),\n        (20, lambda i, N: -1 + 2 * (i - 1) / (N - 1), lambda i, N: 0.4 * (-1 + 2 * (i - 1) / (N - 1)) + 0.9 * np.sin(2 * np.pi * i / N), 0.8, 0.01),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, m_func, z_func, tau, alpha = case\n        \n        # The problem states index i from 1 to N\n        i_indices = np.arange(1, N + 1)\n        \n        m_vec = m_func(i_indices, N)\n        z_vec = z_func(i_indices, N)\n        \n        result = decision_criterion(z_vec, m_vec, N, tau, alpha)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Python's str() for booleans is 'True'/'False' (capitalized).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The true power of using autoencoders in physics lies in their potential to discover *new* or hidden order parameters that are not known beforehand. This requires designing the model to learn the most compact and informative representation possible. This conceptual practice delves into the crucial role of regularization in achieving this goal, exploring how choices between $\\ell_1$ and $\\ell_2$ penalties on latent variables and network weights guide the learning process. By analyzing the interplay between sparsity-inducing priors and model stability, you will develop the intuition needed to architect autoencoders for genuine scientific discovery .",
            "id": "3828664",
            "problem": "A high-dimensional microscopic state $\\mathbf{x} \\in \\mathbb{R}^n$ is generated by a low-dimensional set of order parameters $\\mathbf{s} \\in \\mathbb{R}^d$ through a multiscale linear mechanism with additive noise, modeled as\n$$\n\\mathbf{x} = \\mathbf{A}\\mathbf{s} + \\boldsymbol{\\epsilon},\n$$\nwhere $\\mathbf{A} \\in \\mathbb{R}^{n \\times d}$ is unknown, $d \\ll n$, and $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)$. Assume that the order parameters are sparse in the sense that their population distribution is well approximated by independent Laplace priors, i.e., $p(s_i) \\propto \\exp(-\\tau |s_i|)$ for each component $s_i$ with rate parameter $\\tau  0$. An autoencoder is trained to identify minimal sufficient order parameters by compressing $\\mathbf{x}$ into a latent representation $\\mathbf{z} \\in \\mathbb{R}^m$ via a linear encoder and decoder,\n$$\n\\mathbf{z} = \\mathbf{W}_e \\mathbf{x}, \\quad \\hat{\\mathbf{x}} = \\mathbf{W}_d \\mathbf{z},\n$$\nwith $\\mathbf{W}_e \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{W}_d \\in \\mathbb{R}^{n \\times m}$, where $m \\ge d$. The training objective is a regularized reconstruction loss\n$$\nJ(\\mathbf{W}_e,\\mathbf{W}_d) = \\mathbb{E}_{\\mathbf{x}}\\left[\\|\\mathbf{x} - \\mathbf{W}_d \\mathbf{W}_e \\mathbf{x}\\|_2^2\\right] + \\lambda_z \\, R_z(\\mathbf{z}) + \\lambda_W \\left(\\|\\mathbf{W}_e\\|_F^2 + \\|\\mathbf{W}_d\\|_F^2\\right) + \\lambda_W' \\, R_W(\\mathbf{W}_e,\\mathbf{W}_d),\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm, $\\|\\cdot\\|_F$ denotes the Frobenius norm, $R_z$ is a latent regularizer, and $R_W$ is an optional weight regularizer, with nonnegative hyperparameters $\\lambda_z$, $\\lambda_W$, and $\\lambda_W'$. The minimal sufficient order parameters are defined as the lowest-dimensional statistics of $\\mathbf{x}$ that are sufficient to characterize the conditional distribution of observables at coarse scales, and this problem assumes the linear latent representation seeks to align $\\mathbf{z}$ with $\\mathbf{s}$.\n\nStarting from the definitions of sparsity and sufficiency, the equivalence between regularization and Maximum A Posteriori (MAP) estimation, and the scale-rescaling invariance of linear autoencoders, reason about how the choice between $\\ell_1$ and $\\ell_2$ penalties on the latent $\\mathbf{z}$ and on the weights $\\mathbf{W}_e,\\mathbf{W}_d$ affects the recovery of sparse, minimal sufficient order parameters. In particular, consider the rescaling transformation $\\mathbf{W}_e \\leftarrow \\alpha \\mathbf{W}_e$, $\\mathbf{W}_d \\leftarrow \\alpha^{-1} \\mathbf{W}_d$ for scalar $\\alpha  0$, under which the reconstruction term is invariant, and analyze how different regularization choices interact with this invariance.\n\nWhich option best promotes sparse latent representations that align with the minimal sufficient order parameters $\\mathbf{s}$, while preventing pathological rescaling and preserving stable generalization in the presence of $\\boldsymbol{\\epsilon}$?\n\nA. Use an $\\ell_1$ penalty on the latent $\\mathbf{z}$, i.e., $R_z(\\mathbf{z}) = \\|\\mathbf{z}\\|_1$ with $\\lambda_z  0$, combined with mild $\\ell_2$ weight decay $\\lambda_W  0$ on both $\\mathbf{W}_e$ and $\\mathbf{W}_d$, and set the bottleneck width $m \\ge d$.\n\nB. Use an $\\ell_2$ penalty on the latent $\\mathbf{z}$, i.e., $R_z(\\mathbf{z}) = \\|\\mathbf{z}\\|_2^2$ with $\\lambda_z  0$, combined with $\\ell_1$ penalties on the weights, i.e., $R_W(\\mathbf{W}_e,\\mathbf{W}_d) = \\|\\mathbf{W}_e\\|_1 + \\|\\mathbf{W}_d\\|_1$ with $\\lambda_W'  0$.\n\nC. Use $\\ell_2$ penalties on both the latent and weights, i.e., $R_z(\\mathbf{z}) = \\|\\mathbf{z}\\|_2^2$, and $\\lambda_W  0$ with no $\\ell_1$ terms, and force minimality by setting the bottleneck width $m = d$.\n\nD. Use strong $\\ell_1$ penalties on both the latent and weights, i.e., $R_z(\\mathbf{z}) = \\|\\mathbf{z}\\|_1$ and $R_W(\\mathbf{W}_e,\\mathbf{W}_d) = \\|\\mathbf{W}_e\\|_1 + \\|\\mathbf{W}_d\\|_1$, with large $\\lambda_z$ and $\\lambda_W'$, and rely on early stopping to avoid underfitting.",
            "solution": "This problem requires selecting the optimal regularization strategy for a linear autoencoder to recover sparse, minimal sufficient order parameters. The choice must be guided by the data's statistical properties and the autoencoder's inherent invariances.\n\nThe analysis hinges on two key principles:\n1.  **Maximum A Posteriori (MAP) Estimation:** The training objective can be viewed as minimizing the negative log-posterior probability of the model parameters. The regularization term in the objective function corresponds to the negative log-prior of the regularized quantity. The problem states that the true order parameters $\\mathbf{s}$ follow an independent Laplace distribution, $p(s_i) \\propto \\exp(-\\tau|s_i|)$. The negative log of this prior is proportional to $\\sum_i |s_i|$, which is the $\\ell_1$ norm. Therefore, to encourage the learned latent representation $\\mathbf{z}$ to have the same sparsity as $\\mathbf{s}$, an $\\ell_1$ penalty on $\\mathbf{z}$ ($R_z(\\mathbf{z}) = \\|\\mathbf{z}\\|_1$) is the correct choice. An $\\ell_2$ penalty would correspond to an incorrect Gaussian prior.\n\n2.  **Rescaling Invariance and Stability:** The reconstruction loss term, $\\|\\mathbf{x} - \\mathbf{W}_d \\mathbf{W}_e \\mathbf{x}\\|_2^2$, is invariant to the rescaling transformation $\\mathbf{W}_e \\to \\alpha \\mathbf{W}_e$ and $\\mathbf{W}_d \\to \\alpha^{-1} \\mathbf{W}_d$. If only the latent code $\\mathbf{z}$ were regularized, the model could exploit this invariance pathologically, for example, by making $\\mathbf{W}_e$ arbitrarily small to reduce the penalty on $\\mathbf{z}$, while causing $\\mathbf{W}_d$ to explode. This leads to an unstable and poorly-conditioned model. To prevent this, a penalty on the weights is necessary. An $\\ell_2$ penalty (Frobenius norm), $\\|\\mathbf{W}_e\\|_F^2 + \\|\\mathbf{W}_d\\|_F^2$, breaks this invariance because the penalty term $\\alpha^2\\|\\mathbf{W}_e\\|_F^2 + \\alpha^{-2}\\|\\mathbf{W}_d\\|_F^2$ has a unique minimum with respect to the scaling factor $\\alpha$. This stabilizes the learning process by balancing the norms of the encoder and decoder weights.\n\nBased on these principles, we evaluate the options:\n\n*   **A.** This option correctly combines an $\\ell_1$ penalty on the latent code $\\mathbf{z}$ (matching the Laplace prior) with an $\\ell_2$ penalty on the weights (stabilizing the scales). Setting the bottleneck width $m \\ge d$ allows the model to discover the true dimension $d$ through the sparsity-inducing $\\ell_1$ penalty. This is the ideal strategy.\n*   **B.** This uses an $\\ell_2$ penalty on $\\mathbf{z}$, which is a mismatch for the sparse data-generating process. It promotes small, dense codes, not sparse ones.\n*   **C.** This uses $\\ell_2$ penalties for both latent variables and weights. It is essentially a form of regularized Principal Component Analysis (PCA), which is optimal for finding directions of maximum variance under Gaussian assumptions, not for recovering sparse components. It also requires prior knowledge of the exact dimension $d$.\n*   **D.** This option uses \"strong\" $\\ell_1$ penalties, risking underfitting, and contains a logically flawed statement that \"early stopping\" is used to avoid \"underfitting.\" Early stopping is a technique to prevent overfitting.\n\nTherefore, option A provides the most principled and robust approach.",
            "answer": "$$\n\\boxed{A}\n$$"
        }
    ]
}