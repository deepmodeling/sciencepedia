{
    "hands_on_practices": [
        {
            "introduction": "生成模型很少能完美地捕捉到目标物理系统的真实分布。因此，我们需要一种方法来校正由模型分布 $p_\\theta(x)$ 和真实物理分布 $p^\\star(x)$ 之间的不匹配所引起的偏差。本练习将通过重要性采样这一基础统计学方法，向您展示如何利用有偏的生成器样本获得物理量的无偏估计。通过动手推导估计量的方差，您将更深入地理解其可靠性和统计特性。",
            "id": "3765001",
            "problem": "考虑一个多尺度分子系统，其一维粗粒化变量 $x$ 捕捉了一个缓慢的集体模式。$x$ 的平衡目标分布是 Boltzmann 分布 $p^{\\star}(x) \\propto \\exp(-U(x))$，其温度经选择使得归一化目标密度为标准正态分布 $p^{\\star}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$。一个具有显式密度的生成模型，例如变分自编码器 (VAE)，从参数族 $p_{\\theta}(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)$ 中提出样本，其中 $\\theta = (\\mu,\\sigma)$ 是模型参数。您从 $p_{\\theta}$ 中抽取 $n$ 个独立样本 $\\{x_{i}\\}_{i=1}^{n}$，并试图使用重要性采样计算可观测量在 $p^{\\star}$ 分布下的平衡期望值。\n\n从重要性采样的基本定义和大数定律出发，推导重加权方案 $w(x) = \\frac{p^{\\star}(x)}{p_{\\theta}(x)}$ 以及可观测量 $f(x) = x^{2}$ 的自归一化重要性采样估计量。然后，基于适用于样本均值的极限定理，使用第一性原理推导，得到自归一化估计量的渐近方差，并用在 $p_{\\theta}$ 分布下的期望值表示。\n\n最后，将问题特化到科学上现实的情况，即 $\\mu = 0$ 和 $\\sigma = 1$（即生成模型与目标匹配），并计算可观测量 $f(x) = x^{2}$ 的自归一化重要性采样估计量的渐近方差，结果为一个实数值。最终答案不带单位。无需四舍五入。",
            "solution": "首先验证问题，确保其具有科学依据、提法恰当，并提供了所有必要信息。\n\n**步骤1：提取已知条件**\n- 一维粗粒化变量：$x$。\n- 平衡目标密度：$p^{\\star}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)$，即标准正态分布 $\\mathcal{N}(0,1)$。\n- 生成模型参数密度：$p_{\\theta}(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)$，即参数为 $\\theta = (\\mu, \\sigma)$ 的正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$。\n- 样本：从 $p_{\\theta}(x)$ 中抽取的 $n$ 个独立同分布样本 $\\{x_{i}\\}_{i=1}^{n}$。\n- 可观测量：$f(x) = x^{2}$。\n- 任务1：推导重加权方案 $w(x) = \\frac{p^{\\star}(x)}{p_{\\theta}(x)}$ 以及 $\\langle f(x) \\rangle_{p^{\\star}}$ 的自归一化重要性采样 (SNIS) 估计量。\n- 任务2：推导 SNIS 估计量的渐近方差，并用在 $p_{\\theta}$ 分布下的期望值表示。\n- 任务3：在生成模型与目标完全匹配的特定情况（即 $\\mu = 0$ 和 $\\sigma = 1$）下，计算该渐近方差。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题是有效的。这是一个在统计力学和机器学习领域中提法恰当的标准问题。Boltzmann 分布、生成模型、正态分布和重要性采样的概念都已得到科学公认。问题陈述是自洽的、客观的，并为获得唯一解提供了所有必要信息。最终的计算是针对一个科学上现实的基准案例（一个完美模型）。\n\n**步骤3：结论与行动**\n问题有效。将提供一个完整、论证充分的解答。\n\n**自归一化重要性采样 (SNIS) 估计量的推导**\n\n我们的目标是计算可观测量 $f(x)$ 关于目标分布 $p^{\\star}(x)$ 的期望值，记为 $\\langle f \\rangle_{p^{\\star}}$：\n$$\n\\langle f \\rangle_{p^{\\star}} = \\int f(x) p^{\\star}(x) \\,dx\n$$\n我们拥有来自不同分布，即提议分布 $p_{\\theta}(x)$ 的样本。为了用 $p_{\\theta}(x)$ 来表示这个期望，我们在被积函数上乘以并除以 $p_{\\theta}(x)$，假设只要 $f(x)p^{\\star}(x) \\neq 0$，就有 $p_{\\theta}(x) > 0$：\n$$\n\\langle f \\rangle_{p^{\\star}} = \\int f(x) \\frac{p^{\\star}(x)}{p_{\\theta}(x)} p_{\\theta}(x) \\,dx\n$$\n我们将重要性权重（或重加权因子）定义为目标密度与提议密度的比值：\n$$\nw(x) = \\frac{p^{\\star}(x)}{p_{\\theta}(x)}\n$$\n现在，该期望可以写成关于提议分布 $p_{\\theta}(x)$ 的期望：\n$$\n\\langle f \\rangle_{p^{\\star}} = \\int (f(x) w(x)) p_{\\theta}(x) \\,dx = \\mathbb{E}_{p_{\\theta}}[f(x)w(x)]\n$$\n在许多实际情况中，分布的归一化常数可能是未知的。让我们将 $\\langle f \\rangle_{p^{\\star}}$ 写成积分的比值形式：\n$$\n\\langle f \\rangle_{p^{\\star}} = \\frac{\\int f(x) p^{\\star}(x) \\,dx}{\\int p^{\\star}(x) \\,dx}\n$$\n通过在分子和分母中同时引入 $p_{\\theta}(x)/p_{\\theta}(x)$，我们得到：\n$$\n\\langle f \\rangle_{p^{\\star}} = \\frac{\\int f(x) w(x) p_{\\theta}(x) \\,dx}{\\int w(x) p_{\\theta}(x) \\,dx} = \\frac{\\mathbb{E}_{p_{\\theta}}[f(x)w(x)]}{\\mathbb{E}_{p_{\\theta}}[w(x)]}\n$$\n给定从 $p_{\\theta}(x)$ 中抽取的 $n$ 个独立同分布样本 $\\{x_i\\}_{i=1}^n$，根据大数定律，我们可以使用样本均值来近似分子和分母中的期望：\n$$\n\\mathbb{E}_{p_{\\theta}}[f(x)w(x)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} f(x_i)w(x_i)\n$$\n$$\n\\mathbb{E}_{p_{\\theta}}[w(x)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} w(x_i)\n$$\n将这些近似值代入比率中，得到自归一化重要性采样 (SNIS) 估计量 $\\hat{f}_{SNIS}$：\n$$\n\\hat{f}_{SNIS} = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} f(x_i)w(x_i)}{\\frac{1}{n} \\sum_{i=1}^{n} w(x_i)} = \\frac{\\sum_{i=1}^{n} f(x_i)w(x_i)}{\\sum_{i=1}^{n} w(x_i)}\n$$\n其中 $w_i = w(x_i)$。这完成了推导的第一部分。\n\n**渐近方差的推导**\n\nSNIS 估计量 $\\hat{f}_{SNIS}$ 是两个样本均值的比值。为了求其渐近方差，我们采用多元中心极限定理和 Delta 方法。我们从每个样本 $x_i$ 中定义两个随机变量：$A_i = f(x_i)w(x_i)$ 和 $B_i = w(x_i)$。它们的样本均值是 $\\bar{A}_n = \\frac{1}{n}\\sum A_i$ 和 $\\bar{B}_n = \\frac{1}{n}\\sum B_i$，因此 $\\hat{f}_{SNIS} = \\bar{A}_n / \\bar{B}_n$。\n\n根据大数定律，当 $n \\to \\infty$ 时：\n$\\bar{A}_n \\to \\mu_A = \\mathbb{E}_{p_{\\theta}}[f(x)w(x)] = \\langle f \\rangle_{p^{\\star}}$\n$\\bar{B}_n \\to \\mu_B = \\mathbb{E}_{p_{\\theta}}[w(x)] = \\int \\frac{p^{\\star}(x)}{p_{\\theta}(x)} p_{\\theta}(x) \\,dx = \\int p^{\\star}(x) \\,dx = 1$（因为 $p^{\\star}$ 是一个归一化密度）。\n\n多元中心极限定理指出，样本均值的向量在分布上收敛于一个多元正态分布：\n$$\n\\sqrt{n} \\left( \\begin{pmatrix} \\bar{A}_n \\\\ \\bar{B}_n \\end{pmatrix} - \\begin{pmatrix} \\mu_A \\\\ \\mu_B \\end{pmatrix} \\right) \\xrightarrow{d} \\mathcal{N}\\left( \\mathbf{0}, \\Sigma \\right)\n$$\n其中 $\\Sigma$ 是向量 $(A_i, B_i)^T$ 的协方差矩阵：\n$$\n\\Sigma = \\begin{pmatrix} \\text{Var}_{p_{\\theta}}(A) & \\text{Cov}_{p_{\\theta}}(A, B) \\\\ \\text{Cov}_{p_{\\theta}}(A, B) & \\text{Var}_{p_{\\theta}}(B) \\end{pmatrix} = \\begin{pmatrix} \\text{Var}_{p_{\\theta}}(fw) & \\text{Cov}_{p_{\\theta}}(fw, w) \\\\ \\text{Cov}_{p_{\\theta}}(fw, w) & \\text{Var}_{p_{\\theta}}(w) \\end{pmatrix}\n$$\n我们关心的是函数 $g(a,b) = a/b$。估计量是 $\\hat{f}_{SNIS} = g(\\bar{A}_n, \\bar{B}_n)$。Delta 方法给出了该估计量的渐近分布。$\\sqrt{n}(\\hat{f}_{SNIS} - g(\\mu_A, \\mu_B))$ 的渐近分布的方差由 $V_{asym} = \\nabla g^T \\Sigma \\nabla g$ 给出，其中梯度 $\\nabla g$ 在 $(\\mu_A, \\mu_B)$ 处求值。\n\n$g(a,b)$ 的梯度是 $\\nabla g = (\\frac{\\partial g}{\\partial a}, \\frac{\\partial g}{\\partial b})^T = (1/b, -a/b^2)^T$。\n在 $(\\mu_A, \\mu_B) = (\\langle f \\rangle_{p^{\\star}}, 1)$ 处求值：\n$$\n\\nabla g(\\mu_A, \\mu_B) = \\left( 1, -\\langle f \\rangle_{p^{\\star}} \\right)^T\n$$\n那么渐近方差为：\n$$\nV_{asym} = \\begin{pmatrix} 1 & -\\langle f \\rangle_{p^{\\star}} \\end{pmatrix} \\begin{pmatrix} \\text{Var}_{p_{\\theta}}(fw) & \\text{Cov}_{p_{\\theta}}(fw, w) \\\\ \\text{Cov}_{p_{\\theta}}(fw, w) & \\text{Var}_{p_{\\theta}}(w) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -\\langle f \\rangle_{p^{\\star}} \\end{pmatrix}\n$$\n$V_{asym} = \\text{Var}_{p_{\\theta}}(fw) - 2\\langle f \\rangle_{p^{\\star}}\\text{Cov}_{p_{\\theta}}(fw, w) + \\langle f \\rangle_{p^{\\star}}^2\\text{Var}_{p_{\\theta}}(w)$\n这个表达式可以简化，通过识别它是一个随机变量线性组合的方差：\n$V_{asym} = \\text{Var}_{p_{\\theta}}(1 \\cdot (fw) - \\langle f \\rangle_{p^{\\star}} \\cdot w) = \\text{Var}_{p_{\\theta}}((f - \\langle f \\rangle_{p^{\\star}})w)$。\n\n我们可以展开这个方差：$\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$。\n令 $X = (f - \\langle f \\rangle_{p^{\\star}})w$。$X$ 的期望是：\n$\\mathbb{E}_{p_{\\theta}}[X] = \\mathbb{E}_{p_{\\theta}}[fw - \\langle f \\rangle_{p^{\\star}}w] = \\mathbb{E}_{p_{\\theta}}[fw] - \\langle f \\rangle_{p^{\\star}}\\mathbb{E}_{p_{\\theta}}[w]$。\n我们知道 $\\mathbb{E}_{p_{\\theta}}[fw] = \\langle f \\rangle_{p^{\\star}}$ 且 $\\mathbb{E}_{p_{\\theta}}[w] = 1$。\n所以，$\\mathbb{E}_{p_{\\theta}}[X] = \\langle f \\rangle_{p^{\\star}} - \\langle f \\rangle_{p^{\\star}}(1) = 0$。\n因此，方差就是 $X^2$ 的期望：\n$$\nV_{asym} = \\mathbb{E}_{p_{\\theta}}[((f(x) - \\langle f \\rangle_{p^{\\star}})w(x))^2] = \\mathbb{E}_{p_{\\theta}}[(f(x) - \\langle f \\rangle_{p^{\\star}})^2 w(x)^2]\n$$\n这就是我们所求的 SNIS 估计量的渐近方差表达式，用在提议分布 $p_{\\theta}$ 下的期望表示。对于大的 $n$，估计量 $\\hat{f}_{SNIS}$ 的方差近似为 $\\frac{1}{n} V_{asym}$。\n\n**特定情况下的计算**\n\n我们现在特化到生成模型是完美的情况：$\\mu = 0$ 和 $\\sigma = 1$。\n- 目标分布：$p^{\\star}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)$。\n- 提议分布：$p_{\\theta}(x) = \\frac{1}{\\sqrt{2\\pi}(1)} \\exp\\left(-\\frac{(x-0)^{2}}{2(1)^{2}}\\right) = p^{\\star}(x)$。\n- 可观测量：$f(x) = x^2$。\n\n首先，我们确定权重函数 $w(x)$：\n$$\nw(x) = \\frac{p^{\\star}(x)}{p_{\\theta}(x)} = \\frac{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)}{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)} = 1\n$$\n对于所有 $x$，权重都恰好为 $1$。\n\n接下来，我们计算可观测量的真实期望值 $\\langle f \\rangle_{p^{\\star}}$：\n$$\n\\langle f \\rangle_{p^{\\star}} = \\langle x^2 \\rangle_{p^{\\star}} = \\int_{-\\infty}^{\\infty} x^2 p^{\\star}(x) \\,dx\n$$\n这是标准正态分布 $\\mathcal{N}(0,1)$ 的二阶矩。因为均值为 $0$，所以二阶矩等于方差，即 $1$。\n$$\n\\langle f \\rangle_{p^{\\star}} = 1\n$$\n\n最后，我们将这些量代入渐近方差的公式中：\n$$\nV_{asym} = \\mathbb{E}_{p_{\\theta}}[(f(x) - \\langle f \\rangle_{p^{\\star}})^2 w(x)^2]\n$$\n$$\nV_{asym} = \\mathbb{E}_{p_{\\theta}}[(x^2 - 1)^2 (1)^2] = \\mathbb{E}_{p_{\\theta}}[(x^2 - 1)^2]\n$$\n期望是关于 $p_{\\theta}(x)$ 计算的，而 $p_{\\theta}(x)$ 是标准正态分布。\n我们展开多项式：$(x^2 - 1)^2 = x^4 - 2x^2 + 1$。\n利用期望的线性性质：\n$$\nV_{asym} = \\mathbb{E}_{p_{\\theta}}[x^4 - 2x^2 + 1] = \\mathbb{E}_{p_{\\theta}}[x^4] - 2\\mathbb{E}_{p_{\\theta}}[x^2] + \\mathbb{E}_{p_{\\theta}}[1]\n$$\n我们需要标准正态分布（$Z \\sim \\mathcal{N}(0,1)$）的矩：\n- $\\mathbb{E}_{p_{\\theta}}[1] = 1$。\n- $\\mathbb{E}_{p_{\\theta}}[x^2]$ 是方差，为 $1$。\n- $\\mathbb{E}_{p_{\\theta}}[x^4]$ 是四阶矩。对于一般的正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$，四阶中心矩是 $3\\sigma^4$。由于我们的均值 $\\mu=0$ 且标准差 $\\sigma=1$，所以四阶矩是 $3(1)^4 = 3$。\n\n代入这些值：\n$$\nV_{asym} = 3 - 2(1) + 1 = 3 - 2 + 1 = 2\n$$\n因此，在生成模型是完美的情况下，可观测量 $f(x)=x^2$ 的自归一化重要性采样估计量的渐近方差为 $2$。",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "除了对模型误差进行后处理校正，我们还可以通过将物理原理直接嵌入模型架构中来构建更好的模型。对于分子等物理系统，旋转和平移等对称性至关重要。本练习提供了一个构建 $\\mathrm{SE}(3)$ 等变生成器的具体实现，通过对等变属性进行数值验证，您将对如何强制实现几何一致性获得实践性的理解，这是现代物理和化学生成模型设计的基石。",
            "id": "3764911",
            "problem": "实现一个程序，用于构建和验证三维刚体位形的一个简单等变生成器，重点关注三维特殊欧几里得群，记作 $\\mathrm{SE}(3)$。此设定基于生成模型理论，例如生成对抗网络 (GAN) 和变分自编码器 (VAE)，其中生成器 $G$ 将一个潜表示 $z$ 映射到一个物理位形。要求是为给定的变换和潜输入，展示并数值验证生成器的 $\\mathrm{SE}(3)$-等变性，这在多尺度建模和分析中的跨尺度位形采样中是必需的。\n\n您必须使用以下基本设定：\n- 三维特殊正交群 $\\mathrm{SO}(3)$ 由旋转矩阵 $R \\in \\mathbb{R}^{3 \\times 3}$ 组成，满足 $R^\\top R = I$ 和 $\\det(R)=1$。特殊欧几里得群 $\\mathrm{SE}(3)$ 由数对 $(R,t)$ 组成，其中 $R \\in \\mathrm{SO}(3)$ 且 $t \\in \\mathbb{R}^3$，其复合运算定义为 $(R_1,t_1) \\cdot (R_2,t_2) = (R_1 R_2, R_1 t_2 + t_1)$。\n- 生成器 $G$ 在群作用 $h \\in \\mathrm{SE}(3)$ 下的等变性意味着 $G(h \\cdot z) = h \\cdot G(z)$，其中 $z$ 是一个潜变量，$\\cdot$ 表示在潜空间和输出位形空间上的适当群作用。\n\n将潜变量 $z$ 定义为一对向量 $(u, w)$，其中 $u \\in \\mathbb{R}^3$ 且 $w \\in \\mathbb{R}^3$。生成器 $G$ 通过从 $u$ 经由轴角指数映射构建旋转 $R$，并将平移 $t$ 设置为等于 $w$，从而将 $z$ 映射到刚体位形 $(R, t)$。$h = (R_h,t_h) \\in \\mathrm{SE}(3)$ 在潜变量上的左作用定义为 $h \\cdot (u,w) = (u', w')$，其中 $u'$ 是任意一个轴角向量，其指数映射产生 $R_h R$（$R$ 是 $u$ 的指数映射），并且 $w' = R_h w + t_h$。在输出上的相应左作用是通常的 $\\mathrm{SE}(3)$ 复合运算。\n\n您的任务是：\n1. 使用轴角旋转实现生成器 $G$ 和在潜空间上的左作用 $h \\cdot z$。\n2. 对每个测试用例，计算 $A = h \\cdot G(z)$ 和 $B = G(h \\cdot z)$，并通过计算以下量来数值验证等变性：\n   - 旋转误差角 $\\Delta_{\\mathrm{rot}}$（以弧度为单位），作为 $\\mathrm{SO}(3)$ 中的测地距离，即相对旋转 $R_A^{-1} R_B$ 的轴角向量的模长。\n   - 平移误差范数 $\\Delta_{\\mathrm{trans}} = \\| t_A - t_B \\|_2$。\n3. 如果 $\\Delta_{\\mathrm{rot}} \\leq \\varepsilon_{\\mathrm{rot}}$ 且 $\\Delta_{\\mathrm{trans}} \\leq \\varepsilon_{\\mathrm{trans}}$，则声明测试用例通过，使用旋转容差 $\\varepsilon_{\\mathrm{rot}} = 10^{-9}$ 弧度和无量纲单位的平移容差 $\\varepsilon_{\\mathrm{trans}} = 10^{-6}$。\n\n角度单位必须是弧度。平移单位是无量纲的。\n\n使用以下参数集测试套件，其中每个测试用例提供尺度参数 $\\alpha$ 和 $\\beta$、一个潜基 $(u_{\\mathrm{base}}, w_{\\mathrm{base}})$ 和一个变换 $h = (u_h, w_h)$；待采样的潜变量定义为 $u = \\alpha \\, u_{\\mathrm{base}}$ 和 $w = \\beta \\, w_{\\mathrm{base}}$：\n\n- 测试用例 1（一般情况）：\n  - $\\alpha = 0.5$, $\\beta = 1.2$,\n  - $u_{\\mathrm{base}} = [\\,0.3,\\,-0.2,\\,0.1\\,]$,\n  - $w_{\\mathrm{base}} = [\\,0.5,\\,-1.0,\\,0.25\\,]$,\n  - $u_h = [\\,0.2,\\,0.1,\\,-0.15\\,]$,\n  - $w_h = [\\,0.1,\\,-0.05,\\,0.2\\,]$.\n\n- 测试用例 2（单位元边界）：\n  - $\\alpha = 1.0$, $\\beta = 1.0$,\n  - $u_{\\mathrm{base}} = [\\,0.0,\\,0.0,\\,0.0\\,]$,\n  - $w_{\\mathrm{base}} = [\\,0.0,\\,0.0,\\,0.0\\,]$,\n  - $u_h = [\\,0.0,\\,0.0,\\,0.0\\,]$,\n  - $w_h = [\\,0.0,\\,0.0,\\,0.0\\,]$.\n\n- 测试用例 3（接近 $\\pi$ 旋转边缘）：\n  - $\\alpha = 1.0$, $\\beta = 0.1$,\n  - $u_{\\mathrm{base}} = [\\,\\pi - 10^{-6},\\,0.0,\\,0.0\\,]$,\n  - $w_{\\mathrm{base}} = [\\,0.01,\\,0.02,\\,-0.03\\,]$,\n  - $u_h = [\\,0.05,\\,-0.07,\\,0.02\\,]$,\n  - $w_h = [\\,0.0,\\,0.0,\\,0.0\\,]$.\n\n- 测试用例 4（小角度稳定性）：\n  - $\\alpha = 1.0$, $\\beta = 1.0$,\n  - $u_{\\mathrm{base}} = [\\,10^{-9},\\,-2\\cdot 10^{-9},\\,3\\cdot 10^{-9}\\,]$,\n  - $w_{\\mathrm{base}} = [\\,1.0,\\,2.0,\\,3.0\\,]$,\n  - $u_h = [\\,0.1,\\,0.0,\\,-0.1\\,]$,\n  - $w_h = [\\,0.0,\\,0.0,\\,0.0\\,]$.\n\n- 测试用例 5（大平移幅度）：\n  - $\\alpha = 1.0$, $\\beta = 1.0$,\n  - $u_{\\mathrm{base}} = [\\,0.1,\\,0.2,\\,-0.1\\,]$,\n  - $w_{\\mathrm{base}} = [\\,10^6,\\,-10^6,\\,5\\cdot 10^5\\,]$,\n  - $u_h = [\\,0.3,\\,-0.1,\\,0.4\\,]$,\n  - $w_h = [\\,10^3,\\,-2\\cdot 10^3,\\,3\\cdot 10^3\\,]$.\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，例如 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是一个布尔值，指示相应测试用例的等变性验证是否通过。",
            "solution": "该问题要求实现并数值验证一个用于刚体位形的等变生成器。此任务位于几何深度学习的数学框架内，特别涉及用于具有对称性的物理系统的生成模型，例如由特殊欧几里得群 $\\mathrm{SE}(3)$ 描述的系统。\n\n要验证的核心原理是**等变性**。如果对输入进行变换然后应用函数，与先应用函数再对输出进行变换得到的结果相同，则称一个函数（或神经网络层，或生成器）$G$ 对于某个群作用是等变的。形式上，对于来自一个群的变换 $h$，这表示为 $G(h \\cdot z) = h \\cdot G(z)$，其中 $z$ 是输入，`·` 表示在相应空间上的群作用。\n\n首先，我们对问题的组成部分进行形式化。\n刚体位形空间由特殊欧几里得群 $\\mathrm{SE}(3)$ 建模，其元素是数对 $(R, t)$，其中 $R$ 是 $\\mathrm{SO}(3)$ 中的一个三维旋转矩阵，$t$ 是 $\\mathbb{R}^3$ 中的一个三维平移向量。定义了位形空间上左作用的群复合律是 $(R_1,t_1) \\cdot (R_2,t_2) = (R_1 R_2, R_1 t_2 + t_1)$。\n\n生成器 $G$ 是一个将潜变量 $z = (u, w) \\in \\mathbb{R}^3 \\times \\mathbb{R}^3$ 映射到 $\\mathrm{SE}(3)$ 位形的函数。所提供的定义是：\n$$ G(z) = G((u, w)) = (\\mathrm{exp}(u), w) $$\n这里，$\\mathrm{exp}(u)$ 是从轴角向量 $u \\in \\mathbb{R}^3$（表示李代数 $\\mathfrak{so}(3)$ 的一个元素）到旋转矩阵 $R \\in \\mathrm{SO}(3)$ 的矩阵指数映射。模长 $\\|u\\|_2$ 是旋转角，方向 $u/\\|u\\|_2$ 是旋转轴。位形的平移部分简单地设置为 $w$。\n\n$h = (R_h, t_h) \\in \\mathrm{SE}(3)$ 在潜空间上的左作用定义为 $h \\cdot z = (u', w')$，其中：\n$$ w' = R_h w + t_h $$\n$$ \\mathrm{exp}(u') = R_h \\mathrm{exp}(u) $$\n$u'$ 的定义意味着它是复合旋转 $R_h R$ 的轴角表示，其中 $R = \\mathrm{exp}(u)$。这可以写成 $u' = \\mathrm{log}(R_h \\mathrm{exp}(u))$，其中 $\\mathrm{log}$ 是从 $\\mathrm{SO}(3)$ 到 $\\mathfrak{so}(3)$ 的矩阵对数映射。\n\n验证任务是数值验证等变性属性 $h \\cdot G(z) = G(h \\cdot z)$ 对给定的测试用例是否成立。我们计算等式两边并测量其差异。\n设 $h=(R_h, t_h)$，其中 $R_h = \\mathrm{exp}(u_h)$ 且 $t_h = w_h$。\n\n等式左侧，$A = (R_A, t_A) = h \\cdot G(z)$，计算如下：\n$$ (R_A, t_A) = h \\cdot (\\mathrm{exp}(u), w) = (R_h \\mathrm{exp}(u), R_h w + t_h) $$\n\n等式右侧，$B = (R_B, t_B) = G(h \\cdot z)$，计算如下：\n$$ (R_B, t_B) = G((u', w')) = (\\mathrm{exp}(u'), w') $$\n代入 $u'$ 和 $w'$ 的定义，我们得到：\n$$ R_B = \\mathrm{exp}(\\mathrm{log}(R_h \\mathrm{exp}(u))) $$\n$$ t_B = R_h w + t_h $$\n\n从解析上看，$R_B$ 简化为 $R_h \\mathrm{exp}(u)$，使得 $(R_B, t_B)$ 与 $(R_A, t_A)$ 相同。因此，数值验证就变成了对实现精度的测试。任何差异都源于运算序列中的浮点误差，特别是旋转部分的指数和对数映射的往返过程。\n\n为量化数值差异，我们使用两个度量：\n1.  **旋转误差 ($\\Delta_{\\mathrm{rot}}$)**：$R_A$ 和 $R_B$ 在 $\\mathrm{SO}(3)$ 中的测地距离。这被计算为相对旋转 $R_A^{-1} R_B = R_A^\\top R_B$ 的角度。\n    $$ \\Delta_{\\mathrm{rot}} = \\|\\mathrm{log}(R_A^\\top R_B)\\|_2 $$\n2.  **平移误差 ($\\Delta_{\\mathrm{trans}}$)**：$t_A$ 和 $t_B$ 之间的标准欧几里得距离。\n    $$ \\Delta_{\\mathrm{trans}} = \\|t_A - t_B\\|_2 $$\n\n实现利用了 `scipy.spatial.transform.Rotation` 类，该类提供了数值稳健的方法，用于在轴角向量 (`rotvec`) 和旋转矩阵之间进行转换。这对于处理边缘情况至关重要，例如非常小的旋转角（需要泰勒展开以避免除以零）和接近 $\\pi$ 的旋转角（在某些公式中是奇点）。\n\n对于每个测试用例，算法执行以下步骤：\n1.  根据给定参数构建潜变量 $z=(u, w)$ 和变换 $h=(R_h, t_h)$。\n2.  通过先应用生成器再应用变换来计算位形 $A = (R_A, t_A)$。\n3.  通过对潜空间应用变换再应用生成器来计算位形 $B = (R_B, t_B)$。\n4.  计算误差 $\\Delta_{\\mathrm{rot}}$ 和 $\\Delta_{\\mathrm{trans}}$。\n5.  如果 $\\Delta_{\\mathrm{rot}} \\le 10^{-9}$ 且 $\\Delta_{\\mathrm{trans}} \\le 10^{-6}$，则测试用例通过。收集结果并格式化为布尔值列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.transform import Rotation\n\ndef solve():\n    \"\"\"\n    Constructs and verifies a simple SE(3)-equivariant generator.\n    \"\"\"\n    # Define tolerances for verification\n    TOL_ROT = 1e-9\n    TOL_TRANS = 1e-6\n\n    # Define the test suite of parameter sets\n    test_cases = [\n        # Test case 1 (general case)\n        {\n            \"alpha\": 0.5, \"beta\": 1.2,\n            \"u_base\": np.array([0.3, -0.2, 0.1]),\n            \"w_base\": np.array([0.5, -1.0, 0.25]),\n            \"u_h\": np.array([0.2, 0.1, -0.15]),\n            \"w_h\": np.array([0.1, -0.05, 0.2])\n        },\n        # Test case 2 (identity boundary)\n        {\n            \"alpha\": 1.0, \"beta\": 1.0,\n            \"u_base\": np.array([0.0, 0.0, 0.0]),\n            \"w_base\": np.array([0.0, 0.0, 0.0]),\n            \"u_h\": np.array([0.0, 0.0, 0.0]),\n            \"w_h\": np.array([0.0, 0.0, 0.0])\n        },\n        # Test case 3 (near pi rotation edge)\n        {\n            \"alpha\": 1.0, \"beta\": 0.1,\n            \"u_base\": np.array([np.pi - 1e-6, 0.0, 0.0]),\n            \"w_base\": np.array([0.01, 0.02, -0.03]),\n            \"u_h\": np.array([0.05, -0.07, 0.02]),\n            \"w_h\": np.array([0.0, 0.0, 0.0])\n        },\n        # Test case 4 (small-angle stability)\n        {\n            \"alpha\": 1.0, \"beta\": 1.0,\n            \"u_base\": np.array([1e-9, -2e-9, 3e-9]),\n            \"w_base\": np.array([1.0, 2.0, 3.0]),\n            \"u_h\": np.array([0.1, 0.0, -0.1]),\n            \"w_h\": np.array([0.0, 0.0, 0.0])\n        },\n        # Test case 5 (large translation magnitude)\n        {\n            \"alpha\": 1.0, \"beta\": 1.0,\n            \"u_base\": np.array([0.1, 0.2, -0.1]),\n            \"w_base\": np.array([1e6, -1e6, 5e5]),\n            \"u_h\": np.array([0.3, -0.1, 0.4]),\n            \"w_h\": np.array([1e3, -2e3, 3e3])\n        }\n    ]\n\n    results = []\n    \n    # Helper functions for SO(3) exponential and logarithm maps\n    def exp_map(u_vec):\n        \"\"\"Maps an axis-angle vector to a rotation matrix.\"\"\"\n        return Rotation.from_rotvec(u_vec).as_matrix()\n\n    def log_map(R_mat):\n        \"\"\"Maps a rotation matrix to an axis-angle vector.\"\"\"\n        return Rotation.from_matrix(R_mat).as_rotvec()\n\n    for case in test_cases:\n        # Step 1: Define inputs for the current test case\n        # Latent variable z = (u, w)\n        u = case[\"alpha\"] * case[\"u_base\"]\n        w = case[\"beta\"] * case[\"w_base\"]\n        \n        # SE(3) transformation h = (R_h, t_h)\n        u_h = case[\"u_h\"]\n        t_h = case[\"w_h\"]\n        R_h = exp_map(u_h)\n\n        # Step 2: Compute the left side of the equivariance equation: A = h · G(z)\n        # The generator G maps z to configuration (R, t)\n        R = exp_map(u)\n        t = w\n        \n        # The left action of h on G(z) is the SE(3) composition\n        # A = (R_A, t_A) = (R_h * R, R_h * t + t_h)\n        R_A = R_h @ R\n        t_A = R_h @ t + t_h\n\n        # Step 3: Compute the right side of the equivariance equation: B = G(h · z)\n        # First, compute the transformed latent variable h · z = (u', w')\n        # The action on w is w' = R_h * w + t_h\n        w_prime = R_h @ w + t_h\n        \n        # The action on u gives u' such that exp(u') = R_h * R\n        # This is equivalent to u' = log(R_h * R)\n        u_prime = log_map(R_h @ R)\n        \n        # Then, apply the generator G to the transformed latent z' = (u', w')\n        # B = (R_B, t_B) = G(u', w') = (exp(u'), w')\n        R_B = exp_map(u_prime)\n        t_B = w_prime\n\n        # Step 4: Compute errors and verify against tolerances\n        # Rotation error: geodesic distance in SO(3)\n        # Relative rotation is R_A^{-1} * R_B = R_A^T * R_B\n        R_relative = R_A.T @ R_B\n        # The error angle is the magnitude of the axis-angle vector of the relative rotation\n        delta_rot = np.linalg.norm(log_map(R_relative))\n        \n        # Translation error: Euclidean norm of the difference vector\n        delta_trans = np.linalg.norm(t_A - t_B)\n        \n        # Check if both errors are within the given tolerances\n        passed = (delta_rot = TOL_ROT) and (delta_trans = TOL_TRANS)\n        results.append(passed)\n\n    # Final print statement in the exact required format \"[True,True,True,True,True]\"\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "为具有复杂多模态能量景观的物理系统训练生成模型是一项重大挑战，直接训练很容易陷入糟糕的局部最小值。本练习将探索课程学习（curriculum learning）这一高级训练策略，即模型通过循序渐进地学习一系列从易到难的任务来进行训练。您将实现一个完整的训练流程，并定量地比较课程学习与标准基线方法的效果，分析其对最终采样质量和损失函数景观平滑度的影响。",
            "id": "3765027",
            "problem": "您被要求形式化、实现并定量评估一种课程训练策略，该策略用于多尺度建模与分析中构型采样的简化生成模型。模型、目标、课程和评估指标必须从第一性原理定义，不得使用简化的公式。您的任务是推导并实现一个最小但严谨的实验，将课程训练与在最终复杂目标上的直接训练进行比较。\n\n形式化。设生成器族为维度 $d$ 的各向同性高斯模型，由均值 $m \\in \\mathbb{R}^d$ 和尺度 $s \\in \\mathbb{R}_{0}$ 参数化，其参数向量为 $\\theta = (m, \\log s) \\in \\mathbb{R}^{d+1}$。用 $Q_\\theta = \\mathcal{N}(m, s^2 I_d)$ 表示模型分布。复杂度水平为 $c$ 的目标数据分布是在 $\\mathbb{R}^d$ 中位于半径为 $R$ 的圆上的 $k$ 个各向同性高斯分量的均匀混合，其公共协方差为 $\\tau^2 I_d$。具体来说，$P_c = \\sum_{j=1}^k w_j \\mathcal{N}(\\mu_j, \\tau^2 I_d)$，其中 $w_j = 1/k$，且对于 $d=2$，$\\mu_j = R [\\cos(2\\pi j/k), \\sin(2\\pi j/k)]^\\top$（如果 $d2$ 则用零扩展）。课程是复杂度序列 $c_1 \\prec c_2 \\prec \\cdots \\prec c_T$，其中 $k$ 和 $R$ 不减，并最终达到指定的目标 $c_T$。\n\n目标。使用带高斯核的平方最大均值差异 (MMD) 作为训练目标。对于正核带宽参数 $\\ell \\in \\mathbb{R}_{0}$ 和高斯径向基函数核 $k(x,y) = \\exp\\left(-\\frac{\\|x-y\\|_2^2}{2 \\ell^2}\\right)$，分布 $P$ 和 $Q$ 之间的平方最大均值差异为\n$$\n\\mathrm{MMD}^2(P,Q) = \\mathbb{E}_{x,x' \\sim P}[k(x,x')] + \\mathbb{E}_{y,y' \\sim Q}[k(y,y')] - 2 \\mathbb{E}_{x \\sim P, y \\sim Q}[k(x,y)]。\n$$\n对于高斯输入， $k(x,y)$ 的期望可以从以下经过充分检验的事实以闭式形式表示：如果 $x \\sim \\mathcal{N}(m_1, S_1)$ 和 $y \\sim \\mathcal{N}(m_2, S_2)$ 是独立的，则\n$$\n\\mathbb{E}\\left[\\exp\\left(-\\frac{\\|x-y\\|_2^2}{2 \\ell^2}\\right)\\right]\n=\n\\left\\lvert I_d + \\frac{S_1 + S_2}{\\ell^2} \\right\\rvert^{-\\frac{1}{2}}\n\\exp\\left(-\\frac{1}{2} (m_1 - m_2)^\\top \\left(S_1 + S_2 + \\ell^2 I_d \\right)^{-1} (m_1 - m_2)\\right)。\n$$\n仅使用此恒等式作为基础构建块，通过期望的线性性，推导目标函数 $\\mathcal{L}(\\theta; c, \\ell) = \\mathrm{MMD}^2(P_c, Q_\\theta)$ 的闭式表达式，对于固定的 $c$ 和 $\\ell$，该函数是 $\\theta$ 的平滑函数。\n\n损失景观和采样质量指标。为量化课程训练对损失景观和采样质量的影响：\n- 通过在参数 $\\hat{\\theta}$ 处 $\\mathcal{L}(\\theta; c_T, \\ell)$ 的Hessian矩阵的谱条件数来定义损失景观曲率，即 $\\kappa(\\hat{\\theta}) = \\lambda_{\\max}(H) / \\lambda_{\\min}(H)$，其中 $H$ 是关于 $\\theta$ 坐标的二阶导数的对称Hessian矩阵，$\\lambda_{\\min}(H)$ 表示最小的严格正特征值。Hessian矩阵必须使用围绕 $\\hat{\\theta}$ 的二阶中心有限差分进行数值逼近。如果需要，通过将 $H$ 与其转置相加求平均来确保数值对称性，并且仅在需要时进行正则化，通过将特征值裁剪到小的正阈值以下来避免除以零。\n- 通过在最终参数估计下相对于最终目标复杂度的平方最大均值差异，即达到的目标值 $\\mathcal{L}(\\hat{\\theta}; c_T, \\ell)$，来定义在 $\\hat{\\theta}$ 处的采样质量。\n\n训练协议。给定最终目标复杂度 $c_T$ 和带宽 $\\ell$，考虑两种训练方案：\n- 课程训练：从初始参数 $\\theta_0$ 开始，对 $t = 1, \\dots, T$ 顺序最小化 $\\mathcal{L}(\\theta; c_t, \\ell)$，使用阶段 $t$ 的最小化器作为阶段 $t+1$ 的初始化。通过第一性原理梯度下降法和Armijo回溯线搜索，在 $\\theta = (m, \\log s)$ 中进行无约束优化，其中梯度通过中心有限差分近似。所有更新必须直接在 $\\theta$ 上执行；模型中使用的尺度为 $s = \\exp(\\log s)$。\n- 基线（非课程）训练：从相同的初始参数 $\\theta_0$ 开始，直接最小化 $\\mathcal{L}(\\theta; c_T, \\ell)$，使用的梯度下降总迭代次数与所有课程阶段使用的总次数相同。\n\n对于两种方案，仅使用上述定义的目标函数。梯度和Hessian矩阵必须使用中心差分进行数值计算。Armijo规则必须使用具有固定常数和几何回溯的标准充分下降条件。\n\n测试套件。固定 $d = 2$，对应于 $m = (0,0)$ 和 $\\log s = 0$ 的初始参数 $\\theta_0$，以及分量方差 $\\tau^2 = 0.05$。对于每个测试用例，定义最终目标复杂度 $c_T = (k, R)$ 和核带宽 $\\ell$，以及一个阶段为 $c_1 \\prec \\cdots \\prec c_T$ 的课程计划，该计划由不减的 $k$ 和 $R$ 值定义。在课程中为每个阶段使用相同的迭代预算，并相应地匹配基线的总迭代预算。除非另有说明，否则使用以下通用数值设置：用于梯度和Hessian矩阵的有限差分步长 $h = 10^{-4}$，Armijo常数 $\\alpha = 10^{-4}$，回溯因子 $\\beta = 0.5$，初始步长 $1.0$。每个阶段最多使用 $N_s = 40$ 次迭代。使用以下三个测试用例：\n- 案例A（边界）：$k = 1$, $R = 1.0$, $\\ell = 1.0$, 课程计划 $[(1, 1.0)]$。\n- 案例B（理想路径）：$k = 4$, $R = 1.0$, $\\ell = 0.5$, 课程计划 $[(1, 0.5), (2, 0.8), (4, 1.0)]$。\n- 案例C（边缘，更锐利的核和更多模式）：$k = 6$, $R = 1.2$, $\\ell = 0.25$, 课程计划 $[(1, 0.4), (3, 0.8), (6, 1.2)]$。\n\n对于每个案例，执行课程训练和基线训练，计算最终采样质量 $\\mathcal{L}(\\hat{\\theta}; c_T, \\ell)$ 和最终参数 $\\hat{\\theta}$ 在最终目标 $c_T$ 下的Hessian条件数 $\\kappa(\\hat{\\theta})$。定义每个案例报告的两个指标为：\n- $\\Delta \\mathrm{MMD}^2 = \\mathcal{L}(\\hat{\\theta}_{\\mathrm{baseline}}; c_T, \\ell) - \\mathcal{L}(\\hat{\\theta}_{\\mathrm{curriculum}}; c_T, \\ell)$,\n- $\\rho_\\kappa = \\kappa(\\hat{\\theta}_{\\mathrm{baseline}}) / \\kappa(\\hat{\\theta}_{\\mathrm{curriculum}})$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，按A、B、C的顺序聚合三个案例的两个指标，每个案例内的顺序为 $(\\Delta \\mathrm{MMD}^2, \\rho_\\kappa)$。确切格式必须是：\n\"[$\\Delta \\mathrm{MMD}^2_{\\mathrm{A}},\\rho_{\\kappa,\\mathrm{A}},\\Delta \\mathrm{MMD}^2_{\\mathrm{B}},\\rho_{\\kappa,\\mathrm{B}},\\Delta \\mathrm{MMD}^2_{\\mathrm{C}},\\rho_{\\kappa,\\mathrm{C}}$]\"。\n所有数值结果都必须是实数。此问题不涉及物理单位或角度。",
            "solution": "该问题要求对一个简单的生成模型，在基于课程的训练策略和直接训练基线之间进行严格的定量比较。比较将在学习一个高斯混合分布的特定任务上进行。评估基于最终达到的目标值和优化参数处损失景观的曲率。\n\n首先，我们按照规定将问题各组成部分形式化。\n\n**1. 模型和目标分布**\n\n生成模型是一个 $d$ 维各向同性高斯分布 $Q_\\theta = \\mathcal{N}(m, s^2 I_d)$。其参数是均值 $m \\in \\mathbb{R}^d$ 和尺度 $s \\in \\mathbb{R}_{0}$。对于无约束优化，我们使用参数向量 $\\theta = (m, \\log s) \\in \\mathbb{R}^{d+1}$，从中可恢复 $s$ 为 $s = \\exp(\\log s)$。在此问题中，维度固定为 $d=2$。\n\n复杂度水平为 $c=(k, R)$ 的目标分布是 $k$ 个各向同性高斯分量的均匀混合，$P_c = \\sum_{j=1}^k w_j \\mathcal{N}(\\mu_j, \\tau^2 I_d)$，其中权重是均匀的，$w_j = 1/k$，且分量均值 $\\mu_j$ 位于半径为 $R$ 的圆上：\n$$\n\\mu_j = R \\begin{pmatrix} \\cos(2\\pi j/k) \\\\ \\sin(2\\pi j/k) \\end{pmatrix} \\in \\mathbb{R}^2, \\quad \\text{for } j=1, \\dots, k.\n$$\n分量协方差为 $\\tau^2 I_d$，其中 $\\tau^2$ 是一个固定常数。\n\n**2. 目标函数：平方最大均值差异 (MMD)**\n\n训练目标是使用高斯核的平方MMD，$k(x,y) = \\exp\\left(-\\frac{\\|x-y\\|_2^2}{2 \\ell^2}\\right)$，其中 $\\ell$ 是核带宽。两个分布 $P$ 和 $Q$ 之间的平方MMD由下式给出：\n$$\n\\mathcal{L}(\\theta; c, \\ell) = \\mathrm{MMD}^2(P_c, Q_\\theta) = \\mathbb{E}_{x,x' \\sim P_c}[k(x,x')] + \\mathbb{E}_{y,y' \\sim Q_\\theta}[k(y,y')] - 2 \\mathbb{E}_{x \\sim P_c, y \\sim Q_\\theta}[k(x,y)].\n$$\n为了推导此目标的闭式表达式，我们使用提供的关于两个独立高斯随机变量 $x \\sim \\mathcal{N}(m_1, S_1)$ 和 $y \\sim \\mathcal{N}(m_2, S_2)$ 之间期望核值的恒等式：\n$$\nE(m_1, S_1, m_2, S_2, \\ell) = \\mathbb{E}\\left[k(x,y)\\right] = \\left\\lvert I_d + \\frac{S_1 + S_2}{\\ell^2} \\right\\rvert^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2} (m_1 - m_2)^\\top \\left(S_1 + S_2 + \\ell^2 I_d \\right)^{-1} (m_1 - m_2)\\right).\n$$\n由于本问题中所有的协方差都是各向同性的（即形式为 $\\sigma^2 I_d$），我们可以简化此表达式。对于 $S_1 = \\sigma_1^2 I_d$ 和 $S_2 = \\sigma_2^2 I_d$，该恒等式简化为：\n$$\nH(m_1, \\sigma_1^2, m_2, \\sigma_2^2, \\ell) = \\left(1 + \\frac{\\sigma_1^2 + \\sigma_2^2}{\\ell^2}\\right)^{-d/2} \\exp\\left(-\\frac{\\|m_1 - m_2\\|^2}{2(\\sigma_1^2 + \\sigma_2^2 + \\ell^2)}\\right).\n$$\n使用这个简化的辅助函数 $H$ 和期望的线性性，我们可以表示 $\\mathrm{MMD}^2$ 损失的三个项：\n\n第1项：$\\mathbb{E}_{x,x' \\sim P_c}[k(x,x')]$\n由于 $x$ 和 $x'$ 是从混合分布 $P_c = \\frac{1}{k}\\sum_{i=1}^k \\mathcal{N}(\\mu_i, \\tau^2 I_d)$ 中独立抽取的，我们有：\n$$\n\\mathbb{E}_{x,x' \\sim P_c}[k(x,x')] = \\sum_{i=1}^k \\sum_{j=1}^k \\frac{1}{k^2} \\mathbb{E}_{x_i \\sim \\mathcal{N}(\\mu_i, \\tau^2 I_d), x'_j \\sim \\mathcal{N}(\\mu_j, \\tau^2 I_d)}[k(x_i, x'_j)] = \\frac{1}{k^2} \\sum_{i=1}^k \\sum_{j=1}^k H(\\mu_i, \\tau^2, \\mu_j, \\tau^2, \\ell).\n$$\n\n第2项：$\\mathbb{E}_{y,y' \\sim Q_\\theta}[k(y,y')]$\n这里，$y, y' \\sim \\mathcal{N}(m, s^2 I_d)$ 是从模型分布中独立抽取的：\n$$\n\\mathbb{E}_{y,y' \\sim Q_\\theta}[k(y,y')] = H(m, s^2, m, s^2, \\ell).\n$$\n\n第3项：$-2 \\mathbb{E}_{x \\sim P_c, y \\sim Q_\\theta}[k(x,y)]$\n样本 $x$ 从混合分布 $P_c$ 中抽取，样本 $y$ 从模型 $Q_\\theta$ 中抽取：\n$$\n-2 \\mathbb{E}_{x \\sim P_c, y \\sim Q_\\theta}[k(x,y)] = -2 \\sum_{j=1}^k \\frac{1}{k} \\mathbb{E}_{x_j \\sim \\mathcal{N}(\\mu_j, \\tau^2 I_d), y \\sim \\mathcal{N}(m, s^2 I_d)}[k(x_j, y)] = -\\frac{2}{k} \\sum_{j=1}^k H(\\mu_j, \\tau^2, m, s^2, \\ell).\n$$\n结合这些项，得到完整且可微的目标函数 $\\mathcal{L}(\\theta; c, \\ell)$。\n\n**3. 训练和评估**\n\n训练通过对参数向量 $\\theta = (m_x, m_y, \\log s)$ 进行梯度下降来执行。梯度 $\\nabla_\\theta \\mathcal{L}$ 使用步长为 $h$ 的中心有限差分进行数值计算：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_i} \\approx \\frac{\\mathcal{L}(\\theta + h e_i) - \\mathcal{L}(\\theta - h e_i)}{2h}.\n$$\n每次梯度下降更新的步长由Armijo回溯线搜索确定，这确保了每次迭代中目标函数的充分下降。\n\n评估指标在训练结束时针对最终目标复杂度 $c_T$ 和带宽 $\\ell$ 进行计算。\n- **采样质量**：最终损失值 $\\mathcal{L}(\\hat{\\theta}; c_T, \\ell)$。\n- **损失景观曲率**：最终目标 $\\mathcal{L}(\\theta; c_T, \\ell)$ 在最终参数 $\\hat{\\theta}$ 处评估的Hessian矩阵 $H$ 的谱条件数 $\\kappa(\\hat{\\theta})$。Hessian矩阵也使用二阶中心有限差分进行数值计算：\n  $$\n  H_{ii} \\approx \\frac{\\mathcal{L}(\\hat{\\theta}+he_i) - 2\\mathcal{L}(\\hat{\\theta}) + \\mathcal{L}(\\hat{\\theta}-he_i)}{h^2}\n  $$\n  $$\n  H_{ij} \\approx \\frac{\\mathcal{L}(\\hat{\\theta}+he_i+he_j) - \\mathcal{L}(\\hat{\\theta}+he_i-he_j) - \\mathcal{L}(\\hat{\\theta}-he_i+he_j) + \\mathcal{L}(\\hat{\\theta}-he_i-he_j)}{4h^2}\n  $$\n  条件数为 $\\kappa(\\hat{\\theta}) = \\lambda_{\\max}(H) / \\lambda_{\\min\\_pos}(H)$，其中 $\\lambda_{\\min\\_pos}$ 是最小的严格正特征值，通过将任何低于小正阈值 $\\epsilon  0$ 的特征值裁剪到 $\\epsilon$ 来强制实现。\n\n该实验比较了两种训练方案：\n- **课程训练**：对一系列递增的复杂度 $c_1, \\dots, c_T$ 顺序优化目标，每个阶段使用固定数量的迭代次数 $N_s$。一个阶段的结果初始化下一个阶段。\n- **基线训练**：直接为最终复杂度 $c_T$ 优化目标，使用的总迭代次数等于课程训练中使用的总次数 ($N_s \\times T$)。\n\n两种方案都从对应于标准正态分布的相同初始参数 $\\theta_0$ 开始，即 $m=(0,0)$ 和 $\\log s = 0$。比较基于两个派生指标：$\\Delta \\mathrm{MMD}^2$，即基线和课程之间最终损失的差异，以及 $\\rho_\\kappa$，即它们最终Hessian条件数的比率。正的 $\\Delta \\mathrm{MMD}^2$ 表示课程取得了更好（更低）的最终损失。$\\rho_\\kappa  1$ 的值表示课程在损失景观的一个条件更好（更平坦）的区域找到了解。\n\n实现遵循这些原则，以执行指定的测试案例并计算最终指标。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the curriculum training experiment as specified.\n    \"\"\"\n    # --- Universal Numerical Settings ---\n    D = 2\n    TAU_SQ = 0.05\n    H_FD = 1e-4\n    ARMIJO_ALPHA = 1e-4\n    ARMIJO_BETA = 0.5\n    INIT_STEP_SIZE = 1.0\n    N_S = 40\n    EIG_CLIP_THRESHOLD = 1e-10\n    THETA_0 = np.array([0.0, 0.0, 0.0]) # m=(0,0), log(s)=0\n\n    # --- Test Cases Definition ---\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"k\": 1, \"R\": 1.0, \"l\": 1.0,\n            \"schedule\": [(1, 1.0)]\n        },\n        {\n            \"name\": \"B\",\n            \"k\": 4, \"R\": 1.0, \"l\": 0.5,\n            \"schedule\": [(1, 0.5), (2, 0.8), (4, 1.0)]\n        },\n        {\n            \"name\": \"C\",\n            \"k\": 6, \"R\": 1.2, \"l\": 0.25,\n            \"schedule\": [(1, 0.4), (3, 0.8), (6, 1.2)]\n        }\n    ]\n\n    # --- Core Algorithmic Components ---\n\n    def exp_k_xy_gauss(m1, s1_sq, m2, s2_sq, l_sq):\n        \"\"\"Computes E[k(x,y)] for isotropic Gaussians x and y.\"\"\"\n        term1_base = 1.0 + (s1_sq + s2_sq) / l_sq\n        term1 = term1_base ** (-D / 2.0)\n        m_diff_sq = np.sum((m1 - m2)**2)\n        term2_denom = 2.0 * (s1_sq + s2_sq + l_sq)\n        term2 = np.exp(-m_diff_sq / term2_denom)\n        return term1 * term2\n\n    _mu_cache = {}\n    def compute_mu(k, R):\n        \"\"\"Computes means of the target Gaussian mixture components.\"\"\"\n        if (k, R) in _mu_cache:\n            return _mu_cache[(k, R)]\n        if k == 0:\n            return np.array([])\n        angles = 2 * np.pi * np.arange(1, k + 1) / k\n        mus = np.zeros((k, D))\n        mus[:, 0] = R * np.cos(angles)\n        mus[:, 1] = R * np.sin(angles)\n        _mu_cache[(k, R)] = mus\n        return mus\n\n    _loss_cache = {}\n    def mmd2_loss(theta_tuple, k, R, l, tau_sq):\n        \"\"\"Computes the squared MMD loss L(theta; c, l).\"\"\"\n        theta_key = (theta_tuple, k, R, l, tau_sq)\n        if theta_key in _loss_cache:\n            return _loss_cache[theta_key]\n\n        theta = np.array(theta_tuple)\n        m = theta[:D]\n        s_sq = np.exp(2 * theta[D])\n        l_sq = l**2\n        mus = compute_mu(k, R)\n\n        # Term 1: E_{x,x' ~ P}[k(x,x')]\n        term1 = 0.0\n        if k > 0:\n            for i in range(k):\n                for j in range(k):\n                    term1 += exp_k_xy_gauss(mus[i], tau_sq, mus[j], tau_sq, l_sq)\n            term1 /= (k**2)\n\n        # Term 2: E_{y,y' ~ Q}[k(y,y')]\n        term2 = exp_k_xy_gauss(m, s_sq, m, s_sq, l_sq)\n\n        # Term 3: -2 * E_{x ~ P, y ~ Q}[k(x,y)]\n        term3 = 0.0\n        if k > 0:\n            for j in range(k):\n                term3 += exp_k_xy_gauss(mus[j], tau_sq, m, s_sq, l_sq)\n            term3 *= (2.0 / k)\n            \n        result = term1 + term2 - term3\n        _loss_cache[theta_key] = result\n        return result\n\n    def gradient(func, theta, h):\n        \"\"\"Computes gradient using central finite differences.\"\"\"\n        grad = np.zeros_like(theta)\n        for i in range(len(theta)):\n            theta_plus_h, theta_minus_h = theta.copy(), theta.copy()\n            theta_plus_h[i] += h\n            theta_minus_h[i] -= h\n            grad[i] = (func(tuple(theta_plus_h)) - func(tuple(theta_minus_h))) / (2 * h)\n        return grad\n\n    def gradient_descent_armijo(func, theta_0, max_iter):\n        \"\"\"Performs gradient descent with Armijo backtracking line search.\"\"\"\n        theta = theta_0.copy()\n        for _ in range(max_iter):\n            loss_val = func(tuple(theta))\n            grad_val = gradient(func, theta, H_FD)\n            eta = INIT_STEP_SIZE\n            while True:\n                theta_new = theta - eta * grad_val\n                sufficient_decrease = loss_val - ARMIJO_ALPHA * eta * np.dot(grad_val, grad_val)\n                if func(tuple(theta_new)) = sufficient_decrease:\n                    break\n                eta *= ARMIJO_BETA\n                if eta  1e-12: break \n            theta = theta_new\n        return theta\n\n    def hessian(func, theta, h):\n        \"\"\"Computes Hessian using second-order central finite differences.\"\"\"\n        n = len(theta)\n        H = np.zeros((n, n))\n        f_theta = func(tuple(theta))\n        for i in range(n):\n            for j in range(i, n):\n                if i == j:\n                    tp, tm = theta.copy(), theta.copy()\n                    tp[i] += h; tm[i] -= h\n                    H[i, i] = (func(tuple(tp)) - 2 * f_theta + func(tuple(tm))) / (h**2)\n                else:\n                    tpp, tpm, tmp, tmm = theta.copy(), theta.copy(), theta.copy(), theta.copy()\n                    tpp[i]+=h; tpp[j]+=h\n                    tpm[i]+=h; tpm[j]-=h\n                    tmp[i]-=h; tmp[j]+=h\n                    tmm[i]-=h; tmm[j]-=h\n                    val = (func(tuple(tpp)) - func(tuple(tpm)) - func(tuple(tmp)) + func(tuple(tmm))) / (4 * h**2)\n                    H[i, j] = H[j, i] = val\n        H = (H + H.T) / 2.0\n        return H\n\n    def condition_number(H):\n        \"\"\"Computes the condition number from the Hessian.\"\"\"\n        eigenvalues = np.linalg.eigvalsh(H)\n        eigenvalues = np.maximum(eigenvalues, EIG_CLIP_THRESHOLD)\n        # Check for non-finite eigenvalues which can happen with extreme curvature\n        if not np.all(np.isfinite(eigenvalues)):\n            return np.inf\n        min_eig = np.min(eigenvalues)\n        if min_eig = 0:\n            return np.inf\n        return np.max(eigenvalues) / min_eig\n\n    results = []\n    for case in test_cases:\n        _loss_cache.clear()\n        k_final, R_final, l_final = case[\"k\"], case[\"R\"], case[\"l\"]\n        schedule = case[\"schedule\"]\n        \n        loss_final = lambda theta_tuple: mmd2_loss(theta_tuple, k_final, R_final, l_final, TAU_SQ)\n        \n        # Curriculum Training\n        theta_curr = THETA_0.copy()\n        for k_stage, R_stage in schedule:\n            loss_stage = lambda t_tuple: mmd2_loss(t_tuple, k_stage, R_stage, l_final, TAU_SQ)\n            theta_curr = gradient_descent_armijo(loss_stage, theta_curr, N_S)\n        \n        theta_hat_curr = theta_curr\n        mmd2_curr = loss_final(tuple(theta_hat_curr))\n        H_curr = hessian(loss_final, theta_hat_curr, H_FD)\n        kappa_curr = condition_number(H_curr)\n\n        # Baseline Training\n        total_iter = N_S * len(schedule)\n        theta_base = gradient_descent_armijo(loss_final, THETA_0.copy(), total_iter)\n\n        theta_hat_base = theta_base\n        mmd2_base = loss_final(tuple(theta_hat_base))\n        H_base = hessian(loss_final, theta_hat_base, H_FD)\n        kappa_base = condition_number(H_base)\n\n        # Calculate and store metrics\n        delta_mmd2 = mmd2_base - mmd2_curr\n        rho_kappa = kappa_base / kappa_curr if kappa_curr != 0 and np.isfinite(kappa_curr) else np.inf\n        \n        results.extend([delta_mmd2, rho_kappa])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}