{
    "hands_on_practices": [
        {
            "introduction": "The core of the Fourier Neural Operator is its spectral convolution layer, which operates in Fourier space to manipulate the frequency components of the input function. For most physical systems where inputs and outputs are real-valued, their Fourier transforms exhibit conjugate symmetry. This exercise  is a crucial step in implementing an FNO correctly, as it demonstrates how to enforce this symmetry on the learnable kernel to ensure physically consistent outputs and an efficient parameterization.",
            "id": "3787660",
            "problem": "Consider a Fourier Neural Operator (FNO) applied to a two-dimensional periodic domain for learning solution operators of multiscale partial differential equations. Let the input and output fields be real-valued, with $C_{\\text{in}}$ input channels and $C_{\\text{out}}$ output channels, respectively. In the spectral convolution layer, the learned complex-valued kernel is a tensor $R(\\mathbf{k}) \\in \\mathbb{C}^{C_{\\text{out}} \\times C_{\\text{in}}}$, indexed by discrete two-dimensional wavenumbers $\\mathbf{k} = (k_{x}, k_{y})$ drawn from the truncated rectangular set\n$$\n\\mathcal{I} = \\{ (k_{x}, k_{y}) \\,:\\, k_{x} \\in \\{-K_{x}, -K_{x}+1, \\dots, K_{x}\\}, \\; k_{y} \\in \\{-K_{y}, -K_{y}+1, \\dots, K_{y}\\} \\},\n$$\nwhere $K_{x}$ and $K_{y}$ are nonnegative integers specifying the spectral truncation in each coordinate.\n\nThe spectral convolution acts as $\\widehat{y}(\\mathbf{k}) = R(\\mathbf{k}) \\,\\widehat{x}(\\mathbf{k})$, where $\\widehat{x}(\\mathbf{k})$ and $\\widehat{y}(\\mathbf{k})$ are the Discrete Fourier Transform (DFT) coefficients of the input and output, respectively. Use the fundamental property of the DFT of real-valued fields that $\\widehat{f}(-\\mathbf{k}) = \\overline{\\widehat{f}(\\mathbf{k})}$ to derive the conjugate symmetry constraint that $R(\\mathbf{k})$ must satisfy so that $\\widehat{y}(\\mathbf{k})$ corresponds to a real-valued output field for all real-valued inputs. Then, for spatial dimension $d=2$, construct an explicit parameterization of $R(\\mathbf{k})$ by defining a representative half-set of modes $\\mathcal{H} \\subset \\mathcal{I}$ and real-valued parameter tensors that ensure the required conjugate symmetry $R(-\\mathbf{k}) = \\overline{R(\\mathbf{k})}$ across $\\mathcal{I}$, including the special treatment of the zero mode. Finally, compute a closed-form expression for the total number of independent real scalar parameters needed to specify $R(\\mathbf{k})$ under this parameterization, expressed in terms of $K_{x}$, $K_{y}$, $C_{\\text{in}}$, and $C_{\\text{out}}$.\n\nYour final answer must be a single analytic expression. No rounding is required, and no units are involved. Express the final expression in its simplest exact form.",
            "solution": "The user wants to analyze the parameterization of a spectral convolution layer in a Fourier Neural Operator (FNO).\n\n### Step 1: Extract Givens\n-   The domain is a two-dimensional periodic domain.\n-   Input and output fields are real-valued.\n-   The number of input channels is $C_{\\text{in}}$.\n-   The number of output channels is $C_{\\text{out}}$.\n-   The learned complex-valued kernel is $R(\\mathbf{k}) \\in \\mathbb{C}^{C_{\\text{out}} \\times C_{\\text{in}}}$.\n-   The wavenumbers $\\mathbf{k} = (k_{x}, k_{y})$ are from the truncated set $\\mathcal{I} = \\{ (k_{x}, k_{y}) \\,:\\, k_{x} \\in \\{-K_{x}, \\dots, K_{x}\\}, \\; k_{y} \\in \\{-K_{y}, \\dots, K_{y}\\} \\}$.\n-   The spectral convolution is given by $\\widehat{y}(\\mathbf{k}) = R(\\mathbf{k}) \\,\\widehat{x}(\\mathbf{k})$.\n-   The property for Discrete Fourier Transform (DFT) of any real-valued field $f$ is $\\widehat{f}(-\\mathbf{k}) = \\overline{\\widehat{f}(\\mathbf{k})}$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is based on the established mathematical principles of Fourier analysis and its application in modern machine learning models, specifically Fourier Neural Operators. The properties of the DFT for real-valued functions are standard. The setup is scientifically sound.\n-   **Well-Posed**: The problem requests the derivation of a constraint, construction of a parameterization, and calculation of a number, all of which are well-defined mathematical tasks based on the givens. A unique and meaningful solution is expected.\n-   **Objective**: The problem is stated in precise, objective mathematical language. Terms like \"real-valued\", \"complex-valued kernel\", and the definition of the wavenumber set $\\mathcal{I}$ are unambiguous.\n-   **Self-Contained and Consistent**: All necessary information (definitions, properties) is provided. There are no internal contradictions.\n-   **Topic Relevance**: The problem is centrally located within the specified topic of *neural operators for multiscale PDEs* and the field of *multiscale modeling and analysis*.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Derivation of the Conjugate Symmetry Constraint\n\nThe problem states that both the input field $x$ and the output field $y$ are real-valued. According to the provided property of the DFT for real-valued fields, their Fourier coefficients must satisfy the following conjugate symmetry conditions:\n$$\n\\widehat{x}(-\\mathbf{k}) = \\overline{\\widehat{x}(\\mathbf{k})}\n$$\n$$\n\\widehat{y}(-\\mathbf{k}) = \\overline{\\widehat{y}(\\mathbf{k})}\n$$\nThe spectral convolution layer is defined as $\\widehat{y}(\\mathbf{k}) = R(\\mathbf{k}) \\,\\widehat{x}(\\mathbf{k})$. We can write this relationship for the wavenumber $-\\mathbf{k}$ as:\n$$\n\\widehat{y}(-\\mathbf{k}) = R(-\\mathbf{k}) \\,\\widehat{x}(-\\mathbf{k})\n$$\nNow, we substitute the symmetry properties of $\\widehat{x}$ and $\\widehat{y}$ into this equation:\n$$\n\\overline{\\widehat{y}(\\mathbf{k})} = R(-\\mathbf{k}) \\,\\overline{\\widehat{x}(\\mathbf{k})}\n$$\nFrom the original definition of the layer, we can take the complex conjugate of both sides:\n$$\n\\overline{\\widehat{y}(\\mathbf{k})} = \\overline{R(\\mathbf{k}) \\,\\widehat{x}(\\mathbf{k})} = \\overline{R(\\mathbf{k})} \\;\\overline{\\widehat{x}(\\mathbf{k})}\n$$\nBy equating the two expressions for $\\overline{\\widehat{y}(\\mathbf{k})}$, we get:\n$$\nR(-\\mathbf{k}) \\,\\overline{\\widehat{x}(\\mathbf{k})} = \\overline{R(\\mathbf{k})} \\;\\overline{\\widehat{x}(\\mathbf{k})}\n$$\nThis equation must hold for any real-valued input field $x$, which means it must be true for any set of Fourier coefficients $\\widehat{x}(\\mathbf{k})$ that satisfies the real-valuedness constraint. In general, $\\overline{\\widehat{x}(\\mathbf{k})}$ is a non-zero vector, so we can conclude that the matrices themselves must be equal:\n$$\nR(-\\mathbf{k}) = \\overline{R(\\mathbf{k})}\n$$\nThis is the required conjugate symmetry constraint on the kernel tensor $R(\\mathbf{k})$.\n\n### Explicit Parameterization of the Kernel\n\nTo ensure the constraint $R(-\\mathbf{k}) = \\overline{R(\\mathbf{k})}$ is met, we only need to define the parameters for a representative \"half\" of the wavenumbers. The values for the other half are then determined by the symmetry. The set of all wavenumbers $\\mathcal{I}$ is symmetric with respect to the origin, i.e., if $\\mathbf{k} \\in \\mathcal{I}$, then $-\\mathbf{k} \\in \\mathcal{I}$. We partition $\\mathcal{I}$ into three types of wavenumbers: the zero mode $\\mathbf{k}=(0,0)$, pairs of non-zero modes $\\{\\mathbf{k}, -\\mathbf{k}\\}$, and any modes on the boundary that might be their own negative (which only occurs for the zero mode).\n\nLet's define a representative half-set of modes, $\\mathcal{H}$, as follows:\n$$\n\\mathcal{H} = \\{ (k_{x}, k_{y}) \\in \\mathcal{I} \\,:\\, k_{y}  0 \\} \\cup \\{ (k_{x}, k_{y}) \\in \\mathcal{I} \\,:\\, k_{y}=0, k_{x} \\ge 0 \\}\n$$\nThis set contains exactly one representative for each pair $\\{\\mathbf{k}, -\\mathbf{k}\\}$ (for $\\mathbf{k} \\neq (0,0)$) and includes the zero mode $\\mathbf{k}=(0,0)$.\n\nWe can parameterize the complex kernel $R(\\mathbf{k})$ using two real-valued tensors $A(\\mathbf{k}), B(\\mathbf{k}) \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}}}$ defined for $\\mathbf{k} \\in \\mathcal{H}$. The construction is as follows:\n1.  For any $\\mathbf{k} \\in \\mathcal{H}$:\n    -   If $\\mathbf{k}=(0,0)$, the symmetry constraint is $R(0,0) = \\overline{R(0,0)}$, which implies that $R(0,0)$ must be a real matrix. We therefore set $R(0,0) = A(0,0)$ and enforce that its imaginary part is zero.\n    -   If $\\mathbf{k} \\in \\mathcal{H}$ and $\\mathbf{k} \\neq (0,0)$, we define $R(\\mathbf{k}) = A(\\mathbf{k}) + i B(\\mathbf{k})$.\n2.  For any $\\mathbf{k} \\in \\mathcal{I} \\setminus \\mathcal{H}$:\n    -   By construction of $\\mathcal{H}$, we know that $-\\mathbf{k} \\in \\mathcal{H}$ and $-\\mathbf{k} \\neq (0,0)$. We then define $R(\\mathbf{k})$ using the symmetry constraint:\n    $$\n    R(\\mathbf{k}) = \\overline{R(-\\mathbf{k})} = \\overline{A(-\\mathbf{k}) + i B(-\\mathbf{k})} = A(-\\mathbf{k}) - i B(-\\mathbf{k})\n    $$\nThis construction guarantees that $R(-\\mathbf{k}) = \\overline{R(\\mathbf{k})}$ for all $\\mathbf{k} \\in \\mathcal{I}$ by design. The independent learnable parameters are the entries of the real tensors $A(\\mathbf{k})$ for all $\\mathbf{k} \\in \\mathcal{H}$ and $B(\\mathbf{k})$ for all $\\mathbf{k} \\in \\mathcal{H} \\setminus \\{(0,0)\\}$.\n\n### Calculation of the Total Number of Parameters\n\nWe need to count the total number of independent real scalar parameters required to specify the kernel $R(\\mathbf{k})$ over the entire set $\\mathcal{I}$. This is equivalent to counting the number of scalar parameters in the real tensors $A$ and $B$ used in our construction.\n\nLet $N = C_{\\text{in}} C_{\\text{out}}$ be the number of entries in a single-channel matrix.\nThe total number of wavenumbers in $\\mathcal{I}$ is $|\\mathcal{I}| = (2K_{x}+1)(2K_{y}+1)$.\n\nWe can count the parameters by considering the distinct types of modes based on the symmetry:\n1.  **The zero mode $\\mathbf{k}=(0,0)$**: The kernel $R(0,0)$ must be a real matrix. The number of parameters is the number of entries in this real matrix, which is $C_{\\text{out}} \\times C_{\\text{in}} = N$.\n\n2.  **Non-zero modes $\\mathbf{k} \\neq (0,0)$**: These modes can be grouped into pairs $\\{\\mathbf{k}, -\\mathbf{k}\\}$. The total number of non-zero modes is $|\\mathcal{I}| - 1$. The number of such pairs is $\\frac{|\\mathcal{I}|-1}{2}$. For each pair, we only need to store the parameters for one representative, say $\\mathbf{k} \\in \\mathcal{H}$. The kernel $R(\\mathbf{k})$ is a general complex matrix, which requires $2$ real numbers (real and imaginary parts) for each of its $N$ entries. Thus, for each pair, we need $2N$ parameters. The value of $R(-\\mathbf{k})$ is then fully determined.\n    The number of parameters for all these pairs is:\n    $$\n    \\left( \\frac{|\\mathcal{I}|-1}{2} \\right) \\times (2N) = (|\\mathcal{I}|-1)N\n    $$\n\nThe total number of independent real scalar parameters is the sum of the parameters for the zero mode and all the non-zero mode pairs.\n$$\n\\text{Total Parameters} = (\\text{Params for } \\mathbf{k}=0) + (\\text{Params for } \\mathbf{k} \\neq 0)\n$$\n$$\n\\text{Total Parameters} = N + (|\\mathcal{I}|-1)N = N + |\\mathcal{I}|N - N = |\\mathcal{I}|N\n$$\nSubstituting the expressions for $|\\mathcal{I}|$ and $N$:\n$$\n\\text{Total Parameters} = ((2K_{x}+1)(2K_{y}+1)) \\times (C_{\\text{in}}C_{\\text{out}})\n$$\nThus, the total number of independent real parameters is:\n$$\nC_{\\text{in}}C_{\\text{out}}(2K_{x}+1)(2K_{y}+1)\n$$\nThis elegant result shows that the number of parameters is exactly the number of real parameters needed to store one real matrix of size $C_{\\text{out}} \\times C_{\\text{in}}$ for each mode in the frequency grid $\\mathcal{I}$.",
            "answer": "$$\n\\boxed{C_{\\text{in}}C_{\\text{out}}(2K_{x}+1)(2K_{y}+1)}\n$$"
        },
        {
            "introduction": "The standard FNO architecture leverages the Fast Fourier Transform, implicitly assuming periodic boundary conditions. However, many problems in science and engineering involve non-periodic domains with conditions like Dirichlet or Neumann. This practice  takes you beyond the idealized periodic setting by deriving the appropriate eigenbasis (in this case, sine and cosine transforms) for a mixed-boundary problem, showing how to generalize the FNO framework to a much wider class of physically relevant PDEs.",
            "id": "3787628",
            "problem": "Consider the second-order linear elliptic partial differential equation $-\\kappa \\Delta u + \\mu u = f$ on the rectangular domain $\\Omega = (0,L_{x}) \\times (0,L_{y})$, where $\\kappa  0$, $\\mu \\ge 0$, and $f$ is a square-integrable forcing. Impose homogeneous Dirichlet boundary conditions on the vertical boundaries and homogeneous Neumann boundary conditions on the horizontal boundaries, that is, $u(0,y) = u(L_{x},y) = 0$ for all $y \\in [0,L_{y}]$, and $\\partial_{y} u(x,0) = \\partial_{y} u(x,L_{y}) = 0$ for all $x \\in [0,L_{x}]$. Using the principles of Sturm–Liouville theory and separation of variables, derive an orthonormal basis of eigenfunctions on $\\Omega$ that is compatible with these boundary conditions, and identify the corresponding eigenvalues of the Laplace operator.\n\nAssume constant coefficients and exploit the orthogonal expansion of $u$ and $f$ in this basis to express the spectral relation between the expansion coefficients. Then, by viewing the inverse of $-\\kappa \\Delta + \\mu$ as a convolution-type operator diagonalized in this basis, derive the closed-form spectral multiplier $m_{k,\\ell}$ that maps the coefficient of $f$ at mode $(k,\\ell)$ to the coefficient of $u$ at the same mode. Finally, compute the numerical value of $m_{2,1}$ for $L_{x} = 2$, $L_{y} = 3$, $\\kappa = 0.5$, and $\\mu = 0.2$. Express the final numerical coefficient as a dimensionless number and round your answer to four significant figures.\n\nIn your derivation, clearly indicate how the spectral multiplier would be incorporated into a Fourier Neural Operator (FNO) layer by replacing the standard periodic Fourier basis with the sine/cosine transform basis compatible with the boundary conditions, without using any shortcut formulas and starting from fundamental definitions and properties of orthogonal eigenfunction expansions.",
            "solution": "The user wants me to solve the following problem statement.\n\n**Problem Statement:**\n\nConsider the second-order linear elliptic partial differential equation $-\\kappa \\Delta u + \\mu u = f$ on the rectangular domain $\\Omega = (0,L_{x}) \\times (0,L_{y})$, where $\\kappa  0$, $\\mu \\ge 0$, and $f$ is a square-integrable forcing. Impose homogeneous Dirichlet boundary conditions on the vertical boundaries and homogeneous Neumann boundary conditions on the horizontal boundaries, that is, $u(0,y) = u(L_{x},y) = 0$ for all $y \\in [0,L_{y}]$, and $\\partial_{y} u(x,0) = \\partial_{y} u(x,L_{y}) = 0$ for all $x \\in [0,L_{x}]$. Using the principles of Sturm–Liouville theory and separation of variables, derive an orthonormal basis of eigenfunctions on $\\Omega$ that is compatible with these boundary conditions, and identify the corresponding eigenvalues of the Laplace operator.\n\nAssume constant coefficients and exploit the orthogonal expansion of $u$ and $f$ in this basis to express the spectral relation between the expansion coefficients. Then, by viewing the inverse of $-\\kappa \\Delta + \\mu$ as a convolution-type operator diagonalized in this basis, derive the closed-form spectral multiplier $m_{k,\\ell}$ that maps the coefficient of $f$ at mode $(k,\\ell)$ to the coefficient of $u$ at the same mode. Finally, compute the numerical value of $m_{2,1}$ for $L_{x} = 2$, $L_{y} = 3$, $\\kappa = 0.5$, and $\\mu = 0.2$. Express the final numerical coefficient as a dimensionless number and round your answer to four significant figures.\n\nIn your derivation, clearly indicate how the spectral multiplier would be incorporated into a Fourier Neural Operator (FNO) layer by replacing the standard periodic Fourier basis with the sine/cosine transform basis compatible with the boundary conditions, without using any shortcut formulas and starting from fundamental definitions and properties of orthogonal eigenfunction expansions.\n\n### Step 1: Problem Validation\n\n**Extracted Givens:**\n- PDE: $-\\kappa \\Delta u + \\mu u = f$\n- Domain: $\\Omega = (0,L_{x}) \\times (0,L_{y})$\n- Parameters: $\\kappa  0$, $\\mu \\ge 0$\n- Forcing term: $f \\in L^2(\\Omega)$\n- Boundary Conditions:\n    - $u(0,y) = u(L_{x},y) = 0$ for $y \\in [0,L_{y}]$ (Homogeneous Dirichlet)\n    - $\\partial_{y} u(x,0) = \\partial_{y} u(x,L_{y}) = 0$ for $x \\in [0,L_{x}]$ (Homogeneous Neumann)\n- Numerical values for final calculation: $L_{x} = 2$, $L_{y} = 3$, $\\kappa = 0.5$, $\\mu = 0.2$. Specific mode is $(k,\\ell) = (2,1)$.\n\n**Validation:**\n1.  **Scientific or Factual Unsoundness**: The problem is scientifically sound. It describes a standard boundary value problem for a linear elliptic PDE (a Helmholtz-type equation), a cornerstone of mathematical physics and engineering.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable and directly relevant to the topic of neural operators for multiscale PDEs, as explicitly stated.\n3.  **Incomplete or Contradictory Setup**: All necessary data, including the PDE, domain, parameters, and boundary conditions, are provided\n    and are consistent.\n4.  **Unrealistic or Infeasible**: The physical and mathematical setup is standard and entirely feasible.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. For a given $f \\in L^2(\\Omega)$, the specified elliptic boundary value problem has a unique weak solution. The structure of the question is logical, guiding the solver from first principles to the final application.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is substantive, requiring a multi-step derivation involving separation of variables, Sturm-Liouville theory, eigenfunction expansions, and application to a modern machine learning context.\n7.  **Outside Scientific Verifiability**: The derivation and result are entirely verifiable through standard mathematical procedures.\n\n**Verdict:** The problem is valid.\n\n### Step 2: Solution Derivation\n\nThe solution proceeds in five parts as requested: (1) derivation of eigenfunctions and eigenvalues, (2) derivation of the spectral relation, (3) derivation of the spectral multiplier, (4) computation of a specific numerical value, and (5) explanation of the connection to Fourier Neural Operators.\n\n**Part 1: Eigenfunctions and Eigenvalues of the Laplace Operator**\n\nWe seek the eigenfunctions $\\phi(x,y)$ and eigenvalues $\\lambda$ of the negative Laplacian, $-\\Delta$, subject to the given boundary conditions. The eigenvalue problem is:\n$$ -\\Delta \\phi = \\lambda \\phi \\quad \\text{on } \\Omega $$\nwith boundary conditions:\n$$ \\phi(0,y) = \\phi(L_{x},y) = 0 $$\n$$ \\partial_{y} \\phi(x,0) = \\partial_{y} \\phi(x,L_{y}) = 0 $$\nWe use the method of separation of variables, assuming a solution of the form $\\phi(x,y) = X(x)Y(y)$. Substituting this into the eigenvalue equation gives:\n$$ - (X''(x)Y(y) + X(x)Y''(y)) = \\lambda X(x)Y(y) $$\nDividing by $X(x)Y(y)$ allows for separation of the variables:\n$$ -\\frac{X''(x)}{X(x)} - \\frac{Y''(y)}{Y(y)} = \\lambda $$\nLet $-\\frac{X''(x)}{X(x)} = \\lambda_{x}$ and $-\\frac{Y''(y)}{Y(y)} = \\lambda_{y}$, where $\\lambda = \\lambda_x + \\lambda_y$. This results in two independent one-dimensional Sturm-Liouville problems.\n\n**Problem for $X(x)$:**\nThe ODE is $X''(x) + \\lambda_x X(x) = 0$ on $(0,L_x)$. The boundary conditions for $\\phi$ imply $X(0)Y(y) = 0$ and $X(L_x)Y(y) = 0$. For a non-trivial solution ($Y(y) \\not\\equiv 0$), we must have $X(0) = 0$ and $X(L_x) = 0$.\nThe general solution for $\\lambda_x  0$ is $X(x) = A \\sin(\\sqrt{\\lambda_x} x) + B \\cos(\\sqrt{\\lambda_x} x)$.\n- $X(0)=0 \\implies B=0$.\n- $X(L_x)=0 \\implies A \\sin(\\sqrt{\\lambda_x} L_x) = 0$.\nFor a non-trivial solution ($A \\neq 0$), we must have $\\sqrt{\\lambda_x} L_x = k\\pi$ for an integer $k$. Since $k=0$ gives the trivial solution $X(x)=0$ and negative $k$ values are redundant, we have $k = 1, 2, 3, \\ldots$.\nThe eigenvalues are $\\lambda_{x,k} = (\\frac{k\\pi}{L_x})^2$ for $k \\in \\{1, 2, 3, \\dots\\}$.\nThe corresponding eigenfunctions are $X_k(x) = \\sin(\\frac{k\\pi x}{L_x})$.\n\n**Problem for $Y(y)$:**\nThe ODE is $Y''(y) + \\lambda_y Y(y) = 0$ on $(0,L_y)$. The boundary conditions for $\\phi$ imply $X(x)Y'(0)=0$ and $X(x)Y'(L_y)=0$. For a non-trivial solution ($X(x) \\not\\equiv 0$), we must have $Y'(0) = 0$ and $Y'(L_y) = 0$.\nThe general solution for $\\lambda_y  0$ is $Y(y) = C \\cos(\\sqrt{\\lambda_y} y) + D \\sin(\\sqrt{\\lambda_y} y)$.\nThe derivative is $Y'(y) = -C\\sqrt{\\lambda_y} \\sin(\\sqrt{\\lambda_y} y) + D\\sqrt{\\lambda_y} \\cos(\\sqrt{\\lambda_y} y)$.\n- $Y'(0)=0 \\implies D\\sqrt{\\lambda_y}=0 \\implies D=0$.\n- $Y'(L_y)=0 \\implies -C\\sqrt{\\lambda_y} \\sin(\\sqrt{\\lambda_y} L_y) = 0$.\nFor a non-trivial solution ($C \\neq 0$), we must have $\\sin(\\sqrt{\\lambda_y} L_y) = 0$, which means $\\sqrt{\\lambda_y} L_y = \\ell\\pi$ for an integer $\\ell$.\nWe must also consider the case $\\lambda_y=0$. The ODE becomes $Y''(y)=0$, with general solution $Y(y) = C_1 y + C_2$. The derivative is $Y'(y) = C_1$. Conditions $Y'(0)=0$ and $Y'(L_y)=0$ both imply $C_1=0$. Thus, $Y(y)=C_2$ (a constant) is a valid eigenfunction corresponding to $\\lambda_y=0$. This case is naturally included by allowing $\\ell=0$ in the general formula.\nThe eigenvalues are $\\lambda_{y,\\ell} = (\\frac{\\ell\\pi}{L_y})^2$ for $\\ell \\in \\{0, 1, 2, \\dots\\}$.\nThe corresponding eigenfunctions are $Y_\\ell(y) = \\cos(\\frac{\\ell\\pi y}{L_y})$.\n\n**2D Eigenfunctions and Eigenvalues:**\nThe eigenfunctions for the operator $-\\Delta$ on $\\Omega$ are the products $\\phi_{k,\\ell}(x,y) = X_k(x)Y_\\ell(y)$:\n$$ \\phi_{k,\\ell}(x,y) = \\sin\\left(\\frac{k\\pi x}{L_x}\\right) \\cos\\left(\\frac{\\ell\\pi y}{L_y}\\right) $$\nfor integer indices $k \\ge 1$ and $\\ell \\ge 0$.\nThe corresponding eigenvalues $\\lambda_{k,\\ell}$ are the sums of the one-dimensional eigenvalues:\n$$ \\lambda_{k,\\ell} = \\lambda_{x,k} + \\lambda_{y,\\ell} = \\left(\\frac{k\\pi}{L_x}\\right)^2 + \\left(\\frac{\\ell\\pi}{L_y}\\right)^2 $$\n\n**Orthonormal Basis:**\nTo create an orthonormal basis, we normalize these eigenfunctions with respect to the $L^2$ inner product $\\langle g, h \\rangle = \\int_0^{L_y} \\int_0^{L_x} g(x,y)h(x,y) \\,dx\\,dy$.\nThe squared norm of $\\phi_{k,\\ell}$ is:\n$$ \\|\\phi_{k,\\ell}\\|^2 = \\left(\\int_0^{L_x} \\sin^2\\left(\\frac{k\\pi x}{L_x}\\right) dx\\right) \\left(\\int_0^{L_y} \\cos^2\\left(\\frac{\\ell\\pi y}{L_y}\\right) dy\\right) $$\nThe standard integrals are $\\int_0^L \\sin^2(\\frac{k\\pi x}{L}) dx = \\frac{L}{2}$ for $k \\ge 1$ and $\\int_0^L \\cos^2(\\frac{\\ell\\pi y}{L}) dy = \\frac{L}{2}$ for $\\ell \\ge 1$. For $\\ell=0$, $\\cos(0)=1$, so $\\int_0^{L_y} \\cos^2(0) dy = L_y$.\n- For $\\ell \\ge 1$: $\\|\\phi_{k,\\ell}\\|^2 = (\\frac{L_x}{2})(\\frac{L_y}{2}) = \\frac{L_x L_y}{4}$. The normalization constant is $\\frac{2}{\\sqrt{L_x L_y}}$.\n- For $\\ell = 0$: $\\|\\phi_{k,0}\\|^2 = (\\frac{L_x}{2})(L_y) = \\frac{L_x L_y}{2}$. The normalization constant is $\\sqrt{\\frac{2}{L_x L_y}}$.\n\nThe orthonormal basis of eigenfunctions, denoted $\\{\\psi_{k,\\ell}\\}$, is:\n$$ \\psi_{k,\\ell}(x,y) = \\begin{cases} \\sqrt{\\frac{2}{L_x L_y}} \\sin\\left(\\frac{k\\pi x}{L_x}\\right)  \\text{if } \\ell=0 \\\\ \\frac{2}{\\sqrt{L_x L_y}} \\sin\\left(\\frac{k\\pi x}{L_x}\\right) \\cos\\left(\\frac{\\ell\\pi y}{L_y}\\right)  \\text{if } \\ell \\ge 1 \\end{cases} $$\nfor $k \\ge 1, \\ell \\ge 0$.\n\n**Part 2: Spectral Relation**\nWe expand the solution $u(x,y)$ and the forcing term $f(x,y)$ in the orthonormal basis $\\{\\psi_{k,\\ell}\\}$:\n$$ u(x,y) = \\sum_{k=1}^{\\infty}\\sum_{\\ell=0}^{\\infty} \\hat{u}_{k,\\ell} \\psi_{k,\\ell}(x,y) $$\n$$ f(x,y) = \\sum_{k=1}^{\\infty}\\sum_{\\ell=0}^{\\infty} \\hat{f}_{k,\\ell} \\psi_{k,\\ell}(x,y) $$\nwhere the coefficients are found by projection: $\\hat{u}_{k,\\ell} = \\langle u, \\psi_{k,\\ell} \\rangle$ and $\\hat{f}_{k,\\ell} = \\langle f, \\psi_{k,\\ell} \\rangle$.\nSubstitute these expansions into the PDE $-\\kappa \\Delta u + \\mu u = f$:\n$$ -\\kappa \\Delta \\left(\\sum_{k,\\ell} \\hat{u}_{k,\\ell} \\psi_{k,\\ell}\\right) + \\mu \\left(\\sum_{k,\\ell} \\hat{u}_{k,\\ell} \\psi_{k,\\ell}\\right) = \\sum_{k,\\ell} \\hat{f}_{k,\\ell} \\psi_{k,\\ell} $$\nUsing the linearity of the operators and the fact that $-\\Delta \\psi_{k,\\ell} = \\lambda_{k,\\ell} \\psi_{k,\\ell}$:\n$$ \\sum_{k,\\ell} \\left( \\kappa \\lambda_{k,\\ell} \\hat{u}_{k,\\ell} + \\mu \\hat{u}_{k,\\ell} \\right) \\psi_{k,\\ell} = \\sum_{k,\\ell} \\hat{f}_{k,\\ell} \\psi_{k,\\ell} $$\nBy the orthogonality of the basis functions, we can equate the coefficients for each mode $(k,\\ell)$:\n$$ (\\kappa \\lambda_{k,\\ell} + \\mu) \\hat{u}_{k,\\ell} = \\hat{f}_{k,\\ell} $$\nThis is the spectral relation between the expansion coefficients of the solution and the forcing term.\n\n**Part 3: Spectral Multiplier**\nThe spectral multiplier $m_{k,\\ell}$ is defined as the factor that maps the coefficient of the input $f$ to the coefficient of the output $u$, i.e., $\\hat{u}_{k,\\ell} = m_{k,\\ell} \\hat{f}_{k,\\ell}$. From the spectral relation above, we can directly identify $m_{k,\\ell}$:\n$$ m_{k,\\ell} = \\frac{1}{\\kappa \\lambda_{k,\\ell} + \\mu} $$\nSubstituting the expression for the eigenvalues $\\lambda_{k,\\ell}$:\n$$ m_{k,\\ell} = \\frac{1}{\\kappa \\left[ \\left(\\frac{k\\pi}{L_x}\\right)^2 + \\left(\\frac{\\ell\\pi}{L_y}\\right)^2 \\right] + \\mu} $$\n\n**Part 4: Numerical Computation**\nWe are asked to compute $m_{2,1}$ given the parameters $L_x = 2$, $L_y = 3$, $\\kappa = 0.5$, and $\\mu = 0.2$. The mode is $(k,\\ell) = (2,1)$.\n$$ m_{2,1} = \\frac{1}{(0.5) \\left[ \\left(\\frac{2\\pi}{2}\\right)^2 + \\left(\\frac{1\\pi}{3}\\right)^2 \\right] + 0.2} $$\n$$ m_{2,1} = \\frac{1}{0.5 \\left[ \\pi^2 + \\frac{\\pi^2}{9} \\right] + 0.2} $$\n$$ m_{2,1} = \\frac{1}{0.5 \\left[ \\frac{9\\pi^2 + \\pi^2}{9} \\right] + 0.2} $$\n$$ m_{2,1} = \\frac{1}{0.5 \\left( \\frac{10\\pi^2}{9} \\right) + 0.2} $$\n$$ m_{2,1} = \\frac{1}{\\frac{5\\pi^2}{9} + 0.2} $$\nNow, we compute the numerical value:\n$$ m_{2,1} = \\frac{1}{\\frac{5 \\times (9.8696044...)}{9} + 0.2} = \\frac{1}{5.4831135... + 0.2} = \\frac{1}{5.6831135...} \\approx 0.1759600... $$\nRounding to four significant figures, we get $0.1760$.\n\n**Part 5: Connection to Fourier Neural Operators (FNO)**\nThe derivation above shows that the solution operator $\\mathcal{G}: f \\mapsto u$ for the PDE $-\\kappa\\Delta u + \\mu u = f$ acts as a simple multiplication in the spectral domain defined by the eigenfunctions $\\{\\psi_{k,\\ell}\\}$. This means $\\mathcal{G}$ is a convolution-type operator that is diagonalized by the basis of sine-cosine functions derived. The values on this diagonal are precisely the spectral multipliers $m_{k,\\ell}$.\n\nA Fourier Neural Operator (FNO) is a neural network architecture designed to learn such solution operators. A standard FNO works with periodic boundary conditions, for which the eigenfunctions of the Laplacian are the complex exponentials $\\{e^{i \\mathbf{k} \\cdot \\mathbf{x}}\\}$. The transformation to and from this spectral basis is the Fourier Transform, efficiently implemented by the Fast Fourier Transform (FFT).\n\nTo adapt the FNO architecture for the non-periodic boundary conditions in this problem, one must replace the standard Fourier basis with the problem-specific eigenbasis. The procedure for a generalized FNO layer that learns the operator for this problem would be:\n\n1.  **Input**: The layer receives an input function $v(x,y)$ discretized on a grid.\n2.  **Forward Transform**: Instead of applying the FFT, apply a 2D transform that projects the input $v$ onto the derived eigenbasis. This is practically achieved by performing a Discrete Sine Transform (DST) along the $x$-dimension and a Discrete Cosine Transform (DCT) along the $y$-dimension. Let's denote this combined transform as $\\mathcal{T}$. This step computes the spectral coefficients $\\hat{v}_{k,\\ell} = (\\mathcal{T}(v))_{k,\\ell}$.\n3.  **Spectral Multiplication (Learned Kernel)**: In the spectral domain, apply a point-wise multiplication with a set of learnable parameters (weights) $R_{k,\\ell}$. These parameters are the neural network's approximation of the true spectral multiplier. The FNO typically truncates the series to a maximum number of modes $(k_{\\max}, \\ell_{\\max})$.\n    $$ (\\widehat{K(v)})_{k,\\ell} = R_{k,\\ell} \\cdot \\hat{v}_{k,\\ell} \\quad \\text{for } 1 \\le k \\le k_{\\max}, \\, 0 \\le \\ell \\le \\ell_{\\max} $$\n    The core idea of FNO is that $R_{k,\\ell}$ is learnable through backpropagation, allowing the network to approximate the response of any similar operator, not just the one with the specific $\\kappa$ and $\\mu$ derived here.\n4.  **Inverse Transform**: Apply the inverse transform $\\mathcal{T}^{-1}$ (consisting of an Inverse DST and an Inverse DCT) to the modified coefficients to return the result to the physical domain.\n    $$ K(v)(x,y) = \\mathcal{T}^{-1}\\left(R \\cdot \\hat{v}\\right)(x,y) $$\n5.  **Local Path and Activation**: As in a standard residual network, add a local linear transformation of the input (e.g., a point-wise linear layer implemented as a $1 \\times 1$ convolution, $Wv$) and apply a non-linear activation function $\\sigma$.\n    $$ v_{\\text{out}}(x,y) = \\sigma(K(v)(x,y) + Wv(x,y)) $$\n\nBy choosing the transform $\\mathcal{T}$ that diagonalizes the underlying differential operator, the FNO is provided with the most efficient representation for learning the solution operator. The learnable filter $R_{k,\\ell}$ only needs to learn a diagonal operator, which is far more parameter-efficient and easier to train than learning a full, dense operator in a non-optimal basis (such as the standard Fourier basis for this non-periodic problem).",
            "answer": "$$ \\boxed{0.1760} $$"
        },
        {
            "introduction": "A key advertised advantage of Neural Operators is their \"discretization-invariance\"—the ability to be trained on one grid resolution and evaluated on another. This property, however, is not absolute and can be compromised by practical issues like aliasing, where sampling a function on a coarse grid causes high frequencies to be misinterpreted as low frequencies. This exercise  provides a concrete, quantitative look at this limitation, allowing you to calculate the error induced by aliasing and gain a deeper appreciation for the practical challenges in multi-resolution applications.",
            "id": "3787649",
            "problem": "Consider the periodic domain $[0,1]$ with the function space $L^{2}([0,1])$ and its Fourier basis $\\{ \\exp(2\\pi i k x) \\}_{k \\in \\mathbb{Z}}$. A Fourier neural operator (FNO) defines a resolution-invariant parametric map between functions by acting on a truncated set of Fourier modes. In this setup, an input lifting network computes discrete Fourier coefficients from grid samples and truncates them to a cutoff $|k| \\leq \\Lambda$, the operator applies a fixed linear map on the truncated coefficient vector (in this problem, it acts as the identity), and the output projection network reconstructs the function at the target grid by inverse Fourier synthesis on that grid. Training on a coarse grid learns the operator weights associated with the truncated modes, and evaluation on a fine grid reuses the same weights without retraining. Assume the domain is represented on a coarse training grid of $n$ equispaced points and a fine inference grid of $m$ equispaced points, with the cut-off integer $\\Lambda$ satisfying $2\\Lambda+1 \\leq n$ and $2\\Lambda+1 \\leq m$.\n\nYou are given the following concrete instance to analyze discretization invariance and the interpolation error induced by the lifting and projection networks:\n- The true operator to be learned is the identity map on $L^{2}([0,1])$.\n- The input lifting network computes the discrete Fourier transform (DFT) of the coarse-grid samples and retains only coefficients with $|k|\\leq \\Lambda$; coefficients with $|k|\\Lambda$ are dropped. The DFT respects the usual discrete aliasing: sampling a mode indexed by $k \\in \\mathbb{Z}$ on $n$ points produces an alias at the residue class of $k$ modulo $n$.\n- The output projection network performs inverse Fourier synthesis on the fine grid using the retained modes $|k|\\leq \\Lambda$.\n\nLet the coarse grid have $n=8$ points, the fine grid have $m=64$ points, and the cut-off be $\\Lambda=3$. Consider the input function\n$$\nu(x) = \\cos(2\\pi \\cdot 2 \\, x) + \\beta \\, \\cos(2\\pi \\cdot 11 \\, x),\n$$\nwhere $\\beta \\in \\mathbb{R}$ is a given amplitude parameter. The lifting operation at the coarse grid aliases the mode $k=11$ to $k=11 \\bmod 8 = 3$, which lies within the retained band $|k|\\leq \\Lambda$.\n\nUnder these assumptions, the learned operator acts as the identity on the truncated coefficients. The output projection reconstructs at the fine grid using only the retained modes $k=2$ and $k=3$, producing the prediction\n$$\n\\hat{u}(x) = \\cos(2\\pi \\cdot 2 \\, x) + \\beta \\, \\cos(2\\pi \\cdot 3 \\, x).\n$$\n\nDefine the fine-grid mean squared error induced by input lifting and output projection as\n$$\nE_{m}(\\beta) = \\frac{1}{m} \\sum_{j=0}^{m-1} \\left( u\\!\\left(\\frac{j}{m}\\right) - \\hat{u}\\!\\left(\\frac{j}{m}\\right) \\right)^{2}.\n$$\n\nStarting from the definitions of the Fourier basis on the torus, discrete sampling, discrete Fourier transform aliasing, and orthogonality of discrete Fourier modes, derive a closed-form analytic expression for $E_{m}(\\beta)$ in terms of $\\beta$ for the specified $n$, $m$, and $\\Lambda$. Your final answer must be a single closed-form expression. No rounding is required and no units are involved.",
            "solution": "The user wants me to derive a closed-form analytic expression for the fine-grid mean squared error $E_{m}(\\beta)$.\n\nThe problem is validated and deemed to be scientifically grounded, well-posed, and objective. All necessary data and definitions are provided, and there are no internal contradictions. I will now proceed with the solution.\n\nThe fine-grid mean squared error is defined as:\n$$\nE_{m}(\\beta) = \\frac{1}{m} \\sum_{j=0}^{m-1} \\left( u\\!\\left(\\frac{j}{m}\\right) - \\hat{u}\\!\\left(\\frac{j}{m}\\right) \\right)^{2}\n$$\nThe input function $u(x)$ and the predicted output function $\\hat{u}(x)$ are given as:\n$$\nu(x) = \\cos(2\\pi \\cdot 2 \\, x) + \\beta \\, \\cos(2\\pi \\cdot 11 \\, x)\n$$\n$$\n\\hat{u}(x) = \\cos(2\\pi \\cdot 2 \\, x) + \\beta \\, \\cos(2\\pi \\cdot 3 \\, x)\n$$\nThe difference between the true function and the prediction is:\n$$\nu(x) - \\hat{u}(x) = \\left( \\cos(2\\pi \\cdot 2 \\, x) + \\beta \\, \\cos(2\\pi \\cdot 11 \\, x) \\right) - \\left( \\cos(2\\pi \\cdot 2 \\, x) + \\beta \\, \\cos(2pi \\cdot 3 \\, x) \\right)\n$$\n$$\nu(x) - \\hat{u}(x) = \\beta \\left( \\cos(2\\pi \\cdot 11 \\, x) - \\cos(2\\pi \\cdot 3 \\, x) \\right)\n$$\nWe need to evaluate this difference on the fine grid, where the points are $x_j = \\frac{j}{m}$ for $j=0, 1, \\dots, m-1$.\nThe error at each grid point is:\n$$\nu(x_j) - \\hat{u}(x_j) = \\beta \\left( \\cos\\left(2\\pi \\cdot 11 \\cdot \\frac{j}{m}\\right) - \\cos\\left(2\\pi \\cdot 3 \\cdot \\frac{j}{m}\\right) \\right)\n$$\nSubstituting this into the expression for $E_{m}(\\beta)$:\n$$\nE_{m}(\\beta) = \\frac{1}{m} \\sum_{j=0}^{m-1} \\left[ \\beta \\left( \\cos\\left(\\frac{2\\pi \\cdot 11 j}{m}\\right) - \\cos\\left(\\frac{2\\pi \\cdot 3 j}{m}\\right) \\right) \\right]^2\n$$\n$$\nE_{m}(\\beta) = \\frac{\\beta^2}{m} \\sum_{j=0}^{m-1} \\left( \\cos\\left(\\frac{2\\pi \\cdot 11 j}{m}\\right) - \\cos\\left(\\frac{2\\pi \\cdot 3 j}{m}\\right) \\right)^2\n$$\nWe expand the squared term:\n$$\n\\left( \\cos(A) - \\cos(B) \\right)^2 = \\cos^2(A) - 2\\cos(A)\\cos(B) + \\cos^2(B)\n$$\nThus, the summation becomes:\n$$\n\\sum_{j=0}^{m-1} \\left[ \\cos^2\\left(\\frac{2\\pi \\cdot 11 j}{m}\\right) - 2\\cos\\left(\\frac{2\\pi \\cdot 11 j}{m}\\right)\\cos\\left(\\frac{2\\pi \\cdot 3 j}{m}\\right) + \\cos^2\\left(\\frac{2\\pi \\cdot 3 j}{m}\\right) \\right]\n$$\nWe can evaluate the sum of each term separately. The derivation relies on the discrete orthogonality of Fourier modes. For any integer $K$, the sum of complex exponentials over $m$ equispaced points is:\n$$\n\\sum_{j=0}^{m-1} \\exp\\left(i \\frac{2\\pi K j}{m}\\right) = \\begin{cases} m  \\text{if } K \\text{ is an integer multiple of } m \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nFrom this, it follows that for an integer $K$ which is not a multiple of $m$:\n$$\n\\sum_{j=0}^{m-1} \\cos\\left(\\frac{2\\pi K j}{m}\\right) = \\text{Re}\\left(\\sum_{j=0}^{m-1} \\exp\\left(i \\frac{2\\pi K j}{m}\\right)\\right) = 0\n$$\n\nLet's evaluate the sum of the first term, $S_1 = \\sum_{j=0}^{m-1} \\cos^2\\left(\\frac{2\\pi \\cdot 11 j}{m}\\right)$.\nUsing the identity $\\cos^2(\\theta) = \\frac{1}{2}(1 + \\cos(2\\theta))$:\n$$\nS_1 = \\sum_{j=0}^{m-1} \\frac{1}{2} \\left( 1 + \\cos\\left(\\frac{2\\pi \\cdot 22 j}{m}\\right) \\right) = \\frac{1}{2}\\sum_{j=0}^{m-1} 1 + \\frac{1}{2}\\sum_{j=0}^{m-1} \\cos\\left(\\frac{2\\pi \\cdot 22 j}{m}\\right)\n$$\nThe first part is $\\frac{m}{2}$. For the second part, we have $K=22$ and $m=64$. Since $22$ is not a multiple of $64$, the sum is $0$.\nSo, $S_1 = \\frac{m}{2} + 0 = \\frac{m}{2}$.\n\nLet's evaluate the sum of the third term, $S_3 = \\sum_{j=0}^{m-1} \\cos^2\\left(\\frac{2\\pi \\cdot 3 j}{m}\\right)$.\nSimilarly,\n$$\nS_3 = \\sum_{j=0}^{m-1} \\frac{1}{2} \\left( 1 + \\cos\\left(\\frac{2\\pi \\cdot 6 j}{m}\\right) \\right) = \\frac{1}{2}\\sum_{j=0}^{m-1} 1 + \\frac{1}{2}\\sum_{j=0}^{m-1} \\cos\\left(\\frac{2\\pi \\cdot 6 j}{m}\\right)\n$$\nThe first part is $\\frac{m}{2}$. For the second part, $K=6$ and $m=64$. Since $6$ is not a multiple of $64$, this sum is also $0$.\nSo, $S_3 = \\frac{m}{2} + 0 = \\frac{m}{2}$.\n\nNow, let's evaluate the sum of the cross-term, $S_2 = \\sum_{j=0}^{m-1} \\cos\\left(\\frac{2\\pi \\cdot 11 j}{m}\\right)\\cos\\left(\\frac{2\\pi \\cdot 3 j}{m}\\right)$.\nUsing the product-to-sum identity $\\cos(A)\\cos(B) = \\frac{1}{2}(\\cos(A-B) + \\cos(A+B))$:\n$$\nS_2 = \\sum_{j=0}^{m-1} \\frac{1}{2} \\left[ \\cos\\left(\\frac{2\\pi (11-3) j}{m}\\right) + \\cos\\left(\\frac{2\\pi (11+3) j}{m}\\right) \\right]\n$$\n$$\nS_2 = \\frac{1}{2} \\sum_{j=0}^{m-1} \\cos\\left(\\frac{2\\pi \\cdot 8 j}{m}\\right) + \\frac{1}{2} \\sum_{j=0}^{m-1} \\cos\\left(\\frac{2\\pi \\cdot 14 j}{m}\\right)\n$$\nFor the first sum, $K=8$ and $m=64$. $8$ is not a multiple of $64$, so the sum is $0$.\nFor the second sum, $K=14$ and $m=64$. $14$ is not a multiple of $64$, so the sum is $0$.\nTherefore, $S_2 = \\frac{1}{2}(0) + \\frac{1}{2}(0) = 0$. This reflects the discrete orthogonality of the cosine modes $\\cos(2\\pi \\cdot 11x)$ and $\\cos(2\\pi \\cdot 3x)$ on the grid with $m=64$ points.\n\nFinally, we assemble the terms to compute $E_{m}(\\beta)$:\n$$\nE_{m}(\\beta) = \\frac{\\beta^2}{m} \\left[ S_1 - 2S_2 + S_3 \\right]\n$$\n$$\nE_{m}(\\beta) = \\frac{\\beta^2}{m} \\left[ \\frac{m}{2} - 2(0) + \\frac{m}{2} \\right]\n$$\n$$\nE_{m}(\\beta) = \\frac{\\beta^2}{m} \\left( \\frac{m}{2} + \\frac{m}{2} \\right) = \\frac{\\beta^2}{m} (m)\n$$\n$$\nE_{m}(\\beta) = \\beta^2\n$$\nThe fine-grid mean squared error is $\\beta^2$. This result arises because the error function $u(x)-\\hat{u}(x)$ is a sum of two sinusoids, which are orthogonal to each other both in the continuous $L^2([0,1])$ sense and in the discrete sense on the fine grid of $m=64$ points. The total discrete energy is the sum of the energies of the individual components.",
            "answer": "$$\\boxed{\\beta^2}$$"
        }
    ]
}