{
    "hands_on_practices": [
        {
            "introduction": "主动学习的一项基本策略是在模型不确定性高的区域进行查询。本练习专注于使用分位数回归——一种强大的非参数技术——来量化偶然不确定性（数据中的内在噪声）。通过学习估计预测区间，您可以设计一个采集函数，将采样引向最不可预测的区域，这在数据表现出复杂的、依赖于输入的噪声时尤其有用 。",
            "id": "3729906",
            "problem": "一个多尺度建模工作流使用多层感知机（MLP）代理模型来近似一个从微观描述符 $x \\in \\mathbb{R}^d$ 到由计算成本高昂的高保真求解器产生的宏观响应 $Y \\in \\mathbb{R}$ 的映射。因为 $Y$ 可能表现出以 $x$ 为条件的异方差变异性，团队希望通过预测区间来量化预测不确定性，并驱动一个主动学习循环，将新的求解器调用分配到输入空间中最不确定的区域。令 $F_{Y \\mid X=x}(y)$ 表示给定 $X=x$ 时 $Y$ 的条件累积分布函数，$\\tau$-分位数由逆关系 $q_\\tau(x) = \\inf\\{y : F_{Y \\mid X=x}(y) \\ge \\tau\\}$ 定义，其中 $\\tau \\in (0,1)$。在分位数回归中，通过最小化由分位数“弹球”损失形成的经验风险来训练一个参数化函数 $\\hat{q}_\\tau(x)$，以近似 $q_\\tau(x)$。\n\n考虑一个主动学习的设置，其中有一个未标记候选池 $\\mathcal{U} \\subset \\mathbb{R}^d$ 和每次迭代进行一次新的高保真评估的预算。目标是选择 $x^\\star \\in \\mathcal{U}$，使其最大化预测不确定性的度量，从而将采样集中在代理模型最不确定的区域。哪个选项正确地描述了分位数回归MLP如何提供具有名义覆盖率的预测区间，并提出一种以最大化区间宽度为目标的采集策略，以便在不确定区域集中采样？\n\nA. 训练一个MLP，其输出为 $\\hat{q}_\\alpha(x)$ 和 $\\hat{q}_{1-\\alpha}(x)$（对于某个 $\\alpha \\in (0, \\frac{1}{2})$），并对每个分位数使用弹球损失。使用预测区间 $[\\hat{q}_\\alpha(x), \\hat{q}_{1-\\alpha}(x)]$，在校准情况下，其名义覆盖率约为 $1 - 2\\alpha$。选择 $x^\\star = \\arg\\max_{x \\in \\mathcal{U}} \\left(\\hat{q}_{1-\\alpha}(x) - \\hat{q}_\\alpha(x)\\right)$ 来以最大区间宽度为目标。\n\nB. 训练一个具有两个输出 $\\hat{\\mu}(x)$ 和 $\\hat{\\sigma}(x)$ 的MLP，使用平方损失来拟合两者，将 $\\hat{\\mu}(x)$ 解释为均值，$\\hat{\\sigma}(x)$ 解释为标准差，并构建预测区间为 $[\\hat{\\mu}(x) - 2\\hat{\\sigma}(x), \\hat{\\mu}(x) + 2\\hat{\\sigma}(x)]$。选择 $x^\\star = \\arg\\max_{x \\in \\mathcal{U}} \\hat{\\sigma}(x)$。\n\nC. 训练一个MLP来估计分位数 $\\hat{q}_\\alpha(x)$ 和 $\\hat{q}_{1-\\alpha}(x)$，但将采集策略定义为 $x^\\star = \\arg\\max_{x \\in \\mathcal{U}} \\left(\\hat{q}_{1-\\alpha}(x) - \\hat{q}_\\alpha(x)\\right)^{-1}$，以避免在散布较大的区域过度采样。\n\nD. 在测试时使用蒙特卡洛（MC）丢弃法（dropout）来产生一个输出的集成，并通过计算MC样本的经验分位数来定义预测区间。选择 $x^\\star = \\arg\\max_{x \\in \\mathcal{U}}$ 该区间的宽度；如果使用MC丢弃法，则分位数回归是不必要的。\n\n选择唯一最佳选项。",
            "solution": "在这里，我将首先验证问题陈述，然后提供详细的解决方案。\n\n### 第一步：提取已知条件\n- 多层感知机（MLP）被用作代理模型。\n- 该模型近似一个从微观描述符 $x \\in \\mathbb{R}^d$ 到宏观响应 $Y \\in \\mathbb{R}$ 的映射。\n- 响应 $Y$ 可能表现出以 $x$ 为条件的异方差变异性。\n- 目标是使用区间量化预测不确定性，并用它来驱动一个主动学习循环。\n- 给定 $X=x$ 时 $Y$ 的条件累积分布函数（CDF）表示为 $F_{Y \\mid X=x}(y)$。\n- $\\tau$-分位数定义为 $q_\\tau(x) = \\inf\\{y : F_{Y \\mid X=x}(y) \\ge \\tau\\}$，其中 $\\tau \\in (0,1)$。\n- 分位数回归通过最小化分位数“弹球”损失的经验风险来训练一个参数化函数 $\\hat{q}_\\tau(x)$。\n- 主动学习设置有一个未标记候选池 $\\mathcal{U} \\subset \\mathbb{R}^d$，并且每次迭代有一次新评估的预算。\n- 主动学习的目标是选择 $x^\\star \\in \\mathcal{U}$，使其最大化预测不确定性的度量。\n- 问题要求找出哪个选项正确描述了分位数回归MLP如何提供预测区间，并提出一种以最大化区间宽度为目标的采集策略。\n\n### 第二步：使用提取的已知条件进行验证\n- **科学依据**：该问题在机器学习、不确定性量化和科学计算领域有充分的理论基础。使用弹球损失的分位数回归是估计条件分位数的标准方法。使用这些分位数形成预测区间，并通过在不确定性高的区域（即区间较宽的区域）进行采样来驱动主动学习，是一种常见且有效的方法，尤其适用于处理异方差的偶然不确定性。所有概念都是标准的且在事实上是正确的。\n- **问题定义明确**：问题定义清晰。它指明了模型（MLP）、统计挑战（异方差性）、建议的方法（分位数回归）以及目标（基于最大不确定性的主动学习）。问题的结构旨在从一组备选项中识别出该策略的正确实施方式。存在有意义的解决方案。\n- **客观性**：语言精确、技术性强，没有含糊不清或主观的陈述。\n\n### 第三步：结论与行动\n问题陈述是有效的。它在科学上是合理的，问题定义明确，客观，并且包含了推导出解决方案所需的所有信息。我现在将推导解决方案并评估每个选项。\n\n### 基于原理的推导\n\n该问题需要一个分两部分的解决方案：1）使用分位数回归构建预测区间；2）基于该区间定义一个主动学习采集函数。\n\n1. **预测区间的构建**：问题将 $\\tau$-分位数 $q_\\tau(x)$ 定义为值 $y$，使得观察到小于或等于 $y$ 的响应的概率为 $\\tau$，即 $P(Y \\le q_\\tau(x) \\mid X=x) = \\tau$。一个名义覆盖概率为 $1-2\\alpha$ 的预测区间是通过估计条件分布的下分位数和上分位数来构建的。具体来说，我们需要 $\\alpha$-分位数 $q_\\alpha(x)$ 和 $(1-\\alpha)$-分位数 $q_{1-\\alpha}(x)$，其中 $\\alpha$ 是一个小的正数，$\\alpha \\in (0, \\frac{1}{2})$。然后区间由 $[q_\\alpha(x), q_{1-\\alpha}(x)]$ 给出。未来观测值 $Y$ 落入此区间的概率为：\n$$ P(q_\\alpha(x) \\le Y \\le q_{1-\\alpha}(x) \\mid X=x) = P(Y \\le q_{1-\\alpha}(x) \\mid X=x) - P(Y \\le q_\\alpha(x) \\mid X=x) $$\n假设条件CDF是连续的，这等于：\n$$ F_{Y \\mid X=x}(q_{1-\\alpha}(x)) - F_{Y \\mid X=x}(q_\\alpha(x)) = (1-\\alpha) - \\alpha = 1-2\\alpha $$\n问题指出，通过最小化弹球损失来训练一个MLP $\\hat{q}_\\tau(x)$ 来近似 $q_\\tau(x)$。要构建该区间，我们需要为两个特定的分位数 $\\alpha$ 和 $1-\\alpha$ 训练模型。这可以通过训练一个具有两个输出的MLP来完成，一个用于 $\\hat{q}_\\alpha(x)$，另一个用于 $\\hat{q}_{1-\\alpha}(x)$，其中总损失是每个分位数的弹球损失之和。得到的预测区间是 $[\\hat{q}_\\alpha(x), \\hat{q}_{1-\\alpha}(x)]$。\n\n2. **主动学习采集函数**：既定目标是“选择 $x^\\star \\in \\mathcal{U}$，使其最大化预测不确定性的度量”，以“将采样集中在不确定区域”。预测不确定性很自然地由条件分布的离散程度来量化。预测区间的宽度 $W(x) = \\hat{q}_{1-\\alpha}(x) - \\hat{q}_\\alpha(x)$ 是这种离散程度的直接度量。对于给定的输入 $x$，更宽的区间意味着关于响应 $Y$ 值的不确定性更大。因此，一个合适的选择下一个采样点的采集函数，应该是在候选池 $\\mathcal{U}$ 上最大化这个宽度：\n$$ x^\\star = \\arg\\max_{x \\in \\mathcal{U}} W(x) = \\arg\\max_{x \\in \\mathcal{U}} \\left(\\hat{q}_{1-\\alpha}(x) - \\hat{q}_\\alpha(x)\\right) $$\n这个策略是一种不确定性采样的形式。\n\n### 逐项分析\n\n**A. 训练一个MLP，其输出为 $\\hat{q}_\\alpha(x)$ 和 $\\hat{q}_{1-\\alpha}(x)$（对于某个 $\\alpha \\in (0, \\frac{1}{2})$），并对每个分位数使用弹球损失。使用预测区间 $[\\hat{q}_\\alpha(x), \\hat{q}_{1-\\alpha}(x)]$，在校准情况下，其名义覆盖率约为 $1 - 2\\alpha$。选择 $x^\\star = \\arg\\max_{x \\in \\mathcal{U}} \\left(\\hat{q}_{1-\\alpha}(x) - \\hat{q}_\\alpha(x)\\right)$ 来以最大区间宽度为目标。**\n该选项正确地描述了从第一性原理推导出的整个过程。它正确地指明了用弹球损失训练两个分位数（$\\alpha$ 和 $1-\\alpha$），形成名义覆盖率为 $1-2\\alpha$ 的区间 $[\\hat{q}_\\alpha(x), \\hat{q}_{1-\\alpha}(x)]$，并使用区间宽度的最大化作为主动学习的采集函数。\n**结论：正确。**\n\n**B. 训练一个具有两个输出 $\\hat{\\mu}(x)$ 和 $\\hat{\\sigma}(x)$ 的MLP，使用平方损失来拟合两者，将 $\\hat{\\mu}(x)$ 解释为均值，$\\hat{\\sigma}(x)$ 解释为标准差，并构建预测区间为 $[\\hat{\\mu}(x) - 2\\hat{\\sigma}(x), \\hat{\\mu}(x) + 2\\hat{\\sigma}(x)]$。选择 $x^\\star = \\arg\\max_{x \\in \\mathcal{U}} \\hat{\\sigma}(x)$。**\n这个选项描述了一种参数化方法，隐含地假设了一个分布（例如高斯分布）。问题的设置强调分位数回归，这对于条件分布的形状是非参数的。更关键的是，使用平方损失来拟合标准差 $\\hat{\\sigma}(x)$ 是不正确的；它只能拟合均值 $\\hat{\\mu}(x)$。这种参数化方法的正确做法是最大化对数似然，这会导向一个不同的损失函数，例如 $\\sum_i [ \\frac{(y_i - \\hat{\\mu}(x_i))^2}{2\\hat{\\sigma}^2(x_i)} + \\log \\hat{\\sigma}(x_i) ]$。由于该选项错误地识别了所需的损失函数，并且偏离了问题中指定的分位数回归框架，因此它是有缺陷的。\n**结论：错误。**\n\n**C. 训练一个MLP来估计分位数 $\\hat{q}_\\alpha(x)$ 和 $\\hat{q}_{1-\\alpha}(x)$，但将采集策略定义为 $x^\\star = \\arg\\max_{x \\in \\mathcal{U}} \\left(\\hat{q}_{1-\\alpha}(x) - \\hat{q}_\\alpha(x)\\right)^{-1}$，以避免在散布较大的区域过度采样。**\n该选项的第一部分是正确的。然而，采集函数是 $x^\\star = \\arg\\max_{x \\in \\mathcal{U}} [W(x)]^{-1}$，这等同于 $x^\\star = \\arg\\min_{x \\in \\mathcal{U}} W(x)$。这意味着它选择预测区间最窄的点，即模型*最*确定的地方。这与“在不确定区域集中采样”的既定主动学习目标相矛盾。其给出的理由“以避免在散布较大的区域过度采样”描述的是一种利用（exploitation）而非探索（exploration）的策略，与问题的目标背道而驰。\n**结论：错误。**\n\n**D. 在测试时使用蒙特卡洛（MC）丢弃法（dropout）来产生一个输出的集成，并通过计算MC样本的经验分位数来定义预测区间。选择 $x^\\star = \\arg\\max_{x \\in \\mathcal{U}}$ 该区间的宽度；如果使用MC丢弃法，则分位数回归是不必要的。**\nMC丢弃法是一种有效估计不确定性的方法，但它主要捕捉的是*认知不确定性*（模型参数的不确定性）。问题明确提到了“异方差变异性”，这是一种*偶然不确定性*（数据中固有的随机性）。分位数回归是专门为模拟这种偶然不确定性而设计的，它通过直接学习条件分布的分位数来实现。声称分位数回归是“不必要的”是一个错误的断言，因为对于问题中强调的特定类型的不确定性，它是一种更直接且通常更有效的工具。该选项提出了一个替代性的、不太适合的方法，并摒弃了问题陈述的核心技术。\n**结论：错误。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "不确定性采样关注模型未知的部分，而多样性采样则旨在选择具有代表性的数据池子集，确保广泛的覆盖范围并避免冗余查询。这种方法通常被构建为寻找一个能近似整个数据集的“核心集”。本练习提供了一个具体的动手计算，通过最远优先遍历算法——一种通过迭代选择与已选点最不相似的点来构建核心集的贪心方法——来解决这个问题 。",
            "id": "3729910",
            "problem": "您正在设计一种主动学习策略，用于在多尺度建模流程中开发多层感知器（MLP）代理模型。该学习策略旨在选择一个训练样本的核心集，以最小化MLP特征嵌入空间中的最坏情况表示差距。假设特征嵌入由最后一个隐藏层的映射 $\\phi:\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ 给出，嵌入空间中的距离度量是欧几里得范数。核心集的目标是从可用池 $\\{x_{i}\\}_{i=1}^{n}$ 中选择一个大小为 $k$ 的子集 $S$，以最小化池中任意点到其在嵌入空间中最近的已选点之间的最大距离。请从第一性原理（度量空间覆盖解释）构建此目标，然后为下面描述的玩具数据集计算最远优先遍历序列，以近似极小化极大目标。\n\n考虑输入空间 $\\mathbb{R}^{2}$ 中的点池，由下式给出\n$$\nx_{1}=(0,0),\\quad x_{2}=(2,0),\\quad x_{3}=(0,2),\\quad x_{4}=(2,2),\\quad x_{5}=(5,0),\\quad x_{6}=(5,2).\n$$\n假设嵌入映射是恒等映射 $\\phi(x)=x$。在嵌入空间中使用欧几里得度量 $d(z,z')=\\|z-z'\\|_{2}$。设核心集大小为 $k=3$。从 $x_{1}$ 开始初始化最远优先遍历，并在每一步选择一个点，该点使其与嵌入空间中最近的已选点之间的距离最大化。选择 $k$ 个点后，计算所达到的极小化极大半径，该半径定义为所有池点到其在嵌入空间中最近的已选点的距离的最大值。无需四舍五入，最终答案应报告为单个不带单位的实数。",
            "solution": "用户提供了一个来自机器学习和数值优化领域的有效、适定的问题陈述。\n\n### 第 1 步：问题构建\n\n该问题要求选择一个核心集，这是主动学习和数据摘要中的一个经典问题。我们给定一个输入空间 $\\mathbb{R}^d$ 中的数据点池 $P = \\{x_i\\}_{i=1}^{n}$。这些点通过一个映射 $\\phi$ 被映射到一个嵌入空间 $\\mathbb{R}^m$ 中。目标是选择一个大小为 $k$ 的子集 $S \\subset P$，使其在嵌入空间中最好地“代表”整个池 $P$。\n\n表示的质量由最坏情况表示差距来衡量，即池中任意点到其在核心集中的最近代表点的最大距离，该距离在嵌入空间中测量。设 $Z = \\{\\phi(x) | x \\in P\\}$ 为嵌入点的集合。设 $S_k \\subset Z$ 为大小为 $k$ 的嵌入核心集。任意点 $z \\in Z$ 到核心集 $S_k$ 的距离由 $d(z, S_k) = \\min_{s \\in S_k} d(z, s)$ 给出，其中 $d$ 是指定的距离度量。\n\n问题是要找到核心集 $S_k^*$，以最小化这个最大距离，该距离也被称为覆盖半径。要最小化的目标函数是：\n$$\nR(S_k) = \\max_{z \\in Z} d(z, S_k) = \\max_{x \\in P} \\min_{s \\in S} d(\\phi(x), \\phi(s))\n$$\n这是 $k$-中心问题的表述，该问题是已知的NP-难问题。问题指定使用一种称为**最远优先遍历**的贪婪近似算法。\n\n### 第 2 步：应用最远优先遍历算法\n\n我们给定以下条件：\n- 点池 $P = \\{x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}\\}$，其中\n  $x_{1}=(0,0)$, $x_{2}=(2,0)$, $x_{3}=(0,2)$, $x_{4}=(2,2)$, $x_{5}=(5,0)$, $x_{6}=(5,2)$。\n- 嵌入映射：$\\phi(x) = x$。这意味着嵌入空间与输入空间相同，因此我们直接使用点 $x_i$。\n- 距离度量：欧几里得距离 $d(x_i, x_j) = \\|x_i - x_j\\|_{2}$。\n- 核心集大小：$k=3$。\n- 初始化：核心集 $S$ 中的第一个点是 $x_1$。\n\n最远优先遍历算法的步骤如下：\n1. 用一个起始点初始化核心集 $S$。\n2. 对于 $i$ 从 $2$ 到 $k$：\n   a. 在池 $P \\setminus S$ 中找到距离当前集合 $S$ 最远的点 $p^*$。即，$p^* = \\arg\\max_{p \\in P \\setminus S} \\min_{s \\in S} d(p, s)$。\n   b. 将 $p^*$ 添加到 $S$ 中。\n\n让我们执行这个过程。设 $S_i$ 是经过 $i$ 次选择后的核心集。\n\n**初始化 (i=1)：**\n核心集用 $x_1$ 初始化。\n$$S_1 = \\{x_1\\} = \\{(0,0)\\}$$\n\n**选择第二个点 (i=2)：**\n我们必须在 $P \\setminus S_1$ 中找到距离 $S_1$ 最远的点。对于任意点 $x_j$，它到 $S_1$ 的距离是 $d(x_j, S_1) = d(x_j, x_1)$。我们计算这些距离：\n- $d(x_2, x_1) = \\|(2,0) - (0,0)\\|_2 = \\sqrt{2^2 + 0^2} = 2$。\n- $d(x_3, x_1) = \\|(0,2) - (0,0)\\|_2 = \\sqrt{0^2 + 2^2} = 2$。\n- $d(x_4, x_1) = \\|(2,2) - (0,0)\\|_2 = \\sqrt{2^2 + 2^2} = \\sqrt{8}$。\n- $d(x_5, x_1) = \\|(5,0) - (0,0)\\|_2 = \\sqrt{5^2 + 0^2} = 5 = \\sqrt{25}$。\n- $d(x_6, x_1) = \\|(5,2) - (0,0)\\|_2 = \\sqrt{5^2 + 2^2} = \\sqrt{29}$。\n\n最大距离是 $\\sqrt{29}$，对应于点 $x_6$。因此，选择的第二个点是 $x_6$。\n$$S_2 = \\{x_1, x_6\\} = \\{(0,0), (5,2)\\}$$\n\n**选择第三个点 (i=3)：**\n我们必须在 $P \\setminus S_2 = \\{x_2, x_3, x_4, x_5\\}$ 中找到距离 $S_2$ 最远的点。对于每个候选点 $x_j$，我们计算它到集合 $S_2$ 的距离，即 $d(x_j, S_2) = \\min\\{d(x_j, x_1), d(x_j, x_6)\\}$。\n- 对于 $x_2=(2,0)$:\n  $d(x_2, x_1) = 2$。\n  $d(x_2, x_6) = \\|(2,0) - (5,2)\\|_2 = \\|(-3, -2)\\|_2 = \\sqrt{(-3)^2 + (-2)^2} = \\sqrt{13}$。\n  $d(x_2, S_2) = \\min\\{2, \\sqrt{13}\\} = 2 = \\sqrt{4}$。\n- 对于 $x_3=(0,2)$:\n  $d(x_3, x_1) = 2$。\n  $d(x_3, x_6) = \\|(0,2) - (5,2)\\|_2 = \\|(-5, 0)\\|_2 = \\sqrt{(-5)^2 + 0^2} = 5$。\n  $d(x_3, S_2) = \\min\\{2, 5\\} = 2 = \\sqrt{4}$。\n- 对于 $x_4=(2,2)$:\n  $d(x_4, x_1) = \\sqrt{8}$。\n  $d(x_4, x_6) = \\|(2,2) - (5,2)\\|_2 = \\|(-3, 0)\\|_2 = \\sqrt{(-3)^2 + 0^2} = 3 = \\sqrt{9}$。\n  $d(x_4, S_2) = \\min\\{\\sqrt{8}, 3\\} = \\sqrt{8}$。\n- 对于 $x_5=(5,0)$:\n  $d(x_5, x_1) = 5$。\n  $d(x_5, x_6) = \\|(5,0) - (5,2)\\|_2 = \\|(0, -2)\\|_2 = \\sqrt{0^2 + (-2)^2} = 2$。\n  $d(x_5, S_2) = \\min\\{5, 2\\} = 2 = \\sqrt{4}$。\n\n比较距离 $\\{d(x_2, S_2), d(x_3, S_2), d(x_4, S_2), d(x_5, S_2)\\} = \\{\\sqrt{4}, \\sqrt{4}, \\sqrt{8}, \\sqrt{4}\\}$，最大值是 $\\sqrt{8}$，对应于点 $x_4$。因此，选择的第三个点是 $x_4$。\n\n大小为 $k=3$ 的最终核心集是：\n$$S_3 = \\{x_1, x_4, x_6\\} = \\{(0,0), (2,2), (5,2)\\}$$\n\n### 第 3 步：计算所达到的极小化极大半径\n\n问题要求计算所选核心集 $S_3$ 的极小化极大半径。这是覆盖半径 $R(S_3)$，定义为池中任意点到其在 $S_3$ 中最近点的最大距离。\n$$R(S_3) = \\max_{x_j \\in P} d(x_j, S_3) = \\max_{x_j \\in P} \\min_{s \\in S_3} d(x_j, s)$$\n\n我们需要为所有 $j \\in \\{1, \\dots, 6\\}$ 计算 $d(x_j, S_3)$。\n- 对于核心集 $S_3 = \\{x_1, x_4, x_6\\}$ 中的点，到该集合的距离根据定义为 $0$。\n  $d(x_1, S_3) = 0$。\n  $d(x_4, S_3) = 0$。\n  $d(x_6, S_3) = 0$。\n- 对于剩余的点 $\\{x_2, x_3, x_5\\}$：\n  - 对于 $x_2=(2,0)$:\n    $d(x_2, x_1) = \\|(2,0) - (0,0)\\|_2 = 2$。\n    $d(x_2, x_4) = \\|(2,0) - (2,2)\\|_2 = \\|(0, -2)\\|_2 = 2$。\n    $d(x_2, x_6) = \\|(2,0) - (5,2)\\|_2 = \\|(-3, -2)\\|_2 = \\sqrt{13}$。\n    $d(x_2, S_3) = \\min\\{2, 2, \\sqrt{13}\\} = 2$。\n  - 对于 $x_3=(0,2)$:\n    $d(x_3, x_1) = \\|(0,2) - (0,0)\\|_2 = 2$。\n    $d(x_3, x_4) = \\|(0,2) - (2,2)\\|_2 = \\|(-2, 0)\\|_2 = 2$。\n    $d(x_3, x_6) = \\|(0,2) - (5,2)\\|_2 = \\|(-5, 0)\\|_2 = 5$。\n    $d(x_3, S_3) = \\min\\{2, 2, 5\\} = 2$。\n  - 对于 $x_5=(5,0)$:\n    $d(x_5, x_1) = \\|(5,0) - (0,0)\\|_2 = 5$。\n    $d(x_5, x_4) = \\|(5,0) - (2,2)\\|_2 = \\|(3, -2)\\|_2 = \\sqrt{13}$。\n    $d(x_5, x_6) = \\|(5,0) - (5,2)\\|_2 = \\|(0, -2)\\|_2 = 2$。\n    $d(x_5, S_3) = \\min\\{5, \\sqrt{13}, 2\\} = 2$。\n\n每个池点到核心集 $S_3$ 的距离集合是 $\\{0, 2, 2, 0, 2, 0\\}$。\n极小化极大半径是这些值中的最大值。\n$$R(S_3) = \\max\\{0, 2, 2, 0, 2, 0\\} = 2$$\n\n因此，所达到的极小化极大半径是 $2$。",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "现实世界的应用通常需要根据具体问题结构，对主动学习策略进行精密的融合。这项综合实践将理论付诸实践，要求您在一个异构数据环境中构建一个完整的主动学习流程。您将结合使用蒙特卡洛 dropout 估计模型（认知）不确定性与已知的（偶然）数据不确定性，并实现一个能够感知不同区域的筛选逻辑，从而实际演示如何整合这些概念来构建高效、数据经济的 MLP 模型 。",
            "id": "3729952",
            "problem": "你的任务是为在异构一维数据集上训练的多层感知器（MLP）实现一个主动学习选择机制。该数据集包含具有不同噪声模型的宏观尺度和微观尺度区域。目标是执行能够遵循区域特定阈值和标签预算的、区域感知的（regime-aware）不确定性估计与选择。所有三角函数参数必须以弧度为单位。\n\n从以下基本原理出发：全方差定律以及经过充分检验的高斯分布和拉普拉斯分布的方差公式。全方差定律指出，对于随机变量 $Y$ 和条件变量 $X$，$$\\mathrm{Var}(Y) = \\mathbb{E}\\left[\\mathrm{Var}(Y \\mid X)\\right] + \\mathrm{Var}\\left(\\mathbb{E}[Y \\mid X]\\right).$$ 零均值高斯分布 $\\mathcal{N}(0,\\sigma^{2})$ 的方差为 $\\sigma^{2}$，而尺度参数为 $b$ 的零均值拉普拉斯分布的方差为 $2b^{2}$。\n\n按如下方式构建数据集。在均匀网格上定义输入 $x_{i}$：$$x_{i} = \\frac{i}{N-1}, \\quad i \\in \\{0,1,\\dots,N-1\\},$$ 其中 $N$ 设为 $N = 200$。将定义域划分为两个区域：宏观尺度区域 $\\mathcal{R}_{\\mathrm{macro}} = \\{x \\mid x  0.5\\}$ 和微观尺度区域 $\\mathcal{R}_{\\mathrm{micro}} = \\{x \\mid x \\ge 0.5\\}$。定义无噪声的真实函数（ground-truth function）：$$f(x) = \\sin(2\\pi x) + 0.2\\,\\cos(50\\pi x)\\,\\mathbf{1}_{\\{x \\ge 0.5\\}},$$ 其中 $\\mathbf{1}_{\\{\\cdot\\}}$ 表示指示函数。从 $\\{0,1,\\dots,N-1\\}$ 中取 $S$ 个等间距索引，生成一个大小为 $S$（$S=20$）的初始标记种子集，并将其余点定义为未标记池。对于宏观区域中的标签，添加独立高斯噪声 $\\varepsilon_{\\mathrm{macro}} \\sim \\mathcal{N}(0,\\sigma_{\\mathrm{macro}}^{2})$，其中 $\\sigma_{\\mathrm{macro}} = 0.05$；对于微观区域中的标签，添加独立拉普拉斯噪声 $\\varepsilon_{\\mathrm{micro}} \\sim \\mathrm{Laplace}(0,b_{\\mathrm{micro}}(x))$，其中 $b_{\\mathrm{micro}}(x) = 0.03 + 0.05 x$。三角函数参数和所有计算都必须以弧度为单位。标记的输出为：$$y_{i} = f(x_{i}) + \\begin{cases}\\varepsilon_{\\mathrm{macro}},  x_{i} \\in \\mathcal{R}_{\\mathrm{macro}}, \\\\ \\varepsilon_{\\mathrm{micro}},  x_{i} \\in \\mathcal{R}_{\\mathrm{micro}}. \\end{cases}$$\n\n在标记的种子集上训练一个单隐藏层的多层感知器（MLP），该 MLP 使用修正线性单元（ReLU）激活函数和 Dropout。使用隐藏层宽度 $H$（$H = 32$），隐藏单元上的 Dropout 概率 $p$（$p = 0.1$），均方误差损失函数，学习率 $\\eta$（$\\eta = 0.01$），并运行 $E$ 个周期（$E = 100$）。在训练期间采用反向 Dropout（inverted dropout）。训练结束后，通过蒙特卡洛 Dropout（Monte Carlo dropout）估计未标记池上的预测不确定性：执行 $T$ 次随机前向传播（$T=20$），以获得每个池中点的预测样本方差，记为 $\\widehat{\\mathrm{Var}}_{\\mathrm{pred}}(x)$。应用全方差定律，调整此模型不确定性以考虑区域噪声：将每个池中点的调整后不确定性定义为：$$u(x) = \\widehat{\\mathrm{Var}}_{\\mathrm{pred}}(x) + \\begin{cases}\\sigma_{\\mathrm{macro}}^{2},  x \\in \\mathcal{R}_{\\mathrm{macro}}, \\\\ 2\\,b_{\\mathrm{micro}}(x)^{2},  x \\in \\mathcal{R}_{\\mathrm{micro}}. \\end{cases}$$\n\n对每个测试用例参数集，按如下方式实现区域感知的选择。设 $\\alpha_{\\mathrm{macro}} \\in (0,1)$ 和 $\\alpha_{\\mathrm{micro}} \\in (0,1)$ 为区域特定的风险参数，用于定义基于分位数的阈值。对于区域 $r \\in \\{\\mathrm{macro}, \\mathrm{micro}\\}$，计算 $\\tau_{r}$ 作为池中点集合 $\\{u(x_{j}) : x_{j} \\in \\mathcal{R}_{r}\\}$ 的 $(1-\\alpha_{r})$ 分位数。定义一个总预算 $B \\in \\mathbb{N}$ 和区域权重 $w_{\\mathrm{macro}}, w_{\\mathrm{micro}} \\in [0,1]$，且 $w_{\\mathrm{macro}} + w_{\\mathrm{micro}} = 1$。分配各区域的预算：$$B_{\\mathrm{macro}} = \\left\\lfloor B\\,w_{\\mathrm{macro}} \\right\\rfloor, \\quad B_{\\mathrm{micro}} = \\left\\lfloor B\\,w_{\\mathrm{micro}} \\right\\rfloor.$$ 在每个区域中，选择调整后不确定性严格超过阈值的池中点，即 $u(x)  \\tau_{r}$；如果这类点的数量超过该区域的预算，则按 $u(x)$ 降序选择前 $B_{r}$ 个点，若出现平局则按索引升序选择；如果数量少于该区域的预算，则选择所有严格超过阈值的点。每个测试用例的主动选择输出必须是一个双元素列表 $[n_{\\mathrm{macro}}, n_{\\mathrm{micro}}]$，其中 $n_{\\mathrm{macro}}$ 和 $n_{\\mathrm{micro}}$ 是根据此规则在每个区域中选出的点的数量。\n\n使用以下参数值的测试套件来评估你的实现：\n- 测试用例1：$(\\alpha_{\\mathrm{macro}}, \\alpha_{\\mathrm{micro}}, B, w_{\\mathrm{macro}}, w_{\\mathrm{micro}}) = (0.2, 0.2, 20, 0.5, 0.5)$。\n- 测试用例2：$(\\alpha_{\\mathrm{macro}}, \\alpha_{\\mathrm{micro}}, B, w_{\\mathrm{macro}}, w_{\\mathrm{micro}}) = (0.1, 0.4, 14, 0.4, 0.6)$。\n- 测试用例3：$(\\alpha_{\\mathrm{macro}}, \\alpha_{\\mathrm{micro}}, B, w_{\\mathrm{macro}}, w_{\\mathrm{micro}}) = (0.3, 0.3, 0, 0.5, 0.5)$。\n- 测试用例4：$(\\alpha_{\\mathrm{macro}}, \\alpha_{\\mathrm{micro}}, B, w_{\\mathrm{macro}}, w_{\\mathrm{micro}}) = (0.49, 0.49, 6, 0.7, 0.3)$。\n\n你的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个由逗号分隔的列表，并用方括号括起来，例如 $[ [a_{1},a_{2}], [b_{1},b_{2}], [c_{1},c_{2}], [d_{1},d_{2}] ]$，其中 $[a_{1},a_{2}]$ 是测试用例1的双元素列表，以此类推。角度单位是弧度。无需进行物理单位转换。必须通过在内部固定所有随机种子来确保确定性行为。",
            "solution": "问题陈述经评估有效。它在科学上基于机器学习和统计学原理，问题定义清晰，具有明确的确定性程序，且客观公正。所有参数和约束都已完全指定，从而可以得出一个唯一的、可验证的解。因此，我们可以着手求解。\n\n解决方案是通过遵循问题陈述中描述的步骤序列构建的：数据生成、MLP模型训练、不确定性估计，以及最后的区域感知主动学习选择。\n\n**步骤1：数据集生成**\n\n首先，我们构建一维输入数据集。输入域由在区间 $[0, 1]$ 上均匀分布的 $N=200$ 个点 $x_i$ 组成：\n$$x_{i} = \\frac{i}{N-1}, \\quad i \\in \\{0, 1, \\dots, N-1\\}$$\n定义域被划分为一个宏观尺度区域 $\\mathcal{R}_{\\mathrm{macro}} = \\{x \\mid x  0.5\\}$ 和一个微观尺度区域 $\\mathcal{R}_{\\mathrm{micro}} = \\{x \\mid x \\ge 0.5\\}$。\n\n无噪声的真实信号 $f(x)$ 定义为：\n$$f(x) = \\sin(2\\pi x) + 0.2\\,\\cos(50\\pi x)\\,\\mathbf{1}_{\\{x \\ge 0.5\\}}$$\n其中 $\\mathbf{1}_{\\{\\cdot\\}}$ 是指示函数。包含 $\\cos(50\\pi x)$ 的项仅在微观尺度区域引入高频振荡。\n\n从总共 $N=200$ 个点中，通过取等间距索引选出一个大小为 $S=20$ 的初始标记种子集。剩下的 $N-S=180$ 个点构成了未标记池。\n\n通过向真实值 $f(x_i)$ 添加区域特定的噪声来为种子集生成带噪声的标签 $y_i$：\n$$y_{i} = f(x_{i}) + \\begin{cases} \\varepsilon_{\\mathrm{macro}},  x_{i} \\in \\mathcal{R}_{\\mathrm{macro}} \\\\ \\varepsilon_{\\mathrm{micro}},  x_{i} \\in \\mathcal{R}_{\\mathrm{micro}} \\end{cases}$$\n在宏观区域，噪声 $\\varepsilon_{\\mathrm{macro}}$ 从零均值高斯分布 $\\mathcal{N}(0, \\sigma_{\\mathrm{macro}}^2)$ 中抽取，其中 $\\sigma_{\\mathrm{macro}}=0.05$。在微观区域，噪声 $\\varepsilon_{\\mathrm{micro}}$ 从零均值拉普拉斯分布 $\\mathrm{Laplace}(0, b_{\\mathrm{micro}}(x))$ 中抽取，其尺度参数 $b_{\\mathrm{micro}}(x) = 0.03 + 0.05x$ 依赖于位置。为确保可复现性，所有随机数生成都已设定种子。\n\n**步骤2：MLP模型与训练**\n\n在标记的种子集上训练一个具有单隐藏层的多层感知器（MLP）。\n-   **架构**：输入层（1个神经元）$\\rightarrow$ 隐藏层（$H=32$ 个神经元，使用ReLU激活函数）$\\rightarrow$ 输出层（1个神经元）。\n-   **正则化**：在隐藏层的激活值上应用 Dropout，概率为 $p=0.1$。使用反向 Dropout（inverted dropout），这意味着在训练期间，激活值会按 $1/(1-p)$ 进行缩放。\n-   **训练**：使用梯度下降法对模型进行 $E=100$ 个周期的训练，以最小化均方误差（MSE）损失，学习率为 $\\eta=0.01$。\n\n单个输入 $x$ 的前向传播过程如下：\n1.  隐藏层预激活：$z^{[1]} = x W^{[1]} + b^{[1]}$\n2.  隐藏层激活：$a^{[1]} = \\mathrm{ReLU}(z^{[1]})$\n3.  应用反向 Dropout：创建一个二元掩码 $d^{[1]}$，其中每个元素以概率 $p$ 为 $0$，以概率 $1-p$ 为 $1/(1-p)$。然后，$a_{\\text{drop}}^{[1]} = a^{[1]} \\odot d^{[1]}$。\n4.  输出层预激活：$z^{[2]} = a_{\\text{drop}}^{[1]} W^{[2]} + b^{[2]}$\n5.  预测：$\\hat{y} = z^{[2]}$\n\n权重 $W^{[1]}, W^{[2]}$ 和偏置 $b^{[1]}, b^{[2]}$ 基于训练批次上的 MSE 损失，通过反向传播进行更新。\n\n**步骤3：不确定性量化**\n\n训练结束后，使用蒙特卡洛（MC）Dropout 估计未标记池中每个点 $x_j$ 的预测不确定性。这包括在启用 Dropout 的情况下，对网络进行 $T=20$ 次随机前向传播。对于每个点 $x_j$，这将产生一组 $T$ 个预测值 $\\{\\hat{y}_{j,t}\\}_{t=1}^T$。\n\n这些预测的样本方差可作为模型不确定性（认知不确定性）的估计：\n$$\\widehat{\\mathrm{Var}}_{\\mathrm{pred}}(x_j) = \\frac{1}{T-1} \\sum_{t=1}^{T} (\\hat{y}_{j,t} - \\bar{\\hat{y}}_j)^2$$\n其中 $\\bar{\\hat{y}}_j$ 是对 $x_j$ 的 $T$ 次预测的均值。\n\n通过结合数据不确定性（偶然不确定性），可以获得总预测不确定性 $u(x)$，该不确定性可从问题的噪声模型中得知。应用全方差定律，总不确定性是模型不确定性与期望数据方差之和。期望数据方差就是每个区域指定噪声分布的方差。\n-   对于 $\\mathcal{R}_{\\mathrm{macro}}$，$\\mathcal{N}(0, \\sigma_{\\mathrm{macro}}^2)$ 的方差是 $\\sigma_{\\mathrm{macro}}^2$。\n-   对于 $\\mathcal{R}_{\\mathrm{micro}}$，$\\mathrm{Laplace}(0, b_{\\mathrm{micro}}(x))$ 的方差是 $2b_{\\mathrm{micro}}(x)^2$。\n\n因此，池中点的调整后不确定性 $u(x)$ 为：\n$$u(x) = \\widehat{\\mathrm{Var}}_{\\mathrm{pred}}(x) + \\begin{cases} \\sigma_{\\mathrm{macro}}^{2},  x \\in \\mathcal{R}_{\\mathrm{macro}} \\\\ 2\\,b_{\\mathrm{micro}}(x)^{2},  x \\in \\mathcal{R}_{\\mathrm{micro}} \\end{cases}$$\n\n**步骤4：区域感知的主动选择**\n\n最后一步是根据调整后的不确定性 $u(x)$ 和区域特定标准，从未标记池中选择点。对每个测试用例执行此过程。\n给定一个测试用例 $(\\alpha_{\\mathrm{macro}}, \\alpha_{\\mathrm{micro}}, B, w_{\\mathrm{macro}}, w_{\\mathrm{micro}})$：\n1.  **预算分配**：总预算 $B$ 在两个区域之间进行分配：$B_{\\mathrm{macro}} = \\lfloor B w_{\\mathrm{macro}} \\rfloor$ 和 $B_{\\mathrm{micro}} = \\lfloor B w_{\\mathrm{micro}} \\rfloor$。\n2.  **阈值计算**：对于每个区域 $r \\in \\{\\mathrm{macro}, \\mathrm{micro}\\}$，确定一个不确定性阈值 $\\tau_r$。$\\tau_r$ 是属于区域 $\\mathcal{R}_r$ 的所有池中点的不确定性值 $\\{u(x_j)\\}$ 的 $(1-\\alpha_r)$-分位数。\n3.  **选择**：\n    a.  对于每个区域 $r$，识别出候选点集 $C_r = \\{x_j \\mid x_j \\in \\mathcal{R}_r, u(x_j)  \\tau_r\\}$。\n    b.  确定要从该区域选择的点的数量 $n_r$。如果候选点数量 $|C_r|$ 大于预算 $B_r$，则选择不确定性最高的 $B_r$ 个点，因此 $n_r = B_r$。不确定性相同时，通过选择索引 $i$ 较小的点来打破平局。如果 $|C_r| \\le B_r$，则选择所有候选点，因此 $n_r = |C_r|$。\n4.  **输出**：该测试用例的结果是计数列表 $[n_{\\mathrm{macro}}, n_{\\mathrm{micro}}]$。\n\n从数据生成到最终选择循环的整个过程在以下 Python 代码中实现，通过固定随机种子来确保确定性。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the full active learning pipeline: data generation, MLP training,\n    uncertainty estimation, and regime-aware selection.\n    \"\"\"\n    # Set a global seed for reproducibility\n    SEED = 42\n    np.random.seed(SEED)\n\n    # 1. Constants and Parameters from the problem statement\n    N = 200\n    S = 20\n    H = 32\n    p_dropout = 0.1\n    eta = 0.01\n    E = 100\n    T = 20\n    sigma_macro = 0.05\n\n    test_cases = [\n        (0.2, 0.2, 20, 0.5, 0.5), # Case 1\n        (0.1, 0.4, 14, 0.4, 0.6), # Case 2\n        (0.3, 0.3, 0, 0.5, 0.5),   # Case 3\n        (0.49, 0.49, 6, 0.7, 0.3), # Case 4\n    ]\n\n    # 2. Data Generation\n    x = np.linspace(0, 1, N).reshape(-1, 1)\n\n    def f(x_val):\n        indicator = (x_val = 0.5).astype(float)\n        return np.sin(2 * np.pi * x_val) + 0.2 * np.cos(50 * np.pi * x_val) * indicator\n\n    y_true = f(x)\n\n    labeled_indices = np.round(np.linspace(0, N - 1, S)).astype(int)\n    unlabeled_indices = np.array(sorted(list(set(range(N)) - set(labeled_indices))))\n\n    y_noisy = np.copy(y_true)\n    \n    # Generate noise for all points, but apply it only to the labeled set\n    noise_macro_all = np.random.normal(0, sigma_macro, size=(N, 1))\n    b_micro_all = 0.03 + 0.05 * x\n    noise_micro_all = np.random.laplace(0, b_micro_all, size=(N, 1))\n\n    for idx in labeled_indices:\n        if x[idx]  0.5:\n            y_noisy[idx] += noise_macro_all[idx]\n        else:\n            y_noisy[idx] += noise_micro_all[idx]\n\n    x_train = x[labeled_indices]\n    y_train = y_noisy[labeled_indices]\n    x_pool = x[unlabeled_indices]\n\n    # 3. MLP Training\n    # He initialization for layer 1 (ReLU), Xavier/Glorot for layer 2\n    w1 = np.random.randn(1, H) * np.sqrt(2.0 / 1)\n    b1 = np.zeros((1, H))\n    w2 = np.random.randn(H, 1) * np.sqrt(1.0 / H)\n    b2 = np.zeros((1, 1))\n\n    num_train_samples = len(x_train)\n    for epoch in range(E):\n        # Forward pass\n        z1 = x_train @ w1 + b1\n        a1 = np.maximum(0, z1)\n        \n        # Inverted Dropout\n        dropout_mask = (np.random.rand(*a1.shape)  p_dropout) / (1 - p_dropout)\n        a1_dropped = a1 * dropout_mask\n\n        z2 = a1_dropped @ w2 + b2\n        y_pred = z2\n\n        # Backward pass\n        grad_y_pred = (2 / num_train_samples) * (y_pred - y_train)\n        \n        grad_w2 = a1_dropped.T @ grad_y_pred\n        grad_b2 = np.sum(grad_y_pred, axis=0, keepdims=True)\n\n        grad_a1_dropped = grad_y_pred @ w2.T\n        grad_a1 = grad_a1_dropped * dropout_mask\n        grad_z1 = grad_a1 * (z1  0)\n\n        grad_w1 = x_train.T @ grad_z1\n        grad_b1 = np.sum(grad_z1, axis=0, keepdims=True)\n\n        # Update weights\n        w1 -= eta * grad_w1\n        b1 -= eta * grad_b1\n        w2 -= eta * grad_w2\n        b2 -= eta * grad_b2\n\n    # 4. Uncertainty Estimation\n    pool_predictions = np.zeros((len(x_pool), T))\n    for t_idx in range(T):\n        z1_pool = x_pool @ w1 + b1\n        a1_pool = np.maximum(0, z1_pool)\n        \n        dropout_mask_pred = (np.random.rand(*a1_pool.shape)  p_dropout) / (1 - p_dropout)\n        a1_dropped_pred = a1_pool * dropout_mask_pred\n        \n        z2_pool = a1_dropped_pred @ w2 + b2\n        pool_predictions[:, t_idx] = z2_pool.flatten()\n\n    var_pred = np.var(pool_predictions, axis=1, ddof=1)\n\n    var_data = np.zeros(len(x_pool))\n    pool_is_macro = (x_pool.flatten()  0.5)\n    \n    var_data[pool_is_macro] = sigma_macro**2\n    \n    b_micro_pool = 0.03 + 0.05 * x_pool[~pool_is_macro]\n    var_data[~pool_is_macro] = 2 * (b_micro_pool**2).flatten()\n    \n    u_pool = var_pred + var_data\n    pool_uncertainties = list(zip(unlabeled_indices, u_pool))\n\n    # 5. Active Selection Loop for Test Cases\n    results = []\n    for case in test_cases:\n        alpha_macro, alpha_micro, B, w_macro, w_micro = case\n        \n        B_macro = int(B * w_macro)\n        B_micro = int(B * w_micro)\n\n        uncertainties_macro = [(idx, u) for idx, u in pool_uncertainties if x[idx]  0.5]\n        uncertainties_micro = [(idx, u) for idx, u in pool_uncertainties if x[idx] = 0.5]\n\n        # Process Macro Regime\n        n_macro = 0\n        if len(uncertainties_macro)  0:\n            u_values_macro = [u for _, u in uncertainties_macro]\n            tau_macro = np.quantile(u_values_macro, 1 - alpha_macro)\n            \n            candidates_macro = [(idx, u) for idx, u in uncertainties_macro if u  tau_macro]\n            \n            n_macro = min(len(candidates_macro), B_macro)\n\n        # Process Micro Regime\n        n_micro = 0\n        if len(uncertainties_micro)  0:\n            u_values_micro = [u for _, u in uncertainties_micro]\n            tau_micro = np.quantile(u_values_micro, 1 - alpha_micro)\n            \n            candidates_micro = [(idx, u) for idx, u in uncertainties_micro if u  tau_micro]\n            \n            n_micro = min(len(candidates_micro), B_micro)\n\n        results.append([n_macro, n_micro])\n    \n    # Format the final output string to be exactly as specified\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}