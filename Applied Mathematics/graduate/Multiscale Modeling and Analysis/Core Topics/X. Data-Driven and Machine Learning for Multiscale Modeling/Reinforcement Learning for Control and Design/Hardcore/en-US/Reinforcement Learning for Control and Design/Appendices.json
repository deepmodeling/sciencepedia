{
    "hands_on_practices": [
        {
            "introduction": "Mastering reinforcement learning begins with a solid grasp of the Bellman optimality equations, which define the fixed-point condition for the optimal value function $V^\\star$. This first practice problem challenges you to derive and solve this system of equations for a small, fully specified Markov Decision Process (MDP) . Successfully computing the exact value function provides a foundational understanding of the target that all reinforcement learning algorithms strive to approximate.",
            "id": "3801996",
            "problem": "Consider a coarse-grained controller design problem framed as a Markov Decision Process (MDP), arising from multiscale modeling in which microscopic fluctuations are aggregated into macrostates with control actions that influence their evolution. Let the MDP have two states, labeled $1$ and $2$, and two actions, labeled $a_1$ and $a_2$. The process is infinite-horizon and discounted with discount factor $\\gamma$ satisfying $0  \\gamma  1$. The controlled transition probabilities are given by $P(1 \\mid 1, a_1) = 0.9$, $P(2 \\mid 1, a_1) = 0.1$, $P(1 \\mid 1, a_2) = 0.5$, $P(2 \\mid 1, a_2) = 0.5$, $P(2 \\mid 2, a_1) = 1$, and $P(2 \\mid 2, a_2) = 1$. The instantaneous rewards are $r(1, a_1) = 1$, $r(1, a_2) = 0$, $r(2, a_1) = 0$, and $r(2, a_2) = 0$. The discount factor is $\\gamma = 0.95$. Reinforcement Learning (RL) seeks to compute the optimal value function $V^\\star$, corresponding to the supremum over all admissible policies of the expected sum of discounted rewards.\n\nStarting from the core definitions of an infinite-horizon discounted MDP, the Markov property, and the principle of optimality, derive the fixed-point relationships that $V^\\star(1)$ and $V^\\star(2)$ must satisfy and use them to compute the optimal value function $V^\\star$ exactly for this system. Express the final result as an exact analytic expression; do not round.",
            "solution": "The problem is first subjected to validation.\n\n### Step 1: Extract Givens\n-   **State Space**: $S = \\{1, 2\\}$\n-   **Action Space**: $A = \\{a_1, a_2\\}$\n-   **Discount Factor**: $\\gamma = 0.95$, with the condition $0  \\gamma  1$.\n-   **Transition Probabilities**, $P(s' \\mid s, a)$:\n    -   $P(1 \\mid 1, a_1) = 0.9$\n    -   $P(2 \\mid 1, a_1) = 0.1$\n    -   $P(1 \\mid 1, a_2) = 0.5$\n    -   $P(2 \\mid 1, a_2) = 0.5$\n    -   $P(2 \\mid 2, a_1) = 1$, which implies $P(1 \\mid 2, a_1) = 0$.\n    -   $P(2 \\mid 2, a_2) = 1$, which implies $P(1 \\mid 2, a_2) = 0$.\n-   **Reward Function**, $r(s, a)$:\n    -   $r(1, a_1) = 1$\n    -   $r(1, a_2) = 0$\n    -   $r(2, a_1) = 0$\n    -   $r(2, a_2) = 0$\n-   **Objective**: Compute the optimal value function $V^\\star$, which is a vector $(V^\\star(1), V^\\star(2))$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a finite-state, finite-action, infinite-horizon discounted Markov Decision Process (MDP).\n-   **Scientifically Grounded**: The problem is formulated using the standard mathematical framework of MDPs, which is a cornerstone of reinforcement learning and optimal control theory. All definitions and relationships are consistent with established theory.\n-   **Well-Posed**: For a finite MDP with a discount factor $\\gamma \\in [0, 1)$, the Bellman optimality operator is a contraction mapping. By the Banach fixed-point theorem, this operator has a unique fixed point, which is precisely the optimal value function $V^\\star$. Therefore, a unique, stable, and meaningful solution exists.\n-   **Objective**: The problem is stated using precise mathematical definitions and values. There is no ambiguity or subjective language.\n-   **Incomplete or Contradictory Setup**: The problem is self-contained. All necessary components (states, actions, transition probabilities, rewards, and discount factor) are provided. The transition probabilities for each state-action pair correctly sum to $1$. For example, $P(1 \\mid 1, a_1) + P(2 \\mid 1, a_1) = 0.9 + 0.1 = 1$. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be derived.\n\nThe core principle for solving this problem is the principle of optimality, which states that an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. This principle gives rise to the Bellman optimality equations, which provide a recursive definition for the optimal value function, $V^\\star(s)$. For any state $s \\in S$, the equation is:\n$$V^\\star(s) = \\max_{a \\in A} \\left\\{ r(s, a) + \\gamma \\sum_{s' \\in S} P(s' \\mid s, a) V^\\star(s') \\right\\}$$\nThis equation establishes a system of fixed-point relations that the components of $V^\\star$ must satisfy. We will now write these equations for each state, $s=1$ and $s=2$.\n\nFor state $s=2$:\nThe Bellman optimality equation for $V^\\star(2)$ is:\n$$V^\\star(2) = \\max_{a \\in \\{a_1, a_2\\}} \\left\\{ r(2, a) + \\gamma \\left( P(1 \\mid 2, a)V^\\star(1) + P(2 \\mid 2, a)V^\\star(2) \\right) \\right\\}$$\nWe evaluate the term inside the maximum for each action:\n-   For action $a_1$: $r(2, a_1) + \\gamma (P(1 \\mid 2, a_1)V^\\star(1) + P(2 \\mid 2, a_1)V^\\star(2)) = 0 + \\gamma (0 \\cdot V^\\star(1) + 1 \\cdot V^\\star(2)) = \\gamma V^\\star(2)$.\n-   For action $a_2$: $r(2, a_2) + \\gamma (P(1 \\mid 2, a_2)V^\\star(1) + P(2 \\mid 2, a_2)V^\\star(2)) = 0 + \\gamma (0 \\cdot V^\\star(1) + 1 \\cdot V^\\star(2)) = \\gamma V^\\star(2)$.\n\nThus, the equation for $V^\\star(2)$ becomes:\n$$V^\\star(2) = \\max\\{\\gamma V^\\star(2), \\gamma V^\\star(2)\\} = \\gamma V^\\star(2)$$\nThis implies $V^\\star(2) - \\gamma V^\\star(2) = 0$, or $V^\\star(2)(1 - \\gamma) = 0$. Since we are given $0  \\gamma  1$, the term $(1 - \\gamma)$ is non-zero. Therefore, the only solution is:\n$$V^\\star(2) = 0$$\nThis is intuitive, as state $2$ is an absorbing state that yields zero reward for all subsequent time steps.\n\nFor state $s=1$:\nThe Bellman optimality equation for $V^\\star(1)$ is:\n$$V^\\star(1) = \\max_{a \\in \\{a_1, a_2\\}} \\left\\{ r(1, a) + \\gamma \\left( P(1 \\mid 1, a)V^\\star(1) + P(2 \\mid 1, a)V^\\star(2) \\right) \\right\\}$$\nWe again evaluate the term inside the maximum (the optimal action-value function $Q^\\star(1, a)$) for each action, substituting $V^\\star(2) = 0$:\n-   For action $a_1$:\n    $Q^\\star(1, a_1) = r(1, a_1) + \\gamma \\left( P(1 \\mid 1, a_1)V^\\star(1) + P(2 \\mid 1, a_1)V^\\star(2) \\right)$\n    $Q^\\star(1, a_1) = 1 + \\gamma \\left( 0.9 \\cdot V^\\star(1) + 0.1 \\cdot 0 \\right) = 1 + 0.9\\gamma V^\\star(1)$\n-   For action $a_2$:\n    $Q^\\star(1, a_2) = r(1, a_2) + \\gamma \\left( P(1 \\mid 1, a_2)V^\\star(1) + P(2 \\mid 1, a_2)V^\\star(2) \\right)$\n    $Q^\\star(1, a_2) = 0 + \\gamma \\left( 0.5 \\cdot V^\\star(1) + 0.5 \\cdot 0 \\right) = 0.5\\gamma V^\\star(1)$\n\nThe equation for $V^\\star(1)$ is therefore:\n$$V^\\star(1) = \\max \\{ 1 + 0.9\\gamma V^\\star(1), 0.5\\gamma V^\\star(1) \\}$$\nSince all rewards are non-negative, the value function must be non-negative, i.e., $V^\\star(1) \\ge 0$. Given that $\\gamma  0$, we have $0.9\\gamma  0.5\\gamma$. Consequently, $1 + 0.9\\gamma V^\\star(1)  0.5\\gamma V^\\star(1)$. The maximum is always achieved by taking action $a_1$.\nThe equation for $V^\\star(1)$ simplifies to a linear equation:\n$$V^\\star(1) = 1 + 0.9\\gamma V^\\star(1)$$\nWe solve for $V^\\star(1)$:\n$$V^\\star(1) - 0.9\\gamma V^\\star(1) = 1$$\n$$V^\\star(1)(1 - 0.9\\gamma) = 1$$\n$$V^\\star(1) = \\frac{1}{1 - 0.9\\gamma}$$\nNow, we substitute the given value $\\gamma = 0.95$:\n$$V^\\star(1) = \\frac{1}{1 - 0.9(0.95)}$$\n$$V^\\star(1) = \\frac{1}{1 - 0.855}$$\n$$V^\\star(1) = \\frac{1}{0.145}$$\nTo obtain an exact rational expression, we write the denominator as a fraction: $0.145 = \\frac{145}{1000} = \\frac{29}{200}$.\n$$V^\\star(1) = \\frac{1}{\\frac{29}{200}} = \\frac{200}{29}$$\nThus, the optimal value function is the vector $V^\\star = (V^\\star(1), V^\\star(2))$, which is $(\\frac{200}{29}, 0)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{200}{29}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While solving for $V^\\star$ is possible with a known model, most practical applications require learning directly from experience. This exercise introduces Temporal-Difference (TD) learning, a fundamental model-free reinforcement learning method . You will perform a single parameter update for a linear value function approximator, gaining hands-on insight into how an agent learns from a single transition of reward and state change.",
            "id": "3802032",
            "problem": "A controller is trained by Reinforcement Learning (RL) to regulate a multiscale materials process whose coarse-grained state $s$ aggregates microstructural statistics. At each decision time $t$, the state $s_t$ is summarized by three observables: the micro-scale energy density $e(s)$, the two-point correlation at one lattice spacing $c(s)$, and the interfacial area density $a(s)$. A linear state-value approximation $\\hat{V}(s;\\theta)$ is employed with a feature map\n$$\n\\phi(s) \\;=\\; \\begin{pmatrix} 1 \\\\ e(s) \\\\ c(s) \\\\ a(s) \\\\ e(s)\\,c(s) \\end{pmatrix} \\in \\mathbb{R}^{5}, \n$$\nso that \n$$\n\\hat{V}(s;\\theta) \\;=\\; \\theta^{\\top}\\phi(s),\n$$\nwhere $\\theta \\in \\mathbb{R}^{5}$ is the parameter vector. Consider the following single sample transition $(s_t,a_t,r_t,s_{t+1})$ generated under a fixed policy:\n- For $s_t$: $e(s_t)=\\frac{3}{2}$, $c(s_t)=\\frac{1}{5}$, $a(s_t)=\\frac{7}{10}$.\n- For $s_{t+1}$: $e(s_{t+1})=\\frac{4}{3}$, $c(s_{t+1})=\\frac{2}{5}$, $a(s_{t+1})=\\frac{3}{5}$.\n- The immediate reward is $r_t=\\frac{2}{5}$.\n- The discount factor is $\\gamma=\\frac{9}{10}$.\n\nThe initial parameter is\n$$\n\\theta_t \\;=\\; \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{5} \\\\ \\frac{2}{3} \\\\ \\frac{1}{4} \\\\ -\\frac{3}{10} \\end{pmatrix},\n$$\nand the learning rate is $\\alpha=\\frac{3}{20}$.\n\nUsing Temporal-Difference (TD) learning with zero-step bootstrapping $\\mathrm{TD}(0)$ and the linear function approximation described above, carry out one parameter update at time $t$ for this single sample and evaluate the temporal-difference error at time $t$. Express your final answer as a single exact fraction representing the temporal-difference error. Do not round.",
            "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- **State-value approximation:** $\\hat{V}(s;\\theta) = \\theta^{\\top}\\phi(s)$\n- **Feature map:** $\\phi(s) = \\begin{pmatrix} 1 \\\\ e(s) \\\\ c(s) \\\\ a(s) \\\\ e(s)\\,c(s) \\end{pmatrix}$\n- **Parameter vector:** $\\theta \\in \\mathbb{R}^{5}$\n- **State $s_t$ observables:** $e(s_t)=\\frac{3}{2}$, $c(s_t)=\\frac{1}{5}$, $a(s_t)=\\frac{7}{10}$\n- **State $s_{t+1}$ observables:** $e(s_{t+1})=\\frac{4}{3}$, $c(s_{t+1})=\\frac{2}{5}$, $a(s_{t+1})=\\frac{3}{5}$\n- **Immediate reward:** $r_t=\\frac{2}{5}$\n- **Discount factor:** $\\gamma=\\frac{9}{10}$\n- **Initial parameter vector:** $\\theta_t = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{5} \\\\ \\frac{2}{3} \\\\ \\frac{1}{4} \\\\ -\\frac{3}{10} \\end{pmatrix}$\n- **Learning rate:** $\\alpha=\\frac{3}{20}$\n- **Algorithm:** Temporal-Difference learning with zero-step bootstrapping, $\\mathrm{TD}(0)$.\n- **Objective:** Evaluate the temporal-difference error at time $t$, denoted as $\\delta_t$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined computational exercise in the field of reinforcement learning. It is scientifically grounded in the established principles of temporal-difference learning with linear function approximation. All necessary data and definitions are provided, and there are no internal contradictions. The context of multiscale materials modeling is scientifically relevant and provides a realistic setting for the application of such an algorithm. The problem is objective, unambiguous, and solvable. There are no violations of the validation criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution will be provided.\n\nThe problem requires the calculation of the temporal-difference (TD) error for a single sample update using the $\\mathrm{TD}(0)$ algorithm. For a state-value function approximation $\\hat{V}(s;\\theta)$, the TD error at time step $t$, denoted $\\delta_t$, is defined as the difference between the TD target and the current value estimate of state $s_t$.\n\nThe TD target is the sum of the immediate reward $r_t$ and the discounted value of the next state $s_{t+1}$. The formula for the TD error is:\n$$\n\\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}; \\theta_t) - \\hat{V}(s_t; \\theta_t)\n$$\nHere, both value estimates are computed using the parameter vector at time $t$, $\\theta_t$. The value function is given by the linear approximation $\\hat{V}(s;\\theta_t) = \\theta_t^{\\top}\\phi(s)$.\n\nFirst, we construct the feature vectors $\\phi(s_t)$ and $\\phi(s_{t+1})$ using the provided state observables.\n\nFor state $s_t$:\n$e(s_t) = \\frac{3}{2}$, $c(s_t) = \\frac{1}{5}$, $a(s_t) = \\frac{7}{10}$.\nThe interaction term is $e(s_t)c(s_t) = \\frac{3}{2} \\times \\frac{1}{5} = \\frac{3}{10}$.\nThe feature vector $\\phi(s_t)$ is:\n$$\n\\phi(s_t) = \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ \\frac{1}{5} \\\\ \\frac{7}{10} \\\\ \\frac{3}{10} \\end{pmatrix}\n$$\n\nFor state $s_{t+1}$:\n$e(s_{t+1}) = \\frac{4}{3}$, $c(s_{t+1}) = \\frac{2}{5}$, $a(s_{t+1}) = \\frac{3}{5}$.\nThe interaction term is $e(s_{t+1})c(s_{t+1}) = \\frac{4}{3} \\times \\frac{2}{5} = \\frac{8}{15}$.\nThe feature vector $\\phi(s_{t+1})$ is:\n$$\n\\phi(s_{t+1}) = \\begin{pmatrix} 1 \\\\ \\frac{4}{3} \\\\ \\frac{2}{5} \\\\ \\frac{3}{5} \\\\ \\frac{8}{15} \\end{pmatrix}\n$$\n\nNext, we compute the state-value estimates, $\\hat{V}(s_t; \\theta_t)$ and $\\hat{V}(s_{t+1}; \\theta_t)$, using the given parameter vector $\\theta_t$.\n\nThe value estimate for $s_t$ is:\n$$\n\\hat{V}(s_t; \\theta_t) = \\theta_t^{\\top}\\phi(s_t) = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{5}  \\frac{2}{3}  \\frac{1}{4}  -\\frac{3}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ \\frac{1}{5} \\\\ \\frac{7}{10} \\\\ \\frac{3}{10} \\end{pmatrix}\n$$\n$$\n\\hat{V}(s_t; \\theta_t) = \\left(\\frac{1}{2}\\right)(1) + \\left(-\\frac{1}{5}\\right)\\left(\\frac{3}{2}\\right) + \\left(\\frac{2}{3}\\right)\\left(\\frac{1}{5}\\right) + \\left(\\frac{1}{4}\\right)\\left(\\frac{7}{10}\\right) + \\left(-\\frac{3}{10}\\right)\\left(\\frac{3}{10}\\right)\n$$\n$$\n\\hat{V}(s_t; \\theta_t) = \\frac{1}{2} - \\frac{3}{10} + \\frac{2}{15} + \\frac{7}{40} - \\frac{9}{100}\n$$\nTo sum these fractions, we find a common denominator. The least common multiple of $2$, $10$, $15$, $40$, and $100$ is $600$.\n$$\n\\hat{V}(s_t; \\theta_t) = \\frac{300}{600} - \\frac{180}{600} + \\frac{80}{600} + \\frac{105}{600} - \\frac{54}{600} = \\frac{300 - 180 + 80 + 105 - 54}{600} = \\frac{251}{600}\n$$\n\nThe value estimate for $s_{t+1}$ is:\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\theta_t^{\\top}\\phi(s_{t+1}) = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{5}  \\frac{2}{3}  \\frac{1}{4}  -\\frac{3}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{4}{3} \\\\ \\frac{2}{5} \\\\ \\frac{3}{5} \\\\ \\frac{8}{15} \\end{pmatrix}\n$$\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\left(\\frac{1}{2}\\right)(1) + \\left(-\\frac{1}{5}\\right)\\left(\\frac{4}{3}\\right) + \\left(\\frac{2}{3}\\right)\\left(\\frac{2}{5}\\right) + \\left(\\frac{1}{4}\\right)\\left(\\frac{3}{5}\\right) + \\left(-\\frac{3}{10}\\right)\\left(\\frac{8}{15}\\right)\n$$\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{1}{2} - \\frac{4}{15} + \\frac{4}{15} + \\frac{3}{20} - \\frac{24}{150}\n$$\nThe second and third terms cancel. Simplifying the last term gives $\\frac{24}{150} = \\frac{4}{25}$.\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{1}{2} + \\frac{3}{20} - \\frac{4}{25}\n$$\nThe least common multiple of $2$, $20$, and $25$ is $100$.\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{50}{100} + \\frac{15}{100} - \\frac{16}{100} = \\frac{50 + 15 - 16}{100} = \\frac{49}{100}\n$$\n\nFinally, we compute the TD error $\\delta_t$ using its definition.\n$$\n\\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}; \\theta_t) - \\hat{V}(s_t; \\theta_t)\n$$\nSubstituting the given values and our calculated estimates:\n$$\n\\delta_t = \\frac{2}{5} + \\left(\\frac{9}{10}\\right)\\left(\\frac{49}{100}\\right) - \\frac{251}{600}\n$$\nFirst, calculate the TD target, which is the first part of the expression:\n$$\n\\text{TD Target} = \\frac{2}{5} + \\frac{9 \\times 49}{10 \\times 100} = \\frac{2}{5} + \\frac{441}{1000} = \\frac{400}{1000} + \\frac{441}{1000} = \\frac{841}{1000}\n$$\nNow, subtract the value estimate for the current state:\n$$\n\\delta_t = \\frac{841}{1000} - \\frac{251}{600}\n$$\nThe least common multiple of $1000$ and $600$ is $3000$.\n$$\n\\delta_t = \\frac{841 \\times 3}{3000} - \\frac{251 \\times 5}{3000} = \\frac{2523}{3000} - \\frac{1255}{3000} = \\frac{2523 - 1255}{3000} = \\frac{1268}{3000}\n$$\nThis fraction can be simplified by dividing the numerator and denominator by their greatest common divisor. Both are even.\n$$\n\\delta_t = \\frac{1268 \\div 4}{3000 \\div 4} = \\frac{317}{750}\n$$\nThe number $317$ is prime, and the prime factors of $750$ are $2 \\cdot 3 \\cdot 5^3$. Thus, the fraction is in its simplest form.\nThe temporal-difference error is $\\frac{317}{750}$.",
            "answer": "$$\n\\boxed{\\frac{317}{750}}\n$$"
        },
        {
            "introduction": "Deploying reinforcement learning in real-world control and design, particularly in multiscale systems, requires ensuring that actions are not only effective but also safe. This final practice challenges you to design a safety filter, a critical component that integrates with an RL agent to guarantee constraint satisfaction . By formulating and solving a quadratic program to find the closest safe action, you will engage with an advanced technique that bridges the gap between learning-based policies and the rigorous demands of safe engineering design.",
            "id": "3801990",
            "problem": "Design and implement a safety filter for Reinforcement Learning (RL) that corrects a proposed action by solving a strictly convex quadratic program derived from first principles. The safety filter must project a proposed action $u \\in \\mathbb{R}^m$ to the closest feasible action $u' \\in \\mathbb{R}^m$ that satisfies linearized control and state constraints arising from multiscale dynamics, with feasibility defined in terms of the next-step macro-scale safety constraints.\n\nStart from the following fundamental bases:\n- For a discrete-time controlled system obtained from a first-order forward Euler discretization of a continuous-time model, the next state satisfies $x^{+} \\approx x + B u'$, where $x \\in \\mathbb{R}^n$ is the current state, $u' \\in \\mathbb{R}^m$ is the (to-be-corrected) action, and $B \\in \\mathbb{R}^{n \\times m}$ is the input Jacobian (i.e., the partial derivative of the dynamics with respect to the control, multiplied by the time step).\n- Macro-scale safety constraints are modeled as linear inequalities on the next state, $H x^{+} \\le h$, where $H \\in \\mathbb{R}^{p \\times n}$ and $h \\in \\mathbb{R}^p$. Linearization around the current state and the proposed action yields a linear constraint on $u'$: $H (x + B u') \\le h$, or equivalently $(H B) u' \\le h - H x$.\n- The control must also obey component-wise box constraints $L \\le u' \\le U$, where $L \\in \\mathbb{R}^m$ and $U \\in \\mathbb{R}^m$.\n\nThe safety filter must return the action $u'$ that solves the quadratic program\nminimize over $u' \\in \\mathbb{R}^m$: $\\tfrac{1}{2} \\lVert u' - u \\rVert_2^2$ subject to $A u' \\le b$,\nwhere the constraint matrix $A \\in \\mathbb{R}^{q \\times m}$ and vector $b \\in \\mathbb{R}^q$ stack:\n- the linearized next-state safety constraints $(H B) u' \\le h - H x$, and\n- the box constraints written as inequalities $u' \\le U$ and $-u' \\le -L$.\n\nYour program must:\n1. Derive from the Lagrangian and Karush–Kuhn–Tucker (KKT) conditions a dual problem suitable for algorithmic solution using projected gradient descent on the nonnegative orthant of Lagrange multipliers, and recover the primal solution $u'$ from the optimal dual variables.\n2. Implement a solver that takes $u$, $A$, and $b$, and returns the corrected action $u'$ that solves the quadratic program.\n3. Construct the constraints from the provided data for each test case, and apply the solver to compute $u'$.\n\nUse the following test suite. For each case, $m = 2$ and $n = 2$. Unless otherwise stated, take $H = I_2$ (the $2 \\times 2$ identity). Provide all answers as plain numerical lists; no physical units are involved.\n\n- Test Case $1$ (happy path, interior feasible):\n  - Proposed action $u = [\\,0.5,\\,0.5\\,]$.\n  - Current state $x = [\\,0.2,\\,0.1\\,]$.\n  - Input Jacobian $B = \\begin{bmatrix} 0.5  0.2 \\\\ 0.0  0.8 \\end{bmatrix}$.\n  - Safety thresholds $h = [\\,1.0,\\,0.7\\,]$.\n  - Box bounds $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$.\n\n- Test Case $2$ (violates a state constraint, requires minimal adjustment):\n  - Proposed action $u = [\\,1.0,\\,1.0\\,]$.\n  - Current state $x = [\\,0.2,\\,0.1\\,]$.\n  - Input Jacobian $B = \\begin{bmatrix} 0.5  0.2 \\\\ 0.0  0.8 \\end{bmatrix}$.\n  - Safety thresholds $h = [\\,1.0,\\,0.7\\,]$.\n  - Box bounds $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$.\n\n- Test Case $3$ (tighter safety, coupled constraint active):\n  - Proposed action $u = [\\,0.9,\\,0.9\\,]$.\n  - Current state $x = [\\,0.5,\\,0.5\\,]$.\n  - Input Jacobian $B = \\begin{bmatrix} 0.5  0.2 \\\\ 0.0  0.8 \\end{bmatrix}$.\n  - Safety thresholds $h = [\\,1.0,\\,0.7\\,]$.\n  - Box bounds $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$.\n\n- Test Case $4$ (violates only control bounds; state constraints inactive):\n  - Proposed action $u = [\\,1.5,\\,-1.5\\,]$.\n  - Current state $x = [\\,0.0,\\,0.0\\,]$.\n  - Input Jacobian $B = \\begin{bmatrix} 0.5  0.2 \\\\ 0.0  0.8 \\end{bmatrix}$.\n  - Safety thresholds $h = [\\,10.0,\\,10.0\\,]$.\n  - Box bounds $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$.\n\n- Test Case $5$ (redundant constraints; projection under duplicates):\n  - Proposed action $u = [\\,0.3,\\,0.3\\,]$.\n  - Current state $x = [\\,0.0,\\,0.0\\,]$.\n  - Input Jacobian $B = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$.\n  - Safety matrix $H = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\\\ 0.0  1.0 \\end{bmatrix}$.\n  - Safety thresholds $h = [\\,0.2,\\,0.2,\\,0.2\\,]$.\n  - Box bounds $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the corrected action for the corresponding test case represented as a bracketed list of two floating-point numbers. For example: \"[[a1,a2],[b1,b2],...]\" with numeric values. Round each component to no more than six decimal places in the printed output. The final answers are pure numbers without units or angle measures, and no percentages should be used.",
            "solution": "The problem is to design a safety filter for a reinforcement learning agent. This filter takes a proposed control action $u$ and projects it onto a safety-certified feasible set, yielding a corrected action $u'$. The feasible set is defined by linear constraints on the control action, derived from linearized system dynamics and a set of macro-scale safety requirements. The projection is defined as the point in the feasible set that is closest to the proposed action $u$ in the Euclidean norm sense. This is a classic strictly convex Quadratic Program (QP).\n\nThe primal problem is formulated as:\n$$\n\\underset{u' \\in \\mathbb{R}^m}{\\text{minimize}} \\quad \\frac{1}{2} \\lVert u' - u \\rVert_2^2\n$$\n$$\n\\text{subject to} \\quad A u' \\le b\n$$\nwhere $u \\in \\mathbb{R}^m$ is the proposed action, $u' \\in \\mathbb{R}^m$ is the corrected action, and the inequality constraints $A u' \\le b$ represent the feasible set. The matrix $A \\in \\mathbb{R}^{q \\times m}$ and vector $b \\in \\mathbb{R}^q$ are constructed by stacking the linearized state safety constraints, $(HB) u' \\le h - Hx$, and the control input box constraints, $Iu' \\le U$ and $-Iu' \\le -L$.\n\nWe will solve this QP by deriving and solving its Lagrangian dual. This approach is powerful, especially when the number of constraints $q$ is large, and it transforms the problem into a QP with simpler non-negativity constraints, which can be efficiently solved using projected gradient descent.\n\nFirst, we form the Lagrangian by introducing the dual variables (Lagrange multipliers) $\\lambda \\in \\mathbb{R}^q$, with $\\lambda \\ge 0$:\n$$\n\\mathcal{L}(u', \\lambda) = \\frac{1}{2} \\lVert u' - u \\rVert_2^2 + \\lambda^T (A u' - b)\n$$\nFor an optimal primal-dual pair $(u'^*, \\lambda^*)$, the Karush-Kuhn-Tucker (KKT) conditions must be satisfied. The stationarity condition requires the gradient of the Lagrangian with respect to the primal variable $u'$ to be zero:\n$$\n\\nabla_{u'} \\mathcal{L}(u'^*, \\lambda^*) = (u'^* - u) + A^T \\lambda^* = 0\n$$\nFrom this, we can express the optimal primal solution $u'^*$ in terms of the optimal dual variable $\\lambda^*$:\n$$\nu'^*(\\lambda^*) = u - A^T \\lambda^*\n$$\n\nNext, we formulate the Lagrange dual function $g(\\lambda)$ by minimizing the Lagrangian over $u'$ for a fixed $\\lambda$:\n$$\ng(\\lambda) = \\inf_{u' \\in \\mathbb{R}^m} \\mathcal{L}(u', \\lambda)\n$$\nSubstituting the expression for $u'(\\lambda)$ from the stationarity condition into the Lagrangian, we get:\n$$\ng(\\lambda) = \\frac{1}{2} \\lVert (u - A^T \\lambda) - u \\rVert_2^2 + \\lambda^T (A(u - A^T \\lambda) - b)\n$$\n$$\ng(\\lambda) = \\frac{1}{2} \\lVert -A^T \\lambda \\rVert_2^2 + \\lambda^T A u - \\lambda^T A A^T \\lambda - \\lambda^T b\n$$\n$$\ng(\\lambda) = \\frac{1}{2} \\lambda^T A A^T \\lambda - \\lambda^T A A^T \\lambda + \\lambda^T A u - \\lambda^T b\n$$\n$$\ng(\\lambda) = -\\frac{1}{2} \\lambda^T (A A^T) \\lambda - \\lambda^T (b - A u)\n$$\n\nThe dual problem is to maximize $g(\\lambda)$ subject to the dual feasibility constraint $\\lambda \\ge 0$. Maximizing $g(\\lambda)$ is equivalent to minimizing $-g(\\lambda)$. Let this dual objective function be $\\mathcal{J}(\\lambda)$:\n$$\n\\underset{\\lambda \\in \\mathbb{R}^q}{\\text{minimize}} \\quad \\mathcal{J}(\\lambda) = \\frac{1}{2} \\lambda^T (A A^T) \\lambda + \\lambda^T (b - A u)\n$$\n$$\n\\text{subject to} \\quad \\lambda \\ge 0\n$$\nThis is a convex QP with simple non-negativity constraints, making it suitable for a projected gradient descent algorithm. The gradient of the dual objective $\\mathcal{J}(\\lambda)$ with respect to $\\lambda$ is:\n$$\n\\nabla_\\lambda \\mathcal{J}(\\lambda) = (A A^T) \\lambda + (b - A u)\n$$\nThe projected gradient descent algorithm iteratively updates $\\lambda$ to minimize $\\mathcal{J}(\\lambda)$. At each iteration $k$, the update rule is:\n$$\n\\lambda_{k+1} = \\Pi_{\\mathbb{R}^q_+} \\left( \\lambda_k - \\alpha \\nabla_\\lambda \\mathcal{J}(\\lambda_k) \\right)\n$$\nwhere $\\alpha  0$ is a step size, and $\\Pi_{\\mathbb{R}^q_+}$ is the projection onto the non-negative orthant, which is simply an element-wise $\\max(0, \\cdot)$ operation. The step size $\\alpha$ can be chosen as $\\alpha = 1/L$, where $L$ is the Lipschitz constant of the gradient $\\nabla_\\lambda \\mathcal{J}(\\lambda)$, which is the largest eigenvalue of the positive-semidefinite matrix $A A^T$.\n\nThe overall algorithm is as follows:\n$1$. For each test case, assemble the constraint matrix $A$ and vector $b$ from the provided system parameters $x, B, H, h, L, U$.\n$2$. Initialize the dual variable $\\lambda_0 = 0$.\n$3$. Iteratively update $\\lambda_k$ using the projected gradient descent rule until convergence.\n$4$. Once the optimal dual variable $\\lambda^*$ is found, recover the optimal primal solution $u'^*$ using the relationship derived from the stationarity condition: $u'^* = u - A^T \\lambda^*$.\nThis procedure yields the corrected action $u'$ that is closest to the proposed action $u$ while satisfying all safety and control constraints.",
            "answer": "```python\nimport numpy as np\n\ndef solve_qp_dual_pgd(u, A, b, n_iter=5000, tol=1e-12):\n    \"\"\"\n    Solves a strictly convex QP using projected gradient descent on the dual problem.\n    Primal problem: min 0.5 * ||u' - u||^2_2 s.t. A u' = b.\n    \n    Args:\n        u (np.ndarray): The vector to be projected, shape (m,).\n        A (np.ndarray): The constraint matrix, shape (q, m).\n        b (np.ndarray): The constraint vector, shape (q,).\n        n_iter (int): Maximum number of iterations for gradient descent.\n        tol (float): Tolerance for convergence.\n    \n    Returns:\n        np.ndarray: The optimal primal solution u', shape (m,).\n    \"\"\"\n    # Precompute matrices for the dual problem.\n    # The dual objective is J(lambda) = 0.5 * lambda.T @ Qd @ lambda + cd.T @ lambda\n    AT = A.T\n    Qd = A @ AT\n    cd = b - (A @ u)\n\n    # Determine step size alpha from the Lipschitz constant of the gradient.\n    # The Lipschitz constant is the largest eigenvalue of Qd = A @ A.T.\n    try:\n        # np.linalg.eigvalsh is for Hermitian (or real symmetric) matrices.\n        # It's more efficient and numerically stable for this case.\n        all_eigenvalues = np.linalg.eigvalsh(Qd)\n        # Check if there are any eigenvalues before taking max\n        L_grad = np.max(all_eigenvalues) if len(all_eigenvalues)  0 else 0.0\n    except np.linalg.LinAlgError:\n        # Fallback to spectral norm if eigvalsh fails, though unlikely.\n        L_grad = np.linalg.norm(Qd, ord=2)\n\n    # A stable and effective step size is 1/L.\n    if L_grad  1e-9:\n        alpha = 1.0\n    else:\n        alpha = 1.0 / L_grad\n\n    # Initialize dual variable lambda\n    q_dim = A.shape[0]\n    Lambda = np.zeros(q_dim)\n\n    # Projected gradient descent loop\n    for _ in range(n_iter):\n        # Calculate gradient of the dual objective\n        grad = Qd @ Lambda + cd\n        \n        # Update lambda\n        Lambda_new = Lambda - alpha * grad\n        \n        # Project lambda onto the non-negative orthant\n        Lambda_new = np.maximum(0., Lambda_new)\n        \n        # Check for convergence\n        if np.linalg.norm(Lambda_new - Lambda)  tol:\n            Lambda = Lambda_new\n            break\n        Lambda = Lambda_new\n        \n    # Recover the primal solution u' from the optimal dual variable lambda\n    u_prime = u - (AT @ Lambda)\n    return u_prime\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results in the required format.\n    \"\"\"\n    test_cases = [\n        # Test Case 1 (happy path, interior feasible)\n        {\n            \"u\": np.array([0.5, 0.5]), \"x\": np.array([0.2, 0.1]),\n            \"B\": np.array([[0.5, 0.2], [0.0, 0.8]]), \"H\": np.eye(2),\n            \"h\": np.array([1.0, 0.7]), \"L\": np.array([-1.0, -1.0]), \"U\": np.array([1.0, 1.0])\n        },\n        # Test Case 2 (violates a state constraint)\n        {\n            \"u\": np.array([1.0, 1.0]), \"x\": np.array([0.2, 0.1]),\n            \"B\": np.array([[0.5, 0.2], [0.0, 0.8]]), \"H\": np.eye(2),\n            \"h\": np.array([1.0, 0.7]), \"L\": np.array([-1.0, -1.0]), \"U\": np.array([1.0, 1.0])\n        },\n        # Test Case 3 (tighter safety, coupled constraint active)\n        {\n            \"u\": np.array([0.9, 0.9]), \"x\": np.array([0.5, 0.5]),\n            \"B\": np.array([[0.5, 0.2], [0.0, 0.8]]), \"H\": np.eye(2),\n            \"h\": np.array([1.0, 0.7]), \"L\": np.array([-1.0, -1.0]), \"U\": np.array([1.0, 1.0])\n        },\n        # Test Case 4 (violates only control bounds)\n        {\n            \"u\": np.array([1.5, -1.5]), \"x\": np.array([0.0, 0.0]),\n            \"B\": np.array([[0.5, 0.2], [0.0, 0.8]]), \"H\": np.eye(2),\n            \"h\": np.array([10.0, 10.0]), \"L\": np.array([-1.0, -1.0]), \"U\": np.array([1.0, 1.0])\n        },\n        # Test Case 5 (redundant constraints)\n        {\n            \"u\": np.array([0.3, 0.3]), \"x\": np.array([0.0, 0.0]),\n            \"B\": np.eye(2), \"H\": np.array([[1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]),\n            \"h\": np.array([0.2, 0.2, 0.2]), \"L\": np.array([-1.0, -1.0]), \"U\": np.array([1.0, 1.0])\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        u, x, B, H, h, L, U = params[\"u\"], params[\"x\"], params[\"B\"], params[\"H\"], params[\"h\"], params[\"L\"], params[\"U\"]\n        \n        m = u.shape[0]\n\n        # Construct the aggregated constraint matrix A and vector b\n        # A u' = b\n        \n        # State constraints: (H B) u' = h - H x\n        A_state = H @ B\n        b_state = h - (H @ x)\n        \n        # Control upper-bound constraints: I u' = U\n        A_ctrl_up = np.eye(m)\n        b_ctrl_up = U\n        \n        # Control lower-bound constraints: -I u' = -L\n        A_ctrl_low = -np.eye(m)\n        b_ctrl_low = -L\n        \n        # Stack all constraints\n        A = np.vstack([A_state, A_ctrl_up, A_ctrl_low])\n        b = np.concatenate([b_state, b_ctrl_up, b_ctrl_low])\n        \n        # Solve the QP to find the corrected action u_prime\n        u_prime = solve_qp_dual_pgd(u, A, b)\n        \n        # Round the result to 6 decimal places and append\n        rounded_result = [round(val, 6) for val in u_prime]\n        results.append(str(rounded_result))\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}