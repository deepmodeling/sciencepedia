{
    "hands_on_practices": [
        {
            "introduction": "本练习聚焦于求解马尔可夫决策过程（MDP）的动态规划核心：贝尔曼最优算子。通过手动执行一步价值迭代，您将对价值函数如何迭代改进获得具体的理解。在基于模型的强化学习中，此计算是求解最优策略算法的基础构件。",
            "id": "3802027",
            "problem": "考虑一个马尔可夫决策过程 (MDP)，其状态空间为 $\\mathcal{S} = \\{s_1, s_2, s_3\\}$，动作空间为 $\\mathcal{A} = \\{a, b\\}$。该 MDP 代表了一个用于多尺度材料设计的粗粒度控制模型，其中每个状态 $s_i$ 概括了一个微观结构体系。设转移核由概率 $P(s' \\mid s, a)$ 定义，即时奖励由 $R(s, a)$ 给出，折扣因子为 $\\gamma \\in (0,1)$。对于此 MDP，其各组成部分规定如下：\n- 从 $s_1$ 的转移：\n  - $P(s_1 \\mid s_1, a) = \\frac{1}{2}$, $P(s_2 \\mid s_1, a) = \\frac{1}{4}$, $P(s_3 \\mid s_1, a) = \\frac{1}{4}$；\n  - $P(s_1 \\mid s_1, b) = 0$, $P(s_2 \\mid s_1, b) = \\frac{3}{4}$, $P(s_3 \\mid s_1, b) = \\frac{1}{4}$。\n- 从 $s_2$ 的转移：\n  - $P(s_1 \\mid s_2, a) = \\frac{1}{3}$, $P(s_2 \\mid s_2, a) = \\frac{1}{3}$, $P(s_3 \\mid s_2, a) = \\frac{1}{3}$；\n  - $P(s_1 \\mid s_2, b) = 0$, $P(s_2 \\mid s_2, b) = 0$, $P(s_3 \\mid s_2, b) = 1$。\n- 从 $s_3$ 的转移：\n  - $P(s_1 \\mid s_3, a) = 0$, $P(s_2 \\mid s_3, a) = 0$, $P(s_3 \\mid s_3, a) = 1$；\n  - $P(s_1 \\mid s_3, b) = \\frac{1}{2}$, $P(s_2 \\mid s_3, b) = \\frac{1}{2}$, $P(s_3 \\mid s_3, b) = 0$。\n即时奖励为：\n- $R(s_1, a) = 1$, $R(s_1, b) = 0$；\n- $R(s_2, a) = 2$, $R(s_2, b) = -1$；\n- $R(s_3, a) = 0$, $R(s_3, b) = 1$。\n设折扣因子为 $\\gamma = \\frac{1}{2}$。给定初始值函数 $V_k$，其中 $V_k(s_1) = 2$, $V_k(s_2) = -1$ 且 $V_k(s_3) = 0$，执行一步值迭代，通过在每个状态上对 $V_k$ 应用 Bellman 最优算子来得到 $V_{k+1}$。然后计算残差范数 $\\|V_{k+1} - V_k\\|_{\\infty}$，其中 $\\|\\cdot\\|_{\\infty}$ 表示由 $\\|x\\|_{\\infty} = \\max_i |x_i|$ 定义的上确界范数。将最终答案表示为一个无舍入的精确有理数。",
            "solution": "该问题设置在用于控制与设计的强化学习 (RL) 框架内，并使用了马尔可夫决策过程 (MDP) 的标准定义。值迭代更新由 Bellman 最优算子决定。从基本定义出发，对于任意值函数 $V$ 和状态 $s \\in \\mathcal{S}$，Bellman 最优算子 $\\mathcal{T}$ 按分量定义为\n$$\n(\\mathcal{T}V)(s) = \\max_{a \\in \\mathcal{A}} \\left[ R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a)\\, V(s') \\right].\n$$\n单步值迭代计算 $V_{k+1} = \\mathcal{T} V_k$。我们使用给定的 $V_k$、转移概率、奖励和折扣因子 $\\gamma = \\frac{1}{2}$，逐个状态地应用此公式。\n\n计算 $V_{k+1}(s_1)$：\n- 对于状态 $s_1$ 下的动作 $a$，\n$$\n\\sum_{s'} P(s' \\mid s_1, a) V_k(s') = \\frac{1}{2} V_k(s_1) + \\frac{1}{4} V_k(s_2) + \\frac{1}{4} V_k(s_3) = \\frac{1}{2} \\cdot 2 + \\frac{1}{4} \\cdot (-1) + \\frac{1}{4} \\cdot 0 = 1 - \\frac{1}{4} + 0 = \\frac{3}{4}.\n$$\n因此，\n$$\nQ(s_1, a) = R(s_1,a) + \\gamma \\sum_{s'} P(s' \\mid s_1, a) V_k(s') = 1 + \\frac{1}{2} \\cdot \\frac{3}{4} = 1 + \\frac{3}{8} = \\frac{11}{8}.\n$$\n- 对于状态 $s_1$ 下的动作 $b$，\n$$\n\\sum_{s'} P(s' \\mid s_1, b) V_k(s') = 0 \\cdot V_k(s_1) + \\frac{3}{4} V_k(s_2) + \\frac{1}{4} V_k(s_3) = 0 + \\frac{3}{4} \\cdot (-1) + \\frac{1}{4} \\cdot 0 = -\\frac{3}{4}.\n$$\n因此，\n$$\nQ(s_1, b) = R(s_1,b) + \\gamma \\sum_{s'} P(s' \\mid s_1, b) V_k(s') = 0 + \\frac{1}{2} \\cdot \\left(-\\frac{3}{4}\\right) = -\\frac{3}{8}.\n$$\n于是，\n$$\nV_{k+1}(s_1) = \\max\\left\\{ \\frac{11}{8}, -\\frac{3}{8} \\right\\} = \\frac{11}{8}.\n$$\n\n计算 $V_{k+1}(s_2)$：\n- 对于状态 $s_2$ 下的动作 $a$，\n$$\n\\sum_{s'} P(s' \\mid s_2, a) V_k(s') = \\frac{1}{3} V_k(s_1) + \\frac{1}{3} V_k(s_2) + \\frac{1}{3} V_k(s_3) = \\frac{1}{3} \\cdot 2 + \\frac{1}{3} \\cdot (-1) + \\frac{1}{3} \\cdot 0 = \\frac{2}{3} - \\frac{1}{3} + 0 = \\frac{1}{3}.\n$$\n因此，\n$$\nQ(s_2, a) = R(s_2,a) + \\gamma \\sum_{s'} P(s' \\mid s_2, a) V_k(s') = 2 + \\frac{1}{2} \\cdot \\frac{1}{3} = 2 + \\frac{1}{6} = \\frac{13}{6}.\n$$\n- 对于状态 $s_2$ 下的动作 $b$，\n$$\n\\sum_{s'} P(s' \\mid s_2, b) V_k(s') = 0 \\cdot V_k(s_1) + 0 \\cdot V_k(s_2) + 1 \\cdot V_k(s_3) = 0,\n$$\n所以\n$$\nQ(s_2, b) = R(s_2,b) + \\gamma \\sum_{s'} P(s' \\mid s_2, b) V_k(s') = -1 + \\frac{1}{2} \\cdot 0 = -1.\n$$\n于是，\n$$\nV_{k+1}(s_2) = \\max\\left\\{ \\frac{13}{6}, -1 \\right\\} = \\frac{13}{6}.\n$$\n\n计算 $V_{k+1}(s_3)$：\n- 对于状态 $s_3$ 下的动作 $a$，\n$$\n\\sum_{s'} P(s' \\mid s_3, a) V_k(s') = 1 \\cdot V_k(s_3) = 0,\n$$\n所以\n$$\nQ(s_3, a) = R(s_3,a) + \\gamma \\cdot 0 = 0.\n$$\n- 对于状态 $s_3$ 下的动作 $b$，\n$$\n\\sum_{s'} P(s' \\mid s_3, b) V_k(s') = \\frac{1}{2} V_k(s_1) + \\frac{1}{2} V_k(s_2) + 0 \\cdot V_k(s_3) = \\frac{1}{2} \\cdot 2 + \\frac{1}{2} \\cdot (-1) + 0 = 1 - \\frac{1}{2} = \\frac{1}{2}.\n$$\n因此，\n$$\nQ(s_3, b) = R(s_3,b) + \\gamma \\sum_{s'} P(s' \\mid s_3, b) V_k(s') = 1 + \\frac{1}{2} \\cdot \\frac{1}{2} = 1 + \\frac{1}{4} = \\frac{5}{4}.\n$$\n于是，\n$$\nV_{k+1}(s_3) = \\max\\left\\{ 0, \\frac{5}{4} \\right\\} = \\frac{5}{4}.\n$$\n\n汇总结果，\n$$\nV_{k+1} = \\left( V_{k+1}(s_1), V_{k+1}(s_2), V_{k+1}(s_3) \\right) = \\left( \\frac{11}{8}, \\frac{13}{6}, \\frac{5}{4} \\right).\n$$\n\n现在计算残差范数 $\\|V_{k+1} - V_k\\|_{\\infty}$。首先，计算各分量的差：\n$$\nV_{k+1}(s_1) - V_k(s_1) = \\frac{11}{8} - 2 = \\frac{11}{8} - \\frac{16}{8} = -\\frac{5}{8},\n$$\n$$\nV_{k+1}(s_2) - V_k(s_2) = \\frac{13}{6} - (-1) = \\frac{13}{6} + 1 = \\frac{13}{6} + \\frac{6}{6} = \\frac{19}{6},\n$$\n$$\nV_{k+1}(s_3) - V_k(s_3) = \\frac{5}{4} - 0 = \\frac{5}{4}.\n$$\n取绝对值：\n$$\n\\left| -\\frac{5}{8} \\right| = \\frac{5}{8}, \\quad \\left| \\frac{19}{6} \\right| = \\frac{19}{6}, \\quad \\left| \\frac{5}{4} \\right| = \\frac{5}{4}.\n$$\n上确界范数是这三个值的最大值：\n$$\n\\|V_{k+1} - V_k\\|_{\\infty} = \\max\\left\\{ \\frac{5}{8}, \\frac{19}{6}, \\frac{5}{4} \\right\\} = \\frac{19}{6}.\n$$\n\n这个残差量化了对 $V_k$ 应用一次 Bellman 最优算子后，值函数在所有状态上的最大点态变化。根据 Bellman 最优算子在上确界范数下的压缩性质，随着迭代的进行，这类残差会减小，引导其收敛到最优值函数，不过更广泛的收敛性分析超出了本次计算的要求。",
            "answer": "$$\\boxed{\\frac{19}{6}}$$"
        },
        {
            "introduction": "本问题将介绍时序差分（TD）学习，这是无模型强化学习的基石，超越了基于模型的方法。您将为一个采用线性函数逼近的系统计算TD误差并执行一次参数更新，从而展示智能体如何在不知道底层系统动力学的情况下直接从样本转移中学习。这项技能对于将强化学习应用于状态空间过大以至于无法使用表格方法的复杂问题至关重要。",
            "id": "3802032",
            "problem": "一个控制器通过强化学习 (RL) 进行训练，以调控一个多尺度材料过程，其粗粒度状态 $s$ 聚合了微观结构统计量。在每个决策时刻 $t$，状态 $s_t$ 由三个可观测量来概括：微观尺度能量密度 $e(s)$、一个晶格间距上的两点相关性 $c(s)$，以及界面面积密度 $a(s)$。采用线性状态价值近似 $\\hat{V}(s;\\theta)$，其特征映射为\n$$\n\\phi(s) \\;=\\; \\begin{pmatrix} 1 \\\\ e(s) \\\\ c(s) \\\\ a(s) \\\\ e(s)\\,c(s) \\end{pmatrix} \\in \\mathbb{R}^{5}, \n$$\n使得 \n$$\n\\hat{V}(s;\\theta) \\;=\\; \\theta^{\\top}\\phi(s),\n$$\n其中 $\\theta \\in \\mathbb{R}^{5}$ 是参数向量。考虑在固定策略下生成的以下单个样本转移 $(s_t,a_t,r_t,s_{t+1})$：\n- 对于 $s_t$：$e(s_t)=\\frac{3}{2}$，$c(s_t)=\\frac{1}{5}$，$a(s_t)=\\frac{7}{10}$。\n- 对于 $s_{t+1}$：$e(s_{t+1})=\\frac{4}{3}$，$c(s_{t+1})=\\frac{2}{5}$，$a(s_{t+1})=\\frac{3}{5}$。\n- 即时奖励为 $r_t=\\frac{2}{5}$。\n- 折扣因子为 $\\gamma=\\frac{9}{10}$。\n\n初始参数为\n$$\n\\theta_t \\;=\\; \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{5} \\\\ \\frac{2}{3} \\\\ \\frac{1}{4} \\\\ -\\frac{3}{10} \\end{pmatrix},\n$$\n学习率为 $\\alpha=\\frac{3}{20}$。\n\n使用零步自举的时序差分 (TD) 学习 $\\mathrm{TD}(0)$ 和上述线性函数近似，对该单个样本在时刻 $t$ 执行一次参数更新，并评估时刻 $t$ 的时序差分误差。将你的最终答案表示为代表时序差分误差的单个精确分数。不要四舍五入。",
            "solution": "该问题要求使用 $\\mathrm{TD}(0)$ 算法计算单个样本更新的时序差分 (TD) 误差。对于状态价值函数近似 $\\hat{V}(s;\\theta)$，在时间步 $t$ 的TD误差（记为 $\\delta_t$）定义为TD目标与状态 $s_t$ 的当前价值估计之间的差值。\n\nTD目标是即时奖励 $r_t$ 与下一状态 $s_{t+1}$ 的折扣价值之和。TD误差的公式是：\n$$\n\\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}; \\theta_t) - \\hat{V}(s_t; \\theta_t)\n$$\n在这里，两个价值估计都使用时刻 $t$ 的参数向量 $\\theta_t$ 计算。价值函数由线性近似 $\\hat{V}(s;\\theta_t) = \\theta_t^{\\top}\\phi(s)$ 给出。\n\n首先，我们使用所提供的状态可观测量构造特征向量 $\\phi(s_t)$ 和 $\\phi(s_{t+1})$。\n\n对于状态 $s_t$：\n$e(s_t) = \\frac{3}{2}$，$c(s_t) = \\frac{1}{5}$，$a(s_t) = \\frac{7}{10}$。\n交互项为 $e(s_t)c(s_t) = \\frac{3}{2} \\times \\frac{1}{5} = \\frac{3}{10}$。\n特征向量 $\\phi(s_t)$ 是：\n$$\n\\phi(s_t) = \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ \\frac{1}{5} \\\\ \\frac{7}{10} \\\\ \\frac{3}{10} \\end{pmatrix}\n$$\n\n对于状态 $s_{t+1}$：\n$e(s_{t+1}) = \\frac{4}{3}$，$c(s_{t+1}) = \\frac{2}{5}$，$a(s_{t+1}) = \\frac{3}{5}$。\n交互项为 $e(s_{t+1})c(s_{t+1}) = \\frac{4}{3} \\times \\frac{2}{5} = \\frac{8}{15}$。\n特征向量 $\\phi(s_{t+1})$ 是：\n$$\n\\phi(s_{t+1}) = \\begin{pmatrix} 1 \\\\ \\frac{4}{3} \\\\ \\frac{2}{5} \\\\ \\frac{3}{5} \\\\ \\frac{8}{15} \\end{pmatrix}\n$$\n\n接下来，我们使用给定的参数向量 $\\theta_t$ 计算状态价值估计 $\\hat{V}(s_t; \\theta_t)$ 和 $\\hat{V}(s_{t+1}; \\theta_t)$。\n\n$s_t$ 的价值估计为：\n$$\n\\hat{V}(s_t; \\theta_t) = \\theta_t^{\\top}\\phi(s_t) = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{5}  \\frac{2}{3}  \\frac{1}{4}  -\\frac{3}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ \\frac{1}{5} \\\\ \\frac{7}{10} \\\\ \\frac{3}{10} \\end{pmatrix}\n$$\n$$\n\\hat{V}(s_t; \\theta_t) = \\left(\\frac{1}{2}\\right)(1) + \\left(-\\frac{1}{5}\\right)\\left(\\frac{3}{2}\\right) + \\left(\\frac{2}{3}\\right)\\left(\\frac{1}{5}\\right) + \\left(\\frac{1}{4}\\right)\\left(\\frac{7}{10}\\right) + \\left(-\\frac{3}{10}\\right)\\left(\\frac{3}{10}\\right)\n$$\n$$\n\\hat{V}(s_t; \\theta_t) = \\frac{1}{2} - \\frac{3}{10} + \\frac{2}{15} + \\frac{7}{40} - \\frac{9}{100}\n$$\n为了将这些分数相加，我们找到一个公分母。$2$，$10$，$15$，$40$ 和 $100$ 的最小公倍数是 $600$。\n$$\n\\hat{V}(s_t; \\theta_t) = \\frac{300}{600} - \\frac{180}{600} + \\frac{80}{600} + \\frac{105}{600} - \\frac{54}{600} = \\frac{300 - 180 + 80 + 105 - 54}{600} = \\frac{251}{600}\n$$\n\n$s_{t+1}$ 的价值估计为：\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\theta_t^{\\top}\\phi(s_{t+1}) = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{5}  \\frac{2}{3}  \\frac{1}{4}  -\\frac{3}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{4}{3} \\\\ \\frac{2}{5} \\\\ \\frac{3}{5} \\\\ \\frac{8}{15} \\end{pmatrix}\n$$\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\left(\\frac{1}{2}\\right)(1) + \\left(-\\frac{1}{5}\\right)\\left(\\frac{4}{3}\\right) + \\left(\\frac{2}{3}\\right)\\left(\\frac{2}{5}\\right) + \\left(\\frac{1}{4}\\right)\\left(\\frac{3}{5}\\right) + \\left(-\\frac{3}{10}\\right)\\left(\\frac{8}{15}\\right)\n$$\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{1}{2} - \\frac{4}{15} + \\frac{4}{15} + \\frac{3}{20} - \\frac{24}{150}\n$$\n第二项和第三项抵消。化简最后一项得到 $\\frac{24}{150} = \\frac{4}{25}$。\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{1}{2} + \\frac{3}{20} - \\frac{4}{25}\n$$\n$2$，$20$ 和 $25$ 的最小公倍数是 $100$。\n$$\n\\hat{V}(s_{t+1}; \\theta_t) = \\frac{50}{100} + \\frac{15}{100} - \\frac{16}{100} = \\frac{50 + 15 - 16}{100} = \\frac{49}{100}\n$$\n\n最后，我们使用其定义计算 TD 误差 $\\delta_t$。\n$$\n\\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}; \\theta_t) - \\hat{V}(s_t; \\theta_t)\n$$\n代入给定的值和我们计算出的估计值：\n$$\n\\delta_t = \\frac{2}{5} + \\left(\\frac{9}{10}\\right)\\left(\\frac{49}{100}\\right) - \\frac{251}{600}\n$$\n首先，计算 TD 目标，即表达式的第一部分：\n$$\n\\text{TD Target} = \\frac{2}{5} + \\frac{9 \\times 49}{10 \\times 100} = \\frac{2}{5} + \\frac{441}{1000} = \\frac{400}{1000} + \\frac{441}{1000} = \\frac{841}{1000}\n$$\n现在，减去当前状态的价值估计：\n$$\n\\delta_t = \\frac{841}{1000} - \\frac{251}{600}\n$$\n$1000$ 和 $600$ 的最小公倍数是 $3000$。\n$$\n\\delta_t = \\frac{841 \\times 3}{3000} - \\frac{251 \\times 5}{3000} = \\frac{2523}{3000} - \\frac{1255}{3000} = \\frac{2523 - 1255}{3000} = \\frac{1268}{3000}\n$$\n这个分数可以通过将分子和分母除以它们的最大公约数来化简。两者都是偶数。\n$$\n\\delta_t = \\frac{1268 \\div 4}{3000 \\div 4} = \\frac{317}{750}\n$$\n数字 $317$ 是一个质数，而 $750$ 的质因数是 $2 \\cdot 3 \\cdot 5^3$。因此，该分数已是最简形式。\n时序差分误差是 $\\frac{317}{750}$。",
            "answer": "$$\n\\boxed{\\frac{317}{750}}\n$$"
        },
        {
            "introduction": "现实世界的控制系统通常有严格的安全约束，强化学习智能体绝不能违反。这个高级实践将指导您设计一个“安全滤波器”，它通过求解一个二次规划问题，将一个潜在不安全的动作投影到最近的安全替代方案上。此练习将强化学习与优化理论联系起来，对于为设计应用开发稳健可靠的控制器至关重要。",
            "id": "3801990",
            "problem": "设计并实现一个用于强化学习（RL）的安全滤波器，该滤波器通过求解一个从第一性原理推导出的严格凸二次规划问题来校正一个提议的动作。该安全滤波器必须将提议的动作 $u \\in \\mathbb{R}^m$ 投影到最近的可行点 $u' \\in \\mathbb{R}^m$ 上，该可行点需满足由多尺度动力学产生的线性化控制和状态约束，其可行性根据下一步的宏观尺度安全约束来定义。\n\n从以下基本出发点开始：\n- 对于一个由连续时间模型经过一阶前向欧拉离散化得到的离散时间控制系统，其下一状态满足 $x^{+} \\approx x + B u'$，其中 $x \\in \\mathbb{R}^n$ 是当前状态， $u' \\in \\mathbb{R}^m$ 是（待校正的）动作，$B \\in \\mathbb{R}^{n \\times m}$ 是输入雅可比矩阵（即动力学关于控制的偏导数乘以时间步长）。\n- 宏观尺度的安全约束被建模为关于下一状态的线性不等式，$H x^{+} \\le h$，其中 $H \\in \\mathbb{R}^{p \\times n}$ 且 $h \\in \\mathbb{R}^p$。在当前状态和提议的动作周围进行线性化，可以得到一个关于 $u'$ 的线性约束：$H (x + B u') \\le h$，等价于 $(H B) u' \\le h - H x$。\n- 控制还必须服从分量形式的箱式约束 $L \\le u' \\le U$，其中 $L \\in \\mathbb{R}^m$ 且 $U \\in \\mathbb{R}^m$。\n\n该安全滤波器必须返回解出以下二次规划问题的动作 $u'$\n在 $u' \\in \\mathbb{R}^m$ 上最小化：$\\tfrac{1}{2} \\lVert u' - u \\rVert_2^2$ 约束于 $A u' \\le b$，\n其中约束矩阵 $A \\in \\mathbb{R}^{q \\times m}$ 和向量 $b \\in \\mathbb{R}^q$ 叠加了以下约束：\n- 线性化的下一状态安全约束 $(H B) u' \\le h - H x$，以及\n- 写成不等式形式的箱式约束 $u' \\le U$ 和 $-u' \\le -L$。\n\n您的程序必须：\n1. 从拉格朗日函数和 Karush–Kuhn–Tucker (KKT) 条件推导出一个对偶问题，该问题适合使用在拉格朗日乘子的非负象限上的投影梯度下降法进行算法求解，并从最优对偶变量中恢复原始解 $u'$。\n2. 实现一个求解器，它接受 $u$、$A$ 和 $b$ 作为输入，并返回解出该二次规划问题的校正后动作 $u'$。\n3. 根据每个测试用例提供的数据构造约束，并应用求解器计算 $u'$。\n\n使用以下测试套件。对于每个案例，$m = 2$ 且 $n = 2$。除非另有说明，取 $H = I_2$（$2 \\times 2$ 的单位矩阵）。所有答案均以纯数字列表形式提供；不涉及物理单位。\n\n- 测试用例 1（理想情况，内部可行）：\n  - 提议的动作 $u = [\\,0.5,\\,0.5\\,]$。\n  - 当前状态 $x = [\\,0.2,\\,0.1\\,]$。\n  - 输入雅可比矩阵 $B = \\begin{bmatrix} 0.5  0.2 \\\\ 0.0  0.8 \\end{bmatrix}$。\n  - 安全阈值 $h = [\\,1.0,\\,0.7\\,]$。\n  - 箱式边界 $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$。\n\n- 测试用例 2（违反状态约束，需要最小调整）：\n  - 提议的动作 $u = [\\,1.0,\\,1.0\\,]$。\n  - 当前状态 $x = [\\,0.2,\\,0.1\\,]$。\n  - 输入雅可比矩阵 $B = \\begin{bmatrix} 0.5  0.2 \\\\ 0.0  0.8 \\end{bmatrix}$。\n  - 安全阈值 $h = [\\,1.0,\\,0.7\\,]$。\n  - 箱式边界 $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$。\n\n- 测试用例 3（更严格的安全约束，耦合约束激活）：\n  - 提议的动作 $u = [\\,0.9,\\,0.9\\,]$。\n  - 当前状态 $x = [\\,0.5,\\,0.5\\,]$。\n  - 输入雅可比矩阵 $B = \\begin{bmatrix} 0.5  0.2 \\\\ 0.0  0.8 \\end{bmatrix}$。\n  - 安全阈值 $h = [\\,1.0,\\,0.7\\,]$。\n  - 箱式边界 $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$。\n\n- 测试用例 4（仅违反控制边界；状态约束未激活）：\n  - 提议的动作 $u = [\\,1.5,\\,-1.5\\,]$。\n  - 当前状态 $x = [\\,0.0,\\,0.0\\,]$。\n  - 输入雅可比矩阵 $B = \\begin{bmatrix} 0.5  0.2 \\\\ 0.0  0.8 \\end{bmatrix}$。\n  - 安全阈值 $h = [\\,10.0,\\,10.0\\,]$。\n  - 箱式边界 $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$。\n\n- 测试用例 5（冗余约束；在重复约束下的投影）：\n  - 提议的动作 $u = [\\,0.3,\\,0.3\\,]$。\n  - 当前状态 $x = [\\,0.0,\\,0.0\\,]$。\n  - 输入雅可比矩阵 $B = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$。\n  - 安全矩阵 $H = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\\\ 0.0  1.0 \\end{bmatrix}$。\n  - 安全阈值 $h = [\\,0.2,\\,0.2,\\,0.2\\,]$。\n  - 箱式边界 $L = [\\,-1.0,\\,-1.0\\,],\\ U = [\\,1.0,\\,1.0\\,]$。\n\n您的程序应生成单行输出，其中包含结果，格式为方括号括起来的逗号分隔列表，其中每个元素是对应测试用例的校正后动作，表示为包含两个浮点数的方括号列表。例如：“[[a1,a2],[b1,b2],...]”，其中为数值。在打印输出中，将每个分量四舍五入到最多六位小数。最终答案是纯数字，没有单位或角度度量，且不应使用百分比。",
            "solution": "该问题旨在为强化学习智能体设计一个安全滤波器。此滤波器接收一个提议的控制动作 $u$，并将其投影到一个经过安全认证的可行集上，从而产生一个校正后的动作 $u'$。该可行集由控制动作上的线性约束定义，这些约束源于线性化的系统动力学和一组宏观尺度的安全要求。投影被定义为可行集中在欧几里得范数意义下距离提议动作 $u$ 最近的点。这是一个经典的严格凸二次规划（QP）问题。\n\n原始问题公式化为：\n$$\n\\underset{u' \\in \\mathbb{R}^m}{\\text{minimize}} \\quad \\frac{1}{2} \\lVert u' - u \\rVert_2^2\n$$\n$$\n\\text{subject to} \\quad A u' \\le b\n$$\n其中 $u \\in \\mathbb{R}^m$ 是提议的动作，$u' \\in \\mathbb{R}^m$ 是校正后的动作，不等式约束 $A u' \\le b$ 代表可行集。矩阵 $A \\in \\mathbb{R}^{q \\times m}$ 和向量 $b \\in \\mathbb{R}^q$ 是通过叠加线性化的状态安全约束 $(HB) u' \\le h - Hx$ 和控制输入箱式约束 $Iu' \\le U$ 与 $-Iu' \\le -L$ 构建的。\n\n我们将通过推导并求解其拉格朗日对偶问题来解决这个 QP 问题。这种方法非常有效，特别是当约束数量 $q$ 很大时，它能将问题转化为一个具有更简单的非负性约束的 QP 问题，该问题可以使用投影梯度下降法高效求解。\n\n首先，我们通过引入对偶变量（拉格朗日乘子）$\\lambda \\in \\mathbb{R}^q$（其中 $\\lambda \\ge 0$）来构造拉格朗日函数：\n$$\n\\mathcal{L}(u', \\lambda) = \\frac{1}{2} \\lVert u' - u \\rVert_2^2 + \\lambda^T (A u' - b)\n$$\n对于一个最优的原始-对偶对 $(u'^*, \\lambda^*)$，必须满足 Karush-Kuhn-Tucker (KKT) 条件。平稳性条件要求拉格朗日函数关于原始变量 $u'$ 的梯度为零：\n$$\n\\nabla_{u'} \\mathcal{L}(u'^*, \\lambda^*) = (u'^* - u) + A^T \\lambda^* = 0\n$$\n由此，我们可以用最优对偶变量 $\\lambda^*$ 来表示最优原始解 $u'^*$：\n$$\nu'^*(\\lambda^*) = u - A^T \\lambda^*\n$$\n\n接下来，我们通过对固定 $\\lambda$ 的拉格朗日函数关于 $u'$ 进行最小化，来构建拉格朗日对偶函数 $g(\\lambda)$：\n$$\ng(\\lambda) = \\inf_{u' \\in \\mathbb{R}^m} \\mathcal{L}(u', \\lambda)\n$$\n将平稳性条件中得到的 $u'(\\lambda)$ 表达式代入拉格朗日函数，我们得到：\n$$\ng(\\lambda) = \\frac{1}{2} \\lVert (u - A^T \\lambda) - u \\rVert_2^2 + \\lambda^T (A(u - A^T \\lambda) - b)\n$$\n$$\ng(\\lambda) = \\frac{1}{2} \\lVert -A^T \\lambda \\rVert_2^2 + \\lambda^T A u - \\lambda^T A A^T \\lambda - \\lambda^T b\n$$\n$$\ng(\\lambda) = \\frac{1}{2} \\lambda^T A A^T \\lambda - \\lambda^T A A^T \\lambda + \\lambda^T A u - \\lambda^T b\n$$\n$$\ng(\\lambda) = -\\frac{1}{2} \\lambda^T (A A^T) \\lambda - \\lambda^T (b - A u)\n$$\n\n对偶问题是在对偶可行性约束 $\\lambda \\ge 0$ 下最大化 $g(\\lambda)$。最大化 $g(\\lambda)$ 等价于最小化 $-g(\\lambda)$。设该对偶目标函数为 $\\mathcal{J}(\\lambda)$：\n$$\n\\underset{\\lambda \\in \\mathbb{R}^q}{\\text{minimize}} \\quad \\mathcal{J}(\\lambda) = \\frac{1}{2} \\lambda^T (A A^T) \\lambda + \\lambda^T (b - A u)\n$$\n$$\n\\text{subject to} \\quad \\lambda \\ge 0\n$$\n这是一个带有简单非负性约束的凸 QP 问题，使其适合采用投影梯度下降算法求解。对偶目标函数 $\\mathcal{J}(\\lambda)$ 关于 $\\lambda$ 的梯度是：\n$$\n\\nabla_\\lambda \\mathcal{J}(\\lambda) = (A A^T) \\lambda + (b - A u)\n$$\n投影梯度下降算法通过迭代更新 $\\lambda$ 来最小化 $\\mathcal{J}(\\lambda)$。在每次迭代 $k$ 中，更新规则为：\n$$\n\\lambda_{k+1} = \\Pi_{\\mathbb{R}^q_+} \\left( \\lambda_k - \\alpha \\nabla_\\lambda \\mathcal{J}(\\lambda_k) \\right)\n$$\n其中 $\\alpha > 0$ 是步长，$\\Pi_{\\mathbb{R}^q_+}$ 是到非负象限的投影，这只是一个逐元素的 $\\max(0, \\cdot)$ 操作。步长 $\\alpha$ 可以选择为 $\\alpha = 1/L$，其中 $L$ 是梯度 $\\nabla_\\lambda \\mathcal{J}(\\lambda)$ 的 Lipschitz 常数，也就是半正定矩阵 $A A^T$ 的最大特征值。\n\n整体算法如下：\n1. 对于每个测试用例，根据提供的系统参数 $x, B, H, h, L, U$ 组装约束矩阵 $A$ 和向量 $b$。\n2. 初始化对偶变量 $\\lambda_0 = 0$。\n3. 使用投影梯度下降规则迭代更新 $\\lambda_k$ 直至收敛。\n4. 一旦找到最优对偶变量 $\\lambda^*$，就使用从平稳性条件推导出的关系式 $u'^* = u - A^T \\lambda^*$ 来恢复最优原始解 $u'^*$。\n此过程得出的校正后动作 $u'$ 是在满足所有安全和控制约束的同时，最接近提议动作 $u$ 的动作。",
            "answer": "[[0.5, 0.5],[0.710345, 0.75],[0.627586, 0.25],[1.0, -1.0],[0.2, 0.2]]"
        }
    ]
}