{
    "hands_on_practices": [
        {
            "introduction": "A fundamental task in multiscale modeling is projecting information from a high-resolution (fine) grid onto a low-resolution (coarse) one. This exercise establishes the connection between an intuitive averaging process, known as restriction, and the formal concept of an orthogonal projection. By deriving the projection operator from first principles, you will gain a concrete understanding of how restriction and its corresponding reconstruction (lifting) operators are fundamentally linked through the minimization of error .",
            "id": "3777685",
            "problem": "Consider a fine grid with $8$ equally spaced sites and the corresponding fine state space $\\mathbb{R}^{8}$ endowed with the standard Euclidean inner product. Define the restriction operator $R \\in \\mathbb{R}^{4 \\times 8}$ that maps a fine vector $u \\in \\mathbb{R}^{8}$ to a coarse vector $Ru \\in \\mathbb{R}^{4}$ by averaging adjacent pairs:\n- For $I \\in \\{1,2,3,4\\}$, $(Ru)_{I} = \\frac{u_{2I-1} + u_{2I}}{2}$.\n\nAdopt the principle that the coarse-to-fine mapping should be the orthogonal projection (with respect to the Euclidean inner product on $\\mathbb{R}^{8}$) onto the subspace of fine vectors that are piecewise-constant on each adjacent pair, i.e., vectors $v \\in \\mathbb{R}^{8}$ satisfying $v_{2I-1} = v_{2I}$ for all $I \\in \\{1,2,3,4\\}$.\n\nTasks:\n- Using only the foundational characterization of orthogonal projections as minimizers of squared Euclidean distance onto a linear subspace, derive the orthogonal projector $P \\in \\mathbb{R}^{8 \\times 8}$ that maps any fine vector $u \\in \\mathbb{R}^{8}$ to its closest vector (in Euclidean norm) that is piecewise-constant on adjacent pairs, expressed purely in terms of $R$.\n- Compute $P$ explicitly as an $8 \\times 8$ matrix.\n- Verify analytically that $P^{2} = P$.\n\nProvide only the explicit matrix $P$ as your final answer. No rounding is required, and no units apply. The final answer must be a single analytical expression.",
            "solution": "The problem asks for the derivation of an orthogonal projection operator $P$, its explicit computation, and verification of its properties. The projection is onto a specific subspace of $\\mathbb{R}^{8}$.\n\nFirst, we identify the target subspace for the projection. The problem states that this is the subspace of fine vectors that are piecewise-constant on each adjacent pair. Let this subspace be denoted by $S$. A vector $v \\in \\mathbb{R}^{8}$ is in $S$ if and only if its components satisfy $v_{2I-1} = v_{2I}$ for each $I \\in \\{1, 2, 3, 4\\}$.\nThis means any vector $v \\in S$ can be written in the form:\n$$v = \\begin{pmatrix} c_1 \\\\ c_1 \\\\ c_2 \\\\ c_2 \\\\ c_3 \\\\ c_3 \\\\ c_4 \\\\ c_4 \\end{pmatrix}$$\nfor some set of scalars $\\{c_1, c_2, c_3, c_4\\} \\in \\mathbb{R}^4$.\n\nThe problem specifies that the orthogonal projector $P$ maps any fine vector $u \\in \\mathbb{R}^{8}$ to its closest vector in $S$ with respect to the standard Euclidean norm. This means that for a given $u$, the projected vector $Pu=v$ is the unique vector in $S$ that minimizes the squared Euclidean distance $\\|u-v\\|_2^2$.\n\nWe can express the squared distance as a function of the coefficients $c_I$:\n$$F(c_1, c_2, c_3, c_4) = \\|u - v\\|_2^2 = \\sum_{j=1}^{8} (u_j - v_j)^2$$\nSubstituting the structure of $v \\in S$, we get:\n$$F(c_1, c_2, c_3, c_4) = \\sum_{I=1}^{4} \\left[ (u_{2I-1} - c_I)^2 + (u_{2I} - c_I)^2 \\right]$$\nTo find the coefficients $c_I$ that minimize this function, we take the partial derivative with respect to each $c_K$ for $K \\in \\{1, 2, 3, 4\\}$ and set it to zero. The terms in the sum are independent for each $I$.\n$$\\frac{\\partial F}{\\partial c_K} = \\frac{\\partial}{\\partial c_K} \\left[ (u_{2K-1} - c_K)^2 + (u_{2K} - c_K)^2 \\right] = 0$$\n$$-2(u_{2K-1} - c_K) - 2(u_{2K} - c_K) = 0$$\n$$u_{2K-1} + u_{2K} - 2c_K = 0$$\nSolving for $c_K$, we find the optimal coefficient:\n$$c_K = \\frac{u_{2K-1} + u_{2K}}{2}$$\nThis expression is precisely the definition of the restriction operator $R \\in \\mathbb{R}^{4 \\times 8}$ given in the problem, $(Ru)_I = \\frac{u_{2I-1} + u_{2I}}{2}$. So, the vector of optimal coefficients $c = (c_1, c_2, c_3, c_4)^T$ is given by $c = Ru$.\n\nThe projected vector $Pu = v$ is constructed using these coefficients. Specifically, for each $I \\in \\{1, 2, 3, 4\\}$, the components of $Pu$ are $(Pu)_{2I-1} = c_I$ and $(Pu)_{2I} = c_I$.\nThis operation can be represented by a coarse-to-fine mapping, or prolongation operator, $Q \\in \\mathbb{R}^{8 \\times 4}$, which maps the coarse vector of coefficients $c \\in \\mathbb{R}^4$ to the fine, piecewise-constant vector $v \\in S$. The matrix $Q$ is constructed such that $v=Qc$. Its columns form a basis for the subspace $S$:\n$$Q = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}$$\nSince $Pu = v = Qc$ and $c = Ru$, we can write $Pu = Q(Ru) = (QR)u$. This implies that the projection matrix is $P = QR$.\n\nThe problem requires expressing $P$ purely in terms of $R$. Let's write the explicit matrix for $R$:\n$$R = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}$$\nNow we find the relationship between $Q$ and $R$. The transpose of $R$ is:\n$$R^T = \\begin{pmatrix} \\frac{1}{2} & 0 & 0 & 0 \\\\ \\frac{1}{2} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & 0 & \\frac{1}{2} \\\\ 0 & 0 & 0 & \\frac{1}{2} \\end{pmatrix} = \\frac{1}{2} Q$$\nThus, we have the relationship $Q = 2R^T$. Substituting this into our expression for $P$, we get:\n$$P = QR = (2R^T)R = 2R^T R$$\nThis completes the first task, expressing $P$ purely in terms of $R$.\n\nNext, we compute the explicit $8 \\times 8$ matrix for $P$. We can use the expression $P = 2R^T R$:\n$$R^T R = \\begin{pmatrix} \\frac{1}{2} & 0 & 0 & 0 \\\\ \\frac{1}{2} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & 0 & \\frac{1}{2} \\\\ 0 & 0 & 0 & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & \\frac{1}{4} & 0 & 0 & 0 & 0 & 0 & 0 \\\\ \\frac{1}{4} & \\frac{1}{4} & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{4} & \\frac{1}{4} & 0 & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{4} & \\frac{1}{4} & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & \\frac{1}{4} & \\frac{1}{4} & 0 & 0 \\\\ 0 & 0 & 0 & 0 & \\frac{1}{4} & \\frac{1}{4} & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & \\frac{1}{4} & \\frac{1}{4} \\\\ 0 & 0 & 0 & 0 & 0 & 0 & \\frac{1}{4} & \\frac{1}{4} \\end{pmatrix}$$\n$$P = 2 R^T R = 2 \\times \\begin{pmatrix} \\frac{1}{4} & \\frac{1}{4} & \\dots \\\\ \\vdots \\end{pmatrix}  = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 & 0 & 0 \\\\ \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 & 0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}$$\n\nFinally, we verify analytically that $P$ is idempotent, i.e., $P^2 = P$.\nWe can do this using the expression $P=QR = Q(\\frac{1}{2}Q^T)=\\frac{1}{2}QQ^T$.\n$$P^2 = \\left(\\frac{1}{2}QQ^T\\right) \\left(\\frac{1}{2}QQ^T\\right) = \\frac{1}{4} Q (Q^T Q) Q^T$$\nLet's compute the product $Q^T Q$:\n$$Q^T Q = \\begin{pmatrix} 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix} = 2I_{4}$$\nwhere $I_4$ is the $4 \\times 4$ identity matrix.\nSubstituting this result into the expression for $P^2$:\n$$P^2 = \\frac{1}{4} Q (2I_4) Q^T = \\frac{2}{4} Q I_4 Q^T = \\frac{1}{2} QQ^T = P$$\nThe condition $P^2=P$ is satisfied. This also confirms the general formula for an orthogonal projector onto the column space of a matrix $A$ whose columns are not necessarily orthonormal: $P = A(A^T A)^{-1} A^T$. With $A=Q$, we have $(Q^T Q)^{-1} = (2I_4)^{-1}=\\frac{1}{2}I_4$, so $P=Q(\\frac{1}{2}I_4)Q^T = \\frac{1}{2}QQ^T$. Our derivation from first principles is consistent with this standard formula.\nThe explicit matrix $P$ is given above.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 \\\\\n0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building on the algebraic foundation of projection, we now apply these concepts to a practical problem in numerical analysis: accelerating the solution of differential equations. This practice demonstrates the construction of a coarse-grid operator using the Galerlin principle, $A_c = R A_f P$, which is a cornerstone of efficient multigrid solvers. You will discover the remarkable outcome when standard choices are made for the restriction ($R$) and prolongation ($P$) operators, revealing how the structure of the fine-grid differential operator is elegantly preserved on the coarse grid .",
            "id": "3777667",
            "problem": "Consider the one-dimensional Poisson equation $-u^{\\prime\\prime}(x)=f(x)$ on a uniform grid. Let the fine grid have spacing $h$ and grid points indexed by $i \\in \\mathbb{Z}$, and let the coarse grid have spacing $H=2h$ and grid points indexed by $J \\in \\mathbb{Z}$. The fine-grid discrete operator $A_f$ is the standard three-point ($3$-point) second-difference operator defined by\n$$\n(A_f u)_i \\;=\\; \\frac{1}{h^2}\\left(-u_{i-1}+2u_i-u_{i+1}\\right).\n$$\nDefine the prolongation (fine-grid injection by linear interpolation) operator $P:\\mathbb{R}^{N_c}\\to\\mathbb{R}^{N_f}$ from the coarse grid to the fine grid by\n$$\n(P V)_{2J} \\;=\\; V_J,\\qquad (P V)_{2J+1} \\;=\\; \\frac{1}{2}\\left(V_J+V_{J+1}\\right),\n$$\nand define the full-weighting restriction operator $R:\\mathbb{R}^{N_f}\\to\\mathbb{R}^{N_c}$ from the fine grid to the coarse grid by\n$$\n(R y)_J \\;=\\; \\frac{1}{4}y_{2J-1}+\\frac{1}{2}y_{2J}+\\frac{1}{4}y_{2J+1}.\n$$\nUsing only these definitions and the uniform-grid structure, derive from first principles the Galerkin coarse operator $A_c = R\\,A_f\\,P$. Then, determine the interior three-point stencil of $A_c$ expressed in terms of the coarse-grid spacing $H$.\n\nYour final answer must be the three stencil coefficients written as a single row vector expression in terms of $H$ (no units). Do not provide intermediate steps in the final answer. No rounding is required.",
            "solution": "The objective is to derive the Galerkin coarse-grid operator $A_c = R A_f P$ and determine its interior stencil. We will compute the action of $A_c$ on an arbitrary coarse-grid vector $V$. The result, $(A_c V)_J$, will be a linear combination of coarse-grid values $V_{J-1}$, $V_J$, and $V_{J+1}$. The coefficients of this combination constitute the stencil. The derivation proceeds by applying the operators $P$, $A_f$, and $R$ in sequence.\n\nLet $V$ be a vector defined on the coarse grid with grid points indexed by $J \\in \\mathbb{Z}$.\n\nStep 1: Apply the prolongation operator $P$.\nThe operator $P$ maps the coarse-grid vector $V$ to a fine-grid vector $u = P V$. The components of $u$ are given by the definition of linear interpolation:\n$$\nu_{2J} = (P V)_{2J} = V_J\n$$\n$$\nu_{2J+1} = (P V)_{2J+1} = \\frac{1}{2}(V_J + V_{J+1})\n$$\n\nStep 2: Apply the fine-grid operator $A_f$.\nThe operator $A_f$ is applied to the fine-grid vector $u$. Let the result be $w = A_f u$. The components of $w$ are given by:\n$$\nw_i = (A_f u)_i = \\frac{1}{h^2}(-u_{i-1} + 2u_i - u_{i+1})\n$$\n\nStep 3: Apply the restriction operator $R$.\nThe operator $R$ maps the fine-grid vector $w$ to the coarse-grid vector $A_c V$. The $J$-th component is given by the definition of full-weighting restriction:\n$$\n(A_c V)_J = (R w)_J = \\frac{1}{4}w_{2J-1} + \\frac{1}{2}w_{2J} + \\frac{1}{4}w_{2J+1}\n$$\nTo evaluate this expression, we need to compute the components $w_{2J-1}$, $w_{2J}$, and $w_{2J+1}$.\n\nFirst, we express the required fine-grid values of $u$ in terms of the coarse-grid values of $V$. The indices around the coarse-grid point $J$ correspond to fine-grid indices $2J-1$, $2J$, and $2J+1$. We will also need neighboring points.\nFor the fine-grid point $i=2J-1$:\n$u_{2J-2} = (PV)_{2(J-1)} = V_{J-1}$\n$u_{2J-1} = (PV)_{2(J-1)+1} = \\frac{1}{2}(V_{J-1} + V_J)$\n$u_{2J} = (PV)_{2J} = V_J$\n\nFor the fine-grid point $i=2J$:\n$u_{2J-1} = \\frac{1}{2}(V_{J-1} + V_J)$\n$u_{2J} = V_J$\n$u_{2J+1} = \\frac{1}{2}(V_J + V_{J+1})$\n\nFor the fine-grid point $i=2J+1$:\n$u_{2J} = V_J$\n$u_{2J+1} = \\frac{1}{2}(V_J + V_{J+1})$\n$u_{2J+2} = (PV)_{2(J+1)} = V_{J+1}$\n\nNow we can compute the required components of $w = A_f u$:\nFor $i = 2J-1$:\n$$\nw_{2J-1} = \\frac{1}{h^2}(-u_{2J-2} + 2u_{2J-1} - u_{2J}) = \\frac{1}{h^2}\\left(-V_{J-1} + 2\\left(\\frac{1}{2}(V_{J-1} + V_J)\\right) - V_J\\right)\n$$\n$$\nw_{2J-1} = \\frac{1}{h^2}(-V_{J-1} + V_{J-1} + V_J - V_J) = 0\n$$\n\nFor $i = 2J$:\n$$\nw_{2J} = \\frac{1}{h^2}(-u_{2J-1} + 2u_{2J} - u_{2J+1}) = \\frac{1}{h^2}\\left(-\\frac{1}{2}(V_{J-1} + V_J) + 2V_J - \\frac{1}{2}(V_J + V_{J+1})\\right)\n$$\n$$\nw_{2J} = \\frac{1}{h^2}\\left(-\\frac{1}{2}V_{J-1} + \\left(-\\frac{1}{2} + 2 - \\frac{1}{2}\\right)V_J - \\frac{1}{2}V_{J+1}\\right) = \\frac{1}{h^2}\\left(-\\frac{1}{2}V_{J-1} + 1V_J - \\frac{1}{2}V_{J+1}\\right)\n$$\n\nFor $i = 2J+1$:\n$$\nw_{2J+1} = \\frac{1}{h^2}(-u_{2J} + 2u_{2J+1} - u_{2J+2}) = \\frac{1}{h^2}\\left(-V_J + 2\\left(\\frac{1}{2}(V_J + V_{J+1})\\right) - V_{J+1}\\right)\n$$\n$$\nw_{2J+1} = \\frac{1}{h^2}(-V_J + V_J + V_{J+1} - V_{J+1}) = 0\n$$\n\nFinally, substitute these components of $w$ back into the expression for $(A_c V)_J$:\n$$\n(A_c V)_J = \\frac{1}{4}(0) + \\frac{1}{2}w_{2J} + \\frac{1}{4}(0) = \\frac{1}{2}w_{2J}\n$$\n$$\n(A_c V)_J = \\frac{1}{2} \\left[ \\frac{1}{h^2}\\left(-\\frac{1}{2}V_{J-1} + V_J - \\frac{1}{2}V_{J+1}\\right) \\right]\n$$\n$$\n(A_c V)_J = \\frac{1}{4h^2}(-V_{J-1} + 2V_J - V_{J+1})\n$$\nThe problem specifies the coarse-grid spacing is $H=2h$, which implies $h=H/2$ and $h^2=H^2/4$. Substituting this into the expression for $A_c$:\n$$\n(A_c V)_J = \\frac{1}{4(H^2/4)}(-V_{J-1} + 2V_J - V_{J+1}) = \\frac{1}{H^2}(-V_{J-1} + 2V_J - V_{J+1})\n$$\nThis expression defines the action of the coarse-grid operator $A_c$. The interior three-point stencil is composed of the coefficients of $V_{J-1}$, $V_J$, and $V_{J+1}$. These coefficients are, respectively:\n$$\n\\text{coeff}(V_{J-1}) = -\\frac{1}{H^2}\n$$\n$$\n\\text{coeff}(V_{J}) = \\frac{2}{H^2}\n$$\n$$\n\\text{coeff}(V_{J+1}) = -\\frac{1}{H^2}\n$$\nThe stencil is thus $\\frac{1}{H^2}(-1, 2, -1)$. The problem asks for the three stencil coefficients as a single row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{H^2} & \\frac{2}{H^2} & -\\frac{1}{H^2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The principles of coarse-graining are not limited to deterministic systems but are also powerful tools for analyzing stochastic processes. This exercise introduces \"lumpability\" in continuous-time Markov chains, a crucial concept defining the conditions under which an exact coarse-grained model exists. By applying the conditional exit rate criterion to test for strong lumpability, you will learn to identify when the complex dynamics of a system can be perfectly simplified without loss of information .",
            "id": "3777646",
            "problem": "Consider a continuous-time Markov chain (CTMC) on the fine-grained state space $\\mathcal{S}=\\{1,2,3,4\\}$ with infinitesimal generator $\\mathbf{Q}\\in\\mathbb{R}^{4\\times 4}$ specified by the following symmetric transition structure. Let $\\alpha>0$ denote the intra-block transition rate and let $\\beta>0$ and $\\gamma>0$ denote the two distinct cross-block transition rates. The off-diagonal entries of $\\mathbf{Q}$ are given by\n$$\nQ_{12}=\\alpha,\\quad Q_{21}=\\alpha,\\quad Q_{34}=\\alpha,\\quad Q_{43}=\\alpha,\n$$\n$$\nQ_{13}=\\beta,\\quad Q_{24}=\\beta,\\quad Q_{14}=\\gamma,\\quad Q_{23}=\\gamma,\n$$\nand symmetry $Q_{ij}=Q_{ji}$ for $i\\neq j$. The diagonal entries satisfy $Q_{ii}=-\\sum_{j\\neq i}Q_{ij}$ so that each row sums to zero, as required for a CTMC generator.\n\nWe introduce a coarse-grained description by partitioning the fine-grained states into two blocks (a two-state coarse model):\n$$\nB_{1}=\\{1,2\\},\\qquad B_{2}=\\{3,4\\}.\n$$\nDefine the coarse variables as the block-aggregated populations and let the restriction (aggregation) operator $\\mathbf{R}\\in\\mathbb{R}^{2\\times 4}$ map a fine-grained probability vector to its block sums via $(\\mathbf{R}\\mathbf{p})_{I}=\\sum_{x\\in B_{I}}p_{x}$ for $I\\in\\{1,2\\}$. Consider the lifting (injection) operator $\\mathbf{P}\\in\\mathbb{R}^{4\\times 2}$ that replicates a coarse observable uniformly over the block, i.e., $P_{x,I}=\\frac{1}{|B_{I}|}$ for $x\\in B_{I}$ and $0$ otherwise, with $|B_{1}|=|B_{2}|=2$.\n\nStarting from the generator definition for a CTMC and the definition of strong lumpability for a partition, use the equal conditional exit rate criterion to test whether the given partition $\\{B_{1},B_{2}\\}$ is strongly lumpable. In the strong lumpability case, derive the coarse-grained generator $\\mathbf{Q}_{c}\\in\\mathbb{R}^{2\\times 2}$ that governs the dynamics on the block space under the mapping induced by $(\\mathbf{R},\\mathbf{P})$, expressed in terms of $\\alpha$, $\\beta$, and $\\gamma$.\n\nYour final answer must be a single closed-form symbolic $2\\times 2$ matrix giving $\\mathbf{Q}_{c}$ in terms of $\\alpha$, $\\beta$, and $\\gamma$. No numerical approximation is required, and no units are to be included in the final answer.",
            "solution": "First, we construct the full infinitesimal generator matrix $\\mathbf{Q}$ using the provided off-diagonal entries and the condition that the sum of each row must be zero ($Q_{ii}=-\\sum_{j\\neq i}Q_{ij}$).\n\nThe off-diagonal entries are:\n$Q_{12} = \\alpha$, $Q_{13} = \\beta$, $Q_{14} = \\gamma$.\n$Q_{21} = \\alpha$, $Q_{23} = \\gamma$, $Q_{24} = \\beta$.\n$Q_{31} = \\beta$, $Q_{32} = \\gamma$, $Q_{34} = \\alpha$.\n$Q_{41} = \\gamma$, $Q_{42} = \\beta$, $Q_{43} = \\alpha$.\n\nThe diagonal entries are calculated as follows:\n$Q_{11} = -(Q_{12} + Q_{13} + Q_{14}) = -(\\alpha + \\beta + \\gamma)$.\n$Q_{22} = -(Q_{21} + Q_{23} + Q_{24}) = -(\\alpha + \\gamma + \\beta)$.\n$Q_{33} = -(Q_{31} + Q_{32} + Q_{34}) = -(\\beta + \\gamma + \\alpha)$.\n$Q_{44} = -(Q_{41} + Q_{42} + Q_{43}) = -(\\gamma + \\beta + \\alpha)$.\nNotice that all diagonal entries are equal.\n\nThe complete generator matrix $\\mathbf{Q}$ is:\n$$\n\\mathbf{Q} = \\begin{pmatrix}\n-(\\alpha+\\beta+\\gamma) & \\alpha & \\beta & \\gamma \\\\\n\\alpha & -(\\alpha+\\beta+\\gamma) & \\gamma & \\beta \\\\\n\\beta & \\gamma & -(\\alpha+\\beta+\\gamma) & \\alpha \\\\\n\\gamma & \\beta & \\alpha & -(\\alpha+\\beta+\\gamma)\n\\end{pmatrix}\n$$\n\nNext, we test if the partition $\\mathcal{B} = \\{B_{1}, B_{2}\\}$ with $B_{1}=\\{1,2\\}$ and $B_{2}=\\{3,4\\}$ is strongly lumpable. The \"equal conditional exit rate criterion\" for strong lumpability states that for any two blocks $B_{I}$ and $B_{J}$ in the partition, the total transition rate from any state $i \\in B_{I}$ to the block $B_{J}$ must be the same for all $i \\in B_{I}$. Mathematically, for any pair of blocks $(B_I, B_J)$, the sum $\\sum_{j \\in B_J} Q_{ij}$ must be constant for all $i \\in B_I$.\n\nWe must check this condition for all four combinations of $(I,J) \\in \\{1,2\\} \\times \\{1,2\\}$.\n\nCase 1: Transitions from $B_{1}$ to $B_{1}$ ($I=1, J=1$).\nFor any state $i \\in B_1$, we calculate the rate $\\sum_{j \\in B_1} Q_{ij}$.\nFor $i=1$: $\\sum_{j \\in \\{1,2\\}} Q_{1j} = Q_{11} + Q_{12} = -(\\alpha+\\beta+\\gamma) + \\alpha = -(\\beta+\\gamma)$.\nFor $i=2$: $\\sum_{j \\in \\{1,2\\}} Q_{2j} = Q_{21} + Q_{22} = \\alpha - (\\alpha+\\beta+\\gamma) = -(\\beta+\\gamma)$.\nThe rates are equal.\n\nCase 2: Transitions from $B_{1}$ to $B_{2}$ ($I=1, J=2$).\nFor any state $i \\in B_1$, we calculate the rate $\\sum_{j \\in B_2} Q_{ij}$.\nFor $i=1$: $\\sum_{j \\in \\{3,4\\}} Q_{1j} = Q_{13} + Q_{14} = \\beta + \\gamma$.\nFor $i=2$: $\\sum_{j \\in \\{3,4\\}} Q_{2j} = Q_{23} + Q_{24} = \\gamma + \\beta$.\nThe rates are equal.\n\nCase 3: Transitions from $B_{2}$ to $B_{1}$ ($I=2, J=1$).\nFor any state $i \\in B_2$, we calculate the rate $\\sum_{j \\in B_1} Q_{ij}$.\nFor $i=3$: $\\sum_{j \\in \\{1,2\\}} Q_{3j} = Q_{31} + Q_{32} = \\beta + \\gamma$.\nFor $i=4$: $\\sum_{j \\in \\{1,2\\}} Q_{4j} = Q_{41} + Q_{42} = \\gamma + \\beta$.\nThe rates are equal.\n\nCase 4: Transitions from $B_{2}$ to $B_{2}$ ($I=2, J=2$).\nFor any state $i \\in B_2$, we calculate the rate $\\sum_{j \\in B_2} Q_{ij}$.\nFor $i=3$: $\\sum_{j \\in \\{3,4\\}} Q_{3j} = Q_{33} + Q_{34} = -(\\alpha+\\beta+\\gamma) + \\alpha = -(\\beta+\\gamma)$.\nFor $i=4$: $\\sum_{j \\in \\{3,4\\}} Q_{4j} = Q_{43} + Q_{44} = \\alpha - (\\alpha+\\beta+\\gamma) = -(\\beta+\\gamma)$.\nThe rates are equal.\n\nSince the condition holds for all pairs of blocks, the partition $\\{B_1, B_2\\}$ is strongly lumpable. This is true for any choice of positive rates $\\alpha, \\beta, \\gamma$.\n\nIn the case of strong lumpability, the entries $(Q_c)_{IJ}$ of the coarse-grained generator $\\mathbf{Q}_{c}$ are precisely these constant aggregated rates.\n$$\n(Q_c)_{IJ} = \\sum_{j \\in B_J} Q_{ij} \\quad \\text{for any } i \\in B_I.\n$$\nUsing the values we calculated:\n$(Q_c)_{11} = -(\\beta+\\gamma)$.\n$(Q_c)_{12} = \\beta+\\gamma$.\n$(Q_c)_{21} = \\beta+\\gamma$.\n$(Q_c)_{22} = -(\\beta+\\gamma)$.\n\nThus, the coarse-grained generator $\\mathbf{Q}_{c} \\in \\mathbb{R}^{2\\times 2}$ is:\n$$\n\\mathbf{Q}_{c} = \\begin{pmatrix}\n-(\\beta+\\gamma) & \\beta+\\gamma \\\\\n\\beta+\\gamma & -(\\beta+\\gamma)\n\\end{pmatrix}\n$$\nThe dynamics on the coarse space describe a simple two-state system where the transition rate between the two aggregate states is $\\beta+\\gamma$ in both directions. The intra-block rate $\\alpha$ does not influence the coarse-grained dynamics due to the symmetry of the problem.\n\nAs a verification, the problem mentions the mapping induced by the restriction operator $\\mathbf{R}$ and lifting operator $\\mathbf{P}$. The coarse-grained generator for a strongly lumpable system can be found via the projection $\\mathbf{Q}_{c} = \\mathbf{R} \\mathbf{Q} \\mathbf{P}$.\nThe operators are:\n$$\n\\mathbf{R} = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\end{pmatrix}, \\quad \\mathbf{P} = \\begin{pmatrix} 1/2 & 0 \\\\ 1/2 & 0 \\\\ 0 & 1/2 \\\\ 0 & 1/2 \\end{pmatrix}\n$$\nFirst, we compute $\\mathbf{Q}\\mathbf{P}$:\n$$\n\\mathbf{Q}\\mathbf{P} = \\begin{pmatrix}\n-(\\alpha+\\beta+\\gamma) & \\alpha & \\beta & \\gamma \\\\\n\\alpha & -(\\alpha+\\beta+\\gamma) & \\gamma & \\beta \\\\\n\\beta & \\gamma & -(\\alpha+\\beta+\\gamma) & \\alpha \\\\\n\\gamma & \\beta & \\alpha & -(\\alpha+\\beta+\\gamma)\n\\end{pmatrix}\n\\begin{pmatrix} 1/2 & 0 \\\\ 1/2 & 0 \\\\ 0 & 1/2 \\\\ 0 & 1/2 \\end{pmatrix}\n=\n\\frac{1}{2}\n\\begin{pmatrix}\n-(\\beta+\\gamma) & \\beta+\\gamma \\\\\n-(\\beta+\\gamma) & \\beta+\\gamma \\\\\n\\beta+\\gamma & -(\\beta+\\gamma) \\\\\n\\beta+\\gamma & -(\\beta+\\gamma)\n\\end{pmatrix}\n$$\nNext, we pre-multiply by $\\mathbf{R}$:\n$$\n\\mathbf{Q}_{c} = \\mathbf{R}(\\mathbf{Q}\\mathbf{P}) = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\end{pmatrix}\n\\frac{1}{2}\n\\begin{pmatrix}\n-(\\beta+\\gamma) & \\beta+\\gamma \\\\\n-(\\beta+\\gamma) & \\beta+\\gamma \\\\\n\\beta+\\gamma & -(\\beta+\\gamma) \\\\\n\\beta+\\gamma & -(\\beta+\\gamma)\n\\end{pmatrix}\n$$\n$$\n\\mathbf{Q}_{c} =\n\\frac{1}{2}\n\\begin{pmatrix}\n-2(\\beta+\\gamma) & 2(\\beta+\\gamma) \\\\\n2(\\beta+\\gamma) & -2(\\beta+\\gamma)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-(\\beta+\\gamma) & \\beta+\\gamma \\\\\n\\beta+\\gamma & -(\\beta+\\gamma)\n\\end{pmatrix}\n$$\nThis confirms the result obtained from the direct application of the lumpability criterion.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -(\\beta+\\gamma) & \\beta+\\gamma \\\\ \\beta+\\gamma & -(\\beta+\\gamma) \\end{pmatrix}}\n$$"
        }
    ]
}