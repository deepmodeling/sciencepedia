{
    "hands_on_practices": [
        {
            "introduction": "A fundamental question in any simulation is determining how long to run the initial equilibration, or 'burn-in', phase. This practice provides a quantitative foundation for this decision, moving beyond simple guesswork. By using the concept of exponential convergence to equilibrium, you will derive a common rule of thumb that directly links the system's intrinsic relaxation time, $\\tau$, to the required simulation time to achieve a desired level of statistical confidence .",
            "id": "3755212",
            "problem": "In a hierarchical multiscale simulation that couples fine-scale dynamics to a coarse-grained description, assume the phase-space dynamics induces an ergodic Markov process with a spectral gap that ensures exponential convergence to the unique stationary equilibrium. Let a scalar diagnostic observable $A(t)$ be monitored during an initial equilibration (burn-in) phase and the subsequent production phase. Denote the equilibrium mean by $\\mu$, the equilibrium standard deviation by $\\sigma$, and the relaxation time by $\\tau$, defined as the characteristic timescale of decay of the slowest mode controlling the convergence of expectations. Under linear response near equilibrium and ergodicity, assume that the mean bias $\\Delta(t) \\equiv \\mathbb{E}[A(t)] - \\mu$ contracts exponentially on timescale $\\tau$. You are told that the initial condition is chosen such that $|\\Delta(0)| \\leq \\sigma$. Design the burn-in duration $t_{b}$ as a multiple of the relaxation time, $t_{b} = m \\tau$, so that the residual mean bias at the end of burn-in satisfies $|\\Delta(t_{b})| \\leq \\alpha \\sigma$ with $\\alpha = 0.01$, a conventional threshold for negligible bias in advanced graduate practice.\n\nUsing only the stated assumptions and the exponential convergence property, determine the smallest rule-of-thumb multiple $m$ satisfying the requirement. Report the dimensionless multiple $m$ only (not $t_{b}$), and round your answer to one significant figure.",
            "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and unambiguous.\n\n**Step 1: Extracted Givens**\n- The system dynamics are described by an ergodic Markov process with a spectral gap, implying exponential convergence to a unique stationary equilibrium.\n- A scalar observable is denoted by $A(t)$.\n- The equilibrium mean of the observable is $\\mu$.\n- The equilibrium standard deviation is $\\sigma$.\n- The relaxation time is $\\tau$.\n- The mean bias is defined as $\\Delta(t) \\equiv \\mathbb{E}[A(t)] - \\mu$.\n- The mean bias is assumed to contract exponentially on the timescale $\\tau$: $\\Delta(t) \\propto \\exp(-t/\\tau)$.\n- The initial condition satisfies $|\\Delta(0)| \\leq \\sigma$.\n- The burn-in duration is set as $t_{b} = m \\tau$, where $m$ is a dimensionless multiple.\n- The target condition for the end of the burn-in is that the residual mean bias must satisfy $|\\Delta(t_{b})| \\leq \\alpha \\sigma$.\n- The threshold for negligible bias is given as $\\alpha = 0.01$.\n\n**Step 2: Validation**\n- **Scientific Grounding**: The problem is well-grounded in the principles of statistical mechanics and computational physics, specifically the theory of Markov processes and their application to molecular simulations. The concepts of ergodicity, relaxation time, equilibration, and exponential convergence due to a spectral gap are standard and fundamentally sound.\n- **Well-Posedness**: The problem is well-posed. It provides a clear objective (determine the smallest $m$), a set of mathematical assumptions, and sufficient data to arrive at a unique solution.\n- **Objectivity**: The problem is stated in precise, objective, and quantitative terms common to the field.\n\n**Step 3: Verdict**\nThe problem is valid. We may proceed with the solution.\n\n**Solution Derivation**\nThe problem states that the mean bias, $\\Delta(t)$, contracts exponentially on the timescale $\\tau$. This physical behavior is mathematically expressed as:\n$$\n\\Delta(t) = \\Delta(0) \\exp\\left(-\\frac{t}{\\tau}\\right)\n$$\nWe are concerned with the magnitude of the bias, so we consider the absolute value:\n$$\n|\\Delta(t)| = |\\Delta(0)| \\exp\\left(-\\frac{t}{\\tau}\\right)\n$$\nThe problem provides an upper bound on the initial bias: $|\\Delta(0)| \\leq \\sigma$. To guarantee that the final condition is met for any valid starting state, we must consider the worst-case scenario, which corresponds to the maximum possible initial bias. Therefore, we base our calculation on the case $|\\Delta(0)| = \\sigma$. This leads to an upper bound on the bias at any time $t$:\n$$\n|\\Delta(t)| \\leq \\sigma \\exp\\left(-\\frac{t}{\\tau}\\right)\n$$\nThe burn-in phase has a duration of $t_b$, which is defined as a multiple $m$ of the relaxation time $\\tau$:\n$$\nt_b = m\\tau\n$$\nSubstituting $t = t_b$ into the inequality for the bias magnitude, we find the upper bound for the residual bias at the end of the burn-in phase:\n$$\n|\\Delta(t_b)| \\leq \\sigma \\exp\\left(-\\frac{m\\tau}{\\tau}\\right) = \\sigma \\exp(-m)\n$$\nThe design requirement is that this residual bias must be no larger than a fraction $\\alpha$ of the equilibrium standard deviation $\\sigma$:\n$$\n|\\Delta(t_b)| \\leq \\alpha \\sigma\n$$\nTo ensure this condition is met, the upper bound on $|\\Delta(t_b)|$ must be less than or equal to the required threshold $\\alpha \\sigma$. This establishes the following inequality:\n$$\n\\sigma \\exp(-m) \\leq \\alpha \\sigma\n$$\nSince the standard deviation $\\sigma$ must be positive for a non-trivial observable (if $\\sigma = 0$, the observable is constant and already equilibrated), we can divide both sides by $\\sigma$ without changing the inequality:\n$$\n\\exp(-m) \\leq \\alpha\n$$\nTo solve for $m$, we take the natural logarithm of both sides. As the natural logarithm is a monotonically increasing function, the direction of the inequality is preserved:\n$$\n\\ln(\\exp(-m)) \\leq \\ln(\\alpha)\n$$\n$$\n-m \\leq \\ln(\\alpha)\n$$\nMultiplying by $-1$ reverses the inequality sign:\n$$\nm \\geq -\\ln(\\alpha)\n$$\nThe problem asks for the smallest multiple $m$ that satisfies this requirement. This corresponds to the minimum value, which is given by the equality:\n$$\nm = -\\ln(\\alpha)\n$$\nWe are given the value $\\alpha = 0.01$. Substituting this into the equation for $m$:\n$$\nm = -\\ln(0.01) = -\\ln(10^{-2})\n$$\nUsing the property of logarithms $\\ln(x^y) = y \\ln(x)$, we get:\n$$\nm = -(-2)\\ln(10) = 2\\ln(10)\n$$\nNow, we calculate the numerical value. The natural logarithm of $10$ is approximately $\\ln(10) \\approx 2.302585$.\n$$\nm \\approx 2 \\times 2.302585 = 4.60517\n$$\nThe problem requires the answer to be rounded to one significant figure. The first significant figure is $4$. The next digit is $6$, which is greater than or equal to $5$, so we round the first digit up.\n$$\nm \\approx 5\n$$\nThus, a common rule of thumb is that the burn-in period should be approximately $5$ times the longest relaxation time of the system to ensure the systematic bias has decayed to about $1\\%$ of the system's natural fluctuation scale.",
            "answer": "$$\\boxed{5}$$"
        },
        {
            "introduction": "Once a simulation enters the production phase, the collected data forms a time-correlated series, not a sequence of independent samples. This practice delves into the crucial task of quantifying the statistical quality of this data. You will derive the foundational relationship between the variance of the sample mean, the number of samples $N$, and the integrated autocorrelation time $\\tau_{\\mathrm{int},A}$, and explore the practical challenges of estimating this value from a finite dataset .",
            "id": "3755133",
            "problem": "A single observable $A(t)$ is monitored in a molecular simulation of a multiscale system. After an initial equilibration time $t_{\\mathrm{eq}}$ has been discarded, the subsequent production trajectory is assumed to be stationary and ergodic. The observable is sampled at uniform strides $\\Delta t$ to produce the discrete series $\\{A_i\\}_{i=1}^{N}$ with $A_i \\equiv A(t_{\\mathrm{eq}} + i\\,\\Delta t)$. Let the sample mean be $\\bar{A} \\equiv \\frac{1}{N}\\sum_{i=1}^{N} A_i$ and the population variance be $\\sigma_A^2 \\equiv \\mathrm{Var}(A_i)$, assumed finite. Define the autocovariance function $C_A(k) \\equiv \\mathrm{Cov}(A_i, A_{i+k})$ and the normalized autocorrelation function $\\rho_A(k) \\equiv \\frac{C_A(k)}{\\sigma_A^2}$ for integer lag $k$. The Integrated Autocorrelation Time (IAT) is defined in sampling-step units by $\\tau_{\\mathrm{int},A} \\equiv \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho_A(k)$ when the series is absolutely summable.\n\nStarting only from these definitions and the assumptions of stationarity and ergodicity of the production phase:\n1. Derive the leading-order asymptotic expression for $\\mathrm{Var}(\\bar{A})$ in terms of $N$, $\\sigma_A^2$, and $\\tau_{\\mathrm{int},A}$, valid when $N$ is large compared to the correlation range. Your derivation must begin from the definitions of $\\bar{A}$, $C_A(k)$, and $\\rho_A(k)$, and must show how the integrated autocorrelation time emerges from the covariance summation for $\\mathrm{Var}(\\bar{A})$.\n2. Suppose the normalized autocorrelation function in the production phase is well-approximated by a single-exponential decay $\\rho_A(k) = \\exp\\!\\big(-k\\,\\Delta t/\\tau_c\\big)$ with sampling stride $\\Delta t = 5\\,\\mathrm{fs}$ and correlation time $\\tau_c = 250\\,\\mathrm{fs}$. For a production dataset of $N = 2.0 \\times 10^{5}$ samples with $\\sigma_A^2 = 1.6\\,u_A^2$, compute a closed-form expression for $\\tau_{\\mathrm{int},A}$ in sampling-step units and then the numerical value of $\\mathrm{Var}(\\bar{A})$ predicted by your leading-order expression. Round your final numerical answer to four significant figures. Express the final variance in the unit $u_A^2$.\n3. Explain a statistically principled strategy to estimate $\\tau_{\\mathrm{int},A}$ from finite-length production data after equilibration, including how to choose a truncation window and how to validate the stationarity assumption. Your explanation should be based on standard time-series principles and must justify bias-variance trade-offs without invoking any shortcut formulas.\n\nOnly the final numerical value requested in part $2$ will be graded for the final answer. However, full credit for the solution requires correct derivation and sound justification in parts $1$ and $3$.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the statistical analysis of molecular simulation data, is well-posed with all necessary information provided, and is expressed in objective, formal language free of contradictions or ambiguities.\n\n### Part 1: Derivation of the Variance of the Sample Mean\n\nWe are asked to derive the leading-order asymptotic expression for $\\mathrm{Var}(\\bar{A})$. We begin with the definition of the sample mean, $\\bar{A}$:\n$$\n\\bar{A} \\equiv \\frac{1}{N}\\sum_{i=1}^{N} A_i\n$$\nThe variance of the sample mean is given by:\n$$\n\\mathrm{Var}(\\bar{A}) = \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} A_i\\right)\n$$\nUsing the property of variance, $\\mathrm{Var}(c X) = c^2 \\mathrm{Var}(X)$, we can factor out the constant $\\frac{1}{N}$:\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{1}{N^2} \\mathrm{Var}\\left(\\sum_{i=1}^{N} A_i\\right)\n$$\nThe variance of a sum of random variables is the sum of all elements in their covariance matrix:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{N} A_i\\right) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathrm{Cov}(A_i, A_j)\n$$\nThe problem states that the production trajectory is stationary. This implies that the covariance $\\mathrm{Cov}(A_i, A_j)$ depends only on the time lag between the samples, $k = j-i$. We use the given definition of the autocovariance function, $C_A(k) \\equiv \\mathrm{Cov}(A_i, A_{i+k})$. Therefore, $\\mathrm{Cov}(A_i, A_j) = C_A(j-i)$. Substituting this into the double summation gives:\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} C_A(j-i)\n$$\nTo evaluate the double sum, we change the summation index from $(i,j)$ to $(i,k)$ where $k=j-i$. For a fixed lag $k$, we count the number of pairs $(i,j)$ in the sum. The number of terms for a given lag $k$ is $N-|k|$ for $k \\in \\{-(N-1), \\dots, (N-1)\\}$. The double summation can thus be rewritten as a single sum over the lag $k$:\n$$\n\\sum_{i=1}^{N} \\sum_{j=1}^{N} C_A(j-i) = \\sum_{k=-(N-1)}^{N-1} (N-|k|) C_A(k)\n$$\nSubstituting this back into the expression for $\\mathrm{Var}(\\bar{A})$:\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{1}{N^2} \\sum_{k=-(N-1)}^{N-1} (N-|k|) C_A(k) = \\frac{1}{N} \\sum_{k=-(N-1)}^{N-1} \\left(1-\\frac{|k|}{N}\\right) C_A(k)\n$$\nWe are asked for a leading-order asymptotic expression valid for large $N$, specifically when $N$ is much larger than the correlation range of the observable. The correlation range is the scale of lags $k$ over which $C_A(k)$ is significantly different from zero. For lags $|k|$ much larger than this range, $C_A(k) \\approx 0$. Because $N$ is large, the term $\\frac{|k|}{N}$ is negligible for all $k$ within the correlation range. Therefore, we can approximate $\\left(1-\\frac{|k|}{N}\\right) \\approx 1$. Furthermore, since $C_A(k)$ decays to zero for lags $|k| \\ll N$, we can extend the limits of the summation to $\\pm\\infty$ with negligible error. This yields the asymptotic expression:\n$$\n\\mathrm{Var}(\\bar{A}) \\approx \\frac{1}{N} \\sum_{k=-\\infty}^{\\infty} C_A(k)\n$$\nThe autocovariance function is symmetric, $C_A(k) = C_A(-k)$. We can split the sum as:\n$$\n\\sum_{k=-\\infty}^{\\infty} C_A(k) = C_A(0) + \\sum_{k=1}^{\\infty} C_A(k) + \\sum_{k=-\\infty}^{-1} C_A(k) = C_A(0) + 2\\sum_{k=1}^{\\infty} C_A(k)\n$$\nBy definition, $C_A(0) = \\mathrm{Var}(A_i) = \\sigma_A^2$. We substitute this and the definition of the normalized autocorrelation function, $\\rho_A(k) = C_A(k)/\\sigma_A^2$:\n$$\n\\mathrm{Var}(\\bar{A}) \\approx \\frac{\\sigma_A^2}{N} \\left(\\rho_A(0) + 2\\sum_{k=1}^{\\infty} \\rho_A(k)\\right)\n$$\nSince $\\rho_A(0) = C_A(0)/\\sigma_A^2 = 1$, the expression becomes:\n$$\n\\mathrm{Var}(\\bar{A}) \\approx \\frac{\\sigma_A^2}{N} \\left(1 + 2\\sum_{k=1}^{\\infty} \\rho_A(k)\\right)\n$$\nFinally, we introduce the Integrated Autocorrelation Time (IAT), given as $\\tau_{\\mathrm{int},A} \\equiv \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho_A(k)$. From this definition, we can express the summation part as $2\\sum_{k=1}^{\\infty} \\rho_A(k) = 2\\left(\\tau_{\\mathrm{int},A} - \\frac{1}{2}\\right) = 2\\tau_{\\mathrm{int},A} - 1$. Substituting this into the expression for $\\mathrm{Var}(\\bar{A})$ gives:\n$$\n\\mathrm{Var}(\\bar{A}) \\approx \\frac{\\sigma_A^2}{N} \\left(1 + (2\\tau_{\\mathrm{int},A} - 1)\\right) = \\frac{\\sigma_A^2}{N} (2\\tau_{\\mathrm{int},A})\n$$\nThis is the desired leading-order asymptotic expression:\n$$\n\\mathrm{Var}(\\bar{A}) \\approx \\frac{2 \\sigma_A^2 \\tau_{\\mathrm{int},A}}{N}\n$$\n\n### Part 2: Numerical Calculation\n\nFirst, we find a closed-form expression for $\\tau_{\\mathrm{int},A}$ given the single-exponential decay model for the autocorrelation function, $\\rho_A(k) = \\exp(-k\\,\\Delta t/\\tau_c)$.\nUsing the definition of $\\tau_{\\mathrm{int},A}$:\n$$\n\\tau_{\\mathrm{int},A} = \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho_A(k) = \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\exp\\left(-\\frac{k\\,\\Delta t}{\\tau_c}\\right)\n$$\nThe summation is a geometric series $\\sum_{k=1}^{\\infty} r^k$ with ratio $r = \\exp(-\\Delta t/\\tau_c)$. The sum of this series is $\\frac{r}{1-r}$. Substituting the expression for $r$:\n$$\n\\sum_{k=1}^{\\infty} \\exp\\left(-\\frac{k\\,\\Delta t}{\\tau_c}\\right) = \\frac{\\exp(-\\Delta t/\\tau_c)}{1 - \\exp(-\\Delta t/\\tau_c)} = \\frac{1}{\\exp(\\Delta t/\\tau_c) - 1}\n$$\nSo, the closed-form expression for $\\tau_{\\mathrm{int},A}$ is:\n$$\n\\tau_{\\mathrm{int},A} = \\frac{1}{2} + \\frac{1}{\\exp(\\Delta t/\\tau_c) - 1}\n$$\nThis can be rewritten using hyperbolic functions as $\\tau_{\\mathrm{int},A} = \\frac{1}{2}\\coth\\left(\\frac{\\Delta t}{2\\tau_c}\\right)$.\n\nNext, we compute the numerical value. The given parameters are:\n$\\Delta t = 5\\,\\mathrm{fs}$\n$\\tau_c = 250\\,\\mathrm{fs}$\n$N = 2.0 \\times 10^{5}$\n$\\sigma_A^2 = 1.6\\,u_A^2$\n\nThe argument of the exponential is $\\frac{\\Delta t}{\\tau_c} = \\frac{5}{250} = 0.02$.\nWe calculate $\\tau_{\\mathrm{int},A}$:\n$$\n\\tau_{\\mathrm{int},A} = \\frac{1}{2} + \\frac{1}{\\exp(0.02) - 1} \\approx 0.5 + \\frac{1}{1.02020134 - 1} = 0.5 + \\frac{1}{0.02020134} \\approx 0.5 + 49.501667 \\approx 50.001667\n$$\nNow, we compute $\\mathrm{Var}(\\bar{A})$ using the formula derived in Part 1:\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{2 \\sigma_A^2 \\tau_{\\mathrm{int},A}}{N} = \\frac{2 \\times (1.6\\,u_A^2) \\times (50.001667)}{2.0 \\times 10^{5}}\n$$\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{1.6 \\times 50.001667}{10^5} \\,u_A^2 = \\frac{80.002667}{10^5} \\,u_A^2 = 8.0002667 \\times 10^{-4} \\,u_A^2\n$$\nRounding the final numerical value to four significant figures, we get $8.000 \\times 10^{-4}$.\n\n### Part 3: Strategy to Estimate IAT from Finite Data\n\nEstimating the Integrated Autocorrelation Time, $\\tau_{\\mathrm{int},A}$, from a finite-length production dataset $\\{A_i\\}_{i=1}^N$ requires a careful, statistically principled approach.\n\n1.  **Validation of Stationarity**: Before any analysis, the assumption that the production phase is stationary must be checked. A common practical method is to divide the full time series into a small number of large, equal-sized blocks (e.g., $3$ to $5$ blocks). Calculate the mean and variance of the observable $A$ within each block. If the block means and variances are statistically similar to each other (i.e., they fluctuate around the global mean and variance without any systematic drift or trend), the stationarity assumption is considered reasonable for practical purposes. Plotting a moving average of the data can also provide a visual check for trends.\n\n2.  **Estimating the Autocorrelation Function (ACF)**: The sample autocovariance function $\\hat{C}_A(k)$ for a lag $k$ is estimated from the data:\n    $$\n    \\hat{C}_A(k) = \\frac{1}{N-k} \\sum_{i=1}^{N-k} (A_i - \\bar{A})(A_{i+k} - \\bar{A})\n    $$\n    where $\\bar{A}$ is the sample mean of the entire dataset. The sample ACF is then $\\hat{\\rho}_A(k) = \\hat{C}_A(k) / \\hat{C}_A(0)$.\n\n3.  **The Bias-Variance Trade-off and Truncation**: A naive attempt to estimate $\\tau_{\\mathrm{int},A}$ by summing the estimated ACF to its maximum possible lag, $N-1$, i.e., $\\hat{\\tau}_{\\mathrm{int},A} = \\frac{1}{2} + \\sum_{k=1}^{N-1} \\hat{\\rho}_A(k)$, is flawed. The statistical uncertainty (variance) of the estimator $\\hat{\\rho}_A(k)$ increases as the lag $k$ increases, because fewer data pairs $(N-k)$ are available for its computation. For large $k$, $\\hat{\\rho}_A(k)$ is dominated by noise. Summing these noisy terms adds significant variance to the estimate of $\\tau_{\\mathrm{int},A}$, preventing the sum from converging.\n\n    To manage this, the sum must be truncated at a finite window size $W < N-1$:\n    $$\n    \\hat{\\tau}_{\\mathrm{int},A}(W) = \\frac{1}{2} + \\sum_{k=1}^{W} \\hat{\\rho}_A(k)\n    $$\n    Choosing $W$ involves a critical **bias-variance trade-off**:\n    *   **Small $W$**: If $W$ is too small, the sum is truncated before the true ACF has decayed to zero. This introduces a systematic error (bias), typically an underestimation of $\\tau_{\\mathrm{int},A}$, as the positive tail of the ACF is excluded.\n    *   **Large $W$**: If $W$ is too large, the sum includes noisy estimates $\\hat{\\rho}_A(k)$ from large lags, which increases the statistical error (variance) of the final estimate $\\hat{\\tau}_{\\mathrm{int},A}$.\n\n4.  **A Principled Strategy for Choosing $W$**: A robust strategy is to analyze the behavior of $\\hat{\\tau}_{\\mathrm{int},A}(W)$ as a function of the truncation window $W$. One plots $\\hat{\\tau}_{\\mathrm{int},A}(W)$ versus $W$.\n    *   For small $W$, $\\hat{\\tau}_{\\mathrm{int},A}(W)$ will increase as more positive correlation terms are added.\n    *   Ideally, the curve will reach a plateau where $\\hat{\\tau}_{\\mathrm{int},A}(W)$ becomes relatively constant. This happens because for lags $k$ beyond the true correlation time, the true $\\rho_A(k)$ is close to zero, so adding more terms should not change the sum significantly.\n    *   For very large $W$, noise dominates, and the plot of $\\hat{\\tau}_{\\mathrm{int},A}(W)$ will become erratic and often drift upwards.\n    *   A good choice for the final estimate of $\\tau_{\\mathrm{int},A}$ is the value of $\\hat{\\tau}_{\\mathrm{int},A}(W)$ in this plateau region.\n\n    A self-consistent criterion is to choose $W$ to be several times larger than the estimated $\\tau_{\\mathrm{int},A}$ itself, e.g., choose $W$ such that $W \\ge c \\cdot \\hat{\\tau}_{\\mathrm{int},A}(W)$ for a factor $c$ around $5-6$. One can iterate to find a $W$ that satisfies this condition. This ensures that the summation window is large enough to capture the full decay of the correlations, which by definition happens on the timescale of $\\tau_{\\mathrm{int},A}$. This procedure systematically balances the trade-off by extending the summation as long as it provides meaningful signal without being overwhelmed by statistical noise.",
            "answer": "$$\n\\boxed{8.000 \\times 10^{-4}}\n$$"
        },
        {
            "introduction": "While theoretical guidelines for equilibration are valuable, a robust workflow requires data-driven diagnostics to confirm that a system has reached a stationary state. This exercise transitions from theory to practice by guiding you to build an automated algorithm for detecting equilibration in a time series. By implementing a set of criteria based on the method of batch means, you will create a tool that assesses the stability of running averages and the statistical precision of the estimated mean, formalizing the visual inspection process into a reproducible test .",
            "id": "3755177",
            "problem": "You are studying when to transition from the equilibration phase to the production phase in a univariate time series produced by a multiscale simulation. The central objective is to formalize a stopping criterion for the start of production based on the stabilization of the cumulative average and its confidence intervals estimated by the method of batch means. Your task is to derive a principled decision rule from first principles and implement it.\n\nBegin from the following fundamental bases:\n- The Strong Law of Large Numbers: For a stationary ergodic process with finite mean, the cumulative average $\\bar{X}_n = \\frac{1}{n}\\sum_{t=1}^{n} X_t$ converges almost surely to the true mean as $n \\to \\infty$.\n- The Central Limit Theorem for weakly dependent data (e.g., $\\alpha$-mixing time series) implies an asymptotically normal distribution of the sample mean with variance scaled by an effective long-run variance. The method of batch means approximates this variance consistently by dividing the data into contiguous batches and using the sample variance of batch means.\n\nDesign a criterion that, given a univariate time series $(X_t)_{t=0}^{N-1}$, identifies the smallest equilibration index $t_0$ such that the suffix $S = \\{X_{t_0}, X_{t_0+1}, \\dots, X_{N-1}\\}$ can be considered in production. Your decision rule must be based on:\n- The stabilization of the cumulative averages within $S$ relative to a confidence interval for the mean of $S$ constructed from batch means.\n- A relative precision requirement on the confidence interval half-width.\n- An internal consistency check that rules out unresolved drift within $S$.\n\nDefine the following quantities for any candidate start index $t_0$:\n- Let $n = N - t_0$ be the length of the suffix $S = (S_i)_{i=0}^{n-1}$, where $S_i = X_{t_0+i}$.\n- Let the cumulative running mean within $S$ be $R_k = \\frac{1}{k}\\sum_{i=0}^{k-1} S_i$ for $k \\in \\{1,2,\\dots,n\\}$.\n- Let the overall mean of $S$ be $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=0}^{n-1} S_i$.\n- Partition the first $m b$ points of $S$ into $m$ contiguous batches of equal length $b$. Let $Y_j = \\frac{1}{b}\\sum_{i=0}^{b-1} S_{(j-1)b+i}$ denote the $j$-th batch mean for $j \\in \\{1,2,\\dots,m\\}$.\n- Let $s_Y^2$ be the unbiased sample variance of $\\{Y_1,\\dots,Y_m\\}$.\n- For a target two-sided confidence level $1-\\alpha$, let $q = t_{1-\\alpha/2,\\, m-1}$ be the quantile of the Studentâ€™s $t$ distribution with $m-1$ degrees of freedom. Define the half-width of the confidence interval for the mean of $S$ as $h = q \\sqrt{s_Y^2 / m}$.\n\nImpose the following constraints for a candidate $t_0$ to be accepted:\n- Sufficient data for batch means: $n \\ge m_{\\min} \\, b_{\\min}$, where $m_{\\min}$ and $b_{\\min}$ are minimal counts for batches and batch size, respectively. Use the first $m = \\left\\lfloor \\frac{n}{b_{\\min}} \\right\\rfloor$ batches of size $b_{\\min}$ to compute $\\{Y_j\\}_{j=1}^{m}$. If $m < m_{\\min}$, the candidate is invalid.\n- Relative precision requirement: $\\frac{h}{\\max(|\\hat{\\mu}|, \\delta)} \\le \\epsilon$, where $\\epsilon$ is a user-specified tolerance and $\\delta$ is a small positive number to regularize near-zero means.\n- Internal consistency of halves: If $n$ is even, split $S$ into two halves $S_1$ and $S_2$ of size $n/2$ each, and let their means be $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$. If $n$ is odd, discard the last point and split the first $n-1$ points equally. Require $|\\hat{\\mu}_1 - \\hat{\\mu}_2| \\le h$.\n- Stabilized running mean tail: For every $k \\in \\{\\lceil p n \\rceil, \\lceil p n \\rceil + 1, \\dots, n\\}$ with a user-specified fraction $p \\in (0,1)$, require $|R_k - \\hat{\\mu}| \\le h$.\n\nDefine the accepted equilibration time as the smallest $t_0 \\in \\{0,1,\\dots,N - m_{\\min} b_{\\min}\\}$ satisfying all the above constraints. If none is found, return $N$.\n\nUse the following parameters:\n- Confidence level: $1 - \\alpha$ with $\\alpha = 0.05$.\n- Minimal number of batches: $m_{\\min} = 10$.\n- Minimal batch size: $b_{\\min} = 20$.\n- Relative tolerance: $\\epsilon = 0.05$.\n- Regularization for near-zero means: $\\delta = 10^{-6}$.\n- Tail fraction for stabilization: $p = 0.5$.\n\nAngles for trigonometric functions are in radians.\n\nTest suite. For each test case, you are given a deterministic time series of length $N = 1000$, indexed by $t \\in \\{0,1,\\dots,999\\}$, defined by the formula below. Your program must compute the equilibration index $t_0$ according to the above rule for each case.\n\n- Case A (exponential relaxation to a nonzero mean with small oscillations):\n  - $X_t^{(A)} = 2.0 + 5.0 \\exp(- t / 50.0) + 0.2 \\sin( 2\\pi t / 13.0 )$.\n- Case B (slow linear drift with oscillations; deliberately non-stationary):\n  - $X_t^{(B)} = 0.001 t + 0.2 \\sin( 2\\pi t / 50.0 )$.\n- Case C (approximately stationary from the outset, quasiperiodic):\n  - $X_t^{(C)} = 1.0 + 0.3 \\sin( 2\\pi t / 7.0 ) + 0.1 \\sin( 2\\pi t / 5.0 )$.\n- Case D (damped transient to a negative mean with oscillations):\n  - $X_t^{(D)} = -1.0 + 1.5 \\exp(- t / 20.0) + 0.3 \\cos( 2\\pi t / 15.0 )$.\n\nYour program must:\n- Construct these four series exactly as specified.\n- For each series, scan $t_0$ from $0$ upward and select the smallest $t_0$ that satisfies all constraints. If none satisfy, return $N$ for that case.\n\nFinal output format:\n- Your program should produce a single line of output containing the four resulting integers as a comma-separated list enclosed in square brackets, ordered as $[t_0^{(A)}, t_0^{(B)}, t_0^{(C)}, t_0^{(D)}]$.",
            "solution": "The problem requires the derivation and implementation of a decision rule to determine the equilibration time, denoted by the index $t_0$, in a univariate time series. The core principle is to identify the earliest point in time, $t_0$, such that the remaining portion of the series, the suffix $S = \\{X_{t_0}, X_{t_0+1}, \\dots, X_{N-1}\\}$, can be considered statistically stationary or \"in production.\" The determination is based on a set of quantitative criteria related to the stability of the cumulative average and the precision of the estimated mean of the suffix.\n\nThe validation of the problem statement proceeds as follows:\n- **Givens Extraction**:\n  - Time series: $(X_t)_{t=0}^{N-1}$ where $N=1000$.\n  - Suffix for a candidate equilibration index $t_0$: $S = \\{X_{t_0}, X_{t_0+1}, \\dots, X_{N-1}\\}$. This suffix is treated as a 0-indexed array $S = (S_i)_{i=0}^{n-1}$ where $S_i = X_{t_0+i}$.\n  - Length of suffix: $n = N - t_0$.\n  - Cumulative running mean of $S$: $R_k = \\frac{1}{k}\\sum_{i=0}^{k-1} S_i$ for $k \\in \\{1,2,\\dots,n\\}$.\n  - Overall mean of $S$: $\\hat{\\mu}$.\n  - Batching: The first $m \\cdot b$ points of $S$ are partitioned into $m$ batches of size $b$. The $j$-th batch mean is $Y_j = \\frac{1}{b}\\sum_{i=0}^{b-1} S_{(j-1)b+i}$.\n  - Variance of batch means: $s_Y^2$ is the unbiased sample variance of $\\{Y_1,\\dots,Y_m\\}$.\n  - Confidence interval half-width: $h = q \\sqrt{s_Y^2 / m}$, where $q = t_{1-\\alpha/2,\\, m-1}$ is a quantile from the Student's $t$-distribution.\n  - Parameters: Confidence level $1-\\alpha=0.95$ ($\\alpha=0.05$), minimal batches $m_{\\min}=10$, minimal batch size $b_{\\min}=20$, relative tolerance $\\epsilon=0.05$, regularization $\\delta=10^{-6}$, tail fraction $p=0.5$.\n  - Constraints for acceptance of $t_0$:\n    1. Sufficient data: $n \\ge m_{\\min} b_{\\min}$ and $m = \\lfloor n/b_{\\min} \\rfloor \\ge m_{\\min}$.\n    2. Relative precision: $h / \\max(|\\hat{\\mu}|, \\delta) \\le \\epsilon$.\n    3. Internal consistency: $|\\hat{\\mu}_1 - \\hat{\\mu}_2| \\le h$, where $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$ are means of the two halves of $S$.\n    4. Stabilized tail: $|R_k - \\hat{\\mu}| \\le h$ for all $k \\in \\{\\lceil p n \\rceil, \\dots, n\\}$.\n  - Objective: Find the smallest $t_0 \\in \\{0, 1, \\dots, N - m_{\\min}b_{\\min}\\}$ satisfying all constraints. If none, return $N$.\n  - Test cases: Four deterministic functions $X_t^{(A)}$, $X_t^{(B)}$, $X_t^{(C)}$, $X_t^{(D)}$ are specified.\n\n- **Validation Verdict**: The problem is scientifically grounded in standard time series analysis techniques (batch means method), is mathematically well-posed with precise definitions and constraints, and is objective. It is complete, consistent, and computationally feasible. The problem is therefore deemed **valid**.\n\nThe solution is implemented by creating a function that evaluates all specified constraints for a given candidate start index $t_0$. This function is then called iteratively for $t_0$ starting from $0$ up to the maximum possible value, which is $N - m_{\\min} b_{\\min}$. The first value of $t_0$ that satisfies all constraints is the desired equilibration index.\n\nLet the full time series be an array $X$ of length $N = 1000$, indexed from $0$ to $999$. For each candidate start index $t_0$ in the range $[0, N - m_{\\min} b_{\\min}]$, we perform the following checks on the suffix $S = X[t_0:]$:\n\n1.  **Initialization and Data Sufficiency**:\n    - The length of the suffix is $n = N - t_0$. The loop over $t_0$ is constructed such that $n \\ge m_{\\min} b_{\\min} = 10 \\cdot 20 = 200$, automatically satisfying the first part of the constraint.\n    - We compute the number of available batches of size $b_{\\min}$ as $m = \\lfloor n / b_{\\min} \\rfloor$.\n    - We check if $m \\ge m_{\\min}$. If not, the candidate $t_0$ is rejected.\n\n2.  **Batch Means and Confidence Interval**:\n    - If the data is sufficient, we compute the means of the first $m$ batches. Let the suffix be $S = (S_0, S_1, \\dots, S_{n-1})$. The $j$-th batch mean ($j \\in \\{1,\\dots,m\\}$) is $Y_j = \\frac{1}{b_{\\min}} \\sum_{i=0}^{b_{\\min}-1} S_{(j-1)b_{\\min}+i}$.\n    - The unbiased sample variance of these batch means is calculated as $s_Y^2 = \\frac{1}{m-1} \\sum_{j=1}^{m} (Y_j - \\bar{Y})^2$, where $\\bar{Y} = \\frac{1}{m}\\sum_{j=1}^m Y_j$.\n    - The Student's $t$-distribution quantile is obtained for a $1-\\alpha=0.95$ confidence level and $m-1$ degrees of freedom: $q = t_{1-\\alpha/2,\\, m-1} = t_{0.975,\\, m-1}$.\n    - The confidence interval half-width is then $h = q \\sqrt{s_Y^2 / m}$.\n    - The overall mean of the entire suffix $S$ is $\\hat{\\mu} = \\frac{1}{n} \\sum_{i=0}^{n-1} S_i$.\n\n3.  **Constraint Evaluation**:\n    - **Constraint 1 (Relative Precision)**: We test if the relative half-width is within the tolerance $\\epsilon$. The mean $\\hat{\\mu}$ is regularized to avoid division by zero. The condition is $\\frac{h}{\\max(|\\hat{\\mu}|, \\delta)} \\le \\epsilon$. If this fails, $t_0$ is rejected.\n    - **Constraint 2 (Internal Consistency)**: This check guards against unresolved drift within the suffix. We split the first $n_{even} = 2 \\lfloor n/2 \\rfloor$ points of $S$ into two halves of length $n_{even}/2$. Let their respective means be $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$. We require $|\\hat{\\mu}_1 - \\hat{\\mu}_2| \\le h$. If this fails, $t_0$ is rejected.\n    - **Constraint 3 (Stabilized Running Mean Tail)**: This ensures that the cumulative average has converged. For the specified tail fraction $p=0.5$, we check the running averages $R_k$ for $k$ from $k_{start} = \\lceil p n \\rceil$ to $n$. The condition is that for all such $k$, $|R_k - \\hat{\\mu}| \\le h$, where $R_k = \\frac{1}{k} \\sum_{i=0}^{k-1} S_i$. A cumulative sum approach is efficient for this calculation. If the condition fails for any $k$ in the specified range, $t_0$ is rejected.\n\n4.  **Final Decision**:\n    - A candidate $t_0$ is accepted if it passes all the checks above.\n    - The algorithm iterates through $t_0 = 0, 1, 2, \\dots, N - m_{\\min} b_{\\min}$. The first $t_0$ that is accepted is recorded as the result for that time series.\n    - If the loop completes without any $t_0$ being accepted, the result is assigned the value $N$ as per the problem specification, indicating that the series did not equilibrate according to the criteria within the analyzable window.\n\nThis procedure is applied to each of the four provided time series definitions to compute the final list of equilibration indices.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef check_equilibration(series, t0, N, m_min, b_min, alpha, epsilon, delta, p):\n    \"\"\"\n    Checks if a candidate equilibration index t0 is valid based on the given criteria.\n\n    A 0-based indexing interpretation is used:\n    - The series is X_0, ..., X_{N-1}.\n    - The candidate equilibration index t0 means points X_0, ..., X_{t0-1} are discarded.\n    - The production suffix S starts at index t0, i.e., S = series[t0:].\n    \"\"\"\n    S = series[t0:]\n    n = len(S)\n\n    # Calculate number of batches and check for minimum data\n    m = n // b_min\n    if m < m_min:\n        return False\n\n    # 1. Compute batch means and confidence interval half-width (h)\n    S_for_batches = S[:m * b_min]\n    batches = S_for_batches.reshape((m, b_min))\n    Y = np.mean(batches, axis=1)\n    \n    # Need at least 2 batches to compute variance\n    if m < 2:\n        return False\n        \n    s_Y_2 = np.var(Y, ddof=1)\n    \n    # If variance is zero (e.g., constant series), h will be 0. Avoid issues with sqrt.\n    if s_Y_2 < 0: # Should not happen with real data, but good practice\n        return False\n    \n    # Quantile from Student's t-distribution\n    q = t.ppf(1 - alpha / 2, df=m - 1)\n    \n    # Confidence interval half-width\n    h = q * np.sqrt(s_Y_2 / m)\n    \n    # Overall mean of the suffix S\n    hat_mu = np.mean(S)\n\n    # 2. Relative precision requirement\n    if h / max(abs(hat_mu), delta) > epsilon:\n        return False\n\n    # 3. Internal consistency of halves\n    n_half_len = n // 2\n    S_first_half = S[:n_half_len]\n    S_second_half = S[n_half_len : 2 * n_half_len]\n    mu1 = np.mean(S_first_half)\n    mu2 = np.mean(S_second_half)\n    if abs(mu1 - mu2) > h:\n        return False\n\n    # 4. Stabilized running mean tail\n    k_start = int(np.ceil(p * n))\n    \n    # Cumulative sum for efficient calculation of running means\n    cum_S = np.cumsum(S)\n    \n    # Check tail from k_start to n\n    k_range = np.arange(k_start, n + 1)\n    # R_k = cum_S[k-1] / k (adjusting for 0-based index of cum_S)\n    R_k_values = cum_S[k_range - 1] / k_range\n    \n    if np.any(np.abs(R_k_values - hat_mu) > h):\n        return False\n        \n    return True\n\ndef solve():\n    # Parameters\n    N = 1000\n    alpha = 0.05\n    m_min = 10\n    b_min = 20\n    epsilon = 0.05\n    delta = 1e-6\n    p = 0.5\n    \n    params = {\n        'N': N, 'm_min': m_min, 'b_min': b_min, \n        'alpha': alpha, 'epsilon': epsilon, 'delta': delta, 'p': p\n    }\n\n    t_indices = np.arange(N)\n\n    # Test cases generation\n    series_A = 2.0 + 5.0 * np.exp(-t_indices / 50.0) + 0.2 * np.sin(2 * np.pi * t_indices / 13.0)\n    series_B = 0.001 * t_indices + 0.2 * np.sin(2 * np.pi * t_indices / 50.0)\n    series_C = 1.0 + 0.3 * np.sin(2 * np.pi * t_indices / 7.0) + 0.1 * np.sin(2 * np.pi * t_indices / 5.0)\n    series_D = -1.0 + 1.5 * np.exp(-t_indices / 20.0) + 0.3 * np.cos(2 * np.pi * t_indices / 15.0)\n\n    test_series = [series_A, series_B, series_C, series_D]\n    results = []\n\n    # Maximum t0 to check ensures that the suffix has at least m_min * b_min points\n    max_t0 = N - m_min * b_min\n    \n    for series in test_series:\n        found_t0 = False\n        for t0 in range(max_t0 + 1):\n            if check_equilibration(series, t0, **params):\n                results.append(t0)\n                found_t0 = True\n                break\n        if not found_t0:\n            results.append(N)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}