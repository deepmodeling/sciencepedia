{
    "hands_on_practices": [
        {
            "introduction": "The Green-Kubo relations provide a powerful theoretical link between macroscopic transport coefficients and microscopic fluctuations at equilibrium. This exercise puts theory into practice by guiding you through the foundational steps of computing thermal conductivity from a heat current time series, a typical output of a Molecular Dynamics (MD) simulation. You will implement the core components of the calculation, including the construction of the heat flux autocorrelation function (HFACF) and its numerical integration, to turn raw simulation data into a physically meaningful thermal conductivity value.",
            "id": "3749267",
            "problem": "You are given discrete, uniformly sampled time series of the three Cartesian components of a molecular dynamics heat current vector for an isotropic material. From first principles of equilibrium statistical mechanics and linear response theory, the scalar thermal conductivity for an isotropic material can be determined by constructing the heat flux autocorrelation function from these samples and integrating it over time up to a prescribed cutoff time. Your task is to write a complete, runnable program that, for each specified test case, constructs the heat flux autocorrelation function from the provided discrete samples, integrates it using the trapezoidal rule up to the cutoff time, and converts the result into a scalar thermal conductivity expressed in $\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$. All necessary physical constants and parameters are specified below.\n\nThe input assumptions and definitions you must follow are:\n- The heat current vector samples are the Cartesian components $(J_x(t_n), J_y(t_n), J_z(t_n))$, where $t_n = n\\,\\Delta t$ with uniform time step $\\Delta t$ for $n=0,1,\\dots,N-1$. Each component time series must be treated by first subtracting its sample mean to ensure stationarity.\n- The heat current vector here is the extensive quantity commonly output by molecular dynamics packages, equal to volume times the microscopic heat flux density, and thus has units of $\\mathrm{W}\\cdot\\mathrm{m}$.\n- For an isotropic material, the scalar heat flux autocorrelation function at discrete lag $k$ is defined as the equal-weight average of the three component autocorrelations of the mean-removed series. Use an unbiased discrete-time estimator for each component autocorrelation at lag $k$, meaning you must divide the lag-$k$ correlation sum by $(N-k)$.\n- The trapezoidal integration must be carried out over the discrete times $t_k = k\\,\\Delta t$ from $t_0=0$ up to $t_{K^\\ast} = \\min\\{t_c,\\,(N-1)\\,\\Delta t\\}$, where $t_c$ is the chosen cutoff time for the test case. If $t_c < \\Delta t$, the integral should reduce to the value over a zero-length interval (i.e., zero). If $t_c$ lies strictly between grid points, use only grid points up to the largest $t_k \\le t_c$ for the trapezoidal rule.\n- Use the Boltzmann constant $k_\\mathrm{B} = 1.380649\\times 10^{-23}\\ \\mathrm{J}/\\mathrm{K}$.\n\nFor each test case, the volume $V$ (in $\\mathrm{m}^3$), the absolute temperature $T$ (in $\\mathrm{K}$), the time step $\\Delta t$ (in $\\mathrm{s}$), the number of samples $N$, the cutoff $t_c$ (in $\\mathrm{s}$), and the generative formulae for $(J_x,J_y,J_z)$ (in $\\mathrm{W}\\cdot\\mathrm{m}$) are specified. You must generate the time series from these definitions, construct the unbiased discrete-time scalar heat flux autocorrelation function as described, perform the trapezoidal integration up to $t_c$, and convert this integrated correlation into the scalar thermal conductivity in $\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$ using the appropriate scaling implied by equilibrium fluctuation-dissipation relations for isotropic media.\n\nPhysical units and numerical conventions:\n- Express the final conductivities in $\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$.\n- Any angular frequency $\\omega$ is in radians per second, and any angle used implicitly in trigonometric definitions is in radians.\n- All floating-point outputs must be formatted in scientific notation with exactly six significant digits.\n\nTest suite of parameter sets:\n- Case A (nominal exponential relaxation):\n  - $V = (4.0\\times 10^{-9})^3\\ \\mathrm{m}^3$,\n  - $T = 300\\ \\mathrm{K}$,\n  - $\\Delta t = 5.0\\times 10^{-15}\\ \\mathrm{s}$,\n  - $N = 4096$,\n  - $t_c = 5.0\\times 10^{-12}\\ \\mathrm{s}$.\n  - Define $A = 3.0\\times 10^{-15}\\ \\mathrm{W}\\cdot\\mathrm{m}$ and $\\tau = 1.0\\times 10^{-12}\\ \\mathrm{s}$. For $n=0,1,\\dots,N-1$ and $t_n = n\\,\\Delta t$, set\n    - $J_x(t_n) = A\\,\\exp(-t_n/\\tau)$,\n    - $J_y(t_n) = A\\,\\exp(-t_n/\\tau)$,\n    - $J_z(t_n) = A\\,\\exp(-t_n/\\tau)$.\n- Case B (damped oscillatory relaxation):\n  - $V = (5.0\\times 10^{-9})^3\\ \\mathrm{m}^3$,\n  - $T = 300\\ \\mathrm{K}$,\n  - $\\Delta t = 2.0\\times 10^{-15}\\ \\mathrm{s}$,\n  - $N = 4096$,\n  - $t_c = 2.0\\times 10^{-12}\\ \\mathrm{s}$.\n  - Define $A = 2.0\\times 10^{-15}\\ \\mathrm{W}\\cdot\\mathrm{m}$, $\\tau = 5.0\\times 10^{-13}\\ \\mathrm{s}$, and $\\omega = 2\\pi\\times 1.0\\times 10^{12}\\ \\mathrm{rad}/\\mathrm{s}$. For $n=0,1,\\dots,N-1$ and $t_n = n\\,\\Delta t$, set\n    - $J_x(t_n) = A\\,\\cos(\\omega t_n)\\,\\exp(-t_n/\\tau)$,\n    - $J_y(t_n) = A\\,\\sin(\\omega t_n)\\,\\exp(-t_n/\\tau)$,\n    - $J_z(t_n) = 0$.\n- Case C (impulse-like initial current with minimal window):\n  - $V = 1.0\\times 10^{-27}\\ \\mathrm{m}^3$,\n  - $T = 500\\ \\mathrm{K}$,\n  - $\\Delta t = 1.0\\times 10^{-14}\\ \\mathrm{s}$,\n  - $N = 8$,\n  - $t_c = 1.0\\times 10^{-14}\\ \\mathrm{s}$.\n  - Define $A = 1.0\\times 10^{-14}\\ \\mathrm{W}\\cdot\\mathrm{m}$. For $n=0,1,\\dots,N-1$ set\n    - $J_x(t_0) = A$ and $J_x(t_n) = 0$ for $n\\ge 1$,\n    - $J_y(t_n) = 0$ for all $n$,\n    - $J_z(t_n) = 0$ for all $n$.\n- Case D (short correlation within long cutoff; cutoff beyond available window should be clipped automatically):\n  - $V = 5.0\\times 10^{-27}\\ \\mathrm{m}^3$,\n  - $T = 1000\\ \\mathrm{K}$,\n  - $\\Delta t = 1.0\\times 10^{-14}\\ \\mathrm{s}$,\n  - $N = 100$,\n  - $t_c = 1.0\\times 10^{-11}\\ \\mathrm{s}$.\n  - Define $A = 1.5\\times 10^{-14}\\ \\mathrm{W}\\cdot\\mathrm{m}$ and $\\tau = 2.0\\times 10^{-13}\\ \\mathrm{s}$. For $n=0,1,\\dots,N-1$ and $t_n = n\\,\\Delta t$, set\n    - $J_x(t_n) = A\\,\\exp(-t_n/\\tau)$,\n    - $J_y(t_n) = A\\,\\exp(-t_n/\\tau)$,\n    - $J_z(t_n) = A\\,\\exp(-t_n/\\tau)$.\n\nFinal output requirement:\n- Your program should produce a single line of output containing the scalar thermal conductivities for the four cases, in the order A, B, C, D, formatted as a comma-separated list enclosed in square brackets, with each value in scientific notation with exactly six significant digits, for example, $[1.234560\\mathrm{e}{-02},3.456780\\mathrm{e}{+00},\\dots]$.",
            "solution": "The problem requires the calculation of the scalar thermal conductivity, $\\kappa$, for an isotropic material from discrete time series data of the heat current vector, $\\mathbf{J}(t)$, obtained from a molecular dynamics simulation. The theoretical foundation for this calculation is the Green-Kubo relation, a result from linear response theory in equilibrium statistical mechanics.\n\nThe Green-Kubo formula relates a macroscopic transport coefficient to the time integral of an equilibrium time-autocorrelation function of the corresponding microscopic flux. For thermal conductivity, the relevant flux is the heat current. The thermal conductivity tensor, $\\kappa_{\\alpha\\beta}$, is given by:\n$$ \\kappa_{\\alpha\\beta} = \\frac{1}{V k_\\mathrm{B} T^2} \\int_0^\\infty \\langle J_\\alpha(0) J_\\beta(t) \\rangle dt $$\nwhere $V$ is the system volume, $k_\\mathrm{B}$ is the Boltzmann constant, $T$ is the absolute temperature, and $\\langle J_\\alpha(0) J_\\beta(t) \\rangle$ is the ensemble-averaged time-autocorrelation function of the components of the extensive heat current vector $\\mathbf{J}$. The units of $\\mathbf{J}$ are given as $\\mathrm{W}\\cdot\\mathrm{m}$.\n\nFor an isotropic material, the thermal conductivity tensor is diagonal and its diagonal elements are equal: $\\kappa_{\\alpha\\beta} = \\kappa \\delta_{\\alpha\\beta}$. The scalar thermal conductivity $\\kappa$ can be expressed as the average of the diagonal components:\n$$ \\kappa = \\frac{1}{3} \\sum_{\\alpha=x,y,z} \\kappa_{\\alpha\\alpha} = \\frac{1}{3V k_\\mathrm{B} T^2} \\int_0^\\infty \\langle \\mathbf{J}(0) \\cdot \\mathbf{J}(t) \\rangle dt $$\nThe problem defines the scalar heat flux autocorrelation function (HFACF) as the average of the component autocorrelations, which is consistent with the above:\n$$ C(t) = \\frac{1}{3} \\sum_{\\alpha=x,y,z} \\langle J_\\alpha(0) J_\\alpha(t) \\rangle = \\frac{1}{3} \\langle \\mathbf{J}(0) \\cdot \\mathbf{J}(t) \\rangle $$\nSubstituting this definition into the expression for $\\kappa$ yields the primary equation for this task:\n$$ \\kappa = \\frac{1}{V k_\\mathrm{B} T^2} \\int_0^\\infty C(t) dt $$\nThe solution involves numerically evaluating this expression based on the provided discrete time series data and parameters. The procedure is as follows:\n\n1.  **Data Preparation**: For each Cartesian component of the heat current, $J_\\alpha(t_n)$, where $t_n = n\\,\\Delta t$ for $n=0, 1, \\dots, N-1$, the sample mean is subtracted to obtain a zero-mean series, $J'_\\alpha(t_n)$. This step is crucial for analyzing fluctuations around the equilibrium average, which is assumed to be zero.\n    $$ \\bar{J}_\\alpha = \\frac{1}{N} \\sum_{n=0}^{N-1} J_\\alpha(t_n) $$\n    $$ J'_\\alpha(t_n) = J_\\alpha(t_n) - \\bar{J}_\\alpha $$\n\n2.  **Discrete Autocorrelation Calculation**: The ensemble average $\\langle \\cdot \\rangle$ in the definition of $C(t)$ is replaced by a time average, invoking the ergodic hypothesis. The continuous autocorrelation $C_{\\alpha\\alpha}(t)$ for each component is estimated at discrete lag times $t_k = k\\,\\Delta t$ using the specified unbiased estimator:\n    $$ C_{\\alpha\\alpha}(t_k) = \\frac{1}{N-k} \\sum_{n=0}^{N-1-k} J'_\\alpha(t_n) J'_\\alpha(t_{n+k}) $$\n    The division by $N-k$, the number of products in the sum, corrects for the bias that would arise from dividing by $N$.\n\n3.  **Scalar HFACF Construction**: The scalar HFACF, $C(t_k)$, is computed by averaging the three component autocorrelations, as prescribed for an isotropic material:\n    $$ C(t_k) = \\frac{1}{3} \\left[ C_{xx}(t_k) + C_{yy}(t_k) + C_{zz}(t_k) \\right] $$\n\n4.  **Numerical Integration**: The integral of the HFACF is approximated numerically. The integration is performed from $t=0$ up to an effective cutoff time, $t_{K^\\ast} = \\min\\{t_c, (N-1)\\Delta t\\}$, where $t_c$ is a given cutoff and $(N-1)\\Delta t$ is the maximum possible lag time from a series of length $N$. The number of lag steps for the integration is $K_{max} = \\lfloor t_{K^\\ast} / \\Delta t \\rfloor$. The integral, $I$, is calculated using the trapezoidal rule over the discrete points $(t_k, C(t_k))$ from $k=0$ to $k=K_{max}$:\n    $$ I = \\int_0^{t_{K_{max}}} C(t) dt \\approx \\sum_{k=0}^{K_{max}-1} \\frac{C(t_k) + C(t_{k+1})}{2} \\Delta t $$\n    If $t_c < \\Delta t$, then $K_{max}=0$, the integration interval contains only the point $t_0=0$, and the integral evaluates to $0$.\n\n5.  **Final Conductivity Calculation**: Finally, the scalar thermal conductivity $\\kappa$ is computed by applying the prefactor from the Green-Kubo formula:\n    $$ \\kappa = \\frac{I}{V k_\\mathrm{B} T^2} $$\n    The units of the resulting $\\kappa$ are $\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$, as derived from the SI units of all quantities involved: $I$ in $(\\mathrm{W}\\cdot\\mathrm{m})^2 \\cdot \\mathrm{s}$, $V$ in $\\mathrm{m}^3$, $k_\\mathrm{B}$ in $\\mathrm{J}/\\mathrm{K}$, and $T$ in $\\mathrm{K}$.\n\nThis systematic, principle-based procedure is implemented for each test case to determine its thermal conductivity.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates thermal conductivity from molecular dynamics heat current data\n    using the Green-Kubo relations.\n    \"\"\"\n    # Physical constant\n    k_B = 1.380649e-23  # Boltzmann constant in J/K\n\n    # Test suite of parameter sets\n    test_cases = [\n        # Case A: Nominal exponential relaxation\n        {\n            'V': (4.0e-9)**3,\n            'T': 300.0,\n            'dt': 5.0e-15,\n            'N': 4096,\n            'tc': 5.0e-12,\n            'J_func': lambda t, p: (\n                p['A'] * np.exp(-t / p['tau']),\n                p['A'] * np.exp(-t / p['tau']),\n                p['A'] * np.exp(-t / p['tau'])\n            ),\n            'J_params': {'A': 3.0e-15, 'tau': 1.0e-12}\n        },\n        # Case B: Damped oscillatory relaxation\n        {\n            'V': (5.0e-9)**3,\n            'T': 300.0,\n            'dt': 2.0e-15,\n            'N': 4096,\n            'tc': 2.0e-12,\n            'J_func': lambda t, p: (\n                p['A'] * np.cos(p['w'] * t) * np.exp(-t / p['tau']),\n                p['A'] * np.sin(p['w'] * t) * np.exp(-t / p['tau']),\n                np.zeros_like(t)\n            ),\n            'J_params': {'A': 2.0e-15, 'tau': 5.0e-13, 'w': 2 * np.pi * 1.0e12}\n        },\n        # Case C: Impulse-like initial current\n        {\n            'V': 1.0e-27,\n            'T': 500.0,\n            'dt': 1.0e-14,\n            'N': 8,\n            'tc': 1.0e-14,\n            'J_func': lambda t, p: (\n                np.array([p['A']] + [0.0] * (len(t) - 1)),\n                np.zeros_like(t),\n                np.zeros_like(t)\n            ),\n            'J_params': {'A': 1.0e-14}\n        },\n        # Case D: Short correlation, long cutoff\n        {\n            'V': 5.0e-27,\n            'T': 1000.0,\n            'dt': 1.0e-14,\n            'N': 100,\n            'tc': 1.0e-11,\n            'J_func': lambda t, p: (\n                p['A'] * np.exp(-t / p['tau']),\n                p['A'] * np.exp(-t / p['tau']),\n                p['A'] * np.exp(-t / p['tau'])\n            ),\n            'J_params': {'A': 1.5e-14, 'tau': 2.0e-13}\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        V = case['V']\n        T = case['T']\n        dt = case['dt']\n        N = case['N']\n        tc = case['tc']\n        J_func = case['J_func']\n        J_params = case['J_params']\n\n        # 1. Generate time series for heat current components\n        t_series = np.arange(N) * dt\n        Jx, Jy, Jz = J_func(t_series, J_params)\n\n        # 2. Subtract sample mean from each component\n        Jx_prime = Jx - np.mean(Jx)\n        Jy_prime = Jy - np.mean(Jy)\n        Jz_prime = Jz - np.mean(Jz)\n        \n        # 3. Determine integration range based on cutoff time and data length\n        t_max_data = (N - 1) * dt\n        t_integration_limit = min(tc, t_max_data)\n        \n        # Determine max lag index K_max for correlation and integration\n        if t_integration_limit  dt:\n            K_max = 0\n        else:\n            K_max = int(np.floor(t_integration_limit / dt))\n\n        # 4. Calculate the scalar Heat Flux Autocorrelation Function (HFACF)\n        hfacf_values = np.zeros(K_max + 1)\n        \n        for k in range(K_max + 1):\n            # Unbiased estimator requires division by N-k\n            # If N-k is 0 or negative, this loop structure avoids it.\n            if N > k:\n                # Calculate component autocorrelations C_xx, C_yy, C_zz at lag k\n                # using dot product on sliced arrays for efficiency\n                C_xx_k = np.dot(Jx_prime[:N - k], Jx_prime[k:]) / (N - k)\n                C_yy_k = np.dot(Jy_prime[:N - k], Jy_prime[k:]) / (N - k)\n                C_zz_k = np.dot(Jz_prime[:N - k], Jz_prime[k:]) / (N - k)\n                \n                # Average for isotropic scalar HFACF\n                hfacf_values[k] = (C_xx_k + C_yy_k + C_zz_k) / 3.0\n\n        # 5. Integrate the HFACF using the trapezoidal rule\n        # The times corresponding to the HFACF values\n        hfacf_times = np.arange(K_max + 1) * dt\n        integral = np.trapz(hfacf_values, x=hfacf_times)\n        \n        # 6. Calculate the final thermal conductivity\n        prefactor = 1.0 / (V * k_B * T**2)\n        kappa = prefactor * integral\n        \n        results.append(f\"{kappa:.6e}\")\n\n    # Print results in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A critical challenge in MD simulations is that we study a finite system to understand a macroscopic material, introducing potential artifacts. This practice addresses the crucial issue of finite-size effects on thermal conductivity calculated via the Green-Kubo method. Guided by principles of hydrodynamics, which predict a specific scaling of conductivity $\\kappa(L)$ with system size $L$, you will learn to perform a systematic finite-size analysis to extrapolate your results to the thermodynamic limit ($L \\to \\infty$) and obtain the true bulk conductivity.",
            "id": "3749285",
            "problem": "You are given a task grounded in Linear Response Theory and Continuum Hydrodynamics to determine the thermodynamic-limit thermal conductivity from Molecular Dynamics (MD) data using the Green-Kubo (GK) formulation. Start from the following foundational base: the GK conductivity is the time integral of the properly normalized heat current autocorrelation function, and in a finite periodic cell of linear size $L$ the hydrodynamic long-time tail of the integrand is truncated by the lowest allowed hydrodynamic mode at a cutoff time $t_c(L)$. In three spatial dimensions, the long-time tail of the GK integrand behaves as a power law originating from mode-coupling to diffusive shear modes. The cutoff time obeys $t_c(L) = L^2 / \\left(4 \\pi^2 D\\right)$, where $D$ is a positive transport coefficient with units of $\\mathrm{nm}^2/\\mathrm{ps}$. Use only these bases and the provided parametric forms; do not assume any additional formula that directly gives the final finite-size scaling relationship.\n\nYour program must, for each provided test case, compute the finite-size Green-Kubo conductivity by integrating the specified integrand model, perform a regression in the inverse cell size to extrapolate the thermodynamic-limit conductivity, and compare the fitted finite-size scaling with the hydrodynamic expectation computed from the same parameters.\n\nModel for the GK integrand:\n- Let the GK integrand be $K(t) = K_{\\mathrm{fast}}(t) + K_{\\mathrm{tail}}(t)$, with\n  - $K_{\\mathrm{fast}}(t) = a \\, e^{-t/t_1} + b \\, e^{-t/t_2}$ for $t \\ge 0$,\n  - $K_{\\mathrm{tail}}(t) = C \\, t^{-3/2}$ for $t \\ge t_0$, and $K_{\\mathrm{tail}}(t) = 0$ for $0 \\le t  t_0$.\n- The finite-size cutoff time obeys $t_c(L) = L^2/\\left(4 \\pi^2 D\\right)$. For times $t  t_c(L)$, the contribution of the tail is suppressed by hydrodynamic mode quantization and may be taken as negligible.\n\nMathematical tasks for each test case:\n1. Compute the fast contribution to the conductivity as the time integral $\\int_0^\\infty K_{\\mathrm{fast}}(t) \\, dt$, which yields a finite, positive value with units of $\\mathrm{W} \\, \\mathrm{m}^{-1} \\, \\mathrm{K}^{-1}$.\n2. Compute the tail contribution for the infinite system as the time integral $\\int_{t_0}^\\infty C \\, t^{-3/2} \\, dt$.\n3. For a finite cell of size $L$, compute the tail contribution as $\\int_{t_0}^{t_c(L)} C \\, t^{-3/2} \\, dt$ if $t_c(L) \\ge t_0$, otherwise the tail contributes zero. The total finite-size conductivity is then the sum of the fast contribution and the finite-size tail integral. All conductivities must be expressed in $\\mathrm{W} \\, \\mathrm{m}^{-1} \\, \\mathrm{K}^{-1}$, rounded to six decimal places when reported.\n4. Assemble data pairs $\\left(x_i, y_i\\right)$ with $x_i = 1/L_i$ in $\\mathrm{nm}^{-1}$ and $y_i = \\kappa(L_i)$ in $\\mathrm{W} \\, \\mathrm{m}^{-1} \\, \\mathrm{K}^{-1}$ for the provided sizes $L_i$. Perform an unconstrained linear least-squares fit of the form $y = \\alpha + \\beta x$. Report the fitted thermodynamic-limit conductivity as $\\alpha$ and the fitted finite-size coefficient as $s_{\\mathrm{fit}} = -\\beta$ (so that $\\kappa(L) \\approx \\alpha - s_{\\mathrm{fit}}/L$).\n5. Using only the given parametric model and the hydrodynamic cutoff relation, compute the hydrodynamic prediction for the thermodynamic-limit conductivity and the leading finite-size coefficient from the same parameters. Express both in $\\mathrm{W} \\, \\mathrm{m}^{-1} \\, \\mathrm{K}^{-1}$, rounded to six decimals.\n\nTest suite:\n- Case A:\n  - Parameters: $a = 0.25$, $b = 0.10$, $t_1 = 1.5 \\, \\mathrm{ps}$, $t_2 = 6.0 \\, \\mathrm{ps}$, $C = 0.50$, $t_0 = 2.0 \\, \\mathrm{ps}$, $D = 0.08 \\, \\mathrm{nm}^2/\\mathrm{ps}$.\n  - Sizes: $L \\in \\{5, 8, 12, 20\\} \\, \\mathrm{nm}$.\n- Case B:\n  - Parameters: $a = 0.35$, $b = 0.05$, $t_1 = 1.0 \\, \\mathrm{ps}$, $t_2 = 8.0 \\, \\mathrm{ps}$, $C = 0.40$, $t_0 = 5.0 \\, \\mathrm{ps}$, $D = 0.05 \\, \\mathrm{nm}^2/\\mathrm{ps}$.\n  - Sizes: $L \\in \\{3, 4, 6, 10\\} \\, \\mathrm{nm}$.\n- Case C:\n  - Parameters: $a = 0.20$, $b = 0.15$, $t_1 = 2.0 \\, \\mathrm{ps}$, $t_2 = 3.0 \\, \\mathrm{ps}$, $C = 0.30$, $t_0 = 1.0 \\, \\mathrm{ps}$, $D = 0.20 \\, \\mathrm{nm}^2/\\mathrm{ps}$.\n  - Sizes: $L \\in \\{6, 9, 15, 30\\} \\, \\mathrm{nm}$.\n\nUnits and required outputs:\n- All conductivities and coefficients must be reported in $\\mathrm{W} \\, \\mathrm{m}^{-1} \\, \\mathrm{K}^{-1}$.\n- For each case, output a list of four floats: $\\left[\\kappa_{\\infty,\\mathrm{fit}}, \\kappa_{\\infty,\\mathrm{hydro}}, s_{\\mathrm{fit}}, s_{\\mathrm{hydro}}\\right]$, each rounded to six decimals, where $\\kappa_{\\infty,\\mathrm{fit}}$ and $s_{\\mathrm{fit}}$ come from the regression, and $\\kappa_{\\infty,\\mathrm{hydro}}$ and $s_{\\mathrm{hydro}}$ come from direct computation using the parametric model and the hydrodynamic cutoff relation.\n- Your program should produce a single line of output containing the results for all three cases as a comma-separated list of lists enclosed in square brackets, for example: $\\left[\\left[r_{1}, r_{2}, r_{3}, r_{4}\\right], \\left[\\dots\\right], \\left[\\dots\\right]\\right]$.\n\nNotes:\n- Treat $\\pi$ as the mathematical constant with its usual meaning.\n- Angles are not used in this problem.",
            "solution": "The problem requires the determination of the thermodynamic-limit thermal conductivity, $\\kappa_{\\infty}$, from a model of molecular dynamics (MD) data for finite-sized systems. This is a quintessential problem in computational statistical mechanics, involving the Green-Kubo (GK) relations and the theory of finite-size effects in transport coefficients. The solution will proceed by first deriving the analytical expressions for the quantities of interest from the provided model, and then implementing a numerical procedure to compute these quantities and perform a linear regression for the given test cases.\n\nThe foundation of the problem is the GK formula for thermal conductivity, $\\kappa$, which expresses it as the time integral of the equilibrium heat current autocorrelation function (HCACF). The problem provides a parametric model for the GK integrand, which we denote as $K(t)$:\n$$\nK(t) = K_{\\mathrm{fast}}(t) + K_{\\mathrm{tail}}(t)\n$$\nwhere\n$$\nK_{\\mathrm{fast}}(t) = a \\, e^{-t/t_1} + b \\, e^{-t/t_2} \\quad \\text{for } t \\ge 0\n$$\n$$\nK_{\\mathrm{tail}}(t) = \\begin{cases} C \\, t^{-3/2}  \\text{for } t \\ge t_0 \\\\ 0  \\text{for } 0 \\le t  t_0 \\end{cases}\n$$\nThe parameters $a$, $b$, $t_1$, $t_2$, $C$, and $t_0$ are given for each test case. The units of $a$, $b$, and $C$ are implicitly $\\mathrm{W} \\, \\mathrm{m}^{-1} \\, \\mathrm{K}^{-1} \\cdot \\mathrm{ps}^{-1}$, such that upon integration over time in $\\mathrm{ps}$, the result is a conductivity in $\\mathrm{W} \\, \\mathrm{m}^{-1} \\, \\mathrm{K}^{-1}$.\n\nFirst, we determine the \"hydrodynamic prediction\" for the thermal conductivity in the thermodynamic limit ($L \\to \\infty$) and the leading finite-size correction coefficient. In an infinite system, the conductivity $\\kappa_{\\infty,\\mathrm{hydro}}$ is obtained by integrating $K(t)$ over all time from $0$ to $\\infty$.\n\nThe contribution from the fast part is:\n$$\n\\kappa_{\\mathrm{fast}} = \\int_0^\\infty K_{\\mathrm{fast}}(t) \\, dt = \\int_0^\\infty (a \\, e^{-t/t_1} + b \\, e^{-t/t_2}) \\, dt\n$$\n$$\n\\kappa_{\\mathrm{fast}} = \\left[ -a t_1 e^{-t/t_1} - b t_2 e^{-t/t_2} \\right]_0^{\\infty} = (0 - (-a t_1 - b t_2)) = a t_1 + b t_2\n$$\n\nThe contribution from the tail part in an infinite system is:\n$$\n\\kappa_{\\mathrm{tail},\\infty} = \\int_{t_0}^\\infty K_{\\mathrm{tail}}(t) \\, dt = \\int_{t_0}^\\infty C t^{-3/2} \\, dt\n$$\n$$\n\\kappa_{\\mathrm{tail},\\infty} = C \\left[ -2 t^{-1/2} \\right]_{t_0}^{\\infty} = C (0 - (-2 t_0^{-1/2})) = \\frac{2C}{\\sqrt{t_0}}\n$$\n\nThe total thermodynamic-limit conductivity, from the hydrodynamic model, is the sum of these two contributions:\n$$\n\\kappa_{\\infty,\\mathrm{hydro}} = \\kappa_{\\mathrm{fast}} + \\kappa_{\\mathrm{tail},\\infty} = a t_1 + b t_2 + \\frac{2C}{\\sqrt{t_0}}\n$$\n\nFor a finite system of size $L$, hydrodynamic theory dictates that the long-time behavior of correlation functions is truncated. The long-wavelength modes that give rise to the $t^{-3/2}$ tail are quantized in a periodic box. The slowest mode corresponds to a wavelength of $L$, leading to a cutoff in the time integral at $t_c(L) = L^2 / (4 \\pi^2 D)$, where $D$ is a relevant transport coefficient. The finite-size conductivity, $\\kappa(L)$, is thus obtained by integrating the tail only up to this cutoff time:\n$$\n\\kappa(L) = \\kappa_{\\mathrm{fast}} + \\int_{t_0}^{t_c(L)} C t^{-3/2} \\, dt\n$$\nThis is valid under the condition that $t_c(L) \\ge t_0$. If $t_c(L)  t_0$, the tail part does not contribute, and $\\kappa(L) = \\kappa_{\\mathrm{fast}}$.\nAssuming $t_c(L) \\ge t_0$, the integral evaluates to:\n$$\n\\int_{t_0}^{t_c(L)} C t^{-3/2} \\, dt = C \\left[ -2 t^{-1/2} \\right]_{t_0}^{t_c(L)} = 2C \\left( t_0^{-1/2} - t_c(L)^{-1/2} \\right)\n$$\nSubstituting this into the expression for $\\kappa(L)$:\n$$\n\\kappa(L) = (a t_1 + b t_2) + 2C \\left( \\frac{1}{\\sqrt{t_0}} - \\frac{1}{\\sqrt{t_c(L)}} \\right)\n$$\nWe can recognize the first two terms as $\\kappa_{\\infty,\\mathrm{hydro}}$. Substituting the expression for $t_c(L)$:\n$$\n\\kappa(L) = \\kappa_{\\infty,\\mathrm{hydro}} - \\frac{2C}{\\sqrt{L^2 / (4 \\pi^2 D)}} = \\kappa_{\\infty,\\mathrm{hydro}} - \\frac{2C}{(L / (2 \\pi \\sqrt{D}))}\n$$\n$$\n\\kappa(L) = \\kappa_{\\infty,\\mathrm{hydro}} - \\left( 4 \\pi C \\sqrt{D} \\right) \\frac{1}{L}\n$$\nThis equation is of the form $\\kappa(L) \\approx \\alpha - s/L$. This provides the analytical or \"hydrodynamic\" prediction for the leading finite-size coefficient:\n$$\ns_{\\mathrm{hydro}} = 4 \\pi C \\sqrt{D}\n$$\n\nThe numerical task is to calculate $\\kappa(L_i)$ for a set of system sizes $\\{L_i\\}$ and then perform a linear least-squares fit to the data pairs $(x_i, y_i) = (1/L_i, \\kappa(L_i))$ using the model $y = \\alpha + \\beta x$. The fitted parameters are identified as:\n- $\\kappa_{\\infty,\\mathrm{fit}} = \\alpha$ (the y-intercept, which corresponds to $1/L \\to 0$, i.e., $L \\to \\infty$).\n- $s_{\\mathrm{fit}} = -\\beta$ (the negative of the slope).\n\nFor each test case, we follow these steps:\n1.  Calculate the analytical values $\\kappa_{\\infty,\\mathrm{hydro}}$ and $s_{\\mathrm{hydro}}$ using the derived formulas.\n2.  For each given size $L_i$:\n    a. Calculate $t_c(L_i) = L_i^2 / (4 \\pi^2 D)$.\n    b. Calculate $\\kappa_{\\mathrm{fast}} = a t_1 + b t_2$.\n    c. If $t_c(L_i) \\ge t_0$, calculate $\\kappa_{\\mathrm{tail}}(L_i) = 2C(t_0^{-1/2} - t_c(L_i)^{-1/2})$. The total conductivity is $\\kappa(L_i) = \\kappa_{\\mathrm{fast}} + \\kappa_{\\mathrm{tail}}(L_i)$.\n    d. If $t_c(L_i)  t_0$, $\\kappa(L_i) = \\kappa_{\\mathrm{fast}}$.\n    e. Store the pair $(1/L_i, \\kappa(L_i))$.\n3.  Perform a linear regression on the collected data pairs to find the slope $\\beta$ and intercept $\\alpha$. This can be done by solving the normal equations for the least-squares problem or using a library function like `numpy.linalg.lstsq`.\n4.  Determine the fitted parameters $\\kappa_{\\infty,\\mathrm{fit}} = \\alpha$ and $s_{\\mathrm{fit}} = -\\beta$.\n5.  Report the four values: $\\kappa_{\\infty,\\mathrm{fit}}$, $\\kappa_{\\infty,\\mathrm{hydro}}$, $s_{\\mathrm{fit}}$, and $s_{\\mathrm{hydro}}$, rounded to six decimal places.\n\nIf all data points $(1/L_i, \\kappa(L_i))$ satisfy the condition $t_c(L_i) \\ge t_0$, they will lie perfectly on a straight line. In this scenario, the regression will exactly recover the analytical parameters, i.e., $\\kappa_{\\infty,\\mathrm{fit}} = \\kappa_{\\infty,\\mathrm{hydro}}$ and $s_{\\mathrm{fit}} = s_{\\mathrm{hydro}}$. However, if for some smaller sizes $L_i$, the condition $t_c(L_i)  t_0$ is met, those data points will not lie on the same asymptotic line as the others. This introduces curvature into the plot of $\\kappa$ vs. $1/L$, and a linear fit to all points will yield fitted parameters that deviate from the true hydrodynamic asymptotic values. This situation models a realistic data analysis scenario where data from small systems may not be in the asymptotic scaling regime.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Green-Kubo finite-size scaling problem for the provided test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"params\": {\"a\": 0.25, \"b\": 0.10, \"t1\": 1.5, \"t2\": 6.0, \"C\": 0.50, \"t0\": 2.0, \"D\": 0.08},\n            \"sizes\": [5, 8, 12, 20]\n        },\n        {\n            \"params\": {\"a\": 0.35, \"b\": 0.05, \"t1\": 1.0, \"t2\": 8.0, \"C\": 0.40, \"t0\": 5.0, \"D\": 0.05},\n            \"sizes\": [3, 4, 6, 10]\n        },\n        {\n            \"params\": {\"a\": 0.20, \"b\": 0.15, \"t1\": 2.0, \"t2\": 3.0, \"C\": 0.30, \"t0\": 1.0, \"D\": 0.20},\n            \"sizes\": [6, 9, 15, 30]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        p = case[\"params\"]\n        sizes = case[\"sizes\"]\n        \n        a, b, t1, t2, C, t0, D = p[\"a\"], p[\"b\"], p[\"t1\"], p[\"t2\"], p[\"C\"], p[\"t0\"], p[\"D\"]\n        \n        # 1. Compute hydrodynamic predictions (analytical)\n        kappa_fast = a * t1 + b * t2\n        kappa_tail_inf = 2 * C / np.sqrt(t0)\n        kappa_inf_hydro = kappa_fast + kappa_tail_inf\n        s_hydro = 4 * np.pi * C * np.sqrt(D)\n\n        # 2. Generate finite-size conductivity data for regression\n        x_vals = []\n        y_vals = []\n        \n        for L in sizes:\n            tc = L**2 / (4 * np.pi**2 * D)\n            \n            if tc >= t0:\n                kappa_tail_L = 2 * C * (1/np.sqrt(t0) - 1/np.sqrt(tc))\n                kappa_L = kappa_fast + kappa_tail_L\n            else:\n                kappa_L = kappa_fast\n\n            x_vals.append(1/L)\n            y_vals.append(kappa_L)\n            \n        x = np.array(x_vals)\n        y = np.array(y_vals)\n        \n        # 3. Perform unconstrained linear least-squares fit: y = alpha + beta*x\n        # The design matrix A has a column of ones (for intercept) and a column of x values.\n        A = np.vstack([np.ones(len(x)), x]).T\n        \n        # Solve the linear system A*p = y for the coefficients p = [alpha, beta]\n        p_fit, _, _, _ = np.linalg.lstsq(A, y, rcond=None)\n        alpha, beta = p_fit[0], p_fit[1]\n        \n        # 4. Extract fitted parameters\n        kappa_inf_fit = alpha\n        s_fit = -beta\n        \n        # 5. Store results, rounded to six decimal places\n        result_tuple = (\n            round(kappa_inf_fit, 6),\n            round(kappa_inf_hydro, 6),\n            round(s_fit, 6),\n            round(s_hydro, 6)\n        )\n        all_results.append(result_tuple)\n\n    # Final print statement in the exact required format.\n    # Eg: [[v1,v2,v3,v4],[v1,v2,v3,v4],...]\n    formatted_results = [f\"[{','.join(f'{v:.6f}' for v in res)}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond equilibrium methods, thermal conductivity can be determined using Non-Equilibrium Molecular Dynamics (NEMD), where one directly applies Fourier's law, $J = -\\kappa \\nabla T$. In this approach, a heat flux $J$ is imposed on the system, and the resulting steady-state temperature gradient $\\nabla T$ is measured to find $\\kappa$. This exercise focuses on a critical part of the NEMD workflow: robustly validating that the measured temperature profile is indeed linear, a key assumption for Fourier's law to hold. You will employ a suite of statistical tools to analyze synthetic temperature data, developing essential skills for ensuring the reliability of NEMD calculations.",
            "id": "3749272",
            "problem": "You are given synthetic spatially binned temperature data designed to mimic the output of Reverse Non-Equilibrium Molecular Dynamics (RNEMD), which imposes a steady heat flux and is commonly used to estimate thermal conductivity in multiscale modeling. In a physically valid regime where Fourier's law applies, the steady-state temperature profile away from perturbation regions is expected to be linear in space. Your task is to implement diagnostics that test whether an RNEMD-produced temperature field exhibits a statistically justified linear spatial profile without localized hot spots, using statistically principled fitting and residual analysis.\n\nFundamental base: Start from the definition of temperature in classical Molecular Dynamics as proportional to the average kinetic energy per degree of freedom and the macroscopic steady-state energy conservation that leads, under constant thermal conductivity, to a linear temperature profile in space. Specifically, let the heat flux be denoted by $J$ and let the thermal conductivity be denoted by $\\kappa$. Under a constant cross-sectional area and homogeneous material, the steady-state heat equation and Fourier’s law together imply a spatial temperature field $T(x)$ that is linear, i.e., $T(x) = a x + b$, away from any exchange region or boundary artifacts, for some constants $a$ and $b$. In RNEMD, energy is exchanged between two spatial regions to create $J$, and the temperature profile is measured by spatial binning. Deviations from linearity and localized hot spots invalidate the inference of thermal conductivity from the measured gradient.\n\nYou must write a complete program that, for each test case, performs the following diagnostics on binned data $(x_i, T_i, \\sigma_i)$, where $x_i$ is the bin center location in nanometers, $T_i$ is the bin temperature in Kelvin, and $\\sigma_i$ is the estimated standard deviation of $T_i$ in Kelvin:\n\n- Weighted linear least-squares fitting of $T$ versus $x$, using weights $w_i = 1/\\sigma_i^2$. Quantify the goodness-of-fit using the reduced chi-squared $\\,\\chi^2_\\nu\\,$, where $\\nu$ is the degrees of freedom.\n- Standardized residual analysis using $r_i = (T_i - \\hat{T}(x_i))/\\sigma_i$. Detect localized hot spots by identifying unusually large $|r_i|$ values and residual clustering.\n- Autocorrelation check on residuals using the Durbin–Watson statistic to assess whether residuals are consistent with independent fluctuations.\n- Model comparison between a global linear model and a continuous piecewise-linear model with a single unknown breakpoint $x_0$ that allows a change in slope but enforces continuity at $x_0$. Evaluate evidence using the Akaike Information Criterion (AIC), derived from the Gaussian log-likelihood with known per-bin variances $\\sigma_i^2$. The continuous piecewise-linear model should have three free parameters: the left slope, the right slope, and one intercept, with continuity enforced at the breakpoint.\n\nAcceptance criteria: A profile is considered \"valid linear without localized hot spots\" if all of the following hold simultaneously:\n- The reduced chi-squared $\\,\\chi^2_\\nu\\,$ for the global weighted linear model lies within the interval $[0.5, 2.0]$.\n- The maximum absolute standardized residual $\\max_i |r_i|$ is less than $3.5$.\n- The Durbin–Watson statistic lies within the interval $[1.0, 3.0]$.\n- The best continuous piecewise-linear model (optimized over breakpoints $x_0$) does not substantially outperform the global linear model, quantified by $\\mathrm{AIC}_{\\text{piecewise}} - \\mathrm{AIC}_{\\text{linear}} \\ge -10$. If this difference is less than $-10$, treat it as evidence for a slope change and reject linearity.\n\nUnits: All temperatures must be treated in Kelvin and all positions must be treated in nanometers. The program must not convert units and must assume input temperatures are in Kelvin and positions in nanometers. The program must compute dimensionless statistics as described.\n\nInput specification: There is no external input. Your program must internally construct the following test suite of five cases with deterministic values (using the fixed random seed $42$ where noise is included), each case consisting of arrays $(x_i, T_i, \\sigma_i)$:\n\n- Case $1$ (happy path, linear with homoscedastic noise): $N=40$ bins over a length $L=10$ nanometers, $x_i$ uniformly spaced on $[0, L]$, true profile $T(x) = 300 + 10 x$ Kelvin, homoscedastic Gaussian noise with $\\sigma_i = 0.5$ Kelvin, independent across bins using seed $42$.\n- Case $2$ (localized hot spot): Same as Case $1$, but add a localized increase of $+5$ Kelvin at bin index $30$ (zero-based indexing) to simulate a hot spot, with the same $\\sigma_i = 0.5$ Kelvin.\n- Case $3$ (piecewise slopes, continuous at mid-domain): $N=40$, $L=10$ nanometers, $x_i$ uniformly spaced. Define $x_0 = 5$ nanometers. For $x \\le x_0$, use $T(x) = 300 + 8 x$ Kelvin. For $x  x_0$, use $T(x) = 300 + 8 x_0 + 12 (x - x_0)$ Kelvin, ensuring continuity at $x_0$. Add homoscedastic Gaussian noise with $\\sigma_i = 0.4$ Kelvin using seed $42$.\n- Case $4$ (uniform temperature): $N=30$, $L=9$ nanometers, $x_i$ uniformly spaced, $T(x) = 300$ Kelvin for all $x$, with homoscedastic Gaussian noise $\\sigma_i = 0.3$ Kelvin using seed $42$.\n- Case $5$ (linear with heteroscedastic noise): $N=50$, $L=10$ nanometers, $x_i$ uniformly spaced, true profile $T(x) = 300 + 7 x$ Kelvin. Heteroscedastic noise with $\\sigma_i = 0.2 + 0.07 x_i$ Kelvin, independent across bins using seed $42$.\n\nAlgorithmic requirements:\n- Implement weighted least-squares estimators for the global linear model and the continuous piecewise-linear model at candidate breakpoints $x_0$ drawn from bin centers, excluding breakpoints that would leave fewer than $2$ data points in either segment.\n- Compute the Gaussian log-likelihood under independent residuals with known variances $\\sigma_i^2$ for each model, and compute the Akaike Information Criterion $\\mathrm{AIC} = 2k - 2 \\log L$, where $k$ is the number of free parameters and $\\log L$ is the log-likelihood.\n- Compute the reduced chi-squared $\\,\\chi^2_\\nu\\,$, standardized residuals $r_i$, and Durbin–Watson statistic.\n- Apply the acceptance criteria to each case to produce a boolean result.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4,result_5]$), where each $result_i$ is either $True$ or $False$, indicating whether the case passes all diagnostics as described. No other text should be printed.\n\nAngle unit: Not applicable.\n\nPercentages: Not applicable.\n\nEnsure scientific realism in the construction and analysis. Your diagnostics must be general and universally applicable, without relying on any problem-specific shortcuts.",
            "solution": "The problem requires the implementation of a set of statistical diagnostics to validate synthetic temperature profiles, mimicking data from Reverse Non-Equilibrium Molecular Dynamics (RNEMD) simulations. The core principle is that for a homogeneous material in a steady state with a constant heat flux $J$, Fourier's law, $J = -\\kappa \\nabla T$ (where $\\kappa$ is thermal conductivity), predicts a linear temperature profile $T(x) = ax + b$ in regions away from thermal perturbations. The provided task is to determine if a given binned temperature profile $(x_i, T_i, \\sigma_i)$ is consistent with this linear model, free of localized anomalies, and not better described by a more complex model. A profile is deemed \"valid\" if it simultaneously passes four specific statistical tests.\n\nThe solution is structured by first generating the five specified test cases. Then, for each case, we apply a sequence of four diagnostic analyses: (1) weighted linear regression and goodness-of-fit testing, (2) standardized residual analysis for outlier detection, (3) a Durbin-Watson test for residual autocorrelation, and (4) a model comparison against a piecewise linear model using the Akaike Information Criterion (AIC).\n\nFirst, we generate the data for the five test cases as specified. A single pseudo-random number generator is initialized with a seed of $42$ to ensure reproducibility of the stochastic noise components.\n\n**1. Weighted Linear Regression and Goodness-of-Fit**\n\nThe primary model under consideration is a global linear relationship, $\\hat{T}(x) = ax+b$. Given that the uncertainty $\\sigma_i$ for each temperature measurement $T_i$ is known, a weighted least-squares (WLS) regression is the appropriate method for fitting. The objective is to find the parameters $a$ and $b$ that minimize the chi-squared statistic:\n$$\n\\chi^2 = \\sum_{i=1}^{N} \\left( \\frac{T_i - \\hat{T}(x_i)}{\\sigma_i} \\right)^2 = \\sum_{i=1}^{N} w_i \\left( T_i - (ax_i + b) \\right)^2\n$$\nwhere $N$ is the number of data points and the weights are $w_i = 1/\\sigma_i^2$. This minimization can be solved using the normal equations in matrix form. Let $\\mathbf{T} = [T_1, \\dots, T_N]^T$ be the vector of temperatures, $\\beta = [b, a]^T$ be the vector of parameters, and $X$ be the design matrix:\n$$\nX = \\begin{pmatrix} 1  x_1 \\\\ 1  x_2 \\\\ \\vdots  \\vdots \\\\ 1  x_N \\end{pmatrix}\n$$\nLet $W = \\mathrm{diag}(w_1, \\dots, w_N)$ be the diagonal matrix of weights. The WLS estimate for the parameters is:\n$$\n\\hat{\\beta} = (X^T W X)^{-1} X^T W \\mathbf{T}\n$$\nThe goodness-of-fit is assessed using the reduced chi-squared statistic, $\\chi^2_\\nu = \\chi^2 / \\nu$, where $\\nu = N - k$ is the number of degrees of freedom ($k=2$ for the linear model). For a good fit, $\\chi^2_\\nu$ should be close to $1$. The first acceptance criterion is $\\chi^2_\\nu \\in [0.5, 2.0]$.\n\n**2. Standardized Residual Analysis for Hot Spots**\n\nTo detect localized anomalies such as hot spots, which invalidate the uniform material assumption, we analyze the standardized residuals. The standardized residual for each data point is defined as:\n$$\nr_i = \\frac{T_i - \\hat{T}(x_i)}{\\sigma_i}\n$$\nIf the model is correct and the errors are Gaussian, the set of $\\{r_i\\}$ should be drawn from a standard normal distribution $\\mathcal{N}(0,1)$. A large value of $|r_i|$ indicates an outlier that is poorly explained by the model. The second acceptance criterion is a \"3.5-sigma\" rule: $\\max_i |r_i|  3.5$.\n\n**3. Durbin-Watson Test for Residual Autocorrelation**\n\nThe WLS regression assumes that the measurement errors are independent. Systematic deviations from the linear model, such as those caused by a change in material properties, can introduce correlation in the residuals. The Durbin-Watson statistic is used to detect first-order autocorrelation in the unstandardized residuals $e_i = T_i - \\hat{T}(x_i)$:\n$$\nDW = \\frac{\\sum_{i=2}^N (e_i - e_{i-1})^2}{\\sum_{i=1}^N e_i^2}\n$$\nA value of $DW \\approx 2$ indicates no significant autocorrelation. Values approaching $0$ suggest positive autocorrelation, while values approaching $4$ suggest negative autocorrelation. The third acceptance criterion is that the statistic must be within the interval $[1.0, 3.0]$.\n\n**4. Model Comparison using Akaike Information Criterion (AIC)**\n\nTo check if the data justifies a more complex model, we compare the global linear model to a continuous piecewise linear model with a single breakpoint. The piecewise model is defined as:\n$$\nT(x) = \\begin{cases} a_1 x + b_1  x \\le x_{bp} \\\\ a_2 x + b_2  x  x_{bp} \\end{cases}\n$$\nContinuity is enforced by the constraint $a_1 x_{bp} + b_1 = a_2 x_{bp} + b_2$. This model has three free parameters ($k_{pw}=3$), for example, the two slopes and one intercept value. The optimal breakpoint $x_0$ is found by iterating through all valid internal bin centers as candidate breakpoints and selecting the one that minimizes the $\\chi^2$ value for the piecewise fit. Let this minimum be $\\chi^2_{\\text{piecewise,min}}$.\n\nThe Akaike Information Criterion (AIC) provides a means for model selection, balancing model fit with model complexity. It is defined as $\\mathrm{AIC} = 2k - 2 \\log L$, where $k$ is the number of parameters and $\\log L$ is the model's maximized log-likelihood. For a Gaussian likelihood with known variances $\\sigma_i^2$, $\\log L = -\\frac{1}{2} \\chi^2 + \\text{const}$. Since the constant term cancels when comparing models, we can use the form $\\mathrm{AIC} = 2k + \\chi^2$.\n\nThe AIC values for the two models are:\n$$\n\\mathrm{AIC}_{\\text{linear}} = 2(2) + \\chi^2_{\\text{linear}} = 4 + \\chi^2_{\\text{linear}}\n$$\n$$\n\\mathrm{AIC}_{\\text{piecewise}} = 2(3) + \\chi^2_{\\text{piecewise,min}} = 6 + \\chi^2_{\\text{piecewise,min}}\n$$\nThe fourth acceptance criterion states that the linear model is sufficient unless the piecewise model is substantially better. This is quantified by the condition $\\mathrm{AIC}_{\\text{piecewise}} - \\mathrm{AIC}_{\\text{linear}} \\ge -10$. A difference smaller than $-10$ is considered strong evidence in favor of the more complex piecewise model, leading to the rejection of the simple linear profile.\n\nA temperature profile is classified as \"valid linear without localized hot spots\" if and only if all four of these criteria are met. This comprehensive approach ensures that the inferred linearity is statistically robust.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_data(case_id, rng):\n    \"\"\"\n    Generates synthetic temperature profile data for each test case.\n    \"\"\"\n    if case_id == 1:\n        # Case 1: Happy path, linear with homoscedastic noise\n        N, L = 40, 10.0\n        x = np.linspace(0, L, N)\n        T_true = 300.0 + 10.0 * x\n        sigma_val = 0.5\n        sigma = np.full(N, sigma_val)\n        T = T_true + rng.normal(0, sigma_val, N)\n        return x, T, sigma\n    elif case_id == 2:\n        # Case 2: Localized hot spot\n        N, L = 40, 10.0\n        x = np.linspace(0, L, N)\n        T_true = 300.0 + 10.0 * x\n        sigma_val = 0.5\n        sigma = np.full(N, sigma_val)\n        T = T_true + rng.normal(0, sigma_val, N)\n        T[30] += 5.0  # Add hot spot\n        return x, T, sigma\n    elif case_id == 3:\n        # Case 3: Piecewise slopes, continuous\n        N, L = 40, 10.0\n        x = np.linspace(0, L, N)\n        x0 = 5.0\n        T_true = np.where(x = x0, 300.0 + 8.0 * x, 300.0 + 8.0 * x0 + 12.0 * (x - x0))\n        sigma_val = 0.4\n        sigma = np.full(N, sigma_val)\n        T = T_true + rng.normal(0, sigma_val, N)\n        return x, T, sigma\n    elif case_id == 4:\n        # Case 4: Uniform temperature\n        N, L = 30, 9.0\n        x = np.linspace(0, L, N)\n        T_true = np.full(N, 300.0)\n        sigma_val = 0.3\n        sigma = np.full(N, sigma_val)\n        T = T_true + rng.normal(0, sigma_val, N)\n        return x, T, sigma\n    elif case_id == 5:\n        # Case 5: Linear with heteroscedastic noise\n        N, L = 50, 10.0\n        x = np.linspace(0, L, N)\n        T_true = 300.0 + 7.0 * x\n        sigma = 0.2 + 0.07 * x\n        T = T_true + rng.normal(0, sigma)\n        return x, T, sigma\n    else:\n        raise ValueError(\"Invalid case_id\")\n\ndef solve_case(case_data):\n    \"\"\"\n    Performs all diagnostics for a single case and returns a boolean result.\n    \"\"\"\n    x, T, sigma = case_data\n    N = len(x)\n    weights = 1.0 / sigma**2\n    W = np.diag(weights)\n\n    # 1. Global Weighted Linear Fit\n    X_lin = np.stack([np.ones(N), x], axis=1)\n    \n    try:\n        XTWX_lin = X_lin.T @ W @ X_lin\n        XTWT_lin = X_lin.T @ W @ T\n        params_lin = np.linalg.solve(XTWX_lin, XTWT_lin)\n    except np.linalg.LinAlgError:\n        return False # Should not occur in these tests\n\n    b, a = params_lin\n    T_hat_lin = a * x + b\n    residuals_lin = T - T_hat_lin\n\n    # 2. Perform Diagnostic Checks on the Linear Model\n    # 2.1. Reduced Chi-Squared\n    chi2_lin = np.sum((residuals_lin / sigma)**2)\n    dof_lin = N - 2\n    if dof_lin = 0: return False\n    red_chi2 = chi2_lin / dof_lin\n    cond1 = 0.5 = red_chi2 = 2.0\n\n    # 2.2. Standardized Residuals\n    std_residuals = residuals_lin / sigma\n    max_abs_r = np.max(np.abs(std_residuals))\n    cond2 = max_abs_r  3.5\n\n    # 2.3. Durbin-Watson Statistic\n    e = residuals_lin\n    if np.sum(e**2) == 0:  # Avoid division by zero\n        dw_stat = 2.0\n    else:\n        dw_stat = np.sum(np.diff(e)**2) / np.sum(e**2)\n    cond3 = 1.0 = dw_stat = 3.0\n\n    # 3. Continuous Piecewise Linear Fit\n    min_chi2_pw = np.inf\n    \n    # Iterate through potential breakpoints, excluding ends\n    # Breakpoint index j: left segment has j+1 points, right has N-j-1\n    # To have >=2 points/segment: j+1>=2 -> j>=1; N-j-1>=2 -> j=N-3\n    for j in range(1, N - 2):\n        x_bp = x[j]\n        \n        # Design matrix for model: T = a1*u + a2*v + c\n        # u = (x-xbp) for x=xbp, 0 otherwise\n        # v = (x-xbp) for x>xbp, 0 otherwise\n        u = np.where(x = x_bp, x - x_bp, 0)\n        v = np.where(x > x_bp, x - x_bp, 0)\n        c_col = np.ones(N)\n        X_pw = np.stack([u, v, c_col], axis=1)\n        \n        XTWX_pw = X_pw.T @ W @ X_pw\n        XTWT_pw = X_pw.T @ W @ T\n        try:\n            params_pw = np.linalg.solve(XTWX_pw, XTWT_pw)\n        except np.linalg.LinAlgError:\n            continue\n\n        a1_pw, a2_pw, c_pw = params_pw\n        T_hat_pw = a1_pw * u + a2_pw * v + c_pw\n        chi2_pw = np.sum(((T - T_hat_pw) / sigma)**2)\n        \n        if chi2_pw  min_chi2_pw:\n            min_chi2_pw = chi2_pw\n            \n    if min_chi2_pw == np.inf: # If all piecewise fits failed\n        return False\n\n    # 4. AIC-based Model Comparison\n    k_lin = 2\n    k_pw = 3\n    aic_lin = 2 * k_lin + chi2_lin\n    aic_pw = 2 * k_pw + min_chi2_pw\n    \n    delta_aic = aic_pw - aic_lin\n    cond4 = delta_aic >= -10.0\n\n    return cond1 and cond2 and cond3 and cond4\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    rng = np.random.default_rng(42)\n    num_cases = 5\n    \n    results = []\n    for i in range(1, num_cases + 1):\n        case_data = generate_data(i, rng)\n        result = solve_case(case_data)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}