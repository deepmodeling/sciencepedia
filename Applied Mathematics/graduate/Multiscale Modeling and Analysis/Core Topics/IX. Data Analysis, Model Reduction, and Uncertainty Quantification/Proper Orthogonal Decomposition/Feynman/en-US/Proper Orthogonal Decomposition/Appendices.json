{
    "hands_on_practices": [
        {
            "introduction": "To build a strong foundation, we begin with an analytical derivation. This exercise strips away numerical complexities to reveal the core principle of Proper Orthogonal Decomposition (POD): identifying an optimal basis from the data's correlation structure. By working through this problem , you will see how the spatial and temporal components of a simple system directly give rise to its POD modes, providing a clear and tangible link between the theory and its outcome.",
            "id": "3265890",
            "problem": "Consider the spatial domain $x \\in [0,1]$ with the spatial inner product given by $\\langle f, g \\rangle = \\int_{0}^{1} f(x) g(x) \\, dx$, and the temporal ensemble average defined by the long-time mean $\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} (\\cdot) \\, dt$. Let the space-time field be\n$$\nu(x,t) = \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t),\n$$\nwhere $\\omega_{1} > 0$ and $\\omega_{2} > 0$ are distinct real constants. Using the foundational definition of Proper Orthogonal Decomposition (POD), where the spatial POD modes are the eigenfunctions of the spatial correlation operator\n$$\n\\mathcal{C}[\\phi](x) = \\int_{0}^{1} C(x,y) \\, \\phi(y) \\, dy,\n$$\nconstructed from the correlation kernel\n$$\nC(x,y) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} u(x,t) \\, u(y,t) \\, dt,\n$$\nderive the analytical spatial POD modes associated with $u(x,t)$. Express the modes as $L^{2}([0,1])$-normalized functions. Your final answer must list the two normalized spatial POD modes in a single row matrix. No units are required, and no rounding is needed.",
            "solution": "The problem requires the derivation of the spatial Proper Orthogonal Decomposition (POD) modes for a given space-time field $u(x,t)$. By definition, the spatial POD modes, denoted by $\\phi(x)$, are the eigenfunctions of the spatial two-point correlation operator $\\mathcal{C}$, which is an integral operator with kernel $C(x,y)$. The eigenproblem is given by:\n$$\n\\mathcal{C}[\\phi](x) = \\int_{0}^{1} C(x,y) \\, \\phi(y) \\, dy = \\lambda \\phi(x)\n$$\nThe kernel $C(x,y)$ is defined as the time-average of the product of the field at two spatial locations, $x$ and $y$.\n$$\nC(x,y) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} u(x,t) \\, u(y,t) \\, dt\n$$\nThe first step is to compute this kernel for the given field $u(x,t) = \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t)$.\n\nLet's expand the product $u(x,t) \\, u(y,t)$:\n\\begin{align*}\nu(x,t) \\, u(y,t) = & \\left[ \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t) \\right] \\left[ \\sin(2\\pi y) \\cos(\\omega_{1} t) + \\sin(3\\pi y) \\cos(\\omega_{2} t) \\right] \\\\\n= & \\sin(2\\pi x) \\sin(2\\pi y) \\cos^2(\\omega_{1} t) \\\\\n& + \\sin(3\\pi x) \\sin(3\\pi y) \\cos^2(\\omega_{2} t) \\\\\n& + \\sin(2\\pi x) \\sin(3\\pi y) \\cos(\\omega_{1} t) \\cos(\\omega_{2} t) \\\\\n& + \\sin(3\\pi x) \\sin(2\\pi y) \\cos(\\omega_{1} t) \\cos(\\omega_{2} t)\n\\end{align*}\nTo find $C(x,y)$, we must compute the long-time average of the temporal components. We need the following standard time-average results for harmonic functions:\n$1$. For any non-zero frequency $\\omega > 0$,\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega t) \\, dt = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\frac{1 + \\cos(2\\omega t)}{2} \\, dt = \\lim_{T \\to \\infty} \\frac{1}{T} \\left[ \\frac{t}{2} + \\frac{\\sin(2\\omega t)}{4\\omega} \\right]_{0}^{T} = \\frac{1}{2}\n$$\n$2$. For two distinct positive frequencies $\\omega_1 \\neq \\omega_2$,\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos(\\omega_1 t) \\cos(\\omega_2 t) \\, dt = \\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{0}^{T} \\left[ \\cos((\\omega_1 - \\omega_2)t) + \\cos((\\omega_1 + \\omega_2)t) \\right] dt = 0\n$$\nThe second result holds because the integral of a cosine function over a period is zero, and its indefinite integral is a sine function, which is bounded. Dividing by $T \\to \\infty$ makes the average tend to $0$.\n\nApplying these time averages to the expanded product, the cross-terms involving $\\cos(\\omega_{1} t) \\cos(\\omega_{2} t)$ average to zero. We are left with:\n\\begin{align*}\nC(x,y) &= \\sin(2\\pi x) \\sin(2\\pi y) \\left( \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega_{1} t) \\, dt \\right) + \\sin(3\\pi x) \\sin(3\\pi y) \\left( \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega_{2} t) \\, dt \\right) \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y)\n\\end{align*}\nThis is a separable kernel of rank $2$. The eigenfunctions of an operator with such a kernel must lie in the span of the functions that constitute the kernel, i.e., $\\text{span}\\{\\sin(2\\pi x), \\sin(3\\pi x)\\}$.\n\nLet's check if the basis functions $\\psi_1(x) = \\sin(2\\pi x)$ and $\\psi_2(x) = \\sin(3\\pi x)$ are orthogonal under the given inner product $\\langle f, g \\rangle = \\int_{0}^{1} f(x) g(x) \\, dx$:\n$$\n\\langle \\psi_1, \\psi_2 \\rangle = \\int_{0}^{1} \\sin(2\\pi x) \\sin(3\\pi x) \\, dx = \\frac{1}{2} \\int_{0}^{1} \\left[ \\cos(\\pi x) - \\cos(5\\pi x) \\right] dx = \\frac{1}{2} \\left[ \\frac{\\sin(\\pi x)}{\\pi} - \\frac{\\sin(5\\pi x)}{5\\pi} \\right]_{0}^{1} = 0\n$$\nSince the spatial functions are orthogonal, they are indeed the eigenfunctions of the correlation operator $\\mathcal{C}$. We can verify this by substituting them into the eigenproblem.\n\nFor the first eigenfunction candidate $\\phi(x) = \\psi_1(x) = \\sin(2\\pi x)$:\n\\begin{align*}\n\\mathcal{C}[\\psi_1](x) &= \\int_{0}^{1} \\left[ \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y) \\right] \\sin(2\\pi y) \\, dy \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\int_{0}^{1} \\sin^2(2\\pi y) \\, dy + \\frac{1}{2} \\sin(3\\pi x) \\int_{0}^{1} \\sin(3\\pi y) \\sin(2\\pi y) \\, dy\n\\end{align*}\nThe second integral is zero due to orthogonality. The first integral is:\n$$\n\\int_{0}^{1} \\sin^2(2\\pi y) \\, dy = \\int_{0}^{1} \\frac{1 - \\cos(4\\pi y)}{2} \\, dy = \\left[ \\frac{y}{2} - \\frac{\\sin(4\\pi y)}{8\\pi} \\right]_{0}^{1} = \\frac{1}{2}\n$$\nThus, $\\mathcal{C}[\\psi_1](x) = \\frac{1}{2} \\sin(2\\pi x) \\cdot \\frac{1}{2} = \\frac{1}{4} \\sin(2\\pi x)$.\nSo, $\\phi_1(x) = \\sin(2\\pi x)$ is an eigenfunction with eigenvalue $\\lambda_1 = \\frac{1}{4}$.\n\nFor the second eigenfunction candidate $\\phi(x) = \\psi_2(x) = \\sin(3\\pi x)$:\n\\begin{align*}\n\\mathcal{C}[\\psi_2](x) &= \\int_{0}^{1} \\left[ \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y) \\right] \\sin(3\\pi y) \\, dy \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\int_{0}^{1} \\sin(2\\pi y) \\sin(3\\pi y) \\, dy + \\frac{1}{2} \\sin(3\\pi x) \\int_{0}^{1} \\sin^2(3\\pi y) \\, dy\n\\end{align*}\nThe first integral is zero. The second integral is:\n$$\n\\int_{0}^{1} \\sin^2(3\\pi y) \\, dy = \\int_{0}^{1} \\frac{1 - \\cos(6\\pi y)}{2} \\, dy = \\left[ \\frac{y}{2} - \\frac{\\sin(6\\pi y)}{12\\pi} \\right]_{0}^{1} = \\frac{1}{2}\n$$\nThus, $\\mathcal{C}[\\psi_2](x) = \\frac{1}{2} \\sin(3\\pi x) \\cdot \\frac{1}{2} = \\frac{1}{4} \\sin(3\\pi x)$.\nSo, $\\phi_2(x) = \\sin(3\\pi x)$ is an eigenfunction with eigenvalue $\\lambda_2 = \\frac{1}{4}$.\n\nThe final step is to normalize these eigenfunctions to have unit $L^2$-norm. The squared norm of an eigenfunction $\\phi$ is $\\|\\phi\\|^2 = \\langle \\phi, \\phi \\rangle = \\int_{0}^{1} \\phi(x)^2 \\, dx$.\nFor the first mode, $\\sin(2\\pi x)$:\n$$\n\\|\\sin(2\\pi x)\\|^2 = \\int_{0}^{1} \\sin^2(2\\pi x) \\, dx = \\frac{1}{2}\n$$\nThe normalization constant is $1 / \\sqrt{1/2} = \\sqrt{2}$. The first normalized POD mode is $\\hat{\\phi_1}(x) = \\sqrt{2} \\sin(2\\pi x)$.\n\nFor the second mode, $\\sin(3\\pi x)$:\n$$\n\\|\\sin(3\\pi x)\\|^2 = \\int_{0}^{1} \\sin^2(3\\pi x) \\, dx = \\frac{1}{2}\n$$\nThe normalization constant is also $\\sqrt{2}$. The second normalized POD mode is $\\hat{\\phi_2}(x) = \\sqrt{2} \\sin(3\\pi x)$.\n\nThe two non-trivial spatial POD modes are therefore $\\sqrt{2} \\sin(2\\pi x)$ and $\\sqrt{2} \\sin(3\\pi x)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{2}\\sin(2\\pi x) & \\sqrt{2}\\sin(3\\pi x)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from continuous theory to discrete computation, we explore the numerical heart of POD—the Singular Value Decomposition (SVD). This practice is a crucial \"sanity check\" that builds confidence in the numerical tools used for model reduction. You will programmatically verify that SVD can robustly identify the underlying rank of a dataset , a critical step for any POD application, and learn to navigate the practical realities of finite-precision arithmetic and numerical noise.",
            "id": "3265926",
            "problem": "You will implement a programmatic verification that Proper Orthogonal Decomposition (POD) based on the Singular Value Decomposition (SVD) correctly identifies the numerical rank of a snapshot matrix when the matrix is constructed to have a prescribed rank. The verification must be reproducible and must not require any user input.\n\nStart from the following fundamental linear algebra facts:\n- For any real matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}$, there exists a Singular Value Decomposition (SVD) $\\mathbf{X} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{\\top}$ where $\\mathbf{U} \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{V} \\in \\mathbb{R}^{m \\times m}$ are orthogonal, and $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times m}$ is diagonal (possibly rectangular) with nonnegative diagonal entries $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge 0$, called singular values.\n- The rank of $\\mathbf{X}$ equals the number of strictly positive singular values of $\\mathbf{X}$.\n- Proper Orthogonal Decomposition (POD) of a snapshot matrix is obtained by applying the SVD to that matrix; the nonzero singular values quantify energetic content along orthogonal modes.\n\nYou will construct snapshot matrices with prescribed rank by the following principle: if $\\mathbf{U}_{r} \\in \\mathbb{R}^{n \\times r}$ and $\\mathbf{V}_{r} \\in \\mathbb{R}^{m \\times r}$ have orthonormal columns (that is, $\\mathbf{U}_{r}^{\\top}\\mathbf{U}_{r} = \\mathbf{I}_{r}$ and $\\mathbf{V}_{r}^{\\top}\\mathbf{V}_{r} = \\mathbf{I}_{r}$), and if $\\boldsymbol{\\Sigma}_{r} = \\operatorname{diag}(s_{1},\\dots,s_{r})$ with $s_{i} > 0$, then\n$$\n\\mathbf{X} = \\mathbf{U}_{r}\\, \\boldsymbol{\\Sigma}_{r}\\, \\mathbf{V}_{r}^{\\top}\n$$\nhas rank exactly $r$ and singular values equal to $(s_{1},\\dots,s_{r})$, with all remaining singular values equal to $0$.\n\nBecause floating-point arithmetic introduces small round-off errors, you will determine the numerical rank by counting singular values that exceed a tolerance. Define two counting rules:\n- Absolute rule: count the number of singular values $\\sigma_{i}$ such that $\\sigma_{i} \\ge \\tau$, where $\\tau > 0$ is an absolute tolerance.\n- Relative rule: count the number of singular values $\\sigma_{i}$ such that $\\sigma_{i} \\ge \\rho \\, \\sigma_{\\max}$, where $\\rho > 0$ is a relative tolerance and $\\sigma_{\\max} = \\max_{i} \\sigma_{i}$.\n\nTasks to implement:\n1. Construct three test snapshot matrices $\\mathbf{X}^{(k)} \\in \\mathbb{R}^{n \\times m}$ with $n = 100$, $m = 50$, and target rank $r = 3$ by selecting orthonormal $\\mathbf{U}_{r}$ and $\\mathbf{V}_{r}$ and prescribing positive singular values $(s_{1}, s_{2}, s_{3})$ as follows. Use a fixed random seed to ensure reproducibility. For each case, compute the singular values via SVD and count how many are significant according to the specified rule and tolerance. The three cases are:\n   - Case A (happy path, exact rank-$3$):\n     - Dimensions: $n = 100$, $m = 50$, $r = 3$.\n     - Singular values: $(s_{1}, s_{2}, s_{3}) = (5, 3, 1)$.\n     - No noise added.\n     - Use the absolute rule with $\\tau = 10^{-12}$.\n   - Case B (ill-conditioned but exact rank-$3$):\n     - Dimensions: $n = 100$, $m = 50$, $r = 3$.\n     - Singular values: $(s_{1}, s_{2}, s_{3}) = (1, 10^{-8}, 10^{-12})$.\n     - No noise added.\n     - Use the relative rule with $\\rho = 10^{-13}$.\n   - Case C (near rank-$3$ with tiny additive noise):\n     - Dimensions: $n = 100$, $m = 50$, $r = 3$.\n     - Singular values: $(s_{1}, s_{2}, s_{3}) = (2, 2 \\times 10^{-1}, 2 \\times 10^{-2})$.\n     - Add independent and identically distributed Gaussian noise $\\mathbf{N}$ with entries of zero mean and standard deviation $\\epsilon \\, s_{1}$ with $\\epsilon = 10^{-10}$, i.e., set $\\mathbf{X} \\leftarrow \\mathbf{X} + \\mathbf{N}$ after constructing $\\mathbf{X}$.\n     - Use the relative rule with $\\rho = 10^{-8}$.\n2. For each case, compute the number of significant singular values as an integer according to the specified tolerance rule. This number should equal the target rank $r = 3$ in all cases if the construction and thresholds are sound.\n3. Your program must not read any input and must produce a single line of output containing the three integer results corresponding to Cases A, B, and C, respectively, formatted as a comma-separated list enclosed in square brackets, for example, $[x_{A},x_{B},x_{C}]$.\n\nTest suite and expected coverage rationale:\n- Case A validates the baseline where all but $r$ singular values are exactly $0$ and the absolute tolerance discriminates cleanly.\n- Case B stresses numerical rank detection under ill-conditioning by spanning orders of magnitude across the prescribed singular values while still being exactly rank $r$.\n- Case C simulates a realistic scenario where tiny noise makes all singular values strictly positive in theory; the relative tolerance must still correctly identify the effective rank $r$.\n\nYour final output must be a single line in the format $[c_{A},c_{B},c_{C}]$ where each $c_{\\cdot}$ is an integer. No physical units or angles are involved in this problem. Do not print anything else.",
            "solution": "We start from the fundamental definition of the Singular Value Decomposition (SVD). For any real matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}$, there exist orthogonal matrices $\\mathbf{U} \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{V} \\in \\mathbb{R}^{m \\times m}$, and a diagonal (possibly rectangular) matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times m}$, such that $\\mathbf{X} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{\\top}$, where the diagonal entries $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge 0$ of $\\boldsymbol{\\Sigma}$ are the singular values. A central linear algebra fact is that the rank of $\\mathbf{X}$ equals the number of strictly positive singular values of $\\mathbf{X}$.\n\nProper Orthogonal Decomposition (POD) computes an orthonormal basis that captures the dominant energetic modes of a set of snapshots by applying the SVD to the snapshot matrix. Specifically, for a snapshot matrix $\\mathbf{X}$, the POD modes are the leading left singular vectors of $\\mathbf{X}$, and the associated energies are proportional to the squares of the singular values. Therefore, verifying that POD identifies exactly $r$ nonzero singular values is equivalent to verifying that the SVD of $\\mathbf{X}$ has exactly $r$ singular values that are numerically significant.\n\nTo construct a matrix with prescribed rank $r$, we use the following well-tested fact: if $\\mathbf{U}_{r} \\in \\mathbb{R}^{n \\times r}$ and $\\mathbf{V}_{r} \\in \\mathbb{R}^{m \\times r}$ have orthonormal columns and $\\boldsymbol{\\Sigma}_{r} = \\operatorname{diag}(s_{1},\\dots,s_{r})$ with $s_{i} > 0$, then\n$$\n\\mathbf{X} = \\mathbf{U}_{r}\\, \\boldsymbol{\\Sigma}_{r}\\, \\mathbf{V}_{r}^{\\top}\n$$\nhas rank exactly $r$, with singular values equal to $(s_{1},\\dots,s_{r})$, and with all remaining singular values equal to $0$. We obtain $\\mathbf{U}_{r}$ and $\\mathbf{V}_{r}$ by taking the $Q$ factor from a QR decomposition of random Gaussian matrices of shapes $n \\times r$ and $m \\times r$, respectively, which yields orthonormal columns with probability $1$ in exact arithmetic and is robust in floating-point arithmetic.\n\nNumerical rank detection requires a tolerance because floating-point errors can perturb exact zeros into small nonzero values. Two standard choices are:\n- Absolute threshold: count singular values $\\sigma_{i}$ with $\\sigma_{i} \\ge \\tau$ where $\\tau > 0$ is fixed. This is effective when the scale of the matrix is known a priori.\n- Relative threshold: count singular values $\\sigma_{i}$ with $\\sigma_{i} \\ge \\rho \\, \\sigma_{\\max}$, where $\\sigma_{\\max}$ is the largest singular value and $\\rho > 0$ is dimensionless. This is scale-invariant and is well-suited for comparing across differently scaled matrices.\n\nWe now apply this framework to the specified cases with $n = 100$, $m = 50$, and $r = 3$.\n\nCase A (happy path, exact rank-$3$):\n- Construct $\\mathbf{U}_{3}$ and $\\mathbf{V}_{3}$ with orthonormal columns via QR decompositions of random Gaussian matrices of shapes $100 \\times 3$ and $50 \\times 3$.\n- Choose singular values $(s_{1}, s_{2}, s_{3}) = (5, 3, 1)$ and form $\\mathbf{X} = \\mathbf{U}_{3} \\operatorname{diag}(5, 3, 1) \\mathbf{V}_{3}^{\\top}$.\n- Compute the SVD to obtain singular values $(\\sigma_{i})$.\n- Use the absolute rule with $\\tau = 10^{-12}$ and count how many $\\sigma_{i}$ satisfy $\\sigma_{i} \\ge 10^{-12}$. In exact arithmetic, there are $3$ positive singular values and $47$ zeros. In floating-point arithmetic, the zeros remain far below $10^{-12}$ because the matrix scale is $\\mathcal{O}(1)$, hence the count equals $3$.\n\nCase B (ill-conditioned but exact rank-$3$):\n- Construct $\\mathbf{U}_{3}$ and $\\mathbf{V}_{3}$ as before.\n- Choose singular values $(s_{1}, s_{2}, s_{3}) = (1, 10^{-8}, 10^{-12})$. The matrix remains exactly rank $3$.\n- Compute the SVD to obtain $(\\sigma_{i})$.\n- Use the relative rule with $\\rho = 10^{-13}$ and count how many $\\sigma_{i}$ satisfy $\\sigma_{i} \\ge 10^{-13} \\, \\sigma_{\\max}$. Here, $\\sigma_{\\max} = 1$, so the threshold is $10^{-13}$. The three prescribed singular values are $1$, $10^{-8}$, and $10^{-12}$, all of which exceed $10^{-13}$, while any numerical artifacts corresponding to the remaining $47$ values are on the order of machine precision and remain below the threshold. The count equals $3$.\n\nCase C (near rank-$3$ with tiny additive noise):\n- Construct $\\mathbf{U}_{3}$ and $\\mathbf{V}_{3}$ as before.\n- Choose singular values $(s_{1}, s_{2}, s_{3}) = (2, 2 \\times 10^{-1}, 2 \\times 10^{-2})$ and form $\\mathbf{X} = \\mathbf{U}_{3} \\operatorname{diag}(2, 2 \\times 10^{-1}, 2 \\times 10^{-2}) \\mathbf{V}_{3}^{\\top}$.\n- Add Gaussian noise $\\mathbf{N}$ with independent entries of zero mean and standard deviation $\\epsilon \\, s_{1}$ with $\\epsilon = 10^{-10}$, i.e., $\\mathbf{X} \\leftarrow \\mathbf{X} + \\mathbf{N}$.\n- Compute $(\\sigma_{i})$ via SVD.\n- Use the relative rule with $\\rho = 10^{-8}$ and count how many $\\sigma_{i}$ satisfy $\\sigma_{i} \\ge 10^{-8} \\, \\sigma_{\\max}$. The added noise has a spectral norm on the order of $\\epsilon \\, s_{1} \\, (\\sqrt{n} + \\sqrt{m})$, which is approximately $10^{-10} \\times 2 \\times (10 + 7) \\approx 3.4 \\times 10^{-9}$. The relative threshold is $10^{-8} \\, \\sigma_{\\max} \\approx 2 \\times 10^{-8}$, so the noise-induced singular values remain below the threshold, while the three intended singular values remain far above it. The count equals $3$.\n\nAlgorithmic design:\n- Fix a random seed to ensure reproducibility.\n- For each case, build $\\mathbf{U}_{3}$ and $\\mathbf{V}_{3}$ via QR factorization of Gaussian random matrices, assemble $\\mathbf{X}$ with the prescribed singular values, optionally add noise, compute the SVD to get $(\\sigma_{i})$, and count according to the specified rule.\n- Aggregate the three integer counts into a single list and print it on one line as $[c_{A},c_{B},c_{C}]$.\n\nBy the fundamental link between POD and SVD, the program verifies that POD recovers exactly $3$ numerically significant singular values in all three scenarios when using appropriate tolerances. The expected output is $[3,3,3]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef orthonormal_columns(rng: np.random.Generator, rows: int, cols: int) -> np.ndarray:\n    \"\"\"\n    Generate a matrix with orthonormal columns using QR decomposition.\n    \"\"\"\n    A = rng.standard_normal((rows, cols))\n    Q, _ = np.linalg.qr(A, mode=\"reduced\")\n    # Ensure we have exactly 'cols' columns (qr reduced already does this)\n    return Q[:, :cols]\n\ndef build_rank_r_matrix(rng: np.random.Generator, n: int, m: int, singular_values: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Construct X = U diag(s) V^T with U in R^{n x r}, V in R^{m x r} orthonormal, s in R^r.\n    \"\"\"\n    r = singular_values.size\n    U = orthonormal_columns(rng, n, r)\n    V = orthonormal_columns(rng, m, r)\n    S = np.diag(singular_values)\n    X = U @ S @ V.T\n    return X\n\ndef count_significant_singular_values(X: np.ndarray, tol_type: str, tol_value: float) -> int:\n    \"\"\"\n    Compute the singular values of X and count how many are significant according to:\n    - tol_type == 'abs': sigma_i >= tol_value\n    - tol_type == 'rel': sigma_i >= tol_value * sigma_max\n    \"\"\"\n    svals = np.linalg.svd(X, full_matrices=False, compute_uv=False)\n    if tol_type == 'abs':\n        threshold = tol_value\n    elif tol_type == 'rel':\n        threshold = tol_value * svals[0] if svals.size > 0 else 0.0\n    else:\n        raise ValueError(\"Unknown tol_type\")\n    return int(np.sum(svals >= threshold))\n\ndef solve():\n    rng = np.random.default_rng(123456789)  # Fixed seed for reproducibility\n\n    n, m, r = 100, 50, 3\n\n    # Define test cases: (singular_values, tol_type, tol_value, noise_scale_relative_to_sigma_max)\n    test_cases = [\n        # Case A: exact rank-3, absolute tolerance\n        (np.array([5.0, 3.0, 1.0]), 'abs', 1e-12, 0.0),\n        # Case B: ill-conditioned, relative tolerance\n        (np.array([1.0, 1e-8, 1e-12]), 'rel', 1e-13, 0.0),\n        # Case C: near rank-3 with tiny noise, relative tolerance\n        (np.array([2.0, 2e-1, 2e-2]), 'rel', 1e-8, 1e-10),\n    ]\n\n    results = []\n    for svals_target, tol_type, tol_value, noise_scale in test_cases:\n        X = build_rank_r_matrix(rng, n, m, svals_target)\n        if noise_scale > 0.0:\n            # Add Gaussian noise with std = noise_scale * sigma_max\n            sigma_max = float(np.max(svals_target))\n            noise = rng.standard_normal((n, m)) * (noise_scale * sigma_max)\n            X = X + noise\n        count = count_significant_singular_values(X, tol_type, tol_value)\n        results.append(count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Real-world data is rarely as clean as our previous examples. This advanced practice addresses a common challenge in scientific and engineering applications: data from non-uniform grids or finite element simulations. You will implement a \"weighted\" POD that uses a problem-specific inner product, a necessary step to correctly interpret energy and perform meaningful model reduction in physically complex systems . This exercise demonstrates how to generalize the standard POD algorithm to a much wider and more practical class of problems.",
            "id": "3981898",
            "problem": "Consider a one-dimensional transient heat conduction process in a homogeneous rod of length $L=1$ with homogeneous Dirichlet boundary conditions. Let the temperature field be discretized in space by $m$ grid points and in time by $n_s$ snapshots, forming the snapshot matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n_s}$ whose columns are spatial temperature fields at different times. The spatial inner product that represents thermally meaningful energy content is defined by a symmetric positive-definite (SPD) weight matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times m}$, which arises from spatial quadrature or a finite-volume/finite-element mass matrix, so that for vectors $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^m$ the weighted inner product is $\\langle \\mathbf{u}, \\mathbf{v} \\rangle_{\\mathbf{W}} = \\mathbf{u}^\\top \\mathbf{W} \\mathbf{v}$.\n\nStarting from the conservation of energy and Fourier’s law, the heat equation in one dimension is given by $\\rho c \\,\\partial T/\\partial t = k \\,\\partial^2 T/\\partial x^2$, where $\\rho$ is density, $c$ is specific heat, and $k$ is thermal conductivity. When discretized in space, the $\\mathbf{W}$-weighted inner product is used to measure energy norms of temperature fields. The Proper Orthogonal Decomposition (POD) seeks a reduced-order basis that minimizes the mean squared projection error measured in the $\\mathbf{W}$-weighted norm, and the Reduced-Order Model (ROM) uses the corresponding time-dependent scores to represent the snapshot data.\n\nYour task is to implement a complete algorithm to compute $r$ POD modes $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{m \\times r}$ that are orthonormal with respect to $\\mathbf{W}$ and the corresponding scores $\\mathbf{a}(t)$ for each snapshot, by:\n- Deriving an algorithm from the $\\mathbf{W}$-weighted optimality definition without relying on shortcut formulas, starting from the variational characterization of minimizing the $\\mathbf{W}$-weighted reconstruction error and connecting it to a computational procedure that is well-posed and numerically stable.\n- Ensuring $\\boldsymbol{\\Phi}^\\top \\mathbf{W} \\boldsymbol{\\Phi} = \\mathbf{I}_r$ and computing the coefficient matrix that reconstructs the snapshots from the reduced basis.\n- Quantifying the energy captured by the $r$ modes and the relative reconstruction error using the $\\mathbf{W}$-weighted Frobenius norm.\n\nFor testing, construct $\\mathbf{X}$ using the analytical series solution of the one-dimensional heat equation with homogeneous Dirichlet boundary conditions:\n$$\nT(x,t) = \\sum_{n=1}^{N} b_n \\sin(n\\pi x)\\,\\exp\\left(-\\alpha (n\\pi)^2 t\\right),\n$$\nsampled at $m$ spatial points $\\{x_i\\}_{i=1}^m$ on $[0,1]$ and $n_s$ time instances on $[0,t_{\\max}]$. Use a nonuniform spatial grid defined by $x_i = \\left(\\frac{i}{m-1}\\right)^{p}$ for $i=0,\\dots,m-1$ with exponent $p>0$, and construct $\\mathbf{W}$ as the diagonal SPD matrix of trapezoidal-rule weights $w_i$ that approximate $\\int_0^1 f(x)^2 \\, dx \\approx \\sum_{i=0}^{m-1} w_i f(x_i)^2$. The trapezoidal weights on a nonuniform grid are defined by $w_0 = \\frac{x_1 - x_0}{2}$, $w_{m-1} = \\frac{x_{m-1} - x_{m-2}}{2}$, and $w_i = \\frac{x_{i+1} - x_{i-1}}{2}$ for $i=1,\\dots,m-2$.\n\nCompute the relative reconstruction error for rank $r$ as a dimensionless quantity:\n$$\n\\varepsilon_r = 1 - \\frac{E_r}{E_{\\text{tot}}},\n$$\nwhere $E_{\\text{tot}}$ is the total $\\mathbf{W}$-weighted snapshot energy and $E_r$ is the energy captured by the $r$ modes. Express all final outputs as decimal floats with no physical units.\n\nImplement your program to produce results for the following test suite, each specified by $(m, n_s, t_{\\max}, \\alpha, N, \\{b_n\\}, p, r, c)$, where $c$ scales the weight matrix as $\\mathbf{W} \\leftarrow c\\,\\mathbf{W}$:\n\n- Test Case $1$ (general case, nonuniform grid, moderate diffusivity):\n    - $m=80$, $n_s=60$, $t_{\\max}=1.0$, $\\alpha=0.05$, $N=3$, $\\{b_n\\} = \\{1.0, 0.8, 0.5\\}$, $p=1.2$, $r=3$, $c=1.0$.\n- Test Case $2$ (boundary case where the number of retained modes equals the number of generating spatial modes):\n    - $m=60$, $n_s=40$, $t_{\\max}=1.2$, $\\alpha=0.04$, $N=2$, $\\{b_n\\} = \\{1.0, 0.5\\}$, $p=1.1$, $r=2$, $c=1.0$.\n- Test Case $3$ (edge case testing invariance of relative error under scaling of $\\mathbf{W}$):\n    - $m=80$, $n_s=60$, $t_{\\max}=1.0$, $\\alpha=0.05$, $N=3$, $\\{b_n\\} = \\{1.0, 0.8, 0.5\\}$, $p=1.2$, $r=3$, $c=7.0$.\n- Test Case $4$ (edge case $r=0$):\n    - $m=80$, $n_s=60$, $t_{\\max}=1.0$, $\\alpha=0.05$, $N=3$, $\\{b_n\\} = \\{1.0, 0.8, 0.5\\}$, $p=1.2$, $r=0$, $c=1.0$.\n\nYour program must:\n- Construct $\\mathbf{X}$ according to the specified parameters for each test case.\n- Construct $\\mathbf{W}$ using the trapezoidal rule on the specified nonuniform grid and apply the scaling $c$.\n- Compute $r$ $\\mathbf{W}$-orthonormal POD modes and the corresponding scores.\n- Compute $\\varepsilon_r$ for each test case based on the $\\mathbf{W}$-weighted Frobenius norm of the snapshot matrix and the contribution of the retained modes.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases: for example, $[\\varepsilon_1,\\varepsilon_2,\\varepsilon_3,\\varepsilon_4]$.\n\nAll angles, if any appear, must be expressed in radians. The final numerical outputs are dimensionless decimal floats. No external input is permitted; the program must run self-contained and produce the single-line output.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It poses a standard task in computational engineering: performing a Proper Orthogonal Decomposition (POD) on snapshot data from a physical simulation, specifically using a problem-specific weighted inner product. All parameters are clearly defined, and the test cases are structured to verify the implementation's correctness and robustness, including its handling of fundamental properties like invariance to energy scaling.\n\nThe core of the task is to derive and implement an algorithm for computing POD modes that are optimal with respect to a weighted inner product defined by a symmetric positive-definite (SPD) matrix $\\mathbf{W}$. This is often called weighted POD or generalized POD.\n\nLet the snapshot data be collected in a matrix $\\mathbf{X} = [\\mathbf{x}_1, \\dots, \\mathbf{x}_{n_s}] \\in \\mathbb{R}^{m \\times n_s}$, where each column $\\mathbf{x}_k$ is a vector of temperature values at $m$ spatial points at time $t_k$. The weighted inner product is $\\langle \\mathbf{u}, \\mathbf{v} \\rangle_{\\mathbf{W}} = \\mathbf{u}^\\top \\mathbf{W} \\mathbf{v}$, and the corresponding norm is $||\\mathbf{u}||_{\\mathbf{W}} = \\sqrt{\\mathbf{u}^\\top \\mathbf{W} \\mathbf{u}}$.\n\nThe goal of POD is to find a set of $r$ basis vectors (modes) $\\boldsymbol{\\Phi} = [\\boldsymbol{\\phi}_1, \\dots, \\boldsymbol{\\phi}_r] \\in \\mathbb{R}^{m \\times r}$ that are orthonormal with respect to the $\\mathbf{W}$-inner product, i.e., $\\boldsymbol{\\Phi}^\\top \\mathbf{W} \\boldsymbol{\\Phi} = \\mathbf{I}_r$, and that minimize the average squared error between the original snapshots and their projection onto the subspace spanned by $\\boldsymbol{\\Phi}$. The projection of a snapshot $\\mathbf{x}_k$ is given by $\\mathbf{x}_{k,r} = \\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^\\top \\mathbf{W} \\mathbf{x}_k$.\n\nThe optimization problem is thus:\n$$\n\\min_{\\boldsymbol{\\Phi} \\in \\mathbb{R}^{m \\times r}, \\boldsymbol{\\Phi}^\\top \\mathbf{W} \\boldsymbol{\\Phi} = \\mathbf{I}_r} \\sum_{k=1}^{n_s} || \\mathbf{x}_k - \\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^\\top \\mathbf{W} \\mathbf{x}_k ||_{\\mathbf{W}}^2\n$$\nDue to the properties of projections onto an orthonormal basis, this minimization is equivalent to maximizing the projected energy:\n$$\n\\max_{\\boldsymbol{\\Phi} \\in \\mathbb{R}^{m \\times r}, \\boldsymbol{\\Phi}^\\top \\mathbf{W} \\boldsymbol{\\Phi} = \\mathbf{I}_r} \\sum_{j=1}^r \\sum_{k=1}^{n_s} (\\boldsymbol{\\phi}_j^\\top \\mathbf{W} \\mathbf{x}_k)^2\n$$\nThis can be written in matrix form as maximizing $\\text{Tr}(\\boldsymbol{\\Phi}^\\top \\mathbf{W} \\mathbf{X} \\mathbf{X}^\\top \\mathbf{W} \\boldsymbol{\\Phi})$. By the Courant-Fischer theorem, the solution is given by the eigenvectors of the generalized eigenvalue problem $\\mathbf{W} \\mathbf{X} \\mathbf{X}^\\top \\mathbf{W} \\boldsymbol{\\phi}_j = \\lambda_j \\mathbf{W} \\boldsymbol{\\phi}_j$. Solving this directly can be unstable or inefficient.\n\nA more robust and computationally stable approach involves transforming the problem into a standard POD problem. Since $\\mathbf{W}$ is SPD, it has a unique SPD square root, $\\mathbf{W}^{1/2}$, which can also be viewed as the Cholesky factor $\\mathbf{L}$ where $\\mathbf{W}=\\mathbf{L}\\mathbf{L}^\\top$. As $\\mathbf{W}$ is diagonal, $\\mathbf{W}^{1/2}$ is simply the diagonal matrix with the square roots of the diagonal entries of $\\mathbf{W}$.\n\nLet us define a transformed snapshot matrix $\\tilde{\\mathbf{X}} = \\mathbf{W}^{1/2} \\mathbf{X}$ and transformed basis vectors $\\tilde{\\boldsymbol{\\Phi}} = \\mathbf{W}^{1/2} \\boldsymbol{\\Phi}$. The orthonormality condition $\\boldsymbol{\\Phi}^\\top \\mathbf{W} \\boldsymbol{\\Phi} = \\mathbf{I}_r$ becomes $\\boldsymbol{\\Phi}^\\top (\\mathbf{W}^{1/2})^\\top \\mathbf{W}^{1/2} \\boldsymbol{\\Phi} = (\\mathbf{W}^{1/2}\\boldsymbol{\\Phi})^\\top(\\mathbf{W}^{1/2}\\boldsymbol{\\Phi}) = \\tilde{\\boldsymbol{\\Phi}}^\\top \\tilde{\\boldsymbol{\\Phi}} = \\mathbf{I}_r$. The transformed basis vectors must be orthonormal in the standard Euclidean inner product.\n\nSimilarly, the objective function becomes minimizing the standard Frobenius norm of the reconstruction error for the transformed data:\n$$\n\\sum_{k=1}^{n_s} || \\mathbf{x}_k - \\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^\\top \\mathbf{W} \\mathbf{x}_k ||_{\\mathbf{W}}^2 = || \\mathbf{W}^{1/2}(\\mathbf{X} - \\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^\\top \\mathbf{W} \\mathbf{X}) ||_F^2 = || \\tilde{\\mathbf{X}} - \\tilde{\\boldsymbol{\\Phi}}\\tilde{\\boldsymbol{\\Phi}}^\\top \\tilde{\\mathbf{X}} ||_F^2\n$$\nThis is the standard POD problem for the matrix $\\tilde{\\mathbf{X}}$. The solution is given by the Singular Value Decomposition (SVD) of $\\tilde{\\mathbf{X}}$. Let $\\tilde{\\mathbf{X}} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top$. The optimal rank-$r$ basis $\\tilde{\\boldsymbol{\\Phi}}$ is composed of the first $r$ left singular vectors of $\\tilde{\\mathbf{X}}$, i.e., $\\tilde{\\boldsymbol{\\Phi}} = \\mathbf{U}_{:,1:r}$.\nThe POD modes in the original, physical space are obtained by transforming back: $\\boldsymbol{\\Phi} = (\\mathbf{W}^{1/2})^{-1} \\tilde{\\boldsymbol{\\Phi}} = \\mathbf{W}^{-1/2} \\mathbf{U}_{:,1:r}$.\n\nThe total energy of the snapshots, measured in the $\\mathbf{W}$-weighted Frobenius norm, is:\n$$\nE_{\\text{tot}} = ||\\mathbf{X}||_{\\mathbf{W},F}^2 = \\text{Tr}(\\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}) = \\text{Tr}((\\mathbf{W}^{1/2}\\mathbf{X})^\\top (\\mathbf{W}^{1/2}\\mathbf{X})) = ||\\tilde{\\mathbf{X}}||_F^2\n$$\nFrom SVD properties, $||\\tilde{\\mathbf{X}}||_F^2 = \\sum_j \\sigma_j^2$, where $\\sigma_j$ are the singular values of $\\tilde{\\mathbf{X}}$. The energy captured by the first $r$ POD modes is $E_r = \\sum_{j=1}^r \\sigma_j^2$.\nThe relative reconstruction error is then:\n$$\n\\varepsilon_r = 1 - \\frac{E_r}{E_{\\text{tot}}} = 1 - \\frac{\\sum_{j=1}^r \\sigma_j^2}{\\sum_j \\sigma_j^2} = \\frac{\\sum_{j=r+1}^{\\min(m,n_s)} \\sigma_j^2}{\\sum_{j=1}^{\\min(m,n_s)} \\sigma_j^2}\n$$\n\nThe algorithmic procedure is as follows:\n1.  For each test case, define the parameters $m, n_s, t_{\\max}, \\alpha, N, \\{b_n\\}, p, r, c$.\n2.  Construct the nonuniform spatial grid $\\{x_i\\}_{i=0}^{m-1}$ using $x_i = (\\frac{i}{m-1})^p$.\n3.  Construct the diagonal weight matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times m}$ using the given trapezoidal rule for the nonuniform grid and apply the scaling factor $c$.\n4.  Construct the snapshot matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n_s}$ by evaluating the analytical solution for $T(x,t)$ at the spatial grid points and at $n_s$ uniformly spaced time instances in $[0, t_{\\max}]$.\n5.  Compute the diagonal matrix $\\mathbf{W}^{1/2}$.\n6.  Form the weighted snapshot matrix $\\tilde{\\mathbf{X}} = \\mathbf{W}^{1/2} \\mathbf{X}$.\n7.  Compute the singular values $\\sigma_j$ of $\\tilde{\\mathbf{X}}$ using SVD.\n8.  Calculate the total energy $E_{\\text{tot}} = \\sum_j \\sigma_j^2$ and the captured energy $E_r = \\sum_{j=1}^r \\sigma_j^2$.\n9.  Compute the relative error $\\varepsilon_r = 1 - E_r / E_{\\text{tot}}$. For the edge case $r=0$, this yields $\\varepsilon_0 = 1$. The case $E_{\\text{tot}}=0$ leads to $\\varepsilon_r=0$ by definition.\n10. Store the result for each test case and format the final output as a comma-separated list.\n\nAs an analytical check, scaling the weight matrix $\\mathbf{W}$ by a constant $c>0$ scales the transformed matrix $\\tilde{\\mathbf{X}}$ by $\\sqrt{c}$, its singular values $\\sigma_j$ by $\\sqrt{c}$, and both $E_{\\text{tot}}$ and $E_r$ by $c$. Therefore, the relative error $\\varepsilon_r$ must be invariant to this scaling. Test Case 3 provides a numerical verification of this property.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Main function to solve the POD problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # (m, ns, t_max, alpha, N, b_n, p, r, c)\n        (80, 60, 1.0, 0.05, 3, [1.0, 0.8, 0.5], 1.2, 3, 1.0),\n        (60, 40, 1.2, 0.04, 2, [1.0, 0.5], 1.1, 2, 1.0),\n        (80, 60, 1.0, 0.05, 3, [1.0, 0.8, 0.5], 1.2, 3, 7.0),\n        (80, 60, 1.0, 0.05, 3, [1.0, 0.8, 0.5], 1.2, 0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        m, ns, t_max, alpha, N, b_n_coeffs, p, r, c = case\n\n        # Step 1: Construct spatial and temporal grids\n        # Spatial grid on [0, 1] with m points\n        x = np.linspace(0, 1.0, m)**p\n        # Temporal grid on [0, t_max] with ns points\n        t = np.linspace(0, 1.0, ns) * t_max\n\n        # Step 2: Construct the snapshot matrix X\n        X = np.zeros((m, ns))\n        # Use broadcasting for efficiency: x becomes (m,1), t becomes (1,ns)\n        x_col = x[:, np.newaxis]\n        t_row = t[np.newaxis, :]\n        \n        for n_idx, b_val in enumerate(b_n_coeffs):\n            n = n_idx + 1\n            mode_term = b_val * np.sin(n * np.pi * x_col) * np.exp(-alpha * (n * np.pi)**2 * t_row)\n            X += mode_term\n\n        # Step 3: Construct the W matrix (trapezoidal weights)\n        w = np.zeros(m)\n        if m > 1:\n            dx = np.diff(x)\n            w[0] = dx[0] / 2.0\n            w[-1] = dx[-1] / 2.0\n            if m > 2:\n                w[1:-1] = (dx[:-1] + dx[1:]) / 2.0\n        \n        # W is a diagonal matrix. For multiplication, we can just use the vector w.\n        # Apply scaling factor c\n        w *= c\n\n        # Step 4: Form the weighted snapshot matrix\n        # W_sqrt is diag(sqrt(w)). W_sqrt @ X is equivalent to multiplying each row of X\n        # by the corresponding sqrt(w_i).\n        w_sqrt_diag = np.sqrt(w)\n        X_tilde = w_sqrt_diag[:, np.newaxis] * X\n\n        # Step 5: Compute SVD of the weighted snapshot matrix\n        # We only need the singular values. full_matrices=False is more efficient.\n        # compute_uv=False would be even better if only singular values are needed.\n        sigmas = svd(X_tilde, compute_uv=False)\n\n        # Step 6: Calculate energy and relative error\n        # The eigenvalues of the correlation matrix are the squares of the singular values.\n        eigenvalues = sigmas**2\n        \n        total_energy = np.sum(eigenvalues)\n        \n        if r > len(eigenvalues):\n            # If r is larger than the number of available modes, clamp it.\n            r_clamped = len(eigenvalues)\n        else:\n            r_clamped = r\n            \n        # For r=0, the sum over an empty slice is correctly 0.\n        captured_energy = np.sum(eigenvalues[:r_clamped])\n\n        if total_energy == 0.0:\n            # Avoid division by zero if all snapshots are zero.\n            relative_error = 0.0\n        else:\n            relative_error = 1.0 - (captured_energy / total_energy)\n            \n        results.append(relative_error)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}