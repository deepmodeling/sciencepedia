{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of Bayesian inference is the concept of conjugacy, where the posterior distribution follows the same parametric form as the prior distribution. This exercise  invites you to explore this powerful property in a common scenario: inferring the unknown mean and variance of a Gaussian process. By deriving the posterior distribution for a Normal-Inverse-Gamma prior, you will gain hands-on experience with the mechanics of Bayesian updating and see how prior beliefs are analytically combined with observed data.",
            "id": "3770713",
            "problem": "Consider a two-level multiscale model in which fine-scale microscale simulations produce summaries that are aggregated into $n$ coarse-scale observations $y_1, y_2, \\dots, y_n$ used for inverse uncertainty quantification of an unknown coarse-grained bias parameter $\\theta$ and a coarse-scale noise variance $\\sigma^2$. Assume the conditional data model $y_i \\mid \\theta, \\sigma^2$ are independent and identically distributed Gaussian with $y_i \\mid \\theta, \\sigma^2 \\sim \\mathcal{N}(\\theta, \\sigma^2)$, and adopt the hierarchical Normal–Inverse-Gamma prior, defined as $\\theta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 / \\kappa_0)$ and $\\sigma^2 \\sim \\operatorname{InvGamma}(\\alpha_0, \\beta_0)$, where $\\operatorname{InvGamma}$ denotes the Inverse-Gamma distribution with shape $\\alpha_0$ and scale $\\beta_0$. This Normal–Inverse-Gamma (NIG) prior couples scales by allowing the variance at the coarse scale to inform the spread of the coarse-grained bias.\n\nStarting from Bayes' theorem and the definitions of the Gaussian and Inverse-Gamma densities, derive the full conditional distributions $p(\\theta \\mid \\sigma^2, y_1, \\dots, y_n)$ and $p(\\sigma^2 \\mid \\theta, y_1, \\dots, y_n)$. Then, show conjugacy by expressing the joint posterior $p(\\theta, \\sigma^2 \\mid y_1, \\dots, y_n)$ in the Normal–Inverse-Gamma family and identify the updated hyperparameters $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$ in closed form as functions of $(m_0, \\kappa_0, \\alpha_0, \\beta_0)$ and the sufficient statistics of the data $(n, \\bar{y}, \\sum_{i=1}^{n} (y_i - \\bar{y})^2)$, where $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$.\n\nYour final answer must be the vector of updated hyperparameters $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$ written as a single row matrix using the $\\operatorname{pmatrix}$ environment. Do not provide intermediate steps in the final answer. No numerical rounding is required, and no units are involved. All mathematical symbols must be written in LaTeX.",
            "solution": "The problem is valid as it is a well-posed, scientifically grounded, and objective problem in Bayesian statistics. We are asked to derive the full conditional posterior distributions and the joint posterior distribution for the parameters $\\theta$ and $\\sigma^2$ given a set of observations $y_1, \\dots, y_n$. The model assumes a Gaussian likelihood and a conjugate Normal-Inverse-Gamma (NIG) prior.\n\nThe model is specified as follows:\nLikelihood: $y_i \\mid \\theta, \\sigma^2 \\sim \\mathcal{N}(\\theta, \\sigma^2)$ for $i=1, \\dots, n$, assumed to be independent and identically distributed.\nPrior: A hierarchical prior given by $p(\\theta, \\sigma^2) = p(\\theta \\mid \\sigma^2)p(\\sigma^2)$, where:\n$\\theta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 / \\kappa_0)$\n$\\sigma^2 \\sim \\operatorname{InvGamma}(\\alpha_0, \\beta_0)$\n\nThe probability density functions (PDFs) are:\n-   $p(y_i \\mid \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta)^2}{2\\sigma^2}\\right)$\n-   $p(\\theta \\mid \\sigma^2) = \\sqrt{\\frac{\\kappa_0}{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\kappa_0(\\theta - m_0)^2}{2\\sigma^2}\\right)$\n-   $p(\\sigma^2) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} (\\sigma^2)^{-(\\alpha_0+1)} \\exp\\left(-\\frac{\\beta_0}{\\sigma^2}\\right)$\n\nAccording to Bayes' theorem, the joint posterior distribution is proportional to the product of the likelihood and the prior:\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\theta, \\sigma^2) p(\\theta, \\sigma^2)$\nwhere $\\mathbf{y} = (y_1, \\dots, y_n)$.\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto \\left(\\prod_{i=1}^{n} p(y_i \\mid \\theta, \\sigma^2)\\right) p(\\theta \\mid \\sigma^2) p(\\sigma^2)$\n\nSubstituting the PDFs and dropping constants that do not depend on $\\theta$ or $\\sigma^2$:\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto \\left[ (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - \\theta)^2\\right) \\right] \\times \\left[ (\\sigma^2)^{-1/2} \\exp\\left(-\\frac{\\kappa_0(\\theta - m_0)^2}{2\\sigma^2}\\right) \\right] \\times \\left[ (\\sigma^2)^{-(\\alpha_0+1)} \\exp\\left(-\\frac{\\beta_0}{\\sigma^2}\\right) \\right]$\n\nCombining terms:\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + 1/2 + 1)} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ \\sum_{i=1}^{n}(y_i - \\theta)^2 + \\kappa_0(\\theta - m_0)^2 + 2\\beta_0 \\right] \\right\\}$\n\nFirst, we derive the full conditional distribution $p(\\theta \\mid \\sigma^2, \\mathbf{y})$. We treat $\\sigma^2$ and $\\mathbf{y}$ as given and consider the posterior as a function of $\\theta$:\n$p(\\theta \\mid \\sigma^2, \\mathbf{y}) \\propto \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ \\sum_{i=1}^{n}(y_i - \\theta)^2 + \\kappa_0(\\theta - m_0)^2 \\right] \\right\\}$\nThe term in the exponent is a quadratic in $\\theta$. Let's expand and group terms in $\\theta$:\n$\\sum_{i=1}^{n}(y_i^2 - 2y_i\\theta + \\theta^2) + \\kappa_0(\\theta^2 - 2m_0\\theta + m_0^2) = (n+\\kappa_0)\\theta^2 - 2(n\\bar{y} + \\kappa_0m_0)\\theta + (\\sum y_i^2 + \\kappa_0m_0^2)$\nwhere $\\bar{y} = \\frac{1}{n}\\sum y_i$. This expression is of the form $A\\theta^2 - 2B\\theta + C$. Completing the square:\n$A\\theta^2 - 2B\\theta + C = A(\\theta - B/A)^2 + C - B^2/A$\nHere, $A = n+\\kappa_0$ and $B = n\\bar{y}+\\kappa_0m_0$. So, the kernel for $\\theta$ is:\n$p(\\theta \\mid \\sigma^2, \\mathbf{y}) \\propto \\exp\\left\\{ -\\frac{n+\\kappa_0}{2\\sigma^2} \\left( \\theta - \\frac{n\\bar{y}+\\kappa_0m_0}{n+\\kappa_0} \\right)^2 \\right\\}$\nThis is the kernel of a Normal distribution with mean $\\frac{n\\bar{y}+\\kappa_0m_0}{n+\\kappa_0}$ and variance $\\frac{\\sigma^2}{n+\\kappa_0}$.\nSo, the full conditional for $\\theta$ is:\n$\\theta \\mid \\sigma^2, \\mathbf{y} \\sim \\mathcal{N}\\left(\\frac{\\kappa_0m_0 + n\\bar{y}}{\\kappa_0+n}, \\frac{\\sigma^2}{\\kappa_0+n}\\right)$\n\nNext, we derive the full conditional distribution $p(\\sigma^2 \\mid \\theta, \\mathbf{y})$. We treat $\\theta$ and $\\mathbf{y}$ as given and consider the posterior as a function of $\\sigma^2$:\n$p(\\sigma^2 \\mid \\theta, \\mathbf{y}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + 1/2 + 1)} \\exp\\left\\{ -\\frac{1}{\\sigma^2} \\left[ \\beta_0 + \\frac{1}{2}\\sum_{i=1}^{n}(y_i - \\theta)^2 + \\frac{\\kappa_0}{2}(\\theta - m_0)^2 \\right] \\right\\}$\nThis is the kernel of an Inverse-Gamma distribution, $p(x) \\propto x^{-(\\alpha+1)}\\exp(-\\beta/x)$.\nBy comparison, the shape parameter is $\\alpha_0 + n/2 + 1/2$ and the scale parameter is $\\beta_0 + \\frac{1}{2}\\sum(y_i - \\theta)^2 + \\frac{\\kappa_0}{2}(\\theta - m_0)^2$.\nSo, the full conditional for $\\sigma^2$ is:\n$\\sigma^2 \\mid \\theta, \\mathbf{y} \\sim \\operatorname{InvGamma}\\left(\\alpha_0 + \\frac{n+1}{2}, \\beta_0 + \\frac{1}{2}\\left[\\sum_{i=1}^{n}(y_i - \\theta)^2 + \\kappa_0(\\theta - m_0)^2\\right]\\right)$\n\nFinally, to show conjugacy and find the updated hyperparameters for the joint posterior $p(\\theta, \\sigma^2 \\mid \\mathbf{y})$, we reorganize the full posterior expression. We substitute the completed-square form for $\\theta$ back into the exponent. Let $S = \\sum_{i=1}^{n}(y_i - \\bar{y})^2$. A useful identity is $\\sum(y_i-\\theta)^2 = S + n(\\bar{y}-\\theta)^2$.\nThe exponent term involving $\\theta$ is:\n$n(\\bar{y}-\\theta)^2 + \\kappa_0(m_0-\\theta)^2 = (n+\\kappa_0)\\theta^2 - 2(n\\bar{y}+\\kappa_0m_0)\\theta + n\\bar{y}^2+\\kappa_0m_0^2$.\nCompleting the square, this is equal to:\n$(\\kappa_0+n)\\left(\\theta - \\frac{\\kappa_0m_0+n\\bar{y}}{\\kappa_0+n}\\right)^2 + \\frac{n\\kappa_0}{n+\\kappa_0}(\\bar{y}-m_0)^2$\nSubstituting this back into the full posterior exponent expression (the term inside the square brackets):\n$\\left[ S + (\\kappa_0+n)\\left(\\theta - \\frac{\\kappa_0m_0+n\\bar{y}}{\\kappa_0+n}\\right)^2 + \\frac{n\\kappa_0}{n+\\kappa_0}(\\bar{y}-m_0)^2 + 2\\beta_0 \\right]$\nThe joint posterior becomes:\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + 1/2 + 1)} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ (\\kappa_0+n)\\left(\\theta - \\frac{\\kappa_0m_0+n\\bar{y}}{\\kappa_0+n}\\right)^2 + 2\\beta_0 + S + \\frac{n\\kappa_0}{\\kappa_0+n}(\\bar{y}-m_0)^2 \\right] \\right\\}$\nThis expression has the form of a Normal-Inverse-Gamma distribution, $p(\\theta, \\sigma^2) \\propto (\\sigma^2)^{-(\\alpha_n+1/2+1)} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ \\kappa_n(\\theta-m_n)^2 + 2\\beta_n \\right] \\right\\}$.\n\nBy comparing the derived posterior with the general NIG form, we identify the updated hyperparameters $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$:\n1.  $\\kappa_n$: The coefficient of the squared term for $\\theta$ (divided by $2\\sigma^2$) is $\\kappa_n/(2\\sigma^2)$. So,\n    $\\kappa_n = \\kappa_0 + n$\n2.  $m_n$: The mean of the Normal component is:\n    $m_n = \\frac{\\kappa_0m_0 + n\\bar{y}}{\\kappa_0 + n}$\n3.  $\\alpha_n$: The power of $\\sigma^2$ is $-(\\alpha_n+1/2+1)$. So, $\\alpha_n+1/2+1 = \\alpha_0+n/2+1/2+1$. This gives:\n    $\\alpha_n = \\alpha_0 + \\frac{n}{2}$\n4.  $\\beta_n$: The remaining term in the exponent is $2\\beta_n$. So, $2\\beta_n = 2\\beta_0 + S + \\frac{n\\kappa_0}{\\kappa_0+n}(\\bar{y}-m_0)^2$. This gives:\n    $\\beta_n = \\beta_0 + \\frac{1}{2}S + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2 = \\beta_0 + \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2$\n\nThus, the posterior distribution $p(\\theta, \\sigma^2 \\mid \\mathbf{y})$ is $NIG(m_n, \\kappa_n, \\alpha_n, \\beta_n)$ with the hyperparameters derived above.\n\nThe updated hyperparameters are:\n$m_n = \\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0 + n}$\n$\\kappa_n = \\kappa_0 + n$\n$\\alpha_n = \\alpha_0 + \\frac{n}{2}$\n$\\beta_n = \\beta_0 + \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0 + n}  \\kappa_0 + n  \\alpha_0 + \\frac{n}{2}  \\beta_0 + \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Inverse problems often harbor a subtle but critical challenge: parameter non-identifiability. This exercise  presents a minimalist model to starkly illustrate how a system can exhibit low uncertainty in its forward predictions while the underlying inverse problem remains ill-posed. By analyzing the Jacobian of this simple forward map, you will uncover why certain parameter combinations are impossible to distinguish from data alone, a key insight for designing and interpreting any inverse UQ study.",
            "id": "3770709",
            "problem": "Consider a two-scale conductive composite comprised of two parallel layers with equal area fractions. At the microscale, the layer conductivities are represented by the parameter vector $\\theta = (\\theta_1,\\theta_2)^{\\top}$, and the macroscale effective property of interest is the homogenized in-plane effective conductivity, modeled as the forward map $G:\\mathbb{R}^2 \\to \\mathbb{R}$ given by\n$$\nG(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2.\n$$\nAssume a probabilistic prior for $\\theta$ of the form $\\theta \\sim \\mathcal{N}(m, C)$, where $m \\in \\mathbb{R}^2$ and the covariance matrix is\n$$\nC = \\begin{pmatrix}\ns^2  -\\rho s^2 \\\\\n-\\rho s^2  s^2\n\\end{pmatrix},\n$$\nwith $s0$ and $0\\rho1$ representing a strong negative correlation. The data model is the additive Gaussian noise model $y = G(\\theta) + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_y^2)$ and known noise variance $\\sigma_y^20$. Treat all quantities as dimensionless.\n\nIn this minimal multiscale setting, you will examine the interplay between forward uncertainty quantification (forward Uncertainty Quantification (UQ)) and inverse uncertainty quantification (inverse Uncertainty Quantification (IUQ)). Starting from first principles:\n- Derive the forward output variance $\\operatorname{Var}(y)$ under the given prior and noise model, and explain why it becomes small as $\\rho \\to 1$.\n- Analyze the local identifiability of the inverse problem by examining the Gauss–Newton approximation to the negative log-likelihood Hessian. Specifically, compute the Jacobian $J(\\theta^{\\star})$ of $G$ at any $\\theta^{\\star} \\in \\mathbb{R}^2$, form the Gauss–Newton matrix $H = J(\\theta^{\\star})^{\\top}\\Sigma_y^{-1}J(\\theta^{\\star})$, where $\\Sigma_y = \\sigma_y^2$, and determine its determinant. Use this to justify why the inverse problem is ill-posed despite narrow forward output uncertainty.\n\nProvide as your final answer the exact value of $\\det(H)$ with no rounding. State any additional assumptions you use and ensure every mathematical symbol is written in LaTeX using $...$.",
            "solution": "The problem is first validated against the specified criteria.\n\n**Step 1: Extracted Givens**\n- Forward map: $G(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2$ for $\\theta = (\\theta_1, \\theta_2)^{\\top}$.\n- Prior distribution: $\\theta \\sim \\mathcal{N}(m, C)$, where $m \\in \\mathbb{R}^2$.\n- Prior covariance: $C = \\begin{pmatrix} s^2  -\\rho s^2 \\\\ -\\rho s^2  s^2 \\end{pmatrix}$, with $s0$ and $0\\rho1$.\n- Data model: $y = G(\\theta) + \\varepsilon$.\n- Noise model: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$, with $\\sigma_y^2  0$ known.\n- All quantities are dimensionless.\n- Task 1: Derive $\\operatorname{Var}(y)$ and analyze its behavior as $\\rho \\to 1$.\n- Task 2: Compute the Jacobian $J(\\theta^{\\star})$ of $G$, form the Gauss–Newton matrix $H = J(\\theta^{\\star})^{\\top}\\Sigma_y^{-1}J(\\theta^{\\star)}$ with $\\Sigma_y = \\sigma_y^2$, calculate its determinant $\\det(H)$, and use this to analyze the ill-posedness of the inverse problem.\n\n**Step 2: Validation Using Extracted Givens**\n- **Scientific Grounding:** The problem is scientifically grounded. It presents a canonical Bayesian inverse problem for a simple linear forward model. The model $G(\\theta) = \\frac{1}{2}(\\theta_1 + \\theta_2)$ is the rule of mixtures for the in-plane effective conductivity of a layered composite, a standard result in homogenization theory. The use of Gaussian priors and additive Gaussian noise is a common and mathematically sound framework in uncertainty quantification.\n- **Well-Posedness:** The problem statement is well-posed. It provides all necessary definitions and constraints to perform the requested derivations and calculations. The questions asked have unique, verifiable answers.\n- **Objectivity:** The problem is stated using precise, objective mathematical language, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\n- **Verdict:** The problem is scientifically sound, self-contained, and well-posed. It is a valid problem.\n- **Action:** Proceed with the solution.\n\n**Part 1: Forward Uncertainty Quantification**\n\nThe objective is to derive the variance of the observable output, $\\operatorname{Var}(y)$. The data model is given by $y = G(\\theta) + \\varepsilon$. The parameters $\\theta$ and the noise $\\varepsilon$ are assumed to be independent random variables. Therefore, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}(y) = \\operatorname{Var}(G(\\theta)) + \\operatorname{Var}(\\varepsilon)\n$$\nWe are given that $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$, which directly implies $\\operatorname{Var}(\\varepsilon) = \\sigma_y^2$.\n\nNext, we compute the variance of the forward model output, $\\operatorname{Var}(G(\\theta))$. The forward model is a linear function of the parameter vector $\\theta$:\n$$\nG(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2 = \\begin{pmatrix} \\tfrac{1}{2}  \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\end{pmatrix} = k^{\\top}\\theta\n$$\nwhere $k = (\\tfrac{1}{2}, \\tfrac{1}{2})^{\\top}$. The variance of a linear transformation of a random vector is given by the formula $\\operatorname{Var}(k^{\\top}\\theta) = k^{\\top} \\operatorname{Cov}(\\theta) k$. In this case, $\\operatorname{Cov}(\\theta) = C$.\nSubstituting the expressions for $k$ and $C$:\n$$\n\\operatorname{Var}(G(\\theta)) = \\begin{pmatrix} \\tfrac{1}{2}  \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} s^2  -\\rho s^2 \\\\ -\\rho s^2  s^2 \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix}\n$$\nFirst, we perform the left multiplication:\n$$\n\\begin{pmatrix} \\tfrac{1}{2}  \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} s^2  -\\rho s^2 \\\\ -\\rho s^2  s^2 \\end{pmatrix} = \\begin{pmatrix} \\tfrac{1}{2}s^2 - \\tfrac{1}{2}\\rho s^2  -\\tfrac{1}{2}\\rho s^2 + \\tfrac{1}{2}s^2 \\end{pmatrix} = \\begin{pmatrix} \\tfrac{s^2}{2}(1-\\rho)  \\tfrac{s^2}{2}(1-\\rho) \\end{pmatrix}\n$$\nNow, we complete the quadratic form:\n$$\n\\operatorname{Var}(G(\\theta)) = \\begin{pmatrix} \\tfrac{s^2}{2}(1-\\rho)  \\tfrac{s^2}{2}(1-\\rho) \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix} = \\tfrac{s^2}{2}(1-\\rho)(\\tfrac{1}{2}) + \\tfrac{s^2}{2}(1-\\rho)(\\tfrac{1}{2}) = \\tfrac{s^2}{4}(1-\\rho) + \\tfrac{s^2}{4}(1-\\rho) = \\tfrac{s^2}{2}(1-\\rho)\n$$\nCombining the results, the total output variance is:\n$$\n\\operatorname{Var}(y) = \\tfrac{s^2}{2}(1-\\rho) + \\sigma_y^2\n$$\nAs the negative correlation strength $\\rho$ approaches its upper limit of $1$, the term $(1-\\rho)$ approaches $0$. Consequently,\n$$\n\\lim_{\\rho \\to 1} \\operatorname{Var}(y) = \\lim_{\\rho \\to 1} \\left( \\tfrac{s^2}{2}(1-\\rho) + \\sigma_y^2 \\right) = 0 + \\sigma_y^2 = \\sigma_y^2\n$$\nThis demonstrates that as the prior correlation becomes perfectly negative ($\\rho \\to 1$), the uncertainty in the model output contributed by the parameters, $\\operatorname{Var}(G(\\theta))$, vanishes. The total output uncertainty becomes dominated solely by the measurement noise variance $\\sigma_y^2$. This occurs because if $\\theta_1$ and $\\theta_2$ are strongly negatively correlated, a deviation of $\\theta_1$ above its mean is accompanied by a deviation of $\\theta_2$ below its mean, such that their sum (and thus their average $G(\\theta)$) remains relatively constant. This leads to a very narrow distribution for the forward model output.\n\n**Part 2: Inverse Problem Analysis and Ill-Posedness**\n\nWe analyze the local identifiability of the inverse problem by examining the Gauss-Newton approximation to the Hessian of the negative log-likelihood. The matrix is given by $H = J(\\theta^{\\star})^{\\top}\\Sigma_y^{-1}J(\\theta^{\\star})$. The inverse of this matrix is related to the posterior covariance of the parameters, and its singularity indicates non-identifiability.\n\nFirst, we compute the Jacobian of the forward map $G(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2$. The Jacobian is the matrix of first-order partial derivatives, which in this case is a $1 \\times 2$ row vector:\n$$\nJ(\\theta) = \\begin{pmatrix} \\frac{\\partial G}{\\partial \\theta_1}  \\frac{\\partial G}{\\partial \\theta_2} \\end{pmatrix} = \\begin{pmatrix} \\tfrac{1}{2}  \\tfrac{1}{2} \\end{pmatrix}\n$$\nThe Jacobian is constant and does not depend on the point $\\theta^{\\star}$ at which it is evaluated. Thus, $J(\\theta^{\\star}) = \\begin{pmatrix} \\tfrac{1}{2}  \\tfrac{1}{2} \\end{pmatrix}$.\n\nThe output $y$ is a scalar, so its covariance matrix $\\Sigma_y$ is a $1 \\times 1$ matrix given by $\\Sigma_y = [\\sigma_y^2]$. Its inverse is $\\Sigma_y^{-1} = [\\tfrac{1}{\\sigma_y^2}]$.\n\nNow, we construct the Gauss-Newton matrix $H$. The transpose of the Jacobian is $J(\\theta^{\\star})^{\\top} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix}$.\n$$\nH = J(\\theta^{\\star})^{\\top} \\Sigma_y^{-1} J(\\theta^{\\star}) = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix} \\left( \\tfrac{1}{\\sigma_y^2} \\right) \\begin{pmatrix} \\tfrac{1}{2}  \\tfrac{1}{2} \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\nH = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2}  \\tfrac{1}{2} \\end{pmatrix} = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} (\\tfrac{1}{2})(\\tfrac{1}{2})  (\\tfrac{1}{2})(\\tfrac{1}{2}) \\\\ (\\tfrac{1}{2})(\\tfrac{1}{2})  (\\tfrac{1}{2})(\\tfrac{1}{2}) \\end{pmatrix} = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\tfrac{1}{4}  \\tfrac{1}{4} \\\\ \\tfrac{1}{4}  \\tfrac{1}{4} \\end{pmatrix}\n$$\nThe final step is to compute the determinant of this $2 \\times 2$ matrix $H$:\n$$\n\\det(H) = \\det\\left( \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\tfrac{1}{4}  \\tfrac{1}{4} \\\\ \\tfrac{1}{4}  \\tfrac{1}{4} \\end{pmatrix} \\right)\n$$\nUsing the determinant property $\\det(cA) = c^n\\det(A)$ for an $n \\times n$ matrix $A$, with $n=2$ and $c=1/\\sigma_y^2$:\n$$\n\\det(H) = \\left( \\frac{1}{\\sigma_y^2} \\right)^2 \\det\\begin{pmatrix} \\tfrac{1}{4}  \\tfrac{1}{4} \\\\ \\tfrac{1}{4}  \\tfrac{1}{4} \\end{pmatrix} = \\frac{1}{\\sigma_y^4} \\left( (\\tfrac{1}{4})(\\tfrac{1}{4}) - (\\tfrac{1}{4})(\\tfrac{1}{4}) \\right) = \\frac{1}{\\sigma_y^4} \\left( \\tfrac{1}{16} - \\tfrac{1}{16} \\right) = 0\n$$\nA determinant of zero implies that the matrix $H$ is singular. A singular Gauss-Newton Hessian indicates that the likelihood function is flat along at least one direction in the parameter space. This means that changes to the parameters along this direction have no effect on the model output $G(\\theta)$. Consequently, the data $y$ provides no information to constrain the parameters in that direction. This is a hallmark of an ill-posed inverse problem, as infinitely many parameter combinations yield the same model prediction and thus the same likelihood value. In this case, the null space of $H$ is spanned by the vector $(1, -1)^{\\top}$, meaning any parameters on the line $\\theta_1 + \\theta_2 = \\text{constant}$ are indistinguishable.\n\nThis analysis reveals a critical insight: even though the forward uncertainty $\\operatorname{Var}(y)$ becomes small as $\\rho \\to 1$, suggesting a well-determined output, the inverse problem is fundamentally ill-posed due to the structure of the forward model $G(\\theta)$. The measurement $y$ only provides information about the sum $\\theta_1 + \\theta_2$, not about the individual values of $\\theta_1$ and $\\theta_2$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The computational cost of high-fidelity forward models is a major bottleneck in inverse UQ, particularly in multiscale systems. This hands-on practice introduces a powerful strategy to mitigate this cost: the delayed-acceptance Metropolis-Hastings algorithm. In this exercise , you will implement this sampler, using an inexpensive coarse model to rapidly filter proposals before engaging a costly fine model. This provides practical experience in designing efficient sampling schemes for challenging inverse problems, a crucial skill in modern computational science.",
            "id": "3770685",
            "problem": "Consider a two-level delayed-acceptance Metropolis–Hastings sampler for inverse uncertainty quantification in a multiscale forward model. The unknown parameter is a vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$ with a Gaussian prior $p(\\boldsymbol{\\theta}) = \\mathcal{N}(\\mathbf{0}, \\sigma_{p}^{2} \\mathbf{I}_{2})$. Observations $\\mathbf{y} \\in \\mathbb{R}^{m}$ are generated from a fine-scale forward model with additive independent Gaussian noise of standard deviation $\\sigma$, i.e., $\\mathbf{y} = \\mathbf{f}(\\boldsymbol{\\theta}^{\\star}) + \\boldsymbol{\\eta}$ where $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{m})$. The fine-scale forward model $\\mathbf{f}(\\cdot)$ and a coarse-scale surrogate $\\mathbf{g}(\\cdot)$ are defined by linear and quadratic feature maps coupled with random design matrices as follows:\n- Let $m \\in \\mathbb{N}$ be the number of measurement components, and let $H_{1} \\in \\mathbb{R}^{m \\times 2}$ and $H_{2} \\in \\mathbb{R}^{m \\times 3}$ be random matrices with entries independently distributed as $\\mathcal{N}(0, 1/\\sqrt{m})$. Define the linear feature map $\\boldsymbol{\\phi}_{\\text{lin}}(\\boldsymbol{\\theta}) = [\\theta_{1}, \\theta_{2}]^{\\top}$ and the quadratic feature map $\\boldsymbol{\\phi}_{\\text{quad}}(\\boldsymbol{\\theta}) = [\\theta_{1}^{2}, \\theta_{2}^{2}, \\theta_{1}\\theta_{2}]^{\\top}$. For a fixed nonlinearity scale $c_{\\text{nl}} \\in \\mathbb{R}_{+}$ and a coarse nonlinearity retention factor $\\beta \\in [0,1]$, set\n$$\n\\mathbf{f}(\\boldsymbol{\\theta}) = H_{1}\\,\\boldsymbol{\\phi}_{\\text{lin}}(\\boldsymbol{\\theta}) \\;+\\; c_{\\text{nl}}\\, H_{2}\\,\\boldsymbol{\\phi}_{\\text{quad}}(\\boldsymbol{\\theta}), \\quad\n\\mathbf{g}(\\boldsymbol{\\theta}) = H_{1}\\,\\boldsymbol{\\phi}_{\\text{lin}}(\\boldsymbol{\\theta}) \\;+\\; \\beta\\, c_{\\text{nl}}\\, H_{2}\\,\\boldsymbol{\\phi}_{\\text{quad}}(\\boldsymbol{\\theta}).\n$$\n- The log-likelihoods at fine and coarse scales for a given $\\boldsymbol{\\theta}$ are, up to an additive constant independent of $\\boldsymbol{\\theta}$,\n$$\n\\log L_{\\text{fine}}(\\boldsymbol{\\theta}) = -\\frac{1}{2\\sigma^{2}} \\left\\| \\mathbf{y} - \\mathbf{f}(\\boldsymbol{\\theta}) \\right\\|_{2}^{2}, \\qquad\n\\log L_{\\text{coarse}}(\\boldsymbol{\\theta}) = -\\frac{1}{2\\sigma^{2}} \\left\\| \\mathbf{y} - \\mathbf{g}(\\boldsymbol{\\theta}) \\right\\|_{2}^{2}.\n$$\n- The log-prior is, up to an additive constant, $\\log p(\\boldsymbol{\\theta}) = -\\frac{1}{2\\sigma_{p}^{2}} \\|\\boldsymbol{\\theta}\\|_{2}^{2}$.\n\nThe goal is to implement a two-stage delayed-acceptance Random Walk Metropolis–Hastings sampler with a symmetric Gaussian proposal $q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta}, s^{2}\\mathbf{I}_{2})$, using the coarse model to filter proposals and the fine model to correct them. Each iteration proposes $\\boldsymbol{\\theta}'$ from $q(\\cdot \\mid \\boldsymbol{\\theta})$, performs a first-stage acceptance with probability\n$$\n\\alpha_{1}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\min\\left\\{ 1,\\; \\frac{p(\\boldsymbol{\\theta}') L_{\\text{coarse}}(\\boldsymbol{\\theta}')}{p(\\boldsymbol{\\theta}) L_{\\text{coarse}}(\\boldsymbol{\\theta})} \\right\\},\n$$\nand, if this stage is accepted, performs a second-stage correction with probability\n$$\n\\alpha_{2}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\min\\left\\{ 1,\\; \\frac{L_{\\text{fine}}(\\boldsymbol{\\theta}') L_{\\text{coarse}}(\\boldsymbol{\\theta})}{L_{\\text{fine}}(\\boldsymbol{\\theta}) L_{\\text{coarse}}(\\boldsymbol{\\theta}')} \\right\\}.\n$$\nIf both stages accept, set $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}'$; otherwise, retain the current state.\n\nYour program must:\n- Implement the above sampler and empirically estimate the following acceptance rates over a fixed number of iterations $N$: \n    - The first-stage acceptance rate $r_{1}$ defined as the fraction of proposals passing the first stage.\n    - The conditional second-stage acceptance rate $r_{2}$ defined as the fraction of proposals passing the second stage among those that passed the first stage. If no proposal passes the first stage, define $r_{2} = 0$.\n    - The overall acceptance rate $r_{\\text{overall}}$ defined as the fraction of proposals accepted by both stages.\n- Use a fixed parameter dimension $d = 2$, number of measurements $m = 40$, nonlinearity scale $c_{\\text{nl}} = 0.3$, prior standard deviation $\\sigma_{p} = 5.0$, true parameter $\\boldsymbol{\\theta}^{\\star} = [0.6, -0.8]^{\\top}$, and number of iterations $N = 10000$.\n- Use independent random seeds for reproducibility as follows:\n    - For generating $H_{1}$ and $H_{2}$, set the seed to $2025$ and draw entries independently from $\\mathcal{N}(0, 1/\\sqrt{m})$.\n    - For each test case index $i \\in \\{0,1,2,3\\}$ with observation noise standard deviation $\\sigma^{(i)}$, generate observations as $\\mathbf{y}^{(i)} = \\mathbf{f}(\\boldsymbol{\\theta}^{\\star}) + \\boldsymbol{\\eta}^{(i)}$ with $\\boldsymbol{\\eta}^{(i)} \\sim \\mathcal{N}(\\mathbf{0}, (\\sigma^{(i)})^{2}\\mathbf{I}_{m})$, using a seed set to $2024 + i$.\n    - For the proposal draws within the sampler of test case $i$, set the seed to $3000 + i$.\n- Initialize the Markov chain at $\\boldsymbol{\\theta}_{0} = \\mathbf{0}$ for every test case.\n- Use a symmetric Gaussian proposal with step size $s$ specified per test case.\n\nTest suite. Run the sampler independently for the following four test cases, each specified by a triple $(\\sigma, \\beta, s)$:\n- Case $0$: $(\\sigma, \\beta, s) = (0.05, 0.25, 0.20)$.\n- Case $1$: $(\\sigma, \\beta, s) = (0.20, 0.25, 0.50)$.\n- Case $2$: $(\\sigma, \\beta, s) = (0.05, 0.00, 0.20)$.\n- Case $3$: $(\\sigma, \\beta, s) = (0.10, 0.75, 1.00)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of four sublists, one per test case in order, where each sublist is of the form $[r_{1}, r_{2}, r_{\\text{overall}}]$. Each rate must be expressed as a decimal rounded to four digits after the decimal point. The final output format must be exactly:\n- A single line with no spaces, of the form\n$[[r_{1}^{(0)},r_{2}^{(0)},r_{\\text{overall}}^{(0)}],[r_{1}^{(1)},r_{2}^{(1)},r_{\\text{overall}}^{(1)}],[r_{1}^{(2)},r_{2}^{(2)},r_{\\text{overall}}^{(2)}],[r_{1}^{(3)},r_{2}^{(3)},r_{\\text{overall}}^{(3)}]]$.\n\nNo physical units are involved in this problem. All reported quantities are dimensionless real numbers. Angles are not used. Percentages must be expressed as decimals, not with the percentage sign.",
            "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, self-contained, and scientifically grounded problem in computational statistics, specifically concerning inverse uncertainty quantification using a multiscale Markov Chain Monte Carlo (MCMC) method. All required data, models, and parameters are fully specified.\n\nThe problem asks for the implementation and analysis of a two-level delayed-acceptance Metropolis-Hastings (DA-MH) sampler to perform Bayesian inference on a parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$. The solution involves defining the statistical model, detailing the DA-MH algorithm, and implementing it to compute specified performance metrics.\n\n**1. Bayesian Framework**\n\nThe core task is to characterize the posterior probability distribution of the parameter $\\boldsymbol{\\theta}$ given observed data $\\mathbf{y}$. According to Bayes' theorem, the posterior $p(\\boldsymbol{\\theta} \\mid \\mathbf{y})$ is proportional to the product of the likelihood $L(\\boldsymbol{\\theta} \\mid \\mathbf{y})$ and the prior $p(\\boldsymbol{\\theta})$:\n$$\np(\\boldsymbol{\\theta} \\mid \\mathbf{y}) \\propto L(\\boldsymbol{\\theta} \\mid \\mathbf{y}) \\, p(\\boldsymbol{\\theta})\n$$\nIn this problem, the likelihood is based on the fine-scale model, so we denote it as $L_{\\text{fine}}(\\boldsymbol{\\theta})$. It is computationally more stable and convenient to work with the logarithm of the posterior (log-posterior):\n$$\n\\log p(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\log L_{\\text{fine}}(\\boldsymbol{\\theta}) + \\log p(\\boldsymbol{\\theta}) + \\text{constant}\n$$\n\n**2. Model Components**\n\nThe problem defines all components of the statistical model.\n\n**Prior Distribution:** The parameter $\\boldsymbol{\\theta}$ is endowed with a zero-mean isotropic Gaussian prior with variance $\\sigma_{p}^{2}$.\n$$\np(\\boldsymbol{\\theta}) = \\mathcal{N}(\\mathbf{0}, \\sigma_{p}^{2} \\mathbf{I}_{2})\n$$\nThe corresponding log-prior, up to an additive constant, is:\n$$\n\\log p(\\boldsymbol{\\theta}) = -\\frac{1}{2\\sigma_{p}^{2}} \\|\\boldsymbol{\\theta}\\|_{2}^{2}\n$$\nwhere $\\|\\cdot\\|_{2}$ is the Euclidean norm and $\\sigma_p = 5.0$.\n\n**Observation Model and Likelihood:** The data $\\mathbf{y} \\in \\mathbb{R}^{m}$ are generated by the fine-scale forward model $\\mathbf{f}(\\boldsymbol{\\theta}^{\\star})$ with additive i.i.d. Gaussian noise $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I}_{m})$.\n$$\n\\mathbf{y} = \\mathbf{f}(\\boldsymbol{\\theta}^{\\star}) + \\boldsymbol{\\eta}\n$$\nThis leads to a Gaussian likelihood function. The log-likelihood for a given $\\boldsymbol{\\theta}$ is:\n$$\n\\log L_{\\text{fine}}(\\boldsymbol{\\theta}) = -\\frac{1}{2\\sigma^{2}} \\left\\| \\mathbf{y} - \\mathbf{f}(\\boldsymbol{\\theta}) \\right\\|_{2}^{2}\n$$\n\n**Forward Models (Fine and Coarse):** Both models map the parameter $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2]^{\\top}$ to the observation space $\\mathbb{R}^{m}$. They are constructed from linear and quadratic features of $\\boldsymbol{\\theta}$.\n- Linear feature map: $\\boldsymbol{\\phi}_{\\text{lin}}(\\boldsymbol{\\theta}) = [\\theta_{1}, \\theta_{2}]^{\\top}$\n- Quadratic feature map: $\\boldsymbol{\\phi}_{\\text{quad}}(\\boldsymbol{\\theta}) = [\\theta_{1}^{2}, \\theta_{2}^{2}, \\theta_{1}\\theta_{2}]^{\\top}$\n\nUsing random design matrices $H_{1} \\in \\mathbb{R}^{m \\times 2}$ and $H_{2} \\in \\mathbb{R}^{m \\times 3}$, a nonlinearity scale $c_{\\text{nl}} = 0.3$, and a coarse-model retention factor $\\beta \\in [0,1]$, the models are:\n- Fine-scale model: $\\mathbf{f}(\\boldsymbol{\\theta}) = H_{1}\\,\\boldsymbol{\\phi}_{\\text{lin}}(\\boldsymbol{\\theta}) + c_{\\text{nl}}\\, H_{2}\\,\\boldsymbol{\\phi}_{\\text{quad}}(\\boldsymbol{\\theta})$\n- Coarse-scale model: $\\mathbf{g}(\\boldsymbol{\\theta}) = H_{1}\\,\\boldsymbol{\\phi}_{\\text{lin}}(\\boldsymbol{\\theta}) + \\beta\\, c_{\\text{nl}}\\, H_{2}\\,\\boldsymbol{\\phi}_{\\text{quad}}(\\boldsymbol{\\theta})$\n\nThe coarse model $\\mathbf{g}(\\boldsymbol{\\theta})$ is a cheaper-to-evaluate approximation of $\\mathbf{f}(\\boldsymbol{\\theta})$, where the parameter $\\beta$ controls the fidelity of the approximation. The corresponding coarse log-likelihood is:\n$$\n\\log L_{\\text{coarse}}(\\boldsymbol{\\theta}) = -\\frac{1}{2\\sigma^{2}} \\left\\| \\mathbf{y} - \\mathbf{g}(\\boldsymbol{\\theta}) \\right\\|_{2}^{2}\n$$\n\n**3. Delayed-Acceptance Metropolis-Hastings (DA-MH) Sampler**\n\nThe DA-MH algorithm uses the coarse model as a fast filter to reject unpromising proposals before engaging the expensive fine model. Each iteration consists of two stages.\n\nLet $\\boldsymbol{\\theta}_k$ be the current state of the Markov chain.\n**Proposal:** A new state $\\boldsymbol{\\theta}'$ is proposed from a symmetric Gaussian proposal distribution $q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}_k) = \\mathcal{N}(\\boldsymbol{\\theta}_k, s^{2}\\mathbf{I}_{2})$.\n\n**Stage 1: Coarse-Model Screening:** The proposal is first tested using an approximate posterior defined by the coarse likelihood, $p(\\boldsymbol{\\theta})L_{\\text{coarse}}(\\boldsymbol{\\theta})$. The acceptance probability is:\n$$\n\\alpha_{1}(\\boldsymbol{\\theta}_k, \\boldsymbol{\\theta}') = \\min\\left\\{ 1,\\; \\frac{p(\\boldsymbol{\\theta}') L_{\\text{coarse}}(\\boldsymbol{\\theta}')}{p(\\boldsymbol{\\theta}_k) L_{\\text{coarse}}(\\boldsymbol{\\theta}_k)} \\right\\}\n$$\nA random number $u_1 \\sim U(0,1)$ is drawn. If $u_1  \\alpha_1$, the proposal passes to stage 2. Otherwise, it is rejected, and $\\boldsymbol{\\theta}_{k+1} = \\boldsymbol{\\theta}_k$.\n\n**Stage 2: Fine-Model Correction:** If the proposal is accepted in stage 1, a correction is applied to account for the discrepancy between the coarse and fine models. The second-stage acceptance probability is:\n$$\n\\alpha_{2}(\\boldsymbol{\\theta}_k, \\boldsymbol{\\theta}') = \\min\\left\\{ 1,\\; \\frac{L_{\\text{fine}}(\\boldsymbol{\\theta}') L_{\\text{coarse}}(\\boldsymbol{\\theta}_k)}{L_{\\text{fine}}(\\boldsymbol{\\theta}_k) L_{\\text{coarse}}(\\boldsymbol{\\theta}')} \\right\\}\n$$\nA random number $u_2 \\sim U(0,1)$ is drawn. If $u_2  \\alpha_2$, the proposal is fully accepted, and $\\boldsymbol{\\theta}_{k+1} = \\boldsymbol{\\theta}'$. Otherwise, it is rejected, and $\\boldsymbol{\\theta}_{k+1} = \\boldsymbol{\\theta}_k$.\n\nThe overall acceptance probability of a proposal is $\\alpha(\\boldsymbol{\\theta}_k, \\boldsymbol{\\theta}') = \\alpha_1(\\boldsymbol{\\theta}_k, \\boldsymbol{\\theta}') \\alpha_2(\\boldsymbol{\\theta}_k, \\boldsymbol{\\theta}')$. The product of the ratios within the $\\min$ functions simplifies to the target Metropolis-Hastings ratio for the fine model, ensuring the sampler correctly targets $p(\\boldsymbol{\\theta} \\mid \\mathbf{y})$:\n$$\n\\frac{p(\\boldsymbol{\\theta}') L_{\\text{coarse}}(\\boldsymbol{\\theta}')}{p(\\boldsymbol{\\theta}_k) L_{\\text{coarse}}(\\boldsymbol{\\theta}_k)} \\times \\frac{L_{\\text{fine}}(\\boldsymbol{\\theta}') L_{\\text{coarse}}(\\boldsymbol{\\theta}_k)}{L_{\\text{fine}}(\\boldsymbol{\\theta}_k) L_{\\text{coarse}}(\\boldsymbol{\\theta}')} = \\frac{p(\\boldsymbol{\\theta}') L_{\\text{fine}}(\\boldsymbol{\\theta}')}{p(\\boldsymbol{\\theta}_k) L_{\\text{fine}}(\\boldsymbol{\\theta}_k)}\n$$\n\n**4. Implementation and Calculation of Acceptance Rates**\n\nThe algorithm is implemented for $N=10000$ iterations starting from $\\boldsymbol{\\theta}_0 = \\mathbf{0}$. The following acceptance rates are calculated:\n- **First-stage acceptance rate ($r_1$)**: The fraction of proposals that pass stage 1. Let $N_1$ be the count of first-stage acceptances. Then $r_1 = N_1 / N$.\n- **Conditional second-stage acceptance rate ($r_2$)**: The fraction of proposals that pass stage 2, given they passed stage 1. Let $N_{\\text{overall}}$ be the count of proposals that pass both stages. Then $r_2 = N_{\\text{overall}} / N_1$ (if $N_1  0$, else $r_2=0$).\n- **Overall acceptance rate ($r_{\\text{overall}}$)**: The fraction of proposals that pass both stages. $r_{\\text{overall}} = N_{\\text{overall}} / N$.\n\nLogarithms of probabilities are used for numerical stability. The acceptance conditions become $\\log(u_1)  \\log(\\alpha_1)$ and $\\log(u_2)  \\log(\\alpha_2)$, where $\\log(\\alpha) = \\min(0, \\log(\\text{ratio}))$. The implementation respects the specified random seeds for reproducibility.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the two-level delayed-acceptance Metropolis-Hastings sampler\n    for the specified inverse problem and computes the acceptance rates.\n    \"\"\"\n    # --- Problem Parameters ---\n    d = 2\n    m = 40\n    c_nl = 0.3\n    sigma_p = 5.0\n    theta_star = np.array([0.6, -0.8])\n    N = 10000\n    theta_0 = np.zeros(d)\n\n    # --- Test Cases ---\n    test_cases = [\n        # (sigma, beta, s)\n        (0.05, 0.25, 0.20),  # Case 0\n        (0.20, 0.25, 0.50),  # Case 1\n        (0.05, 0.00, 0.20),  # Case 2\n        (0.10, 0.75, 1.00),  # Case 3\n    ]\n\n    # --- Generate common random matrices H1 and H2 ---\n    rng_design = np.random.default_rng(2025)\n    H1 = rng_design.normal(loc=0.0, scale=1/np.sqrt(m), size=(m, d))\n    H2 = rng_design.normal(loc=0.0, scale=1/np.sqrt(m), size=(m, 3))\n\n    # --- Helper Functions ---\n    def phi_lin(theta):\n        return theta\n\n    def phi_quad(theta):\n        return np.array([theta[0]**2, theta[1]**2, theta[0]*theta[1]])\n\n    def f_model(theta, H1, H2, c_nl):\n        return H1 @ phi_lin(theta) + c_nl * (H2 @ phi_quad(theta))\n\n    def g_model(theta, H1, H2, c_nl, beta):\n        return H1 @ phi_lin(theta) + beta * c_nl * (H2 @ phi_quad(theta))\n\n    def log_prior(theta, sigma_p):\n        return -0.5 * np.sum(theta**2) / sigma_p**2\n\n    def log_likelihood(y, y_model, sigma):\n        return -0.5 * np.sum((y - y_model)**2) / sigma**2\n\n    # --- Main Loop over Test Cases ---\n    all_results = []\n    \n    # Pre-calculate f(theta_star) as it's common\n    f_theta_star = f_model(theta_star, H1, H2, c_nl)\n\n    for i, (sigma, beta, s) in enumerate(test_cases):\n        # --- Generate observation vector y for this case ---\n        rng_obs = np.random.default_rng(2024 + i)\n        eta = rng_obs.normal(loc=0.0, scale=sigma, size=m)\n        y = f_theta_star + eta\n\n        # --- Initialize Sampler for this case ---\n        rng_sampler = np.random.default_rng(3000 + i)\n        theta_current = np.copy(theta_0)\n        \n        stage1_accepts = 0\n        overall_accepts = 0\n\n        # Initial values for log posterior components used in the sampler\n        log_p_current = log_prior(theta_current, sigma_p)\n        g_current_val = g_model(theta_current, H1, H2, c_nl, beta)\n        log_L_coarse_current = log_likelihood(y, g_current_val, sigma)\n\n        for _ in range(N):\n            # --- 1. Propose ---\n            proposal_noise = rng_sampler.normal(loc=0.0, scale=s, size=d)\n            theta_prime = theta_current + proposal_noise\n\n            # --- 2. Stage 1: Coarse Model Filter ---\n            log_p_prime = log_prior(theta_prime, sigma_p)\n            g_prime_val = g_model(theta_prime, H1, H2, c_nl, beta)\n            log_L_coarse_prime = log_likelihood(y, g_prime_val, sigma)\n\n            log_alpha1_ratio = (log_p_prime + log_L_coarse_prime) - (log_p_current + log_L_coarse_current)\n            \n            if np.log(rng_sampler.random())  min(0.0, log_alpha1_ratio):\n                # Stage 1 accepted\n                stage1_accepts += 1\n                \n                # --- 3. Stage 2: Fine Model Correction ---\n                f_current_val = f_model(theta_current, H1, H2, c_nl)\n                log_L_fine_current = log_likelihood(y, f_current_val, sigma)\n                \n                f_prime_val = f_model(theta_prime, H1, H2, c_nl)\n                log_L_fine_prime = log_likelihood(y, f_prime_val, sigma)\n\n                log_alpha2_ratio = (log_L_fine_prime - log_L_fine_current) - (log_L_coarse_prime - log_L_coarse_current)\n\n                if np.log(rng_sampler.random())  min(0.0, log_alpha2_ratio):\n                    # Stage 2 accepted\n                    overall_accepts += 1\n                    \n                    # Update chain state\n                    theta_current = theta_prime\n                    log_p_current = log_p_prime\n                    log_L_coarse_current = log_L_coarse_prime\n            \n        # --- 4. Calculate Acceptance Rates ---\n        r1 = stage1_accepts / N\n        r2 = overall_accepts / stage1_accepts if stage1_accepts  0 else 0.0\n        r_overall = overall_accepts / N\n\n        case_results = [round(r1, 4), round(r2, 4), round(r_overall, 4)]\n        all_results.append(case_results)\n\n    # --- Format and Print Final Output ---\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}