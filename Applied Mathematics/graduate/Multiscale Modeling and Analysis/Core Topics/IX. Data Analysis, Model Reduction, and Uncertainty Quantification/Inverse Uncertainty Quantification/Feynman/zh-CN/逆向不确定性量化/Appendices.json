{
    "hands_on_practices": [
        {
            "introduction": "共轭先验是贝叶斯分析中的一个强大工具，它允许我们以闭合形式解析地推导出后验分布。本练习  将通过一个经典的正态-逆伽马（Normal-Inverse-Gamma）模型，让你亲手实践贝叶斯更新的完整推导过程，从而为你后续更复杂的逆不确定性量化问题打下坚实的解析基础。",
            "id": "3770713",
            "problem": "考虑一个双层多尺度模型，其中，细观尺度模拟产生摘要信息，这些信息被聚合成 $n$ 个粗观尺度观测值 $y_1, y_2, \\dots, y_n$。这些观测值用于对未知的粗粒化偏差参数 $\\theta$ 和粗观尺度噪声方差 $\\sigma^2$ 进行逆不确定性量化。假设条件数据模型 $y_i \\mid \\theta, \\sigma^2$ 是独立同分布的高斯分布，即 $y_i \\mid \\theta, \\sigma^2 \\sim \\mathcal{N}(\\theta, \\sigma^2)$，并采用分层正态-逆伽马先验，定义为 $\\theta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 / \\kappa_0)$ 和 $\\sigma^2 \\sim \\operatorname{InvGamma}(\\alpha_0, \\beta_0)$，其中 $\\operatorname{InvGamma}$ 表示形状参数为 $\\alpha_0$、尺度参数为 $\\beta_0$ 的逆伽马分布。这种正态-逆伽马 (NIG) 先验通过允许粗观尺度上的方差为粗粒化偏差的分布提供信息，从而将不同尺度耦合起来。\n\n从 Bayes 定理以及高斯和逆伽马密度的定义出发，推导全条件分布 $p(\\theta \\mid \\sigma^2, y_1, \\dots, y_n)$ 和 $p(\\sigma^2 \\mid \\theta, y_1, \\dots, y_n)$。然后，通过将联合后验 $p(\\theta, \\sigma^2 \\mid y_1, \\dots, y_n)$ 表示为正态-逆伽马族的形式来证明其共轭性，并以闭合形式确定更新后的超参数 $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$，将其表示为 $(m_0, \\kappa_0, \\alpha_0, \\beta_0)$ 和数据的充分统计量 $(n, \\bar{y}, \\sum_{i=1}^{n} (y_i - \\bar{y})^2)$ 的函数，其中 $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$。\n\n您的最终答案必须是更新后的超参数向量 $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$，使用 $\\operatorname{pmatrix}$ 环境写成一个单行矩阵。最终答案中不要提供中间步骤。不需要进行数值四舍五入，也不涉及任何单位。所有数学符号都必须用 LaTeX 书写。",
            "solution": "该问题是有效的，因为它是贝叶斯统计中一个适定的、有科学依据的、客观的问题。我们被要求推导在给定一组观测值 $y_1, \\dots, y_n$ 的情况下，参数 $\\theta$ 和 $\\sigma^2$ 的全条件后验分布和联合后验分布。该模型假设一个高斯似然和一个共轭的正态-逆伽马 (NIG) 先验。\n\n模型规定如下：\n似然：$y_i \\mid \\theta, \\sigma^2 \\sim \\mathcal{N}(\\theta, \\sigma^2)$，对于 $i=1, \\dots, n$，假设独立同分布。\n先验：一个由 $p(\\theta, \\sigma^2) = p(\\theta \\mid \\sigma^2)p(\\sigma^2)$ 给出的分层先验，其中：\n$\\theta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 / \\kappa_0)$\n$\\sigma^2 \\sim \\operatorname{InvGamma}(\\alpha_0, \\beta_0)$\n\n概率密度函数 (PDF) 如下：\n-   $p(y_i \\mid \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta)^2}{2\\sigma^2}\\right)$\n-   $p(\\theta \\mid \\sigma^2) = \\sqrt{\\frac{\\kappa_0}{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\kappa_0(\\theta - m_0)^2}{2\\sigma^2}\\right)$\n-   $p(\\sigma^2) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} (\\sigma^2)^{-(\\alpha_0+1)} \\exp\\left(-\\frac{\\beta_0}{\\sigma^2}\\right)$\n\n根据 Bayes 定理，联合后验分布正比于似然和先验的乘积：\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\theta, \\sigma^2) p(\\theta, \\sigma^2)$\n其中 $\\mathbf{y} = (y_1, \\dots, y_n)$。\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto \\left(\\prod_{i=1}^{n} p(y_i \\mid \\theta, \\sigma^2)\\right) p(\\theta \\mid \\sigma^2) p(\\sigma^2)$\n\n代入 PDF 并省略不依赖于 $\\theta$ 或 $\\sigma^2$ 的常数项：\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto \\left[ (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - \\theta)^2\\right) \\right] \\times \\left[ (\\sigma^2)^{-1/2} \\exp\\left(-\\frac{\\kappa_0(\\theta - m_0)^2}{2\\sigma^2}\\right) \\right] \\times \\left[ (\\sigma^2)^{-(\\alpha_0+1)} \\exp\\left(-\\frac{\\beta_0}{\\sigma^2}\\right) \\right]$\n\n合并各项：\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + 1/2 + 1)} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ \\sum_{i=1}^{n}(y_i - \\theta)^2 + \\kappa_0(\\theta - m_0)^2 + 2\\beta_0 \\right] \\right\\}$\n\n首先，我们推导全条件分布 $p(\\theta \\mid \\sigma^2, \\mathbf{y})$。我们将 $\\sigma^2$ 和 $\\mathbf{y}$ 视为给定，并将后验视为 $\\theta$ 的函数：\n$p(\\theta \\mid \\sigma^2, \\mathbf{y}) \\propto \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ \\sum_{i=1}^{n}(y_i - \\theta)^2 + \\kappa_0(\\theta - m_0)^2 \\right] \\right\\}$\n指数中的项是关于 $\\theta$ 的二次式。我们展开并按 $\\theta$ 合并各项：\n$\\sum_{i=1}^{n}(y_i^2 - 2y_i\\theta + \\theta^2) + \\kappa_0(\\theta^2 - 2m_0\\theta + m_0^2) = (n+\\kappa_0)\\theta^2 - 2(n\\bar{y} + \\kappa_0m_0)\\theta + (\\sum y_i^2 + \\kappa_0m_0^2)$\n其中 $\\bar{y} = \\frac{1}{n}\\sum y_i$。该表达式形如 $A\\theta^2 - 2B\\theta + C$。进行配方：\n$A\\theta^2 - 2B\\theta + C = A(\\theta - B/A)^2 + C - B^2/A$\n这里，$A = n+\\kappa_0$ 且 $B = n\\bar{y}+\\kappa_0m_0$。所以，$\\theta$ 的核为：\n$p(\\theta \\mid \\sigma^2, \\mathbf{y}) \\propto \\exp\\left\\{ -\\frac{n+\\kappa_0}{2\\sigma^2} \\left( \\theta - \\frac{n\\bar{y}+\\kappa_0m_0}{n+\\kappa_0} \\right)^2 \\right\\}$\n这是均值为 $\\frac{n\\bar{y}+\\kappa_0m_0}{n+\\kappa_0}$、方差为 $\\frac{\\sigma^2}{n+\\kappa_0}$ 的正态分布的核。\n所以，$\\theta$ 的全条件分布为：\n$\\theta \\mid \\sigma^2, \\mathbf{y} \\sim \\mathcal{N}\\left(\\frac{\\kappa_0m_0 + n\\bar{y}}{\\kappa_0+n}, \\frac{\\sigma^2}{\\kappa_0+n}\\right)$\n\n接下来，我们推导全条件分布 $p(\\sigma^2 \\mid \\theta, \\mathbf{y})$。我们将 $\\theta$ 和 $\\mathbf{y}$ 视为给定，并将后验视为 $\\sigma^2$ 的函数：\n$p(\\sigma^2 \\mid \\theta, \\mathbf{y}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + 1/2 + 1)} \\exp\\left\\{ -\\frac{1}{\\sigma^2} \\left[ \\beta_0 + \\frac{1}{2}\\sum_{i=1}^{n}(y_i - \\theta)^2 + \\frac{\\kappa_0}{2}(\\theta - m_0)^2 \\right] \\right\\}$\n这是一个逆伽马分布的核，$p(x) \\propto x^{-(\\alpha+1)}\\exp(-\\beta/x)$。\n通过比较，形状参数为 $\\alpha_0 + n/2 + 1/2$，尺度参数为 $\\beta_0 + \\frac{1}{2}\\sum(y_i - \\theta)^2 + \\frac{\\kappa_0}{2}(\\theta - m_0)^2$。\n所以，$\\sigma^2$ 的全条件分布为：\n$\\sigma^2 \\mid \\theta, \\mathbf{y} \\sim \\operatorname{InvGamma}\\left(\\alpha_0 + \\frac{n+1}{2}, \\beta_0 + \\frac{1}{2}\\left[\\sum_{i=1}^{n}(y_i - \\theta)^2 + \\kappa_0(\\theta - m_0)^2\\right]\\right)$\n\n最后，为了证明共轭性并找到联合后验 $p(\\theta, \\sigma^2 \\mid \\mathbf{y})$ 的更新后超参数，我们重新整理完整的后验表达式。我们将关于 $\\theta$ 的配方形式代回指数中。令 $S = \\sum_{i=1}^{n}(y_i - \\bar{y})^2$。一个有用的恒等式是 $\\sum(y_i-\\theta)^2 = S + n(\\bar{y}-\\theta)^2$。\n涉及 $\\theta$ 的指数项为：\n$n(\\bar{y}-\\theta)^2 + \\kappa_0(m_0-\\theta)^2 = (n+\\kappa_0)\\theta^2 - 2(n\\bar{y}+\\kappa_0m_0)\\theta + n\\bar{y}^2+\\kappa_0m_0^2$.\n配方后，这等于：\n$(\\kappa_0+n)\\left(\\theta - \\frac{\\kappa_0m_0+n\\bar{y}}{\\kappa_0+n}\\right)^2 + \\frac{n\\kappa_0}{n+\\kappa_0}(\\bar{y}-m_0)^2$\n将此代回完整的后验指数表达式（方括号内的项）：\n$\\left[ S + (\\kappa_0+n)\\left(\\theta - \\frac{\\kappa_0m_0+n\\bar{y}}{\\kappa_0+n}\\right)^2 + \\frac{n\\kappa_0}{n+\\kappa_0}(\\bar{y}-m_0)^2 + 2\\beta_0 \\right]$\n联合后验变为：\n$p(\\theta, \\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + 1/2 + 1)} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ (\\kappa_0+n)\\left(\\theta - \\frac{\\kappa_0m_0+n\\bar{y}}{\\kappa_0+n}\\right)^2 + 2\\beta_0 + S + \\frac{n\\kappa_0}{\\kappa_0+n}(\\bar{y}-m_0)^2 \\right] \\right\\}$\n该表达式具有正态-逆伽马分布的形式，$p(\\theta, \\sigma^2) \\propto (\\sigma^2)^{-(\\alpha_n+1/2+1)} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ \\kappa_n(\\theta-m_n)^2 + 2\\beta_n \\right] \\right\\}$。\n\n通过将推导出的后验与一般的 NIG 形式进行比较，我们确定更新后的超参数 $(m_n, \\kappa_n, \\alpha_n, \\beta_n)$：\n1.  $\\kappa_n$：$\\theta$ 的平方项的系数（除以 $2\\sigma^2$）是 $\\kappa_n/(2\\sigma^2)$。所以，\n    $\\kappa_n = \\kappa_0 + n$\n2.  $m_n$：正态分量的均值是：\n    $m_n = \\frac{\\kappa_0m_0 + n\\bar{y}}{\\kappa_0 + n}$\n3.  $\\alpha_n$：$\\sigma^2$ 的幂是 $-(\\alpha_n+1/2+1)$。所以，$\\alpha_n+1/2+1 = \\alpha_0+n/2+1/2+1$。这得出：\n    $\\alpha_n = \\alpha_0 + \\frac{n}{2}$\n4.  $\\beta_n$：指数中剩余的项是 $2\\beta_n$。所以，$2\\beta_n = 2\\beta_0 + S + \\frac{n\\kappa_0}{\\kappa_0+n}(\\bar{y}-m_0)^2$。这得出：\n    $\\beta_n = \\beta_0 + \\frac{1}{2}S + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2 = \\beta_0 + \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2$\n\n因此，后验分布 $p(\\theta, \\sigma^2 \\mid \\mathbf{y})$ 是 $NIG(m_n, \\kappa_n, \\alpha_n, \\beta_n)$，其超参数如上所推导。\n\n更新后的超参数为：\n$m_n = \\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0 + n}$\n$\\kappa_n = \\kappa_0 + n$\n$\\alpha_n = \\alpha_0 + \\frac{n}{2}$\n$\\beta_n = \\beta_0 + \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\kappa_0 m_0 + n\\bar{y}}{\\kappa_0 + n} & \\kappa_0 + n & \\alpha_0 + \\frac{n}{2} & \\beta_0 + \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(m_0 - \\bar{y})^2 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "一个看似精确的正向模型预测（即输出不确定性很小）是否意味着其参数可以被精确地反推出来？这个思想实验  揭示了答案是否定的，并阐明了逆问题中一个核心的挑战：不适定性。通过分析一个简单的物理模型，你将发现参数之间的相关性如何导致模型预测看似稳定，但反向推断参数却变得不可能，这对于理解参数的可识别性至关重要。",
            "id": "3770709",
            "problem": "考虑一个由两个面积分数相等的平行层组成的双尺度导电复合材料。在微观尺度上，各层的电导率由参数向量 $\\theta = (\\theta_1,\\theta_2)^{\\top}$ 表示，而我们感兴趣的宏观有效性质是均匀化后的面内有效电导率，其被建模为前向映射 $G:\\mathbb{R}^2 \\to \\mathbb{R}$，具体如下：\n$$\nG(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2.\n$$\n假设 $\\theta$ 的概率先验形式为 $\\theta \\sim \\mathcal{N}(m, C)$，其中 $m \\in \\mathbb{R}^2$，协方差矩阵为\n$$\nC = \\begin{pmatrix}\ns^2 & -\\rho s^2 \\\\\n-\\rho s^2 & s^2\n\\end{pmatrix},\n$$\n其中 $s>0$ 且 $0 < \\rho < 1$ 表示强负相关。数据模型为加性高斯噪声模型 $y = G(\\theta) + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_y^2)$ 且噪声方差 $\\sigma_y^2>0$ 已知。将所有量视为无量纲量。\n\n在这个最简化的多尺度情景中，您将研究正向不确定性量化（forward Uncertainty Quantification, UQ）与逆向不确定性量化（inverse Uncertainty Quantification, IUQ）之间的相互作用。从第一性原理出发：\n- 在给定的先验和噪声模型下，推导前向输出方差 $\\operatorname{Var}(y)$，并解释为什么当 $\\rho \\to 1$ 时该方差会变小。\n- 通过检验负对数似然Hessian矩阵的高斯-牛顿近似，分析逆问题的局部可辨识性。具体而言，计算 $G$ 在任意点 $\\theta^{\\star} \\in \\mathbb{R}^2$ 的雅可比矩阵 $J(\\theta^{\\star})$，构建高斯-牛顿矩阵 $H = J(\\theta^{\\star})^{\\top}\\Sigma_y^{-1}J(\\theta^{\\star})$（其中 $\\Sigma_y = \\sigma_y^2$），并确定其行列式。利用该结果来证明，尽管前向输出不确定性很小，该逆问题仍然是不适定的（ill-posed）。\n\n请给出 $\\det(H)$ 的精确值作为最终答案，无需四舍五入。请说明您使用的任何附加假设，并确保每个数学符号都使用 $...$ 以 LaTeX 格式书写。",
            "solution": "首先根据指定标准对问题进行验证。\n\n**步骤1：提取已知条件**\n- 前向映射：$G(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2$ 对于 $\\theta = (\\theta_1, \\theta_2)^{\\top}$。\n- 先验分布：$\\theta \\sim \\mathcal{N}(m, C)$，其中 $m \\in \\mathbb{R}^2$。\n- 先验协方差：$C = \\begin{pmatrix} s^2 & -\\rho s^2 \\\\ -\\rho s^2 & s^2 \\end{pmatrix}$，其中 $s>0$ 且 $0 < \\rho < 1$。\n- 数据模型：$y = G(\\theta) + \\varepsilon$。\n- 噪声模型：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$，其中 $\\sigma_y^2 > 0$ 已知。\n- 所有量均为无量纲量。\n- 任务1：推导 $\\operatorname{Var}(y)$ 并分析其在 $\\rho \\to 1$ 时的行为。\n- 任务2：计算 $G$ 的雅可比矩阵 $J(\\theta^{\\star})$，构建高斯-牛顿矩阵 $H = J(\\theta^{\\star})^{\\top}\\Sigma_y^{-1}J(\\theta^{\\star})$（其中 $\\Sigma_y = \\sigma_y^2$），计算其行列式 $\\det(H)$，并以此分析逆问题的不适定性。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题具有科学依据。它为一个简单的线性正向模型提出了一个典型的贝叶斯逆问题。模型 $G(\\theta) = \\frac{1}{2}(\\theta_1 + \\theta_2)$ 是层状复合材料面内有效电导率的混合法则，这是均匀化理论中的一个标准结果。使用高斯先验和加性高斯噪声是不确定性量化中一个常见且在数学上合理的框架。\n- **适定性：** 问题陈述是适定的。它提供了执行所要求的推导和计算所需的所有必要定义和约束。所提出的问题具有唯一且可验证的答案。\n- **客观性：** 该问题使用精确、客观的数学语言陈述，没有歧义或主观性断言。\n\n**步骤3：结论与行动**\n- **结论：** 该问题在科学上是合理的、自洽的且适定的。这是一个有效的问题。\n- **行动：** 继续求解。\n\n**第1部分：正向不确定性量化**\n\n目标是推导可观测输出的方差 $\\operatorname{Var}(y)$。数据模型由 $y = G(\\theta) + \\varepsilon$ 给出。参数 $\\theta$ 和噪声 $\\varepsilon$ 被假定为独立的随机变量。因此，它们和的方差等于它们各自方差的和：\n$$\n\\operatorname{Var}(y) = \\operatorname{Var}(G(\\theta)) + \\operatorname{Var}(\\varepsilon)\n$$\n给定 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$，这直接意味着 $\\operatorname{Var}(\\varepsilon) = \\sigma_y^2$。\n\n接下来，我们计算前向模型输出的方差 $\\operatorname{Var}(G(\\theta))$。前向模型是参数向量 $\\theta$ 的一个线性函数：\n$$\nG(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2 = \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\end{pmatrix} = k^{\\top}\\theta\n$$\n其中 $k = (\\tfrac{1}{2}, \\tfrac{1}{2})^{\\top}$。随机向量线性变换的方差由公式 $\\operatorname{Var}(k^{\\top}\\theta) = k^{\\top} \\operatorname{Cov}(\\theta) k$ 给出。在本例中，$\\operatorname{Cov}(\\theta) = C$。\n代入 $k$ 和 $C$ 的表达式：\n$$\n\\operatorname{Var}(G(\\theta)) = \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} s^2 & -\\rho s^2 \\\\ -\\rho s^2 & s^2 \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix}\n$$\n首先，我们执行左乘：\n$$\n\\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} s^2 & -\\rho s^2 \\\\ -\\rho s^2 & s^2 \\end{pmatrix} = \\begin{pmatrix} \\tfrac{1}{2}s^2 - \\tfrac{1}{2}\\rho s^2 & -\\tfrac{1}{2}\\rho s^2 + \\tfrac{1}{2}s^2 \\end{pmatrix} = \\begin{pmatrix} \\tfrac{s^2}{2}(1-\\rho) & \\tfrac{s^2}{2}(1-\\rho) \\end{pmatrix}\n$$\n现在，我们完成二次型计算：\n$$\n\\operatorname{Var}(G(\\theta)) = \\begin{pmatrix} \\tfrac{s^2}{2}(1-\\rho) & \\tfrac{s^2}{2}(1-\\rho) \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix} = \\tfrac{s^2}{2}(1-\\rho)(\\tfrac{1}{2}) + \\tfrac{s^2}{2}(1-\\rho)(\\tfrac{1}{2}) = \\tfrac{s^2}{4}(1-\\rho) + \\tfrac{s^2}{4}(1-\\rho) = \\tfrac{s^2}{2}(1-\\rho)\n$$\n综合以上结果，总输出方差为：\n$$\n\\operatorname{Var}(y) = \\tfrac{s^2}{2}(1-\\rho) + \\sigma_y^2\n$$\n当负相关强度 $\\rho$ 接近其上限 $1$ 时，项 $(1-\\rho)$ 趋近于 $0$。因此，\n$$\n\\lim_{\\rho \\to 1} \\operatorname{Var}(y) = \\lim_{\\rho \\to 1} \\left( \\tfrac{s^2}{2}(1-\\rho) + \\sigma_y^2 \\right) = 0 + \\sigma_y^2 = \\sigma_y^2\n$$\n这表明，随着先验相关性变为完全负相关（$\\rho \\to 1$），由参数贡献的模型输出不确定性 $\\operatorname{Var}(G(\\theta))$ 消失了。总输出不确定性变得完全由测量噪声方差 $\\sigma_y^2$ 主导。这是因为如果 $\\theta_1$ 和 $\\theta_2$ 强负相关，那么 $\\theta_1$ 高于其均值的偏差会伴随着 $\\theta_2$ 低于其均值的偏差，从而使得它们的和（以及它们的平均值 $G(\\theta)$）保持相对恒定。这导致前向模型输出的分布非常窄。\n\n**第2部分：逆问题分析与不适定性**\n\n我们通过检验负对数似然的Hessian矩阵的高斯-牛顿近似来分析逆问题的局部可辨识性。该矩阵由 $H = J(\\theta^{\\star})^{\\top}\\Sigma_y^{-1}J(\\theta^{\\star})$ 给出。此矩阵的逆矩阵与参数的后验协方差有关，其奇异性表示不可辨識性。\n\n首先，我们计算前向映射 $G(\\theta) = \\tfrac{1}{2}\\theta_1 + \\tfrac{1}{2}\\theta_2$ 的雅可比矩阵。雅可比矩阵是一阶偏导数矩阵，在本例中是一个 $1 \\times 2$ 的行向量：\n$$\nJ(\\theta) = \\begin{pmatrix} \\frac{\\partial G}{\\partial \\theta_1} & \\frac{\\partial G}{\\partial \\theta_2} \\end{pmatrix} = \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix}\n$$\n雅可比矩阵是常数，不依赖于其求值点 $\\theta^{\\star}$。因此，$J(\\theta^{\\star}) = \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix}$。\n\n输出 $y$ 是一个标量，所以其协方差矩阵 $\\Sigma_y$ 是一个 $1 \\times 1$ 矩阵，由 $\\Sigma_y = [\\sigma_y^2]$ 给出。其逆矩阵为 $\\Sigma_y^{-1} = [\\tfrac{1}{\\sigma_y^2}]$。\n\n现在，我们构建高斯-牛顿矩阵 $H$。雅可比矩阵的转置为 $J(\\theta^{\\star})^{\\top} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix}$。\n$$\nH = J(\\theta^{\\star})^{\\top} \\Sigma_y^{-1} J(\\theta^{\\star}) = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix} \\left( \\tfrac{1}{\\sigma_y^2} \\right) \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix}\n$$\n执行矩阵乘法：\n$$\nH = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix} = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} (\\tfrac{1}{2})(\\tfrac{1}{2}) & (\\tfrac{1}{2})(\\tfrac{1}{2}) \\\\ (\\tfrac{1}{2})(\\tfrac{1}{2}) & (\\tfrac{1}{2})(\\tfrac{1}{2}) \\end{pmatrix} = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\tfrac{1}{4} & \\tfrac{1}{4} \\\\ \\tfrac{1}{4} & \\tfrac{1}{4} \\end{pmatrix}\n$$\n最后一步是计算这个 $2 \\times 2$ 矩阵 $H$ 的行列式：\n$$\n\\det(H) = \\det\\left( \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\tfrac{1}{4} & \\tfrac{1}{4} \\\\ \\tfrac{1}{4} & \\tfrac{1}{4} \\end{pmatrix} \\right)\n$$\n使用行列式性质 $\\det(cA) = c^n\\det(A)$（对于一个 $n \\times n$ 矩阵 $A$），其中 $n=2$ 且 $c=1/\\sigma_y^2$：\n$$\n\\det(H) = \\left( \\frac{1}{\\sigma_y^2} \\right)^2 \\det\\begin{pmatrix} \\tfrac{1}{4} & \\tfrac{1}{4} \\\\ \\tfrac{1}{4} & \\tfrac{1}{4} \\end{pmatrix} = \\frac{1}{\\sigma_y^4} \\left( (\\tfrac{1}{4})(\\tfrac{1}{4}) - (\\tfrac{1}{4})(\\tfrac{1}{4}) \\right) = \\frac{1}{\\sigma_y^4} \\left( \\tfrac{1}{16} - \\tfrac{1}{16} \\right) = 0\n$$\n行列式为零意味着矩阵 $H$ 是奇异的。奇异的高斯-牛頓Hessian矩阵表明，似然函数在参数空间中至少沿着一个方向是平坦的。这意味着沿此方向改变参数对模型输出 $G(\\theta)$ 没有影响。因此，数据 $y$ 没有提供任何信息来约束该方向上的参数。这是不适定逆问题的一个标志，因为无穷多的参数组合会产生相同的模型预测，从而得到相同的似然值。在本例中，$H$ 的零空间由向量 $(1, -1)^{\\top}$ 张成，这意味着任何在直线 $\\theta_1 + \\theta_2 = \\text{常数}$ 上的参数都是无法区分的。\n\n这个分析揭示了一个关键的见解：尽管当 $\\rho \\to 1$ 时前向不确定性 $\\operatorname{Var}(y)$ 变小，表明输出是良好确定的，但由于前向模型 $G(\\theta)$ 的结构，逆问题根本上是不适定的。测量值 $y$ 只提供了关于和 $\\theta_1 + \\theta_2$ 的信息，而没有提供关于 $\\theta_1$ 和 $\\theta_2$ 各自值的信息。",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "在许多实际应用中，测量误差并非是相互独立的，而是存在相关性；忽略这种相关性可能会导致错误的推断结果。本计算练习  旨在让你量化这种模型假设带来的影响。你将使用Kullback-Leibler (KL) 散度作为度量，比较在独立误差与相关误差假设下得到的后验分布，从而深刻理解在构建统计模型时正确刻画误差结构的重要性。",
            "id": "3770737",
            "problem": "考虑一个多尺度建模与分析中的逆不确定性量化（UQ）问题，其中粗尺度可观测量向量 $\\mathbf{y} \\in \\mathbb{R}^m$ 是由一个细尺度参数向量 $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ 通过一个可微正向模型 $\\mathbf{f}(\\boldsymbol{\\theta})$ 生成的。假设高斯先验 $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\theta}_0, \\boldsymbol{\\Gamma}_0)$ 和观测噪声 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$。数据由 $\\mathbf{y}_{\\text{obs}} = \\mathbf{f}(\\boldsymbol{\\theta}) + \\boldsymbol{\\varepsilon}$ 给出。为便于处理，使用关于先验均值的一阶线性化，即 $\\mathbf{f}(\\boldsymbol{\\theta}) \\approx \\mathbf{f}(\\boldsymbol{\\theta}_0) + \\mathbf{J}(\\boldsymbol{\\theta}_0)\\left(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0\\right)$，其中 $\\mathbf{J}(\\boldsymbol{\\theta}_0) \\in \\mathbb{R}^{m \\times d}$ 是在 $\\boldsymbol{\\theta}_0$ 处的敏感度雅可比矩阵。\n\n从贝叶斯定理和多维高斯分布的定义出发，推导在线性化观测模型下 $\\boldsymbol{\\theta}$ 的后验分布。然后，通过比较在两种不同误差假设下获得的后验分布，量化显式建模输出相关性的益处：\n- 独立误差：$\\boldsymbol{\\Sigma}_{\\text{ind}} = \\operatorname{diag}(\\boldsymbol{s} \\odot \\boldsymbol{s})$，其中 $\\boldsymbol{s} \\in \\mathbb{R}^m$ 包含边际标准差，$\\odot$ 表示逐元素乘积。\n- 相关误差：$\\boldsymbol{\\Sigma}_{\\text{corr}} = \\operatorname{diag}(\\boldsymbol{s}) \\, \\mathbf{R} \\, \\operatorname{diag}(\\boldsymbol{s})$，其中 $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ 是一个对称正定的相关矩阵。\n\n将益处度量定义为从相关误差后验到独立误差后验的 Kullback–Leibler 散度（KLD），即散度 $\\mathcal{D}_{\\text{KL}}\\!\\left(\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{corr}}, \\boldsymbol{\\Gamma}_{\\text{corr}}) \\,\\Vert\\, \\mathcal{N}(\\boldsymbol{\\mu}_{\\text{ind}}, \\boldsymbol{\\Gamma}_{\\text{ind}})\\right)$，其值为非负，且当且仅当两个高斯后验分布相同时为零。\n\n在所有测试用例中，使用以下固定的实例：\n- 参数维度 $d = 2$ 和输出维度 $m = 3$。\n- 先验均值 $\\boldsymbol{\\theta}_0 = [\\,0.0,\\;0.0\\,]^T$。\n- 先验协方差 $\\boldsymbol{\\Gamma}_0 = \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.1 & 0.8 \\end{bmatrix}$。\n- 雅可比矩阵 $\\mathbf{J}(\\boldsymbol{\\theta}_0) = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.5 & -0.3 \\\\ 0.1 & 0.8 \\end{bmatrix}$。\n- 在先验均值处的正向模型值 $\\mathbf{f}(\\boldsymbol{\\theta}_0) = [\\,0.2,\\;-0.1,\\;0.0\\,]^T$。\n- 观测数据 $\\mathbf{y}_{\\text{obs}} = [\\,1.0,\\;-0.3,\\;0.4\\,]^T$。\n\n通过在 $\\boldsymbol{\\theta}_0$ 附近进行线性化，将似然函数写成 $\\mathbf{z} = \\mathbf{y}_{\\text{obs}} - \\mathbf{f}(\\boldsymbol{\\theta}_0)$ 的形式，并执行高斯条件化，计算两种误差假设下的后验分布。然后，为下面的每个测试用例评估 Kullback–Leibler 散度。\n\n测试套件（每个用例指定了定义 $\\boldsymbol{\\Sigma}_{\\text{corr}}$ 的 $\\boldsymbol{s}$ 和 $\\mathbf{R}$）：\n- 情况 A（标准情况，中等相关性）：\n  - $\\boldsymbol{s} = [\\,0.2,\\;0.3,\\;0.15\\,]^T$，\n  - $\\mathbf{R} = \\begin{bmatrix} 1 & 0.5 & 0.1 \\\\ 0.5 & 1 & 0.2 \\\\ 0.1 & 0.2 & 1 \\end{bmatrix}$。\n- 情况 B（强相关性，近共线性）：\n  - $\\boldsymbol{s} = [\\,0.1,\\;0.1,\\;0.08\\,]^T$，\n  - $\\mathbf{R} = \\begin{bmatrix} 1 & 0.95 & 0.7 \\\\ 0.95 & 1 & 0.6 \\\\ 0.7 & 0.6 & 1 \\end{bmatrix}$。\n- 情况 C（独立基准，零相关性）：\n  - $\\boldsymbol{s} = [\\,0.25,\\;0.25,\\;0.25\\,]^T$，\n  - $\\mathbf{R} = \\mathbf{I}_3$。\n- 情况 D（病态但正定的相关性）：\n  - $\\boldsymbol{s} = [\\,0.12,\\;0.12,\\;0.20\\,]^T$，\n  - $\\mathbf{R} = \\begin{bmatrix} 1 & 0.99 & 0 \\\\ 0.99 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$。\n\n对于每种情况：\n1. 构建 $\\boldsymbol{\\Sigma}_{\\text{corr}} = \\operatorname{diag}(\\boldsymbol{s}) \\, \\mathbf{R} \\, \\operatorname{diag}(\\boldsymbol{s})$ 和 $\\boldsymbol{\\Sigma}_{\\text{ind}} = \\operatorname{diag}(\\boldsymbol{s} \\odot \\boldsymbol{s})$。\n2. 使用线性化模型和先验，在 $\\boldsymbol{\\Sigma}_{\\text{corr}}$ 和 $\\boldsymbol{\\Sigma}_{\\text{ind}}$ 下计算 $\\boldsymbol{\\theta}$ 的两个高斯后验分布。\n3. 计算从相关误差后验到独立误差后验的 Kullback–Leibler 散度。\n\n您的程序应生成单行输出，其中包含四个 KLD 值，格式为方括号内的逗号分隔浮点数列表（例如，$[a,b,c,d]$）。所有量均为无量纲，且本问题不涉及角度。答案必须以浮点数表示。通过使用适当的线性代数例程，确保矩阵求逆和行列式计算的数值稳定性。该测试套件旨在覆盖一个一般情况、一个强相关性边界、一个独立性基准以及一个保持对称正定的病态相关性情况。",
            "solution": "目标是在贝叶斯框架内推导参数向量 $\\boldsymbol{\\theta}$ 的后验分布，然后量化建模观测误差相关性的影响。量化是通过计算在两种不同误差协方差结构假设下推导出的后验分布之间的 Kullback-Leibler (KL) 散度来实现的：一种假设误差独立，另一种假设误差相关。\n\n### 1. 贝叶斯框架与线性化\n\n给定一个细尺度参数向量 $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ 和一个粗尺度可观测量向量 $\\mathbf{y} \\in \\mathbb{R}^m$。它们之间的关系由一个正向模型 $\\mathbf{f}(\\boldsymbol{\\theta})$ 和加性高斯噪声 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$ 描述。数据生成过程为：\n$$ \\mathbf{y}_{\\text{obs}} = \\mathbf{f}(\\boldsymbol{\\theta}) + \\boldsymbol{\\varepsilon} $$\n关于参数的先验信念也是高斯的：$\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\theta}_0, \\boldsymbol{\\Gamma}_0)$。\n\n根据贝叶斯定理，给定观测数据，参数的后验概率分布为：\n$$ p(\\boldsymbol{\\theta} | \\mathbf{y}_{\\text{obs}}) \\propto p(\\mathbf{y}_{\\text{obs}} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) $$\n这里，$p(\\boldsymbol{\\theta})$ 是先验分布，$p(\\mathbf{y}_{\\text{obs}} | \\boldsymbol{\\theta})$ 是似然函数。给定高斯噪声模型，似然函数为：\n$$ p(\\mathbf{y}_{\\text{obs}} | \\boldsymbol{\\theta}) = \\mathcal{N}(\\mathbf{y}_{\\text{obs}} | \\mathbf{f}(\\boldsymbol{\\theta}), \\boldsymbol{\\Sigma}) $$\n为了便于处理，正向模型 $\\mathbf{f}(\\boldsymbol{\\theta})$ 在先验均值 $\\boldsymbol{\\theta}_0$ 附近进行线性化：\n$$ \\mathbf{f}(\\boldsymbol{\\theta}) \\approx \\mathbf{f}(\\boldsymbol{\\theta}_0) + \\mathbf{J}(\\boldsymbol{\\theta}_0)(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0) $$\n其中 $\\mathbf{J}(\\boldsymbol{\\theta}_0)$ 是 $\\mathbf{f}$ 在 $\\boldsymbol{\\theta}_0$ 处求值的雅可比矩阵。我们记 $\\mathbf{J} = \\mathbf{J}(\\boldsymbol{\\theta}_0)$。在此线性近似下，似然函数变为：\n$$ p(\\mathbf{y}_{\\text{obs}} | \\boldsymbol{\\theta}) \\approx \\mathcal{N}(\\mathbf{y}_{\\text{obs}} | \\mathbf{f}(\\boldsymbol{\\theta}_0) + \\mathbf{J}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0), \\boldsymbol{\\Sigma}) $$\n\n### 2. 高斯后验的推导\n\n两个高斯分布（先验和线性化似然）的乘积是另一个高斯分布。因此，后验分布为 $p(\\boldsymbol{\\theta} | \\mathbf{y}_{\\text{obs}}) = \\mathcal{N}(\\boldsymbol{\\theta} | \\boldsymbol{\\mu}_{\\text{post}}, \\boldsymbol{\\Gamma}_{\\text{post}})$。我们可以通过考察后验密度的指数部分来找到后验均值 $\\boldsymbol{\\mu}_{\\text{post}}$ 和协方差 $\\boldsymbol{\\Gamma}_{\\text{post}}$，该指数是先验和似然密度指数的总和（忽略常数项）。\n$$ \\ln p(\\boldsymbol{\\theta} | \\mathbf{y}_{\\text{obs}}) \\propto -\\frac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)^T \\boldsymbol{\\Gamma}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0) - \\frac{1}{2} (\\mathbf{y}_{\\text{obs}} - \\mathbf{f}(\\boldsymbol{\\theta}_0) - \\mathbf{J}(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_0))^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y}_{\\text{obs}} - \\mathbf{f}(\\boldsymbol{\\theta}_0) - \\mathbf{J}(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_0)) $$\n令新息向量为 $\\mathbf{z} = \\mathbf{y}_{\\text{obs}} - \\mathbf{f}(\\boldsymbol{\\theta}_0)$。表达式简化为：\n$$ \\ln p(\\boldsymbol{\\theta} | \\mathbf{y}_{\\text{obs}}) \\propto -\\frac{1}{2} \\left[ (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)^T \\boldsymbol{\\Gamma}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0) + (\\mathbf{z} - \\mathbf{J}(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_0))^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{z} - \\mathbf{J}(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_0)) \\right] $$\n通过对 $\\boldsymbol{\\theta}$ 进行配方，我们确定后验逆协方差（精度矩阵）$\\boldsymbol{\\Gamma}_{\\text{post}}^{-1}$ 和均值 $\\boldsymbol{\\mu}_{\\text{post}}$。$\\boldsymbol{\\theta}$ 的二次项给出了后验精度：\n$$ \\boldsymbol{\\Gamma}_{\\text{post}}^{-1} = \\boldsymbol{\\Gamma}_0^{-1} + \\mathbf{J}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{J} $$\n后验协方差是其逆矩阵：\n$$ \\boldsymbol{\\Gamma}_{\\text{post}} = (\\boldsymbol{\\Gamma}_0^{-1} + \\mathbf{J}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{J})^{-1} $$\n后验均值由下式给出：\n$$ \\boldsymbol{\\mu}_{\\text{post}} = \\boldsymbol{\\Gamma}_{\\text{post}} (\\mathbf{J}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{z} + \\boldsymbol{\\Gamma}_0^{-1} \\boldsymbol{\\theta}_0) $$\n由于问题指定在先验均值 $\\boldsymbol{\\theta}_0 = \\mathbf{0}$ 附近进行线性化，后验均值的公式简化为：\n$$ \\boldsymbol{\\mu}_{\\text{post}} = \\boldsymbol{\\Gamma}_{\\text{post}} (\\mathbf{J}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{z}) $$\n\n我们根据 $\\boldsymbol{\\Sigma}$ 的不同选择计算两个后验分布：\n1.  **独立误差**：$\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}_{\\text{ind}} = \\operatorname{diag}(\\boldsymbol{s} \\odot \\boldsymbol{s})$。得到的后验分布为 $\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{ind}}, \\boldsymbol{\\Gamma}_{\\text{ind}})$。\n2.  **相关误差**：$\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}_{\\text{corr}} = \\operatorname{diag}(\\boldsymbol{s}) \\, \\mathbf{R} \\, \\operatorname{diag}(\\boldsymbol{s})$。得到的后验分布为 $\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{corr}}, \\boldsymbol{\\Gamma}_{\\text{corr}})$。\n\n### 3. Kullback–Leibler 散度\n\n建模相关性的益处由从相关误差后验 $P_1 = \\mathcal{N}(\\boldsymbol{\\mu}_{\\text{corr}}, \\boldsymbol{\\Gamma}_{\\text{corr}})$ 到独立误差后验 $P_2 = \\mathcal{N}(\\boldsymbol{\\mu}_{\\text{ind}}, \\boldsymbol{\\Gamma}_{\\text{ind}})$ 的 KL 散度来量化。设参数空间的维度为 $d$。两个多维高斯分布 $P_1 = \\mathcal{N}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Gamma}_1)$ 和 $P_2 = \\mathcal{N}(\\boldsymbol{\\mu}_2, \\boldsymbol{\\Gamma}_2)$ 之间的 KL 散度公式为：\n$$ \\mathcal{D}_{\\text{KL}}(P_1 \\Vert P_2) = \\frac{1}{2} \\left[ \\text{tr}(\\boldsymbol{\\Gamma}_2^{-1} \\boldsymbol{\\Gamma}_1) + (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Gamma}_2^{-1} (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1) - d + \\ln\\left(\\frac{\\det \\boldsymbol{\\Gamma}_2}{\\det \\boldsymbol{\\Gamma}_1}\\right) \\right] $$\n代入我们问题中的具体后验分布，我们得到益处度量：\n$$ \\mathcal{D}_{\\text{KL}} = \\frac{1}{2} \\left[ \\text{tr}(\\boldsymbol{\\Gamma}_{\\text{ind}}^{-1} \\boldsymbol{\\Gamma}_{\\text{corr}}) + (\\boldsymbol{\\mu}_{\\text{ind}} - \\boldsymbol{\\mu}_{\\text{corr}})^T \\boldsymbol{\\Gamma}_{\\text{ind}}^{-1} (\\boldsymbol{\\mu}_{\\text{ind}} - \\boldsymbol{\\mu}_{\\text{corr}}) - d + \\ln\\left(\\frac{\\det \\boldsymbol{\\Gamma}_{\\text{ind}}}{\\det \\boldsymbol{\\Gamma}_{\\text{corr}}}\\right) \\right] $$\n将使用给定的数值为每个测试用例计算此度量。如果 $\\boldsymbol{\\Sigma}_{\\text{corr}} = \\boldsymbol{\\Sigma}_{\\text{ind}}$（如情况 C 中 $\\mathbf{R} = \\mathbf{I}$），则 $\\boldsymbol{\\mu}_{\\text{corr}} = \\boldsymbol{\\mu}_{\\text{ind}}$ 且 $\\boldsymbol{\\Gamma}_{\\text{corr}} = \\boldsymbol{\\Gamma}_{\\text{ind}}$，导致 KL 散度恰好为零，因为所有项都相互抵消。对于其他情况，该度量量化了因忽略误差相关性而损失的信息。",
            "answer": "```python\nimport numpy as np\n\ndef _compute_posterior(Sigma, Gamma_0_inv, J, z, theta_0):\n    \"\"\"\n    Computes the posterior mean and covariance for a linear Gaussian model.\n    The model is z = J(theta - theta_0) + epsilon, where epsilon ~ N(0, Sigma)\n    and the prior on theta is theta ~ N(theta_0, Gamma_0).\n    \"\"\"\n    # Inverse of the observation error covariance\n    Sigma_inv = np.linalg.inv(Sigma)\n    \n    # Posterior precision matrix: Gamma_post^-1 = Gamma_0^-1 + J^T * Sigma^-1 * J\n    Gamma_post_inv = Gamma_0_inv + J.T @ Sigma_inv @ J\n    \n    # Posterior covariance: Gamma_post = (Gamma_post^-1)^-1\n    Gamma_post = np.linalg.inv(Gamma_post_inv)\n    \n    # Posterior mean: mu_post = Gamma_post * (J^T * Sigma^-1 * z + Gamma_0^-1 * theta_0)\n    # The term `theta_0` is the mean of the prior. In this problem, it's zero.\n    mu_post = Gamma_post @ (J.T @ Sigma_inv @ (z) + Gamma_0_inv @ theta_0)\n    \n    return mu_post, Gamma_post\n\ndef _compute_kullback_leibler(mu1, Gamma1, mu2, Gamma2, dim):\n    \"\"\"\n    Computes the KL divergence D_KL(P1 || P2) for two multivariate Gaussians\n    P1 ~ N(mu1, Gamma1) and P2 ~ N(mu2, Gamma2).\n    \"\"\"\n    # For cases where posteriors are identical (e.g., Case C), KLD is exactly 0.\n    # This check avoids potential floating-point inaccuracies.\n    if np.allclose(mu1, mu2) and np.allclose(Gamma1, Gamma2):\n        return 0.0\n\n    # Inverse of the second distribution's covariance\n    Gamma2_inv = np.linalg.inv(Gamma2)\n    \n    # Term 1: Trace of the product of inverse covariance of P2 and covariance of P1\n    trace_term = np.trace(Gamma2_inv @ Gamma1)\n    \n    # Term 2: Mahalanobis distance between the means, with respect to P2's covariance\n    mu_diff = mu2 - mu1\n    mahalanobis_term = mu_diff.T @ Gamma2_inv @ mu_diff\n    \n    # Term 3: Log-determinant ratio, using slogdet for numerical stability\n    sign1, logdet1 = np.linalg.slogdet(Gamma1)\n    sign2, logdet2 = np.linalg.slogdet(Gamma2)\n    # For SPD covariance matrices, sign will be +1.\n    log_det_term = logdet2 - logdet1\n    \n    # KLD formula: 0.5 * [ trace_term + mahalanobis_term - dimension + log_det_term ]\n    kld = 0.5 * (trace_term + mahalanobis_term - dim + log_det_term)\n    return kld\n\ndef solve():\n    # Define fixed problem parameters based on the problem statement.\n    # Parameter dimension d\n    d = 2 \n    # Prior mean (theta_0)\n    theta_0 = np.array([0.0, 0.0])\n    # Prior covariance (Gamma_0)\n    Gamma_0 = np.array([[0.5, 0.1], [0.1, 0.8]])\n    # Jacobian matrix (J)\n    J = np.array([[1.0, 0.2], [0.5, -0.3], [0.1, 0.8]])\n    # Forward model evaluation at the prior mean (f(theta_0))\n    f_at_theta_0 = np.array([0.2, -0.1, 0.0])\n    # Observed data (y_obs)\n    y_obs = np.array([1.0, -0.3, 0.4])\n\n    # Pre-calculate derived quantities that are constant across test cases.\n    # The innovation vector z = y_obs - f(theta_0)\n    z = y_obs - f_at_theta_0\n    # The inverse of the prior covariance (prior precision)\n    Gamma_0_inv = np.linalg.inv(Gamma_0)\n\n    # Define the test suite as a list of dictionaries.\n    test_cases = [\n        # Case A: Moderate correlations\n        {'s': np.array([0.2, 0.3, 0.15]),\n         'R': np.array([[1.0, 0.5, 0.1], [0.5, 1.0, 0.2], [0.1, 0.2, 1.0]])},\n        # Case B: Strong correlations\n        {'s': np.array([0.1, 0.1, 0.08]),\n         'R': np.array([[1.0, 0.95, 0.7], [0.95, 1.0, 0.6], [0.7, 0.6, 1.0]])},\n        # Case C: Independent baseline\n        {'s': np.array([0.25, 0.25, 0.25]),\n         'R': np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])},\n        # Case D: Ill-conditioned correlations\n        {'s': np.array([0.12, 0.12, 0.20]),\n         'R': np.array([[1.0, 0.99, 0.0], [0.99, 1.0, 0.0], [0.0, 0.0, 1.0]])},\n    ]\n\n    results = []\n    for case in test_cases:\n        s_vector = case['s']\n        R_matrix = case['R']\n        \n        # 1. Construct the observation error covariance matrices.\n        # Correlated error model: Sigma_corr = diag(s) * R * diag(s)\n        Sigma_corr = np.diag(s_vector) @ R_matrix @ np.diag(s_vector)\n        # Independent error model: Sigma_ind = diag(s^2)\n        Sigma_ind = np.diag(s_vector**2)\n        \n        # 2. Compute the posteriors for both error models.\n        # This function encapsulates the Bayesian linear model update equations.\n        mu_corr, Gamma_corr = _compute_posterior(Sigma_corr, Gamma_0_inv, J, z, theta_0)\n        mu_ind, Gamma_ind = _compute_posterior(Sigma_ind, Gamma_0_inv, J, z, theta_0)\n        \n        # 3. Compute the Kullback-Leibler divergence from the correlated-error posterior\n        #    to the independent-error posterior: D_KL(P_corr || P_ind).\n        kld_value = _compute_kullback_leibler(mu_corr, Gamma_corr, mu_ind, Gamma_ind, d)\n        results.append(kld_value)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}