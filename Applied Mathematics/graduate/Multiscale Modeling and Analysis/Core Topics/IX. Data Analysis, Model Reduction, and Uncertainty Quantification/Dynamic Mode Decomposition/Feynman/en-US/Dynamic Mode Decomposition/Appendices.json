{
    "hands_on_practices": [
        {
            "introduction": "Dynamic Mode Decomposition bridges the gap between discrete data snapshots and the underlying continuous-time dynamics of a system. A key step in this process is translating the discrete-time eigenvalues, $\\mu$, produced by the DMD algorithm into their continuous-time counterparts, $\\omega$. This fundamental practice allows you to determine the physical characteristics of each dynamic mode, such as its growth or decay rate and its frequency of oscillation, from the raw output of the DMD computation .",
            "id": "860820",
            "problem": "Dynamic Mode Decomposition (DMD) is a data-driven method for analyzing the dynamics of complex systems. Consider a spatiotemporal system, such as a fluid flow or a reaction-diffusion process, whose state is described by a field $\\mathbf{u}(x, t)$. A sequence of snapshots of the system state, $\\mathbf{v}_k = \\mathbf{u}(x, t_k)$, is collected at discrete, uniformly spaced times $t_k = k \\Delta t$.\n\nThe core assumption of DMD is that there exists a linear operator $\\mathbf{A}$ that approximates the evolution of the state from one snapshot to the next, such that $\\mathbf{v}_{k+1} \\approx \\mathbf{A} \\mathbf{v}_k$. The DMD algorithm computes the eigenvalues $\\mu$ and corresponding eigenvectors (DMD modes) of this operator $\\mathbf{A}$.\n\nThese discrete-time eigenvalues are related to the eigenvalues $\\omega$ of an underlying continuous-time linear operator $\\mathbf{L}$ (from the model $\\frac{d\\mathbf{v}}{dt} = \\mathbf{L}\\mathbf{v}$) by the mapping $\\mu = e^{\\omega \\Delta t}$. The real part of a continuous-time eigenvalue, $\\text{Re}(\\omega)$, determines the temporal growth or decay rate of the corresponding mode, while the imaginary part, $\\text{Im}(\\omega)$, determines its frequency of oscillation.\n\nA researcher applies DMD to a dataset from a chaotic system and identifies a dynamically significant mode. The complex eigenvalue associated with this mode is found to be $\\mu = a + i b$, where $a$ and $b$ are given real constants. Given the sampling time step $\\Delta t$, determine the temporal growth rate of this continuous-time mode.",
            "solution": "1. The discrete‐to‐continuous mapping is  \n$$\\mu = e^{\\omega \\,\\Delta t}\\,. $$\n\n2. Take the complex logarithm:  \n$$\\ln \\mu = \\omega\\,\\Delta t\\quad\\Longrightarrow\\quad \\omega = \\frac{1}{\\Delta t}\\,\\ln\\mu\\,. $$\n\n3. Write $\\mu=a+ib$ in polar form:  \n$$|\\mu|=\\sqrt{a^2+b^2},\\quad \\arg(\\mu)=\\tan^{-1}\\!\\frac{b}{a},$$  \nso  \n$$\\ln\\mu = \\ln|\\mu| + i\\,\\arg(\\mu) = \\ln\\sqrt{a^2+b^2} + i\\,\\tan^{-1}\\!\\frac{b}{a}\\,. $$\n\n4. The temporal growth rate is the real part of $\\omega$:  \n$$\\Re(\\omega) = \\frac{1}{\\Delta t}\\,\\Re\\bigl(\\ln\\mu\\bigr)\n= \\frac{1}{\\Delta t}\\,\\ln\\sqrt{a^2+b^2}\n= \\frac{1}{2\\,\\Delta t}\\,\\ln\\bigl(a^2+b^2\\bigr)\\,. $$",
            "answer": "$$\\boxed{\\frac{1}{2\\,\\Delta t}\\,\\ln\\!\\bigl(a^2+b^2\\bigr)}$$"
        },
        {
            "introduction": "Applying DMD to real-world data often requires careful preprocessing to avoid misinterpretations. A common pitfall is the presence of global scaling trends, which the standard algorithm can mistake for a dynamically growing mode. This hands-on exercise demonstrates this issue and introduces per-snapshot normalization as a robust solution to separate true dynamics from simple scaling effects . Mastering this technique is crucial for obtaining physically meaningful results from experimental or simulation data.",
            "id": "3121296",
            "problem": "You are to implement a complete program that constructs illustrative data sets and applies Dynamic Mode Decomposition (DMD) to demonstrate how per-snapshot scale changes can cause DMD to misinterpret a trend as a growing mode, and how normalization of each snapshot prevents this misinterpretation.\n\nThe fundamental base you must use is as follows:\n- The data consist of discrete-time snapshots of a state vector, where a best-fit linear operator maps each snapshot to the next. Specifically, the state evolves according to a linear model $x_{t+1} \\approx A x_t$ that minimizes the total squared error over all consecutive snapshot pairs. This is a least-squares problem in which the Frobenius norm of the residual is minimized.\n- The Dynamic Mode Decomposition (DMD) algorithm constructs a low-dimensional representation of the best-fit operator from the snapshot data and returns eigenvalues that approximate the discrete-time dynamics. An eigenvalue with magnitude greater than $1$ indicates growth per time step; a magnitude equal to $1$ indicates neutral oscillation; a magnitude less than $1$ indicates decay.\n\nDesign the program to:\n- Construct matrices of column snapshots for multiple cases, where each column is one snapshot at an integer time $t$.\n- Compute the eigenvalues of the DMD operator and report the spectral radius, defined as the maximum of the magnitudes of the eigenvalues.\n- Apply per-snapshot normalization in the specified cases: divide each column by its Euclidean norm to remove overall scale variations.\n\nData construction details to ensure scientific realism:\n- Use a base two-dimensional oscillatory signal with angular frequency $\\omega$ (in radians per time step) given by $\\omega = 0.37$. For each integer time $t$ with $0 \\le t \\le T-1$ and with number of snapshots $T = 80$, define the unscaled state as $x_t = [\\cos(\\omega t), \\sin(\\omega t)]^\\top$. This corresponds to a pure planar rotation of constant amplitude.\n- For cases with scaling, multiply each snapshot by a scaling factor $s_t = \\alpha^t$ with $\\alpha = 1.02$. This introduces a monotone scale change across time that is not part of the true underlying neutral oscillation but can be incorrectly interpreted by DMD as growth unless snapshots are normalized.\n- For the degenerate edge case, use a constant spatial direction $v \\in \\mathbb{R}^3$ with components $v = [1, -1, 0]^\\top$, and define snapshots $x_t = s_t v$ with the same scaling factor $s_t = \\alpha^t$ and number of snapshots $T = 80$.\n\nAlgorithmic requirements:\n- From the snapshot matrix $X = [x_0, x_1, \\dots, x_{T-1}]$, form $X_1 = [x_0, x_1, \\dots, x_{T-2}]$ and $X_2 = [x_1, x_2, \\dots, x_{T-1}]$.\n- Compute the best-fit linear operator in the least-squares sense and extract its eigenvalues in a numerically stable manner by using a reduced low-rank representation constructed from singular value decomposition. You must not assume prior knowledge of the target matrix or its spectrum; you must derive it from the data.\n- For per-snapshot normalization, scale each column $x_t$ to $\\widehat{x}_t = x_t / \\|x_t\\|_2$ for all $t$ for which the norm is nonzero.\n\nTest suite:\n- Case $1$ (baseline, no scaling): Two-dimensional rotation with $T = 80$, $\\omega = 0.37$, snapshots $x_t = [\\cos(\\omega t), \\sin(\\omega t)]^\\top$. Compute the DMD spectral radius $\\rho_1$. Expect $\\rho_1$ close to $1$.\n- Case $2$ (scale-induced trend): Same as Case $1$ but scaled by $s_t = \\alpha^t$ with $\\alpha = 1.02$. Compute the DMD spectral radius $\\rho_2$. Expect $\\rho_2 > 1$.\n- Case $3$ (normalized to remove scale trend): Same as Case $2$, but normalize each snapshot by its Euclidean norm before applying DMD. Compute the DMD spectral radius $\\rho_3$. Expect $\\rho_3$ close to $1$.\n- Case $4$ (degenerate edge case with rank-$1$ data): Three-dimensional constant direction $v = [1, -1, 0]^\\top$, scaling $s_t = \\alpha^t$ with $\\alpha = 1.02$ and $T = 80$. Compute two spectral radii: $\\rho_{4,\\text{raw}}$ without normalization and $\\rho_{4,\\text{norm}}$ with per-snapshot normalization. Expect $\\rho_{4,\\text{raw}} > 1$ and $\\rho_{4,\\text{norm}}$ close to $1$.\n\nAngle unit specification: All angles are in radians.\n\nAnswer specification:\n- For each case, produce a boolean indicating whether the expected behavior is observed within a small tolerance. Use tolerance $\\varepsilon = 0.05$ for testing closeness to $1$, and test “greater than $1$” by requiring a margin $\\delta = 0.01$.\n- Define the four results as:\n  - $b_1 = (|\\rho_1 - 1| < \\varepsilon)$\n  - $b_2 = (\\rho_2 > 1 + \\delta)$\n  - $b_3 = (|\\rho_3 - 1| < \\varepsilon)$\n  - $b_4 = [(\\rho_{4,\\text{raw}} > 1 + \\delta) \\wedge (|\\rho_{4,\\text{norm}} - 1| < \\varepsilon)]$\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[True,False,True,True]\").\n\nConstraints:\n- Implement the DMD eigenvalue computation using standard numerical linear algebra routines without relying on domain-specific black-box libraries.\n- Do not require any user input and do not access external files or networks. The program must run as is and print the required output line.",
            "solution": "The problem requires the implementation of the Dynamic Mode Decomposition (DMD) algorithm to analyze synthetic datasets. The goal is to demonstrate how DMD can mistake a global scaling trend for dynamic growth and how per-snapshot normalization rectifies this issue. The solution proceeds by first detailing the DMD algorithm, then applying it to the four specified cases.\n\nThe core principle of DMD is to model a time series of state vectors, or snapshots, $\\{x_0, x_1, \\dots, x_{T-1}\\}$ where each $x_t \\in \\mathbb{R}^n$, with a linear dynamical system of the form $x_{t+1} \\approx A x_t$. The DMD algorithm seeks to find the best-fit linear operator $A \\in \\mathbb{R}^{n \\times n}$ that advances the system state in time.\n\nFirst, we assemble the snapshot data into two matrices. Let $X$ be the full data matrix whose columns are the snapshots:\n$$\nX = [x_0, x_1, \\dots, x_{T-1}]\n$$\nWe then form two sub-matrices, $X_1$ and $X_2$, by splitting the data:\n$$\nX_1 = [x_0, x_1, \\dots, x_{T-2}] \\in \\mathbb{R}^{n \\times (T-1)}\n$$\n$$\nX_2 = [x_1, x_2, \\dots, x_{T-1}] \\in \\mathbb{R}^{n \\times (T-1)}\n$$\nThe relationship between these matrices is $X_2 \\approx A X_1$. The operator $A$ is found by solving the least-squares problem that minimizes the Frobenius norm of the residual, $\\|X_2 - A X_1\\|_F$. The solution is given by:\n$$\nA = X_2 X_1^+\n$$\nwhere $X_1^+$ is the Moore-Penrose pseudoinverse of $X_1$.\n\nFor large state dimensions $n$, forming the operator $A$ directly is computationally expensive and numerically unstable. The standard DMD algorithm avoids this by computing the spectral properties of $A$ via a low-rank approximation. This is achieved using the Singular Value Decomposition (SVD) of $X_1$. Let the reduced SVD of $X_1$ be:\n$$\nX_1 = U \\Sigma V^*\n$$\nwhere $U \\in \\mathbb{R}^{n \\times r}$, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is a diagonal matrix of singular values, and $V \\in \\mathbb{R}^{(T-1) \\times r}$ is a matrix of right singular vectors. Here, $r$ is the rank of $X_1$. The columns of $U$ form an orthonormal basis for the range of $X_1$. The pseudoinverse can be expressed as $X_1^+ = V \\Sigma^{-1} U^*$.\n\nSubstituting this into the expression for $A$ gives $A = X_2 V \\Sigma^{-1} U^*$. Instead of computing $A$, we analyze a smaller, rank-$r$ operator $\\tilde{A}$ that represents the projection of $A$ onto the basis $U$:\n$$\n\\tilde{A} = U^* A U = U^* (X_2 V \\Sigma^{-1} U^*) U = U^* X_2 V \\Sigma^{-1}\n$$\nThe matrix $\\tilde{A} \\in \\mathbb{R}^{r \\times r}$ has the same non-zero eigenvalues as $A$. These eigenvalues, the DMD eigenvalues, characterize the dynamics of the system. The spectral radius $\\rho$ is the maximum of the magnitudes of these eigenvalues, $\\rho = \\max_i(|\\lambda_i|)$. $|\\lambda_i| > 1$ signals growth, $|\\lambda_i| < 1$ signals decay, and $|\\lambda_i| = 1$ signals energy-preserving oscillation.\n\nFor certain cases, per-snapshot normalization is required. Before forming the matrices $X_1$ and $X_2$, each snapshot $x_t$ is scaled by its Euclidean norm, provided the norm is non-zero:\n$$\n\\widehat{x}_t = \\frac{x_t}{\\|x_t\\|_2}\n$$\nThis process removes variations in the overall magnitude of the state, isolating the underlying directional dynamics.\n\nThe problem defines four test cases to validate this understanding.\n\nCase $1$: The data consists of snapshots $x_t = [\\cos(\\omega t), \\sin(\\omega t)]^\\top$ for $T = 80$ snapshots and $\\omega = 0.37$. This represents a pure rotation. The underlying dynamics are energy-preserving, so the true operator is a rotation matrix whose eigenvalues lie on the unit circle. Therefore, the DMD spectral radius $\\rho_1$ is expected to be very close to $1$. The test is $|\\rho_1 - 1| < \\varepsilon$ with $\\varepsilon = 0.05$.\n\nCase $2$: The snapshots from Case $1$ are scaled by a growing factor $s_t = \\alpha^t$ with $\\alpha = 1.02$, so $x_t = \\alpha^t [\\cos(\\omega t), \\sin(\\omega t)]^\\top$. The relationship between consecutive snapshots is $x_{t+1} \\approx \\alpha R x_t$, where $R$ is the rotation operator from Case $1$. The DMD operator will capture both the rotation and the scaling factor $\\alpha$. Its eigenvalues will be approximately $\\alpha$ times the eigenvalues of $R$. The spectral radius $\\rho_2$ is thus expected to be approximately $\\alpha = 1.02$. The test is $\\rho_2 > 1 + \\delta$ with $\\delta = 0.01$.\n\nCase $3$: This case uses the same scaled data as Case $2$ but applies per-snapshot normalization before running DMD. The normalization of a snapshot $x_t$ is $\\widehat{x}_t = x_t / \\|x_t\\|_2 = (\\alpha^t [\\cos(\\omega t), \\sin(\\omega t)]^\\top) / \\|\\alpha^t [\\cos(\\omega t), \\sin(\\omega t)]^\\top\\|_2$. Since $\\|\\cdot\\|_2$ is a norm and $\\alpha^t > 0$, this simplifies to $\\widehat{x}_t = [\\cos(\\omega t), \\sin(\\omega t)]^\\top$, as the base vector has unit norm. Normalization completely removes the scaling trend, recovering the data from Case $1$. Consequently, the spectral radius $\\rho_3$ is expected to be close to $1$, just like $\\rho_1$. The test is $|\\rho_3 - 1| < \\varepsilon$.\n\nCase $4$: This is a degenerate case with a rank-$1$ data matrix. The snapshots are $x_t = s_t v = \\alpha^t [1, -1, 0]^\\top$.\nWithout normalization, the dynamics are exact: $x_{t+1} = \\alpha^{t+1} v = \\alpha (\\alpha^t v) = \\alpha x_t$. The system is perfectly linear with a single eigenvalue $\\alpha = 1.02$. The spectral radius $\\rho_{4,\\text{raw}}$ must therefore be approximately $1.02$. The test is $\\rho_{4,\\text{raw}} > 1 + \\delta$.\nWith normalization, each snapshot becomes $\\widehat{x}_t = x_t / \\|x_t\\|_2 = (\\alpha^t v) / (\\alpha^t \\|v\\|_2) = v / \\|v\\|_2$. All snapshots are identical constant vectors. Thus, $\\widehat{x}_{t+1} = \\widehat{x}_t$. The governing operator is the identity, with a single eigenvalue of $1$. The spectral radius $\\rho_{4,\\text{norm}}$ is expected to be $1$. The test is $|\\rho_{4,\\text{norm}} - 1| < \\varepsilon$.\nThe combined test for this case is $(\\rho_{4,\\text{raw}} > 1 + \\delta) \\wedge (|\\rho_{4,\\text{norm}} - 1| < \\varepsilon)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef dmd(X):\n    \"\"\"\n    Computes the Dynamic Mode Decomposition of a snapshot matrix X.\n    \n    Args:\n        X (np.ndarray): The snapshot matrix, where each column is a snapshot.\n        \n    Returns:\n        np.ndarray: The eigenvalues of the DMD operator.\n    \"\"\"\n    # 1. Split data into X1 and X2\n    X1 = X[:, :-1]\n    X2 = X[:, 1:]\n\n    # 2. Compute SVD of X1\n    # Use full_matrices=False for economy SVD\n    U, s, Vh = np.linalg.svd(X1, full_matrices=False)\n    \n    # Handle the case where singular values are close to zero to avoid division by zero.\n    # We can truncate, but for this problem, the ranks are well-defined.\n    # Let's ensure no division by a value smaller than machine epsilon occurs.\n    s_inv = np.zeros_like(s)\n    s_inv[s > 1e-15] = 1.0 / s[s > 1e-15]\n\n    # 3. Compute the low-rank operator Atilde\n    # Atilde = U.T @ A @ U = U.T @ (X2 @ V @ inv(Sigma) @ U.T) @ U\n    # Atilde = U.T @ X2 @ V @ inv(Sigma)\n    V = Vh.T\n    Atilde = U.T @ X2 @ V @ np.diag(s_inv)\n\n    # 4. Compute eigenvalues of Atilde\n    eigenvalues = np.linalg.eigvals(Atilde)\n    \n    return eigenvalues\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define problem parameters\n    T = 80\n    omega = 0.37\n    alpha = 1.02\n    v = np.array([1, -1, 0])\n\n    # Tolerances for checks\n    epsilon = 0.05\n    delta = 0.01\n\n    results = []\n    \n    t = np.arange(T)\n\n    # --- Case 1: Baseline, no scaling ---\n    X1_data = np.vstack([np.cos(omega * t), np.sin(omega * t)])\n    eigs1 = dmd(X1_data)\n    rho1 = np.max(np.abs(eigs1))\n    b1 = np.abs(rho1 - 1) < epsilon\n    results.append(b1)\n\n    # --- Case 2: Scale-induced trend ---\n    scaling_factor = alpha**t\n    X2_data = np.vstack([np.cos(omega * t), np.sin(omega * t)]) * scaling_factor\n    eigs2 = dmd(X2_data)\n    rho2 = np.max(np.abs(eigs2))\n    b2 = rho2 > (1 + delta)\n    results.append(b2)\n    \n    # --- Case 3: Normalized to remove scale trend ---\n    # Data is the same as Case 2\n    norms3 = np.linalg.norm(X2_data, axis=0)\n    # Avoid division by zero, although not expected here\n    non_zero_norms = norms3 > 1e-15\n    X3_data = np.zeros_like(X2_data)\n    X3_data[:, non_zero_norms] = X2_data[:, non_zero_norms] / norms3[non_zero_norms]\n    eigs3 = dmd(X3_data)\n    rho3 = np.max(np.abs(eigs3))\n    b3 = np.abs(rho3 - 1) < epsilon\n    results.append(b3)\n\n    # --- Case 4: Degenerate edge case with rank-1 data ---\n    # Reshape v to a column vector (3, 1) and scaling_factor to a row vector (1, T)\n    # The result is a (3, T) matrix\n    X4_data_raw = v[:, np.newaxis] @ scaling_factor[np.newaxis, :]\n    \n    # Compute for raw data\n    eigs4_raw = dmd(X4_data_raw)\n    rho4_raw = np.max(np.abs(eigs4_raw))\n    \n    # Compute for normalized data\n    norms4 = np.linalg.norm(X4_data_raw, axis=0)\n    non_zero_norms_4 = norms4 > 1e-15\n    X4_data_norm = np.zeros_like(X4_data_raw)\n    X4_data_norm[:, non_zero_norms_4] = X4_data_raw[:, non_zero_norms_4] / norms4[non_zero_norms_4]\n    \n    eigs4_norm = dmd(X4_data_norm)\n    rho4_norm = np.max(np.abs(eigs4_norm))\n    \n    b4 = (rho4_raw > (1 + delta)) and (np.abs(rho4_norm - 1) < epsilon)\n    results.append(b4)\n    \n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While standard DMD excels at analyzing autonomous systems, many engineering applications involve systems with external actuation or control inputs. The framework can be extended to these scenarios through Dynamic Mode Decomposition with Control (DMDc), which identifies a linear state-space model incorporating both intrinsic dynamics and control effects. This advanced problem guides you through the derivation of the DMDc estimators and connects the method to the classical Eigensystem Realization Algorithm (ERA), illustrating the power of DMDc for system identification .",
            "id": "3987561",
            "problem": "In a reduced-order model of an actuated flow in aerospace Computational Fluid Dynamics (CFD), suppose a single dominant Proper Orthogonal Decomposition (POD) coordinate is used as the state. The evolution of this scalar reduced state is modeled as a discrete-time, linear, time-invariant, control-affine system,\n$$\nx_{k+1} = A x_k + B u_k,\n$$\nwhere $x_k \\in \\mathbb{R}$ is the scalar state at discrete time index $k$, $u_k \\in \\mathbb{R}$ is the scalar control input, and $A, B \\in \\mathbb{R}$ are unknown constants. You observe the state and input snapshots at uniform sampling as follows:\n$$\nx_0 = 0,\\quad u_0 = 1,\\quad x_1 = 1, \\\\\nx_1 = 1,\\quad u_1 = 2,\\quad x_2 = \\frac{5}{2}, \\\\\nx_2 = \\frac{5}{2},\\quad u_2 = 0,\\quad x_3 = \\frac{5}{4}.\n$$\nStarting from first principles of least squares for linear regression, derive the Dynamic Mode Decomposition with control (DMDc) estimators for $A$ and $B$ using the pseudoinverse-based normal equations on concatenated snapshot matrices. Then, independently, consider input-output identification via a short impulse-response experiment under the Eigensystem Realization Algorithm (ERA). For a single-input single-output, first-order, stable system with strictly proper dynamics, the impulse-response Markov parameters are given by\n$$\nh_1 = 1,\\quad h_2 = \\frac{1}{2}.\n$$\nUse a minimal Hankel construction consistent with a first-order model to derive the ERA estimate of $A$ from these Markov parameters.\n\nFinally, compute the absolute difference between the DMDc estimate of $A$ obtained from the provided state-input snapshots and the ERA estimate of $A$ obtained from the Markov parameters. Express your final answer as a real number.",
            "solution": "The problem requires computing the system parameter $A$ using two methods: Dynamic Mode Decomposition with control (DMDc) and the Eigensystem Realization Algorithm (ERA), then finding the absolute difference between the estimates.\n\n### DMDc Estimation of $A$\nThe governing equation is $x_{k+1} = A x_k + B u_k$. For a sequence of snapshots, this can be written as $X' \\approx G \\Omega$, where $G = \\begin{pmatrix} A & B \\end{pmatrix}$ and $\\Omega$ concatenates the state and input data. The least-squares solution for $G$ is $G = X' \\Omega^+$, where $\\Omega^+$ is the Moore-Penrose pseudoinverse.\n\nFrom the provided data, we construct the snapshot matrices:\n$$\nX' = \\begin{pmatrix} x_1 & x_2 & x_3 \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{5}{2} & \\frac{5}{4} \\end{pmatrix}\n$$\n$$\n\\Omega = \\begin{pmatrix} x_0 & x_1 & x_2 \\\\ u_0 & u_1 & u_2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & \\frac{5}{2} \\\\ 1 & 2 & 0 \\end{pmatrix}\n$$\nFor a 'fat' matrix $\\Omega$, the pseudoinverse is $\\Omega^+ = \\Omega^T (\\Omega \\Omega^T)^{-1}$.\nFirst, we compute $\\Omega \\Omega^T$:\n$$\n\\Omega \\Omega^T = \\begin{pmatrix} 0 & 1 & \\frac{5}{2} \\\\ 1 & 2 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 2 \\\\ \\frac{5}{2} & 0 \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{25}{4} & 2 \\\\ 2 & 1 + 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{29}{4} & 2 \\\\ 2 & 5 \\end{pmatrix}\n$$\nThe inverse is $(\\Omega \\Omega^T)^{-1} = \\frac{1}{\\det(\\Omega \\Omega^T)} \\begin{pmatrix} 5 & -2 \\\\ -2 & \\frac{29}{4} \\end{pmatrix}$, where the determinant is $(\\frac{29}{4})(5) - (2)(2) = \\frac{129}{4}$.\n$$\n(\\Omega \\Omega^T)^{-1} = \\frac{4}{129} \\begin{pmatrix} 5 & -2 \\\\ -2 & \\frac{29}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{20}{129} & -\\frac{8}{129} \\\\ -\\frac{8}{129} & \\frac{29}{129} \\end{pmatrix}\n$$\nThe estimator matrix is $G = X' \\Omega^+ = X' \\Omega^T (\\Omega \\Omega^T)^{-1}$. We compute the term $X' \\Omega^T$:\n$$\nX' \\Omega^T = \\begin{pmatrix} 1 & \\frac{5}{2} & \\frac{5}{4} \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 2 \\\\ \\frac{5}{2} & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{2} + \\frac{25}{8} & 1 + 5 \\end{pmatrix} = \\begin{pmatrix} \\frac{45}{8} & 6 \\end{pmatrix}\n$$\nNow, we find $G$:\n$$\nG = \\begin{pmatrix} A & B \\end{pmatrix} = \\begin{pmatrix} \\frac{45}{8} & 6 \\end{pmatrix} \\begin{pmatrix} \\frac{20}{129} & -\\frac{8}{129} \\\\ -\\frac{8}{129} & \\frac{29}{129} \\end{pmatrix}\n$$\nThe estimate for $A$ is the first component of $G$:\n$$\nA_{\\text{DMDc}} = \\left(\\frac{45}{8}\\right) \\left(\\frac{20}{129}\\right) + (6) \\left(-\\frac{8}{129}\\right) = \\frac{900}{1032} - \\frac{48}{129} = \\frac{900 - 384}{1032} = \\frac{516}{1032} = \\frac{1}{2}\n$$\n\n### ERA Estimation of $A$\nThe Eigensystem Realization Algorithm (ERA) uses the system's impulse response, defined by the sequence of Markov parameters $h_k = C A^{k-1} B$ (for $k \\ge 1$ and a strictly proper system). For a first-order SISO system, $A, B, C$ are scalars.\nThe given Markov parameters are:\n$$\nh_1 = C B = 1\n$$\n$$\nh_2 = C A B = \\frac{1}{2}\n$$\nAssuming a minimal realization ($C \\neq 0, B \\neq 0$), we can find $A$ by taking the ratio of $h_2$ to $h_1$:\n$$\nA_{\\text{ERA}} = \\frac{h_2}{h_1} = \\frac{CAB}{CB} = \\frac{1/2}{1} = \\frac{1}{2}\n$$\n\n### Final Calculation\nThe absolute difference between the two estimates of $A$ is:\n$$\n|A_{\\text{DMDc}} - A_{\\text{ERA}}| = \\left|\\frac{1}{2} - \\frac{1}{2}\\right| = 0\n$$",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}