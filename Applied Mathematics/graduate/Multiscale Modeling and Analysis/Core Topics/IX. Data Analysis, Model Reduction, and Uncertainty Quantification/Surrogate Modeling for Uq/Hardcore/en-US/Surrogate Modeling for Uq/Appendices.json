{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a foundational walkthrough of the entire Polynomial Chaos Expansion (PCE) workflow. For a simple and common case where the input follows a $\\mathrm{Uniform}(-1,1)$ distribution, you will construct the appropriate orthonormal basis using Legendre polynomials. This practice will guide you through calculating the expansion coefficients for a given model and then demonstrate the power of PCE by showing how to estimate statistical moments, such as mean and variance, directly from these coefficients .",
            "id": "3812034",
            "problem": "You are given a univariate random input $X$ distributed as $\\mathrm{Uniform}(-1,1)$ with density $p(x) = \\tfrac{1}{2}$ on $[-1,1]$. Consider the classical Legendre polynomials $\\{P_n(x)\\}_{n \\ge 0}$, which are orthogonal on $[-1,1]$ with respect to the weight $w(x) = 1$, i.e., the well-tested fact $\\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \\tfrac{2}{2n+1} \\delta_{nm}$. In uncertainty quantification, a polynomial chaos expansion uses an orthonormal basis with respect to the probability measure. For the $\\mathrm{Uniform}(-1,1)$ measure, define the normalized Legendre polynomials $\\{\\psi_n(x)\\}_{n=0}^{4}$ by scaling $\\{P_n(x)\\}$ so that $\\mathbb{E}[\\psi_n(X)\\psi_m(X)] = \\delta_{nm}$, where the expectation is with respect to the law of $X$. Angles in all trigonometric functions below are in radians.\n\nUsing only these foundational definitions and facts:\n- Construct the normalized Legendre polynomials $\\psi_n(x)$ for $n \\in \\{0,1,2,3,4\\}$ starting from $P_0(x)=1$, $P_1(x)=x$, $P_2(x)=\\tfrac{1}{2}(3x^2-1)$, $P_3(x)=\\tfrac{1}{2}(5x^3-3x)$, $P_4(x)=\\tfrac{1}{8}(35x^4-30x^2+3)$.\n- For each smooth function $f$, compute the truncated polynomial chaos expansion coefficients $\\{c_n\\}_{n=0}^{4}$ obtained by the $L^2$ projection of $f$ onto $\\{\\psi_n\\}_{n=0}^{4}$ with respect to the $\\mathrm{Uniform}(-1,1)$ probability measure.\n- Use these coefficients to estimate the mean and variance of $f(X)$ under the $\\mathrm{Uniform}(-1,1)$ measure by exploiting orthonormality.\n- Numerically approximate any required expectation using Gauss–Legendre quadrature on $[-1,1]$ with at least $N \\ge 200$ nodes. Ensure the probability density $p(x) = \\tfrac{1}{2}$ is correctly incorporated when computing expectations.\n\nTest suite (angles in $\\sin(\\cdot)$ use radians):\n- Case $1$: $f_1(x) = \\exp(0.3\\,x) + x^2$.\n- Case $2$: $f_2(x) = \\sin(\\pi x) + 0.5\\,x^3$.\n- Case $3$: $f_3(x) = 3\\,x^4 - 2\\,x^2 + \\tfrac{1}{5}$.\n- Case $4$: $f_4(x) = x$.\n- Case $5$: $f_5(x) = -\\tfrac{3}{4}$.\n\nFor each case, compute:\n- The estimated mean $\\widehat{\\mu} = \\mathbb{E}[f(X)]$ using the truncated expansion coefficients obtained by projection onto $\\{\\psi_n\\}_{n=0}^{4}$.\n- The estimated variance $\\widehat{\\sigma^2} = \\mathrm{Var}[f(X)]$ using only the non-constant polynomial chaos expansion modes up to degree $4$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the $5$ cases as a comma-separated list of $5$ two-element lists, where each inner list contains the estimated mean and variance $[\\widehat{\\mu},\\widehat{\\sigma^2}]$ for that case.\n- Each floating-point number must be rounded to exactly $10$ decimal places and printed in fixed-point decimal notation.\n- For example, the output should look like $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4],[a_5,b_5]]$ where each $a_i$ and $b_i$ are decimals with exactly $10$ digits after the decimal point.\n\nYour program must not read any input and must use $N \\ge 200$ quadrature nodes. The only required output is this single line. All answers are dimensionless real numbers. Angles are in radians. The final answers for the test suite must be a list of lists of floats as specified.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of orthogonal polynomials and uncertainty quantification, is well-posed with a unique and stable solution, and is stated objectively using precise mathematical language. All necessary information is provided, and there are no contradictions. We proceed with the solution.\n\nThe core of this problem lies in constructing a surrogate model for a function $f(X)$ of a random variable $X$ using a Polynomial Chaos Expansion (PCE). Given that the input random variable $X$ follows a uniform distribution on $[-1, 1]$, i.e., $X \\sim \\mathrm{Uniform}(-1,1)$, the appropriate orthogonal polynomials for the expansion are the Legendre polynomials.\n\n**1. Orthonormal Basis Construction**\n\nThe random variable $X$ has a probability density function (PDF) $p(x) = \\frac{1}{2}$ for $x \\in [-1, 1]$ and $p(x) = 0$ otherwise. The expectation of a function $g(X)$ is thus given by the inner product with respect to the probability measure $p(x)dx$:\n$$ \\mathbb{E}[g(X)] = \\int_{-1}^{1} g(x) p(x) \\, dx = \\frac{1}{2} \\int_{-1}^{1} g(x) \\, dx $$\nThe problem requires an orthonormal polynomial basis $\\{\\psi_n(x)\\}_{n \\ge 0}$ such that $\\mathbb{E}[\\psi_n(X)\\psi_m(X)] = \\delta_{nm}$, where $\\delta_{nm}$ is the Kronecker delta.\n\nWe are given the classical Legendre polynomials $\\{P_n(x)\\}$, which are orthogonal on $[-1, 1]$ with respect to the weight function $w(x) = 1$. Their orthogonality relation is:\n$$ \\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \\frac{2}{2n+1} \\delta_{nm} $$\nTo find the corresponding orthonormal basis for our probability space, we first compute the squared norm of $P_n(x)$ with respect to the expectation operator:\n$$ \\mathbb{E}[P_n(X)P_m(X)] = \\frac{1}{2} \\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \\frac{1}{2} \\left( \\frac{2}{2n+1} \\delta_{nm} \\right) = \\frac{1}{2n+1} \\delta_{nm} $$\nThis shows that the squared norm of $P_n(x)$ is $\\mathbb{E}[P_n(X)^2] = \\frac{1}{2n+1}$. To create an orthonormal basis, we must normalize each $P_n(x)$ by its norm, which is $\\sqrt{\\mathbb{E}[P_n(X)^2]} = \\frac{1}{\\sqrt{2n+1}}$.\nThus, the orthonormal Legendre polynomials are defined as:\n$$ \\psi_n(x) = \\frac{P_n(x)}{\\sqrt{\\mathbb{E}[P_n(X)^2]}} = \\sqrt{2n+1} \\, P_n(x) $$\nUsing the provided expressions for $P_n(x)$ for $n \\in \\{0, 1, 2, 3, 4\\}$, we construct the required basis:\n- $\\psi_0(x) = \\sqrt{1} P_0(x) = 1$\n- $\\psi_1(x) = \\sqrt{3} P_1(x) = \\sqrt{3} x$\n- $\\psi_2(x) = \\sqrt{5} P_2(x) = \\frac{\\sqrt{5}}{2}(3x^2 - 1)$\n- $\\psi_3(x) = \\sqrt{7} P_3(x) = \\frac{\\sqrt{7}}{2}(5x^3 - 3x)$\n- $\\psi_4(x) = \\sqrt{9} P_4(x) = \\frac{3}{8}(35x^4 - 30x^2 + 3)$\n\n**2. Polynomial Chaos Expansion and Coefficient Calculation**\n\nA function $f(x)$ with finite variance can be represented by its PCE, $f(X) = \\sum_{n=0}^{\\infty} c_n \\psi_n(X)$. We are asked to compute the coefficients for a truncated expansion of order $4$:\n$$ \\hat{f}(X) = \\sum_{n=0}^{4} c_n \\psi_n(X) $$\nThe coefficients $c_n$ are determined by projecting the function $f$ onto the basis functions $\\psi_n$. Exploiting the orthonormality of the basis, the coefficients are given by the inner product:\n$$ c_n = \\mathbb{E}[f(X) \\psi_n(X)] = \\frac{1}{2} \\int_{-1}^{1} f(x) \\psi_n(x) \\, dx $$\nFor non-polynomial functions, this integral is computed numerically. We employ Gauss-Legendre quadrature, which is highly accurate for smooth integrands on $[-1, 1]$. The quadrature rule approximates the integral as:\n$$ \\int_{-1}^{1} g(x) \\, dx \\approx \\sum_{i=1}^{N} w_i g(x_i) $$\nwhere $x_i$ are the quadrature nodes (roots of $P_N(x)$) and $w_i$ are the corresponding weights. The problem specifies $N \\ge 200$. Substituting this into the formula for the coefficients yields the numerical approximation:\n$$ c_n \\approx \\frac{1}{2} \\sum_{i=1}^{N} w_i f(x_i) \\psi_n(x_i) $$\nThis calculation is performed for each $n \\in \\{0, 1, 2, 3, 4\\}$ and for each test function $f_k(x)$.\n\n**3. Mean and Variance Estimation**\n\nThe primary benefit of the PCE is the ease with which statistical moments can be computed from the coefficients.\nThe estimated mean, $\\widehat{\\mu}$, is the expectation of the truncated expansion:\n$$ \\widehat{\\mu} = \\mathbb{E}[\\hat{f}(X)] = \\mathbb{E}\\left[\\sum_{n=0}^{4} c_n \\psi_n(X)\\right] = \\sum_{n=0}^{4} c_n \\mathbb{E}[\\psi_n(X)] $$\nThe expectation of the basis functions is $\\mathbb{E}[\\psi_n(X)] = \\mathbb{E}[\\psi_n(X)\\psi_0(X)] = \\delta_{n0}$, since $\\psi_0(x) = 1$. Therefore, only the $n=0$ term survives:\n$$ \\widehat{\\mu} = c_0 \\mathbb{E}[\\psi_0(X)] = c_0 $$\nThe estimated variance, $\\widehat{\\sigma^2}$, is the variance of the truncated expansion. Using the property $\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$:\n$$ \\widehat{\\sigma^2} = \\mathrm{Var}[\\hat{f}(X)] = \\mathbb{E}\\left[\\left(\\sum_{n=0}^{4} c_n \\psi_n(X)\\right)^2\\right] - (c_0)^2 $$\nExpanding the squared sum and using the orthonormality property $\\mathbb{E}[\\psi_n(X)\\psi_m(X)] = \\delta_{nm}$:\n$$ \\mathbb{E}\\left[\\left(\\sum_{n=0}^{4} c_n \\psi_n(X)\\right)^2\\right] = \\mathbb{E}\\left[\\sum_{n=0}^{4}\\sum_{m=0}^{4} c_n c_m \\psi_n(X)\\psi_m(X)\\right] = \\sum_{n=0}^{4}\\sum_{m=0}^{4} c_n c_m \\delta_{nm} = \\sum_{n=0}^{4} c_n^2 $$\nThe variance is then:\n$$ \\widehat{\\sigma^2} = \\left(\\sum_{n=0}^{4} c_n^2\\right) - c_0^2 = \\sum_{n=1}^{4} c_n^2 $$\nThis confirms that the variance is exclusively determined by the coefficients of the non-constant basis polynomials, as stated in the problem.\n\nThis completes the theoretical framework. The implementation will involve defining the basis functions $\\psi_n(x)$, obtaining Gauss-Legendre quadrature nodes and weights, and applying the derived formulas for $c_n$, $\\widehat{\\mu}$, and $\\widehat{\\sigma^2}$ to each specified test function.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mean and variance of several functions of a U(-1,1) random variable\n    using a 4th-order Polynomial Chaos Expansion based on Legendre polynomials.\n    \"\"\"\n\n    # 1. Define constants and quadrature setup\n    # The number of quadrature points must be >= 200.\n    N_QUAD = 200\n    # Gauss-Legendre quadrature nodes and weights for the interval [-1, 1]\n    x_quad, w_quad = np.polynomial.legendre.leggauss(N_QUAD)\n\n    # 2. Define the orthonormal basis polynomials psi_n(x)\n    # First, define the classical Legendre polynomials P_n(x)\n    P = [\n        lambda x: np.ones_like(x),\n        lambda x: x,\n        lambda x: 0.5 * (3 * x**2 - 1),\n        lambda x: 0.5 * (5 * x**3 - 3 * x),\n        lambda x: 0.125 * (35 * x**4 - 30 * x**2 + 3)\n    ]\n    # Then, define the orthonormal polynomials psi_n(x) = sqrt(2n+1) * P_n(x)\n    psi = [\n        lambda x, n=n: np.sqrt(2 * n + 1) * P[n](x) for n in range(5)\n    ]\n\n    # 3. Define the test case functions f_i(x)\n    test_functions = [\n        lambda x: np.exp(0.3 * x) + x**2,\n        lambda x: np.sin(np.pi * x) + 0.5 * x**3,\n        lambda x: 3 * x**4 - 2 * x**2 + 1/5,\n        lambda x: x,\n        lambda x: -0.75 * np.ones_like(x)\n    ]\n\n    # List to store the [mean, variance] pairs for each case\n    results = []\n\n    # 4. Loop through each test case\n    for f in test_functions:\n        # Evaluate the function at the quadrature nodes\n        f_vals = f(x_quad)\n        \n        # Calculate the PCE coefficients c_n\n        coeffs = np.zeros(5)\n        for n in range(5):\n            # The integrand for c_n is f(x) * psi_n(x)\n            integrand_vals = f_vals * psi[n](x_quad)\n            \n            # The integral is approximated by the quadrature sum\n            integral_val = np.sum(w_quad * integrand_vals)\n            \n            # The coefficient is c_n = E[f(X)psi_n(X)] = 0.5 * integral\n            c_n = 0.5 * integral_val\n            coeffs[n] = c_n\n            \n        # 5. Estimate mean and variance from coefficients\n        # Mean_hat = c_0\n        mean_hat = coeffs[0]\n        # Variance_hat = sum of squares of coefficients from n=1 to 4\n        var_hat = np.sum(coeffs[1:]**2)\n        \n        results.append([mean_hat, var_hat])\n\n    # 6. Format and print the final output string\n    # Each number is formatted to 10 decimal places.\n    # The output is a string representation of a list of lists.\n    formatted_results = []\n    for res_pair in results:\n        formatted_pair = f\"[{res_pair[0]:.10f},{res_pair[1]:.10f}]\"\n        formatted_results.append(formatted_pair)\n\n    output_string = f\"[{','.join(formatted_results)}]\"\n    print(output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from a single random variable to a high-dimensional input space introduces significant computational challenges. This practice explores how the number of basis functions in a PCE can grow explosively, a phenomenon known as the \"curse of dimensionality.\" By comparing the standard total-degree truncation with a more sophisticated hyperbolic truncation scheme , you will gain a quantitative understanding of essential methods used to keep surrogate models computationally tractable in problems with many uncertain parameters.",
            "id": "3812020",
            "problem": "Consider a multiscale model in which the mesoscale response is approximated by a surrogate built using Polynomial Chaos Expansion (PCE) to propagate uncertainties across scales, and Gaussian Process Regression (GPR; Kriging) to calibrate mesoscale closures from microscale simulations. Let the input random vector be $d=8$-dimensional with independent standard normal components so that the multivariate basis is formed by tensor-product Hermite polynomials. Define the multi-index set for the PCE basis as a subset of $\\mathbb{N}_{0}^{d}$.\n\nTwo truncation strategies are considered for an isotropic basis with maximal polynomial order $p=3$:\n- Total-degree truncation: include all multi-indices $\\boldsymbol{\\alpha} \\in \\mathbb{N}_{0}^{d}$ such that $\\sum_{i=1}^{d} \\alpha_{i} \\leq p$.\n- Hyperbolic truncation with parameter $q=0.5$: include all multi-indices $\\boldsymbol{\\alpha} \\in \\mathbb{N}_{0}^{d}$ such that $\\sum_{i=1}^{d} \\alpha_{i}^{q} \\leq p^{q}$.\n\nUsing these definitions, compute the cardinality of the basis under each truncation strategy for $d=8$ and $p=3$. Then, quantify the sparsity gain due to hyperbolic truncation as the fractional reduction in basis size, defined by\n$$\nS \\equiv 1 - \\frac{N_{\\mathrm{hyper}}}{N_{\\mathrm{total}}},\n$$\nwhere $N_{\\mathrm{total}}$ is the total-degree basis size and $N_{\\mathrm{hyper}}$ is the hyperbolic-truncated basis size. Express your final answer for $S$ as a reduced fraction. No rounding is required.",
            "solution": "The problem requires the calculation of the basis cardinalities for two different truncation strategies of a Polynomial Chaos Expansion (PCE), and subsequently, the computation of the sparsity gain. The input random vector is $d=8$-dimensional and the maximal polynomial order is $p=3$.\n\nFirst, we calculate the cardinality of the basis for the total-degree truncation strategy, denoted as $N_{\\mathrm{total}}$. The multi-index set for this strategy is defined as all $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_d) \\in \\mathbb{N}_{0}^{d}$ such that the sum of its components is at most $p$:\n$$\n\\sum_{i=1}^{d} \\alpha_{i} \\leq p\n$$\nFor the given values $d=8$ and $p=3$, the condition is $\\sum_{i=1}^{8} \\alpha_{i} \\leq 3$. The number of such multi-indices, $N_{\\mathrm{total}}$, is a classic combinatorial problem. The number of non-negative integer solutions to this inequality is given by the formula for combinations with repetition, often derived using a \"stars and bars\" argument. This count is equivalent to the number of ways to choose $p$ items from a set of $d$ with replacement, summarized by the multiset coefficient, which can be written as a binomial coefficient:\n$$\nN_{\\mathrm{total}} = \\binom{p+d}{d} = \\binom{p+d}{p}\n$$\nSubstituting the given values $p=3$ and $d=8$:\n$$\nN_{\\mathrm{total}} = \\binom{3+8}{8} = \\binom{11}{8} = \\binom{11}{11-8} = \\binom{11}{3}\n$$\nThe value of this binomial coefficient is:\n$$\nN_{\\mathrm{total}} = \\frac{11!}{3!(11-3)!} = \\frac{11!}{3!8!} = \\frac{11 \\times 10 \\times 9}{3 \\times 2 \\times 1} = 11 \\times 5 \\times 3 = 165\n$$\nSo, the total-degree truncated basis contains $165$ polynomial terms.\n\nNext, we calculate the cardinality of the basis for the hyperbolic truncation strategy, denoted as $N_{\\mathrm{hyper}}$. The multi-index set is defined by the condition:\n$$\n\\sum_{i=1}^{d} \\alpha_{i}^{q} \\leq p^{q}\n$$\nWith the given parameters $d=8$, $p=3$, and $q=0.5$, the condition becomes:\n$$\n\\sum_{i=1}^{8} \\alpha_{i}^{0.5} \\leq 3^{0.5} \\implies \\sum_{i=1}^{8} \\sqrt{\\alpha_{i}} \\leq \\sqrt{3}\n$$\nThe components $\\alpha_i$ of the multi-index $\\boldsymbol{\\alpha}$ are non-negative integers, i.e., $\\alpha_i \\in \\{0, 1, 2, 3, \\dots\\}$. The value of $\\sqrt{3}$ is approximately $1.732$. We must enumerate all multi-indices $\\boldsymbol{\\alpha} \\in \\mathbb{N}_{0}^{8}$ that satisfy this inequality.\n\nLet's analyze the possible values for the components $\\alpha_i$:\nThe smallest possible non-zero value for any $\\alpha_i$ is $1$, for which $\\sqrt{\\alpha_i}=1$.\nIf a multi-index $\\boldsymbol{\\alpha}$ had two or more non-zero components, the minimal sum would be for $\\alpha_i=1$ and $\\alpha_j=1$ (with $i \\neq j$), resulting in a sum $\\sqrt{1} + \\sqrt{1} = 2$. Since $2 > \\sqrt{3}$, no multi-index with two or more non-zero components can satisfy the condition. Therefore, the valid multi-indices can have at most one non-zero component.\n\nWe can categorize the valid multi-indices as follows:\n1.  The zero vector: $\\boldsymbol{\\alpha} = (0, 0, 0, 0, 0, 0, 0, 0)$. The sum is $\\sum_{i=1}^{8} \\sqrt{0} = 0$, which is less than or equal to $\\sqrt{3}$. This gives $1$ multi-index. This corresponds to the constant basis function.\n\n2.  Vectors with exactly one non-zero component: Let $\\alpha_k$ be the only non-zero component, where $k \\in \\{1, \\dots, 8\\}$. The condition simplifies to $\\sqrt{\\alpha_k} \\leq \\sqrt{3}$. Squaring both sides gives $\\alpha_k \\leq 3$. Since $\\alpha_k$ must be a positive integer, the possible values for $\\alpha_k$ are $1$, $2$, and $3$.\n    - If $\\alpha_k=1$: $\\sqrt{1} = 1 \\leq \\sqrt{3}$. This is valid. The non-zero entry can be in any of the $d=8$ positions, so there are $\\binom{8}{1}=8$ such multi-indices.\n    - If $\\alpha_k=2$: $\\sqrt{2} \\approx 1.414 \\leq \\sqrt{3}$. This is valid. There are $\\binom{8}{1}=8$ such multi-indices.\n    - If $\\alpha_k=3$: $\\sqrt{3} \\leq \\sqrt{3}$. This is valid. There are $\\binom{8}{1}=8$ such multi-indices.\n    - For any $\\alpha_k \\ge 4$, $\\sqrt{\\alpha_k} \\ge 2 > \\sqrt{3}$, so these are not allowed.\n\nThe total number of multi-indices for the hyperbolic truncation is the sum of the counts from these cases:\n$$\nN_{\\mathrm{hyper}} = 1 + 8 + 8 + 8 = 25\n$$\nSo, the hyperbolic-truncated basis contains $25$ polynomial terms. This basis consists only of the constant term and univariate polynomials up to degree $3$ in each variable, with no mixed-interaction terms.\n\nFinally, we compute the sparsity gain $S$, defined as the fractional reduction in basis size:\n$$\nS = 1 - \\frac{N_{\\mathrm{hyper}}}{N_{\\mathrm{total}}}\n$$\nSubstituting the calculated values for $N_{\\mathrm{total}}$ and $N_{\\mathrm{hyper}}$:\n$$\nS = 1 - \\frac{25}{165}\n$$\nTo simplify the fraction, we find the greatest common divisor of the numerator and denominator. Both $25$ and $165$ are divisible by $5$:\n$$\n\\frac{25}{165} = \\frac{5 \\times 5}{33 \\times 5} = \\frac{5}{33}\n$$\nNow, we can calculate $S$:\n$$\nS = 1 - \\frac{5}{33} = \\frac{33}{33} - \\frac{5}{33} = \\frac{33-5}{33} = \\frac{28}{33}\n$$\nThe fraction $\\frac{28}{33}$ is in reduced form, as the prime factors of $28$ are $2^2$ and $7$, and the prime factors of $33$ are $3$ and $11$. They share no common factors.",
            "answer": "$$ \\boxed{\\frac{28}{33}} $$"
        },
        {
            "introduction": "This final practice addresses a critical question for real-world applications: what if the input variables do not follow a standard probability distribution for which canonical orthogonal polynomials are known? This exercise demonstrates how to construct a custom orthonormal polynomial basis from the ground up, using only the statistical moments of an arbitrary input distribution via the Gram–Schmidt procedure . You will also confront the inherent numerical stability challenges of this powerful method, learning to use essential diagnostic tools to assess the quality and reliability of the resulting basis.",
            "id": "3811933",
            "problem": "You are given a scalar random input $X$ whose distribution is nonstandard but whose raw moments are known. In surrogate modeling for Uncertainty Quantification (UQ), particularly in Polynomial Chaos Expansion (PCE), one constructs orthonormal polynomial bases with respect to the inner product induced by the input distribution in order to represent model responses efficiently across scales. The task is to derive and implement the Gram–Schmidt construction of an orthonormal polynomial basis $\\{p_0(x), p_1(x), \\dots, p_n(x)\\}$ with respect to the measure of $X$, using only the raw moments of $X$.\n\nStart from the following fundamental bases and definitions:\n1. The space of polynomials up to degree $n$ can be spanned by monomials $\\{1, x, x^2, \\dots, x^n\\}$.\n2. Define the inner product $\\langle f, g \\rangle$ by $\\langle f, g \\rangle = \\mathbb{E}[f(X) g(X)]$.\n3. The raw moments of $X$ are $\\mu_k = \\mathbb{E}[X^k]$ for integers $k \\ge 0$.\n4. If $f(x) = \\sum_{i=0}^n a_i x^i$ and $g(x) = \\sum_{j=0}^n b_j x^j$, then by linearity of expectation, $\\langle f, g \\rangle = \\sum_{i=0}^n \\sum_{j=0}^n a_i b_j \\mu_{i+j}$.\n5. The Gram–Schmidt process orthonormalizes a basis $\\{v_0, v_1, \\dots, v_n\\}$ under an inner product $\\langle \\cdot, \\cdot \\rangle$ by iteratively subtracting projections and normalizing: for $j = 0, 1, \\dots, n$, set $w_j = v_j - \\sum_{k=0}^{j-1} \\langle v_j, u_k \\rangle u_k$ and $u_j = w_j / \\sqrt{\\langle w_j, w_j \\rangle}$, where $\\{u_k\\}$ are the orthonormalized vectors.\n\nYour program must:\n1. Implement a robust Gram–Schmidt orthonormalization of the monomial basis $\\{1, x, x^2, \\dots, x^n\\}$ with respect to $\\langle \\cdot, \\cdot \\rangle$ defined above, using only $\\{\\mu_k\\}_{k=0}^{2n}$. To mitigate numerical issues, apply a single reorthogonalization pass and scale each initial monomial $x^j$ by $1/\\sqrt{\\mu_{2j}}$ when $\\mu_{2j} > 0$ (leave it unscaled if $\\mu_{2j} = 0$), before orthonormalization.\n2. Construct the Hankel moment matrix $H \\in \\mathbb{R}^{(n+1) \\times (n+1)}$ with entries $H_{ij} = \\mu_{i+j}$ for $0 \\le i, j \\le n$, and compute its $2$-norm condition number $\\kappa_2(H)$ via singular values.\n3. Compute the following orthonormality diagnostics for the constructed basis polynomials $\\{p_0, \\dots, p_m\\}$ where $m \\le n$ is the largest degree for which the process successfully produces a nondegenerate polynomial (i.e., $\\langle w_j, w_j \\rangle$ strictly positive within numerical tolerance):\n   a. The maximum absolute off-diagonal inner product $r_{\\text{off}} = \\max_{0 \\le i < j \\le m} |\\langle p_i, p_j \\rangle|$.\n   b. The maximum deviation of the norms from $1$, $r_{\\text{norm}} = \\max_{0 \\le i \\le m} |\\langle p_i, p_i \\rangle - 1|$.\n4. Define a boolean stability indicator $s$ that is true if and only if $r_{\\text{off}} \\le 10^{-8}$ and $r_{\\text{norm}} \\le 10^{-10}$ and $\\kappa_2(H) < 10^{12}$, and false otherwise.\n\nNo physical units are involved, and no angles are used. All outputs must be unitless real numbers or booleans.\n\nTest suite specification (provide coverage across scenarios):\n- Test case $1$ (general smooth case): $X \\sim \\text{Beta}(\\alpha, \\beta)$ on $[0,1]$ with $\\alpha = 2.3$ and $\\beta = 1.7$, degree $n = 3$. The raw moments satisfy $\\, \\mu_0 = 1 \\,$ and for $k \\ge 1$, $\\, \\mu_k = \\frac{(\\alpha)_k}{(\\alpha+\\beta)_k} \\,$, where $(a)_k$ is the rising Pochhammer symbol $(a)_k = a(a+1)\\cdots(a+k-1)$. Use $\\{\\mu_k\\}_{k=0}^{6}$.\n- Test case $2$ (near-degenerate discrete case): $X \\sim \\text{Bernoulli}(p)$ with $p = 0.2$, degree $n = 3$. The raw moments are $\\, \\mu_0 = 1 \\,$ and $\\, \\mu_k = p \\,$ for $k \\ge 1$. Use $\\{\\mu_k\\}_{k=0}^{6}$.\n- Test case $3$ (heavy-tailed moment growth): $X \\sim \\text{Lognormal}(\\mu, \\sigma^2)$ with $\\mu = 0$ and $\\sigma = 0.75$, degree $n = 3$. The raw moments are $\\, \\mu_k = \\exp\\!\\left(k \\mu + \\tfrac{1}{2} k^2 \\sigma^2 \\right) = \\exp\\!\\left( \\tfrac{1}{2} k^2 \\sigma^2 \\right) \\,$. Use $\\{\\mu_k\\}_{k=0}^{6}$.\n\nFinal output specification:\n- For each test case, produce a list $[r_{\\text{off}}, r_{\\text{norm}}, \\kappa_2(H), s]$ where $r_{\\text{off}}$ and $r_{\\text{norm}}$ and $\\kappa_2(H)$ are floats rounded to six decimal places, and $s$ is a boolean.\n- Your program should produce a single line of output containing the three per-case lists as a comma-separated list enclosed in square brackets, for example, $$[ [r_{\\text{off},1}, r_{\\text{norm},1}, \\kappa_2(H_1), s_1], [r_{\\text{off},2}, r_{\\text{norm},2}, \\kappa_2(H_2), s_2], [r_{\\text{off},3}, r_{\\text{norm},3}, \\kappa_2(H_3), s_3] ].$$",
            "solution": "The proposed problem is a well-posed and standard exercise in numerical linear algebra and approximation theory, specifically in the context of Uncertainty Quantification (UQ). It requires the construction of an orthonormal polynomial basis from the raw moments of a non-standard probability distribution. This procedure is fundamental to techniques like Polynomial Chaos Expansion (PCE), where such a basis provides an efficient representation for stochastic model outputs. The problem is scientifically sound, internally consistent, and requires the implementation of a numerically non-trivial algorithm.\n\nThe core of the solution is the implementation of the Gram-Schmidt orthonormalization procedure, tailored to operate on a basis of monomials $\\{1, x, x^2, \\dots, x^n\\}$ using an inner product defined through statistical expectation.\n\nFirst, we must formalize the representation of polynomials and the computation of the inner product. A polynomial $p(x)$ of degree at most $n$ is represented by a vector of its coefficients in the monomial basis, $p(x) = \\sum_{i=0}^n c_i x^i \\Leftrightarrow \\mathbf{c} = [c_0, c_1, \\dots, c_n]^T$. The inner product of two polynomials, $f(x) = \\sum_{i=0}^n a_i x^i$ and $g(x) = \\sum_{j=0}^n b_j x^j$, is defined as $\\langle f, g \\rangle = \\mathbb{E}[f(X)g(X)]$. By linearity of expectation, this can be expressed using the raw moments $\\mu_k = \\mathbb{E}[X^k]$ of the random variable $X$:\n$$\n\\langle f, g \\rangle = \\mathbb{E}\\left[ \\left(\\sum_{i=0}^n a_i X^i\\right) \\left(\\sum_{j=0}^n b_j X^j\\right) \\right] = \\mathbb{E}\\left[ \\sum_{i=0}^n \\sum_{j=0}^n a_i b_j X^{i+j} \\right] = \\sum_{i=0}^n \\sum_{j=0}^n a_i b_j \\mathbb{E}[X^{i+j}] = \\sum_{i=0}^n \\sum_{j=0}^n a_i b_j \\mu_{i+j}\n$$\nThis formula allows the computation of inner products using only the polynomials' coefficient vectors and the raw moments $\\{\\mu_k\\}_{k=0}^{2n}$.\n\nThe Gram-Schmidt process is known to be numerically unstable when applied to a nearly linearly dependent basis, a situation common with the monomial basis $\\{x^j\\}_{j=0}^n$ over a finite interval. The problem specifies two modifications to the classical algorithm to enhance numerical stability: pre-scaling and reorthogonalization.\n\nThe algorithm proceeds as follows:\n1.  **Initialization and Pre-scaling**: We start with the monomial basis $\\{v_j(x) = x^j\\}_{j=0}^n$. As a pre-conditioning step, each basis vector $v_j$ is scaled by the inverse of its norm. The squared norm is $\\langle v_j, v_j \\rangle = \\langle x^j, x^j \\rangle = \\mathbb{E}[X^{2j}] = \\mu_{2j}$. Thus, the initial, scaled basis is $\\{v'_j\\}_{j=0}^n$, where $v'_j(x) = x^j / \\sqrt{\\mu_{2j}}$ for $\\mu_{2j} > 0$. In terms of coefficient vectors, the vector for $v_j$ has a $1$ at index $j$ and zeros elsewhere. The vector for $v'_j$ has $1/\\sqrt{\\mu_{2j}}$ at index $j$.\n\n2.  **Iterative Orthonormalization**: We generate the orthonormal basis $\\{p_0, p_1, \\dots, p_m\\}$ iteratively for $j=0, 1, \\dots, n$. Let $\\{p_k\\}_{k=0}^{j-1}$ be the set of orthonormal polynomials already constructed.\n    - An orthogonal polynomial $w_j$ is sought by taking the corresponding pre-scaled monomial $v'_j$ and subtracting its projections onto the space spanned by $\\{p_k\\}_{k=0}^{j-1}$:\n      $$\n      w_j \\leftarrow v'_j - \\sum_{k=0}^{j-1} \\langle v'_j, p_k \\rangle p_k\n      $$\n    - **Reorthogonalization**: Due to finite-precision arithmetic, the computed $w_j$ may have lost orthogonality with respect to the earlier $p_k$. To mitigate this, the projection step is repeated a second time on the already-computed $w_j$:\n      $$\n      w_j \\leftarrow w_j - \\sum_{k=0}^{j-1} \\langle w_j, p_k \\rangle p_k\n      $$\n      This constitutes a single reorthogonalization pass.\n    - **Normalization and Degeneracy Check**: The squared norm of $w_j$, $\\|w_j\\|^2 = \\langle w_j, w_j \\rangle$, is computed. If this value is below a small numerical tolerance (e.g., $10^{-20}$), it indicates that $v'_j$ is numerically linearly dependent on the previous basis vectors. The process is terminated, and the maximal degree achieved is $m = j-1$. Otherwise, the new orthonormal polynomial is obtained by normalization:\n      $$\n      p_j = \\frac{w_j}{\\|w_j\\|}\n      $$\nThe set of coefficient vectors for $\\{p_0, \\dots, p_m\\}$ constitutes the primary output of the algorithm.\n\nNext, we evaluate the numerical quality and stability of this process.\n1.  **Hankel Moment Matrix**: The ill-conditioning of the monomial basis is directly related to the Hankel matrix of moments, $H$, with entries $H_{ij} = \\mu_{i+j}$ for $0 \\le i, j \\le n$. The $2$-norm condition number, $\\kappa_2(H) = \\sigma_{\\max}(H) / \\sigma_{\\min}(H)$, where $\\sigma$ are the singular values of $H$, quantifies this ill-conditioning. A large $\\kappa_2(H)$ suggests that small errors in the moments can lead to large errors in the resulting orthogonal polynomials.\n\n2.  **Orthonormality Diagnostics**: The quality of the computed basis $\\{p_i\\}_{i=0}^m$ is checked by computing its Gram matrix $G$, where $G_{ij} = \\langle p_i, p_j \\rangle$. Ideally, $G$ should be the identity matrix. Deviations are measured by:\n    - $r_{\\text{off}} = \\max_{0 \\le i  j \\le m} |\\langle p_i, p_j \\rangle|$: Measures the loss of orthogonality.\n    - $r_{\\text{norm}} = \\max_{0 \\le i \\le m} |\\langle p_i, p_i \\rangle - 1|$: Measures the deviation from unit norm.\n\n3.  **Stability Indicator**: A boolean flag $s$ consolidates these metrics into a single summary indicator, defined as true if and only if $r_{\\text{off}} \\le 10^{-8}$, $r_{\\text{norm}} \\le 10^{-10}$, and $\\kappa_2(H)  10^{12}$. This provides a clear verdict on the success and numerical trustworthiness of the construction for the given input moments and degree.\n\nThe implementation will apply this complete procedure to each of the three test cases, calculating the raw moments according to their specified distributions (Beta, Bernoulli, Lognormal) and then executing the analysis.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final results.\n    \"\"\"\n\n    def compute_moments(dist_type, params, max_k):\n        \"\"\"Computes raw moments for specified distributions.\"\"\"\n        moments = np.zeros(max_k + 1)\n        if dist_type == 'beta':\n            alpha, beta = params\n            moments[0] = 1.0\n            for k in range(1, max_k + 1):\n                moments[k] = moments[k - 1] * (alpha + k - 1) / (alpha + beta + k - 1)\n        elif dist_type == 'bernoulli':\n            p, = params\n            moments[0] = 1.0\n            moments[1:] = p\n        elif dist_type == 'lognormal':\n            mu, sigma = params\n            for k in range(max_k + 1):\n                moments[k] = np.exp(k * mu + 0.5 * k**2 * sigma**2)\n        return moments\n\n    def inner_product(c1, c2, moments):\n        \"\"\"Computes the inner product of two polynomials using their moments.\"\"\"\n        res = 0.0\n        # Determine the non-zero coefficient indices to optimize loop\n        c1_indices = np.where(c1 != 0)[0]\n        c2_indices = np.where(c2 != 0)[0]\n        for i in c1_indices:\n            for j in c2_indices:\n                res += c1[i] * c2[j] * moments[i + j]\n        return res\n\n    def gram_schmidt(n, moments):\n        \"\"\"Performs Gram-Schmidt with pre-scaling and reorthogonalization.\"\"\"\n        # 1. Initialization with pre-scaling\n        scaled_vectors = []\n        for j in range(n + 1):\n            vj = np.zeros(n + 1)\n            vj[j] = 1.0\n            norm_sq_vj = moments[2 * j]  # x^j, x^j = E[x^{2j}]\n            if norm_sq_vj  1e-30:  # Avoid division by zero\n                vj = vj / np.sqrt(norm_sq_vj)\n            scaled_vectors.append(vj)\n\n        orthonormal_basis = []\n        for j in range(n + 1):\n            wj = scaled_vectors[j].copy()\n\n            # 2. First Orthogonalization\n            for k in range(j):\n                uk = orthonormal_basis[k]\n                proj_coeff = inner_product(wj, uk, moments)\n                wj -= proj_coeff * uk\n\n            # 3. Reorthogonalization\n            for k in range(j):\n                uk = orthonormal_basis[k]\n                proj_coeff = inner_product(wj, uk, moments)\n                wj -= proj_coeff * uk\n\n            # 4. Normalization and Degeneracy Check\n            norm_sq_wj = inner_product(wj, wj, moments)\n            if norm_sq_wj  1e-20:\n                break  # Degeneracy detected, stop the process\n\n            norm_wj = np.sqrt(norm_sq_wj)\n            uj = wj / norm_wj\n            orthonormal_basis.append(uj)\n\n        return orthonormal_basis\n\n    def run_case(case_spec):\n        \"\"\"Runs the entire analysis for a single test case.\"\"\"\n        n = case_spec['n']\n        \n        # 1. Compute moments up to degree 2n\n        moments = compute_moments(case_spec['type'], case_spec['params'], 2 * n)\n        \n        # 2. Hankel matrix and its condition number\n        H = np.zeros((n + 1, n + 1))\n        for i in range(n + 1):\n            for j in range(n + 1):\n                H[i, j] = moments[i + j]\n        kappa_H = np.linalg.cond(H, 2)\n        \n        # 3. Perform Gram-Schmidt\n        ortho_basis = gram_schmidt(n, moments)\n        m = len(ortho_basis) - 1\n\n        # 4. Compute orthonormality diagnostics\n        r_off = 0.0\n        r_norm = 0.0\n        if m = 0:\n            gram_matrix = np.zeros((m + 1, m + 1))\n            for i in range(m + 1):\n                for j in range(i, m + 1):\n                    val = inner_product(ortho_basis[i], ortho_basis[j], moments)\n                    gram_matrix[i, j] = val\n                    gram_matrix[j, i] = val\n            \n            # Off-diagonal error\n            if m  0:\n                off_diagonal_indices = np.triu_indices(m + 1, k=1)\n                r_off = np.max(np.abs(gram_matrix[off_diagonal_indices]))\n            \n            # Norm error\n            diagonal = np.diag(gram_matrix)\n            r_norm = np.max(np.abs(diagonal - 1.0))\n\n        # 5. Stability indicator\n        is_stable = (r_off = 1e-8) and (r_norm = 1e-10) and (np.isfinite(kappa_H) and kappa_H  1e12)\n        \n        return [r_off, r_norm, kappa_H, is_stable]\n\n    test_cases = [\n        {'type': 'beta', 'params': (2.3, 1.7), 'n': 3},\n        {'type': 'bernoulli', 'params': (0.2,), 'n': 3},\n        {'type': 'lognormal', 'params': (0, 0.75), 'n': 3}\n    ]\n\n    all_results = [run_case(case) for case in test_cases]\n\n    # Format the final output string\n    formatted_results = []\n    for res in all_results:\n        r_off, r_norm, kappa, s_val = res\n        s_list = [f\"{r_off:.6f}\", f\"{r_norm:.6f}\", f\"{kappa:.6f}\", str(s_val).lower()]\n        formatted_results.append(f\"[{','.join(s_list)}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}