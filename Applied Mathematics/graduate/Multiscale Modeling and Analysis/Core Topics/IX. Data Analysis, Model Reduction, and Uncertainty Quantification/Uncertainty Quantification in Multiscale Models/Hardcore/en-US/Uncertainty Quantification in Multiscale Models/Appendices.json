{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of modern Uncertainty Quantification is the ability to represent a complex, random model output with a simpler, more structured mathematical object. The generalized Polynomial Chaos (gPC) expansion provides just such a framework, approximating a quantity of interest as a spectral series of orthogonal polynomials. This hands-on problem takes you through the fundamental mechanics of gPC, requiring you to derive the expansion coefficients for a simple quadratic model and then use the properties of the expansion to directly compute the output's mean and variance .",
            "id": "3827665",
            "problem": "Consider a two-scale model in which a microscale parameter $ \\theta $ enters a coarse-scale quantity of interest $ Y $ through a second-order surrogate $ Y(\\theta) = a_{0} + a_{1}\\,\\theta + a_{2}\\,\\theta^{2} $, where $ a_{0}, a_{1}, a_{2} $ are deterministic constants. Assume $ \\theta \\sim \\mathcal{N}(0,1) $ and perform uncertainty quantification (UQ) using the generalized Polynomial Chaos (gPC) framework. Use an orthonormal Hermite polynomial basis $ \\{ \\psi_{0}, \\psi_{1}, \\psi_{2} \\} $ with respect to the standard normal measure. The gPC representation is the mean-square projection of $ Y $ onto the span of $ \\{ \\psi_{0}, \\psi_{1}, \\psi_{2} \\} $, yielding $ Y(\\theta) = c_{0}\\,\\psi_{0}(\\theta) + c_{1}\\,\\psi_{1}(\\theta) + c_{2}\\,\\psi_{2}(\\theta) $ exactly for this $ Y $.\n\nStarting from the fundamental definitions of orthonormal polynomial bases with respect to a probability measure, and using only properties of the standard normal distribution and orthogonality, compute the gPC coefficients $ c_{0}, c_{1}, c_{2} $ in terms of $ a_{0}, a_{1}, a_{2} $. Then deduce the output mean $ \\mu_{Y} $ and variance $ \\sigma_{Y}^{2} $ directly from the gPC expansion and orthonormality. Express your final answer symbolically as a single row matrix containing $ c_{0}, c_{1}, c_{2}, \\mu_{Y}, \\sigma_{Y}^{2} $ in that order. No numerical rounding is required, and no physical units apply.",
            "solution": "The problem requires the determination of the generalized Polynomial Chaos (gPC) coefficients for a given quadratic surrogate model and the subsequent calculation of the mean and variance of the output quantity of interest.\n\nThe problem is valid. It is self-contained, scientifically grounded in the theory of uncertainty quantification and orthogonal polynomials, and well-posed. We proceed with the solution.\n\nThe core of the gPC method is to represent a random variable as a spectral expansion in terms of orthogonal polynomials of the underlying input random variable.\nGiven the input random parameter $\\theta \\sim \\mathcal{N}(0,1)$, the appropriate orthogonal polynomials are the Hermite polynomials. The orthogonality is defined with respect to the standard normal probability measure, with the probability density function $w(\\theta) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{\\theta^2}{2})$. The inner product of two functions $f(\\theta)$ and $g(\\theta)$ is defined as the expectation of their product:\n$$ \\langle f, g \\rangle = \\mathbb{E}[f(\\theta)g(\\theta)] = \\int_{-\\infty}^{\\infty} f(\\theta)g(\\theta) w(\\theta) d\\theta $$\nThe problem specifies an orthonormal Hermite polynomial basis $\\{\\psi_k(\\theta)\\}_{k=0}^{\\infty}$, which satisfies the orthonormality condition:\n$$ \\langle \\psi_i, \\psi_j \\rangle = \\mathbb{E}[\\psi_i(\\theta)\\psi_j(\\theta)] = \\delta_{ij} $$\nwhere $\\delta_{ij}$ is the Kronecker delta.\n\nThe probabilists' Hermite polynomials, denoted $He_k(\\theta)$, are orthogonal (but not orthonormal) with respect to this measure. Their orthogonality relation is $\\mathbb{E}[He_i(\\theta)He_j(\\theta)] = i! \\delta_{ij}$. The first three are:\n$$ He_0(\\theta) = 1 $$\n$$ He_1(\\theta) = \\theta $$\n$$ He_2(\\theta) = \\theta^2 - 1 $$\nTo obtain the required orthonormal basis $\\{\\psi_k(\\theta)\\}$, we normalize the $He_k(\\theta)$ polynomials:\n$$ \\psi_k(\\theta) = \\frac{He_k(\\theta)}{\\sqrt{\\mathbb{E}[He_k(\\theta)^2]}} = \\frac{He_k(\\theta)}{\\sqrt{k!}} $$\nThe first three orthonormal basis polynomials are therefore:\n$$ \\psi_0(\\theta) = \\frac{He_0(\\theta)}{\\sqrt{0!}} = \\frac{1}{1} = 1 $$\n$$ \\psi_1(\\theta) = \\frac{He_1(\\theta)}{\\sqrt{1!}} = \\frac{\\theta}{1} = \\theta $$\n$$ \\psi_2(\\theta) = \\frac{He_2(\\theta)}{\\sqrt{2!}} = \\frac{\\theta^2 - 1}{\\sqrt{2}} $$\nThe quantity of interest $Y(\\theta)$ is a polynomial of degree $2$. Its gPC expansion is given as an exact, finite series:\n$$ Y(\\theta) = c_0 \\psi_0(\\theta) + c_1 \\psi_1(\\theta) + c_2 \\psi_2(\\theta) $$\nThe coefficients $c_k$ are the projections of $Y(\\theta)$ onto the basis functions:\n$$ c_k = \\langle Y(\\theta), \\psi_k(\\theta) \\rangle = \\mathbb{E}[Y(\\theta)\\psi_k(\\theta)] $$\nHowever, since the expansion is exact and we have an explicit representation for both the powers of $\\theta$ and the basis polynomials, we can find the coefficients by algebraic substitution. First, we express the standard polynomial basis $\\{1, \\theta, \\theta^2\\}$ in terms of the orthonormal basis $\\{\\psi_0, \\psi_1, \\psi_2\\}$:\nFrom $\\psi_0(\\theta) = 1$, we have $1 = \\psi_0(\\theta)$.\nFrom $\\psi_1(\\theta) = \\theta$, we have $\\theta = \\psi_1(\\theta)$.\nFrom $\\psi_2(\\theta) = \\frac{\\theta^2 - 1}{\\sqrt{2}}$, we can solve for $\\theta^2$:\n$$ \\sqrt{2} \\psi_2(\\theta) = \\theta^2 - 1 \\implies \\theta^2 = \\sqrt{2} \\psi_2(\\theta) + 1 = \\sqrt{2} \\psi_2(\\theta) + \\psi_0(\\theta) $$\nNow, we substitute these expressions into the surrogate model for $Y(\\theta)$:\n$$ Y(\\theta) = a_{0} + a_{1}\\theta + a_{2}\\theta^{2} $$\n$$ Y(\\theta) = a_0 \\psi_0(\\theta) + a_1 \\psi_1(\\theta) + a_2 (\\sqrt{2} \\psi_2(\\theta) + \\psi_0(\\theta)) $$\nNext, we group the terms by the basis polynomials $\\psi_k(\\theta)$:\n$$ Y(\\theta) = (a_0 + a_2) \\psi_0(\\theta) + a_1 \\psi_1(\\theta) + a_2 \\sqrt{2} \\psi_2(\\theta) $$\nBy comparing this directly with the gPC expansion form $Y(\\theta) = c_0 \\psi_0(\\theta) + c_1 \\psi_1(\\theta) + c_2 \\psi_2(\\theta)$, we can identify the coefficients by uniqueness of the expansion:\n$$ c_0 = a_0 + a_2 $$\n$$ c_1 = a_1 $$\n$$ c_2 = \\sqrt{2} a_2 $$\nWith the gPC coefficients determined, we can now compute the mean $\\mu_Y$ and variance $\\sigma_Y^2$ of the output.\n\nThe mean of $Y$ is its expectation:\n$$ \\mu_Y = \\mathbb{E}[Y(\\theta)] = \\mathbb{E}[c_0 \\psi_0(\\theta) + c_1 \\psi_1(\\theta) + c_2 \\psi_2(\\theta)] $$\nUsing the linearity of expectation:\n$$ \\mu_Y = c_0 \\mathbb{E}[\\psi_0(\\theta)] + c_1 \\mathbb{E}[\\psi_1(\\theta)] + c_2 \\mathbb{E}[\\psi_2(\\theta)] $$\nThe expectation of any basis function $\\psi_k$ for $k > 0$ is zero, as $\\mathbb{E}[\\psi_k(\\theta)] = \\mathbb{E}[\\psi_k(\\theta)\\psi_0(\\theta)] = \\langle \\psi_k, \\psi_0 \\rangle = \\delta_{k0}$. Also, $\\mathbb{E}[\\psi_0(\\theta)] = \\mathbb{E}[1] = 1$.\nThus, the expression for the mean simplifies to:\n$$ \\mu_Y = c_0 \\cdot 1 + c_1 \\cdot 0 + c_2 \\cdot 0 = c_0 $$\nSubstituting the expression for $c_0$, we get:\n$$ \\mu_Y = a_0 + a_2 $$\nThe variance of $Y$ is given by $\\sigma_Y^2 = \\text{Var}(Y(\\theta)) = \\mathbb{E}[(Y(\\theta) - \\mu_Y)^2]$.\nSince $\\mu_Y = c_0$ and $\\psi_0(\\theta)=1$, we have $\\mu_Y = c_0\\psi_0(\\theta)$.\n$$ Y(\\theta) - \\mu_Y = (c_0 \\psi_0(\\theta) + c_1 \\psi_1(\\theta) + c_2 \\psi_2(\\theta)) - c_0 \\psi_0(\\theta) = c_1 \\psi_1(\\theta) + c_2 \\psi_2(\\theta) $$\nThe variance is then:\n$$ \\sigma_Y^2 = \\mathbb{E}[(c_1 \\psi_1(\\theta) + c_2 \\psi_2(\\theta))^2] = \\mathbb{E}[c_1^2 \\psi_1(\\theta)^2 + 2c_1c_2 \\psi_1(\\theta)\\psi_2(\\theta) + c_2^2 \\psi_2(\\theta)^2] $$\nBy linearity of expectation:\n$$ \\sigma_Y^2 = c_1^2 \\mathbb{E}[\\psi_1(\\theta)^2] + 2c_1c_2 \\mathbb{E}[\\psi_1(\\theta)\\psi_2(\\theta)] + c_2^2 \\mathbb{E}[\\psi_2(\\theta)^2] $$\nUsing the orthonormality property $\\mathbb{E}[\\psi_i(\\theta)\\psi_j(\\theta)] = \\delta_{ij}$:\n$$ \\mathbb{E}[\\psi_1(\\theta)^2] = 1 $$\n$$ \\mathbb{E}[\\psi_1(\\theta)\\psi_2(\\theta)] = 0 $$\n$$ \\mathbb{E}[\\psi_2(\\theta)^2] = 1 $$\nThe variance simplifies to a sum of squares of the higher-order gPC coefficients:\n$$ \\sigma_Y^2 = c_1^2 (1) + 2c_1c_2 (0) + c_2^2 (1) = c_1^2 + c_2^2 $$\nSubstituting the expressions for $c_1$ and $c_2$:\n$$ \\sigma_Y^2 = (a_1)^2 + (\\sqrt{2} a_2)^2 = a_1^2 + 2a_2^2 $$\nFinally, we assemble the required quantities $c_0, c_1, c_2, \\mu_Y, \\sigma_Y^2$ into a single row matrix.\n$$ c_0 = a_0 + a_2 $$\n$$ c_1 = a_1 $$\n$$ c_2 = \\sqrt{2}a_2 $$\n$$ \\mu_Y = a_0 + a_2 $$\n$$ \\sigma_Y^2 = a_1^2 + 2a_2^2 $$",
            "answer": "$$ \\boxed{\\begin{pmatrix} a_{0} + a_{2}  a_{1}  \\sqrt{2}a_{2}  a_{0} + a_{2}  a_{1}^{2} + 2a_{2}^{2} \\end{pmatrix}} $$"
        },
        {
            "introduction": "While spectral methods are elegant, Monte Carlo sampling remains a robust and universally applicable tool for uncertainty propagation, but its slow convergence can be prohibitive for expensive multiscale models. This challenge motivates variance reduction techniques, which aim to achieve the same statistical accuracy with fewer model evaluations. This exercise explores the powerful control variate method, showing how a computationally cheap, coarse-scale model can be leveraged to reduce the statistical error in estimating the mean of an expensive, fine-scale model .",
            "id": "3827688",
            "problem": "Consider a multiscale model where a fine-scale solver produces a random output $Y_{f}$ with unknown mean $\\mu_{f}$ and finite variance $\\sigma_{f}^{2}$, and a coarse-scale solver produces a random output $Y_{c}$ with known mean $\\mu_{c}$ and finite variance $\\sigma_{c}^{2}$. Assume joint sampling of $(Y_{f}, Y_{c})$ under common random inputs so that $Y_{f}$ and $Y_{c}$ are correlated. Let $\\rho \\in [-1,1]$ denote the Pearson correlation coefficient between $Y_{f}$ and $Y_{c}$, defined by $\\rho = \\mathrm{Cov}(Y_{f}, Y_{c})/(\\sigma_{f}\\sigma_{c})$, where $\\mathrm{Cov}(Y_{f}, Y_{c})$ is the covariance and $\\sigma_{f}$, $\\sigma_{c}$ are the standard deviations of $Y_{f}$ and $Y_{c}$, respectively.\n\nIn the context of Uncertainty Quantification (UQ), consider estimating $\\mu_{f}$ by Monte Carlo using $n$ independent and identically distributed joint samples $\\{(Y_{f}^{(i)}, Y_{c}^{(i)})\\}_{i=1}^{n}$. Construct a control variate estimator of the form\n$$\\widehat{\\mu}_{\\mathrm{cv}} = \\overline{Y}_{f} - \\beta \\left(\\overline{Y}_{c} - \\mu_{c}\\right),$$\nwhere $\\overline{Y}_{f} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{f}^{(i)}$ and $\\overline{Y}_{c} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{c}^{(i)}$, and $\\beta \\in \\mathbb{R}$ is a tunable scalar. Starting from the definitions of expectation, variance, covariance, and correlation, derive the choice of $\\beta$ that minimizes the variance of $\\widehat{\\mu}_{\\mathrm{cv}}$ and then derive the variance reduction factor, defined as\n$$\\mathrm{VRF} = \\frac{\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right)}{\\mathrm{Var}\\!\\left(\\overline{Y}_{f}\\right)},$$\nexpressed solely as a function of $\\rho$. Provide the final expression for $\\mathrm{VRF}$ in closed form. No numerical evaluation is required. If any approximation were needed it should be stated in significant figures, but here an exact analytical expression is expected.",
            "solution": "The user wants to find the optimal control variate parameter $\\beta$ and the resulting variance reduction factor for a multiscale Monte Carlo simulation.\n\n### Problem Validation\nThe problem statement is assessed against the required criteria.\n#### Step 1: Extract Givens\n- Fine-scale random output: $Y_{f}$\n- Fine-scale mean: $\\mu_{f} = E[Y_{f}]$ (unknown)\n- Fine-scale variance: $\\sigma_{f}^{2} = \\mathrm{Var}(Y_{f})$ (finite)\n- Coarse-scale random output: $Y_{c}$\n- Coarse-scale mean: $\\mu_{c} = E[Y_{c}]$ (known)\n- Coarse-scale variance: $\\sigma_{c}^{2} = \\mathrm{Var}(Y_{c})$ (finite)\n- Correlation coefficient: $\\rho = \\frac{\\mathrm{Cov}(Y_{f}, Y_{c})}{\\sigma_{f}\\sigma_{c}}$, where $\\rho \\in [-1,1]$\n- Sample size: $n$ i.i.d. joint samples $\\{(Y_{f}^{(i)}, Y_{c}^{(i)})\\}_{i=1}^{n}$\n- Sample means: $\\overline{Y}_{f} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{f}^{(i)}$ and $\\overline{Y}_{c} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{c}^{(i)}$\n- Control variate estimator: $\\widehat{\\mu}_{\\mathrm{cv}} = \\overline{Y}_{f} - \\beta \\left(\\overline{Y}_{c} - \\mu_{c}\\right)$ for a tunable scalar $\\beta \\in \\mathbb{R}$\n- Objective: Find the value of $\\beta$ that minimizes $\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right)$ and derive the variance reduction factor $\\mathrm{VRF} = \\frac{\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right)}{\\mathrm{Var}\\!\\left(\\overline{Y}_{f}\\right)}$ as a function of $\\rho$.\n\n#### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is a standard derivation in the field of Monte Carlo methods and Uncertainty Quantification (UQ). All terms are clearly defined, and the task is to perform a mathematical derivation based on fundamental principles of probability theory. No flaws are identified.\n\n#### Step 3: Verdict and Action\nThe problem is valid. The solution will now be derived.\n\n### Derivation\nThe goal is to find the value of $\\beta$ that minimizes the variance of the control variate estimator $\\widehat{\\mu}_{\\mathrm{cv}}$ and then to compute the resulting variance reduction factor.\n\nFirst, we determine the variance of the estimator $\\widehat{\\mu}_{\\mathrm{cv}}$. The estimator is given by:\n$$ \\widehat{\\mu}_{\\mathrm{cv}} = \\overline{Y}_{f} - \\beta \\left(\\overline{Y}_{c} - \\mu_{c}\\right) $$\nSince $\\mu_c$ is a known constant, subtracting it from the random variable $\\overline{Y}_c$ shifts the mean but does not change the variance or its covariance with other variables. Therefore, for the purpose of variance calculation, we can analyze the variance of $\\overline{Y}_{f} - \\beta \\overline{Y}_{c}$:\n$$ \\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right) = \\mathrm{Var}\\!\\left(\\overline{Y}_{f} - \\beta \\overline{Y}_{c}\\right) $$\nUsing the standard formula for the variance of a difference of two random variables, $\\mathrm{Var}(A - B) = \\mathrm{Var}(A) + \\mathrm{Var}(B) - 2\\mathrm{Cov}(A,B)$:\n$$ \\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right) = \\mathrm{Var}\\!\\left(\\overline{Y}_{f}\\right) + \\mathrm{Var}\\!\\left(\\beta \\overline{Y}_{c}\\right) - 2\\mathrm{Cov}\\!\\left(\\overline{Y}_{f}, \\beta \\overline{Y}_{c}\\right) $$\nUsing the properties of variance and covariance, $\\mathrm{Var}(cX) = c^2\\mathrm{Var}(X)$ and $\\mathrm{Cov}(X, cY) = c\\mathrm{Cov}(X,Y)$:\n$$ \\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right) = \\mathrm{Var}\\!\\left(\\overline{Y}_{f}\\right) + \\beta^{2} \\mathrm{Var}\\!\\left(\\overline{Y}_{c}\\right) - 2\\beta \\mathrm{Cov}\\!\\left(\\overline{Y}_{f}, \\overline{Y}_{c}\\right) $$\nNext, we express the variances and covariances of the sample means in terms of the properties of the underlying random variables $Y_f$ and $Y_c$. The samples $\\{(Y_{f}^{(i)}, Y_{c}^{(i)})\\}$ are independent and identically distributed.\nThe variance of a sample mean for i.i.d. samples is $\\frac{1}{n}$ times the population variance:\n$$ \\mathrm{Var}\\!\\left(\\overline{Y}_{f}\\right) = \\mathrm{Var}\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n} Y_{f}^{(i)}\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}\\!\\left(Y_{f}^{(i)}\\right) = \\frac{n\\sigma_{f}^{2}}{n^2} = \\frac{\\sigma_{f}^{2}}{n} $$\n$$ \\mathrm{Var}\\!\\left(\\overline{Y}_{c}\\right) = \\mathrm{Var}\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n} Y_{c}^{(i)}\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}\\!\\left(Y_{c}^{(i)}\\right) = \\frac{n\\sigma_{c}^{2}}{n^2} = \\frac{\\sigma_{c}^{2}}{n} $$\nThe covariance of the sample means is:\n$$ \\mathrm{Cov}\\!\\left(\\overline{Y}_{f}, \\overline{Y}_{c}\\right) = \\mathrm{Cov}\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n} Y_{f}^{(i)}, \\frac{1}{n}\\sum_{j=1}^{n} Y_{c}^{(j)}\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\mathrm{Cov}\\!\\left(Y_{f}^{(i)}, Y_{c}^{(j)}\\right) $$\nSince samples $(Y_{f}^{(i)}, Y_{c}^{(i)})$ are independent for $i \\neq j$, it follows that $\\mathrm{Cov}(Y_{f}^{(i)}, Y_{c}^{(j)}) = 0$ for $i \\neq j$. The double summation reduces to a single sum where $i=j$:\n$$ \\mathrm{Cov}\\!\\left(\\overline{Y}_{f}, \\overline{Y}_{c}\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Cov}\\!\\left(Y_{f}^{(i)}, Y_{c}^{(i)}\\right) = \\frac{n \\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)}{n^2} = \\frac{\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)}{n} $$\nSubstituting these expressions back into the equation for $\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right)$:\n$$ \\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right) = \\frac{\\sigma_{f}^{2}}{n} + \\beta^{2} \\frac{\\sigma_{c}^{2}}{n} - 2\\beta \\frac{\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)}{n} $$\nTo find the value of $\\beta$ that minimizes this variance, we treat $\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right)$ as a function of $\\beta$ and find its minimum by setting the first derivative with respect to $\\beta$ to zero:\n$$ \\frac{d}{d\\beta} \\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right) = \\frac{d}{d\\beta} \\left( \\frac{\\sigma_{f}^{2}}{n} + \\beta^{2} \\frac{\\sigma_{c}^{2}}{n} - 2\\beta \\frac{\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)}{n} \\right) = \\frac{2\\beta\\sigma_{c}^{2}}{n} - \\frac{2\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)}{n} $$\nSetting the derivative to zero:\n$$ \\frac{2\\beta\\sigma_{c}^{2}}{n} - \\frac{2\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)}{n} = 0 \\implies \\beta\\sigma_{c}^{2} = \\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right) $$\nThis yields the optimal value of $\\beta$, which we denote as $\\beta^{*}$:\n$$ \\beta^{*} = \\frac{\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)}{\\sigma_{c}^{2}} $$\nThe second derivative is $\\frac{d^2}{d\\beta^2}\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{cv}}) = \\frac{2\\sigma_c^2}{n} > 0$ (assuming $\\sigma_c^2 > 0$), confirming this is a minimum.\nNow, we substitute $\\beta^{*}$ back into the expression for the variance:\n$$ \\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right)_{\\mathrm{opt}} = \\frac{\\sigma_{f}^{2}}{n} + \\left(\\frac{\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)}{\\sigma_{c}^{2}}\\right)^{2} \\frac{\\sigma_{c}^{2}}{n} - 2\\left(\\frac{\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)}{\\sigma_{c}^{2}}\\right) \\frac{\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)}{n} $$\n$$ \\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right)_{\\mathrm{opt}} = \\frac{\\sigma_{f}^{2}}{n} + \\frac{\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)^{2}}{n\\sigma_{c}^{2}} - \\frac{2\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)^{2}}{n\\sigma_{c}^{2}} = \\frac{\\sigma_{f}^{2}}{n} - \\frac{\\mathrm{Cov}\\!\\left(Y_{f}, Y_{c}\\right)^{2}}{n\\sigma_{c}^{2}} $$\nThe problem asks for the result in terms of the correlation coefficient $\\rho = \\frac{\\mathrm{Cov}(Y_{f}, Y_{c})}{\\sigma_{f}\\sigma_{c}}$. From this, we have $\\mathrm{Cov}(Y_{f}, Y_{c}) = \\rho\\sigma_{f}\\sigma_{c}$.\nSubstituting this into the expression for the optimal variance:\n$$ \\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right)_{\\mathrm{opt}} = \\frac{\\sigma_{f}^{2}}{n} - \\frac{\\left(\\rho\\sigma_{f}\\sigma_{c}\\right)^{2}}{n\\sigma_{c}^{2}} = \\frac{\\sigma_{f}^{2}}{n} - \\frac{\\rho^{2}\\sigma_{f}^{2}\\sigma_{c}^{2}}{n\\sigma_{c}^{2}} = \\frac{\\sigma_{f}^{2}}{n}\\left(1 - \\rho^{2}\\right) $$\nFinally, we compute the variance reduction factor, $\\mathrm{VRF}$, defined as the ratio of the optimized control variate variance to the variance of the standard Monte Carlo estimator $\\overline{Y}_f$:\n$$ \\mathrm{VRF} = \\frac{\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{cv}}\\right)_{\\mathrm{opt}}}{\\mathrm{Var}\\!\\left(\\overline{Y}_{f}\\right)} $$\nSubstituting the expressions we derived:\n$$ \\mathrm{VRF} = \\frac{\\frac{\\sigma_{f}^{2}}{n}\\left(1 - \\rho^{2}\\right)}{\\frac{\\sigma_{f}^{2}}{n}} $$\nThe term $\\frac{\\sigma_f^2}{n}$ cancels, leaving the final expression solely in terms of $\\rho$:\n$$ \\mathrm{VRF} = 1 - \\rho^{2} $$\nThis result shows that the variance reduction is determined by the square of the correlation coefficient between the fine-scale and coarse-scale model outputs. A stronger correlation (i.e., $|\\rho|$ closer to $1$) leads to a greater reduction in variance.",
            "answer": "$$\\boxed{1 - \\rho^{2}}$$"
        },
        {
            "introduction": "Uncertainty in multiscale modeling arises not only from uncertain inputs but also from the limitations of the simulation itself, such as the use of finite-sized computational domains to represent bulk materials. This practice shifts our focus to a critical data analysis task: extrapolating results from finite-size simulations to predict the infinite-volume limit and rigorously quantifying the uncertainty in that prediction. In this exercise, you will implement a weighted least squares regression based on a finite-size scaling law, a standard and essential technique for obtaining reliable effective properties from Representative Volume Element (RVE) simulations .",
            "id": "3827718",
            "problem": "Consider a random heterogeneous medium in which a scalar effective property $p$ (for example, an effective conductivity or stiffness) is estimated via Representative Volume Elements (RVEs) of linear size $L$. Let $p(L)$ denote the RVE-based estimator of the effective property at size $L$, computed as an average over $n(L)$ independent microstructure samples. Assume that, due to finite-size bias, the leading-order deviation of $p(L)$ from the infinite-volume limit $p_{\\infty}$ follows a finite-size scaling law with known exponent $\\alpha  0$, and that the sample average at each $L$ is approximately Gaussian by the Central Limit Theorem (CLT). Specifically, suppose the following measurement model holds:\n$$\n\\bar{p}_i = p_{\\infty} + A \\, L_i^{-\\alpha} + \\varepsilon_i \\quad \\text{for} \\quad i = 1,\\dots,m,\n$$\nwhere $L_i$ are distinct RVE sizes, $\\bar{p}_i$ is the sample mean computed from $n_i$ independent realizations at size $L_i$, $A$ is an unknown amplitude, and $\\varepsilon_i$ are independent zero-mean Gaussian errors with variance\n$$\n\\operatorname{Var}(\\varepsilon_i) = \\sigma_i^2 = \\frac{s_i^2}{n_i},\n$$\nwhere $s_i$ is the measured sample standard deviation of the $n_i$ realizations at size $L_i$. The deterministic $L_i^{-\\alpha}$ scaling represents the leading-order finite-size bias, while the random errors capture the sampling variability of the $\\bar{p}_i$.\n\nYour task is to implement a program that:\n- Given the data $\\{(L_i, \\bar{p}_i, s_i, n_i)\\}_{i=1}^m$ and the known $\\alpha$, estimates $p_{\\infty}$ and provides a statistically principled uncertainty estimate for $p_{\\infty}$ using weighted least squares under the Gaussian error model and a reduced chi-square scaling to account for possible model discrepancy.\n- Uses the linearized form of the model by setting $x_i = L_i^{-\\alpha}$ and fitting\n$$\n\\bar{p}_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i,\n$$\nwhere $\\beta_0 = p_{\\infty}$ and $\\beta_1 = A$.\n- Employs weights $w_i = 1/\\sigma_i^2$ with $\\sigma_i = s_i/\\sqrt{n_i}$, solves for $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)$ via the normal equations in matrix form\n$$\n\\hat{\\beta} = (X^{\\top} W X)^{-1} X^{\\top} W \\, \\bar{p},\n$$\nwith design matrix $X = \\begin{bmatrix} 1  x_1 \\\\ \\vdots  \\vdots \\\\ 1  x_m \\end{bmatrix}$, weight matrix $W = \\operatorname{diag}(w_1,\\dots,w_m)$, and response vector $\\bar{p} = (\\bar{p}_1,\\dots,\\bar{p}_m)^{\\top}$.\n- Computes the residuals $r_i = \\bar{p}_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i$, the weighted sum of squares\n$$\n\\chi^2 = \\sum_{i=1}^m w_i r_i^2,\n$$\nand the reduced chi-square scaling\n$$\n\\hat{\\sigma}^2_{\\text{red}} = \\frac{\\chi^2}{m - 2} \\quad \\text{for} \\quad m  2.\n$$\n- Estimates the covariance of $\\hat{\\beta}$ as\n$$\n\\operatorname{Cov}(\\hat{\\beta}) \\approx \\hat{\\sigma}^2_{\\text{red}} \\, (X^{\\top} W X)^{-1},\n$$\nand reports the standard uncertainty of $p_{\\infty}$ as $u(p_{\\infty}) = \\sqrt{\\operatorname{Cov}(\\hat{\\beta})_{00}}$. If $m \\leq 2$, use $\\operatorname{Cov}(\\hat{\\beta}) \\approx (X^{\\top} W X)^{-1}$.\n\nInput data are fixed and provided below for five test cases. For each case, use the given $\\alpha$, sizes $L_i$, sample means $\\bar{p}_i$, sample standard deviations $s_i$, and replicate counts $n_i$ exactly as specified. All quantities are dimensionless.\n\nTest Suite:\n- Case $1$:\n  - $\\alpha = 1.0$\n  - $L = [50, 100, 200, 400]$\n  - $\\bar{p} = [2.104, 1.947, 1.877, 1.8365]$\n  - $s = [0.2, 0.16, 0.14, 0.1]$\n  - $n = [30, 30, 30, 30]$\n- Case $2$:\n  - $\\alpha = 1.0$\n  - $L = [40, 80, 160, 320]$\n  - $\\bar{p} = [2.028, 2.0105, 2.00775, 2.002125]$\n  - $s = [0.05, 0.05, 0.05, 0.05]$\n  - $n = [20, 20, 20, 20]$\n- Case $3$:\n  - $\\alpha = 1.0$\n  - $L = [100, 200, 400]$\n  - $\\bar{p} = [1.06, 1.02, 1.0125]$\n  - $s = [0.3, 0.2, 0.2]$\n  - $n = [10, 10, 10]$\n- Case $4$:\n  - $\\alpha = 1.0$\n  - $L = [30, 60, 120, 240, 480]$\n  - $\\bar{p} = [3.3383333, 3.1626666, 3.0863333, 3.0396666, 3.0218333]$\n  - $s = [0.5, 0.35, 0.25, 0.18, 0.12]$\n  - $n = [25, 25, 25, 25, 25]$\n- Case $5$:\n  - $\\alpha = 1.5$\n  - $L = [50, 100, 200, 400]$\n  - $\\bar{p} = [5.286842712, 5.097, 5.037355339, 5.0115]$\n  - $s = [0.2, 0.15, 0.1, 0.08]$\n  - $n = [50, 50, 50, 50]$\n\nYour program should, for each test case, compute and return a two-element list containing $[\\hat{p}_{\\infty}, u(\\hat{p}_{\\infty})]$, where $\\hat{p}_{\\infty} = \\hat{\\beta}_0$ and $u(\\hat{p}_{\\infty}) = \\sqrt{\\operatorname{Cov}(\\hat{\\beta})_{00}}$. The final output should aggregate all five test-case results into a single line formatted as a comma-separated list enclosed in square brackets, containing five inner lists with no extra whitespace, for example:\n$$\n[\\,[\\hat{p}_{\\infty}^{(1)}, u^{(1)}], [\\hat{p}_{\\infty}^{(2)}, u^{(2)}], [\\hat{p}_{\\infty}^{(3)}, u^{(3)}], [\\hat{p}_{\\infty}^{(4)}, u^{(4)}], [\\hat{p}_{\\infty}^{(5)}, u^{(5)}]\\,].\n$$\nNo physical units are involved. Angles are not used. Express all numerical outputs as floating-point numbers. The program must not read any input and must rely only on the provided test cases.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of statistical inference and finite-size scaling analysis, which are standard in computational sciences. The problem is well-posed, with a clear objective, sufficient data, and a mathematically sound, non-contradictory procedure for finding a unique solution. All terms are defined, and the task is a direct application of established methods to a realistic scenario in multiscale modeling.\n\nThe objective is to estimate the infinite-volume effective property, $p_{\\infty}$, and its associated uncertainty, $u(p_{\\infty})$, from a series of finite-size sample means, $\\{\\bar{p}_i\\}_{i=1}^m$. The provided data follows a measurement model that combines a deterministic finite-size bias with stochastic sampling error.\n\nThe model is given by:\n$$\n\\bar{p}_i = p_{\\infty} + A \\, L_i^{-\\alpha} + \\varepsilon_i\n$$\nwhere $\\bar{p}_i$ is the sample mean at size $L_i$, $\\alpha$ is a known scaling exponent, $A$ is an unknown amplitude, and $\\varepsilon_i$ represents the random error. This error is assumed to be an independent, zero-mean Gaussian random variable, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$, with variance $\\sigma_i^2 = s_i^2/n_i$ derived from the sample standard deviation $s_i$ and the number of realizations $n_i$.\n\nThe core of the solution lies in transforming this model into a linear form suitable for regression analysis. By defining a new independent variable $x_i = L_i^{-\\alpha}$ and relabeling the parameters as $\\beta_0 = p_{\\infty}$ and $\\beta_1 = A$, the model becomes a standard simple linear regression model:\n$$\n\\bar{p}_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n$$\nOur goal is to estimate the intercept parameter, $\\beta_0$, and its standard uncertainty.\n\nBecause the variances $\\sigma_i^2$ are not equal for all measurements (a condition known as heteroscedasticity), an ordinary least squares approach is suboptimal. The prescribed method, Weighted Least Squares (WLS), is the appropriate technique as it gives more weight to more precise measurements (those with smaller variance). WLS seeks to find the parameters $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)^{\\top}$ that minimize the weighted sum of squared residuals:\n$$\n\\chi^2(\\beta) = \\sum_{i=1}^m w_i (\\bar{p}_i - (\\beta_0 + \\beta_1 x_i))^2\n$$\nThe optimal weights, $w_i$, are the inverse of the variances of the corresponding measurements: $w_i = 1/\\sigma_i^2 = n_i/s_i^2$.\n\nThis minimization problem can be elegantly solved using matrix algebra. We define the response vector $\\bar{p}$, the parameter vector $\\beta$, the design matrix $X$, and the diagonal weight matrix $W$:\n$$\n\\bar{p} = \\begin{pmatrix} \\bar{p}_1 \\\\ \\vdots \\\\ \\bar{p}_m \\end{pmatrix}, \\quad\n\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}, \\quad\nX = \\begin{bmatrix} 1  x_1 \\\\ \\vdots  \\vdots \\\\ 1  x_m \\end{bmatrix} = \\begin{bmatrix} 1  L_1^{-\\alpha} \\\\ \\vdots  \\vdots \\\\ 1  L_m^{-\\alpha} \\end{bmatrix}, \\quad\nW = \\operatorname{diag}(w_1, \\dots, w_m)\n$$\nThe WLS solution $\\hat{\\beta}$ is found by solving the normal equations, $(X^{\\top} W X) \\hat{\\beta} = X^{\\top} W \\bar{p}$, which yields:\n$$\n\\hat{\\beta} = (X^{\\top} W X)^{-1} X^{\\top} W \\bar{p}\n$$\nThe estimate for the infinite-volume property is the first component of this vector: $\\hat{p}_{\\infty} = \\hat{\\beta}_0$.\n\nTo quantify the uncertainty in this estimate, we first consider the covariance matrix of the estimator $\\hat{\\beta}$. Under the assumption that the error model is correct, the covariance is given by $\\operatorname{Cov}(\\hat{\\beta}) = (X^{\\top} W X)^{-1}$. However, the problem specifies a refinement to account for potential model inadequacy or misspecification of error variances. This is done by scaling the covariance matrix by the reduced chi-square statistic, $\\hat{\\sigma}^2_{\\text{red}}$. This statistic compares the observed scatter of the data around the fitted line to the expected scatter from the a priori error estimates.\n\nFirst, the residuals are computed: $r_i = \\bar{p}_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$. The minimized weighted sum of squares is then $\\chi^2 = \\sum_{i=1}^m w_i r_i^2$. The reduced chi-square is this value normalized by the degrees of freedom, $\\nu = m - k$, where $m$ is the number of data points and $k=2$ is the number of fitted parameters. For $m > 2$:\n$$\n\\hat{\\sigma}^2_{\\text{red}} = \\frac{\\chi^2}{m-2}\n$$\nThe value of $\\hat{\\sigma}^2_{\\text{red}}$ provides a measure of goodness-of-fit. A value near $1$ indicates that the model fits the data consistently with the assumed error variances. A value significantly larger than $1$ suggests that the model is a poor fit or the error variances were underestimated. The adjusted covariance matrix is then:\n$$\n\\operatorname{Cov}(\\hat{\\beta}) \\approx \\hat{\\sigma}^2_{\\text{red}} (X^{\\top} W X)^{-1} \\quad (\\text{for } m > 2)\n$$\nIf $m \\le 2$, the degrees of freedom are non-positive, so this scaling is not applied, and the original covariance expression $\\operatorname{Cov}(\\hat{\\beta}) \\approx (X^{\\top} W X)^{-1}$ is used.\n\nThe variance of our estimator $\\hat{p}_{\\infty} = \\hat{\\beta}_0$ is the top-left element of this covariance matrix, denoted $\\operatorname{Cov}(\\hat{\\beta})_{00}$. The standard uncertainty is its square root:\n$$\nu(p_{\\infty}) = \\sqrt{\\operatorname{Cov}(\\hat{\\beta})_{00}}\n$$\nThe implemented algorithm will, for each test case, construct the matrices $X$ and $W$ from the input data, solve the normal equations for $\\hat{\\beta}$, compute the reduced chi-square statistic to scale the covariance matrix (since $m > 2$ in all cases), and extract the final estimate $\\hat{p}_{\\infty}$ and its uncertainty $u(p_{\\infty})$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"alpha\": 1.0,\n            \"L\": [50, 100, 200, 400],\n            \"p_bar\": [2.104, 1.947, 1.877, 1.8365],\n            \"s\": [0.2, 0.16, 0.14, 0.1],\n            \"n\": [30, 30, 30, 30],\n        },\n        {\n            \"alpha\": 1.0,\n            \"L\": [40, 80, 160, 320],\n            \"p_bar\": [2.028, 2.0105, 2.00775, 2.002125],\n            \"s\": [0.05, 0.05, 0.05, 0.05],\n            \"n\": [20, 20, 20, 20],\n        },\n        {\n            \"alpha\": 1.0,\n            \"L\": [100, 200, 400],\n            \"p_bar\": [1.06, 1.02, 1.0125],\n            \"s\": [0.3, 0.2, 0.2],\n            \"n\": [10, 10, 10],\n        },\n        {\n            \"alpha\": 1.0,\n            \"L\": [30, 60, 120, 240, 480],\n            \"p_bar\": [3.3383333, 3.1626666, 3.0863333, 3.0396666, 3.0218333],\n            \"s\": [0.5, 0.35, 0.25, 0.18, 0.12],\n            \"n\": [25, 25, 25, 25, 25],\n        },\n        {\n            \"alpha\": 1.5,\n            \"L\": [50, 100, 200, 400],\n            \"p_bar\": [5.286842712, 5.097, 5.037355339, 5.0115],\n            \"s\": [0.2, 0.15, 0.1, 0.08],\n            \"n\": [50, 50, 50, 50],\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        p_inf_hat, u_p_inf = calculate_wls_extrapolation(\n            case[\"alpha\"],\n            case[\"L\"],\n            case[\"p_bar\"],\n            case[\"s\"],\n            case[\"n\"],\n        )\n        all_results.append(f\"[{p_inf_hat},{u_p_inf}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\ndef calculate_wls_extrapolation(alpha, L, p_bar, s, n):\n    \"\"\"\n    Performs weighted least squares extrapolation to find p_infinity and its uncertainty.\n    \n    Args:\n        alpha (float): The scaling exponent.\n        L (list): List of RVE sizes.\n        p_bar (list): List of sample means.\n        s (list): List of sample standard deviations.\n        n (list): List of replicate counts.\n\n    Returns:\n        tuple: A tuple containing (p_infinity_estimate, p_infinity_uncertainty).\n    \"\"\"\n    # Convert input lists to numpy arrays for vectorized calculations.\n    L_arr = np.array(L, dtype=float)\n    p_bar_arr = np.array(p_bar, dtype=float)\n    s_arr = np.array(s, dtype=float)\n    n_arr = np.array(n, dtype=float)\n    \n    m = len(L_arr)\n\n    # 1. Linearize the model: x_i = L_i^(-alpha)\n    x = L_arr ** (-alpha)\n\n    # 2. Construct the design matrix X.\n    X = np.vstack((np.ones(m), x)).T\n\n    # 3. Calculate weights w_i = 1 / sigma_i^2 = n_i / s_i^2 and construct matrix W.\n    variances = s_arr**2 / n_arr\n    weights = 1.0 / variances\n    W = np.diag(weights)\n\n    # 4. Solve the normal equations: beta_hat = (X^T W X)^-1 X^T W p_bar\n    XT_W_X = X.T @ W @ X\n    XT_W_X_inv = np.linalg.inv(XT_W_X)\n    XT_W_p = X.T @ W @ p_bar_arr\n    beta_hat = XT_W_X_inv @ XT_W_p\n\n    p_infinity_estimate = beta_hat[0]\n\n    # 5. Calculate the covariance matrix of beta_hat and the uncertainty of p_infinity.\n    if m > 2:\n        # Calculate residuals and reduced chi-square for m > 2.\n        residuals = p_bar_arr - (X @ beta_hat)\n        chi_squared = np.sum(weights * residuals**2)\n        degrees_of_freedom = m - 2\n        reduced_chi_squared = chi_squared / degrees_of_freedom\n        \n        # Scale the covariance matrix by the reduced chi-square.\n        cov_beta_hat = reduced_chi_squared * XT_W_X_inv\n    else: # m = 2\n        # Use the unscaled covariance matrix.\n        cov_beta_hat = XT_W_X_inv\n\n    # The variance of p_infinity is the (0,0) element of the covariance matrix.\n    # The standard uncertainty is its square root.\n    variance_p_inf = cov_beta_hat[0, 0]\n    p_infinity_uncertainty = np.sqrt(variance_p_inf)\n\n    return p_infinity_estimate, p_infinity_uncertainty\n\nsolve()\n```"
        }
    ]
}