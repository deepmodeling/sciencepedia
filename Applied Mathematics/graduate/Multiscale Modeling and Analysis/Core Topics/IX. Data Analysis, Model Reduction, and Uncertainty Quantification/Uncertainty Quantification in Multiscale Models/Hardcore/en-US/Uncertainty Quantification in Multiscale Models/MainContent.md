## Introduction
In the quest for predictive computational science, multiscale models have become indispensable tools for linking microscopic phenomena to macroscopic behavior. However, the predictive power of these models is fundamentally limited by uncertainty, which arises from inherent system randomness, incomplete knowledge of parameters, and simplifying modeling assumptions. Ignoring these uncertainties can lead to overconfident predictions and flawed engineering decisions. This article provides a comprehensive guide to Uncertainty Quantification (UQ) in multiscale models, addressing the critical need for a rigorous framework to manage and interpret uncertainty.

Across three chapters, you will gain a deep understanding of the UQ workflow. The first chapter, **Principles and Mechanisms**, establishes the foundational concepts, distinguishing between [aleatoric and epistemic uncertainty](@entry_id:184798) and detailing the core mathematical methods for forward propagation and sensitivity analysis. The second chapter, **Applications and Interdisciplinary Connections**, demonstrates the practical utility of these methods across diverse fields, from materials science to energy systems, with a focus on [inverse problems](@entry_id:143129) and model calibration. Finally, **Hands-On Practices** offers a series of guided problems to solidify your computational skills in implementing key UQ techniques. This journey will equip you with the knowledge to build more reliable, robust, and credible multiscale models.

## Principles and Mechanisms

The endeavor to create predictive multiscale models invariably confronts the challenge of uncertainty. In complex systems, perfect knowledge is an unachievable ideal. Instead, we must rigorously identify, characterize, and propagate the effects of uncertainty from the microscopic scales at which they originate to the macroscopic scales where predictions are made. This chapter lays out the fundamental principles and mathematical mechanisms for performing Uncertainty Quantification (UQ) in the context of multiscale models. We will begin by establishing the primary classification of uncertainty, then explore its various manifestations in multiscale systems, and finally detail the core methodologies for its propagation and analysis.

### The Fundamental Dichotomy: Aleatoric vs. Epistemic Uncertainty

The first and most crucial step in any UQ analysis is to distinguish between two fundamentally different types of uncertainty: aleatoric and epistemic.

**Aleatoric uncertainty** (from the Latin *alea*, meaning 'die') refers to the inherent, irreducible randomness present in a system. It is a feature of the system itself, reflecting [stochasticity](@entry_id:202258) that would persist even with perfect knowledge of all model parameters and governing laws. In multiscale [material modeling](@entry_id:173674), the canonical example of aleatoric uncertainty is the precise, yet random, configuration of the underlying microstructure. Even if we knew the statistical laws governing the microstructure perfectly, the specific realization within any given physical specimen would remain a random outcome . This type of uncertainty is often described as "what we can't know in principle."

**Epistemic uncertainty** (from the Greek *episteme*, meaning 'knowledge') stems from a lack of knowledge on the part of the modeler. This includes uncertainty about the appropriate values of model parameters, the correct form of the governing equations, the boundary conditions, or the assumptions made to couple scales. Crucially, epistemic uncertainty is, in principle, reducible. By collecting more data, performing more refined experiments, or developing better theories, our state of knowledge can improve, and this uncertainty can be diminished . This type of uncertainty is often described as "what we don't currently know."

A principled mathematical framework for separating these two uncertainty types is provided by the **law of total variance**. Let $Y$ be a quantity of interest (QoI) that is a function of both aleatoric variables (e.g., a microstate realization $\omega$) and epistemic variables (e.g., a vector of unknown model parameters $\theta$). We can formally partition the total variance of $Y$ by conditioning on the sources of epistemic uncertainty, which we group into a knowledge set $\mathcal{K}$ (for instance, $\mathcal{K}$ could be the [sigma-algebra](@entry_id:137915) generated by the uncertain parameter $\theta$). The law of total variance states:
$$
\mathrm{Var}(Y) = \mathbb{E}[\mathrm{Var}(Y \mid \mathcal{K})] + \mathrm{Var}(\mathbb{E}[Y \mid \mathcal{K}])
$$

The two terms on the right-hand side have clear interpretations:

1.  $\mathbb{E}[\mathrm{Var}(Y \mid \mathcal{K})]$ is the **aleatoric contribution**. The inner term, $\mathrm{Var}(Y \mid \mathcal{K})$, represents the variance of the QoI that remains even when the sources of epistemic uncertainty are fixed (i.e., we know the value of $\theta$). This is the inherent variability due to $\omega$. The outer expectation averages this inherent variability over all possible values of the epistemic quantities.

2.  $\mathrm{Var}(\mathbb{E}[Y \mid \mathcal{K}])$ is the **epistemic contribution**. The inner term, $\mathbb{E}[Y \mid \mathcal{K}]$, is the expected value of the QoI for a fixed set of epistemic choices. The outer variance then measures how much this expected prediction changes as we vary our epistemic choices according to their assessed uncertainty.

This decomposition provides a powerful conceptual and practical tool. For instance, in a Bayesian setting where we update our knowledge about a parameter $\theta$ with data $D_n$, the epistemic uncertainty term, $\mathrm{Var}_{p(\theta \mid D_n)}(\mathbb{E}[Y \mid \theta])$, will decrease as the amount of data $n$ increases and the posterior distribution $p(\theta \mid D_n)$ becomes more concentrated. In contrast, the aleatoric term, $\mathbb{E}_{p(\theta \mid D_n)}[\mathrm{Var}(Y \mid \theta)]$, will converge to a non-zero value representing the system's intrinsic randomness .

### A Deeper Look at Epistemic Uncertainty in Multiscale Models

While the aleatoric-epistemic dichotomy is fundamental, it is often necessary to further categorize the sources of epistemic uncertainty, as they arise from different modeling stages and are treated with different techniques. In multiscale modeling, we commonly distinguish between parametric, structural, and [numerical uncertainty](@entry_id:752838).

**Parametric Uncertainty** refers to the lack of knowledge about specific, fixed parameters appearing in a chosen model. For a multiscale diffusion problem governed by a stochastic PDE, this might include hyperparameters $\phi$ that control the [correlation length](@entry_id:143364) or variance of the random coefficient field , or microstructural descriptors $\theta$ like volume fraction or cell geometry . This uncertainty is typically represented by assigning a probability distribution (a prior, in the Bayesian sense) to the finite-dimensional parameter vector.

**Structural Uncertainty**, also known as **[model-form uncertainty](@entry_id:752061)**, is a more profound form of epistemic uncertainty that arises from our ignorance about the true form of the model itself. In [multiscale analysis](@entry_id:1128330), a primary source of structural uncertainty is the **homogenization error**: the discrepancy between the true, fine-scale physics and the approximate, upscaled model. The very act of replacing a complex, heterogeneous system with a simplified effective medium model is an assumption. The validity of this assumption often rests on conditions like **scale separation**, where microscopic and macroscopic length scales are vastly different . When these conditions are not perfectly met, or when the chosen upscaling map (e.g., the boundary conditions on a [representative volume element](@entry_id:164290)) is not uniquely dictated by physics, a [structural error](@entry_id:1132551) is introduced . This type of uncertainty can be mathematically represented in sophisticated ways, such as by introducing a random operator or a stochastic discrepancy term into the model equations .

**Numerical Uncertainty** arises from the computational implementation of the mathematical model. It encompasses all errors introduced by approximating continuous problems with discrete algorithms. Key examples include:
*   **Discretization error**, resulting from the use of a finite mesh (with characteristic size $h$) in finite element or [finite difference methods](@entry_id:147158). This is an epistemic uncertainty because it can be systematically reduced by refining the mesh ($h \to 0$) .
*   **Statistical sampling error**, which occurs when expectations are estimated using a finite number of samples $N$ in methods like Monte Carlo. This error is also epistemic, as it can be reduced by increasing the sample size ($N \to \infty$) .

These distinct error sources can be managed in a unified framework. Consider estimating the expected value of a QoI, $\mu = \mathbb{E}[J(u^\varepsilon)]$. A typical multiscale workflow computes a Monte Carlo estimate $\widehat{\mu}_S$ based on a discretized, homogenized solution $u_h^H$. The total error can be decomposed via a telescoping sum into its constituent parts :
$$
\mu - \widehat{\mu}_S = \underbrace{\left(\mathbb{E}[J(u^\varepsilon)] - \mathbb{E}[J(u^H)]\right)}_{\text{Homogenization Error}} + \underbrace{\left(\mathbb{E}[J(u^H)] - \mathbb{E}[J(u_h^H)]\right)}_{\text{Discretization Error}} + \underbrace{\left(\mathbb{E}[J(u_h^H)] - \widehat{\mu}_S\right)}_{\text{Sampling Error}}
$$
This decomposition is not just a theoretical exercise. **A posteriori error estimators** can be designed to provide computable bounds for each term, allowing for an adaptive refinement strategy where computational effort is directed toward reducing the dominant source of error.

### Forward Uncertainty Propagation: From Inputs to Outputs

Forward UQ is the process of determining the uncertainty in a model's output, given a characterization of the uncertainty in its inputs.

The process is founded on the measure-theoretic concept of the **[pushforward measure](@entry_id:201640)**. Let us represent our uncertain inputs by a parameter vector $\theta$ residing in a parameter space $\Theta$, with our uncertainty about $\theta$ encoded in a probability measure $\mu_\Theta$. The computational model can be viewed as a measurable map $f: \Theta \to \mathcal{Y}$ that transforms inputs $\theta$ into outputs $y \in \mathcal{Y}$. The uncertainty in the output space is then described by the [pushforward measure](@entry_id:201640) $\mu_\mathcal{Y} = f_* \mu_\Theta$, which is defined for any [measurable set](@entry_id:263324) of outputs $B \subset \mathcal{Y}$ as:
$$
\mu_\mathcal{Y}(B) = \mu_\Theta(f^{-1}(B))
$$
where $f^{-1}(B) = \{\theta \in \Theta \mid f(\theta) \in B\}$ is the [preimage](@entry_id:150899) of $B$. This definition simply states that the probability of the output landing in set $B$ is equal to the probability of the input being in the set of all parameters that map into $B$ . A key consequence is the change of variables formula for expectations, which allows us to compute expected values of functions of the output, $\varphi(y)$, by integrating over the input space:
$$
\int_{\mathcal{Y}} \varphi(y) \,d\mu_{\mathcal{Y}}(y) = \int_{\Theta} \varphi(f(\theta)) \,d\mu_{\Theta}(\theta)
$$
This identity is the theoretical basis for many computational UQ methods. In practice, we employ various numerical techniques to approximate the [pushforward measure](@entry_id:201640) or its statistical moments.

A straightforward but often computationally intensive method is the **Monte Carlo (MC)** method, which involves drawing many samples from the input distribution $\mu_\Theta$, running the forward model for each sample, and collecting the outputs to approximate the output distribution. Its slow convergence rate motivates the development of more sophisticated techniques.

One of the most powerful techniques is **Generalized Polynomial Chaos (gPC)**. This method seeks to represent the model output $Y(\theta, \omega)$ as a spectral expansion in terms of polynomials that are orthogonal with respect to the input probability measure $\mu_\theta$. For a model depending on a $d$-dimensional parameter vector $\theta$, the expansion takes the form :
$$
Y(\theta, \omega) = \sum_{\alpha \in \mathbb{N}^d} y_\alpha(\omega) \Psi_\alpha(\theta)
$$
Here, $\{\Psi_\alpha\}$ is a basis of multivariate polynomials orthonormal with respect to $\mu_\theta$. If the components of $\theta$ are independent, these are often tensor products of classical univariate [orthogonal polynomials](@entry_id:146918) from the Wiener-Askey scheme (e.g., Hermite polynomials for Gaussian inputs, Legendre for uniform). The coefficients $y_\alpha(\omega)$ are determined by [orthogonal projection](@entry_id:144168):
$$
y_\alpha(\omega) = \int_{\Theta} Y(\theta, \omega) \Psi_\alpha(\theta) \,d\mu_\theta(\theta)
$$
The great advantage of gPC is its potential for **[spectral convergence](@entry_id:142546)**: if the model response $Y(\cdot, \omega)$ is analytic with respect to the parameters $\theta$, the error of the truncated gPC expansion can decay exponentially with the polynomial order, far outpacing the algebraic convergence of MC methods.

For multiscale problems where model evaluations are expensive and come in a natural hierarchy of fidelities and costs, the **Multilevel Monte Carlo (MLMC)** method is particularly effective. Let $\{Q_\ell\}_{\ell=0}^L$ be a hierarchy of approximations to a QoI, where level $\ell$ is more accurate but more costly to compute than level $\ell-1$. MLMC recasts the problem of estimating the expectation of the finest-level approximation, $\mathbb{E}[Q_L]$, as a telescoping sum :
$$
\mathbb{E}[Q_L] = \mathbb{E}[Q_0] + \sum_{\ell=1}^L \mathbb{E}[Q_\ell - Q_{\ell-1}]
$$
The insight is that the variance of the correction term, $\mathrm{Var}(Q_\ell - Q_{\ell-1})$, decreases as the level $\ell$ increases because the two approximations become more correlated. The MLMC estimator then involves estimating the expectation of each correction term independently. By optimally allocating computational effort—many samples for the cheap, low-variance high-level corrections and few samples for the expensive, high-variance low-level terms—MLMC can achieve a desired accuracy for a fraction of the cost of a standard Monte Carlo simulation at the highest fidelity level. The optimal number of samples $N_\ell^\star$ at each level, for a target total variance $\varepsilon^2$ and with level costs $C_\ell$ and correction variances $V_\ell = \mathrm{Var}(Q_\ell - Q_{\ell-1})$, is given by:
$$
N_{\ell}^{\star} = \frac{1}{\varepsilon^{2}} \sqrt{\frac{V_{\ell}}{C_{\ell}}} \sum_{k=0}^{L} \sqrt{V_{k} C_{k}}
$$

### Sensitivity Analysis: Identifying Key Uncertainty Drivers

After propagating uncertainty, a critical task is to perform sensitivity analysis to apportion the output uncertainty to the various input sources. **Global Sensitivity Analysis (GSA)** aims to do this over the entire range of input variability. The gold standard for GSA is the variance-based method of **Sobol' indices**.

This method relies on the **Analysis of Variance (ANOVA)** decomposition of the model function $f(\boldsymbol{\theta})$, which is valid when the input parameters $\boldsymbol{\theta} = (\theta_1, \dots, \theta_d)$ are independent. The function is decomposed into a sum of terms of increasing dimensionality :
$$
f(\boldsymbol{\theta}) = f_0 + \sum_{i=1}^d f_i(\theta_i) + \sum_{1 \le i  j \le d} f_{ij}(\theta_i, \theta_j) + \cdots + f_{1\cdots d}(\boldsymbol{\theta})
$$
where $f_0 = \mathbb{E}[Y]$ is the mean, $f_i(\theta_i)$ are the first-order effects, $f_{ij}(\theta_i, \theta_j)$ are the second-order interaction effects, and so on. A key property of this decomposition is that the summands are orthogonal, which leads to an [additive decomposition](@entry_id:1120795) of the total variance $\mathrm{Var}(Y)$:
$$
\mathrm{Var}(Y) = \sum_{i=1}^d V_i + \sum_{1 \le i  j \le d} V_{ij} + \cdots + V_{1\cdots d}
$$
where $V_u = \mathrm{Var}(f_u(\boldsymbol{\theta}_u))$ are the partial variances.

Sobol' indices are simply the partial variances normalized by the total variance.
The **first-order Sobol' index** for input $\theta_i$ is:
$$
S_i = \frac{V_i}{\mathrm{Var}(Y)} = \frac{\mathrm{Var}(\mathbb{E}[Y \mid \theta_i])}{\mathrm{Var}(Y)}
$$
This measures the fraction of the total output variance that can be attributed to the variation in $\theta_i$ alone. It quantifies the "main effect" of the parameter.

The **total-effect Sobol' index** for input $\theta_i$ is:
$$
S_{T_i} = \frac{\sum_{u \ni i} V_u}{\mathrm{Var}(Y)} = \frac{\mathbb{E}[\mathrm{Var}(Y \mid \boldsymbol{\theta}_{\sim i})]}{\mathrm{Var}(Y)} = 1 - \frac{\mathrm{Var}(\mathbb{E}[Y \mid \boldsymbol{\theta}_{\sim i}])}{\mathrm{Var}(Y)}
$$
where $\boldsymbol{\theta}_{\sim i}$ denotes all input parameters except for $\theta_i$. The [total-effect index](@entry_id:1133257) $S_{T_i}$ accounts for the main effect of $\theta_i$ plus all interaction effects involving $\theta_i$. It measures the total contribution of $\theta_i$ to the output variance. A large difference between $S_{T_i}$ and $S_i$ indicates that the parameter $\theta_i$ is heavily involved in interactions with other parameters.

### Connecting Theory to Practice: The Representative Volume Element

Many of the principles discussed can be illustrated through the practical application of the **Representative Volume Element (RVE)** in [computational homogenization](@entry_id:163942). The theoretical concept of homogenization assumes an infinite separation of scales, leading to a deterministic effective property. In practice, we compute an estimate of this property by solving [boundary value problems](@entry_id:137204) on a [finite domain](@entry_id:176950)—the RVE.

The use of a finite-size RVE introduces an epistemic uncertainty: a [statistical error](@entry_id:140054) arising from sampling only a finite portion of an infinite random medium. If we consider a stationary and ergodic microscopic response field $Q(x, \omega)$, the RVE estimator is simply its spatial average over the RVE domain $D_L$ of size $L$ :
$$
\hat{\theta}_L(\omega) := \frac{1}{|D_L|} \int_{D_L} Q(x,\omega)\, \mathrm{d}x
$$
This estimator is itself a random variable, fluctuating with the specific microstructural realization $\omega$ contained within the RVE. Its uncertainty can be quantified by its variance. For a random field with [short-range correlations](@entry_id:158693) described by a covariance function $C(r)$, the variance of the RVE estimator for large $L$ behaves as:
$$
\mathrm{Var}[\hat{\theta}_L] \approx \frac{1}{|D_L|} \int_{\mathbb{R}^d} C(r)\, \mathrm{d}r
$$
Since the volume $|D_L|$ scales as $L^d$ in $d$ spatial dimensions, this implies $\mathrm{Var}[\hat{\theta}_L] = O(L^{-d})$. This result is powerful: it not only confirms that the uncertainty in the RVE estimate decreases as the RVE gets larger (as expected from the law of large numbers), but it also quantifies the rate of this decrease. This provides a direct, quantitative link between a modeling choice (the size $L$ of the RVE) and the magnitude of the resulting epistemic uncertainty.