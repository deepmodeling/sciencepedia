{
    "hands_on_practices": [
        {
            "introduction": "这项首个实践是应用两种最广泛使用的信息准则（AIC和BIC）的基础练习。通过一个比较两个模型的假设情景（给定对数似然和参数数量），你将直接计算和对比这些准则。该练习旨在巩固你对AIC和BIC如何量化模型拟合优度与复杂度之间权衡的理解，以及它们不同的惩罚项如何导致不同的模型偏好 。",
            "id": "3780497",
            "problem": "在多尺度建模与分析中，比较在系统不同有效分辨率下运行的机理模型是很常见的。考虑两个候选的粗粒化随机模型，记为 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$，每个模型都旨在描述由一个潜在的快慢过程生成的慢流形统计特性。两个模型都是在证明渐近近似合理性的标准正则性条件下，通过对 $n$ 个独立观测值进行最大似然估计得到的。\n\n你需要使用两个基于第一性原理的信息论准则来比较这两个模型：\n\n- 一个准则是通过近似真实数据生成过程与拟合模型之间的预期样本外 Kullback–Leibler 散度得出的，它使用了一个基于最大化对数似然和与自由参数数量成正比的显式复杂性惩罚的偏差校正估计量。\n- 另一个准则是通过 Laplace 近似来近似每个模型下数据的负二倍对数边际似然得出的，这产生了一个与样本大小的对数和模型维度成比例的复杂性惩罚。\n\n设 $\\ell_{1}$ 和 $\\ell_{2}$ 分别表示在大小为 $n$ 的同一数据集上计算出的 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 的最大化对数似然。设 $k_{1}$ 和 $k_{2}$ 分别表示 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 中的自由参数数量。假设\n$$\n\\ell_{1}=-120,\\quad \\ell_{2}=-118,\\quad k_{1}=4,\\quad k_{2}=6,\\quad n=200.\n$$\n\n使用上述隐含的渐近定义，计算 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 的两个准则，并根据通常的惯例（值越小表示拟合度和复杂性之间的权衡越好）确定每个准则偏好哪个模型。将你的最终答案报告为单个行向量 $\\big[\\mathrm{AIC}_{1},\\ \\mathrm{AIC}_{2},\\ \\mathrm{BIC}_{1},\\ \\mathrm{BIC}_{2},\\ p_{\\mathrm{AIC}},\\ p_{\\mathrm{BIC}}\\big]$，其中，如果 Akaike 信息准则 (AIC) 偏好 $\\mathcal{M}_{1}$，则 $p_{\\mathrm{AIC}}$ 等于 $1$；如果偏好 $\\mathcal{M}_{2}$，则等于 $2$；如果两者持平，则等于 $0$；$p_{\\mathrm{BIC}}$ 对贝叶斯信息准则 (BIC) 的定义类似。仅使用包含 $\\ln$ 和整数的精确符号表达式；不要进行近似或四舍五入。",
            "solution": "问题要求为两个竞争模型 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 计算两个信息准则，并确定每个准则偏好哪个模型。这些准则由其理论推导来描述。\n\n首先，我们必须根据所提供的描述确定这些准则的具体公式。\n\n第一个准则被描述为“通过近似真实数据生成过程与拟合模型之间的预期样本外 Kullback–Leibler 散度得出的，它使用了一个基于最大化对数似然和与自由参数数量成正比的显式复杂性惩罚的偏差校正估计量。” 这是 Akaike 信息准则 (AIC) 的定义。其标准公式为：\n$$\n\\mathrm{AIC} = 2k - 2\\ell\n$$\n其中 $k$ 是模型中的自由参数数量，$\\ell$ 是对数似然函数的最大化值。\n\n第二个准则被描述为“通过 Laplace 近似来近似每个模型下数据的负二倍对数边际似然得出的，这产生了一个与样本大小的对数和模型维度成比例的复杂性惩罚。” 这是贝叶斯信息准则 (BIC) 的定义，也称为 Schwarz 准则。其公式为：\n$$\n\\mathrm{BIC} = k\\ln(n) - 2\\ell\n$$\n其中 $n$ 是观测数量（样本大小）。\n\n对于 AIC 和 BIC，惯例是值越小表示模型越好，代表了拟合优度（高 $\\ell$）和模型复杂性（低 $k$）之间更有利的权衡。\n\n问题提供了以下值：\n- 对于模型 $\\mathcal{M}_{1}$：最大化对数似然 $\\ell_{1} = -120$ 和参数数量 $k_{1} = 4$。\n- 对于模型 $\\mathcal{M}_{2}$：最大化对数似然 $\\ell_{2} = -118$ 和参数数量 $k_{2} = 6$。\n- 样本大小为 $n=200$。\n\n现在，我们计算每个模型的 AIC。\n对于模型 $\\mathcal{M}_{1}$：\n$$\n\\mathrm{AIC}_{1} = 2k_{1} - 2\\ell_{1} = 2(4) - 2(-120) = 8 + 240 = 248.\n$$\n对于模型 $\\mathcal{M}_{2}$：\n$$\n\\mathrm{AIC}_{2} = 2k_{2} - 2\\ell_{2} = 2(6) - 2(-118) = 12 + 236 = 248.\n$$\n为了根据 AIC 确定偏好的模型，我们比较 $\\mathrm{AIC}_{1}$ 和 $\\mathrm{AIC}_{2}$。由于 $\\mathrm{AIC}_{1} = \\mathrm{AIC}_{2} = 248$，两个模型没有优劣之分。这被认为是一个平局。根据问题的定义，我们设 $p_{\\mathrm{AIC}} = 0$。\n\n接下来，我们计算每个模型的 BIC。\n对于模型 $\\mathcal{M}_{1}$：\n$$\n\\mathrm{BIC}_{1} = k_{1}\\ln(n) - 2\\ell_{1} = 4\\ln(200) - 2(-120) = 240 + 4\\ln(200).\n$$\n对于模型 $\\mathcal{M}_{2}$：\n$$\n\\mathrm{BIC}_{2} = k_{2}\\ln(n) - 2\\ell_{2} = 6\\ln(200) - 2(-118) = 236 + 6\\ln(200).\n$$\n问题要求答案以精确的符号形式表示，所以我们保持这些表达式不变。\n\n为了根据 BIC 确定偏好的模型，我们比较 $\\mathrm{BIC}_{1}$ 和 $\\mathrm{BIC}_{2}$。让我们研究它们的差值：\n$$\n\\mathrm{BIC}_{2} - \\mathrm{BIC}_{1} = \\big(236 + 6\\ln(200)\\big) - \\big(240 + 4\\ln(200)\\big) = (236 - 240) + (6-4)\\ln(200) = -4 + 2\\ln(200).\n$$\n为了确定这个差值的符号，我们需要比较 $2\\ln(200)$ 和 $4$，这等价于比较 $\\ln(200)$ 和 $2$。自然对数函数 $\\ln(x)$ 是单调递增的。常数 $e$ 约等于 $2.718$，所以 $e^{2} \\approx (2.718)^{2} \\approx 7.389$。由于 $200 > e^{2}$，因此 $\\ln(200) > \\ln(e^{2})$，这简化为 $\\ln(200) > 2$。\n因此，$2\\ln(200) > 4$，这意味着差值 $-4 + 2\\ln(200)$ 是正的。\n所以，$\\mathrm{BIC}_{2} - \\mathrm{BIC}_{1} > 0$，这意味着 $\\mathrm{BIC}_{2} > \\mathrm{BIC}_{1}$。\n由于 BIC 值较小的模型更优，因此 BIC 偏好模型 $\\mathcal{M}_{1}$。根据问题的定义，我们设 $p_{\\mathrm{BIC}} = 1$。\n\n最后，我们组装所需的行向量：\n$$\n\\big[\\mathrm{AIC}_{1},\\ \\mathrm{AIC}_{2},\\ \\mathrm{BIC}_{1},\\ \\mathrm{BIC}_{2},\\ p_{\\mathrm{AIC}},\\ p_{\\mathrm{BIC}}\\big].\n$$\n代入计算出的值：\n- $\\mathrm{AIC}_{1} = 248$\n- $\\mathrm{AIC}_{2} = 248$\n- $\\mathrm{BIC}_{1} = 240 + 4\\ln(200)$\n- $\\mathrm{BIC}_{2} = 236 + 6\\ln(200)$\n- $p_{\\mathrm{AIC}} = 0$\n- $p_{\\mathrm{BIC}} = 1$\n\n最终的向量是 $[248, 248, 240 + 4\\ln(200), 236 + 6\\ln(200), 0, 1]$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n248 & 248 & 240 + 4\\ln(200) & 236 + 6\\ln(200) & 0 & 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在基础之上，本实践将模型选择准则应用于时间序列分析中的一个常见场景：选择自回归（AR）模型的阶数。你不仅需要计算AIC和BIC，还需要计算修正的赤池信息准则（AICc），这对于小样本量至关重要。通过展示如何比较一组嵌套模型并解释不同准则之间可能出现分歧的原因，本练习将深化你的实践技能，这是任何建模工作中都不可或缺的关键能力 。",
            "id": "3780541",
            "problem": "在多尺度时间序列模型中，单一分辨率下的单变量高斯自回归过程通常被用作粗粒度分量。考虑对一个长度为 $n=300$ 的序列拟合一个阶为 $p$ 的自回归模型，记作 $\\mathrm{AR}(p)$，其中 $p\\in\\{1,2,3\\}$。对于每次拟合，假设参数族包含 $p$ 个自回归系数、一个截距项和一个新息方差，因此自由参数的总数为 $k=p+2$。在高斯新息假设下，三次拟合的最大化对数似然值分别为：$p=1$ 时为 $-210$，$p=2$ 时为 $-205$，$p=3$ 时为 $-203$。使用最大似然估计和信息论模型比较的基本原理，计算每个 $p\\in\\{1,2,3\\}$ 对应的赤池信息准则 (AIC)、小样本修正的赤池信息准则 (AICc) 和贝叶斯信息准则 (BIC) 的值，并简要解释这些准则在多尺度建模中对模型选择的不同启示。将每个准则值四舍五入至四位有效数字，并以无单位的最终数值答案表示。将你的最终答案表示为一个行矩阵，其元素按顺序为\n$$\\text{AIC}(p=1),\\ \\text{AICc}(p=1),\\ \\text{BIC}(p=1),\\ \\text{AIC}(p=2),\\ \\text{AICc}(p=2),\\ \\text{BIC}(p=2),\\ \\text{AIC}(p=3),\\ \\text{AICc}(p=3),\\ \\text{BIC}(p=3).$$",
            "solution": "在尝试求解之前，对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n-   **过程**：单变量高斯自回归过程。\n-   **模型**：阶为 $p$ 的自回归模型，记为 $\\mathrm{AR}(p)$。\n-   **样本大小**：$n = 300$。\n-   **考虑的模型阶数**：$p \\in \\{1, 2, 3\\}$。\n-   **自由参数数量**：$k = p + 2$。这包括 $p$ 个自回归系数、一个截距项和一个新息方差。\n-   **最大化对数似然** ($\\ln(L)$)：\n    -   当 $p=1$ 时，$\\ln(L_1) = -210$。\n    -   当 $p=2$ 时，$\\ln(L_2) = -205$。\n    -   当 $p=3$ 时，$\\ln(L_3) = -203$。\n-   **要求计算**：每个 $p$ 对应的赤池信息准则 (AIC)、小样本修正的赤池信息准则 (AICc) 和贝叶斯信息准则 (BIC)。\n-   **要求输出格式**：一个包含九个计算出的准则值的行矩阵，四舍五入至四位有效数字。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据，提法恰当且客观。它提供了计算标准的、定义明确的统计指标所需的完整且一致的数据和定义集。该情景是时间序列分析中模型选择的典型应用，而时间序列分析是多尺度建模的基础组成部分。所提供的样本大小、模型阶数和对数似然值是合理的。该问题不违反任何科学原理，没有歧义，并且可以使用已建立的范式直接求解。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整的解答。\n\n### 解答推导\n任务是为三个不同的自回归模型计算三种信息准则，并解释结果。这些准则的定义如下：\n\n1.  **赤池信息准则 (AIC)**:\n    $$ \\mathrm{AIC} = 2k - 2\\ln(L) $$\n    其中 $k$ 是估计参数的数量，$\\ln(L)$ 是最大化对数似然。\n\n2.  **修正的赤池信息准则 (AICc)**:\n    $$ \\mathrm{AICc} = \\mathrm{AIC} + \\frac{2k(k+1)}{n-k-1} $$\n    其中 $n$ 是样本大小。该准则对小样本情况下的 AIC 进行了修正。\n\n3.  **贝叶斯信息准则 (BIC)**:\n    $$ \\mathrm{BIC} = k\\ln(n) - 2\\ln(L) $$\n    该准则对模型复杂度的惩罚比 AIC 更强，尤其是在 $n$ 较大时。\n\n已知 $n=300$ 且 $k=p+2$。我们为每个 $p \\in \\{1, 2, 3\\}$ 计算这些准则。\n\n**情况 1：模型 AR(1)，$p=1$**\n-   参数数量：$k_1 = 1 + 2 = 3$。\n-   最大化对数似然：$\\ln(L_1) = -210$。\n\n-   $\\mathrm{AIC}_1 = 2k_1 - 2\\ln(L_1) = 2(3) - 2(-210) = 6 + 420 = 426$。\n    四舍五入到四位有效数字得到 $426.0$。\n-   $\\mathrm{AICc}_1 = \\mathrm{AIC}_1 + \\frac{2k_1(k_1+1)}{n-k_1-1} = 426 + \\frac{2(3)(3+1)}{300-3-1} = 426 + \\frac{24}{296} \\approx 426 + 0.081081... = 426.081081...$。\n    四舍五入到四位有效数字得到 $426.1$。\n-   $\\mathrm{BIC}_1 = k_1\\ln(n) - 2\\ln(L_1) = 3\\ln(300) - 2(-210) = 3\\ln(300) + 420 \\approx 3(5.70378) + 420 = 17.11134 + 420 = 437.11134...$。\n    四舍五入到四位有效数字得到 $437.1$。\n\n**情况 2：模型 AR(2)，$p=2$**\n-   参数数量：$k_2 = 2 + 2 = 4$。\n-   最大化对数似然：$\\ln(L_2) = -205$。\n\n-   $\\mathrm{AIC}_2 = 2k_2 - 2\\ln(L_2) = 2(4) - 2(-205) = 8 + 410 = 418$。\n    四舍五入到四位有效数字得到 $418.0$。\n-   $\\mathrm{AICc}_2 = \\mathrm{AIC}_2 + \\frac{2k_2(k_2+1)}{n-k_2-1} = 418 + \\frac{2(4)(4+1)}{300-4-1} = 418 + \\frac{40}{295} \\approx 418 + 0.135593... = 418.135593...$。\n    四舍五入到四位有效数字得到 $418.1$。\n-   $\\mathrm{BIC}_2 = k_2\\ln(n) - 2\\ln(L_2) = 4\\ln(300) - 2(-205) = 4\\ln(300) + 410 \\approx 4(5.70378) + 410 = 22.81512 + 410 = 432.81512...$。\n    四舍五入到四位有效数字得到 $432.8$。\n\n**情况 3：模型 AR(3)，$p=3$**\n-   参数数量：$k_3 = 3 + 2 = 5$。\n-   最大化对数似然：$\\ln(L_3) = -203$。\n\n-   $\\mathrm{AIC}_3 = 2k_3 - 2\\ln(L_3) = 2(5) - 2(-203) = 10 + 406 = 416$。\n    四舍五入到四位有效数字得到 $416.0$。\n-   $\\mathrm{AICc}_3 = \\mathrm{AIC}_3 + \\frac{2k_3(k_3+1)}{n-k_3-1} = 416 + \\frac{2(5)(5+1)}{300-5-1} = 416 + \\frac{60}{294} \\approx 416 + 0.204081... = 416.204081...$。\n    四舍五入到四位有效数字得到 $416.2$。\n-   $\\mathrm{BIC}_3 = k_3\\ln(n) - 2\\ln(L_3) = 5\\ln(300) - 2(-203) = 5\\ln(300) + 406 \\approx 5(5.70378) + 406 = 28.51890 + 406 = 434.51890...$。\n    四舍五入到四位有效数字得到 $434.5$。\n\n### 解释\n使用这些准则进行模型选择的原则是选择准则值最小的模型。\n\n-   **AIC 和 AICc 的选择**：\n    -   $\\mathrm{AIC}$ 值：$\\mathrm{AIC}_1 = 426.0$, $\\mathrm{AIC}_2 = 418.0$, $\\mathrm{AIC}_3 = 416.0$。\n    -   $\\mathrm{AICc}$ 值：$\\mathrm{AICc}_1 = 426.1$, $\\mathrm{AICc}_2 = 418.1$, $\\mathrm{AICc}_3 = 416.2$。\n    AIC 和 AICc 都选择了 $\\mathrm{AR}(3)$ 模型 ($p=3$)，因为它具有最低的值。AICc 的修正量很小，因为样本大小 $n=300$ 相对于参数数量 $k$（最多为 $5$）来说很大，所以 $n-k-1$ 不小。具体来说，比率 $k/n$ 很小。\n\n-   **BIC 的选择**：\n    -   $\\mathrm{BIC}$ 值：$\\mathrm{BIC}_1 = 437.1$, $\\mathrm{BIC}_2 = 432.8$, $\\mathrm{BIC}_3 = 434.5$。\n    BIC 选择了 $\\mathrm{AR}(2)$ 模型 ($p=2$)，因为它对应于最小的 BIC 值。\n\n-   **比较与启示**：\n    模型选择上的差异源于对模型复杂度的不同惩罚项。AIC 的惩罚项是 $2k$，而 BIC 的惩罚项是 $k\\ln(n)$。对于 $n=300$，$\\ln(300) \\approx 5.70$。因此，BIC 对每个参数的惩罚（$\\approx 5.70$）远大于 AIC 的惩罚（$2$）。这使得 BIC 倾向于更简约（更简单）的模型。\n    在本例中，从 $\\mathrm{AR}(2)$ 模型转到 $\\mathrm{AR}(3)$ 模型将对数似然从 $-205$ 提高到 $-203$。对于 AIC，这种拟合度的提升（$-2\\ln(L)$ 项减少了 $2\\times(-203 - (-205)) = 4$）超过了惩罚的增加（$2(k_3-k_2) = 2(1) = 2$）。而对于 BIC，同样的拟合度提升不足以克服更大的惩罚增加（$(k_3-k_2)\\ln(n) \\approx 5.70$）。\n    在多尺度建模的背景下，这凸显了一个基本的权衡。在每个尺度上选择分量模型（例如，这里的粗粒度 AR 过程）涉及到在保真度和复杂度之间取得平衡。如果目标是预测准确性，可能会首选 AIC，因为它可以选择能捕捉更精细细节的、稍微复杂的模型。如果目标是识别最可能的潜在生成结构，避免在每个尺度上发生过拟合，那么可能会选择偏好简约性的 BIC，这对于整体多尺度模型的稳定性和可解释性至关重要。\n\n行矩阵的最终值为：\n-   $p=1$：$426.0$, $426.1$, $437.1$\n-   $p=2$：$418.0$, $418.1$, $432.8$\n-   $p=3$：$416.0$, $416.2$, $434.5$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n426.0 & 426.1 & 437.1 & 418.0 & 418.1 & 432.8 & 416.0 & 416.2 & 434.5\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "这个综合性实践将从公式应用转向计算实现，模拟了现代建模研究中的一项核心任务。你将进行一项敏感性分析，以评估模型选择决策对于准则选择、惩罚强度和先验假设的稳健性。通过实现一个基于似然的和一个基于贝叶斯后验的选择框架，你将获得评估结论稳定性的实践经验，这对于开发可靠且可辩护的多尺度模型至关重要 。",
            "id": "3780533",
            "problem": "考虑一个综合性的双尺度线性回归设置，其中响应信号由一个粗尺度分量和一个细尺度分量的叠加以及加性高斯噪声生成。设存在 $n$ 个观测值，索引为 $t \\in \\{1,2,\\dots,n\\}$，并定义一个归一化时间坐标 $u_t = \\frac{t-1}{n-1} \\in [0,1]$。通过 $x_c(t) = \\sin(2\\pi C u_t)$ 和 $x_f(t) = \\sin(2\\pi F u_t)$ 构建两个预测变量，其中 $C$ 和 $F$ 是正整数且 $C \\ll F$，以表示粗、细时间尺度。响应按如下方式生成：$y(t) = \\beta_0 + \\beta_c x_c(t) + \\beta_f x_f(t) + \\epsilon_t$，其中 $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ 是独立同分布的。考虑用于选择的候选模型是：\n- $M_1$：仅截距项，即 $y(t) = \\beta_0 + \\epsilon_t$；\n- $M_2$：仅粗尺度，即 $y(t) = \\beta_0 + \\beta_c x_c(t) + \\epsilon_t$；\n- $M_3$：仅细尺度，即 $y(t) = \\beta_0 + \\beta_f x_f(t) + \\epsilon_t$；\n- $M_4$：粗尺度加细尺度，即 $y(t) = \\beta_0 + \\beta_c x_c(t) + \\beta_f x_f(t) + \\epsilon_t$。\n\n您的任务是实现一个敏感性分析，通过改变惩罚参数和先验假设来评估模型选择决策在不同尺度上的稳健性。程序必须构建所述的数据集，并使用两个源于第一性原理的互补准则来选择模型：\n\n1. 一个基于似然的、带有可调惩罚项的信息准则，其定义始于在噪声方差未知情况下的高斯对数似然，并加上一个与估计参数数量成线性关系的惩罚项。对于每个模型，设 $p$ 表示估计的回归系数数量（包括截距项），并将方差 $\\sigma^2$ 视为一个额外的待估参数。定义一个依赖于用户指定的惩罚参数 $\\alpha > 0$ 的广义信息分数，并用它来选择能在拟合优度和复杂性之间达到最佳平衡的模型。\n\n2. 一个基于贝叶斯后验的分数，该分数对回归系数采用高斯先验以反映关于尺度的先验假设：为粗尺度系数分配一个均值为零、方差为 $\\tau_c^2$ 的高斯先验，为细尺度系数分配一个均值为零、方差为 $\\tau_f^2$ 的高斯先验，同时将截距项视为无惩罚的。对于此贝叶斯准则，假设噪声方差 $\\sigma^2$ 已知。通过结合高斯对数似然和高斯对数先验来推导一个基于后验的分数，并包含一个可调的复杂性惩罚参数 $\\alpha$ 作为独立的稳健性控制。选择使该后验分数最小化的模型。\n\n使用的基本原理：\n- 对于均值为 $\\mu$、方差为 $\\sigma^2$ 的标量观测值，高斯概率密度函数为 $p(y \\mid \\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$，这意味着独立高斯误差的对数似然是各个对数密度的总和。\n- 在具有高斯误差的线性回归中，回归系数的最大似然估计 (MLE) 会最小化残差平方和，当 $\\sigma^2$ 未知时，其 MLE 等于残差均方。\n- 对于高斯先验 $p(\\beta_j) = \\mathcal{N}(0,\\tau_j^2)$，对数先验由 $\\ln p(\\beta_j) = -\\frac{1}{2}\\ln(2\\pi\\tau_j^2) - \\frac{\\beta_j^2}{2\\tau_j^2}$ 给出，最大后验 (MAP) 估计会最小化数据拟合项和二次正则化项之和。\n\n您的程序必须：\n- 使用以下固定参数生成数据集：$n = 200$, $C = 2$, $F = 15$, $\\beta_0 = 0$, $\\beta_c = 1$, $\\beta_f = 0.3$, and $\\sigma = 0.2$。使用确定性伪随机生成器种子，以使结果可复现。\n- 对于从 $M_1$ 到 $M_4$ 的每个候选模型，在 $\\sigma^2$ 未知的情况下计算 MLE，拟合值对应的高斯对数似然，以及广义的基于似然的信息分数，其惩罚项为 $\\alpha$ 乘以总估计参数数量 $k = p + 1$，其中 $p$ 是回归系数的数量，额外的 $1$ 用于解释未知的方差参数。\n- 对于从 $M_1$ 到 $M_4$ 的每个候选模型，在 $\\sigma^2$ 已知的情况下计算 MAP 估计，MAP 估计在已知方差下的相应高斯对数似然，以及模型中存在的被惩罚系数的高斯对数先验，然后通过将这些与惩罚项 $\\alpha$ 乘以回归系数数量 $p$ 相结合，计算基于后验的分数。\n- 对于每个测试案例，返回在基于似然的分数和基于后验的分数下所选模型的索引，以及一个指示选择是否一致的布尔值，即在给定设置下，选择对于先验的存在是否是稳健的。\n\n测试套件：\n在以下六个测试案例中评估敏感性，每个案例由三元组 $(\\alpha,\\tau_c^2,\\tau_f^2)$ 指定，数据集如上所述保持固定：\n- 案例 1：$(\\alpha = 2,\\ \\tau_c^2 = 10^6,\\ \\tau_f^2 = 10^6)$，表示 Akaike 型惩罚和无信息先验。\n- 案例 2：$(\\alpha = 0,\\ \\tau_c^2 = 10^6,\\ \\tau_f^2 = 10^6)$，表示无惩罚和无信息先验。\n- 案例 3：$(\\alpha = \\ln n,\\ \\tau_c^2 = 10^6,\\ \\tau_f^2 = 10^6)$，表示贝叶斯型惩罚幅度和无信息先验。\n- 案例 4：$(\\alpha = 2,\\ \\tau_c^2 = 10^6,\\ \\tau_f^2 = 10^{-4})$，表示在细尺度上的强先验收缩。\n- 案例 5：$(\\alpha = 2,\\ \\tau_c^2 = 10^{-4},\\ \\tau_f^2 = 10^6)$，表示在粗尺度上的强先验收缩。\n- 案例 6：$(\\alpha = \\ln n,\\ \\tau_c^2 = 10^{-6},\\ \\tau_f^2 = 10^{-6})$，表示在强惩罚下两个尺度上均有强先验收缩。\n\n最终输出格式：\n您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表形式的结果，其中每个元素对应一个测试案例，并且本身是一个形式为 $[m_{\\text{like}}, m_{\\text{post}}, r]$ 的列表，$m_{\\text{like}}$ 和 $m_{\\text{post}}$ 是 $\\{1,2,3,4\\}$ 中的整数，分别表示在基于似然和基于后验的分数下选择的模型，$r$ 是一个指示它们是否一致的布尔值。例如：$[[1,1,True],[4,4,True],\\dots]$。",
            "solution": "该问题要求在一个综合性双尺度线性回归设置中，对两种模型选择准则——一种基于似然，一种基于后验——进行比较分析。该分析涉及生成一个数据集，拟合四个复杂度递增的候选模型，并在各种超参数配置下评估它们的分数。\n\n首先，我们为线性模型建立通用的矩阵表示法。响应向量 $\\mathbf{y} \\in \\mathbb{R}^n$、设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$、系数向量 $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ 和误差向量 $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^n$ 之间的关系由下式给出：\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n$$\n其中 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I}_n)$。回归系数的数量 $p$ 取决于模型 $M_i$。$\\mathbf{X}$ 的列由截距项（一个全为1的向量）、粗尺度预测变量 $x_c(t)$ 和细尺度预测变量 $x_f(t)$ 构成，具体取决于每个模型 $M_1, M_2, M_3, M_4$ 的规定。数据使用固定参数 $n=200$, $C=2$, $F=15$ 以及真实系数 $\\beta_0=0, \\beta_c=1, \\beta_f=0.3$ 和噪声标准差 $\\sigma=0.2$ 生成。\n\n### 1. 基于似然的信息分数\n\n该准则基于最大似然估计 (MLE) 的原理，并对模型复杂度施加显式惩罚。对于一个给定的具有 $p$ 个回归系数的模型，我们将噪声方差 $\\sigma^2$ 视为一个额外的待估参数，使得估计参数总数为 $k = p+1$。\n\n给定参数 $\\boldsymbol{\\beta}$ 和 $\\sigma^2$，数据的对数似然函数为：\n$$\n\\mathcal{L}(\\boldsymbol{\\beta}, \\sigma^2; \\mathbf{y}, \\mathbf{X}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\n$\\boldsymbol{\\beta}$ 的最大似然估计是普通最小二乘 (OLS) 解，它最小化了残差平方和 (RSS)：\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n在此估计下的 RSS 为 $RSS = (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}})^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}})$。方差 $\\sigma^2$ 的 MLE 接着是：\n$$\n\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{RSS}{n}\n$$\n将这些估计值代回对数似然函数，得到最大化对数似然 $\\hat{\\mathcal{L}}$：\n$$\n\\hat{\\mathcal{L}} = \\mathcal{L}(\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}, \\hat{\\sigma}^2_{\\text{MLE}}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{RSS}{n}\\right) - \\frac{RSS}{2(RSS/n)} = -\\frac{n}{2}\\left(\\ln(2\\pi\\hat{\\sigma}^2_{\\text{MLE}}) + 1\\right)\n$$\n问题指定了一个广义信息分数。遵循 AIC 等准则的惯例，我们将要最小化的分数定义为：\n$$\nScore_{\\text{like}} = -2\\hat{\\mathcal{L}} + \\alpha \\cdot k = n\\left(\\ln(2\\pi\\hat{\\sigma}^2_{\\text{MLE}}) + 1\\right) + \\alpha(p+1)\n$$\n对于每个模型 $M_i$，我们计算这个分数。选择分数最小的模型。参数 $p$ 对于 $M_1$ 是 $1$，对于 $M_2$ 和 $M_3$ 是 $2$，对于 $M_4$ 是 $3$。\n\n### 2. 基于贝叶斯后验的分数\n\n该准则使用贝叶斯原理，融入了关于模型参数的先验信念。方差 $\\sigma^2$ 假定为已知（固定在真实值 $\\sigma^2=0.2^2=0.04$）。回归系数 $\\beta_c$ 和 $\\beta_f$ 被赋予零均值高斯先验：$\\beta_c \\sim \\mathcal{N}(0, \\tau_c^2)$ 和 $\\beta_f \\sim \\mathcal{N}(0, \\tau_f^2)$。截距 $\\beta_0$ 是无惩罚的，这对应于一个非正常的均匀先验或方差无限的高斯先验。\n\n根据贝叶斯定理，参数的后验概率与似然和先验的乘积成正比：$p(\\boldsymbol{\\beta} | \\mathbf{y}, \\mathbf{X}, \\sigma^2) \\propto p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2) p(\\boldsymbol{\\beta})$。最大后验 (MAP) 估计 $\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}$ 最大化这个后验概率，这等价于最小化负对数后验：\n$$\n-\\ln p(\\boldsymbol{\\beta} | \\mathbf{y}) \\propto -\\ln p(\\mathbf{y} | \\boldsymbol{\\beta}) - \\ln p(\\boldsymbol{\\beta})\n$$\n最小化上式等价于最小化：\n$$\n\\frac{1}{2\\sigma^2}||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||_2^2 + \\sum_{j \\in \\text{penalized}} \\frac{\\beta_j^2}{2\\tau_j^2}\n$$\n这是一个岭回归问题。解为：\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}} = (\\mathbf{X}^T\\mathbf{X} + \\sigma^2\\mathbf{P})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n其中 $\\mathbf{P}$ 是逆先验方差的对角矩阵。对于一个具有系数 $(\\beta_0, \\beta_c, \\beta_f)$ 的模型，$\\mathbf{P} = \\text{diag}(0, 1/\\tau_c^2, 1/\\tau_f^2)$。$\\mathbf{P}$ 的结构会根据每个模型中存在的系数进行调整。对于 $M_1$，$\\mathbf{P}$ 是一个零矩阵。\n\n要最小化的基于后验的分数是通过将负对数后验（在 MAP 估计处求值）与一个显式的复杂性惩罚相结合来定义的：\n$$\nScore_{\\text{post}} = -\\left( \\mathcal{L}(\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}, \\sigma^2) + \\ln p(\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}) \\right) + \\alpha \\cdot p\n$$\n其中 $p$ 是模型中的回归系数数量。各项为：\n- MAP 处的对数似然：$\\mathcal{L}(\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}||\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}||_2^2$。\n- MAP 处的对数先验：$\\ln p(\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}) = \\sum_{j \\in \\text{penalized}} \\left( -\\frac{1}{2}\\ln(2\\pi\\tau_j^2) - \\frac{\\hat{\\beta}_{j, \\text{MAP}}^2}{2\\tau_j^2} \\right)$。求和是在模型中存在且具有指定先验的系数上进行的。\n\n对于每个模型 $M_i$，计算该分数。选择分数最小的模型。\n\n### 计算步骤\n\n对于由 $(\\alpha, \\tau_c^2, \\tau_f^2)$ 定义的六个测试案例中的每一个：\n1. 综合数据集保持不变。\n2. 对于四个模型中的每一个（$M_1$ 到 $M_4$）：\n    a. 构建适当的设计矩阵 $\\mathbf{X}$。\n    b. 使用 MLE 程序计算 $Score_{\\text{like}}$。\n    c. 使用 MAP 程序计算 $Score_{\\text{post}}$。\n3. 确定最小化 $Score_{\\text{like}}$ 的模型索引（$1$ 到 $4$）（称之为 $m_{\\text{like}}$）。\n4. 确定最小化 $Score_{\\text{post}}$ 的模型索引（称之为 $m_{\\text{post}}$）。\n5. 确定选择是否一致：$r = (m_{\\text{like}} == m_{\\text{post}})$。\n6. 该测试案例的结果是列表 $[m_{\\text{like}}, m_{\\text{post}}, r]$。\n\n对所有测试案例重复此过程，并将结果汇总到一个最终列表中。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a sensitivity analysis of model selection criteria for a two-scale linear regression model.\n    \"\"\"\n    # 1. FIXED PARAMETERS and DATA GENERATION\n    n = 200\n    C = 2\n    F = 15\n    beta_0 = 0.0\n    beta_c = 1.0\n    beta_f = 0.3\n    sigma = 0.2\n    seed = 42\n\n    # Generate deterministic synthetic data\n    np.random.seed(seed)\n    u_t = np.linspace(0, 1, n)\n    x_c = np.sin(2 * np.pi * C * u_t)\n    x_f = np.sin(2 * np.pi * F * u_t)\n    epsilon = np.random.normal(0, sigma, n)\n    y = beta_0 + beta_c * x_c + beta_f * x_f + epsilon\n\n    # Define candidate models by their design matrices and parameters\n    # The models are defined by which columns of X_full they use.\n    # X_full has columns for [intercept, x_c, x_f]\n    X_full = np.vstack([np.ones(n), x_c, x_f]).T\n    model_definitions = [\n        {'name': 'M1', 'cols': [0], 'p': 1, 'coeffs': ['b0']},\n        {'name': 'M2', 'cols': [0, 1], 'p': 2, 'coeffs': ['b0', 'bc']},\n        {'name': 'M3', 'cols': [0, 2], 'p': 2, 'coeffs': ['b0', 'bf']},\n        {'name': 'M4', 'cols': [0, 1, 2], 'p': 3, 'coeffs': ['b0', 'bc', 'bf']},\n    ]\n\n    # 2. TEST SUITE\n    test_cases = [\n        (2.0, 1e6, 1e6),\n        (0.0, 1e6, 1e6),\n        (np.log(n), 1e6, 1e6),\n        (2.0, 1e6, 1e-4),\n        (2.0, 1e-4, 1e6),\n        (np.log(n), 1e-6, 1e-6),\n    ]\n\n    final_results = []\n\n    for alpha, tau_c2, tau_f2 in test_cases:\n        scores_like = []\n        scores_post = []\n\n        for model_def in model_definitions:\n            p = model_def['p']\n            X = X_full[:, model_def['cols']]\n            \n            # --- Likelihood-based Information Score ---\n            # OLS/MLE solution for beta\n            try:\n                XTX = X.T @ X\n                XTy = X.T @ y\n                beta_mle = np.linalg.solve(XTX, XTy)\n                \n                # Residual Sum of Squares (RSS)\n                residuals = y - X @ beta_mle\n                rss = np.sum(residuals**2)\n                \n                # MLE of sigma^2\n                sigma2_mle = rss / n\n                \n                # Maximized log-likelihood (up to a constant)\n                # The full -2*logL has a n*log(2*pi) term, which is constant across models\n                # and doesn't affect argmin. However, for correctness, we compute the full value.\n                if sigma2_mle > 1e-12: # for numerical stability\n                    logL_max = -n/2 * (np.log(2 * np.pi * sigma2_mle) + 1)\n                else: \n                    logL_max = np.inf # Effectively a huge penalty for a perfect but unlikely fit\n                \n                # Number of estimated parameters k = p (coeffs) + 1 (variance)\n                k = p + 1\n                score_like = -2 * logL_max + alpha * k\n                scores_like.append(score_like)\n\n            except np.linalg.LinAlgError:\n                scores_like.append(np.inf)\n\n\n            # --- Bayesian Posterior-based Score ---\n            # Known variance for this score\n            sigma2_known = sigma**2\n            \n            # Build penalty matrix P for Ridge/MAP\n            P = np.zeros((p, p))\n            \n            # Check which penalized coefficients are in the model\n            model_coeffs = model_def['coeffs']\n            \n            if 'bc' in model_coeffs:\n                idx = model_coeffs.index('bc')\n                P[idx, idx] = 1.0 / tau_c2\n            if 'bf' in model_coeffs:\n                idx = model_coeffs.index('bf')\n                P[idx, idx] = 1.0 / tau_f2\n\n            try:\n                # MAP solution for beta\n                beta_map = np.linalg.solve(X.T @ X + sigma2_known * P, X.T @ y)\n\n                # Log-likelihood at MAP\n                residuals_map = y - X @ beta_map\n                rss_map = np.sum(residuals_map**2)\n                logL_map = -n/2 * np.log(2 * np.pi * sigma2_known) - rss_map / (2 * sigma2_known)\n\n                # Log-prior at MAP\n                log_prior_val = 0.0\n                if 'bc' in model_coeffs:\n                    beta_c_map = beta_map[model_coeffs.index('bc')]\n                    log_prior_val += -0.5 * np.log(2 * np.pi * tau_c2) - (beta_c_map**2) / (2 * tau_c2)\n                if 'bf' in model_coeffs:\n                    beta_f_map = beta_map[model_coeffs.index('bf')]\n                    log_prior_val += -0.5 * np.log(2 * np.pi * tau_f2) - (beta_f_map**2) / (2 * tau_f2)\n\n                # Score is -log_posterior + penalty\n                # -log_posterior = -logL - log_prior\n                score_post = -(logL_map + log_prior_val) + alpha * p\n                scores_post.append(score_post)\n\n            except np.linalg.LinAlgError:\n                scores_post.append(np.inf)\n\n        # Select model with minimum score for each criterion\n        m_like = np.argmin(scores_like) + 1\n        m_post = np.argmin(scores_post) + 1\n        \n        # Check for agreement\n        agreement = (m_like == m_post)\n        \n        final_results.append([m_like, m_post, agreement])\n\n    # 3. FINAL OUTPUT\n    # Convert boolean to Python's True/False string representation for output\n    result_str = ','.join([f\"[{r[0]},{r[1]},{str(r[2])}]\" for r in final_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}