{
    "hands_on_practices": [
        {
            "introduction": "掌握模型选择的第一步是熟练计算和解释核心信息准则，即赤池信息准则（AIC）和贝叶斯信息准则（BIC）。本练习提供了一个基础场景，让您从这些准则的理论描述（而非仅仅记忆公式）出发来练习计算。这个过程将加深您对模型拟合优度（由对数似然 $\\ell$ 体现）与模型复杂度（由参数数量 $k$ 体现）之间权衡的理解。",
            "id": "3780497",
            "problem": "在多尺度建模与分析中，通常需要比较在系统不同有效分辨率下运行的机理模型。考虑两个候选的粗粒化随机模型，记为 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$，每个模型都旨在描述由一个潜在的快慢过程生成的慢流形统计量。两个模型都是在证明渐近近似合理性的标准正则性条件下，通过对 $n$ 个独立观测值进行最大似然估计得到的。\n\n您需要使用两个基于第一性原理的信息论准则来比较这两个模型：\n\n- 一个准则是通过近似真实数据生成过程与拟合模型之间的预期样本外 Kullback–Leibler 散度得出的，它使用了一个基于最大化对数似然和与自由参数数量成正比的显式复杂度惩罚的偏差校正估计量。\n- 另一个准则是通过 Laplace 近似来近似每个模型下数据的负两倍对数边际似然得出的，这产生了一个随样本量和模型维度的对数而变化的复杂度惩罚。\n\n设 $\\ell_{1}$ 和 $\\ell_{2}$ 分别表示在大小为 $n$ 的同一数据集上计算出的 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 的最大化对数似然。设 $k_{1}$ 和 $k_{2}$ 分别表示 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 中的自由参数数量。假设\n$$\n\\ell_{1}=-120,\\quad \\ell_{2}=-118,\\quad k_{1}=4,\\quad k_{2}=6,\\quad n=200.\n$$\n\n使用上述所暗示的渐近定义，计算 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 的两个准则，并根据通常的惯例（较小的值表示在拟合优度与复杂度之间有更好的权衡）确定每个准则偏好哪个模型。将您的最终答案报告为单个行向量\n$$\n\\big[\\mathrm{AIC}_{1},\\ \\mathrm{AIC}_{2},\\ \\mathrm{BIC}_{1},\\ \\mathrm{BIC}_{2},\\ p_{\\mathrm{AIC}},\\ p_{\\mathrm{BIC}}\\big],\n$$\n其中，如果 Akaike 信息准则 (AIC) 偏好 $\\mathcal{M}_{1}$，则 $p_{\\mathrm{AIC}}$ 等于 $1$，如果偏好 $\\mathcal{M}_{2}$，则等于 $2$，如果两者不分上下，则等于 $0$；$p_{\\mathrm{BIC}}$ 的定义对于贝叶斯信息准则 (BIC) 也是类似的。仅使用包含 $\\ln$ 和整数的精确符号表达式；不要进行近似或四舍五入。",
            "solution": "该问题要求计算两个竞争模型 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 的两个信息准则，并确定每个准则偏好哪个模型。这些准则是通过其理论推导来描述的。\n\n首先，我们必须根据所提供的描述确定这些准则的具体公式。\n\n第一个准则被描述为“通过近似真实数据生成过程与拟合模型之间的预期样本外 Kullback–Leibler 散度得出的，它使用了一个基于最大化对数似然和与自由参数数量成正比的显式复杂度惩罚的偏差校正估计量。”这是赤池信息准则 (AIC) 的定义。其标准公式为：\n$$\n\\mathrm{AIC} = 2k - 2\\ell\n$$\n其中 $k$ 是模型中自由参数的数量，$\\ell$ 是对数似然函数的最大化值。\n\n第二个准则被描述为“通过 Laplace 近似来近似每个模型下数据的负两倍对数边际似然得出的，这产生了一个随样本量和模型维度的对数而变化的复杂度惩罚。”这是贝叶斯信息准则 (BIC) 的定义，也称为施瓦茨准则。其公式为：\n$$\n\\mathrm{BIC} = k\\ln(n) - 2\\ell\n$$\n其中 $n$ 是观测值的数量（样本量）。\n\n对于 AIC 和 BIC，惯例是值越低表示模型越好，代表在拟合优度（高 $\\ell$）和模型复杂度（低 $k$）之间有更有利的权衡。\n\n问题提供了以下数值：\n- 对于模型 $\\mathcal{M}_{1}$：最大化对数似然 $\\ell_{1} = -120$ 和参数数量 $k_{1} = 4$。\n- 对于模型 $\\mathcal{M}_{2}$：最大化对数似然 $\\ell_{2} = -118$ 和参数数量 $k_{2} = 6$。\n- 样本量为 $n=200$。\n\n现在，我们为每个模型计算 AIC。\n对于模型 $\\mathcal{M}_{1}$：\n$$\n\\mathrm{AIC}_{1} = 2k_{1} - 2\\ell_{1} = 2(4) - 2(-120) = 8 + 240 = 248。\n$$\n对于模型 $\\mathcal{M}_{2}$：\n$$\n\\mathrm{AIC}_{2} = 2k_{2} - 2\\ell_{2} = 2(6) - 2(-118) = 12 + 236 = 248。\n$$\n为了根据 AIC 确定偏好的模型，我们比较 $\\mathrm{AIC}_{1}$ 和 $\\mathrm{AIC}_{2}$。由于 $\\mathrm{AIC}_{1} = \\mathrm{AIC}_{2} = 248$，两个模型没有优劣之分。这被认为是不分上下。根据问题的定义，我们设置 $p_{\\mathrm{AIC}} = 0$。\n\n接下来，我们为每个模型计算 BIC。\n对于模型 $\\mathcal{M}_{1}$：\n$$\n\\mathrm{BIC}_{1} = k_{1}\\ln(n) - 2\\ell_{1} = 4\\ln(200) - 2(-120) = 240 + 4\\ln(200)。\n$$\n对于模型 $\\mathcal{M}_{2}$：\n$$\n\\mathrm{BIC}_{2} = k_{2}\\ln(n) - 2\\ell_{2} = 6\\ln(200) - 2(-118) = 236 + 6\\ln(200)。\n$$\n问题要求答案以精确符号形式表示，所以我们保持这些表达式的原样。\n\n为了根据 BIC 确定偏好的模型，我们比较 $\\mathrm{BIC}_{1}$ 和 $\\mathrm{BIC}_{2}$。我们来考察它们的差值：\n$$\n\\mathrm{BIC}_{2} - \\mathrm{BIC}_{1} = \\big(236 + 6\\ln(200)\\big) - \\big(240 + 4\\ln(200)\\big) = (236 - 240) + (6-4)\\ln(200) = -4 + 2\\ln(200)。\n$$\n为了确定这个差值的符号，我们需要比较 $2\\ln(200)$ 和 $4$，这等价于比较 $\\ln(200)$ 和 $2$。自然对数函数 $\\ln(x)$ 是单调递增的。常数 $e$ 约等于 $2.718$，所以 $e^{2} \\approx (2.718)^{2} \\approx 7.389$。由于 $200 > e^{2}$，因此有 $\\ln(200) > \\ln(e^{2})$，化简后得到 $\\ln(200) > 2$。\n因此，$2\\ln(200) > 4$，这意味着差值 $-4 + 2\\ln(200)$ 是正的。\n所以，$\\mathrm{BIC}_{2} - \\mathrm{BIC}_{1} > 0$，这意味着 $\\mathrm{BIC}_{2} > \\mathrm{BIC}_{1}$。\n由于 BIC 值较低的模型更优，因此 BIC 偏好模型 $\\mathcal{M}_{1}$。根据问题的定义，我们设置 $p_{\\mathrm{BIC}} = 1$。\n\n最后，我们组装所需的行向量：\n$$\n\\big[\\mathrm{AIC}_{1},\\ \\mathrm{AIC}_{2},\\ \\mathrm{BIC}_{1},\\ \\mathrm{BIC}_{2},\\ p_{\\mathrm{AIC}},\\ p_{\\mathrm{BIC}}\\big].\n$$\n代入计算出的值：\n- $\\mathrm{AIC}_{1} = 248$\n- $\\mathrm{AIC}_{2} = 248$\n- $\\mathrm{BIC}_{1} = 240 + 4\\ln(200)$\n- $\\mathrm{BIC}_{2} = 236 + 6\\ln(200)$\n- $p_{\\mathrm{AIC}} = 0$\n- $p_{\\mathrm{BIC}} = 1$\n\n最终的向量是 $[248, 248, 240 + 4\\ln(200), 236 + 6\\ln(200), 0, 1]$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n248  248  240 + 4\\ln(200)  236 + 6\\ln(200)  0  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在掌握了基本计算之后，下一个练习将这些准则应用于时间序列分析中的一个常见任务：选择自回归模型的阶数。这个问题引入了小样本修正的赤池信息准则（AICc），并突出了模型选择中的一个关键点：不同的准则可能得出不同的结论。通过比较 AIC、AICc 和 BIC 的结果，您可以深入了解它们各自复杂度惩罚项的相对强度，特别是在样本量 $n$ 较大时，BIC 对复杂度的惩罚会更重。",
            "id": "3780541",
            "problem": "单变量高斯自回归过程在单一分辨率下通常被用作多尺度时间序列模型中的一个粗粒度分量。考虑对一个长度为 $n=300$ 的序列拟合一个 $p$ 阶自回归模型，记作 $\\mathrm{AR}(p)$，其中 $p\\in\\{1,2,3\\}$。对于每次拟合，假设参数族包括 $p$ 个自回归系数、一个截距和一个新息方差，因此自由参数的总数为 $k=p+2$。在高斯新息假设下，三次拟合的最大化对数似然值分别为：$p=1$ 时为 $-210$，$p=2$ 时为 $-205$，$p=3$ 时为 $-203$。使用最大似然估计和信息论模型比较的基本原理，计算每个 $p\\in\\{1,2,3\\}$ 对应的赤池信息准则 (AIC)、小样本修正的赤池信息准则 (AICc) 和贝叶斯信息准则 (BIC) 的值，并简要解释这些准则之间的差异对多尺度建模中模型选择的影响。将每个准则值四舍五入到四位有效数字，并以无单位的形式表示最终数值答案。将您的最终答案以行矩阵的形式返回，其条目按顺序为\n$$\\text{AIC}(p=1),\\ \\text{AICc}(p=1),\\ \\text{BIC}(p=1),\\ \\text{AIC}(p=2),\\ \\text{AICc}(p=2),\\ \\text{BIC}(p=2),\\ \\text{AIC}(p=3),\\ \\text{AICc}(p=3),\\ \\text{BIC}(p=3).$$",
            "solution": "在尝试解答之前，对问题陈述进行验证。\n\n### 步驟1：提取已知条件\n-   **过程**：单变量高斯自回归过程。\n-   **模型**：$p$ 阶自回归模型，记作 $\\mathrm{AR}(p)$。\n-   **样本量**：$n = 300$。\n-   **考虑的模型阶数**：$p \\in \\{1, 2, 3\\}$。\n-   **自由参数数量**：$k = p + 2$。这包括 $p$ 个自回归系数、一个截距和一个新息方差。\n-   **最大化对数似然值** ($\\ln(L)$)：\n    -   $p=1$ 时，$\\ln(L_1) = -210$。\n    -   $p=2$ 时，$\\ln(L_2) = -205$。\n    -   $p=3$ 时，$\\ln(L_3) = -203$。\n-   **要求计算**：每个 $p$ 对应的赤池信息准则 (AIC)、小样本修正的赤池信息准则 (AICc) 和贝叶斯信息准则 (BIC)。\n-   **要求的输出格式**：一个包含九个计算出的准则值的行矩阵，四舍五入到四位有效数字。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题具有科学依据，提法明確且客观。它提供了计算标准的、定义明确的统计指标所需的一套完整且一致的数据和定义。该场景是时间序列分析中模型选择的一个典型应用，而时间序列分析是多尺度建模的基础组成部分。提供的样本量、模型阶数和对数似然值都是合理的。该问题不违反任何科学原理，没有歧义，并且可以使用既定的范式直接求解。\n\n### 步骤3：结论与行动\n问题有效。将提供完整的解答。\n\n### 解答推导\n任务是为三个不同的自回归模型计算三个信息准则，并解释结果。这些准则的定义如下：\n\n1.  **赤池信息准则 (AIC)**：\n    $$ \\mathrm{AIC} = 2k - 2\\ln(L) $$\n    其中 $k$ 是估计参数的数量，$\\ln(L)$ 是最大化对数似然值。\n\n2.  **修正的赤池信息准则 (AICc)**：\n    $$ \\mathrm{AICc} = \\mathrm{AIC} + \\frac{2k(k+1)}{n-k-1} $$\n    其中 $n$ 是样本量。该准则对小样本量的AIC进行调整。\n\n3.  **贝叶斯信息准则 (BIC)**：\n    $$ \\mathrm{BIC} = k\\ln(n) - 2\\ln(L) $$\n    该准则对模型复杂度的惩罚比AIC更强，特别是对于大样本量 $n$。\n\n我们已知 $n=300$ 且 $k=p+2$。我们为每个 $p \\in \\{1, 2, 3\\}$ 计算这些准则。\n\n**情况1：模型 AR(1)，$p=1$**\n-   参数数量：$k_1 = 1 + 2 = 3$。\n-   最大化对数似然值：$\\ln(L_1) = -210$。\n\n-   $\\mathrm{AIC}_1 = 2k_1 - 2\\ln(L_1) = 2(3) - 2(-210) = 6 + 420 = 426$。\n    四舍五入到四位有效数字得到 $426.0$。\n-   $\\mathrm{AICc}_1 = \\mathrm{AIC}_1 + \\frac{2k_1(k_1+1)}{n-k_1-1} = 426 + \\frac{2(3)(3+1)}{300-3-1} = 426 + \\frac{24}{296} \\approx 426 + 0.081081... = 426.081081...$。\n    四舍五入到四位有效数字得到 $426.1$。\n-   $\\mathrm{BIC}_1 = k_1\\ln(n) - 2\\ln(L_1) = 3\\ln(300) - 2(-210) = 3\\ln(300) + 420 \\approx 3(5.70378) + 420 = 17.11134 + 420 = 437.11134...$。\n    四舍五入到四位有效数字得到 $437.1$。\n\n**情况2：模型 AR(2)，$p=2$**\n-   参数数量：$k_2 = 2 + 2 = 4$。\n-   最大化对数似然值：$\\ln(L_2) = -205$。\n\n-   $\\mathrm{AIC}_2 = 2k_2 - 2\\ln(L_2) = 2(4) - 2(-205) = 8 + 410 = 418$。\n    四舍五入到四位有效数字得到 $418.0$。\n-   $\\mathrm{AICc}_2 = \\mathrm{AIC}_2 + \\frac{2k_2(k_2+1)}{n-k_2-1} = 418 + \\frac{2(4)(4+1)}{300-4-1} = 418 + \\frac{40}{295} \\approx 418 + 0.135593... = 418.135593...$。\n    四舍五入到四位有效数字得到 $418.1$。\n-   $\\mathrm{BIC}_2 = k_2\\ln(n) - 2\\ln(L_2) = 4\\ln(300) - 2(-205) = 4\\ln(300) + 410 \\approx 4(5.70378) + 410 = 22.81512 + 410 = 432.81512...$。\n    四舍五入到四位有效数字得到 $432.8$。\n\n**情况3：模型 AR(3)，$p=3$**\n-   参数数量：$k_3 = 3 + 2 = 5$。\n-   最大化对数似然值：$\\ln(L_3) = -203$。\n\n-   $\\mathrm{AIC}_3 = 2k_3 - 2\\ln(L_3) = 2(5) - 2(-203) = 10 + 406 = 416$。\n    四舍五入到四位有效数字得到 $416.0$。\n-   $\\mathrm{AICc}_3 = \\mathrm{AIC}_3 + \\frac{2k_3(k_3+1)}{n-k_3-1} = 416 + \\frac{2(5)(5+1)}{300-5-1} = 416 + \\frac{60}{294} \\approx 416 + 0.204081... = 416.204081...$。\n    四舍五入到四位有效数字得到 $416.2$。\n-   $\\mathrm{BIC}_3 = k_3\\ln(n) - 2\\ln(L_3) = 5\\ln(300) - 2(-203) = 5\\ln(300) + 406 \\approx 5(5.70378) + 406 = 28.51890 + 406 = 434.51890...$。\n    四舍五入到四位有效数字得到 $434.5$。\n\n### 解释\n使用这些准则进行模型选择的原则是选择准则值最小的模型。\n\n-   **AIC 和 AICc 的选择**：\n    -   $\\mathrm{AIC}$ 值：$\\mathrm{AIC}_1 = 426.0$，$\\mathrm{AIC}_2 = 418.0$，$\\mathrm{AIC}_3 = 416.0$。\n    -   $\\mathrm{AICc}$ 值：$\\mathrm{AICc}_1 = 426.1$，$\\mathrm{AICc}_2 = 418.1$，$\\mathrm{AICc}_3 = 416.2$。\n    AIC 和 AICc 都选择了 $\\mathrm{AR}(3)$ 模型 ($p=3$)，因为它具有最低的值。AICc 的修正是很小的，因为样本量 $n=300$ 相对于参数数量 $k$ (最多为5) 来说很大，所以 $n-k-1$ 不小。具体来说，比率 $k/n$ 很小。\n\n-   **BIC 的选择**：\n    -   $\\mathrm{BIC}$ 值：$\\mathrm{BIC}_1 = 437.1$，$\\mathrm{BIC}_2 = 432.8$，$\\mathrm{BIC}_3 = 434.5$。\n    BIC 选择了 $\\mathrm{AR}(2)$ 模型 ($p=2$)，因为它对应于最小的 BIC 值。\n\n-   **比较与启示**：\n    模型选择上的差异源于对模型复杂度的不同惩罚项。AIC 的惩罚项是 $2k$，而 BIC 的惩罰项是 $k\\ln(n)$。对于 $n=300$，$\\ln(300) \\approx 5.70$。因此，BIC对每个参数的惩罚 ($\\approx 5.70$) 实质上大于 AIC 的惩罚 ($2$)。这使得 BIC 更倾向于选择更简约（更简单）的模型。\n    在这种情况下，从 $\\mathrm{AR}(2)$ 模型转到 $\\mathrm{AR}(3)$ 模型，对数似然值从 $-205$ 提高到 $-203$。对于 AIC，这种拟合度的改善（$-2\\ln(L)$ 项减少了 $2\\times(-203 - (-205)) = 4$）超过了惩罰项的增加（$2(k_3-k_2) = 2(1) = 2$）。而对于 BIC，同样的拟合度改善不足以克服更大的惩罚项增加（$(k_3-k_2)\\ln(n) \\approx 5.70$）。\n    在多尺度建模的背景下，这突显了一个根本性的权衡。在每个尺度上选择分量模型（例如这里的粗粒度 AR 过程）涉及到在保真度和复杂性之间取得平衡。如果目标是预测准确性，可能会首选 AIC，因为它能选择稍微复杂一些的模型来捕捉更精细的细节。而 BIC 倾向于简约性，如果目标是识别最可能的潜在生成结构，避免在每个尺度上发生过拟合，那么 BIC 可能是更好的选择，因为这对于整个多尺度模型的稳定性和可解释性至关重要。\n\n行矩阵的最终值为：\n-   $p=1$: $426.0$, $426.1$, $437.1$\n-   $p=2$: $418.0$, $418.1$, $432.8$\n-   $p=3$: $416.0$, $416.2$, $434.5$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n426.0  426.1  437.1  418.0  418.1  432.8  416.0  416.2  434.5\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "除了计算之外，对模型选择的深刻理解还需要掌握这些准则背后的理论假设。本练习探讨了参数可识别性这一关键问题，这是多尺度建模中的一个常见挑战，因为快速动态的参数可能无法从慢尺度数据中推断出来。它展示了像 AIC 和 BIC 这样的信息准则如何通过其固定的惩罚机制，正确地惩罚包含此类不可识别参数的模型，从而防止过拟合并提倡简约性。",
            "id": "3780455",
            "problem": "一个多尺度动力系统具有由下式控制的慢态 $x_s(t)$ 和快态 $x_f(t)$:\n$$\n\\dot{x}_s(t) = f\\big(x_s(t), \\theta\\big), \\qquad \\dot{x}_f(t) = \\frac{1}{\\varepsilon}\\, g\\big(x_s(t), x_f(t), \\eta\\big),\n$$\n其中 $0  \\varepsilon \\ll 1$ 是一个很小的尺度分离参数。在时间 $t_1,\\dots,t_n$，你观测到 $n$ 个关于时间平均慢变量的独立含噪测量值：\n$$\ny_i = h\\big(x_s(t_i)\\big) + \\epsilon_i, \\quad \\epsilon_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^2),\n$$\n其中 $h(\\cdot)$ 已知，$\\sigma^2$ 未知。考虑两个用于该数据的嵌套参数模型：\n- $M_1$：一个简化模型，它使用由 $\\theta \\in \\mathbb{R}^p$ 参数化的平均慢动力学和噪声方差 $\\sigma^2$，总共有 $k_1 = p + 1$ 个自由参数。\n- $M_2$：一个增广模型，它在 $\\theta$ 和 $\\sigma^2$ 之外，还保留了一个额外的快尺度参数 $\\eta \\in \\mathbb{R}$，总共有 $k_2 = p + 2$ 个自由参数。\n\n由于观测值仅与慢变量相关且经过时间平均，假设以下情况成立：对于任何固定的 $(\\theta,\\sigma^2)$，观测数据在 $M_2$ 模型下的边际似然不依赖于 $\\eta$，即似然函数 $L_2(\\theta,\\eta,\\sigma^2)$ 对于所有 $\\eta$ 都等于 $L_1(\\theta,\\sigma^2)$。因此，最大对数似然值重合：\n$$\n\\hat{\\ell}_1 = \\max_{\\theta,\\sigma^2} \\log L_1(\\theta,\\sigma^2) \\quad \\text{和} \\quad \\hat{\\ell}_2 = \\max_{\\theta,\\eta,\\sigma^2} \\log L_2(\\theta,\\eta,\\sigma^2)\n$$\n满足 $\\hat{\\ell}_1 = \\hat{\\ell}_2$。\n\n根据参数推断和模型选择的基本原理，回答以下多项选择题。选择所有正确的陈述。\n\nA. 参数统计模型的可辨识性意味着不同的参数值会导出不同的数据生成分布。在此场景中，由于对于固定的 $(\\theta,\\sigma^2)$，$L_2(\\theta,\\eta,\\sigma^2)$ 关于 $\\eta$ 是常数，因此参数 $\\eta$ 是不可辨识的。\n\nB. 根据 Akaike Information Criterion (AIC) 和 Bayesian Information Criterion (BIC) 的标准定义，添加像 $\\eta$ 这样的不可辨识参数会减少惩罚项，因为其在该方向上的 Fisher 信息为零，所以当 $\\hat{\\ell}_1=\\hat{\\ell}_2$ 时，增广模型 $M_2$ 的 BIC 会比 $M_1$ 小。\n\nC. 在 $\\hat{\\ell}_1=\\hat{\\ell}_2$ 和 $k_2 = k_1 + 1$ 的情况下，从 $M_1$ 移动到 $M_2$ 时，经典的 Akaike Information Criterion 会增加 2，尽管拟合优度没有提高，因此会倾向于不选择增广模型。\n\nD. 不可辨识的参数只能通过在不改变拟合的情况下减少方差来帮助泛化，因此经典的 Akaike Information Criterion 会相应地减少惩罚项，并倾向于选择 $M_2$。\n\nE. 即使 $\\hat{\\ell}_1=\\hat{\\ell}_2$，$M_1$ 和 $M_2$ 的经典 Akaike Information Criterion 也会相等，因为额外的参数 $\\eta$ 不会改变最大似然值，并且两个模型中的有效维度都保持为 $k_1$。\n\n选择所有适用项。",
            "solution": "用户希望我验证问题陈述的有效性，如果有效，则通过推导正确答案并评估每个提供的选项来解决问题。\n\n### 问题验证\n\n我将首先根据指定标准验证问题陈述。\n\n**步骤 1：提取已知条件**\n\n-   **系统动力学**:\n    -   慢态: $\\dot{x}_s(t) = f\\big(x_s(t), \\theta\\big)$\n    -   快态: $\\dot{x}_f(t) = \\frac{1}{\\varepsilon}\\, g\\big(x_s(t), x_f(t), \\eta\\big)$\n    -   尺度分离参数: $0  \\varepsilon \\ll 1$\n-   **观测**:\n    -   $n$ 个独立含噪测量: $y_i = h\\big(x_s(t_i)\\big) + \\epsilon_i$ for $i=1,\\dots,n$。\n    -   噪声分布: $\\epsilon_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^2)$。\n    -   函数 $h(\\cdot)$ 已知。\n    -   噪声方差 $\\sigma^2$ 未知。\n-   **模型**:\n    -   模型 $M_1$：一个简化模型，参数为 $(\\theta, \\sigma^2)$，其中 $\\theta \\in \\mathbb{R}^p$。自由参数总数为 $k_1 = p + 1$。\n    -   模型 $M_2$：一个增广模型，参数为 $(\\theta, \\eta, \\sigma^2)$，其中 $\\eta \\in \\mathbb{R}$。自由参数总数为 $k_2 = p + 2$。\n-   **似然与最大对数似然**:\n    -   $M_2$ 的似然函数 $L_2(\\theta,\\eta,\\sigma^2)$ 对于所有 $\\eta$ 值都等于 $M_1$ 的似然函数 $L_1(\\theta,\\sigma^2)$。即 $L_2(\\theta,\\eta,\\sigma^2) = L_1(\\theta,\\sigma^2)$。\n    -   $M_1$ 的最大对数似然为 $\\hat{\\ell}_1 = \\max_{\\theta,\\sigma^2} \\log L_1(\\theta,\\sigma^2)$。\n    -   $M_2$ 的最大对数似然为 $\\hat{\\ell}_2 = \\max_{\\theta,\\eta,\\sigma^2} \\log L_2(\\theta,\\eta,\\sigma^2)$。\n    -   给定这两个值相等：$\\hat{\\ell}_1 = \\hat{\\ell}_2$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n-   **科学依据**：该问题描述了多尺度建模和系统辨识中的一个典型设置。关于慢过程的观测可能对某些控制快动力学的参数不敏感的想法是一个公认的概念，通常源于平均或均质化理论。使用 AIC 和 BIC 等信息准则进行模型选择是标准做法。该设置在科学上和数学上都是合理的。\n-   **适定性**：该问题是适定的。它提出了一个清晰的理论场景，并要求基于统计推断中的既定定义对具体陈述进行评估。所提供的信息足以确定每个选项的正确性。\n-   **客观性**：语言正式、精确，没有主观或基于意见的陈述。\n-   **不完整或矛盾的设置**：该设置是完整且一致的。关键假设 $L_2(\\theta,\\eta,\\sigma^2) = L_1(\\theta,\\sigma^2)$ 直接意味着 $\\hat{\\ell}_2 = \\max_{\\theta,\\eta,\\sigma^2} \\log L_2(\\theta,\\eta,\\sigma^2) = \\max_{\\theta,\\sigma^2} \\left[ \\max_{\\eta} \\log L_1(\\theta,\\sigma^2) \\right] = \\max_{\\theta,\\sigma^2} \\log L_1(\\theta,\\sigma^2) = \\hat{\\ell}_1$。参数数量 $k_1 = p+1$ 和 $k_2 = p+2$ 也是一致的。没有矛盾之处。\n-   **不切实际或不可行**：该场景是模型选择中的一个常见理论案例研究，特别是对于具有不可辨识参数的模型。在理论背景下，它并非不切实际或不可行。\n-   **不适定或结构不良**：该问题结构良好。基于所涉概念的标准定义，它能为每个选项导出唯一的逻辑结论。\n-   **伪深刻、琐碎或同义反复**：该问题并非琐碎。它探讨了一个微妙但重要的问题：在存在不可辨识参数的情况下，标准模型选择准则的行为方式，这是一个常见的混淆源。\n-   **超出科学可验证性范围**：所有陈述都可以使用可辨识性、AIC 和 BIC 的数学定义进行验证。\n\n**步骤 3：结论和行动**\n\n问题陈述有效。我将继续进行解答。\n\n### 解答推导\n\n问题要求我们评估关于两个嵌套模型 M₁ 和 M₂ 模型选择的几个陈述，其中 M₂ 包含一个额外的参数 η，该参数不影响观测数据的似然。\n\n首先我们来建立关键原则。\n\n1.  **可辨识性**：如果对于任意两个不同的参数向量 $\\phi_1 \\neq \\phi_2$，其对应的可观测数据概率分布 $P(y|\\phi_1)$ 和 $P(y|\\phi_2)$ 也不同，那么模型的参数向量 $\\phi$ 就是可辨识的。就似然函数 $L(\\phi|y) = P(y|\\phi)$ 而言，这意味着 $\\phi_1 \\neq \\phi_2$ 蕴含着对于至少一个可能的数据集 $y$，$L(\\phi_1|y) \\neq L(\\phi_2|y)$。如果似然函数在一系列参数值上是常数，那么这些参数是不可辨识的。\n\n2.  **Akaike Information Criterion (AIC)**：AIC 的经典定义是：\n    $$AIC = 2k - 2\\hat{\\ell}$$\n    其中 $k$ 是模型中估计的参数数量，$\\hat{\\ell}$ 是对数似然函数的最大值。AIC 值较低的模型更优。\n\n3.  **Bayesian Information Criterion (BIC)**：BIC 的经典定义是：\n    $$BIC = k \\log(n) - 2\\hat{\\ell}$$\n    其中 $k$ 是参数数量，$n$ 是数据点数量，$\\hat{\\ell}$ 是最大对数似然。BIC 值较低的模型更优。\n\n现在，让我们基于这些原则分析给定的模型。\n-   对于模型 $M_1$，参数为 $(\\theta, \\sigma^2)$，因此参数数量为 $k_1 = p+1$。最大对数似然为 $\\hat{\\ell}_1$。\n-   对于模型 $M_2$，参数为 $(\\theta, \\eta, \\sigma^2)$，因此参数数量为 $k_2 = p+2$。最大对数似然为 $\\hat{\\ell}_2$。\n\n我们已知 $\\hat{\\ell}_1 = \\hat{\\ell}_2$。这意味着，从最大似然的角度衡量，添加参数 $\\eta$ 并没有改善模型对数据的拟合。参数数量由 $k_2 = k_1 + 1$ 相关联。\n\n### 逐项分析\n\n**A. 参数统计模型的可辨识性意味着不同的参数值会导出不同的数据生成分布。在此场景中，由于对于固定的 $(\\theta,\\sigma^2)$，$L_2(\\theta,\\eta,\\sigma^2)$ 关于 $\\eta$ 是常数，因此参数 $\\eta$ 是不可辨识的。**\n\n-   **分析**：第一句话给出了（结构）可辨识性的正确定义。问题陈述中说明，模型 $M_2$ 的似然函数 $L_2(\\theta, \\eta, \\sigma^2)$ 对于所有 $\\eta$ 都等于 $L_1(\\theta, \\sigma^2)$。这明确意味着对于任何固定的 $(\\theta, \\sigma^2)$，改变 $\\eta$ 的值不会改变观测数据的似然。因此，数据分布不依赖于 $\\eta$，并且不可能根据数据区分 $\\eta$ 的不同值。这就是不可辨识参数的定义。\n-   **结论**：**正确**。\n\n**B. 根据 Akaike Information Criterion (AIC) 和 Bayesian Information Criterion (BIC) 的标准定义，添加像 $\\eta$ 这样的不可辨识参数会减少惩罚项，因为其在该方向上的 Fisher 信息为零，所以当 $\\hat{\\ell}_1=\\hat{\\ell}_2$ 时，增广模型 $M_2$ 的 BIC 会比 $M_1$ 小。**\n\n-   **分析**：该陈述包含几个错误。\n    1.  AIC 和 BIC 的标准定义使用的惩罚项取决于参数的*数量* $k$，而不是直接取决于 Fisher 信息矩阵 (FIM)。虽然 FIM 的奇异性是不可辨识性的一个后果，但经典的 AIC/BIC 不会因此“减少惩罚项”。惩罚项反而会*增加*，因为参数数量从 $k_1$ 增加到 $k_2$。\n    2.  该陈述声称惩罚项会减少。对于 BIC，惩罚项是 $k \\log(n)$。对于 $M_1$，惩罚项是 $k_1 \\log(n) = (p+1)\\log(n)$。对于 $M_2$，它是 $k_2 \\log(n) = (p+2)\\log(n)$。由于对于 $n1$，$\\log(n)  0$，所以 $M_2$ 的惩罚项更大，而不是更小。\n    3.  该陈述声称 $M_2$ 将具有更小的 BIC。让我们计算一下 BIC。\n        $BIC_1 = k_1 \\log(n) - 2\\hat{\\ell}_1 = (p+1)\\log(n) - 2\\hat{\\ell}_1$。\n        $BIC_2 = k_2 \\log(n) - 2\\hat{\\ell}_2 = (p+2)\\log(n) - 2\\hat{\\ell}_2$。\n        由于 $\\hat{\\ell}_1 = \\hat{\\ell}_2$，差值为 $BIC_2 - BIC_1 = (p+2)\\log(n) - (p+1)\\log(n) = \\log(n)$。\n        假设在典型情况下 $n1$，我们有 $\\log(n)0$，这意味着 $BIC_2  BIC_1$。因此，BIC 不倾向于选择 $M_2$。\n-   **结论**：**不正确**。\n\n**C. 在 $\\hat{\\ell}_1=\\hat{\\ell}_2$ 和 $k_2 = k_1 + 1$ 的情况下，从 $M_1$ 移动到 $M_2$ 时，经典的 Akaike Information Criterion 会增加 2，尽管拟合优度没有提高，因此会倾向于不选择增广模型。**\n\n-   **分析**：让我们计算两个模型的 AIC。\n    $AIC_1 = 2k_1 - 2\\hat{\\ell}_1$。\n    $AIC_2 = 2k_2 - 2\\hat{\\ell}_2$。\n    问题陈述中说明 $\\hat{\\ell}_1 = \\hat{\\ell}_2$ 且 $k_2 = k_1 + 1$。\n    AIC 的差值为：\n    $AIC_2 - AIC_1 = (2k_2 - 2\\hat{\\ell}_2) - (2k_1 - 2\\hat{\\ell}_1) = 2k_2 - 2k_1 = 2(k_1+1) - 2k_1 = 2$。\n    所以，$AIC_2 = AIC_1 + 2$。模型 $M_2$ 的 AIC 确实比 $M_1$ 高 2。较高的 AIC 表示模型较差。该陈述正确地得出结论，AIC 不倾向于选择增广模型，因为它在没有改善拟合（$\\hat{\\ell}$ 不变）的情况下增加了一个参数（$k$ 增加）。\n-   **结论**：**正确**。\n\n**D. 不可辨识的参数只能通过在不改变拟合的情况下减少方差来帮助泛化，因此经典的 Akaike Information Criterion 会相应地减少惩罚项，并倾向于选择 $M_2$。**\n\n-   **分析**：该陈述的前提是有缺陷的。添加不可辨识的参数本质上并不能“通过减少方差来帮助泛化”。它使估计复杂化，并可能导致数值不稳定。更重要的是，关于经典 AIC 的结论在事实上是错误的。如选项 B 和 C 的分析所示，经典 AIC 的惩罚项是 $2k$。它严格取决于为模型指定的参数数量。它不会为不可辨识的参数“减少惩罚项”；它会惩罚模型拥有更多参数，仅此而已。由于 $k_2  k_1$，M₂ 的惩罚项更大，AIC 会倾向于选择 M₁，而不是 M₂。\n-   **结论**：**不正确**。\n\n**E. 即使 $\\hat{\\ell}_1=\\hat{\\ell}_2$，$M_1$ 和 $M_2$ 的经典 Akaike Information Criterion 也会相等，因为额外的参数 $\\eta$ 不会改变最大似然值，并且两个模型中的有效维度都保持为 $k_1$。**\n\n-   **分析**：该陈述依赖于“有效维度”的概念。尽管某些高级信息准则（例如，在奇异学习理论中）在不可辨识性的情况下，会将名义参数计数 $k$ 替换为可能小于 $k$ 的有效参数数量，但“经典的 Akaike Information Criterion”并非如此。经典 AIC 公式 $AIC = 2k - 2\\hat{\\ell}$ 使用的是自由参数的名义数量 $k$。对于模型 $M_2$，我们拟合了 $k_2 = p+2$ 个参数，因此惩罚项为 $2k_2$。一个参数不可辨识的事实并不会改变其在经典 AIC 公式中的参数计数。如选项 C 的计算所示，$AIC_2 - AIC_1 = 2$。因此，AIC 不相等。\n-   **结论**：**不正确**。",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}