{
    "hands_on_practices": [
        {
            "introduction": "The Potential of Mean Force (PMF) is a cornerstone concept for understanding rare events, representing the free energy landscape along a chosen reaction coordinate. To build a solid intuition, this first exercise  strips away the complexities of high-dimensional systems to examine the PMF in a simple one-dimensional double-well potential. By directly calculating the PMF from first principles of statistical mechanics, you will discover the fundamental and revealing relationship between the PMF and the potential energy surface when there are no orthogonal degrees of freedom to average over.",
            "id": "3822374",
            "problem": "Consider a single particle moving in one spatial dimension with coordinate $x$ in a double-well potential $U(x)=\\frac{k}{4}\\left(x^{2}-x_{0}^{2}\\right)^{2}$, where $k>0$ and $x_{0}>0$ are constants. Let the reaction coordinate be the identity map $\\xi(x)=x$. Assume the system is at thermodynamic equilibrium with temperature $T$ in the canonical ensemble. Define the potential of mean force (PMF) $F(\\xi)$ along the reaction coordinate via the constrained canonical partition function, and use the Blue Moon ensemble (BME) mean-force identity to relate the derivative of $F(\\xi)$ to microscopic quantities.\n\nNow, consider the String Method for Rare Events (SMRE), which identifies a minimal free-energy path (MFEP). In one dimension, the MFEP coincides with the coordinate line itself. Parameterize the string connecting the two minima at $x=-x_{0}$ and $x=+x_{0}$ by a monotone parameter $\\alpha\\in[0,1]$ such that $x(\\alpha)=-x_{0}+2x_{0}\\alpha$. Using the PMF $F(\\xi)$ derived from first principles, express the free energy along the string, $F(\\alpha)\\equiv F\\big(\\xi(x(\\alpha))\\big)$, in closed form. Choose the additive constant in $F$ such that $F(0)=0$. Then, compare the barrier height along the PMF to the barrier height in the underlying potential $U(x)$ at the saddle $x=0$, and state whether they coincide in this one-dimensional setting.\n\nYour final answer must be the single closed-form analytic expression for $F(\\alpha)$ as a function of $\\alpha$, $k$, and $x_{0}$. No units are required. Do not provide intermediate steps in the final answer.",
            "solution": "The problem is deemed valid as it is scientifically grounded in statistical mechanics, well-posed, objective, and contains sufficient information for a unique solution.\n\nThe potential of mean force (PMF), denoted by $F(\\xi)$, along a reaction coordinate $\\xi$ for a system in the canonical ensemble at temperature $T$ is defined by the equilibrium probability density $P(\\xi)$ of finding the system at a value $\\xi$ for the reaction coordinate:\n$$F(\\xi) = -k_B T \\ln P(\\xi) + C_0$$\nwhere $k_B$ is the Boltzmann constant and $C_0$ is an arbitrary additive constant. The probability density $P(\\xi')$ is given by the statistical average over the canonical ensemble:\n$$P(\\xi') = \\frac{\\int \\delta(\\xi(x) - \\xi') \\exp\\left(-\\frac{U(x)}{k_B T}\\right) dV}{\\int \\exp\\left(-\\frac{U(x)}{k_B T}\\right) dV}$$\nwhere the integration is over the entire configuration space volume $V$. In this one-dimensional problem, the configuration space is simply the coordinate line $x \\in (-\\infty, \\infty)$, and the volume element is $dx$. The reaction coordinate is given as the identity map, $\\xi(x) = x$. Therefore, $\\xi'$ is simply a value of the coordinate, which we can denote by $x'$. The probability density becomes:\n$$P(x') = \\frac{\\int_{-\\infty}^{\\infty} \\delta(x - x') \\exp\\left(-\\frac{U(x)}{k_B T}\\right) dx}{\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{U(x)}{k_B T}\\right) dx}$$\nUsing the sifting property of the Dirac delta function, $\\int f(x)\\delta(x-a)dx = f(a)$, the numerator simplifies to $\\exp(-U(x')/(k_B T))$. The denominator is the partition function $Z = \\int_{-\\infty}^{\\infty} \\exp(-U(x)/(k_B T)) dx$. Thus,\n$$P(x') = \\frac{1}{Z} \\exp\\left(-\\frac{U(x')}{k_B T}\\right)$$\nSubstituting this into the definition of the PMF, let's use $x$ as the variable for the reaction coordinate value:\n$$F(x) = -k_B T \\ln\\left[\\frac{1}{Z} \\exp\\left(-\\frac{U(x)}{k_B T}\\right)\\right] + C_0$$\n$$F(x) = -k_B T \\left(-\\ln Z - \\frac{U(x)}{k_B T}\\right) + C_0$$\n$$F(x) = U(x) + k_B T \\ln Z + C_0$$\nCombining the constant terms into a single constant $C$, we find that the PMF is identical to the potential energy up to an additive constant:\n$$F(x) = U(x) + C$$\nThis result can be confirmed using the Blue Moon ensemble mean-force identity, which states that the derivative of the PMF is the constrained average of the generalized force: $\\frac{dF}{d\\xi} = \\langle -\\frac{\\partial U}{\\partial n} \\rangle_{\\xi}$, where $\\frac{\\partial}{\\partial n}$ is the gradient in the direction of the reaction coordinate. For $\\xi(x)=x$, the direction is simply along the $x$-axis, so $\\frac{\\partial U}{\\partial n} = \\frac{dU}{dx}$. The constrained average $\\langle \\cdot \\rangle_{\\xi}$ is over all microscopic configurations for which $\\xi(x)$ has a fixed value. Since $\\xi(x)=x$, the constraint $x=\\text{const}$ fixes the entire configuration. The average is therefore trivial, and we have:\n$$\\frac{dF}{dx} = \\frac{dU}{dx}$$\nIntegrating this confirms that $F(x) = U(x) + C$.\n\nThe problem specifies a normalization condition for the free energy along the string, $F(\\alpha)$, parameterizing the path from $x=-x_0$ to $x=x_0$. The path is given by $x(\\alpha) = -x_0 + 2x_0\\alpha =\nx_0(2\\alpha-1)$ for $\\alpha \\in [0,1]$. The free energy along the string is $F(\\alpha) \\equiv F(x(\\alpha))$. The condition is $F(0)=0$, where $F(0)$ corresponds to $F(\\alpha=0)$.\nAt $\\alpha=0$, the position is $x(0) = -x_0$. The condition is $F(x(-x_0))=0$. Using our expression for $F(x)$:\n$$F(x(-x_0)) = U(-x_0) + C = 0$$\nThe potential is $U(x) = \\frac{k}{4}(x^2 - x_0^2)^2$. At $x=-x_0$:\n$$U(-x_0) = \\frac{k}{4}((-x_0)^2 - x_0^2)^2 = \\frac{k}{4}(x_0^2 - x_0^2)^2 = 0$$\nTherefore, $0 + C = 0$, which implies the constant $C=0$. With this choice of reference, the PMF is exactly equal to the potential energy:\n$$F(x) = U(x)$$\nNow we can find the closed-form expression for $F(\\alpha)$:\n$$F(\\alpha) = F(x(\\alpha)) = U(x(\\alpha)) = \\frac{k}{4}\\left([x(\\alpha)]^2 - x_0^2\\right)^2$$\nSubstitute $x(\\alpha) = x_0(2\\alpha-1)$:\n$$F(\\alpha) = \\frac{k}{4}\\left([x_0(2\\alpha-1)]^2 - x_0^2\\right)^2 = \\frac{k}{4}\\left(x_0^2(2\\alpha-1)^2 - x_0^2\\right)^2$$\nFactor out $x_0^2$ from the inner parenthesis:\n$$F(\\alpha) = \\frac{k}{4}\\left(x_0^2[(2\\alpha-1)^2 - 1]\\right)^2 = \\frac{k x_0^4}{4}\\left[(2\\alpha-1)^2 - 1\\right]^2$$\nExpand the term $(2\\alpha-1)^2 = 4\\alpha^2 - 4\\alpha + 1$:\n$$F(\\alpha) = \\frac{k x_0^4}{4}\\left[(4\\alpha^2 - 4\\alpha + 1) - 1\\right]^2 = \\frac{k x_0^4}{4}\\left[4\\alpha^2 - 4\\alpha\\right]^2$$\nFactor out $4\\alpha$ from the inner bracket:\n$$F(\\alpha) = \\frac{k x_0^4}{4}\\left[4\\alpha(\\alpha - 1)\\right]^2 = \\frac{k x_0^4}{4} \\cdot 16\\alpha^2(\\alpha - 1)^2$$\n$$F(\\alpha) = 4 k x_0^4 \\alpha^2 (\\alpha - 1)^2$$\nThis is the required closed-form expression.\n\nFinally, we compare the barrier height of the PMF to that of the potential $U(x)$. The saddle point of the potential $U(x)$ is at $x=0$. The minima are at $x=\\pm x_0$, where $U(\\pm x_0)=0$. The barrier height in the potential is:\n$$\\Delta U = U(0) - U(-x_0) = \\frac{k}{4}(0^2-x_0^2)^2 - 0 = \\frac{k x_0^4}{4}$$\nThe PMF along the string is $F(\\alpha)$. The minima are at $\\alpha=0$ and $\\alpha=1$ (corresponding to $x=-x_0$ and $x=x_0$), where $F(0)=F(1)=0$. The saddle point at $x=0$ corresponds to $\\alpha=1/2$. The barrier height along the PMF path is:\n$$\\Delta F = F(1/2) - F(0) = 4 k x_0^4 (1/2)^2(1/2 - 1)^2 - 0 = 4 k x_0^4 \\left(\\frac{1}{4}\\right)\\left(-\\frac{1}{2}\\right)^2 = k x_0^4 \\left(\\frac{1}{4}\\right) = \\frac{k x_0^4}{4}$$\nThus, $\\Delta F = \\Delta U$. The barrier heights coincide. This is a direct consequence of the fact that for a one-dimensional system with the spatial coordinate as the reaction coordinate, there are no other degrees of freedom to average over. The PMF is therefore equivalent to the potential energy (up to a constant), and their topographical features, including barrier heights, are identical. In higher-dimensional systems, the PMF includes entropic contributions from averaging over the orthogonal degrees of freedom, and the PMF barrier height generally does not coincide with the potential energy barrier height.",
            "answer": "$$\n\\boxed{4 k x_{0}^{4} \\alpha^{2} (\\alpha - 1)^{2}}\n$$"
        },
        {
            "introduction": "When we constrain a simulation to sample specific values of a reaction coordinate $\\xi$, as in the Blue Moon ensemble, we must account for the geometric distortion this constraint imposes on the configuration space. This exercise  introduces the crucial Jacobian correction factor, which arises from the mass-weighted metric and ensures that the resulting free energy calculation is unbiased. You will derive this factor from first principles for a general linear reaction coordinate, revealing how it depends on the coordinate's definition and the system's mass distribution.",
            "id": "3822435",
            "problem": "Consider a molecular system with $n$ Cartesian coordinates collected in $x \\in \\mathbb{R}^{n}$ and a diagonal mass matrix $M=\\mathrm{diag}(m_{1},m_{2},\\dots,m_{n})$ with positive entries $m_{i}$. In constrained sampling of rare events using the Blue Moon Ensemble (BME), the constrained configurational measure on the hypersurface $\\{\\xi(x)=\\xi_{0}\\}$ includes a Jacobian factor. This factor is calculated using an inner product defined by the inverse mass matrix: $\\langle u,v \\rangle = u^{\\top} M^{-1} v$. The Jacobian factor is the norm of the gradient $\\nabla\\xi$ with respect to this inner product.\n\nLet the reaction coordinate be linear, $\\xi(x) = a^{\\top} x$, with a constant vector $a = (a_{1},a_{2},\\dots,a_{n})^{\\top}$. Using only the definitions of the gradient in Cartesian coordinates and the provided inner product, derive the expression for the Jacobian factor in terms of $a_{i}$ and $m_{i}$. Then, explain whether this factor depends on the configuration $x$ (that is, whether it varies along the constrained hypersurface or along a string parametrized by the same $\\xi$ used in the String Method).\n\nExpress your final answer as a single closed-form analytic expression. No rounding is required, and no units are necessary.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It provides a clear set of definitions and asks for a specific derivation and conceptual analysis within the established framework of computational statistical mechanics, specifically concerning rare event sampling methods. All necessary information is provided, and there are no contradictions or ambiguities.\n\nWe begin by following the definitions provided in the problem statement. The system is described by $n$ Cartesian coordinates, denoted by the vector $x \\in \\mathbb{R}^{n}$. The reaction coordinate, $\\xi(x)$, is given as a linear function of these coordinates:\n$$\n\\xi(x) = a^{\\top} x = \\sum_{i=1}^{n} a_i x_i\n$$\nwhere $a = (a_1, a_2, \\dots, a_n)^{\\top}$ is a vector of constant coefficients.\n\nThe first step is to compute the gradient of the reaction coordinate, $\\nabla \\xi(x)$, in Cartesian coordinates. The gradient is a vector whose $j$-th component is the partial derivative of $\\xi$ with respect to the coordinate $x_j$:\n$$\n(\\nabla \\xi(x))_j = \\frac{\\partial \\xi}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{i=1}^{n} a_i x_i \\right)\n$$\nSince the coordinates $x_i$ are independent, the partial derivative is non-zero only when $i=j$:\n$$\n\\frac{\\partial}{\\partial x_j} \\left( \\sum_{i=1}^{n} a_i x_i \\right) = \\sum_{i=1}^{n} a_i \\frac{\\partial x_i}{\\partial x_j} = \\sum_{i=1}^{n} a_i \\delta_{ij} = a_j\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. Therefore, the gradient vector is:\n$$\n\\nabla \\xi(x) = (a_1, a_2, \\dots, a_n)^{\\top} = a\n$$\nAn important observation is that since the vector $a$ is constant, the gradient $\\nabla \\xi$ is also a constant vector, independent of the configuration $x$.\n\nNext, we need to calculate the Jacobian factor, which is the norm of this gradient, $\\sqrt{\\langle \\nabla\\xi, \\nabla\\xi \\rangle}$. The inner product is given as $\\langle u, v \\rangle = u^{\\top} M^{-1} v$. Substituting $u=v=\\nabla \\xi = a$, we get:\n$$\n\\text{Jacobian Factor} = \\sqrt{a^{\\top} M^{-1} a}\n$$\nThe mass matrix $M$ is given as a diagonal matrix with positive entries $m_i$:\n$$\nM = \\mathrm{diag}(m_1, m_2, \\dots, m_n) = \\begin{pmatrix} m_1 & 0 & \\dots & 0 \\\\ 0 & m_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & m_n \\end{pmatrix}\n$$\nThe inverse of a diagonal matrix is a diagonal matrix whose entries are the reciprocals of the original entries. Thus, $M^{-1}$ is:\n$$\nM^{-1} = \\mathrm{diag}(m_1^{-1}, m_2^{-1}, \\dots, m_n^{-1}) = \\begin{pmatrix} m_1^{-1} & 0 & \\dots & 0 \\\\ 0 & m_2^{-1} & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & m_n^{-1} \\end{pmatrix}\n$$\nNow we can compute the quadratic form $a^{\\top} M^{-1} a$:\n$$\na^{\\top} M^{-1} a = \\begin{pmatrix} a_1 & a_2 & \\dots & a_n \\end{pmatrix} \\begin{pmatrix} m_1^{-1} & 0 & \\dots & 0 \\\\ 0 & m_2^{-1} & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & m_n^{-1} \\end{pmatrix} \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{pmatrix}\n$$\nPerforming the matrix-vector multiplication, we obtain:\n$$\na^{\\top} M^{-1} a = \\begin{pmatrix} a_1 & a_2 & \\dots & a_n \\end{pmatrix} \\begin{pmatrix} a_1/m_1 \\\\ a_2/m_2 \\\\ \\vdots \\\\ a_n/m_n \\end{pmatrix}\n$$\nFinally, computing the dot product gives the scalar result:\n$$\na^{\\top} M^{-1} a = \\sum_{i=1}^{n} \\frac{a_i^2}{m_i}\n$$\nSubstituting this result back into the expression for the norm, we arrive at the closed-form expression for the Jacobian factor:\n$$\n\\text{Jacobian Factor} = \\sqrt{\\sum_{i=1}^{n} \\frac{a_i^2}{m_i}}\n$$\nThis expression depends only on the components $a_i$ of the vector $a$ and the masses $m_i$.\n\nFor the second part of the question, we must determine if the Jacobian factor depends on the configuration $x$.\nThe derived expression is $\\sqrt{\\sum_{i=1}^{n} \\frac{a_i^2}{m_i}}$. By the problem statement, the vector $a$ is constant, meaning its components $a_i$ are constants. The masses $m_i$ are also physical constants of the system. The expression does not contain the variable $x$ or any of its components $x_i$. Therefore, the value of the Jacobian factor is a constant.\nThis means that for a linear reaction coordinate, the Jacobian factor does not depend on the configuration $x$. It has the same constant value for all points on the constrained hypersurface $\\{\\xi(x)=\\xi_0\\}$ in the Blue Moon Ensemble. Similarly, it is constant along the entire path of a string parametrized by $\\xi$ in the String Method, provided that the reaction coordinate is linear as defined. The constancy of this factor simplifies the calculation of thermodynamic properties, such as the potential of mean force, from constrained simulations.",
            "answer": "$$\n\\boxed{\\sqrt{\\sum_{i=1}^{n} \\frac{a_{i}^{2}}{m_{i}}}}\n$$"
        },
        {
            "introduction": "Beyond theoretical understanding, applying the string method effectively is a matter of computational science, requiring a strategic allocation of limited resources. This final practice  moves into the practical realm of simulation design, modeling the trade-off between the number of images, $N$, used to discretize the path and the amount of sampling, $M$, performed for each image. By learning to optimize the total error under a fixed computational budget, you will develop a quantitative framework for making one of the most critical decisions when planning rare event simulations.",
            "id": "3822388",
            "problem": "Consider a simplified quantitative model for the computational design of the String Method for rare events combined with constrained sampling using the Blue Moon ensemble along a reaction pathway. Suppose the continuous Minimum Free Energy Path (MFEP) is smooth with bounded curvature, and it is approximated by a piecewise-linear string of $N$ windows (discrete images). For each window, a constrained sampling is performed with $M$ independent samples to estimate local mean forces. Assume the following modeling hypotheses based on fundamental, well-tested facts.\n\n- The interpolation error of a sufficiently smooth curve by a polygonal approximation scales like $O(h^{2})$ with $h$ the segment size, which gives a discretization contribution to the mean squared path error of the form $E_{\\mathrm{disc}}(N)=a N^{-p}$ with $p \\ge 1$ and $a>0$. For piecewise-linear approximations of $C^{2}$ curves, one typically takes $p=2$.\n\n- The Monte Carlo variance of a sample mean from independent, identically distributed data scales like the variance divided by the number of samples, which in this setting yields a sampling contribution per window that scales like $O(1/M)$. Aggregating errors along $N$ windows in a root-mean-square fashion with homogeneous weights produces a sampling contribution $E_{\\mathrm{samp}}(N,M)=b\\,N/M$ with $b>0$ incorporating variance and geometric weights.\n\n- The total mean squared error is modeled additively as $E_{\\mathrm{tot}}(N,M)=E_{\\mathrm{disc}}(N)+E_{\\mathrm{samp}}(N,M)=a N^{-p}+b\\,N/M$.\n\n- The computational budget constraint is modeled as $C(N,M)=c_{s}\\,N\\,M+c_{i}\\,N \\le B$, where $c_{s}>0$ is the per-sample cost within a window, $c_{i}\\ge 0$ is a per-window overhead cost (e.g., string reparametrization or constraint setup), and $B>0$ is the total available budget. The decision variables $N$ and $M$ must be integers with $N \\ge 2$ and $M \\ge 1$.\n\nYour task is to, for each parameter set, compute the integer pair $(N^{\\star},M^{\\star})$ that minimizes $E_{\\mathrm{tot}}(N,M)$ subject to the budget constraint and integrality. If there are multiple minimizers with identical objective value, break ties by selecting the smallest $N$, and, if still tied, the smallest $M$.\n\nDerivations should be grounded in the aforementioned scaling laws for interpolation error and Monte Carlo variance, and in the linear budget model. From these principles, deduce a computationally efficient algorithm to find the optimal integers. You must not assume any additional formulas beyond the stated foundational laws.\n\nYour program must implement the algorithm and solve the following test suite. For each test case, use the given parameters $(a,b,p,c_{s},c_{i},B)$, where all quantities are in arbitrary consistent computational cost units without physical dimensions. Ensure that $N$ and $M$ are integers and the budget constraint is satisfied.\n\n- Test $1$: $a=1$, $b=1$, $p=2$, $c_{s}=1$, $c_{i}=0$, $B=1000$.\n\n- Test $2$: $a=1$, $b=2$, $p=2$, $c_{s}=1$, $c_{i}=50$, $B=1000$.\n\n- Test $3$: $a=0.5$, $b=1.5$, $p=2$, $c_{s}=2$, $c_{i}=20$, $B=120$.\n\n- Test $4$: $a=2$, $b=0.5$, $p=1$, $c_{s}=1$, $c_{i}=0$, $B=500$.\n\nRequirements for the final output format:\n\n- For each test case, output the triple $[N^{\\star},M^{\\star},E_{\\mathrm{tot}}(N^{\\star},M^{\\star})]$ where $N^{\\star}$ and $M^{\\star}$ are integers and $E_{\\mathrm{tot}}(N^{\\star},M^{\\star})$ is a decimal rounded to $6$ places using standard rounding rules.\n\n- Your program should produce a single line of output containing the results for all tests as a comma-separated list enclosed in square brackets, in the order of the tests above. For example, the output should look like $[[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot]]$ with no additional text.\n\n- If a test case is infeasible (i.e., no integers $N \\ge 2$, $M \\ge 1$ satisfy the budget), return $[0,0,\\mathrm{nan}]$ for that case, where $\\mathrm{nan}$ denotes a not-a-number floating value.",
            "solution": "The problem presented is a constrained integer optimization problem. We are tasked with minimizing a total mean squared error function, $E_{\\mathrm{tot}}(N,M)$, subject to a computational budget constraint. The decision variables are the number of discrete windows, $N$, and the number of samples per window, $M$.\n\nFirst, we shall formally state the problem based on the provided givens.\n\n**Problem Formulation**\n\nThe objective is to find the integer pair $(N^{\\star}, M^{\\star})$ that solves:\n$$\n\\min_{N, M} E_{\\mathrm{tot}}(N,M) = a N^{-p} + b \\frac{N}{M}\n$$\nsubject to the constraints:\n$$\n\\begin{cases}\nc_{s}NM + c_{i}N \\le B \\\\\nN \\in \\mathbb{Z}, N \\ge 2 \\\\\nM \\in \\mathbb{Z}, M \\ge 1\n\\end{cases}\n$$\nwhere the parameters $a>0$, $b>0$, $p \\ge 1$, $c_s>0$, $c_i \\ge 0$, and $B>0$ are given.\n\n**Analysis of the Optimization Problem**\n\nThe error function $E_{\\mathrm{tot}}(N,M)$ is composed of two terms: a discretization error $E_{\\mathrm{disc}}(N) = a N^{-p}$ and a sampling error $E_{\\mathrm{samp}}(N,M) = b N/M$. The discretization error is a decreasing function of $N$, while the sampling error is an increasing function of $N$ and a decreasing function of $M$. To minimize the total error, we should make $N$ and $M$ as large as possible. This implies that the optimal solution will lie on the boundary of the feasible region defined by the budget constraint, i.e., it will utilize the maximum available budget.\nTherefore, we can analyze the problem by considering the budget constraint as an equality:\n$$\nc_{s}NM + c_{i}N = B\n$$\nThis equation links the variables $N$ and $M$. We can express $M$ as a function of $N$:\n$$\nN(c_{s}M + c_{i}) = B \\implies c_{s}M = \\frac{B}{N} - c_{i} \\implies M(N) = \\frac{1}{c_{s}}\\left(\\frac{B}{N} - c_{i}\\right)\n$$\nSubstituting this expression for $M$ into the total error function would yield a function of $N$ alone:\n$$\nE(N) = a N^{-p} + bN \\left( \\frac{c_s}{B/N - c_i} \\right) = a N^{-p} + \\frac{b c_s N^2}{B - c_i N}\n$$\nFinding the minimum of this function for a continuous variable $N$ would involve solving $\\frac{dE}{dN} = 0$, which leads to a high-degree polynomial equation in $N$ with no straightforward general analytical solution.\n\nHowever, the variables $N$ and $M$ are integers. This suggests an algorithmic approach by performing a structured search over the finite space of feasible integer pairs $(N,M)$.\n\n**Algorithmic Design**\n\nThe search space for $(N, M)$ is bounded. We can establish a hard upper limit on the variable $N$. From the budget constraint and the condition $M \\ge 1$:\n$$\nc_{s}N(1) + c_{i}N \\le B \\implies (c_{s} + c_{i})N \\le B \\implies N \\le \\frac{B}{c_{s} + c_{i}}\n$$\nSince $N$ must be an integer, its valid range is $2 \\le N \\le \\lfloor \\frac{B}{c_{s} + c_{i}} \\rfloor$. Let us denote $N_{\\mathrm{max}} = \\lfloor \\frac{B}{c_{s} + c_{i}} \\rfloor$. If $N_{\\mathrm{max}} < 2$, no feasible solution exists.\n\nFor any given valid integer $N$ in this range, the error term $E_{\\mathrm{tot}}(N,M) = a N^{-p} + bN/M$ is a monotonically decreasing function of $M$. To minimize the error for a fixed $N$, we must choose the largest possible integer value for $M$ that satisfies the budget constraint.\nFrom the budget constraint $c_{s}NM + c_{i}N \\le B$, we can solve for $M$:\n$$\nc_{s}NM \\le B - c_{i}N \\implies M \\le \\frac{B - c_{i}N}{c_{s}N}\n$$\nSince $M$ must be an integer, the optimal choice for a given $N$ is:\n$$\nM^{\\star}(N) = \\left\\lfloor \\frac{B - c_{i}N}{c_{s}N} \\right\\rfloor\n$$\nThe condition $M \\ge 1$ must hold for this pair $(N, M^{\\star}(N))$ to be feasible. This is guaranteed for all $N$ in the range $[2, N_{\\mathrm{max}}]$.\n\nThis leads to the following exhaustive search algorithm:\n1.  Calculate the maximum possible value for $N$, $N_{\\mathrm{max}} = \\lfloor B / (c_{s} + c_{i}) \\rfloor$.\n2.  If $N_{\\mathrm{max}} < 2$, the problem is infeasible.\n3.  Initialize a minimum error, $E_{\\min}$, to a value of infinity, and optimal parameters $(N^{\\star}, M^{\\star})$ to an invalid state (e.g., $(0,0)$).\n4.  Iterate through all integers $N$ from $2$ to $N_{\\mathrm{max}}$.\n    a. For each $N$, calculate the corresponding optimal integer $M$ that maximizes budget utilization: $M = \\lfloor (B - c_{i}N) / (c_{s}N) \\rfloor$. As established, $M$ will be at least $1$.\n    b. Calculate the total error for this pair: $E(N, M) = aN^{-p} + bN/M$.\n    c. If $E(N, M) < E_{\\min}$, update the optimal solution: $E_{\\min} = E(N, M)$, $N^{\\star} = N$, and $M^{\\star} = M$.\n5.  The tie-breaking rule states that for identical error values, the solution with the smallest $N$ is preferred. Since our algorithm iterates $N$ in increasing order, the first time a minimum error is found, it will be with the smallest possible $N$. Subsequent pairs with the same error will have larger $N$ values and will not replace the current optimum. The rule for $M$ is automatically satisfied as for a fixed $N$, there is only one optimal $M$.\n6.  After the loop completes, the triplet $(N^{\\star}, M^{\\star}, E_{\\min})$ is the solution. If no feasible solution was found (i.e., the loop was never entered), the case is reported as infeasible.\n\nThis algorithm is computationally efficient as the search space for $N$ is relatively small for typical budget values $B$. It guarantees finding the global integer optimum by systematically exploring all promising candidates.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of constrained integer optimization problems related to\n    the String Method and Blue Moon ensemble.\n    \"\"\"\n    test_cases = [\n        # (a, b, p, c_s, c_i, B)\n        (1.0, 1.0, 2.0, 1.0, 0.0, 1000.0),\n        (1.0, 2.0, 2.0, 1.0, 50.0, 1000.0),\n        (0.5, 1.5, 2.0, 2.0, 20.0, 120.0),\n        (2.0, 0.5, 1.0, 1.0, 0.0, 500.0),\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        a, b, p, c_s, c_i, B = case\n        \n        opt_N, opt_M = 0, 0\n        min_E = float('inf')\n        found_solution = False\n\n        # Determine the search range for N.\n        # N must be >= 2. M must be >= 1.\n        # c_s*N*M + c_i*N <= B\n        # N * (c_s*M + c_i) <= B\n        # Since M >= 1, c_s*M + c_i >= c_s + c_i.\n        # N * (c_s + c_i) <= B  => N <= B / (c_s + c_i)\n        # c_s > 0, so c_s + c_i > 0, no division by zero.\n        \n        if c_s + c_i == 0:\n            # Avoid division by zero, though problem constraints c_s > 0.\n            N_max = float('inf')\n        else:\n            N_max = int(np.floor(B / (c_s + c_i)))\n\n        # Iterate through all plausible integer values of N.\n        if N_max >= 2:\n            for N in range(2, N_max + 1):\n                # For a fixed N, to minimize error, we must choose the largest\n                # possible integer M allowed by the budget.\n                # M <= (B - c_i*N) / (c_s*N)\n                \n                M = int(np.floor((B - c_i * N) / (c_s * N)))\n                if M < 1: continue\n                \n                # Calculate the total error E_tot = a*N**(-p) + b*N/M\n                E = a * (N ** (-p)) + b * N / M\n\n                # Check for a new minimum. The loop iterates N in increasing\n                # order, so the tie-breaking rule (smallest N first) is\n                # naturally handled.\n                if E < min_E:\n                    min_E = E\n                    opt_N = N\n                    opt_M = M\n                    found_solution = True\n        \n        if found_solution:\n            # Format the successful result according to requirements.\n            # The .6f format specifier performs standard rounding.\n            result_str = f\"[{opt_N},{opt_M},{min_E:.6f}]\"\n        else:\n            # Format the infeasible case result.\n            result_str = \"[0,0,nan]\"\n            \n        final_results.append(result_str)\n\n    # Print the final output in the exact specified format.\n    print(f\"[{','.join(final_results)}]\")\n\n# This is a helper function to be executed by the platform, not part of the answer content.\n# solve()\n```"
        }
    ]
}