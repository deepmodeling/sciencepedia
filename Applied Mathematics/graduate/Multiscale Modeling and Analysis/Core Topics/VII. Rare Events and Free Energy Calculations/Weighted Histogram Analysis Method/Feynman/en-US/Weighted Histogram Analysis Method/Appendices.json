{
    "hands_on_practices": [
        {
            "introduction": "The Weighted Histogram Analysis Method (WHAM) is fundamentally an iterative algorithm that solves a set of self-consistent equations. This first practice invites you to implement these core equations for a simple \"toy\" system, guiding you to code the fixed-point iteration that couples the unbiased probability distribution and the window free energies. By completing this exercise, you will gain a foundational, hands-on understanding of how WHAM combines data from multiple biased simulations to reconstruct the underlying free energy landscape. ",
            "id": "3832602",
            "problem": "Consider the Weighted Histogram Analysis Method (WHAM) for combining biased histograms from multiple windows to estimate the underlying unbiased probability mass function over a discrete reaction coordinate. Weighted Histogram Analysis Method (WHAM) is a maximum-likelihood estimator for the unbiased distribution that corrects for biasing potentials applied in separate simulation windows and solves a self-consistent fixed-point problem coupling the unbiased distribution and window normalization free energies. In this problem, there are two windows and three discrete bins per window. Energies are reported in units of Boltzmann’s constant times temperature $k_{\\mathrm{B}} T$ so that inverse temperatures are dimensionless. Statistical inefficiencies are ignored (set to $1$) and bin widths are taken to be constant and absorbed into normalization.\n\nStarting from a uniform initial distribution $p^{(0)}(x_j)$ over $j \\in \\{1,2,3\\}$, implement the fixed-point iteration implied by the WHAM equations to update the unbiased distribution $p^{(t)}(x_j)$ and the window free energies $f_k^{(t)}$ for $k \\in \\{1,2\\}$, with normalization of $p^{(t)}(x_j)$ at each step to ensure $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$. Stop when the maximum absolute change $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right|$ is less than the tolerance $10^{-6}$. Quantify the number of iterations required to reach the tolerance for each of the test cases below. Your program should begin with $p^{(0)}(x_j) = 1/3$ for all $j$, use the provided histogram counts and bias potentials, and treat the inverse temperature values as given. All computations should be performed in pure mathematical terms with no physical units reported.\n\nTest Suite:\n- Case A (balanced counts, symmetric biases, equal temperature):\n  - Window $1$ counts $H^{(1)} = [60,120,60]$, window $2$ counts $H^{(2)} = [40,80,40]$.\n  - Window $1$ bias potentials $U^{(1)} = [0.0,0.5,1.0]$, window $2$ bias potentials $U^{(2)} = [1.0,0.5,0.0]$.\n  - Inverse temperatures $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$.\n- Case B (scarce counts, asymmetric coverage, equal temperature):\n  - Window $1$ counts $H^{(1)} = [1,0,0]$, window $2$ counts $H^{(2)} = [0,1,1]$.\n  - Window $1$ bias potentials $U^{(1)} = [0.0,1.0,2.0]$, window $2$ bias potentials $U^{(2)} = [2.0,1.0,0.0]$.\n  - Inverse temperatures $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$.\n- Case C (unequal temperatures, mildly varying biases):\n  - Window $1$ counts $H^{(1)} = [100,50,25]$, window $2$ counts $H^{(2)} = [10,20,40]$.\n  - Window $1$ bias potentials $U^{(1)} = [0.2,0.2,0.2]$, window $2$ bias potentials $U^{(2)} = [0.0,0.5,1.0]$.\n  - Inverse temperatures $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 0.5$.\n- Case D (extreme biases with strongly polarized counts, equal temperature):\n  - Window $1$ counts $H^{(1)} = [200,10,1]$, window $2$ counts $H^{(2)} = [1,10,200]$.\n  - Window $1$ bias potentials $U^{(1)} = [0.0,5.0,10.0]$, window $2$ bias potentials $U^{(2)} = [10.0,5.0,0.0]$.\n  - Inverse temperatures $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$.\n\nAlgorithmic Requirements:\n- Initialize $p^{(0)}(x_j) = 1/3$ for $j \\in \\{1,2,3\\}$.\n- At each iteration $t$, update the window free energies $f_k^{(t)}$ using the normalization condition for window $k$, and then update the unbiased distribution $p^{(t)}(x_j)$ by combining the histograms from both windows with appropriate reweighting by the window biases and $f_k^{(t)}$, followed by normalization so that $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$.\n- Use the stopping criterion $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right| < 10^{-6}$ and report the total number of iterations executed to reach the tolerance.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets (e.g., $[7,42,13,5]$), where the integers correspond to the iteration counts for Cases A, B, C, and D, in that order.",
            "solution": "The problem has been validated and is determined to be a valid, well-posed scientific problem. All necessary data and conditions are provided, and the task aligns with established principles in statistical mechanics and computational science.\n\nThe problem requires the implementation of the Weighted Histogram Analysis Method (WHAM) to find the unbiased probability mass function, $p(x_j)$, for a system with a discrete reaction coordinate defined by $M=3$ bins. The data is derived from simulations in $K=2$ windows, each biased by a potential $U^{(k)}(x_j)$. The goal is to solve the WHAM equations iteratively for four different test cases and report the number of iterations required for convergence.\n\nThe WHAM equations form a set of self-consistent relations for the unbiased probabilities $p(x_j)$ and the dimensionless free energies $f_k$ associated with each simulation window $k$. For a system with $K$ windows and $M$ bins, these equations are:\n$$\np(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k e^{\\beta^{(k)} f_k} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n\\quad \\quad (1)\n$$\n$$\ne^{-\\beta^{(k)} f_k} = \\sum_{j=1}^{M} p(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)}\n\\quad \\quad (2)\n$$\nHere, $H^{(k)}(x_j)$ is the number of counts observed in bin $j$ from window $k$, $N_k = \\sum_{j=1}^{M} H^{(k)}(x_j)$ is the total number of samples from window $k$, $U^{(k)}(x_j)$ is the bias potential applied to bin $j$ in window $k$, and $\\beta^{(k)}$ is the corresponding inverse temperature (given as a dimensionless quantity). The probability distribution must satisfy the normalization condition $\\sum_{j=1}^{M} p(x_j) = 1$. Note that equation $(1)$ must be interpreted such that the resulting $p(x_j)$ is normalized.\n\nThese coupled, non-linear equations can be solved using a fixed-point iterative scheme. The algorithm proceeds as follows:\n\n1.  **Initialization**: Begin with an initial guess for the probability distribution, $p^{(0)}(x_j)$. The problem specifies a uniform distribution:\n    $$\n    p^{(0)}(x_j) = \\frac{1}{M}\n    $$\n    for $j \\in \\{1, 2, ..., M\\}$. Set the iteration counter $t=0$ and a tolerance $\\epsilon=10^{-6}$.\n\n2.  **Iteration**: For $t = 1, 2, 3, \\dots$, perform the following steps:\n    a.  **Update Free Energies**: Calculate the free energy $f_k^{(t)}$ for each window $k$ using the probability distribution from the previous iteration, $p^{(t-1)}(x_j)$. Rearranging equation $(2)$ gives:\n        $$\n        f_k^{(t)} = -\\frac{1}{\\beta^{(k)}} \\ln\\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)\n        $$\n        For numerical stability, it is often better to compute the quantities $C_k^{(t)} = e^{\\beta^{(k)} f_k^{(t)}}$:\n        $$\n        C_k^{(t)} = \\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)^{-1}\n        $$\n\n    b.  **Update Probabilities**: Use the updated free energy terms $C_k^{(t)}$ to compute a new, unnormalized probability distribution $p_{\\text{un}}^{(t)}(x_j)$ based on equation $(1)$:\n        $$\n        p_{\\text{un}}^{(t)}(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k C_k^{(t)} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n        $$\n        The numerator is the total number of counts in bin $j$ across all windows. The denominator is a reweighting factor that combines information from all windows.\n\n    c.  **Normalization**: The updated probability distribution $p^{(t)}(x_j)$ is obtained by normalizing $p_{\\text{un}}^{(t)}(x_j)$:\n        $$\n        p^{(t)}(x_j) = \\frac{p_{\\text{un}}^{(t)}(x_j)}{\\sum_{j'=1}^{M} p_{\\text{un}}^{(t)}(x_{j'})}\n        $$\n\n    d.  **Convergence Check**: The iteration is terminated when the maximum absolute difference between the current and previous probability distributions is less than the specified tolerance $\\epsilon$:\n        $$\n        \\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right| < \\epsilon\n        $$\n        If the condition is met, the process stops. Otherwise, increment $t$ and return to step 2a.\n\nThe final output is the number of iterations required to satisfy the convergence criterion for each of the four test cases provided. The implementation will utilize `numpy` for efficient vectorized calculations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_wham_iteration(H, U, beta, tol=1e-6):\n    \"\"\"\n    Performs WHAM fixed-point iteration to find the unbiased probability distribution.\n\n    Args:\n        H (np.ndarray): A (num_windows, num_bins) array of histogram counts.\n        U (np.ndarray): A (num_windows, num_bins) array of bias potentials.\n        beta (np.ndarray): A (num_windows,) array of inverse temperatures.\n        tol (float): The convergence tolerance.\n\n    Returns:\n        int: The number of iterations required to reach convergence.\n    \"\"\"\n    num_windows, num_bins = H.shape\n    \n    # Total number of samples in each window\n    N = np.sum(H, axis=1)\n    \n    # Total histogram counts across all windows for each bin\n    total_H = np.sum(H, axis=0)\n    \n    # Initial guess for the probability distribution (uniform)\n    p = np.full(num_bins, 1.0 / num_bins, dtype=np.float64)\n    \n    iterations = 0\n    while True:\n        iterations += 1\n        p_old = p.copy()\n        \n        # Pre-compute exponential terms for efficiency\n        # exp(-beta_k * U_kj) for all k, j\n        exp_neg_beta_U = np.exp(-beta[:, np.newaxis] * U)\n        \n        # Step 2a: Update free energies. We compute C_k = exp(beta_k * f_k).\n        # C_k = 1 / sum_j(p_j * exp(-beta_k * U_kj))\n        sum_val = np.sum(p_old[np.newaxis, :] * exp_neg_beta_U, axis=1)\n        # Avoid division by zero, although p_old and exp are always positive\n        # for this problem's setup.\n        C = 1.0 / sum_val\n        \n        # Step 2b: Update unnormalized probabilities p_un(x_j)\n        # Denominator: sum_k(N_k * C_k * exp(-beta_k * U_kj))\n        denominator = np.sum(N[:, np.newaxis] * C[:, np.newaxis] * exp_neg_beta_U, axis=0)\n        \n        p_unnormalized = np.zeros(num_bins, dtype=np.float64)\n        # Avoid division by zero if a bin has no counts at all\n        non_zero_H_mask = total_H > 0\n        non_zero_denom_mask = denominator > 0\n        valid_mask = non_zero_H_mask & non_zero_denom_mask\n        \n        p_unnormalized[valid_mask] = total_H[valid_mask] / denominator[valid_mask]\n\n        # Step 2c: Normalize probabilities\n        p_sum = np.sum(p_unnormalized)\n        if p_sum > 0:\n            p = p_unnormalized / p_sum\n        else:\n            # This would happen if total_H is all zeros, not the case for these problems.\n            # We can stop if no meaningful update is possible.\n            break\n\n        # Step 2d: Check for convergence\n        error = np.max(np.abs(p - p_old))\n        if error < tol:\n            break\n            \n    return iterations\n\ndef solve():\n    \"\"\"\n    Solves the WHAM iteration problem for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: balanced counts, symmetric biases, equal temperature\n        {\n            \"H\": np.array([[60, 120, 60], [40, 80, 40]], dtype=np.float64),\n            \"U\": np.array([[0.0, 0.5, 1.0], [1.0, 0.5, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case B: scarce counts, asymmetric coverage, equal temperature\n        {\n            \"H\": np.array([[1, 0, 0], [0, 1, 1]], dtype=np.float64),\n            \"U\": np.array([[0.0, 1.0, 2.0], [2.0, 1.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case C: unequal temperatures, mildly varying biases\n        {\n            \"H\": np.array([[100, 50, 25], [10, 20, 40]], dtype=np.float64),\n            \"U\": np.array([[0.2, 0.2, 0.2], [0.0, 0.5, 1.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 0.5], dtype=np.float64),\n        },\n        # Case D: extreme biases with strongly polarized counts, equal temperature\n        {\n            \"H\": np.array([[200, 10, 1], [1, 10, 200]], dtype=np.float64),\n            \"U\": np.array([[0.0, 5.0, 10.0], [10.0, 5.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        iterations = run_wham_iteration(case[\"H\"], case[\"U\"], case[\"beta\"])\n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A naive implementation of WHAM is susceptible to numerical instability, as the calculation of Boltzmann factors often involves exponentiating very large or small numbers, leading to overflow or underflow errors. This exercise addresses this critical practical challenge by guiding you to implement the \"log-sum-exp\" trick, a standard technique in statistical mechanics for working in the logarithmic domain. Mastering this method is essential for developing a robust WHAM code that can handle the wide dynamic range of energies and probabilities encountered in real-world simulations. ",
            "id": "3832603",
            "problem": "You are to implement a numerically stable computation that arises in the Weighted Histogram Analysis Method (WHAM). In WHAM, reweighting of biased ensembles introduces a normalization denominator at a discrete bin indexed by $j$ that combines contributions from multiple simulation windows indexed by $i$. For each window $i$, you are given a sample count $N_i \\in \\mathbb{N}$, a free-energy offset $f_i \\in \\mathbb{R}$, a bias potential evaluated at the bin $V_i(x_j) \\in \\mathbb{R}$, and an inverse thermal energy $\\beta > 0$. The denominator for bin $j$ is the weighted sum over windows of exponentiated linear combinations of these inputs. Direct evaluation of this sum is numerically unstable when the exponential arguments are large in magnitude. Your task is to compute the natural logarithm of this denominator in a numerically stable way using a log-domain aggregation strategy that prevents overflow and underflow, without altering the mathematically exact value.\n\nFundamental base to use:\n- Canonical Boltzmann weighting in the canonical ensemble states that contributions scale as $\\exp(-\\beta U)$ for an energy $U \\in \\mathbb{R}$ at inverse thermal energy $\\beta > 0$.\n- In WHAM, the reweighting across windows introduces additive free-energy offsets and subtracts the applied bias potentials at the state $x_j$, leading to an exponential argument that is an affine function of the inputs.\n\nSpecification:\n- Given arrays $\\{N_i\\}_{i=1}^K$, $\\{f_i\\}_{i=1}^K$, $\\{V_i(x_j)\\}_{i=1}^K$, and a scalar $\\beta > 0$, define\n$$\nD_j \\equiv \\sum_{i=1}^{K} N_i \\exp\\!\\left(\\beta f_i - \\beta V_i(x_j)\\right).\n$$\n- Your program must compute $\\log D_j$ robustly by working in the log-domain and using a numerically stable aggregation that avoids evaluating exponentials of very large positive or very large negative arguments. You must treat any $N_i = 0$ as contributing exactly $0$ to $D_j$ (equivalently, as an excluded term in the aggregation).\n- Use the natural logarithm (base $e$) everywhere.\n\nTest suite:\nProvide code that evaluates $\\log D_j$ for the following five independent test cases. For each case, the program must compute a single real number and round it to $6$ decimal places.\n\n- Case A (happy path, moderate magnitudes): \n  - $K = 3$\n  - $\\beta = 1.0$\n  - $\\{N_i\\} = [1000, 800, 1200]$\n  - $\\{f_i\\} = [-2.0, -1.5, -1.8]$\n  - $\\{V_i(x_j)\\} = [0.5, 1.0, 0.2]$\n\n- Case B (overflow-prone large positive exponents when done naively): \n  - $K = 3$\n  - $\\beta = 400.0$\n  - $\\{N_i\\} = [50, 50, 50]$\n  - $\\{f_i\\} = [0.0, 0.1, -0.2]$\n  - $\\{V_i(x_j)\\} = [-1.8, -1.9, -2.1]$\n\n- Case C (underflow-prone large negative exponents when done naively): \n  - $K = 3$\n  - $\\beta = 400.0$\n  - $\\{N_i\\} = [100, 100, 100]$\n  - $\\{f_i\\} = [0.0, 0.0, 0.0]$\n  - $\\{V_i(x_j)\\} = [2.5, 2.4, 2.6]$\n\n- Case D (windows with zero counts that must be ignored safely): \n  - $K = 4$\n  - $\\beta = 2.0$\n  - $\\{N_i\\} = [0, 1000, 0, 500]$\n  - $\\{f_i\\} = [-1.0, -1.0, -1.0, -1.0]$\n  - $\\{V_i(x_j)\\} = [0.0, 0.1, -0.2, 0.5]$\n\n- Case E (large dynamic range across counts and offsets): \n  - $K = 3$\n  - $\\beta = 1.5$\n  - $\\{N_i\\} = [1, 10^6, 10^{12}]$\n  - $\\{f_i\\} = [10.0, -20.0, -30.0]$\n  - $\\{V_i(x_j)\\} = [0.0, -10.0, -25.0]$\n\nOutput format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order A, B, C, D, E, for example $[r_A,r_B,r_C,r_D,r_E]$ with each $r_\\cdot$ rounded to $6$ decimal places.\n- The outputs are the $5$ floating-point values $\\log D_j$ for the respective cases.",
            "solution": "The problem requires the numerically stable computation of the natural logarithm of a weighted sum of exponentials, a common task in statistical mechanics, particularly within the framework of the Weighted Histogram Analysis Method (WHAM). The quantity to be computed is $\\log D_j$, where $D_j$ is the denominator of the WHAM equations for a discrete bin $j$, defined as:\n$$\nD_j \\equiv \\sum_{i=1}^{K} N_i \\exp\\!\\left(\\beta f_i - \\beta V_i(x_j)\\right)\n$$\nHere, $N_i$ represents the number of samples from simulation window $i$, $f_i$ is the free energy of that window, $V_i(x_j)$ is the bias potential applied in window $i$ evaluated at the configuration corresponding to bin $j$, $\\beta$ is the inverse thermal energy, and $K$ is the total number of simulation windows.\n\nA direct, or naive, computation of $D_j$ involves evaluating each term $N_i \\exp(\\beta (f_i - V_i(x_j)))$ and then summing them up. This approach is prone to severe numerical errors. If the argument of the exponential, $\\beta (f_i - V_i(x_j))$, is a large positive number, the exponential function will overflow, resulting in `infinity`. Conversely, if the argument is a large negative number, the exponential will underflow to $0$. If all terms underflow, the sum is incorrectly computed as $0$, and its logarithm is $-\\infty$. The test cases provided are designed to expose these vulnerabilities.\n\nTo circumvent these issues, we must work in the logarithmic domain and employ a standard numerical stabilization technique known as the \"log-sum-exp\" trick. The goal is to compute $\\log(\\sum_{i} \\exp(y_i))$ without computing $\\exp(y_i)$ for large $y_i$.\n\nFirst, we transform the expression for $D_j$ into the required log-sum-exp form. For each term $i$ where the sample count $N_i > 0$, we can write $N_i = \\exp(\\log N_i)$. The case $N_i=0$ is trivial, as the term contributes exactly $0$ to the sum and should be excluded from the calculation. Let $I = \\{i \\mid N_i > 0\\}$ be the set of indices for windows with non-zero sample counts. The sum $D_j$ can be written as:\n$$\nD_j = \\sum_{i \\in I} \\exp(\\log N_i) \\exp\\left(\\beta (f_i - V_i(x_j))\\right) = \\sum_{i \\in I} \\exp\\left(\\log N_i + \\beta f_i - \\beta V_i(x_j)\\right)\n$$\nLet us define the argument of each exponential term as:\n$$\ny_i = \\log N_i + \\beta(f_i - V_i(x_j))\n$$\nWe are now tasked with computing $\\log D_j = \\log\\left(\\sum_{i \\in I} \\exp(y_i)\\right)$.\n\nThe log-sum-exp trick relies on factoring out the largest term from the sum. Let $y_{\\max} = \\max_{i \\in I} \\{y_i\\}$. We can then write:\n\\begin{align*}\n\\log D_j &= \\log\\left(\\sum_{i \\in I} \\exp(y_i)\\right) \\\\\n&= \\log\\left(\\exp(y_{\\max}) \\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right) \\\\\n&= \\log(\\exp(y_{\\max})) + \\log\\left(\\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right) \\\\\n&= y_{\\max} + \\log\\left(\\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right)\n\\end{align*}\nThis formulation is numerically stable. The arguments of the exponentials inside the sum, $y_i - y_{\\max}$, are now all less than or equal to $0$. The largest argument is $0$, corresponding to the term where $y_i=y_{\\max}$, and $\\exp(0)=1$. All other arguments are negative. This transformation prevents overflow since no argument to $\\exp$ is a large positive number. It also mitigates the damaging effects of underflow; if a term $y_i$ is significantly smaller than $y_{\\max}$, the quantity $y_i - y_{\\max}$ will be a large negative number, and $\\exp(y_i - y_{\\max})$ will correctly evaluate to a value near $0$, contributing negligibly to the sum, which is its mathematically correct behavior to machine precision.\n\nThe algorithm to compute $\\log D_j$ is as follows:\n1.  For the given set of inputs $\\{N_i\\}$, $\\{f_i\\}$, $\\{V_i(x_j)\\}$, and $\\beta$, identify the set of indices $I = \\{i \\mid N_i > 0\\}$. If this set is empty, the sum $D_j$ is $0$ and $\\log D_j$ is $-\\infty$.\n2.  For each index $i \\in I$, compute the corresponding log-domain term $y_i = \\log N_i + \\beta(f_i - V_i(x_j))$.\n3.  Find the maximum value over all computed terms, $y_{\\max} = \\max_{i \\in I} \\{y_i\\}$.\n4.  Compute the final result using the stable formula: $\\log D_j = y_{\\max} + \\log\\left(\\sum_{i \\in I} \\exp(y_i - y_{\\max})\\right)$.\n\nThis procedure guarantees a robust and accurate calculation for all provided test cases, correctly handling large positive exponents (potential overflow), large negative exponents (potential underflow), and terms that must be excluded due to zero counts.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the WHAM log-denominator problem for a suite of test cases.\n    It computes the natural logarithm of the denominator term D_j from the\n    Weighted Histogram Analysis Method (WHAM) in a numerically stable way.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: ( {N_i}, {f_i}, {V_i(x_j)}, beta )\n    test_cases = [\n        # Case A: Happy path, moderate magnitudes\n        ([1000, 800, 1200], [-2.0, -1.5, -1.8], [0.5, 1.0, 0.2], 1.0),\n        \n        # Case B: Overflow-prone large positive exponents\n        ([50, 50, 50], [0.0, 0.1, -0.2], [-1.8, -1.9, -2.1], 400.0),\n        \n        # Case C: Underflow-prone large negative exponents\n        ([100, 100, 100], [0.0, 0.0, 0.0], [2.5, 2.4, 2.6], 400.0),\n        \n        # Case D: Windows with zero counts\n        ([0, 1000, 0, 500], [-1.0, -1.0, -1.0, -1.0], [0.0, 0.1, -0.2, 0.5], 2.0),\n        \n        # Case E: Large dynamic range across inputs\n        ([1., 1e6, 1e12], [10.0, -20.0, -30.0], [0.0, -10.0, -25.0], 1.5)\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack case parameters and convert to NumPy arrays for vectorized operations.\n        # Using float64 for better precision is a good practice.\n        N, f, V, beta = (\n            np.array(case[0], dtype=np.float64),\n            np.array(case[1], dtype=np.float64),\n            np.array(case[2], dtype=np.float64),\n            np.float64(case[3])\n        )\n        \n        # Main logic to calculate the result for one case goes here.\n        \n        # Step 1: Filter out terms where N_i is 0, as they contribute 0 to the sum.\n        # In the log domain, log(0) is undefined, so these terms must be handled separately.\n        non_zero_mask = N > 0\n        \n        if not np.any(non_zero_mask):\n            # Edge case: if all N_i are 0, the sum is 0, and log(0) is -infinity.\n            result = -np.inf\n        else:\n            # Apply the mask to filter the arrays.\n            N_filt = N[non_zero_mask]\n            f_filt = f[non_zero_mask]\n            V_filt = V[non_zero_mask]\n            \n            # Step 2: Calculate the log-domain arguments for the sum of exponentials.\n            # y_i = log(N_i) + beta * (f_i - V_i(x_j))\n            y = np.log(N_filt) + beta * (f_filt - V_filt)\n            \n            # Step 3: Apply the log-sum-exp trick for numerical stability.\n            # log(sum(exp(y_i))) = y_max + log(sum(exp(y_i - y_max)))\n            y_max = np.max(y)\n            result = y_max + np.log(np.sum(np.exp(y - y_max)))\n\n        # Append the result, formatted to 6 decimal places as required.\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond a correct implementation, applying WHAM effectively requires making informed choices about its parameters, such as the histogram bin width $\\Delta x$. This parameter controls a fundamental trade-off: smaller bins reduce systematic binning bias but increase statistical variance, while larger bins do the opposite. This advanced practice guides you through a complete numerical experiment to quantify this bias-variance trade-off, generate synthetic data from a known ground truth, and determine the optimal bin width that minimizes the total error for a given amount of sampling. ",
            "id": "2465743",
            "problem": "You will implement a numerical study of the Weighted Histogram Analysis Method (WHAM) to quantify, from first principles, the trade-off between systematic bias and statistical variance in the reconstructed potential of mean force (PMF) as a function of the histogram bin width $\\Delta x$. Your implementation must adhere to the following mathematical model and tasks.\n\nContext and core definitions:\n- In one dimension, the unbiased equilibrium probability density along a reaction coordinate $x$ is $p(x) \\propto \\exp(-\\beta U(x))$, where $U(x)$ is the potential energy and $\\beta = 1/(k_\\mathrm{B} T)$. In this problem, work in reduced units where $\\beta = 1$ so that energies are in units of $k_\\mathrm{B}T$.\n- The potential of mean force (PMF) is defined (up to an additive constant) as $F(x) = -\\ln p(x)$. With $\\beta = 1$ and $p(x) \\propto \\exp(-U(x))$, the true PMF equals $U(x)$ plus a constant.\n- Umbrella sampling measures data under $K$ biased potentials $U_k^\\text{tot}(x) = U(x) + w_k(x)$, where $w_k(x)$ is a known bias potential, here harmonic: $w_k(x) = \\frac{1}{2}\\kappa(x - x_k)^2$ with spring constant $\\kappa > 0$ and window center $x_k$.\n- A histogram estimator partitions the domain into bins of width $\\Delta x$ and uses bin counts. The Weighted Histogram Analysis Method (WHAM) combines histograms from multiple biased windows to estimate the unbiased density, and thus the PMF, by solving a self-consistent system for the density and the window free-energy offsets.\n\nYour tasks:\n1) Synthetic data generation from first principles. Let the true potential be the symmetric double-well\n$$\nU(x) = \\alpha \\bigl(x^2 - c^2\\bigr)^2,\n$$\nwith $\\alpha > 0$ and $c > 0$, and consider the domain $x \\in [-L, L]$. For each umbrella window $k \\in \\{1,\\dots,K\\}$ with harmonic bias $w_k(x) = \\frac{1}{2}\\kappa(x-x_k)^2$ and a specified total sample size $N_k$, generate synthetic histogram counts by:\n- Computing the exact biased bin probabilities from the continuous densities $p_k(x) \\propto \\exp\\bigl(-U(x) - w_k(x)\\bigr)$ via numerical quadrature on a sufficiently fine grid over $[-L,L]$.\n- Drawing counts per bin for window $k$ from a multinomial distribution with total $N_k$ and the computed bin probabilities. This ensures scientific realism by directly reflecting the Boltzmann distribution under the bias, without relying on shortcuts or closed-form sampling.\n\n2) WHAM reconstruction. For a given $\\Delta x$, implement WHAM to estimate the unbiased density at the bin centers. Your implementation must use a fixed-point iteration that enforces proper normalization of the density and determines the free-energy offsets for each window. The PMF estimate is $\\widehat{F}(x_b) = -\\ln \\widehat{p}(x_b)$ up to an additive constant; for numerical comparison, choose the constant such that $\\min_b \\widehat{F}(x_b) = 0$, and likewise set the true PMF $F_\\text{true}(x)=U(x)$ to have $\\min_b F_\\text{true}(x_b) = 0$ when evaluated on the same bin centers.\n\n3) Bias–variance analysis as a function of $\\Delta x$. For each candidate $\\Delta x$, repeat the synthetic dataset generation and WHAM reconstruction for $R$ independent replicates to empirically estimate:\n- The pointwise mean PMF $\\overline{F}_{\\Delta x}(x_b)$ and variance $\\operatorname{Var}_{\\Delta x}[F(x_b)]$ across replicates.\n- The integrated squared bias\n$$\nB^2(\\Delta x) = \\sum_b \\bigl(\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\bigr)^2 \\Delta x,\n$$\nand the integrated variance\n$$\nV(\\Delta x) = \\sum_b \\operatorname{Var}_{\\Delta x}[F(x_b)] \\Delta x.\n$$\n- The mean integrated squared error (MISE) $\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)$, which reflects the fundamental trade-off: increasing $\\Delta x$ increases bin-averaging bias while reducing statistical variance; decreasing $\\Delta x$ reduces bias while increasing variance due to fewer counts per bin.\n\nTest suite:\nAdopt the following fixed physical and numerical parameters in reduced units:\n- Double-well potential: $\\alpha = 2$, $c = 1$, domain $[-L,L] = [-2,2]$.\n- Number of umbrellas: $K = 7$ with centers $x_k \\in \\{-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5\\}$ and spring constant $\\kappa = 20$.\n- Number of independent replicates for bias–variance estimation: $R = 40$.\n- For all cases, assume equal counts per window $N_k = N$.\n\nDefine three test cases that explore variance-dominated, balanced, and bias-dominated regimes by varying $N$ and candidate bin widths:\n- Case 1 (moderate sampling): $N = 2000$ and candidate $\\Delta x \\in \\{0.02, 0.05, 0.10, 0.20\\}$.\n- Case 2 (low sampling): $N = 500$ and candidate $\\Delta x \\in \\{0.02, 0.05, 0.10, 0.20\\}$.\n- Case 3 (high sampling): $N = 10000$ and candidate $\\Delta x \\in \\{0.02, 0.05, 0.10, 0.20\\}$.\n\nWhat your program must do:\n- For each test case, and for each candidate $\\Delta x$, perform the replicate-based bias–variance analysis described above and compute $\\mathrm{MISE}(\\Delta x)$.\n- For each test case, select the $\\Delta x$ that minimizes $\\mathrm{MISE}(\\Delta x)$. If there is a tie, choose the smallest $\\Delta x$ among the minimizers.\n\nFinal output format:\n- Your program should produce a single line of output containing the selected optimal bin widths for the three test cases as a comma-separated list enclosed in square brackets, for example $\\texttt{[0.05,0.10,0.05]}$. No additional text should be printed.\n\nImportant notes:\n- All energies are in units of $k_\\mathrm{B}T$ and $x$ is dimensionless.\n- Angles are not involved.\n- Randomness must be handled internally and reproducibly; fix a random seed.\n- The implementation must be self-contained and must not read or write any files or require user input.",
            "solution": "The problem requires a numerical investigation of the bias-variance trade-off for the Weighted Histogram Analysis Method (WHAM) as a function of histogram bin width, $\\Delta x$. The problem is scientifically valid, well-posed, and all necessary parameters are provided. It represents a standard task in computational chemistry and statistical mechanics, based on established principles. I will proceed with a full solution.\n\nThe solution is implemented in three main stages: synthetic data generation, PMF reconstruction using WHAM, and a bias-variance analysis over multiple replicates.\n\n### 1. Synthetic Data Generation\n\nTo ensure a scientifically realistic analysis, synthetic data must be generated directly from the underlying Boltzmann distribution. The true, unbiased potential is a symmetric double-well potential given by $U(x) = \\alpha(x^2 - c^2)^2$, with $\\alpha=2$ and $c=1$. The domain is $x \\in [-2, 2]$. All calculations are performed in reduced units where $\\beta = (k_B T)^{-1} = 1$.\n\nIn umbrella sampling, the system is simulated under $K$ different biasing potentials, $w_k(x)$, to enhance sampling of high-energy regions. Here, harmonic biases $w_k(x) = \\frac{1}{2}\\kappa(x - x_k)^2$ are used, with $\\kappa=20$ and $K=7$ windows centered at $x_k \\in \\{-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5\\}$.\n\nFor each window $k$, the total potential is $U_k^\\text{tot}(x) = U(x) + w_k(x)$, and the corresponding equilibrium probability density is $p_k(x) \\propto \\exp(-U_k^\\text{tot}(x))$. To generate histogram counts for a given bin width $\\Delta x$, we first partition the domain $[-L, L]$ into discrete bins. The exact probability of observing a sample in bin $b$ (from $x_b^{\\text{start}}$ to $x_b^{\\text{end}}$) for window $k$ is given by the integral of the normalized density:\n$$\nP_{k,b} = \\frac{\\int_{x_b^{\\text{start}}}^{x_b^{\\text{end}}} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}{\\int_{-L}^{L} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}\n$$\nThese integrals are computed via numerical quadrature over a fine grid. A high-resolution grid (with spacing significantly smaller than any $\\Delta x$) is used to approximate the integrals by a discrete sum (trapezoidal rule).\n\nWith the vector of bin probabilities $\\{P_{k,b}\\}$ for each window $k$, synthetic histogram counts $\\{N_{k,b}\\}$ are drawn from a multinomial distribution with $N_k$ total trials (samples). This process accurately models the statistical nature of sampling in a molecular simulation.\n\n### 2. WHAM Reconstruction\n\nWHAM provides a way to combine data from multiple biased simulations to compute the optimal estimate of the unbiased probability distribution, $p(x)$, and thus the potential of mean force (PMF), $F(x) = -\\ln p(x)$. The method solves a set of self-consistent equations for the unbiased probabilities $p_b$ at each bin center $x_b$ and the dimensionless free energies $f_k$ of each simulation window. With $\\beta=1$, the WHAM equations are:\n$$\np_b = \\frac{\\sum_{k=1}^K N_{k,b}}{\\sum_{k=1}^K N_k \\exp(f_k - w_k(x_b))}\n$$\n$$\n\\exp(-f_k) = \\sum_b p_b \\exp(-w_k(x_b))\n$$\nHere, $p_b$ are unnormalized probabilities proportional to the true density. These equations are solved using a fixed-point iteration:\n1. Initialize all free energies $f_k = 0$.\n2. Repeatedly compute the probabilities $p_b$ using the current $f_k$.\n3. Use the new $p_b$ to compute updated free energies $f_k^{\\text{new}}$.\n4. To prevent drift, a constraint is applied, such as fixing one free energy (e.g., $f_1=0$).\n5. The iteration continues until the free energies converge to a specified tolerance.\n\nAfter convergence, the final probabilities $p_b$ are used to compute the PMF estimate for the given dataset: $\\widehat{F}(x_b) = -\\ln p_b$. For comparison, the estimated PMF is shifted by an additive constant such that its minimum value is zero. The true PMF, $F_{\\text{true}}(x_b) = U(x_b)$, is also normalized to have a minimum of zero over the same set of bin centers.\n\nA critical issue arises when a bin $b$ has zero counts across all windows, i.e., $\\sum_k N_{k,b} = 0$. In this case, $p_b=0$ and the estimated PMF, $\\widehat{F}(x_b)$, is infinite. This represents a catastrophic failure of the estimator for that bin.\n\n### 3. Bias-Variance Analysis\n\nThe core of the problem is to quantify the trade-off between systematic error (bias) and statistical error (variance) as a function of bin width $\\Delta x$. For each candidate $\\Delta x$, we perform $R=40$ independent replicates of the data generation and WHAM reconstruction process.\n\nAcross the $R$ replicates, we compute:\n- The mean estimated PMF: $\\overline{F}_{\\Delta x}(x_b) = \\frac{1}{R} \\sum_{r=1}^R \\widehat{F}_r(x_b)$.\n- The sample variance of the PMF: $\\operatorname{Var}_{\\Delta x}[F(x_b)] = \\frac{1}{R-1} \\sum_{r=1}^R (\\widehat{F}_r(x_b) - \\overline{F}_{\\Delta x}(x_b))^2$.\n\nThese are then integrated over the domain to yield the integrated squared bias $B^2(\\Delta x)$ and integrated variance $V(\\Delta x)$:\n$$\nB^2(\\Delta x) = \\sum_b \\left(\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\right)^2 \\Delta x\n$$\n$$\nV(\\Delta x) = \\sum_b \\operatorname{Var}_{\\Delta x}[F(x_b)] \\Delta x\n$$\nThe Mean Integrated Squared Error (MISE) is the sum: $\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)$.\n\nThe choice of $\\Delta x$ governs the trade-off:\n- **Small $\\Delta x$**: Low bias, as bin-averaging error is minimal. However, with fewer samples per bin, the statistical variance is high. This increases the probability of empty bins, which results in an infinite PMF estimate. If any replicate yields an infinite PMF for any bin, the mean PMF for that bin will also be infinite, leading to an infinite MISE.\n- **Large $\\Delta x$**: Low variance, as more samples are pooled into each bin, reducing the chance of empty bins. However, bias increases due to averaging the potential over a wider region.\n\nOur implementation handles the infinite PMF values by assigning an infinite MISE to any $\\Delta x$ that results in one or more empty bins in any of the $R$ replicates. The optimal $\\Delta x$ is then the one that minimizes the MISE among the choices that produce a finite MISE. This correctly penalizes bin widths that are too small for the given sample size. The final algorithm iterates through the candidate bin widths for each test case, calculates the MISE, and selects the optimal $\\Delta x$ according to the specified criteria.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the WHAM bias-variance analysis for all test cases.\n    \"\"\"\n    \n    # --- Fixed Physical and Numerical Parameters ---\n    ALPHA = 2.0\n    C = 1.0\n    L = 2.0\n    K_UMBRELLA = 7\n    X_CENTERS = np.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n    KAPPA = 20.0\n    R_REPLICATES = 40\n    \n    # --- WHAM Solver Parameters ---\n    WHAM_MAX_ITER = 10000\n    WHAM_TOL = 1e-8\n    \n    # --- Data Generation Parameters ---\n    FINE_GRID_FACTOR = 100\n    RANDOM_SEED = 1234\n    \n    RNG = np.random.default_rng(RANDOM_SEED)\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case 1 (moderate sampling)\n        (2000, [0.02, 0.05, 0.10, 0.20]),\n        # Case 2 (low sampling)\n        (500, [0.02, 0.05, 0.10, 0.20]),\n        # Case 3 (high sampling)\n        (10000, [0.02, 0.05, 0.10, 0.20]),\n    ]\n    \n    # --- Helper Functions ---\n\n    def true_potential(x, alpha, c):\n        return alpha * (x**2 - c**2)**2\n\n    def bias_potential(x, x_k, kappa):\n        return 0.5 * kappa * (x - x_k)**2\n\n    def generate_synthetic_data(n_samples, delta_x):\n        \"\"\"\n        Generates one set of synthetic histogram data for all K windows.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        n_bins = len(bins) - 1\n        \n        fine_x = np.linspace(-L, L, n_bins * FINE_GRID_FACTOR)\n        fine_dx = fine_x[1] - fine_x[0]\n\n        U_fine = true_potential(fine_x, ALPHA, C)\n        \n        hist_counts = np.zeros((K_UMBRELLA, n_bins), dtype=np.int32)\n        \n        for k in range(K_UMBRELLA):\n            w_k_fine = bias_potential(fine_x, X_CENTERS[k], KAPPA)\n            U_tot_k_fine = U_fine + w_k_fine\n            \n            # Numerically integrate to get bin probabilities\n            unnorm_p_density = np.exp(-U_tot_k_fine)\n            Z_k = np.sum(unnorm_p_density) * fine_dx\n            \n            p_k_bins = np.zeros(n_bins)\n            for b in range(n_bins):\n                bin_mask = (fine_x >= bins[b]) & (fine_x < bins[b+1])\n                p_k_bins[b] = np.sum(unnorm_p_density[bin_mask]) * fine_dx / Z_k\n            \n            # Ensure probabilities sum to 1 due to potential floating point errors\n            p_k_bins /= np.sum(p_k_bins)\n            \n            # Generate counts from multinomial distribution\n            hist_counts[k, :] = RNG.multinomial(n_samples, p_k_bins)\n            \n        return hist_counts\n\n    def run_wham(hist_counts, delta_x, n_samples_vec):\n        \"\"\"\n        Implements the WHAM fixed-point iteration to find the PMF.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n\n        # Pre-compute biases at bin centers\n        w_kb = np.zeros((K_UMBRELLA, n_bins))\n        for k in range(K_UMBRELLA):\n            w_kb[k, :] = bias_potential(bin_centers, X_CENTERS[k], KAPPA)\n\n        f_k = np.zeros(K_UMBRELLA)\n        \n        for _ in range(WHAM_MAX_ITER):\n            f_k_old = f_k.copy()\n            \n            # Equation for probabilities p_b\n            numer = np.sum(hist_counts, axis=0) # N_b\n            log_denom_terms = f_k[:, np.newaxis] - w_kb\n            max_log = np.max(log_denom_terms, axis=0)\n            denom_arg = n_samples_vec[:, np.newaxis] * np.exp(log_denom_terms - max_log)\n            denom = np.exp(max_log) * np.sum(denom_arg, axis=0)\n\n            # p_b are unnormalized probabilities\n            p_b = np.zeros_like(numer, dtype=float)\n            non_zero_denom = denom > 1e-15 # Avoid division by zero\n            p_b[non_zero_denom] = numer[non_zero_denom] / denom[non_zero_denom]\n\n            # Equation for free energies f_k\n            exp_neg_w_kb = np.exp(-w_kb)\n            f_k_new_arg = np.sum(p_b[np.newaxis, :] * exp_neg_w_kb, axis=1)\n\n            # Avoid log(0) for windows with no overlap with data\n            f_k = -np.log(f_k_new_arg, where=f_k_new_arg > 0, out=np.full_like(f_k_new_arg, np.inf))\n            \n            # Shift to set f_1 = 0\n            f_k -= f_k[0]\n            \n            if np.linalg.norm(f_k - f_k_old) < WHAM_TOL:\n                break\n        \n        # Final PMF calculation\n        pmf = -np.log(p_b, where=p_b > 0, out=np.full_like(p_b, np.inf))\n        \n        if not np.all(np.isfinite(pmf)):\n             return pmf # Return with inf values\n        \n        pmf -= np.min(pmf)\n        return pmf\n\n    def analyze_bias_variance(n_samples, delta_x):\n        \"\"\"\n        Performs the full bias-variance analysis for a given N and delta_x.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n        \n        # True PMF on bin centers, normalized\n        F_true = true_potential(bin_centers, ALPHA, C)\n        F_true -= np.min(F_true)\n        \n        all_pmfs = np.zeros((R_REPLICATES, n_bins))\n        n_samples_vec = np.full(K_UMBRELLA, n_samples)\n        \n        for r in range(R_REPLICATES):\n            hist_counts = generate_synthetic_data(n_samples, delta_x)\n            pmf_estimate = run_wham(hist_counts, delta_x, n_samples_vec)\n            all_pmfs[r, :] = pmf_estimate\n\n        # If any replicate resulted in an empty bin, MISE is infinite\n        if np.isinf(all_pmfs).any():\n            return np.inf\n            \n        # Calculate mean and variance of PMF estimates\n        mean_pmf = np.mean(all_pmfs, axis=0)\n        var_pmf = np.var(all_pmfs, axis=0, ddof=1) # Sample variance\n        \n        # Integrated squared bias\n        bias_sq = np.sum((mean_pmf - F_true)**2) * delta_x\n        \n        # Integrated variance\n        variance = np.sum(var_pmf) * delta_x\n        \n        return bias_sq + variance\n\n    # --- Main Loop ---\n    \n    optimal_dx_results = []\n    \n    for n_samples, dx_candidates in test_cases:\n        mises = []\n        for dx in dx_candidates:\n            mise = analyze_bias_variance(n_samples, dx)\n            mises.append(mise)\n        \n        mises_array = np.array(mises)\n        \n        # Find the minimum finite MISE\n        min_mise = np.min(mises_array[np.isfinite(mises_array)]) if np.any(np.isfinite(mises_array)) else np.inf\n\n        if not np.isfinite(min_mise):\n             # This case should not happen with the given parameters\n             # If all MISE are inf, pick the largest dx as it's most robust\n            optimal_dx = dx_candidates[-1]\n        else:\n            # Find all indices matching the minimum MISE\n            best_indices = np.where(mises_array == min_mise)[0]\n            # Tie-breaking rule: choose smallest dx\n            optimal_dx = dx_candidates[best_indices[0]]\n\n        optimal_dx_results.append(optimal_dx)\n\n    # Format the final output\n    print(f\"[{','.join(f'{x:.2f}' for x in optimal_dx_results)}]\")\n\n\nsolve()\n```"
        }
    ]
}