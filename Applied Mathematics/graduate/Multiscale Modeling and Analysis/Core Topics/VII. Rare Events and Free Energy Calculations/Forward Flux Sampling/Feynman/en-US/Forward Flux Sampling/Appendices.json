{
    "hands_on_practices": [
        {
            "introduction": "The Forward Flux Sampling (FFS) method begins with quantifying the rate at which a system escapes an initial stable state. This rate, known as the initial flux $\\Phi_{A}$, measures the frequency of trajectories crossing the first interface $\\lambda_0$ that separates the initial basin $A$ from the rest of the state space. This exercise  provides a concrete, hands-on calculation of this flux within the well-defined framework of a discrete-time Markov chain, solidifying the fundamental connection between the stationary state probabilities and the transition rates that define the flux.",
            "id": "3761068",
            "problem": "Consider a discrete-time Markov chain used to coarse-grain a rare-event process for Forward Flux Sampling (FFS). The chain has the state space $\\{a_{1}, a_{2}, 0, i_{1}, i_{2}\\}$, where the set $A = \\{a_{1}, a_{2}\\}$ denotes a metastable basin and the set $\\lambda_{0} = \\{0\\}$ denotes the boundary interface associated with the order parameter level $\\lambda = 0$. The one-step transition probabilities are given by the matrix\n$$\nP \\;=\\;\n\\begin{pmatrix}\n0.2 & 0.3 & 0.25 & 0.15 & 0.1 \\\\\n0.3 & 0.2 & 0.2 & 0.1 & 0.2 \\\\\n0.25 & 0.2 & 0.2 & 0.2 & 0.15 \\\\\n0.15 & 0.1 & 0.2 & 0.2 & 0.35 \\\\\n0.1 & 0.2 & 0.15 & 0.35 & 0.2\n\\end{pmatrix},\n$$\nwritten in the order $(a_{1}, a_{2}, 0, i_{1}, i_{2})$ for both rows and columns. Assume the chain is irreducible and aperiodic, and let $\\pi$ denote its stationary measure, that is, the row vector that satisfies $ \\pi P = \\pi $ and $\\sum_{s} \\pi_{s} = 1$.\n\nIn the FFS framework, the forward flux $\\Phi_{A}$ through the interface $\\lambda_{0}$ is defined as the steady-state expected number of boundary-crossing transitions from $A$ into $\\lambda_{0}$ per unit time step. Starting only from the definitions of a stationary measure for a discrete-time Markov chain and the operational meaning of flux as a steady-state expectation of crossing events, derive an expression for $\\Phi_{A}$ in terms of $\\pi$ and the transition probabilities in $P$, and then evaluate $\\Phi_{A}$ for the given $P$, $A$, and $\\lambda_{0}$.\n\nProvide your final answer as a single real number. No rounding is required.",
            "solution": "The problem is first subjected to validation.\n\n### Step 1: Extract Givens\n-   **State Space**: A discrete-time Markov chain with state space $S = \\{a_{1}, a_{2}, 0, i_{1}, i_{2}\\}$.\n-   **State Sets**:\n    -   Metastable basin: $A = \\{a_{1}, a_{2}\\}$.\n    -   Boundary interface: $\\lambda_{0} = \\{0\\}$.\n-   **Transition Matrix**: The one-step transition probability matrix, with rows and columns ordered as $(a_{1}, a_{2}, 0, i_{1}, i_{2})$, is:\n    $$\n    P \\;=\\;\n    \\begin{pmatrix}\n    0.2 & 0.3 & 0.25 & 0.15 & 0.1 \\\\\n    0.3 & 0.2 & 0.2 & 0.1 & 0.2 \\\\\n    0.25 & 0.2 & 0.2 & 0.2 & 0.15 \\\\\n    0.15 & 0.1 & 0.2 & 0.2 & 0.35 \\\\\n    0.1 & 0.2 & 0.15 & 0.35 & 0.2\n    \\end{pmatrix}\n    $$\n-   **Properties**: The chain is stated to be irreducible and aperiodic.\n-   **Stationary Measure**: $\\pi$ is the stationary measure, a row vector satisfying $\\pi P = \\pi$ and $\\sum_{s \\in S} \\pi_{s} = 1$.\n-   **Definition of Flux**: The forward flux $\\Phi_{A}$ through the interface $\\lambda_{0}$ is defined as \"the steady-state expected number of boundary-crossing transitions from $A$ into $\\lambda_{0}$ per unit time step\".\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n-   **Scientifically Grounded**: The problem is based on the standard mathematical framework of discrete-time Markov chains and their application in rare event simulations (specifically Forward Flux Sampling), which is a well-established topic in computational physics and chemistry. All definitions are standard.\n-   **Well-Posed**: The problem is well-posed. It provides a complete definition of the system (state space, transition matrix) and asks for the calculation of a specific, well-defined quantity ($\\Phi_{A}$) based on given definitions. The existence and uniqueness of the stationary measure $\\pi$ are guaranteed by the stated properties of irreducibility and aperiodicity for a finite state space.\n-   **Objective**: The problem is expressed in precise, objective mathematical language.\n-   **Completeness**: All necessary information is provided. The transition matrix is a valid stochastic matrix, as each row sums to $1$.\n    -   Row 1: $0.2 + 0.3 + 0.25 + 0.15 + 0.1 = 1.0$\n    -   Row 2: $0.3 + 0.2 + 0.2 + 0.1 + 0.2 = 1.0$\n    -   Row 3: $0.25 + 0.2 + 0.2 + 0.2 + 0.15 = 1.0$\n    -   Row 4: $0.15 + 0.1 + 0.2 + 0.2 + 0.35 = 1.0$\n    -   Row 5: $0.1 + 0.2 + 0.15 + 0.35 + 0.2 = 1.0$\n\nThe problem is free of any scientific, logical, or structural flaws.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe forward flux $\\Phi_{A}$ is defined as the steady-state expected number of boundary-crossing transitions from the set of states $A$ to the set of states $\\lambda_{0}$ per unit time step. Let $X_t$ denote the state of the Markov chain at time $t$. A transition from $A$ to $\\lambda_{0}$ occurs if $X_t \\in A$ and $X_{t+1} \\in \\lambda_{0}$.\n\nIn steady state, the probability of the system being in a particular state $s$ is given by the stationary probability $\\pi_s = P(X_t = s)$. The probability of a transition from state $s$ to state $s'$ in one time step is given by the transition probability $P_{s,s'}$. The joint probability of being in state $s$ at time $t$ and state $s'$ at time $t+1$ is:\n$$\nP(X_t = s, X_{t+1} = s') = P(X_{t+1} = s' | X_t = s) P(X_t = s) = \\pi_s P_{s,s'}\n$$\nThe total probability of a crossing from $A$ to $\\lambda_0$ in one time step is the sum of probabilities of all such specific transitions. This probability is a direct measure of the expected number of such events per unit time, which is the definition of the flux $\\Phi_A$.\n$$\n\\Phi_{A} = P(X_t \\in A, X_{t+1} \\in \\lambda_{0}) = \\sum_{s \\in A} \\sum_{s' \\in \\lambda_{0}} P(X_t = s, X_{t+1} = s') = \\sum_{s \\in A} \\sum_{s' \\in \\lambda_{0}} \\pi_s P_{s,s'}\n$$\nIn this specific problem, the set $A = \\{a_{1}, a_{2}\\}$ and the interface $\\lambda_{0} = \\{0\\}$. Let us denote the states by indices $1, 2, 3, 4, 5$ corresponding to the given order $(a_{1}, a_{2}, 0, i_{1}, i_{2})$. Thus, $a_1$ is state $1$, $a_2$ is state $2$, and $0$ is state $3$. The expression for the flux becomes:\n$$\n\\Phi_{A} = \\sum_{s \\in \\{a_1, a_2\\}} \\sum_{s' \\in \\{0\\}} \\pi_s P_{s,s'} = \\pi_{a_1} P_{a_1, 0} + \\pi_{a_2} P_{a_2, 0}\n$$\nTo evaluate $\\Phi_A$, we first need to determine the stationary measure $\\pi = (\\pi_{a_1}, \\pi_{a_2}, \\pi_0, \\pi_{i_1}, \\pi_{i_2})$. The vector $\\pi$ is the solution to the system of equations $\\pi P = \\pi$ under the constraint $\\sum_s \\pi_s = 1$.\n\nWe observe that the given transition matrix $P$ is symmetric, i.e., $P_{s,s'} = P_{s',s}$ for all states $s, s'$. A stochastic matrix (rows sum to $1$) that is also symmetric is known as a doubly stochastic matrix (columns also sum to $1$). For a finite-state, irreducible Markov chain (which is given), if the transition matrix is doubly stochastic, the unique stationary distribution is the uniform distribution over the state space.\nThe state space $S$ has $5$ states. Therefore, the stationary probability for any state $s \\in S$ is:\n$$\n\\pi_s = \\frac{1}{5} = 0.2\n$$\nSo, we have $\\pi_{a_1} = 0.2$ and $\\pi_{a_2} = 0.2$.\n\nNext, we identify the required transition probabilities from the matrix $P$.\nThe probability of transitioning from $a_1$ to $0$ is the element in the first row and third column of $P$:\n$$\nP_{a_1, 0} = P_{1,3} = 0.25\n$$\nThe probability of transitioning from $a_2$ to $0$ is the element in the second row and third column of $P$:\n$$\nP_{a_2, 0} = P_{2,3} = 0.2\n$$\nNow, we can substitute these values into the expression for the flux $\\Phi_A$:\n$$\n\\Phi_{A} = \\pi_{a_1} P_{a_1, 0} + \\pi_{a_2} P_{a_2, 0}\n$$\n$$\n\\Phi_{A} = (0.2)(0.25) + (0.2)(0.2)\n$$\n$$\n\\Phi_{A} = 0.05 + 0.04\n$$\n$$\n\\Phi_{A} = 0.09\n$$\nThus, the forward flux from basin $A$ through the interface $\\lambda_{0}$ is $0.09$ events per unit time step.",
            "answer": "$$\\boxed{0.09}$$"
        },
        {
            "introduction": "Once the initial flux is determined, the core of FFS involves estimating the chain of conditional probabilities, $p_i = P(\\lambda_{i+1} | \\lambda_i)$, which represent the likelihood of advancing from one interface to the next. These probabilities are not calculated analytically but are estimated through repeated stochastic trials. This exercise  delves into the statistical foundation of this process, guiding you to derive the properties of the sample proportion as an unbiased estimator for $p_i$ and to compute its variance. Mastering this concept is essential for understanding the sources of statistical error in an FFS simulation.",
            "id": "2645588",
            "problem": "Consider a rare transition in a stochastic reaction network monitored by an order parameter $\\lambda$ with interfaces $\\lambda_0 \\equiv \\lambda_A < \\lambda_1 < \\cdots < \\lambda_n \\equiv \\lambda_B$, as used in Forward Flux Sampling (FFS). At interface $\\lambda_i$, define $p_i \\equiv P(\\lambda_{i+1} \\mid \\lambda_i)$ as the probability that a trajectory initiated from the ensemble of first-crossing points at $\\lambda_i$ reaches $\\lambda_{i+1}$ before returning to the basin $A$. You perform $M_i$ independent, identically prepared short trial trajectories from $\\lambda_i$, each either reaching $\\lambda_{i+1}$ (a “success”) or returning to $A$ (a “failure”). Using only the definitions of conditional probability, indicator random variables, and the properties of independent Bernoulli trials, derive an unbiased estimator $\\hat{p}_i$ for $p_i$ and compute the variance of this estimator under the sampling protocol.\n\nProvide, as your final answer, the closed-form expression for the variance $\\mathrm{Var}(\\hat{p}_i)$ in terms of $p_i$ and $M_i$. No numerical values are required, and no units apply. Your final answer must be a single analytical expression.",
            "solution": "The problem presented is a standard exercise in statistical estimation theory, applied to the context of Forward Flux Sampling (FFS), a method used in computational statistical physics. We will validate its premises and, if valid, proceed with a rigorous derivation.\n\nFirst, the givens are extracted and analyzed.\nThe problem defines:\n1. A stochastic process monitored by an order parameter $\\lambda$.\n2. A sequence of interfaces $\\lambda_0 < \\lambda_1 < \\dots < \\lambda_n$.\n3. The conditional probability $p_i \\equiv P(\\lambda_{i+1} \\mid \\lambda_i)$ as the probability for a trajectory starting at interface $\\lambda_i$ to reach $\\lambda_{i+1}$ before returning to basin $A$ (defined by $\\lambda < \\lambda_0$).\n4. A sampling procedure consisting of $M_i$ independent, identically prepared trials launched from interface $\\lambda_i$.\n5. Each trial is a binary outcome: success (reaching $\\lambda_{i+1}$) or failure (returning to $A$).\n\nThe problem is scientifically grounded, well-posed, and objective. It describes a Bernoulli sampling scheme, which is a fundamental concept in probability and statistics, and correctly situates it within the FFS algorithm. The language is precise, and the setup is self-contained. There are no contradictions, ambiguities, or factual inaccuracies. The problem is valid. We proceed to the solution.\n\nThe task is to derive an unbiased estimator for $p_i$ and compute its variance. Let us model the sampling process mathematically. Each of the $M_i$ trials is an independent random experiment. We can associate an indicator random variable, let us call it $X_j$, with the outcome of the $j$-th trial, for $j = 1, 2, \\dots, M_i$.\n\nBy definition, the indicator variable $X_j$ is defined as:\n$$\nX_j = \\begin{cases}\n    1 & \\text{if the } j\\text{-th trajectory reaches } \\lambda_{i+1} \\text{ (success)} \\\\\n    0 & \\text{if the } j\\text{-th trajectory returns to } A \\text{ (failure)}\n\\end{cases}\n$$\nThe probability of a success is given as $p_i$. Therefore, the probability mass function for $X_j$ is:\n$$\nP(X_j = 1) = p_i\n$$\n$$\nP(X_j = 0) = 1 - p_i\n$$\nThis is the definition of a Bernoulli distribution with parameter $p_i$. Since the trials are \"identically prepared,\" all $X_j$ follow the same distribution.\n\nThe natural and standard estimator for the probability parameter $p_i$ is the sample mean of the outcomes, which is the fraction of successful trials. We define the estimator $\\hat{p}_i$ as:\n$$\n\\hat{p}_i = \\frac{1}{M_i} \\sum_{j=1}^{M_i} X_j\n$$\nTo verify that this estimator is unbiased, we must show that its expected value, $E[\\hat{p}_i]$, is equal to the true parameter, $p_i$. We use the linearity of the expectation operator:\n$$\nE[\\hat{p}_i] = E\\left[ \\frac{1}{M_i} \\sum_{j=1}^{M_i} X_j \\right] = \\frac{1}{M_i} \\sum_{j=1}^{M_i} E[X_j]\n$$\nThe expectation of a single Bernoulli random variable $X_j$ is:\n$$\nE[X_j] = 1 \\cdot P(X_j = 1) + 0 \\cdot P(X_j = 0) = 1 \\cdot p_i + 0 \\cdot (1 - p_i) = p_i\n$$\nSubstituting this result into the expression for $E[\\hat{p}_i]$:\n$$\nE[\\hat{p}_i] = \\frac{1}{M_i} \\sum_{j=1}^{M_i} p_i = \\frac{1}{M_i} (M_i \\cdot p_i) = p_i\n$$\nThus, $E[\\hat{p}_i] = p_i$, and the estimator is proven to be unbiased.\n\nNext, we compute the variance of this estimator, $\\mathrm{Var}(\\hat{p}_i)$. We use the property of variance that for a constant $c$ and random variable $Y$, $\\mathrm{Var}(cY) = c^2 \\mathrm{Var}(Y)$.\n$$\n\\mathrm{Var}(\\hat{p}_i) = \\mathrm{Var}\\left(\\frac{1}{M_i} \\sum_{j=1}^{M_i} X_j\\right) = \\left(\\frac{1}{M_i}\\right)^2 \\mathrm{Var}\\left(\\sum_{j=1}^{M_i} X_j\\right)\n$$\nThe problem statement specifies that the $M_i$ trials are \"independent.\" For a sum of independent random variables, the variance of the sum is the sum of the variances. Therefore:\n$$\n\\mathrm{Var}\\left(\\sum_{j=1}^{M_i} X_j\\right) = \\sum_{j=1}^{M_i} \\mathrm{Var}(X_j)\n$$\nNow, we must find the variance of a single Bernoulli variable $X_j$. The variance is defined as $\\mathrm{Var}(X_j) = E[X_j^2] - (E[X_j])^2$. We already know $E[X_j] = p_i$. To find $E[X_j^2]$, we note that for an indicator variable, $X_j^2 = X_j$, because $1^2 = 1$ and $0^2 = 0$. Thus:\n$$\nE[X_j^2] = E[X_j] = p_i\n$$\nThe variance of $X_j$ is therefore:\n$$\n\\mathrm{Var}(X_j) = E[X_j^2] - (E[X_j])^2 = p_i - (p_i)^2 = p_i(1 - p_i)\n$$\nSince the trials are identically prepared, the variance is the same for all $j$. We can now substitute this back into our expression for $\\mathrm{Var}(\\hat{p}_i)$:\n$$\n\\mathrm{Var}(\\hat{p}_i) = \\frac{1}{M_i^2} \\sum_{j=1}^{M_i} p_i(1 - p_i)\n$$\nThe sum consists of $M_i$ identical terms:\n$$\n\\mathrm{Var}(\\hat{p}_i) = \\frac{1}{M_i^2} \\left[ M_i \\cdot p_i(1 - p_i) \\right]\n$$\nSimplifying the expression by canceling one factor of $M_i$ yields the final result for the variance of the estimator $\\hat{p}_i$:\n$$\n\\mathrm{Var}(\\hat{p}_i) = \\frac{p_i(1 - p_i)}{M_i}\n$$\nThis result is fundamental. It shows that the variance of the sample proportion is inversely proportional to the number of trials, a cornerstone of statistical inference.",
            "answer": "$$\\boxed{\\frac{p_i(1 - p_i)}{M_i}}$$"
        },
        {
            "introduction": "A key challenge in any large-scale simulation is achieving maximum accuracy for a given amount of computational effort. Having understood that each stage of FFS contributes to the overall statistical error, the next logical step is to ask how to best allocate computational resources among the different interfaces. This advanced exercise  addresses this exact question, tasking you with formulating and solving an optimization problem to minimize the total variance of the estimated rate constant. This practice demonstrates how a theoretical understanding of statistical variance can be translated into a powerful strategy for designing efficient and robust FFS simulations.",
            "id": "3761014",
            "problem": "Consider a rare-event transition from basin $A$ to basin $B$ simulated by Forward Flux Sampling (FFS), which employs a sequence of interfaces $\\lambda_{0} < \\lambda_{1} < \\cdots < \\lambda_{n}$ separating $A$ from $B$. The FFS estimator for the rate constant $k_{AB}$ is constructed as $\\hat{k}_{AB} = \\hat{\\Phi}_{A} \\prod_{i=0}^{n-1} \\hat{p}_{i}$, where $\\hat{\\Phi}_{A}$ is the estimated flux of trajectories leaving basin $A$ and hitting the first interface $\\lambda_{0}$, and $\\hat{p}_{i}$ is the estimated conditional probability that a trajectory launched from interface $\\lambda_{i}$ reaches the next interface $\\lambda_{i+1}$ before returning to $A$.\n\nYou are given that for each interface $i$, the success probability $p_{i}$ is strictly between $0$ and $1$, and that $\\hat{p}_{i}$ is obtained from $M_{i}$ independent Bernoulli trials with success probability $p_{i}$ by $\\hat{p}_{i} = S_{i}/M_{i}$, where $S_{i}$ is the number of successes. Assume the contributions of $\\hat{\\Phi}_{A}$ to the variance of $\\log \\hat{k}_{AB}$ are negligible compared to those from the $\\hat{p}_{i}$ factors and may be treated as constant for the purpose of optimization. Further assume that the $\\hat{p}_{i}$ are statistically independent across interfaces (by independent sampling and conditioning in FFS), and that the Central Limit Theorem (CLT) and the delta method apply for large $M_{i}$.\n\nEach trial at interface $i$ has a known average computational cost $c_{i} > 0$ (measured in a consistent computational work unit), so that allocating $M_{i}$ trials at interface $i$ incurs a total cost $c_{i} M_{i}$. You are given a fixed total computational budget $B > 0$ that must be fully allocated across all interfaces, satisfying $\\sum_{i=0}^{n-1} c_{i} M_{i} = B$.\n\nStarting from the fundamental definitions of binomial estimation and using the CLT with the delta method to obtain the leading-order variance of $\\log \\hat{k}_{AB}$ in the large-$M_{i}$ regime, do the following:\n\n- Derive the leading-order expression for $\\operatorname{Var}[\\log \\hat{k}_{AB}]$ in terms of $\\{p_{i}\\}$ and $\\{M_{i}\\}$, under the stated assumptions.\n- Formulate the continuous optimization problem that minimizes this leading-order variance subject to the fixed-budget constraint.\n- Solve this optimization problem to obtain the closed-form expression for the optimal allocation $\\{M_{i}^{\\ast}\\}$ as a function of $\\{p_{i}\\}$, $\\{c_{i}\\}$, and $B$.\n\nExpress your final answer as a single closed-form analytic expression for the vector of optimal allocations $\\{M_{i}^{\\ast}\\}_{i=0}^{n-1}$. No rounding is required.",
            "solution": "The user's request is to derive the optimal allocation of computational effort in a Forward Flux Sampling (FFS) simulation to minimize the variance of the estimated rate constant, subject to a fixed total budget. The problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. Therefore, the problem is deemed valid.\n\nThe solution process consists of three main parts:\n1.  Derive the leading-order expression for the variance of $\\log \\hat{k}_{AB}$.\n2.  Formulate the constrained optimization problem.\n3.  Solve the optimization problem to find the optimal allocation $\\{M_{i}^{\\ast}\\}$.\n\n### Part 1: Derivation of the Variance of $\\log \\hat{k}_{AB}$\n\nThe FFS estimator for the rate constant $k_{AB}$ is given by:\n$$\n\\hat{k}_{AB} = \\hat{\\Phi}_{A} \\prod_{i=0}^{n-1} \\hat{p}_{i}\n$$\nWe are interested in the variance of its logarithm, $\\log \\hat{k}_{AB}$. Taking the natural logarithm of the expression yields:\n$$\n\\log \\hat{k}_{AB} = \\log\\left(\\hat{\\Phi}_{A} \\prod_{i=0}^{n-1} \\hat{p}_{i}\\right) = \\log \\hat{\\Phi}_{A} + \\sum_{i=0}^{n-1} \\log \\hat{p}_{i}\n$$\nThe variance of this quantity is:\n$$\n\\operatorname{Var}[\\log \\hat{k}_{AB}] = \\operatorname{Var}\\left[\\log \\hat{\\Phi}_{A} + \\sum_{i=0}^{n-1} \\log \\hat{p}_{i}\\right]\n$$\nThe problem states that the contribution of $\\hat{\\Phi}_{A}$ to the variance is negligible, so we treat $\\log \\hat{\\Phi}_{A}$ as a constant. Furthermore, the estimators $\\hat{p}_{i}$ for different interfaces are statistically independent. Therefore, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}[\\log \\hat{k}_{AB}] = \\operatorname{Var}\\left[\\sum_{i=0}^{n-1} \\log \\hat{p}_{i}\\right] = \\sum_{i=0}^{n-1} \\operatorname{Var}[\\log \\hat{p}_{i}]\n$$\nTo find $\\operatorname{Var}[\\log \\hat{p}_{i}]$, we use the delta method. The estimator $\\hat{p}_{i} = S_{i}/M_{i}$, where $S_{i}$ is the number of successful trials out of $M_{i}$ independent Bernoulli trials, follows a scaled binomial distribution. The true probability of success is $p_i$. The number of successes $S_i$ follows a binomial distribution $S_i \\sim \\text{Binomial}(M_i, p_i)$.\n\nThe expected value and variance of the estimator $\\hat{p}_{i}$ are:\n$$\nE[\\hat{p}_{i}] = E\\left[\\frac{S_{i}}{M_{i}}\\right] = \\frac{1}{M_{i}} E[S_{i}] = \\frac{M_{i} p_{i}}{M_{i}} = p_{i}\n$$\n$$\n\\operatorname{Var}[\\hat{p}_{i}] = \\operatorname{Var}\\left[\\frac{S_{i}}{M_{i}}\\right] = \\frac{1}{M_{i}^{2}} \\operatorname{Var}[S_{i}] = \\frac{M_{i} p_{i} (1 - p_{i})}{M_{i}^{2}} = \\frac{p_{i}(1 - p_{i})}{M_{i}}\n$$\nThe delta method provides an approximation for the variance of a function of a random variable. For a function $g(X)$, the approximation is $\\operatorname{Var}[g(X)] \\approx (g'(E[X]))^{2} \\operatorname{Var}[X]$. Here, our random variable is $\\hat{p}_{i}$ and the function is $g(x) = \\log(x)$. The derivative is $g'(x) = 1/x$.\n\nApplying the delta method, with $E[\\hat{p}_{i}]=p_i$:\n$$\n\\operatorname{Var}[\\log \\hat{p}_{i}] \\approx (g'(p_{i}))^{2} \\operatorname{Var}[\\hat{p}_{i}] = \\left(\\frac{1}{p_{i}}\\right)^{2} \\left(\\frac{p_{i}(1 - p_{i})}{M_{i}}\\right) = \\frac{1 - p_{i}}{p_{i} M_{i}}\n$$\nThis is the leading-order term for the variance in the large-$M_i$ limit, as justified by the Central Limit Theorem.\n\nSubstituting this back into the expression for the total variance, we obtain the objective function to be minimized:\n$$\n\\operatorname{Var}[\\log \\hat{k}_{AB}] \\approx \\sum_{i=0}^{n-1} \\frac{1 - p_{i}}{p_{i} M_{i}}\n$$\n\n### Part 2: Formulation of the Optimization Problem\n\nWe aim to minimize the derived variance subject to a fixed total computational budget. Let $V(\\{M_i\\})$ denote the approximate variance. The problem is formulated as:\n\nMinimize:\n$$\nV(\\{M_i\\}) = \\sum_{i=0}^{n-1} \\frac{1 - p_{i}}{p_{i} M_{i}}\n$$\nSubject to the constraint:\n$$\n\\sum_{i=0}^{n-1} c_{i} M_{i} = B\n$$\nwhere $M_{i} > 0$ for all $i \\in \\{0, 1, \\dots, n-1\\}$.\n\n### Part 3: Solving the Optimization Problem\n\nThis is a constrained optimization problem that can be solved using the method of Lagrange multipliers. We define the Lagrangian $\\mathcal{L}$:\n$$\n\\mathcal{L}(\\{M_i\\}, \\lambda) = \\sum_{i=0}^{n-1} \\frac{1 - p_{i}}{p_{i} M_{i}} + \\lambda \\left( \\left( \\sum_{i=0}^{n-1} c_{i} M_{i} \\right) - B \\right)\n$$\nTo find the optimal values $\\{M_{i}^{\\ast}\\}$, we set the partial derivative of $\\mathcal{L}$ with respect to each $M_k$ (for $k \\in \\{0, \\dots, n-1\\}$) to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial M_k} = \\frac{\\partial}{\\partial M_k} \\left( \\frac{1 - p_k}{p_k M_k} \\right) + \\lambda \\frac{\\partial}{\\partial M_k} (c_k M_k) = 0\n$$\n$$\n-\\frac{1 - p_k}{p_k M_k^2} + \\lambda c_k = 0\n$$\nSolving for $M_k$:\n$$\n\\lambda c_k = \\frac{1 - p_k}{p_k M_k^2} \\implies M_k^2 = \\frac{1 - p_k}{\\lambda c_k p_k}\n$$\nSince $M_k$ must be positive, we take the positive root:\n$$\nM_k = \\sqrt{\\frac{1 - p_k}{\\lambda c_k p_k}} = \\frac{1}{\\sqrt{\\lambda}} \\sqrt{\\frac{1 - p_k}{c_k p_k}}\n$$\nNow, we use the budget constraint to determine the Lagrange multiplier $\\lambda$. We substitute the expression for $M_k$ into the constraint equation:\n$$\n\\sum_{k=0}^{n-1} c_k M_k = \\sum_{k=0}^{n-1} c_k \\left( \\frac{1}{\\sqrt{\\lambda}} \\sqrt{\\frac{1 - p_k}{c_k p_k}} \\right) = B\n$$\nFactoring out the term with $\\lambda$:\n$$\n\\frac{1}{\\sqrt{\\lambda}} \\sum_{k=0}^{n-1} c_k \\frac{\\sqrt{1 - p_k}}{\\sqrt{c_k p_k}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{k=0}^{n-1} \\sqrt{c_k} \\sqrt{\\frac{1-p_k}{p_k}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{k=0}^{n-1} \\sqrt{\\frac{c_k(1-p_k)}{p_k}} = B\n$$\nSolving for $1/\\sqrt{\\lambda}$:\n$$\n\\frac{1}{\\sqrt{\\lambda}} = \\frac{B}{\\sum_{j=0}^{n-1} \\sqrt{\\frac{c_j(1 - p_j)}{p_j}}}\n$$\nFinally, we substitute this expression for $1/\\sqrt{\\lambda}$ back into the equation for $M_k$ to find the optimal allocation $M_k^{\\ast}$:\n$$\nM_k^{\\ast} = \\left( \\frac{B}{\\sum_{j=0}^{n-1} \\sqrt{\\frac{c_j(1 - p_j)}{p_j}}} \\right) \\sqrt{\\frac{1 - p_k}{c_k p_k}}\n$$\nReplacing the index $k$ with $i$ as requested in the problem statement, we have the final expression for the vector of optimal allocations $\\{M_i^{\\ast}\\}_{i=0}^{n-1}$. Each component $M_i^{\\ast}$ is given by:\n$$\nM_{i}^{\\ast} = B \\frac{\\sqrt{\\frac{1-p_i}{c_i p_i}}}{\\sum_{j=0}^{n-1} \\sqrt{\\frac{c_j(1-p_j)}{p_j}}}\n$$",
            "answer": "$$\n\\boxed{M_{i}^{\\ast} = B \\frac{\\sqrt{\\frac{1-p_{i}}{c_{i}p_{i}}}}{\\sum_{j=0}^{n-1} \\sqrt{\\frac{c_{j}(1-p_{j})}{p_{j}}}}}\n$$"
        }
    ]
}