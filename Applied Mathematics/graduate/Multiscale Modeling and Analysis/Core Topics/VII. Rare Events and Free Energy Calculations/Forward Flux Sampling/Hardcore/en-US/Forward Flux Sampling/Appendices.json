{
    "hands_on_practices": [
        {
            "introduction": "At the heart of the Forward Flux Sampling algorithm is the estimation of conditional probabilities, $p_i$, for transitioning between successive interfaces. This exercise strips the process down to its statistical core, treating each FFS stage as a series of Bernoulli trials. By deriving the unbiased estimator for $p_i$ and its associated variance, you will build a foundational understanding of the statistical error inherent in FFS, which is essential for both interpreting results and optimizing simulations .",
            "id": "2645588",
            "problem": "Consider a rare transition in a stochastic reaction network monitored by an order parameter $\\lambda$ with interfaces $\\lambda_0 \\equiv \\lambda_A  \\lambda_1  \\cdots  \\lambda_n \\equiv \\lambda_B$, as used in Forward Flux Sampling (FFS). At interface $\\lambda_i$, define $p_i \\equiv P(\\lambda_{i+1} \\mid \\lambda_i)$ as the probability that a trajectory initiated from the ensemble of first-crossing points at $\\lambda_i$ reaches $\\lambda_{i+1}$ before returning to the basin $A$. You perform $M_i$ independent, identically prepared short trial trajectories from $\\lambda_i$, each either reaching $\\lambda_{i+1}$ (a “success”) or returning to $A$ (a “failure”). Using only the definitions of conditional probability, indicator random variables, and the properties of independent Bernoulli trials, derive an unbiased estimator $\\hat{p}_i$ for $p_i$ and compute the variance of this estimator under the sampling protocol.\n\nProvide, as your final answer, the closed-form expression for the variance $\\mathrm{Var}(\\hat{p}_i)$ in terms of $p_i$ and $M_i$. No numerical values are required, and no units apply. Your final answer must be a single analytical expression.",
            "solution": "The problem presented is a standard exercise in statistical estimation theory, applied to the context of Forward Flux Sampling (FFS), a method used in computational statistical physics. We will validate its premises and, if valid, proceed with a rigorous derivation.\n\nFirst, the givens are extracted and analyzed.\nThe problem defines:\n1. A stochastic process monitored by an order parameter $\\lambda$.\n2. A sequence of interfaces $\\lambda_0  \\lambda_1  \\dots  \\lambda_n$.\n3. The conditional probability $p_i \\equiv P(\\lambda_{i+1} \\mid \\lambda_i)$ as the probability for a trajectory starting at interface $\\lambda_i$ to reach $\\lambda_{i+1}$ before returning to basin $A$ (defined by $\\lambda  \\lambda_0$).\n4. A sampling procedure consisting of $M_i$ independent, identically prepared trials launched from interface $\\lambda_i$.\n5. Each trial is a binary outcome: success (reaching $\\lambda_{i+1}$) or failure (returning to $A$).\n\nThe problem is scientifically grounded, well-posed, and objective. It describes a Bernoulli sampling scheme, which is a fundamental concept in probability and statistics, and correctly situates it within the FFS algorithm. The language is precise, and the setup is self-contained. There are no contradictions, ambiguities, or factual inaccuracies. The problem is valid. We proceed to the solution.\n\nThe task is to derive an unbiased estimator for $p_i$ and compute its variance. Let us model the sampling process mathematically. Each of the $M_i$ trials is an independent random experiment. We can associate an indicator random variable, let us call it $X_j$, with the outcome of the $j$-th trial, for $j = 1, 2, \\dots, M_i$.\n\nBy definition, the indicator variable $X_j$ is defined as:\n$$\nX_j = \\begin{cases}\n    1  \\text{if the } j\\text{-th trajectory reaches } \\lambda_{i+1} \\text{ (success)} \\\\\n    0  \\text{if the } j\\text{-th trajectory returns to } A \\text{ (failure)}\n\\end{cases}\n$$\nThe probability of a success is given as $p_i$. Therefore, the probability mass function for $X_j$ is:\n$$\nP(X_j = 1) = p_i\n$$\n$$\nP(X_j = 0) = 1 - p_i\n$$\nThis is the definition of a Bernoulli distribution with parameter $p_i$. Since the trials are \"identically prepared,\" all $X_j$ follow the same distribution.\n\nThe natural and standard estimator for the probability parameter $p_i$ is the sample mean of the outcomes, which is the fraction of successful trials. We define the estimator $\\hat{p}_i$ as:\n$$\n\\hat{p}_i = \\frac{1}{M_i} \\sum_{j=1}^{M_i} X_j\n$$\nTo verify that this estimator is unbiased, we must show that its expected value, $E[\\hat{p}_i]$, is equal to the true parameter, $p_i$. We use the linearity of the expectation operator:\n$$\nE[\\hat{p}_i] = E\\left[ \\frac{1}{M_i} \\sum_{j=1}^{M_i} X_j \\right] = \\frac{1}{M_i} \\sum_{j=1}^{M_i} E[X_j]\n$$\nThe expectation of a single Bernoulli random variable $X_j$ is:\n$$\nE[X_j] = 1 \\cdot P(X_j = 1) + 0 \\cdot P(X_j = 0) = 1 \\cdot p_i + 0 \\cdot (1 - p_i) = p_i\n$$\nSubstituting this result into the expression for $E[\\hat{p}_i]$:\n$$\nE[\\hat{p}_i] = \\frac{1}{M_i} \\sum_{j=1}^{M_i} p_i = \\frac{1}{M_i} (M_i \\cdot p_i) = p_i\n$$\nThus, $E[\\hat{p}_i] = p_i$, and the estimator is proven to be unbiased.\n\nNext, we compute the variance of this estimator, $\\mathrm{Var}(\\hat{p}_i)$. We use the property of variance that for a constant $c$ and random variable $Y$, $\\mathrm{Var}(cY) = c^2 \\mathrm{Var}(Y)$.\n$$\n\\mathrm{Var}(\\hat{p}_i) = \\mathrm{Var}\\left(\\frac{1}{M_i} \\sum_{j=1}^{M_i} X_j\\right) = \\left(\\frac{1}{M_i}\\right)^2 \\mathrm{Var}\\left(\\sum_{j=1}^{M_i} X_j\\right)\n$$\nThe problem statement specifies that the $M_i$ trials are \"independent.\" For a sum of independent random variables, the variance of the sum is the sum of the variances. Therefore:\n$$\n\\mathrm{Var}\\left(\\sum_{j=1}^{M_i} X_j\\right) = \\sum_{j=1}^{M_i} \\mathrm{Var}(X_j)\n$$\nNow, we must find the variance of a single Bernoulli variable $X_j$. The variance is defined as $\\mathrm{Var}(X_j) = E[X_j^2] - (E[X_j])^2$. We already know $E[X_j] = p_i$. To find $E[X_j^2]$, we note that for an indicator variable, $X_j^2 = X_j$, because $1^2 = 1$ and $0^2 = 0$. Thus:\n$$\nE[X_j^2] = E[X_j] = p_i\n$$\nThe variance of $X_j$ is therefore:\n$$\n\\mathrm{Var}(X_j) = E[X_j^2] - (E[X_j])^2 = p_i - (p_i)^2 = p_i(1 - p_i)\n$$\nSince the trials are identically prepared, the variance is the same for all $j$. We can now substitute this back into our expression for $\\mathrm{Var}(\\hat{p}_i)$:\n$$\n\\mathrm{Var}(\\hat{p}_i) = \\frac{1}{M_i^2} \\sum_{j=1}^{M_i} p_i(1 - p_i)\n$$\nThe sum consists of $M_i$ identical terms:\n$$\n\\mathrm{Var}(\\hat{p}_i) = \\frac{1}{M_i^2} \\left[ M_i \\cdot p_i(1 - p_i) \\right]\n$$\nSimplifying the expression by canceling one factor of $M_i$ yields the final result for the variance of the estimator $\\hat{p}_i$:\n$$\n\\mathrm{Var}(\\hat{p}_i) = \\frac{p_i(1 - p_i)}{M_i}\n$$\nThis result is fundamental. It shows that the variance of the sample proportion is inversely proportional to the number of trials, a cornerstone of statistical inference.",
            "answer": "$$\\boxed{\\frac{p_i(1 - p_i)}{M_i}}$$"
        },
        {
            "introduction": "Moving from theory to practice, implementing FFS in a discrete-time simulation environment introduces practical challenges that are not apparent in the continuous-time idealization. This problem tackles the common issue of 'overshoots,' where a trajectory jumps past one or more interfaces in a single time step. Choosing the correct way to handle these events is critical for avoiding systematic bias in your rate estimate and requires a clear understanding of the event definitions that underpin the FFS method .",
            "id": "2645632",
            "problem": "Consider a rare-event rate calculation using Forward Flux Sampling (FFS) for a stochastic reaction network with an order parameter $\\lambda(\\mathbf{x})$. Two metastable sets are defined by $A=\\{\\lambda(\\mathbf{x})\\lambda_0\\}$ and $B=\\{\\lambda(\\mathbf{x})\\lambda_n\\}$, with nested non-intersecting interfaces $\\lambda_0\\lambda_1\\cdots\\lambda_n$. At FFS stage $i$ (with $0\\le i\\le n-1$), trajectories are initiated from configurations on the isosurface $\\lambda(\\mathbf{x})=\\lambda_i$ and propagated under the same dynamics used to generate the steady-state flux from $A$. Let $p_i$ denote the probability that such a trajectory reaches any state with $\\lambda(\\mathbf{x})\\lambda_{i+1}$ before returning to $A$; in the idealized continuous-time limit with a continuous order parameter, this coincides with the probability to reach the next interface $\\lambda_{i+1}$ before returning to $A$.\n\nIn a practical discrete-time integrator or when using a jumpy or coarsely binned order parameter, it can happen that a single step advances a trajectory from $\\lambda_i$ to a configuration with $\\lambda(\\mathbf{x})\\lambda_{i+k}$ for some integer $k\\ge 1$ without registering visits to intermediate interfaces. This is an “overshoot.” You must decide how to treat such overshoots in estimating $p_i$ and in handing off configurations to subsequent FFS stages, under the standard assumptions that the dynamics is Markovian and that interfaces define stopping times for first-hitting events.\n\nWhich of the following rules yields an unbiased FFS estimator and a consistent path-ensemble construction?\n\nA. Count any trajectory whose first-hitting event is at some interface index $ji$ as a success for stage $i$. Resolve the first-hitting interface as $j=\\min\\{mi:\\text{ the trajectory crosses }\\lambda_m\\}$, using time interpolation if necessary; increment the success count for $i$, attribute the landing to interface $j$, and, for subsequent sampling, use the configuration at the first-hitting time on $\\lambda_j$. Do not split the event across multiple interfaces.\n\nB. Discard any trajectory that overshoots beyond $\\lambda_{i+1}$ as invalid, since skipping $\\lambda_{i+1}$ violates the sequential-crossing requirement; keep only trajectories that register a hit of $\\lambda_{i+1}$ exactly, to avoid overestimating $p_i$.\n\nC. Treat a single overshoot from $\\lambda_i$ to $\\lambda_{i+k}$ as $k$ sequential successes (at $\\lambda_i,\\lambda_{i+1},\\dots,\\lambda_{i+k-1}$) and increment the success counters for all these stages without additional simulation, because the trajectory would have necessarily crossed all intervening interfaces.\n\nD. Count an overshoot as a failure at stage $i$ unless $\\lambda(\\mathbf{x})$ is known to be strictly monotone along all reactive trajectories, in which case count it as a success; in both cases, continue subsequent sampling from the configuration beyond $\\lambda_{i+k}$.",
            "solution": "The problem statement must first be validated for scientific soundness, self-consistency, and clarity before a solution is attempted.\n\n### Step 1: Extract Givens\n-   **System**: A stochastic reaction network.\n-   **Method**: Forward Flux Sampling (FFS) for rare-event rate calculation.\n-   **Order Parameter**: $\\lambda(\\mathbf{x})$.\n-   **Metastable States**: State $A$ is defined by $\\{\\lambda(\\mathbf{x})\\lambda_0\\}$ and state $B$ is defined by $\\{\\lambda(\\mathbf{x})\\lambda_n\\}$.\n-   **Interfaces**: A set of nested, non-intersecting interfaces defined by $\\lambda_0\\lambda_1\\cdots\\lambda_n$.\n-   **FFS Stage $i$**: Trajectories are initiated from configurations on the isosurface $\\lambda(\\mathbf{x})=\\lambda_i$ for $0\\le i\\le n-1$.\n-   **Probability $p_i$**: The probability that a trajectory starting from $\\lambda_i$ reaches any state with $\\lambda(\\mathbf{x})\\lambda_{i+1}$ before returning to state $A$.\n-   **Problem Context**: In discrete-time integration, a single step can advance a trajectory from $\\lambda_i$ to a configuration with $\\lambda(\\mathbf{x})\\lambda_{i+k}$ for some integer $k\\ge 1$. This is termed an \"overshoot\".\n-   **Assumptions**: The system dynamics are Markovian, and interfaces define stopping times for first-hitting events.\n-   **Objective**: Identify the rule for treating overshoots that yields an unbiased FFS estimator and a consistent path-ensemble construction.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria.\n\n-   **Scientifically Grounded**: The problem is firmly grounded in computational statistical physics and chemical kinetics. Forward Flux Sampling is a well-established and widely used technique for simulating rare events. The issue of \"overshoots\" or \"long jumps\" is a known and critical practical detail in the implementation of FFS, especially for discrete-time simulations or systems with large, discrete changes (e.g., jump processes). The assumptions of Markovian dynamics and interfaces as stopping times are standard for the theoretical foundation of FFS. The problem is scientifically valid.\n-   **Well-Posed**: The problem is well-posed. It asks for the correct algorithmic rule among a set of choices to ensure two specific desirable properties of the FFS method: an unbiased estimator and a consistent path ensemble. A unique correct answer is expected based on the principles of FFS.\n-   **Objective**: The language is technical, precise, and objective. Terms like \"unbiased estimator\" and \"Markovian\" have rigorous definitions in this context.\n-   **Other Flaws**: The problem setup does not violate any fundamental principles, is not incomplete, and presents a realistic challenge in the application of a computational method. The terminology is standard within the field.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is a legitimate and non-trivial question concerning the correct implementation of the Forward Flux Sampling algorithm. I will proceed with the solution.\n\n### Solution Derivation\nThe goal of Forward Flux Sampling is to calculate the rate constant $k_{AB}$ for the transition from a state $A$ to a state $B$. The rate is given by the flux $\\Phi_A$ of trajectories leaving $A$ multiplied by the probability $P(B|A)$ that a trajectory leaving $A$ reaches $B$ before returning to $A$.\n$$k_{AB} = \\Phi_A P(B|A)$$\nFFS computes $P(B|A)$ by decomposing it into a product of conditional probabilities using a series of interfaces $\\lambda_i$:\n$$P(B|A) = P(\\lambda_1|A) \\times P(\\lambda_2|\\lambda_1) \\times \\dots \\times P(B|\\lambda_{n-1})$$\nIn the notation of the problem, $p_i$ is the estimator for the conditional probability $P(\\lambda_{i+1}|\\lambda_i)$, which is defined as the probability that a trajectory initiated at interface $\\lambda_i$ will reach interface $\\lambda_{i+1}$ before returning to state $A$. More precisely, as stated in the problem, $p_i$ is the probability that a trajectory from $\\lambda_i$ \"reaches any state with $\\lambda(\\mathbf{x})\\lambda_{i+1}$ before returning to $A$\". The overall rate is then estimated as:\n$$k_{AB} \\approx \\Phi_A \\prod_{i=0}^{n-1} \\hat{p}_i$$\nwhere $\\hat{p}_i$ is the numerical estimate of $p_i$. This estimate is obtained by running $N_i$ trials from configurations on interface $\\lambda_i$ and counting the number of successful trials, $N_{i+1}$, that reach $\\lambda_{i+1}$. Thus, $\\hat{p}_i = N_{i+1}/N_i$.\n\nThe core of the problem is how to classify an \"overshoot\" event—a trajectory that jumps from interface $\\lambda_i$ to a state with $\\lambda(\\mathbf{x})  \\lambda_{i+k}$ for $k \\ge 1$ in a single discrete step. According to the definition of success for stage $i$, a trial is successful if the trajectory reaches *any* state with $\\lambda(\\mathbf{x})  \\lambda_{i+1}$ before returning to $A$. An overshoot trajectory to $\\lambda_{i+k}$ (where $k \\ge 1$) has, by definition, reached a state with $\\lambda(\\mathbf{x})  \\lambda_{i+1}$. Therefore, it is unequivocally a successful event for stage $i$. Any rule that classifies this event as a failure or discards it is fundamentally incorrect and will introduce a systematic bias by undercounting successful paths.\n\nWith this principle established, we evaluate each option.\n\n### Option-by-Option Analysis\n\n**A. Count any trajectory whose first-hitting event is at some interface index $ji$ as a success for stage $i$. Resolve the first-hitting interface as $j=\\min\\{mi:\\text{ the trajectory crosses }\\lambda_m\\}$, using time interpolation if necessary; increment the success count for $i$, attribute the landing to interface $j$, and, for subsequent sampling, use the configuration at the first-hitting time on $\\lambda_j$. Do not split the event across multiple interfaces.**\n\nThis rule is consistent with the fundamental principles of FFS.\n1.  It correctly counts the overshoot as a success for stage $i$, since the trajectory has reached a region with $\\lambda  \\lambda_{i+1}$. This ensures the estimator $\\hat{p}_i$ is unbiased.\n2.  It correctly identifies the need to generate a valid configuration for subsequent stages. By finding the first interface crossed, $\\lambda_j$, and using the configuration at that point, it generates a valid point for the ensemble of states at interface $\\lambda_j$. This configuration can then be used to initiate trials for the transition from $\\lambda_j$ to $\\lambda_{j+1}$. This maintains the Markovian property; the future evolution from $\\lambda_j$ depends only on the configuration at $\\lambda_j$, not on the fact that it arrived there via an overshoot.\n3.  The phrase \"Do not split the event across multiple interfaces\" correctly avoids the statistical fallacy described in option C.\nThis procedure ensures that both the estimator for each $p_i$ is constructed correctly and the path ensemble for subsequent stages is populated with statistically valid configurations. This is the standard, correct procedure, often referred to as \"Direct FFS\".\n\n**Verdict: Correct.**\n\n**B. Discard any trajectory that overshoots beyond $\\lambda_{i+1}$ as invalid, since skipping $\\lambda_{i+1}$ violates the sequential-crossing requirement; keep only trajectories that register a hit of $\\lambda_{i+1}$ exactly, to avoid overestimating $p_i$.**\n\nThis rule is incorrect. As established, an overshoot is a valid successful event for stage $i$. Discarding these trajectories means we are systematically throwing away real, successful pathways. This will cause the numerator $N_{i+1}$ in the estimator $\\hat{p}_i = N_{i+1}/N_i$ to be artificially low, leading to a systematic underestimation of $p_i$ and, consequently, the overall rate $k_{AB}$. The justification provided in the option (\"to avoid overestimating $p_i$\") is the opposite of the actual effect; this rule leads to underestimation.\n\n**Verdict: Incorrect.**\n\n**C. Treat a single overshoot from $\\lambda_i$ to $\\lambda_{i+k}$ as $k$ sequential successes (at $\\lambda_i,\\lambda_{i+1},\\dots,\\lambda_{i+k-1}$) and increment the success counters for all these stages without additional simulation, because the trajectory would have necessarily crossed all intervening interfaces.**\n\nThis rule is statistically flawed. A single trajectory realization from $\\lambda_i$ to $\\lambda_{i+k}$ is a single event. While it implies that the unconditional probability of reaching $\\lambda_{i+k}$ from $\\lambda_i$ is non-zero, it cannot be treated as $k$ independent successful trials for the intermediate conditional probabilities $p_i, p_{i+1}, \\dots, p_{i+k-1}$. The FFS method relies on estimating each $p_j$ by running a statistically significant number of trials from an ensemble of configurations at $\\lambda_j$. This rule bypasses the generation of these intermediate ensembles, effectively assuming that the conditional probability of crossing from one interface to the next is $1$ along this one path. This is a form of pseudo-replication or \"double counting\" that will correlate the estimators for different stages and lead to a significant overestimation of the total rate constant. The path-ensemble construction becomes inconsistent as it would lack configurations at interfaces $\\lambda_{i+1}, \\dots, \\lambda_{i+k-1}$.\n\n**Verdict: Incorrect.**\n\n**D. Count an overshoot as a failure at stage $i$ unless $\\lambda(\\mathbf{x})$ is known to be strictly monotone along all reactive trajectories, in which case count it as a success; in both cases, continue subsequent sampling from the configuration beyond $\\lambda_{i+k}$.**\n\nThis rule is incorrect because its primary instruction for the general case is wrong. An overshoot must be counted as a success for stage $i$ irrespective of whether the order parameter is monotone or not. Classifying it as a failure introduces a systematic underestimation of the rate. The special case for monotone $\\lambda(\\mathbf{x})$ does not rescue the rule, as the general case is flawed. The problem asks for a rule that works under the stated general assumptions, not only for a special, simplified case. Therefore, the entire rule is invalid.\n\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "An effective FFS simulation is not just correct, but also computationally efficient. Given that the total computational budget is always finite, a key question arises: how should we allocate resources among the different FFS stages to achieve the most precise result? This advanced exercise guides you through an optimization problem to minimize the overall statistical error in the final rate constant. Solving it reveals the principle of optimal resource allocation in FFS, a crucial step in designing and executing large-scale rare-event studies .",
            "id": "3761014",
            "problem": "Consider a rare-event transition from basin $A$ to basin $B$ simulated by Forward Flux Sampling (FFS), which employs a sequence of interfaces $\\lambda_{0}  \\lambda_{1}  \\cdots  \\lambda_{n}$ separating $A$ from $B$. The FFS estimator for the rate constant $k_{AB}$ is constructed as $\\hat{k}_{AB} = \\hat{\\Phi}_{A} \\prod_{i=0}^{n-1} \\hat{p}_{i}$, where $\\hat{\\Phi}_{A}$ is the estimated flux of trajectories leaving basin $A$ and hitting the first interface $\\lambda_{0}$, and $\\hat{p}_{i}$ is the estimated conditional probability that a trajectory launched from interface $\\lambda_{i}$ reaches the next interface $\\lambda_{i+1}$ before returning to $A$.\n\nYou are given that for each interface $i$, the success probability $p_{i}$ is strictly between $0$ and $1$, and that $\\hat{p}_{i}$ is obtained from $M_{i}$ independent Bernoulli trials with success probability $p_{i}$ by $\\hat{p}_{i} = S_{i}/M_{i}$, where $S_{i}$ is the number of successes. Assume the contributions of $\\hat{\\Phi}_{A}$ to the variance of $\\log \\hat{k}_{AB}$ are negligible compared to those from the $\\hat{p}_{i}$ factors and may be treated as constant for the purpose of optimization. Further assume that the $\\hat{p}_{i}$ are statistically independent across interfaces (by independent sampling and conditioning in FFS), and that the Central Limit Theorem (CLT) and the delta method apply for large $M_{i}$.\n\nEach trial at interface $i$ has a known average computational cost $c_{i}  0$ (measured in a consistent computational work unit), so that allocating $M_{i}$ trials at interface $i$ incurs a total cost $c_{i} M_{i}$. You are given a fixed total computational budget $B  0$ that must be fully allocated across all interfaces, satisfying $\\sum_{i=0}^{n-1} c_{i} M_{i} = B$.\n\nStarting from the fundamental definitions of binomial estimation and using the CLT with the delta method to obtain the leading-order variance of $\\log \\hat{k}_{AB}$ in the large-$M_{i}$ regime, do the following:\n\n- Derive the leading-order expression for $\\operatorname{Var}[\\log \\hat{k}_{AB}]$ in terms of $\\{p_{i}\\}$ and $\\{M_{i}\\}$, under the stated assumptions.\n- Formulate the continuous optimization problem that minimizes this leading-order variance subject to the fixed-budget constraint.\n- Solve this optimization problem to obtain the closed-form expression for the optimal allocation $\\{M_{i}^{\\ast}\\}$ as a function of $\\{p_{i}\\}$, $\\{c_{i}\\}$, and $B$.\n\nExpress your final answer as a single closed-form analytic expression for the vector of optimal allocations $\\{M_{i}^{\\ast}\\}_{i=0}^{n-1}$. No rounding is required.",
            "solution": "The user's request is to derive the optimal allocation of computational effort in a Forward Flux Sampling (FFS) simulation to minimize the variance of the estimated rate constant, subject to a fixed total budget. The problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. Therefore, the problem is deemed valid.\n\nThe solution process consists of three main parts:\n1.  Derive the leading-order expression for the variance of $\\log \\hat{k}_{AB}$.\n2.  Formulate the constrained optimization problem.\n3.  Solve the optimization problem to find the optimal allocation $\\{M_{i}^{\\ast}\\}$.\n\n### Part 1: Derivation of the Variance of $\\log \\hat{k}_{AB}$\n\nThe FFS estimator for the rate constant $k_{AB}$ is given by:\n$$\n\\hat{k}_{AB} = \\hat{\\Phi}_{A} \\prod_{i=0}^{n-1} \\hat{p}_{i}\n$$\nWe are interested in the variance of its logarithm, $\\log \\hat{k}_{AB}$. Taking the natural logarithm of the expression yields:\n$$\n\\log \\hat{k}_{AB} = \\log\\left(\\hat{\\Phi}_{A} \\prod_{i=0}^{n-1} \\hat{p}_{i}\\right) = \\log \\hat{\\Phi}_{A} + \\sum_{i=0}^{n-1} \\log \\hat{p}_{i}\n$$\nThe variance of this quantity is:\n$$\n\\operatorname{Var}[\\log \\hat{k}_{AB}] = \\operatorname{Var}\\left[\\log \\hat{\\Phi}_{A} + \\sum_{i=0}^{n-1} \\log \\hat{p}_{i}\\right]\n$$\nThe problem states that the contribution of $\\hat{\\Phi}_{A}$ to the variance is negligible, so we treat $\\log \\hat{\\Phi}_{A}$ as a constant. Furthermore, the estimators $\\hat{p}_{i}$ for different interfaces are statistically independent. Therefore, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}[\\log \\hat{k}_{AB}] = \\operatorname{Var}\\left[\\sum_{i=0}^{n-1} \\log \\hat{p}_{i}\\right] = \\sum_{i=0}^{n-1} \\operatorname{Var}[\\log \\hat{p}_{i}]\n$$\nTo find $\\operatorname{Var}[\\log \\hat{p}_{i}]$, we use the delta method. The estimator $\\hat{p}_{i} = S_{i}/M_{i}$, where $S_{i}$ is the number of successful trials out of $M_{i}$ independent Bernoulli trials, follows a scaled binomial distribution. The true probability of success is $p_i$. The number of successes $S_i$ follows a binomial distribution $S_i \\sim \\text{Binomial}(M_i, p_i)$.\n\nThe expected value and variance of the estimator $\\hat{p}_{i}$ are:\n$$\nE[\\hat{p}_{i}] = E\\left[\\frac{S_{i}}{M_{i}}\\right] = \\frac{1}{M_{i}} E[S_{i}] = \\frac{M_{i} p_{i}}{M_{i}} = p_{i}\n$$\n$$\n\\operatorname{Var}[\\hat{p}_{i}] = \\operatorname{Var}\\left[\\frac{S_{i}}{M_{i}}\\right] = \\frac{1}{M_{i}^{2}} \\operatorname{Var}[S_{i}] = \\frac{M_{i} p_{i} (1 - p_{i})}{M_{i}^{2}} = \\frac{p_{i}(1 - p_{i})}{M_{i}}\n$$\nThe delta method provides an approximation for the variance of a function of a random variable. For a function $g(X)$, the approximation is $\\operatorname{Var}[g(X)] \\approx (g'(E[X]))^{2} \\operatorname{Var}[X]$. Here, our random variable is $\\hat{p}_{i}$ and the function is $g(x) = \\log(x)$. The derivative is $g'(x) = 1/x$.\n\nApplying the delta method, with $E[\\hat{p}_{i}]=p_i$:\n$$\n\\operatorname{Var}[\\log \\hat{p}_{i}] \\approx (g'(p_{i}))^{2} \\operatorname{Var}[\\hat{p}_{i}] = \\left(\\frac{1}{p_{i}}\\right)^{2} \\left(\\frac{p_{i}(1 - p_{i})}{M_{i}}\\right) = \\frac{1 - p_{i}}{p_{i} M_{i}}\n$$\nThis is the leading-order term for the variance in the large-$M_i$ limit, as justified by the Central Limit Theorem.\n\nSubstituting this back into the expression for the total variance, we obtain the objective function to be minimized:\n$$\n\\operatorname{Var}[\\log \\hat{k}_{AB}] \\approx \\sum_{i=0}^{n-1} \\frac{1 - p_{i}}{p_{i} M_{i}}\n$$\n\n### Part 2: Formulation of the Optimization Problem\n\nWe aim to minimize the derived variance subject to a fixed total computational budget. Let $V(\\{M_i\\})$ denote the approximate variance. The problem is formulated as:\n\nMinimize:\n$$\nV(\\{M_i\\}) = \\sum_{i=0}^{n-1} \\frac{1 - p_{i}}{p_{i} M_{i}}\n$$\nSubject to the constraint:\n$$\n\\sum_{i=0}^{n-1} c_{i} M_{i} = B\n$$\nwhere $M_{i}  0$ for all $i \\in \\{0, 1, \\dots, n-1\\}$.\n\n### Part 3: Solving the Optimization Problem\n\nThis is a constrained optimization problem that can be solved using the method of Lagrange multipliers. We define the Lagrangian $\\mathcal{L}$:\n$$\n\\mathcal{L}(\\{M_i\\}, \\lambda) = \\sum_{i=0}^{n-1} \\frac{1 - p_{i}}{p_{i} M_{i}} + \\lambda \\left( \\left( \\sum_{i=0}^{n-1} c_{i} M_{i} \\right) - B \\right)\n$$\nTo find the optimal values $\\{M_{i}^{\\ast}\\}$, we set the partial derivative of $\\mathcal{L}$ with respect to each $M_k$ (for $k \\in \\{0, \\dots, n-1\\}$) to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial M_k} = \\frac{\\partial}{\\partial M_k} \\left( \\frac{1 - p_k}{p_k M_k} \\right) + \\lambda \\frac{\\partial}{\\partial M_k} (c_k M_k) = 0\n$$\n$$\n-\\frac{1 - p_k}{p_k M_k^2} + \\lambda c_k = 0\n$$\nSolving for $M_k$:\n$$\n\\lambda c_k = \\frac{1 - p_k}{p_k M_k^2} \\implies M_k^2 = \\frac{1 - p_k}{\\lambda c_k p_k}\n$$\nSince $M_k$ must be positive, we take the positive root:\n$$\nM_k = \\sqrt{\\frac{1 - p_k}{\\lambda c_k p_k}} = \\frac{1}{\\sqrt{\\lambda}} \\sqrt{\\frac{1 - p_k}{c_k p_k}}\n$$\nNow, we use the budget constraint to determine the Lagrange multiplier $\\lambda$. We substitute the expression for $M_k$ into the constraint equation:\n$$\n\\sum_{k=0}^{n-1} c_k M_k = \\sum_{k=0}^{n-1} c_k \\left( \\frac{1}{\\sqrt{\\lambda}} \\sqrt{\\frac{1 - p_k}{c_k p_k}} \\right) = B\n$$\nFactoring out the term with $\\lambda$:\n$$\n\\frac{1}{\\sqrt{\\lambda}} \\sum_{k=0}^{n-1} c_k \\frac{\\sqrt{1 - p_k}}{\\sqrt{c_k p_k}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{k=0}^{n-1} \\sqrt{c_k} \\sqrt{\\frac{1-p_k}{p_k}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{k=0}^{n-1} \\sqrt{\\frac{c_k(1-p_k)}{p_k}} = B\n$$\nSolving for $1/\\sqrt{\\lambda}$:\n$$\n\\frac{1}{\\sqrt{\\lambda}} = \\frac{B}{\\sum_{j=0}^{n-1} \\sqrt{\\frac{c_j(1 - p_j)}{p_j}}}\n$$\nFinally, we substitute this expression for $1/\\sqrt{\\lambda}$ back into the equation for $M_k$ to find the optimal allocation $M_k^{\\ast}$:\n$$\nM_k^{\\ast} = \\left( \\frac{B}{\\sum_{j=0}^{n-1} \\sqrt{\\frac{c_j(1 - p_j)}{p_j}}} \\right) \\sqrt{\\frac{1 - p_k}{c_k p_k}}\n$$\nReplacing the index $k$ with $i$ as requested in the problem statement, we have the final expression for the vector of optimal allocations $\\{M_i^{\\ast}\\}_{i=0}^{n-1}$. Each component $M_i^{\\ast}$ is given by:\n$$\nM_{i}^{\\ast} = B \\frac{\\sqrt{\\frac{1-p_i}{c_i p_i}}}{\\sum_{j=0}^{n-1} \\sqrt{\\frac{c_j(1-p_j)}{p_j}}}\n$$",
            "answer": "$$\n\\boxed{M_{i}^{\\ast} = B \\frac{\\sqrt{\\frac{1-p_{i}}{c_{i}p_{i}}}}{\\sum_{j=0}^{n-1} \\sqrt{\\frac{c_{j}(1-p_{j})}{p_{j}}}}}\n$$"
        }
    ]
}