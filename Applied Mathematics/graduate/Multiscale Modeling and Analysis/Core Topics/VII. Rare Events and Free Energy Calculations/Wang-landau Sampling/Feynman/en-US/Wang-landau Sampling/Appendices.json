{
    "hands_on_practices": [
        {
            "introduction": "A primary result from a Wang-Landau simulation is an estimate of the density of states, $g(E)$, which is accurate up to a global multiplicative constant. While this relative estimate is sufficient for calculating canonical probabilities, it is not enough for determining absolute thermodynamic potentials like the free energy. This exercise guides you through the essential post-processing step of normalizing the density of states by anchoring it to a known physical value, such as the degeneracy of the ground state. ",
            "id": "3832190",
            "problem": "Consider a finite discrete-energy system intended for multiscale modeling, where accurate absolute thermodynamic potentials must be computed to couple microscopic and mesoscopic descriptions. The microcanonical density of states $g(E)$ is defined as the exact number of microstates with energy $E$. Wang-Landau sampling (WLS) constructs a random walk in energy space and iteratively refines an estimate of $\\ln g(E)$ to flatten the visited-energy histogram. In a finite system, the Wang-Landau sampling output is known to be accurate up to a global multiplicative constant in $g(E)$ (equivalently, an additive constant in $\\ln g(E)$).\n\nYou are given a Wang-Landau sampling estimate $\\ln \\tilde{g}(E)$ for three energy levels $E_0 < E_1 < E_2$ of a finite system. The known ground-state multiplicity (degeneracy) is $g(E_0) = d_0$, determined independently from symmetry. Assume that Boltzmann’s constant $k_{\\mathrm{B}}$ is set to $1$ and energies are reported in consistent units. The provided data are:\n- Known degeneracy at the ground state: $d_0 = 2$ at $E_0$.\n- Wang-Landau sampling outputs: $\\ln \\tilde{g}(E_0) = \\ln 5$, $\\ln \\tilde{g}(E_1) = \\ln 25$, $\\ln \\tilde{g}(E_2) = \\ln 50$.\n- Energies: $E_0 = -6$, $E_1 = -2$, $E_2 = 0$.\n- Consider the canonical ensemble at inverse temperature $\\beta = 1$.\n\nUsing only first principles and core definitions of the microcanonical density of states and canonical partition function, explain how to obtain an absolute normalization of $g(E)$ from the Wang-Landau sampling estimate by leveraging the known degeneracy at the reference energy $E_0$. Then, demonstrate the normalization procedure on the given data and compute the corresponding canonical partition function $Z(\\beta)$ at $\\beta = 1$.\n\nWhich option correctly describes the normalization principle and provides the correct normalized $g(E)$ values and canonical partition function for the given data?\n\nA. Enforce that the normalized $g(E)$ matches the known degeneracy at $E_0$ by a single global rescaling of $\\tilde{g}(E)$ across all energies. This yields $g(E) = c\\,\\tilde{g}(E)$ with $c$ chosen to satisfy $g(E_0) = d_0$. For the given data, $c = \\dfrac{2}{5}$, so $g(E_0) = 2$, $g(E_1) = 10$, $g(E_2) = 20$, and the canonical partition function at $\\beta = 1$ is $Z(1) = 2\\,e^{6} + 10\\,e^{2} + 20$.\n\nB. Correct the Wang-Landau sampling estimate by adding a constant to $g(E)$ so that $g(E_0)$ equals the known degeneracy, i.e., $g(E) = \\tilde{g}(E) + \\left(d_0 - \\tilde{g}(E_0)\\right)$. For the given data, this yields $g(E_0) = 2$, $g(E_1) = 22$, $g(E_2) = 47$, and $Z(1) = 2\\,e^{6} + 22\\,e^{2} + 47$.\n\nC. Normalize $\\ln \\tilde{g}(E)$ by subtracting $\\left(\\ln d_0 - \\ln \\tilde{g}(E_0)\\right)$ to obtain $\\ln g(E) = \\ln \\tilde{g}(E) - \\left(\\ln d_0 - \\ln \\tilde{g}(E_0)\\right)$. For the given data, this produces $g(E_0) = \\dfrac{25}{2}$, $g(E_1) = \\dfrac{125}{2}$, $g(E_2) = \\dfrac{250}{2}$, and $Z(1) = \\dfrac{25}{2}\\,e^{6} + \\dfrac{125}{2}\\,e^{2} + \\dfrac{250}{2}$.\n\nD. Normalize by converting $\\tilde{g}(E)$ into probabilities via $g_{\\mathrm{prob}}(E) = \\dfrac{\\tilde{g}(E)}{\\sum_{E'} \\tilde{g}(E')}$ and use these to define the partition function as $Z(\\beta) = \\sum_{E} g_{\\mathrm{prob}}(E)\\,e^{-\\beta E}$. For the given data, $\\sum_{E'} \\tilde{g}(E') = 80$, so $g_{\\mathrm{prob}}(E_0) = \\dfrac{5}{80}$, $g_{\\mathrm{prob}}(E_1) = \\dfrac{25}{80}$, $g_{\\mathrm{prob}}(E_2) = \\dfrac{50}{80}$, and $Z(1) = \\dfrac{5}{80}\\,e^{6} + \\dfrac{25}{80}\\,e^{2} + \\dfrac{50}{80}$.",
            "solution": "Begin from the definition of the microcanonical density of states $g(E)$ as the exact count of microstates that have energy $E$. In a finite system, $g(E)$ is an integer-valued function for discrete levels $E$. Wang-Landau sampling (WLS) performs a random walk in energy and updates the estimator of the logarithm of the density of states to flatten the energy histogram. The standard property of the WLS estimator is that it recovers the relative density of states but not its absolute scale, i.e., it determines $\\ln g(E)$ up to an additive constant. Consequently, the estimate $\\tilde{g}(E)$ is proportional to the true $g(E)$ by an unknown multiplicative constant, so there exists a constant $c > 0$ such that\n$$\ng(E) = c\\,\\tilde{g}(E), \\quad \\text{equivalently} \\quad \\ln g(E) = \\ln \\tilde{g}(E) + \\ln c.\n$$\n\nTo fix the absolute scale, one uses a known degeneracy at a reference energy $E_{\\mathrm{ref}}$ (here the ground state $E_0$), as is often available from symmetry or exact counting in finite systems. Imposing $g(E_{\\mathrm{ref}}) = d_{\\mathrm{ref}}$ determines the constant:\n$$\nc = \\frac{d_{\\mathrm{ref}}}{\\tilde{g}(E_{\\mathrm{ref}})}, \\quad \\text{or} \\quad \\ln c = \\ln d_{\\mathrm{ref}} - \\ln \\tilde{g}(E_{\\mathrm{ref}}).\n$$\nThis ensures a consistent global normalization of $g(E)$ across all energies. Notably, in the canonical ensemble, probabilities\n$$\np(E) = \\frac{g(E) \\, e^{-\\beta E}}{Z(\\beta)}, \\quad Z(\\beta) = \\sum_{E} g(E) \\, e^{-\\beta E},\n$$\nare invariant under the global rescaling $g(E) \\to c\\,g(E)$ because both numerator and denominator scale by $c$. However, absolute quantities such as the free energy $F(\\beta) = -\\beta^{-1} \\ln Z(\\beta)$ depend on the absolute normalization, which is required for multiscale coupling when combining contributions from different subsystems or scales.\n\nApply the normalization to the given data. The Wang-Landau sampling outputs are\n$$\n\\ln \\tilde{g}(E_0) = \\ln 5, \\quad \\ln \\tilde{g}(E_1) = \\ln 25, \\quad \\ln \\tilde{g}(E_2) = \\ln 50,\n$$\nso\n$$\n\\tilde{g}(E_0) = 5, \\quad \\tilde{g}(E_1) = 25, \\quad \\tilde{g}(E_2) = 50.\n$$\nWith $d_0 = 2$ known at $E_0$, we set\n$$\nc = \\frac{d_0}{\\tilde{g}(E_0)} = \\frac{2}{5}.\n$$\nTherefore, the normalized density of states is\n$$\ng(E_0) = c\\,\\tilde{g}(E_0) = \\frac{2}{5}\\cdot 5 = 2, \\quad\ng(E_1) = c\\,\\tilde{g}(E_1) = \\frac{2}{5}\\cdot 25 = 10, \\quad\ng(E_2) = c\\,\\tilde{g}(E_2) = \\frac{2}{5}\\cdot 50 = 20.\n$$\nAt inverse temperature $\\beta = 1$, the canonical partition function is\n$$\nZ(1) = \\sum_{i=0}^{2} g(E_i)\\, e^{- E_i} = g(E_0)\\, e^{-(-6)} + g(E_1)\\, e^{-(-2)} + g(E_2)\\, e^{-0} = 2\\,e^{6} + 10\\,e^{2} + 20.\n$$\nThis procedure aligns the Wang-Landau sampling estimate with the known ground-state multiplicity, resulting in an absolute normalization suitable for computing free energies and for multiscale coupling.\n\nOption-by-option analysis:\n\n- Option A: It correctly states the principle that the Wang-Landau sampling estimate is normalized by a single global multiplicative constant chosen to match the known degeneracy at the reference energy. It computes $c = 2/5$, yielding $g(E_0) = 2$, $g(E_1) = 10$, $g(E_2) = 20$, and the correct partition function $Z(1) = 2\\,e^{6} + 10\\,e^{2} + 20$. Verdict — Correct.\n\n- Option B: It incorrectly applies an additive correction to $g(E)$ rather than a multiplicative scaling. The quantity $g(E)$ is a count of states, and Wang-Landau sampling errors manifest as a global multiplicative factor, not an additive offset. The proposed $g(E)$ values and partition function are therefore incorrect. Verdict — Incorrect.\n\n- Option C: It proposes adjusting $\\ln \\tilde{g}(E)$ by subtracting $\\left(\\ln d_0 - \\ln \\tilde{g}(E_0)\\right)$, i.e., $\\ln g(E) = \\ln \\tilde{g}(E) - \\left(\\ln d_0 - \\ln \\tilde{g}(E_0)\\right) = \\ln \\tilde{g}(E) - \\ln(2/5)$, which implies $g(E) = \\tilde{g}(E)\\cdot (5/2)$. This is the wrong sign; the correct shift is $\\ln g(E) = \\ln \\tilde{g}(E) + \\left(\\ln d_0 - \\ln \\tilde{g}(E_0)\\right)$, corresponding to $g(E) = \\tilde{g}(E)\\cdot (2/5)$. As written, Option C yields $g(E_0) = 25/2 \\neq 2$, so it fails the normalization requirement. Verdict — Incorrect.\n\n- Option D: It converts $\\tilde{g}(E)$ into probabilities by dividing by their sum and then uses these probabilities in place of a density of states in the partition function. This conflates microcanonical densities with a normalized probability measure and produces an unphysical $Z(\\beta)$ that depends on the arbitrary normalization of probabilities rather than the absolute degeneracies. The canonical partition function must be computed from absolute $g(E)$, not normalized probabilities of energy levels. Verdict — Incorrect.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The power of Wang-Landau sampling extends beyond single-variable systems, enabling the exploration of complex phase spaces defined by multiple macroscopic variables. This practice challenges you to work with a joint density of states, $g(E, M)$, which depends on both energy and magnetization. By implementing an exact enumeration for a small system, you will construct this joint DOS and then use it to perform canonical reweighting, a powerful technique for calculating thermodynamic properties across a range of temperatures from a single simulation's output. ",
            "id": "3832210",
            "problem": "Consider a two-dimensional square Ising model with periodic boundary conditions on an $L \\times L$ lattice, with spins $s_{i} \\in \\{-1,+1\\}$ and nearest-neighbor coupling set to unity so that all quantities are dimensionless. Let the energy of a configuration be given by the standard nearest-neighbor Ising Hamiltonian $E(\\mathbf{s}) = -\\sum_{\\langle i,j\\rangle} s_{i} s_{j}$ where the sum is over each unordered nearest-neighbor bond exactly once, and the magnetization be $M(\\mathbf{s}) = \\sum_{i=1}^{L^2} s_{i}$. The joint density of states $g(E,M)$ counts the number of configurations with energy $E$ and magnetization $M$.\n\nYour task is to formulate joint sampling of $g(E,M)$ in the spirit of Wang-Landau (WL) sampling, specify an appropriate binning scheme for the discrete macrostates $(E,M)$, and then use this joint density of states to reweight and obtain the canonical magnetization distribution $P_{\\beta}(M)$ at inverse temperature $\\beta$. You must implement a program that constructs $g(E,M)$ by exact enumeration for small $L$ using the same $(E,M)$ binning you would employ in a joint WL simulation, and then computes $P_{\\beta}(M)$ by canonical reweighting from $g(E,M)$.\n\nStart from fundamental definitions and well-tested facts only; do not use any shortcut formulas that skip the derivation of acceptance criteria or reweighting. In particular, you must:\n- State the target stationary distribution for joint WL sampling over macrostates $(E,M)$ that yields a flat histogram in $(E,M)$ space, and from this derive a Metropolis-Hastings acceptance rule for a single-spin flip proposal that is symmetric in configuration space.\n- Justify an appropriate $(E,M)$ binning for this discrete Ising system to be used in joint WL sampling, and implement the same binning for exact enumeration to construct $g(E,M)$ as integer counts over bins.\n- Using only the canonical ensemble definition, explain how to obtain $P_{\\beta}(M)$ from $g(E,M)$ via reweighting, including proper normalization.\n\nThen implement a self-contained program that:\n- Enumerates all spin configurations for a given $L$ and constructs the exact joint density of states $g(E,M)$ using one bin per distinct energy and one bin per distinct magnetization observed under the model definitions above.\n- For a given inverse temperature $\\beta$, computes the canonical magnetization distribution $P_{\\beta}(M)$ by reweighting from $g(E,M)$ using only the canonical weights and normalizing properly.\n- Computes requested scalar statistics from $P_{\\beta}(M)$ for a small test suite.\n\nUse the following test suite of parameter values to exercise different regimes and edge cases:\n- Test $1$ (boundary case, infinite temperature): $L=2$, $\\beta=0$. Compute $P_{\\beta}(M)$ and return the value at $M=0$.\n- Test $2$ (moderate temperature): $L=2$, $\\beta=1$. Compute the mean absolute magnetization per spin, i.e., $\\langle |m| \\rangle$ with $m = M/L^2$.\n- Test $3$ (near critical region for two-dimensional Ising): $L=3$, $\\beta=0.44068679350977147$. Compute $P_{\\beta}(M)$ and return the value at $M=0$.\n\nAll energies, magnetizations, and inverse temperatures are dimensionless under the given definitions, so no physical unit conversions are required. Angles are not involved. Each requested answer must be a real number. Your program should produce a single line of output containing the results for Tests $1$–$3$ as a comma-separated list enclosed in square brackets, with each floating-point number rounded to six decimal places, in the order specified above, i.e., $[r_1,r_2,r_3]$ where $r_1$ corresponds to Test $1$, $r_2$ to Test $2$, and $r_3$ to Test $3$.",
            "solution": "The problem asks for a theoretical formulation and a computational implementation for analyzing a two-dimensional Ising model. The analysis involves deriving key aspects of joint Wang-Landau sampling, specifying a binning scheme for macrostates, explaining the canonical reweighting procedure, and finally, implementing an exact enumeration for small lattices to compute specific physical quantities.\n\nThe system is defined on an $L \\times L$ square lattice with periodic boundary conditions. The spin at each site $i$ is $s_i \\in \\{-1, +1\\}$. The total number of spins is $N=L^2$. The energy of a configuration $\\mathbf{s} = (s_1, \\dots, s_N)$ is given by the nearest-neighbor Ising Hamiltonian $E(\\mathbf{s}) = -\\sum_{\\langle i,j\\rangle} s_{i} s_{j}$, where the coupling constant is set to unity and the sum is over unique nearest-neighbor pairs. The magnetization is $M(\\mathbf{s}) = \\sum_{i=1}^{N} s_{i}$.\n\nThe problem is valid as it is scientifically grounded in the principles of statistical mechanics, is well-posed with all necessary information provided, and is objective in its formulation.\n\n### 1. Joint Wang-Landau Sampling and Acceptance Criteria\n\nThe goal of Wang-Landau (WL) sampling is to obtain an estimate of the density of states (DOS), in this case, the joint density of states $g(E,M)$, which counts the number of microscopic configurations for each macroscopic state $(E,M)$. The WL algorithm performs a random walk in the space of macrostates, designed to produce a flat histogram across all visited states $(E,M)$. This is achieved by sampling microstates with a probability inversely proportional to the density of states.\n\nThe target stationary distribution for a microstate $\\mathbf{s}$ is therefore $\\pi(\\mathbf{s}) \\propto 1/g(E(\\mathbf{s}), M(\\mathbf{s}))$. The random walk proceeds by proposing a move from a configuration $\\mathbf{s}$ to a new configuration $\\mathbf{s}'$. The problem specifies a single-spin flip proposal mechanism. Flipping a single spin is a symmetric operation: the probability of proposing a move from $\\mathbf{s}$ to $\\mathbf{s}'$ by flipping a specific spin is the same as proposing the reverse move from $\\mathbf{s}'$ to $\\mathbf{s}$ by flipping the same spin. Thus, the proposal probability distribution is symmetric, $p(\\mathbf{s} \\to \\mathbf{s}') = p(\\mathbf{s}' \\to \\mathbf{s})$.\n\nUnder a symmetric proposal, the Metropolis-Hastings acceptance probability $A(\\mathbf{s} \\to \\mathbf{s}')$ simplifies to the Metropolis rule:\n$$A(\\mathbf{s} \\to \\mathbf{s}') = \\min\\left(1, \\frac{\\pi(\\mathbf{s}')}{\\pi(\\mathbf{s})}\\right)$$\nLet the macrostate of $\\mathbf{s}$ be $(E,M)$ and the macrostate of $\\mathbf{s}'$ be $(E',M')$. Substituting our target distribution, we get:\n$$A((E,M) \\to (E',M')) = \\min\\left(1, \\frac{1/g(E',M')}{1/g(E,M)}\\right) = \\min\\left(1, \\frac{g(E,M)}{g(E',M')}\\right)$$\nIn practice, the WL algorithm iteratively builds an estimate for $g(E,M)$, typically by working with its logarithm, $\\ln g(E,M)$. The acceptance probability is then calculated as:\n$$A((E,M) \\to (E',M')) = \\min\\left(1, \\exp[\\ln g(E,M) - \\ln g(E',M')]\\right)$$\nWhenever a macrostate is visited, its corresponding $\\ln g$ value is updated by adding a modification factor, which gradually decreases to refine the DOS estimate.\n\n### 2. Binning Scheme for Macrovariables $(E,M)$\n\nFor the specified Ising model, both energy $E$ and magnetization $M$ are discrete quantities.\n\n**Magnetization ($M$)**: The total magnetization is $M = \\sum_{i=1}^{L^2} s_i$. Let $N_{up}$ be the number of spins with value $+1$ and $N_{down}$ be the number of spins with value $-1$. Then $N_{up} + N_{down} = L^2$ and $M = N_{up} - N_{down}$. Substituting $N_{down} = L^2 - N_{up}$, we find $M = 2N_{up} - L^2$. Since $N_{up}$ can be any integer from $0$ to $L^2$, the possible values of $M$ are $-L^2, -L^2+2, \\dots, L^2-2, L^2$. The magnetization values are integers separated by a step of $\\Delta M = 2$.\n\n**Energy ($E$)**: The energy is $E = -\\sum_{\\langle i,j \\rangle} s_i s_j$. On an $L \\times L$ lattice with periodic boundary conditions, there are $2L^2$ nearest-neighbor bonds. Each term $s_i s_j$ is either $+1$ or $-1$. The energy is therefore an integer. It can be shown that for a single-spin flip, the change in energy $\\Delta E$ is always a multiple of $4$. Consequently, all accessible energy levels for a given lattice are separated by integer multiples of $4$.\n\nGiven that both $E$ and $M$ take on discrete, well-separated values, the most natural and accurate binning scheme is to use one bin for each distinct pair $(E,M)$ that can be realized by the system. This discrete binning avoids any loss of information that would occur with wider bins. For the task of exact enumeration, this corresponds to creating a map or dictionary where each key is a unique $(E,M)$ tuple observed.\n\n### 3. Canonical Reweighting from $g(E,M)$ to $P_{\\beta}(M)$\n\nThe connection between the microcanonical information contained in the density of states $g(E,M)$ and the canonical ensemble properties at an inverse temperature $\\beta = 1/(k_B T)$ is established through the partition function $Z$.\n\nIn the canonical ensemble, the probability of a microstate $\\mathbf{s}$ is given by $P(\\mathbf{s}) = \\frac{e^{-\\beta E(\\mathbf{s})}}{Z}$. The partition function $Z$ is the sum over all microstates:\n$$Z = \\sum_{\\mathbf{s}} e^{-\\beta E(\\mathbf{s})}$$\nWe can restructure this sum by first grouping all microstates that share the same macrostate $(E,M)$. The number of such states is precisely the joint density of states, $g(E,M)$.\n$$Z = \\sum_{E,M} \\sum_{\\substack{\\mathbf{s} \\text{ s.t.} \\\\ E(\\mathbf{s})=E, M(\\mathbf{s})=M}} e^{-\\beta E} = \\sum_{E,M} g(E,M) e^{-\\beta E}$$\nThe probability of observing a particular macrostate $(E,M)$ is the sum of probabilities of all microstates belonging to it:\n$$P_{\\beta}(E,M) = \\frac{g(E,M) e^{-\\beta E}}{Z}$$\nTo obtain the canonical magnetization distribution $P_{\\beta}(M)$, we marginalize this joint probability distribution over all possible energy values $E$:\n$$P_{\\beta}(M) = \\sum_{E} P_{\\beta}(E,M) = \\sum_{E} \\frac{g(E,M) e^{-\\beta E}}{Z}$$\nThe procedure for calculating $P_{\\beta}(M)$ from a known $g(E,M)$ is therefore:\n1.  Calculate the partition function $Z(\\beta) = \\sum_{E,M} g(E,M) e^{-\\beta E}$. The sum is over all $(E,M)$ pairs for which $g(E,M)>0$.\n2.  For each possible magnetization value $M_k$, calculate the unnormalized probability $\\tilde{P}_{\\beta}(M_k) = \\sum_{E} g(E,M_k) e^{-\\beta E}$, where the sum is over all energies $E$ associated with $M_k$.\n3.  The normalized probability is $P_{\\beta}(M_k) = \\frac{\\tilde{P}_{\\beta}(M_k)}{Z(\\beta)}$.\n\n### 4. Implementation via Exact Enumeration\n\nFor small lattice sizes, such as $L=2$ and $L=3$, it is feasible to exactly enumerate all $2^{L^2}$ possible spin configurations. The implementation proceeds as follows:\n1.  Initialize an empty dictionary, `g_EM`, to store the joint density of states $g(E,M)$.\n2.  Iterate through all integers from $0$ to $2^{L^2}-1$. Each integer's binary representation maps to a unique spin configuration (e.g., bit $0 \\to s=-1$, bit $1 \\to s=+1$).\n3.  For each configuration, calculate its total energy $E$ and total magnetization $M$. The energy calculation under periodic boundary conditions can be efficiently implemented by summing the product of each spin with its neighbors to the \"right\" and \"down\", which counts each bond exactly once: $E = -\\sum_{i=0}^{L-1}\\sum_{j=0}^{L-1} s_{i,j}(s_{(i+1)\\%L, j} + s_{i,(j+1)\\%L})$.\n4.  Use the calculated $(E,M)$ pair as a key in the `g_EM` dictionary and increment its value (count).\n5.  Once all $2^{L^2}$ configurations have been processed, `g_EM` contains the exact joint density of states.\n6.  Using the `g_EM` dictionary and the reweighting formulas from Part 3, compute the partition function $Z$ and the probability distribution $P_{\\beta}(M)$ for any given $\\beta$.\n7.  Finally, compute the requested statistics from the resulting $P_{\\beta}(M)$ distribution, such as point probabilities $P_{\\beta}(M=M_0)$ or expectation values like $\\langle |m| \\rangle = \\sum_M |M/L^2| P_{\\beta}(M)$.\n\nThis procedure provides an exact solution for the given finite systems, serving as a benchmark for stochastic methods like Wang-Landau sampling.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by enumerating states for small Ising models,\n    calculating the joint density of states g(E,M), and then using\n    canonical reweighting to find properties at given temperatures.\n    \"\"\"\n    test_cases = [\n        # (L, beta, statistic_type)\n        (2, 0.0, \"P(M=0)\"),\n        (2, 1.0, \"|m|>\"),\n        (3, 0.44068679350977147, \"P(M=0)\")\n    ]\n\n    results = []\n\n    for L, beta, stat_type in test_cases:\n        N = L * L\n        \n        # 1. Enumerate configurations to get the exact joint DOS g(E,M)\n        # g_em is a dictionary mapping (E, M) tuples to their counts.\n        g_em = {}\n        num_configs = 1  N  # This is 2**N\n\n        for i in range(num_configs):\n            # Generate spin configuration from integer i\n            # bit 0 -> spin -1, bit 1 -> spin +1\n            spins_1d = np.array([1 if (i >> k)  1 else -1 for k in range(N)])\n            spins_2d = spins_1d.reshape((L, L))\n            \n            # Calculate Magnetization M\n            M = np.sum(spins_1d)\n            \n            # Calculate Energy E with periodic boundary conditions.\n            # Summing interaction of each spin with its 'right' and 'down'\n            # neighbor counts each bond exactly once.\n            # np.roll(..., -1, ...) shifts elements to the left/up, so\n            # s_grid[i,j] is multiplied by s_grid[i+1,j] and s_grid[i,j+1]\n            E = -np.sum(spins_2d * (np.roll(spins_2d, -1, axis=0) + \n                                   np.roll(spins_2d, -1, axis=1)))\n\n            # Store in g(E,M) dictionary, using integer keys\n            key = (int(E), int(M))\n            g_em[key] = g_em.get(key, 0) + 1\n\n        # 2. Reweight from g(E,M) to get the canonical distribution P_beta(M)\n        # Find the set of all unique magnetization values\n        M_values = sorted(list(set(m for e, m in g_em.keys())))\n        \n        # Calculate unnormalized probabilities and the partition function Z\n        P_M_unnormalized = {m: 0.0 for m in M_values}\n        Z = 0.0\n\n        for (E, M), count in g_em.items():\n            # Use np.longdouble for precision with large exponentials\n            boltzmann_factor = np.exp(np.longdouble(-beta * E))\n            term = count * boltzmann_factor\n            P_M_unnormalized[M] += term\n            Z += term\n\n        # Normalize to get P_beta(M)\n        if Z == 0.0:\n            # This case should not be reached for finite beta\n            P_M = {m: 0.0 for m in M_values}\n        else:\n            P_M = {m: p / Z for m, p in P_M_unnormalized.items()}\n\n        # 3. Compute the requested statistic\n        if stat_type == \"P(M=0)\":\n            # P_beta(M=0)\n            result = P_M.get(0, 0.0)\n        elif stat_type == \"|m|>\":\n            # |m|> = |M/L^2|>\n            mean_abs_M = sum(abs(m) * p for m, p in P_M.items())\n            result = mean_abs_M / N\n        else:\n            raise ValueError(\"Unknown statistic type\")\n\n        results.append(result)\n\n    # Final print statement in the exact required format\n    final_results_str = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(final_results_str)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "For systems with vast energy landscapes or significant free energy barriers, a single Wang-Landau random walk may struggle to sample the entire space effectively. A robust and scalable strategy is to partition the energy range into overlapping windows and run independent simulations in each. This advanced practice explores the statistically rigorous methods required to \"stitch\" the resulting logarithmic density of states, $\\ln g(E)$, from each window into a single, globally consistent function. ",
            "id": "5305881",
            "problem": "Consider a classical many-body system with microstates labeled by an energy variable $E$ taking values on a discrete grid $\\{E_i\\}_{i=1}^{M}$ spanning an interval $[E_{\\min},E_{\\max}]$. The density of states (DOS) $g(E)$ counts the number of microstates at energy $E$, and the canonical partition function at inverse temperature $\\beta$ is $Z(\\beta)=\\sum_{i=1}^{M} g(E_i)\\,\\mathrm{e}^{-\\beta E_i}$. In Wang-Landau (WL) density-of-states sampling, one performs a random walk in energy space while iteratively modifying an estimator $\\hat g(E)$ to achieve a flat histogram in $E$; once the histogram is sufficiently flat by a chosen criterion, the modification factor is reduced and the process continues until convergence. Because the WL estimator within a restricted energy domain is determined only up to a multiplicative constant, the logarithm $\\ln \\hat g(E)$ within a subdomain is known only up to an additive constant.\n\nTo make the computation tractable in large systems, one partitions the energy range into $K$ overlapping windows $W_k=[E_k^{-},E_k^{+}]$ such that $E_1^{-}=E_{\\min}$, $E_K^{+}=E_{\\max}$, and for each $k$ the overlap with its neighbor is nonempty: $O_{k,k+1}=W_k\\cap W_{k+1}=[E_k^{+}-\\Delta,\\,E_{k+1}^{-}+\\Delta]$ with $\\Delta0$. In each window $W_k$, one performs Wang-Landau (WL) sampling confined to $W_k$ (using a boundary treatment that prevents bias from edge visits) to obtain an estimator $\\ln \\hat g_k(E)$ for $E\\in W_k$. Assume that for each discrete energy $E\\in W_k$, the WL estimator yields $\\ln \\hat g_k(E)=\\ln g(E)+c_k+\\epsilon_k(E)$, where $c_k$ is an unknown window-dependent additive constant and $\\epsilon_k(E)$ is a zero-mean error with variance $\\sigma_k^2(E)$ that is approximately Gaussian and uncorrelated across windows. The goal is to combine the windowed estimates into a single global estimate $\\ln \\hat g(E)$ on $[E_{\\min},E_{\\max}]$ with minimal bias and quantified uncertainty.\n\nWhich of the following procedures is a statistically consistent way to divide the energy range into windows, run WL in each window, and stitch the resulting $\\ln \\hat g_k(E)$ across overlaps to obtain a global $\\ln \\hat g(E)$?\n\nA. Choose windows with substantial overlaps (e.g., overlap width larger than typical local energy space correlation length), confine WL random walks within each window using reflective boundaries to avoid boundary-induced bias, and require a stringent flatness criterion for the energy histogram within each window. In each overlap $O_{k,k+1}$, determine the additive constants $c_k$ by minimizing the weighted sum of squared differences $\\sum_{E\\in O_{k,k+1}} w(E)\\left[\\ln \\hat g_k(E)-\\ln \\hat g_{k+1}(E)+c_k-c_{k+1}\\right]^2$ with weights $w(E)=\\left[\\sigma_k^2(E)+\\sigma_{k+1}^2(E)\\right]^{-1}$, fixing the global gauge by setting $c_1=0$ or by imposing a normalization. Then construct the global $\\ln \\hat g(E)$ in overlaps by inverse-variance weighted averaging and outside overlaps by the uniquely available window estimate.\n\nB. Use non-overlapping windows to avoid redundant sampling and confine WL random walks by rejecting any proposed moves that cross window boundaries. At stitching, set $c_k$ by matching the single boundary value at $E_k^{+}$, i.e., choose $c_{k+1}$ such that $\\ln \\hat g_k(E_k^{+})+c_k=\\ln \\hat g_{k+1}(E_k^{+})+c_{k+1}$, and then concatenate the window estimates without further averaging.\n\nC. Employ overlapping windows, but instead of aligning $\\ln \\hat g_k(E)$, enforce continuity of the microcanonical inverse temperature $\\beta_{\\mathrm{micro}}(E)=\\mathrm{d}\\ln g(E)/\\mathrm{d}E$ across overlaps by fitting a smooth function to estimates of $\\mathrm{d}\\ln \\hat g_k(E)/\\mathrm{d}E$ and adjusting the stitched $\\ln \\hat g(E)$ so that the derivative is continuous. Rely on global polynomial smoothing to set the additive constants across windows.\n\nD. Determine the window constants $c_k$ by requiring that the integrated DOS over each window equals a common value, i.e., enforce $\\sum_{E\\in W_k} \\mathrm{e}^{\\ln \\hat g_k(E)+c_k} = \\text{constant}$ for all $k$, thereby equalizing the normalizations across windows. Use equal-weight averaging in overlaps and reject boundary moves to prevent edge artifacts.\n\nE. In overlapping windows, estimate $c_k$ by matching the canonical energy probability distributions $P_k(E;\\beta)=\\mathrm{e}^{\\ln \\hat g_k(E)+c_k-\\beta E}/Z_k(\\beta)$ at a single fixed inverse temperature $\\beta^\\star$ across each overlap, i.e., choose constants so that $P_k(E;\\beta^\\star)=P_{k+1}(E;\\beta^\\star)$ for all $E\\in O_{k,k+1}$, and use these constants to define the global $\\ln \\hat g(E)$.\n\nSelect all options that are correct and provide a justification grounded in first principles of density-of-states estimation, statistical weighting, and Wang-Landau sampling. Your reasoning should address window design, boundary treatment, estimation of additive constants, variance-aware stitching, and why alternative procedures may introduce bias or fail to control uncertainty.",
            "solution": "The user has requested a critical validation of the problem statement, followed by a detailed solution and evaluation of the provided options.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   System: A classical many-body system.\n-   Energy variable: $E$, taking values on a discrete grid $\\{E_i\\}_{i=1}^{M}$ in the interval $[E_{\\min},E_{\\max}]$.\n-   Density of States (DOS): $g(E)$.\n-   Canonical Partition Function: $Z(\\beta)=\\sum_{i=1}^{M} g(E_i)\\,\\mathrm{e}^{-\\beta E_i}$ at inverse temperature $\\beta$.\n-   Method: Wang-Landau (WL) sampling.\n-   WL Estimator Property: For a restricted energy domain, $\\ln \\hat g(E)$ is determined only up to an additive constant.\n-   Windowing: The energy range is partitioned into $K$ overlapping windows $W_k=[E_k^{-},E_k^{+}]$ for $k=1, \\dots, K$.\n-   Window Setup: $E_1^{-}=E_{\\min}$, $E_K^{+}=E_{\\max}$.\n-   Overlap Regions: $O_{k,k+1}=W_k\\cap W_{k+1}=[E_k^{+}-\\Delta,\\,E_{k+1}^{-}+\\Delta]$ with $\\Delta0$.\n-   Window Estimate Model: For $E\\in W_k$, the WL estimator is $\\ln \\hat g_k(E)=\\ln g(E)+c_k+\\epsilon_k(E)$.\n-   Model Parameters: $c_k$ is an unknown, window-dependent additive constant. $\\epsilon_k(E)$ is a zero-mean error, approximately Gaussian, with variance $\\sigma_k^2(E)$, and is uncorrelated across different windows.\n-   Objective: Combine the windowed estimates $\\ln \\hat g_k(E)$ into a single global estimate $\\ln \\hat g(E)$ on $[E_{\\min},E_{\\max}]$ with minimal bias and quantified uncertainty.\n-   Question: Identify the statistically consistent procedure among the options provided.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is assessed against the required criteria:\n-   **Scientifically Grounded**: Yes. The problem is set firmly within the domain of computational statistical physics. Wang-Landau sampling, density of states, windowing techniques for parallelization, and the statistical challenge of stitching results are all standard and well-established concepts in this field. The provided statistical model for the estimator error is a standard and appropriate simplification used in data analysis of such simulation results.\n-   **Well-Posed**: Yes. The problem provides a clear objective (obtaining a global DOS estimate from partial ones) and a clear statistical model for the data. The question asks for a 'statistically consistent' procedure, which is a well-defined goal in statistics, requiring an approach that is methodologically sound and minimizes bias and variance.\n-   **Objective**: Yes. The language is formal, precise, and free of subjective, ambiguous, or opinion-based statements. It uses standard scientific terminology.\n\nThere are no identifiable flaws. The problem does not violate scientific principles, is formalizable, is self-contained, and describes a realistic and non-trivial task in computational physics.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. Proceeding to the solution.\n\n**Derivation and Option Analysis**\n\nThe core of the problem is to determine the unknown relative additive constants between the windowed log-DOS estimates, $\\ln \\hat g_k(E)$. The provided model is $\\ln \\hat g_k(E) = \\ln g(E) + c_k + \\epsilon_k(E)$, where $c_k$ is the unknown bias for window $k$ and $\\epsilon_k(E)$ is a zero-mean noise term with variance $\\sigma_k^2(E)$.\n\nThe key to finding the relative constants lies in the overlap regions $O_{k,k+1}$, where we have two independent estimates for the same true underlying function $\\ln g(E)$. For any energy $E \\in O_{k,k+1}$, we have:\n$$ \\ln \\hat g_k(E) = \\ln g(E) + c_k + \\epsilon_k(E) $$\n$$ \\ln \\hat g_{k+1}(E) = \\ln g(E) + c_{k+1} + \\epsilon_{k+1}(E) $$\nSubtracting these two equations eliminates the unknown true value $\\ln g(E)$:\n$$ \\ln \\hat g_k(E) - \\ln \\hat g_{k+1}(E) = (c_k - c_{k+1}) + (\\epsilon_k(E) - \\epsilon_{k+1}(E)) $$\nLet $Y(E) = \\ln \\hat g_k(E) - \\ln \\hat g_{k+1}(E)$ be our measurement, and let $\\delta_{k,k+1} = c_k - c_{k+1}$ be the unknown parameter we want to estimate. The equation is of the form $Y(E) = \\delta_{k,k+1} + \\eta(E)$, where the error term is $\\eta(E) = \\epsilon_k(E) - \\epsilon_{k+1}(E)$. Since $\\epsilon_k(E)$ and $\\epsilon_{k+1}(E)$ are independent and have zero mean, the mean of $\\eta(E)$ is $E[\\eta(E)] = 0$. Its variance is $\\text{Var}(\\eta(E)) = \\text{Var}(\\epsilon_k(E)) + \\text{Var}(\\epsilon_{k+1}(E)) = \\sigma_k^2(E) + \\sigma_{k+1}^2(E)$.\n\nWe have multiple measurements $Y(E)$ for each $E$ in the overlap region $O_{k,k+1}$ to estimate the single constant $\\delta_{k,k+1}$. According to the Gauss-Markov theorem, the Best Linear Unbiased Estimator (BLUE) for $\\delta_{k,k+1}$ is found by minimizing the weighted sum of squared residuals, where the weights are the inverse of the variance of each measurement. This corresponds to a weighted least-squares fit. The function to minimize is:\n$$ S(\\delta_{k,k+1}) = \\sum_{E \\in O_{k,k+1}} \\frac{1}{\\sigma_k^2(E) + \\sigma_{k+1}^2(E)} \\left( Y(E) - \\delta_{k,k+1} \\right)^2 $$\n$$ S(\\delta_{k,k+1}) = \\sum_{E \\in O_{k,k+1}} \\frac{1}{\\sigma_k^2(E) + \\sigma_{k+1}^2(E)} \\left( (\\ln \\hat g_k(E) - \\ln \\hat g_{k+1}(E)) - (c_k - c_{k+1}) \\right)^2 $$\nThis procedure optimally determines the relative offsets. To fix the absolute values of the constants $c_k$, one must impose an overall constraint, such as setting one constant to zero (e.g., $c_1=0$, which is a gauge choice) or applying some global normalization.\n\nOnce the constants are determined, the best estimate for $\\ln g(E)$ in an overlap region is the inverse-variance weighted average of the aligned estimates. In a non-overlap region belonging only to window $W_k$, the best estimate is simply the aligned estimate from that window. This entire procedure is statistically robust and makes optimal use of the available data and error model.\n\nNow we evaluate each option based on these principles.\n\n**A. Choose windows with substantial overlaps (e.g., overlap width larger than typical local energy space correlation length), confine WL random walks within each window using reflective boundaries to avoid boundary-induced bias, and require a stringent flatness criterion for the energy histogram within each window. In each overlap $O_{k,k+1}$, determine the additive constants $c_k$ by minimizing the weighted sum of squared differences $\\sum_{E\\in O_{k,k+1}} w(E)\\left[\\ln \\hat g_k(E)-\\ln \\hat g_{k+1}(E)+c_k-c_{k+1}\\right]^2$ with weights $w(E)=\\left[\\sigma_k^2(E)+\\sigma_{k+1}^2(E)\\right]^{-1}$, fixing the global gauge by setting $c_1=0$ or by imposing a normalization. Then construct the global $\\ln \\hat g(E)$ in overlaps by inverse-variance weighted averaging and outside overlaps by the uniquely available window estimate.**\n\nThis option describes the statistically optimal procedure derived above.\n-   It correctly identifies the need for substantial overlaps for a good statistical basis for matching.\n-   It specifies reflective boundaries, a valid method for handling window edges without introducing the bias characteristic of simple rejection schemes.\n-   It proposes minimizing a weighted sum of squared differences to find the constants. The expression to be minimized is $\\sum w(E)[(\\ln \\hat g_k(E)-\\ln \\hat g_{k+1}(E))-(c_{k+1}-c_k)]^2$, which perfectly matches the weighted least-squares formulation for estimating the difference $c_{k+1}-c_k$.\n-   It uses the correct inverse-variance weights $w(E)=\\left[\\sigma_k^2(E)+\\sigma_{k+1}^2(E)\\right]^{-1}$.\n-   It correctly identifies the need for a gauge fixing (e.g., $c_1=0$).\n-   It employs the optimal inverse-variance weighted averaging for combining the data in the overlap regions.\nEvery step is statistically sound and consistent with best practices.\n**Verdict: Correct**\n\n**B. Use non-overlapping windows to avoid redundant sampling and confine WL random walks by rejecting any proposed moves that cross window boundaries. At stitching, set $c_k$ by matching the single boundary value at $E_k^{+}$, i.e., choose $c_{k+1}$ such that $\\ln \\hat g_k(E_k^{+})+c_k=\\ln \\hat g_{k+1}(E_k^{+})+c_{k+1}$, and then concatenate the window estimates without further averaging.**\n\nThis option is deeply flawed.\n-   Non-overlapping windows prevent a robust statistical determination of the relative constants. There is no region of redundant data to average over to reduce noise.\n-   Matching is based on a single, noisy data point at the boundary, $\\ln \\hat g_k(E_k^{+})$. This is statistically fragile and highly susceptible to fluctuations.\n-   Rejecting moves at boundaries is known to introduce systematic bias. A random walk that is prevented from leaving a region will oversample states at the boundary, artificially increasing their estimated density of states.\n**Verdict: Incorrect**\n\n**C. Employ overlapping windows, but instead of aligning $\\ln \\hat g_k(E)$, enforce continuity of the microcanonical inverse temperature $\\beta_{\\mathrm{micro}}(E)=\\mathrm{d}\\ln g(E)/\\mathrm{d}E$ across overlaps by fitting a smooth function to estimates of $\\mathrm{d}\\ln \\hat g_k(E)/\\mathrm{d}E$ and adjusting the stitched $\\ln \\hat g(E)$ so that the derivative is continuous. Rely on global polynomial smoothing to set the additive constants across windows.**\n\nThis option is based on a physically correct principle but proposes a flawed implementation.\n-   The derivative of the log-DOS, $\\beta_{\\mathrm{micro}}(E)$, should indeed be continuous. However, numerically differentiating noisy data, such as $\\ln \\hat g_k(E)$, significantly amplifies the noise. Basing the procedure on these highly noisy derivative estimates is statistically unstable compared to using the smoother, integrated data $\\ln \\hat g_k(E)$.\n-   Crucially, the constants $c_k$ are additive: $\\ln \\hat g_k(E) = \\ln g(E) + c_k + \\epsilon_k(E)$. The derivative is $\\mathrm{d}\\ln\\hat g_k(E)/\\mathrm{d}E = \\mathrm{d}\\ln g(E)/\\mathrm{d}E + \\mathrm{d}\\epsilon_k(E)/\\mathrm{d}E$. The constants $c_k$ disappear. Therefore, enforcing continuity of the derivative *cannot* be used to determine the relative values of the $c_k$. The statement that this procedure \"set[s] the additive constants\" is incorrect. One must still integrate the resulting $\\beta(E)$ and fix the integration constants, which brings us back to the original problem of matching the function values.\n**Verdict: Incorrect**\n\n**D. Determine the window constants $c_k$ by requiring that the integrated DOS over each window equals a common value, i.e., enforce $\\sum_{E\\in W_k} \\mathrm{e}^{\\ln \\hat g_k(E)+c_k} = \\text{constant}$ for all $k$, thereby equalizing the normalizations across windows. Use equal-weight averaging in overlaps and reject boundary moves to prevent edge artifacts.**\n\nThis option is based on a physically baseless assumption.\n-   The core premise is to enforce $\\int_{W_k} g(E) dE = \\text{constant}$ for all windows $k$. The density of states $g(E)$ typically grows super-exponentially with energy for many-body systems. A window at higher energy will contain orders of magnitude more states than a window at lower energy. Forcing the integrated DOS to be equal across windows is a violation of the underlying physics and will introduce gross systematic errors.\n-   It proposes equal-weight averaging in overlaps, which is statistically suboptimal as it ignores the heteroscedasticity (unequal variances) of the data.\n-   It proposes rejecting boundary moves, which, as noted for option B, introduces bias.\n**Verdict: Incorrect**\n\n**E. In overlapping windows, estimate $c_k$ by matching the canonical energy probability distributions $P_k(E;\\beta)=\\mathrm{e}^{\\ln \\hat g_k(E)+c_k-\\beta E}/Z_k(\\beta)$ at a single fixed inverse temperature $\\beta^\\star$ across each overlap, i.e., choose constants so that $P_k(E;\\beta^\\star)=P_{k+1}(E;\\beta^\\star)$ for all $E\\in O_{k,k+1}$, and use these constants to define the global $\\ln \\hat g(E)$.**\n\nThis option describes a procedure that is mathematically incapable of achieving its stated goal. The probability distribution in window $k$ at temperature $\\beta^\\star$, using the provided estimates, is:\n$$ P_k(E;\\beta^\\star) = \\frac{\\mathrm{e}^{\\ln \\hat g_k(E)+c_k-\\beta^\\star E}}{Z_k(\\beta^\\star)} $$\nThe partition function $Z_k(\\beta^\\star)$ is the normalization constant, calculated by summing over all states within window $W_k$:\n$$ Z_k(\\beta^\\star) = \\sum_{E' \\in W_k} \\mathrm{e}^{\\ln \\hat g_k(E')+c_k-\\beta^\\star E'} = \\mathrm{e}^{c_k} \\sum_{E' \\in W_k} \\mathrm{e}^{\\ln \\hat g_k(E')-\\beta^\\star E'} $$\nSubstituting this into the expression for $P_k(E;\\beta^\\star)$:\n$$ P_k(E;\\beta^\\star) = \\frac{\\mathrm{e}^{c_k} \\mathrm{e}^{\\ln \\hat g_k(E)-\\beta^\\star E}}{\\mathrm{e}^{c_k} \\sum_{E' \\in W_k} \\mathrm{e}^{\\ln \\hat g_k(E')-\\beta^\\star E'}} = \\frac{\\mathrm{e}^{\\ln \\hat g_k(E)-\\beta^\\star E}}{\\sum_{E' \\in W_k} \\mathrm{e}^{\\ln \\hat g_k(E')-\\beta^\\star E'}} $$\nThe constant $c_k$ cancels completely from the expression for the normalized probability distribution $P_k$. The same applies to $P_{k+1}$. Therefore, the condition $P_k(E;\\beta^\\star)=P_{k+1}(E;\\beta^\\star)$ is an equation that does not contain the unknown constants $c_k$ and $c_{k+1}$. It is thus impossible to solve this equation for them. The procedure is ill-conceived.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}