{
    "hands_on_practices": [
        {
            "introduction": "A high-fidelity interatomic potential must accurately predict not only a material's energy and atomic forces but also its mechanical response to deformation. This exercise establishes the crucial link between the quantum mechanical energy calculated by DFT and the macroscopic Cauchy stress tensor. By working through the derivation, you will understand how to generate stress data from first principles and incorporate it into a comprehensive objective function for fitting, leading to potentials that are reliable for mechanical simulations.",
            "id": "3828947",
            "problem": "Consider a periodic crystalline system modeled by Density Functional Theory (DFT), with total ground-state energy $E(\\{\\mathbf{R}_{\\alpha}\\}, \\mathbf{h})$ depending on the set of atomic Cartesian positions $\\{\\mathbf{R}_{\\alpha}\\}$ and lattice matrix $\\mathbf{h}$, whose columns are the Bravais lattice vectors. Assume zero temperature and a fixed number of electrons, and consider an infinitesimal, homogeneous, symmetric, small strain $\\boldsymbol{\\epsilon}$ applied to the cell, such that $\\mathbf{h} \\mapsto (\\mathbf{I} + \\boldsymbol{\\epsilon}) \\mathbf{h}$ while holding the fractional coordinates $\\{\\mathbf{s}_{\\alpha}\\}$ fixed, with $\\mathbf{R}_{\\alpha} = \\mathbf{h}\\,\\mathbf{s}_{\\alpha}$. Let the volume be $V = \\det(\\mathbf{h})$, and suppose the basis is complete so that Pulay contributions can be neglected, and the configuration is stationary with respect to atomic positions so that $\\mathbf{F}_{\\alpha} = -\\partial E/\\partial \\mathbf{R}_{\\alpha} = \\mathbf{0}$.\n\nStarting from the virtual work principle for small homogeneous deformations and standard variational definitions in continuum mechanics, derive an explicit analytical expression for the Cauchy stress tensor components $\\sigma_{ij}$ in terms of derivatives of the DFT energy with respect to the components of the infinitesimal strain tensor $\\epsilon_{ij}$. Clearly state the assumptions under which this expression holds.\n\nThen, within the context of constructing an interatomic potential parameterized by a vector of parameters $\\boldsymbol{\\theta}$ to reproduce DFT data across a training set $\\mathcal{D}$ of configurations $c$, formulate a dimensionless least-squares objective function that simultaneously fits energies, forces, and stresses. Let each configuration $c \\in \\mathcal{D}$ have $N_c$ atoms, volume $V_c$, DFT reference energy $E^{\\mathrm{DFT}}(c)$, forces $\\{\\mathbf{F}^{\\mathrm{DFT}}_{\\alpha}(c)\\}_{\\alpha=1}^{N_c}$, and stress tensor $\\boldsymbol{\\sigma}^{\\mathrm{DFT}}(c)$. The model predicts $E_{\\boldsymbol{\\theta}}(c)$, $\\{\\mathbf{F}_{\\boldsymbol{\\theta},\\alpha}(c)\\}$, and $\\boldsymbol{\\sigma}_{\\boldsymbol{\\theta}}(c)$. Use positive scalar weights $w_E$, $w_F$, $w_{\\sigma}$ and characteristic scales $E_0$ (energy per atom), $F_0$ (force), and $\\sigma_0$ (stress) to ensure the terms are commensurate and dimensionless, and use the Frobenius norm for stress misfit. Express energy misfit on a per-atom basis to avoid bias with respect to system size, and average force misfit over atoms and Cartesian components.\n\nProvide the final expressions for $\\sigma_{ij}$ and the least-squares objective in closed-form symbolic notation. Express the final expressions symbolically; do not include units and do not perform any numerical rounding.",
            "solution": "The user has presented a two-part problem concerning the calculation of stress from Density Functional Theory (DFT) and the formulation of an objective function for parameterizing interatomic potentials. The problem statement is scientifically sound, well-posed, objective, and contains sufficient information to derive the requested expressions. It is therefore deemed valid.\n\n### Part 1: Derivation of the Cauchy Stress Tensor\n\nThe derivation begins with the principle of virtual work, which equates the change in the internal energy $dE$ of a system to the mechanical work $dW$ done on it by external forces during an infinitesimal deformation. For a continuous medium of volume $V$ subjected to an infinitesimal strain $d\\boldsymbol{\\epsilon}$, the work done by the Cauchy stress tensor $\\boldsymbol{\\sigma}$ is given by:\n$$\ndW = V (\\boldsymbol{\\sigma}:d\\boldsymbol{\\epsilon}) = V \\sum_{i,j=1}^3 \\sigma_{ij} d\\epsilon_{ij}\n$$\nThe change in the total ground-state energy $E$ of the periodic system due to this strain is:\n$$\ndE = \\sum_{i,j=1}^3 \\frac{dE}{d\\epsilon_{ij}} d\\epsilon_{ij}\n$$\nEquating the work and the energy change ($dE = dW$) yields:\n$$\n\\sum_{i,j=1}^3 \\frac{dE}{d\\epsilon_{ij}} d\\epsilon_{ij} = V \\sum_{i,j=1}^3 \\sigma_{ij} d\\epsilon_{ij}\n$$\nThe total derivative of the energy $E$ with respect to a component of the strain tensor $\\epsilon_{kl}$ must be evaluated using the chain rule. The energy $E$ is an explicit function of the lattice matrix $\\mathbf{h}$ and the atomic positions $\\{\\mathbf{R}_\\alpha\\}$, which in turn depend on the applied strain $\\boldsymbol{\\epsilon}$.\n$$\n\\frac{dE}{d\\epsilon_{kl}} = \\sum_{i,j=1}^3 \\frac{\\partial E}{\\partial h_{ij}} \\frac{\\partial h_{ij}}{\\partial \\epsilon_{kl}} + \\sum_{\\alpha=1}^N \\sum_{i=1}^3 \\frac{\\partial E}{\\partial R_{\\alpha,i}} \\frac{\\partial R_{\\alpha,i}}{\\partial \\epsilon_{kl}}\n$$\nThe problem specifies two key assumptions. First, the atomic configuration is stationary, meaning the forces on the atoms are zero in the initial unstrained state:\n$$\nF_{\\alpha,i} = -\\frac{\\partial E}{\\partial R_{\\alpha,i}} = 0\n$$\nThis causes the second term in the chain rule expansion (the internal work term) to vanish. Second, Pulay stress contributions, which arise from the change in the basis set with respect to lattice deformation, are negligible, as stated by the assumption of a complete basis.\n\nThe problem remains to evaluate the derivative $\\partial h_{ij}/\\partial \\epsilon_{kl}$. The lattice matrix transforms as $\\mathbf{h}' = (\\mathbf{I} + \\boldsymbol{\\epsilon})\\mathbf{h}$, or in component form:\n$$\nh'_{ij} = \\sum_m (\\delta_{im} + \\epsilon_{im}) h_{mj}\n$$\nThe derivative of a component of the new lattice matrix $h'_{ij}$ with respect to a strain component $\\epsilon_{kl}$ is:\n$$\n\\frac{\\partial h'_{ij}}{\\partial \\epsilon_{kl}} = \\frac{\\partial}{\\partial \\epsilon_{kl}} \\left( \\sum_m (\\delta_{im} + \\epsilon_{im}) h_{mj} \\right) = \\sum_m \\frac{\\partial \\epsilon_{im}}{\\partial \\epsilon_{kl}} h_{mj} = \\sum_m \\delta_{ik}\\delta_{ml} h_{mj} = \\delta_{ik}h_{lj}\n$$\nSubstituting this back into the simplified expression for the energy derivative (with zero forces):\n$$\n\\frac{dE}{d\\epsilon_{kl}} = \\sum_{i,j=1}^3 \\frac{\\partial E}{\\partial h_{ij}} (\\delta_{ik} h_{lj}) = \\sum_{j=1}^3 \\frac{\\partial E}{\\partial h_{kj}} h_{lj}\n$$\nNow we return to the work-energy balance. The strain tensor $\\boldsymbol{\\epsilon}$ and the Cauchy stress tensor $\\boldsymbol{\\sigma}$ are both symmetric, i.e., $\\epsilon_{ij} = \\epsilon_{ji}$ and $\\sigma_{ij} = \\sigma_{ji}$. We must account for this when relating the coefficients. For arbitrary independent variations $d\\epsilon_{ij}$ where $i \\le j$:\nThe change in energy $dE$ is:\n$$\ndE = \\sum_i \\frac{dE}{d\\epsilon_{ii}} d\\epsilon_{ii} + \\sum_{i<j} \\left(\\frac{dE}{d\\epsilon_{ij}} + \\frac{dE}{d\\epsilon_{ji}}\\right) d\\epsilon_{ij}\n$$\nThe work done $dW$ is:\n$$\ndW = V \\left( \\sum_i \\sigma_{ii} d\\epsilon_{ii} + \\sum_{i<j} (\\sigma_{ij} + \\sigma_{ji}) d\\epsilon_{ij} \\right) = V \\left( \\sum_i \\sigma_{ii} d\\epsilon_{ii} + \\sum_{i<j} 2 \\sigma_{ij} d\\epsilon_{ij} \\right)\n$$\nBy comparing the coefficients of the independent variations $d\\epsilon_{ij}$ in $dE=dW$, we find:\nFor diagonal components ($i=j$):\n$$\n\\frac{dE}{d\\epsilon_{ii}} = V \\sigma_{ii} \\implies \\sigma_{ii} = \\frac{1}{V} \\frac{dE}{d\\epsilon_{ii}}\n$$\nFor off-diagonal components ($i \\ne j$):\n$$\n\\frac{dE}{d\\epsilon_{ij}} + \\frac{dE}{d\\epsilon_{ji}} = 2V \\sigma_{ij} \\implies \\sigma_{ij} = \\frac{1}{2V} \\left( \\frac{dE}{d\\epsilon_{ij}} + \\frac{dE}{d\\epsilon_{ji}} \\right)\n$$\nA single expression encompassing both cases can be written as:\n$$\n\\sigma_{ij} = \\frac{1}{V(1+\\delta_{ij})} \\left( \\frac{dE}{d\\epsilon_{ij}} + \\frac{dE}{d\\epsilon_{ji}} \\right)\n$$\nSubstituting the expression for $dE/d\\epsilon_{kl}$:\n$$\n\\sigma_{ij} = \\frac{1}{2V} \\left( \\sum_{m=1}^3 \\frac{\\partial E}{\\partial h_{im}} h_{jm} + \\sum_{m=1}^3 \\frac{\\partial E}{\\partial h_{jm}} h_{im} \\right)\n$$\nThis expression is valid under the assumptions of:\n1.  Infinitesimal, homogeneous strain at zero temperature.\n2.  The initial atomic configuration is fully relaxed, such that all interatomic forces are zero ($\\mathbf{F}_{\\alpha} = \\mathbf{0}$).\n3.  The basis set is complete, such that Pulay stress contributions are zero.\n\n### Part 2: Formulation of the Least-Squares Objective Function\n\nThe objective is to construct a dimensionless least-squares loss function $\\mathcal{L}(\\boldsymbol{\\theta})$ for fitting a parameterized interatomic potential to a dataset $\\mathcal{D}$ of DFT reference configurations. The total loss is a weighted sum of the loss contributions from energy, forces, and stresses: $\\mathcal{L}(\\boldsymbol{\\theta}) = w_E L_E + w_F L_F + w_{\\sigma} L_{\\sigma}$. Following the problem's requirements, each term is constructed as follows.\n\nThe energy loss term, $L_E$, is based on the per-atom energy error, averaged over all configurations in the dataset. For a given configuration $c \\in \\mathcal{D}$ with $N_c$ atoms, the squared residual, made dimensionless with the characteristic energy-per-atom scale $E_0$, is $\\left( \\frac{(E_{\\boldsymbol{\\theta}}(c) - E^{\\mathrm{DFT}}(c))/N_c}{E_0} \\right)^2$. Averaging over the dataset of size $|\\mathcal{D}|$:\n$$\nL_E = \\frac{1}{|\\mathcal{D}|} \\sum_{c \\in \\mathcal{D}} \\left( \\frac{E_{\\boldsymbol{\\theta}}(c) - E^{\\mathrm{DFT}}(c)}{N_c E_0} \\right)^2\n$$\nThe force loss term, $L_F$, is averaged over all atoms and all Cartesian components. For a configuration $c$, the sum of squared errors over all $3N_c$ components is $\\sum_{\\alpha=1}^{N_c} ||\\mathbf{F}_{\\boldsymbol{\\theta},\\alpha}(c) - \\mathbf{F}^{\\mathrm{DFT}}_{\\alpha}(c)||_2^2$. The average squared error is this sum divided by $3N_c$. This is made dimensionless with the characteristic force scale $F_0$. Averaging this per-configuration loss over the dataset:\n$$\nL_F = \\frac{1}{|\\mathcal{D}|} \\sum_{c \\in \\mathcal{D}} \\left[ \\frac{1}{3 N_c F_0^2} \\sum_{\\alpha=1}^{N_c} \\sum_{i=1}^3 (F_{\\boldsymbol{\\theta},\\alpha,i}(c) - F^{\\mathrm{DFT}}_{\\alpha,i}(c))^2 \\right]\n$$\nThe stress loss term, $L_{\\sigma}$, uses the Frobenius norm of the stress tensor difference, $||\\boldsymbol{\\sigma}_{\\boldsymbol{\\theta}}(c) - \\boldsymbol{\\sigma}^{\\mathrm{DFT}}(c)||_F^2 = \\sum_{i,j=1}^3 (\\sigma_{\\boldsymbol{\\theta},ij}(c) - \\sigma^{\\mathrm{DFT}}_{ij}(c))^2$. This is made dimensionless with the characteristic stress scale $\\sigma_0$. Averaging over the dataset:\n$$\nL_{\\sigma} = \\frac{1}{|\\mathcal{D}|} \\sum_{c \\in \\mathcal{D}} \\left[ \\frac{1}{\\sigma_0^2} \\sum_{i=1}^3 \\sum_{j=1}^3 (\\sigma_{\\boldsymbol{\\theta},ij}(c) - \\sigma^{\\mathrm{DFT}}_{ij}(c))^2 \\right]\n$$\nCombining these terms with their respective weights ($w_E, w_F, w_{\\sigma}$) gives the total objective function. A common $1/|\\mathcal{D}|$ factor can be extracted.\n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{|\\mathcal{D}|} \\sum_{c \\in \\mathcal{D}} \\left[ w_E \\left( \\frac{E_{\\boldsymbol{\\theta}}(c) - E^{\\mathrm{DFT}}(c)}{N_c E_0} \\right)^2 + \\frac{w_F}{3 N_c F_0^2} \\sum_{\\alpha=1}^{N_c} \\sum_{i=1}^3 (F_{\\boldsymbol{\\theta},\\alpha,i}(c) - F^{\\mathrm{DFT}}_{\\alpha,i}(c))^2 + \\frac{w_{\\sigma}}{\\sigma_0^2} \\sum_{i,j=1}^3 (\\sigma_{\\boldsymbol{\\theta},ij}(c) - \\sigma^{\\mathrm{DFT}}_{ij}(c))^2 \\right]\n$$\nThis formulation satisfies all conditions specified in the problem statement.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sigma_{ij} = \\frac{1}{2V} \\left( \\sum_{m=1}^3 \\frac{\\partial E}{\\partial h_{im}} h_{jm} + \\sum_{m=1}^3 \\frac{\\partial E}{\\partial h_{jm}} h_{im} \\right)\n&\n\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{|\\mathcal{D}|} \\sum_{c \\in \\mathcal{D}} \\left[ w_E \\left( \\frac{E_{\\boldsymbol{\\theta}}(c) - E^{\\mathrm{DFT}}(c)}{N_c E_0} \\right)^2 + \\frac{w_F}{3 N_c F_0^2} \\sum_{\\alpha=1}^{N_c} \\sum_{i=1}^3 (F_{\\boldsymbol{\\theta},\\alpha,i}(c) - F^{\\mathrm{DFT}}_{\\alpha,i}(c))^2 + \\frac{w_{\\sigma}}{\\sigma_0^2} \\sum_{i,j=1}^3 (\\sigma_{\\boldsymbol{\\theta},ij}(c) - \\sigma^{\\mathrm{DFT}}_{ij}(c))^2 \\right]\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "When fitting a flexible model to a finite, noisy dataset, a central challenge is the 'bias-variance trade-off': a simple model may be biased (underfit), while a complex one may be too sensitive to noise (overfit). Regularization is the primary tool to navigate this trade-off. This practice offers a quantitative exploration of Tikhonov regularization, allowing you to see firsthand how the regularization strength, $\\lambda$, directly controls model complexity and its ability to generalize to new data.",
            "id": "3828933",
            "problem": "You are given a linearized surrogate for the mapping from an interatomic potential parameter vector to force components computed by Density Functional Theory (DFT) on a set of atomic configurations. The surrogate is modeled as a linear observation model with additive noise: training force components are represented as $y = A \\boldsymbol{\\theta}^\\star + \\boldsymbol{\\varepsilon}$, where $A \\in \\mathbb{R}^{n \\times p}$ is a fixed design matrix of descriptors and their derivatives, $\\boldsymbol{\\theta}^\\star \\in \\mathbb{R}^{p}$ is an unknown true parameter vector of the interatomic potential, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$ is zero-mean noise with independent components of variance $\\sigma^2$. The goal is to parameterize the interatomic potential by estimating $\\boldsymbol{\\theta}$ via Tikhonov regularization, and analyze the bias–variance trade-off in reproducing DFT forces on a separate test set described by $A_{\\text{test}} \\in \\mathbb{R}^{m \\times p}$.\n\nUse the following fixed data (all forces are in $\\text{eV}/\\text{\\AA}$, and all entries are dimensionless unless otherwise specified):\n\n- Training design matrix $A \\in \\mathbb{R}^{8 \\times 4}$:\n  $$\n  A = \\begin{bmatrix}\n  1.2 & -0.7 & 0.3 & 0.0 \\\\\n  0.9 & -0.4 & 0.5 & 0.2 \\\\\n  1.5 & -1.0 & 0.8 & -0.1 \\\\\n  0.7 & -0.2 & 0.1 & 0.3 \\\\\n  1.1 & -0.6 & 0.4 & 0.1 \\\\\n  0.8 & -0.5 & 0.2 & -0.2 \\\\\n  1.3 & -0.9 & 0.7 & 0.0 \\\\\n  0.6 & -0.1 & 0.0 & 0.4\n  \\end{bmatrix}.\n  $$\n- Test design matrix $A_{\\text{test}} \\in \\mathbb{R}^{5 \\times 4}$:\n  $$\n  A_{\\text{test}} = \\begin{bmatrix}\n  1.0 & -0.5 & 0.4 & 0.1 \\\\\n  1.4 & -0.8 & 0.6 & -0.1 \\\\\n  0.95 & -0.45 & 0.35 & 0.15 \\\\\n  0.75 & -0.25 & 0.15 & 0.25 \\\\\n  1.2 & -0.7 & 0.5 & 0.0\n  \\end{bmatrix}.\n  $$\n- True parameter vector $\\boldsymbol{\\theta}^\\star \\in \\mathbb{R}^{4}$:\n  $$\n  \\boldsymbol{\\theta}^\\star = \\begin{bmatrix} -0.4 \\\\ 0.9 \\\\ 0.7 \\\\ -0.2 \\end{bmatrix}.\n  $$\n- Noise variance $\\sigma^2$ (training noise only): $\\sigma^2 = 0.0025$ corresponding to a standard deviation $\\sigma = 0.05$ $\\text{eV}/\\text{\\AA}$.\n\nThe Tikhonov-regularized estimator of $\\boldsymbol{\\theta}$ is defined as the minimizer of a penalized least squares objective with a user-specified penalty matrix $\\Gamma \\in \\mathbb{R}^{p \\times p}$ and prior vector $\\boldsymbol{\\theta}_0 \\in \\mathbb{R}^{p}$. Consider the standard formulation with a nonnegative regularization parameter $\\lambda \\ge 0$.\n\nAssume the test outputs are the noise-free DFT forces $A_{\\text{test}} \\boldsymbol{\\theta}^\\star$ (that is, no test noise). Your task is to compute, for each test case below, the expected per-component mean squared discrepancy between the model predictions at the test set and the noise-free DFT forces, decomposed into bias and variance contributions induced by the training noise through the linear estimator. Formally, if $\\widehat{\\boldsymbol{\\theta}}$ is the Tikhonov-regularized estimator trained on $(A,y)$ with training noise variance $\\sigma^2$, and $\\widehat{\\boldsymbol{y}}_{\\text{test}} = A_{\\text{test}} \\widehat{\\boldsymbol{\\theta}}$, compute\n$$\n\\mathrm{EMSE} = \\frac{1}{m} \\, \\mathbb{E}\\left[ \\left\\| \\widehat{\\boldsymbol{y}}_{\\text{test}} - A_{\\text{test}} \\boldsymbol{\\theta}^\\star \\right\\|_2^2 \\right],\n$$\nexpressed in $(\\text{eV}/\\text{\\AA})^2$. The expectation is with respect to the training noise only. Express the final numerical answers as decimal floats in $(\\text{eV}/\\text{\\AA})^2$, rounded to six decimal places.\n\nTest suite (each case is independent):\n\n- Case $1$ (happy path): $\\lambda = 0.1$, $\\Gamma = I_4$, $\\boldsymbol{\\theta}_0 = \\boldsymbol{0}$.\n- Case $2$ (near-unregularized boundary): $\\lambda = 10^{-8}$, $\\Gamma = I_4$, $\\boldsymbol{\\theta}_0 = \\boldsymbol{0}$.\n- Case $3$ (strong regularization): $\\lambda = 100.0$, $\\Gamma = I_4$, $\\boldsymbol{\\theta}_0 = \\boldsymbol{0}$.\n- Case $4$ (anisotropic prior and penalty): $\\lambda = 1.0$, $\\Gamma = \\mathrm{diag}(1.0, 10.0, 1.0, 0.1)$, $\\boldsymbol{\\theta}_0 = \\begin{bmatrix} -0.3 \\\\ 1.1 \\\\ 0.5 \\\\ -0.15 \\end{bmatrix}$.\n\nYour program must compute the expected per-component mean squared discrepancy for each case using first principles of linear estimation, the definition of Tikhonov regularization, and the bias–variance decomposition under the stated Gaussian noise model.\n\nFinal output format requirement: Your program should produce a single line of output containing the four results, in order of the cases above, as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places (for example, $[0.123456,0.234567,0.345678,0.456789]$), and the implicit unit $(\\text{eV}/\\text{\\AA})^2$ applies to each entry.",
            "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe data and conditions provided in the problem statement are as follows:\n\n-   A linear observation model for training data: $\\boldsymbol{y} = A \\boldsymbol{\\theta}^\\star + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{y} \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{n \\times p}$, $\\boldsymbol{\\theta}^\\star \\in \\mathbb{R}^{p}$, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$.\n-   Number of training data points (force components): $n=8$.\n-   Number of test data points: $m=5$.\n-   Number of parameters: $p=4$.\n-   Training design matrix $A \\in \\mathbb{R}^{8 \\times 4}$:\n    $$\n    A = \\begin{bmatrix}\n    1.2 & -0.7 & 0.3 & 0.0 \\\\\n    0.9 & -0.4 & 0.5 & 0.2 \\\\\n    1.5 & -1.0 & 0.8 & -0.1 \\\\\n    0.7 & -0.2 & 0.1 & 0.3 \\\\\n    1.1 & -0.6 & 0.4 & 0.1 \\\\\n    0.8 & -0.5 & 0.2 & -0.2 \\\\\n    1.3 & -0.9 & 0.7 & 0.0 \\\\\n    0.6 & -0.1 & 0.0 & 0.4\n    \\end{bmatrix}.\n    $$\n-   Test design matrix $A_{\\text{test}} \\in \\mathbb{R}^{5 \\times 4}$:\n    $$\n    A_{\\text{test}} = \\begin{bmatrix}\n    1.0 & -0.5 & 0.4 & 0.1 \\\\\n    1.4 & -0.8 & 0.6 & -0.1 \\\\\n    0.95 & -0.45 & 0.35 & 0.15 \\\\\n    0.75 & -0.25 & 0.15 & 0.25 \\\\\n    1.2 & -0.7 & 0.5 & 0.0\n    \\end{bmatrix}.\n    $$\n-   True parameter vector $\\boldsymbol{\\theta}^\\star \\in \\mathbb{R}^{4}$:\n    $$\n    \\boldsymbol{\\theta}^\\star = \\begin{bmatrix} -0.4 \\\\ 0.9 \\\\ 0.7 \\\\ -0.2 \\end{bmatrix}.\n    $$\n-   Training noise characteristics: $\\boldsymbol{\\varepsilon}$ is zero-mean, $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}$, with independent components of variance $\\sigma^2 = 0.0025$, meaning $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^T] = \\sigma^2 I_n$.\n-   Test outputs are noise-free: $\\boldsymbol{y}_{\\text{test}}^\\star = A_{\\text{test}} \\boldsymbol{\\theta}^\\star$.\n-   The Tikhonov estimator $\\widehat{\\boldsymbol{\\theta}}$ minimizes $J(\\boldsymbol{\\theta}) = \\| A\\boldsymbol{\\theta} - \\boldsymbol{y} \\|_2^2 + \\lambda \\| \\Gamma (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0) \\|_2^2$.\n-   The quantity to compute is the expected per-component mean squared discrepancy on the test set:\n    $$\n    \\mathrm{EMSE} = \\frac{1}{m} \\, \\mathbb{E}\\left[ \\left\\| A_{\\text{test}}\\widehat{\\boldsymbol{\\theta}} - A_{\\text{test}} \\boldsymbol{\\theta}^\\star \\right\\|_2^2 \\right].\n    $$\n-   Test Cases:\n    -   Case $1$: $\\lambda = 0.1$, $\\Gamma = I_4$, $\\boldsymbol{\\theta}_0 = \\boldsymbol{0}$.\n    -   Case $2$: $\\lambda = 10^{-8}$, $\\Gamma = I_4$, $\\boldsymbol{\\theta}_0 = \\boldsymbol{0}$.\n    -   Case $3$: $\\lambda = 100.0$, $\\Gamma = I_4$, $\\boldsymbol{\\theta}_0 = \\boldsymbol{0}$.\n    -   Case $4$: $\\lambda = 1.0$, $\\Gamma = \\mathrm{diag}(1.0, 10.0, 1.0, 0.1)$, $\\boldsymbol{\\theta}_0 = \\begin{bmatrix} -0.3 \\\\ 1.1 \\\\ 0.5 \\\\ -0.15 \\end{bmatrix}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n-   **Scientifically Grounded**: The problem is based on linear regression, Tikhonov regularization, and bias-variance decomposition, which are fundamental and standard concepts in statistics, machine learning, and computational science. The application to parameterizing interatomic potentials is a common and valid practice in multiscale modeling.\n-   **Well-Posed**: The problem is well-posed. The objective function for Tikhonov regularization is strictly convex for $\\lambda > 0$ and a full-rank penalty matrix $\\Gamma$, guaranteeing a unique solution for the estimator $\\widehat{\\boldsymbol{\\theta}}$. The training matrix $A$ has $8$ rows and $4$ columns and can be verified to have full column rank ($p=4$), meaning $A^T A$ is invertible. This ensures that even for $\\lambda \\to 0$, the problem remains well-posed. All necessary numerical values and matrix/vector definitions are provided.\n-   **Objective**: The problem is stated using precise mathematical and scientific language, free of any subjective or ambiguous terminology.\n-   **Incomplete or Contradictory Setup**: The setup is complete and self-contained. All dimensions ($n=8, m=5, p=4$) are consistent across all matrices and vectors. There are no contradictions in the provided data or requirements.\n-   **Unrealistic or Infeasible**: The numerical values are within a realistic range for the described physical quantities (e.g., $\\sigma = 0.05 \\, \\text{eV}/\\text{\\AA}$ is a plausible error for DFT forces). The setup represents a simplified but scientifically plausible scenario.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is a well-defined, scientifically sound, and self-contained problem in linear estimation theory. A solution will be provided.\n\n### Solution Derivation\n\nThe Tikhonov-regularized estimator $\\widehat{\\boldsymbol{\\theta}}$ is found by minimizing the objective function:\n$$\nJ(\\boldsymbol{\\theta}) = (\\boldsymbol{y} - A\\boldsymbol{\\theta})^T (\\boldsymbol{y} - A\\boldsymbol{\\theta}) + \\lambda (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)^T \\Gamma^T \\Gamma (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)\n$$\nTo find the minimum, we compute the gradient with respect to $\\boldsymbol{\\theta}$ and set it to zero:\n$$\n\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} = -2 A^T (\\boldsymbol{y} - A\\boldsymbol{\\theta}) + 2 \\lambda \\Gamma^T \\Gamma (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0) = \\boldsymbol{0}\n$$\nRearranging the terms to solve for $\\boldsymbol{\\theta}$:\n$$\nA^T A \\boldsymbol{\\theta} + \\lambda \\Gamma^T \\Gamma \\boldsymbol{\\theta} = A^T \\boldsymbol{y} + \\lambda \\Gamma^T \\Gamma \\boldsymbol{\\theta}_0\n$$\n$$\n(A^T A + \\lambda \\Gamma^T \\Gamma) \\boldsymbol{\\theta} = A^T \\boldsymbol{y} + \\lambda \\Gamma^T \\Gamma \\boldsymbol{\\theta}_0\n$$\nThis gives the closed-form solution for the estimator $\\widehat{\\boldsymbol{\\theta}}$:\n$$\n\\widehat{\\boldsymbol{\\theta}} = (A^T A + \\lambda \\Gamma^T \\Gamma)^{-1} (A^T \\boldsymbol{y} + \\lambda \\Gamma^T \\Gamma \\boldsymbol{\\theta}_0)\n$$\nThe training data $\\boldsymbol{y}$ depends on the true parameter vector $\\boldsymbol{\\theta}^\\star$ and the random noise $\\boldsymbol{\\varepsilon}$ as $\\boldsymbol{y} = A \\boldsymbol{\\theta}^\\star + \\boldsymbol{\\varepsilon}$. Substituting this into the expression for $\\widehat{\\boldsymbol{\\theta}}$ shows its dependence on the noise:\n$$\n\\widehat{\\boldsymbol{\\theta}} = (A^T A + \\lambda \\Gamma^T \\Gamma)^{-1} (A^T (A \\boldsymbol{\\theta}^\\star + \\boldsymbol{\\varepsilon}) + \\lambda \\Gamma^T \\Gamma \\boldsymbol{\\theta}_0)\n$$\nThe goal is to compute the expected per-component mean squared discrepancy on the test set, $\\mathrm{EMSE} = \\frac{1}{m} \\mathbb{E}[ \\| \\widehat{\\boldsymbol{y}}_{\\text{test}} - \\boldsymbol{y}_{\\text{test}}^\\star \\|_2^2 ]$, where $\\widehat{\\boldsymbol{y}}_{\\text{test}} = A_{\\text{test}}\\widehat{\\boldsymbol{\\theta}}$ and $\\boldsymbol{y}_{\\text{test}}^\\star = A_{\\text{test}}\\boldsymbol{\\theta}^\\star$. The expectation is over the training noise $\\boldsymbol{\\varepsilon}$.\n\nWe use the bias-variance decomposition. The total mean squared error is the sum of the squared bias and the variance:\n$$\n\\mathbb{E}[ \\| \\widehat{\\boldsymbol{y}}_{\\text{test}} - \\boldsymbol{y}_{\\text{test}}^\\star \\|_2^2 ] = \\| \\mathbb{E}[\\widehat{\\boldsymbol{y}}_{\\text{test}}] - \\boldsymbol{y}_{\\text{test}}^\\star \\|_2^2 + \\mathrm{Tr}(\\mathrm{Cov}(\\widehat{\\boldsymbol{y}}_{\\text{test}}))\n$$\n\n**1. Bias Calculation**\nFirst, we find the expectation of the estimator $\\widehat{\\boldsymbol{\\theta}}$. Since $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}$:\n$$\n\\mathbb{E}[\\widehat{\\boldsymbol{\\theta}}] = (A^T A + \\lambda \\Gamma^T \\Gamma)^{-1} (A^T A \\boldsymbol{\\theta}^\\star + \\lambda \\Gamma^T \\Gamma \\boldsymbol{\\theta}_0)\n$$\nThe bias of the estimator is $\\mathrm{Bias}(\\widehat{\\boldsymbol{\\theta}}) = \\mathbb{E}[\\widehat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta}^\\star$. Let $S = A^T A$ and $P = \\lambda \\Gamma^T \\Gamma$.\n$$\n\\mathrm{Bias}(\\widehat{\\boldsymbol{\\theta}}) = (S + P)^{-1} (S \\boldsymbol{\\theta}^\\star + P \\boldsymbol{\\theta}_0) - (S + P)^{-1} (S+P) \\boldsymbol{\\theta}^\\star\n$$\n$$\n\\mathrm{Bias}(\\widehat{\\boldsymbol{\\theta}}) = (S + P)^{-1} (S \\boldsymbol{\\theta}^\\star + P \\boldsymbol{\\theta}_0 - S \\boldsymbol{\\theta}^\\star - P \\boldsymbol{\\theta}^\\star) = (S + P)^{-1} P (\\boldsymbol{\\theta}_0 - \\boldsymbol{\\theta}^\\star)\n$$\nThe bias of the test set prediction is $\\mathrm{Bias}(\\widehat{\\boldsymbol{y}}_{\\text{test}}) = \\mathbb{E}[A_{\\text{test}}\\widehat{\\boldsymbol{\\theta}}] - A_{\\text{test}}\\boldsymbol{\\theta}^\\star = A_{\\text{test}}\\mathrm{Bias}(\\widehat{\\boldsymbol{\\theta}})$.\nThe squared bias contribution to the error is:\n$$\n\\text{Bias}^2 = \\| \\mathrm{Bias}(\\widehat{\\boldsymbol{y}}_{\\text{test}}) \\|_2^2 = \\| A_{\\text{test}} (S + P)^{-1} P (\\boldsymbol{\\theta}_0 - \\boldsymbol{\\theta}^\\star) \\|_2^2\n$$\n\n**2. Variance Calculation**\nThe variance of the estimator $\\widehat{\\boldsymbol{\\theta}}$ is determined by its random component, which comes from the noise term $\\boldsymbol{\\varepsilon}$.\n$$\n\\widehat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\widehat{\\boldsymbol{\\theta}}] = (S+P)^{-1} A^T \\boldsymbol{\\varepsilon}\n$$\nThe covariance matrix of $\\widehat{\\boldsymbol{\\theta}}$ is:\n$$\n\\mathrm{Cov}(\\widehat{\\boldsymbol{\\theta}}) = \\mathbb{E}[(\\widehat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\widehat{\\boldsymbol{\\theta}}])(\\widehat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\widehat{\\boldsymbol{\\theta}}])^T] = (S+P)^{-1} A^T \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^T] A ((S+P)^{-1})^T\n$$\nGiven that $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^T] = \\sigma^2 I_n$ and $(S+P)^{-1}$ is symmetric:\n$$\n\\mathrm{Cov}(\\widehat{\\boldsymbol{\\theta}}) = \\sigma^2 (S+P)^{-1} S (S+P)^{-1}\n$$\nThe covariance of the test set prediction is $\\mathrm{Cov}(\\widehat{\\boldsymbol{y}}_{\\text{test}}) = \\mathrm{Cov}(A_{\\text{test}}\\widehat{\\boldsymbol{\\theta}}) = A_{\\text{test}} \\mathrm{Cov}(\\widehat{\\boldsymbol{\\theta}}) A_{\\text{test}}^T$.\nThe variance contribution to the error is the trace of this matrix:\n$$\n\\text{Variance} = \\mathrm{Tr}(\\mathrm{Cov}(\\widehat{\\boldsymbol{y}}_{\\text{test}})) = \\mathrm{Tr}(A_{\\text{test}} \\mathrm{Cov}(\\widehat{\\boldsymbol{\\theta}}) A_{\\text{test}}^T)\n$$\nUsing the cyclic property of the trace, $\\mathrm{Tr}(ABC) = \\mathrm{Tr}(CAB)$:\n$$\n\\text{Variance} = \\mathrm{Tr}(A_{\\text{test}}^T A_{\\text{test}} \\mathrm{Cov}(\\widehat{\\boldsymbol{\\theta}})) = \\sigma^2 \\mathrm{Tr}(A_{\\text{test}}^T A_{\\text{test}} (S+P)^{-1} S (S+P)^{-1})\n$$\n\n**3. Final EMSE Formula**\nCombining the bias and variance terms, the expected mean squared discrepancy per test component is:\n$$\n\\mathrm{EMSE} = \\frac{1}{m} (\\text{Bias}^2 + \\text{Variance})\n$$\n$$\n\\mathrm{EMSE} = \\frac{1}{m} \\left( \\| A_{\\text{test}} (A^T A + \\lambda \\Gamma^T \\Gamma)^{-1} (\\lambda \\Gamma^T \\Gamma) (\\boldsymbol{\\theta}_0 - \\boldsymbol{\\theta}^\\star) \\|_2^2 + \\sigma^2 \\mathrm{Tr}\\left(A_{\\text{test}}^T A_{\\text{test}} (A^T A + \\lambda \\Gamma^T \\Gamma)^{-1} A^T A (A^T A + \\lambda \\Gamma^T \\Gamma)^{-1}\\right) \\right)\n$$\nThese formulas are implemented numerically for each of the four test cases specified in the problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected per-component mean squared discrepancy for Tikhonov-regularized\n    estimators under four different regularization scenarios.\n    \"\"\"\n    # Define fixed data from the problem statement.\n    A = np.array([\n        [1.2, -0.7, 0.3, 0.0],\n        [0.9, -0.4, 0.5, 0.2],\n        [1.5, -1.0, 0.8, -0.1],\n        [0.7, -0.2, 0.1, 0.3],\n        [1.1, -0.6, 0.4, 0.1],\n        [0.8, -0.5, 0.2, -0.2],\n        [1.3, -0.9, 0.7, 0.0],\n        [0.6, -0.1, 0.0, 0.4]\n    ])\n\n    A_test = np.array([\n        [1.0, -0.5, 0.4, 0.1],\n        [1.4, -0.8, 0.6, -0.1],\n        [0.95, -0.45, 0.35, 0.15],\n        [0.75, -0.25, 0.15, 0.25],\n        [1.2, -0.7, 0.5, 0.0]\n    ])\n\n    theta_star = np.array([-0.4, 0.9, 0.7, -0.2]).reshape(-1, 1)\n\n    sigma_sq = 0.0025\n    m = A_test.shape[0]  # Number of test data points\n    p = A.shape[1]       # Number of parameters\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: lambda = 0.1, Gamma = I_4, theta_0 = 0\n        {'lambda': 0.1, 'Gamma': np.identity(p), 'theta_0': np.zeros((p, 1))},\n        # Case 2: lambda = 1e-8, Gamma = I_4, theta_0 = 0\n        {'lambda': 1e-8, 'Gamma': np.identity(p), 'theta_0': np.zeros((p, 1))},\n        # Case 3: lambda = 100.0, Gamma = I_4, theta_0 = 0\n        {'lambda': 100.0, 'Gamma': np.identity(p), 'theta_0': np.zeros((p, 1))},\n        # Case 4: lambda = 1.0, Gamma = diag(...), theta_0 = [...]\n        {'lambda': 1.0, \n         'Gamma': np.diag([1.0, 10.0, 1.0, 0.1]), \n         'theta_0': np.array([-0.3, 1.1, 0.5, -0.15]).reshape(-1, 1)}\n    ]\n\n    results = []\n    \n    # Pre-compute matrices that are constant across test cases\n    S = A.T @ A\n    A_test_T_A_test = A_test.T @ A_test\n\n    for case in test_cases:\n        lam = case['lambda']\n        Gamma = case['Gamma']\n        theta_0 = case['theta_0']\n\n        # Penalty matrix in the objective function\n        P_reg = lam * Gamma.T @ Gamma\n        \n        # Matrix to be inverted: (A^T*A + lambda*Gamma^T*Gamma)\n        M = S + P_reg\n        M_inv = np.linalg.inv(M)\n\n        # --- Bias Calculation ---\n        # Bias of the parameter estimator: Bias(theta_hat) = M_inv * P_reg * (theta_0 - theta_star)\n        bias_theta = M_inv @ P_reg @ (theta_0 - theta_star)\n        \n        # Bias of the test predictions: Bias(y_hat_test) = A_test * Bias(theta_hat)\n        bias_y_test = A_test @ bias_theta\n        \n        # Squared norm of the prediction bias vector\n        sq_bias_term = np.sum(bias_y_test**2)\n\n        # --- Variance Calculation ---\n        # Component of the covariance matrix of theta_hat: M_inv * S * M_inv\n        V_matrix_comp = M_inv @ S @ M_inv\n        \n        # Trace of the prediction covariance matrix: Tr(Cov(y_hat_test))\n        # Tr(Cov(y_hat_test)) = sigma^2 * Tr(A_test.T * A_test * M_inv * S * M_inv)\n        var_term = sigma_sq * np.trace(A_test_T_A_test @ V_matrix_comp)\n        \n        # --- Expected Mean Squared Error (EMSE) per component ---\n        # EMSE = (1/m) * (Total Squared Bias + Total Variance)\n        emse = (sq_bias_term + var_term) / m\n        \n        results.append(emse)\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of floats, rounded to six decimal places, enclosed in brackets.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A low error on the training set can be misleading; it does not guarantee the model will perform well on new, unseen configurations. To build a truly predictive and transferable potential, we must assess its robustness using cross-validation. This exercise guides you through implementing the k-fold cross-validation workflow, a gold-standard technique for estimating a model's generalization error and parameter stability, ensuring your final potential is not just accurate but also reliable.",
            "id": "3828943",
            "problem": "You are given a synthetic dataset representing results from Density Functional Theory (DFT) calculations used to parameterize an interatomic potential. You must implement a program to perform $k$-fold cross-validation on a linearized interatomic potential model and compare the stability of fitted parameters and validation errors across folds to assess robustness.\n\nThe modeling assumptions are as follows. For each configuration $i$, there exists an energy descriptor vector $\\mathbf{x}_i \\in \\mathbb{R}^p$ and a force descriptor matrix $\\mathbf{X}^{(f)}_i \\in \\mathbb{R}^{3N_i \\times p}$ obtained from linearized force descriptors consistent with the gradient of the potential energy. The potential energy per configuration and the forces are modeled as linear in the parameter vector $\\mathbf{w} \\in \\mathbb{R}^p$ with a scalar bias $b \\in \\mathbb{R}$:\n$$E_i = \\mathbf{x}_i^\\top \\mathbf{w} + b,$$\n$$\\mathbf{F}_i = \\mathbf{X}^{(f)}_i \\mathbf{w}.$$\nThe bias $b$ contributes only to energy and not to forces. Given training data $\\{(\\mathbf{x}_i, \\mathbf{X}^{(f)}_i, E_i, \\mathbf{F}_i)\\}$, the parameters $\\mathbf{w}$ and $b$ are determined by minimizing a weighted regularized least squares objective\n$$J(\\mathbf{w}, b) = \\alpha_E \\sum_i \\left( E_i - (\\mathbf{x}_i^\\top \\mathbf{w} + b) \\right)^2 + \\alpha_F \\sum_i \\left\\lVert \\mathbf{F}_i - \\mathbf{X}^{(f)}_i \\mathbf{w} \\right\\rVert_2^2 + \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert_2^2,$$\nwhere $\\alpha_E \\ge 0$ and $\\alpha_F \\ge 0$ are weights balancing energies and forces, and $\\lambda \\ge 0$ is an $\\ell_2$ regularization coefficient applied to $\\mathbf{w}$ but not to $b$.\n\nConstruct a single augmented linear system for the parameters $\\boldsymbol{\\theta} = [\\mathbf{w}; b] \\in \\mathbb{R}^{p+1}$ by stacking energy rows $\\left[\\mathbf{x}_i^\\top, 1\\right]$ and force rows $\\left[\\mathbf{X}^{(f)}_i, \\mathbf{0}\\right]$ and scaling them by $\\sqrt{\\alpha_E}$ and $\\sqrt{\\alpha_F}$, respectively. Add $\\sqrt{\\lambda}$-scaled identity rows to regularize $\\mathbf{w}$ only. Use the Euclidean norm definitions and solve the least squares problem to obtain $\\boldsymbol{\\theta}$.\n\nDataset specification. Use a reproducible synthetic dataset defined as follows:\n- The number of configurations is $M = 30$, with a fixed random seed $s = 7$ for all random draws to ensure reproducibility.\n- The descriptor dimension is $p = 5$.\n- The true parameters used to generate data are $\\mathbf{w}_{\\mathrm{true}} = [0.8, -0.4, 0.3, 0.0, 0.2]$ and $b_{\\mathrm{true}} = -0.1$.\n- For each configuration $i$, draw $\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_p)$ and $N_i$ uniformly from $\\{2, 3, 4\\}$, then form $\\mathbf{X}^{(f)}_i \\in \\mathbb{R}^{3N_i \\times p}$ with rows $\\mathbf{x}_i^\\top + \\boldsymbol{\\eta}$, where $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, 0.5^2 \\mathbf{I}_p)$ independently per row.\n- Define noise levels $\\sigma_E = 0.02$ for energy and $\\sigma_F = 0.05$ for forces, with independent Gaussian noise. The measured values are $E_i = \\mathbf{x}_i^\\top \\mathbf{w}_{\\mathrm{true}} + b_{\\mathrm{true}} + \\epsilon_E$ with $\\epsilon_E \\sim \\mathcal{N}(0, \\sigma_E^2)$ and $\\mathbf{F}_i = \\mathbf{X}^{(f)}_i \\mathbf{w}_{\\mathrm{true}} + \\boldsymbol{\\epsilon}_F$ with $\\boldsymbol{\\epsilon}_F \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_F^2 \\mathbf{I}_{3N_i})$.\n\nCross-validation protocol. For a given $k$, partition the $M$ configurations into $k$ folds of sizes differing by at most $1$ using a fixed shuffle seed $s_{\\mathrm{cv}} = 123$. For each fold $j \\in \\{1, \\dots, k\\}$, train on the union of all folds except $j$ using the weighted regularized least squares above, and evaluate on fold $j$. Compute per-fold validation metrics:\n- The Root Mean Square Error (RMSE) of energy in electronvolts, defined by\n$$\\mathrm{RMSE}_E^{(j)} = \\sqrt{\\frac{1}{n_E^{(j)}} \\sum_{i \\in \\text{fold } j} \\left( E_i - (\\mathbf{x}_i^\\top \\hat{\\mathbf{w}}^{(j)} + \\hat{b}^{(j)}) \\right)^2},$$\nwhere $n_E^{(j)}$ is the number of configurations in fold $j$ and $(\\hat{\\mathbf{w}}^{(j)}, \\hat{b}^{(j)})$ are the fitted parameters from training folds.\n- The RMSE of forces in electronvolts per Angstrom, defined by\n$$\\mathrm{RMSE}_F^{(j)} = \\sqrt{\\frac{1}{n_F^{(j)}} \\sum_{i \\in \\text{fold } j} \\left\\lVert \\mathbf{F}_i - \\mathbf{X}^{(f)}_i \\hat{\\mathbf{w}}^{(j)} \\right\\rVert_2^2},$$\nwhere $n_F^{(j)}$ is the total number of force components $\\sum_{i \\in \\text{fold } j} 3N_i$.\n\nRobustness assessment. Across all $k$ folds, compute parameter stability metrics:\n- Let $\\bar{\\mathbf{w}}$ and $\\mathrm{std}(\\mathbf{w})$ denote the fold-wise mean and standard deviation of the vectors $\\hat{\\mathbf{w}}^{(j)}$. Define the weight stability ratio\n$$\\rho_w = \\frac{\\left\\lVert \\mathrm{std}(\\mathbf{w}) \\right\\rVert_2}{\\max\\left(\\left\\lVert \\bar{\\mathbf{w}} \\right\\rVert_2, \\varepsilon\\right)},$$\nwith a small $\\varepsilon = 10^{-12}$ to avoid division by zero.\n- Let $\\bar{b}$ and $\\mathrm{std}(b)$ denote the mean and standard deviation of the scalars $\\hat{b}^{(j)}$. Define the bias stability ratio\n$$\\rho_b = \\frac{\\mathrm{std}(b)}{\\max\\left(|\\bar{b}|, \\varepsilon\\right)}.$$\nAlso report the mean validation RMSEs\n$$\\overline{\\mathrm{RMSE}_E} = \\frac{1}{k} \\sum_{j=1}^k \\mathrm{RMSE}_E^{(j)}, \\quad \\overline{\\mathrm{RMSE}_F} = \\frac{1}{k} \\sum_{j=1}^k \\mathrm{RMSE}_F^{(j)}.$$\n\nImplementation requirements:\n- Build the augmented design matrix by stacking energy and force rows with weights $\\sqrt{\\alpha_E}$ and $\\sqrt{\\alpha_F}$, respectively, and add $\\sqrt{\\lambda}$-weighted identity rows for $\\mathbf{w}$ only. Solve the linear least squares for $\\boldsymbol{\\theta}$ using a numerically stable solver.\n- Energy RMSE must be expressed in electronvolts (eV). Force RMSE must be expressed in electronvolts per Angstrom (eV/Å). Angles are not used in this problem.\n- The final program must produce a single line of output containing the results of all test cases as a comma-separated list enclosed in square brackets, where each test case result is a list of four floats $[\\rho_w, \\rho_b, \\overline{\\mathrm{RMSE}_E}, \\overline{\\mathrm{RMSE}_F}]$. For example, the format should be like $[[r_{11},r_{12},r_{13},r_{14}],[r_{21},r_{22},r_{23},r_{24}],\\dots]$ without any spaces.\n\nTest suite. Run the program for the following parameter sets $(k, \\alpha_E, \\alpha_F, \\lambda)$:\n- Case $1$: $(k = 2, \\alpha_E = 1.0, \\alpha_F = 0.0, \\lambda = 10^{-8})$.\n- Case $2$: $(k = 5, \\alpha_E = 1.0, \\alpha_F = 1.0, \\lambda = 10^{-8})$.\n- Case $3$: $(k = 10, \\alpha_E = 0.5, \\alpha_F = 2.0, \\lambda = 10^{-8})$.\n- Case $4$: $(k = 30, \\alpha_E = 1.0, \\alpha_F = 0.5, \\lambda = 10^{-8})$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the order of the test suite cases. Each case must output a list $[\\rho_w, \\rho_b, \\overline{\\mathrm{RMSE}_E}, \\overline{\\mathrm{RMSE}_F}]$ as floats.",
            "solution": "The user has provided a problem that requires the implementation of a $k$-fold cross-validation procedure to assess the robustness of a linearized interatomic potential model fit to synthetic Density Functional Theory (DFT) data.\n\n### Step 1: Extract Givens\n\nThe problem provides the following information:\n- **Model Equations**:\n  - Potential energy per configuration $i$: $E_i = \\mathbf{x}_i^\\top \\mathbf{w} + b$, where $\\mathbf{x}_i \\in \\mathbb{R}^p$ is an energy descriptor vector, $\\mathbf{w} \\in \\mathbb{R}^p$ is a parameter vector, and $b \\in \\mathbb{R}$ is a scalar bias.\n  - Forces per configuration $i$: $\\mathbf{F}_i = \\mathbf{X}^{(f)}_i \\mathbf{w}$, where $\\mathbf{X}^{(f)}_i \\in \\mathbb{R}^{3N_i \\times p}$ is a force descriptor matrix.\n- **Objective Function**: Parameters $(\\mathbf{w}, b)$ are found by minimizing the weighted regularized least squares objective:\n$$J(\\mathbf{w}, b) = \\alpha_E \\sum_i \\left( E_i - (\\mathbf{x}_i^\\top \\mathbf{w} + b) \\right)^2 + \\alpha_F \\sum_i \\left\\lVert \\mathbf{F}_i - \\mathbf{X}^{(f)}_i \\mathbf{w} \\right\\rVert_2^2 + \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert_2^2$$\n  with weights $\\alpha_E \\ge 0$, $\\alpha_F \\ge 0$, and regularization coefficient $\\lambda \\ge 0$.\n- **Linear System Formulation**: The problem is to be solved by constructing an augmented linear system for $\\boldsymbol{\\theta} = [\\mathbf{w}; b] \\in \\mathbb{R}^{p+1}$.\n- **Synthetic Dataset Specification**:\n  - Number of configurations: $M = 30$.\n  - Random seed for data generation: $s = 7$.\n  - Descriptor dimension: $p = 5$.\n  - True parameters: $\\mathbf{w}_{\\mathrm{true}} = [0.8, -0.4, 0.3, 0.0, 0.2]$ and $b_{\\mathrm{true}} = -0.1$.\n  - Data generation process for each configuration $i$:\n    - $\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_p)$.\n    - Number of atoms $N_i$ drawn uniformly from $\\{2, 3, 4\\}$.\n    - Force descriptor matrix $\\mathbf{X}^{(f)}_i \\in \\mathbb{R}^{3N_i \\times p}$ has rows $\\mathbf{x}_i^\\top + \\boldsymbol{\\eta}$, with $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, 0.5^2 \\mathbf{I}_p)$.\n  - Noise model:\n    - Energy noise: $\\sigma_E = 0.02$, $E_i = (\\mathbf{x}_i^\\top \\mathbf{w}_{\\mathrm{true}} + b_{\\mathrm{true}}) + \\epsilon_E$, where $\\epsilon_E \\sim \\mathcal{N}(0, \\sigma_E^2)$.\n    - Force noise: $\\sigma_F = 0.05$, $\\mathbf{F}_i = \\mathbf{X}^{(f)}_i \\mathbf{w}_{\\mathrm{true}} + \\boldsymbol{\\epsilon}_F$, where $\\boldsymbol{\\epsilon}_F \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_F^2 \\mathbf{I}_{3N_i})$.\n- **Cross-Validation Protocol**:\n  - For a given $k$, partition $M$ configurations into $k$ folds.\n  - Shuffle seed for partitioning: $s_{\\mathrm{cv}} = 123$.\n  - For each fold $j$, train on all other folds and evaluate on fold $j$.\n- **Per-Fold Validation Metrics**:\n  - Energy RMSE: $\\mathrm{RMSE}_E^{(j)} = \\sqrt{\\frac{1}{n_E^{(j)}} \\sum_{i \\in \\text{fold } j} \\left( E_i - (\\mathbf{x}_i^\\top \\hat{\\mathbf{w}}^{(j)} + \\hat{b}^{(j)}) \\right)^2}$.\n  - Force RMSE: $\\mathrm{RMSE}_F^{(j)} = \\sqrt{\\frac{1}{n_F^{(j)}} \\sum_{i \\in \\text{fold } j} \\left\\lVert \\mathbf{F}_i - \\mathbf{X}^{(f)}_i \\hat{\\mathbf{w}}^{(j)} \\right\\rVert_2^2}$.\n- **Robustness Assessment Metrics**:\n  - Weight stability ratio: $\\rho_w = \\frac{\\left\\lVert \\mathrm{std}(\\mathbf{w}) \\right\\rVert_2}{\\max\\left(\\left\\lVert \\bar{\\mathbf{w}} \\right\\rVert_2, \\varepsilon\\right)}$ with $\\varepsilon = 10^{-12}$.\n  - Bias stability ratio: $\\rho_b = \\frac{\\mathrm{std}(b)}{\\max\\left(|\\bar{b}|, \\varepsilon\\right)}$.\n  - Mean validation RMSEs: $\\overline{\\mathrm{RMSE}_E} = \\frac{1}{k} \\sum_{j=1}^k \\mathrm{RMSE}_E^{(j)}$, $\\overline{\\mathrm{RMSE}_F} = \\frac{1}{k} \\sum_{j=1}^k \\mathrm{RMSE}_F^{(j)}$.\n- **Test Suite**:\n  - T1: $(k = 2, \\alpha_E = 1.0, \\alpha_F = 0.0, \\lambda = 10^{-8})$.\n  - T2: $(k = 5, \\alpha_E = 1.0, \\alpha_F = 1.0, \\lambda = 10^{-8})$.\n  - T3: $(k = 10, \\alpha_E = 0.5, \\alpha_F = 2.0, \\lambda = 10^{-8})$.\n  - T4: $(k = 30, \\alpha_E = 1.0, \\alpha_F = 0.5, \\lambda = 10^{-8})$.\n- **Output Format**: A single line `[[r11,r12,r13,r14],[r21,r22,r23,r24],...]` without spaces.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded**: The problem describes a simplified but conceptually sound method for parameterizing empirical interatomic potentials from first-principles data (DFT). The use of a linear model, a weighted least-squares objective function with Tikhonov ($\\ell_2$) regularization, and k-fold cross-validation are all standard, well-established techniques in computational physics, materials science, and machine learning. The model is a valid simplification for pedagogical and testing purposes.\n2.  **Well-Posed**: The use of a positive regularization parameter $\\lambda = 10^{-8} > 0$ ensures that the normal equations matrix of the least-squares problem is positive definite, guaranteeing a unique and stable solution for the parameters $\\mathbf{w}$. The parameter $b$ is also uniquely determined as long as the training data is not pathologically centered. The entire procedure, including data generation and cross-validation, is algorithmically specified, leading to a unique and meaningful result.\n3.  **Objective**: The problem is stated in precise, mathematical terms, free of any subjectivity or ambiguity.\n4.  **Incomplete or Contradictory Setup**: All necessary parameters, seeds, and definitions are explicitly provided, and there are no internal contradictions.\n5.  **Unrealistic or Infeasible**: The problem utilizes a synthetic dataset, for which all parameters are within reasonable computational and physical bounds for such a model.\n6.  **Ill-Posed or Poorly Structured**: The problem is well-structured as a standard machine learning workflow.\n7.  **Pseudo-Profound, Trivial, or Tautological**: The problem is non-trivial, requiring careful implementation of data generation, linear system construction, cross-validation, and metric calculation. It is a substantive computational task.\n8.  **Outside Scientific Verifiability**: The problem is deterministic due to the specified random seeds, making the results computationally verifiable.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. We may proceed with the solution.\n\nThe solution will be implemented by following these principal steps:\n1.  **Synthetic Data Generation**: Construct the set of $M=30$ data configurations $\\{(\\mathbf{x}_i, \\mathbf{X}^{(f)}_i, E_i, \\mathbf{F}_i, N_i)\\}_{i=1}^M$ using the specified probabilistic models and the fixed random seed $s=7$.\n2.  **Cross-Validation Partitioning**: For each test case's value of $k$, the $M$ configurations will be deterministically shuffled (using seed $s_{\\mathrm{cv}}=123$) and partitioned into $k$ disjoint folds.\n3.  **Iterative Model Fitting and Validation**: For each fold $j \\in \\{1, \\dots, k\\}$:\n    a.  The model is trained using all configurations *not* in fold $j$. This involves constructing and solving an augmented linear least-squares problem.\n    b.  The trained model $(\\hat{\\mathbf{w}}^{(j)}, \\hat{b}^{(j)})$ is then used to predict energies and forces for the configurations *in* the validation fold $j$.\n    c.  The per-fold validation errors $\\mathrm{RMSE}_E^{(j)}$ and $\\mathrm{RMSE}_F^{(j)}$ are computed and stored.\n    d.  The fitted parameters $\\hat{\\mathbf{w}}^{(j)}$ and $\\hat{b}^{(j)}$ are stored.\n4.  **Robustness and Error Aggregation**: After iterating through all $k$ folds, the collected sets of parameters and validation errors are used to compute the final four metrics: $\\rho_w$, $\\rho_b$, $\\overline{\\mathrm{RMSE}_E}$, and $\\overline{\\mathrm{RMSE}_F}$.\n\nThe core of the fitting procedure lies in transforming the objective function $J(\\mathbf{w}, b)$ into a standard linear least-squares problem $\\min_{\\boldsymbol{\\theta}} \\lVert \\mathbf{A}\\boldsymbol{\\theta} - \\mathbf{y} \\rVert_2^2$, where $\\boldsymbol{\\theta} = [\\mathbf{w}^\\top, b]^\\top$. The objective function is a sum of squared norms:\n$$\nJ(\\mathbf{w}, b) = \\sum_{i \\in \\text{train}} \\left\\lVert \\sqrt{\\alpha_E} E_i - (\\sqrt{\\alpha_E}\\mathbf{x}_i^\\top\\mathbf{w} + \\sqrt{\\alpha_E}b) \\right\\rVert_2^2 + \\sum_{i \\in \\text{train}} \\left\\lVert \\sqrt{\\alpha_F} \\mathbf{F}_i - \\sqrt{\\alpha_F}\\mathbf{X}^{(f)}_i \\mathbf{w} \\right\\rVert_2^2 + \\left\\lVert \\sqrt{\\lambda} \\mathbf{I}_{p} \\mathbf{w} \\right\\rVert_2^2\n$$\nThis structure dictates the composition of the augmented matrix $\\mathbf{A}$ and vector $\\mathbf{y}$. For each training configuration $i$, we add:\n- One row to $\\mathbf{A}$ of $[\\sqrt{\\alpha_E}\\mathbf{x}_i^\\top, \\sqrt{\\alpha_E}]$ and one element $\\sqrt{\\alpha_E}E_i$ to $\\mathbf{y}$ for the energy term.\n- A block of $3N_i$ rows to $\\mathbf{A}$ of $[\\sqrt{\\alpha_F}\\mathbf{X}^{(f)}_i, \\mathbf{0}_{3N_i \\times 1}]$ and a corresponding vector block $\\sqrt{\\alpha_F}\\mathbf{F}_i$ to $\\mathbf{y}$ for the force term.\n\nFor regularization, we add a block of $p$ rows to $\\mathbf{A}$ of $[\\sqrt{\\lambda}\\mathbf{I}_p, \\mathbf{0}_{p \\times 1}]$ and a corresponding block of $p$ zeros to $\\mathbf{y}$.\n\nThis augmented system $\\mathbf{A}\\boldsymbol{\\theta} \\approx \\mathbf{y}$ is then solved using a numerically stable least-squares solver to find the optimal parameter vector $\\boldsymbol{\\theta} = [\\hat{\\mathbf{w}}^\\top, \\hat{b}]^\\top$. The procedure is repeated for each fold of the cross-validation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a k-fold cross-validation for a linearized interatomic potential model\n    and assesses robustness based on a synthetic dataset.\n    \"\"\"\n    # --------------------------------------------------------------------------\n    # 1. Synthetic Dataset Generation\n    # --------------------------------------------------------------------------\n    def generate_data(M, p, s, w_true, b_true, n_atoms_range, xf_noise_std, sigma_e, sigma_f):\n        \"\"\"Generates the synthetic dataset based on problem specifications.\"\"\"\n        rng = np.random.default_rng(seed=s)\n        dataset = []\n        for _ in range(M):\n            # Generate descriptors and configuration size\n            x_i = rng.standard_normal(size=p)\n            N_i = rng.integers(n_atoms_range[0], n_atoms_range[1])\n            num_force_components = 3 * N_i\n\n            # Generate force descriptor matrix X_f\n            eta = rng.normal(loc=0.0, scale=xf_noise_std, size=(num_force_components, p))\n            X_f_i = np.tile(x_i, (num_force_components, 1)) + eta\n\n            # Calculate true energy and forces\n            E_i_true = x_i.T @ w_true + b_true\n            F_i_true = X_f_i @ w_true\n\n            # Add Gaussian noise to get \"measured\" values\n            epsilon_E = rng.normal(loc=0.0, scale=sigma_e)\n            E_i_measured = E_i_true + epsilon_E\n            epsilon_F = rng.normal(loc=0.0, scale=sigma_f, size=num_force_components)\n            F_i_measured = F_i_true + epsilon_F\n\n            dataset.append({\n                'x': x_i,\n                'X_f': X_f_i,\n                'E': E_i_measured,\n                'F': F_i_measured,\n                'N': N_i\n            })\n        return dataset\n\n    # --- Global dataset parameters ---\n    M = 30\n    p = 5\n    s = 7\n    w_true = np.array([0.8, -0.4, 0.3, 0.0, 0.2])\n    b_true = -0.1\n    N_i_range = (2, 5) # uniform from {2, 3, 4} -> integers(low, high) where high is exclusive\n    xf_noise_std_val = 0.5\n    sigma_E_val = 0.02\n    sigma_F_val = 0.05\n    s_cv = 123\n    epsilon = 1e-12\n\n    dataset = generate_data(M, p, s, w_true, b_true, N_i_range, xf_noise_std_val, sigma_E_val, sigma_F_val)\n\n    # --------------------------------------------------------------------------\n    # 2. Test Suite Execution\n    # --------------------------------------------------------------------------\n    test_cases = [\n        # (k, alpha_E, alpha_F, lambda)\n        (2, 1.0, 0.0, 1e-8),\n        (5, 1.0, 1.0, 1e-8),\n        (10, 0.5, 2.0, 1e-8),\n        (30, 1.0, 0.5, 1e-8),\n    ]\n\n    all_results = []\n    \n    for k, alpha_E, alpha_F, lam in test_cases:\n        # Cross-validation setup\n        rng_cv = np.random.default_rng(seed=s_cv)\n        indices = rng_cv.permutation(M)\n        folds = np.array_split(indices, k)\n\n        fold_params_w = []\n        fold_params_b = []\n        fold_rmse_e_list = []\n        fold_rmse_f_list = []\n\n        # ----------------------------------------------------------------------\n        # 3. K-Fold Cross-Validation Loop\n        # ----------------------------------------------------------------------\n        for i in range(k):\n            val_indices = folds[i]\n            train_indices = np.concatenate([folds[j] for j in range(k) if i != j])\n\n            # --- Build augmented linear system for training data ---\n            \n            # Calculate total rows for pre-allocation\n            num_train_configs = len(train_indices)\n            num_force_rows_train = sum(3 * dataset[idx]['N'] for idx in train_indices)\n            total_rows_train = num_train_configs + num_force_rows_train + p\n            \n            A_train = np.zeros((total_rows_train, p + 1))\n            y_train = np.zeros(total_rows_train)\n            \n            current_row = 0\n            sqrt_alpha_E = np.sqrt(alpha_E)\n            sqrt_alpha_F = np.sqrt(alpha_F)\n            \n            for idx in train_indices:\n                config = dataset[idx]\n                \n                # Energy row\n                if alpha_E > 0:\n                    A_train[current_row, :p] = sqrt_alpha_E * config['x']\n                    A_train[current_row, p] = sqrt_alpha_E\n                    y_train[current_row] = sqrt_alpha_E * config['E']\n                    current_row += 1\n            \n            # Force rows block\n            if alpha_F > 0:\n                for idx in train_indices:\n                    config = dataset[idx]\n                    num_f = 3 * config['N']\n                    A_train[current_row : current_row + num_f, :p] = sqrt_alpha_F * config['X_f']\n                    # The bias column (p+1) is already zero\n                    y_train[current_row : current_row + num_f] = sqrt_alpha_F * config['F']\n                    current_row += num_f\n\n            # Regularization rows\n            if lam > 0:\n                sqrt_lambda = np.sqrt(lam)\n                reg_block = sqrt_lambda * np.identity(p)\n                A_train[current_row : current_row + p, :p] = reg_block\n                # The rest of the rows/cols are zero\n                current_row += p\n            \n            # Solve the linear system\n            theta, _, _, _ = np.linalg.lstsq(A_train, y_train, rcond=None)\n            w_fit = theta[:p]\n            b_fit = theta[p]\n            \n            fold_params_w.append(w_fit)\n            fold_params_b.append(b_fit)\n\n            # --- Evaluate on validation fold ---\n            val_E_errors_sq = []\n            val_F_errors_sq = []\n            num_F_components_val = 0\n            for idx in val_indices:\n                config = dataset[idx]\n                \n                # Energy prediction\n                E_pred = config['x'].T @ w_fit + b_fit\n                val_E_errors_sq.append((config['E'] - E_pred)**2)\n                \n                # Force prediction\n                F_pred = config['X_f'] @ w_fit\n                val_F_errors_sq.append(np.sum((config['F'] - F_pred)**2))\n                num_F_components_val += 3 * config['N']\n            \n            rmse_e = np.sqrt(np.mean(val_E_errors_sq))\n            rmse_f = np.sqrt(np.sum(val_F_errors_sq) / num_F_components_val) if num_F_components_val > 0 else 0.0\n            \n            fold_rmse_e_list.append(rmse_e)\n            fold_rmse_f_list.append(rmse_f)\n\n        # ----------------------------------------------------------------------\n        # 4. Compute Final Metrics\n        # ----------------------------------------------------------------------\n        # Parameter stability\n        w_array = np.array(fold_params_w)\n        b_array = np.array(fold_params_b)\n\n        w_mean = np.mean(w_array, axis=0)\n        w_std = np.std(w_array, axis=0, ddof=0)\n        \n        rho_w = np.linalg.norm(w_std) / max(np.linalg.norm(w_mean), epsilon)\n\n        b_mean = np.mean(b_array)\n        b_std = np.std(b_array, ddof=0)\n        \n        rho_b = b_std / max(abs(b_mean), epsilon)\n\n        # Mean validation errors\n        mean_rmse_e = np.mean(fold_rmse_e_list)\n        mean_rmse_f = np.mean(fold_rmse_f_list)\n\n        all_results.append([rho_w, rho_b, mean_rmse_e, mean_rmse_f])\n\n    # Format the final output string to remove all spaces\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}