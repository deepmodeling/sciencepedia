{
    "hands_on_practices": [
        {
            "introduction": "The choice of a loss function is a foundational decision in training any machine learning model. In multiscale systems where noise can vary dramatically across scales—a property known as heteroskedasticity—this choice becomes critical for model robustness and transferability. This exercise  guides you through a comparative analysis of the Mean Squared Error ($\\|u-y\\|^2$) and Mean Absolute Error ($\\|u-y\\|_1$) losses, revealing how each objective induces different statistical estimators and sensitivities during training.",
            "id": "3825709",
            "problem": "A research group is training a Multi-Layer Perceptron (MLP) to map multiscale inputs $X$ (with features representing both coarse and fine scales) to outputs $Y$ in a setting where the response exhibits scale-dependent variability. Let the data-generating process be $Y = g(X) + \\epsilon$, where $g$ is a deterministic target map and $\\epsilon$ is a random noise term whose conditional distribution $p(\\epsilon \\mid X)$ is zero-centered at each $X$ but heteroskedastic across scales: at coarse-scale regimes the conditional variance is relatively small, while at micro-scale regimes it may be large and may exhibit heavy tails. The model is trained by Empirical Risk Minimization (ERM) to minimize the average loss over a dataset of size $n$, with either the Mean Squared Error (MSE) $\\ell_{\\text{MSE}}(u,y) = \\|u-y\\|^{2}$ or the Mean Absolute Error (MAE) $\\ell_{\\text{MAE}}(u,y) = \\|u-y\\|_{1}$, where $u$ denotes the prediction and $y$ the observed response. Consider the expected risk $R_{\\ell}(f) = \\mathbb{E}\\left[\\ell\\left(f(X), Y\\right)\\right]$, the induced pointwise Bayes estimator $u^{*}(x) \\in \\arg\\min_{u} \\mathbb{E}\\left[\\ell\\left(u, Y\\right) \\mid X=x\\right]$, and the gradient of the empirical risk with respect to model parameters under each loss. The group also evaluates models using a validation metric consistent with the training loss. The target application requires transferability to a related domain in which the distribution $p(Y \\mid X)$ changes only in its micro-scale tail heaviness and variance, while its central tendency (location) remains approximately the same at each $X$.\n\nWhich of the following statements are correct in this setting?\n\nA. Minimizing the expected risk under the squared loss induces the pointwise estimator $u^{*}(x)$ equal to the conditional expectation $\\mathbb{E}\\left[Y \\mid X=x\\right]$, whereas minimizing the expected risk under the absolute loss induces the pointwise estimator equal to a conditional median of $Y$ given $X=x$. Under heteroskedastic multiscale noise, the gradient contributions in ERM with squared loss scale with residual magnitude, causing high-variance regimes to dominate optimization; with absolute loss, the per-sample influence is bounded, increasing robustness to such heteroskedasticity.\n\nB. For any symmetric unimodal conditional distribution $p(Y \\mid X=x)$, both MSE and MAE induce the same optimal estimator equal to the conditional mode; moreover, the squared loss is inherently robust to outliers since averaging reduces their impact.\n\nC. If at the microscale the conditional variance of $\\epsilon$ is multiplied by a factor $c^{2}$ with $c>0$ while the conditional mean of $Y$ is unchanged, then the Bayes estimator under squared loss scales by a factor $c$ (i.e., $u^{*}(x)$ under squared loss changes multiplicatively with $c$), whereas the Bayes estimator under absolute loss remains unchanged.\n\nD. Using MSE as the validation metric tends to select models that prioritize reduction of large residuals associated with high-variance scales, potentially at the expense of performance on low-variance scales; using MAE as the validation metric reduces this dominance by penalizing residuals linearly, mitigating sensitivity to heteroskedasticity across scales.\n\nE. If the target domain differs from the source only by heavier tails and larger variance at the microscale while preserving the conditional location (for example, median) of $Y$ at each $X$, then, all else equal, an MAE-trained MLP is expected to transfer more robustly than an MSE-trained MLP because the bounded per-sample influence under absolute loss makes optimization and generalization less sensitive to the increased tail heaviness, whereas the unbounded influence under squared loss can be dominated by rare, large residuals in the new domain.\n\nSelect all that apply.",
            "solution": "The problem statement is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Model**: Multi-Layer Perceptron (MLP).\n- **Mapping**: From multiscale inputs $X$ to outputs $Y$.\n- **Data-Generating Process**: $Y = g(X) + \\epsilon$, where $g$ is a deterministic map and $\\epsilon$ is a random noise term.\n- **Noise Characteristics**: The conditional distribution $p(\\epsilon \\mid X)$ is zero-centered, i.e., $\\mathbb{E}[\\epsilon \\mid X=x] = 0$ for all $x$. The noise is heteroskedastic: conditional variance is small for coarse-scale inputs and large for micro-scale inputs, where the distribution may also have heavy tails.\n- **Training**: Empirical Risk Minimization (ERM) on a dataset of size $n$.\n- **Loss Functions**:\n    1.  Mean Squared Error (MSE): $\\ell_{\\text{MSE}}(u,y) = \\|u-y\\|^{2}$.\n    2.  Mean Absolute Error (MAE): $\\ell_{\\text{MAE}}(u,y) = \\|u-y\\|_{1}$.\n- **Notation**: $u$ denotes the model prediction, $y$ the observed response. The use of norms suggests $Y$ could be a vector.\n- **Theoretical Quantities**:\n    - Expected Risk: $R_{\\ell}(f) = \\mathbb{E}\\left[\\ell\\left(f(X), Y\\right)\\right]$.\n    - Pointwise Bayes Estimator: $u^{*}(x) \\in \\arg\\min_{u} \\mathbb{E}\\left[\\ell\\left(u, Y\\right) \\mid X=x\\right]$.\n- **Evaluation**: A validation metric consistent with the training loss is used.\n- **Transfer Learning Scenario**: Transfer to a related domain where $p(Y \\mid X)$ changes only in its micro-scale tail heaviness and variance. The central tendency (location) of $p(Y \\mid X)$ remains approximately the same at each $X$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in statistical learning theory. The concepts of ERM, MSE and MAE losses, Bayes estimators, heteroskedasticity, and transfer learning are standard and central to machine learning. The setup describes a common regression problem with realistic complexities.\n2.  **Well-Posed**: The problem is well-posed, asking for an evaluation of several statements based on a clearly defined statistical setting. There is sufficient information to derive the properties of the estimators and training dynamics.\n3.  **Objective**: The language is precise and objective, using standard terminology from statistics and machine learning. There are no subjective or opinion-based claims in the problem statement.\n4.  **Complete and Consistent**: The problem statement is self-contained. The assumption $\\mathbb{E}[\\epsilon \\mid X=x] = 0$ implies $\\mathbb{E}[Y \\mid X=x] = \\mathbb{E}[g(X) + \\epsilon \\mid X=x] = g(x)$. This is consistent with the notion of a deterministic target map $g$ and additive, zero-centered noise. The description of the transfer domain is also consistent with this setup.\n5.  **No Other Flaws**: The problem is not trivial, unrealistic, or ill-posed. It addresses a core conceptual issue in robust machine learning and model selection.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived by analyzing the properties of the loss functions and their implications in the given context.\n\n### Derivations and Analysis\n\n**1. Pointwise Bayes Estimators**\nThe pointwise Bayes estimator $u^*(x)$ minimizes the expected loss conditioned on $X=x$.\n\n- **For MSE ($\\ell_2$ loss)**: We seek to minimize $J(u) = \\mathbb{E}\\left[\\|u-Y\\|^2 \\mid X=x\\right]$. Assuming $Y$ is a random vector in $\\mathbb{R}^d$, this is $J(u) = \\mathbb{E}\\left[\\sum_{j=1}^d (u_j - Y_j)^2 \\mid X=x\\right]$.\nTaking the gradient with respect to the vector $u$ and setting it to zero:\n$$ \\nabla_u J(u) = \\nabla_u \\mathbb{E}\\left[(u-Y)^T(u-Y) \\mid X=x\\right] = \\mathbb{E}\\left[2(u-Y) \\mid X=x\\right] = 2(u - \\mathbb{E}[Y \\mid X=x]) $$\nSetting $\\nabla_u J(u) = 0$ gives $u = \\mathbb{E}[Y \\mid X=x]$. Thus, the Bayes estimator for MSE is the conditional expectation (mean) of $Y$.\n\n- **For MAE ($\\ell_1$ loss)**: We seek to minimize $J(u) = \\mathbb{E}\\left[\\|u-Y\\|_1 \\mid X=x\\right] = \\mathbb{E}\\left[\\sum_{j=1}^d |u_j - Y_j| \\mid X=x\\right] = \\sum_{j=1}^d \\mathbb{E}\\left[|u_j - Y_j| \\mid X=x\\right]$.\nThis minimization can be done component-wise. For each component $j$, we minimize $\\mathbb{E}\\left[|u_j - Y_j| \\mid X=x\\right]$.\nThe value of $u_j$ that minimizes this is the conditional median of the random variable $Y_j$ given $X=x$. To see this, let $p(y_j|x)$ be the conditional probability density function. Differentiating with respect to $u_j$ gives:\n$$ \\frac{\\partial}{\\partial u_j} \\int_{-\\infty}^{\\infty} |u_j - y_j| p(y_j|x) dy_j = \\int_{-\\infty}^{u_j} p(y_j|x) dy_j - \\int_{u_j}^{\\infty} p(y_j|x) dy_j = P(Y_j \\le u_j \\mid X=x) - P(Y_j > u_j \\mid X=x) $$\nSetting this to zero yields $P(Y_j \\le u_j \\mid X=x) = 1/2$. This is the definition of the median. Thus, the Bayes estimator for MAE is the vector of conditional medians.\n\n**2. ERM Gradient Analysis**\nFor a single sample $(x_i, y_i)$, the loss is $\\ell(f_\\theta(x_i), y_i)$. The gradient of the empirical risk with respect to model parameters $\\theta$ contains the term $\\frac{\\partial \\ell(u_i, y_i)}{\\partial u_i} \\nabla_\\theta f_\\theta(x_i)$, where $u_i = f_\\theta(x_i)$.\n\n- **For MSE**: $\\ell_{\\text{MSE}}(u_i, y_i) = (u_i - y_i)^2$ (for simplicity, consider a scalar output, the logic extends to vectors). The derivative with respect to the prediction is $\\frac{\\partial \\ell}{\\partial u_i} = 2(u_i - y_i)$. The gradient contribution is proportional to the residual $(u_i - y_i)$. Samples with large residuals (from high-variance regions) will result in large gradients, dominating the optimization process.\n\n- **For MAE**: $\\ell_{\\text{MAE}}(u_i, y_i) = |u_i - y_i|$. The subgradient with respect to the prediction is $\\frac{\\partial \\ell}{\\partial u_i} = \\text{sign}(u_i - y_i)$, which is either $-1$ or $1$ (or any value in $[-1, 1]$ if the residual is zero). The magnitude of this term is bounded by $1$, regardless of the magnitude of the residual. This bounds the influence of any single sample on the gradient updates, making the optimization process more robust to outliers and heteroskedasticity.\n\n### Option-by-Option Analysis\n\n**A. Minimizing the expected risk under the squared loss induces the pointwise estimator $u^{*}(x)$ equal to the conditional expectation $\\mathbb{E}\\left[Y \\mid X=x\\right]$, whereas minimizing the expected risk under the absolute loss induces the pointwise estimator equal to a conditional median of $Y$ given $X=x$. Under heteroskedastic multiscale noise, the gradient contributions in ERM with squared loss scale with residual magnitude, causing high-variance regimes to dominate optimization; with absolute loss, the per-sample influence is bounded, increasing robustness to such heteroskedasticity.**\n\nThis statement consists of two parts. The first part correctly identifies the Bayes estimators for squared loss (conditional mean) and absolute loss (conditional median), as shown in our derivation. The second part correctly analyzes the gradients in ERM, noting that the gradient for squared loss scales with the residual magnitude while the influence for absolute loss is bounded. This leads to the correct conclusion that MAE is more robust to heteroskedasticity during optimization. Both parts of the statement are correct.\n\nVerdict: **Correct**.\n\n**B. For any symmetric unimodal conditional distribution $p(Y \\mid X=x)$, both MSE and MAE induce the same optimal estimator equal to the conditional mode; moreover, the squared loss is inherently robust to outliers since averaging reduces their impact.**\n\nThe first part of this statement is correct. For a symmetric unimodal distribution, the mean, median, and mode coincide. Since the MSE estimator is the mean and the MAE estimator is the median, they will be identical and also equal to the mode. However, the second part of the statement, \"...the squared loss is inherently robust to outliers since averaging reduces their impact,\" is fundamentally incorrect. The act of squaring the residuals in the MSE loss function magnifies the influence of large errors (outliers), making the loss function and the corresponding training process *sensitive*, not robust, to outliers. Averaging is a feature of the empirical risk for both MSE and MAE and does not confer any special robustness to MSE.\n\nVerdict: **Incorrect**.\n\n**C. If at the microscale the conditional variance of $\\epsilon$ is multiplied by a factor $c^{2}$ with $c>0$ while the conditional mean of $Y$ is unchanged, then the Bayes estimator under squared loss scales by a factor $c$ (i.e., $u^{*}(x)$ under squared loss changes multiplicatively with $c$), whereas the Bayes estimator under absolute loss remains unchanged.**\n\nThe Bayes estimator under squared loss is the conditional mean, $u^*(x) = \\mathbb{E}[Y \\mid X=x]$. The problem statement specifies that \"the conditional mean of $Y$ is unchanged.\" Therefore, the Bayes estimator under squared loss must also be unchanged. The claim that it scales by a factor $c$ is a direct contradiction of the premise. Let's analyze this more formally: the new response is $Y' = g(X) + \\epsilon'$ where $\\text{Var}(\\epsilon'|X) = c^2 \\text{Var}(\\epsilon|X)$ and $\\mathbb{E}[Y'|X] = \\mathbb{E}[Y|X]$. The new Bayes estimator is $\\mathbb{E}[Y'|X]$, which, by the premise, is unchanged. Thus, the first part of the claim is false. Consequently, the entire statement is incorrect.\n\nVerdict: **Incorrect**.\n\n**D. Using MSE as the validation metric tends to select models that prioritize reduction of large residuals associated with high-variance scales, potentially at the expense of performance on low-variance scales; using MAE as the validation metric reduces this dominance by penalizing residuals linearly, mitigating sensitivity to heteroskedasticity across scales.**\n\nThis statement concerns model selection via a validation metric. A validation metric guides the choice of the best model (e.g., from different hyperparameter settings or training epochs). An MSE-based metric will be the average of squared residuals. A model that achieves very low error on a few high-variance points, even at the cost of slightly higher error on many low-variance points, can attain a lower overall MSE. Thus, MSE as a metric prioritizes fitting the high-variance regions. Conversely, an MAE-based metric, which penalizes residuals linearly, gives less weight to large residuals compared to MSE. This reduces the dominance of high-variance regions in the overall score, leading to a model selection process that is less sensitive to heteroskedasticity. The statement accurately describes this dynamic.\n\nVerdict: **Correct**.\n\n**E. If the target domain differs from the source only by heavier tails and larger variance at the microscale while preserving the conditional location (for example, median) of $Y$ at each $X$, then, all else equal, an MAE-trained MLP is expected to transfer more robustly than an MSE-trained MLP because the bounded per-sample influence under absolute loss makes optimization and generalization less sensitive to the increased tail heaviness, whereas the unbounded influence under squared loss can be dominated by rare, large residuals in the new domain.**\n\nThis statement addresses the transferability of the trained models. The core of the transfer problem is that the ideal target function (the conditional location/central tendency) is preserved, while the noise distribution becomes more challenging (heavier tails, larger variance). An MAE-trained model aims to learn the conditional median. Since the conditional median is preserved in the target domain, the function learned by the MAE-trained model remains relevant. Furthermore, the MAE training process is robust to outliers and heavy tails due to its bounded-influence gradients. This robustness during training on the source domain makes the model a better approximation of the true, underlying median function, less biased by the specific noise realization. An MSE-trained model, conversely, learns the conditional mean. While the mean might also be preserved, the training process is highly sensitive to large residuals. The function it learns can be skewed by heteroskedasticity in the source data. When evaluated in the target domain which has even heavier tails, its performance is likely to degrade significantly because the squared error metric will be dominated by the now more frequent and larger outliers. The reasoning in the statement is sound: the inherent robustness of the MAE objective function leads to a model that is less sensitive to the specific characteristics of the noise distribution, making it better suited for transfer to a domain where those characteristics change but the underlying signal does not.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ADE}$$"
        },
        {
            "introduction": "Successfully modeling multiscale phenomena often requires representing functions with significant high-frequency details. Standard network architectures, like those with ReLU activations, exhibit a \"spectral bias\" that favors learning low-frequency components first, which can hinder the representation of fine-scale features. This practice  explores how a model's inductive bias, shaped by its activation functions, determines its ability to learn and generalize across different frequency bands, contrasting a standard MLP with a Sinusoidal Representation Network (SIREN).",
            "id": "3825653",
            "problem": "Consider learning a one-dimensional multiscale field on the unit interval. Let the target field be $u:[0,1]\\to\\mathbb{R}$ given by a band-limited Fourier series $u(x)=\\sum_{k\\in\\mathcal{K}_{\\mathrm{all}}}a_k\\sin(2\\pi k x)$ with amplitudes $a_k$ decaying as $|a_k|\\le C k^{-\\beta}$ for some constants $C>0$ and $\\beta>1$. A training dataset $\\mathcal{D}_{\\mathrm{train}}=\\{(x_i,u(x_i))\\}_{i=1}^N$ is constructed by sampling $x_i$ uniformly in $[0,1]$, but the generating series for $u$ during training only includes a subset of frequencies $\\mathcal{K}_{\\mathrm{train}}\\subset\\mathcal{K}_{\\mathrm{all}}$ that are bounded above by $K_0$, i.e., $\\max \\mathcal{K}_{\\mathrm{train}}=K_0$. A validation dataset $\\mathcal{D}_{\\mathrm{val}}$ is independently drawn from the same $x$-distribution and the same frequency set $\\mathcal{K}_{\\mathrm{train}}$ and is used for early stopping based on the validation Mean Squared Error (MSE).\n\nTwo models are trained under identical optimization settings (same optimizer, step sizes, batch sizes, and weight decay): (i) a Multilayer Perceptron (MLP) with Rectified Linear Unit (ReLU) activations, denoted $f_R(x;\\theta)$; (ii) a Sinusoidal Representation Network (SIREN) with sine activations, denoted $f_S(x;\\phi)$, using a first-layer frequency scale parameter $\\omega_0$ chosen so that the distribution of pre-activations yields $\\mathbb{E}[|\\partial f_S/\\partial x|]\\approx \\mathrm{const}$ at initialization. Both models have sufficient width and depth to be universal approximators on $[0,1]$ under standard assumptions. After training with early stopping guided by $\\mathcal{D}_{\\mathrm{val}}$, the models are evaluated on a test dataset $\\mathcal{D}_{\\mathrm{test}}$ drawn from $x\\sim \\mathrm{Unif}([0,1])$ but with target fields containing frequencies exclusively in an unseen band $\\mathcal{K}_{\\mathrm{test}}\\subset\\mathcal{K}_{\\mathrm{all}}$ satisfying $\\min \\mathcal{K}_{\\mathrm{test}}=K_0+\\Delta K$ for some $\\Delta K>0$ and $\\mathcal{K}_{\\mathrm{test}}\\cap \\mathcal{K}_{\\mathrm{train}}=\\emptyset$.\n\nAssume the following well-tested facts and definitions hold: functions on $[0,1]$ admit Fourier representations; piecewise linear functions have Fourier spectra with amplitudes that decay with frequency; gradient-based training of ReLU networks exhibits a spectral bias favoring low-frequency components; sine activations can represent high-frequency oscillations with moderate parameter norms by adjusting effective frequencies through weights; early stopping controls overfitting to training distributions but does not alter the inductive bias of the activation nonlinearity.\n\nWhich statement best predicts, from first principles, the comparative ability of $f_R$ and $f_S$ to represent fine-scale oscillations and to transfer to the unseen high-frequency band $\\mathcal{K}_{\\mathrm{test}}$ under the described training and validation regime?\n\nA. The ReLU MLP $f_R$ will outperform the SIREN $f_S$ on both representing fine-scale oscillations and transfer to $\\mathcal{K}_{\\mathrm{test}}$, because piecewise linear functions can approximate any sinusoid with sufficiently many breakpoints and universal approximation implies equal inductive bias.\n\nB. The SIREN $f_S$, with sine activations and appropriate $\\omega_0$ initialization ensuring stable derivatives, will more compactly represent fine-scale oscillations and will yield better transfer to $\\mathcal{K}_{\\mathrm{test}}$ than the ReLU MLP $f_R$, because its periodic inductive bias and capacity to realize high effective frequencies with moderate parameter norms align with the unseen test band.\n\nC. Both $f_R$ and $f_S$ will perform equally on representing fine-scale oscillations and transfer to $\\mathcal{K}_{\\mathrm{test}}$, provided training continues until training MSE is near zero, because validation early stopping only prevents overfitting and does not interact with frequency content.\n\nD. A ReLU MLP $f_R$ augmented with fixed random Fourier feature positional encoding $z(x)=[\\cos(2\\pi f_j x),\\sin(2\\pi f_j x)]_{j=1}^M$ for frequencies $f_j \\le K_0$ will strictly surpass $f_S$ on transfer to $\\mathcal{K}_{\\mathrm{test}}$, since the encoded features guarantee extrapolation to any higher frequency outside the training band.",
            "solution": "### Step 1: Validate Problem Statement\nThe problem statement is scientifically sound and well-posed. It sets up a clear comparison between two different neural network architectures (ReLU MLP and SIREN) in a scenario designed to test their inductive biases and generalization capabilities. The context involves learning a multiscale field, where generalization to unseen (higher) frequencies is the key challenge. All concepts used—spectral bias, inductive bias, ReLU activation, sine activation (SIREN), early stopping, and Fourier series—are standard in modern machine learning and signal processing literature. The setup is not trivial and probes a deep aspect of how neural networks generalize.\n\n### Step 2: Main Derivation and Analysis\nThe core of this problem is to compare the *inductive biases* of the two architectures, which determines how they generalize from a low-frequency training regime to a high-frequency test regime.\n\n1.  **ReLU MLP ($f_R$)**:\n    *   **Architecture Bias**: An MLP with ReLU activations produces continuous piecewise linear functions. To approximate a smooth, oscillatory function like a sinusoid, it needs to combine many linear pieces. This representation is not very efficient for high-frequency signals.\n    *   **Learning Bias (Spectral Bias)**: It is a well-established phenomenon that standard MLPs trained with gradient descent exhibit a strong *spectral bias*. They learn low-frequency functions much more quickly and easily than high-frequency functions. Early stopping, which terminates training based on validation performance on a low-frequency dataset, will therefore select a model that has learned the low-frequency components but has not had sufficient training time to fit any high-frequency content.\n    *   **Generalization**: The model has no intrinsic mechanism to represent frequencies it has not seen. Its piecewise linear nature is ill-suited for extrapolating to a completely different, higher-frequency band ($\\mathcal{K}_{\\text{test}}$). The model will act as a low-pass filter and fail to represent the test signals.\n\n2.  **Sinusoidal Representation Network (SIREN) ($f_S$)**:\n    *   **Architecture Bias**: A SIREN uses the sine function as its activation. The entire network is a composition of sinusoids. This provides a powerful and natural inductive bias for representing oscillatory signals. A single layer can be seen as a form of generalized Fourier decomposition where the network learns the optimal frequencies, phases, and amplitudes.\n    *   **Learning Bias**: The SIREN architecture, coupled with its specific initialization scheme (controlling $\\omega_0$), is designed to mitigate the spectral bias of standard MLPs. It allows gradients to propagate without decay, enabling the network to learn both high and low frequencies effectively.\n    *   **Generalization**: Because its fundamental building block is a sinusoid, a SIREN learns a more fundamental representation of the signal itself, rather than a piecewise approximation. This allows it to generalize, or extrapolate, to unseen frequencies far more effectively than a ReLU network. It has learned *how to oscillate*, a skill that can be applied to new frequency regimes.\n\n### Step 3: Option-by-Option Evaluation\n\n*   **A**: This is incorrect. Universal approximation only guarantees that a representation *exists*, not that it is easy to find or efficient. The inductive biases are fundamentally different and are the key to generalization performance.\n*   **B**: This is correct. The SIREN's periodic inductive bias makes it highly efficient at representing sinusoids. Its architecture is specifically designed to overcome the spectral bias of ReLU networks, enabling it to learn and, more importantly, generalize to unseen high-frequency content.\n*   **C**: This is incorrect. Early stopping's effect is deeply intertwined with spectral bias. It explicitly favors the low-frequency solution that is learned first by the ReLU MLP, preventing it from ever fitting high frequencies. The two models have vastly different learning dynamics and will not perform equally.\n*   **D**: This is incorrect. Using fixed Fourier features with frequencies only up to $K_0$ provides a good basis for the training set but offers no information about the test set frequencies in $\\mathcal{K}_{\\text{test}}$ (where frequencies are $> K_0$). The model still has no basis functions to represent the test signals and will fail to extrapolate.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "In the modern era of deep learning, models are often trained in a highly overparameterized regime where the number of parameters far exceeds the number of data points, challenging classical learning theory. This can lead to the counter-intuitive \"double descent\" phenomenon, where test error first increases and then decreases as model complexity passes the point of data interpolation. This problem  challenges you to reason about this behavior within a multiscale context, analyzing how unresolved microscale physics and measurement noise contribute to a characteristic peak in test error.",
            "id": "3825655",
            "problem": "You are building a surrogate for a two-scale materials model using a Multilayer Perceptron (MLP). The goal is to predict the effective elastic modulus of a composite from descriptors of its microstructure and loading. The target map decomposes as $y = f_{\\ell}(x) + \\eta(x) + \\varepsilon$, where $f_{\\ell}$ is a macroscale component that is smooth in $x$, $\\eta$ is a microscale fluctuation component that appears high-frequency with respect to the learned representation, and $\\varepsilon$ is independent measurement noise with variance $\\sigma^{2}$. You train with squared loss, no explicit regularization, and in all settings you drive training error to (numerically) zero when possible by increasing optimization steps. Assume the wide-network limit justifies the Neural Tangent Kernel (NTK) approximation, so that gradient descent converges to the minimum-norm interpolant in the induced feature space when interpolation is feasible.\n\nYou collect $N$ independent and identically distributed input-output pairs $(x_{i},y_{i})$ drawn from a fixed training distribution over $x$. Consider two experimental families:\n\n- Family P: Fix $N$ and vary the number of trainable parameters $P$ from underparameterized ($P \\ll N$) to highly overparameterized ($P \\gg N$). Define the interpolation threshold in $P$ as the smallest $P$ at which the empirical training error first reaches zero.\n\n- Family N: Fix the architecture and its initialization to be sufficiently wide that the NTK is stable, and vary $N$ from small to large, defining the interpolation threshold in $N$ as the smallest $N$ at which the empirical training error first reaches zero for that fixed model and optimization protocol.\n\nIn both families, define the expected test mean-squared error as $R = \\mathbb{E}_{x,\\mathcal{D}}[(\\hat{f}(x)-f(x))^{2}]$, where $f(x) = f_{\\ell}(x) + \\eta(x)$, $\\hat{f}$ is the learned predictor, the expectation is over a fresh test input $x$ from the same distribution and over the random draw of the training dataset $\\mathcal{D}$, and $\\varepsilon$ contributes only through its effect on the learned $\\hat{f}$.\n\nStarting from the bias-variance decomposition $R = \\text{bias}^{2} + \\text{variance} + \\text{irreducible noise}$ and the linearization of training dynamics in the NTK feature space, reason about how the conditioning of the empirical Gram matrix and the energy in the microscale component $\\eta$ control the amplification of label noise and unresolved high-frequency content near interpolation. Use these principles to assess how $R$ behaves as a function of $P$ or $N$, and how multiscale structure (e.g., slow decay of kernel eigenvalues induced by a wide range of relevant length scales) modifies these behaviors.\n\nWhich of the following statements correctly characterize double descent with respect to dataset size and parameter count in this multiscale setting and predict regimes where the expected test error $R$ rises and then falls?\n\nA. In Family P, for fixed $N$ and increasing $P$, $R$ initially decreases (bias reduction), then increases near the interpolation threshold due to variance amplification from a nearly singular empirical normal matrix, and then decreases again for $P \\gg N$ as the minimum-norm implicit bias in the NTK regime suppresses high-frequency fitting; the height of the peak is larger when the combined high-frequency energy $\\mathbb{E}[\\eta(x)^{2}] + \\sigma^{2}$ is larger.\n\nB. In Family N, for a fixed very wide network in the ridgeless NTK regime, $R$ as a function of $N$ is strictly nonincreasing regardless of the spectrum of the kernel and the sizes of $\\mathbb{E}[\\eta(x)^{2}]$ and $\\sigma^{2}$, because more data always reduces both bias and variance without exception.\n\nC. The presence of a substantial microscale component $\\eta$ that is effectively high-frequency relative to the learned kernel increases the variance term near interpolation in both families, so that reducing the amplitude of $\\eta$ (e.g., by smoothing labels via homogenization) lowers or can even remove the double-descent peak, potentially restoring a monotone decrease of $R$.\n\nD. If training and test distributions differ only in the proportion of microscale content (with test having more $\\eta$ energy than train), then for Family P the double-descent peak in $R$ necessarily disappears at large $P$, because spectral bias of the MLP permanently favors low frequencies and prevents overfitting of high-frequency test content.\n\nE. When the NTK spectrum over the training distribution decays slowly, for example with eigenvalues $\\lambda_{j}$ obeying $\\lambda_{j} \\sim j^{-p}$ with small $p$ due to multiscale structure, the effective dimension is large and the empirical Gram matrix is ill-conditioned over a broader range near interpolation, making the rise-then-fall in $R$ more pronounced; adding explicit $\\ell_{2}$ regularization or early stopping narrows or suppresses this peak.\n\nSelect all that apply.",
            "solution": "The problem asks for an analysis of the expected test error, $R$, for a Multilayer Perceptron (MLP) surrogate model in a multiscale materials context, focusing on the double descent phenomenon. The analysis is to be performed within the framework of the Neural Tangent Kernel (NTK).\n\nFirst, let us establish the theoretical foundation based on the problem statement. We are in the wide-network limit, where the MLP's training dynamics under gradient descent are equivalent to a kernel method with the NTK, denoted $K(x, x')$. Since training is performed with squared loss, no explicit regularization, and is driven to zero error, the resulting predictor $\\hat{f}$ is the minimum-norm interpolant in the NTK's Reproducing Kernel Hilbert Space (RKHS). For a training dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$, this predictor is given by the formula:\n$$ \\hat{f}(x) = \\mathbf{k}(x)^T \\mathbf{K}^{-1} \\mathbf{y} $$\nwhere $\\mathbf{k}(x)$ is the vector of kernel evaluations $[K(x, x_1), \\dots, K(x, x_N)]^T$, $\\mathbf{K}$ is the $N \\times N$ Gram matrix with entries $K_{ij} = K(x_i, x_j)$, and $\\mathbf{y} = [y_1, \\dots, y_N]^T$ is the vector of training labels.\n\nThe labels $y_i$ are given by $y_i = f_{\\ell}(x_i) + \\eta(x_i) + \\varepsilon_i$. The true function we aim to learn is $f(x) = f_{\\ell}(x) + \\eta(x)$. Thus, the training label vector can be written as $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{f} = [f(x_1), \\dots, f(x_N)]^T$ and $\\boldsymbol{\\varepsilon} = [\\varepsilon_1, \\dots, \\varepsilon_N]^T$. The predictor is then:\n$$ \\hat{f}(x) = \\mathbf{k}(x)^T \\mathbf{K}^{-1} (\\mathbf{f} + \\boldsymbol{\\varepsilon}) $$\nThe expected test error is $R = \\mathbb{E}_{x,\\mathcal{D}}[(\\hat{f}(x) - f(x))^2]$. We can decompose this into bias and variance:\n$$ R = \\text{Bias}^2 + \\text{Variance} $$\nThe average predictor over draws of the noise $\\boldsymbol{\\varepsilon}$ (assuming fixed training inputs $x_i$) is $\\bar{f}(x) = \\mathbb{E}_{\\boldsymbol{\\varepsilon}}[\\hat{f}(x)] = \\mathbf{k}(x)^T \\mathbf{K}^{-1} \\mathbf{f}$.\nThe squared bias is $\\text{Bias}^2 = \\mathbb{E}_{x,\\mathcal{D}}[(\\bar{f}(x) - f(x))^2]$. This term measures the model's systematic inability to represent the true function $f(x) = f_{\\ell}(x) + \\eta(x)$. Due to the spectral bias of the NTK, which favors smooth, low-frequency functions, there will be a significant bias in approximating the high-frequency component $\\eta(x)$.\nThe variance is $\\text{Variance} = \\mathbb{E}_{x,\\mathcal{D}}[(\\hat{f}(x) - \\bar{f}(x))^2] = \\mathbb{E}_{x,\\mathcal{D}}[(\\mathbf{k}(x)^T \\mathbf{K}^{-1} \\boldsymbol{\\varepsilon})^2]$. Given that $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^T] = \\sigma^2 \\mathbf{I}$, this becomes:\n$$ \\text{Variance} = \\sigma^2 \\mathbb{E}_{\\mathcal{D}}[\\mathbf{k}(x)^T \\mathbf{K}^{-2} \\mathbf{k}(x)] $$\nThis variance term quantifies how much the predictor changes due to random noise $\\varepsilon$ in the training labels. Crucially, its magnitude is controlled by the matrix inverse $\\mathbf{K}^{-1}$ (or its square).\n\nThe double descent phenomenon arises from the behavior of $\\mathbf{K}^{-1}$ at the interpolation threshold. This threshold is reached when the model complexity (related to $P$ in Family P, or inversely to $N$ in Family N) is just sufficient to fit the $N$ training points. At this point, the Gram matrix $\\mathbf{K}$ becomes singular or nearly singular. The norms of $\\mathbf{K}^{-1}$ and $\\mathbf{K}^{-2}$ explode, causing a sharp peak in the variance term and, consequently, in the total test error $R$.\n\nFurthermore, the model is forced to interpolate $y_i = f(x_i) + \\varepsilon_i$. The deviation from the smooth part of the signal, $f_{\\ell}(x_i)$, is $\\eta(x_i) + \\varepsilon_i$. The model must fit this combined \"effective noise\". The high-frequency nature of $\\eta(x)$ makes it act like noise from the perspective of the smooth kernel, so the challenge of fitting $\\eta$ contributes to the overfitting problem near the peak, similarly to fitting $\\varepsilon$.\n\nWith this foundation, we can analyze each statement.\n\n**A. In Family P, for fixed $N$ and increasing $P$, $R$ initially decreases (bias reduction), then increases near the interpolation threshold due to variance amplification from a nearly singular empirical normal matrix, and then decreases again for $P \\gg N$ as the minimum-norm implicit bias in the NTK regime suppresses high-frequency fitting; the height of the peak is larger when the combined high-frequency energy $\\mathbb{E}[\\eta(x)^{2}] + \\sigma^{2}$ is larger.**\n\nThis statement accurately describes the model-wise double descent curve.\n1.  For $P \\ll N$ (underparameterized), increasing $P$ reduces bias faster than variance increases, so $R$ decreases. This is the classical regime.\n2.  Near the interpolation threshold ($P \\approx N_{crit}$), the model just gains enough capacity to interpolate. The Gram matrix $\\mathbf{K}$ becomes ill-conditioned, its inverse blows up, and the variance term spikes dramatically.\n3.  For $P \\gg N$ (overparameterized), we are in the NTK regime where gradient descent finds the minimum RKHS-norm interpolant. This implicit regularization favors smoother solutions, taming the variance. $R$ decreases again.\nThe peak's height is determined by the magnitude of what is being overfitted. The model is forced to fit the total deviation from a smooth function, which includes both the measurement noise $\\varepsilon$ and the structural high-frequency component $\\eta$. The total variance of this effective noise is $\\text{Var}(\\eta(x) + \\varepsilon) = \\mathbb{E}[\\eta(x)^2] + \\sigma^2$ (assuming independence). A larger value for this sum means there is more high-frequency content to be fit by an unstable model, leading to a higher variance peak. The statement is entirely correct.\nVerdict: **Correct**.\n\n**B. In Family N, for a fixed very wide network in the ridgeless NTK regime, $R$ as a function of $N$ is strictly nonincreasing regardless of the spectrum of the kernel and the sizes of $\\mathbb{E}[\\eta(x)^{2}]$ and $\\sigma^{2}$, because more data always reduces both bias and variance without exception.**\n\nThis statement claims that sample-wise double descent does not occur. This is factually incorrect. Double descent is also observed as a function of the number of samples, $N$, for a fixed, highly overparameterized model. The mechanism is analogous to model-wise double descent. For a fixed model, there is an effective complexity or dimension, let's call it $d_{eff}$.\n1.  When $N \\ll d_{eff}$, the model is effectively overparameterized, and we are in the \"modern\" interpolation regime.\n2.  As $N$ increases and approaches $d_{eff}$, the Gram matrix $\\mathbf{K}$ again becomes ill-conditioned, leading to a variance spike and a peak in test error $R$.\n3.  When $N \\gg d_{eff}$, the model becomes underparameterized relative to the data, and we enter the classical learning regime where adding more data monotonically improves performance.\nThe claim that \"more data always reduces both bias and variance without exception\" is a principle from classical statistics that is violated in the modern overparameterized setting near this interpolation threshold.\nVerdict: **Incorrect**.\n\n**C. The presence of a substantial microscale component $\\eta$ that is effectively high-frequency relative to the learned kernel increases the variance term near interpolation in both families, so that reducing the amplitude of $\\eta$ (e.g., by smoothing labels via homogenization) lowers or can even remove the double-descent peak, potentially restoring a monotone decrease of $R$.**\n\nThis statement correctly identifies the role of the microscale component $\\eta$. As established, $\\eta$ acts as a form of structural or model-misspecification noise. To achieve zero training error, the model must interpolate the values $\\eta(x_i)$ at the training points. Because $\\eta$ is high-frequency, this requires a complex, high-norm solution that generalizes poorly, contributing to the error peak near interpolation. The total \"noise\" that is amplified by the ill-conditioned inverse $\\mathbf{K}^{-1}$ is effectively a combination of $\\eta$ and $\\varepsilon$. If the amplitude of $\\eta$ is reduced (e.g., by filtering or homogenization of the labels, which would produce a smoother target function), the magnitude of the component being overfitted is smaller. This would directly lead to a lower variance spike. If the combined effective noise from $\\eta$ and $\\varepsilon$ is small enough, the peak could become negligible, and the test error curve might appear monotonic.\nVerdict: **Correct**.\n\n**D. If training and test distributions differ only in the proportion of microscale content (with test having more $\\eta$ energy than train), then for Family P the double-descent peak in $R$ necessarily disappears at large $P$, because spectral bias of the MLP permanently favors low frequencies and prevents overfitting of high-frequency test content.**\n\nThis statement contains several flaws. First, the double-descent peak occurs near the interpolation threshold, not at \"large $P$\". At large $P \\gg N$, the model is in the second descent phase, past the peak. The reasoning provided is also flawed. The model is trained on the training distribution (low $\\eta$ content). In the overparameterized NTK regime ($P \\gg N$), the minimum-norm bias will produce a smooth function $\\hat{f}$ that interpolates the training data. Because the training data is smooth, $\\hat{f}$ itself will be smooth. When this smooth predictor $\\hat{f}(x)$ is evaluated on the test distribution, where the true function $f_{test}(x) = f_{\\ell}(x) + \\eta_{test}(x)$ has high energy in $\\eta_{test}$, there will be a large mismatch. The test error $R = \\mathbb{E}[(\\hat{f}(x) - f_{test}(x))^2]$ will be large, dominated by the energy of the portion of the function the model could not learn, i.e., approximately $\\mathbb{E}[\\eta_{test}(x)^2]$. The spectral bias does not prevent high error; on the contrary, it causes a large bias with respect to the high-frequency test function, leading to high test error. The model is not \"overfitting test content\"; it is simply a poor model for the test distribution.\nVerdict: **Incorrect**.\n\n**E. When the NTK spectrum over the training distribution decays slowly, for example with eigenvalues $\\lambda_{j}$ obeying $\\lambda_{j} \\sim j^{-p}$ with small $p$ due to multiscale structure, the effective dimension is large and the empirical Gram matrix is ill-conditioned over a broader range near interpolation, making the rise-then-fall in $R$ more pronounced; adding explicit $\\ell_{2}$ regularization or early stopping narrows or suppresses this peak.**\n\nThis statement is a sophisticated and correct analysis of the effect of the kernel's spectral properties.\n1.  A slow decay of eigenvalues $\\lambda_j$ (small $p$ in $\\lambda_j \\sim j^{-p}$) indicates that the kernel can represent complex, \"rough\" functions. This is typical for problems with multiscale structure.\n2.  A slow spectral decay implies a large effective dimension, as many eigenmodes are significant.\n3.  This means the transition from the overparameterized regime ($N \\ll d_{eff}$) to the underparameterized regime ($N \\gg d_{eff}$) is less sharp. The Gram matrix $\\mathbf{K}$ will be ill-conditioned over a wider range of $N$ (or $P$), making the double-descent peak broader and more \"pronounced.\"\n4.  Explicit $\\ell_2$ regularization (ridge regression) replaces $\\mathbf{K}^{-1}$ with $(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}$, which is well-conditioned for any $\\lambda > 0$. This directly regularizes the inverse, taming the variance explosion and suppressing the peak.\n5.  Early stopping of gradient descent is a form of implicit regularization that is known to be equivalent to a spectral filtering operation, preventing the fit from converging to the noisy, high-frequency components. This also tames the variance and suppresses the peak.\nThe entire statement is consonant with advanced learning theory.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}