{
    "hands_on_practices": [
        {
            "introduction": "本练习是理解相对熵最小化方法的基础。它将最小化相对熵这个抽象概念与一个具体而直观的原则——矩匹配（moment matching）联系起来。通过亲手推导KL散度的梯度，您将明白为何将梯度设为零会使粗粒度模型的矩与真实底层分布的相应矩相匹配 ()。这个练习为所有基于优化的粗粒度方法奠定了理论基石。",
            "id": "3802834",
            "problem": "考虑平衡统计力学中的粗粒化（CG）。令 $Y$ 表示一个粗粒化可观测量，其取值于 $\\mathbb{R}^{d}$，目标粗粒分布为 $P_{Y}$（一个细尺度平衡测度的边缘分布）。我们通过一个基于能量的参数族 $Q_{\\theta}$ 来近似 $P_{Y}$，其密度为\n$$\nq_{\\theta}(y) \\equiv \\frac{1}{Z(\\theta)} \\exp\\!\\big(-U_{\\theta}(y)\\big), \\quad Z(\\theta) \\equiv \\int_{\\mathbb{R}^{d}} \\exp\\!\\big(-U_{\\theta}(y)\\big)\\,\\mathrm{d}y,\n$$\n其中 $U_{\\theta}:\\mathbb{R}^{d}\\to\\mathbb{R}$ 是一个可微的粗粒势，$\\theta \\in \\mathbb{R}^{p}$ 是参数向量。近似的质量由从 $P_{Y}$ 到 $Q_{\\theta}$ 的 Kullback–Leibler (KL) 散度（也称为相对熵）来量化，\n$$\nD_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) \\equiv \\int_{\\mathbb{R}^{d}} p_{Y}(y)\\,\\ln\\!\\left(\\frac{p_{Y}(y)}{q_{\\theta}(y)}\\right)\\,\\mathrm{d}y.\n$$\n\n任务 A. 仅从上述定义和基础微积分出发，推导梯度 $\\nabla_{\\theta} D_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta})$ 的表达式，用在 $P_{Y}$ 和 $Q_{\\theta}$ 下的期望值来表示，并解释为什么当 $U_{\\theta}$ 在一组充分统计量中是线性时，平稳条件 $\\nabla_{\\theta} D_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) = 0$ 蕴含了一个矩匹配原理。你的推导不得假设或引用任何关于梯度或配分函数 $Z(\\theta)$ 的已知公式，并且必须明确说明任何微分和积分交换的合理性。\n\n任务 B. 特化到 $d=1$ 的情况和具有以下能量的高斯粗粒化模型\n$$\nU_{(\\mu,\\lambda)}(y) \\equiv \\frac{\\lambda}{2}\\,(y-\\mu)^{2}, \\quad \\lambda>0,\n$$\n使得 $Q_{(\\mu,\\lambda)}$ 是一个均值为 $\\mu$、方差为 $\\lambda^{-1}$ 的单变量正态分布。假设 $P_{Y}$ 具有有限的前两阶矩\n$$\nm \\equiv \\mathbb{E}_{P_{Y}}[Y], \\quad v \\equiv \\mathrm{Var}_{P_{Y}}(Y)>0.\n$$\n使用你从任务 A 中得到的结果，确定 $D_{\\mathrm{KL}}(P_{Y}\\|Q_{(\\mu,\\lambda)})$ 的精确最小化子 $(\\mu^{\\star},\\lambda^{\\star})$，以 $m$ 和 $v$ 的函数形式给出闭式解。将你的最终答案表示为一个行向量 $\\big(\\mu^{\\star}\\ \\ \\lambda^{\\star}\\big)$，不带单位。不需要数值舍入。",
            "solution": "我们从第一性原理出发。根据定义，从 $P_{Y}$ 到 $Q_{\\theta}$ 的 Kullback–Leibler (KL) 散度（相对熵）为\n$$\nD_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) \\equiv \\int_{\\mathbb{R}^{d}} p_{Y}(y)\\,\\ln\\!\\left(\\frac{p_{Y}(y)}{q_{\\theta}(y)}\\right)\\,\\mathrm{d}y.\n$$\n使用 $q_{\\theta}(y) = Z(\\theta)^{-1} \\exp(-U_{\\theta}(y))$，我们有\n$$\n\\ln q_{\\theta}(y) = -U_{\\theta}(y) - \\ln Z(\\theta),\n$$\n因此\n$$\nD_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) = \\int p_{Y}(y)\\,\\big(\\ln p_{Y}(y) + U_{\\theta}(y) + \\ln Z(\\theta)\\big)\\,\\mathrm{d}y.\n$$\n由于 $\\int p_{Y}(y)\\,\\ln p_{Y}(y)\\,\\mathrm{d}y$ 与 $\\theta$ 无关，唯一与 $\\theta$ 相关的项是涉及 $U_{\\theta}(y)$ 和 $\\ln Z(\\theta)$ 的项。假设正则性条件允许在积分号下进行微分（例如，控制收敛定理以及 $U_{\\theta}$ 在某邻域内对 $\\theta$ 一致的可积控制函数下的可微性），我们得到\n$$\n\\nabla_{\\theta} D_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) = \\int p_{Y}(y)\\,\\nabla_{\\theta} U_{\\theta}(y)\\,\\mathrm{d}y + \\nabla_{\\theta} \\ln Z(\\theta).\n$$\n我们现在从定义 $Z(\\theta)=\\int \\exp(-U_{\\theta}(y))\\,\\mathrm{d}y$ 计算 $\\nabla_{\\theta}\\ln Z(\\theta)$。再次在相同的正则性条件下，\n$$\n\\nabla_{\\theta} Z(\\theta) = \\int -\\nabla_{\\theta}U_{\\theta}(y)\\,\\exp\\!\\big(-U_{\\theta}(y)\\big)\\,\\mathrm{d}y,\n$$\n因此\n$$\n\\nabla_{\\theta}\\ln Z(\\theta) = \\frac{\\nabla_{\\theta}Z(\\theta)}{Z(\\theta)} \n= \\frac{\\int -\\nabla_{\\theta}U_{\\theta}(y)\\,\\exp(-U_{\\theta}(y))\\,\\mathrm{d}y}{\\int \\exp(-U_{\\theta}(y))\\,\\mathrm{d}y}\n= -\\int \\nabla_{\\theta}U_{\\theta}(y)\\,q_{\\theta}(y)\\,\\mathrm{d}y.\n$$\n因此，\n$$\n\\nabla_{\\theta} D_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta}) \n= \\int p_{Y}(y)\\,\\nabla_{\\theta} U_{\\theta}(y)\\,\\mathrm{d}y - \\int q_{\\theta}(y)\\,\\nabla_{\\theta} U_{\\theta}(y)\\,\\mathrm{d}y\n= \\mathbb{E}_{P_{Y}}\\big[\\nabla_{\\theta}U_{\\theta}(Y)\\big] - \\mathbb{E}_{Q_{\\theta}}\\big[\\nabla_{\\theta}U_{\\theta}(Y)\\big].\n$$\n\n这就严格地从定义出发，建立了梯度的期望形式。\n\n为了说明为什么平稳条件蕴含矩匹配原理，假设 $U_{\\theta}$ 在一组充分统计量 $\\{\\phi_{i}(y)\\}_{i=1}^{p}$ 中是线性的，即\n$$\nU_{\\theta}(y) = \\sum_{i=1}^{p} \\theta_{i}\\,\\phi_{i}(y),\n$$\n或者更紧凑地写为 $U_{\\theta}(y)=\\theta^{\\top}\\phi(y)$，其中 $\\phi(y)\\in\\mathbb{R}^{p}$。那么 $\\nabla_{\\theta}U_{\\theta}(y) = \\phi(y)$，平稳性条件 $\\nabla_{\\theta} D_{\\mathrm{KL}}(P_{Y}\\|Q_{\\theta})=0$ 可写为\n$$\n\\mathbb{E}_{P_{Y}}\\big[\\phi(Y)\\big] - \\mathbb{E}_{Q_{\\theta}}\\big[\\phi(Y)\\big] = 0,\n$$\n即，每个分量都满足\n$$\n\\mathbb{E}_{P_{Y}}\\big[\\phi_{i}(Y)\\big] = \\mathbb{E}_{Q_{\\theta}}\\big[\\phi_{i}(Y)\\big], \\quad i\\in\\{1,\\dots,p\\}.\n$$\n这些是矩匹配方程：模型 $Q_{\\theta}$ 在由充分统计量 $\\phi$ 诱导的矩上与目标 $P_{Y}$ 相匹配。\n\n我们现在将此应用于任务 B 中的单变量高斯能量模型。令 $d=1$ 且\n$$\nU_{(\\mu,\\lambda)}(y) = \\frac{\\lambda}{2}\\,(y-\\mu)^{2}, \\quad \\lambda>0.\n$$\n相应的模型 $Q_{(\\mu,\\lambda)}$ 是均值为 $\\mu$、方差为 $\\lambda^{-1}$ 的正态分布。我们计算参数梯度：\n$$\n\\frac{\\partial U_{(\\mu,\\lambda)}(y)}{\\partial \\mu} = -\\lambda\\,(y-\\mu), \n\\quad \n\\frac{\\partial U_{(\\mu,\\lambda)}(y)}{\\partial \\lambda} = \\frac{1}{2}\\,(y-\\mu)^{2}.\n$$\n根据一般的梯度恒等式，平稳性条件是\n$$\n\\mathbb{E}_{P_{Y}}\\!\\left[-\\lambda\\,(Y-\\mu)\\right] - \\mathbb{E}_{Q_{(\\mu,\\lambda)}}\\!\\left[-\\lambda\\,(Y-\\mu)\\right] = 0,\n\\quad\n\\mathbb{E}_{P_{Y}}\\!\\left[\\frac{1}{2}\\,(Y-\\mu)^{2}\\right] - \\mathbb{E}_{Q_{(\\mu,\\lambda)}}\\!\\left[\\frac{1}{2}\\,(Y-\\mu)^{2}\\right] = 0.\n$$\n第一个方程简化为\n$$\n-\\lambda\\big(\\mathbb{E}_{P_{Y}}[Y]-\\mu\\big) + \\lambda\\big(\\mathbb{E}_{Q_{(\\mu,\\lambda)}}[Y]-\\mu\\big) = 0.\n$$\n由于 $\\mathbb{E}_{Q_{(\\mu,\\lambda)}}[Y] = \\mu$，这简化为 $\\mathbb{E}_{P_{Y}}[Y] = \\mu$。使用符号 $m \\equiv \\mathbb{E}_{P_{Y}}[Y]$，我们得出结论\n$$\n\\mu^{\\star} = m.\n$$\n第二个平稳性方程变为\n$$\n\\mathbb{E}_{P_{Y}}\\!\\left[(Y-\\mu)^{2}\\right] = \\mathbb{E}_{Q_{(\\mu,\\lambda)}}\\!\\left[(Y-\\mu)^{2}\\right].\n$$\n对于 $Q_{(\\mu,\\lambda)}$，我们有 $\\mathbb{E}_{Q_{(\\mu,\\lambda)}}[(Y-\\mu)^{2}] = \\mathrm{Var}_{Q_{(\\mu,\\lambda)}}(Y) = \\lambda^{-1}$。代入 $\\mu^{\\star} = m$ 得到\n$$\n\\mathbb{E}_{P_{Y}}\\!\\left[(Y-m)^{2}\\right] = \\lambda^{-1}.\n$$\n使用 $v \\equiv \\mathrm{Var}_{P_{Y}}(Y) = \\mathbb{E}_{P_{Y}}[(Y-m)^{2}]>0$，我们得到\n$$\n\\lambda^{\\star} = \\frac{1}{v}.\n$$\n因此，唯一的最小化子是\n$$\n\\big(\\mu^{\\star},\\lambda^{\\star}\\big) = \\big(m,\\, v^{-1}\\big),\n$$\n这明确地实现了矩匹配原理：最优的高斯分布 $Q_{(\\mu,\\lambda)}$ 与目标分布 $P_{Y}$ 的均值和方差相匹配。",
            "answer": "$$\\boxed{\\begin{pmatrix} m  \\frac{1}{v} \\end{pmatrix}}$$"
        },
        {
            "introduction": "从解析理论转向计算实践，本练习将指导您从第一性原理出发，构建一个离散的粗粒度模型。您将对一个连续系统进行粗粒化，建立带正则化的相对熵目标函数，并使用数值优化方法寻找最优参数 ()。这个实践模拟了现代多尺度建模中的一个通用工作流程，是连接理论与应用的关键一步。",
            "id": "3802808",
            "problem": "考虑一个一维粒子，其位置为 $x \\in \\mathbb{R}$，在一个由双阱势给出的势能景观中演化。粗粒化 (CG) 映射将连续位置 $x$ 划分到有限数量的离散区间中。目标是构建一个离散 CG 模型，并通过最小化正则化的 Kullback-Leibler 散度 (KLD) 来确定其最优对数概率。您编写的程序必须从第一性原理和经过充分检验的定义出发，实现以下步骤。\n\n1. 基本基底与目标分布。设双阱势为 $U(x) = a\\,(x^2 - b^2)^2$，其中 $a > 0$ 和 $b > 0$ 是参数。在逆温度 $\\beta > 0$ 下的热平衡分布是玻尔兹曼分布\n$$\np(x) = \\frac{1}{Z}\\,\\exp\\left(-\\beta\\,U(x)\\right),\n$$\n其配分函数为\n$$\nZ = \\int_{x_{\\min}}^{x_{\\max}} \\exp\\left(-\\beta\\,U(x)\\right)\\,\\mathrm{d}x,\n$$\n其中 $[x_{\\min}, x_{\\max}]$ 是一个有限域，已知对于所考虑的参数，该域基本包含了 $p(x)$ 的所有概率质量。\n\n2. 到离散区间的粗粒化映射。将区间 $[x_{\\min}, x_{\\max}]$ 划分为 $K$ 个等宽区间，其边界为 $x_0 = x_{\\min}  x_1  \\cdots  x_K = x_{\\max}$。CG 变量 $i \\in \\{1,\\dots,K\\}$ 表示区间索引。区间 $i$ 的精细到粗粒的边缘概率为\n$$\nP_i = \\int_{x_{i-1}}^{x_{i}} p(x)\\,\\mathrm{d}x,\n$$\n这定义了一个离散概率质量函数 $\\mathbf{P} = (P_1,\\dots,P_K)$，满足 $\\sum_{i=1}^{K} P_i = 1$ 和 $P_i \\ge 0$。\n\n3. 离散模型参数化。通过无约束的对数概率 $\\mathbf{l} = (l_1,\\dots,l_K)$，根据 softmax 关系对 CG 模型分布 $\\mathbf{Q} = (Q_1,\\dots,Q_K)$ 进行参数化\n$$\nQ_i(\\mathbf{l}) = \\frac{\\exp(l_i)}{\\sum_{j=1}^{K} \\exp(l_j)}.\n$$\n这确保了 $Q_i \\ge 0$ 和 $\\sum_{i=1}^K Q_i = 1$。\n\n4. 带正则化的相对熵最小化目标。定义从精细粒度的 CG 边缘分布 $\\mathbf{P}$ 到模型 $\\mathbf{Q}(\\mathbf{l})$ 的 Kullback-Leibler 散度 (KLD) 为\n$$\nD_{\\mathrm{KL}}(\\mathbf{P}\\,\\|\\,\\mathbf{Q}(\\mathbf{l})) = \\sum_{i=1}^{K} P_i \\log\\left(\\frac{P_i}{Q_i(\\mathbf{l})}\\right).\n$$\n为防止过拟合并打破 $\\mathbf{l}$ 的规范不变性，引入一个相对于参考对数概率向量 $\\mathbf{l}^{\\mathrm{ref}}$ 的二次正则化，其强度为 $\\lambda > 0$。完整目标是\n$$\n\\mathcal{F}(\\mathbf{l}) = D_{\\mathrm{KL}}(\\mathbf{P}\\,\\|\\,\\mathbf{Q}(\\mathbf{l})) + \\lambda \\sum_{i=1}^{K} \\left(l_i - l_i^{\\mathrm{ref}}\\right)^2.\n$$\n您必须在 $\\mathbf{l} \\in \\mathbb{R}^K$ 上最小化 $\\mathcal{F}(\\mathbf{l})$，以计算最优对数概率 $\\mathbf{l}^\\star$。\n\n5. 数值计算要求。\n- 通过在每个区间上对 $\\exp(-\\beta U(x))$ 进行数值积分，并计算在 $[x_{\\min}, x_{\\max}]$ 上的配分函数 $Z$ 以确保正确归一化，来计算区间概率 $\\mathbf{P}$。使用足够精确的数值求积方法。\n- 使用稳健的、基于梯度的凸优化过程来最小化 $\\mathcal{F}(\\mathbf{l})$。您必须明确构建目标函数 $\\mathcal{F}(\\mathbf{l})$ 及其关于 $\\mathbf{l}$ 的梯度，并在优化器中使用它们。\n\n6. 测试套件。实现您的程序，以计算以下测试用例的 $\\mathbf{l}^\\star$，在指定的域上使用等宽区间和给定的参考对数概率。对于每个案例，以十进制浮点数列表的形式输出最优对数概率向量 $\\mathbf{l}^\\star$。输出不需要物理单位。\n- 案例 A (顺利路径)：$a = 1.0$, $b = 1.0$, $\\beta = 5.0$, $x_{\\min} = -2.5$, $x_{\\max} = 2.5$, $K = 5$, $\\lambda = 0.1$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0,0)$。\n- 案例 B (近均匀边缘分布，非常弱的正则化)：$a = 1.0$, $b = 1.0$, $\\beta = 0.1$, $x_{\\min} = -3.0$, $x_{\\max} = 3.0$, $K = 4$, $\\lambda = 10^{-6}$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0)$。\n- 案例 C (带有偏斜参考的强正则化)：$a = 1.0$, $b = 1.0$, $\\beta = 2.0$, $x_{\\min} = -2.0$, $x_{\\max} = 2.0$, $K = 6$, $\\lambda = 10.0$, $\\mathbf{l}^{\\mathrm{ref}} = (-1,0,1,2,1,0)$。\n- 案例 D (低温、尖锐双峰)：$a = 1.0$, $b = 1.0$, $\\beta = 50.0$, $x_{\\min} = -2.0$, $x_{\\max} = 2.0$, $K = 8$, $\\lambda = 0.01$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0,0,0,0,0)$。\n\n7. 最终输出格式。您的程序应生成单行输出，其中包含上述四个案例的结果列表，格式如下：一个由逗号分隔的浮点数列表组成的列表，包含在一对单独的方括号内，每个内部列表对应一个案例的 $\\mathbf{l}^\\star$，且不含任何额外文本。例如：\n$$\n\\text{print } [[l^\\star_{A,1},\\dots,l^\\star_{A,K_A}],[l^\\star_{B,1},\\dots,l^\\star_{B,K_B}],\\dots].\n$$\n所有角度（如果有）必须以弧度为单位；然而，此问题中没有出现角度。输出必须是十进制浮点数。输出的任何位置都不允许出现百分号。数值积分和优化必须以足够的精度执行，以产生稳定的结果。",
            "solution": "该问题要求通过最小化正则化的相对熵（Kullback-Leibler 散度）来确定粗粒化（CG）模型的最优对数概率。该过程涉及多个步骤，从定义底层连续系统到为离散模型参数执行数值优化。\n\n### 步骤1：目标概率密度与粗粒化\n\n系统是一个在一维双阱势 $U(x)$ 中的粒子，由下式给出：\n$$\nU(x) = a(x^2 - b^2)^2\n$$\n其中 $a > 0$ 且 $b > 0$。在逆温度 $\\beta > 0$ 的热平衡状态下，粒子的位置 $x$ 在指定域 $[x_{\\min}, x_{\\max}]$ 上遵循玻尔兹曼分布：\n$$\np(x) = \\frac{1}{Z} \\exp(-\\beta U(x))\n$$\n归一化常数，即配分函数，是未归一化的玻尔兹曼因子在该域上的积分：\n$$\nZ = \\int_{x_{\\min}}^{x_{\\max}} \\exp(-\\beta U(x)) \\, \\mathrm{d}x\n$$\n粗粒化过程将连续域 $[x_{\\min}, x_{\\max}]$ 映射到 $K$ 个离散区间的集合中。该区间由 $K+1$ 个等间距点 $x_0, x_1, \\dots, x_K$ 划分，其中 $x_0 = x_{\\min}$ 且 $x_K = x_{\\max}$。每个区间的宽度为 $\\Delta x = (x_{\\max} - x_{\\min}) / K$，第 $i$ 个区间对应于区间 $[x_{i-1}, x_i]$。\n\nCG模型的目标概率质量函数，表示为 $\\mathbf{P} = (P_1, \\dots, P_K)$，通过对每个区间上的精细粒度概率密度 $p(x)$ 进行积分得出：\n$$\nP_i = \\int_{x_{i-1}}^{x_i} p(x) \\, \\mathrm{d}x = \\frac{1}{Z} \\int_{x_{i-1}}^{x_i} \\exp(-\\beta U(x)) \\, \\mathrm{d}x\n$$\n这些积分必须进行数值计算。一种稳健的自适应求积方法，例如 `scipy.integrate.quad` 提供的方法，适用于此任务。计算过程首先计算 $Z$，然后计算每个区间 $i$ 的积分，并除以 $Z$。\n\n### 步骤2：粗粒化模型与目标函数\n\nCG 模型分布 $\\mathbf{Q} = (Q_1, \\dots, Q_K)$ 由一个无约束的对数概率向量 $\\mathbf{l} = (l_1, \\dots, l_K)$ 参数化。softmax 函数确保 $\\mathbf{Q}$ 是一个有效的概率分布（即 $Q_i \\ge 0$ 且 $\\sum_i Q_i = 1$）：\n$$\nQ_i(\\mathbf{l}) = \\frac{\\exp(l_i)}{\\sum_{j=1}^{K} \\exp(l_j)}\n$$\n如果 $l_j$ 的值很大或很小，$\\sum_{j=1}^{K} \\exp(l_j)$ 这一项可能在数值上不稳定。其对数是 log-sum-exp 函数，$\\text{logsumexp}(\\mathbf{l}) = \\log(\\sum_j \\exp(l_j))$，可以以稳定的方式计算。利用这一点，我们可以写出 $\\log Q_i(\\mathbf{l}) = l_i - \\text{logsumexp}(\\mathbf{l})$。\n\n目标是找到最优参数 $\\mathbf{l}^\\star$，使模型分布 $\\mathbf{Q}(\\mathbf{l})$ 尽可能接近目标分布 $\\mathbf{P}$。这种“接近度”通过 Kullback-Leibler 散度 (KLD) 来衡量：\n$$\nD_{\\mathrm{KL}}(\\mathbf{P} \\,\\|\\, \\mathbf{Q}(\\mathbf{l})) = \\sum_{i=1}^{K} P_i \\log\\left(\\frac{P_i}{Q_i(\\mathbf{l})}\\right) = \\sum_{i=1}^{K} P_i (\\log P_i - \\log Q_i(\\mathbf{l}))\n$$\n问题指定在目标函数中添加一个二次正则化项，该项惩罚 $\\mathbf{l}$ 与参考向量 $\\mathbf{l}^{\\mathrm{ref}}$ 的偏差。待最小化的完整目标函数为：\n$$\n\\mathcal{F}(\\mathbf{l}) = D_{\\mathrm{KL}}(\\mathbf{P} \\,\\|\\, \\mathbf{Q}(\\mathbf{l})) + \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2\n$$\n其中 $\\lambda > 0$ 是正则化强度。当 $\\lambda > 0$ 时，该目标函数 $\\mathcal{F}(\\mathbf{l})$ 是严格凸的，保证了唯一的最小化子 $\\mathbf{l}^\\star$。\n\n### 步骤3：基于梯度的优化\n\n为了高效地最小化 $\\mathcal{F}(\\mathbf{l})$，我们使用基于梯度的优化算法，例如 L-BFGS-B。这需要 $\\mathcal{F}(\\mathbf{l})$ 相对于 $\\mathbf{l}$ 的每个分量 $l_k$ 的梯度。\n\n让我们推导梯度 $\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l})$。我们将 $\\mathcal{F}(\\mathbf{l})$ 对分量 $l_k$ 求导：\n$$\n\\frac{\\partial \\mathcal{F}}{\\partial l_k} = \\frac{\\partial}{\\partial l_k} \\left( \\sum_{i=1}^{K} P_i \\log P_i - \\sum_{i=1}^{K} P_i \\log Q_i(\\mathbf{l}) \\right) + \\frac{\\partial}{\\partial l_k} \\left( \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2 \\right)\n$$\n项 $\\sum P_i \\log P_i$ 相对于 $\\mathbf{l}$ 是常数。正则化项的导数很简单：\n$$\n\\frac{\\partial}{\\partial l_k} \\left( \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2 \\right) = 2\\lambda (l_k - l_k^{\\mathrm{ref}})\n$$\n现在我们关注涉及 $\\mathbf{Q}(\\mathbf{l})$ 的 KLD 项。回顾 $\\log Q_i(\\mathbf{l}) = l_i - \\text{logsumexp}(\\mathbf{l})$，其对目标的贡献是：\n$$\n-\\sum_{i=1}^{K} P_i (l_i - \\text{logsumexp}(\\mathbf{l})) = -\\sum_{i=1}^{K} P_i l_i + \\left(\\sum_{i=1}^{K} P_i\\right) \\text{logsumexp}(\\mathbf{l}) = -\\sum_{i=1}^{K} P_i l_i + \\text{logsumexp}(\\mathbf{l})\n$$\n这里我们使用了 $\\sum_i P_i = 1$ 的事实。该表达式对 $l_k$ 的导数是：\n$$\n\\frac{\\partial}{\\partial l_k} \\left( -\\sum_{i=1}^{K} P_i l_i + \\text{logsumexp}(\\mathbf{l}) \\right) = -P_k + \\frac{\\partial}{\\partial l_k} \\log\\left(\\sum_{j=1}^{K} \\exp(l_j)\\right)\n$$\n已知 log-sum-exp 函数对 $l_k$ 的导数是 $Q_k(\\mathbf{l})$：\n$$\n\\frac{\\partial}{\\partial l_k} \\log\\left(\\sum_{j=1}^{K} \\exp(l_j)\\right) = \\frac{1}{\\sum_{j} \\exp(l_j)} \\cdot \\exp(l_k) = Q_k(\\mathbf{l})\n$$\n因此，KLD 项的导数是 $Q_k(\\mathbf{l}) - P_k$。结合所有部分，梯度的第 $k$ 个分量是：\n$$\n\\frac{\\partial \\mathcal{F}}{\\partial l_k} = Q_k(\\mathbf{l}) - P_k + 2\\lambda (l_k - l_k^{\\mathrm{ref}})\n$$\n用向量表示法，完整梯度为：\n$$\n\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l}) = \\mathbf{Q}(\\mathbf{l}) - \\mathbf{P} + 2\\lambda (\\mathbf{l} - \\mathbf{l}^{\\mathrm{ref}})\n$$\n\n### 步骤4：算法实现\n\n每个测试用例的总体算法如下：\n1.  定义势 $U(x)$、被积函数 $f(x) = \\exp(-\\beta U(x))$ 和域 $[x_{\\min}, x_{\\max}]$。\n2.  使用数值求积计算配分函数 $Z = \\int_{x_{\\min}}^{x_{\\max}} f(x) \\, \\mathrm{d}x$。\n3.  在 $[x_{\\min}, x_{\\max}]$ 上建立 $K$ 个区间边界 $x_0, \\dots, x_K$。\n4.  通过在每个区间 $[x_{i-1}, x_i]$ 上对 $f(x)$ 进行数值积分并除以 $Z$，计算目标概率向量 $\\mathbf{P}$。\n5.  实现一个函数，该函数以 $\\mathbf{l}$ 为参数，使用上面推导的表达式和 `logsumexp` 等数值稳定函数，返回目标值 $\\mathcal{F}(\\mathbf{l})$ 及其梯度 $\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l})$。\n6.  使用数值优化器，例如带有 `L-BFGS-B` 方法的 `scipy.optimize.minimize`，找到最小化 $\\mathcal{F}(\\mathbf{l})$ 的向量 $\\mathbf{l}^\\star$。将步骤5中的函数以及初始猜测值（例如 $\\mathbf{l}_0 = \\mathbf{l}^{\\mathrm{ref}}$）提供给优化器。\n7.  优化的结果 $\\mathbf{l}^\\star$ 是给定测试用例的解。对所有提供的测试用例重复此过程。",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Computes the optimal log-probabilities for a coarse-grained model by minimizing\n    a regularized Kullback-Leibler divergence.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {'a': 1.0, 'b': 1.0, 'beta': 5.0, 'xmin': -2.5, 'xmax': 2.5,\n         'K': 5, 'lam': 0.1, 'l_ref': [0.0, 0.0, 0.0, 0.0, 0.0]},\n        # Case B\n        {'a': 1.0, 'b': 1.0, 'beta': 0.1, 'xmin': -3.0, 'xmax': 3.0,\n         'K': 4, 'lam': 1e-6, 'l_ref': [0.0, 0.0, 0.0, 0.0]},\n        # Case C\n        {'a': 1.0, 'b': 1.0, 'beta': 2.0, 'xmin': -2.0, 'xmax': 2.0,\n         'K': 6, 'lam': 10.0, 'l_ref': [-1.0, 0.0, 1.0, 2.0, 1.0, 0.0]},\n        # Case D\n        {'a': 1.0, 'b': 1.0, 'beta': 50.0, 'xmin': -2.0, 'xmax': 2.0,\n         'K': 8, 'lam': 0.01, 'l_ref': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        a = case['a']\n        b = case['b']\n        beta = case['beta']\n        xmin = case['xmin']\n        xmax = case['xmax']\n        K = case['K']\n        lam = case['lam']\n        l_ref = np.array(case['l_ref'])\n\n        # 1. Define potential and integrand\n        def U(x):\n            return a * (x**2 - b**2)**2\n\n        def integrand(x):\n            return np.exp(-beta * U(x))\n\n        # 2. Compute partition function Z\n        Z, _ = quad(integrand, xmin, xmax, epsabs=1e-12, epsrel=1e-12)\n        if Z == 0:\n            # This case should not happen with the given parameters\n            raise ValueError(\"Partition function Z is zero.\")\n            \n        # 3. Compute target marginal probabilities P\n        bin_edges = np.linspace(xmin, xmax, K + 1)\n        P = np.zeros(K)\n        for i in range(K):\n            bin_integral, _ = quad(integrand, bin_edges[i], bin_edges[i+1], epsabs=1e-12, epsrel=1e-12)\n            P[i] = bin_integral / Z\n        \n        # Ensure P is a valid probability distribution\n        P[P  0] = 0\n        P /= np.sum(P)\n\n        # 4. Define objective function and its gradient\n        # Handle the P_i * log(P_i) term, where the result is 0 if P_i = 0\n        P_log_P = np.zeros_like(P)\n        non_zero_mask = P > 0\n        P_log_P[non_zero_mask] = P[non_zero_mask] * np.log(P[non_zero_mask])\n        dkl_const_term = np.sum(P_log_P)\n\n        def objective_and_grad(l, P_vec, lam_val, l_ref_vec, dkl_const):\n            \"\"\"\n            Computes the objective function and its gradient.\n            F(l) = D_KL(P||Q(l)) + lambda * ||l - l_ref||^2\n            \"\"\"\n            # Compute Q(l) using logsumexp for stability\n            lse = logsumexp(l)\n            log_Q = l - lse\n            Q = np.exp(log_Q)\n\n            # Objective Function F(l)\n            # D_KL = sum(P * (logP - logQ))\n            dkl = dkl_const - np.sum(P_vec * log_Q)\n            reg = lam_val * np.sum((l - l_ref_vec)**2)\n            obj_val = dkl + reg\n\n            # Gradient of F(l)\n            # grad(F) = Q - P + 2*lambda*(l - l_ref)\n            grad_vec = Q - P_vec + 2 * lam_val * (l - l_ref_vec)\n            \n            return obj_val, grad_vec\n\n        # 5. Perform the optimization\n        l_initial = np.copy(l_ref)\n        \n        result = minimize(\n            fun=objective_and_grad,\n            x0=l_initial,\n            args=(P, lam, l_ref, dkl_const_term),\n            method='L-BFGS-B',\n            jac=True,  # function returns both objective and gradient\n            options={'gtol': 1e-9, 'disp': False}\n        )\n\n        l_star = result.x\n        all_results.append(l_star.tolist())\n\n    # 6. Format the final output string\n    # E.g., [[-4.7,-0.7,0.3,-0.7,-4.7],[-0.1,0.0,0.0,-0.1]]\n    output_parts = []\n    for res_list in all_results:\n        # Format each list as '[val1,val2,...]'\n        formatted_list = f'[{\",\".join(map(str, res_list))}]'\n        output_parts.append(formatted_list)\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "粗粒度建模的一个核心挑战是选择一个表达能力足够强的模型。这个高级练习介绍了一种强大的解决方案：自适应基函数选择方案。您将实现一个算法，该算法利用梯度信息来智能地扩展粗粒度势函数，展示了一种系统性提高模型精度的复杂方法 ()。这代表了将核心原理应用于前沿研究级问题的实践。",
            "id": "3802835",
            "problem": "考虑一个有限状态空间 $\\mathcal{X} = \\{1,2,\\dots,n\\}$，其上有一个严格为正的目标分布 $\\mu$，以及一组候选基函数 $\\{\\varphi_j\\}_{j=1}^m$，其中每个 $\\varphi_j:\\mathcal{X}\\to\\mathbb{R}$。对于参数 $\\theta\\in\\mathbb{R}^k$，定义一个指数族模型 $p_\\theta$，其参数限制于一个活性索引集 $A\\subseteq\\{1,\\dots,m\\}$，形式为 $p_\\theta(x) \\propto \\exp\\left(\\sum_{j\\in A}\\theta_j\\varphi_j(x)\\right)$（对于 $x\\in\\mathcal{X}$）。其中，一个归一化配分函数确保 $\\sum_{x\\in\\mathcal{X}}p_\\theta(x)=1$。相对熵最小化（也称为 Kullback-Leibler 散度最小化）的目标函数是 $\\mu$ 与 $p_\\theta$ 之间的 Kullback-Leibler (KL) 散度，记作 $F(\\theta) = \\mathrm{KL}(\\mu\\parallel p_\\theta)$。\n\n您的任务是：\n- 从 Kullback-Leibler 散度的核心定义以及指数族对数配分函数的性质出发，推导目标函数 $F(\\theta)$ 相对于限制在活性基函数集上的参数 $\\theta$ 的梯度和 Hessian 矩阵。不要假设任何预先推导好的公式；请从第一性原理出发进行推导。\n- 提出一种用于粗粒化（Coarse-Graining, CG）的自适应基选择方案，该方案由相对于候选基函数的最大梯度分量引导。该方案应在每次迭代时，选择一个新的基函数，其特征向量不在当前活性特征的线性张成空间内，并且其相关的绝对梯度分量在所有此类候选者中最大。然后在扩大的活性集上重新优化 $F(\\theta)$。证明：只要新方向不在当前张成空间内，并且当前最优解沿此新方向的方向导数非零，添加一个新的基函数就会严格减小目标函数 $F(\\theta)$。您的证明必须仅依赖于从上述基本原理推导出的可微性和凸性原理。\n- 将该方案实现为一个完整的、可运行的程序，对指定的测试套件执行以下计算，并按要求格式输出单行结果。\n\n假设：\n- 状态空间 $\\mathcal{X}$ 是有限的，且 $n\\geq 2$。\n- 对所有 $x\\in\\mathcal{X}$，目标分布 $\\mu(x)0$ 且 $\\sum_{x\\in\\mathcal{X}}\\mu(x)=1$。\n- 候选基函数 $\\{\\varphi_j\\}$ 是 $\\mathcal{X}$ 上的实值函数。线性张成指的是在 $\\mathcal{X}$ 上求值的特征向量 $\\varphi_j$ 的线性相关性。\n\n测试套件：\n- 测试用例 1（通用/正常情况）：设 $n=6$，目标分布为 $\\mu = [0.05, 0.10, 0.15, 0.25, 0.20, 0.25]$，候选基函数集为在 $\\mathcal{X}=\\{1,2,3,4,5,6\\}$ 上求值的 $\\varphi_1(x)=x$、$\\varphi_2(x)=x^2$、$\\varphi_3(x)=(-1)^x$。初始化活性集为 $A=\\{1\\}$，并运行一个自适应选择步骤以添加一个新基并重新优化。报告添加新基并重新优化后，目标函数是否严格减小。\n- 测试用例 2（边界/张成空间内方向）：设 $n=5$，目标分布为 $\\mu = [0.10, 0.15, 0.25, 0.20, 0.30]$，候选基函数集为在 $\\mathcal{X}=\\{1,2,3,4,5\\}$ 上求值的 $\\psi_1(x)=x$ 和 $\\psi_2(x)=2x$，活性集为 $A=\\{1\\}$。强制尝试添加 $\\psi_2$（它在当前活性集的张成空间内）并重新优化。报告目标函数是否严格减小；此用例测试新方向位于当前张成空间内时的行为。\n- 测试用例 3（边缘情况/零方向导数）：设 $n=5$，$\\tilde{\\varphi}_1(x)=x$ 且 $\\tilde{\\theta}=0.8$。定义 $\\mu$ 为仅由 $\\tilde{\\varphi}_1$ 生成的指数族分布，即在 $\\mathcal{X}=\\{1,2,3,4,5\\}$ 上 $\\mu(x) \\propto \\exp\\left(\\tilde{\\theta}\\,\\tilde{\\varphi}_1(x)\\right)$。设第二个候选基为 $\\tilde{\\varphi}_2(x)=x^2$，并初始化 $A=\\{1\\}$。运行一个自适应选择步骤和重新优化，并报告在尝试添加 $\\tilde{\\varphi}_2$ 后目标函数是否严格减小。此用例测试沿一个独立新方向的方向导数在当前最优解处为零的情况。\n\n输出规范：\n- 对于每个测试用例，计算一个布尔值，指示在尝试添加和重新优化后目标函数是否严格减小。“严格减小”定义为重新优化后的目标值比添加前的目标值小至少一个数值容差 $\\varepsilon=10^{-10}$。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如 $[\\text{result1},\\text{result2},\\text{result3}]$。\n\n不使用角度单位。不需要物理单位。输出为布尔值。",
            "solution": "该问题是有效的。它在科学上基于信息论和统计建模的原理，由于目标函数的凸性而问题是良定的，并且客观地陈述了所有必要的定义和数据。\n\n### 第 1 部分：梯度和 Hessian 矩阵的推导\n\n设状态空间为 $\\mathcal{X} = \\{1, 2, \\dots, n\\}$。目标分布 $\\mu(x)$ 是严格为正的。模型分布 $p_\\theta(x)$ 属于一个由一组基函数 $\\{\\varphi_j\\}_{j \\in A}$ 定义的指数族，其中 $A$ 是活性索引集。该模型由以下公式给出：\n$$\np_\\theta(x) = \\frac{1}{Z(\\theta)} \\exp\\left(\\sum_{j \\in A} \\theta_j \\varphi_j(x)\\right)\n$$\n其中 $Z(\\theta)$ 是配分函数，确保归一化：\n$$\nZ(\\theta) = \\sum_{x \\in \\mathcal{X}} \\exp\\left(\\sum_{j \\in A} \\theta_j \\varphi_j(x)\\right)\n$$\n目标是最小化 $p_\\theta$ 相对于 $\\mu$ 的 Kullback-Leibler (KL) 散度（或称相对熵）：\n$$\nF(\\theta) = \\mathrm{KL}(\\mu \\parallel p_\\theta) = \\sum_{x \\in \\mathcal{X}} \\mu(x) \\log\\left(\\frac{\\mu(x)}{p_\\theta(x)}\\right)\n$$\n我们可以展开这个表达式：\n$$\nF(\\theta) = \\sum_{x \\in \\mathcal{X}} \\mu(x) \\log \\mu(x) - \\sum_{x \\in \\mathcal{X}} \\mu(x) \\log p_\\theta(x)\n$$\n第一项 $\\sum \\mu(x) \\log \\mu(x)$ 是固定目标分布 $\\mu$ 的负熵，并且相对于参数 $\\theta$ 是一个常数。因此，最小化 $F(\\theta)$ 等价于最小化负交叉熵项，或最大化交叉熵：\n$$\n\\min_\\theta F(\\theta) \\iff \\min_\\theta \\left( -\\sum_{x \\in \\mathcal{X}} \\mu(x) \\log p_\\theta(x) \\right)\n$$\n让我们代入 $\\log p_\\theta(x)$ 的表达式：\n$$\n\\log p_\\theta(x) = \\sum_{j \\in A} \\theta_j \\varphi_j(x) - \\log Z(\\theta)\n$$\n要最小化的量变为：\n$$\nF(\\theta) = C - \\sum_{x \\in \\mathcal{X}} \\mu(x) \\left( \\sum_{j \\in A} \\theta_j \\varphi_j(x) - \\log Z(\\theta) \\right)\n$$\n$$\nF(\\theta) = C + \\log Z(\\theta) \\sum_{x \\in \\mathcal{X}} \\mu(x) - \\sum_{j \\in A} \\theta_j \\sum_{x \\in \\mathcal{X}} \\mu(x) \\varphi_j(x)\n$$\n由于 $\\sum_{x \\in \\mathcal{X}} \\mu(x) = 1$ 且忽略常数 $C$，目标函数（在相差一个常数的情况下）为：\n$$\nF(\\theta) = \\log Z(\\theta) - \\sum_{j \\in A} \\theta_j \\langle \\varphi_j \\rangle_\\mu\n$$\n其中 $\\langle \\cdot \\rangle_\\mu$ 表示关于目标分布 $\\mu$ 的期望，即 $\\langle f \\rangle_\\mu = \\sum_{x \\in \\mathcal{X}} \\mu(x) f(x)$。\n\n**$F(\\theta)$ 的梯度**\n\n为了求梯度，我们将 $F(\\theta)$ 对分量 $\\theta_k$ (对于 $k \\in A$)求导：\n$$\n\\frac{\\partial F(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\log Z(\\theta) - \\frac{\\partial}{\\partial \\theta_k} \\sum_{j \\in A} \\theta_j \\langle \\varphi_j \\rangle_\\mu\n$$\n第二项很简单：$\\frac{\\partial}{\\partial \\theta_k} \\sum_{j \\in A} \\theta_j \\langle \\varphi_j \\rangle_\\mu = \\langle \\varphi_k \\rangle_\\mu$。\n\n对于第一项，我们使用链式法则：\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\frac{\\partial Z(\\theta)}{\\partial \\theta_k}\n$$\n配分函数的导数是：\n$$\n\\frac{\\partial Z(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\sum_{x \\in \\mathcal{X}} \\exp\\left(\\sum_{j \\in A} \\theta_j \\varphi_j(x)\\right) = \\sum_{x \\in \\mathcal{X}} \\varphi_k(x) \\exp\\left(\\sum_{j \\in A} \\theta_j \\varphi_j(x)\\right)\n$$\n将其代回，我们得到：\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\sum_{x \\in \\mathcal{X}} \\varphi_k(x) \\exp\\left(\\sum_{j \\in A} \\theta_j \\varphi_j(x)\\right) = \\sum_{x \\in \\mathcal{X}} \\varphi_k(x) p_\\theta(x) = \\langle \\varphi_k \\rangle_{p_\\theta}\n$$\n这是指数族的一个基本性质：对数配分函数关于某个参数的导数是相应充分统计量在模型分布下的期望。\n\n结合这两项，梯度的第 $k$ 个分量是：\n$$\n\\frac{\\partial F(\\theta)}{\\partial \\theta_k} = \\langle \\varphi_k \\rangle_{p_\\theta} - \\langle \\varphi_k \\rangle_\\mu\n$$\n梯度向量 $\\nabla F(\\theta)$ 的分量为 $(\\langle \\varphi_j \\rangle_{p_\\theta} - \\langle \\varphi_j \\rangle_\\mu)_{j \\in A}$。$F(\\theta)$ 的最小值在 $\\nabla F(\\theta) = \\mathbf{0}$ 时达到，这意味着矩匹配条件：对于所有 $j \\in A$，$\\langle \\varphi_j \\rangle_{p_\\theta} = \\langle \\varphi_j \\rangle_\\mu$。\n\n**$F(\\theta)$ 的 Hessian 矩阵**\n\n为了求 Hessian 矩阵，我们计算二阶偏导数 $\\frac{\\partial^2 F(\\theta)}{\\partial \\theta_l \\partial \\theta_k}$，其中 $k, l \\in A$：\n$$\n\\frac{\\partial^2 F(\\theta)}{\\partial \\theta_l \\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_l} \\left( \\langle \\varphi_k \\rangle_{p_\\theta} - \\langle \\varphi_k \\rangle_\\mu \\right) = \\frac{\\partial}{\\partial \\theta_l} \\langle \\varphi_k \\rangle_{p_\\theta} = \\frac{\\partial}{\\partial \\theta_l} \\sum_{x \\in \\mathcal{X}} \\varphi_k(x) p_\\theta(x)\n$$\n我们需要 $p_\\theta(x)$ 对 $\\theta_l$ 的导数：\n$$\n\\frac{\\partial p_\\theta(x)}{\\partial \\theta_l} = \\frac{\\partial}{\\partial \\theta_l} \\left( \\frac{\\exp(\\sum_j \\theta_j \\varphi_j(x))}{Z(\\theta)} \\right)\n$$\n使用商法则：\n$$\n\\frac{\\partial p_\\theta(x)}{\\partial \\theta_l} = \\frac{\\varphi_l(x) \\exp(\\dots) Z(\\theta) - \\exp(\\dots) \\frac{\\partial Z(\\theta)}{\\partial \\theta_l}}{Z(\\theta)^2}\n$$\n$$\n= \\frac{\\varphi_l(x) \\exp(\\dots)}{Z(\\theta)} - \\frac{\\exp(\\dots)}{Z(\\theta)} \\frac{1}{Z(\\theta)} \\frac{\\partial Z(\\theta)}{\\partial \\theta_l} = p_\\theta(x) \\varphi_l(x) - p_\\theta(x) \\langle \\varphi_l \\rangle_{p_\\theta}\n$$\n$$\n= p_\\theta(x) \\left( \\varphi_l(x) - \\langle \\varphi_l \\rangle_{p_\\theta} \\right)\n$$\n现在，将其代回 Hessian 矩阵元素的表达式中：\n$$\n\\frac{\\partial^2 F(\\theta)}{\\partial \\theta_l \\partial \\theta_k} = \\sum_{x \\in \\mathcal{X}} \\varphi_k(x) \\left[ p_\\theta(x) \\left( \\varphi_l(x) - \\langle \\varphi_l \\rangle_{p_\\theta} \\right) \\right]\n$$\n$$\n= \\sum_{x \\in \\mathcal{X}} p_\\theta(x) \\varphi_k(x) \\varphi_l(x) - \\langle \\varphi_l \\rangle_{p_\\theta} \\sum_{x \\in \\mathcal{X}} p_\\theta(x) \\varphi_k(x)\n$$\n$$\n= \\langle \\varphi_k \\varphi_l \\rangle_{p_\\theta} - \\langle \\varphi_k \\rangle_{p_\\theta} \\langle \\varphi_l \\rangle_{p_\\theta}\n$$\n这是在分布 $p_\\theta$ 下 $\\varphi_k$ 和 $\\varphi_l$ 的协方差。Hessian 矩阵 $H$ 是基函数的协方差矩阵：\n$$\nH_{kl} = (\\nabla^2 F(\\theta))_{kl} = \\mathrm{Cov}_{p_\\theta}(\\varphi_k, \\varphi_l)\n$$\n由于协方差矩阵总是半正定的，目标函数 $F(\\theta)$ 是凸的。如果基函数 $\\{\\varphi_j\\}_{j \\in A}$ 作为 $\\mathcal{X}$ 上的向量是线性无关的，那么 Hessian 矩阵是严格正定的，F(θ) 是严格凸的，从而保证了唯一的最小值。\n\n### 第 2 部分：自适应基选择和严格减小证明\n\n**自适应方案**\n\n自适应基选择方案通过添加对减少 KL 散度最有效的信息的基函数，迭代地构建粗粒化模型。\n\n1.  **初始化**：从一个活性集 $A_0$（例如，$A_0 = \\emptyset$，这意味着 $p_\\theta(x)$ 是均匀分布）和一组候选基函数 $C = \\{\\varphi_1, \\dots, \\varphi_m\\}$ 开始。\n2.  **迭代**：在第 $k$ 步，给定活性集 $A_k$：\n    a.  **优化**：找到最优参数 $\\theta_k^*$，以最小化由 $A_k$ 定义的模型的 $F(\\theta)$。这通过求解 $\\nabla_A F(\\theta) = \\mathbf{0}$ 来实现，即对所有 $j \\in A_k$ 设定 $\\langle \\varphi_j \\rangle_{p_{\\theta_k^*}} = \\langle \\varphi_j \\rangle_\\mu$。令 $p_k^* = p_{\\theta_k^*}$。\n    b.  **评估候选者**：对于每个不在活性集张成空间内的候选基函数 $\\varphi_j$，计算当前最优解处对应的梯度分量：\n        $$\n        g_j = \\frac{\\partial F}{\\partial \\theta_j}\\bigg|_{p_k^*} = \\langle \\varphi_j \\rangle_{p_k^*} - \\langle \\varphi_j \\rangle_\\mu\n        $$\n        注意，对于所有 $j \\in A_k$，$g_j = 0$。\n    c.  **选择**：通过找到在合格候选者中具有最大绝对梯度分量的基函数 $\\varphi_{j_{\\text{new}}}$，将其添加到活性集中：\n        $$\n        j_{\\text{new}} = \\arg\\max_{j \\in C \\setminus A_k, \\, \\varphi_j \\notin \\text{span}(\\{\\varphi_i\\}_{i \\in A_k})} |g_j|\n        $$\n    d.  **更新**：形成新的活性集 $A_{k+1} = A_k \\cup \\{j_{\\text{new}}\\}$。\n3.  **终止**：重复此过程，直到满足停止准则，例如达到最大基函数数量，或对于某个小容差 $\\epsilon$，$\\max_j |g_j|  \\epsilon$。\n\n**严格减小证明**\n\n令 $\\theta^*$ 为活性集 $A$ 的最优参数向量，令 $F(\\theta^*)$ 为相应的最小目标值。此时，我们有对所有 $j \\in A$，$\\frac{\\partial F}{\\partial \\theta_j}|_{\\theta^*} = 0$。\n\n现在，我们考虑添加一个新的基函数 $\\varphi_k$，其中 $k \\notin A$ 且 $\\varphi_k$ 不在 $\\{\\varphi_j\\}_{j \\in A}$ 的线性张成空间内。我们将参数空间从 $\\mathbb{R}^{|A|}$ 扩充到 $\\mathbb{R}^{|A|+1}$。当前最优点对应于一个增广向量 $\\tilde{\\theta}^* = (\\theta_1^*, \\dots, \\theta_{|A|}^*, 0)$，其中最后一个分量对应新的基函数 $\\varphi_k$。此时目标函数的值为 $F(\\tilde{\\theta}^*) = F(\\theta^*)$。\n\n问题陈述中指出，我们选择一个其方向导数非零的新基。令 $d$ 为新参数 $\\theta_k$ 方向上的单位向量。$F$ 在 $\\tilde{\\theta}^*$ 处沿 $d$ 的方向导数是：\n$$\n\\nabla F(\\tilde{\\theta}^*) \\cdot d = \\frac{\\partial F}{\\partial \\theta_k}\\bigg|_{\\tilde{\\theta}^*}\n$$\n根据我们的梯度推导，这等于：\n$$\n\\frac{\\partial F}{\\partial \\theta_k}\\bigg|_{\\tilde{\\theta}^*} = \\langle \\varphi_k \\rangle_{p_{\\tilde{\\theta}^*}} - \\langle \\varphi_k \\rangle_\\mu\n$$\n由于 $\\tilde{\\theta}^*$ 的第 $k$ 个分量为零，所以 $p_{\\tilde{\\theta}^*} = p_{\\theta^*}$。因此，方向导数是 $g_k = \\langle \\varphi_k \\rangle_{p_{\\theta^*}} - \\langle \\varphi_k \\rangle_\\mu$。自适应方案的选择标准确保我们选择的 $\\varphi_k$ 满足 $g_k \\neq 0$。\n\n由于 $F(\\theta)$ 是一个凸函数，且我们处于一个点 $\\tilde{\\theta}^*$，该点在某个方向（新基向量的方向）上梯度非零，因此 $\\tilde{\\theta}^*$ 不是增广空间中函数的最小值。具体来说，方向 $v = -g_k d$ 是一个下降方向。对于一个小的步长 $\\alpha  0$，一阶泰勒展开给出：\n$$\nF(\\tilde{\\theta}^* + \\alpha v) \\approx F(\\tilde{\\theta}^*) + \\alpha (\\nabla F(\\tilde{\\theta}^*) \\cdot v) = F(\\tilde{\\theta}^*) + \\alpha (-g_k d) \\cdot (g_k d) = F(\\tilde{\\theta}^*) - \\alpha g_k^2\n$$\n由于 $\\alpha  0$ 且我们选择的 $k$ 使得 $g_k \\neq 0$，项 $-\\alpha g_k^2$ 是严格为负的。这表明我们可以从 $\\tilde{\\theta}^*$ 出发迈出小步，并严格减小目标函数。\n\n因为 $F(\\theta)$ 是凸的（并且由于 $\\varphi_k$ 与现有基函数线性无关，所以是严格凸的），从一个非最优点出发的下降方向保证了通过在增广参数空间上优化找到的新最小值 $F(\\theta^{**})$ 将严格低于起始点的值 $F(\\tilde{\\theta}^*)$。\n$$\nF(\\theta^{**})  F(\\tilde{\\theta}^*) = F(\\theta^*)\n$$\n因此，添加一个具有非零梯度分量的线性无关基函数并重新优化，会严格地减小目标函数。如果新方向在当前活性集的张成空间内，或者梯度分量为零，则不保证严格减小。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the adaptive basis selection problem for three test cases.\n    \"\"\"\n    \n    class RelativeEntropyMinimizer:\n        \"\"\"\n        Manages the state and calculations for relative entropy minimization.\n        \"\"\"\n        def __init__(self, mu, phis):\n            self.mu = np.array(mu, dtype=float)\n            self.n = len(mu)\n            self.X = np.arange(1, self.n + 1)\n            \n            # Evaluate basis functions over the state space\n            self.phis_matrix = np.array([phi(self.X) for phi in phis], dtype=float)\n            self.m = self.phis_matrix.shape[0]\n\n            # Pre-compute expectations w.r.t. the target distribution mu\n            self.mu_expects = self.phis_matrix @ self.mu\n\n        def _get_model_dist(self, theta, active_indices):\n            \"\"\"Calculates the model distribution p_theta for a given theta and active set.\"\"\"\n            active_phis = self.phis_matrix[active_indices, :]\n            log_p_unnorm = theta @ active_phis\n            \n            # Numerical stability: shift log probabilities before exponentiating\n            # to avoid overflow, without changing the final probability.\n            log_p_unnorm -= np.max(log_p_unnorm)\n            p_unnorm = np.exp(log_p_unnorm)\n            Z = np.sum(p_unnorm)\n            return p_unnorm / Z\n\n        def _objective_func(self, theta, active_indices):\n            \"\"\"Calculates the KL divergence KL(mu || p_theta).\"\"\"\n            p_theta = self._get_model_dist(theta, active_indices)\n            # Add a small epsilon to avoid log(0) if p_theta has zero entries.\n            p_theta = np.clip(p_theta, 1e-15, None)\n            kl_div = np.sum(self.mu * (np.log(self.mu) - np.log(p_theta)))\n            return kl_div\n\n        def _jacobian(self, theta, active_indices):\n            \"\"\"Calculates the gradient of the KL divergence.\"\"\"\n            p_theta = self._get_model_dist(theta, active_indices)\n            active_phis = self.phis_matrix[active_indices, :]\n            p_expects = active_phis @ p_theta\n            mu_expects_active = self.mu_expects[active_indices]\n            return p_expects - mu_expects_active\n\n        def _hessian(self, theta, active_indices):\n            \"\"\"Calculates the Hessian of the KL divergence.\"\"\"\n            p_theta = self._get_model_dist(theta, active_indices)\n            active_phis = self.phis_matrix[active_indices, :]\n            k = len(active_indices)\n            \n            p_expects = active_phis @ p_theta\n            \n            # H_ij = Cov_p(phi_i, phi_j) = E_p[phi_i * phi_j] - E_p[phi_i] * E_p[phi_j]\n            # Broadcast to compute E_p[phi_i * phi_j] efficiently\n            # (k, n) * (1, n) -> (k, n), then matmul with (n, k) -> (k, k)\n            e_phi_phi = (active_phis * p_theta) @ active_phis.T\n            # Outer product of expectations: E_p[phi_i] * E_p[phi_j]\n            e_phi_outer_e_phi = np.outer(p_expects, p_expects)\n\n            hess = e_phi_phi - e_phi_outer_e_phi\n            return hess\n\n        def optimize(self, active_indices):\n            \"\"\"Finds the optimal theta and KL divergence for a given active set.\"\"\"\n            theta0 = np.zeros(len(active_indices))\n            res = minimize(\n                fun=self._objective_func,\n                x0=theta0,\n                args=(active_indices,),\n                method='Newton-CG',\n                jac=self._jacobian,\n                hess=self._hessian,\n                tol=1e-12\n            )\n            return res.x, res.fun\n\n        def get_gradient_components(self, theta_opt, active_indices_opt):\n            \"\"\"Calculates gradient components for all candidate basis functions.\"\"\"\n            p_opt = self._get_model_dist(theta_opt, active_indices_opt)\n            p_expects = self.phis_matrix @ p_opt\n            return p_expects - self.mu_expects\n\n    # --- Test Cases ---\n    \n    # Test Case 1: General/happy path\n    def run_case_1():\n        n = 6\n        mu = np.array([0.05, 0.10, 0.15, 0.25, 0.20, 0.25])\n        phis = [lambda x: x, lambda x: x**2, lambda x: (-1)**x]\n        \n        solver = RelativeEntropyMinimizer(mu, phis)\n        \n        # Step 1: Optimize with initial active set A={1}\n        active_set_1 = [0]\n        theta_1, f_1 = solver.optimize(active_set_1)\n        \n        # Step 2: Adaptive selection\n        # Check linear independence of candidates\n        candidate_indices = [i for i in range(solver.m) if i not in active_set_1]\n        active_vectors = solver.phis_matrix[active_set_1]\n\n        eligible_candidates = []\n        for idx in candidate_indices:\n            candidate_vec = solver.phis_matrix[idx]\n            # Check if candidate_vec is in the span of active_vectors\n            mat = np.vstack([active_vectors, candidate_vec])\n            if np.linalg.matrix_rank(mat) > np.linalg.matrix_rank(active_vectors):\n                eligible_candidates.append(idx)\n\n        # Calculate gradients for eligible candidates\n        grads = solver.get_gradient_components(theta_1, active_set_1)\n        best_candidate = -1\n        max_grad = -1.0\n        for idx in eligible_candidates:\n            if abs(grads[idx]) > max_grad:\n                max_grad = abs(grads[idx])\n                best_candidate = idx\n\n        # Step 3: Re-optimize with the new basis function\n        active_set_2 = sorted(active_set_1 + [best_candidate])\n        _, f_2 = solver.optimize(active_set_2)\n        \n        # Step 4: Check for strict decrease\n        return (f_1 - f_2) > 1e-10\n\n    # Test Case 2: In-span direction\n    def run_case_2():\n        n = 5\n        mu = np.array([0.10, 0.15, 0.25, 0.20, 0.30])\n        phis = [lambda x: x, lambda x: 2*x]\n        \n        solver = RelativeEntropyMinimizer(mu, phis)\n        \n        # Step 1: Optimize with initial active set A={1}\n        active_set_1 = [0]\n        _, f_1 = solver.optimize(active_set_1)\n        \n        # Step 2: Forcibly add linearly dependent basis psi_2\n        active_set_2 = [0, 1]\n        \n        # Re-optimization will likely fail to find a unique theta,\n        # but the objective value should plateau at f_1.\n        _, f_2 = solver.optimize(active_set_2)\n        \n        return (f_1 - f_2) > 1e-10\n\n    # Test Case 3: Zero directional derivative\n    def run_case_3():\n        n = 5\n        tilde_theta_val = 0.8\n        X = np.arange(1, n + 1)\n        \n        log_mu_unnorm = tilde_theta_val * X\n        mu_unnorm = np.exp(log_mu_unnorm)\n        mu = mu_unnorm / np.sum(mu_unnorm)\n        \n        phis = [lambda x: x, lambda x: x**2]\n        \n        solver = RelativeEntropyMinimizer(mu, phis)\n        \n        # Step 1: Optimize with initial active set A={1}\n        active_set_1 = [0]\n        theta_1, f_1 = solver.optimize(active_set_1)\n        # We expect theta_1 ~ [0.8] and f_1 ~ 0.0\n\n        # Step 2: Attempt to add phi_2. The gradient should be zero.\n        grads = solver.get_gradient_components(theta_1, active_set_1)\n        # As shown in the proof, the gradient for candidate 1 (x^2) should be 0.\n        # grads[1] should be very close to zero.\n        \n        # Step 3: Forcibly re-optimize with the new basis function\n        active_set_2 = [0, 1]\n        _, f_2 = solver.optimize(active_set_2)\n        \n        # The new optimum should be theta=[0.8, 0.0] and f=0.0.\n        return (f_1 - f_2) > 1e-10\n\n    results = [run_case_1(), run_case_2(), run_case_3()]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}