{
    "hands_on_practices": [
        {
            "introduction": "Before embarking on computationally expensive Monte Carlo simulations, it is crucial to have confidence that the resulting estimate will be sufficiently accurate. Non-asymptotic concentration inequalities provide this assurance by giving rigorous, finite-sample guarantees on an estimator's precision. This practice  guides you through the fundamental derivation of such a bound for a sub-Gaussian observable, and then applies it to a practical problem: determining the minimum number of samples needed to achieve a desired accuracy with high probability.",
            "id": "3739081",
            "problem": "A slow–fast multiscale system is simulated to estimate a coarse observable defined as follows. For a fixed slow variable state, let $X$ denote the value of a bounded Lipschitz observable evaluated on a fast-process microstate drawn from its stationary distribution. Assume $X$ has finite mean $\\mu$ and is sub-Gaussian with proxy variance parameter $s^{2}$ in the sense that its centered Moment Generating Function (MGF) satisfies, for all $\\lambda \\in \\mathbb{R}$,\n$$\n\\mathbb{E}\\big[\\exp\\big(\\lambda\\,(X-\\mu)\\big)\\big] \\le \\exp\\!\\left(\\frac{s^{2}\\,\\lambda^{2}}{2}\\right).\n$$\nYou run $M$ independent microsimulations to obtain independent and identically distributed samples $X_{1},\\dots,X_{M}$ of $X$ and form the Monte Carlo average $\\overline{X}_{M} \\equiv \\frac{1}{M}\\sum_{i=1}^{M} X_{i}$.\n\nStarting from the sub-Gaussian MGF bound and Markov’s inequality, and without invoking any asymptotic approximations, derive a non-asymptotic two-sided concentration bound for the deviation $\\overline{X}_{M}-\\mu$. Then, using this bound, determine the smallest integer $M$ that guarantees\n$$\n\\mathbb{P}\\big(|\\overline{X}_{M}-\\mu| \\le \\varepsilon\\big) \\ge 1-\\delta\n$$\nfor the concrete parameters $s^{2} = 0.36$, $\\varepsilon = 0.03$, and $\\delta = 0.005$. Report the minimal $M$ as an exact integer. No rounding by significant figures is required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- A random variable $X$ represents an observable evaluated on a fast-process microstate drawn from its stationary distribution.\n- $X$ has a finite mean $\\mu$.\n- $X$ is sub-Gaussian with proxy variance parameter $s^{2}$.\n- The centered Moment Generating Function (MGF) of $X$ satisfies $\\mathbb{E}[\\exp(\\lambda(X-\\mu))] \\le \\exp\\left(\\frac{s^{2}\\lambda^{2}}{2}\\right)$ for all $\\lambda \\in \\mathbb{R}$.\n- $X_{1}, \\dots, X_{M}$ are independent and identically distributed (i.i.d.) samples of $X$.\n- The Monte Carlo average is defined as $\\overline{X}_{M} \\equiv \\frac{1}{M}\\sum_{i=1}^{M} X_{i}$.\n- The task is to derive a non-asymptotic two-sided concentration bound for the deviation $\\overline{X}_{M}-\\mu$ starting from the given MGF bound and Markov's inequality.\n- Subsequently, determine the smallest integer $M$ that guarantees $\\mathbb{P}(|\\overline{X}_{M}-\\mu| \\le \\varepsilon) \\ge 1-\\delta$.\n- The concrete parameters are $s^{2} = 0.36$, $\\varepsilon = 0.03$, and $\\delta = 0.005$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically sound. The concept of a sub-Gaussian random variable and its MGF bound is a standard tool in modern probability theory and statistics. The setup of using i.i.d. samples to form a Monte Carlo average is fundamental. The request to derive a concentration inequality (a form of Hoeffding's inequality) from first principles (MGF bound and Markov's inequality) is a standard textbook exercise, known as the Chernoff bound method. The problem is well-posed, as a unique minimal integer $M$ is expected. The language is precise and objective. All necessary data and definitions are provided. Therefore, the problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe objective is to find a bound on the probability of the deviation of the sample mean from the true mean. Specifically, we want to find an upper bound for $\\mathbb{P}(|\\overline{X}_{M}-\\mu| \\ge \\varepsilon)$ for some $\\varepsilon  0$. We will derive this by first bounding the one-sided probabilities $\\mathbb{P}(\\overline{X}_{M}-\\mu \\ge \\varepsilon)$ and $\\mathbb{P}(\\overline{X}_{M}-\\mu \\le -\\varepsilon)$ and then combining them using the union bound.\n\nLet $Y_i = X_i - \\mu$. The variables $Y_i$ are i.i.d. with mean $\\mathbb{E}[Y_i] = 0$. The MGF bound for $X_i$ implies that for each $Y_i$:\n$$\n\\mathbb{E}[\\exp(\\lambda Y_i)] \\le \\exp\\left(\\frac{s^{2}\\lambda^{2}}{2}\\right)\n$$\nThe deviation of the sample mean is $\\overline{X}_{M} - \\mu = \\frac{1}{M}\\sum_{i=1}^{M} (X_i - \\mu) = \\frac{1}{M}\\sum_{i=1}^{M} Y_i$.\n\nLet's first bound the probability of a positive deviation, $\\mathbb{P}(\\overline{X}_{M} - \\mu \\ge \\varepsilon)$. Using the Chernoff bound method, for any $\\lambda  0$:\n$$\n\\mathbb{P}(\\overline{X}_{M} - \\mu \\ge \\varepsilon) = \\mathbb{P}\\left(\\frac{1}{M}\\sum_{i=1}^{M} Y_i \\ge \\varepsilon\\right) = \\mathbb{P}\\left(\\sum_{i=1}^{M} Y_i \\ge M\\varepsilon\\right)\n$$\nSince $\\lambda  0$, the exponential function is monotonically increasing, so the event is unchanged:\n$$\n\\mathbb{P}\\left(\\sum_{i=1}^{M} Y_i \\ge M\\varepsilon\\right) = \\mathbb{P}\\left(\\exp\\left(\\lambda \\sum_{i=1}^{M} Y_i\\right) \\ge \\exp(\\lambda M\\varepsilon)\\right)\n$$\nWe apply Markov's inequality, which states that for a non-negative random variable $Z$ and any $a  0$, $\\mathbb{P}(Z \\ge a) \\le \\frac{\\mathbb{E}[Z]}{a}$. Let $Z = \\exp(\\lambda \\sum Y_i)$ and $a = \\exp(\\lambda M\\varepsilon)$.\n$$\n\\mathbb{P}(\\overline{X}_{M} - \\mu \\ge \\varepsilon) \\le \\frac{\\mathbb{E}\\left[\\exp\\left(\\lambda \\sum_{i=1}^{M} Y_i\\right)\\right]}{\\exp(\\lambda M\\varepsilon)}\n$$\nSince the $Y_i$ are independent, the expectation of the product is the product of expectations:\n$$\n\\mathbb{E}\\left[\\exp\\left(\\lambda \\sum_{i=1}^{M} Y_i\\right)\\right] = \\mathbb{E}\\left[\\prod_{i=1}^{M} \\exp(\\lambda Y_i)\\right] = \\prod_{i=1}^{M} \\mathbb{E}[\\exp(\\lambda Y_i)]\n$$\nUsing the sub-Gaussian MGF bound for each $Y_i$:\n$$\n\\prod_{i=1}^{M} \\mathbb{E}[\\exp(\\lambda Y_i)] \\le \\prod_{i=1}^{M} \\exp\\left(\\frac{s^{2}\\lambda^{2}}{2}\\right) = \\left(\\exp\\left(\\frac{s^{2}\\lambda^{2}}{2}\\right)\\right)^{M} = \\exp\\left(\\frac{M s^{2}\\lambda^{2}}{2}\\right)\n$$\nSubstituting this back into the inequality for the probability:\n$$\n\\mathbb{P}(\\overline{X}_{M} - \\mu \\ge \\varepsilon) \\le \\frac{\\exp\\left(\\frac{M s^{2}\\lambda^{2}}{2}\\right)}{\\exp(\\lambda M\\varepsilon)} = \\exp\\left(\\frac{M s^{2}\\lambda^{2}}{2} - M\\lambda\\varepsilon\\right)\n$$\nThis bound holds for any $\\lambda  0$. To obtain the tightest bound, we minimize the expression in the exponent with respect to $\\lambda$. Let $f(\\lambda) = \\frac{M s^{2}\\lambda^{2}}{2} - M\\lambda\\varepsilon$. The minimum is found by setting the derivative to zero:\n$$\n\\frac{df}{d\\lambda} = M s^{2}\\lambda - M\\varepsilon = 0 \\implies \\lambda = \\frac{M\\varepsilon}{Ms^{2}} = \\frac{\\varepsilon}{s^{2}}\n$$\nSince $\\varepsilon  0$ and $s^{2}  0$, this value of $\\lambda$ is positive. Substituting this optimal $\\lambda$ back into the exponent:\n$$\n\\frac{M s^{2}}{2}\\left(\\frac{\\varepsilon}{s^{2}}\\right)^{2} - M\\left(\\frac{\\varepsilon}{s^{2}}\\right)\\varepsilon = \\frac{M s^{2}\\varepsilon^{2}}{2s^{4}} - \\frac{M\\varepsilon^{2}}{s^{2}} = \\frac{M\\varepsilon^{2}}{2s^{2}} - \\frac{M\\varepsilon^{2}}{s^{2}} = -\\frac{M\\varepsilon^{2}}{2s^{2}}\n$$\nThus, the one-sided bound is:\n$$\n\\mathbb{P}(\\overline{X}_{M} - \\mu \\ge \\varepsilon) \\le \\exp\\left(-\\frac{M\\varepsilon^{2}}{2s^{2}}\\right)\n$$\nA similar argument applies to the other tail, $\\mathbb{P}(\\overline{X}_{M} - \\mu \\le -\\varepsilon)$. This is equivalent to $\\mathbb{P}(-\\sum Y_i \\ge M\\varepsilon)$. We can use the same technique with the random variables $-Y_i$. Since the MGF bound $\\mathbb{E}[\\exp(\\lambda Y_i)] \\le \\exp(s^2 \\lambda^2 / 2)$ holds for all $\\lambda \\in \\mathbb{R}$, it is symmetric around $\\lambda = 0$. The MGF of $-Y_i$ is $\\mathbb{E}[\\exp(\\lambda(-Y_i))] = \\mathbb{E}[\\exp((-\\lambda)Y_i)] \\le \\exp(s^2(-\\lambda)^2/2) = \\exp(s^2\\lambda^2/2)$. Thus, $-Y_i$ has the same sub-Gaussian bound, and by symmetry:\n$$\n\\mathbb{P}(\\overline{X}_{M} - \\mu \\le -\\varepsilon) \\le \\exp\\left(-\\frac{M\\varepsilon^{2}}{2s^{2}}\\right)\n$$\nTo obtain the two-sided bound, we use the union bound: $\\mathbb{P}(A \\cup B) \\le \\mathbb{P}(A) + \\mathbb{P}(B)$.\n$$\n\\mathbb{P}(|\\overline{X}_{M} - \\mu| \\ge \\varepsilon) = \\mathbb{P}\\left( (\\overline{X}_{M} - \\mu \\ge \\varepsilon) \\cup (\\overline{X}_{M} - \\mu \\le -\\varepsilon) \\right)\n$$\n$$\n\\mathbb{P}(|\\overline{X}_{M} - \\mu| \\ge \\varepsilon) \\le \\mathbb{P}(\\overline{X}_{M} - \\mu \\ge \\varepsilon) + \\mathbb{P}(\\overline{X}_{M} - \\mu \\le -\\varepsilon)\n$$\n$$\n\\mathbb{P}(|\\overline{X}_{M} - \\mu| \\ge \\varepsilon) \\le \\exp\\left(-\\frac{M\\varepsilon^{2}}{2s^{2}}\\right) + \\exp\\left(-\\frac{M\\varepsilon^{2}}{2s^{2}}\\right) = 2\\exp\\left(-\\frac{M\\varepsilon^{2}}{2s^{2}}\\right)\n$$\nThis is the desired non-asymptotic two-sided concentration bound.\n\n### Calculation of Minimal Sample Size $M$\nWe are given the requirement $\\mathbb{P}(|\\overline{X}_{M}-\\mu| \\le \\varepsilon) \\ge 1-\\delta$. This is equivalent to:\n$$\n1 - \\mathbb{P}(|\\overline{X}_{M}-\\mu|  \\varepsilon) \\ge 1-\\delta\n$$\nSince the distribution of $\\overline{X}_{M}$ could be discrete, the event $|\\overline{X}_{M}-\\mu|  \\varepsilon$ is a subset of $|\\overline{X}_{M}-\\mu| \\ge \\varepsilon$. Thus, $\\mathbb{P}(|\\overline{X}_{M}-\\mu|  \\varepsilon) \\le \\mathbb{P}(|\\overline{X}_{M}-\\mu| \\ge \\varepsilon)$. To satisfy the requirement, it is sufficient to ensure:\n$$\n\\mathbb{P}(|\\overline{X}_{M}-\\mu| \\ge \\varepsilon) \\le \\delta\n$$\nUsing the derived bound, we must find the smallest integer $M$ that satisfies:\n$$\n2\\exp\\left(-\\frac{M\\varepsilon^{2}}{2s^{2}}\\right) \\le \\delta\n$$\nWe now solve for $M$.\n$$\n\\exp\\left(-\\frac{M\\varepsilon^{2}}{2s^{2}}\\right) \\le \\frac{\\delta}{2}\n$$\nTaking the natural logarithm of both sides (and reversing the inequality sign because $\\ln$ is an increasing function):\n$$\n-\\frac{M\\varepsilon^{2}}{2s^{2}} \\le \\ln\\left(\\frac{\\delta}{2}\\right)\n$$\nMultiplying by $-1$ reverses the inequality again:\n$$\n\\frac{M\\varepsilon^{2}}{2s^{2}} \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\left(\\frac{\\delta}{2}\\right)^{-1}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right)\n$$\nFinally, solving for $M$:\n$$\nM \\ge \\frac{2s^{2}}{\\varepsilon^{2}}\\ln\\left(\\frac{2}{\\delta}\\right)\n$$\nNow we substitute the given parameter values: $s^{2} = 0.36$, $\\varepsilon = 0.03$, and $\\delta = 0.005$.\n- $\\varepsilon^{2} = (0.03)^{2} = 0.0009$.\n- $\\frac{2s^{2}}{\\varepsilon^{2}} = \\frac{2 \\times 0.36}{0.0009} = \\frac{0.72}{0.0009} = \\frac{7200}{9} = 800$.\n- $\\frac{2}{\\delta} = \\frac{2}{0.005} = \\frac{2}{5/1000} = \\frac{2000}{5} = 400$.\nSo the inequality for $M$ becomes:\n$$\nM \\ge 800 \\ln(400)\n$$\nUsing a calculator for the value of $\\ln(400)$:\n$$\n\\ln(400) \\approx 5.991464547\n$$\nTherefore:\n$$\nM \\ge 800 \\times 5.991464547 \\approx 4793.171638\n$$\nSince $M$ must be an integer, the smallest integer value for $M$ that satisfies this condition is the ceiling of this number.\n$$\nM = \\lceil 4793.171638 \\rceil = 4794\n$$\nThus, a minimum of $4794$ independent microsimulations are required to guarantee the desired precision and confidence level.",
            "answer": "$$\n\\boxed{4794}\n$$"
        },
        {
            "introduction": "Simply increasing the number of samples is not always the most efficient way to improve a Monte Carlo estimate, especially in multiscale systems that naturally partition into different regimes or 'strata.' Stratified sampling allows us to allocate our computational budget more intelligently by sampling each stratum at a different rate. This problem  challenges you to derive the celebrated optimal allocation rule that minimizes the estimator's variance for a fixed total cost, a cornerstone of efficient simulation design.",
            "id": "3739095",
            "problem": "In a two-scale multiscale system, a slow variable $S$ takes values in $\\{1,\\ldots,K\\}$ with probabilities $\\{p_i\\}_{i=1}^K$, and conditioned on $S=i$, a fast microscopic simulator produces identically distributed samples $\\{X_{ij}\\}_{j=1}^{n_i}$ with $\\mu_i=\\mathbb{E}[f(X)\\mid S=i]$ and conditional variance $\\sigma_i^2=\\mathrm{Var}(f(X)\\mid S=i)$. The quantity of interest is the overall average $\\mu=\\mathbb{E}[f(X)]=\\sum_{i=1}^K p_i \\mu_i$. Each microscopic sample in stratum $i$ has computational cost $c_i0$. You construct the stratified Monte Carlo (MC) estimator $\\hat{\\mu}=\\sum_{i=1}^K p_i \\bar{f}_i$, where $\\bar{f}_i=\\frac{1}{n_i}\\sum_{j=1}^{n_i} f(X_{ij})$, under a fixed total budget $C0$ satisfying $\\sum_{i=1}^K c_i n_i=C$. Within each stratum, samples are independent and identically distributed, and different strata are simulated independently. You plan an adaptive scheme that uses an initial pilot to estimate $\\sigma_i$ and then allocates the remaining samples according to a proportionality rule $n_i \\propto \\phi(p_i,\\sigma_i,c_i)$, aiming to minimize the asymptotic variance of $\\hat{\\mu}$ under the budget constraint in the large-sample regime, where integer constraints on $\\{n_i\\}$ can be relaxed.\n\nWhich of the following proportionality rules for $n_i$ achieves the asymptotically minimal variance for a fixed total budget $C$?\n\nA. $n_i \\propto p_i\\,\\sigma_i\\,\\sqrt{c_i}$\n\nB. $n_i \\propto p_i\\,\\sigma_i\\,/\\,\\sqrt{c_i}$\n\nC. $n_i \\propto p_i\\,\\sigma_i^{2}\\,/\\,c_i$\n\nD. $n_i \\propto p_i^{2}\\,\\sigma_i^{2}\\,/\\,\\sqrt{c_i}$\n\nE. $n_i \\propto p_i\\,/\\,\\sqrt{c_i}$",
            "solution": "The user wants me to find the optimal allocation rule for sample sizes in a stratified Monte Carlo simulation to minimize the estimator's variance under a fixed computational budget.\n\n### Step 1: Extract Givens\n\nThe problem statement provides the following information:\n-   A multiscale system with a slow variable $S$ taking values in $\\{1, 2, \\ldots, K\\}$.\n-   The probability of the slow variable being in state $i$ is $P(S=i) = p_i$.\n-   For each state $i$, a microscopic simulator generates $n_i$ samples, $\\{X_{ij}\\}_{j=1}^{n_i}$.\n-   The conditional expectation of a function $f(X)$ given $S=i$ is $\\mu_i = \\mathbb{E}[f(X) \\mid S=i]$.\n-   The conditional variance of $f(X)$ given $S=i$ is $\\sigma_i^2 = \\mathrm{Var}(f(X) \\mid S=i)$.\n-   The quantity of interest is the overall average $\\mu = \\mathbb{E}[f(X)] = \\sum_{i=1}^K p_i \\mu_i$.\n-   The computational cost of a single sample in stratum $i$ is $c_i  0$.\n-   The stratified Monte Carlo estimator for $\\mu$ is $\\hat{\\mu} = \\sum_{i=1}^K p_i \\bar{f}_i$, where $\\bar{f}_i = \\frac{1}{n_i} \\sum_{j=1}^{n_i} f(X_{ij})$ is the sample mean for stratum $i$.\n-   There is a fixed total computational budget $C  0$, leading to the constraint $\\sum_{i=1}^K c_i n_i = C$.\n-   Samples within a stratum are independent and identically distributed (i.i.d.), and samples between different strata are independent.\n-   The goal is to find the proportionality rule for the sample sizes $\\{n_i\\}$ that minimizes the asymptotic variance of $\\hat{\\mu}$, allowing for non-integer values of $n_i$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically and mathematically sound. It describes a standard and well-posed problem in computational statistics, specifically the optimal allocation of resources in stratified sampling.\n-   **Scientifically Grounded**: The setup is a canonical example of stratified Monte Carlo, a fundamental technique in numerical methods, statistics, and multiscale modeling. All concepts used (expectation, variance, estimators, cost functions) are standard.\n-   **Well-Posed**: The task is a constrained optimization problem: minimize a function (the variance of the estimator) subject to a linear constraint (the budget). Such problems are well-posed and typically have a unique solution, which can be found using methods like Lagrange multipliers. The problem explicitly allows relaxing the integer constraint on sample sizes, which makes the optimization problem tractable with calculus.\n-   **Objective**: The problem is stated in precise, unambiguous mathematical language.\n\nThe problem does not violate any of the invalidity criteria. It is a valid, well-defined, and solvable problem.\n\n### Step 3: Derivation of the Optimal Allocation Rule\n\nThe goal is to minimize the variance of the estimator $\\hat{\\mu}$ subject to a fixed total cost.\n\n**1. Formulate the Objective Function: The Variance of $\\hat{\\mu}$**\n\nThe estimator is given by $\\hat{\\mu} = \\sum_{i=1}^K p_i \\bar{f}_i$.\nSince the simulations for different strata are independent, the variance of the sum is the sum of the variances:\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\mathrm{Var}\\left(\\sum_{i=1}^K p_i \\bar{f}_i\\right) = \\sum_{i=1}^K \\mathrm{Var}(p_i \\bar{f}_i) $$\nThe probabilities $p_i$ are constants, so they come out squared:\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\sum_{i=1}^K p_i^2 \\mathrm{Var}(\\bar{f}_i) $$\nThe term $\\bar{f}_i$ is the sample mean of $n_i$ i.i.d. random variables, each with variance $\\sigma_i^2$. The variance of a sample mean is the population variance divided by the sample size:\n$$ \\mathrm{Var}(\\bar{f}_i) = \\frac{\\mathrm{Var}(f(X) \\mid S=i)}{n_i} = \\frac{\\sigma_i^2}{n_i} $$\nSubstituting this back, we get the total variance, which is our objective function to minimize:\n$$ V(\\{n_i\\}) = \\mathrm{Var}(\\hat{\\mu}) = \\sum_{i=1}^K \\frac{p_i^2 \\sigma_i^2}{n_i} $$\n\n**2. Formulate the Constraint**\n\nThe total computational cost is fixed at $C$. The cost for stratum $i$ is $c_i n_i$. The total cost is the sum over all strata:\n$$ \\sum_{i=1}^K c_i n_i = C $$\nThis can be written as a constraint function $G(\\{n_i\\}) = 0$:\n$$ G(\\{n_i\\}) = \\sum_{i=1}^K c_i n_i - C = 0 $$\n\n**3. Solve the Constrained Optimization Problem**\n\nWe use the method of Lagrange multipliers. The Lagrangian $\\mathcal{L}$ is defined as:\n$$ \\mathcal{L}(\\{n_i\\}, \\lambda) = V(\\{n_i\\}) + \\lambda G(\\{n_i\\}) = \\left(\\sum_{i=1}^K \\frac{p_i^2 \\sigma_i^2}{n_i}\\right) + \\lambda \\left(\\sum_{i=1}^K c_i n_i - C\\right) $$\nTo find the minimum, we set the partial derivatives of $\\mathcal{L}$ with respect to each $n_k$ (for $k=1, \\ldots, K$) to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial n_k} = \\frac{\\partial}{\\partial n_k} \\left( \\frac{p_k^2 \\sigma_k^2}{n_k} \\right) + \\lambda \\frac{\\partial}{\\partial n_k} (c_k n_k) = 0 $$\n$$ -\\frac{p_k^2 \\sigma_k^2}{n_k^2} + \\lambda c_k = 0 $$\nRearranging this equation to solve for $n_k^2$:\n$$ n_k^2 = \\frac{p_k^2 \\sigma_k^2}{\\lambda c_k} $$\nSince sample sizes $n_k$ must be positive, we take the positive square root:\n$$ n_k = \\sqrt{\\frac{p_k^2 \\sigma_k^2}{\\lambda c_k}} = \\frac{p_k \\sigma_k}{\\sqrt{\\lambda} \\sqrt{c_k}} $$\nHere, $\\lambda$ is the Lagrange multiplier, which is a constant independent of the stratum index $k$. Therefore, $1/\\sqrt{\\lambda}$ is also a constant of proportionality. This gives the proportionality rule for the optimal allocation $n_k$:\n$$ n_k \\propto \\frac{p_k \\sigma_k}{\\sqrt{c_k}} $$\nThis result is intuitive: we should allocate more samples to strata that are more probable (large $p_k$), have higher intrinsic variability (large $\\sigma_k$), and are cheaper to simulate (small $c_k$).\n\n### Evaluation of Options\n\nBased on the derived proportionality $n_i \\propto p_i\\,\\sigma_i\\,/\\,\\sqrt{c_i}$, we evaluate each option:\n\n**A. $n_i \\propto p_i\\,\\sigma_i\\,\\sqrt{c_i}$**\nThis rule suggests allocating more samples to strata with higher costs, which is incorrect. The cost term should be in the denominator.\n**Verdict: Incorrect.**\n\n**B. $n_i \\propto p_i\\,\\sigma_i\\,/\\,\\sqrt{c_i}$**\nThis rule exactly matches our derived optimal allocation.\n**Verdict: Correct.**\n\n**C. $n_i \\propto p_i\\,\\sigma_i^{2}\\,/\\,c_i$**\nThis rule has incorrect powers for both the standard deviation $\\sigma_i$ and the cost $c_i$. The correct powers are $1$ for $\\sigma_i$ and $1/2$ for $c_i$ (in the denominator).\n**Verdict: Incorrect.**\n\n**D. $n_i \\propto p_i^{2}\\,\\sigma_i^{2}\\,/\\,\\sqrt{c_i}$**\nThis rule has incorrect powers for the probability $p_i$ and the standard deviation $\\sigma_i$. These terms appear squared in the variance expression, but the optimization leads to a proportionality with their first powers.\n**Verdict: Incorrect.**\n\n**E. $n_i \\propto p_i\\,/\\,\\sqrt{c_i}$**\nThis rule omits the standard deviation $\\sigma_i$. This allocation would only be optimal in the special case where all strata have the same variance (i.e., $\\sigma_i$ is constant for all $i$), which is not generally true.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "A powerful way to analyze a complex multiscale model is to understand the sources of its output variance. Does the uncertainty arise primarily from individual input parameters, or from their intricate interactions? The Analysis of Variance (ANOVA) decomposition, also known as the Hoeffding-Sobol decomposition, provides a formal method to answer this question. This exercise  offers hands-on practice in applying this technique to partition the total variance of an observable into contributions from main effects and interactions of different orders.",
            "id": "3739084",
            "problem": "In a multiscale modeling and analysis setting, a macroscopic observable is computed as the population mean of a square-integrable integrand under independent microscopic inputs. Consider a Monte Carlo (MC) estimation context with independent random inputs $X$, $Y$, and $Z$ representing coarse and fine scales that interact. The inputs satisfy $X \\sim \\text{Uniform}([0,1])$, $Y \\sim \\text{Uniform}([0,1])$, and $Z \\sim \\mathcal{N}(0,1)$, and they are mutually independent. The integrand is\n$$\nf(X,Y,Z) \\;=\\; \\mu \\;+\\; \\alpha\\,(X-\\tfrac{1}{2}) \\;+\\; \\beta\\,(Y-\\tfrac{1}{2}) \\;+\\; \\gamma\\,Z \\;+\\; \\delta\\,(X-\\tfrac{1}{2})(Y-\\tfrac{1}{2}) \\;+\\; \\varepsilon\\,(X-\\tfrac{1}{2})Z \\;+\\; \\eta\\,(Y-\\tfrac{1}{2})Z \\;+\\; \\theta\\,(X-\\tfrac{1}{2})(Y-\\tfrac{1}{2})Z,\n$$\nwith fixed coefficients $\\mu$, $\\alpha$, $\\beta$, $\\gamma$, $\\delta$, $\\varepsilon$, $\\eta$, and $\\theta$. For this problem, take $\\mu = 0$, $\\alpha = 1$, $\\beta = 2$, $\\gamma = 3$, $\\delta = 4$, $\\varepsilon = 5$, $\\eta = 6$, and $\\theta = 7$.\n\nStarting from first principles (definitions of expectation, variance, and independence), and using only the structure of square-integrable functions and orthogonality induced by independence and centering, decompose the variance of $f$ by interaction order in the sense of Analysis of Variance (ANOVA; also known as Hoeffding–Sobol decomposition). Compute the exact fraction $\\Phi$ of the total variance of $f$ that is attributable to second-order interactions (i.e., the interactions between the pairs $(X,Y)$, $(X,Z)$, and $(Y,Z)$), expressed as a single simplified rational number. Do not round; provide $\\Phi$ exactly.",
            "solution": "### Step 1: Extract Givens\nThe problem provides the following information:\n-   Independent random variables: $X$, $Y$, and $Z$.\n-   Distributions: $X \\sim \\text{Uniform}([0,1])$, $Y \\sim \\text{Uniform}([0,1])$, and $Z \\sim \\mathcal{N}(0,1)$.\n-   Integrand function: $f(X,Y,Z) = \\mu + \\alpha(X-\\frac{1}{2}) + \\beta(Y-\\frac{1}{2}) + \\gamma Z + \\delta(X-\\frac{1}{2})(Y-\\frac{1}{2}) + \\varepsilon(X-\\frac{1}{2})Z + \\eta(Y-\\frac{1}{2})Z + \\theta(X-\\frac{1}{2})(Y-\\frac{1}{2})Z$.\n-   Coefficients: $\\mu = 0$, $\\alpha = 1$, $\\beta = 2$, $\\gamma = 3$, $\\delta = 4$, $\\varepsilon = 5$, $\\eta = 6$, and $\\theta = 7$.\n-   Objective: Compute the exact fraction $\\Phi$ of the total variance of $f$ attributable to second-order interactions.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n-   **Scientific Grounding**: The problem concerns the Analysis of Variance (ANOVA) or Hoeffding-Sobol decomposition, a standard and rigorous technique in statistics, sensitivity analysis, and multiscale modeling for quantifying the influence of input variables on a model output. The specified probability distributions are standard. The integrand $f(X,Y,Z)$ is a polynomial in its arguments, ensuring it is square-integrable with respect to the given probability distributions.\n-   **Well-Posedness**: The problem is fully specified with all necessary data and relationships. The function form and distributions allow for a unique, stable, and meaningful solution for the variance decomposition.\n-   **Objectivity**: The problem is stated in precise mathematical terms, free of ambiguity or subjective claims.\n\nThe problem is valid as it represents a standard exercise in applied mathematics and statistics.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution\nThe core of this problem is the Analysis of Variance (ANOVA) decomposition of the function $f(X,Y,Z)$, which leverages the statistical independence of the input variables $X$, $Y$, and $Z$. This decomposition separates the total variance of $f$ into contributions from individual variables (main effects) and their interactions.\n\nLet us define centered random variables for convenience, as suggested by the structure of $f$:\nLet $X' = X - \\frac{1}{2}$ and $Y' = Y - \\frac{1}{2}$. The variable $Z$ is already centered.\nSince $X \\sim \\text{Uniform}([0,1])$, its expectation is $\\mathbb{E}[X] = \\frac{1}{2}$. Thus, the centered variable $X'$ has zero expectation: $\\mathbb{E}[X'] = \\mathbb{E}[X - \\frac{1}{2}] = \\frac{1}{2} - \\frac{1}{2} = 0$. Similarly, $\\mathbb{E}[Y'] = 0$. The variable $Z \\sim \\mathcal{N}(0,1)$ is given with zero mean, $\\mathbb{E}[Z]=0$.\n\nThe variance of a Uniform distribution on an interval $[a,b]$ is $\\frac{(b-a)^2}{12}$.\nFor $X \\sim \\text{Uniform}([0,1])$, $\\text{Var}(X) = \\frac{(1-0)^2}{12} = \\frac{1}{12}$.\nThe variance of $X'$ is the same as the variance of $X$ since variance is invariant under shifts: $\\text{Var}(X') = \\text{Var}(X) = \\frac{1}{12}$.\nSimilarly, $\\text{Var}(Y') = \\text{Var}(Y) = \\frac{1}{12}$.\nFor $Z \\sim \\mathcal{N}(0,1)$, the variance is $\\text{Var}(Z) = 1$.\n\nFor any random variable $W$ with zero mean, its variance is the expectation of its square: $\\text{Var}(W) = \\mathbb{E}[(W-\\mathbb{E}[W])^2] = \\mathbb{E}[W^2]$.\nThus, we have:\n$\\mathbb{E}[(X')^2] = \\text{Var}(X') = \\frac{1}{12}$\n$\\mathbb{E}[(Y')^2] = \\text{Var}(Y') = \\frac{1}{12}$\n$\\mathbb{E}[Z^2] = \\text{Var}(Z) = 1$\n\nSubstituting the given coefficients and the centered variables into the expression for $f$:\n$f(X,Y,Z) = 1 \\cdot X' + 2 \\cdot Y' + 3 \\cdot Z + 4 \\cdot X'Y' + 5 \\cdot X'Z + 6 \\cdot Y'Z + 7 \\cdot X'Y'Z$.\nThe expectation of $f$ is $\\mathbb{E}[f]$. By linearity of expectation and independence, the expectation of any term containing at least one centered variable is zero. For example, $\\mathbb{E}[X'Y'] = \\mathbb{E}[X']\\mathbb{E}[Y'] = 0 \\cdot 0 = 0$. Since all terms in the sum for $f$ contain at least one centered variable, $\\mathbb{E}[f]=0$.\n\nThe total variance of $f$ is $\\text{Var}(f) = \\mathbb{E}[f^2] - (\\mathbb{E}[f])^2 = \\mathbb{E}[f^2]$.\nThe function $f$ is given as a sum of terms. Let's denote these terms as:\n$f_X = \\alpha X'$, $f_Y = \\beta Y'$, $f_Z = \\gamma Z$ (first-order terms)\n$f_{XY} = \\delta X'Y'$, $f_{XZ} = \\varepsilon X'Z$, $f_{YZ} = \\eta Y'Z$ (second-order terms)\n$f_{XYZ} = \\theta X'Y'Z$ (third-order term)\nSo, $f = f_X + f_Y + f_Z + f_{XY} + f_{XZ} + f_{YZ} + f_{XYZ}$.\n\nThese terms are mutually orthogonal, meaning the expectation of their product is zero. For any two distinct terms $f_S$ and $f_U$ (where $S, U$ are subsets of $\\{X,Y,Z\\}$), $\\mathbb{E}[f_S f_U] = 0$. This is because the product $f_S f_U$ will contain at least one variable raised to an odd power (in this case, power $1$), and the expectation of that centered variable will be zero, making the entire expectation zero due to independence. For example:\n$\\mathbb{E}[f_X f_{XY}] = \\mathbb{E}[(\\alpha X')(\\delta X'Y')] = \\alpha\\delta \\mathbb{E}[(X')^2 Y'] = \\alpha\\delta \\mathbb{E}[(X')^2] \\mathbb{E}[Y'] = \\alpha\\delta \\mathbb{E}[(X')^2] \\cdot 0 = 0$.\n\nDue to this orthogonality, the variance of the sum is the sum of the variances:\n$\\text{Var}(f) = \\text{Var}(\\sum f_S) = \\sum \\text{Var}(f_S)$.\nEach partial variance $\\text{Var}(f_S)$ is equal to $\\mathbb{E}[f_S^2]$ because $\\mathbb{E}[f_S]=0$.\n\nLet's compute the partial variances:\n$D_X = \\text{Var}(f_X) = \\text{Var}(\\alpha X') = \\alpha^2 \\text{Var}(X') = 1^2 \\cdot \\frac{1}{12} = \\frac{1}{12}$.\n$D_Y = \\text{Var}(f_Y) = \\text{Var}(\\beta Y') = \\beta^2 \\text{Var}(Y') = 2^2 \\cdot \\frac{1}{12} = \\frac{4}{12}$.\n$D_Z = \\text{Var}(f_Z) = \\text{Var}(\\gamma Z) = \\gamma^2 \\text{Var}(Z) = 3^2 \\cdot 1 = 9$.\n\n$D_{XY} = \\text{Var}(f_{XY}) = \\mathbb{E}[(\\delta X'Y')^2] = \\delta^2 \\mathbb{E}[(X')^2]\\mathbb{E}[(Y')^2] = 4^2 \\cdot \\frac{1}{12} \\cdot \\frac{1}{12} = \\frac{16}{144}$.\n$D_{XZ} = \\text{Var}(f_{XZ}) = \\mathbb{E}[(\\varepsilon X'Z)^2] = \\varepsilon^2 \\mathbb{E}[(X')^2]\\mathbb{E}[Z^2] = 5^2 \\cdot \\frac{1}{12} \\cdot 1 = \\frac{25}{12}$.\n$D_{YZ} = \\text{Var}(f_{YZ}) = \\mathbb{E}[(\\eta Y'Z)^2] = \\eta^2 \\mathbb{E}[(Y')^2]\\mathbb{E}[Z^2] = 6^2 \\cdot \\frac{1}{12} \\cdot 1 = \\frac{36}{12} = 3$.\n\n$D_{XYZ} = \\text{Var}(f_{XYZ}) = \\mathbb{E}[(\\theta X'Y'Z)^2] = \\theta^2 \\mathbb{E}[(X')^2]\\mathbb{E}[(Y')^2]\\mathbb{E}[Z^2] = 7^2 \\cdot \\frac{1}{12} \\cdot \\frac{1}{12} \\cdot 1 = \\frac{49}{144}$.\n\nThe total variance attributable to second-order interactions, $D_{order\\_2}$, is the sum of the variances of the second-order terms:\n$D_{order\\_2} = D_{XY} + D_{XZ} + D_{YZ} = \\frac{16}{144} + \\frac{25}{12} + 3$.\nTo sum these, we use a common denominator of $144$:\n$D_{order\\_2} = \\frac{16}{144} + \\frac{25 \\cdot 12}{144} + \\frac{3 \\cdot 144}{144} = \\frac{16 + 300 + 432}{144} = \\frac{748}{144}$.\n\nThe total variance, $D_{total}$, is the sum of all partial variances:\n$D_{total} = D_X + D_Y + D_Z + D_{XY} + D_{XZ} + D_{YZ} + D_{XYZ}$.\n$D_{total} = \\frac{1}{12} + \\frac{4}{12} + 9 + \\frac{16}{144} + \\frac{25}{12} + 3 + \\frac{49}{144}$.\nAgain, using the common denominator of $144$:\n$D_{total} = \\frac{12}{144} + \\frac{48}{144} + \\frac{9 \\cdot 144}{144} + \\frac{16}{144} + \\frac{25 \\cdot 12}{144} + \\frac{3 \\cdot 144}{144} + \\frac{49}{144}$\n$D_{total} = \\frac{12 + 48 + 1296 + 16 + 300 + 432 + 49}{144} = \\frac{2153}{144}$.\n\nThe required fraction $\\Phi$ is the ratio of the variance from second-order interactions to the total variance:\n$\\Phi = \\frac{D_{order\\_2}}{D_{total}} = \\frac{748/144}{2153/144} = \\frac{748}{2153}$.\n\nTo ensure the fraction is simplified, we check for common factors. The prime factorization of the numerator is $748 = 4 \\cdot 187 = 2^2 \\cdot 11 \\cdot 17$. The denominator $2153$ is not divisible by $2$, $11$ (since $2-1+5-3=3$), or $17$ (since $2153 = 17 \\cdot 126 + 11$). Thus, the fraction is irreducible.",
            "answer": "$$\\boxed{\\frac{748}{2153}}$$"
        }
    ]
}