{
    "hands_on_practices": [
        {
            "introduction": "The ability to correctly sample volume fluctuations is the defining feature of the isobaric-isothermal ($NPT$) ensemble. This exercise is fundamental, tasking you with deriving the Metropolis-Hastings acceptance criterion for a volume change from first principles. By working through the derivation , you will build a deep understanding of how the $NPT$ probability distribution, proposal densities, and the principle of detailed balance come together to form a valid and correct simulation algorithm.",
            "id": "320670",
            "problem": "In an isobaric-isothermal (NPT) ensemble Monte Carlo simulation, the volume of the simulation box $V$ is a fluctuating variable, in addition to the particle coordinates. A state of the system is described by the volume $V$ and the set of scaled particle coordinates $\\mathbf{s}^N$, where $\\mathbf{s}_i \\in [0, 1)^3$ are related to the real coordinates $\\mathbf{r}_i$ by $\\mathbf{r}_i = L \\mathbf{s}_i$ with $V=L^3$. The equilibrium probability density $\\pi(\\mathbf{s}^N, V)$ of finding the system in a state $(\\mathbf{s}^N, V)$ is given by:\n$$\n\\pi(\\mathbf{s}^N, V) \\propto V^N \\exp\\left[-\\beta\\left(U(\\mathbf{s}^N, V) + PV\\right)\\right]\n$$\nHere, $N$ is the number of particles, $P$ is the constant external pressure, $\\beta = 1/(k_B T)$ is the inverse temperature, and $U(\\mathbf{s}^N, V)$ is the potential energy of the system evaluated at the real particle positions $\\mathbf{r}^N = V^{1/3}\\mathbf{s}^N$.\n\nA common type of Monte Carlo move in NPT simulations is an isotropic volume change, where the system attempts a transition from an old state $o = (\\mathbf{s}^N, V_o)$ to a new state $n = (\\mathbf{s}^N, V_n)$. Note that the scaled coordinates $\\mathbf{s}^N$ remain fixed during this move. The acceptance probability for such a move is given by the Metropolis-Hastings criterion:\n$$\nA(o \\to n) = \\min\\left(1, \\frac{\\pi(n)}{\\pi(o)}\\frac{g(n \\to o)}{g(o \\to n)}\\right)\n$$\nwhere $g(i \\to j)$ is the proposal probability of generating state $j$ from state $i$.\n\nConsider a specific proposal scheme for the volume change where the new volume $V_n$ is generated from the old volume $V_o$ by making a random perturbation to the *logarithm* of the volume. Specifically, a random number $\\xi$ is drawn from a symmetric probability distribution $f(\\xi)$ (i.e., $f(\\xi) = f(-\\xi)$), and the new volume is determined by the relation $\\ln V_n = \\ln V_o + \\xi$.\n\nDerive the argument of the acceptance probability for this specific volume move. The argument is the second term inside the $\\min(1, \\cdot)$ function. Express your final answer in terms of the initial and final volumes ($V_o, V_n$), the change in potential energy $\\Delta U = U(\\mathbf{s}^N, V_n) - U(\\mathbf{s}^N, V_o)$, the change in volume $\\Delta V = V_n - V_o$, the number of particles $N$, the pressure $P$, and the inverse temperature $\\beta$.",
            "solution": "1. Equilibrium weight ratio  \n$$\n\\frac{\\pi(n)}{\\pi(o)}\n=\\frac{V_n^N\\exp\\bigl[-\\beta\\bigl(U_n+P\\,V_n\\bigr)\\bigr]}{V_o^N\\exp\\bigl[-\\beta\\bigl(U_o+P\\,V_o\\bigr)\\bigr]}\n=\\Bigl(\\frac{V_n}{V_o}\\Bigr)^N\\exp\\bigl[-\\beta\\bigl(\\Delta U+P\\,\\Delta V\\bigr)\\bigr].\n$$\n\n2. Proposal probabilities  \nFor $\\ln V_n=\\ln V_o+\\xi$ with symmetric $f(\\xi)$,\n$$\ng(o\\to n)=f(\\xi)\\Bigl|\\frac{d\\xi}{dV_n}\\Bigr|=f(\\xi)\\frac1{V_n},\\qquad\ng(n\\to o)=f(-\\xi)\\frac1{V_o}=f(\\xi)\\frac1{V_o}.\n$$\nThus\n$$\n\\frac{g(n\\to o)}{g(o\\to n)}=\\frac{1/V_o}{1/V_n}=\\frac{V_n}{V_o}.\n$$\n\n3. Acceptance ratio  \n$$\n\\frac{\\pi(n)}{\\pi(o)}\\frac{g(n\\to o)}{g(o\\to n)}\n=\\Bigl(\\frac{V_n}{V_o}\\Bigr)^N\\exp\\bigl[-\\beta(\\Delta U+P\\Delta V)\\bigr]\\times\\frac{V_n}{V_o}\n=\\Bigl(\\frac{V_n}{V_o}\\Bigr)^{N+1}\\exp\\bigl[-\\beta(\\Delta U+P\\Delta V)\\bigr].\n$$",
            "answer": "$$\\boxed{\\left(\\frac{V_n}{V_o}\\right)^{N+1}\\exp\\!\\left[-\\beta\\bigl(\\Delta U+P\\,\\Delta V\\bigr)\\right]}$$"
        },
        {
            "introduction": "Most real-world simulations involve collections of molecules or rigid groups, not just independent atoms. This advanced practice  challenges you to extend the $NPT$ framework to these more complex systems. You will derive the correct acceptance criterion for rigid bodies, which subtly depends on the number of translating groups $M$ rather than atoms $N$, and design a robust algorithm to update coordinates under periodic boundary conditions without violating internal constraints, a critical skill for developing production-level simulation code.",
            "id": "3783187",
            "problem": "You are to design and implement a program that models an isotropic volume-change trial in the constant Number of particles, Pressure, and Temperature (NPT) ensemble, while maintaining holonomic rigid-bond constraints and periodic boundary conditions. The goal is to construct a constraint-preserving coordinate update method after box scaling that guarantees that rigid bonds remain exactly satisfied under periodic boundary conditions, and to compute the generalized Metropolis acceptance exponent associated with the trial, taking into account any additional energy terms required by constraint enforcement.\n\nStart from the following foundational base in statistical mechanics and constrained dynamics:\n- The isothermal-isobaric ensemble (NPT) is defined by a probability density on configuration space and volume proportional to the Boltzmann factor of the enthalpy, with a configuration-space measure that transforms under coordinate changes. \n- Holonomic constraints restrict configurations to a manifold specified by equality relations, and the constrained equilibrium measure contains a metric-determinant factor associated with the constraint manifold.\n- Under isotropic box scaling, the linear dimensions of a cubic box of side length $L$ are scaled by a factor $\\gamma$, so the new side length is $L' = \\gamma L$ and the volume transforms as $V' = \\gamma^{3} V$.\n- For collections of rigid molecules, a physically consistent choice is to scale only the centers of mass and leave internal coordinates (e.g., rigid bond lengths) unchanged, thereby enforcing constraints exactly after the volume move.\n\nYour tasks are:\n1. Derive, from the above base, the acceptance exponent for a Metropolis trial that proposes isotropic box scaling by a factor $\\gamma$ and updates positions using a constraint-preserving rule in which only the centers of mass of rigid groups are scaled and internal coordinates are left unchanged. You must explicitly account for:\n   - The change in potential energy $\\Delta U$.\n   - The mechanical work done against pressure $\\Delta W = p\\,\\Delta V$.\n   - The change in the configuration-space measure induced by scaling the translational degrees of freedom of the rigid groups.\n   - Any additional contributions required by enforcing holonomic constraints (e.g., metric-determinant terms) and whether these terms change under the specified update.\n   The acceptance exponent must be expressed in terms of the temperature $T$, Boltzmann’s constant $k_{\\mathrm{B}}$, the pressure $p$, the initial and final volumes $V$ and $V'$, and the number $M$ of independently translating rigid groups (e.g., molecules). Do not assume point particles; treat rigid groups as the fundamental translating entities.\n\n2. Construct an algorithm that, given a set of atomic positions, a list of rigid groups (each specified by the indices of its atoms), and a set of rigid bonds (pairs of atom indices with fixed lengths), performs an isotropic box-rescaling by the factor $\\gamma$ and updates positions so that:\n   - Rigid bonds remain exactly satisfied after the move.\n   - Periodic boundary conditions in a cubic box are preserved, using the minimum-image convention when checking constraints.\n   Your method must be scientifically sound and numerically robust, avoiding unrealistic assumptions.\n\n3. Implement the derived acceptance exponent and the constraint-preserving update method in a complete, runnable program. The program must compute the acceptance exponent for each test case and verify that all rigid bonds remain satisfied to within a tolerance.\n\nUse the following test suite with specified parameters and units. All energies are in Joules ($\\mathrm{J}$), pressure in Pascals ($\\mathrm{Pa}$), lengths in meters ($\\mathrm{m}$), volumes in cubic meters ($\\mathrm{m^{3}}$), and temperature in Kelvin ($\\mathrm{K}$). Angles are not used. Express the final program outputs as decimal floats with no units.\n\n- Test case $1$ (happy path):\n  - Temperature $T = 300$.\n  - Boltzmann constant $k_{\\mathrm{B}} = 1.380649\\times 10^{-23}$.\n  - Pressure $p = 1.0\\times 10^{5}$.\n  - Initial cubic box side $L = 5.0\\times 10^{-9}$, so $V = L^{3}$.\n  - Isotropic scale factor $\\gamma = 1.02$, so $L' = \\gamma L$ and $V' = L'^{3}$.\n  - Two rigid diatomic molecules ($M = 2$), each with bond length $1.0\\times 10^{-10}$.\n  - Potential energy change $\\Delta U = -2.0\\times 10^{-21}$.\n\n- Test case $2$ (volume decrease):\n  - Same geometry and constants as test case $1$ but with $\\gamma = 0.98$ and $\\Delta U = 1.5\\times 10^{-21}$.\n\n- Test case $3$ (identity move boundary):\n  - Same geometry and constants as test case $1$ but with $\\gamma = 1.00$ and $\\Delta U = 0.0$.\n\n- Test case $4$ (mixed composition and different pressure/temperature):\n  - Temperature $T = 310$.\n  - Boltzmann constant $k_{\\mathrm{B}} = 1.380649\\times 10^{-23}$.\n  - Pressure $p = 5.0\\times 10^{5}$.\n  - Initial cubic box side $L = 6.0\\times 10^{-9}$, so $V = L^{3}$.\n  - Isotropic scale factor $\\gamma = 1.05$, so $L' = \\gamma L$ and $V' = L'^{3}$.\n  - One rigid triatomic group with two rigid bonds of length $1.0\\times 10^{-10}$ and one free atom, giving $M = 2$ translating entities.\n  - Potential energy change $\\Delta U = -1.0\\times 10^{-21}$.\n\nFor each test case, your program must:\n- Perform the constraint-preserving position update after isotropic box scaling.\n- Verify all specified rigid bonds remain satisfied to within a tolerance of $1.0\\times 10^{-12}$ in $\\mathrm{m}$ under the minimum-image convention.\n- Compute the acceptance exponent derived in task $1$.\n- Collect the acceptance exponents for all test cases into a single list.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[result1,result2,result3,result4]”). Each result must be a decimal float. No additional text may be printed.",
            "solution": "The problem requires the derivation of the Metropolis-Hastings acceptance exponent for an isotropic volume change in the NPT (isothermal-isobaric) ensemble for a system of rigid molecules, and the implementation of a corresponding constraint-preserving coordinate update algorithm.\n\n### 1. Derivation of the Acceptance Exponent\n\nThe Metropolis-Hastings acceptance probability for a trial move from an old state `o` to a new state `n` is given by:\n$$\nA(o \\rightarrow n) = \\min\\left(1, \\frac{\\mathcal{P}(n) T(n \\rightarrow o)}{\\mathcal{P}(o) T(o \\rightarrow n)}\\right)\n$$\nwhere $\\mathcal{P}(x)$ is the probability of state $x$ and $T(x \\rightarrow y)$ is the transition probability of proposing a move from $x$ to $y$. The acceptance exponent is the natural logarithm of the ratio, $X = \\ln\\left(\\frac{\\mathcal{P}(n) T(n \\rightarrow o)}{\\mathcal{P}(o) T(o \\rightarrow n)}\\right)$.\n\n**State Probability in the NPT Ensemble**\nIn the NPT ensemble, a microstate is defined by the particle coordinates $\\mathbf{r}^N$ and the system volume $V$. The probability density is proportional to the Boltzmann factor of the enthalpy $H = U(\\mathbf{r}^N) + pV$. For a system of $M$ rigid, independently translating groups (e.g., molecules or free atoms), the relevant coordinates for the volume scaling move are the centers of mass (COMs) of these groups, $\\{\\mathbf{R}_k\\}_{k=1}^M$. The internal coordinates of each group are unaffected by the proposed move.\n\nThe partition function for such a system in the NPT ensemble is given by:\n$$\n\\Delta(N, p, T) \\propto \\int dV \\exp(-\\beta pV) \\int_V d\\mathbf{R}^M \\int d\\mathbf{\\Omega}^M \\exp(-\\beta U(\\{\\mathbf{R}_k\\}, \\{\\mathbf{\\Omega}_k\\}))\n$$\nwhere $\\beta = 1/(k_{\\mathrm{B}}T)$, $\\{\\mathbf{R}_k\\}$ are the $M$ COM coordinates, and $\\{\\mathbf{\\Omega}_k\\}$ are the orientational/internal coordinates. The integration element $d\\mathbf{r}^N$ is separable into translational and internal parts, $d\\mathbf{r}^N = d\\mathbf{R}^M d\\mathbf{\\Omega}^M$ (up to constants).\n\nThe probability of observing a microstate $(\\{\\mathbf{R}_k\\}, \\{\\mathbf{\\Omega}_k\\}, V)$ is proportional to the integrand:\n$$\n\\mathcal{P}(\\{\\mathbf{R}_k\\}, \\{\\mathbf{\\Omega}_k\\}, V) d\\mathbf{R}^M d\\mathbf{\\Omega}^M dV \\propto \\exp(-\\beta(U + pV)) d\\mathbf{R}^M d\\mathbf{\\Omega}^M dV\n$$\n\n**The Trial Move and Coordinate Transformation**\nThe trial move consists of:\n1.  Scaling the box volume from $V$ to $V' = \\gamma^3 V$, where $\\gamma$ is a scaling factor. This means the box side length scales from $L$ to $L' = \\gamma L$.\n2.  Scaling the COM coordinates of the $M$ rigid groups: $\\mathbf{R}_k \\rightarrow \\mathbf{R}_k' = \\gamma \\mathbf{R}_k$.\n3.  Leaving the internal coordinates (and thus orientations $\\mathbf{\\Omega}_k$) unchanged.\n\nThis move is designed to preserve holonomic constraints within each rigid group. Since internal bond vectors are defined by differences of atomic positions relative to the COM, and these are not scaled, the bond lengths remain invariant. Consequently, any metric-determinant factor (Fixman potential) associated with these internal constraints is also unchanged and cancels from the acceptance ratio.\n\n**Jacobian of the Transformation**\nThe ratio of probability densities depends on the Jacobian of the coordinate transformation. The differential volume element transforms as:\n$$\nd\\mathbf{R}'^M = \\left| \\frac{\\partial \\mathbf{R}'^M}{\\partial \\mathbf{R}^M} \\right| d\\mathbf{R}^M\n$$\nSince $\\mathbf{R}_k' = \\gamma \\mathbf{R}_k$ for each of the $M$ groups, the transformation is a scaling by $\\gamma$ of a $3M$-dimensional vector. The Jacobian determinant is:\n$$\n\\left| \\frac{\\partial \\mathbf{R}'^M}{\\partial \\mathbf{R}^M} \\right| = \\det(\\gamma \\mathbf{I}_{3M}) = \\gamma^{3M}\n$$\nwhere $\\mathbf{I}_{3M}$ is the $3M \\times 3M$ identity matrix. The elements $d\\mathbf{\\Omega}^M$ and $dV$ are independent variables of integration.\n\n**Assembling the Acceptance Ratio**\nThe ratio of state probabilities is:\n$$\n\\frac{\\mathcal{P}(n)}{\\mathcal{P}(o)} = \\frac{\\exp(-\\beta(U' + pV')) d\\mathbf{R}'^M d\\mathbf{\\Omega}'^M dV'}{\\exp(-\\beta(U + pV)) d\\mathbf{R}^M d\\mathbf{\\Omega}^M dV}\n$$\nSubstituting $d\\mathbf{R}'^M = \\gamma^{3M} d\\mathbf{R}^M$ and noting that $d\\mathbf{\\Omega}'^M = d\\mathbf{\\Omega}^M$:\n$$\n\\frac{\\mathcal{P}(n)}{\\mathcalP(o)} = \\exp(-\\beta(\\Delta U + p \\Delta V)) \\gamma^{3M}\n$$\nwhere $\\Delta U = U' - U$ and $\\Delta V = V' - V$.\n\nThe factor $\\gamma^{3M}$ can be re-expressed in terms of volumes:\n$$\nV' = \\gamma^3 V \\implies \\gamma^3 = \\frac{V'}{V} \\implies \\gamma^{3M} = \\left(\\frac{V'}{V}\\right)^M = \\exp\\left(M \\ln\\left(\\frac{V'}{V}\\right)\\right)\n$$\nAssuming a symmetric proposal probability for the volume change (e.g., proposing $\\ln V'$ from a symmetric distribution around $\\ln V$), the transition probability ratio $T(n \\rightarrow o)/T(o \\rightarrow n)$ is $1$.\nThe acceptance exponent $X$ is the logarithm of the ratio above:\n$$\nX = \\ln\\left( \\exp(-\\beta(\\Delta U + p \\Delta V)) \\exp\\left(M \\ln\\left(\\frac{V'}{V}\\right)\\right) \\right)\n$$\n$$\nX = -\\frac{\\Delta U + p(V' - V)}{k_{\\mathrm{B}}T} + M \\ln\\left(\\frac{V'}{V}\\right)\n$$\nThis is the final expression for the acceptance exponent. It correctly depends on $M$, the number of independently translating rigid groups, rather than $N$, the total number of atoms.\n\n### 2. Constraint-Preserving Coordinate Update Algorithm\n\nThe algorithm updates atomic positions after an isotropic volume scaling while preserving rigid bond lengths and periodic boundary conditions (PBC).\n\n**Inputs:**\n-   Initial atomic positions: $\\{\\mathbf{r}_i\\}_{i=1}^N$.\n-   Initial cubic box side length: $L$.\n-   Scaling factor: $\\gamma$.\n-   List of rigid groups, where each group is a list of atom indices.\n-   List of rigid bonds (for verification).\n-   Atomic masses (if not uniform, otherwise can be ignored). Assuming uniform masses, the center of mass is the geometric center.\n\n**Algorithm:**\n1.  Calculate the new box side length $L' = \\gamma L$.\n2.  Create an array for the new atomic positions, $\\mathbf{r}'$.\n3.  For each rigid group $k \\in \\{1, \\dots, M\\}$:\n    a.  **Unwrap coordinates**: To correctly calculate the center of mass (COM) of a molecule that may be split by PBC, its atoms must be brought into a contiguous representation.\n        i.   Select a reference atom in the group, e.g., the first one, with position $\\mathbf{r}_{\\text{ref}}$.\n        ii.  For each other atom $j$ in the group, calculate the displacement vector $\\mathbf{d}_j = \\mathbf{r}_j - \\mathbf{r}_{\\text{ref}}$.\n        iii. Apply the minimum image convention to $\\mathbf{d}_j$ with respect to the old box length $L$. The minimum image displacement is $\\mathbf{d}_{j, \\text{mic}} = \\mathbf{d}_j - L \\cdot \\text{round}(\\mathbf{d}_j / L)$.\n        iv.  The unwrapped position of atom $j$ is $\\mathbf{r}_{j,\\text{unwrapped}} = \\mathbf{r}_{\\text{ref}} + \\mathbf{d}_{j, \\text{mic}}$. The reference atom's unwrapped position is just its original position.\n    b.  **Calculate COM**: Compute the COM, $\\mathbf{R}_k$, of the unwrapped atomic positions. For uniform masses: $\\mathbf{R}_k = \\frac{1}{N_k} \\sum_{j \\in \\text{group } k} \\mathbf{r}_{j, \\text{unwrapped}}$, where $N_k$ is the number of atoms in group $k$.\n    c.  **Scale COM**: Calculate the new COM position: $\\mathbf{R}_k' = \\gamma \\mathbf{R}_k$.\n    d.  **Update atom positions**: For each atom $j$ in the group:\n        i.   Find its position vector relative to the old COM: $\\mathbf{u}_j = \\mathbf{r}_{j, \\text{unwrapped}} - \\mathbf{R}_k$.\n        ii.  The new unwrapped position is $\\mathbf{r}'_{j, \\text{unwrapped}} = \\mathbf{R}_k' + \\mathbf{u}_j$.\n        iii. **Rewrap coordinates**: Fold the new unwrapped position back into the new periodic box: $\\mathbf{r}'_j = \\mathbf{r}'_{j, \\text{unwrapped}} - L' \\cdot \\text{floor}(\\mathbf{r}'_{j, \\textunwrapped} / L')$. Store $\\mathbf{r}'_j$.\n\n4.  Repeat for all groups. The collection $\\{\\mathbf{r}'_j\\}$ is the final set of coordinates.\n\n**Verification:**\nTo verify the algorithm, for each specified rigid bond between atoms $i$ and $j$ with original length $d_{ij}$:\n1.  Retrieve the new positions $\\mathbf{r}'_i$ and $\\mathbf{r}'_j$.\n2.  Calculate the new displacement vector $\\mathbf{v}' = \\mathbf{r}'_i - \\mathbf{r}'_j$.\n3.  Apply the minimum image convention to $\\mathbf{v}'$ using the new box length $L'$ to get $\\mathbf{v}'_{\\text{mic}}$.\n4.  Compute the new bond length $d'_{ij} = ||\\mathbf{v}'_{\\text{mic}}||$.\n5.  Confirm that $|d'_{ij} - d_{ij}|$ is less than a small numerical tolerance. This check should pass for all bonds.\n\nThis procedure ensures that the internal structure of rigid groups is perfectly preserved, while their centers of mass are scaled along with the box, satisfying the premises of the NPT move.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing the NPT acceptance exponent for rigid bodies\n    and verifying the constraint-preserving coordinate update algorithm.\n    \"\"\"\n    \n    # Define physical constant\n    KB = 1.380649e-23  # Boltzmann constant in J/K\n\n    # Define test cases from the problem statement.\n    test_cases = [\n        {\n            \"T\": 300.0,\n            \"p\": 1.0e5,\n            \"L\": 5.0e-9,\n            \"gamma\": 1.02,\n            \"M\": 2,\n            \"delta_U\": -2.0e-21,\n            \"groups\": [[0, 1], [2, 3]],\n            \"bonds\": [(0, 1), (2, 3)],\n            \"bond_lengths\": {(0, 1): 1.0e-10, (2, 3): 1.0e-10}\n        },\n        {\n            \"T\": 300.0,\n            \"p\": 1.0e5,\n            \"L\": 5.0e-9,\n            \"gamma\": 0.98,\n            \"M\": 2,\n            \"delta_U\": 1.5e-21,\n            \"groups\": [[0, 1], [2, 3]],\n            \"bonds\": [(0, 1), (2, 3)],\n            \"bond_lengths\": {(0, 1): 1.0e-10, (2, 3): 1.0e-10}\n        },\n        {\n            \"T\": 300.0,\n            \"p\": 1.0e5,\n            \"L\": 5.0e-9,\n            \"gamma\": 1.00,\n            \"M\": 2,\n            \"delta_U\": 0.0,\n            \"groups\": [[0, 1], [2, 3]],\n            \"bonds\": [(0, 1), (2, 3)],\n            \"bond_lengths\": {(0, 1): 1.0e-10, (2, 3): 1.0e-10}\n        },\n        {\n            \"T\": 310.0,\n            \"p\": 5.0e5,\n            \"L\": 6.0e-9,\n            \"gamma\": 1.05,\n            \"M\": 2,\n            \"delta_U\": -1.0e-21,\n            \"groups\": [[0, 1, 2], [3]],\n            \"bonds\": [(0, 1), (0, 2)],\n            \"bond_lengths\": {(0, 1): 1.0e-10, (0, 2): 1.0e-10}\n        }\n    ]\n\n    results = []\n    \n    # --- Helper functions for coordinate manipulation and verification ---\n\n    def minimum_image_vector(vec, box_length):\n        \"\"\"Applies minimum image convention to a vector.\"\"\"\n        return vec - box_length * np.round(vec / box_length)\n\n    def generate_initial_coords(case):\n        \"\"\"Generates a sample initial configuration for verification.\"\"\"\n        L = case[\"L\"]\n        num_atoms = sum(len(g) for g in case[\"groups\"])\n        coords = np.zeros((num_atoms, 3))\n\n        if case[\"M\"] == 2 and len(case[\"groups\"][0]) == 2: # Diatomic cases\n            d = case[\"bond_lengths\"][(0, 1)]\n            # Mol 1\n            com1 = np.array([0.25 * L, 0.5 * L, 0.5 * L])\n            coords[0] = com1 - np.array([d/2, 0, 0])\n            coords[1] = com1 + np.array([d/2, 0, 0])\n            # Mol 2\n            com2 = np.array([0.75 * L, 0.5 * L, 0.5 * L])\n            coords[2] = com2 - np.array([0, d/2, 0])\n            coords[3] = com2 + np.array([0, d/2, 0])\n        elif case[\"M\"] == 2 and len(case[\"groups\"][0]) == 3: # Triatomic + atom case\n            d = case[\"bond_lengths\"][(0, 1)]\n            # Triatomic group at a corner\n            coords[0] = np.array([0.75 * L, 0.75 * L, 0.75 * L])\n            coords[1] = coords[0] + np.array([d, 0, 0])\n            coords[2] = coords[0] + np.array([0, d, 0])\n            # Free atom\n            coords[3] = np.array([0.25 * L, 0.25 * L, 0.25 * L])\n        \n        return coords\n\n    def update_positions(coords, L, gamma, groups):\n        \"\"\"Performs a constraint-preserving coordinate update.\"\"\"\n        L_new = gamma * L\n        new_coords = np.zeros_like(coords)\n\n        for group in groups:\n            # 1. Unwrap coordinates for the group\n            ref_idx = group[0]\n            unwrapped_group_coords = np.zeros((len(group), 3))\n            unwrapped_group_coords[0] = coords[ref_idx]\n            \n            for i, atom_idx in enumerate(group[1:]):\n                disp_vec = coords[atom_idx] - coords[ref_idx]\n                mic_disp = minimum_image_vector(disp_vec, L)\n                unwrapped_group_coords[i+1] = coords[ref_idx] + mic_disp\n\n            # 2. Calculate COM (geometric center)\n            com = np.mean(unwrapped_group_coords, axis=0)\n\n            # 3. Scale COM\n            new_com = gamma * com\n\n            # 4. Update atomic positions\n            for i, atom_idx in enumerate(group):\n                relative_pos = unwrapped_group_coords[i] - com\n                new_unwrapped_pos = new_com + relative_pos\n                \n                # 5. Rewrap into new box\n                new_coords[atom_idx] = new_unwrapped_pos - L_new * np.floor(new_unwrapped_pos / L_new)\n        \n        return new_coords\n\n    def verify_bonds(coords, L, bonds, bond_lengths, tolerance):\n        \"\"\"Verifies that bond lengths are preserved to within tolerance.\"\"\"\n        for i, j in bonds:\n            disp_vec = coords[i] - coords[j]\n            mic_disp = minimum_image_vector(disp_vec, L)\n            new_length = np.linalg.norm(mic_disp)\n            original_length = bond_lengths.get((i, j)) or bond_lengths.get((j, i))\n            \n            if abs(new_length - original_length) > tolerance:\n                raise ValueError(\n                    f\"Bond constraint violated for bond ({i},{j}). \"\n                    f\"Expected: {original_length:.4e}, Found: {new_length:.4e}, \"\n                    f\"Error: {abs(new_length - original_length):.4e}\"\n                )\n\n    # --- Main loop to process each test case ---\n    for case in test_cases:\n        T, p, L, gamma, M, delta_U = case[\"T\"], case[\"p\"], case[\"L\"], case[\"gamma\"], case[\"M\"], case[\"delta_U\"]\n        \n        # --- Task 2/3: Algorithm Verification ---\n        initial_coords = generate_initial_coords(case)\n        new_coords = update_positions(initial_coords, L, gamma, case[\"groups\"])\n        verify_bonds(new_coords, L * gamma, case[\"bonds\"], case[\"bond_lengths\"], tolerance=1.0e-12)\n        \n        # --- Task 1/3: Exponent Calculation ---\n        V = L**3\n        V_prime = (gamma * L)**3\n        \n        # Handle the case gamma = 1 to avoid floating point issues in log\n        if gamma == 1.0:\n            log_term = 0.0\n        else:\n            log_term = M * np.log(V_prime / V)\n            \n        energy_term = -(delta_U + p * (V_prime - V)) / (KB * T)\n        \n        acceptance_exponent = energy_term + log_term\n        results.append(acceptance_exponent)\n\n    # --- Final Output Formatting ---\n    print(f\"[{','.join(f'{r:.15f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A primary application of $NPT$ simulations is to determine a system's equation of state, $\\langle V \\rangle(p)$, which can then be used to build simpler, computationally cheaper models. This exercise  bridges the gap between running simulations and the practice of multiscale modeling. You will use the principles of statistical inference to formulate a weighted least-squares procedure that calibrates a coarse-grained model to reference atomistic data, learning how to properly account for simulation uncertainty in the process.",
            "id": "3783189",
            "problem": "You are tasked with formulating and implementing a calibration procedure for a coarse-grained (CG) model so that Monte Carlo in the constant number, pressure, and temperature (NPT) ensemble reproduces the atomistic equation of state (EOS) across a specified range of pressures. Begin from fundamental properties of the NPT ensemble and derive a principled least-squares formulation for matching the average volume as a function of pressure. Then implement a program that, given reference EOS data, estimates CG parameters by solving a weighted least-squares problem.\n\nThe following scientific setting must be used to derive the calibration method:\n\n- In the NPT ensemble, the probability density of a microstate with coordinates $\\mathbf{R}$ and volume $V$ for a system of $N$ particles at absolute temperature $T$ and external pressure $p$ is, up to normalization, proportional to $\\exp\\left[-\\beta\\left(U(\\mathbf{R};\\boldsymbol{\\theta}) + p\\,V\\right)\\right]\\,V^{N}$, where $U$ is the configurational energy of the CG model parametrized by $\\boldsymbol{\\theta}$, $\\beta = 1/(k_{\\mathrm{B}} T)$, and $k_{\\mathrm{B}}$ is the Boltzmann constant.\n- The equation of state is defined by the relationship between the ensemble average volume $\\langle V \\rangle$ and pressure $p$ at fixed $N$ and $T$, and can be written as $\\langle V\\rangle(p)$.\n- A Monte Carlo (MC) simulation in the NPT ensemble produces an estimator $\\widehat{V}(p)$ of $\\langle V\\rangle(p)$ for each applied pressure $p$, where the estimator fluctuates due to finite sampling.\n\nFrom this base, derive a least-squares objective that estimates the CG parameters $\\boldsymbol{\\theta}$ by matching the model-predicted $\\langle V\\rangle_{\\mathrm{CG}}(p;\\boldsymbol{\\theta})$ to a reference atomistic EOS $\\langle V\\rangle_{\\mathrm{ref}}(p)$ over a discrete pressure set $\\{p_i\\}_{i=1}^M$. The derivation must be first-principles, beginning from the NPT distribution and statistical estimation, without shortcut formulas.\n\nFor numerical implementation, consider the following two parametric surrogates for $\\langle V\\rangle_{\\mathrm{CG}}(p;\\boldsymbol{\\theta})$ over a pressure window where such forms are physically plausible:\n\n1. Exponential compressibility model: $\\langle V\\rangle_{\\mathrm{CG}}(p;\\boldsymbol{\\theta}) = \\theta_0 \\exp\\left(-\\theta_1 \\left(p - p_{\\mathrm{ref}}\\right)\\right)$, where $\\theta_0$ has units of $\\mathrm{nm}^3$, $\\theta_1$ has units of $\\mathrm{bar}^{-1}$, and $p_{\\mathrm{ref}}$ is a fixed reference pressure in $\\mathrm{bar}$.\n2. Quadratic polynomial model: $\\langle V\\rangle_{\\mathrm{CG}}(p;\\boldsymbol{\\theta}) = \\theta_0 - \\theta_1 p + \\theta_2 p^2$, where $\\theta_0$ has units of $\\mathrm{nm}^3$, $\\theta_1$ has units of $\\mathrm{nm}^3/\\mathrm{bar}$, and $\\theta_2$ has units of $\\mathrm{nm}^3/\\mathrm{bar}^2$.\n\nAssume that the reference values $\\widehat{V}_i$ at pressures $p_i$ are provided along with per-point standard deviations $\\sigma_i$ that quantify the measurement uncertainty in $\\mathrm{nm}^3$. Using a weighted least-squares formulation with weights $w_i = 1/\\sigma_i^2$, estimate $\\boldsymbol{\\theta}$ for each model by minimizing the sum of squared, uncertainty-normalized residuals between $\\langle V\\rangle_{\\mathrm{CG}}(p_i;\\boldsymbol{\\theta})$ and $\\widehat{V}_i$.\n\nYour program must implement the following:\n\n- A weighted least-squares solver for both parametric models. For the exponential model, use the logarithmic transformation $\\ln \\widehat{V}_i = \\ln \\theta_0 - \\theta_1 \\left(p_i - p_{\\mathrm{ref}}\\right)$ to obtain a linear least-squares problem in $(\\ln \\theta_0, \\theta_1)$ with weights $w_i$. For the quadratic model, directly solve the linear weighted least-squares system for $(\\theta_0, \\theta_1, \\theta_2)$.\n- A fixed test suite of three cases, each specifying $(p_i, \\widehat{V}_i, \\sigma_i)$ and model type, with the following datasets. Pressures must be in $\\mathrm{bar}$ and volumes must be in $\\mathrm{nm}^3$. The standard deviations must be in $\\mathrm{nm}^3$.\n\nTest suite:\n- Case A (happy path, exponential): $p_{\\mathrm{ref}} = 1$ $\\mathrm{bar}$; pressures $[0, 50, 100, 150, 200]$ $\\mathrm{bar}$; true parameters $\\theta_0 = 10.0$ $\\mathrm{nm}^3$, $\\theta_1 = 0.002$ $\\mathrm{bar}^{-1}$; reference volumes computed as $\\widehat{V}_i = \\theta_0 \\exp\\left(-\\theta_1\\left(p_i - p_{\\mathrm{ref}}\\right)\\right)$; standard deviations $\\sigma_i = 0.01$ $\\mathrm{nm}^3$ for all $i$.\n- Case B (boundary coverage, quadratic): pressures $[0, 250, 500]$ $\\mathrm{bar}$; true parameters $(\\theta_0, \\theta_1, \\theta_2) = (12.0, 0.01, 10^{-5})$ with units as defined above; reference volumes computed as $\\widehat{V}_i = \\theta_0 - \\theta_1 p_i + \\theta_2 p_i^2$; standard deviations $\\sigma_i = 0.01$ $\\mathrm{nm}^3$ for all $i$.\n- Case C (edge case, exponential with minimal points): $p_{\\mathrm{ref}} = 1$ $\\mathrm{bar}$; pressures $[1, 2]$ $\\mathrm{bar}$; true parameters $\\theta_0 = 9.9$ $\\mathrm{nm}^3$, $\\theta_1 = 0.005$ $\\mathrm{bar}^{-1}$; reference volumes computed as $\\widehat{V}_i = \\theta_0 \\exp\\left(-\\theta_1\\left(p_i - p_{\\mathrm{ref}}\\right)\\right)$; standard deviations $\\sigma_i = 0.01$ $\\mathrm{nm}^3$ for all $i$.\n\nImplementation requirements:\n- Solve the weighted least-squares problems deterministically using numerical linear algebra. Do not perform any random sampling.\n- For the exponential model, return the parameters as $(\\theta_0, \\theta_1)$ where $\\theta_0$ is in $\\mathrm{nm}^3$ and $\\theta_1$ is in $\\mathrm{bar}^{-1}$. For the quadratic model, return $(\\theta_0, \\theta_1, \\theta_2)$ with units as defined above.\n- Express all output parameters as floating-point numbers without units in the output string, but you must ensure the code internally respects the physical units.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each list element should itself be a Python-style list representing the parameter vector for one case, in the order Case A, Case B, Case C. For example: $[\\,[\\theta_{0}^{(A)},\\theta_{1}^{(A)}],\\,[\\theta_{0}^{(B)},\\theta_{1}^{(B)},\\theta_{2}^{(B)}],\\,[\\theta_{0}^{(C)},\\theta_{1}^{(C)}]\\,]$.\n\nThe expected output types are lists of floats. There must be exactly three list elements corresponding to the three cases.",
            "solution": "The objective is to determine the parameters $\\boldsymbol{\\theta}$ of a coarse-grained (CG) model by ensuring that its equation of state, $\\langle V \\rangle_{\\mathrm{CG}}(p; \\boldsymbol{\\theta})$, reproduces a set of reference data points $\\{ (p_i, \\widehat{V}_i, \\sigma_i) \\}_{i=1}^M$ obtained from a more fundamental, atomistic model. The value $\\widehat{V}_i$ represents the estimated average volume at pressure $p_i$, and $\\sigma_i$ is the standard deviation of this estimate, quantifying its statistical uncertainty. The calibration will be performed by minimizing a properly weighted sum of squared differences between the CG model's predictions and the reference data.\n\nThe principled foundation for this procedure is the method of Maximum Likelihood Estimation (MLE). We assume that each data point $\\widehat{V}_i$ is an independent measurement drawn from a probability distribution centered on the true, underlying average volume $\\langle V \\rangle_{\\mathrm{ref}}(p_i)$, with a standard deviation of $\\sigma_i$. Invoking the Central Limit Theorem, which applies to averages computed from long simulations, we model this distribution as a Gaussian (Normal) distribution: $\\widehat{V}_i \\sim \\mathcal{N}(\\langle V \\rangle_{\\mathrm{ref}}(p_i), \\sigma_i^2)$.\n\nThe goal is to find parameters $\\boldsymbol{\\theta}$ such that the CG model provides the best possible approximation to the reference system, i.e., $\\langle V \\rangle_{\\mathrm{CG}}(p; \\boldsymbol{\\theta}) \\approx \\langle V \\rangle_{\\mathrm{ref}}(p)$. We can therefore rephrase the statistical model for our observations in terms of the CG model: $\\widehat{V}_i \\sim \\mathcal{N}(\\langle V \\rangle_{\\mathrm{CG}}(p_i; \\boldsymbol{\\theta}), \\sigma_i^2)$. The probability density of observing a single data point $\\widehat{V}_i$ given the model is:\n$$ P(\\widehat{V}_i | \\boldsymbol{\\theta}) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left( -\\frac{(\\widehat{V}_i - \\langle V\\rangle_{\\mathrm{CG}}(p_i; \\boldsymbol{\\theta}))^2}{2\\sigma_i^2} \\right) $$\nSince the measurements are independent, the total likelihood of observing the entire dataset $\\{\\widehat{V}_i\\}$ is the product of the individual probabilities:\n$$ \\mathcal{L}(\\boldsymbol{\\theta}) = \\prod_{i=1}^M P(\\widehat{V}_i | \\boldsymbol{\\theta}) = \\prod_{i=1}^M \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left( -\\frac{(\\widehat{V}_i - \\langle V\\rangle_{\\mathrm{CG}}(p_i; \\boldsymbol{\\theta}))^2}{2\\sigma_i^2} \\right) $$\nTo find the optimal parameters $\\boldsymbol{\\theta}$, we maximize this likelihood function. It is mathematically equivalent and numerically more stable to maximize the logarithm of the likelihood (the log-likelihood, $\\ln\\mathcal{L}$):\n$$ \\ln \\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_{i=1}^M \\left( -\\frac{1}{2}\\ln(2\\pi\\sigma_i^2) - \\frac{(\\widehat{V}_i - \\langle V\\rangle_{\\mathrm{CG}}(p_i; \\boldsymbol{\\theta}))^2}{2\\sigma_i^2} \\right) $$\nMaximizing $\\ln \\mathcal{L}(\\boldsymbol{\\theta})$ with respect to $\\boldsymbol{\\theta}$ is equivalent to minimizing the terms that depend on $\\boldsymbol{\\theta}$. This leads to the minimization of the objective function $\\chi^2(\\boldsymbol{\\theta})$:\n$$ \\min_{\\boldsymbol{\\theta}} \\chi^2(\\boldsymbol{\\theta}) = \\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^M \\frac{(\\widehat{V}_i - \\langle V\\rangle_{\\mathrm{CG}}(p_i; \\boldsymbol{\\theta}))^2}{\\sigma_i^2} = \\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^M w_i (\\widehat{V}_i - \\langle V\\rangle_{\\mathrm{CG}}(p_i; \\boldsymbol{\\theta}))^2 $$\nwhere $w_i = 1/\\sigma_i^2$ are the weights. This is the definition of a weighted least-squares (WLS) problem. It demonstrates that WLS is the statistically rigorous method for parameter fitting when the data points have independent, Gaussian-distributed errors with known variances.\n\nWe now apply this WLS framework to the two specified parametric models.\n\n**1. Quadratic Polynomial Model**\n\nThe model is given by $\\langle V\\rangle_{\\mathrm{CG}}(p;\\boldsymbol{\\theta}) = \\theta_0 - \\theta_1 p + \\theta_2 p^2$, with $\\boldsymbol{\\theta} = (\\theta_0, \\theta_1, \\theta_2)$. This model is linear in its parameters. We can express the system of equations for all data points in matrix form, $\\mathbf{y} \\approx \\mathbf{A}\\mathbf{c}$, where we aim to find the coefficient vector $\\mathbf{c}$ that minimizes the weighted squared error.\nLet's define a standard polynomial basis:\n$$ V(p) = c_0 \\cdot 1 + c_1 \\cdot p + c_2 \\cdot p^2 $$\nComparing this to the model, we have the relationships $c_0 = \\theta_0$, $c_1 = -\\theta_1$, and $c_2 = \\theta_2$.\nThe WLS problem is to find $\\mathbf{c} = (c_0, c_1, c_2)^T$ that minimizes $\\sum_{i=1}^M w_i (\\widehat{V}_i - (c_0 + c_1 p_i + c_2 p_i^2))^2$.\nThis can be written in matrix algebra as minimizing $\\|\\mathbf{W}^{1/2}(\\mathbf{y} - \\mathbf{A}\\mathbf{c})\\|_2^2$, where:\n- $\\mathbf{y}$ is the column vector of observed volumes, $\\mathbf{y}_i = \\widehat{V}_i$.\n- $\\mathbf{A}$ is the design matrix, whose $i$-th row is $(1, p_i, p_i^2)$.\n- $\\mathbf{W}$ is the diagonal matrix of weights, $\\mathbf{W}_{ii} = w_i = 1/\\sigma_i^2$.\n\nThis WLS problem is solved numerically by transforming it into a standard unweighted least-squares problem. We define a scaled system $\\mathbf{y}' = \\mathbf{A}'\\mathbf{c}$, where $\\mathbf{A}' = \\mathbf{W}^{1/2}\\mathbf{A}$ and $\\mathbf{y}' = \\mathbf{W}^{1/2}\\mathbf{y}$. This is achieved by multiplying the $i$-th row of $\\mathbf{A}$ and the $i$-th element of $\\mathbf{y}$ by $\\sqrt{w_i} = 1/\\sigma_i$. The standard least-squares solution for $\\mathbf{c}$ can then be found using numerical linear algebra, for which the normal equations are $(\\mathbf{A}'^T \\mathbf{A}')\\mathbf{c} = \\mathbf{A}'^T \\mathbf{y}'$. After solving for $\\mathbf{c} = (c_0, c_1, c_2)^T$, we recover the model parameters as $\\theta_0=c_0$, $\\theta_1=-c_1$, and $\\theta_2=c_2$.\n\n**2. Exponential Compressibility Model**\n\nThe model is $\\langle V\\rangle_{\\mathrm{CG}}(p;\\boldsymbol{\\theta}) = \\theta_0 \\exp\\left(-\\theta_1 \\left(p - p_{\\mathrm{ref}}\\right)\\right)$, with $\\boldsymbol{\\theta} = (\\theta_0, \\theta_1)$. This model is non-linear in $\\theta_1$. As suggested, we linearize it by taking the natural logarithm:\n$$ \\ln \\langle V\\rangle_{\\mathrm{CG}} = \\ln \\theta_0 - \\theta_1 (p - p_{\\mathrm{ref}}) $$\nLet's define a new set of parameters for the linear problem, $c_0 = \\ln \\theta_0$ and $c_1 = \\theta_1$. Let the transformed target variable be $y'_i = \\ln \\widehat{V}_i$. The model becomes:\n$$ y'_i \\approx c_0 - c_1 (p_i - p_{\\mathrm{ref}}) = c_0 \\cdot 1 + c_1 \\cdot (-(p_i - p_{\\mathrm{ref}})) $$\nThis is a linear model for the parameters $\\mathbf{c} = (c_0, c_1)^T$.\n\nCrucially, transforming the dependent variable from $\\widehat{V}_i$ to $\\ln \\widehat{V}_i$ also transforms the error distribution. We must propagate the uncertainty to define the correct weights for the new linear problem. For a function $f(x)$ with uncertainty $\\sigma_x$, the uncertainty in $f$ is $\\sigma_f \\approx |f'(x)| \\sigma_x$. Here, our function is $f(V) = \\ln V$, so $f'(V) = 1/V$. The standard deviation $\\sigma'_{i}$ for the variable $y'_i = \\ln \\widehat{V}_i$ is therefore:\n$$ \\sigma'_{i} \\approx \\left| \\frac{d(\\ln V)}{dV} \\bigg|_{V=\\widehat{V}_i} \\right| \\sigma_i = \\frac{\\sigma_i}{\\widehat{V}_i} $$\nThe new weights $w'_i$ for the linearized WLS problem are the reciprocal of the new variances:\n$$ w'_i = \\frac{1}{(\\sigma'_{i})^2} = \\left(\\frac{\\widehat{V}_i}{\\sigma_i}\\right)^2 $$\nThe WLS problem for the linearized exponential model is to find $\\mathbf{c} = (\\ln\\theta_0, \\theta_1)^T$ that minimizes $\\sum_{i=1}^M w'_i (\\ln\\widehat{V}_i - (\\ln\\theta_0 - \\theta_1(p_i-p_{\\text{ref}})))^2$.\nWe solve this by setting up the linear system $\\mathbf{y}' \\approx \\mathbf{A}\\mathbf{c}$, where:\n- $\\mathbf{y}'$ is the column vector with elements $\\mathbf{y}'_i = \\ln\\widehat{V}_i$.\n- $\\mathbf{A}$ is the design matrix whose $i$-th row is $(1, -(p_i-p_{\\text{ref}}))$.\n- $\\mathbf{c}$ is the parameter vector $(c_0, c_1)^T = (\\ln \\theta_0, \\theta_1)^T$.\n\nThe WLS solution is found by scaling the $i$-th row of $\\mathbf{A}$ and $\\mathbf{y}'$ by $\\sqrt{w'_i} = \\widehat{V}_i/\\sigma_i$ and solving the resulting standard least-squares problem. Once the coefficient vector $\\mathbf{c}=(c_0, c_1)^T$ is found, we recover the original model parameters as $\\theta_0 = \\exp(c_0)$ and $\\theta_1 = c_1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_exponential(p, V, sigma, p_ref):\n    \"\"\"\n    Solves for the parameters of the exponential model using weighted least-squares\n    on the logarithmic form of the model.\n    Model: V(p) = theta0 * exp(-theta1 * (p - p_ref))\n    Linearized model: ln(V) = ln(theta0) - theta1 * (p - p_ref)\n    Let c0 = ln(theta0), c1 = theta1.\n    ln(V) = c0 - c1 * (p - p_ref) = c0 * 1 + c1 * -(p - p_ref)\n    \"\"\"\n    p = np.asarray(p, dtype=float)\n    V = np.asarray(V, dtype=float)\n    sigma = np.asarray(sigma, dtype=float)\n\n    # Dependent variable for the linear model\n    y_prime = np.log(V)\n\n    # Design matrix for the linear model: y' = A.c\n    # where columns of A are basis functions: 1 and -(p - p_ref)\n    A = np.vstack([np.ones_like(p), -(p - p_ref)]).T\n\n    # Weights for the linearized problem, derived from error propagation:\n    # sigma_lnV = sigma_V / V\n    # w_lnV = 1 / sigma_lnV^2 = (V / sigma)^2\n    # We apply sqrt(w) to A and y to perform WLS.\n    w_sqrt = V / sigma\n    \n    # Scale system for WLS: (w_sqrt * A) c = (w_sqrt * y_prime)\n    A_w = A * w_sqrt[:, np.newaxis]\n    y_w = y_prime * w_sqrt\n\n    # Solve the ordinary least-squares problem on the weighted system\n    c, _, _, _ = np.linalg.lstsq(A_w, y_w, rcond=None)\n    \n    c0, c1 = c\n\n    # Recover original parameters\n    theta0 = np.exp(c0)\n    theta1 = c1\n    \n    return [theta0, theta1]\n\ndef solve_quadratic(p, V, sigma):\n    \"\"\"\n    Solves for the parameters of the quadratic model using weighted least-squares.\n    Model: V(p) = theta0 - theta1*p + theta2*p^2\n    Let's fit to V(p) = c0 + c1*p + c2*p^2\n    Then: theta0=c0, theta1=-c1, theta2=c2\n    \"\"\"\n    p = np.asarray(p, dtype=float)\n    V = np.asarray(V, dtype=float)\n    sigma = np.asarray(sigma, dtype=float)\n\n    y = V\n\n    # Design matrix from polynomial basis functions: 1, p, p^2\n    # np.vander with increasing=True gives columns p^0, p^1, p^2, ...\n    A = np.vander(p, N=3, increasing=True)\n    \n    # Weights for WLS: w = 1 / sigma^2\n    # We apply sqrt(w) = 1/sigma to A and y\n    w_sqrt = 1.0 / sigma\n    \n    # Scale system for WLS: (w_sqrt * A) c = (w_sqrt * y)\n    A_w = A * w_sqrt[:, np.newaxis]\n    y_w = y * w_sqrt\n    \n    # Solve the ordinary least-squares problem for coefficients c = (c0, c1, c2)\n    c, _, _, _ = np.linalg.lstsq(A_w, y_w, rcond=None)\n    \n    c0, c1, c2 = c\n\n    # Recover model parameters\n    theta0 = c0\n    theta1 = -c1\n    theta2 = c2\n    \n    return [theta0, theta1, theta2]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = []\n\n    # Case A: happy path, exponential\n    p_a = np.array([0, 50, 100, 150, 200], dtype=float)\n    theta0_a_true, theta1_a_true = 10.0, 0.002\n    p_ref_a = 1.0\n    V_a = theta0_a_true * np.exp(-theta1_a_true * (p_a - p_ref_a))\n    sigma_a = np.full_like(p_a, 0.01)\n    test_cases.append({\n        'model': 'exponential',\n        'p': p_a,\n        'V': V_a,\n        'sigma': sigma_a,\n        'p_ref': p_ref_a\n    })\n\n    # Case B: boundary coverage, quadratic\n    p_b = np.array([0, 250, 500], dtype=float)\n    theta0_b_true, theta1_b_true, theta2_b_true = 12.0, 0.01, 1.0e-5\n    V_b = theta0_b_true - theta1_b_true * p_b + theta2_b_true * p_b**2\n    sigma_b = np.full_like(p_b, 0.01)\n    test_cases.append({\n        'model': 'quadratic',\n        'p': p_b,\n        'V': V_b,\n        'sigma': sigma_b\n    })\n\n    # Case C: edge case, exponential with minimal points\n    p_c = np.array([1, 2], dtype=float)\n    theta0_c_true, theta1_c_true = 9.9, 0.005\n    p_ref_c = 1.0\n    V_c = theta0_c_true * np.exp(-theta1_c_true * (p_c - p_ref_c))\n    sigma_c = np.full_like(p_c, 0.01)\n    test_cases.append({\n        'model': 'exponential',\n        'p': p_c,\n        'V': V_c,\n        'sigma': sigma_c,\n        'p_ref': p_ref_c\n    })\n\n    results = []\n    for case in test_cases:\n        if case['model'] == 'exponential':\n            params = solve_exponential(case['p'], case['V'], case['sigma'], case['p_ref'])\n        elif case['model'] == 'quadratic':\n            params = solve_quadratic(case['p'], case['V'], case['sigma'])\n        results.append(params)\n\n    # Format output as a string representation of a list of lists.\n    # The str() function on a list provides the required spacing.\n    # Ex: str([1.2, 3.4]) - '[1.2, 3.4]'\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}