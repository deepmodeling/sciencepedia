{
    "hands_on_practices": [
        {
            "introduction": "The stability of a numerical method, which governs how errors propagate over time, is as crucial as its accuracy, especially for the stiff systems common in multiscale modeling. We analyze stability by applying a method to the linear test equation $y' = \\lambda y$ and studying the amplification factor, or stability function, $R(z)$, where $z = h\\lambda$. This exercise will guide you through deriving the stability function for the high-order 2-stage Gauss-Legendre method, providing deep insight into fundamental concepts like A-stability and L-stability that determine a method's suitability for stiff problems. ",
            "id": "3824469",
            "problem": "Consider the linear test equation for an Ordinary Differential Equation (ODE) initial value problem, $y'(t)=\\lambda y(t)$, where $\\lambda \\in \\mathbb{C}$ and $y(t)$ is scalar. In multiscale modeling contexts, time integration schemes are assessed via the amplification factor on this test equation, defined by $z=h\\lambda$, where $h$ is the time step. Consider the $2$-stage Gauss–Legendre collocation Runge–Kutta (RK) method with the following Butcher tableau data:\n$$\nA=\\begin{pmatrix}\n\\frac{1}{4}  \\frac{1}{4}-\\frac{\\sqrt{3}}{6}\\\\\n\\frac{1}{4}+\\frac{\\sqrt{3}}{6}  \\frac{1}{4}\n\\end{pmatrix},\\quad\n\\mathbf{b}=\\begin{pmatrix}\\frac{1}{2}\\\\ \\frac{1}{2}\\end{pmatrix},\\quad\n\\mathbf{c}=\\begin{pmatrix}\\frac{1}{2}-\\frac{\\sqrt{3}}{6}\\\\ \\frac{1}{2}+\\frac{\\sqrt{3}}{6}\\end{pmatrix}.\n$$\nStarting from the stage equations and update rule that define a general Runge–Kutta method applied to the test equation $y'(t)=\\lambda y(t)$, derive the amplification factor (stability function) $R(z)$ so that the one-step update satisfies $y_{n+1}=R(z)\\,y_n$. Then, discuss the analytic properties of $R(z)$ as a function of $z\\in\\mathbb{C}$, including:\n- The location of its poles,\n- Its behavior on the imaginary axis,\n- Its asymptotic behavior as $|z|\\to\\infty$,\n- Whether the method is $A$-stable and whether it is $L$-stable.\n\nExpress your final $R(z)$ in a closed-form analytic expression in $z$. No numerical rounding is required. The final answer must be a single analytic expression for $R(z)$.",
            "solution": "We begin from the fundamental definition of a Runge–Kutta (RK) method applied to the linear test equation $y'(t)=\\lambda y(t)$, with $z=h\\lambda$. For an $s$-stage RK method with Butcher coefficients $A\\in\\mathbb{R}^{s\\times s}$, $\\mathbf{b}\\in\\mathbb{R}^{s}$, and $\\mathbf{c}\\in\\mathbb{R}^{s}$, the stage values $\\mathbf{K}\\in\\mathbb{C}^{s}$ are defined by\n$$\n\\mathbf{K}=\\lambda\\,y_n\\,\\mathbf{1}+z\\,A\\,\\mathbf{K},\n$$\nwhere $\\mathbf{1}\\in\\mathbb{R}^{s}$ is the vector of ones and we have used $z=h\\lambda$. Rearranging gives\n$$\n\\left(I-zA\\right)\\mathbf{K}=\\lambda\\,y_n\\,\\mathbf{1},\\quad\\text{so}\\quad \\mathbf{K}=\\lambda\\,y_n\\,(I-zA)^{-1}\\mathbf{1}.\n$$\nThe RK update is\n$$\ny_{n+1}=y_n+h\\,\\mathbf{b}^{\\top}\\mathbf{K}=y_n+z\\,\\mathbf{b}^{\\top}(I-zA)^{-1}\\mathbf{1}\\,y_n,\n$$\nhence the amplification factor (stability function) is\n$$\nR(z)=1+z\\,\\mathbf{b}^{\\top}(I-zA)^{-1}\\mathbf{1}.\n$$\nWe now compute $R(z)$ for the given $2$-stage Gauss–Legendre collocation method. Denote\n$$\nA=\\begin{pmatrix}\na  b\\\\\nc  a\n\\end{pmatrix},\\quad a=\\frac{1}{4},\\quad b=\\frac{1}{4}-\\frac{\\sqrt{3}}{6},\\quad c=\\frac{1}{4}+\\frac{\\sqrt{3}}{6},\\quad \\mathbf{b}=\\begin{pmatrix}\\frac{1}{2}\\\\ \\frac{1}{2}\\end{pmatrix}.\n$$\nThen\n$$\nI-zA=\\begin{pmatrix}\n1-za  -zb\\\\\n-zc  1-za\n\\end{pmatrix}.\n$$\nThe inverse of a $2\\times 2$ matrix is\n$$\n(I-zA)^{-1}=\\frac{1}{\\det(I-zA)}\\begin{pmatrix}\n1-za  zb\\\\\nzc  1-za\n\\end{pmatrix},\n$$\nwith determinant\n$$\n\\det(I-zA)=(1-za)^2-z^2\\,bc.\n$$\nCompute $bc$:\n$$\nbc=\\left(\\frac{1}{4}-\\frac{\\sqrt{3}}{6}\\right)\\left(\\frac{1}{4}+\\frac{\\sqrt{3}}{6}\\right)=\\frac{1}{16}-\\left(\\frac{\\sqrt{3}}{6}\\right)^2=\\frac{1}{16}-\\frac{3}{36}=\\frac{1}{16}-\\frac{1}{12}=-\\frac{1}{48}.\n$$\nTherefore\n$$\n\\det(I-zA)=(1-za)^2+\\frac{z^2}{48}=\\left(1-\\frac{z}{4}\\right)^2+\\frac{z^2}{48}=1-\\frac{z}{2}+\\frac{z^2}{16}+\\frac{z^2}{48}=1-\\frac{z}{2}+\\frac{z^2}{12}.\n$$\nNext, compute $(I-zA)^{-1}\\mathbf{1}$:\n$$\n(I-zA)^{-1}\\mathbf{1}=\\frac{1}{\\det(I-zA)}\\begin{pmatrix}\n(1-za)+zb\\\\\nzc+(1-za)\n\\end{pmatrix}.\n$$\nNote $a-b=\\frac{1}{4}-\\left(\\frac{1}{4}-\\frac{\\sqrt{3}}{6}\\right)=\\frac{\\sqrt{3}}{6}$ and $c-a=\\left(\\frac{1}{4}+\\frac{\\sqrt{3}}{6}\\right)-\\frac{1}{4}=\\frac{\\sqrt{3}}{6}$, so\n$$\n(1-za)+zb=1-z(a-b)=1-z\\frac{\\sqrt{3}}{6},\\qquad zc+(1-za)=1+z(c-a)=1+z\\frac{\\sqrt{3}}{6}.\n$$\nTherefore\n$$\n(I-zA)^{-1}\\mathbf{1}=\\frac{1}{\\det(I-zA)}\\begin{pmatrix}\n1-\\frac{\\sqrt{3}}{6}z\\\\\n1+\\frac{\\sqrt{3}}{6}z\n\\end{pmatrix}.\n$$\nNow multiply by $\\mathbf{b}^{\\top}$:\n$$\n\\mathbf{b}^{\\top}(I-zA)^{-1}\\mathbf{1}=\\frac{1}{\\det(I-zA)}\\left[\\frac{1}{2}\\left(1-\\frac{\\sqrt{3}}{6}z\\right)+\\frac{1}{2}\\left(1+\\frac{\\sqrt{3}}{6}z\\right)\\right]=\\frac{1}{\\det(I-zA)}.\n$$\nHence\n$$\nR(z)=1+z\\,\\frac{1}{\\det(I-zA)}=1+\\frac{z}{1-\\frac{z}{2}+\\frac{z^2}{12}}=\\frac{1-\\frac{z}{2}+\\frac{z^2}{12}+z}{1-\\frac{z}{2}+\\frac{z^2}{12}}=\\frac{1+\\frac{z}{2}+\\frac{z^2}{12}}{1-\\frac{z}{2}+\\frac{z^2}{12}}.\n$$\nThis rational form is the $[2/2]$ Padé approximant of $\\exp(z)$, consistent with the order-$4$ collocation method.\n\nWe now discuss the analytic properties of $R(z)$:\n\n$1.$ Poles: Poles occur where the denominator vanishes,\n$$\n1-\\frac{z}{2}+\\frac{z^2}{12}=0\\quad\\Longleftrightarrow\\quad z^2-6z+12=0.\n$$\nThe roots are\n$$\nz=\\frac{6\\pm\\sqrt{36-48}}{2}=3\\pm i\\sqrt{3}.\n$$\nThus, $R(z)$ is meromorphic with two simple poles at $z=3\\pm i\\sqrt{3}$, both in the open right half-plane, implying analyticity in the closed left half-plane.\n\n$2.$ Behavior on the imaginary axis: Let $z=i\\omega$, with $\\omega\\in\\mathbb{R}$. Then the numerator and denominator are complex conjugates,\n$$\n\\text{num}=1+\\frac{i\\omega}{2}-\\frac{\\omega^2}{12},\\qquad \\text{den}=1-\\frac{i\\omega}{2}-\\frac{\\omega^2}{12}=\\overline{\\text{num}},\n$$\nso\n$$\n|R(i\\omega)|=\\left|\\frac{\\text{num}}{\\text{den}}\\right|=1.\n$$\nThis unit-modulus property on the imaginary axis reflects the symplectic and energy-preserving character on oscillatory problems.\n\n$3.$ Asymptotics as $|z|\\to\\infty$: The highest-degree terms in numerator and denominator coincide, each being $\\frac{z^2}{12}$, so\n$$\n\\lim_{|z|\\to\\infty}R(z)=1.\n$$\nTherefore, the method is not $L$-stable (which would require $\\lim_{z\\to -\\infty}R(z)=0$ along the negative real axis).\n\n$4.$ $A$-stability: A method is $A$-stable if $|R(z)|\\leq 1$ for all $z$ with $\\text{Re}(z)\\leq 0$. Gauss–Legendre collocation RK methods are known to be $A$-stable, and for this $2$-stage case, the poles lie strictly in the right half-plane and $|R(i\\omega)|=1$ on the boundary $\\text{Re}(z)=0$, indicating no growth across the imaginary axis. Together with algebraic stability of Gauss methods, this implies $A$-stability. However, as noted above, the method is not $L$-stable.\n\nFinally, we may confirm consistency with $\\exp(z)$ through series expansion. Using the geometric series for the denominator,\n$$\n\\frac{1}{1-\\frac{z}{2}+\\frac{z^2}{12}}=1+\\frac{z}{2}+\\frac{z^2}{6}+\\frac{z^3}{24}+\\frac{z^4}{120}+O(z^5),\n$$\nand multiplying by the numerator $1+\\frac{z}{2}+\\frac{z^2}{12}$ yields\n$$\nR(z)=1+z+\\frac{z^2}{2}+\\frac{z^3}{6}+\\frac{z^4}{24}+O(z^5),\n$$\nmatching the Taylor expansion of $\\exp(z)$ through terms of order $z^4$, consistent with the order-$4$ accuracy of the method on the test equation.\n\nThus, the closed-form amplification factor is\n$$\nR(z)=\\frac{1+\\frac{z}{2}+\\frac{z^2}{12}}{1-\\frac{z}{2}+\\frac{z^2}{12}}.\n$$",
            "answer": "$$\\boxed{\\frac{1+\\frac{z}{2}+\\frac{z^{2}}{12}}{1-\\frac{z}{2}+\\frac{z^{2}}{12}}}$$"
        },
        {
            "introduction": "Many multiscale models in chemistry and biology describe quantities, like species concentrations, that must physically remain non-negative. Standard numerical integrators do not automatically respect this constraint, risking unphysical results and simulation failure. This practice challenges you to address this by designing a positivity-preserving scheme, where you will derive the maximum allowable time step for the explicit forward Euler method to ensure that all concentrations remain non-negative, a foundational technique in building robust solvers for reaction systems. ",
            "id": "3824513",
            "problem": "Consider a closed reaction network with $d$ chemical species whose nonnegative concentrations are collected in the vector $x(t) \\in \\mathbb{R}_{\\ge 0}^{d}$. The dynamics are modeled by an autonomous system of ordinary differential equations (ODEs): $$\\frac{d x}{d t} = S \\, r(x),$$ where $S \\in \\mathbb{R}^{d \\times L}$ is the stoichiometric matrix and $r(x) \\in \\mathbb{R}_{\\ge 0}^{L}$ is the vector of reaction rates. Assume mass-action kinetics with nonnegative rate constants, so each component $r_{\\ell}(x)$ is a smooth nonnegative function on the nonnegative orthant. For each species index $i \\in \\{1,\\dots,d\\}$, decompose the right-hand side into production and destruction contributions by defining $$p_{i}(x) := \\sum_{\\ell \\, : \\, S_{i \\ell}  0} S_{i \\ell} \\, r_{\\ell}(x), \\qquad d_{i}(x) := \\sum_{\\ell \\, : \\, S_{i \\ell}  0} \\left(-S_{i \\ell}\\right) \\, r_{\\ell}(x),$$ so that $$\\frac{d x_{i}}{d t} = p_{i}(x) - d_{i}(x), \\qquad p_{i}(x) \\ge 0, \\quad d_{i}(x) \\ge 0 \\quad \\text{for all } x \\in \\mathbb{R}_{\\ge 0}^{d}.$$ Assume a fixed time $t^{n}$ with state $x^{n} := x(t^{n}) \\in \\mathbb{R}_{\\ge 0}^{d}$ satisfying $x^{n}_{i} \\ge 0$ for all $i$, and that whenever $x^{n}_{i}  0$, there exists at least one reaction $\\ell$ with $S_{i \\ell}  0$ and $r_{\\ell}(x^{n})  0$ (this ensures $d_{i}(x^{n})  0$ on the active set of species).\n\nDesign an explicit one-step time integrator that preserves nonnegativity of the concentrations over a single time step, starting from $x^{n}$, by using the forward Euler building block and an adaptive step-size selection. Derive, from first principles and using only the definitions above, a closed-form expression for the largest admissible step size $\\Delta t_{\\max}(x^{n})$ such that the forward Euler update $$x^{n+1} = x^{n} + \\Delta t \\, \\left( S \\, r(x^{n}) \\right) = x^{n} + \\Delta t \\, \\left( p(x^{n}) - d(x^{n}) \\right)$$ remains componentwise nonnegative for all species. Your final expression must depend only on $S$ and $r(x^{n})$ and must be valid under the stated assumptions. Express your final answer as a single analytic expression for $\\Delta t_{\\max}(x^{n})$. No rounding is required. If any angles appear, express them in radians. If any physical units appear, express the final answer without units.",
            "solution": "The objective is to find the largest time step $\\Delta t_{\\max}(x^{n}) > 0$ for which the forward Euler update preserves the nonnegativity of all species concentrations. The state at time $t^n$ is given by the vector $x^n \\in \\mathbb{R}_{\\ge 0}^d$, meaning $x_i^n \\ge 0$ for all species $i \\in \\{1, \\dots, d\\}$.\n\nThe forward Euler method updates the state from $x^n$ to $x^{n+1}$ over a time step $\\Delta t$ according to the rule:\n$$x^{n+1} = x^n + \\Delta t \\, S \\, r(x^n)$$\nWriting this equation for each component $i$ gives:\n$$x_i^{n+1} = x_i^n + \\Delta t \\, (S \\, r(x^n))_i$$\nThe problem provides a decomposition of the right-hand side, $f_i(x) := (S \\, r(x))_i$, into production and destruction terms, $p_i(x)$ and $d_i(x)$, such that $f_i(x) = p_i(x) - d_i(x)$. Using this, the update for component $i$ is:\n$$x_i^{n+1} = x_i^n + \\Delta t (p_i(x^n) - d_i(x^n))$$\nThe condition for preserving nonnegativity is that $x_i^{n+1} \\ge 0$ for all $i = 1, \\dots, d$. This yields a system of $d$ inequalities:\n$$x_i^n + \\Delta t (p_i(x^n) - d_i(x^n)) \\ge 0 \\qquad \\text{for } i = 1, \\dots, d$$\nTo find the constraint on $\\Delta t$, we must analyze each inequality.\n\nWe consider two cases for each species $i$:\n\nCase 1: The net rate of change for species $i$ is nonnegative, i.e., $p_i(x^n) - d_i(x^n) \\ge 0$.\nIn this case, since $x_i^n \\ge 0$ and $\\Delta t > 0$, the term $\\Delta t (p_i(x^n) - d_i(x^n))$ is nonnegative. The sum $x_i^n + \\Delta t (p_i(x^n) - d_i(x^n))$ is therefore guaranteed to be nonnegative. Thus, for any species whose concentration is not decreasing, the nonnegativity condition is satisfied for any $\\Delta t \\ge 0$. Such a species imposes no upper bound on the time step.\n\nCase 2: The net rate of change for species $i$ is negative, i.e., $p_i(x^n) - d_i(x^n)  0$.\nIn this case, the concentration of species $i$ is decreasing. The inequality can be rearranged to isolate $\\Delta t$:\n$$\\Delta t (p_i(x^n) - d_i(x^n)) \\ge -x_i^n$$\nSince the term $p_i(x^n) - d_i(x^n)$ is negative, dividing by it reverses the inequality sign:\n$$\\Delta t \\le \\frac{-x_i^n}{p_i(x^n) - d_i(x^n)}$$\nThis can be written more intuitively using the destruction and production terms:\n$$\\Delta t \\le \\frac{x_i^n}{d_i(x^n) - p_i(x^n)}$$\nThis inequality provides an upper bound on $\\Delta t$ to prevent the concentration of species $i$ from becoming negative.\n\nLet's analyze the denominator. The condition for this case is $p_i(x^n) - d_i(x^n)  0$, which is equivalent to $d_i(x^n) > p_i(x^n)$. Since $p_i(x^n) \\ge 0$ by definition, this implies $d_i(x^n) > 0$. Therefore, the denominator $d_i(x^n) - p_i(x^n)$ is strictly positive, and the bound on $\\Delta t$ is well-defined.\n\nThe problem specified mass-action kinetics. This implies that if a species concentration $x_i^n = 0$, then any reaction rate $r_\\ell(x^n)$ for which species $i$ is a reactant must be zero. Consequently, the destruction term $d_i(x^n)$ must be zero. In this situation, the net rate is $p_i(x^n) - d_i(x^n) = p_i(x^n) \\ge 0$. This means that a species with zero concentration cannot have a negative net rate, so it falls under Case 1. Therefore, Case 2 only applies to species $i$ for which $x_i^n > 0$. This ensures that the numerator $x_i^n$ is strictly positive for all constraints derived in Case 2.\n\nThe global time step $\\Delta t$ must satisfy the nonnegativity constraint for all species simultaneously. It must therefore be smaller than or equal to all the individual upper bounds derived. The largest admissible step size, $\\Delta t_{\\max}(x^n)$, is the minimum of all these bounds.\nThe species in Case 1 contribute an effective upper bound of $\\infty$, which does not constrain the minimum. Thus, we only need to consider the minimum over the set of species that are being consumed (Case 2).\n\nLet the set of indices for species with decreasing concentration be $I = \\{i \\in \\{1,\\dots,d\\} \\mid p_i(x^n) - d_i(x^n)  0\\}$. The maximum admissible step size is:\n$$\\Delta t_{\\max}(x^n) = \\min_{i \\in I} \\left\\{ \\frac{x_i^n}{d_i(x^n) - p_i(x^n)} \\right\\}$$\nIf the set $I$ is empty (i.e., no species concentration is decreasing), the minimum over an empty set is defined as $+\\infty$, meaning any step size is permissible.\n\nTo express this in the requested terms of $S$ and $r(x^n)$, we use the relation $(S r(x^n))_i = p_i(x^n) - d_i(x^n)$. The condition for being in set $I$ is $(S r(x^n))_i  0$. The denominator is $d_i(x^n) - p_i(x^n) = -(p_i(x^n) - d_i(x^n)) = -(S r(x^n))_i$.\nSubstituting these into the expression for $\\Delta t_{\\max}(x^n)$ gives the final form:\n$$\\Delta t_{\\max}(x^n) = \\min_{i \\,:\\, (S r(x^n))_i  0} \\left\\{ \\frac{x_i^n}{-(S r(x^n))_i} \\right\\}$$\nThis expression is a function of the current state $x^n$ and the reaction data encapsulated in $S$ and the rate function $r$. It provides the largest step size for which a forward Euler step is guaranteed to not produce negative concentrations.",
            "answer": "$$\\boxed{\\min_{i \\in \\{1, \\dots, d\\} \\,:\\, (S r(x^n))_i  0} \\left\\{ \\frac{-x_i^n}{(S r(x^n))_i} \\right\\}}$$"
        },
        {
            "introduction": "Implicit methods are the workhorses for stiff ODEs, but their power comes at the cost of solving a nonlinear algebraic system at each time step. The efficiency of the Newton-Raphson solver used for this task hinges on how one handles the Jacobian matrix, presenting a classic trade-off between the high cost of frequent updates and the poor convergence from using a stale approximation. This hands-on coding exercise directs you to implement and compare Jacobian reuse strategies for a Diagonally Implicit Runge-Kutta (DIRK) method, offering practical, quantitative insight into the balance between computational cost and convergence robustness in real-world solvers. ",
            "id": "3824480",
            "problem": "Consider a system of ordinary differential equations defined by the vector function $f:\\mathbb{R}^2\\to\\mathbb{R}^2$ with the state $u(t)=[x(t),y(t)]^\\top$ satisfying the initial value problem $u'(t)=f(u(t))$, $u(0)=u_0$, where $f(u)$ is chosen to exhibit multiscale stiffness to reflect the context of multiscale modeling and analysis. The fundamental base comprises the definition of ordinary differential equations and the method-of-lines time integration framework that approximates $u(t)$ at discrete times via consistent implicit time-stepping and nonlinear solves derived from fixed-point conditions. You are to implement a Diagonally Implicit Runge-Kutta (DIRK) single-diagonal scheme (also known as Single Diagonal Implicit Runge-Kutta (SDIRK)) with two stages and stiff accuracy, and compare two strategies for reusing the Jacobian across time steps. The objective is to quantify the trade-off between Jacobian factorization cost and convergence robustness.\n\nLet the model be the Van der Pol oscillator in a stiff regime, defined by\n$$\nf\\left(\\begin{bmatrix}x\\\\y\\end{bmatrix}\\right)=\n\\begin{bmatrix}\ny \\\\\n\\mu\\,(1-x^2)\\,y - x\n\\end{bmatrix},\n$$\nwith stiffness parameter $\\mu0$. The Jacobian $J(u)=\\frac{\\partial f}{\\partial u}(u)$ is\n$$\nJ(x,y)=\n\\begin{bmatrix}\n0  1 \\\\\n-2\\mu x y - 1  \\mu(1-x^2)\n\\end{bmatrix}.\n$$\n\nUse the two-stage $2$nd-order stiffly accurate SDIRK scheme with diagonal coefficient $\\gamma=1-\\frac{1}{\\sqrt{2}}$. Its Butcher coefficients are\n$$\nA=\n\\begin{bmatrix}\n\\gamma  0\\\\\n1-\\gamma  \\gamma\n\\end{bmatrix},\\quad\nb=\\begin{bmatrix}1-\\gamma\\\\ \\gamma\\end{bmatrix},\\quad\nc=\\begin{bmatrix}\\gamma\\\\ 1\\end{bmatrix},\n$$\nwhere $\\gamma=1-\\frac{1}{\\sqrt{2}}$.\n\nGiven a current time step from $t_n$ to $t_{n+1}=t_n+h$ with current solution $u_n$, the DIRK stage equations are defined by the fixed-point conditions for stage values $Y_i$:\n$$\nG_i(Y_i)=Y_i - u_n - h\\sum_{j=1}^{i} a_{ij} f(Y_j)=0,\n$$\nfor $i\\in\\{1,2\\}$, with $a_{ij}$ the entries of $A$. Solve each stage equation using the Newton-Raphson method (NR), which linearizes $G_i$ about an iterate $Y_i^{(k)}$ and solves\n$$\n\\left[I - h\\,a_{ii}\\,J\\left(Y_i^{(k)}\\right)\\right]\\,\\Delta_i^{(k)} = -G_i\\left(Y_i^{(k)}\\right),\n$$\nthen updates $Y_i^{(k+1)}=Y_i^{(k)}+\\Delta_i^{(k)}$. Use $Y_1^{(0)}=u_n$ and $Y_2^{(0)}=u_n + h\\,(1-\\gamma)\\,f(Y_1)$ as initial guesses. The step update is $u_{n+1}=Y_2$ due to stiff accuracy.\n\nImplement and compare the following two Jacobian reuse strategies across steps:\n\n- Static-Reuse Across Steps: Compute and factorize the linear system matrix $M=I - h\\,\\gamma\\,J(u_0)$ once at the beginning of the integration, with $u_0$ the initial condition. Reuse this single factorization for all Newton solves in all stages and all time steps without refreshing, even though $J(u)$ changes over time. Count each factorization of $M$ as cost $1$. Count each triangular solve using the factorization as cost $1$ solve. This strategy models extreme reuse to minimize factorization cost at the potential expense of Newton convergence robustness.\n\n- Dynamic-Recompute Per Newton Iteration: At each Newton iteration for each stage in each time step, evaluate the Jacobian $J(Y_i^{(k)})$ at the current iterate, form $M=I - h\\,\\gamma\\,J(Y_i^{(k)})$, factorize it, and solve for $\\Delta_i^{(k)}$. Count each factorization and each solve as described. This strategy models aggressive updating to maximize convergence robustness at the expense of higher factorization cost.\n\nConvergence robustness is quantified by the number of steps for which at least one stage fails to reach the Newton-Raphson tolerance within the maximum allowed Newton iterations. For each time step and stage, declare convergence if $\\|G_i(Y_i^{(k)})\\|_2 \\le \\text{tol}$ within at most $\\text{max\\_it}$ iterations; otherwise declare failure for that step. Record the Euclidean norm $\\|G_2(Y_2^{(\\text{final})})\\|_2$ at the end of the last time step as a representative final residual.\n\nYou must implement a complete program that:\n- Integrates the system using both strategies over a fixed time grid with constant step size $h$.\n- Tracks and reports, for each strategy, the total number of Jacobian factorizations, the total number of linear solves, the total number of failed steps, and the final residual norm at the end of the last step.\n- Uses the specified SDIRK method, Newton-Raphson method, and cost accounting precisely as defined above.\n\nUse the following test suite of parameter sets, each given as $(\\mu,h,T,\\text{tol},\\text{max\\_it})$, with initial condition $u_0=[x(0),y(0)]^\\top=[2,0]^\\top$:\n- Test $1$: $(\\mu=10,\\;h=0.02,\\;T=2.0,\\;\\text{tol}=10^{-10},\\;\\text{max\\_it}=20)$\n- Test $2$: $(\\mu=1000,\\;h=0.001,\\;T=0.1,\\;\\text{tol}=10^{-9},\\;\\text{max\\_it}=20)$\n- Test $3$: $(\\mu=1000,\\;h=0.01,\\;T=0.1,\\;\\text{tol}=10^{-9},\\;\\text{max\\_it}=25)$\n- Test $4$: $(\\mu=1,\\;h=0.1,\\;T=2.0,\\;\\text{tol}=10^{-10},\\;\\text{max\\_it}=20)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes one sublist with eight entries in the following order:\n[`fact_static`, `solves_static`, `fails_static`, `final_res_static`, `fact_dynamic`, `solves_dynamic`, `fails_dynamic`, `final_res_dynamic`].\nAll entries must be of type integer or floating-point number. For instance, the output format should be like $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$ as a single line.",
            "solution": "The problem is valid as it presents a well-posed, scientifically grounded, and fully specified task in the field of numerical analysis for stiff ordinary differential equations, which is a core component of multiscale modeling. All necessary data, equations, and evaluation criteria are provided without ambiguity or contradiction. We proceed with the solution.\n\nThe objective is to implement a specific two-stage, second-order stiffly accurate Single Diagonal Implicit Runge-Kutta (SDIRK) method to solve the stiff Van der Pol oscillator and to compare the performance of two distinct strategies for handling the Jacobian matrix within the Newton-Raphson solver for the stage equations.\n\nThe system of ordinary differential equations (ODEs) is given by $u'(t) = f(u(t))$, with the state vector $u(t) = [x(t), y(t)]^\\top$. For the Van der Pol oscillator, the function $f: \\mathbb{R}^2 \\to \\mathbb{R}^2$ is defined as:\n$$\nf\\left(\\begin{bmatrix}x\\\\y\\end{bmatrix}\\right)=\n\\begin{bmatrix}\ny \\\\\n\\mu\\,(1-x^2)\\,y - x\n\\end{bmatrix}\n$$\nwhere $\\mu  0$ is a parameter controlling the stiffness of the system. The initial condition is specified as $u_0 = u(0) = [2, 0]^\\top$. The Jacobian of the right-hand side function, $J(u) = \\frac{\\partial f}{\\partial u}(u)$, is:\n$$\nJ(x,y)=\n\\begin{bmatrix}\n0  1 \\\\\n-2\\mu x y - 1  \\mu(1-x^2)\n\\end{bmatrix}\n$$\n\nThe numerical integration from time $t_n$ to $t_{n+1} = t_n + h$ is performed using a two-stage SDIRK method defined by the Butcher coefficients:\n$$\nA=\n\\begin{bmatrix}\n\\gamma  0\\\\\n1-\\gamma  \\gamma\n\\end{bmatrix},\\quad\nb=\\begin{bmatrix}1-\\gamma\\\\ \\gamma\\end{bmatrix},\\quad\nc=\\begin{bmatrix}\\gamma\\\\ 1\\end{bmatrix}\n$$\nwith the diagonal element $\\gamma = 1 - \\frac{1}{\\sqrt{2}}$. The stage values, $Y_1$ and $Y_2$, are the solutions to the following implicit equations:\n$$\nG_1(Y_1) = Y_1 - u_n - h \\gamma f(Y_1) = 0 \\\\\nG_2(Y_2) = Y_2 - u_n - h(1-\\gamma)f(Y_1) - h \\gamma f(Y_2) = 0\n$$\nwhere $u_n$ is the solution at time $t_n$, and $Y_1$ in the second equation is the converged solution from the first stage. This method is stiffly accurate because $c_2 = 1$ and the coefficients of the second row of $A$ match the coefficients of $b$. This property allows for a simple and stable update: $u_{n+1} = Y_2$.\n\nEach stage equation is a nonlinear system of algebraic equations solved using the Newton-Raphson method. For a general stage equation of the form $Y - C - h \\gamma f(Y) = 0$, where $C$ represents all explicit terms, we define the function to be zeroed as $F(Y) = Y - C - h \\gamma f(Y)$. The Newton-Raphson iteration proceeds by linearizing $F(Y)$ at the current iterate $Y^{(k)}$:\n$$\nF(Y^{(k)}) + F'(Y^{(k)})(Y^{(k+1)} - Y^{(k)}) \\approx 0\n$$\nThe Jacobian of $F$ is $F'(Y) = I - h \\gamma J(Y)$, where $I$ is the $2 \\times 2$ identity matrix. Denoting the update as $\\Delta^{(k)} = Y^{(k+1)} - Y^{(k)}$, we solve the following linear system for $\\Delta^{(k)}$ at each iteration:\n$$\n\\left[I - h \\gamma J\\left(Y^{(k)}\\right)\\right]\\,\\Delta^{(k)} = -\\left( Y^{(k)} - C - h \\gamma f(Y^{(k)}) \\right)\n$$\nThe next iterate is then $Y^{(k+1)} = Y^{(k)} + \\Delta^{(k)}$. The initial guesses for the Newton solvers are $Y_1^{(0)} = u_n$ for the first stage and $Y_2^{(0)} = u_n + h(1-\\gamma)f(Y_1)$ for the second stage. Iterations continue until the Euclidean norm of the residual, $\\|F(Y^{(k)})\\|_2$, is less than or equal to a specified tolerance `tol`, or a maximum number of iterations `max_it` is reached.\n\nThe core of the problem is the comparison of two Jacobian handling strategies within this framework:\n\n1.  **Static-Reuse Across Steps**: The Jacobian matrix is evaluated only once at the initial condition $u_0$. The linear system matrix $M_{static} = I - h \\gamma J(u_0)$ is formed and LU-factorized at the beginning of the integration. This single factorization is then reused for all subsequent Newton-Raphson solves in all stages and for all time steps. According to the problem's cost model, this strategy incurs a cost of $1$ factorization for the entire simulation and $1$ solve (a forward/backward substitution using the LU factors) per Newton iteration. This approach minimizes computational cost associated with factorizations but may suffer from poor convergence rates or failure if the true Jacobian $J(u(t))$ deviates significantly from $J(u_0)$.\n\n2.  **Dynamic-Recompute Per Newton Iteration**: The Jacobian $J(Y_i^{(k)})$ is re-evaluated at every Newton-Raphson iteration $k$ for each stage $i$. The matrix $M^{(k)} = I - h \\gamma J(Y_i^{(k)})$ is formed, LU-factorized, and used to solve for the update $\\Delta^{(k)}$. This incurs a cost of $1$ factorization and $1$ solve for every single Newton iteration. This strategy is computationally expensive but is expected to be much more robust, offering faster Newton convergence (typically quadratic) because it uses the most current linearization of the nonlinear system.\n\nThe algorithm is implemented to step through time from $t=0$ to $T$ with a constant step size $h$. For each strategy, we track four metrics: total Jacobian factorizations, total linear solves, total number of time steps in which at least one stage failed to converge, and the final residual norm $\\|G_2(Y_2^{(\\text{final})})\\|_2$ from the last time step. The provided Python code in the final answer executes this comparison for the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef f_vdp(u, mu):\n    \"\"\"Computes the right-hand side of the Van der Pol oscillator ODE.\"\"\"\n    x, y = u\n    return np.array([y, mu * (1.0 - x**2) * y - x])\n\ndef J_vdp(u, mu):\n    \"\"\"Computes the Jacobian of the Van der Pol oscillator ODE.\"\"\"\n    x, y = u\n    return np.array([[0.0, 1.0], [-2.0 * mu * x * y - 1.0, mu * (1.0 - x**2)]])\n\ndef integrate(strategy, u0, mu, h, T, tol, max_it):\n    \"\"\"\n    Integrates the Van der Pol ODE using a 2-stage SDIRK method with a specified\n    Jacobian handling strategy.\n\n    Args:\n        strategy (str): 'static' or 'dynamic'.\n        u0 (np.ndarray): Initial condition [x0, y0].\n        mu (float): Stiffness parameter.\n        h (float): Time step size.\n        T (float): Final time.\n        tol (float): Newton-Raphson convergence tolerance.\n        max_it (int): Maximum Newton iterations per stage.\n\n    Returns:\n        tuple: (total_factorizations, total_solves, total_failed_steps, final_residual_norm)\n    \"\"\"\n    gamma = 1.0 - 1.0 / np.sqrt(2.0)\n    a11 = gamma\n    a21 = 1.0 - gamma\n    a22 = gamma\n\n    u = u0.copy()\n    num_steps = int(round(T / h))\n    I = np.identity(2)\n\n    total_factorizations = 0\n    total_solves = 0\n    total_failed_steps = 0\n    final_residual_norm = 0.0\n    \n    lu, piv = None, None\n    if strategy == 'static':\n        J_u0 = J_vdp(u0, mu)\n        M_static = I - h * a11 * J_u0\n        try:\n            lu, piv = linalg.lu_factor(M_static)\n            total_factorizations += 1\n        except linalg.LinAlgError:\n            # If initial matrix is singular, the static method cannot proceed.\n            return (1, 0, num_steps, np.inf)\n\n    for n in range(num_steps):\n        u_n = u.copy()\n        \n        # --- Stage 1 ---\n        Y1 = u_n.copy()  # Initial guess: Y_1^(0) = u_n\n        s1_converged = False\n        G1 = np.array([np.inf, np.inf])\n        \n        for _ in range(max_it):\n            G1 = Y1 - u_n - h * a11 * f_vdp(Y1, mu)\n            if linalg.norm(G1) = tol:\n                s1_converged = True\n                break\n            \n            try:\n                if strategy == 'static':\n                    delta = linalg.lu_solve((lu, piv), -G1)\n                    total_solves += 1\n                else:  # dynamic\n                    J_Y1 = J_vdp(Y1, mu)\n                    M = I - h * a11 * J_Y1\n                    lu_k, piv_k = linalg.lu_factor(M)\n                    total_factorizations += 1\n                    delta = linalg.lu_solve((lu_k, piv_k), -G1)\n                    total_solves += 1\n                Y1 += delta\n            except linalg.LinAlgError:\n                break  # Newton solver failure, stage fails\n\n        # --- Stage 2 ---\n        u_n_stage2_const = u_n + h * a21 * f_vdp(Y1, mu)\n        Y2 = u_n_stage2_const.copy()  # Initial guess: Y_2^(0)\n        s2_converged = False\n        G2 = np.array([np.inf, np.inf])\n\n        for _ in range(max_it):\n            G2 = Y2 - u_n_stage2_const - h * a22 * f_vdp(Y2, mu)\n            if linalg.norm(G2) = tol:\n                s2_converged = True\n                break\n            \n            try:\n                if strategy == 'static':\n                    delta = linalg.lu_solve((lu, piv), -G2)\n                    total_solves += 1\n                else:  # dynamic\n                    J_Y2 = J_vdp(Y2, mu)\n                    M = I - h * a22 * J_Y2\n                    lu_k, piv_k = linalg.lu_factor(M)\n                    total_factorizations += 1\n                    delta = linalg.lu_solve((lu_k, piv_k), -G2)\n                    total_solves += 1\n                Y2 += delta\n            except linalg.LinAlgError:\n                break # Newton solver failure, stage fails\n\n        if not s1_converged or not s2_converged:\n            total_failed_steps += 1\n        \n        u = Y2\n        \n        if n == num_steps - 1:\n            final_residual_norm = linalg.norm(G2)\n\n    return total_factorizations, total_solves, total_failed_steps, final_residual_norm\n\ndef solve():\n    # Test cases: (mu, h, T, tol, max_it)\n    test_cases = [\n        (10, 0.02, 2.0, 1e-10, 20),\n        (1000, 0.001, 0.1, 1e-9, 20),\n        (1000, 0.01, 0.1, 1e-9, 25),\n        (1, 0.1, 2.0, 1e-10, 20),\n    ]\n\n    results = []\n    for params in test_cases:\n        mu, h, T, tol, max_it = params\n        u0 = np.array([2.0, 0.0])\n\n        fact_s, solves_s, fails_s, res_s = integrate('static', u0, mu, h, T, tol, max_it)\n        fact_d, solves_d, fails_d, res_d = integrate('dynamic', u0, mu, h, T, tol, max_it)\n\n        results.append([fact_s, solves_s, fails_s, res_s, fact_d, solves_d, fails_d, res_d])\n\n    # Final print statement must be a single line with the specified format.\n    print(f\"[[{results[0][0]}, {results[0][1]}, {results[0][2]}, {results[0][3]}, {results[0][4]}, {results[0][5]}, {results[0][6]}, {results[0][7]}], [{results[1][0]}, {results[1][1]}, {results[1][2]}, {results[1][3]}, {results[1][4]}, {results[1][5]}, {results[1][6]}, {results[1][7]}], [{results[2][0]}, {results[2][1]}, {results[2][2]}, {results[2][3]}, {results[2][4]}, {results[2][5]}, {results[2][6]}, {results[2][7]}], [{results[3][0]}, {results[3][1]}, {results[3][2]}, {results[3][3]}, {results[3][4]}, {results[3][5]}, {results[3][6]}, {results[3][7]}]]\")\n\nsolve()\n```"
        }
    ]
}