{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of any multigrid solver is fundamentally dependent on its smoother. A good smoother does not need to eliminate all error, but it must effectively damp high-frequency error components, leaving behind smooth error that can be accurately represented and solved on a coarser grid. This practice will guide you through a Local Fourier Analysis (LFA), a powerful theoretical tool, to analyze the weighted Jacobi smoother. By deriving its smoothing factor, you will determine the optimal relaxation parameter that maximizes its high-frequency damping rate, a key skill in multigrid design .",
            "id": "3783291",
            "problem": "Consider the one-dimensional Poisson equation discretized on a periodic uniform grid of spacing $h$ with $N$ points, where $N$ is large. The discrete operator is the standard second-order finite-difference Laplacian, which acts on a grid function $\\{u_{j}\\}$ as $A u_{j} = \\frac{1}{h^{2}}(-u_{j-1} + 2 u_{j} - u_{j+1})$. In a parallel two-grid multigrid method designed for domain decomposition, a pointwise weighted Jacobi smoother is employed with relaxation parameter $\\omega \\in (0,1)$, operating on each subdomain independently (this locality enables efficient parallelization). Let the error-propagation matrix of the weighted Jacobi iteration be $E = I - \\omega D^{-1} A$, where $D$ is the diagonal of $A$.\n\nUsing Local Fourier Analysis (LFA), in which Fourier modes $e^{\\mathrm{i} \\theta j}$ diagonalize constant-coefficient circulant operators on periodic grids, derive the Fourier symbol $\\mu(\\theta)$ of the iteration matrix $E$ from first principles and then compute the smoothing factor $\\rho(\\omega)$ defined as the maximum of $|\\mu(\\theta)|$ over the high-frequency set relative to a $2h$-coarse grid, taken to be $\\theta \\in [\\pi/2,\\pi]$. Finally, determine the value of $\\omega \\in (0,1)$ that minimizes $\\rho(\\omega)$ and report the minimized smoothing factor as a single real number. Do not round the final answer.",
            "solution": "The user has provided a valid problem statement from the field of numerical analysis, specifically concerning the analysis of multigrid methods. The problem is scientifically grounded, well-posed, and objective. We shall proceed with a formal derivation.\n\nThe problem asks for the analysis of a weighted Jacobi smoother for the one-dimensional discrete Poisson equation on a periodic grid. The core of the analysis will be performed using Local Fourier Analysis (LFA).\n\nFirst, we identify the components of the error-propagation matrix for the weighted Jacobi iteration, $E = I - \\omega D^{-1} A$.\nThe discrete operator is given by\n$$A u_{j} = \\frac{1}{h^{2}}(-u_{j-1} + 2 u_{j} - u_{j+1})$$\nThe matrix $A$ is circulant due to the periodic boundary conditions. The diagonal part of $A$, denoted by $D$, consists of the coefficients of $u_j$. From the expression, the diagonal entries of $A$ are all $2/h^2$. Thus, $D$ is a scalar multiple of the identity matrix, $D = \\frac{2}{h^2} I$.\nThe inverse of $D$ is $D^{-1} = \\frac{h^2}{2} I$.\n\nThe analysis proceeds by examining the action of the operators on the Fourier modes $\\phi_j(\\theta) = e^{\\mathrm{i} \\theta j}$, where $\\theta$ is the angular frequency. These modes are the eigenvectors of any linear, constant-coefficient operator on a periodic grid. The corresponding eigenvalue is called the Fourier symbol of the operator.\n\nWe first derive the symbol of the operator $A$, denoted $\\tilde{A}(\\theta)$.\n$$\n\\begin{aligned}\nA \\phi_j(\\theta) &= A e^{\\mathrm{i} \\theta j} \\\\\n&= \\frac{1}{h^{2}}(-e^{\\mathrm{i} \\theta (j-1)} + 2 e^{\\mathrm{i} \\theta j} - e^{\\mathrm{i} \\theta (j+1)}) \\\\\n&= \\frac{e^{\\mathrm{i} \\theta j}}{h^{2}}(-e^{-\\mathrm{i} \\theta} + 2 - e^{\\mathrm{i} \\theta}) \\\\\n&= \\frac{e^{\\mathrm{i} \\theta j}}{h^{2}}(2 - (e^{\\mathrm{i} \\theta} + e^{-\\mathrm{i} \\theta}))\n\\end{aligned}\n$$\nUsing Euler's formula, $\\cos(\\theta) = \\frac{e^{\\mathrm{i} \\theta} + e^{-\\mathrm{i} \\theta}}{2}$, we get:\n$$\nA \\phi_j(\\theta) = \\frac{e^{\\mathrm{i} \\theta j}}{h^{2}}(2 - 2\\cos(\\theta)) = \\frac{2}{h^2}(1 - \\cos(\\theta)) e^{\\mathrm{i} \\theta j}\n$$\nUsing the half-angle identity $1 - \\cos(\\theta) = 2\\sin^2(\\frac{\\theta}{2})$, the expression simplifies to:\n$$\nA \\phi_j(\\theta) = \\frac{4}{h^2}\\sin^2\\left(\\frac{\\theta}{2}\\right) e^{\\mathrm{i} \\theta j}\n$$\nThus, the Fourier symbol of $A$ is $\\tilde{A}(\\theta) = \\frac{4}{h^2}\\sin^2(\\frac{\\theta}{2})$.\n\nNext, we derive the symbol of the iteration matrix $E = I - \\omega D^{-1} A$, denoted $\\mu(\\theta)$. The symbol of the identity matrix $I$ is $1$. The symbol of $D^{-1}$ is the scalar $\\frac{h^2}{2}$.\n$$\n\\begin{aligned}\n\\mu(\\theta) &= \\tilde{I}(\\theta) - \\omega \\tilde{D}^{-1}(\\theta) \\tilde{A}(\\theta) \\\\\n&= 1 - \\omega \\left(\\frac{h^2}{2}\\right) \\left(\\frac{4}{h^2}\\sin^2\\left(\\frac{\\theta}{2}\\right)\\right) \\\\\n&= 1 - 2\\omega \\sin^2\\left(\\frac{\\theta}{2}\\right)\n\\end{aligned}\n$$\nThis is the amplification factor of the weighted Jacobi method for the mode with frequency $\\theta$.\n\nThe smoothing factor, $\\rho(\\omega)$, is defined as the maximum magnitude of the amplification factor over the set of high frequencies. For coarsening to a grid with spacing $2h$, the high-frequency set is $\\theta \\in [\\frac{\\pi}{2}, \\pi]$.\n$$\n\\rho(\\omega) = \\max_{\\theta \\in [\\pi/2, \\pi]} |\\mu(\\theta)| = \\max_{\\theta \\in [\\pi/2, \\pi]} \\left| 1 - 2\\omega \\sin^2\\left(\\frac{\\theta}{2}\\right) \\right|\n$$\nTo find this maximum, we analyze the range of the function $f(\\theta) = \\sin^2(\\frac{\\theta}{2})$ for $\\theta \\in [\\frac{\\pi}{2}, \\pi]$.\nThe argument $\\frac{\\theta}{2}$ is in the interval $[\\frac{\\pi}{4}, \\frac{\\pi}{2}]$. In this interval, $\\sin(x)$ is a monotonically increasing function.\nAt $\\theta = \\frac{\\pi}{2}$, we have $\\sin^2(\\frac{\\pi}{4}) = (\\frac{\\sqrt{2}}{2})^2 = \\frac{1}{2}$.\nAt $\\theta = \\pi$, we have $\\sin^2(\\frac{\\pi}{2}) = 1^2 = 1$.\nLet $x = \\sin^2(\\frac{\\theta}{2})$. The range of $x$ is $[\\frac{1}{2}, 1]$.\nThe expression for the smoothing factor becomes:\n$$ \\rho(\\omega) = \\max_{x \\in [1/2, 1]} |1 - 2\\omega x| $$\nThe function $g(x) = 1 - 2\\omega x$ is linear in $x$. The maximum of its absolute value on a closed interval must occur at one of the endpoints of the interval. So we evaluate $|g(x)|$ at $x = \\frac{1}{2}$ and $x=1$.\nAt $x = \\frac{1}{2}$: $|1 - 2\\omega (\\frac{1}{2})| = |1 - \\omega|$.\nAt $x = 1$: $|1 - 2\\omega (1)| = |1 - 2\\omega|$.\n\nSo, the smoothing factor is $\\rho(\\omega) = \\max(|1-\\omega|, |1-2\\omega|)$.\nGiven that the relaxation parameter $\\omega$ is in the interval $(0,1)$, the term $1-\\omega$ is always positive, so $|1-\\omega| = 1-\\omega$.\nThe expression simplifies to $\\rho(\\omega) = \\max(1-\\omega, |1-2\\omega|)$.\n\nThe final step is to find the value of $\\omega \\in (0,1)$ that minimizes $\\rho(\\omega)$. The minimum of the maximum of two continuous functions typically occurs where the two functions are equal. We set the arguments of the $\\max$ function to be equal:\n$$ 1 - \\omega = |1 - 2\\omega| $$\nWe consider two cases for the absolute value term:\nCase 1: $1 - 2\\omega \\geq 0$, which implies $\\omega \\leq \\frac{1}{2}$. The equation becomes $1 - \\omega = 1 - 2\\omega$, which gives $\\omega = 0$. This is not in the specified domain $\\omega \\in (0,1)$.\nCase 2: $1 - 2\\omega < 0$, which implies $\\omega > \\frac{1}{2}$. The equation becomes $1 - \\omega = -(1 - 2\\omega) = 2\\omega - 1$.\nSolving for $\\omega$:\n$$ 2 = 3\\omega \\implies \\omega = \\frac{2}{3} $$\nThis value $\\omega = \\frac{2}{3}$ is in the domain $(0,1)$ and satisfies the condition $\\omega > \\frac{1}{2}$, so it is the optimal relaxation parameter, $\\omega_{opt} = \\frac{2}{3}$.\n\nThe minimized smoothing factor is found by substituting $\\omega_{opt}$ into the expression for $\\rho(\\omega)$:\n$$ \\rho_{min} = \\rho\\left(\\frac{2}{3}\\right) = \\max\\left(1-\\frac{2}{3}, \\left|1-2\\left(\\frac{2}{3}\\right)\\right|\\right) $$\n$$ \\rho_{min} = \\max\\left(\\frac{1}{3}, \\left|1-\\frac{4}{3}\\right|\\right) = \\max\\left(\\frac{1}{3}, \\left|-\\frac{1}{3}\\right|\\right) = \\max\\left(\\frac{1}{3}, \\frac{1}{3}\\right) = \\frac{1}{3} $$\nThe minimal smoothing factor is $\\frac{1}{3}$.",
            "answer": "$$ \\boxed{\\frac{1}{3}} $$"
        },
        {
            "introduction": "While theoretical analysis provides invaluable insight, implementing a numerical method brings its principles to life. This practice bridges the gap between theory and computation by asking you to construct the full two-grid iteration operator from its constituent parts: the fine-grid operator, the smoother, and the inter-grid transfer operators. By numerically computing the spectral radius of this operator, you will obtain a concrete estimate of the method's convergence factor, allowing you to verify theoretical predictions and explore how performance changes with different parameters .",
            "id": "3783314",
            "problem": "Consider the one-dimensional Poisson boundary value problem $-u''(x)=f(x)$ on the interval $(0,1)$ with homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. Discretize the problem using the standard second-order centered finite difference method on a uniform grid with $n$ interior points, yielding the linear system $A u = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is the symmetric positive definite tridiagonal matrix arising from the discretization.\n\nYour task is to construct a Two-Grid (TG) multigrid error-propagation operator and estimate its convergence factor numerically, defined as the spectral radius of the TG error-propagation matrix. Use the following components:\n\n- Weighted Jacobi smoother for both pre-smoothing and post-smoothing with relaxation parameter $\\omega$, applied $\\nu_1$ times as pre-smoothing and $\\nu_2$ times as post-smoothing.\n- Linear interpolation for the prolongation (from coarse to fine grid).\n- Full-weighting restriction (from fine to coarse grid), defined consistently through the Galerkin coarse-grid operator $A_c = R A P$, where $P$ denotes the prolongation matrix and $R$ denotes the restriction matrix.\n\nStart from the following fundamental definitions and modeling assumptions:\n- The discrete operator $A$ for the one-dimensional Poisson problem with Dirichlet boundary conditions on a uniform grid of spacing $h = \\frac{1}{n+1}$ is the tridiagonal matrix with diagonal entries $\\frac{2}{h^2}$ and sub- and super-diagonal entries $-\\frac{1}{h^2}$.\n- The weighted Jacobi iteration for solving $A u = b$ updates $u^{(k+1)} = u^{(k)} + \\omega D^{-1}(b - A u^{(k)})$, where $D$ is the diagonal of $A$.\n- For the error $e^{(k)} = u - u^{(k)}$, the weighted Jacobi error-propagation matrix is $S = I - \\omega D^{-1} A$.\n- The prolongation matrix $P \\in \\mathbb{R}^{n \\times n_c}$ is given by linear interpolation, where $n_c = \\lfloor n/2 \\rfloor$ is the number of interior points on the coarse grid. Specifically, a coarse variable at index $j$ maps to fine grid indices $2j-1$, $2j$, and $2j+1$ with weights $\\frac{1}{2}$, $1$, and $\\frac{1}{2}$ respectively when those indices are in range.\n- The restriction matrix is the full-weighting operator $R = \\frac{1}{2} P^T$, ensuring symmetry of the Galerkin coarse-grid operator $A_c = R A P$.\n- The Two-Grid error-propagation operator for one pre-smoothing and one post-smoothing sweep is\n$$\nE_{\\mathrm{TG}} = S_{\\text{post}}^{\\nu_2} \\left(I - P A_c^{-1} R A\\right) S_{\\text{pre}}^{\\nu_1},\n$$\nwhere $S_{\\text{pre}} = S_{\\text{post}} = S$ is the weighted Jacobi error-propagation matrix. The Two-Grid convergence factor is the spectral radius $\\rho(E_{\\mathrm{TG}})$.\n\nImplement a program that, for given parameters $(n, \\omega, \\nu_1, \\nu_2)$, constructs $A$, $P$, $R$, $A_c$, and $E_{\\mathrm{TG}}$, and computes $\\rho(E_{\\mathrm{TG}})$ as the maximum magnitude of the eigenvalues of $E_{\\mathrm{TG}}$. The computation must be entirely numerical and self-contained, without requiring external input.\n\nUse the following test suite of parameter sets to assess different facets of the computation:\n- Case $1$: $n=31$, $\\omega=\\frac{2}{3}$, $\\nu_1=1$, $\\nu_2=1$ (baseline requested configuration).\n- Case $2$: $n=3$, $\\omega=\\frac{2}{3}$, $\\nu_1=1$, $\\nu_2=1$ (smallest nontrivial coarse grid).\n- Case $3$: $n=63$, $\\omega=\\frac{2}{3}$, $\\nu_1=1$, $\\nu_2=1$ (larger grid to assess asymptotic behavior).\n- Case $4$: $n=31$, $\\omega=\\frac{1}{2}$, $\\nu_1=1$, $\\nu_2=1$ (effect of a different relaxation parameter).\n- Case $5$: $n=31$, $\\omega=\\frac{2}{3}$, $\\nu_1=2$, $\\nu_2=2$ (effect of stronger smoothing).\n\nAll computations are dimensionless in this formulation; no physical units are required. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the cases above. Each result must be a floating-point number formatted with exactly six digits after the decimal point, for example, $[\\rho_1,\\rho_2,\\rho_3,\\rho_4,\\rho_5]$ where each $\\rho_i$ is the computed spectral radius corresponding to Case $i$.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of numerical analysis for partial differential equations, objective, and provides a complete and consistent set of definitions to construct a numerical solution. All components—the Poisson problem, finite difference discretization, weighted Jacobi smoother, and two-grid cycle components (prolongation, restriction, Galerkin operator)—are standard topics in the field of multigrid methods. The problem is well-posed, asking for the numerical computation of the spectral radius of a well-defined error-propagation matrix.\n\nThe solution proceeds by constructing the matrices required for the two-grid analysis for each given parameter set $(n, \\omega, \\nu_1, \\nu_2)$.\n\n**Step 1: Discretization and Fine-Grid Operator $A$**\nThe one-dimensional Poisson equation $-u''(x) = f(x)$ on $(0, 1)$ with $u(0)=u(1)=0$ is discretized on a uniform grid with $n$ interior points $x_i = i h$ for $i=1, \\dots, n$, where the grid spacing is $h = \\frac{1}{n+1}$. The second derivative $u''(x_i)$ is approximated using a second-order centered finite difference:\n$$\nu''(x_i) \\approx \\frac{u(x_{i-1}) - 2u(x_i) + u(x_{i+1})}{h^2}\n$$\nApplying this to the equation at each interior point $x_i$ and incorporating the boundary conditions $u(x_0)=u_0=0$ and $u(x_{n+1})=u_{n+1}=0$ leads to the linear system $A\\mathbf{u} = \\mathbf{b}$. The matrix $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite tridiagonal matrix defined by:\n$$\nA = \\frac{1}{h^2} \\begin{pmatrix}\n2 & -1 & & & \\\\\n-1 & 2 & -1 & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & -1 & 2 & -1 \\\\\n& & & -1 & 2\n\\end{pmatrix}\n$$\nThe diagonal entries are $A_{i,i} = \\frac{2}{h^2}$ and the sub- and super-diagonal entries are $A_{i,i\\pm1} = -\\frac{1}{h^2}$.\n\n**Step 2: Weighted Jacobi Smoother and Error Propagation Matrix $S$**\nThe weighted Jacobi method for solving $A\\mathbf{u}=\\mathbf{b}$ is an iterative scheme. The error $\\mathbf{e}^{(k)} = \\mathbf{u} - \\mathbf{u}^{(k)}$ propagates according to the matrix $S = I - \\omega D^{-1} A$, where $D$ is the diagonal of $A$, $I$ is the identity matrix, and $\\omega$ is the relaxation parameter. For our specific matrix $A$, the diagonal is constant: $D = \\frac{2}{h^2} I$. Therefore, $D^{-1} = \\frac{h^2}{2} I$, and the smoother matrix simplifies to:\n$$\nS = I - \\omega \\left(\\frac{h^2}{2} I\\right) A = I - \\frac{\\omega h^2}{2} A\n$$\n\n**Step 3: Intergrid Transfer Operators: Prolongation $P$ and Restriction $R$**\nGrid transfer operators map vectors between the fine grid (with $n$ points) and a coarse grid (with $n_c = \\lfloor n/2 \\rfloor$ points).\nThe prolongation (or interpolation) operator $P \\in \\mathbb{R}^{n \\times n_c}$ maps a coarse-grid vector to a fine-grid vector. The problem specifies a particular stencil for $P$. Using 0-based indexing for matrices, where fine indices are $i \\in \\{0, \\dots, n-1\\}$ and coarse indices are $j \\in \\{0, \\dots, n_c-1\\}$, the $j$-th column of $P$ (corresponding to the $j$-th coarse-grid basis vector) has non-zero entries at rows $2j$, $2j+1$, and $2j+2$, provided they are within the bounds of the fine grid. The weights are specified as $\\frac{1}{2}$, $1$, and $\\frac{1}{2}$ respectively.\n$$\n(P \\mathbf{v}_c)_i = \\begin{cases}\n\\frac{1}{2} v_{c,j} & \\text{if } i = 2j \\\\\nv_{c,j} & \\text{if } i = 2j+1 \\\\\n\\frac{1}{2} v_{c,j} & \\text{if } i = 2j+2 \\\\\n\\end{cases}\n$$\nsubject to indices being in range. This structure defines the matrix $P$.\nThe restriction operator $R \\in \\mathbb{R}^{n_c \\times n}$ maps a fine-grid vector to a coarse-grid vector. It is defined as the scaled transpose of the prolongation operator to ensure a symmetric Galerkin coarse-grid operator:\n$$\nR = \\frac{1}{2} P^T\n$$\n\n**Step 4: Coarse-Grid Operator $A_c$**\nThe coarse-grid operator $A_c$ should approximate the fine-grid operator $A$ on the coarse grid. The Galerkin approach defines $A_c$ via the intergrid transfer operators:\n$$\nA_c = R A P = \\left(\\frac{1}{2} P^T\\right) A P\n$$\nFor the given problem, if $A$ is symmetric positive definite, $A_c$ will also be symmetric positive definite and thus invertible. $A_c$ is an $n_c \\times n_c$ matrix.\n\n**Step 5: Two-Grid Error Propagation Operator $E_{\\mathrm{TG}}$**\nA two-grid V-cycle consists of pre-smoothing, coarse-grid correction, and post-smoothing. The error propagation for this full cycle is given by the operator:\n$$\nE_{\\mathrm{TG}} = S^{\\nu_2} \\left(I - P A_c^{-1} R A\\right) S^{\\nu_1}\n$$\nHere, $S^{\\nu_1}$ represents the effect of $\\nu_1$ pre-smoothing steps. The term in the parentheses represents the coarse-grid correction step, which involves restricting the residual, solving the error equation on the coarse grid ($A_c e_c = r_c$), and prolongating the correction back to the fine grid. $S^{\\nu_2}$ represents the effect of $\\nu_2$ post-smoothing steps.\n\n**Step 6: Convergence Factor Calculation**\nThe asymptotic convergence factor of the two-grid method is the spectral radius of the error propagation operator, $\\rho(E_{\\mathrm{TG}})$. The spectral radius is defined as the maximum of the absolute values of the eigenvalues of $E_{\\mathrm{TG}}$:\n$$\n\\rho(E_{\\mathrm{TG}}) = \\max \\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } E_{\\mathrm{TG}}\\}\n$$\nThis value is computed numerically for each set of parameters $(n, \\omega, \\nu_1, \\nu_2)$ by constructing the matrix $E_{\\mathrm{TG}}$ and then finding its eigenvalues.\n\nThe implementation will follow these steps precisely for each test case provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_spectral_radius(n, omega, nu1, nu2):\n    \"\"\"\n    Constructs the Two-Grid error-propagation operator and computes its spectral radius.\n\n    Args:\n        n (int): Number of interior grid points on the fine grid.\n        omega (float): Relaxation parameter for the weighted Jacobi smoother.\n        nu1 (int): Number of pre-smoothing steps.\n        nu2 (int): Number of post-smoothing steps.\n\n    Returns:\n        float: The spectral radius of the Two-Grid operator.\n    \"\"\"\n    # Step 1: Construct Fine-Grid Operator A\n    h = 1.0 / (n + 1)\n    diag_val = 2.0 / h**2\n    offdiag_val = -1.0 / h**2\n    \n    A = np.diag(np.full(n, diag_val))\n    A += np.diag(np.full(n - 1, offdiag_val), k=1)\n    A += np.diag(np.full(n - 1, offdiag_val), k=-1)\n\n    # Step 2: Construct Weighted Jacobi Smoother S\n    # D is diag(A), which is a constant diagonal matrix (diag_val * I).\n    # D_inv is (1/diag_val) * I = (h^2 / 2) * I.\n    # S = I - omega * D_inv @ A\n    S = np.identity(n) - omega * (h**2 / 2.0) * A\n\n    # Step 3: Construct Prolongation Operator P\n    n_c = n // 2\n    if n_c == 0:\n        # Trivial case: no coarse grid, so no correction. Factor is rho(S).\n        eigvals_S = np.linalg.eigvals(np.linalg.matrix_power(S, nu1 + nu2))\n        return np.max(np.abs(eigvals_S))\n        \n    P = np.zeros((n, n_c))\n    \n    # Using 0-based indexing for arrays. The problem statement implies a stencil\n    # centered at fine point 2j+1 for coarse point j.\n    # A coarse value at index j (0-based) contributes to fine grid\n    # values at indices 2j, 2j+1, 2j+2.\n    for j in range(n_c):\n        # Contribution to fine grid index 2j\n        if (2 * j) < n:\n            P[2 * j, j] = 0.5\n        # Contribution to fine grid index 2j+1\n        if (2 * j + 1) < n:\n            P[2 * j + 1, j] = 1.0\n        # Contribution to fine grid index 2j+2\n        if (2 * j + 2) < n:\n            P[2 * j + 2, j] = 0.5\n            \n    # Step 4: Construct Restriction Operator R\n    R = 0.5 * P.T\n    \n    # Step 5: Construct Coarse-Grid Operator Ac\n    A_c = R @ A @ P\n    \n    # Step 6: Construct Two-Grid Error Propagation Operator E_TG\n    A_c_inv = np.linalg.inv(A_c)\n    \n    # Coarse-grid correction operator\n    CGC = np.identity(n) - P @ A_c_inv @ R @ A\n    \n    # Smoothing operators\n    S_pre = np.linalg.matrix_power(S, nu1)\n    S_post = np.linalg.matrix_power(S, nu2)\n    \n    E_tg = S_post @ CGC @ S_pre\n    \n    # Step 7: Compute Spectral Radius\n    eigenvalues = np.linalg.eigvals(E_tg)\n    spectral_radius = np.max(np.abs(eigenvalues))\n    \n    return spectral_radius\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: (n=31, omega=2/3, nu1=1, nu2=1)\n        (31, 2.0/3.0, 1, 1),\n        # Case 2: (n=3, omega=2/3, nu1=1, nu2=1)\n        (3, 2.0/3.0, 1, 1),\n        # Case 3: (n=63, omega=2/3, nu1=1, nu2=1)\n        (63, 2.0/3.0, 1, 1),\n        # Case 4: (n=31, omega=1/2, nu1=1, nu2=1)\n        (31, 0.5, 1, 1),\n        # Case 5: (n=31, omega=2/3, nu1=2, nu2=2)\n        (31, 2.0/3.0, 2, 2)\n    ]\n\n    results = []\n    for params in test_cases:\n        n, omega, nu1, nu2 = params\n        rho = compute_spectral_radius(n, omega, nu1, nu2)\n        results.append(rho)\n\n    # Final print statement in the exact required format.\n    # Format each result to exactly six decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond multigrid, domain decomposition methods are a cornerstone of parallel scientific computing. A central challenge in designing these methods is managing the trade-offs that govern parallel performance. This exercise explores the classic trade-off in the Additive Schwarz method: increasing the overlap between subdomains improves the preconditioner's quality and reduces the number of iterations, but at the cost of increased communication and computation per iteration. Using a standard performance model, you will determine the optimal overlap size that minimizes the total time-to-solution, a crucial aspect of practical algorithm tuning .",
            "id": "3783303",
            "problem": "Consider the symmetric positive definite linear system arising from the standard five-point finite difference discretization of the Poisson equation $\\,-\\nabla^{2} u = f\\,$ with homogeneous Dirichlet boundary conditions on the unit square. The fine-grid spacing is $h$, and the domain is partitioned into an $8 \\times 8$ array of non-overlapping square subdomains, each of side length $H = 1/8$, so that $H/h = 64$. A two-level Additive Schwarz Domain Decomposition (ASDD) preconditioner is used within the Preconditioned Conjugate Gradient (PCG) method.\n\nYou may use the following widely accepted facts as the modeling base:\n\n- The condition number $\\kappa$ of the preconditioned operator for ASDD under standard assumptions on shape-regular partitions and coarse spaces is bounded in two space dimensions by\n$$\n\\kappa(\\delta) \\leq C_{\\mathrm{AS}} \\left( 1 + \\frac{H}{\\delta} \\right)^{2},\n$$\nwhere $\\delta$ is the physical overlap size and $C_{\\mathrm{AS}} > 0$ is a mesh-independent constant.\n- For the PCG method applied to a symmetric positive definite system, the number of iterations $n(\\delta)$ needed to reduce a fixed norm of the error by a prescribed factor scales as\n$$\nn(\\delta) \\propto \\sqrt{\\kappa(\\delta)}.\n$$\n\nAssume the following execution-time model per PCG iteration on a parallel system:\n\n- The per-iteration wall-clock time is the sum of a baseline cost and an overlap-dependent cost,\n$$\nT_{\\mathrm{iter}}(\\delta) = A + B \\,\\frac{\\delta}{h},\n$$\nwhere $A > 0$ captures latency and base computation, and $B > 0$ captures bandwidth and extra computation proportional to the number of fine-grid overlap layers. Define the dimensionless overlap as $x = \\delta/h$ (the number of fine-grid layers).\n\nThe total time-to-solution for a fixed tolerance target can be modeled (up to an overall positive multiplicative constant that does not affect the minimizer) by\n$$\nT_{\\mathrm{tot}}(x) = \\left( 1 + \\frac{H/h}{x} \\right)\\, \\left( A + B x \\right),\n$$\nwhich encodes the trade-off that increasing overlap $x$ reduces the condition number (and thus the iteration count), but increases the per-iteration communication and computation.\n\nGiven $H/h = 64$, $A = 0.005$ s, and $B = 0.005$ s, determine the value $x^{\\star}$ that minimizes $T_{\\mathrm{tot}}(x)$ over $x > 0$. Express your final answer as the number of fine-grid overlap layers $x^{\\star}$ (dimensionless). No rounding is required if the result is exact; otherwise, round your answer to four significant figures.",
            "solution": "The user wants me to solve the following problem:\nConsider the symmetric positive definite linear system arising from the standard five-point finite difference discretization of the Poisson equation $\\,-\\nabla^{2} u = f\\,$ with homogeneous Dirichlet boundary conditions on the unit square. The fine-grid spacing is $h$, and the domain is partitioned into an $8 \\times 8$ array of non-overlapping square subdomains, each of side length $H = 1/8$, so that $H/h = 64$. A two-level Additive Schwarz Domain Decomposition (ASDD) preconditioner is used within the Preconditioned Conjugate Gradient (PCG) method.\n\nYou may use the following widely accepted facts as the modeling base:\n\n- The condition number $\\kappa$ of the preconditioned operator for ASDD under standard assumptions on shape-regular partitions and coarse spaces is bounded in two space dimensions by\n$$\n\\kappa(\\delta) \\leq C_{\\mathrm{AS}} \\left( 1 + \\frac{H}{\\delta} \\right)^{2},\n$$\nwhere $\\delta$ is the physical overlap size and $C_{\\mathrm{AS}} > 0$ is a mesh-independent constant.\n- For the PCG method applied to a symmetric positive definite system, the number of iterations $n(\\delta)$ needed to reduce a fixed norm of the error by a prescribed factor scales as\n$$\nn(\\delta) \\propto \\sqrt{\\kappa(\\delta)}.\n$$\n\nAssume the following execution-time model per PCG iteration on a parallel system:\n\n- The per-iteration wall-clock time is the sum of a baseline cost and an overlap-dependent cost,\n$$\nT_{\\mathrm{iter}}(\\delta) = A + B \\,\\frac{\\delta}{h},\n$$\nwhere $A > 0$ captures latency and base computation, and $B > 0$ captures bandwidth and extra computation proportional to the number of fine-grid overlap layers. Define the dimensionless overlap as $x = \\delta/h$ (the number of fine-grid layers).\n\nThe total time-to-solution for a fixed tolerance target can be modeled (up to an overall positive multiplicative constant that does not affect the minimizer) by\n$$\nT_{\\mathrm{tot}}(x) = \\left( 1 + \\frac{H/h}{x} \\right)\\, \\left( A + B x \\right),\n$$\nwhich encodes the trade-off that increasing overlap $x$ reduces the condition number (and thus the iteration count), but increases the per-iteration communication and computation.\n\nGiven $H/h = 64$, $A = 0.005$ s, and $B = 0.005$ s, determine the value $x^{\\star}$ that minimizes $T_{\\mathrm{tot}}(x)$ over $x > 0$. Express your final answer as the number of fine-grid overlap layers $x^{\\star}$ (dimensionless). No rounding is required if the result is exact; otherwise, round your answer to four significant figures.\n\n### Step 1: Extract Givens\n- Total time-to-solution model: $T_{\\mathrm{tot}}(x) = \\left( 1 + \\frac{H/h}{x} \\right) \\left( A + B x \\right)$.\n- Domain of interest for the dimensionless overlap $x$: $x > 0$.\n- Parameter $H/h$: $H/h = 64$.\n- Parameter $A$: $A = 0.005$ s.\n- Parameter $B$: $B = 0.005$ s.\n- Objective: Find the value $x^{\\star}$ that minimizes $T_{\\mathrm{tot}}(x)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is to find the minimum of a given function over a specified domain.\n1.  **Scientific or Factual Soundness**: The problem is scientifically grounded. It presents a standard performance model used in the analysis of parallel domain decomposition methods for solving partial differential equations. The model for total time, $T_{\\mathrm{tot}}(x)$, correctly incorporates the trade-off between communication costs (proportional to $B x$) and computation costs (proportional to iteration count, which is reduced by increasing $x$). The functional forms for iteration count scaling and per-iteration time are standard, simplified models in high-performance computing and numerical analysis.\n2.  **Non-Formalizable or Irrelevant**: The problem is fully formalizable as a standard calculus optimization problem and is directly relevant to the specified scientific domain.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained. All necessary parameters ($H/h$, $A$, $B$) and the explicit function $T_{\\mathrm{tot}}(x)$ are provided. There are no contradictions.\n4.  **Unrealistic or Infeasible**: The parameters and model are physically and computationally plausible for a simplified analysis.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The function $T_{\\mathrm{tot}}(x)$ is a continuous and differentiable function for $x > 0$. A unique minimum can be found using standard calculus techniques.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial; it requires applying calculus to solve an optimization problem that represents a genuine trade-off in a real-world scientific computing scenario.\n7.  **Outside Scientific Verifiability**: The solution is mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\nThe objective is to find the value $x^{\\star}$ that minimizes the total time-to-solution function $T_{\\mathrm{tot}}(x)$ for $x > 0$. The function is given by:\n$$\nT_{\\mathrm{tot}}(x) = \\left( 1 + \\frac{H/h}{x} \\right) (A + Bx)\n$$\nThe given parameters are $H/h = 64$, $A = 0.005$, and $B = 0.005$. Substituting these values into the expression for $T_{\\mathrm{tot}}(x)$:\n$$\nT_{\\mathrm{tot}}(x) = \\left( 1 + \\frac{64}{x} \\right) (0.005 + 0.005x)\n$$\nTo find the minimum of this function, we first find its derivative with respect to $x$ and then set the derivative to zero to find the critical points. Let's expand the expression for $T_{\\mathrm{tot}}(x)$:\n$$\nT_{\\mathrm{tot}}(x) = 1 \\cdot (A + Bx) + \\frac{H/h}{x} \\cdot (A + Bx)\n$$\n$$\nT_{\\mathrm{tot}}(x) = A + Bx + \\frac{A(H/h)}{x} + B(H/h)\n$$\nNow, we compute the first derivative of $T_{\\mathrm{tot}}(x)$ with respect to $x$:\n$$\n\\frac{d T_{\\mathrm{tot}}}{dx} = \\frac{d}{dx} \\left( A + Bx + \\frac{A(H/h)}{x} + B(H/h) \\right)\n$$\nThe terms $A$ and $B(H/h)$ are constants, so their derivatives are zero. The derivative of $Bx$ is $B$. The derivative of $A(H/h)x^{-1}$ is $-A(H/h)x^{-2}$. Therefore,\n$$\n\\frac{d T_{\\mathrm{tot}}}{dx} = B - \\frac{A(H/h)}{x^2}\n$$\nTo find the critical points, we set the first derivative to zero:\n$$\nB - \\frac{A(H/h)}{x^2} = 0\n$$\n$$\nB = \\frac{A(H/h)}{x^2}\n$$\nSolving for $x^2$:\n$$\nx^2 = \\frac{A(H/h)}{B}\n$$\nNow, we substitute the given numerical values $A = 0.005$, $B = 0.005$, and $H/h = 64$:\n$$\nx^2 = \\frac{0.005 \\cdot 64}{0.005}\n$$\n$$\nx^2 = 64\n$$\nSince $x$ represents the number of fine-grid overlap layers, it must be a positive quantity ($x > 0$). Taking the positive square root of both sides:\n$$\nx = \\sqrt{64} = 8\n$$\nThus, the only critical point in the domain $x > 0$ is $x = 8$. To verify that this corresponds to a minimum, we use the second derivative test. We compute the second derivative of $T_{\\mathrm{tot}}(x)$:\n$$\n\\frac{d^2 T_{\\mathrm{tot}}}{dx^2} = \\frac{d}{dx} \\left( B - A(H/h)x^{-2} \\right)\n$$\n$$\n\\frac{d^2 T_{\\mathrm{tot}}}{dx^2} = -A(H/h)(-2)x^{-3} = \\frac{2A(H/h)}{x^3}\n$$\nWe evaluate the second derivative at the critical point $x = 8$. With $A = 0.005 > 0$ and $H/h = 64 > 0$:\n$$\n\\left. \\frac{d^2 T_{\\mathrm{tot}}}{dx^2} \\right|_{x=8} = \\frac{2(0.005)(64)}{8^3} = \\frac{0.64}{512}\n$$\nSince the result is a positive number, the function $T_{\\mathrm{tot}}(x)$ has a local minimum at $x = 8$. As this is the only critical point in the domain $x > 0$, and the function is convex on this domain (since its second derivative is positive for all $x > 0$), this local minimum is also the global minimum.\n\nThe value $x^{\\star}$ that minimizes $T_{\\mathrm{tot}}(x)$ is $8$. This is an exact integer, so no rounding is necessary.",
            "answer": "$$\n\\boxed{8}\n$$"
        }
    ]
}