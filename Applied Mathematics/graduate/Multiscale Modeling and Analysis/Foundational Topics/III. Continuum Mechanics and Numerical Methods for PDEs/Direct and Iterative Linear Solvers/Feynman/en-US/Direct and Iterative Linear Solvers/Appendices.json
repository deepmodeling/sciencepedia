{
    "hands_on_practices": [
        {
            "introduction": "The convergence of basic iterative solvers like the Jacobi method is not unconditional; it depends critically on the properties of the system matrix. This first practice provides a concrete counterexample to illustrate how a matrix that is not diagonally dominant can cause the Jacobi method to diverge . You will then explore a common stabilization technique, diagonal perturbation, and derive the precise condition needed to restore convergence, offering a first-hand look at matrix regularization.",
            "id": "3749921",
            "problem": "Consider a linear system $A x = b$ arising from a two-scale coupling in a multiscale discretization where coarse-scale elimination produces a dense, symmetric positive semidefinite operator with strong inter-node couplings. Let the coarse operator be modeled by the matrix\n$$\nA \\in \\mathbb{R}^{3 \\times 3}, \\quad\nA = \\begin{pmatrix}\n1  2  2 \\\\\n2  1  2 \\\\\n2  2  1\n\\end{pmatrix}.\n$$\nDefine the standard Jacobi method via the matrix splitting $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is the strict lower-triangular part, and $U$ is the strict upper-triangular part. The Jacobi iteration is $x^{(k+1)} = D^{-1}(b - (L+U) x^{(k)})$, with iteration matrix $G_{J} = -D^{-1}(L+U)$. Convergence of the Jacobi method is governed by the spectral radius (SR) criterion, namely, Jacobi converges if and only if $\\rho(G_{J})  1$, where $\\rho(\\cdot)$ denotes the spectral radius.\n\nTask:\n- Construct and demonstrate explicitly that the above $A$ is a counterexample where the Jacobi method fails to converge, by computing $\\rho(G_{J})$.\n- Now consider a diagonal stabilization motivated by coarse-scale penalty regularization: define the perturbed operator $A_{\\gamma} = A + \\gamma I$, where $I$ is the identity matrix and $\\gamma \\in \\mathbb{R}$ is a scalar. For the Jacobi method applied to $A_{\\gamma}$, derive the exact condition on $\\gamma$ that ensures $\\rho(G_{J}(\\gamma))  1$ and determine the critical threshold $\\gamma_{\\star}$ such that Jacobi converges for all $\\gamma > \\gamma_{\\star}$.\n\nProvide the final answer as the exact analytical value of the critical threshold $\\gamma_{\\star}$. No rounding is required and no physical units are involved.",
            "solution": "The problem requires an analysis of the convergence of the Jacobi method for a given matrix $A$ and a perturbed version $A_{\\gamma}$. The validation of the problem statement confirms that it is well-posed, scientifically grounded, and contains all necessary information to proceed with a solution.\n\nThe problem is divided into two tasks. First, to demonstrate that the Jacobi method fails to converge for the matrix $A$. Second, to find the condition on a stabilization parameter $\\gamma$ for the method to converge when applied to the perturbed matrix $A_{\\gamma}$.\n\n**Part 1: Convergence Analysis for Matrix $A$**\n\nThe given matrix is:\n$$\nA = \\begin{pmatrix}\n1  2  2 \\\\\n2  1  2 \\\\\n2  2  1\n\\end{pmatrix}\n$$\nThe Jacobi method is defined by the matrix splitting $A = D + L + U$, where $D$ is the diagonal part of $A$, $L$ is the strictly lower-triangular part, and $U$ is the strictly upper-triangular part.\nFor the given matrix $A$, we have:\n$$\nD = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = I\n$$\n$$\nL + U = \\begin{pmatrix}\n0  2  2 \\\\\n2  0  2 \\\\\n2  2  0\n\\end{pmatrix}\n$$\nThe Jacobi iteration matrix, $G_{J}$, is given by $G_{J} = -D^{-1}(L+U)$.\nSince $D = I$, its inverse is $D^{-1} = I$. Therefore, the iteration matrix is:\n$$\nG_{J} = -I (L+U) = -(L+U) = \\begin{pmatrix}\n0  -2  -2 \\\\\n-2  0  -2 \\\\\n-2  -2  0\n\\end{pmatrix}\n$$\nThe convergence of the Jacobi method is determined by the spectral radius of the iteration matrix, $\\rho(G_{J})$. The method converges if and only if $\\rho(G_{J})  1$. The spectral radius is the maximum absolute value of the eigenvalues of $G_{J}$.\n\nTo find the eigenvalues $\\lambda$ of $G_{J}$, we solve the characteristic equation $\\det(G_{J} - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix}\n-\\lambda  -2  -2 \\\\\n-2  -\\lambda  -2 \\\\\n-2  -2  -\\lambda\n\\end{pmatrix} = 0\n$$\nExpanding the determinant:\n$$\n(-\\lambda)((-\\lambda)(-\\lambda) - (-2)(-2)) - (-2)((-2)(-\\lambda) - (-2)(-2)) + (-2)((-2)(-2) - (-\\lambda)(-2)) = 0\n$$\n$$\n-\\lambda(\\lambda^2 - 4) + 2(2\\lambda - 4) - 2(4 - 2\\lambda) = 0\n$$\n$$\n-\\lambda^3 + 4\\lambda + 4\\lambda - 8 - 8 + 4\\lambda = 0\n$$\n$$\n-\\lambda^3 + 12\\lambda - 16 = 0\n$$\nMultiplying by $-1$, we get the characteristic polynomial:\n$$\n\\lambda^3 - 12\\lambda + 16 = 0\n$$\nWe can find the roots of this polynomial. By the rational root theorem, we can test integer divisors of $16$.\nFor $\\lambda = 2$: $2^3 - 12(2) + 16 = 8 - 24 + 16 = 0$. So, $\\lambda = 2$ is a root.\nWe can perform polynomial division by $(\\lambda - 2)$:\n$$\n\\frac{\\lambda^3 - 12\\lambda + 16}{\\lambda - 2} = \\lambda^2 + 2\\lambda - 8\n$$\nThe quadratic factor can be factored further: $\\lambda^2 + 2\\lambda - 8 = (\\lambda + 4)(\\lambda - 2)$.\nSo, the characteristic equation is $(\\lambda - 2)(\\lambda + 4)(\\lambda - 2) = 0$, which simplifies to $(\\lambda - 2)^2(\\lambda + 4) = 0$.\nThe eigenvalues of $G_{J}$ are $\\lambda_1 = -4$, $\\lambda_2 = 2$, and $\\lambda_3 = 2$.\n\nThe spectral radius is $\\rho(G_{J}) = \\max \\{|\\lambda_1|, |\\lambda_2|, |\\lambda_3|\\} = \\max \\{|-4|, |2|, |2|\\} = 4$.\nSince $\\rho(G_{J}) = 4 \\ge 1$, the Jacobi method for the matrix $A$ does not converge. This completes the first task.\n\n**Part 2: Convergence Analysis for Perturbed Matrix $A_{\\gamma}$**\n\nNow, we consider the perturbed matrix $A_{\\gamma} = A + \\gamma I$, where $\\gamma \\in \\mathbb{R}$.\n$$\nA_{\\gamma} = \\begin{pmatrix}\n1  2  2 \\\\\n2  1  2 \\\\\n2  2  1\n\\end{pmatrix} + \\gamma \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n1+\\gamma  2  2 \\\\\n2  1+\\gamma  2 \\\\\n2  2  1+\\gamma\n\\end{pmatrix}\n$$\nWe apply the Jacobi method to the system $A_{\\gamma}x = b$. The splitting of $A_{\\gamma}$ is $A_{\\gamma} = D_{\\gamma} + L_{\\gamma} + U_{\\gamma}$.\n$$\nD_{\\gamma} = \\begin{pmatrix}\n1+\\gamma  0  0 \\\\\n0  1+\\gamma  0 \\\\\n0  0  1+\\gamma\n\\end{pmatrix} = (1+\\gamma)I\n$$\nThe off-diagonal parts remain unchanged, so $L_{\\gamma} + U_{\\gamma} = L+U$.\nThe new Jacobi iteration matrix, $G_{J}(\\gamma)$, is:\n$$\nG_{J}(\\gamma) = -D_{\\gamma}^{-1}(L_{\\gamma}+U_{\\gamma})\n$$\nFor $D_{\\gamma}$ to be invertible, we must have $1+\\gamma \\neq 0$. Assuming this condition holds, $D_{\\gamma}^{-1} = \\frac{1}{1+\\gamma}I$.\n$$\nG_{J}(\\gamma) = -\\left(\\frac{1}{1+\\gamma}I\\right)(L+U) = \\frac{1}{1+\\gamma} \\left( - (L+U) \\right) = \\frac{1}{1+\\gamma} G_{J}\n$$\nIf $\\lambda$ is an eigenvalue of $G_{J}$ with corresponding eigenvector $v$, then $G_{J}v = \\lambda v$.\nFor the new iteration matrix, we have:\n$$\nG_{J}(\\gamma)v = \\frac{1}{1+\\gamma} G_{J}v = \\frac{\\lambda}{1+\\gamma} v\n$$\nThus, the eigenvalues of $G_{J}(\\gamma)$, let's call them $\\mu$, are related to the eigenvalues of $G_{J}$ by $\\mu = \\frac{\\lambda}{1+\\gamma}$.\nThe eigenvalues of $G_{J}(\\gamma)$ are:\n$$\n\\mu_1 = \\frac{-4}{1+\\gamma}, \\quad \\mu_2 = \\frac{2}{1+\\gamma}, \\quad \\mu_3 = \\frac{2}{1+\\gamma}\n$$\nThe spectral radius of $G_{J}(\\gamma)$ is:\n$$\n\\rho(G_{J}(\\gamma)) = \\max \\left\\{ \\left|\\frac{-4}{1+\\gamma}\\right|, \\left|\\frac{2}{1+\\gamma}\\right| \\right\\} = \\frac{1}{|1+\\gamma|} \\max\\{4, 2\\} = \\frac{4}{|1+\\gamma|}\n$$\nThe condition for convergence is $\\rho(G_{J}(\\gamma))  1$.\n$$\n\\frac{4}{|1+\\gamma|}  1\n$$\nThis is equivalent to $|1+\\gamma| > 4$. This inequality holds if either $1+\\gamma > 4$ or $1+\\gamma  -4$.\nCase 1: $1+\\gamma > 4 \\implies \\gamma > 3$.\nCase 2: $1+\\gamma  -4 \\implies \\gamma  -5$.\nSo, the Jacobi method for $A_{\\gamma}$ converges if and only if $\\gamma > 3$ or $\\gamma  -5$.\n\nThe problem asks for the critical threshold $\\gamma_{\\star}$ such that Jacobi converges for all $\\gamma > \\gamma_{\\star}$. This corresponds to the interval $(3, \\infty)$. The lower bound of this interval defines the critical threshold.\nTherefore, the critical threshold is $\\gamma_{\\star} = 3$.",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "For most real-world problems, iterative solvers require a preconditioner to converge efficiently. This exercise introduces one of the most powerful families of preconditioners: Incomplete LU (ILU) factorization . By working with a system that includes both strong local and weak non-local couplings, you will analyze the fundamental trade-off between the preconditioner's accuracy and its sparsity, controlled by a drop tolerance $\\tau$.",
            "id": "3749911",
            "problem": "Consider a symmetric positive definite sparse matrix $A(\\delta) \\in \\mathbb{R}^{3 \\times 3}$ arising from a two-scale diffusion model, in which fine-scale nearest-neighbor interactions are encoded by the tridiagonal entries and a coarse-scale nonlocal interaction is homogenized into a weak long-range coupling of magnitude $\\delta > 0$ between the first and third nodes:\n$$\nA(\\delta) \\;=\\; \\begin{pmatrix}\n2  -1  \\delta \\\\\n-1  2  -1 \\\\\n\\delta  -1  2\n\\end{pmatrix}.\n$$\nWe construct a preconditioner $M$ by performing an Incomplete Lower-Upper factorization (ILU) with level-of-fill $k=0$ together with an absolute drop tolerance $\\tau > 0$ applied to all entries allowed by the $k=0$ pattern. The rule is: any original or intermediate fill-in entry of magnitude strictly less than $\\tau$ is set to zero before factorization. In particular, if $\\delta  \\tau$, the nonlocal entries at $(1,3)$ and $(3,1)$ are dropped, producing the tridiagonal matrix\n$$\n\\tilde{A} \\;=\\; A(0) \\;=\\; \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix},\n$$\nand $M$ is taken as the exact $LU$ factors of $\\tilde{A}$ under the $k=0$ pattern; otherwise, when $\\delta \\ge \\tau$, $M$ is taken as the exact $LU$ factors of $A(\\delta)$ under the same $k=0$ pattern.\n\nStarting from the core definitions of factorization and preconditioning, analyze how the choice of $\\tau$ affects both the sparsity of $M$ and the spectral condition of the preconditioned system. Then, for a concrete instance with $\\delta = 0.2$ and $\\tau = 0.3$, compute the spectral condition number\n$$\n\\kappa_{2}\\!\\left(M^{-1}A(\\delta)\\right) \\;=\\; \\frac{\\lambda_{\\max}\\!\\left(M^{-1}A(\\delta)\\right)}{\\lambda_{\\min}\\!\\left(M^{-1}A(\\delta)\\right)}\n$$\nwhere $\\lambda_{\\max}$ and $\\lambda_{\\min}$ denote the largest and smallest eigenvalues, respectively. Round your final numerical answer to four significant figures. Express the final answer as a pure number without units.",
            "solution": "The problem is evaluated to be valid. It is scientifically grounded in the principles of numerical linear algebra, specifically preconditioning techniques for sparse linear systems arising from physical models. The problem is well-posed, providing all necessary matrices, parameters, and definitions to arrive at a unique solution. It is stated objectively and contains no contradictions or ambiguities.\n\nThe problem asks for an analysis of a specific preconditioning strategy and a concrete calculation of a spectral condition number.\n\nPart 1: Analysis of the Preconditioner\n\nThe preconditioner $M$ is constructed based on an Incomplete LU factorization with level-of-fill $k=0$ (ILU(0)) and a drop tolerance $\\tau$. The matrix being preconditioned is $A(\\delta)$. The construction of $M$ depends on the comparison between $\\delta$ and $\\tau$.\n\n1.  Influence on Sparsity: The sparsity of the preconditioner $M$ is determined by the sparsity of the matrix that is being factorized. The ILU(0) process, by definition, only allows non-zero entries in the factors $L$ and $U$ where there were non-zero entries in the original matrix.\n    *   If $\\delta  \\tau$, the entries $A_{13}$ and $A_{31}$, both equal to $\\delta$, are dropped. The factorization is performed on the tridiagonal matrix $\\tilde{A} = A(0)$. The LU factors of a tridiagonal matrix are bidiagonal, preserving the tridiagonal sparsity pattern. Thus, $M$ (representing the incomplete factors of $\\tilde{A}$) is sparser. Specifically, the storage for the factors corresponds to a tridiagonal structure.\n    *   If $\\delta \\ge \\tau$, no entries are dropped from $A(\\delta)$. The ILU(0) factorization is performed on $A(\\delta)$, which has non-zero entries at positions $(1,3)$ and $(3,1)$. The factors $L$ and $U$ will have non-zeros in these positions. The resulting preconditioner $M$ is thus denser than in the previous case, as it retains the full sparsity pattern of $A(\\delta)$.\n\n    In summary, a larger value of $\\tau$ relative to $\\delta$ promotes sparsity in $M$ by dropping weak long-range couplings.\n\n2.  Influence on Spectral Condition: The goal of a preconditioner $M$ is to be a good approximation of $A(\\delta)$, such that $M^{-1}A(\\delta)$ is close to the identity matrix $I$. The closer $M^{-1}A(\\delta)$ is to $I$, the closer its eigenvalues are to $1$, and the smaller its spectral condition number $\\kappa_2(M^{-1}A(\\delta))$.\n    *   If $\\delta  \\tau$, $M$ is constructed from $\\tilde{A}=A(0)$. The preconditioned matrix is $M^{-1}A(\\delta)$. Here, $M^{-1}$ is not a good approximation of $A(\\delta)^{-1}$ if $\\delta$ is non-negligible, as $M$ completely ignores the coupling $\\delta$. The difference $A(\\delta) - M = A(\\delta) - A(0)$ introduces an error that can move the eigenvalues of $M^{-1}A(\\delta)$ away from $1$, potentially increasing the condition number.\n    *   If $\\delta \\ge \\tau$, $M$ is the ILU(0) factorization of $A(\\delta)$. For a $3 \\times 3$ matrix with the given sparsity pattern, the LU factorization does not introduce any fill-in. Therefore, the ILU(0) factorization is the exact LU factorization, meaning $M = A(\\delta)$. In this ideal scenario, the preconditioned matrix is $M^{-1}A(\\delta) = (A(\\delta))^{-1}A(\\delta) = I$. The eigenvalues are all $1$, and the condition number is $\\kappa_2(I) = 1$, which is the optimal value.\n\n    In summary, a smaller value of $\\tau$ relative to $\\delta$ leads to a more accurate preconditioner and a better (smaller) condition number, with the trade-off of requiring a denser factorization.\n\nPart 2: Calculation for $\\delta = 0.2$ and $\\tau = 0.3$\n\nWe are given the specific values $\\delta = 0.2$ and $\\tau = 0.3$. Since $0.2  0.3$, we have $\\delta  \\tau$. This corresponds to the first case in our analysis.\n\nThe matrix to be preconditioned is:\n$$\nA(\\delta) \\;=\\; A(0.2) \\;=\\; \\begin{pmatrix}\n2  -1  0.2 \\\\\n-1  2  -1 \\\\\n0.2  -1  2\n\\end{pmatrix}\n$$\nAccording to the problem rule for $\\delta  \\tau$, the off-diagonal entries with magnitude $\\delta$ are dropped. The resulting matrix, denoted $\\tilde{A}$, is used to construct the preconditioner $M$. The problem states that $M$ is taken as the exact LU factors of $\\tilde{A}$ under the $k=0$ pattern. For the tridiagonal matrix $\\tilde{A}$, the exact LU factorization produces no fill-in, so this means we can take $M = \\tilde{A}$.\n$$\nM \\;=\\; \\tilde{A} \\;=\\; A(0) \\;=\\; \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}\n$$\nWe need to compute the spectral condition number of $M^{-1}A(0.2)$. First, we compute the inverse of $M$. The determinant of $M$ is:\n$$\n\\det(M) = 2(2 \\cdot 2 - (-1)(-1)) - (-1)(-1 \\cdot 2 - (-1) \\cdot 0) = 2(3) - (-1)(-2) = 6 - 2 = 4\n$$\nThe inverse $M^{-1}$ is given by $\\frac{1}{\\det(M)}\\text{adj}(M)$. The adjugate matrix, which is the transpose of the cofactor matrix, is:\n$$\n\\text{adj}(M) = \\begin{pmatrix}\n3  2  1 \\\\\n2  4  2 \\\\\n1  2  3\n\\end{pmatrix}\n$$\nSo, the inverse is:\n$$\nM^{-1} = \\frac{1}{4} \\begin{pmatrix}\n3  2  1 \\\\\n2  4  2 \\\\\n1  2  3\n\\end{pmatrix}\n$$\nNext, we compute the preconditioned matrix $C = M^{-1}A(0.2)$:\n$$\nC = \\frac{1}{4} \\begin{pmatrix}\n3  2  1 \\\\\n2  4  2 \\\\\n1  2  3\n\\end{pmatrix}\n\\begin{pmatrix}\n2  -1  0.2 \\\\\n-1  2  -1 \\\\\n0.2  -1  2\n\\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\n4C = \\begin{pmatrix}\n3(2)+2(-1)+1(0.2)  3(-1)+2(2)+1(-1)  3(0.2)+2(-1)+1(2) \\\\\n2(2)+4(-1)+2(0.2)  2(-1)+4(2)+2(-1)  2(0.2)+4(-1)+2(2) \\\\\n1(2)+2(-1)+3(0.2)  1(-1)+2(2)+3(-1)  1(0.2)+2(-1)+3(2)\n\\end{pmatrix}\n$$\n$$\n4C = \\begin{pmatrix}\n6-2+0.2  -3+4-1  0.6-2+2 \\\\\n4-4+0.4  -2+8-2  0.4-4+4 \\\\\n2-2+0.6  -1+4-3  0.2-2+6\n\\end{pmatrix}\n= \\begin{pmatrix}\n4.2  0  0.6 \\\\\n0.4  4  0.4 \\\\\n0.6  0  4.2\n\\end{pmatrix}\n$$\nDividing by $4$:\n$$\nC = \\begin{pmatrix}\n1.05  0  0.15 \\\\\n0.1  1  0.1 \\\\\n0.15  0  1.05\n\\end{pmatrix}\n$$\nTo find the condition number, we need the eigenvalues of $C$. We solve the characteristic equation $\\det(C - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix}\n1.05 - \\lambda  0  0.15 \\\\\n0.1  1 - \\lambda  0.1 \\\\\n0.15  0  1.05 - \\lambda\n\\end{pmatrix} = 0\n$$\nWe expand the determinant along the second column:\n$$\n(1 - \\lambda) \\det\\begin{pmatrix}\n1.05 - \\lambda  0.15 \\\\\n0.15  1.05 - \\lambda\n\\end{pmatrix} = 0\n$$\nThis gives $(1 - \\lambda) [ (1.05 - \\lambda)^2 - (0.15)^2 ] = 0$.\nThe solutions for $\\lambda$ are found from two factors:\n1.  $1 - \\lambda = 0 \\implies \\lambda_1 = 1$.\n2.  $(1.05 - \\lambda)^2 - (0.15)^2 = 0 \\implies (1.05 - \\lambda)^2 = (0.15)^2$.\n    Taking the square root of both sides gives $1.05 - \\lambda = \\pm 0.15$.\n    This yields two more eigenvalues:\n    $\\lambda_2 = 1.05 - 0.15 = 0.9$.\n    $\\lambda_3 = 1.05 + 0.15 = 1.2$.\n\nThe eigenvalues of the preconditioned matrix $C$ are $\\{0.9, 1.0, 1.2\\}$.\nThe largest eigenvalue is $\\lambda_{\\max} = 1.2$.\nThe smallest eigenvalue is $\\lambda_{\\min} = 0.9$.\n\nThe spectral condition number is the ratio of the magnitude of the largest eigenvalue to the smallest eigenvalue:\n$$\n\\kappa_2(M^{-1}A(\\delta)) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{1.2}{0.9} = \\frac{12}{9} = \\frac{4}{3}\n$$\nAs a decimal, this is $1.3333\\ldots$. Rounding to four significant figures gives $1.333$.",
            "answer": "$$\\boxed{1.333}$$"
        },
        {
            "introduction": "Building a robust preconditioner involves navigating potential numerical instabilities. This practice explores a crucial challenge in constructing Incomplete Cholesky (IC) factorizations, a go-to method for symmetric positive definite (SPD) systems . You will discover that IC can break down even for an SPD matrix and then derive the minimal diagonal shift required to guarantee a stable factorization, a technique known as Modified Incomplete Cholesky (MIC).",
            "id": "3749898",
            "problem": "Let $A \\in \\mathbb{R}^{4 \\times 4}$ be the symmetric matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n3  1  0  2 \\\\\n1  3  \\sqrt{7}  0 \\\\\n0  \\sqrt{7}  3  -1 \\\\\n2  0  -1  3\n\\end{pmatrix}.\n$$\nYou will analyze the incomplete Cholesky factorization with zero fill (IC$(0)$), performed without any reordering on the sparsity pattern of $A$. Begin from the definition of the Cholesky factorization, the notion of Symmetric Positive Definite (SPD) matrices, and the algorithmic definition of IC$(0)$: for an SPD matrix $A$, the exact Cholesky factorization satisfies $A = L L^{\\top}$ with $L$ lower triangular; IC$(0)$ computes $L$ by the same recurrence but drops any fill-in that would fall outside the lower-triangular sparsity pattern of $A$.\n\nTasks:\n1. Prove that $A$ is Symmetric Positive Definite (SPD) by verifying that all its leading principal minors are strictly positive.\n2. Execute the IC$(0)$ algorithm symbolically on $A$ in the natural ordering $(1,2,3,4)$ and show that the factorization breaks down at the final pivot because the computed quantity under the square root for the $(4,4)$ entry becomes negative.\n3. A simple and widely used stabilization is the Modified Incomplete Cholesky (MIC) with a uniform diagonal shift: replace $A$ by $A + \\tau I$ for some $\\tau \\ge 0$ and run IC$(0)$ on $A+\\tau I$ in the same ordering. Derive, from first principles, an explicit condition on $\\tau$ under which all IC$(0)$ pivots are strictly positive, and determine the smallest such $\\tau$ in closed form.\n\nProvide as your final answer the smallest $\\tau$ that guarantees IC$(0)$ of $A+\\tau I$ is breakdown-free. Do not round; give an exact closed-form analytic expression.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in numerical linear algebra, well-posed, objective, and self-contained. It presents a standard, albeit detailed, exercise in the analysis of incomplete factorization methods. We proceed with the solution, addressing each of the three tasks sequentially.\n\nThe given matrix is\n$$\nA \\;=\\; \\begin{pmatrix}\n3  1  0  2 \\\\\n1  3  \\sqrt{7}  0 \\\\\n0  \\sqrt{7}  3  -1 \\\\\n2  0  -1  3\n\\end{pmatrix}.\n$$\n\n_Task 1: Prove that $A$ is Symmetric Positive Definite (SPD)_\n\nA symmetric matrix is positive definite if and only if all its leading principal minors are strictly positive. Let $A_k$ be the leading principal submatrix of size $k \\times k$. We must compute $\\det(A_k)$ for $k \\in \\{1, 2, 3, 4\\}$.\n\nFor $k=1$:\nThe first leading principal minor is $\\det(A_1) = |3| = 3$. Since $3 > 0$, this condition is met.\n\nFor $k=2$:\nThe second leading principal minor is $\\det(A_2) = \\det \\begin{pmatrix} 3  1 \\\\ 1  3 \\end{pmatrix} = (3)(3) - (1)(1) = 9 - 1 = 8$. Since $8 > 0$, this condition is met.\n\nFor $k=3$:\nThe third leading principal minor is $\\det(A_3) = \\det \\begin{pmatrix} 3  1  0 \\\\ 1  3  \\sqrt{7} \\\\ 0  \\sqrt{7}  3 \\end{pmatrix}$.\nExpanding along the first row:\n$$ \\det(A_3) = 3 \\det \\begin{pmatrix} 3  \\sqrt{7} \\\\ \\sqrt{7}  3 \\end{pmatrix} - 1 \\det \\begin{pmatrix} 1  \\sqrt{7} \\\\ 0  3 \\end{pmatrix} = 3(3 \\cdot 3 - \\sqrt{7} \\cdot \\sqrt{7}) - 1(1 \\cdot 3 - 0 \\cdot \\sqrt{7}) = 3(9-7) - 3 = 3(2) - 3 = 3. $$\nSince $3 > 0$, this condition is met.\n\nFor $k=4$:\nThe fourth leading principal minor is $\\det(A_4) = \\det(A)$. Expanding along the first row:\n$$ \\det(A) = 3 \\det \\begin{pmatrix} 3  \\sqrt{7}  0 \\\\ \\sqrt{7}  3  -1 \\\\ 0  -1  3 \\end{pmatrix} - 1 \\det \\begin{pmatrix} 1  \\sqrt{7}  0 \\\\ 0  3  -1 \\\\ 2  -1  3 \\end{pmatrix} + 0 - 2 \\det \\begin{pmatrix} 1  3  \\sqrt{7} \\\\ 0  \\sqrt{7}  3 \\\\ 2  0  -1 \\end{pmatrix}. $$\nWe compute the determinants of the $3 \\times 3$ submatrices:\n$$ \\det \\begin{pmatrix} 3  \\sqrt{7}  0 \\\\ \\sqrt{7}  3  -1 \\\\ 0  -1  3 \\end{pmatrix} = 3(9-1) - \\sqrt{7}(3\\sqrt{7}-0) = 24 - 21 = 3. $$\n$$ \\det \\begin{pmatrix} 1  \\sqrt{7}  0 \\\\ 0  3  -1 \\\\ 2  -1  3 \\end{pmatrix} = 1(9-1) - \\sqrt{7}(0 - (-2)) = 8 - 2\\sqrt{7}. $$\n$$ \\det \\begin{pmatrix} 1  3  \\sqrt{7} \\\\ 0  \\sqrt{7}  3 \\\\ 2  0  -1 \\end{pmatrix} = 1(-\\sqrt{7}-0) - 3(0-6) + \\sqrt{7}(0-2\\sqrt{7}) = -\\sqrt{7} + 18 - 14 = 4-\\sqrt{7}. $$\nSubstituting these back:\n$$ \\det(A) = 3(3) - 1(8-2\\sqrt{7}) - 2(4-\\sqrt{7}) = 9 - 8 + 2\\sqrt{7} - 8 + 2\\sqrt{7} = -7 + 4\\sqrt{7}. $$\nTo check the sign, we compare $4\\sqrt{7}$ with $7$. This is equivalent to comparing $(4\\sqrt{7})^2=16 \\cdot 7 = 112$ with $7^2=49$. Since $112 > 49$, we have $4\\sqrt{7} > 7$, so $\\det(A) = -7+4\\sqrt{7} > 0$.\nAll four leading principal minors ($3, 8, 3, -7+4\\sqrt{7}$) are strictly positive. Thus, the matrix $A$ is symmetric and positive definite.\n\n_Task 2: Execute IC$(0)$ and demonstrate breakdown_\n\nThe IC$(0)$ factorization computes a lower triangular matrix $L$ with the same sparsity pattern as the lower part of $A$. The non-zero entries of $L$ are computed via the standard Cholesky recurrence relations. Let $L$ be the IC$(0)$ factor. The required sparsity pattern is $L_{31}=0$ and $L_{42}=0$.\nThe recurrence relations are:\n$$ L_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2} $$\n$$ L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=1}^{j-1} L_{ik}L_{jk} \\right) \\quad \\text{for } i>j \\text{ and } A_{ij} \\ne 0 $$\n\nColumn 1 ($j=1$):\n$L_{11} = \\sqrt{A_{11}} = \\sqrt{3}$.\n$L_{21} = A_{21}/L_{11} = 1/\\sqrt{3}$.\n$L_{31} = 0$ (due to sparsity constraint).\n$L_{41} = A_{41}/L_{11} = 2/\\sqrt{3}$.\n\nColumn 2 ($j=2$):\n$L_{22} = \\sqrt{A_{22} - L_{21}^2} = \\sqrt{3 - (1/\\sqrt{3})^2} = \\sqrt{3 - 1/3} = \\sqrt{8/3}$.\n$L_{32} = \\frac{1}{L_{22}}(A_{32} - L_{31}L_{21}) = \\frac{1}{\\sqrt{8/3}}(\\sqrt{7} - 0 \\cdot 1/\\sqrt{3}) = \\sqrt{7 \\cdot 3/8} = \\sqrt{21/8}$.\n$L_{42} = 0$ (due to sparsity constraint).\n\nColumn 3 ($j=3$):\n$L_{33} = \\sqrt{A_{33} - L_{31}^2 - L_{32}^2} = \\sqrt{3 - 0^2 - (\\sqrt{21/8})^2} = \\sqrt{3 - 21/8} = \\sqrt{(24-21)/8} = \\sqrt{3/8}$.\n$L_{43} = \\frac{1}{L_{33}}(A_{43} - L_{41}L_{31} - L_{42}L_{32}) = \\frac{1}{\\sqrt{3/8}}(-1 - (2/\\sqrt{3}) \\cdot 0 - 0 \\cdot \\sqrt{21/8}) = -1/\\sqrt{3/8} = -\\sqrt{8/3}$.\n\nColumn 4 ($j=4$):\nWe compute the term under the square root for $L_{44}$:\n$$ L_{44}^2 = A_{44} - \\sum_{k=1}^{3} L_{4k}^2 = A_{44} - L_{41}^2 - L_{42}^2 - L_{43}^2 $$\n$$ L_{44}^2 = 3 - (2/\\sqrt{3})^2 - 0^2 - (-\\sqrt{8/3})^2 = 3 - 4/3 - 8/3 = 3 - 12/3 = 3 - 4 = -1. $$\nSince $L_{44}^2 = -1  0$, its square root $L_{44}$ is not a real number. The IC$(0)$ factorization breaks down at the final pivot.\n\n_Task 3: Modified Incomplete Cholesky (MIC) and minimal shift $\\tau$_\n\nWe apply IC$(0)$ to $A' = A + \\tau I$, where $\\tau \\ge 0$. The diagonal entries become $A'_{ii} = A_{ii} + \\tau = 3+\\tau$. The off-diagonal entries remain the same. The sparsity pattern does not change. We re-compute the squared diagonal entries of the factor, now denoted $L_{ii}^2(\\tau)$, and enforce they are all positive.\n\n$L_{11}^2(\\tau) = A'_{11} = 3+\\tau$. For $\\tau \\ge 0$, this is positive.\n\n$L_{22}^2(\\tau) = A'_{22} - L_{21}(\\tau)^2 = (3+\\tau) - (A_{21}/L_{11}(\\tau))^2 = (3+\\tau) - 1^2/(3+\\tau) = \\frac{(3+\\tau)^2-1}{3+\\tau} = \\frac{\\tau^2+6\\tau+8}{3+\\tau} = \\frac{(\\tau+2)(\\tau+4)}{3+\\tau}$. For $\\tau \\ge 0$, this is positive.\n\n$L_{33}^2(\\tau) = A'_{33} - L_{31}(\\tau)^2 - L_{32}(\\tau)^2 = (3+\\tau) - 0 - (A_{32}/L_{22}(\\tau))^2 = (3+\\tau) - (\\sqrt{7})^2/L_{22}^2(\\tau)$.\n$$ L_{33}^2(\\tau) = (3+\\tau) - \\frac{7(3+\\tau)}{(\\tau+2)(\\tau+4)} = (3+\\tau) \\left( 1 - \\frac{7}{(\\tau+2)(\\tau+4)} \\right) = (3+\\tau) \\frac{\\tau^2+6\\tau+8-7}{\\tau^2+6\\tau+8} = (3+\\tau)\\frac{\\tau^2+6\\tau+1}{\\tau^2+6\\tau+8}. $$\nFor $\\tau \\ge 0$, the terms $3+\\tau$ and $\\tau^2+6\\tau+8$ are positive. The term $\\tau^2+6\\tau+1$ has roots at $\\tau=-3\\pm\\sqrt{8}$, which are both negative. Thus, for $\\tau \\ge 0$, $\\tau^2+6\\tau+1 > 0$, and $L_{33}^2(\\tau)$ is positive.\n\n$L_{44}^2(\\tau) = A'_{44} - L_{41}(\\tau)^2 - L_{42}(\\tau)^2 - L_{43}(\\tau)^2$.\nUsing $L_{41}(\\tau) = A_{41}/L_{11}(\\tau)$, $L_{42}(\\tau)=0$, and $L_{43}(\\tau) = A_{43}/L_{33}(\\tau)$:\n$$ L_{44}^2(\\tau) = (3+\\tau) - \\frac{A_{41}^2}{L_{11}^2(\\tau)} - \\frac{A_{43}^2}{L_{33}^2(\\tau)} = (3+\\tau) - \\frac{4}{3+\\tau} - \\frac{1}{L_{33}^2(\\tau)}. $$\n$$ L_{44}^2(\\tau) = \\frac{(3+\\tau)^2-4}{3+\\tau} - \\frac{\\tau^2+6\\tau+8}{(3+\\tau)(\\tau^2+6\\tau+1)} = \\frac{\\tau^2+6\\tau+5}{3+\\tau} - \\frac{\\tau^2+6\\tau+8}{(3+\\tau)(\\tau^2+6\\tau+1)}. $$\n$$ L_{44}^2(\\tau) = \\frac{1}{3+\\tau} \\left( (\\tau^2+6\\tau+5) - \\frac{\\tau^2+6\\tau+8}{\\tau^2+6\\tau+1} \\right). $$\nFor the factorization to be free of breakdown, we need $L_{44}^2(\\tau) > 0$. Since $\\tau \\ge 0$, $3+\\tau > 0$. The condition becomes:\n$$ (\\tau^2+6\\tau+5) - \\frac{\\tau^2+6\\tau+8}{\\tau^2+6\\tau+1} > 0. $$\nLet $y = \\tau^2+6\\tau$. For $\\tau \\ge 0$, we have $y \\ge 0$. The inequality is:\n$$ (y+5) - \\frac{y+8}{y+1} > 0 \\implies \\frac{(y+5)(y+1) - (y+8)}{y+1} > 0 \\implies \\frac{y^2+6y+5-y-8}{y+1} > 0 \\implies \\frac{y^2+5y-3}{y+1} > 0. $$\nThe denominator is $y+1 = \\tau^2+6\\tau+1$, which we have already shown to be positive for $\\tau \\ge 0$. Thus, we need the numerator to be strictly positive:\n$$ y^2+5y-3 > 0. $$\nThe roots of $y^2+5y-3=0$ are $y = \\frac{-5 \\pm \\sqrt{25-4(1)(-3)}}{2} = \\frac{-5 \\pm \\sqrt{37}}{2}$.\nSince $y \\ge 0$, we must satisfy $y > \\frac{-5+\\sqrt{37}}{2}$.\nSubstituting back $y=\\tau^2+6\\tau$:\n$$ \\tau^2+6\\tau > \\frac{\\sqrt{37}-5}{2} \\implies \\tau^2+6\\tau - \\frac{\\sqrt{37}-5}{2} > 0. $$\nTo find for which $\\tau$ this inequality holds, we find the roots of the quadratic equation $\\tau^2+6\\tau - \\frac{\\sqrt{37}-5}{2} = 0$.\n$$ \\tau = \\frac{-6 \\pm \\sqrt{36 - 4(1)(-\\frac{\\sqrt{37}-5}{2})}}{2} = \\frac{-6 \\pm \\sqrt{36 + 2(\\sqrt{37}-5)}}{2} = \\frac{-6 \\pm \\sqrt{26+2\\sqrt{37}}}{2}. $$\n$$ \\tau = -3 \\pm \\frac{\\sqrt{26+2\\sqrt{37}}}{2} = -3 \\pm \\sqrt{\\frac{26+2\\sqrt{37}}{4}} = -3 \\pm \\sqrt{\\frac{13+\\sqrt{37}}{2}}. $$\nThe quadratic function is positive for $\\tau$ greater than the larger root or less than the smaller root. Since we require $\\tau \\ge 0$, we must take the region defined by the positive root:\n$$ \\tau > -3 + \\sqrt{\\frac{13+\\sqrt{37}}{2}}. $$\nThe smallest value of $\\tau$ is the infimum of this set. This threshold value is what the problem asks for.",
            "answer": "$$\\boxed{-3 + \\sqrt{\\frac{13+\\sqrt{37}}{2}}}$$"
        }
    ]
}