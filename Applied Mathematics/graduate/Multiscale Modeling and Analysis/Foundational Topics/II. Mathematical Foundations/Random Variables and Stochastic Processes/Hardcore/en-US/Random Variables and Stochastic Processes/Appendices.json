{
    "hands_on_practices": [
        {
            "introduction": "In multiscale modeling, it is often intractable or unnecessary to resolve a system at its finest scale. Instead, we define coarse-grained or macroscopic variables that capture the essential behavior. This exercise provides a hands-on look at the rigorous, measure-theoretic foundation of this process . You will construct a macroscale random variable by averaging a microscale observable over partitions of the state space, directly engaging with the concepts of $\\sigma$-algebras, measurability, and conditional expectation.",
            "id": "3799984",
            "problem": "Consider a microscale probability space defined as follows. Let $\\Omega = [0,1]$ equipped with the Borel $\\sigma$-algebra and a probability measure $\\mu$ that is absolutely continuous with respect to Lebesgue measure, with probability density function (PDF) $p(x) = 1 + \\sin(2\\pi x)$ for $x \\in [0,1]$. Let $\\mathcal{P} = \\{A_{1},A_{2},A_{3}\\}$ be the partition of $\\Omega$ given by $A_{1} = [0,\\frac{1}{3})$, $A_{2} = [\\frac{1}{3},\\frac{2}{3})$, and $A_{3} = [\\frac{2}{3},1]$. Let $\\sigma(\\mathcal{P})$ denote the $\\sigma$-algebra generated by the partition $\\mathcal{P}$, which represents the macroscale resolution of observation in a multiscale model.\n\nDefine a microscale observable $g:\\Omega \\to \\mathbb{R}$ by $g(\\omega) = \\omega^{2}$. Construct a $\\sigma(\\mathcal{P})$-measurable random variable $X:\\Omega \\to \\mathbb{R}$ that is constant on each partition element and represents the coarse-grained average of $g$ over that element, namely, for $\\omega \\in A_{i}$,\n$$\nX(\\omega) = c_{i}, \\quad \\text{where } c_{i} = \\frac{\\int_{A_{i}} g(x)\\,p(x)\\,dx}{\\int_{A_{i}} p(x)\\,dx}.\n$$\nStarting from the foundational definitions of $\\sigma$-algebra, measurability, and the distribution of a random variable induced by $\\mu$, and using only standard properties of integrals, do the following:\n\n- Establish that $X$ is $\\sigma(\\mathcal{P})$-measurable and constant on each $A_{i}$.\n- Compute the distribution of $X$ by identifying its support $\\{c_{1},c_{2},c_{3}\\}$ and the probabilities $\\mu(A_{i})$.\n- Compute the expectation $\\mathbb{E}[X]$ by first principles.\n\nReport the exact value of $\\mathbb{E}[X]$ in simplest closed form as your final answer. No rounding is required. No units are required. Express all intermediate and final mathematical steps using formal mathematical notation only.",
            "solution": "The problem statement defines a valid probability space and poses a well-defined question within the framework of measure-theoretic probability and multiscale analysis. The probability density function $p(x) = 1 + \\sin(2\\pi x)$ is non-negative on $\\Omega = [0,1]$ and its integral over $\\Omega$ is $\\int_{0}^{1} (1 + \\sin(2\\pi x))\\,dx = [x - \\frac{1}{2\\pi}\\cos(2\\pi x)]_{0}^{1} = (1 - \\frac{1}{2\\pi}) - (0 - \\frac{1}{2\\pi}) = 1$. The problem is scientifically grounded, self-contained, and objective. We may proceed with the solution.\n\nThe problem asks for three tasks: establishing the measurability of the random variable $X$, computing its distribution, and calculating its expectation $\\mathbb{E}[X]$.\n\nFirst, we establish that $X$ is $\\sigma(\\mathcal{P})$-measurable. The partition is $\\mathcal{P} = \\{A_{1}, A_{2}, A_{3}\\}$ with $A_{1} = [0,\\frac{1}{3})$, $A_{2} = [\\frac{1}{3},\\frac{2}{3})$, and $A_{3} = [\\frac{2}{3},1]$. The $\\sigma$-algebra generated by this partition, $\\sigma(\\mathcal{P})$, consists of all possible unions of these sets, including the empty set $\\emptyset$ and the total space $\\Omega$:\n$$ \\sigma(\\mathcal{P}) = \\{\\emptyset, A_{1}, A_{2}, A_{3}, A_{1}\\cup A_{2}, A_{1}\\cup A_{3}, A_{2}\\cup A_{3}, \\Omega\\} $$\nThe random variable $X:\\Omega \\to \\mathbb{R}$ is defined as a simple function, constant on each element of the partition $\\mathcal{P}$:\n$$\nX(\\omega) =\n\\begin{cases}\nc_1  \\text{if } \\omega \\in A_1 \\\\\nc_2  \\text{if } \\omega \\in A_2 \\\\\nc_3  \\text{if } \\omega \\in A_3\n\\end{cases}\n$$\nwhere the constants $c_i$ are given. The range of $X$ is the set $\\{c_1, c_2, c_3\\}$. For $X$ to be $\\sigma(\\mathcal{P})$-measurable, the preimage $X^{-1}(B)$ must belong to $\\sigma(\\mathcal{P})$ for any Borel set $B \\subseteq \\mathbb{R}$. The preimage $X^{-1}(B) = \\{\\omega \\in \\Omega \\mid X(\\omega) \\in B\\}$ can be determined by which of $c_1, c_2, c_3$ are in $B$. For any subset $S \\subseteq \\{c_1, c_2, c_3\\}$, if $B \\cap \\{c_1, c_2, c_3\\} = S$, then $X^{-1}(B)$ is the union of the sets $A_i$ for which $c_i \\in S$. Since any such union is an element of $\\sigma(\\mathcal{P})$, $X$ is $\\sigma(\\mathcal{P})$-measurable. By its definition, $X$ is constant on each $A_i$.\n\nSecond, we compute the distribution of $X$. This requires finding the support $\\{c_1, c_2, c_3\\}$ and the corresponding probabilities $P(X=c_i) = \\mu(A_i)$. The probabilities are calculated by integrating the PDF $p(x)$ over each partition element $A_i$.\nLet the indefinite integral of $p(x)$ be $F(x) = \\int (1+\\sin(2\\pi x))\\,dx = x - \\frac{1}{2\\pi}\\cos(2\\pi x)$.\nThe probabilities are:\n$$ \\mu(A_1) = \\int_{0}^{1/3} p(x)\\,dx = F(\\frac{1}{3}) - F(0) = \\left(\\frac{1}{3} - \\frac{1}{2\\pi}\\cos(\\frac{2\\pi}{3})\\right) - \\left(0 - \\frac{1}{2\\pi}\\cos(0)\\right) = \\frac{1}{3} - \\frac{-\\frac{1}{2}}{2\\pi} + \\frac{1}{2\\pi} = \\frac{1}{3} + \\frac{1}{4\\pi} + \\frac{2}{4\\pi} = \\frac{1}{3} + \\frac{3}{4\\pi} $$\n$$ \\mu(A_2) = \\int_{1/3}^{2/3} p(x)\\,dx = F(\\frac{2}{3}) - F(\\frac{1}{3}) = \\left(\\frac{2}{3} - \\frac{1}{2\\pi}\\cos(\\frac{4\\pi}{3})\\right) - \\left(\\frac{1}{3} - \\frac{1}{2\\pi}\\cos(\\frac{2\\pi}{3})\\right) = \\left(\\frac{2}{3} - \\frac{-\\frac{1}{2}}{2\\pi}\\right) - \\left(\\frac{1}{3} - \\frac{-\\frac{1}{2}}{2\\pi}\\right) = \\frac{1}{3} $$\n$$ \\mu(A_3) = \\int_{2/3}^{1} p(x)\\,dx = F(1) - F(\\frac{2}{3}) = \\left(1 - \\frac{1}{2\\pi}\\cos(2\\pi)\\right) - \\left(\\frac{2}{3} - \\frac{1}{2\\pi}\\cos(\\frac{4\\pi}{3})\\right) = \\left(1 - \\frac{1}{2\\pi}\\right) - \\left(\\frac{2}{3} + \\frac{1}{4\\pi}\\right) = \\frac{1}{3} - \\frac{3}{4\\pi} $$\nThe support values $c_i$ are defined as $c_i = \\frac{\\int_{A_i} g(x)p(x)dx}{\\mu(A_i)}$. This requires computing the numerators $N_i = \\int_{A_i} x^2(1+\\sin(2\\pi x))dx$. Let the indefinite integral be $I(x) = \\int x^2(1+\\sin(2\\pi x))dx$.\n$I(x) = \\int x^2 dx + \\int x^2 \\sin(2\\pi x) dx$. The second integral requires integration by parts twice, which yields:\n$$ \\int x^2 \\sin(ax) dx = \\left(\\frac{2}{a^3}-\\frac{x^2}{a}\\right)\\cos(ax) + \\frac{2x}{a^2}\\sin(ax) $$\nFor $a=2\\pi$, this gives:\n$$ \\int x^2 \\sin(2\\pi x) dx = \\left(\\frac{1}{4\\pi^3}-\\frac{x^2}{2\\pi}\\right)\\cos(2\\pi x) + \\frac{x}{2\\pi^2}\\sin(2\\pi x) $$\nThus, $I(x) = \\frac{x^3}{3} + \\left(\\frac{1}{4\\pi^3}-\\frac{x^2}{2\\pi}\\right)\\cos(2\\pi x) + \\frac{x}{2\\pi^2}\\sin(2\\pi x)$.\nThe numerators are $N_i = I(b_i) - I(a_i)$ where $A_i = [a_i, b_i)$.\n$N_1 = I(\\frac{1}{3}) - I(0) = \\left(\\frac{1}{81} + \\frac{\\sqrt{3}}{12\\pi^2} + \\frac{1}{36\\pi} - \\frac{1}{8\\pi^3}\\right) - \\left(\\frac{1}{4\\pi^3}\\right) = \\frac{1}{81} + \\frac{\\sqrt{3}}{12\\pi^2} + \\frac{1}{36\\pi} - \\frac{3}{8\\pi^3}$.\n$N_2 = I(\\frac{2}{3}) - I(\\frac{1}{3}) = \\left(\\frac{8}{81} - \\frac{\\sqrt{3}}{6\\pi^2} + \\frac{1}{9\\pi} - \\frac{1}{8\\pi^3}\\right) - \\left(\\frac{1}{81} + \\frac{\\sqrt{3}}{12\\pi^2} + \\frac{1}{36\\pi} - \\frac{1}{8\\pi^3}\\right) = \\frac{7}{81} - \\frac{\\sqrt{3}}{4\\pi^2} + \\frac{1}{12\\pi}$.\n$N_3 = I(1) - I(\\frac{2}{3}) = \\left(\\frac{1}{3} - \\frac{1}{2\\pi} + \\frac{1}{4\\pi^3}\\right) - \\left(\\frac{8}{81} - \\frac{\\sqrt{3}}{6\\pi^2} + \\frac{1}{9\\pi} - \\frac{1}{8\\pi^3}\\right) = \\frac{19}{81} + \\frac{\\sqrt{3}}{6\\pi^2} - \\frac{11}{18\\pi} + \\frac{3}{8\\pi^3}$.\nThe support is $\\{c_1, c_2, c_3\\} = \\{N_1/\\mu(A_1), N_2/\\mu(A_2), N_3/\\mu(A_3)\\}$.\n\nThird, we compute the expectation $\\mathbb{E}[X]$. By first principles, the expectation of this discrete random variable is:\n$$ \\mathbb{E}[X] = \\sum_{i=1}^3 c_i P(X=c_i) = \\sum_{i=1}^3 c_i \\mu(A_i) $$\nSubstituting the definition of $c_i$:\n$$ \\mathbb{E}[X] = \\sum_{i=1}^3 \\left( \\frac{\\int_{A_i} g(x)p(x)dx}{\\mu(A_i)} \\right) \\mu(A_i) = \\sum_{i=1}^3 \\int_{A_i} g(x)p(x)dx $$\nSince $\\{A_1, A_2, A_3\\}$ is a partition of $\\Omega=[0,1]$, the sum of integrals is the integral over the whole space:\n$$ \\mathbb{E}[X] = \\int_{0}^1 g(x)p(x)dx = \\int_{0}^1 x^2(1+\\sin(2\\pi x))dx $$\nThis integral is precisely $I(1) - I(0)$, which is the sum of the numerators $N_1+N_2+N_3$. We can either evaluate the definite integral directly or sum the numerators. Summing the numerators:\n$$ N_1+N_2+N_3 = \\left(\\frac{1}{81}+\\frac{7}{81}+\\frac{19}{81}\\right) + \\left(\\frac{\\sqrt{3}}{12\\pi^2}-\\frac{\\sqrt{3}}{4\\pi^2}+\\frac{\\sqrt{3}}{6\\pi^2}\\right) + \\left(\\frac{1}{36\\pi}+\\frac{1}{12\\pi}-\\frac{11}{18\\pi}\\right) + \\left(-\\frac{3}{8\\pi^3}+\\frac{3}{8\\pi^3}\\right) $$\n$$ = \\left(\\frac{27}{81}\\right) + \\left(\\frac{1-3+2}{12\\pi^2}\\right)\\sqrt{3} + \\left(\\frac{1+3-22}{36\\pi}\\right) + 0 $$\n$$ = \\frac{1}{3} + 0 + \\frac{-18}{36\\pi} = \\frac{1}{3} - \\frac{1}{2\\pi} $$\nAlternatively, we evaluate the definite integral $\\int_0^1 x^2(1+\\sin(2\\pi x))dx = I(1)-I(0)$ directly:\n$$ I(1) = \\frac{1}{3} + \\left(\\frac{1}{4\\pi^3}-\\frac{1}{2\\pi}\\right)\\cos(2\\pi) + \\frac{1}{2\\pi^2}\\sin(2\\pi) = \\frac{1}{3} - \\frac{1}{2\\pi} + \\frac{1}{4\\pi^3} $$\n$$ I(0) = 0 + \\left(\\frac{1}{4\\pi^3}-0\\right)\\cos(0) + 0 = \\frac{1}{4\\pi^3} $$\n$$ \\mathbb{E}[X] = I(1) - I(0) = \\left(\\frac{1}{3} - \\frac{1}{2\\pi} + \\frac{1}{4\\pi^3}\\right) - \\left(\\frac{1}{4\\pi^3}\\right) = \\frac{1}{3} - \\frac{1}{2\\pi} $$\nBoth methods yield the same result. The value asked for is $\\mathbb{E}[X]$.",
            "answer": "$$\\boxed{\\frac{1}{3} - \\frac{1}{2\\pi}}$$"
        },
        {
            "introduction": "Many phenomena in science and engineering are modeled as stochastic processes or random fields, where the quantity of interest is a random function. A cornerstone for analyzing and representing these processes is the Karhunen-Loève (KL) expansion, which provides an optimal basis for the process in a mean-square sense. This practice guides you through the fundamental derivation of the KL expansion for a Brownian bridge process, starting from its covariance kernel and solving the associated integral eigenvalue problem . This skill is essential for model reduction and uncertainty quantification.",
            "id": "3799980",
            "problem": "Consider a zero-mean Gaussian process $X(t)$ on $[0,1]$ with covariance kernel $K(s,t) = \\min(s,t) - s t$. This kernel defines a compact, self-adjoint, positive semi-definite covariance operator $\\mathcal{K}$ on $L^{2}([0,1])$ by $(\\mathcal{K}\\varphi)(t) = \\int_{0}^{1} K(s,t) \\varphi(s) \\,\\mathrm{d}s$. This problem asks you to construct the Karhunen–Loève expansion (KLE) of $X(t)$ and use it to approximate a finite-dimensional distribution at two time points.\n\nTasks:\n1. Starting from the definition of the covariance operator and its eigenvalue problem $\\int_{0}^{1} K(s,t)\\,\\varphi(s)\\,\\mathrm{d}s = \\lambda\\,\\varphi(t)$, derive the associated differential equation and boundary conditions that the eigenfunctions must satisfy. Solve for the orthonormal eigenfunctions $\\{\\varphi_{n}\\}_{n\\geq 1}$ and eigenvalues $\\{\\lambda_{n}\\}_{n\\geq 1}$ of $\\mathcal{K}$.\n2. Using the Karhunen–Loève expansion $X(t) = \\sum_{n=1}^{\\infty} \\sqrt{\\lambda_{n}}\\,\\xi_{n}\\,\\varphi_{n}(t)$, where $\\{\\xi_{n}\\}_{n\\geq 1}$ are independent standard normal random variables, define the $N$-term truncation $X_{N}(t) = \\sum_{n=1}^{N} \\sqrt{\\lambda_{n}}\\,\\xi_{n}\\,\\varphi_{n}(t)$ and express the covariance matrix of the bivariate Gaussian vector $(X_{N}(t_{1}), X_{N}(t_{2}))$ in terms of the truncated expansion for arbitrary $t_{1}, t_{2} \\in [0,1]$.\n3. Specialize to $t_{1} = 1/3$ and $t_{2} = 2/3$, and compute the correlation coefficient $\\rho_{N}$ of $(X_{N}(1/3), X_{N}(2/3))$. Determine the exact limit $\\lim_{N\\to\\infty} \\rho_{N}$ and provide this limit value as your final answer. Express the final answer exactly; no rounding is required.\n\nYour derivations must be grounded in core definitions of Gaussian processes, properties of compact self-adjoint operators, and the Sturm–Liouville framework for integral-equation eigenproblems. Do not invoke precomputed eigenpairs or shortcut formulas; derive them from first principles appropriate to this context.",
            "solution": "The problem will be addressed in three sequential parts as requested.\n\nPart 1: Derivation of the Eigenfunctions and Eigenvalues.\n\nThe starting point is the eigenvalue problem for the covariance operator $\\mathcal{K}$:\n$$\n\\lambda \\varphi(t) = \\int_{0}^{1} K(s,t) \\varphi(s) \\,\\mathrm{d}s\n$$\nwhere the kernel is given by $K(s,t) = \\min(s,t) - st$. We substitute the kernel into the integral equation:\n$$\n\\lambda \\varphi(t) = \\int_{0}^{1} (\\min(s,t) - st) \\varphi(s) \\,\\mathrm{d}s\n$$\nTo facilitate differentiation, we split the integral at the point $s=t$, where the definition of $\\min(s,t)$ changes.\n$$\n\\lambda \\varphi(t) = \\int_{0}^{t} (s - st) \\varphi(s) \\,\\mathrm{d}s + \\int_{t}^{1} (t - st) \\varphi(s) \\,\\mathrm{d}s\n$$\nWe can factor out terms that are constant with respect to the integration variable $s$:\n$$\n\\lambda \\varphi(t) = (1-t) \\int_{0}^{t} s \\varphi(s) \\,\\mathrm{d}s + t \\int_{t}^{1} (1-s) \\varphi(s) \\,\\mathrm{d}s\n$$\nThis is an integral equation for the eigenfunction $\\varphi(t)$. To convert it to a differential equation, we differentiate with respect to $t$ using the product rule and the Fundamental Theorem of Calculus (via Leibniz integral rule). Differentiating the equation once yields:\n$$\n\\lambda \\varphi'(t) = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left[ (1-t) \\int_{0}^{t} s \\varphi(s) \\,\\mathrm{d}s \\right] + \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left[ t \\int_{t}^{1} (1-s) \\varphi(s) \\,\\mathrm{d}s \\right]\n$$\n$$\n\\lambda \\varphi'(t) = (-1) \\int_{0}^{t} s \\varphi(s) \\,\\mathrm{d}s + (1-t) (t \\varphi(t)) + (1) \\int_{t}^{1} (1-s) \\varphi(s) \\,\\mathrm{d}s + t (-(1-t)\\varphi(t))\n$$\nThe terms involving $(1-t)t\\varphi(t)$ cancel out, simplifying the expression to:\n$$\n\\lambda \\varphi'(t) = \\int_{t}^{1} (1-s) \\varphi(s) \\,\\mathrm{d}s - \\int_{0}^{t} s \\varphi(s) \\,\\mathrm{d}s\n$$\nDifferentiating a second time with respect to $t$:\n$$\n\\lambda \\varphi''(t) = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left[ \\int_{t}^{1} (1-s) \\varphi(s) \\,\\mathrm{d}s \\right] - \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left[ \\int_{0}^{t} s \\varphi(s) \\,\\mathrm{d}s \\right]\n$$\n$$\n\\lambda \\varphi''(t) = (-(1-t)\\varphi(t)) - (t\\varphi(t)) = - \\varphi(t) + t\\varphi(t) - t\\varphi(t) = -\\varphi(t)\n$$\nThis gives the second-order ordinary differential equation:\n$$\n\\varphi''(t) + \\frac{1}{\\lambda} \\varphi(t) = 0\n$$\nNext, we derive the boundary conditions from the integral equation form. At $t=0$:\n$$\n\\lambda \\varphi(0) = (1-0) \\int_{0}^{0} s \\varphi(s) \\,\\mathrm{d}s + 0 \\cdot \\int_{0}^{1} (1-s) \\varphi(s) \\,\\mathrm{d}s = 0\n$$\nSince the eigenvalues $\\lambda$ must be positive for a non-trivial solution (as the kernel is positive semi-definite and not identically zero), we must have $\\varphi(0) = 0$.\nAt $t=1$:\n$$\n\\lambda \\varphi(1) = (1-1) \\int_{0}^{1} s \\varphi(s) \\,\\mathrm{d}s + 1 \\cdot \\int_{1}^{1} (1-s) \\varphi(s) \\,\\mathrm{d}s = 0\n$$\nThis implies $\\varphi(1) = 0$.\n\nWe now solve the boundary value problem (BVP):\n$$\n\\varphi''(t) + k^2 \\varphi(t) = 0, \\quad \\text{with } \\varphi(0)=0, \\varphi(1)=0, \\quad \\text{where } k^2 = \\frac{1}{\\lambda}\n$$\nThe general solution to the ODE is $\\varphi(t) = A \\cos(kt) + B \\sin(kt)$.\nApplying the first boundary condition, $\\varphi(0)=0$:\n$$\nA \\cos(0) + B \\sin(0) = A = 0\n$$\nThe solution simplifies to $\\varphi(t) = B \\sin(kt)$.\nApplying the second boundary condition, $\\varphi(1)=0$:\n$$\nB \\sin(k) = 0\n$$\nFor a non-trivial solution, we require $B \\neq 0$, which implies $\\sin(k)=0$. The solutions are $k_n = n\\pi$ for any non-zero integer $n$. We take positive integers $n \\geq 1$, as negative integers yield linearly dependent eigenfunctions.\nThe eigenvalues are thus:\n$$\n\\lambda_n = \\frac{1}{k_n^2} = \\frac{1}{(n\\pi)^2}, \\quad n=1, 2, 3, \\ldots\n$$\nThe corresponding eigenfunctions are of the form $\\varphi_n(t) = B_n \\sin(n\\pi t)$.\nTo complete the specification, we must normalize the eigenfunctions to be an orthonormal basis in $L^2([0,1])$.\n$$\n\\int_{0}^{1} \\varphi_n(t)^2 \\,\\mathrm{d}t = 1 \\implies \\int_{0}^{1} B_n^2 \\sin^2(n\\pi t) \\,\\mathrm{d}t = 1\n$$\n$$\nB_n^2 \\int_{0}^{1} \\frac{1-\\cos(2n\\pi t)}{2} \\,\\mathrm{d}t = B_n^2 \\left[ \\frac{t}{2} - \\frac{\\sin(2n\\pi t)}{4n\\pi} \\right]_0^1 = B_n^2 \\left(\\frac{1}{2} - 0 \\right) = \\frac{B_n^2}{2} = 1\n$$\nThis gives $B_n^2=2$, so we choose the positive root $B_n = \\sqrt{2}$.\nThe orthonormal eigenfunctions and eigenvalues are:\n$$\n\\varphi_n(t) = \\sqrt{2} \\sin(n\\pi t) \\quad \\text{and} \\quad \\lambda_n = \\frac{1}{(n\\pi)^2} \\quad \\text{for } n \\geq 1\n$$\n\nPart 2: Covariance Matrix of the Truncated Process.\n\nThe $N$-term truncation of the Karhunen-Loève expansion is $X_N(t) = \\sum_{n=1}^{N} \\sqrt{\\lambda_n} \\xi_n \\varphi_n(t)$, where $\\xi_n$ are i.i.d. standard normal random variables ($\\xi_n \\sim \\mathcal{N}(0,1)$). Since $\\mathbb{E}[\\xi_n]=0$, the truncated process is also zero-mean: $\\mathbb{E}[X_N(t)]=0$.\nThe covariance function of the truncated process, $K_N(t_1, t_2)$, is given by:\n$$\nK_N(t_1, t_2) = \\mathbb{E}[X_N(t_1) X_N(t_2)] = \\mathbb{E}\\left[ \\left(\\sum_{n=1}^{N} \\sqrt{\\lambda_n}\\xi_n\\varphi_n(t_1)\\right) \\left(\\sum_{m=1}^{N} \\sqrt{\\lambda_m}\\xi_m\\varphi_m(t_2)\\right) \\right]\n$$\n$$\n= \\sum_{n=1}^{N} \\sum_{m=1}^{N} \\sqrt{\\lambda_n \\lambda_m} \\varphi_n(t_1) \\varphi_m(t_2) \\mathbb{E}[\\xi_n \\xi_m]\n$$\nUsing the property that $\\mathbb{E}[\\xi_n \\xi_m] = \\delta_{nm}$ (the Kronecker delta) due to the independence and standard variance of the $\\xi_n$:\n$$\nK_N(t_1, t_2) = \\sum_{n=1}^{N} \\sum_{m=1}^{N} \\sqrt{\\lambda_n \\lambda_m} \\varphi_n(t_1) \\varphi_m(t_2) \\delta_{nm} = \\sum_{n=1}^{N} \\lambda_n \\varphi_n(t_1) \\varphi_n(t_2)\n$$\nThe bivariate vector $(X_N(t_1), X_N(t_2))$ is Gaussian with zero mean. Its covariance matrix $\\Sigma_N$ is a $2 \\times 2$ matrix whose elements are given by the covariance function $K_N$:\n$$\n\\Sigma_N(t_1, t_2) = \\begin{pmatrix} \\mathbb{E}[X_N(t_1)^2]  \\mathbb{E}[X_N(t_1) X_N(t_2)] \\\\ \\mathbb{E}[X_N(t_2) X_N(t_1)]  \\mathbb{E}[X_N(t_2)^2] \\end{pmatrix} = \\begin{pmatrix} K_N(t_1, t_1)  K_N(t_1, t_2) \\\\ K_N(t_2, t_1)  K_N(t_2, t_2) \\end{pmatrix}\n$$\nSubstituting the expression for $K_N$, we obtain the covariance matrix in terms of the truncated expansion:\n$$\n\\Sigma_N(t_1, t_2) = \\begin{pmatrix} \\sum_{n=1}^{N} \\lambda_n \\varphi_n(t_1)^2  \\sum_{n=1}^{N} \\lambda_n \\varphi_n(t_1)\\varphi_n(t_2) \\\\ \\sum_{n=1}^{N} \\lambda_n \\varphi_n(t_1)\\varphi_n(t_2)  \\sum_{n=1}^{N} \\lambda_n \\varphi_n(t_2)^2 \\end{pmatrix}\n$$\n\nPart 3: Correlation Coefficient and its Limit.\n\nThe correlation coefficient $\\rho_N$ of the vector $(X_N(t_1), X_N(t_2))$ is defined as:\n$$\n\\rho_N(t_1, t_2) = \\frac{\\text{Cov}(X_N(t_1), X_N(t_2))}{\\sqrt{\\text{Var}(X_N(t_1)) \\text{Var}(X_N(t_2))}} = \\frac{K_N(t_1, t_2)}{\\sqrt{K_N(t_1, t_1) K_N(t_2, t_2)}}\n$$\nWe are asked to find the limit of $\\rho_N$ as $N \\to \\infty$ for $t_1 = 1/3$ and $t_2 = 2/3$.\nAccording to Mercer's theorem, for a continuous, symmetric, positive semi-definite kernel $K(s,t)$, the series expansion in terms of its eigenfunctions and eigenvalues converges absolutely and uniformly to the kernel itself:\n$$\n\\lim_{N\\to\\infty} K_N(s,t) = \\lim_{N\\to\\infty} \\sum_{n=1}^{N} \\lambda_n \\varphi_n(s) \\varphi_n(t) = \\sum_{n=1}^{\\infty} \\lambda_n \\varphi_n(s) \\varphi_n(t) = K(s,t)\n$$\nThe correlation coefficient is a continuous function of the elements of the covariance matrix, provided the variances are non-zero. Since $K_N(t,t) \\to K(t,t) = t-t^2  0$ for $t \\in (0,1)$, we can interchange the limit and the function:\n$$\n\\rho_\\infty = \\lim_{N\\to\\infty} \\rho_N(t_1, t_2) = \\frac{\\lim_{N\\to\\infty} K_N(t_1, t_2)}{\\sqrt{(\\lim_{N\\to\\infty} K_N(t_1, t_1)) (\\lim_{N\\to\\infty} K_N(t_2, t_2))}} = \\frac{K(t_1, t_2)}{\\sqrt{K(t_1, t_1) K(t_2, t_2)}}\n$$\nThus, the limit of the correlation coefficient of the truncated process is simply the correlation coefficient of the original process $X(t)$. We now compute this value for $t_1 = 1/3$ and $t_2 = 2/3$ using the given kernel $K(s,t) = \\min(s,t) - st$.\n\nCovariance:\n$$\nK(1/3, 2/3) = \\min(1/3, 2/3) - (1/3)(2/3) = \\frac{1}{3} - \\frac{2}{9} = \\frac{3}{9} - \\frac{2}{9} = \\frac{1}{9}\n$$\nVariance at $t_1=1/3$:\n$$\nK(1/3, 1/3) = \\min(1/3, 1/3) - (1/3)(1/3) = \\frac{1}{3} - \\frac{1}{9} = \\frac{3}{9} - \\frac{1}{9} = \\frac{2}{9}\n$$\nVariance at $t_2=2/3$:\n$$\nK(2/3, 2/3) = \\min(2/3, 2/3) - (2/3)(2/3) = \\frac{2}{3} - \\frac{4}{9} = \\frac{6}{9} - \\frac{4}{9} = \\frac{2}{9}\n$$\nNow, we compute the limit of the correlation coefficient:\n$$\n\\rho_\\infty = \\frac{K(1/3, 2/3)}{\\sqrt{K(1/3, 1/3) K(2/3, 2/3)}} = \\frac{1/9}{\\sqrt{(2/9)(2/9)}} = \\frac{1/9}{2/9} = \\frac{1}{2}\n$$\nThe exact limit of the correlation coefficient as $N\\to\\infty$ is $1/2$.",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "Beyond representing static random fields, a key task is to model their evolution in time. Continuous-Time Markov Chains (CTMCs) are a fundamental tool for systems that transition randomly between a set of discrete states. This computational practice will have you move from theory to implementation by solving the Kolmogorov forward equations exactly using matrix exponentials . Furthermore, you will diagnose a critical feature of multiscale systems—stiffness—by analyzing the eigenvalues of the generator matrix, a crucial step for choosing appropriate numerical solvers in real-world applications.",
            "id": "3799981",
            "problem": "Consider a continuous-time Markov chain (CTMC) with a finite state space and a generator matrix $Q$. A generator matrix $Q$ satisfies the following properties: for all states $i$ and $j$ with $i \\neq j$, the off-diagonal entries satisfy $Q_{ij} \\geq 0$, and each row sums to zero, i.e., $\\sum_{j} Q_{ij} = 0$. Let $p(t)$ denote the row vector of transient state probabilities at time $t$, and $p(0)$ the initial distribution. The Kolmogorov forward equation for a CTMC is the linear ordinary differential equation $\\frac{d}{dt} p(t) = p(t) Q$ with initial condition $p(0)$.\n\nYour task is to write a program that, for each provided test case, computes the transient probabilities exactly by solving the Kolmogorov forward equation using a matrix exponential routine (do not discretize time), and then analyzes stiffness due to multiscale rate separation via a spectral indicator. Specifically:\n\n- For each test case, compute $p(t)$ from $p(0)$ and $Q$ exactly using a matrix exponential routine applied to $Q$ and $t$.\n- Analyze stiffness caused by scale separation by computing the eigenvalues of $Q$, taking their real parts, excluding zeros, and defining the stiffness indicator $\\sigma$ as the ratio of the largest to the smallest absolute nonzero real parts. A system is classified as stiff if $\\sigma  \\theta$, where $\\theta$ is a specified threshold.\n- Round each component of $p(t)$ and $\\sigma$ to $6$ decimal places.\n\nFundamental base and definitions to be used:\n- A generator matrix $Q$ satisfies $Q_{ij} \\geq 0$ for $i \\neq j$ and $\\sum_{j} Q_{ij} = 0$ for each row $i$.\n- The Kolmogorov forward equation is $\\frac{d}{dt} p(t) = p(t) Q$ with $p(0)$ a probability distribution over the states.\n- The matrix exponential $\\exp(Q t)$ is the unique function that solves the linear system $\\frac{d}{dt} X(t) = X(t) Q$ with $X(0) = I$, where $I$ is the identity matrix.\n\nTest Suite:\nUse the following three test cases. The matrices and vectors are ordered by states as written. All numbers are to be interpreted as dimensionless rates or probabilities.\n\n- Test case $1$ (happy path, $4$ states, fast intra-cluster and slow inter-cluster rates):\n  - Generator matrix $Q_1$:\n    $$\n    Q_1 =\n    \\begin{bmatrix}\n    -200.05  200  0.05  0 \\\\\n    200  -200.1  0.1  0 \\\\\n    0  0.1  -150.1  150 \\\\\n    0.05  0  150  -150.05\n    \\end{bmatrix}\n    $$\n  - Initial distribution $p_1(0) = [1, 0, 0, 0]$.\n  - Time $t_1 = 0.05$.\n  - Stiffness threshold $\\theta = 10^3$.\n\n- Test case $2$ (boundary case, $t = 0$, includes an absorbing state):\n  - Generator matrix $Q_2$:\n    $$\n    Q_2 =\n    \\begin{bmatrix}\n    -5  5  0 \\\\\n    5  -5  0 \\\\\n    0  0  0\n    \\end{bmatrix}\n    $$\n  - Initial distribution $p_2(0) = \\left[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\right]$.\n  - Time $t_2 = 0$.\n  - Stiffness threshold $\\theta = 10^3$.\n\n- Test case $3$ (edge case, $5$ states, extreme multiscale rates with near-separable clusters):\n  - Generator matrix $Q_3$:\n    $$\n    Q_3 =\n    \\begin{bmatrix}\n    -2000.001  1000  1000  0.001  0 \\\\\n    1000  -2000.001  1000  0  0.001 \\\\\n    1000  1000  -2000.001  0.001  0 \\\\\n    0.001  0  0  -10.001  10 \\\\\n    0  0.001  0  10  -10.001\n    \\end{bmatrix}\n    $$\n  - Initial distribution $p_3(0) = [0, 1, 0, 0, 0]$.\n  - Time $t_3 = 0.001$.\n  - Stiffness threshold $\\theta = 10^3$.\n\nRequired outputs per test case:\n- Compute $p(t)$ via exact solution of the Kolmogorov forward equation using a matrix exponential routine.\n- Compute the stiffness indicator $\\sigma$ as the ratio of the largest to smallest absolute nonzero real parts among the eigenvalues of $Q$.\n- Determine the boolean stiff classification $b$ as $b = (\\sigma  \\theta)$.\n\nNumerical handling requirements:\n- Round each component of $p(t)$ to $6$ decimal places.\n- Round $\\sigma$ to $6$ decimal places.\n- Ensure $p(t)$ is a valid probability vector by clipping negative values to $0$ (to address numerical round-off) and renormalizing to sum to $1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list containing, in order, the rounded components of $p(t)$, followed by the rounded $\\sigma$, followed by the boolean stiff classification. For example, the output must look like:\n$$\n\\text{[ [p_{1,0}, p_{1,1}, \\dots, \\sigma_1, b_1], [p_{2,0}, p_{2,1}, \\dots, \\sigma_2, b_2], [p_{3,0}, p_{3,1}, \\dots, \\sigma_3, b_3] ]}\n$$\nwhere each $p_{k,i}$ is a float rounded to $6$ decimal places, each $\\sigma_k$ is a float rounded to $6$ decimal places, and each $b_k$ is a boolean. Only this single line should be printed.",
            "solution": "The problem requires the computation of transient probabilities for a continuous-time Markov chain (CTMC) and the analysis of its stiffness. The solution proceeds in two main parts for each test case: solving the Kolmogorov forward equation and calculating a spectral stiffness indicator.\n\nA CTMC with a finite state space is characterized by a generator matrix $Q$. The entries $Q_{ij}$ for $i \\neq j$ represent the transition rate from state $i$ to state $j$, and they must be non-negative, $Q_{ij} \\geq 0$. The diagonal entries are defined by the condition that each row sums to zero, $\\sum_{j} Q_{ij} = 0$, which implies $Q_{ii} = -\\sum_{j \\neq i} Q_{ij}$. The evolution of the state probabilities $p(t)$, represented as a row vector, is governed by the Kolmogorov forward equation, a system of linear ordinary differential equations (ODEs):\n$$\n\\frac{d}{dt} p(t) = p(t) Q\n$$\nwith a given initial probability distribution $p(0)$.\n\nThe formal solution to this system of linear ODEs with constant coefficients is given by the matrix exponential:\n$$\np(t) = p(0) \\exp(Qt)\n$$\nwhere $\\exp(A)$ denotes the matrix exponential of a square matrix $A$, defined by the Taylor series $\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{A^k}{k!}$. This provides an exact analytical solution, which can be computed numerically to high precision using algorithms such as Padé approximations. This approach avoids the need for time-stepping schemes that are often required for more complex, non-linear ODEs.\n\nThe second part of the problem is to analyze the stiffness of the system. Stiffness in a system of ODEs arises when there is a wide separation of time scales in the dynamics. For a CTMC, these time scales are related to the eigenvalues of the generator matrix $Q$. The eigenvalues $\\lambda_i$ of any generator matrix $Q$ have non-positive real parts, i.e., $\\operatorname{Re}(\\lambda_i) \\leq 0$. At least one eigenvalue is always $0$, corresponding to the stationary distribution(s) of the chain. The non-zero eigenvalues govern the transient dynamics. Each non-zero eigenvalue $\\lambda_i$ corresponds to a characteristic time scale $\\tau_i = -1 / \\operatorname{Re}(\\lambda_i)$.\n\nStiffness is caused by a large disparity between the fastest and slowest time scales. The fastest transient process corresponds to the eigenvalue with the largest-magnitude real part, $\\tau_{fast} = -1 / \\max(|\\operatorname{Re}(\\lambda_i)|)$. The slowest transient process corresponds to the eigenvalue with the smallest-magnitude non-zero real part, $\\tau_{slow} = -1 / \\min(|\\operatorname{Re}(\\lambda_i)| \\text{ for } \\operatorname{Re}(\\lambda_i) \\neq 0)$.\n\nThe stiffness indicator, $\\sigma$, is defined as the ratio of these slowest to fastest time scales:\n$$\n\\sigma = \\frac{\\tau_{slow}}{\\tau_{fast}} = \\frac{\\max(|\\operatorname{Re}(\\lambda_i)| \\text{ where } \\operatorname{Re}(\\lambda_i) \\neq 0)}{\\min(|\\operatorname{Re}(\\lambda_i)| \\text{ where } \\operatorname{Re}(\\lambda_i) \\neq 0)}\n$$\nA large value of $\\sigma$ indicates significant scale separation and thus high stiffness. The problem specifies a threshold $\\theta$ to classify a system as stiff if $\\sigma  \\theta$.\n\nThe computational procedure for each test case is as follows:\n$1$. Given the generator matrix $Q$, the initial distribution $p(0)$, the time $t$, and the stiffness threshold $\\theta$.\n$2$. Compute the matrix product $Qt$.\n$3$. Compute the matrix exponential $P(t) = \\exp(Qt)$ using a numerical library function.\n$4$. Calculate the transient probability vector $p(t) = p(0) P(t)$.\n$5$. Apply numerical corrections to the computed $p(t)$: clip any values less than $0$ and renormalize the vector so its components sum to $1$. This corrects for minor floating-point inaccuracies.\n$6$. Round each component of the resulting $p(t)$ vector to $6$ decimal places as required for the output.\n$7$. Compute the eigenvalues of the matrix $Q$.\n$8$. Extract the real parts of the eigenvalues. Filter out values that are numerically close to zero (e.g., with absolute value less than a small tolerance like $10^{-9}$).\n$9$. From the set of non-zero real parts, find the maximum and minimum of their absolute values.\n$10$. Calculate the stiffness indicator $\\sigma$ as their ratio. If there is only one unique non-zero real part, $\\sigma = 1$.\n$11$. Round $\\sigma$ to $6$ decimal places.\n$12$. Determine the boolean stiffness classification $b = (\\sigma  \\theta)$.\n$13$. Assemble the final result for the test case as a list containing the components of the rounded $p(t)$, the rounded $\\sigma$, and the boolean $b$.\n\nThis procedure will be applied to each of the three test cases provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Solves for transient probabilities of CTMCs and analyzes stiffness for given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"Q\": np.array([\n                [-200.05, 200, 0.05, 0],\n                [200, -200.1, 0.1, 0],\n                [0, 0.1, -150.1, 150],\n                [0.05, 0, 150, -150.05]\n            ]),\n            \"p0\": np.array([1.0, 0.0, 0.0, 0.0]),\n            \"t\": 0.05,\n            \"theta\": 1000.0\n        },\n        {\n            \"Q\": np.array([\n                [-5, 5, 0],\n                [5, -5, 0],\n                [0, 0, 0]\n            ]),\n            \"p0\": np.array([1/3, 1/3, 1/3]),\n            \"t\": 0.0,\n            \"theta\": 1000.0\n        },\n        {\n            \"Q\": np.array([\n                [-2000.001, 1000, 1000, 0.001, 0],\n                [1000, -2000.001, 1000, 0, 0.001],\n                [1000, 1000, -2000.001, 0.001, 0],\n                [0.001, 0, 0, -10.001, 10],\n                [0, 0.001, 0, 10, -10.001]\n            ]),\n            \"p0\": np.array([0.0, 1.0, 0.0, 0.0, 0.0]),\n            \"t\": 0.001,\n            \"theta\": 1000.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        Q = case[\"Q\"]\n        p0 = case[\"p0\"]\n        t = case[\"t\"]\n        theta = case[\"theta\"]\n\n        # Part 1: Compute transient probabilities p(t)\n        # The solution to dp/dt = p(t)Q is p(t) = p(0)exp(Qt)\n        if t == 0:\n            p_t = p0\n        else:\n            P_t = expm(Q * t)\n            p_t = p0 @ P_t\n\n        # Numerical handling: clip negative values and renormalize\n        p_t = np.clip(p_t, 0, None)\n        p_t_sum = np.sum(p_t)\n        if p_t_sum > 0:\n            p_t = p_t / p_t_sum\n        \n        # Round the components of p(t) to 6 decimal places\n        p_t_rounded = np.round(p_t, 6).tolist()\n        \n        # Part 2: Analyze stiffness via spectral indicator sigma\n        eigenvalues = np.linalg.eigvals(Q)\n        \n        # Get real parts of eigenvalues, filtering out numerical zeros\n        real_parts = np.real(eigenvalues)\n        nonzero_real_parts = real_parts[np.abs(real_parts) > 1e-9]\n        \n        if len(nonzero_real_parts) == 0:\n            # If all eigenvalues are zero (e.g., for a zero matrix), system is not stiff\n            sigma = 1.0\n        else:\n            abs_nonzero_real_parts = np.abs(nonzero_real_parts)\n            max_abs_real = np.max(abs_nonzero_real_parts)\n            min_abs_real = np.min(abs_nonzero_real_parts)\n            sigma = max_abs_real / min_abs_real\n            \n        # Round sigma to 6 decimal places\n        sigma_rounded = round(sigma, 6)\n\n        # Classify stiffness\n        is_stiff = sigma > theta\n\n        # Assemble the result for this case\n        case_result = p_t_rounded + [sigma_rounded, is_stiff]\n        results.append(case_result)\n\n    # Final print statement in the exact required format\n    # Using str() on a list of lists produces the desired format with spaces\n    print(str(results).replace(\"'\", \"\"))\n\nsolve()\n```"
        }
    ]
}