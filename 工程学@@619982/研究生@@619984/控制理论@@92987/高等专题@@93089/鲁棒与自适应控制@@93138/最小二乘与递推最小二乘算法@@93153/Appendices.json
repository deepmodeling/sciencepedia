{"hands_on_practices": [{"introduction": "递归最小二乘 ($RLS$) 算法是参数在线估计的基石。与其直接使用现成的公式，从第一性原理出发推导该算法能够加深对其内在机制的理解。本练习将指导您从一个带遗忘因子 $\\lambda$ 的加权最小二乘代价函数出发，为最简单的标量情况推导出完整的 $RLS$ 递推关系，并分析其收敛性，从而为您掌握更复杂的多变量情况奠定坚实的理论基础 [@problem_id:2718871]。", "problem": "考虑离散时间下的标量线性回归模型，其测量值 $y_k \\in \\mathbb{R}$ 由 $y_k = \\varphi \\,\\theta^{\\star} + v_k$ 生成，其中回归量 $\\varphi \\in \\mathbb{R}$ 是一个已知的非零常数（即 $\\varphi \\neq 0$），未知常数参数为 $\\theta^{\\star} \\in \\mathbb{R}$，$\\{v_k\\}$ 是一个零均值、独立同分布且方差有限的测量噪声序列。您需要在线估计 $\\theta^{\\star}$，通过在每个时间 $k \\geq 1$ 最小化指数加权最小二乘代价函数\n$$\nJ_k(\\theta) \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\big(y_i - \\varphi \\,\\theta\\big)^2 + \\lambda^{k} (\\theta - \\theta_0)^2 P_0^{-1},\n$$\n其中 $\\lambda \\in (0,1)$ 是一个已知的遗忘因子，$\\theta_0 \\in \\mathbb{R}$ 是先验均值，$P_0>0$ 是先验方差。从最小二乘估计量是 $J_k(\\theta)$ 的最小化器这一定义出发，推导估计值 $\\theta_k$ 及其相关协方差 $P_k$ 的标量递推最小二乘 (RLS) 递归公式，且不假定任何预先给定的递归形式。然后，针对常数回归量的特殊情况，建立 $P_k$ 的闭式表达式，并利用该表达式，在给定的噪声假设下，分析当 $k \\to \\infty$ 时 $\\theta_k$ 和 $P_k$ 的收敛性质。作为最终答案，请报告 $\\lim_{k\\to\\infty} P_k$ 的闭式极限，仅用 $\\lambda$ 和 $\\varphi$ 表示。不需要数值取整，也不涉及物理单位。", "solution": "所提出的问题是递推参数估计中的一个标准练习，具有科学依据、适定性和客观性。因此，该问题被认为是有效的。我们按要求进行推导和分析。\n\n目标是在每个时间步 $k \\geq 1$ 找到最小化指数加权最小二乘代价函数的参数估计值 $\\theta_k$：\n$$\nJ_k(\\theta) \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\big(y_i - \\varphi \\,\\theta\\big)^2 + \\lambda^{k} (\\theta - \\theta_0)^2 P_0^{-1}\n$$\n其中 $\\lambda \\in (0,1)$。由于 $J_k(\\theta)$ 是关于 $\\theta$ 的二次凸函数，其最小值可以通过将其关于 $\\theta$ 的一阶导数设为零来找到。\n\n首先，我们计算导数：\n$$\n\\frac{dJ_k(\\theta)}{d\\theta} = \\sum_{i=1}^{k} \\lambda^{k-i} \\cdot 2 \\big(y_i - \\varphi \\,\\theta\\big) (-\\varphi) + \\lambda^{k} \\cdot 2 (\\theta - \\theta_0) P_0^{-1}\n$$\n令 $\\theta = \\theta_k$ 处的 $\\frac{dJ_k(\\theta)}{d\\theta} = 0$ 并除以 $2$：\n$$\n-\\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\varphi^2 \\theta_k \\sum_{i=1}^{k} \\lambda^{k-i} + \\lambda^{k} P_0^{-1} (\\theta_k - \\theta_0) = 0\n$$\n重新整理各项以求解 $\\theta_k$：\n$$\n\\theta_k \\left( \\varphi^2 \\sum_{i=1}^{k} \\lambda^{k-i} + \\lambda^{k} P_0^{-1} \\right) = \\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\lambda^{k} P_0^{-1} \\theta_0\n$$\n代价函数的Hessian矩阵是 $\\frac{d^2J_k(\\theta)}{d\\theta^2}$，估计的协方差 $P_k$ 定义为Hessian矩阵一半的逆。因此，\n$$\nP_k^{-1} \\triangleq \\varphi^2 \\sum_{i=1}^{k} \\lambda^{k-i} + \\lambda^{k} P_0^{-1}\n$$\n根据此定义，$\\theta_k$ 的批处理解为：\n$$\n\\theta_k = P_k \\left( \\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\lambda^{k} P_0^{-1} \\theta_0 \\right)\n$$\n为了推导递推形式，我们首先建立 $P_k^{-1}$ 的递推关系。我们将 $P_k^{-1}$ 与 $P_{k-1}^{-1}$ 联系起来：\n$$\nP_k^{-1} = \\varphi^2 \\lambda^{k-k} + \\varphi^2 \\sum_{i=1}^{k-1} \\lambda^{k-i} + \\lambda^{k} P_0^{-1}\n$$\n$$\nP_k^{-1} = \\varphi^2 + \\lambda \\left( \\varphi^2 \\sum_{i=1}^{k-1} \\lambda^{k-1-i} + \\lambda^{k-1} P_0^{-1} \\right)\n$$\n括号中的项正是 $P_{k-1}^{-1}$ 的定义。这就得到了逆协方差的简单递推关系：\n$$\nP_k^{-1} = \\lambda P_{k-1}^{-1} + \\varphi^2\n$$\n为了得到 $P_k$ 的递推公式，我们应用Sherman-Morrison-Woodbury公式（或者，在这种标量情况下，直接求逆）：\n$$\nP_k = (\\lambda P_{k-1}^{-1} + \\varphi^2)^{-1} = \\left(\\frac{\\lambda}{P_{k-1}} + \\varphi^2\\right)^{-1} = \\left(\\frac{\\lambda + \\varphi^2 P_{k-1}}{P_{k-1}}\\right)^{-1} = \\frac{P_{k-1}}{\\lambda + \\varphi^2 P_{k-1}}\n$$\n现在，我们推导 $\\theta_k$ 的递推公式。我们展开 $P_k^{-1}\\theta_k$ 的表达式：\n$$\nP_k^{-1}\\theta_k = \\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\lambda^{k} P_0^{-1} \\theta_0 = \\varphi y_k + \\lambda \\left( \\varphi \\sum_{i=1}^{k-1} \\lambda^{k-1-i} y_i + \\lambda^{k-1} P_0^{-1} \\theta_0 \\right)\n$$\n注意到括号中的项是 $P_{k-1}^{-1}\\theta_{k-1}$，我们有：\n$$\nP_k^{-1}\\theta_k = \\varphi y_k + \\lambda P_{k-1}^{-1} \\theta_{k-1}\n$$\n代入 $P_k^{-1} = \\lambda P_{k-1}^{-1} + \\varphi^2$，这意味着 $\\lambda P_{k-1}^{-1} = P_k^{-1} - \\varphi^2$：\n$$\nP_k^{-1}\\theta_k = \\varphi y_k + (P_k^{-1} - \\varphi^2) \\theta_{k-1}\n$$\n从左侧乘以 $P_k$：\n$$\n\\theta_k = P_k \\varphi y_k + (I - P_k \\varphi^2) \\theta_{k-1} = \\theta_{k-1} + P_k \\varphi (y_k - \\varphi \\theta_{k-1})\n$$\n因此，完整的标量RLS递推公式为：\n$$\n\\theta_k = \\theta_{k-1} + K_k (y_k - \\varphi \\theta_{k-1})\n$$\n$$\nK_k = P_k \\varphi = \\frac{P_{k-1}\\varphi}{\\lambda + \\varphi^2 P_{k-1}}\n$$\n$$\nP_k = (1 - K_k \\varphi) P_{k-1} \\lambda^{-1} \\text{ 或直接地 } P_k = \\frac{P_{k-1}}{\\lambda + \\varphi^2 P_{k-1}}\n$$\n接下来，我们求 $P_k$ 的闭式表达式。递推关系 $P_k^{-1} = \\lambda P_{k-1}^{-1} + \\varphi^2$ 是关于变量 $x_k \\triangleq P_k^{-1}$ 的一阶线性非齐次差分方程。其解可通过展开递推得到：\n$$\nP_k^{-1} = \\lambda^k P_0^{-1} + \\varphi^2 \\sum_{j=0}^{k-1} \\lambda^j\n$$\n有限几何级数的和为 $\\sum_{j=0}^{k-1} \\lambda^j = \\frac{1-\\lambda^k}{1-\\lambda}$。代入此结果可得到 $P_k^{-1}$ 的闭式表达式：\n$$\nP_k^{-1} = \\lambda^k P_0^{-1} + \\varphi^2 \\left(\\frac{1-\\lambda^k}{1-\\lambda}\\right)\n$$\n$P_k$ 的闭式表达式是上式的逆：\n$$\nP_k = \\left( \\lambda^k P_0^{-1} + \\frac{\\varphi^2(1-\\lambda^k)}{1-\\lambda} \\right)^{-1}\n$$\n我们现在分析当 $k \\to \\infty$ 时的收敛性。鉴于 $\\lambda \\in (0,1)$，我们有 $\\lim_{k\\to\\infty} \\lambda^k = 0$。\n逆协方差的极限是：\n$$\n\\lim_{k\\to\\infty} P_k^{-1} = \\lim_{k\\to\\infty} \\left( \\lambda^k P_0^{-1} + \\frac{\\varphi^2(1-\\lambda^k)}{1-\\lambda} \\right) = 0 \\cdot P_0^{-1} + \\frac{\\varphi^2(1-0)}{1-\\lambda} = \\frac{\\varphi^2}{1-\\lambda}\n$$\n由于 $\\varphi \\neq 0$ 且 $\\lambda \\in (0,1)$，该极限是有限的正数。协方差 $P_k$ 的极限是该值的倒数：\n$$\n\\lim_{k\\to\\infty} P_k = \\left( \\lim_{k\\to\\infty} P_k^{-1} \\right)^{-1} = \\left(\\frac{\\varphi^2}{1-\\lambda}\\right)^{-1} = \\frac{1-\\lambda}{\\varphi^2}\n$$\n对于 $\\theta_k$ 的收敛性，考虑估计误差 $\\tilde{\\theta}_k \\triangleq \\theta_k - \\theta^{\\star}$。误差动态由 $\\tilde{\\theta}_k = (1 - P_k \\varphi^2)\\tilde{\\theta}_{k-1} + P_k \\varphi v_k$ 给出。当 $k \\to \\infty$ 时，$P_k \\to \\frac{1-\\lambda}{\\varphi^2}$，所以收缩因子 $(1 - P_k \\varphi^2) \\to (1 - (\\frac{1-\\lambda}{\\varphi^2})\\varphi^2) = \\lambda$。由于 $|\\lambda| < 1$，该估计是渐近无偏的，即 $\\lim_{k\\to\\infty} E[\\theta_k] = \\theta^{\\star}$。估计误差的方差收敛到一个非零的稳态值 $E[\\tilde{\\theta}_{\\infty}^2] = \\frac{(1-\\lambda)\\sigma_v^2}{\\varphi^2(1+\\lambda)}$，其中 $\\sigma_v^2$ 是噪声 $v_k$ 的方差。这意味着 $\\theta_k$ 在分布上收敛到一个以真值 $\\theta^{\\star}$ 为中心的随机变量，而不是收敛到真值本身。这是带有遗忘因子 $\\lambda < 1$ 的RLS算法的预期行为。\n\n该问题明确要求 $P_k$ 的闭式极限。", "answer": "$$\\boxed{\\frac{1-\\lambda}{\\varphi^2}}$$", "id": "2718871"}, {"introduction": "$RLS$ 算法在理论上被视为批处理最小二乘 (Batch LS) 估计的递推实现。本练习旨在通过数值编程的方式，具体地验证这一核心等价关系。您将对同一组数据分别实施批处理和递推两种计算方法，并通过比较它们的最终输出来确认结果的一致性，从而将抽象的数学等价性转化为可操作的、具体的编程实践 [@problem_id:2718829]。这项练习不仅能加深您对两种算法关系的理解，还能锻炼您在估计问题中的编程与验证能力。", "problem": "考虑一个具有确定性回归量序列和未知参数矢量高斯先验的线性回归测量模型。令参数为一个矢量 $\\theta \\in \\mathbb{R}^n$，对于每个样本索引 $k \\in \\{1,\\dots,N\\}$，标量测量值 $y_k \\in \\mathbb{R}$ 通过线性模型 $y_k = \\varphi_k^\\top \\theta + v_k$ 与回归量矢量 $\\varphi_k \\in \\mathbb{R}^n$ 相关，其中 $v_k$ 是一个已知方差为 $R_k > 0$ 的零均值噪声。假设高斯先验为 $\\theta \\sim \\mathcal{N}(\\theta_0, P_0)$，其均值 $\\theta_0 \\in \\mathbb{R}^n$ 和对称正定协方差 $P_0 \\in \\mathbb{R}^{n \\times n}$ 均为已知。此问题的基本基础是带有二次先验的加权最小二乘准则的定义，以及线性高斯建模假设。批处理估计器定义为严格凸二次目标函数的最小化器\n$$\nJ(\\theta) = (\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N \\frac{\\left(y_k - \\varphi_k^\\top \\theta\\right)^2}{R_k}.\n$$\n递归最小二乘 (RLS) 估计器从相同的先验 $(\\theta_0, P_0)$ 开始，使用相同的测量方差 $\\{R_k\\}$，按给定顺序依次处理相同的数据，并在处理完所有 $N$ 个样本后，生成一个更新后的估计值 $\\hat{\\theta}_N$。\n\n任务：编写一个完整的、可运行的程序，针对下面列出的每个测试用例，计算：\n- 作为 $J(\\theta)$ 唯一最小化器的批处理估计器 $\\hat{\\theta}_{\\text{batch}}$，以及\n- 以相同的先验 $(\\theta_0, P_0)$ 初始化，并按给定顺序处理所有样本后得到的最终 RLS 估计器 $\\hat{\\theta}_{N}$。\n\n然后，对于每个测试用例，计算欧几里得范数 $\\|\\hat{\\theta}_{\\text{batch}} - \\hat{\\theta}_N\\|_2$，将其与容差 $\\tau = 10^{-7}$ 进行比较，并返回一个布尔结果：当且仅当该范数小于或等于 $\\tau$ 时，结果为 $true$，否则为 $false$。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[true,false,true,true,false]”）。该列表必须为每个测试用例包含一个布尔值，顺序与下面呈现的顺序相同。\n\n测试套件（每个用例指定 $n, N$，回归矩阵 $\\Phi \\in \\mathbb{R}^{N \\times n}$（其第 $k$ 行为 $\\varphi_k^\\top$），测量矢量 $y \\in \\mathbb{R}^N$，测量方差 $R \\in \\mathbb{R}^N$（其元素为 $R_k$），先验均值 $\\theta_0 \\in \\mathbb{R}^n$ 和先验协方差 $P_0 \\in \\mathbb{R}^{n \\times n}$）：\n\n- 用例 1（理想情况，满秩，异构方差）：\n  - $n = 3$, $N = 6$.\n  - $\\Phi = \\begin{bmatrix}\n  1.0 & 0.5 & -0.3 \\\\\n  0.2 & -1.0 & 0.7 \\\\\n  1.5 & 0.0 & 0.5 \\\\\n  -0.3 & 0.8 & 1.2 \\\\\n  0.0 & -0.4 & 2.0 \\\\\n  1.0 & 1.0 & 1.0\n  \\end{bmatrix}$.\n  - $y = [\\, 0.7,\\, -1.2,\\, 1.5,\\, 0.3,\\, 0.0,\\, 2.0 \\,]$.\n  - $R = [\\, 0.5,\\, 1.2,\\, 0.8,\\, 1.0,\\, 0.3,\\, 2.0 \\,]$.\n  - $\\theta_0 = [\\, 0.1,\\, -0.2,\\, 0.3 \\,]$.\n  - $P_0 = \\mathrm{diag}([\\, 1.0,\\, 2.0,\\, 0.5 \\,])$.\n\n- 用例 2（秩亏设计，通过先验进行正则化以确保唯一性）：\n  - $n = 4$, $N = 5$.\n  - $\\Phi = \\begin{bmatrix}\n  1.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.0 & 1.0 & 0.0 & 0.0 \\\\\n  1.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.0 & 1.0 & 0.0 & 0.0 \\\\\n  1.0 & 1.0 & 0.0 & 0.0\n  \\end{bmatrix}$.\n  - $y = [\\, 1.0,\\, 2.0,\\, 1.5,\\, 1.8,\\, 3.1 \\,]$.\n  - $R = [\\, 0.1,\\, 0.2,\\, 0.3,\\, 0.4,\\, 0.5 \\,]$.\n  - $\\theta_0 = [\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0 \\,]$.\n  - $P_0 = \\mathrm{diag}([\\, 10.0,\\, 10.0,\\, 1.0,\\, 1.0 \\,])$.\n\n- 用例 3（单样本边界情况）：\n  - $n = 2$, $N = 1$.\n  - $\\Phi = \\begin{bmatrix} 2.0 & -1.0 \\end{bmatrix}$.\n  - $y = [\\, 0.5 \\,]$.\n  - $R = [\\, 0.05 \\,]$.\n  - $\\theta_0 = [\\, 1.0,\\, -1.0 \\,]$.\n  - $P_0 = \\begin{bmatrix} 0.2 & 0.05 \\\\ 0.05 & 0.5 \\end{bmatrix}$.\n\n- 用例 4（病态缩放，异构先验方差）：\n  - $n = 3$, $N = 4$.\n  - $\\Phi = \\begin{bmatrix}\n  10^{6} & 10^{-6} & 1.0 \\\\\n  10^{6} & -10^{-6} & -1.0 \\\\\n  2\\cdot 10^{6} & 0.0 & 0.5 \\\\\n  -10^{6} & 10^{-6} & -0.5\n  \\end{bmatrix}$.\n  - $y = [\\, 1.0,\\, -1.0,\\, 0.5,\\, -0.2 \\,]$.\n  - $R = [\\, 1.0,\\, 2.0,\\, 1.5,\\, 0.7 \\,]$.\n  - $\\theta_0 = [\\, 0.0,\\, 0.0,\\, 0.0 \\,]$.\n  - $P_0 = \\mathrm{diag}([\\, 10^{-12},\\, 10^{12},\\, 1.0 \\,])$.\n\n- 用例 5（不等测量方差，强调加权行为）：\n  - $n = 3$, $N = 3$.\n  - $\\Phi = \\begin{bmatrix}\n  1.0 & 2.0 & 3.0 \\\\\n  4.0 & 5.0 & 6.0 \\\\\n  7.0 & 8.0 & 10.0\n  \\end{bmatrix}$.\n  - $y = [\\, 14.0,\\, 32.0,\\, 50.0 \\,]$.\n  - $R = [\\, 0.5,\\, 100.0,\\, 0.1 \\,]$.\n  - $\\theta_0 = [\\, 0.0,\\, 0.0,\\, 0.0 \\,]$.\n  - $P_0 = I_3$（$3 \\times 3$ 单位矩阵）。\n\n角度单位不适用。不涉及物理单位。容差为 $\\tau = 10^{-7}$，欧几里得范数必须与此容差进行比较。最终输出必须是单行，包含一个对应于五个用例的布尔值列表，并按顺序排列。", "solution": "在尝试任何解决方案之前，首先对问题陈述的有效性进行严格审查。\n\n**步骤 1：提取已知条件**\n\n测量模型由 $y_k = \\varphi_k^\\top \\theta + v_k$ 给出，其中 $k \\in \\{1, \\dots, N\\}$，$\\theta \\in \\mathbb{R}^n$ 是未知参数矢量，$y_k \\in \\mathbb{R}$ 是标量测量值，$\\varphi_k \\in \\mathbb{R}^n$ 是回归量矢量，$v_k$ 是零均值噪声，其已知方差为 $R_k > 0$。\n参数的先验是高斯分布 $\\theta \\sim \\mathcal{N}(\\theta_0, P_0)$，其已知均值为 $\\theta_0 \\in \\mathbb{R}^n$，已知对称正定协方差为 $P_0 \\in \\mathbb{R}^{n \\times n}$。\n批处理估计器 $\\hat{\\theta}_{\\text{batch}}$ 定义为目标函数的唯一最小化器：\n$$\nJ(\\theta) = (\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N \\frac{\\left(y_k - \\varphi_k^\\top \\theta\\right)^2}{R_k}.\n$$\n递归最小二乘 (RLS) 估计器 $\\hat{\\theta}_N$ 定义为从相同的先验 $(\\theta_0, P_0)$ 开始，顺序处理测量值 $\\{y_k\\}_{k=1}^N$ 的结果。\n任务要求比较这两种方法的最终估计器 $\\hat{\\theta}_{\\text{batch}}$ 和 $\\hat{\\theta}_N$，通过计算它们差值的欧几里得范数 $\\|\\hat{\\theta}_{\\text{batch}} - \\hat{\\theta}_N\\|_2$，并检查此范数是否小于或等于容差 $\\tau = 10^{-7}$。\n提供了五个测试用例，每个都指定了参数 $n$、$N$、$\\Phi$、$y$、$R$、$\\theta_0$ 和 $P_0$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n根据指定的验证标准对问题进行分析。\n\n1.  **科学依据**：此问题是线性高斯系统中贝叶斯估计的标准练习。加权最小二乘与二次先验（对应于高斯先验）、批处理估计和递归估计（特别是针对静态参数的卡尔曼滤波器）的概念是统计信号处理和控制理论中基本且成熟的原则。该问题在科学上是合理的。\n\n2.  **适定性**：目标函数 $J(\\theta)$ 是二次项之和。先验项 $(\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0)$ 是 $\\theta$ 的严格凸函数，因为 $P_0$ 被给定为对称正定，这意味着其逆 $P_0^{-1}$ 也是对称正定的。测量项 $\\sum_{k=1}^N \\frac{(y_k - \\varphi_k^\\top \\theta)^2}{R_k}$ 是凸的（半正定二次型）。一个严格凸函数与一个或多个凸函数之和是严格凸的。一个严格凸函数有唯一的最小值。因此，最小化 $J(\\theta)$ 的唯一解 $\\hat{\\theta}_{\\text{batch}}$ 总是存在的。即使在仅凭测量信息是秩亏的情况下（如用例 2），这也成立，因为先验项起到了正则化问题的作用。该问题是适定的。\n\n3.  **客观性**：问题以精确的数学语言表述。所有变量和目标都得到了明确的定义。它不含主观或基于观点的陈述。\n\n4.  **完整性**：为五个测试用例中的每一个都提供了所有必要的数据（$n, N, \\Phi, y, R, \\theta_0, P_0$）。问题是自洽的。\n\n5.  **结论**：该问题有效。它是估计理论领域中一个适定的、有科学依据的、客观的任务。它是对批处理和递归贝叶斯最小二乘估计之间基本代数等价性的验证。\n\n**步骤 3：判定与行动**\n\n该问题被判定为**有效**。将提供解决方案。\n\n问题的核心是证明线性高斯模型的批处理估计器和递归估计器产生相同的结果。我们将首先推导批处理估计器的闭式表达式，然后推导递归估计器的迭代方程。\n\n**批处理估计器推导**\n\n批处理估计器 $\\hat{\\theta}_{\\text{batch}}$ 最小化成本函数 $J(\\theta)$。为找到最小值，我们计算 $J(\\theta)$ 关于 $\\theta$ 的梯度并将其设为零。\n$$\n\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\left( (\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N R_k^{-1} (y_k - \\varphi_k^\\top \\theta)^2 \\right) = 0\n$$\n使用矢量微积分的标准法则，梯度为：\n$$\n\\nabla_\\theta J(\\theta) = 2 P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N R_k^{-1} \\cdot 2(y_k - \\varphi_k^\\top \\theta)(-\\varphi_k) = 0\n$$\n两边除以 $2$ 并重新排列各项：\n$$\nP_0^{-1} \\theta - P_0^{-1} \\theta_0 - \\sum_{k=1}^N R_k^{-1} \\varphi_k y_k + \\sum_{k=1}^N R_k^{-1} \\varphi_k \\varphi_k^\\top \\theta = 0\n$$\n将包含 $\\theta$ 的项组合在一起：\n$$\n\\left( P_0^{-1} + \\sum_{k=1}^N \\frac{\\varphi_k \\varphi_k^\\top}{R_k} \\right) \\theta = P_0^{-1} \\theta_0 + \\sum_{k=1}^N \\frac{\\varphi_k y_k}{R_k}\n$$\n令回归矩阵为 $\\Phi \\in \\mathbb{R}^{N \\times n}$（其行为 $\\varphi_k^\\top$），测量矢量为 $y \\in \\mathbb{R}^N$，加权矩阵为 $W = \\mathrm{diag}(R_1^{-1}, \\dots, R_N^{-1})$。这些和可以用矩阵形式表示：$\\sum_{k=1}^N \\frac{\\varphi_k \\varphi_k^\\top}{R_k} = \\Phi^\\top W \\Phi$ 以及 $\\sum_{k=1}^N \\frac{\\varphi_k y_k}{R_k} = \\Phi^\\top W y$。\n方程变为：\n$$\n\\left( P_0^{-1} + \\Phi^\\top W \\Phi \\right) \\hat{\\theta}_{\\text{batch}} = P_0^{-1} \\theta_0 + \\Phi^\\top W y\n$$\n因此，批处理估计器的解为：\n$$\n\\hat{\\theta}_{\\text{batch}} = \\left( P_0^{-1} + \\Phi^\\top W \\Phi \\right)^{-1} \\left( P_0^{-1} \\theta_0 + \\Phi^\\top W y \\right)\n$$\n矩阵 $\\left( P_0^{-1} + \\Phi^\\top W \\Phi \\right)$ 是后验协方差 $P_N^{-1}$ 的逆，并保证是可逆的。\n\n**递归最小二乘 (RLS) 估计器推导**\n\n当每个测量值 $y_k$ 可用时，RLS 估计器会顺序更新对 $\\theta$ 的估计。这是卡尔曼滤波方程在静态参数系统（$\\theta_{k} = \\theta_{k-1}$）上的直接应用。第 $k$ 步的估计值 $\\hat{\\theta}_k$ 及其相关的协方差 $P_k$ 是基于第 $k-1$ 步的估计值和新的测量值 $y_k$ 计算得出的。\n\n该过程从先验信息开始：\n- 初始估计值：$\\hat{\\theta}_0 = \\theta_0$\n- 初始协方差：$P_0 = P_0$\n\n对于每个测量 $k = 1, 2, \\dots, N$，执行以下更新步骤：\n1.  **新息（预测误差）**：实际测量值与预测测量值之间的差异。\n    $$e_k = y_k - \\varphi_k^\\top \\hat{\\theta}_{k-1}$$\n2.  **新息协方差**：新息的方差。\n    $$S_k = \\varphi_k^\\top P_{k-1} \\varphi_k + R_k$$\n3.  **卡尔曼增益**：赋予新息的权重。它平衡了当前估计的不确定性和新测量的不确定性。\n    $$K_k = P_{k-1} \\varphi_k S_k^{-1}$$\n4.  **状态（参数）更新**：新的估计值是旧的估计值加上一个与新息成比例的校正量。\n    $$\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + K_k e_k$$\n5.  **协方差更新**：估计的不确定性减小。使用数值稳定的形式。\n    $$P_k = (I - K_k \\varphi_k^\\top) P_{k-1}$$\n\n在遍历所有 $N$ 个测量值之后，最终的 RLS 估计值为 $\\hat{\\theta}_N$，最终的协方差为 $P_N$。估计理论中的一个基本结论是，在理想算术条件下，$\\hat{\\theta}_N$ 在代数上与 $\\hat{\\theta}_{\\text{batch}}$ 完全相同。数值实现将在指定的浮点容差范围内验证此等价性。\n\n**数值实现计划**\n\n将编写一个程序，为每个测试用例执行以下操作：\n1.  使用推导出的矩阵公式计算 $\\hat{\\theta}_{\\text{batch}}$。为了数值稳定性，将使用线性系统求解器而不是显式矩阵求逆：求解 $A x = b$ 以得到 $x$ 优于计算 $A^{-1}b$。\n2.  通过实现 RLS 循环来计算 $\\hat{\\theta}_N$，从 $(\\theta_0, P_0)$ 开始，并遍历 $N$ 个测量值。为保持数值稳定性和正确性，可以在每一步显式地对协方差矩阵 $P_k$ 进行对称化，以抵消潜在的浮点不精确性。\n3.  计算差值的欧几里得范数：$\\|\\hat{\\theta}_{\\text{batch}} - \\hat{\\theta}_N\\|_2$。\n4.  将此范数与容差 $\\tau = 10^{-7}$ 进行比较，并生成布尔结果。\n\n最终输出将是这些布尔结果的列表，每个测试用例一个，并按要求格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    \n    # Tolerance for comparison of the Euclidean norm.\n    tau = 1e-7\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, full rank, heterogeneous variances)\n        {\n            \"n\": 3, \"N\": 6,\n            \"Phi\": np.array([\n                [1.0, 0.5, -0.3], [0.2, -1.0, 0.7], [1.5, 0.0, 0.5],\n                [-0.3, 0.8, 1.2], [0.0, -0.4, 2.0], [1.0, 1.0, 1.0]\n            ]),\n            \"y\": np.array([0.7, -1.2, 1.5, 0.3, 0.0, 2.0]),\n            \"R\": np.array([0.5, 1.2, 0.8, 1.0, 0.3, 2.0]),\n            \"theta0\": np.array([0.1, -0.2, 0.3]),\n            \"P0\": np.diag([1.0, 2.0, 0.5]),\n        },\n        # Case 2 (rank-deficient design, regularization from prior ensures uniqueness)\n        {\n            \"n\": 4, \"N\": 5,\n            \"Phi\": np.array([\n                [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0]\n            ]),\n            \"y\": np.array([1.0, 2.0, 1.5, 1.8, 3.1]),\n            \"R\": np.array([0.1, 0.2, 0.3, 0.4, 0.5]),\n            \"theta0\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"P0\": np.diag([10.0, 10.0, 1.0, 1.0]),\n        },\n        # Case 3 (single-sample boundary case)\n        {\n            \"n\": 2, \"N\": 1,\n            \"Phi\": np.array([[2.0, -1.0]]),\n            \"y\": np.array([0.5]),\n            \"R\": np.array([0.05]),\n            \"theta0\": np.array([1.0, -1.0]),\n            \"P0\": np.array([[0.2, 0.05], [0.05, 0.5]]),\n        },\n        # Case 4 (ill-conditioned scaling, heterogeneous prior variances)\n        {\n            \"n\": 3, \"N\": 4,\n            \"Phi\": np.array([\n                [1e6, 1e-6, 1.0], [1e6, -1e-6, -1.0],\n                [2e6, 0.0, 0.5], [-1e6, 1e-6, -0.5]\n            ]),\n            \"y\": np.array([1.0, -1.0, 0.5, -0.2]),\n            \"R\": np.array([1.0, 2.0, 1.5, 0.7]),\n            \"theta0\": np.array([0.0, 0.0, 0.0]),\n            \"P0\": np.diag([1e-12, 1e12, 1.0]),\n        },\n        # Case 5 (unequal measurement variances emphasizing weighting behavior)\n        {\n            \"n\": 3, \"N\": 3,\n            \"Phi\": np.array([\n                [1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 10.0]\n            ]),\n            \"y\": np.array([14.0, 32.0, 50.0]),\n            \"R\": np.array([0.5, 100.0, 0.1]),\n            \"theta0\": np.array([0.0, 0.0, 0.0]),\n            \"P0\": np.eye(3),\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack case parameters\n        n, N = case[\"n\"], case[\"N\"]\n        Phi, y = case[\"Phi\"], case[\"y\"]\n        R, theta0, P0 = case[\"R\"], case[\"theta0\"], case[\"P0\"]\n\n        # 1. Batch Estimator\n        # theta_batch = (P0_inv + Phi^T W Phi)^-1 (P0_inv theta0 + Phi^T W y)\n        # where W = diag(1/R_k)\n        P0_inv = np.linalg.inv(P0)\n        W = np.diag(1.0 / R)\n        \n        # Information matrix: I = P0_inv + Phi^T W Phi\n        info_matrix = P0_inv + Phi.T @ W @ Phi\n        # Information vector: i = P0_inv theta0 + Phi^T W y\n        info_vector = P0_inv @ theta0 + Phi.T @ W @ y\n        \n        # Solve I * theta = i for theta. Numerically more stable than inv(I) @ i.\n        theta_batch = np.linalg.solve(info_matrix, info_vector)\n\n        # 2. Recursive Least Squares (RLS) Estimator\n        theta_rls = theta0.copy().astype(np.float64)\n        P_rls = P0.copy().astype(np.float64)\n        \n        for k in range(N):\n            # Get k-th measurement and regressor\n            phi_k = Phi[k, :].reshape(n, 1)\n            y_k = y[k]\n            R_k = R[k]\n            \n            # Innovation\n            e_k = y_k - phi_k.T @ theta_rls\n            \n            # Innovation covariance\n            S_k = phi_k.T @ P_rls @ phi_k + R_k\n            \n            # Kalman Gain\n            K_k = (P_rls @ phi_k) / S_k\n            \n            # State update\n            theta_rls = theta_rls + (K_k * e_k).flatten()\n            \n            # Covariance update (Joseph form is most stable, but this is simpler and sufficient)\n            P_rls = (np.eye(n) - K_k @ phi_k.T) @ P_rls\n            # Enforce symmetry to prevent numerical drift\n            P_rls = 0.5 * (P_rls + P_rls.T)\n\n        # 3. Comparison\n        # Calculate the Euclidean norm of the difference vector\n        norm_diff = np.linalg.norm(theta_batch - theta_rls)\n        \n        # Compare against the tolerance\n        is_equivalent = norm_diff <= tau\n        results.append(str(is_equivalent).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2718829"}, {"introduction": "最小二乘法的一个关键假设是回归量与噪声不相关，但在反馈控制等闭环系统中，这一假设常常被打破，导致估计结果出现偏差，即所谓的“内生性问题”。本练习通过一个典型的闭环系统辨识场景，让您亲身体验标准最小二乘法 (OLS/RLS) 在此情况下的失效。更重要的是，它将引导您使用工具变量 (IV) 法作为一种强大的补救措施，来获取对系统参数的一致性估计，从而让您深刻理解估计算法在实际应用中的局限性与扩展方法 [@problem_id:2718799]。", "problem": "考虑一个具有静态被控对象和瞬时反馈的闭环控制系统。该标量被控对象由方程 $y_t = b\\,u_t + e_t$ 给出，其中 $y_t$ 是被控对象输出，$u_t$ 是控制输入，$b$ 是待辨识的真实被控对象增益，$e_t$ 是零均值扰动。控制器在静态反馈律 $u_t = g\\,r_t - k\\,y_t$ 中使用当前的被控对象输出，其中 $r_t$ 是一个外生参考信号，$g$ 是已知的参考增益，$k$ 是已知的反馈增益。假设 $\\{r_t\\}$ 和 $\\{e_t\\}$ 是相互独立的独立同分布高斯随机变量序列，$r_t \\sim \\mathcal{N}(0,\\sigma_r^2)$ 且 $e_t \\sim \\mathcal{N}(0,\\sigma_e^2)$，其中 $\\sigma_r^2 > 0$ 且 $\\sigma_e^2 > 0$。所有量均为实值。目标是使用普通最小二乘法 (OLS) 和以 $z_t = r_t$ 为工具变量的工具变量 (IV) 估计器，从一批 $N$ 个样本 $\\{(u_t,y_t)\\}_{t=1}^N$ 中估计 $b$，并将这些估计结果与递推最小二乘法 (RLS) 的估计结果进行比较。\n\n您的任务如下。\n\n1) 仅使用基本定义（期望的线性性质、大数定律以及普通最小二乘法 (OLS)、工具变量 (IV) 和递推最小二乘法 (RLS) 的定义），推导在指定反馈下，将 $y_t$ 对 $u_t$ 进行回归的 OLS 估计器的概率极限，并确定当 $k \\neq 0$ 时该极限是否等于 $b$。请勿使用任何预先引用的最终结果；从 $y_t = b\\,u_t + e_t$ 和 $u_t = g\\,r_t - k\\,y_t$ 出发，并就反馈引起的内生性进行推理。总结在哪种关于 $k$ 的条件下 OLS 对 $b$ 是一致的。\n\n2) 再次从第一性原理出发，论证在所述假设下，使用 $z_t = r_t$ 作为工具变量的 IV 估计器对 $b$ 是一致的。根据 $r_t$、$u_t$ 和 $e_t$ 的联合统计量，明确指出相关性条件和外生性条件。\n\n3) 在仿真数据上实现以下估计器：\n- 从 $\\{(u_t,y_t)\\}$ 中得到的批处理 OLS 估计值 $\\hat{b}_{\\mathrm{OLS}}$。\n- 使用工具变量 $z_t = r_t$，从 $\\{(z_t,u_t,y_t)\\}$ 中得到的批处理 IV 估计值 $\\hat{b}_{\\mathrm{IV}}$。\n- 在相同数据上顺序更新的递推最小二乘法 (RLS) 估计值 $\\hat{b}_{\\mathrm{RLS}}$，其遗忘因子 $\\lambda = 1$，标量协方差初始化 $P_0 = 10^6$，初始参数 $\\hat{b}_0 = 0$。\n\n4) 对于下面的每个测试用例，使用明确的数值容差计算以下真值（布尔）断言：\n- 将真实性容差定义为 $\\varepsilon_{\\text{true}} = 10^{-2}$，OLS 概率极限容差定义为 $\\varepsilon_{\\text{lim}} = 10^{-2}$，RLS 一致性容差定义为 $\\varepsilon_{\\text{rls}} = 5 \\times 10^{-3}$。\n- 令 $b_{\\infty,\\mathrm{OLS}}$ 表示您在任务1中推导出的理论 OLS 概率极限，用 $(b,k,g,\\sigma_r,\\sigma_e)$ 表示。\n- 对于每个测试用例，按此顺序报告三个布尔值：\n    a) $\\text{OLS\\_inconsistent}$：如果 $|\\hat{b}_{\\mathrm{OLS}} - b| > \\varepsilon_{\\text{true}}$ 且 $|\\hat{b}_{\\mathrm{OLS}} - b_{\\infty,\\mathrm{OLS}}| \\le \\varepsilon_{\\text{lim}}$，则为真。\n    b) $\\text{IV\\_eliminates\\_bias}$：如果 $|\\hat{b}_{\\mathrm{IV}} - b| \\le \\varepsilon_{\\text{true}}$，则为真。\n    c) $\\text{RLS\\_matches\\_OLS}$：如果 $|\\hat{b}_{\\mathrm{RLS}} - \\hat{b}_{\\mathrm{OLS}}| \\le \\varepsilon_{\\text{rls}}$，则为真。\n\n仿真细节：\n- 为保证可复现性，使用一个固定的伪随机种子，其值为 $12345$。\n- 对于每个测试用例，生成 $N$ 个独立的 $r_t$ 和 $e_t$ 样本，然后根据上述方程构成 $u_t$ 和 $y_t$。\n- 对于下面的测试用例，您可以假设 $1 + k\\,b \\neq 0$。\n\n测试套件：\n- 用例 A (存在反馈): $(b,k,g,\\sigma_r,\\sigma_e,N) = (0.8,0.5,1.0,1.0,0.7,200000)$.\n- 用例 B (无反馈): $(b,k,g,\\sigma_r,\\sigma_e,N) = (0.8,0.0,1.0,1.0,0.7,200000)$.\n- 用例 C (更强反馈): $(b,k,g,\\sigma_r,\\sigma_e,N) = (0.8,1.5,1.0,1.0,0.7,200000)$.\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按 A、B、C 顺序连接的三个用例的所有结果，每个用例贡献三个布尔值 $(\\text{OLS\\_inconsistent}, \\text{IV\\_eliminates\\_bias}, \\text{RLS\\_matches\\_OLS})$，形式为用方括号括起来的逗号分隔列表，例如 $[\\text{True},\\text{False},\\text{True},\\ldots]$。\n- 不应产生任何其他输出。", "solution": "所提出的问题要求对在闭环反馈系统中运行的线性静态被控对象的参数估计进行严谨的分析。我们必须首先验证问题的完整性。该问题是系统辨识中的一个典型练习，其基础是控制理论和统计估计的既定原理。该问题提法明确、客观，并包含了所有必要信息。其参数在科学上是合理的，并且它提出了一个关于反馈存在时估计器一致性的非平凡问题。因此，该问题被认为是有效的，并将提供一个正式的解决方案。\n\n该系统由以下方程组描述：\n被控对象模型：$y_t = b u_t + e_t$\n反馈控制器：$u_t = g r_t - k y_t$\n\n这里，$y_t$ 是输出，$u_t$ 是控制输入，$b$ 是待估计的未知参数，$e_t \\sim \\mathcal{N}(0, \\sigma_e^2)$ 是随机扰动，$r_t \\sim \\mathcal{N}(0, \\sigma_r^2)$ 是外生参考信号，$g$ 和 $k$ 是已知的控制器增益。序列 $\\{r_t\\}$ 和 $\\{e_t\\}$ 是独立的。我们假设 $1+kb \\neq 0$ 以确保闭环系统是适定的。\n\n首先，我们必须仅用外生输入 $r_t$ 和 $e_t$ 来表示内生变量 $u_t$ 和 $y_t$。将对象方程代入控制器方程可得：\n$u_t = g r_t - k (b u_t + e_t) \\implies u_t(1+kb) = g r_t - k e_t \\implies u_t = \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t$\n将此式代回对象方程可得：\n$y_t = b \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right) + e_t \\implies y_t = \\frac{bg}{1+kb} r_t + \\left(1 - \\frac{bk}{1+kb}\\right) e_t \\implies y_t = \\frac{bg}{1+kb} r_t + \\frac{1}{1+kb} e_t$\n这些是该系统的简化式方程。\n\n**1. 普通最小二乘 (OLS) 估计器分析**\n\n在 $y_t$ 对 $u_t$ 的回归中，参数 $b$ 的 OLS 估计器由下式给出：\n$$ \\hat{b}_{\\mathrm{OLS}} = \\left( \\sum_{t=1}^N u_t^2 \\right)^{-1} \\left( \\sum_{t=1}^N u_t y_t \\right) = \\frac{\\frac{1}{N}\\sum_{t=1}^N u_t y_t}{\\frac{1}{N}\\sum_{t=1}^N u_t^2} $$\n根据大数定律，当 $N \\to \\infty$ 时，该估计器的概率极限 (plim) 是期望值的比率：\n$$ b_{\\infty,\\mathrm{OLS}} = \\mathrm{plim}_{N \\to \\infty} \\hat{b}_{\\mathrm{OLS}} = \\frac{\\mathrm{E}[u_t y_t]}{\\mathrm{E}[u_t^2]} $$\nOLS 一致性 ($b_{\\infty,\\mathrm{OLS}} = b$) 的基本条件是回归量 $u_t$ 必须与误差项 $e_t$ 不相关。这就是外生性属性。我们来检验这个条件。我们计算协方差 $\\mathrm{E}[u_t e_t]$：\n$$ \\mathrm{E}[u_t e_t] = \\mathrm{E}\\left[ \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right) e_t \\right] = \\frac{g}{1+kb}\\mathrm{E}[r_t e_t] - \\frac{k}{1+kb}\\mathrm{E}[e_t^2] $$\n鉴于 $r_t$ 和 $e_t$ 是独立的且均值为零，$\\mathrm{E}[r_t e_t] = \\mathrm{E}[r_t]\\mathrm{E}[e_t] = 0$。又因 $\\mathrm{E}[e_t^2] = \\sigma_e^2$，我们得出：\n$$ \\mathrm{E}[u_t e_t] = -\\frac{k \\sigma_e^2}{1+kb} $$\n当且仅当 $k \\neq 0$ 时（因为 $\\sigma_e^2 > 0$ 且 $1+kb \\neq 0$），该协方差非零。反馈增益 $k$ 在扰动 $e_t$ 和输入 $u_t$ 之间建立了一个直接的代数联系。这种回归量与误差项相关的现象被称为内生性。它违反了 OLS 的核心假设，并导致一个有偏且不一致的估计器。\n\n为了找到 OLS 估计器的渐近值，我们继续计算必要的矩。\n$$ \\mathrm{E}[u_t^2] = \\mathrm{E}\\left[ \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right)^2 \\right] = \\frac{1}{(1+kb)^2} (g^2 \\mathrm{E}[r_t^2] + k^2 \\mathrm{E}[e_t^2]) = \\frac{g^2 \\sigma_r^2 + k^2 \\sigma_e^2}{(1+kb)^2} $$\n概率极限的分子是 $\\mathrm{E}[u_t y_t] = \\mathrm{E}[u_t(b u_t + e_t)] = b\\mathrm{E}[u_t^2] + \\mathrm{E}[u_t e_t]$。\n因此，概率极限为：\n$$ b_{\\infty,\\mathrm{OLS}} = \\frac{b\\mathrm{E}[u_t^2] + \\mathrm{E}[u_t e_t]}{\\mathrm{E}[u_t^2]} = b + \\frac{\\mathrm{E}[u_t e_t]}{\\mathrm{E}[u_t^2]} $$\n代入我们推导出的矩：\n$$ b_{\\infty,\\mathrm{OLS}} = b - \\frac{k \\sigma_e^2 (1+kb)}{g^2 \\sigma_r^2 + k^2 \\sigma_e^2} $$\n这个表达式证实了 OLS 估计器是不一致的，只要存在反馈 ($k \\neq 0$)，它就会收敛到一个不同于真实参数 $b$ 的值。只有在开环情况（即 $k=0$）下才能实现一致性。\n\n**2. 工具变量 (IV) 估计器分析**\n\nIV 方法旨在克服内生性问题。它需要一个工具变量 $z_t$，该变量需满足两个条件：\n1.  **工具变量相关性**: 工具变量必须与内生回归量 $u_t$ 相关。形式上，$\\mathrm{E}[z_t u_t] \\neq 0$。\n2.  **工具变量外生性**: 工具变量必须与模型的误差项 $e_t$ 不相关。形式上，$\\mathrm{E}[z_t e_t] = 0$。\n\n所提出的工具变量是外生参考信号 $z_t=r_t$。我们来验证这两个条件。\n外生性条件根据定义是满足的：$r_t$ 和 $e_t$ 来自相互独立的序列，因此 $\\mathrm{E}[r_t e_t] = 0$。\n对于相关性条件，我们计算协方差 $\\mathrm{E}[r_t u_t]$：\n$$ \\mathrm{E}[r_t u_t] = \\mathrm{E}\\left[ r_t \\left( \\frac{g}{1+kb} r_t - \\frac{k}{1+kb} e_t \\right) \\right] = \\frac{g}{1+kb}\\mathrm{E}[r_t^2] - \\frac{k}{1+kb}\\mathrm{E}[r_t e_t] = \\frac{g \\sigma_r^2}{1+kb} $$\n假设 $g \\neq 0$ 且 $\\sigma_r^2 > 0$（这对于任何有意义的控制目标都是成立的，并且在测试用例中得到满足），该协方差非零。因此，$r_t$ 是一个有效的工具变量。\n\nIV 估计器的构造如下：\n$$ \\hat{b}_{\\mathrm{IV}} = \\left( \\sum_{t=1}^N z_t u_t \\right)^{-1} \\left( \\sum_{t=1}^N z_t y_t \\right) = \\frac{\\frac{1}{N}\\sum_{t=1}^N r_t y_t}{\\frac{1}{N}\\sum_{t=1}^N r_t u_t} $$\n其概率极限为：\n$$ \\mathrm{plim}_{N \\to \\infty} \\hat{b}_{\\mathrm{IV}} = \\frac{\\mathrm{E}[r_t y_t]}{\\mathrm{E}[r_t u_t]} $$\n分子是 $\\mathrm{E}[r_t y_t] = \\mathrm{E}[r_t(b u_t + e_t)] = b\\mathrm{E}[r_t u_t] + \\mathrm{E}[r_t e_t]$。由于外生性，$\\mathrm{E}[r_t e_t]=0$。因此，$\\mathrm{E}[r_t y_t] = b\\mathrm{E}[r_t u_t]$。\n将此式代入概率极限表达式：\n$$ \\mathrm{plim}_{N \\to \\infty} \\hat{b}_{\\mathrm{IV}} = \\frac{b\\mathrm{E}[r_t u_t]}{\\mathrm{E}[r_t u_t]} = b $$\nIV 估计器对 $b$ 是一致的，它正确地处理了由反馈回路引起的内生性。\n\n**3. 递推最小二乘 (RLS) 估计器**\n\nRLS 算法为最小二乘估计提供了一种序贯更新方法。当遗忘因子 $\\lambda$ 设置为 $1$ 时，假设使用无信息先验（即初始协方差 $P_0$ 很大），RLS 在相同数据集上会得到与批处理 OLS 估计器完全相同的结果。因此，我们预期当 $k \\neq 0$ 时，$\\hat{b}_{\\mathrm{RLS}}$ 在数值上会非常接近 $\\hat{b}_{\\mathrm{OLS}}$，并遭受同样的渐近偏差。仿真将验证这一等价性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the system identification problem for OLS, IV, and RLS estimators.\n    \"\"\"\n    # Set the fixed pseudorandom seed for reproducibility.\n    np.random.seed(12345)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (b, k, g, sigma_r, sigma_e, N)\n        (0.8, 0.5, 1.0, 1.0, 0.7, 200000),  # Case A: feedback present\n        (0.8, 0.0, 1.0, 1.0, 0.7, 200000),  # Case B: no feedback\n        (0.8, 1.5, 1.0, 1.0, 0.7, 200000),  # Case C: stronger feedback\n    ]\n\n    # Define numerical tolerances for asserting truth values.\n    eps_true = 1e-2    # Tolerance for proximity to the true parameter b\n    eps_lim = 1e-2     # Tolerance for proximity to the OLS theoretical limit\n    eps_rls = 5e-3     # Tolerance for agreement between RLS and OLS estimates\n\n    all_results = []\n\n    for case in test_cases:\n        b, k, g, sigma_r, sigma_e, N = case\n\n        # Generate exogenous signals r_t and e_t.\n        r = np.random.normal(0, sigma_r, N)\n        e = np.random.normal(0, sigma_e, N)\n\n        # Compute endogenous signals u_t and y_t using the reduced-form equations.\n        # This is more efficient than a step-by-step simulation loop.\n        denom = 1.0 + k * b\n        if denom == 0:\n            # This case is excluded by the problem statement.\n            # Handle defensively to avoid division by zero.\n            u = np.full(N, np.nan)\n            y = np.full(N, np.nan)\n        else:\n            u = (g / denom) * r - (k / denom) * e\n            y = (b * g / denom) * r + (1.0 / denom) * e\n\n        # Task 3.1: Batch Ordinary Least Squares (OLS) estimate.\n        # This is the standard formula for scalar regression.\n        b_ols = np.dot(u, y) / np.dot(u, u)\n\n        # Task 3.2: Batch Instrumental Variables (IV) estimate using z_t = r_t.\n        # The instrument r_t replaces the regressor u_t in the \"X'X\" part of the projection.\n        b_iv = np.dot(r, y) / np.dot(r, u)\n\n        # Task 3.3: Recursive Least Squares (RLS) estimate.\n        # Initialize parameter and covariance matrix (scalar in this case).\n        b_rls = 0.0\n        P = 1e6\n        lambda_rls = 1.0  # Forgetting factor set to 1.\n        \n        # Sequentially update the estimate for each data point.\n        for i in range(N):\n            phi = u[i]  # The regressor at step i\n            y_i = y[i]  # The observation at step i\n            \n            # Kalman gain calculation\n            # denom_K = lambda_rls + phi * P * phi is dimensionally incorrect for vector case\n            # Correct scalar form is lambda_rls + P * phi^2\n            gain = (P * phi) / (lambda_rls + P * phi * phi)\n            \n            # Prediction error\n            prediction_error = y_i - phi * b_rls\n            \n            # Update parameter estimate\n            b_rls = b_rls + gain * prediction_error\n            \n            # Update covariance\n            P = (1.0 / lambda_rls) * (1.0 - gain * phi) * P\n            \n        # Task 4: Compute boolean assertions.\n        # Calculate the theoretical probability limit of the OLS estimator.\n        if k == 0:\n            b_inf_ols = b\n        else:\n            b_inf_ols = b - (k * sigma_e**2 * (1.0 + k * b)) / (g**2 * sigma_r**2 + k**2 * sigma_e**2)\n\n        # a) OLS_inconsistent: True if OLS is far from true 'b' but close to its theoretical limit.\n        ols_inconsistent = (abs(b_ols - b) > eps_true) and (abs(b_ols - b_inf_ols) = eps_lim)\n        \n        # b) IV_eliminates_bias: True if IV estimate is close to the true 'b'.\n        iv_eliminates_bias = (abs(b_iv - b) = eps_true)\n        \n        # c) RLS_matches_OLS: True if final RLS estimate is close to batch OLS estimate.\n        rls_matches_ols = (abs(b_rls - b_ols) = eps_rls)\n\n        all_results.extend([ols_inconsistent, iv_eliminates_bias, rls_matches_ols])\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) correctly converts Python booleans (True/False) to strings (\"True\"/\"False\").\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2718799"}]}