{"hands_on_practices": [{"introduction": "物理信息神经网络（PINN）的核心在于其损失函数，它将物理定律编码为可优化的目标。这个练习将指导你从头开始构建这样一个损失函数，这是将任何偏微分方程（PDE）的求解问题转化为神经网络训练任务的第一步。通过为泊松方程这样一个经典且普适的PDE构建损失函数，你将掌握PINN学习物理规律的基本原理，即通过惩罚在内部“配置点”上PDE残差的均方误差以及在边界上不满足边界条件的误差来实现 [@problem_id:2126324]。", "problem": "一位研究人员正在构建一个物理信息神经网络 (PINN)，以寻找一个二维方形区域内静电势 $V(x,y)$ 的近似解。该电势的物理行为由泊松方程描述：\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\n其中 $f(x,y)$ 代表给定的电荷分布密度，$\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ 是拉普拉斯算子。该电势定义在域 $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$ 上。该域的边界 $\\partial D$ 保持在零电势（接地），这施加了边界条件 $V(x,y) = 0$，对于所有 $(x,y) \\in \\partial D$ 均成立。\n\nPINN 模型，记为 $\\hat{V}(x,y; \\theta)$，通过最小化一个包含了该问题物理信息的损失函数 $L(\\theta)$ 来学习 $V(x,y)$ 的近似值。在这里，$\\theta$ 代表神经网络的所有可训练参数。损失函数使用两组离散点进行计算：\n1.  一组 $N_{pde}$ 个配置点，$S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$，位于域 $D$ 的内部。\n2.  一组 $N_{bc}$ 个边界点，$S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$，位于边界 $\\partial D$ 上。\n\n总损失函数 $L(\\theta)$ 是两个均方误差项的和：一项用于控制偏微分方程 ($L_{pde}$)，另一项用于边界条件 ($L_{bc}$)。\n\n构建总损失函数 $L(\\theta) = L_{pde} + L_{bc}$ 的数学表达式。您的表达式应使用网络输出 $\\hat{V}$、其二阶偏导数、函数 $f$、给定的点集及其各自的大小 $N_{pde}$ 和 $N_{bc}$ 来表示。", "solution": "我们从控制泊松方程和边界条件开始：\n$$\n\\nabla^{2}V(x,y)=-f(x,y), \\quad V(x,y)=0 \\text{ for } (x,y)\\in \\partial D.\n$$\n物理信息神经网络通过 $\\hat{V}(x,y;\\theta)$ 来近似 $V$。内部配置点 $(x_{i},y_{i})\\in S_{pde}$ 处的偏微分方程残差通过将泊松方程施加于 $\\hat{V}$ 来定义：\n$$\nr_{i}(\\theta)=\\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\n使用二维拉普拉斯算子的定义，这等价于\n$$\nr_{i}(\\theta)=\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\n于是在 $S_{pde}$ 上强制执行偏微分方程的均方误差为\n$$\nL_{pde}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}.\n$$\n边界 $\\partial D$ 上的边界条件 $V=0$ 通过惩罚在边界点 $(x_{j},y_{j})\\in S_{bc}$ 上 $\\hat{V}$ 与零的偏差来强制执行：\n$$\nL_{bc}(\\theta)=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$\n因此，总损失是这两个均方误差项的和：\n$$\nL(\\theta)=L_{pde}(\\theta)+L_{bc}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$", "id": "2126324"}, {"introduction": "在定义了损失函数之后，一个关键的实践挑战是如何有效地施加边界条件。仅仅在损失函数中惩罚边界误差（一种“软”约束）有时效率不高。本练习探讨了一种更为巧妙的“硬”约束方法，即通过改造网络结构来精确满足给定的狄利克雷（Dirichlet）边界条件 [@problem_id:2126300]。这种方法的核心是设计一个特定的解形式（ansatz），它能够保证无论神经网络的权重如何变化，其最终输出总能自动满足边界条件，从而显著提升训练的稳定性和解的精度。", "problem": "在科学计算领域，物理信息神经网络（PINNs）已成为求解微分方程的强大工具。设计 PINN 的一个关键方面是确保其输出（即解的近似）满足给定的边界条件。实现这一点的一种可靠方法是，通过构造来构建网络的最终输出函数，使其满足这些条件。\n\n考虑空间域 $x \\in [0, L]$ 上的一个一维问题。一个神经网络提供了一个原始的、无约束的输出函数，记为 $\\hat{u}_{NN}(x)$。我们希望使用这个网络来为一个满足以下非齐次狄利克雷边界条件的微分方程找到一个近似解 $u(x)$：\n$$u(0) = A$$\n$$u(L) = B$$\n这里，$A$、$B$ 和 $L > 0$ 是给定的实数常量。\n\n您的任务是设计一个变换，该变换接收原始网络输出 $\\hat{u}_{NN}(x)$ 并生成一个新函数 $u_{NN}(x)$，用作最终的近似解。无论网络生成的函数 $\\hat{u}_{NN}(x)$ 是什么，该变换都必须保证 $u_{NN}(x)$ 严格满足指定的边界条件。\n\n请提供一个用原始网络输出 $\\hat{u}_{NN}(x)$ 和参数 $x$、$L$、$A$ 及 $B$ 表示的 $u_{NN}(x)$ 的表达式。", "solution": "我们寻求一个变换，将原始网络输出 $\\hat{u}_{NN}(x)$ 映射到一个函数 $u_{NN}(x)$，该函数对任意 $\\hat{u}_{NN}(x)$ 都能强制满足狄利克雷边界条件 $u_{NN}(0)=A$ 和 $u_{NN}(L)=B$。一种标准的构造方法是将 $u_{NN}(x)$ 分解为\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\n其中 $g(x)$ 是任何满足边界条件的固定函数，而 $s(x)$ 是任何在两个边界上都为零的函数。具体来说，我们要求\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\n对于 $g(x)$，一个方便的选择是线性插值，\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\n以及简单的消失因子\n$$\ns(x)=x(L-x),\n$$\n它满足 $s(0)=0$ 和 $s(L)=0$。因此，定义\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\n为了验证边界条件，在 $x=0$ 和 $x=L$ 处求值：\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot L\\,\\hat{u}_{NN}(0)=A,\n$$\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B.\n$$\n因此，对于任何 $\\hat{u}_{NN}(x)$，构造出的 $u_{NN}(x)$ 都严格满足 $u_{NN}(0)=A$ 和 $u_{NN}(L)=B$。", "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$", "id": "2126300"}, {"introduction": "现在，我们将前述概念应用于固体力学的核心领域。这个实践要求你为二维线弹性问题推导物理残差的具体形式，这是将连续介质力学理论与PINN实践联系起来的关键一步。你将从弹性力学的基本控制方程出发——即应变-位移关系、本构关系（胡克定律）和动量平衡方程——并将它们组合成PINN需要最小化的最终残差表达式，即纳维-柯西（Navier-Cauchy）方程 [@problem_id:2668927]。这个过程对于任何希望将PINN应用于固体力学问题的研究者而言都至关重要，它填补了抽象的张量方程与训练网络所需的具体计算表达式之间的鸿沟。", "problem": "考虑一个二维、小应变的线性弹性固体，在准静态环境下，单位体积的体力为 $\\mathbf{b} = (b_x, b_y)$。令位移场为 $\\mathbf{u}(x,y) = (u_x(x,y), u_y(x,y))$。仅从线性动量守恒、小应变运动学关系以及用 Lamé 参数 $(\\lambda, \\mu)$ 表示的线性各向同性本构律出发，完成以下任务：\n\n1) 写出 Cauchy 应力张量 $\\boldsymbol{\\sigma}$ 关于应变张量 $\\boldsymbol{\\varepsilon}$ 和 $(\\lambda,\\mu)$ 的本构律，并根据位移场定义小应变张量。所有物理量都用笛卡尔坐标表示。\n\n2) 使用您的定义，将物理信息神经网络（PINN）在内部配置点上强制执行的内部平衡残差分量，即 $\\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b}$ 的两个分量，用 $u_x$ 和 $u_y$ 的空间导数以及 Lamé 参数 $(\\lambda,\\mu)$ 在笛卡尔坐标系中显式展开。\n\n3) 现在考虑位移场的一个单隐藏神经元神经网络拟设，其激活函数为双曲正切函数，\n$$\nz(x,y) = w_1 x + w_2 y + b_1,\\quad \\mathbf{u}(x,y) = \\mathbf{W}_2 \\,\\tanh\\!\\big(z(x,y)\\big) + \\mathbf{b}_2,\n$$\n其参数为 $\\mathbf{W}_2 \\in \\mathbb{R}^{2 \\times 1}$ 和 $\\mathbf{b}_2 \\in \\mathbb{R}^{2}$。取特定参数\n$$\nw_1 = 1,\\quad w_2 = 2,\\quad b_1 = 0,\\quad \\mathbf{W}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix},\\quad \\mathbf{b}_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n$$\n并假设体力为零 $\\mathbf{b}=\\mathbf{0}$。在点 $(x_0,y_0)=(1,0)$ 处，计算内部平衡残差向量的两个分量。\n\n假设采用无量纲化公式，因此所有量均为无量纲。将最终结果表示为包含两个残差分量的单个行向量，并写成闭式解析表达式。不要对任何常数进行近似或四舍五入；保持双曲函数的原始形式（例如，明确写出 $\\tanh(1)$、$\\cosh(1)$）。", "solution": "该问题经验证是适定的、有科学依据的，并包含获得唯一解所需的所有信息。我们开始进行推导。\n\n该问题要求从连续介质力学的基本原理出发，进行逐步推导，以计算给定神经网络位移场拟设的平衡残差。\n\n首先，我们解决任务1：在笛卡尔坐标系中定义相关的运动学关系和本构关系。\n\n在坐标为 $(x,y)$ 的二维域中，位移场为 $\\mathbf{u}(x,y) = (u_x(x,y), u_y(x,y))$。小应变（或无穷小应变）张量 $\\boldsymbol{\\varepsilon}$ 通过位移场的梯度定义如下：\n$$\n\\boldsymbol{\\varepsilon} = \\frac{1}{2} \\left[ \\nabla \\mathbf{u} + (\\nabla \\mathbf{u})^T \\right]\n$$\n其笛卡尔分量形式为：\n$$\n\\varepsilon_{xx} = \\frac{\\partial u_x}{\\partial x}, \\quad \\varepsilon_{yy} = \\frac{\\partial u_y}{\\partial y}, \\quad \\varepsilon_{xy} = \\varepsilon_{yx} = \\frac{1}{2}\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right)\n$$\n应变张量的迹代表体积应变，即 $\\text{tr}(\\boldsymbol{\\varepsilon}) = \\varepsilon_{kk} = \\varepsilon_{xx} + \\varepsilon_{yy}$。\n\n对于线性、各向同性的弹性材料，联系 Cauchy 应力张量 $\\boldsymbol{\\sigma}$ 与应变张量 $\\boldsymbol{\\varepsilon}$ 的本构律由胡克定律（Hooke's Law）给出，该定律可以用 Lamé 参数 $\\lambda$ 和 $\\mu$ （也称为第一和第二 Lamé 参数，其中 $\\mu$ 是剪切模量）表示：\n$$\n\\boldsymbol{\\sigma} = \\lambda \\, \\text{tr}(\\boldsymbol{\\varepsilon}) \\, \\mathbf{I} + 2\\mu \\, \\boldsymbol{\\varepsilon}\n$$\n其中 $\\mathbf{I}$ 是二阶单位张量。在笛卡尔分量形式中，应力分量为：\n$$\n\\sigma_{xx} = \\lambda(\\varepsilon_{xx} + \\varepsilon_{yy}) + 2\\mu \\varepsilon_{xx} = (\\lambda + 2\\mu)\\frac{\\partial u_x}{\\partial x} + \\lambda\\frac{\\partial u_y}{\\partial y}\n$$\n$$\n\\sigma_{yy} = \\lambda(\\varepsilon_{xx} + \\varepsilon_{yy}) + 2\\mu \\varepsilon_{yy} = \\lambda\\frac{\\partial u_x}{\\partial x} + (\\lambda + 2\\mu)\\frac{\\partial u_y}{\\partial y}\n$$\n$$\n\\sigma_{xy} = \\sigma_{yx} = 2\\mu \\varepsilon_{xy} = \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right)\n$$\n\n接下来，我们解决任务2：推导内部平衡残差分量的显式形式。准静态环境下的线性动量守恒即为平衡方程 $\\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b} = \\mathbf{0}$。物理信息神经网络（PINN）旨在最小化此方程的残差，即 $\\mathbf{R} = \\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b}$。该残差向量在笛卡尔坐标系中的分量为：\n$$\nR_x = \\frac{\\partial \\sigma_{xx}}{\\partial x} + \\frac{\\partial \\sigma_{xy}}{\\partial y} + b_x\n$$\n$$\nR_y = \\frac{\\partial \\sigma_{yx}}{\\partial x} + \\frac{\\partial \\sigma_{yy}}{\\partial y} + b_y\n$$\n将用位移导数表示的应力分量表达式代入，得到 Navier-Cauchy 方程。我们假设材料是均匀的，因此 $\\lambda$ 和 $\\mu$ 是常数。\n对于残差的 $x$ 分量：\n$$\nR_x = \\frac{\\partial}{\\partial x} \\left( (\\lambda + 2\\mu)\\frac{\\partial u_x}{\\partial x} + \\lambda\\frac{\\partial u_y}{\\partial y} \\right) + \\frac{\\partial}{\\partial y} \\left( \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right) \\right) + b_x\n$$\n$$\nR_x = (\\lambda + 2\\mu)\\frac{\\partial^2 u_x}{\\partial x^2} + \\lambda\\frac{\\partial^2 u_y}{\\partial x \\partial y} + \\mu\\frac{\\partial^2 u_x}{\\partial y^2} + \\mu\\frac{\\partial^2 u_y}{\\partial y \\partial x} + b_x\n$$\n根据混合偏导数相等（$\\frac{\\partial^2 u_y}{\\partial x \\partial y} = \\frac{\\partial^2 u_y}{\\partial y \\partial x}$），我们可以对各项进行分组：\n$$\nR_x = (\\lambda+2\\mu)\\frac{\\partial^2 u_x}{\\partial x^2} + \\mu \\frac{\\partial^2 u_x}{\\partial y^2} + (\\lambda+\\mu) \\frac{\\partial^2 u_y}{\\partial x \\partial y} + b_x\n$$\n对于残差的 $y$ 分量：\n$$\nR_y = \\frac{\\partial}{\\partial x} \\left( \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right) \\right) + \\frac{\\partial}{\\partial y} \\left( \\lambda\\frac{\\partial u_x}{\\partial x} + (\\lambda + 2\\mu)\\frac{\\partial u_y}{\\partial y} \\right) + b_y\n$$\n$$\nR_y = \\mu\\frac{\\partial^2 u_x}{\\partial x \\partial y} + \\mu\\frac{\\partial^2 u_y}{\\partial x^2} + \\lambda\\frac{\\partial^2 u_x}{\\partial y \\partial x} + (\\lambda + 2\\mu)\\frac{\\partial^2 u_y}{\\partial y^2} + b_y\n$$\n再次分组：\n$$\nR_y = (\\lambda+\\mu) \\frac{\\partial^2 u_x}{\\partial x \\partial y} + \\mu \\frac{\\partial^2 u_y}{\\partial x^2} + (\\lambda+2\\mu) \\frac{\\partial^2 u_y}{\\partial y^2} + b_y\n$$\n这些就是 PINN 将强制执行的残差分量的显式表达式。\n\n最后，我们解决任务3：根据指定的神经网络拟设和参数计算这些残差。该拟设由下式给出：\n$z(x,y) = w_1 x + w_2 y + b_1$ 和 $\\mathbf{u}(x,y) = \\mathbf{W}_2 \\tanh(z(x,y)) + \\mathbf{b}_2$。\n根据给定参数 $w_1 = 1$, $w_2 = 2$, $b_1 = 0$, $\\mathbf{W}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$ 和 $\\mathbf{b}_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，我们有：\n$z(x,y) = x + 2y$\n位移分量为：\n$$\nu_x(x,y) = 2 \\tanh(x+2y)\n$$\n$$\nu_y(x,y) = -1 \\tanh(x+2y)\n$$\n我们必须计算 $u_x$ 和 $u_y$ 的二阶偏导数。令 $T(x,y) = \\tanh(x+2y)$ 且 $S(x,y) = \\text{sech}^2(x+2y) = 1-\\tanh^2(x+2y)$。我们使用链式法则和导数恒等式 $\\frac{d}{d\\alpha}\\tanh(\\alpha) = \\text{sech}^2(\\alpha)$ 以及 $\\frac{d}{d\\alpha}\\text{sech}^2(\\alpha) = -2\\tanh(\\alpha)\\text{sech}^2(\\alpha)$。\n\n$u_x = 2T$ 的导数：\n$\\frac{\\partial u_x}{\\partial x} = 2 \\frac{\\partial T}{\\partial x} = 2S \\cdot 1 = 2S$\n$\\frac{\\partial u_x}{\\partial y} = 2 \\frac{\\partial T}{\\partial y} = 2S \\cdot 2 = 4S$\n$\\frac{\\partial^2 u_x}{\\partial x^2} = \\frac{\\partial}{\\partial x}(2S) = 2(-2TS) \\cdot 1 = -4TS$\n$\\frac{\\partial^2 u_x}{\\partial y^2} = \\frac{\\partial}{\\partial y}(4S) = 4(-2TS) \\cdot 2 = -16TS$\n$\\frac{\\partial^2 u_x}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(2S) = 2(-2TS) \\cdot 2 = -8TS$\n\n$u_y = -T$ 的导数：\n$\\frac{\\partial u_y}{\\partial x} = -S \\cdot 1 = -S$\n$\\frac{\\partial u_y}{\\partial y} = -S \\cdot 2 = -2S$\n$\\frac{\\partial^2 u_y}{\\partial x^2} = \\frac{\\partial}{\\partial x}(-S) = -(-2TS) \\cdot 1 = 2TS$\n$\\frac{\\partial^2 u_y}{\\partial y^2} = \\frac{\\partial}{\\partial y}(-2S) = -2(-2TS) \\cdot 2 = 8TS$\n$\\frac{\\partial^2 u_y}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(-S) = -(-2TS) \\cdot 2 = 4TS$\n\n给定体力为零，因此 $\\mathbf{b}=\\mathbf{0}$。我们将这些导数代入残差表达式中：\n$$\nR_x = (\\lambda+2\\mu)(-4TS) + \\mu(-16TS) + (\\lambda+\\mu)(4TS)\n$$\n$$\nR_x = [-4(\\lambda+2\\mu) - 16\\mu + 4(\\lambda+\\mu)]TS = [-4\\lambda - 8\\mu - 16\\mu + 4\\lambda + 4\\mu]TS = -20\\mu TS\n$$\n$$\nR_y = (\\lambda+\\mu)(-8TS) + \\mu(2TS) + (\\lambda+2\\mu)(8TS)\n$$\n$$\nR_y = [-8(\\lambda+\\mu) + 2\\mu + 8(\\lambda+2\\mu)]TS = [-8\\lambda - 8\\mu + 2\\mu + 8\\lambda + 16\\mu]TS = 10\\mu TS\n$$\n残差向量为 $\\mathbf{R}(x,y) = \\begin{pmatrix} -20\\mu TS & 10\\mu TS \\end{pmatrix}$。\n\n我们必须在点 $(x_0, y_0) = (1, 0)$ 处计算该值。在该点，双曲函数的参数为 $z_0 = 1 + 2(0) = 1$。\n项 $TS$ 变为 $\\tanh(1)\\text{sech}^2(1)$。使用恒等式 $\\text{sech}^2(\\alpha) = 1/\\cosh^2(\\alpha)$，此项为 $\\frac{\\tanh(1)}{\\cosh^2(1)}$。\n在 $(1, 0)$ 处的残差分量为：\n$$\nR_x(1,0) = -20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n$$\n$$\nR_y(1,0) = 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n$$\n该问题要求将结果表示为单个行向量。\n$$\n\\mathbf{R}(1,0) = \\begin{pmatrix} -20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} & 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} \\end{pmatrix}\n$$\n这是最终的解析表达式。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} & 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n\\end{pmatrix}\n}\n$$", "id": "2668927"}]}