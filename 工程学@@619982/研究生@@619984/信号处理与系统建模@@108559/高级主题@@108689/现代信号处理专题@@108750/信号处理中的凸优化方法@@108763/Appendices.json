{"hands_on_practices": [{"introduction": "最小绝对收缩和选择算子 (LASSO) 是现代信号处理中为线性逆问题寻找稀疏解的基石。其强大的功能源于 $\\ell_1$ 范数正则化项，但这也带来了一个挑战：目标函数是不可微的。本实践将引导你探索解决 LASSO 问题最有效的方法之一——坐标下降法 (Coordinate Descent)，通过该方法，一个复杂的多变量问题被分解为一系列简单的一维最小化子问题，而每个子问题都有一个称为软阈值化 (soft-thresholding) 的封闭解。[@problem_id:2861565]", "problem": "您的任务是利用凸优化和信号处理的原理，为最小绝对值收敛和选择算子 (LASSO) 问题推导并实现一个循环坐标下降算法。\n\n对于一个设计矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和观测向量 $b \\in \\mathbb{R}^{m}$，考虑 LASSO 目标函数：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 是一个给定的正则化参数，$\\|\\cdot\\|_1$ 表示 $\\ell_1$ 范数。\n\n您的任务是：\n1. 从第一性原理出发，为 $f(x)$ 推导循环坐标下降法中使用的单坐标最小化更新规则。从 $f(x)$ 的定义和 $\\ell_1$ 范数的次梯度最优性条件开始，在保持所有其他坐标固定的情况下，对 $f(x)$ 关于单个坐标 $x_i$ 的最小化进行推理。仅使用基础事实，包括凸函数的性质、绝对值的次梯度以及基础线性代数。不要先验地假设任何特定的闭式更新。\n2. 证明坐标级最小化器是通过将软阈值算子应用于当前迭代点和残差的仿射函数得到的。清晰地定义您推导过程中引入的所有量。\n3. 实现一个使用所推导更新规则的循环坐标下降算法。您的实现必须：\n   - 维护残差 $r \\triangleq b - A x$，并在每次坐标更新后增量式地更新它，以实现每次坐标更新 $\\mathcal{O}(m)$ 的成本。\n   - 使用由 $S_{\\tau}(z) \\triangleq \\mathrm{sign}(z)\\max(|z| - \\tau, 0)$ 定义的软阈值算子。\n   - 当在一次完整循环中任何坐标的最大绝对变化小于容差 $\\varepsilon$ 或达到最大轮次数时，算法终止。\n   - 返回最终的迭代点 $x$，并在请求时返回每轮结束时的目标函数值序列，以评估单调性。\n\n您可以使用的基础知识：\n- $\\|\\cdot\\|_2^2$ 和 $\\|\\cdot\\|_1$ 的凸性及其次梯度的性质。\n- 次梯度最优性条件：对于凸函数 $f$ 的一个最优解 $x^\\star$，有 $0 \\in \\partial f(x^\\star)$。\n- 绝对值的次微分：对于 $t \\in \\mathbb{R}$，如果 $t \\ne 0$，则 $\\partial |t| = \\{\\mathrm{sign}(t)\\}$；如果 $t = 0$，则 $\\partial |t| = [-1,1]$。\n- 用于残差更新的线性代数恒等式。\n\n将目标函数值定义为：\n$$\nf(x) = \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1.\n$$\n\n测试套件：\n实现您的程序以运行以下五个测试用例，并将结果汇总到单行输出中。\n\n- 测试 1 (标准正交列，解析解验证)：设置 $A = I_4$, $b = [3,-1,0.2,-0.5]^\\top$, $\\lambda = 0.7$。运行您的坐标下降算法得到 $x_{\\mathrm{cd}}$。对于标准正交列，已知的解析解是 $x^\\star = S_{\\lambda}(A^\\top b) = S_{\\lambda}(b)$。输出标量\n  $$\n  e_1 \\triangleq \\|x_{\\mathrm{cd}} - x^\\star\\|_{\\infty}.\n  $$\n\n- 测试 2 (一般高矩阵系统，Karush–Kuhn–Tucker (KKT) 条件验证)：生成一个 $A \\in \\mathbb{R}^{60 \\times 30}$ 矩阵，其元素为独立的标准正态分布，然后将每列归一化为单位 $\\ell_2$ 范数。使用固定的伪随机种子 $0$ 以使实例确定。定义 $x_{\\mathrm{true}} \\in \\mathbb{R}^{30}$，其在索引 $0,5,10,15,20$ 处的非零元素值分别为 $[2.5,-1.7,1.2,-0.9,1.8]$，其他位置为零。设 $b = A x_{\\mathrm{true}} + \\eta$，其中 $\\eta \\in \\mathbb{R}^{60}$ 的元素是使用相同种子 $0$ 生成的、标准差为 $0.01$ 的独立正态分布。设 $\\lambda = 0.05$。运行坐标下降得到 $x_{\\mathrm{cd}}$。验证 LASSO 的 KKT 条件：令 $g \\triangleq A^\\top(A x_{\\mathrm{cd}} - b)$，\n  - 如果 $x_{\\mathrm{cd},i} \\ne 0$，则 $g_i + \\lambda \\,\\mathrm{sign}(x_{\\mathrm{cd},i}) = 0$。\n  - 如果 $x_{\\mathrm{cd},i} = 0$，则 $|g_i| \\le \\lambda$。\n  由于数值误差，在这些检查中实施一个 $10^{-4}$ 的容差。输出布尔值 $b_2$，表示所有坐标是否在容差范围内满足 KKT 条件。\n\n- 测试 3 (大正则化参数将解驱动至零)：使用 $A = I_4$, $b = [3,-1,0.2,-0.5]^\\top$, 以及 $\\lambda = 10^6$。输出布尔值 $b_3$，表示返回的解是否在 $10^{-12}$ 的绝对容差内为零向量。\n\n- 测试 4 (零正则化参数简化为最小二乘问题)：使用伪随机种子 $1$ 生成 $A \\in \\mathbb{R}^{40 \\times 10}$，其元素为独立的标准正态分布。使用种子 $2$ 生成 $b \\in \\mathbb{R}^{40}$，其元素为独立的标准正态分布。设 $\\lambda = 0$。令 $x_{\\mathrm{ls}}$ 表示通过标准线性最小二乘法计算的、最小化 $\\frac{1}{2}\\|A x - b\\|_2^2$ 的最小二乘解。运行坐标下降得到 $x_{\\mathrm{cd}}$。输出标量\n  $$\n  e_4 \\triangleq \\frac{\\|x_{\\mathrm{cd}} - x_{\\mathrm{ls}}\\|_2}{\\max(\\|x_{\\mathrm{ls}}\\|_2, 10^{-12})}.\n  $$\n\n- 测试 5 (目标函数值随轮次单调下降)：使用伪随机种子 $3$ 生成 $A \\in \\mathbb{R}^{30 \\times 15}$ 和 $b \\in \\mathbb{R}^{30}$，其元素为独立的标准正态分布。设 $\\lambda = 0.1$。记录每次完整遍历所有坐标后的目标函数值，并验证该序列在数值容差 $10^{-10}$ 内是单调非增的。输出布尔值 $b_5$，表示单调性是否成立。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，顺序为 $[e_1, b_2, b_3, e_4, b_5]$。此问题不涉及物理单位，也与角度单位无关。所有数值输出应为指定的实数或布尔值，不带百分号。您的实现必须对给定的实例具有鲁棒性，并且不应需要任何用户输入。", "solution": "我们从凸优化问题开始\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，且 $\\lambda \\ge 0$。函数 $f$ 是凸函数，因为它是凸函数 $\\frac{1}{2}\\|A x - b\\|_2^2$ 和凸函数 $\\lambda \\|x\\|_1$ 的和。\n\n循环坐标下降法一次最小化一个坐标上的 $f$ 值，同时保持其他坐标固定。固定一个索引 $i \\in \\{1,\\dots,n\\}$，并用 $a_i \\in \\mathbb{R}^m$ 表示 $A$ 的第 $i$ 列。设 $x \\in \\mathbb{R}^n$ 为当前迭代点，并定义残差\n$$\nr \\triangleq b - A x.\n$$\n因为 $A x = \\sum_{j=1}^n a_j x_j$，只将 $x_i$ 更改为一个新值 $t \\in \\mathbb{R}$ 会得到一个新的向量 $x^{(i \\leftarrow t)}$ 和残差\n$$\nr^{(i \\leftarrow t)} = b - A x^{(i \\leftarrow t)} = b - \\left(A x + a_i (t - x_i)\\right) = r - a_i (t - x_i).\n$$\n将目标函数视为 $t$ 的函数（其他坐标固定）变为\n\\begin{align*}\n\\phi_i(t) &\\triangleq \\frac{1}{2}\\|A x^{(i \\leftarrow t)} - b\\|_2^2 + \\lambda \\left(\\sum_{j \\ne i} |x_j| + |t|\\right) \\\\\n&= \\frac{1}{2}\\|r^{(i \\leftarrow t)}\\|_2^2 + \\lambda |t| + \\text{与 } t \\text{ 无关的常数} \\\\\n&= \\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2 + \\lambda |t| + \\text{常数}.\n\\end{align*}\n使用 $\\|u - v\\|_2^2 = \\|u\\|_2^2 - 2 u^\\top v + \\|v\\|_2^2$ 展开平方范数，我们得到\n\\begin{align*}\n\\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2\n&= \\frac{1}{2}\\|r\\|_2^2 - (t - x_i) a_i^\\top r + \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2.\n\\end{align*}\n舍去与 $t$ 无关的项，坐标级目标函数简化为一元凸函数\n$$\n\\tilde{\\phi}_i(t) \\triangleq \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t|.\n$$\n通过配方法，定义 $d_i \\triangleq \\|a_i\\|_2^2$ 和\n$$\nc_i \\triangleq x_i + \\frac{a_i^\\top r}{d_i} \\quad \\text{当 } d_i > 0.\n$$\n则\n\\begin{align*}\n\\tilde{\\phi}_i(t)\n&= \\frac{1}{2} d_i (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t| \\\\\n&= \\frac{1}{2} d_i \\left(t - x_i - \\frac{a_i^\\top r}{d_i}\\right)^2 - \\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i} + \\lambda |t|.\n\\end{align*}\n忽略常数 $-\\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i}$，对 $\\tilde{\\phi}_i(t)$ 关于 $t$ 的最小化等价于最小化\n$$\n\\psi_i(t) \\triangleq \\frac{1}{2} d_i (t - c_i)^2 + \\lambda |t|.\n$$\n这个一维凸问题的次梯度最优性条件是\n$$\n0 \\in \\partial \\psi_i(t^\\star) = d_i (t^\\star - c_i) + \\lambda \\,\\partial |t^\\star|,\n$$\n其中绝对值的次微分为 $\\partial |t^\\star| = \\{\\mathrm{sign}(t^\\star)\\}$ 如果 $t^\\star \\ne 0$，以及 $\\partial |t^\\star| = [-1, 1]$ 如果 $t^\\star = 0$。\n\n考虑两种情况。\n\n情况 1：$t^\\star \\ne 0$。那么次梯度条件是\n$$\nd_i (t^\\star - c_i) + \\lambda \\,\\mathrm{sign}(t^\\star) = 0 \\;\\;\\Longleftrightarrow\\;\\; t^\\star = c_i - \\frac{\\lambda}{d_i} \\,\\mathrm{sign}(t^\\star).\n$$\n这意味着 $|c_i| > \\lambda/d_i$，解是通过将 $c_i$ 向零收缩 $\\lambda/d_i$ 同时保持符号得到的：\n$$\nt^\\star = \\mathrm{sign}(c_i)\\left(|c_i| - \\frac{\\lambda}{d_i}\\right).\n$$\n\n情况 2：$t^\\star = 0$。那么次梯度条件变为\n$$\n0 \\in - d_i c_i + \\lambda [-1,1] \\;\\;\\Longleftrightarrow\\;\\; |d_i c_i| \\le \\lambda \\;\\;\\Longleftrightarrow\\;\\; |c_i| \\le \\frac{\\lambda}{d_i}.\n$$\n结合这两种情况，我们得到软阈值形式\n$$\nt^\\star = S_{\\lambda/d_i}(c_i) \\triangleq \\mathrm{sign}(c_i)\\max\\left(|c_i| - \\frac{\\lambda}{d_i}, \\, 0 \\right).\n$$\n等价地，使用残差的定义 $r = b - A x$，我们有\n$$\nc_i = x_i + \\frac{a_i^\\top r}{d_i} = \\frac{a_i^\\top r + d_i x_i}{d_i},\n$$\n所以坐标级最小化器是\n$$\nx_i \\leftarrow S_{\\lambda/\\|a_i\\|_2^2}\\!\\left(\\frac{a_i^\\top r + \\|a_i\\|_2^2 x_i}{\\|a_i\\|_2^2}\\right).\n$$\n如果 $d_i = \\|a_i\\|_2^2 = 0$（一个零列），$x_i$ 的任何变化都不会影响二次项；对于 $\\lambda > 0$，$\\lambda |t|$ 的最小化器是 $t^\\star = 0$。在我们的实现中，如果 $d_i = 0$ 且 $\\lambda > 0$，我们设置 $x_i \\leftarrow 0$；如果 $\\lambda = 0$ 且 $d_i = 0$，该坐标无关紧要，可以保持不变。\n\n高效的残差更新：如果 $\\Delta_i \\triangleq x_i^{\\text{new}} - x_i^{\\text{old}}$，那么\n$$\nr^{\\text{new}} = b - A x^{\\text{new}} = b - \\left(A x^{\\text{old}} + a_i \\Delta_i\\right) = r^{\\text{old}} - a_i \\Delta_i,\n$$\n这需要 $\\mathcal{O}(m)$ 次操作。\n\n收敛性和单调性：每个坐标更新都会精确地在该坐标上最小化 $f$，因此 $f$ 在每次坐标更新后是单调非增的，从而在每轮（完整遍历所有坐标）后也是如此。当在一轮中最大的坐标绝对变化低于容差或达到最大轮次数时，算法终止。\n\n通过 Karush–Kuhn–Tucker (KKT) 条件进行最优性验证：令 $g(x) \\triangleq A^\\top (A x - b)$ 为光滑部分的梯度。LASSO 问题中 $x^\\star$ 最优性的 KKT 条件是\n$$\n0 \\in g(x^\\star) + \\lambda \\,\\partial \\|x^\\star\\|_1,\n$$\n这等价于分量级条件\n$$\n\\begin{cases}\ng_i(x^\\star) + \\lambda \\,\\mathrm{sign}(x_i^\\star) = 0, & \\text{如果 } x_i^\\star \\ne 0, \\\\\n|g_i(x^\\star)| \\le \\lambda, & \\text{如果 } x_i^\\star = 0.\n\\end{cases}\n$$\n在实践中，我们在一个小的数值容差范围内检查这些等式和不等式。\n\n测试用例和输出：我们实现了指定的五个测试用例并计算\n- $e_1 = \\|x_{\\mathrm{cd}} - S_{\\lambda}(b)\\|_\\infty$ 用于标准正交列，\n- $b_2$ 指示 KKT 条件在高矩阵系统中是否在容差内满足，\n- $b_3$ 指示对于非常大的 $\\lambda$ 解是否为零，\n- $e_4$ 当 $\\lambda = 0$ 时与最小二乘解的相对误差，\n- $b_5$ 指示目标函数值在各轮次是否单调非增。\n\n最终程序将结果作为单个列表 $[e_1, b_2, b_3, e_4, b_5]$ 输出在一行上。", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S_tau(z) = sign(z) * max(|z| - tau, 0).\"\"\"\n    if tau <= 0:\n        return z\n    abs_z = abs(z)\n    if abs_z <= tau:\n        return 0.0\n    return np.copysign(abs_z - tau, z)\n\ndef objective_value(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float) -> float:\n    r = A @ x - b\n    return 0.5 * float(r.T @ r) + lambd * float(np.linalg.norm(x, 1))\n\ndef coordinate_descent_lasso(\n    A: np.ndarray,\n    b: np.ndarray,\n    lambd: float,\n    max_epochs: int = 2000,\n    tol: float = 1e-8,\n    record_objective: bool = False\n):\n    \"\"\"\n    Cyclic coordinate descent for LASSO:\n        minimize 0.5 * ||A x - b||_2^2 + lambd * ||x||_1.\n    Uses residual updates for O(m) per coordinate.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n, dtype=float)\n    r = b.copy()  # r = b - A x, initially x=0\n    obj_hist = []\n\n    # Precompute squared column norms d_i = ||a_i||_2^2\n    col_sq_norms = np.sum(A * A, axis=0)\n\n    for epoch in range(max_epochs):\n        max_delta = 0.0\n        for i in range(n):\n            ai = A[:, i]\n            di = col_sq_norms[i]\n            xi_old = x[i]\n\n            if di == 0.0:\n                # If the column is zero, the quadratic does not depend on x_i.\n                # Minimizer of lambd * |t| is t=0 for lambd>0; do nothing if lambd==0.\n                xi_new = 0.0 if lambd > 0 else xi_old\n            else:\n                # c_i = x_i + (a_i^T r) / d_i = (a_i^T r + d_i x_i) / d_i\n                ci = xi_old + float(ai.T @ r) / di\n                # Update via soft-thresholding with threshold lambd / d_i\n                xi_new = soft_threshold(ci, lambd / di)\n\n            delta = xi_new - xi_old\n            if delta != 0.0:\n                # Update residual: r_new = r_old - a_i * delta\n                r -= ai * delta\n                x[i] = xi_new\n                max_delta = max(max_delta, abs(delta))\n\n        if record_objective:\n            obj_hist.append(objective_value(A, b, x, lambd))\n\n        if max_delta < tol:\n            break\n\n    if record_objective:\n        return x, obj_hist\n    return x\n\ndef kkt_satisfied(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float, tol: float = 1e-4) -> bool:\n    \"\"\"\n    Check KKT conditions for LASSO:\n      g = A^T (A x - b)\n      If x_i != 0: g_i + lambd * sign(x_i) = 0\n      If x_i == 0: |g_i| <= lambd\n    with numerical tolerance tol.\n    \"\"\"\n    g = A.T @ (A @ x - b)\n    for i in range(x.size):\n        xi = x[i]\n        gi = g[i]\n        if abs(xi) > 1e-12:\n            if abs(gi + lambd * np.sign(xi)) > tol:\n                return False\n        else:\n            if abs(gi) - lambd > tol:\n                return False\n    return True\n\ndef test_suite():\n    results = []\n\n    # Test 1: Orthonormal columns (A = I), analytical solution S_lambda(b)\n    A1 = np.eye(4)\n    b1 = np.array([3.0, -1.0, 0.2, -0.5])\n    lambd1 = 0.7\n    x1 = coordinate_descent_lasso(A1, b1, lambd1, max_epochs=100, tol=1e-12)\n    # Analytical solution: S_lambda(b)\n    x1_star = np.array([soft_threshold(b1[i], lambd1) for i in range(4)])\n    e1 = float(np.max(np.abs(x1 - x1_star)))\n    results.append(e1)\n\n    # Test 2: General tall system, KKT check\n    rng = np.random.default_rng(0)\n    A2 = rng.standard_normal((60, 30))\n    # Normalize columns to unit norm\n    col_norms = np.linalg.norm(A2, axis=0)\n    # Avoid division by zero in extremely unlikely all-zero columns\n    col_norms[col_norms == 0.0] = 1.0\n    A2 = A2 / col_norms\n    x_true = np.zeros(30)\n    nz_idx = [0, 5, 10, 15, 20]\n    nz_vals = [2.5, -1.7, 1.2, -0.9, 1.8]\n    for idx, val in zip(nz_idx, nz_vals):\n        x_true[idx] = val\n    noise = rng.normal(0.0, 0.01, size=60)\n    b2 = A2 @ x_true + noise\n    lambd2 = 0.05\n    x2 = coordinate_descent_lasso(A2, b2, lambd2, max_epochs=2000, tol=1e-10)\n    b2_ok = kkt_satisfied(A2, b2, x2, lambd2, tol=1e-4)\n    results.append(bool(b2_ok))\n\n    # Test 3: Large lambda -> zero solution\n    lambd3 = 1e6\n    x3 = coordinate_descent_lasso(A1, b1, lambd3, max_epochs=50, tol=1e-14)\n    b3_ok = bool(np.allclose(x3, 0.0, atol=1e-12))\n    results.append(b3_ok)\n\n    # Test 4: Zero lambda -> least squares\n    rng1 = np.random.default_rng(1)\n    A4 = rng1.standard_normal((40, 10))\n    rng2 = np.random.default_rng(2)\n    b4 = rng2.standard_normal(40)\n    lambd4 = 0.0\n    x4_cd = coordinate_descent_lasso(A4, b4, lambd4, max_epochs=5000, tol=1e-12)\n    # Least squares solution via numpy\n    x4_ls, *_ = np.linalg.lstsq(A4, b4, rcond=None)\n    denom = max(np.linalg.norm(x4_ls), 1e-12)\n    e4 = float(np.linalg.norm(x4_cd - x4_ls) / denom)\n    results.append(e4)\n\n    # Test 5: Monotone objective decrease across epochs\n    rng3 = np.random.default_rng(3)\n    A5 = rng3.standard_normal((30, 15))\n    b5 = rng3.standard_normal(30)\n    lambd5 = 0.1\n    x5, obj_hist = coordinate_descent_lasso(A5, b5, lambd5, max_epochs=200, tol=1e-10, record_objective=True)\n    # Check nonincreasing sequence within small tolerance\n    diffs = np.diff(obj_hist)\n    b5_ok = bool(np.all(diffs <= 1e-10))\n    results.append(b5_ok)\n\n    return results\n\ndef solve():\n    results = test_suite()\n    # Format booleans and floats in a single list\n    # Convert to string with Python default formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2861565"}, {"introduction": "在 LASSO 问题中遇到的软阈值算子，实际上是一个更通用、更强大的概念——邻近算子 (proximal operator) 的特例。本实践将从简单的稀疏性推广到结构化稀疏性，引入能促使整组系数同时为零的组 LASSO (group LASSO) 正则化项。你将从第一性原理出发，推导其对应的“块软阈值” (block soft-thresholding) 算子，这是诸如邻近梯度下降 (proximal gradient descent) 和交替方向乘子法 (ADMM) 等先进一阶优化算法的基本构件。[@problem_id:2861514]", "problem": "考虑一个信号处理中的线性反问题，其中我们寻求一个结构化稀疏估计 $x \\in \\mathbb{R}^{n}$。该向量根据 $\\{1,\\dots,n\\}$ 的一个固定索引划分，被分为 $G$ 个不重叠的组 $\\{x_{g}\\}_{g=1}^{G}$。设正则化项为加权组套索（group-lasso）罚项 $R(x) = \\sum_{g=1}^{G} w_{g} \\|x_{g}\\|_{2}$，其中权重 $w_{g} > 0$ 严格为正。在许多基于近端分裂（proximal splitting）的一阶凸优化方法中，需要重复计算 $R$ 的近端算子 (prox)。对于任意 $\\lambda > 0$ 和任意 $y \\in \\mathbb{R}^{n}$，其定义如下：\n$$\n\\operatorname{prox}_{\\lambda R}(y) \\triangleq \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} \\right\\}.\n$$\n从近端算子的定义和凸函数的次微分最优性条件出发，推导与 $R$ 相关联的块软阈值算子，并给出每个组 $g \\in \\{1,\\dots,G\\}$ 关于 $y_{g}$、$\\lambda$ 和 $w_{g}$ 的闭式表达式。你的最终答案必须是关于 $[\\operatorname{prox}_{\\lambda R}(y)]_{g}$ 的单一闭式解析表达式，且该表达式对所有 $y_{g} \\in \\mathbb{R}^{|g|}$ 和所有 $g \\in \\{1,\\dots,G\\}$ 都有效。不要给出一个不等式或一个待求解的方程；请提供显式表达式。不需要进行数值舍入。", "solution": "问题陈述已被解析和验证。该问题被认定为具有科学依据、适定（well-posed）、客观且自洽（self-contained）。这是信号处理领域中凸优化的一个标准问题。未发现任何瑕疵。可以开始推导解答。\n\n目标是找到加权组套索罚项 $R(x) = \\sum_{g=1}^{G} w_{g} \\|x_{g}\\|_{2}$ 的近端算子的闭式表达式。近端算子被定义为以下最小化问题的解 $z^*$：\n$$\nz^{*} = \\operatorname{prox}_{\\lambda R}(y) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} \\right\\}\n$$\n令目标函数为 $F(z) = \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda R(z)$。欧几里得范数的平方可以按不重叠的组进行分解：\n$$\n\\|z - y\\|_{2}^{2} = \\sum_{g=1}^{G} \\|z_g - y_g\\|_2^2\n$$\n将此代入目标函数，我们得到：\n$$\nF(z) = \\frac{1}{2} \\sum_{g=1}^{G} \\|z_g - y_g\\|_2^2 + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} = \\sum_{g=1}^{G} \\left( \\frac{1}{2} \\|z_g - y_g\\|_2^2 + \\lambda w_{g} \\|z_{g}\\|_{2} \\right)\n$$\n总目标函数 $F(z)$ 是一系列函数之和，其中每一项仅依赖于单一一组变量 $z_g$。这个性质被称为可分离性。因此，对 $z \\in \\mathbb{R}^n$ 最小化 $F(z)$ 的问题，可以通过对求和中的每一项就其对应的组变量 $z_g \\in \\mathbb{R}^{|g|}$ 独立进行最小化来解决。\n\n对于每个组 $g \\in \\{1,\\dots,G\\}$，相应块的解（记为 $z_g^* = [\\operatorname{prox}_{\\lambda R}(y)]_g$）由以下公式给出：\n$$\nz_g^* = \\arg\\min_{z_g \\in \\mathbb{R}^{|g|}} \\left\\{ J(z_g) \\triangleq \\frac{1}{2} \\|z_g - y_g\\|_2^2 + \\lambda w_{g} \\|z_{g}\\|_{2} \\right\\}\n$$\n函数 $J(z_g)$ 是凸函数，因为它是一个严格凸函数（二次项）和一个凸函数（$\\ell_2$-范数项）的和。因此，存在唯一的最小化点 $z_g^*$。凸函数的最优性条件表明，$z_g^*$ 是一个最小化点当且仅当零向量是 $J$ 在 $z_g^*$ 处的次微分（subdifferential）的元素，即 $0 \\in \\partial J(z_g^*)$。\n\n函数 $J(z_g)$ 由两项组成。第一项 $\\frac{1}{2}\\|z_g - y_g\\|_2^2$ 是可微的，其梯度为 $z_g - y_g$。第二项是 $\\lambda w_g \\|z_g\\|_2$。$\\ell_2$-范数 $\\|z_g\\|_2$ 的次微分是：\n$$\n\\partial \\|z_g\\|_2 =\n\\begin{cases}\n\\{ \\frac{z_g}{\\|z_g\\|_2} \\} & \\text{if } z_g \\neq 0 \\\\\n\\{ v \\in \\mathbb{R}^{|g|} \\mid \\|v\\|_2 \\le 1 \\} & \\text{if } z_g = 0\n\\end{cases}\n$$\n根据次微分的求和法则（因为其中一个函数是可微的），我们有 $\\partial J(z_g) = (z_g - y_g) + \\lambda w_g \\partial \\|z_g\\|_2$。最优性条件 $0 \\in \\partial J(z_g^*)$ 变为：\n$$\n0 \\in (z_g^* - y_g) + \\lambda w_g \\partial \\|z_g^*\\|_2\n$$\n这可以被重写为：\n$$\ny_g - z_g^* \\in \\lambda w_g \\partial \\|z_g^*\\|_2\n$$\n我们现在根据 $z_g^*$ 的值分两种情况进行分析。\n\n情况1：$z_g^* \\neq 0$。\n在这种情况下，次微分 $\\partial \\|z_g^*\\|_2$ 是单点集 $\\{ \\frac{z_g^*}{\\|z_g^*\\|_2} \\}$。最优性条件变为一个等式：\n$$\ny_g - z_g^* = \\lambda w_g \\frac{z_g^*}{\\|z_g^*\\|_2}\n$$\n重新整理各项以求解 $y_g$：\n$$\ny_g = z_g^* + \\lambda w_g \\frac{z_g^*}{\\|z_g^*\\|_2} = z_g^* \\left( 1 + \\frac{\\lambda w_g}{\\|z_g^*\\|_2} \\right)\n$$\n从这个方程可以看出，$y_g$ 是 $z_g^*$ 的一个正向缩放。这意味着 $z_g^*$ 必须与 $y_g$ 共线且指向同一方向。因此，我们有 $\\frac{z_g^*}{\\|z_g^*\\|_2} = \\frac{y_g}{\\|y_g\\|_2}$。注意这要求 $y_g \\neq 0$。将此代回关于 $y_g - z_g^*$ 的方程中：\n$$\nz_g^* = y_g - \\lambda w_g \\frac{y_g}{\\|y_g\\|_2} = \\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g\n$$\n因为我们假设 $z_g^* \\neq 0$，所以其范数必须为正，即 $\\|z_g^*\\|_2 > 0$。对 $z_g^*$ 的表达式取范数：\n$$\n\\|z_g^*\\|_2 = \\left\\| \\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g \\right\\|_2 = \\left| 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right| \\|y_g\\|_2\n$$\n由于 $z_g^*$ 和 $y_g$ 方向相同，标量因子必须为正：$1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} > 0$。这意味着 $\\|y_g\\|_2 > \\lambda w_g$。如果此条件成立，则我们关于 $z_g^* \\neq 0$ 的假设是一致的，解也如推导所示。\n\n情况2：$z_g^* = 0$。\n在这种情况下，次微分是闭单位球：$\\partial \\|z_g^*\\|_2 = \\{ v \\in \\mathbb{R}^{|g|} \\mid \\|v\\|_2 \\le 1 \\}$。最优性条件变为：\n$$\ny_g - 0 \\in \\lambda w_g \\{ v \\mid \\|v\\|_2 \\le 1 \\}\n$$\n这等价于说 $y_g$ 位于半径为 $\\lambda w_g$ 的球内：\n$$\n\\|y_g\\|_2 \\le \\lambda w_g\n$$\n如果满足此条件，则最小化点为 $z_g^* = 0$。\n\n结合这两种情况，我们得到：\n$$\nz_g^* = [\\operatorname{prox}_{\\lambda R}(y)]_g =\n\\begin{cases}\n\\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g & \\text{if } \\|y_g\\|_2 > \\lambda w_g \\\\\n0 & \\text{if } \\|y_g\\|_2 \\le \\lambda w_g\n\\end{cases}\n$$\n这个分段表达式可以使用最大值函数紧凑地写成一个单一的公式。收缩因子 $\\left(1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}\\right)$ 在0处被阈值化。\n$$\nz_g^* = \\max\\left\\{0, 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}\\right\\} y_g\n$$\n该表达式是块软阈值算子的闭式表示。它对所有 $y_g \\in \\mathbb{R}^{|g|}$ 都是良定义的（well-defined）。具体来说，若 $y_g = 0$，则 $\\|y_g\\|_2 = 0$。由于 $\\lambda w_g > 0$，项 $\\frac{\\lambda w_g}{\\|y_g\\|_2}$ 变为无穷大。max函数的参数 $1 - \\infty$ 的计算结果为 $-\\infty$。因此，$\\max\\{0, -\\infty\\} = 0$，结果为 $z_g^* = 0 \\cdot y_g = 0$，这与我们的推导是一致的。\n对于任意满足 $\\|y_g\\|_2 \\le \\lambda w_g$ 的非零 $y_g$，项 $1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}$ 是非正的，所以max函数返回0，导致 $z_g^*=0$。对于 $\\|y_g\\|_2 > \\lambda w_g$ 的情况，缩放因子是正的，该表达式得出正确的收缩结果。", "answer": "$$\n\\boxed{\\max\\left\\{0, 1 - \\frac{\\lambda w_{g}}{\\|y_{g}\\|_{2}}\\right\\} y_{g}}\n$$", "id": "2861514"}, {"introduction": "当一个算法给出了一个潜在解后，我们如何知道它距离真正的最优解有多近？本实践将通过凸对偶 (convex duality) 的视角来探讨这个关键问题。你将为 LASSO 问题推导其对偶问题，并学习如何利用原始-对偶间隙 (primal-dual gap) 为任意候选解的次优性提供一个可计算的、严格的量化指标。理解这个间隙对于为你的优化算法设计可靠的停止准则是至关重要的。[@problem_id:2861525]", "problem": "信号处理中一个常见的稀疏线性逆问题是最小绝对值收敛和选择算子（LASSO），它旨在寻求一个能够平衡数据保真度和稀疏性的系数向量。考虑以下凸优化问题\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\,\\|A x - b\\|_{2}^{2} + \\lambda \\,\\|x\\|_{1},\n$$\n其中数据矩阵\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\quad \\lambda = 1,\n$$\n以及一个候选原始点\n$$\nx = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\n另外给定一个候选对偶向量\n$$\ny = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix},\n$$\n以及针对对偶候选点的无穷范数可行性条件\n$$\n\\|A^{\\top} y\\|_{\\infty} \\le \\lambda.\n$$\n从拉格朗日函数、拉格朗日对偶函数，以及指示函数和范数函数的凸共轭等基本定义出发，完成以下任务，过程中不得调用任何预先记下的最终公式。\n\n1. 引入一个辅助变量 $u$ 来表示测量拟合，从定义 $g(y) = \\inf_{x,u} \\mathcal{L}(x,u,y)$ 出发，推导 LASSO 问题的对偶函数 $g(y)$，并根据 $\\|A^{\\top} y\\|_{\\infty}$ 确定相应的对偶可行集。\n\n2. 定义原始-对偶间隙\n$$\nG(x,y) \\triangleq \\left(\\frac{1}{2}\\,\\|A x - b\\|_{2}^{2} + \\lambda \\,\\|x\\|_{1}\\right) - g(y),\n$$\n并从弱对偶性的基本原理出发，解释为什么 $G(x,y)$ 是原始次优性 $\\left(\\frac{1}{2}\\,\\|A x^{\\star} - b\\|_{2}^{2} + \\lambda \\,\\|x^{\\star}\\|_{1}\\right) - \\left(\\frac{1}{2}\\,\\|A x^{\\star} - b\\|_{2}^{2} + \\lambda \\,\\|x^{\\star}\\|_{1}\\right)$ 的一个上界，其中 $x^{\\star}$ 是一个最优原始解。\n\n3. 验证所给定的 $y$ 满足对偶可行性条件 $\\|A^{\\top} y\\|_{\\infty} \\le \\lambda$。\n\n4. 对于给定的 $A$、$b$、$\\lambda$、$x$ 和 $y$，计算原始-对偶间隙 $G(x,y)$ 的精确值。以实数形式给出精确值，无需四舍五入。", "solution": "所给出的问题是 LASSO 优化问题的一个标准实例，这是信号处理和凸优化领域一个基础且适定的问题。该问题具有科学依据、自洽且客观。不存在任何缺陷，因此可以构建一个严谨的解。我们将按顺序处理四个指定的任务。\n\n首先，我们来推导拉格朗日对偶函数。原始问题为\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\,\\|A x - b\\|_{2}^{2} + \\lambda \\,\\|x\\|_{1}.\n$$\n按照指示，我们引入一个辅助变量 $u \\in \\mathbb{R}^{3}$，使得 $u = Ax - b$。该问题可以重写为等价的约束优化问题：\n$$\n\\min_{x \\in \\mathbb{R}^{2}, u \\in \\mathbb{R}^{3}} \\; \\frac{1}{2}\\,\\|u\\|_{2}^{2} + \\lambda \\,\\|x\\|_{1} \\quad \\text{subject to} \\quad Ax - u - b = 0.\n$$\n针对该问题，我们为等式约束引入一个对偶变量（拉格朗日乘子）$y \\in \\mathbb{R}^{3}$，从而构成拉格朗日函数 $\\mathcal{L}(x, u, y)$：\n$$\n\\mathcal{L}(x, u, y) = \\frac{1}{2}\\,\\|u\\|_{2}^{2} + \\lambda \\,\\|x\\|_{1} + y^{\\top}(Ax - u - b).\n$$\n拉格朗日对偶函数 $g(y)$ 定义为拉格朗日函数关于原始变量 $x$ 和 $u$ 的下确界：\n$$\ng(y) = \\inf_{x, u} \\mathcal{L}(x, u, y).\n$$\n为计算该下确界，我们可以重排拉格朗日函数中的项，以分离变量 $x$ 和 $u$：\n$$\n\\mathcal{L}(x, u, y) = (\\lambda \\,\\|x\\|_{1} + y^{\\top}Ax) + \\left(\\frac{1}{2}\\,\\|u\\|_{2}^{2} - y^{\\top}u\\right) - y^{\\top}b.\n$$\n下确界可以对包含 $x$ 和 $u$ 的项分别求取：\n$$\ng(y) = \\inf_{x} \\left(\\lambda \\,\\|x\\|_{1} + (A^{\\top}y)^{\\top}x\\right) + \\inf_{u} \\left(\\frac{1}{2}\\,\\|u\\|_{2}^{2} - y^{\\top}u\\right) - y^{\\top}b.\n$$\n第一项 $\\inf_{x} \\left(\\lambda \\,\\|x\\|_{1} + (A^{\\top}y)^{\\top}x\\right)$ 与函数 $f(x) = \\lambda \\|x\\|_{1}$ 的凸共轭有关。共轭函数定义为 $f^{*}(z) = \\sup_{x}(z^{\\top}x - f(x))$。我们表达式中的项是 $-\\sup_{x}(- (A^{\\top}y)^{\\top}x - \\lambda \\|x\\|_1) = -f^{*}(-A^{\\top}y)$。$\\lambda \\|x\\|_{1}$ 的共轭函数是按 $\\lambda$ 缩放的对偶范数球的指示函数。具体来说，如果 $\\|z\\|_{\\infty} \\le \\lambda$，则 $f^{*}(z)$ 为 $0$，否则为 $+\\infty$。因此，如果 $\\|-A^{\\top}y\\|_{\\infty} \\le \\lambda$（等价于 $\\|A^{\\top}y\\|_{\\infty} \\le \\lambda$）成立，关于 $x$ 的下确界为 $0$，否则为 $-\\infty$。为了使对偶函数非平凡（即不为 $-\\infty$），必须满足对偶可行性条件 $\\|A^{\\top}y\\|_{\\infty} \\le \\lambda$。\n\n第二项 $\\inf_{u} \\left(\\frac{1}{2}\\,\\|u\\|_{2}^{2} - y^{\\top}u\\right)$ 是关于 $u$ 的凸二次函数的最小值。当其关于 $u$ 的梯度为零时取得最小值：$\\nabla_{u}(\\frac{1}{2}\\,\\|u\\|_{2}^{2} - y^{\\top}u) = u - y = 0$，这意味着 $u = y$。将其代回可得最小值为：$\\frac{1}{2}\\|y\\|_{2}^{2} - y^{\\top}y = -\\frac{1}{2}\\|y\\|_{2}^{2}$。\n\n综合这些结果，对偶函数 $g(y)$ 为：\n$$\ng(y) = \\begin{cases} -b^{\\top}y - \\frac{1}{2}\\|y\\|_{2}^{2} & \\text{if } \\|A^{\\top}y\\|_{\\infty} \\le \\lambda \\\\ -\\infty & \\text{otherwise} \\end{cases}\n$$\n对偶问题是最大化 $g(y)$，这等价于在对偶可行集 $\\{y \\in \\mathbb{R}^{3} \\mid \\|A^{\\top}y\\|_{\\infty} \\le \\lambda \\}$ 上最大化 $-b^{\\top}y - \\frac{1}{2}\\|y\\|_{2}^{2}$。\n\n其次，我们解释原始-对偶间隙的作用。令原始目标函数为 $p(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$，并令原始问题的最优值为 $p^{\\star} = \\inf_{x} p(x) = p(x^{\\star})$，其中 $x^{\\star}$ 是一个最优解。根据对偶函数的定义，对于任意的 $y$ 和任意的 $x$，我们有 $g(y) \\le \\mathcal{L}(x, Ax-b, y)$。对于一个原始可行对 $(x, u=Ax-b)$，拉格朗日函数中的项 $y^{\\top}(Ax - u - b)$ 为零。因此，$\\mathcal{L}(x, Ax-b, y) = p(x)$。这引出了弱对偶性原理：对于任意 $x$ 和任意 $y$，$g(y) \\le p(x)$。由于这对任意 $x$ 都成立，因此对最优解 $x^{\\star}$ 也必然成立。所以，对于任意 $y$，有 $g(y) \\le p(x^{\\star}) = p^{\\star}$。候选解 $x$ 的原始次优性是非负量 $p(x) - p^{\\star}$。原始-对偶间隙定义为 $G(x,y) = p(x) - g(y)$。由弱对偶性不等式 $g(y) \\le p^{\\star}$，我们可以写出 $-p^{\\star} \\le -g(y)$。在该不等式两边同时加上 $p(x)$，得到 $p(x) - p^{\\star} \\le p(x) - g(y)$。这表明原始次优性由原始-对偶间隙作为上界：\n$$\np(x) - p^{\\star} \\le G(x,y).\n$$\n因此，间隙 $G(x,y)$ 提供了一个可计算的上界，用以衡量当前原始目标值 $p(x)$ 与真实最优值 $p^{\\star}$ 之间的差距。\n\n第三，我们验证给定的候选对偶向量 $y$ 是对偶可行的。对偶可行性的条件是 $\\|A^{\\top}y\\|_{\\infty} \\le \\lambda$。给定的数据为：\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix}, \\quad \\lambda = 1.\n$$\n首先，我们计算 $A$ 的转置，即 $A^{\\top} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}$。\n然后，我们计算乘积 $A^{\\top}y$：\n$$\nA^{\\top}y = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(-\\frac{1}{2}) + (0)(\\frac{1}{2}) + (1)(0) \\\\ (0)(-\\frac{1}{2}) + (1)(\\frac{1}{2}) + (1)(0) \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\n现在我们计算所得向量的无穷范数：\n$$\n\\|A^{\\top}y\\|_{\\infty} = \\max\\left(\\left|-\\frac{1}{2}\\right|, \\left|\\frac{1}{2}\\right|\\right) = \\frac{1}{2}.\n$$\n将其与 $\\lambda = 1$ 进行比较，我们发现 $\\frac{1}{2} \\le 1$。该条件得到满足，因此给定的向量 $y$ 是对偶可行的。\n\n第四，我们为给定的原始点和对偶点计算原始-对偶间隙 $G(x,y) = p(x) - g(y)$ 的精确值。\n原始点是 $x = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。原始目标值为 $p(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$。\n我们首先计算残差项 $Ax - b$：\n$$\nAx = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n$$\nAx - b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\n残差的 $\\ell_2$ 范数平方为 $\\|Ax - b\\|_{2}^{2} = 0^{2} + 1^{2} + 1^{2} = 2$。\n$x$ 的 $\\ell_1$ 范数为 $\\|x\\|_{1} = |1| + |0| = 1$。\n当 $\\lambda = 1$ 时，原始目标值为：\n$$\np(x) = \\frac{1}{2}(2) + (1)(1) = 1 + 1 = 2.\n$$\n接下来，我们计算对偶目标值 $g(y) = -b^{\\top}y - \\frac{1}{2}\\|y\\|_{2}^{2}$。所需的向量为 $b = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$ 和 $y = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix}$。\n项 $-b^{\\top}y$ 为：\n$$\n-b^{\\top}y = - \\left( (1)\\left(-\\frac{1}{2}\\right) + (-1)\\left(\\frac{1}{2}\\right) + (0)(0) \\right) = - \\left(-\\frac{1}{2} - \\frac{1}{2}\\right) = -(-1) = 1.\n$$\n项 $-\\frac{1}{2}\\|y\\|_{2}^{2}$ 为：\n$$\n-\\frac{1}{2}\\|y\\|_{2}^{2} = -\\frac{1}{2} \\left( \\left(-\\frac{1}{2}\\right)^{2} + \\left(\\frac{1}{2}\\right)^{2} + 0^{2} \\right) = -\\frac{1}{2} \\left( \\frac{1}{4} + \\frac{1}{4} \\right) = -\\frac{1}{2} \\left( \\frac{1}{2} \\right) = -\\frac{1}{4}.\n$$\n对偶目标值为：\n$$\ng(y) = 1 - \\frac{1}{4} = \\frac{3}{4}.\n$$\n最后，原始-对偶间隙 $G(x,y)$ 是原始目标值与对偶目标值之差：\n$$\nG(x,y) = p(x) - g(y) = 2 - \\frac{3}{4} = \\frac{8}{4} - \\frac{3}{4} = \\frac{5}{4}.\n$$\n原始-对偶间隙的精确值为 $\\frac{5}{4}$。", "answer": "$$\n\\boxed{\\frac{5}{4}}\n$$", "id": "2861525"}]}