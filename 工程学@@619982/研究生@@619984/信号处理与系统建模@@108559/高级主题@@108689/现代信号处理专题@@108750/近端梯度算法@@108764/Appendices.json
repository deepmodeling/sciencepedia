{"hands_on_practices": [{"introduction": "要真正掌握近端梯度算法，最好的方式莫过于亲手实践。我们将从最核心的步骤开始：为经典的 LASSO（最小绝对收缩和选择算子）问题实现单次迭代。这个练习将带你推导并实现算法的基础构件——一个针对问题平滑部分的梯度下降步骤，以及一个通过软阈值算子处理稀疏性诱导项的近端步骤 [@problem_id:2897782]。通过这个实践，你将把抽象的数学理论转化为可执行的代码，并直观地看到单次迭代如何将一个带噪的信号向其稀疏的真实解推进。", "problem": "给定一个在有限维实数空间中的复合凸优化问题：最小化目标函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\frac{1}{2}\\lVert x - b \\rVert_2^2$ 且 $g(x) = \\lambda \\lVert x \\rVert_1$，$x \\in \\mathbb{R}^n$，数据 $b \\in \\mathbb{R}^n$，正则化参数 $\\lambda \\in \\mathbb{R}_{\\ge 0}$。从光滑函数的梯度和凸函数的近端算子的基本定义出发，推导近端梯度法用于求解 $F(x)$ 的单步更新映射，包括基于 $\\nabla f(x)$ 的 Lipschitz 连续性的步长可取条件。然后，实现一个程序，对几个含噪声稀疏去噪的确定性测试实例应用一次近端梯度迭代，并量化其相对于已知的稀疏真实值对估计值的即时影响。\n\n您的程序必须：\n- 使用从定义推导出的、仅有一次的近端梯度迭代，其步长 $t \\in \\mathbb{R}_{>0}$ 和初始值 $x^{(0)} \\in \\mathbb{R}^n$ 由用户指定（不得假定任何预封装的更新形式；必须进行显式推导）。\n- 对于每个测试用例，计算：\n  1. 一次迭代后的均方误差，定义为 $\\mathrm{MSE} = \\frac{1}{n}\\lVert x^{(1)} - x_{\\mathrm{true}} \\rVert_2^2$。\n  2. 以分数表示的支撑集召回率，定义为 $\\mathrm{recall} = \\frac{\\lvert \\mathrm{supp}(x^{(1)}) \\cap \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}{\\lvert \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}$，其中 $\\mathrm{supp}(z) = \\{ i : |z_i| > \\varepsilon \\}$，阈值 $\\varepsilon = 10^{-12}$。\n  3. 在相同阈值 $\\varepsilon$ 下，$x^{(1)}$ 中非零项的数量，为一个整数。\n- 在输出中，将 $\\mathrm{MSE}$ 和 $\\mathrm{recall}$ 四舍五入到6位小数。非零项计数必须为整数。\n- 满足步长可取条件 $t \\in (0, 2/L)$，其中 $L$ 是 $\\nabla f$ 的 Lipschitz 常数，您必须从基本原理确定该常数。\n\n测试套件：\n- 用例 1（理想情况，从零初始化进行精确收缩）：\n  - $n = 8$\n  - $x_{\\mathrm{true}} = [0, 1.5, 0, 0, -2.0, 0, 0.7, 0]$\n  - $\\mathrm{noise} = [0.05, -0.08, 0.10, -0.02, 0.03, 0.00, -0.05, 0.04]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0, 0, 0, 0, 0, 0, 0, 0]$\n  - $\\lambda = 0.4$\n  - $t = 1.0$\n- 用例 2（在收缩阈值处的边界条件）：\n  - $n = 8$\n  - $x_{\\mathrm{true}} = [0.4, -0.4, 0, 0, 0, 0, 0, 0]$\n  - $\\mathrm{noise} = [0, 0, 0, 0, 0, 0, 0, 0]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0, 0, 0, 0, 0, 0, 0, 0]$\n  - $\\lambda = 0.4$\n  - $t = 1.0$\n- 用例 3（非零热启动和部分步长）：\n  - $n = 6$\n  - $x_{\\mathrm{true}} = [0, 0, 1.0, 0, -1.2, 0]$\n  - $\\mathrm{noise} = [0.0, 0.05, -0.02, 0.00, 0.03, 0.10]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0.2, 0, 0.5, 0, -0.8, 0]$\n  - $\\lambda = 0.6$\n  - $t = 0.5$\n- 用例 4（接近稳定性极限但仍可取的大步长）：\n  - $n = 5$\n  - $x_{\\mathrm{true}} = [0, 0, 0, 2.0, 0]$\n  - $\\mathrm{noise} = [0.02, -0.01, 0.00, -0.05, 0.01]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0, 0, 0, 0, 0]$\n  - $\\lambda = 0.2$\n  - $t = 1.9$\n\n实现要求：\n- 推导并实现针对 $f$ 和 $g$ 特化的近端梯度单步更新。\n- 直接根据 $g$ 的定义实现推导中所需的近端算子，在特化时按元素应用。\n- 对每个用例，计算 $x^{(1)}$，然后计算三元组 $[\\mathrm{MSE}, \\mathrm{recall}, \\mathrm{nnz}]$，其中 $\\mathrm{nnz}$ 是满足 $\\lvert x^{(1)}_i \\rvert > \\varepsilon$ 的索引 $i$ 的整数计数。\n- 您的程序应生成单行输出，其中包含用方括号括起来的结果，结果为逗号分隔的列表，每个元素是对应一个测试用例的列表 $[\\mathrm{MSE}, \\mathrm{recall}, \\mathrm{nnz}]$，其中 $\\mathrm{MSE}$ 和 $\\mathrm{recall}$ 四舍五入到6位小数。例如，包含两个用例的输出应如下所示：$[[0.123456,0.500000,3],[0.010000,1.000000,2]]$。\n\n角度单位、物理单位和百分比不适用。所有报告的分数必须以四舍五入到6位的小数形式出现，而不是百分比形式。", "solution": "问题已经过验证。\n\n**步骤 1：提取的已知条件**\n- **目标函数**：对 $x \\in \\mathbb{R}^n$ 最小化 $F(x) = f(x) + g(x)$。\n- **光滑部分**：$f(x) = \\frac{1}{2}\\lVert x - b \\rVert_2^2$，其中 $b \\in \\mathbb{R}^n$。\n- **非光滑部分**：$g(x) = \\lambda \\lVert x \\rVert_1$，其中 $\\lambda \\in \\mathbb{R}_{\\ge 0}$。\n- **算法**：从 $x^{(0)}$ 开始，使用步长 $t > 0$ 进行一次近端梯度法迭代。\n- **步长条件**：$t \\in (0, 2/L)$，其中 $L$ 是 $\\nabla f(x)$ 的 Lipschitz 常数。\n- **指标**：\n    1. $\\mathrm{MSE} = \\frac{1}{n}\\lVert x^{(1)} - x_{\\mathrm{true}} \\rVert_2^2$。\n    2. $\\mathrm{recall} = \\frac{\\lvert \\mathrm{supp}(x^{(1)}) \\cap \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}{\\lvert \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}$。\n    3. $\\mathrm{supp}(z) = \\{ i : |z_i| > \\varepsilon \\}$，其中 $\\varepsilon = 10^{-12}$。\n    4. $\\mathrm{nnz}$：$x^{(1)}$ 中非零项的数量。\n- **测试用例**：四个具有给定参数 $n$、$x_{\\mathrm{true}}$、噪声、$x^{(0)}$、$\\lambda$ 和 $t$ 的特定实例。\n\n**步骤 2：验证**\n问题已经过严格验证。\n- **科学依据**：该问题描述了用于稀疏信号恢复的 LASSO 公式，这是信号处理和统计学中的一个典型问题。函数 $f(x)$ 和 $g(x)$ 是凸函数。对于此类复合凸优化问题，近端梯度法是标准且适用的算法。问题的前提具有科学合理性。\n- **适定性**：目标函数 $F(x)$ 是严格凸且强制的，这保证了唯一最小化子的存在。任务是执行一个定义明确的算法步骤并计算确定性指标。该问题是适定的。\n- **客观性**：语言精确且数学化，没有歧义或主观论断。\n- **一致性**：问题是自洽的。必须确定 $\\nabla f(x)$ 的 Lipschitz 常数 $L$。对于 $f(x) = \\frac{1}{2}\\lVert x - b \\rVert_2^2$，梯度为 $\\nabla f(x) = x-b$，Hessian 矩阵为 $\\nabla^2 f(x) = I_n$（一个单位矩阵）。梯度的 Lipschitz 常数是 Hessian 矩阵的最大特征值，因此 $L=1$。所要求的步长条件是 $t \\in (0, 2/1) = (0, 2)$。所有测试用例提供的步长（$t=1.0, 1.0, 0.5, 1.9$）都在此区间内。问题设定是一致且完整的。\n\n**步骤 3：结论**\n问题有效。这是一个定义明确且标准的信号处理优化练习。开始求解。\n\n---\n\n该问题是最小化复合凸函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\frac{1}{2}\\lVert x-b \\rVert_2^2$ 且 $g(x) = \\lambda \\lVert x \\rVert_1$。近端梯度法是为此类问题设计的迭代算法。每次迭代包括两个步骤：对光滑部分 $f(x)$ 进行梯度下降步，然后是对非光滑部分 $g(x)$ 进行近端步。\n\n迭代更新是通过最小化 $f(x)$ 在当前估计值 $x^{(k)}$ 附近的二次近似，再加上非光滑项 $g(x)$ 来推导的。$f(x)$ 在 $x^{(k)}$ 附近的 Taylor 展开为 $f(x) \\approx f(x^{(k)}) + \\langle \\nabla f(x^{(k)}), x - x^{(k)} \\rangle + \\frac{1}{2}(x - x^{(k)})^T \\nabla^2 f(x^{(k)}) (x - x^{(k)})$。对于一个通用步长 $t > 0$，我们用 $\\frac{1}{t}I$ 替换 Hessian 项，这引出了放大-最小化步骤：\n$$x^{(k+1)} = \\arg\\min_x \\left( f(x^{(k)}) + \\langle \\nabla f(x^{(k)}), x - x^{(k)} \\rangle + \\frac{1}{2t}\\lVert x - x^{(k)} \\rVert_2^2 + g(x) \\right)$$\n在最小化过程中，可以忽略与 $x$ 无关的项 $f(x^{(k)})$ 和其他常数。通过配方法，该问题等价于：\n$$x^{(k+1)} = \\arg\\min_x \\left( \\frac{1}{2t}\\lVert x - (x^{(k)} - t\\nabla f(x^{(k)})) \\rVert_2^2 + g(x) \\right)$$\n这就是函数 $tg(x)$ 的近端算子的定义，它被应用于对 $f(x)$ 执行梯度下降步后得到的点。近端梯度更新的一般形式是：\n$$x^{(k+1)} = \\mathrm{prox}_{tg} \\left( x^{(k)} - t\\nabla f(x^{(k)}) \\right)$$\n\n为保证该方法的收敛性，步长 $t$ 的选择必须满足 $t \\in (0, 2/L)$，其中 $L$ 是梯度 $\\nabla f(x)$ 的 Lipschitz 常数。首先，我们计算 $f(x) = \\frac{1}{2}(x-b)^T(x-b)$ 的梯度：\n$$\\nabla f(x) = x - b$$\nLipschitz 常数 $L$ 必须对所有 $x_1, x_2$ 满足 $\\lVert \\nabla f(x_1) - \\nabla f(x_2) \\rVert_2 \\le L \\lVert x_1 - x_2 \\rVert_2$。\n$$\\lVert \\nabla f(x_1) - \\nabla f(x_2) \\rVert_2 = \\lVert (x_1 - b) - (x_2 - b) \\rVert_2 = \\lVert x_1 - x_2 \\rVert_2$$\n因此，$L=1$。步长条件为 $t \\in (0, 2)$。\n\n接下来，我们必须推导 $g(x) = \\lambda \\lVert x \\rVert_1$ 的近端算子。近端算子 $\\mathrm{prox}_{\\alpha g}(v)$ 定义为：\n$$\\mathrm{prox}_{\\alpha g}(v) = \\arg\\min_x \\left( \\frac{1}{2} \\lVert x - v \\rVert_2^2 + \\alpha g(x) \\right)$$\n在我们的问题中，函数是 $tg(x) = t\\lambda\\lVert x \\rVert_1$。令 $\\gamma = t\\lambda$。该优化问题变为：\n$$\\mathrm{prox}_{\\gamma \\lVert \\cdot \\rVert_1}(v) = \\arg\\min_x \\left( \\frac{1}{2} \\sum_{i=1}^n (x_i - v_i)^2 + \\gamma \\sum_{i=1}^n |x_i| \\right)$$\n目标函数是可分的，这意味着我们可以独立地求解每个分量 $x_i$：\n$$x_i^* = \\arg\\min_{x_i} \\left( \\frac{1}{2} (x_i - v_i)^2 + \\gamma |x_i| \\right)$$\n根据次梯度微积分的一阶最优性条件，要求 $0$ 必须在目标函数于 $x_i^*$ 处的次微分中：\n$$0 \\in x_i^* - v_i + \\gamma \\cdot \\partial|x_i^*|$$\n其中 $\\partial|\\cdot|$ 是绝对值函数的次微分。\n如果 $x_i^* > 0$，则 $\\partial|x_i^*| = \\{1\\}$，所以 $x_i^* - v_i + \\gamma = 0 \\implies x_i^* = v_i - \\gamma$。这仅在 $v_i - \\gamma > 0$（即 $v_i > \\gamma$）时成立。\n如果 $x_i^* < 0$，则 $\\partial|x_i^*| = \\{-1\\}$，所以 $x_i^* - v_i - \\gamma = 0 \\implies x_i^* = v_i + \\gamma$。这仅在 $v_i + \\gamma < 0$（即 $v_i < -\\gamma$）时成立。\n如果 $x_i^* = 0$，则 $\\partial|x_i^*| = [-1, 1]$，所以 $v_i \\in \\gamma[-1, 1]$，即 $|v_i| \\le \\gamma$。\n综合这些情况，得到解，即所谓的软阈值算子 $S_\\gamma(\\cdot)$：\n$$x_i^* = S_\\gamma(v_i) = \\begin{cases} v_i - \\gamma & \\text{if } v_i > \\gamma \\\\ v_i + \\gamma & \\text{if } v_i < -\\gamma \\\\ 0 & \\text{if } |v_i| \\le \\gamma \\end{cases}$$\n这可以被紧凑地写作 $S_\\gamma(v_i) = \\mathrm{sign}(v_i) \\max(|v_i| - \\gamma, 0)$。\n\n为了从 $x^{(0)}$ 开始执行一次近端梯度法迭代，我们首先计算梯度下降更新：\n$$z^{(0)} = x^{(0)} - t\\nabla f(x^{(0)}) = x^{(0)} - t(x^{(0)} - b) = (1-t)x^{(0)} + tb$$\n然后，我们将近端算子应用于 $z^{(0)}$：\n$$x^{(1)} = \\mathrm{prox}_{t g}(z^{(0)}) = S_{t\\lambda}(z^{(0)})$$\n该更新是按元素进行的：$x_i^{(1)} = S_{t\\lambda}(z_i^{(0)})$，其中 $i=1, \\dots, n$。这就是需要实现的单步更新映射。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the sparse denoising problem for multiple test cases using one\n    iteration of the proximal gradient method.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 8,\n            \"x_true\": np.array([0, 1.5, 0, 0, -2.0, 0, 0.7, 0]),\n            \"noise\": np.array([0.05, -0.08, 0.10, -0.02, 0.03, 0.00, -0.05, 0.04]),\n            \"x0\": np.array([0, 0, 0, 0, 0, 0, 0, 0]),\n            \"lambda\": 0.4,\n            \"t\": 1.0,\n        },\n        {\n            \"n\": 8,\n            \"x_true\": np.array([0.4, -0.4, 0, 0, 0, 0, 0, 0]),\n            \"noise\": np.array([0, 0, 0, 0, 0, 0, 0, 0]),\n            \"x0\": np.array([0, 0, 0, 0, 0, 0, 0, 0]),\n            \"lambda\": 0.4,\n            \"t\": 1.0,\n        },\n        {\n            \"n\": 6,\n            \"x_true\": np.array([0, 0, 1.0, 0, -1.2, 0]),\n            \"noise\": np.array([0.0, 0.05, -0.02, 0.00, 0.03, 0.10]),\n            \"x0\": np.array([0.2, 0, 0.5, 0, -0.8, 0]),\n            \"lambda\": 0.6,\n            \"t\": 0.5,\n        },\n        {\n            \"n\": 5,\n            \"x_true\": np.array([0, 0, 0, 2.0, 0]),\n            \"noise\": np.array([0.02, -0.01, 0.00, -0.05, 0.01]),\n            \"x0\": np.array([0, 0, 0, 0, 0]),\n            \"lambda\": 0.2,\n            \"t\": 1.9,\n        },\n    ]\n\n    results = []\n    \n    # Epsilon for support calculation\n    epsilon = 1e-12\n\n    def soft_thresholding(v, gamma):\n        \"\"\"\n        Implementation of the soft-thresholding operator S_gamma(v).\n        This is the proximal operator of the L1 norm.\n        \"\"\"\n        return np.sign(v) * np.maximum(np.abs(v) - gamma, 0)\n\n    def get_support(z, eps):\n        \"\"\"\n        Computes the support of a vector z.\n        supp(z) = { i : |z_i| > eps }\n        \"\"\"\n        return set(np.where(np.abs(z) > eps)[0])\n\n    for case in test_cases:\n        n = case[\"n\"]\n        x_true = case[\"x_true\"]\n        noise = case[\"noise\"]\n        x0 = case[\"x0\"]\n        lam = case[\"lambda\"]\n        t = case[\"t\"]\n        \n        b = x_true + noise\n\n        # Step 1: Gradient descent step on the smooth part f(x)\n        # z = x0 - t * grad_f(x0) = x0 - t*(x0 - b)\n        z = (1 - t) * x0 + t * b\n\n        # Step 2: Proximal step on the non-smooth part g(x)\n        # x1 = prox_{t*g}(z) = prox_{t*lambda*||.||_1}(z)\n        # This is the soft-thresholding operator\n        gamma = t * lam\n        x1 = soft_thresholding(z, gamma)\n\n        # Calculate metrics\n        # 1. MSE\n        mse = np.mean((x1 - x_true)**2)\n        \n        # 2. Support Recall\n        supp_x_true = get_support(x_true, epsilon)\n        supp_x1 = get_support(x1, epsilon)\n        \n        if len(supp_x_true) == 0:\n            # If true support is empty, recall is 1.0 if estimated support is also empty, 0.0 otherwise.\n            recall = 1.0 if len(supp_x1) == 0 else 0.0\n        else:\n            intersection_size = len(supp_x_true.intersection(supp_x1))\n            recall = intersection_size / len(supp_x_true)\n            \n        # 3. Number of nonzeros (nnz)\n        nnz = len(supp_x1)\n        \n        results.append([mse, recall, nnz])\n\n    # Format the final output string exactly as specified.\n    results_str_list = []\n    for res in results:\n        mse_val, recall_val, nnz_val = res\n        results_str_list.append(f\"[{mse_val:.6f},{recall_val:.6f},{nnz_val}]\")\n    \n    final_output = f\"[{','.join(results_str_list)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2897782"}, {"introduction": "在掌握了基础的近端梯度步骤后，我们来探索两个重要的拓展：算法加速和更复杂的正则化项。本练习将引导你为总变差（Total Variation, TV）正则化去噪问题实现单步快速迭代阈值收缩算法（FISTA）。你不仅会应用一个动量项来加速收敛，还将学习如何通过求解一个对偶问题来计算更为复杂的总变差近端算子 [@problem_id:2897783]。这个练习将展示近端梯度框架的强大通用性，让你能够将其应用于更广泛的信号与图像处理任务中。", "problem": "在信号处理和系统建模的背景下，考虑对一个一维双像素信号进行全变分（TV）正则化去噪。设观测数据为 $b \\in \\mathbb{R}^{2}$，其中 $b = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$。我们最小化复合目标函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2}\\|x - b\\|_{2}^{2}$ 且 $g(x) = \\lambda \\,\\mathrm{TV}(x)$。对于一个双像素信号，其各向同性全变分为 $\\mathrm{TV}(x) = |x_{2} - x_{1}|$，前向差分算子 $D \\in \\mathbb{R}^{1 \\times 2}$ 为 $D = \\begin{pmatrix} -1 & 1 \\end{pmatrix}$，因此 $Dx = x_{2} - x_{1}$。\n\n使用一次快速迭代收缩阈值算法（FISTA）的迭代，该算法定义为一个对 $f$ 的梯度步，后跟一个 $g$ 的近端算子，以及一个惯性（动量）更新。使用 $x^{(0)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$y^{(0)} = x^{(0)}$ 和动量标量 $s_{0} = 1$ 进行初始化。使用步长 $\\tau = 1$ 和正则化参数 $\\lambda = 1$。在您的推导中，从第一性原理出发处理数据保真项梯度 $\\nabla f(x)$ 和 $g$ 的近端算子，并通过使用与 $D$ 关联的对偶变量的对偶投影刻画来实现各向同性TV项的近端步。\n\n执行恰好一次 FISTA 迭代，明确写出：\n- 近端算子的梯度步输入，\n- 用于计算各向同性TV项近端算子的对偶投影，\n- 动量更新和惯性点。\n\n第一次FISTA迭代返回的 $x^{(1)}$ 的第二个分量的值是多少？请将您的最终答案表示为一个精确的数字。", "solution": "对问题陈述进行验证。\n\n逐字提取给定条件：\n- 优化问题涉及一个一维双像素信号。\n- 观测数据为 $b = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} \\in \\mathbb{R}^{2}$。\n- 目标函数为 $F(x) = f(x) + g(x)$，其中 $x \\in \\mathbb{R}^{2}$。\n- 目标函数的光滑部分是数据保真项 $f(x) = \\frac{1}{2}\\|x - b\\|_{2}^{2}$。\n- 非光滑部分是正则化项 $g(x) = \\lambda \\,\\mathrm{TV}(x)$。\n- 对于双像素信号 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ 的全变分是 $\\mathrm{TV}(x) = |x_{2} - x_{1}|$。\n- 前向差分算子是 $D = \\begin{pmatrix} -1 & 1 \\end{pmatrix}$，因此 $Dx = x_{2} - x_{1}$。\n- 要使用的算法是快速迭代收缩阈值算法（FISTA）。\n- 迭代次数恰好为一次。\n- 步长为 $\\tau = 1$。\n- 正则化参数为 $\\lambda = 1$。\n- 初始条件为 $x^{(0)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$y^{(0)} = x^{(0)}$，以及 $s_{0} = 1$。\n- TV项的近端算子将通过其对偶投影刻画来计算。\n- 所求输出是 $x^{(1)}$ 的第二个分量的值。\n\n该问题在信号处理的凸优化这一成熟领域中有其科学依据。全变分正则化和FISTA算法是标准的、有据可查的技术。该问题是适定的，为算法单次迭代的唯一且稳定的计算提供了所有必要的参数和初始条件。其语言客观而精确。这个双像素信号的玩具问题是一个标准的教学工具，并不代表科学上的缺陷。该问题被认为是有效的。\n\nFISTA迭代由以下针对迭代 $k$ 的序列定义：\n$1$. $z^{(k)} = y^{(k)} - \\tau \\nabla f(y^{(k)})$\n$2$. $x^{(k+1)} = \\mathrm{prox}_{\\tau g}(z^{(k)})$\n$3$. $s_{k+1} = \\frac{1 + \\sqrt{1 + 4s_k^2}}{2}$\n$4$. $y^{(k+1)} = x^{(k+1)} + \\frac{s_k - 1}{s_{k+1}}(x^{(k+1)} - x^{(k)})$\n\n我们从 $k=0$ 开始执行一次迭代。给定 $x^{(0)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$y^{(0)} = x^{(0)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$s_0 = 1$，$\\tau=1$ 和 $\\lambda=1$。观测数据为 $b = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$。\n\n首先，我们计算 $f(x) = \\frac{1}{2}\\|x-b\\|_2^2$ 的梯度。梯度为 $\\nabla f(x) = x-b$。\n我们在惯性点 $y^{(0)}$ 处计算梯度：\n$$\n\\nabla f(y^{(0)}) = y^{(0)} - b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\n\n接下来，我们执行梯度下降步来找到近端算子的输入 $z^{(0)}$：\n$$\nz^{(0)} = y^{(0)} - \\tau \\nabla f(y^{(0)}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ 0-(-2) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}\n$$\n该向量 $z^{(0)}$ 是近端算子的输入。\n\n现在我们计算近端步，$x^{(1)} = \\mathrm{prox}_{\\tau g}(z^{(0)})$。函数为 $g(x) = \\lambda |Dx|$。需要求解的问题是：\n$$\nx^{(1)} = \\arg\\min_x \\left\\{ \\frac{1}{2}\\|x - z^{(0)}\\|_2^2 + \\tau \\lambda |Dx| \\right\\}\n$$\n按照规定，我们使用对偶投影刻画。形如 $\\min_x \\frac{1}{2}\\|x-z\\|_2^2 + \\gamma \\|Dx\\|_1$ 的问题的解由 $x^* = z - D^T p^*$ 给出，其中 $p^*$ 是一个相关对偶问题的解。对于这种结构，$p^*$ 可以通过投影找到。通解为：\n$$\nx^* = z - D^T \\mathrm{proj}_{\\mathcal{B}} \\left( (DD^T)^{-1} Dz \\right)\n$$\n其中 $\\mathcal{B}$ 是对应于 $\\gamma \\|\\cdot\\|_1$ 的对偶范数球。$\\ell_1$ 范数的对偶范数是 $\\ell_\\infty$ 范数。在我们 $D$ 的一维输出空间中，这只是一个区间。这个球是 $\\mathcal{B} = \\{p \\in \\mathbb{R} : |p| \\le \\gamma \\}$。这里，$\\gamma = \\tau \\lambda = 1 \\cdot 1 = 1$。用于投影的集合是 $\\{p \\in \\mathbb{R} : |p| \\le 1\\}$，即区间 $[-1, 1]$。\n\n我们计算必要的组成部分：\n算子为 $D = \\begin{pmatrix} -1 & 1 \\end{pmatrix}$，所以其转置为 $D^T = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$。\n项 $DD^T$ 为：\n$$\nDD^T = \\begin{pmatrix} -1 & 1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = (-1)(-1) + (1)(1) = 2\n$$\n其逆为 $(DD^T)^{-1} = \\frac{1}{2}$。\n项 $Dz^{(0)}$ 为：\n$$\nDz^{(0)} = \\begin{pmatrix} -1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = (-1)(0) + (1)(2) = 2\n$$\n投影的参数为 $(DD^T)^{-1} Dz^{(0)} = \\frac{1}{2} \\cdot 2 = 1$。\n\n对偶投影是将此值投影到区间 $[-1, 1]$ 上。\n$$\np^* = \\mathrm{proj}_{[-1,1]}(1)\n$$\n由于值 $1$ 已经在区间 $[-1, 1]$ 内，投影就是 $1$。因此，$p^* = 1$。\n\n现在我们计算 $x^{(1)}$：\n$$\nx^{(1)} = z^{(0)} - D^T p^* = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} (1) = \\begin{pmatrix} 0 - (-1) \\\\ 2 - 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n\n最后，我们执行动量更新。首先，我们将标量 $s_0=1$ 更新为 $s_1$：\n$$\ns_1 = \\frac{1 + \\sqrt{1 + 4s_0^2}}{2} = \\frac{1 + \\sqrt{1 + 4(1)^2}}{2} = \\frac{1 + \\sqrt{5}}{2}\n$$\n然后我们计算下一个惯性点 $y^{(1)}$：\n$$\ny^{(1)} = x^{(1)} + \\frac{s_0 - 1}{s_1}(x^{(1)} - x^{(0)})\n$$\n由于 $s_0=1$，项 $s_0-1=0$。这极大地简化了计算：\n$$\ny^{(1)} = x^{(1)} + \\frac{1 - 1}{s_1}(x^{(1)} - x^{(0)}) = x^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n\n问题要求的是 $x^{(1)}$ 的第二个分量的值。\n根据我们的计算，$x^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。第二个分量是 $1$。", "answer": "$$\\boxed{1}$$", "id": "2897783"}, {"introduction": "一个迭代算法不仅包含其核心更新步骤，还必须有一个明确的终止条件。在学会了如何执行单次近端梯度迭代后，我们必须回答一个关键的实践问题：“算法何时收敛？” 本练习将探讨为近端梯度法设计有效终止准则的几种主流策略 [@problem_id:2897755]。你将分析并权衡基于目标函数相对下降、梯度映射范数以及对偶间隙的各种准则，从而学会如何在保证解的质量与控制计算成本之间做出明智的权衡，这是将理论算法转化为实用工具的关键一步。", "problem": "你正在使用最小绝对收缩和选择算子 (LASSO) 为稀疏恢复问题实现一个带回溯法的近端梯度方法，即求解复合凸优化问题\n$$\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\triangleq f(x) + g(x), \\quad f(x) \\triangleq \\tfrac{1}{2}\\|A x - y\\|_2^2, \\quad g(x) \\triangleq \\lambda \\|x\\|_1,$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^m$，且 $\\lambda > 0$。假设 $f$ 是凸且连续可微的，其梯度是 Lipschitz 连续的，并且 $g$ 是凸的、正常的和下半连续的。你希望选择一个理论上有意义且计算上高效的实用停止准则。考虑基于以下几项构建的准则：相对目标值下降、梯度映射范数以及对偶间隙（当其可以计算时）。请选择所有选项，这些选项同时给出了准则的正确且可实现的定义，并正确陈述了在用近端梯度法进行稀疏恢复的背景下一个主要优点和一个局限性。\n\nA. 相对目标值下降：在第一个满足以下条件的迭代索引 $k$ 处停止\n$$\\frac{F(x^{k}) - F(x^{k+1})}{\\max\\{F(x^{k}), \\, 1\\}} \\le \\varepsilon_{\\mathrm{rel}}, \\quad \\varepsilon_{\\mathrm{rel}} \\in (0,1)。$$ \n优点：每次迭代的评估成本低，因为在回溯线搜索下 $F(x^{k})$ 和 $F(x^{k+1})$ 已经可用。局限性：当由于强正则化或病态条件导致 $F$ 变得平坦时，即使一阶最优性残差还不小，它也可能过早地判断收敛。\n\nB. 梯度映射范数：使用回溯法估计的 $\\nabla f$ 的 Lipschitz 常数 $L_k > 0$，定义梯度映射\n$$G_{L_k}(x^{k}) \\triangleq L_k\\!\\left(x^{k} - \\operatorname{prox}_{(\\lambda/L_k)\\|\\cdot\\|_1}\\!\\left(x^{k} - \\tfrac{1}{L_k}\\,A^\\top(Ax^{k} - y)\\right)\\right)。$$ \n当满足以下条件时停止\n$$\\|G_{L_k}(x^{k})\\|_2 \\le \\varepsilon_{\\mathrm{abs}} \\quad \\text{或} \\quad \\|G_{L_k}(x^{k})\\|_2 \\le \\varepsilon_{\\mathrm{rel}} \\max\\{1,\\|x^{k}\\|_2\\},$$ \n其中容忍度 $\\varepsilon_{\\mathrm{abs}}, \\varepsilon_{\\mathrm{rel}} > 0$。优点：$\\|G_{L_k}(x^{k})\\|_2 = 0$ 当且仅当复合一阶最优性条件 $0 \\in \\nabla f(x) + \\partial g(x)$ 成立，因此它直接衡量非光滑问题的平稳性。局限性：它依赖于 $L_k$ 的局部选择，并且除非从迭代中复用，否则会产生额外的近端算子评估，这会略微增加每次迭代的成本。\n\nC. 对偶间隙：通过重新缩放残差 $\\theta^{k} \\triangleq \\tau^{k}(A x^{k} - y)$ 来构造一个对偶可行向量，其中\n$$\\tau^{k} \\triangleq \\min\\!\\left\\{1, \\frac{\\lambda}{\\|A^\\top(Ax^{k} - y)\\|_\\infty}\\right\\}$$ \n以使得 $\\|A^\\top \\theta^{k}\\|_\\infty \\le \\lambda$。定义对偶目标\n$$d(\\theta) \\triangleq -\\tfrac{1}{2}\\|\\theta\\|_2^2 - y^\\top \\theta,$$ \n该定义对所有满足 $\\|A^\\top \\theta\\|_\\infty \\le \\lambda$ 的 $\\theta$ 均有效，并计算对偶间隙\n$$\\mathrm{gap}(x^{k},\\theta^{k}) \\triangleq F(x^{k}) - d(\\theta^{k}) = \\tfrac{1}{2}\\|A x^{k} - y\\|_2^2 + \\lambda\\|x^{k}\\|_1 + \\tfrac{1}{2}\\|\\theta^{k}\\|_2^2 + y^\\top \\theta^{k}。$$ \n当 $\\mathrm{gap}(x^{k},\\theta^{k}) \\le \\varepsilon_{\\mathrm{abs}}$ 或 $\\mathrm{gap}(x^{k},\\theta^{k}) \\le \\varepsilon_{\\mathrm{rel}} F(x^{k})$ 时停止。优点：对偶间隙是原始目标 $\\varepsilon$-最优性的一个尺度感知凭证。局限性：获得 $\\theta^{k}$ 需要计算 $A^\\top(Ax^{k} - y)$ 和维持对偶可行性，如果只提供了一个梯度预言机或者 $A$ 是被隐式访问的，这可能是不可用或成本较高的。\n\nD. 对光滑代理的梯度范数测试：当 $\\|\\nabla f(x^{k})\\|_2 \\le \\varepsilon_{\\mathrm{abs}}$ 时停止。优点：这是经典的光滑最优性测试，且计算成本低；非光滑项 $g$ 不改变一阶平稳性。局限性：在此背景下无本质局限性。\n\nE. 约束变体捷径：对于基追踪问题 $\\min_{x}\\|x\\|_1 \\;\\text{subject to}\\; A x = y$，当通过添加二次惩罚项并应用近端梯度法求解时，可以使用代理“对偶间隙” $\\tfrac{1}{2}\\|A x^{k} - y\\|_2^2$ 作为停止准则，而无需构造对偶可行点；这个量是约束问题真实对偶间隙的上界，因此可以证明次优性。优点：无需对偶计算。局限性：仅需选择惩罚参数。", "solution": "我们从复合凸优化问题开始，其中 $F(x) \\triangleq f(x) + g(x)$，$f$ 是凸的、可微的，且其梯度对于某个 $L > 0$ 是 $L$-Lipschitz 连续的，而 $g$ 是凸的、正常的、下半连续的。近端梯度法使用由 $\\operatorname{prox}_{\\alpha g}(v) \\triangleq \\arg\\min_{x}\\{g(x) + \\tfrac{1}{2\\alpha}\\|x - v\\|_2^2\\}$ 定义的近端算子，并采用步长 $\\alpha_k = 1/L_k$ 来生成迭代点\n$$x^{k+1} = \\operatorname{prox}_{\\alpha_k g}\\!\\left(x^{k} - \\alpha_k \\nabla f(x^{k})\\right)。$$\n对于 $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$ 和 $g(x) = \\lambda\\|x\\|_1$，我们有 $\\nabla f(x) = A^\\top(Ax - y)$ 且 $\\operatorname{prox}_{\\alpha \\lambda \\|\\cdot\\|_1}$ 是阈值为 $\\alpha \\lambda$ 的软阈值算子。\n\n一个基于原则的停止准则设计必须与以下三者之一相关：(i) $F$ 的单调下降，(ii) 复合问题的一阶最优性，或 (iii) 通过强对偶性实现的原始-对偶最优性。我们现在对这三类方法进行论证。\n\n相对目标值下降。在强制执行下降条件的回溯线搜索下，对于所有 $k$，$F(x^{k})$ 是一个非增序列。一个相对下降测试使用\n$$\\frac{F(x^{k}) - F(x^{k+1})}{\\max\\{F(x^{k}), \\, 1\\}} \\le \\varepsilon_{\\mathrm{rel}}。$$\n分子反映了实际的进展，分母则对量级进行归一化以防止除以一个非常小的值；当 $F(x^{k})$很小时，使用 $\\max\\{F(x^{k}),1\\}$ 提供了数值稳定性。这个准则的成本很低，因为为了验证回溯条件，量 $f(x^{k})$ 和 $g(x^{k})$ 已经计算过了。然而，在 $F$ 变得平坦的病态或强正则化问题中，小的相对下降可能发生在远离平稳点的地方，并且它不提供关于一阶残差或原始-对偶间隙的任何保证。因此，即使最优性残差还不小，它也可能提前停止。\n\n梯度映射范数。复合一阶最优性条件是\n$$0 \\in \\nabla f(x^\\star) + \\partial g(x^\\star),$$\n其中 $\\partial g$ 是 $g$ 的次微分。在点 $x$ 处，参数为 $L > 0$ 的梯度映射定义为\n$$G_L(x) \\triangleq L\\left(x - \\operatorname{prox}_{g/L}\\!\\left(x - \\tfrac{1}{L}\\nabla f(x)\\right)\\right)。$$\n通过 Moreau 分解和近端步的一阶最优性，可以证明 $G_L(x) = 0$ 当且仅当 $0 \\in \\nabla f(x) + \\partial g(x)$，即 $x$ 是一阶平稳点（在此由于凸性，它是一个极小点）。在我们的 LASSO 设定中，对于 $g(x) = \\lambda\\|x\\|_1$，这变为\n$$G_{L_k}(x^{k}) = L_k\\!\\left(x^{k} - \\operatorname{prox}_{(\\lambda/L_k)\\|\\cdot\\|_1}\\!\\left(x^{k} - \\tfrac{1}{L_k}A^\\top(Ax^{k} - y)\\right)\\right)。$$\n因此，$\\|G_{L_k}(x^{k})\\|_2$ 可作为一个有原则的残差。它直接衡量了复合意义下对平稳性的违反程度，这与忽略了 $g$ 的 $\\|\\nabla f(x^{k})\\|_2$ 不同。由于近端梯度更新已经计算了在 $x^{k}$ 处的 $\\operatorname{prox}_{(\\lambda/L_k)\\|\\cdot\\|_1}$ 来形成 $x^{k+1}$，所以通过复用该计算，几乎可以无额外成本地获得该映射，尽管如果单独实现，会产生一次额外的近端算子评估。它确实依赖于 $L_k$ 的选择；然而，$G_{L_k}$ 的零点集与 $L_k > 0$ 无关，所以它仍然是一个有效的残差。\n\n对偶间隙。对于 LASSO，我们从 Fenchel 对偶性推导其凸对偶问题。使用表示\n$$\\tfrac{1}{2}\\|A x - y\\|_2^2 = \\sup_{\\theta \\in \\mathbb{R}^m}\\left\\{\\theta^\\top(Ax - y) - \\tfrac{1}{2}\\|\\theta\\|_2^2\\right\\}，$$\n我们写出类拉格朗日鞍点形式\n$$\\min_{x}\\left[\\sup_{\\theta}\\left\\{\\theta^\\top(Ax - y) - \\tfrac{1}{2}\\|\\theta\\|_2^2\\right\\} + \\lambda\\|x\\|_1\\right] = \\sup_{\\theta}\\left\\{-\\tfrac{1}{2}\\|\\theta\\|_2^2 - y^\\top \\theta + \\min_{x}\\left[\\theta^\\top A x + \\lambda\\|x\\|_1\\right]\\right\\}。$$\n内部的最小值是有限的当且仅当 $\\|A^\\top \\theta\\|_\\infty \\le \\lambda$，此时其值为 $0$；否则为 $-\\infty$。因此，对偶问题是\n$$\\max_{\\theta \\in \\mathbb{R}^m}\\; d(\\theta) \\triangleq -\\tfrac{1}{2}\\|\\theta\\|_2^2 - y^\\top \\theta \\quad \\text{subject to} \\quad \\|A^\\top \\theta\\|_\\infty \\le \\lambda。$$\n强对偶性成立，因为 $f$ 和 $g$ 是闭的、正常的、凸的，且 Slater 条件得到满足。对于任意原始变量 $x$ 和对偶可行变量 $\\theta$，我们有 $d(\\theta) \\le F(x)$，并且对偶间隙\n$$\\mathrm{gap}(x,\\theta) \\triangleq F(x) - d(\\theta) \\ge 0$$\n量化了次优性，并在最优解处等于 $0$。给定 $x^{k}$，我们可以通过重新缩放残差 $r^{k} \\triangleq A x^{k} - y$ 来产生一个可行的 $\\theta^{k}$：\n$$\\theta^{k} \\triangleq \\tau^{k} r^{k}, \\quad \\tau^{k} \\triangleq \\min\\!\\left\\{1, \\frac{\\lambda}{\\|A^\\top r^{k}\\|_\\infty}\\right\\}，$$\n这保证了 $\\|A^\\top \\theta^{k}\\|_\\infty \\le \\lambda$。然后间隙的计算结果为\n$$\\mathrm{gap}(x^{k},\\theta^{k}) = \\tfrac{1}{2}\\|r^{k}\\|_2^2 + \\lambda\\|x^{k}\\|_1 + \\tfrac{1}{2}\\|\\theta^{k}\\|_2^2 + y^\\top \\theta^{k}。$$\n当此值低于 $\\varepsilon_{\\mathrm{abs}}$ 时停止，可在凭证意义上给出一个 $\\varepsilon_{\\mathrm{abs}}$-精确的原始目标值。计算 $\\theta^{k}$ 需要 $A^\\top r^{k}$ 和 $r^{k}$，通常比计算 $\\nabla f(x^{k}) = A^\\top r^{k}$ 所需的多一次与 $A^\\top$ 的乘法。如果 $\\nabla f$ 只是作为一个黑箱提供，而无法单独访问 $A$ 和 $A^\\top$，或者如果模型被推广到最小二乘之外，一个简单的对偶构造可能不可用。\n\n我们现在评估每个选项。\n\n选项 A. 该公式利用了回溯法下 $F(x^{k})$ 的单调性。优点是准确的：它复用了已计算的量并且成本低。局限性也是准确的：在强正则化或条件不佳的情况下，小的相对下降可能在达到真正平稳性之前发生，并且它不保证任何最优性残差。结论：正确。\n\n选项 B. 映射 $G_{L_k}(x^{k})$ 为复合模型正确定义，其范数是一个有原则的平稳性残差，当且仅当复合最优性条件成立时为零。所述优点是正确的。局限性是合理的：它依赖于局部的 $L_k$，并且除非复用，否则可能涉及额外的近端计算。结论：正确。\n\n选项 C. 对偶问题对 LASSO 的陈述是正确的，可行性条件 $\\|A^\\top \\theta\\|_\\infty \\le \\lambda$ 是正确的，并且重新缩放 $\\theta^{k} = \\tau^{k}(A x^{k} - y)$ 产生了一个可行的对偶点。对偶间隙表达式 $\\mathrm{gap}(x^{k},\\theta^{k}) = F(x^{k}) - d(\\theta^{k})$ 展开为给定的形式，并且是一个有效的凭证：如果它 $\\le \\varepsilon$，则 $F(x^{k}) - F^\\star \\le \\varepsilon$。优点是准确的。局限性也是准确的：计算并强制 $\\theta^{k}$ 的可行性需要访问 $A^\\top(Ax^{k} - y)$，并且在广义模型中可能不那么简单。结论：正确。\n\n选项 D. 使用 $\\|\\nabla f(x^{k})\\|_2$ 作为停止测试忽略了非光滑项 $g(x) = \\lambda\\|x\\|_1$。一般而言，复合最优性条件是 $0 \\in \\nabla f(x) + \\partial g(x)$，而不是 $\\nabla f(x) = 0$。在 LASSO 最优解处，通常 $\\nabla f(x^\\star) \\neq 0$，而 $-\\nabla f(x^\\star) \\in \\partial g(x^\\star)$ 通过 $\\ell_1$ 范数的次梯度成立。因此，这个测试可能会产生误导，在真实解处判断为不收敛或收敛到非解的点。“非光滑项不改变一阶平稳性”的说法是错误的。结论：不正确。\n\n选项 E. 对于约束基追踪问题 $\\min_{x}\\|x\\|_1 \\;\\text{s.t.}\\; A x = y$，真实的对偶问题涉及对对偶向量的约束和强对偶性；$\\tfrac{1}{2}\\|A x^{k} - y\\|_2^2$ 仅仅是约束的可行性残差的平方，而不是一个对偶间隙。在惩罚方法中，这个残差与约束违反相关，但如果没有对偶可行点和适当的拉格朗日乘子，它不能证明约束问题的最优性。因此，关于它“是真实对偶间隙的上界”并“证明次优性”的说法是不正确的。结论：不正确。\n\n因此，正确的选项是 A、B 和 C。", "answer": "$$\\boxed{ABC}$$", "id": "2897755"}]}