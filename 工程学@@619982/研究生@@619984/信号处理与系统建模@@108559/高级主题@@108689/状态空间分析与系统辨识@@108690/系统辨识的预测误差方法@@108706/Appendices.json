{"hands_on_practices": [{"introduction": "我们从一个基础练习开始，旨在巩固预测误差方法 (PEM) 的核心思想。通过研究最简单的动态模型之一——有限脉冲响应 (FIR) 模型，您将直接从条件期望的定义中推导出一步超前预测器。这个练习将帮助您理解如何构建 PEM 成本函数 $V_N(\\theta)$，为解决更复杂的系统辨识问题打下坚实的理论基础 [@problem_id:2892835]。", "problem": "考虑一个由 $y_{t}=\\sum_{k=1}^{m} b_{k}\\,u_{t-k}+e_{t}$ 给出的阶数为 $m \\in \\mathbb{N}$ 的单输入单输出有限脉冲响应模型，其中 $\\{u_{t}\\}_{t \\in \\mathbb{Z}}$ 和 $\\{y_{t}\\}_{t \\in \\mathbb{Z}}$ 分别是输入和输出序列，而 $\\{e_{t}\\}_{t \\in \\mathbb{Z}}$ 是一个零均值、具有有限方差且独立于输入的白噪声序列。假设有一批 $N \\in \\mathbb{N}$ 个样本 $\\{(u_{t},y_{t})\\}_{t=1}^{N}$ 可用。未知参数向量为 $\\theta \\triangleq [\\,b_{1}\\ \\cdots\\ b_{m}\\,]^{\\top}$。令 $\\mathcal{F}_{t-1}$ 表示由直到时间 $t-1$ 的过去输入输出数据生成的 $\\sigma$-代数。从单步超前预测器是在给定模型和假设下，$y_{t}$ 关于 $\\mathcal{F}_{t-1}$ 的条件期望这一定义出发，为预测器 $\\hat{y}_{t}(\\theta) \\triangleq \\mathbb{E}[\\,y_{t}\\mid \\mathcal{F}_{t-1}\\,]$ 推导一个显式表达式，该表达式需对所有能从观测数据中获得全部回归量的时间索引 $t$ 成立。接着，使用预测误差法 (PEM) 代价函数的定义，即其为白新息的单步超前预测误差平方的样本均值，写出待对 $b_{1},\\dots,b_{m}$ 最小化的显式有限样本 PEM 代价函数 $V_{N}(\\theta)$，并明确指定为避免未知样本前数据所用的索引范围。将您的最终答案表示为 $\\hat{y}_{t}(\\theta)$ 和 $V_{N}(\\theta)$ 的显式公式。无需进行数值计算，也无需进行舍入。", "solution": "该问题要求针对一个指定的有限脉冲响应 (FIR) 模型，推导其单步超前预测器和相关的预测误差法 (PEM) 代价函数。对问题陈述进行严格验证是一个强制性的先决条件。\n\n首先，我们必须逐字提取给定信息。\n模型由 $y_{t}=\\sum_{k=1}^{m} b_{k}\\,u_{t-k}+e_{t}$ 给出，其阶数为 $m \\in \\mathbb{N}$。\n参数向量为 $\\theta \\triangleq [\\,b_{1}\\ \\cdots\\ b_{m}\\,]^{\\top}$。\n涉及的信号是输入序列 $\\{u_{t}\\}_{t \\in \\mathbb{Z}}$ 和输出序列 $\\{y_{t}\\}_{t \\in \\mathbb{Z}}$。\n噪声序列 $\\{e_{t}\\}_{t \\in \\mathbb{Z}}$ 被规定为一个零均值、具有有限方差且独立于输入序列的白噪声序列。\n可用数据由一批 $N \\in \\mathbb{N}$ 个样本组成，记作 $\\{(u_{t},y_{t})\\}_{t=1}^{N}$。\n信息集 $\\mathcal{F}_{t-1}$ 被定义为由直到时间 $t-1$ 的过去输入输出数据生成的 $\\sigma$-代数。\n单步超前预测器定义为 $\\hat{y}_{t}(\\theta) \\triangleq \\mathbb{E}[\\,y_{t}\\mid \\mathcal{F}_{t-1}\\,]$。\nPEM 代价函数 $V_{N}(\\theta)$ 被定义为单步超前预测误差平方的样本均值。\n\n我们对问题陈述的有效性进行了审查。它具有科学依据，牢固地建立在系统辨识的标准理论框架内。其模型和假设是规范的。该问题是适定的，提供了所有必要的定义和数据以得到唯一且有意义的解。要求明确索引范围以处理有限数据集是问题不可或缺的一部分，而非不完备的标志。该问题是应用预测理论和统计信号处理第一性原理的一个基本练习。因此，该问题被认定为有效，并将着手构建解答。\n\n我们开始推导单步超前预测器 $\\hat{y}_{t}(\\theta)$。根据其定义：\n$$\n\\hat{y}_{t}(\\theta) = \\mathbb{E}[\\,y_{t}\\mid \\mathcal{F}_{t-1}\\,]\n$$\n将给定的 $y_{t}$ 模型代入：\n$$\n\\hat{y}_{t}(\\theta) = \\mathbb{E}\\left[\\,\\sum_{k=1}^{m} b_{k}\\,u_{t-k}+e_{t} \\,\\middle|\\, \\mathcal{F}_{t-1}\\,\\right]\n$$\n利用条件期望的线性性质，该表达式可分为两项：\n$$\n\\hat{y}_{t}(\\theta) = \\mathbb{E}\\left[\\,\\sum_{k=1}^{m} b_{k}\\,u_{t-k} \\,\\middle|\\, \\mathcal{F}_{t-1}\\,\\right] + \\mathbb{E}[\\,e_{t} \\mid \\mathcal{F}_{t-1}\\,]\n$$\n我们来分析第一项。参数 $b_{k}$ 是向量 $\\theta$ 的确定性分量。根据定义，对于 $k \\in \\{1, 2, \\dots, m\\}$ 的输入 $u_{t-k}$ 是在时间 $t-1$ 或之前发生的事件。因此，它们关于 $\\sigma$-代数 $\\mathcal{F}_{t-1}$ 是可测的。所以，整个和 $\\sum_{k=1}^{m} b_{k}\\,u_{t-k}$ 是一个 $\\mathcal{F}_{t-1}$-可测的量。一个 $\\mathcal{F}_{t-1}$-可测变量在给定 $\\mathcal{F}_{t-1}$ 下的条件期望是该变量本身。\n$$\n\\mathbb{E}\\left[\\,\\sum_{k=1}^{m} b_{k}\\,u_{t-k} \\,\\middle|\\, \\mathcal{F}_{t-1}\\,\\right] = \\sum_{k=1}^{m} b_{k}\\,u_{t-k}\n$$\n现在考虑第二项 $\\mathbb{E}[\\,e_{t} \\mid \\mathcal{F}_{t-1}\\,]$。噪声 $e_{t}$ 被假设为一个白噪声序列，意味着它与其过去值 $\\{e_{s}\\}_{s<t}$ 不相关。它也独立于整个输入序列 $\\{u_{s}\\}_{s \\in \\mathbb{Z}}$。信息集 $\\mathcal{F}_{t-1}$ 由过去的输入 $\\{u_{s}\\}_{s \\le t-1}$ 和过去的输出 $\\{y_{s}\\}_{s \\le t-1}$ 生成。由于每个过去的输出 $y_{s}$ 是过去输入和过去噪声项 $e_{s}$ 的函数，$\\sigma$-代数 $\\mathcal{F}_{t-1}$ 仅包含关于 $\\{u_{s}\\}_{s \\le t-1}$ 和 $\\{e_{s}\\}_{s \\le t-1}$ 的信息。因为 $e_{t}$ 独立于这两组随机变量，所以它独立于 $\\sigma$-代数 $\\mathcal{F}_{t-1}$。一个随机变量在给定一个独立 $\\sigma$-代数下的条件期望是它的无条件期望。\n$$\n\\mathbb{E}[\\,e_{t} \\mid \\mathcal{F}_{t-1}\\,] = \\mathbb{E}[\\,e_{t}\\,]\n$$\n如问题所述，$\\{e_{t}\\}$ 是一个零均值序列，所以 $\\mathbb{E}[\\,e_{t}\\,] = 0$。\n结合这些结果，得到预测器的最终表达式：\n$$\n\\hat{y}_{t}(\\theta) = \\sum_{k=1}^{m} b_{k}\\,u_{t-k}\n$$\n接下来，我们推导 PEM 代价函数 $V_{N}(\\theta)$。单步超前预测误差定义为 $\\varepsilon_{t}(\\theta) = y_{t} - \\hat{y}_{t}(\\theta)$。使用我们推导出的预测器，我们有：\n$$\n\\varepsilon_{t}(\\theta) = y_{t} - \\sum_{k=1}^{m} b_{k}\\,u_{t-k}\n$$\n问题将代价函数定义为预测误差平方的样本均值。求和必须在所有量（$y_{t}$ 和回归量 $u_{t-1}, \\dots, u_{t-m}$）在给定数据集 $\\{(u_{t},y_{t})\\}_{t=1}^{N}$ 内都可用的时间索引 $t$ 上进行。时间索引最低的回归量是 $u_{t-m}$。为了使其在我们的数据记录中，必须有 $t-m \\ge 1$，这意味着 $t \\ge m+1$。我们拥有输出 $y_{t}$ 的最晚时间是 $t=N$。因此，求和的范围必须从 $t=m+1$ 到 $t=N$。这仅在 $N \\ge m+1$ 时有效。此求和中的项数为 $N - (m+1) + 1 = N-m$。\n因此，代价函数 $V_N(\\theta)$ 为：\n$$\nV_{N}(\\theta) = \\frac{1}{N-m} \\sum_{t=m+1}^{N} \\left( \\varepsilon_{t}(\\theta) \\right)^2 = \\frac{1}{N-m} \\sum_{t=m+1}^{N} \\left( y_{t} - \\sum_{k=1}^{m} b_{k}\\,u_{t-k} \\right)^2\n$$\n推导至此完成。结果提供了所要求的显式公式。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\hat{y}_{t}(\\theta) = \\sum_{k=1}^{m} b_{k}u_{t-k} & V_{N}(\\theta) = \\frac{1}{N-m} \\sum_{t=m+1}^{N} \\left( y_{t} - \\sum_{k=1}^{m} b_{k}u_{t-k} \\right)^{2}\n\\end{pmatrix}\n}\n$$", "id": "2892835"}, {"introduction": "在掌握了基础知识之后，我们将处理一个更具表现力也更复杂的模型：带有外生输入的自回归移动平均 (ARMAX) 模型。本练习介绍了一种强大的工具——多项式丢番图方程，它是推导 ARMAX 模型最优预测器的关键 [@problem_id:2892827]。通过结合理论推导与具体的数值计算，您将学会如何递归地计算预测误差 $\\epsilon_t(\\theta)$，从而将抽象的理论与实际应用联系起来。", "problem": "给定一个单输入单输出的带外生输入的自回归移动平均 (AutoRegressive Moving Average with eXogenous input, ARMAX) 模型，用后向移位算子表示为\n$$\nA(q,\\theta)\\,y_t \\;=\\; B(q,\\theta)\\,u_t \\;+\\; C(q,\\theta)\\,e_t,\n$$\n其中 $q^{-1}$ 表示单位延迟算子，$A(q,\\theta)$ 和 $C(q,\\theta)$ 是关于 $q^{-1}$ 的首一多项式，其所有零点严格位于单位圆盘内部，$B(q,\\theta)$ 是关于 $q^{-1}$ 的有限多项式，而 $\\{e_t\\}$ 是一个均值为零、方差有限的白噪声序列，且独立于 $\\{u_t\\}$。假设 $A(q,\\theta)$ 和 $C(q,\\theta)$ 互质。单步超前预测器 $\\hat{y}_{t|t-1}(\\theta)$ 定义为条件期望 $\\mathbb{E}\\!\\left[y_t \\mid \\mathcal{F}_{t-1}\\right]$，其中 $\\mathcal{F}_{t-1}$ 是由 $\\{y_{t-k},u_{t-k}\\}_{k\\ge 1}$ 生成的 $\\sigma$-代数。\n\n任务：\n1) 从上述模型和条件期望的定义出发，使用一个多项式丢番图方程推导出一个严格因果的单步超前预测器 $\\hat{y}_{t|t-1}(\\theta)$，该预测器仅依赖于过去的测量数据和先前计算的预测误差。具体而言，找到满足 $A(q,\\theta)$ 和 $C(q,\\theta)$ 之间适当丢番图关系的多项式 $\\Gamma(q,\\theta)$ 和 $\\Delta(q,\\theta)$（均为 $q^{-1}$ 的多项式），并用它来表示 $\\hat{y}_{t|t-1}(\\theta)$（用 $u_t$、$y_t$ 和过去的新息表示）。清晰地陈述你为确保因果性和唯一性而施加的阶数条件。\n2) 根据你的预测器，展示如何仅使用过去的 $\\epsilon$ 值以及当前和过去的输入与输出来递归地计算预测误差 $\\epsilon_t(\\theta) \\equiv y_t - \\hat{y}_{t|t-1}(\\theta)$；假设多项式参数化为 $A(q,\\theta)=1+\\sum_{k=1}^{n_a} a_k q^{-k}$，$B(q,\\theta)=\\sum_{k=0}^{n_b} b_k q^{-k}$ 和 $C(q,\\theta)=1+\\sum_{k=1}^{n_c} c_k q^{-k}$，明确地写出这个关于 $A(q,\\theta)$、$B(q,\\theta)$ 和 $C(q,\\theta)$ 系数的递归式。\n3) 考虑以下给定的特定参数矢量 $\\theta$ 和短数据记录：\n- 模型：$A(q,\\theta)=1-1.2\\,q^{-1}+0.32\\,q^{-2}$，$B(q,\\theta)=0.5+0.1\\,q^{-1}$，$C(q,\\theta)=1+0.5\\,q^{-1}$。\n- 输入：$u_{-1}=0$，$u_0=1.0$，$u_1=-0.5$，$u_2=0.0$，$u_3=0.25$。\n- 输出：$y_{-2}=0$，$y_{-1}=0$，$y_0=0.8$，$y_1=-0.4$，$y_2=0.5$，$y_3=0.1$。\n假设预测器递归处于初始静止状态，即对于所有 $t<0$，$\\epsilon_t(\\theta)=0$。使用你在第 2 部分得到的递归式，计算 $\\epsilon_3(\\theta)$ 的数值。答案四舍五入到四位有效数字。", "solution": "此问题要求对 ARMAX 模型的单步超前预测器进行严谨推导，然后进行数值计算。问题提法合理且科学上严谨。我将按顺序处理每个部分。为清晰起见，除非出现歧义，否则在符号中将省略多项式对参数矢量 $\\theta$ 的依赖，例如，使用 $A(q)$ 而不是 $A(q,\\theta)$。\n\n**第 1 部分：单步超前预测器的推导**\n\nARMAX 模型由以下随机差分方程给出：\n$$\nA(q) y_t = B(q) u_t + C(q) e_t\n$$\n其中 $A(q)$ 和 $C(q)$ 是后向移位算子 $q^{-1}$ 的首一多项式，其零点位于单位圆盘内部，$\\{e_t\\}$ 是一个均值为零的白噪声过程。我们寻求单步超前预测器 $\\hat{y}_{t|t-1}$，其定义为条件期望 $\\mathbb{E}[y_t | \\mathcal{F}_{t-1}]$，其中 $\\mathcal{F}_{t-1}$ 是由过去的输入和输出 $\\{y_{t-k}, u_{t-k}\\}_{k \\ge 1}$ 生成的 $\\sigma$-代数。按照标准惯例，我们假设输入序列 $\\{u_t\\}$ 是已知的。\n\n首先，我们将 $y_t$ 表示为驱动信号 $u_t$ 和 $e_t$ 的函数。由于 $A(q)$ 的零点位于单位圆盘内部，其逆 $A(q)^{-1}$ 对应一个稳定且因果的滤波器。\n$$\ny_t = \\frac{B(q)}{A(q)} u_t + \\frac{C(q)}{A(q)} e_t\n$$\n为了将 $y_t$ 的可预测部分与不可预测的新息 $e_t$ 分离开，我们使用多项式丢番图方程来分解有理函数 $C(q)/A(q)$。对于单步超前预测器，该方程为：\n$$\nC(q) = A(q) \\Gamma(q) + q^{-1} \\Delta(q)\n$$\n这里，$\\Gamma(q)$ 和 $\\Delta(q)$ 是 $q^{-1}$ 的多项式。为了确保解的唯一性，我们必须施加阶数约束。对于单步预测域 ($d=1$)，我们要求 $\\Gamma(q)$ 的阶数为 $\\deg(\\Gamma) = 1-1=0$。由于 $A(q)$ 和 $C(q)$ 都是首一多项式（首项系数为 1），因此 $\\Gamma(q) = 1$ 是必要的。这意味着丢番图方程简化为：\n$$\nC(q) = A(q) \\cdot 1 + q^{-1} \\Delta(q)\n$$\n由此，我们解出 $\\Delta(q)$：\n$$\n\\Delta(q) = q(C(q) - A(q))\n$$\n因为 $A(q)$ 和 $C(q)$ 是首一的，多项式 $C(q)-A(q)$ 的常数项为 $1-1=0$，这意味着它的形式为 $(c_1-a_1)q^{-1} + (c_2-a_2)q^{-2} + \\dots$。因此，$\\Delta(q) = q(C(q)-A(q))$ 是一个关于 $q^{-1}$ 的有限多项式。$\\Delta(q)$ 的阶数为 $\\max(\\deg A, \\deg C) - 1$。这证实了我们分解的因果性和唯一性。\n\n现在，我们将丢番图恒等式代入 $y_t$ 的表达式中：\n$$\ny_t = \\frac{B(q)}{A(q)} u_t + \\frac{A(q)\\Gamma(q) + q^{-1}\\Delta(q)}{A(q)} e_t\n$$\n当 $\\Gamma(q)=1$ 时，上式变为：\n$$\ny_t = \\frac{B(q)}{A(q)} u_t + e_t + \\frac{q^{-1}\\Delta(q)}{A(q)} e_t\n$$\n项 $e_t$ 是在时间 $t$ 的新息。根据定义，给定过去的信息 $\\mathcal{F}_{t-1}$，它是不可预测的，所以 $\\mathbb{E}[e_t | \\mathcal{F}_{t-1}] = 0$。其他项是 $e_t$ 的过去值（即 $\\{e_{t-k}\\}_{k \\ge 1}$）以及 $u_t$ 的当前和过去值的函数。由于假设序列 $\\{u_t\\}$ 是已知的，并且过去的新息 $\\{e_{t-k}\\}_{k \\ge 1}$ 相对于 $\\mathcal{F}_{t-1}$ 是可测的（因为它们可以由过去的输入和输出构造出来，假设 $C(q)$ 可逆），这些项是可预测的。\n\n因此，取条件期望得到预测器：\n$$\n\\hat{y}_{t|t-1} = \\mathbb{E}[y_t | \\mathcal{F}_{t-1}] = \\frac{B(q)}{A(q)} u_t + \\frac{q^{-1}\\Delta(q)}{A(q)} e_t\n$$\n这个表达式依赖于不可测量的噪声序列 $\\{e_t\\}$。为了得到一个实用的形式，我们必须用可测量的量来表示 $e_t$。从原始 ARMAX 模型可知，$C(q)e_t = A(q)y_t - B(q)u_t$。由于 $C(q)$ 是稳定且可逆的，我们有 $e_t = C(q)^{-1}(A(q)y_t - B(q)u_t)$。将此代入预测器方程得到：\n$$\n\\hat{y}_{t|t-1} = \\frac{B(q)}{A(q)} u_t + \\frac{q^{-1}\\Delta(q)}{A(q)} \\left( \\frac{A(q)}{C(q)}y_t - \\frac{B(q)}{C(q)}u_t \\right) = \\frac{B(q)}{A(q)} u_t + \\frac{q^{-1}\\Delta(q)}{C(q)} y_t - \\frac{q^{-1}\\Delta(q)B(q)}{A(q)C(q)} u_t\n$$\n合并包含 $u_t$ 的项：\n$$\n\\hat{y}_{t|t-1} = \\left( \\frac{B(q)}{A(q)} - \\frac{q^{-1}\\Delta(q)B(q)}{A(q)C(q)} \\right) u_t + \\frac{q^{-1}\\Delta(q)}{C(q)} y_t\n$$\n代入 $q^{-1}\\Delta(q) = C(q) - A(q)$：\n$$\n\\hat{y}_{t|t-1} = \\left( \\frac{B(q)C(q) - (C(q)-A(q))B(q)}{A(q)C(q)} \\right) u_t + \\frac{C(q)-A(q)}{C(q)} y_t\n$$\n括号中的项可以简化：$B(q)C(q) - C(q)B(q) + A(q)B(q) = A(q)B(q)$。\n$$\n\\hat{y}_{t|t-1} = \\frac{A(q)B(q)}{A(q)C(q)} u_t + \\frac{C(q)-A(q)}{C(q)} y_t = \\frac{B(q)}{C(q)} u_t + \\left(1 - \\frac{A(q)}{C(q)}\\right) y_t\n$$\n这是单步超前预测器的最终表达式。它可以递归地实现为：\n$$\nC(q)\\hat{y}_{t|t-1} = B(q)u_t + (C(q)-A(q))y_t\n$$\n这种形式仅依赖于过去的输出、当前和过去的输入以及过去的预测器值，使其在使用测量输出方面是严格因果的。\n\n**第 2 部分：预测误差的递归公式**\n\n预测误差定义为 $\\epsilon_t(\\theta) = y_t - \\hat{y}_{t|t-1}(\\theta)$。从第 1 部分推导的预测器，我们有：\n$$\nC(q)\\hat{y}_{t|t-1} = B(q)u_t + (C(q)-A(q))y_t\n$$\n代入 $\\hat{y}_{t|t-1} = y_t - \\epsilon_t$：\n$$\nC(q)(y_t - \\epsilon_t) = B(q)u_t + C(q)y_t - A(q)y_t\n$$\n展开左侧：\n$$\nC(q)y_t - C(q)\\epsilon_t = B(q)u_t + C(q)y_t - A(q)y_t\n$$\n$C(q)y_t$ 项在两边消去，剩下：\n$$\n-C(q)\\epsilon_t = B(q)u_t - A(q)y_t\n$$\n或\n$$\nC(q)\\epsilon_t = A(q)y_t - B(q)u_t\n$$\n这表明预测误差 $\\epsilon_t$ 可以通过对输入 $u_t$ 和输出 $y_t$ 进行滤波得到。请注意，如果参数 $\\theta$ 是真实参数，$\\epsilon_t(\\theta)$ 就成为真实的新息 $e_t$，此时这个方程仅仅是原始 ARMAX 模型的重新整理。\n\n为了获得一个显式的递归公式，我们写出多项式：\n$A(q) = 1+\\sum_{k=1}^{n_a} a_k q^{-k}$，$B(q) = \\sum_{k=0}^{n_b} b_k q^{-k}$，和 $C(q) = 1+\\sum_{k=1}^{n_c} c_k q^{-k}$。\n方程 $C(q)\\epsilon_t = A(q)y_t - B(q)u_t$ 变为：\n$$\n\\left(1+\\sum_{k=1}^{n_c} c_k q^{-k}\\right)\\epsilon_t = \\left(1+\\sum_{k=1}^{n_a} a_k q^{-k}\\right)y_t - \\left(\\sum_{k=0}^{n_b} b_k q^{-k}\\right)u_t\n$$\n应用移位算子：\n$$\n\\epsilon_t + \\sum_{k=1}^{n_c} c_k \\epsilon_{t-k} = y_t + \\sum_{k=1}^{n_a} a_k y_{t-k} - \\sum_{k=0}^{n_b} b_k u_{t-k}\n$$\n分离出 $\\epsilon_t$ 得到所需的递归式：\n$$\n\\epsilon_t = y_t + \\sum_{k=1}^{n_a} a_k y_{t-k} - \\sum_{k=0}^{n_b} b_k u_{t-k} - \\sum_{k=1}^{n_c} c_k \\epsilon_{t-k}\n$$\n这个公式允许使用当前输出 $y_t$、当前和过去的输入 $u_t, u_{t-1}, \\dots$、过去的输出 $y_{t-1}, \\dots$ 和过去的预测误差 $\\epsilon_{t-1}, \\dots$ 来计算 $\\epsilon_t$。\n\n**第 3 部分：数值计算**\n\n给定具体的模型参数：\n$A(q) = 1-1.2\\,q^{-1}+0.32\\,q^{-2} \\implies n_a=2, a_1 = -1.2, a_2 = 0.32$\n$B(q) = 0.5+0.1\\,q^{-1} \\implies n_b=1, b_0 = 0.5, b_1 = 0.1$\n$C(q) = 1+0.5\\,q^{-1} \\implies n_c=1, c_1 = 0.5$\n\n$\\epsilon_t$ 的递归式为：\n$$\n\\epsilon_t = y_t + a_1 y_{t-1} + a_2 y_{t-2} - (b_0 u_t + b_1 u_{t-1}) - c_1 \\epsilon_{t-1}\n$$\n$$\n\\epsilon_t = y_t - 1.2 y_{t-1} + 0.32 y_{t-2} - (0.5 u_t + 0.1 u_{t-1}) - 0.5 \\epsilon_{t-1}\n$$\n我们有数据和初始条件 $\\epsilon_t=0$ for $t<0$。我们必须计算 $\\epsilon_3$。这需要按顺序计算 $\\epsilon_0, \\epsilon_1, \\epsilon_2$。\n\n对于 $t=0$:\n$\\epsilon_0 = y_0 - 1.2 y_{-1} + 0.32 y_{-2} - (0.5 u_0 + 0.1 u_{-1}) - 0.5 \\epsilon_{-1}$\n$\\epsilon_0 = 0.8 - 1.2(0) + 0.32(0) - (0.5(1.0) + 0.1(0)) - 0.5(0)$\n$\\epsilon_0 = 0.8 - 0.5 = 0.3$\n\n对于 $t=1$:\n$\\epsilon_1 = y_1 - 1.2 y_{0} + 0.32 y_{-1} - (0.5 u_1 + 0.1 u_{0}) - 0.5 \\epsilon_{0}$\n$\\epsilon_1 = -0.4 - 1.2(0.8) + 0.32(0) - (0.5(-0.5) + 0.1(1.0)) - 0.5(0.3)$\n$\\epsilon_1 = -0.4 - 0.96 - (-0.25 + 0.1) - 0.15$\n$\\epsilon_1 = -1.36 - (-0.15) - 0.15 = -1.36$\n\n对于 $t=2$:\n$\\epsilon_2 = y_2 - 1.2 y_{1} + 0.32 y_{0} - (0.5 u_2 + 0.1 u_{1}) - 0.5 \\epsilon_{1}$\n$\\epsilon_2 = 0.5 - 1.2(-0.4) + 0.32(0.8) - (0.5(0.0) + 0.1(-0.5)) - 0.5(-1.36)$\n$\\epsilon_2 = 0.5 + 0.48 + 0.256 - (-0.05) + 0.68$\n$\\epsilon_2 = 0.5 + 0.48 + 0.256 + 0.05 + 0.68 = 1.966$\n\n最后，对于 $t=3$:\n$\\epsilon_3 = y_3 - 1.2 y_{2} + 0.32 y_{1} - (0.5 u_3 + 0.1 u_{2}) - 0.5 \\epsilon_{2}$\n$\\epsilon_3 = 0.1 - 1.2(0.5) + 0.32(-0.4) - (0.5(0.25) + 0.1(0.0)) - 0.5(1.966)$\n$\\epsilon_3 = 0.1 - 0.6 - 0.128 - (0.125) - 0.983$\n$\\epsilon_3 = -0.5 - 0.128 - 0.125 - 0.983$\n$\\epsilon_3 = -0.628 - 0.125 - 0.983 = -1.736$\n\n计算出的值为 $\\epsilon_3 = -1.736$。此结果已按要求保留四位有效数字。", "answer": "$$\n\\boxed{-1.736}\n$$", "id": "2892827"}, {"introduction": "定义了模型和成本函数后，下一个关键问题是：我们如何实际找到最小化成本的参数 $\\theta$？这个实践练习深入探讨了 PEM 的数值优化核心。您将推导高斯-牛顿法的参数更新规则，这是一种用于非线性最小二乘问题的经典算法，并通过一个具体的数值算例来实践一个完整的迭代步骤，包括如何利用 strong Wolfe 条件进行步长选择，从而确保算法的收敛性 [@problem_id:2892776]。", "problem": "考虑一个通过预测误差法 (PEM) 辨识的单输入单输出 (SISO) 输出误差模型。令采样索引为 $t$ 时的预测误差定义为 $\\epsilon(t,\\theta) = y(t) - \\hat{y}(t,\\theta)$，其中 $\\hat{y}(t,\\theta)$ 是在给定输入 $u(t)$ 和参数矢量 $\\theta \\in \\mathbb{R}^{n_{\\theta}}$ 时的模型输出。$N$ 个采样的 PEM 代价函数为 $V_{N}(\\theta) = \\frac{1}{2N} \\sum_{t=1}^{N} \\epsilon(t,\\theta)^{2}$。从 $V_{N}(\\theta)$ 的最小二乘结构和堆叠残差矢量在当前迭代点附近的一阶泰勒展开出发，推导 Gauss-Newton 方法使用的搜索方向及相关的参数更新法则。然后，解释强 Wolfe 型线搜索条件如何通过强制满足充分下降和可接受曲率来确保下降方向的下降特性。\n\n接下来，特化为一个参数线性的静态模型 $\\hat{y}(t,\\theta) = \\theta_{1} u(t) + \\theta_{2}$，其中 $\\theta = \\begin{pmatrix}\\theta_{1} & \\theta_{2}\\end{pmatrix}^{\\top}$。给定 $N=3$ 个样本\n$$(u(1),y(1)) = (0, -0.45), \\quad (u(2),y(2)) = (1, 1.48), \\quad (u(3),y(3)) = (2, 3.51),$$\n以及初始参数 $\\theta^{0} = \\begin{pmatrix}1.0 & 0.0\\end{pmatrix}^{\\top}$。计算在 $\\theta^{0}$ 处的 Gauss-Newton 方向 $p$，定义 $\\phi(\\alpha) = V_{N}(\\theta^{0} + \\alpha p)$，并使用强 Wolfe 条件（其中 $c_{1} = 10^{-4}$ 和 $c_{2} = 0.9$）从候选集 $\\{\\;1,\\; \\tfrac{1}{2},\\; \\tfrac{1}{4}\\;\\}$ 中选择一个步长 $\\alpha$。报告该集合中同时满足充分下降条件和曲率条件的最大 $\\alpha$。将最终数值答案四舍五入到四位有效数字。", "solution": "所提出的问题是应用于系统辨识的非线性优化中的一个标准练习，该问题是适定的且科学上是合理的。我将首先提供所要求的一般推导和解释，然后进行具体的数值计算。\n\n预测误差法 (PEM) 旨在找到使基于预测误差 $\\epsilon(t,\\theta) = y(t) - \\hat{y}(t,\\theta)$ 的代价函数最小化的参数矢量 $\\theta$。代价函数以误差平方和的形式给出：\n$$V_{N}(\\theta) = \\frac{1}{2N} \\sum_{t=1}^{N} \\epsilon(t,\\theta)^{2}$$\n这可以通过定义堆叠残差矢量 $\\mathbf{\\epsilon}(\\theta) \\in \\mathbb{R}^{N}$（其分量为 $\\epsilon(t, \\theta)$，$t=1, \\dots, N$）写成矢量形式。代价函数变为：\n$$V_{N}(\\theta) = \\frac{1}{2N} \\mathbf{\\epsilon}(\\theta)^{\\top} \\mathbf{\\epsilon}(\\theta)$$\n\nGauss-Newton 法是解决非线性最小二乘问题的一种迭代算法。在每次迭代 $k$ 中，我们寻找一个搜索方向 $p^k$ 来更新当前的参数估计 $\\theta^k$，即 $\\theta^{k+1} = \\theta^k + \\alpha_k p^k$，其中 $\\alpha_k$ 是一个步长。方向 $p^k$ 是通过在 $\\theta^k$ 附近线性化残差矢量 $\\mathbf{\\epsilon}(\\theta)$ 并最小化所得到的代价函数的二次近似来找到的。\n\n$\\mathbf{\\epsilon}(\\theta)$ 在 $\\theta^k$ 附近的一阶泰勒展开为：\n$$\\mathbf{\\epsilon}(\\theta^k + p) \\approx \\mathbf{\\epsilon}(\\theta^k) + J(\\theta^k) p$$\n其中 $p = \\theta - \\theta^k$ 是步长，而 $J(\\theta^k)$ 是在 $\\theta^k$ 处计算的残差矢量的雅可比矩阵。其元素为 $[J(\\theta^k)]_{ti} = \\frac{\\partial \\epsilon(t,\\theta)}{\\partial \\theta_i} \\Big|_{\\theta=\\theta^k}$。\n\n将此线性化代入代价函数，得到一个近似代价函数 $\\tilde{V}_{N}(p)$：\n$$\\tilde{V}_{N}(p) = \\frac{1}{2N} \\left( \\mathbf{\\epsilon}(\\theta^k) + J(\\theta^k) p \\right)^{\\top} \\left( \\mathbf{\\epsilon}(\\theta^k) + J(\\theta^k) p \\right)$$\n为了找到最小化此二次函数的最优步长 $p$，我们将其关于 $p$ 的梯度设为零：\n$$\\nabla_{p} \\tilde{V}_{N}(p) = \\frac{1}{N} J(\\theta^k)^{\\top} \\left( \\mathbf{\\epsilon}(\\theta^k) + J(\\theta^k) p \\right) = 0$$\n重新整理此方程可得到 Gauss-Newton 法方程：\n$$ \\left( J(\\theta^k)^{\\top} J(\\theta^k) \\right) p^k = - J(\\theta^k)^{\\top} \\mathbf{\\epsilon}(\\theta^k)$$\nGauss-Newton 搜索方向 $p^k$ 是此线性系统的解：\n$$p^k = - \\left( J(\\theta^k)^{\\top} J(\\theta^k) \\right)^{-1} J(\\theta^k)^{\\top} \\mathbf{\\epsilon}(\\theta^k)$$\n相关的参数更新法则是 $\\theta^{k+1} = \\theta^k + \\alpha_k p^k$，其中 $\\alpha_k > 0$ 是由线搜索过程确定的步长。\n\n线搜索旨在找到一个合适的 $\\alpha_k$，以确保向最小值前进。强 Wolfe 条件为可接受的步长 $\\alpha$ 提供了一套标准准则。对于一个给定的下降方向 $p$，我们定义一个一维函数 $\\phi(\\alpha) = V_N(\\theta^k + \\alpha p)$。强 Wolfe 条件是：\n1.  **充分下降 (Armijo) 条件**:\n    $$V_N(\\theta^k + \\alpha p^k) \\le V_N(\\theta^k) + c_1 \\alpha \\nabla V_N(\\theta^k)^{\\top} p^k$$\n    其中 $c_1 \\in (0,1)$ 是一个常数。由于 $p^k$ 是一个下降方向，因此 $\\nabla V_N(\\theta^k)^{\\top} p^k < 0$。此条件要求代价函数的实际减少量至少是在 $\\alpha=0$ 处由一阶近似预测的减少量的一小部分 $c_1$。它防止步长过大而只能提供极小的改进。\n\n2.  **强曲率条件**:\n    $$|\\nabla V_N(\\theta^k + \\alpha p^k)^{\\top} p^k| \\le c_2 |\\nabla V_N(\\theta^k)^{\\top} p^k|$$\n    其中 $c_2 \\in (c_1, 1)$ 是一个常数。此条件确保在新点 $\\theta^k + \\alpha p^k$ 处方向导数的量值显著小于在起始点 $\\theta^k$ 处的量值。它防止步长过小，因为非常小的步长往往会落入斜率尚未充分平坦化的区域。绝对值也防止了步长进入方向导数变得很大且为正的区域，这表明可能已经越过了最小值点。\n\n现在，我们特化到给定的参数线性静态模型：$\\hat{y}(t,\\theta) = \\theta_{1} u(t) + \\theta_{2}$。\n参数矢量为 $\\theta = \\begin{pmatrix}\\theta_1 & \\theta_2\\end{pmatrix}^\\top$。\n预测误差为 $\\epsilon(t, \\theta) = y(t) - (\\theta_1 u(t) + \\theta_2)$。\n数据包含 $N=3$ 个样本：$(u(1),y(1)) = (0, -0.45)$, $(u(2),y(2)) = (1, 1.48)$, $(u(3),y(3)) = (2, 3.51)$。\n初始参数估计为 $\\theta^0 = \\begin{pmatrix}1.0 & 0.0\\end{pmatrix}^\\top$。\n\n首先，我们计算雅可比矩阵 $J$。误差的偏导数是：\n$$\\frac{\\partial \\epsilon(t,\\theta)}{\\partial \\theta_1} = -u(t), \\quad \\frac{\\partial \\epsilon(t,\\theta)}{\\partial \\theta_2} = -1$$\n对于这个线性模型，雅可比矩阵 $J$ 是常数：\n$$J = \\begin{pmatrix} -u(1) & -1 \\\\ -u(2) & -1 \\\\ -u(3) & -1 \\end{pmatrix} = \\begin{pmatrix} 0 & -1 \\\\ -1 & -1 \\\\ -2 & -1 \\end{pmatrix}$$\n接下来，我们计算在初始点 $\\theta^0$ 的残差矢量 $\\mathbf{\\epsilon}(\\theta^0)$：\n$$\\epsilon(1,\\theta^0) = -0.45 - (1.0 \\cdot 0 + 0.0) = -0.45$$\n$$\\epsilon(2,\\theta^0) = 1.48 - (1.0 \\cdot 1 + 0.0) = 0.48$$\n$$\\epsilon(3,\\theta^0) = 3.51 - (1.0 \\cdot 2 + 0.0) = 1.51$$\n因此，$\\mathbf{\\epsilon}(\\theta^0) = \\begin{pmatrix}-0.45 & 0.48 & 1.51\\end{pmatrix}^\\top$。\n\n为了找到 Gauss-Newton 方向 $p$，我们计算 $J^\\top J$ 和 $J^\\top \\mathbf{\\epsilon}(\\theta^0)$：\n$$J^\\top J = \\begin{pmatrix} 0 & -1 & -2 \\\\ -1 & -1 & -1 \\end{pmatrix} \\begin{pmatrix} 0 & -1 \\\\ -1 & -1 \\\\ -2 & -1 \\end{pmatrix} = \\begin{pmatrix} 5 & 3 \\\\ 3 & 3 \\end{pmatrix}$$\n$$(J^\\top J)^{-1} = \\frac{1}{5 \\cdot 3 - 3 \\cdot 3} \\begin{pmatrix} 3 & -3 \\\\ -3 & 5 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 3 & -3 \\\\ -3 & 5 \\end{pmatrix}$$\n$$J^\\top \\mathbf{\\epsilon}(\\theta^0) = \\begin{pmatrix} 0 & -1 & -2 \\\\ -1 & -1 & -1 \\end{pmatrix} \\begin{pmatrix} -0.45 \\\\ 0.48 \\\\ 1.51 \\end{pmatrix} = \\begin{pmatrix} 0 - 0.48 - 3.02 \\\\ 0.45 - 0.48 - 1.51 \\end{pmatrix} = \\begin{pmatrix} -3.50 \\\\ -1.54 \\end{pmatrix}$$\n搜索方向 $p$ 则是：\n$$p = - (J^\\top J)^{-1} (J^\\top \\mathbf{\\epsilon}(\\theta^0)) = -\\frac{1}{6} \\begin{pmatrix} 3 & -3 \\\\ -3 & 5 \\end{pmatrix} \\begin{pmatrix} -3.50 \\\\ -1.54 \\end{pmatrix} = -\\frac{1}{6} \\begin{pmatrix} -10.50 + 4.62 \\\\ 10.50 - 7.70 \\end{pmatrix} = -\\frac{1}{6} \\begin{pmatrix} -5.88 \\\\ 2.80 \\end{pmatrix} = \\begin{pmatrix} 0.98 \\\\ -\\frac{2.80}{6} \\end{pmatrix} = \\begin{pmatrix} 0.98 \\\\ -\\frac{7}{15} \\end{pmatrix}$$\n\n现在我们检查集合 $\\{\\;1,\\; \\tfrac{1}{2},\\; \\tfrac{1}{4}\\;\\}$ 中的 $\\alpha$ 是否满足强 Wolfe 条件，其中 $c_1=10^{-4}$ 和 $c_2=0.9$。我们必须首先计算 $\\alpha=0$ 时的各项：\n$$V_N(\\theta^0) = \\frac{1}{2(3)} \\left( (-0.45)^2 + (0.48)^2 + (1.51)^2 \\right) = \\frac{1}{6} (0.2025 + 0.2304 + 2.2801) = \\frac{2.713}{6} \\approx 0.452167$$\n代价函数的梯度是 $\\nabla V_N(\\theta) = \\frac{1}{N} J^\\top \\mathbf{\\epsilon}(\\theta)$。\n$$\\nabla V_N(\\theta^0) = \\frac{1}{3} \\begin{pmatrix} -3.50 \\\\ -1.54 \\end{pmatrix}$$\n在 $\\alpha=0$ 处的方向导数是 $\\phi'(0) = \\nabla V_N(\\theta^0)^{\\top} p$：\n$$\\phi'(0) = \\frac{1}{3} \\begin{pmatrix} -3.50 & -1.54 \\end{pmatrix} \\begin{pmatrix} 0.98 \\\\ -7/15 \\end{pmatrix} = \\frac{1}{3} \\left( -3.43 + \\frac{10.78}{15} \\right) = \\frac{1}{3} \\left( -3.43 + 0.71866... \\right) \\approx -0.903778$$\n\n我们从测试最大的候选步长 $\\alpha=1$ 开始：\n新的参数估计为 $\\theta(1) = \\theta^0 + 1 \\cdot p = \\begin{pmatrix} 1.0 \\\\ 0.0 \\end{pmatrix} + \\begin{pmatrix} 0.98 \\\\ -7/15 \\end{pmatrix} = \\begin{pmatrix} 1.98 \\\\ -7/15 \\end{pmatrix}$。\n让我们计算在这个新点的残差矢量 $\\mathbf{\\epsilon}(1)$：\n$$\\epsilon(1, \\theta(1)) = -0.45 - (1.98 \\cdot 0 - 7/15) = -0.45 + 7/15 = \\frac{-27+28}{60} = \\frac{1}{60}$$\n$$\\epsilon(2, \\theta(1)) = 1.48 - (1.98 \\cdot 1 - 7/15) = 1.48 - (1.98 - 0.4666...) = 1.48 - 1.5133... = \\frac{37}{25} - \\frac{227}{150} = \\frac{222-227}{150} = -\\frac{5}{150}=-\\frac{1}{30}$$\n$$\\epsilon(3, \\theta(1)) = 3.51 - (1.98 \\cdot 2 - 7/15) = 3.51 - (3.96 - 0.4666...) = 3.51 - 3.4933... = \\frac{351}{100} - \\frac{524}{150} = \\frac{1053-1048}{300} = \\frac{5}{300}=\\frac{1}{60}$$\n新的代价是 $V_N(\\theta(1)) = \\frac{1}{6} \\left( (\\frac{1}{60})^2 + (-\\frac{1}{30})^2 + (\\frac{1}{60})^2 \\right) = \\frac{1}{6 \\cdot 3600} (1+4+1) = \\frac{6}{6 \\cdot 3600} = \\frac{1}{3600} \\approx 0.0002778$。\n\n检查 $\\alpha=1$ 是否满足条件1 (充分下降)：\n$$V_N(\\theta(1)) \\le V_N(\\theta^0) + c_1 \\alpha \\phi'(0)$$\n$$0.0002778 \\le 0.452167 + 10^{-4} (1) (-0.903778)$$\n$$0.0002778 \\le 0.452167 - 0.00009038 = 0.452076...$$\n这个不等式显然成立。第一个条件满足。\n\n检查 $\\alpha=1$ 是否满足条件2 (强曲率)：\n$$|\\nabla V_N(\\theta(1))^{\\top} p | \\le c_2 |\\phi'(0)|$$\n首先，我们在 $\\theta(1)$ 处计算梯度：\n$$\\nabla V_N(\\theta(1)) = \\frac{1}{3} J^\\top \\mathbf{\\epsilon}(1) = \\frac{1}{3} \\begin{pmatrix} 0 & -1 & -2 \\\\ -1 & -1 & -1 \\end{pmatrix} \\begin{pmatrix} 1/60 \\\\ -1/30 \\\\ 1/60 \\end{pmatrix} = \\frac{1}{180} \\begin{pmatrix} 0 & -1 & -2 \\\\ -1 & -1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix} = \\frac{1}{180} \\begin{pmatrix} 0+2-2 \\\\ -1+2-1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n梯度为零。这是因为对于线性最小二乘问题，Gauss-Newton 方法等同于 Newton 方法，它在单次完整步长 $(\\alpha=1)$ 内找到精确的最小值点。\n方向导数为 $\\phi'(1) = \\nabla V_N(\\theta(1))^{\\top} p = \\begin{pmatrix} 0 & 0 \\end{pmatrix} p = 0$。\n条件变为：\n$$|0| \\le 0.9 \\cdot |-0.903778| \\approx 0.8134$$\n这个不等式也成立。对于 $\\alpha=1$，两个强 Wolfe 条件都满足。\n\n由于问题要求找出集合 $\\{\\;1,\\; \\tfrac{1}{2},\\; \\tfrac{1}{4}\\;\\}$ 中满足条件的最大 $\\alpha$，而 $\\alpha=1$ 满足这些条件，因此它就是正确答案。无需测试更小的值。计算出的步长，四舍五入到四位有效数字，是 $1.000$。", "answer": "$$\\boxed{1.000}$$", "id": "2892776"}]}