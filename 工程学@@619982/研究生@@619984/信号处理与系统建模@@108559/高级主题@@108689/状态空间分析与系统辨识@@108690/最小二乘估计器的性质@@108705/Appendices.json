{"hands_on_practices": [{"introduction": "最小二乘估计可以通过几何学的视角得到优雅的理解。本实践将超越优化目标，探索一个基本的线性算子——投影矩阵 $P$，它将观测向量 $y$ 映射到拟合向量 $\\hat{y}$。通过推导该矩阵的性质，您将证明其迹等于参数的数量 $p$ [@problem_id:2897104]。这个练习将为您理解“自由度”这一关键统计概念提供坚实的理论基础，将抽象的线性代数与核心统计思想联系起来。", "problem": "考虑一个线性信号模型，其中观测到的确定性信号向量 $y \\in \\mathbb{R}^{n}$ 被建模为 $p$ 个已知回归变量的线性组合，这些回归变量作为设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列。该矩阵具有满列秩 $p$ 且 $p \\leq n$。最小二乘估计量 (LS) 选择系数向量 $\\hat{\\beta} \\in \\mathbb{R}^{p}$ 以最小化平方误差 $\\|y - X \\beta\\|_{2}^{2}$。令拟合信号为 $\\hat{y} = X \\hat{\\beta}$，并定义线性映射 $P \\in \\mathbb{R}^{n \\times n}$，使得对于任意 $y \\in \\mathbb{R}^{n}$ 都有 $\\hat{y} = P y$。\n\n从刻画最小二乘解的最优性条件以及指出残差与模型子空间正交的正交性原理出发，推导 $P$ 的显式表达式，并从第一性原理证明 $P$ 是对称且幂等的。然后，仅利用正交投影矩阵的结构性质，且不假设 $\\mathbb{R}^{n}$ 的任何特定基，用 $X$ 的内在性质来确定 $\\operatorname{tr}(P)$。将此迹解释为该模型中最小二乘法使用的有效自由度。\n\n请以 $p$ 的形式给出 $\\operatorname{tr}(P)$ 的解析值作为您的最终答案。不包含任何单位。如果引入任何其他记号，请明确定义。您的最终答案必须是单一的封闭形式表达式。", "solution": "我们从最小二乘估计量的定义开始：$\\hat{\\beta}$ 在 $\\beta \\in \\mathbb{R}^{p}$ 上最小化 $\\|y - X \\beta\\|_{2}^{2}$。其基本最优性条件是正规方程组，通过将目标函数对 $\\beta$ 求导并令其为零得到：\n$$\nX^{\\top}\\bigl(y - X \\hat{\\beta}\\bigr) = 0.\n$$\n在 $X$ 具有满列秩 $p$ 的假设下，格拉姆矩阵 (Gram matrix) $X^{\\top} X \\in \\mathbb{R}^{p \\times p}$ 是对称正定的，因此是可逆的。所以，\n$$\n\\hat{\\beta} = \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top} y.\n$$\n于是，拟合向量为\n$$\n\\hat{y} = X \\hat{\\beta} = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top} y.\n$$\n根据对所有 $y \\in \\mathbb{R}^{n}$ 均有 $\\hat{y} = P y$ 的线性映射 $P$ 的定义，我们确定\n$$\nP = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}.\n$$\n\n现在我们来证明 $P$ 是对称的。利用转置的性质以及 $X^{\\top} X$ 的对称性，\n$$\nP^{\\top} = \\bigl(X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\\bigr)^{\\top} = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top} = P.\n$$\n接着，我们验证幂等性，即 $P^{2} = P$。计算\n$$\nP^{2} = \\Bigl(X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\\Bigr)\\Bigl(X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\\Bigr)\n= X \\bigl(X^{\\top} X\\bigr)^{-1} \\underbrace{X^{\\top} X}_{\\text{invertible}} \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\n= X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\n= P.\n$$\n因此 $P$ 是一个对称幂等矩阵，即到模型子空间 $\\mathcal{R}(X)$（$X$ 的值域（列空间））上的正交投影矩阵。这一点也可以从正交性原理得出：残差 $r = y - \\hat{y}$ 满足 $X^{\\top} r = 0$，这表明 $r \\in \\mathcal{R}(X)^{\\perp}$ 且 $\\hat{y} \\in \\mathcal{R}(X)$，因此 $P$ 是到 $\\mathcal{R}(X)$ 上的正交投影矩阵。\n\n为了根据 $X$ 的内在性质确定 $\\operatorname{tr}(P)$，我们观察到任何实对称幂等矩阵的特征值都在 $\\{0, 1\\}$ 中。确实，如果对于某个非零向量 $v$ 有 $P v = \\lambda v$，那么 $P^{2} v = \\lambda^{2} v$ 并且也有 $P^{2} v = P v = \\lambda v$，因此 $\\lambda^{2} = \\lambda$，所以 $\\lambda \\in \\{0, 1\\}$。此外，$P$ 的秩等于其值为 $1$ 的特征值的数量，并且等于 $\\dim\\bigl(\\mathcal{R}(P)\\bigr)$。由于 $P$ 是到 $\\mathcal{R}(X)$ 上的正交投影矩阵，我们有 $\\mathcal{R}(P) = \\mathcal{R}(X)$，因此\n$$\n\\operatorname{rank}(P) = \\dim\\bigl(\\mathcal{R}(X)\\bigr) = \\operatorname{rank}(X).\n$$\n因为矩阵的迹等于其特征值之和，对于一个对称幂等矩阵 $P$，其迹就等于其特征值中 $1$ 的个数，也就是 $P$ 的秩。所以，\n$$\n\\operatorname{tr}(P) = \\operatorname{rank}(P) = \\operatorname{rank}(X).\n$$\n在给定 $X$ 具有满列秩 $p$ 的假设下，我们有 $\\operatorname{rank}(X) = p$，因此\n$$\n\\operatorname{tr}(P) = p.\n$$\n\n另一个明确揭示其结构的推导方法是使用奇异值分解 (Singular Value Decomposition, SVD)。令 $X = U \\Sigma V^{\\top}$ 为一个紧奇异值分解 (SVD)，其中 $U \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$V \\in \\mathbb{R}^{p \\times p}$ 是正交矩阵，而 $\\Sigma \\in \\mathbb{R}^{n \\times p}$ 的形式为 $\\Sigma = \\begin{bmatrix} D \\\\ 0 \\end{bmatrix}$，其中 $D \\in \\mathbb{R}^{p \\times p}$ 是一个对角矩阵，其对角线元素严格为正，因为 $X$ 具有满列秩 $p$。那么\n$$\nP = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\n= U \\Sigma V^{\\top} \\bigl(V \\Sigma^{\\top} \\Sigma V^{\\top}\\bigr)^{-1} V \\Sigma^{\\top} U^{\\top}\n= U \\Sigma \\bigl(\\Sigma^{\\top} \\Sigma\\bigr)^{-1} \\Sigma^{\\top} U^{\\top}.\n$$\n由于 $\\Sigma^{\\top} \\Sigma = D^{2}$，我们得到\n$$\n\\Sigma \\bigl(\\Sigma^{\\top} \\Sigma\\bigr)^{-1} \\Sigma^{\\top}\n= \\begin{bmatrix} D \\\\ 0 \\end{bmatrix} D^{-2} \\begin{bmatrix} D & 0 \\end{bmatrix}\n= \\begin{bmatrix} I_{p} & 0 \\\\ 0 & 0 \\end{bmatrix},\n$$\n所以\n$$\nP = U \\begin{bmatrix} I_{p} & 0 \\\\ 0 & 0 \\end{bmatrix} U^{\\top}.\n$$\n迹在正交矩阵 $U$ 的相似变换下是不变的，因此\n$$\n\\operatorname{tr}(P) = \\operatorname{tr}\\!\\left(\\begin{bmatrix} I_{p} & 0 \\\\ 0 & 0 \\end{bmatrix}\\right) = p.\n$$\n\n作为有效自由度的解释：在最小二乘拟合中，$P$ 将任意数据向量 $y$ 映射到 $p$ 维子空间 $\\mathcal{R}(X)$ 上的正交投影。迹 $\\operatorname{tr}(P)$ 等于 $p$，这代表了拟合过程为适应数据而使用的有效参数数量或线性约束的数量。因此，残差子空间的维度为 $n - p$，这通常被解释为可用于评估模型与数据之间差异的剩余自由度（例如，在存在加性噪声时用于方差估计）。因此，$\\operatorname{tr}(P) = p$ 量化了最小二乘法所使用的有效自由度。", "answer": "$$\\boxed{p}$$", "id": "2897104"}, {"introduction": "虽然最小二乘估计在理想条件下具有许多优良性质，但其性能对模型设定的错误非常敏感。本实践将探讨应用统计学中最常见的陷阱之一：遗漏变量偏误 [@problem_id:1948135]。通过推导当一个相关变量被排除在模型之外时，某个估计系数的期望值，您将精确地理解偏误产生的方式和原因，这对于正确解释回归结果至关重要。", "problem": "一位经济学家正在研究决定小时工资的因素。对于一个大小为 $n$ 的样本中的个体 $i$ ，其真实的潜在关系被认为是一个多元线性回归模型：\n$$Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\epsilon_i$$\n在这里，$Y_i$ 是小时工资，$x_i$ 是受教育年限，$z_i$ 是工作经验年限。参数 $\\beta_0$、$\\beta_1$ 和 $\\beta_2$ 是未知的常数。误差项 $\\epsilon_i$ 被假定为独立同分布，且期望值为零，即 $E[\\epsilon_i] = 0$。$x_i$ 和 $z_i$ 都被视为非随机变量。\n\n不幸的是，这位经济学家丢失了工作经验的数据 $z_i$。他们转而仅使用教育数据来估计一个设定错误的简单线性回归模型：\n$$Y_i = \\alpha_0 + \\alpha_1 x_i + \\nu_i$$\n该经济学家使用普通最小二乘法 (OLS) 来获得斜率参数 $\\alpha_1$ 的一个估计量，记作 $\\hat{\\alpha}_1$。\n\n推导这个估计量的期望值 $E[\\hat{\\alpha}_1]$。用真实模型参数（$\\beta_1$、$\\beta_2$）、涉及样本数据（$x_i$、$z_i$）及其各自样本均值 $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ 和 $\\bar{z} = \\frac{1}{n} \\sum_{i=1}^n z_i$ 的总和来表示你的答案。", "solution": "根据 $Y_i$ 对 $x_i$ 的设定错误的简单回归，OLS 斜率估计量为\n$$\n\\hat{\\alpha}_{1}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(Y_{i}-\\bar{Y})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}.\n$$\n在真实模型 $Y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\beta_{2}z_{i}+\\epsilon_{i}$ 下，其中 $E[\\epsilon_{i}]=0$ 且回归变量为非随机的， $Y_i$ 的样本均值为\n$$\n\\bar{Y}=\\beta_{0}+\\beta_{1}\\bar{x}+\\beta_{2}\\bar{z}+\\bar{\\epsilon},\\quad \\text{其中 } \\bar{\\epsilon}=\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}.\n$$\n因此\n$$\nY_{i}-\\bar{Y}=\\beta_{1}(x_{i}-\\bar{x})+\\beta_{2}(z_{i}-\\bar{z})+(\\epsilon_{i}-\\bar{\\epsilon}).\n$$\n代入分子，\n$$\n\\sum_{i=1}^{n}(x_{i}-\\bar{x})(Y_{i}-\\bar{Y})=\\beta_{1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}+\\beta_{2}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})+\\sum_{i=1}^{n}(x_{i}-\\bar{x})(\\epsilon_{i}-\\bar{\\epsilon}).\n$$\n取期望，并使用 $E[\\epsilon_{i}]=0$ 意味着 $E[\\bar{\\epsilon}]=0$，因此\n$$\nE\\!\\left[\\sum_{i=1}^{n}(x_{i}-\\bar{x})(\\epsilon_{i}-\\bar{\\epsilon})\\right]=0.\n$$\n由于 $x_i$ 和 $z_i$ 是非随机的，分母 $\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}$ 是一个固定值，所以\n$$\nE[\\hat{\\alpha}_{1}]=\\frac{\\beta_{1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}+\\beta_{2}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}=\\beta_{1}+\\beta_{2}\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}.\n$$\n这表明遗漏变量偏误项是 $\\beta_{2}\\,\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}$，该项当且仅当 $\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})=0$ 或 $\\beta_2=0$ 时为零。", "answer": "$$\\boxed{\\beta_{1}+\\beta_{2}\\,\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}}$$", "id": "1948135"}, {"introduction": "最终，衡量一个估计量成功与否的关键标准是其预测新数据的能力。本实践聚焦于通过计算预期的样本外风险来量化这种预测性能 [@problem_id:2897085]。您将把这个风险分解为其基本组成部分——偏差的平方和方差——从而理解最小二乘估计量 $\\hat{\\beta}$ 的统计特性如何直接影响其泛化误差，为经典回归分析与现代统计学习理论之间架起一座桥梁。", "problem": "一个具有未知有限冲激响应向量 $\\beta_{0} \\in \\mathbb{R}^{p}$ 的广义平稳离散时间线性时不变系统，通过普通最小二乘法（OLS）从 $n$ 次测量中进行辨识。训练数据被建模为 $y = X \\beta_{0} + \\varepsilon$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个由输入序列构建的固定的满列秩设计矩阵，$\\varepsilon \\in \\mathbb{R}^{n}$ 是均值为零、协方差为 $\\sigma^{2} I_{n}$ 的噪声，而 $I_{n}$ 是 $n \\times n$ 的单位矩阵。OLS 估计量 $\\hat{\\beta}$ 被定义为经验残差平方和的最小化器，并用于通过预测器 $x^{\\top} \\hat{\\beta}$ 来预测新输入特征向量 $x \\in \\mathbb{R}^{p}$ 的无噪声输出。假设在部署时，新输入 $x$ 独立于训练噪声，并从一个均值为零、协方差为 $\\Sigma_{x} \\in \\mathbb{R}^{p \\times p}$ 的分布中抽取。\n\n仅使用陈述的建模假设和第一性原理，推导无噪声目标 $x^{\\top} \\beta_{0}$ 的期望样本外预测平方风险，其定义为\n$$\nR_{\\mathrm{out}} \\triangleq \\mathbb{E}\\!\\left[\\left(x^{\\top} \\hat{\\beta} - x^{\\top} \\beta_{0}\\right)^{2} \\,\\middle|\\, X \\right],\n$$\n其中期望是针对训练噪声和新输入 $x$ 的分布计算的。将 $R_{\\mathrm{out}}$ 用 $X$、$\\Sigma_{x}$ 和 $\\sigma^{2}$ 显式表达，并将其分解为相对于 $\\hat{\\beta}$ 分布的偏差平方和方差贡献。以解析表达式的形式提供最终答案；不需要数值近似。", "solution": "最小化残差平方和 $\\|y - X\\beta\\|_{2}^{2}$ 的 OLS 估计量 $\\hat{\\beta}$ 由正规方程给出，解为 $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$。我们将模型 $y = X \\beta_{0} + \\varepsilon$ 代入此表达式中：\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}(X \\beta_{0} + \\varepsilon) = (X^{\\top}X)^{-1}X^{\\top}X \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon = \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon\n$$\n风险 $R_{\\mathrm{out}}$ 定义为预测误差平方 $(x^{\\top} \\hat{\\beta} - x^{\\top} \\beta_{0})^{2}$ 在训练噪声 $\\varepsilon$ 和新输入 $x$ 的联合分布上的期望。由于 $x$ 和 $\\varepsilon$ 是独立的，我们可以利用期望的迭代定律（以 $X$ 为条件）来分析风险：\n$$\nR_{\\mathrm{out}} = \\mathbb{E}_{x, \\varepsilon} \\left[ \\left(x^{\\top} (\\hat{\\beta} - \\beta_{0})\\right)^{2} \\,\\middle|\\, X \\right] = \\mathbb{E}_{x} \\left[ \\mathbb{E}_{\\varepsilon} \\left[ \\left(x^{\\top} \\hat{\\beta} - x^{\\top} \\beta_{0}\\right)^{2} \\,\\middle|\\, X, x \\right] \\right]\n$$\n对于一个固定的新输入 $x$，内部的期望是预测器 $x^{\\top}\\hat{\\beta}$ 的条件均方误差 (MSE)，可以将其分解为偏差平方和方差：\n$$\n\\mathbb{E}_{\\varepsilon} \\left[ \\left(x^{\\top} \\hat{\\beta} - x^{\\top} \\beta_{0}\\right)^{2} \\,\\middle|\\, X, x \\right] = \\left( \\mathbb{E}_{\\varepsilon} \\left[ x^{\\top} \\hat{\\beta} \\,\\middle|\\, X, x \\right] - x^{\\top} \\beta_{0} \\right)^{2} + \\mathrm{Var}_{\\varepsilon} \\left( x^{\\top} \\hat{\\beta} \\,\\middle|\\, X, x \\right)\n$$\n首先分析偏差项。由于 $\\mathbb{E}[\\varepsilon] = 0$，OLS 估计量 $\\hat{\\beta}$ 是无偏的：\n$$\n\\mathbb{E}_{\\varepsilon} \\left[ \\hat{\\beta} \\,\\middle|\\, X \\right] = \\mathbb{E}_{\\varepsilon} \\left[ \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon \\,\\middle|\\, X \\right] = \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\mathbb{E}_{\\varepsilon}[\\varepsilon] = \\beta_{0}\n$$\n因此，预测器 $x^{\\top}\\hat{\\beta}$ 也是无偏的，其偏差平方项为零：\n$$\n\\left( \\mathbb{E}_{\\varepsilon} \\left[ x^{\\top} \\hat{\\beta} \\,\\middle|\\, X, x \\right] - x^{\\top} \\beta_{0} \\right)^{2} = (x^{\\top}\\beta_{0} - x^{\\top}\\beta_{0})^{2} = 0\n$$\n对 $R_{\\mathrm{out}}$ 的总偏差平方贡献（在 $x$ 上取平均后）因此为 0。\n\n接下来分析方差项。对于固定的 $x$，预测器 $x^{\\top}\\hat{\\beta}$ 在 $\\varepsilon$ 上的方差为：\n$$\n\\mathrm{Var}_{\\varepsilon} \\left( x^{\\top} \\hat{\\beta} \\,\\middle|\\, X, x \\right) = \\mathrm{Var}_{\\varepsilon} \\left( x^{\\top}\\beta_{0} + x^{\\top}(X^{\\top}X)^{-1}X^{\\top}\\varepsilon \\,\\middle|\\, X, x \\right) = \\mathrm{Var}_{\\varepsilon} \\left( x^{\\top}(X^{\\top}X)^{-1}X^{\\top}\\varepsilon \\right)\n$$\n利用二次型方差的性质 $\\mathrm{Var}(c^{\\top}\\varepsilon) = c^{\\top}\\mathrm{Cov}(\\varepsilon)c$，并给定 $\\mathrm{Cov}(\\varepsilon) = \\sigma^{2}I_{n}$，我们得到：\n$$\n\\mathrm{Var}_{\\varepsilon} \\left( \\dots \\right) = \\sigma^{2} \\left(x^{\\top}(X^{\\top}X)^{-1}X^{\\top}\\right) \\left(X(X^{\\top}X)^{-1}x\\right) = \\sigma^{2} x^{\\top}(X^{\\top}X)^{-1}x\n$$\n为了求得对 $R_{\\mathrm{out}}$ 的总方差贡献，我们必须将这个量在 $x$ 的分布上取平均。利用二次型期望的性质 $\\mathbb{E}[x^{\\top}Mx] = \\mathrm{Tr}(M\\Sigma_{x}) + \\mu_{x}^{\\top}M\\mu_{x}$，在我们的情况中，$M=(X^{\\top}X)^{-1}$，$\\mu_{x}=0$，协方差为 $\\Sigma_{x}$，因此：\n$$\n\\mathbb{E}_{x} \\left[ \\sigma^{2} x^{\\top}(X^{\\top}X)^{-1}x \\right] = \\sigma^{2} \\mathrm{Tr}\\left((X^{\\top}X)^{-1}\\Sigma_{x}\\right)\n$$\n总风险 $R_{\\mathrm{out}}$ 是偏差平方贡献和方差贡献之和。因此，风险分解为 (偏差平方, 方差) 的形式为 $(0, \\sigma^{2} \\mathrm{Tr}((X^{\\top}X)^{-1}\\Sigma_{x}))$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & \\sigma^{2} \\mathrm{Tr}((X^{\\top}X)^{-1} \\Sigma_{x}) \\end{pmatrix}}\n$$", "id": "2897085"}]}