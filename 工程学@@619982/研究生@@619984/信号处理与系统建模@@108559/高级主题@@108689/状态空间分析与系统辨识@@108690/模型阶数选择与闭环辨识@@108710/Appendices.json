{"hands_on_practices": [{"introduction": "在闭环系统中进行系统辨识充满了挑战，直接应用开环方法往往会导致严重错误。本练习旨在通过一个典型的思想实验揭示其中的一个基本陷阱：当系统缺乏外部参考激励时，直接的ARX辨识方法会产生怎样的系统性偏差。通过推导这个偏差的表达式，你将深刻理解为何在闭环辨识中，数据激励的性质和辨识方法的设计至关重要 [@problem_id:2883948]。", "problem": "考虑一个单输入单输出离散时间线性时不变闭环系统，其受控对象传递函数 $G_{0}(q)$ 未知，而稳定控制器 $C(q)$ 已知。该环路在没有外部参考的情况下运行，即 $r(t) \\equiv 0$。测得的输出 $y(t)$ 和控制输入 $u(t)$ 满足\n- $y(t) = G_{0}(q)\\,u(t) + v(t)$，\n- $u(t) = -\\,C(q)\\,y(t)$，\n其中 $v(t) = H_{0}(q)\\,e(t)$ 是对象扰动，$H_{0}(q)$ 是一个稳定且可逆的噪声模型，$e(t)$ 是一个零均值、单位方差、独立同分布的序列。\n\n通过最小化关于参数矢量 $\\theta$ 的均方单步预测误差，将一个带外源输入的自回归（ARX）模型拟合到闭环数据上，该误差定义为\n$$\n\\varepsilon_{\\theta}(t) \\triangleq A(q,\\theta)\\,y(t) - B(q,\\theta)\\,u(t),\n$$\n其中 $A(q,\\theta)$ 是首一的，$B(q,\\theta)$ 是正常的。假设模型阶数足够大，使得比率 $B(q,\\theta)/A(q,\\theta)$ 能够表示由闭环数据导出的从 $u(t)$到 $y(t)$ 的均方最优线性时不变映射。\n\n仅从上述定义以及广义平稳过程和线性时不变系统（包括谱表示）的基本性质出发，推导在没有参考输入的情况下，ARX 估计相对于真实对象 $G_{0}(q)$ 的渐近偏差表达式。请将最终答案表示为仅包含 $G_{0}(q)$ 和 $C(q)$ 的单个解析表达式。无需进行数值计算。", "solution": "所陈述的问题具有科学依据、提法恰当且客观。它描述了闭环系统辨识中的一个标准场景，没有任何逻辑不一致或事实不健全之处。因此，我们可以进行严谨的求解推导。\n\n目标是求出在闭环中运行的对象 $G_0(q)$ 的带外源输入的自回归（ARX）模型估计的渐近偏差。偏差定义为 $\\hat{G}(q) - G_0(q)$，其中 $\\hat{G}(q)$ 是估计的传递函数。\n\n问题陈述，ARX模型是通过最小化均方单步预测误差来拟合的，并且对于足够大的模型阶数，所得的估计 $\\hat{G}(q,\\hat{\\theta}) = B(q,\\hat{\\theta})/A(q,\\hat{\\theta})$ 代表了从输入 $u(t)$ 到输出 $y(t)$ 的均方最优线性时不变（LTI）映射。这是用于从 $u(t)$ 估计 $y(t)$ 的维纳滤波器的定义。在频域中，这个最优滤波器的传递函数，我们记为 $\\hat{G}(e^{i\\omega})$，由输出和输入的互功率谱密度 $\\Phi_{yu}(\\omega)$ 与输入的功率谱密度 $\\Phi_{uu}(\\omega)$ 之比给出。\n\n$$\n\\hat{G}(e^{i\\omega}) = \\frac{\\Phi_{yu}(\\omega)}{\\Phi_{uu}(\\omega)}\n$$\n\n为了计算这些谱，我们必须首先用外源噪声源 $e(t)$ 来表示信号 $y(t)$ 和 $u(t)$。系统动态由以下公式给出：\n$$\ny(t) = G_{0}(q)\\,u(t) + v(t) \\quad (1)\n$$\n$$\nu(t) = -C(q)\\,y(t) \\quad (2)\n$$\n扰动为 $v(t) = H_0(q)e(t)$，其中 $e(t)$ 是一个零均值单位方差的白噪声过程，即 $E[e(t)]=0$ 且 $E[e(t)e(t-\\tau)] = \\delta_{\\tau,0}$，所以其功率谱密度为 $\\Phi_{ee}(\\omega)=1$。\n\n将方程 $(2)$ 代入 $(1)$：\n$$\ny(t) = G_0(q)[-C(q)y(t)] + v(t)\n$$\n$$\ny(t) + G_0(q)C(q)y(t) = v(t)\n$$\n$$\n[1 + G_0(q)C(q)]y(t) = v(t)\n$$\n求解 $y(t)$ 可得：\n$$\ny(t) = \\frac{1}{1 + G_0(q)C(q)} v(t) = S_0(q) v(t)\n$$\n其中 $S_0(q) \\triangleq (1 + G_0(q)C(q))^{-1}$ 是真实闭环系统的灵敏度函数。\n代入 $v(t) = H_0(q)e(t)$，我们得到：\n$$\ny(t) = S_0(q) H_0(q) e(t)\n$$\n现在，将 $y(t)$ 的这个表达式代回方程 $(2)$ 中以求得 $u(t)$：\n$$\nu(t) = -C(q)y(t) = -C(q)S_0(q)H_0(q)e(t)\n$$\n\n有了 $y(t)$ 和 $u(t)$ 作为由白噪声 $e(t)$ 驱动的 LTI 系统输出的表达式，我们就可以计算它们的功率谱密度。对于一个信号 $x(t) = F(q)e(t)$，其功率谱为 $\\Phi_{xx}(\\omega) = |F(e^{i\\omega})|^2 \\Phi_{ee}(\\omega)$。由于 $\\Phi_{ee}(\\omega)=1$，我们有：\n\n输入 $u(t)$ 的功率谱密度为：\n$$\n\\Phi_{uu}(\\omega) = |-C(e^{i\\omega})S_0(e^{i\\omega})H_0(e^{i\\omega})|^2 = |C(e^{i\\omega})|^2 |S_0(e^{i\\omega})|^2 |H_0(e^{i\\omega})|^2\n$$\n$y(t)$ 和 $u(t)$ 的互功率谱密度由 $\\Phi_{yu}(\\omega) = F_y(e^{i\\omega}) F_u(e^{i\\omega})^* \\Phi_{ee}(\\omega)$ 给出，其中 $F_y(q) = S_0(q)H_0(q)$ 且 $F_u(q) = -C(q)S_0(q)H_0(q)$，$F_u(e^{i\\omega})^*$ 表示 $F_u(e^{i\\omega})$ 的复共轭。\n$$\n\\Phi_{yu}(\\omega) = \\left(S_0(e^{i\\omega})H_0(e^{i\\omega})\\right) \\left(-C(e^{i\\omega})S_0(e^{i\\omega})H_0(e^{i\\omega})\\right)^*\n$$\n$$\n\\Phi_{yu}(\\omega) = S_0(e^{i\\omega})H_0(e^{i\\omega}) \\left(-C(e^{-i\\omega})S_0(e^{-i\\omega})H_0(e^{-i\\omega})\\right)\n$$\n$$\n\\Phi_{yu}(\\omega) = -C(e^{-i\\omega}) S_0(e^{i\\omega})S_0(e^{-i\\omega}) H_0(e^{i\\omega})H_0(e^{-i\\omega})\n$$\n$$\n\\Phi_{yu}(\\omega) = -C(e^{-i\\omega}) |S_0(e^{i\\omega})|^2 |H_0(e^{i\\omega})|^2\n$$\n现在我们计算传递函数估计 $\\hat{G}(e^{i\\omega})$：\n$$\n\\hat{G}(e^{i\\omega}) = \\frac{\\Phi_{yu}(\\omega)}{\\Phi_{uu}(\\omega)} = \\frac{-C(e^{-i\\omega}) |S_0(e^{i\\omega})|^2 |H_0(e^{i\\omega})|^2}{|C(e^{i\\omega})|^2 |S_0(e^{i\\omega})|^2 |H_0(e^{i\\omega})|^2}\n$$\n$|S_0(e^{i\\omega})|^2$ 和 $|H_0(e^{i\\omega})|^2$ 项可以消去，因为 $H_0$ 可逆，且对于一个未处于不稳定性边缘的稳定闭环系统，$S_0$ 非零。我们也假设控制器 $C(q)$ 不恒为零。\n$$\n\\hat{G}(e^{i\\omega}) = \\frac{-C(e^{-i\\omega})}{|C(e^{i\\omega})|^2} = \\frac{-C(e^{-i\\omega})}{C(e^{i\\omega})C(e^{-i\\omega})} = \\frac{-1}{C(e^{i\\omega})}\n$$\n通过解析延拓，用移位算子 $q$ 表示的估计传递函数为：\n$$\n\\hat{G}(q) = -\\frac{1}{C(q)}\n$$\n这个结果表明，在没有外部参考信号的闭环中直接进行 ARX 辨识，所辨识出的是控制器的负倒数，而不是对象本身。这是闭环辨识中的一个基本结论。\n\n渐近偏差是估计模型 $\\hat{G}(q)$ 与真实对象模型 $G_0(q)$ 之间的差异：\n$$\n\\text{Bias} = \\hat{G}(q) - G_0(q) = -\\frac{1}{C(q)} - G_0(q)\n$$\n\n该表达式表示了当 ARX 辨识方法直接应用于无外部参考信号的闭环数据时产生的系统误差。", "answer": "$$\n\\boxed{-\\frac{1}{C(q)} - G_{0}(q)}\n$$", "id": "2883948"}, {"introduction": "理解了闭环辨识的理论难点后，我们转向一个建设性的实践任务：模型阶数选择。本练习要求你设计一个算法，根据子空间辨识方法中常见的奇异值序列来确定系统阶数。这个动手实践将帮助你掌握一种被称为“间隙启发法”的实用技术，学会如何从数据中分离出系统动态与噪声，这是建立精确模型的关键第一步 [@problem_id:2883912]。", "problem": "给定一个按降序排列的严格正奇异值序列，这些奇异值是某个线性时不变（LTI）系统在闭环子空间辨识中产生的块汉克尔（block Hankel）矩阵或信息矩阵的估计奇异值。此外，还提供了一个外部估计的噪声基底。您的任务是设计并实现一个程序，利用间隙启发式方法选择系统阶数，并从低秩近似和闭环辨识下奇异值预期结构的基本原理出发，对该方法进行论证。\n\n假设与基础理论：\n1. 实矩阵的奇异值分解（SVD）和 Eckart–Young–Mirsky 定理：对于一个奇异值为 $\\{s_1,s_2,\\dots,s_n\\}$（按非增序排列）的矩阵，其在弗罗贝尼乌斯范数下的最佳秩-$r$近似保留了前 $r$ 个奇异值。\n2. 在对闭环数据进行子空间辨识并有充分的工具变量处理时，与确定性动态特性对应的奇异值将在达到真实系统阶数之前占据主导地位，此后剩余的奇异值将在一个噪声基底附近趋于平坦。因此，对于一个真实阶数 $r^\\star$，我们期望 $s_1 \\ge \\cdots \\ge s_{r^\\star} \\gg s_{r^\\star+1} \\approx \\cdots \\approx s_n \\approx \\tau$，其中 $\\tau$ 表示一个有效噪声基底水平。\n\n推导目标：\n- 基于上述理论，推导一个数学上不变的准则，通过识别主导奇异值和噪声基底奇异值之间的“拐点”来选择一个阶数 $r$。该准则应对于整体数据缩放具有尺度不变性，并且应能捕捉到连续奇异值之间最大的相对下降，同时在存在跨越噪声基底的拐点时，有原则地偏好该拐点。\n\n待实现的算法规范：\n- 输入：一个奇异值列表 $\\mathbf{s} = (s_1,\\dots,s_n)$，其中 $s_i > 0$ 且 $s_1 \\ge \\cdots \\ge s_n$；以及一个正标量噪声基底 $\\tau > 0$。\n- 输出：一个由稳定的间隙启发式方法选择的整数模型阶数 $r \\in \\{0,1,\\dots,n-1\\}$，该方法：\n  1. 使用一种对 $\\mathbf{s}$ 的公共缩放不变的相对下降度量。\n  2. 当存在跨越噪声基底的拐点时，即存在索引 $i$ 使得 $s_i > \\tau$ 且 $s_{i+1} \\le \\tau$，偏好选择该索引。\n  3. 确定性地解决相等情况。\n\n需要处理的边界情况：\n- 如果对所有 $i$ 都有 $s_i \\le \\tau$，则返回 $r = 0$。\n- 如果不存在满足 $s_{i+1} \\le \\tau$ 的索引，则在高于噪声基底的区域内选择一个索引。\n- 准则值出现的相等情况必须被确定性地解决。\n\n您的程序必须为以下测试套件实现此选择算法。每个测试用例包含一个奇异值向量 $\\mathbf{s}^{(k)}$ 和一个噪声基底 $\\tau^{(k)}$。\n\n测试套件：\n- 用例 1：$\\mathbf{s}^{(1)} = (10.0,\\, 5.1,\\, 2.6,\\, 0.51,\\, 0.49,\\, 0.48,\\, 0.47)$，$\\tau^{(1)} = 0.5$。\n- 用例 2：$\\mathbf{s}^{(2)} = (20.0,\\, 10.0,\\, 1.0,\\, 0.9,\\, 0.88,\\, 0.87)$，$\\tau^{(2)} = 0.85$。\n- 用例 3：$\\mathbf{s}^{(3)} = (5.0,\\, 4.0,\\, 3.2,\\, 2.7,\\, 2.4,\\, 2.1)$，$\\tau^{(3)} = 0.5$。\n- 用例 4：$\\mathbf{s}^{(4)} = (0.49,\\, 0.48,\\, 0.47,\\, 0.46)$，$\\tau^{(4)} = 0.5$。\n- 用例 5：$\\mathbf{s}^{(5)} = (8.0,\\, 4.0,\\, 2.0,\\, 1.0,\\, 0.5,\\, 0.5,\\, 0.5)$，$\\tau^{(5)} = 0.5$。\n- 用例 6：$\\mathbf{s}^{(6)} = (3.0,\\, 0.4)$，$\\tau^{(6)} = 0.5$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含为用例 1 到 6 选择的阶数，形式为一个用方括号括起来的逗号分隔列表，例如 $\\text{\"[}r_1,r_2,r_3,r_4,r_5,r_6\\text{]\"}$。\n- 所有输出都必须是整数。此问题不要求任何物理单位或角度单位。", "solution": "所提出的问题已经过验证并被认定为有效。它在科学上基于线性代数和系统辨识的原理，特别是通过奇异值分解（SVD）实现的低秩近似以及子空间辨识方法中汉克尔矩阵的结构。该问题是适定的、客观的，并包含足够的信息来推导和实现一个唯一的、确定性的算法。不存在矛盾、事实错误或伪科学论断。\n\n任务是从一个估计的奇异值序列 $\\mathbf{s} = (s_1, s_2, \\dots, s_n)$（其中 $s_1 \\ge s_2 \\ge \\dots \\ge s_n > 0$）中，为一个线性时不变系统选择模型阶数 $r$。一个外部估计的噪声基底 $\\tau > 0$ 辅助这一选择。所选阶数 $r$ 应对应于奇异值图中的“拐点”，该拐点将与系统动态特性相关的主导奇异值与和噪声相关的较小奇异值分离开。\n\n一个有效的模型阶数 $r$ 意味着系统能被一个秩-$r$模型最佳地近似。根据 Eckart-Young-Mirsky 定理，这会保留前 $r$ 个奇异值。因此，信号与噪声之间的分界预计会出现在 $s_r$ 和 $s_{r+1}$ 之间。我们的目标是识别出这个索引 $r \\in \\{1, \\dots, n-1\\}$。\n\n首先，必须建立一个度量来量化连续奇异值之间的“下降”或“间隙”。让我们考虑索引 $i$ 处的间隙，即从 $s_i$ 到 $s_{i+1}$ 的下降。一个简单的差值 $s_i - s_{i+1}$ 并不适用，因为它不是尺度不变的；将整个数据矩阵乘以一个因子 $c$ 会使所有奇异值及其差值都乘以 $c$，从而改变所选的阶数。需要一个尺度不变的度量，因为信号的总体能量不应影响推导出的模型阶数。比率 $g_i = s_i / s_{i+1}$ 对此类缩放是不变的：$(c s_i) / (c s_{i+1}) = s_i / s_{i+1}$。一个大的比率 $g_i$ 表示一个大的相对下降，预示着一个潜在的拐点。因此，任务简化为找到使该准则最大化的索引 $i$：$r = \\arg\\max_{i} g_i$。\n\n接下来，我们必须整合噪声基底 $\\tau$。基本假设是，对于一个真实阶数为 $r^\\star$ 的系统，奇异值会划分为一个“信号子空间”和一个“噪声子空间”，使得 $s_1 \\ge \\dots \\ge s_{r^\\star} \\gg s_{r^\\star+1} \\approx \\dots \\approx s_n \\approx \\tau$。这意味着最显著的下降，即真实的拐点，理想情况下应该发生在奇异值穿过噪声基底的索引 $i$ 处。我们通过将“跨越”索引 $i$ 定义为满足 $s_i > \\tau$ 且 $s_{i+1} \\le \\tau$ 的索引来形式化这一点。\n\n问题陈述了对此类跨越拐点的“偏好”。为了以一种有原则的、非任意的方式实现这种偏好，我们采用字典序或两阶段搜索策略。这避免了引入任意的权重参数。\n\n由此产生的稳定间隙启发式方法如下：\n\n1.  **初始验证**：检查是否存在任何高于噪声基底的信号。如果最大奇异值 $s_1 \\le \\tau$，那么所有后续奇异值也处于或低于噪声基底。无法从噪声中分辨出任何系统动态。在这种情况下，模型阶数必须为 $r = 0$。\n\n2.  **主搜索（跨越间隙）**：如果 $s_1 > \\tau$，我们继续。我们首先识别所有跨越索引的集合，$I_{straddle} = \\{ i \\in \\{1, \\dots, n-1\\} \\mid s_i > \\tau \\text{ and } s_{i+1} \\le \\tau \\}$。如果这个集合非空，意味着奇异值在一个或多个位置穿过了噪声基底。我们将最优阶数的搜索完全限制在这个集合内，因为这些是物理上最合理的拐点位置。所选阶数对应于该子集中最大间隙比率的阶数：\n    $$ r = \\arg\\max_{i \\in I_{straddle}} \\left(\\frac{s_i}{s_{i+1}}\\right) $$\n\n3.  **后备搜索（所有间隙）**：如果集合 $I_{straddle}$ 为空而 $s_1 > \\tau$，则意味着所有奇异值 $\\{s_1, \\dots, s_n\\}$ 都严格大于噪声基底 $\\tau$。在这种情况下，噪声基底的估计无助于划分奇异值。根据问题规范，我们必须从“高于基底的区域”中选择一个阶数。该过程回归到在所有可能的索引中寻找最大的相对下降：\n    $$ r = \\arg\\max_{i \\in \\{1, \\dots, n-1\\}} \\left(\\frac{s_i}{s_{i+1}}\\right) $$\n\n4.  **相等情况处理**：在任一搜索阶段，如果 $\\arg\\max$ 操作产生多个具有相同最大间隙比率的索引，则需要一个确定性的相等情况处理规则。根据简约原则（奥卡姆剃刀），我们选择最小的索引。这对应于选择能够同等解释数据的最简单模型（最低阶）。\n\n该算法为模型阶数选择提供了一个完整、鲁棒且确定性的方法，它遵循所有指定的要求。它是尺度不变的，在物理上有意义的特征可用时优先考虑它们，并处理了所有指定的边界情况。", "answer": "```python\nimport numpy as np\n\ndef select_order(s_values, tau):\n    \"\"\"\n    Selects the system order using a stabilized gap heuristic.\n\n    Args:\n        s_values (list or np.ndarray): A list of strictly positive singular values\n                                      in descending order.\n        tau (float): A positive scalar representing the noise floor.\n\n    Returns:\n        int: The selected model order r.\n    \"\"\"\n    s = np.array(s_values, dtype=float)\n    n = len(s)\n\n    # Edge Case 1: All singular values are at or below the noise floor.\n    # If the largest singular value is not above tau, no signal is detectable.\n    if s[0] <= tau:\n        return 0\n\n    # There is at least one singular value above the noise floor.\n    # Possible orders are 1, ..., n-1. If n=1, no gaps exist, return 0.\n    if n <= 1:\n        return 0\n\n    # Calculate gap ratios for all possible orders i = 1, ..., n-1.\n    # The gap for order i corresponds to the ratio s_i / s_{i+1}.\n    # In 0-based indexing, this is s[i-1] / s[i] for order i.\n    gaps = s[:-1] / s[1:]\n    \n    # Identify indices i (1-based) that straddle the noise floor.\n    # An index i straddles if s_i > tau and s_{i+1} <= tau.\n    # In 0-based indexing, this corresponds to indices k = 0..n-2\n    # where s[k] > tau and s[k+1] <= tau.\n    possible_orders = np.arange(1, n)\n    is_straddling_mask = (s[:-1] > tau) & (s[1:] <= tau)\n    \n    straddling_orders = possible_orders[is_straddling_mask]\n\n    # Primary Criterion: Search among straddling orders if they exist.\n    if len(straddling_orders) > 0:\n        # Get the gap ratios corresponding to the straddling orders.\n        straddling_gaps = gaps[is_straddling_mask]\n        \n        # Find the index of the maximum gap within the straddling subset.\n        # np.argmax provides deterministic tie-breaking by choosing the first max.\n        best_local_idx = np.argmax(straddling_gaps)\n        \n        # Return the corresponding order.\n        return straddling_orders[best_local_idx]\n    \n    # Fallback Criterion: If no index straddles the noise floor,\n    # it implies all singular values are above it.\n    # Find the largest gap among all possible orders.\n    else:\n        # np.argmax returns the 0-based index of the first maximum value.\n        # The corresponding order is index + 1.\n        best_overall_idx = np.argmax(gaps)\n        return best_overall_idx + 1\n\ndef solve():\n    \"\"\"\n    Runs the model order selection algorithm on the test suite and prints the result.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        (np.array([10.0, 5.1, 2.6, 0.51, 0.49, 0.48, 0.47]), 0.5),\n        # Case 2\n        (np.array([20.0, 10.0, 1.0, 0.9, 0.88, 0.87]), 0.85),\n        # Case 3\n        (np.array([5.0, 4.0, 3.2, 2.7, 2.4, 2.1]), 0.5),\n        # Case 4\n        (np.array([0.49, 0.48, 0.47, 0.46]), 0.5),\n        # Case 5\n        (np.array([8.0, 4.0, 2.0, 1.0, 0.5, 0.5, 0.5]), 0.5),\n        # Case 6\n        (np.array([3.0, 0.4]), 0.5),\n    ]\n\n    results = []\n    for s_values, tau in test_cases:\n        order = select_order(s_values, tau)\n        results.append(order)\n\n    # Print results in the required format \"[r1,r2,r3,r4,r5,r6]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2883912"}, {"introduction": "除了基于奇异值的方法，信息准则也是模型阶数选择的主流工具，尤其是在基于预测误差的模型（如ARX模型）中。本练习探讨了赤池信息准则（$AIC$）及其修正版（$AICc$）在实践中的选择问题。通过分析它们的适用场景，你将学会如何在样本数据有限（这在闭环实验中很常见）的情况下，更明智地权衡模型的复杂性与拟合优度，从而避免过拟合 [@problem_id:2883949]。", "problem": "您正在从长度为 $N$ 的开环或闭环采集中得到的数据中，选择一个带有外生输入的自回归 (ARX) 模型的阶数。ARX($n_a,n_b,n_k$) 结构为\n$$\ny(t) + a_1 y(t-1) + \\cdots + a_{n_a} y(t-n_a) = b_1 u(t-n_k) + \\cdots + b_{n_b} u(t-n_k-n_b+1) + e(t),\n$$\n其中 $e(t)$ 是方差为 $\\sigma^2$ 的零均值高斯白噪声。模型通过最大似然法（在高斯假设下等同于最小二乘法）进行估计，并通过最小化根据拟合的一步预测误差计算出的信息准则来选择模型阶数。考虑两个准则：赤池信息准则 (AIC) 和修正的赤池信息准则 (AICc)，其中 AICc 是 AIC 的一个小样本修正，旨在减少预期 Kullback–Leibler 散度的偏差。在闭环操作中，输入 $u(t)$ 由一个使用 $y(t)$ 的稳定控制器生成，这会因回归量中的相关性和共线性而减少有效自由度。\n\n对于 ARX 阶数选择，哪个陈述最好地解释了何时应优先选择 AICc 而不是 AIC，以及原因为何？\n\nA. 当样本量 $N$ 相对于候选 ARX 模型的自由参数数量 $k$ 不大时（例如，当 $N/k \\lesssim 40$ 时），应优先选择 AICc，包括在闭环操作下，因为反馈会减少有效样本量；当 $N \\gg k$ 以至于渐近近似占主导地位且小样本修正可忽略不计时，应优先选择 AIC。\n\nB. 仅当噪声 $e(t)$ 是重尾且非高斯时才优先选择 AICc；当 $e(t)$ 是高斯噪声时使用 AIC，因为在这种情况下，无论 $N$ 为何值，AIC 都是精确的。\n\nC. 在开环辨识中优先选择 AICc，但在闭环辨识中不选择，因为反馈会使最小二乘法产生偏差，导致 AICc 无效而 AIC 仍然适用。\n\nD. 当 $N - k - 1 \\le 0$ 时优先选择 AICc 以避免过度惩罚，否则选择 AIC，因为 AICc 是专为极小样本 $N$ 设计的，即使其修正项的分母变为非正数。\n\nE. 只要实验中存在任何反馈，就优先选择 AICc，无论 $N$ 和 $k$ 的值如何，因为 AICc 明确惩罚闭环复杂性，而 AIC 不会。", "solution": "该问题陈述具有科学依据、提法恰当、客观且自洽。它描述了系统辨识中的一个标准问题：使用信息准则为带有外生输入的自回归 (ARX) 模型选择模型阶数，同时考虑到闭环数据采集引入的复杂性。所提出的定义和概念，如 ARX 模型结构、最大似然估计、赤池信息准则 ($AIC$) 及其修正版本 ($AICc$)，都是标准的且描述正确。关于闭环操作可能减少有效自由度的说法，在此背景下也是一个有效且相关的考虑因素。因此，该问题是有效的，并且可以推导出解决方案。\n\n问题要求对选择 ARX 模型阶数时，何时应优先选择修正的赤池信息准则 ($AICc$) 而不是标准的赤池信息准则 ($AIC$) 给出最佳解释。\n\n首先，让我们定义这些准则。ARX($n_a, n_b, n_k$) 模型可以写成线性回归形式：\n$$ y(t) = \\phi(t)^T \\theta + e(t) $$\n其中 $\\theta$ 是待估计的 $k = n_a + n_b$ 个参数的向量，$\\phi(t)$ 是相应的回归量向量。在噪声 $e(t)$ 是高斯白噪声的假设下，参数 $\\theta$ 的最大似然估计是通过最小化预测误差的平方和（最小二乘法）得到的。\n\n赤池信息准则 ($AIC$) 由下式给出：\n$$ AIC = 2k - 2 \\ln(\\hat{L}) $$\n其中 $k$ 是模型中估计参数的数量，$\\hat{L}$ 是似然函数的最大化值。对于具有 $N$ 个有效数据点并假设为高斯噪声的线性回归模型，这在相差一个加性常数的情况下变为：\n$$ AIC = N \\ln(\\hat{\\sigma}^2) + 2k $$\n其中 $\\hat{\\sigma}^2$ 是噪声方差的最大似然估计，$\\hat{\\sigma}^2 = RSS/N$，而 $RSS$ 是残差平方和。$AIC$ 是作为真实数据生成过程与拟合模型之间预期 Kullback-Leibler 散度的渐近无偏估计量推导出来的。其推导依赖于对大样本量（即 $N \\to \\infty$）有效的近似。\n\n修正的赤池信息准则 ($AICc$) 是为小样本场景开发的 $AIC$ 的一种修改。对于线性回归模型，其公式为：\n$$ AICc = AIC + \\frac{2k(k+1)}{N - k - 1} $$\n代入 $AIC$ 的表达式，我们得到：\n$$ AICc = N \\ln(\\hat{\\sigma}^2) + 2k + \\frac{2k(k+1)}{N-k-1} = N \\ln(\\hat{\\sigma}^2) + 2k \\left( \\frac{N-1}{N-k-1} \\right) $$\n关键部分是修正项 $\\frac{2k(k+1)}{N-k-1}$。该项根据样本量 $N$ 相对于参数数量 $k$ 的大小来调整对模型复杂度的惩罚。\n\n我们观察到 $AICc$ 的以下性质：\n1.  当样本量 $N$ 远大于参数数量 $k$ 时（$N \\gg k$），修正项趋近于零：\n    $$ \\lim_{N \\to \\infty} \\frac{2k(k+1)}{N-k-1} = 0 $$\n    在这种渐近情况下，$AICc \\approx AIC$。\n2.  当 $N$ 相对于 $k$ 不大时，修正项为正且可能相当大。这意味着 $AICc$ 对复杂度 ($k$) 施加的惩罚比 $AIC$ 更大。这种增加的惩罚有助于抵消在样本量较小时 $AIC$ 选择过于复杂模型（过拟合）的趋势。由 Burnham 和 Anderson 提出的一个常用经验法则是，当比率 $N/k$ 较小时（例如，当 $N/k \\lesssim 40$ 时），使用 $AICc$ 代替 $AIC$。\n\n现在，考虑闭环辨识的背景。问题陈述中提到，输入 $u(t)$ 由一个使用 $y(t)$ 的控制器生成。这种反馈在过去输出和当前输入之间产生相关性，可能导致回归量向量 $\\phi(t) = [y(t-1), \\dots, y(t-n_a), u(t-n_k), \\dots]^T$ 中出现共线性或近共线性。尽管对于具有白噪声 $e(t)$ 的 ARX 模型结构，最小二乘估计仍然是一致的，但回归量中的高度共线性会降低数据中信息的质量。这通常被描述为“有效样本量”或“自由度”的减少。参数估计的性质（例如，它们的方差）表现得好像它们是从一个更小但信息更丰富的数据集中获得的。因为 $AIC$ 的性能依赖于大样本渐近性，有效样本量的减少会削弱其有效性。在这种情况下，$AICc$ 提供的有限样本修正变得更为关键。因此，反馈的存在，特别是来自没有足够外部激励的低复杂度控制器的反馈，更强调了使用 $AICc$ 的必要性。\n\n在确立了这些原则之后，我们来评估给出的选项。\n\nA. 当样本量 $N$ 相对于候选 ARX 模型的自由参数数量 $k$ 不大时（例如，当 $N/k \\lesssim 40$ 时），应优先选择 AICc，包括在闭环操作下，因为反馈会减少有效样本量；当 $N \\gg k$ 以至于渐近近似占主导地位且小样本修正可忽略不计时，应优先选择 AIC。\n这个陈述与我们的推导完全一致。它正确地指出，在 $AICc$ 和 $AIC$ 之间的选择取决于样本量 $N$ 与参数数量 $k$ 的比率。它提供了一个标准指南 ($N/k \\lesssim 40$)。它正确地陈述了对于大的 $N$，$AICc$ 会收敛到 $AIC$。最重要的是，它正确地将闭环操作与有效样本量的减少联系起来，使得在这种情况下使用 $AICc$ 更为贴切。这是一个全面而准确的解释。\n结论：**正确**\n\nB. 仅当噪声 $e(t)$ 是重尾且非高斯时才优先选择 AICc；当 $e(t)$ 是高斯噪声时使用 AIC，因为在这种情况下，无论 $N$ 为何值，AIC 都是精确的。\n这个陈述是不正确的。$AICc$ 的标准推导是专门针对具有高斯噪声的线性模型的。其目的是为了修正小样本量，而不是非高斯噪声。此外，对于高斯噪声，$AIC$ 并非在任何 $N$ 值下都是“精确”的；它是一个*渐近*准则，并且已知在小样本 $N$ 时存在偏差，这正是开发 $AICc$ 的原因。\n结论：**不正确**\n\nC. 在开环辨识中优先选择 AICc，但在闭环辨识中不选择，因为反馈会使最小二乘法产生偏差，导致 AICc 无效而 AIC 仍然适用。\n这个陈述是不正确的。首先，对于所指定的具有白噪声 $e(t)$ 的 ARX 模型，即使在闭环中，直接最小二乘估计也是一致的。其次，即使估计量有偏差，这个问题也会同时影响 $AIC$ 和 $AICc$，因为两者都基于相同的最大化似然。没有理由一个会“无效”而另一个仍然“适用”。事实上，如上所述，闭环辨识的条件通常会加强使用 $AICc$ 的理由。\n结论：**不正确**\n\nD. 当 $N - k - 1 \\le 0$ 时优先选择 AICc 以避免过度惩罚，否则选择 AIC，因为 AICc 是专为极小样本 $N$ 设计的，即使其修正项的分母变为非正数。\n这个陈述是荒谬的。条件 $N - k - 1 \\le 0$ 意味着参数数量 $k$ 至少为 $N-1$。在这种情况下，模型是饱和的或过参数化的，估计问题是病态的。$AICc$ 修正项的分母变为零或负，导致 $AICc$ 的值为无穷大或未定义。这正确地表明模型不可用；这是终极的惩罚，而不是避免过度惩罚。$AICc$ 是为 $N > k+1$ 的情况设计的。\n结论：**不正确**\n\nE. 只要实验中存在任何反馈，就优先选择 AICc，无论 $N$ 和 $k$ 的值如何，因为 AICc 明确惩罚闭环复杂性，而 AIC 不会。\n这个陈述是不正确的。$AICc$ 的公式中没有包含与反馈或“闭环复杂性”相关的明确项。其惩罚项仅是 $N$ 和 $k$ 的函数。$AICc$ 在闭环设置中的相关性是反馈如何影响数据质量的间接结果，模拟了小样本情况。这种偏好不是绝对的（“只要”），也不是“无论 $N$ 和 $k$ 的值如何”。如果 $N \\gg k$，即使对于闭环数据，$AIC$ 也足够了。\n结论：**不正确**", "answer": "$$\\boxed{A}$$", "id": "2883949"}]}