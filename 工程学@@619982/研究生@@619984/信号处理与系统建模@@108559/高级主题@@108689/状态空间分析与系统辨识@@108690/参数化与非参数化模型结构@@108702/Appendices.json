{"hands_on_practices": [{"introduction": "我们的实践探索始于一项基础的参数化建模任务。这项练习将指导你使用经典的最小二乘法来估计一个自回归外源输入 ($ARX$) 模型的参数。通过将此技术应用于一组给定的输入输出数据，你将从零开始构建正规方程并求解模型系数，从而获得系统辨识中最基本技能之一的实践经验 [@problem_id:2889301]。", "problem": "考虑以预测误差形式指定的线性时不变带外源输入的自回归 (ARX) 模型\n$$\ny(k) \\;=\\; -a_{1}\\,y(k-1)\\;-\\;a_{2}\\,y(k-2)\\;+\\;b_{1}\\,u(k-1)\\;+\\;b_{2}\\,u(k-2)\\;+\\;e(k),\n$$\n其中 $y(k)$ 是系统输出，$u(k)$ 是已知输入，$e(k)$ 是零均值扰动，未知参数向量为 $\\theta \\equiv \\begin{pmatrix} a_{1} & a_{2} & b_{1} & b_{2} \\end{pmatrix}^{\\top}$。给定测量的输入输出数据和初始条件\n$$\nu(-1)=0,\\quad u(0)=0,\\quad y(-1)=0,\\quad y(0)=0,\n$$\n以及以下有限序列：\n$$\n\\begin{aligned}\n&u(1)=1,\\; u(2)=2,\\; u(3)=0,\\; u(4)=-1,\\; u(5)=1,\\; u(6)=0,\\\\\n&y(1)=0,\\; y(2)=1,\\; y(3)=2.5,\\; y(4)=1.05,\\; y(5)=-0.975,\\; y(6)=0.3025,\\; y(7)=0.34625.\n\\end{aligned}\n$$\n假设对于此数据集，扰动恒为零，即对于下面使用的所有 $k$，都有 $e(k)=0$。建立一个批处理最小二乘估计问题，该问题最小化数据索引 $k=2,3,4,5,6,7$ 上的单步向前预测误差平方和，并根据上述数据构建相应的线性正规方程。然后计算参数 $b_{1}$ 的最小二乘估计值。\n\n将标量 $b_{1}$ 的最终值表示为一个精确数。无需四舍五入。最终答案必须是单个实数。", "solution": "我们从模型定义和最小二乘原理开始。在时刻 $k$ 的单步向前预测误差是\n$$\n\\varepsilon(k;\\theta)\\;=\\;y(k)\\;-\\;\\big(-a_{1}\\,y(k-1)-a_{2}\\,y(k-2)+b_{1}\\,u(k-1)+b_{2}\\,u(k-2)\\big).\n$$\n定义回归向量\n$$\n\\varphi(k)\\;\\equiv\\;\\begin{pmatrix}-y(k-1)\\\\ -y(k-2)\\\\ u(k-1)\\\\ u(k-2)\\end{pmatrix},\\qquad \\theta\\;\\equiv\\;\\begin{pmatrix}a_{1}\\\\ a_{2}\\\\ b_{1}\\\\ b_{2}\\end{pmatrix},\n$$\n从而误差为 $\\varepsilon(k;\\theta)=y(k)-\\varphi(k)^{\\top}\\theta$。在索引集 $\\mathcal{K}=\\{2,3,4,5,6,7\\}$ 上的批处理最小二乘 (LS) 准则是\n$$\nJ(\\theta)\\;=\\;\\sum_{k\\in\\mathcal{K}}\\varepsilon(k;\\theta)^{2}\\;=\\;\\sum_{k\\in\\mathcal{K}}\\big(y(k)-\\varphi(k)^{\\top}\\theta\\big)^{2}.\n$$\n通过将其关于 $\\theta$ 的梯度设为零来最小化 $J(\\theta)$，即可得到正规方程\n$$\n\\left(\\sum_{k\\in\\mathcal{K}}\\varphi(k)\\,\\varphi(k)^{\\top}\\right)\\theta\\;=\\;\\sum_{k\\in\\mathcal{K}}\\varphi(k)\\,y(k).\n$$\n等价地，如果我们定义数据矩阵 $\\Phi\\in\\mathbb{R}^{6\\times 4}$（其行为 $\\varphi(k)^{\\top}$, $k=2,\\dots,7$）和数据向量 $Y\\in\\mathbb{R}^{6}$（其元素为 $y(k)$, $k=2,\\dots,7$），则有紧凑形式\n$$\n\\Phi^{\\top}\\Phi\\,\\theta\\;=\\;\\Phi^{\\top}Y.\n$$\n\n我们现在根据所提供的数据显式地构建 $\\Phi$ 和 $Y$。对于每个 $k\\in\\{2,3,4,5,6,7\\}$，我们计算 $\\varphi(k)$：\n$$\n\\begin{aligned}\n&k=2:\\;\\; \\varphi(2)^{\\top}=\\begin{pmatrix}-y(1)&-y(0)&u(1)&u(0)\\end{pmatrix}=\\begin{pmatrix}0&0&1&0\\end{pmatrix},\\;\\; y(2)=1,\\\\\n&k=3:\\;\\; \\varphi(3)^{\\top}=\\begin{pmatrix}-y(2)&-y(1)&u(2)&u(1)\\end{pmatrix}=\\begin{pmatrix}-1&0&2&1\\end{pmatrix},\\;\\; y(3)=2.5,\\\\\n&k=4:\\;\\; \\varphi(4)^{\\top}=\\begin{pmatrix}-y(3)&-y(2)&u(3)&u(2)\\end{pmatrix}=\\begin{pmatrix}-2.5&-1&0&2\\end{pmatrix},\\;\\; y(4)=1.05,\\\\\n&k=5:\\;\\; \\varphi(5)^{\\top}=\\begin{pmatrix}-y(4)&-y(3)&u(4)&u(3)\\end{pmatrix}=\\begin{pmatrix}-1.05&-2.5&-1&0\\end{pmatrix},\\;\\; y(5)=-0.975,\\\\\n&k=6:\\;\\; \\varphi(6)^{\\top}=\\begin{pmatrix}-y(5)&-y(4)&u(5)&u(4)\\end{pmatrix}=\\begin{pmatrix}0.975&-1.05&1&-1\\end{pmatrix},\\;\\; y(6)=0.3025,\\\\\n&k=7:\\;\\; \\varphi(7)^{\\top}=\\begin{pmatrix}-y(6)&-y(5)&u(6)&u(5)\\end{pmatrix}=\\begin{pmatrix}-0.3025&0.975&0&1\\end{pmatrix},\\;\\; y(7)=0.34625.\n\\end{aligned}\n$$\n因此\n$$\n\\Phi=\\begin{pmatrix}\n0 & 0 & 1 & 0\\\\\n-1 & 0 & 2 & 1\\\\\n-2.5 & -1 & 0 & 2\\\\\n-1.05 & -2.5 & -1 & 0\\\\\n0.975 & -1.05 & 1 & -1\\\\\n-0.3025 & 0.975 & 0 & 1\n\\end{pmatrix},\\qquad\nY=\\begin{pmatrix}\n1\\\\\n2.5\\\\\n1.05\\\\\n-0.975\\\\\n0.3025\\\\\n0.34625\n\\end{pmatrix}.\n$$\n正规方程的具体形式为\n$$\n\\underbrace{\\Phi^{\\top}\\Phi}_{G}\\,\\theta\\;=\\;\\underbrace{\\Phi^{\\top}Y}_{g},\n$$\n其中\n$$\nG=\\begin{pmatrix}\n9.39463125 & 3.8063125 & 0.025 & -7.2775\\\\\n3.8063125 & 9.303125 & 1.45 & 0.025\\\\\n0.025 & 1.45 & 7 & 1\\\\\n-7.2775 & 0.025 & 1 & 7\n\\end{pmatrix},\\qquad\ng=\\begin{pmatrix}\n-3.911053125\\\\\n1.40746875\\\\\n7.2775\\\\\n4.64375\n\\end{pmatrix}.\n$$\n至此，正规方程的构建完成。\n\n为了计算最小二乘估计，我们注意到，如果数据是无噪声的且 $\\Phi$ 具有满列秩，则最小二乘解 $\\hat{\\theta}$ 精确满足 $\\Phi\\,\\hat{\\theta}=Y$。从第一个回归行（对应于 $k=2$）来看，\n$$\ny(2)\\;=\\;-a_{1}\\,y(1)\\;-\\;a_{2}\\,y(0)\\;+\\;b_{1}\\,u(1)\\;+\\;b_{2}\\,u(0)\\;=\\;0\\;+\\;0\\;+\\;b_{1}\\cdot 1\\;+\\;0,\n$$\n这立即意味着\n$$\nb_{1}\\;=\\;y(2)\\;=\\;1.\n$$\n为完整起见，我们可以验证存在 $(a_{1},a_{2},b_{2})$，在 $b_1$ 取该值时，使得所有残差为零。使用 $b_{1}=1$ 和随后的三个方程（对于 $k=3,4,5$），\n$$\n\\begin{aligned}\n&k=3:\\;\\;2.5=-a_{1}\\cdot 1-a_{2}\\cdot 0+1\\cdot 2+b_{2}\\cdot 1\\;\\;\\Rightarrow\\;\\;-a_{1}+b_{2}=0.5,\\\\\n&k=4:\\;\\;1.05=-a_{1}\\cdot 2.5-a_{2}\\cdot 1+1\\cdot 0+b_{2}\\cdot 2\\;\\;\\Rightarrow\\;\\;-2.5a_{1}-a_{2}+2b_{2}=1.05,\\\\\n&k=5:\\;\\;-0.975=-a_{1}\\cdot 1.05-a_{2}\\cdot 2.5+1\\cdot(-1)+b_{2}\\cdot 0\\;\\;\\Rightarrow\\;\\;-1.05a_{1}-2.5a_{2}=-0.975+1=0.025,\n\\end{aligned}\n$$\n解得 $a_{1}=-0.5$，$a_{2}=0.2$ 和 $b_{2}=0$。使用这些值，所有六个方程都被精确满足，因此残差向量恒为零。因此，最小二乘估计量与此解一致，特别是 $b_{1}$ 的最小二乘估计值为\n$$\n\\hat{b}_{1}\\;=\\;1.\n$$\n这个值也满足正规方程的第三个分量，因为代入 $\\theta=\\begin{pmatrix}-0.5&0.2&1&0\\end{pmatrix}^{\\top}$ 可得\n$$\n\\begin{pmatrix}0.025 & 1.45 & 7 & 1\\end{pmatrix}\\theta\\;=\\;0.025(-0.5)+1.45(0.2)+7(1)+1(0)\\;=\\;-0.0125+0.29+7\\;=\\;7.2775\\;=\\;g_{3}.\n$$\n因此，对于给定的数据集，$b_{1}$ 的最小二乘估计值精确地为 $1$。", "answer": "$$\\boxed{1}$$", "id": "2889301"}, {"introduction": "接着，我们将焦点从参数模型转向探索非参数谱分析的基石：Welch 法。这个问题 [@problem_id:2889322] 集中于计算功率谱密度估计的频率分辨率，而该分辨率从根本上由所选窗函数的性质决定。通过这个练习，你将明白分析者的选择如何直接塑造谱细节与估计器稳定性之间的权衡，这是与参数化方法的一个关键区别。", "problem": "一个实、广义平稳、零均值的离散时间过程 $x[n]$ 以采样频率 $f_{s} = 48\\,\\text{kHz}$ 进行采样。你需要使用Welch方法（一种非参数谱估计器）来估计其功率谱密度(PSD)，并将其频率分辨率的概念与参数化方法的频率分辨率进行对比，而无需计算任何参数化估计。总记录长度为 $N_{\\text{tot}} = 1{,}228{,}800$ 个样本。你选择以下Welch参数：段长度 $L = 4096$ 个样本，相邻段之间有 $50\\%$ 的重叠，以及周期性汉宁窗 $w[n]$，其在 $n=0,1,\\dots,L-1$ 上的定义为\n$$\nw[n] = \\tfrac{1}{2}\\Big(1 - \\cos\\!\\big(\\tfrac{2\\pi n}{L}\\big)\\Big).\n$$\n假设每个段的离散傅里叶变换(DFT)长度为 $K=L$（无补零）。\n\n在此设置下，将Welch PSD估计的频率分辨率定义为谱窗的等效噪声带宽(ENBW)，以赫兹(Hz)为单位表示。仅使用Welch方法和ENBW的基本定义，以及离散时间余弦序列求和的性质，计算最终的频率分辨率（以赫兹为单位）。请给出无舍入的精确值作为最终答案，并以 $\\text{Hz}$ 表示结果（在最终的方框答案中不要包含单位）。", "solution": "在尝试求解之前，将对所述问题进行验证。\n\n首先，从问题陈述中逐字提取已知条件。\n- 过程：一个实、广义平稳、零均值的离散时间过程 $x[n]$。\n- 采样频率：$f_{s} = 48\\,\\text{kHz}$。\n- 总记录长度：$N_{\\text{tot}} = 1{,}228{,}800$ 个样本。\n- Welch方法参数：段长度 $L = 4096$ 个样本，$50\\%$ 重叠。\n- 窗函数：周期性汉宁窗 $w[n] = \\tfrac{1}{2}\\Big(1 - \\cos\\!\\big(\\tfrac{2\\pi n}{L}\\big)\\Big)$，对于 $n=0,1,\\dots,L-1$。\n- DFT长度：$K=L$。\n- 频率分辨率的定义：谱窗的等效噪声带宽(ENBW)，以赫兹为单位表示。\n- 约束：推导必须仅使用基本定义。\n\n其次，根据所需标准对问题进行验证。\n- **科学基础**：该问题具有科学合理性。它描述了使用Welch方法进行功率谱密度(PSD)估计的标准应用，这是信号处理中非参数谱分析的基石。汉宁窗和等效噪声带宽的定义是标准且正确的。\n- **适定性**：该问题是适定的。它提供了计算ENBW所需的所有必要参数（$L$、$f_s$ 和窗函数定义）。问题明确，且有唯一、稳定的解。关于 $N_{\\text{tot}}$ 和重叠的信息，虽然与完整Welch估计的方差有关，但对于计算单个加窗段的频率分辨率而言并非必需，其存在不会造成矛盾。\n- **客观性**：语言客观、技术性强，没有主观或非科学性的断言。\n- **完整性与一致性**：问题是自洽的，其约束条件是一致的。\n\n结论是：该问题有效，可以求解。\n\n对于像Welch方法这样的非参数谱估计器，其频率分辨率由应用于每个数据段的窗函数的光谱特性决定。问题将此分辨率定义为窗的等效噪声带宽(ENBW)，以赫兹为单位表示。\n\n长度为 $L$ 的离散时间窗 $w[n]$ 的ENBW，以归一化频率（周期/样本）定义为窗系数平方和与窗系数和的平方之比：\n$$\n\\text{ENBW}_{\\text{norm}} = \\frac{\\sum_{n=0}^{L-1} w^2[n]}{\\left(\\sum_{n=0}^{L-1} w[n]\\right)^2}\n$$\n要将此归一化带宽转换为以赫兹为单位的物理频率，必须乘以采样频率 $f_s$：\n$$\n\\text{Resolution} = \\text{ENBW}_{\\text{Hz}} = \\text{ENBW}_{\\text{norm}} \\times f_s\n$$\n任务简化为计算给定周期性汉宁窗 $w[n] = \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)$ 在 $n=0, 1, \\dots, L-1$ 上的两个和。\n\n首先，我们计算窗系数的和，记为 $S_1$：\n$$\nS_1 = \\sum_{n=0}^{L-1} w[n] = \\sum_{n=0}^{L-1} \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)\n$$\n根据求和的线性性：\n$$\nS_1 = \\frac{1}{2} \\left[ \\left(\\sum_{n=0}^{L-1} 1\\right) - \\left(\\sum_{n=0}^{L-1} \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right) \\right]\n$$\n第一项是 $\\sum_{n=0}^{L-1} 1 = L$。第二项是一个余弦函数在一个完整周期（$L$个样本）上的和。对于 $L > 1$，此和为零。这可以通过考虑复指数的和 $\\sum_{n=0}^{L-1} \\exp\\left(j\\frac{2\\pi k n}{L}\\right)$ 来证明，对于任何不是 $L$ 的倍数的整数 $k$，该和等于 $0$。此处，$k=1$。取实部即可证实 $\\sum_{n=0}^{L-1} \\cos(\\frac{2\\pi n}{L}) = 0$。鉴于 $L = 4096$，此条件成立。\n因此，\n$$\nS_1 = \\frac{1}{2} (L - 0) = \\frac{L}{2}\n$$\n\n接下来，我们计算窗系数的平方和，记为 $S_2$：\n$$\nS_2 = \\sum_{n=0}^{L-1} w^2[n] = \\sum_{n=0}^{L-1} \\left[ \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right) \\right]^2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)^2\n$$\n展开平方项：\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\cos^2\\left(\\frac{2\\pi n}{L}\\right)\\right)\n$$\n我们使用降幂恒等式 $\\cos^2(\\theta) = \\frac{1}{2}(1 + \\cos(2\\theta))$：\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{4\\pi n}{L}\\right)\\right)\\right)\n$$\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(\\frac{3}{2} - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\cos\\left(\\frac{4\\pi n}{L}\\right)\\right)\n$$\n利用求和的线性性：\n$$\nS_2 = \\frac{1}{4} \\left[ \\frac{3}{2}\\sum_{n=0}^{L-1} 1 - 2\\sum_{n=0}^{L-1} \\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\sum_{n=0}^{L-1} \\cos\\left(\\frac{4\\pi n}{L}\\right) \\right]\n$$\n如前所述，$\\sum \\cos(\\frac{2\\pi n}{L}) = 0$。类似地，对于 $L>2$，$\\sum \\cos(\\frac{4\\pi n}{L}) = 0$，这对 $L=4096$ 成立。\n剩下：\n$$\nS_2 = \\frac{1}{4} \\left[ \\frac{3L}{2} - 2(0) + \\frac{1}{2}(0) \\right] = \\frac{3L}{8}\n$$\n\n现在，我们可以计算归一化的ENBW：\n$$\n\\text{ENBW}_{\\text{norm}} = \\frac{S_2}{S_1^2} = \\frac{\\frac{3L}{8}}{\\left(\\frac{L}{2}\\right)^2} = \\frac{\\frac{3L}{8}}{\\frac{L^2}{4}} = \\frac{3L \\cdot 4}{8 \\cdot L^2} = \\frac{12L}{8L^2} = \\frac{3}{2L}\n$$\n这是汉宁窗众所周知的ENBW。频率分辨率与窗长 $L$ 成反比。\n\n最后，我们使用给定的值 $L=4096$ 和 $f_s = 48000\\,\\text{Hz}$ 计算以赫兹为单位的分辨率：\n$$\n\\text{Resolution} = \\text{ENBW}_{\\text{Hz}} = \\frac{3}{2L} \\times f_s = \\frac{3}{2 \\times 4096} \\times 48000 = \\frac{3 \\times 48000}{8192} = \\frac{144000}{8192}\n$$\n为了简化分数：\n$$\n\\frac{144000}{8192} = \\frac{144 \\times 1000}{8 \\times 1024} = \\frac{18 \\times 1000}{1024} = \\frac{18000}{1024} = \\frac{9000}{512} = \\frac{4500}{256} = \\frac{2250}{128} = \\frac{1125}{64}\n$$\n精确值为 $1125/64$，其十进制形式为 $17.578125$。\n\n问题还要求与参数化方法进行对比。在像Welch这样的非参数方法中，频率分辨率（此处定义为 $1.5 \\frac{f_s}{L}$）由分析者选择的窗长 $L$ 固定。较长的窗提供更好的分辨率（更窄的主瓣），但代价是对于固定的总数据记录，估计器的方差会增加，因为可用于平均的段数变少。这种分辨率与信号内容无关。相比之下，参数化方法（例如，自回归(AR)、移动平均(MA)或自回归移动平均(ARMA)模型）假设信号是由白噪声驱动的线性时不变系统生成的。PSD则是模型参数的函数。在这种情况下，“分辨率”不是一个固定的带宽，而是表示尖锐谱特征的能力。如果基础数据符合所选的模型结构，参数化方法可以获得卓越的分辨率，即使在数据记录很短的情况下，也能分辨出频率相近的正弦波，远超非参数方法 $1/L$ 的限制。然而，这种性能严重依赖于模型假设的正确性；模型失配可能导致谱估计严重不准确和误导。", "answer": "$$\\boxed{17.578125}$$", "id": "2889322"}, {"introduction": "我们的最后一个练习回归到参数化建模，以解决一个更高级且非常实际的挑战：模型正则化。这项练习 [@problem_id:2889347] 介绍用于有限脉冲响应 ($FIR$) 模型的岭回归，并演示如何应用广义交叉验证 ($GCV$) 准则来自动选择最优的正则化参数 $\\lambda$。这项强大的技术能够平衡模型对数据的拟合度与其复杂性，从而构建出更稳健、泛化能力更强的模型。", "problem": "考虑从输入输出数据 $\\{(u(t),y(t))\\}_{t=1}^{T}$ 中辨识一个阶数为 $m$ 的有限脉冲响应 (FIR) 模型。设回归矩阵为 $\\Phi \\in \\mathbb{R}^{T \\times m}$，其第 $t$ 行为 $\\varphi(t)^{\\top} \\coloneqq \\big(u(t),u(t-1),\\dots,u(t-m+1)\\big)$，参数模型为 $y(t) \\approx \\varphi(t)^{\\top}\\theta$，其中 $\\theta \\in \\mathbb{R}^{m}$。岭正则化辨识通过最小化惩罚最小二乘准则 $\\|\\mathbf{y}-\\Phi\\theta\\|_{2}^{2}+\\lambda\\|\\theta\\|_{2}^{2}$ 来估计 $\\theta$，其中正则化参数 $\\lambda \\ge 0$，$\\mathbf{y} \\in \\mathbb{R}^{T}$ 是由 $y(t)$ 堆叠而成的向量。\n\n仅从正规方程和基本线性代数恒等式出发，推导用于岭正则化 FIR 辨识的广义交叉验证 (GCV) 准则。你的推导必须：\n- 明确指出将 $\\mathbf{y}$ 映射到其拟合值 $\\widehat{\\mathbf{y}}$ 的线性平滑（帽子）矩阵 $S(\\lambda)$，\n- 使用样本内残差和 $S(\\lambda)$ 的对角线元素来表示留一法交叉验证 (LOOCV) 的残差，以及\n- 通过将各个对角线元素替换为其平均值来获得 GCV 泛函 $V_{\\mathrm{GCV}}(\\lambda)$。\n\n然后将你的表达式特化到 $m=1$（单个 FIR 抽头）的情况，此时 $\\Phi=\\mathbf{u}\\in\\mathbb{R}^{T\\times 1}$，其中 $\\mathbf{u} \\coloneqq \\big(u(1),\\dots,u(T)\\big)^{\\top}$。证明 $V_{\\mathrm{GCV}}(\\lambda)$ 可简化为一个关于 $\\lambda$ 的一维函数，该函数依赖于 $\\Phi$ 的奇异值以及 $\\mathbf{y}$ 在其列空间和正交补上的投影。\n\n最后，对于具体数据集 $T=5$，输入序列 $\\mathbf{u}=\\begin{pmatrix}1\\\\2\\\\-1\\\\0\\\\2\\end{pmatrix}$ 和输出序列 $\\mathbf{y}=\\begin{pmatrix}5\\\\4\\\\1\\\\0\\\\4\\end{pmatrix}$，计算使所推导的 $V_{\\mathrm{GCV}}(\\lambda)$ 最小化的唯一值 $\\lambda^{\\star}\\ge 0$。请给出 $\\lambda^{\\star}$ 的精确值。无需四舍五入。最终答案必须是单个实数。", "solution": "所述问题具有科学依据，是适定的、客观的且内部一致的。它构成了统计学习和系统辨识中的一个标准练习。所有必要的数据和定义都已提供。因此，该问题是有效的，我们应继续进行推导和求解。\n\n目标是找到岭回归参数估计值 $\\widehat{\\theta}$，以最小化成本函数\n$$J(\\theta) = \\|\\mathbf{y}-\\Phi\\theta\\|_{2}^{2}+\\lambda\\|\\theta\\|_{2}^{2}$$\n其中 $\\mathbf{y} \\in \\mathbb{R}^{T}$ 是输出向量，$\\Phi \\in \\mathbb{R}^{T \\times m}$ 是回归矩阵，$\\theta \\in \\mathbb{R}^{m}$ 是参数向量，$\\lambda \\ge 0$ 是正则化参数。\n\n首先，我们推导正规方程。成本函数可以写为 $J(\\theta) = (\\mathbf{y}-\\Phi\\theta)^{\\top}(\\mathbf{y}-\\Phi\\theta)+\\lambda\\theta^{\\top}\\theta$。为了找到最小值，我们计算其关于 $\\theta$ 的梯度并将其设为零：\n$$ \\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta} (\\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{y}^{\\top}\\Phi\\theta + \\theta^{\\top}\\Phi^{\\top}\\Phi\\theta + \\lambda\\theta^{\\top}\\theta) = -2\\Phi^{\\top}\\mathbf{y} + 2\\Phi^{\\top}\\Phi\\theta + 2\\lambda\\theta $$\n将梯度设为零，我们得到岭回归的正规方程：\n$$ (\\Phi^{\\top}\\Phi + \\lambda I)\\theta = \\Phi^{\\top}\\mathbf{y} $$\n对于任何 $\\lambda > 0$，矩阵 $(\\Phi^{\\top}\\Phi + \\lambda I)$ 都是正定的，因此是可逆的。解是唯一的：\n$$ \\widehat{\\theta}(\\lambda) = (\\Phi^{\\top}\\Phi + \\lambda I)^{-1}\\Phi^{\\top}\\mathbf{y} $$\n\n我们现在来推导广义交叉验证 (GCV) 准则。\n\n**第一步：确定平滑（帽子）矩阵**\n拟合值向量 $\\widehat{\\mathbf{y}}$ 由 $\\widehat{\\mathbf{y}} = \\Phi\\widehat{\\theta}(\\lambda)$ 给出。代入 $\\widehat{\\theta}(\\lambda)$ 的表达式：\n$$ \\widehat{\\mathbf{y}} = \\Phi(\\Phi^{\\top}\\Phi + \\lambda I)^{-1}\\Phi^{\\top}\\mathbf{y} $$\n根据定义，平滑矩阵或帽子矩阵 $S(\\lambda)$ 将观测数据 $\\mathbf{y}$ 映射到拟合值 $\\widehat{\\mathbf{y}}$，即 $\\widehat{\\mathbf{y}} = S(\\lambda)\\mathbf{y}$。因此，我们确定帽子矩阵为：\n$$ S(\\lambda) = \\Phi(\\Phi^{\\top}\\Phi + \\lambda I)^{-1}\\Phi^{\\top} $$\n该矩阵是一个依赖于正则化参数 $\\lambda$ 的线性算子。\n\n**第二步：表示留一法交叉验证 (LOOCV) 残差**\n留一法交叉验证误差为 $V_{\\mathrm{LOOCV}}(\\lambda) = \\frac{1}{T} \\sum_{k=1}^{T} (y(k) - \\widehat{y}^{(-k)}(k))^2$，其中 $\\widehat{y}^{(-k)}(k)$ 是使用除第 $k$ 个数据点外的所有数据训练的模型对第 $k$ 个数据点的预测值。线性回归分析中的一个基本结果，即 PRESS（预测误差平方和）恒等式，将 LOOCV 残差 $y(k) - \\widehat{y}^{(-k)}(k)$ 与普通样本内残差 $e(k) = y(k) - \\widehat{y}(k)$ 联系起来，其中 $\\widehat{y}(k)$ 是 $\\widehat{\\mathbf{y}} = S(\\lambda)\\mathbf{y}$ 的第 $k$ 个分量。该恒等式为：\n$$ y(k) - \\widehat{y}^{(-k)}(k) = \\frac{y(k) - \\widehat{y}(k)}{1 - S_{kk}(\\lambda)} $$\n其中 $S_{kk}(\\lambda)$ 是帽子矩阵 $S(\\lambda)$ 的第 $k$ 个对角元素。样本内残差向量为 $\\mathbf{e} = \\mathbf{y} - \\widehat{\\mathbf{y}} = (I-S(\\lambda))\\mathbf{y}$。\n\n**第三步：获得 GCV 泛函**\nLOOCV 准则由这些 LOOCV 残差的平方和构成：\n$$ V_{\\mathrm{LOOCV}}(\\lambda) = \\frac{1}{T} \\sum_{k=1}^{T} \\left( \\frac{y(k) - \\widehat{y}(k)}{1 - S_{kk}(\\lambda)} \\right)^2 $$\nGCV 准则通过将分母中的各个对角元素 $S_{kk}(\\lambda)$ 替换为其平均值，从而提供了对 LOOCV 的近似。该平均值由帽子矩阵的迹除以 $T$ 得到：\n$$ \\frac{1}{T} \\sum_{k=1}^{T} S_{kk}(\\lambda) = \\frac{1}{T} \\mathrm{tr}(S(\\lambda)) $$\n将此平均值代入 LOOCV 表达式，得到 GCV 泛函 $V_{\\mathrm{GCV}}(\\lambda)$：\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{1}{T} \\sum_{k=1}^{T} \\left( \\frac{y(k) - \\widehat{y}(k)}{1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))} \\right)^2 = \\frac{\\frac{1}{T} \\sum_{k=1}^{T} (y(k) - \\widehat{y}(k))^2}{\\left(1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))\\right)^2} $$\n这通常写成：\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{T}\\|\\mathbf{y} - \\widehat{\\mathbf{y}}\\|_2^2}{\\left(1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))\\right)^2} = \\frac{\\frac{1}{T}\\|(I-S(\\lambda))\\mathbf{y}\\|_2^2}{\\left(1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))\\right)^2} $$\n至此，一般性推导完成。\n\n**特化到 $m=1$ 的情况**\n对于 $m=1$，回归矩阵是单个列向量 $\\Phi = \\mathbf{u} \\in \\mathbb{R}^{T \\times 1}$。项 $\\Phi^{\\top}\\Phi$ 变为一个标量：$\\mathbf{u}^{\\top}\\mathbf{u} = \\|\\mathbf{u}\\|_2^2$。这是 $\\Phi$ 的奇异值的平方，我们记为 $\\sigma_1^2 = \\sigma^2 = \\|\\mathbf{u}\\|_2^2$。\n\n帽子矩阵的迹变为：\n$$ \\mathrm{tr}(S(\\lambda)) = \\mathrm{tr}(\\mathbf{u}(\\mathbf{u}^{\\top}\\mathbf{u} + \\lambda)^{-1}\\mathbf{u}^{\\top}) = \\mathrm{tr}((\\mathbf{u}^{\\top}\\mathbf{u} + \\lambda)^{-1}\\mathbf{u}^{\\top}\\mathbf{u}) = \\frac{\\mathbf{u}^{\\top}\\mathbf{u}}{\\mathbf{u}^{\\top}\\mathbf{u} + \\lambda} = \\frac{\\sigma^2}{\\sigma^2 + \\lambda} $$\n$V_{\\mathrm{GCV}}(\\lambda)$ 的分母是 $(1 - \\frac{1}{T}\\frac{\\sigma^2}{\\sigma^2+\\lambda})^2$。\n\n接下来，我们分析分子。帽子矩阵简化为：\n$$ S(\\lambda) = \\mathbf{u}(\\sigma^2+\\lambda)^{-1}\\mathbf{u}^{\\top} = \\frac{\\mathbf{u}\\mathbf{u}^{\\top}}{\\sigma^2+\\lambda} $$\n令 $P_{\\mathbf{u}} = \\frac{\\mathbf{u}\\mathbf{u}^{\\top}}{\\mathbf{u}^{\\top}\\mathbf{u}} = \\frac{\\mathbf{u}\\mathbf{u}^{\\top}}{\\sigma^2}$ 为到 $\\mathbf{u}$ 的列空间上的投影矩阵。则 $S(\\lambda) = \\frac{\\sigma^2}{\\sigma^2+\\lambda}P_{\\mathbf{u}}$。\n残差平方和项为 $\\|\\mathbf{y} - S(\\lambda)\\mathbf{y}\\|_2^2$。我们将 $\\mathbf{y}$ 分解为其在 $\\mathbf{u}$ 列空间上的投影 $\\mathbf{y}_{\\parallel} = P_{\\mathbf{u}}\\mathbf{y}$ 与其在正交补上的投影 $\\mathbf{y}_{\\perp} = (I-P_{\\mathbf{u}})\\mathbf{y}$ 的和。\n残差向量为：\n$$ \\mathbf{y} - S(\\lambda)\\mathbf{y} = \\mathbf{y} - \\frac{\\sigma^2}{\\sigma^2+\\lambda}P_{\\mathbf{u}}\\mathbf{y} = \\mathbf{y}_{\\parallel} + \\mathbf{y}_{\\perp} - \\frac{\\sigma^2}{\\sigma^2+\\lambda}\\mathbf{y}_{\\parallel} = \\left(1-\\frac{\\sigma^2}{\\sigma^2+\\lambda}\\right)\\mathbf{y}_{\\parallel} + \\mathbf{y}_{\\perp} = \\frac{\\lambda}{\\sigma^2+\\lambda}\\mathbf{y}_{\\parallel} + \\mathbf{y}_{\\perp} $$\n由于 $\\mathbf{y}_{\\parallel}$ 和 $\\mathbf{y}_{\\perp}$ 是正交的，其范数的平方等于它们各自范数平方的和：\n$$ \\|\\mathbf{y} - S(\\lambda)\\mathbf{y}\\|_2^2 = \\left\\|\\frac{\\lambda}{\\sigma^2+\\lambda}\\mathbf{y}_{\\parallel}\\right\\|_2^2 + \\|\\mathbf{y}_{\\perp}\\|_2^2 = \\left(\\frac{\\lambda}{\\sigma^2+\\lambda}\\right)^2\\|\\mathbf{y}_{\\parallel}\\|_2^2 + \\|\\mathbf{y}_{\\perp}\\|_2^2 $$\n综合这些结果，GCV 泛函为：\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{T}\\left[ \\left(\\frac{\\lambda}{\\sigma^2+\\lambda}\\right)^2\\|\\mathbf{y}_{\\parallel}\\|_2^2 + \\|\\mathbf{y}_{\\perp}\\|_2^2 \\right]}{\\left(1 - \\frac{1}{T}\\frac{\\sigma^2}{\\sigma^2+\\lambda}\\right)^2} $$\n该表达式清楚地表明它依赖于 $\\Phi$ 的奇异值（通过 $\\sigma^2$）和 $\\mathbf{y}$ 的投影。\n\n**针对特定数据集的计算**\n给定 $T=5$，$\\mathbf{u}=\\begin{pmatrix}1\\\\2\\\\-1\\\\0\\\\2\\end{pmatrix}$，和 $\\mathbf{y}=\\begin{pmatrix}5\\\\4\\\\1\\\\0\\\\4\\end{pmatrix}$，我们计算所需的量。\n奇异值的平方为 $\\sigma^2 = \\|\\mathbf{u}\\|_2^2 = 1^2+2^2+(-1)^2+0^2+2^2 = 1+4+1+0+4=10$。\n$\\mathbf{y}$ 在 $\\mathbf{u}$ 上的投影需要内积 $\\mathbf{u}^{\\top}\\mathbf{y}$：\n$$ \\mathbf{u}^{\\top}\\mathbf{y} = (1)(5) + (2)(4) + (-1)(1) + (0)(0) + (2)(4) = 5+8-1+0+8 = 20 $$\n$\\mathbf{y}$ 的平行分量的平方范数为：\n$$ \\|\\mathbf{y}_{\\parallel}\\|_2^2 = \\|P_{\\mathbf{u}}\\mathbf{y}\\|_2^2 = \\frac{(\\mathbf{u}^{\\top}\\mathbf{y})^2}{\\mathbf{u}^{\\top}\\mathbf{u}} = \\frac{20^2}{10} = \\frac{400}{10} = 40 $$\n原始输出向量的平方范数为 $\\|\\mathbf{y}\\|_2^2 = 5^2+4^2+1^2+0^2+4^2 = 25+16+1+0+16 = 58$。\n正交分量的平方范数通过勾股定理求得：\n$$ \\|\\mathbf{y}_{\\perp}\\|_2^2 = \\|\\mathbf{y}\\|_2^2 - \\|\\mathbf{y}_{\\parallel}\\|_2^2 = 58 - 40 = 18 $$\n现在, 我们将 $T=5$，$\\sigma^2=10$，$\\|\\mathbf{y}_{\\parallel}\\|_2^2=40$，以及 $\\|\\mathbf{y}_{\\perp}\\|_2^2=18$ 代入 $V_{\\mathrm{GCV}}(\\lambda)$ 的表达式：\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{5}\\left[ \\left(\\frac{\\lambda}{10+\\lambda}\\right)^2(40) + 18 \\right]}{\\left(1 - \\frac{1}{5}\\frac{10}{10+\\lambda}\\right)^2} = \\frac{\\frac{1}{5}\\left[ \\frac{40\\lambda^2}{(10+\\lambda)^2} + 18 \\right]}{\\left(\\frac{5(10+\\lambda)-10}{5(10+\\lambda)}\\right)^2} $$\n化简得：\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{5} \\frac{40\\lambda^2 + 18(10+\\lambda)^2}{(10+\\lambda)^2}}{\\frac{(40+5\\lambda)^2}{25(10+\\lambda)^2}} = \\frac{5[40\\lambda^2 + 18(100+20\\lambda+\\lambda^2)]}{(40+5\\lambda)^2} $$\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{5[40\\lambda^2 + 1800+360\\lambda+18\\lambda^2]}{25(8+\\lambda)^2} = \\frac{58\\lambda^2+360\\lambda+1800}{5(8+\\lambda)^2} $$\n为了找到使 $V_{\\mathrm{GCV}}(\\lambda)$ 最小化的值 $\\lambda^{\\star} \\ge 0$，我们对 $\\lambda$ 求导并将导数设为零。我们只需考虑函数 $f(\\lambda) = \\frac{58\\lambda^2+360\\lambda+1800}{(\\lambda+8)^2}$。\n使用商法则 $\\frac{d}{d\\lambda}\\left(\\frac{N(\\lambda)}{D(\\lambda)}\\right) = \\frac{N'(\\lambda)D(\\lambda)-N(\\lambda)D'(\\lambda)}{D(\\lambda)^2}$：\n$$ N'(\\lambda) = 116\\lambda + 360 $$\n$$ D(\\lambda) = (\\lambda+8)^2 \\implies D'(\\lambda) = 2(\\lambda+8) $$\n对于 $\\lambda \\neq -8$，将导数的分子设为零：\n$$ (116\\lambda+360)(\\lambda+8) - (58\\lambda^2+360\\lambda+1800)(2) = 0 $$\n$$ 116\\lambda^2 + 928\\lambda + 360\\lambda + 2880 - 116\\lambda^2 - 720\\lambda - 3600 = 0 $$\n$\\lambda^2$ 项消去。我们合并余下的项：\n$$ (928+360-720)\\lambda + (2880-3600) = 0 $$\n$$ 568\\lambda - 720 = 0 $$\n$$ 568\\lambda = 720 $$\n$$ \\lambda^{\\star} = \\frac{720}{568} $$\n我们简化这个分数。分子和分母都能被 $8$ 整除：$720/8 = 90$，$568/8 = 71$。\n$$ \\lambda^{\\star} = \\frac{90}{71} $$\n由于 $\\lambda^{\\star} = \\frac{90}{71} > 0$，它是一个有效的候选项。二阶导数检验或一阶导数符号分析证实了这是一个最小值。因此，对于 $\\lambda \\ge 0$，使 $V_{\\mathrm{GCV}}(\\lambda)$ 最小化的唯一值是 $\\frac{90}{71}$。", "answer": "$$\\boxed{\\frac{90}{71}}$$", "id": "2889347"}]}