{"hands_on_practices": [{"introduction": "要想真正掌握递归最小二乘 (RLS) 算法，我们必须首先精通其核心计算引擎：单步参数更新。本练习将引导您完成从指数加权最小二乘代价函数 $J_k(\\theta)$ 出发的基本推导，最终得到高效的递归形式。通过将此推导应用于一个具体的数值示例，您将巩固对驱动整个算法的底层机制的理解。[@problem_id:2899717]", "problem": "考虑一个参数线性模型，其中，在离散时间索引 $k$ 处的标量测量输出被建模为 $y_k \\approx \\varphi_k^{\\top} \\theta$，其中 $\\varphi_k \\in \\mathbb{R}^n$ 是时变回归向量，$\\theta \\in \\mathbb{R}^n$ 是未知参数向量。将时间 $k$ 的指数加权最小二乘代价定义为\n$$\nJ_k(\\theta) \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\big(y_i - \\varphi_i^{\\top}\\theta\\big)^2,\n$$\n其中遗忘因子满足 $0<\\lambda\\leq 1$。令加权正规方程矩阵和互相关向量为\n$$\nR_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i \\varphi_i^{\\top}, \\qquad r_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i y_i,\n$$\n因此最小化器满足 $R_k \\hat{\\theta}_k = r_k$。令 $P_k \\triangleq R_k^{-1}$，并假设 $R_k$ 是非奇异的。\n\n从代价定义 $J_k(\\theta)$ 和正规方程出发，推导参数估计 $\\hat{\\theta}_k$ 的单步递推最小二乘更新，该更新用 $P_{k-1}$、$\\hat{\\theta}_{k-1}$、$\\varphi_k$、$y_k$ 和 $\\lambda$ 表示。然后，对以下数据（所有量均为精确值）进行数值计算更新：\n- 维度 $n=2$，\n- 遗忘因子 $\\lambda = \\frac{1}{2}$，\n- 先前的逆正规方程矩阵\n$$\nP_{k-1} = \\begin{pmatrix} 2 & \\frac{1}{2} \\\\ \\frac{1}{2} & 1 \\end{pmatrix},\n$$\n- 先前的参数估计\n$$\n\\hat{\\theta}_{k-1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix},\n$$\n- 当前的回归量和输出\n$$\n\\varphi_k = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\qquad y_k = 5.\n$$\n\n以最简精确形式报告更新后的参数估计向量 $\\hat{\\theta}_k$。不要四舍五入。您的最终答案必须写成一个单行矩阵。", "solution": "所述问题是自适应滤波中的一个标准练习，具体涉及递推最小二乘（RLS）算法的推导和应用。该问题在科学上是合理的，提法恰当，并包含了所有必要的信息。我将着手解决它。\n\n问题要求分为两部分：首先，对参数估计 $\\hat{\\theta}_k$ 的单步更新进行形式化推导；其次，使用所提供的数据进行数值评估。\n\n首先，我们推导 $\\hat{\\theta}_k$ 的递推更新。推导的基础在于正规方程矩阵 $R_k$ 和互相关向量 $r_k$ 的递推性质。\n\n根据它们的定义：\n$$\nR_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i \\varphi_i^{\\top}\n$$\n$$\nr_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i y_i\n$$\n我们可以将索引为 $k$ 的项从求和中分离出来：\n$$\nR_k = \\lambda \\left( \\sum_{i=1}^{k-1} \\lambda^{k-1-i} \\varphi_i \\varphi_i^{\\top} \\right) + \\varphi_k \\varphi_k^{\\top}\n$$\n$$\nr_k = \\lambda \\left( \\sum_{i=1}^{k-1} \\lambda^{k-1-i} \\varphi_i y_i \\right) + \\varphi_k y_k\n$$\n这揭示了递推关系：\n$$\nR_k = \\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top}\n$$\n$$\nr_k = \\lambda r_{k-1} + \\varphi_k y_k\n$$\n最小二乘估计 $\\hat{\\theta}_k$ 是正规方程 $R_k \\hat{\\theta}_k = r_k$ 的解。代入递推表达式，我们得到：\n$$\n( \\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top} ) \\hat{\\theta}_k = \\lambda r_{k-1} + \\varphi_k y_k\n$$\n使用前一个估计的正规方程 $r_{k-1} = R_{k-1} \\hat{\\theta}_{k-1}$，我们代入 $r_{k-1}$：\n$$\n( \\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top} ) \\hat{\\theta}_k = \\lambda R_{k-1} \\hat{\\theta}_{k-1} + \\varphi_k y_k\n$$\n我们现在重新整理这个方程，以将 $\\hat{\\theta}_k$ 表示为对 $\\hat{\\theta}_{k-1}$ 的修正。我们可以在左边代入 $R_k$，并将右边的 $\\lambda R_{k-1}$ 改写为 $R_k - \\varphi_k \\varphi_k^{\\top}$：\n$$\nR_k \\hat{\\theta}_k = (R_k - \\varphi_k \\varphi_k^{\\top}) \\hat{\\theta}_{k-1} + \\varphi_k y_k\n$$\n整理各项可得：\n$$\nR_k \\hat{\\theta}_k - R_k \\hat{\\theta}_{k-1} = \\varphi_k y_k - \\varphi_k \\varphi_k^{\\top} \\hat{\\theta}_{k-1}\n$$\n$$\nR_k (\\hat{\\theta}_k - \\hat{\\theta}_{k-1}) = \\varphi_k (y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1})\n$$\n左乘 $P_k = R_k^{-1}$ 得到更新方程：\n$$\n\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + P_k \\varphi_k (y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1})\n$$\n这个方程依赖于 $P_k$。为了用 $P_{k-1}$ 表示更新，我们必须找到 $P_k$ 的递推公式。利用 $R_k$ 的递推公式和 Sherman-Morrison-Woodbury 矩阵求逆引理，我们有：\n$$\nP_k = R_k^{-1} = (\\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top})^{-1} = \\frac{1}{\\lambda} \\left( P_{k-1} - \\frac{P_{k-1} \\varphi_k \\varphi_k^{\\top} P_{k-1}}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} \\right)\n$$\n$\\hat{\\theta}_k$ 更新中的增益向量是 $K_k \\triangleq P_k \\varphi_k$。让我们推导它以 $P_{k-1}$ 表示的表达式：\n$$\nK_k = P_k \\varphi_k = \\frac{1}{\\lambda} \\left( P_{k-1} - \\frac{P_{k-1} \\varphi_k \\varphi_k^{\\top} P_{k-1}}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} \\right) \\varphi_k\n$$\n令标量项为 $s_k = \\varphi_k^{\\top} P_{k-1} \\varphi_k$。\n$$\nK_k = \\frac{1}{\\lambda} \\left( P_{k-1} \\varphi_k - \\frac{(P_{k-1} \\varphi_k) (\\varphi_k^{\\top} P_{k-1} \\varphi_k)}{\\lambda + s_k} \\right) = \\frac{1}{\\lambda} P_{k-1} \\varphi_k \\left( 1 - \\frac{s_k}{\\lambda + s_k} \\right)\n$$\n$$\nK_k = \\frac{1}{\\lambda} P_{k-1} \\varphi_k \\left( \\frac{\\lambda + s_k - s_k}{\\lambda + s_k} \\right) = \\frac{1}{\\lambda} P_{k-1} \\varphi_k \\left( \\frac{\\lambda}{\\lambda + s_k} \\right) = \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k}\n$$\n将增益向量的这个结果代入 $\\hat{\\theta}_k$ 的更新公式，得到基于 $k-1$ 时刻各量的完整 RLS 更新算法：\n$$\n\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} (y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1})\n$$\n推导至此完成。\n\n第二部分，我们进行数值计算。给定的数据是：\n$$\n\\lambda = \\frac{1}{2}, \\quad P_{k-1} = \\begin{pmatrix} 2 & \\frac{1}{2} \\\\ \\frac{1}{2} & 1 \\end{pmatrix}, \\quad \\hat{\\theta}_{k-1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad \\varphi_k = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad y_k = 5\n$$\n我们按步骤进行。\n\n1.  计算*先验*估计误差，$e_k = y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1}$：\n    $$\n    \\varphi_k^{\\top} \\hat{\\theta}_{k-1} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (1)(0) + (2)(1) = 2\n    $$\n    $$\n    e_k = 5 - 2 = 3\n    $$\n\n2.  计算增益向量的分子，$P_{k-1} \\varphi_k$：\n    $$\n    P_{k-1} \\varphi_k = \\begin{pmatrix} 2 & \\frac{1}{2} \\\\ \\frac{1}{2} & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2(1) + \\frac{1}{2}(2) \\\\ \\frac{1}{2}(1) + 1(2) \\end{pmatrix} = \\begin{pmatrix} 2 + 1 \\\\ \\frac{1}{2} + 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix}\n    $$\n\n3.  计算增益向量的分母，$\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k$：\n    $$\n    \\varphi_k^{\\top} P_{k-1} \\varphi_k = \\varphi_k^{\\top} (P_{k-1} \\varphi_k) = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix} = (1)(3) + (2)\\left(\\frac{5}{2}\\right) = 3 + 5 = 8\n    $$\n    $$\n    \\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k = \\frac{1}{2} + 8 = \\frac{17}{2}\n    $$\n\n4.  计算增益向量，$K_k$：\n    $$\n    K_k = \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} = \\frac{\\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix}}{\\frac{17}{2}} = \\frac{2}{17} \\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{17} \\\\ \\frac{5}{17} \\end{pmatrix}\n    $$\n\n5.  最后，计算更新后的参数估计，$\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + K_k e_k$：\n    $$\n    \\hat{\\theta}_k = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{6}{17} \\\\ \\frac{5}{17} \\end{pmatrix} (3) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{18}{17} \\\\ \\frac{15}{17} \\end{pmatrix}\n    $$\n    $$\n    \\hat{\\theta}_k = \\begin{pmatrix} \\frac{18}{17} \\\\ 1 + \\frac{15}{17} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{17} \\\\ \\frac{17}{17} + \\frac{15}{17} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{17} \\\\ \\frac{32}{17} \\end{pmatrix}\n    $$\n更新后的参数估计向量为 $\\hat{\\theta}_k$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{18}{17} \\\\ \\frac{32}{17} \\end{pmatrix}\n}\n$$", "id": "2899717"}, {"introduction": "在掌握了单步更新的基础上，我们现在将 RLS 滤波器应用于一个完整的、迭代的实际场景中。本练习让您扮演系统工程师的角色，负责辨识一个未知过程的参数，该过程被建模为带有外源输入的自回归（ARX）系统。通过编程实现 RLS 算法并用其处理合成数据，您将获得关于滤波器如何随时间收敛和跟踪参数的实践经验。[@problem_id:2899673]", "problem": "给定一个由已知输入驱动的自回归外源输入（Autoregressive with Exogenous input, ARX）模型。数据生成模型为\n$$\ny_k \\;=\\; -\\sum_{i=1}^{p} a_i\\,y_{k-i} \\;+\\; \\sum_{j=1}^{q} b_j\\,u_{k-j} \\;+\\; e_k,\n$$\n其中 $y_k$ 是输出，$u_k$ 是已知输入，$e_k$ 是白噪声，待估计的未知参数向量为\n$$\n\\theta \\;=\\; [\\,a_1,\\dots,a_p,\\,b_1,\\dots,b_q\\,]^{\\top}.\n$$\n假设对于所有负数索引，$y_k=0$ 且 $u_k=0$。白噪声序列是独立同分布的高斯噪声，均值为零，方差为 $\\sigma^2$，即 $e_k \\sim \\mathcal{N}(0,\\sigma^2)$，并且与输入过程无关。\n\n您的任务是实现一个带遗忘因子的递推最小二乘（Recursive Least Squares, RLS）估计器，以从该模型生成的合成数据中估计 $\\theta$。在每个时间 $k$ 构建回归向量为\n$$\n\\varphi_k \\;=\\; \\big[\\, -y_{k-1},\\,-y_{k-2},\\,\\dots,\\,-y_{k-p},\\; u_{k-1},\\,u_{k-2},\\,\\dots,\\,u_{k-q}\\,\\big]^{\\top},\n$$\n因此单步线性预测器为 $\\widehat{y}_k = \\varphi_k^{\\top}\\,\\widehat{\\theta}_{k-1}$。使用 $\\widehat{\\theta}_0 = 0 \\in \\mathbb{R}^{p+q}$ 和 $P_0 = \\delta I_{p+q}$ 初始化 RLS 递推。使用遗忘因子 $\\lambda \\in (0,1]$。\n\n在每个测试用例中，您必须按如下方式生成合成数据集：\n- 使用指定的随机种子，生成输入序列 $u_k$ 作为均值为零、单位方差的独立高斯样本。\n- 使用指定的随机种子，生成噪声序列 $e_k$ 作为均值为零、方差为 $\\sigma^2$ 的独立高斯样本（如果 $\\sigma=0$，则确定性地设置 $e_k=0$）。\n- 使用上述 ARX 递推公式和给定的初始条件，顺序地构成输出 $y_k$。\n- 运行 RLS 递推 $N$ 次迭代，处理样本 $k=0,1,\\dots,N-1$，并报告最终的估计值 $\\widehat{\\theta}_N$。\n\n重要要求：\n- 对每个测试用例，输出最终估计的参数向量 $\\widehat{\\theta}_N$，其顺序为 $[a_1,\\dots,a_p,b_1,\\dots,b_q]$。\n- 将每个估计参数向量的每个元素四舍五入到 $6$ 位小数。\n- 本问题不涉及物理单位。\n- 您的程序必须生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，列表中的每个元素是对应测试用例的四舍五入后的估计值列表。例如，一个有效的输出格式是\n$[ [\\dots], [\\dots], [\\dots], [\\dots] ]$，\n不打印额外文本。\n\n测试套件：\n- 测试用例 1（理想路径，中等噪声，强先验）：\n  - $p=1$, $q=1$,\n  - 真实 $a_1 = 0.7$，真实 $b_1 = 0.9$，\n  - 遗忘因子 $\\lambda = 0.99$，\n  - 初始协方差尺度 $\\delta = 1000$，\n  - 噪声标准差 $\\sigma = 0.05$，\n  - 样本数量 $N = 80$，\n  - 输入种子 = 12345，噪声种子 = 54321。\n- 测试用例 2（无遗忘，无噪声）：\n  - $p=2$, $q=1$,\n  - 真实 $[a_1,a_2] = [0.4,-0.15]$，真实 $[b_1] = [1.2]$，\n  - 遗忘因子 $\\lambda = 1.0$，\n  - 初始协方差尺度 $\\delta = 1000$，\n  - 噪声标准差 $\\sigma = 0.0$，\n  - 样本数量 $N = 120$，\n  - 输入种子 = 111，噪声种子 = 222。\n- 测试用例 3（纯 FIR，$p=0$ 边界情况，轻度噪声，强遗忘）：\n  - $p=0$, $q=2$,\n  - 真实 $[b_1,b_2] = [0.3,-0.1]$，\n  - 遗忘因子 $\\lambda = 0.98$，\n  - 初始协方差尺度 $\\delta = 1000$，\n  - 噪声标准差 $\\sigma = 0.1$，\n  - 样本数量 $N = 60$，\n  - 输入种子 = 7，噪声种子 = 8。\n- 测试用例 4（较高噪声，接近 1 的遗忘因子，较小先验尺度）：\n  - $p=1$, $q=2$,\n  - 真实 $[a_1] = [0.3]$，真实 $[b_1,b_2] = [0.5,0.2]$，\n  - 遗忘因子 $\\lambda = 0.995$，\n  - 初始协方差尺度 $\\delta = 10$，\n  - 噪声标准差 $\\sigma = 0.5$，\n  - 样本数量 $N = 150$，\n  - 输入种子 = 999，噪声种子 = 1001。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，列表中的每个元素是对应一个测试用例的四舍五入后的估计参数列表，顺序与上面列出的一致，例如\n$[[\\text{case1\\_estimates}],[\\text{case2\\_estimates}],[\\text{case3\\_estimates}],[\\text{case4\\_estimates}]]$。", "solution": "所呈现的问题是系统辨识中的一个标准练习，具体来说，是使用递推最小二乘 (RLS) 自适应滤波器来估计自回归外源输入 (ARX) 模型的参数。该问题定义明确、科学上合理且内部一致。它为数据生成和参数估计提供了所有必要的参数、初始条件和程序步骤。因此，该问题是有效的，并将提供一个解决方案。\n\n问题的核心在于应用 RLS 算法，从线性系统的输入输出数据中辨识其参数。该系统由 ARX 模型描述：\n$$\ny_k = -\\sum_{i=1}^{p} a_i y_{k-i} + \\sum_{j=1}^{q} b_j u_{k-j} + e_k\n$$\n其中 $k$ 是离散时间索引，$y_k$ 是系统输出，$u_k$ 是外源输入，$e_k$ 是零均值白噪声过程。目标是估计包含模型系数的未知参数向量 $\\theta$：\n$$\n\\theta = [a_1, \\dots, a_p, b_1, \\dots, b_q]^{\\top} \\in \\mathbb{R}^{p+q}\n$$\n为便于使用线性回归框架，我们定义一个包含过去输出和输入值的回归向量 $\\varphi_k$：\n$$\n\\varphi_k = [-y_{k-1}, -y_{k-2}, \\dots, -y_{k-p}, u_{k-1}, u_{k-2}, \\dots, u_{k-q}]^{\\top}\n$$\n通过这个定义，ARX 模型可以表示为紧凑的线性形式：\n$$\ny_k = \\varphi_k^{\\top} \\theta + e_k\n$$\n该公式表明，当前输出 $y_k$ 是回归向量 $\\varphi_k$ 和真实参数向量 $\\theta$ 的线性函数，并受到噪声 $e_k$ 的干扰。我们的任务是在每个时间步 $k$ 找到 $\\theta$ 的一个估计值 $\\widehat{\\theta}_k$。\n\n递推最小二乘算法提供了一种有效的方法来解决指数加权最小二乘问题，其目标是找到最小化以下代价函数的参数估计值 $\\widehat{\\theta}_k$：\n$$\nJ_k(\\theta) = \\sum_{i=1}^{k} \\lambda^{k-i} (y_i - \\varphi_i^{\\top} \\theta)^2 + \\lambda^k (\\theta - \\widehat{\\theta}_0)^{\\top} P_0^{-1} (\\theta - \\widehat{\\theta}_0)\n$$\n这里, $\\lambda \\in (0, 1]$ 是遗忘因子，它对较早的数据赋予指数级递减的权重，从而使算法能够跟踪时变参数或消除初始条件的影响。第二项包含了关于参数的先验知识，其中 $\\widehat{\\theta}_0$ 是初始估计值，$P_0$ 是初始逆协方差矩阵，通常为大标量 $\\delta$ 选择 $P_0 = \\delta I$。\n\nRLS 算法并不在每个时间步通过批处理来解决这个最小化问题，而是提供了一个递推更新规则。给定在时间 $k-1$ 的估计值 $\\widehat{\\theta}_{k-1}$ 和逆协方差矩阵 $P_{k-1}$，以及一个新的数据对 $(y_k, u_k)$，该算法会计算新的估计值 $\\widehat{\\theta}_k$ 和矩阵 $P_k$。\n\n标准的 RLS 算法流程如下：\n\n1.  **初始化**（在时间 $k=0$）：\n    -   参数估计：$\\widehat{\\theta}_0 = 0 \\in \\mathbb{R}^{p+q}$\n    -   逆协方差矩阵：$P_0 = \\delta I_{p+q}$，其中 $I$ 是单位矩阵，$\\delta$ 是一个大的正标量。\n\n2.  **递推**（对每个时间步 $k=1, 2, \\dots, N$，或者按照问题的索引，对于 $N$ 个样本，为 $k=0, 1, \\dots, N-1$）：\n    a. 使用过去的数据 $\\{y_{k-i}\\}_{i=1}^p$ 和 $\\{u_{k-j}\\}_{j=1}^q$ 构建回归向量 $\\varphi_k$。根据问题陈述，负时间索引的值为零。\n    b. 计算*先验*预测误差 $\\epsilon_k$，它是测量输出与使用先前估计值 $\\widehat{\\theta}_{k-1}$ 预测的输出之间的差值：\n    $$\n    \\epsilon_k = y_k - \\varphi_k^{\\top} \\widehat{\\theta}_{k-1}\n    $$\n    c. 计算卡尔曼增益向量 $K_k$：\n    $$\n    K_k = \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k}\n    $$\n    d. 更新参数向量估计值。新的估计值是旧估计值加上一个与增益和预测误差成比例的校正项：\n    $$\n    \\widehat{\\theta}_k = \\widehat{\\theta}_{k-1} + K_k \\epsilon_k\n    $$\n    e. 更新逆协方差矩阵 $P_k$。这一步由矩阵求逆引理导出，并简化了相关矩阵的逆的更新：\n    $$\n    P_k = \\frac{1}{\\lambda} (I - K_k \\varphi_k^{\\top}) P_{k-1}\n    $$\n\n实现过程将首先根据指定的 ARX 模型、真实参数和噪声特性，生成 $k=0, \\dots, N-1$ 的合成数据 $(u_k, y_k)$。输出 $y_k$ 必须顺序生成，因为它依赖于自身的过去值。然后，执行 RLS 递推 $N$ 次迭代，每个步骤处理一个样本，以获得最终的参数估计值 $\\widehat{\\theta}_N$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the RLS estimation problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"p\": 1, \"q\": 1, \"a\": [0.7], \"b\": [0.9],\n            \"lambda\": 0.99, \"delta\": 1000, \"sigma\": 0.05, \"N\": 80,\n            \"input_seed\": 12345, \"noise_seed\": 54321,\n        },\n        {\n            \"p\": 2, \"q\": 1, \"a\": [0.4, -0.15], \"b\": [1.2],\n            \"lambda\": 1.0, \"delta\": 1000, \"sigma\": 0.0, \"N\": 120,\n            \"input_seed\": 111, \"noise_seed\": 222,\n        },\n        {\n            \"p\": 0, \"q\": 2, \"a\": [], \"b\": [0.3, -0.1],\n            \"lambda\": 0.98, \"delta\": 1000, \"sigma\": 0.1, \"N\": 60,\n            \"input_seed\": 7, \"noise_seed\": 8,\n        },\n        {\n            \"p\": 1, \"q\": 2, \"a\": [0.3], \"b\": [0.5, 0.2],\n            \"lambda\": 0.995, \"delta\": 10, \"sigma\": 0.5, \"N\": 150,\n            \"input_seed\": 999, \"noise_seed\": 1001,\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        p, q = case[\"p\"], case[\"q\"]\n        true_a = np.array(case[\"a\"])\n        true_b = np.array(case[\"b\"])\n        lam, delta, sigma, N = case[\"lambda\"], case[\"delta\"], case[\"sigma\"], case[\"N\"]\n        input_seed, noise_seed = case[\"input_seed\"], case[\"noise_seed\"]\n\n        # 1. Generate synthetic data\n        rng_u = np.random.default_rng(input_seed)\n        rng_e = np.random.default_rng(noise_seed)\n\n        u = rng_u.normal(0, 1, size=N)\n        if sigma == 0.0:\n            e = np.zeros(N)\n        else:\n            e = rng_e.normal(0, sigma, size=N)\n\n        y = np.zeros(N)\n        for k in range(N):\n            ar_term = 0.0\n            if p > 0:\n                for i in range(1, p + 1):\n                    if k - i >= 0:\n                        ar_term += true_a[i - 1] * y[k - i]\n            \n            x_term = 0.0\n            if q > 0:\n                for j in range(1, q + 1):\n                    if k - j >= 0:\n                        x_term += true_b[j - 1] * u[k - j]\n            \n            y[k] = -ar_term + x_term + e[k]\n\n        # 2. Run RLS estimation\n        dim = p + q\n        if dim == 0:\n            results.append([])\n            continue\n\n        theta_hat = np.zeros(dim)\n        P = delta * np.eye(dim)\n\n        for k in range(N):\n            # Construct regressor vector phi_k\n            phi = np.zeros(dim)\n            if p > 0:\n                for i in range(1, p + 1):\n                    if k - i >= 0:\n                        phi[i - 1] = -y[k - i]\n            if q > 0:\n                for j in range(1, q + 1):\n                    if k - j >= 0:\n                        phi[p + j - 1] = u[k - j]\n\n            # RLS update equations\n            phi = phi.reshape(-1, 1) # Ensure phi is a column vector\n            \n            # Gain vector K_k\n            P_phi = P @ phi\n            denominator = lam + (phi.T @ P_phi)\n            K = P_phi / denominator\n\n            # A priori prediction error\n            prediction_error = y[k] - (phi.T @ theta_hat.reshape(-1, 1))\n\n            # Update parameter estimate\n            theta_hat = theta_hat + (K * prediction_error).flatten()\n\n            # Update inverse covariance matrix\n            P = (1 / lam) * (P - K @ phi.T @ P)\n        \n        # 3. Format result\n        final_estimate = np.round(theta_hat, 6).tolist()\n        results.append(final_estimate)\n        \n    # Final output formatting\n    # Using map(str, results) correctly stringifies each inner list.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2899673"}, {"introduction": "RLS 滤波器的性能关键取决于遗忘因子 $\\lambda$ 的选择，该因子控制着算法的“记忆”长度。本练习将从实现转向分析，探索该参数的深层含义。通过为一个参数 $\\theta_k$ 随时间漂移的系统推导稳态跟踪误差 $e_{\\mathrm{tr},\\infty}$，您将对跟踪灵活性与噪声抑制能力之间的基本权衡建立起定量的理解。[@problem_id:2899704]", "problem": "考虑一个标量参数跟踪问题，其中离散时间 $k \\in \\{1,2,\\dots\\}$ 的观测值由 $y_k = \\theta_k + v_k$ 给出，其中 $\\theta_k$ 是一个时变真实参数，$v_k$ 是均值为零、方差有限的观测噪声。时间 $k$ 的估计器是指数加权最小二乘解，其是指数加权残差平方和的最小化器\n$$\nJ_k(\\theta) = \\sum_{i=1}^{k} \\lambda^{\\,k-i} \\big(y_i - \\theta\\big)^{2},\n$$\n其中遗忘因子 $\\lambda$ 满足 $0<\\lambda<1$。假设真实参数以 $\\theta_k = \\theta_0 + r k$ 的形式线性漂移，其中漂移速率 $r \\in \\mathbb{R}$ 为常数。将跟踪误差定义为 $e_{\\mathrm{tr}}(k) \\triangleq \\theta_k - \\hat{\\theta}(k)$，其中 $\\hat{\\theta}(k)$ 是 $J_k(\\theta)$ 在时间 $k$ 的最小化器。请仅使用上述指数加权最小二乘准则的基本定义和等比级数的标准性质，推导稳态期望跟踪误差\n$$\ne_{\\mathrm{tr},\\infty} \\triangleq \\lim_{k \\to \\infty} \\mathbb{E}\\big[e_{\\mathrm{tr}}(k)\\big]\n$$\n并将其表示为关于漂移速率 $r$ 和遗忘因子 $\\lambda$ 的闭式解析表达式。请将最终答案表示为单个解析表达式。不需要进行数值近似或舍入。", "solution": "所述问题定义明确，具有科学依据，并包含了获得唯一解所需的所有信息。我们开始推导。\n\n估计器 $\\hat{\\theta}(k)$ 定义为使指数加权残差平方和最小化的 $\\theta$ 值：\n$$\nJ_k(\\theta) = \\sum_{i=1}^{k} \\lambda^{k-i} (y_i - \\theta)^{2}\n$$\n为求最小化器，我们计算 $J_k(\\theta)$ 关于 $\\theta$ 的导数，并令其为零。\n$$\n\\frac{dJ_k(\\theta)}{d\\theta} = \\sum_{i=1}^{k} \\lambda^{k-i} \\cdot 2(y_i - \\theta) \\cdot (-1) = -2 \\sum_{i=1}^{k} \\lambda^{k-i} (y_i - \\theta)\n$$\n当 $\\theta = \\hat{\\theta}(k)$ 时，令导数为零可得：\n$$\n\\sum_{i=1}^{k} \\lambda^{k-i} (y_i - \\hat{\\theta}(k)) = 0\n$$\n$$\n\\left( \\sum_{i=1}^{k} \\lambda^{k-i} \\right) \\hat{\\theta}(k) = \\sum_{i=1}^{k} \\lambda^{k-i} y_i\n$$\n由此，我们得到估计器 $\\hat{\\theta}(k)$ 的表达式，它是观测值 $y_i$ 的指数加权移动平均：\n$$\n\\hat{\\theta}(k) = \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} y_i}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\n问题要求解稳态期望跟踪误差。我们首先求估计器 $\\hat{\\theta}(k)$ 的期望值。观测模型为 $y_i = \\theta_i + v_i$，其中噪声项 $v_i$ 均值为零，即 $\\mathbb{E}[v_i] = 0$。真实参数 $\\theta_i$ 是一个确定性序列。\n利用期望算子的线性性质：\n$$\n\\mathbb{E}[\\hat{\\theta}(k)] = \\mathbb{E}\\left[ \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} y_i}{\\sum_{i=1}^{k} \\lambda^{k-i}} \\right] = \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} \\mathbb{E}[y_i]}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\n由于 $\\mathbb{E}[y_i] = \\mathbb{E}[\\theta_i + v_i] = \\mathbb{E}[\\theta_i] + \\mathbb{E}[v_i] = \\theta_i + 0 = \\theta_i$，我们有：\n$$\n\\mathbb{E}[\\hat{\\theta}(k)] = \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} \\theta_i}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\n时间 $k$ 的期望跟踪误差定义为 $\\mathbb{E}[e_{\\mathrm{tr}}(k)] = \\mathbb{E}[\\theta_k - \\hat{\\theta}(k)]$。由于 $\\theta_k$ 是确定性的，上式可写为：\n$$\n\\mathbb{E}[e_{\\mathrm{tr}}(k)] = \\theta_k - \\mathbb{E}[\\hat{\\theta}(k)] = \\theta_k - \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} \\theta_i}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\n现在，我们将给定的真实参数的线性漂移模型 $\\theta_i = \\theta_0 + r i$ 代入：\n$$\n\\mathbb{E}[e_{\\mathrm{tr}}(k)] = (\\theta_0 + rk) - \\frac{\\sum_{i=1}^{k} \\lambda^{k-i} (\\theta_0 + r i)}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\n我们可以将分数中的分子项分开：\n$$\n\\frac{\\sum_{i=1}^{k} \\lambda^{k-i} \\theta_0 + \\sum_{i=1}^{k} \\lambda^{k-i} r i}{\\sum_{i=1}^{k} \\lambda^{k-i}} = \\frac{\\theta_0 \\sum_{i=1}^{k} \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}} + \\frac{r \\sum_{i=1}^{k} \\lambda^{k-i} i}{\\sum_{i=1}^{k} \\lambda^{k-i}} = \\theta_0 + r \\frac{\\sum_{i=1}^{k} i \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\n将此结果代回期望跟踪误差的表达式中，得到：\n$$\n\\mathbb{E}[e_{\\mathrm{tr}}(k)] = (\\theta_0 + rk) - \\left( \\theta_0 + r \\frac{\\sum_{i=1}^{k} i \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}} \\right) = rk - r \\frac{\\sum_{i=1}^{k} i \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\n这个表达式可以合并成一个分数：\n$$\n\\mathbb{E}[e_{\\mathrm{tr}}(k)] = r \\left( \\frac{k \\sum_{i=1}^{k} \\lambda^{k-i} - \\sum_{i=1}^{k} i \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}} \\right) = r \\frac{\\sum_{i=1}^{k} (k-i) \\lambda^{k-i}}{\\sum_{i=1}^{k} \\lambda^{k-i}}\n$$\n这是时间 $k$ 的跟踪误差偏差，或称滞后误差。为了求得稳态误差 $e_{\\mathrm{tr},\\infty}$，我们必须计算当 $k \\to \\infty$ 时该表达式的极限。我们引入求和指数的变化 $j = k-i$。当 $i$ 从 $1$ 取到 $k$ 时，$j$ 从 $k-1$ 递减到 $0$。表达式变为：\n$$\n\\mathbb{E}[e_{\\mathrm{tr}}(k)] = r \\frac{\\sum_{j=0}^{k-1} j \\lambda^{j}}{\\sum_{j=0}^{k-1} \\lambda^{j}}\n$$\n现在，我们取 $k \\to \\infty$ 的极限。在 $0 < \\lambda < 1$ 的条件下，这些和收敛到它们各自的无穷级数：\n$$\ne_{\\mathrm{tr},\\infty} = \\lim_{k \\to \\infty} \\mathbb{E}[e_{\\mathrm{tr}}(k)] = r \\frac{\\sum_{j=0}^{\\infty} j \\lambda^{j}}{\\sum_{j=0}^{\\infty} \\lambda^{j}}\n$$\n分母是一个标准的等比级数：\n$$\n\\sum_{j=0}^{\\infty} \\lambda^{j} = \\frac{1}{1-\\lambda}\n$$\n分子是一个等差-等比级数。其和可以通过对等比级数公式求导得到。对于 $|x|<1$，我们有 $\\sum_{j=0}^{\\infty} x^j = (1-x)^{-1}$。对 $x$ 求导：\n$$\n\\frac{d}{dx} \\sum_{j=0}^{\\infty} x^j = \\sum_{j=1}^{\\infty} j x^{j-1} = \\frac{d}{dx}(1-x)^{-1} = (1-x)^{-2}\n$$\n注意，求和从 $j=1$ 开始，因为 $j=0$ 项是常数。两边乘以 $x$ 得到：\n$$\n\\sum_{j=1}^{\\infty} j x^{j} = \\frac{x}{(1-x)^2}\n$$\n由于 $j=0$ 项是 $0 \\cdot x^0 = 0$，所以这也是从 $j=0$ 到 $\\infty$ 的和。代入 $x=\\lambda$：\n$$\n\\sum_{j=0}^{\\infty} j \\lambda^{j} = \\frac{\\lambda}{(1-\\lambda)^2}\n$$\n最后，我们将这两个无穷级数的值代回 $e_{\\mathrm{tr},\\infty}$ 的表达式中：\n$$\ne_{\\mathrm{tr},\\infty} = r \\frac{\\frac{\\lambda}{(1-\\lambda)^2}}{\\frac{1}{1-\\lambda}} = r \\left( \\frac{\\lambda}{(1-\\lambda)^2} \\cdot (1-\\lambda) \\right)\n$$\n这可以简化为稳态期望跟踪误差的最终表达式。", "answer": "$$ \\boxed{r \\frac{\\lambda}{1-\\lambda}} $$", "id": "2899704"}]}