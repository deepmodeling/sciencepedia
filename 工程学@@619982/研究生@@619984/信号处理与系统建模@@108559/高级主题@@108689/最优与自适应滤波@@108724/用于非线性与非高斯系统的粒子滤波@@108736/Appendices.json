{"hands_on_practices": [{"introduction": "本练习将引导你完成“自举”粒子滤波器 (Bootstrap Particle Filter) 的一次完整迭代，这是最基础也是最核心的粒子滤波算法。你将亲手实现粒子传播、重要性权重更新、有效样本量计算以及系统重采样等关键步骤。通过这个实践，你能够将前面学到的理论知识转化为实际代码，为你后续掌握更复杂的滤波技术打下坚实的基础。[@problem_id:2890374]", "problem": "请考虑以下离散时间状态空间模型，该模型旨在对非线性和非高斯系统的序贯蒙特卡洛 (SMC) 方法进行压力测试。潜状态 $x_t \\in \\mathbb{R}$ 根据非线性动态过程演化\n$$\nx_t = g(x_{t-1}, t) + v_t,\n$$\n其中\n$$\ng(x, t) = \\frac{1}{2} x + \\frac{25 x}{1 + x^2} + 8 \\cos(1.2 t),\n$$\n并且过程噪声 $v_t$ 是独立同分布的，服从位置为 $0$、尺度为 $b_v$ 的拉普拉斯分布，记作 $v_t \\sim \\mathrm{Laplace}(0, b_v)$，其概率密度函数为\n$$\np_{V}(v) = \\frac{1}{2 b_v} \\exp\\!\\left(-\\frac{|v|}{b_v}\\right).\n$$\n标量观测值 $y_t \\in \\mathbb{R}$ 由非线性测量模型给出\n$$\ny_t = \\arctan(x_t) + e_t,\n$$\n其中测量噪声 $e_t$ 是独立同分布的，服从位置为 $0$、尺度为 $b_e$ 的拉普拉斯分布，记作 $e_t \\sim \\mathrm{Laplace}(0, b_e)$，其概率密度函数为\n$$\np_{E}(e) = \\frac{1}{2 b_e} \\exp\\!\\left(-\\frac{|e|}{b_e}\\right).\n$$\n所有角度均以弧度为单位。函数 $\\arctan(\\cdot)$ 表示反正切主值。\n\n您需要实现 bootstrap 粒子滤波器（也称为采样重要性重采样滤波器）从时间 $t-1$ 到时间 $t$ 的单次迭代，如下所示，从 $t-1$ 时刻滤波分布的先验粒子近似 $\\{(x_{t-1}^{(i)}, w_{t-1}^{(i)})\\}_{i=1}^N$ 开始，该近似包含 $N$ 个粒子，其中 $w_{t-1}^{(i)} \\ge 0$ 且 $\\sum_{i=1}^N w_{t-1}^{(i)} = 1$：\n\n1. 传播（提议分布等于转移先验）：对于每个粒子 $i \\in \\{1,\\dots,N\\}$，进行采样\n$$\nx_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}) = \\mathrm{Laplace}\\!\\left(g\\!\\left(x_{t-1}^{(i)}, t\\right),\\, b_v\\right).\n$$\n\n2. 使用测量似然和先验权重更新权重：\n$$\n\\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\, p\\!\\left(y_t \\,\\middle|\\, x_t^{(i)}\\right), \\quad\np\\!\\left(y_t \\,\\middle|\\, x_t\\right) = \\frac{1}{2 b_e} \\exp\\!\\left(-\\frac{|y_t - \\arctan(x_t)|}{b_e}\\right).\n$$\n然后归一化得到 $w_t^{(i)} = \\tilde{w}_t^{(i)} \\Big/ \\sum_{j=1}^N \\tilde{w}_t^{(j)}$。\n\n3. 计算有效样本量 (ESS, Effective Sample Size)，定义为\n$$\n\\mathrm{ESS}_t = \\frac{1}{\\sum_{i=1}^N \\left(w_t^{(i)}\\right)^2}.\n$$\n\n4. 重采样决策与执行：给定一个阈值比率 $\\tau \\in [0, 1]$，如果 $\\mathrm{ESS}_t \\le \\tau N$，则执行系统性重采样，生成一个重采样后的粒子集 $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$，其权重均等，为 $\\bar{w}_t^{(i)} = 1/N$。如果未触发重采样，则保留 $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$。\n\n对于这单次迭代，在重采样决策之后，后验估计定义如下：\n- 如果发生了重采样，则使用重采样后等权重的粒子集 $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$ 计算后验均值和方差，作为标准的无权样本均值和方差。\n- 如果没有发生重采样，则使用加权粒子集 $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ 计算后验均值和方差，作为加权均值和加权方差。\n\n您实现的程序必须：\n- 通过独立采样 $x_{t-1}^{(i)} \\sim \\mathcal{N}(m_0, s_0^2)$（对于 $i \\in \\{1,\\dots,N\\}$）来初始化先验粒子，并使用均匀先验权重 $w_{t-1}^{(i)} = 1/N$。\n- 对每个测试用例使用固定的伪随机种子，以确保确定性行为。\n- 使用数值稳定的方式计算权重（例如，通过对数权重稳定化）。\n- 实现系统性重采样。\n\n您的程序必须处理以下测试套件，其中每个元组指定 $(N, \\mathrm{seed}, t, y_t, m_0, s_0, b_v, b_e, \\tau)$:\n- 测试 1: $(200, 42, 1, 0.3, 0.0, 1.0, 0.5, 0.2, 0.5)$。\n- 测试 2: $(200, 314, 5, 1.2, 0.0, 2.0, 0.5, 1.0, 0.9)$。\n- 测试 3: $(150, 123, 3, -0.5, 0.0, 1.0, 0.2, 0.05, 0.9)$。\n- 测试 4: $(100, 777, 2, 2.0, 0.0, 1.0, 0.3, 0.001, 0.0)$。\n\n对于每个测试，在完成一次完整的 bootstrap 滤波迭代（包括重采样决策）后，报告：\n- $x_t$ 的后验均值，四舍五入到 $6$ 位小数，\n- $x_t$ 的后验方差，四舍五入到 $6$ 位小数，\n- 有效样本量 $\\mathrm{ESS}_t$，四舍五入到 $6$ 位小数（在任何重采样之前计算），\n- 重采样指示符，其中整数 $1$ 表示进行了重采样，整数 $0$ 表示未进行重采样。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个类 JSON 格式的结果列表，每个测试用例对应一个结果，每个结果本身是按上述确切顺序包含四个值的列表。例如，输出应如下所示\n$$\n\\big[\\,[m_1, v_1, \\mathrm{ESS}_1, r_1],\\,[m_2, v_2, \\mathrm{ESS}_2, r_2],\\,\\dots\\,\\big],\n$$\n其中每个 $m_k$、$v_k$ 和 $\\mathrm{ESS}_k$ 都是四舍五入到 $6$ 位小数的十进制数，每个 $r_k$ 为 $0$ 或 $1$。除了指明角度以弧度为单位外，不需要其他物理单位。程序不得读取任何输入，并且必须是自包含的。", "solution": "该问题在非线性状态估计领域构成了一个定义明确的练习。它为实现自举粒子滤波器的单次迭代提供了一套完整且一致的规范，包括非线性和非高斯动态模型、所有必要的参数以及明确的算法步骤。我们着手进行求解。\n\n任务是针对一个给定的非线性、非高斯状态空间模型，实现序贯蒙特卡洛方法的一个迭代，具体来说是 bootstrap 粒子滤波器。该过程从时刻 $t-1$ 开始，到时刻 $t$ 的重采样决策之后结束。\n\n**1. 系统模型规范**\n状态空间模型由两个方程定义：\n状态转移方程：\n$$x_t = g(x_{t-1}, t) + v_t$$\n其中非线性函数为 $g(x, t) = \\frac{1}{2} x + \\frac{25 x}{1 + x^2} + 8 \\cos(1.2 t)$，过程噪声为 $v_t \\sim \\mathrm{Laplace}(0, b_v)$。\n\n测量方程：\n$$y_t = \\arctan(x_t) + e_t$$\n其中测量噪声为 $e_t \\sim \\mathrm{Laplace}(0, b_e)$。\n\n**2. Bootstrap 粒子滤波器迭代**\n该算法分几步进行，从一个近似后验分布 $p(x_{t-1} | y_{1:t-1})$ 的粒子集 $\\{x_{t-1}^{(i)}, w_{t-1}^{(i)}\\}_{i=1}^N$ 开始。\n\n**第 0 步：时刻 $t-1$ 的初始化**\n根据规定，为这次单次迭代初始化先验粒子集。状态 $x_{t-1}^{(i)}$ 从高斯分布中独立抽取，$x_{t-1}^{(i)} \\sim \\mathcal{N}(m_0, s_0^2)$ for $i=1, \\dots, N$。初始权重是均匀的，$w_{t-1}^{(i)} = 1/N$ for all $i$。\n\n**第 1 步：传播（预测）**\n每个粒子根据状态转移模型向前传播。对于 bootstrap 滤波器，提议分布就是转移先验本身。这意味着，对于每个粒子 $i$，我们从分布 $p(x_t | x_{t-1}^{(i)})$ 中采样一个新的状态 $x_t^{(i)}$。这通过以下方式完成：\n1.  计算状态演化的确定性部分：$\\mu_t^{(i)} = g(x_{t-1}^{(i)}, t)$。\n2.  从过程噪声分布中采样一个噪声项 $v_t^{(i)}$：$v_t^{(i)} \\sim \\mathrm{Laplace}(0, b_v)$。\n3.  将它们组合起来得到新的粒子状态：$x_t^{(i)} = \\mu_t^{(i)} + v_t^{(i)}$。\n\n**第 2 步：权重更新（校正）**\n在接收到测量值 $y_t$ 后，更新传播后粒子的重要性权重，以反映每个粒子对观测值的解释程度。更新后的未归一化权重 $\\tilde{w}_t^{(i)}$ 是先验权重与给定粒子状态下该测量值的似然的乘积：\n$$\\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\, p(y_t | x_t^{(i)})$$\n似然函数 $p(y_t | x_t)$ 由测量模型和测量噪声 $e_t$ 的分布导出：\n$$p(y_t | x_t) = p_{E}(y_t - \\arctan(x_t)) = \\frac{1}{2 b_e} \\exp\\left(-\\frac{|y_t - \\arctan(x_t)|}{b_e}\\right)$$\n由于先验权重 $w_{t-1}^{(i)}$ 是均匀的 ($1/N$)，它们对所有粒子都是常数，在归一化过程中可以忽略。因此，未归一化的权重与似然成正比：$\\tilde{w}_t^{(i)} \\propto p(y_t | x_t^{(i)})$。\n\n为保证数值稳定性，计算在对数域中进行。粒子 $i$ 的对数似然为：\n$$\\log p(y_t | x_t^{(i)}) = -\\log(2 b_e) - \\frac{|y_t - \\arctan(x_t^{(i)})|}{b_e}$$\n令 $l_i = \\log p(y_t | x_t^{(i)})$。我们找到最大对数似然 $l_{\\max} = \\max_i \\{l_i\\}$。然后使用 log-sum-exp 技巧计算归一化权重 $w_t^{(i)}$，以防止数值下溢：\n$$w_t^{(i)} = \\frac{\\exp(l_i - l_{\\max})}{\\sum_{j=1}^N \\exp(l_j - l_{\\max})}$$\n集合 $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ 现在代表了滤波分布 $p(x_t | y_{1:t})$ 的一个近似。\n\n**第 3 步：有效样本量 (ESS) 计算**\n粒子集的退化程度由有效样本量 $\\mathrm{ESS}_t$ 来量化。低的 ESS 表示少数几个粒子具有非常高的权重，近似效果很差。其计算公式为：\n$$\\mathrm{ESS}_t = \\frac{1}{\\sum_{i=1}^N (w_t^{(i)})^2}$$\n$\\mathrm{ESS}_t$ 的值范围从 $1$（完全退化）到 $N$（均匀权重）。\n\n**第 4 步：重采样决策与执行**\n重采样是一种通过复制高权重粒子并丢弃低权重粒子来减轻粒子退化现象的机制。通过将 ESS 与一个阈值 $\\tau N$ 进行比较来决定是否重采样，其中 $\\tau \\in [0, 1]$ 是用户定义的比率。\n- 如果 $\\mathrm{ESS}_t \\le \\tau N$，则执行重采样。问题指定了系统性重采样。该算法从当前的离散分布 $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ 中抽取 $N$ 个粒子，形成一个新的集合 $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$，其中每个新粒子的权重都相等，为 $\\bar{w}_t^{(i)}=1/N$。\n- 如果 $\\mathrm{ESS}_t > \\tau N$，则不采取任何行动。粒子集保持为 $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$。\n\n如果发生了重采样，则重采样指示符 $r_t$ 设为 $1$，否则设为 $0$。\n\n**第 5 步：后验统计量计算**\n最后一步是从得到的粒子集中计算 $x_t$ 后验分布的均值和方差。计算公式取决于是否执行了重采样。\n- **如果发生了重采样 ($r_t=1$)**：后验由等权重的粒子 $\\{\\bar{x}_t^{(i)}\\}_{i=1}^N$ 近似。均值和方差是标准的无权样本均值和方差：\n  $$\\hat{x}_t = \\frac{1}{N} \\sum_{i=1}^N \\bar{x}_t^{(i)}$$\n  $$\\hat{V}_t = \\frac{1}{N} \\sum_{i=1}^N (\\bar{x}_t^{(i)} - \\hat{x}_t)^2$$\n- **如果没有发生重采样 ($r_t=0$)**：后验由加权的粒子 $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ 近似。均值和方差是加权样本均值和方差：\n  $$\\hat{x}_t = \\sum_{i=1}^N w_t^{(i)} x_t^{(i)}$$\n  $$\\hat{V}_t = \\sum_{i=1}^N w_t^{(i)} (x_t^{(i)} - \\hat{x}_t)^2$$\n\n这些计算出的值——后验均值、后验方差、ESS 和重采样指示符——构成了滤波器一次迭代所需的输出。", "answer": "```python\nimport numpy as np\n\ndef g(x, t):\n    \"\"\"The nonlinear state transition function.\"\"\"\n    return 0.5 * x + 25.0 * x / (1.0 + x**2) + 8.0 * np.cos(1.2 * t)\n\ndef particle_filter_step(N, seed, t, yt, m0, s0, bv, be, tau):\n    \"\"\"\n    Performs a single iteration of a bootstrap particle filter.\n    Returns posterior mean, variance, ESS, and resampling indicator.\n    \"\"\"\n    # Initialize the random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # Step 0: Initialize prior particles at time t-1.\n    # The weights w_{t-1} are uniform (1/N) for all particles.\n    xt_prev = rng.normal(loc=m0, scale=s0, size=N)\n\n    # Step 1: Propagation (Prediction) to time t.\n    # The proposal is the transition prior p(x_t|x_{t-1}).\n    mu_t = g(xt_prev, t)\n    # Sample from Laplace(mu_t, bv) which is mu_t + Laplace(0, bv)\n    xt = mu_t + rng.laplace(loc=0.0, scale=bv, size=N)\n\n    # Step 2: Weight update using the measurement y_t.\n    # We work with log-weights for numerical stability.\n    # The log-likelihood is log p(y_t|x_t^(i)).\n    log_likelihood = -np.log(2.0 * be) - np.abs(yt - np.arctan(xt)) / be\n\n    # Since w_{t-1} are uniform, log(w_{t-1}) is a constant offset\n    # that cancels during normalization. So, log_tilde_wt is proportional\n    # to the log_likelihood.\n    # We use the log-sum-exp trick for normalization.\n    log_wt_max = np.max(log_likelihood)\n    wt_unnorm = np.exp(log_likelihood - log_wt_max)\n    wt = wt_unnorm / np.sum(wt_unnorm)\n\n    # Step 3: Compute Effective Sample Size (ESS).\n    # This is calculated before any resampling.\n    ess = 1.0 / np.sum(wt**2)\n\n    # Step 4: Resampling decision and execution.\n    resampling_occurred = 0\n    # The threshold condition ESS <= tau * N might be sensitive to floating-point\n    # issues, but for typical values, direct comparison is acceptable.\n    # Note: ESS is always >= 1. If tau*N < 1, resampling will not trigger.\n    if ess <= tau * N:\n        resampling_occurred = 1\n        \n        # Perform systematic resampling.\n        csw = np.cumsum(wt)\n        csw[-1] = 1.0  # Ensure the sum is exactly 1\n        u0 = rng.random()\n        positions = (np.arange(N) + u0) / N\n        \n        indices = np.searchsorted(csw, positions)\n        \n        # The new particles are copies of the old ones based on indices.\n        xt = xt[indices]\n        # After resampling, all weights are reset to uniform 1/N.\n        wt = np.full(N, 1.0 / N)\n\n    # Step 5: Compute posterior mean and variance.\n    if resampling_occurred == 1:\n        # Use unweighted sample statistics for the resampled set.\n        post_mean = np.mean(xt)\n        post_var = np.var(xt) # np.var uses 1/N denominator\n    else:\n        # Use weighted sample statistics.\n        post_mean = np.sum(wt * xt)\n        post_var = np.sum(wt * (xt - post_mean)**2)\n\n    return [\n        round(post_mean, 6),\n        round(post_var, 6),\n        round(ess, 6),\n        resampling_occurred\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # (N, seed, t, y_t, m_0, s_0, b_v, b_e, tau)\n        (200, 42, 1, 0.3, 0.0, 1.0, 0.5, 0.2, 0.5),\n        (200, 314, 5, 1.2, 0.0, 2.0, 0.5, 1.0, 0.9),\n        (150, 123, 3, -0.5, 0.0, 1.0, 0.2, 0.05, 0.9),\n        (100, 777, 2, 2.0, 0.0, 1.0, 0.3, 0.001, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = particle_filter_step(*case)\n        results.append(result)\n\n    # Format the final output string as a JSON-like list of lists.\n    # f'{val:.6f}' is used to ensure 6 decimal places and avoid scientific notation.\n    inner_strings = []\n    for res_list in results:\n        m, v, e, r = res_list\n        inner_strings.append(f\"[{m:.6f},{v:.6f},{e:.6f},{r}]\")\n    \n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2890374"}, {"introduction": "在掌握了基本的自举滤波器之后，我们来深入探讨其性能的关键影响因素——提议分布 (proposal distribution)。本练习旨在通过经验性地比较“自举”提议分布（即状态转移先验）与理论上的“最优”提议分布所产生的权重方差，来揭示提议分布的重要性。通过这个对比，你将直观地理解为何一个优秀的提议分布能够显著减小权重退化，从而提升滤波器的效率和准确性。[@problem_id:2890437]", "problem": "考虑一个用于非线性系统的序贯蒙特卡洛（SMC）的一维离散时间状态空间模型。设时间 $t$ 的潜在状态为 $x_t \\in \\mathbb{R}$。该系统遵循以下动态和观测模型：\n- 状态转移：$x_t \\mid x_{t-1} \\sim \\mathcal{N}(f(x_{t-1}), Q)$，其中 $f(x) = 0.5\\,x + 0.05\\,x^3$。\n- 观测：$y_t \\mid x_t \\sim \\mathcal{N}(c\\,x_t, R)$，其中 $c$ 为已知标量。\n\n您需要在一个固定的时间 $t$ 对单个滤波步骤比较两种重要性提议分布：\n- 自助提议分布 $q(x_t \\mid x_{t-1}, y_t) = p(x_t \\mid x_{t-1})$。\n- 最优提议分布 $q^\\star(x_t \\mid x_{t-1}, y_t) = p(x_t \\mid x_{t-1}, y_t)$。\n\n您的任务是，对于一组测试用例，在相同的上一时刻粒子集 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ 和相同的固定观测 $y_t$ 条件下，凭经验估计并比较这两种提议分布所产生的未归一化增量对数权重的方差。\n\n您必须使用的基本原理：\n- 贝叶斯法则：$p(x_t \\mid x_{t-1}, y_t) \\propto p(y_t \\mid x_t)\\,p(x_t \\mid x_{t-1})$。\n- 重要性采样恒等式：对于任何提议分布 $q(x_t \\mid x_{t-1}, y_t)$，增量权重满足 $w_t^{(i)} \\propto \\frac{p(y_t \\mid x_t^{(i)})\\,p(x_t^{(i)} \\mid x_{t-1}^{(i)})}{q(x_t^{(i)} \\mid x_{t-1}^{(i)}, y_t)}$，其中未归一化增量对数权重 $\\ell_t^{(i)} = \\log w_t^{(i)}$ 的定义不考虑与粒子索引 $i$ 无关的加性常数。\n- 用于线性高斯模型的高斯代数。\n\n每个测试用例需要计算的内容：\n1. 根据指定的规则（确定性或高斯分布），使用给定的随机种子生成上一时刻的粒子集 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$，以确保可复现性。\n2. 使用自助提议分布：\n   - 对每个粒子 $i$，采样 $x_t^{(i)} \\sim \\mathcal{N}(f(x_{t-1}^{(i)}), Q)$。\n   - 使用高斯观测似然计算未归一化的增量对数权重，忽略所有粒子共有的任何加性常数：\n     $$\\ell_{\\text{boot}}^{(i)} = -\\tfrac{1}{2}\\,\\frac{(y_t - c\\,x_t^{(i)})^2}{R}。$$\n   - 计算 $\\{\\ell_{\\text{boot}}^{(i)}\\}_{i=1}^N$ 的无偏样本方差，分母为 $N-1$。\n3. 使用最优提议分布 $q^\\star(x_t \\mid x_{t-1}, y_t) = p(x_t \\mid x_{t-1}, y_t)$：\n   - 推导 $p(x_t \\mid x_{t-1}, y_t)$ 的闭式解，并为每个 $i$ 相应地采样 $x_t^{(i)}$。\n   - 使用通过将 $x_t$ 从 $p(y_t \\mid x_t)\\,p(x_t \\mid x_{t-1}^{(i)})$ 中边缘化得到的预测密度 $p(y_t \\mid x_{t-1}^{(i)})$，为每个粒子计算未归一化的增量对数权重，忽略所有粒子共有的加性常数：\n     $$\\ell_{\\star}^{(i)} = -\\tfrac{1}{2}\\,\\frac{(y_t - c\\,f(x_{t-1}^{(i)}))^2}{c^2 Q + R}。$$\n   - 计算 $\\{\\ell_{\\star}^{(i)}\\}_{i=1}^N$ 的无偏样本方差，分母为 $N-1$。\n4. 为每个测试用例报告一个浮点数三元组 $[\\mathrm{var}_{\\text{boot}}, \\mathrm{var}_{\\star}, \\mathrm{var}_{\\text{boot}} - \\mathrm{var}_{\\star}]$，其中每个值都四舍五入到 $6$ 位小数。\n\n测试套件：\n- 所有用例均使用非线性转移 $f(x) = 0.5\\,x + 0.05\\,x^3$。不涉及角度。不涉及物理单位。\n- 对于所有随机抽样，每个测试用例使用提供的种子以确保确定性输出。\n\n用例：\n- 用例 $1$（退化的上一时刻状态；展示了在 $q^\\star$ 下的零方差）：\n  - $N = 20000$，$Q = 0.25$，$R = 0.09$，$c = 1.0$，$y_t = 0.2$，上一时刻粒子：所有 $x_{t-1}^{(i)} = 0.5$，种子 $= 42$。\n- 用例 $2$（中等噪声，上一时刻粒子存在散布）：\n  - $N = 20000$，$Q = 1.0$，$R = 0.25$，$c = 1.2$，$y_t = -0.3$，上一时刻粒子：$x_{t-1}^{(i)} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0.0, 1.0)$，种子 $= 7$。\n- 用例 $3$（信息非常丰富的观测，大的过程噪声；对自助提议分布构成挑战）：\n  - $N = 20000$，$Q = 2.0$，$R = 0.0025$，$c = 1.0$，$y_t = 1.0$，上一时刻粒子：$x_{t-1}^{(i)} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0.0, 2.0)$，种子 $= 13$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的、逗号分隔的三元组列表（例如，$[[v_{b1}, v_{\\star 1}, \\Delta_1],[v_{b2}, v_{\\star 2}, \\Delta_2],[v_{b3}, v_{\\star 3}, \\Delta_3]]$），其中每个数值都四舍五入到 $6$ 位小数。\n- 每个三元组按顺序对应于用例 1 到 3 的 $[\\mathrm{var}_{\\text{boot}}, \\mathrm{var}_{\\star}, \\mathrm{var}_{\\text{boot}} - \\mathrm{var}_{\\star}]$。", "solution": "所提出的问题是统计信号处理领域中一个明确定义的练习，具体涉及序贯蒙特卡洛方法（即粒子滤波器）。它要求比较标准自助提议分布和最优提议分布产生的未归一化增量重要性权重的方差。该模型是一个具有非线性状态转移和线性高斯观测模型的离散时间一维状态空间系统，这种结构使得推导最优提议分布成为可能。该问题在科学上是合理的，数学上是一致的，并且所有参数都已指定，因此适合进行分析。\n\n我们首先建立数学框架。该系统定义如下：\n- 状态转移模型：$p(x_t|x_{t-1}) = \\mathcal{N}(x_t; f(x_{t-1}), Q)$，其中转移函数为 $f(x) = 0.5x + 0.05x^3$。\n- 观测模型：$p(y_t|x_t) = \\mathcal{N}(y_t; c x_t, R)$。\n\n目标是计算在单个时间步 $t$ 时，一组粒子 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ 的未归一化增量对数权重 $\\ell_t^{(i)}$ 的样本方差。对于提议分布 $q(x_t|x_{t-1}^{(i)}, y_t)$，增量权重 $w_t^{(i)}$ 的一般形式由重要性采样恒等式给出：\n$$w_t^{(i)} \\propto \\frac{p(y_t|x_t^{(i)}) p(x_t^{(i)}|x_{t-1}^{(i)})}{q(x_t^{(i)}|x_{t-1}^{(i)}, y_t)}$$\n其中 $x_t^{(i)} \\sim q(x_t|x_{t-1}^{(i)}, y_t)$。未归一化的对数权重为 $\\ell_t^{(i)} = \\log w_t^{(i)}$。\n\n**1. 自助提议分布**\n\n自助提议分布是最简单的选择，它使用状态转移先验作为提议分布：\n$$q_{\\text{boot}}(x_t | x_{t-1}, y_t) = p(x_t|x_{t-1}) = \\mathcal{N}(x_t; f(x_{t-1}), Q)$$\n将此代入权重方程，先验项 $p(x_t^{(i)}|x_{t-1}^{(i)})$ 与提议密度 $q_{\\text{boot}}$ 相抵消，使得权重与似然成正比：\n$$w_{\\text{boot}}^{(i)} \\propto p(y_t|x_t^{(i)})$$\n粒子首先从先验 $x_t^{(i)} \\sim \\mathcal{N}(x_t; f(x_{t-1}^{(i)}), Q)$ 中采样，然后根据它们对观测 $y_t$ 的解释程度进行加权。未归一化增量对数权重是观测的对数似然，忽略与粒子索引 $i$ 无关的加性常数：\n$$\\ell_{\\text{boot}}^{(i)} = -\\frac{1}{2R}(y_t - c x_t^{(i)})^2$$\n这些对数权重的方差 $\\mathrm{var}(\\{\\ell_{\\text{boot}}^{(i)}\\}_{i=1}^N)$ 源于 $x_t^{(i)}$ 的随机采样以及父粒子 $\\{x_{t-1}^{(i)}\\}$ 中任何已存在的变异。如果似然 $p(y_t|x_t)$ 集中在先验 $p(x_t|x_{t-1})$ 密度很低的区域，这个方差可能会很大，这是自助滤波器常见的低效问题。\n\n**2. 最优提议分布**\n\n能够最小化重要性权重方差的最优提议分布是状态的真实后验分布：\n$$q^{\\star}(x_t | x_{t-1}, y_t) = p(x_t|x_{t-1}, y_t)$$\n此选择的权重由后验分布的归一化常数导出：\n$$p(x_t|x_{t-1}, y_t) = \\frac{p(y_t|x_t) p(x_t|x_{t-1})}{\\int p(y_t|x_t) p(x_t|x_{t-1}) dx_t} = \\frac{p(y_t|x_t) p(x_t|x_{t-1})}{p(y_t|x_{t-1})}$$\n将 $q^{\\star}$ 代入权重方程可得：\n$$w_{\\star}^{(i)} \\propto \\frac{p(y_t|x_t^{(i)}) p(x_t^{(i)}|x_{t-1}^{(i)})}{p(x_t^{(i)}|x_{t-1}^{(i)}, y_t)} \\propto p(y_t|x_{t-1}^{(i)})$$\n权重是在给定上一时刻状态下的观测的边缘似然（或证据）。对于我们的特定模型，给定一个 $x_{t-1}$，量 $x_t$ 和 $y_t$ 是联合高斯分布的。给定 $x_{t-1}$ 时 $y_t$ 的条件分布是通过将 $x_t$ 边缘化掉来找到的。状态 $x_t$ 的均值为 $f(x_{t-1})$，方差为 $Q$。观测 $y_t$ 是 $x_t$ 的线性变换加上噪声。因此，$y_t$ 的预测分布也是高斯分布：\n$$p(y_t | x_{t-1}) = \\mathcal{N}(y_t; c f(x_{t-1}), c^2 Q + R)$$\n因此，最优提议分布的未归一化增量对数权重为：\n$$\\ell_{\\star}^{(i)} = -\\frac{1}{2(c^2 Q + R)}(y_t - c f(x_{t-1}^{(i)}))^2$$\n至关重要的是，该对数权重仅取决于上一时刻的状态 $x_{t-1}^{(i)}$，而不取决于新采样的状态 $x_t^{(i)}$。这些权重中的任何方差 $\\mathrm{var}(\\{\\ell_{\\star}^{(i)}\\}_{i=1}^N)$，完全是由父粒子 $\\{x_{t-1}^{(i)}\\}$ 的变异引起的。如果像用例 1 中那样，所有父粒子都相同，则最优权重的方差恰好为零。这证明了最优提议分布在理论上的优越性。\n\n每个测试用例的计算过程如下：\n1.  根据用例规范，生成包含 $N$ 个上一时刻状态粒子的集合 $\\{x_{t-1}^{(i)}\\}$。\n2.  对于自助提议分布，采样新粒子 $\\{x_t^{(i)}\\}$ 并计算 $\\{\\ell_{\\text{boot}}^{(i)}\\}$ 的方差。\n3.  对于最优提议分布，直接从 $\\{x_{t-1}^{(i)}\\}$ 计算 $\\{\\ell_{\\star}^{(i)}\\}$ 的方差。\n4.  每个用例的最终结果是三元组 $[\\mathrm{var}_{\\text{boot}}, \\mathrm{var}_{\\star}, \\mathrm{var}_{\\text{boot}} - \\mathrm{var}_{\\star}]$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the variance of unnormalized incremental log-weights\n    for the bootstrap and optimal proposals in a particle filter step.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"N\": 20000, \"Q\": 0.25, \"R\": 0.09, \"c\": 1.0, \"y_t\": 0.2,\n            \"x_tm1_spec\": (\"const\", 0.5), \"seed\": 42\n        },\n        {\n            \"N\": 20000, \"Q\": 1.0, \"R\": 0.25, \"c\": 1.2, \"y_t\": -0.3,\n            \"x_tm1_spec\": (\"normal\", 0.0, 1.0), \"seed\": 7\n        },\n        {\n            \"N\": 20000, \"Q\": 2.0, \"R\": 0.0025, \"c\": 1.0, \"y_t\": 1.0,\n            \"x_tm1_spec\": (\"normal\", 0.0, 2.0), \"seed\": 13\n        }\n    ]\n\n    # State transition function\n    def f(x):\n        return 0.5 * x + 0.05 * x**3\n\n    results_data = []\n\n    for case in test_cases:\n        N = case[\"N\"]\n        Q = case[\"Q\"]\n        R = case[\"R\"]\n        c = case[\"c\"]\n        y_t = case[\"y_t\"]\n        x_tm1_spec = case[\"x_tm1_spec\"]\n        seed = case[\"seed\"]\n\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Generate the previous particle set {x_{t-1}^{(i)}}\n        if x_tm1_spec[0] == \"const\":\n            x_tm1 = np.full(N, x_tm1_spec[1], dtype=np.float64)\n        elif x_tm1_spec[0] == \"normal\":\n            mean, var = x_tm1_spec[1], x_tm1_spec[2]\n            x_tm1 = rng.normal(loc=mean, scale=np.sqrt(var), size=N)\n        \n        # Calculate the mean of the state transition for all particles\n        mu_t = f(x_tm1)\n\n        # Step 2: Bootstrap Proposal\n        # Sample x_t from the prior p(x_t | x_{t-1})\n        x_t_boot = rng.normal(loc=mu_t, scale=np.sqrt(Q))\n        \n        # Compute unnormalized incremental log-weights\n        log_w_boot = -0.5 * ((y_t - c * x_t_boot)**2) / R\n        \n        # Compute unbiased sample variance\n        var_boot = np.var(log_w_boot, ddof=1)\n\n        # Step 3: Optimal Proposal\n        # The log-weight is a deterministic function of x_{t-1}\n        var_pred = c*c * Q + R\n        log_w_opt = -0.5 * ((y_t - c * mu_t)**2) / var_pred\n        \n        # Compute unbiased sample variance\n        var_opt = np.var(log_w_opt, ddof=1)\n        \n        # Step 4: Report results\n        diff = var_boot - var_opt\n        results_data.append([var_boot, var_opt, diff])\n\n    # Format the final output string exactly as required\n    formatted_triples = []\n    for triple in results_data:\n        # Format each triple as [val1,val2,val3] with numbers to 6 decimal places\n        s_triple = f\"[{triple[0]:.6f},{triple[1]:.6f},{triple[2]:.6f}]\"\n        formatted_triples.append(s_triple)\n    \n    final_output = f\"[{','.join(formatted_triples)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2890437"}, {"introduction": "现在，我们将学习一种适用于特定结构问题的高级技巧：Rao-Blackwellized 粒子滤波器 (RBPF)。许多实际系统包含线性和非线性两种状态分量，RBPF 正是为这类混合状态模型而生。在这个练习中，你将通过在每个粒子内部嵌入一个卡尔曼滤波器 (Kalman Filter) 来精确处理线性子状态，从而仅用粒子来估计棘手的非线性部分，最终实现一个高效且精确的混合滤波器。[@problem_id:2890368]", "problem": "要求您为一个混合状态隐马尔可夫模型实现一个 Rao-Blackwellized 粒子滤波器 (RBPF)。在该模型中，状态分解为 $x_t = \\begin{bmatrix} z_t^\\top & u_t^\\top \\end{bmatrix}^\\top$，其中 $z_t$ 是一个条件线性高斯子状态，$u_t$ 是一个非线性、非高斯子状态。目标是编写一个程序，该程序能针对一组指定的测试套件，模拟数据，运行一个在每个粒子内部对 $z_t$ 使用卡尔曼滤波器 (KF) 的 RBPF，并计算观测序列的总对数边际似然，即 $\\log p(y_{1:T})$，该值通过序贯蒙特卡洛 (SMC) 的归一化常数进行估计。所有角度必须以弧度为单位。不涉及物理单位。\n\n系统规定如下。设非线性子状态为标量，$u_t \\in \\mathbb{R}$，线性高斯子状态为二维，$z_t \\in \\mathbb{R}^{2}$。观测值为标量，$y_t \\in \\mathbb{R}$。对于所有 $t \\in \\{1,2,\\dots,T\\}$：\n- 非线性子状态动态：$u_t = g(u_{t-1}, t) + \\eta_t$，其中 $g(u, t) = 0.5\\,u + \\dfrac{25\\,u}{1 + u^2} + 8\\,\\cos(1.2\\,t)$ 且过程噪声 $\\eta_t$ 独立地从一个缩放的 Student-$t$ 分布中抽取，该分布具有 $\\nu_u$ 个自由度，其尺度因子的选择使得最终的标准差为 $\\sigma_u$。具体来说，若 $s_u = \\sigma_u \\sqrt{\\dfrac{\\nu_u - 2}{\\nu_u}}$，则 $\\eta_t \\sim s_u \\cdot \\mathcal{T}_{\\nu_u}$，其中 $\\mathcal{T}_{\\nu_u}$ 表示具有 $\\nu_u$ 个自由度的标准 Student-$t$ 分布。\n- 线性高斯子状态动态 (以 $u_t$ 为条件)：$z_t = A(u_t)\\,z_{t-1} + B(u_t) + w_t$，其中 $w_t \\sim \\mathcal{N}(0, Q(u_t))$ 且 $Q(u) = \\left(q_0 + q_1 u^2\\right) I_{2}$。矩阵由 $A(u) = \\alpha \\begin{bmatrix} \\cos(\\phi(u)) & \\sin(\\phi(u)) \\\\ -\\sin(\\phi(u)) & \\cos(\\phi(u)) \\end{bmatrix}$ 给出，其中 $\\alpha = 0.9$ 和 $\\phi(u) = 0.15 \\tanh(0.5 u)$；以及由 $B(u) = \\begin{bmatrix} 0.1\\,u \\\\ 0.05 \\sin(u) \\end{bmatrix}$ 给出。\n- 观测模型 (以 $z_t$ 和 $u_t$ 为条件)：$y_t = C(u_t)\\,z_t + D\\,u_t + v_t$，其中 $C(u) = \\begin{bmatrix} 1 & 0.5 \\tanh(u/3) \\end{bmatrix}$，$D = \\gamma$ 且 $\\gamma = 0.2$，以及 $v_t \\sim \\mathcal{N}(0, r)$。\n\n假设初始分布为 $u_0 \\sim \\mathcal{N}(0, s_{u0}^2)$ 且 $s_{u0} = 1$，以及 $z_0 \\sim \\mathcal{N}(m_0, P_0)$ 且 $m_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ 和 $P_0 = I_2$，两者相互独立。\n\n您的 RBPF 必须：\n- 使用 N 个粒子表示 $u_t$，并以先验作为提议分布。\n- 对每个粒子，在以该粒子的 $u_t$ 为条件下，对 $z_t$ 运行卡尔曼滤波器，以计算每个粒子的预测边际似然 $p(y_t \\mid y_{1:t-1}, u_{1:t}^{(i)})$ 并更新每个粒子 $z_t$ 的条件后验。\n- 使用计算出的边际似然更新粒子权重。通过在每个时间步 $t$ 累加归一化常数，使用标准的 SMC 估计器估计边际似然。\n- 当有效样本量低于 $N$ 的一个分数 $\\rho$（其中 $\\rho = 0.5$）时，可选择使用系统重采样进行重采样。重采样后，将所有粒子的权重重置为均匀权重。\n\n在您的推导和实现中必须依赖的基本原理包括：隐马尔可夫模型的 Bayes 滤波递归、线性高斯系统的性质 (用于构建卡尔曼滤波器) 以及带重采样的序贯蒙特卡洛重要性采样结构。\n\n您的程序必须实现以下测试套件，其中每个测试用例定义了一个模拟种子、一个时间范围 $T$、粒子数量 $N$ 和噪声参数。角度以弧度为单位。\n- 测试用例 1：seed $= 123$, $T = 25$, $N = 200$, $q_0 = 0.05$, $q_1 = 0.01$, $r = 0.25$, $\\sigma_u = 1.0$, $\\nu_u = 7$。\n- 测试用例 2：seed $= 2025$, $T = 10$, $N = 300$, $q_0 = 10^{-6}$, $q_1 = 10^{-4}$, $r = 0.09$, $\\sigma_u = 0.5$, $\\nu_u = 5$。\n- 测试用例 3：seed $= 777$, $T = 30$, $N = 150$, $q_0 = 0.1$, $q_1 = 0.02$, $r = 1.0$, $\\sigma_u = 1.5$, $\\nu_u = 9$。\n\n对于每个测试用例：\n- 使用指定的种子和上述模型，模拟一个单一的轨迹和观测序列 $\\{(u_t, z_t, y_t)\\}_{t=1}^T$。\n- 使用相同的参数运行 RBPF，并为粒子滤波器的随机数生成器设置相同的种子。\n- 计算估计的总对数边际似然 $\\log \\widehat{p}(y_{1:T})$，方法是将在 RBPF 权重更新中得到的每个时间步的归一化常数的对数相加，这与将先验用作 $u_t$ 的提议分布并将每个粒子的卡尔曼滤波器边际似然用作 $y_t$ 的似然是一致的。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表，结果的顺序与测试用例的顺序相同。每个值都必须是精确到 $6$ 位小数的十进制浮点数，例如 $\\left[\\ell_1, \\ell_2, \\ell_3\\right]$，其中每个 $\\ell_i$ 格式化为 $6$ 位小数。\n\n答案必须是一个完整、可运行的程序。不允许用户输入或使用外部文件。唯一允许使用的库是 Python 标准库和指定的数值计算库。", "solution": "该问题在统计信号处理和计算统计领域是一个定义明确且科学上合理的挑战。它要求为一个具有条件线性高斯结构的混合状态隐马尔可夫模型实现一个 Rao-Blackwellized 粒子滤波器 (RBPF)。该模型由一个非线性标量状态 $u_t$ 和一个条件线性的二维状态 $z_t$ 组成，所有必要的动态方程、观测模型、噪声分布和参数都得到了明确的规定。此设置是应用 RBPF 的典型场景，其中该算法可以通过解析地处理线性子状态（使用卡尔曼滤波器）来极大地提高效率。\n\n### 基于原理的设计与推导\n\n问题要求对一个隐马尔可夫模型的状态 $x_t = (z_t, u_t)$ 进行滤波。模型结构是条件线性高斯的：在非线性状态 $u_{1:t}$ 的轨迹为条件下， $z_t$ 的动态和 $y_t$ 的观测模型是线性和高斯的。Rao-Blackwellized 粒子滤波器 (RBPF) 正是利用了这种结构。\n\n这里的 Rao-Blackwellization 原理是将状态的联合后验分解为：\n$$\np(u_{1:t}, z_{1:t} | y_{1:t}) = p(z_{1:t} | u_{1:t}, y_{1:t}) p(u_{1:t} | y_{1:t})\n$$\n第一项，即在给定非线性轨迹 $u_{1:t}$ 和观测值 $y_{1:t}$ 的条件下，线性状态 $z_{1:t}$ 的后验，可以使用卡尔曼滤波器解析计算。第二项，即非线性状态轨迹的后验 $p(u_{1:t} | y_{1:t})$，是难以处理的，并使用序贯蒙特卡洛 (SMC) 方法，即粒子滤波器，进行近似。\n\nRBPF 中的每个粒子 $i$ 代表对非线性状态轨迹 $u_{1:t}^{(i)}$ 的一个假设。每个粒子都附带一个卡尔曼滤波器，用于跟踪线性状态 $z_t$ 的后验分布，该分布是高斯分布：$p(z_t | u_{1:t}^{(i)}, y_{1:t}) = \\mathcal{N}(z_t; m_{t|t}^{(i)}, P_{t|t}^{(i)})$。\n\n滤波过程递归进行。在时间 $t$，对每个粒子 $i$：\n\n1.  **传播**：从一个提议分布中采样一个新的非线性状态 $u_t^{(i)}$。问题指定使用先验作为提议分布：\n    $$\n    u_t^{(i)} \\sim p(u_t | u_{t-1}^{(i)})\n    $$\n    这里的噪声 $\\eta_t$ 来自一个缩放的 Student-$t$ 分布，因此我们从中抽取一个样本来传播状态。\n\n2.  **卡尔曼滤波器预测**：与粒子 $i$ 相关联的卡尔曼滤波器执行预测步骤。使用来自前一时间步的后验 $\\mathcal{N}(z_{t-1}; m_{t-1|t-1}^{(i)}, P_{t-1|t-1}^{(i)})$，并以新采样的 $u_t^{(i)}$ 为条件， $z_t$ 的预测分布是一个高斯分布 $\\mathcal{N}(z_t; m_{t|t-1}^{(i)}, P_{t|t-1}^{(i)})$，其中：\n    $$\n    m_{t|t-1}^{(i)} = A(u_t^{(i)}) m_{t-1|t-1}^{(i)} + B(u_t^{(i)})\n    $$\n    $$\n    P_{t|t-1}^{(i)} = A(u_t^{(i)}) P_{t-1|t-1}^{(i)} A(u_t^{(i)})^\\top + Q(u_t^{(i)})\n    $$\n\n3.  **权重更新**：每个粒子的重要性权重是在给定该粒子历史的条件下观测值 $y_t$ 的似然。这就是卡尔曼滤波器的预测似然：\n    $$\n    \\tilde{w}_t^{(i)} = p(y_t | u_{1:t}^{(i)}, y_{1:t-1})\n    $$\n    观测模型是 $y_t = C(u_t^{(i)})z_t + D u_t^{(i)} + v_t$。预测的观测值是一个具有均值和方差的高斯分布：\n    $$\n    y_{pred, t}^{(i)} = C(u_t^{(i)}) m_{t|t-1}^{(i)} + D u_t^{(i)}\n    $$\n    $$\n    S_t^{(i)} = C(u_t^{(i)}) P_{t|t-1}^{(i)} C(u_t^{(i)})^\\top + r\n    $$\n    因此，权重为 $\\tilde{w}_t^{(i)} = \\mathcal{N}(y_t; y_{pred, t}^{(i)}, S_t^{(i)})$。未归一化的权重递归更新：$W_t^{(i)} \\propto W_{t-1}^{(i)} \\tilde{w}_t^{(i)}$。\n\n4.  **对数似然估计**：总对数边际似然 $\\log p(y_{1:T})$ 通过对每个步骤的归一化常数的对数求和来估计：\n    $$\n    \\log \\widehat{p}(y_{1:T}) = \\sum_{t=1}^T \\log \\left( \\frac{1}{N} \\sum_{i=1}^N \\tilde{w}_t^{(i)} \\right)\n    $$\n    其中，我们在每个步骤之后（可能在重采样之后）都假定权重是归一化的。一个更直接的公式是：\n    $$\n    \\log \\widehat{p}(y_{1:T}) = \\sum_{t=1}^T \\log \\left( \\sum_{i=1}^N w_{t-1}^{(i)} \\tilde{w}_t^{(i)} \\right)\n    $$\n    其中 $w_{t-1}^{(i)}$ 是来自步骤 $t-1$ 的归一化权重。\n\n5.  **卡尔曼滤波器更新**：每个粒子的卡尔曼滤波器状态使用观测值 $y_t$ 进行更新。后验变为 $\\mathcal{N}(z_t; m_{t|t}^{(i)}, P_{t|t}^{(i)})$：\n    $$\n    K_t^{(i)} = P_{t|t-1}^{(i)} C(u_t^{(i)})^\\top (S_t^{(i)})^{-1}\n    $$\n    $$\n    m_{t|t}^{(i)} = m_{t|t-1}^{(i)} + K_t^{(i)} (y_t - y_{pred, t}^{(i)})\n    $$\n    $$\n    P_{t|t}^{(i)} = (I - K_t^{(i)} C(u_t^{(i)})) P_{t|t-1}^{(i)}\n    $$\n\n6.  **重采样**：为了减轻粒子退化，当有效样本量 $\\text{ESS} = 1 / \\sum_{i=1}^N (w_t^{(i)})^2$ 低于阈值 $\\rho N$ 时，执行重采样步骤。根据权重进行有放回的粒子抽取，然后将权重重置为均匀的 $1/N$。实现将使用系统重采样以提高效率。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Rao-Blackwellized Particle Filter for the specified mixed-state\n    hidden Markov model, computes the log marginal likelihood for given test cases.\n    \"\"\"\n    test_cases = [\n        {'seed': 123, 'T': 25, 'N': 200, 'q0': 0.05, 'q1': 0.01, 'r': 0.25, 'sigma_u': 1.0, 'nu_u': 7},\n        {'seed': 2025, 'T': 10, 'N': 300, 'q0': 1e-6, 'q1': 1e-4, 'r': 0.09, 'sigma_u': 0.5, 'nu_u': 5},\n        {'seed': 777, 'T': 30, 'N': 150, 'q0': 0.1, 'q1': 0.02, 'r': 1.0, 'sigma_u': 1.5, 'nu_u': 9}\n    ]\n\n    # Global model constants\n    alpha = 0.9\n    gamma = 0.2\n    m0 = np.array([[0.0], [0.0]])\n    P0 = np.eye(2)\n    s_u0 = 1.0\n    resample_threshold_frac = 0.5\n\n    # --- Vectorized Model Functions ---\n    def g_func(u, t):\n        return 0.5 * u + (25.0 * u) / (1.0 + u**2) + 8.0 * np.cos(1.2 * t)\n\n    def A_func(u):\n        phi = 0.15 * np.tanh(0.5 * u)\n        cos_phi = np.cos(phi)\n        sin_phi = np.sin(phi)\n        \n        if np.isscalar(u):\n            return alpha * np.array([[cos_phi, sin_phi], [-sin_phi, cos_phi]])\n        \n        N = u.shape[0]\n        A = np.zeros((N, 2, 2))\n        A[:, 0, 0] = alpha * cos_phi\n        A[:, 0, 1] = alpha * sin_phi\n        A[:, 1, 0] = -alpha * sin_phi\n        A[:, 1, 1] = alpha * cos_phi\n        return A\n\n    def B_func(u):\n        if np.isscalar(u):\n            return np.array([[0.1 * u], [0.05 * np.sin(u)]])\n\n        N = u.shape[0]\n        B = np.zeros((N, 2, 1))\n        B[:, 0, 0] = 0.1 * u\n        B[:, 1, 0] = 0.05 * np.sin(u)\n        return B\n\n    def Q_func(u, q0, q1):\n        q_val = q0 + q1 * u**2\n        if np.isscalar(u):\n            return q_val * np.eye(2)\n        \n        N = u.shape[0]\n        Q = np.zeros((N, 2, 2))\n        Q[:, 0, 0] = q_val\n        Q[:, 1, 1] = q_val\n        return Q\n\n    def C_func(u):\n        if np.isscalar(u):\n            return np.array([[1.0, 0.5 * np.tanh(u / 3.0)]])\n        \n        N = u.shape[0]\n        C = np.zeros((N, 1, 2))\n        C[:, 0, 0] = 1.0\n        C[:, 0, 1] = 0.5 * np.tanh(u / 3.0)\n        return C\n\n    def systematic_resample(log_weights, rng):\n        N = len(log_weights)\n        # Normalize weights from logs\n        weights = np.exp(log_weights - np.max(log_weights))\n        weights /= np.sum(weights)\n\n        positions = (np.arange(N) + rng.uniform()) / N\n        indices = np.zeros(N, 'i')\n        cumulative_sum = np.cumsum(weights)\n        i, j = 0, 0\n        while i  N:\n            if positions[i]  cumulative_sum[j]:\n                indices[i] = j\n                i += 1\n            else:\n                j += 1\n                if j == N: # Safeguard for floating point issues\n                    j = N - 1\n        return indices\n\n    results = []\n    \n    for case in test_cases:\n        seed, T, N = case['seed'], case['T'], case['N']\n        q0, q1, r = case['q0'], case['q1'], case['r']\n        sigma_u, nu_u = case['sigma_u'], case['nu_u']\n        \n        rng = np.random.default_rng(seed)\n        \n        # --- 1. Simulate Data ---\n        u_hist = np.zeros(T + 1)\n        z_hist = np.zeros((T + 1, 2, 1))\n        y_obs = np.zeros(T)\n        \n        u_hist[0] = rng.normal(0, s_u0)\n        z_hist[0, :, :] = rng.multivariate_normal(m0.flatten(), P0).reshape(2, 1)\n        \n        s_u = sigma_u * np.sqrt((nu_u - 2) / nu_u)\n        \n        for t in range(1, T + 1):\n            eta_t = s_u * rng.standard_t(nu_u)\n            u_hist[t] = g_func(u_hist[t - 1], t) + eta_t\n            \n            w_t = rng.multivariate_normal([0, 0], Q_func(u_hist[t], q0, q1)).reshape(2, 1)\n            z_hist[t] = A_func(u_hist[t]) @ z_hist[t - 1] + B_func(u_hist[t]) + w_t\n            \n            v_t = rng.normal(0, np.sqrt(r))\n            y_obs[t - 1] = (C_func(u_hist[t]) @ z_hist[t] + gamma * u_hist[t] + v_t).item()\n        \n        # --- 2. Run RBPF ---\n        rng = np.random.default_rng(seed)\n        \n        u_particles = rng.normal(0, s_u0, size=N)\n        m_particles = np.tile(m0, (N, 1, 1))\n        P_particles = np.tile(P0, (N, 1, 1))\n        log_weights = np.full(N, -np.log(N))\n        \n        total_log_likelihood = 0.0\n        \n        for t_idx, y_t in enumerate(y_obs):\n            t = t_idx + 1\n            \n            # Resample if needed (based on weights from previous step)\n            weights = np.exp(log_weights - np.max(log_weights))\n            weights /= np.sum(weights)\n            ess = 1.0 / np.sum(weights**2)\n\n            if ess  resample_threshold_frac * N:\n                indices = systematic_resample(log_weights, rng)\n                u_particles = u_particles[indices]\n                m_particles = m_particles[indices]\n                P_particles = P_particles[indices]\n                log_weights.fill(-np.log(N))\n            \n            u_tm1_particles = u_particles\n            m_tm1_particles = m_particles\n            P_tm1_particles = P_particles\n            \n            # Propagate non-linear state\n            eta_t_p = s_u * rng.standard_t(nu_u, size=N)\n            u_particles = g_func(u_tm1_particles, t) + eta_t_p\n            \n            # KF Prediction (vectorized over N particles)\n            A_p = A_func(u_particles)\n            B_p = B_func(u_particles)\n            Q_p = Q_func(u_particles, q0, q1)\n            m_pred = A_p @ m_tm1_particles + B_p\n            P_pred = A_p @ P_tm1_particles @ A_p.transpose(0, 2, 1) + Q_p\n            \n            # Predictive Likelihood  Weight Update\n            C_p = C_func(u_particles)\n\n            y_pred = C_p @ m_pred + gamma * u_particles[:, np.newaxis, np.newaxis]\n            S = C_p @ P_pred @ C_p.transpose(0, 2, 1) + r\n            \n            y_pred_sq, S_sq = y_pred.squeeze(), S.squeeze()\n            \n            log_particle_likelihoods = -0.5 * (np.log(2 * np.pi) + np.log(S_sq) + (y_t - y_pred_sq)**2 / S_sq)\n            \n            log_unnormalized_weights = log_weights + log_particle_likelihoods\n            \n            max_log_w = np.max(log_unnormalized_weights)\n            log_norm_const = max_log_w + np.log(np.sum(np.exp(log_unnormalized_weights - max_log_w)))\n            \n            total_log_likelihood += log_norm_const\n            \n            log_weights = log_unnormalized_weights - log_norm_const\n            \n            # KF Update (vectorized)\n            S_inv = 1.0 / S\n            K = P_pred @ C_p.transpose(0, 2, 1) * S_inv\n            innovation = y_t - y_pred\n            m_particles = m_pred + K * innovation\n            I_N = np.eye(2)[np.newaxis, :, :]\n            P_particles = (I_N - K @ C_p) @ P_pred\n        \n        results.append(f\"{total_log_likelihood:.6f}\")\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2890368"}]}