{"hands_on_practices": [{"introduction": "在我们深入研究像LMS这样的随机算法之前，理解其“理想”情况至关重要：即在完全了解误差曲面情况下的最速下降法。本练习将引导您动手推导并计算单次迭代中的最优步长，将抽象的梯度概念与具体的最优下降过程联系起来。通过这个基础计算，您将为理解更复杂的自适应算法奠定坚实的直觉基础。[@problem_id:2874690]", "problem": "考虑三种经典的自适应滤波应用——系统辨识、噪声消除和线性均衡——每种应用都由相同的二次均方误差准则建模。在这三种情况下，观测一个长度为$3$的输入向量 $\\mathbf{x}(n) \\in \\mathbb{R}^{3}$ 驱动一个未知线性系统，其系数向量为 $\\mathbf{w}_{\\star} \\in \\mathbb{R}^{3}$，并带有一个测量噪声 $v(n)$，该噪声为零均值，与 $\\mathbf{x}(n)$ 独立，且方差有限。期望信号为 $d(n) = \\mathbf{w}_{\\star}^{\\top} \\mathbf{x}(n) + v(n)$。自适应滤波器寻求一个 $\\mathbf{w} \\in \\mathbb{R}^{3}$ 来最小化均方误差代价 $J(\\mathbf{w}) \\triangleq \\mathbb{E}\\{(d(n) - \\mathbf{w}^{\\top}\\mathbf{x}(n))^{2}\\}$。\n\n假设输入 $\\mathbf{x}(n)$ 是由一个零均值、广义平稳、单位方差且相关系数为 $\\rho = \\tfrac{1}{2}$ 的1阶自回归（AR($1$)）过程的三个连续样本堆叠而成，因此其自相关函数为 $r_{x}(k) = \\rho^{|k|}$ (其中 $k \\in \\mathbb{Z}$)。相应的输入协方差矩阵是 Toeplitz 矩阵 $\\mathbf{R} \\in \\mathbb{R}^{3 \\times 3}$，其元素为 $[\\mathbf{R}]_{i,j} = r_{x}(i-j)$ (其中 $i,j \\in \\{1,2,3\\}$)。设真实系统为 $\\mathbf{w}_{\\star} = [\\,1,\\,-\\tfrac{1}{2},\\,\\tfrac{1}{4}\\,]^{\\top}$。定义互相关向量 $\\mathbf{p} \\triangleq \\mathbb{E}\\{\\mathbf{x}(n)\\,d(n)\\}$。\n\n为最小化 $J(\\mathbf{w})$，采用带有精确线搜索的最速下降递归：\n$\\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\mu_{k}\\,\\nabla J(\\mathbf{w}_{k})$，\n其中第 $k$ 次迭代的步长 $\\mu_{k}$ 的选择旨在最小化关于 $\\mu \\in \\mathbb{R}$ 的 $J(\\mathbf{w}_{k} - \\mu\\,\\nabla J(\\mathbf{w}_{k}))$。假设当前迭代值为 $\\mathbf{w}_{k} = \\mathbf{0}$。\n\n仅从 $J(\\mathbf{w})$、$\\mathbf{R}$ 和 $\\mathbf{p}$ 的定义以及二次型的标准矩阵微积分法则出发，推导精确线搜索步长 $\\mu_{k}$，并利用上述数据对其进行数值计算。将最终答案表示为一个精确分数（无小数近似）。无需四舍五入，不涉及单位。", "solution": "该问题是有效的。这是一个在自适应信号处理领域中适定且有科学依据的问题。我们将给出完整的解答。\n\n均方误差 (MSE) 代价函数定义为\n$$ J(\\mathbf{w}) \\triangleq \\mathbb{E}\\{(d(n) - \\mathbf{w}^{\\top}\\mathbf{x}(n))^{2}\\} $$\n我们展开此表达式，以揭示其关于权重向量 $\\mathbf{w}$ 的二次结构。\n$$ J(\\mathbf{w}) = \\mathbb{E}\\{d(n)^2 - 2d(n)\\mathbf{x}(n)^{\\top}\\mathbf{w} + \\mathbf{w}^{\\top}\\mathbf{x}(n)\\mathbf{x}(n)^{\\top}\\mathbf{w}\\} $$\n利用期望算子的线性性质，我们可以写出\n$$ J(\\mathbf{w}) = \\mathbb{E}\\{d(n)^2\\} - 2\\mathbb{E}\\{d(n)\\mathbf{x}(n)^{\\top}\\}\\mathbf{w} + \\mathbf{w}^{\\top}\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}(n)^{\\top}\\}\\mathbf{w} $$\n根据问题陈述中给出的定义，我们识别出以下各项：\n- $\\sigma_d^2 \\triangleq \\mathbb{E}\\{d(n)^2\\}$ 是期望信号 $d(n)$ 的方差。\n- $\\mathbf{p} \\triangleq \\mathbb{E}\\{\\mathbf{x}(n)d(n)\\}$ 是输入 $\\mathbf{x}(n)$ 与期望信号 $d(n)$ 之间的互相关向量。因此，$\\mathbb{E}\\{d(n)\\mathbf{x}(n)^{\\top}\\} = \\mathbf{p}^{\\top}$。\n- $\\mathbf{R} \\triangleq \\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}(n)^{\\top}\\}$ 是输入向量 $\\mathbf{x}(n)$ 的自相关矩阵。\n\n代入这些定义，代价函数变为一个二次型：\n$$ J(\\mathbf{w}) = \\sigma_d^2 - 2\\mathbf{p}^{\\top}\\mathbf{w} + \\mathbf{w}^{\\top}\\mathbf{R}\\mathbf{w} $$\n最速下降算法需要此代价函数关于 $\\mathbf{w}$ 的梯度。对于对称矩阵 $\\mathbf{R}$（自相关矩阵必然是对称的），使用向量微积分的标准法则，我们有：\n$$ \\nabla J(\\mathbf{w}) = \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} = -2\\mathbf{p} + 2\\mathbf{R}\\mathbf{w} $$\n最速下降递归在第 $k$ 次迭代时按如下方式更新权重向量 $\\mathbf{w}_k$：\n$$ \\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\mu_{k}\\,\\nabla J(\\mathbf{w}_{k}) $$\n此处，步长 $\\mu_k$ 通过精确线搜索选择，这意味着它会最小化沿负梯度方向的代价函数。令 $\\mathbf{g}_k \\triangleq \\nabla J(\\mathbf{w}_k)$。我们寻求找到使函数 $f(\\mu) = J(\\mathbf{w}_k - \\mu \\mathbf{g}_k)$ 最小化的 $\\mu_k$。\n$$ f(\\mu) = J(\\mathbf{w}_k - \\mu \\mathbf{g}_k) = \\sigma_d^2 - 2\\mathbf{p}^{\\top}(\\mathbf{w}_k - \\mu \\mathbf{g}_k) + (\\mathbf{w}_k - \\mu \\mathbf{g}_k)^{\\top}\\mathbf{R}(\\mathbf{w}_k - \\mu \\mathbf{g}_k) $$\n为求最小值，我们将 $f(\\mu)$ 对 $\\mu$ 的导数设为零。\n$$ \\frac{df(\\mu)}{d\\mu} = 2\\mathbf{p}^{\\top}\\mathbf{g}_k - 2\\mathbf{w}_k^{\\top}\\mathbf{R}\\mathbf{g}_k + 2\\mu \\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{g}_k = 0 $$\n注意，由于结果是标量，所以 $\\mathbf{w}_k^{\\top}\\mathbf{R}\\mathbf{g}_k = \\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{w}_k$。\n对 $\\mu$ 求解，我们得到：\n$$ \\mu (\\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{g}_k) = \\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{w}_k - \\mathbf{g}_k^{\\top}\\mathbf{p} = \\mathbf{g}_k^{\\top}(\\mathbf{R}\\mathbf{w}_k - \\mathbf{p}) $$\n我们知道 $\\mathbf{g}_k = 2\\mathbf{R}\\mathbf{w}_k - 2\\mathbf{p}$，这意味着 $\\mathbf{R}\\mathbf{w}_k - \\mathbf{p} = \\frac{1}{2}\\mathbf{g}_k$。\n将此代入 $\\mu$ 的表达式中：\n$$ \\mu (\\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{g}_k) = \\mathbf{g}_k^{\\top}\\left(\\frac{1}{2}\\mathbf{g}_k\\right) = \\frac{1}{2}\\mathbf{g}_k^{\\top}\\mathbf{g}_k $$\n因此，精确线搜索的最优步长为：\n$$ \\mu_k = \\frac{\\mathbf{g}_k^{\\top}\\mathbf{g}_k}{2\\mathbf{g}_k^{\\top}\\mathbf{R}\\mathbf{g}_k} $$\n问题陈述当前迭代值为 $\\mathbf{w}_k = \\mathbf{0}$。我们在此点计算梯度：\n$$ \\mathbf{g}_k = \\nabla J(\\mathbf{0}) = -2\\mathbf{p} + 2\\mathbf{R}(\\mathbf{0}) = -2\\mathbf{p} $$\n将此特定梯度代入 $\\mu_k$ 的公式中：\n$$ \\mu_k = \\frac{(-2\\mathbf{p})^{\\top}(-2\\mathbf{p})}{2(-2\\mathbf{p})^{\\top}\\mathbf{R}(-2\\mathbf{p})} = \\frac{4\\mathbf{p}^{\\top}\\mathbf{p}}{2 \\cdot 4 \\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p}} = \\frac{\\mathbf{p}^{\\top}\\mathbf{p}}{2\\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p}} $$\n现在，我们必须计算 $\\mathbf{R}$ 和 $\\mathbf{p}$ 的数值。\n输入自相关矩阵 $\\mathbf{R}$ 是一个 $3 \\times 3$ 的 Toeplitz 矩阵，其元素为 $[\\mathbf{R}]_{i,j} = r_x(i-j) = \\rho^{|i-j|}$，其中 $\\rho = \\frac{1}{2}$。\n$$ \\mathbf{R} = \\begin{pmatrix} \\rho^0 & \\rho^1 & \\rho^2 \\\\ \\rho^1 & \\rho^0 & \\rho^1 \\\\ \\rho^2 & \\rho^1 & \\rho^0 \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ \\frac{1}{2} & 1 & \\frac{1}{2} \\\\ \\frac{1}{4} & \\frac{1}{2} & 1 \\end{pmatrix} $$\n互相关向量 $\\mathbf{p}$ 由 $\\mathbf{p} = \\mathbb{E}\\{\\mathbf{x}(n)d(n)\\}$ 给出。代入 $d(n) = \\mathbf{w}_{\\star}^{\\top}\\mathbf{x}(n) + v(n)$：\n$$ \\mathbf{p} = \\mathbb{E}\\{\\mathbf{x}(n)(\\mathbf{x}(n)^{\\top}\\mathbf{w}_{\\star} + v(n))\\} = \\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}(n)^{\\top}\\}\\mathbf{w}_{\\star} + \\mathbb{E}\\{\\mathbf{x}(n)v(n)\\} $$\n由于 $\\mathbf{x}(n)$ 和 $v(n)$ 是独立的，且 $v(n)$ 是零均值，所以 $\\mathbb{E}\\{\\mathbf{x}(n)v(n)\\} = \\mathbb{E}\\{\\mathbf{x}(n)\\}\\mathbb{E}\\{v(n)\\} = \\mathbf{0}$。因此：\n$$ \\mathbf{p} = \\mathbf{R}\\mathbf{w}_{\\star} $$\n给定 $\\mathbf{w}_{\\star} = [\\,1,\\,-\\tfrac{1}{2},\\,\\tfrac{1}{4}\\,]^{\\top}$：\n$$ \\mathbf{p} = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ \\frac{1}{2} & 1 & \\frac{1}{2} \\\\ \\frac{1}{4} & \\frac{1}{2} & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1(1) + \\frac{1}{2}(-\\frac{1}{2}) + \\frac{1}{4}(\\frac{1}{4}) \\\\ \\frac{1}{2}(1) + 1(-\\frac{1}{2}) + \\frac{1}{2}(\\frac{1}{4}) \\\\ \\frac{1}{4}(1) + \\frac{1}{2}(-\\frac{1}{2}) + 1(\\frac{1}{4}) \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{1}{4} + \\frac{1}{16} \\\\ \\frac{1}{2} - \\frac{1}{2} + \\frac{1}{8} \\\\ \\frac{1}{4} - \\frac{1}{4} + \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{16} \\\\ \\frac{1}{8} \\\\ \\frac{1}{4} \\end{pmatrix} $$\n接下来，我们计算二次型 $\\mathbf{p}^{\\top}\\mathbf{p}$ 和 $\\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p}$。\n$$ \\mathbf{p}^{\\top}\\mathbf{p} = \\left(\\frac{13}{16}\\right)^2 + \\left(\\frac{1}{8}\\right)^2 + \\left(\\frac{1}{4}\\right)^2 = \\left(\\frac{13}{16}\\right)^2 + \\left(\\frac{2}{16}\\right)^2 + \\left(\\frac{4}{16}\\right)^2 = \\frac{13^2 + 2^2 + 4^2}{16^2} = \\frac{169 + 4 + 16}{256} = \\frac{189}{256} $$\n为计算 $\\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p}$，我们首先计算向量 $\\mathbf{z} = \\mathbf{R}\\mathbf{p}$：\n$$ \\mathbf{z} = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ \\frac{1}{2} & 1 & \\frac{1}{2} \\\\ \\frac{1}{4} & \\frac{1}{2} & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{13}{16} \\\\ \\frac{2}{16} \\\\ \\frac{4}{16} \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ \\frac{1}{2} & 1 & \\frac{1}{2} \\\\ \\frac{1}{4} & \\frac{1}{2} & 1 \\end{pmatrix} \\begin{pmatrix} 13 \\\\ 2 \\\\ 4 \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} 13 + 1 + 1 \\\\ \\frac{13}{2} + 2 + 2 \\\\ \\frac{13}{4} + 1 + 4 \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} 15 \\\\ \\frac{21}{2} \\\\ \\frac{33}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{15}{16} \\\\ \\frac{21}{32} \\\\ \\frac{33}{64} \\end{pmatrix} $$\n现在，我们计算点积 $\\mathbf{p}^{\\top}\\mathbf{z}$：\n$$ \\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p} = \\mathbf{p}^{\\top}\\mathbf{z} = \\begin{pmatrix} \\frac{13}{16} & \\frac{2}{16} & \\frac{4}{16} \\end{pmatrix} \\begin{pmatrix} \\frac{15}{16} \\\\ \\frac{21}{32} \\\\ \\frac{33}{64} \\end{pmatrix} = \\frac{13}{16}\\frac{15}{16} + \\frac{2}{16}\\frac{21}{32} + \\frac{4}{16}\\frac{33}{64} $$\n$$ = \\frac{195}{256} + \\frac{42}{512} + \\frac{132}{1024} = \\frac{195 \\cdot 4}{1024} + \\frac{42 \\cdot 2}{1024} + \\frac{132}{1024} = \\frac{780 + 84 + 132}{1024} = \\frac{996}{1024} $$\n将此分数分子分母同除以 $4$ 进行化简：\n$$ \\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p} = \\frac{249}{256} $$\n最后，我们将这些值代入 $\\mu_k$ 的表达式中：\n$$ \\mu_k = \\frac{\\mathbf{p}^{\\top}\\mathbf{p}}{2\\mathbf{p}^{\\top}\\mathbf{R}\\mathbf{p}} = \\frac{\\frac{189}{256}}{2 \\cdot \\frac{249}{256}} = \\frac{189}{2 \\cdot 249} = \\frac{189}{498} $$\n我们对最终的分数进行化简。分子和分母都含有因数3：\n$$ \\mu_k = \\frac{189}{498} = \\frac{189 \\div 3}{498 \\div 3} = \\frac{63}{166} $$\n该分数为最简分数，因为分子 $63 = 3^2 \\cdot 7$ 与分母 $166 = 2 \\cdot 83$ 没有共同的质因数。", "answer": "$$ \\boxed{\\frac{63}{166}} $$", "id": "2874690"}, {"introduction": "理论上的稳定性边界，例如步长 $\\mu$ 必须满足的条件，是自适应滤波器设计的基石。这个实践练习不仅要求您推导出这些边界，更挑战您通过编写数值仿真来检验它们的“紧致性”，即观察算法在收敛与发散临界点上的实际行为。这个练习将磨练您在解析推导和经验验证两方面的关键技能。[@problem_id:2874693]", "problem": "考虑线性模型 $d(n) = x^\\top(n) w^\\star + v(n)$ 的系数向量 $w \\in \\mathbb{R}^m$ 的自适应估计问题。其中，$x(n) \\in \\mathbb{R}^m$ 是零均值、宽平稳的输入，其自相关矩阵 $R = \\mathbb{E}\\{x(n) x^\\top(n)\\}$ 为正定矩阵；$w^\\star$ 是未知的最优向量；$v(n)$ 是与 $x(n)$ 无关的零均值噪声。均方误差代价函数为 $J(w) = \\mathbb{E}\\{(d(n) - x^\\top(n) w)^2\\}$。您将分析三种算法的收敛步长界限：确定性最速下降法、最小均方（LMS）算法和归一化最小均方（NLMS）算法，重点关注其必要、充分和紧凑界限。\n\n基于以下基本原理：\n- 形式为 $J(w) = \\tfrac{1}{2} (w - w^\\star)^\\top R (w - w^\\star) + \\text{const}$ 的二次代价函数的梯度为 $\\nabla J(w) = R (w - w^\\star)$。\n- 最速下降法递归式为 $w_{n+1} = w_n - \\mu \\nabla J(w_n)$，由此可导出线性误差递归式 $e_{n+1} = (I - \\mu R) e_n$，其中 $e_n = w_n - w^\\star$。\n- 对于线性时不变递归式 $e_{n+1} = A e_n$，其收敛至 $e_n \\to 0$ 的充分必要条件是谱半径 $\\rho(A)$ 满足 $\\rho(A) < 1$。\n- 对于 LMS 算法，使用 $w_n$ 与当前回归量 $x(n)$ 之间的独立性假设，均值误差更新为 $\\mathbb{E}\\{e_{n+1}\\} = (I - \\mu R) \\mathbb{E}\\{e_n\\}$，而均方分析涉及高阶矩，通常得到以 $R$ 表示的保守充分界限。\n- 对于 NLMS 算法，其归一化更新使用 $\\mu_0$ 作为无量纲步长参数。\n\n您的任务：\n1) 确定性最速下降法的收敛界及其紧凑性。从 $I - \\mu R$ 的谱条件出发，推导 $\\mu$ 关于 $R$ 的最大特征值的充分必要区间。然后，设计一个数值测试，通过仿真边界附近的误差递归来评估其紧凑性。\n2) LMS 算法的均值收敛界和一个保守的均方稳定性充分界。从独立性假设和导出的线性均值递归出发，推导 $\\mu$ 关于 $R$ 的极端特征值的均值稳定性充分必要区间。此外，仅用 $R$ 推导一个易于计算的、保守的均方稳定性充分界。设计一个数值测试，用于检验均值稳定界在边界及其附近的紧凑性。\n3) NLMS 算法的稳定性界及其紧凑性。从归一化更新出发，推导无量纲步长参数 $\\mu_0$ 的一个与 $R$ 无关的稳定区间（假设分母中使用一个小的正常数正则化项 $ \\delta $ 来避免除以零）。设计一个数值测试，通过仿真边界附近的 NLMS 算法来评估其紧凑性。\n\n测试套件规范：\n- 对以下三种情况使用下列数值数据。\n- 情况 A（最速下降法，维度 $m=2$）：\n  - $R_A = \\begin{bmatrix} 1.2 & 0.3 \\\\ 0.3 & 0.8 \\end{bmatrix}$。\n  - 选择 $w_A^\\star = \\begin{bmatrix} 1.0 \\\\ -2.0 \\end{bmatrix}$。\n  - 通过设置 $w_{A,0} = w_A^\\star + \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$，使初始误差 $e_0$ 包含所有特征方向上的分量。\n  - 考虑步长 $\\mu_A \\in \\{0.3, 1.0, 1.6\\}$。\n  - 为进行紧凑性测试，定义 $\\mu_{A,\\text{crit}} = \\dfrac{2}{\\lambda_{\\max}(R_A)}$，并测试三元组 $\\{\\mu_{A,\\text{low}}, \\mu_{A,\\text{crit}}, \\mu_{A,\\text{high}}\\} = \\{0.999 \\mu_{A,\\text{crit}}, \\mu_{A,\\text{crit}}, 1.001 \\mu_{A,\\text{crit}}\\}$。\n  - 使用 $N_A = 2000$ 次迭代来运行递归式 $e_{n+1} = (I - \\mu R_A) e_n$，并通过最终误差范数与初始误差范数之比来凭经验判断其收敛性。\n- 情况 B（LMS 算法，维度 $m=3$）：\n  - $R_B = \\begin{bmatrix} 3.0 & 0.1 & 0.0 \\\\ 0.1 & 1.0 & 0.2 \\\\ 0.0 & 0.2 & 0.5 \\end{bmatrix}$。\n  - $w_B^\\star = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 1.5 \\end{bmatrix}$，噪声方差 $\\sigma_v^2 = 0.01$。\n  - 为进行均值分析，初始化 $w_{B,0} = w_B^\\star + \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$，并使用确定性均值递归式 $e_{n+1} = (I - \\mu R_B) e_n$ 进行紧凑性测试。\n  - 考虑步长 $\\mu_B \\in \\{0.3, 0.5, 0.7, 0.9\\}$。\n  - 对于保守的均方充分界，仅用 $R_B$ 将其表示出来；并对每个 $\\mu_B$ 进行数值评估。\n  - 为进行均值界的紧凑性测试，定义 $\\mu_{B,\\text{crit}} = \\dfrac{2}{\\lambda_{\\max}(R_B)}$ 和三元组 $\\{\\mu_{B,\\text{low}}, \\mu_{B,\\text{crit}}, \\mu_{B,\\text{high}}\\} = \\{0.999 \\mu_{B,\\text{crit}}, \\mu_{B,\\text{crit}}, 1.001 \\mu_{B,\\text{crit}}\\}$，并对均值递归式进行 $N_B = 2000$ 次迭代。\n- 情况 C（NLMS 算法，维度 $m=3$）：\n  - 使用与情况 B 中相同的 $R_B$、$w_B^\\star$ 和 $\\sigma_v^2$。\n  - 通过 Cholesky 分解生成协方差为 $R_B$ 的零均值高斯数据 $x(n)$，以及独立的 $v(n) \\sim \\mathcal{N}(0, \\sigma_v^2)$。\n  - 带正则化项 $\\delta = 10^{-6}$ 的 NLMS 更新：\n    $$ w_{n+1} = w_n + \\mu_0 \\frac{e(n) x(n)}{\\delta + \\lVert x(n) \\rVert_2^2}, \\quad e(n) = d(n) - x^\\top(n) w_n. $$\n  - 考虑 $\\mu_{0,C} \\in \\{0.5, 1.5, 2.0, 2.2\\}$。\n  - 为进行紧凑性测试，定义 $\\{\\mu_{0,\\text{low}}, \\mu_{0,\\text{crit}}, \\mu_{0,\\text{high}}\\} = \\{1.999, 2.0, 2.001\\}$。\n  - 使用 $N_C = 1500$ 次迭代，并对 $R_C = 20$ 次独立运行（使用固定随机种子）的均值误差向量进行平均，以凭经验判断该三元组的均值收敛性。\n  - 为保证仿真可复现性，将随机种子设置为 $42$；通过 $x(n) = L z(n)$ 生成 $x(n)$，其中 $L$ 来自 $R_B$ 的 Cholesky 分解，$z(n) \\sim \\mathcal{N}(0, I)$。\n\n经验收敛性分类的判定规则：\n- 对于情况 A 和 B 中的确定性均值递归，如果最后 $K = 50$ 个误差范数的平均值除以初始误差范数所得的比值严格小于 $10^{-2}$，则判定为“收敛”；如果该比值超过 $10^{2}$，则判定为“发散”；否则，为了得到所需的布尔值输出，将其视为不收敛。\n- 对于情况 C 中的 NLMS 蒙特卡洛测试，在每个测试的 $\\mu_0$ 下，计算第 0 次迭代时 $R_C$ 次运行的误差向量样本均值的范数，以及最后 $K=50$ 次迭代的范数的均值；如果后者小于前者的 $10^{-2}$ 倍，则判定为“收敛”。\n\n要求计算和打印的输出：\n- 情况 A（最速下降法）：对于 $\\mu_A = \\{0.3, 1.0, 1.6\\}$，输出三个布尔值，表明每个 $\\mu$ 是否位于从 $R_A$ 推导出的充分必要收敛区间内。然后，为紧凑性测试三元组 $\\{\\mu_{A,\\text{low}}, \\mu_{A,\\text{crit}}, \\mu_{A,\\text{high}}\\}$ 输出三个布尔值，表明确定性递归是否收敛。\n- 情况 B（LMS 算法）：对于 $\\mu_B = \\{0.3, 0.5, 0.7, 0.9\\}$，首先输出四个布尔值，表明每个 $\\mu$ 是否满足推导出的均值稳定性充分必要条件。然后，输出四个布尔值，表明每个 $\\mu$ 是否满足推导出的保守的均方稳定性充分界。接着，为紧凑性测试三元组 $\\{\\mu_{B,\\text{low}}, \\mu_{B,\\text{crit}}, \\mu_{B,\\text{high}}\\}$ 输出三个布尔值，表明确定性均值递归是否收敛。\n- 情况 C（NLMS 算法）：对于 $\\mu_{0,C} = \\{0.5, 1.5, 2.0, 2.2\\}$，输出四个布尔值，表明每个 $\\mu_0$ 是否位于推导出的稳定区间内。然后，为紧凑性测试三元组 $\\{\\mu_{0,\\text{low}}, \\mu_{0,\\text{crit}}, \\mu_{0,\\text{high}}\\}$ 输出三个布尔值，表明多次运行的经验均值收敛性。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表通过按顺序连接上述情况 A、B 和 C 的所有布尔结果构成。具体来说，输出应为一个长度为 24 的列表，严格按照以下顺序包含布尔值：\n  - 情况 A：3 个关于 $\\mu_A$ 区间成员资格的布尔值，然后是 3 个关于紧凑性测试三元组的布尔值。\n  - 情况 B：4 个关于均值稳定性的布尔值，然后是 4 个关于保守均方界的布尔值，接着是 3 个关于紧凑性测试三元组的布尔值。\n  - 情况 C：4 个关于 NLMS 稳定区间的布尔值，然后是 3 个关于紧凑性测试三元组的布尔值。\n\n不涉及物理单位或角度；所有量均为无量纲实数。数值仿真应遵守上面列出的参数，无需任何用户输入。", "solution": "所给出的问题是自适应信号处理领域中一个良构且科学合理的问题。它要求对三种基本的基于梯度的自适应算法——最速下降法、最小均方（LMS）算法和归一化最小均方（NLMS）算法——的收敛界进行推导和数值验证。该问题是自洽的，提供了所有必要的定义、常数和测试条件。所有已知条件均与既有理论一致。我们将着手进行分析。\n\n分析基于线性模型 $d(n) = x^\\top(n) w^\\star + v(n)$，其目标是估计未知向量 $w^\\star \\in \\mathbb{R}^m$。误差向量定义为 $e_n = w_n - w^\\star$，其中 $w_n$ 是第 $n$ 次迭代时的估计值。\n\n**1. 确定性最速下降法的收敛性**\n\n最速下降法通过更新权重向量来最小化均方误差（MSE）代价函数 $J(w)$。问题指出梯度为 $\\nabla J(w_n) = R(w_n - w^\\star)$，其中 $R = \\mathbb{E}\\{x(n)x^\\top(n)\\}$ 是输入信号的正定自相关矩阵。更新递归式为 $w_{n+1} = w_n - \\mu \\nabla J(w_n)$。\n\n代入定义，我们得到权重向量的递归式：\n$$w_{n+1} = w_n - \\mu R(w_n - w^\\star)$$\n从等式两边减去最优向量 $w^\\star$，得到误差向量的递归式：\n$$w_{n+1} - w^\\star = w_n - w^\\star - \\mu R(w_n - w^\\star)$$\n$$e_{n+1} = e_n - \\mu R e_n = (I - \\mu R) e_n$$\n这是一个线性时不变系统。为使误差向量 $e_n$ 在 $n \\to \\infty$ 时收敛到零，其充分必要条件是状态转移矩阵 $I - \\mu R$ 的谱半径 $\\rho(I - \\mu R)$ 严格小于 $1$。\n\n设 $\\lambda_i$ 为 $R$ 的特征值（$i=1, \\dots, m$）。由于 $R$ 是对称正定矩阵，其特征值均为正实数，即 $\\lambda_i > 0$。矩阵 $I - \\mu R$ 的特征值为 $1 - \\mu \\lambda_i$。因此，谱半径条件为：\n$$\\max_i |1 - \\mu \\lambda_i| < 1$$\n此条件必须对所有特征值 $\\lambda_i$ 成立。这一个不等式等价于以下不等式组：\n$$-1 < 1 - \\mu \\lambda_i < 1 \\quad \\forall i$$\n我们据此求解 $\\mu$：\n1.  右侧不等式 $1 - \\mu \\lambda_i < 1$ 意味着 $-\\mu \\lambda_i < 0$。由于要保证更新方向为下降方向，$\\mu$ 必须为正，且 $\\lambda_i > 0$，因此该式可简化为 $\\mu > 0$。\n2.  左侧不等式 $-1 < 1 - \\mu \\lambda_i$ 意味着 $\\mu \\lambda_i < 2$，即 $\\mu < 2/\\lambda_i$。\n\n为使该条件对所有 $i$ 成立，$\\mu$ 必须小于所有 $2/\\lambda_i$ 中的最小值，而该最小值由最大特征值 $\\lambda_{\\max}(R)$ 决定。综合以上条件，最速下降法收敛的充分必要条件是：\n$$0 < \\mu < \\frac{2}{\\lambda_{\\max}(R)}$$\n这个界是紧凑的。如果 $\\mu = 2/\\lambda_{\\max}(R)$，则 $(I - \\mu R)$ 对应于 $\\lambda_{\\max}(R)$ 的特征值为 $1 - (2/\\lambda_{\\max}(R)) \\lambda_{\\max}(R) = -1$。模为 $1$ 的特征值意味着该特征方向上的误差分量不会衰减，从而违反了严格收敛条件。如果 $\\mu > 2/\\lambda_{\\max}(R)$，该特征值的模将超过 $1$，导致误差发散。\n\n**2. 最小均方（LMS）算法的收敛性**\n\nLMS 算法使用梯度的瞬时估计。其权重更新规则为 $w_{n+1} = w_n + \\mu e(n) x(n)$，其中 $e(n) = d(n) - x^\\top(n) w_n$ 是瞬时误差信号。\n\n**2.1. 均值收敛**\n\n为了分析均值收敛，我们研究误差向量的期望 $\\mathbb{E}\\{e_n\\}$。误差递归式为：\n$$e_{n+1} = w_{n+1} - w^\\star = e_n + \\mu (d(n) - x^\\top(n) w_n) x(n)$$\n$$e_{n+1} = e_n + \\mu (x^\\top(n)w^\\star + v(n) - x^\\top(n) w_n) x(n) = e_n - \\mu (x^\\top(n)e_n) x(n) + \\mu v(n) x(n)$$\n取期望并利用独立性假设（即 $w_n$ 与 $x(n)$ 和 $v(n)$ 无关），我们得到：\n$$\\mathbb{E}\\{e_{n+1}\\} = \\mathbb{E}\\{e_n\\} - \\mu \\mathbb{E}\\{x(n)x^\\top(n)e_n\\} + \\mu \\mathbb{E}\\{v(n)x(n)\\}$$\n考虑到 $v(n)$ 是零均值且与 $x(n)$ 无关，有 $\\mathbb{E}\\{v(n)x(n)\\} = \\mathbf{0}$。利用 $e_n$ 和 $x(n)$ 的独立性，有 $\\mathbb{E}\\{x(n)x^\\top(n)e_n\\} = \\mathbb{E}\\{x(n)x^\\top(n)\\} \\mathbb{E}\\{e_n\\} = R \\mathbb{E}\\{e_n\\}$。这给出了均值误差的递归式：\n$$\\mathbb{E}\\{e_{n+1}\\} = (I - \\mu R) \\mathbb{E}\\{e_n\\}$$\n该递归式在形式上与确定性最速下降法的误差递归式完全相同。因此，均值权重向量收敛至 $w^\\star$（即 $\\mathbb{E}\\{e_n\\} \\to \\mathbf{0}$）的充分必要条件也与之相同：\n$$0 < \\mu < \\frac{2}{\\lambda_{\\max}(R)}$$\n\n**2.2. 均方稳定性**\n\n均方稳定性或均方收敛性分析考虑的是 $\\mathbb{E}\\{\\|e_n\\|^2\\}$ 的行为。完整的分析非常复杂。该问题要求一个保守且易于计算的充分界。一个广泛使用的均方收敛充分条件是：\n$$0 < \\mu < \\frac{2}{\\text{tr}(R)}$$\n其中 $\\text{tr}(R) = \\sum_{i=1}^m \\lambda_i$ 是自相关矩阵的迹。这个条件比均值收敛条件更严格（更保守），因为对于正定矩阵，$\\text{tr}(R) \\ge \\lambda_{\\max}(R)$ 恒成立。该界是在某些假设（如高斯数据）下，或通过在推导权误差协方差矩阵的递归式时应用不等式得出的。它的价值在于其简便性，因为计算它无需计算特征值。\n\n**3. 归一化最小均方（NLMS）算法的稳定性**\n\nNLMS 算法通过输入向量的欧几里得范数的平方来对步长进行归一化，实际上是使用了一个时变步长。其更新公式为：\n$$w_{n+1} = w_n + \\mu_0 \\frac{e(n) x(n)}{\\delta + \\lVert x(n) \\rVert_2^2}$$\n其中 $\\mu_0$ 是一个无量纲步长参数，$\\delta > 0$ 是一个小的正则化常数。为了对齐次系统进行稳定性分析而忽略噪声项，误差递归式为：\n$$e_{n+1} = e_n - \\mu_0 \\frac{x^\\top(n) e_n}{\\delta + \\lVert x(n) \\rVert_2^2} x(n) = \\left(I - \\mu_0 \\frac{x(n) x^\\top(n)}{\\delta + \\lVert x(n) \\rVert_2^2}\\right) e_n$$\n为了分析稳定性，我们可以考察误差向量范数的平方如何变化。我们将 $e_n$ 分解为平行于 $x(n)$ 和正交于 $x(n)$ 的分量。设 $P_n = \\frac{x(n)x^\\top(n)}{\\|x(n)\\|_2^2}$ 是到 $x(n)$ 生成空间上的投影矩阵。对于非零的 $x(n)$，近似 $\\delta \\approx 0$，更新式变为 $e_{n+1} = (I - \\mu_0 P_n) e_n$。\n向量 $e_n$ 可以写作 $e_n = e_{n, \\parallel} + e_{n, \\perp}$，其中 $e_{n, \\parallel} = P_n e_n$ 和 $e_{n, \\perp} = (I-P_n) e_n$ 是相互正交的。\n应用更新算子：\n$$e_{n+1} = (I - \\mu_0 P_n)(e_{n, \\parallel} + e_{n, \\perp}) = (I - \\mu_0 P_n)e_{n, \\parallel} + (I - \\mu_0 P_n)e_{n, \\perp}$$\n由于 $P_n e_{n, \\parallel} = e_{n, \\parallel}$ 且 $P_n e_{n, \\perp} = \\mathbf{0}$，我们得到：\n$$e_{n+1} = (1 - \\mu_0) e_{n, \\parallel} + e_{n, \\perp}$$\n其范数的平方为（根据正交性）：\n$$\\|e_{n+1}\\|_2^2 = (1-\\mu_0)^2 \\|e_{n, \\parallel}\\|_2^2 + \\|e_{n, \\perp}\\|_2^2$$\n为使误差减小，我们需要 $\\|e_{n+1}\\|_2^2 < \\|e_n\\|_2^2 = \\|e_{n, \\parallel}\\|_2^2 + \\|e_{n, \\perp}\\|_2^2$。如果 $(1 - \\mu_0)^2 < 1$，该条件成立，而这在 $-1 < 1 - \\mu_0 < 1$ 时为真。这意味着：\n$$0 < \\mu_0 < 2$$\n这是 NLMS 算法的标准稳定范围，是一个充分条件。正则化项 $\\delta > 0$ 的存在确保了在 $\\|x(n)\\|_2^2$ 非常小或为零时更新仍然是良态的，并且不会改变这个基本稳定性界。这个界显著地独立于输入信号的统计特性（即与 $R$ 无关）。紧凑性测试将证实，处于或超出此范围的 $\\mu_0$ 值会导致不收敛或发散。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the adaptive filtering convergence problem, providing theoretical and\n    empirical validation for Steepest Descent, LMS, and NLMS algorithms.\n    \"\"\"\n    results = []\n    \n    # --- Case A: Steepest Descent ---\n    R_A = np.array([[1.2, 0.3], [0.3, 0.8]])\n    w_A_star = np.array([1.0, -2.0])\n    w_A_0 = w_A_star + np.array([1.0, 1.0])\n    e_A_0 = w_A_0 - w_A_star\n    mu_A_values = [0.3, 1.0, 1.6]\n    N_A = 2000\n    K = 50\n\n    # Theoretical bound calculation\n    lambda_max_A = np.linalg.eigvalsh(R_A).max()\n    mu_A_crit = 2.0 / lambda_max_A\n    \n    for mu in mu_A_values:\n        results.append(0  mu  mu_A_crit)\n\n    # Tightness test\n    mu_A_triplet = [0.999 * mu_A_crit, mu_A_crit, 1.001 * mu_A_crit]\n    e_init_norm_A = np.linalg.norm(e_A_0)\n    for mu in mu_A_triplet:\n        e = np.copy(e_A_0)\n        error_norms = np.zeros(N_A)\n        A = np.eye(2) - mu * R_A\n        for n in range(N_A):\n            e = A @ e\n            error_norms[n] = np.linalg.norm(e)\n        \n        final_avg_norm = np.mean(error_norms[-K:])\n        ratio = final_avg_norm / e_init_norm_A if e_init_norm_A > 0 else 0\n        is_convergent = ratio  1e-2\n        results.append(is_convergent)\n\n    # --- Case B: LMS ---\n    R_B = np.array([[3.0, 0.1, 0.0], [0.1, 1.0, 0.2], [0.0, 0.2, 0.5]])\n    w_B_star = np.array([0.5, -1.0, 1.5])\n    w_B_0 = w_B_star + np.array([1.0, 1.0, 1.0])\n    e_B_0 = w_B_0 - w_B_star\n    mu_B_values = [0.3, 0.5, 0.7, 0.9]\n    N_B = 2000\n\n    # Mean convergence theoretical bound\n    lambda_max_B = np.linalg.eigvalsh(R_B).max()\n    mu_B_crit = 2.0 / lambda_max_B\n    for mu in mu_B_values:\n        results.append(0  mu  mu_B_crit)\n        \n    # Conservative mean-square stability bound\n    tr_R_B = np.trace(R_B)\n    mu_ms_bound = 2.0 / tr_R_B\n    for mu in mu_B_values:\n        results.append(0  mu  mu_ms_bound)\n        \n    # Tightness test for mean convergence\n    mu_B_triplet = [0.999 * mu_B_crit, mu_B_crit, 1.001 * mu_B_crit]\n    e_init_norm_B = np.linalg.norm(e_B_0)\n    for mu in mu_B_triplet:\n        e = np.copy(e_B_0)\n        error_norms = np.zeros(N_B)\n        A = np.eye(3) - mu * R_B\n        for n in range(N_B):\n            e = A @ e\n            error_norms[n] = np.linalg.norm(e)\n\n        final_avg_norm = np.mean(error_norms[-K:])\n        ratio = final_avg_norm / e_init_norm_B if e_init_norm_B > 0 else 0\n        is_convergent = ratio  1e-2\n        results.append(is_convergent)\n\n    # --- Case C: NLMS ---\n    m = 3\n    sigma_v_sq = 0.01\n    sigma_v = np.sqrt(sigma_v_sq)\n    delta = 1e-6\n    mu0_C_values = [0.5, 1.5, 2.0, 2.2]\n    N_C = 1500\n    R_C = 20\n    \n    # Theoretical stability bound\n    for mu0 in mu0_C_values:\n        results.append(0  mu0  2)\n\n    # Tightness test\n    mu0_triplet = [1.999, 2.0, 2.001]\n    L_B = np.linalg.cholesky(R_B)\n    rng = np.random.default_rng(42)\n    \n    w_C_0 = w_B_star + np.array([1.0, 1.0, 1.0])\n    e_C_0 = w_C_0 - w_B_star\n\n    for mu0 in mu0_triplet:\n        error_runs = np.zeros((R_C, N_C + 1, m))\n        for r in range(R_C):\n            w = np.copy(w_C_0)\n            error_runs[r, 0, :] = w - w_B_star\n            x_samples = (L_B @ rng.standard_normal(size=(m, N_C))).T\n            v_samples = rng.standard_normal(size=N_C) * sigma_v\n            \n            for n in range(N_C):\n                x = x_samples[n]\n                v = v_samples[n]\n                d = x.T @ w_B_star + v\n                y = x.T @ w\n                e_scalar = d - y\n                w = w + mu0 * e_scalar * x / (delta + np.dot(x, x))\n                error_runs[r, n + 1, :] = w - w_B_star\n\n        mean_error_trajectory = np.mean(error_runs, axis=0)\n        initial_mean_error_norm = np.linalg.norm(mean_error_trajectory[0])\n        final_error_norms = np.linalg.norm(mean_error_trajectory[-K:], axis=1)\n        final_avg_norm = np.mean(final_error_norms)\n        \n        ratio = final_avg_norm / initial_mean_error_norm if initial_mean_error_norm > 0 else 0\n        is_convergent = ratio  1e-2\n        results.append(is_convergent)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2874693"}, {"introduction": "仅仅知道一个算法是否收敛是不够的，我们还需要能够预测其性能。这项综合性练习将指导您设计并执行一个完整的数值实验，以验证LMS和NLMS算法的关键理论，包括稳定性和稳态失调。这个实践过程模拟了研究人员验证理论主张的真实工作流程，是理论联系实际的绝佳训练。[@problem_id:2874688]", "problem": "考虑一个长度为 $M$ 的未知有限脉冲响应（FIR）系统，其系数向量为 $\\mathbf{w}^{\\star} \\in \\mathbb{R}^{M}$。一个零均值、宽平稳的输入信号 $x(n)$ 被施加到该系统。期望信号为 $d(n) = \\mathbf{w}^{\\star \\top}\\mathbf{x}(n) + v(n)$，其中 $\\mathbf{x}(n) \\in \\mathbb{R}^{M}$ 是由最近 $M$ 个输入样本构成的输入回归向量， $v(n)$ 是与 $x(n)$ 无关的零均值测量噪声。任务是设计一个数值实验，以验证使用最小均方（LMS）和归一化最小均方（NLMS）算法的梯度自适应方法的基本收敛理论。\n\n起点（基本基础）：定义均方误差代价函数 $J(\\mathbf{w}) = \\mathbb{E}\\{(d(n) - \\mathbf{w}^{\\top}\\mathbf{x}(n))^{2}\\}$，其梯度为 $\\nabla J(\\mathbf{w})$，Hessian矩阵为 $\\mathbf{H}$。考虑使用正标量步长的最速下降递归，引出LMS的近似方法，以及引出NLMS的归一化方法。使用以下内容指导推导和验证：\n- 最速下降的均值行为由输入自相关矩阵 $\\mathbf{R} = \\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}$ 决定。\n- 均方行为与自适应滤波器分析中的能量守恒和独立性假设相关联。\n- 对于方差为 $\\sigma_{x}^{2}$ 的白噪声输入，矩阵 $\\mathbf{R}$ 为 $\\sigma_{x}^{2}\\mathbf{I}$。\n\n该实验将验证两个理论属性：\n1) LMS和NLMS在允许步长范围方面的均值收敛稳定性。\n2) 小步长条件下的稳态失调。\n\n实现约束和实验协议：\n- 通过抽取独立的标准正态分布项生成一个长度为 $M$ 的未知系统 $\\mathbf{w}^{\\star}$，然后保持其不缩放。使用固定的伪随机种子 $2025$ 以确保 $\\mathbf{w}^{\\star}$、输入 $x(n)$ 和噪声 $v(n)$ 的完全可复现性。\n- 使用方差为 $\\sigma_{x}^{2}$ 的白高斯输入，并添加方差为 $\\sigma_{v}^{2}$ 的独立白高斯测量噪声 $v(n)$。\n- 在 $\\mathbf{w}(0)=\\mathbf{0}$ 处初始化自适应滤波器。\n- 对于LMS，使用递归式 $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu\\,\\mathbf{x}(n)\\,e(n)$，其中 $e(n) = d(n) - \\mathbf{w}^{\\top}(n)\\mathbf{x}(n)$ 且 $\\mu  0$。\n- 对于NLMS，使用递归式 $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\dfrac{\\tilde{\\mu}}{\\epsilon + \\|\\mathbf{x}(n)\\|_{2}^{2}}\\,\\mathbf{x}(n)\\,e(n)$，其中 $\\tilde{\\mu}  0$ 且 $\\epsilon$ 是一个为避免实现中出现除以零而明确引入的小正则化常数。使用 $\\epsilon = 10^{-9}$。\n- 为验证均值收敛稳定性，将算法的经验行为与从均值权重误差动态推导出的理论稳定域进行比较。通过比较仿真中期窗口和后期窗口上 $e^{2}(n)$ 的平均值，来检测经验稳定性和发散性。\n- 为验证失调，通过在稳态下进行时间平均并用 $\\sigma_{v}^{2}$ 进行归一化，从仿真中估计稳态失调 $\\widehat{\\mathcal{M}}$，并与标准小步长和独立性假设下的理论预测失调进行比较。\n\n测试套件：\n对于所有情况，使用 $M = 8$，$\\sigma_{v}^{2} = 0.01$ 和 $N = 20000$ 次迭代。对于白噪声输入，$\\sigma_{x}^{2}$ 按情况指定。回归向量 $\\mathbf{x}(n)$ 由最近的 $M$ 个输入样本构成。对所有情况使用相同的随机种子 $2025$。\n\n定义六个测试用例，每个用例由一个元组指定，其中包括算法、$\\sigma_{x}^{2}$、步长参数以及是否在给定容差下验证失调：\n- 案例 1：LMS 算法，$\\sigma_{x}^{2} = 1.0$，$\\mu = 0.05$，在容差 $\\delta = 0.20$（相对误差）下验证失调。\n- 案例 2：LMS 算法，$\\sigma_{x}^{2} = 1.0$，$\\mu = 1.9$，跳过失调验证；仅检查均值收敛稳定性。\n- 案例 3：LMS 算法，$\\sigma_{x}^{2} = 1.0$，$\\mu = 2.1$，检查算法在均值上是否发散（不稳定性）。\n- 案例 4：NLMS 算法，$\\sigma_{x}^{2} = 1.0$，$\\tilde{\\mu} = 0.5$，在容差 $\\delta = 0.25$（相对误差）下验证失调。\n- 案例 5：NLMS 算法，$\\sigma_{x}^{2} = 1.0$，$\\tilde{\\mu} = 1.9$，跳过失调验证；仅检查均值收敛稳定性。\n- 案例 6：NLMS 算法，$\\sigma_{x}^{2} = 1.0$，$\\tilde{\\mu} = 2.1$，检查算法在均值上是否发散（不稳定性）。\n\n您的程序必须：\n- 对每个测试用例实现仿真，使用上述定义和固定的种子 $2025$。\n- 对于LMS，通过使用白噪声输入的 $\\mathbf{R} = \\sigma_{x}^{2}\\mathbf{I}$ 分析谱半径条件来确定理论均值稳定性。\n- 对于NLMS，通过分析归一化步长条件来确定理论均值稳定性。\n- 通过计算比率 $\\rho = \\overline{e^{2}}_{\\text{late}} / \\overline{e^{2}}_{\\text{mid}}$ 来确定经验稳定性或发散性，其中平均值分别在运行的最后四分之一和中间四分之一上计算。如果 $\\rho \\leq 2$，则声明经验稳定性；如果 $\\rho \\geq 10$ 或权重范数变得过大，则声明经验发散性。否则，声明结果不确定，并视为与理论预测不匹配。\n- 对于失调验证案例，通过对运行的后半部分 $e^{2}(n)$ 进行平均，减去 $\\sigma_{v}^{2}$，再除以 $\\sigma_{v}^{2}$ 来估计稳态失调 $\\widehat{\\mathcal{M}}$，并与理论预测的失调进行比较。如果相对误差最多为给定容差，则声明成功。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含六个案例的结果，格式为方括号内以逗号分隔的列表（例如，\"[True,False,True,True,False,True]\"）。每个条目都是一个布尔值，当且仅当经验结果与该案例的理论预测（在需要时包括失调验证）匹配时为 True，否则为 False。不应打印任何其他文本。", "solution": "问题陈述已经过严格验证，并被认定为有效。它在科学上基于自适应信号处理的既定理论，定义了所有必要的参数和条件，问题提法得当，并且其表述和验证标准是客观的。任务是设计并执行一个数值实验，以验证最小均方（LMS）和归一化最小均方（NLMS）算法的基本收敛性和稳态性能理论。\n\n问题的核心在于将理论预测与单次可复现的仿真运行所得到的经验结果进行比较。该分析的理论基础概述如下。\n\n自适应滤波器试图通过调整其自身的权重向量 $\\mathbf{w}(n) \\in \\mathbb{R}^{M}$ 来对由权重向量 $\\mathbf{w}^{\\star} \\in \\mathbb{R}^{M}$ 表示的未知系统进行建模。目标是最小化均方误差（MSE）代价函数 $J(\\mathbf{w}) = \\mathbb{E}\\{e^{2}(n)\\}$，其中 $e(n) = d(n) - y(n)$ 是误差信号，$d(n)$ 是期望信号，$y(n) = \\mathbf{w}^{\\top}(n)\\mathbf{x}(n)$ 是滤波器输出。对于给定的问题，$d(n) = \\mathbf{w}^{\\star \\top}\\mathbf{x}(n) + v(n)$，其中 $v(n)$ 是方差为 $\\sigma_{v}^{2}$ 的零均值测量噪声。因此，最小均方误差为 $J_{min} = \\mathbb{E}\\{v^{2}(n)\\} = \\sigma_{v}^{2}$。\n\nMSE曲面的梯度为 $\\nabla J(\\mathbf{w}) = -2(\\mathbf{p} - \\mathbf{R}\\mathbf{w})$，其中 $\\mathbf{p} = \\mathbb{E}\\{d(n)\\mathbf{x}(n)\\}$ 且 $\\mathbf{R} = \\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}$ 是输入自相关矩阵。LMS和NLMS算法是基于最速下降法的随机近似的实际实现。\n\n**LMS算法分析**\nLMS算法的更新公式为：\n$$ \\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu\\,\\mathbf{x}(n)\\,e(n) $$\n其中 $\\mu  0$ 是步长参数。\n\n1.  **均值收敛**：均值权重误差向量 $\\mathbb{E}\\{\\tilde{\\mathbf{w}}(n)\\} = \\mathbb{E}\\{\\mathbf{w}(n) - \\mathbf{w}^{\\star}\\}$ 的行为由以下递归方程决定：\n    $$ \\mathbb{E}\\{\\tilde{\\mathbf{w}}(n+1)\\} = (\\mathbf{I} - \\mu \\mathbf{R}) \\mathbb{E}\\{\\tilde{\\mathbf{w}}(n)\\} $$\n    为了使均值权重向量收敛到零，矩阵 $(\\mathbf{I} - \\mu \\mathbf{R})$ 必须是稳定的。这要求其所有特征值的模都小于 $1$。如果 $\\lambda_{i}$ 是 $\\mathbf{R}$ 的特征值，那么 $(\\mathbf{I} - \\mu \\mathbf{R})$ 的特征值为 $(1 - \\mu \\lambda_{i})$。条件 $|1 - \\mu \\lambda_{i}|  1$ 必须对所有 $i$ 成立。这导出了步长的稳定性边界：\n    $$ 0  \\mu  \\frac{2}{\\lambda_{max}} $$\n    其中 $\\lambda_{max}$ 是 $\\mathbf{R}$ 的最大特征值。对于指定的方差为 $\\sigma_{x}^{2}$ 的白高斯输入，自相关矩阵为 $\\mathbf{R} = \\sigma_{x}^{2}\\mathbf{I}$。其所有特征值都等于 $\\sigma_{x}^{2}$。因此，均值收敛的条件简化为：\n    $$ 0  \\mu  \\frac{2}{\\sigma_{x}^{2}} $$\n\n2.  **稳态失调**：在稳态下，滤波器权重在最优解 $\\mathbf{w}^{\\star}$ 附近波动，导致超量均方误差 $J_{ex}(\\infty) = J(\\infty) - J_{min}$。失调 $\\mathcal{M}$ 是该超量MSE与最小MSE之比。对于具有小步长和白噪声输入的LMS算法，理论失调为：\n    $$ \\mathcal{M}_{LMS} = \\frac{J_{ex}(\\infty)}{J_{min}} = \\frac{\\mu \\, \\text{Tr}(\\mathbf{R})}{2} = \\frac{\\mu M \\sigma_{x}^{2}}{2} $$\n    其中 $\\text{Tr}(\\mathbf{R}) = M \\sigma_{x}^{2}$ 是自相关矩阵的迹。\n\n**NLMS算法分析**\nNLMS算法通过输入回归向量的欧几里得范数的平方来归一化步长：\n$$ \\mathbf{w}(n+1) = \\mathbf{w}(n) + \\frac{\\tilde{\\mu}}{\\epsilon + \\|\\mathbf{x}(n)\\|_{2}^{2}}\\,\\mathbf{x}(n)\\,e(n) $$\n其中 $\\tilde{\\mu}$ 是归一化步长，$\\epsilon$ 是用于正则化的小正数。\n\n1.  **均值收敛**：NLMS的一个关键优势是其收敛特性对输入信号功率的依赖性较小。均方收敛（因此也是均值收敛）的一个充分条件由下式给出：\n    $$ 0  \\tilde{\\mu}  2 $$\n    值得注意的是，这个条件与输入信号的统计特性无关。\n\n2.  **稳态失调**：对于NLMS算法，在小步长假设下，对失调的一个常用近似是：\n    $$ \\mathcal{M}_{NLMS} \\approx \\frac{\\tilde{\\mu}}{2} $$\n    这个近似假设对于足够长度 $M$ 的回归向量，项 $\\|\\mathbf{x}(n)\\|_{2}^{2}$ 接近其期望值 $\\mathbb{E}\\{\\|\\mathbf{x}(n)\\|_{2}^{2}\\} = M \\sigma_{x}^{2}$。\n\n**实验协议和验证**\n\n数值实验将对六个指定的测试用例验证这些理论预测。所有案例都使用相同的伪随机种子（$2025$）以确保可复现性。对于每个案例，我们确定理论预测并将其与经验结果进行比较。\n\n-   **经验稳定性**：通过计算比率 $\\rho = \\overline{e^{2}}_{\\text{late}} / \\overline{e^{2}}_{\\text{mid}}$ 来评估，其中平均值分别在仿真运行的最后四分之一和中间四分之一上取。\n    -   理论稳定性预期对应于经验稳定性（$\\rho \\le 2$）。\n    -   理论不稳定性预期对应于经验发散性（$\\rho \\ge 10$ 或权重向量范数的灾难性增长）。\n    -   如果理论预测（稳定/不稳定）与经验观察（稳定/发散）一致，则匹配。\n\n-   **失调验证**：对于相关情况，经验失调从仿真数据中估计为 $\\widehat{\\mathcal{M}}_{emp} = (\\overline{e^{2}}_{ss} - \\sigma_{v}^{2}) / \\sigma_{v}^{2}$，其中平均值在运行的后半部分上取。如果相对误差 $|\\widehat{\\mathcal{M}}_{emp} - \\mathcal{M}_{theory}| / \\mathcal{M}_{theory}$ 在指定的容差 $\\delta$ 之内，则验证成功。\n\n**测试用例分析**\n所有案例均使用 $M=8$，$\\sigma_{v}^{2}=0.01$，$\\sigma_{x}^{2}=1.0$，以及 $N=20000$ 次迭代。\n\n-   **案例 1**：LMS, $\\mu = 0.05$。\n    -   理论稳定性：$0  0.05  2/1.0$。该算法理论上是稳定的。\n    -   理论失调：$\\mathcal{M}_{LMS} = (0.05 \\cdot 8 \\cdot 1.0) / 2 = 0.2$。\n    -   验证：仿真必须是经验稳定的（$\\rho \\le 2$），且估计的失调 $\\widehat{\\mathcal{M}}_{emp}$ 必须满足 $|\\widehat{\\mathcal{M}}_{emp} - 0.2|/0.2 \\le 0.20$。\n\n-   **案例 2**：LMS, $\\mu = 1.9$。\n    -   理论稳定性：$0  1.9  2/1.0$。该算法理论上是稳定的。\n    -   验证：仿真必须是经验稳定的（$\\rho \\le 2$）。\n\n-   **案例 3**：LMS, $\\mu = 2.1$。\n    -   理论稳定性：$2.1$ 不小于 $2/1.0$。该算法理论上是不稳定的。\n    -   验证：仿真必须是经验发散的（$\\rho \\ge 10$ 或权重范数爆炸）。\n\n-   **案例 4**：NLMS, $\\tilde{\\mu} = 0.5$。\n    -   理论稳定性：$0  0.5  2$。该算法理论上是稳定的。\n    -   理论失调：$\\mathcal{M}_{NLMS} \\approx 0.5 / 2 = 0.25$。\n    -   验证：仿真必须是经验稳定的（$\\rho \\le 2$），且估计的失调 $\\widehat{\\mathcal{M}}_{emp}$ 必须满足 $|\\widehat{\\mathcal{M}}_{emp} - 0.25|/0.25 \\le 0.25$。\n\n-   **案例 5**：NLMS, $\\tilde{\\mu} = 1.9$。\n    -   理论稳定性：$0  1.9  2$。该算法理论上是稳定的。\n    -   验证：仿真必须是经验稳定的（$\\rho \\le 2$）。\n\n-   **案例 6**：NLMS, $\\tilde{\\mu} = 2.1$。\n    -   理论稳定性：$2.1$不小于$2$。该算法理论上是不稳定的。\n    -   验证：仿真必须是经验发散的（$\\rho \\ge 10$ 或权重范数爆炸）。\n\n以下代码实现了这一完整的实验验证。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and runs a numerical experiment to validate the convergence theory\n    of LMS and NLMS adaptive filtering algorithms.\n    \"\"\"\n\n    # --- Test Case Definitions ---\n    # Each case specifies the algorithm, parameters, and validation task.\n    test_cases = [\n        {'id': 1, 'algo': 'LMS',  'sigma_x2': 1.0, 'step_size': 0.05, 'val_misadj': True,  'tolerance': 0.20},\n        {'id': 2, 'algo': 'LMS',  'sigma_x2': 1.0, 'step_size': 1.9,  'val_misadj': False, 'tolerance': None},\n        {'id': 3, 'algo': 'LMS',  'sigma_x2': 1.0, 'step_size': 2.1,  'val_misadj': False, 'tolerance': None},\n        {'id': 4, 'algo': 'NLMS', 'sigma_x2': 1.0, 'step_size': 0.5,  'val_misadj': True,  'tolerance': 0.25},\n        {'id': 5, 'algo': 'NLMS', 'sigma_x2': 1.0, 'step_size': 1.9,  'val_misadj': False, 'tolerance': None},\n        {'id': 6, 'algo': 'NLMS', 'sigma_x2': 1.0, 'step_size': 2.1,  'val_misadj': False, 'tolerance': None},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # --- Experiment Setup ---\n        # Fixed parameters for all simulations.\n        M = 8\n        N = 20000\n        sigma_v2 = 0.01\n        epsilon = 1e-9\n        seed = 2025\n\n        # Use a fresh random number generator for each case to ensure it starts from the same state.\n        rng = np.random.default_rng(seed)\n\n        # Generate the unknown system, input signal, and measurement noise.\n        w_star = rng.standard_normal(size=M)\n        \n        # We need N regressors, each of length M. This requires N+M-1 samples of x(n).\n        x_signal = rng.normal(loc=0.0, scale=np.sqrt(case['sigma_x2']), size=N + M - 1)\n        v_noise = rng.normal(loc=0.0, scale=np.sqrt(sigma_v2), size=N)\n\n        # --- Run Simulation ---\n        w = np.zeros(M)\n        e_sq_history = np.zeros(N)\n        instantaneous_divergence = False\n\n        for n in range(N):\n            # Form regressor vector x(n) = [x(n), x(n-1), ..., x(n-M+1)]^T\n            # Array slice x_signal[n : n+M] corresponds to samples for times n-(M-1) to n\n            # if we map time t to index t+M-1. We need [x(n), ..., x(n-M+1)],\n            # which in the data array is [x_signal[n+M-1], ..., x_signal[n]].\n            x_vec = x_signal[n : n + M][::-1]\n\n            # Calculate desired signal d(n)\n            d_n = np.dot(w_star, x_vec) + v_noise[n]\n\n            # Calculate error e(n) and store its square\n            e_n = d_n - np.dot(w, x_vec)\n            e_sq_history[n] = e_n**2\n\n            # Update weights based on the algorithm specified in the test case\n            if case['algo'] == 'LMS':\n                mu = case['step_size']\n                w = w + mu * x_vec * e_n\n            elif case['algo'] == 'NLMS':\n                tilde_mu = case['step_size']\n                x_norm_sq = np.dot(x_vec, x_vec)\n                w = w + (tilde_mu / (epsilon + x_norm_sq)) * x_vec * e_n\n\n            # Check for catastrophic divergence to halt simulation early\n            if np.linalg.norm(w) > 1e6:\n                instantaneous_divergence = True\n                break\n\n        # --- Analysis and Validation ---\n        case_result = False\n\n        # Determine theoretical stability based on standard convergence conditions\n        is_theoretically_stable = False\n        if case['algo'] == 'LMS':\n            mu = case['step_size']\n            sigma_x2 = case['sigma_x2']\n            if 0  mu  2 / sigma_x2:\n                is_theoretically_stable = True\n        elif case['algo'] == 'NLMS':\n            tilde_mu = case['step_size']\n            if 0  tilde_mu  2:\n                is_theoretically_stable = True\n\n        # Determine empirical stability from simulation results\n        is_empirically_stable = None  # None indicates an inconclusive result\n        if instantaneous_divergence:\n            is_empirically_stable = False\n        else:\n            N_quarter = N // 4\n            mid_window_mean_sq_err = np.mean(e_sq_history[N_quarter : 2 * N_quarter])\n            late_window_mean_sq_err = np.mean(e_sq_history[3 * N_quarter : 4 * N_quarter])\n            \n            # Avoid division by zero if error is effectively zero\n            if mid_window_mean_sq_err > 1e-12:\n                rho = late_window_mean_sq_err / mid_window_mean_sq_err\n                if rho = 2.0:\n                    is_empirically_stable = True\n                elif rho >= 10.0:\n                    is_empirically_stable = False\n            else: # If mid error is zero, and late is non-zero, it must have diverged.\n                is_empirically_stable = False if late_window_mean_sq_err > 1e-12 else True\n        \n        # Check if the theoretical prediction matches the empirical observation\n        stability_match = (is_theoretically_stable and is_empirically_stable is True) or \\\n                          (not is_theoretically_stable and is_empirically_stable is False)\n\n        if case['val_misadj']:\n            if stability_match and is_theoretically_stable:\n                # Proceed to misadjustment validation only if stable\n                m_theory = 0.0\n                if case['algo'] == 'LMS':\n                    mu = case['step_size']\n                    sigma_x2 = case['sigma_x2']\n                    m_theory = (mu * M * sigma_x2) / 2\n                elif case['algo'] == 'NLMS':\n                    tilde_mu = case['step_size']\n                    m_theory = tilde_mu / 2\n\n                # Estimate empirical misadjustment from the steady-state portion of the run\n                steady_state_err_sq = np.mean(e_sq_history[N // 2:])\n                m_emp = (steady_state_err_sq - sigma_v2) / sigma_v2\n\n                # Check if relative error is within the specified tolerance\n                if m_theory > 1e-9:\n                    rel_error = np.abs(m_emp - m_theory) / m_theory\n                    if rel_error = case['tolerance']:\n                        case_result = True\n        else:  # This is a stability-only check case\n            if stability_match:\n                case_result = True\n        \n        results.append(case_result)\n\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2874688"}]}