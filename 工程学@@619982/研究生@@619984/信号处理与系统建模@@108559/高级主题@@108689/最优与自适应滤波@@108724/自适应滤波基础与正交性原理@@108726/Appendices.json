{"hands_on_practices": [{"introduction": "本练习将抽象的正交性原理应用于一个具体的计算。通过求解给定相关矩阵 $\\mathbf{R}$ 和互相关向量 $\\mathbf{p}$ 下的最优滤波器权重 $\\mathbf{w}^{\\star}$ 及最小均方误差 $J_{\\min}$，您将直接应用维纳-霍普夫方程。该方程是线性估计问题中正交性原理的数学体现，是理解最优滤波的基石。[@problem_id:2850274]", "problem": "考虑一个估计问题，目标是利用一个二维回归量 $\\mathbf{x} \\in \\mathbb{R}^{2}$ 对一个标量期望随机变量 $d$ 进行线性估计：$y = \\mathbf{w}^{\\top}\\mathbf{x}$。其中，$d$ 和 $\\mathbf{x}$ 均为实值、零均值，且联合广义平稳（WSS）。令均方误差（MSE）代价定义为\n$$\nJ(\\mathbf{w}) \\triangleq \\mathbb{E}\\!\\left[(d - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\right],\n$$\n并分别定义输入相关矩阵、互相关向量和期望功率为\n$$\n\\mathbf{R} \\triangleq \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}], \\quad \\mathbf{p} \\triangleq \\mathbb{E}[\\mathbf{x}d], \\quad \\sigma_{d}^{2} \\triangleq \\mathbb{E}[d^{2}],\n$$\n仅使用基本定义和正交性原理，推导最优 Wiener 解 $\\mathbf{w}^{\\star}$ 必须满足的正规方程，并根据给定的\n$$\n\\mathbf{R}=\\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix}, \\qquad \\mathbf{p}=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix},\n$$\n显式求解 $\\mathbf{w}^{\\star}$，并以 $\\sigma_{d}^{2}$ 的函数形式，计算可达到的最小均方误差\n$$\nJ_{\\min} \\triangleq \\min_{\\mathbf{w}} J(\\mathbf{w}),\n$$\n的闭式解。请以单行的形式给出最终答案，包含 $\\mathbf{w}^{\\star}$ 的两个分量和 $J_{\\min}$，形式为精确值，无需四舍五入。答案无单位，也不涉及角度单位。", "solution": "在尝试进行任何求解之前，我们首先对问题进行了严格的验证。\n\n问题给出的条件逐字摘录如下：\n- 一个实值、零均值的标量随机变量 $d$。\n- 一个实值、零均值的二维随机向量 $\\mathbf{x} \\in \\mathbb{R}^{2}$。\n- 该对 $(d, \\mathbf{x})$ 是联合广义平稳（WSS）的。\n- 线性估计器为 $y = \\mathbf{w}^{\\top}\\mathbf{x}$。\n- 均方误差（MSE）代价函数为 $J(\\mathbf{w}) \\triangleq \\mathbb{E}\\!\\left[(d - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\right]$。\n- 输入相关矩阵为 $\\mathbf{R} \\triangleq \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$。\n- 互相关向量为 $\\mathbf{p} \\triangleq \\mathbb{E}[\\mathbf{x}d]$。\n- 期望信号的功率为 $\\sigma_{d}^{2} \\triangleq \\mathbb{E}[d^{2}]$。\n- 相关矩阵和互相关向量的具体值为：\n$$\n\\mathbf{R}=\\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix}, \\qquad \\mathbf{p}=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\n$$\n- 目标是推导正规方程，找到最优 Wiener 解 $\\mathbf{w}^{\\star}$，并计算最小均方误差 $J_{\\min} \\triangleq \\min_{\\mathbf{w}} J(\\mathbf{w})$。\n\n对问题的有效性进行了评估。这是一个线性估计理论中的标准问题，特别是 Wiener 滤波。所有术语都定义明确，且其背景牢固地植根于信号处理领域。给定的相关矩阵 $\\mathbf{R}$ 是对称的，其行列式为 $\\det(\\mathbf{R}) = (2)(2) - (1)(1) = 3 \\neq 0$，这确保了 $\\mathbf{R}$ 是可逆的，并且最优权重向量 $\\mathbf{w}^{\\star}$ 存在唯一解。该问题是自洽的、有科学依据的、且是适定的。因此，该问题被认为是有效的。我们开始进行求解。\n\n均方误差由 $J(\\mathbf{w}) = \\mathbb{E}[(d - \\mathbf{w}^{\\top}\\mathbf{x})^2]$ 给出。我们寻求最小化此代价函数的最优权重向量 $\\mathbf{w}^{\\star}$。估计误差定义为 $e = d - y = d - \\mathbf{w}^{\\top}\\mathbf{x}$。对于最优滤波器 $\\mathbf{w} = \\mathbf{w}^{\\star}$，正交性原理指出，估计误差 $e^{\\star} = d - (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x}$ 必须与输入向量 $\\mathbf{x}$ 正交。在数学上，这表示为误差与输入向量之积的期望值为零。\n$$\n\\mathbb{E}[\\mathbf{x} e^{\\star}] = \\mathbf{0}\n$$\n代入误差 $e^{\\star}$ 的表达式：\n$$\n\\mathbb{E}[\\mathbf{x} (d - (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x})] = \\mathbf{0}\n$$\n利用期望算子的线性性质：\n$$\n\\mathbb{E}[\\mathbf{x}d] - \\mathbb{E}[\\mathbf{x} (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x}] = \\mathbf{0}\n$$\n项 $(\\mathbf{w}^{\\star})^{\\top}\\mathbf{x}$ 是一个标量，因此可以重新排序：$\\mathbf{x} (\\mathbf{w}^{\\star})^{\\top}\\mathbf{x} = \\mathbf{x} \\mathbf{x}^{\\top} \\mathbf{w}^{\\star}$。对于期望而言，权重向量 $\\mathbf{w}^{\\star}$ 是确定性的。\n$$\n\\mathbb{E}[\\mathbf{x}d] - \\mathbb{E}[\\mathbf{x} \\mathbf{x}^{\\top}] \\mathbf{w}^{\\star} = \\mathbf{0}\n$$\n使用所提供的定义 $\\mathbf{p} = \\mathbb{E}[\\mathbf{x}d]$ 和 $\\mathbf{R} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$，我们得到正规方程，在本离散情况下也称为 Wiener-Hopf 方程：\n$$\n\\mathbf{p} - \\mathbf{R} \\mathbf{w}^{\\star} = \\mathbf{0}\n$$\n$$\n\\mathbf{R} \\mathbf{w}^{\\star} = \\mathbf{p}\n$$\n这就是最优 Wiener 解 $\\mathbf{w}^{\\star}$ 必须满足的线性方程组。\n\n为了找到 $\\mathbf{w}^{\\star}$ 的显式解，我们必须解这个方程组。由于 $\\mathbf{R}$ 是可逆的，唯一解由下式给出：\n$$\n\\mathbf{w}^{\\star} = \\mathbf{R}^{-1} \\mathbf{p}\n$$\n我们给定的矩阵是：\n$$\n\\mathbf{R}=\\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix}, \\qquad \\mathbf{p}=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\n$$\n首先，我们计算 $\\mathbf{R}$ 的逆。对于一个 $2 \\times 2$ 矩阵 $\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}$，其逆为 $\\frac{1}{ad-bc}\\begin{bmatrix}d & -b \\\\ -c & a\\end{bmatrix}$。$\\mathbf{R}$ 的行列式是 $\\det(\\mathbf{R}) = (2)(2) - (1)(1) = 4 - 1 = 3$。\n因此，逆矩阵为：\n$$\n\\mathbf{R}^{-1} = \\frac{1}{3}\\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix}\n$$\n现在我们可以计算 $\\mathbf{w}^{\\star}$：\n$$\n\\mathbf{w}^{\\star} = \\frac{1}{3}\\begin{bmatrix}2 & -1 \\\\ -1 & 2\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix}(2)(1) + (-1)(0) \\\\ (-1)(1) + (2)(0)\\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix}2 \\\\ -1\\end{bmatrix} = \\begin{bmatrix}2/3 \\\\ -1/3\\end{bmatrix}\n$$\n所以，最优权重向量的分量是 $w_1^{\\star} = 2/3$ 和 $w_2^{\\star} = -1/3$。\n\n接下来，我们计算可达到的最小均方误差 $J_{\\min} = J(\\mathbf{w}^{\\star})$。我们从 $J(\\mathbf{w})$ 的定义开始，并将其展开：\n$$\nJ(\\mathbf{w}) = \\mathbb{E}[(d - \\mathbf{w}^{\\top}\\mathbf{x})^2] = \\mathbb{E}[d^2 - 2d\\mathbf{w}^{\\top}\\mathbf{x} + (\\mathbf{w}^{\\top}\\mathbf{x})^2]\n$$\n根据期望的线性性质：\n$$\nJ(\\mathbf{w}) = \\mathbb{E}[d^2] - 2\\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}d] + \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\mathbf{w} = \\sigma_{d}^{2} - 2\\mathbf{w}^{\\top}\\mathbf{p} + \\mathbf{w}^{\\top}\\mathbf{R}\\mathbf{w}\n$$\n为了求得 $J_{\\min}$，我们代入 $\\mathbf{w} = \\mathbf{w}^{\\star}$：\n$$\nJ_{\\min} = \\sigma_{d}^{2} - 2(\\mathbf{w}^{\\star})^{\\top}\\mathbf{p} + (\\mathbf{w}^{\\star})^{\\top}\\mathbf{R}\\mathbf{w}^{\\star}\n$$\n从正规方程可知 $\\mathbf{R}\\mathbf{w}^{\\star} = \\mathbf{p}$。将此代入最后一项：\n$$\nJ_{\\min} = \\sigma_{d}^{2} - 2(\\mathbf{w}^{\\star})^{\\top}\\mathbf{p} + (\\mathbf{w}^{\\star})^{\\top}\\mathbf{p}\n$$\n这可以简化为最小均方误差的一般表达式：\n$$\nJ_{\\min} = \\sigma_{d}^{2} - (\\mathbf{w}^{\\star})^{\\top}\\mathbf{p}\n$$\n现在，我们代入我们求得的 $\\mathbf{w}^{\\star}$ 的数值和给定的 $\\mathbf{p}$：\n$$\n(\\mathbf{w}^{\\star})^{\\top}\\mathbf{p} = \\begin{bmatrix}2/3 & -1/3\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = (2/3)(1) + (-1/3)(0) = 2/3\n$$\n因此，最小均方误差为：\n$$\nJ_{\\min} = \\sigma_{d}^{2} - \\frac{2}{3}\n$$\n最终答案包含 $\\mathbf{w}^{\\star}$ 的两个分量和 $J_{\\min}$ 的表达式。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{3} & -\\frac{1}{3} & \\sigma_{d}^{2} - \\frac{2}{3} \\end{pmatrix}}\n$$", "id": "2850274"}, {"introduction": "维纳滤波器提供了静态的最优解，但许多现实世界的应用要求算法能够实时自适应。本练习让您亲手操作递归最小二乘（RLS）算法，通过完成一个完整的单步更新，您将清晰地看到滤波器权重如何根据新数据进行演化。这有助于将理论与自适应系统在实践中的动态行为联系起来。[@problem_id:2850229]", "problem": "考虑一个阶为 $M=2$ 的实数、时变、线性有限冲激响应自适应滤波器，它通过带有遗忘因子 $\\lambda$ 的递归最小二乘 (RLS) 方法进行更新。在时间 $n$，RLS 方法旨在寻找一个权重向量 $\\mathbf{w}(n) \\in \\mathbb{R}^{2}$，以最小化指数加权最小二乘代价函数\n$$\nJ(n) \\triangleq \\sum_{i=1}^{n} \\lambda^{n-i} \\left(d(i) - \\mathbf{u}^{\\top}(i)\\,\\mathbf{w}\\right)^{2},\n$$\n其中 $\\mathbf{u}(i) \\in \\mathbb{R}^{2}$ 是回归量， $d(i) \\in \\mathbb{R}$ 是期望响应。该最小化器满足由 $\\lambda$ 导出的加权内积的正交性原理，并且更新过程使用逆加权输入相关矩阵 $\\mathbf{P}(n) \\in \\mathbb{R}^{2 \\times 2}$。\n\n在时间 $n$，给定：\n- $\\lambda = 1$,\n- $\\mathbf{u}(n) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$,\n- $d(n) = 2$,\n- $\\mathbf{P}(n-1) = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$,\n- $\\mathbf{w}(n-1) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n\n使用与指数加权最小二乘准则和正交性原理一致的标准 RLS 递归，在时间 $n$ 执行一个更新步骤，以获得更新后的权重向量 $\\mathbf{w}(n)$、更新后的逆相关矩阵 $\\mathbf{P}(n)$ 以及后验误差 $e_{\\mathrm{po}}(n) \\triangleq d(n) - \\mathbf{u}^{\\top}(n)\\,\\mathbf{w}(n)$。\n\n将您的最终答案表示为一个单行矩阵，其中依次包含 $\\mathbf{w}(n)$ 的两个分量、$\\mathbf{P}(n)$ 按行主序排列的四个元素以及 $e_{\\mathrm{po}}(n)$。请使用精确的有理数值，不要进行四舍五入。", "solution": "首先将对问题的科学性和形式正确性进行验证。\n\n### 步骤 1：提取给定条件\n该问题为递归最小二乘 (RLS) 自适应滤波器在时间步长 $n$ 的更新提供了以下明确数据和定义：\n- 滤波器阶数：$M=2$\n- 遗忘因子：$\\lambda = 1$\n- 时间 $n$ 的输入回归向量：$\\mathbf{u}(n) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n- 时间 $n$ 的期望响应：$d(n) = 2$\n- 时间 $n-1$ 的逆相关矩阵：$\\mathbf{P}(n-1) = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n- 时间 $n-1$ 的权重向量：$\\mathbf{w}(n-1) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n- 待最小化的代价函数：$J(n) = \\sum_{i=1}^{n} \\lambda^{n-i} (d(i) - \\mathbf{u}^{\\top}(i)\\,\\mathbf{w})^{2}$\n- 后验误差定义：$e_{\\mathrm{po}}(n) = d(n) - \\mathbf{u}^{\\top}(n)\\,\\mathbf{w}(n)$\n\n### 步骤 2：使用提取的给定条件进行验证\n根据既定标准对问题进行验证。\n- **科学基础**：该问题描述了递归最小二乘算法的一个标准应用，这是信号处理领域自适应滤波理论的基石。RLS 方程源于最小化加权最小二乘误差准则的原理，这是一个在数学上严谨且在科学上有效的过程。该问题不含任何伪科学或事实性错误。\n- **适定性**：该问题提供了一套完整的初始条件（$\\mathbf{w}(n-1)$，$\\mathbf{P}(n-1)$）和新数据（$\\mathbf{u}(n)$，$d(n)$），这些是执行 RLS 算法单次、唯一更新所必需的。所指定的操作在计算上是明确定义的。\n- **客观性**：该问题使用信号处理领域的标准符号，以精确的数学语言陈述。它没有任何主观、模糊或基于观点的陈述。\n- **其他缺陷**：该问题设置并非不完整、矛盾、不切实际、不适定、微不足道或无法验证。这是一个应用已知算法的典型练习。\n\n### 步骤 3：结论与行动\n该问题被判定为**有效**。这是一个定义明确且自洽的工程数学问题。将提供一个解答。\n\nRLS 算法提供了一种递归方法，用于找到最小化指数加权最小二乘代价函数 $J(n)$ 的权重向量 $\\mathbf{w}(n)$。时间步长 $n$ 的标准更新过程包括以下计算序列。\n\n1.  **计算增益向量 $\\mathbf{k}(n)$**：\n    增益向量决定了对权重向量进行校正的方向和幅度。它的计算公式为：\n    $$\n    \\mathbf{k}(n) = \\frac{\\mathbf{P}(n-1)\\mathbf{u}(n)}{\\lambda + \\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1)\\mathbf{u}(n)}\n    $$\n    首先，我们计算分子 $\\mathbf{P}(n-1)\\mathbf{u}(n)$：\n    $$\n    \\mathbf{P}(n-1)\\mathbf{u}(n) = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (2)(1) + (0)(1) \\\\ (0)(1) + (1)(1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n    $$\n    接着，我们计算分母中的项 $\\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1)\\mathbf{u}(n)$：\n    $$\n    \\mathbf{u}^{\\top}(n)[\\mathbf{P}(n-1)\\mathbf{u}(n)] = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = (1)(2) + (1)(1) = 3\n    $$\n    分母为 $\\lambda + 3 = 1 + 3 = 4$。\n    因此，增益向量为：\n    $$\n    \\mathbf{k}(n) = \\frac{1}{4} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}\n    $$\n\n2.  **计算先验估计误差 $e_{\\mathrm{pr}}(n)$**：\n    该误差是期望响应 $d(n)$ 与权重更新前使用前一权重向量 $\\mathbf{w}(n-1)$ 的滤波器输出之间的差值。\n    $$\n    e_{\\mathrm{pr}}(n) = d(n) - \\mathbf{u}^{\\top}(n)\\mathbf{w}(n-1)\n    $$\n    $$\n    e_{\\mathrm{pr}}(n) = 2 - \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 2 - ((1)(1) + (1)(-1)) = 2 - 0 = 2\n    $$\n\n3.  **更新滤波器权重向量以获得 $\\mathbf{w}(n)$**：\n    新的权重向量是通过将一个与先验误差和增益向量成比例的校正项加到旧的权重向量上来计算的。\n    $$\n    \\mathbf{w}(n) = \\mathbf{w}(n-1) + \\mathbf{k}(n) e_{\\mathrm{pr}}(n)\n    $$\n    $$\n    \\mathbf{w}(n) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} (2) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1+1 \\\\ -1+\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\end{pmatrix}\n    $$\n\n4.  **更新逆相关矩阵以获得 $\\mathbf{P}(n)$**：\n    由矩阵求逆引理推导出的 $\\mathbf{P}(n)$ 的更新规则是：\n    $$\n    \\mathbf{P}(n) = \\frac{1}{\\lambda} \\left[ \\mathbf{P}(n-1) - \\mathbf{k}(n)\\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1) \\right]\n    $$\n    因为 $\\lambda = 1$，方程简化为 $\\mathbf{P}(n) = \\mathbf{P}(n-1) - \\mathbf{k}(n)\\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1)$。首先，我们计算项 $\\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1)$：\n    $$\n    \\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1) = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} (1)(2)+(1)(0) & (1)(0)+(1)(1) \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\end{pmatrix}\n    $$\n    接着，我们计算外积 $\\mathbf{k}(n)[\\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1)]$：\n    $$\n    \\mathbf{k}(n)\\mathbf{u}^{\\top}(n)\\mathbf{P}(n-1) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\end{pmatrix} = \\begin{pmatrix} (\\frac{1}{2})(2) & (\\frac{1}{2})(1) \\\\ (\\frac{1}{4})(2) & (\\frac{1}{4})(1) \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{4} \\end{pmatrix}\n    $$\n    最后，我们更新 $\\mathbf{P}(n)$：\n    $$\n    \\mathbf{P}(n) = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 1 & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 2-1 & 0-\\frac{1}{2} \\\\ 0-\\frac{1}{2} & 1-\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1 & -\\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{3}{4} \\end{pmatrix}\n    $$\n\n5.  **计算后验估计误差 $e_{\\mathrm{po}}(n)$**：\n    这是使用更新后权重向量 $\\mathbf{w}(n)$ 得到的最终误差。\n    $$\n    e_{\\mathrm{po}}(n) = d(n) - \\mathbf{u}^{\\top}(n)\\mathbf{w}(n)\n    $$\n    $$\n    e_{\\mathrm{po}}(n) = 2 - \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\end{pmatrix} = 2 - \\left((1)(2) + (1)\\left(-\\frac{1}{2}\\right)\\right) = 2 - \\left(2 - \\frac{1}{2}\\right) = 2 - \\frac{3}{2} = \\frac{1}{2}\n    $$\n    另外，后验误差也可以由先验误差计算得出：$e_{\\mathrm{po}}(n) = (1 - \\mathbf{u}^{\\top}(n)\\mathbf{k}(n))e_{\\mathrm{pr}}(n)$。我们来验证一下：\n    $$\n    \\mathbf{u}^{\\top}(n)\\mathbf{k}(n) = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}\n    $$\n    $$\n    e_{\\mathrm{po}}(n) = \\left(1 - \\frac{3}{4}\\right)(2) = \\left(\\frac{1}{4}\\right)(2) = \\frac{1}{2}\n    $$\n    结果是一致的。\n\n所需的输出是 $\\mathbf{w}(n)$ 的分量、$\\mathbf{P}(n)$ 按行主序排列的元素以及 $e_{\\mathrm{po}}(n)$ 的值。\n- $\\mathbf{w}(n) = \\begin{pmatrix} w_1(n) \\\\ w_2(n) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\end{pmatrix}$\n- $\\mathbf{P}(n) = \\begin{pmatrix} p_{11}(n) & p_{12}(n) \\\\ p_{21}(n) & p_{22}(n) \\end{pmatrix} = \\begin{pmatrix} 1 & -\\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{3}{4} \\end{pmatrix}$\n- $e_{\\mathrm{po}}(n) = \\frac{1}{2}$\n\n这些按要求被组合成一个单行矩阵。", "answer": "$$\n\\boxed{\\begin{pmatrix} 2 & -\\frac{1}{2} & 1 & -\\frac{1}{2} & -\\frac{1}{2} & \\frac{3}{4} & \\frac{1}{2} \\end{pmatrix}}\n$$", "id": "2850229"}, {"introduction": "自适应算法的选择常常涉及计算复杂性与性能（特别是收敛速度）之间的权衡。本练习深入分析了广泛使用的最小均方（LMS）算法，揭示了其收敛速度如何与输入信号的统计特性（由相关矩阵的条件数 $\\kappa$ 量化）紧密相关。理解这种依赖关系对于诊断性能问题和在不同场景下选择合适的算法至关重要。[@problem_id:2850237]", "problem": "考虑一个线性系统辨识问题，其中未知向量为 $w^{\\star} \\in \\mathbb{R}^{m}$，该系统由一个零均值、宽平稳的输入 $x(n) \\in \\mathbb{R}^{m}$ 驱动，其相关矩阵 $R \\triangleq \\mathbb{E}\\{x(n)x^{\\top}(n)\\}$ 是正定的。期望响应为 $d(n) = x^{\\top}(n) w^{\\star} + v(n)$，其中 $v(n)$ 是零均值、白噪声且与 $x(n)$ 无关。最小均方 (LMS) 算法根据 $w(n+1) = w(n) + \\mu\\, x(n)\\, e(n)$ 更新 $w(n)$，其中 $e(n) \\triangleq d(n) - x^{\\top}(n) w(n)$。设步长选择为 $\\mu = \\beta / \\lambda_{\\max}$，其中 $\\beta \\in (0,2)$ 是一个固定值，$\\lambda_{\\max}$ 是 $R$ 的最大特征值。$R$ 的条件数为 $\\kappa \\triangleq \\lambda_{\\max} / \\lambda_{\\min}$，其中 $\\lambda_{\\min}$ 是最小特征值。\n\n仅使用基本定义（包括正交性原理，即维纳解 $w^{\\star}$ 使最优误差与回归量正交）以及标准的小步长和独立性假设，从第一性原理出发，推导瞬态均方偏差如何沿 $R$ 的特征方向衰减。定义一个目标均方偏差衰减因子 $\\rho \\in (0,1)$，该衰减应在瞬态过程（在由梯度噪声主导的稳态平台之前）中实现，并令 $n(\\kappa,\\rho,\\beta)$表示在上述步长规则下，当输入条件数为 $\\kappa$ 时，达到此衰减所需的迭代次数。\n\n对于“多多少次迭代”，将问题解释为乘性因子 $F(\\kappa,\\beta) \\triangleq \\dfrac{n(\\kappa,\\rho,\\beta)}{n(1,\\rho,\\beta)}$，它是当从 $\\kappa = 1$（白噪声输入）变为具有一般条件数 $\\kappa$ 的有色输入时，为达到相同的固定衰减因子 $\\rho$ 所需迭代次数的乘性因子。在小 $\\beta$ 的情况下，以闭合形式估计 $F(\\kappa,\\beta)$，然后计算其在 $\\kappa = 100$ 时的值。最终答案以一个无单位的数字形式给出。无需四舍五入。", "solution": "首先必须验证问题陈述的科学合理性、一致性和完整性。\n\n步骤1：提取已知条件。\n-   未知线性系统：一个向量 $w^{\\star} \\in \\mathbb{R}^{m}$。\n-   输入信号：$x(n) \\in \\mathbb{R}^{m}$，一个零均值、宽平稳 (WSS) 过程。\n-   输入相关矩阵：$R \\triangleq \\mathbb{E}\\{x(n)x^{\\top}(n)\\}$，该矩阵是正定的。\n-   $R$ 的特征值：$\\lambda_{\\max}$ (最大) 和 $\\lambda_{\\min}$ (最小)。\n-   期望响应：$d(n) = x^{\\top}(n) w^{\\star} + v(n)$。\n-   噪声信号：$v(n)$，一个零均值、白噪声过程，且与 $x(n)$ 无关。\n-   LMS 算法：$w(n+1) = w(n) + \\mu\\, x(n)\\, e(n)$。\n-   估计误差：$e(n) = d(n) - x^{\\top}(n) w(n)$。\n-   步长：$\\mu = \\beta / \\lambda_{\\max}$，其中常数 $\\beta \\in (0,2)$。\n-   条件数：$\\kappa \\triangleq \\lambda_{\\max} / \\lambda_{\\min}$。\n-   假设：小步长和独立性假设。独立性假设是指 LMS 分析中的标准简化，即假定输入向量 $x(n)$ 与过去的权向量 $\\{w(k) | k \\leq n\\}$ 在统计上是独立的。\n-   目标：推导瞬态均方偏差沿特征方向的衰减规律，定义迭代次数的乘性因子 $F(\\kappa,\\beta) \\triangleq n(\\kappa,\\rho,\\beta)/n(1,\\rho,\\beta)$，在小 $\\beta$ 的情况下估计该因子，并计算其在 $\\kappa=100$ 时的值。\n\n步骤2：使用提取的已知条件进行验证。\n该问题是 LMS 自适应算法分析中的一个经典练习，LMS 算法是信号处理的基石之一。所有定义和模型都是标准的。参数定义明确且一致。假设（小步长、独立性）已明确说明，并且是该算法动态学入门分析中的标准假设。目标明确，并要求基于自适应滤波器理论的基本原理进行推导。该问题具有科学依据，提法得当，客观，并且没有明显的缺陷。\n\n步骤3：结论与行动。\n问题有效。将构建一个严谨的解法。\n\n分析从定义权误差向量 $\\tilde{w}(n) \\triangleq w(n) - w^{\\star}$ 开始。我们推导其动态特性。\n将 $e(n)$ 和 $d(n)$ 的定义代入 LMS 更新规则中：\n$$\nw(n+1) = w(n) + \\mu x(n) [ (x^{\\top}(n) w^{\\star} + v(n)) - x^{\\top}(n) w(n) ]\n$$\n$$\nw(n+1) = w(n) + \\mu x(n) [ v(n) - x^{\\top}(n) (w(n) - w^{\\star}) ]\n$$\n从两侧减去最优权向量 $w^{\\star}$，得到权误差向量的更新方程：\n$$\n\\tilde{w}(n+1) = \\tilde{w}(n) - \\mu x(n) x^{\\top}(n) \\tilde{w}(n) + \\mu x(n) v(n)\n$$\n为分析瞬态行为，我们研究权误差相关矩阵 $K(n) \\triangleq \\mathbb{E}\\{\\tilde{w}(n) \\tilde{w}^{\\top}(n)\\}$ 的演化。我们对误差向量更新方程与其自身的外积求期望。由于 $v(n)$ 是零均值且与时刻 $n$ 及之前的所有其他信号无关，涉及 $v(n)$ 的交叉项在求期望后会消失。\n$$\nK(n+1) = \\mathbb{E}\\{ [ \\tilde{w}(n) - \\mu x(n) x^{\\top}(n) \\tilde{w}(n) ] [ \\tilde{w}(n) - \\mu x(n) x^{\\top}(n) \\tilde{w}(n) ]^{\\top} \\} + \\mu^2 \\mathbb{E}\\{ v^2(n) x(n) x^{\\top}(n) \\}\n$$\n第二项是梯度噪声，它决定了稳态均方误差：$\\mu^2 \\sigma_v^2 R$，其中 $\\sigma_v^2 = \\mathbb{E}\\{v^2(n)\\}$。瞬态行为由第一项决定。展开后得到：\n$$\nK(n+1)_{\\text{transient}} = \\mathbb{E}\\{ \\tilde{w}(n)\\tilde{w}^{\\top}(n) - \\mu \\tilde{w}(n)\\tilde{w}^{\\top}(n)x(n)x^{\\top}(n) - \\mu x(n)x^{\\top}(n)\\tilde{w}(n)\\tilde{w}^{\\top}(n) + \\mu^2 x(n)x^{\\top}(n)\\tilde{w}(n)\\tilde{w}^{\\top}(n)x(n)x^{\\top}(n) \\}\n$$\n使用独立性假设（即 $x(n)$ 与 $\\tilde{w}(n)$ 无关）并忽略 $\\mu^2$ 阶项（这在步长很小时是有效的），我们得到一个简化的递推关系：\n$$\nK(n+1) \\approx K(n) - \\mu K(n) R - \\mu R K(n)\n$$\n这个一阶近似描述了权误差相关矩阵的瞬态演化。为了分析沿 $R$ 的特征方向的衰减，我们进行坐标变换。设 $R = Q \\Lambda Q^{\\top}$ 是 $R$ 的特征分解，其中 $Q$ 是特征向量构成的正交矩阵，$\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_m)$ 是对应特征值的对角矩阵。\n我们定义变换后的相关矩阵 $K'(n) \\triangleq Q^{\\top} K(n) Q$。用 $Q^{\\top}$ 和 $Q$ 分别左乘和右乘该递推关系：\n$$\nQ^{\\top}K(n+1)Q \\approx Q^{\\top}K(n)Q - \\mu (Q^{\\top}K(n)Q)(Q^{\\top}RQ) - \\mu (Q^{\\top}RQ)(Q^{\\top}K(n)Q)\n$$\n$$\nK'(n+1) \\approx K'(n) - \\mu K'(n) \\Lambda - \\mu \\Lambda K'(n)\n$$\n由于 $\\Lambda$ 是对角矩阵，这个矩阵方程对 $K'(n)$ 的对角元素是解耦的。设 $k'_i(n) \\triangleq [K'(n)]_{ii}$ 为第 $i$ 个对角元素，它表示权误差沿第 $i$ 个特征向量方向的均方值。$k'_i(n)$ 的递推关系是：\n$$\nk'_i(n+1) \\approx k'_i(n) - \\mu k'_i(n) \\lambda_i - \\mu \\lambda_i k'_i(n) = (1 - 2\\mu\\lambda_i)k'_i(n)\n$$\n这是一个简单的几何级数。其解为：\n$$\nk'_i(n) \\approx (1 - 2\\mu\\lambda_i)^n k'_i(0)\n$$\n整个算法的收敛速度受限于最慢的衰减模式。模式 $i$ 的衰减因子是 $(1 - 2\\mu\\lambda_i)$。由于 $\\mu > 0$ 且所有 $\\lambda_i > 0$，最慢的收敛发生在衰减因子最接近 1 的模式，这对应于最小的特征值 $\\lambda_{\\min}$。\n在该最慢模式的瞬态分量中，实现衰减因子 $\\rho$ 所需的迭代次数 $n$ 由下式给出：\n$$\n(1 - 2\\mu\\lambda_{\\min})^n = \\rho\n$$\n解出 $n$ 得到：\n$$\nn = \\frac{\\ln(\\rho)}{\\ln(1 - 2\\mu\\lambda_{\\min})}\n$$\n问题指定了步长规则 $\\mu = \\beta / \\lambda_{\\max}$ 和条件数 $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$。将这些代入 $n$ 的表达式，得到迭代次数作为这些参数的函数：\n$$\nn(\\kappa, \\rho, \\beta) = \\frac{\\ln(\\rho)}{\\ln(1 - 2 (\\frac{\\beta}{\\lambda_{\\max}}) \\lambda_{\\min})} = \\frac{\\ln(\\rho)}{\\ln(1 - \\frac{2\\beta}{\\kappa})}\n$$\n对于白噪声输入的参考情况，$\\kappa=1$。这意味着 $\\lambda_{\\min} = \\lambda_{\\max}$。迭代次数变为：\n$$\nn(1, \\rho, \\beta) = \\frac{\\ln(\\rho)}{\\ln(1 - 2\\beta)}\n$$\n乘性因子 $F(\\kappa, \\beta)$ 是这两个量的比值：\n$$\nF(\\kappa, \\beta) = \\frac{n(\\kappa, \\rho, \\beta)}{n(1, \\rho, \\beta)} = \\frac{\\ln(\\rho) / \\ln(1 - 2\\beta/\\kappa)}{\\ln(\\rho) / \\ln(1 - 2\\beta)} = \\frac{\\ln(1 - 2\\beta)}{\\ln(1 - 2\\beta/\\kappa)}\n$$\n问题要求在“小 $\\beta$”情况下估计该因子。这个条件意味着 $\\beta \\ll 1$。如果 $\\beta$ 很小，那么对数函数的自变量 $2\\beta$ 和 $2\\beta/\\kappa$（因为 $\\kappa \\ge 1$）也都很小。因此，我们可以对小的 $x$ 使用一阶泰勒近似式 $\\ln(1-x) \\approx -x$。\n将此近似应用于分子和分母：\n$$\nF(\\kappa, \\beta) \\approx \\frac{-2\\beta}{-2\\beta/\\kappa} = \\kappa\n$$\n这就是所求的估计值。它表明，对于归一化的步长，LMS 算法的收敛时间与输入相关矩阵的条件数成线性关系。\n\n最后，我们被要求计算该因子在 $\\kappa = 100$ 时的值。\n$$\nF(100, \\beta) \\approx 100\n$$\n在此近似下，结果与 $\\beta$ 无关。", "answer": "$$\n\\boxed{100}\n$$", "id": "2850237"}]}