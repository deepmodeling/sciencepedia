{"hands_on_practices": [{"introduction": "要比较两种自适应算法，一个核心的起点是理解它们的平均收敛行为。这个练习将引导你对LMS和RLS算法的均值权重误差向量进行理论分析，揭示它们在响应输入信号统计特性方面的根本差异。通过在经典的独立性假设下推导它们的均值动态，你将清晰地看到为何RLS通常能提供与输入信号相关性无关的更快收敛速度，而LMS的性能则受限于输入相关矩阵$R$的特征值分布 [@problem_id:2891049]。", "problem": "考虑一个线性数据模型，其输入向量过程 $\\{\\mathbf{x}(n)\\} \\in \\mathbb{R}^{M}$ 为零均值、广义平稳，相关矩阵为 $R \\triangleq E\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\} \\succ 0$。该模型有一个未知的恒定参数向量 $\\mathbf{w}_{o} \\in \\mathbb{R}^{M}$，以及标量期望信号 $d(n) = \\mathbf{x}^{\\top}(n) \\mathbf{w}_{o} + v(n)$，其中 $\\{v(n)\\}$ 是独立于 $\\{\\mathbf{x}(n)\\}$ 且具有有限方差的零均值测量噪声。一个自适应估计器根据数据 $\\{\\mathbf{x}(k), d(k)\\}_{k \\le n}$ 产生 $\\mathbf{w}_{o}$ 的一个估计 $\\mathbf{w}(n)$。\n\n定义权重误差向量 $\\tilde{\\mathbf{w}}(n) \\triangleq \\mathbf{w}(n) - \\mathbf{w}_{o}$。根据自适应滤波器均值分析中使用的标准独立性假设 (IA)，$\\tilde{\\mathbf{w}}(n)$ 被视为与当前回归量 $\\mathbf{x}(n)$ 相互独立。最小均方 (LMS) 算法对均方误差使用随机梯度下降法，而递归最小二乘 (RLS) 算法递归地计算在每个时刻 $n$ 最小化指数加权代价 $\\sum_{k=0}^{n} \\lambda^{n-k} (d(k)-\\mathbf{w}^{\\top}\\mathbf{x}(k))^2$ 的权重向量 $\\mathbf{w}(n)$，遗忘因子 $\\lambda \\in (0,1]$。\n\n从这些定义和 IA 出发，在上述平稳情况下，当 $\\lambda \\lesssim 1$ 时，比较 LMS 和 RLS 的平均权重误差动态特性。以下哪个/哪些陈述正确地描述了它们均值更新之间的对比？\n\nA. 在 IA 下，LMS 表现出模式相关的均值收缩，该收缩由 $R$ 的特征值决定（对于步长 $\\mu$，每个模式 $i$ 大致按因子 $1-\\mu \\lambda_{i}(R)$ 缩放），而对于 $\\lambda \\lesssim 1$ 的 RLS，平均权重误差近似均匀收缩，即 $E\\{\\tilde{\\mathbf{w}}(n+1)\\} \\approx \\lambda\\, E\\{\\tilde{\\mathbf{w}}(n)\\}$，且与 $R$ 无关。\n\nB. 在 IA 下，RLS 的均值更新行为类似于 $E\\{\\tilde{\\mathbf{w}}(n+1)\\} \\approx \\left(I - \\mu R^{-1}\\right) E\\{\\tilde{\\mathbf{w}}(n)\\}$（对于某个步长 $\\mu$），这使得 RLS 沿着 $R$ 的小特征值方向收敛更快，而沿着大特征值方向收敛更慢。\n\nC. 对于 $\\lambda \\lesssim 1$ 的情况，可以通过选择 LMS 步长 $\\mu$ 使得 $I-\\mu R = \\lambda I$ 来使 RLS 和 LMS 在均值上匹配，因此无论 $R$ 为何，它们的均值动态特性都无法区分。\n\nD. 在平稳条件下，当 $\\lambda < 1$ 且 $E\\{v(n)\\}=0$ 时，RLS 在均值上会变得有偏，因此 $\\lim_{n\\to\\infty} E\\{\\mathbf{w}(n)\\} \\neq \\mathbf{w}_{o}$，这与 LMS 不同，LMS 在均值稳定的情况下会保持均值无偏。\n\nE. LMS 的平均收敛速度基本上与 $R$ 的特征值分布无关，而当 $R$ 是病态矩阵时，RLS 的平均收敛速度会严重下降；因此 RLS 对 $R$ 的条件数 $\\kappa(R)$ 比 LMS 更敏感。", "solution": "在尝试任何解答之前，首先对所提供的问题陈述进行严格验证。\n\n**问题验证**\n\n**第 1 步：提取已知信息**\n\n问题提供了以下信息：\n- 数据模型：一个线性模型 $d(n) = \\mathbf{x}^{\\top}(n) \\mathbf{w}_{o} + v(n)$。\n- 输入过程：$\\{\\mathbf{x}(n)\\} \\in \\mathbb{R}^{M}$ 是一个零均值、广义平稳 (WSS) 向量过程。\n- 输入相关矩阵：$R \\triangleq E\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\} \\succ 0$ (正定)。\n- 真实参数向量：$\\mathbf{w}_{o} \\in \\mathbb{R}^{M}$ 是一个未知的常数。\n- 测量噪声：$\\{v(n)\\}$ 是一个零均值标量过程，具有有限方差，独立于过程 $\\{\\mathbf{x}(n)\\}$。\n- 权重估计：$\\mathbf{w}(n)$ 是在时刻 $n$ 对 $\\mathbf{w}_{o}$ 的估计。\n- 权重误差向量：$\\tilde{\\mathbf{w}}(n) \\triangleq \\mathbf{w}(n) - \\mathbf{w}_{o}$。\n- 关键假设：将使用独立性假设 (IA)，该假设将 $\\tilde{\\mathbf{w}}(n)$ 视为与当前输入向量 $\\mathbf{x}(n)$ 统计上独立。\n- LMS 算法：一种随机梯度下降法。\n- RLS 算法：递归地计算最小化指数加权代价 $\\sum_{k=0}^{n} \\lambda^{n-k} (d(k)-\\mathbf{w}^{\\top}\\mathbf{x}(k))^2$ 的权重向量 $\\mathbf{w}(n)$，遗忘因子 $\\lambda \\in (0,1]$。\n- 背景：比较是在平稳环境中，当 $\\lambda \\lesssim 1$ 时进行。\n\n**第 2 步：使用提取的已知信息进行验证**\n\n- **科学依据**：该问题牢固地建立在自适应滤波器理论这一成熟领域中，这是信号处理和系统工程的核心课题。所有概念，包括 LMS 和 RLS 算法、数据模型以及独立性假设，在该领域都有标准且严格的定义。该设置没有违反任何科学或数学原理。\n- **适定性**：问题要求在指定的标准假设下，对两种标准算法的平均权重误差动态特性进行对比分析。问题的结构决定了其有一个基于这些假设的、确定的、可推导的答案。\n- **客观性**：问题陈述使用了精确、无歧义的数学和技术语言，不含任何主观或基于观点的内容。\n- **完整性**：问题提供了推导两种算法的平均误差动态特性所需的所有必要信息（数据模型、算法定义、信号的统计特性以及分析假设）。\n\n**第 3 步：结论与行动**\n\n问题陈述在科学上是合理的、适定的、客观且完整的。它提出了自适应滤波器理论中一个标准的、非平凡的问题。因此，该问题是**有效的**。我们继续推导解答。\n\n**平均权重误差动态特性的推导**\n\n**最小均方 (LMS) 算法**\n\nLMS 算法的权重更新公式为：\n$$ \\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\mathbf{x}(n) e(n) $$\n其中 $\\mu$ 是步长参数，$e(n) = d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n)$ 是估计误差。代入 $d(n)$ 和权重误差向量 $\\tilde{\\mathbf{w}}(n) = \\mathbf{w}(n) - \\mathbf{w}_o$ 的定义，我们有：\n$$ \\tilde{\\mathbf{w}}(n+1) + \\mathbf{w}_o = \\tilde{\\mathbf{w}}(n) + \\mathbf{w}_o + \\mu \\mathbf{x}(n) [\\mathbf{x}^{\\top}(n)\\mathbf{w}_o + v(n) - \\mathbf{x}^{\\top}(n)(\\tilde{\\mathbf{w}}(n) + \\mathbf{w}_o)] $$\n$$ \\tilde{\\mathbf{w}}(n+1) = \\tilde{\\mathbf{w}}(n) + \\mu \\mathbf{x}(n) [-\\mathbf{x}^{\\top}(n)\\tilde{\\mathbf{w}}(n) + v(n)] $$\n$$ \\tilde{\\mathbf{w}}(n+1) = (I - \\mu \\mathbf{x}(n)\\mathbf{x}^{\\top}(n))\\tilde{\\mathbf{w}}(n) + \\mu \\mathbf{x}(n)v(n) $$\n为了分析其均值行为，我们对两边取期望：\n$$ E\\{\\tilde{\\mathbf{w}}(n+1)\\} = E\\{(I - \\mu \\mathbf{x}(n)\\mathbf{x}^{\\top}(n))\\tilde{\\mathbf{w}}(n)\\} + \\mu E\\{\\mathbf{x}(n)v(n)\\} $$\n由于 $v(n)$ 独立于 $\\mathbf{x}(n)$ 且均值为零，所以 $E\\{\\mathbf{x}(n)v(n)\\} = E\\{\\mathbf{x}(n)\\}E\\{v(n)\\} = 0$。应用独立性假设 (IA)，即 $\\tilde{\\mathbf{w}}(n)$ 独立于 $\\mathbf{x}(n)$，我们可以分离期望：\n$$ E\\{\\tilde{\\mathbf{w}}(n+1)\\} = E\\{I - \\mu \\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\} E\\{\\tilde{\\mathbf{w}}(n)\\} $$\n$$ E\\{\\tilde{\\mathbf{w}}(n+1)\\} = (I - \\mu E\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}) E\\{\\tilde{\\mathbf{w}}(n)\\} $$\n根据定义，$R = E\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}$，因此 LMS 的平均权重误差动态特性为：\n$$ E\\{\\tilde{\\mathbf{w}}(n+1)\\} = (I - \\mu R) E\\{\\tilde{\\mathbf{w}}(n)\\} $$\n平均误差的收敛由矩阵 $(I - \\mu R)$ 的特征值决定。如果 $R = Q\\Lambda Q^{\\top}$ 是 $R$ 的特征分解，其特征值为 $\\{\\lambda_i\\}$，那么在主轴坐标系中，动态特性是解耦的：变换后的误差向量的每个模式都以因子 $(1 - \\mu \\lambda_i)$ 收缩。因此，总体的收敛速度受限于最慢的模式，该模式对应于 $R$ 的最小特征值，而稳定性则受限于最大特征值。因此，LMS 的平均收敛性高度依赖于 $R$ 的特征值分布。\n\n**递归最小二乘 (RLS) 算法**\n\nRLS 算法递归地计算权重向量 $\\mathbf{w}(n)$，该向量最小化指数加权的最小二乘代价函数。权重更新公式为：\n$$ \\mathbf{w}(n) = \\mathbf{w}(n-1) + \\mathbf{k}(n)[d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n-1)] $$\n其中 $\\mathbf{k}(n) = P(n)\\mathbf{x}(n)$ 是增益向量，而 $P(n) = \\left(\\sum_{k=0}^n \\lambda^{n-k}\\mathbf{x}(k)\\mathbf{x}^\\top(k)\\right)^{-1}$。\n用权重误差向量 $\\tilde{\\mathbf{w}}(n) = \\mathbf{w}(n) - \\mathbf{w}_o$ 表示，更新变为：\n$$ \\tilde{\\mathbf{w}}(n) = (I - \\mathbf{k}(n)\\mathbf{x}^{\\top}(n))\\tilde{\\mathbf{w}}(n-1) + \\mathbf{k}(n)v(n) $$\n取期望可得：\n$$ E\\{\\tilde{\\mathbf{w}}(n)\\} = E\\{(I - P(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n))\\tilde{\\mathbf{w}}(n-1)\\} + E\\{P(n)\\mathbf{x}(n)v(n)\\} $$\n第二项为零，因为 $v(n)$ 是零均值且与决定 $P(n)$ 和 $\\mathbf{x}(n)$ 的过去数据无关。应用 IA，我们将 $E\\{\\tilde{\\mathbf{w}}(n-1)\\}$ 近似为与当前数据 $\\mathbf{x}(n)$（以及 $P(n)$）无关：\n$$ E\\{\\tilde{\\mathbf{w}}(n)\\} \\approx (I - E\\{P(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}) E\\{\\tilde{\\mathbf{w}}(n-1)\\} $$\n在平稳环境中，对于较大的 $n$，矩阵 $P(n)$ 趋于一个稳态值。其期望值可以近似为：\n$$ E\\{P(n)\\} \\approx \\left(E\\left\\{\\sum_{k=0}^n \\lambda^{n-k}\\mathbf{x}(k)\\mathbf{x}^\\top(k)\\right\\}\\right)^{-1} = \\left(R \\sum_{k=0}^n \\lambda^{n-k}\\right)^{-1} $$\n对于较大的 $n$，几何级数和为 $\\sum_{k=0}^n \\lambda^{n-k} \\approx \\frac{1}{1-\\lambda}$。因此，$E\\{P(n)\\} \\approx (1-\\lambda)R^{-1}$。\n将此近似代入期望 $E\\{P(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}$ 中：\n$$ E\\{P(n)\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\} \\approx E\\{(1-\\lambda)R^{-1}\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\} = (1-\\lambda)R^{-1}E\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\} = (1-\\lambda)R^{-1}R = (1-\\lambda)I $$\n因此，RLS 的平均权重误差动态特性近似为：\n$$ E\\{\\tilde{\\mathbf{w}}(n)\\} \\approx (I - (1-\\lambda)I)E\\{\\tilde{\\mathbf{w}}(n-1)\\} = \\lambda E\\{\\tilde{\\mathbf{w}}(n-1)\\} $$\n这个结果表明，RLS 平均权重误差的所有模式在每次迭代中都近似以相同的因子 $\\lambda$ 收缩。因此，均值的收敛与输入相关矩阵 $R$ 的特征值结构无关。\n\n**选项评估**\n\n**A. 在 IA 下，LMS 表现出模式相关的均值收缩，该收缩由 $R$ 的特征值决定（对于步长 $\\mu$，每个模式 $i$ 大致按因子 $1-\\mu \\lambda_{i}(R)$ 缩放），而对于 $\\lambda \\lesssim 1$ 的 RLS，平均权重误差近似均匀收缩，即 $E\\{\\tilde{\\mathbf{w}}(n+1)\\} \\approx \\lambda\\, E\\{\\tilde{\\mathbf{w}}(n)\\}$，且与 $R$ 无关。**\n- 该陈述准确地总结了我们对 LMS 和 RLS 的推导。LMS 部分正确地指出了模式相关的收敛因子 $(1-\\mu \\lambda_i)$。RLS 部分正确地陈述了按因子 $\\lambda$ 的近似均匀收缩，这是使其收敛速度对输入相关性不敏感的关键特征。\n- **结论：正确。**\n\n**B. 在 IA 下，RLS 的均值更新行为类似于 $E\\{\\tilde{\\mathbf{w}}(n+1)\\} \\approx \\left(I - \\mu R^{-1}\\right) E\\{\\tilde{\\mathbf{w}}(n)\\}$（对于某个步长 $\\mu$），这使得 RLS 沿着 $R$ 的小特征值方向收敛更快，而沿着大特征值方向收敛更慢。**\n- 所提出的动态矩阵 $(I - \\mu R^{-1})$ 是不正确的。RLS 算法是牛顿法的一种近似，其均值动态是由一个与 $I$ 成比例的矩阵（而非 $R^{-1}$）决定的。如推导所示，近似传播算子的正确形式是 $\\lambda I$。该陈述错误地描述了 RLS 的更新及其收敛特性。\n- **结论：不正确。**\n\n**C. 对于 $\\lambda \\lesssim 1$ 的情况，可以通过选择 LMS 步长 $\\mu$ 使得 $I-\\mu R = \\lambda I$ 来使 RLS 和 LMS 在均值上匹配，因此无论 $R$ 为何，它们的均值动态特性都无法区分。**\n- 匹配动态特性的条件是 $I - \\mu R = \\lambda I$，可简化为 $\\mu R = (1-\\lambda)I$。这要求 $R = \\frac{1-\\lambda}{\\mu}I$。这意味着相关矩阵 $R$ 必须是单位矩阵的标量倍（即输入信号必须是白噪声）。对于一个任意矩阵 $R$，单个标量 $\\mu$ 无法满足这个等式。声称“无论 R 为何”都可以匹配，这在根本上是错误的。\n- **结论：不正确。**\n\n**D. 在平稳条件下，当 $\\lambda < 1$ 且 $E\\{v(n)\\}=0$ 时，RLS 在均值上会变得有偏，因此 $\\lim_{n\\to\\infty} E\\{\\mathbf{w}(n)\\} \\neq \\mathbf{w}_{o}$，这与 LMS 不同，LMS 在均值稳定的情况下会保持均值无偏。**\n- 标准分析表明，RLS 算法在均值上是渐近无偏的。即，$\\lim_{n\\to\\infty} E\\{\\mathbf{w}(n)\\} = \\mathbf{w}_{o}$。噪声项 $v(n)$ 均值为零且与输入信号无关，因此其对权重估计均值的影响在平均后为零。在 $\\lambda<1$ 的 RLS 的背景下，“偏置”一词通常指的是该算法最小化的代价函数是真实MSE的一个有偏估计，这会导致一个非零的稳态超量均方误差（失调），但并不会导致权重本身均值的偏置。该陈述在关于 RLS 的事实上是错误的。\n- **结论：不正确。**\n\n**E. LMS 的平均收敛速度基本上与 $R$ 的特征值分布无关，而当 $R$ 是病态矩阵时，RLS 的平均收敛速度会严重下降；因此 RLS 对 $R$ 的条件数 $\\kappa(R)$ 比 LMS 更敏感。**\n- 这个陈述完全颠倒了已知的事实。LMS 的平均收敛性严重依赖于 $R$ 的特征值分布（条件数）。相比之下，RLS 的一个主要优点是它对 $R$ 的特征值分布不敏感，因为其平均收敛速率是由 $\\lambda$ 决定的。\n- **结论：不正确。**", "answer": "$$\\boxed{A}$$", "id": "2891049"}, {"introduction": "在理解了平均收敛特性之后，我们可以进一步探索更精细的二阶性能。这个练习深入研究了LMS算法中一个著名且违反直觉的瞬态行为——“峰值”现象，即在某些情况下（尤其是有色输入信号），即使在无噪声的环境中，LMS的均方误差也可能在收敛过程中出现暂时的增加。通过推导这一现象发生的条件，你将深刻理解LMS在处理相关信号时的局限性，并能从一个独特的角度体会到RLS算法内在的鲁棒性 [@problem_id:2891097]。", "problem": "考虑一个双抽头未知线性时不变系统，其系数向量为 $\\mathbf{w}_{o} \\in \\mathbb{R}^{2}$，由一个标量、零均值、平稳的一阶自回归输入 $x(n)$ 驱动，该输入满足 $x(n) = \\alpha x(n-1) + u(n)$，其中 $|\\alpha| < 1$，$u(n)$ 是一个零均值、独立同分布的高斯过程，其方差为 $\\sigma_{u}^{2}$。设回归量为 $\\mathbf{X}(n) = [\\,x(n)\\;\\;x(n-1)\\,]^{\\mathsf{T}}$，并假设期望响应是无噪声的，$d(n) = \\mathbf{w}_{o}^{\\mathsf{T}} \\mathbf{X}(n)$，因此观测噪声方差为 $\\sigma_{v}^{2} = 0$。一个长度为 $2$ 的自适应滤波器使用最小均方 (LMS) 算法来辨识 $\\mathbf{w}_{o}$，该算法使用恒定步长 $\\mu > 0$，权重更新公式为 $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\mathbf{X}(n) e(n)$，其中先验误差为 $e(n) = d(n) - \\mathbf{w}(n)^{\\mathsf{T}} \\mathbf{X}(n)$。定义权重误差向量 $\\tilde{\\mathbf{w}}(n) = \\mathbf{w}_{o} - \\mathbf{w}(n)$ 及其协方差 $C(n) = \\mathbb{E}[\\tilde{\\mathbf{w}}(n)\\tilde{\\mathbf{w}}(n)^{\\mathsf{T}}]$。输入协方差为 $R = \\mathbb{E}[\\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}]$。在时间 $n$ 的超量均方误差 (EMSE) 为 $\\xi(n) = \\operatorname{Tr}(R C(n))$，且由于 $\\sigma_{v}^{2} = 0$，均方误差 (MSE) 等于 $\\xi(n)$。\n\n假设初始权重误差协方差是各向同性的，$C(0) = \\beta I_{2}$，其中 $\\beta > 0$。仅使用核心定义（LMS 递推、$\\xi(n)$ 的定义以及零均值高斯回归量的四阶矩恒等式 $\\mathbb{E}[\\mathbf{X} \\mathbf{X}^{\\mathsf{T}} C \\mathbf{X} \\mathbf{X}^{\\mathsf{T}}] = R C R + R\\,\\operatorname{Tr}(R C)$），推导一个以 $\\mu$、$\\alpha$ 和 $\\sigma_{u}^{2}$ 表示的闭式不等式，该不等式保证单步 EMSE 不会增加，即对于所有 $\\beta > 0$，都有 $\\xi(1) \\leq \\xi(0)$。然后，将此不等式化简，求出使单步 EMSE 对任何 $\\beta > 0$ 都非增的最大容许步长 $\\mu_{\\mathrm{th}}(\\alpha,\\sigma_{u}^{2})$。将 $\\mu_{\\mathrm{th}}(\\alpha,\\sigma_{u}^{2})$ 表示为 $\\alpha$ 和 $\\sigma_{u}^{2}$ 的简化解析函数。\n\n最后，从第一性原理出发，分析具有单位遗忘因子的递推最小二乘 (RLS) 算法在相同的回归过程下是否会表现出类似的 MSE 瞬态峰值现象。你必须使用 RLS 协方差递推的结构来证明你的结论，但你的最终数值答案应仅为 $\\mu_{\\mathrm{th}}(\\alpha,\\sigma_{u}^{2})$。无需进行四舍五入，最终表达式中不应包含任何单位。", "solution": "所提出的问题具有科学依据、提法恰当、客观，并包含了进行严谨求解所需的所有信息。这是自适应滤波器瞬态分析中的一个典型问题。因此，该问题是有效的，我们着手进行推导。\n\n第一步是推导权重误差协方差矩阵 $C(n) = \\mathbb{E}[\\tilde{\\mathbf{w}}(n)\\tilde{\\mathbf{w}}(n)^{\\mathsf{T}}]$ 的递推式。权重误差向量为 $\\tilde{\\mathbf{w}}(n) = \\mathbf{w}_{o} - \\mathbf{w}(n)$。其演化由 LMS 算法的结构给出。\n从 LMS 权重更新开始：\n$$\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\mathbf{X}(n) e(n)$$\n其中先验误差为 $e(n) = d(n) - \\mathbf{w}(n)^{\\mathsf{T}}\\mathbf{X}(n)$。给定无噪声的期望响应 $d(n) = \\mathbf{w}_{o}^{\\mathsf{T}}\\mathbf{X}(n)$，误差变为：\n$$e(n) = \\mathbf{w}_{o}^{\\mathsf{T}}\\mathbf{X}(n) - \\mathbf{w}(n)^{\\mathsf{T}}\\mathbf{X}(n) = (\\mathbf{w}_{o} - \\mathbf{w}(n))^{\\mathsf{T}}\\mathbf{X}(n) = \\tilde{\\mathbf{w}}(n)^{\\mathsf{T}}\\mathbf{X}(n)$$\n权重误差向量的递推式则为：\n$$\\tilde{\\mathbf{w}}(n+1) = \\mathbf{w}_{o} - \\mathbf{w}(n+1) = \\mathbf{w}_{o} - (\\mathbf{w}(n) + \\mu \\mathbf{X}(n) e(n)) = \\tilde{\\mathbf{w}}(n) - \\mu \\mathbf{X}(n) \\tilde{\\mathbf{w}}(n)^{\\mathsf{T}}\\mathbf{X}(n)$$\n$$\\tilde{\\mathbf{w}}(n+1) = (I_{2} - \\mu \\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}) \\tilde{\\mathbf{w}}(n)$$\n其中 $I_{2}$ 是 $2 \\times 2$ 的单位矩阵。\n\n接下来，我们求协方差矩阵 $C(n+1)$ 的递推式：\n$$C(n+1) = \\mathbb{E}[\\tilde{\\mathbf{w}}(n+1)\\tilde{\\mathbf{w}}(n+1)^{\\mathsf{T}}] = \\mathbb{E}[(I_{2} - \\mu \\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}) \\tilde{\\mathbf{w}}(n) \\tilde{\\mathbf{w}}(n)^{\\mathsf{T}} (I_{2} - \\mu \\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}})^{\\mathsf{T}}]$$\n在 LMS 分析中，一个标准的假设（通常称为“独立性假设”）是权重误差向量 $\\tilde{\\mathbf{w}}(n)$ 与输入回归量向量 $\\mathbf{X}(n)$ 统计独立。这允许我们分离期望：\n$$C(n+1) = \\mathbb{E}[(I_{2} - \\mu \\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}) C(n) (I_{2} - \\mu \\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}})]$$\n展开此表达式可得：\n$$C(n+1) = \\mathbb{E}[C(n) - \\mu C(n)\\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}} - \\mu \\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}C(n) + \\mu^{2} \\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}C(n)\\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}]$$\n对 $\\mathbf{X}(n)$ 取期望，并使用输入协方差矩阵的定义 $R = \\mathbb{E}[\\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}]$，得到：\n$$C(n+1) = C(n) - \\mu C(n)R - \\mu RC(n) + \\mu^{2} \\mathbb{E}[\\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}C(n)\\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}]$$\n问题给出了零均值高斯回归量的四阶矩恒等式：$\\mathbb{E}[\\mathbf{X} \\mathbf{X}^{\\mathsf{T}} C \\mathbf{X} \\mathbf{X}^{\\mathsf{T}}] = R C R + R\\,\\operatorname{Tr}(R C)$。将此代入递推式，得到高斯假设下协方差矩阵的精确演化：\n$$C(n+1) = C(n) - \\mu(C(n)R + RC(n)) + \\mu^{2}(RC(n)R + R\\operatorname{Tr}(RC(n)))$$\n超量均方误差 (EMSE) 为 $\\xi(n) = \\operatorname{Tr}(RC(n))$。我们可以求出从 $n=0$到 $n=1$ 的 EMSE 的单步演化：\n$$\\xi(1) = \\operatorname{Tr}(RC(1)) = \\operatorname{Tr}[R(C(0) - \\mu(C(0)R + RC(0)) + \\mu^{2}(RC(0)R + R\\operatorname{Tr}(RC(0))))]$$\n利用迹算子的线性和循环性质 ($\\operatorname{Tr}(AB) = \\operatorname{Tr}(BA)$)：\n$$\\xi(1) = \\operatorname{Tr}(RC(0)) - \\mu(\\operatorname{Tr}(RC(0)R) + \\operatorname{Tr}(R^2C(0))) + \\mu^{2}(\\operatorname{Tr}(R^2C(0)R) + \\operatorname{Tr}(R^2)\\operatorname{Tr}(RC(0)))$$\n$$\\xi(1) = \\xi(0) - 2\\mu\\operatorname{Tr}(R^2C(0)) + \\mu^{2}(\\operatorname{Tr}(R^3C(0)) + \\operatorname{Tr}(R^2)\\xi(0))$$\n第一步中 EMSE 非增的条件是 $\\xi(1) \\leq \\xi(0)$。这导致了以下不等式：\n$$-2\\mu\\operatorname{Tr}(R^2C(0)) + \\mu^{2}(\\operatorname{Tr}(R^3C(0)) + \\operatorname{Tr}(R^2)\\xi(0)) \\leq 0$$\n因为 $\\mu > 0$，我们可以用 $\\mu$ 去除两边：\n$$\\mu(\\operatorname{Tr}(R^3C(0)) + \\operatorname{Tr}(R^2)\\operatorname{Tr}(RC(0))) \\leq 2\\operatorname{Tr}(R^2C(0))$$\n问题规定，该不等式必须对任何各向同性的初始条件 $C(0) = \\beta I_{2}$（其中 $\\beta > 0$）都成立。代入此条件：\n$$\\mu(\\operatorname{Tr}(R^3(\\beta I_{2})) + \\operatorname{Tr}(R^2)\\operatorname{Tr}(R(\\beta I_{2}))) \\leq 2\\operatorname{Tr}(R^2(\\beta I_{2}))$$\n标量 $\\beta$ 可以被提取出来并消去，因为这是条件对所有 $\\beta > 0$ 成立的必然结果：\n$$\\mu(\\operatorname{Tr}(R^3) + \\operatorname{Tr}(R^2)\\operatorname{Tr}(R)) \\leq 2\\operatorname{Tr}(R^2)$$\n因此，保证此条件的最大容许步长 $\\mu_{\\mathrm{th}}$ 为：\n$$\\mu_{\\mathrm{th}} = \\frac{2\\operatorname{Tr}(R^2)}{\\operatorname{Tr}(R^3) + \\operatorname{Tr}(R^2)\\operatorname{Tr}(R)}$$\n为了得到最终表达式，我们必须计算输入协方差矩阵 $R$ 各次幂的迹。输入 $x(n)$ 是一个 AR($1$) 过程，$x(n) = \\alpha x(n-1) + u(n)$。其方差为 $r(0) = \\mathbb{E}[x^2(n)] = \\frac{\\sigma_{u}^{2}}{1-\\alpha^2}$，其滞后为1的自相关为 $r(1) = \\mathbb{E}[x(n)x(n-1)] = \\alpha r(0) = \\frac{\\alpha\\sigma_{u}^{2}}{1-\\alpha^2}$。\n回归量为 $\\mathbf{X}(n) = [\\,x(n)\\;\\;x(n-1)\\,]^{\\mathsf{T}}$，所以协方差矩阵为：\n$$R = \\mathbb{E}[\\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}] = \\begin{pmatrix} r(0) & r(1) \\\\ r(1) & r(0) \\end{pmatrix} = \\frac{\\sigma_{u}^{2}}{1-\\alpha^2} \\begin{pmatrix} 1 & \\alpha \\\\ \\alpha & 1 \\end{pmatrix}$$\n令 $S = \\frac{\\sigma_{u}^{2}}{1-\\alpha^2}$。则 $R = S \\begin{pmatrix} 1 & \\alpha \\\\ \\alpha & 1 \\end{pmatrix}$。\n矩阵 $\\begin{pmatrix} 1 & \\alpha \\\\ \\alpha & 1 \\end{pmatrix}$ 的特征值为 $1 \\pm \\alpha$。因此，$R$ 的特征值为 $\\lambda_{1} = S(1-\\alpha)$ 和 $\\lambda_{2} = S(1+\\alpha)$。\n我们利用特征值的幂之和来计算所需的迹：\n$\\operatorname{Tr}(R) = \\lambda_{1} + \\lambda_{2} = S(1-\\alpha) + S(1+\\alpha) = 2S$。\n$\\operatorname{Tr}(R^2) = \\lambda_{1}^{2} + \\lambda_{2}^{2} = S^2((1-\\alpha)^2 + (1+\\alpha)^2) = S^2(1-2\\alpha+\\alpha^2 + 1+2\\alpha+\\alpha^2) = 2S^2(1+\\alpha^2)$。\n$\\operatorname{Tr}(R^3) = \\lambda_{1}^{3} + \\lambda_{2}^{3} = S^3((1-\\alpha)^3 + (1+\\alpha)^3) = S^3((1-3\\alpha+3\\alpha^2-\\alpha^3) + (1+3\\alpha+3\\alpha^2+\\alpha^3)) = 2S^3(1+3\\alpha^2)$。\n\n将这些代入 $\\mu_{\\mathrm{th}}$ 的表达式中：\n分子: $2\\operatorname{Tr}(R^2) = 2(2S^2(1+\\alpha^2)) = 4S^2(1+\\alpha^2)$。\n分母: $\\operatorname{Tr}(R^3) + \\operatorname{Tr}(R^2)\\operatorname{Tr}(R) = 2S^3(1+3\\alpha^2) + (2S^2(1+\\alpha^2))(2S) = 2S^3(1+3\\alpha^2) + 4S^3(1+\\alpha^2) = S^3(2+6\\alpha^2+4+4\\alpha^2) = 2S^3(3+5\\alpha^2)$。\n$$\\mu_{\\mathrm{th}} = \\frac{4S^2(1+\\alpha^2)}{2S^3(3+5\\alpha^2)} = \\frac{2(1+\\alpha^2)}{S(3+5\\alpha^2)}$$\n将 $S = \\frac{\\sigma_{u}^{2}}{1-\\alpha^2}$ 代回：\n$$\\mu_{\\mathrm{th}} = \\frac{2(1+\\alpha^2)}{\\frac{\\sigma_{u}^{2}}{1-\\alpha^2}(3+5\\alpha^2)} = \\frac{2(1+\\alpha^2)(1-\\alpha^2)}{\\sigma_{u}^{2}(3+5\\alpha^2)}$$\n使用 $(a+b)(a-b) = a^2 - b^2$ 简化分子：\n$$\\mu_{\\mathrm{th}}(\\alpha, \\sigma_{u}^{2}) = \\frac{2(1-\\alpha^4)}{\\sigma_{u}^{2}(3+5\\alpha^2)}$$\n\n最后，我们分析在相同条件下具有单位遗忘因子 ($\\lambda=1$) 的递推最小二乘 (RLS) 算法。LMS 中的“峰值”现象是因为随机梯度更新 $\\mathbf{w}(n+1) = \\mathbf{w}(n) - \\mu \\mathbf{X}(n)\\mathbf{X}(n)^{\\mathsf{T}}\\tilde{\\mathbf{w}}(n)$ 在高度相关过程 $\\mathbf{X}(n)$ 的某些实现下，可能将权重估计值推离真值更远，导致系综平均误差的暂时增加。\nRLS 的行为有根本的不同。在 $\\lambda=1$ 和无噪声的情况下，RLS 递推地计算确定性最小二乘问题 $\\mathbf{w}(n) = \\arg\\min_{\\mathbf{w}} \\sum_{i=1}^{n} (d(i) - \\mathbf{w}^{\\mathsf{T}}\\mathbf{X}(i))^2$ 的精确解。权重更新不是一个简单的梯度步，而是涉及样本协方差矩阵的逆 $P(n) = (\\sum_{i=1}^{n} \\mathbf{X}(i)\\mathbf{X}(i)^{\\mathsf{T}})^{-1}$（假设初始化 $P(0)^{-1}=0$）。这种更新在每一步都有效地对输入数据进行去相关。在这种无噪声背景下，RLS 的一个关键特性是存在一个李雅普诺夫函数。对于该过程的任何单个实现，误差向量的加权平方范数 $\\tilde{\\mathbf{w}}(n)^{\\mathsf{T}}P(n)^{-1}\\tilde{\\mathbf{w}}(n)$ 是一个守恒量。更能说明问题的是，量 $\\tilde{\\mathbf{w}}(n)^{\\mathsf{T}}P(n-1)^{-1}\\tilde{\\mathbf{w}}(n)$ 被保证在每个时间步 $n$ 都是非增的。这种固有的稳定性（源于算法作为精确递推最小化器的结构）确保了权重向量在明确定义的意义上单调收敛到真解。这种单调收敛性排除了在 LMS 中观察到的瞬态峰值出现的可能性。因此，具有 $\\lambda=1$ 的 RLS 不会表现出 MSE 瞬态的峰值现象。", "answer": "$$\\boxed{\\frac{2(1-\\alpha^{4})}{\\sigma_{u}^{2}(3+5\\alpha^{2})}}$$", "id": "2891097"}, {"introduction": "理论为我们提供了坚实的基础，但仿真则能将抽象概念变得生动具体。这个动手编码实践要求你实现LMS和RLS算法，以直观地观察和量化它们的瞬态行为。你将具体研究RLS收敛过程中的“超调”现象——一个与其初始化参数$P(0)=\\delta^{-1}I$密切相关的效应，并将其与LMS在保守步长下更可预测的单调衰减进行对比，从而在数学分析和实际性能之间架起一座桥梁 [@problem_id:2891050]。", "problem": "您将实现并比较两种用于线性时不变系统的自适应辨识算法：最小均方 (LMS) 和递归最小二乘 (RLS)。目标是量化 RLS 信息矩阵 $P(0)=\\delta^{-1} I$ 中的初始化参数 $\\delta$ 如何影响协方差特征模态上的早期过冲，并将其与 LMS 算法在保守步长下获得的按模态单调衰减进行对比。\n\n推导的基本依据：\n- 线性系统辨识，其中回归量 $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{0}, R)$ 为零均值、独立同分布的高斯向量，其协方差矩阵 $R$ 为对称正定矩阵。\n- 期望响应为 $d_t = \\mathbf{x}_t^\\top \\mathbf{w}^\\star$，其中 $\\mathbf{w}^\\star$ 是一个固定的未知参数矢量，且无测量噪声。\n- 最小均方 (LMS) 算法基于瞬时平方误差的梯度下降，初始化为 $\\mathbf{w}(0)=\\mathbf{0}$。\n- 递归最小二乘 (RLS) 算法，带有指数遗忘因子 $\\lambda_{\\mathrm{RLS}} \\in (0,1]$，初始化为 $\\mathbf{w}(0)=\\mathbf{0}$ 和 $P(0)=\\delta^{-1} I$。\n- $R$ 的特征分解为 $R = U \\Lambda U^\\top$，其中 $\\Lambda=\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ 且 $U$ 是标准正交的，这在独立性假设下为 LMS 导出了统计上解耦的模态动力学。\n\n您必须实现的定义：\n- 由瞬时平方误差的梯度下降驱动的 LMS 更新：$\\mathbf{w}(t+1) = \\mathbf{w}(t) + \\mu \\,\\mathbf{x}_t \\,\\big(d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t)\\big)$，其中步长 $\\mu>0$。\n- 由带遗忘因子 $\\lambda_{\\mathrm{RLS}}$ 和初始信息矩阵 $P(0)$ 定义的 RLS 更新：\n  - 增益 $\\mathbf{k}_t = \\dfrac{P(t-1)\\mathbf{x}_t}{\\lambda_{\\mathrm{RLS}} + \\mathbf{x}_t^\\top P(t-1) \\mathbf{x}_t}$，\n  - 权重更新 $\\mathbf{w}(t) = \\mathbf{w}(t-1) + \\mathbf{k}_t \\big(d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t-1)\\big)$，\n  - 信息矩阵更新 $P(t) = \\lambda_{\\mathrm{RLS}}^{-1} \\big(P(t-1) - \\mathbf{k}_t \\mathbf{x}_t^\\top P(t-1)\\big)$。\n\n需要计算的性能指标：\n- 令模态失调为 $\\mathbf{z}(t) = U^\\top (\\mathbf{w}(t) - \\mathbf{w}^\\star)$ 且 $m_k(t) = \\mathbb{E}[z_k(t)^2]$。\n- 对于 RLS，定义在时间窗口 $t \\in \\{0,1,\\dots,T_0\\}$ 内模态 $k$ 的早期过冲因子为\n  $$\\rho_k = \\frac{\\max_{0 \\le t \\le T_0} m_k(t)}{m_k(0)} - 1.$$\n- 通过跨模态的最大值来概括在给定 $\\delta$ 下的 RLS 早期过冲，\n  $$\\rho_{\\max} = \\max_k \\rho_k.$$\n- 对于 LMS，评估对于每个模态 $k$，平均模态失调 $m_k(t)$ 在 $t \\in \\{0,1,\\dots,T_0\\}$ 上是否是单调非增的。\n\n用于近似期望值的仿真协议：\n- 使用 $n=4$ 个参数，$R$ 为对角矩阵以使 $U=I$ 且 $\\Lambda=\\mathrm{diag}(\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4)$，其中 $\\lambda_1=3, \\lambda_2=1, \\lambda_3=0.3, \\lambda_4=0.1$。\n- 令真实参数为 $\\mathbf{w}^\\star = [1,-0.5,0.25,-0.125]^\\top$。\n- 通过从 $\\mathbf{g}_t \\sim \\mathcal{N}(\\mathbf{0}, I)$ 中抽样并设置 $\\mathbf{x}_t = \\Lambda^{1/2} \\mathbf{g}_t$ 来生成独立同分布的回归量 $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{0}, R)$。\n- 不使用观测噪声，因此 $d_t = \\mathbf{x}_t^\\top \\mathbf{w}^\\star$。\n- 将 LMS 步长固定为 $\\mu=0.05$ 并初始化 $\\mathbf{w}(0)=\\mathbf{0}$。\n- 将 RLS 遗忘因子固定为 $\\lambda_{\\mathrm{RLS}}=0.995$ 并初始化 $\\mathbf{w}(0)=\\mathbf{0}$ 及 $P(0)=\\delta^{-1} I$。\n- 仿真总时长为 $T=200$ 次迭代，并将早期窗口定义为 $T_0=30$。\n- 通过对 $M=200$ 次独立的蒙特卡洛试验进行平均来近似 $\\mathbb{E}[\\,\\cdot\\,]$。通过使用固定的伪随机数生成器种子 $s=2025$ 来生成所有蒙特卡洛序列，并确保在所有算法和参数设置中重用完全相同的蒙特卡洛回归量序列，以保证可复现性。\n- 评估对应于 RLS 初始化尺度 $\\delta \\in \\{10^{-3}, 1, 10^2\\}$ 的三个测试用例，所有三个 $\\delta$ 值使用相同的 $\\lambda_{\\mathrm{RLS}}$ 和数据。LMS 的设置保持不变，并用于一次性的单调性评估。\n\n每个测试用例需要计算的内容：\n- 对于每个 $\\delta \\in \\{10^{-3}, 1, 10^2\\}$，根据 RLS 仿真结果计算如上定义的 $\\rho_{\\max}$。\n- 根据 LMS 仿真结果，基于蒙特卡洛平均值，计算一个布尔值，该值指示对于所有模态 $k \\in \\{1,2,3,4\\}$，$m_k(t)$ 在窗口 $t \\in \\{0,1,\\dots,T_0\\}$ 上是否关于 t 非增。\n\n最终输出格式：\n- 您的程序必须生成单行输出，包含一个格式为 $[\\rho_{\\max}(\\delta=10^{-3}), \\rho_{\\max}(\\delta=1), \\rho_{\\max}(\\delta=10^{2}), \\mathrm{LMS\\_all\\_modes\\_monotone}]$ 的列表，其中前三项是浮点数，最后一项是布尔值。不应打印任何额外文本。\n\n确保覆盖范围的测试套件摘要：\n- 正常路径：$\\delta=1$，具有中等的初始信息矩阵幅度。\n- 类边界的激进自适应：$\\delta=10^{-3}$，产生大的 $P(0)$，可能导致更大的早期过冲。\n- 类边缘的保守自适应：$\\delta=10^{2}$，产生小的 $P(0)$，早期过冲可能可以忽略。\n- LMS 的单调性布尔值必须在固定的 $\\mu$ 下计算一次，并为所有 $\\delta$ 测试用例共享。\n\n此问题不涉及物理单位或角度单位。所有结果都是无量纲实数或指定的布尔值。", "solution": "该问题是有效的。这是一个适定的、有科学依据的自适应信号处理领域的练习，要求比较最小均方 (LMS) 和递归最小二乘 (RLS) 算法。所有参数和程序都得到了充分详细的说明，并与既有理论和实践一致。对 RLS 初始化参数 $\\delta$ 及其对收敛过冲影响的研究是一个经典且具有启发性的课题。我现在将提供完整的解决方案。\n\n目标是实现并数值比较 LMS 和 RLS 算法在系统辨识任务中的收敛行为。主要焦点是早期性能，特别是量化 RLS 模态误差动力学中的过冲（作为其初始化的函数），并将其与 LMS 在保守步长下的单调收敛进行对比。\n\n此分析的基础是一个线性系统模型，其中期望信号 $d_t$ 由一个真实的未知权重矢量 $\\mathbf{w}^\\star$ 作用于输入回归量矢量 $\\mathbf{x}_t$ 生成，即 $d_t = \\mathbf{x}_t^\\top \\mathbf{w}^\\star$。回归量 $\\mathbf{x}_t$ 从具有已知协方差矩阵 $R$ 的零均值高斯分布中抽取，即 $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{0}, R)$。自适应算法以零权重矢量 $\\mathbf{w}(0) = \\mathbf{0}$ 初始化，旨在通过顺序处理 $(\\mathbf{x}_t, d_t)$ 对来估计 $\\mathbf{w}^\\star$。\n\n性能在由输入协方差矩阵 $R$ 的特征向量定义的模态域中进行评估。由于 $R$被指定为对角矩阵 $R = \\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$，其特征向量是标准基向量，因此模态分解大大简化。第 $k$ 个模态的模态失调为 $z_k(t) = (\\mathbf{w}(t) - \\mathbf{w}^\\star)_k$。性能指标是均方模态失调 $m_k(t) = \\mathbb{E}[z_k(t)^2]$，我们将通过对许多独立的蒙特卡洛试验求平均来近似该值。\n\n仿真将根据以下步骤进行：\n\n1.  **环境与数据生成**：我们首先确定仿真参数：维度数 $n=4$，总迭代次数 $T=200$，早期时间窗口大小 $T_0=30$，以及蒙特卡洛试验次数 $M=200$。固定参数是真实权重矢量 $\\mathbf{w}^\\star = [1, -0.5, 0.25, -0.125]^\\top$ 和输入协方差矩阵 $R = \\Lambda = \\mathrm{diag}(3, 1, 0.3, 0.1)$。为确保可复现性和公平比较，我们使用固定的随机数生成器种子 $s=2025$ 一次性生成 $M$ 个输入向量序列 $\\{\\mathbf{x}_t\\}_{t=0}^{T-1}$ 及对应的期望信号序列 $\\{d_t\\}_{t=0}^{T-1}$。这些数据集将为两种算法及所有参数设置重用。输入向量 $\\mathbf{x}_t$ 通过从 $\\mathbf{g}_t \\sim \\mathcal{N}(\\mathbf{0}, I)$ 采样并设置 $\\mathbf{x}_t = \\Lambda^{1/2}\\mathbf{g}_t$ 来生成。\n\n2.  **LMS 算法仿真与分析**：LMS 算法使用基于瞬时平方误差的简单梯度下降方法来更新权重估计。其更新规则是：\n    $$ \\mathbf{w}(t+1) = \\mathbf{w}(t) + \\mu \\mathbf{x}_t (d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t)) $$\n    我们使用步长 $\\mu=0.05$ 对 $M$ 个数据序列中的每一个模拟此过程。对于每次试验，我们记录在 $i \\in \\{1, \\dots, M\\}$，$t \\in \\{0, \\dots, T\\}$ 和 $k \\in \\{1, \\dots, n\\}$ 下的平方模态失调轨迹 $(z_k^{(i)}(t))^2 = (\\mathbf{w}^{(i)}(t) - \\mathbf{w}^\\star)_k^2$。通过对 $M$ 次试验进行平均，得到估计的均方模态失调曲线 $m_k(t)$。\n    \n    接着，我们评估这些曲线的单调性。对于每个模态 $k$，我们检查 $m_k(t) \\le m_k(t-1)$ 是否对所有 $t \\in \\{1, \\dots, T_0\\}$ 成立。最终结果是一个单一的布尔值 `LMS_all_modes_monotone`，当且仅当所有四个模态在早期时间窗口内都表现出非增的均方失调时，该值为真。\n\n3.  **RLS 算法仿真与分析**：RLS 算法为最小二乘问题提供了一个递归解。我们为遗忘因子 $\\lambda_{\\mathrm{RLS}}=0.995$ 实现指定的更新方程。从初始状态 $\\mathbf{w}(0)=\\mathbf{0}$ 和 $P(0)=\\delta^{-1}I$ 开始，在每个时间步 $t$，算法计算：\n    -   增益矢量：$\\mathbf{k}_t = \\dfrac{P(t-1)\\mathbf{x}_t}{\\lambda_{\\mathrm{RLS}} + \\mathbf{x}_t^\\top P(t-1) \\mathbf{x}_t}$\n    -   权重矢量：$\\mathbf{w}(t) = \\mathbf{w}(t-1) + \\mathbf{k}_t (d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t-1))$\n    -   逆协方差矩阵：$P(t) = \\lambda_{\\mathrm{RLS}}^{-1} (P(t-1) - \\mathbf{k}_t \\mathbf{x}_t^\\top P(t-1))$\n\n    我们对 RLS 算法执行三组独立的仿真，分别对应于初始化参数 $\\delta \\in \\{10^{-3}, 1, 10^2\\}$。对于每个 $\\delta$，我们使用与 LMS 仿真相同的数据运行 $M$ 次试验。与 LMS 一样，我们通过对试验求平均来计算均方模态失调曲线 $m_k(t)$。\n\n    RLS 的关键性能指标是早期过冲因子 $\\rho_{\\max}$。对于每个模态 $k$，我们首先计算初始误差 $m_k(0) = \\mathbb{E}[z_k(0)^2] = \\mathbb{E}[(-w^\\star_k)^2] = (w^\\star_k)^2$。然后，我们找到早期时间窗口内的峰值误差 $\\max_{0 \\le t \\le T_0} m_k(t)$。该模态的过冲因子为：\n    $$ \\rho_k = \\frac{\\max_{0 \\le t \\le T_0} m_k(t)}{m_k(0)} - 1 $$\n    给定 $\\delta$ 的最终指标是所有模态中的最大过冲 $\\rho_{\\max} = \\max_k \\{\\rho_k\\}$。$\\rho_{\\max} > 0$ 的值表示至少有一个模态的均方误差超过了其初始值。\n\n4.  **实现与最终输出**：所述过程在一个 Python 脚本中实现。该脚本首先生成共享数据集，然后运行 LMS 仿真以确定单调性布尔值。随后，它遍历指定的 $\\delta$ 值，为每个值运行 RLS 仿真并计算相应的 $\\rho_{\\max}$。最终结果被收集并以指定的列表格式打印。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares LMS and RLS algorithms for system identification\n    to analyze early-time convergence behavior.\n    \"\"\"\n    # --- Simulation Parameters ---\n    n = 4\n    T = 200\n    T0 = 30\n    M = 200\n    seed = 2025\n    \n    w_star = np.array([1.0, -0.5, 0.25, -0.125])\n    lambda_vals = np.array([3.0, 1.0, 0.3, 0.1])\n    Lambda = np.diag(lambda_vals)\n    Lambda_sqrt = np.diag(np.sqrt(lambda_vals))\n    \n    mu_lms = 0.05\n    lambda_rls = 0.995\n    delta_values = [1e-3, 1.0, 1e2]\n    \n    # --- Data Generation ---\n    rng = np.random.default_rng(seed)\n    \n    # Generate M sets of data sequences, each of length T\n    # x_mc shape: (M, T, n), d_mc shape: (M, T)\n    g_mc = rng.standard_normal(size=(M, T, n))\n    x_mc = np.einsum('ij,ktj->kti', Lambda_sqrt, g_mc)\n    d_mc = np.einsum('kti,i->kt', x_mc, w_star)\n    \n    # --- LMS Simulation and Analysis ---\n    \n    # Store squared modal error for each trial\n    # mse_lms shape: (M, T+1, n)\n    mse_lms = np.zeros((M, T + 1, n))\n    \n    for i in range(M):\n        w = np.zeros(n)\n        initial_error_sq = (w - w_star)**2\n        mse_lms[i, 0, :] = initial_error_sq\n        \n        for t in range(T):\n            xt = x_mc[i, t, :]\n            dt = d_mc[i, t]\n            \n            error_signal = dt - xt.T @ w\n            w = w + mu_lms * xt * error_signal\n            \n            modal_error_sq = (w - w_star)**2\n            mse_lms[i, t + 1, :] = modal_error_sq\n            \n    # Average over Monte Carlo trials to get m_k(t)\n    m_k_lms = np.mean(mse_lms, axis=0) # Shape: (T+1, n)\n    \n    # Check for monotonicity in all modes over t in [0, T0]\n    lms_all_modes_monotone = True\n    for k in range(n):\n        for t in range(1, T0 + 1):\n            if m_k_lms[t, k] > m_k_lms[t-1, k]:\n                # Use a small tolerance for floating point comparisons\n                if not np.isclose(m_k_lms[t, k], m_k_lms[t-1, k]):\n                    lms_all_modes_monotone = False\n                    break\n        if not lms_all_modes_monotone:\n            break\n            \n    # --- RLS Simulation and Analysis ---\n    \n    rls_rho_max_results = []\n    \n    for delta in delta_values:\n        # mse_rls shape: (M, T+1, n)\n        # The first arugment of range for time is 1, so the first update is at index 1\n        # To align with LMS (0 to T), we use slightly different indexing logic from problem\n        # The logic here is: w(t+1) is updated from w(t) using x(t), d(t)\n        \n        mse_rls = np.zeros((M, T + 1, n))\n\n        for i in range(M):\n            w = np.zeros(n)\n            P = (1.0 / delta) * np.identity(n)\n            \n            initial_error_sq = (w - w_star)**2\n            mse_rls[i, 0, :] = initial_error_sq\n            \n            for t in range(T):\n                xt = x_mc[i, t, :]\n                dt = d_mc[i, t]\n                \n                # RLS update equations re-indexed for a causal loop t=0..T-1\n                # producing w(t+1) from w(t), P(t) and x(t),d(t)\n                den = lambda_rls + xt.T @ P @ xt\n                k = (P @ xt) / den\n                \n                error_signal = dt - xt.T @ w\n                w = w + k * error_signal\n                P = (1.0 / lambda_rls) * (P - np.outer(k, xt) @ P)\n\n                modal_error_sq = (w - w_star)**2\n                mse_rls[i, t + 1, :] = modal_error_sq\n\n        # Average over Monte Carlo trials\n        m_k_rls = np.mean(mse_rls, axis=0) # Shape: (T+1, n)\n        \n        # Calculate overshoot factor rho_k for each mode\n        rho_k_list = []\n        for k in range(n):\n            m_k_0 = (w_star[k])**2\n            if m_k_0 == 0:\n                # If initial error is zero, any deviation is infinite overshoot.\n                # Avoid division by zero. This shouldn't happen with the given w_star.\n                max_val = np.max(m_k_rls[0 : T0 + 1, k])\n                rho_k = np.inf if max_val > 0 else 0.0\n            else:\n                max_m_k_t0 = np.max(m_k_rls[0 : T0 + 1, k])\n                rho_k = (max_m_k_t0 / m_k_0) - 1.0\n            rho_k_list.append(rho_k)\n            \n        rho_max = np.max(rho_k_list)\n        rls_rho_max_results.append(rho_max)\n        \n    # --- Final Output ---\n    final_results = rls_rho_max_results + [lms_all_modes_monotone]\n    print(f\"[{final_results[0]},{final_results[1]},{final_results[2]},{final_results[3]}]\")\n\nsolve()\n```", "id": "2891050"}]}