{
    "hands_on_practices": [
        {
            "introduction": "Understanding any new concept begins with mastering the fundamentals. This first exercise provides a crucial baseline by applying the definition of quantum mutual information to the simplest possible case: a system composed of two completely uncorrelated parts . By working from first principles, you will verify that for a product state, which by definition contains no correlations, the quantum mutual information is zero, solidifying your grasp of this foundational measure.",
            "id": "3780239",
            "problem": "Consider two finite-dimensional quantum systems $A$ and $B$ with Hamiltonians $H_{A}$ and $H_{B}$, respectively. Each system is weakly coupled to its own heat bath and relaxes to a local Gibbs state at inverse temperatures $\\beta_{A}$ and $\\beta_{B}$. The resulting marginal states are $ \\rho_{A} = \\exp(-\\beta_{A} H_{A})/Z_{A} $ and $ \\rho_{B} = \\exp(-\\beta_{B} H_{B})/Z_{B} $, where $Z_{A}$ and $Z_{B}$ are the partition functions. The composite state is the product state $ \\rho_{AB} = \\rho_{A} \\otimes \\rho_{B} $, which is diagonal in the joint energy eigenbasis, and can be written as\n$$\n\\rho_{AB} = \\sum_{a,b} p_{a} \\, q_{b} \\, |a\\rangle\\langle a| \\otimes |b\\rangle\\langle b| ,\n$$\nwith $p_{a} = \\exp(-\\beta_{A} E_{a})/Z_{A}$ and $q_{b} = \\exp(-\\beta_{B} F_{b})/Z_{B}$, where $E_{a}$ and $F_{b}$ are the energy eigenvalues of $H_{A}$ and $H_{B}$, respectively. Define classical random variables $A_{\\mathrm{cl}}$ and $B_{\\mathrm{cl}}$ as the outcomes of projective energy measurements on $A$ and $B$, yielding the joint classical distribution $p_{ab} = p_{a} q_{b}$.\n\nUsing only fundamental definitions, compute the quantum mutual information $I(A\\!:\\!B)_{\\rho}$ of the state $\\rho_{AB}$ in natural units (nats), and compare it to the classical mutual information of $(A_{\\mathrm{cl}}, B_{\\mathrm{cl}})$ computed using the same natural logarithm base. Provide the final answer as a single real number. No rounding is required.",
            "solution": "The problem requires the calculation of the quantum mutual information for a bipartite state $\\rho_{AB}$ and the classical mutual information for a corresponding joint probability distribution $p_{ab}$.\n\nFirst, we address the quantum mutual information, $I(A\\!:\\!B)_{\\rho}$. The definition of quantum mutual information for a bipartite state $\\rho_{AB}$ with marginals $\\rho_{A} = \\operatorname{tr}_{B}(\\rho_{AB})$ and $\\rho_{B} = \\operatorname{tr}_{A}(\\rho_{AB})$ is given by:\n$$\nI(A\\!:\\!B)_{\\rho} = S(\\rho_{A}) + S(\\rho_{B}) - S(\\rho_{AB})\n$$\nwhere $S(\\sigma) = -\\operatorname{tr}(\\sigma \\ln \\sigma)$ is the von Neumann entropy, calculated using the natural logarithm.\n\nThe problem states that the composite system's state is the product state $\\rho_{AB} = \\rho_{A} \\otimes \\rho_{B}$. We will now demonstrate a fundamental property of the von Neumann entropy for product states.\nLet $\\rho_{AB} = \\rho_{A} \\otimes \\rho_{B}$. The logarithm of this operator is given by:\n$$\n\\ln(\\rho_{AB}) = \\ln(\\rho_{A} \\otimes \\rho_{B}) = (\\ln \\rho_{A}) \\otimes I_{B} + I_{A} \\otimes (\\ln \\rho_{B})\n$$\nwhere $I_{A}$ and $I_{B}$ are the identity operators on the Hilbert spaces of systems $A$ and $B$, respectively.\n\nNow, we compute the term $\\rho_{AB} \\ln \\rho_{AB}$:\n$$\n\\rho_{AB} \\ln \\rho_{AB} = (\\rho_{A} \\otimes \\rho_{B}) \\left( (\\ln \\rho_{A}) \\otimes I_{B} + I_{A} \\otimes (\\ln \\rho_{B}) \\right)\n$$\nUsing the property $(M_1 \\otimes M_2)(N_1 \\otimes N_2) = (M_1 N_1) \\otimes (M_2 N_2)$, we get:\n$$\n\\rho_{AB} \\ln \\rho_{AB} = (\\rho_{A} \\ln \\rho_{A}) \\otimes (\\rho_{B} I_{B}) + (\\rho_{A} I_{A}) \\otimes (\\rho_{B} \\ln \\rho_{B})\n$$\n$$\n\\rho_{AB} \\ln \\rho_{AB} = (\\rho_{A} \\ln \\rho_{A}) \\otimes \\rho_{B} + \\rho_{A} \\otimes (\\rho_{B} \\ln \\rho_{B})\n$$\nTo find the entropy $S(\\rho_{AB})$, we take the negative of the trace of this expression. Using the property $\\operatorname{tr}(M \\otimes N) = \\operatorname{tr}(M)\\operatorname{tr}(N)$, we have:\n$$\n\\operatorname{tr}(\\rho_{AB} \\ln \\rho_{AB}) = \\operatorname{tr}\\left( (\\rho_{A} \\ln \\rho_{A}) \\otimes \\rho_{B} \\right) + \\operatorname{tr}\\left( \\rho_{A} \\otimes (\\rho_{B} \\ln \\rho_{B}) \\right)\n$$\n$$\n\\operatorname{tr}(\\rho_{AB} \\ln \\rho_{AB}) = \\operatorname{tr}(\\rho_{A} \\ln \\rho_{A}) \\operatorname{tr}(\\rho_{B}) + \\operatorname{tr}(\\rho_{A}) \\operatorname{tr}(\\rho_{B} \\ln \\rho_{B})\n$$\nSince $\\rho_{A}$ and $\\rho_{B}$ are density matrices, their traces are equal to $1$, i.e., $\\operatorname{tr}(\\rho_{A}) = 1$ and $\\operatorname{tr}(\\rho_{B}) = 1$. Substituting these values:\n$$\n\\operatorname{tr}(\\rho_{AB} \\ln \\rho_{AB}) = \\operatorname{tr}(\\rho_{A} \\ln \\rho_{A}) \\cdot 1 + 1 \\cdot \\operatorname{tr}(\\rho_{B} \\ln \\rho_{B}) = \\operatorname{tr}(\\rho_{A} \\ln \\rho_{A}) + \\operatorname{tr}(\\rho_{B} \\ln \\rho_{B})\n$$\nMultiplying by $-1$ gives the relationship for the entropies:\n$$\nS(\\rho_{AB}) = S(\\rho_{A}) + S(\\rho_{B})\n$$\nThis additivity of von Neumann entropy holds for any product state.\n\nNow, we can compute the quantum mutual information for the given state $\\rho_{AB} = \\rho_{A} \\otimes \\rho_{B}$:\n$$\nI(A\\!:\\!B)_{\\rho} = S(\\rho_{A}) + S(\\rho_{B}) - S(\\rho_{AB}) = S(\\rho_{A}) + S(\\rho_{B}) - (S(\\rho_{A}) + S(\\rho_{B})) = 0\n$$\nThus, the quantum mutual information of the product state $\\rho_{AB}$ is exactly $0$. This result reflects the fact that a product state contains no quantum correlations between its subsystems $A$ and $B$.\n\nNext, we perform the comparison with the classical mutual information, $I(A_{\\mathrm{cl}}:B_{\\mathrm{cl}})$. The classical random variables $A_{\\mathrm{cl}}$ and $B_{\\mathrm{cl}}$ have a joint probability distribution $p_{ab} = p_{a} q_{b}$. The definition of classical mutual information is:\n$$\nI(A_{\\mathrm{cl}}:B_{\\mathrm{cl}}) = H(A_{\\mathrm{cl}}) + H(B_{\\mathrm{cl}}) - H(A_{\\mathrm{cl}}, B_{\\mathrm{cl}})\n$$\nwhere $H(X) = -\\sum_{x} p(x) \\ln p(x)$ is the Shannon entropy. The marginal probabilities are $p(a) = \\sum_{b} p_{ab} = \\sum_{b} p_{a}q_{b} = p_{a}\\sum_{b}q_{b} = p_{a}$, and similarly $p(b) = \\sum_{a} p_{ab} = q_{b}$. These are the distributions given in the problem statement. The corresponding Shannon entropies are:\n$$\nH(A_{\\mathrm{cl}}) = -\\sum_{a} p_{a} \\ln p_{a}\n$$\n$$\nH(B_{\\mathrm{cl}}) = -\\sum_{b} q_{b} \\ln q_{b}\n$$\nThe joint entropy $H(A_{\\mathrm{cl}}, B_{\\mathrm{cl}})$ is:\n$$\nH(A_{\\mathrm{cl}}, B_{\\mathrm{cl}}) = -\\sum_{a,b} p_{ab} \\ln p_{ab}\n$$\nSince the joint distribution is a product, $p_{ab} = p_{a}q_{b}$, the random variables $A_{\\mathrm{cl}}$ and $B_{\\mathrm{cl}}$ are statistically independent. We can calculate the joint entropy as follows:\n$$\nH(A_{\\mathrm{cl}}, B_{\\mathrm{cl}}) = -\\sum_{a,b} (p_{a}q_{b}) \\ln(p_{a}q_{b}) = -\\sum_{a,b} p_{a}q_{b} (\\ln p_{a} + \\ln q_{b})\n$$\n$$\nH(A_{\\mathrm{cl}}, B_{\\mathrm{cl}}) = -\\sum_{a,b} p_{a}q_{b} \\ln p_{a} - \\sum_{a,b} p_{a}q_{b} \\ln q_{b}\n$$\nWe can separate the sums:\n$$\nH(A_{\\mathrm{cl}}, B_{\\mathrm{cl}}) = -\\left(\\sum_{a} p_{a} \\ln p_{a}\\right)\\left(\\sum_{b} q_{b}\\right) - \\left(\\sum_{a} p_{a}\\right)\\left(\\sum_{b} q_{b} \\ln q_{b}\\right)\n$$\nSince $\\sum_{a} p_{a} = 1$ and $\\sum_{b} q_{b} = 1$:\n$$\nH(A_{\\mathrm{cl}}, B_{\\mathrm{cl}}) = -\\left(\\sum_{a} p_{a} \\ln p_{a}\\right) \\cdot 1 - 1 \\cdot \\left(\\sum_{b} q_{b} \\ln q_{b}\\right)\n$$\n$$\nH(A_{\\mathrm{cl}}, B_{\\mathrm{cl}}) = \\left(-\\sum_{a} p_{a} \\ln p_{a}\\right) + \\left(-\\sum_{b} q_{b} \\ln q_{b}\\right) = H(A_{\\mathrm{cl}}) + H(B_{\\mathrm{cl}})\n$$\nThis is the additivity property of Shannon entropy for independent random variables.\n\nNow, we substitute this result into the formula for classical mutual information:\n$$\nI(A_{\\mathrm{cl}}:B_{\\mathrm{cl}}) = H(A_{\\mathrm{cl}}) + H(B_{\\mathrm{cl}}) - H(A_{\\mathrm{cl}}, B_{\\mathrm{cl}}) = H(A_{\\mathrm{cl}}) + H(B_{\\mathrm{cl}}) - (H(A_{\\mathrm{cl}}) + H(B_{\\mathrm{cl}})) = 0\n$$\nThe classical mutual information is also exactly $0$.\n\nThe comparison shows that both the quantum and classical mutual information are $0$. This is the expected result, as the quantum state is a product state (implying no quantum correlations) and the corresponding classical probability distribution describes two independent random variables (implying no classical correlations). The problem asks for the numerical value of the quantum mutual information.\n\nThe final answer is $0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "Quantum mutual information quantifies the total correlation between two systems, but it does not distinguish between classical-like correlations and genuine quantum entanglement. This practice delves into this crucial distinction by analyzing a physically relevant two-mode squeezed thermal state . You will compute both the total correlation using mutual information and the entanglement using logarithmic negativity, allowing you to identify the precise conditions under which the interaction generates non-classical correlations.",
            "id": "3780221",
            "problem": "Consider two bosonic modes $A$ and $B$ modeled as quantum harmonic oscillators in the continuous-variable framework, with canonical quadratures collected in the phase-space vector $R=(q_{A},p_{A},q_{B},p_{B})^{\\top}$ and the canonical commutation relations $[q_{j},p_{k}]=\\mathrm{i}\\delta_{jk}$ in units where $\\hbar=1$. Each mode is initially in a thermal state with mean occupancy $\\bar{n}\\geq 0$, so the initial covariance matrix is $V_{0}=\\mathrm{diag}(v\\,\\mathbb{I}_{2},v\\,\\mathbb{I}_{2})$, where $v=\\bar{n}+\\tfrac{1}{2}$ and $\\mathbb{I}_{2}$ denotes the $2\\times 2$ identity matrix. The two modes are then coupled by a unitary Gaussian interaction generated by two-mode squeezing with real parameter $r\\geq 0$, represented at the covariance level by the symplectic transformation $S(r)$ acting as $V=S(r)V_{0}S(r)^{\\top}$, where\n$$\nS(r)=\\begin{pmatrix}\n\\cosh r\\,\\mathbb{I}_{2}  \\sinh r\\,Z \\\\\n\\sinh r\\,Z  \\cosh r\\,\\mathbb{I}_{2}\n\\end{pmatrix},\\qquad Z=\\mathrm{diag}(1,-1).\n$$\nThe resulting two-mode Gaussian state $V$ is a symmetric two-mode squeezed thermal state. Using first principles for Gaussian quantum states in open quantum systems and quantum thermodynamics, do the following:\n\n- Starting from the definitions of quantum mutual information $I(A\\!:\\!B)=S(\\rho_{A})+S(\\rho_{B})-S(\\rho_{AB})$ and logarithmic negativity $E_{N}$ obtained from the smallest symplectic eigenvalue of the partially transposed covariance matrix, derive explicit expressions for $I(A\\!:\\!B)$ and $E_{N}$ as functions of $\\bar{n}$ and $r$ for the above state. Discuss how these quantify total correlations and quantum (entanglement) correlations, respectively.\n\n- Determine the exact threshold value of the two-mode squeezing parameter $r=r_{c}(\\bar{n})$ at which the logarithmic negativity becomes strictly positive. Using this threshold, identify and explain the parameter regimes in which the correlations are predominantly classical versus genuinely quantum.\n\nYour final answer must be a single closed-form analytic expression for $r_{c}(\\bar{n})$. No numerical approximation is required.",
            "solution": "The problem asks for the derivation of two correlation measures, the quantum mutual information $I(A\\!:\\!B)$ and the logarithmic negativity $E_N$, for a two-mode squeezed thermal state. It also asks for the threshold squeezing parameter $r_c(\\bar{n})$ at which entanglement appears.\n\nThe first step is to determine the covariance matrix (CM) of the final state. The initial state is a product of two identical thermal states, each with mean occupancy $\\bar{n} \\geq 0$. The CM of a single thermal mode is $(\\bar{n}+\\frac{1}{2})\\mathbb{I}_{2}$. We define $v = \\bar{n}+\\frac{1}{2}$. The initial $4 \\times 4$ CM for the two-mode system $(A, B)$ is given as $V_0=\\mathrm{diag}(v\\,\\mathbb{I}_{2},v\\,\\mathbb{I}_{2})$, which can be written in block form as:\n$$\nV_0 = \\begin{pmatrix} v\\,\\mathbb{I}_{2}  0 \\\\ 0  v\\,\\mathbb{I}_{2} \\end{pmatrix}\n$$\nThis state is subjected to a two-mode squeezing interaction, which is a unitary Gaussian operation. At the level of the CM, this corresponds to a symplectic transformation $V = S(r)V_{0}S(r)^{\\top}$. The symplectic matrix for a two-mode squeezing operation is given by:\n$$\nS(r)=\\begin{pmatrix}\n\\cosh r\\,\\mathbb{I}_{2}  \\sinh r\\,Z \\\\\n\\sinh r\\,Z  \\cosh r\\,\\mathbb{I}_{2}\n\\end{pmatrix}\n$$\nwhere $r \\geq 0$ is the squeezing parameter and $Z=\\mathrm{diag}(1,-1)$. We compute the final CM, $V$:\n$$\nV = S(r)V_{0}S(r)^{\\top} = \\begin{pmatrix} \\cosh r\\,\\mathbb{I}_{2}  \\sinh r\\,Z \\\\ \\sinh r\\,Z  \\cosh r\\,\\mathbb{I}_{2} \\end{pmatrix} \\begin{pmatrix} v\\,\\mathbb{I}_{2}  0 \\\\ 0  v\\,\\mathbb{I}_{2} \\end{pmatrix} \\begin{pmatrix} \\cosh r\\,\\mathbb{I}_{2}  \\sinh r\\,Z \\\\ \\sinh r\\,Z  \\cosh r\\,\\mathbb{I}_{2} \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\nV = \\begin{pmatrix} v\\,\\cosh r\\,\\mathbb{I}_{2}  v\\,\\sinh r\\,Z \\\\ v\\,\\sinh r\\,Z  v\\,\\cosh r\\,\\mathbb{I}_{2} \\end{pmatrix} \\begin{pmatrix} \\cosh r\\,\\mathbb{I}_{2}  \\sinh r\\,Z \\\\ \\sinh r\\,Z  \\cosh r\\,\\mathbb{I}_{2} \\end{pmatrix}\n$$\n$$\nV = \\begin{pmatrix} v\\,\\cosh^{2} r\\,\\mathbb{I}_{2} + v\\,\\sinh^{2} r\\,Z^{2}  v\\,\\cosh r \\sinh r\\,\\mathbb{I}_{2}Z + v\\,\\sinh r \\cosh r\\,Z \\\\ v\\,\\sinh r \\cosh r\\,Z  v\\,\\sinh^{2} r\\,Z^{2} + v\\,\\cosh^{2} r\\,\\mathbb{I}_{2} \\end{pmatrix}\n$$\nUsing the identities $Z^2 = \\mathbb{I}_{2}$, $\\cosh^2 r + \\sinh^2 r = \\cosh(2r)$, and $2\\sinh r \\cosh r = \\sinh(2r)$, we simplify the CM to:\n$$\nV = \\begin{pmatrix} v\\,\\cosh(2r)\\,\\mathbb{I}_{2}  v\\,\\sinh(2r)\\,Z \\\\ v\\,\\sinh(2r)\\,Z  v\\,\\cosh(2r)\\,\\mathbb{I}_{2} \\end{pmatrix}\n$$\nThis is the CM of a symmetric two-mode squeezed thermal state. The diagonal blocks $V_A = V_B = v\\,\\cosh(2r)\\,\\mathbb{I}_{2}$ are the CMs of the reduced states of modes $A$ and $B$, respectively.\n\n**Quantum Mutual Information, $I(A\\!:\\!B)$**\n\nThe quantum mutual information is defined as $I(A\\!:\\!B) = S(\\rho_A) + S(\\rho_B) - S(\\rho_{AB})$, where $S(\\rho) = -\\mathrm{tr}(\\rho \\ln \\rho)$ is the von Neumann entropy.\nFor a Gaussian state, the entropy is determined by its symplectic eigenvalues. For a single-mode Gaussian state with CM $V_s$, its single symplectic eigenvalue is $\\nu_s = \\sqrt{\\det V_s}$. The entropy is given by the function $h(\\nu_s) = (\\nu_s+\\frac{1}{2})\\ln(\\nu_s+\\frac{1}{2}) - (\\nu_s-\\frac{1}{2})\\ln(\\nu_s-\\frac{1}{2})$.\nThe total state $\\rho_{AB}$ is obtained by a unitary evolution from the initial state $\\rho_0$. Since entropy is invariant under unitary transformations, $S(\\rho_{AB}) = S(\\rho_0)$. The initial state is a product of two identical thermal states, $\\rho_0 = \\rho_{A,th} \\otimes \\rho_{B,th}$. Thus, $S(\\rho_{AB}) = S(\\rho_{A,th}) + S(\\rho_{B,th})$.\nThe initial CM for each mode is $v\\,\\mathbb{I}_{2}$. The symplectic eigenvalue is $v = \\bar{n}+\\frac{1}{2}$. The entropy of each initial mode is $S(\\rho_{A,th}) = S(\\rho_{B,th}) = h(v)$.\nTherefore, the total entropy is $S(\\rho_{AB}) = 2h(v)$.\n\nThe reduced states on modes $A$ and $B$ are identical due to the symmetry of the state. Their CM is $V_A = V_B = v\\,\\cosh(2r)\\,\\mathbb{I}_{2}$. This corresponds to a thermal state. The symplectic eigenvalue of the reduced state $\\rho_A$ is $\\nu_A = \\sqrt{\\det V_A} = v\\,\\cosh(2r)$.\nThe entropy of the reduced state is $S(\\rho_A) = S(\\rho_B) = h(\\nu_A) = h(v\\,\\cosh(2r))$.\nNow, we can compute the mutual information:\n$$\nI(A\\!:\\!B) = S(\\rho_A) + S(\\rho_B) - S(\\rho_{AB}) = 2h(v\\,\\cosh(2r)) - 2h(v)\n$$\nSubstituting $v=\\bar{n}+\\frac{1}{2}$ and the expression for $h(x)$:\n$$\nI(A\\!:\\!B) = 2 \\left[ h\\left(\\left(\\bar{n}+\\frac{1}{2}\\right)\\cosh(2r)\\right) - h\\left(\\bar{n}+\\frac{1}{2}\\right) \\right]\n$$\nThis quantity measures the total correlation (both classical and quantum) between the two modes. Since $r \\geq 0$, $\\cosh(2r) \\geq 1$. The function $h(x)$ is monotonically increasing for $x \\geq \\frac{1}{2}$. Thus, $I(A\\!:\\!B) \\geq 0$, and it is strictly positive for any $r  0$.\n\n**Logarithmic Negativity, $E_N$**\n\nLogarithmic negativity quantifies entanglement. For Gaussian states, it is computed from the smallest symplectic eigenvalue, $\\tilde{\\nu}_-$, of the partially transposed CM, $\\tilde{V}$. A state is entangled if and only if $\\tilde{\\nu}_-  \\frac{1}{2}$ (the Simon criterion). The logarithmic negativity is defined as $E_N = \\max(0, -\\ln(2\\tilde{\\nu}_-))$.\nPartial transposition with respect to mode $B$ corresponds to the transformation $p_B \\to -p_B$. This is represented by the matrix $\\Lambda = \\mathrm{diag}(1,1,1,-1) = \\begin{pmatrix} \\mathbb{I}_{2}  0 \\\\ 0  Z \\end{pmatrix}$.\nThe partially transposed CM is $\\tilde{V} = \\Lambda V \\Lambda$.\n$$\n\\tilde{V} = \\begin{pmatrix} \\mathbb{I}_{2}  0 \\\\ 0  Z \\end{pmatrix} \\begin{pmatrix} v\\,\\cosh(2r)\\,\\mathbb{I}_{2}  v\\,\\sinh(2r)\\,Z \\\\ v\\,\\sinh(2r)\\,Z  v\\,\\cosh(2r)\\,\\mathbb{I}_{2} \\end{pmatrix} \\begin{pmatrix} \\mathbb{I}_{2}  0 \\\\ 0  Z \\end{pmatrix}\n$$\nMultiplying the matrices and using the property $Z^2=\\mathbb{I}_{2}$, we get:\n$$\n\\tilde{V} = \\begin{pmatrix} v\\,\\cosh(2r)\\,\\mathbb{I}_{2}  v\\,\\sinh(2r)\\,\\mathbb{I}_{2} \\\\ v\\,\\sinh(2r)\\,\\mathbb{I}_{2}  v\\,\\cosh(2r)\\,\\mathbb{I}_{2} \\end{pmatrix}\n$$\nTo find the symplectic eigenvalues of $\\tilde{V}$, we compute two invariants:\n$\\det \\tilde{V} = (v^2\\cosh^2(2r) - v^2\\sinh^2(2r))^2 = (v^2(\\cosh^2(2r)-\\sinh^2(2r)))^2 = (v^2)^2 = v^4$.\nThe other invariant, $\\Delta(\\tilde{V})$, is $\\det \\tilde{A} + \\det \\tilde{B} + 2\\det \\tilde{C}$, where $\\tilde{A}, \\tilde{B}, \\tilde{C}$ are the blocks of $\\tilde{V}$.\n$\\tilde{A} = \\tilde{B} = v\\,\\cosh(2r)\\,\\mathbb{I}_{2}$, $\\tilde{C} = v\\,\\sinh(2r)\\,\\mathbb{I}_{2}$.\n$\\det \\tilde{A} = \\det \\tilde{B} = v^2\\cosh^2(2r)$, $\\det \\tilde{C} = v^2\\sinh^2(2r)$.\n$\\Delta(\\tilde{V}) = 2v^2\\cosh^2(2r) + 2v^2\\sinh^2(2r) = 2v^2(\\cosh^2(2r)+\\sinh^2(2r)) = 2v^2\\cosh(4r)$.\nThe symplectic eigenvalues $\\tilde{\\nu}_\\pm$ are given by $\\tilde{\\nu}_\\pm^2 = \\frac{\\Delta(\\tilde{V}) \\pm \\sqrt{\\Delta(\\tilde{V})^2 - 4\\det \\tilde{V}}}{2}$:\n$$\n\\tilde{\\nu}_\\pm^2 = \\frac{2v^2\\cosh(4r) \\pm \\sqrt{4v^4\\cosh^2(4r) - 4v^4}}{2} = v^2\\cosh(4r) \\pm v^2\\sqrt{\\cosh^2(4r)-1}\n$$\n$$\n\\tilde{\\nu}_\\pm^2 = v^2\\cosh(4r) \\pm v^2\\sinh(4r) = v^2 e^{\\pm 4r}\n$$\nThe symplectic eigenvalues are $\\tilde{\\nu}_\\pm = v e^{\\pm 2r}$. The smallest is $\\tilde{\\nu}_- = v e^{-2r} = (\\bar{n}+\\frac{1}{2})e^{-2r}$.\nThe logarithmic negativity is then:\n$$\nE_N = \\max(0, -\\ln(2\\tilde{\\nu}_-)) = \\max\\left(0, -\\ln\\left(2\\left(\\bar{n}+\\frac{1}{2}\\right)e^{-2r}\\right)\\right)\n$$\n$$\nE_N = \\max\\left(0, -\\ln\\left((2\\bar{n}+1)e^{-2r}\\right)\\right) = \\max(0, -\\ln(2\\bar{n}+1) + 2r)\n$$\nThis expression provides a measure of the distillable entanglement in the state. If $E_N  0$, the state is entangled.\n\n**Entanglement Threshold and Correlation Regimes**\n\nLogarithmic negativity becomes strictly positive, indicating the presence of entanglement, when $2r - \\ln(2\\bar{n}+1)  0$. The threshold value of the squeezing parameter, $r_c(\\bar{n})$, is found by setting this expression to zero:\n$$\n2r_c(\\bar{n}) - \\ln(2\\bar{n}+1) = 0\n$$\n$$\nr_c(\\bar{n}) = \\frac{1}{2}\\ln(2\\bar{n}+1)\n$$\nThis result allows us to distinguish two regimes of correlations:\n1.  **Classical Correlation Regime ($0  r \\leq r_c(\\bar{n})$)**: In this regime, $I(A\\!:\\!B)  0$ but $E_N = 0$. The two modes are correlated, but these correlations are not strong enough to be classified as quantum entanglement. The state is separable. The correlations are said to be classical because they can, in principle, be simulated by local operations on the two modes assisted by classical communication.\n2.  **Quantum Correlation Regime ($r  r_c(\\bar{n})$)**: In this regime, both $I(A\\!:\\!B)  0$ and $E_N  0$. The state possesses genuinely quantum correlations, i.e., it is entangled and non-separable. The two-mode squeezing has overcome the initial thermal noise (mixedness) of the modes. The threshold $r_c(\\bar{n})$ increases with $\\bar{n}$, which signifies that a stronger squeezing interaction is required to entangle modes that are initially more mixed. For the case of initial vacuum states ($\\bar{n}=0$), the threshold is $r_c(0) = \\frac{1}{2}\\ln(1)=0$, meaning any amount of squeezing ($r0$) creates entanglement.",
            "answer": "$$\n\\boxed{\\frac{1}{2}\\ln(2\\bar{n}+1)}\n$$"
        },
        {
            "introduction": "Beyond simply quantifying static correlations, information-theoretic measures have profound operational consequences for quantum processes. This exercise explores the connection between conditional mutual information, $I(A:C|B)$, and the task of quantum state recovery . By analyzing a specific tripartite state, you will see how this quantity acts as an obstruction to reversing local evolution, providing a tangible link between abstract information theory and the dynamics of open quantum systems.",
            "id": "3780232",
            "problem": "Consider a tripartite, classically correlated quantum state on systems $A$, $B$, and $C$ with Hilbert spaces of equal dimension $d=16$. The state is diagonal in a product basis and is specified by the following classical joint distribution: draw $A$ and $C$ independently and uniformly from $\\{0,1,\\dots,d-1\\}$, and set $B=(A-C) \\bmod d$. Let $\\rho_{ABC}$ be the corresponding diagonal density operator on $\\mathcal{H}_{A} \\otimes \\mathcal{H}_{B} \\otimes \\mathcal{H}_{C}$.\n\nWork in base-$2$ logarithms for all entropies, so that entropies are measured in bits. Use only fundamental definitions of quantum entropy and quantum mutual information for classical diagonal states, and standard facts from the recoverability theory connecting conditional mutual information to optimal recovery fidelity from $B$ to $BC$ (do not assume any special structure of a recovery map beyond complete positivity and trace preservation).\n\nDefine the conditional mutual information $I(A:C|B)$ of $\\rho_{ABC}$ and compute its value for the state described. Then, invoking the established lower bound that relates $I(A:C|B)$ to the optimal recovery fidelity $F_{\\star}$ (the supremum, over all completely positive trace-preserving maps $\\mathcal{R}_{B \\to BC}$, of the Uhlmann fidelity between $\\rho_{ABC}$ and $(\\mathrm{id}_{A}\\otimes \\mathcal{R}_{B \\to BC})(\\rho_{AB})$), determine the guaranteed lower bound $F_{\\mathrm{lb}}$ on $F_{\\star}$ for this state.\n\nFinally, define the bound-predicted fidelity shortfall $\\Delta \\equiv 1 - F_{\\mathrm{lb}}$ from perfect recovery, and evaluate $\\Delta$ exactly for $d=16$. Express your final result as a single exact number. No rounding is required, and no physical units are needed.",
            "solution": "The problem statement is confirmed to be valid as it is scientifically grounded in quantum information theory, well-posed, objective, and internally consistent. We may proceed with the solution.\n\nThe problem describes a tripartite quantum state $\\rho_{ABC}$ on a system with Hilbert space $\\mathcal{H}_{A} \\otimes \\mathcal{H}_{B} \\otimes \\mathcal{H}_{C}$, where each subsystem has a dimension of $d=16$. The state is diagonal in a product basis, which means it is a classical-quantum state. Its properties are entirely determined by a classical probability distribution $p(a,b,c)$ over variables $A, B, C$ taking values in $\\{0, 1, \\dots, d-1\\}$. The state is given by\n$$ \\rho_{ABC} = \\sum_{a,b,c=0}^{d-1} p(a,b,c) |a\\rangle\\langle a| \\otimes |b\\rangle\\langle b| \\otimes |c\\rangle\\langle c| $$\nThe distribution is defined as follows: $A$ and $C$ are independent random variables drawn uniformly from $\\{0, 1, \\dots, d-1\\}$, and $B$ is determined by the relation $B=(A-C) \\bmod d$.\n\nFirst, we formalize the joint probability distribution $p(a,b,c)$.\nThe probability of drawing a specific value $a$ for $A$ is $p(a) = \\frac{1}{d}$.\nThe probability of drawing a specific value $c$ for $C$ is $p(c) = \\frac{1}{d}$.\nSince $A$ and $C$ are independent, their joint probability is $p(a,c) = p(a)p(c) = \\frac{1}{d^2}$.\nThe variable $B$ is a deterministic function of $A$ and $C$, so the conditional probability $p(b|a,c)$ is given by a Kronecker delta: $p(b|a,c) = \\delta_{b, (a-c) \\bmod d}$.\nThe full joint probability distribution is therefore:\n$$ p(a,b,c) = p(a,c) p(b|a,c) = \\frac{1}{d^2} \\delta_{b, (a-c) \\bmod d} $$\nThis distribution is non-zero only when $b = (a-c) \\bmod d$. There are $d^2$ such triplets $(a,b,c)$, since for each of the $d^2$ pairs of $(a,c)$, the value of $b$ is uniquely fixed. Each of these non-zero probability entries is $\\frac{1}{d^2}$.\n\nTo compute the conditional mutual information $I(A:C|B)$, we use the formula involving von Neumann entropies:\n$$ I(A:C|B) = S(\\rho_{AB}) + S(\\rho_{BC}) - S(\\rho_{B}) - S(\\rho_{ABC}) $$\nSince the state is classical (diagonal), the von Neumann entropy of any of its marginals is equal to the Shannon entropy of the corresponding classical probability distribution. We must compute the marginal distributions first.\n\nThe marginal distribution for $B$ is $p(b) = \\sum_{a,c} p(a,b,c) = \\sum_{a,c} \\frac{1}{d^2} \\delta_{b, (a-c) \\bmod d}$. For a fixed $b$, we need to count the pairs $(a,c)$ satisfying $a-c \\equiv b \\pmod d$. For each choice of $a$ (of which there are $d$), $c$ is uniquely determined as $c \\equiv a-b \\pmod d$. Thus, there are $d$ such pairs.\n$$ p(b) = d \\times \\frac{1}{d^2} = \\frac{1}{d} $$\nSo, $B$ is uniformly distributed. The entropy is $S(\\rho_B) = -\\sum_b p(b) \\log_2 p(b) = -\\sum_{b=0}^{d-1} \\frac{1}{d} \\log_2(\\frac{1}{d}) = \\log_2 d$.\n\nThe joint marginal distribution for $A$ and $B$ is $p(a,b) = \\sum_c p(a,b,c) = \\sum_{c=0}^{d-1} \\frac{1}{d^2} \\delta_{b, (a-c) \\bmod d}$. For a fixed pair $(a,b)$, the condition $b=(a-c) \\bmod d$ uniquely determines $c$ as $c=(a-b) \\bmod d$. Thus, only one term in the sum is non-zero.\n$$ p(a,b) = \\frac{1}{d^2} $$\nThe distribution $p(a,b)$ is uniform over all $d^2$ possible pairs. The entropy is $S(\\rho_{AB}) = \\log_2(d^2) = 2 \\log_2 d$.\n\nSimilarly, the joint marginal for $B$ and $C$ is $p(b,c) = \\sum_a p(a,b,c) = \\sum_{a=0}^{d-1} \\frac{1}{d^2} \\delta_{b, (a-c) \\bmod d}$. For a fixed pair $(b,c)$, the condition $b=(a-c) \\bmod d$ uniquely determines $a$ as $a=(b+c) \\bmod d$.\n$$ p(b,c) = \\frac{1}{d^2} $$\nThe distribution $p(b,c)$ is also uniform over all $d^2$ pairs. The entropy is $S(\\rho_{BC}) = \\log_2(d^2) = 2 \\log_2 d$.\n\nThe entropy of the full state $\\rho_{ABC}$ is the Shannon entropy of $p(a,b,c)$. There are $d^2$ equally likely outcomes (corresponding to the $d^2$ choices for $(a,c)$), each with probability $\\frac{1}{d^2}$.\n$$ S(\\rho_{ABC}) = -\\sum_{a,b,c} p(a,b,c) \\log_2 p(a,b,c) = -d^2 \\times \\frac{1}{d^2} \\log_2(\\frac{1}{d^2}) = \\log_2(d^2) = 2 \\log_2 d $$\nNow we can compute the conditional mutual information:\n$$ I(A:C|B) = S(\\rho_{AB}) + S(\\rho_{BC}) - S(\\rho_{B}) - S(\\rho_{ABC}) $$\n$$ I(A:C|B) = (2 \\log_2 d) + (2 \\log_2 d) - (\\log_2 d) - (2 \\log_2 d) = \\log_2 d $$\n\nNext, we use the connection between conditional mutual information and recoverability. The problem invokes the established lower bound relating $I(A:C|B)$ to the optimal recovery fidelity $F_{\\star}$. This bound is a key result in quantum Shannon theory, often associated with the work of Fawzi and Renner, which states:\n$$ I(A:C|B) \\geq -2 \\log_2 F_{\\star} $$\nwhere $F_{\\star} = \\sup_{\\mathcal{R}_{B \\to BC}} F(\\rho_{ABC}, (\\mathrm{id}_{A}\\otimes \\mathcal{R}_{B \\to BC})(\\rho_{AB}))$. We are asked to find the guaranteed lower bound $F_{\\mathrm{lb}}$ on $F_{\\star}$. Rearranging the inequality to solve for $F_{\\star}$ gives:\n$$ \\log_2 F_{\\star} \\geq -\\frac{1}{2} I(A:C|B) $$\nExponentiating both sides (with base $2$) yields the lower bound on fidelity:\n$$ F_{\\star} \\geq 2^{-I(A:C|B)/2} $$\nTherefore, the guaranteed lower bound is $F_{\\mathrm{lb}} = 2^{-I(A:C|B)/2}$.\nSubstituting our result for $I(A:C|B) = \\log_2 d$:\n$$ F_{\\mathrm{lb}} = 2^{-(\\log_2 d)/2} = 2^{\\log_2(d^{-1/2})} = d^{-1/2} = \\frac{1}{\\sqrt{d}} $$\n\nFinally, we are asked to compute the fidelity shortfall $\\Delta = 1 - F_{\\mathrm{lb}}$ for the specific case where $d=16$.\n$$ \\Delta = 1 - \\frac{1}{\\sqrt{d}} $$\nSubstituting $d=16$:\n$$ \\Delta = 1 - \\frac{1}{\\sqrt{16}} = 1 - \\frac{1}{4} = \\frac{3}{4} $$\nThe exact value of the fidelity shortfall is $\\frac{3}{4}$.",
            "answer": "$$\\boxed{\\frac{3}{4}}$$"
        }
    ]
}