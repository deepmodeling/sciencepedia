{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of noise analysis is understanding a signal's frequency content through its power spectral density (PSD). This practice provides a foundational exercise in deriving the characteristic Lorentzian PSD of a single Random Telegraph Noise (RTN) process from first principles. By connecting the time-domain parameters of mean capture time ($\\tau_c$) and emission time ($\\tau_e$) to the frequency domain, you will gain a deeper appreciation for the low-frequency plateau and high-frequency $1/f^2$ roll-off that define this fundamental noise source .",
            "id": "3769556",
            "problem": "A nanoscale metal–oxide–semiconductor field-effect transistor is biased at a fixed gate and drain condition such that a single oxide/interface defect intermittently captures and emits a carrier, modulating the channel current between two levels. The resulting current fluctuation is a stationary, ergodic, two-state continuous-time Markov process commonly referred to as random telegraph noise (RTN). Let the mean capture time be $\\tau_{c}$ and the mean emission time be $\\tau_{e}$, so the transition rates are $\\lambda_{c}=1/\\tau_{c}$ and $\\lambda_{e}=1/\\tau_{e}$. The current steps between two levels that differ by $2A$, so that the fluctuation about the mean can be modeled as $\\delta I(t)\\in\\{+A,-A\\}$.\n\nStarting from the definitions of a two-state Markov process, the exponential autocorrelation of such a process, and the one-sided current power spectral density (PSD) $S_{I}(f)$ as the Fourier transform of the autocorrelation function, derive the analytic expression of $S_{I}(f)$ in terms of $A$, $\\lambda_{c}$, and $\\lambda_{e}$. Define the corner frequency $f_{c}$ as the frequency at which $S_{I}(f)$ equals one-half of its low-frequency plateau $S_{I}(0)$, and express $f_{c}$ in terms of $\\lambda_{c}$ and $\\lambda_{e}$. Then, for $\\tau_{c}=10\\,\\mathrm{ms}$, $\\tau_{e}=5\\,\\mathrm{ms}$, and $2A=100\\,\\mathrm{nA}$, compute $f_{c}$ and the value of $S_{I}(f)$ at $f=10\\,\\mathrm{Hz}$. In doing so, explicitly demonstrate the low-frequency plateau and the high-frequency $1/f^{2}$ roll-off from your derived expression. Express the corner frequency in $\\mathrm{Hz}$ and the PSD in $\\mathrm{A}^{2}/\\mathrm{Hz}$. Round your numerical results to three significant figures.",
            "solution": "The user wants me to solve the following problem:\n\"A nanoscale metal–oxide–semiconductor field-effect transistor is biased at a fixed gate and drain condition such that a single oxide/interface defect intermittently captures and emits a carrier, modulating the channel current between two levels. The resulting current fluctuation is a stationary, ergodic, two-state continuous-time Markov process commonly referred to as random telegraph noise (RTN). Let the mean capture time be $\\tau_{c}$ and the mean emission time be $\\tau_{e}$, so the transition rates are $\\lambda_{c}=1/\\tau_{c}$ and $\\lambda_{e}=1/\\tau_{e}$. The current steps between two levels that differ by $2A$, so that the fluctuation about the mean can be modeled as $\\delta I(t)\\in\\{+A,-A\\}$.\n\nStarting from the definitions of a two-state Markov process, the exponential autocorrelation of such a process, and the one-sided current power spectral density (PSD) $S_{I}(f)$ as the Fourier transform of the autocorrelation function, derive the analytic expression of $S_{I}(f)$ in terms of $A$, $\\lambda_{c}$, and $\\lambda_{e}$. Define the corner frequency $f_{c}$ as the frequency at which $S_{I}(f)$ equals one-half of its low-frequency plateau $S_{I}(0)$, and express $f_{c}$ in terms of $\\lambda_{c}$ and $\\lambda_{e}$. Then, for $\\tau_{c}=10\\,\\mathrm{ms}$, $\\tau_{e}=5\\,\\mathrm{ms}$, and $2A=100\\,\\mathrm{nA}$, compute $f_{c}$ and the value of $S_{I}(f)$ at $f=10\\,\\mathrm{Hz}$. In doing so, explicitly demonstrate the low-frequency plateau and the high-frequency $1/f^{2}$ roll-off from your derived expression. Express the corner frequency in $\\mathrm{Hz}$ and the PSD in $\\mathrm{A}^{2}/\\mathrm{Hz}$. Round your numerical results to three significant figures.\"\n\n### Step 1: Extract Givens\n- The process is a stationary, ergodic, two-state continuous-time Markov process (random telegraph noise).\n- Mean capture time: $\\tau_c$.\n- Mean emission time: $\\tau_e$.\n- Transition rate for capture: $\\lambda_c = 1/\\tau_c$.\n- Transition rate for emission: $\\lambda_e = 1/\\tau_e$.\n- The current steps between two levels that differ by $2A$.\n- The fluctuation about the mean is modeled as $\\delta I(t) \\in \\{+A, -A\\}$.\n- Problem requires derivation of one-sided power spectral density $S_I(f)$ in terms of $A$, $\\lambda_c$, $\\lambda_e$.\n- Problem requires derivation of corner frequency $f_c$, where $S_I(f_c) = S_I(0)/2$.\n- Problem requires demonstration of low-frequency plateau and high-frequency $1/f^2$ roll-off.\n- Numerical values for calculation: $\\tau_c = 10\\,\\mathrm{ms}$, $\\tau_e = 5\\,\\mathrm{ms}$, $2A = 100\\,\\mathrm{nA}$.\n- Calculation required for $f_c$ and $S_I(f)$ at $f = 10\\,\\mathrm{Hz}$.\n- Numerical results must be rounded to three significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes the standard model of random telegraph noise (RTN) due to a single trap in a semiconductor device. This is a well-established topic in solid-state physics and electronic device engineering. The model is based on the theory of stochastic processes (specifically, two-state Markov chains) and its application to electronic noise, which is scientifically sound.\n- **Well-Posed**: The problem provides all necessary information to derive the requested quantities. The relationships between mean times, transition rates, current levels, and the power spectral density are well-defined. The request for both an analytical derivation and a numerical calculation is clear.\n- **Objective**: The problem is stated in precise, quantitative terms. There is a minor ambiguity in the statement \"the fluctuation about the mean can be modeled as $\\delta I(t)\\in\\{+A,-A\\}$\". For a general case where $\\tau_c \\neq \\tau_e$, the process is asymmetric and the fluctuations around the mean are not of equal magnitude $\\pm A$. However, the core physical parameter is the total current step height, $\\Delta I$, which is unambiguously given as $2A$. As will be shown, the resulting Power Spectral Density (PSD) depends only on this step height $\\Delta I$ and the transition rates, not on the absolute levels or their centering around the mean. Thus, this imprecision does not prevent a unique and correct solution. The problem can be formalized and solved rigorously.\n- **All other validation criteria are met**: The problem is not metaphorical, incomplete, contradictory, or trivial.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\nThe physical process is a two-state system. Let the states be State $0$ (low current, trap occupied) and State $1$ (high current, trap empty). The transition rate from State $1$ to State $0$ is the capture rate, $\\lambda_c = 1/\\tau_c$. The transition rate from State $0$ to State $1$ is the emission rate, $\\lambda_e = 1/\\tau_e$.\n\nFor a stationary process, the probabilities $P_0$ and $P_1$ of being in State $0$ and State $1$, respectively, are constant. They are determined by the balance equation $\\lambda_c P_1 = \\lambda_e P_0$, along with the normalization condition $P_0 + P_1 = 1$.\nSolving this system gives the steady-state probabilities:\n$$ P_0 = \\frac{\\lambda_c}{\\lambda_c + \\lambda_e} $$\n$$ P_1 = \\frac{\\lambda_e}{\\lambda_c + \\lambda_e} $$\n\nLet the current in the two states be $I_0$ and $I_1$. The problem states that the levels differ by $2A$, so we define the current step height as $\\Delta I = |I_1 - I_0| = 2A$. The signal of interest is the fluctuation of the current, $\\delta I(t)$, around its mean value, $\\langle I \\rangle$.\nThe mean current is $\\langle I \\rangle = I_0 P_0 + I_1 P_1$.\nThe fluctuation signal is $\\delta I(t) = I(t) - \\langle I \\rangle$.\nIn State $0$, the fluctuation is $\\delta I_0 = I_0 - \\langle I \\rangle = I_0 - (I_0 P_0 + I_1 P_1) = I_0(1-P_0) - I_1 P_1 = I_0 P_1 - I_1 P_1 = -(I_1-I_0)P_1 = -\\Delta I P_1$.\nIn State $1$, the fluctuation is $\\delta I_1 = I_1 - \\langle I \\rangle = I_1 - (I_0 P_0 + I_1 P_1) = I_1(1-P_1) - I_0 P_0 = I_1 P_0 - I_0 P_0 = (I_1-I_0)P_0 = \\Delta I P_0$.\nNote that the mean fluctuation is $\\langle \\delta I \\rangle = \\delta I_0 P_0 + \\delta I_1 P_1 = (-\\Delta I P_1)P_0 + (\\Delta I P_0)P_1 = 0$, as expected.\n\nThe autocorrelation function of the fluctuation, $R_{\\delta I}(\\tau) = \\langle \\delta I(t) \\delta I(t+\\tau) \\rangle$, for a two-state Markov process is known to be an exponential decay:\n$$ R_{\\delta I}(\\tau) = \\sigma_{\\delta I}^{2} \\exp\\left(-\\frac{|\\tau|}{\\tau_{eff}}\\right) $$\nwhere $\\sigma_{\\delta I}^{2}$ is the variance of the fluctuation and $\\tau_{eff}$ is the effective time constant of the correlation decay.\nThe variance is $\\sigma_{\\delta I}^{2} = \\langle (\\delta I)^2 \\rangle = (\\delta I_0)^2 P_0 + (\\delta I_1)^2 P_1$.\n$$ \\sigma_{\\delta I}^{2} = (-\\Delta I P_1)^2 P_0 + (\\Delta I P_0)^2 P_1 = (\\Delta I)^2 (P_1^2 P_0 + P_0^2 P_1) = (\\Delta I)^2 P_0 P_1 (P_0+P_1) = (\\Delta I)^2 P_0 P_1 $$\nSubstituting the expressions for $P_0$ and $P_1$:\n$$ \\sigma_{\\delta I}^{2} = (\\Delta I)^2 \\frac{\\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e)^2} $$\nThe effective time constant $\\tau_{eff}$ is given by the sum of the transition rates:\n$$ \\tau_{eff} = \\frac{1}{\\lambda_c + \\lambda_e} $$\nSubstituting $\\Delta I = 2A$, the autocorrelation function is:\n$$ R_{\\delta I}(\\tau) = (2A)^2 \\frac{\\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e)^2} \\exp(-(\\lambda_c + \\lambda_e)|\\tau|) = \\frac{4A^2 \\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e)^2} \\exp(-(\\lambda_c + \\lambda_e)|\\tau|) $$\n\nThe one-sided power spectral density (PSD) $S_I(f)$ is given by the Wiener-Khinchin theorem as twice the Fourier transform of the autocorrelation function for $f \\ge 0$. More commonly, it is expressed as:\n$$ S_I(f) = 4 \\int_{0}^{\\infty} R_{\\delta I}(\\tau) \\cos(2\\pi f \\tau) d\\tau $$\nSubstituting the expression for $R_{\\delta I}(\\tau)$:\n$$ S_I(f) = 4 \\left( \\frac{4A^2 \\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e)^2} \\right) \\int_{0}^{\\infty} \\exp(-(\\lambda_c + \\lambda_e)\\tau) \\cos(2\\pi f \\tau) d\\tau $$\nThe integral is a standard Laplace transform result: $\\int_0^\\infty e^{-s t} \\cos(\\omega t) dt = \\frac{s}{s^2+\\omega^2}$.\nHere, $s = \\lambda_c + \\lambda_e$ and $\\omega = 2\\pi f$.\n$$ \\int_{0}^{\\infty} \\exp(-(\\lambda_c + \\lambda_e)\\tau) \\cos(2\\pi f \\tau) d\\tau = \\frac{\\lambda_c + \\lambda_e}{(\\lambda_c + \\lambda_e)^2 + (2\\pi f)^2} $$\nTherefore, the PSD is:\n$$ S_I(f) = \\frac{16A^2 \\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e)^2} \\frac{\\lambda_c + \\lambda_e}{(\\lambda_c + \\lambda_e)^2 + (2\\pi f)^2} $$\n$$ S_I(f) = \\frac{16A^2 \\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e) \\left[ (\\lambda_c + \\lambda_e)^2 + (2\\pi f)^2 \\right]} $$\nThis is the analytic expression for the one-sided PSD, which is of Lorentzian form.\n\nNext, we find the corner frequency $f_c$. First, we determine the low-frequency plateau $S_I(0)$ by setting $f=0$ in the expression for $S_I(f)$:\n$$ S_I(0) = \\frac{16A^2 \\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e) [(\\lambda_c + \\lambda_e)^2 + 0]} = \\frac{16A^2 \\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e)^3} $$\nThe corner frequency $f_c$ is defined by $S_I(f_c) = S_I(0)/2$.\n$$ \\frac{16A^2 \\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e) \\left[ (\\lambda_c + \\lambda_e)^2 + (2\\pi f_c)^2 \\right]} = \\frac{1}{2} \\frac{16A^2 \\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e)^3} $$\nCanceling common terms:\n$$ \\frac{1}{(\\lambda_c + \\lambda_e)^2 + (2\\pi f_c)^2} = \\frac{1}{2(\\lambda_c + \\lambda_e)^2} $$\n$$ 2(\\lambda_c + \\lambda_e)^2 = (\\lambda_c + \\lambda_e)^2 + (2\\pi f_c)^2 $$\n$$ (\\lambda_c + \\lambda_e)^2 = (2\\pi f_c)^2 $$\nSince $f_c$ and the rates are positive, we take the positive root:\n$$ 2\\pi f_c = \\lambda_c + \\lambda_e \\implies f_c = \\frac{\\lambda_c + \\lambda_e}{2\\pi} $$\n\nWe can now express the PSD in terms of $S_I(0)$ and $f_c$:\n$$ S_I(f) = \\frac{S_I(0)(\\lambda_c + \\lambda_e)^2}{(\\lambda_c + \\lambda_e)^2 + (2\\pi f)^2} = S_I(0) \\frac{(2\\pi f_c)^2}{(2\\pi f_c)^2 + (2\\pi f)^2} = S_I(0) \\frac{1}{1 + (f/f_c)^2} $$\nThis form clearly demonstrates the behavior at different frequency regimes.\n- **Low-frequency plateau**: For $f \\ll f_c$, the term $(f/f_c)^2 \\to 0$, so $S_I(f) \\approx S_I(0)$, a constant.\n- **High-frequency roll-off**: For $f \\gg f_c$, the term $(f/f_c)^2 \\gg 1$, so $S_I(f) \\approx S_I(0) \\frac{1}{(f/f_c)^2} = S_I(0) \\frac{f_c^2}{f^2}$. This shows that the PSD is proportional to $1/f^2$ at high frequencies.\n\nFinally, we compute the numerical values.\nGiven:\n$\\tau_c = 10\\,\\mathrm{ms} = 1 \\times 10^{-2}\\,\\mathrm{s}$\n$\\tau_e = 5\\,\\mathrm{ms} = 5 \\times 10^{-3}\\,\\mathrm{s}$\n$2A = 100\\,\\mathrm{nA} = 1 \\times 10^{-7}\\,\\mathrm{A} \\implies A = 5 \\times 10^{-8}\\,\\mathrm{A}$\n\nFirst, calculate the transition rates:\n$\\lambda_c = 1/\\tau_c = 1/(1 \\times 10^{-2}\\,\\mathrm{s}) = 100\\,\\mathrm{s}^{-1}$\n$\\lambda_e = 1/\\tau_e = 1/(5 \\times 10^{-3}\\,\\mathrm{s}) = 200\\,\\mathrm{s}^{-1}$\n\nNext, calculate the corner frequency $f_c$:\n$$ f_c = \\frac{\\lambda_c + \\lambda_e}{2\\pi} = \\frac{100\\,\\mathrm{s}^{-1} + 200\\,\\mathrm{s}^{-1}}{2\\pi} = \\frac{300}{2\\pi}\\,\\mathrm{Hz} \\approx 47.746\\,\\mathrm{Hz} $$\nRounding to three significant figures, $f_c = 47.7\\,\\mathrm{Hz}$.\n\nNow, we calculate the PSD value at $f=10\\,\\mathrm{Hz}$. We first compute the low-frequency plateau value $S_I(0)$:\n$$ S_I(0) = \\frac{16A^2 \\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e)^3} = \\frac{16(5 \\times 10^{-8}\\,\\mathrm{A})^2 (100\\,\\mathrm{s}^{-1})(200\\,\\mathrm{s}^{-1})}{(100\\,\\mathrm{s}^{-1} + 200\\,\\mathrm{s}^{-1})^3} $$\n$$ S_I(0) = \\frac{16(25 \\times 10^{-16}\\,\\mathrm{A}^2) (2 \\times 10^4\\,\\mathrm{s}^{-2})}{(300\\,\\mathrm{s}^{-1})^3} = \\frac{8000 \\times 10^{-16}\\,\\mathrm{A}^2\\cdot\\mathrm{s}^{-2}}{27 \\times 10^6\\,\\mathrm{s}^{-3}} = \\frac{8 \\times 10^{-13}}{2.7 \\times 10^7}\\,\\mathrm{A}^2 \\cdot \\mathrm{s} $$\n$$ S_I(0) = \\frac{8}{27} \\times 10^{-16}\\,\\mathrm{A}^2/\\mathrm{Hz} \\approx 2.96296 \\times 10^{-17}\\,\\mathrm{A}^2/\\mathrm{Hz} $$\nNow we use the Lorentzian form to find $S_I(10\\,\\mathrm{Hz})$:\n$$ S_I(10\\,\\mathrm{Hz}) = S_I(0) \\frac{1}{1 + (10\\,\\mathrm{Hz}/f_c)^2} = (2.96296 \\times 10^{-17}) \\frac{1}{1 + (10 / 47.746)^2} $$\n$$ S_I(10\\,\\mathrm{Hz}) \\approx (2.96296 \\times 10^{-17}) \\frac{1}{1 + (0.20944)^2} \\approx (2.96296 \\times 10^{-17}) \\frac{1}{1 + 0.043865} $$\n$$ S_I(10\\,\\mathrm{Hz}) \\approx \\frac{2.96296 \\times 10^{-17}}{1.043865} \\approx 2.8384 \\times 10^{-17}\\,\\mathrm{A}^2/\\mathrm{Hz} $$\nRounding to three significant figures, $S_I(10\\,\\mathrm{Hz}) = 2.84 \\times 10^{-17}\\,\\mathrm{A}^2/\\mathrm{Hz}$.\n\nSummary of results:\n- Derived expression for $S_I(f)$: $S_I(f) = \\frac{16A^2 \\lambda_c \\lambda_e}{(\\lambda_c + \\lambda_e) [(\\lambda_c + \\lambda_e)^2 + (2\\pi f)^2]}$.\n- Derived corner frequency: $f_c = \\frac{\\lambda_c + \\lambda_e}{2\\pi}$.\n- Calculated corner frequency: $f_c \\approx 47.7\\,\\mathrm{Hz}$.\n- Calculated PSD at $10\\,\\mathrm{Hz}$: $S_I(10\\,\\mathrm{Hz}) \\approx 2.84 \\times 10^{-17}\\,\\mathrm{A}^2/\\mathrm{Hz}$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 47.7 & 2.84 \\times 10^{-17} \\end{pmatrix} } $$"
        },
        {
            "introduction": "Beyond being a source of variability, Random Telegraph Noise serves as a sensitive probe into the local electrostatic environment within a nanoscale transistor. This exercise bridges the gap between the abstract stochastic model of RTN and the concrete device physics of a modern field-effect transistor. You will investigate how the RTN switching asymmetry is directly modulated by the gate voltage, demonstrating how noise measurements can be used to extract quantitative information about carrier populations at the semiconductor-insulator interface .",
            "id": "4297478",
            "problem": "A two-state defect located at the interface of a top-gated molybdenum disulfide ($\\mathrm{MoS_2}$) field-effect transistor produces Random Telegraph Noise (RTN). The defect captures conduction-band electrons at a rate $c_n$ and emits them at a rate $e_n$, causing stochastic switching between a low-current (trap filled) and high-current (trap empty) state. Consider a monolayer $\\mathrm{MoS_2}$ channel of physical thickness $t_{\\mathrm{ch}}$ under a high-$\\kappa$ hafnium dioxide ($\\mathrm{HfO_2}$) gate oxide of thickness $t_{\\mathrm{ox}}$ and relative dielectric constant $\\kappa_{\\mathrm{ox}}$. The gate oxide capacitance per unit area is $C_{\\mathrm{ox}} = \\varepsilon_{\\mathrm{ox}}/t_{\\mathrm{ox}}$, with $\\varepsilon_{\\mathrm{ox}} = \\kappa_{\\mathrm{ox}} \\varepsilon_0$. Assume strong inversion so that the inversion sheet charge density is $Q_{\\mathrm{inv}} = C_{\\mathrm{ox}}(V_{GS} - V_T)$ for gate-source voltage $V_{GS}$ above threshold voltage $V_T$. Model the inversion electron volume density in the monolayer as $n(V_{GS}) = Q_{\\mathrm{inv}}/(q t_{\\mathrm{ch}})$, where $q$ is the elementary charge. The capture rate is $c_n = n(V_{GS}) \\sigma_n v_{\\mathrm{th}}$, where $\\sigma_n$ is the capture cross-section and $v_{\\mathrm{th}}$ is the electron thermal speed. Use the Maxwell–Boltzmann average speed $v_{\\mathrm{th}} = \\sqrt{\\frac{8 k_B T}{\\pi m^*}}$, where $k_B$ is the Boltzmann constant, $T$ is the absolute temperature, and $m^*$ is the electron conduction-band effective mass. Assume Shockley–Read–Hall emission $e_n = \\sigma_n v_{\\mathrm{th}} n_1$, with $n_1 = N_C \\exp\\!\\left(-\\frac{E_C - E_t}{k_B T}\\right)$ and the effective conduction-band density of states $N_C = 2 \\left(\\frac{2 \\pi m^* k_B T}{h^2}\\right)^{3/2}$, where $h$ is Planck’s constant and $E_C - E_t$ is the energy separation between the conduction-band edge and the trap level. Define the RTN switching asymmetry factor as $A(V_{GS}) = \\frac{\\tau_{\\mathrm{filled}}}{\\tau_{\\mathrm{empty}}} = \\frac{c_n}{e_n}$, where $\\tau_{\\mathrm{filled}}$ and $\\tau_{\\mathrm{empty}}$ are the mean dwell times in the filled and empty states, respectively.\n\nGiven the following parameters: $T = 300 \\, \\mathrm{K}$, $m^* = 0.45 m_e$, $\\sigma_n = 1.0 \\times 10^{-19} \\, \\mathrm{m}^2$, $E_C - E_t = 0.20 \\, \\mathrm{eV}$, $\\kappa_{\\mathrm{ox}} = 20$, $t_{\\mathrm{ox}} = 3.0 \\times 10^{-9} \\, \\mathrm{m}$, $t_{\\mathrm{ch}} = 6.5 \\times 10^{-10} \\, \\mathrm{m}$, $V_T = 0.50 \\, \\mathrm{V}$, and two gate biases $V_{GS,1} = 0.70 \\, \\mathrm{V}$ and $V_{GS,2} = 1.10 \\, \\mathrm{V}$. Use $q = 1.602176634 \\times 10^{-19} \\, \\mathrm{C}$, $\\varepsilon_0 = 8.854187817 \\times 10^{-12} \\, \\mathrm{F/m}$, $k_B = 1.380649 \\times 10^{-23} \\, \\mathrm{J/K}$, $h = 6.62607015 \\times 10^{-34} \\, \\mathrm{J \\cdot s}$, and $m_e = 9.10938356 \\times 10^{-31} \\, \\mathrm{kg}$.\n\nStarting from the stated fundamental definitions and formulas, derive $A(V_{GS})$ in terms of $V_{GS}$ and the given constants, and compute the ratio\n$$R \\equiv \\frac{A(V_{GS,2})}{A(V_{GS,1})}$$\nthat quantifies the change in RTN switching asymmetry with gate overdrive. Express the final ratio $R$ as a dimensionless number and round your answer to four significant figures.",
            "solution": "The objective is to derive the RTN switching asymmetry factor, $A(V_{GS})$, and then compute the ratio $R \\equiv \\frac{A(V_{GS,2})}{A(V_{GS,1})}$ for two given gate-source voltages, $V_{GS,1}$ and $V_{GS,2}$.\n\nAccording to the problem statement, the asymmetry factor $A(V_{GS})$ is defined as the ratio of the mean dwell time in the filled state, $\\tau_{\\mathrm{filled}}$, to the mean dwell time in the empty state, $\\tau_{\\mathrm{empty}}$. This is equivalent to the ratio of the capture rate, $c_n$, to the emission rate, $e_n$.\n$$A(V_{GS}) = \\frac{\\tau_{\\mathrm{filled}}}{\\tau_{\\mathrm{empty}}} = \\frac{c_n}{e_n}$$\nThe expressions for the capture and emission rates are provided:\n$$c_n = n(V_{GS}) \\sigma_n v_{\\mathrm{th}}$$\n$$e_n = \\sigma_n v_{\\mathrm{th}} n_1$$\nHere, $n(V_{GS})$ is the inversion electron volume density, $\\sigma_n$ is the capture cross-section, $v_{\\mathrm{th}}$ is the electron thermal speed, and $n_1$ is a parameter related to the trap energy level.\n\nSubstituting these expressions into the definition of $A(V_{GS})$, we find that several terms cancel:\n$$A(V_{GS}) = \\frac{n(V_{GS}) \\sigma_n v_{\\mathrm{th}}}{\\sigma_n v_{\\mathrm{th}} n_1} = \\frac{n(V_{GS})}{n_1}$$\nThis simplification holds provided that $\\sigma_n \\neq 0$ and $v_{\\mathrm{th}} \\neq 0$, which is the case.\n\nNext, we derive the explicit dependence of $A(V_{GS})$ on the gate-source voltage $V_{GS}$. The term $n_1$ is given by $n_1 = N_C \\exp\\!\\left(-\\frac{E_C - E_t}{k_B T}\\right)$, where $N_C$ is the effective density of states. Both $N_C$ and the exponential term depend on temperature $T$ and material properties, but are independent of the gate voltage $V_{GS}$. Therefore, $n_1$ is a constant with respect to $V_{GS}$.\n\nThe electron volume density $n(V_{GS})$ is given by:\n$$n(V_{GS}) = \\frac{Q_{\\mathrm{inv}}}{q t_{\\mathrm{ch}}}$$\nwhere $q$ is the elementary charge and $t_{\\mathrm{ch}}$ is the channel thickness.\nThe inversion sheet charge density, $Q_{\\mathrm{inv}}$, for a gate voltage $V_{GS}$ above the threshold voltage $V_T$ is given by:\n$$Q_{\\mathrm{inv}} = C_{\\mathrm{ox}}(V_{GS} - V_T)$$\nThe gate oxide capacitance per unit area, $C_{\\mathrm{ox}}$, is:\n$$C_{\\mathrm{ox}} = \\frac{\\varepsilon_{\\mathrm{ox}}}{t_{\\mathrm{ox}}} = \\frac{\\kappa_{\\mathrm{ox}} \\varepsilon_0}{t_{\\mathrm{ox}}}$$\nwhere $\\kappa_{\\mathrm{ox}}$ is the relative dielectric constant of the oxide, $\\varepsilon_0$ is the vacuum permittivity, and $t_{\\mathrm{ox}}$ is the oxide thickness.\n\nCombining these expressions, we obtain the gate-voltage dependent electron density:\n$$n(V_{GS}) = \\frac{1}{q t_{\\mathrm{ch}}} \\left( \\frac{\\kappa_{\\mathrm{ox}} \\varepsilon_0}{t_{\\mathrm{ox}}} \\right) (V_{GS} - V_T)$$\nNow, we can write the full expression for the asymmetry factor $A(V_{GS})$:\n$$A(V_{GS}) = \\frac{n(V_{GS})}{n_1} = \\frac{\\kappa_{\\mathrm{ox}} \\varepsilon_0}{q t_{\\mathrm{ch}} t_{\\mathrm{ox}} n_1} (V_{GS} - V_T)$$\nThis expression shows that $A(V_{GS})$ is linearly proportional to the gate overdrive voltage, $(V_{GS} - V_T)$. Let us define a constant $K$ that groups all the voltage-independent terms:\n$$K = \\frac{\\kappa_{\\mathrm{ox}} \\varepsilon_0}{q t_{\\mathrm{ch}} t_{\\mathrm{ox}} n_1}$$\nThen, $A(V_{GS}) = K (V_{GS} - V_T)$.\n\nWe are asked to compute the ratio $R = \\frac{A(V_{GS,2})}{A(V_{GS,1})}$. Using the derived expression for $A(V_{GS})$:\n$$R = \\frac{K (V_{GS,2} - V_T)}{K (V_{GS,1} - V_T)}$$\nThe constant $K$ cancels out, leading to a remarkably simple result that depends only on the applied gate voltages and the threshold voltage:\n$$R = \\frac{V_{GS,2} - V_T}{V_{GS,1} - V_T}$$\nThis means we do not need to calculate the numerical values of the numerous physical constants provided in the problem statement, as they are all subsumed into the constant $K$ which cancels in the final ratio.\n\nWe are given the following numerical values:\n$V_{GS,1} = 0.70 \\, \\mathrm{V}$\n$V_{GS,2} = 1.10 \\, \\mathrm{V}$\n$V_T = 0.50 \\, \\mathrm{V}$\n\nSubstituting these values into the expression for $R$:\nThe gate overdrive for the first bias point is $V_{GS,1} - V_T = 0.70 - 0.50 = 0.20 \\, \\mathrm{V}$.\nThe gate overdrive for the second bias point is $V_{GS,2} - V_T = 1.10 - 0.50 = 0.60 \\, \\mathrm{V}$.\n\nThe ratio $R$ is therefore:\n$$R = \\frac{0.60}{0.20} = 3$$\nThe problem requires the result to be expressed as a dimensionless number rounded to four significant figures.\n\nTherefore, $R = 3.000$.",
            "answer": "$$\\boxed{3.000}$$"
        },
        {
            "introduction": "Analyzing raw experimental data requires robust computational tools that can extract a clean signal from measurement noise and infer underlying physical states. This hands-on coding practice guides you through the development of a complete simulation and analysis pipeline for multi-level RTN using Hidden Markov Models (HMMs). You will implement the Viterbi algorithm, a powerful maximum a posteriori decoder, to determine the most probable sequence of hidden states from a noisy time series, a technique central to modern research in nanoelectronics and quantum device characterization .",
            "id": "4297514",
            "problem": "Construct a program to synthesize and decode multi-level random telegraph noise using a Hidden Markov Model (HMM) under a continuous-time physical model and to validate the decoded hidden state path against the ground truth via the maximum a posteriori (MAP) criterion. The physical base is that a multi-level random telegraph noise process in a nanoelectronic device with discrete conduction states can be modeled as a continuous-time Markov chain whose transitions are governed by an infinitesimal generator (rate matrix). Observations sampled at uniform intervals are corrupted by additive measurement noise. The decoding objective is the most probable hidden state path given the observed sequence and the HMM parameters. The derivation and implementation must begin from fundamental definitions: the Markov property, the structure of a continuous-time Markov chain and its discrete-time counterpart over a sampling interval, and the conditional independence of emissions given states.\n\nDefinitions and modeling assumptions to use:\n- Let the hidden state process be a $K$-state continuous-time Markov chain with rate matrix $\\Lambda \\in \\mathbb{R}^{K \\times K}$, where $\\Lambda_{ij} \\ge 0$ for $i \\ne j$ and $\\Lambda_{ii} = -\\sum_{j \\ne i} \\Lambda_{ij}$. For small sampling interval $d t$, the transition probability from state $i$ to $j$ over $d t$ is induced by the matrix exponential, i.e., the discrete-time transition matrix is $P = \\exp(\\Lambda \\, d t)$, which is row-stochastic.\n- Let the emission model be additive Gaussian measurement noise: if the hidden state at sample index $n$ is $s_n \\in \\{0,1,\\dots,K-1\\}$, the observed voltage is $y_n = L_{s_n} + \\varepsilon_n$, where $L_i$ is the conduction level for state $i$ (in volts), and $\\varepsilon_n \\sim \\mathcal{N}(0,\\sigma^2)$ independently across $n$. The likelihood $p(y_n \\mid s_n = i)$ is Gaussian with mean $L_i$ and variance $\\sigma^2$.\n- Let the initial distribution $\\pi$ for $s_0$ be the stationary distribution of the discrete-time transition matrix $P$.\n\nYour program must:\n1. For each test case, construct the synthetic HMM by computing $P = \\exp(\\Lambda \\, d t)$, compute the stationary distribution $\\pi$ of $P$, simulate a length-$N$ hidden state trajectory $\\{s_n\\}_{n=0}^{N-1}$ by the Markov chain with initial state drawn from $\\pi$ and transitions governed by $P$, and generate observations $\\{y_n\\}_{n=0}^{N-1}$ using the Gaussian model above.\n2. Derive and implement a numerically stable maximum a posteriori decoder for the hidden state path under the HMM described, using the logarithmic domain to avoid numerical underflow. The result of the decoder is the most probable state path $\\{\\hat{s}_n\\}_{n=0}^{N-1}$ given the observed sequence and the known HMM parameters $(\\pi,P,L,\\sigma)$.\n3. Compute the path accuracy for each test case as the fraction of indices $n$ for which $\\hat{s}_n = s_n$, expressed as a decimal in the interval $[0,1]$. No physical units are associated with accuracy. Round each accuracy to six digits after the decimal point.\n\nPhysical and numerical units:\n- Conduction levels $L_i$ must be specified and used in volts.\n- The rate matrix entries $\\Lambda_{ij}$ must be specified and used in inverse seconds.\n- The sampling interval $d t$ must be specified and used in seconds.\n- The noise standard deviation $\\sigma$ must be specified and used in volts.\n- The number of samples $N$ is a count and has no physical units.\n\nTest suite and parameters:\n- Test Case $1$ (binary random telegraph noise, low noise, moderate transitions):\n  - $K = 2$,\n  - $L = [\\,0\\,\\text{V},\\,20 \\times 10^{-3}\\,\\text{V}\\,]$,\n  - $\\Lambda = \\begin{bmatrix} -1500 & 1500 \\\\ 2000 & -2000 \\end{bmatrix}$ in inverse seconds,\n  - $d t = 10^{-5}$ seconds,\n  - $\\sigma = 10^{-3}$ volts,\n  - $N = 4000$ samples,\n  - random seed $= 12345$.\n- Test Case $2$ (three-level random telegraph noise, moderate noise, asymmetric transitions):\n  - $K = 3$,\n  - $L = [\\,0\\,\\text{V},\\,10 \\times 10^{-3}\\,\\text{V},\\,25 \\times 10^{-3}\\,\\text{V}\\,]$,\n  - Off-diagonal transition rates in inverse seconds: $r_{01} = 600$, $r_{02} = 100$, $r_{10} = 300$, $r_{12} = 400$, $r_{20} = 200$, $r_{21} = 500$; assemble $\\Lambda$ with diagonals $\\Lambda_{ii} = -\\sum_{j \\ne i} r_{ij}$,\n  - $d t = 2 \\times 10^{-5}$ seconds,\n  - $\\sigma = 2 \\times 10^{-3}$ volts,\n  - $N = 5000$ samples,\n  - random seed $= 202311$.\n- Test Case $3$ (three-level random telegraph noise, high noise challenging decoding):\n  - $K = 3$,\n  - $L = [\\,0\\,\\text{V},\\,10 \\times 10^{-3}\\,\\text{V},\\,25 \\times 10^{-3}\\,\\text{V}\\,]$,\n  - Use the same off-diagonal transition rates as in Test Case $2$ and the corresponding $\\Lambda$,\n  - $d t = 2 \\times 10^{-5}$ seconds,\n  - $\\sigma = 12 \\times 10^{-3}$ volts,\n  - $N = 5000$ samples,\n  - random seed $= 777$.\n- Test Case $4$ (four-level random telegraph noise with rare nearest-neighbor transitions):\n  - $K = 4$,\n  - $L = [\\,0\\,\\text{V},\\,5 \\times 10^{-3}\\,\\text{V},\\,10 \\times 10^{-3}\\,\\text{V},\\,15 \\times 10^{-3}\\,\\text{V}\\,]$,\n  - Off-diagonal transition rates in inverse seconds: nearest-neighbor only with $r_{01} = r_{10} = 10$, $r_{12} = r_{21} = 10$, $r_{23} = r_{32} = 10$, all other $r_{ij} = 0$; assemble $\\Lambda$ accordingly,\n  - $d t = 10^{-3}$ seconds,\n  - $\\sigma = 2 \\times 10^{-3}$ volts,\n  - $N = 6000$ samples,\n  - random seed $= 42$.\n\nAnswer specification and final output format:\n- For each test case, compute the path accuracy as a float.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[a_1,a_2,a_3,a_4]$), where each $a_i$ is rounded to six digits after the decimal point.",
            "solution": "The problem requires the synthesis and decoding of multi-level random telegraph noise, a process modeled by a continuous-time Hidden Markov Model (HMM). The task involves several steps: defining the HMM parameters from a physical model, simulating a trajectory of states and observations, decoding the most probable state path from the observations, and evaluating the accuracy of this decoding.\n\n### HMM Parameterization from the Physical Model\n\n1.  **Continuous-Time Markov Chain**: The hidden state process is a continuous-time Markov chain with $K$ states, $\\{0, 1, \\dots, K-1\\}$. Its dynamics are governed by an infinitesimal generator matrix, or rate matrix, $\\Lambda \\in \\mathbb{R}^{K \\times K}$. The off-diagonal elements $\\Lambda_{ij}$ for $i \\neq j$ represent the transition rate from state $i$ to state $j$. These rates are non-negative, $\\Lambda_{ij} \\ge 0$. The diagonal elements are defined to ensure that the rows sum to zero: $\\Lambda_{ii} = -\\sum_{j \\neq i} \\Lambda_{ij}$.\n\n2.  **Discrete-Time Transition Matrix**: The observations are sampled at uniform intervals of duration $dt$. The transition probability matrix $P \\in \\mathbb{R}^{K \\times K}$ for this discrete-time process is derived from the continuous-time generator $\\Lambda$ via the matrix exponential:\n    $$ P = \\exp(\\Lambda \\, dt) $$\n    The element $P_{ij}$ gives the probability of transitioning from state $i$ to state $j$ over a time interval $dt$, i.e., $P_{ij} = p(s_{n+1}=j \\mid s_n=i)$. The matrix $P$ is row-stochastic, meaning all its elements are non-negative and each row sums to $1$.\n\n3.  **Initial State Distribution**: The initial state $s_0$ is drawn from a probability distribution $\\pi \\in \\mathbb{R}^K$, where $\\pi_i = p(s_0=i)$. The problem specifies that $\\pi$ is the stationary distribution of the discrete-time Markov chain. The stationary distribution is a row vector that satisfies the eigenvector equation:\n    $$ \\pi P = \\pi $$\n    subject to the normalization constraint $\\sum_{i=0}^{K-1} \\pi_i = 1$. This is equivalent to finding the left eigenvector of $P$ corresponding to the eigenvalue $1$.\n\n4.  **Emission Model**: When the system is in a hidden state $s_n=i$, it generates an observable voltage $y_n$. This observation is corrupted by additive white Gaussian noise. The model is:\n    $$ y_n = L_i + \\varepsilon_n $$\n    where $L_i$ is the mean voltage level for state $i$, and $\\varepsilon_n$ is a noise term drawn from a normal distribution with zero mean and variance $\\sigma^2$, i.e., $\\varepsilon_n \\sim \\mathcal{N}(0, \\sigma^2)$. The emission probability density for an observation $y_n$ given the state $s_n=i$ is therefore:\n    $$ p(y_n \\mid s_n=i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_n - L_i)^2}{2\\sigma^2}\\right) $$\n\n### Synthetic Data Generation\n\nFor each test case, we synthesize a dataset as follows:\n1.  Construct the rate matrix $\\Lambda$ from the given rates.\n2.  Compute the discrete-time transition matrix $P = \\exp(\\Lambda \\, dt)$.\n3.  Compute the stationary distribution $\\pi$ of $P$.\n4.  Set the random number generator seed for reproducibility.\n5.  Generate a sequence of $N$ hidden states $\\{s_n\\}_{n=0}^{N-1}$:\n    - Draw the initial state $s_0$ from the categorical distribution defined by $\\pi$.\n    - For $n = 1, \\dots, N-1$, draw state $s_n$ from the categorical distribution defined by the $s_{n-1}$-th row of $P$.\n6.  Generate the corresponding sequence of $N$ observations $\\{y_n\\}_{n=0}^{N-1}$:\n    - For each $n = 0, \\dots, N-1$, draw the observation $y_n$ from the Gaussian distribution $\\mathcal{N}(L_{s_n}, \\sigma^2)$.\n\n### Maximum a Posteriori (MAP) Path Decoding\n\nThe goal is to find the most probable sequence of hidden states $\\{\\hat{s}_n\\}_{n=0}^{N-1}$ given the sequence of observations $\\{y_n\\}_{n=0}^{N-1}$ and the HMM parameters $(\\pi, P, L, \\sigma)$. This is known as MAP decoding.\n$$ \\{\\hat{s}_n\\} = \\arg\\max_{\\{s_n\\}} p(\\{s_n\\} \\mid \\{y_n\\}) $$\nBy Bayes' theorem, this is equivalent to maximizing the joint probability $p(\\{s_n\\}, \\{y_n\\})$. The joint probability for a specific path $\\{s_n\\}$ is given by:\n$$ p(\\{y_n\\}, \\{s_n\\}) = p(s_0) p(y_0|s_0) \\prod_{n=1}^{N-1} p(s_n|s_{n-1}) p(y_n|s_n) = \\pi_{s_0} p(y_0|s_0=s_0) \\prod_{n=1}^{N-1} P_{s_{n-1},s_n} p(y_n|s_n=s_n) $$\nDirect computation of this product is prone to numerical underflow for long sequences. We therefore work in the logarithmic domain, maximizing the log-joint-probability. This is achieved using the Viterbi algorithm.\n\n### Viterbi Algorithm in Log-Domain\n\nLet $\\delta_n(i)$ be the maximum log-probability of any state path of length $n+1$ ending in state $i$ at time $n$, having generated the observations $y_0, \\dots, y_n$.\n\n1.  **Initialization ($n=0$):**\n    For each state $i \\in \\{0, \\dots, K-1\\}$:\n    $$ \\delta_0(i) = \\log(\\pi_i) + \\log p(y_0 | s_0=i) $$\n    where the log-emission probability is:\n    $$ \\log p(y_n | s_n=i) = -\\frac{(y_n - L_i)^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2) $$\n\n2.  **Recursion ($n=1, \\dots, N-1$):**\n    For each state $j \\in \\{0, \\dots, K-1\\}$:\n    $$ \\delta_n(j) = \\max_{i=0,\\dots,K-1} \\left( \\delta_{n-1}(i) + \\log(P_{ij}) \\right) + \\log p(y_n | s_n=j) $$\n    To reconstruct the path, we store backpointers in an array $\\psi_n(j)$:\n    $$ \\psi_n(j) = \\arg\\max_{i=0,\\dots,K-1} \\left( \\delta_{n-1}(i) + \\log(P_{ij}) \\right) $$\n\n3.  **Termination:**\n    The log-probability of the most likely path is $p^* = \\max_{i=0,\\dots,K-1} \\delta_{N-1}(i)$.\n    The final state of this path is $\\hat{s}_{N-1} = \\arg\\max_{i=0,\\dots,K-1} \\delta_{N-1}(i)$.\n\n4.  **Path Backtracking:**\n    We recover the most likely path by backtracking from the final state:\n    For $n = N-2, \\dots, 0$:\n    $$ \\hat{s}_n = \\psi_{n+1}(\\hat{s}_{n+1}) $$\n\n### Path Accuracy Calculation\nThe final step is to quantify the performance of the decoder. The path accuracy is computed by comparing the decoded state sequence $\\{\\hat{s}_n\\}$ to the ground truth (synthetically generated) state sequence $\\{s_n\\}$. It is the fraction of time steps where the decoded state matches the true state:\n$$ \\text{Accuracy} = \\frac{1}{N} \\sum_{n=0}^{N-1} \\mathbf{1}(\\hat{s}_n = s_n) $$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise. The result is reported as a decimal rounded to six digits.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm, eig\n\ndef solve():\n    \"\"\"\n    Main function to run the HMM simulation and decoding for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"K\": 2,\n            \"L\": np.array([0.0, 20e-3]),\n            \"Lambda\": np.array([[-1500.0, 1500.0], [2000.0, -2000.0]]),\n            \"dt\": 1e-5,\n            \"sigma\": 1e-3,\n            \"N\": 4000,\n            \"seed\": 12345,\n        },\n        {\n            \"K\": 3,\n            \"L\": np.array([0.0, 10e-3, 25e-3]),\n            \"Lambda\": np.array([[-700.0, 600.0, 100.0], [300.0, -700.0, 400.0], [200.0, 500.0, -700.0]]),\n            \"dt\": 2e-5,\n            \"sigma\": 2e-3,\n            \"N\": 5000,\n            \"seed\": 202311,\n        },\n        {\n            \"K\": 3,\n            \"L\": np.array([0.0, 10e-3, 25e-3]),\n            \"Lambda\": np.array([[-700.0, 600.0, 100.0], [300.0, -700.0, 400.0], [200.0, 500.0, -700.0]]),\n            \"dt\": 2e-5,\n            \"sigma\": 12e-3,\n            \"N\": 5000,\n            \"seed\": 777,\n        },\n        {\n            \"K\": 4,\n            \"L\": np.array([0.0, 5e-3, 10e-3, 15e-3]),\n            \"Lambda\": np.array([[-10.0, 10.0, 0.0, 0.0], [10.0, -20.0, 10.0, 0.0], [0.0, 10.0, -20.0, 10.0], [0.0, 0.0, 10.0, -10.0]]),\n            \"dt\": 1e-3,\n            \"sigma\": 2e-3,\n            \"N\": 6000,\n            \"seed\": 42,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        K, L, Lambda, dt, sigma, N, seed = case.values()\n        rng = np.random.default_rng(seed)\n\n        # 1. Synthesize HMM data\n        s_true, y_obs, pi, P = synthesize_hmm_data(K, L, Lambda, dt, sigma, N, rng)\n\n        # 2. Decode using Viterbi\n        s_decoded = viterbi_decode(K, y_obs, pi, P, L, sigma)\n\n        # 3. Compute accuracy\n        accuracy = np.mean(s_true == s_decoded)\n        results.append(f\"{accuracy:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef get_stationary_distribution(P):\n    \"\"\"\n    Calculates the stationary distribution of a transition matrix P.\n    It is the normalized left eigenvector corresponding to eigenvalue 1.\n    \"\"\"\n    eigenvalues, eigenvectors = eig(P.T)\n    # Find the eigenvector corresponding to the eigenvalue closest to 1\n    idx = np.argmin(np.abs(eigenvalues - 1.0))\n    pi_unnormalized = eigenvectors[:, idx].real\n    pi = pi_unnormalized / np.sum(pi_unnormalized)\n    return pi\n\ndef synthesize_hmm_data(K, L, Lambda, dt, sigma, N, rng):\n    \"\"\"\n    Synthesizes hidden states and observations from an HMM.\n    \"\"\"\n    # Discrete-time transition matrix\n    P = expm(Lambda * dt)\n    \n    # Initial state distribution (stationary)\n    pi = get_stationary_distribution(P)\n\n    # Generate hidden states\n    states = np.zeros(N, dtype=int)\n    states[0] = rng.choice(K, p=pi)\n    for n in range(1, N):\n        states[n] = rng.choice(K, p=P[states[n-1], :])\n\n    # Generate observations\n    observations = rng.normal(loc=L[states], scale=sigma)\n    \n    return states, observations, pi, P\n\ndef viterbi_decode(K, y_obs, pi, P, L, sigma):\n    \"\"\"\n    Finds the most likely sequence of hidden states given observations\n    using the Viterbi algorithm in the log domain.\n    \"\"\"\n    N = len(y_obs)\n    \n    # Pre-compute log probabilities to avoid repeated calculations\n    # Small epsilon to avoid log(0)\n    log_pi = np.log(pi + 1e-300)\n    log_P = np.log(P + 1e-300)\n\n    # Log emission probabilities\n    log_emission_const = -0.5 * np.log(2 * np.pi * sigma**2)\n    def log_emission_prob(y, state_k):\n        return log_emission_const - ((y - L[state_k])**2) / (2 * sigma**2)\n\n    # Viterbi variables\n    delta = np.zeros((N, K))\n    psi = np.zeros((N, K), dtype=int)\n\n    # Initialization (n=0)\n    for i in range(K):\n        delta[0, i] = log_pi[i] + log_emission_prob(y_obs[0], i)\n\n    # Recursion (n=1 to N-1)\n    for n in range(1, N):\n        y_n_log_probs = np.array([log_emission_prob(y_obs[n], j) for j in range(K)])\n        for j in range(K):\n            # The transition part: delta_{n-1}(i) + log(P_ij)\n            trans_probs = delta[n-1, :] + log_P[:, j]\n            # Maximize over previous states i\n            psi[n, j] = np.argmax(trans_probs)\n            delta[n, j] = np.max(trans_probs) + y_n_log_probs[j]\n\n    # Path backtracking\n    decoded_path = np.zeros(N, dtype=int)\n    decoded_path[N-1] = np.argmax(delta[N-1, :])\n    for n in range(N-2, -1, -1):\n        decoded_path[n] = psi[n+1, decoded_path[n+1]]\n\n    return decoded_path\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}