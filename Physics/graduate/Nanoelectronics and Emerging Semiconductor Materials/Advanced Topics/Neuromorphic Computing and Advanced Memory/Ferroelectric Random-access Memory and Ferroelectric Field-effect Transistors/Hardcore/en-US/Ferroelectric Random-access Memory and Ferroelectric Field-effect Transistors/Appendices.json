{
    "hands_on_practices": [
        {
            "introduction": "The defining characteristic of a ferroelectric material is its polarization-electric field ($P$-$E$) hysteresis loop. This practice delves into the fundamental physical meaning of this loop by connecting its area to the energy dissipated during a memory write cycle. By calculating the energy cost for a single FeRAM cell, you will gain a tangible understanding of how material properties like remanent polarization ($P_r$) and coercive field ($E_c$) directly impact device power consumption .",
            "id": "4275892",
            "problem": "A planar capacitor in a Ferroelectric Random-Access Memory (FeRAM) cell is fabricated using a ferroelectric thin film. Under quasi-static cycling, the work per unit volume done by the applied electric field over one full polarization reversal is given by the closed-path integral of the field with respect to the polarization. A measured major polarization–electric field hysteresis loop is symmetric and can be well approximated by an ideal rectangular loop characterized by a remanent polarization $P_{r}$ and a coercive field $E_{c}$, with negligible reversible (linear dielectric) contribution compared to the switching polarization. The device has capacitor area $A$ and ferroelectric thickness $t$. Using a first-principles definition for the field–displacement work density and the loop integral, compute the total energy dissipated per switching cycle in Joules for a single device with the following measured and geometrical parameters:\n- Remanent polarization $P_{r} = 30\\,\\mu\\mathrm{C}/\\mathrm{cm}^{2}$,\n- Coercive field $E_{c} = 1.2\\,\\mathrm{MV}/\\mathrm{cm}$,\n- Area $A = 0.50\\,\\mu\\mathrm{m}^{2}$,\n- Thickness $t = 10\\,\\mathrm{nm}$.\n\nExpress your final answer in Joules and round to three significant figures. Do not use a percentage sign anywhere in your answer.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- The work per unit volume over one full polarization reversal is given by the closed-path integral $W_v = \\oint E \\, dP$, where $E$ is the electric field and $P$ is the polarization.\n- The major polarization–electric field hysteresis loop is approximated as a symmetric, ideal rectangular loop.\n- Reversible (linear dielectric) contribution is negligible.\n- Remanent polarization: $P_{r} = 30\\,\\mu\\mathrm{C}/\\mathrm{cm}^{2}$.\n- Coercive field: $E_{c} = 1.2\\,\\mathrm{MV}/\\mathrm{cm}$.\n- Capacitor area: $A = 0.50\\,\\mu\\mathrm{m}^{2}$.\n- Ferroelectric thickness: $t = 10\\,\\mathrm{nm}$.\n- The final answer is to be expressed in Joules, rounded to three significant figures.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is based on the well-established physics of ferroelectric materials. The energy dissipated per cycle corresponding to the area of the hysteresis loop is a fundamental concept. The values for $P_r$, $E_c$, $A$, and $t$ are realistic for modern ferroelectric devices.\n2.  **Well-Posed:** The problem provides a clear physical model (ideal rectangular loop) and all necessary parameters to calculate a single, unique numerical value for the dissipated energy.\n3.  **Objective:** The problem is stated using precise, objective scientific terminology.\n4.  **Completeness and Consistency:** All required data is provided, and there are no contradictions. The assumption of an ideal rectangular loop is a valid simplification for this type of problem.\n5.  **Realism:** The physical dimensions and material properties are within the plausible range for contemporary nanoelectronic devices.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be formulated.\n\nThe energy dissipated per unit volume, $W_v$, during one complete switching cycle is equal to the area enclosed by the polarization-electric field ($P$-$E$) hysteresis loop. The problem defines this as $W_v = \\oint E \\, dP$.\n\nFor an ideal rectangular hysteresis loop, the polarization switches between the positive remanent value, $+P_r$, and the negative remanent value, $-P_r$. The switching from $+P_r$ to $-P_r$ occurs at a constant electric field of $-E_c$, and the switching from $-P_r$ to $+P_r$ occurs at a constant field of $+E_c$.\n\nThe loop in the $P$-$E$ plane is thus a rectangle with vertices at $(E_c, P_r)$, $(-E_c, P_r)$, $(-E_c, -P_r)$, and $(E_c, -P_r)$.\nThe width of this rectangle is the difference between the positive and negative coercive fields: $\\Delta E = E_c - (-E_c) = 2E_c$.\nThe height of this rectangle is the difference between the positive and negative remanent polarizations: $\\Delta P = P_r - (-P_r) = 2P_r$.\n\nThe area of this rectangular loop, which represents the work per unit volume, is the product of its width and height:\n$$W_v = (\\text{width}) \\times (\\text{height}) = (2E_c)(2P_r) = 4E_c P_r$$\n\nTo find the total energy dissipated in the device, $W_{total}$, we must multiply this energy density by the volume, $V$, of the ferroelectric material. The volume is given by the product of the capacitor area $A$ and the film thickness $t$.\n$$V = A \\times t$$\n$$W_{total} = W_v \\times V = (4E_c P_r) \\times (A t)$$\n\nBefore calculating the final value, all given parameters must be converted to a consistent system of units, namely the International System of Units (SI).\n- Remanent polarization:\n$P_{r} = 30\\,\\mu\\mathrm{C}/\\mathrm{cm}^{2} = 30 \\times 10^{-6}\\,\\mathrm{C} / (10^{-2}\\,\\mathrm{m})^{2} = 30 \\times 10^{-6}\\,\\mathrm{C} / 10^{-4}\\,\\mathrm{m}^{2} = 30 \\times 10^{-2}\\,\\mathrm{C}/\\mathrm{m}^{2} = 0.3\\,\\mathrm{C}/\\mathrm{m}^{2}$.\n- Coercive field:\n$E_{c} = 1.2\\,\\mathrm{MV}/\\mathrm{cm} = 1.2 \\times 10^{6}\\,\\mathrm{V} / (10^{-2}\\,\\mathrm{m}) = 1.2 \\times 10^{8}\\,\\mathrm{V}/\\mathrm{m}$.\n- Area:\n$A = 0.50\\,\\mu\\mathrm{m}^{2} = 0.50 \\times (10^{-6}\\,\\mathrm{m})^{2} = 0.50 \\times 10^{-12}\\,\\mathrm{m}^{2}$.\n- Thickness:\n$t = 10\\,\\mathrm{nm} = 10 \\times 10^{-9}\\,\\mathrm{m} = 1.0 \\times 10^{-8}\\,\\mathrm{m}$.\n\nNow, substitute these SI values into the expression for $W_{total}$:\n$$W_{total} = 4 \\times (1.2 \\times 10^{8}\\,\\mathrm{V}/\\mathrm{m}) \\times (0.3\\,\\mathrm{C}/\\mathrm{m}^{2}) \\times (0.50 \\times 10^{-12}\\,\\mathrm{m}^{2}) \\times (1.0 \\times 10^{-8}\\,\\mathrm{m})$$\n\nThe units combine as $(\\mathrm{V}/\\mathrm{m}) \\times (\\mathrm{C}/\\mathrm{m}^{2}) \\times \\mathrm{m}^{2} \\times \\mathrm{m} = \\mathrm{V} \\cdot \\mathrm{C} = \\mathrm{J}$, since $1\\,\\mathrm{Joule} = 1\\,\\mathrm{Volt} \\times 1\\,\\mathrm{Coulomb}$.\n\nLet's compute the numerical value:\n$$W_{total} = (4 \\times 1.2 \\times 0.3 \\times 0.50 \\times 1.0) \\times 10^{8 - 12 - 8}\\,\\mathrm{J}$$\n$$W_{total} = (0.72) \\times 10^{-12}\\,\\mathrm{J}$$\n$$W_{total} = 7.2 \\times 10^{-13}\\,\\mathrm{J}$$\n\nThe problem requires the answer to be rounded to three significant figures.\n$$W_{total} = 7.20 \\times 10^{-13}\\,\\mathrm{J}$$\nThis is the total energy dissipated per switching cycle for the specified FeRAM cell.",
            "answer": "$$\\boxed{7.20 \\times 10^{-13}}$$"
        },
        {
            "introduction": "Beyond the static $P$-$E$ loop, the speed at which polarization can be switched is critical for memory performance. The Kolmogorov–Avrami–Ishibashi (KAI) model provides a powerful framework for describing the time-dependent kinetics of polarization reversal, which is governed by domain nucleation and growth. This hands-on coding exercise challenges you to fit switching data to the KAI model to extract key kinetic parameters, a crucial skill for characterizing and optimizing the write speed of ferroelectric devices .",
            "id": "4275866",
            "problem": "Consider a ferroelectric memory element in either Ferroelectric Random-Access Memory (FeRAM) or a Ferroelectric Field-Effect Transistor (FeFET), where the polarization switching under a constant rectangular electric field pulse is governed by nucleation-and-growth kinetics. A well-tested model for time-dependent polarization is the Kolmogorov–Avrami–Ishibashi (KAI) model, which expresses the switched polarization as a function of time as a monotonic transformation governed by dimensionality and a characteristic time scale. Use the KAI model as a foundational base to infer the kinetic parameters that describe the switching process from time-series data.\n\nStarting from the KAI model, define the polarization response to a constant field pulse of amplitude specified for each dataset as a function of time $t$ and material parameters: saturation polarization $P_s$, characteristic switching time $\\tau$, and dimensionality $n$. Treat $n$ as a dimensionless exponent quantifying the effective dimensionality of the domain growth process, and $\\tau$ as a time parameter that sets the kinetic timescale. Assume $P_s$ is known for each dataset from independent saturation measurements and remains constant during the pulse.\n\nYour task is to design a program that, for each provided dataset, performs parameter estimation of $n$ and $\\tau$ by fitting the KAI polarization function to the given polarization-versus-time values. The fitting should be done via a principled numerical approach that uses a transformation of the model to obtain initial estimates and then refines them through nonlinear least-squares optimization.\n\nFundamental base and definitions to be used:\n- Polarization $P(t)$ evolves monotonically from $0$ toward $P_s$ under a constant field pulse, and the KAI kinetics capture nucleation and growth with an exponent $n$ and timescale $\\tau$.\n- The natural logarithm and linear least-squares are standard transformations for model linearization when appropriate.\n- Nonlinear least-squares optimization refines parameter estimates by minimizing the sum of squared residuals between model and data.\n\nUnits and numerical conventions:\n- Time $t$ is expressed in microseconds (µs). All time quantities, including the output $\\tau$, must be expressed in µs.\n- Polarization $P$ and saturation polarization $P_s$ are expressed in microcoulombs per square centimeter (µC/cm$^2$).\n- Electric field amplitude $E$ is expressed in kilovolt per centimeter (kV/cm). The field amplitude labels each dataset but is not directly used in the fitting function; it contextualizes why $\\tau$ and $n$ may differ across datasets.\n- The dimensionality exponent $n$ is dimensionless.\n- The final numerical outputs for $n$ and $\\tau$ must be rounded to three decimal places.\n\nTest suite specification:\nYour program must fit $n$ and $\\tau$ for each of the following datasets. For each dataset, $P_s$, $E$, and the time points $t$ (in µs) are provided. The polarization samples $P(t)$ for each dataset are defined using the KAI model evaluated at the specified time points with the given ground-truth parameters $(n_{\\text{true}}, \\tau_{\\text{true}})$, without added noise, as follows:\n$$ P(t) = P_s \\left(1 - \\exp\\left(-\\left(\\frac{t}{\\tau_{\\text{true}}}\\right)^{n_{\\text{true}}}\\right)\\right). $$\n\n- Dataset A (slow, near one-dimensional growth):\n    - $E = 100$ kV/cm\n    - $P_s = 20$ µC/cm$^2$\n    - Ground truth: $n_{\\text{true}} = 1.2$, $\\tau_{\\text{true}} = 5.0$ µs\n    - Time points $t$ in µs: $[0.1, 0.2, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 7.0, 10.0]$\n\n- Dataset B (moderate switching, two-dimensional growth):\n    - $E = 200$ kV/cm\n    - $P_s = 25$ µC/cm$^2$\n    - Ground truth: $n_{\\text{true}} = 2.0$, $\\tau_{\\text{true}} = 1.5$ µs\n    - Time points $t$ in µs: $[0.05, 0.1, 0.2, 0.4, 0.8, 1.2, 1.6, 2.5, 3.5, 5.0]$\n\n- Dataset C (fast switching, higher-dimensional growth):\n    - $E = 300$ kV/cm\n    - $P_s = 15$ µC/cm$^2$\n    - Ground truth: $n_{\\text{true}} = 3.1$, $\\tau_{\\text{true}} = 0.7$ µs\n    - Time points $t$ in µs: $[0.02, 0.05, 0.08, 0.12, 0.2, 0.3, 0.5, 0.8, 1.2, 2.0]$\n\nAlgorithmic requirements:\n- Use a transformation-based method to obtain initial estimates for $n$ and $\\tau$ by exploiting the monotonicity of the KAI function and appropriate logarithmic transformations, ensuring only physically valid samples ($0  P(t)  P_s$) are used in the transformation.\n- Refine the parameter estimates using nonlinear least-squares fitting of the KAI model to the provided $P(t)$ data.\n- Constrain the fit to physically reasonable ranges: $0.5 \\le n \\le 5.0$, $\\tau  0$.\n\nEdge-case handling requirements:\n- Exclude samples too close to $P(t) = 0$ or $P(t) = P_s$ from the initial logarithmic transformation to avoid singularities from logarithms of $0$ or $1$.\n- Ensure numerical stability when computing logarithms by applying appropriate small thresholds.\n\nFinal output format:\nYour program should produce a single line of output containing the fitted dimensionality and characteristic time, rounded to three decimal places, for each dataset in the order A, B, C. The line must be a comma-separated list enclosed in square brackets, with the sequence $[n_A, \\tau_A, n_B, \\tau_B, n_C, \\tau_C]$, where each $\\tau$ is in µs, for example $[1.200,5.000,2.000,1.500,3.100,0.700]$.\n\nYour program must be self-contained, generate the synthetic $P(t)$ data internally from the KAI model using the specified ground-truth parameters and time points, perform the fitting procedure, and print the final results in the exact specified format.",
            "solution": "The problem requires the estimation of the kinetic parameters—the dimensionality exponent $n$ and the characteristic switching time $\\tau$—of the Kolmogorov–Avrami–Ishibashi (KAI) model for ferroelectric polarization switching. The estimation is to be performed on three distinct, noise-free datasets generated from the model itself. The specified methodology involves a two-step process: first, an initial estimation of parameters via a linearizing transformation of the model, and second, a refinement of these estimates using nonlinear least-squares optimization.\n\nThe KAI model describes the time-dependent switched polarization $P(t)$ under a constant applied electric field as:\n$$\nP(t) = P_s \\left(1 - \\exp\\left(-\\left(\\frac{t}{\\tau}\\right)^n\\right)\\right)\n$$\nHere, $P_s$ is the saturation polarization, which is provided as a known constant for each dataset. The parameters to be determined are $n$ and $\\tau$.\n\n**Step 1: Initial Parameter Estimation via Model Linearization**\n\nTo obtain initial estimates for $n$ and $\\tau$, we can transform the KAI equation into a linear form. This technique is often referred to as an Avrami plot.\n\nFirst, we rearrange the equation to isolate the exponential term:\n$$\n\\frac{P(t)}{P_s} = 1 - \\exp\\left(-\\left(\\frac{t}{\\tau}\\right)^n\\right)\n$$\n$$\n1 - \\frac{P(t)}{P_s} = \\exp\\left(-\\left(\\frac{t}{\\tau}\\right)^n\\right)\n$$\n\nThis transformation is valid for data points where $P(t)  P_s$. Taking the natural logarithm of both sides gives:\n$$\n\\ln\\left(1 - \\frac{P(t)}{P_s}\\right) = -\\left(\\frac{t}{\\tau}\\right)^n\n$$\n\nThis step requires that $1 - \\frac{P(t)}{P_s}  0$, which is equivalent to $P(t)  P_s$. To proceed, we make the left-hand side positive and take the logarithm again:\n$$\n-\\ln\\left(1 - \\frac{P(t)}{P_s}\\right) = \\left(\\frac{t}{\\tau}\\right)^n\n$$\n\nTaking the natural logarithm of both sides once more yields the desired linear form. This step is only valid for $P(t)  0$, as this ensures $-\\ln(1 - P(t)/P_s)  0$.\n$$\n\\ln\\left(-\\ln\\left(1 - \\frac{P(t)}{P_s}\\right)\\right) = \\ln\\left(\\left(\\frac{t}{\\tau}\\right)^n\\right)\n$$\n\nUsing the properties of logarithms, we can expand the right side:\n$$\n\\ln\\left(-\\ln\\left(1 - \\frac{P(t)}{P_s}\\right)\\right) = n \\left( \\ln(t) - \\ln(\\tau) \\right)\n$$\n$$\n\\ln\\left(-\\ln\\left(1 - \\frac{P(t)}{P_s}\\right)\\right) = n \\ln(t) - n \\ln(\\tau)\n$$\n\nThis equation is in the form of a straight line, $Y = mX + C$, where:\n- $Y = \\ln\\left(-\\ln\\left(1 - \\frac{P(t)}{P_s}\\right)\\right)$\n- $X = \\ln(t)$\n- The slope is $m = n$.\n- The y-intercept is $C = -n \\ln(\\tau)$.\n\nBy performing a linear least-squares regression on the transformed data points $(X_i, Y_i)$, we can determine the slope $m$ and intercept $C$. From these, we derive the initial estimates for our parameters, denoted as $n_0$ and $\\tau_0$:\n$$\nn_0 = m\n$$\n$$\n\\tau_0 = \\exp\\left(-\\frac{C}{m}\\right) = \\exp\\left(-\\frac{C}{n_0}\\right)\n$$\n\nAs stipulated by the problem's edge-case requirements, this transformation is numerically unstable when $P(t)$ is very close to $0$ or $P_s$. Specifically, $\\ln(0)$ is encountered. Therefore, data points where $P(t)$ is near these limits must be excluded from this initial estimation step. A practical approach is to filter out data points where the switched fraction $P(t)/P_s$ is outside a certain range, for example, $0.01  P(t)/P_s  0.99$.\n\n**Step 2: Parameter Refinement via Nonlinear Least-Squares (NLLS) Optimization**\n\nThe initial estimates $(n_0, \\tau_0)$ from the linear fit serve as a starting point for a more robust NLLS optimization. This method directly fits the original KAI model to the data without any transformation, thereby avoiding the biases that can be introduced by the logarithmic operations.\n\nThe goal of NLLS is to find the parameter values $(n, \\tau)$ that minimize the sum of the squared residuals ($SSR$) between the observed polarization data $P_i$ and the values predicted by the model $P_{\\text{model}}(t_i; n, \\tau)$:\n$$\nSSR(n, \\tau) = \\sum_{i=1}^{N} \\left( P_i - P_{\\text{model}}(t_i; n, \\tau) \\right)^2\n$$\nwhere $N$ is the number of data points.\n\nThis optimization problem is solved numerically. The `scipy.optimize.curve_fit` function in Python's SciPy library is an excellent tool for this purpose. It implements the Levenberg-Marquardt algorithm, which is a standard and efficient method for NLLS. Crucially, this function allows for the incorporation of bounds on the parameters, as required by the problem:\n- $0.5 \\le n \\le 5.0$\n- $\\tau  0$ (practically, $\\tau \\ge \\epsilon$ for a small positive $\\epsilon$)\n\n**Algorithmic Procedure for Each Dataset**\n\n1.  **Data Generation**: For each test case, generate the synthetic polarization data $P(t_i)$ using the provided ground-truth parameters ($n_{\\text{true}}$, $\\tau_{\\text{true}}$), saturation polarization $P_s$, and time points $t_i$.\n2.  **Initial Guess**:\n    a. Filter the $(t_i, P_i)$ data pairs, excluding points where $P_i/P_s$ is too close to $0$ or $1$.\n    b. Apply the double-logarithmic transformation to the filtered data to obtain $(X_i, Y_i)$.\n    c. Perform a linear regression on $(X_i, Y_i)$ to find the slope $m$ and intercept $C$.\n    d. Calculate the initial estimates $n_0 = m$ and $\\tau_0 = \\exp(-C/m)$.\n3.  **Nonlinear Refinement**:\n    a. Define the KAI model function for the NLLS fitter, with $t$ as the independent variable and $(n, \\tau)$ as the parameters to be fitted. $P_s$ is a fixed constant for the fit.\n    b. Call `scipy.optimize.curve_fit` with the KAI function, the full $(t_i, P_i)$ dataset, the initial guesses $(n_0, \\tau_0)$, and the specified bounds on $n$ and $\\tau$.\n4.  **Result Collection**: Extract the optimized parameters $(n_{\\text{fit}}, \\tau_{\\text{fit}})$ and round them to three decimal places as required.\n5.  **Final Output**: Collate the results from all datasets into a single list and format them into the specified string format for printing.\n\nSince the input data is noiseless and generated directly from the KAI model, the fitting procedure is expected to recover the ground-truth parameters with high accuracy, limited only by the numerical precision of the algorithms.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef solve():\n    \"\"\"\n    Solves for the KAI model parameters for multiple datasets\n    by generating synthetic data and then applying a two-step fitting procedure.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            \"name\": \"A\",\n            \"E\": 100.0,  # kV/cm\n            \"Ps\": 20.0,  # µC/cm^2\n            \"n_true\": 1.2,\n            \"tau_true\": 5.0,  # µs\n            \"t_points\": np.array([0.1, 0.2, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 7.0, 10.0]), # µs\n        },\n        # Dataset B\n        {\n            \"name\": \"B\",\n            \"E\": 200.0,\n            \"Ps\": 25.0,\n            \"n_true\": 2.0,\n            \"tau_true\": 1.5,\n            \"t_points\": np.array([0.05, 0.1, 0.2, 0.4, 0.8, 1.2, 1.6, 2.5, 3.5, 5.0]),\n        },\n        # Dataset C\n        {\n            \"name\": \"C\",\n            \"E\": 300.0,\n            \"Ps\": 15.0,\n            \"n_true\": 3.1,\n            \"tau_true\": 0.7,\n            \"t_points\": np.array([0.02, 0.05, 0.08, 0.12, 0.2, 0.3, 0.5, 0.8, 1.2, 2.0]),\n        },\n    ]\n\n    results = []\n\n    def kai_model(t, Ps, n, tau):\n        \"\"\"KAI model for polarization switching.\"\"\"\n        return Ps * (1.0 - np.exp(-(t / tau)**n))\n\n    for case in test_cases:\n        # Step 1: Generate synthetic data\n        Ps = case[\"Ps\"]\n        n_true = case[\"n_true\"]\n        tau_true = case[\"tau_true\"]\n        t_data = case[\"t_points\"]\n        P_data = kai_model(t_data, Ps, n_true, tau_true)\n\n        # Step 2: Obtain initial parameter estimates via linearization\n        \n        # Filter data to avoid log(0) issues, as per problem requirements.\n        # Only use points where the switched fraction is between 1% and 99%.\n        # This is robust for typical Avrami analysis.\n        valid_indices = np.where((P_data > 0.01 * Ps)  (P_data  0.99 * Ps))\n        t_filtered = t_data[valid_indices]\n        P_filtered = P_data[valid_indices]\n\n        # Apply the linearizing transformation: Y = n*X - n*ln(tau)\n        # where Y = ln(-ln(1 - P/Ps)) and X = ln(t)\n        X = np.log(t_filtered)\n        Y = np.log(-np.log(1.0 - P_filtered / Ps))\n        \n        if len(X)  2:\n            # Fallback if too few points remain after filtering.\n            # Using true values as guess for robustness, though not expected here.\n            n_initial, tau_initial = n_true, tau_true\n        else:\n            # Perform linear regression to find slope (n) and intercept (-n*ln(tau))\n            # `polyfit` with degree 1 returns [slope, intercept]\n            slope, intercept = np.polyfit(X, Y, 1)\n\n            n_initial = slope\n            # From intercept = -n*ln(tau) => tau = exp(-intercept/n)\n            tau_initial = np.exp(-intercept / n_initial)\n\n        # Step 3: Refine estimates using nonlinear least-squares (NLLS)\n\n        # Define the model function for curve_fit. Ps is fixed.\n        def kai_fit_func(t, n, tau):\n            return kai_model(t, Ps, n, tau)\n            \n        # Define parameter bounds as per problem specification.\n        # tau > 0 is handled by setting a small positive lower bound.\n        bounds = ([0.5, 1e-9], [5.0, np.inf])\n\n        # provide the initial guesses to the optimizer\n        initial_guess = [n_initial, tau_initial]\n\n        # Perform the NLLS fit\n        popt, _ = curve_fit(\n            f=kai_fit_func,\n            xdata=t_data,\n            ydata=P_data,\n            p0=initial_guess,\n            bounds=bounds\n        )\n\n        n_fit, tau_fit = popt\n\n        # Append rounded results to the list\n        results.append(round(n_fit, 3))\n        results.append(round(tau_fit, 3))\n\n    # Final print statement in the exact required format.\n    formatted_results = \",\".join([f\"{x:.3f}\" for x in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This capstone problem bridges the gap between single-device physics and system-level reliability. Real-world memory arrays consist of millions of devices, and their performance is limited by device-to-device variability and random noise, which lead to errors in reading stored data. By modeling statistical variations in a FeFET array and connecting the resulting bit error rate (BER) to the requirements of error-correcting codes (ECC), you will tackle a real-world engineering challenge: designing a reliable memory system from imperfect components .",
            "id": "4275863",
            "problem": "You are given an idealized statistical model of an array of Ferroelectric Field-Effect Transistors (FeFETs), used to store binary information through the orientation of ferroelectric polarization. Each device has an intrinsic threshold voltage $V_{th,0}$ and a remanent polarization $P_r$ that induces bound charge density at the gate interface. The ferroelectric contribution shifts the effective threshold voltage according to the capacitor relation, where bound charge density modifies the gate potential via an effective gate capacitance per unit area $C_{ox}$. Treating the ferroelectric bound charge density as a sheet charge of magnitude $P_r$ and using the capacitor relation $Q = C V$ with $Q = P_r$ per unit area, the polarization-induced threshold shift has magnitude $|P_r|/C_{ox}$. The sign depends on the stored bit: define bit $1$ to correspond to a polarization orientation that lowers the threshold and bit $0$ to correspond to a polarization orientation that raises the threshold. The readout uses a comparator at a fixed decision voltage $V_{cmp}$ that classifies the device as storing bit $1$ if the measured effective threshold is less than or equal to $V_{cmp}$, and bit $0$ otherwise. Measurement noise is modeled as an additive zero-mean Gaussian noise on the effective threshold.\n\nAssume the following statistical model:\n- The intrinsic threshold voltage $V_{th,0}$ is independent and identically distributed across devices as a normal random variable with mean $\\mu_V$ and standard deviation $\\sigma_V$.\n- The remanent polarization $P_r$ is independent and identically distributed across devices as a normal random variable with mean $\\mu_P$ and standard deviation $\\sigma_P$.\n- The readout noise is an independent, zero-mean normal random variable with standard deviation $\\sigma_R$ that adds to the measured effective threshold at read time.\n\nUnder these assumptions, the effective threshold voltage for storing a bit $1$ is $V_{th,1} = V_{th,0} - P_r/C_{ox} + \\eta_R$, and for bit $0$ is $V_{th,0,\\mathrm{eff}} = V_{th,0} + P_r/C_{ox} + \\eta_R$, where $\\eta_R$ is the read noise. Using the comparator decision rule, a bit error occurs for bit $1$ if $V_{th,1}  V_{cmp}$, and for bit $0$ if $V_{th,0,\\mathrm{eff}} \\le V_{cmp}$. Assume bits are equally likely and independent.\n\nYour tasks are:\n1. Derive from first principles, starting from $Q = C V$ and the properties of independent normal random variables, the bit error rate (BER) as a single decimal number for the above model as a function of $\\mu_V$, $\\sigma_V$, $\\mu_P$, $\\sigma_P$, $C_{ox}$, $V_{cmp}$, and $\\sigma_R$.\n2. For a word of $n$ bits evaluated under independent bit errors with probability equal to the BER obtained in part 1, compute the minimal integer $t$ such that the probability that the word experiences more than $t$ bit errors is less than or equal to a target word failure probability $\\mathcal{F}_{\\mathrm{target}}$ (expressed as a decimal). Under binary linear coding and the sphere-packing argument in Hamming space, the number of redundancy bits $r$ must satisfy $2^r \\ge \\sum_{i=0}^{t} \\binom{n}{i}$. Compute the minimal integer redundancy $r$ that satisfies this inequality for your chosen $t$.\n3. Return, for each test case, a list containing the BER, the redundancy $r$ (an integer), the error-correction capability $t$ (an integer), and the resulting word failure probability after correction (decimal), computed under the assumption of independent bit errors and a decoder that corrects up to $t$ errors.\n\nExpress all probabilities as decimals. All voltages must be in volts (V), polarization in coulombs per square meter (C/m$^2$), and capacitance per unit area in farads per square meter (F/m$^2$). No percentage signs are permitted.\n\nTest suite:\nProvide results for the following parameter sets $(\\mu_V,\\sigma_V,\\mu_P,\\sigma_P,C_{ox},V_{cmp},\\sigma_R,n,\\mathcal{F}_{\\mathrm{target}})$:\n- Case A (typical operating point): $(0.4, 0.05, 0.2, 0.02, 0.1, 0.5, 0.1, 128, 1\\times 10^{-9})$\n- Case B (weak polarization and larger noise): $(0.4, 0.08, 0.05, 0.03, 0.1, 0.5, 0.15, 256, 1\\times 10^{-6})$\n- Case C (zero read noise): $(0.55, 0.1, 0.1, 0.01, 0.08, 0.6, 0.0, 64, 1\\times 10^{-12})$\n- Case D (high device variability): $(0.4, 0.25, 0.1, 0.05, 0.12, 0.5, 0.2, 128, 1\\times 10^{-4})$\n- Case E (comparator centered at intrinsic mean, large word): $(0.5, 0.05, 0.08, 0.02, 0.1, 0.5, 0.05, 1024, 1\\times 10^{-15})$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of the form $[\\mathrm{BER},r,t,\\mathcal{F}_{\\mathrm{word}}]$ for the corresponding test case, with no spaces (for example, [[0.001,5,1,0.0001],[...],...]).",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in a simplified but physically plausible model of FeFET device statistics, well-posed with all necessary information provided, and stated objectively. The tasks are mathematically and computationally tractable. We can therefore proceed with a full solution.\n\nThe solution is divided into three main parts as requested: first, the derivation of the bit error rate (BER); second, the methodology to determine the error-correction capability $t$ and the required redundancy bits $r$; and third, the application of this methodology to the given test cases.\n\n**Part 1: Derivation of the Bit Error Rate (BER)**\n\nThe bit error rate (BER) is the probability of a single bit being read incorrectly. Since the problem states that bits $0$ and $1$ are stored with equal likelihood, the BER is the average of the error probabilities for each case:\n$$ \\mathrm{BER} = P(\\text{error}) = P(\\text{error} | \\text{bit } 0)P(\\text{bit } 0) + P(\\text{error} | \\text{bit } 1)P(\\text{bit } 1) = \\frac{1}{2} P(\\text{error}_0) + \\frac{1}{2} P(\\text{error}_1) $$\nwhere $P(\\text{error}_0)$ and $P(\\text{error}_1)$ are the conditional error probabilities for a stored bit $0$ and bit $1$, respectively.\n\nThe model specifies three independent sources of variation, all following normal distributions:\n1. Intrinsic threshold voltage: $V_{th,0} \\sim \\mathcal{N}(\\mu_V, \\sigma_V^2)$\n2. Remanent polarization: $P_r \\sim \\mathcal{N}(\\mu_P, \\sigma_P^2)$\n3. Readout noise: $\\eta_R \\sim \\mathcal{N}(0, \\sigma_R^2)$\n\nThe effective threshold voltage shift due to polarization $P_r$ is given by the capacitor relation $\\Delta V = Q/C$. With the charge per unit area $Q$ being $P_r$, the shift is $P_r/C_{ox}$.\n\nLet us define the random variable $\\Delta V_P = P_r/C_{ox}$. As $P_r$ is a normal random variable, $\\Delta V_P$ is also a normal random variable. Its mean and variance are:\n$$ E[\\Delta V_P] = E[P_r/C_{ox}] = \\frac{1}{C_{ox}}E[P_r] = \\frac{\\mu_P}{C_{ox}} $$\n$$ \\mathrm{Var}(\\Delta V_P) = \\mathrm{Var}(P_r/C_{ox}) = \\frac{1}{C_{ox}^2}\\mathrm{Var}(P_r) = \\frac{\\sigma_P^2}{C_{ox}^2} $$\nSo, $\\Delta V_P \\sim \\mathcal{N}\\left(\\frac{\\mu_P}{C_{ox}}, \\left(\\frac{\\sigma_P}{C_{ox}}\\right)^2\\right)$.\n\nWe now analyze the measured effective threshold voltage for each bit.\nFor a stored bit $0$, the threshold is raised: $V_{th,0,\\mathrm{eff}} = V_{th,0} + \\Delta V_P + \\eta_R$.\nFor a stored bit $1$, the threshold is lowered: $V_{th,1} = V_{th,0} - \\Delta V_P + \\eta_R$.\n\nThese effective thresholds are linear combinations of independent normal random variables, and thus are themselves normally distributed. Let's find their parameters.\nThe mean of $V_{th,0,\\mathrm{eff}}$ is $\\mu_{0,\\mathrm{eff}} = E[V_{th,0}] + E[\\Delta V_P] + E[\\eta_R] = \\mu_V + \\frac{\\mu_P}{C_{ox}} + 0 = \\mu_V + \\frac{\\mu_P}{C_{ox}}$.\nThe mean of $V_{th,1}$ is $\\mu_{1,\\mathrm{eff}} = E[V_{th,0}] - E[\\Delta V_P] + E[\\eta_R] = \\mu_V - \\frac{\\mu_P}{C_{ox}} + 0 = \\mu_V - \\frac{\\mu_P}{C_{ox}}$.\n\nDue to the independence of the components, the variance is the sum of the individual variances for both cases:\n$$ \\sigma_{\\mathrm{eff}}^2 = \\mathrm{Var}(V_{th,0}) + \\mathrm{Var}(\\pm \\Delta V_P) + \\mathrm{Var}(\\eta_R) = \\sigma_V^2 + \\left(\\frac{\\sigma_P}{C_{ox}}\\right)^2 + \\sigma_R^2 $$\nSo, $V_{th,0,\\mathrm{eff}} \\sim \\mathcal{N}(\\mu_{0,\\mathrm{eff}}, \\sigma_{\\mathrm{eff}}^2)$ and $V_{th,1} \\sim \\mathcal{N}(\\mu_{1,\\mathrm{eff}}, \\sigma_{\\mathrm{eff}}^2)$.\n\nAn error occurs for bit $0$ if it is read as a $1$, which happens when $V_{th,0,\\mathrm{eff}} \\le V_{cmp}$. The probability is:\n$$ P(\\text{error}_0) = P(V_{th,0,\\mathrm{eff}} \\le V_{cmp}) = \\Phi\\left(\\frac{V_{cmp} - \\mu_{0,\\mathrm{eff}}}{\\sigma_{\\mathrm{eff}}}\\right) = \\Phi\\left(\\frac{V_{cmp} - (\\mu_V + \\mu_P/C_{ox})}{\\sqrt{\\sigma_V^2 + (\\sigma_P/C_{ox})^2 + \\sigma_R^2}}\\right) $$\nwhere $\\Phi(z)$ is the cumulative distribution function (CDF) of the standard normal distribution.\n\nAn error occurs for bit $1$ if it is read as a $0$, which happens when $V_{th,1}  V_{cmp}$. The probability is:\n$$ P(\\text{error}_1) = P(V_{th,1}  V_{cmp}) = 1 - P(V_{th,1} \\le V_{cmp}) = 1 - \\Phi\\left(\\frac{V_{cmp} - \\mu_{1,\\mathrm{eff}}}{\\sigma_{\\mathrm{eff}}}\\right) $$\nUsing the symmetry property $1 - \\Phi(z) = \\Phi(-z)$:\n$$ P(\\text{error}_1) = \\Phi\\left(\\frac{\\mu_{1,\\mathrm{eff}} - V_{cmp}}{\\sigma_{\\mathrm{eff}}}\\right) = \\Phi\\left(\\frac{(\\mu_V - \\mu_P/C_{ox}) - V_{cmp}}{\\sqrt{\\sigma_V^2 + (\\sigma_P/C_{ox})^2 + \\sigma_R^2}}\\right) $$\n\nFinally, the total BER is:\n$$ \\mathrm{BER} = \\frac{1}{2} \\left[ \\Phi\\left(\\frac{V_{cmp} - \\mu_V - \\mu_P/C_{ox}}{\\sigma_{\\mathrm{eff}}}\\right) + \\Phi\\left(\\frac{\\mu_V - \\mu_P/C_{ox} - V_{cmp}}{\\sigma_{\\mathrm{eff}}}\\right) \\right] $$\nwith $\\sigma_{\\mathrm{eff}} = \\sqrt{\\sigma_V^2 + (\\sigma_P/C_{ox})^2 + \\sigma_R^2}$.\n\n**Part 2: Error Correction Parameters $t$ and $r$**\n\nGiven the BER, $p$, calculated in Part 1, we model the number of errors, $k$, in a word of length $n$ bits. Assuming independent bit errors, $k$ follows a binomial distribution, $k \\sim B(n, p)$, with probability mass function $P(k; n, p) = \\binom{n}{k} p^k (1-p)^{n-k}$.\n\nAn error correction code with correction capability $t$ can correct any error pattern with $t$ or fewer errors. The word is uncorrectable if the number of errors $k$ is greater than $t$. The word failure probability, $\\mathcal{F}_{\\mathrm{word}}$, is thus:\n$$ \\mathcal{F}_{\\mathrm{word}}(t) = P(k  t) = \\sum_{k=t+1}^{n} \\binom{n}{k} p^k (1-p)^{n-k} $$\nThis is the survival function of the binomial distribution. The first task is to find the minimal integer $t \\ge 0$ such that this probability is less than or equal to the target word failure probability, $\\mathcal{F}_{\\mathrm{target}}$:\n$$ \\text{Find minimal integer } t \\text{ such that } \\mathcal{F}_{\\mathrm{word}}(t) \\le \\mathcal{F}_{\\mathrm{target}} $$\nThis is found by iterating $t$ from $0, 1, 2, \\dots$ and calculating $\\mathcal{F}_{\\mathrm{word}}(t)$ until the condition is met. The corresponding value of $\\mathcal{F}_{\\mathrm{word}}(t)$ is the final reported word failure probability.\n\nThe second task is to find the minimum number of redundancy bits, $r$, required to construct a code that can correct up to $t$ errors for a word of length $n$. The sphere-packing bound (or Hamming bound) gives a lower limit on $r$. It states that a code must have enough redundancy to uniquely identify every possible error pattern it is designed to correct. For a binary code of length $n$ that corrects up to $t$ errors, the $2^r$ redundant states must be able to distinguish the error patterns for each of the $2^{n-r}$ valid codewords. The number of error patterns of weight $i$ is $\\binom{n}{i}$. Thus, for each codeword, we must be able to identify all error patterns up to weight $t$. The bound is:\n$$ 2^r \\ge \\sum_{i=0}^{t} \\binom{n}{i} $$\nTo find the minimal integer $r$, we first compute the volume of the Hamming ball $V(n,t) = \\sum_{i=0}^{t} \\binom{n}{i}$, and then solve for $r$:\n$$ r \\ge \\log_2(V(n,t)) $$\nThe minimal integer $r$ is therefore:\n$$ r = \\lceil \\log_2(V(n,t)) \\rceil $$\nwhere $\\lceil \\cdot \\rceil$ is the ceiling function. For numerical stability with large $n$ and $t$, the sum for $V(n,t)$ is best computed in the log domain using `logsumexp` and `gammaln` functions.\n\n**Part 3: Computational Procedure**\n\nThe final output is a list of results for each test case. The overall procedure for each case is:\n1. Calculate the effective standard deviation $\\sigma_{\\mathrm{eff}}$.\n2. Calculate the arguments of the CDF $\\Phi$ for $P(\\text{error}_0)$ and $P(\\text{error}_1)$.\n3. Compute the BER, $p$, using the formula derived in Part 1. Special care is taken for very small probabilities using logarithmic representations to maintain numerical precision.\n4. Search for the minimal integer $t$ by evaluating $\\mathcal{F}_{\\mathrm{word}}(t)$ for $t=0, 1, 2, \\ldots$ until $\\mathcal{F}_{\\mathrm{word}}(t) \\le \\mathcal{F}_{\\mathrm{target}}$. The final $\\mathcal{F}_{\\mathrm{word}}$ is this value.\n5. Compute the sum $V(n,t) = \\sum_{i=0}^{t} \\binom{n}{i}$.\n6. Calculate the minimal integer redundancy $r = \\lceil \\log_2(V(n,t)) \\rceil$.\n7. Collate the four computed values: $[\\text{BER}, r, t, \\mathcal{F}_{\\mathrm{word}}]$.\n\nThe following program implements this procedure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, binom\nfrom scipy.special import gammaln, logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the FeFET statistical analysis problem for all test cases.\n    \"\"\"\n    # Test cases parameters:\n    # (mu_V, sigma_V, mu_P, sigma_P, C_ox, V_cmp, sigma_R, n, F_target)\n    test_cases = [\n        # Case A: typical operating point\n        (0.4, 0.05, 0.2, 0.02, 0.1, 0.5, 0.1, 128, 1e-9),\n        # Case B: weak polarization and larger noise\n        (0.4, 0.08, 0.05, 0.03, 0.1, 0.5, 0.15, 256, 1e-6),\n        # Case C: zero read noise\n        (0.55, 0.1, 0.1, 0.01, 0.08, 0.6, 0.0, 64, 1e-12),\n        # Case D: high device variability\n        (0.4, 0.25, 0.1, 0.05, 0.12, 0.5, 0.2, 128, 1e-4),\n        # Case E: comparator centered at intrinsic mean, large word\n        (0.5, 0.05, 0.08, 0.02, 0.1, 0.5, 0.05, 1024, 1e-15),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = calculate_metrics(*params)\n        results.append(result)\n\n    # Format the final output string as specified\n    output_str = \"[\" + \",\".join([f\"[{res[0]},{res[1]},{res[2]},{res[3]}]\" for res in results]) + \"]\"\n    print(output_str)\n\ndef calculate_metrics(mu_V, sigma_V, mu_P, sigma_P, C_ox, V_cmp, sigma_R, n, F_target):\n    \"\"\"\n    Calculates BER, r, t, and F_word for a single set of parameters.\n    \"\"\"\n    # Part 1: Calculate Bit Error Rate (BER)\n    \n    # Calculate the variance of the effective threshold voltage\n    # sigma_eff^2 = sigma_V^2 + (sigma_P/C_ox)^2 + sigma_R^2\n    sigma_p_on_cox_sq = (sigma_P / C_ox)**2\n    sigma_eff_sq = sigma_V**2 + sigma_p_on_cox_sq + sigma_R**2\n    sigma_eff = np.sqrt(sigma_eff_sq)\n\n    # Calculate means of effective threshold voltages for bits 0 and 1\n    mu_p_on_cox = mu_P / C_ox\n    mu_0_eff = mu_V + mu_p_on_cox\n    mu_1_eff = mu_V - mu_p_on_cox\n\n    # Calculate error probabilities for bits 0 and 1\n    # To maintain precision for very small probabilities, work with log-CDF\n    if sigma_eff > 0:\n        z0 = (V_cmp - mu_0_eff) / sigma_eff\n        z1 = (mu_1_eff - V_cmp) / sigma_eff\n        log_p_err0 = norm.logcdf(z0)\n        log_p_err1 = norm.logcdf(z1)\n        # BER = 0.5 * (P(error_0) + P(error_1))\n        # Use logsumexp for numerical stability: log(a+b) = log(exp(log_a)+exp(log_b))\n        log_sum_p_err = logsumexp([log_p_err0, log_p_err1])\n        log_ber = np.log(0.5) + log_sum_p_err\n        ber = np.exp(log_ber)\n    else: # Deterministic case, no variance\n        p_err0 = 1.0 if mu_0_eff = V_cmp else 0.0\n        p_err1 = 1.0 if mu_1_eff > V_cmp else 0.0\n        ber = 0.5 * (p_err0 + p_err1)\n\n    if ber == 0.0:\n        # If BER is effectively zero, no errors are expected\n        t = 0\n        r = 0\n        F_word = 0.0\n        return [ber, r, t, F_word]\n\n    # Part 2: Find error-correction capability t and redundancy r\n    \n    # Find minimal integer t such that P(errors > t) = F_target\n    t_final = -1\n    F_word_final = -1.0\n    for t_candidate in range(n + 1):\n        # P(errors > t) is the survival function (sf) of the binomial distribution\n        F_word_candidate = binom.sf(t_candidate, n, ber)\n        if F_word_candidate = F_target:\n            t_final = t_candidate\n            F_word_final = F_word_candidate\n            break\n    \n    # This should always find a t, as F_word(n) = 0.\n    if t_final == -1:\n        t_final = n\n        F_word_final = 0.0\n    \n    # Calculate redundancy bits r using the sphere-packing bound\n    # 2^r >= sum_{i=0 to t} C(n, i)\n    # r >= log2(sum_{i=0 to t} C(n, i))\n    if t_final == 0:\n        # Sum is C(n,0) = 1, so log2(1) = 0\n        r_final = 0\n    else:\n        # For numerical stability, calculate the sum of combinations in log space\n        # log(C(n,k)) = gammaln(n+1) - gammaln(k+1) - gammaln(n-k+1)\n        log_comb_terms = [gammaln(n + 1) - gammaln(i + 1) - gammaln(n - i + 1) for i in range(t_final + 1)]\n        log_sum_comb = logsumexp(log_comb_terms)\n        # r = ceil(log_sum_comb / log(2))\n        r_final = int(np.ceil(log_sum_comb / np.log(2)))\n\n    return [ber, r_final, t_final, F_word_final]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}