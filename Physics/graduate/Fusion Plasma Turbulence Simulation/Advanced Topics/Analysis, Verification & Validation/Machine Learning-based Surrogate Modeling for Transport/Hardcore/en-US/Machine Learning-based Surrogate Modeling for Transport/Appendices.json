{
    "hands_on_practices": [
        {
            "introduction": "A machine learning surrogate is only as good as its adherence to the physical principles it aims to model. Before deploying a surrogate for complex predictions, it is crucial to verify that it respects fundamental physical laws. This first practice focuses on validating a surrogate against two foundational constraints: the principle that turbulent flux should vanish in the absence of driving thermodynamic gradients, and the odd-parity response of the flux to a small reversal of a single driving gradient.\n\nThis exercise  guides you through the process of defining and calculating quantitative residuals that measure a model's deviation from these basic symmetries. Mastering this type of validation is a critical first step in building trust in any surrogate model and ensuring its predictions are physically plausible, especially near the operational limits of a fusion device.",
            "id": "4194327",
            "problem": "Consider a machine learning-based surrogate for fusion plasma turbulent transport that maps dimensionless local gradient inputs to a gyro-Bohm normalized radial heat flux. Let $x$ denote the dimensionless density gradient parameter and $y$ the dimensionless temperature gradient parameter, both defined as ratios of a macroscopic length scale to their respective gradient scale lengths. The surrogate predicts the normalized heat flux $Q_{\\text{s}}(x,y)$, which is expressed in units of Gyro-Bohm (gB) normalization. For a flux-surface averaged, local diffusive closure, fundamental transport theory implies that the turbulent heat flux is driven by thermodynamic gradients. In particular, at vanishing gradients the diffusive component of the turbulent flux vanishes, and to leading order the flux reverses sign when a single driving gradient is reversed, reflecting odd parity in the gradient. These analytic limits are used to verify surrogate consistency.\n\nStarting from the fundamental constitutive relation for diffusive heat transport, where the heat flux is proportional to the negative temperature gradient, the following two analytic limits should hold for a physically consistent surrogate:\n- Zero-gradient limit: $Q_{\\text{true}}(0,0)=0$.\n- Odd symmetry under single-gradient reversal at small amplitude: for fixed $x=0$ and small $|y|$, the leading-order response satisfies $Q_{\\text{true}}(0,-y)=-Q_{\\text{true}}(0,y)$.\n\nDefine verification residuals that measure the surrogate’s deviation from these analytic limits as follows:\n- Zero-gradient residual: $R_{0}=\\left|Q_{\\text{s}}(0,0)-0\\right|$.\n- Odd-symmetry residual at a prescribed test gradient magnitude $y_{0}$: \n$$\nR_{\\text{odd}}(y_{0})=\\left|\\frac{Q_{\\text{s}}(0,y_{0})+Q_{\\text{s}}(0,-y_{0})}{Q_{\\text{s}}(0,y_{0})-Q_{\\text{s}}(0,-y_{0})}\\right|,\n$$\nwhich equals the magnitude of the even component divided by the magnitude of the odd component at $y_{0}$.\n\nSuppose the surrogate takes the following polynomial form in the gradients:\n$$\nQ_{\\text{s}}(x,y)=a\\,x+b\\,y+c\\,x\\,y+d\\,y^{2}+e,\n$$\nwith parameters $a=0.8$, $b=1.2$, $c=0.05$, $d=0.1$, and $e=0.02$. Use the test value $y_{0}=1.3$. Compute the two residuals $R_{0}$ and $R_{\\text{odd}}(y_{0})$. Express both residuals as dimensionless numbers and round your answers to four significant figures.",
            "solution": "The problem requires the computation of two verification residuals for a given surrogate model of turbulent heat flux, $Q_{\\text{s}}(x,y)$.\n\nFirst, we compute the zero-gradient residual, $R_{0}$, defined as $R_{0}=\\left|Q_{\\text{s}}(0,0)\\right|$. The surrogate model is:\n$$\nQ_{\\text{s}}(x,y)=a\\,x+b\\,y+c\\,x\\,y+d\\,y^{2}+e\n$$\nEvaluating the model at $x=0$ and $y=0$:\n$$\nQ_{\\text{s}}(0,0) = a(0) + b(0) + c(0)(0) + d(0)^{2} + e = e\n$$\nGiven the parameter value $e=0.02$, the residual is:\n$$\nR_{0} = |e| = |0.02| = 0.02\n$$\nTo four significant figures, this is $0.02000$.\n\nSecond, we compute the odd-symmetry residual, $R_{\\text{odd}}(y_{0})$, at the test gradient magnitude $y_{0}=1.3$. The definition is:\n$$\nR_{\\text{odd}}(y_{0})=\\left|\\frac{Q_{\\text{s}}(0,y_{0})+Q_{\\text{s}}(0,-y_{0})}{Q_{\\text{s}}(0,y_{0})-Q_{\\text{s}}(0,-y_{0})}\\right|\n$$\nFirst, we find the surrogate's form at $x=0$:\n$$\nQ_{\\text{s}}(0,y) = b\\,y + d\\,y^{2} + e\n$$\nNext, we evaluate this at $y=y_{0}$ and $y=-y_{0}$:\n$$\nQ_{\\text{s}}(0,y_{0}) = b\\,y_{0} + d\\,y_{0}^{2} + e\n$$\n$$\nQ_{\\text{s}}(0,-y_{0}) = b(-y_{0}) + d(-y_{0})^{2} + e = -b\\,y_{0} + d\\,y_{0}^{2} + e\n$$\nNow, we compute the sum (numerator) and difference (denominator) for the residual expression.\nSum:\n$$\nQ_{\\text{s}}(0,y_{0}) + Q_{\\text{s}}(0,-y_{0}) = (b\\,y_{0} + d\\,y_{0}^{2} + e) + (-b\\,y_{0} + d\\,y_{0}^{2} + e) = 2\\,d\\,y_{0}^{2} + 2\\,e\n$$\nDifference:\n$$\nQ_{\\text{s}}(0,y_{0}) - Q_{\\text{s}}(0,-y_{0}) = (b\\,y_{0} + d\\,y_{0}^{2} + e) - (-b\\,y_{0} + d\\,y_{0}^{2} + e) = 2\\,b\\,y_{0}\n$$\nSubstituting these into the definition of $R_{\\text{odd}}(y_{0})$ gives:\n$$\nR_{\\text{odd}}(y_{0}) = \\left|\\frac{2\\,d\\,y_{0}^{2} + 2\\,e}{2\\,b\\,y_{0}}\\right| = \\left|\\frac{d\\,y_{0}^{2} + e}{b\\,y_{0}}\\right|\n$$\nFinally, we substitute the numerical values: $b=1.2$, $d=0.1$, $e=0.02$, and $y_{0}=1.3$.\n$$\nR_{\\text{odd}}(1.3) = \\left|\\frac{(0.1)(1.3)^{2} + 0.02}{(1.2)(1.3)}\\right| = \\frac{0.1(1.69) + 0.02}{1.56} = \\frac{0.169 + 0.02}{1.56} = \\frac{0.189}{1.56}\n$$\n$$\nR_{\\text{odd}}(1.3) \\approx 0.1211538...\n$$\nRounding to four significant figures, we get $0.1212$.\n\nThe two residuals are $R_{0}=0.02000$ and $R_{\\text{odd}}(1.3)=0.1212$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.02000  0.1212 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Once a surrogate model has been verified, it can be integrated into larger transport codes to accelerate simulations and perform predictive analysis. A key challenge in this application is handling the inherent uncertainties in both the model's predictions and its inputs. This practice explores how to make predictions with a surrogate in the presence of such uncertainty.\n\nWe will focus on a common feature of turbulent transport known as \"stiffness,\" where the heat flux increases sharply above a critical gradient threshold. By modeling the input temperature gradient as a Gaussian probability distribution, this exercise  will teach you how to calculate the expected change in heat flux due to a shift in plasma conditions. This skill is essential for quantitatively assessing the sensitivity of plasma confinement to changes in external factors like heating power.",
            "id": "4194387",
            "problem": "A fusion plasma transport code uses a Machine Learning (ML)-based surrogate model to close the ion heat flux. The surrogate provides a stiffness-like closure based on the conductive heat flux definition and gyro-Bohm normalization. Begin from the conductive ion heat flux definition $Q_i = - n_i \\chi_i \\nabla T_i$, where $n_i$ is the ion density, $\\chi_i$ is the ion thermal diffusivity, and $T_i$ is the ion temperature. Under a large-aspect-ratio tokamak approximation with radial coordinate $r$ and major radius $R$, the normalized temperature gradient is defined by $R/L_{T_i}$ with $L_{T_i}^{-1} = - \\frac{1}{T_i}\\frac{d T_i}{d r}$. The normalized ion heat flux is $q \\equiv \\frac{Q_i}{n_i T_i c_s}$, where $c_s$ is the sound speed defined as $c_s = \\sqrt{\\frac{T_e}{m_i}}$ for electron temperature $T_e$ and ion mass $m_i$. The surrogate model returns a predicted normalized diffusivity parameter $\\chi_0$ and enforces a thresholded stiffness closure\n$$\nq = \\chi_0 \\left( \\frac{R}{L_{T_i}} - \\left(\\frac{R}{L_{T_i}}\\right)_{\\text{crit}} \\right)_+,\n$$\nwhere $(x)_+ \\equiv \\max(x,0)$ and $\\left(\\frac{R}{L_{T_i}}\\right)_{\\text{crit}}$ is the critical gradient for turbulence onset.\n\nBecause the ML-based surrogate is probabilistic, its predictive distribution for the baseline normalized temperature gradient $X \\equiv \\frac{R}{L_{T_i}}$ in the operating scenario is modeled as Gaussian, $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, independent of $\\chi_0$. A proposed heating change increases $X$ by a fixed amount $\\Delta$, i.e., $X \\mapsto X + \\Delta$. Using the stiffness closure above and the probabilistic prediction for $X$, derive from first principles an analytic expression for the expected change in the normalized heat flux,\n$$\n\\mathbb{E}[\\Delta q] = \\mathbb{E}\\left[ \\chi_0 \\left( (X+\\Delta - c)_+ - (X - c)_+ \\right) \\right],\n$$\nwhere $c \\equiv \\left(\\frac{R}{L_{T_i}}\\right)_{\\text{crit}}$, and then evaluate it numerically for the scenario:\n- $\\mu = 3.4$,\n- $\\sigma = 0.1$,\n- $c = 3.5$,\n- $\\Delta = 0.2$,\n- $\\chi_0 = 0.5$.\n\nExpress the final answer for $\\mathbb{E}[\\Delta q]$ as a dimensionless number and round your numerical result to four significant figures. No intermediate numerical rounding should be performed before the final step.",
            "solution": "The objective is to derive an analytical expression for the expected change in the normalized ion heat flux, $\\mathbb{E}[\\Delta q]$, and evaluate it numerically. The expected change is defined as:\n$$\n\\mathbb{E}[\\Delta q] = \\mathbb{E}\\left[ \\chi_0 \\left( (X+\\Delta - c)_+ - (X - c)_+ \\right) \\right]\n$$\nwhere $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is a Gaussian random variable, $(z)_+ \\equiv \\max(z,0)$, and all other parameters are constants.\n\nSince $\\chi_0$ is a constant, we can factor it out of the expectation. Using the linearity of expectation, we get:\n$$\n\\mathbb{E}[\\Delta q] = \\chi_0 \\left( \\mathbb{E}\\left[ (X+\\Delta - c)_+ \\right] - \\mathbb{E}\\left[ (X - c)_+ \\right] \\right)\n$$\nThe core task is to evaluate the expectation of the form $\\mathbb{E}[(Z-a)_+]$ where $Z \\sim \\mathcal{N}(\\mu_Z, \\sigma_Z^2)$. The general formula for this expectation is:\n$$\n\\mathbb{E}[(Z-a)_+] = \\int_{a}^{\\infty} (z-a) f_Z(z) dz = (\\mu_Z - a) \\Phi\\left(\\frac{\\mu_Z-a}{\\sigma_Z}\\right) + \\sigma_Z \\phi\\left(\\frac{\\mu_Z-a}{\\sigma_Z}\\right)\n$$\nwhere $\\phi(\\cdot)$ is the standard normal PDF and $\\Phi(\\cdot)$ is the standard normal CDF.\n\nWe apply this formula to both terms in the expression for $\\mathbb{E}[\\Delta q]$.\nFor the first term, we set $Z=X$, $\\mu_Z=\\mu$, $\\sigma_Z=\\sigma$, and $a = c-\\Delta$:\n$$\n\\mathbb{E}\\left[ (X+\\Delta - c)_+ \\right] = (\\mu - c + \\Delta) \\Phi\\left(\\frac{\\mu-c+\\Delta}{\\sigma}\\right) + \\sigma \\phi\\left(\\frac{\\mu-c+\\Delta}{\\sigma}\\right)\n$$\nFor the second term, we set $Z=X$, $\\mu_Z=\\mu$, $\\sigma_Z=\\sigma$, and $a = c$:\n$$\n\\mathbb{E}\\left[ (X - c)_+ \\right] = (\\mu - c) \\Phi\\left(\\frac{\\mu-c}{\\sigma}\\right) + \\sigma \\phi\\left(\\frac{\\mu-c}{\\sigma}\\right)\n$$\nNow, we substitute the given numerical values: $\\mu=3.4$, $\\sigma=0.1$, $c=3.5$, $\\Delta=0.2$, and $\\chi_0=0.5$.\n\nFirst, compute the arguments for the special functions:\n$$\n\\frac{\\mu-c+\\Delta}{\\sigma} = \\frac{3.4 - 3.5 + 0.2}{0.1} = \\frac{0.1}{0.1} = 1\n$$\n$$\n\\frac{\\mu-c}{\\sigma} = \\frac{3.4 - 3.5}{0.1} = \\frac{-0.1}{0.1} = -1\n$$\nSubstitute these into the combined expression for $\\mathbb{E}[\\Delta q]$:\n$$\n\\mathbb{E}[\\Delta q] = \\chi_0 \\left[ \\left( (\\mu-c+\\Delta) \\Phi(1) + \\sigma \\phi(1) \\right) - \\left( (\\mu-c) \\Phi(-1) + \\sigma \\phi(-1) \\right) \\right]\n$$\nUsing the identities $\\phi(-1) = \\phi(1)$ and $\\Phi(-1) = 1 - \\Phi(1)$, the terms involving $\\sigma$ cancel out: $\\sigma \\phi(1) - \\sigma \\phi(-1) = 0$. The expression simplifies to:\n$$\n\\mathbb{E}[\\Delta q] = \\chi_0 \\left[ (\\mu-c+\\Delta) \\Phi(1) - (\\mu-c) \\Phi(-1) \\right]\n$$\nSubstituting the values $\\mu-c+\\Delta=0.1$ and $\\mu-c=-0.1$:\n$$\n\\mathbb{E}[\\Delta q] = \\chi_0 \\left[ (0.1) \\Phi(1) - (-0.1) \\Phi(-1) \\right] = 0.1 \\chi_0 \\left( \\Phi(1) + \\Phi(-1) \\right)\n$$\nUsing $\\Phi(1) + \\Phi(-1) = \\Phi(1) + (1-\\Phi(1)) = 1$:\n$$\n\\mathbb{E}[\\Delta q] = 0.1 \\chi_0 (1) = 0.1 \\chi_0\n$$\nFinally, substituting $\\chi_0=0.5$:\n$$\n\\mathbb{E}[\\Delta q] = 0.1 \\times 0.5 = 0.05\n$$\nRounding to four significant figures gives $0.05000$. This can be written in scientific notation as $5.000 \\times 10^{-2}$.",
            "answer": "$$\\boxed{5.000 \\times 10^{-2}}$$"
        },
        {
            "introduction": "Rather than merely verifying a model's physical consistency after training, advanced methods in scientific machine learning allow us to enforce these constraints during the training process itself. By encoding physical laws directly into the learning objective, we can produce surrogates that are inherently more robust, accurate, and reliable. This final practice delves into the construction of such a physics-informed training objective.\n\nThis exercise  challenges you to derive a custom, differentiable loss function that enforces monotonicity—the principle that flux should not decrease as its driving gradient increases above a critical threshold. You will learn to use sophisticated tools like kernel weighting and smooth approximations of non-differentiable functions to translate a physical constraint into a mathematical penalty. This powerful technique lies at the heart of modern physics-informed machine learning and is key to developing the next generation of scientific surrogate models.",
            "id": "4194347",
            "problem": "In ion-scale gyrokinetic turbulence driven by the ion-temperature-gradient mechanism, the ion heat flux normalized to gyro-Bohm units, denoted by $Q$, is empirically observed to be thresholded and stiffness-dominated: at fixed auxiliary control variables $z$ (e.g., safety factor $q$, magnetic shear $\\hat{s}$, collisionality $\\nu^{\\ast}$, ExB shearing rate $\\gamma_{E}$), and as a function of the normalized temperature gradient $g \\equiv R/L_{T_{i}}$, there exists a critical gradient $g_{\\mathrm{crit}}(z)$ such that, for $g \\geq g_{\\mathrm{crit}}(z)$, the mapping $g \\mapsto Q(z,g)$ is non-decreasing. Consider a machine learning surrogate model $f_{\\theta}(z,g)$ trained by empirical risk minimization to predict $Q$ from training data $\\{(z_{i}, g_{i}, y_{i})\\}_{i=1}^{N}$, where $y_{i}$ are noisy observations of $Q(z_{i}, g_{i})$.\n\nAssume a known, smooth approximation of the critical gradient function $T(z)$, which estimates $g_{\\mathrm{crit}}(z)$, and let the objective be to impose the physical monotonicity constraint above threshold: for any two inputs $(z, g_{a})$ and $(z, g_{b})$ with identical $z$ and $g_{a}  g_{b} \\geq T(z)$, the model must satisfy $f_{\\theta}(z, g_{a}) \\geq f_{\\theta}(z, g_{b})$. Because exact duplicates of $z$ are rare in high-dimensional training data, construct constraints by comparing nearby inputs in $z$-space, weighted by a positive definite kernel. Let $K(r)$ be a positive radial kernel (use the Gaussian $K(r)=\\exp(-r^{2}/(2\\ell^{2}))$ with bandwidth $\\ell  0$), and let $\\sigma(x) \\equiv 1/(1+\\exp(-x))$ be the logistic sigmoid used as a smooth indicator. Define a smooth approximation of the rectified linear unit using the softplus with temperature $\\tau  0$, namely $\\operatorname{softplus}_{\\tau}(x) \\equiv \\tau \\ln(1+\\exp(x/\\tau))$. Optionally introduce a non-negative margin slope $\\lambda \\geq 0$ to bias toward strictly increasing behavior proportional to the gradient difference.\n\nStarting from the definitions of monotonicity and thresholding described above, and using only standard properties of convex surrogate penalties and kernels, derive a differentiable, pairwise, kernel-weighted penalty $L_{\\mathrm{mon}}(\\theta)$ that is:\n- zero when the model obeys the non-decreasing constraint for all pairs with $g_{a}  g_{b} \\geq T(z)$ at fixed $z$,\n- strictly positive when the model violates the constraint,\n- smoothly gates out pairs that are not ordered in $g$ or are below threshold, and\n- smoothly downweights pairs that are dissimilar in $z$.\n\nNormalize the penalty by the total effective weight of active pairs so that its scale is independent of batch composition. Express the final monotonicity loss $L_{\\mathrm{mon}}(\\theta)$ explicitly as a single closed-form analytic expression in terms of $\\{(z_{i}, g_{i})\\}_{i=1}^{N}$, $f_{\\theta}$, $K$, $\\sigma$, $\\operatorname{softplus}_{\\tau}$, $T$, $\\ell$, $\\tau$, and $\\lambda$. Your final answer must be this single analytic expression. No numerical evaluation is required, and no units are needed.",
            "solution": "The objective is to derive a differentiable, pairwise, kernel-weighted penalty function, $L_{\\mathrm{mon}}(\\theta)$, that enforces a physical monotonicity constraint on a surrogate model $f_{\\theta}(z,g)$. The derivation proceeds by constructing the penalty from its constituent parts.\n\n1.  **Core Monotonicity Constraint**: The physical constraint states that for a fixed $z$ and for gradients $g_a  g_b \\geq T(z)$, the flux $Q$ should be non-decreasing, i.e., $Q(z, g_a) \\geq Q(z, g_b)$. We want the surrogate $f_{\\theta}$ to obey this. The problem introduces an optional margin slope $\\lambda \\geq 0$ to encourage strictly increasing behavior, leading to the condition $f_{\\theta}(z, g_a) - f_{\\theta}(z, g_b) \\geq \\lambda(g_a - g_b)$. A violation occurs if this inequality is not met.\n\n2.  **Pairwise Violation Penalty**: For any pair of training data points $(i, j)$, let's test if the constraint is violated with point $i$ as $(z_i, g_i)$ and point $j$ as $(z_j, g_j)$. A violation occurs when $f_{\\theta}(z_i, g_i)  f_{\\theta}(z_j, g_j) + \\lambda(g_i - g_j)$, assuming the preconditions on gradients are met. The magnitude of this violation can be written as $V_{ij} = f_{\\theta}(z_j, g_j) - f_{\\theta}(z_i, g_i) + \\lambda(g_i - g_j)$. To create a differentiable, non-negative penalty, we use the specified softplus function:\n    $$P_{ij} = \\operatorname{softplus}_{\\tau}(V_{ij}) = \\operatorname{softplus}_{\\tau}\\left(f_{\\theta}(z_j, g_j) - f_{\\theta}(z_i, g_i) + \\lambda(g_i-g_j)\\right)$$\n    This penalty is positive when there is a violation ($V_{ij}  0$) and approaches zero when the constraint is satisfied ($V_{ij} \\le 0$).\n\n3.  **Smooth Gating and Weighting Factors**: The penalty $P_{ij}$ should only be active for pairs that meet the physical criteria. We build a composite weight $W_{ij}$ to smoothly enable the penalty.\n    -   **Gradient Ordering**: The constraint applies only for $g_i  g_j$. We use the logistic sigmoid function $\\sigma(\\cdot)$ to create a smooth gate: $\\sigma(g_i - g_j)$. This term is close to 1 for $g_i  g_j$ and close to 0 for $g_i  g_j$.\n    -   **Critical Gradient Threshold**: The constraint is active only for $g_j \\geq T(z_j)$. We create another smooth gate for this condition: $\\sigma(g_j - T(z_j))$.\n    -   **Proximity in Auxiliary Variables**: Since the physical constraint is for fixed $z$, we downweight pairs where $z_i$ and $z_j$ are far apart. The problem specifies using the kernel $K(r)$ for this, where $r$ is the distance between $z_i$ and $z_j$: $K(\\|z_i - z_j\\|)$.\n\n    Combining these factors gives the total weight for the penalty term associated with the ordered pair $(i, j)$:\n    $$W_{ij} = K(\\|z_i - z_j\\|) \\sigma(g_i - g_j) \\sigma(g_j - T(z_j))$$\n\n4.  **Full Loss Expression**: The final loss function is constructed as the sum of all weighted pairwise penalties, normalized by the sum of all weights to make it independent of batch size and composition. This gives a weighted average of the penalty over all relevant pairs.\n    -   **Numerator (Total Weighted Penalty)**: $\\sum_{i=1}^{N} \\sum_{j=1}^{N} W_{ij} P_{ij}$\n    -   **Denominator (Total Weight)**: $\\sum_{i=1}^{N} \\sum_{j=1}^{N} W_{ij}$\n\n    Substituting the full expressions for $W_{ij}$ and $P_{ij}$ results in the final, single analytic expression for the loss function:\n    $$L_{\\mathrm{mon}}(\\theta) = \\frac{\\sum_{i=1}^{N} \\sum_{j=1}^{N} K(\\|z_i - z_j\\|) \\sigma(g_i - g_j) \\sigma(g_j - T(z_j)) \\operatorname{softplus}_{\\tau}\\left(f_{\\theta}(z_j, g_j) - f_{\\theta}(z_i, g_i) + \\lambda(g_i-g_j)\\right)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} K(\\|z_i - z_j\\|) \\sigma(g_i - g_j) \\sigma(g_j - T(z_j))}$$\n    This expression is differentiable with respect to $\\theta$ and fulfills all the requirements set out in the problem statement.",
            "answer": "$$\n\\boxed{\n\\frac{\\sum_{i=1}^{N} \\sum_{j=1}^{N} K(\\|z_i - z_j\\|) \\sigma(g_i - g_j) \\sigma(g_j - T(z_j)) \\operatorname{softplus}_{\\tau}\\left(f_{\\theta}(z_j, g_j) - f_{\\theta}(z_i, g_i) + \\lambda(g_i-g_j)\\right)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} K(\\|z_i - z_j\\|) \\sigma(g_i - g_j) \\sigma(g_j - T(z_j))}\n}\n$$"
        }
    ]
}