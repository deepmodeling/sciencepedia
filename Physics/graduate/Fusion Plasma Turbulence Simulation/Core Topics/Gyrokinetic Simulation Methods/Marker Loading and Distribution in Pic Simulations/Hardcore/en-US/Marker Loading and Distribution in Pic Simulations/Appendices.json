{
    "hands_on_practices": [
        {
            "introduction": "A fundamental challenge in Particle-In-Cell (PIC) simulations is the representation of a continuous plasma using a finite number of discrete markers, which inherently introduces statistical noise. This practice explores the direct relationship between the number of markers and the \"signal-to-noise\" ratio for resolving a physical fluctuation. By applying the Central Limit Theorem, you will determine the minimum number of markers needed to resolve a signal with a desired statistical confidence, providing a quantitative basis for a crucial simulation design choice .",
            "id": "4195830",
            "problem": "Consider a one-dimensional gyrokinetic (GK) slab test in the Particle-In-Cell (PIC) method for fusion plasma turbulence simulation. The domain is a periodic interval of length $L$ and we aim to resolve a single electrostatic density fluctuation mode. Markers are loaded uniformly in configuration space with equal weights, and the density fluctuation is characterized by a small relative amplitude $A$, such that the fluctuating component of the density scales as $A n_0$, where $n_0$ is the background density scale. The goal is to determine the minimal marker count $N$ required so that the estimated Fourier cosine amplitude of the density fluctuation is resolved to within a prescribed relative tolerance with a specified confidence level, and to verify this requirement against an analytic variance estimate.\n\nUse the following fundamental base:\n- Independence of marker samples and uniform loading imply that marker positions $x_i$ are independent and identically distributed as a uniform random variable on $[0,L]$.\n- The estimator for the cosine Fourier amplitude at wave number $k$ is defined by Monte Carlo integration over marker positions using equal weights. For mode number $m \\in \\mathbb{Z}^+$, the wave number is $k = 2\\pi m / L$ (angles in radians).\n- The Central Limit Theorem implies that the estimator is approximately normally distributed with variance that decreases as $1/N$ for independent samples.\n- For uniform sampling on $[0,L]$, $\\mathrm{Var}[\\cos(k x)] = 1/2$ for integer $m \\ge 1$ and $m \\ne M/2$ (non-Nyquist frequency).\n\nFrom these principles, derive the minimal integer marker count $N$ such that the two-sided normal confidence interval at confidence level $c$ has half-width less than or equal to $\\epsilon A n_0$, where $\\epsilon$ is the prescribed relative tolerance (dimensionless) and $c$ is given as a decimal in $(0,1)$.\n\nAdditionally, consider a uniform grid deposition with $M$ cells of equal size $\\Delta x = L/M$. Let $c_j$ denote the marker count in cell $j$ (with $j=0,1,\\dots,M-1$), and define the discrete cosine Fourier amplitude estimator by grid deposition using the marker weights. Using the properties of Poisson statistics for counts under uniform sampling, verify that the analytically derived variance for the Fourier cosine amplitude matches the grid-based variance expression when $m$ is not the Nyquist mode.\n\nYour program must implement the following:\n- For each test case, compute the minimal integer marker count $N_{\\min}$ that satisfies the confidence requirement.\n- Verify tightness by checking that $N_{\\min} - 1$ fails the requirement.\n- Verify the analytic variance against a grid-based variance computed from the discrete sum of $\\cos^2(k x_j)$ over cell centers $x_j = j \\Delta x$, and assert that they match within a small numerical tolerance.\n\nAll quantities $A$, $\\epsilon$, and $c$ are dimensionless. Angles must be treated in radians. The marker count $N$ is a unitless integer. Use $L = 1.0$ and $n_0 = 1.0$ for all tests to avoid extraneous units.\n\nTest suite:\n- Case 1 (happy path): $A = 10^{-2}$, $\\epsilon = 10^{-1}$, $c = 0.95$, $m=3$, $M=64$.\n- Case 2 (high confidence and stricter tolerance): $A = 5 \\times 10^{-3}$, $\\epsilon = 5 \\times 10^{-2}$, $c = 0.99$, $m=5$, $M=128$.\n- Case 3 (very small amplitude): $A = 10^{-4}$, $\\epsilon = 2 \\times 10^{-1}$, $c = 0.90$, $m=7$, $M=256$.\n- Case 4 (near-certainty confidence): $A = 2 \\times 10^{-2}$, $\\epsilon = 10^{-1}$, $c = 0.999$, $m=9$, $M=512$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case should result in a list of the form $[N_{\\min}, \\text{tight}, \\text{var\\_match}]$, where $N_{\\min}$ is an integer, $\\text{tight}$ is a boolean indicating whether $N_{\\min}-1$ fails the requirement, and $\\text{var\\_match}$ is a boolean indicating whether the analytic and grid-based variances match within numerical tolerance. For example, the output must look like $[[N_1,\\text{True},\\text{True}],[N_2,\\text{True},\\text{True}],\\dots]$.",
            "solution": "The problem is scientifically and mathematically well-posed, providing a clear objective and all necessary information. It is grounded in the established principles of Particle-In-Cell (PIC) simulations, Monte Carlo methods, and statistical analysis. The problem is therefore deemed valid and a solution will be provided.\n\nThe primary goal is to determine the minimal number of simulation markers, $N$, required to resolve a density fluctuation of a given amplitude with a specified statistical confidence. This is a classic signal-to-noise problem in PIC simulations, where the \"noise\" originates from the discrete particle representation of a continuous plasma density.\n\nLet the one-dimensional, periodic domain have length $L$. We are considering a single cosine density fluctuation mode with wave number $k = 2\\pi m / L$ for a positive integer mode number $m$. The physical density fluctuation is given to have a cosine amplitude of $A n_0$, where $n_0$ is the background density and $A$ is the small, dimensionless relative amplitude. The full density is $n(x) = n_0 + A n_0 \\cos(kx)$.\n\nIn a PIC simulation, the plasma is represented by a large number of markers. The problem states that markers are loaded uniformly in configuration space with equal weights. This corresponds to representing the uniform background density, $n_0$, using $N$ markers. The position of each marker, $x_i$ for $i=1, \\dots, N$, is an independent random variable drawn from a uniform distribution on the interval $[0, L]$.\n\nThe discrete nature of the markers introduces sampling noise. Even though the markers represent a uniform background, a Fourier analysis of their distribution will yield non-zero amplitudes for all modes. This is the particle noise. We must ensure this noise is sufficiently small compared to the physical signal we wish to resolve.\n\nThe Fourier cosine amplitude of the marker-based density representation is given by the Monte Carlo estimator:\n$$ \\hat{C}_m = \\frac{2}{L} \\int_0^L \\left(\\sum_{i=1}^N w_i \\delta(x - x_i)\\right) \\cos(kx) dx $$\nWith equal weights for a background density $n_0$, each marker weight is $w_i = n_0 L / N$. Substituting this into the estimator gives:\n$$ \\hat{C}_m = \\frac{2}{L} \\sum_{i=1}^N \\frac{n_0 L}{N} \\cos(kx_i) = \\frac{2 n_0}{N} \\sum_{i=1}^N \\cos(kx_i) $$\nThis estimator, $\\hat{C}_m$, is a random variable because the marker positions $x_i$ are random. We first compute its expected value. Since $x_i \\sim U[0, L]$ and $m \\ge 1$:\n$$ E[\\cos(kx_i)] = \\frac{1}{L} \\int_0^L \\cos\\left(\\frac{2\\pi m x}{L}\\right) dx = 0 $$\nBy linearity of expectation, the expected value of the estimator is:\n$$ E[\\hat{C}_m] = \\frac{2 n_0}{N} \\sum_{i=1}^N E[\\cos(kx_i)] = 0 $$\nThis confirms that, on average, the noise does not produce a systematic Fourier component.\n\nNext, we compute the variance of the estimator. Since the marker positions $x_i$ are independent, the variance of the sum is the sum of the variances:\n$$ \\mathrm{Var}[\\hat{C}_m] = \\mathrm{Var}\\left[\\frac{2 n_0}{N} \\sum_{i=1}^N \\cos(kx_i)\\right] = \\left(\\frac{2 n_0}{N}\\right)^2 \\sum_{i=1}^N \\mathrm{Var}[\\cos(kx_i)] = \\frac{4 n_0^2}{N^2} \\cdot N \\cdot \\mathrm{Var}[\\cos(kx)] = \\frac{4 n_0^2}{N} \\mathrm{Var}[\\cos(kx)] $$\nThe problem provides the fundamental result that for $x \\sim U[0, L]$ and $k = 2\\pi m / L$ with $m \\in \\mathbb{Z}^+$, $\\mathrm{Var}[\\cos(kx)] = 1/2$. This is derived from $E[\\cos(kx)]=0$ and $E[\\cos^2(kx)] = \\frac{1}{L} \\int_0^L \\cos^2(kx) dx = \\frac{1}{L} \\int_0^L \\frac{1 + \\cos(2kx)}{2} dx = 1/2$.\nSubstituting this into the variance expression for $\\hat{C}_m$:\n$$ \\mathrm{Var}[\\hat{C}_m] = \\frac{4 n_0^2}{N} \\cdot \\frac{1}{2} = \\frac{2 n_0^2}{N} $$\nThe standard deviation of the noise estimator is $\\sigma_{\\hat{C}_m} = \\sqrt{\\mathrm{Var}[\\hat{C}_m]} = n_0 \\sqrt{2/N}$.\n\nAccording to the Central Limit Theorem, for large $N$, the estimator $\\hat{C}_m$ is approximately normally distributed: $\\hat{C}_m \\sim \\mathcal{N}(0, \\sigma_{\\hat{C}_m}^2)$.\nThe problem requires that the two-sided normal confidence interval for the noise amplitude, at a confidence level $c$, has a half-width less than or equal to a fraction $\\epsilon$ of the signal amplitude $A n_0$.\nThe half-width $HW$ of the confidence interval is given by $HW = z \\sigma_{\\hat{C}_m}$, where $z$ is the critical value from the standard normal distribution corresponding to the confidence level $c$. This value is the quantile $z = \\Phi^{-1}((1+c)/2)$, where $\\Phi$ is the cumulative distribution function of the standard normal distribution.\n\nThe resolution requirement is thus formulated as:\n$$ z \\sigma_{\\hat{C}_m} \\le \\epsilon A n_0 $$\nSubstituting the expression for $\\sigma_{\\hat{C}_m}$:\n$$ z \\left(n_0 \\sqrt{\\frac{2}{N}}\\right) \\le \\epsilon A n_0 $$\nThe background density scale $n_0$ cancels, as expected for a condition on relative amplitudes. For all calculations, we use $n_0=1.0$ as specified.\n$$ z \\sqrt{\\frac{2}{N}} \\le \\epsilon A $$\nWe solve this inequality for $N$:\n$$ \\sqrt{\\frac{2}{N}} \\le \\frac{\\epsilon A}{z} \\implies \\frac{2}{N} \\le \\left(\\frac{\\epsilon A}{z}\\right)^2 \\implies N \\ge 2 \\left(\\frac{z}{\\epsilon A}\\right)^2 $$\nThe minimal marker count $N_{\\min}$ must be an integer, so we take the ceiling of the right-hand side:\n$$ N_{\\min} = \\left\\lceil 2 \\left(\\frac{z}{\\epsilon A}\\right)^2 \\right\\rceil $$\nThis formula allows for the calculation of $N_{\\min}$ for each test case. The tightness check, verifying that $N_{\\min}-1$ fails the condition, is satisfied by the definition of the ceiling function. For $N' = N_{\\min}-1$, we have $N' < 2(z/(\\epsilon A))^2$, which violates the required inequality.\n\nFinally, we must verify the analytic variance of the cosine term, $\\mathrm{Var}[\\cos(kx)] = 1/2$, against a grid-based calculation. A uniform grid of $M$ cells has cell centers at $x_j = j \\Delta x = j (L/M)$ for $j=0, 1, \\dots, M-1$. The discrete variance is computed by treating $\\cos(kx_j)$ as a discrete random variable over the grid points.\nThe discrete expectation of a function $g(x_j)$ is $E_{\\text{grid}}[g] = \\frac{1}{M}\\sum_{j=0}^{M-1} g(x_j)$.\nThe wave number is $k = 2\\pi m / L$. At the grid points, $kx_j = 2\\pi m j / M$.\nThe discrete mean is $E_{\\text{grid}}[\\cos(kx)] = \\frac{1}{M}\\sum_{j=0}^{M-1} \\cos(2\\pi mj/M)$. This sum is zero for any integer $m$ that is not a multiple of $M$.\nThe discrete second moment is $E_{\\text{grid}}[\\cos^2(kx)] = \\frac{1}{M}\\sum_{j=0}^{M-1} \\cos^2(2\\pi mj/M) = \\frac{1}{M}\\sum_{j=0}^{M-1} \\frac{1+\\cos(4\\pi mj/M)}{2}$.\nThe sum $\\sum_{j=0}^{M-1} \\cos(4\\pi mj/M)$ is zero for non-Nyquist modes, i.e., when $2m$ is not a multiple of $M$. The problem guarantees this by stating $m \\neq M/2$.\nThus, $E_{\\text{grid}}[\\cos^2(kx)] = \\frac{1}{2M}(M+0) = 1/2$.\nThe grid-based variance is $\\mathrm{Var}_{\\text{grid}}[\\cos(kx)] = E_{\\text{grid}}[\\cos^2(kx)] - (E_{\\text{grid}}[\\cos(kx)])^2 = 1/2 - 0^2 = 1/2$.\nThis analytically confirms that the grid-based variance is identical to the continuous-space variance, $1/2$, under the specified conditions. The program will verify this numerically.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nimport math\n\ndef solve():\n    \"\"\"\n    Solves for the minimal marker count in a PIC simulation test case\n    and performs verification checks.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: A, epsilon, c, m, M\n        (1e-2, 1e-1, 0.95, 3, 64),\n        # Case 2\n        (5e-3, 5e-2, 0.99, 5, 128),\n        # Case 3\n        (1e-4, 2e-1, 0.90, 7, 256),\n        # Case 4\n        (2e-2, 1e-1, 0.999, 9, 512),\n    ]\n\n    results = []\n\n    # Constants for all tests\n    L = 1.0\n    n0 = 1.0\n\n    for A, epsilon, c, m, M in test_cases:\n        # 1. Compute minimal marker count N_min\n\n        # The critical value z for a two-sided confidence interval\n        z = norm.ppf((1 + c) / 2)\n\n        # The condition is z * n0 * sqrt(2/N) <= epsilon * A * n0.\n        # This simplifies to N >= 2 * (z / (epsilon * A))^2.\n        N_star = 2 * (z / (epsilon * A))**2\n        N_min = math.ceil(N_star)\n\n        # 2. Verify tightness: check that N_min - 1 fails the requirement.\n        \n        # Half-width of the confidence interval\n        def get_half_width(N_val):\n            if N_val <= 0:\n                return float('inf')\n            # The standard deviation of the noise estimator is n0 * sqrt(2/N)\n            std_dev_est = n0 * np.sqrt(2 / N_val)\n            return z * std_dev_est\n        \n        tolerance_BW = epsilon * A * n0\n        \n        # Check if N_min passes and N_min-1 fails.\n        # Due to floating point precision, use a small tolerance for comparison.\n        tightness_check_pass = get_half_width(N_min) <= tolerance_BW\n        tightness_check_fail = get_half_width(N_min - 1) > tolerance_BW\n        tight = tightness_check_pass and tightness_check_fail\n\n        # 3. Verify analytic variance against grid-based variance.\n        # The analytic variance of cos(kx) for x in U[0,L] is 1/2.\n        analytic_var = 0.5\n\n        # Grid-based variance calculation\n        # The condition m != M/2 ensures the discrete sums behave as expected.\n        if 2 * m == M:\n            # This case is excluded by the problem statement, but as a safeguard:\n            var_match = False\n        else:\n            k = 2 * np.pi * m / L\n            # Grid cell centers\n            xj = np.arange(M) * L / M\n            cos_kxj = np.cos(k * xj)\n            \n            # Discrete expectation values\n            E_grid_cos = np.mean(cos_kxj)\n            E_grid_cos_sq = np.mean(cos_kxj**2)\n            \n            grid_based_var = E_grid_cos_sq - E_grid_cos**2\n            \n            # Assert they match within numerical tolerance\n            var_match = np.isclose(grid_based_var, analytic_var)\n\n        # Format result for the current test case\n        case_result = [N_min, tight, var_match]\n        results.append(case_result)\n\n    # Final print statement in the exact required format\n    # Convert each inner list to its string representation\n    # e.g., [123, True, True] -> \"[123, True, True]\"\n    # Then join these strings with commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\nsolve()\n\n```"
        },
        {
            "introduction": "Building on the basic concept of particle noise, this exercise considers more realistic scenarios found in modern PIC codes, which use particle shape functions and variable marker weights. You will perform a first-principles derivation of the statistical error in a cell-based density estimate, revealing how this error depends on the number of markers, the shape function properties, and the weight variance. This analysis  is essential for understanding and designing advanced PIC algorithms where non-uniform weights are a key feature.",
            "id": "4195903",
            "problem": "In a one-dimensional Particle-In-Cell (PIC) simulation for fusion plasma turbulence, consider estimating the cell-centered particle density using a compact support shape function. The domain is periodic with length $L$ and is discretized into $M$ uniform cells of width $\\Delta = L/M$, with cell centers at positions $\\{x_i\\}$. A total of $N$ computational markers (also called macro-particles) are used. Marker positions $\\{x_j\\}_{j=1}^{N}$ are independent and identically distributed, uniform on $[0,L)$, and marker weights $\\{w_j\\}_{j=1}^{N}$ are independent and identically distributed with mean $\\bar{w}$ and variance $\\sigma_{w}^{2}$, independent of positions.\n\nThe density at cell center $x_i$ is estimated by linear deposition of marker weights using a cardinal $p$th-order $B$-spline shape function $M_p(u)$ with unit integral, applied at the grid scale. Specifically, define the grid-scale shape as\n$$\nS(x) = \\frac{1}{\\Delta} M_p\\!\\left(\\frac{x}{\\Delta}\\right),\n$$\nand the estimator of the cell-centered density as\n$$\n\\hat{n}_i = \\sum_{j=1}^{N} w_j\\, S(x_i - x_j).\n$$\nAssume the true density is spatially uniform and that the estimator is unbiased. You are interested in the root-mean-square relative error of the estimator at a given cell,\n$$\n\\eta \\equiv \\frac{\\sqrt{\\mathrm{Var}[\\hat{n}_i]}}{\\mathbb{E}[\\hat{n}_i]}.\n$$\n\nUsing only fundamental facts about independent sampling, variance additivity for independent summands, and the normalization of the shape function, and without invoking any pre-derived PIC noise formula, derive the minimal mean number of markers per cell $N_c \\equiv N/M$ required to ensure that the cell-wise root-mean-square relative error satisfies $\\eta \\le \\epsilon$ for a given target relative error $\\epsilon$. Express your final result as a closed-form analytic expression in terms of $\\epsilon$, the shape-function order $p$ via the dimensionless constant\n$$\nI_p \\equiv \\int_{-\\infty}^{\\infty} M_p(u)^2\\, du,\n$$\nand the weight statistics $\\bar{w}$ and $\\sigma_w^{2}$. Assume a sufficiently large periodic domain so that edge effects are negligible and $M$ is large enough for any finite-size corrections that vanish as $M \\to \\infty$ to be neglected.\n\nProvide your answer as the minimal $N_c$ in closed form. No numerical evaluation is required. Your final answer must be a single closed-form analytic expression with no units.",
            "solution": "The problem requires the determination of the minimal mean number of markers per cell, denoted by $N_c$, required to ensure that the cell-wise root-mean-square (RMS) relative error, $\\eta$, of the density estimator $\\hat{n}_i$ is no greater than a specified tolerance $\\epsilon$. The RMS relative error is defined as $\\eta \\equiv \\frac{\\sqrt{\\mathrm{Var}[\\hat{n}_i]}}{\\mathbb{E}[\\hat{n}_i]}$. The condition is therefore $\\eta \\le \\epsilon$, which is equivalent to $\\eta^2 = \\frac{\\mathrm{Var}[\\hat{n}_i]}{(\\mathbb{E}[\\hat{n}_i])^2} \\le \\epsilon^2$.\n\nTo solve this, we must first derive expressions for the expectation $\\mathbb{E}[\\hat{n}_i]$ and the variance $\\mathrm{Var}[\\hat{n}_i]$ of the density estimator.\n\nThe estimator for the density at cell center $x_i$ is given by the sum:\n$$\n\\hat{n}_i = \\sum_{j=1}^{N} w_j S(x_i - x_j)\n$$\nwhere $\\{w_j\\}_{j=1}^{N}$ are the marker weights and $\\{x_j\\}_{j=1}^{N}$ are the marker positions. The pairs $(w_j, x_j)$ are independent and identically distributed (i.i.d.) for all $j \\in \\{1, \\dots, N\\}$. Let's define a single term in the sum as $Z_j = w_j S(x_i - x_j)$. The random variables $\\{Z_j\\}_{j=1}^{N}$ are i.i.d.\n\nFirst, we calculate the expectation of $\\hat{n}_i$. By the linearity of expectation:\n$$\n\\mathbb{E}[\\hat{n}_i] = \\mathbb{E}\\left[\\sum_{j=1}^{N} Z_j\\right] = \\sum_{j=1}^{N} \\mathbb{E}[Z_j] = N \\mathbb{E}[Z_1]\n$$\nThe expectation of a single term $Z_j$ is $\\mathbb{E}[Z_j] = \\mathbb{E}[w_j S(x_i - x_j)]$. Since the marker weight $w_j$ and position $x_j$ are independent, we can separate the expectations:\n$$\n\\mathbb{E}[Z_j] = \\mathbb{E}[w_j] \\mathbb{E}[S(x_i - x_j)]\n$$\nWe are given that $\\mathbb{E}[w_j] = \\bar{w}$. The marker position $x_j$ is uniformly distributed on the interval $[0, L)$, so its probability density function is $f_X(x) = 1/L$ for $x \\in [0, L)$ and $0$ otherwise. The expectation of the shape function term is:\n$$\n\\mathbb{E}[S(x_i - x_j)] = \\int_{0}^{L} S(x_i - x) f_X(x) dx = \\frac{1}{L} \\int_{0}^{L} S(x_i - x) dx\n$$\nLet's perform a change of variables $y = x_i - x$, so $dx = -dy$. The integration limits change from $x=0 \\to y=x_i$ and $x=L \\to y=x_i-L$.\n$$\n\\mathbb{E}[S(x_i - x_j)] = \\frac{1}{L} \\int_{x_i}^{x_i-L} S(y) (-dy) = \\frac{1}{L} \\int_{x_i-L}^{x_i} S(y) dy\n$$\nSince the domain is periodic and large, this integral over one period is approximately equal to the integral over the entire real line. The problem statement allows us to neglect finite-size corrections.\n$$\n\\int_{x_i-L}^{x_i} S(y) dy \\approx \\int_{-\\infty}^{\\infty} S(y) dy = \\int_{-\\infty}^{\\infty} \\frac{1}{\\Delta} M_p\\left(\\frac{y}{\\Delta}\\right) dy\n$$\nUsing the substitution $u = y/\\Delta$, for which $dy = \\Delta du$:\n$$\n\\int_{-\\infty}^{\\infty} \\frac{1}{\\Delta} M_p(u) (\\Delta du) = \\int_{-\\infty}^{\\infty} M_p(u) du = 1\n$$\nThis is given by the normalization of the $B$-spline $M_p(u)$.\nThus, $\\mathbb{E}[S(x_i - x_j)] = 1/L$.\nCombining these results, the expectation of a single term is $\\mathbb{E}[Z_j] = \\bar{w} \\frac{1}{L}$.\nThe expectation of the density estimator is:\n$$\n\\mathbb{E}[\\hat{n}_i] = N \\frac{\\bar{w}}{L}\n$$\nThis is the mean physical density, as expected for an unbiased estimator.\n\nNext, we calculate the variance of $\\hat{n}_i$. Since the terms $Z_j$ are i.i.d., the variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}[\\hat{n}_i] = \\mathrm{Var}\\left[\\sum_{j=1}^{N} Z_j\\right] = \\sum_{j=1}^{N} \\mathrm{Var}[Z_j] = N \\mathrm{Var}[Z_1]\n$$\nThe variance of a single term is given by $\\mathrm{Var}[Z_j] = \\mathbb{E}[Z_j^2] - (\\mathbb{E}[Z_j])^2$. We need to compute $\\mathbb{E}[Z_j^2]$:\n$$\n\\mathbb{E}[Z_j^2] = \\mathbb{E}[w_j^2 S(x_i-x_j)^2] = \\mathbb{E}[w_j^2] \\mathbb{E}[S(x_i-x_j)^2]\n$$\nThe second moment of the weight is related to its variance by $\\mathbb{E}[w_j^2] = \\mathrm{Var}[w_j] + (\\mathbb{E}[w_j])^2 = \\sigma_w^2 + \\bar{w}^2$.\nThe expectation of the squared shape function is:\n$$\n\\mathbb{E}[S(x_i-x_j)^2] = \\int_{0}^{L} S(x_i - x)^2 \\frac{1}{L} dx \\approx \\frac{1}{L} \\int_{-\\infty}^{\\infty} S(y)^2 dy\n$$\nwhere we have again used the large domain assumption. We substitute $S(y)=\\frac{1}{\\Delta}M_p(y/\\Delta)$:\n$$\n\\int_{-\\infty}^{\\infty} S(y)^2 dy = \\int_{-\\infty}^{\\infty} \\left(\\frac{1}{\\Delta} M_p\\left(\\frac{y}{\\Delta}\\right)\\right)^2 dy = \\frac{1}{\\Delta^2} \\int_{-\\infty}^{\\infty} M_p\\left(\\frac{y}{\\Delta}\\right)^2 dy\n$$\nUsing the substitution $u = y/\\Delta$, so $dy = \\Delta du$:\n$$\n\\frac{1}{\\Delta^2} \\int_{-\\infty}^{\\infty} M_p(u)^2 (\\Delta du) = \\frac{1}{\\Delta} \\int_{-\\infty}^{\\infty} M_p(u)^2 du = \\frac{I_p}{\\Delta}\n$$\nwhere $I_p \\equiv \\int_{-\\infty}^{\\infty} M_p(u)^2 du$ is the given dimensionless constant.\nSo, $\\mathbb{E}[S(x_i-x_j)^2] \\approx \\frac{1}{L} \\frac{I_p}{\\Delta}$.\nPutting these pieces together:\n$$\n\\mathbb{E}[Z_j^2] \\approx (\\sigma_w^2 + \\bar{w}^2) \\frac{I_p}{L\\Delta}\n$$\nThe variance of a single term is:\n$$\n\\mathrm{Var}[Z_j] = \\mathbb{E}[Z_j^2] - (\\mathbb{E}[Z_j])^2 \\approx (\\sigma_w^2 + \\bar{w}^2) \\frac{I_p}{L\\Delta} - \\left(\\frac{\\bar{w}}{L}\\right)^2\n$$\nThe total variance of the estimator is:\n$$\n\\mathrm{Var}[\\hat{n}_i] \\approx N \\left[ (\\sigma_w^2 + \\bar{w}^2) \\frac{I_p}{L\\Delta} - \\frac{\\bar{w}^2}{L^2} \\right]\n$$\nNow we can form the squared relative error, $\\eta^2$:\n$$\n\\eta^2 = \\frac{\\mathrm{Var}[\\hat{n}_i]}{(\\mathbb{E}[\\hat{n}_i])^2} \\approx \\frac{N \\left[ (\\sigma_w^2 + \\bar{w}^2) \\frac{I_p}{L\\Delta} - \\frac{\\bar{w}^2}{L^2} \\right]}{(N \\bar{w}/L)^2} = \\frac{1}{N} \\frac{L^2}{\\bar{w}^2} \\left[ (\\sigma_w^2 + \\bar{w}^2) \\frac{I_p}{L\\Delta} - \\frac{\\bar{w}^2}{L^2} \\right]\n$$\n$$\n\\eta^2 \\approx \\frac{1}{N \\bar{w}^2} \\left[ (\\sigma_w^2 + \\bar{w}^2) \\frac{L I_p}{\\Delta} - \\bar{w}^2 \\right]\n$$\nWe are given $\\Delta = L/M$, so $L/\\Delta = M$, and $N_c = N/M$, so $N=N_c M$. Substituting these into the expression:\n$$\n\\eta^2 \\approx \\frac{1}{N_c M \\bar{w}^2} \\left[ (\\sigma_w^2 + \\bar{w}^2) M I_p - \\bar{w}^2 \\right] = \\frac{1}{N_c \\bar{w}^2} \\left[ (\\sigma_w^2 + \\bar{w}^2) I_p - \\frac{\\bar{w}^2}{M} \\right]\n$$\nThe problem states that we can neglect any finite-size corrections that vanish as $M \\to \\infty$. The term $\\bar{w}^2/M$ is such a correction. In the limit of a large number of cells $M$, this term becomes negligible.\n$$\n\\eta^2 \\approx \\frac{I_p}{N_c \\bar{w}^2} (\\sigma_w^2 + \\bar{w}^2) = \\frac{I_p}{N_c} \\left( \\frac{\\sigma_w^2}{\\bar{w}^2} + 1 \\right)\n$$\nWe require $\\eta^2 \\le \\epsilon^2$:\n$$\n\\frac{I_p}{N_c} \\left( 1 + \\frac{\\sigma_w^2}{\\bar{w}^2} \\right) \\le \\epsilon^2\n$$\nSolving for $N_c$, we multiply both sides by $N_c$ and divide by $\\epsilon^2$ (as $N_c > 0$ and $\\epsilon^2 > 0$):\n$$\nI_p \\left( 1 + \\frac{\\sigma_w^2}{\\bar{w}^2} \\right) \\le N_c \\epsilon^2\n$$\n$$\nN_c \\ge \\frac{I_p}{\\epsilon^2} \\left( 1 + \\frac{\\sigma_w^2}{\\bar{w}^2} \\right)\n$$\nThe minimal mean number of markers per cell, $N_c$, is the lower bound of this inequality.\n$$\nN_{c, \\text{min}} = \\frac{I_p}{\\epsilon^2} \\left( 1 + \\frac{\\sigma_w^2}{\\bar{w}^2} \\right)\n$$",
            "answer": "$$\n\\boxed{\\frac{I_p}{\\epsilon^2} \\left( 1 + \\frac{\\sigma_w^2}{\\bar{w}^2} \\right)}\n$$"
        },
        {
            "introduction": "In advanced simulations, \"importance sampling\" is a powerful technique where markers are loaded from a proposal distribution $g$ to represent a more complex target distribution $f$. This practice introduces a critical diagnostic for such schemes: an entropy-like quantity that you will relate to the Kullback-Leibler divergence, $D_{\\mathrm{KL}}(f \\| g)$. This exercise  provides an indispensable tool for validating marker loading algorithms, ensuring the discrete plasma representation is both accurate and statistically robust.",
            "id": "4195834",
            "problem": "In a Particle-In-Cell (PIC) simulation of fusion plasma turbulence, markers are loaded from a proposal distribution $g(\\mathbf{z})$ to represent a target phase-space distribution $f(\\mathbf{z})$, where $\\mathbf{z}$ denotes a one-dimensional velocity coordinate $v$ for simplicity. Each marker $i$ is assigned the importance weight $w_{i} = f(v_{i})/g(v_{i})$. Consider the entropy-like diagnostic\n$$\\widehat{S} = \\frac{1}{N} \\sum_{i=1}^{N} w_{i} \\ln w_{i},$$\nwhich is intended to quantify the mismatch between $g$ and $f$. Starting from the definitions of importance sampling weights and the Kullback–Leibler divergence, derive a rigorous relation connecting the large-$N$ limit of $\\widehat{S}$ to the Kullback–Leibler divergence $D_{\\mathrm{KL}}(f \\| g)$. Then, from first principles of variance-based degeneracy in importance sampling, derive the large-$N$ limit of the effective sample size fraction $\\phi = \\lim_{N \\to \\infty} N_{\\mathrm{eff}}/N$ in terms of $\\mathbb{E}_{g}[w^{2}]$, and explain how this quantity provides a complementary degeneracy diagnostic that bounds the entropy-based diagnostic.\n\nNow specialize to a one-dimensional velocity-space setting where both $f(v)$ and $g(v)$ are Gaussian probability density functions,\n$$f(v) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{f}} \\exp\\!\\left( -\\frac{(v - \\mu_{f})^{2}}{2 \\sigma_{f}^{2}} \\right), \\qquad g(v) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{g}} \\exp\\!\\left( -\\frac{(v - \\mu_{g})^{2}}{2 \\sigma_{g}^{2}} \\right),$$\nrepresenting a drifted Maxwellian target and an undrifted or differently broadened loading distribution, respectively. Using only fundamental definitions and first-principles derivations (not shortcut formulas), compute the value of\n$$S_{\\infty} \\equiv \\lim_{N \\to \\infty} \\widehat{S} = D_{\\mathrm{KL}}(f \\| g)$$\nfor the parameter set\n$$\\mu_{f} = 5.0 \\times 10^{3}, \\quad \\sigma_{f} = 1.8 \\times 10^{3}, \\quad \\mu_{g} = 2.0 \\times 10^{3}, \\quad \\sigma_{g} = 1.5 \\times 10^{3},$$\nwhere all velocities and widths are in $\\mathrm{m}\\,\\mathrm{s}^{-1}$. Express your final diagnostic value as a dimensionless number and round your answer to four significant figures.",
            "solution": "The problem statement is well-posed, scientifically grounded, and contains sufficient information for a unique solution. It requests a series of derivations concerning importance sampling diagnostics in the context of Particle-In-Cell (PIC) simulations, followed by a specific calculation.\n\nFirst, we address the relationship between the entropy-like diagnostic $\\widehat{S}$ and the Kullback–Leibler (KL) divergence. The diagnostic is defined as\n$$ \\widehat{S} = \\frac{1}{N} \\sum_{i=1}^{N} w_{i} \\ln w_{i} $$\nwhere the marker velocities $v_{i}$ are sampled from a proposal distribution $g(v)$, and the weights are $w_{i} = f(v_{i})/g(v_{i})$. The quantity $\\widehat{S}$ is a sample mean. By the strong law of large numbers, as the number of markers $N$ approaches infinity, this sample mean converges almost surely to the expectation of the quantity $w(v) \\ln w(v)$, where the expectation is taken with respect to the distribution from which the samples are drawn, which is $g(v)$.\n$$ S_{\\infty} \\equiv \\lim_{N \\to \\infty} \\widehat{S} = \\mathbb{E}_{g}[w(v) \\ln w(v)] $$\nWe can write this expectation as an integral:\n$$ S_{\\infty} = \\int_{-\\infty}^{\\infty} \\left[ w(v) \\ln w(v) \\right] g(v) \\, dv $$\nSubstituting the definition of the weight $w(v) = f(v)/g(v)$:\n$$ S_{\\infty} = \\int_{-\\infty}^{\\infty} \\left[ \\frac{f(v)}{g(v)} \\ln\\left(\\frac{f(v)}{g(v)}\\right) \\right] g(v) \\, dv $$\nThe factor of $g(v)$ in the integrand cancels with the $g(v)$ in the denominator of the weight:\n$$ S_{\\infty} = \\int_{-\\infty}^{\\infty} f(v) \\ln\\left(\\frac{f(v)}{g(v)}\\right) \\, dv $$\nThis expression is the definition of the Kullback–Leibler divergence from $g$ to $f$, denoted $D_{\\mathrm{KL}}(f \\| g)$. Thus, we have rigorously shown that\n$$ \\lim_{N \\to \\infty} \\widehat{S} = D_{\\mathrm{KL}}(f \\| g) $$\n\nSecond, we derive the large-$N$ limit of the effective sample size fraction $\\phi$. The effective sample size $N_{\\mathrm{eff}}$ is commonly defined as:\n$$ N_{\\mathrm{eff}} = \\frac{\\left(\\sum_{i=1}^{N} w_{i}\\right)^{2}}{\\sum_{i=1}^{N} w_{i}^{2}} $$\nTo find the large-$N$ limit, we analyze the numerator and the denominator. The sums can be expressed in terms of sample means:\n$$ \\sum_{i=1}^{N} w_{i} = N \\left( \\frac{1}{N} \\sum_{i=1}^{N} w_{i} \\right), \\quad \\sum_{i=1}^{N} w_{i}^{2} = N \\left( \\frac{1}{N} \\sum_{i=1}^{N} w_{i}^{2} \\right) $$\nAs $N \\to \\infty$, the sample means converge to their respective expectation values with respect to $g(v)$:\n$$ \\frac{1}{N} \\sum_{i=1}^{N} w_{i} \\to \\mathbb{E}_{g}[w] = \\int w(v) g(v) \\, dv = \\int \\frac{f(v)}{g(v)} g(v) \\, dv = \\int f(v) \\, dv = 1 $$\nsince $f(v)$ is a probability density function. For the second term:\n$$ \\frac{1}{N} \\sum_{i=1}^{N} w_{i}^{2} \\to \\mathbb{E}_{g}[w^{2}] = \\int w(v)^{2} g(v) \\, dv = \\int \\left(\\frac{f(v)}{g(v)}\\right)^{2} g(v) \\, dv = \\int \\frac{f(v)^{2}}{g(v)} \\, dv $$\nSubstituting these limits back into the expression for $N_{\\mathrm{eff}}$:\n$$ \\lim_{N \\to \\infty} N_{\\mathrm{eff}} = \\lim_{N \\to \\infty} \\frac{\\left(N \\cdot \\frac{1}{N}\\sum w_{i}\\right)^{2}}{N \\cdot \\frac{1}{N}\\sum w_{i}^{2}} = \\frac{(N \\cdot 1)^{2}}{N \\cdot \\mathbb{E}_{g}[w^{2}]} = \\frac{N^{2}}{N \\mathbb{E}_{g}[w^{2}]} = \\frac{N}{\\mathbb{E}_{g}[w^{2}]} $$\nThe effective sample size fraction $\\phi$ is therefore:\n$$ \\phi = \\lim_{N \\to \\infty} \\frac{N_{\\mathrm{eff}}}{N} = \\frac{1}{\\mathbb{E}_{g}[w^{2}]} $$\n\nThird, we explain how $\\phi$ serves as a complementary diagnostic that bounds $S_{\\infty}$. The quantity $1/\\phi = \\mathbb{E}_{g}[w^2]$ measures the second moment of the weights. A large value of $\\mathbb{E}_{g}[w^2]$ indicates a large spread in weight values, implying that a few markers have very large weights, a condition known as sample degeneracy. This reduces the \"effective\" number of markers, lowering $\\phi$. The relationship between $S_{\\infty}$ and $\\phi$ can be established using Jensen's inequality. The natural logarithm, $\\ln(x)$, is a strictly concave function. For a random variable $X$, Jensen's inequality states $\\mathbb{E}[\\ln(X)] \\le \\ln(\\mathbb{E}[X])$. Let the random variable be the weight $w$, and let the expectation be taken with respect to the target distribution $f(v)$.\n$$ \\mathbb{E}_{f}[\\ln w] \\le \\ln(\\mathbb{E}_{f}[w]) $$\nThe left-hand side is:\n$$ \\mathbb{E}_{f}[\\ln w] = \\int f(v) \\ln\\left(\\frac{f(v)}{g(v)}\\right) \\, dv = D_{\\mathrm{KL}}(f \\| g) = S_{\\infty} $$\nThe right-hand side is:\n$$ \\mathbb{E}_{f}[w] = \\int f(v) w(v) \\, dv = \\int f(v) \\frac{f(v)}{g(v)} \\, dv = \\int \\frac{f(v)^{2}}{g(v)} \\, dv = \\mathbb{E}_{g}[w^{2}] = \\frac{1}{\\phi} $$\nCombining these results yields the inequality:\n$$ S_{\\infty} \\le \\ln\\left(\\frac{1}{\\phi}\\right) $$\nThis result shows that the effective sample size fraction $\\phi$ provides a rigorous upper bound on the entropy diagnostic $S_{\\infty}$. A small value of $\\phi$ (high degeneracy) implies a large upper bound for $S_{\\infty}$, which is consistent because high degeneracy arises from a large mismatch between $f$ and $g$, which in turn corresponds to a large $D_{\\mathrm{KL}}(f \\| g)$.\n\nFinally, we compute $S_{\\infty} = D_{\\mathrm{KL}}(f \\| g)$ for the specialized case where $f(v)$ and $g(v)$ are Gaussian PDFs.\n$$ f(v) = \\mathcal{N}(v; \\mu_{f}, \\sigma_{f}^{2}), \\quad g(v) = \\mathcal{N}(v; \\mu_{g}, \\sigma_{g}^{2}) $$\nWe must evaluate the integral from first principles:\n$$ S_{\\infty} = \\int_{-\\infty}^{\\infty} f(v) \\ln\\left(\\frac{f(v)}{g(v)}\\right) \\, dv = \\mathbb{E}_{f}\\left[\\ln\\left(\\frac{f(v)}{g(v)}\\right)\\right] $$\nThe logarithm of the ratio of the two PDFs is:\n$$ \\ln\\left(\\frac{f(v)}{g(v)}\\right) = \\ln\\left(\\frac{1/\\left(\\sqrt{2\\pi}\\sigma_{f}\\right) \\exp\\left(-\\frac{(v-\\mu_{f})^2}{2\\sigma_{f}^2}\\right)}{1/\\left(\\sqrt{2\\pi}\\sigma_{g}\\right) \\exp\\left(-\\frac{(v-\\mu_{g})^2}{2\\sigma_{g}^2}\\right)}\\right) = \\ln\\left(\\frac{\\sigma_{g}}{\\sigma_{f}}\\right) - \\frac{(v-\\mu_{f})^2}{2\\sigma_{f}^2} + \\frac{(v-\\mu_{g})^2}{2\\sigma_{g}^2} $$\nTaking the expectation with respect to $f(v)$:\n$$ S_{\\infty} = \\mathbb{E}_{f}\\left[\\ln\\left(\\frac{\\sigma_{g}}{\\sigma_{f}}\\right)\\right] - \\mathbb{E}_{f}\\left[\\frac{(v-\\mu_{f})^2}{2\\sigma_{f}^2}\\right] + \\mathbb{E}_{f}\\left[\\frac{(v-\\mu_{g})^2}{2\\sigma_{g}^2}\\right] $$\nWe evaluate each term:\n1.  The first term is a constant: $\\mathbb{E}_{f}\\left[\\ln\\left(\\frac{\\sigma_{g}}{\\sigma_{f}}\\right)\\right] = \\ln\\left(\\frac{\\sigma_{g}}{\\sigma_{f}}\\right)$.\n2.  The second term involves the variance of $f(v)$. By definition, $\\mathbb{E}_{f}[(v-\\mu_{f})^2] = \\sigma_{f}^2$.\n    $$ \\mathbb{E}_{f}\\left[\\frac{(v-\\mu_{f})^2}{2\\sigma_{f}^2}\\right] = \\frac{1}{2\\sigma_{f}^2} \\mathbb{E}_{f}[(v-\\mu_{f})^2] = \\frac{\\sigma_{f}^2}{2\\sigma_{f}^2} = \\frac{1}{2} $$\n3.  For the third term, we expand the squared term:\n    $$ \\mathbb{E}_{f}[(v-\\mu_{g})^2] = \\mathbb{E}_{f}[((v-\\mu_{f})+(\\mu_{f}-\\mu_{g}))^2] = \\mathbb{E}_{f}[(v-\\mu_{f})^2 + 2(v-\\mu_{f})(\\mu_{f}-\\mu_{g}) + (\\mu_{f}-\\mu_{g})^2] $$\n    By linearity of expectation:\n    $$ \\mathbb{E}_{f}[(v-\\mu_{g})^2] = \\mathbb{E}_{f}[(v-\\mu_{f})^2] + 2(\\mu_{f}-\\mu_{g})\\mathbb{E}_{f}[v-\\mu_{f}] + (\\mu_{f}-\\mu_{g})^2 $$\n    We know $\\mathbb{E}_{f}[(v-\\mu_{f})^2] = \\sigma_{f}^2$ and $\\mathbb{E}_{f}[v-\\mu_{f}] = \\mathbb{E}_{f}[v]-\\mu_{f} = \\mu_{f}-\\mu_{f}=0$.\n    $$ \\mathbb{E}_{f}[(v-\\mu_{g})^2] = \\sigma_{f}^2 + (\\mu_{f}-\\mu_{g})^2 $$\n    So, the third term is $\\frac{\\sigma_{f}^2 + (\\mu_{f}-\\mu_{g})^2}{2\\sigma_{g}^2}$.\n\nCombining all terms gives the analytical expression for the KL divergence:\n$$ S_{\\infty} = D_{\\mathrm{KL}}(f \\| g) = \\ln\\left(\\frac{\\sigma_{g}}{\\sigma_{f}}\\right) - \\frac{1}{2} + \\frac{\\sigma_{f}^2 + (\\mu_{f}-\\mu_{g})^2}{2\\sigma_{g}^2} $$\nNow, we substitute the given numerical values:\n$\\mu_{f} = 5.0 \\times 10^{3}$, $\\sigma_{f} = 1.8 \\times 10^{3}$, $\\mu_{g} = 2.0 \\times 10^{3}$, $\\sigma_{g} = 1.5 \\times 10^{3}$.\nThe ratio of standard deviations is $\\frac{\\sigma_{g}}{\\sigma_{f}} = \\frac{1.5 \\times 10^{3}}{1.8 \\times 10^{3}} = \\frac{1.5}{1.8} = \\frac{5}{6}$.\nThe difference in means is $\\mu_{f} - \\mu_{g} = (5.0 - 2.0) \\times 10^3 = 3.0 \\times 10^3$.\n$$ S_{\\infty} = \\ln\\left(\\frac{5}{6}\\right) - \\frac{1}{2} + \\frac{(1.8 \\times 10^{3})^2 + (3.0 \\times 10^{3})^2}{2(1.5 \\times 10^{3})^2} $$\n$$ S_{\\infty} = \\ln\\left(\\frac{5}{6}\\right) - 0.5 + \\frac{3.24 \\times 10^{6} + 9.0 \\times 10^{6}}{2 \\times (2.25 \\times 10^{6})} $$\n$$ S_{\\infty} = \\ln\\left(\\frac{5}{6}\\right) - 0.5 + \\frac{12.24 \\times 10^{6}}{4.5 \\times 10^{6}} $$\n$$ S_{\\infty} = \\ln\\left(\\frac{5}{6}\\right) - 0.5 + 2.72 $$\nUsing the value $\\ln(5/6) \\approx -0.18232155...$:\n$$ S_{\\infty} \\approx -0.18232155 - 0.5 + 2.72 = 2.03767845 $$\nRounding to four significant figures, we get $2.038$.",
            "answer": "$$\\boxed{2.038}$$"
        }
    ]
}