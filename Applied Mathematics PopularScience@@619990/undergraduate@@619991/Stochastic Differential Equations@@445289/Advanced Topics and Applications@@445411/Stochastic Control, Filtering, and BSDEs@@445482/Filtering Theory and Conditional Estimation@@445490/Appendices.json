{"hands_on_practices": [{"introduction": "The essence of filtering lies in optimally combining our prior belief about a system's state with new, incoming information. This exercise focuses on the heart of the Kalman filter: the measurement update step. By working through a concrete numerical example [@problem_id:3053885], you will see exactly how a new observation is used to refine the state estimate and reduce its uncertainty, grounding the abstract theory in a tangible calculation.", "problem": "Consider the discrete-time linear Gaussian state-space model\n$$x_{k+1} = A x_k + w_k,\\qquad y_k = C x_k + v_k,$$\nwhere $w_k \\sim \\mathcal{N}(0,Q)$ and $v_k \\sim \\mathcal{N}(0,R)$ are independent of each other and of the initial state. At time $k=0$, you are given a prior (prediction) for the state as $x_0 \\sim \\mathcal{N}(m_0, P_0)$, and then you observe a single measurement $y_0$. The model parameters are\n$$A = 0.7,\\quad C = 1.2,\\quad Q = 0.09,\\quad R = 0.16,$$\nand the prior at time $k=0$ is\n$$m_0 = 1.5,\\quad P_0 = 0.5,$$\nwith observation\n$$y_0 = 2.0.$$\nUsing first principles of conditional estimation for jointly Gaussian variables and the optimal linear minimum mean-squared error (LMMSE) update implied by the model, compute the posterior mean $m_0^{+}$ and posterior variance $P_0^{+}$ of $x_0$ after observing $y_0$. Express your final numeric answer as a single row matrix $\\big[m_0^{+},\\,P_0^{+}\\big]$ in exact form (no rounding).", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Model Equations**:\n  - State dynamics: $x_{k+1} = A x_k + w_k$\n  - Measurement model: $y_k = C x_k + v_k$\n- **Noise Distributions**:\n  - Process noise: $w_k \\sim \\mathcal{N}(0,Q)$\n  - Measurement noise: $v_k \\sim \\mathcal{N}(0,R)$\n  - The noise sequences $\\{w_k\\}$ and $\\{v_k\\}$ are independent of each other and of the initial state $x_0$.\n- **Model Parameters**:\n  - $A = 0.7$\n  - $C = 1.2$\n  - $Q = 0.09$\n  - $R = 0.16$\n- **Prior Information at time $k=0$**:\n  - The state $x_0$ has a prior distribution $x_0 \\sim \\mathcal{N}(m_0, P_0)$.\n  - Prior mean: $m_0 = 1.5$\n  - Prior variance: $P_0 = 0.5$\n- **Observation at time $k=0$**:\n  - $y_0 = 2.0$\n- **Objective**:\n  - Compute the posterior mean $m_0^{+}$ and posterior variance $P_0^{+}$ of $x_0$ after observing $y_0$. The computation must be based on first principles of conditional estimation for jointly Gaussian variables.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a standard application of Bayesian inference for a linear Gaussian state-space model. This is a fundamental topic in filtering theory and stochastic control, specifically representing the measurement update step of a Kalman filter. The model is mathematically and scientifically sound.\n2.  **Well-Posed**: The problem is well-posed. The prior distribution of the state and the likelihood function (implied by the measurement model) are fully specified and are Gaussian. This ensures that the posterior distribution is also Gaussian and its parameters (mean and variance) are uniquely determined. All necessary numerical values ($m_0, P_0, C, R, y_0$) are provided to find the solution. The parameters $A$ and $Q$ describe the dynamics for a full filtering problem but are not required for the specific task of updating the estimate at time $k=0$ given the measurement $y_0$. Their presence does not create a contradiction.\n3.  **Objective**: The problem is stated in precise, objective mathematical language, free of ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be provided.\n\nThe problem requires us to compute the conditional mean $E[x_0 | y_0]$ and conditional variance $\\text{Var}(x_0 | y_0)$ after a single measurement $y_0$ is observed at time $k=0$. These are the posterior mean $m_0^{+}$ and posterior variance $P_0^{+}$, respectively. Since the prior distribution of $x_0$ is Gaussian and the observation $y_0$ is a linear function of $x_0$ plus Gaussian noise, the state $x_0$ and observation $y_0$ are jointly Gaussian.\n\nLet us define the random vector $\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix}$. We first find its mean and covariance matrix.\n\nThe mean of $x_0$ is given as $E[x_0] = m_0$.\nThe mean of $y_0$ is derived from the measurement equation:\n$$E[y_0] = E[C x_0 + v_0] = C E[x_0] + E[v_0]$$\nSince $v_0 \\sim \\mathcal{N}(0,R)$, we have $E[v_0] = 0$. Therefore,\n$$E[y_0] = C m_0$$\nThe mean vector of $\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix}$ is:\n$$\\mu = E\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} = \\begin{pmatrix} m_0 \\\\ C m_0 \\end{pmatrix}$$\n\nNext, we compute the covariance matrix $\\Sigma = \\text{Cov}\\left(\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix}\\right) = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$.\nThe components are:\n- $\\Sigma_{11} = \\text{Var}(x_0) = P_0$.\n- $\\Sigma_{12} = \\text{Cov}(x_0, y_0) = E[(x_0 - m_0)(y_0 - C m_0)] = E[(x_0 - m_0)(C x_0 + v_0 - C m_0)] = E[(x_0 - m_0)(C(x_0 - m_0) + v_0)]$.\nExpanding this gives $C E[(x_0 - m_0)^2] + E[(x_0 - m_0)v_0] = C \\text{Var}(x_0) + 0$, since $x_0$ and $v_0$ are independent. So, $\\Sigma_{12} = C P_0$.\n- $\\Sigma_{21} = \\text{Cov}(y_0, x_0) = \\Sigma_{12}^T$. Since these are scalars, $\\Sigma_{21} = C P_0$.\n- $\\Sigma_{22} = \\text{Var}(y_0) = \\text{Var}(C x_0 + v_0)$. Since $x_0$ and $v_0$ are independent, the variances add: $\\text{Var}(C x_0) + \\text{Var}(v_0) = C^2 \\text{Var}(x_0) + R = C^2 P_0 + R$.\n\nSo the joint distribution is:\n$$\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} \\sim \\mathcal{N}\\left( \\begin{pmatrix} m_0 \\\\ C m_0 \\end{pmatrix}, \\begin{pmatrix} P_0 & C P_0 \\\\ C P_0 & C^2 P_0 + R \\end{pmatrix} \\right)$$\n\nFor a general jointly Gaussian vector $\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}\\right)$, the conditional distribution of $X_1$ given $X_2=x_2$ is Gaussian with mean and variance:\n$$\\mu_{1|2} = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (x_2 - \\mu_2)$$\n$$\\Sigma_{11|2} = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}$$\n\nApplying this to our problem, we identify $X_1 \\to x_0$, $X_2 \\to y_0$, and $x_2 \\to y_0$. The posterior mean $m_0^{+}$ and variance $P_0^{+}$ are:\n$$m_0^{+} = E[x_0|y_0] = m_0 + (C P_0) (C^2 P_0 + R)^{-1} (y_0 - C m_0)$$\n$$P_0^{+} = \\text{Var}(x_0|y_0) = P_0 - (C P_0) (C^2 P_0 + R)^{-1} (C P_0)$$\nThese are the standard measurement update equations for the Kalman filter.\n\nWe now substitute the given numerical values, using exact fractional forms to avoid rounding errors.\n$C = 1.2 = \\frac{12}{10} = \\frac{6}{5}$\n$R = 0.16 = \\frac{16}{100} = \\frac{4}{25}$\n$m_0 = 1.5 = \\frac{3}{2}$\n$P_0 = 0.5 = \\frac{1}{2}$\n$y_0 = 2.0 = 2$\n\nFirst, let's compute the term $(C^2 P_0 + R)$, which is the innovation covariance, often denoted as $S_0$.\n$$S_0 = C^2 P_0 + R = \\left(\\frac{6}{5}\\right)^2 \\left(\\frac{1}{2}\\right) + \\frac{4}{25} = \\frac{36}{25} \\cdot \\frac{1}{2} + \\frac{4}{25} = \\frac{18}{25} + \\frac{4}{25} = \\frac{22}{25}$$\n\nNow, we can compute the posterior variance $P_0^{+}$:\n$$P_0^{+} = P_0 - (C P_0) S_0^{-1} (C P_0) = P_0 - \\frac{(C P_0)^2}{S_0}$$\n$$C P_0 = \\frac{6}{5} \\cdot \\frac{1}{2} = \\frac{3}{5}$$\n$$P_0^{+} = \\frac{1}{2} - \\frac{(\\frac{3}{5})^2}{\\frac{22}{25}} = \\frac{1}{2} - \\frac{\\frac{9}{25}}{\\frac{22}{25}} = \\frac{1}{2} - \\frac{9}{22} = \\frac{11}{22} - \\frac{9}{22} = \\frac{2}{22} = \\frac{1}{11}$$\n\nNext, we compute the posterior mean $m_0^{+}$.\nThe term $(y_0 - C m_0)$ is the innovation or measurement residual.\n$$C m_0 = \\frac{6}{5} \\cdot \\frac{3}{2} = \\frac{9}{5}$$\n$$y_0 - C m_0 = 2 - \\frac{9}{5} = \\frac{10}{5} - \\frac{9}{5} = \\frac{1}{5}$$\nNow we can compute $m_0^{+}$:\n$$m_0^{+} = m_0 + (C P_0) S_0^{-1} (y_0 - C m_0)$$\n$$m_0^{+} = \\frac{3}{2} + \\left(\\frac{3}{5}\\right) \\left(\\frac{22}{25}\\right)^{-1} \\left(\\frac{1}{5}\\right) = \\frac{3}{2} + \\left(\\frac{3}{5}\\right) \\left(\\frac{25}{22}\\right) \\left(\\frac{1}{5}\\right)$$\n$$m_0^{+} = \\frac{3}{2} + \\frac{3 \\cdot 25 \\cdot 1}{5 \\cdot 22 \\cdot 5} = \\frac{3}{2} + \\frac{3 \\cdot (5 \\cdot 5)}{(5 \\cdot 5) \\cdot 22} = \\frac{3}{2} + \\frac{3}{22}$$\n$$m_0^{+} = \\frac{3 \\cdot 11}{2 \\cdot 11} + \\frac{3}{22} = \\frac{33}{22} + \\frac{3}{22} = \\frac{36}{22} = \\frac{18}{11}$$\n\nThe posterior mean is $m_0^{+} = \\frac{18}{11}$ and the posterior variance is $P_0^{+} = \\frac{1}{11}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{18}{11} & \\frac{1}{11}\n\\end{pmatrix}\n}\n$$", "id": "3053885"}, {"introduction": "While a single update is fundamental, a filter's long-term performance depends on the intrinsic properties of the system it observes. A key property is observability: can all components of the state be inferred from the measurements? This practice [@problem_id:3053882] challenges you to first diagnose a system's observability and then compute the steady-state error covariance, revealing the direct link between a system's structure and the ultimate limits of estimation accuracy.", "problem": "Consider the continuous-time linear stochastic system governed by the stochastic differential equation (SDE) for the state and the measurement model\n$$\n\\mathrm{d}x(t) = A\\,x(t)\\,\\mathrm{d}t + \\mathrm{d}w(t), \\quad \\mathrm{d}y(t) = C\\,x(t)\\,\\mathrm{d}t + \\mathrm{d}v(t),\n$$\nwhere $A \\in \\mathbb{R}^{2 \\times 2}$ and $C \\in \\mathbb{R}^{1 \\times 2}$ are given by\n$$\nA = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1 & 0 \\end{pmatrix}.\n$$\nAssume the process noise $w(t)$ and measurement noise $v(t)$ are independent standard Wiener processes with covariances\n$$\nQ = I_2 \\quad \\text{and} \\quad R = 1,\n$$\nwhere $I_2$ denotes the $2 \\times 2$ identity matrix. Starting from the foundational definitions of observability for linear time-invariant systems and the well-established steady-state algebraic Riccati equation for continuous-time filtering (Kalman-Bucy filtering), do the following:\n\n1. Test the observability of the pair $(A,C)$ using the rank of the observability matrix constructed from $A$ and $C$.\n\n2. Using the steady-state algebraic Riccati equation for the estimation error covariance,\n$$\nA P + P A^{\\top} - P C^{\\top} R^{-1} C P + Q = 0,\n$$\nsolve for the steady-state error covariance matrix $P \\in \\mathbb{R}^{2 \\times 2}$ under the assumption that $P$ is symmetric and positive semidefinite. Then determine whether the resulting $P$ is positive definite.\n\nProvide your final numeric answer as the determinant of the steady-state solution $P$. No rounding is required. Do not include any units in your final answer.", "solution": "The problem requires a two-part analysis of a continuous-time linear stochastic system. First, we must test the observability of the system, and second, we must solve for the steady-state estimation error covariance matrix $P$ and find its determinant.\n\nPart 1: Observability Analysis\n\nThe system is described by the state and measurement equations:\n$$\n\\mathrm{d}x(t) = A\\,x(t)\\,\\mathrm{d}t + \\mathrm{d}w(t), \\quad \\mathrm{d}y(t) = C\\,x(t)\\,\\mathrm{d}t + \\mathrm{d}v(t)\n$$\nwith matrices $A = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix}$ and $C = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$.\nThe state dimension is $n=2$. Observability of the pair $(A, C)$ is determined by the rank of the observability matrix $\\mathcal{O}$. For a second-order system, the observability matrix is defined as:\n$$\n\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\end{pmatrix}\n$$\nWe first compute the product $CA$:\n$$\nCA = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix} = \\begin{pmatrix} (1)(-1) + (0)(0) & (1)(0) + (0)(-2) \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\end{pmatrix}\n$$\nNow, we construct the observability matrix $\\mathcal{O}$:\n$$\n\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 0 \\end{pmatrix}\n$$\nThe rank of a matrix is the maximum number of linearly independent rows or columns. The second row of $\\mathcal{O}$ is $(-1)$ times the first row, so the rows are linearly dependent. Consequently, the rank of $\\mathcal{O}$ is $1$.\nA system is fully observable if and only if the rank of its observability matrix equals the dimension of the state vector, $n$. Here, $\\mathrm{rank}(\\mathcal{O}) = 1$, which is less than the state dimension $n=2$. Therefore, the pair $(A, C)$ is not observable.\n\nPart 2: Steady-State Algebraic Riccati Equation\n\nWe are asked to solve the steady-state algebraic Riccati equation (ARE) for the error covariance matrix $P$:\n$$\nA P + P A^{\\top} - P C^{\\top} R^{-1} C P + Q = 0\n$$\nThe provided values are $A = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix}$, $C = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$, $Q = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$, and $R=1$. Let $P$ be a symmetric matrix, $P = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix}$.\n\nSince $A$ is symmetric, $A^{\\top} = A$. The ARE simplifies to $A P + P A - P C^{\\top} R^{-1} C P + Q = 0$.\nFirst, let's compute the quadratic term $P C^{\\top} R^{-1} C P$. We have $R^{-1} = 1^{-1} = 1$ and $C^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n$$\nC^{\\top} R^{-1} C = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} (1) \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThen,\n$$\nP (C^{\\top} R^{-1} C) P = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} = \\begin{pmatrix} p_{11} & 0 \\\\ p_{12} & 0 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} = \\begin{pmatrix} p_{11}^2 & p_{11} p_{12} \\\\ p_{11} p_{12} & p_{12}^2 \\end{pmatrix}\n$$\nNext, we compute the linear terms $AP+PA$:\n$$\nAP + PA = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} + \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} -p_{11} & -p_{12} \\\\ -2p_{12} & -2p_{22} \\end{pmatrix} + \\begin{pmatrix} -p_{11} & -2p_{12} \\\\ -p_{12} & -2p_{22} \\end{pmatrix} = \\begin{pmatrix} -2p_{11} & -3p_{12} \\\\ -3p_{12} & -4p_{22} \\end{pmatrix}\n$$\nSubstituting all terms back into the ARE:\n$$\n\\begin{pmatrix} -2p_{11} & -3p_{12} \\\\ -3p_{12} & -4p_{22} \\end{pmatrix} - \\begin{pmatrix} p_{11}^2 & p_{11} p_{12} \\\\ p_{11} p_{12} & p_{12}^2 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis matrix equation yields a system of scalar algebraic equations for the elements of $P$:\n1.  $(1,1)$: $-2p_{11} - p_{11}^2 + 1 = 0$, which can be rewritten as $p_{11}^2 + 2p_{11} - 1 = 0$.\n2.  $(1,2)$: $-3p_{12} - p_{11}p_{12} = 0$, which factors to $p_{12}(3+p_{11})=0$.\n3.  $(2,2)$: $-4p_{22} - p_{12}^2 + 1 = 0$.\n\nFrom equation (1), we use the quadratic formula to solve for $p_{11}$:\n$$\np_{11} = \\frac{-2 \\pm \\sqrt{2^2 - 4(1)(-1)}}{2(1)} = \\frac{-2 \\pm \\sqrt{8}}{2} = -1 \\pm \\sqrt{2}\n$$\nSince $P$ must be a positive semidefinite matrix, its diagonal elements must be non-negative ($p_{11} \\ge 0$). The value $p_{11} = -1 - \\sqrt{2}$ is negative. Thus, we must select the positive root: $p_{11} = \\sqrt{2} - 1$.\n\nFrom equation (2), since $p_{11} = \\sqrt{2} - 1$, the term $3 + p_{11} = 3 + \\sqrt{2} - 1 = 2 + \\sqrt{2} \\neq 0$. Therefore, for the equation $p_{12}(3+p_{11})=0$ to hold, we must have $p_{12}=0$.\n\nFrom equation (3), we substitute $p_{12}=0$:\n$$\n-4p_{22} - 0^2 + 1 = 0 \\implies 4p_{22} = 1 \\implies p_{22} = \\frac{1}{4}\n$$\nThe steady-state error covariance matrix is therefore:\n$$\nP = \\begin{pmatrix} \\sqrt{2}-1 & 0 \\\\ 0 & \\frac{1}{4} \\end{pmatrix}\n$$\nTo check if $P$ is positive definite, we examine its eigenvalues. As $P$ is a diagonal matrix, its eigenvalues are its diagonal entries: $\\lambda_1 = \\sqrt{2}-1$ and $\\lambda_2=\\frac{1}{4}$. Since $\\sqrt{2} \\approx 1.414$, $\\lambda_1 \\approx 0.414 > 0$. And $\\lambda_2 = 0.25 > 0$. Both eigenvalues are strictly positive, which confirms that $P$ is a positive definite matrix.\n\nThe problem asks for the determinant of $P$.\n$$\n\\det(P) = p_{11} p_{22} - p_{12}^2 = (\\sqrt{2}-1)\\left(\\frac{1}{4}\\right) - (0)^2 = \\frac{\\sqrt{2}-1}{4}\n$$", "answer": "$$\\boxed{\\frac{\\sqrt{2}-1}{4}}$$", "id": "3053882"}, {"introduction": "In any real-world application, the true state of a system is unknown, which begs the question: how do we know if our filter is working correctly? A powerful diagnostic technique is to analyze the innovation sequenceâ€”the difference between the measurements predicted by the filter and the actual measurements received. This practice [@problem_id:3053906] guides you through a whiteness test, a crucial statistical tool for validating a filter's performance by checking if its innovations are truly unpredictable, as theory dictates they should be.", "problem": "You are given a scalar linear Gaussian state-space model with a hidden state and observable outputs. The model is specified by the stochastic difference equations\n- State dynamics: $x_{k+1} = A x_k + w_k$\n- Measurement: $y_k = C x_k + v_k$\nwhere $\\{w_k\\}$ is a sequence of independent Gaussian random variables with $w_k \\sim \\mathcal{N}(0,Q)$, and $\\{v_k\\}$ is a sequence of independent Gaussian random variables with $v_k \\sim \\mathcal{N}(0,R)$. The sequences $\\{w_k\\}$ and $\\{v_k\\}$ are independent of each other and of the initial state $x_0$, which is Gaussian with mean $\\mu_0$ and variance $P_0$. This setting is a standard scalar instance of a linear Gaussian system in the sense of stochastic differential equations discretized for filtering.\n\nYour task is to:\n- Construct the empirical innovations sequence from given observed data $\\{y_k\\}_{k=0}^{N-1}$ using the one-step-ahead predictor from the Kalman filter.\n- Estimate the sample autocorrelation function (ACF) at positive lags from this innovations sequence using a centered, biased estimator.\n- Perform a whiteness test for the innovations based on large-sample bounds for sample autocorrelations.\n\nUse only the following fundamental definitions and widely accepted results as the base for your derivation and implementation:\n- The Kalman filter one-step predictor and innovation definitions for a linear Gaussian state-space model:\n  - Prediction: $x_{k\\mid k-1} = A x_{k-1\\mid k-1}$ and $P_{k\\mid k-1} = A P_{k-1\\mid k-1} A + Q$.\n  - Innovation: $e_k = y_k - C x_{k\\mid k-1}$ and innovation variance $S_k = C P_{k\\mid k-1} C + R$.\n  - Kalman gain: $K_k = \\dfrac{P_{k\\mid k-1} C}{S_k}$.\n  - Update: $x_{k\\mid k} = x_{k\\mid k-1} + K_k e_k$ and $P_{k\\mid k} = (1 - K_k C) P_{k\\mid k-1}$.\n- The centered, biased sample autocorrelation estimate at lag $\\ell \\ge 1$ for a sequence $\\{e_k\\}_{k=0}^{N-1}$:\n  - Let $\\bar{e} = \\dfrac{1}{N}\\sum_{k=0}^{N-1} e_k$ and $d = \\sum_{k=0}^{N-1} (e_k - \\bar{e})^2$.\n  - Then $r_\\ell = \\dfrac{\\sum_{k=\\ell}^{N-1} (e_k - \\bar{e})(e_{k-\\ell} - \\bar{e})}{d}$ for $\\ell = 1,2,\\dots,L'$, where $L' = \\min(L, N-2)$ and $L$ is a user-specified maximum lag.\n- A large-sample decision rule for whiteness of innovations: under the null hypothesis that $\\{e_k\\}$ are independent with zero mean (white), it is a widely used approximation that for each fixed $\\ell \\ge 1$, $r_\\ell$ is approximately Gaussian with mean $0$ and variance $1/N$. Therefore, a two-sided $95\\%$ confidence bound is given by $\\pm \\dfrac{1.96}{\\sqrt{N}}$. Conclude the innovations are white if $\\lvert r_\\ell \\rvert \\le \\dfrac{1.96}{\\sqrt{N}}$ holds for all $\\ell = 1,\\dots,L'$. If $L' = 0$ (that is, when $N \\le 2$), accept whiteness by convention since no positive-lag autocorrelation can be computed.\n\nImplement the above for the specific model and test suite below. In all cases, the system parameters are\n- $A = 0$, $C = 1$, $Q = 0$, $R = 1$, $\\mu_0 = 0$, $P_0 = 1$,\nand the maximum lag is $L = 3$. Note that these parameters imply $x_{k\\mid k-1} = 0$ for all $k \\ge 0$, hence $e_k = y_k$.\n\nTest suite (three cases):\n- Case A (happy path, length $N = 12$): $y = [0, 2.0, 0, 0, 0, 0, 0, 0, 0, 0, -2.0, 0]$.\n- Case B (strong serial correlation, length $N = 20$): $y = [1,1,1,1,1,1,1,1,1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]$.\n- Case C (short sequence edge case, length $N = 3$): $y = [1.0, -1.0, 1.0]$.\n\nYour program must:\n- For each case, compute the innovations $\\{e_k\\}$ via the Kalman one-step predictor and the centered, biased sample autocorrelations $r_\\ell$ for $\\ell = 1,\\dots,L'$, where $L' = \\min(L,N-2)$ with $L = 3$.\n- Decide whiteness for each case using the two-sided bound $\\pm \\dfrac{1.96}{\\sqrt{N}}$ applied to all lags $\\ell = 1,\\dots,L'$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[{\\tt True},{\\tt False},{\\tt True}]$. The result for each case must be a boolean indicating whether whiteness is accepted.\n\nFinal output format:\n- A single line with a Python-style list of three booleans corresponding to Cases A, B, and C in that order, for example, $[{\\tt True},{\\tt False},{\\tt True}]$.", "solution": "The problem requires an analysis of a time series based on the theory of linear Gaussian state-space models and hypothesis testing for whiteness. The analysis proceeds in two main stages: first, simplifying the Kalman filter equations under the given parameters, and second, applying statistical tests to the resulting innovations sequence.\n\n**1. Kalman Filter Simplification**\n\nThe state-space model is given by the scalar equations:\nState dynamics: $x_{k+1} = A x_k + w_k$, where $w_k \\sim \\mathcal{N}(0,Q)$\nMeasurement: $y_k = C x_k + v_k$, where $v_k \\sim \\mathcal{N}(0,R)$\n\nThe specified parameters are $A = 0$, $C = 1$, $Q = 0$, $R = 1$. The initial state is $x_0 \\sim \\mathcal{N}(\\mu_0, P_0)$, with $\\mu_0=0$ and $P_0=1$. The task is to compute the innovations sequence, $e_k = y_k - C x_{k\\mid k-1}$, where $x_{k\\mid k-1}$ is the one-step-ahead prediction of the state.\n\nThe Kalman filter prediction for the state is given by $x_{k\\mid k-1} = A x_{k-1\\mid k-1}$.\nLet's analyze this recursion. The process starts with a prior estimate for $x_0$, which is $x_{0\\mid -1} = \\mu_0$. Given $\\mu_0 = 0$, we have $x_{0\\mid -1} = 0$.\n\nFor $k=0$, the innovation is $e_0 = y_0 - C x_{0\\mid -1} = y_0 - (1)(0) = y_0$.\nThe updated state estimate is $x_{0\\mid 0} = x_{0\\mid-1} + K_0 e_0 = 0 + K_0 y_0$.\n\nFor $k=1$, the prediction is $x_{1\\mid 0} = A x_{0\\mid 0}$. With $A=0$, we have $x_{1\\mid 0} = 0 \\cdot x_{0\\mid 0} = 0$.\nThe innovation is $e_1 = y_1 - C x_{1\\mid 0} = y_1 - (1)(0) = y_1$.\n\nBy induction, let's assume $x_{k-1\\mid k-2} = 0$. The prediction for step $k$ is $x_{k\\mid k-1} = A x_{k-1\\mid k-1}$. Since $A=0$, this gives $x_{k\\mid k-1}=0$ regardless of the value of $x_{k-1\\mid k-1}$. Therefore, the one-step-ahead prediction of the state is identically zero for all time steps $k \\ge 0$.\n\n$x_{k\\mid k-1} = 0$ for all $k \\ge 0$.\n\nConsequently, the innovations sequence $\\{e_k\\}$ simplifies to the observation sequence $\\{y_k\\}$:\n$$e_k = y_k - C x_{k\\mid k-1} = y_k - (1)(0) = y_k$$\nThis simplification reduces the problem from a filtering problem to a direct time series analysis of the given data $\\{y_k\\}$.\n\n**2. Whiteness Test Methodology**\n\nThe whiteness of the innovations sequence $\\{e_k\\}_{k=0}^{N-1}$ is tested using its sample autocorrelation function (ACF).\n\nThe centered, biased sample ACF estimate at lag $\\ell \\ge 1$ is defined as:\n$$r_\\ell = \\frac{\\sum_{k=\\ell}^{N-1} (e_k - \\bar{e})(e_{k-\\ell} - \\bar{e})}{\\sum_{k=0}^{N-1} (e_k - \\bar{e})^2}$$\nwhere $\\bar{e} = \\frac{1}{N}\\sum_{k=0}^{N-1} e_k$ is the sample mean. The ACF is computed for lags $\\ell = 1, 2, \\dots, L'$, where $L' = \\min(L, N-2)$ and the maximum lag is specified as $L=3$.\n\nThe whiteness test is based on the large-sample approximation that, under the null hypothesis of whiteness, each $r_\\ell$ is approximately distributed as $\\mathcal{N}(0, 1/N)$. A two-sided $95\\%$ confidence interval for $r_\\ell$ is thus given by $\\pm 1.96/\\sqrt{N}$. The sequence $\\{e_k\\}$ is considered white if all computed sample autocorrelations fall within this bound:\n$$|r_\\ell| \\le \\frac{1.96}{\\sqrt{N}} \\quad \\text{for all } \\ell = 1, \\dots, L'$$\nIf $L' = 0$ (i.e., for $N \\le 2$), whiteness is accepted by convention.\n\n**3. Case-by-Case Analysis**\n\nWe now apply this methodology to the three test cases provided.\n\n**Case A:**\n- Data: $y = [0, 2.0, 0, 0, 0, 0, 0, 0, 0, 0, -2.0, 0]$\n- Length: $N=12$.\n- Innovations: $e = y$.\n- Maximum lag to test: $L' = \\min(3, 12-2) = \\min(3, 10) = 3$. We must compute $r_1, r_2, r_3$.\n- Sample mean: $\\bar{e} = \\frac{1}{12}(2.0 - 2.0) = 0$.\n- Denominator: $d = \\sum_{k=0}^{11} (e_k - 0)^2 = (2.0)^2 + (-2.0)^2 = 4 + 4 = 8$.\n- ACF numerators for $\\ell=1, 2, 3$:\n  - $\\ell=1$: $\\sum_{k=1}^{11} e_k e_{k-1} = e_1 e_0 + \\dots + e_{11}e_{10} = (2.0)(0) + \\dots + (0)(-2.0) = 0$.\n  - $\\ell=2$: $\\sum_{k=2}^{11} e_k e_{k-2} = 0$.\n  - $\\ell=3$: $\\sum_{k=3}^{11} e_k e_{k-3} = 0$.\n- Sample ACF: $r_1 = r_2 = r_3 = 0/8 = 0$.\n- Whiteness test threshold: $T = \\frac{1.96}{\\sqrt{12}} \\approx 0.5657$.\n- Decision: For $\\ell=1, 2, 3$, we have $|r_\\ell|=0 \\le 0.5657$. The condition holds.\n- Conclusion for Case A: **Whiteness is accepted (True)**.\n\n**Case B:**\n- Data: $y = [1,1,1,1,1,1,1,1,1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]$\n- Length: $N=20$.\n- Innovations: $e = y$.\n- Maximum lag to test: $L' = \\min(3, 20-2) = \\min(3, 18) = 3$. We must compute $r_1, r_2, r_3$.\n- Sample mean: $\\bar{e} = \\frac{1}{20}(10 \\times 1 + 10 \\times (-1)) = 0$.\n- Denominator: $d = \\sum_{k=0}^{19} (e_k - 0)^2 = 10 \\times (1)^2 + 10 \\times (-1)^2 = 20$.\n- ACF numerator for $\\ell=1$: $\\sum_{k=1}^{19} e_k e_{k-1} = (9 \\times (1)(1)) + (( -1)(1)) + (9 \\times (-1)(-1)) = 9 - 1 + 9 = 17$.\n- Sample ACF for $\\ell=1$: $r_1 = 17/20 = 0.85$.\n- Whiteness test threshold: $T = \\frac{1.96}{\\sqrt{20}} \\approx 0.4382$.\n- Decision: For $\\ell=1$, we have $|r_1|=0.85 > 0.4382$. The condition is violated. The test fails at the first lag.\n- Conclusion for Case B: **Whiteness is rejected (False)**.\n\n**Case C:**\n- Data: $y = [1.0, -1.0, 1.0]$\n- Length: $N=3$.\n- Innovations: $e = y$.\n- Maximum lag to test: $L' = \\min(3, 3-2) = \\min(3, 1) = 1$. We must compute $r_1$.\n- Sample mean: $\\bar{e} = \\frac{1}{3}(1.0 - 1.0 + 1.0) = 1/3$.\n- Denominator: $d = \\sum_{k=0}^{2} (e_k - \\bar{e})^2 = (1 - 1/3)^2 + (-1 - 1/3)^2 + (1 - 1/3)^2 = (2/3)^2 + (-4/3)^2 + (2/3)^2 = 4/9 + 16/9 + 4/9 = 24/9 = 8/3$.\n- ACF numerator for $\\ell=1$: $\\sum_{k=1}^{2} (e_k - \\bar{e})(e_{k-1} - \\bar{e}) = (e_1-\\bar{e})(e_0-\\bar{e}) + (e_2-\\bar{e})(e_1-\\bar{e}) = (-1-1/3)(1-1/3) + (1-1/3)(-1-1/3) = (-4/3)(2/3) + (2/3)(-4/3) = -8/9 - 8/9 = -16/9$.\n- Sample ACF for $\\ell=1$: $r_1 = \\frac{-16/9}{24/9} = -16/24 = -2/3 \\approx -0.6667$.\n- Whiteness test threshold: $T = \\frac{1.96}{\\sqrt{3}} \\approx 1.1315$.\n- Decision: For $\\ell=1$, we have $|r_1| = |-2/3| \\approx 0.6667 \\le 1.1315$. The condition holds.\n- Conclusion for Case C: **Whiteness is accepted (True)**.\n\n**Summary of Results**\nThe whiteness test results for the three cases are:\n- Case A: True\n- Case B: False\n- Case C: True\nThe final output will be a list of these boolean values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a whiteness test on three time series based on their sample autocorrelation function.\n    \"\"\"\n    \n    # Test cases as defined in the problem statement.\n    test_cases = [\n        # Case A (happy path)\n        {\"y\": [0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0]},\n        # Case B (strong serial correlation)\n        {\"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]},\n        # Case C (short sequence edge case)\n        {\"y\": [1.0, -1.0, 1.0]},\n    ]\n\n    L = 3  # Maximum lag specified in the problem\n\n    results = []\n    \n    for case in test_cases:\n        y = case[\"y\"]\n        N = len(y)\n\n        # The problem states that for the given parameters, the innovations e_k are equal to the observations y_k.\n        e = np.array(y)\n        \n        # Calculate the effective maximum lag L'\n        L_prime = min(L, N - 2)\n        \n        # If L' is 0 (i.e., N <= 2), accept whiteness by convention.\n        if L_prime <= 0:\n            results.append(True)\n            continue\n            \n        # Calculate the sample mean of the innovations\n        e_mean = np.mean(e)\n        \n        # Calculate the denominator for the ACF: sum of squared deviations\n        d = np.sum((e - e_mean)**2)\n        \n        # Initialize whiteness assumption\n        is_white = True\n        \n        # Calculate the 95% confidence threshold\n        threshold = 1.96 / np.sqrt(N)\n        \n        # Check ACF for lags from 1 to L'\n        for lag in range(1, L_prime + 1):\n            # The denominator d could be zero if all e_k are the same.\n            if d == 0:\n                # If all e_k are the same, they are perfectly predictable.\n                # The ACF is ill-defined. We can treat this as non-white, but the test cases avoid this.\n                # For robustness, we can handle it, though not strictly required by problem.\n                # If N > 1 and all e_k are equal, e-e_mean is zero, numerator is zero. \n                # ACF is 0/0. A constant non-zero signal is not white. Let's assume d > 0 for problem cases.\n                pass\n\n            # Calculate the numerator of the ACF\n            # (e_k - e_mean) for k from lag to N-1\n            term1 = e[lag:] - e_mean\n            # (e_{k-lag} - e_mean) for k from lag to N-1\n            term2 = e[:-lag] - e_mean\n            \n            numerator = np.sum(term1 * term2)\n            \n            r_lag = numerator / d if d != 0 else 0\n            \n            # Check if the absolute value of the ACF exceeds the threshold\n            if abs(r_lag) > threshold:\n                is_white = False\n                break\n        \n        results.append(is_white)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3053906"}]}