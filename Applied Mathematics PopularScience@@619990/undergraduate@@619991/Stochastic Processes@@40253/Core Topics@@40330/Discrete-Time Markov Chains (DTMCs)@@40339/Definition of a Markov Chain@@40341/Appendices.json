{"hands_on_practices": [{"introduction": "To build a solid understanding of Markov chains, we begin with a foundational exercise. This problem models the length of a consecutive run of \"Heads\" in a series of coin flips. By analyzing this seemingly simple process, you will gain hands-on experience in applying the formal definition of the Markov property and see how the \"memoryless\" nature of a process can arise directly from the independence of underlying random events [@problem_id:1295273].", "problem": "Consider a sequence of independent and identically distributed (IID) coin flips. Let the probability of getting Heads (H) be $p$ and the probability of getting Tails (T) be $1-p$, where $0 < p < 1$.\n\nA stochastic process $\\{X_n\\}_{n \\geq 1}$ is defined based on this sequence of flips. For each time step $n \\geq 1$, the random variable $X_n$ represents the length of the current, unbroken run of Heads ending at time $n$. If the $n$-th flip is Tails, the run of Heads is broken, and its length is defined as 0. For example, if the sequence of flips is H, H, T, H, the corresponding sequence of states for the process would be $X_1 = 1, X_2 = 2, X_3 = 0, X_4 = 1$. The state space for this process is the set of non-negative integers, $S = \\{0, 1, 2, \\dots\\}$.\n\nIs the process $\\{X_n\\}_{n \\geq 1}$ a Markov chain? Select the correct statement and justification from the options below.\n\nA. Yes, because the value of $X_{n+1}$ is completely determined by the value of $X_n$.\n\nB. Yes, because the conditional probability distribution of the future state $X_{n+1}$ depends only upon the present state $X_n$, not on the sequence of states that preceded it.\n\nC. No, because to determine $X_{n+1}$, one must know not only $X_n$ but also the outcome of the $(n+1)$-th coin flip.\n\nD. No, because the state $X_{n+1}$ depends on the entire history of coin flips up to time $n+1$, not just the state at time $n$.\n\nE. No, because the state space is infinite, and a process with an infinite state space cannot be a Markov chain.", "solution": "Let $\\{Y_{n}\\}_{n \\geq 1}$ be the IID coin flips with $Y_{n}=1$ for Heads and $Y_{n}=0$ for Tails, where $\\mathbb{P}(Y_{n}=1)=p$ and $\\mathbb{P}(Y_{n}=0)=1-p$. By definition, $X_{n}$ is the length of the current run of Heads ending at time $n$, so it satisfies the recursive update\n$$\nX_{n+1}=\n\\begin{cases}\nX_{n}+1, & \\text{if } Y_{n+1}=1,\\\\\n0, & \\text{if } Y_{n+1}=0.\n\\end{cases}\n$$\nFix $n \\geq 1$ and any realization of the past states $X_{1}=x_{1},\\dots,X_{n-1}=x_{n-1},X_{n}=i$. Because $Y_{n+1}$ is independent of $(Y_{1},\\dots,Y_{n})$, and the process $\\{X_{k}\\}$ is a measurable function of $(Y_{1},\\dots,Y_{k})$, we have independence of $Y_{n+1}$ from the sigma-algebra generated by $(X_{1},\\dots,X_{n})$. Therefore,\n$$\n\\mathbb{P}(X_{n+1}=i+1 \\mid X_{n}=i, X_{n-1}=x_{n-1},\\dots,X_{1}=x_{1})\n= \\mathbb{P}(Y_{n+1}=1 \\mid X_{n}=i, X_{n-1}=x_{n-1},\\dots,X_{1}=x_{1})=p,\n$$\nand\n$$\n\\mathbb{P}(X_{n+1}=0 \\mid X_{n}=i, X_{n-1}=x_{n-1},\\dots,X_{1}=x_{1})\n= \\mathbb{P}(Y_{n+1}=0 \\mid X_{n}=i, X_{n-1}=x_{n-1},\\dots,X_{1}=x_{1})=1-p,\n$$\nwith\n$$\n\\mathbb{P}(X_{n+1}=j \\mid X_{n}=i, X_{n-1}=x_{n-1},\\dots,X_{1}=x_{1})=0 \\quad \\text{for all } j \\notin \\{0,i+1\\}.\n$$\nEach of these conditional probabilities depends only on the current state $i=X_{n}$ and not on the earlier states $(X_{1},\\dots,X_{n-1})$. Hence the process satisfies the Markov property:\n$$\n\\mathbb{P}(X_{n+1}=j \\mid X_{n}=i, X_{n-1},\\dots,X_{1})=\\mathbb{P}(X_{n+1}=j \\mid X_{n}=i).\n$$\nThus $\\{X_{n}\\}$ is a Markov chain with transition probabilities\n$$\n\\mathbb{P}(i \\to i+1)=p,\\quad \\mathbb{P}(i \\to 0)=1-p,\\quad \\mathbb{P}(i \\to j)=0 \\text{ for } j \\notin \\{0,i+1\\}.\n$$\nOption A is incorrect because $X_{n+1}$ is not determined solely by $X_{n}$; it is random, driven by the $(n+1)$-th flip. Option C misstates the Markov property: while the realized value uses new randomness, the conditional distribution given $X_{n}$ suffices. Option D is false since the distribution of $X_{n+1}$ does not depend on the entire history. Option E is false because Markov chains can have infinite state spaces. Therefore, the correct statement is B.", "answer": "$$\\boxed{B}$$", "id": "1295273"}, {"introduction": "Not all stochastic processes that appear simple are Markov chains. This practice presents a classic counterexample: the running maximum of a simple random walk. Your task is to demonstrate that knowing the maximum value achieved so far is insufficient to predict the next maximum; the specific path taken to reach that maximum matters. This exercise will sharpen your critical thinking and help you identify subtle ways in which a process can retain \"memory\" of its history, thus violating the Markov property [@problem_id:730489].", "problem": "A simple symmetric random walk $\\{S_n\\}_{n \\ge 0}$ on the integers $\\mathbb{Z}$ is defined by $S_0 = 0$ and $S_n = \\sum_{k=1}^n X_k$ for $n \\ge 1$, where $\\{X_k\\}$ are independent and identically distributed random variables with $P(X_k = 1) = P(X_k = -1) = 1/2$.\n\nThe running maximum of this walk is the process $\\{M_n\\}_{n \\ge 0}$ defined by $M_n = \\max\\{S_0, S_1, \\dots, S_n\\}$. A stochastic process $\\{Z_n\\}_{n \\ge 0}$ is a Markov chain if the conditional probability of future states depends only on the present state, not on the sequence of events that preceded it. That is, for all $n \\ge 0$ and any states $i_0, \\dots, i_{n+1}$,\n$P(Z_{n+1}=i_{n+1} | Z_n=i_n, \\dots, Z_0=i_0) = P(Z_{n+1}=i_{n+1} | Z_n=i_n)$.\n\nTo demonstrate that the running maximum process $\\{M_n\\}$ is not a Markov chain, we can find two different histories up to a time $n$ that end in the same state, $M_n=i$, but yield different probabilities for the state at time $n+1$.\n\nConsider the following two histories for the running maximum process up to time $n=3$:\n1.  History A: $(M_0, M_1, M_2, M_3) = (0, 1, 1, 1)$. Let this event be denoted by $H_A$.\n2.  History B: $(M_0, M_1, M_2, M_3) = (0, 0, 0, 1)$. Let this event be denoted by $H_B$.\n\nBoth histories end in the state $M_3 = 1$. If $\\{M_n\\}$ were a Markov chain, the probability of reaching any future state, say $M_4=1$, would be the same regardless of which history, A or B, occurred.\n\nYour task is to calculate the difference $D = P(M_4 = 1 | H_A) - P(M_4 = 1 | H_B)$. A non-zero value for $D$ will formally prove that $\\{M_n\\}$ does not possess the Markov property.", "solution": "We label the increments by $X_k \\in \\{+1,-1\\}$, so $S_n=\\sum_{k=1}^n X_k$ and $M_n=\\max_{0\\le j\\le n}S_j$.  Define the two histories up to time $3$:\n\nHistory A: $M_1=1, M_2=1, M_3=1$. Then $M_1=1 \\implies X_1=+1$, and $M_2=1 \\implies X_2=-1$ (since $X_2=+1$ would give $M_2=2$). Thus under $H_A$ we have\n$X_1=+1, X_2=-1, S_2=0$,\nbut $X_3$ is unrestricted (both $+1$ or $-1$ yield $M_3=1$). Hence\n$P(X_3=+1\\mid H_A)=P(X_3=-1\\mid H_A)=\\tfrac{1}{2}$.\nSince $S_3=X_3$, we split on the two cases:\n\nCase 1: $S_3=+1$. Then $S_4=1+X_4$, so $M_4=1 \\iff S_4\\le1 \\iff X_4=-1$, giving\n$P(M_4=1\\mid H_A,S_3=1)=\\tfrac{1}{2}$.\n\nCase 2: $S_3=-1$. Then $S_4=-1+X_4 \\in \\{0,-2\\} \\le 1$, so $M_4=1$ always, hence\n$P(M_4=1\\mid H_A,S_3=-1)=1$.\n\nAveraging over $S_3=\\pm1$ under $H_A$,\n$P(M_4=1\\mid H_A)\n=\\tfrac{1}{2}\\cdot\\tfrac{1}{2}+\\tfrac{1}{2}\\cdot 1\n=\\tfrac{3}{4}$.\n\nHistory B: $M_1=0, M_2=0, M_3=1$. One finds\n$X_1=-1, X_2=+1, X_3=+1, S_3=1$.\nThus under $H_B$ we know $S_3=1$, so $S_4=1+X_4$ and\n$P(M_4=1\\mid H_B)=P(X_4=-1)=\\tfrac{1}{2}$.\n\nTherefore the difference is\n$D\n=P(M_4=1\\mid H_A)-P(M_4=1\\mid H_B)\n=\\tfrac{3}{4}-\\tfrac{1}{2}\n=\\tfrac{1}{4}$.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "730489"}, {"introduction": "In many real-world applications, an observable process may not be Markovian because it is influenced by unobserved, or \"hidden,\" variables. This problem explores such a scenario involving a self-driving car whose movement depends on a fluctuating weather state. You will discover that while the car's position alone does not form a Markov chain, the combined system of (position, weather) does. This illustrates the powerful and widely-used technique of state augmentation, where we redefine the state of a system to recover the valuable Markov property [@problem_id:1295253].", "problem": "A self-driving car operates along a single-dimensional track, which can be represented by the set of integers $\\mathbb{Z}$. The car's state is updated at discrete time steps $n=0, 1, 2, \\dots$. Let $V_n$ denote the position of the car on the track at time $n$.\n\nThe car's movement is affected by the weather. Let $W_n$ be the weather state at time $n$, which can either be \"Sunny\" or \"Rainy\". The weather process itself evolves randomly: the weather at time $n+1$ depends probabilistically only on the weather at time $n$. The probabilities of transitioning between weather states are all strictly between 0 and 1.\n\nAt each time step, from its current position $V_n$, the car moves exactly one unit, either to the right ($V_{n+1} = V_n + 1$) or to the left ($V_{n+1} = V_n - 1$). The probability of moving right depends on the weather at that time step, $W_n$:\n- If $W_n = \\text{Sunny}$, the probability of moving right is $p_S$.\n- If $W_n = \\text{Rainy}$, the probability of moving right is $p_R$.\n\nAssume that $0 < p_S < 1$, $0 < p_R < 1$, and importantly, $p_S \\neq p_R$. The evolution of the weather does not depend on the car's position or movement history.\n\nA stochastic process $\\{X_n\\}$ is defined as a Markov chain if for all time steps $n$ and all possible states, the conditional probability of the next state $X_{n+1}$ given the entire history of the process $\\{X_0, X_1, \\dots, X_n\\}$ depends only on the present state $X_n$.\n\nConsider the following statements:\nI. The process of the car's position, $\\{V_n\\}_{n \\ge 0}$, is a Markov chain.\nII. The process of the weather, $\\{W_n\\}_{n \\ge 0}$, is a Markov chain.\nIII. The process of the combined state of position and weather, $\\{(V_n, W_n)\\}_{n \\ge 0}$, is a Markov chain.\n\nWhich of the above statements is/are true?\n\nA. II only\n\nB. III only\n\nC. I and II only\n\nD. I and III only\n\nE. II and III only\n\nF. I, II, and III\n\nG. None of the statements are true.", "solution": "We analyze each statement against the Markov property definition: for a process $\\{X_n\\}$ to be a Markov chain, it must satisfy for all $n$ and all states,\n$$\n\\mathbb{P}(X_{n+1} \\in A \\mid X_0, X_1, \\dots, X_n) = \\mathbb{P}(X_{n+1} \\in A \\mid X_n).\n$$\n\nFirst, the weather process $\\{W_n\\}$ is Markov by construction: it is specified that the weather at time $n+1$ depends probabilistically only on the weather at time $n$, with transition probabilities strictly between $0$ and $1$. Therefore, for all $n$,\n$$\n\\mathbb{P}(W_{n+1}=w' \\mid W_0, W_1, \\dots, W_n) = \\mathbb{P}(W_{n+1}=w' \\mid W_n),\n$$\nso statement II is true.\n\nSecond, consider the joint process $\\{(V_n, W_n)\\}$. Given $(V_n, W_n)$, the next pair $(V_{n+1}, W_{n+1})$ is determined as follows:\n- $W_{n+1}$ depends only on $W_n$ by the weatherâ€™s Markov property.\n- $V_{n+1}$ depends on $(V_n, W_n)$ via\n$$\n\\mathbb{P}(V_{n+1}=V_n+1 \\mid V_n, W_n) =\n\\begin{cases}\np_S, & W_n=\\text{Sunny},\\\\\np_R, & W_n=\\text{Rainy},\n\\end{cases}\n$$\nand $\\mathbb{P}(V_{n+1}=V_n-1 \\mid V_n, W_n)=1$ minus the above.\nThus, for any history,\n$$\n\\mathbb{P}\\big((V_{n+1}, W_{n+1}) \\in B \\mid (V_0, W_0), \\dots, (V_n, W_n)\\big)\n$$\ndepends only on $(V_n, W_n)$, so $\\{(V_n, W_n)\\}$ is a Markov chain and statement III is true.\n\nThird, we examine whether $\\{V_n\\}$ alone is Markov. Define $\\Delta_n := V_{n+1}-V_n \\in \\{-1, 1\\}$. Then for any history of positions,\n$$\n\\mathbb{P}(\\Delta_n=1 \\mid V_0, V_1, \\dots, V_n)\n= \\mathbb{E}\\big[ \\mathbb{P}(\\Delta_n=1 \\mid V_0, \\dots, V_n, W_n) \\mid V_0, \\dots, V_n \\big]\n= p_S \\,\\mathbb{P}(W_n=\\text{Sunny} \\mid V_0, \\dots, V_n) + p_R \\,\\mathbb{P}(W_n=\\text{Rainy} \\mid V_0, \\dots, V_n).\n$$\nBecause $p_S \\neq p_R$, the term above depends on the posterior $\\mathbb{P}(W_n=\\text{Sunny} \\mid V_0, \\dots, V_n)$. Under the given model, the positions up to time $n$ are generated through the sequence of increments $\\Delta_0, \\dots, \\Delta_{n-1}$, with each $\\Delta_k$ depending on $W_k$. Since the weather is a Markov chain, $W_n$ depends on $W_{n-1}$, which in turn is statistically informed by the observed past increments (and thus the full position history), not merely the current position $V_n$. Consequently, the filtered probability $\\mathbb{P}(W_n=\\text{Sunny} \\mid V_0, \\dots, V_n)$ is, in general, a function of the entire history and not a function solely of $V_n$. Therefore,\n$$\n\\mathbb{P}(V_{n+1}=V_n+1 \\mid V_0, \\dots, V_n)\n$$\ndepends on the full past through that posterior and cannot, in general, be reduced to a function of $V_n$ alone. This violates the Markov property for $\\{V_n\\}$, so statement I is false in general.\n\nA concrete two-step illustration makes the dependence explicit. Let $n=1$ and consider two histories that both end at $V_1=1$ but differ in $V_0$:\n- If $(V_0, V_1)=(0,1)$, then $\\Delta_0=+1$; this makes $W_0=\\text{Sunny}$ more likely than $W_0=\\text{Rainy}$ when $p_S>p_R$, and less likely when $p_S<p_R$. Because $W_1$ depends on $W_0$ through the weather transition matrix, this shifts $\\mathbb{P}(W_1=\\text{Sunny} \\mid V_0, V_1)$ relative to its prior.\n- If $(V_0, V_1)=(2,1)$, then $\\Delta_0=-1$, producing the opposite shift.\nHence\n$$\n\\mathbb{P}(V_2=2 \\mid V_1=1, V_0=0) \\neq \\mathbb{P}(V_2=2 \\mid V_1=1, V_0=2)\n$$\nwhenever $p_S \\neq p_R$ and the weather transition law depends on the current weather state, demonstrating that $\\{V_n\\}$ is not Markov.\n\nIn summary: II is true; III is true; I is false under the model as stated (with $p_S \\neq p_R$). Therefore, the correct choice is II and III only.", "answer": "$$\\boxed{E}$$", "id": "1295253"}]}