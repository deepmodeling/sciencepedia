{
    "hands_on_practices": [
        {
            "introduction": "Efficiently modeling radiative transfer is fundamental to atmospheric retrieval, as direct line-by-line calculations are often computationally prohibitive. This exercise guides you through the implementation of the correlated-$k$ method, a powerful technique for accelerating the calculation of band-averaged transmittance. By comparing the Random Overlap and Resort-and-Rebin mixing strategies, you will gain hands-on experience with the trade-offs between computational simplicity and physical accuracy when modeling gas mixtures .",
            "id": "4153639",
            "problem": "Implement a program that computes band-averaged transmittance for absorbing gases using the correlated-$k$ method and two $k$-mixing strategies. The derivation must start from the Beer–Lambert law and the definition of the $k$-distribution. The numerical task is to implement these ideas on a prescribed discrete quadrature, compute the band-averaged transmittances for a set of test cases, and output the results in a machine-checked format.\n\nThe only permitted physical principle you may use is the Beer–Lambert law for monochromatic transmittance along a path, and the definition of the band-averaged correlated-$k$ formulation. The Beer–Lambert law states that the monochromatic transmittance $T(\\nu)$ through a path with total optical depth is\n$$\nT(\\nu) = \\exp\\!\\left(-\\sum_{s} k_{s}(\\nu)\\,u_{s}\\right),\n$$\nwhere $k_{s}(\\nu)$ is the mass absorption coefficient of species $s$ at wavenumber $\\nu$ in units of $m^2\\,\\text{kg}^{-1}$, and $u_{s}$ is the mass path of species $s$ in units of $\\text{kg}\\,\\text{m}^{-2}$. The correlated-$k$ method replaces the spectral average over $\\nu$ by an average over the cumulative probability variable $g$ of the sorted absorption coefficient. On a discrete quadrature $\\{(w_{i},k_{i})\\}_{i=1}^{N}$ for a single species, the band-averaged transmittance for that species at mass path $u$ is approximated by\n$$\n\\langle T \\rangle \\approx \\sum_{i=1}^{N} w_{i}\\,\\exp(-k_{i}\\,u),\n$$\nwith $\\sum_{i=1}^{N} w_{i} = 1$.\n\nYou will implement and compare two mixing strategies for multiple absorbing species:\n\n- Random Overlap (RO): Assume the spectral $k$-distributions of different species are independent. Under this assumption, the band-mean of the product equals the product of the band-means, yielding\n$$\n\\langle T \\rangle_{\\mathrm{RO}} = \\prod_{s} \\left(\\sum_{i=1}^{N_{s}} w_{s,i}\\,\\exp(-k_{s,i}\\,u_{s})\\right).\n$$\n\n- Resort-and-Rebin (RnR): Construct the mixed $k$-distribution by discrete convolution of optical depths, followed by sorting and rebinning:\n  1. Form all combinations of indices across species. For each combination, compute the combined optical depth $\\tau = \\sum_{s} k_{s,i_{s}}\\,u_{s}$ and the combined weight $w = \\prod_{s} w_{s,i_{s}}$. If the total mass path $u_{\\text{tot}} = \\sum_{s} u_{s}$ is nonzero, define the combined mass absorption coefficient $k_{\\text{comb}} = \\tau / u_{\\text{tot}}$; if $u_{\\text{tot}} = 0$, define the transmittance to be $1$ by convention.\n  2. Sort the list $\\{(w,k_{\\text{comb}})\\}$ by $k_{\\text{comb}}$ in ascending order.\n  3. Rebin the sorted list into $N_{\\text{out}}$ equal-weight bins in $g$-space. Within each bin $b$, compute the arithmetic weighted mean\n     $$\n     \\bar{k}_{b} = \\frac{1}{W_{b}}\\sum_{j \\in b} w_{j}\\,k_{\\mathrm{comb},j},\n     $$\n     where $W_{b}$ is the total weight in bin $b$ (equal to $1/N_{\\text{out}}$ if the initial weights sum to $1$). Use linear splitting of an entry if a bin boundary cuts through it.\n  4. Approximate the band-averaged transmittance as\n     $$ \n     \\langle T \\rangle_{\\mathrm{RnR}} \\approx \\sum_{b=1}^{N_{\\mathrm{out}}} W_{b}\\,\\exp(-\\bar{k}_{b}\\,u_{\\mathrm{tot}}).\n     $$\n\nAll quantities must be handled in the specified units: $k$ in $m^2\\,\\text{kg}^{-1}$, $u$ in $\\text{kg}\\,\\text{m}^{-2}$, so that optical depth $\\tau$ is dimensionless and transmittance is dimensionless. Angles are not used. No percentages are used; all ratios must be decimals.\n\nImplement a program that:\n\n- Defines three species with fixed correlated-$k$ quadratures at a single band and thermodynamic state:\n  - Species A: $k_{\\mathrm{A}} = [\\,10^{-2},\\,5\\times 10^{-2},\\,2\\times 10^{-1},\\,1.0\\,]$ with weights $w_{\\mathrm{A}} = [\\,0.1,\\,0.2,\\,0.3,\\,0.4\\,]$.\n  - Species B: $k_{\\mathrm{B}} = [\\,5\\times 10^{-3},\\,2\\times 10^{-2},\\,8\\times 10^{-2},\\,3\\times 10^{-1}\\,]$ with weights $w_{\\mathrm{B}} = [\\,0.25,\\,0.25,\\,0.25,\\,0.25\\,]$.\n  - Species C: $k_{\\mathrm{C}} = [\\,10^{-4},\\,2\\times 10^{-3},\\,10^{-2},\\,5\\times 10^{-2}\\,]$ with weights $w_{\\mathrm{C}} = [\\,0.2,\\,0.3,\\,0.3,\\,0.2\\,]$.\n  All $k$ are in $\\mathrm{m^{2}\\,kg^{-1}}$ and all weights are dimensionless and sum to $1$ for each species.\n\n- Implements functions to compute:\n  1. Single-species band-mean transmittance $\\sum_{i} w_{i}\\exp(-k_{i}u)$,\n  2. Random Overlap (RO) transmittance for a set of species at given mass paths,\n  3. Resort-and-Rebin (RnR) transmittance for a set of species at given mass paths and a given integer $N_{\\text{out}} \\ge 1$.\n\n- Evaluates the following test suite of $5$ cases. For each case, compute two floats: the RO transmittance and the absolute error of RnR relative to RO, both dimensionless:\n  1. Case $1$ (single-species baseline): species A only with $u_{\\mathrm{A}} = 1.0$, $N_{\\mathrm{out}} = 3$.\n  2. Case $2$ (two-species moderate path): species A and B with $u_{\\mathrm{A}} = 1.0$, $u_{\\mathrm{B}} = 0.5$, $N_{\\mathrm{out}} = 4$.\n  3. Case $3$ (edge case with zero path for one species): species A and B with $u_{\\mathrm{A}} = 0.8$, $u_{\\mathrm{B}} = 0.0$, $N_{\\mathrm{out}} = 4$.\n  4. Case $4$ (optically thick): species A and B with $u_{\\mathrm{A}} = 4.0$, $u_{\\mathrm{B}} = 3.0$, $N_{\\mathrm{out}} = 6$.\n  5. Case $5$ (three-species mix): species A, B, and C with $u_{\\mathrm{A}} = 1.5$, $u_{\\mathrm{B}} = 0.7$, $u_{\\mathrm{C}} = 0.2$, $N_{\\mathrm{out}} = 6$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $10$ floats in the following order:\n$$\n[\\;T_{\\mathrm{RO},1},\\;|T_{\\mathrm{RnR},1}-T_{\\mathrm{RO},1}|\\;,\\;T_{\\mathrm{RO},2},\\;|T_{\\mathrm{RnR},2}-T_{\\mathrm{RO},2}|\\;,\\;\\dots\\;,\\;T_{\\mathrm{RO},5},\\;|T_{\\mathrm{RnR},5}-T_{\\mathrm{RO},5}|\\;],\n$$\nrounded to $8$ decimal places. Transmittances are dimensionless. The program must not read any input and must run as-is.",
            "solution": "The objective is to compute the band-averaged transmittance for a mixture of absorbing gases using two different $k$-mixing strategies: Random Overlap (RO) and Resort-and-Rebin (RnR). The implementation will be validated against a specified set of test cases. The derivation and implementation are based entirely on the provided physical laws and definitions.\n\n### Principles and Derivations\n\n#### The Beer–Lambert Law and Correlated-$k$ Method\n\nThe foundational principle is the Beer–Lambert law, which describes the monochromatic transmittance $T(\\nu)$ at a specific wavenumber $\\nu$ through a medium. For a mixture of absorbing species, the total optical depth is the sum of the optical depths of each species. The transmittance is given by:\n$$\nT(\\nu) = \\exp(-\\tau(\\nu)) = \\exp\\!\\left(-\\sum_{s} k_{s}(\\nu)\\,u_{s}\\right)\n$$\nHere, $k_s(\\nu)$ is the mass absorption coefficient of species $s$ and $u_s$ is its mass path (column density).\n\nDirectly integrating $T(\\nu)$ over a spectral band can be computationally intensive due to the highly fluctuating nature of $k_s(\\nu)$. The correlated-$k$ method simplifies this by re-sorting the absorption spectrum. Instead of integrating over wavenumber $\\nu$, we integrate over the cumulative distribution function, $g$, of the absorption coefficient $k$. For a single species, the band-averaged transmittance $\\langle T \\rangle$ is approximated using a discrete quadrature on $g$-space. This quadrature consists of $N$ pairs of weights $w_i$ and corresponding absorption coefficients $k_i$, such that $\\sum_{i=1}^{N} w_i = 1$. The band-averaged transmittance is then:\n$$\n\\langle T \\rangle \\approx \\sum_{i=1}^{N} w_{i}\\,\\exp(-k_{i}\\,u)\n$$\nThis formula provides the basis for a function to compute single-species transmittance.\n\n#### Mixing Strategies for Multiple Species\n\nWhen multiple absorbing gases are present, their combined effect must be calculated. We implement two common strategies.\n\n**1. Random Overlap (RO) Strategy**\n\nThe Random Overlap model assumes that the absorption lines of different species are randomly and independently positioned with respect to one another within the spectral band. This statistical independence allows the band-averaged transmittance of the mixture to be calculated as the product of the individual band-averaged transmittances of each species:\n$$\n\\langle T \\rangle_{\\mathrm{RO}} = \\left\\langle \\prod_{s} \\exp(-k_s(\\nu) u_s) \\right\\rangle\n$$\nUnder the assumption of independence, the average of the product is the product of the averages:\n$$\n\\langle T \\rangle_{\\mathrm{RO}} = \\prod_{s} \\langle \\exp(-k_s(\\nu) u_s) \\rangle = \\prod_{s} \\langle T_s \\rangle\n$$\nUsing the discrete correlated-$k$ approximation for each species $s$ with its own quadrature set $\\{ (w_{s,i}, k_{s,i}) \\}_{i=1}^{N_s}$, the formula becomes:\n$$\n\\langle T \\rangle_{\\mathrm{RO}} = \\prod_{s} \\left(\\sum_{i=1}^{N_{s}} w_{s,i}\\,\\exp(-k_{s,i}\\,u_{s})\\right)\n$$\nThis method is computationally efficient, as it only requires computing the transmittance for each species independently and then multiplying the results.\n\n**2. Resort-and-Rebin (RnR) Strategy**\n\nThe Resort-and-Rebin method provides a more physically correlated treatment of the overlap. It involves creating a new, combined $k$-distribution for the gas mixture, which is then used to compute the total transmittance. The procedure involves four steps:\n\n**Step 1: Construct the Combined High-Resolution $k$-Distribution**\nAssuming statistical independence in the $g$-space of the individual species, we can form all possible combinations of their quadrature points. For a set of species $\\{s\\}$, each with quadrature points indexed by $i_s$, a specific combination corresponds to a combined weight $w$ and a combined optical depth $\\tau$:\n$$\nw = \\prod_{s} w_{s,i_{s}} \\quad \\text{and} \\quad \\tau = \\sum_{s} k_{s,i_{s}}\\,u_{s}\n$$\nThe product of weights gives the joint probability of this combination occurring. If the total mass path $u_{\\text{tot}} = \\sum_{s} u_s$ is non-zero, an effective mass absorption coefficient for the combination is defined as $k_{\\text{comb}} = \\tau / u_{\\text{tot}}$. This process generates a high-resolution $k$-distribution, typically with $\\prod_s N_s$ points. If $u_{\\text{tot}} = 0$, the transmittance is $1$ by definition, as there is no absorbing medium.\n\n**Step 2: Sort the Combined Distribution**\nThe generated list of pairs $(w, k_{\\text{comb}})$ is sorted in ascending order of $k_{\\text{comb}}$. This re-establishes the monotonic relationship between the absorption coefficient and its cumulative probability $g$, creating a well-defined, albeit high-resolution, $k$-distribution for the mixture.\n\n**Step 3: Rebin to a Coarser Quadrature**\nTo maintain computational efficiency, the sorted high-resolution distribution is rebinned onto a coarser quadrature with a desired number of points, $N_{\\text{out}}$. The $g$-space (cumulative weight, from $0$ to $1$) is divided into $N_{\\text{out}}$ equal-width bins, each of width $W_b = 1/N_{\\text{out}}$. For each new bin $b$, the representative absorption coefficient $\\bar{k}_b$ is computed as the weighted arithmetic mean of all $k_{\\text{comb}, j}$ values (or parts of values) that fall into that bin:\n$$\n\\bar{k}_{b} = \\frac{1}{W_{b}}\\sum_{j \\in b} w_{j}\\,k_{\\mathrm{comb},j}\n$$\nIf an original sorted entry $(w_j, k_{\\text{comb},j})$ straddles a bin boundary, its weight $w_j$ is split linearly between the adjacent bins, in proportion to how much of its $g$-space interval falls into each bin.\n\n**Step 4: Compute Final Transmittance**\nThe band-averaged transmittance is then calculated using the new, rebinned quadrature $\\{ (W_b, \\bar{k}_b) \\}_{b=1}^{N_{\\text{out}}}$ and the total mass path $u_{\\text{tot}}$:\n$$\n\\langle T \\rangle_{\\mathrm{RnR}} \\approx \\sum_{b=1}^{N_{\\mathrm{out}}} W_{b}\\,\\exp(-\\bar{k}_{b}\\,u_{\\mathrm{tot}})\n$$\nSince each bin has equal weight $W_b = 1/N_{\\text{out}}$, this simplifies to:\n$$\n\\langle T \\rangle_{\\mathrm{RnR}} \\approx \\frac{1}{N_{\\mathrm{out}}} \\sum_{b=1}^{N_{\\mathrm{out}}} \\exp(-\\bar{k}_{b}\\,u_{\\mathrm{tot}})\n$$\nThis approach, while more computationally involved than RO, often provides a more accurate representation of the overlapping absorption features. The absolute error $|T_{\\mathrm{RnR}} - T_{\\mathrm{RO}}|$ serves as a metric to quantify the difference between these two physical assumptions for a given scenario.",
            "answer": "```python\nimport numpy as np\nfrom itertools import product\nfrom typing import List, Dict, Tuple, Union\n\n# Meticulously and exactly implement the validation and solution as required.\n\ndef solve():\n    \"\"\"\n    Main function to define species, test cases, and compute results.\n    \"\"\"\n\n    # Define the fixed correlated-k quadratures for the three species.\n    # k-values are in m^2/kg, weights are dimensionless.\n    species_db = {\n        'A': {\n            'k': np.array([1.0e-2, 5.0e-2, 2.0e-1, 1.0]),\n            'w': np.array([0.1, 0.2, 0.3, 0.4])\n        },\n        'B': {\n            'k': np.array([5.0e-3, 2.0e-2, 8.0e-2, 3.0e-1]),\n            'w': np.array([0.25, 0.25, 0.25, 0.25])\n        },\n        'C': {\n            'k': np.array([1.0e-4, 2.0e-3, 1.0e-2, 5.0e-2]),\n            'w': np.array([0.2, 0.3, 0.3, 0.2])\n        }\n    }\n\n    # Define the 5 test cases from the problem statement.\n    test_cases = [\n        {'species_ids': ['A'], 'mass_paths': [1.0], 'n_out': 3},\n        {'species_ids': ['A', 'B'], 'mass_paths': [1.0, 0.5], 'n_out': 4},\n        {'species_ids': ['A', 'B'], 'mass_paths': [0.8, 0.0], 'n_out': 4},\n        {'species_ids': ['A', 'B'], 'mass_paths': [4.0, 3.0], 'n_out': 6},\n        {'species_ids': ['A', 'B', 'C'], 'mass_paths': [1.5, 0.7, 0.2], 'n_out': 6},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        species_info = [(sid, species_db[sid], u) for sid, u in zip(case['species_ids'], case['mass_paths'])]\n        \n        # Compute Random Overlap (RO) transmittance\n        t_ro = compute_ro_transmittance(species_info)\n        \n        # Compute Resort-and-Rebin (RnR) transmittance\n        t_rnr = compute_rnr_transmittance(species_info, case['n_out'])\n\n        # Calculate absolute error\n        error = abs(t_rnr - t_ro)\n\n        results.extend([t_ro, error])\n\n    # Format the final output string as specified\n    formatted_results = [f\"{val:.8f}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef compute_single_species_transmittance(k: np.ndarray, w: np.ndarray, u: float) -> float:\n    \"\"\"\n    Computes band-mean transmittance for a single species.\n    T = sum(w_i * exp(-k_i * u))\n    \"\"\"\n    if u == 0.0:\n        return 1.0\n    return np.sum(w * np.exp(-k * u))\n\ndef compute_ro_transmittance(species_info: List[Tuple[str, Dict[str, np.ndarray], float]]) -> float:\n    \"\"\"\n    Computes band-mean transmittance using the Random Overlap (RO) method.\n    T_ro = product(T_s for s in species)\n    \"\"\"\n    total_transmittance = 1.0\n    for _, spec_data, u in species_info:\n        total_transmittance *= compute_single_species_transmittance(spec_data['k'], spec_data['w'], u)\n    return total_transmittance\n\ndef compute_rnr_transmittance(species_info: List[Tuple[str, Dict[str, np.ndarray], float]], n_out: int) -> float:\n    \"\"\"\n    Computes band-mean transmittance using the Resort-and-Rebin (RnR) method.\n    \"\"\"\n    mass_paths = np.array([info[2] for info in species_info])\n    u_total = np.sum(mass_paths)\n\n    if u_total == 0.0:\n        return 1.0\n\n    # Step 1: Construct the combined high-resolution k-distribution\n    # List of (k_array, w_array) for each species\n    k_sets = [info[1]['k'] for info in species_info]\n    w_sets = [info[1]['w'] for info in species_info]\n    \n    # Generate all index combinations using itertools.product\n    index_ranges = [range(len(k_set)) for k_set in k_sets]\n    index_combinations = product(*index_ranges)\n\n    combined_dist = []\n    for indices in index_combinations:\n        # Combined weight is the product of individual weights\n        w_comb = np.prod([w_sets[s][indices[s]] for s in range(len(species_info))])\n        \n        # Combined optical depth is the sum of individual optical depths\n        tau_comb = np.sum([k_sets[s][indices[s]] * mass_paths[s] for s in range(len(species_info))])\n        \n        k_comb = tau_comb / u_total\n        combined_dist.append((k_comb, w_comb))\n\n    # Step 2: Sort the combined distribution by k_comb\n    combined_dist.sort(key=lambda x: x[0])\n    \n    # Step 3: Rebin to a coarser quadrature\n    sorted_k, sorted_w = zip(*combined_dist)\n    \n    k_out_binned = rebin_k_distribution(np.array(sorted_k), np.array(sorted_w), n_out)\n    \n    # Step 4: Compute final transmittance\n    w_b = 1.0 / n_out\n    t_rnr = np.sum(w_b * np.exp(-k_out_binned * u_total))\n    \n    return t_rnr\n\ndef rebin_k_distribution(sorted_k: np.ndarray, sorted_w: np.ndarray, n_out: int) -> np.ndarray:\n    \"\"\"\n    Rebins a sorted k-distribution into n_out equal-weight bins with linear splitting.\n    \"\"\"\n    k_out_weighted_sum = np.zeros(n_out)\n    bin_width = 1.0 / n_out\n    \n    g_cursor = 0.0          # Tracks the cumulative weight (g-space)\n    bin_idx = 0             # Current output bin index\n    \n    # Epsilon for robust floating-point comparisons\n    epsilon = 1e-12\n\n    for k_entry, w_entry in zip(sorted_k, sorted_w):\n        # Ignore entries with negligible weight\n        if w_entry < epsilon:\n            continue\n            \n        # g-space interval for the current entry\n        g_entry_start = g_cursor\n        g_entry_end = g_cursor + w_entry\n        \n        # This loop distributes the weight of the current entry across one or more bins\n        g_rem_start = g_entry_start # Start of remaining weight from this entry\n        while g_rem_start < g_entry_end - epsilon:\n            # g-space end of the current bin\n            g_bin_end = (bin_idx + 1) * bin_width\n            \n            # Weight from this entry that falls into the current bin\n            dw = min(g_entry_end, g_bin_end) - g_rem_start\n\n            if dw > 0:\n                k_out_weighted_sum[bin_idx] += dw * k_entry\n            \n            g_rem_start += dw\n            \n            # If we've processed up to the bin boundary, move to the next bin\n            if g_rem_start >= g_bin_end - epsilon:\n                bin_idx += 1\n                if bin_idx >= n_out : # Clamp to last bin to handle potential float issues at g=1.0\n                    bin_idx = n_out - 1\n\n        g_cursor = g_entry_end\n        \n    # Normalize to get the average k for each bin\n    k_b_averaged = k_out_weighted_sum / bin_width\n    \n    return k_b_averaged\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Moving from a forward model to an inverse problem requires a rigorous statistical framework connecting data to model parameters. A crucial step is constructing a likelihood function that realistically accounts for multiple sources of uncertainty, including not just measurement noise but also imperfections in the model itself. This exercise challenges you to derive and apply a marginal likelihood that analytically integrates over uncertainties in line-list data and other forms of model inadequacy, demonstrating how to build a more robust basis for parameter estimation .",
            "id": "4153640",
            "problem": "Consider an exoplanet transmission spectroscopy setting in which the observed differential transit depth at a small set of wavelengths is modeled in a linearized regime. Begin from the Beer–Lambert law for attenuation, which states that the optical depth at wavelength $\\lambda$, denoted by $\\tau(\\lambda)$, satisfies $\\tau(\\lambda) = \\int \\rho(l) \\kappa(\\lambda, l) \\, dl$, where $\\rho(l)$ is the absorber density and $\\kappa(\\lambda, l)$ is the absorption coefficient along the path $l$. In the optically thin limit, the transmittance is approximated by $T(\\lambda) \\approx 1 - \\tau(\\lambda)$, and the measured residual (relative to a baseline level) is proportional to $\\tau(\\lambda)$. Model the wavelength-dependent residual as a linear combination of predetermined line-shape basis functions and their strengths.\n\nAssume there are $N_\\lambda$ wavelength samples and $N_{\\text{lines}}$ known spectral lines. Let the matrix $G \\in \\mathbb{R}^{N_\\lambda \\times N_{\\text{lines}}}$ contain the line-shape basis functions evaluated at the wavelengths, the vector of nominal line strengths be $s_0 \\in \\mathbb{R}^{N_{\\text{lines}}}$, the unknown mixing ratio be $q \\in \\mathbb{R}_{\\ge 0}$, and the unknown additive baseline offset be $b \\in \\mathbb{R}$. The measured data vector is $y \\in \\mathbb{R}^{N_\\lambda}$. Under a linearized forward model consistent with the optically thin limit, the residuals satisfy\n$$y = b \\mathbf{1} + q G s_0 + n + \\delta + q G \\epsilon,$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{N_\\lambda}$ is the vector of ones, $n \\in \\mathbb{R}^{N_\\lambda}$ is measurement noise, $\\delta \\in \\mathbb{R}^{N_\\lambda}$ represents model inadequacy, and $\\epsilon \\in \\mathbb{R}^{N_{\\text{lines}}}$ captures line-strength errors relative to $s_0$. Assume the following probabilistic structure:\n- Measurement noise is Independent and Identically Distributed (IID) Gaussian: $n \\sim \\mathcal{N}(0, \\sigma_n^2 I)$.\n- Line-strength errors are Gaussian with nominal mean: $\\epsilon \\sim \\mathcal{N}(0, \\sigma_s^2 I)$.\n- Model inadequacy is approximated as IID Gaussian: $\\delta \\sim \\mathcal{N}(0, \\sigma_\\delta^2 I)$.\n\nFrom these assumptions and the linear model, derive the exact Gaussian marginal likelihood for $y$ conditioned on $q$ and $b$ by analytically integrating out both $\\epsilon$ and $\\delta$. Your derivation must start from the Beer–Lambert law, the optically thin approximation, the linear forward model stated above, and the specified Gaussian priors, and proceed to the marginal distribution $p(y \\mid q, b)$ without introducing any shortcut formulas. Explicitly characterize the mean and covariance of the resulting Gaussian distribution in terms of $G, s_0, \\sigma_n, \\sigma_s, \\sigma_\\delta$, and $q$.\n\nUsing the derived marginal likelihood, obtain the Maximum Likelihood Estimate (MLE) of the baseline $b$ for a fixed $q$, and then construct the profile log-marginal-likelihood as a function of $q$ alone by substituting the $b$-MLE. Finally, compute the $q$ and $b$ that maximize the marginal likelihood. All computations must be done in exact arithmetic with no approximations beyond those stated.\n\nImplement a program that, for each test case, computes:\n- The MLE mixing ratio $\\hat{q}$ (dimensionless).\n- The corresponding MLE baseline $\\hat{b}$ (dimensionless).\n- The maximized log marginal likelihood $\\log \\mathcal{L}(\\hat{q}, \\hat{b})$ (dimensionless).\n\nAll quantities are dimensionless, and angles are not involved. Express the final results as decimal floats. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\hat{q}_1,\\hat{b}_1,\\log \\mathcal{L}_1,\\hat{q}_2,\\hat{b}_2,\\log \\mathcal{L}_2,\\hat{q}_3,\\hat{b}_3,\\log \\mathcal{L}_3]$.\n\nUse the following test suite of parameter values to ensure coverage of a typical case, a boundary case with no line-list uncertainty, and an edge case with dominant model inadequacy. The same line-shape matrix $G$ and nominal strengths $s_0$ are used across cases.\n\nLet $N_\\lambda = 5$, $N_{\\text{lines}} = 2$, and\n$$\nG = \\begin{bmatrix}\n0.8 & 0.0 \\\\\n0.6 & 0.1 \\\\\n0.3 & 0.3 \\\\\n0.1 & 0.7 \\\\\n0.0 & 0.9\n\\end{bmatrix}, \\quad\ns_0 = \\begin{bmatrix} 1.0 \\\\ 0.7 \\end{bmatrix}.\n$$\n\nCompute the data vectors $y$ deterministically for each case using the forward model components specified. The program must construct $y$ as indicated below, and then perform retrieval under the stated uncertainty parameters.\n\n- Case $1$ (happy path): Use $q_{\\text{true}} = 0.6$, $b_{\\text{true}} = 0.001$, $s_{\\text{true}} = s_0 + [0.2, -0.1]^T$, measurement noise $n = [0.0002, -0.0001, 0.0, 0.00015, -0.00005]^T$, and model inadequacy $\\delta = [0.0, 0.0, 0.0, 0.0, 0.0]^T$. Form $y = b_{\\text{true}} \\mathbf{1} + q_{\\text{true}} G s_{\\text{true}} + n + \\delta$. Retrieval assumptions: $\\sigma_n = 0.0003$, $\\sigma_s = 0.15$, $\\sigma_\\delta = 0.0002$.\n\n- Case $2$ (boundary, no line-list uncertainty): Use the same $y$ as in Case $1$. Retrieval assumptions: $\\sigma_n = 0.0003$, $\\sigma_s = 0.0$, $\\sigma_\\delta = 0.0002$.\n\n- Case $3$ (edge, dominant model inadequacy): Use $q_{\\text{true}} = 0.5$, $b_{\\text{true}} = 0.0$, $s_{\\text{true}} = s_0$, measurement noise $n = [0.0, 0.0, 0.0, 0.0, 0.0]^T$, and model inadequacy $\\delta = [0.001, -0.0005, 0.0008, -0.0012, 0.0004]^T$. Form $y = b_{\\text{true}} \\mathbf{1} + q_{\\text{true}} G s_{\\text{true}} + n + \\delta$. Retrieval assumptions: $\\sigma_n = 0.0001$, $\\sigma_s = 0.2$, $\\sigma_\\delta = 0.001$.\n\nYour program must:\n- Implement the exact Gaussian marginalization over line-strength errors $\\epsilon$ and model inadequacy $\\delta$.\n- Use a numerically stable method to evaluate the log marginal likelihood (for example, via Cholesky factorization).\n- Maximize the profile log marginal likelihood over $q \\in [0, 10]$ and compute the corresponding $\\hat{b}$ via its MLE for that $\\hat{q}$.\n- Produce the final single-line output in the exact required format.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of spectroscopy and statistical inference, is well-posed with a clear objective and sufficient data, and is free of ambiguities or contradictions.\n\nThe solution proceeds by first deriving the marginal likelihood as requested, then finding the maximum likelihood estimates for the unknown parameters.\n\n### 1. Derivation of the Marginal Likelihood $p(y \\mid q, b)$\n\nThe analysis begins with the provided linear forward model:\n$$y = b \\mathbf{1} + q G s_0 + n + \\delta + q G \\epsilon$$\nHere, $y$ is the data vector, $b$ and $q$ are the parameters of interest, and $n$, $\\delta$, and $\\epsilon$ are random variables representing different sources of uncertainty. To derive the likelihood of the data $p(y \\mid q, b)$, we must integrate out, or marginalize over, the nuisance random variables $\\epsilon$ and $\\delta$.\n\nWe can rearrange the model equation to group the deterministic and random terms. Let the deterministic mean model be $\\mu(q, b) = b \\mathbf{1} + q G s_0$. The equation becomes:\n$$y - \\mu(q, b) = n + \\delta + q G \\epsilon$$\nThe right-hand side is a composite noise term, which is a linear combination of the independent random vectors $n$, $\\delta$, and $\\epsilon$. A fundamental property of Gaussian distributions is that a linear transformation of a Gaussian random vector is also a Gaussian random vector. Furthermore, the sum of independent Gaussian random vectors is a Gaussian random vector.\n\nThe individual random vectors have the following distributions:\n- Measurement noise: $n \\sim \\mathcal{N}(0, \\sigma_n^2 I_{N_\\lambda})$\n- Model inadequacy: $\\delta \\sim \\mathcal{N}(0, \\sigma_\\delta^2 I_{N_\\lambda})$\n- Line-strength errors: $\\epsilon \\sim \\mathcal{N}(0, \\sigma_s^2 I_{N_{\\text{lines}}})$\n\nLet the composite noise term be $v = n + \\delta + q G \\epsilon$. The distribution of $v$ is Gaussian. We determine its mean and covariance.\n\nThe mean of $v$ is the sum of the means of its components:\n$$\\mathbb{E}[v] = \\mathbb{E}[n] + \\mathbb{E}[\\delta] + \\mathbb{E}[q G \\epsilon]$$\nSince $\\mathbb{E}[n] = 0$, $\\mathbb{E}[\\delta] = 0$, and $\\mathbb{E}[\\epsilon] = 0$, we have:\n$$\\mathbb{E}[v] = 0 + 0 + q G \\mathbb{E}[\\epsilon] = 0$$\n\nThe covariance of $v$, which we denote $C_y(q)$, is the sum of the covariances of its independent components. For a random vector $X$ and a matrix $A$, $\\text{Cov}(AX) = A \\text{Cov}(X) A^T$.\n$$\\text{Cov}(v) = C_y(q) = \\text{Cov}(n) + \\text{Cov}(\\delta) + \\text{Cov}(q G \\epsilon)$$\n$$C_y(q) = \\sigma_n^2 I_{N_\\lambda} + \\sigma_\\delta^2 I_{N_\\lambda} + (qG) \\text{Cov}(\\epsilon) (qG)^T$$\n$$C_y(q) = (\\sigma_n^2 + \\sigma_\\delta^2) I_{N_\\lambda} + q^2 \\sigma_s^2 G I_{N_{\\text{lines}}} G^T$$\n$$C_y(q) = (\\sigma_n^2 + \\sigma_\\delta^2) I_{N_\\lambda} + q^2 \\sigma_s^2 G G^T$$\nThe covariance matrix $C_y$ depends on the mixing ratio $q$ but not on the baseline $b$.\n\nSince $y - \\mu(q, b) = v$, it follows that $y$ conditioned on $q$ and $b$ is a multivariate Gaussian random variable with mean $\\mu(q, b)$ and covariance $C_y(q)$:\n$$y \\mid q, b \\sim \\mathcal{N}(\\mu(q, b), C_y(q))$$\nThe probability density function for this distribution is the desired marginal likelihood, $\\mathcal{L}(q, b) \\equiv p(y \\mid q, b)$:\n$$\\mathcal{L}(q, b) = \\frac{1}{\\sqrt{(2\\pi)^{N_\\lambda} \\det(C_y(q))}} \\exp\\left(-\\frac{1}{2} (y - \\mu(q, b))^T C_y(q)^{-1} (y - \\mu(q, b))\\right)$$\nThe log-marginal-likelihood is:\n$$\\log \\mathcal{L}(q, b) = -\\frac{N_\\lambda}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det(C_y(q)) - \\frac{1}{2} (y - (b\\mathbf{1} + q G s_0))^T C_y(q)^{-1} (y - (b\\mathbf{1} + q G s_0))$$\n\n### 2. Maximum Likelihood Estimate (MLE) of the Baseline $b$\n\nTo find the MLE for $b$ for a fixed value of $q$, denoted $\\hat{b}(q)$, we maximize $\\log \\mathcal{L}(q, b)$ with respect to $b$. This is equivalent to minimizing the quadratic term in the exponent:\n$$\\chi^2(b) = (y - b\\mathbf{1} - q G s_0)^T C_y(q)^{-1} (y - b\\mathbf{1} - q G s_0)$$\nLet $z(q) = y - q G s_0$. The expression becomes $\\chi^2(b) = (z(q) - b\\mathbf{1})^T C_y(q)^{-1} (z(q) - b\\mathbf{1})$. We find the minimum by setting the derivative with respect to $b$ to zero:\n$$\\frac{\\partial \\chi^2}{\\partial b} = -2 \\mathbf{1}^T C_y(q)^{-1} (z(q) - b\\mathbf{1}) = 0$$\n$$\\mathbf{1}^T C_y(q)^{-1} z(q) - b (\\mathbf{1}^T C_y(q)^{-1} \\mathbf{1}) = 0$$\nSolving for $b$ gives the MLE $\\hat{b}(q)$:\n$$\\hat{b}(q) = \\frac{\\mathbf{1}^T C_y(q)^{-1} z(q)}{\\mathbf{1}^T C_y(q)^{-1} \\mathbf{1}} = \\frac{\\mathbf{1}^T C_y(q)^{-1} (y - q G s_0)}{\\mathbf{1}^T C_y(q)^{-1} \\mathbf{1}}$$\nThis is the expression for the generalized least squares estimate of the intercept parameter $b$.\n\n### 3. Profile Log-Likelihood and Parameter Estimation\n\nBy substituting $\\hat{b}(q)$ back into the log-likelihood function, we obtain the profile log-likelihood, which is a function of $q$ alone:\n$$\\log \\mathcal{L}_{\\text{prof}}(q) = \\log \\mathcal{L}(q, \\hat{b}(q))$$\nThe overall MLE for the mixing ratio, $\\hat{q}$, is found by maximizing this profile log-likelihood over the allowed range for $q$, which is specified as $q \\in [0, 10]$ for the numerical implementation:\n$$\\hat{q} = \\arg\\max_{q \\in [0, 10]} \\log \\mathcal{L}_{\\text{prof}}(q)$$\nThis is a one-dimensional optimization problem. Once $\\hat{q}$ is found, the corresponding MLE for the baseline, $\\hat{b}$, is computed directly using the formula for $\\hat{b}(\\hat{q})$:\n$$\\hat{b} = \\hat{b}(\\hat{q}) = \\frac{\\mathbf{1}^T C_y(\\hat{q})^{-1} (y - \\hat{q} G s_0)}{\\mathbf{1}^T C_y(\\hat{q})^{-1} \\mathbf{1}}$$\nFinally, the maximized log-marginal-likelihood is $\\log \\mathcal{L}(\\hat{q}, \\hat{b})$.\n\n### 4. Algorithmic Implementation\n\nThe numerical implementation proceeds as follows:\n1.  For each test case, the necessary parameters ($G, s_0$) and case-specific data ($y$, $\\sigma_n, \\sigma_s, \\sigma_\\delta$) are defined.\n2.  A Python function is created to compute the profile log-likelihood, $\\log \\mathcal{L}_{\\text{prof}}(q)$, and the corresponding $\\hat{b}(q)$.\n    -   Inside this function, for a given $q$, the covariance matrix $C_y(q)$ is constructed.\n    -   To compute $\\hat{b}(q)$ and the final $\\chi^2$ term, direct matrix inversion is avoided for numerical stability. Instead, systems of linear equations are solved using `numpy.linalg.solve`. For example, to compute a term like $C_y^{-1}v$, we solve the system $C_y x = v$ for $x$.\n    -   The term $\\log \\det(C_y(q))$ is computed stably using `numpy.linalg.slogdet`, which returns the sign and natural logarithm of the determinant.\n3.  The negative of the profile log-likelihood function is minimized with respect to $q$ over the interval $[0, 10]$ using `scipy.optimize.minimize_scalar` with the `bounded` method. This yields $\\hat{q}$.\n4.  With the optimized $\\hat{q}$, the corresponding $\\hat{b}$ is re-calculated using its explicit formula, and the maximum log-likelihood value $\\log \\mathcal{L}(\\hat{q}, \\hat{b})$ is obtained.\n5.  The computed triplet $(\\hat{q}, \\hat{b}, \\log \\mathcal{L})$ is stored for each test case.\n6.  The final results are formatted and printed as a single comma-separated list.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Solves for the MLE of q and b based on a marginalized likelihood.\n    \"\"\"\n    #\n    # --- Define common parameters from the problem statement ---\n    #\n    N_lambda = 5\n    N_lines = 2\n    G = np.array([\n        [0.8, 0.0],\n        [0.6, 0.1],\n        [0.3, 0.3],\n        [0.1, 0.7],\n        [0.0, 0.9]\n    ])\n    s0 = np.array([1.0, 0.7])\n    \n    one_vec = np.ones(N_lambda)\n    \n    # Pre-compute terms to optimize calculations inside the loop\n    GGt = G @ G.T\n    Gs0 = G @ s0\n\n    #\n    # --- Define test cases ---\n    #\n    test_cases_spec = [\n        {\n            \"case_name\": \"Case 1: Happy Path\",\n            \"y_gen\": {\n                \"q_true\": 0.6,\n                \"b_true\": 0.001,\n                \"s_true\": s0 + np.array([0.2, -0.1]),\n                \"n\": np.array([0.0002, -0.0001, 0.0, 0.00015, -0.00005]),\n                \"delta\": np.zeros(N_lambda),\n            },\n            \"retrieval\": {\"sigma_n\": 0.0003, \"sigma_s\": 0.15, \"sigma_delta\": 0.0002}\n        },\n        {\n            \"case_name\": \"Case 2: No Line-list Uncertainty\",\n            \"y_gen\": \"use_case_1\", # Use the same y as case 1\n            \"retrieval\": {\"sigma_n\": 0.0003, \"sigma_s\": 0.0, \"sigma_delta\": 0.0002}\n        },\n        {\n            \"case_name\": \"Case 3: Dominant Model Inadequacy\",\n            \"y_gen\": {\n                \"q_true\": 0.5,\n                \"b_true\": 0.0,\n                \"s_true\": s0,\n                \"n\": np.zeros(N_lambda),\n                \"delta\": np.array([0.001, -0.0005, 0.0008, -0.0012, 0.0004]),\n            },\n            \"retrieval\": {\"sigma_n\": 0.0001, \"sigma_s\": 0.2, \"sigma_delta\": 0.001}\n        }\n    ]\n\n    all_results = []\n    y_case1 = None\n\n    #\n    # --- Define helper function for likelihood calculation and optimization ---\n    #\n    def get_profile_logL_and_b(q, y, Gs0_term, GGt_term, one_vec_term, ret_params):\n        \"\"\"\n        Calculates the profile log-likelihood for a given q, also returning b_hat.\n        \"\"\"\n        sigma_n = ret_params[\"sigma_n\"]\n        sigma_s = ret_params[\"sigma_s\"]\n        sigma_delta = ret_params[\"sigma_delta\"]\n\n        var_n_delta = sigma_n**2 + sigma_delta**2\n        var_s = sigma_s**2\n        \n        # Construct the covariance matrix C_y(q)\n        C_y = var_n_delta * np.eye(N_lambda) + (q**2 * var_s) * GGt_term\n        \n        # Calculate b_hat(q) using a stable solver\n        z = y - q * Gs0_term\n        try:\n            # Solve C_y * x = 1_vec to get C_y_inv_one\n            C_y_inv_one = np.linalg.solve(C_y, one_vec_term)\n            b_hat_num = C_y_inv_one @ z\n            b_hat_den = C_y_inv_one @ one_vec_term\n            if np.isclose(b_hat_den, 0): return -np.inf, 0 # Avoid division by zero\n            b_hat = b_hat_num / b_hat_den\n        except np.linalg.LinAlgError:\n            return -np.inf, 0 # Return a very small logL if matrix is singular\n\n        # Calculate the log-likelihood\n        sign, log_det_Cy = np.linalg.slogdet(C_y)\n        if sign <= 0: return -np.inf, b_hat # Covariance not positive definite\n\n        residual = y - b_hat * one_vec_term - q * Gs0_term\n        try:\n            # Calculate chi-squared term: res^T * C_y^{-1} * res\n            chi2 = residual @ np.linalg.solve(C_y, residual)\n        except np.linalg.LinAlgError:\n            return -np.inf, b_hat\n\n        logL = -0.5 * (N_lambda * np.log(2 * np.pi) + log_det_Cy + chi2)\n        return logL, b_hat\n\n    #\n    # --- Loop through test cases ---\n    #\n    for spec in test_cases_spec:\n        # Step 1: Generate or retrieve the data vector y\n        if spec[\"y_gen\"] == \"use_case_1\":\n            y = y_case1\n        else:\n            y_params = spec[\"y_gen\"]\n            y = (y_params[\"b_true\"] * one_vec +\n                 y_params[\"q_true\"] * (G @ y_params[\"s_true\"]) +\n                 y_params[\"n\"] +\n                 y_params[\"delta\"])\n            if y_case1 is None:\n                y_case1 = y\n\n        retrieval_params = spec[\"retrieval\"]\n\n        # Step 2: Define objective function for the optimizer\n        def objective_func(q, y, Gs0, GGt, one_vec, ret_params):\n            logL, _ = get_profile_logL_and_b(q, y, Gs0, GGt, one_vec, ret_params)\n            return -logL\n\n        # Step 3: Find q_hat by maximizing the profile log-likelihood\n        opt_result = minimize_scalar(\n            objective_func,\n            args=(y, Gs0, GGt, one_vec, retrieval_params),\n            bounds=(0, 10),\n            method='bounded'\n        )\n        q_hat = opt_result.x\n        \n        # Step 4: Calculate the final b_hat and log-likelihood at q_hat\n        log_L_hat, b_hat = get_profile_logL_and_b(q_hat, y, Gs0, GGt, one_vec, retrieval_params)\n\n        all_results.extend([q_hat, b_hat, log_L_hat])\n\n    #\n    # --- Final Print Statement ---\n    #\n    print(f\"[{','.join(f'{x:.7f}' for x in all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Defining the posterior distribution is only half the battle in Bayesian retrieval; we must also be able to explore it efficiently, often with samplers like MCMC. The geometry of the posterior, particularly strong parameter correlations inherited from the prior or the physics, can severely hinder sampler performance. In this problem, you will investigate how a simple change of variables—prior whitening—can dramatically improve the numerical conditioning of the posterior, making inference in high-dimensional models tractable .",
            "id": "4153635",
            "problem": "Consider a linearized atmospheric retrieval for an exoplanet transmission spectrum around a reference state. The measurement model is given by a linear map from the state to the data with additive Gaussian noise. Let $x \\in \\mathbb{R}^n$ be the vector of $n$ atmospheric parameters (for example, logarithms of gas mixing ratios and a temperature parameter, treated here as dimensionless for generality), and let $y \\in \\mathbb{R}^m$ be the vector of $m$ spectral data points. The forward model is linearized as\n$$\ny = K x + \\varepsilon,\n$$\nwhere $K \\in \\mathbb{R}^{m \\times n}$ is the Jacobian matrix of the forward model evaluated at the reference state, and $\\varepsilon \\sim \\mathcal{N}(0, N)$ is zero-mean Gaussian noise with covariance $N \\in \\mathbb{R}^{m \\times m}$, which is symmetric positive definite. The prior for $x$ is Gaussian,\n$$\nx \\sim \\mathcal{N}(\\mu, S),\n$$\nwith mean $\\mu \\in \\mathbb{R}^n$ and covariance $S \\in \\mathbb{R}^{n \\times n}$, which is symmetric positive definite.\n\nIn high-dimensional atmospheric retrievals, Markov Chain Monte Carlo (MCMC) sampling efficiency can degrade when parameters are strongly correlated or scale-separated. A classic remedy is to reparameterize the state vector using a prior-whitening transform. Let $L \\in \\mathbb{R}^{n \\times n}$ be the lower-triangular Cholesky factor of $S$ such that $S = L L^\\top$. Define the reparameterized variable $z \\in \\mathbb{R}^n$ by\n$$\nx = \\mu + L z,\n$$\nwhich implies a standard normal prior $z \\sim \\mathcal{N}(0, I)$ with $I \\in \\mathbb{R}^{n \\times n}$ the identity matrix. In terms of $z$, the measurement model becomes\n$$\ny = K \\mu + K L z + \\varepsilon.\n$$\n\nStarting from Bayes’ theorem and the given Gaussian prior and likelihood, derive from first principles the posterior distributions in both the original parameterization and the prior-whitened parameterization. Your derivation must begin at the definitions above and proceed using standard rules for multivariate Gaussians (completing the square), without invoking any shortcut formulas not justified by the fundamental setup.\n\nDefine the notion of numerical conditioning for a symmetric positive definite covariance matrix. Specifically, for a symmetric positive definite matrix $A$, define its $2$-norm condition number by\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)},\n$$\nwhere $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ are the largest and smallest eigenvalues of $A$, respectively.\n\nImplement a program that, for each provided test case, computes the posterior covariance in the original parameterization and in the prior-whitened parameterization, and then outputs the ratio of their condition numbers, namely\n$$\nr = \\frac{\\kappa_2(\\Sigma_x)}{\\kappa_2(\\Sigma_z)},\n$$\nwhere $\\Sigma_x$ is the posterior covariance of $x$ and $\\Sigma_z$ is the posterior covariance of $z$. A value $r > 1$ indicates that the prior-whitened parameterization yields a better-conditioned posterior than the original parameterization.\n\nAll quantities in this problem are dimensionless. Express each ratio $r$ as a decimal number rounded to six decimal places.\n\nTest suite:\n- Case 1 (correlated sensitivities and prior correlations): Let $n = 3$, $m = 3$, and\n$$\nK_1 = \\begin{pmatrix}\n1.0 & 0.9 & 0.1 \\\\\n0.9 & 1.0 & 0.2 \\\\\n0.1 & 0.2 & 0.5\n\\end{pmatrix},\\quad\nN_1 = \\operatorname{diag}(0.01,\\,0.02,\\,0.015),\\quad\nS_1 = \\begin{pmatrix}\n0.5 & 0.45 & 0.05 \\\\\n0.45 & 0.5 & 0.05 \\\\\n0.05 & 0.05 & 0.2\n\\end{pmatrix}.\n$$\nTake $\\mu_1 = \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n- Case 2 (nearly collinear columns in $K$ and broad correlated prior): Let $n = 3$, $m = 3$, and\n$$\nK_2 = \\begin{pmatrix}\n1.0 & 1.0 & 0.0 \\\\\n0.99 & 1.0 & 0.01 \\\\\n0.0 & 0.01 & 0.0\n\\end{pmatrix},\\quad\nN_2 = \\operatorname{diag}(1.0,\\,1.0,\\,1.0),\\quad\nS_2 = \\begin{pmatrix}\n2.0 & 1.8 & 0.0 \\\\\n1.8 & 2.0 & 0.0 \\\\\n0.0 & 0.0 & 0.5\n\\end{pmatrix}.\n$$\nTake $\\mu_2 = \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n- Case 3 (orthogonal sensitivities and low noise): Let $n = 3$, $m = 3$, and\n$$\nK_3 = I = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix},\\quad\nN_3 = \\operatorname{diag}(10^{-4},\\,10^{-4},\\,10^{-4}),\\quad\nS_3 = \\operatorname{diag}(1.0,\\,1.0,\\,1.0).\n$$\nTake $\\mu_3 = \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n\nYour program should produce a single line of output containing the three computed ratios as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3]$, with each $r_i$ rounded to six decimal places. No other text should be printed.",
            "solution": "The user has requested a derivation of posterior distributions for a linearized Bayesian atmospheric retrieval problem in both its original and prior-whitened parameterizations, followed by a numerical implementation to compare the conditioning of these posteriors.\n\n### 1. Derivation of Posterior Distributions\n\nThe problem is defined by a Gaussian likelihood and a Gaussian prior. The posterior distribution is found using Bayes' theorem, which states that the posterior probability density function (PDF) is proportional to the product of the likelihood PDF and the prior PDF. For a multivariate Gaussian distribution $\\mathcal{N}(v | \\bar{v}, \\Sigma_v)$, its PDF is given by $p(v) \\propto \\exp\\left(-\\frac{1}{2} (v - \\bar{v})^\\top \\Sigma_v^{-1} (v - \\bar{v})\\right)$.\n\n#### 1.1 Posterior for Original Parameters ($x$)\n\nThe model for the original parameters $x \\in \\mathbb{R}^n$ is:\n-   Likelihood: $p(y|x) = \\mathcal{N}(y | Kx, N)$, where $y \\in \\mathbb{R}^m$, $K \\in \\mathbb{R}^{m \\times n}$, and $N$ is the noise covariance matrix.\n-   Prior: $p(x) = \\mathcal{N}(x | \\mu, S)$, where $\\mu \\in \\mathbb{R}^n$ and $S$ is the prior covariance matrix.\n\nAccording to Bayes' theorem, the posterior PDF $p(x|y)$ is:\n$$\np(x|y) \\propto p(y|x)p(x)\n$$\nSubstituting the Gaussian forms:\n$$\np(x|y) \\propto \\exp\\left( -\\frac{1}{2} (y - Kx)^\\top N^{-1} (y - Kx) \\right) \\cdot \\exp\\left( -\\frac{1}{2} (x - \\mu)^\\top S^{-1} (x - \\mu) \\right)\n$$\n$$\np(x|y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ (y - Kx)^\\top N^{-1} (y - Kx) + (x - \\mu)^\\top S^{-1} (x - \\mu) \\right] \\right)\n$$\nTo identify the form of the posterior distribution, we expand the quadratic expression in the exponent, which we denote as $J(x)$:\n$$\nJ(x) = (y^\\top - x^\\top K^\\top) N^{-1} (y - Kx) + (x^\\top - \\mu^\\top) S^{-1} (x - \\mu)\n$$\n$$\nJ(x) = (y^\\top N^{-1} y - y^\\top N^{-1} Kx - x^\\top K^\\top N^{-1} y + x^\\top K^\\top N^{-1} Kx) + (x^\\top S^{-1} x - x^\\top S^{-1} \\mu - \\mu^\\top S^{-1} x + \\mu^\\top S^{-1} \\mu)\n$$\nRecognizing that the transpose of a scalar is itself (e.g., $x^\\top K^\\top N^{-1} y = y^\\top N^{-1} Kx$), we combine the terms linear in $x$:\n$$\nJ(x) = y^\\top N^{-1} y - 2x^\\top K^\\top N^{-1} y + x^\\top K^\\top N^{-1} Kx + x^\\top S^{-1} x - 2x^\\top S^{-1} \\mu + \\mu^\\top S^{-1} \\mu\n$$\nGrouping terms by powers of $x$:\n$$\nJ(x) = x^\\top (K^\\top N^{-1} K + S^{-1}) x - 2x^\\top (K^\\top N^{-1} y + S^{-1}\\mu) + \\text{const}\n$$\nwhere $\\text{const}$ includes all terms not dependent on $x$. This is a quadratic form in $x$, which implies the posterior distribution is also Gaussian, say $p(x|y) = \\mathcal{N}(x | \\mu_x, \\Sigma_x)$. The exponent of its PDF is $-\\frac{1}{2}(x - \\mu_x)^\\top \\Sigma_x^{-1} (x - \\mu_x)$, which expands to:\n$$\n-\\frac{1}{2} (x^\\top \\Sigma_x^{-1} x - 2x^\\top \\Sigma_x^{-1}\\mu_x + \\mu_x^\\top \\Sigma_x^{-1} \\mu_x)\n$$\nBy comparing the quadratic term in $x$ from $J(x)$ with this form, we identify the inverse posterior covariance (also known as the precision matrix):\n$$\n\\Sigma_x^{-1} = K^\\top N^{-1} K + S^{-1}\n$$\nThe posterior covariance for the original parameterization $x$ is therefore:\n$$\n\\Sigma_x = (K^\\top N^{-1} K + S^{-1})^{-1}\n$$\nNote that the posterior covariance $\\Sigma_x$ is independent of the measurement data $y$.\n\n#### 1.2 Posterior for Prior-Whitened Parameters ($z$)\n\nThe prior-whitened parameters $z \\in \\mathbb{R}^n$ are related to $x$ by the transformation $x = \\mu + Lz$, where $S = LL^\\top$ is the Cholesky decomposition of the prior covariance $S$. This transformation implies a standard Gaussian prior for $z$: $p(z) = \\mathcal{N}(z | 0, I)$, where $I$ is the identity matrix.\n\nWe rewrite the measurement model in terms of $z$:\n$$\ny = K(\\mu + Lz) + \\varepsilon = K\\mu + KLz + \\varepsilon\n$$\nLet's define a shifted data vector $y' = y - K\\mu$. The model becomes $y' = (KL)z + \\varepsilon$.\n-   Likelihood: $p(y|z) = p(y'|z) = \\mathcal{N}(y' | KLz, N)$.\n-   Prior: $p(z) = \\mathcal{N}(z | 0, I)$.\n\nApplying Bayes' theorem for $z$:\n$$\np(z|y) \\propto p(y|z)p(z)\n$$\n$$\np(z|y) \\propto \\exp\\left( -\\frac{1}{2} (y' - KLz)^\\top N^{-1} (y' - KLz) \\right) \\cdot \\exp\\left( -\\frac{1}{2} z^\\top I^{-1} z \\right)\n$$\n$$\np(z|y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ (y - K\\mu - KLz)^\\top N^{-1} (y - K\\mu - KLz) + z^\\top z \\right] \\right)\n$$\nLet the term in the exponent be $-J(z)/2$. We expand $J(z)$:\n$$\nJ(z) = (y' - KLz)^\\top N^{-1} (y' - KLz) + z^\\top z\n$$\n$$\nJ(z) = z^\\top (KL)^\\top N^{-1} (KL)z - 2z^\\top (KL)^\\top N^{-1} y' + (y')^\\top N^{-1} y' + z^\\top z\n$$\nGrouping terms by powers of $z$:\n$$\nJ(z) = z^\\top (L^\\top K^\\top N^{-1} K L + I) z - 2z^\\top L^\\top K^\\top N^{-1} y' + \\text{const}\n$$\nThis quadratic form implies a Gaussian posterior for $z$, $p(z|y) = \\mathcal{N}(z | \\mu_z, \\Sigma_z)$. By comparing the quadratic term with that of a general Gaussian PDF, we identify the inverse posterior covariance for $z$:\n$$\n\\Sigma_z^{-1} = L^\\top K^\\top N^{-1} K L + I\n$$\nThe posterior covariance for the prior-whitened parameterization $z$ is:\n$$\n\\Sigma_z = (L^\\top K^\\top N^{-1} K L + I)^{-1}\n$$\n\n### 2. Numerical Conditioning and Computational Procedure\n\nThe numerical conditioning of a symmetric positive definite (SPD) covariance matrix $A$ is given by its $2$-norm condition number:\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}\n$$\nwhere $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ are the maximum and minimum eigenvalues of $A$, respectively. A smaller condition number (closer to $1$) indicates a better-conditioned matrix.\n\nThe program will compute the ratio $r = \\kappa_2(\\Sigma_x) / \\kappa_2(\\Sigma_z)$ for each test case. The following steps are performed:\n1.  Given matrices $K$, $S$, and the diagonal of $N$, construct the full matrices.\n2.  Compute the Cholesky decomposition of the prior covariance $S$ to find the lower-triangular matrix $L$.\n3.  Compute the inverse matrices $N^{-1}$ and $S^{-1}$.\n4.  Calculate the posterior covariance $\\Sigma_x = (K^\\top N^{-1} K + S^{-1})^{-1}$.\n5.  Calculate the posterior covariance $\\Sigma_z = (L^\\top K^\\top N^{-1} K L + I)^{-1}$.\n6.  For both $\\Sigma_x$ and $\\Sigma_z$, compute their eigenvalues and find the ratio of the largest to the smallest to obtain their respective condition numbers.\n7.  Compute the final ratio $r$ and round it to the specified precision.\n\nThis procedure is applied to each of the three test cases provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky\n\ndef solve():\n    \"\"\"\n    Solves the atmospheric retrieval conditioning problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Correlated sensitivities and prior\n        {\n            \"K\": np.array([\n                [1.0, 0.9, 0.1],\n                [0.9, 1.0, 0.2],\n                [0.1, 0.2, 0.5]\n            ]),\n            \"N_diag\": np.array([0.01, 0.02, 0.015]),\n            \"S\": np.array([\n                [0.5, 0.45, 0.05],\n                [0.45, 0.5, 0.05],\n                [0.05, 0.05, 0.2]\n            ]),\n        },\n        # Case 2: Nearly collinear columns in K and broad correlated prior\n        {\n            \"K\": np.array([\n                [1.0, 1.0, 0.0],\n                [0.99, 1.0, 0.01],\n                [0.0, 0.01, 0.0]\n            ]),\n            \"N_diag\": np.array([1.0, 1.0, 1.0]),\n            \"S\": np.array([\n                [2.0, 1.8, 0.0],\n                [1.8, 2.0, 0.0],\n                [0.0, 0.0, 0.5]\n            ]),\n        },\n        # Case 3: Orthogonal sensitivities and low noise\n        {\n            \"K\": np.array([\n                [1.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0],\n                [0.0, 0.0, 1.0]\n            ]),\n            \"N_diag\": np.array([1e-4, 1e-4, 1e-4]),\n            \"S\": np.array([\n                [1.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0],\n                [0.0, 0.0, 1.0]\n            ]),\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        K = case[\"K\"]\n        S = case[\"S\"]\n        N = np.diag(case[\"N_diag\"])\n        \n        n = K.shape[1]\n        I = np.identity(n)\n\n        # Inverses needed for the formulas.\n        N_inv = np.linalg.inv(N)\n        S_inv = np.linalg.inv(S)\n        \n        # Cholesky factor of the prior covariance S such that S = L L^T\n        # We use scipy.linalg.cholesky to ensure we get the lower triangular factor.\n        L = cholesky(S, lower=True)\n\n        # 1. Posterior covariance for original parameters x\n        # Sigma_x = (K^T N^-1 K + S^-1)^-1\n        precision_x = K.T @ N_inv @ K + S_inv\n        Sigma_x = np.linalg.inv(precision_x)\n\n        # 2. Posterior covariance for whitened parameters z\n        # Sigma_z = (L^T K^T N^-1 K L + I)^-1\n        precision_z = L.T @ K.T @ N_inv @ K @ L + I\n        Sigma_z = np.linalg.inv(precision_z)\n        \n        def condition_number_2(A):\n            \"\"\"\n            Calculates the 2-norm condition number for a symmetric positive definite matrix.\n            kappa_2(A) = lambda_max(A) / lambda_min(A)\n            \"\"\"\n            # eigvalsh is for hermitian (or real symmetric) matrices.\n            # It returns eigenvalues in ascending order.\n            eigenvalues = np.linalg.eigvalsh(A)\n            return eigenvalues[-1] / eigenvalues[0]\n\n        kappa_Sigma_x = condition_number_2(Sigma_x)\n        kappa_Sigma_z = condition_number_2(Sigma_z)\n\n        # Compute the ratio of condition numbers\n        ratio = kappa_Sigma_x / kappa_Sigma_z\n        results.append(f\"{ratio:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}