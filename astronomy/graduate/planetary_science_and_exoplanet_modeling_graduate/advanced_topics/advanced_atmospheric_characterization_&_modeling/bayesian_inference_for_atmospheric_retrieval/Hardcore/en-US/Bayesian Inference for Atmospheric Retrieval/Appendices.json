{
    "hands_on_practices": [
        {
            "introduction": "At the heart of any atmospheric retrieval is the forward model, a function that maps a set of atmospheric parameters to a predicted observation. Before applying complex numerical codes, it is invaluable to understand how fundamental physical principles shape the transmission spectrum. This practice guides you through the derivation of a classic, analytical forward model for an exoplanet's transit radius from first principles, solidifying the connection between atmospheric properties like temperature and composition, and the observable spectrum .",
            "id": "4154218",
            "problem": "An exoplanetary atmosphere is modeled for transmission spectroscopy within a Bayesian inference framework, where the forward model maps composition and thermodynamic parameters to the wavelength-dependent transit radius. Consider a spherically symmetric atmosphere that is isothermal and in hydrostatic equilibrium with constant gravitational acceleration $g$. Let the reference radius $R_{0}$ be defined at the reference pressure $P_{0}$ where the atmosphere becomes opaque at lower altitudes. Assume the ideal gas law with Boltzmann constant $k_{B}$, a uniform mean mass per particle $\\mu$, and a single absorbing species with constant volume mixing ratio $\\xi$ and absorption cross section $\\sigma_{\\lambda}$ that is independent of temperature and pressure. Let the stellar radius be $R_{\\star}$, and assume the thin-atmosphere limit $H \\ll R_{0}$, where $H$ is the pressure scale height under isothermal hydrostatic conditions.\n\nStarting only from hydrostatic equilibrium $dP/dr = -\\rho g$, the ideal gas law $P = n k_{B} T$, and the definition of the chord optical depth integral along a ray of impact parameter $b$ through the limb,\n$$\n\\tau_{\\lambda}(b) = \\int_{-\\infty}^{+\\infty} \\xi \\, \\sigma_{\\lambda} \\, n\\!\\left(\\sqrt{b^{2}+x^{2}}\\right) \\, dx,\n$$\nderive the analytic expression, valid to leading order in $H/R_{0}$, for the wavelength-dependent effective transit radius $R(\\lambda)$ defined by the occulted area condition\n$$\n\\pi R^{2}(\\lambda) = \\pi R_{0}^{2} + 2 \\pi \\int_{0}^{\\infty} \\left[1 - \\exp\\!\\left(-\\tau_{\\lambda}(R_{0}+z)\\right)\\right] (R_{0}+z) \\, dz.\n$$\nExpress the final closed-form result for $R(\\lambda)$ in terms of $R_{0}$, $P_{0}$, $T$, $\\mu$, $g$, $\\xi$, $\\sigma_{\\lambda}$, and fundamental constants, and use the atmospheric number density at the reference level $n_{0} = P_{0}/(k_{B} T)$. You may assume and use the thin-limb approximation that near the tangent point the radius satisfies $r \\approx b + x^{2}/(2b)$ for $|x| \\ll b$, with $b \\approx R_{0}+z$. The expression should include all leading-order terms in $H/R_{0}$, including any constants arising from the limb integration.\n\nFinally, enumerate at least four scientifically grounded limitations of the isothermal, hydrostatic, single-species chord optical depth approximation as it is used in atmospheric retrieval, discussing regimes in which the derived forward model would introduce biased inferences.\n\nProvide your final answer as a single closed-form analytic expression for $R(\\lambda)$ only. Do not include any units inside your final answer. No rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and provides a complete and consistent setup for a standard derivation in the field of exoplanetary atmospheric science.\n\nThe derivation proceeds in three main stages: first, determining the atmospheric number density profile $n(r)$ under the given assumptions; second, calculating the chord optical depth $\\tau_{\\lambda}(b)$; and third, substituting this into the definition of the effective transit radius $R(\\lambda)$ and evaluating the integral to leading order.\n\n**1. Atmospheric Density Profile**\n\nWe begin with the equation of hydrostatic equilibrium for a spherically symmetric atmosphere with constant gravitational acceleration $g$:\n$$\n\\frac{dP}{dr} = -\\rho g\n$$\nwhere $P$ is the pressure, $r$ is the radial distance from the planet's center, and $\\rho$ is the mass density. The density is related to the number density $n$ and the mean mass per particle $\\mu$ by $\\rho = n\\mu$. Substituting this gives:\n$$\n\\frac{dP}{dr} = -n\\mu g\n$$\nThe atmosphere is assumed to follow the ideal gas law, $P = n k_B T$, where $k_B$ is the Boltzmann constant and $T$ is the constant temperature. We can express the number density as $n = P/(k_B T)$. Substituting this into the hydrostatic equilibrium equation yields:\n$$\n\\frac{dP}{dr} = -\\frac{P}{k_B T}\\mu g\n$$\nThis is a first-order linear ordinary differential equation for $P(r)$. We can rearrange it to separate variables:\n$$\n\\frac{dP}{P} = -\\frac{\\mu g}{k_B T} dr\n$$\nWe define the pressure scale height $H$ as the characteristic vertical length scale over which the pressure changes by a factor of $e$. For an isothermal atmosphere, it is a constant:\n$$\nH = \\frac{k_B T}{\\mu g}\n$$\nSubstituting $H$ into the differential equation gives:\n$$\n\\frac{dP}{P} = -\\frac{1}{H} dr\n$$\nWe integrate this equation from the reference level $(R_0, P_0)$ to an arbitrary level $(r, P(r))$:\n$$\n\\int_{P_0}^{P(r)} \\frac{dP'}{P'} = \\int_{R_0}^{r} -\\frac{1}{H} dr'\n$$\n$$\n\\ln\\left(\\frac{P(r)}{P_0}\\right) = -\\frac{r-R_0}{H}\n$$\nSolving for $P(r)$, we find the pressure profile:\n$$\nP(r) = P_0 \\exp\\left(-\\frac{r-R_0}{H}\\right)\n$$\nSince $n = P/(k_B T)$ and $T$ is constant, the number density profile follows the same exponential form:\n$$\nn(r) = \\frac{P_0}{k_B T} \\exp\\left(-\\frac{r-R_0}{H}\\right) = n_0 \\exp\\left(-\\frac{r-R_0}{H}\\right)\n$$\nwhere $n_0 = P_0/(k_B T)$ is the number density at the reference radius $R_0$.\n\n**2. Chord Optical Depth**\n\nThe chord optical depth $\\tau_{\\lambda}(b)$ through the atmosphere limb along a ray with impact parameter $b$ is given by the integral:\n$$\n\\tau_{\\lambda}(b) = \\int_{-\\infty}^{+\\infty} \\xi \\, \\sigma_{\\lambda} \\, n(r) \\, dx\n$$\nHere, $x$ is the coordinate along the ray, with $x=0$ at the point of closest approach. The radial distance $r$ is related to $b$ and $x$ by $r = \\sqrt{b^2 + x^2}$. Substituting the density profile into the integral:\n$$\n\\tau_{\\lambda}(b) = \\int_{-\\infty}^{+\\infty} \\xi \\, \\sigma_{\\lambda} \\, n_0 \\exp\\left(-\\frac{\\sqrt{b^2 + x^2} - R_0}{H}\\right) \\, dx\n$$\nThe problem specifies using the thin-limb approximation for the radial distance near the tangent point, valid for $|x| \\ll b$:\n$$\nr = \\sqrt{b^2 + x^2} = b\\sqrt{1 + (x/b)^2} \\approx b\\left(1 + \\frac{x^2}{2b^2}\\right) = b + \\frac{x^2}{2b}\n$$\nThis approximation is valid because the exponential term in the integrand decreases rapidly away from $x=0$ due to the small scale height $H$, so the main contribution to the integral comes from the region where $|x|$ is small. Substituting this approximation for $r$:\n$$\n\\tau_{\\lambda}(b) \\approx \\int_{-\\infty}^{+\\infty} \\xi \\, \\sigma_{\\lambda} \\, n_0 \\exp\\left(-\\frac{b + x^2/(2b) - R_0}{H}\\right) \\, dx\n$$\nWe can separate the terms in the exponent that do not depend on $x$:\n$$\n\\tau_{\\lambda}(b) \\approx \\xi \\, \\sigma_{\\lambda} \\, n_0 \\exp\\left(-\\frac{b - R_0}{H}\\right) \\int_{-\\infty}^{+\\infty} \\exp\\left(-\\frac{x^2}{2bH}\\right) \\, dx\n$$\nThe remaining integral is a standard Gaussian integral of the form $\\int_{-\\infty}^{+\\infty} \\exp(-ax^2)dx = \\sqrt{\\pi/a}$. In our case, $a = 1/(2bH)$, so the integral evaluates to $\\sqrt{2\\pi bH}$. This gives the chord optical depth as:\n$$\n\\tau_{\\lambda}(b) \\approx \\xi \\, \\sigma_{\\lambda} \\, n_0 \\sqrt{2\\pi bH} \\exp\\left(-\\frac{b - R_0}{H}\\right)\n$$\n\n**3. Effective Transit Radius**\n\nThe effective transit radius $R(\\lambda)$ is defined by the total occulted area:\n$$\n\\pi R^{2}(\\lambda) = \\pi R_{0}^{2} + 2 \\pi \\int_{0}^{\\infty} \\left[1 - \\exp(-\\tau_{\\lambda}(R_{0}+z))\\right] (R_{0}+z) \\, dz\n$$\nHere, the integration variable is the altitude $z$ above the reference opaque level $R_0$, and the impact parameter is $b = R_0 + z$. We first substitute our expression for $\\tau_{\\lambda}(b)$ with $b=R_0+z$:\n$$\n\\tau_{\\lambda}(R_0+z) \\approx \\xi \\sigma_{\\lambda} n_0 \\sqrt{2\\pi (R_0+z)H} \\exp\\left(-\\frac{(R_0+z)-R_0}{H}\\right) = \\xi \\sigma_{\\lambda} n_0 \\sqrt{2\\pi (R_0+z)H} \\exp\\left(-\\frac{z}{H}\\right)\n$$\nIn the thin-atmosphere limit ($H \\ll R_0$), the integrand is significant only for small values of $z/R_0$. We can therefore make the leading-order approximation that $(R_0+z) \\approx R_0$ within the terms that vary slowly with $z$. This applies to the factor $(R_0+z)$ in the integral and the $\\sqrt{R_0+z}$ term within the optical depth expression. This yields:\n$$\n\\pi (R^2(\\lambda) - R_0^2) \\approx 2\\pi R_0 \\int_{0}^{\\infty} \\left[1 - \\exp\\left(-\\left(\\xi \\sigma_{\\lambda} n_0 \\sqrt{2\\pi R_0 H}\\right) \\exp(-z/H)\\right)\\right] dz\n$$\nLet's define the optical depth at the reference altitude level $z=0$ (grazing $R_0$) as $\\tau_0(\\lambda)$:\n$$\n\\tau_0(\\lambda) = \\xi \\sigma_{\\lambda} n_0 \\sqrt{2\\pi R_0 H}\n$$\nThe integral becomes:\n$$\nR^2(\\lambda) - R_0^2 \\approx 2R_0 \\int_{0}^{\\infty} \\left[1 - \\exp\\left(-\\tau_0(\\lambda) \\exp(-z/H)\\right)\\right] dz\n$$\nTo solve this integral, we perform a substitution. Let $u = \\tau_0(\\lambda) \\exp(-z/H)$. Then $du = -\\frac{1}{H} \\tau_0(\\lambda) \\exp(-z/H) dz = -\\frac{u}{H} dz$, which means $dz = -\\frac{H}{u} du$. The limits of integration change from $z \\in [0, \\infty)$ to $u \\in [\\tau_0(\\lambda), 0]$.\n$$\n\\int_{0}^{\\infty} \\dots dz = \\int_{\\tau_0(\\lambda)}^{0} [1 - \\exp(-u)] \\left(-\\frac{H}{u}\\right) du = H \\int_{0}^{\\tau_0(\\lambda)} \\frac{1-\\exp(-u)}{u} du\n$$\nThis integral defines the exponential integral function $\\text{Ein}(x) = \\int_0^x \\frac{1-e^{-t}}{t} dt$. For large arguments, which is a reasonable assumption for the optical depths that define the transit radius, this function has the asymptotic expansion $\\text{Ein}(x) \\approx \\gamma + \\ln(x)$, where $\\gamma \\approx 0.5772$ is the Euler-Mascheroni constant. This constant arises from the integral and must be included.\nThus, the integral evaluates to $H(\\gamma + \\ln(\\tau_0(\\lambda)))$.\nSubstituting this back into the expression for $R^2(\\lambda)$:\n$$\nR^2(\\lambda) - R_0^2 \\approx 2R_0 H (\\gamma + \\ln(\\tau_0(\\lambda)))\n$$\n$$\nR^2(\\lambda) \\approx R_0^2 \\left[1 + \\frac{2H}{R_0}(\\gamma + \\ln(\\tau_0(\\lambda)))\\right]\n$$\nTaking the square root and using the first-order binomial approximation $(1+x)^{1/2} \\approx 1 + x/2$ for small $x = \\frac{2H}{R_0}(\\dots)$, which is valid in the thin-atmosphere limit $H \\ll R_0$:\n$$\nR(\\lambda) \\approx R_0 \\left[1 + \\frac{1}{2} \\frac{2H}{R_0}(\\gamma + \\ln(\\tau_0(\\lambda)))\\right] = R_0 + H(\\gamma + \\ln(\\tau_0(\\lambda)))\n$$\nFinally, we substitute the expressions for $H$, $n_0$, and $\\tau_0(\\lambda)$ to obtain the final closed-form result for $R(\\lambda)$:\n$$\nR(\\lambda) = R_0 + H \\left(\\gamma + \\ln\\left(\\xi \\sigma_{\\lambda} n_0 \\sqrt{2\\pi R_0 H}\\right)\\right)\n$$\n$$\nR(\\lambda) = R_0 + \\frac{k_B T}{\\mu g} \\left(\\gamma + \\ln\\left(\\xi \\sigma_{\\lambda} \\frac{P_0}{k_B T} \\sqrt{2\\pi R_0 \\frac{k_B T}{\\mu g}}\\right)\\right)\n$$\nThis expression provides the wavelength-dependent effective transit radius in terms of the specified physical parameters and fundamental constants.\n\n**Limitations of the Model**\n\nThe derived forward model, while analytically convenient, relies on several strong simplifications. Failure to meet these assumptions in a real planetary atmosphere can introduce significant biases in the parameters retrieved from observational data. Four prominent limitations are:\n\n1.  **Isothermal Assumption:** Real planetary atmospheres exhibit temperature-pressure profiles that vary with altitude due to radiative heating/cooling and dynamics. An isothermal model assigns a single temperature $T$ to the entire atmosphere, causing the scale height $H$ to be constant. In reality, $H$ is a function of altitude, $H(r)$. For example, a stratosphere with a temperature inversion would make the atmosphere expand more at higher altitudes than an isothermal model predicts. Fitting data from such an atmosphere with an isothermal model would lead to biased retrievals, potentially overestimating absorber abundances or mischaracterizing the reference pressure level.\n\n2.  **Constant Volume Mixing Ratio ($\\xi$):** The model assumes the absorbing species is uniformly mixed throughout the atmosphere. In reality, mixing ratios change with altitude due to processes like photochemistry (e.g., dissociation of molecules by stellar UV radiation at high altitudes), condensation (e.g., formation of water ice clouds at a cold trap), and diffusive separation (where lighter species become enhanced at the top of the atmosphere). Using a constant-$\\xi$ model for an atmosphere with a vertically varying composition will result in a retrieved abundance that is a complex, altitude-weighted average that may not be representative of any specific atmospheric layer, biasing our understanding of the planet's atmospheric chemistry.\n\n3.  **Pressure and Temperature Independent Cross-Sections ($\\sigma_{\\lambda}$):** The absorption cross-section of a molecule is not constant; it depends on the local temperature and pressure. These effects broaden the spectral lines, altering their shape. Pressure (or collisional) broadening dominates in the lower, denser regions of the atmosphere, while thermal (Doppler) broadening is more significant in the upper, hotter, and more rarefied regions. By ignoring line broadening, the model misrepresents the shape of spectral features. This can lead to significant errors in the retrieved temperature profile and abundances, as the model cannot correctly interpret the information contained in the wings of strong absorption lines.\n\n4.  **Neglect of Clouds and Hazes:** The model only accounts for gaseous absorption. Real exoplanetary atmospheres frequently contain aerosols, such as clouds (condensates) or photochemical hazes. These particles provide a significant source of opacity, which is typically a smooth function of wavelength and can obscure the gaseous absorption features below a certain altitude. Neglecting aerosols when they are present forces the retrieval model to misinterpret the muted spectral features, often leading to a severe underestimation of gaseous abundances and a retrieved temperature profile that is erroneously flattened (more isothermal). This is a well-known degeneracy in atmospheric retrieval.",
            "answer": "$$\\boxed{R_0 + \\frac{k_B T}{\\mu g} \\left(\\gamma + \\ln\\left(\\xi \\sigma_{\\lambda} \\frac{P_0}{k_B T} \\sqrt{2\\pi R_0 \\frac{k_B T}{\\mu g}}\\right)\\right)}$$"
        },
        {
            "introduction": "Modern astronomical studies frequently combine datasets from multiple instruments to achieve broader wavelength coverage and improved signal-to-noise. This practice moves from theory to a realistic data analysis scenario, demonstrating how to perform a joint Bayesian inference. You will learn to construct a unified model that incorporates shared atmospheric parameters and instrument-specific nuisance parameters, correctly accounting for different noise properties to derive robust, combined constraints .",
            "id": "4154226",
            "problem": "Consider a simplified transmission spectroscopy atmospheric retrieval for an exoplanet, in which an observer collects two independent datasets from two instruments with distinct noise covariances. The signal in each dataset is modeled by a linearized forward model around a reference atmospheric state, consistent with standard first-order sensitivity analysis in radiative transfer. Let the shared atmospheric state vector be $\\mathbf{x} \\in \\mathbb{R}^2$ with components $\\mathbf{x} = [T,\\ \\ln q]^\\top$, where $T$ is temperature in kelvin and $\\ln q$ is the natural logarithm of the volume mixing ratio (dimensionless). Each instrument $i \\in \\{1,2\\}$ has an instrument-specific nuisance parameter $w_i \\in \\mathbb{R}$ representing a scalar baseline offset (dimensionless) that is constant across its spectral channels. For dataset $i$, suppose the observation vector $\\mathbf{y}_i \\in \\mathbb{R}^{m_i}$ is related to the parameters by\n$$\n\\mathbf{y}_i = \\mathbf{J}_i \\mathbf{x} + \\mathbf{L}_i w_i + \\boldsymbol{\\epsilon}_i,\n$$\nwhere $\\mathbf{J}_i \\in \\mathbb{R}^{m_i \\times 2}$ is the linearized Jacobian of the transit depth with respect to the atmospheric parameters $(T,\\ \\ln q)$, $\\mathbf{L}_i \\in \\mathbb{R}^{m_i \\times 1}$ maps the scalar offset to all channels (a column of ones), and $\\boldsymbol{\\epsilon}_i \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_i)$ is Gaussian noise with known covariance $\\boldsymbol{\\Sigma}_i$. Assume the datasets are conditionally independent given the parameters, so the joint likelihood factorizes.\n\nAdopt a Gaussian prior on all parameters,\n$$\n\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_x,\\ \\boldsymbol{\\Sigma}_x), \\quad w_1 \\sim \\mathcal{N}(\\mu_{w_1},\\ \\sigma_{w_1}^2), \\quad w_2 \\sim \\mathcal{N}(\\mu_{w_2},\\ \\sigma_{w_2}^2),\n$$\nwith block-diagonal joint prior covariance and concatenated prior mean for the parameter vector $\\boldsymbol{\\theta} = [T,\\ \\ln q,\\ w_1,\\ w_2]^\\top \\in \\mathbb{R}^4$ given by\n$$\n\\boldsymbol{\\mu}_0 = \\begin{bmatrix}\\boldsymbol{\\mu}_x \\\\ \\mu_{w_1} \\\\ \\mu_{w_2}\\end{bmatrix}, \\quad \\mathbf{S}_0 = \\mathrm{diag}(\\boldsymbol{\\Sigma}_x,\\ \\sigma_{w_1}^2,\\ \\sigma_{w_2}^2).\n$$\nLet the concatenated observation vector be $\\mathbf{y} = [\\mathbf{y}_1^\\top,\\ \\mathbf{y}_2^\\top]^\\top \\in \\mathbb{R}^{m_1+m_2}$. The global design matrix that maps $\\boldsymbol{\\theta}$ to $\\mathbf{y}$ is\n$$\n\\mathbf{H} \\in \\mathbb{R}^{(m_1+m_2)\\times 4}, \\quad\n\\mathbf{H} = \\begin{bmatrix}\n\\mathbf{J}_1 & \\mathbf{L}_1 & \\mathbf{0}_{m_1\\times 1} \\\\\n\\mathbf{J}_2 & \\mathbf{0}_{m_2\\times 1} & \\mathbf{L}_2\n\\end{bmatrix}.\n$$\nThe joint noise covariance is block-diagonal,\n$$\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n\\boldsymbol{\\Sigma}_1 & \\mathbf{0} \\\\\n\\mathbf{0} & \\boldsymbol{\\Sigma}_2\n\\end{bmatrix}.\n$$\nUnder these assumptions, the joint posterior for $\\boldsymbol{\\theta}$ is Gaussian. Your task is to compute the joint posterior mean and covariance, and then report the marginal constraints (posterior mean and standard deviation) for each component of $\\boldsymbol{\\theta}$.\n\nFundamental base to be used:\n- Bayes’ rule with Gaussian prior and Gaussian likelihood, yielding a Gaussian posterior for linear models.\n- Properties of multivariate normal distributions, particularly the information form and conjugacy.\n\nMathematical objective:\n- Derive and implement the posterior precision and mean for the linear-Gaussian model,\n$$\n\\mathbf{J}_{\\text{post}} = \\mathbf{S}_0^{-1} + \\mathbf{H}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{H}, \\quad\n\\boldsymbol{\\mu}_{\\text{post}} = \\mathbf{J}_{\\text{post}}^{-1}\\left(\\mathbf{S}_0^{-1}\\boldsymbol{\\mu}_0 + \\mathbf{H}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{y}\\right),\n$$\nand extract marginal variances from $\\mathbf{J}_{\\text{post}}^{-1}$.\n\nUnits and reporting requirements:\n- Report the posterior mean and standard deviation for $T$ in kelvin.\n- Report the posterior mean and standard deviation for $\\ln q$ as dimensionless values.\n- Report the posterior mean and standard deviation for $w_1$ and $w_2$ as dimensionless values.\n- All reported quantities must be floats. No angle units are involved. No percentages are involved.\n\nTest suite and parameter specification:\nUse the following three test cases, each with $m_1 = 3$ channels for dataset $1$ and $m_2 = 4$ channels for dataset $2$. In all cases, $\\mathbf{L}_1$ and $\\mathbf{L}_2$ are columns of ones of lengths $m_1$ and $m_2$, respectively.\n\nShared prior for all cases:\n- $\\boldsymbol{\\mu}_x = [1000,\\ -5]^\\top$ with units $[\\,\\mathrm{K},\\ \\text{dimensionless}\\,]$.\n- $\\boldsymbol{\\Sigma}_x = \\mathrm{diag}([100^2,\\ 1^2])$.\n- $\\mu_{w_1} = 0$ with $\\sigma_{w_1} = 0.01$.\n- $\\mu_{w_2} = 0$ with $\\sigma_{w_2} = 0.02$.\n\nCase $1$ (balanced noise with moderate correlation in dataset $2$):\n- $\\mathbf{y}_1 = [0.01225,\\ 0.01080,\\ 0.00950]^\\top$.\n- $\\mathbf{y}_2 = [0.01360,\\ 0.01310,\\ 0.00990,\\ 0.00920]^\\top$.\n- $\\mathbf{J}_1 = \\begin{bmatrix}\n1.2\\times 10^{-5} & 6.0\\times 10^{-4} \\\\\n1.0\\times 10^{-5} & 5.0\\times 10^{-4} \\\\\n0.8\\times 10^{-5} & 4.0\\times 10^{-4}\n\\end{bmatrix}$.\n- $\\mathbf{J}_2 = \\begin{bmatrix}\n1.5\\times 10^{-5} & 7.0\\times 10^{-4} \\\\\n1.4\\times 10^{-5} & 6.0\\times 10^{-4} \\\\\n1.0\\times 10^{-5} & 5.0\\times 10^{-4} \\\\\n0.9\\times 10^{-5} & 4.5\\times 10^{-4}\n\\end{bmatrix}$.\n- $\\boldsymbol{\\Sigma}_1 = \\mathrm{diag}([1.0\\times 10^{-6},\\ 2.0\\times 10^{-6},\\ 1.5\\times 10^{-6}])$.\n- $\\boldsymbol{\\Sigma}_2$ constructed from variances $[1.2\\times 10^{-6},\\ 1.2\\times 10^{-6},\\ 1.5\\times 10^{-6},\\ 1.5\\times 10^{-6}]$ and correlation coefficient $\\rho = 0.6$ via\n$$\n(\\boldsymbol{\\Sigma}_2)_{ij} = \\rho^{|i-j|}\\,\\sqrt{v_i v_j}, \\quad \\text{with } v_i \\text{ the specified variances}.\n$$\n\nCase $2$ (dataset $2$ has high noise with weak correlation):\n- $\\mathbf{y}_1 = [0.01200,\\ 0.01070,\\ 0.00940]^\\top$.\n- $\\mathbf{y}_2 = [0.01380,\\ 0.01300,\\ 0.00970,\\ 0.00910]^\\top$.\n- $\\mathbf{J}_1$ and $\\mathbf{J}_2$ identical to Case $1$.\n- $\\boldsymbol{\\Sigma}_1 = \\mathrm{diag}([1.0\\times 10^{-6},\\ 2.0\\times 10^{-6},\\ 1.5\\times 10^{-6}])$.\n- $\\boldsymbol{\\Sigma}_2$ constructed from variances $[4.0\\times 10^{-6},\\ 4.0\\times 10^{-6},\\ 3.5\\times 10^{-6},\\ 3.5\\times 10^{-6}]$ and correlation coefficient $\\rho = 0.3$ via the same formula.\n\nCase $3$ (dataset $2$ strongly correlated and low noise; dataset $1$ low noise):\n- $\\mathbf{y}_1 = [0.01230,\\ 0.01060,\\ 0.00960]^\\top$.\n- $\\mathbf{y}_2 = [0.01350,\\ 0.01320,\\ 0.00980,\\ 0.00930]^\\top$.\n- $\\mathbf{J}_1$ and $\\mathbf{J}_2$ identical to Case $1$.\n- $\\boldsymbol{\\Sigma}_1 = \\mathrm{diag}([1.0\\times 10^{-6},\\ 1.0\\times 10^{-6},\\ 1.0\\times 10^{-6}])$.\n- $\\boldsymbol{\\Sigma}_2$ constructed from variances $[1.0\\times 10^{-6},\\ 1.0\\times 10^{-6},\\ 1.0\\times 10^{-6},\\ 1.0\\times 10^{-6}]$ and correlation coefficient $\\rho = 0.95$ via the same formula.\n\nImplementation instructions:\n- Build $\\mathbf{H}$ and $\\boldsymbol{\\Sigma}$ for each case exactly as specified.\n- Compute $\\mathbf{J}_{\\text{post}}$ and $\\boldsymbol{\\mu}_{\\text{post}}$ using the information form. Compute the posterior covariance $\\mathbf{C}_{\\text{post}} = \\mathbf{J}_{\\text{post}}^{-1}$.\n- Extract the marginal posterior mean and standard deviation for each parameter in the order $[T,\\ \\ln q,\\ w_1,\\ w_2]$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The result for each test case must be a list of four parameter summaries ordered as $[T,\\ \\ln q,\\ w_1,\\ w_2]$, and each parameter summary must itself be a two-element list $[\\text{mean},\\ \\text{standard deviation}]$.\n- Therefore, the final output must look like\n$$\n[\\ [\\mu_T,\\ \\sigma_T,\\ \\mu_{\\ln q},\\ \\sigma_{\\ln q},\\ \\mu_{w_1},\\ \\sigma_{w_1},\\ \\mu_{w_2},\\ \\sigma_{w_2}],\\ \\ldots\\ ]\n$$\naggregated over the three cases, with all values as floats. Express $T$ in $\\mathrm{K}$ and all other quantities as dimensionless numbers. No additional text should be printed.",
            "solution": "The problem requires the computation of the posterior distribution for a set of parameters in a simplified atmospheric retrieval model. The model is linear and the noise is assumed to be Gaussian, and the prior distribution on the parameters is also Gaussian. This setup is a classic example of Bayesian linear regression, for which the posterior distribution is also Gaussian and can be derived analytically.\n\nThe overall model combines two datasets and can be expressed in a compact, global form. Let the full parameter vector be $\\boldsymbol{\\theta} = [T, \\ln q, w_1, w_2]^\\top \\in \\mathbb{R}^4$. The concatenated observation vector $\\mathbf{y} = [\\mathbf{y}_1^\\top, \\mathbf{y}_2^\\top]^\\top \\in \\mathbb{R}^{m_1+m_2}$ is related to the parameters via the linear model:\n$$\n\\mathbf{y} = \\mathbf{H}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}\n$$\nwhere $\\mathbf{H}$ is the global design matrix that maps the parameters to the observations, and $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$ represents the concatenated Gaussian noise with a block-diagonal covariance matrix $\\boldsymbol{\\Sigma}$.\n\nBayes' rule states that the posterior probability density is proportional to the product of the likelihood and the prior:\n$$\np(\\boldsymbol{\\theta} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\n$$\nIn this problem, both the likelihood, derived from the linear model with Gaussian noise, and the prior are multivariate normal distributions.\nThe likelihood is $p(\\mathbf{y} | \\boldsymbol{\\theta}) = \\mathcal{N}(\\mathbf{y}; \\mathbf{H}\\boldsymbol{\\theta}, \\boldsymbol{\\Sigma})$, with probability density function proportional to:\n$$\n\\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\mathbf{H}\\boldsymbol{\\theta})\\right)\n$$\nThe prior is $p(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta}; \\boldsymbol{\\mu}_0, \\mathbf{S}_0)$, with probability density function proportional to:\n$$\n\\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)^\\top \\mathbf{S}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)\\right)\n$$\nThe product of these two functions yields the posterior, which will also be a Gaussian, $\\mathcal{N}(\\boldsymbol{\\theta}; \\boldsymbol{\\mu}_{\\text{post}}, \\mathbf{C}_{\\text{post}})$. The argument of the exponential for the posterior is the sum of the arguments from the likelihood and prior. By expanding these quadratic forms and collecting terms, one can identify the posterior precision matrix $\\mathbf{J}_{\\text{post}} = \\mathbf{C}_{\\text{post}}^{-1}$ and the posterior mean $\\boldsymbol{\\mu}_{\\text{post}}$. The result of this well-known derivation is:\n$$\n\\mathbf{J}_{\\text{post}} = \\mathbf{S}_0^{-1} + \\mathbf{H}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{H}\n$$\n$$\n\\boldsymbol{\\mu}_{\\text{post}} = \\mathbf{J}_{\\text{post}}^{-1} \\left( \\mathbf{S}_0^{-1}\\boldsymbol{\\mu}_0 + \\mathbf{H}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{y} \\right)\n$$\nThese are the formulas provided in the problem statement, which we will implement. The quantity $\\mathbf{S}_0^{-1}$ is the prior precision matrix, and $\\mathbf{H}^\\top \\boldsymbol{\\Sigma}^{-1} \\mathbf{H}$ is the precision (or Fisher information) contributed by the data. The posterior precision is simply the sum of the prior and data precisions.\n\nThe computational procedure for each test case is as follows:\n$1$. Construct the prior mean vector $\\boldsymbol{\\mu}_0 \\in \\mathbb{R}^4$ and the prior covariance matrix $\\mathbf{S}_0 \\in \\mathbb{R}^{4 \\times 4}$. The prior covariance $\\mathbf{S}_0$ is diagonal, so its inverse, the prior precision $\\mathbf{S}_0^{-1}$, is easily found by taking the reciprocal of its diagonal elements.\n$2$. Construct the global observation vector $\\mathbf{y} \\in \\mathbb{R}^{7}$ by concatenating the observation vectors $\\mathbf{y}_1 \\in \\mathbb{R}^3$ and $\\mathbf{y}_2 \\in \\mathbb{R}^4$.\n$3$. Construct the global design matrix $\\mathbf{H} \\in \\mathbb{R}^{7 \\times 4}$ by assembling the Jacobians $\\mathbf{J}_1$ and $\\mathbf{J}_2$ and the offset mapping vectors $\\mathbf{L}_1$ and $\\mathbf{L}_2$ into the specified block structure.\n$4$. Construct the global noise covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{7 \\times 7}$. This is a block-diagonal matrix composed of $\\boldsymbol{\\Sigma}_1 \\in \\mathbb{R}^{3 \\times 3}$ and $\\boldsymbol{\\Sigma}_2 \\in \\mathbb{R}^{4 \\times 4}$. For each case, $\\boldsymbol{\\Sigma}_1$ is given as a diagonal matrix. The matrix $\\boldsymbol{\\Sigma}_2$ must be constructed from the given channel variances $v_i$ and correlation coefficient $\\rho$ using the formula $(\\boldsymbol{\\Sigma}_2)_{ij} = \\rho^{|i-j|}\\sqrt{v_i v_j}$.\n$5$. Compute the inverse of the noise covariance matrix, $\\boldsymbol{\\Sigma}^{-1}$. Since $\\boldsymbol{\\Sigma}$ is block-diagonal, its inverse is also block-diagonal with blocks $\\boldsymbol{\\Sigma}_1^{-1}$ and $\\boldsymbol{\\Sigma}_2^{-1}$.\n$6$. Compute the posterior precision matrix $\\mathbf{J}_{\\text{post}}$ and the posterior covariance matrix $\\mathbf{C}_{\\text{post}} = \\mathbf{J}_{\\text{post}}^{-1}$.\n$7$. Compute the posterior mean vector $\\boldsymbol{\\mu}_{\\text{post}}$.\n$8$. Extract the marginal posterior means and standard deviations. The means are the elements of $\\boldsymbol{\\mu}_{\\text{post}}$. The variances are the diagonal elements of $\\mathbf{C}_{\\text{post}}$, and the standard deviations are their square roots.\n\nThis procedure is applied to each of the three test cases, and the results are reported in the specified format. All matrix operations are performed using numerical linear algebra routines.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian atmospheric retrieval problem for the three specified test cases.\n    \"\"\"\n    # Shared prior parameters for all cases\n    mu_x = np.array([1000.0, -5.0])\n    Sigma_x = np.diag([100.0**2, 1.0**2])\n    mu_w1, sigma_w1 = 0.0, 0.01\n    mu_w2, sigma_w2 = 0.0, 0.02\n    \n    mu0 = np.array([mu_x[0], mu_x[1], mu_w1, mu_w2])\n    S0 = np.block([\n        [Sigma_x, np.zeros((2, 2))],\n        [np.zeros((2, 2)), np.diag([sigma_w1**2, sigma_w2**2])]\n    ])\n    \n    # Test cases definition\n    J1 = np.array([\n        [1.2e-5, 6.0e-4],\n        [1.0e-5, 5.0e-4],\n        [0.8e-5, 4.0e-4]\n    ])\n    \n    J2 = np.array([\n        [1.5e-5, 7.0e-4],\n        [1.4e-5, 6.0e-4],\n        [1.0e-5, 5.0e-4],\n        [0.9e-5, 4.5e-4]\n    ])\n\n    test_cases = [\n        {\n            \"y1\": np.array([0.01225, 0.01080, 0.00950]),\n            \"y2\": np.array([0.01360, 0.01310, 0.00990, 0.00920]),\n            \"Sigma1_vars\": np.array([1.0e-6, 2.0e-6, 1.5e-6]),\n            \"Sigma2_vars\": np.array([1.2e-6, 1.2e-6, 1.5e-6, 1.5e-6]),\n            \"Sigma2_rho\": 0.6\n        },\n        {\n            \"y1\": np.array([0.01200, 0.01070, 0.00940]),\n            \"y2\": np.array([0.01380, 0.01300, 0.00970, 0.00910]),\n            \"Sigma1_vars\": np.array([1.0e-6, 2.0e-6, 1.5e-6]),\n            \"Sigma2_vars\": np.array([4.0e-6, 4.0e-6, 3.5e-6, 3.5e-6]),\n            \"Sigma2_rho\": 0.3\n        },\n        {\n            \"y1\": np.array([0.01230, 0.01060, 0.00960]),\n            \"y2\": np.array([0.01350, 0.01320, 0.00980, 0.00930]),\n            \"Sigma1_vars\": np.array([1.0e-6, 1.0e-6, 1.0e-6]),\n            \"Sigma2_vars\": np.array([1.0e-6, 1.0e-6, 1.0e-6, 1.0e-6]),\n            \"Sigma2_rho\": 0.95\n        }\n    ]\n\n    def construct_sigma2(variances, rho):\n        \"\"\"Constructs the correlated covariance matrix Sigma_2.\"\"\"\n        n = len(variances)\n        stds = np.sqrt(variances)\n        sigma2 = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                sigma2[i, j] = (rho**abs(i - j)) * stds[i] * stds[j]\n        return sigma2\n\n    def calculate_posterior(case_data):\n        \"\"\"\n        Calculates the posterior mean and standard deviation for a given case.\n        \"\"\"\n        m1, m2 = 3, 4\n        \n        # 1. Construct prior-related terms\n        S0_inv = np.linalg.inv(S0)\n        S0_inv_mu0 = S0_inv @ mu0\n        \n        # 2. Construct global vectors and matrices\n        y = np.concatenate((case_data[\"y1\"], case_data[\"y2\"]))\n        \n        L1 = np.ones((m1, 1))\n        L2 = np.ones((m2, 1))\n        H = np.block([\n            [J1, L1, np.zeros((m1, 1))],\n            [J2, np.zeros((m2, 1)), L2]\n        ])\n        \n        Sigma1 = np.diag(case_data[\"Sigma1_vars\"])\n        Sigma2 = construct_sigma2(case_data[\"Sigma2_vars\"], case_data[\"Sigma2_rho\"])\n        \n        Sigma = np.zeros((m1 + m2, m1 + m2))\n        Sigma[:m1, :m1] = Sigma1\n        Sigma[m1:, m1:] = Sigma2\n        \n        # 3. Perform Bayesian update\n        Sigma_inv = np.linalg.inv(Sigma)\n        H_T_Sigma_inv = H.T @ Sigma_inv\n        \n        J_post = S0_inv + H_T_Sigma_inv @ H\n        C_post = np.linalg.inv(J_post)\n        \n        mu_post = C_post @ (S0_inv_mu0 + H_T_Sigma_inv @ y)\n        \n        # 4. Extract marginals\n        posterior_stds = np.sqrt(np.diag(C_post))\n        \n        return mu_post, posterior_stds\n\n    results_strings = []\n    for case in test_cases:\n        means, stds = calculate_posterior(case)\n        case_result_values = []\n        for i in range(len(means)):\n            case_result_values.append(means[i])\n            case_result_values.append(stds[i])\n        \n        # Format the result for this case into a string \"[v1,v2,...]\"\n        case_string = f\"[{','.join(str(v) for v in case_result_values)}]\"\n        results_strings.append(case_string)\n\n    # Format the final output as a single string \"[[...],[...],[...]]\"\n    print(f\"[{','.join(results_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond interpreting existing data, a key strength of the Bayesian framework is its ability to guide future research through quantitative experimental design. This exercise introduces one of the field's core concepts: Expected Information Gain (EIG), which measures the scientific value of a proposed observation before it is taken. By calculating the EIG for several potential instrument configurations, you will practice making an optimal, data-driven decision to maximize the scientific return of future observations .",
            "id": "4154230",
            "problem": "You are tasked with designing an algorithmic recommender for instrument modes in exoplanet transmission spectroscopy based on Bayesian inference for atmospheric retrieval. Consider an isothermal, hydrostatic atmosphere where the band-averaged transit depth is modeled after a linearization of Beer–Lambert absorption and path-geometry around a nominal state. The retrieval parameters are the cloud-top pressure and molecular mixing ratios. Specifically, define the parameter vector as $$\\boldsymbol{\\theta} \\in \\mathbb{R}^3,$$ with components $$\\boldsymbol{\\theta} = [\\log_{10} P_c,\\ \\log_{10} X_{\\mathrm{H_2O}},\\ \\log_{10} X_{\\mathrm{CH_4}}],$$ which are unitless. The measurement model for instrument mode $$m$$ is a linear Gaussian approximation of the forward model: $$\\mathbf{y}_m = \\mathbf{A}_m \\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}_m,$$ where $$\\mathbf{A}_m$$ is the sensitivity matrix derived from a first-order Taylor expansion of the radiative transfer around the nominal retrieval point, and $$\\boldsymbol{\\varepsilon}_m \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_{\\mathrm{n},m})$$ is zero-mean Gaussian measurement noise with covariance $$\\boldsymbol{\\Sigma}_{\\mathrm{n},m}$$. Assume a Gaussian prior $$\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}_{\\mathrm{p}}).$$\n\nStarting from Bayes' theorem and the definition of differential entropy, compute the Expected Information Gain (EIG) for each instrument mode as the expected reduction in the entropy of $$\\boldsymbol{\\theta}$$ upon observing $$\\mathbf{y}_m$$ under the linear-Gaussian assumptions. Use natural logarithms so the EIG is expressed in nats. Then recommend the mode with the maximal EIG for each test case.\n\nYour program must implement the computation in a numerically stable manner without explicit matrix inversion wherever possible. Use Cholesky factorizations and linear solves. The final program must output a single line containing all results for all test cases in a comma-separated list enclosed in square brackets, as specified below.\n\nTest suite:\n\nTest Case $$1$$ (happy path, multiple channels with diagonal noise):\n- Prior mean $$\\boldsymbol{\\mu}_1 = [-2.0,\\ -4.0,\\ -5.0]$$.\n- Prior covariance $$\\boldsymbol{\\Sigma}_{\\mathrm{p},1} = \\begin{bmatrix} 0.30 & 0.08 & 0.02 \\\\ 0.08 & 0.50 & 0.10 \\\\ 0.02 & 0.10 & 0.40 \\end{bmatrix}.$$\n- Mode $$0$$:\n  - $$\\mathbf{A}_{0} = \\begin{bmatrix} -0.8 & 1.2 & 0.1 \\\\ -0.5 & 0.9 & 0.5 \\end{bmatrix}.$$\n  - $$\\boldsymbol{\\Sigma}_{\\mathrm{n},0} = \\operatorname{diag}([400,\\ 625]).$$\n- Mode $$1$$:\n  - $$\\mathbf{A}_{1} = \\begin{bmatrix} -0.7 & 1.0 & 0.2 \\\\ -0.6 & 0.4 & 1.1 \\\\ -0.2 & 0.7 & 0.3 \\end{bmatrix}.$$\n  - $$\\boldsymbol{\\Sigma}_{\\mathrm{n},1} = \\operatorname{diag}([225,\\ 225,\\ 225]).$$\n- Mode $$2$$:\n  - $$\\mathbf{A}_{2} = \\begin{bmatrix} -0.9 & 1.1 & 0.3 \\\\ -0.4 & 0.8 & 0.6 \\\\ -0.3 & 0.5 & 0.4 \\\\ -0.1 & 0.2 & 0.1 \\end{bmatrix}.$$\n  - $$\\boldsymbol{\\Sigma}_{\\mathrm{n},2} = \\operatorname{diag}([625,\\ 625,\\ 625,\\ 625]).$$\n\nTest Case $$2$$ (boundary case with an extremely noisy mode):\n- Prior mean $$\\boldsymbol{\\mu}_2 = [-1.5,\\ -3.8,\\ -5.2]$$.\n- Prior covariance $$\\boldsymbol{\\Sigma}_{\\mathrm{p},2} = \\begin{bmatrix} 1.0 & 0.2 & 0.0 \\\\ 0.2 & 1.2 & 0.1 \\\\ 0.0 & 0.1 & 0.8 \\end{bmatrix}.$$\n- Mode $$0$$ (extremely noisy):\n  - $$\\mathbf{A}_{0} = \\begin{bmatrix} -0.5 & 0.9 & 0.4 \\\\ -0.4 & 0.7 & 0.3 \\\\ -0.3 & 0.6 & 0.2 \\end{bmatrix}.$$\n  - $$\\boldsymbol{\\Sigma}_{\\mathrm{n},0} = \\operatorname{diag}([10000,\\ 10000,\\ 10000]).$$\n- Mode $$1$$ (higher precision, fewer channels):\n  - $$\\mathbf{A}_{1} = \\begin{bmatrix} -0.6 & 1.1 & 0.0 \\\\ -0.2 & 0.4 & 1.2 \\end{bmatrix}.$$\n  - $$\\boldsymbol{\\Sigma}_{\\mathrm{n},1} = \\operatorname{diag}([100,\\ 144]).$$\n\nTest Case $$3$$ (correlated noise across channels):\n- Prior mean $$\\boldsymbol{\\mu}_3 = [-2.2,\\ -4.1,\\ -5.1]$$.\n- Prior covariance $$\\boldsymbol{\\Sigma}_{\\mathrm{p},3} = \\begin{bmatrix} 0.4 & 0.05 & 0.05 \\\\ 0.05 & 0.6 & 0.0 \\\\ 0.05 & 0.0 & 0.5 \\end{bmatrix}.$$\n- Mode $$0$$:\n  - $$\\mathbf{A}_{0} = \\begin{bmatrix} -0.6 & 0.8 & 0.6 \\\\ -0.1 & 0.2 & 1.0 \\\\ -0.4 & 0.7 & 0.1 \\end{bmatrix}.$$\n  - $$\\boldsymbol{\\Sigma}_{\\mathrm{n},0} = \\begin{bmatrix} 400 & 120 & 150 \\\\ 120 & 900 & 75 \\\\ 150 & 75 & 625 \\end{bmatrix}.$$\n- Mode $$1$$:\n  - $$\\mathbf{A}_{1} = \\begin{bmatrix} -0.8 & 0.9 & 0.3 \\\\ -0.3 & 0.5 & 0.9 \\\\ -0.2 & 0.4 & 0.2 \\end{bmatrix}.$$\n  - $$\\boldsymbol{\\Sigma}_{\\mathrm{n},1} = \\begin{bmatrix} 225 & 75 & 0 \\\\ 75 & 400 & 0 \\\\ 0 & 0 & 225 \\end{bmatrix}.$$\n\nYour tasks:\n- For each test case, compute the EIG in nats for each candidate instrument mode under the linear-Gaussian retrieval approximation using natural logarithms.\n- Recommend the optimal mode as the one with maximal EIG for that test case.\n- The final output must be a single line containing a comma-separated list enclosed in square brackets. For Test Case $$1$$, list the EIG values for Mode $$0$$, Mode $$1$$, and Mode $$2$$, followed by the integer index (using $$0$$-based indexing) of the optimal mode. For Test Case $$2$$, list the EIG values for Mode $$0$$ and Mode $$1$$, followed by the optimal index. For Test Case $$3$$, list the EIG values for Mode $$0$$ and Mode $$1$$, followed by the optimal index. The overall output concatenates these segments in order, resulting in $$[EIG_{1,0},EIG_{1,1},EIG_{1,2},i_1,EIG_{2,0},EIG_{2,1},i_2,EIG_{3,0},EIG_{3,1},i_3]$$ where $$EIG_{t,m}$$ is the EIG for test case $$t$$ and mode $$m$$, and $$i_t$$ is the recommended mode index for test case $$t$$.\n\nAll EIG values are unitless and must be expressed in nats. No physical unit conversions are required. Angles do not appear. Percentages do not appear.",
            "solution": "The stated problem is to design a recommender for instrument modes in exoplanet transmission spectroscopy. The recommendation criterion is the Expected Information Gain (EIG), computed within a Bayesian framework under a linear-Gaussian approximation of the atmospheric retrieval problem.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Parameter Vector**: $$\\boldsymbol{\\theta} \\in \\mathbb{R}^3$$ representing $$[\\log_{10} P_c,\\ \\log_{10} X_{\\mathrm{H_2O}},\\ \\log_{10} X_{\\mathrm{CH_4}}]$$.\n- **Prior Distribution**: The parameters are assumed to follow a Gaussian prior, $$\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}_{\\mathrm{p}})$$.\n- **Measurement Model (Likelihood)**: For each instrument mode $$m$$, the measurement vector $$\\mathbf{y}_m$$ is modeled as a linear function of the parameters with additive Gaussian noise: $$\\mathbf{y}_m = \\mathbf{A}_m \\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}_m$$, where $$\\boldsymbol{\\varepsilon}_m \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_{\\mathrm{n},m})$$. This defines the likelihood as $$p(\\mathbf{y}_m | \\boldsymbol{\\theta}) = \\mathcal{N}(\\mathbf{y}_m; \\mathbf{A}_m \\boldsymbol{\\theta}, \\boldsymbol{\\Sigma}_{\\mathrm{n},m})$$.\n- **Objective**: Compute the Expected Information Gain (EIG) for each mode, defined as the expected reduction in the entropy of $$\\boldsymbol{\\theta}$$ upon observing $$\\mathbf{y}_m$$. The result should be in nats (using natural logarithms). The mode with the maximal EIG is to be recommended.\n- **Numerical Constraint**: The implementation should avoid explicit matrix inversion and use Cholesky factorizations for numerical stability.\n\n- **Test Case 1**:\n    - $$\\boldsymbol{\\mu}_1 = [-2.0,\\ -4.0,\\ -5.0]$$.\n    - $$\\boldsymbol{\\Sigma}_{\\mathrm{p},1} = \\begin{bmatrix} 0.30 & 0.08 & 0.02 \\\\ 0.08 & 0.50 & 0.10 \\\\ 0.02 & 0.10 & 0.40 \\end{bmatrix}$$.\n    - Mode $$0$$: $$\\mathbf{A}_{0} = \\begin{bmatrix} -0.8 & 1.2 & 0.1 \\\\ -0.5 & 0.9 & 0.5 \\end{bmatrix}$$, $$\\boldsymbol{\\Sigma}_{\\mathrm{n},0} = \\operatorname{diag}([400,\\ 625])$$.\n    - Mode $$1$$: $$\\mathbf{A}_{1} = \\begin{bmatrix} -0.7 & 1.0 & 0.2 \\\\ -0.6 & 0.4 & 1.1 \\\\ -0.2 & 0.7 & 0.3 \\end{bmatrix}$$, $$\\boldsymbol{\\Sigma}_{\\mathrm{n},1} = \\operatorname{diag}([225,\\ 225,\\ 225])$$.\n    - Mode $$2$$: $$\\mathbf{A}_{2} = \\begin{bmatrix} -0.9 & 1.1 & 0.3 \\\\ -0.4 & 0.8 & 0.6 \\\\ -0.3 & 0.5 & 0.4 \\\\ -0.1 & 0.2 & 0.1 \\end{bmatrix}$$, $$\\boldsymbol{\\Sigma}_{\\mathrm{n},2} = \\operatorname{diag}([625,\\ 625,\\ 625,\\ 625])$$.\n\n- **Test Case 2**:\n    - $$\\boldsymbol{\\mu}_2 = [-1.5,\\ -3.8,\\ -5.2]$$.\n    - $$\\boldsymbol{\\Sigma}_{\\mathrm{p},2} = \\begin{bmatrix} 1.0 & 0.2 & 0.0 \\\\ 0.2 & 1.2 & 0.1 \\\\ 0.0 & 0.1 & 0.8 \\end{bmatrix}$$.\n    - Mode $$0$$: $$\\mathbf{A}_{0} = \\begin{bmatrix} -0.5 & 0.9 & 0.4 \\\\ -0.4 & 0.7 & 0.3 \\\\ -0.3 & 0.6 & 0.2 \\end{bmatrix}$$, $$\\boldsymbol{\\Sigma}_{\\mathrm{n},0} = \\operatorname{diag}([10000,\\ 10000,\\ 10000])$$.\n    - Mode $$1$$: $$\\mathbf{A}_{1} = \\begin{bmatrix} -0.6 & 1.1 & 0.0 \\\\ -0.2 & 0.4 & 1.2 \\end{bmatrix}$$, $$\\boldsymbol{\\Sigma}_{\\mathrm{n},1} = \\operatorname{diag}([100,\\ 144])$$.\n\n- **Test Case 3**:\n    - $$\\boldsymbol{\\mu}_3 = [-2.2,\\ -4.1,\\ -5.1]$$.\n    - $$\\boldsymbol{\\Sigma}_{\\mathrm{p},3} = \\begin{bmatrix} 0.4 & 0.05 & 0.05 \\\\ 0.05 & 0.6 & 0.0 \\\\ 0.05 & 0.0 & 0.5 \\end{bmatrix}$$.\n    - Mode $$0$$: $$\\mathbf{A}_{0} = \\begin{bmatrix} -0.6 & 0.8 & 0.6 \\\\ -0.1 & 0.2 & 1.0 \\\\ -0.4 & 0.7 & 0.1 \\end{bmatrix}$$, $$\\boldsymbol{\\Sigma}_{\\mathrm{n},0} = \\begin{bmatrix} 400 & 120 & 150 \\\\ 120 & 900 & 75 \\\\ 150 & 75 & 625 \\end{bmatrix}$$.\n    - Mode $$1$$: $$\\mathbf{A}_{1} = \\begin{bmatrix} -0.8 & 0.9 & 0.3 \\\\ -0.3 & 0.5 & 0.9 \\\\ -0.2 & 0.4 & 0.2 \\end{bmatrix}$$, $$\\boldsymbol{\\Sigma}_{\\mathrm{n},1} = \\begin{bmatrix} 225 & 75 & 0 \\\\ 75 & 400 & 0 \\\\ 0 & 0 & 225 \\end{bmatrix}$$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded in the fields of Bayesian statistics and atmospheric science. The use of a linear-Gaussian model is a standard and valid approximation for uncertainty quantification and experimental design. The concept of Expected Information Gain (mutual information) is a cornerstone of Bayesian experimental design. The problem is well-posed, with all necessary mathematical objects (matrices, vectors) defined, and their dimensions are consistent for the required matrix operations. The data provided in the test cases, including covariance matrices, appear physically and mathematically sound (e.g., symmetric, and on inspection, positive definite). The problem is objective and free of ambiguity. It requires a specific, derivable quantity to be computed. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe Expected Information Gain (EIG) for an instrument mode $$m$$ is the mutual information between the parameter vector $$\\boldsymbol{\\theta}$$ and the data vector $$\\mathbf{y}_m$$, denoted $$I(\\boldsymbol{\\theta}; \\mathbf{y}_m)$$. It measures the expected reduction in entropy of the parameters due to an observation. A convenient identity for mutual information is:\n$$\nI(\\boldsymbol{\\theta}; \\mathbf{y}_m) = H(\\mathbf{y}_m) - H(\\mathbf{y}_m | \\boldsymbol{\\theta})\n$$\nwhere $$H(\\cdot)$$ denotes the differential entropy. We will compute each term on the right-hand side.\n\n**1. Conditional Entropy $$H(\\mathbf{y}_m | \\boldsymbol{\\theta})$$**\n\nThe conditional distribution of the data given the parameters is specified by the measurement model: $$p(\\mathbf{y}_m | \\boldsymbol{\\theta}) = \\mathcal{N}(\\mathbf{y}_m; \\mathbf{A}_m \\boldsymbol{\\theta}, \\boldsymbol{\\Sigma}_{\\mathrm{n},m})$$.\nThe differential entropy of a $$d$$-dimensional multivariate Gaussian distribution with covariance $$\\boldsymbol{\\Sigma}$$ is given by $$\\frac{1}{2} \\ln \\det(2 \\pi e \\boldsymbol{\\Sigma})$$. This expression depends only on the covariance, not the mean. For $$p(\\mathbf{y}_m|\\boldsymbol{\\theta})$$, the covariance is $$\\boldsymbol{\\Sigma}_{\\mathrm{n},m}$$. Therefore, the entropy of this conditional distribution is constant with respect to $$\\boldsymbol{\\theta}$$. The expectation over $$p(\\boldsymbol{\\theta})$$ is thus trivial. Let $$d_m$$ be the dimension of $$\\mathbf{y}_m$$.\n$$\nH(\\mathbf{y}_m | \\boldsymbol{\\theta}) = \\mathbb{E}_{p(\\boldsymbol{\\theta})} \\left[ H(p(\\mathbf{y}_m|\\boldsymbol{\\theta})) \\right] = H(p(\\mathbf{y}_m|\\boldsymbol{\\theta})) = \\frac{d_m}{2} \\ln(2 \\pi e) + \\frac{1}{2} \\ln \\det(\\boldsymbol{\\Sigma}_{\\mathrm{n},m})\n$$\n\n**2. Marginal Entropy $$H(\\mathbf{y}_m)$**\n\nTo find the entropy of the marginal distribution of the data, $$p(\\mathbf{y}_m)$$, we must first derive this distribution. It is obtained by integrating the joint distribution over the parameters: $$p(\\mathbf{y}_m) = \\int p(\\mathbf{y}_m | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}$$.\nGiven that both the prior $$p(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}_{\\mathrm{p}})$$ and the likelihood $$p(\\mathbf{y}_m | \\boldsymbol{\\theta})$$ are Gaussian, the marginal distribution $$p(\\mathbf{y}_m)$$ is also Gaussian. Its mean and covariance are found as follows:\n- **Mean**: $$\\mathbb{E}[\\mathbf{y}_m] = \\mathbb{E}[\\mathbf{A}_m \\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}_m] = \\mathbf{A}_m \\mathbb{E}[\\boldsymbol{\\theta}] + \\mathbb{E}[\\boldsymbol{\\varepsilon}_m] = \\mathbf{A}_m \\boldsymbol{\\mu} + \\mathbf{0} = \\mathbf{A}_m \\boldsymbol{\\mu}$$.\n- **Covariance**: Using the law of total covariance, and noting the independence of $$\\boldsymbol{\\theta}$$ and $$\\boldsymbol{\\varepsilon}_m$$:\n$$\n\\operatorname{Cov}(\\mathbf{y}_m) = \\mathbb{E}[\\operatorname{Cov}(\\mathbf{y}_m|\\boldsymbol{\\theta})] + \\operatorname{Cov}(\\mathbb{E}[\\mathbf{y}_m|\\boldsymbol{\\theta}]) = \\mathbb{E}[\\boldsymbol{\\Sigma}_{\\mathrm{n},m}] + \\operatorname{Cov}(\\mathbf{A}_m \\boldsymbol{\\theta}) = \\boldsymbol{\\Sigma}_{\\mathrm{n},m} + \\mathbf{A}_m \\operatorname{Cov}(\\boldsymbol{\\theta}) \\mathbf{A}_m^T = \\boldsymbol{\\Sigma}_{\\mathrm{n},m} + \\mathbf{A}_m \\boldsymbol{\\Sigma}_{\\mathrm{p}} \\mathbf{A}_m^T\n$$\nSo, $$p(\\mathbf{y}_m) = \\mathcal{N}(\\mathbf{y}_m; \\mathbf{A}_m \\boldsymbol{\\mu}, \\mathbf{A}_m \\boldsymbol{\\Sigma}_{\\mathrm{p}} \\mathbf{A}_m^T + \\boldsymbol{\\Sigma}_{\\mathrm{n},m})$$.\nThe entropy of this marginal distribution is:\n$$\nH(\\mathbf{y}_m) = \\frac{d_m}{2} \\ln(2 \\pi e) + \\frac{1}{2} \\ln \\det(\\mathbf{A}_m \\boldsymbol{\\Sigma}_{\\mathrm{p}} \\mathbf{A}_m^T + \\boldsymbol{\\Sigma}_{\\mathrm{n},m})\n$$\n\n**3. Expected Information Gain (EIG)**\n\nSubstituting the expressions for $$H(\\mathbf{y}_m)$$ and $$H(\\mathbf{y}_m | \\boldsymbol{\\theta})$$ into the mutual information identity, the constant term $$\\frac{d_m}{2} \\ln(2 \\pi e)$$ cancels out:\n$$\n\\text{EIG}_m = I(\\boldsymbol{\\theta}; \\mathbf{y}_m) = H(\\mathbf{y}_m) - H(\\mathbf{y}_m | \\boldsymbol{\\theta}) = \\frac{1}{2} \\left[ \\ln \\det(\\mathbf{A}_m \\boldsymbol{\\Sigma}_{\\mathrm{p}} \\mathbf{A}_m^T + \\boldsymbol{\\Sigma}_{\\mathrm{n},m}) - \\ln \\det(\\boldsymbol{\\Sigma}_{\\mathrm{n},m}) \\right]\n$$\nThis is the final formula for the EIG, expressed in nats as required. Notably, it is independent of the prior mean $$\\boldsymbol{\\mu}$$.\n\n### Numerical Implementation\n\nTo compute the EIG in a numerically stable manner and avoid explicit matrix inversion as requested, we utilize the properties of the Cholesky decomposition. For a symmetric positive-definite matrix $$\\mathbf{S}$$, its Cholesky decomposition is $$\\mathbf{S} = \\mathbf{L}\\mathbf{L}^T$$, where $$\\mathbf{L}$$ is a lower-triangular matrix. The determinant of $$\\mathbf{S}$$ is $$\\det(\\mathbf{S}) = (\\det(\\mathbf{L}))^2 = (\\prod_i L_{ii})^2$$. The log-determinant is therefore:\n$$\n\\ln \\det(\\mathbf{S}) = 2 \\sum_i \\ln(L_{ii})\n$$\nThis approach avoids the potential for numerical underflow or overflow that can occur when multiplying many small or large numbers.\n\nThe algorithm to calculate the EIG for each mode $$m$$ is:\n1.  Construct the marginal data covariance matrix $$\\mathbf{M}_m = \\mathbf{A}_m \\boldsymbol{\\Sigma}_{\\mathrm{p}} \\mathbf{A}_m^T + \\boldsymbol{\\Sigma}_{\\mathrm{n},m}$$.\n2.  Compute the Cholesky decomposition of $$\\mathbf{M}_m = \\mathbf{L}_{\\mathbf{M}} \\mathbf{L}_{\\mathbf{M}}^T$$.\n3.  Compute the Cholesky decomposition of the noise covariance matrix $$\\boldsymbol{\\Sigma}_{\\mathrm{n},m} = \\mathbf{L}_{\\mathbf{n}} \\mathbf{L}_{\\mathbf{n}}^T$$.\n4.  Calculate the log-determinants:\n    - $$\\ln \\det(\\mathbf{M}_m) = 2 \\sum_i \\ln(\\text{diag}(\\mathbf{L}_{\\mathbf{M}}))$$\n    - $$\\ln \\det(\\boldsymbol{\\Sigma}_{\\mathrm{n},m}) = 2 \\sum_i \\ln(\\text{diag}(\\mathbf{L}_{\\mathbf{n}}))$$\n5.  Compute the EIG:\n    $$\n    \\text{EIG}_m = \\frac{1}{2} (\\ln \\det(\\mathbf{M}_m) - \\ln \\det(\\boldsymbol{\\Sigma}_{\\mathrm{n},m})) = \\left( \\sum_i \\ln(\\text{diag}(\\mathbf{L}_{\\mathbf{M}})) \\right) - \\left( \\sum_i \\ln(\\text{diag}(\\mathbf{L}_{\\mathbf{n}})) \\right)\n    $$\nThis procedure is applied to each mode in each test case. The mode with the maximum EIG is selected as the optimal choice.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the instrument mode recommendation problem by calculating\n    Expected Information Gain (EIG) for each mode in each test case.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1\n        {\n            \"Sigma_p\": np.array([\n                [0.30, 0.08, 0.02],\n                [0.08, 0.50, 0.10],\n                [0.02, 0.10, 0.40]\n            ]),\n            \"modes\": [\n                { # Mode 0\n                    \"A\": np.array([\n                        [-0.8, 1.2, 0.1],\n                        [-0.5, 0.9, 0.5]\n                    ]),\n                    \"Sigma_n\": np.diag([400., 625.])\n                },\n                { # Mode 1\n                    \"A\": np.array([\n                        [-0.7, 1.0, 0.2],\n                        [-0.6, 0.4, 1.1],\n                        [-0.2, 0.7, 0.3]\n                    ]),\n                    \"Sigma_n\": np.diag([225., 225., 225.])\n                },\n                { # Mode 2\n                    \"A\": np.array([\n                        [-0.9, 1.1, 0.3],\n                        [-0.4, 0.8, 0.6],\n                        [-0.3, 0.5, 0.4],\n                        [-0.1, 0.2, 0.1]\n                    ]),\n                    \"Sigma_n\": np.diag([625., 625., 625., 625.])\n                }\n            ]\n        },\n        # Test Case 2\n        {\n            \"Sigma_p\": np.array([\n                [1.0, 0.2, 0.0],\n                [0.2, 1.2, 0.1],\n                [0.0, 0.1, 0.8]\n            ]),\n            \"modes\": [\n                { # Mode 0\n                    \"A\": np.array([\n                        [-0.5, 0.9, 0.4],\n                        [-0.4, 0.7, 0.3],\n                        [-0.3, 0.6, 0.2]\n                    ]),\n                    \"Sigma_n\": np.diag([10000., 10000., 10000.])\n                },\n                { # Mode 1\n                    \"A\": np.array([\n                        [-0.6, 1.1, 0.0],\n                        [-0.2, 0.4, 1.2]\n                    ]),\n                    \"Sigma_n\": np.diag([100., 144.])\n                }\n            ]\n        },\n        # Test Case 3\n        {\n            \"Sigma_p\": np.array([\n                [0.4, 0.05, 0.05],\n                [0.05, 0.6, 0.0],\n                [0.05, 0.0, 0.5]\n            ]),\n            \"modes\": [\n                { # Mode 0\n                    \"A\": np.array([\n                        [-0.6, 0.8, 0.6],\n                        [-0.1, 0.2, 1.0],\n                        [-0.4, 0.7, 0.1]\n                    ]),\n                    \"Sigma_n\": np.array([\n                        [400., 120., 150.],\n                        [120., 900., 75.],\n                        [150., 75., 625.]\n                    ])\n                },\n                { # Mode 1\n                    \"A\": np.array([\n                        [-0.8, 0.9, 0.3],\n                        [-0.3, 0.5, 0.9],\n                        [-0.2, 0.4, 0.2]\n                    ]),\n                    \"Sigma_n\": np.array([\n                        [225., 75., 0.],\n                        [75., 400., 0.],\n                        [0., 0., 225.]\n                    ])\n                }\n            ]\n        }\n    ]\n\n    def calculate_eig(Sigma_p, A_m, Sigma_n_m):\n        \"\"\"\n        Calculates the Expected Information Gain (EIG) for a given mode.\n        \n        Args:\n            Sigma_p (np.ndarray): Prior covariance matrix.\n            A_m (np.ndarray): Sensitivity matrix for the mode.\n            Sigma_n_m (np.ndarray): Noise covariance matrix for the mode.\n            \n        Returns:\n            float: The EIG in nats.\n        \"\"\"\n        # Form the marginal data covariance matrix: M = A * Sigma_p * A.T + Sigma_n\n        M_m = A_m @ Sigma_p @ A_m.T + Sigma_n_m\n        \n        # Calculate log-determinants using Cholesky factorization for stability\n        try:\n            L_M = np.linalg.cholesky(M_m)\n            L_n = np.linalg.cholesky(Sigma_n_m)\n        except np.linalg.LinAlgError:\n            # This case should not be reached with the given valid problem data.\n            return -np.inf\n\n        # log(det(S)) = 2 * sum(log(diag(L))) where S = L*L.T\n        log_det_M = 2 * np.sum(np.log(np.diag(L_M)))\n        log_det_n = 2 * np.sum(np.log(np.diag(L_n)))\n        \n        # EIG = 0.5 * (log_det_M - log_det_n)\n        eig = 0.5 * (log_det_M - log_det_n)\n        \n        return eig\n\n    final_results = []\n    for case in test_cases:\n        Sigma_p = case[\"Sigma_p\"]\n        modes = case[\"modes\"]\n        \n        eigs_for_case = []\n        for mode in modes:\n            A_m = mode[\"A\"]\n            Sigma_n_m = mode[\"Sigma_n\"]\n            eig = calculate_eig(Sigma_p, A_m, Sigma_n_m)\n            eigs_for_case.append(eig)\n            \n        optimal_mode_index = np.argmax(eigs_for_case)\n        \n        final_results.extend(eigs_for_case)\n        final_results.append(optimal_mode_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}