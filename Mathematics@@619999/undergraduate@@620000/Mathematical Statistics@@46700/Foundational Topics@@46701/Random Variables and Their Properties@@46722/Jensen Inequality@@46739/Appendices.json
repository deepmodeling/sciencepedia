{"hands_on_practices": [{"introduction": "Let's begin by grounding Jensen's inequality in a tangible physical scenario. We will explore the relationship between the average kinetic energy of particles in a system and the kinetic energy calculated from their average velocity. This exercise provides a concrete demonstration of the inequality $E[g(X)] \\ge g(E[X])$ for the convex function for kinetic energy, revealing a non-intuitive but fundamental principle in statistical mechanics. [@problem_id:1926157]", "problem": "In a simplified model of a rarefied gas, a collection of identical, non-interacting particles is analyzed. Each particle has a mass of $m = 0.500 \\text{ kg}$. The one-dimensional velocity $V$ of any given particle is a random variable that can take on one of two distinct values. Specifically, a particle has a velocity of $v_1 = 10.0 \\text{ m/s}$ with a probability of $p_1 = 0.700$, or a velocity of $v_2 = -8.00 \\text{ m/s}$ with a probability of $p_2 = 0.300$.\n\nWe define two distinct energy quantities for this system:\n1. The *mean kinetic energy*, $\\langle K \\rangle$, which is the statistical expectation of the kinetic energy of a particle, given by $\\langle K \\rangle = E\\left[\\frac{1}{2}mV^2\\right]$.\n2. The *kinetic energy of the mean velocity*, $K_{\\langle v \\rangle}$, which is the kinetic energy calculated using the statistical expectation of the velocity, given by $K_{\\langle v \\rangle} = \\frac{1}{2}m\\left(E[V]\\right)^2$.\n\nCalculate the difference $\\Delta K = \\langle K \\rangle - K_{\\langle v \\rangle}$. Express your final answer in Joules (J), rounded to three significant figures.", "solution": "We use the definitions $\\langle K \\rangle = E\\!\\left[\\frac{1}{2}mV^{2}\\right]$ and $K_{\\langle v \\rangle} = \\frac{1}{2}m\\left(E[V]\\right)^{2}$. The difference is\n$$\n\\Delta K = \\langle K \\rangle - K_{\\langle v \\rangle} = \\frac{1}{2}m\\left(E[V^{2}] - \\left(E[V]\\right)^{2}\\right).\n$$\nWith $P(V=v_{1})=p_{1}=0.700$, $P(V=v_{2})=p_{2}=0.300$, $v_{1}=10.0$, $v_{2}=-8.00$, and $m=0.500$, we compute\n$$\nE[V] = p_{1}v_{1} + p_{2}v_{2} = 0.700\\cdot 10.0 + 0.300\\cdot(-8.00) = 4.6,\n$$\n$$\nE[V^{2}] = p_{1}v_{1}^{2} + p_{2}v_{2}^{2} = 0.700\\cdot 100 + 0.300\\cdot 64 = 89.2.\n$$\nThus\n$$\nE[V^{2}] - \\left(E[V]\\right)^{2} = 89.2 - (4.6)^{2} = 89.2 - 21.16 = 68.04,\n$$\nand therefore\n$$\n\\Delta K = \\frac{1}{2}\\cdot 0.500 \\cdot 68.04 = 0.25 \\cdot 68.04 = 17.01.\n$$\nRounded to three significant figures, this yields $17.0$.", "answer": "$$\\boxed{17.0}$$", "id": "1926157"}, {"introduction": "Now, we shift our focus to the field of statistical inference, a cornerstone of data science. This practice examines the \"plug-in\" estimator for the variance of a Bernoulli process, a common tool in quality control and polling. By applying Jensen's inequality to a concave function, you will uncover a subtle but systematic bias in this estimator, highlighting the inequality's importance in understanding the properties of statistical methods. [@problem_id:1926116]", "problem": "In a quality control process for manufacturing smartphones, each phone is tested to determine if it is defective or non-defective. The outcome for a single phone can be modeled as a Bernoulli trial, where the probability of a phone being defective is $p$. To estimate the variability of this manufacturing process, a random sample of $n$ phones is drawn from the production line.\n\nLet $X_1, X_2, \\ldots, X_n$ be the sequence of independent and identically distributed Bernoulli random variables representing the outcomes for the $n$ phones, where $X_i=1$ if the $i$-th phone is defective and $X_i=0$ otherwise. The sample proportion of defective phones is given by $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nA common way to estimate the true variance of the process, $\\sigma^2 = p(1-p)$, is to use the \"plug-in\" estimator, which is formed by substituting the sample proportion $\\hat{p}$ into the formula for the variance. Let this estimator be denoted by $\\hat{\\sigma}^2 = \\hat{p}(1-\\hat{p})$. The bias of this estimator is defined as $\\text{Bias}(\\hat{\\sigma}^2) = E[\\hat{\\sigma}^2] - \\sigma^2$.\n\nDetermine the bias of the estimator $\\hat{\\sigma}^2$ as a function of the true proportion $p$ and the sample size $n$.", "solution": "We have independent and identically distributed Bernoulli random variables $X_{1},\\ldots,X_{n}$ with parameter $p$, so $E[X_{i}]=p$ and $\\operatorname{Var}(X_{i})=p(1-p)$ for each $i$. The sample proportion is $\\hat{p}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$, and the plug-in estimator of the variance is $\\hat{\\sigma}^{2}=\\hat{p}(1-\\hat{p})$.\n\nFirst compute $E[\\hat{p}]$ and $\\operatorname{Var}(\\hat{p})$:\n$$\nE[\\hat{p}]=E\\!\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right]=\\frac{1}{n}\\sum_{i=1}^{n}E[X_{i}]=\\frac{1}{n}\\cdot n p=p.\n$$\nUsing independence,\n$$\n\\operatorname{Var}(\\hat{p})=\\operatorname{Var}\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right)=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\operatorname{Var}(X_{i})=\\frac{1}{n^{2}}\\cdot n\\,p(1-p)=\\frac{p(1-p)}{n}.\n$$\n\nNext compute $E[\\hat{p}^{2}]$ via the identity $\\operatorname{Var}(\\hat{p})=E[\\hat{p}^{2}]-\\left(E[\\hat{p}]\\right)^{2}$:\n$$\nE[\\hat{p}^{2}]=\\operatorname{Var}(\\hat{p})+\\left(E[\\hat{p}]\\right)^{2}=\\frac{p(1-p)}{n}+p^{2}.\n$$\n\nNow evaluate $E[\\hat{\\sigma}^{2}]$:\n$$\nE[\\hat{\\sigma}^{2}]=E[\\hat{p}(1-\\hat{p})]=E[\\hat{p}]-E[\\hat{p}^{2}]=p-\\left(\\frac{p(1-p)}{n}+p^{2}\\right)=p(1-p)-\\frac{p(1-p)}{n}.\n$$\nThus\n$$\nE[\\hat{\\sigma}^{2}]=p(1-p)\\left(1-\\frac{1}{n}\\right)=p(1-p)\\frac{n-1}{n}.\n$$\n\nThe bias is\n$$\n\\text{Bias}(\\hat{\\sigma}^{2})=E[\\hat{\\sigma}^{2}]-\\sigma^{2}=p(1-p)\\frac{n-1}{n}-p(1-p)=-\\frac{p(1-p)}{n}.\n$$\nThis shows the plug-in estimator is downward biased by $\\frac{p(1-p)}{n}$.", "answer": "$$\\boxed{-\\frac{p(1-p)}{n}}$$", "id": "1926116"}, {"introduction": "Building on our understanding, we now use Jensen's inequality as a powerful problem-solving tool for optimization. We will tackle a problem of resource allocation—minimizing the operational cost in a distributed computing system under a fixed total power. This exercise showcases how the discrete form of Jensen's inequality can elegantly determine the optimal distribution of resources to achieve minimum cost. [@problem_id:2304653]", "problem": "A distributed computing system consists of $n$ independent processing nodes. The total processing power allocated across all nodes is a fixed positive value, $S$. Let $p_i$ be the processing power allocated to the $i$-th node, where $p_i > 0$ for all $i=1, 2, \\ldots, n$. The allocations must satisfy the constraint $\\sum_{i=1}^n p_i = S$. The operational cost for the $i$-th node is found to be a function of its allocated power, given by the expression $C_i(p_i) = p_i + \\frac{\\alpha^2}{p_i}$, where $\\alpha$ is a positive real constant representing a system-specific inefficiency factor. Your task is to determine the minimum possible total operational cost for the entire system, which is the sum of the costs of all nodes, $\\sum_{i=1}^n C_i(p_i)$. Express your answer as a function of $n$, $S$, and $\\alpha$.", "solution": "We are to minimize the total cost\n$$\nF(p_{1},\\ldots,p_{n})=\\sum_{i=1}^{n}\\left(p_{i}+\\frac{\\alpha^{2}}{p_{i}}\\right)\n$$\nsubject to $p_{i}>0$ for all $i$ and the linear constraint $\\sum_{i=1}^{n}p_{i}=S$, where $S>0$ and $\\alpha>0$ are fixed.\n\nFirst, define $f(p)=p+\\frac{\\alpha^{2}}{p}$ for $p>0$. Its second derivative is\n$$\nf''(p)=\\frac{2\\alpha^{2}}{p^{3}}>0 \\quad \\text{for all } p>0,\n$$\nso $f$ is strictly convex on $(0,\\infty)$. Therefore, by Jensen’s inequality,\n$$\n\\frac{1}{n}\\sum_{i=1}^{n}f(p_{i}) \\geq f\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n}p_{i}\\right)=f\\!\\left(\\frac{S}{n}\\right),\n$$\nwith equality if and only if $p_{1}=\\cdots=p_{n}=\\frac{S}{n}$. Hence the unique minimizer under the constraint is\n$$\np_{i}^{\\star}=\\frac{S}{n} \\quad \\text{for all } i=1,\\ldots,n.\n$$\nEvaluating the total cost at this allocation gives\n$$\nF_{\\min}= \\sum_{i=1}^{n}\\left(\\frac{S}{n}+\\frac{\\alpha^{2}}{S/n}\\right)\n= n\\cdot \\frac{S}{n} + n\\cdot \\frac{\\alpha^{2}n}{S}\n= S + \\frac{n^{2}\\alpha^{2}}{S}.\n$$\nBecause $f$ is strictly convex and the feasible set defined by $\\sum_{i=1}^{n}p_{i}=S$, $p_{i}>0$ is convex and nonempty, this value is the unique global minimum.", "answer": "$$\\boxed{S+\\frac{n^{2}\\alpha^{2}}{S}}$$", "id": "2304653"}]}