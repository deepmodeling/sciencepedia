{"hands_on_practices": [{"introduction": "The characteristic function of a random variable is more than just a mathematical curiosity; it serves as a complete blueprint for its probability distribution. This first exercise provides practice in \"decoding\" this blueprint. By recognizing familiar structures within the function, such as those revealed by Euler's formula, you can directly determine the possible values of a discrete random variable and their associated probabilities. This practice builds foundational intuition for how a characteristic function's form mirrors the distribution's structure [@problem_id:1348208].", "problem": "Let $X$ be a discrete random variable. The characteristic function of $X$, denoted by $\\phi_X(t)$, is defined as the expected value $E[\\exp(itX)]$, where $i$ is the imaginary unit and $t$ is a real number. For a discrete random variable, this can be written as $\\phi_X(t) = \\sum_{k} P(X=x_k) \\exp(itx_k)$, where $\\{x_k\\}$ are the possible values of $X$ and $P(X=x_k)$ are their corresponding probabilities.\n\nSuppose the characteristic function of a particular random variable $X$ is given by:\n$$ \\phi_X(t) = \\cos(2t) $$\nWhich of the following statements correctly describes the Probability Mass Function (PMF) of $X$?\n\nA. $X$ takes the values $1$ and $2$ with probabilities $P(X=1)=\\frac{1}{2}$ and $P(X=2)=\\frac{1}{2}$.\n\nB. $X$ takes the values $-2$ and $2$ with probabilities $P(X=-2)=\\frac{1}{2}$ and $P(X=2)=\\frac{1}{2}$.\n\nC. $X$ takes the values $-1$ and $1$ with probabilities $P(X=-1)=\\frac{1}{2}$ and $P(X=1)=\\frac{1}{2}$.\n\nD. $X$ takes a single value $0$ with probability $1$.\n\nE. $X$ is a continuous random variable uniformly distributed on the interval $[-2, 2]$.", "solution": "By definition, the characteristic function of a discrete random variable $X$ is $\\phi_{X}(t)=E[\\exp(i t X)]=\\sum_{k}P(X=x_{k})\\exp(i t x_{k})$.\n\nUsing Euler's identity, for any real $\\theta$,\n$$\n\\cos(\\theta)=\\frac{1}{2}\\left(\\exp(i\\theta)+\\exp(-i\\theta)\\right).\n$$\nGiven $\\phi_{X}(t)=\\cos(2t)$, we rewrite it as\n$$\n\\phi_{X}(t)=\\frac{1}{2}\\left(\\exp(i\\cdot 2 t)+\\exp(-i\\cdot 2 t)\\right)=\\frac{1}{2}\\exp(i t\\cdot 2)+\\frac{1}{2}\\exp(i t\\cdot(-2)).\n$$\nThis matches the form $\\sum_{k}P(X=x_{k})\\exp(i t x_{k})$ with $P(X=2)=\\frac{1}{2}$ and $P(X=-2)=\\frac{1}{2}$. Therefore, $X$ takes values $-2$ and $2$ with equal probabilities.\n\nTo confirm the uniqueness among the options:\n- Option A would yield $\\phi_{X}(t)=\\frac{1}{2}\\exp(i t)+\\frac{1}{2}\\exp(i 2 t)$, which is not equal to $\\cos(2t)$ for all $t$.\n- Option C corresponds to $\\phi_{X}(t)=\\cos(t)$, not $\\cos(2t)$.\n- Option D corresponds to $\\phi_{X}(t)=1$, not $\\cos(2t)$.\n- Option E, for a continuous uniform on $[-2,2]$, has $\\phi_{X}(t)=\\frac{\\sin(2 t)}{2 t}$, not $\\cos(2t)$.\n\nHence the correct description is that $X$ takes values $-2$ and $2$ with probabilities $\\frac{1}{2}$ each.", "answer": "$$\\boxed{B}$$", "id": "1348208"}, {"introduction": "One of the most powerful applications of characteristic functions is their ability to simplify the calculation of a distribution's moments. This practice moves from identifying a distribution to quantifying its key features, such as its mean and variance. You will apply the fundamental theorem that connects the derivatives of a characteristic function at the origin to the moments of the random variable, a technique that is often far simpler than direct summation or integration [@problem_id:1348184]. This method demonstrates a core computational advantage of working in the frequency domain.", "problem": "Let $X$ be a random variable. The characteristic function of a random variable, $\\phi_X(t)$, is defined as the expected value of $\\exp(itX)$, that is, $\\phi_X(t) = \\mathbb{E}[\\exp(itX)]$, where $t$ is a real number and $i$ is the imaginary unit satisfying $i^2 = -1$.\nSuppose the characteristic function of $X$ is given by:\n$$ \\phi_X(t) = \\frac{5}{12} + \\frac{5}{12}\\exp(it) + \\frac{1}{6}\\exp(2it) $$\nCalculate the variance of the random variable $X$. Round your final answer to three significant figures.", "solution": "The variance of a random variable $X$, denoted as $\\text{Var}(X)$, is given by the formula $\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. The moments of $X$, $\\mathbb{E}[X^k]$, can be obtained from the derivatives of its characteristic function, $\\phi_X(t)$, evaluated at $t=0$. The general formula is:\n$$ \\mathbb{E}[X^k] = \\frac{1}{i^k} \\phi_X^{(k)}(0) $$\nwhere $\\phi_X^{(k)}(0)$ is the $k$-th derivative of $\\phi_X(t)$ with respect to $t$, evaluated at $t=0$.\n\nFirst, we will find the first moment, $\\mathbb{E}[X]$, which is the mean of $X$. For this, we need the first derivative of $\\phi_X(t)$.\nThe given characteristic function is:\n$$ \\phi_X(t) = \\frac{5}{12} + \\frac{5}{12}\\exp(it) + \\frac{1}{6}\\exp(2it) $$\nThe first derivative with respect to $t$ is:\n$$ \\phi_X'(t) = \\frac{d}{dt} \\left( \\frac{5}{12} + \\frac{5}{12}\\exp(it) + \\frac{1}{6}\\exp(2it) \\right) $$\nUsing the chain rule, we get:\n$$ \\phi_X'(t) = 0 + \\frac{5}{12}(i)\\exp(it) + \\frac{1}{6}(2i)\\exp(2it) = \\frac{5i}{12}\\exp(it) + \\frac{2i}{6}\\exp(2it) $$\n$$ \\phi_X'(t) = \\frac{5i}{12}\\exp(it) + \\frac{4i}{12}\\exp(2it) $$\nNow, we evaluate this derivative at $t=0$:\n$$ \\phi_X'(0) = \\frac{5i}{12}\\exp(0) + \\frac{4i}{12}\\exp(0) = \\frac{5i}{12} + \\frac{4i}{12} = \\frac{9i}{12} = \\frac{3i}{4} $$\nThe first moment is then:\n$$ \\mathbb{E}[X] = \\frac{1}{i} \\phi_X'(0) = \\frac{1}{i} \\left( \\frac{3i}{4} \\right) = \\frac{3}{4} $$\n\nNext, we find the second moment, $\\mathbb{E}[X^2]$. For this, we need the second derivative of $\\phi_X(t)$. We differentiate $\\phi_X'(t)$:\n$$ \\phi_X''(t) = \\frac{d}{dt} \\left( \\frac{5i}{12}\\exp(it) + \\frac{4i}{12}\\exp(2it) \\right) $$\n$$ \\phi_X''(t) = \\frac{5i}{12}(i)\\exp(it) + \\frac{4i}{12}(2i)\\exp(2it) = \\frac{5i^2}{12}\\exp(it) + \\frac{8i^2}{12}\\exp(2it) $$\nSince $i^2 = -1$, we have:\n$$ \\phi_X''(t) = -\\frac{5}{12}\\exp(it) - \\frac{8}{12}\\exp(2it) $$\nEvaluate the second derivative at $t=0$:\n$$ \\phi_X''(0) = -\\frac{5}{12}\\exp(0) - \\frac{8}{12}\\exp(0) = -\\frac{5}{12} - \\frac{8}{12} = -\\frac{13}{12} $$\nThe second moment is then:\n$$ \\mathbb{E}[X^2] = \\frac{1}{i^2} \\phi_X''(0) = \\frac{1}{-1} \\left( -\\frac{13}{12} \\right) = \\frac{13}{12} $$\n\nFinally, we calculate the variance:\n$$ \\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\frac{13}{12} - \\left(\\frac{3}{4}\\right)^2 $$\n$$ \\text{Var}(X) = \\frac{13}{12} - \\frac{9}{16} $$\nTo subtract the fractions, we find a common denominator, which is 48.\n$$ \\text{Var}(X) = \\frac{13 \\times 4}{12 \\times 4} - \\frac{9 \\times 3}{16 \\times 3} = \\frac{52}{48} - \\frac{27}{48} = \\frac{52 - 27}{48} = \\frac{25}{48} $$\nTo provide the answer as a decimal rounded to three significant figures, we compute the value of the fraction:\n$$ \\frac{25}{48} \\approx 0.520833... $$\nRounding to three significant figures gives $0.521$.", "answer": "$$\\boxed{0.521}$$", "id": "1348184"}, {"introduction": "The link between a characteristic function's differentiability and the existence of moments has profound theoretical implications. This final practice explores a classic and important case: the Cauchy distribution, whose characteristic function is given by $\\phi_X(t) = \\exp(-|t|)$. By examining the behavior of this function at the origin, you can rigorously demonstrate why the mean of the Cauchy distribution is undefined [@problem_id:1348219]. This exercise showcases the diagnostic power of characteristic functions, providing a solid theoretical explanation for a counter-intuitive but fundamental result in probability theory.", "problem": "The characteristic function of a random variable $X$, denoted by $\\phi_X(t)$, is defined as the expected value $\\phi_X(t) = E[\\exp(itX)]$, where $t$ is a real number and $i$ is the imaginary unit. A fundamental theorem in probability theory establishes a relationship between the moments of a random variable and the derivatives of its characteristic function. The theorem states that if the $n$-th moment of $X$, $E[X^n]$, exists, then the characteristic function $\\phi_X(t)$ is $n$ times differentiable at the origin ($t=0$), and the derivative is related to the moment by the formula $\\phi_X^{(n)}(0) = i^n E[X^n]$.\n\nNow, consider a random variable $X$ that follows a standard Cauchy distribution. The characteristic function for this distribution is given by the expression $\\phi_X(t) = \\exp(-|t|)$.\n\nUsing the provided theorem and the form of the characteristic function, what can be rigorously concluded about the mean (the first moment, $E[X]$) of the standard Cauchy distribution?\n\nA. The mean is $0$.\n\nB. The mean is $1$.\n\nC. The mean is undefined.\n\nD. This information is insufficient to determine if the mean exists.\n\nE. The mean is defined but cannot be calculated from the characteristic function.", "solution": "We use the stated theorem: if the first moment $E[X]$ exists (is finite), then the characteristic function $\\phi_{X}(t)$ is differentiable at $t=0$ and $\\phi_{X}'(0)=iE[X]$. Its contrapositive is: if $\\phi_{X}$ is not differentiable at $t=0$, then $E[X]$ does not exist.\n\nFor the standard Cauchy distribution, $\\phi_{X}(t)=\\exp(-|t|)$. This is piecewise given by\n$$\n\\phi_{X}(t)=\n\\begin{cases}\n\\exp(-t), & t>0,\\\\\n\\exp(t), & t<0.\n\\end{cases}\n$$\nThus, for $t>0$,\n$$\n\\frac{d}{dt}\\phi_{X}(t)=-\\exp(-t)\\quad\\Rightarrow\\quad \\lim_{t\\to 0^{+}}\\phi_{X}'(t)=-1,\n$$\nand for $t<0$,\n$$\n\\frac{d}{dt}\\phi_{X}(t)=\\exp(t)\\quad\\Rightarrow\\quad \\lim_{t\\to 0^{-}}\\phi_{X}'(t)=1.\n$$\nSince the one-sided derivatives at $0$ are unequal, $\\phi_{X}'(0)$ does not exist; hence $\\phi_{X}$ is not differentiable at $0$. By the contrapositive of the theorem, the first moment $E[X]$ does not exist (is undefined). Therefore, the correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "1348219"}]}