{"hands_on_practices": [{"introduction": "Markov's inequality is a foundational tool in probability theory, offering a way to bound the likelihood of rare events with very limited information. Its power lies in its generality: for any non-negative random variable, it provides a robust upper limit on the probability of exceeding a certain value, using only the variable's average. This first practice [@problem_id:1933050] grounds the abstract inequality in a concrete, real-world scenario, allowing you to apply it directly and build a foundational understanding of its practical utility in making estimations under uncertainty.", "id": "1933050", "problem": "A data analyst at a digital marketing startup is monitoring the performance of a new online advertisement. The number of clicks the ad receives in any given hour is a non-negative random variable. Based on extensive historical data, the analyst has determined that the average number of clicks per hour for this ad is 185. The engineering team needs to prepare for potential traffic surges and has asked for a worst-case estimate on the likelihood of high traffic. Without making any further assumptions about the specific probability distribution of the hourly clicks, what is the maximum possible value for the probability that the advertisement will receive 1000 or more clicks in a single, given hour? Express your answer as a decimal.\n\n", "solution": "Let $X$ be the non-negative random variable representing the number of clicks in an hour, with mean $\\mathbb{E}[X]=185$. We are asked for the maximum possible value of $\\mathbb{P}(X \\geq 1000)$ without assuming any specific distribution beyond non-negativity and the mean.\n\nWe apply Markov's inequality, which states that for any non-negative random variable $X$ and any $t>0$,\n$$\n\\mathbb{P}(X \\geq t) \\leq \\frac{\\mathbb{E}[X]}{t}.\n$$\nA standard derivation is as follows. Since $X \\geq 0$ and $X \\geq t$ on the event $\\{X \\geq t\\}$, we have\n$$\nX \\mathbf{1}_{\\{X \\geq t\\}} \\geq t \\mathbf{1}_{\\{X \\geq t\\}}.\n$$\nTaking expectations,\n$$\n\\mathbb{E}[X \\mathbf{1}_{\\{X \\geq t\\}}] \\geq t \\mathbb{P}(X \\geq t).\n$$\nAlso,\n$$\n\\mathbb{E}[X] = \\mathbb{E}[X \\mathbf{1}_{\\{X < t\\}}] + \\mathbb{E}[X \\mathbf{1}_{\\{X \\geq t\\}}] \\geq \\mathbb{E}[X \\mathbf{1}_{\\{X \\geq t\\}}],\n$$\nso combining yields\n$$\n\\mathbb{E}[X] \\geq t \\mathbb{P}(X \\geq t) \\quad \\Rightarrow \\quad \\mathbb{P}(X \\geq t) \\leq \\frac{\\mathbb{E}[X]}{t}.\n$$\n\nSetting $t=1000$ and $\\mathbb{E}[X]=185$, we obtain\n$$\n\\mathbb{P}(X \\geq 1000) \\leq \\frac{185}{1000} = 0.185.\n$$\n\nThis bound is tight. Consider a two-point distribution defined by\n$$\n\\mathbb{P}(X=1000) = \\frac{185}{1000}, \\quad \\mathbb{P}(X=0) = 1 - \\frac{185}{1000}.\n$$\nThen $X \\geq 0$ almost surely, and\n$$\n\\mathbb{E}[X] = 1000 \\cdot \\frac{185}{1000} + 0 \\cdot \\left(1 - \\frac{185}{1000}\\right) = 185,\n$$\nwhile\n$$\n\\mathbb{P}(X \\geq 1000) = \\frac{185}{1000} = 0.185.\n$$\nTherefore, without further distributional assumptions, the maximum possible value of $\\mathbb{P}(X \\geq 1000)$ is $0.185$.", "answer": "$$\\boxed{0.185}$$"}, {"introduction": "While the basic Markov bound is incredibly useful due to its minimal requirements, it can sometimes be quite loose. A key skill in probabilistic modeling is learning to incorporate additional information to refine our estimates. This exercise [@problem_id:1933078] demonstrates a powerful and general technique for doing just that: by applying Markov's inequality to a cleverly chosen function of the random variable, $(X-b)^2$, and then optimizing a free parameter, we can leverage knowledge of the variance to derive a significantly tighter bound known as Cantelli's inequality.", "id": "1933078", "problem": "Let $X$ be a random variable with a well-defined mean $E[X] = \\mu$ and a finite, non-zero variance $\\text{Var}(X) = \\sigma^2$. We want to find a tight upper bound for the tail probability $P(X \\ge a)$, where $a$ is a constant strictly greater than the mean, i.e., $a > \\mu$.\n\nA general method for deriving such bounds involves transforming the random variable. Consider the function $B(b) = \\frac{E[(X-b)^2]}{(a-b)^2}$, where $b$ is a real-valued parameter. This function provides an upper bound for $P(X \\ge a)$ for any choice of $b < a$.\n\nYour task is to find the tightest possible bound that can be obtained from this family of functions. To do this, you must find the minimum value of $B(b)$ by optimizing the choice of the parameter $b$ over its valid domain ($b<a$).\n\nExpress this optimal (minimum) value of $B(b)$ as a closed-form analytic expression in terms of $\\mu$, $\\sigma$, and $a$.\n\n", "solution": "We are given that for any $b<a$,\n$$\nP(X\\ge a)\\le B(b):=\\frac{E[(X-b)^{2}]}{(a-b)^{2}}.\n$$\nUsing $E[(X-b)^{2}]=\\operatorname{Var}(X)+\\left(E[X]-b\\right)^{2}$, we have\n$$\nE[(X-b)^{2}]=\\sigma^{2}+(\\mu-b)^{2},\n$$\nso the bound is\n$$\nB(b)=\\frac{\\sigma^{2}+(\\mu-b)^{2}}{(a-b)^{2}},\\quad b<a.\n$$\n\nTo minimize $B(b)$ over $b<a$, define\n$$\nf(b)=\\frac{\\sigma^{2}+(b-\\mu)^{2}}{(a-b)^{2}},\n$$\nwith numerator $N(b)=\\sigma^{2}+(b-\\mu)^{2}$ and denominator $D(b)=(a-b)^{2}$. Then\n$$\nf'(b)=\\frac{N'(b)D(b)-N(b)D'(b)}{D(b)^{2}},\n$$\nwith $N'(b)=2(b-\\mu)$ and $D'(b)=-2(a-b)$. Hence\n$$\nf'(b)=\\frac{2(b-\\mu)(a-b)^{2}+2\\left[\\sigma^{2}+(b-\\mu)^{2}\\right](a-b)}{(a-b)^{4}}\n=\\frac{2(a-b)\\left((b-\\mu)(a-b)+\\sigma^{2}+(b-\\mu)^{2}\\right)}{(a-b)^{4}}.\n$$\nFor $b<a$, we have $a-b>0$, so critical points satisfy\n$$\n(b-\\mu)(a-b)+\\sigma^{2}+(b-\\mu)^{2}=0.\n$$\nLet $t=b-\\mu$ and write $a-b=(a-\\mu)-t$. Then the equation becomes\n$$\nt\\big((a-\\mu)-t\\big)+\\sigma^{2}+t^{2}=t(a-\\mu)-t^{2}+\\sigma^{2}+t^{2}=t(a-\\mu)+\\sigma^{2}=0,\n$$\nso\n$$\nt^{*}=-\\frac{\\sigma^{2}}{a-\\mu},\\qquad b^{*}=\\mu+t^{*}=\\mu-\\frac{\\sigma^{2}}{a-\\mu}.\n$$\nSince $a>\\mu$, we have $a-\\mu>0$ and thus\n$$\nb^{*}<a \\;\\;\\Longleftrightarrow\\;\\; \\mu-\\frac{\\sigma^{2}}{a-\\mu}<a \\;\\;\\Longleftrightarrow\\;\\; -\\sigma^{2}<(a-\\mu)^{2},\n$$\nwhich is always true. Therefore $b^{*}$ is feasible. Moreover, $f(b)\\to+\\infty$ as $b\\to a^{-}$ and $f(b)\\to 1$ as $b\\to -\\infty$, and there is a unique critical point, so $b^{*}$ yields the global minimum.\n\nEvaluate $B(b)$ at $b=b^{*}$. Set $d=a-\\mu>0$. Then\n$$\n\\mu-b^{*}=\\frac{\\sigma^{2}}{d},\\qquad a-b^{*}=d+\\frac{\\sigma^{2}}{d}=\\frac{d^{2}+\\sigma^{2}}{d}.\n$$\nThus\n$$\nB(b^{*})=\\frac{\\sigma^{2}+(\\mu-b^{*})^{2}}{(a-b^{*})^{2}}\n=\\frac{\\sigma^{2}+\\left(\\frac{\\sigma^{2}}{d}\\right)^{2}}{\\left(\\frac{d^{2}+\\sigma^{2}}{d}\\right)^{2}}\n=\\frac{\\sigma^{2}\\left(1+\\frac{\\sigma^{2}}{d^{2}}\\right)}{\\frac{(d^{2}+\\sigma^{2})^{2}}{d^{2}}}\n=\\frac{\\sigma^{2}\\left(\\frac{d^{2}+\\sigma^{2}}{d^{2}}\\right)}{\\frac{(d^{2}+\\sigma^{2})^{2}}{d^{2}}}\n=\\frac{\\sigma^{2}}{d^{2}+\\sigma^{2}}\n=\\frac{\\sigma^{2}}{(a-\\mu)^{2}+\\sigma^{2}}.\n$$\n\nTherefore, the optimal (minimum) value of $B(b)$ over $b<a$ is $\\sigma^{2}/\\big((a-\\mu)^{2}+\\sigma^{2}\\big)$.", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{(a-\\mu)^{2}+\\sigma^{2}}}$$"}, {"introduction": "After learning how to apply Markov's inequality and how to strengthen its bounds with more information, a crucial final step is to understand its limits. When is the original, simple bound the very best we can do? This final practice [@problem_id:1316841] explores this fundamental question of *sharpness*, challenging you to analyze a specific scenario where the inequality becomes an exact equality. By deconstructing the conditions that create this \"worst-case\" distribution, you will gain a deeper appreciation for the theoretical underpinnings that make Markov's inequality both simple and profound.", "id": "1316841", "problem": "A specialized electronic component's power consumption, denoted by the non-negative random variable $X$, can take one of three discrete values. It can be in an idle state with $0$ Watts, a nominal state with $A$ Watts, or a high-performance state with $B$ Watts. The parameters satisfy $0 < \\mu < A < B$, where $\\mu$ is the long-term average power consumption, i.e., $E[X] = \\mu$. A performance analysis of the component reveals that the probability of it consuming power at or above the nominal level, $P(X \\ge A)$, is exactly equal to the theoretical upper bound given by Markov's inequality. Based on this information, determine the probability that the component is in its high-performance state, $P(X = B)$.\n\n", "solution": "Let $p = P(X = B)$, $q = P(X = A)$, and $r = P(X = 0) = 1 - p - q$. Since $X$ is non-negative and takes values in $\\{0, A, B\\}$ with $0 < \\mu < A < B$, we have:\n$$\nE[X] = \\mu = A q + B p.\n$$\nMarkov's inequality for non-negative $X$ states that for any $t > 0$,\n$$\nP(X \\ge t) \\le \\frac{E[X]}{t}.\n$$\nTaking $t = A$ and using the given equality case,\n$$\nP(X \\ge A) = \\frac{\\mu}{A}.\n$$\nBut $P(X \\ge A) = P(X = A) + P(X = B) = q + p$, hence\n$$\nq + p = \\frac{\\mu}{A}.\n$$\nWe now solve the system\n$$\n\\begin{cases}\nq + p = \\frac{\\mu}{A}, \\\\\nA q + B p = \\mu.\n\\end{cases}\n$$\nFrom the first equation, $q = \\frac{\\mu}{A} - p$. Substitute into the second:\n$$\nA\\left(\\frac{\\mu}{A} - p\\right) + B p = \\mu \\;\\;\\Rightarrow\\;\\; \\mu - A p + B p = \\mu \\;\\;\\Rightarrow\\;\\; (B - A) p = 0.\n$$\nSince $B > A$, it follows that $p = 0$. Therefore,\n$$\nP(X = B) = 0.\n$$\nThis is also consistent with the equality condition in Markov's inequality: equality requires $X = A \\mathbf{1}_{\\{X \\ge A\\}}$ almost surely, which precludes any positive probability at values strictly greater than $A$.", "answer": "$$\\boxed{0}$$"}]}