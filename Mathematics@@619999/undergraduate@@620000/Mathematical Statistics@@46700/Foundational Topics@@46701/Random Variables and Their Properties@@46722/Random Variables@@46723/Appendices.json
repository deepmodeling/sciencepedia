{"hands_on_practices": [{"introduction": "Many problems in probability that seem combinatorially complex can be solved with surprising elegance. This exercise, a modern version of the classic 'hat-check problem,' is a prime example. The key is to use indicator random variables and the powerful property of linearity of expectation, which allows us to calculate the expected value of a sum of random variables simply by summing their individual expectations, even if they are not independent. This practice will build your intuition for decomposing a problem into simpler parts. [@problem_id:1329488]", "problem": "A data center operates $N$ specialized servers, where each server is designated to run a specific computational task from a set of $N$ unique tasks. During a system-wide reboot, the task assignment scheduler malfunctions. It reassigns the $N$ tasks to the $N$ servers completely at random. The reassignment is a permutation, meaning that each server receives exactly one task and each task is assigned to exactly one server. All $N!$ possible permutations (assignments) are equally likely.\n\nWhat is the expected number of servers that are assigned their originally designated task? Express your answer as a function of $N$.", "solution": "Let $X$ be the random variable representing the total number of servers that receive their correctly designated task. We are asked to find the expected value of $X$, denoted as $E[X]$.\n\nTo solve this, we can express $X$ as a sum of indicator random variables. Let's define an indicator random variable $X_i$ for each server $i$, where $i$ ranges from $1$ to $N$. The variable $X_i$ is defined as follows:\n$$\nX_i = \\begin{cases} 1 & \\text{if server } i \\text{ is assigned its correct task} \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe total number of correctly assigned servers, $X$, is the sum of these individual indicator variables because each $X_i$ that equals 1 contributes one count to the total.\n$$\nX = X_1 + X_2 + \\dots + X_N = \\sum_{i=1}^{N} X_i\n$$\nThe key property we will use is the linearity of expectation. This property states that the expected value of a sum of random variables is equal to the sum of their individual expected values. This holds true even if the random variables are not independent.\n$$\nE[X] = E\\left[\\sum_{i=1}^{N} X_i\\right] = \\sum_{i=1}^{N} E[X_i]\n$$\nNow, we must find the expected value of a single indicator variable, $E[X_i]$. By the definition of expected value for a discrete random variable, $E[X_i]$ is calculated as the sum of each possible value multiplied by its probability:\n$$\nE[X_i] = (1) \\cdot P(X_i = 1) + (0) \\cdot P(X_i = 0) = P(X_i = 1)\n$$\nSo, the problem reduces to finding the probability that any given server, say server $i$, is assigned its correct task.\n\nConsider server $i$. There are $N$ unique tasks available to be assigned to it. Since the assignment is completely random, each of the $N$ tasks has an equal probability of being assigned to server $i$. Only one of these tasks is the correct one for server $i$.\nTherefore, the probability that server $i$ is assigned its correct task is:\n$$\nP(X_i = 1) = \\frac{1}{N}\n$$\nThis means the expected value of the indicator variable $X_i$ is:\n$$\nE[X_i] = \\frac{1}{N}\n$$\nThis probability, and thus the expectation, is the same for every server from $i=1$ to $N$.\n\nFinally, we can substitute this result back into the sum for the total expectation $E[X]$:\n$$\nE[X] = \\sum_{i=1}^{N} E[X_i] = \\sum_{i=1}^{N} \\frac{1}{N}\n$$\nThis summation consists of $N$ identical terms, each equal to $1/N$. Therefore, the sum is:\n$$\nE[X] = N \\times \\frac{1}{N} = 1\n$$\nThe expected number of correctly assigned servers is 1. Notably, this result is independent of the total number of servers $N$ (assuming $N \\ge 1$).", "answer": "$$\\boxed{1}$$", "id": "1329488"}, {"introduction": "Moving from theoretical principles to practical applications is a key step in mastering statistics. This problem models a common scenario in technology and business where a process is repeated until a failure occurs. By applying the geometric distribution to this scenario, you'll practice calculating the expected outcome of a process that involves both gains and losses, a fundamental skill in risk assessment and financial modeling. [@problem_id:1913504]", "problem": "A data analytics startup is stress-testing its new proprietary algorithm. The test consists of a sequence of trials, or \"runs\". Each run is independent of the others. For any given run, there is a constant probability $p$ that the algorithm encounters a critical failure and the entire testing process is immediately halted. If no failure occurs, the run is considered a success.\n\nThe startup has a performance-based contract with a client, leading to the following financial model for the testing phase:\n- For each successful run, the startup earns a revenue of $R_S$ dollars.\n- Each run, whether successful or a failure, incurs an operational cost of $C_O$ dollars.\n- A critical failure incurs a one-time penalty cost of $C_F$ dollars for data recovery and reporting.\n\nThe testing process begins and continues until the first critical failure is observed. Given the following parameters, determine the expected net profit from this entire testing process.\n\nParameters:\n- Probability of a critical failure on any given run, $p = 0.05$\n- Revenue from a successful run, $R_S = 300$ dollars\n- Operational cost per run, $C_O = 40$ dollars\n- One-time failure penalty cost, $C_F = 800$ dollars\n\nExpress your final answer in dollars.", "solution": "Let $p$ be the probability of failure on any run and $q=1-p$ the probability of success. The process stops at the first failure. Let $K$ be the number of successful runs before the first failure. Then $K$ has the geometric distribution on $\\{0,1,2,\\dots\\}$ with $\\mathbb{P}(K=k)=q^{k}p$ and expectation $\\mathbb{E}[K]=\\frac{q}{p}$.\n\nFor a realization with $k$ successes followed by one failure:\n- Revenue is $kR_{S}$.\n- Operational cost is $(k+1)C_{O}$ (every run, including the failing run, incurs $C_{O}$).\n- Failure penalty is $C_{F}$.\n\nHence the net profit for that realization is\n$$\n\\Pi(k)=kR_{S}-(k+1)C_{O}-C_{F}=k(R_{S}-C_{O})-C_{O}-C_{F}.\n$$\nTaking expectation and using $\\mathbb{E}[K]=\\frac{q}{p}$ gives\n$$\n\\mathbb{E}[\\Pi]=\\mathbb{E}[K](R_{S}-C_{O})-C_{O}-C_{F}\n=\\frac{q}{p}(R_{S}-C_{O})-C_{O}-C_{F}.\n$$\nSubstitute $p=0.05$, $q=1-0.05=0.95$, $R_{S}=300$, $C_{O}=40$, and $C_{F}=800$:\n$$\n\\mathbb{E}[\\Pi]=\\frac{0.95}{0.05}(300-40)-40-800\n=19\\cdot 260-40-800\n=4940-840\n=4100.\n$$\nThus, the expected net profit is $4100$ dollars.", "answer": "$$\\boxed{4100}$$", "id": "1913504"}, {"introduction": "In statistics, precision in language and concepts is paramount. A common point of confusion for students is the relationship between uncorrelated and independent random variables. This exercise is designed to clarify this distinction decisively by having you work through a concrete counterexample. Calculating the covariance and checking the condition for independence will demonstrate that zero covariance does not necessarily imply that two variables are independent, a crucial insight for developing statistical rigor. [@problem_id:1922916]", "problem": "Consider a pair of discrete random variables $(X, Y)$ whose joint probability mass function (PMF), denoted by $p(x, y) = P(X=x, Y=y)$, has non-zero values only at four specific points. These probabilities are given as:\n- $p(-1, 0) = \\frac{1}{3}$\n- $p(1, 0) = \\frac{1}{6}$\n- $p(0, -1) = \\frac{1}{4}$\n- $p(0, 1) = \\frac{1}{4}$\n\nFor all other pairs $(x, y)$, the joint PMF is $p(x, y) = 0$.\n\nYour task is to compute two quantities based on this distribution. First, calculate the covariance between $X$ and $Y$, denoted as $\\text{Cov}(X,Y)$. Second, calculate the value of $\\delta = P(X=1)P(Y=0) - P(X=1, Y=0)$, which is a measure related to the independence of the events $\\{X=1\\}$ and $\\{Y=0\\}$.\n\nPresent your final answer as an ordered pair $(\\text{Cov}(X,Y), \\delta)$.", "solution": "We use the definitions of expectation and covariance for discrete random variables. The covariance is defined by $\\operatorname{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]$. The given joint PMF has support at $(-1,0)$ with probability $\\frac{1}{3}$, $(1,0)$ with probability $\\frac{1}{6}$, $(0,-1)$ with probability $\\frac{1}{4}$, and $(0,1)$ with probability $\\frac{1}{4}$, and zero elsewhere. The total probability is $\\frac{1}{3}+\\frac{1}{6}+\\frac{1}{4}+\\frac{1}{4}=1$, so it is a valid PMF.\n\nFirst, compute the marginals. For $X$:\n$$\nP(X=-1)=\\frac{1}{3},\\quad P(X=1)=\\frac{1}{6},\\quad P(X=0)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}.\n$$\nFor $Y$:\n$$\nP(Y=0)=\\frac{1}{3}+\\frac{1}{6}=\\frac{1}{2},\\quad P(Y=-1)=\\frac{1}{4},\\quad P(Y=1)=\\frac{1}{4}.\n$$\n\nCompute expectations using $\\mathbb{E}[X]=\\sum_{x} x\\,P(X=x)$ and $\\mathbb{E}[Y]=\\sum_{y} y\\,P(Y=y)$:\n$$\n\\mathbb{E}[X]=(-1)\\cdot \\frac{1}{3}+1\\cdot \\frac{1}{6}+0\\cdot \\frac{1}{2}=-\\frac{1}{6},\n$$\n$$\n\\mathbb{E}[Y]=0\\cdot \\frac{1}{2}+(-1)\\cdot \\frac{1}{4}+1\\cdot \\frac{1}{4}=0.\n$$\n\nCompute $\\mathbb{E}[XY]$ using $\\mathbb{E}[XY]=\\sum_{x,y} xy\\,p(x,y)$ over the support:\n$$\n\\mathbb{E}[XY]=(-1\\cdot 0)\\cdot \\frac{1}{3}+(1\\cdot 0)\\cdot \\frac{1}{6}+(0\\cdot (-1))\\cdot \\frac{1}{4}+(0\\cdot 1)\\cdot \\frac{1}{4}=0.\n$$\n\nTherefore, by the covariance formula,\n$$\n\\operatorname{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]=0-\\left(-\\frac{1}{6}\\right)\\cdot 0=0.\n$$\n\nNext, compute $\\delta=P(X=1)P(Y=0)-P(X=1,Y=0)$ from the marginals and joint probability:\n$$\n\\delta=\\left(\\frac{1}{6}\\right)\\left(\\frac{1}{2}\\right)-\\frac{1}{6}=\\frac{1}{12}-\\frac{1}{6}=-\\frac{1}{12}.\n$$\n\nThus, the ordered pair $(\\operatorname{Cov}(X,Y),\\delta)$ is $\\left(0,-\\frac{1}{12}\\right)$.", "answer": "$$\\boxed{(0, -\\frac{1}{12})}$$", "id": "1922916"}]}