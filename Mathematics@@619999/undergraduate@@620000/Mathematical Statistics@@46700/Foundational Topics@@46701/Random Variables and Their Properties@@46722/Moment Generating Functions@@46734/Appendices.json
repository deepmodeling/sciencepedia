{"hands_on_practices": [{"introduction": "Mastering a new tool begins with applying it to a familiar context. This first exercise provides a hands-on opportunity to build the moment generating function (MGF) for a simple discrete random variableâ€”the outcome of a fair die roll. By using the MGF to calculate the variance, a value you might already know how to find using other methods, you'll gain direct experience with the mechanics of MGFs and appreciate their systematic approach to computing moments [@problem_id:1937162].", "problem": "A quality control process involves selecting one component at random from a bin containing six distinct types of components, labeled sequentially from 1 to 6. Let the random variable $X$ represent the label of the selected component. Assuming each component type has an equal probability of being selected, determine the variance of $X$. Your calculation must proceed by first finding the Moment Generating Function (MGF) of $X$ and then using its properties to find the required moments. The final answer should be an exact fraction.", "solution": "Let $X$ be uniformly distributed on the set $\\{1,2,3,4,5,6\\}$, so $\\mathbb{P}(X=k)=\\frac{1}{6}$ for $k=1,2,3,4,5,6$.\n\nBy definition, the moment generating function (MGF) of $X$ is\n$$\nM_{X}(t)=\\mathbb{E}\\!\\left[\\exp(tX)\\right]=\\sum_{k=1}^{6}\\exp(tk)\\,\\mathbb{P}(X=k)=\\frac{1}{6}\\sum_{k=1}^{6}\\exp(tk).\n$$\nUsing the finite geometric series identity with $r=\\exp(t)$, we can write\n$$\nM_{X}(t)=\\frac{1}{6}\\sum_{k=1}^{6}\\exp(tk)=\\frac{1}{6}\\,\\frac{\\exp(t)\\left(1-\\exp(6t)\\right)}{1-\\exp(t)}.\n$$\n\nThe MGF property states that the $n$-th moment is given by $M_{X}^{(n)}(0)=\\mathbb{E}[X^{n}]$. Differentiating the finite sum term-by-term,\n$$\nM_{X}'(t)=\\frac{1}{6}\\sum_{k=1}^{6}k\\,\\exp(tk),\\qquad M_{X}''(t)=\\frac{1}{6}\\sum_{k=1}^{6}k^{2}\\,\\exp(tk).\n$$\nEvaluating at $t=0$ gives\n$$\n\\mathbb{E}[X]=M_{X}'(0)=\\frac{1}{6}\\sum_{k=1}^{6}k=\\frac{1}{6}\\cdot\\frac{6\\cdot 7}{2}=\\frac{7}{2},\n$$\nand\n$$\n\\mathbb{E}[X^{2}]=M_{X}''(0)=\\frac{1}{6}\\sum_{k=1}^{6}k^{2}=\\frac{1}{6}\\cdot\\frac{6\\cdot 7\\cdot 13}{6}=\\frac{91}{6}.\n$$\n\nUsing $\\operatorname{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}$, we obtain\n$$\n\\operatorname{Var}(X)=\\frac{91}{6}-\\left(\\frac{7}{2}\\right)^{2}=\\frac{91}{6}-\\frac{49}{4}=\\frac{182}{12}-\\frac{147}{12}=\\frac{35}{12}.\n$$\nThus, the variance is the exact fraction $\\frac{35}{12}$.", "answer": "$$\\boxed{\\frac{35}{12}}$$", "id": "1937162"}, {"introduction": "One of the most powerful properties of the MGF is its uniqueness; it acts as a unique \"fingerprint\" for a probability distribution. This practice moves beyond simple moment calculation and challenges you to work in reverse. Given the MGF of an unknown random variable, your task is to identify the underlying distribution and its parameters by recognizing its characteristic form [@problem_id:1376232]. This skill is crucial for efficiently solving problems and for understanding the relationships between different families of distributions.", "problem": "An engineer is analyzing the performance of a wireless sensor network designed for environmental monitoring. The network consists of 15 independent and identical sensors deployed across a region. Each sensor is polled once per hour to transmit its collected data. Due to various factors like channel noise and interference, a sensor may or may not successfully transmit its data packet to the central hub in any given hour.\n\nLet the random variable $X$ represent the total number of successful data transmissions from the 15 sensors in a particular hour. The engineer has derived the moment generating function (MGF) for $X$, which is given by:\n$$M_X(t) = (0.6e^t + 0.4)^{15}$$\nCalculate the variance of the random variable $X$. Express your answer as a number with two significant figures.", "solution": "We are given the moment generating function (MGF) of $X$ as $M_{X}(t) = (0.6 \\exp(t) + 0.4)^{15}$. The MGF of a binomial random variable with parameters $n$ and $p$ is $M(t) = (p \\exp(t) + 1 - p)^{n}$. By identification, $X \\sim \\mathrm{Bin}(n, p)$ with $n = 15$ and $p = 0.6$.\n\nUsing the MGF definition of moments, let $f(t) = p \\exp(t) + 1 - p$. Then $M(t) = f(t)^{n}$, $f'(t) = p \\exp(t)$, and $f''(t) = p \\exp(t)$. By the chain rule and product rule,\n$$\nM'(t) = n f(t)^{n-1} f'(t), \\quad M''(t) = n \\left[(n-1) f(t)^{n-2} \\left(f'(t)\\right)^{2} + f(t)^{n-1} f''(t)\\right].\n$$\nEvaluating at $t = 0$ gives $f(0) = 1$, $f'(0) = p$, and $f''(0) = p$, hence\n$$\nM'(0) = n p, \\quad M''(0) = n p \\left[(n-1) p + 1\\right].\n$$\nThe variance is $\\operatorname{Var}(X) = M''(0) - \\left(M'(0)\\right)^{2}$, so\n$$\n\\operatorname{Var}(X) = n p \\left[(n-1) p + 1\\right] - n^{2} p^{2} = n p (1 - p).\n$$\nSubstituting $n = 15$ and $p = 0.6$ yields\n$$\n\\operatorname{Var}(X) = 15 \\cdot 0.6 \\cdot 0.4 = 3.6.\n$$\nThis is expressed with two significant figures as required.", "answer": "$$\\boxed{3.6}$$", "id": "1376232"}, {"introduction": "Moment generating functions do more than just generate moments; their algebraic structure can reveal deep insights into the nature of a random variable. This advanced problem demonstrates how we can use mathematical techniques, in this case partial fraction decomposition, to deconstruct a given MGF. By doing so, you will discover that a seemingly single distribution is, in fact, a probabilistic mixture of simpler, more familiar distributions [@problem_id:1937156]. This exercise showcases the elegance and analytical power of the MGF in theoretical statistics.", "problem": "Let a random variable $X$ have the moment generating function (MGF) $M_X(t) = (1 - \\beta^2 t^2)^{-1}$ for a positive constant $\\beta$ and for all $t$ in a neighborhood of zero where the MGF is defined. It is known that $X$ can be described as a probabilistic mixture of two simpler random variables. Specifically, the MGF of $X$ can be written in the form $M_X(t) = p \\cdot M_{W_1}(t) + (1-p) \\cdot M_{W_2}(t)$, where $p$ is the mixing probability ($0 < p < 1$), and $M_{W_1}(t)$ and $M_{W_2}(t)$ are the MGFs for an exponential random variable and its negative, respectively. Let the exponential random variable, $W_1$, have a rate parameter $\\lambda > 0$. The other variable is then $W_2 = -W_1$.\n\nDetermine the value of the mixing probability $p$ and the rate parameter $\\lambda$ in terms of $\\beta$. Present your answer as a pair $(p, \\lambda)$.", "solution": "The problem requires us to find the parameters of a mixture distribution that result in a given moment generating function (MGF).\n\nFirst, let's establish the MGFs for the component random variables, $W_1$ and $W_2$.\nThe random variable $W_1$ follows an Exponential distribution with rate $\\lambda$. The MGF of an exponential random variable with rate $\\lambda$ is given by:\n$$M_{W_1}(t) = E[\\exp(tW_1)] = \\frac{\\lambda}{\\lambda - t} = \\frac{1}{1 - t/\\lambda}$$\nThis MGF is defined for $t < \\lambda$.\n\nThe random variable $W_2$ is the negative of $W_1$, so $W_2 = -W_1$. Its MGF can be found from the MGF of $W_1$:\n$$M_{W_2}(t) = E[\\exp(tW_2)] = E[\\exp(t(-W_1))] = E[\\exp((-t)W_1)] = M_{W_1}(-t)$$\nSubstituting $-t$ into the expression for $M_{W_1}(t)$, we get:\n$$M_{W_2}(t) = \\frac{1}{1 - (-t)/\\lambda} = \\frac{1}{1 + t/\\lambda}$$\nThis MGF is defined for $t > -\\lambda$.\n\nThe problem states that $X$ is a mixture of $W_1$ and $W_2$ with mixing probability $p$. The MGF of such a mixture is the weighted sum of the individual MGFs:\n$$M_X(t) = p \\cdot M_{W_1}(t) + (1-p) \\cdot M_{W_2}(t)$$\nSubstituting the expressions for the MGFs of $W_1$ and $W_2$:\n$$M_X(t) = \\frac{p}{1 - t/\\lambda} + \\frac{1-p}{1 + t/\\lambda}$$\n\nNext, we analyze the given MGF for $X$, $M_X(t) = (1 - \\beta^2 t^2)^{-1}$. To compare this with the mixture form, we need to express it as a sum of simpler fractions. This is achieved through partial fraction decomposition.\nFirst, factor the denominator:\n$$M_X(t) = \\frac{1}{1 - \\beta^2 t^2} = \\frac{1}{(1 - \\beta t)(1 + \\beta t)}$$\nWe seek to find constants $A$ and $B$ such that:\n$$\\frac{1}{(1 - \\beta t)(1 + \\beta t)} = \\frac{A}{1 - \\beta t} + \\frac{B}{1 + \\beta t}$$\nTo find $A$ and $B$, we combine the terms on the right-hand side and equate the numerators:\n$$1 = A(1 + \\beta t) + B(1 - \\beta t)$$\nThis equation must hold for all values of $t$. We can solve for $A$ and $B$ by substituting convenient values for $t$.\nLet $t = 1/\\beta$:\n$$1 = A(1 + \\beta(1/\\beta)) + B(1 - \\beta(1/\\beta)) = A(1+1) + B(0) = 2A$$\nThis gives $A = 1/2$.\n\nLet $t = -1/\\beta$:\n$$1 = A(1 + \\beta(-1/\\beta)) + B(1 - \\beta(-1/\\beta)) = A(0) + B(1+1) = 2B$$\nThis gives $B = 1/2$.\n\nSo, the partial fraction decomposition of $M_X(t)$ is:\n$$M_X(t) = \\frac{1/2}{1 - \\beta t} + \\frac{1/2}{1 + \\beta t}$$\n\nFinally, we equate the two expressions for $M_X(t)$:\n$$\\frac{p}{1 - t/\\lambda} + \\frac{1-p}{1 + t/\\lambda} = \\frac{1/2}{1 - \\beta t} + \\frac{1/2}{1 + \\beta t}$$\nBy comparing the terms, we can identify the unknown parameters. The uniqueness of the partial fraction decomposition allows for a direct term-by-term comparison.\nComparing the first term on each side:\n$$\\frac{p}{1 - t/\\lambda} = \\frac{1/2}{1 - \\beta t}$$\nFor the denominators to be equal for all $t$, we must have $t/\\lambda = \\beta t$, which implies $\\lambda = 1/\\beta$.\nFor the numerators to be equal, we must have $p = 1/2$.\n\nComparing the second term on each side provides a consistency check:\n$$\\frac{1-p}{1 + t/\\lambda} = \\frac{1/2}{1 + \\beta t}$$\nAgain, the denominators imply $\\lambda = 1/\\beta$. The numerators imply $1-p = 1/2$, which also gives $p = 1/2$.\nBoth comparisons yield the same results.\n\nThus, the mixing probability is $p=1/2$, and the rate parameter of the exponential distribution is $\\lambda = 1/\\beta$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} & \\frac{1}{\\beta} \\end{pmatrix}}$$", "id": "1937156"}]}