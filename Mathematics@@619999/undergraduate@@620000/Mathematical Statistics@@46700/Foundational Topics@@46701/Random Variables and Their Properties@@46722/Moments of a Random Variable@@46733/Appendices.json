{"hands_on_practices": [{"introduction": "Mastering the calculation of moments begins with applying their fundamental definitions to key probability distributions. This exercise [@problem_id:1937415] provides practice with the Poisson distribution, a cornerstone of modeling count data. By calculating the expectation of $N(N-1)$, known as the second factorial moment, you will not only reinforce your summation skills but also uncover a computationally elegant path to finding the variance of the distribution.", "problem": "In a simplified model of spontaneous signal generation in a neural circuit, the number of action potentials, $N$, fired by a neuron in a fixed time interval is described by a Poisson random variable. The probability of observing $n$ action potentials in this interval is given by the probability mass function (PMF):\n$$ P(N=n) = \\frac{\\lambda^n \\exp(-\\lambda)}{n!}, \\quad \\text{for } n = 0, 1, 2, \\dots $$\nwhere $\\lambda > 0$ is a dimensionless parameter representing the average number of action potentials in the interval.\n\nA particular synaptic potentiation mechanism is triggered by the near-simultaneous arrival of pairs of action potentials. The strength of this potentiation is found to be proportional to a quantity $S = N(N-1)$, which counts the number of ordered pairs of distinct action potentials.\n\nDetermine the expected value of the potentiation strength measure, $S$, in terms of the parameter $\\lambda$.", "solution": "We are given that $N$ is a Poisson random variable with parameter $\\lambda>0$ and probability mass function $P(N=n)=\\frac{\\lambda^{n}\\exp(-\\lambda)}{n!}$ for $n=0,1,2,\\dots$. The potentiation strength is $S=N(N-1)$. For a discrete random variable, the expectation of a function $g(N)$ is computed as $\\mathbb{E}[g(N)]=\\sum_{n=0}^{\\infty}g(n)P(N=n)$. Applying this with $g(n)=n(n-1)$ gives\n$$\n\\mathbb{E}[S]=\\mathbb{E}[N(N-1)]=\\sum_{n=0}^{\\infty}n(n-1)\\frac{\\lambda^{n}\\exp(-\\lambda)}{n!}.\n$$\nThe terms for $n=0$ and $n=1$ vanish, so we can start the sum at $n=2$. Using the identity $\\frac{n(n-1)}{n!}=\\frac{1}{(n-2)!}$, we obtain\n$$\n\\mathbb{E}[N(N-1)]=\\exp(-\\lambda)\\sum_{n=2}^{\\infty}\\frac{\\lambda^{n}}{(n-2)!}\n=\\exp(-\\lambda)\\lambda^{2}\\sum_{n=2}^{\\infty}\\frac{\\lambda^{n-2}}{(n-2)!}.\n$$\nWith the change of variable $m=n-2$, this becomes\n$$\n\\mathbb{E}[N(N-1)]=\\exp(-\\lambda)\\lambda^{2}\\sum_{m=0}^{\\infty}\\frac{\\lambda^{m}}{m!}\n=\\exp(-\\lambda)\\lambda^{2}\\exp(\\lambda)=\\lambda^{2}.\n$$\nTherefore, the expected potentiation strength is $\\lambda^{2}$.", "answer": "$$\\boxed{\\lambda^{2}}$$", "id": "1937415"}, {"introduction": "Sometimes, the most elegant solutions in statistics come not from brute-force integration, but from insightful structural arguments. This problem [@problem_id:1319673] challenges you to find the expected value of a ratio of squared standard normal variables, a quantity relevant in signal processing. The key to solving this efficiently lies in recognizing the underlying symmetry of the problem, a powerful technique that can dramatically simplify seemingly complex expectations.", "problem": "In a digital communication system, a receiver measures two quadrature components of a noisy signal. Let these measured components be represented by the random variables $X$ and $Y$. Both $X$ and $Y$ are modeled as independent standard normal random variables, i.e., $X \\sim \\mathcal{N}(0, 1)$ and $Y \\sim \\mathcal{N}(0, 1)$. The instantaneous power of the in-phase component is proportional to $X^2$, and the instantaneous power of the quadrature component is proportional to $Y^2$. The total instantaneous power is thus proportional to $X^2 + Y^2$.\n\nAn analyst is interested in understanding the statistical properties of the signal distribution and wants to determine the expected fraction of power contributed by the in-phase component relative to the total power.\n\nCalculate the expected value of the ratio $\\frac{X^2}{X^2 + Y^2}$. Your final answer should be a single numerical value, expressed as a fraction or a decimal.", "solution": "Let $X$ and $Y$ be independent with $X \\sim \\mathcal{N}(0,1)$ and $Y \\sim \\mathcal{N}(0,1)$. Then $X^{2}$ and $Y^{2}$ are independent chi-square random variables with one degree of freedom. A chi-square random variable with one degree of freedom is a Gamma random variable with shape parameter $1/2$ and scale parameter $2$. Define\n$$\nU = \\frac{X^{2}}{2}, \\quad V = \\frac{Y^{2}}{2}.\n$$\nThen $U$ and $V$ are independent and $U \\sim \\text{Gamma}\\!\\left(\\frac{1}{2}, 1\\right)$, $V \\sim \\text{Gamma}\\!\\left(\\frac{1}{2}, 1\\right)$. Consider the ratio\n$$\nR = \\frac{X^{2}}{X^{2} + Y^{2}} = \\frac{U}{U+V}.\n$$\nA standard result states that if $U \\sim \\text{Gamma}(a,1)$ and $V \\sim \\text{Gamma}(b,1)$ are independent, then $\\frac{U}{U+V} \\sim \\text{Beta}(a,b)$. Therefore,\n$$\nR \\sim \\text{Beta}\\!\\left(\\frac{1}{2}, \\frac{1}{2}\\right).\n$$\nThe expected value of a $\\text{Beta}(a,b)$ random variable is $\\frac{a}{a+b}$. Hence,\n$$\n\\mathbb{E}\\!\\left[\\frac{X^{2}}{X^{2} + Y^{2}}\\right] = \\mathbb{E}[R] = \\frac{\\frac{1}{2}}{\\frac{1}{2} + \\frac{1}{2}} = \\frac{1}{2}.\n$$\n\nAs a consistency check using symmetry, note that by exchangeability of $X$ and $Y$,\n$$\n\\mathbb{E}\\!\\left[\\frac{X^{2}}{X^{2} + Y^{2}}\\right] = \\mathbb{E}\\!\\left[\\frac{Y^{2}}{X^{2} + Y^{2}}\\right].\n$$\nSince\n$$\n\\frac{X^{2}}{X^{2} + Y^{2}} + \\frac{Y^{2}}{X^{2} + Y^{2}} = 1,\n$$\ntaking expectations gives\n$$\n\\mathbb{E}\\!\\left[\\frac{X^{2}}{X^{2} + Y^{2}}\\right] + \\mathbb{E}\\!\\left[\\frac{Y^{2}}{X^{2} + Y^{2}}\\right] = 1,\n$$\nso each expectation equals $\\frac{1}{2}$, in agreement with the Beta result.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1319673"}, {"introduction": "While the mean and variance provide a crucial first look at a distribution's center and spread, they don't tell the whole story. This exercise [@problem_id:1937417] delves into the descriptive power of higher-order moments by asking you to construct a distribution that shares the same mean and variance as another but differs in its symmetry. By calculating the third central moment, you will gain hands-on experience with how moments, particularly skewness, quantify the shape and asymmetry of a probability distribution.", "problem": "An analyst is studying two different stochastic processes, which generate integer-valued data. The outputs of these processes are modeled by two distinct discrete random variables, $X$ and $Y$. Both variables are known to have a distribution described by a Probability Mass Function (PMF) supported on exactly three distinct integer values.\n\nThrough preliminary analysis, it has been determined that both variables share the same mean, $\\mu = 0$, and the same variance, $\\sigma^2 = 2$. To distinguish the two processes, the analyst decides to compute their third central moments. Since the mean is zero, these are given by $\\mu_{3,X} = E[X^3]$ and $\\mu_{3,Y} = E[Y^3]$.\n\nIt is known that the distribution of $X$ is symmetric about its mean. The support set for $X$ is of the form $\\{-a, 0, a\\}$ for some positive integer $a$.\nThe distribution of $Y$ is not symmetric. The support set for $Y$ is of the form $\\{-b, 0, c\\}$ for some distinct positive integers $b$ and $c$.\n\nTo ensure a unique construction for $Y$, assume that $b$ and $c$ are chosen from the set of positive integers such that their sum $b+c$ is minimized. Additionally, assume $b < c$.\n\nCalculate the value of the third central moment of $Y$, which is $\\mu_{3,Y}$. Express your answer as an exact fraction or an integer.", "solution": "The problem asks for the third central moment of a random variable $Y$, whose distribution must be constructed based on a set of given conditions. Let's break down the problem into two parts: first, characterizing the distribution of $X$, and second, constructing the distribution of $Y$ to find its third central moment.\n\n**Part 1: Characterizing the distribution of X**\n\nThe random variable $X$ has a support set $S_X = \\{-a, 0, a\\}$, where $a$ is a positive integer. Let the probabilities be $P(X=-a) = p_{-a}$, $P(X=0) = p_0$, and $P(X=a) = p_a$.\n\nThe sum of probabilities must be 1:\n$p_{-a} + p_0 + p_a = 1$.\n\nThe mean is given as $\\mu = 0$:\n$E[X] = (-a)p_{-a} + (0)p_0 + (a)p_a = a(p_a - p_{-a}) = 0$.\nSince $a > 0$, this implies $p_a = p_{-a}$. This is also consistent with the given information that the distribution is symmetric.\n\nLet's call this common probability $p = p_a = p_{-a}$. The sum of probabilities becomes $2p + p_0 = 1$, so $p_0 = 1 - 2p$. For $p_0$ to be a valid probability, $1 - 2p \\ge 0$, which implies $p \\le 1/2$.\n\nThe variance is given as $\\sigma^2 = 2$. Since the mean is 0, the variance is equal to the second raw moment $E[X^2]$.\n$\\text{Var}(X) = E[X^2] = (-a)^2 p_{-a} + (0)^2 p_0 + (a)^2 p_a = a^2 p + a^2 p = 2pa^2$.\nWe are given $\\text{Var}(X) = 2$, so we have the equation:\n$2pa^2 = 2 \\implies pa^2 = 1$.\n\nSubstituting $p = 1/a^2$ into the constraint $p \\le 1/2$, we get $1/a^2 \\le 1/2$, which means $a^2 \\ge 2$.\nSince $a$ must be a positive integer, the smallest possible value for $a$ is $2$. Although any integer $a \\ge 2$ could define a valid distribution for $X$, the properties of $X$ are only used to establish the target mean and variance. We can proceed with the established values of $\\mu=0$ and $\\sigma^2=2$.\n\nThe third central moment of $X$ is $\\mu_{3,X} = E[X^3]$.\n$\\mu_{3,X} = (-a)^3 p + (0)^3 (1-2p) + (a)^3 p = -a^3 p + a^3 p = 0$.\nThis confirms that the symmetric distribution has a zero third central moment, as expected.\n\n**Part 2: Constructing the distribution of Y**\n\nThe random variable $Y$ has a support set $S_Y = \\{-b, 0, c\\}$, where $b$ and $c$ are distinct positive integers with $b < c$. Let the probabilities be $P(Y=-b) = q_{-b}$, $P(Y=0) = q_0$, and $P(Y=c) = q_c$.\n\nThe sum of probabilities is 1: $q_{-b} + q_0 + q_c = 1$.\n\nThe mean is $\\mu = 0$:\n$E[Y] = (-b)q_{-b} + (0)q_0 + (c)q_c = c q_c - b q_{-b} = 0 \\implies q_c = \\frac{b}{c} q_{-b}$.\n\nThe variance is $\\sigma^2 = 2$:\n$\\text{Var}(Y) = E[Y^2] = (-b)^2 q_{-b} + (0)^2 q_0 + (c)^2 q_c = b^2 q_{-b} + c^2 q_c = 2$.\n\nNow we solve for the probabilities in terms of $b$ and $c$. Substitute $q_c = \\frac{b}{c} q_{-b}$ into the variance equation:\n$b^2 q_{-b} + c^2 \\left(\\frac{b}{c} q_{-b}\\right) = 2$\n$b^2 q_{-b} + bc q_{-b} = 2$\n$q_{-b}(b^2 + bc) = 2 \\implies q_{-b}(b(b+c)) = 2$\n$q_{-b} = \\frac{2}{b(b+c)}$.\n\nUsing this, we find $q_c$:\n$q_c = \\frac{b}{c} q_{-b} = \\frac{b}{c} \\frac{2}{b(b+c)} = \\frac{2}{c(b+c)}$.\n\nFinally, we find $q_0$ from the sum of probabilities:\n$q_0 = 1 - q_{-b} - q_c = 1 - \\frac{2}{b(b+c)} - \\frac{2}{c(b+c)} = 1 - \\frac{2c + 2b}{bc(b+c)} = 1 - \\frac{2(b+c)}{bc(b+c)} = 1 - \\frac{2}{bc}$.\n\nFor $\\{q_{-b}, q_0, q_c\\}$ to be a valid PMF for a three-point distribution, all probabilities must be strictly positive. $q_{-b}$ and $q_c$ are positive since $b, c > 0$. For $q_0 > 0$, we require:\n$1 - \\frac{2}{bc} > 0 \\implies 1 > \\frac{2}{bc} \\implies bc > 2$.\n\nThe problem states that we must choose a pair of distinct positive integers $(b,c)$ with $b<c$ that minimizes the sum $b+c$, subject to the constraint $bc>2$. Let's test possible values for $b$:\n- If $b=1$: The constraint becomes $c > 2$. Since $c$ must be an integer, the smallest possible value for $c$ is $3$. This gives the pair $(1,3)$. Here, $b \\ne c$ and $b<c$ are satisfied. The sum is $b+c = 1+3=4$.\n- If $b=2$: The constraint becomes $2c > 2$, so $c > 1$. Since we require $b < c$, the smallest possible integer value for $c$ is $3$. This gives the pair $(2,3)$. The sum is $b+c = 2+3=5$.\n- If $b=3$: The constraint is $3c>2$, so $c>2/3$. Since $b<c$, smallest integer $c$ is 4. The sum is $b+c=3+4=7$.\n\nComparing the sums, the minimum sum is $4$, which corresponds to the unique pair $(b,c) = (1,3)$.\n\nNow we can determine the specific PMF for $Y$:\nSupport $S_Y = \\{-1, 0, 3\\}$.\n$q_{-1} = \\frac{2}{1(1+3)} = \\frac{2}{4} = \\frac{1}{2}$.\n$q_3 = \\frac{2}{3(1+3)} = \\frac{2}{12} = \\frac{1}{6}$.\n$q_0 = 1 - \\frac{2}{1 \\cdot 3} = 1 - \\frac{2}{3} = \\frac{1}{3}$.\nSo, the PMF for $Y$ is $P(Y=-1) = 1/2$, $P(Y=0) = 1/3$, and $P(Y=3) = 1/6$.\n\n**Part 3: Calculating the third central moment of Y**\n\nThe final step is to calculate $\\mu_{3,Y} = E[Y^3]$:\n$\\mu_{3,Y} = E[Y^3] = (-1)^3 P(Y=-1) + (0)^3 P(Y=0) + (3)^3 P(Y=3)$\n$\\mu_{3,Y} = (-1)\\left(\\frac{1}{2}\\right) + (0)\\left(\\frac{1}{3}\\right) + (27)\\left(\\frac{1}{6}\\right)$\n$\\mu_{3,Y} = -\\frac{1}{2} + \\frac{27}{6} = -\\frac{3}{6} + \\frac{27}{6} = \\frac{24}{6} = 4$.\n\nThe third central moment of $Y$ is 4.", "answer": "$$\\boxed{4}$$", "id": "1937417"}]}