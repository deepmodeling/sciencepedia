{"hands_on_practices": [{"introduction": "This first practice problem introduces a classic two-stage experiment, a common scenario where the Law of Total Variance is applied. By analyzing a game where a die-roll's outcome determines the parameters of a subsequent prize draw, we can partition the total uncertainty in the prize's value. This exercise [@problem_id:1401034] will help you build intuition for the two components of total variance: the average variance inherent in the second stage, and the variance caused by the randomness of the first.", "problem": "A player participates in a two-stage game of chance. In the first stage, they roll a single fair six-sided die. Let the outcome of the roll be denoted by the random variable $K$. In the second stage, the player draws a prize with a monetary value, represented by the random variable $X$. The distribution from which the prize value is drawn depends on the outcome of the die roll. Specifically, for a given die roll outcome $K=k$, the prize value $X$ is drawn from a distribution with a mean of $k$ and a variance of $k^2$.\n\nCalculate the overall variance, $\\operatorname{Var}(X)$, of the prize value. Express your answer as an exact fraction in simplest form.", "solution": "Let $K$ be the outcome of a fair six-sided die, so $K \\in \\{1,2,3,4,5,6\\}$ with equal probability. Given $K=k$, the conditional mean and variance of $X$ are $\\mathbb{E}[X \\mid K=k]=k$ and $\\operatorname{Var}(X \\mid K=k)=k^{2}$.\n\nBy the law of total variance,\n$$\n\\operatorname{Var}(X)=\\mathbb{E}\\!\\left[\\operatorname{Var}(X \\mid K)\\right]+\\operatorname{Var}\\!\\left(\\mathbb{E}[X \\mid K]\\right).\n$$\nUsing the given conditional moments, this becomes\n$$\n\\operatorname{Var}(X)=\\mathbb{E}[K^{2}]+\\operatorname{Var}(K).\n$$\n\nCompute the moments of $K$ for a fair die:\n$$\n\\mathbb{E}[K]=\\frac{1}{6}\\sum_{k=1}^{6}k=\\frac{21}{6}=\\frac{7}{2},\n$$\n$$\n\\mathbb{E}[K^{2}]=\\frac{1}{6}\\sum_{k=1}^{6}k^{2}=\\frac{91}{6}.\n$$\nThus,\n$$\n\\operatorname{Var}(K)=\\mathbb{E}[K^{2}]-(\\mathbb{E}[K])^{2}=\\frac{91}{6}-\\left(\\frac{7}{2}\\right)^{2}=\\frac{91}{6}-\\frac{49}{4}=\\frac{35}{12}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(X)=\\mathbb{E}[K^{2}]+\\operatorname{Var}(K)=\\frac{91}{6}+\\frac{35}{12}=\\frac{182+35}{12}=\\frac{217}{12}.\n$$\nThis fraction is in simplest form.", "answer": "$$\\boxed{\\frac{217}{12}}$$", "id": "1401034"}, {"introduction": "Building on the previous example, this problem explores a scenario that mixes discrete and continuous randomness. We first make a discrete choice between two functions, $\\sin(x)$ and $\\cos(x)$, and then evaluate the chosen function at a random angle. This practice [@problem_id:1929471] highlights how the Law of Total Variance elegantly partitions variance and can reveal interesting symmetries, such as when the expected outcome is the same regardless of the initial random choice.", "problem": "Consider a two-stage random process.\nIn the first stage, a function, let's call it $F$, is selected. With a probability of $1/2$, the function is $F(x) = \\sin(x)$. With the remaining probability of $1/2$, the function is $F(x) = \\cos(x)$.\nIn the second stage, an angle $\\Theta$ is drawn as a random variable from a continuous uniform distribution over the interval $[0, 2\\pi]$.\nA new random variable, $Y$, is then defined as the result of applying the chosen function $F$ to the random angle $\\Theta$, such that $Y = F(\\Theta)$.\n\nCalculate the total variance of the random variable $Y$.", "solution": "Let $S$ denote the random choice of function, with $\\mathbb{P}(S=\\sin)=\\mathbb{P}(S=\\cos)=\\frac{1}{2}$. Let $\\Theta$ be independent of $S$ and uniformly distributed on $[0,2\\pi]$. Define $Y=\\sin(\\Theta)$ if $S=\\sin$ and $Y=\\cos(\\Theta)$ if $S=\\cos$.\n\nBy the law of total variance,\n$$\n\\operatorname{Var}(Y)=\\mathbb{E}\\!\\left[\\operatorname{Var}(Y\\mid S)\\right]+\\operatorname{Var}\\!\\left(\\mathbb{E}[Y\\mid S]\\right).\n$$\n\nFirst compute the conditional means. For $S=\\sin$,\n$$\n\\mathbb{E}[Y\\mid S=\\sin]=\\mathbb{E}[\\sin(\\Theta)]=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\sin(x)\\,dx=0,\n$$\nand for $S=\\cos$,\n$$\n\\mathbb{E}[Y\\mid S=\\cos]=\\mathbb{E}[\\cos(\\Theta)]=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\cos(x)\\,dx=0.\n$$\nHence $\\mathbb{E}[Y\\mid S]=0$ almost surely, so\n$$\n\\operatorname{Var}\\!\\left(\\mathbb{E}[Y\\mid S]\\right)=0.\n$$\n\nNext compute the conditional variances. For $S=\\sin$,\n$$\n\\operatorname{Var}(Y\\mid S=\\sin)=\\mathbb{E}[\\sin^{2}(\\Theta)]-\\left(\\mathbb{E}[\\sin(\\Theta)]\\right)^{2}=\\mathbb{E}[\\sin^{2}(\\Theta)],\n$$\nand\n$$\n\\mathbb{E}[\\sin^{2}(\\Theta)]=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\sin^{2}(x)\\,dx=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\frac{1-\\cos(2x)}{2}\\,dx=\\frac{1}{2\\pi}\\left[\\frac{x}{2}-\\frac{\\sin(2x)}{4}\\right]_{0}^{2\\pi}=\\frac{1}{2}.\n$$\nSimilarly, for $S=\\cos$,\n$$\n\\operatorname{Var}(Y\\mid S=\\cos)=\\mathbb{E}[\\cos^{2}(\\Theta)]=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\cos^{2}(x)\\,dx=\\frac{1}{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[\\operatorname{Var}(Y\\mid S)\\right]=\\frac{1}{2}\\cdot\\frac{1}{2}+\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{2}.\n$$\n\nCombining, we obtain\n$$\n\\operatorname{Var}(Y)=\\frac{1}{2}+0=\\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1929471"}, {"introduction": "Our final practice problem tackles a more advanced and widely applicable scenario: finding the variance of a sum of random variables where the number of terms in the sum is itself a random variable. This situation models many real-world processes, from the total claims an insurance company faces in a year to the final position of a particle in a random walk. By conditioning on the number of steps taken, this exercise [@problem_id:1401012] demonstrates a powerful and general technique for analyzing compound random variables using the Law of Total Variance.", "problem": "A computational server is programmed to run an iterative simulation. The simulation proceeds in discrete time steps. At each step $i$, the simulation generates a real-valued numerical update $X_i$. These updates, $X_1, X_2, X_3, \\dots$, are independent and identically distributed (i.i.d.) random variables, each with a mean $\\mathbb{E}[X_i] = \\mu$ and a variance $\\operatorname{Var}(X_i) = \\sigma^2$. The mean $\\mu$ is non-zero, and the variance $\\sigma^2$ is finite and positive.\n\nThe simulation incorporates a probabilistic stopping rule. After each step, an independent check is performed to decide whether to terminate. The probability of termination after any given step is a constant $p$, where $p \\in (0, 1)$. The simulation is guaranteed to run for at least one step. Let $T$ be the random variable representing the total number of steps for which the simulation runs.\n\nThe final output of the simulation is the cumulative sum of all updates generated, denoted by $S_T = \\sum_{i=1}^{T} X_i$.\n\nDetermine a closed-form expression for the variance of this final output, $\\operatorname{Var}(S_T)$, in terms of the parameters $\\mu$, $\\sigma^2$, and $p$.", "solution": "Let $\\{X_{i}\\}_{i \\geq 1}$ be i.i.d. with $\\mathbb{E}[X_{i}] = \\mu$ and $\\operatorname{Var}(X_{i}) = \\sigma^{2}$. Let $T$ be independent of the $X_{i}$ and be geometric with support $\\{1,2,\\dots\\}$ and parameter $p \\in (0,1)$, so $\\mathbb{P}(T=t) = p(1-p)^{t-1}$.\n\nWe seek $\\operatorname{Var}(S_{T})$ for $S_{T} = \\sum_{i=1}^{T} X_{i}$. By the law of total variance,\n$$\n\\operatorname{Var}(S_{T}) = \\mathbb{E}\\!\\left[\\operatorname{Var}(S_{T}\\mid T)\\right] + \\operatorname{Var}\\!\\left(\\mathbb{E}[S_{T}\\mid T]\\right).\n$$\nGiven $T=t$, $S_{T}$ is a sum of $t$ i.i.d. variables, so\n$$\n\\mathbb{E}[S_{T}\\mid T=t] = t\\mu,\\qquad \\operatorname{Var}(S_{T}\\mid T=t) = t\\sigma^{2}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(S_{T}) = \\sigma^{2}\\mathbb{E}[T] + \\mu^{2}\\operatorname{Var}(T).\n$$\n\nIt remains to compute $\\mathbb{E}[T]$ and $\\operatorname{Var}(T)$ for $T \\sim \\operatorname{Geom}(p)$ on $\\{1,2,\\dots\\}$. Let $q = 1 - p$. Then\n$$\n\\mathbb{E}[T] = \\sum_{t=1}^{\\infty} t\\, p q^{t-1} = p \\sum_{t=1}^{\\infty} t q^{t-1}.\n$$\nUsing the geometric series $\\sum_{t=0}^{\\infty} q^{t} = \\frac{1}{1-q}$ for $|q|<1$ and differentiating with respect to $q$ gives\n$$\n\\sum_{t=1}^{\\infty} t q^{t-1} = \\frac{1}{(1-q)^{2}},\n$$\nso\n$$\n\\mathbb{E}[T] = p \\cdot \\frac{1}{(1-q)^{2}} = p \\cdot \\frac{1}{p^{2}} = \\frac{1}{p}.\n$$\nNext,\n$$\n\\mathbb{E}[T^{2}] = \\sum_{t=1}^{\\infty} t^{2} p q^{t-1} = p \\sum_{t=1}^{\\infty} t^{2} q^{t-1}.\n$$\nFrom $g(q) = \\sum_{t=1}^{\\infty} t q^{t} = \\frac{q}{(1-q)^{2}}$, differentiating yields\n$$\n\\sum_{t=1}^{\\infty} t^{2} q^{t-1} = g'(q) = \\frac{1+q}{(1-q)^{3}},\n$$\nhence\n$$\n\\mathbb{E}[T^{2}] = p \\cdot \\frac{1+q}{(1-q)^{3}} = p \\cdot \\frac{2 - p}{p^{3}} = \\frac{2 - p}{p^{2}}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(T) = \\mathbb{E}[T^{2}] - (\\mathbb{E}[T])^{2} = \\frac{2 - p}{p^{2}} - \\frac{1}{p^{2}} = \\frac{1 - p}{p^{2}}.\n$$\nSubstituting into $\\operatorname{Var}(S_{T})$ gives\n$$\n\\operatorname{Var}(S_{T}) = \\sigma^{2}\\left(\\frac{1}{p}\\right) + \\mu^{2}\\left(\\frac{1 - p}{p^{2}}\\right) = \\frac{\\sigma^{2}}{p} + \\frac{\\mu^{2}(1 - p)}{p^{2}}.\n$$\nThis is the required closed-form expression.", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{p}+\\frac{\\mu^{2}(1-p)}{p^{2}}}$$", "id": "1401012"}]}