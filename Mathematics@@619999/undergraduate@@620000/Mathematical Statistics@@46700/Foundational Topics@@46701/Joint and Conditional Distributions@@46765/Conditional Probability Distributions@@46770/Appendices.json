{"hands_on_practices": [{"introduction": "We begin with a foundational scenario in probability: sampling from an urn without replacement. This classic problem is an excellent way to build intuition about how information changes probabilities. By conditioning on the outcome of the first draw, we alter the composition of the remaining population, which directly impacts the distribution of the overall sample [@problem_id:1906185]. This exercise demonstrates the transition from a standard hypergeometric setting to a conditional one, a crucial skill for analyzing dependent events.", "problem": "An urn contains a total of $N$ balls, of which $K$ are red and the remaining $N-K$ are black. A random sample of $n$ balls is drawn from the urn without replacement, where $1 \\le n \\le N$ and $1 \\le K  N$. Let $X$ be the random variable representing the total number of red balls in the drawn sample of $n$ balls. Given the event $E$, defined as the first ball drawn being red, determine the conditional probability $P(X=k | E)$. Express your answer as a function of $k, n, N,$ and $K$. Assume $k$ is an integer within the valid support of this conditional distribution.", "solution": "Let the urn contain $K$ red and $N-K$ black balls, and let $n$ balls be drawn without replacement. Define $E$ as the event that the first ball drawn is red. Condition on $E$.\n\nGiven $E$, after the first (red) draw, there remain $N-1$ balls, of which $K-1$ are red and $N-K$ are black. The remaining $n-1$ draws are made without replacement from these $N-1$ balls. Let $Y$ denote the number of red balls among these remaining $n-1$ draws. Then the total number of red balls in the full sample satisfies\n$$\nX=1+Y.\n$$\nTherefore, for any integer $k$ in the valid support,\n$$\nP(X=k \\mid E)=P(Y=k-1).\n$$\nSince $Y$ has a hypergeometric distribution with population size $N-1$, number of successes $K-1$, and draw size $n-1$, its probability mass function is\n$$\nP(Y=y)=\\frac{\\binom{K-1}{y}\\binom{N-K}{(n-1)-y}}{\\binom{N-1}{n-1}}.\n$$\nSubstituting $y=k-1$ gives\n$$\nP(X=k \\mid E)=\\frac{\\binom{K-1}{k-1}\\binom{N-K}{n-k}}{\\binom{N-1}{n-1}},\n$$\nfor integers $k$ satisfying $\\max\\{1,\\,n-(N-K)\\}\\leq k \\leq \\min\\{n,\\,K\\}$.", "answer": "$$\\boxed{\\frac{\\binom{K-1}{k-1}\\binom{N-K}{n-k}}{\\binom{N-1}{n-1}}}$$", "id": "1906185"}, {"introduction": "Moving into the realm of continuous distributions, this practice explores a scenario common in fields like signal processing and communications. We model an observed measurement as the sum of a true signal and random noise, both following normal distributions. The central task is to deduce the properties of the original signal given the total observed value, a fundamental problem in estimation theory [@problem_id:1906118]. This exercise is a hands-on application of the powerful properties of jointly normal random variables, showing how to calculate a conditional distribution's mean and variance.", "problem": "In a simplified model for a communication system, an original signal $X$ is corrupted by additive noise $Y$. Assume that both the signal $X$ and the noise $Y$ can be modeled as independent and identically distributed random variables, each following a standard normal distribution. A standard normal distribution is a normal distribution with a mean of 0 and a variance of 1.\n\nAn observer at a receiver does not see the original signal $X$ or the noise $Y$ separately. Instead, they only observe the total received signal, which is the sum $S = X+Y$. To estimate the original signal, it is crucial to understand the statistical properties of $X$ given a specific observation.\n\nSuppose the receiver measures the total signal to be $S=s$, where $s$ is a real constant. Determine the conditional probability distribution of the original signal $X$ given this measurement. Which of the following correctly describes this conditional distribution?\n\nA. A Normal distribution with mean $s$ and variance $1$.\n\nB. A Normal distribution with mean $\\frac{s}{2}$ and variance $\\frac{1}{2}$.\n\nC. A Normal distribution with mean $\\frac{s}{2}$ and variance $2$.\n\nD. A Uniform distribution on the interval $[s-\\sqrt{3}, s+\\sqrt{3}]$.\n\nE. A Normal distribution with mean $0$ and variance $\\frac{1}{2}$.\n\nF. A Chi-squared distribution with 1 degree of freedom.", "solution": "Let $X$ and $Y$ be independent with $X \\sim \\mathcal{N}(0,1)$ and $Y \\sim \\mathcal{N}(0,1)$. Define $S=X+Y$. Since $(X,Y)$ is jointly normal and $S$ is a linear function of $(X,Y)$, the pair $(X,S)$ is jointly normal.\n\nCompute the first and second moments:\n- $\\mathbb{E}[X]=0$, $\\mathbb{E}[S]=\\mathbb{E}[X+Y]=0$.\n- $\\operatorname{Var}(X)=1$, $\\operatorname{Var}(S)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)=2$ (by independence).\n- $\\operatorname{Cov}(X,S)=\\operatorname{Cov}(X,X+Y)=\\operatorname{Var}(X)+\\operatorname{Cov}(X,Y)=1+0=1$ (by independence).\n\nFor jointly normal variables, the conditional distribution $X \\mid S=s$ is normal with\n$$\n\\mathbb{E}[X \\mid S=s]=\\mathbb{E}[X]+\\frac{\\operatorname{Cov}(X,S)}{\\operatorname{Var}(S)}\\left(s-\\mathbb{E}[S]\\right)=0+\\frac{1}{2}s=\\frac{s}{2},\n$$\nand\n$$\n\\operatorname{Var}(X \\mid S)=\\operatorname{Var}(X)-\\frac{\\operatorname{Cov}(X,S)^{2}}{\\operatorname{Var}(S)}=1-\\frac{1^{2}}{2}=\\frac{1}{2}.\n$$\n\nTherefore, $X \\mid S=s \\sim \\mathcal{N}\\!\\left(\\frac{s}{2},\\frac{1}{2}\\right)$, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1906118"}, {"introduction": "Our final practice tackles a more advanced topic, revealing the intricate structure within a set of ordered random variables. Order statistics are fundamental in non-parametric statistics and reliability analysis. Here, we examine a sample from a uniform distribution and fix the value of the median, $X_{(3)}$ [@problem_id:1906139]. This problem challenges you to derive the joint conditional distribution of the sample minimum and maximum, illustrating how knowledge of one order statistic provides significant information about the others.", "problem": "Consider a random sample of size $n=5$, denoted by $X_1, \\dots, X_5$, drawn from a continuous probability distribution. Let this parent distribution be the standard uniform distribution on the interval $(0, 1)$. The order statistics of the sample are denoted by $X_{(1)} \\le X_{(2)} \\le X_{(3)} \\le X_{(4)} \\le X_{(5)}$.\n\nSuppose the value of the sample median is observed to be $X_{(3)} = m$, where $m$ is a constant such that $0  m  1$.\n\nYour task is to determine the conditional joint Probability Density Function (PDF) of the sample minimum, $X_{(1)}$, and the sample maximum, $X_{(5)}$, given that $X_{(3)}=m$. A PDF, $f(x)$, is a function that describes the relative likelihood for a continuous random variable to take on a given value. The joint PDF for multiple variables describes their relative likelihoods simultaneously.\n\nExpress your answer as a function of the variables $x_1$ and $x_5$, which represent possible values for $X_{(1)}$ and $X_{(5)}$ respectively. The resulting expression for the function $f_{X_{(1)}, X_{(5)}|X_{(3)}}(x_1, x_5|m)$ is valid over the region where the density is non-zero, namely for $0  x_1  m  x_5  1$.", "solution": "We have an i.i.d. sample of size $n=5$ from the $\\mathrm{Uniform}(0,1)$ distribution. Let $X_{(1)} \\le X_{(2)} \\le X_{(3)} \\le X_{(4)} \\le X_{(5)}$ denote the order statistics, and suppose $X_{(3)}=m$ with $0m1$. We seek the conditional joint PDF $f_{X_{(1)},X_{(5)} \\mid X_{(3)}}(x_{1},x_{5} \\mid m)$ for $0x_{1}mx_{5}1$.\n\nThe general joint PDF of three order statistics $X_{(i)},X_{(j)},X_{(k)}$ for $ijk$ from a continuous distribution with CDF $F$ and PDF $f$ is\n$$\nf_{X_{(i)},X_{(j)},X_{(k)}}(x,y,z)\n=\\frac{n!}{(i-1)!\\,(j-i-1)!\\,(k-j-1)!\\,(n-k)!}\\,[F(x)]^{i-1}\\,[F(y)-F(x)]^{j-i-1}\\,[F(z)-F(y)]^{k-j-1}\\,[1-F(z)]^{n-k}\\,f(x)f(y)f(z),\n$$\nvalid for $xyz$.\n\nFor the $\\mathrm{Uniform}(0,1)$ distribution, $F(t)=t$ and $f(t)=1$ for $t \\in (0,1)$. With $n=5$, $(i,j,k)=(1,3,5)$, we obtain\n$$\nf_{X_{(1)},X_{(3)},X_{(5)}}(x_{1},m,x_{5})\n=\\frac{5!}{0!\\,1!\\,1!\\,0!}\\,[x_{1}]^{0}\\,[m-x_{1}]^{1}\\,[x_{5}-m]^{1}\\,[1-x_{5}]^{0}\\cdot 1\\cdot 1\\cdot 1\n=120\\,(m-x_{1})\\,(x_{5}-m),\n$$\nvalid for $0x_{1}mx_{5}1$.\n\nThe marginal PDF of $X_{(3)}$ for $n=5$ and $\\mathrm{Uniform}(0,1)$ is the Beta density\n$$\nf_{X_{(3)}}(m)=\\frac{5!}{(3-1)!\\,(5-3)!}\\,m^{3-1}\\,(1-m)^{5-3}\n=\\frac{120}{2!\\,2!}\\,m^{2}\\,(1-m)^{2}\n=30\\,m^{2}\\,(1-m)^{2},\n$$\nfor $0m1$.\n\nBy the definition of conditional density,\n$$\nf_{X_{(1)},X_{(5)}\\mid X_{(3)}}(x_{1},x_{5}\\mid m)\n=\\frac{f_{X_{(1)},X_{(3)},X_{(5)}}(x_{1},m,x_{5})}{f_{X_{(3)}}(m)}\n=\\frac{120\\,(m-x_{1})\\,(x_{5}-m)}{30\\,m^{2}\\,(1-m)^{2}}\n=\\frac{4\\,(m-x_{1})\\,(x_{5}-m)}{m^{2}\\,(1-m)^{2}},\n$$\nvalid for $0x_{1}mx_{5}1$. A normalization check confirms\n$$\n\\int_{m}^{1}\\int_{0}^{m}\\frac{4\\,(m-x_{1})\\,(x_{5}-m)}{m^{2}\\,(1-m)^{2}}\\,\\mathrm{d}x_{1}\\,\\mathrm{d}x_{5}\n=\\frac{4}{m^{2}(1-m)^{2}}\\left(\\int_{0}^{m}(m-x_{1})\\,\\mathrm{d}x_{1}\\right)\\left(\\int_{m}^{1}(x_{5}-m)\\,\\mathrm{d}x_{5}\\right)\n=\\frac{4}{m^{2}(1-m)^{2}}\\cdot\\frac{m^{2}}{2}\\cdot\\frac{(1-m)^{2}}{2}=1.\n$$\nTherefore, the conditional joint PDF is as stated.", "answer": "$$\\boxed{\\frac{4\\,(m-x_{1})\\,(x_{5}-m)}{m^{2}\\,(1-m)^{2}}}$$", "id": "1906139"}]}