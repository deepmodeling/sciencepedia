{"hands_on_practices": [{"introduction": "Working with joint probability density functions (PDFs) begins with two fundamental steps: ensuring the function is a valid PDF and then using it to compute probabilities. This first exercise [@problem_id:1369415] guides you through this essential process. You will first determine the normalization constant $c$ that makes the total probability equal to one, and then calculate the probability of an event defined by an inequality, which involves integrating the PDF over a specific sub-region of its domain.", "problem": "A particle's position in a two-dimensional plane is described by the random variables $(X, Y)$. The joint probability density function for these variables is given by $f(x,y) = c(x+2y)$ for $0 \\le x \\le 1$ and $0 \\le y \\le 1$, and $f(x,y) = 0$ otherwise. Here, $c$ is a normalization constant that must be determined.\n\nCalculate the probability that the particle is located in the region where its y-coordinate is less than or equal to its x-coordinate.", "solution": "We are given the joint probability density function $f(x,y) = c(x+2y)$ for $0 \\le x \\le 1$ and $0 \\le y \\le 1$, and $f(x,y)=0$ otherwise. The normalization condition for a joint probability density function on its support is\n$$\n\\int_{0}^{1}\\int_{0}^{1} f(x,y)\\,dy\\,dx = 1.\n$$\nSubstituting $f(x,y)=c(x+2y)$, we obtain\n$$\n\\int_{0}^{1}\\int_{0}^{1} c(x+2y)\\,dy\\,dx = c \\int_{0}^{1}\\int_{0}^{1} (x+2y)\\,dy\\,dx = 1.\n$$\nEvaluate the inner integral:\n$$\n\\int_{0}^{1} (x+2y)\\,dy = \\left[xy + y^{2}\\right]_{0}^{1} = x + 1.\n$$\nThen the outer integral is\n$$\n\\int_{0}^{1} (x+1)\\,dx = \\left[\\frac{x^{2}}{2} + x\\right]_{0}^{1} = \\frac{1}{2} + 1 = \\frac{3}{2}.\n$$\nThus,\n$$\nc \\cdot \\frac{3}{2} = 1 \\quad \\Rightarrow \\quad c = \\frac{2}{3}.\n$$\n\nWe seek the probability that $Y \\le X$ on the unit square, which is the triangular region $\\{(x,y): 0 \\le y \\le x \\le 1\\}$. The probability is\n$$\nP(Y \\le X) = \\int_{0}^{1} \\int_{0}^{x} c(x+2y)\\,dy\\,dx.\n$$\nFirst compute the inner integral:\n$$\n\\int_{0}^{x} (x+2y)\\,dy = \\left[xy + y^{2}\\right]_{0}^{x} = x^{2} + x^{2} = 2x^{2}.\n$$\nTherefore,\n$$\nP(Y \\le X) = c \\int_{0}^{1} 2x^{2}\\,dx = c \\cdot 2 \\left[\\frac{x^{3}}{3}\\right]_{0}^{1} = c \\cdot \\frac{2}{3}.\n$$\nSubstituting $c = \\frac{2}{3}$ yields\n$$\nP(Y \\le X) = \\frac{2}{3} \\cdot \\frac{2}{3} = \\frac{4}{9}.\n$$", "answer": "$$\\boxed{\\frac{4}{9}}$$", "id": "1369415"}, {"introduction": "While a joint PDF describes the behavior of two variables together, we often need to understand one variable's behavior given that we know the value of the other. This practice [@problem_id:1926391] introduces the critical concept of conditional distributions. You will derive the conditional PDF for a variable $X$ given a specific value for $Y=y_0$, and then use it to calculate the conditional variance, $\\operatorname{Var}(X \\mid Y=y_0)$, providing a measure of the remaining uncertainty in $X$.", "problem": "Let the random variables $X$ and $Y$ have a joint probability density function (PDF) that is uniform over the disk defined by the inequality $x^2 + y^2 \\le R^2$. Your task is to compute the variance of the conditional distribution of $X$ given that $Y=y_0$, where $|y_0|  R$. For your calculation, use the specific values $R=3$ and $y_0=1$. Express your answer as a single closed-form expression.", "solution": "Let the joint density be $f_{X,Y}(x,y) = \\frac{1}{\\pi R^{2}}$ for $x^{2} + y^{2} \\leq R^{2}$ and $0$ otherwise. For a fixed $y_{0}$ with $|y_{0}|  R$, the admissible $x$-values satisfy $x^{2} \\leq R^{2} - y_{0}^{2}$, i.e., $x \\in \\left[-\\sqrt{R^{2} - y_{0}^{2}}, \\sqrt{R^{2} - y_{0}^{2}}\\right]$.\n\nThe marginal density of $Y$ at $y_{0}$ is\n$$\nf_{Y}(y_{0}) = \\int_{-\\sqrt{R^{2} - y_{0}^{2}}}^{\\sqrt{R^{2} - y_{0}^{2}}} \\frac{1}{\\pi R^{2}} \\, dx = \\frac{2\\sqrt{R^{2} - y_{0}^{2}}}{\\pi R^{2}}.\n$$\nHence, the conditional density of $X$ given $Y=y_{0}$ is\n$$\nf_{X \\mid Y}(x \\mid y_{0}) = \\frac{f_{X,Y}(x,y_{0})}{f_{Y}(y_{0})} = \\frac{\\frac{1}{\\pi R^{2}}}{\\frac{2\\sqrt{R^{2} - y_{0}^{2}}}{\\pi R^{2}}} = \\frac{1}{2\\sqrt{R^{2} - y_{0}^{2}}}\n$$\nfor $x \\in \\left[-\\sqrt{R^{2} - y_{0}^{2}}, \\sqrt{R^{2} - y_{0}^{2}}\\right]$, and $0$ otherwise. Thus $X \\mid (Y=y_{0})$ is uniform on $[-a,a]$ with $a = \\sqrt{R^{2} - y_{0}^{2}}$.\n\nFor a uniform distribution on $[-a,a]$, we have\n$$\n\\mathbb{E}[X \\mid Y=y_{0}] = \\int_{-a}^{a} x \\cdot \\frac{1}{2a} \\, dx = 0,\n$$\nand\n$$\n\\mathbb{E}[X^{2} \\mid Y=y_{0}] = \\int_{-a}^{a} x^{2} \\cdot \\frac{1}{2a} \\, dx = \\frac{1}{2a} \\cdot \\left[ \\frac{x^{3}}{3} \\right]_{-a}^{a} = \\frac{1}{2a} \\cdot \\frac{2a^{3}}{3} = \\frac{a^{2}}{3}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(X \\mid Y=y_{0}) = \\mathbb{E}[X^{2} \\mid Y=y_{0}] - \\left(\\mathbb{E}[X \\mid Y=y_{0}]\\right)^{2} = \\frac{a^{2}}{3} = \\frac{R^{2} - y_{0}^{2}}{3}.\n$$\nSubstituting $R=3$ and $y_{0}=1$ gives\n$$\n\\operatorname{Var}(X \\mid Y=1) = \\frac{3^{2} - 1^{2}}{3} = \\frac{9 - 1}{3} = \\frac{8}{3}.\n$$", "answer": "$$\\boxed{\\frac{8}{3}}$$", "id": "1926391"}, {"introduction": "Often in science and engineering, a quantity of interest is the sum of two or more independent random variables. This final practice [@problem_id:9618] explores how to find the probability distribution for such a sum, $Z = X+Y$. You will apply the convolution formula, a powerful mathematical tool that combines the two individual PDFs to derive the PDF for their sum, revealing how the resulting distribution's shape emerges from the interaction of the original variables.", "problem": "Let $X$ and $Y$ be two independent random variables. $X$ is uniformly distributed on the interval $[0, a]$ and $Y$ is uniformly distributed on the interval $[0, b]$, where $a$ and $b$ are positive constants with the additional constraint that $a > b$.\n\nThe probability density function (PDF) for a random variable $U$ uniformly distributed on an interval $[\\alpha, \\beta]$ is given by:\n$$\nf_U(u) = \\begin{cases}\n\\frac{1}{\\beta - \\alpha}  \\text{if } \\alpha \\le u \\le \\beta \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nGiven that $X$ and $Y$ are independent, their joint PDF is the product of their individual PDFs, i.e., $f_{X,Y}(x,y) = f_X(x) f_Y(y)$.\n\nConsider a new random variable $Z$ defined as the sum of $X$ and $Y$:\n$$\nZ = X + Y\n$$\nDerive the probability density function for the random variable $Z$, denoted as $f_Z(z)$.", "solution": "The joint PDF is \n$$f_{X,Y}(x,y)=\\frac{1}{ab},\\quad 0\\le x\\le a,\\;0\\le y\\le b.$$\nSince $Z=X+Y$, by convolution\n$$\nf_Z(z)=\\int_{-\\infty}^{\\infty}f_X(x)f_Y(z-x)dx\n=\\frac{1}{ab}\\int_{0}^{a}\\mathbf{1}_{\\{0\\le z-x\\le b\\}}dx\n=\\frac{1}{ab}\\left(\\min(a,z)-\\max(0,z-b)\\right).\n$$\nWe split into cases (using $ab0$):\n\nFor $0\\le z\\le b$, $\\min(a,z)=z$, $\\max(0,z-b)=0$ so\n$$f_Z(z)=\\frac{z}{ab}.$$\n\nFor $b\\le z\\le a$, $\\min(a,z)=z$, $\\max(0,z-b)=z-b$ so\n$$f_Z(z)=\\frac{z-(z-b)}{ab}=\\frac{b}{ab}=\\frac{1}{a}.$$\n\nFor $a\\le z\\le a+b$, $\\min(a,z)=a$, $\\max(0,z-b)=z-b$ so\n$$f_Z(z)=\\frac{a-(z-b)}{ab}=\\frac{a+b-z}{ab}.$$\n\nElsewhere $f_Z(z)=0$.", "answer": "$$\\boxed{\\begin{cases}\n\\frac{z}{ab},  0\\le z\\le b\\\\\n\\frac{1}{a},    b\\le z\\le a\\\\\n\\frac{a+b-z}{ab},  a\\le z\\le a+b\\\\\n0,\\text{otherwise.}\n\\end{cases}}$$", "id": "9618"}]}