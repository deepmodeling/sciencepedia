{"hands_on_practices": [{"introduction": "Understanding how to handle combinations of random variables is a cornerstone of statistical analysis. This exercise focuses on a linear combination of two correlated variables, $Z = aX + bY$. By deriving the variance of $Z$, you will practice applying the fundamental properties of variance and covariance, a crucial skill for applications ranging from financial portfolio theory to signal processing. This problem [@problem_id:1488] lays the groundwork for understanding how individual variabilities and their interplay contribute to the total variance of a system.", "problem": "Let the random variables $X$ and $Y$ follow a bivariate normal distribution. The joint probability density function (PDF) is given by $f(x, y)$, but for this problem, you only need to know the parameters that define this distribution:\n- The mean of $X$ is $E[X] = \\mu_X$.\n- The mean of $Y$ is $E[Y] = \\mu_Y$.\n- The variance of $X$ is $\\operatorname{Var}(X) = \\sigma_X^2$.\n- The variance of $Y$ is $\\operatorname{Var}(Y) = \\sigma_Y^2$.\n- The correlation coefficient between $X$ and $Y$ is $\\rho$.\n\nRecall that the covariance between $X$ and $Y$ is defined as $\\operatorname{Cov}(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)]$, and it is related to the correlation coefficient by $\\operatorname{Cov}(X, Y) = \\rho \\sigma_X \\sigma_Y$.\n\nConsider a new random variable $Z$ which is a linear combination of $X$ and $Y$, defined as:\n$$\nZ = aX + bY\n$$\nwhere $a$ and $b$ are arbitrary real constants.\n\nUsing the fundamental properties of variance and covariance, derive an expression for the variance of $Z$, denoted as $\\operatorname{Var}(Z)$, in terms of the constants $a, b$ and the parameters of the distribution $\\sigma_X, \\sigma_Y, \\rho$.", "solution": "We seek $\\operatorname{Var}(Z)$ for $Z=aX+bY$.  Using the bilinearity of variance and covariance, we have  \n$$\n\\operatorname{Var}(Z)=\\operatorname{Var}(aX+bY)\n=\\operatorname{Var}(aX)+\\operatorname{Var}(bY)+2\\,\\operatorname{Cov}(aX,bY).\n$$\nBy properties of variance and covariance,  \n$$\n\\operatorname{Var}(aX)=a^2\\operatorname{Var}(X)=a^2\\sigma_X^2,\\quad\n\\operatorname{Var}(bY)=b^2\\operatorname{Var}(Y)=b^2\\sigma_Y^2,\n$$\n$$\n\\operatorname{Cov}(aX,bY)=ab\\,\\operatorname{Cov}(X,Y)=ab\\,\\rho\\,\\sigma_X\\sigma_Y.\n$$\nCombining these terms gives  \n$$\n\\operatorname{Var}(Z)=a^2\\sigma_X^2+b^2\\sigma_Y^2+2ab\\,\\rho\\,\\sigma_X\\sigma_Y.\n$$", "answer": "$$\\boxed{a^2\\sigma_X^2 + b^2\\sigma_Y^2 + 2ab\\,\\rho\\,\\sigma_X\\sigma_Y}$$", "id": "1488"}, {"introduction": "A key feature of the bivariate normal distribution is the nature of the relationship between its constituent variables. This practice explores this relationship through conditional expectation, $E[Y \\mid X=x]$. You will discover that the expected value of one variable, given the value of the other, follows a simple linear function. Deriving this result [@problem_id:1521] provides deep insight into why the bivariate normal distribution is the theoretical foundation for linear regression, a widely used predictive modeling technique.", "problem": "Two random variables, $X$ and $Y$, are said to follow a bivariate normal distribution if their joint probability density function (PDF), $f_{X,Y}(x,y)$, is given by:\n$$\nf_{X,Y}(x,y) = \\frac{1}{2\\pi\\sigma_X\\sigma_Y\\sqrt{1-\\rho^2}} \\exp\\left( -\\frac{1}{2(1-\\rho^2)} \\left[ \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)^2 - 2\\rho\\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right) + \\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \\right] \\right)\n$$\nHere, $\\mu_X$ and $\\mu_Y$ are the means of $X$ and $Y$, respectively. $\\sigma_X$ and $\\sigma_Y$ are their standard deviations, and $\\rho$ is the correlation coefficient between them, with $|\\rho| < 1$.\n\nThe conditional expectation of $Y$ given that $X$ has taken on a specific value $x$, denoted as $E[Y \\mid X=x]$, is defined as the mean of the conditional probability distribution $f_{Y|X}(y|x)$. This is calculated by the integral:\n$$\nE[Y \\mid X=x] = \\int_{-\\infty}^{\\infty} y \\cdot f_{Y|X}(y|x) \\, dy\n$$\nwhere the conditional PDF is given by $f_{Y|X}(y \\mid x) = \\frac{f_{X,Y}(x,y)}{f_X(x)}$. The marginal PDF for $X$, $f_X(x)$, is known to be a normal distribution $N(\\mu_X, \\sigma_X^2)$.\n\nYour task is to derive the expression for the conditional expectation $E[Y \\mid X=x]$. You can achieve this by recognizing that the conditional distribution $f_{Y|X}(y \\mid x)$ is itself a normal distribution. By algebraically manipulating the exponent of the joint PDF $f_{X,Y}(x,y)$, you can identify the form of this conditional normal distribution and thereby find its mean without performing the full integration.", "solution": "By definition, the conditional PDF is\n$$\nf_{Y|X}(y \\mid x)=\\frac{f_{X,Y}(x,y)}{f_X(x)}\n\\propto\\exp\\Bigl(-\\frac{1}{2(1-\\rho^2)}\\Bigl[\\bigl(\\tfrac{x-\\mu_X}{\\sigma_X}\\bigr)^2\n-2\\rho\\bigl(\\tfrac{x-\\mu_X}{\\sigma_X}\\bigr)\\bigl(\\tfrac{y-\\mu_Y}{\\sigma_Y}\\bigr)\n+\\bigl(\\tfrac{y-\\mu_Y}{\\sigma_Y}\\bigr)^2\\Bigr]\\Bigr).\n$$\n\nFocus on the terms involving $y$.  Set\n$$\nA=\\frac{y-\\mu_Y}{\\sigma_Y},\\qquad B=\\frac{x-\\mu_X}{\\sigma_X},\n$$\nso that the exponent in $y$ is proportional to\n$$\nA^2-2\\rho\\,A B.\n$$\n\nComplete the square:\n$$\nA^2-2\\rho\\,A B=(A-\\rho B)^2-\\rho^2B^2.\n$$\nHence\n$$\nf_{Y|X}(y \\mid x)\\propto\\exp\\Bigl(-\\frac{1}{2(1-\\rho^2)}(A-\\rho B)^2\\Bigr),\n$$\nwhich is the kernel of a normal distribution with mean determined by\n$$\nA-\\rho B=0\\quad\\Longrightarrow\\quad\n\\frac{y-\\mu_Y}{\\sigma_Y}=\\rho\\frac{x-\\mu_X}{\\sigma_X}\n\\quad\\Longrightarrow\\quad\ny=\\mu_Y+\\rho\\frac{\\sigma_Y}{\\sigma_X}(x-\\mu_X).\n$$\n\nTherefore,\n$$\nE[Y\\mid X=x]=\\mu_Y+\\rho\\frac{\\sigma_Y}{\\sigma_X}(x-\\mu_X).\n$$", "answer": "$$\\boxed{\\mu_Y+\\rho\\frac{\\sigma_Y}{\\sigma_X}(x-\\mu_X)}$$", "id": "1521"}, {"introduction": "A common point of confusion in probability theory is the distinction between uncorrelated and independent variables. While zero correlation does not generally imply independence, the bivariate normal distribution presents a powerful exception. This exercise [@problem_id:1901219] invites you to explore this special property through a linear transformation of variables. By determining the condition under which the new variables $U=X+Y$ and $V=X-Y$ become independent, you will gain a more profound understanding of the unique structure of joint normality.", "problem": "Let the random pair $(X, Y)$ follow a bivariate normal distribution with means $\\mu_X$ and $\\mu_Y$, positive variances $\\sigma_X^2$ and $\\sigma_Y^2$, and correlation coefficient $\\rho$. We define two new random variables as linear combinations of $X$ and $Y$:\n$$U = X + Y$$\n$$V = X - Y$$\nIt is a known property that since $(X, Y)$ is a bivariate normal pair, the transformed pair $(U, V)$ also follows a bivariate normal distribution. For variables that are jointly normally distributed, the condition of zero correlation is both necessary and sufficient for statistical independence.\n\nBased on this information, which of the following statements correctly specifies the condition under which the random variables $U$ and $V$ are independent?\n\nA. $U$ and $V$ are independent if and only if $X$ and $Y$ are uncorrelated ($\\rho = 0$).\n\nB. $U$ and $V$ are independent if and only if the means are equal ($\\mu_X = \\mu_Y$).\n\nC. $U$ and $V$ are independent if and only if the variances are equal ($\\sigma_X^2 = \\sigma_Y^2$).\n\nD. $U$ and $V$ are always independent, regardless of the parameters of the distribution of $(X, Y)$.\n\nE. $U$ and $V$ can never be independent unless both $X$ and $Y$ are constant random variables.", "solution": "Because $(X,Y)$ is bivariate normal, any linear transformation is also jointly normal. Therefore $(U,V)$ is jointly normal. For jointly normal variables, zero covariance is equivalent to independence.\n\nCompute the covariance:\n$$\n\\operatorname{Cov}(U,V)=\\operatorname{Cov}(X+Y, X-Y).\n$$\nUsing bilinearity and symmetry of covariance,\n$$\n\\operatorname{Cov}(X+Y, X-Y)=\\operatorname{Cov}(X,X)-\\operatorname{Cov}(X,Y)+\\operatorname{Cov}(Y,X)-\\operatorname{Cov}(Y,Y).\n$$\nSince $\\operatorname{Cov}(X,X)=\\sigma_X^2$, $\\operatorname{Cov}(Y,Y)=\\sigma_Y^2$, and $\\operatorname{Cov}(X,Y)=\\operatorname{Cov}(Y,X)=\\rho\\,\\sigma_X\\sigma_Y$, we get\n$$\n\\operatorname{Cov}(U,V)=\\sigma_X^2-\\sigma_Y^2.\n$$\nThis covariance does not depend on $\\rho$ or the means. Hence, $\\operatorname{Cov}(U,V)=0$ if and only if $\\sigma_X^2=\\sigma_Y^2$. Since $(U,V)$ is jointly normal, zero covariance implies independence. Therefore, $U$ and $V$ are independent if and only if $\\sigma_X^2=\\sigma_Y^2$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1901219"}]}