{"hands_on_practices": [{"introduction": "Understanding how to interpret a joint probability distribution is the first step toward mastering multivariable statistics. This exercise provides a hands-on look at a discrete joint probability mass function presented in a simple table, a common format for empirical data [@problem_id:1635049]. By calculating the probability of a specific event—an email containing exactly one of two keywords—you will practice directly applying the foundational rules of probability to a joint distribution.", "problem": "A cybersecurity analyst is developing a simple spam filter. The filter models the content of an incoming email by tracking the presence of two specific keywords, \"special\" and \"offer\". Let $X$ be a binary random variable such that $X=1$ if the email contains the word \"special\" and $X=0$ otherwise. Similarly, let $Y$ be a binary random variable such that $Y=1$ if the email contains the word \"offer\" and $Y=0$ otherwise.\n\nBased on a large dataset of emails, the analyst has determined the joint probability distribution for these two variables, $P(X=x, Y=y)$, which is given in the table below:\n\n| | $X=0$ | $X=1$ |\n| :--- | :---: | :---: |\n| **$Y=0$** | 0.82 | 0.09 |\n| **$Y=1$** | 0.05 | 0.04 |\n\nCalculate the probability that a randomly selected email contains exactly one of these two keywords. Express your answer as a decimal.", "solution": "We are asked for the probability that exactly one of the two keywords appears. Define the event of interest as the exclusive-or event\n$$\nE=\\{(X=1,Y=0)\\}\\cup\\{(X=0,Y=1)\\}.\n$$\nThe two subevents are mutually exclusive, so by additivity of probability for disjoint events,\n$$\nP(E)=P(X=1,Y=0)+P(X=0,Y=1).\n$$\nFrom the given joint distribution table,\n$$\nP(X=1,Y=0)=0.09,\\quad P(X=0,Y=1)=0.05.\n$$\nTherefore,\n$$\nP(E)=0.09+0.05=0.14.\n$$\nThus, the probability that a randomly selected email contains exactly one of the two keywords is $0.14$.", "answer": "$$\\boxed{0.14}$$", "id": "1635049"}, {"introduction": "While discrete distributions handle countable outcomes, many real-world phenomena involve continuous variables. This practice shifts our focus to joint probability density functions (PDFs), which describe the likelihoods for continuous random variables [@problem_id:1926675]. You will calculate the probability that the composition of a chemical mixture falls within a certain 'standard grade' by integrating the joint PDF over the specified region, a fundamental technique for continuous systems.", "problem": "A chemical engineer is studying the composition of a new solvent mixture. The mixture consists of two active components, labeled A and B, dissolved in a base solvent. Let $X$ and $Y$ be the random variables representing the mass fractions of component A and component B, respectively, in a randomly selected sample. Due to the synthesis process, the joint probability density function (PDF) for the pair $(X, Y)$ is given by\n$$f(x,y) = \\begin{cases} 24xy & \\text{if } x > 0, y > 0, \\text{ and } x+y < 1 \\\\ 0 & \\text{otherwise} \\end{cases}$$\nA sample of the mixture is considered to be of \"standard grade\" if the mass fraction of component A is less than one-half, and the mass fraction of component B is also less than one-half.\n\nCalculate the probability that a randomly selected sample is of standard grade. Express your final answer as an exact fraction.", "solution": "We are given the joint PDF\n$$\nf(x,y)=\\begin{cases}\n24xy & \\text{if } x>0,\\ y>0,\\ x+y<1,\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\nA sample is of standard grade if $X<\\frac{1}{2}$ and $Y<\\frac{1}{2}$. The probability of standard grade is the integral of the joint PDF over the event $\\{0<x<\\frac{1}{2},\\ 0<y<\\frac{1}{2}\\}$ intersected with the support $\\{x+y<1\\}$. On the square $[0,\\frac{1}{2}]\\times[0,\\frac{1}{2}]$, we have $x+y\\leq 1$, and the set where $x+y=1$ is a boundary set of measure zero. Therefore,\n$$\n\\mathbb{P}\\left(X<\\frac{1}{2},\\,Y<\\frac{1}{2}\\right)\n=\\int_{0}^{1/2}\\int_{0}^{1/2} 24xy\\,dy\\,dx.\n$$\nCompute the inner integral:\n$$\n\\int_{0}^{1/2} 24xy\\,dy\n=24x\\int_{0}^{1/2} y\\,dy\n=24x\\left[\\frac{y^{2}}{2}\\right]_{0}^{1/2}\n=24x\\cdot\\frac{1}{8}\n=3x.\n$$\nNow integrate with respect to $x$:\n$$\n\\int_{0}^{1/2} 3x\\,dx\n=3\\left[\\frac{x^{2}}{2}\\right]_{0}^{1/2}\n=3\\cdot\\frac{1}{8}\n=\\frac{3}{8}.\n$$\nThus, the probability that a randomly selected sample is of standard grade is $\\frac{3}{8}$.", "answer": "$$\\boxed{\\frac{3}{8}}$$", "id": "1926675"}, {"introduction": "Beyond calculating simple probabilities, a deeper understanding comes from analyzing the structure of a joint distribution itself. This problem challenges you to investigate the relationship between two random variables by examining their joint probability mass function (PMF) [@problem_id:1926671]. You will discover how the form of the PMF reveals whether the variables are statistically independent and then use this property to determine the variance of their sum, a concept with wide applications in modeling aggregate phenomena.", "problem": "Let $X$ and $Y$ be two discrete random variables whose sample spaces are the set of non-negative integers, i.e., $\\{0, 1, 2, ...\\}$. Their joint probability mass function (PMF) is given by\n$$P(X=i, Y=j) = c \\frac{\\lambda^i \\mu^j}{i! j!}$$\nfor $i=0, 1, 2, ...$ and $j=0, 1, 2, ...$, where $\\lambda$ and $\\mu$ are positive real constants, and $c$ is a normalization constant. Determine the variance of the random variable $Z = X+Y$. Express your answer as a closed-form analytic expression in terms of $\\lambda$ and $\\mu$.", "solution": "We first determine the normalization constant $c$ by imposing that the joint PMF sums to $1$:\n$$\n\\sum_{i=0}^{\\infty}\\sum_{j=0}^{\\infty} P(X=i,Y=j)\n= c \\sum_{i=0}^{\\infty}\\frac{\\lambda^{i}}{i!}\\sum_{j=0}^{\\infty}\\frac{\\mu^{j}}{j!}\n= c\\,\\exp(\\lambda)\\exp(\\mu)\n= c\\,\\exp(\\lambda+\\mu)\n= 1,\n$$\nwhich gives\n$$\nc=\\exp\\!\\big(-( \\lambda+\\mu )\\big).\n$$\nThus the joint PMF factors as\n$$\nP(X=i,Y=j)=\\exp(-\\lambda)\\frac{\\lambda^{i}}{i!}\\cdot \\exp(-\\mu)\\frac{\\mu^{j}}{j!},\n$$\nso $X$ and $Y$ are independent with $X\\sim \\text{Poisson}(\\lambda)$ and $Y\\sim \\text{Poisson}(\\mu)$.\n\nSince $X$ and $Y$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}(Z) = \\operatorname{Var}(X+Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y).\n$$\nFor a Poisson distribution with parameter $\\theta$, the variance is equal to $\\theta$. Therefore,\n$$\n\\operatorname{Var}(X) = \\lambda \\quad \\text{and} \\quad \\operatorname{Var}(Y) = \\mu.\n$$\nSumming these gives the final result:\n$$\n\\operatorname{Var}(Z) = \\lambda + \\mu.\n$$\nAlternatively, one could find the moment generating function (MGF) of $Z=X+Y$. Since $X$ and $Y$ are independent, $M_Z(t)=M_X(t)M_Y(t)$. The MGF for a Poisson($\\theta$) random variable is $M(t)=\\exp(\\theta(e^t-1))$. Therefore,\n$$\nM_Z(t) = \\exp(\\lambda(e^t-1)) \\exp(\\mu(e^t-1)) = \\exp((\\lambda+\\mu)(e^t-1)).\n$$\nThis is the MGF of a Poisson distribution with parameter $\\lambda+\\mu$. The variance of this distribution is $\\lambda+\\mu$.", "answer": "$$\\boxed{\\lambda+\\mu}$$", "id": "1926671"}]}