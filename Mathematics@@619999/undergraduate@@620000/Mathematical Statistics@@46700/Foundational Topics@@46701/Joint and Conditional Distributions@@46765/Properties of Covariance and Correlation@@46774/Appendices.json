{"hands_on_practices": [{"introduction": "This first practice is a foundational exercise designed to build your computational fluency with covariance. Starting from a basic joint probability mass function, you will calculate the essential statistical moments and then apply the bilinearity property of covariance to find the covariance between two linear combinations of random variables [@problem_id:1947661]. This drill is crucial for mastering the mechanics that underpin many statistical models and analyses.", "problem": "Let $X$ and $Y$ be two discrete random variables. The pair $(X, Y)$ can take values $(x, y)$ where $x \\in \\{0, 1\\}$ and $y \\in \\{0, 1\\}$. The joint probability mass function, $p(x, y) = P(X=x, Y=y)$, is given by the following:\n$p(0, 0) = \\frac{3}{20}$\n$p(0, 1) = \\frac{4}{20}$\n$p(1, 0) = \\frac{5}{20}$\n$p(1, 1) = \\frac{8}{20}$\n\nTwo new random variables, $U$ and $V$, are defined as linear combinations of $X$ and $Y$:\n$U = 2X + Y$\n$V = X - 3Y$\n\nCalculate the covariance between $U$ and $V$, denoted as $\\text{Cov}(U, V)$. Express your answer as an exact fraction in its simplest form.", "solution": "The problem asks for the covariance of two new random variables, $U = 2X+Y$ and $V = X-3Y$. We can express $\\text{Cov}(U, V)$ in terms of the variances and covariance of $X$ and $Y$ by using the bilinearity property of covariance.\n\nThe general property is $\\text{Cov}(aX+bY, cX+dY) = ac\\,\\text{Var}(X) + bd\\,\\text{Var}(Y) + (ad+bc)\\,\\text{Cov}(X,Y)$.\nFor our specific variables, $U = 2X+Y$ and $V=X-3Y$, we have $a=2, b=1, c=1, d=-3$.\nSo, $\\text{Cov}(U, V) = \\text{Cov}(2X+Y, X-3Y)$\n$= \\text{Cov}(2X, X) + \\text{Cov}(2X, -3Y) + \\text{Cov}(Y, X) + \\text{Cov}(Y, -3Y)$\nUsing the properties $\\text{Cov}(aZ_1, bZ_2) = ab\\,\\text{Cov}(Z_1, Z_2)$, $\\text{Cov}(Z,Z) = \\text{Var}(Z)$, and $\\text{Cov}(X,Y) = \\text{Cov}(Y,X)$, we get:\n$\\text{Cov}(U, V) = 2\\,\\text{Var}(X) - 5\\,\\text{Cov}(X,Y) - 3\\,\\text{Var}(Y)$\n\nTo find the value of this expression, we must compute $\\text{Var}(X)$, $\\text{Var}(Y)$, and $\\text{Cov}(X,Y)$ from the given joint probability mass function.\n\nFirst, we determine the marginal probability mass functions for $X$ and $Y$.\nThe marginal PMF of $X$, $p_X(x) = P(X=x)$, is found by summing over the values of $y$:\n$p_X(0) = p(0,0) + p(0,1) = \\frac{3}{20} + \\frac{4}{20} = \\frac{7}{20}$\n$p_X(1) = p(1,0) + p(1,1) = \\frac{5}{20} + \\frac{8}{20} = \\frac{13}{20}$\n\nThe marginal PMF of $Y$, $p_Y(y) = P(Y=y)$, is found by summing over the values of $x$:\n$p_Y(0) = p(0,0) + p(1,0) = \\frac{3}{20} + \\frac{5}{20} = \\frac{8}{20} = \\frac{2}{5}$\n$p_Y(1) = p(0,1) + p(1,1) = \\frac{4}{20} + \\frac{8}{20} = \\frac{12}{20} = \\frac{3}{5}$\n\nNext, we calculate the expected values $E[X]$ and $E[Y]$.\n$E[X] = \\sum_x x \\cdot p_X(x) = (0)\\left(\\frac{7}{20}\\right) + (1)\\left(\\frac{13}{20}\\right) = \\frac{13}{20}$\n$E[Y] = \\sum_y y \\cdot p_Y(y) = (0)\\left(\\frac{2}{5}\\right) + (1)\\left(\\frac{3}{5}\\right) = \\frac{3}{5}$\n\nTo find the variances, we use the formula $\\text{Var}(Z) = E[Z^2] - (E[Z])^2$. Since $X$ and $Y$ only take values 0 and 1, we have $X^2=X$ and $Y^2=Y$.\nTherefore, $E[X^2] = E[X] = \\frac{13}{20}$ and $E[Y^2] = E[Y] = \\frac{3}{5}$.\n$\\text{Var}(X) = E[X^2] - (E[X])^2 = \\frac{13}{20} - \\left(\\frac{13}{20}\\right)^2 = \\frac{13}{20} - \\frac{169}{400} = \\frac{260}{400} - \\frac{169}{400} = \\frac{91}{400}$\n$\\text{Var}(Y) = E[Y^2] - (E[Y])^2 = \\frac{3}{5} - \\left(\\frac{3}{5}\\right)^2 = \\frac{3}{5} - \\frac{9}{25} = \\frac{15}{25} - \\frac{9}{25} = \\frac{6}{25}$\n\nNow, we compute $\\text{Cov}(X, Y) = E[XY] - E[X]E[Y]$. First we find $E[XY]$:\n$E[XY] = \\sum_x \\sum_y xy \\cdot p(x, y)$\nThe only term that is not zero is for $x=1$ and $y=1$.\n$E[XY] = (1)(1) \\cdot p(1,1) = \\frac{8}{20} = \\frac{2}{5}$\nNow, we can find the covariance of $X$ and $Y$:\n$\\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = \\frac{2}{5} - \\left(\\frac{13}{20}\\right)\\left(\\frac{3}{5}\\right) = \\frac{2}{5} - \\frac{39}{100} = \\frac{40}{100} - \\frac{39}{100} = \\frac{1}{100}$\n\nFinally, we substitute $\\text{Var}(X)$, $\\text{Var}(Y)$, and $\\text{Cov}(X,Y)$ into our expression for $\\text{Cov}(U,V)$:\n$\\text{Cov}(U, V) = 2\\,\\text{Var}(X) - 5\\,\\text{Cov}(X,Y) - 3\\,\\text{Var}(Y)$\n$= 2\\left(\\frac{91}{400}\\right) - 5\\left(\\frac{1}{100}\\right) - 3\\left(\\frac{6}{25}\\right)$\n$= \\frac{91}{200} - \\frac{5}{100} - \\frac{18}{25}$\nWe find a common denominator, which is 200:\n$= \\frac{91}{200} - \\frac{5 \\times 2}{100 \\times 2} - \\frac{18 \\times 8}{25 \\times 8}$\n$= \\frac{91}{200} - \\frac{10}{200} - \\frac{144}{200}$\n$= \\frac{91 - 10 - 144}{200}$\n$= \\frac{81 - 144}{200}$\n$= \\frac{-63}{200}$\nThe fraction is in its simplest form, as the prime factors of 63 are $3^2 \\times 7$ and the prime factors of 200 are $2^3 \\times 5^2$.", "answer": "$$\\boxed{\\frac{-63}{200}}$$", "id": "1947661"}, {"introduction": "Now that you can calculate covariance, it's time to explore its conceptual limitations. This exercise presents a scenario where two random variables, $X$ and $X^2$, are clearly dependent, yet their covariance is zero [@problem_id:1947625]. Working through this problem is vital for understanding the critical distinction between statistical independence and zero covariance, reminding us that covariance primarily captures linear relationships.", "problem": "In the development of a high-precision gyroscope, the random error in its angular velocity measurement is modeled by a continuous random variable $X$. The probability density function (PDF) for this error is given by $f(x) = C x^2$ for any $x$ in the interval $[-1, 1]$, and $f(x)=0$ otherwise. Here, $C$ is a normalization constant. To analyze potential systematic biases, an engineer wants to study the statistical relationship between the error $X$ and the square of the error $X^2$, which relates to the kinetic energy of the rotational fluctuations.\n\nCalculate the covariance between the error $X$ and its square $X^2$.", "solution": "We are given a continuous random variable $X$ with probability density function $f(x)=C x^{2}$ for $x \\in [-1,1]$ and $f(x)=0$ otherwise. First, determine the normalization constant $C$ by requiring that the total probability is one:\n$$\n\\int_{-\\;1}^{1} f(x)\\,dx=\\int_{-\\;1}^{1} C x^{2}\\,dx = C \\int_{-\\;1}^{1} x^{2}\\,dx = C \\left[\\frac{x^{3}}{3}\\right]_{-1}^{1} = C \\left(\\frac{1}{3}-\\left(-\\frac{1}{3}\\right)\\right) = C \\cdot \\frac{2}{3} = 1.\n$$\nThus,\n$$\nC = \\frac{3}{2}.\n$$\nThe covariance between $X$ and $X^{2}$ is defined by\n$$\n\\operatorname{Cov}(X,X^{2}) = \\mathbb{E}[X^{3}] - \\mathbb{E}[X]\\mathbb{E}[X^{2}].\n$$\nCompute the moments needed.\n\nFirst, compute $\\mathbb{E}[X]$:\n$$\n\\mathbb{E}[X] = \\int_{-\\;1}^{1} x f(x)\\,dx = \\int_{-\\;1}^{1} x \\left(\\frac{3}{2} x^{2}\\right)\\,dx = \\frac{3}{2} \\int_{-\\;1}^{1} x^{3}\\,dx = \\frac{3}{2} \\cdot 0 = 0,\n$$\nsince $x^{3}$ is an odd function integrated over a symmetric interval.\n\nNext, compute $\\mathbb{E}[X^{2}]$:\n$$\n\\mathbb{E}[X^{2}] = \\int_{-\\;1}^{1} x^{2} f(x)\\,dx = \\int_{-\\;1}^{1} x^{2} \\left(\\frac{3}{2} x^{2}\\right)\\,dx = \\frac{3}{2} \\int_{-\\;1}^{1} x^{4}\\,dx = \\frac{3}{2} \\left[\\frac{x^{5}}{5}\\right]_{-1}^{1} = \\frac{3}{2} \\cdot \\frac{2}{5} = \\frac{3}{5}.\n$$\n\nFinally, compute $\\mathbb{E}[X^{3}]$:\n$$\n\\mathbb{E}[X^{3}] = \\int_{-\\;1}^{1} x^{3} f(x)\\,dx = \\int_{-\\;1}^{1} x^{3} \\left(\\frac{3}{2} x^{2}\\right)\\,dx = \\frac{3}{2} \\int_{-\\;1}^{1} x^{5}\\,dx = \\frac{3}{2} \\cdot 0 = 0,\n$$\nsince $x^{5}$ is also odd over $[-1,1]$.\n\nTherefore,\n$$\n\\operatorname{Cov}(X,X^{2}) = \\mathbb{E}[X^{3}] - \\mathbb{E}[X]\\mathbb{E}[X^{2}] = 0 - 0 \\cdot \\frac{3}{5} = 0.\n$$", "answer": "$$\\boxed{0}$$", "id": "1947625"}, {"introduction": "Our final practice moves from static variables to a dynamic context common in time series analysis and stochastic processes. We will investigate the relationship between cumulative sums of independent random variables at different points in time, $S_m$ and $S_n$ [@problem_id:1947618]. Deriving the correlation coefficient in this scenario reveals how dependence structures can emerge in aggregated data, a key insight for fields from signal processing to finance.", "problem": "In the field of signal processing, it is common to analyze time-series data by examining cumulative sums. Consider a sequence of experimental measurements where the error on day $i$ is represented by a random variable $X_i$. Let the sequence of errors, $X_1, X_2, \\dots, X_n$, be independent and identically distributed (i.i.d.) random variables, each with an expected value of zero and a finite, non-zero variance $\\sigma^2$.\n\nTo study the propagation of error over time, an analyst considers the cumulative error up to day $k$, defined as the partial sum $S_k = \\sum_{i=1}^k X_i$. The analyst is interested in understanding the statistical relationship between the cumulative error at an early time point and a later one.\n\nGiven two time points $m$ and $n$ such that $1 \\le m  n$, determine the correlation coefficient, $\\rho(S_m, S_n)$, between the cumulative sum at day $m$ and the cumulative sum at day $n$. Express your answer as a closed-form analytic expression in terms of $m$ and $n$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. with $\\mathbb{E}[X_{i}]=0$ and $\\operatorname{Var}(X_{i})=\\sigma^{2}$, and define $S_{k}=\\sum_{i=1}^{k}X_{i}$. First, compute the mean and variance of $S_{k}$:\n$$\n\\mathbb{E}[S_{k}]=\\sum_{i=1}^{k}\\mathbb{E}[X_{i}]=0,\n\\qquad\n\\operatorname{Var}(S_{k})=\\sum_{i=1}^{k}\\operatorname{Var}(X_{i})=k\\sigma^{2},\n$$\nsince the $X_{i}$ are independent so all cross-covariances vanish.\n\nFor $1\\leq mn$, write $S_{n}=S_{m}+R$, where $R=\\sum_{i=m+1}^{n}X_{i}$. Because $S_{m}$ depends only on $X_{1},\\dots,X_{m}$ and $R$ depends only on $X_{m+1},\\dots,X_{n}$, independence of the $X_{i}$ implies that $S_{m}$ and $R$ are independent. Thus,\n$$\n\\operatorname{Cov}(S_{m},S_{n})\n=\\operatorname{Cov}(S_{m},S_{m}+R)\n=\\operatorname{Cov}(S_{m},S_{m})+\\operatorname{Cov}(S_{m},R)\n=\\operatorname{Var}(S_{m})+0\n=m\\sigma^{2}.\n$$\nAlso,\n$$\n\\operatorname{Var}(S_{m})=m\\sigma^{2},\\qquad \\operatorname{Var}(S_{n})=n\\sigma^{2}.\n$$\nThe correlation coefficient is\n$$\n\\rho(S_{m},S_{n})\n=\\frac{\\operatorname{Cov}(S_{m},S_{n})}{\\sqrt{\\operatorname{Var}(S_{m})\\,\\operatorname{Var}(S_{n})}}\n=\\frac{m\\sigma^{2}}{\\sqrt{(m\\sigma^{2})(n\\sigma^{2})}}\n=\\frac{m}{\\sqrt{mn}}\n=\\sqrt{\\frac{m}{n}}.\n$$", "answer": "$$\\boxed{\\sqrt{\\frac{m}{n}}}$$", "id": "1947618"}]}