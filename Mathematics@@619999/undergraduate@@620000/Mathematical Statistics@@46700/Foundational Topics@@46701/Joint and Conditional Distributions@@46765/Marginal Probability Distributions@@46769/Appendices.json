{"hands_on_practices": [{"introduction": "To begin our exploration of marginal distributions, we start with a foundational exercise. This problem [@problem_id:1932521] focuses on computing a core property of a single random variable, its expected value, directly from a joint probability density function defined on a simple rectangular domain. This practice is essential for mastering the fundamental mechanics of double integration and normalization, which are the building blocks for analyzing multi-variable systems.", "problem": "Consider two continuous random variables, $X$ and $Y$, whose interaction is described by a joint probability density function (PDF) given by $f(x, y) = c(x^{2} + y)$. This function is defined over the square region where $0 \\le x \\le 1$ and $0 \\le y \\le 1$, and $f(x, y) = 0$ everywhere else. Here, $c$ is a normalization constant.\n\nYour task is to calculate the expected value of the random variable $X$, denoted as $E[X]$. Express your answer as a single fraction in its simplest form.", "solution": "We first determine the normalization constant $c$ by enforcing that a probability density integrates to $1$ over its support. The support is the square $\\{(x,y): 0 \\leq x \\leq 1, 0 \\leq y \\leq 1\\}$. Hence,\n$$\n\\int_{0}^{1}\\int_{0}^{1} f(x,y)\\,dy\\,dx \\;=\\; \\int_{0}^{1}\\int_{0}^{1} c\\left(x^{2}+y\\right)\\,dy\\,dx \\;=\\; 1.\n$$\nCompute the inner integral with respect to $y$:\n$$\n\\int_{0}^{1} \\left(x^{2}+y\\right)\\,dy \\;=\\; x^{2}\\int_{0}^{1} dy + \\int_{0}^{1} y\\,dy \\;=\\; x^{2}\\cdot 1 + \\frac{1}{2} \\;=\\; x^{2} + \\frac{1}{2}.\n$$\nNow integrate with respect to $x$:\n$$\n\\int_{0}^{1} \\left(x^{2} + \\frac{1}{2}\\right)\\,dx \\;=\\; \\left[\\frac{x^{3}}{3} + \\frac{x}{2}\\right]_{0}^{1} \\;=\\; \\frac{1}{3} + \\frac{1}{2} \\;=\\; \\frac{5}{6}.\n$$\nThus,\n$$\nc \\cdot \\frac{5}{6} \\;=\\; 1 \\;\\;\\Rightarrow\\;\\; c \\;=\\; \\frac{6}{5}.\n$$\n\nThe expected value of $X$ is defined by\n$$\nE[X] \\;=\\; \\int_{0}^{1}\\int_{0}^{1} x\\, f(x,y)\\,dy\\,dx \\;=\\; \\int_{0}^{1}\\int_{0}^{1} x \\cdot c\\left(x^{2}+y\\right)\\,dy\\,dx \\;=\\; c \\int_{0}^{1}\\int_{0}^{1} \\left(x^{3} + x y\\right)\\,dy\\,dx.\n$$\nCompute the inner integral with respect to $y$:\n$$\n\\int_{0}^{1} \\left(x^{3} + x y\\right)\\,dy \\;=\\; x^{3}\\int_{0}^{1} dy + x \\int_{0}^{1} y\\,dy \\;=\\; x^{3}\\cdot 1 + x \\cdot \\frac{1}{2} \\;=\\; x^{3} + \\frac{x}{2}.\n$$\nNow integrate with respect to $x$:\n$$\n\\int_{0}^{1} \\left(x^{3} + \\frac{x}{2}\\right)\\,dx \\;=\\; \\left[\\frac{x^{4}}{4} + \\frac{x^{2}}{4}\\right]_{0}^{1} \\;=\\; \\frac{1}{4} + \\frac{1}{4} \\;=\\; \\frac{1}{2}.\n$$\nTherefore,\n$$\nE[X] \\;=\\; c \\cdot \\frac{1}{2} \\;=\\; \\frac{6}{5} \\cdot \\frac{1}{2} \\;=\\; \\frac{3}{5}.\n$$", "answer": "$$\\boxed{\\frac{3}{5}}$$", "id": "1932521"}, {"introduction": "Building upon the basics, this exercise introduces a common and important complexity: a non-rectangular domain. When the ranges of random variables are interdependent, as in the triangular region in this problem [@problem_id:1932559], we must explicitly derive the marginal probability density function by carefully defining the limits of integration. This practice is crucial for developing a deeper understanding of how the geometric constraints on a system directly influence the probabilistic behavior of its individual components.", "problem": "A point with coordinates $(X, Y)$ is chosen uniformly at random from the area enclosed by the triangle with vertices at $(0,0)$, $(2,0)$, and $(1,1)$. The random variables $X$ and $Y$ represent the coordinates of the chosen point.\n\nCalculate the variance of the random variable $Y$, denoted as $\\text{Var}(Y)$.", "solution": "The region is the triangle with vertices at $(0,0)$, $(2,0)$, and $(1,1)$. Its area is \n$$\nA=\\frac{1}{2}\\times 2\\times 1=1.\n$$\nFor a uniform distribution over this triangle, the joint density is $f_{X,Y}(x,y)=\\frac{1}{A}=1$ on the triangle and $0$ outside.\n\nFor a fixed $y$ with $0\\leq y\\leq 1$, the horizontal slice of the triangle runs from the left edge along the line from $(0,0)$ to $(1,1)$, which has equation $x=y$, to the right edge along the line from $(2,0)$ to $(1,1)$, which has equation $x=2-y$. Therefore, the marginal density of $Y$ is\n$$\nf_{Y}(y)=\\int_{x=y}^{2-y} 1\\,dx=(2-y)-y=2-2y,\\quad 0\\leq y\\leq 1,\n$$\nand $f_{Y}(y)=0$ otherwise.\n\nCompute the first and second moments of $Y$:\n$$\n\\mathbb{E}[Y]=\\int_{0}^{1} y\\,f_{Y}(y)\\,dy=\\int_{0}^{1} y(2-2y)\\,dy=2\\int_{0}^{1} y\\,dy-2\\int_{0}^{1} y^{2}\\,dy.\n$$\nEvaluate the integrals:\n$$\n\\int_{0}^{1} y\\,dy=\\left.\\frac{y^{2}}{2}\\right|_{0}^{1}=\\frac{1}{2},\\quad \\int_{0}^{1} y^{2}\\,dy=\\left.\\frac{y^{3}}{3}\\right|_{0}^{1}=\\frac{1}{3}.\n$$\nThus,\n$$\n\\mathbb{E}[Y]=2\\cdot \\frac{1}{2}-2\\cdot \\frac{1}{3}=1-\\frac{2}{3}=\\frac{1}{3}.\n$$\nNext,\n$$\n\\mathbb{E}[Y^{2}]=\\int_{0}^{1} y^{2}\\,f_{Y}(y)\\,dy=\\int_{0}^{1} y^{2}(2-2y)\\,dy=2\\int_{0}^{1} y^{2}\\,dy-2\\int_{0}^{1} y^{3}\\,dy.\n$$\nEvaluate the integrals:\n$$\n\\int_{0}^{1} y^{2}\\,dy=\\frac{1}{3},\\quad \\int_{0}^{1} y^{3}\\,dy=\\left.\\frac{y^{4}}{4}\\right|_{0}^{1}=\\frac{1}{4}.\n$$\nHence,\n$$\n\\mathbb{E}[Y^{2}]=2\\cdot \\frac{1}{3}-2\\cdot \\frac{1}{4}=\\frac{2}{3}-\\frac{1}{2}=\\frac{1}{6}.\n$$\nBy the variance formula $\\text{Var}(Y)=\\mathbb{E}[Y^{2}]-(\\mathbb{E}[Y])^{2}$, we obtain\n$$\n\\text{Var}(Y)=\\frac{1}{6}-\\left(\\frac{1}{3}\\right)^{2}=\\frac{1}{6}-\\frac{1}{9}=\\frac{1}{18}.\n$$", "answer": "$$\\boxed{\\frac{1}{18}}$$", "id": "1932559"}, {"introduction": "This advanced problem [@problem_id:1932526] demonstrates the power and versatility of marginalization in the context of hierarchical models, a structure frequently encountered in fields like engineering and Bayesian statistics. Here, the parameter $\\Lambda$ of a Poisson distribution is itself a random variable, and we seek the unconditional variance of the outcome $X$. This requires us to \"average over\" the uncertainty in the parameter, showcasing how the Law of Total Variance provides an elegant pathway to understanding the total variability in a system with multiple layers of randomness.", "problem": "In a semiconductor manufacturing process, the number of defects, $X$, on a single chip is found to follow a Poisson distribution. However, the average defect rate, $\\Lambda$, is not constant across production batches and is itself a random variable. It has been determined that $\\Lambda$ follows an exponential distribution.\n\nLet the conditional probability mass function (PMF) of $X$ given a specific defect rate $\\lambda$ be:\n$$P(X=k | \\Lambda=\\lambda) = \\frac{\\exp(-\\lambda) \\lambda^k}{k!}, \\quad \\text{for } k=0, 1, 2, \\dots$$\nThe probability density function (PDF) for the random variable $\\Lambda$ is given by:\n$$f_{\\Lambda}(\\lambda) = \\beta \\exp(-\\beta \\lambda), \\quad \\text{for } \\lambda > 0$$\nwhere $\\beta$ is a positive constant parameterizing the variability of the defect rate.\n\nYour task is to determine the unconditional variance of the number of defects, $\\text{Var}(X)$. Express your answer as an analytic expression in terms of the parameter $\\beta$.", "solution": "Let $X|\\Lambda=\\lambda$ follow a Poisson distribution with mean $\\lambda$, so the conditional mean and variance are\n$$\n\\mathbb{E}[X|\\Lambda=\\lambda]=\\lambda, \\quad \\operatorname{Var}(X|\\Lambda=\\lambda)=\\lambda.\n$$\nThe unconditional variance is obtained by the law of total variance:\n$$\n\\operatorname{Var}(X)=\\mathbb{E}\\!\\left[\\operatorname{Var}(X|\\Lambda)\\right]+\\operatorname{Var}\\!\\left(\\mathbb{E}[X|\\Lambda]\\right).\n$$\nSubstituting the conditional moments of the Poisson distribution gives\n$$\n\\operatorname{Var}(X)=\\mathbb{E}[\\Lambda]+\\operatorname{Var}(\\Lambda).\n$$\n\nNow compute the first two moments of $\\Lambda$ when $\\Lambda$ has the exponential density $f_{\\Lambda}(\\lambda)=\\beta \\exp(-\\beta \\lambda)$ for $\\lambda>0$.\n\nFirst moment:\n$$\n\\mathbb{E}[\\Lambda]=\\int_{0}^{\\infty}\\lambda\\,\\beta \\exp(-\\beta \\lambda)\\,d\\lambda.\n$$\nLet $t=\\beta \\lambda$, so $d\\lambda=dt/\\beta$ and $\\lambda=t/\\beta$. Then\n$$\n\\mathbb{E}[\\Lambda]=\\beta \\int_{0}^{\\infty}\\frac{t}{\\beta}\\exp(-t)\\frac{dt}{\\beta}=\\frac{1}{\\beta}\\int_{0}^{\\infty}t\\exp(-t)\\,dt=\\frac{\\Gamma(2)}{\\beta}=\\frac{1}{\\beta}.\n$$\n\nSecond moment:\n$$\n\\mathbb{E}[\\Lambda^{2}]=\\int_{0}^{\\infty}\\lambda^{2}\\,\\beta \\exp(-\\beta \\lambda)\\,d\\lambda.\n$$\nWith the same substitution $t=\\beta \\lambda$,\n$$\n\\mathbb{E}[\\Lambda^{2}]=\\beta \\int_{0}^{\\infty}\\left(\\frac{t}{\\beta}\\right)^{2}\\exp(-t)\\frac{dt}{\\beta}=\\frac{1}{\\beta^{2}}\\int_{0}^{\\infty}t^{2}\\exp(-t)\\,dt=\\frac{\\Gamma(3)}{\\beta^{2}}=\\frac{2}{\\beta^{2}}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(\\Lambda)=\\mathbb{E}[\\Lambda^{2}]-(\\mathbb{E}[\\Lambda])^{2}=\\frac{2}{\\beta^{2}}-\\left(\\frac{1}{\\beta}\\right)^{2}=\\frac{1}{\\beta^{2}}.\n$$\n\nSubstituting into the total variance expression yields\n$$\n\\operatorname{Var}(X)=\\mathbb{E}[\\Lambda]+\\operatorname{Var}(\\Lambda)=\\frac{1}{\\beta}+\\frac{1}{\\beta^{2}}.\n$$", "answer": "$$\\boxed{\\frac{1}{\\beta}+\\frac{1}{\\beta^{2}}}$$", "id": "1932526"}]}