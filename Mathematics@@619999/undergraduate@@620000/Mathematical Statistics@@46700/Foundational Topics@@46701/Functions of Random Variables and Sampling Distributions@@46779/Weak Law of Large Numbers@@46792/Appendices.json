{"hands_on_practices": [{"introduction": "The Weak Law of Large Numbers (WLLN) gives us confidence that a sample average will approach the true mean as the sample grows. But in practice, we often need to know, \"how large is large enough?\" This first exercise [@problem_id:1967340] tackles this fundamental question by using Chebyshev's inequality as a practical tool to determine the minimum sample size needed to achieve a desired level of accuracy and confidence when estimating the mean lifetime of a product. It is a classic example of how theoretical probability provides concrete guarantees for real-world statistical estimation.", "problem": "A company is manufacturing a new type of Resistive Random-Access Memory (ReRAM) chip. The operational lifetime of a single chip, measured in years, is modeled as a random variable following an exponential distribution with an unknown rate parameter $\\lambda > 0$. The expected lifetime is thus $\\mu = 1/\\lambda$. To estimate this mean lifetime, the company tests a sample of $n$ chips and calculates their sample mean lifetime, $\\bar{X}_n$, where the individual lifetimes are independent and identically distributed.\n\nThe company's quality assurance standard requires that the probability of the sample mean $\\bar{X}_n$ being within 10% of the true mean lifetime $\\mu$ is at least 95%.\n\nBy applying a general probability inequality that provides a bound using only the mean and variance, determine the smallest integer sample size $n$ required to meet this standard.", "solution": "Let $X_{1},\\dots,X_{n}$ be independent and identically distributed exponential random variables with rate $\\lambda>0$, so the mean and variance of each $X_{i}$ are\n$$\n\\mathbb{E}[X_{i}]=\\mu=\\frac{1}{\\lambda}, \\qquad \\operatorname{Var}(X_{i})=\\frac{1}{\\lambda^{2}}=\\mu^{2}.\n$$\nThe sample mean is $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$, which satisfies, by independence,\n$$\n\\mathbb{E}[\\bar{X}_{n}]=\\mu, \\qquad \\operatorname{Var}(\\bar{X}_{n})=\\frac{\\operatorname{Var}(X_{i})}{n}=\\frac{\\mu^{2}}{n}.\n$$\nApply Chebyshevâ€™s inequality, which uses only mean and variance: for any $\\epsilon>0$,\n$$\n\\mathbb{P}\\big(|\\bar{X}_{n}-\\mu|\\geq \\epsilon\\big)\\leq \\frac{\\operatorname{Var}(\\bar{X}_{n})}{\\epsilon^{2}}.\n$$\nSet the tolerance to ten percent of the mean, $\\epsilon=\\frac{1}{10}\\mu$. Then\n$$\n\\mathbb{P}\\big(|\\bar{X}_{n}-\\mu|\\geq \\tfrac{1}{10}\\mu\\big)\\leq \\frac{\\mu^{2}/n}{\\left(\\tfrac{1}{10}\\mu\\right)^{2}}=\\frac{\\mu^{2}/n}{\\tfrac{1}{100}\\mu^{2}}=\\frac{100}{n}.\n$$\nTherefore,\n$$\n\\mathbb{P}\\big(|\\bar{X}_{n}-\\mu|\\leq \\tfrac{1}{10}\\mu\\big)\\geq 1-\\frac{100}{n}.\n$$\nTo meet the requirement that this probability be at least $0.95$ (or $\\frac{19}{20}$), impose\n$$\n1-\\frac{100}{n}\\geq \\frac{19}{20}\\quad \\Longleftrightarrow \\quad \\frac{100}{n}\\leq \\frac{1}{20}\\quad \\Longleftrightarrow \\quad n\\geq 2000.\n$$\nSince $n$ must be an integer, the smallest such $n$ is $2000$.", "answer": "$$\\boxed{2000}$$", "id": "1967340"}, {"introduction": "Beyond simply confirming that the sample mean converges to the true mean, the WLLN is a far more general and powerful tool. This problem [@problem_id:1967327] explores that versatility by asking what the sample average of the *squares* of random variables, known as the sample second moment, converges to. Solving this requires you to recognize that the Law of Large Numbers can be applied to a new set of transformed variables, allowing us to find the limit of more complex statistical summaries.", "problem": "Consider a sequence of random variables $X_1, X_2, \\dots, X_n$ that are independent and identically distributed (i.i.d.). Each random variable $X_i$ in this sequence has a known finite mean $E[X_i] = \\mu$ and a known finite, positive variance $\\text{Var}(X_i) = \\sigma^2$.\n\nWe define the sample second moment about the origin for this sequence as:\n$$M_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i^2$$\nThis quantity is of interest in various fields, for instance, in physics where it might relate to the average energy of a system of particles.\n\nDetermine the value to which $M_n$ converges in probability as the sample size $n$ approaches infinity. Express your answer as a closed-form analytic expression in terms of $\\mu$ and $\\sigma$.", "solution": "The problem asks for the value to which the sample second moment, $M_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i^2$, converges in probability. This is a direct application of the Weak Law of Large Numbers (WLLN).\n\nThe WLLN states that for a sequence of independent and identically distributed (i.i.d.) random variables $Y_1, Y_2, \\dots$ with a finite expected value $E[Y_i] = \\mu_Y$, their sample mean $\\bar{Y}_n = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$ converges in probability to $\\mu_Y$. We can write this as $\\bar{Y}_n \\xrightarrow{p} \\mu_Y$ as $n \\to \\infty$.\n\nTo apply the WLLN to our problem, let's define a new sequence of random variables $Y_i = X_i^2$. The quantity $M_n$ can then be rewritten as the sample mean of this new sequence:\n$$M_n = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}_n$$\n\nNow, we must check if the conditions for the WLLN are met for the sequence $Y_i$.\n1.  **I.I.D. Condition:** The problem states that the random variables $X_1, X_2, \\dots$ are i.i.d. Since each $Y_i$ is a function of the corresponding $X_i$ (specifically $Y_i = X_i^2$), and the function is the same for all $i$, the sequence of random variables $Y_1, Y_2, \\dots$ is also independent and identically distributed.\n\n2.  **Finite Mean Condition:** The WLLN requires that the expected value of $Y_i$, denoted $E[Y_i]$, is finite. Let's calculate this expectation.\n$$E[Y_i] = E[X_i^2]$$\nWe can relate $E[X_i^2]$ to the given mean and variance of $X_i$. The definition of variance is:\n$$\\text{Var}(X_i) = E[X_i^2] - (E[X_i])^2$$\nWe are given that $\\text{Var}(X_i) = \\sigma^2$ and $E[X_i] = \\mu$. Substituting these values into the variance formula gives:\n$$\\sigma^2 = E[X_i^2] - \\mu^2$$\nSolving for $E[X_i^2]$, we get:\n$$E[X_i^2] = \\mu^2 + \\sigma^2$$\nSince $\\mu$ and $\\sigma^2$ are given as finite, the expectation $E[Y_i] = \\mu^2 + \\sigma^2$ is also finite. Let's denote this common mean of the $Y_i$ sequence as $\\mu_Y = \\mu^2 + \\sigma^2$.\n\nSince both conditions for the WLLN are satisfied for the sequence $Y_i = X_i^2$, we can conclude that their sample mean, $M_n$, converges in probability to their true mean, $\\mu_Y$.\n$$M_n \\xrightarrow{p} E[Y_i] = \\mu^2 + \\sigma^2$$\n\nThus, the value to which $M_n$ converges in probability is $\\mu^2 + \\sigma^2$.", "answer": "$$\\boxed{\\mu^{2} + \\sigma^{2}}$$", "id": "1967327"}, {"introduction": "Statistical analysis often involves combining multiple estimates. For example, we might be interested in the ratio of a defect rate to an average physical measurement. This final exercise [@problem_id:1967350] demonstrates how to handle such cases by combining the Weak Law of Large Numbers with the Continuous Mapping Theorem. You will determine the limiting value of a ratio of two different sample means, a skill that is essential for analyzing the long-run behavior of more complex estimators.", "problem": "An electronics manufacturer is conducting a quality control study on a particular model of resistor. They model two key characteristics for a random sample of resistors. For the $i$-th resistor in the sample, let $X_i$ be an indicator variable such that $X_i=1$ if the resistor is defective and $X_i=0$ if it is not. Let $Y_i$ be the measured resistance in ohms ($\\Omega$).\n\nThe random variables $X_1, X_2, \\dots$ are Independent and Identically Distributed (IID) Bernoulli random variables with a probability of being defective given by $p = 0.04$.\n\nThe random variables $Y_1, Y_2, \\dots$ are also IID, and their values are drawn from a continuous Uniform distribution on the interval $[980, 1020]$. The sequences $\\{X_i\\}$ and $\\{Y_i\\}$ are independent of each other.\n\nThe manufacturer is interested in a specific metric derived from a large sample of size $n$. Let $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample proportion of defective resistors and $\\bar{Y}_n = \\frac{1}{n}\\sum_{i=1}^{n} Y_i$ be the sample average resistance.\n\nDetermine the numerical value that the ratio $R_n = \\frac{\\bar{X}_n}{\\bar{Y}_n}$ converges to as the sample size $n$ tends to infinity. Express your answer in scientific notation of the form $a \\times 10^b$, where $1 \\le a < 10$. The value of $a$ should be rounded to two significant figures. Answer in units of $\\Omega^{-1}$.", "solution": "We have $X_{i} \\sim \\text{Bernoulli}(p)$ with $p=0.04$, $Y_{i} \\sim \\text{Uniform}[a,b]$ with $a=980$ and $b=1020$, and the sequences $\\{X_{i}\\}$ and $\\{Y_{i}\\}$ are independent. Define $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ and $\\bar{Y}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}$.\n\nBy the Weak Law of Large Numbers, \n$$\n\\bar{X}_{n} \\to \\mathbb{E}[X_{1}] \\quad \\text{and} \\quad \\bar{Y}_{n} \\to \\mathbb{E}[Y_{1}] \\quad \\text{in probability as } n \\to \\infty.\n$$\nCompute the expectations:\n$$\n\\mathbb{E}[X_{1}] = p = 0.04,\n$$\nand for the uniform distribution,\n$$\n\\mathbb{E}[Y_{1}] = \\frac{a+b}{2} = \\frac{980+1020}{2} = 1000.\n$$\nSince $\\mathbb{E}[Y_{1}] = 1000 \\neq 0$, by the Continuous Mapping Theorem,\n$$\nR_{n}=\\frac{\\bar{X}_{n}}{\\bar{Y}_{n}} \\to \\frac{\\mathbb{E}[X_{1}]}{\\mathbb{E}[Y_{1}]} = \\frac{0.04}{1000} = 4.0 \\times 10^{-5} \\quad \\text{in probability}.\n$$\nThe units are $\\Omega^{-1}$, as $\\bar{X}_{n}$ is dimensionless and $\\bar{Y}_{n}$ has units of $\\Omega$. The required scientific notation with two significant figures is $4.0 \\times 10^{-5}$.", "answer": "$$\\boxed{4.0 \\times 10^{-5}}$$", "id": "1967350"}]}