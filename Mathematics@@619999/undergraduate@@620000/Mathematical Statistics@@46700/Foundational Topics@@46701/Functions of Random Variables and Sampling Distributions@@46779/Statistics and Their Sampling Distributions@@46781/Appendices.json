{"hands_on_practices": [{"introduction": "In many scientific and engineering contexts, we are interested in a quantity that results from combining two or more random variables. This exercise provides a foundational practice in this area, exploring the sampling distribution of the difference between two independent, normally distributed variables. Understanding how to determine the mean and variance of such a combination is a critical first step in predicting the behavior and variability of complex systems [@problem_id:1956549].", "problem": "An electronics manufacturer produces high-precision resistors on two independent production lines. The resistance of a resistor from the first line, denoted as the random variable $X$, follows a normal distribution with a mean $\\mu_X = 150.0 \\, \\Omega$ and a variance $\\sigma_X^2 = 2.5 \\, \\Omega^2$. The resistance of a resistor from the second line, denoted as the random variable $Y$, follows a normal distribution with a mean $\\mu_Y = 148.0 \\, \\Omega$ and a variance $\\sigma_Y^2 = 1.5 \\, \\Omega^2$.\n\nA quality control engineer selects one resistor at random from each production line. Let the random variable $D$ represent the difference between the resistance of the first resistor and the second resistor, such that $D = X - Y$.\n\nDetermine the mean and the variance of the sampling distribution of this difference $D$. Present your answer as a pair of numerical values: the mean in $\\Omega$, and the variance in $\\Omega^2$.", "solution": "Let $X$ and $Y$ be independent with $X \\sim \\mathcal{N}(\\mu_{X}, \\sigma_{X}^{2})$ and $Y \\sim \\mathcal{N}(\\mu_{Y}, \\sigma_{Y}^{2})$, where $\\mu_{X} = 150.0$, $\\sigma_{X}^{2} = 2.5$, $\\mu_{Y} = 148.0$, and $\\sigma_{Y}^{2} = 1.5$. Define $D = X - Y$.\n\nBy linearity of expectation,\n$$\n\\mathbb{E}[D] = \\mathbb{E}[X - Y] = \\mathbb{E}[X] - \\mathbb{E}[Y] = \\mu_{X} - \\mu_{Y}.\n$$\nSubstituting the given values,\n$$\n\\mathbb{E}[D] = 150.0 - 148.0 = 2.0.\n$$\n\nUsing the variance formula for a linear combination and independence (so $\\operatorname{Cov}(X,Y) = 0$),\n$$\n\\operatorname{Var}(D) = \\operatorname{Var}(X - Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) - 2\\,\\operatorname{Cov}(X,Y) = \\sigma_{X}^{2} + \\sigma_{Y}^{2}.\n$$\nSubstituting the given variances,\n$$\n\\operatorname{Var}(D) = 2.5 + 1.5 = 4.0.\n$$\n\nTherefore, the mean of $D$ is $2.0 \\, \\Omega$ and the variance of $D$ is $4.0 \\, \\Omega^2$.", "answer": "$$\\boxed{\\begin{pmatrix} 2 & 4 \\end{pmatrix}}$$", "id": "1956549"}, {"introduction": "While the normal distribution is common, many real-world processes are better described by other distributions. This problem challenges you to move beyond simply calculating moments and instead derive the complete shape of a sampling distribution from first principles. By finding the probability density function for the sum of two independent uniform random variables, you will use the powerful technique of convolution and see how simple, rectangular distributions combine to form a new, triangular shape [@problem_id:1956529].", "problem": "A computer simulation generates two independent random numbers, let's call them $X_1$ and $X_2$. Each number is drawn from the continuous uniform distribution on the interval $[0, 1]$. A new random variable $S$ is defined as the sum of these two numbers, $S = X_1 + X_2$.\n\nWhich of the following statements provides the most accurate description of the probability density function (PDF), denoted $f_S(s)$, for the random variable $S$?\n\nA. The PDF is a triangular function which increases linearly from $s=0$ to $s=1$ and then decreases linearly from $s=1$ to $s=2$.\n\nB. The PDF is a uniform distribution on the interval $[0, 2]$.\n\nC. The PDF is a normal distribution, as a consequence of the Central Limit Theorem.\n\nD. The PDF is an exponential distribution with a rate parameter of $\\lambda=1$.\n\nE. The PDF is a piecewise constant function on the interval $[0, 2]$.", "solution": "Let $X_{1}$ and $X_{2}$ be independent with $X_{i} \\sim \\mathrm{Unif}[0,1]$. Then each has density $f_{X_{i}}(x)=1$ for $0 \\leq x \\leq 1$ and $f_{X_{i}}(x)=0$ otherwise. For the sum $S=X_{1}+X_{2}$, the probability density function is given by the convolution of the two densities:\n$$\nf_{S}(s)=\\int_{-\\infty}^{\\infty} f_{X_{1}}(x)\\,f_{X_{2}}(s-x)\\,dx.\n$$\nSince $f_{X_{1}}(x)$ is nonzero only for $0 \\leq x \\leq 1$ and $f_{X_{2}}(s-x)$ is nonzero only when $0 \\leq s-x \\leq 1$, the integrand is $1$ precisely on the set of $x$ satisfying both\n$$\n0 \\leq x \\leq 1 \\quad \\text{and} \\quad 0 \\leq s-x \\leq 1,\n$$\nwhich is equivalent to\n$$\n0 \\leq x \\leq 1 \\quad \\text{and} \\quad s-1 \\leq x \\leq s.\n$$\nTherefore, the integration interval is the intersection $[0,1] \\cap [s-1,s]$. The value of the integral equals the length of this intersection:\n- If $s<0$ or $s>2$, the intersection is empty, so $f_{S}(s)=0$.\n- If $0 \\leq s \\leq 1$, the intersection is $[0,s]$ with length $s$, so $f_{S}(s)=s$.\n- If $1 \\leq s \\leq 2$, the intersection is $[s-1,1]$ with length $2-s$, so $f_{S}(s)=2-s$.\n\nThus,\n$$\nf_{S}(s)=\n\\begin{cases}\n0, & s<0,\\\\\ns, & 0 \\leq s \\leq 1,\\\\\n2-s, & 1 \\leq s \\leq 2,\\\\\n0, & s>2.\n\\end{cases}\n$$\nThis is a triangular density that increases linearly on $[0,1]$ and decreases linearly on $[1,2]$, which matches statement A. Statements B, C, D, and E do not match this derived form.", "answer": "$$\\boxed{A}$$", "id": "1956529"}, {"introduction": "The sample variance, $S^2$, is a cornerstone of statistical inference, widely used as an 'unbiased' estimator for the population variance $\\sigma^2$. But what does unbiasedness imply about the shape of its sampling distribution? This thought-provoking exercise [@problem_id:1953206] pushes you to look beyond the mean and consider the median, revealing a subtle but crucial insight about the skewed nature of the sampling distribution of the variance for data from a normal distribution.", "problem": "A materials science lab is investigating the consistency of a new alloy's tensile strength. From extensive historical data on similar materials, it is well established that the tensile strength measurements for any given alloy batch follow a normal distribution. For this new alloy, the true population variance of the tensile strength is denoted by $\\sigma^2$.\n\nA researcher takes a random sample of $n$ specimens of the new alloy, where $n > 1$. They measure the tensile strength for each specimen, let's call the measurements $X_1, X_2, \\dots, X_n$. From this sample, they compute the sample variance using the standard unbiased estimator:\n$$S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2$$\nwhere $\\bar{X}$ is the sample mean.\n\nImagine this sampling and calculation process is repeated a very large number of times, each time with a new random sample of size $n$. This would generate a sampling distribution for the statistic $S^2$. Let $m_{S^2}$ represent the median of this sampling distribution. Which of the following statements correctly describes the relationship between the median of the sample variance distribution, $m_{S^2}$, and the true population variance, $\\sigma^2$?\n\nA. $m_{S^2} > \\sigma^2$\n\nB. $m_{S^2} < \\sigma^2$\n\nC. $m_{S^2} = \\sigma^2$\n\nD. The relationship depends on the specific value of the sample size $n$; for some $n$, $m_{S^2} < \\sigma^2$, while for others, $m_{S^2} > \\sigma^2$.\n\nE. The relationship cannot be determined without knowing the value of the population mean $\\mu$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. normal with mean $\\mu$ and variance $\\sigma^{2}$. The unbiased sample variance is $S^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}$. A fundamental result for normal samples is the chi-square representation\n$$\n\\frac{(n-1)S^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{\\nu}, \\quad \\nu=n-1.\n$$\nLet $Y=\\frac{(n-1)S^{2}}{\\sigma^{2}}$. Then $Y \\sim \\chi^{2}_{\\nu}$ and $S^{2}=\\frac{\\sigma^{2}}{n-1}Y$. Denote the median of $S^{2}$ by $m_{S^{2}}$ and the median of $\\chi^{2}_{\\nu}$ by $m_{\\chi^{2}_{\\nu}}$. For a positive constant $a>0$, if $Z=aY$ then the cumulative distribution functions satisfy $F_{Z}(t)=\\Pr(Z \\le t)=\\Pr(Y \\le t/a)=F_{Y}(t/a)$, hence the medians scale linearly: $F_{Z}(a\\,m_{Y})=F_{Y}(m_{Y})=\\frac{1}{2}$, so $m_{Z}=a\\,m_{Y}$. Applying this with $a=\\frac{\\sigma^{2}}{n-1}$ gives\n$$\nm_{S^{2}}=\\frac{\\sigma^{2}}{n-1}\\,m_{\\chi^{2}_{\\nu}}.\n$$\nIt remains to compare $m_{\\chi^{2}_{\\nu}}$ and $\\nu$. The chi-square distribution is a Gamma distribution with shape $\\alpha=\\frac{\\nu}{2}$ and scale $\\theta=2$, so its mean is $\\nu$ and it is positively skewed for all $\\nu>0$. A known inequality for the median of the chi-square (equivalently, Gamma) distribution is that $m_{\\chi^{2}_{\\nu}}<\\nu$ for all $\\nu>0$. For instance, the Wilsonâ€“Hilferty formula gives the approximation $m_{\\chi^{2}_{\\nu}}\\approx \\nu\\left(1-\\frac{2}{9\\nu}\\right)^{3}$, which is strictly less than $\\nu$ for every $\\nu>0$, and sharper rigorous bounds also establish $m_{\\chi^{2}_{\\nu}}<\\nu$. Therefore,\n$$\nm_{S^{2}}=\\frac{\\sigma^{2}}{n-1}\\,m_{\\chi^{2}_{\\nu}}<\\frac{\\sigma^{2}}{n-1}\\,\\nu=\\sigma^{2}.\n$$\nThis strict inequality holds for every $n>1$ and does not depend on $\\mu$. Hence the correct choice is that the median of the sampling distribution of $S^{2}$ is less than $\\sigma^{2}$.", "answer": "$$\\boxed{B}$$", "id": "1953206"}]}