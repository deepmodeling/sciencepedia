{"hands_on_practices": [{"introduction": "The cornerstone result for inference on a population variance is that the statistic $\\frac{(n-1)S^2}{\\sigma^2}$ follows a chi-squared distribution. To build deep intuition for this fact, this exercise guides you through the simplest possible case: a sample of size $n=2$ [@problem_id:1953272]. By directly deriving the distribution from the properties of the normal distribution, you will see how the chi-squared distribution naturally arises and why it has one degree of freedom in this specific scenario.", "id": "1953272", "problem": "In a quality control process for manufacturing high-precision gyroscopes, a sensor takes two independent measurements, $X_1$ and $X_2$, of a component's angular drift rate. Each measurement is modeled as a random variable drawn from a normal distribution with mean $\\mu$ (the true drift rate) and a known variance $\\sigma^2$. The sample variance for this pair of measurements is calculated as $S^2 = \\sum_{i=1}^{2} (X_i - \\bar{X})^2$, where $\\bar{X}$ is the sample mean of the two measurements.\n\nA batch of components is subjected to a secondary review if the observed sample variance is excessively large compared to the inherent process variance. Calculate the exact probability that the sample variance $S^2$ is more than four times the true population variance $\\sigma^2$.\n\nYour final answer should be an analytic expression. You may use the standard normal Cumulative Distribution Function (CDF), denoted by $\\Phi(z) = P(Z \\le z)$ for a standard normal random variable $Z \\sim N(0,1)$, in your expression.\n\n", "solution": "We are given two independent measurements $X_{1}$ and $X_{2}$ with $X_{i} \\sim N(\\mu,\\sigma^{2})$, and the statistic $S^{2} = \\sum_{i=1}^{2} (X_{i} - \\bar{X})^{2}$, where $\\bar{X} = (X_{1}+X_{2})/2$. For $n=2$, compute $S^{2}$ explicitly:\n$$\nX_{1} - \\bar{X} = \\frac{X_{1} - X_{2}}{2}, \\quad X_{2} - \\bar{X} = \\frac{X_{2} - X_{1}}{2} = -\\frac{X_{1} - X_{2}}{2}.\n$$\nTherefore,\n$$\nS^{2} = \\left(\\frac{X_{1} - X_{2}}{2}\\right)^{2} + \\left(-\\frac{X_{1} - X_{2}}{2}\\right)^{2} = \\frac{(X_{1} - X_{2})^{2}}{2}.\n$$\nDefine $D = X_{1} - X_{2}$. By independence and normality, the difference of independent normal variables is normal with mean equal to the difference of means and variance equal to the sum of variances, so\n$$\nD \\sim N(0, 2\\sigma^{2}).\n$$\nThus,\n$$\n\\frac{S^{2}}{\\sigma^{2}} = \\frac{D^{2}/2}{\\sigma^{2}} = \\left(\\frac{D}{\\sqrt{2}\\,\\sigma}\\right)^{2}.\n$$\nLet $Z = D/(\\sqrt{2}\\,\\sigma) \\sim N(0,1)$, then $S^{2}/\\sigma^{2} = Z^{2}$. We seek\n$$\n\\mathbb{P}\\!\\left(S^{2} > 4\\sigma^{2}\\right) = \\mathbb{P}\\!\\left(\\frac{S^{2}}{\\sigma^{2}} > 4\\right) = \\mathbb{P}\\!\\left(Z^{2} > 4\\right) = \\mathbb{P}(|Z| > 2).\n$$\nFor a standard normal $Z$, $\\mathbb{P}(|Z| > 2) = 2\\left(1 - \\Phi(2)\\right)$, where $\\Phi$ is the standard normal CDF. Hence the exact probability is $2\\left(1 - \\Phi(2)\\right)$.", "answer": "$$\\boxed{2\\left(1-\\Phi(2)\\right)}$$"}, {"introduction": "Having built a foundational understanding, we can now apply the general theorem to a practical problem in industrial quality control. This practice asks you to determine a critical threshold for sample variance to monitor the consistency of a manufacturing process [@problem_id:1953229]. By solving this, you will practice using the chi-squared distribution to find quantiles and translate statistical theory into actionable operational guidelines.", "id": "1953229", "problem": "A manufacturing plant produces high-precision spherical ball bearings. The process is designed such that the diameters of the ball bearings are normally distributed with a population variance of $\\sigma^2 = 0.0250$ millimeters squared (mm$^2$). To ensure the consistency of the manufacturing process, a quality control team periodically takes a random sample of bearings and measures their diameters.\n\nA sample of $n=20$ ball bearings is selected. The sample variance is calculated using the formula $S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2$, where $X_i$ is the diameter of the $i$-th bearing and $\\bar{X}$ is the mean diameter of the sample.\n\nThe team has a protocol that flags the process for review if the sample variance is unusually high. Determine the threshold value $k$ for the sample variance such that there is a 90% probability that $S^2$ will be less than $k$. Express your answer in mm$^2$, rounded to three significant figures. You may need to use a chi-squared distribution table, with the relevant value for an upper-tail probability of 0.10 with 19 degrees of freedom being $\\chi^2_{0.10, 19} = 27.204$.\n\n", "solution": "Given a normally distributed population with variance $\\sigma^{2}$, and a sample of size $n$ from this population, the statistic\n$$\n\\frac{(n-1)S^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n-1}.\n$$\nWe seek $k$ such that $\\Pr(S^{2} < k) = 0.90$. Using the monotonicity of the transformation,\n$$\n\\Pr\\!\\left(S^{2} < k\\right) = \\Pr\\!\\left(\\frac{(n-1)S^{2}}{\\sigma^{2}} < \\frac{(n-1)k}{\\sigma^{2}}\\right) = 0.90.\n$$\nLet $q$ denote the $0.90$ quantile of $\\chi^{2}_{n-1}$. By the problemâ€™s convention, $\\chi^{2}_{0.10,19} = 27.204$ is the point with upper-tail probability $0.10$, so\n$$\n\\Pr\\!\\left(\\chi^{2}_{19} < 27.204\\right) = 0.90,\n$$\nhence $q = 27.204$ for $n-1=19$. Therefore,\n$$\n\\frac{(n-1)k}{\\sigma^{2}} = 27.204 \\quad \\Longrightarrow \\quad k = \\frac{\\sigma^{2}\\cdot 27.204}{n-1}.\n$$\nSubstituting $\\sigma^{2} = 0.0250$ and $n-1 = 19$,\n$$\nk = \\frac{0.0250 \\times 27.204}{19} = \\frac{27.204}{760} \\approx 0.035794736842\\ \\text{mm}^{2}.\n$$\nRounding to three significant figures gives\n$$\nk \\approx 0.0358\\ \\text{mm}^{2}.\n$$", "answer": "$$\\boxed{0.0358}$$"}, {"introduction": "We often have multiple ways to estimate the same parameter, which leads to a critical question: which estimator is \"best\"? This exercise delves into this question by comparing the unbiased sample variance, $S^2$, with the Maximum Likelihood Estimator, $\\hat{\\sigma}^2$, using the Mean Squared Error ($\\text{MSE}$) as a criterion [@problem_id:1953254]. This comparative analysis will illuminate the fundamental bias-variance tradeoff, a central concept in estimation theory and machine learning.", "id": "1953254", "problem": "In statistical estimation, the quality of an estimator is often evaluated using its Mean Squared Error (MSE). Consider a random sample $X_1, X_2, \\ldots, X_n$ of size $n \\ge 2$ drawn from a normal distribution with an unknown mean $\\mu$ and an unknown positive variance $\\sigma^2$.\n\nTwo common estimators for the population variance $\\sigma^2$ are:\n1.  The Maximum Likelihood Estimator (MLE), given by $\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2$, where $\\bar{X}$ is the sample mean. This estimator is known to be biased.\n2.  The unbiased sample variance, given by $S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2$.\n\nThe MSE of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\text{MSE}(\\hat{\\theta}) = \\text{E}[(\\hat{\\theta} - \\theta)^2]$.\n\nFor which integer values of the sample size $n$ is the MSE of the biased MLE, $\\text{MSE}(\\hat{\\sigma}^2)$, strictly smaller than the MSE of the unbiased estimator, $\\text{MSE}(S^2)$?\n\nA. For all $n \\ge 2$\nB. For no values of $n \\ge 2$\nC. Only for $n=2$\nD. Only for $n > 21$\nE. For $2 \\le n \\le 20$\n\n", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\mathcal{N}(\\mu,\\sigma^{2})$ with $n \\ge 2$. Define the unbiased sample variance $S^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}$ and the MLE $\\hat{\\sigma}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}$. These satisfy the relation\n$$\n\\hat{\\sigma}^{2}=\\frac{n-1}{n}S^{2}.\n$$\n\nA standard result for normal samples is\n$$\n\\frac{(n-1)S^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n-1}.\n$$\nHence $\\mathbb{E}[S^{2}]=\\sigma^{2}$ and, using $\\operatorname{Var}(\\chi^{2}_{\\nu})=2\\nu$, we get\n$$\n\\operatorname{Var}(S^{2})=\\frac{\\sigma^{4}}{(n-1)^{2}}\\operatorname{Var}\\!\\left(\\chi^{2}_{n-1}\\right)=\\frac{\\sigma^{4}}{(n-1)^{2}}\\cdot 2(n-1)=\\frac{2\\sigma^{4}}{n-1}.\n$$\nTherefore the MSE of the unbiased estimator is\n$$\n\\operatorname{MSE}(S^{2})=\\operatorname{Var}(S^{2})=\\frac{2\\sigma^{4}}{n-1}.\n$$\n\nFor the MLE, using $\\hat{\\sigma}^{2}=\\frac{n-1}{n}S^{2}$ and the above distribution, we also have\n$$\n\\frac{n\\hat{\\sigma}^{2}}{\\sigma^{2}}=\\frac{(n-1)S^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n-1}.\n$$\nThus\n$$\n\\mathbb{E}[\\hat{\\sigma}^{2}]=\\sigma^{2}\\cdot \\frac{n-1}{n}, \\quad \\operatorname{Bias}(\\hat{\\sigma}^{2})=\\mathbb{E}[\\hat{\\sigma}^{2}]-\\sigma^{2}=-\\frac{\\sigma^{2}}{n},\n$$\nand\n$$\n\\operatorname{Var}(\\hat{\\sigma}^{2})=\\frac{\\sigma^{4}}{n^{2}}\\operatorname{Var}\\!\\left(\\chi^{2}_{n-1}\\right)=\\frac{\\sigma^{4}}{n^{2}}\\cdot 2(n-1)=\\frac{2\\sigma^{4}(n-1)}{n^{2}}.\n$$\nTherefore\n$$\n\\operatorname{MSE}(\\hat{\\sigma}^{2})=\\operatorname{Var}(\\hat{\\sigma}^{2})+\\operatorname{Bias}(\\hat{\\sigma}^{2})^{2}=\\frac{2\\sigma^{4}(n-1)}{n^{2}}+\\left(\\frac{\\sigma^{2}}{n}\\right)^{2}=\\sigma^{4}\\cdot \\frac{2n-1}{n^{2}}.\n$$\n\nTo compare the MSEs, we require\n$$\n\\operatorname{MSE}(\\hat{\\sigma}^{2})<\\operatorname{MSE}(S^{2}) \\quad \\Longleftrightarrow \\quad \\frac{2n-1}{n^{2}}<\\frac{2}{n-1}.\n$$\nSince $n \\ge 2$, denominators are positive; cross-multiplying yields\n$$\n(2n-1)(n-1)<2n^{2} \\;\\Longleftrightarrow\\; 2n^{2}-3n+1<2n^{2} \\;\\Longleftrightarrow\\; -3n+1<0 \\;\\Longleftrightarrow\\; n>\\frac{1}{3}.\n$$\nThis holds for all integers $n \\ge 2$. Hence $\\operatorname{MSE}(\\hat{\\sigma}^{2})$ is strictly smaller than $\\operatorname{MSE}(S^{2})$ for all $n \\ge 2$.", "answer": "$$\\boxed{A}$$"}]}