{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with the most fundamental of cases: a discrete distribution. By examining a sample from a Bernoulli distribution, we can build clear intuition about the sample range without the complexities of continuous variables. This exercise [@problem_id:1914575] guides you in deriving the complete probability mass function for the range, reinforcing the core definition and its dependence on the underlying probability of success, $p$.", "problem": "Consider a sequence of $n$ independent trials, where $n \\ge 2$ is an integer. Each trial has only two outcomes: \"success,\" which is numerically coded as 1, or \"failure,\" which is coded as 0. The probability of success in any single trial is $p$, with the constraint that $0 < p < 1$. Let the random variables $X_1, X_2, \\ldots, X_n$ represent the outcomes of these $n$ trials, forming a random sample from a Bernoulli distribution with parameter $p$.\n\nThe sample range, denoted by $R$, is defined as the difference between the maximum and minimum values observed in the sample:\n$$R = \\max(X_1, X_2, \\ldots, X_n) - \\min(X_1, \\ldots, X_n)$$\n\nWhich of the following represents the correct Probability Mass Function (PMF), denoted as $f_R(r) = P(R=r)$, for the sample range $R$?\n\nA. $f_R(r) = \\begin{cases} p^n + (1-p)^n & \\text{if } r=0 \\\\ 1 - p^n - (1-p)^n & \\text{if } r=1 \\\\ 0 & \\text{otherwise} \\end{cases}$\n\nB. $f_R(r) = \\begin{cases} (1-p)^n & \\text{if } r=0 \\\\ 1 - (1-p)^n & \\text{if } r=1 \\\\ 0 & \\text{otherwise} \\end{cases}$\n\nC. $f_R(r) = \\begin{cases} p^n(1-p)^n & \\text{if } r=0 \\\\ 1 - p^n(1-p)^n & \\text{if } r=1 \\\\ 0 & \\text{otherwise} \\end{cases}$\n\nD. $f_R(r) = \\begin{cases} np(1-p)^{n-1} & \\text{if } r=0 \\\\ 1 - np(1-p)^{n-1} & \\text{if } r=1 \\\\ 0 & \\text{otherwise} \\end{cases}$\n\nE. $f_R(r) = \\begin{cases} p(1-p) & \\text{if } r=0 \\\\ 1 - p(1-p) & \\text{if } r=1 \\\\ 0 & \\text{otherwise} \\end{cases}$", "solution": "Each $X_{i}$ takes values in $\\{0,1\\}$. Therefore the sample minimum and maximum are each in $\\{0,1\\}$, so the range $R=\\max(X_{1},\\ldots,X_{n})-\\min(X_{1},\\ldots,X_{n})$ can take only values $0$ or $1$.\n\nWe have $R=0$ if and only if all observed values are equal. Since the only possible common values are $0$ or $1$, this occurs if and only if either all $X_{i}=0$ or all $X_{i}=1$. By independence,\n$$\nP(R=0)=P(X_{1}=\\cdots=X_{n}=0)+P(X_{1}=\\cdots=X_{n}=1)=(1-p)^{n}+p^{n}.\n$$\nConsequently,\n$$\nP(R=1)=1-P(R=0)=1-p^{n}-(1-p)^{n}.\n$$\nThus the PMF is\n$$\nf_{R}(r)=\\begin{cases}\np^{n}+(1-p)^{n} & \\text{if } r=0,\\\\\n1-p^{n}-(1-p)^{n} & \\text{if } r=1,\\\\\n0 & \\text{otherwise},\n\\end{cases}\n$$\nwhich matches option A.", "answer": "$$\\boxed{A}$$", "id": "1914575"}, {"introduction": "Having explored the discrete case, we now turn to the classic example of a continuous uniform distribution, often a starting point for modeling in the absence of other information. This practice [@problem_id:1914582] focuses on a key property of any statistic: its expected value. You will discover how the average sample range behaves as the sample size $n$ changes, a foundational concept in understanding sample variability.", "problem": "In a quality control test for a new type of digital noise generator, the device's output is normalized such that it produces random values that are continuously and uniformly distributed between 0 and 1. To analyze the output's variability, a sample of $n$ independent values, denoted by $X_1, X_2, \\ldots, X_n$, is collected from the generator. The sample range, $R$, is defined as the difference between the maximum and minimum values observed in the sample, i.e., $R = \\max(X_1, \\ldots, X_n) - \\min(X_1, \\ldots, X_n)$.\n\nDetermine the expected value of the sample range, $E[R]$, as a function of the sample size $n$. Assume $n \\ge 2$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\operatorname{Uniform}(0,1)$, and let $X_{(1)}=\\min(X_{1},\\ldots,X_{n})$ and $X_{(n)}=\\max(X_{1},\\ldots,X_{n})$. The sample range is $R=X_{(n)}-X_{(1)}$. By linearity of expectation,\n$$\nE[R]=E[X_{(n)}]-E[X_{(1)}].\n$$\nWe compute these expectations using the distributions of the extreme order statistics. For $x\\in[0,1]$,\n$$\nF_{X_{(n)}}(x)=P(X_{(n)}\\leq x)=P(X_{1}\\leq x,\\ldots,X_{n}\\leq x)=x^{n},\n$$\nso the density is $f_{X_{(n)}}(x)=n x^{n-1}$. Hence,\n$$\nE[X_{(n)}]=\\int_{0}^{1} x \\, f_{X_{(n)}}(x)\\,dx=\\int_{0}^{1} x \\, n x^{n-1}\\,dx=n\\int_{0}^{1} x^{n}\\,dx=\\frac{n}{n+1}.\n$$\nFor the minimum, for $x\\in[0,1]$,\n$$\nF_{X_{(1)}}(x)=P(X_{(1)}\\leq x)=1-P(X_{1}>x,\\ldots,X_{n}>x)=1-(1-x)^{n},\n$$\nso the density is $f_{X_{(1)}}(x)=n(1-x)^{n-1}$. Hence,\n$$\nE[X_{(1)}]=\\int_{0}^{1} x \\, f_{X_{(1)}}(x)\\,dx=n\\int_{0}^{1} x(1-x)^{n-1}\\,dx.\n$$\nUsing the substitution $u=1-x$, we obtain\n$$\nE[X_{(1)}]=n\\int_{0}^{1} (1-u)u^{n-1}\\,du=n\\left(\\int_{0}^{1} u^{n-1}\\,du-\\int_{0}^{1} u^{n}\\,du\\right)=n\\left(\\frac{1}{n}-\\frac{1}{n+1}\\right)=\\frac{1}{n+1}.\n$$\nTherefore,\n$$\nE[R]=E[X_{(n)}]-E[X_{(1)}]=\\frac{n}{n+1}-\\frac{1}{n+1}=\\frac{n-1}{n+1}.\n$$", "answer": "$$\\boxed{\\frac{n-1}{n+1}}$$", "id": "1914582"}, {"introduction": "A complete understanding of a statistic requires knowing not only its average value but also its spread, or variance. This final practice [@problem_id:1914615] deepens our analysis of the sample range from a uniform distribution by tasking you with calculating its variance. To do so, you will need to work with the joint distribution of the minimum and maximum order statistics, a powerful technique essential for more advanced statistical inference.", "problem": "Consider a random sample of five independent observations, denoted as $X_1, X_2, X_3, X_4, X_5$, drawn from a continuous standard uniform distribution, $U(0,1)$. The sample range, $R$, is defined as the difference between the maximum and minimum values observed in this sample. Determine the exact variance of the sample range, $\\text{Var}(R)$.\n\nYour final answer should be a single, simplified fraction.", "solution": "Let the random sample be $X_1, X_2, X_3, X_4, X_5$ from $U(0,1)$. The size of the sample is $n=5$. The Probability Density Function (PDF) for this distribution is $f(x)=1$ for $0 < x < 1$, and its Cumulative Distribution Function (CDF) is $F(x)=x$ for $0 < x < 1$.\n\nLet the order statistics of this sample be $Y_1 \\le Y_2 \\le Y_3 \\le Y_4 \\le Y_5$. By definition, $Y_1 = \\min(X_1, ..., X_5)$ and $Y_5 = \\max(X_1, ..., X_5)$. The sample range is $R = Y_5 - Y_1$.\n\nTo find the variance of $R$, we use the formula $\\text{Var}(R) = E[R^2] - (E[R])^2$. This requires us to first find the expected value of $R$ and the expected value of $R^2$. We can compute these expectations using the joint PDF of the minimum and maximum order statistics, $f_{Y_1, Y_5}(y_1, y_5)$.\n\nThe general formula for the joint PDF of the first and $n$-th order statistics from a distribution with PDF $f(x)$ and CDF $F(x)$ is:\n$$f_{Y_1, Y_n}(y_1, y_n) = n(n-1)[F(y_n) - F(y_1)]^{n-2} f(y_1) f(y_n)$$\nfor the domain where the order statistics are defined.\n\nIn our case, $n=5$, $f(x)=1$, and $F(x)=x$. The domain is $0 < y_1 < y_5 < 1$. Substituting these into the general formula:\n$$f_{Y_1, Y_5}(y_1, y_5) = 5(5-1)[y_5 - y_1]^{5-2} (1)(1) = 20(y_5 - y_1)^3$$\nfor $0 < y_1 < y_5 < 1$, and $f_{Y_1, Y_5}(y_1, y_5) = 0$ otherwise.\n\nNow, we can compute the expected value of the range, $E[R] = E[Y_5 - Y_1]$.\n$$E[R] = \\int_0^1 \\int_0^{y_5} (y_5 - y_1) f_{Y_1, Y_5}(y_1, y_5) \\, dy_1 \\, dy_5$$\n$$E[R] = \\int_0^1 \\int_0^{y_5} (y_5 - y_1) [20(y_5 - y_1)^3] \\, dy_1 \\, dy_5$$\n$$E[R] = 20 \\int_0^1 \\int_0^{y_5} (y_5 - y_1)^4 \\, dy_1 \\, dy_5$$\nLet's evaluate the inner integral with respect to $y_1$ by making a substitution $u = y_5 - y_1$, which implies $du = -dy_1$. The limits of integration for $u$ become $y_5$ (when $y_1=0$) and $0$ (when $y_1=y_5$).\n$$\\int_0^{y_5} (y_5 - y_1)^4 \\, dy_1 = \\int_{y_5}^0 u^4 (-du) = \\int_0^{y_5} u^4 \\, du = \\left[ \\frac{u^5}{5} \\right]_0^{y_5} = \\frac{y_5^5}{5}$$\nSubstituting this back into the expression for $E[R]$:\n$$E[R] = 20 \\int_0^1 \\frac{y_5^5}{5} \\, dy_5 = 4 \\int_0^1 y_5^5 \\, dy_5 = 4 \\left[ \\frac{y_5^6}{6} \\right]_0^1 = 4 \\left(\\frac{1}{6}\\right) = \\frac{4}{6} = \\frac{2}{3}$$\n\nNext, we compute the second moment of the range, $E[R^2] = E[(Y_5 - Y_1)^2]$.\n$$E[R^2] = \\int_0^1 \\int_0^{y_5} (y_5 - y_1)^2 f_{Y_1, Y_5}(y_1, y_5) \\, dy_1 \\, dy_5$$\n$$E[R^2] = \\int_0^1 \\int_0^{y_5} (y_5 - y_1)^2 [20(y_5 - y_1)^3] \\, dy_1 \\, dy_5$$\n$$E[R^2] = 20 \\int_0^1 \\int_0^{y_5} (y_5 - y_1)^5 \\, dy_1 \\, dy_5$$\nUsing the same substitution $u = y_5 - y_1$, the inner integral becomes:\n$$\\int_0^{y_5} (y_5 - y_1)^5 \\, dy_1 = \\int_0^{y_5} u^5 \\, du = \\left[ \\frac{u^6}{6} \\right]_0^{y_5} = \\frac{y_5^6}{6}$$\nSubstituting this back into the expression for $E[R^2]$:\n$$E[R^2] = 20 \\int_0^1 \\frac{y_5^6}{6} \\, dy_5 = \\frac{20}{6} \\int_0^1 y_5^6 \\, dy_5 = \\frac{10}{3} \\left[ \\frac{y_5^7}{7} \\right]_0^1 = \\frac{10}{3} \\left(\\frac{1}{7}\\right) = \\frac{10}{21}$$\n\nFinally, we calculate the variance of $R$:\n$$\\text{Var}(R) = E[R^2] - (E[R])^2 = \\frac{10}{21} - \\left(\\frac{2}{3}\\right)^2$$\n$$\\text{Var}(R) = \\frac{10}{21} - \\frac{4}{9}$$\nTo subtract these fractions, we find a common denominator, which is $63$.\n$$\\text{Var}(R) = \\frac{10 \\times 3}{21 \\times 3} - \\frac{4 \\times 7}{9 \\times 7} = \\frac{30}{63} - \\frac{28}{63} = \\frac{2}{63}$$", "answer": "$$\\boxed{\\frac{2}{63}}$$", "id": "1914615"}]}