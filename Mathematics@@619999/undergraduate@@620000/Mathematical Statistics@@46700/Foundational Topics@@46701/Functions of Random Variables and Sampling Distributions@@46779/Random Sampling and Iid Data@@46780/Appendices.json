{"hands_on_practices": [{"introduction": "A cornerstone of statistical inference is the assumption that our data forms an independent and identically distributed (i.i.d.) random sample. This assumption is powerful because it allows us to derive the properties of summary statistics, such as their expected value and variance. This practice problem [@problem_id:1949474] provides a concrete example by asking you to calculate the variance of a statistic representing the average power of a signal, a common task in engineering and physics, thereby demonstrating how the i.i.d. property simplifies the analysis of an estimator's variability.", "problem": "In the field of Digital Signal Processing (DSP), a common task is to characterize a noisy signal. Consider a sequence of $n$ voltage measurements, $V_1, V_2, \\ldots, V_n$. This sequence is modeled as a random sample where each measurement $V_i$ is drawn from a normal distribution with a constant but unknown mean DC offset $\\mu$ and a constant but unknown noise variance $\\sigma^2$. The measurements are independent and identically distributed (i.i.d.).\n\nAn engineer proposes a simple statistic, $T$, to quantify the average power of the received signal over the sampling period. This statistic is defined as the mean of the squared voltage values:\n$$T = \\frac{1}{n} \\sum_{i=1}^{n} V_i^2$$\nTo assess the reliability of this statistic, it is crucial to understand its variability. Determine the variance of the statistic $T$. Your answer should be a single closed-form analytic expression in terms of the sample size $n$, the mean DC offset $\\mu$, and the noise variance $\\sigma^2$.", "solution": "We define $Y_{i} = V_{i}^{2}$ so that $T = \\frac{1}{n} \\sum_{i=1}^{n} Y_{i}$. Because the $V_{i}$ are i.i.d., the $Y_{i}$ are also i.i.d. Therefore, using variance properties for independent sums,\n$$\n\\operatorname{Var}(T) = \\operatorname{Var}\\!\\left(\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\right) = \\frac{1}{n^{2}} \\sum_{i=1}^{n} \\operatorname{Var}(Y_{i}) = \\frac{1}{n} \\operatorname{Var}(Y_{1}) = \\frac{1}{n} \\operatorname{Var}(V^{2}),\n$$\nwhere $V \\sim \\mathcal{N}(\\mu,\\sigma^{2})$.\n\nWe compute $\\operatorname{Var}(V^{2})$ via moments. First,\n$$\n\\mathbb{E}[V^{2}] = \\operatorname{Var}(V) + (\\mathbb{E}[V])^{2} = \\sigma^{2} + \\mu^{2}.\n$$\nTo find $\\mathbb{E}[V^{4}]$, write $V = \\mu + Z$ with $Z \\sim \\mathcal{N}(0,\\sigma^{2})$. Then\n$$\nV^{4} = (\\mu + Z)^{4} = \\mu^{4} + 4 \\mu^{3} Z + 6 \\mu^{2} Z^{2} + 4 \\mu Z^{3} + Z^{4}.\n$$\nTaking expectations and using $\\mathbb{E}[Z] = 0$, $\\mathbb{E}[Z^{3}] = 0$, $\\mathbb{E}[Z^{2}] = \\sigma^{2}$, and for a zero-mean normal $\\mathbb{E}[Z^{4}] = 3 \\sigma^{4}$, we obtain\n$$\n\\mathbb{E}[V^{4}] = \\mu^{4} + 6 \\mu^{2} \\sigma^{2} + 3 \\sigma^{4}.\n$$\nThus,\n$$\n\\operatorname{Var}(V^{2}) = \\mathbb{E}[V^{4}] - \\big(\\mathbb{E}[V^{2}]\\big)^{2} = \\left(\\mu^{4} + 6 \\mu^{2} \\sigma^{2} + 3 \\sigma^{4}\\right) - \\left(\\mu^{2} + \\sigma^{2}\\right)^{2} = 4 \\mu^{2} \\sigma^{2} + 2 \\sigma^{4}.\n$$\nFinally,\n$$\n\\operatorname{Var}(T) = \\frac{1}{n} \\operatorname{Var}(V^{2}) = \\frac{1}{n} \\left(4 \\mu^{2} \\sigma^{2} + 2 \\sigma^{4}\\right) = \\frac{2 \\sigma^{2} \\left(2 \\mu^{2} + \\sigma^{2}\\right)}{n}.\n$$", "answer": "$$\\boxed{\\frac{2 \\sigma^{2} \\left(2 \\mu^{2} + \\sigma^{2}\\right)}{n}}$$", "id": "1949474"}, {"introduction": "While the i.i.d. assumption simplifies many calculations, it does not guarantee that our intuitive estimators are perfect. A crucial property of any estimator is its biasâ€”the difference between its expected value and the true parameter value. This exercise [@problem_id:1949420] challenges you to explore this concept by calculating the bias of a common but \"naive\" estimator for population covariance, revealing a fundamental insight that leads to the well-known Bessel's correction in statistics.", "problem": "In statistical analysis, it is common to estimate population parameters from a sample of data. Consider a set of $n$ bivariate data points, $(X_1, Y_1), (X_2, Y_2), \\dots, (X_n, Y_n)$, which form a random sample from a population. This sample is considered Independent and Identically Distributed (IID), meaning each pair $(X_i, Y_i)$ is drawn from the same underlying bivariate probability distribution and is independent of all other pairs.\n\nLet the population parameters be as follows:\n- Population mean of $X$: $E[X_i] = \\mu_X$\n- Population mean of $Y$: $E[Y_i] = \\mu_Y$\n- Population variance of $X$: $\\text{Var}(X_i) = \\sigma_X^2$\n- Population variance of $Y$: $\\text{Var}(Y_i) = \\sigma_Y^2$\n- Population covariance of $X$ and $Y$: $\\text{Cov}(X_i, Y_i) = E[(X_i - \\mu_X)(Y_i - \\mu_Y)] = \\sigma_{xy}$\n\nA \"naive\" estimator for the population covariance, analogous to a maximum likelihood estimator under normality, is defined as:\n$$ \\hat{\\sigma}_{xy} = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) $$\nwhere $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ and $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i$ are the sample means.\n\nThe bias of an estimator is the difference between its expected value and the true value of the parameter being estimated. For this estimator, the bias is given by $B(\\hat{\\sigma}_{xy}) = E[\\hat{\\sigma}_{xy}] - \\sigma_{xy}$.\n\nDetermine the bias of the naive sample covariance estimator, $\\hat{\\sigma}_{xy}$. Express your answer as a closed-form analytic expression in terms of the population covariance $\\sigma_{xy}$ and the sample size $n$.", "solution": "We start from the definition of the naive sample covariance estimator:\n$$\n\\hat{\\sigma}_{xy}=\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y}),\n$$\nwith $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ and $\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}$.\n\nFirst, use the algebraic identity for centered cross-products:\n$$\n\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})\n=\\sum_{i=1}^{n}X_{i}Y_{i}-\\bar{Y}\\sum_{i=1}^{n}X_{i}-\\bar{X}\\sum_{i=1}^{n}Y_{i}+n\\bar{X}\\bar{Y}.\n$$\nSince $\\sum_{i=1}^{n}X_{i}=n\\bar{X}$ and $\\sum_{i=1}^{n}Y_{i}=n\\bar{Y}$, this simplifies to\n$$\n\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})=\\sum_{i=1}^{n}X_{i}Y_{i}-n\\bar{X}\\bar{Y}.\n$$\nTherefore,\n$$\nE[\\hat{\\sigma}_{xy}]=\\frac{1}{n}\\sum_{i=1}^{n}E[X_{i}Y_{i}]-E[\\bar{X}\\bar{Y}].\n$$\n\nCompute each expectation:\n1) By the definition of covariance, $\\text{Cov}(X_{i},Y_{i})=E[X_{i}Y_{i}]-\\mu_{X}\\mu_{Y}=\\sigma_{xy}$, hence\n$$\nE[X_{i}Y_{i}]=\\sigma_{xy}+\\mu_{X}\\mu_{Y}.\n$$\nThus,\n$$\n\\frac{1}{n}\\sum_{i=1}^{n}E[X_{i}Y_{i}]=\\sigma_{xy}+\\mu_{X}\\mu_{Y}.\n$$\n\n2) For $E[\\bar{X}\\bar{Y}]$, write\n$$\n\\bar{X}\\bar{Y}=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}X_{i}Y_{j}.\n$$\nBecause the pairs $(X_{i},Y_{i})$ are IID and independent across $i$, we have for $i\\neq j$: $X_{i}$ is independent of $Y_{j}$, so $E[X_{i}Y_{j}]=E[X_{i}]E[Y_{j}]=\\mu_{X}\\mu_{Y}$. For $i=j$, $E[X_{i}Y_{i}]=\\sigma_{xy}+\\mu_{X}\\mu_{Y}$. Therefore,\n$$\nE[\\bar{X}\\bar{Y}]=\\frac{1}{n^{2}}\\left[n(\\sigma_{xy}+\\mu_{X}\\mu_{Y})+n(n-1)\\mu_{X}\\mu_{Y}\\right]\n=\\frac{\\sigma_{xy}}{n}+\\mu_{X}\\mu_{Y}.\n$$\n\nSubstituting back,\n$$\nE[\\hat{\\sigma}_{xy}]=\\left(\\sigma_{xy}+\\mu_{X}\\mu_{Y}\\right)-\\left(\\frac{\\sigma_{xy}}{n}+\\mu_{X}\\mu_{Y}\\right)\n=\\left(1-\\frac{1}{n}\\right)\\sigma_{xy}=\\frac{n-1}{n}\\,\\sigma_{xy}.\n$$\n\nHence the bias\n$$\nB(\\hat{\\sigma}_{xy})=E[\\hat{\\sigma}_{xy}]-\\sigma_{xy}=\\frac{n-1}{n}\\sigma_{xy}-\\sigma_{xy}=-\\frac{1}{n}\\sigma_{xy}.\n$$", "answer": "$$\\boxed{-\\frac{\\sigma_{xy}}{n}}$$", "id": "1949420"}, {"introduction": "The \"independent\" and \"identically distributed\" conditions are distinct, and it is vital to understand when they might not hold. A classic scenario where independence is violated is sampling without replacement from a finite population, a common practice in quality control and surveys. This problem [@problem_id:1949462] asks you to analyze the relationship between the first and second draws in such a scenario, demonstrating that the observations are no longer independent and, in fact, possess a non-zero covariance.", "problem": "A manufacturer produces a limited batch of $N$ advanced microprocessors. For quality control, a key performance metric is measured for each chip. Let the value of this metric for the $i$-th chip be $c_i$, for $i=1, \\ldots, N$. The set of all $N$ metric values, $\\{c_1, \\ldots, c_N\\}$, constitutes a finite population. The population mean of this metric is $\\mu$, and the population variance is $\\sigma^2$, defined as $\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (c_i - \\mu)^2$.\n\nAn inspector performs a simple random sample of size two, drawing two distinct chips from the batch without replacement. Let the random variable $X_1$ represent the metric value of the first chip selected, and $X_2$ represent the metric value of the second chip selected.\n\nDetermine the covariance between these two measurements, $\\text{Cov}(X_1, X_2)$. Express your answer as a closed-form analytic expression in terms of the population size $N$ and the population variance $\\sigma^2$.", "solution": "Let the finite population be the values $\\{c_{1},\\ldots,c_{N}\\}$ with population mean $\\mu$ and variance $\\sigma^{2}=\\frac{1}{N}\\sum_{i=1}^{N}(c_{i}-\\mu)^{2}$. The inspector draws two distinct chips without replacement, and $X_{1}$ and $X_{2}$ denote the metric values observed on the first and second draws, respectively.\n\nFirst, compute the expectations:\n$$\n\\mathbb{E}[X_{1}]=\\sum_{i=1}^{N}c_{i}\\cdot\\frac{1}{N}=\\frac{1}{N}\\sum_{i=1}^{N}c_{i}=\\mu,\n\\qquad\n\\mathbb{E}[X_{2}]=\\mu,\n$$\nby symmetry and uniform sampling.\n\nNext, compute $\\mathbb{E}[X_{1}X_{2}]$. Because the sampling is without replacement, for $i\\neq j$,\n$$\n\\mathbb{P}(X_{1}=c_{i},X_{2}=c_{j})=\\frac{1}{N}\\cdot\\frac{1}{N-1}.\n$$\nHence,\n$$\n\\mathbb{E}[X_{1}X_{2}]\n=\\sum_{i=1}^{N}\\sum_{\\substack{j=1\\\\ j\\neq i}}^{N} c_{i}c_{j}\\cdot\\frac{1}{N}\\cdot\\frac{1}{N-1}\n=\\frac{1}{N(N-1)}\\sum_{i\\neq j}c_{i}c_{j}.\n$$\nUse the identity\n$$\n\\sum_{i\\neq j}c_{i}c_{j}=\\left(\\sum_{i=1}^{N}c_{i}\\right)^{2}-\\sum_{i=1}^{N}c_{i}^{2}=(N\\mu)^{2}-\\sum_{i=1}^{N}c_{i}^{2}.\n$$\nFrom the definition of $\\sigma^{2}$,\n$$\n\\sigma^{2}=\\frac{1}{N}\\sum_{i=1}^{N}(c_{i}-\\mu)^{2}\n=\\frac{1}{N}\\left(\\sum_{i=1}^{N}c_{i}^{2}-2\\mu\\sum_{i=1}^{N}c_{i}+N\\mu^{2}\\right)\n=\\frac{1}{N}\\left(\\sum_{i=1}^{N}c_{i}^{2}-N\\mu^{2}\\right),\n$$\nso\n$$\n\\sum_{i=1}^{N}c_{i}^{2}=N(\\sigma^{2}+\\mu^{2}).\n$$\nTherefore,\n$$\n\\sum_{i\\neq j}c_{i}c_{j}=N^{2}\\mu^{2}-N(\\sigma^{2}+\\mu^{2})=N(N-1)\\mu^{2}-N\\sigma^{2}.\n$$\nSubstituting into $\\mathbb{E}[X_{1}X_{2}]$ gives\n$$\n\\mathbb{E}[X_{1}X_{2}]\n=\\frac{1}{N(N-1)}\\left(N(N-1)\\mu^{2}-N\\sigma^{2}\\right)\n=\\mu^{2}-\\frac{\\sigma^{2}}{N-1}.\n$$\nFinally, the covariance is\n$$\n\\operatorname{Cov}(X_{1},X_{2})=\\mathbb{E}[X_{1}X_{2}]-\\mathbb{E}[X_{1}]\\mathbb{E}[X_{2}]\n=\\left(\\mu^{2}-\\frac{\\sigma^{2}}{N-1}\\right)-\\mu^{2}\n=-\\frac{\\sigma^{2}}{N-1}.\n$$", "answer": "$$\\boxed{-\\frac{\\sigma^{2}}{N-1}}$$", "id": "1949462"}]}