{"hands_on_practices": [{"introduction": "The Strong Law of Large Numbers (SLLN) is a cornerstone of probability theory, providing the theoretical justification for why long-term averages tend to stabilize. This first practice provides a foundational exercise in applying the SLLN directly. By calculating the almost sure limit of a sample mean for a given probability distribution, you will reinforce the core concept that empirical averages converge to the theoretical expected value, a principle that underpins much of statistical inference and simulation [@problem_id:1460774].", "problem": "A research team is studying the output of a novel signal processor. Each time the processor runs, it generates a normalized value, which can be modeled as a random variable. A sequence of these values, $X_1, X_2, X_3, \\dots$, is recorded. These values are considered to be independent and identically distributed (i.i.d.) random variables. The theoretical model for the probability distribution of any single value $X_i$ is described by the probability density function (PDF):\n$$\nf(x) = \\begin{cases} 3x^2 & \\text{for } 0 \\le x \\le 1 \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe long-term average of these measurements after $n$ trials is given by the sample mean $S_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. As the number of trials $n$ grows infinitely large, the value of $S_n$ will converge almost surely to a specific constant.\n\nDetermine the value of this constant. Express your answer as a fraction in simplest form.", "solution": "We are given independent and identically distributed random variables $X_{1},X_{2},\\dots$ with common density\n$$\nf(x)=\\begin{cases}\n3x^{2}, & 0\\le x\\le 1,\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\nFirst, verify that $f$ is a valid probability density by checking normalization:\n$$\n\\int_{-\\infty}^{\\infty} f(x)\\,dx=\\int_{0}^{1}3x^{2}\\,dx=3\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}=1.\n$$\nBy the Strong Law of Large Numbers, since the $X_{i}$ are i.i.d. with finite mean $E[|X_{1}|]<\\infty$ (which holds because $0\\le X_{1}\\le 1$ almost surely), the sample mean\n$$\nS_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\n$$\nconverges almost surely to $E[X_{1}]$ as $n\\to\\infty$.\n\nCompute the expectation:\n$$\nE[X_{1}]=\\int_{-\\infty}^{\\infty} x f(x)\\,dx=\\int_{0}^{1} x\\cdot 3x^{2}\\,dx=3\\int_{0}^{1} x^{3}\\,dx=3\\left[\\frac{x^{4}}{4}\\right]_{0}^{1}=\\frac{3}{4}.\n$$\nTherefore,\n$$\n\\lim_{n\\to\\infty} S_{n}=\\frac{3}{4}\\quad \\text{almost surely}.\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1460774"}, {"introduction": "Moving from abstract distributions to a more tangible, geometric context allows us to see the SLLN in action. This problem explores the average distance of points randomly scattered in a disk from its center. To solve it, you must first derive the probability distribution of the distance itself before applying the SLLN, a common task in real-world modeling. This exercise is a beautiful illustration of how the SLLN serves as the engine for Monte Carlo methods, enabling us to estimate complex quantities through repeated random sampling [@problem_id:1957055].", "problem": "Consider a sequence of points, $P_1, P_2, P_3, \\dots$, that are chosen independently from the same distribution, formally known as being independent and identically distributed (i.i.d.). The points are selected from a uniform distribution over the unit disk, defined as the set of all points $(x, y)$ in a Cartesian plane satisfying the inequality $x^2 + y^2 \\le 1$.\n\nFor each point $P_n$ in the sequence, let $D_n$ be its Euclidean distance from the center of the disk, the origin $(0,0)$. We are interested in the long-term behavior of the average of these distances. Let $\\bar{D}_N$ be the arithmetic mean of the first $N$ distances, that is, $\\bar{D}_N = \\frac{1}{N} \\sum_{n=1}^{N} D_n$.\n\nDetermine the value to which the average distance $\\bar{D}_N$ converges almost surely (i.e., with probability 1) as the number of points $N$ approaches infinity. Present your answer as a single closed-form analytic expression.", "solution": "Let $\\{P_{n}\\}_{n\\geq 1}$ be i.i.d. uniformly distributed on the unit disk $\\{(x,y): x^{2}+y^{2}\\leq 1\\}$, and let $D_{n}$ be the Euclidean distance from $P_{n}$ to the origin. The strong law of large numbers states that for i.i.d. random variables with finite mean,\n$$\n\\bar{D}_{N}=\\frac{1}{N}\\sum_{n=1}^{N}D_{n}\\xrightarrow[\\;N\\to\\infty\\;]{\\text{a.s.}}\\mathbb{E}[D_{1}],\n$$\nprovided $\\mathbb{E}[|D_{1}|]<\\infty$. Since $0\\leq D_{1}\\leq 1$, we have $\\mathbb{E}[|D_{1}|]<\\infty$. Therefore, it suffices to compute $\\mathbb{E}[D_{1}]$.\n\nLet $R$ denote the distance from the origin for a point uniformly distributed on the unit disk. Then\n$$\n\\mathbb{P}(R\\leq r)=\\frac{\\text{area of the disk of radius }r}{\\text{area of the unit disk}}=\\frac{\\pi r^{2}}{\\pi}=r^{2},\\quad 0\\leq r\\leq 1.\n$$\nDifferentiating gives the radial density\n$$\nf_{R}(r)=\\frac{d}{dr}\\big(r^{2}\\big)=2r,\\quad 0\\leq r\\leq 1.\n$$\nHence,\n$$\n\\mathbb{E}[R]=\\int_{0}^{1}r\\,f_{R}(r)\\,dr=\\int_{0}^{1}r\\cdot 2r\\,dr=\\int_{0}^{1}2r^{2}\\,dr=\\left.\\frac{2}{3}r^{3}\\right|_{0}^{1}=\\frac{2}{3}.\n$$\nBy the strong law of large numbers,\n$$\n\\bar{D}_{N}\\xrightarrow[\\;N\\to\\infty\\;]{\\text{a.s.}}\\mathbb{E}[D_{1}]=\\frac{2}{3}.\n$$\nThus the almost sure limit of the average distance is $\\frac{2}{3}$.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "1957055"}, {"introduction": "A deeper understanding of any scientific law comes from testing its boundaries. This final practice challenges a key assumption of the classic SLLN: the independence of the random variables. Here, we analyze a sequence where each term depends on the previous one, a structure known as a 1-dependent sequence. By learning how to decompose the problem and apply the SLLN to independent subsequences, you will gain insight into the robustness and extensibility of the law, a crucial step toward tackling more advanced problems in stochastic processes [@problem_id:1460782].", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $\\{X_n\\}_{n=1}^{\\infty}$ defined on a common probability space. These variables are characterized by a mean of $E[X_n] = 0$ and a second moment of $E[X_n^2] = 1$ for all $n \\ge 1$.\n\nWe form a new sequence of sample averages defined as $A_n = \\frac{1}{n} \\sum_{i=1}^{n-1} X_i X_{i+1}$ for $n \\ge 2$.\n\nDetermine the value to which the sequence $A_n$ converges almost surely as $n$ approaches infinity.", "solution": "Define $Y_i = X_iX_{i+1}$ for $i \\ge 1$. Then $A_n = \\frac{1}{n} \\sum_{i=1}^{n-1} Y_i$ for $n \\ge 2$. By independence of the $X_i$ and identical distribution, each $Y_i$ has the same distribution, with\n$$\nE[Y_i] = E[X_iX_{i+1}] = E[X_i]\\,E[X_{i+1}] = 0,\n$$\nand\n$$\nE[Y_i^2] = E[X_i^2X_{i+1}^2] = E[X_i^2]\\,E[X_{i+1}^2] = 1.\n$$\nHence $E[|Y_i|] \\le \\sqrt{E[Y_i^2]} = 1$ by Cauchyâ€“Schwarz, so $Y_i$ are integrable.\n\nNext, observe the dependence structure: the family $\\{Y_i\\}_{i \\ge 1}$ is $1$-dependent, and more specifically the subsequences $\\{Y_{2j-1}\\}_{j \\ge 1}$ and $\\{Y_{2j}\\}_{j \\ge 1}$ consist of independent random variables because they depend on disjoint sets of the independent $X_i$. Moreover, each subsequence is identically distributed with mean $0$ and finite second moment. By the Kolmogorov strong law of large numbers applied separately to each subsequence,\n$$\n\\frac{1}{k} \\sum_{j=1}^{k} Y_{2j-1} \\to 0 \\quad \\text{a.s.}, \n\\qquad\n\\frac{1}{k} \\sum_{j=1}^{k} Y_{2j} \\to 0 \\quad \\text{a.s.}\n$$\n\nFor each $n \\ge 2$, let $N_o(n)$ be the number of odd indices in $\\{1,\\dots,n-1\\}$ and $N_e(n)$ the number of even indices in $\\{1,\\dots,n-1\\}$. Then $N_o(n) + N_e(n) = n-1$, with $N_o(n) = \\lceil (n-1)/2 \\rceil$ and $N_e(n) = \\lfloor (n-1)/2 \\rfloor$, so $N_o(n)/n \\to \\frac{1}{2}$ and $N_e(n)/n \\to \\frac{1}{2}$ deterministically as $n \\to \\infty$. Decompose\n$$\nA_n\n= \\frac{1}{n} \\sum_{i=1}^{n-1} Y_i\n= \\frac{N_o(n)}{n} \\left( \\frac{1}{N_o(n)} \\sum_{j=1}^{N_o(n)} Y_{2j-1} \\right)\n+ \\frac{N_e(n)}{n} \\left( \\frac{1}{N_e(n)} \\sum_{j=1}^{N_e(n)} Y_{2j} \\right).\n$$\nAs $n \\to \\infty$, we have $N_o(n) \\to \\infty$ and $N_e(n) \\to \\infty$, so by the almost sure limits of the subsequence averages and the deterministic limits of the weights, it follows that $A_n \\to 0$ almost surely.\n\nTherefore, the sequence $A_n$ converges almost surely to $0$.", "answer": "$$\\boxed{0}$$", "id": "1460782"}]}