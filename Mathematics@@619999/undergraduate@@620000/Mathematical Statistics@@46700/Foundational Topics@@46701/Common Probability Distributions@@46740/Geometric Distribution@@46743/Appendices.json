{"hands_on_practices": [{"introduction": "Understanding a probabilistic model often means being able to work in reverse. Instead of just calculating probabilities from a known parameter, we frequently need to deduce the parameter itself from experimental observations. This first practice problem is a fundamental exercise in this skill, where we will use a cumulative probability—the chance of an event happening by a certain time—to solve for the underlying success probability $p$ of a geometric process. This type of reverse-reasoning is a common task in quality control and process characterization [@problem_id:1920080].", "problem": "A materials science lab is testing a new procedure for synthesizing high-purity graphene flakes. The outcome of each synthesis attempt is a binary event: either the flake meets the required purity standard (a 'success') or it does not (a 'failure'). Each attempt is an independent trial with the same constant probability of success, denoted by $p$.\n\nLet the random variable $X$ represent the number of synthesis attempts required to produce the first successful high-purity graphene flake. After conducting a large number of experimental runs, the lab analysts determined that the probability of achieving the first success on or before the second attempt is exactly $0.51$.\n\nBased on this experimental finding, determine the exact value of the success probability $p$.", "solution": "Let $X$ be a geometric random variable with success probability $p$, representing the number of trials until the first success. The probability of achieving the first success on or before the second attempt is\n$$\n\\mathbb{P}(X \\leq 2) = \\mathbb{P}(X=1) + \\mathbb{P}(X=2).\n$$\nUsing the geometric distribution,\n$$\n\\mathbb{P}(X=1) = p, \\quad \\mathbb{P}(X=2) = (1-p)p.\n$$\nTherefore,\n$$\np + (1-p)p = 0.51.\n$$\nSimplify:\n$$\np + p - p^{2} = 0.51 \\quad \\Rightarrow \\quad 2p - p^{2} = 0.51.\n$$\nRewriting as a quadratic equation,\n$$\np^{2} - 2p + 0.51 = 0.\n$$\nMultiply through by $100$ to clear denominators:\n$$\n100p^{2} - 200p + 51 = 0.\n$$\nSolve using the quadratic formula:\n$$\np = \\frac{200 \\pm \\sqrt{(-200)^{2} - 4 \\cdot 100 \\cdot 51}}{2 \\cdot 100} = \\frac{200 \\pm \\sqrt{40000 - 20400}}{200} = \\frac{200 \\pm \\sqrt{19600}}{200} = \\frac{200 \\pm 140}{200}.\n$$\nThus,\n$$\np = 1 \\pm 0.7.\n$$\nThe two solutions are $p = 1.7$ and $p = 0.3$. Since a probability must satisfy $0 \\leq p \\leq 1$, the valid solution is\n$$\np = 0.3.\n$$", "answer": "$$\\boxed{0.3}$$", "id": "1920080"}, {"introduction": "Moving beyond single-event probabilities, we can explore more complex properties of the geometric distribution. This exercise challenges you to calculate the probability that the first success occurs on any odd-numbered trial. Solving this requires command of the geometric series and demonstrates how to sum probabilities over an infinite set of outcomes, a technique essential for analyzing the long-term behavior of systems like communication networks and queuing processes [@problem_id:1920121].", "problem": "In a simplified model of a wireless communication channel, a data packet is transmitted repeatedly from a source to a receiver until it is successfully acknowledged. Due to noise and interference, each transmission is an independent trial with a constant probability of success, denoted by $p$, where $0  p  1$. Let the random variable $X$ represent the total number of transmissions required to achieve the first successful reception.\n\nAssuming the transmissions continue indefinitely until success, determine the probability that the first successful transmission occurs on an odd-numbered trial (i.e., the 1st, 3rd, 5th, and so on). Express your answer as a closed-form analytic expression in terms of $p$.", "solution": "Each transmission is an independent Bernoulli trial with success probability $p$, so the number of transmissions $X$ until the first success is geometrically distributed with\n$$\n\\Pr(X=n)=(1-p)^{n-1}p \\quad \\text{for } n\\in\\{1,2,3,\\dots\\}.\n$$\nThe event that the first success occurs on an odd-numbered trial is $\\{X=1,3,5,\\dots\\}$. Its probability is the sum over odd $n$:\n$$\n\\Pr(\\text{odd})=\\sum_{k=1}^{\\infty}\\Pr(X=2k-1)=\\sum_{k=1}^{\\infty}(1-p)^{2k-2}p.\n$$\nReindexing with $j=k-1$ gives a geometric series:\n$$\n\\Pr(\\text{odd})=p\\sum_{j=0}^{\\infty}\\left((1-p)^{2}\\right)^{j}.\n$$\nSince $0p1$, we have $0(1-p)^{2}1$, so the series converges and\n$$\n\\sum_{j=0}^{\\infty}r^{j}=\\frac{1}{1-r}\\quad \\text{with } r=(1-p)^{2}.\n$$\nTherefore,\n$$\n\\Pr(\\text{odd})=\\frac{p}{1-(1-p)^{2}}=\\frac{p}{1-(1-2p+p^{2})}=\\frac{p}{2p-p^{2}}=\\frac{1}{2-p}.\n$$", "answer": "$$\\boxed{\\frac{1}{2-p}}$$", "id": "1920121"}, {"introduction": "A key goal when applying probability distributions to real-world problems is to estimate their parameters from collected data. This capstone exercise introduces you to one of the most powerful and fundamental techniques in statistics: Maximum Likelihood Estimation (MLE). By working through this problem, you will derive the estimator for the success probability $p$ of a geometric distribution based on a sample of observations. This exercise bridges the gap from probability theory to statistical inference, providing you with a cornerstone method for data analysis used across science and engineering [@problem_id:1399040].", "problem": "A cybersecurity firm is analyzing the effectiveness of a new social engineering attack vector. The firm models the process as a series of independent attempts, each with an unknown but constant probability of success, $p$. The number of attempts required to achieve the first successful breach is a random variable $X$. The firm assumes that $X$ follows a geometric distribution with the probability mass function given by $P(X=k) = (1-p)^{k-1}p$ for $k = 1, 2, 3, \\ldots$.\n\nTo estimate the parameter $p$, the firm conducts $n$ independent penetration tests against identical, isolated systems. The number of attempts required for the first success in each test is recorded, yielding a sample of observations $x_1, x_2, \\ldots, x_n$.\n\nYour task is to derive the maximum likelihood estimator, $\\hat{p}$, for the success probability $p$. Express your final answer as a closed-form analytic expression in terms of the observations $x_1, x_2, \\ldots, x_n$ and the sample size $n$.", "solution": "The goal is to find the value of $p$ that maximizes the likelihood of observing the given sample $x_1, x_2, \\ldots, x_n$. This value is the maximum likelihood estimator (MLE), denoted as $\\hat{p}$.\n\nFirst, we construct the likelihood function, $L(p)$. Since the observations are independent and identically distributed (i.i.d.), the likelihood function is the product of the probabilities of each individual observation. The probability of a single observation $x_i$ is given by the probability mass function $P(X=x_i) = (1-p)^{x_i-1}p$.\n\nThe likelihood function is therefore:\n$$L(p) = \\prod_{i=1}^{n} P(X=x_i) = \\prod_{i=1}^{n} (1-p)^{x_i-1}p$$\n\nWe can simplify this expression by grouping the terms involving $p$ and $(1-p)$:\n$$L(p) = \\left( \\prod_{i=1}^{n} p \\right) \\left( \\prod_{i=1}^{n} (1-p)^{x_i-1} \\right)$$\n$$L(p) = p^n (1-p)^{\\sum_{i=1}^{n} (x_i-1)}$$\n$$L(p) = p^n (1-p)^{\\left(\\sum_{i=1}^{n} x_i\\right) - n}$$\n\nTo find the maximum of $L(p)$, it is often easier to work with its natural logarithm, the log-likelihood function, $\\ell(p) = \\ln(L(p))$. Since the logarithm is a monotonically increasing function, maximizing $\\ell(p)$ is equivalent to maximizing $L(p)$.\n\nThe log-likelihood function is:\n$$\\ell(p) = \\ln\\left(p^n (1-p)^{\\left(\\sum_{i=1}^{n} x_i\\right) - n}\\right)$$\nUsing the properties of logarithms, we can expand this to:\n$$\\ell(p) = \\ln(p^n) + \\ln\\left((1-p)^{\\left(\\sum_{i=1}^{n} x_i\\right) - n}\\right)$$\n$$\\ell(p) = n \\ln(p) + \\left(\\left(\\sum_{i=1}^{n} x_i\\right) - n\\right) \\ln(1-p)$$\n\nNext, we differentiate the log-likelihood function with respect to $p$ to find the critical points.\n$$\\frac{d\\ell}{dp} = \\frac{d}{dp} \\left[ n \\ln(p) + \\left(\\left(\\sum_{i=1}^{n} x_i\\right) - n\\right) \\ln(1-p) \\right]$$\n$$\\frac{d\\ell}{dp} = \\frac{n}{p} + \\left(\\left(\\sum_{i=1}^{n} x_i\\right) - n\\right) \\left(\\frac{-1}{1-p}\\right)$$\n$$\\frac{d\\ell}{dp} = \\frac{n}{p} - \\frac{\\sum_{i=1}^{n} x_i - n}{1-p}$$\n\nTo find the value of $p$ that maximizes $\\ell(p)$, we set the derivative equal to zero and solve for $p$.\n$$\\frac{n}{p} - \\frac{\\sum_{i=1}^{n} x_i - n}{1-p} = 0$$\n$$\\frac{n}{p} = \\frac{\\sum_{i=1}^{n} x_i - n}{1-p}$$\nCross-multiplying gives:\n$$n(1-p) = p\\left(\\sum_{i=1}^{n} x_i - n\\right)$$\n$$n - np = p\\left(\\sum_{i=1}^{n} x_i\\right) - np$$\nThe $-np$ terms on both sides cancel out:\n$$n = p\\left(\\sum_{i=1}^{n} x_i\\right)$$\nFinally, solving for $p$ gives the maximum likelihood estimator $\\hat{p}$:\n$$\\hat{p} = \\frac{n}{\\sum_{i=1}^{n} x_i}$$\nThis expression can also be written in terms of the sample mean $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$ as $\\hat{p} = 1/\\bar{x}$. The problem asks for the expression in terms of the individual observations and $n$.\n\nTo confirm this critical point is a maximum, we can use the second derivative test. The second derivative of the log-likelihood function is:\n$$\\frac{d^2\\ell}{dp^2} = \\frac{d}{dp} \\left( \\frac{n}{p} - \\frac{\\sum x_i - n}{1-p} \\right) = -\\frac{n}{p^2} - (\\sum x_i - n)\\frac{1}{(1-p)^2}$$\nFrom our first-derivative-equals-zero condition, we found $n(1-p) = p(\\sum x_i - n)$, so $\\sum x_i - n = \\frac{n(1-p)}{p}$. Substituting this into the second derivative:\n$$\\frac{d^2\\ell}{dp^2} = -\\frac{n}{p^2} - \\left(\\frac{n(1-p)}{p}\\right)\\frac{1}{(1-p)^2} = -\\frac{n}{p^2} - \\frac{n}{p(1-p)}$$\nSince $n  0$ and $p \\in (0, 1)$, both terms are negative. Therefore, $\\frac{d^2\\ell}{dp^2}  0$, which confirms that the value we found for $p$ corresponds to a local maximum of the likelihood function.", "answer": "$$\\boxed{\\frac{n}{\\sum_{i=1}^{n} x_i}}$$", "id": "1399040"}]}