{"hands_on_practices": [{"introduction": "The first step in mastering a new probability distribution is to become comfortable with its fundamental formula. This exercise focuses on a direct application of the negative binomial probability mass function (PMF). By calculating the probability of achieving a specific number of successes by a certain trial, you'll build the foundational skill needed for all subsequent analysis. [@problem_id:12863]", "problem": "Consider a sequence of independent Bernoulli trials, where each trial can result in one of two outcomes: 'success' or 'failure'. The probability of a success in any given trial is constant and denoted by $p$, while the probability of a failure is $1-p$.\n\nLet the random variable $X$ be the trial number on which the $r$-th success occurs. In this problem, we are interested in the specific case where we want to find the probability that the 3rd success occurs on the 10th trial.\n\nDerive the expression for the probability $P(X=10)$ for $r=3$ successes. Your final answer should be a closed-form expression in terms of the success probability $p$.", "solution": "We model the number of trials $X$ required to obtain $r$ successes in independent Bernoulli trials with success probability $p$ by the negative-binomial distribution.  The probability that the $r$-th success occurs on the $n$-th trial is\n$$\nP(X=n)\\;=\\;\\binom{n-1}{r-1}\\,p^r\\,(1-p)^{\\,n-r}\\,.\n$$\nFor $r=3$ and $n=10$, this gives\n$$\nP(X=10)\n=\\binom{10-1}{3-1}\\,p^3\\,(1-p)^{10-3}\n=\\binom{9}{2}\\,p^3\\,(1-p)^7.\n$$\nSince \n$$\n\\binom{9}{2}=\\frac{9\\cdot8}{2}=36,\n$$\nwe obtain\n$$\nP(X=10)=36\\,p^3\\,(1-p)^7.\n$$", "answer": "$$\\boxed{36\\,p^3\\,(1-p)^7}$$", "id": "12863"}, {"introduction": "In many scientific and engineering contexts, the parameters of a distribution are unknown and must be inferred from data. This practice problem demonstrates how to use sample statistics, like the mean and variance, to estimate the parameters of a negative binomial distribution. This powerful technique, related to the method of moments, provides a crucial bridge between theoretical probability and practical data analysis. [@problem_id:1939495]", "problem": "A team of data scientists is modeling user engagement on a new social media platform. They are interested in the number of posts a user makes until they achieve a certain number of \"successful\" posts, defined as posts that receive a high level of community interaction.\n\nThe team models this process using a negative binomial distribution. Let the random variable $X$ represent the number of \"unsuccessful\" posts a user creates before achieving a target of $r$ successful posts. The probability of any given post being successful is $p$, and each post is an independent trial.\n\nFrom a large dataset of user histories, the team has calculated the sample mean and variance for the number of unsuccessful posts. The estimated mean is $E[X] = 10$ and the estimated variance is $Var(X) = 20$.\n\nBased on these empirical estimates, determine the parameters of the negative binomial distribution that models this process: the target number of successful posts, $r$, and the probability of success for a single post, $p$. Your answer should be the ordered pair $(r, p)$.", "solution": "Let $X$ denote the number of failures before achieving $r$ successes in independent Bernoulli trials with success probability $p$. In this parameterization of the negative binomial distribution, $X$ can be written as the sum of $r$ independent geometric random variables (each counting failures before one success), each with mean $(1-p)/p$ and variance $(1-p)/p^2$. Therefore,\n$$\nE[X]=r\\frac{1-p}{p},\\qquad \\operatorname{Var}(X)=r\\frac{1-p}{p^{2}}.\n$$\nLet $\\mu=E[X]$ and $\\sigma^{2}=\\operatorname{Var}(X)$. Then\n$$\n\\frac{\\sigma^{2}}{\\mu}=\\frac{r(1-p)/p^{2}}{r(1-p)/p}=\\frac{1}{p}\\quad\\Longrightarrow\\quad p=\\frac{\\mu}{\\sigma^{2}}.\n$$\nNext, solve for $r$ from the mean:\n$$\n\\mu=r\\frac{1-p}{p}\\quad\\Longrightarrow\\quad r=\\frac{\\mu p}{1-p}.\n$$\nSubstitute $p=\\mu/\\sigma^{2}$:\n$$\nr=\\frac{\\mu\\left(\\frac{\\mu}{\\sigma^{2}}\\right)}{1-\\frac{\\mu}{\\sigma^{2}}}\n=\\frac{\\mu^{2}}{\\sigma^{2}-\\mu}.\n$$\nWith the empirical estimates $\\mu=10$ and $\\sigma^{2}=20$, we obtain\n$$\np=\\frac{10}{20}=\\frac{1}{2},\\qquad r=\\frac{10^{2}}{20-10}=\\frac{100}{10}=10.\n$$\nThus, the parameters are $(r,p)=(10,\\frac{1}{2})$.", "answer": "$$\\boxed{\\begin{pmatrix}10  \\frac{1}{2}\\end{pmatrix}}$$", "id": "1939495"}, {"introduction": "Moving beyond methods based on moments, Maximum Likelihood Estimation (MLE) offers a cornerstone principle for statistical inference. This exercise challenges you to derive the MLE for the success probability $p$ of a negative binomial process based on a set of experimental observations. Mastering this technique is essential for understanding how statistical models are formally constructed and validated in modern scientific research. [@problem_id:1321150]", "problem": "A cybersecurity research team is evaluating a new algorithm for detecting malicious data packets within a network stream. This algorithm constitutes an Intrusion Detection System (IDS). In a controlled environment, data packets are streamed to the detection system one by one. Each packet has a constant and independent probability $p$ of being malicious and correctly identified by the algorithm, which is defined as a \"success\". If a packet is not a success for any reason (i.e., it is benign or it is a malicious packet that was missed), it is considered a \"failure\".\n\nTo test the algorithm's performance, the team runs $n$ independent experiments. In each experiment $i$ (for $i=1, \\dots, n$), the team counts the number of failures, $X_i$, that occur before observing exactly $r$ successes. The value of $r$ is a predetermined positive integer and is the same for all experiments.\n\nGiven the observed data from these $n$ experiments, $x_1, x_2, \\dots, x_n$, find the maximum likelihood estimator, $\\hat{p}$, for the success probability $p$. Express your answer as a symbolic expression in terms of $r$, $n$, and the total number of observed failures, $S = \\sum_{i=1}^{n} x_i$.", "solution": "We model each experiment as follows. Let $X_{i}$ denote the number of failures before the $r$-th success in experiment $i$, with constant success probability $p$ per trial, independently across trials and experiments. Then $X_{i}$ follows a negative binomial distribution (counting failures before $r$ successes) with probability mass function\n$$\n\\Pr(X_{i}=x_{i}\\,|\\,p)=\\binom{x_{i}+r-1}{x_{i}}(1-p)^{x_{i}}p^{r},\\quad x_{i}=0,1,2,\\dots.\n$$\nGiven $n$ independent experiments with observations $x_{1},\\dots,x_{n}$, the likelihood function is\n$$\nL(p)=\\prod_{i=1}^{n}\\binom{x_{i}+r-1}{x_{i}}(1-p)^{x_{i}}p^{r}.\n$$\nLet $S=\\sum_{i=1}^{n}x_{i}$. The likelihood factors as\n$$\nL(p)=\\left[\\prod_{i=1}^{n}\\binom{x_{i}+r-1}{x_{i}}\\right](1-p)^{S}p^{nr}.\n$$\nSince the product of binomial coefficients does not depend on $p$, the log-likelihood is\n$$\n\\ell(p)=\\ln L(p)=\\text{constant}+S\\ln(1-p)+nr\\ln p.\n$$\nDifferentiate with respect to $p$ and set to zero to obtain the score equation:\n$$\n\\frac{d\\ell}{dp}=\\frac{nr}{p}-\\frac{S}{1-p}=0.\n$$\nSolving for $p$ yields\n$$\n\\frac{nr}{p}=\\frac{S}{1-p}\\quad\\Longrightarrow\\quad nr(1-p)=Sp\\quad\\Longrightarrow\\quad nr= p(nr+S)\\quad\\Longrightarrow\\quad \\hat{p}=\\frac{nr}{nr+S}.\n$$\nTo verify that this critical point is a maximum, compute the second derivative:\n$$\n\\frac{d^{2}\\ell}{dp^{2}}=-\\frac{nr}{p^{2}}-\\frac{S}{(1-p)^{2}}0\n$$\nfor $p\\in(0,1)$, which confirms a maximum. Therefore, the maximum likelihood estimator is\n$$\n\\hat{p}=\\frac{nr}{nr+S}.\n$$", "answer": "$$\\boxed{\\frac{nr}{nr+S}}$$", "id": "1321150"}]}