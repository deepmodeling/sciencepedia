{"hands_on_practices": [{"introduction": "The true power of the Bernoulli distribution is revealed when we consider multiple trials. This exercise serves as a fundamental building block by exploring the simplest case: combining two independent events. By calculating the probability of achieving exactly one success in two trials [@problem_id:682], you will practice applying the concepts of independence and mutually exclusive outcomes, which are essential for modeling the more general Binomial distribution.", "problem": "A discrete random variable $X$ is said to follow a Bernoulli distribution with parameter $p$, denoted as $X \\sim \\text{Bernoulli}(p)$, if it describes a single trial with two possible outcomes: success (value 1) or failure (value 0). The probability of success is $P(X=1) = p$, and the probability of failure is $P(X=0) = 1-p$, where $0 \\le p \\le 1$.\n\nConsider two random variables, $X_1$ and $X_2$, that are independent and identically distributed (i.i.d.) according to a Bernoulli distribution with parameter $p$.\n\nDerive an expression for the probability that the sum of these two variables is exactly equal to one. That is, find $P(X_1 + X_2 = 1)$.", "solution": "Let $X_1$ and $X_2$ be two independent and identically distributed random variables, both following a Bernoulli distribution with parameter $p$.\nThe probability mass function (PMF) for each variable is given by:\n$$\nP(X_i=1) = p\n$$\n$$\nP(X_i=0) = 1-p\n$$\nfor $i=1, 2$.\n\nWe want to find the probability that their sum is equal to one, $P(X_1 + X_2 = 1)$. Since $X_1$ and $X_2$ can only take values of 0 or 1, their sum can be 0, 1, or 2. The event $X_1 + X_2 = 1$ can occur in two mutually exclusive ways:\n1.  $X_1 = 1$ and $X_2 = 0$.\n2.  $X_1 = 0$ and $X_2 = 1$.\n\nThe probability of the event $X_1 + X_2 = 1$ is the sum of the probabilities of these two mutually exclusive outcomes.\n$$\nP(X_1 + X_2 = 1) = P( (X_1=1 \\text{ and } X_2=0) \\text{ or } (X_1=0 \\text{ and } X_2=1) )\n$$\nBecause the two cases are mutually exclusive, we can write this as:\n$$\nP(X_1 + X_2 = 1) = P(X_1=1, X_2=0) + P(X_1=0, X_2=1)\n$$\n\nSince $X_1$ and $X_2$ are independent, the joint probability of any combination of their outcomes is the product of their individual probabilities.\nFor the first case, $(X_1=1, X_2=0)$:\n$$\nP(X_1=1, X_2=0) = P(X_1=1) \\times P(X_2=0)\n$$\nSubstituting the probabilities from the Bernoulli distribution definition:\n$$\nP(X_1=1, X_2=0) = p \\times (1-p) = p(1-p)\n$$\n\nFor the second case, $(X_1=0, X_2=1)$:\n$$\nP(X_1=0, X_2=1) = P(X_1=0) \\times P(X_2=1)\n$$\nSubstituting the probabilities:\n$$\nP(X_1=0, X_2=1) = (1-p) \\times p = p(1-p)\n$$\n\nNow, we sum the probabilities of these two cases to find the total probability of $X_1 + X_2 = 1$:\n$$\nP(X_1 + X_2 = 1) = p(1-p) + p(1-p)\n$$\n$$\nP(X_1 + X_2 = 1) = 2p(1-p)\n$$", "answer": "$$\\boxed{2p(1-p)}$$", "id": "682"}, {"introduction": "A key aspect of a random event is its unpredictability, a concept we quantify using variance. This practice invites you to explore the relationship between the success probability $p$ and the variance of a Bernoulli trial [@problem_id:1899936]. By determining the value of $p$ that maximizes this variance, you will uncover the conditions under which a binary outcome is most uncertain, providing a crucial insight into the nature of randomness.", "problem": "A team of data scientists is designing a machine learning model to classify emails as either \"spam\" or \"not spam\". The outcome for a single email is modeled as a Bernoulli random variable, $X$, where $X=1$ corresponds to the email being classified as \"spam\" (a \"success\") and $X=0$ corresponds to \"not spam\". The probability that the model classifies a randomly selected email as spam is given by a parameter $p$, where $p$ is a real number in the range $0 \\le p \\le 1$.\n\nFor testing the robustness of their system, the team is interested in the scenario that represents the highest level of unpredictability in the model's classification output. In statistical terms, this corresponds to finding the value of the parameter $p$ that maximizes the variance of the random variable $X$.\n\nDetermine this value of $p$. Express your answer as a single decimal or a fraction.", "solution": "Let $X$ be a Bernoulli random variable with parameter $p$, so $P(X=1)=p$ and $P(X=0)=1-p$. The mean and second moment are\n$$\nE[X]=0\\cdot(1-p)+1\\cdot p=p, \\quad E[X^2]=0^2\\cdot(1-p)+1^2\\cdot p=p.\n$$\nThe variance is\n$$\n\\text{Var}(X)=E[X^2]-(E[X])^2=p-p^2=p(1-p).\n$$\nTo maximize $\\text{Var}(X)$ over $p\\in[0,1]$, define $v(p)=p(1-p)$. Then\n$$\nv'(p)=1-2p, \\quad v''(p)=-2.\n$$\nSetting $v'(p)=0$ gives $p=\\frac{1}{2}$. Since $v''(p) = -2 < 0$, this critical point is a local maximum. Checking the endpoints,\n$$\nv(0)=0, \\quad v(1)=0, \\quad v\\!\\left(\\frac{1}{2}\\right)=\\frac{1}{4},\n$$\nso the global maximum on $[0,1]$ occurs at $p=\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1899936"}, {"introduction": "How can we generate a perfectly fair outcome from a biased source, like a bent coin? This fascinating question lies at the heart of applications in computing and cryptography. This exercise introduces an elegant algorithm for extracting unbiased random bits from a sequence of biased Bernoulli trials [@problem_id:1392786]. Solving this problem will challenge you to synthesize your understanding of independence, conditional probability, and expected value to analyze a clever and powerful real-world procedure.", "problem": "An engineer is designing a subsystem for a deep-space probe that relies on a fundamentally noisy physical process to generate random bits. The raw output is a sequence of bits, $X_1, X_2, X_3, \\dots$, which can be modeled as a sequence of independent and identically distributed Bernoulli random variables. For each bit $X_i$, the probability of it being a '1' is an unknown but constant value $p$, and the probability of it being a '0' is $1-p$. The system operates under the constraint that $p$ is strictly between 0 and 1, i.e., $p \\in (0,1)$.\n\nTo produce a \"fair\" bit (i.e., a bit with a probability of 0.5 of being '1'), the engineer implements the following algorithm, often attributed to John von Neumann:\n1.  The raw bits are consumed in non-overlapping consecutive pairs: $(X_1, X_2), (X_3, X_4), (X_5, X_6), \\dots$.\n2.  Each pair is processed according to the following rules:\n    *   If the pair is $(0,1)$, the algorithm outputs a single bit '0' and halts.\n    *   If the pair is $(1,0)$, the algorithm outputs a single bit '1' and halts.\n    *   If the pair is $(0,0)$ or $(1,1)$, the pair is discarded, and the algorithm proceeds to the next pair to repeat the process.\n\nThis procedure is guaranteed to eventually produce one output bit. Let's call this first generated output bit $Y$.\n\n(a) Calculate the probability that the output bit is a '1', i.e., find $P(Y=1)$.\n(b) Calculate the expected number of raw bits from the source sequence ($X_i$) that must be consumed to produce the single output bit $Y$.\n\nProvide your answer as a row matrix containing the symbolic expressions for Part (a) and Part (b), in that order. Express your answer for Part (b) in terms of $p$.", "solution": "Let the raw bits be independent and identically distributed with $P(X_{i}=1)=p$ and $P(X_{i}=0)=1-p$, with $p\\in(0,1)$. Consider disjoint consecutive pairs $(X_{1},X_{2}), (X_{3},X_{4}), \\dots$. By independence and identical distribution of the $X_{i}$, these pairs are independent and identically distributed.\n\nFor a single pair, the probabilities of pair outcomes are:\n$$\nP((1,0))=p(1-p),\\quad P((0,1))=(1-p)p,\\quad P(\\text{equal})=P((0,0))+P((1,1))=p^{2}+(1-p)^{2}.\n$$\nDefine $q=P(\\text{unequal})=P((1,0))+P((0,1))=2p(1-p)$.\n\nThe algorithm halts at the first pair that is unequal. Let $K$ be the index of this first unequal pair. Then $K$ is a geometric random variable on $\\{1,2,\\dots\\}$ with success probability $q=2p(1-p)$:\n$$\nP(K=k)=\\left(p^{2}+(1-p)^{2}\\right)^{k-1}\\cdot 2p(1-p).\n$$\n\n(a) The algorithm outputs $Y=1$ exactly when the first unequal pair is $(1,0)$. Thus\n$$\nP(Y=1)=\\sum_{k=1}^{\\infty}\\left(p^{2}+(1-p)^{2}\\right)^{k-1}\\cdot p(1-p)\n= p(1-p)\\sum_{k=1}^{\\infty}\\left(p^{2}+(1-p)^{2}\\right)^{k-1}.\n$$\nThis is a geometric series with ratio $r=p^{2}+(1-p)^{2}$, so\n$$\n\\sum_{k=1}^{\\infty}r^{k-1}=\\frac{1}{1-r}=\\frac{1}{1-\\left(p^{2}+(1-p)^{2}\\right)}=\\frac{1}{2p(1-p)}.\n$$\nTherefore,\n$$\nP(Y=1)=p(1-p)\\cdot \\frac{1}{2p(1-p)}=\\frac{1}{2}.\n$$\n\n(b) Each processed pair consumes exactly $2$ raw bits. Since $K$ is a geometric random variable with success probability $q=2p(1-p)$, its expectation is $E[K]=\\frac{1}{q}=\\frac{1}{2p(1-p)}$. Hence the expected number of raw bits consumed is\n$$\nE[\\text{bits}]=2E[K]=\\frac{2}{2p(1-p)}=\\frac{1}{p(1-p)}.\n$$\nThus the required expressions are $P(Y=1)=\\frac{1}{2}$ and $E[\\text{bits}]=\\frac{1}{p(1-p)}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & \\frac{1}{p(1-p)}\\end{pmatrix}}$$", "id": "1392786"}]}