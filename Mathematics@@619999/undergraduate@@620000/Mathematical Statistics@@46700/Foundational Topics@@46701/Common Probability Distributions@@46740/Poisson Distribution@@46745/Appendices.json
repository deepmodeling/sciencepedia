{"hands_on_practices": [{"introduction": "In many natural and engineered systems, we observe events that arise from multiple independent sources. This exercise explores a fundamental property of the Poisson distribution: additivity. By understanding how to combine independent Poisson processes, you can model the aggregate behavior of complex systems, such as the total number of particles detected from several radioactive sources or customers arriving at a business from different entrances. [@problem_id:13665]", "problem": "Two independent radioactive sources, A and B, emit particles that are registered by a detector. The number of particles detected from source A in a time interval $T$ is a random variable $N_A$ that follows a Poisson distribution with rate parameter $\\lambda_A$. Similarly, the number of particles detected from source B in the same time interval is a random variable $N_B$ that follows a Poisson distribution with rate parameter $\\lambda_B$.\n\nThe probability mass function (PMF) for a Poisson random variable $K$ with rate parameter $\\lambda$ is given by:\n$$\nP(K=k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}\n$$\nA key property of the Poisson distribution is that its variance is equal to its rate parameter, i.e., $\\text{Var}(K) = \\lambda$.\n\nThe total number of particles detected in the time interval $T$ is the sum of the particles from both sources, $N_{total} = N_A + N_B$. Given that the processes of particle emission from A and B are independent, derive an expression for the standard deviation of the total number of particles detected, $\\sigma_{N_{total}}$, in terms of the rate parameters $\\lambda_A$ and $\\lambda_B$.", "solution": "The total count is \n$$N_{\\rm total}=N_A+N_B\\,. $$\nSince $N_A$ and $N_B$ are independent Poisson variables with \n$$\\Var(N_A)=\\lambda_A,\\quad \\Var(N_B)=\\lambda_B,$$ \nwe use the property that the variance of a sum of independent variables is the sum of their variances:\n$$\n\\Var(N_{\\rm total})\n=\\Var(N_A)+\\Var(N_B)\n=\\lambda_A+\\lambda_B\\,.\n$$\nThe standard deviation is the square root of the variance:\n$$\n\\sigma_{N_{\\rm total}}\n=\\sqrt{\\Var(N_{\\rm total})}\n=\\sqrt{\\lambda_A+\\lambda_B}\\,.\n$$", "answer": "$$\\boxed{\\sqrt{\\lambda_A+\\lambda_B}}$$", "id": "13665"}, {"introduction": "Often, after observing a stream of events, we need to classify them into distinct categories. This practice delves into the concept of Poisson \"thinning,\" an elegant principle that describes how a single Poisson process can be split into multiple independent subprocesses. Working through this scenario, involving classifying emails as spam or not, will illuminate the powerful relationship between the Poisson and Binomial distributions and provide a practical tool for analyzing categorized count data. [@problem_id:13691]", "problem": "An IT administrator observes that emails arrive at a server according to a Poisson process with an average rate of $\\lambda$ emails per hour. Each incoming email is independently classified as \"spam\" with a constant probability $p$, or \"not spam\" with probability $1-p$.\n\nLet $N$ be the random variable for the total number of emails arriving in one hour, which follows the Poisson probability mass function (PMF):\n$$P(N=n) = \\frac{\\lambda^n e^{-\\lambda}}{n!}$$\nfor $n=0, 1, 2, \\dots$.\n\nGiven that a total of $n$ emails have arrived, the number of spam emails, $K$, follows a binomial distribution. The probability of having exactly $k$ spam emails out of $n$ total emails is given by:\n$$P(K=k | N=n) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n\nUsing the law of total probability, which states $P(A) = \\sum_{i} P(A|B_i)P(B_i)$, derive a closed-form expression for the probability of receiving exactly one spam email in a single one-hour period. Your answer should be expressed in terms of $\\lambda$ and $p$.", "solution": "We seek $P(K=1)$, the probability of exactly one spam email in one hour.  By the law of total probability,\n$$\nP(K=1)\n=\\sum_{n=0}^{\\infty}P(K=1\\mid N=n)\\,P(N=n).\n$$\nSince $P(K=1\\mid N=n)=0$ for $n<1$, we start the sum at $n=1$:\n$$\nP(K=1)\n=\\sum_{n=1}^{\\infty}\\binom{n}{1}p^{1}(1-p)^{n-1}\\,\\frac{\\lambda^n e^{-\\lambda}}{n!}.\n$$\nNoting $\\binom{n}{1}=n$ and $n/n!=1/(n-1)!$, we have\n$$\nP(K=1)\n=p\\,e^{-\\lambda}\\sum_{n=1}^{\\infty}n\\,(1-p)^{\\,n-1}\\frac{\\lambda^n}{n!}\n=p\\,e^{-\\lambda}\\sum_{n=1}^{\\infty}(1-p)^{\\,n-1}\\frac{\\lambda^n}{(n-1)!}.\n$$\nChange index via $k=n-1$, so $n=k+1$ and the sum over $k=0$ to $\\infty$:\n$$\nP(K=1)\n=p\\,e^{-\\lambda}\\sum_{k=0}^{\\infty}(1-p)^{k}\\frac{\\lambda^{k+1}}{k!}\n=p\\,\\lambda\\,e^{-\\lambda}\\sum_{k=0}^{\\infty}\\frac{(\\lambda(1-p))^{k}}{k!}.\n$$\nRecognize the series as the exponential:\n$$\n\\sum_{k=0}^{\\infty}\\frac{(\\lambda(1-p))^k}{k!}\n=e^{\\lambda(1-p)}.\n$$\nThus\n$$\nP(K=1)\n=p\\,\\lambda\\,e^{-\\lambda}\\,e^{\\lambda(1-p)}\n=\\lambda p\\,e^{-\\lambda p}.\n$$\nEquivalently, since thinning a Poisson process yields $K\\sim\\mathrm{Poisson}(\\lambda p)$, one also obtains $P(K=1)=(\\lambda p)e^{-\\lambda p}$.", "answer": "$$\\boxed{\\lambda p e^{-\\lambda p}}$$", "id": "13691"}, {"introduction": "A key task in any scientific discipline is to connect theoretical models with observational data by estimating unknown parameters. This problem advances from probability theory into the practical realm of statistical inference. You will apply the powerful method of Maximum Likelihood Estimation (MLE) to a hierarchical model, where one Poisson process influences another, to discover how the properties of the distribution lead to a remarkably intuitive estimator for the model's parameters. [@problem_id:815247]", "problem": "Consider a hierarchical stochastic model describing a two-stage event process. In the first stage, the number of primary events, $X$, follows a Poisson distribution with an unknown rate parameter $\\lambda > 0$. In the second stage, conditional on the observation of $X=x$ primary events, the number of secondary events, $Y$, follows a Poisson distribution with a rate parameter of $\\theta x$, where $\\theta > 0$ is an unknown scaling parameter.\n\nThe probability mass functions for this model are given by:\n$$ P(X=x; \\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!}, \\quad \\text{for } x = 0, 1, 2, \\dots $$\n$$ P(Y=y | X=x; \\theta) = \\frac{e^{-\\theta x}(\\theta x)^y}{y!}, \\quad \\text{for } y = 0, 1, 2, \\dots $$\n\nFor the case where $x=0$, the rate of the conditional distribution for $Y$ is zero, which implies that $P(Y=0|X=0) = 1$ and $P(Y>0|X=0)=0$.\n\nYou are given a sample of $n$ independent and identically distributed observations of pairs $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$ generated from this process. It is assumed that the sample is not trivial, meaning at least one primary event was observed, i.e., $\\sum_{i=1}^n x_i > 0$.\n\nYour task is to derive the maximum likelihood estimator (MLE), denoted $\\hat{\\theta}$, for the parameter $\\theta$ in terms of the observed data $\\{ (x_i, y_i) \\}_{i=1}^n$.", "solution": "Relevant likelihood components:\n$$P(X=x_i;\\lambda)=\\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!},\\quad P(Y=y_i\\mid X=x_i;\\theta)=\\frac{e^{-\\theta x_i}(\\theta x_i)^{y_i}}{y_i!}.$$\nJoint likelihood (conditional on $\\{x_i\\}$):\n$$L(\\theta)=\\prod_{i=1}^nP(Y=y_i\\mid X=x_i;\\theta)\n=\\prod_{i=1}^n e^{-\\theta x_i}\\frac{(\\theta x_i)^{y_i}}{y_i!}.$$\nLog-likelihood:\n$$\\ell(\\theta)=\\sum_{i=1}^n\\Bigl(-\\theta x_i+y_i\\ln(\\theta x_i)-\\ln(y_i!)\\Bigr)\n=-\\theta\\sum_{i=1}^n x_i+\\sum_{i=1}^n y_i\\ln\\theta+\\sum_{i=1}^n y_i\\ln x_i-\\sum_{i=1}^n\\ln(y_i!).$$\nDifferentiate w.r.t.\\ $\\theta$ and set to zero:\n$$\\frac{d\\ell}{d\\theta}\n=-\\sum_{i=1}^n x_i+\\sum_{i=1}^n\\frac{y_i}{\\theta}=0\n\\;\\Longrightarrow\\;\n-\\sum_{i=1}^n x_i+\\frac{1}{\\theta}\\sum_{i=1}^n y_i=0.$$\nSolve for $\\theta$:\n$$\\hat\\theta=\\frac{\\sum_{i=1}^n y_i}{\\sum_{i=1}^n x_i}.$$", "answer": "$$\\boxed{\\frac{\\sum_{i=1}^n y_i}{\\sum_{i=1}^n x_i}}$$", "id": "815247"}]}