{"hands_on_practices": [{"introduction": "To begin, we will ground our understanding of the power function with a direct calculation. This exercise involves a quality control scenario where a test is based on a single observation from a uniform distribution. By determining the power of this test, you'll see firsthand how the probability of correctly detecting an 'out of control' process depends explicitly on the true value of the underlying parameter $\\theta$ [@problem_id:1963236].", "problem": "A quality control specialist is examining a manufacturing process that produces a certain component. The length of the component, denoted by the random variable $X$, is known to follow a uniform distribution, $U(0, \\theta)$, where $\\theta > 0$ is an unknown parameter representing the maximum possible length. The Probability Density Function (PDF) of $X$ is given by $f(x; \\theta) = 1/\\theta$ for $0 < x < \\theta$, and $f(x; \\theta) = 0$ otherwise.\n\nThe process is considered to be 'in control' if $\\theta = 1$. It is considered 'out of control' if $\\theta > 1$. The specialist decides to test the null hypothesis $H_0: \\theta = 1$ against the alternative hypothesis $H_1: \\theta > 1$. A single component is sampled, and its length $X$ is measured. The decision rule is to reject the null hypothesis $H_0$ if the measurement $X$ is greater than 0.95.\n\nThe power of this test, for a specific value of $\\theta$ under the alternative hypothesis, is the probability of correctly rejecting $H_0$ when that $\\theta$ is the true parameter.\n\nDetermine the exact value of the parameter $\\theta$ for which the power of this statistical test is exactly 0.5.", "solution": "Let $X \\sim U(0,\\theta)$ with density $f(x;\\theta)=\\frac{1}{\\theta}$ for $0<x<\\theta$. The cumulative distribution function for $0<x<\\theta$ is\n$$\nF(x;\\theta)=\\mathbb{P}_{\\theta}(X\\leq x)=\\int_{0}^{x}\\frac{1}{\\theta}\\,dt=\\frac{x}{\\theta}.\n$$\nThe test rejects $H_{0}$ when $X>0.95$, where $0.95=\\frac{19}{20}$. The power at parameter $\\theta$ is\n$$\n\\pi(\\theta)=\\mathbb{P}_{\\theta}(X>\\tfrac{19}{20})=\n\\begin{cases}\n0, & \\theta\\leq \\tfrac{19}{20} \\\\\n1-F(\\tfrac{19}{20};\\theta)=1-\\dfrac{\\tfrac{19}{20}}{\\theta}, & \\theta>\\tfrac{19}{20}.\n\\end{cases}\n$$\nUnder the alternative $H_{1}:\\theta>1$, we have $\\theta>\\tfrac{19}{20}$, so\n$$\n\\pi(\\theta)=1-\\frac{\\tfrac{19}{20}}{\\theta}.\n$$\nSet the power equal to $\\frac{1}{2}$ and solve for $\\theta$:\n$$\n1-\\frac{\\tfrac{19}{20}}{\\theta}=\\frac{1}{2}\n\\;\\Longrightarrow\\;\n\\frac{\\tfrac{19}{20}}{\\theta}=\\frac{1}{2}\n\\;\\Longrightarrow\\;\n\\frac{19}{20\\theta}=\\frac{1}{2}\n\\;\\Longrightarrow\\;\n19=10\\theta\n\\;\\Longrightarrow\\;\n\\theta=\\frac{19}{10}.\n$$\nThis value satisfies $\\theta>1$, hence it is admissible under $H_{1}$.", "answer": "$$\\boxed{\\frac{19}{10}}$$", "id": "1963236"}, {"introduction": "A key application of the power function is to evaluate and compare the effectiveness of different statistical tests. In this practice, you will analyze two vastly different procedures for testing the success probability $p$ of a Bernoulli trial: one based on experimental data and a 'foolish' one that operates randomly. Comparing their power functions reveals why one test is useful and the other is not, sharpening your intuition about what makes a statistical test valuable [@problem_id:1963239].", "problem": "An experiment is conducted to determine the probability of success, $p$, of a particular process. The process is modeled as a sequence of independent Bernoulli trials. A small sample of size $n=2$ is collected, with outcomes denoted by $X_1$ and $X_2$. The total number of successes in the sample is $S_2 = X_1 + X_2$.\n\nA researcher wants to test the null hypothesis $H_0: p = 1/3$. Two different testing procedures are considered.\n\n**Test A:** A standard test based on the collected data. The rule is to reject the null hypothesis $H_0$ if the observed number of successes $S_2$ is equal to 0 or 2.\n\n**Test B:** A \"foolish\" test that operates independently of the collected data. The rule is to reject the null hypothesis $H_0$ with a fixed probability of $\\gamma = 5/9$.\n\nThe power function of a test, denoted $\\pi(p)$, gives the probability of rejecting $H_0$ as a function of the true underlying parameter $p$. For what value of $p$ in the interval $(0, 1)$, other than $p=1/3$, is the power function of Test A equal to the power function of Test B? Present your answer as a fraction.", "solution": "We model $S_{2}=X_{1}+X_{2}$ as a binomial random variable with parameters $n=2$ and success probability $p$, i.e., $S_{2}\\sim \\text{Bin}(2,p)$. For Test A, the rejection region is $\\{S_{2}=0\\}\\cup\\{S_{2}=2\\}$. Using binomial probabilities,\n$$\n\\pi_{A}(p)=\\mathbb{P}(S_{2}=0)+\\mathbb{P}(S_{2}=2)=\\binom{2}{0}p^{0}(1-p)^{2}+\\binom{2}{2}p^{2}(1-p)^{0}=(1-p)^{2}+p^{2}.\n$$\nFor Test B, the test rejects independently of the data with fixed probability $\\gamma=\\frac{5}{9}$, so its power function is constant:\n$$\n\\pi_{B}(p)=\\frac{5}{9}\\quad \\text{for all }p\\in(0,1).\n$$\nWe seek $p\\in(0,1)$, $p\\neq \\frac{1}{3}$, such that $\\pi_{A}(p)=\\pi_{B}(p)$. Set the two power functions equal and solve:\n$$\n(1-p)^{2}+p^{2}=\\frac{5}{9}.\n$$\nExpand and simplify:\n$$\np^{2}+(1-2p+p^{2})=\\frac{5}{9}\\;\\;\\Longrightarrow\\;\\;2p^{2}-2p+1=\\frac{5}{9}.\n$$\nClear denominators and rearrange:\n$$\n18p^{2}-18p+9=5\\;\\;\\Longrightarrow\\;\\;18p^{2}-18p+4=0\\;\\;\\Longrightarrow\\;\\;9p^{2}-9p+2=0.\n$$\nSolve the quadratic using the quadratic formula:\n$$\np=\\frac{9\\pm\\sqrt{(-9)^{2}-4\\cdot 9\\cdot 2}}{2\\cdot 9}=\\frac{9\\pm\\sqrt{81-72}}{18}=\\frac{9\\pm \\sqrt{9}}{18}=\\frac{9\\pm 3}{18}.\n$$\nThus,\n$$\np=\\frac{6}{18}=\\frac{1}{3}\\quad \\text{or}\\quad p=\\frac{12}{18}=\\frac{2}{3}.\n$$\nExcluding $p=\\frac{1}{3}$ as specified, the other solution in $(0,1)$ is $p=\\frac{2}{3}$.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "1963239"}, {"introduction": "Having practiced calculating and comparing power functions, we now turn to a more general and conceptual question. This exercise asks you to consider a fundamental property of any 'unbiased' test, without specifying the data's distribution. By reasoning from the definition of an unbiased test, you will discover a universal characteristic of its power function's shape, deepening your understanding of the theoretical principles that govern good statistical testing [@problem_id:1945687].", "problem": "A team of materials scientists is developing a new statistical procedure to test the average tensile strength, $\\theta$, of a newly formulated polymer. The design specification requires a mean strength of $\\theta_0$. The team devises a statistical test for the hypotheses $H_0: \\theta = \\theta_0$ versus the two-sided alternative $H_1: \\theta \\neq \\theta_0$.\n\nLet $\\pi(\\theta)$ denote the power function of this test, which represents the probability of rejecting the null hypothesis $H_0$ when the true mean strength is $\\theta$. The significance level of the test, the probability of rejecting $H_0$ when it is true, is set to $\\alpha$, where $0 < \\alpha < 1$.\n\nThe test is required to be \"unbiased\". A test is defined as unbiased if the probability of rejecting a false null hypothesis is always greater than or equal to the probability of rejecting a true null hypothesis. For this specific hypothesis test, this condition is formally stated as:\n1. $\\pi(\\theta_0) = \\alpha$\n2. $\\pi(\\theta) \\geq \\alpha$ for all possible values of $\\theta \\neq \\theta_0$.\n\nFurthermore, assume that the power function $\\pi(\\theta)$ is a continuous and twice-differentiable function of $\\theta$ in a neighborhood around $\\theta_0$.\n\nGiven these properties, which of the following statements most accurately describes the behavior of the power function $\\pi(\\theta)$ at the point $\\theta = \\theta_0$?\n\nA. $\\pi(\\theta)$ has a local maximum at $\\theta = \\theta_0$.\nB. $\\pi(\\theta)$ has a local minimum at $\\theta = \\theta_0$.\nC. $\\pi(\\theta)$ has a point of inflection (but not an extremum) at $\\theta = \\theta_0$.\nD. $\\pi(\\theta)$ is strictly decreasing at $\\theta = \\theta_0$.\nE. The behavior cannot be determined; it could be a minimum, maximum, or neither, depending on the underlying data distribution.", "solution": "Let $\\pi(\\theta)$ denote the power function of the test with size $\\alpha$, so by definition of significance level,\n$$\n\\pi(\\theta_{0})=\\alpha.\n$$\nUnbiasedness requires that for all $\\theta \\neq \\theta_{0}$,\n$$\n\\pi(\\theta)\\geq \\alpha.\n$$\nCombining these two facts yields, for all $\\theta \\neq \\theta_{0}$,\n$$\n\\pi(\\theta)\\geq \\alpha=\\pi(\\theta_{0}).\n$$\nHence $\\theta_{0}$ is a global minimizer of $\\pi(\\theta)$ (since every other parameter value has power at least as large as at $\\theta_{0}$). In particular, $\\theta_{0}$ is a local minimum.\n\nBecause $\\pi(\\theta)$ is assumed continuous and twice differentiable in a neighborhood of $\\theta_{0}$, the standard necessary conditions from calculus for a local minimum apply:\n$$\n\\pi'(\\theta_{0})=0, \\qquad \\pi''(\\theta_{0})\\geq 0,\n$$\nwith the possibility that $\\pi''(\\theta_{0})=0$ if the minimum is flat. However, the qualitative behavior at $\\theta_{0}$ is that $\\pi(\\theta)$ has a local minimum there.\n\nTherefore, among the options provided, the correct description is that $\\pi(\\theta)$ has a local minimum at $\\theta=\\theta_{0}$.", "answer": "$$\\boxed{B}$$", "id": "1945687"}]}