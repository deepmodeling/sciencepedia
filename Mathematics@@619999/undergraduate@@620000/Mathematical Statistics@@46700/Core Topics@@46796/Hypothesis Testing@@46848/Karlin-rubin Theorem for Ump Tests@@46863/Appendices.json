{"hands_on_practices": [{"introduction": "Before constructing a Uniformly Most Powerful (UMP) test, we must first identify the correct test statistic. The Karlin-Rubin theorem applies to distributions in the one-parameter exponential family, and the UMP test is based on the family's sufficient statistic. This problem challenges you to find this key statistic for the Weibull distribution by analyzing its likelihood function, a vital first step in applying the theorem correctly [@problem_id:1927237].", "problem": "Let $X_1, X_2, \\dots, X_n$ be an independent and identically distributed random sample from a Weibull distribution. The probability density function (PDF) for a single observation $X$ is given by\n$$f(x; k, \\lambda) = \\frac{k}{\\lambda} \\left(\\frac{x}{\\lambda}\\right)^{k-1} \\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^k\\right), \\quad \\text{for } x \\ge 0$$\nwhere $\\lambda > 0$ is the scale parameter and $k > 0$ is the shape parameter. Assume that the shape parameter $k$ is a known positive constant.\n\nConsider the problem of testing the null hypothesis $H_0: \\lambda = \\lambda_0$ against the alternative hypothesis $H_1: \\lambda > \\lambda_0$, where $\\lambda_0$ is a specified positive constant. The test is to be based on the random sample.\n\nWhich of the following statistics should be used to construct a Uniformly Most Powerful (UMP) test of significance level $\\alpha$?\n\nA. $T(\\mathbf{X}) = \\sum_{i=1}^n X_i$\n\nB. $T(\\mathbf{X}) = \\sum_{i=1}^n X_i^k$\n\nC. $T(\\mathbf{X}) = \\prod_{i=1}^n X_i$\n\nD. $T(\\mathbf{X}) = \\sum_{i=1}^n \\ln(X_i)$\n\nE. $T(\\mathbf{X}) = \\left(\\sum_{i=1}^n X_i\\right)^k$", "solution": "The joint probability density for the independent sample $\\mathbf{X}=(X_{1},\\dots,X_{n})$ from the Weibull distribution with known shape $k>0$ and scale $\\lambda>0$ is\n$$\nL(\\lambda;\\mathbf{X})=\\prod_{i=1}^{n} \\frac{k}{\\lambda}\\left(\\frac{X_{i}}{\\lambda}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{X_{i}}{\\lambda}\\right)^{k}\\right).\n$$\nTaking logarithms and collecting terms that depend on $\\lambda$ yields\n$$\n\\ln L(\\lambda;\\mathbf{X})=n\\ln k+(k-1)\\sum_{i=1}^{n}\\ln X_{i}-k n \\ln \\lambda-\\sum_{i=1}^{n}\\frac{X_{i}^{k}}{\\lambda^{k}}.\n$$\nThus, apart from the factor $n\\ln k+(k-1)\\sum_{i=1}^{n}\\ln X_{i}$ which does not involve $\\lambda$, the $\\lambda$-dependence of the likelihood is\n$$\nL(\\lambda;\\mathbf{X})\\propto \\lambda^{-k n}\\exp\\!\\left(-\\frac{1}{\\lambda^{k}}\\sum_{i=1}^{n}X_{i}^{k}\\right).\n$$\nDefine $\\theta=\\lambda^{-k}$. Then\n$$\nL(\\theta;\\mathbf{X})\\propto \\theta^{n}\\exp\\!\\left(-\\theta \\sum_{i=1}^{n}X_{i}^{k}\\right),\n$$\nwhich is a one-parameter exponential family in the natural parameter $\\theta$ with natural statistic $T(\\mathbf{X})=\\sum_{i=1}^{n}X_{i}^{k}$. The null hypothesis $H_{0}:\\lambda=\\lambda_{0}$ is equivalent to $H_{0}:\\theta=\\theta_{0}$ with $\\theta_{0}=\\lambda_{0}^{-k}$, and the one-sided alternative $H_{1}:\\lambda>\\lambda_{0}$ is equivalent to $H_{1}:\\theta\\theta_{0}$.\n\nFor any fixed $\\theta_{1}\\theta_{0}$, the likelihood ratio satisfies\n$$\n\\frac{L(\\theta_{1};\\mathbf{X})}{L(\\theta_{0};\\mathbf{X})}=\\left(\\frac{\\theta_{1}}{\\theta_{0}}\\right)^{n}\\exp\\!\\left(-(\\theta_{1}-\\theta_{0})\\,T(\\mathbf{X})\\right),\n$$\nand since $\\theta_{1}-\\theta_{0}0$, the function of $T(\\mathbf{X})$ on the right-hand side is strictly increasing in $T(\\mathbf{X})$. Therefore, the family has a monotone likelihood ratio in $T(\\mathbf{X})$, and by the Karlin–Rubin theorem, the uniformly most powerful test of level $\\alpha$ for $H_{0}:\\theta=\\theta_{0}$ versus $H_{1}:\\theta\\theta_{0}$ rejects for large values of $T(\\mathbf{X})$. Translating back to $\\lambda$, this is the UMP test for $H_{0}:\\lambda=\\lambda_{0}$ versus $H_{1}:\\lambda>\\lambda_{0}$.\n\nConsequently, the correct statistic to use is $T(\\mathbf{X})=\\sum_{i=1}^{n}X_{i}^{k}$, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1927237"}, {"introduction": "With the test statistic in hand, our next task is to define the precise rule for making a decision—the rejection region. This involves using the monotone likelihood ratio property to determine if we should reject the null hypothesis for large or small values of our statistic. This practice walks you through the full procedure, from verifying the test direction to calculating the critical value that achieves a specified significance level $\\alpha$ [@problem_id:1927190].", "problem": "Let $X$ be a random variable representing the number of trials required to get the first success in a sequence of independent Bernoulli trials. The probability mass function (PMF) of $X$ follows a geometric distribution with parameter $p$, given by $f(x|p) = p(1-p)^{x-1}$ for $x \\in \\{1, 2, 3, \\dots\\}$, where $p \\in (0, 1)$ is the unknown probability of success. The geometric distribution is a member of the one-parameter exponential family, and for a single observation $X$, a sufficient statistic for the parameter $p$ is $T(X) = X$.\n\nAn analyst wants to construct a Uniformly Most Powerful (UMP) test for the hypotheses:\n$H_0: p \\ge 0.4$\nversus\n$H_1: p  0.4$\n\nThe test should have a significance level (size) of $\\alpha = 0.1296$. What is the rejection region $R$ for this UMP test?\n\nA. $R = \\{x \\in \\mathbb{N} : x \\le 2\\}$\n\nB. $R = \\{x \\in \\mathbb{N} : x \\ge 3\\}$\n\nC. $R = \\{x \\in \\mathbb{N} : x \\le 4\\}$\n\nD. $R = \\{x \\in \\mathbb{N} : x \\ge 5\\}$\n\nE. $R = \\{x \\in \\mathbb{N} : x = 5\\}$", "solution": "We have one observation from a geometric distribution with pmf $f(x \\mid p) = p(1-p)^{x-1}$ for $x \\in \\{1,2,\\dots\\}$ and $p \\in (0,1)$. Write this in exponential family form:\n$$\nf(x \\mid p) = \\exp\\{\\ln p + (x-1)\\ln(1-p)\\} = \\exp\\{\\ln p - \\ln(1-p) + x \\ln(1-p)\\}.\n$$\nThus it is a one-parameter exponential family with natural parameter $\\eta = \\ln(1-p)$ and statistic $T(X) = X$. The family has a monotone likelihood ratio in $T(X) = X$ because for $\\eta_{2} > \\eta_{1}$,\n$$\n\\frac{f(x \\mid \\eta_{2})}{f(x \\mid \\eta_{1})} = \\exp\\{x(\\eta_{2}-\\eta_{1})\\} \\times \\text{(constant in $x$)},\n$$\nwhich is increasing in $x$. By the Karlin–Rubin theorem, for testing one-sided hypotheses in $\\eta$, the UMP test rejects for large values of $T(X)$ when $H_{1}$ corresponds to larger $\\eta$.\n\nNow $H_{0}: p \\ge p_{0}$ versus $H_{1}: p  p_{0}$ with $p_{0} = 0.4$. Since $\\eta = \\ln(1-p)$ is decreasing in $p$, we have $H_{0}: \\eta \\le \\eta_{0}$ versus $H_{1}: \\eta > \\eta_{0}$, where $\\eta_{0} = \\ln(1-p_{0})$. Therefore, the UMP test rejects for large $X$, i.e., has rejection region of the form $R = \\{x \\ge c\\}$.\n\nTo achieve size $\\alpha = 0.1296$, we choose $c$ so that the probability under the boundary $p = p_{0}$ equals $\\alpha$. For a geometric distribution,\n$$\n\\mathbb{P}_{p_{0}}(X \\ge c) = \\sum_{x=c}^{\\infty} p_{0}(1-p_{0})^{x-1} = (1-p_{0})^{c-1}.\n$$\nSet $(1-p_{0})^{c-1} = \\alpha$. With $p_{0} = 0.4$, we have $1-p_{0} = 0.6$, and we note that $0.6^{4} = 0.1296$. Hence $(1-p_{0})^{c-1} = \\alpha$ is satisfied by $c-1 = 4$, i.e., $c = 5$.\n\nTherefore, the UMP level-$\\alpha$ rejection region is $R = \\{x \\in \\mathbb{N} : x \\ge 5\\}$, which corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1927190"}, {"introduction": "Theory becomes practice when we design an experiment. A UMP test guarantees the best possible power for a given significance level, but how large must our sample be to reliably detect a meaningful effect? This exercise bridges this gap by tasking you with a common real-world problem: calculating the minimum sample size required to ensure a test has adequate power, connecting the concepts of UMP tests and practical experimental design [@problem_id:1927218].", "problem": "A manufacturer of optical components is developing a new anti-reflective coating. The success of a single coating process is modeled as a Bernoulli trial, where 'success' means the coating's reflectance is below a specific threshold. The current industry-standard process has a success probability of $p=0.80$. The manufacturer wishes to test if their new process is superior. They set up a hypothesis test with the null hypothesis $H_0: p \\le 0.80$ against the alternative hypothesis $H_1: p > 0.80$. The test must be conducted at a significance level of $\\alpha = 0.05$. Furthermore, the manufacturer requires that if the true success probability of the new process is $p_1 = 0.85$, the test should have a power of at least $0.90$. You are tasked to find the necessary sample size $n$ for this experiment. Assume that the sample size will be large enough to permit the use of a normal approximation for the distribution of the sample proportion of successes. Determine the minimum integer sample size $n$ required to meet these specifications.\n\nFor your calculations, use the standard normal cumulative distribution function quantiles $z_{0.05} \\approx 1.645$ and $z_{0.10} \\approx 1.282$.", "solution": "We consider a one-sided z-test for a single proportion. Let $p_{0}=0.80$ under $H_{0}$ and $p_{1}=0.85$ be the effect at which the power requirement must be met. For a significance level $\\alpha=0.05$, the one-sided critical value is $z_{\\alpha}=1.645$. For power $1-\\beta=0.90$, the corresponding quantile is $z_{\\beta}=1.282$.\n\nUsing the normal approximation for the sample proportion $\\hat{p}$, the rejection rule based on the null standardization is to reject $H_{0}$ when\n$$\n\\hat{p} \\geq p_{0} + z_{\\alpha}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}.\n$$\nThe power at $p_{1}$ is\n$$\n\\Pr_{p_{1}}\\!\\left(\\hat{p} \\geq p_{0} + z_{\\alpha}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}\\right)\n= \\Pr\\!\\left(\\frac{\\hat{p}-p_{1}}{\\sqrt{p_{1}(1-p_{1})/n}} \\geq \\frac{p_{0} + z_{\\alpha}\\sqrt{p_{0}(1-p_{0})/n} - p_{1}}{\\sqrt{p_{1}(1-p_{1})/n}}\\right).\n$$\nTo ensure this probability is at least $1-\\beta$, it suffices that\n$$\n\\frac{p_{0} + z_{\\alpha}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}} - p_{1}}{\\sqrt{\\frac{p_{1}(1-p_{1})}{n}}} \\leq -z_{\\beta}.\n$$\nRearranging,\n$$\n(p_{1}-p_{0}) \\geq z_{\\alpha}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}} + z_{\\beta}\\sqrt{\\frac{p_{1}(1-p_{1})}{n}},\n$$\nwhich is equivalent to\n$$\nn \\geq \\frac{\\left(z_{\\alpha}\\sqrt{p_{0}(1-p_{0})} + z_{\\beta}\\sqrt{p_{1}(1-p_{1})}\\right)^{2}}{(p_{1}-p_{0})^{2}}.\n$$\n\nNow substitute $p_{0}=0.80$, $p_{1}=0.85$, $z_{\\alpha}=1.645$, and $z_{\\beta}=1.282$:\n$p_{0}(1-p_{0})=0.16$, $\\sqrt{p_{0}(1-p_{0})}=0.4$,\n$p_{1}(1-p_{1})=0.1275$, $\\sqrt{p_{1}(1-p_{1})}\\approx 0.357071$,\nand $p_{1}-p_{0}=0.05$.\n\nThus\n$$\nn \\geq \\frac{\\left(1.645\\cdot 0.4 + 1.282\\cdot 0.357071\\right)^{2}}{0.05^{2}}\n= \\frac{\\left(0.658 + 0.457766\\right)^{2}}{0.0025}\n= \\frac{1.115766^{2}}{0.0025}\n\\approx \\frac{1.2450}{0.0025} \\approx 497.973.\n$$\nTherefore, the minimum integer sample size meeting the specifications is\n$n=\\lceil 497.973 \\rceil = 498$.", "answer": "$$\\boxed{498}$$", "id": "1927218"}]}