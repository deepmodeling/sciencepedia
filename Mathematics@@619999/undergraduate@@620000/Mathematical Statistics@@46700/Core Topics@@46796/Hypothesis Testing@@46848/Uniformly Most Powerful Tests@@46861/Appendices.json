{"hands_on_practices": [{"introduction": "The Karlin-Rubin theorem provides a powerful tool for finding Uniformly Most Powerful (UMP) tests, especially for models belonging to the one-parameter exponential family. This first exercise [@problem_id:1966245] guides you through the process of deriving the UMP test statistic for the scale parameter of a Weibull distribution. By recasting the likelihood function, you will practice identifying the sufficient statistic and confirming the monotone likelihood ratio property, a fundamental skill in hypothesis testing.", "problem": "In industrial quality control and reliability engineering, the lifetime of certain electronic components is often modeled using the Weibull distribution. A random sample of $n$ such components is taken, and their lifetimes are measured, yielding the observations $X_1, X_2, \\ldots, X_n$. These lifetimes are assumed to be independent and identically distributed according to a Weibull distribution with a known, fixed shape parameter $k > 0$ and an unknown scale parameter $\\lambda > 0$.\n\nThe probability density function (PDF) for a single lifetime observation $X$ is given by:\n$$f(x; \\lambda, k) = \\frac{k}{\\lambda} \\left(\\frac{x}{\\lambda}\\right)^{k-1} \\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^k\\right), \\quad \\text{for } x \\ge 0$$\nAn engineer wants to determine if the average lifetime of the components has increased. This corresponds to testing the null hypothesis $H_0: \\lambda \\le \\lambda_0$ against the alternative hypothesis $H_1: \\lambda > \\lambda_0$, where $\\lambda_0$ is a pre-specified performance benchmark.\n\nA Uniformly Most Powerful (UMP) test for this hypothesis problem exists. The rejection region for this test, at a given significance level $\\alpha$, can be expressed in the form $T(X_1, \\ldots, X_n) > c$, where $T$ is a test statistic derived from the sample and $c$ is a critical value.\n\nDetermine the test statistic $T(X_1, \\ldots, X_n)$. Your answer should be an expression in terms of the sample values $X_i$ and the known parameter $k$.", "solution": "We begin from the given Weibull density for a single observation with known shape parameter $k>0$ and scale parameter $\\lambda>0$:\n$$\nf(x;\\lambda,k)=\\frac{k}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{x}{\\lambda}\\right)^{k}\\right),\\quad x\\ge 0.\n$$\nFor independent observations $X_{1},\\ldots,X_{n}$, the joint likelihood is\n$$\nL(\\lambda; x_{1},\\ldots,x_{n})=\\prod_{i=1}^{n}\\frac{k}{\\lambda}\\left(\\frac{x_{i}}{\\lambda}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{x_{i}}{\\lambda}\\right)^{k}\\right).\n$$\nAlgebraically,\n$$\nL(\\lambda;x)=\\left(\\prod_{i=1}^{n}k x_{i}^{k-1}\\right)\\lambda^{-n}\\lambda^{-(k-1)n}\\exp\\!\\left(-\\sum_{i=1}^{n}\\frac{x_{i}^{k}}{\\lambda^{k}}\\right)\n=\\left(\\prod_{i=1}^{n}k x_{i}^{k-1}\\right)\\lambda^{-kn}\\exp\\!\\left(-\\sum_{i=1}^{n}\\frac{x_{i}^{k}}{\\lambda^{k}}\\right).\n$$\nIntroduce the reparameterization $\\theta=\\lambda^{k}$ with $\\theta>0$. Then $\\lambda^{-kn}=\\theta^{-n}$ and $\\sum x_{i}^{k}/\\lambda^{k}=\\sum x_{i}^{k}/\\theta$, so\n$$\nL(\\theta;x)=\\left(\\prod_{i=1}^{n}k x_{i}^{k-1}\\right)\\theta^{-n}\\exp\\!\\left(-\\frac{1}{\\theta}\\sum_{i=1}^{n}x_{i}^{k}\\right).\n$$\nThis exhibits a one-parameter exponential family in $\\theta$ with sufficient statistic\n$$\nS=\\sum_{i=1}^{n}x_{i}^{k}.\n$$\nFor testing $H_{0}:\\lambda\\le\\lambda_{0}$ versus $H_{1}:\\lambda\\lambda_{0}$, equivalently $H_{0}:\\theta\\le\\theta_{0}$ versus $H_{1}:\\theta\\theta_{0}$ where $\\theta_{0}=\\lambda_{0}^{k}$, the likelihood ratio for any $\\theta_{1}\\theta_{0}$ is\n$$\n\\frac{L(\\theta_{1};x)}{L(\\theta_{0};x)}=\\left(\\frac{\\theta_{0}}{\\theta_{1}}\\right)^{n}\\exp\\!\\left(-S\\left(\\frac{1}{\\theta_{1}}-\\frac{1}{\\theta_{0}}\\right)\\right),\n$$\nwhich is an increasing function of $S$ because $\\theta_{1}\\theta_{0}$ implies $\\frac{1}{\\theta_{1}}-\\frac{1}{\\theta_{0}}0$. Thus the family has a monotone likelihood ratio in $S$, and by the Karlin–Rubin theorem, the uniformly most powerful test for the one-sided hypothesis rejects for large values of $S$.\n\nTherefore, the test statistic may be taken as\n$$\nT(X_{1},\\ldots,X_{n})=\\sum_{i=1}^{n}X_{i}^{k}.\n$$\nEquivalently, any positive scalar multiple such as $\\sum_{i=1}^{n}(X_{i}/\\lambda_{0})^{k}$ induces the same rejection ordering, but the canonical sufficient statistic is $\\sum_{i=1}^{n}X_{i}^{k}$.", "answer": "$$\\boxed{\\sum_{i=1}^{n}X_{i}^{k}}$$", "id": "1966245"}, {"introduction": "Real-world problems often involve comparing two populations, which can introduce nuisance parameters that complicate hypothesis testing. This practice problem [@problem_id:1966251] demonstrates a powerful strategy: transforming a two-parameter problem into a solvable one-parameter problem by conditioning on a sufficient statistic. You will explore how to construct a UMP test for comparing two Poisson rates, a method widely used in A/B testing and comparative studies, by first deriving the relevant conditional distribution.", "problem": "In the context of online advertising, a company runs an A/B test on two different ad campaigns, Campaign A and Campaign B, over a fixed, identical time period. The number of user clicks generated by each campaign can be modeled as an independent Poisson process. Let $X_A$ be the number of clicks from Campaign A, assumed to be a random variable from a Poisson distribution with mean $\\lambda_A$. Similarly, let $X_B$ be the number of clicks from Campaign B, from a Poisson distribution with mean $\\lambda_B$. The two random variables $X_A$ and $X_B$ are independent.\n\nThe company wants to determine if Campaign A is significantly more effective than Campaign B. This can be formulated as a hypothesis test on the ratio of the click rates, $\\theta = \\lambda_A / \\lambda_B$. The null hypothesis is that Campaign A is not more effective than Campaign B, $H_0: \\theta \\le 1$, and the alternative is that it is more effective, $H_1: \\theta  1$.\n\nLet $x_A$ and $x_B$ be the observed number of clicks for each campaign, and let the total number of clicks be $s = x_A + x_B$. You are tasked with identifying the Uniformly Most Powerful (UMP) test of size $\\alpha$.\n\nWhich of the following describes the rejection region for the size-$\\alpha$ UMP test for $H_0: \\lambda_A/\\lambda_B \\le 1$ versus $H_1: \\lambda_A/\\lambda_B > 1$?\n\nA. Reject $H_0$ if $x_A  k_s$, where the critical value $k_s$ depends on the total count $s$ and is chosen to satisfy the size constraint based on a Binomial distribution.\n\nB. Reject $H_0$ if $s  k$, where the critical value $k$ is a constant chosen to satisfy the size constraint based on a Poisson distribution.\n\nC. Reject $H_0$ if the ratio $x_A / x_B  k$, where the critical value $k$ is a constant chosen to satisfy the size constraint based on an F-distribution.\n\nD. Reject $H_0$ if $x_A  k_s$, where the critical value $k_s$ depends on the total count $s$ and is chosen to satisfy the size constraint based on a Binomial distribution.\n\nE. Reject $H_0$ if $x_A - x_B  k$, where the critical value $k$ is a constant chosen to satisfy the size constraint based on a Normal distribution.", "solution": "Let $X_A \\sim \\text{Poisson}(\\lambda_A)$ and $X_B \\sim \\text{Poisson}(\\lambda_B)$ be independent random variables. The joint probability mass function (PMF) of $(X_A, X_B)$ for observed values $(x_A, x_B)$ is given by:\n$$ P(X_A=x_A, X_B=x_B; \\lambda_A, \\lambda_B) = \\frac{e^{-\\lambda_A}\\lambda_A^{x_A}}{x_A!} \\frac{e^{-\\lambda_B}\\lambda_B^{x_B}}{x_B!} $$\nThis can be written as:\n$$ f(x_A, x_B; \\lambda_A, \\lambda_B) = \\frac{1}{x_A! x_B!} \\exp\\left(x_A \\ln(\\lambda_A) + x_B \\ln(\\lambda_B) - (\\lambda_A+\\lambda_B)\\right) $$\nThis is a two-parameter exponential family with a two-dimensional sufficient statistic $(X_A, X_B)$. The hypothesis is on the parameter $\\theta = \\lambda_A / \\lambda_B$. This is a composite hypothesis with a nuisance parameter (e.g., $\\lambda_A + \\lambda_B$). To derive a UMP test, we can use the method of conditioning on the sufficient statistic for the nuisance parameter. Let $S = X_A + X_B$. The distribution of $S$ is Poisson with mean $\\lambda_A + \\lambda_B$, since the sum of independent Poisson random variables is also a Poisson random variable.\n\nLet's find the conditional distribution of $X_A$ given $S=s$. For $x_A = 0, 1, \\dots, s$:\n$$ P(X_A=x_A | S=s) = \\frac{P(X_A=x_A, X_A+X_B=s)}{P(X_A+X_B=s)} = \\frac{P(X_A=x_A, X_B=s-x_A)}{P(S=s)} $$\nUsing the independence of $X_A$ and $X_B$:\n$$ P(X_A=x_A | S=s) = \\frac{\\frac{e^{-\\lambda_A}\\lambda_A^{x_A}}{x_A!} \\frac{e^{-\\lambda_B}\\lambda_B^{s-x_A}}{(s-x_A)!}}{\\frac{e^{-(\\lambda_A+\\lambda_B)}(\\lambda_A+\\lambda_B)^s}{s!}} $$\n$$ = \\frac{s!}{x_A!(s-x_A)!} \\frac{e^{-(\\lambda_A+\\lambda_B)}\\lambda_A^{x_A}\\lambda_B^{s-x_A}}{e^{-(\\lambda_A+\\lambda_B)}(\\lambda_A+\\lambda_B)^s} $$\n$$ = \\binom{s}{x_A} \\frac{\\lambda_A^{x_A}\\lambda_B^{s-x_A}}{(\\lambda_A+\\lambda_B)^s} = \\binom{s}{x_A} \\left(\\frac{\\lambda_A}{\\lambda_A+\\lambda_B}\\right)^{x_A} \\left(\\frac{\\lambda_B}{\\lambda_A+\\lambda_B}\\right)^{s-x_A} $$\nThis is the PMF of a Binomial distribution, $X_A | (S=s) \\sim \\text{Binomial}(s, p)$, where the probability of success is $p = \\frac{\\lambda_A}{\\lambda_A+\\lambda_B}$.\n\nNow, we re-parameterize the hypothesis in terms of $p$. Let $\\theta = \\lambda_A / \\lambda_B$.\n$$ p = \\frac{\\lambda_A}{\\lambda_A+\\lambda_B} = \\frac{\\lambda_A/\\lambda_B}{(\\lambda_A/\\lambda_B) + 1} = \\frac{\\theta}{\\theta+1} $$\nThe function $g(\\theta) = \\frac{\\theta}{\\theta+1}$ is a strictly increasing function of $\\theta$ for $\\theta > 0$. Therefore, the original hypothesis test is equivalent to a test on $p$.\n$H_0: \\theta \\le 1 \\iff p \\le \\frac{1}{1+1} = \\frac{1}{2}$\n$H_1: \\theta > 1 \\iff p > \\frac{1}{2}$\n\nThe problem is now reduced to finding a UMP test for $H_0: p \\le 1/2$ versus $H_1: p > 1/2$ for a random variable $X_A$ which follows a Binomial$(s, p)$ distribution (conditional on the observed total $s$). The PMF of this distribution, $f(x_A; p) = \\binom{s}{x_A} p^{x_A} (1-p)^{s-x_A}$, belongs to a one-parameter exponential family with sufficient statistic $T(X_A) = X_A$. The likelihood ratio for any $p_2 > p_1$ is an increasing function of $X_A$.\n\nBy the Karlin-Rubin Theorem, a UMP test of size $\\alpha$ for this one-sided hypothesis exists and its rejection region is of the form $\\{x_A : x_A > k_s\\}$. The critical value $k_s$ is chosen for a given total count $s$ to satisfy the size constraint:\n$$ \\sup_{p \\le 1/2} P_p(X_A > k_s | S=s) = P_{p=1/2}(X_A > k_s | S=s) \\le \\alpha $$\nThe equality holds because the power function is increasing in $p$. So, for a given observed total $s=x_A+x_B$, the test rejects $H_0$ if the observed count $x_A$ is greater than some critical value $k_s$. This critical value is determined from the null distribution, which is $\\text{Binomial}(s, 1/2)$.\n\nThis matches option A.\n- Option B tests the total rate $\\lambda_A+\\lambda_B$, not the ratio.\n- Option C is an intuitive but incorrect test statistic form, and the F-distribution is used for ratios of variances of normal populations, not ratios of Poisson means.\n- Option D describes a test for the alternative $H_1: \\lambda_A/\\lambda_B  1$.\n- Option E uses a statistic which is also not the UMP one, and a Normal approximation is an approximation, not the exact UMP test.", "answer": "$$\\boxed{A}$$", "id": "1966251"}, {"introduction": "When a test statistic follows a discrete distribution, the set of possible significance levels is also discrete, making it challenging to construct a test with an exact, pre-specified size $\\alpha$. This exercise [@problem_id:1966286] introduces the concept of a randomized test, a theoretical tool used to achieve any desired significance level precisely. You will determine the randomization probability needed to construct a UMP test for a binomial parameter, gaining insight into the formal requirements for controlling Type I error.", "problem": "A quality control engineer is tasked with assessing the defect rate of a manufacturing process. From each very large batch of components, a small sample of $n$ items is randomly selected for inspection. The number of defective items found in the sample, denoted by the random variable $X$, is modeled by a Binomial distribution, $X \\sim \\text{Binomial}(n, p)$, where $p$ is the true proportion of defective items produced by the process.\n\nThe engineer wishes to test the null hypothesis that the process is in an acceptable state, $H_0: p \\le 0.5$, against the alternative hypothesis that the process has an unacceptably high defect rate, $H_1: p  0.5$.\n\nFor this test, a sample of size $n=3$ is used. The engineer needs to construct a Uniformly Most Powerful (UMP) test at an exact significance level of $\\alpha = 0.2$. Since the test statistic $X$ has a discrete distribution, achieving this exact significance level requires a randomized test. The structure of such a test is defined by a critical value $c$ and a randomization probability $\\gamma \\in [0, 1]$. The decision rule is as follows:\n- If $X  c$, reject the null hypothesis $H_0$.\n- If $X = c$, reject the null hypothesis $H_0$ with probability $\\gamma$.\n- If $X  c$, do not reject the null hypothesis $H_0$.\n\nBased on this framework, determine the specific value of the randomization probability $\\gamma$ required for this test. Express your answer as an exact fraction in its simplest form.", "solution": "We model the count of defectives as $X \\sim \\text{Binomial}(n,p)$ with $n=3$. We test $H_{0}: p \\leq \\frac{1}{2}$ versus $H_{1}: p  \\frac{1}{2}$. For a one-sided test with binomial data, the family has a monotone likelihood ratio in $X$, so by the Karlin–Rubin theorem the uniformly most powerful size-$\\alpha$ test rejects for large $X$; the least favorable value under $H_{0}$ is $p=\\frac{1}{2}$. Therefore, to achieve exact size $\\alpha=\\frac{1}{5}$, we choose a critical value $c$ and randomization probability $\\gamma$ so that\n$$\n\\sup_{p \\leq \\frac{1}{2}} \\Pr_{p}(\\text{reject})=\\Pr_{p=\\frac{1}{2}}(\\text{reject})=\\alpha=\\frac{1}{5}.\n$$\nBecause $X$ is discrete with $n=3$, we must consider upper-tail critical regions. If $c=3$, then the rejection probability under $p=\\frac{1}{2}$ is at most $\\Pr(X=3 \\mid p=\\frac{1}{2})=\\frac{1}{8}$, which is less than $\\frac{1}{5}$; thus we must include $X=2$ in the rejection region with randomization. Hence take $c=2$, so the test rejects always when $X=3$ and rejects with probability $\\gamma$ when $X=2$. The size condition becomes\n$$\n\\Pr_{p=\\frac{1}{2}}(\\text{reject})=\\Pr(X=3)+\\gamma\\,\\Pr(X=2)=\\alpha.\n$$\nFor $X \\sim \\text{Binomial}(3,\\frac{1}{2})$,\n$$\n\\Pr(X=3)=\\binom{3}{3}\\left(\\frac{1}{2}\\right)^{3}=\\frac{1}{8}, \\qquad \\Pr(X=2)=\\binom{3}{2}\\left(\\frac{1}{2}\\right)^{3}=\\frac{3}{8}.\n$$\nImposing exact size $\\alpha=\\frac{1}{5}$ gives\n$$\n\\frac{1}{8}+\\gamma \\cdot \\frac{3}{8}=\\frac{1}{5}.\n$$\nSolving,\n$$\n\\gamma=\\frac{\\frac{1}{5}-\\frac{1}{8}}{\\frac{3}{8}}=\\frac{\\frac{3}{40}}{\\frac{3}{8}}=\\frac{8}{40}=\\frac{1}{5}.\n$$\nThus the required randomization probability is $\\gamma=\\frac{1}{5}$.", "answer": "$$\\boxed{\\frac{1}{5}}$$", "id": "1966286"}]}