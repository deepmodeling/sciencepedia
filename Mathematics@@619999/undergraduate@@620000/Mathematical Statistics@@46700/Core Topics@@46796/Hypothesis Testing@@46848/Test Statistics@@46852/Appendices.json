{"hands_on_practices": [{"introduction": "The first step in hypothesis testing is to define a quantity—the test statistic—that summarizes sample data into a single value. This value helps us decide whether to reject the null hypothesis. This exercise [@problem_id:1958165] explores this foundational concept in one of the most common statistical settings: a series of independent success/failure trials. You will determine the appropriate test statistic for the population proportion $p$ and, crucially, identify its exact probability distribution under the null hypothesis, forming the basis for an exact test.", "problem": "A semiconductor manufacturer has developed a new etching process for creating logic gates on a silicon wafer. The company's internal specification claims that the proportion of defective gates produced by this process is exactly $p_0$. To verify this claim, a quality control engineer plans an experiment. A random sample of $n$ logic gates is selected from the production line, and each gate is tested. The tests are independent, and each gate is classified as either \"defective\" or \"non-defective\".\n\nLet $X_i$ be an indicator random variable for the $i$-th gate tested, such that $X_i=1$ if the gate is defective and $X_i=0$ otherwise. The engineer's goal is to perform an exact hypothesis test of the null hypothesis $H_0: p = p_0$ against the two-sided alternative $H_1: p \\neq p_0$, where $p$ is the true proportion of defective gates.\n\nThe test-statistic, let's call it $T$, is chosen to be the total number of defective gates observed in the sample. For an *exact* test, which of the following statements correctly identifies both the test statistic $T$ and its exact probability distribution under the assumption that the null hypothesis $H_0$ is true?\n\nA. The test statistic is $T = \\frac{\\sum_{i=1}^{n} X_i - n p_0}{\\sqrt{n p_0 (1-p_0)}}$, and its distribution under $H_0$ is the Standard Normal distribution, $N(0,1)$.\n\nB. The test statistic is $T = \\sum_{i=1}^{n} X_i$, and its distribution under $H_0$ is the Poisson distribution with parameter $\\lambda = n p_0$.\n\nC. The test statistic is $T = \\sum_{i=1}^{n} X_i$, and its distribution under $H_0$ is the Binomial distribution with parameters $n$ and $p_0$.\n\nD. The test statistic is $T = \\frac{1}{n}\\sum_{i=1}^{n} X_i$, and its distribution under $H_0$ is the Bernoulli distribution with parameter $p_0$.\n\nE. The test statistic is $T = \\sum_{i=1}^{n} X_i$, and its distribution under $H_0$ is the Geometric distribution with parameter $p_0$.", "solution": "Under the null hypothesis $H_{0}: p = p_{0}$, each indicator $X_{i}$ is a Bernoulli random variable with parameter $p_{0}$, i.e., $\\Pr(X_{i}=1 \\mid H_{0}) = p_{0}$ and $\\Pr(X_{i}=0 \\mid H_{0}) = 1 - p_{0}$. The problem states the tests are independent, so $X_{1}, \\ldots, X_{n}$ are independent and identically distributed as $\\text{Bernoulli}(p_{0})$ under $H_{0}$.\n\nThe chosen test statistic is the total number of defectives,\n$$\nT = \\sum_{i=1}^{n} X_{i}.\n$$\nA fundamental result for sums of independent Bernoulli random variables is that the sum has a Binomial distribution. Therefore, under $H_{0}$,\n$$\nT \\sim \\text{Binomial}(n, p_{0}),\n$$\nwith the exact probability mass function\n$$\n\\Pr(T = k \\mid H_{0}) = \\binom{n}{k} p_{0}^{k} (1 - p_{0})^{n-k}, \\quad k = 0, 1, \\ldots, n.\n$$\n\nThis establishes that the exact distribution of $T$ under $H_{0}$ is Binomial with parameters $n$ and $p_{0}$, which matches option C. For completeness regarding the other options: option A standardizes $T$ to use a normal approximation via the central limit theorem, which is not exact; option B uses a Poisson approximation valid when $n$ is large and $p_{0}$ is small, which is not exact; option D uses the sample proportion whose exact distribution is a scaled Binomial, not Bernoulli; option E refers to a Geometric distribution appropriate for the number of trials until the first defect, which does not correspond to $T$ as defined.", "answer": "$$\\boxed{C}$$", "id": "1958165"}, {"introduction": "Beyond testing a single parameter, we often need to assess how well a theoretical probability distribution fits our observed data. The chi-squared ($\\chi^2$) goodness-of-fit test provides a robust method for this by quantifying the discrepancy between observed frequencies and the frequencies expected under a hypothesized model. In this hands-on problem [@problem_id:1958160], you will calculate the $\\chi^2$ test statistic from real-world count data, a fundamental skill for validating statistical models in any scientific discipline.", "problem": "A network administrator for a popular video streaming service is analyzing traffic patterns during peak hours. The administrator hypothesizes that the number of new user-session requests arriving per second follows a Poisson distribution with a mean of $\\lambda = 3.0$. To test this hypothesis, the administrator records the number of requests in 200 distinct one-second intervals. The observed frequencies are summarized in the table below:\n\n| Number of Requests per Second | Observed Frequency (Number of Intervals) |\n|:-----------------------------:|:----------------------------------------:|\n| 0                             | 8                                        |\n| 1                             | 32                                       |\n| 2                             | 45                                       |\n| 3                             | 50                                       |\n| 4                             | 35                                       |\n| 5                             | 20                                       |\n| 6 or more                     | 10                                       |\n\nUsing this data, calculate the value of the chi-squared ($\\chi^2$) goodness-of-fit test statistic to evaluate the administrator's hypothesis. Round your final answer to three significant figures.", "solution": "We test a Poisson model with mean $\\lambda=3$ for $N=200$ one-second intervals. For a Poisson($\\lambda$) random variable $X$, the probabilities are $p_{k}=\\Pr(X=k)=\\frac{\\exp(-\\lambda)\\lambda^{k}}{k!}$. The expected frequency in each category is $E_{k}=N\\,p_{k}$, and for the grouped tail $k \\ge 6$ we use $p_{\\ge 6}=1-\\sum_{k=0}^{5}p_{k}$ with $E_{\\ge 6}=N\\,p_{\\ge 6}$. With $\\lambda=3$,\n$$\n\\begin{aligned}\np_{0}&=\\exp(-3),\\\\\np_{1}&=3\\exp(-3),\\\\\np_{2}&=\\frac{3^{2}}{2}\\exp(-3),\\\\\np_{3}&=\\frac{3^{3}}{3!}\\exp(-3)=p_{2},\\\\\np_{4}&=\\frac{3}{4}p_{3},\\\\\np_{5}&=\\frac{3}{5}p_{4},\\\\\np_{\\geq 6}&=1-\\sum_{k=0}^{5}p_{k}.\n\\end{aligned}\n$$\nNumerically,\n$$\n\\begin{aligned}\n&p_{0}\\approx 0.0497871,\\quad p_{1}\\approx 0.149361,\\quad p_{2}\\approx 0.224042,\\quad p_{3}\\approx 0.224042,\\\\\n&p_{4}\\approx 0.168031,\\quad p_{5}\\approx 0.100819,\\quad p_{\\geq 6}\\approx 0.0839179,\n\\end{aligned}\n$$\nso\n$$\n\\begin{aligned}\n&E_{0}\\approx 9.95741,\\; E_{1}\\approx 29.87224,\\; E_{2}\\approx 44.80836,\\; E_{3}\\approx 44.80836,\\\\\n&E_{4}\\approx 33.60627,\\; E_{5}\\approx 20.16376,\\; E_{\\geq 6}\\approx 16.78359,\n\\end{aligned}\n$$\nwhich sum to $200$ as required. The chi-squared statistic is\n$$\n\\chi^{2}=\\sum_{\\text{cats}}\\frac{(O-E)^{2}}{E},\n$$\nwith observed counts $(O_{0},O_{1},O_{2},O_{3},O_{4},O_{5},O_{\\geq 6})=(8,32,45,50,35,20,10)$. Computing each term:\n$$\n\\begin{aligned}\n&\\frac{(8-9.95741)^{2}}{9.95741}\\approx 0.3847,\\quad \\frac{(32-29.87224)^{2}}{29.87224}\\approx 0.1516,\\\\\n&\\frac{(45-44.80836)^{2}}{44.80836}\\approx 0.000820,\\quad \\frac{(50-44.80836)^{2}}{44.80836}\\approx 0.6013,\\\\\n&\\frac{(35-33.60627)^{2}}{33.60627}\\approx 0.0578,\\quad \\frac{(20-20.16376)^{2}}{20.16376}\\approx 0.00133,\\\\\n&\\frac{(10-16.78359)^{2}}{16.78359}\\approx 2.7406.\n\\end{aligned}\n$$\nSumming yields\n$$\n\\chi^{2}\\approx 3.94,\n$$\nwhich, rounded to three significant figures, is $3.94$.", "answer": "$$\\boxed{3.94}$$", "id": "1958160"}, {"introduction": "Statistical tests are not infallible; their conclusions are dependent on both the underlying data and the assumptions of the test itself. This practice [@problem_id:1954966] moves from calculation to conceptual understanding, exploring how a test statistic behaves under non-ideal conditions. Using a thought experiment involving the Shapiro-Wilk test for normality, you will develop an intuition for how a single extreme outlier can impact the test statistic $W$ and its p-value, underscoring the critical importance of data inspection before formal testing.", "problem": "A data analyst is working with a small dataset of 15 measurements, which are believed to have been sampled from a normally distributed population. To verify this assumption, the analyst performs a Shapiro-Wilk test. The test is a formal statistical test for normality, where the null hypothesis is that the sample data comes from a normally distributed population. The test statistic, denoted as $W$, ranges from 0 to 1. A value of $W$ close to 1 supports the null hypothesis (i.e., indicates normality), while a value closer to 0 provides evidence against it.\n\nInitially, the dataset appears to conform well to normality. Upon review, the analyst discovers a data entry error. One of the measurements, which was near the sample mean, is corrected. The corrected value is an extreme outlier, far from the other 14 data points. The analyst reruns the Shapiro-Wilk test on this new, corrected dataset containing the outlier.\n\nHow will the presence of this single, extreme outlier most likely affect the value of the Shapiro-Wilk test statistic $W$ and its corresponding p-value, compared to their values from the initial, well-behaved dataset?\n\nA. Both the statistic $W$ and the p-value will increase.\n\nB. The statistic $W$ will increase, while the p-value will decrease.\n\nC. The statistic $W$ will decrease, while the p-value will increase.\n\nD. Both the statistic $W$ and the p-value will decrease.", "solution": "The Shapiro-Wilk test assesses normality using the statistic $W$, defined for a sample of size $n$ with order statistics $x_{(1)} \\leq \\cdots \\leq x_{(n)}$ by\n$$\nW \\;=\\; \\frac{\\left(\\sum_{i=1}^{m} a_{i}\\,\\big(x_{(n+1-i)} - x_{(i)}\\big)\\right)^{2}}{\\sum_{i=1}^{n} \\big(x_{i} - \\bar{x}\\big)^{2}}, \\quad m \\;=\\; \\left\\lfloor \\frac{n}{2} \\right\\rfloor,\n$$\nwhere the constants $a_{i}$ depend only on $n$ and the expected order statistics of a standard normal sample. Under $H_{0}$ (normality), $W$ tends to be close to $1$; smaller values of $W$ indicate departures from normality. The $p$-value is $p = \\mathbb{P}\\!\\left(W \\leq w_{\\text{obs}} \\mid H_{0}\\right)$, which is a decreasing function of $w_{\\text{obs}}$.\n\nConsider the effect of replacing one observation near the mean by a single extreme outlier. Let the corrected largest observation be $x_{(n)}$ with very large magnitude while the remaining $n-1$ observations stay near their original values. Analyze numerator and denominator:\n\n1) Denominator effect:\n$$\nS^{2} \\;=\\; \\sum_{i=1}^{n} \\big(x_{i} - \\bar{x}\\big)^{2}.\n$$\nWith one extreme outlier, the sample mean $\\bar{x}$ shifts by an amount of order $\\frac{x_{(n)}}{n}$, and the term $\\big(x_{(n)} - \\bar{x}\\big)^{2}$ dominates the sum, so $S^{2}$ grows on the order of $\\big(x_{(n)} - \\bar{x}\\big)^{2}$, which is large.\n\n2) Numerator effect:\n$$\nN \\;=\\; \\sum_{i=1}^{m} a_{i}\\,\\big(x_{(n+1-i)} - x_{(i)}\\big).\n$$\nOnly the term with $i=1$ contains $x_{(n)}$; all other terms involve unaffected order statistics and remain bounded. Hence,\n$$\nN \\;=\\; a_{1}\\big(x_{(n)} - x_{(1)}\\big) \\;+\\; C,\n$$\nwhere $C$ is bounded with respect to $x_{(n)}$. Therefore $N$ grows at most linearly in $x_{(n)}$, and $N^{2}$ grows on the order of $x_{(n)}^{2}$.\n\n3) Combined effect on $W$:\n$$\nW \\;=\\; \\frac{N^{2}}{S^{2}} \\;\\approx\\; \\frac{a_{1}^{2}\\,x_{(n)}^{2}}{\\big(x_{(n)} - \\bar{x}\\big)^{2}} \\;\\to\\; a_{1}^{2},\n$$\nas $|x_{(n)}|$ becomes very large, where $0 < a_{1}^{2} < 1$. Since, for a well-behaved normal-like sample, $W$ is typically close to $1$, introducing a single extreme outlier will reduce $W$ markedly from near $1$ toward a smaller constant determined by $a_{1}$, i.e., $W$ decreases.\n\nBecause the $p$-value is $p = \\mathbb{P}\\!\\left(W \\leq w_{\\text{obs}} \\mid H_{0}\\right)$ and is monotonically decreasing in $w_{\\text{obs}}$, a decrease in $W$ results in a decrease in $p$ as well. Thus, the presence of a single extreme outlier makes $W$ smaller and its corresponding $p$-value smaller compared to the initial, well-behaved dataset.\n\nTherefore, both $W$ and its $p$-value decrease.", "answer": "$$\\boxed{D}$$", "id": "1954966"}]}