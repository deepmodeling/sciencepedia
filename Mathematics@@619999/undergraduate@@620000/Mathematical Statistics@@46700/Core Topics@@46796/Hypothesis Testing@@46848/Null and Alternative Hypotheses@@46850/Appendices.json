{"hands_on_practices": [{"introduction": "This exercise walks through a classic hypothesis test from beginning to end, grounding the abstract theory in a concrete engineering problem. We move beyond the familiar normal distribution to test a hypothesis about the mean time to failure ($MTTF$) of components whose lifetimes follow an exponential distribution, a common model in reliability analysis. This practice [@problem_id:1940676] will solidify your understanding of the core workflow: stating hypotheses, calculating a test statistic, and making a decision by comparing it to a given critical value.", "problem": "An aerospace engineering team is developing a new type of radioisotope thermoelectric generator for a long-duration deep-space mission. The design specification states that the mean time to failure (MTTF) of these generators must be greater than 5000 hours. To verify this, the team conducts a life-test on a random sample of $n=10$ prototype generators. The lifetimes of these generators are assumed to be independent and to follow an exponential distribution. The test is run until all 10 generators fail, and the sum of their lifetimes is recorded as 60,000 hours.\n\nThe team decides to perform a hypothesis test with the null hypothesis that the true MTTF is exactly 5000 hours, against the alternative hypothesis that it is greater than 5000 hours. The test is conducted at a significance level of $\\alpha = 0.05$. For the purpose of this problem, the critical value for the appropriate test statistic at the given significance level is 31.410. A test statistic value greater than this critical value would lead to the rejection of the null hypothesis.\n\nBased on the experimental data, which of the following conclusions is correct?\n\nA. The team should reject the null hypothesis.\n\nB. The team should fail to reject the null hypothesis.\n\nC. The results are inconclusive because the sample size is too small to draw a meaningful conclusion.\n\nD. The test cannot be performed because both the population mean and variance are unknown parameters.\n\nE. The team should accept the null hypothesis as proven true.", "solution": "The problem asks us to perform a hypothesis test for the mean of an exponential distribution. Let the random variable $X$ represent the lifetime of a generator. We are given that $X$ follows an exponential distribution. The probability density function (PDF) of an exponential distribution can be parameterized by its mean $\\mu$, or by its rate parameter $\\lambda = 1/\\mu$. The PDF in terms of $\\mu$ is $f(x; \\mu) = \\frac{1}{\\mu} \\exp(-\\frac{x}{\\mu})$ for $x \\ge 0$.\n\nFirst, we state the null and alternative hypotheses in terms of the mean time to failure (MTTF), $\\mu$.\nThe null hypothesis ($H_0$) is that the MTTF is exactly 5000 hours.\n$$H_0: \\mu = 5000$$\nThe alternative hypothesis ($H_a$) is that the MTTF is greater than 5000 hours.\n$$H_a: \\mu > 5000$$\n\nFor an exponential distribution, a powerful and commonly used test statistic is based on the sum of the observations. Let $X_1, X_2, \\dots, X_n$ be the lifetimes of the $n$ generators in the sample. A key result in mathematical statistics states that if $X_i$ are independent and identically distributed as $\\text{Exp}(\\lambda)$, where $\\lambda=1/\\mu$, then the quantity $2\\lambda \\sum_{i=1}^n X_i$ follows a chi-squared distribution with $2n$ degrees of freedom, i.e., $2\\lambda \\sum_{i=1}^n X_i \\sim \\chi^2_{2n}$.\n\nWe construct our test based on this property. Under the null hypothesis, the mean is $\\mu_0 = 5000$, which corresponds to a rate parameter of $\\lambda_0 = 1/\\mu_0 = 1/5000$. So, under $H_0$, the test statistic\n$$S = 2\\lambda_0 \\sum_{i=1}^n X_i = \\frac{2}{\\mu_0} \\sum_{i=1}^n X_i$$\nfollows a chi-squared distribution with $2n$ degrees of freedom. In our case, the sample size is $n=10$, so the degrees of freedom are $2n = 2(10) = 20$.\n\nThe alternative hypothesis is $H_a: \\mu > 5000$. A larger sample mean, $\\bar{X} = (\\sum X_i)/n$, provides evidence in favor of $H_a$. A large sample mean corresponds to a large sum $\\sum X_i$, which in turn corresponds to a large value of the test statistic $S$. Therefore, we have a right-tailed test. We will reject $H_0$ if the observed value of our statistic, $s_{obs}$, is greater than a certain critical value. The problem provides this critical value as 31.410 for a significance level of $\\alpha = 0.05$.\n\nNow, we calculate the observed value of the test statistic, $s_{obs}$, using the given data:\nSample size, $n=10$.\nSum of lifetimes, $\\sum_{i=1}^{10} x_i = 60,000$ hours.\nNull hypothesis mean, $\\mu_0 = 5000$ hours.\n\n$$s_{obs} = \\frac{2}{\\mu_0} \\sum_{i=1}^{10} x_i = \\frac{2}{5000} \\times 60,000 = \\frac{120,000}{5000} = 24$$\n\nFinally, we compare the observed statistic to the critical value.\nObserved statistic: $s_{obs} = 24$.\nCritical value (from the problem statement): $c = 31.410$.\n\nSince $s_{obs} = 24$ is not greater than the critical value of $31.410$, the test statistic does not fall into the rejection region. Therefore, we do not have sufficient evidence at the $\\alpha = 0.05$ significance level to reject the null hypothesis. The correct conclusion is to \"fail to reject the null hypothesis\".\n\nLet's evaluate the given options:\nA. The team should reject the null hypothesis. This is incorrect.\nB. The team should fail to reject the null hypothesis. This is the correct conclusion.\nC. The results are inconclusive because the sample size is too small. This is a misinterpretation. Hypothesis testing provides a definite conclusion (reject or fail to reject) based on the chosen framework.\nD. The test cannot be performed because both the population mean and variance are unknown. This is incorrect. The test is specifically designed for this situation. For an exponential distribution, the variance is $\\mu^2$, so knowing the mean determines the variance. The test makes an assumption about the mean under the null hypothesis to proceed.\nE. The team should accept the null hypothesis as proven true. This is a common logical fallacy. Failing to reject the null hypothesis simply means there is not enough evidence to discard it, not that it has been proven true.\n\nThus, the only correct statement is B.", "answer": "$$\\boxed{B}$$", "id": "1940676"}, {"introduction": "The correct formulation of the null ($H_0$) and alternative ($H_1$) hypotheses is the most critical part of a statistical test, as it defines the question being asked and places the burden of proof. This problem [@problem_id:2410302] challenges you to think beyond simple superiority tests (e.g., is the new method better?) and correctly frame the hypotheses for a non-inferiority trial. Understanding this setup is crucial in fields like medicine and technology, where the goal is often to demonstrate that a new, perhaps cheaper or faster, method is \"not unacceptably worse\" than an established gold standard.", "problem": "A bioinformatics core facility is evaluating a new sequencing-by-synthesis chemistry against an Illumina platform using matched libraries from the same reference sample. For each method, they compute the per-base error rate, defined as the probability that a called base does not match the trusted reference after alignment and variant masking. Let $e_{\\text{new}}$ denote the new chemistry’s error rate and $e_{\\text{Illumina}}$ denote Illumina’s error rate. Because lower error rates are better, the facility pre-specifies a non-inferiority margin $\\Delta \\gt 0$ as the maximum acceptable absolute increase in error rate for the new chemistry relative to Illumina that would still be considered “not unacceptably worse” for downstream computational biology pipelines.\n\nUsing the core definitions of hypothesis testing, where the null hypothesis should represent the boundary of performance that would be unacceptable to falsely declare as non-inferior, and the alternative hypothesis represents the claim the study aims to establish, select the correct pair of null and alternative hypotheses for a non-inferiority test based on the difference $\\delta = e_{\\text{new}} - e_{\\text{Illumina}}$.\n\nOptions:\n\nA. $H_{0}: e_{\\text{new}} - e_{\\text{Illumina}} \\ge \\Delta$ and $H_{1}: e_{\\text{new}} - e_{\\text{Illumina}} \\lt \\Delta$\n\nB. $H_{0}: e_{\\text{new}} - e_{\\text{Illumina}} \\le -\\Delta$ and $H_{1}: e_{\\text{new}} - e_{\\text{Illumina}} \\gt -\\Delta$\n\nC. $H_{0}: \\lvert e_{\\text{new}} - e_{\\text{Illumina}} \\rvert \\ge \\Delta$ and $H_{1}: \\lvert e_{\\text{new}} - e_{\\text{Illumina}} \\rvert \\lt \\Delta$\n\nD. $H_{0}: e_{\\text{new}} - e_{\\text{Illumina}} \\ge 0$ and $H_{1}: e_{\\text{new}} - e_{\\text{Illumina}} \\lt 0$\n\nE. $H_{0}: e_{\\text{new}} - e_{\\text{Illumina}} \\le \\Delta$ and $H_{1}: e_{\\text{new}} - e_{\\text{Illumina}} \\gt \\Delta$", "solution": "The problem requires the correct formulation of null and alternative hypotheses for a non-inferiority test in a bioinformatics context. The validation of the problem statement must precede any attempt at a solution.\n\n**Problem Validation**\n\n**Step 1: Extracted Givens**\n- The context is a comparison of a new sequencing chemistry against an Illumina platform.\n- The performance metric is the per-base error rate, denoted $e_{\\text{new}}$ for the new chemistry and $e_{\\text{Illumina}}$ for the Illumina platform.\n- The goal is to determine if the new chemistry is non-inferior, where lower error rates are preferable.\n- A non-inferiority margin, $\\Delta > 0$, is defined as the maximum acceptable absolute increase in error rate for the new chemistry.\n- The condition for the new chemistry to be \"not unacceptably worse\" (i.e., non-inferior) is that the increase in its error rate is not more than $\\Delta$.\n- The hypothesis test is to be based on the difference $\\delta = e_{\\text{new}} - e_{\\text{Illumina}}$.\n- The null hypothesis ($H_{0}$) must represent the boundary of unacceptable performance. A false declaration of non-inferiority (a Type I error) must be controlled.\n- The alternative hypothesis ($H_{1}$) must represent the claim the study aims to establish, which is non-inferiority.\n\n**Step 2: Validation of Givens**\nThe problem is scientifically and logically sound. It presents a standard, well-defined scenario in biostatistics and bioinformatics for a non-inferiority trial. The definitions of error rate, non-inferiority margin, and the roles of the null and alternative hypotheses are consistent with established statistical theory. The problem is self-contained, unambiguous, and allows for a unique, correct solution based on fundamental principles of hypothesis testing. There are no factual errors, contradictions, or instances of pseudoscience.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be derived.\n\n**Derivation of Solution**\n\nThe fundamental principle of frequentist hypothesis testing is to formulate a null hypothesis ($H_{0}$) that represents a default state or a condition of \"no effect\" or \"no difference\" (or in this case, \"inferiority\"), which one seeks to disprove. The alternative hypothesis ($H_{1}$) represents the state for which one seeks to find evidence. The burden of proof lies in gathering sufficient evidence to reject $H_{0}$ in favor of $H_{1}$.\n\n1.  **Define the States of Interest**:\n    - **Inferiority (Unacceptable Performance)**: The new chemistry is unacceptably worse than Illumina if its error rate exceeds Illumina's by more than or equal to the margin $\\Delta$. In mathematical terms, this is $e_{\\text{new}} - e_{\\text{Illumina}} \\ge \\Delta$. Using the notation $\\delta = e_{\\text{new}} - e_{\\text{Illumina}}$, this is $\\delta \\ge \\Delta$.\n    - **Non-Inferiority (Acceptable Performance)**: The new chemistry is considered non-inferior if its error rate does not exceed Illumina's by the margin $\\Delta$. Mathematically, this is $e_{\\text{new}} - e_{\\text{Illumina}} < \\Delta$ (or at the limit, $e_{\\text{new}} - e_{\\text{Illumina}} \\le \\Delta$). This is the condition the study aims to establish. Using $\\delta$, this is $\\delta < \\Delta$.\n\n2.  **Formulate the Hypotheses**:\n    - **Alternative Hypothesis ($H_{1}$)**: The problem states that $H_{1}$ is the claim the study aims to establish. The goal is to prove non-inferiority. Therefore, the alternative hypothesis must represent the state of non-inferiority. This is expressed as the strict inequality:\n      $$H_{1}: e_{\\text{new}} - e_{\\text{Illumina}} < \\Delta$$\n      or\n      $$H_{1}: \\delta < \\Delta$$\n    - **Null Hypothesis ($H_{0}$)**: The null hypothesis is the logical complement of the alternative hypothesis. It represents the state that is assumed to be true unless contradicted by data. This is the state of inferiority.\n      $$H_{0}: e_{\\text{new}} - e_{\\text{Illumina}} \\ge \\Delta$$\n      or\n      $$H_{0}: \\delta \\ge \\Delta$$\n    This formulation correctly places the burden of proof. To conclude non-inferiority, we must collect enough evidence to reject the null hypothesis that the new chemistry is, in fact, inferior. Falsely rejecting this $H_{0}$ would mean declaring an inferior method as non-inferior, which is the precise Type I error the experimental design seeks to control. The boundary of this unacceptable region is $\\delta = \\Delta$, which is included in the null hypothesis.\n\n**Evaluation of Options**\n\n-   **A. $H_{0}: e_{\\text{new}} - e_{\\text{Illumina}} \\ge \\Delta$ and $H_{1}: e_{\\text{new}} - e_{\\text{Illumina}} \\lt \\Delta$**:\n    This formulation perfectly matches our derivation. $H_{0}$ represents the state of inferiority (the difference in error rates is at least the unacceptable margin $\\Delta$), and $H_{1}$ represents the state of non-inferiority (the difference is less than $\\Delta$). This is the canonical structure for a non-inferiority test where lower values of the metric (error rate) are better.\n    **Verdict: Correct.**\n\n-   **B. $H_{0}: e_{\\text{new}} - e_{\\text{Illumina}} \\le -\\Delta$ and $H_{1}: e_{\\text{new}} - e_{\\text{Illumina}} \\gt -\\Delta$**:\n    This tests whether the new method is significantly *better* than the old one, but not by a margin $\\Delta$. The term $-\\Delta$ is nonsensical here. If the new method were better, the difference $e_{\\text{new}} - e_{\\text{Illumina}}$ would be negative. This structure might relate to a non-superiority test, but it is incorrectly formulated for the stated problem of non-inferiority.\n    **Verdict: Incorrect.**\n\n-   **C. $H_{0}: \\lvert e_{\\text{new}} - e_{\\text{Illumina}} \\rvert \\ge \\Delta$ and $H_{1}: \\lvert e_{\\text{new}} - e_{\\text{Illumina}} \\rvert \\lt \\Delta$**:\n    This is the formulation for an **equivalence test**. It aims to demonstrate that the two methods are \"close enough\" by showing that the absolute difference between their error rates is smaller than $\\Delta$. This is a two-sided test. The problem describes a **non-inferiority test**, which is one-sided, as we are only concerned with the new method being unacceptably *worse*, not unacceptably *different* in either direction.\n    **Verdict: Incorrect.**\n\n-   **D. $H_{0}: e_{\\text{new}} - e_{\\text{Illumina}} \\ge 0$ and $H_{1}: e_{\\text{new}} - e_{\\text{Illumina}} \\lt 0$**:\n    This is the formulation for a **superiority test**. It aims to prove that the new chemistry is strictly *better* (has a lower error rate) than the Illumina platform. This is equivalent to a non-inferiority test with a margin of $\\Delta = 0$. However, the problem explicitly states that $\\Delta > 0$, allowing the new method to be slightly worse and still be considered acceptable.\n    **Verdict: Incorrect.**\n\n-   **E. $H_{0}: e_{\\text{new}} - e_{\\text{Illumina}} \\le \\Delta$ and $H_{1}: e_{\\text{new}} - e_{\\text{Illumina}} \\gt \\Delta$**:\n    This formulation incorrectly swaps the null and alternative hypotheses. Here, the null hypothesis is non-inferiority, and the alternative is inferiority. In the frequentist paradigm, one does not seek to \"prove\" the null hypothesis. The burden of proof is to reject the null. Following this setup, to claim non-inferiority, one would have to *fail to reject* $H_{0}$. This is a methodologically weak conclusion and violates the principle that the claim to be established should be the alternative hypothesis.\n    **Verdict: Incorrect.**\n\nBased on this rigorous analysis, only option A correctly represents the hypothesis test for non-inferiority as described.", "answer": "$$\\boxed{A}$$", "id": "2410302"}, {"introduction": "In many real-world scenarios, a pre-packaged test for your specific hypothesis may not be available. This advanced exercise [@problem_id:1940664] guides you through the process of constructing a test from first principles using the powerful framework of the Generalized Log-Likelihood Ratio Test (GLRT). You will derive the analytical form of the test statistic needed to detect a single, abrupt change in the mean of a sequence of observations, a fundamental problem in areas ranging from financial analysis to system monitoring.", "problem": "A systems engineer is monitoring the daily average latency of a critical server. The latency measurements on consecutive days are recorded as a sequence of random variables $X_1, X_2, \\ldots, X_n$. It is assumed that these measurements are independent and drawn from a normal distribution with a known variance $\\sigma^2$, so that $X_i \\sim N(\\mu_i, \\sigma^2)$ for $i=1, \\ldots, n$.\n\nThe engineer wants to detect if a single, abrupt change in the server's performance occurred during the observation period. This corresponds to a change in the mean latency, $\\mu_i$. The problem is framed as a hypothesis test between a \"stable system\" null hypothesis and a \"single change-point\" alternative hypothesis:\n\n- **Null Hypothesis ($H_0$):** The system is stable, meaning the mean latency is constant over the period. The population means are all equal to some unknown constant $\\mu$.\n$$H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_n = \\mu$$\n\n- **Alternative Hypothesis ($H_A$):** A single change occurred at an unknown time $\\tau \\in \\{1, \\dots, n-1\\}$. The population mean is $\\theta_1$ up to time $\\tau$ and changes to $\\theta_2$ after time $\\tau$, where $\\theta_1 \\neq \\theta_2$. The parameters $\\theta_1$, $\\theta_2$, and the change-point $\\tau$ are all unknown.\n$$H_A: \\text{There exists } \\tau \\in \\{1, \\dots, n-1\\} \\text{ such that } \\mu_1 = \\dots = \\mu_\\tau = \\theta_1 \\text{ and } \\mu_{\\tau+1} = \\dots = \\mu_n = \\theta_2, \\text{ with } \\theta_1 \\neq \\theta_2$$\n\nTo perform this test, a Generalized Log-Likelihood Ratio Test (GLRT) statistic, denoted by $T_n$, is constructed. It is defined as $T_n = 2 \\left( \\sup_{H_A} \\ln L - \\sup_{H_0} \\ln L \\right)$, where $\\ln L$ is the log-likelihood function of the observations and the supremum is taken over all unknown parameters within the respective hypothesis.\n\nDerive the analytical expression for the statistic $T_n$. Your final answer should be expressed in terms of $n$, $\\sigma^2$, and the observations $X_i$.", "solution": "The joint density under independent Gaussian measurements with known variance $\\sigma^{2}$ is\n$$\nL(\\mu_{1},\\ldots,\\mu_{n})=(2\\pi\\sigma^{2})^{-n/2}\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(X_{i}-\\mu_{i})^{2}\\right),\n$$\nso the log-likelihood is\n$$\n\\ln L(\\mu_{1},\\ldots,\\mu_{n})=-\\frac{n}{2}\\ln(2\\pi\\sigma^{2})-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(X_{i}-\\mu_{i})^{2}.\n$$\n\nUnder $H_{0}$, $\\mu_{i}=\\mu$ for all $i$. The maximizer satisfies\n$$\n\\frac{\\partial}{\\partial \\mu}\\ln L=\\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}(X_{i}-\\mu)=0\n\\;\\;\\Longrightarrow\\;\\;\n\\hat{\\mu}=\\bar{X}_{n}:=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}.\n$$\nHence\n$$\n\\sup_{H_{0}}\\ln L=-\\frac{n}{2}\\ln(2\\pi\\sigma^{2})-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(X_{i}-\\bar{X}_{n})^{2}.\n$$\n\nUnder $H_{A}$ with a fixed change-point $\\tau\\in\\{1,\\ldots,n-1\\}$, we have $\\mu_{1}=\\cdots=\\mu_{\\tau}=\\theta_{1}$ and $\\mu_{\\tau+1}=\\cdots=\\mu_{n}=\\theta_{2}$. The maximizers solve\n$$\n\\frac{\\partial}{\\partial \\theta_{1}}\\ln L=\\frac{1}{\\sigma^{2}}\\sum_{i=1}^{\\tau}(X_{i}-\\theta_{1})=0,\\quad\n\\frac{\\partial}{\\partial \\theta_{2}}\\ln L=\\frac{1}{\\sigma^{2}}\\sum_{i=\\tau+1}^{n}(X_{i}-\\theta_{2})=0,\n$$\nso\n$$\n\\hat{\\theta}_{1}=\\bar{X}_{1:\\tau}:=\\frac{1}{\\tau}\\sum_{i=1}^{\\tau}X_{i},\\qquad\n\\hat{\\theta}_{2}=\\bar{X}_{\\tau+1:n}:=\\frac{1}{n-\\tau}\\sum_{i=\\tau+1}^{n}X_{i}.\n$$\nThe corresponding maximized log-likelihood for this $\\tau$ is\n$$\n\\sup_{\\theta_{1},\\theta_{2}}\\ln L=-\\frac{n}{2}\\ln(2\\pi\\sigma^{2})-\\frac{1}{2\\sigma^{2}}\\left[\\sum_{i=1}^{\\tau}(X_{i}-\\bar{X}_{1:\\tau})^{2}+\\sum_{i=\\tau+1}^{n}(X_{i}-\\bar{X}_{\\tau+1:n})^{2}\\right].\n$$\nMaximizing over $\\tau$ is equivalent to minimizing the bracketed within-group sum of squares, hence\n$$\n\\sup_{H_{A}}\\ln L=-\\frac{n}{2}\\ln(2\\pi\\sigma^{2})-\\frac{1}{2\\sigma^{2}}\\min_{1\\le \\tau\\le n-1}\\left[\\sum_{i=1}^{\\tau}(X_{i}-\\bar{X}_{1:\\tau})^{2}+\\sum_{i=\\tau+1}^{n}(X_{i}-\\bar{X}_{\\tau+1:n})^{2}\\right].\n$$\n\nTherefore, the GLRT statistic is\n$$\nT_{n}=2\\left(\\sup_{H_{A}}\\ln L-\\sup_{H_{0}}\\ln L\\right)\n=\\frac{1}{\\sigma^{2}}\\left\\{\\sum_{i=1}^{n}(X_{i}-\\bar{X}_{n})^{2}-\\min_{1\\le \\tau\\le n-1}\\left[\\sum_{i=1}^{\\tau}(X_{i}-\\bar{X}_{1:\\tau})^{2}+\\sum_{i=\\tau+1}^{n}(X_{i}-\\bar{X}_{\\tau+1:n})^{2}\\right]\\right\\}.\n$$\n\nUsing the ANOVA identity\n$$\n\\sum_{i=1}^{n}(X_{i}-\\bar{X}_{n})^{2}-\\left[\\sum_{i=1}^{\\tau}(X_{i}-\\bar{X}_{1:\\tau})^{2}+\\sum_{i=\\tau+1}^{n}(X_{i}-\\bar{X}_{\\tau+1:n})^{2}\\right]\n=\\frac{\\tau(n-\\tau)}{n}\\left(\\bar{X}_{1:\\tau}-\\bar{X}_{\\tau+1:n}\\right)^{2},\n$$\nwe obtain the explicit maximization form\n$$\nT_{n}=\\frac{1}{\\sigma^{2}}\\max_{1\\le \\tau\\le n-1}\\left\\{\\frac{\\tau(n-\\tau)}{n}\\left(\\bar{X}_{1:\\tau}-\\bar{X}_{\\tau+1:n}\\right)^{2}\\right\\}\n=\\frac{1}{\\sigma^{2}}\\max_{1\\le \\tau\\le n-1}\\left\\{\\frac{\\tau(n-\\tau)}{n}\\left(\\frac{1}{\\tau}\\sum_{i=1}^{\\tau}X_{i}-\\frac{1}{n-\\tau}\\sum_{i=\\tau+1}^{n}X_{i}\\right)^{2}\\right\\}.\n$$\nThis expression is in terms of $n$, $\\sigma^{2}$, and the observations $X_{i}$.", "answer": "$$\\boxed{\\frac{1}{\\sigma^{2}}\\max_{1\\le \\tau\\le n-1}\\left\\{\\frac{\\tau\\,(n-\\tau)}{n}\\left(\\frac{1}{\\tau}\\sum_{i=1}^{\\tau}X_{i}-\\frac{1}{n-\\tau}\\sum_{i=\\tau+1}^{n}X_{i}\\right)^{2}\\right\\}}$$", "id": "1940664"}]}