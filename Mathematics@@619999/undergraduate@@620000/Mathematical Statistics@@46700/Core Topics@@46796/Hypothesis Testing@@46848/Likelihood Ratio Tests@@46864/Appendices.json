{"hands_on_practices": [{"introduction": "Our first practice exercise grounds us in the fundamental mechanics of deriving a Likelihood Ratio Test (LRT) statistic. We will consider a scenario of modeling data packet processing times using a Gamma distribution, a common model for waiting times. This problem ([@problem_id:1930657]) provides a clear, step-by-step application of the LRT principle for a simple null hypothesis, requiring you to find the maximum likelihood estimates and construct the likelihood ratio, which illuminates how the test statistic is built upon a sufficient statistic.", "problem": "A telecommunications engineer is modeling the processing times of data packets at a network router. Based on the router's architecture, the time $X$ (in milliseconds) required to process a single packet is known to follow a Gamma distribution with a known shape parameter $\\alpha > 0$ and an unknown rate parameter $\\beta > 0$. The Probability Density Function (PDF) is given by:\n$$f(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x) \\quad \\text{for } x > 0$$\nThe engineer has collected a random sample of $n$ packet processing times, $X_1, X_2, \\dots, X_n$, which can be considered independent and identically distributed. She wants to test whether the router is operating under a nominal load, which corresponds to a specific rate parameter $\\beta = \\beta_0$, against the alternative that it is not. The formal hypotheses are:\n$$H_0: \\beta = \\beta_0$$\n$$H_1: \\beta \\neq \\beta_0$$\nDerive the Likelihood Ratio Test (LRT) statistic, $\\lambda(x_1, \\dots, x_n)$, for this hypothesis test. Express your answer as a function of the sample size $n$, the known shape parameter $\\alpha$, the hypothesized rate $\\beta_0$, and the sample mean $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$.", "solution": "The joint likelihood for independent observations $X_{1},\\dots,X_{n}$ from the Gamma$(\\alpha,\\beta)$ distribution with known $\\alpha$ and unknown $\\beta$ is\n$$\nL(\\beta \\mid x_{1},\\dots,x_{n})=\\prod_{i=1}^{n}\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x_{i}^{\\alpha-1}\\exp(-\\beta x_{i})\n=\\frac{\\beta^{n\\alpha}}{\\Gamma(\\alpha)^{n}}\\left(\\prod_{i=1}^{n}x_{i}^{\\alpha-1}\\right)\\exp\\!\\left(-\\beta\\sum_{i=1}^{n}x_{i}\\right).\n$$\nLet $S=\\sum_{i=1}^{n}x_{i}=n\\bar{x}$. The log-likelihood is\n$$\n\\ell(\\beta)=n\\alpha\\ln\\beta-n\\ln\\Gamma(\\alpha)+(\\alpha-1)\\sum_{i=1}^{n}\\ln x_{i}-\\beta S.\n$$\nMaximizing over $\\beta>0$ gives the unrestricted MLE via\n$$\n\\frac{\\partial \\ell}{\\partial \\beta}=\\frac{n\\alpha}{\\beta}-S=0\\quad\\Rightarrow\\quad \\hat{\\beta}=\\frac{n\\alpha}{S}=\\frac{\\alpha}{\\bar{x}}.\n$$\nThe likelihood ratio statistic is\n$$\n\\lambda(x_{1},\\dots,x_{n})=\\frac{\\sup_{\\beta=\\beta_{0}}L(\\beta\\mid x)}{\\sup_{\\beta>0}L(\\beta\\mid x)}=\\frac{L(\\beta_{0}\\mid x)}{L(\\hat{\\beta}\\mid x)}.\n$$\nUsing the likelihood form, terms not depending on $\\beta$ cancel, yielding\n$$\n\\lambda(x_{1},\\dots,x_{n})=\\left(\\frac{\\beta_{0}}{\\hat{\\beta}}\\right)^{n\\alpha}\\exp\\!\\left(-(\\beta_{0}-\\hat{\\beta})S\\right).\n$$\nSubstituting $\\hat{\\beta}=\\alpha/\\bar{x}$ and $S=n\\bar{x}$ gives\n$$\n\\lambda(x_{1},\\dots,x_{n})=\\left(\\frac{\\beta_{0}\\bar{x}}{\\alpha}\\right)^{n\\alpha}\\exp\\!\\left(n\\alpha-n\\beta_{0}\\bar{x}\\right).\n$$\nThis expresses the LRT statistic in terms of $n$, $\\alpha$, $\\beta_{0}$, and $\\bar{x}$.", "answer": "$$\\boxed{\\left(\\frac{\\beta_{0}\\bar{x}}{\\alpha}\\right)^{n\\alpha}\\exp\\!\\left(n\\alpha-n\\beta_{0}\\bar{x}\\right)}$$", "id": "1930657"}, {"introduction": "Moving from a simple to a composite null hypothesis, our next exercise explores a classic A/B testing scenario, a cornerstone of data-driven decision-making. Here, we investigate whether a new algorithm improves user engagement, which translates to a one-sided hypothesis test for a Bernoulli parameter. This practice ([@problem_id:1930665]) is key to understanding how the LRT handles inequality constraints, revealing how the structure of the likelihood ratio naturally defines an intuitive, one-sided rejection region.", "problem": "A data science team at a major online streaming service is conducting an A/B test to determine if a newly developed recommendation algorithm is more effective than the current one. Historically, the probability that a user watches a movie recommended by the old algorithm is known to be $p_0 = 0.30$. The team hopes the new algorithm increases this probability.\n\nTo test this, they conduct an experiment on a random sample of $n$ users. For each user $i$, let the random variable $X_i=1$ if the user watches the movie recommended by the new algorithm, and $X_i=0$ otherwise. The observations $X_1, X_2, \\ldots, X_n$ are assumed to be a sequence of independent and identically distributed Bernoulli trials with an unknown success probability $p$.\n\nThe team sets up a formal hypothesis test to assess the evidence. The null hypothesis ($H_0$) is that the new algorithm is no better than the old one, while the alternative hypothesis ($H_1$) is that the new algorithm is an improvement. The hypotheses are formally stated as:\n$$ H_0: p \\le p_0 \\quad \\text{versus} \\quad H_1: p > p_0 $$\nLet $k = \\sum_{i=1}^{n} X_i$ be the total number of users in the sample who watched the recommended movie. The team will use a Likelihood Ratio Test (LRT) to make their decision.\n\nWhich of the following best describes the structure of the rejection region for an LRT of size $\\alpha$, where $k$ is the observed number of successes and $C, C_1, C_2$ are constants that depend on $n$, $p_0$, and $\\alpha$?\n\nA. The rejection region is of the form $\\{k : k < C_1 \\text{ or } k > C_2\\}$.\n\nB. The rejection region is of the form $\\{k : k < C\\}$.\n\nC. The rejection region is of the form $\\{k : k > C\\}$.\n\nD. The rejection region is of the form $\\{k : |k - np_0| > C \\}$.\n\nE. The rejection region is of the form $\\{k : (k/n) > p_0 \\text{ and } (k/n)/p_0 < C\\}$.", "solution": "We model $X_{1},\\ldots,X_{n}\\overset{\\text{iid}}{\\sim}\\text{Bernoulli}(p)$ and let $k=\\sum_{i=1}^{n}X_{i}$. The likelihood based on $k$ is, up to the factor $\\binom{n}{k}$ that does not depend on $p$,\n$$\nL(p;k)=p^{k}(1-p)^{n-k}.\n$$\nWe test $H_{0}:p\\le p_{0}$ versus $H_{1}:p>p_{0}$ via the likelihood ratio\n$$\n\\lambda(k)=\\frac{\\sup_{p\\le p_{0}}L(p;k)}{\\sup_{p\\in(0,1)}L(p;k)}.\n$$\nThe unconstrained MLE is obtained by maximizing $\\ln L(p;k)=k\\ln p+(n-k)\\ln(1-p)$:\n$$\n\\frac{\\partial}{\\partial p}\\ln L(p;k)=\\frac{k}{p}-\\frac{n-k}{1-p}=0\\;\\;\\Rightarrow\\;\\;\\hat p=\\frac{k}{n}.\n$$\nUnder $H_{0}$, the constrained MLE is $\\hat p_{0}=\\min\\!\\left\\{\\frac{k}{n},\\,p_{0}\\right\\}$. Therefore\n$$\n\\lambda(k)=\n\\begin{cases}\n1, & \\text{if } \\frac{k}{n}\\le p_{0},\\\\\n\\dfrac{p_{0}^{k}(1-p_{0})^{n-k}}{\\left(\\frac{k}{n}\\right)^{k}\\left(1-\\frac{k}{n}\\right)^{n-k}}, & \\text{if } \\frac{k}{n}>p_{0}.\n\\end{cases}\n$$\nTo determine the rejection region $\\{\\lambda(k)\\le c\\}$, analyze $\\lambda(k)$ for $\\frac{k}{n}>p_{0}$. Define $a=\\frac{k}{n}$. For $a>p_{0}$,\n$$\n\\ln \\lambda(k)=n\\Big[a\\ln p_{0}+(1-a)\\ln(1-p_{0})-a\\ln a-(1-a)\\ln(1-a)\\Big].\n$$\nDifferentiate with respect to $a$:\n$$\n\\frac{1}{n}\\frac{\\mathrm{d}}{\\mathrm{d}a}\\ln \\lambda(k)=\\ln p_{0}-\\ln(1-p_{0})-\\ln a+\\ln(1-a)=\\ln\\!\\left(\\frac{p_{0}}{1-p_{0}}\\cdot\\frac{1-a}{a}\\right).\n$$\nFor $a>p_{0}$ we have $\\frac{1-a}{a}<\\frac{1-p_{0}}{p_{0}}$, hence the logarithm is negative and $\\lambda(k)$ is strictly decreasing in $a$ (and thus in $k$) on $\\{k/n>p_{0}\\}$. Combined with $\\lambda(k)=1$ for $k/n\\le p_{0}$, the likelihood ratio test rejects for large $k$, i.e., for $k$ exceeding a cutoff $C$ chosen so that the size is $\\alpha$ (allowing for possible randomization at the boundary due to discreteness).\n\nTherefore, the LRT rejection region has the form $\\{k:k>C\\}$, which corresponds to option C. The other options are inappropriate: A and D are two-sided regions suited to $H_{1}:p\\ne p_{0}$, B is a lower-tail region suited to $H_{1}:p<p_{0}$, and E does not represent a standard LRT structure for this problem.", "answer": "$$\\boxed{C}$$", "id": "1930665"}, {"introduction": "Our final practice problem ventures into the important territory of non-regular statistical models, where the support of the distribution depends on the unknown parameter. We will derive an LRT for the parameter $\\theta$ of a Uniform$(0, \\theta)$ distribution, a case where standard asymptotic theories do not apply. This exercise ([@problem_id:1930681]) demonstrates the versatility of the likelihood principle, showing how it yields a test based on the sample maximum and allows for the construction of an exact rejection region, reinforcing the need to carefully consider model assumptions.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample from a Uniform distribution on the interval $(0, \\theta)$, where $\\theta > 0$ is an unknown parameter. A statistician wants to test the null hypothesis $H_0: \\theta = \\theta_0$ against the two-sided alternative hypothesis $H_1: \\theta \\ne \\theta_0$ using a test with a significance level of $\\alpha$, where $\\theta_0$ is a specified positive constant and $\\alpha \\in (0, 1)$.\n\nThe rejection region for the corresponding Likelihood Ratio Test (LRT) can be shown to be of the form $X_{(n)} \\le k_1$ or $X_{(n)} > k_2$, where $X_{(n)} = \\max(X_1, X_2, \\ldots, X_n)$ is the sample maximum. Under the null hypothesis, the probability of the event $X_{(n)} > \\theta_0$ is zero, so for the purpose of constructing the test, we can set $k_2 = \\theta_0$. Your task is to find the value of the other critical value, $k_1$.\n\nExpress $k_1$ as a closed-form analytic expression in terms of $n$, $\\theta_0$, and $\\alpha$.", "solution": "Let $M = X_{(n)} = \\max\\{X_{1},\\ldots,X_{n}\\}$. The likelihood for a sample $x=(x_{1},\\ldots,x_{n})$ from $\\operatorname{Unif}(0,\\theta)$ is\n$$\nL(\\theta\\mid x)=\\theta^{-n}\\,\\mathbf{1}\\{M\\le \\theta\\}.\n$$\nThe unrestricted supremum of the likelihood over $\\theta>0$ occurs at $\\theta=M$, giving\n$$\n\\sup_{\\theta>0}L(\\theta\\mid x)=M^{-n}.\n$$\nUnder $H_{0}:\\theta=\\theta_{0}$,\n$$\nL(\\theta_{0}\\mid x)=\\theta_{0}^{-n}\\,\\mathbf{1}\\{M\\le \\theta_{0}\\}.\n$$\nHence, the likelihood ratio statistic is\n$$\n\\lambda(x)=\\frac{L(\\theta_{0}\\mid x)}{\\sup_{\\theta>0}L(\\theta\\mid x)}=\\left(\\frac{M}{\\theta_{0}}\\right)^{n}\\mathbf{1}\\{M\\le \\theta_{0}\\},\n$$\nwith $\\lambda(x)=0$ if $M>\\theta_{0}$. The LRT rejects for small values of $\\lambda(x)$, which is equivalent to rejecting when $M$ is small (or when $M>\\theta_{0}$). Therefore, the rejection region has the form $M\\le k_{1}$ or $M>\\theta_{0}$.\n\nTo achieve significance level $\\alpha$, we require under $H_{0}$ that\n$$\n\\mathbb{P}_{\\theta_{0}}(M\\le k_{1})+\\mathbb{P}_{\\theta_{0}}(M>\\theta_{0})=\\alpha.\n$$\nSince $\\mathbb{P}_{\\theta_{0}}(M>\\theta_{0})=0$, this reduces to\n$$\n\\mathbb{P}_{\\theta_{0}}(M\\le k_{1})=\\alpha.\n$$\nFor $X_{i}\\sim \\operatorname{Unif}(0,\\theta_{0})$ i.i.d., the distribution of the maximum is\n$$\n\\mathbb{P}_{\\theta_{0}}(M\\le t)=\\left(\\frac{t}{\\theta_{0}}\\right)^{n},\\quad 0<t<\\theta_{0}.\n$$\nSetting this equal to $\\alpha$ and solving for $k_{1}$ gives\n$$\n\\left(\\frac{k_{1}}{\\theta_{0}}\\right)^{n}=\\alpha\n\\quad\\Longrightarrow\\quad\nk_{1}=\\theta_{0}\\,\\alpha^{\\frac{1}{n}}.\n$$", "answer": "$$\\boxed{\\theta_{0}\\alpha^{\\frac{1}{n}}}$$", "id": "1930681"}]}