{"hands_on_practices": [{"introduction": "Our first practice exercise provides a foundational application of the Neyman-Fisher Factorization Theorem. Using the familiar exponential distribution, a common model for lifetimes or waiting times, we will discover how to distill an entire random sample into a single representative value without losing any information about the unknown parameter $\\theta$. This exercise [@problem_id:1957844] is crucial for building intuition on how a sufficient statistic efficiently summarizes data.", "problem": "A quality control engineer is analyzing the longevity of a new type of Light Emitting Diode (LED). The lifetime $X$ of an individual LED, measured in thousands of hours, is modeled by an exponential distribution. The probability density function (PDF) for the lifetime is given by:\n$$f(x; \\theta) = \\frac{1}{\\theta} \\exp\\left(-\\frac{x}{\\theta}\\right), \\quad \\text{for } x \\ge 0$$\nand $f(x; \\theta) = 0$ for $x < 0$. In this model, the parameter $\\theta > 0$ represents the mean lifetime of the LEDs. To make inferences about this parameter, the engineer collects a random sample of $n$ lifetimes, denoted by $X_1, X_2, \\dots, X_n$.\n\nTo efficiently estimate the mean lifetime $\\theta$, the engineer needs to find a statistic that summarizes all the information the sample contains about $\\theta$. Find a sufficient statistic for the parameter $\\theta$ based on this sample.", "solution": "We are given that $X_{1},\\dots,X_{n}$ are independent and identically distributed with density $f(x;\\theta)=\\theta^{-1}\\exp(-x/\\theta)$ for $x\\ge 0$ and $f(x;\\theta)=0$ for $x<0$, where $\\theta>0$ is the mean of the exponential distribution. By independence, the joint density of the sample $\\mathbf{X}=(X_{1},\\dots,X_{n})$ is\n$$\nf(\\mathbf{x};\\theta)=\\prod_{i=1}^{n}\\left[\\frac{1}{\\theta}\\exp\\left(-\\frac{x_{i}}{\\theta}\\right)\\right]\\cdot \\prod_{i=1}^{n}\\mathbf{1}_{\\{x_{i}\\ge 0\\}},\n$$\nwhere $\\mathbf{1}_{\\{x_{i}\\ge 0\\}}$ denotes the indicator that $x_{i}\\ge 0$. Simplifying the product over the positive part of the density, we obtain\n$$\nf(\\mathbf{x};\\theta)=\\theta^{-n}\\exp\\left(-\\frac{1}{\\theta}\\sum_{i=1}^{n}x_{i}\\right)\\cdot \\prod_{i=1}^{n}\\mathbf{1}_{\\{x_{i}\\ge 0\\}}.\n$$\nDefine the statistic $T(\\mathbf{X})=\\sum_{i=1}^{n}X_{i}$. Then we can write\n$$\nf(\\mathbf{x};\\theta)=\\underbrace{\\theta^{-n}\\exp\\left(-\\frac{T(\\mathbf{x})}{\\theta}\\right)}_{g(T(\\mathbf{x});\\theta)}\\cdot \\underbrace{\\prod_{i=1}^{n}\\mathbf{1}_{\\{x_{i}\\ge 0\\}}}_{h(\\mathbf{x})}.\n$$\nThis is a factorization of the joint density into a function $g$ that depends on the sample only through $T(\\mathbf{x})$ and the parameter $\\theta$, multiplied by a function $h$ that does not depend on $\\theta$. By the Neyman–Fisher factorization theorem, $T(\\mathbf{X})=\\sum_{i=1}^{n}X_{i}$ is a sufficient statistic for $\\theta$.", "answer": "$$\\boxed{\\sum_{i=1}^{n} X_{i}}$$", "id": "1957844"}, {"introduction": "This next exercise challenges us to think beyond simple sums and averages. When the support of a distribution depends on the unknown parameter, as in the uniform distribution on $[-\\theta, \\theta]$, the key to sufficiency often lies in the sample's extreme values. This problem [@problem_id:1957871] will guide you to understand why order statistics, such as the minimum or maximum, become the carriers of information about the parameter $\\theta$.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample from a continuous uniform distribution on the interval $[-\\theta, \\theta]$, where $\\theta > 0$ is an unknown parameter. The order statistics are denoted by $X_{(1)} = \\min(X_1, \\ldots, X_n)$ and $X_{(n)} = \\max(X_1, \\ldots, X_n)$.\n\nWhich of the following statistics is/are sufficient for the parameter $\\theta$?\n\nA. The sample mean, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nB. The sum of the observations, $\\sum_{i=1}^{n} X_i$.\n\nC. The maximum of the absolute values of the observations, $T = \\max(|X_1|, |X_2|, \\ldots, |X_n|)$.\n\nD. The pair of the minimum and maximum order statistics, $(X_{(1)}, X_{(n)})$.\n\nE. The sample range, $R = X_{(n)} - X_{(1)}$.\n\nF. The sum of the squares of the observations, $\\sum_{i=1}^{n} X_i^2$.\n\nSelect all that apply. Your answer should be a string of the letters corresponding to the correct options, written in alphabetical order (for example, if you believe A, B, and D are correct, your answer should be 'ABD').", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. with density\n$$\nf(x\\mid \\theta)=\\frac{1}{2\\theta}\\,\\mathbf{1}\\{|x|\\leq \\theta\\},\\quad \\theta>0.\n$$\nThe joint density for $\\mathbf{x}=(x_{1},\\ldots,x_{n})$ is\n$$\nL(\\theta;\\mathbf{x})=\\prod_{i=1}^{n}\\frac{1}{2\\theta}\\,\\mathbf{1}\\{|x_{i}|\\leq \\theta\\}=\\left(\\frac{1}{2\\theta}\\right)^{n}\\mathbf{1}\\{\\max_{1\\leq i\\leq n}|x_{i}|\\leq \\theta\\}.\n$$\nDefine $T(\\mathbf{x})=\\max_{1\\leq i\\leq n}|x_{i}|$. Then\n$$\nL(\\theta;\\mathbf{x})=\\left(\\frac{1}{2\\theta}\\right)^{n}\\mathbf{1}\\{\\theta\\geq T(\\mathbf{x})\\},\n$$\nwhich factors as $g_{\\theta}(T(\\mathbf{x}))\\,h(\\mathbf{x})$ with $g_{\\theta}(t)=(2\\theta)^{-n}\\mathbf{1}\\{\\theta\\geq t\\}$ and $h(\\mathbf{x})=1$. By the Neyman–Fisher factorization theorem, $T=\\max_{i}|X_{i}|$ is sufficient for $\\theta$. This confirms option C.\n\nTo identify the minimal sufficient statistic, consider the likelihood ratio for two samples $\\mathbf{x}$ and $\\mathbf{y}$:\n$$\n\\frac{L(\\theta;\\mathbf{x})}{L(\\theta;\\mathbf{y})}=\\frac{\\mathbf{1}\\{\\theta\\geq T(\\mathbf{x})\\}}{\\mathbf{1}\\{\\theta\\geq T(\\mathbf{y})\\}}.\n$$\nThis ratio is constant in $\\theta$ if and only if $T(\\mathbf{x})=T(\\mathbf{y})$. Hence, by the Lehmann–Scheffé criterion, $T(\\mathbf{X})=\\max_{i}|X_{i}|$ is minimal sufficient. Therefore, any sufficient statistic must determine $T$ (i.e., $T$ must be a function of it).\n\nNow evaluate each option:\n\nA. $\\bar{X}$ does not determine $T$. For fixed $\\bar{X}$ (equivalently fixed $\\sum X_{i}$), the value of $T$ can vary by redistributing mass among components while keeping the sum fixed. Since $T$ is not a function of $\\bar{X}$, $\\bar{X}$ is not sufficient.\n\nB. $\\sum_{i=1}^{n}X_{i}$ is proportional to $\\bar{X}$ and the same argument as in A applies; it is not sufficient.\n\nC. $T=\\max_{i}|X_{i}|$ is exactly the minimal sufficient statistic, so it is sufficient.\n\nD. $(X_{(1)},X_{(n)})$ is sufficient because the likelihood can be written as\n$$\nL(\\theta;\\mathbf{x})=\\left(\\frac{1}{2\\theta}\\right)^{n}\\mathbf{1}\\{-\\theta\\leq X_{(1)}\\}\\,\\mathbf{1}\\{X_{(n)}\\leq \\theta\\},\n$$\nwhich depends on $\\theta$ only through $(X_{(1)},X_{(n)})$. Equivalently, it determines $T$ via\n$$\nT=\\max\\{-X_{(1)},\\,X_{(n)}\\}.\n$$\nThus $(X_{(1)},X_{(n)})$ is sufficient (though not minimal).\n\nE. $R=X_{(n)}-X_{(1)}$ does not determine $T$. For fixed $R=r>0$, letting $X_{(1)}=-t$ and $X_{(n)}=r-t$ with $t\\in(0,r)$ yields $T=\\max\\{t,\\,r-t\\}$, which varies with $t$. Hence $T$ is not a function of $R$, so $R$ is not sufficient.\n\nF. $\\sum_{i=1}^{n}X_{i}^{2}$ does not determine $T$. For fixed $\\sum X_{i}^{2}=q>0$, one can redistribute the squared magnitudes among coordinates to produce different values of $\\max_{i}|X_{i}|$. Thus $T$ is not a function of $\\sum X_{i}^{2}$, so it is not sufficient.\n\nTherefore, the sufficient statistics among the options are C and D.", "answer": "$$\\boxed{CD}$$", "id": "1957871"}, {"introduction": "Moving to a more complex and realistic scenario, this final practice problem explores a two-parameter model. The Pareto distribution is widely used to model phenomena with heavy tails, such as wealth or download counts, and here both its minimum value $x_{min}$ and its tail shape $\\alpha$ are unknown. Your task in this exercise [@problem_id:1957831] is to find a two-dimensional, jointly sufficient statistic, combining the principles of parameter-dependent support and parameters within the density's functional form.", "problem": "In the analysis of a new digital marketplace for mobile applications, the number of downloads for an app after its first year is a key metric for its success. The distribution of these download counts is often highly skewed. A statistician models the download count, $X$, for a randomly selected app using a Pareto distribution. The probability density function (PDF) for this distribution is given by:\n$$f(x | \\alpha, x_{min}) = \\alpha x_{min}^{\\alpha} x^{-(\\alpha+1)}$$\nfor $x \\ge x_{min}$, and $f(x | \\alpha, x_{min}) = 0$ for $x < x_{min}$. Here, $\\alpha > 0$ is the shape parameter, which describes the steepness of the distribution's tail, and $x_{min} > 0$ is the scale parameter, representing the minimum possible number of downloads for any app in the marketplace.\n\nSuppose a random sample of $n$ applications, $X_1, X_2, \\ldots, X_n$, is selected, and their download counts are recorded. Identify a two-dimensional, jointly sufficient statistic for the parameter vector $\\theta = (\\alpha, x_{min})$.\n\nYour answer should be a 2-element row vector containing the two statistics, expressed in terms of the sample data $X_1, X_2, \\ldots, X_n$.", "solution": "We are given i.i.d. $X_{1},\\ldots,X_{n}$ from a Pareto distribution with parameters $\\alpha>0$ and $x_{min}>0$, with density\n$$\nf(x\\mid \\alpha,x_{min})=\\alpha x_{min}^{\\alpha}x^{-(\\alpha+1)}\\quad\\text{for }x\\ge x_{min},\\quad f(x\\mid \\alpha,x_{min})=0\\text{ for }x<x_{min}.\n$$\nThe joint density (likelihood as a function of the data) is\n$$\nL(\\alpha,x_{min}\\mid x_{1},\\ldots,x_{n})=\\prod_{i=1}^{n}\\alpha x_{min}^{\\alpha}x_{i}^{-(\\alpha+1)}\\cdot \\prod_{i=1}^{n}\\mathbf{1}\\{x_{i}\\ge x_{min}\\}.\n$$\nThis simplifies to\n$$\nL(\\alpha,x_{min}\\mid \\mathbf{x})=\\alpha^{n}x_{min}^{n\\alpha}\\left(\\prod_{i=1}^{n}x_{i}^{-(\\alpha+1)}\\right)\\mathbf{1}\\{x_{min}\\le \\min_{1\\le i\\le n}x_{i}\\}.\n$$\nLet $S=\\sum_{i=1}^{n}\\ln x_{i}$ and $M=\\min_{1\\le i\\le n}x_{i}$. Using $\\prod_{i=1}^{n}x_{i}^{-(\\alpha+1)}=\\exp\\left(-(\\alpha+1)\\sum_{i=1}^{n}\\ln x_{i}\\right)=\\exp(-\\alpha S)\\exp(-S)$, we obtain\n$$\nL(\\alpha,x_{min}\\mid \\mathbf{x})=\\left[\\alpha^{n}x_{min}^{n\\alpha}\\exp(-\\alpha S)\\mathbf{1}\\{x_{min}\\le M\\}\\right]\\cdot \\exp(-S).\n$$\nBy the Neyman–Fisher factorization theorem, this expresses the likelihood as $g\\big((M,S),\\alpha,x_{min}\\big)\\,h(\\mathbf{x})$, where $h(\\mathbf{x})=\\exp(-S)$ does not depend on $(\\alpha,x_{min})$, and $g$ depends on the sample only through $(M,S)$. Therefore, the pair\n$$\n\\left(M,\\;S\\right)=\\left(\\min_{1\\le i\\le n}X_{i},\\;\\sum_{i=1}^{n}\\ln X_{i}\\right)\n$$\nis a two-dimensional jointly sufficient statistic for $\\theta=(\\alpha,x_{min})$.\n\nThus, a valid two-element row vector sufficient statistic is $\\begin{pmatrix}\\min_{1\\le i\\le n}X_{i} & \\sum_{i=1}^{n}\\ln X_{i}\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\min_{1\\le i\\le n}X_{i} & \\sum_{i=1}^{n}\\ln X_{i}\\end{pmatrix}}$$", "id": "1957831"}]}