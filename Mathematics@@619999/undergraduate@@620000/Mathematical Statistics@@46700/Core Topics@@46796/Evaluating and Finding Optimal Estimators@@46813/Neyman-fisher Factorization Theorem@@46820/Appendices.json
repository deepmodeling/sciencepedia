{"hands_on_practices": [{"introduction": "This first exercise provides a practical entry point into the concept of sufficiency. Using a familiar scenario from quality control involving binomial distributions, you will apply the Neyman-Fisher Factorization Theorem to demonstrate that the total number of observed defects across all samples is a sufficient statistic for the unknown defect probability [@problem_id:1939675]. This practice is fundamental for understanding how a complex dataset can often be summarized by a much simpler statistic without any loss of information regarding the parameter of interest.", "problem": "A company manufactures integrated circuits. The probability that any given circuit is defective is $p$, where $0  p  1$. This probability $p$ is constant but unknown. To monitor the production process, $m$ independent batches are sampled. For each batch $i$ (where $i = 1, 2, ..., m$), a fixed number of $n$ circuits are randomly selected and tested. The number of defective circuits in batch $i$ is denoted by the random variable $X_i$. Each $X_i$ is assumed to follow a Binomial distribution with $n$ trials and a \"success\" probability $p$ of being defective. The number of trials $n$ is a known positive integer.\n\nYou are given a set of observations $x_1, x_2, ..., x_m$, which are specific outcomes of the random variables $X_1, X_2, ..., X_m$. The Neyman-Fisher Factorization Theorem provides a method for finding a sufficient statistic, which is a function of the data that summarizes all of the information the sample contains about the unknown parameter $p$. The theorem states that a statistic $T = T(x_1, x_2, ..., x_m)$ is sufficient for $p$ if and only if the joint Probability Mass Function (PMF) of the sample, $f(x_1, ..., x_m | p)$, can be factored into two non-negative functions:\n$$f(x_1, ..., x_m | p) = g(T(x_1, ..., x_m), p) \\cdot h(x_1, ..., x_m)$$\nwhere the function $g$ depends on the data only through the value of the statistic $T$, and the function $h$ does not depend on the parameter $p$.\n\nYour task is to apply this theorem to the given scenario. Find the simplest non-constant statistic $T(x_1, x_2, ..., x_m)$ that captures all the information about the unknown defect probability $p$. Your answer should be an expression in terms of the observations $x_1, x_2, ..., x_m$.", "solution": "We model each batch count as $X_{i} \\sim \\mathrm{Bin}(n,p)$, independently for $i=1,\\ldots,m$. Hence, for observed values $x_{i} \\in \\{0,1,\\ldots,n\\}$, the individual PMF is\n$$\nf_{X_{i}}(x_{i}\\mid p)=\\binom{n}{x_{i}} p^{x_{i}} (1-p)^{n-x_{i}}.\n$$\nBy independence, the joint PMF is the product\n$$\nf(x_{1},\\ldots,x_{m}\\mid p)=\\prod_{i=1}^{m} \\binom{n}{x_{i}} p^{x_{i}} (1-p)^{n-x_{i}}.\n$$\nCollecting like terms yields\n$$\nf(x_{1},\\ldots,x_{m}\\mid p)=\\left[\\prod_{i=1}^{m} \\binom{n}{x_{i}}\\right] p^{\\sum_{i=1}^{m} x_{i}} (1-p)^{\\sum_{i=1}^{m} (n-x_{i})}\n=\\left[\\prod_{i=1}^{m} \\binom{n}{x_{i}}\\right] p^{T} (1-p)^{mn-T},\n$$\nwhere we define the statistic\n$$\nT=\\sum_{i=1}^{m} x_{i}.\n$$\nThe support $\\{x_{i}\\in\\{0,\\ldots,n\\}\\text{ for all }i\\}$ does not depend on $p$, so the Neyman-Fisher factorization theorem applies directly with\n$$\ng(T,p)=p^{T}(1-p)^{mn-T},\\qquad h(x_{1},\\ldots,x_{m})=\\prod_{i=1}^{m} \\binom{n}{x_{i}}.\n$$\nSince $g$ depends on the data only through $T$ and $h$ does not depend on $p$, the statistic $T=\\sum_{i=1}^{m} x_{i}$ is sufficient for $p$. Any one-to-one function of $T$ (such as the sample proportion $T/(mn)$) is also sufficient, but the simplest non-constant choice is the total number of defectives across all batches, $T=\\sum_{i=1}^{m} x_{i}$.", "answer": "$$\\boxed{\\sum_{i=1}^{m} x_{i}}$$", "id": "1939675"}, {"introduction": "This problem highlights a critical and distinct application of the factorization theorem where the parameter defines the support of the distribution. Unlike distributions where the parameter only appears in the functional form, here the parameter $\\theta$ directly sets the boundaries of the data's possible range. This exercise [@problem_id:1939672] will challenge you to correctly handle the indicator function in the joint density, revealing that the sufficient statistic is based on an order statistic rather than a sum.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample drawn from a Uniform distribution on the symmetric interval $[-\\theta, \\theta]$, where the parameter $\\theta  0$ is unknown. Identify which one of the following statistics is a sufficient statistic for $\\theta$.\n\nA. The sample mean, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$\n\nB. The sample variance, $S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\bar{X})^2$\n\nC. The maximum order statistic, $X_{(n)} = \\max(X_1, X_2, \\ldots, X_n)$\n\nD. The maximum of the absolute values of the observations, $\\max(|X_1|, |X_2|, \\ldots, |X_n|)$\n\nE. The sample range, $X_{(n)} - X_{(1)}$, where $X_{(1)} = \\min(X_1, X_2, \\ldots, X_n)$", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. with density\n$$\nf_{X}(x;\\theta)=\\frac{1}{2\\theta}\\,\\mathbf{1}\\{-\\theta\\leq x\\leq \\theta\\},\\quad \\theta0.\n$$\nThe joint density of the sample $x=(x_{1},\\ldots,x_{n})$ is\n$$\nf(x;\\theta)=\\prod_{i=1}^{n}\\frac{1}{2\\theta}\\,\\mathbf{1}\\{-\\theta\\leq x_{i}\\leq \\theta\\}\n=(2\\theta)^{-n}\\,\\prod_{i=1}^{n}\\mathbf{1}\\{|x_{i}|\\leq \\theta\\}\n=(2\\theta)^{-n}\\,\\mathbf{1}\\left\\{\\max_{1\\leq i\\leq n}|x_{i}|\\leq \\theta\\right\\}.\n$$\nDefine the statistic $T(x)=\\max_{1\\leq i\\leq n}|x_{i}|$. Then the joint density can be written as\n$$\nf(x;\\theta)=g_{\\theta}(T(x))\\,h(x),\\quad\\text{with }g_{\\theta}(t)=(2\\theta)^{-n}\\,\\mathbf{1}\\{\\theta\\geq t\\},\\; h(x)=1.\n$$\nBy the Neyman–Fisher factorization theorem, $T(X)=\\max_{1\\leq i\\leq n}|X_{i}|$ is sufficient for $\\theta$. This corresponds to option D.\n\nTo see why the other options are not sufficient, observe that the likelihood depends on $\\theta$ only through $(2\\theta)^{-n}$ and the support constraint $\\theta\\geq \\max_{i}|x_{i}|$. Any statistic that does not determine $\\max_{i}|x_{i}|$ cannot be sufficient:\n- The sample mean $\\bar{X}$ does not determine $\\max_{i}|x_{i}|$; samples can share the same mean but have different maximum absolute observations, leading to different support constraints.\n- The sample variance $S^{2}$ likewise does not determine $\\max_{i}|x_{i}|$.\n- The maximum order statistic $X_{(n)}$ fails when the largest magnitude observation is negative (e.g., $x=(-5,-4)$ gives $X_{(n)}=-4$ but $\\max|x_{i}|=5$).\n- The range $X_{(n)}-X_{(1)}$ does not determine $\\max_{i}|x_{i}|$ (e.g., $(-5,3)$ and $(-4,4)$ have the same range but different $\\max|x_{i}|$).\n\nTherefore, only $\\max(|X_{1}|,\\ldots,|X_{n}|)$ is sufficient for $\\theta$ among the given choices.", "answer": "$$\\boxed{D}$$", "id": "1939672"}, {"introduction": "Moving to continuous distributions, this exercise explores the Laplace distribution, a key model in robust statistics. You will find a sufficient statistic for its scale parameter $b$, when the location is known to be zero [@problem_id:1939653]. This practice is valuable as it demonstrates how the sufficient statistic can be a function of the absolute values of the observations, a common feature when dealing with distributions that have heavier tails than the normal distribution.", "problem": "Consider a random sample $X_1, X_2, \\dots, X_n$ drawn from a Laplace distribution. The probability density function (PDF) of this distribution is given by\n$$f(x | \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|x - \\mu|}{b}\\right)$$\nfor $x \\in (-\\infty, \\infty)$. In a specific experimental setup, it is known that the location parameter $\\mu$ is fixed at $\\mu = 0$, but the scale parameter $b  0$ is unknown.\n\nBased on this information, which of the following statistics is a sufficient statistic for the unknown parameter $b$?\n\nA. $T_A(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$\n\nB. $T_B(\\mathbf{X}) = \\left(\\prod_{i=1}^{n} X_i\\right)^{1/n}$\n\nC. $T_C(\\mathbf{X}) = \\sum_{i=1}^{n} X_i^2$\n\nD. $T_D(\\mathbf{X}) = \\sum_{i=1}^{n} |X_i|$\n\nE. $T_E(\\mathbf{X}) = \\max(|X_1|, |X_2|, \\dots, |X_n|)$", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. from the Laplace distribution with known location $\\mu=0$ and unknown scale $b0$, having density for each $i$ given by\n$$\nf(x_{i}\\mid b)=\\frac{1}{2b}\\exp\\left(-\\frac{|x_{i}|}{b}\\right), \\quad x_{i}\\in(-\\infty,\\infty).\n$$\nBy independence, the joint density (likelihood) for $\\mathbf{x}=(x_{1},\\dots,x_{n})$ is\n$$\nL(b;\\mathbf{x})=\\prod_{i=1}^{n}\\frac{1}{2b}\\exp\\left(-\\frac{|x_{i}|}{b}\\right)\n=\\left(\\frac{1}{2b}\\right)^{n}\\exp\\left(-\\frac{1}{b}\\sum_{i=1}^{n}|x_{i}|\\right).\n$$\nBy the Neyman–Fisher factorization theorem, a statistic $T(\\mathbf{X})$ is sufficient for $b$ if the joint density can be written as\n$$\nL(b;\\mathbf{x})=g\\big(T(\\mathbf{x}),b\\big)\\,h(\\mathbf{x}),\n$$\nwhere $g$ depends on the data only through $T(\\mathbf{x})$ and $b$, and $h$ does not depend on $b$. From the expression above, take\n$$\nT(\\mathbf{X})=\\sum_{i=1}^{n}|X_{i}|,\\quad g(t,b)=\\left(\\frac{1}{2b}\\right)^{n}\\exp\\left(-\\frac{t}{b}\\right),\\quad h(\\mathbf{x})=1.\n$$\nHence $T(\\mathbf{X})=\\sum_{i=1}^{n}|X_{i}|$ is sufficient for $b$. Among the given options, this corresponds to option D. The other listed statistics do not appear in the likelihood and therefore are not sufficient for $b$ in this model.", "answer": "$$\\boxed{D}$$", "id": "1939653"}]}