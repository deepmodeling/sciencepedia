{"hands_on_practices": [{"introduction": "Proving that a statistic is complete is a fundamental skill, and this exercise demonstrates a powerful, standard technique. For many common distributions, such as the Gamma distribution which belongs to the exponential family, the proof of completeness elegantly uses the uniqueness property of the Laplace transform. Mastering this method [@problem_id:1905409] will solidify your understanding of the definition of completeness—that $E_{\\beta}[g(T)]=0$ for all $\\beta$ implies $g(T)=0$ almost surely—and reinforce your knowledge of integral transforms.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample from a Gamma distribution with a known shape parameter $\\alpha > 0$ and an unknown rate parameter $\\beta > 0$. The Probability Density Function (PDF) of this distribution is given by\n$$f(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x)$$\nfor $x > 0$. Let $T = \\sum_{i=1}^n X_i$ be the sum of the random variables in the sample. Which of the following assertions about the statistic $T$ is true?\n\nA. $T$ is a complete statistic for $\\beta$ because the family of distributions for the sample belongs to a regular one-parameter exponential family, and $T$ is the corresponding sufficient statistic.\n\nB. $T$ is not a complete statistic for $\\beta$ because the definition of completeness, which requires that $E_{\\beta}[g(T)]=0$ for all $\\beta>0$ implies $g(T)=0$ almost surely, cannot be verified without a specific function $g$.\n\nC. $T$ is a sufficient statistic for $\\beta$, but it is not complete because the parameter space for $\\beta$, which is $(0, \\infty)$, is not the entire real line $\\mathbb{R}$.\n\nD. $T$ is a complete statistic for $\\beta$ only if the sample size $n$ is greater than the shape parameter $\\alpha$.\n\nE. $T$ is not a complete statistic for $\\beta$ because only minimal sufficient statistics can be complete, and $T$ is not minimal sufficient.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. with density $f(x\\mid\\alpha,\\beta)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha-1}\\exp(-\\beta x)$ for $x>0$, with known $\\alpha>0$ and unknown $\\beta>0$. Let $T=\\sum_{i=1}^{n}X_{i}$.\n\n1) Sufficient statistic by factorization:\nThe joint density is\n$$\nL(\\beta\\mid x_{1},\\dots,x_{n})=\\prod_{i=1}^{n}\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x_{i}^{\\alpha-1}\\exp(-\\beta x_{i})\n=\\left(\\frac{1}{\\Gamma(\\alpha)^{n}}\\prod_{i=1}^{n}x_{i}^{\\alpha-1}\\right)\\beta^{n\\alpha}\\exp\\!\\left(-\\beta\\sum_{i=1}^{n}x_{i}\\right).\n$$\nBy the Neyman–Fisher factorization theorem, $T=\\sum_{i=1}^{n}X_{i}$ is sufficient for $\\beta$.\n\n2) Minimal sufficiency:\nFor two samples $x=(x_{1},\\dots,x_{n})$ and $y=(y_{1},\\dots,y_{n})$, the likelihood ratio is\n$$\n\\frac{L(\\beta\\mid x)}{L(\\beta\\mid y)}=\\frac{\\prod_{i=1}^{n}x_{i}^{\\alpha-1}}{\\prod_{i=1}^{n}y_{i}^{\\alpha-1}}\\exp\\!\\left(-\\beta\\left(\\sum_{i=1}^{n}x_{i}-\\sum_{i=1}^{n}y_{i}\\right)\\right).\n$$\nThis ratio is free of $\\beta$ if and only if $\\sum_{i=1}^{n}x_{i}=\\sum_{i=1}^{n}y_{i}$. Hence $T$ is a minimal sufficient statistic for $\\beta$.\n\n3) Distribution of $T$:\nThe Laplace transform of $X_{i}$ is $E[\\exp(-sX_{i})]=\\left(\\frac{\\beta}{\\beta+s}\\right)^{\\alpha}$ for $s>-\\beta$. Independence gives\n$$\nE[\\exp(-sT)]=\\prod_{i=1}^{n}\\left(\\frac{\\beta}{\\beta+s}\\right)^{\\alpha}=\\left(\\frac{\\beta}{\\beta+s}\\right)^{n\\alpha},\n$$\nwhich is the Laplace transform of a Gamma distribution with shape $n\\alpha$ and rate $\\beta$. Therefore\n$$\nT\\sim\\text{Gamma}(n\\alpha,\\beta),\\quad f_{T}(t\\mid\\beta)=\\frac{\\beta^{n\\alpha}}{\\Gamma(n\\alpha)}t^{n\\alpha-1}\\exp(-\\beta t),\\quad t>0.\n$$\n\n4) Completeness of $T$:\nAssume a measurable function $g$ satisfies $E_{\\beta}[g(T)]=0$ for all $\\beta>0$ and $E_{\\beta}[|g(T)|]<\\infty$ for at least one (hence all) $\\beta>0$. Using the density of $T$,\n$$\n0=E_{\\beta}[g(T)]=\\int_{0}^{\\infty}g(t)\\frac{\\beta^{n\\alpha}}{\\Gamma(n\\alpha)}t^{n\\alpha-1}\\exp(-\\beta t)\\,dt\n\\quad\\text{for all }\\beta>0.\n$$\nEquivalently,\n$$\n\\int_{0}^{\\infty}\\bigl(g(t)t^{n\\alpha-1}\\bigr)\\exp(-\\beta t)\\,dt=0\n\\quad\\text{for all }\\beta>0.\n$$\nDefine $h(t)=g(t)t^{n\\alpha-1}$. The function $\\beta\\mapsto\\int_{0}^{\\infty}h(t)\\exp(-\\beta t)\\,dt$ is the Laplace transform of $h$ on $(0,\\infty)$, and it equals $0$ for all $\\beta>0$. By the uniqueness theorem for the Laplace transform, $h(t)=0$ for almost all $t>0$. Since $t^{n\\alpha-1}>0$ for all $t>0$, this implies $g(t)=0$ almost surely with respect to the distribution of $T$. Hence $T$ is complete.\n\n5) Evaluation of the options:\n- A is true: the family is a regular one-parameter exponential family with open natural parameter space and $T$ is the natural sufficient statistic; as shown, $T$ is complete.\n- B is false: the completeness is verified via the Laplace transform argument.\n- C is false: completeness does not require the parameter space to be all of $\\mathbb{R}$; $(0,\\infty)$ (or the open natural parameter space $(-\\infty,0)$) suffices.\n- D is false: the completeness proof holds for all $n\\in\\mathbb{N}$ and $\\alpha>0$; there is no requirement that $n>\\alpha$.\n- E is false: completeness does not require minimality, and in fact $T$ is minimal sufficient here.\n\nTherefore, the correct assertion is A.", "answer": "$$\\boxed{A}$$", "id": "1905409"}, {"introduction": "A common misconception is that a sufficient, or even minimal sufficient, statistic must be complete. This practice confronts that idea directly by exploring a classic case where a minimal sufficient statistic is incomplete. Your task is to construct a non-zero function $g(T)$ of the statistic $T = (X_{(1)}, X_{(n)})$ whose expected value is zero for all possible values of the parameter $\\theta$, thereby proving incompleteness by counterexample. Working through this problem [@problem_id:1905414] highlights that sufficiency and completeness are distinct properties, and that completeness is a necessary additional condition for powerful results like the Lehmann-Scheffé theorem.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n > 1$ from a Uniform distribution on the interval $[\\theta, 2\\theta]$, where $\\theta > 0$ is an unknown parameter. The order statistics are denoted by $X_{(1)} \\le X_{(2)} \\le \\dots \\le X_{(n)}$. The minimal sufficient statistic for $\\theta$ is the pair $T = (X_{(1)}, X_{(n)})$.\n\nA statistic $T$ is defined as incomplete if there exists a function $g(T)$, which is not identically zero for all possible outcomes of $T$, such that its expected value $E[g(T)] = 0$ for all possible values of the parameter $\\theta$.\n\nWhich of the following functions $g(T)$ demonstrates that the statistic $T=(X_{(1)}, X_{(n)})$ is incomplete for the family of Uniform distributions $U(\\theta, 2\\theta)$?\n\nA. $g(X_{(1)}, X_{(n)}) = (n+2)X_{(1)} - (2n+1)X_{(n)}$\n\nB. $g(X_{(1)}, X_{(n)}) = (2n+1)X_{(1)} - (n+2)X_{(n)}$\n\nC. $g(X_{(1)}, X_{(n)}) = (n+1)X_{(1)} - (2n+1)X_{(n)}$\n\nD. $g(X_{(1)}, X_{(n)}) = X_{(n)} - 2X_{(1)}$\n\nE. $g(X_{(1)}, X_{(n)}) = (n+2)X_{(1)} - (2n-1)X_{(n)}$", "solution": "We sample $X_{1},\\dots,X_{n}$ iid from $U(\\theta,2\\theta)$ with $\\theta>0$ and denote the order statistics by $X_{(1)}\\leq \\dots \\leq X_{(n)}$. For a Uniform$(a,b)$ sample, the $k$-th order statistic satisfies $X_{(k)}=a+(b-a)U_{(k)}$ with $U_{(k)}\\sim \\text{Beta}(k,n+1-k)$, hence\n$$\n\\mathbb{E}[X_{(k)}]=a+(b-a)\\frac{k}{n+1}.\n$$\nApplying this with $a=\\theta$ and $b=2\\theta$ gives\n$$\n\\mathbb{E}[X_{(1)}]=\\theta+\\theta\\frac{1}{n+1}=\\theta\\frac{n+2}{n+1},\\qquad\n\\mathbb{E}[X_{(n)}]=\\theta+\\theta\\frac{n}{n+1}=2\\theta-\\theta\\frac{1}{n+1}=\\theta\\frac{2n+1}{n+1}.\n$$\nConsider a linear function $g(X_{(1)},X_{(n)})=\\alpha X_{(1)}-\\beta X_{(n)}$. Its expectation is\n$$\n\\mathbb{E}[g]=\\alpha\\,\\mathbb{E}[X_{(1)}]-\\beta\\,\\mathbb{E}[X_{(n)}]\n=\\theta\\left[\\alpha\\frac{n+2}{n+1}-\\beta\\frac{2n+1}{n+1}\\right].\n$$\nFor $\\mathbb{E}[g]=0$ for all $\\theta>0$, we require\n$$\n\\alpha(n+2)-\\beta(2n+1)=0.\n$$\nA nontrivial choice is $\\alpha=2n+1$ and $\\beta=n+2$, which yields\n$$\ng(X_{(1)},X_{(n)})=(2n+1)X_{(1)}-(n+2)X_{(n)}.\n$$\nThis function is not identically zero and satisfies $\\mathbb{E}[g]=0$ for all $\\theta>0$, thereby demonstrating incompleteness of $T=(X_{(1)},X_{(n)})$. Among the given options, this corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1905414"}, {"introduction": "The concept of completeness applies to discrete distributions just as it does to continuous ones, though the proof techniques often differ. This problem investigates completeness for a discrete uniform distribution where the support, $\\{1, 2, \\ldots, \\theta\\}$, depends on the parameter itself. Instead of relying on integral transforms, the proof involves a clever algebraic argument based on the summation that defines the expectation. This exercise [@problem_id:1905386] demonstrates that the principles of completeness are not confined to exponential families and encourages flexible problem-solving by showing how a simple inductive argument can prove a function is identically zero.", "problem": "A simple random number generator produces a single integer, $X$, drawn from a discrete uniform distribution on the set of integers $\\{1, 2, \\dots, \\theta\\}$, where the upper bound $\\theta$ is an unknown parameter that can be any integer greater than or equal to 1.\n\nIn statistical theory, a statistic $T$ is defined as being 'complete' for a family of distributions indexed by a parameter $\\theta$ if, for any suitably defined function $g$, the condition $E_{\\theta}[g(T)] = 0$ for all possible values of $\\theta$ implies that the probability $P_{\\theta}(g(T) = 0)$ is equal to 1 for all $\\theta$.\n\nConsider the statistic $T = X$, which is simply the observed integer itself. Which of the following statements about the completeness of $T$ for the parameter $\\theta$ is correct?\n\nA. Yes, $T$ is a complete statistic.\n\nB. No, $T$ is not a complete statistic because for the function defined as $g(1)=1$, $g(2)=-1$, and $g(t)=0$ for all $t > 2$, the expectation $E_{\\theta}[g(T)]$ is zero for most values of $\\theta$, yet the function is not identically zero.\n\nC. No, $T$ is not a complete statistic because the support of the distribution, $\\{1, 2, \\dots, \\theta\\}$, depends on the parameter $\\theta$.\n\nD. No, $T$ is not a complete statistic because the function $g$ in the definition can depend on $\\theta$, and a valid non-zero choice is $g(t, \\theta) = t - E_{\\theta}[T]$, which always has an expectation of zero.", "solution": "Let $X$ have the discrete uniform distribution on $\\{1,2,\\dots,\\theta\\}$ with unknown integer parameter $\\theta \\geq 1$. Its probability mass function is\n$$\np_{\\theta}(x)=\\begin{cases}\n\\frac{1}{\\theta}, & x \\in \\{1,2,\\dots,\\theta\\},\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nBy definition, the statistic $T=X$ is complete for the family $\\{p_{\\theta}:\\theta \\geq 1\\}$ if for every function $g$ (not depending on $\\theta$) such that $E_{\\theta}[g(T)]=0$ for all $\\theta \\geq 1$, it follows that $P_{\\theta}(g(T)=0)=1$ for all $\\theta \\geq 1$.\n\nCompute the expectation:\n$$\nE_{\\theta}[g(T)] = \\sum_{t=1}^{\\theta} g(t) \\cdot \\frac{1}{\\theta} = \\frac{1}{\\theta} \\sum_{t=1}^{\\theta} g(t).\n$$\nIf $E_{\\theta}[g(T)]=0$ for all $\\theta \\geq 1$, then\n$$\n\\sum_{t=1}^{\\theta} g(t) = 0 \\quad \\text{for all } \\theta \\geq 1.\n$$\nDefine $S_{\\theta} = \\sum_{t=1}^{\\theta} g(t)$. Then $S_{\\theta}=0$ for all $\\theta \\geq 1$. For $k \\geq 2$,\n$$\ng(k) = S_{k} - S_{k-1} = 0 - 0 = 0,\n$$\nand for $k=1$,\n$$\ng(1) = S_{1} = 0.\n$$\nHence $g(k)=0$ for all positive integers $k$, which implies $P_{\\theta}(g(T)=0)=1$ for all $\\theta$. Therefore, $T=X$ is a complete statistic.\n\nRegarding the options:\n- A is correct, as shown.\n- B is invalid because its proposed $g$ yields $E_{\\theta}[g(T)]=1$ when $\\theta=1$, so the expectation is not zero for all $\\theta$.\n- C is incorrect because dependence of support on $\\theta$ does not preclude completeness; in fact, $T$ is complete here.\n- D is incorrect because the definition of completeness requires $g$ to be a function of $T$ only, not depending on $\\theta$.\n\nThus the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1905386"}]}