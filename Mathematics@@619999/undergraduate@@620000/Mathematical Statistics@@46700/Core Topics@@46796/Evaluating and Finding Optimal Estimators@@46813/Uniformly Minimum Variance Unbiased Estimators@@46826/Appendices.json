{"hands_on_practices": [{"introduction": "To begin our hands-on practice, we will tackle a foundational problem in a discrete setting. We explore a random sample from a Bernoulli distribution, a building block for many statistical models. While estimating the success probability $p$ itself is a familiar task, this exercise challenges us to find the best unbiased estimator for a function of the parameter, specifically $\\theta = p^2$ [@problem_id:1966071]. This practice is valuable because it introduces the core workflow of identifying a complete sufficient statistic and then constructing an unbiased estimator based upon it, often requiring clever use of properties like factorial moments.", "problem": "A large-scale manufacturing process for a specific type of microchip produces a very small, but non-zero, fraction of defective units. Let the probability that a randomly selected microchip is defective be $p$. To estimate certain quality control metrics, a random sample of $n$ microchips, where $n \\ge 2$, is selected from the production line. Let $X_i$ be a random variable that equals 1 if the $i$-th chip in the sample is defective and 0 otherwise, for $i = 1, 2, \\ldots, n$. The collection $\\{X_1, X_2, \\ldots, X_n\\}$ can be modeled as a random sample from a Bernoulli distribution with parameter $p$.\n\nYour task is to find the Uniformly Minimum Variance Unbiased Estimator (UMVUE) for the parameter $\\theta = p^2$. This parameter $\\theta$ represents the probability that two independently produced microchips are both defective. Express your answer as a function of the total number of defective chips in the sample, $T = \\sum_{i=1}^{n} X_i$, and the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\text{Bernoulli}(p)$ and $T=\\sum_{i=1}^{n}X_{i}$. Then $T\\sim\\text{Binomial}(n,p)$ and the parameter of interest is $\\theta=p^{2}$.\n\nFirst, $T$ is sufficient for $p$ by the factorization theorem, since the joint pmf of $(X_{1},\\ldots,X_{n})$ can be written as\n$$\n\\prod_{i=1}^{n} p^{X_{i}}(1-p)^{1-X_{i}} \\;=\\; p^{\\sum X_{i}}(1-p)^{n-\\sum X_{i}} \\;=\\; p^{T}(1-p)^{n-T},\n$$\nwhich depends on the sample only through $T$.\n\nSecond, $T$ is complete. If $g$ is any function with $\\mathbb{E}_{p}[g(T)]=0$ for all $p\\in(0,1)$, then\n$$\n\\sum_{t=0}^{n} g(t)\\binom{n}{t}p^{t}(1-p)^{n-t}=0 \\quad \\text{for all } p\\in(0,1).\n$$\nMultiplying by $(1-p)^{-n}$ and setting $u=p/(1-p)$ gives\n$$\n\\sum_{t=0}^{n} g(t)\\binom{n}{t} u^{t}=0 \\quad \\text{for all } u>0,\n$$\nwhich is a polynomial identity. Hence all coefficients vanish, implying $g(t)=0$ for all $t$, so $T$ is complete.\n\nTo construct an unbiased estimator of $\\theta$, use factorial moments of $T$:\n$$\nT(T-1)=\\sum_{i\\neq j} X_{i}X_{j}.\n$$\nTaking expectations and using independence,\n$$\n\\mathbb{E}[T(T-1)] \\;=\\; \\sum_{i\\neq j} \\mathbb{E}[X_{i}X_{j}] \\;=\\; \\sum_{i\\neq j} p^{2} \\;=\\; n(n-1)p^{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[\\frac{T(T-1)}{n(n-1)}\\right]=p^{2}=\\theta,\n$$\nso $g(T)=\\frac{T(T-1)}{n(n-1)}$ is unbiased for $\\theta$ and is a function of the sufficient statistic $T$.\n\nBy the Lehmann–Scheffé theorem, since $T$ is complete and sufficient and $g(T)$ is unbiased for $\\theta$, the uniformly minimum variance unbiased estimator (UMVUE) of $\\theta$ is\n$$\n\\frac{T(T-1)}{n(n-1)}.\n$$", "answer": "$$\\boxed{\\frac{T(T-1)}{n(n-1)}}$$", "id": "1966071"}, {"introduction": "Next, we move to a classic problem from continuous distributions that has immense practical importance: comparing the means of two populations. In this scenario, we analyze two independent samples from Normal distributions with different means but a common known variance [@problem_id:1966060]. While the difference between the two sample means, $\\bar{X} - \\bar{Y}$, is an intuitive estimator for the difference in population means $\\mu_1 - \\mu_2$, this exercise demonstrates how the Lehmann–Scheffé theorem provides the rigorous justification for why this simple estimator is, in fact, the best one possible among all unbiased estimators. This practice solidifies the connection between statistical theory and common applied methods.", "problem": "A pharmaceutical company is evaluating two different production lines for a medication. To compare the average amount of active ingredient, they take two independent random samples.\n\nLet $X_1, X_2, \\ldots, X_n$ be the measurements of the active ingredient from a sample of $n$ pills from the first production line. These measurements are assumed to be independent and identically distributed random variables from a Normal distribution with an unknown mean $\\mu_1$ and a known variance $\\sigma^2$.\n\nSimilarly, let $Y_1, Y_2, \\ldots, Y_m$ be the measurements from a sample of $m$ pills from the second production line. These are assumed to be independent and identically distributed random variables from a Normal distribution with an unknown mean $\\mu_2$ and the same known variance $\\sigma^2$. The two samples are independent of each other.\n\nFind the Uniformly Minimum Variance Unbiased Estimator (UMVUE) for the difference in the mean active ingredient amounts, $\\theta = \\mu_1 - \\mu_2$. Express your answer in terms of the sample observations $X_1, \\dots, X_n$ and $Y_1, \\dots, Y_m$, and the sample sizes $n$ and $m$.", "solution": "We have two independent samples: $X_{1},\\dots,X_{n}$ iid $\\mathcal{N}(\\mu_{1},\\sigma^{2})$ and $Y_{1},\\dots,Y_{m}$ iid $\\mathcal{N}(\\mu_{2},\\sigma^{2})$, with the same known variance $\\sigma^{2}$ and unknown means $\\mu_{1}$ and $\\mu_{2}$. The parameter of interest is $\\theta=\\mu_{1}-\\mu_{2}$.\n\nFirst, identify sufficient statistics. The joint likelihood for the $X$-sample is\n$$\nL_{X}(\\mu_{1})=\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{1})^{2}}{2\\sigma^{2}}\\right)=c_{X}(x)\\exp\\!\\left(\\frac{\\mu_{1}}{\\sigma^{2}}\\sum_{i=1}^{n}x_{i}-\\frac{n\\mu_{1}^{2}}{2\\sigma^{2}}\\right),\n$$\nwhere $c_{X}(x)=\\prod_{i=1}^{n}(2\\pi\\sigma^{2})^{-1/2}\\exp\\!\\left(-\\frac{x_{i}^{2}}{2\\sigma^{2}}\\right)$ does not depend on $\\mu_{1}$. Similarly, for the $Y$-sample,\n$$\nL_{Y}(\\mu_{2})=\\prod_{j=1}^{m}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\!\\left(-\\frac{(y_{j}-\\mu_{2})^{2}}{2\\sigma^{2}}\\right)=c_{Y}(y)\\exp\\!\\left(\\frac{\\mu_{2}}{\\sigma^{2}}\\sum_{j=1}^{m}y_{j}-\\frac{m\\mu_{2}^{2}}{2\\sigma^{2}}\\right).\n$$\nBecause the samples are independent, the joint likelihood is\n$$\nL(\\mu_{1},\\mu_{2})=L_{X}(\\mu_{1})L_{Y}(\\mu_{2})=c(x,y)\\exp\\!\\left(\\frac{\\mu_{1}}{\\sigma^{2}}\\sum_{i=1}^{n}x_{i}-\\frac{n\\mu_{1}^{2}}{2\\sigma^{2}}+\\frac{\\mu_{2}}{\\sigma^{2}}\\sum_{j=1}^{m}y_{j}-\\frac{m\\mu_{2}^{2}}{2\\sigma^{2}}\\right),\n$$\nwith $c(x,y)=c_{X}(x)c_{Y}(y)$ free of $(\\mu_{1},\\mu_{2})$. By the factorization theorem, $(\\sum_{i=1}^{n}X_{i},\\sum_{j=1}^{m}Y_{j})$, equivalently $(\\bar{X},\\bar{Y})$ with $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ and $\\bar{Y}=\\frac{1}{m}\\sum_{j=1}^{m}Y_{j}$, is a sufficient statistic for $(\\mu_{1},\\mu_{2})$.\n\nNext, establish completeness. The joint model is a full two-parameter exponential family with natural parameters $\\left(\\frac{\\mu_{1}}{\\sigma^{2}},\\frac{\\mu_{2}}{\\sigma^{2}}\\right)$ ranging over $\\mathbb{R}^{2}$ (an open set) and natural sufficient statistic $\\left(\\sum_{i=1}^{n}X_{i},\\sum_{j=1}^{m}Y_{j}\\right)$. In a full regular exponential family with open natural parameter space, the minimal sufficient statistic is complete. Therefore, $(\\bar{X},\\bar{Y})$ is complete sufficient for $(\\mu_{1},\\mu_{2})$.\n\nNow, construct an unbiased estimator of $\\theta=\\mu_{1}-\\mu_{2}$ that is a function of the complete sufficient statistic. Since $E[\\bar{X}]=\\mu_{1}$ and $E[\\bar{Y}]=\\mu_{2}$, the estimator\n$$\n\\bar{X}-\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}-\\frac{1}{m}\\sum_{j=1}^{m}Y_{j}\n$$\nsatisfies $E[\\bar{X}-\\bar{Y}]=\\mu_{1}-\\mu_{2}=\\theta$, hence it is unbiased. By the Lehmann–Scheffé theorem, because it is an unbiased function of a complete sufficient statistic, it is the uniformly minimum variance unbiased estimator (UMVUE) of $\\theta$.\n\nTherefore, the UMVUE for $\\theta$ is $\\bar{X}-\\bar{Y}$, expressed in terms of the sample observations as\n$$\n\\frac{1}{n}\\sum_{i=1}^{n}X_{i}-\\frac{1}{m}\\sum_{j=1}^{m}Y_{j}.\n$$", "answer": "$$\\boxed{\\frac{1}{n}\\sum_{i=1}^{n} X_{i}-\\frac{1}{m}\\sum_{j=1}^{m} Y_{j}}$$", "id": "1966060"}, {"introduction": "For our final practice, we explore a scenario that deviates from the familiar exponential family forms where the sample sum is typically the sufficient statistic. Here, we investigate a shifted exponential distribution, where the unknown parameter $\\theta$ defines the lower bound of the distribution's support [@problem_id:1966035]. This structure leads to a different kind of sufficient statistic: the sample minimum, $X_{(1)}$. This exercise is crucial for developing a deeper understanding of sufficiency and completeness, as it requires deriving the estimator from first principles and highlights how the UMVUE framework can be used to perform a \"bias correction\" on an intuitive but biased initial guess.", "problem": "An electronics manufacturer is conducting reliability tests on a new type of semiconductor device. The testing apparatus has a fixed, but unknown, startup delay of $\\theta$ seconds before it can begin recording the lifetime of a device. As a result, the recorded lifetime $X$ of any given device is always greater than $\\theta$. It is determined that the recorded lifetimes follow a shifted exponential distribution.\n\nLet $X_1, X_2, \\ldots, X_n$ be a random sample of $n$ recorded lifetimes from a population with the probability density function (PDF):\n$$\nf(x; \\theta) = \\exp(-(x-\\theta)) \\quad \\text{for } x > \\theta\n$$\nand $f(x; \\theta) = 0$ otherwise. Here, $\\theta > 0$ is the unknown startup delay parameter.\n\nYour task is to find the Uniformly Minimum Variance Unbiased Estimator (UMVUE) for the parameter $\\theta$. Express your answer in terms of the sample size $n$ and the order statistics of the sample. Use $X_{(1)}$ to denote the minimum value in the sample, i.e., $X_{(1)} = \\min(X_1, X_2, \\ldots, X_n)$.", "solution": "We have a random sample from the shifted exponential family with density\n$$\nf(x;\\theta)=\\exp\\!\\big(-(x-\\theta)\\big)\\quad \\text{for }x>\\theta,\\quad \\theta>0.\n$$\nThe joint density of $X_{1},\\ldots,X_{n}$ is\n$$\nL(\\theta;x_{1},\\ldots,x_{n})=\\prod_{i=1}^{n}\\exp\\!\\big(-(x_{i}-\\theta)\\big)\\,\\mathbf{1}\\{x_{i}>\\theta\\}\n=\\exp\\!\\Big(-\\sum_{i=1}^{n}x_{i}+n\\theta\\Big)\\,\\mathbf{1}\\{\\theta<X_{(1)}\\}.\n$$\nBy the factorization theorem, $T=X_{(1)}$ is a sufficient statistic for $\\theta$.\n\nThe distribution of $T$ is found via the survival function:\n$$\n\\mathbb{P}_{\\theta}(T>t)=\\prod_{i=1}^{n}\\mathbb{P}_{\\theta}(X_{i}>t)\n=\\big(\\exp\\!\\big(-(t-\\theta)\\big)\\big)^{n}=\\exp\\!\\big(-n(t-\\theta)\\big),\\quad t>\\theta.\n$$\nHence the density of $T$ is\n$$\nf_{T}(t;\\theta)=n\\exp\\!\\big(-n(t-\\theta)\\big),\\quad t>\\theta,\n$$\nso that $T-\\theta$ is $\\mathrm{Exp}(n)$ and its distribution does not depend on $\\theta$.\n\nTo show completeness of $T$, suppose a measurable function $a$ satisfies $\\mathbb{E}_{\\theta}[a(T)]=0$ for all $\\theta$. Then\n$$\n0=\\mathbb{E}_{\\theta}[a(T)]=\\int_{\\theta}^{\\infty}a(t)\\,n\\exp\\!\\big(-n(t-\\theta)\\big)\\,\\mathrm{d}t\n=n\\exp(n\\theta)\\int_{\\theta}^{\\infty}a(t)\\exp(-nt)\\,\\mathrm{d}t\n=:F(\\theta).\n$$\nDifferentiate $F(\\theta)$:\n$$\nF'(\\theta)=F(\\theta)-n\\,a(\\theta).\n$$\nSince $F(\\theta)\\equiv 0$ for all $\\theta$, it follows that $0=F'(\\theta)=-n\\,a(\\theta)$ for all $\\theta$, hence $a(\\theta)=0$ for all $\\theta$. Therefore $T$ is complete.\n\nNext, compute the expectation of $X_{(1)}$:\n$$\n\\mathbb{E}_{\\theta}[X_{(1)}]=\\int_{\\theta}^{\\infty}x\\,n\\exp\\!\\big(-n(x-\\theta)\\big)\\,\\mathrm{d}x\n=\\int_{0}^{\\infty}(y+\\theta)\\,n\\exp(-ny)\\,\\mathrm{d}y\n=\\theta\\,n\\int_{0}^{\\infty}\\exp(-ny)\\,\\mathrm{d}y+n\\int_{0}^{\\infty}y\\exp(-ny)\\,\\mathrm{d}y.\n$$\nUsing $\\int_{0}^{\\infty}\\exp(-ny)\\,\\mathrm{d}y=\\frac{1}{n}$ and $\\int_{0}^{\\infty}y\\exp(-ny)\\,\\mathrm{d}y=\\frac{1}{n^{2}}$, we obtain\n$$\n\\mathbb{E}_{\\theta}[X_{(1)}]=\\theta+\\frac{1}{n}.\n$$\nTherefore $X_{(1)}-\\frac{1}{n}$ is an unbiased estimator of $\\theta$. Since it is a function of the complete sufficient statistic $T=X_{(1)}$, by the Lehmann–Scheffé theorem it is the UMVUE for $\\theta$.\n\nThus, the UMVUE for $\\theta$ is $X_{(1)}-\\frac{1}{n}$.", "answer": "$$\\boxed{X_{(1)}-\\frac{1}{n}}$$", "id": "1966035"}]}