{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with a classic application of Basu's Theorem to a scale family. The Rayleigh distribution is often used to model phenomena in communications and physics, and this exercise [@problem_id:1898157] demonstrates a fundamental property: the independence between a complete sufficient statistic for the scale parameter $\\sigma$, and an ancillary statistic that describes the 'shape' of the sample data. This practice provides a clear blueprint for how to apply the theorem to separate information about scale from other properties of the data.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n > 1$ from a Rayleigh distribution with an unknown scale parameter $\\sigma > 0$. The probability density function (PDF) for a single observation $X$ is given by\n$$ f(x; \\sigma) = \\frac{x}{\\sigma^2} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\quad \\text{for } x \\ge 0 $$\nConsider two statistics calculated from this sample:\n1.  The sum of squares, $T = \\sum_{i=1}^{n} X_i^2$.\n2.  The ratio of the sample mean to the sample standard deviation, $V = \\frac{\\bar{X}}{S}$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ is the sample mean and $S = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\bar{X})^2}$ is the sample standard deviation.\n\nA statistic is called *sufficient* for a parameter if it captures all the information about the parameter that is contained in the sample. A statistic is called *complete* if the only real-valued function of the statistic that has an expected value of zero for all values of the parameter is the function that is identically zero. A statistic is called *ancillary* for a parameter if its sampling distribution does not depend on the parameter.\n\nWhich of the following statements about the statistics $T$ and $V$ are true? Select all that apply.\n\nA. The statistic $T$ is ancillary for $\\sigma$.\n\nB. The statistic $V$ is ancillary for $\\sigma$.\n\nC. The statistic $T$ is a complete sufficient statistic for $\\sigma$.\n\nD. The statistics $T$ and $V$ are statistically independent.", "solution": "We start by identifying a key transformation for Rayleigh random variables. For a single observation with density\n$$\nf(x;\\sigma)=\\frac{x}{\\sigma^{2}}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right),\\quad x\\ge 0,\n$$\nlet $Y=X^{2}$. Then $x=\\sqrt{y}$ and $\\frac{dx}{dy}=\\frac{1}{2\\sqrt{y}}$, so the density of $Y$ is\n$$\nf_{Y}(y;\\sigma)=f_{X}(\\sqrt{y};\\sigma)\\cdot\\frac{1}{2\\sqrt{y}}=\\frac{1}{2\\sigma^{2}}\\exp\\!\\left(-\\frac{y}{2\\sigma^{2}}\\right),\\quad y\\ge 0,\n$$\nthat is, $Y\\sim\\text{Exp}(\\text{rate }1/(2\\sigma^{2}))$, or equivalently $Y\\sim\\text{Gamma}(\\text{shape }1,\\text{ rate }1/(2\\sigma^{2}))$.\n\nFor a sample $X_{1},\\dots,X_{n}$, the variables $Y_{i}=X_{i}^{2}$ are independent exponentials with common rate $1/(2\\sigma^{2})$, hence\n$$\nT=\\sum_{i=1}^{n}X_{i}^{2}=\\sum_{i=1}^{n}Y_{i}\\sim\\text{Gamma}\\!\\left(\\text{shape }n,\\ \\text{rate }\\frac{1}{2\\sigma^{2}}\\right),\n$$\nwith density\n$$\nf_{T}(t;\\sigma)=\\frac{\\left(\\frac{1}{2\\sigma^{2}}\\right)^{n}}{\\Gamma(n)}\\,t^{n-1}\\exp\\!\\left(-\\frac{t}{2\\sigma^{2}}\\right),\\quad t\\ge 0.\n$$\n\nAssessment of statement A (T is ancillary): Since the distribution of $T$ depends on $\\sigma$ through the rate (or scale) parameter, $T$ is not ancillary for $\\sigma$. Therefore A is false.\n\nSufficiency of $T$: The joint likelihood of $X_{1},\\dots,X_{n}$ is\n$$\nL(\\sigma;x_{1},\\dots,x_{n})=\\prod_{i=1}^{n}\\frac{x_{i}}{\\sigma^{2}}\\exp\\!\\left(-\\frac{x_{i}^{2}}{2\\sigma^{2}}\\right)\\mathbf{1}_{\\{x_{i}\\ge 0\\}}\n=\\left(\\prod_{i=1}^{n}x_{i}\\right)\\sigma^{-2n}\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}x_{i}^{2}\\right)\\prod_{i=1}^{n}\\mathbf{1}_{\\{x_{i}\\ge 0\\}}.\n$$\nBy the Neyman–Fisher factorization criterion, the data enter $\\sigma$ only through $T=\\sum_{i=1}^{n}x_{i}^{2}$, so $T$ is sufficient for $\\sigma$.\n\nCompleteness of $T$: Suppose $g$ is a measurable function such that $\\mathbb{E}_{\\sigma}[g(T)]=0$ for all $\\sigma>0$. Using the density of $T$,\n$$\n0=\\int_{0}^{\\infty}g(t)\\frac{\\left(\\frac{1}{2\\sigma^{2}}\\right)^{n}}{\\Gamma(n)}\\,t^{n-1}\\exp\\!\\left(-\\frac{t}{2\\sigma^{2}}\\right)\\,dt\\quad\\text{for all }\\sigma>0.\n$$\nLet $\\lambda=\\frac{1}{2\\sigma^{2}}$, so $\\lambda>0$. Multiply both sides by $\\Gamma(n)$ and define $h(t)=g(t)t^{n-1}$. Then\n$$\n\\int_{0}^{\\infty}h(t)\\exp(-\\lambda t)\\,dt=0\\quad\\text{for all }\\lambda>0.\n$$\nThe left-hand side is the Laplace transform of $h$ evaluated at $\\lambda$, and uniqueness of the Laplace transform on $[0,\\infty)$ implies $h(t)=0$ almost everywhere, hence $g(T)=0$ almost surely. Therefore, $T$ is complete. Combining sufficiency and completeness, statement C is true.\n\nAssessment of statement B (V is ancillary): Note that the Rayleigh family is a scale family. Write $X_{i}=\\sigma R_{i}$ with $R_{i}\\sim\\text{Rayleigh}(1)$ i.i.d. Then\n$$\n\\bar{X}=\\sigma\\bar{R},\\qquad S=\\sigma S_{R},\n$$\nwhere $\\bar{R}$ and $S_{R}$ are the sample mean and sample standard deviation computed from $R_{1},\\dots,R_{n}$. Therefore\n$$\nV=\\frac{\\bar{X}}{S}=\\frac{\\bar{R}}{S_{R}},\n$$\nwhose sampling distribution does not depend on $\\sigma$. Hence $V$ is ancillary, and B is true.\n\nAssessment of statement D (independence of T and V): By Basu’s theorem, any complete sufficient statistic is independent of any ancillary statistic. Since $T$ is complete and sufficient for $\\sigma$ and $V$ is ancillary for $\\sigma$, it follows that $T$ and $V$ are independent. Therefore D is true.\n\nCollecting the results: A is false, B is true, C is true, D is true.", "answer": "$$\\boxed{BCD}$$", "id": "1898157"}, {"introduction": "To demonstrate the versatility of Basu's Theorem beyond simple scale families, our next practice explores a random sample from a Beta distribution. This exercise [@problem_id:1898181] introduces a logarithmic transformation, a powerful technique for converting multiplicative structures into additive ones, which simplifies the analysis. You will verify the conditions of Basu's Theorem to establish the independence between the complete sufficient statistic and a cleverly constructed ancillary statistic.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n \\geq 2$ from a population with a probability density function (PDF) given by\n$$f(x; \\theta) = \\theta x^{\\theta-1}$$\nfor $0 < x < 1$ and $\\theta > 0$. This distribution is a specific case of the Beta distribution.\n\nConsider two statistics constructed from this sample:\n1. The product of the observations: $T = \\prod_{i=1}^n X_i$.\n2. A ratio involving the natural logarithms of the observations: $V = \\frac{\\ln X_1}{\\sum_{i=1}^n \\ln X_i}$.\n\nWhich of the following statements correctly describes the statistical relationship between the statistic $T$ and the statistic $V$?\n\nA. $T$ and $V$ are independent.\n\nB. $T$ and $V$ are always positively correlated.\n\nC. $T$ and $V$ are always negatively correlated.\n\nD. The relationship between $T$ and $V$ depends on the value of the parameter $\\theta$.\n\nE. The relationship between $T$ and $V$ is one of direct proportionality, i.e., $T = cV$ for some constant $c$.", "solution": "We begin by reparameterizing the sample to exploit a known independence structure. Let $Y_{i}=-\\ln X_{i}$ for $i=1,\\dots,n$. For $0<x<1$ with density $f(x;\\theta)=\\theta x^{\\theta-1}$ and $y=-\\ln x$, we have $x=\\exp(-y)$, $\\frac{dx}{dy}=-\\exp(-y)$, and hence\n$$\nf_{Y}(y)=f_{X}(\\exp(-y))\\exp(-y)=\\theta \\exp(-y)^{\\theta-1}\\exp(-y)=\\theta \\exp(-\\theta y),\\quad y>0,\n$$\nso $Y_{1},\\dots,Y_{n}$ are independent and identically distributed as $\\operatorname{Exp}(\\text{rate } \\theta)$, equivalently $\\operatorname{Gamma}(\\text{shape }1,\\text{ rate }\\theta)$.\n\nExpress the given statistics in terms of the $Y_{i}$. Let $S=\\sum_{i=1}^{n}Y_{i}$. Then\n$$\n\\ln T=\\sum_{i=1}^{n}\\ln X_{i}=-\\sum_{i=1}^{n}Y_{i}=-S\\quad\\Rightarrow\\quad T=\\exp(-S),\n$$\nand\n$$\nV=\\frac{\\ln X_{1}}{\\sum_{i=1}^{n}\\ln X_{i}}=\\frac{-Y_{1}}{-S}=\\frac{Y_{1}}{S}.\n$$\nThus $T$ is a function of $S$ only, while $V$ is the ratio $Y_{1}/S$.\n\nNext, use the standard Gamma-Dirichlet decomposition. The joint density of $Y=(Y_{1},\\dots,Y_{n})$ is\n$$\nf_{Y}(y_{1},\\dots,y_{n})=\\theta^{n}\\exp\\!\\left(-\\theta\\sum_{i=1}^{n}y_{i}\\right),\\quad y_{i}>0.\n$$\nPerform the change of variables to\n$$\nS=\\sum_{i=1}^{n}Y_{i},\\qquad U_{i}=\\frac{Y_{i}}{S}\\ (i=1,\\dots,n-1),\\qquad U_{n}=1-\\sum_{i=1}^{n-1}U_{i},\n$$\nwith domain $S>0$, $U_{i}>0$, and $\\sum_{i=1}^{n}U_{i}=1$. The Jacobian determinant for $(Y_{1},\\dots,Y_{n})\\mapsto(S,U_{1},\\dots,U_{n-1})$ is $S^{n-1}$. Therefore the joint density becomes\n$$\nf_{S,U}(s,u_{1},\\dots,u_{n-1})=\\theta^{n}\\exp(-\\theta s)\\,s^{n-1}\\,\\mathbf{1}_{\\{s>0\\}}\\mathbf{1}_{\\{u\\in\\Delta_{n-1}\\}},\n$$\nwhere $\\Delta_{n-1}=\\{u\\in(0,1)^{n}:\\sum_{i=1}^{n}u_{i}=1\\}$. This factors as\n$$\n\\left[\\frac{\\theta^{n}}{(n-1)!}s^{n-1}\\exp(-\\theta s)\\,\\mathbf{1}_{\\{s>0\\}}\\right]\\cdot\\left[(n-1)!\\,\\mathbf{1}_{\\{u\\in\\Delta_{n-1}\\}}\\right],\n$$\nshowing that $S\\sim \\operatorname{Gamma}(n,\\text{ rate }\\theta)$ and $U=(U_{1},\\dots,U_{n})\\sim \\operatorname{Dirichlet}(1,\\dots,1)$ are independent. In particular,\n$$\nW\\equiv \\frac{Y_{1}}{S}=U_{1}\\sim \\operatorname{Beta}(1,n-1),\n$$\nand $W$ is independent of $S$.\n\nSince $T=\\exp(-S)$ is a measurable function of $S$ only and $V=Y_{1}/S=W$ is a measurable function of $U$ only, the independence of $(S,U)$ implies that $T$ and $V$ are independent. This does not depend on $\\theta$ and rules out the other options (no fixed sign correlation, no proportionality).", "answer": "$$\\boxed{A}$$", "id": "1898181"}, {"introduction": "A core part of mastering any mathematical theorem is understanding its limitations. This final practice [@problem_id:1898195] serves as a crucial counterexample, investigating a scenario with the discrete Poisson distribution where Basu's Theorem does not apply. By attempting to use the theorem and discovering that a key condition is not met, you will see precisely why independence cannot be concluded and, in fact, why the statistics are dependent.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n > 2$ from a Poisson distribution with unknown parameter $\\lambda > 0$. The probability mass function is given by $P(X=k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}$ for $k = 0, 1, 2, \\ldots$.\n\nConsider two statistics derived from this sample:\n1. The total sum, $T = \\sum_{i=1}^n X_i$.\n2. The parity of the first observation, represented by a Bernoulli random variable $A$, where $A=1$ if $X_1$ is an even integer (including 0), and $A=0$ if $X_1$ is an odd integer.\n\nWhich of the following statements correctly describes the relationship between the statistic $T$ and the statistic $A$?\n\nA. $T$ and $A$ are statistically independent because $A$ is an ancillary statistic.\n\nB. $T$ and $A$ are statistically independent for all values of $\\lambda > 0$.\n\nC. $T$ and $A$ are statistically dependent because the total sum $T$ is not a sufficient statistic for the parameter $\\lambda$.\n\nD. $T$ and $A$ are statistically dependent because the conditional probability of $A=1$ given $T=t$ is a function of $t$.\n\nE. Whether $T$ and $A$ are independent or dependent is determined by the specific numerical value of $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be independent and identically distributed as $\\operatorname{Poisson}(\\lambda)$ with $\\lambda>0$, and let $T=\\sum_{i=1}^{n}X_{i}$. Then $T\\sim \\operatorname{Poisson}(n\\lambda)$. Define $A=\\mathbf{1}\\{X_{1}\\text{ is even}\\}$.\n\nTo study the relationship between $T$ and $A$, compute the conditional distribution of $X_{1}$ given $T=t$. Let $S=\\sum_{i=2}^{n}X_{i}$ so that $S\\sim \\operatorname{Poisson}((n-1)\\lambda)$ and $X_{1}$ and $S$ are independent. For integers $t\\geq 0$ and $0\\leq k\\leq t$,\n$$\n\\mathbb{P}(X_{1}=k\\mid T=t)\n=\\frac{\\mathbb{P}(X_{1}=k,\\,S=t-k)}{\\mathbb{P}(T=t)}\n=\\frac{\\left[\\exp(-\\lambda)\\frac{\\lambda^{k}}{k!}\\right]\\left[\\exp(-(n-1)\\lambda)\\frac{((n-1)\\lambda)^{t-k}}{(t-k)!}\\right]}{\\exp(-n\\lambda)\\frac{(n\\lambda)^{t}}{t!}}.\n$$\nSimplifying,\n$$\n\\mathbb{P}(X_{1}=k\\mid T=t)\n=\\frac{t!}{k!(t-k)!}\\left(\\frac{1}{n}\\right)^{k}\\left(1-\\frac{1}{n}\\right)^{t-k},\n$$\nso $X_{1}\\mid(T=t)\\sim \\operatorname{Binomial}\\!\\left(t,\\frac{1}{n}\\right)$.\n\nTherefore,\n$$\n\\mathbb{P}(A=1\\mid T=t)=\\sum_{\\substack{0\\leq k\\leq t\\\\ k\\ \\text{even}}}\\binom{t}{k}\\left(\\frac{1}{n}\\right)^{k}\\left(1-\\frac{1}{n}\\right)^{t-k}.\n$$\nUsing the binomial identity\n$$\n\\sum_{\\substack{k=0\\\\ k\\ \\text{even}}}^{t}\\binom{t}{k}p^{k}(1-p)^{t-k}\n=\\frac{1}{2}\\left[(p+(1-p))^{t}+((1-p)-p)^{t}\\right]\n=\\frac{1}{2}\\left[1+(1-2p)^{t}\\right],\n$$\nwith $p=\\frac{1}{n}$, we obtain\n$$\n\\mathbb{P}(A=1\\mid T=t)=\\frac{1}{2}\\left[1+\\left(1-\\frac{2}{n}\\right)^{t}\\right].\n$$\nFor $n>2$, the factor $\\left(1-\\frac{2}{n}\\right)\\neq 0$, so $\\mathbb{P}(A=1\\mid T=t)$ is a nonconstant function of $t$. Hence $A$ and $T$ are statistically dependent when $n>2$.\n\nThis directly verifies statement D: they are dependent because the conditional probability of $A=1$ given $T=t$ is a function of $t$. The other options are incorrect: A is false because $A$ is not ancillary (its marginal distribution depends on $\\lambda$, since $\\mathbb{P}(A=1)=\\frac{1}{2}\\left[1+\\exp(-2\\lambda)\\right]$); B is false because dependence holds for all $\\lambda>0$ when $n>2$; C is false because $T$ is in fact sufficient for $\\lambda$ (by the factorization theorem for the Poisson family); E is not correct under the stated condition $n>2$, since in this regime dependence always holds and does not vary with the specific value of $n$.", "answer": "$$\\boxed{D}$$", "id": "1898195"}]}