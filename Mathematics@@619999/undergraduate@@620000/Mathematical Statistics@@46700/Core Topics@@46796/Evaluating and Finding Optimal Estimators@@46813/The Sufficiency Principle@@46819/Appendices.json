{"hands_on_practices": [{"introduction": "The first step in mastering the Sufficiency Principle is to apply the Neyman-Fisher Factorization Theorem in a straightforward context. This exercise provides an ideal starting point, asking for a sufficient statistic for the parameter of a Negative Binomial distribution, a model frequently used in scenarios counting trials until a fixed number of successes occur. By working through this problem [@problem_id:1963683], you will practice the fundamental algebraic technique of factoring the joint likelihood function to isolate the parameter's dependence on the data through a single summary statistic.", "problem": "In a study of particle decay, a physicist observes a specific type of rare decay event. The experiment consists of observing particles one by one until a fixed number, $r$, of these rare decay events have been recorded. The probability that any given observed particle undergoes this specific decay is $p$, which is unknown. This entire experimental procedure is repeated independently $n$ times to create a statistically significant dataset.\n\nFor the $i$-th repetition (where $i=1, \\dots, n$), let the random variable $Y_i$ represent the total number of particles that had to be observed to see the $r$-th decay event. Each $Y_i$ is independent and follows a Negative Binomial distribution, with the probability mass function (PMF) given by:\n$$f(y_i; p) = \\binom{y_i-1}{r-1} p^r (1-p)^{y_i-r}, \\quad \\text{for } y_i \\in \\{r, r+1, r+2, \\dots\\}$$\nHere, the parameter $r$ is a known, fixed positive integer.\n\nBased on a random sample of observations $Y_1, Y_2, \\dots, Y_n$, determine a sufficient statistic for the unknown parameter $p$. Your answer should be an expression in terms of the sample random variables $Y_1, \\dots, Y_n$.", "solution": "Let $\\mathbf{y} = (y_1, \\dots, y_n)$ be the vector of observations. Each $Y_{i}$ has pmf $f(y_{i};p)=\\binom{y_{i}-1}{r-1}p^{r}(1-p)^{y_{i}-r}$ for $y_{i}\\in\\{r,r+1,\\dots\\}$, with fixed known $r$. For independent observations $Y_{1},\\dots,Y_{n}$, the joint likelihood is\n$$\nL(p;\\mathbf{y})=\\prod_{i=1}^{n}\\binom{y_{i}-1}{r-1}p^{r}(1-p)^{y_{i}-r}.\n$$\nCollecting terms in $p$ and $(1-p)$ gives\n$$\nL(p;\\mathbf{y})=\\left[\\prod_{i=1}^{n}\\binom{y_{i}-1}{r-1}\\right]\\,p^{nr}\\,(1-p)^{\\sum_{i=1}^{n}y_{i}-nr}.\n$$\nThis factors as $L(p;\\mathbf{y})=h(\\mathbf{y})\\,g(T(\\mathbf{y}),p)$ with\n$$\nh(\\mathbf{y})=\\prod_{i=1}^{n}\\binom{y_{i}-1}{r-1},\\quad T(\\mathbf{y})=\\sum_{i=1}^{n}y_{i},\\quad g(T,p)=p^{nr}(1-p)^{T-nr},\n$$\nwhere $h$ does not depend on $p$ and $g$ depends on the data only through $T(\\mathbf{y})$. By the Neyman–Fisher factorization theorem, $T(Y)=\\sum_{i=1}^{n}Y_{i}$ is sufficient for $p$. Since $n$ and $r$ are known constants, any one-to-one function of $T$, such as $\\sum_{i=1}^{n}(Y_{i}-r)$, is also sufficient; a conventional choice is $T=\\sum_{i=1}^{n}Y_{i}$.", "answer": "$$\\boxed{\\sum_{i=1}^{n} Y_{i}}$$", "id": "1963683"}, {"introduction": "We now move to a more nuanced application of sufficiency, dealing with distributions where the sample space itself depends on the parameter of interest. This problem presents a geometric scenario involving a particle detector, where the data points are uniformly distributed within a circle of unknown radius $R$. Here [@problem_id:1963659], the sufficient statistic is not a simple sum or mean, but rather a value that defines the observed boundary of the data, demonstrating a key feature of such \"range-dependent\" models.", "problem": "A circular particle detector with an unknown radius $R$ is centered at the origin of a 2D coordinate system. It is known from the physics of the experiment that when particles are detected, their impact locations $(X, Y)$ are uniformly distributed over the area of the detector. An experiment is conducted, and a random sample of $n$ particle impact locations is recorded: $(X_1, Y_1), (X_2, Y_2), \\dots, (X_n, Y_n)$.\n\nAccording to the principle of sufficiency in statistical inference, a sufficient statistic for a parameter is a statistic that captures all the information about the parameter that is contained in the sample. Which one of the following statistics is a sufficient statistic for the unknown radius $R$?\n\nA. $T_A = \\max_{i=1,...,n} \\sqrt{X_i^2 + Y_i^2}$\n\nB. $T_B = \\sum_{i=1}^n (X_i^2 + Y_i^2)$\n\nC. $T_C = \\left(\\frac{1}{n}\\sum_{i=1}^n X_i, \\frac{1}{n}\\sum_{i=1}^n Y_i\\right)$\n\nD. $T_D = \\frac{1}{n} \\sum_{i=1}^n \\sqrt{X_i^2 + Y_i^2}$\n\nE. $T_E = \\max_{i=1,...,n} |X_i|$", "solution": "Let $(X_{i},Y_{i})$, $i=1,\\dots,n$, be i.i.d. uniform over the disk of radius $R$ centered at the origin. The joint density of a single observation is\n$$\nf_{X,Y}(x,y;R)=\\frac{1}{\\pi R^{2}}\\mathbf{1}\\{x^{2}+y^{2}\\le R^{2}\\},\n$$\nso the joint density (likelihood) for the sample is\n$$\nL(R;\\{(x_{i},y_{i})\\}_{i=1}^{n})=\\prod_{i=1}^{n}\\frac{1}{\\pi R^{2}}\\mathbf{1}\\{x_{i}^{2}+y_{i}^{2}\\le R^{2}\\}=(\\pi R^{2})^{-n}\\,\\mathbf{1}\\Big\\{\\max_{1\\le i\\le n}(x_{i}^{2}+y_{i}^{2})\\le R^{2}\\Big\\}.\n$$\nDefine the sample statistic\n$$\nT_{A}=\\max_{1\\le i\\le n}\\sqrt{X_{i}^{2}+Y_{i}^{2}}.\n$$\nThen the likelihood can be written as\n$$\nL(R;\\text{data})=(\\pi R^{2})^{-n}\\,\\mathbf{1}\\{T_{A}\\le R\\},\n$$\nwhich is of the form $g(T_{A},R)h(\\text{data})$ with $g(T_{A},R)=(\\pi R^{2})^{-n}\\mathbf{1}\\{T_{A}\\le R\\}$ and $h(\\text{data})=1$. By the Neyman–Fisher factorization theorem, $T_{A}$ is sufficient for $R$.\n\nTo see that the other options are not sufficient, note that the indicator in the likelihood depends on the sample only through $\\max_{i}(X_{i}^{2}+Y_{i}^{2})$, equivalently $T_{A}$. Any statistic that does not determine this maximum radius cannot render the factorization independent of the remaining sample details:\n- $T_{B}=\\sum_{i=1}^{n}(X_{i}^{2}+Y_{i}^{2})$ does not determine the maximum; different samples can have the same sum of squares but different maxima, leading to different support sets $\\{R:T_{A}\\le R\\}$.\n- $T_{C}=(\\bar{X},\\bar{Y})$ does not determine the support boundary and does not capture the constraint $\\max_{i}(X_{i}^{2}+Y_{i}^{2})\\le R^{2}$.\n- $T_{D}=\\frac{1}{n}\\sum_{i=1}^{n}\\sqrt{X_{i}^{2}+Y_{i}^{2}}$ again averages radii and does not determine the maximum radius.\n- $T_{E}=\\max_{i}|X_{i}|$ ignores the $Y$-coordinates in the radial constraint; two samples can have the same $T_{E}$ but different $\\max_{i}\\sqrt{X_{i}^{2}+Y_{i}^{2}}$, so it cannot determine the support.\n\nTherefore, among the given options, the sufficient statistic for $R$ is $T_{A}$.", "answer": "$$\\boxed{A}$$", "id": "1963659"}, {"introduction": "Our final practice challenges the intuition that a single unknown parameter always corresponds to a single summary statistic. We investigate a Normal distribution where the mean and standard deviation are both functions of a single parameter, $\\theta$. This hypothetical model of semiconductor activation energy requires us to find a sufficient statistic that captures all information about $\\theta$ [@problem_id:1963696]. You will discover that a two-dimensional statistic is necessary, illustrating that the dimensionality of the sufficient statistic is determined by the structure of the likelihood function, not just the number of parameters.", "problem": "In the development of a novel semiconductor device, the activation energy, denoted by $X$, is a critical parameter. Due to quantum effects at the nanoscale, the manufacturing process results in devices where the activation energy is a random variable. A theoretical model, supported by preliminary experiments, posits that $X$ follows a normal distribution where the mean $\\theta$ is tunable, but the standard deviation is intrinsically linked to the mean. Specifically, for a given mean $\\theta > 0$, the variance of the distribution is $\\theta^2$. We denote this distribution as $N(\\theta, \\theta^2)$.\n\nA quality control engineer collects a random sample of $n$ devices, with activation energies $X_1, X_2, \\dots, X_n$, all produced under identical conditions corresponding to the parameter $\\theta$. To make inferences about $\\theta$, the engineer wishes to compress the data into a lower-dimensional summary without losing any information about the parameter. Such a summary is known as a sufficient statistic.\n\nWhich of the following two-dimensional statistics are sufficient for the parameter $\\theta$? Note that more than one option may be correct.\n\nA. $T_A = \\left(\\sum_{i=1}^{n} X_i, \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right)$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nB. $T_B = \\left(\\bar{X}, \\sum_{i=1}^{n} X_i^2\\right)$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nC. $T_C = \\left(\\sum_{i=1}^{n} X_i, \\frac{1}{n}\\sum_{i=1}^{n} X_i^2\\right)$.\n\nD. $T_D = \\left(\\bar{X}, \\left(\\sum_{i=1}^{n} X_i\\right)^2\\right)$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nE. $T_E = \\left(\\sum_{i=1}^{n} X_i^2, \\left(\\sum_{i=1}^{n} X_i\\right)^2\\right)$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. $N(\\theta,\\theta^{2})$ with $\\theta>0$, and let $\\mathbf{x} = (x_1, \\dots, x_n)$ be the vector of observations. The joint density is\n$$\nf(\\mathbf{x}\\mid\\theta)\n=(2\\pi)^{-n/2}\\theta^{-n}\\exp\\!\\left(-\\frac{1}{2\\theta^{2}}\\sum_{i=1}^{n}(x_{i}-\\theta)^{2}\\right).\n$$\nExpanding the quadratic term,\n$$\n\\sum_{i=1}^{n}(x_{i}-\\theta)^{2}\n=\\sum_{i=1}^{n}x_{i}^{2}-2\\theta\\sum_{i=1}^{n}x_{i}+n\\theta^{2},\n$$\nso the likelihood can be written as\n$$\nL(\\theta;\\mathbf{x})=C(\\mathbf{x})\\,\\theta^{-n}\\exp\\!\\left(-\\frac{1}{2\\theta^{2}}\\sum_{i=1}^{n}x_{i}^{2}+\\frac{1}{\\theta}\\sum_{i=1}^{n}x_{i}-\\frac{n}{2}\\right),\n$$\nwhere $C(\\mathbf{x})=(2\\pi)^{-n/2}$ does not depend on $\\theta$. Thus,\n$$\nL(\\theta;\\mathbf{x})=h(\\mathbf{x})\\,g_{\\theta}\\!\\left(\\sum_{i=1}^{n}x_{i},\\sum_{i=1}^{n}x_{i}^{2}\\right),\n$$\nshowing by the Neyman–Fisher factorization that $T_{\\min}=(\\sum_{i=1}^{n}X_{i},\\sum_{i=1}^{n}X_{i}^{2})$ is sufficient. To verify minimal sufficiency, use the likelihood ratio criterion: for two sample vectors $\\mathbf{x}$ and $\\mathbf{y}$,\n$$\n\\frac{L(\\theta;\\mathbf{x})}{L(\\theta;\\mathbf{y})}=\\left(\\frac{\\theta^{-n}}{\\theta^{-n}}\\right)\\exp\\!\\left(-\\frac{1}{2\\theta^{2}}\\left[\\sum x_{i}^{2}-\\sum y_{i}^{2}\\right]+\\frac{1}{\\theta}\\left[\\sum x_{i}-\\sum y_{i}\\right]\\right),\n$$\nwhich is independent of $\\theta$ if and only if $\\sum x_{i}=\\sum y_{i}$ and $\\sum x_{i}^{2}=\\sum y_{i}^{2}$. Hence $T_{\\min}$ is minimal sufficient. Therefore, a statistic $S$ is sufficient if and only if $T_{\\min}$ is a measurable function of $S$, i.e., $S$ determines both $\\sum X_{i}$ and $\\sum X_{i}^{2}$.\n\nNow check each proposed statistic:\n\nA. $T_{A}=\\left(\\sum X_{i},\\sum (X_{i}-\\bar{X})^{2}\\right)$. Since $\\sum (X_{i}-\\bar{X})^{2}=\\sum X_{i}^{2}-n\\bar{X}^{2}=\\sum X_{i}^{2}-\\frac{(\\sum X_{i})^{2}}{n}$, we can recover $\\sum X_{i}^{2}$ as $\\sum (X_{i}-\\bar{X})^{2}+(\\sum X_{i})^{2}/n$. Thus $T_{\\min}$ is a function of $T_{A}$, so $T_{A}$ is sufficient.\n\nB. $T_{B}=(\\bar{X},\\sum X_{i}^{2})$. From $\\bar{X}$ we get $\\sum X_{i}=n\\bar{X}$. Hence $T_{\\min}$ is a function of $T_{B}$, so $T_{B}$ is sufficient.\n\nC. $T_{C}=\\left(\\sum X_{i},\\frac{1}{n}\\sum X_{i}^{2}\\right)$. Multiplying the second component by $n$ recovers $\\sum X_{i}^{2}$. Thus $T_{\\min}$ is a function of $T_{C}$, so $T_{C}$ is sufficient.\n\nD. $T_{D}=(\\bar{X},(\\sum X_{i})^{2})$. This determines $\\sum X_{i}=n\\bar{X}$ (including its sign), but it does not determine $\\sum X_{i}^{2}$. Therefore $T_{\\min}$ is not a function of $T_{D}$, and $T_{D}$ is not sufficient.\n\nE. $T_{E}=\\left(\\sum X_{i}^{2},(\\sum X_{i})^{2}\\right)$. This determines $\\sum X_{i}^{2}$ and only the square of $\\sum X_{i}$; the sign of $\\sum X_{i}$ is not determined, and the likelihood depends on $\\sum X_{i}$ through $\\exp((1/\\theta)\\sum X_{i})$, so the sign matters for $\\theta>0$. Hence $T_{\\min}$ is not a function of $T_{E}$, and $T_{E}$ is not sufficient.\n\nTherefore, the sufficient statistics among the options are A, B, and C.", "answer": "$$\\boxed{ABC}$$", "id": "1963696"}]}