{"hands_on_practices": [{"introduction": "The Rao-Blackwell theorem provides a powerful, constructive method for improving an existing unbiased estimator. This first exercise serves as a foundational illustration, where we begin with a simple, intuitive estimator for the mean of a Geometric distribution—a single observation, $X_1$. By conditioning on the complete sufficient statistic, you will see how the theorem systematically incorporates information from the entire sample to produce a much better estimator, which in this case turns out to be a very familiar and optimal one.", "problem": "A software engineer is testing a new stochastic algorithm. The number of attempts required for the algorithm to succeed for the first time is a random variable $X$ that follows a Geometric distribution with probability mass function $P(X=k) = (1-p)^{k-1}p$ for $k=1, 2, 3, \\ldots$, where $p$ is the unknown probability of success on any given attempt. The engineer runs the experiment independently $n$ times, obtaining a random sample $X_1, X_2, \\ldots, X_n$.\n\nThe parameter of interest is the expected number of attempts for a success, $\\theta = 1/p$. An initial, simple unbiased estimator for $\\theta$ is $T_0 = X_1$. Your task is to find the Uniformly Minimum Variance Unbiased Estimator (UMVUE) for $\\theta$.\n\nExpress your answer in terms of the sample sum $S = \\sum_{i=1}^n X_i$ and the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be independent $\\operatorname{Geom}(p)$ with $P(X=k)=(1-p)^{k-1}p$ for $k=1,2,\\ldots$, and let $S=\\sum_{i=1}^{n}X_{i}$. The joint pmf factors as\n$$\nP(X_{1}=x_{1},\\ldots,X_{n}=x_{n})\n=\\prod_{i=1}^{n}\\left[(1-p)^{x_{i}-1}p\\right]\n=p^{n}(1-p)^{\\sum_{i=1}^{n}(x_{i}-1)}\n=p^{n}(1-p)^{S-n},\n$$\nwhich depends on $p$ only through $S$, so by the factorization theorem $S$ is sufficient for $p$.\n\nThe distribution of $S$ is negative binomial with\n$$\nP(S=s)=\\binom{s-1}{n-1}p^{n}(1-p)^{s-n},\\quad s=n,n+1,\\ldots\n$$\nTo show completeness, suppose $g$ satisfies $\\mathbb{E}_{p}[g(S)]=0$ for all $p\\in(0,1)$. Then\n$$\n\\sum_{s=n}^{\\infty}g(s)\\binom{s-1}{n-1}p^{n}(1-p)^{s-n}=0\\quad\\text{for all }p\\in(0,1).\n$$\nDividing by $p^{n}$ and setting $q=1-p$ gives\n$$\n\\sum_{k=0}^{\\infty}g(n+k)\\binom{n+k-1}{n-1}q^{k}=0\\quad\\text{for all }q\\in(0,1).\n$$\nThis is a power series in $q$ that vanishes on an interval, so all coefficients are zero, implying $g(n+k)\\binom{n+k-1}{n-1}=0$ for all $k$, hence $g(s)=0$ for all $s$. Therefore, $S$ is complete and sufficient.\n\nThe parameter of interest is $\\theta=1/p$. Since\n$$\n\\mathbb{E}[X_{1}]=p\\sum_{k=1}^{\\infty}k(1-p)^{k-1}\n=p\\cdot\\frac{1}{\\left(1-(1-p)\\right)^{2}}\n=\\frac{1}{p}=\\theta,\n$$\nthe estimator $T_{0}=X_{1}$ is unbiased for $\\theta$. By Rao–Blackwell, the estimator\n$$\nT^{*}=\\mathbb{E}[T_{0}\\mid S]=\\mathbb{E}[X_{1}\\mid S]\n$$\nhas variance no larger than that of $T_{0}$ and is a function of the sufficient statistic $S$. By exchangeability given $S=s$, all $\\mathbb{E}[X_{i}\\mid S=s]$ are equal and\n$$\n\\sum_{i=1}^{n}\\mathbb{E}[X_{i}\\mid S=s]=\\mathbb{E}\\left[\\sum_{i=1}^{n}X_{i}\\mid S=s\\right]=s,\n$$\nso $\\mathbb{E}[X_{1}\\mid S=s]=s/n$. Therefore,\n$$\nT^{*}=\\frac{S}{n}.\n$$\nSince $S$ is complete and sufficient, the Lehmann–Scheffé theorem implies $T^{*}=S/n$ is the UMVUE for $\\theta$.\n\nFinally, note $\\mathbb{E}[S/n]=\\mathbb{E}[X_{1}]=1/p=\\theta$, confirming unbiasedness.", "answer": "$$\\boxed{\\frac{S}{n}}$$", "id": "1950082"}, {"introduction": "Building on the principles from the previous exercise [@problem_id:1950082], we now turn our attention to a continuous distribution. This problem explores a hypothetical scenario of estimating a parameter $\\Theta$ from a Uniform$(0, \\Theta)$ distribution. Here, the sufficient statistic is not a sum but the maximum order statistic, $M_{(n)}$, providing a valuable contrast and demonstrating the versatility of the Rao-Blackwellization process across different types of distributions and statistics.", "problem": "In a simplified cosmological model, the mass $M$ of a newly formed micro-halo is assumed to be a random variable following a uniform distribution on the interval $(0, \\Theta)$, where $\\Theta > 0$ is an unknown cosmic parameter representing the maximum possible mass for such a halo in a given region of space. A team of astrophysicists collects a random sample of $n$ such micro-halo masses, denoted $M_1, M_2, \\ldots, M_n$.\n\nA junior researcher proposes using the sample range, $R = M_{(n)} - M_{(1)}$, as a simple statistic related to the parameter $\\Theta$. Here, $M_{(1)}$ and $M_{(n)}$ are the minimum and maximum observed masses in the sample, respectively. While $R$ is a valid statistic, it is not optimal because it does not use all the information contained in the complete sufficient statistic for $\\Theta$.\n\nAccording to the Rao-Blackwell theorem, an improved estimator can be constructed by conditioning an initial estimator on a sufficient statistic. Your task is to apply this principle to improve upon the sample range $R$. Derive the improved estimator by computing the conditional expectation of $R$ given the complete sufficient statistic for $\\Theta$. Express your final answer as an analytical expression in terms of $M_{(n)}$ and the sample size $n$.", "solution": "Let $M_{1},\\ldots,M_{n}$ be i.i.d. $\\mathrm{Unif}(0,\\Theta)$. The maximum order statistic $M_{(n)}$ is a complete sufficient statistic for $\\Theta$ (its distribution depends on $\\Theta$ only through the scale and $M_{(n)}/\\Theta$ has a $\\mathrm{Beta}(n,1)$ distribution, which yields completeness). By the Rao-Blackwell theorem, conditioning the initial estimator $R=M_{(n)}-M_{(1)}$ on $M_{(n)}$ yields an improved estimator:\n$$\n\\delta^{*}(M_{(n)})=\\mathbb{E}[R \\mid M_{(n)}]=\\mathbb{E}[M_{(n)}-M_{(1)} \\mid M_{(n)}].\n$$\nWrite $t$ for a realized value of $M_{(n)}$. Given $M_{(n)}=t$, the remaining $n-1$ observations are distributed as i.i.d. $\\mathrm{Unif}(0,t)$. Let $V_{(1)}$ denote the minimum of these $n-1$ i.i.d. $\\mathrm{Unif}(0,t)$ variables. Then\n$$\n\\mathbb{E}[R \\mid M_{(n)}=t]=t-\\mathbb{E}[V_{(1)}].\n$$\nTo compute $\\mathbb{E}[V_{(1)}]$, let $U_{1},\\ldots,U_{m}$ be i.i.d. $\\mathrm{Unif}(0,1)$ with $m=n-1$, and $U_{(1)}=\\min(U_{1},\\ldots,U_{m})$. The density of $U_{(1)}$ is $f_{U_{(1)}}(u)=m(1-u)^{m-1}$ for $0<u<1$. Hence\n$$\n\\mathbb{E}[U_{(1)}]=\\int_{0}^{1}u\\,m(1-u)^{m-1}\\,du\n= m\\,\\mathrm{B}(2,m)\n= m\\,\\frac{\\Gamma(2)\\Gamma(m)}{\\Gamma(m+2)}\n= \\frac{1}{m+1}.\n$$\nBy scaling, $V_{(1)}\\stackrel{d}{=}t\\,U_{(1)}$, so $\\mathbb{E}[V_{(1)}]=t\\,\\mathbb{E}[U_{(1)}]=t\\cdot\\frac{1}{m+1}=t\\cdot\\frac{1}{n}$. Therefore,\n$$\n\\mathbb{E}[R \\mid M_{(n)}=t]=t-\\frac{t}{n}=t\\cdot\\frac{n-1}{n}.\n$$\nReplacing $t$ by $M_{(n)}$ gives the Rao-Blackwellized estimator\n$$\n\\delta^{*}(M_{(n)})=\\frac{n-1}{n}\\,M_{(n)}.\n$$\nThis is a function of the complete sufficient statistic $M_{(n)}$ and is thus the desired improved estimator.", "answer": "$$\\boxed{\\frac{n-1}{n}\\,M_{(n)}}$$", "id": "1950098"}, {"introduction": "This final practice problem elevates the complexity by considering a Uniform distribution on the interval $[\\theta, 2\\theta]$, where the sufficient statistic is a two-dimensional vector composed of the minimum and maximum order statistics, $(X_{(1)}, X_{(n)})$. This exercise will challenge you to extend the conditioning argument you developed in the previous problems [@problem_id:1950082] [@problem_id:1950098] to a vector-valued statistic. Successfully solving this demonstrates a deeper mastery of the Rao-Blackwell theorem and its application in more sophisticated estimation scenarios.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n \\ge 2$ drawn from a Uniform distribution on the interval $[\\theta, 2\\theta]$, where the parameter $\\theta > 0$ is unknown. An initial unbiased estimator for $\\theta$ is given by $T = \\frac{2}{3}X_1$. Based on this information, find the improved estimator for $\\theta$ that results from applying the Rao-Blackwell theorem to $T$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be iid with density $f(x\\mid\\theta)=\\theta^{-1}\\,\\mathbf{1}\\{\\theta\\le x\\le 2\\theta\\}$. The joint density is\n$$\nf(x_{1},\\ldots,x_{n}\\mid\\theta)=\\theta^{-n}\\,\\mathbf{1}\\{\\theta\\le X_{(1)},\\;X_{(n)}\\le 2\\theta\\}\n=\\theta^{-n}\\,\\mathbf{1}\\{X_{(n)}/2\\le \\theta\\le X_{(1)}\\}.\n$$\nBy the Neyman–Fisher factorization theorem, the statistic $S=(X_{(1)},X_{(n)})$ is sufficient for $\\theta$.\n\nThe Rao–Blackwell theorem states that the improved estimator based on the initial unbiased estimator $T=\\frac{2}{3}X_{1}$ is\n$$\n\\tilde{T}=E\\!\\left[T\\mid S\\right]=E\\!\\left[\\frac{2}{3}X_{1}\\mid X_{(1)},X_{(n)}\\right]=\\frac{2}{3}\\,E\\!\\left[X_{1}\\mid X_{(1)},X_{(n)}\\right].\n$$\nSet $a=X_{(1)}$ and $b=X_{(n)}$. By exchangeability of the sample, conditional on the full set of order statistics, each labeled observation equals each order statistic with probability $1/n$. Hence\n$$\nE\\!\\left[X_{1}\\mid X_{(1)}=a,X_{(2)},\\ldots,X_{(n-1)},X_{(n)}=b\\right]=\\frac{1}{n}\\sum_{k=1}^{n}X_{(k)}.\n$$\nTaking conditional expectation with respect to $X_{(2)},\\ldots,X_{(n-1)}$ given $a$ and $b$, and using that given $(a,b)$ the middle order statistics are those of $n-2$ iid $\\mathrm{Uniform}(a,b)$ variables, we obtain\n$$\nE\\!\\left[X_{1}\\mid X_{(1)}=a,X_{(n)}=b\\right]\n=\\frac{1}{n}\\left(a+b+\\sum_{k=2}^{n-1}E\\!\\left[X_{(k)}\\mid a,b\\right]\\right)\n=\\frac{1}{n}\\left(a+b+(n-2)\\frac{a+b}{2}\\right)\n=\\frac{a+b}{2}.\n$$\nTherefore,\n$$\n\\tilde{T}\n=\\frac{2}{3}\\cdot\\frac{X_{(1)}+X_{(n)}}{2}\n=\\frac{X_{(1)}+X_{(n)}}{3}.\n$$\nSince $E[\\tilde{T}]=E[E[T\\mid S]]=E[T]=\\theta$, the estimator $\\tilde{T}$ is unbiased and has variance no larger than that of $T$, as guaranteed by Rao–Blackwell.", "answer": "$$\\boxed{\\frac{X_{(1)}+X_{(n)}}{3}}$$", "id": "1950044"}]}