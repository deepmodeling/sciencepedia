{"hands_on_practices": [{"introduction": "The fundamental concept of Fisher information can often be best understood by starting with a simple, discrete scenario. This practice problem provides such a case, using a relatable quality control example where a manufactured switch is classified into one of three distinct categories. This exercise is valuable because it demonstrates the direct calculation of Fisher information from a probability mass function and highlights how information is quantified even with non-binary, categorical outcomes. [@problem_id:1918236]", "problem": "In a quality control process for a newly designed electronic switch, each switch is tested and classified into one of three categories. A random variable $X$ represents the outcome of a test for a single switch, where:\n- $X=0$ if the switch is fully functional.\n- $X=1$ if the switch has a minor, correctable flaw.\n- $X=2$ if the switch has a major, uncorrectable flaw.\n\nThe manufacturing process is governed by a single parameter $\\theta \\in (0, 1)$, which represents the probability that a switch is flawed (either minor or major). The probability mass function (PMF) for $X$ is given by:\n- $P(X=0) = 1-\\theta$\n- $P(X=1) = \\frac{\\theta}{2}$\n- $P(X=2) = \\frac{\\theta}{2}$\n\nThis model assumes that if a switch is flawed, it is equally likely to have a minor or a major flaw. Based on a single observation of $X$, determine the Fisher information, $I(\\theta)$, for the parameter $\\theta$.", "solution": "Let $X$ have the PMF $P_{\\theta}(X=0)=1-\\theta$, $P_{\\theta}(X=1)=\\frac{\\theta}{2}$, $P_{\\theta}(X=2)=\\frac{\\theta}{2}$, with $\\theta\\in(0,1)$. For a single observation $X=x$, the log-likelihood is\n$$\n\\ell(\\theta;x)=\n\\begin{cases}\n\\ln(1-\\theta), & x=0,\\\\\n\\ln\\theta - \\ln 2, & x=1,2.\n\\end{cases}\n$$\nThe score function $S(\\theta;x)=\\frac{\\partial}{\\partial\\theta}\\ell(\\theta;x)$ is\n$$\nS(\\theta;x)=\n\\begin{cases}\n-\\frac{1}{1-\\theta}, & x=0,\\\\\n\\frac{1}{\\theta}, & x=1,2.\n\\end{cases}\n$$\nThe Fisher information for one observation is $I(\\theta)=\\mathbb{E}_{\\theta}[S(\\theta;X)^{2}]$. Using the PMF,\n$$\nI(\\theta)=(1-\\theta)\\left(\\frac{1}{1-\\theta}\\right)^{2}+\\frac{\\theta}{2}\\left(\\frac{1}{\\theta}\\right)^{2}+\\frac{\\theta}{2}\\left(\\frac{1}{\\theta}\\right)^{2}\n=\\frac{1}{1-\\theta}+\\frac{1}{\\theta}\n=\\frac{1}{\\theta(1-\\theta)}.\n$$\nEquivalently, using $I(\\theta)=-\\mathbb{E}_{\\theta}\\left[\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\ell(\\theta;X)\\right]$ gives the same result, since $\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\ell(\\theta;0)=-\\frac{1}{(1-\\theta)^{2}}$ and $\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\ell(\\theta;1)=\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\ell(\\theta;2)=-\\frac{1}{\\theta^{2}}$.", "answer": "$$\\boxed{\\frac{1}{\\theta\\left(1-\\theta\\right)}}$$", "id": "1918236"}, {"introduction": "Moving from discrete outcomes to continuous measurements, we encounter distributions that are cornerstones of modern science and engineering. This exercise focuses on the Normal distribution, but with a specific and practical goal: to determine the information available for the variance parameter, $\\theta = \\sigma^{2}$, when the mean is already known. Such a scenario is common in calibration and sensor design, and this problem will give you hands-on experience in applying the Fisher information formula to understand the precision of variance estimation. [@problem_id:1918246]", "problem": "An advanced materials science lab is developing a novel nanoscale sensor to measure temperature. The sensor's output is an electrical signal, represented by a random variable $X$. According to the sensor's design model, a single measurement $X$ follows a normal distribution with a mean $\\mu_0$ that is a known, precisely calibrated constant, and a variance $\\theta$. This variance $\\theta$ is directly proportional to the absolute temperature that the sensor is exposed to, making it the parameter of interest. To quantify the theoretical limit of precision for estimating this temperature-dependent variance, the research team needs to calculate the Fisher information for $\\theta$ contained in a single measurement $X$.\n\nAssume a single observation $X$ is drawn from a normal distribution with known mean $\\mu_0$ and unknown variance $\\theta = \\sigma^2$. Find the Fisher information, $I(\\theta)$, for the parameter $\\theta$.", "solution": "We model a single observation $X$ under the normal density with known mean $\\mu_{0}$ and unknown variance $\\theta$:\n$$\nf(x;\\theta)=\\frac{1}{\\sqrt{2\\pi\\theta}}\\exp\\!\\left(-\\frac{(x-\\mu_{0})^{2}}{2\\theta}\\right).\n$$\nThe log-likelihood for a single observation $x$ is\n$$\n\\ell(\\theta;x)=\\ln f(x;\\theta)=-\\frac{1}{2}\\ln(2\\pi)-\\frac{1}{2}\\ln\\theta-\\frac{(x-\\mu_{0})^{2}}{2\\theta}.\n$$\nDifferentiate with respect to $\\theta$ to obtain the score:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta}=-\\frac{1}{2\\theta}+\\frac{(x-\\mu_{0})^{2}}{2\\theta^{2}}.\n$$\nDifferentiate again to obtain the observed information (negative Hessian):\n$$\n\\frac{\\partial^{2} \\ell}{\\partial \\theta^{2}}=\\frac{1}{2\\theta^{2}}-\\frac{(x-\\mu_{0})^{2}}{\\theta^{3}}.\n$$\nThe Fisher information is the negative expectation of the second derivative under the model:\n$$\nI(\\theta)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2} \\ell}{\\partial \\theta^{2}}\\right]\n=-\\left(\\frac{1}{2\\theta^{2}}-\\frac{\\mathbb{E}\\!\\left[(X-\\mu_{0})^{2}\\right]}{\\theta^{3}}\\right).\n$$\nSince $X\\sim \\mathcal{N}(\\mu_{0},\\theta)$, we have $\\mathbb{E}\\!\\left[(X-\\mu_{0})^{2}\\right]=\\theta$. Substituting gives\n$$\nI(\\theta)=-\\left(\\frac{1}{2\\theta^{2}}-\\frac{\\theta}{\\theta^{3}}\\right)\n=-\\left(\\frac{1}{2\\theta^{2}}-\\frac{1}{\\theta^{2}}\\right)\n=\\frac{1}{2\\theta^{2}}.\n$$\nTherefore, the Fisher information in a single observation for the variance parameter $\\theta$ is $\\frac{1}{2\\theta^{2}}$.", "answer": "$$\\boxed{\\frac{1}{2\\theta^{2}}}$$", "id": "1918246"}, {"introduction": "This final practice problem demonstrates the application of Fisher information in a sophisticated engineering context: wireless communications. The amplitude of a signal propagating through a complex environment is often modeled by the Rayleigh distribution, and your task is to quantify the information a single signal measurement provides about the parameter $\\sigma$. This exercise not only reinforces the computational techniques for continuous distributions but also solidifies the link between abstract statistical theory and its crucial role in practical signal processing applications. [@problem_id:1918268]", "problem": "In wireless communications, the amplitude of a signal that has traveled through a complex environment with many scatterers can often be modeled by a Rayleigh distribution. Let the random variable $X$ represent the measured amplitude of such a signal. Its behavior is described by the Probability Density Function (PDF) given by:\n$$ f(x; \\sigma) = \\frac{x}{\\sigma^2} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\quad \\text{for } x > 0 $$\nand $f(x; \\sigma) = 0$ for $x \\leq 0$. The parameter $\\sigma > 0$ is related to the average power of the received signal.\n\nAn engineer wants to estimate the parameter $\\sigma$ from a single measurement of the amplitude $X$. A key quantity for understanding the quality of any estimator is the Fisher information, which measures the amount of information that an observation carries about an unknown parameter.\n\nDetermine the Fisher information, $I(\\sigma)$, for the parameter $\\sigma$ based on a single observation $X$. Express your answer as a function of $\\sigma$.", "solution": "We seek the Fisher information for a single observation, defined by\n$$\nI(\\sigma) = \\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\sigma}\\ln f(X;\\sigma)\\right)^{2}\\right],\n$$\nwhere for the Rayleigh distribution\n$$\nf(x;\\sigma) = \\frac{x}{\\sigma^{2}} \\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right), \\quad x>0.\n$$\nThe log-likelihood for one observation is\n$$\n\\ln f(x;\\sigma) = \\ln x - 2 \\ln \\sigma - \\frac{x^{2}}{2\\sigma^{2}}.\n$$\nDifferentiate with respect to $\\sigma$:\n$$\n\\frac{\\partial}{\\partial \\sigma}\\ln f(x;\\sigma) = -\\frac{2}{\\sigma} + \\frac{x^{2}}{\\sigma^{3}}.\n$$\nThus,\n$$\n\\left(\\frac{\\partial}{\\partial \\sigma}\\ln f(X;\\sigma)\\right)^{2} = \\left(-\\frac{2}{\\sigma} + \\frac{X^{2}}{\\sigma^{3}}\\right)^{2} = \\frac{4}{\\sigma^{2}} - \\frac{4X^{2}}{\\sigma^{4}} + \\frac{X^{4}}{\\sigma^{6}}.\n$$\nWe need $\\mathbb{E}[X^{2}]$ and $\\mathbb{E}[X^{4}]$. Let $Y=X^{2}$. Then for $y>0$,\n$$\nf_{Y}(y;\\sigma) = f_{X}(\\sqrt{y};\\sigma)\\cdot \\frac{1}{2\\sqrt{y}} = \\frac{1}{2\\sigma^{2}}\\exp\\!\\left(-\\frac{y}{2\\sigma^{2}}\\right),\n$$\nso $Y$ is exponential with rate $\\lambda = \\frac{1}{2\\sigma^{2}}$. Hence $\\mathbb{E}[Y]=2\\sigma^{2}$ and $\\mathbb{E}[Y^{2}]=2(2\\sigma^{2})^{2}=8\\sigma^{4}$. Therefore,\n$$\n\\mathbb{E}[X^{2}] = 2\\sigma^{2}, \\quad \\mathbb{E}[X^{4}] = 8\\sigma^{4}.\n$$\nTaking expectations,\n$$\nI(\\sigma) = \\frac{4}{\\sigma^{2}} - \\frac{4\\,\\mathbb{E}[X^{2}]}{\\sigma^{4}} + \\frac{\\mathbb{E}[X^{4}]}{\\sigma^{6}} = \\frac{4}{\\sigma^{2}} - \\frac{4(2\\sigma^{2})}{\\sigma^{4}} + \\frac{8\\sigma^{4}}{\\sigma^{6}} = \\frac{4}{\\sigma^{2}}.\n$$\nAs a check, using $I(\\sigma) = -\\mathbb{E}\\!\\left[\\frac{\\partial^{2}}{\\partial \\sigma^{2}}\\ln f(X;\\sigma)\\right]$, we have\n$$\n\\frac{\\partial^{2}}{\\partial \\sigma^{2}}\\ln f(x;\\sigma) = \\frac{2}{\\sigma^{2}} - \\frac{3x^{2}}{\\sigma^{4}}, \\quad -\\mathbb{E}[\\cdot] = -\\frac{2}{\\sigma^{2}} + \\frac{3\\,\\mathbb{E}[X^{2}]}{\\sigma^{4}} = -\\frac{2}{\\sigma^{2}} + \\frac{6}{\\sigma^{2}} = \\frac{4}{\\sigma^{2}},\n$$\nwhich matches the result.", "answer": "$$\\boxed{\\frac{4}{\\sigma^{2}}}$$", "id": "1918268"}]}