{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we'll start with the most fundamental distribution in statistics: the Normal distribution. This exercise investigates whether it's possible to find a \"perfect\" estimator for the variance parameter, $\\theta$, one whose variance is as low as theoretically possible. By working through this problem [@problem_id:1896978], you will see a classic example of when the Cramér-Rao Lower Bound is attainable and discover that the efficient estimator is directly related to the sum of squared observations.", "problem": "In a simplified model for a particular type of astronomical observation, the measured signal strength $X$ is affected by a random, zero-mean fluctuation. A series of $n$ independent measurements $X_1, X_2, \\dots, X_n$ are taken. Each measurement $X_i$ is modeled by the relationship $X_i = \\sqrt{\\theta} Z_i$, where $\\theta  0$ is an unknown parameter representing the intrinsic power of the source, and $Z_i$ are independent random variables, each following a standard normal distribution, $N(0,1)$.\n\nThe Cramér-Rao Lower Bound (CRLB) provides a theoretical minimum for the variance of any unbiased estimator. The question is whether an unbiased estimator for the power parameter $\\theta$ can actually achieve this lower bound.\n\nWhich of the following statements is correct regarding the existence of an unbiased estimator for $\\theta$ whose variance attains the CRLB for all $\\theta  0$?\n\nA. Yes, such an estimator exists, and it is a function of $\\sum_{i=1}^n X_i$.\n\nB. Yes, such an estimator exists, and it is a function of $\\sum_{i=1}^n X_i^2$.\n\nC. No, such an estimator does not exist because the distribution of $X_i$ is not part of a one-parameter exponential family.\n\nD. No, such an estimator does not exist because the regularity conditions for the CRLB are not met for this distribution.", "solution": "We model $X_{i}=\\sqrt{\\theta}\\,Z_{i}$ with $Z_{i}\\overset{\\text{iid}}{\\sim}N(0,1)$, so $X_{i}\\overset{\\text{iid}}{\\sim}N(0,\\theta)$ for $\\theta0$. Let $S=\\sum_{i=1}^{n}X_{i}^{2}$.\n\nThe log-likelihood for $\\theta$ given data $x=(x_{1},\\dots,x_{n})$ is\n$$\n\\ell(\\theta;x)=-\\frac{n}{2}\\ln(2\\pi\\theta)-\\frac{1}{2\\theta}\\sum_{i=1}^{n}x_{i}^{2}.\n$$\nThe score function is\n$$\n\\frac{\\partial\\ell}{\\partial\\theta}=-\\frac{n}{2\\theta}+\\frac{1}{2\\theta^{2}}S,\n$$\nand the second derivative is\n$$\n\\frac{\\partial^{2}\\ell}{\\partial\\theta^{2}}=\\frac{n}{2\\theta^{2}}-\\frac{S}{\\theta^{3}}.\n$$\nUsing $E_{\\theta}[S]=n\\theta$, the Fisher information is\n$$\nI(\\theta)=-E_{\\theta}\\!\\left[\\frac{\\partial^{2}\\ell}{\\partial\\theta^{2}}\\right]=-\\left(\\frac{n}{2\\theta^{2}}-\\frac{n\\theta}{\\theta^{3}}\\right)=\\frac{n}{2\\theta^{2}}.\n$$\nFor any unbiased estimator $T$ of $\\theta$, the Cramér-Rao Lower Bound gives\n$$\n\\operatorname{Var}_{\\theta}(T)\\geq \\frac{1}{I(\\theta)}=\\frac{2\\theta^{2}}{n}.\n$$\n\nConsider $\\widehat{\\theta}=\\frac{1}{n}S=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}^{2}$. Since $E_{\\theta}[X_{i}^{2}]=\\theta$,\n$$\nE_{\\theta}[\\widehat{\\theta}]=\\frac{1}{n}E_{\\theta}[S]=\\frac{1}{n}\\cdot n\\theta=\\theta,\n$$\nso $\\widehat{\\theta}$ is unbiased. Also, $S/\\theta\\sim\\chi^{2}_{n}$, hence $\\operatorname{Var}(S)=\\theta^{2}\\operatorname{Var}(\\chi^{2}_{n})=2n\\theta^{2}$, and therefore\n$$\n\\operatorname{Var}_{\\theta}(\\widehat{\\theta})=\\operatorname{Var}\\!\\left(\\frac{S}{n}\\right)=\\frac{1}{n^{2}}\\operatorname{Var}(S)=\\frac{2\\theta^{2}}{n},\n$$\nwhich equals the CRLB for all $\\theta0$. Thus an unbiased estimator attaining the CRLB exists and is a function of $\\sum_{i=1}^{n}X_{i}^{2}$.\n\nFor completeness, note that an unbiased estimator based solely on $\\sum_{i=1}^{n}X_{i}$, for example $T'=\\frac{1}{n}\\left(\\sum_{i=1}^{n}X_{i}\\right)^{2}$ with $E_{\\theta}[T']=\\theta$, has variance $\\operatorname{Var}(T')=2\\theta^{2}$, which exceeds the CRLB when $n1$. Hence such a function of $\\sum X_{i}$ does not attain the bound (except when $n=1$).\n\nThe model belongs to a regular one-parameter exponential family with parameter-independent support and standard differentiability, so the regularity conditions for the CRLB are satisfied; statements asserting non-regularity or exclusion from the exponential family are incorrect.\n\nTherefore, the correct statement is that such an estimator exists and is a function of $\\sum_{i=1}^{n}X_{i}^{2}$.", "answer": "$$\\boxed{B}$$", "id": "1896978"}, {"introduction": "Just because an estimator is unbiased doesn't automatically make it efficient. This next practice serves as a crucial case study, demonstrating that our choice of estimator matters profoundly. We will analyze an intuitive estimator for the mean $\\theta$ of an exponential distribution, which is based on the smallest value in the sample, $X_{(1)}$ [@problem_id:1896985]. This exercise will reveal the surprising result that this estimator, while unbiased, is generally not efficient, highlighting the gap that can exist between an unbiased estimate and a truly optimal one.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be an independent and identically distributed random sample of size $n$ from an exponential distribution with the probability density function (PDF) given by:\n$$f(x; \\theta) = \\frac{1}{\\theta} \\exp\\left(-\\frac{x}{\\theta}\\right), \\quad \\text{for } x  0$$\nHere, the parameter $\\theta  0$ represents the mean of the distribution.\n\nConsider the following estimator for the parameter $\\theta$:\n$$\\tilde{\\theta} = n X_{(1)}$$\nwhere $X_{(1)} = \\min\\{X_1, X_2, \\ldots, X_n\\}$ is the first order statistic, i.e., the minimum value in the sample.\n\nRecall that an unbiased estimator is defined as an efficient estimator if its variance attains the Cramér-Rao Lower Bound (CRLB). Based on this definition and the properties of the exponential distribution, which of the following statements correctly describes the estimator $\\tilde{\\theta}$? Assume the sample size $n$ is a positive integer.\n\nA. The estimator $\\tilde{\\theta}$ is biased, and therefore cannot be an efficient estimator.\n\nB. The estimator $\\tilde{\\theta}$ is unbiased and efficient for all sample sizes $n \\ge 1$.\n\nC. The estimator $\\tilde{\\theta}$ is unbiased, but it is not efficient for any sample size $n \\ge 1$ because its variance is strictly greater than the Cramér-Rao Lower Bound.\n\nD. The estimator $\\tilde{\\theta}$ is unbiased, but it is only efficient for the specific case where the sample size is $n=1$.\n\nE. The analysis is invalid because the regularity conditions required for the Cramér-Rao Lower Bound are not satisfied by this estimation problem.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. with density $f(x;\\theta)=\\theta^{-1}\\exp(-x/\\theta)$ for $x0$, $\\theta0$. Define $\\tilde{\\theta}=nX_{(1)}$, where $X_{(1)}=\\min\\{X_{1},\\dots,X_{n}\\}$.\n\nFirst, determine the distribution of $X_{(1)}$. For $x0$,\n$$\n\\Pr(X_{(1)}x)=\\Pr(X_{1}x,\\dots,X_{n}x)=\\prod_{i=1}^{n}\\Pr(X_{i}x)=\\left[\\exp\\!\\left(-\\frac{x}{\\theta}\\right)\\right]^{n}=\\exp\\!\\left(-\\frac{nx}{\\theta}\\right).\n$$\nThus $X_{(1)}$ is exponential with rate $n/\\theta$, i.e., with mean $\\theta/n$ and variance $\\theta^{2}/n^{2}$. Therefore,\n$$\n\\mathbb{E}[\\tilde{\\theta}]=\\mathbb{E}[nX_{(1)}]=n\\cdot\\frac{\\theta}{n}=\\theta,\n$$\nso $\\tilde{\\theta}$ is unbiased for all $n\\ge 1$, and\n$$\n\\operatorname{Var}(\\tilde{\\theta})=\\operatorname{Var}(nX_{(1)})=n^{2}\\cdot\\frac{\\theta^{2}}{n^{2}}=\\theta^{2}.\n$$\n\nNext, compute the Cramér-Rao Lower Bound for unbiased estimators of $\\theta$. The log-likelihood for a single observation is\n$$\n\\ell(\\theta;x)=-\\ln\\theta-\\frac{x}{\\theta},\n$$\nso the score and its derivative are\n$$\n\\ell'(\\theta;x)=-\\frac{1}{\\theta}+\\frac{x}{\\theta^{2}},\\qquad \\ell''(\\theta;x)=\\frac{1}{\\theta^{2}}-\\frac{2x}{\\theta^{3}}.\n$$\nTaking expectation under $x\\sim f(x;\\theta)$ with $\\mathbb{E}[X]=\\theta$,\n$$\n\\mathbb{E}[\\ell''(\\theta;X)]=\\frac{1}{\\theta^{2}}-\\frac{2\\theta}{\\theta^{3}}=-\\frac{1}{\\theta^{2}},\n$$\nso the Fisher information for one observation is $I_{1}(\\theta)=-\\mathbb{E}[\\ell''(\\theta;X)]=\\theta^{-2}$, and for $n$ i.i.d. observations $I_{n}(\\theta)=n\\theta^{-2}$. Hence the CRLB is\n$$\n\\operatorname{Var}(\\hat{\\theta})\\ge \\frac{1}{I_{n}(\\theta)}=\\frac{\\theta^{2}}{n}.\n$$\n\nComparing with $\\operatorname{Var}(\\tilde{\\theta})=\\theta^{2}$, we have\n$$\n\\operatorname{Var}(\\tilde{\\theta})=\\theta^{2}\\frac{\\theta^{2}}{n}\\quad\\text{for }n1,\\qquad \\operatorname{Var}(\\tilde{\\theta})=\\frac{\\theta^{2}}{1}\\ \\text{for }n=1.\n$$\nThus $\\tilde{\\theta}$ is unbiased for all $n$, and it attains the CRLB only when $n=1$. The regularity conditions for the CRLB are satisfied here because the support does not depend on $\\theta$, and the required expectations exist and are finite.\n\nTherefore, the correct statement is that $\\tilde{\\theta}$ is unbiased, and it is only efficient when $n=1$.", "answer": "$$\\boxed{D}$$", "id": "1896985"}, {"introduction": "Let's apply our skills to a scenario from signal processing that involves the Rayleigh distribution. This problem introduces a common challenge in practice: the parameter of interest, $\\theta = 2\\sigma^2$, is a function of the distribution's standard parameter, $\\sigma$. This exercise [@problem_id:1896981] provides excellent practice in reparameterizing a probability density function and then proceeding with the standard analysis to check if a proposed estimator meets the benchmark for efficiency set by the Cramér-Rao Lower Bound.", "problem": "In statistical signal processing, the Rayleigh distribution is often used to model the envelope of a received signal. Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n$ from a Rayleigh distribution with the probability density function (PDF) given by:\n$$f(x; \\sigma) = \\frac{x}{\\sigma^2} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right), \\quad \\text{for } x \\ge 0 \\text{ and } \\sigma  0$$\nConsider the parameter $\\theta = 2\\sigma^2$. An engineer proposes the estimator $\\hat{\\theta} = \\frac{1}{n}\\sum_{i=1}^n X_i^2$ for $\\theta$. It can be shown that this estimator is unbiased, i.e., $E[\\hat{\\theta}] = \\theta$.\n\nWhich of the following statements correctly describes the efficiency of this estimator?\n\nA. The estimator is efficient because its variance is equal to the Cramér-Rao Lower Bound.\n\nB. The estimator is not efficient because its variance is greater than the Cramér-Rao Lower Bound.\n\nC. The estimator is not efficient because its variance is less than the Cramér-Rao Lower Bound.\n\nD. The efficiency of the estimator cannot be determined because the Rayleigh distribution does not belong to the one-parameter exponential family with respect to the parameter $\\theta$.\n\nE. The estimator is efficient, but its variance is not equal to the Cramér-Rao Lower Bound.", "solution": "We reparameterize the Rayleigh density in terms of $\\theta=2\\sigma^{2}$. Since $\\sigma^{2}=\\theta/2$, the density becomes\n$$\nf(x;\\theta)=\\frac{2x}{\\theta}\\exp\\left(-\\frac{x^{2}}{\\theta}\\right), \\quad x\\ge 0,\\ \\theta0.\n$$\nLet $Y=X^{2}$. Using the transformation $y=x^{2}$ with $x=\\sqrt{y}$ and $\\frac{dx}{dy}=\\frac{1}{2\\sqrt{y}}$, the density of $Y$ is\n$$\nf_{Y}(y;\\theta)=f_{X}(\\sqrt{y};\\theta)\\frac{1}{2\\sqrt{y}}=\\frac{2\\sqrt{y}}{\\theta}\\exp\\left(-\\frac{y}{\\theta}\\right)\\cdot\\frac{1}{2\\sqrt{y}}=\\frac{1}{\\theta}\\exp\\left(-\\frac{y}{\\theta}\\right),\\quad y\\ge 0.\n$$\nHence $Y\\sim\\text{Exp}$ with mean $\\theta$, so $E[Y]=\\theta$ and $\\operatorname{Var}(Y)=\\theta^{2}$. For the sample, $\\hat{\\theta}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}$, so\n$$\nE[\\hat{\\theta}]=\\theta,\\qquad \\operatorname{Var}(\\hat{\\theta})=\\frac{\\operatorname{Var}(Y)}{n}=\\frac{\\theta^{2}}{n}.\n$$\n\nTo find the Cramér-Rao Lower Bound for unbiased estimators of $\\theta$, compute the Fisher information. For one observation $Y$ with density $f_{Y}(y;\\theta)=(1/\\theta)\\exp(-y/\\theta)$, the log-likelihood is\n$$\n\\ell_{1}(\\theta)=-\\ln\\theta-\\frac{y}{\\theta}.\n$$\nThe score and its derivative are\n$$\nu_{1}(\\theta)=\\frac{d\\ell_{1}}{d\\theta}=-\\frac{1}{\\theta}+\\frac{y}{\\theta^{2}},\\qquad \\frac{du_{1}}{d\\theta}=\\frac{1}{\\theta^{2}}-\\frac{2y}{\\theta^{3}}.\n$$\nThus the Fisher information for one observation is\n$$\nI_{1}(\\theta)=-E\\!\\left[\\frac{du_{1}}{d\\theta}\\right]=- \\left(\\frac{1}{\\theta^{2}}-\\frac{2E[Y]}{\\theta^{3}}\\right)=-\\frac{1}{\\theta^{2}}+\\frac{2\\theta}{\\theta^{3}}=\\frac{1}{\\theta^{2}}.\n$$\nFor $n$ i.i.d. observations, $I_{n}(\\theta)=n/\\theta^{2}$, and the Cramér-Rao Lower Bound is\n$$\n\\operatorname{Var}(\\hat{\\theta})\\ge \\frac{1}{I_{n}(\\theta)}=\\frac{\\theta^{2}}{n}.\n$$\nSince $\\operatorname{Var}(\\hat{\\theta})=\\theta^{2}/n$ equals the bound, $\\hat{\\theta}$ is efficient. Regularity conditions hold because the support does not depend on $\\theta$ and the density is smooth in $\\theta$. Therefore, the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1896981"}]}