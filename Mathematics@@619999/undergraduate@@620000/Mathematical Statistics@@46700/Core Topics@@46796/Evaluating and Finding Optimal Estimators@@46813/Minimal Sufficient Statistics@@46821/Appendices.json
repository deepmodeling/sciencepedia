{"hands_on_practices": [{"introduction": "How can we boil down a series of experiments to a single number without losing any information about the underlying probability of success? This problem [@problem_id:1935626] provides a hands-on introduction to this concept of data reduction using the geometric distribution. By working through this foundational example, you will apply the Neyman-Fisher Factorization Theorem to find the simplest possible summary of the data—a minimal sufficient statistic for the success probability $p$.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n$ from a distribution representing the number of trials required to achieve the first success in a sequence of independent Bernoulli trials. These random variables are independent and identically distributed (i.i.d.). The probability of success in any single trial is $p$, where $0 < p < 1$. The probability mass function (PMF) for each $X_i$ is given by:\n$$f(x; p) = (1-p)^{x-1} p, \\quad \\text{for } x = 1, 2, 3, \\ldots$$\nFind a minimal sufficient statistic for the parameter $p$. Express your answer as a function of the sample observations $x_1, x_2, \\ldots, x_n$.", "solution": "To find a minimal sufficient statistic for the parameter $p$, we will first find a sufficient statistic using the Neyman-Fisher Factorization Theorem and then demonstrate its minimality.\n\nLet $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ represent a an observed outcome of the random sample $\\mathbf{X} = (X_1, X_2, \\ldots, X_n)$. The first step is to write down the joint probability mass function (PMF) of the sample, which is also known as the likelihood function $L(p; \\mathbf{x})$. Since the random variables $X_i$ are independent and identically distributed, the joint PMF is the product of the individual PMFs:\n$$L(p; \\mathbf{x}) = \\prod_{i=1}^{n} f(x_i; p) = \\prod_{i=1}^{n} \\left( (1-p)^{x_i-1} p \\right)$$\nWe can simplify this expression by grouping the terms involving $p$ and $(1-p)$:\n$$L(p; \\mathbf{x}) = \\left( \\prod_{i=1}^{n} (1-p)^{x_i-1} \\right) \\left( \\prod_{i=1}^{n} p \\right)$$\nUsing the properties of exponents and products, this becomes:\n$$L(p; \\mathbf{x}) = (1-p)^{\\sum_{i=1}^{n} (x_i-1)} p^n$$\n$$L(p; \\mathbf{x}) = (1-p)^{(\\sum_{i=1}^{n} x_i) - n} p^n$$\nThe Neyman-Fisher Factorization Theorem states that a statistic $T(\\mathbf{X})$ is sufficient for a parameter $\\theta$ if and only if the likelihood function can be factored into two non-negative functions, $g(T(\\mathbf{x}); \\theta)$ and $h(\\mathbf{x})$, such that $L(\\theta; \\mathbf{x}) = g(T(\\mathbf{x}); \\theta) \\cdot h(\\mathbf{x})$. Here, the function $g$ depends on the data $\\mathbf{x}$ only through the statistic $T(\\mathbf{x})$, and the function $h$ depends only on the data $\\mathbf{x}$ and not on the parameter $\\theta$.\n\nIn our case, we can identify these functions as:\n$$g(T(\\mathbf{x}); p) = (1-p)^{(\\sum_{i=1}^{n} x_i) - n} p^n$$\n$$h(\\mathbf{x}) = 1$$\nThe function $g$ depends on the data $\\mathbf{x}$ only through the sum $\\sum_{i=1}^{n} x_i$. Therefore, according to the Factorization Theorem, the statistic $T(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$ is a sufficient statistic for $p$.\n\nNext, we must verify if this statistic is minimal. A sufficient statistic $T(\\mathbf{X})$ is minimal if for any two sample points $\\mathbf{x}$ and $\\mathbf{y}$, the ratio of the likelihoods $L(p; \\mathbf{x}) / L(p; \\mathbf{y})$ is constant as a function of $p$ if and only if $T(\\mathbf{x}) = T(\\mathbf{y})$.\n\nLet's compute this ratio for two sample points $\\mathbf{x} = (x_1, \\ldots, x_n)$ and $\\mathbf{y} = (y_1, \\ldots, y_n)$:\n$$\\frac{L(p; \\mathbf{x})}{L(p; \\mathbf{y})} = \\frac{(1-p)^{(\\sum_{i=1}^{n} x_i) - n} p^n}{(1-p)^{(\\sum_{i=1}^{n} y_i) - n} p^n}$$\nThe $p^n$ terms cancel, and we can combine the terms with base $(1-p)$:\n$$\\frac{L(p; \\mathbf{x})}{L(p; \\mathbf{y})} = (1-p)^{(\\sum_{i=1}^{n} x_i) - n - ((\\sum_{i=1}^{n} y_i) - n)}$$\n$$\\frac{L(p; \\mathbf{x})}{L(p; \\mathbf{y})} = (1-p)^{(\\sum_{i=1}^{n} x_i) - (\\sum_{i=1}^{n} y_i)}$$\nFor this ratio to be a constant that does not depend on the parameter $p$, the exponent of $(1-p)$ must be zero. This is because if the exponent is non-zero, the value of the expression will change as $p$ varies in its domain $(0,1)$.\nThe exponent is zero if and only if:\n$$(\\sum_{i=1}^{n} x_i) - (\\sum_{i=1}^{n} y_i) = 0$$\nwhich is equivalent to:\n$$\\sum_{i=1}^{n} x_i = \\sum_{i=1}^{n} y_i$$\nThis is precisely the condition that $T(\\mathbf{x}) = T(\\mathbf{y})$. Since the likelihood ratio is independent of $p$ if and only if $T(\\mathbf{x}) = T(\\mathbf{y})$, we conclude that the statistic $T(\\mathbf{X}) = \\sum_{i=1}^{n} X_i$ is a minimal sufficient statistic for $p$.\n\nThe question asks for the statistic as a function of the sample observations, which is $T(\\mathbf{x}) = \\sum_{i=1}^{n} x_i$.", "answer": "$$\\boxed{\\sum_{i=1}^{n} x_{i}}$$", "id": "1935626"}, {"introduction": "Not all information about a parameter is contained in the average of the data; sometimes, the extremes tell the more complete story. This problem [@problem_id:1935627] explores a scenario where the unknown parameter $\\theta$ defines the very range of possible measurements. You will learn how to handle such cases by using the likelihood function to show that the sample minimum and maximum, taken together, form the minimal sufficient statistic.", "problem": "A quality control engineer is analyzing a manufacturing process that produces high-precision metal rods. The length of each rod is modeled as a random variable from a Uniform distribution on the interval $[\\theta, 2\\theta]$, where the parameter $\\theta > 0$ is an unknown characteristic of the machine's current calibration. To monitor and estimate $\\theta$, the engineer collects a random sample of $n$ rod lengths, denoted by $X_1, X_2, \\dots, X_n$.\n\nTo summarize the data efficiently without losing information about the parameter $\\theta$, the engineer needs to compute a minimal sufficient statistic. Let $X_{(1)}$ be the minimum value in the sample (the first order statistic) and $X_{(n)}$ be the maximum value in the sample (the last order statistic).\n\nWhich of the following represents a minimal sufficient statistic for the parameter $\\theta$?\n\nA. $X_{(n)}$\n\nB. $\\frac{X_{(1)}}{X_{(n)}}$\n\nC. $(X_{(1)}, X_{(n)})$\n\nD. $X_{(1)} + X_{(n)}$\n\nE. $X_{(n)} - X_{(1)}$\n\nF. $\\frac{1}{n} \\sum_{i=1}^{n} X_i$", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. with density\n$$\nf(x\\mid \\theta)=\\frac{1}{\\theta}\\,\\mathbf{1}\\{\\theta \\leq x \\leq 2\\theta\\},\\quad \\theta>0.\n$$\nFor a sample $x=(x_{1},\\dots,x_{n})$, the joint density is\n$$\nf(x\\mid \\theta)=\\prod_{i=1}^{n}\\frac{1}{\\theta}\\,\\mathbf{1}\\{\\theta \\leq x_{i} \\leq 2\\theta\\}\n=\\theta^{-n}\\,\\mathbf{1}\\{\\theta \\leq x_{(1)}\\}\\,\\mathbf{1}\\{2\\theta \\geq x_{(n)}\\}.\n$$\nEquivalently,\n$$\nf(x\\mid \\theta)=\\theta^{-n}\\,\\mathbf{1}\\left\\{\\frac{x_{(n)}}{2} \\leq \\theta \\leq x_{(1)}\\right\\}.\n$$\nThis shows the likelihood depends on the data only through $(x_{(1)},x_{(n)})$. Define $T(x)=(x_{(1)},x_{(n)})$. Then we can write\n$$\nf(x\\mid \\theta)=g(T(x),\\theta)\\,h(x),\n$$\nwith $g((a,b),\\theta)=\\theta^{-n}\\,\\mathbf{1}\\{b/2 \\leq \\theta \\leq a\\}$ and $h(x)=1$. By the Neyman–Fisher factorization theorem, $T=(X_{(1)},X_{(n)})$ is sufficient for $\\theta$.\n\nTo establish minimal sufficiency, use the Lehmann–Scheffé criterion: $T$ is minimal sufficient if for any two samples $x$ and $y$,\n$$\n\\frac{f(x\\mid \\theta)}{f(y\\mid \\theta)} \\text{ is free of } \\theta \\quad \\Longleftrightarrow \\quad T(x)=T(y).\n$$\nCompute\n$$\n\\frac{f(x\\mid \\theta)}{f(y\\mid \\theta)}=\\frac{\\theta^{-n}\\,\\mathbf{1}\\{x_{(n)}/2 \\leq \\theta \\leq x_{(1)}\\}}{\\theta^{-n}\\,\\mathbf{1}\\{y_{(n)}/2 \\leq \\theta \\leq y_{(1)}\\}}=\\frac{\\mathbf{1}\\{x_{(n)}/2 \\leq \\theta \\leq x_{(1)}\\}}{\\mathbf{1}\\{y_{(n)}/2 \\leq \\theta \\leq y_{(1)}\\}}.\n$$\nThis ratio is constant in $\\theta$ if and only if the indicator functions coincide for all $\\theta$ (up to sets of measure zero), which occurs if and only if the intervals $[x_{(n)}/2,x_{(1)}]$ and $[y_{(n)}/2,y_{(1)}]$ are identical. That is equivalent to $x_{(1)}=y_{(1)}$ and $x_{(n)}=y_{(n)}$, i.e., $T(x)=T(y)$. Hence $T=(X_{(1)},X_{(n)})$ is minimal sufficient.\n\nAmong the given options, only option C includes both $X_{(1)}$ and $X_{(n)}$, so the minimal sufficient statistic is $(X_{(1)},X_{(n)})$.", "answer": "$$\\boxed{C}$$", "id": "1935627"}, {"introduction": "Moving from abstract theory to real-world application, this problem [@problem_id:1935612] situates the concept of minimal sufficiency within the context of population genetics and the Hardy-Weinberg equilibrium. You will work with data from multiple categories (genotypes) to derive a single value that captures all the relevant information about the underlying allele frequency $\\theta$. This practice demonstrates how a cleverly constructed statistic can provide a complete and physically meaningful summary of a complex dataset.", "problem": "In population genetics, a fundamental concept is the Hardy-Weinberg equilibrium, which describes the genetic makeup of a population that is not evolving. For a gene with two alleles, A and a, let the frequency of allele A in the population be $\\theta$ and the frequency of allele a be $1-\\theta$, where the parameter $\\theta$ lies in the interval $(0, 1)$. Under the Hardy-Weinberg principle, the expected frequencies of the three possible genotypes—AA, Aa, and aa—are $\\theta^2$, $2\\theta(1-\\theta)$, and $(1-\\theta)^2$, respectively.\n\nA geneticist collects a random sample of $n$ individuals from this large population to estimate the allele frequency $\\theta$. Let $N_1$ be the number of individuals in the sample with genotype AA, $N_2$ be the number with genotype Aa, and $N_3$ be the number with genotype aa. The total sample size is $n = N_1 + N_2 + N_3$. The vector of observed counts $(N_1, N_2, N_3)$ can be modeled as a single draw from a multinomial distribution.\n\nBased on the observed counts $(N_1, N_2, N_3)$, determine a minimal sufficient statistic for the parameter $\\theta$. Your answer should be an expression in terms of $N_1$, $N_2$, and $N_3$.", "solution": "Let the genotype probabilities under Hardy–Weinberg equilibrium be $p_{1}=\\theta^{2}$ for AA, $p_{2}=2\\theta(1-\\theta)$ for Aa, and $p_{3}=(1-\\theta)^{2}$ for aa. For observed counts $(N_{1},N_{2},N_{3})$ with fixed total $n=N_{1}+N_{2}+N_{3}$, the joint pmf under a multinomial model is\n$$\nf_{\\theta}(N_{1},N_{2},N_{3})=\\frac{n!}{N_{1}!N_{2}!N_{3}!}\\left(\\theta^{2}\\right)^{N_{1}}\\left(2\\theta(1-\\theta)\\right)^{N_{2}}\\left((1-\\theta)^{2}\\right)^{N_{3}}.\n$$\nThis simplifies to\n$$\nf_{\\theta}(N_{1},N_{2},N_{3})=\\frac{n!}{N_{1}!N_{2}!N_{3}!}\\,2^{N_{2}}\\,\\theta^{2N_{1}+N_{2}}(1-\\theta)^{2N_{3}+N_{2}}.\n$$\nBy the Neyman–Fisher factorization theorem, with $n$ fixed and known, we can write\n$$\nf_{\\theta}(N_{1},N_{2},N_{3})=\\underbrace{\\theta^{T}(1-\\theta)^{2n-T}}_{g_{\\theta}(T)}\\;\\underbrace{\\frac{n!}{N_{1}!N_{2}!N_{3}!}\\,2^{N_{2}}}_{h(N_{1},N_{2},N_{3})},\n$$\nwhere $T=2N_{1}+N_{2}$. Hence $T=2N_{1}+N_{2}$ is a sufficient statistic for $\\theta$.\n\nTo show minimal sufficiency, use the Lehmann–Scheffé–Bahadur criterion: for two outcomes $(n_{1},n_{2},n_{3})$ and $(m_{1},m_{2},m_{3})$ with the same $n$, consider\n$$\n\\frac{f_{\\theta}(n_{1},n_{2},n_{3})}{f_{\\theta}(m_{1},m_{2},m_{3})}\n=\\frac{\\frac{n!}{n_{1}!n_{2}!n_{3}!}2^{n_{2}}\\theta^{2n_{1}+n_{2}}(1-\\theta)^{2n_{3}+n_{2}}}{\\frac{n!}{m_{1}!m_{2}!m_{3}!}2^{m_{2}}\\theta^{2m_{1}+m_{2}}(1-\\theta)^{2m_{3}+m_{2}}}\n=C(n,m)\\left(\\frac{\\theta}{1-\\theta}\\right)^{(2n_{1}+n_{2})-(2m_{1}+m_{2})},\n$$\nwhere $C(n,m)$ is free of $\\theta$. This ratio is independent of $\\theta$ if and only if $2n_{1}+n_{2}=2m_{1}+m_{2}$. Therefore, the equivalence classes induced by the family are exactly those with the same value of $T=2N_{1}+N_{2}$, proving $T$ is minimal sufficient.\n\nThus, a minimal sufficient statistic for $\\theta$ is the total number of $A$ alleles in the sample, namely $2N_{1}+N_{2}$.", "answer": "$$\\boxed{2N_{1}+N_{2}}$$", "id": "1935612"}]}