{"hands_on_practices": [{"introduction": "The principle of sufficiency allows us to distill a large dataset into a single number or a set of numbers without losing any information about the parameter of interest. This first practice provides a foundational exercise in applying the Neyman-Fisher Factorization Theorem, a powerful tool for identifying sufficient statistics. By working with the geometric distribution, a common model for 'time-to-first-success' scenarios, you will see how to algebraically separate the joint probability function to isolate the statistic that captures all information about the success probability $p$. [@problem_id:1957623]", "problem": "In a wireless communication system, a data packet is sent from a transmitter to a receiver. Due to noise in the channel, a sent packet may be corrupted. The system employs an Automatic Repeat reQuest (ARQ) protocol, where the transmitter repeatedly sends the packet until it receives a successful acknowledgment. Assume that each transmission attempt is an independent trial with a constant probability of success $p$, where $0 < p < 1$.\n\nLet a random variable $X$ represent the number of failed transmission attempts before the first successful transmission. Thus, the probability mass function of $X$ is given by $P(X=k) = (1-p)^k p$ for $k = 0, 1, 2, \\dots$.\n\nAn engineer collects data from $n$ independent successful packet transmissions to estimate the channel's success probability $p$. Let $X_1, X_2, \\dots, X_n$ be a random sample, where each $X_i$ represents the number of failures for the $i$-th successful transmission.\n\nWhich of the following statistics is a sufficient statistic for the parameter $p$?\n\nA. $\\frac{1}{n} \\sum_{i=1}^n X_i^2$\n\nB. $\\prod_{i=1}^n X_i$\n\nC. $\\sum_{i=1}^n X_i$\n\nD. $\\max(X_1, X_2, \\dots, X_n)$\n\nE. $X_1$", "solution": "Each $X_{i}$ is geometric with support $\\{0,1,2,\\dots\\}$ and pmf $f_{X}(x_{i};p)=p(1-p)^{x_{i}}$, where $0<p<1$. For a random sample $X_{1},\\dots,X_{n}$, the joint pmf is\n$$\nf(x_{1},\\dots,x_{n};p)=\\prod_{i=1}^{n}p(1-p)^{x_{i}}=p^{n}(1-p)^{\\sum_{i=1}^{n}x_{i}}.\n$$\nBy the Neyman–Fisher factorization theorem, a statistic $T(X_{1},\\dots,X_{n})$ is sufficient for $p$ if the joint pmf can be written as\n$$\nf(x_{1},\\dots,x_{n};p)=g\\big(T(x_{1},\\dots,x_{n});p\\big)\\,h(x_{1},\\dots,x_{n}),\n$$\nwhere $g$ depends on the data only through $T$ and $h$ does not depend on $p$. From the displayed factorization,\n$$\nf(x_{1},\\dots,x_{n};p)=\\underbrace{p^{n}(1-p)^{\\sum_{i=1}^{n}x_{i}}}_{g\\big(T;p\\big)}\\cdot\\underbrace{1}_{h(x)},\n$$\nwith $T=\\sum_{i=1}^{n}X_{i}$. Therefore, $T=\\sum_{i=1}^{n}X_{i}$ is sufficient for $p$.\n\nAmong the given options, only option C equals $\\sum_{i=1}^{n}X_{i}$. The other options are not functions of $T$ and are not sufficient for this one-parameter geometric family, whose minimal sufficient statistic is $\\sum_{i=1}^{n}X_{i}$.", "answer": "$$\\boxed{C}$$", "id": "1957623"}, {"introduction": "Moving beyond single-parameter models, this exercise explores joint sufficiency for distributions where the parameters define the data's boundaries. For a uniform distribution over an unknown interval $[\\alpha, \\beta]$, the information about these endpoints is not effectively captured by statistics like the sample mean or variance, but rather by the sample's extremes. This practice demonstrates how the factorization theorem elegantly identifies the pair of order statistics, the minimum and maximum observed values, as the joint sufficient statistic for the parameter pair $(\\alpha, \\beta)$. [@problem_id:1963665]", "problem": "In the field of digital signal processing, an analog-to-digital converter is sampling a continuous voltage signal. The signal's voltage level is known to be uniformly distributed over an unknown range $[\\alpha, \\beta]$, where $\\alpha$ and $\\beta$ are the minimum and maximum possible voltages, respectively ($\\alpha < \\beta$). To calibrate the system, a set of $n$ independent voltage measurements, denoted by $X_1, X_2, \\ldots, X_n$, are taken.\n\nA central goal in statistical inference is to find a *sufficient statistic*, which is a function of the data that captures all the relevant information about the unknown parameters. In this case, we are interested in a joint sufficient statistic for the parameter pair $(\\alpha, \\beta)$.\n\nLet $X_{(1)} = \\min(X_1, X_2, \\ldots, X_n)$ be the minimum observed voltage and $X_{(n)} = \\max(X_1, X_2, \\ldots, X_n)$ be the maximum observed voltage in the sample. Let $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ be the sample mean and $S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$ be the sample variance.\n\nWhich of the following represents a joint sufficient statistic for the parameters $(\\alpha, \\beta)$?\n\nA. $(X_{(1)}, X_{(n)})$\n\nB. $(\\bar{X}, S^2)$\n\nC. $\\left(\\sum_{i=1}^n X_i, \\sum_{i=1}^n X_i^2\\right)$\n\nD. $X_{(n)}$\n\nE. The sample median", "solution": "Assume $X_{1},\\ldots,X_{n}$ are independent and identically distributed from the $\\text{Uniform}[\\alpha,\\beta]$ model, with density\n$$\nf(x\\mid \\alpha,\\beta)=\\frac{1}{\\beta-\\alpha}\\,\\mathbf{1}_{[\\alpha,\\beta]}(x),\n$$\nwhere $\\mathbf{1}_{[\\alpha,\\beta]}(x)$ is the indicator of $x\\in[\\alpha,\\beta]$.\n\nBy independence, the joint density (likelihood as a function of $(\\alpha,\\beta)$ given data $\\boldsymbol{x}=(x_{1},\\ldots,x_{n})$) is\n$$\nL(\\alpha,\\beta;\\boldsymbol{x})=\\prod_{i=1}^{n} f(x_{i}\\mid \\alpha,\\beta)\n=\\left(\\frac{1}{\\beta-\\alpha}\\right)^{n}\\prod_{i=1}^{n}\\mathbf{1}_{[\\alpha,\\beta]}(x_{i}).\n$$\nThe product of indicators equals the indicator that all observations lie in $[\\alpha,\\beta]$, which is equivalent to requiring $\\alpha\\le x_{(1)}$ and $x_{(n)}\\le \\beta$, where $x_{(1)}=\\min_{i}x_{i}$ and $x_{(n)}=\\max_{i}x_{i}$. Hence\n$$\nL(\\alpha,\\beta;\\boldsymbol{x})=\\left(\\frac{1}{\\beta-\\alpha}\\right)^{n}\\mathbf{1}_{\\{\\alpha\\le x_{(1)}\\}}\\mathbf{1}_{\\{x_{(n)}\\le \\beta\\}}\n=\\left(\\frac{1}{\\beta-\\alpha}\\right)^{n}\\mathbf{1}_{\\{\\alpha\\le x_{(1)}\\le x_{(n)}\\le \\beta\\}}.\n$$\nThis factors as\n$$\nL(\\alpha,\\beta;\\boldsymbol{x})=h(\\boldsymbol{x})\\,q_{\\alpha,\\beta}\\!\\left(x_{(1)},x_{(n)}\\right),\n$$\nwith $h(\\boldsymbol{x})=1$ and $q_{\\alpha,\\beta}(t_{1},t_{2})=(\\beta-\\alpha)^{-n}\\mathbf{1}_{\\{\\alpha\\le t_{1}\\le t_{2}\\le \\beta\\}}$. By the Fisher–Neyman factorization theorem, $T(\\boldsymbol{X})=(X_{(1)},X_{(n)})$ is sufficient for $(\\alpha,\\beta)$.\n\nTherefore, option A is a joint sufficient statistic. The other options are not sufficient: the likelihood depends on the data only through $(X_{(1)},X_{(n)})$, so any sufficient statistic must determine these extremes. Neither $(\\bar{X},S^{2})$ nor $\\left(\\sum_{i=1}^{n}X_{i},\\sum_{i=1}^{n}X_{i}^{2}\\right)$ nor $X_{(n)}$ alone nor the sample median determines both $X_{(1)}$ and $X_{(n)}$ in general, so they are not sufficient.", "answer": "$$\\boxed{A}$$", "id": "1963665"}, {"introduction": "What happens when we use a statistic that isn't sufficient? This thought experiment provides a clear and intuitive answer by exploring the practical consequences of information loss. By comparing the inferences that can be made from a full dataset versus only its sum for a uniform distribution, you will understand why the sample sum is an insufficient statistic for the parameter $\\theta$. This exercise reinforces the core idea that a sufficient statistic—in this specific case, the maximum observation—retains crucial boundary information that other summary statistics discard. [@problem_id:1963654]", "problem": "A random sample of size $n=3$, denoted by $X_1, X_2, X_3$, is drawn from a Uniform distribution on the interval $(0, \\theta)$, where the parameter $\\theta > 0$ is unknown. The physical constraints of the measurement process ensure that all observations $x_i$ must be positive.\n\nTwo data analysts, Alice and Bob, are tasked with placing a lower bound on the unknown parameter $\\theta$.\n\nAlice is given the complete, specific dataset of observations: $\\{2, 5, 8\\}$.\n\nBob is not given the full dataset. He is only told that the sum of the three observations is 15, i.e., $x_1 + x_2 + x_3 = 15$. Bob is aware that the specific dataset given to Alice is one possibility that results in this sum, but he must consider all possible sets of three positive observations that sum to 15.\n\nBoth analysts know that for any sample to be validly drawn from a $U(0, \\theta)$ distribution, every individual observation $x_i$ must satisfy $x_i < \\theta$. This implies that $\\theta$ must be strictly greater than the maximum observation in the sample, $\\theta > \\max(x_1, x_2, x_3)$.\n\nBob's goal is to find the greatest possible lower bound for $\\theta$, let's call it $\\theta_{min}$, such that the statement \"$\\theta > \\theta_{min}$\" is guaranteed to be true for *any* possible set of three positive observations that sum to 15.\n\nWhat is the value of this guaranteed lower bound $\\theta_{min}$?", "solution": "For any sample from a Uniform distribution on $(0,\\theta)$, validity requires $\\theta>\\max(x_{1},x_{2},x_{3})$. Bob knows only that $x_{1},x_{2},x_{3}>0$ and $x_{1}+x_{2}+x_{3}=15$. He seeks the largest $\\theta_{\\min}$ such that $\\theta>\\theta_{\\min}$ holds for every triple satisfying these constraints. This is equivalent to\n$$\n\\theta_{\\min}=\\inf\\left\\{\\max(x_{1},x_{2},x_{3}): x_{i}>0,\\; x_{1}+x_{2}+x_{3}=15\\right\\}.\n$$\n\nLet $m=\\max(x_{1},x_{2},x_{3})$. Since each $x_{i}\\leq m$, we have\n$$\nx_{1}+x_{2}+x_{3}\\leq m+m+m=3m.\n$$\nWith $x_{1}+x_{2}+x_{3}=15$, it follows that\n$$\n3m\\geq 15 \\quad\\Rightarrow\\quad m\\geq \\frac{15}{3}=5.\n$$\nHence for any admissible triple, $\\max(x_{1},x_{2},x_{3})\\geq 5$, so the infimum is at least $5$. The bound is attained by the admissible triple $(5,5,5)$, for which $\\max(x_{1},x_{2},x_{3})=5$. Therefore\n$$\n\\inf\\left\\{\\max(x_{1},x_{2},x_{3})\\right\\}=5.\n$$\n\nBecause $\\theta>\\max(x_{1},x_{2},x_{3})$ for every admissible triple and $\\max(x_{1},x_{2},x_{3})\\geq 5$, it follows that $\\theta>5$ is guaranteed. The greatest possible lower bound is thus $5$.", "answer": "$$\\boxed{5}$$", "id": "1963654"}]}