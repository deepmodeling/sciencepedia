{"hands_on_practices": [{"introduction": "Understanding ancillary statistics often begins with location parameters. This exercise [@problem_id:1895670] explores a scenario where an unknown systematic bias, a location parameter $\\theta$, shifts all measurements. We will investigate the sample range, $R = X_{(n)} - X_{(1)}$, and see that its probability distribution is independent of this shift, which makes the range a classic example of an ancillary statistic. This principle is fundamental for separating information about the parameter of interest from other 'nuisance' aspects of the data.", "problem": "A manufacturer of environmental sensors is testing a new device that measures atmospheric particle concentration. The device outputs an integer value. Due to a fixed but unknown systematic calibration error, the true concentration is shifted by an unknown integer offset, $\\theta$. For any given measurement, the sensor's output, $X$, is a discrete random variable uniformly distributed on the set of consecutive integers $S = \\{\\theta+1, \\theta+2, \\dots, \\theta+10\\}$. While the value of $\\theta$ is unknown, quality control has established that the sensor will always output one of these 10 specific integer values for a stable environment.\n\nAn engineer takes a random sample of three independent measurements, $X_1, X_2, X_3$, under stable environmental conditions. Let $M$ be the maximum value observed in the sample, and let $m$ be the minimum value observed. The engineer is interested in the sample range, defined as $R = M-m$.\n\nCalculate the expected value of this sample range, $E[R]$. Express your answer as an exact fraction in simplest form.", "solution": "Let $X_{1},X_{2},X_{3}$ be i.i.d. uniform on $S=\\{\\theta+1,\\dots,\\theta+10\\}$. Since adding a constant does not change a range, define $Y_{i}=X_{i}-\\theta$, which are i.i.d. uniform on $\\{1,\\dots,10\\}$. Then $R=M-m$ is unchanged, where $M=\\max(Y_{1},Y_{2},Y_{3})$ and $m=\\min(Y_{1},Y_{2},Y_{3})$.\n\nWe use linearity: $E[R]=E[M]-E[m]$. For i.i.d. discrete samples, by independence,\n$$\nP(M\\leq k)=\\left(\\frac{k}{10}\\right)^{3}\\quad\\text{for }k\\in\\{1,\\dots,10\\},\n$$\nso by the tail-sum formula for expectations of integer-valued variables,\n$$\nE[M]=\\sum_{k=1}^{10}P(M\\geq k)=\\sum_{k=1}^{10}\\left[1-\\left(\\frac{k-1}{10}\\right)^{3}\\right]\n=10-\\frac{1}{10^{3}}\\sum_{k=0}^{9}k^{3}.\n$$\nSimilarly,\n$$\nP(m\\geq k)=\\left(\\frac{10-k+1}{10}\\right)^{3},\\quad\nE[m]=\\sum_{k=1}^{10}P(m\\geq k)=\\frac{1}{10^{3}}\\sum_{j=1}^{10}j^{3},\n$$\nwhere we substituted $j=10-k+1$.\n\nUsing the identity $\\sum_{j=1}^{m}j^{3}=\\left(\\frac{m(m+1)}{2}\\right)^{2}$, we have\n$$\n\\sum_{j=1}^{10}j^{3}=\\left(\\frac{10\\cdot 11}{2}\\right)^{2}=55^{2}=3025,\\quad\n\\sum_{k=0}^{9}k^{3}=\\sum_{j=1}^{9}j^{3}=\\left(\\frac{9\\cdot 10}{2}\\right)^{2}=45^{2}=2025.\n$$\nTherefore,\n$$\nE[m]=\\frac{3025}{1000}=\\frac{121}{40},\\qquad\nE[M]=10-\\frac{2025}{1000}=10-\\frac{81}{40}=\\frac{319}{40}.\n$$\nHence,\n$$\nE[R]=E[M]-E[m]=\\frac{319}{40}-\\frac{121}{40}=\\frac{198}{40}=\\frac{99}{20}.\n$$", "answer": "$$\\boxed{\\frac{99}{20}}$$", "id": "1895670"}, {"introduction": "Moving from location to scale, this next practice examines a situation where the parameter $\\theta$ defines the 'size' or 'scale' of the observation space. By analyzing a point chosen uniformly from a circular sector, we can intuitively grasp why some measurements, like the angle, are unaffected by the sector's radius [@problem_id:1895664]. This problem helps solidify the concept of ancillarity within a geometric context, showing that not all observable quantities contain information about a scale parameter.", "problem": "A new type of short-range sensor is being tested. The sensor is designed to scan a fixed angular sector of width $\\alpha$ (in radians), where $0 < \\alpha \\le 2\\pi$ is a known constant. Due to manufacturing variations, the maximum effective range of any given sensor unit, denoted by the parameter $\\theta > 0$, is a positive but unknown constant. It is known from the sensor's design that when it detects a single object, the object's location is a random point drawn from a uniform distribution over the entire circular sector that the sensor can scan.\n\nSuppose a single object is detected. Its position is recorded in polar coordinates $(R, \\Phi)$, where $R$ is the radial distance from the sensor and $\\Phi$ is the angle relative to the start of the sector's sweep, with $0 \\le R \\le \\theta$ and $0 \\le \\Phi \\le \\alpha$. For this statistical model, the parameter of interest is the unknown maximum range $\\theta$. A statistic is a function of the observed data $(R, \\Phi)$ that does not itself depend on any unknown parameters. A statistic is said to be ancillary for the parameter $\\theta$ if its probability distribution is entirely independent of $\\theta$.\n\nWhich of the following statistics is ancillary for the parameter $\\theta$?\n\nA. $R$ (The radial coordinate)\n\nB. $\\Phi$ (The angular coordinate)\n\nC. $R^2$ (The squared radial coordinate)\n\nD. $R+\\Phi$\n\nE. $R\\Phi$", "solution": "The object is uniformly distributed over the circular sector with angle width $\\alpha$ and radius $\\theta$. The area of this sector is\n$$\n\\text{Area}=\\frac{1}{2}\\alpha \\theta^{2}.\n$$\nA uniform distribution over this region in Cartesian coordinates has constant density with respect to area measure equal to\n$$\nc=\\frac{1}{\\text{Area}}=\\frac{2}{\\alpha \\theta^{2}}.\n$$\nIn polar coordinates, the Jacobian of the transformation is $r$, so the joint density of $(R,\\Phi)$ with respect to $dr\\,d\\phi$ is\n$$\nf_{R,\\Phi}(r,\\phi\\mid \\theta)=c\\,r=\\frac{2r}{\\alpha \\theta^{2}}, \\quad 0\\le r\\le \\theta,\\; 0\\le \\phi\\le \\alpha.\n$$\nTo test ancillarity, compute the marginal distributions and check dependence on $\\theta$.\n\nFor $\\Phi$, integrate out $r$:\n$$\nf_{\\Phi}(\\phi\\mid \\theta)=\\int_{0}^{\\theta} \\frac{2r}{\\alpha \\theta^{2}}\\,dr=\\frac{2}{\\alpha \\theta^{2}}\\cdot \\frac{\\theta^{2}}{2}=\\frac{1}{\\alpha}, \\quad 0\\le \\phi\\le \\alpha.\n$$\nThis is uniform on $[0,\\alpha]$ and does not depend on $\\theta$, so $\\Phi$ is ancillary.\n\nFor $R$, integrate out $\\phi$:\n$$\nf_{R}(r\\mid \\theta)=\\int_{0}^{\\alpha} \\frac{2r}{\\alpha \\theta^{2}}\\,d\\phi=\\frac{2r}{\\theta^{2}}, \\quad 0\\le r\\le \\theta,\n$$\nwhich depends on $\\theta$ through both its form and support, so $R$ is not ancillary.\n\nFor $R^{2}$, let $T=R^{2}$ with $0\\le T\\le \\theta^{2}$. Using the change of variables $r=\\sqrt{t}$ and $\\frac{dr}{dt}=\\frac{1}{2\\sqrt{t}}$, the density is\n$$\nf_{T}(t\\mid \\theta)=f_{R}(\\sqrt{t}\\mid \\theta)\\cdot \\frac{1}{2\\sqrt{t}}=\\frac{2\\sqrt{t}}{\\theta^{2}}\\cdot \\frac{1}{2\\sqrt{t}}=\\frac{1}{\\theta^{2}}, \\quad 0\\le t\\le \\theta^{2}.\n$$\nAlthough constant on its support, it depends on $\\theta$ via the support $[0,\\theta^{2}]$, hence $R^{2}$ is not ancillary.\n\nFor $R+\\Phi$ and $R\\Phi$, since $\\Phi$ is independent of $R$ and $R$ scales with $\\theta$ (specifically, $R/\\theta$ has a fixed distribution on $[0,1]$), any statistic involving $R$ in a non-scale-invariant way will have a distribution that changes with $\\theta$. Therefore both $R+\\Phi$ and $R\\Phi$ are not ancillary.\n\nConsequently, among the given options, only $\\Phi$ is ancillary for $\\theta$.", "answer": "$$\\boxed{B}$$", "id": "1895664"}, {"introduction": "Ancillary statistics can also arise in more intricate models that go beyond simple location or scale families. This challenging problem [@problem_id:1895641] presents a hypothetical model where the parameter of interest, $\\theta$, governs both a Poisson process's rate and, in a specific inverse relationship, the duration of observation. You will discover the somewhat surprising result that the total number of observed events is an ancillary statistic, a powerful illustration of how a statistic's distribution can be rendered independent of a parameter due to the model's unique structure.", "problem": "A research team is characterizing a new type of Single-Photon Avalanche Diode (SPAD). The SPAD is illuminated by a stable, low-intensity light source, causing photon arrivals to be well-modeled by a Poisson process with a constant, but unknown, average rate of $\\theta$ events per second.\n\nThe SPAD operates until it experiences a catastrophic breakdown. The time to breakdown, denoted by the random variable $T$ in seconds, is found to be accurately described by an exponential distribution. A peculiar aspect of this SPAD's failure mechanism is that its mean time to breakdown is inversely proportional to the incident photon rate. Based on the underlying physics of the thermal stress induced by photon absorption, the relationship is determined to be exactly $E[T] = 1/\\theta$.\n\nAn experiment consists of running a single SPAD until it fails and recording the total number of photons it detected. Let this total count be the random variable $N$. Your task is to calculate the variance of $N$.", "solution": "Let photon arrivals be a Poisson process with rate $\\theta$, and let the breakdown time $T$ be independent and exponentially distributed with mean $E[T]=1/\\theta$, thus $T \\sim \\text{Exp}(\\theta)$ with density $f_{T}(t)=\\theta \\exp(-\\theta t)$ for $t \\geq 0$. Conditional on $T=t$, the total count $N$ is Poisson with parameter $\\theta t$, so\n$$\n\\Pr(N=n \\mid T=t)=\\frac{\\exp(-\\theta t)(\\theta t)^{n}}{n!}, \\quad n=0,1,2,\\dots\n$$\nand\n$$\nE[N \\mid T=t]=\\theta t, \\quad \\operatorname{Var}(N \\mid T=t)=\\theta t.\n$$\nUsing the law of total expectation,\n$$\nE[N]=E\\!\\left[E[N \\mid T]\\right]=E[\\theta T]=\\theta E[T]=\\theta \\cdot \\frac{1}{\\theta}=1.\n$$\nUsing the law of total variance,\n$$\n\\operatorname{Var}(N)=E\\!\\left[\\operatorname{Var}(N \\mid T)\\right]+\\operatorname{Var}\\!\\left(E[N \\mid T]\\right)\n=E[\\theta T]+\\operatorname{Var}(\\theta T).\n$$\nSince $T \\sim \\text{Exp}(\\theta)$, we have $E[T]=\\frac{1}{\\theta}$ and $\\operatorname{Var}(T)=\\frac{1}{\\theta^{2}}$. Therefore,\n$$\nE[\\theta T]=\\theta \\cdot \\frac{1}{\\theta}=1, \\quad \\operatorname{Var}(\\theta T)=\\theta^{2} \\cdot \\frac{1}{\\theta^{2}}=1,\n$$\nwhich gives\n$$\n\\operatorname{Var}(N)=1+1=2.\n$$\nAs a consistency check, the marginal distribution of $N$ can be computed as\n$$\n\\Pr(N=n)=\\int_{0}^{\\infty}\\Pr(N=n \\mid T=t)f_{T}(t)\\,dt\n=\\int_{0}^{\\infty}\\frac{\\exp(-\\theta t)(\\theta t)^{n}}{n!}\\,\\theta \\exp(-\\theta t)\\,dt\n=\\frac{1}{2}\\left(\\frac{1}{2}\\right)^{n},\n$$\nwhich is geometric on $\\{0,1,2,\\dots\\}$ with parameter $\\frac{1}{2}$ and thus variance $\\frac{1/2}{(1/2)^{2}}=2$, agreeing with the result above.", "answer": "$$\\boxed{2}$$", "id": "1895641"}]}