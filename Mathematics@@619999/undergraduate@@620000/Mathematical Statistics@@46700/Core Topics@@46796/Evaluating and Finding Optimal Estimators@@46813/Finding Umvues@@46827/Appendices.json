{"hands_on_practices": [{"introduction": "This first practice serves as a foundational exercise in finding a Uniformly Minimum-Variance Unbiased Estimator (UMVUE). We will tackle a common scenario in quality control involving the Binomial distribution, where the goal is to estimate a quadratic function of the success probability, $p^2$. This problem [@problem_id:1917710] illustrates the standard application of the Lehmann–Scheffé theorem and introduces the useful technique of factorial moments for constructing an unbiased estimator for polynomial functions of a parameter.", "problem": "A quality control engineer is inspecting large batches of manufactured components. A random sample of size $n$ is drawn from a batch, where $n$ is a known integer constant such that $n \\geq 2$. The number of defective components in the sample, denoted by the random variable $X$, is assumed to follow a Binomial distribution, $X \\sim \\text{Binomial}(n, p)$, where $p$ is the unknown proportion of defective components in the entire batch, with $0 < p < 1$.\n\nA cost analysis model for future warranty claims suggests that the long-term financial risk associated with a production process is proportional to the parameter $\\theta = p^2$. To estimate this risk, the engineer needs an efficient estimator for $\\theta$.\n\nFind the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for $\\theta = p^2$. Express your answer as a function of the observed number of defects, $X$, and the sample size, $n$.", "solution": "We have $X \\sim \\text{Binomial}(n,p)$ with $n \\geq 2$ and the target parameter is $\\theta = p^{2}$. The statistic $X$ is sufficient for $p$ by the factorization theorem, since the joint pmf of the $n$ Bernoulli trials depends on the sample only through $X = \\sum_{i=1}^{n} Y_{i}$. Moreover, for fixed $n$, the binomial family $\\{\\text{Binomial}(n,p): 0<p<1\\}$ is complete with respect to $X$, because if $\\sum_{x=0}^{n} g(x) \\binom{n}{x} p^{x} (1-p)^{n-x} = 0$ for all $p \\in (0,1)$, then the resulting polynomial in $p$ is identically zero, implying $g(x)=0$ for all $x$.\n\nTo find an unbiased estimator of $p^{2}$ as a function of $X$, write $X = \\sum_{i=1}^{n} Y_{i}$ with $Y_{i} \\sim \\text{Bernoulli}(p)$ independent. Consider the second factorial moment:\n$$\nX(X-1) = \\sum_{i \\neq j} Y_{i} Y_{j}.\n$$\nTaking expectations and using independence gives\n$$\n\\mathbb{E}[X(X-1)] = \\sum_{i \\neq j} \\mathbb{E}[Y_{i} Y_{j}] = \\sum_{i \\neq j} p^{2} = n(n-1) p^{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[\\frac{X(X-1)}{n(n-1)}\\right] = p^{2}.\n$$\nHence $X(X-1)/[n(n-1)]$ is an unbiased estimator of $p^{2}$ and is a function of the complete sufficient statistic $X$. By the Lehmann–Scheffé theorem, this estimator is the UMVUE for $\\theta = p^{2}$.\n\nThus, the UMVUE is\n$$\n\\frac{X(X-1)}{n(n-1)}.\n$$", "answer": "$$\\boxed{\\frac{X(X-1)}{n(n-1)}}$$", "id": "1917710"}, {"introduction": "Building on our foundation, this exercise explores a different class of estimation problems. Here, the parameter of interest, $\\theta$, defines the upper boundary of the distribution's support, a common feature in models of maximum thresholds or lifetimes. We will discover that the sufficient statistic is not a sum, but the maximum order statistic, $X_{(n)}$. This practice [@problem_id:1917722] provides essential experience in working with order statistics, from finding their distribution to using them to construct a UMVUE.", "problem": "In a particle physics experiment, the energy deposited by a certain type of exotic particle in a detector is modeled as a continuous random variable $X$. The probability density function (PDF) for this energy is given by\n$$f(x|\\theta) = \\frac{2x}{\\theta^2}$$\nfor $0 < x < \\theta$, and $f(x|\\theta) = 0$ otherwise. The parameter $\\theta > 0$ represents the unknown maximum possible energy that can be deposited by a single particle.\n\nAn experiment is conducted where $n$ such particles are independently detected, yielding a random sample of energy measurements $X_1, X_2, \\ldots, X_n$. Let $X_{(n)} = \\max(X_1, X_2, \\ldots, X_n)$ denote the maximum observed energy in the sample.\n\nYour task is to find the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for the parameter $\\theta$. Express your final answer as an analytic expression in terms of the sample size $n$ and the statistic $X_{(n)}$.", "solution": "We begin by identifying a sufficient statistic for the parameter. The joint density of the sample is\n$$\nf(x_{1},\\ldots,x_{n}\\mid \\theta)=\\prod_{i=1}^{n}\\frac{2x_{i}}{\\theta^{2}}\\mathbf{1}_{(0,\\theta)}(x_{i})\n=\\left(2^{n}\\prod_{i=1}^{n}x_{i}\\right)\\theta^{-2n}\\mathbf{1}_{(0,\\theta)}\\!\\left(x_{(n)}\\right),\n$$\nwhere $x_{(n)}=\\max\\{x_{1},\\ldots,x_{n}\\}$. By the Neyman–Fisher factorization theorem, $X_{(n)}$ is a sufficient statistic for $\\theta$.\n\nNext, we show that $X_{(n)}$ is complete. Let $g$ be any measurable function such that $\\mathbb{E}_{\\theta}[g(X_{(n)})]=0$ for all $\\theta>0$. The density of $X_{(n)}$ is\n$$\nf_{X_{(n)}|\\theta}(x)=\\frac{2n}{\\theta^{2n}}x^{2n-1}\\mathbf{1}_{(0,\\theta)}(x).\n$$\nThus,\n$$\n0=\\mathbb{E}_{\\theta}[g(X_{(n)})]=\\int_{0}^{\\theta} g(x)\\frac{2n}{\\theta^{2n}}x^{2n-1}\\,dx\n\\quad\\text{for all}\\ \\theta>0.\n$$\nDefine $H(\\theta)=\\int_{0}^{\\theta} g(x)x^{2n-1}\\,dx$. Then the condition becomes $H(\\theta)=0$ for all $\\theta>0$. Differentiating with respect to $\\theta$ yields $g(\\theta)\\theta^{2n-1}=0$ for all $\\theta>0$, hence $g(x)=0$ almost everywhere. Therefore $X_{(n)}$ is complete.\n\nBy the Lehmann–Scheffé theorem, the UMVUE of $\\theta$ is the unique unbiased estimator that is a function of $X_{(n)}$. We now compute the distribution of $X_{(n)}$ and its expectation. The cumulative distribution function of $X$ is $F(x)=x^{2}/\\theta^{2}$ on $(0,\\theta)$, so\n$$\n\\mathbb{P}_{\\theta}(X_{(n)}\\le t)=\\left(F(t)\\right)^{n}=\\left(\\frac{t^{2}}{\\theta^{2}}\\right)^{n}=\\frac{t^{2n}}{\\theta^{2n}},\\quad 0<t<\\theta,\n$$\nand the corresponding density is\n$$\nf_{X_{(n)}|\\theta}(t)=\\frac{d}{dt}\\left(\\frac{t^{2n}}{\\theta^{2n}}\\right)=\\frac{2n}{\\theta^{2n}}t^{2n-1},\\quad 0<t<\\theta.\n$$\nTherefore,\n$$\n\\mathbb{E}_{\\theta}[X_{(n)}]\n=\\int_{0}^{\\theta} t\\cdot \\frac{2n}{\\theta^{2n}}t^{2n-1}\\,dt\n=\\frac{2n}{\\theta^{2n}}\\int_{0}^{\\theta} t^{2n}\\,dt\n=\\frac{2n}{\\theta^{2n}}\\cdot \\frac{\\theta^{2n+1}}{2n+1}\n=\\frac{2n}{2n+1}\\,\\theta.\n$$\nIt follows that the scaled statistic\n$$\n\\widehat{\\theta}_{\\text{UMVUE}}=\\frac{2n+1}{2n}\\,X_{(n)}\n$$\nsatisfies\n$$\n\\mathbb{E}_{\\theta}\\!\\left[\\frac{2n+1}{2n}X_{(n)}\\right]=\\theta,\n$$\nso it is unbiased. Since it is a function of the complete sufficient statistic $X_{(n)}$, it is the UMVUE for $\\theta$.", "answer": "$$\\boxed{\\frac{2n+1}{2n}\\,X_{(n)}}$$", "id": "1917722"}, {"introduction": "Our final practice ventures into estimating a more abstract and complex quantity: the probability of a future observation being an even number. The target parameter is a non-trivial function of the underlying rate $\\lambda$, specifically $\\frac{1}{2}(1 + \\exp(-2\\lambda))$. This problem [@problem_id:1917757] demonstrates a powerful and elegant problem-solving strategy, leveraging the properties of probability generating functions to find an initial unbiased estimator, which is then refined into a UMVUE using the sample's complete sufficient statistic.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n$ from a Poisson distribution with an unknown mean parameter $\\lambda > 0$. The primary goal is to estimate a particular property of this distribution. Specifically, we are interested in the probability that a new, independent observation $Y$, also drawn from the same Poisson($\\lambda$) distribution, is an even integer (i.e., $Y \\in \\{0, 2, 4, \\dots\\}$).\n\nFind the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for this probability. Express your answer as a function of the sample size $n$ and the observed sample values $X_1, X_2, \\dots, X_n$.", "solution": "Let $Y \\sim \\mathrm{Poisson}(\\lambda)$ be independent of the sample. The target is $g(\\lambda) = \\mathbb{P}(Y \\text{ is even})$. Using the probability generating function of a Poisson random variable, for any $s \\in \\mathbb{R}$,\n$$\nG_{Y}(s) = \\mathbb{E}\\left[s^{Y}\\right] = \\exp\\left(\\lambda(s-1)\\right).\n$$\nTaking $s=-1$ yields\n$$\n\\mathbb{E}\\left[(-1)^{Y}\\right] = \\exp\\left(\\lambda(-1-1)\\right) = \\exp(-2\\lambda).\n$$\nSince $\\mathbb{P}(Y \\text{ even}) - \\mathbb{P}(Y \\text{ odd}) = \\mathbb{E}\\left[(-1)^{Y}\\right]$ and $\\mathbb{P}(Y \\text{ even}) + \\mathbb{P}(Y \\text{ odd}) = 1$, it follows that\n$$\ng(\\lambda) = \\mathbb{P}(Y \\text{ even}) = \\frac{1}{2}\\left(1+\\exp(-2\\lambda)\\right).\n$$\n\nLet $X_{1},\\dots,X_{n}$ be i.i.d. $\\mathrm{Poisson}(\\lambda)$ and define $S=\\sum_{i=1}^{n} X_{i}$. Then $S \\sim \\mathrm{Poisson}(n\\lambda)$, and $S$ is a complete and sufficient statistic for $\\lambda$ for the Poisson family. By the Lehmann–Scheffé theorem, the UMVUE of $g(\\lambda)$ is the unique function $h(S)$ satisfying $\\mathbb{E}_{\\lambda}[h(S)] = g(\\lambda)$ for all $\\lambda>0$.\n\nFor any $a \\in \\mathbb{R}$, using the probability generating function of $S$,\n$$\n\\mathbb{E}\\left[a^{S}\\right] = \\exp\\left(n\\lambda(a-1)\\right).\n$$\nChoose $a = 1 - \\frac{2}{n}$. Then\n$$\n\\mathbb{E}\\left[\\left(1-\\frac{2}{n}\\right)^{S}\\right] = \\exp\\left(n\\lambda\\left(1-\\frac{2}{n}-1\\right)\\right] = \\exp(-2\\lambda).\n$$\nTherefore the function\n$$\nh(S) = \\frac{1}{2}\\left(1+\\left(1-\\frac{2}{n}\\right)^{S}\\right)\n$$\nsatisfies $\\mathbb{E}_{\\lambda}[h(S)] = \\frac{1}{2}\\left(1+\\exp(-2\\lambda)\\right) = g(\\lambda)$ for all $\\lambda>0$. Since $h$ depends only on the complete sufficient statistic $S$, it is the UMVUE.\n\nExpressed in terms of the observed data, $S=\\sum_{i=1}^{n} X_{i}$, so the UMVUE is\n$$\n\\frac{1}{2}\\left(1+\\left(1-\\frac{2}{n}\\right)^{\\sum_{i=1}^{n} X_{i}}\\right).\n$$", "answer": "$$\\boxed{\\frac{1}{2}\\left(1+\\left(1-\\frac{2}{n}\\right)^{\\sum_{i=1}^{n} X_{i}}\\right)}$$", "id": "1917757"}]}