{"hands_on_practices": [{"introduction": "We begin with a foundational application of the Lehmann-Scheffé theorem. In many real-world scenarios, such as modeling the decay time of a subatomic particle, we are interested in estimating the mean of a process. This exercise asks you to find the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for the mean lifetime, $\\theta = 1/p$, of a particle whose decay follows a Geometric distribution [@problem_id:1929833]. By working through this problem, you will see how the theorem can rigorously confirm the optimality of an intuitive estimator, the sample mean.", "problem": "A physicist is studying a newly discovered unstable subatomic particle. In a simplified discrete-time model, the particle has a constant probability $p$ of decaying during any given time interval. Let the random variable $X$ denote the number of time intervals until the particle decays. The probability distribution for $X$ is thus given by the geometric distribution with probability mass function $f(k;p) = (1-p)^{k-1} p$ for $k \\in \\{1, 2, 3, \\ldots\\}$, where $p \\in (0, 1)$ is the unknown parameter representing the decay probability. An experiment is conducted where $n$ such independent particles are observed, and their decay times are recorded as a random sample $X_1, X_2, \\ldots, X_n$. The physicist is interested in estimating the mean lifetime of the particle, which theory suggests is given by $\\theta=1/p$. Find the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for $\\theta$. Express your answer as an analytic expression in terms of the sample data $X_1, X_2, \\ldots, X_n$.", "solution": "To find the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for $\\theta = 1/p$, we will use the Lehmann-Scheffé theorem. This involves finding a complete sufficient statistic for the parameter $p$ and then finding a function of this statistic that is an unbiased estimator for $\\theta$.\n\nFirst, let's find a sufficient statistic for $p$. The joint probability mass function (PMF) of the random sample $X_1, \\dots, X_n$ is:\n$$ L(p; \\mathbf{x}) = \\prod_{i=1}^{n} f(x_i; p) = \\prod_{i=1}^{n} (1-p)^{x_i-1} p = p^n (1-p)^{\\sum_{i=1}^{n} x_i - n} $$\nWe can rewrite this expression to identify the structure of the exponential family of distributions:\n$$ L(p; \\mathbf{x}) = p^n (1-p)^{-n} (1-p)^{\\sum_{i=1}^{n} x_i} = \\left(\\frac{p}{1-p}\\right)^n (1-p)^{\\sum_{i=1}^{n} x_i} $$\n$$ L(p; \\mathbf{x}) = \\exp\\left( n \\ln\\left(\\frac{p}{1-p}\\right) + \\left(\\sum_{i=1}^{n} x_i\\right) \\ln(1-p) \\right) $$\nThis is in the form of a one-parameter exponential family, $h(\\mathbf{x})c(p)\\exp(w(p)T(\\mathbf{x}))$, with $T(\\mathbf{x}) = \\sum_{i=1}^{n} X_i$. By the Fisher-Neyman Factorization Theorem, $T = \\sum_{i=1}^{n} X_i$ is a sufficient statistic for $p$.\n\nFurthermore, because the distribution is a member of the one-parameter exponential family and the parameter space for $w(p) = \\ln(1-p)$ is $(-\\infty, 0)$, which contains an open interval, the sufficient statistic $T$ is also a complete statistic.\n\nNext, according to the Lehmann-Scheffé theorem, if we can find an estimator for $\\theta$ that is a function of the complete sufficient statistic $T$ and is also unbiased, then it is the UMVUE. An alternative approach is to find any simple unbiased estimator and then use the Rao-Blackwell theorem to find its conditional expectation given the complete sufficient statistic.\n\nLet's find a simple unbiased estimator for $\\theta = 1/p$. Consider the estimator $W = X_1$. The expected value of a geometric random variable $X$ is\n$$ E[X] = \\sum_{k=1}^{\\infty} k (1-p)^{k-1} p $$\nLet $q=1-p$. The sum becomes $p \\sum_{k=1}^{\\infty} k q^{k-1}$. We recognize the sum as the derivative of a geometric series: $\\sum_{k=1}^{\\infty} k q^{k-1} = \\frac{d}{dq} \\sum_{k=0}^{\\infty} q^k = \\frac{d}{dq} \\frac{1}{1-q} = \\frac{1}{(1-q)^2}$.\nSo, $E[X] = p \\frac{1}{(1-q)^2} = p \\frac{1}{(1-(1-p))^2} = p \\frac{1}{p^2} = \\frac{1}{p}$.\nThus, $E[W] = E[X_1] = 1/p = \\theta$. So, $W=X_1$ is an unbiased estimator for $\\theta$.\n\nNow we find the UMVUE, $\\hat{\\theta}$, by conditioning $W$ on the complete sufficient statistic $T$:\n$$ \\hat{\\theta} = E[W | T] = E\\left[X_1 \\left| \\sum_{i=1}^{n} X_i\\right.\\right] $$\nSince the random variables $X_1, X_2, \\ldots, X_n$ are independent and identically distributed (i.i.d.), the conditional expectation $E[X_i | \\sum_{j=1}^{n} X_j]$ is the same for all $i=1, \\ldots, n$. Let's call this common value $E^*$.\n$$ E^* = E\\left[X_1 \\left| \\sum_{j=1}^{n} X_j\\right.\\right] = E\\left[X_2 \\left| \\sum_{j=1}^{n} X_j\\right.\\right] = \\cdots = E\\left[X_n \\left| \\sum_{j=1}^{n} X_j\\right.\\right] $$\nUsing the linearity of expectation, we can write:\n$$ E\\left[\\sum_{i=1}^{n} X_i \\left| \\sum_{j=1}^{n} X_j\\right.\\right] = \\sum_{i=1}^{n} E\\left[X_i \\left| \\sum_{j=1}^{n} X_j\\right.\\right] = \\sum_{i=1}^{n} E^* = n E^* $$\nThe left side of the equation is the expectation of the sum given the sum itself, which is just the value of the sum. Let $T = \\sum_{j=1}^{n} X_j$.\n$$ E[T | T] = T $$\nTherefore, we have $T = n E^*$, which implies $E^* = T/n$.\nSo, the UMVUE is:\n$$ \\hat{\\theta} = E^* = \\frac{T}{n} = \\frac{\\sum_{i=1}^{n} X_i}{n} $$\nThis estimator, the sample mean, is a function of the complete sufficient statistic $T$. We can quickly verify its unbiasedness: $E[\\hat{\\theta}] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right] = \\frac{1}{n}\\sum_{i=1}^{n} E[X_i] = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{p} = \\frac{1}{n} \\cdot n \\cdot \\frac{1}{p} = \\frac{1}{p}$. Since it is an unbiased estimator and a function of the complete sufficient statistic, by the Lehmann-Scheffé theorem, it is the UMVUE for $\\theta=1/p$.", "answer": "$$\\boxed{\\frac{1}{n} \\sum_{i=1}^{n} X_i}$$", "id": "1929833"}, {"introduction": "Often, we need to estimate not just a basic parameter, but a more complex function of it. For example, some physical theories might predict interaction rates proportional not to a base rate $\\lambda$, but to its square, $\\lambda^2$. This practice challenges you to find the UMVUE for $\\lambda^2$ from a sample of Poisson-distributed data [@problem_id:1929886]. This process will sharpen your skills in using the moments of a complete sufficient statistic to construct a unique, optimal estimator for a non-linear parameter function.", "problem": "In astrophysical studies, the number of high-energy neutrinos detected by a cubic-kilometer detector from a specific cosmic event during a fixed time interval is often modeled by a Poisson distribution with an unknown average rate parameter $\\lambda$. An astrophysicist collects $n$ independent measurements of these counts from $n$ similar, consecutive time intervals, resulting in a random sample $X_1, X_2, \\ldots, X_n$. Certain theoretical models of neutrino emission predict that the interaction rate between these neutrinos is proportional to $\\lambda^2$. To test these models, a highly efficient and accurate estimator for $\\lambda^2$ is required.\n\nGiven the random sample $X_1, X_2, \\ldots, X_n$ from a Poisson($\\lambda$) distribution, find the Uniformly Minimum Variance Unbiased Estimator (UMVUE) for the parameter $\\lambda^2$. Express your answer as a single closed-form analytic expression in terms of the sample mean, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$, and the sample size $n$.", "solution": "Let $X_{1},\\dots,X_{n}$ be independent with $X_{i}\\sim\\text{Poisson}(\\lambda)$. The sum $S=\\sum_{i=1}^{n}X_{i}$ then satisfies $S\\sim\\text{Poisson}(n\\lambda)$ by the additivity of independent Poisson random variables. The family is a full regular exponential family, and $S$ is a complete and sufficient statistic for $\\lambda$.\n\nBy the Lehmann–Scheffé theorem, the UMVUE of any function of $\\lambda$ is the unique unbiased estimator that is a function of $S$. We therefore seek a function $g(S)$ such that $\\mathbb{E}_{\\lambda}[g(S)]=\\lambda^{2}$.\n\nCompute the first two moments of $S$. For a Poisson random variable with parameter $\\theta$, $\\mathbb{E}[Y]=\\theta$ and $\\operatorname{Var}(Y)=\\theta$, so\n$$\n\\mathbb{E}[S]=n\\lambda,\\qquad \\operatorname{Var}(S)=n\\lambda,\\qquad \\mathbb{E}[S^{2}]=\\operatorname{Var}(S)+\\{\\mathbb{E}[S]\\}^{2}=n\\lambda+n^{2}\\lambda^{2}.\n$$\nConsider\n$$\ng(S)=\\frac{S^{2}-S}{n^{2}}.\n$$\nThen\n$$\n\\mathbb{E}[g(S)]=\\frac{\\mathbb{E}[S^{2}]-\\mathbb{E}[S]}{n^{2}}=\\frac{(n\\lambda+n^{2}\\lambda^{2})-(n\\lambda)}{n^{2}}=\\lambda^{2}.\n$$\nThus $g(S)$ is unbiased for $\\lambda^{2}$ and, being a function of the complete sufficient statistic $S$, is the UMVUE by Lehmann–Scheffé.\n\nExpressing $g(S)$ in terms of the sample mean $\\bar{X}=S/n$ gives\n$$\ng(S)=\\bar{X}^{2}-\\frac{\\bar{X}}{n}.\n$$\nTherefore, the UMVUE for $\\lambda^{2}$ is $\\bar{X}^{2}-\\frac{\\bar{X}}{n}$.", "answer": "$$\\boxed{\\bar{X}^{2}-\\frac{\\bar{X}}{n}}$$", "id": "1929886"}, {"introduction": "Statistical analysis frequently involves comparing or combining information from multiple independent sources. This problem extends the principles of optimal estimation to a two-sample context, a common scenario in experimental science where two processes are observed independently. Here, you will derive the UMVUE for the product of two independent Poisson rates, $\\lambda_1 \\lambda_2$ [@problem_id:1929838]. This exercise demonstrates the power and flexibility of the Lehmann-Scheffé theorem in multi-parameter settings, a crucial concept for more advanced statistical modeling.", "problem": "In a study of particle emissions, two independent sources are observed. Source A emits particles according to a Poisson process with an unknown rate $\\lambda_1 > 0$. The number of particles detected from Source A in $n$ disjoint time intervals of equal duration are recorded as $X_1, X_2, \\dots, X_n$, which can be modeled as an independent and identically distributed (i.i.d.) random sample from a Poisson($\\lambda_1$) distribution. Similarly, Source B emits particles according to an independent Poisson process with an unknown rate $\\lambda_2 > 0$. The counts from Source B over $m$ similar, independent time intervals are recorded as $Y_1, Y_2, \\dots, Y_m$, forming an i.i.d. random sample from a Poisson($\\lambda_2$) distribution.\n\nYour task is to find the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for the product of the two rates, $\\tau = \\lambda_1 \\lambda_2$.\n\nExpress your final answer as an analytical expression in terms of the sample sums $S_X = \\sum_{i=1}^n X_i$ and $S_Y = \\sum_{j=1}^m Y_j$, and the sample sizes $n$ and $m$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. $\\operatorname{Poisson}(\\lambda_{1})$ and $Y_{1},\\dots,Y_{m}$ be i.i.d. $\\operatorname{Poisson}(\\lambda_{2})$, with the two samples independent. Define $S_{X}=\\sum_{i=1}^{n}X_{i}$ and $S_{Y}=\\sum_{j=1}^{m}Y_{j}$. Then $S_{X}\\sim \\operatorname{Poisson}(n\\lambda_{1})$ and $S_{Y}\\sim \\operatorname{Poisson}(m\\lambda_{2})$, and $S_{X}$ and $S_{Y}$ are independent.\n\nBy the factorization theorem, $(S_{X},S_{Y})$ is a sufficient statistic for $(\\lambda_{1},\\lambda_{2})$. Since for a Poisson sample the sum is complete, $S_{X}$ is complete for $\\lambda_{1}$ and $S_{Y}$ is complete for $\\lambda_{2}$. Independence implies that the joint family of $(S_{X},S_{Y})$ is complete for $(\\lambda_{1},\\lambda_{2})$: if $\\mathbb{E}[g(S_{X},S_{Y})]=0$ for all $\\lambda_{1},\\lambda_{2}$, then for each fixed $s_{Y}$ the function $s_{X}\\mapsto g(s_{X},s_{Y})$ has zero expectation under all $\\operatorname{Poisson}(n\\lambda_{1})$, hence is almost surely zero by completeness of $S_{X}$; varying $s_{Y}$ and applying completeness of $S_{Y}$ yields $g\\equiv 0$.\n\nWe seek an unbiased estimator of $\\tau=\\lambda_{1}\\lambda_{2}$ that is a function of $(S_{X},S_{Y})$. Consider the sample means $\\bar{X}=\\frac{S_{X}}{n}$ and $\\bar{Y}=\\frac{S_{Y}}{m}$. Since the two samples are independent,\n$$\n\\mathbb{E}\\!\\left[\\bar{X}\\bar{Y}\\right]\n=\\mathbb{E}[\\bar{X}]\\,\\mathbb{E}[\\bar{Y}]\n=\\lambda_{1}\\lambda_{2}.\n$$\nThus $h(S_{X},S_{Y})=\\frac{S_{X}S_{Y}}{nm}$ is unbiased for $\\tau$. Alternatively, starting from the unbiased estimator $T=X_{1}Y_{1}$ with $\\mathbb{E}[T]=\\lambda_{1}\\lambda_{2}$, Rao–Blackwellization yields\n$$\n\\mathbb{E}[T\\mid S_{X},S_{Y}]\n=\\mathbb{E}[X_{1}\\mid S_{X}]\\,\\mathbb{E}[Y_{1}\\mid S_{Y}]\n=\\frac{S_{X}}{n}\\cdot\\frac{S_{Y}}{m}\n=\\frac{S_{X}S_{Y}}{nm},\n$$\nusing the multinomial splitting property of the Poisson and independence of the two samples. Since $(S_{X},S_{Y})$ is complete and sufficient, by the Lehmann–Scheffé theorem, the unbiased function $\\frac{S_{X}S_{Y}}{nm}$ is the UMVUE for $\\tau$ and is unique.", "answer": "$$\\boxed{\\frac{S_{X}S_{Y}}{n m}}$$", "id": "1929838"}]}