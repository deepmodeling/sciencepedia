{"hands_on_practices": [{"introduction": "Let's begin by applying the Rao-Blackwell theorem to a familiar discrete distribution. This problem [@problem_id:1922440] challenges you to improve an estimator for a function of the Poisson parameter, $\\lambda^2$, by conditioning on the sample sum. This practice is invaluable as it demonstrates a classic statistical result: the conditional distribution of Poisson variates, given their sum, is Multinomial (or Binomial, in this specific case).", "problem": "Let $X_1, X_2, \\ldots, X_n$ be an independent and identically distributed random sample of size $n > 1$ from a Poisson distribution with an unknown mean $\\lambda > 0$. We are interested in estimating the parameter $\\theta = \\lambda^2$.\n\nIt can be shown that the statistic $T = X_1(X_1 - 1)$ is an unbiased estimator for $\\theta$. However, this estimator is not optimal as it is based on only a single observation, $X_1$. To construct a more statistically efficient estimator, one can compute the conditional expectation of $T$ given the sufficient statistic for $\\lambda$. For a Poisson sample, the sufficient statistic is the sum of the observations, $S = \\sum_{i=1}^n X_i$.\n\nYour task is to calculate the improved estimator $T'$ defined as the conditional expectation of $T$ given $S$.\n$$T' = E[T | S]$$\nExpress your final answer as an analytic function of the sufficient statistic $S$ and the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be iid $\\operatorname{Poisson}(\\lambda)$ and $S=\\sum_{i=1}^{n}X_{i}$ the sufficient statistic for $\\lambda$. Define $T=X_{1}(X_{1}-1)$. The Rao-Blackwell improvement is $T'=\\mathbb{E}[T\\mid S]$.\n\nFor iid Poisson variables, conditional on $S=s$, the vector $(X_{1},\\ldots,X_{n})$ has a multinomial distribution with $s$ trials and equal cell probabilities $1/n$. In particular,\n$$\nX_{1}\\mid S=s \\sim \\operatorname{Binomial}\\!\\left(s,\\frac{1}{n}\\right).\n$$\nUsing the identity $\\mathbb{E}[X(X-1)]=\\operatorname{Var}(X)+\\{\\mathbb{E}[X]\\}^{2}-\\mathbb{E}[X]$, applied conditionally on $S=s$, we compute\n$$\n\\mathbb{E}\\!\\left[X_{1}(X_{1}-1)\\mid S=s\\right]\n=\\operatorname{Var}(X_{1}\\mid S=s)+\\left(\\mathbb{E}[X_{1}\\mid S=s]\\right)^{2}-\\mathbb{E}[X_{1}\\mid S=s].\n$$\nFor $X_{1}\\mid S=s\\sim \\operatorname{Binomial}(s,1/n)$, we have\n$$\n\\mathbb{E}[X_{1}\\mid S=s]=\\frac{s}{n},\\qquad \\operatorname{Var}(X_{1}\\mid S=s]=\\frac{s}{n}\\left(1-\\frac{1}{n}\\right)=\\frac{s(n-1)}{n^{2}}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[X_{1}(X_{1}-1)\\mid S=s\\right]\n=\\frac{s(n-1)}{n^{2}}+\\left(\\frac{s}{n}\\right)^{2}-\\frac{s}{n}\n=\\frac{s^{2}-s}{n^{2}}\n=\\frac{s(s-1)}{n^{2}}.\n$$\nSince this holds for every $s$, the improved estimator as a function of $S$ is\n$$\nT'=\\mathbb{E}[T\\mid S]=\\frac{S(S-1)}{n^{2}}.\n$$\nAs a check of unbiasedness, note that $S\\sim \\operatorname{Poisson}(n\\lambda)$, so $\\mathbb{E}[S(S-1)]=(n\\lambda)^{2}$ and hence $\\mathbb{E}[T']=\\lambda^{2}$, agreeing with the target $\\theta$.", "answer": "$$\\boxed{\\frac{S(S-1)}{n^{2}}}$$", "id": "1922440"}, {"introduction": "Having explored a discrete case, we now turn to a continuous distribution. In this exercise [@problem_id:1922421], you will work with a sample from a Gamma distribution and improve a simple estimator based on a single observation. The key is to condition on the sample sum, which is a sufficient statistic, and this process will highlight the deep connection between the Gamma and Dirichlet distributions.", "problem": "Let $X_1, X_2, \\dots, X_n$ be an independent and identically distributed random sample drawn from a Gamma distribution. The probability density function of this distribution is given by\n$$f(x; \\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}} x^{\\alpha-1} \\exp(-x/\\beta)$$\nfor $x > 0$. The shape parameter $\\alpha > 0$ is a known constant, while the scale parameter $\\beta > 0$ is unknown.\n\nAn initial, simple estimator for the unknown parameter $\\beta$ is constructed using only the first observation, defined as $T_0 = \\frac{X_1}{\\alpha}$. To obtain a more refined estimator, one can compute the conditional expectation of $T_0$ given the total sum of the observations, $S = \\sum_{i=1}^{n} X_i$.\n\nLet this refined estimator be $T_1 = E[T_0 | S]$. Find the expression for $T_1$ in terms of $n$, $\\alpha$, and the sample sum $S$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. with density $f(x;\\alpha,\\beta)=\\left(\\Gamma(\\alpha)\\beta^{\\alpha}\\right)^{-1}x^{\\alpha-1}\\exp(-x/\\beta)$ for $x>0$, with known $\\alpha>0$ and unknown $\\beta>0$. Define $T_{0}=X_{1}/\\alpha$ and $S=\\sum_{i=1}^{n}X_{i}$. We seek $T_{1}=E[T_{0}\\mid S]$.\n\nBy linearity of conditional expectation,\n$$\nT_{1}=E\\!\\left[\\frac{X_{1}}{\\alpha}\\,\\middle|\\,S\\right]=\\frac{1}{\\alpha}E[X_{1}\\mid S].\n$$\nThus it suffices to compute $E[X_{1}\\mid S]$.\n\nWrite the joint density of $X=(X_{1},\\dots,X_{n})$:\n$$\nf_{X}(x_{1},\\dots,x_{n})=\\left(\\Gamma(\\alpha)\\beta^{\\alpha}\\right)^{-n}\\left(\\prod_{i=1}^{n}x_{i}^{\\alpha-1}\\right)\\exp\\!\\left(-\\frac{1}{\\beta}\\sum_{i=1}^{n}x_{i}\\right),\\quad x_{i}>0.\n$$\nMake the change of variables\n$$\nS=\\sum_{i=1}^{n}X_{i},\\quad U_{i}=\\frac{X_{i}}{S}\\ \\text{for}\\ i=1,\\dots,n-1,\\quad U_{n}=1-\\sum_{i=1}^{n-1}U_{i},\n$$\nwith Jacobian determinant $J=S^{n-1}$. Then\n$$\nf_{S,U}(s,u_{1},\\dots,u_{n-1})=\\left(\\Gamma(\\alpha)\\beta^{\\alpha}\\right)^{-n}\\left(\\prod_{i=1}^{n}(su_{i})^{\\alpha-1}\\right)\\exp\\!\\left(-\\frac{s}{\\beta}\\right)S^{n-1}\n$$\n$$\n=\\left(\\Gamma(\\alpha)\\beta^{\\alpha}\\right)^{-n}s^{n\\alpha-1}\\exp\\!\\left(-\\frac{s}{\\beta}\\right)\\prod_{i=1}^{n}u_{i}^{\\alpha-1},\n$$\nfor $s>0$, $u_{i}>0$, and $\\sum_{i=1}^{n}u_{i}=1$. This factors into a function of $s$ times a function of $u=(u_{1},\\dots,u_{n})$, so the conditional density of $U$ given $S=s$ is\n$$\nf_{U\\mid S}(u\\mid s)=\\frac{\\Gamma(n\\alpha)}{\\Gamma(\\alpha)^{n}}\\prod_{i=1}^{n}u_{i}^{\\alpha-1},\n$$\nwhich is Dirichlet with parameters $(\\alpha,\\dots,\\alpha)$. Therefore $U_{1}\\mid S=s$ has the Beta distribution $\\mathrm{Beta}(\\alpha,(n-1)\\alpha)$, and hence\n$$\nE[U_{1}\\mid S=s]=\\frac{\\alpha}{\\alpha+(n-1)\\alpha}=\\frac{1}{n}.\n$$\nSince $X_{1}=SU_{1}$, we obtain\n$$\nE[X_{1}\\mid S=s]=s\\,E[U_{1}\\mid S=s]=\\frac{s}{n}.\n$$\nThus, as a function of $S$,\n$$\nE[X_{1}\\mid S]=\\frac{S}{n},\n$$\nand consequently\n$$\nT_{1}=E\\!\\left[\\frac{X_{1}}{\\alpha}\\,\\middle|\\,S\\right]=\\frac{1}{\\alpha}\\cdot\\frac{S}{n}=\\frac{S}{n\\alpha}.\n$$", "answer": "$$\\boxed{\\frac{S}{n\\alpha}}$$", "id": "1922421"}, {"introduction": "To complete our practice, we'll tackle a scenario that broadens our perspective on sufficient statistics. This problem [@problem_id:1922455] involves a uniform distribution, where the sufficient statistic is the sample maximum, $X_{(n)}$, rather than the sample sum. This exercise is crucial for understanding how to apply the Rao-Blackwell theorem when dealing with order statistics and distributions whose support depends on the parameter.", "problem": "Let $X_1, \\dots, X_n$ be a random sample of size $n > 1$ from a continuous uniform distribution on the interval $(0, \\theta)$, where $\\theta > 0$ is an unknown parameter. The probability density function for this distribution is $f(x) = \\frac{1}{\\theta}$ for $0  x  \\theta$, and $f(x)=0$ otherwise.\n\nLet $X_{(1)} = \\min(X_1, \\dots, X_n)$ be the sample minimum and $X_{(n)} = \\max(X_1, \\dots, X_n)$ be the sample maximum.\n\nAn initial, though suboptimal, unbiased estimator for $\\theta$ is given by $T = (n+1)X_{(1)}$. By leveraging the information contained in the sample maximum $X_{(n)}$, a new estimator, $T'$, can be constructed that is also unbiased for $\\theta$ but has a smaller variance.\n\nFind the expression for this improved estimator $T'$. Your answer should be a function of the sample size $n$ and the sample maximum $X_{(n)}$.", "solution": "We begin with the joint density of the sample from the Uniform distribution on $(0,\\theta)$:\n$$\nf(x_{1},\\dots,x_{n};\\theta)=\\theta^{-n}\\,\\mathbb{I}\\{0x_{(1)}x_{(n)}\\theta\\},\n$$\nwhich factors as $h(x_{1},\\dots,x_{n})\\,g_{\\theta}(x_{(n)})$ with $g_{\\theta}(x_{(n)})=\\theta^{-n}\\,\\mathbb{I}\\{x_{(n)}\\theta\\}$. By the factorization theorem, $X_{(n)}$ is a sufficient statistic for $\\theta$.\n\nGiven any unbiased estimator $T$, the Rao-Blackwell theorem states that $T'=\\mathbb{E}[T\\mid X_{(n)}]$ is also unbiased for $\\theta$ and has variance no larger than that of $T$. Here, $T=(n+1)X_{(1)}$ is an unbiased estimator of $\\theta$, so we set\n$$\nT'=\\mathbb{E}[(n+1)X_{(1)}\\mid X_{(n)}].\n$$\n\nTo compute this conditional expectation, fix $t\\in(0,\\theta)$ and condition on $X_{(n)}=t$. Given $X_{(n)}=t$, the remaining $n-1$ observations behave as if they were drawn i.i.d. from $\\text{Uniform}(0,t)$, and $X_{(1)}$ is the minimum of these $n-1$ values. The conditional density of $X_{(1)}$ given $X_{(n)}=t$ is therefore\n$$\nf_{X_{(1)}\\mid X_{(n)}=t}(y)=\\frac{n-1}{t}\\left(1-\\frac{y}{t}\\right)^{n-2},\\qquad 0yt.\n$$\nThus,\n$$\n\\mathbb{E}[X_{(1)}\\mid X_{(n)}=t]\n=\\int_{0}^{t}y\\,\\frac{n-1}{t}\\left(1-\\frac{y}{t}\\right)^{n-2}\\,dy.\n$$\nWith the substitution $u=y/t$, $dy=t\\,du$, this becomes\n$$\n\\mathbb{E}[X_{(1)}\\mid X_{(n)}=t]\n=(n-1)t\\int_{0}^{1}u(1-u)^{n-2}\\,du\n=(n-1)t\\,\\mathrm{B}(2,n-1),\n$$\nwhere $\\mathrm{B}(a,b)=\\int_{0}^{1}u^{a-1}(1-u)^{b-1}\\,du=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$. Hence,\n$$\n\\mathbb{E}[X_{(1)}\\mid X_{(n)}=t]\n=(n-1)t\\,\\frac{\\Gamma(2)\\Gamma(n-1)}{\\Gamma(n+1)}\n=(n-1)t\\,\\frac{1!\\,(n-2)!}{n!}\n=\\frac{t}{n}.\n$$\nTherefore,\n$$\nT'=(n+1)\\,\\mathbb{E}[X_{(1)}\\mid X_{(n)}]\n=(n+1)\\,\\frac{X_{(n)}}{n}\n=\\frac{n+1}{n}\\,X_{(n)}.\n$$\nUnbiasedness can be verified directly: the density of $X_{(n)}$ is $f_{X_{(n)}}(t)=\\frac{n}{\\theta^{n}}t^{n-1}$ for $0t\\theta$, so\n$$\n\\mathbb{E}[X_{(n)}]=\\int_{0}^{\\theta}t\\,\\frac{n}{\\theta^{n}}t^{n-1}\\,dt=\\frac{n}{n+1}\\,\\theta,\n$$\nand thus\n$$\n\\mathbb{E}[T']=\\frac{n+1}{n}\\,\\mathbb{E}[X_{(n)}]=\\frac{n+1}{n}\\cdot\\frac{n}{n+1}\\,\\theta=\\theta.\n$$\nBy Rao-Blackwell, $\\mathrm{Var}(T')\\le \\mathrm{Var}(T)$, and $T'$ uses the information in the sufficient statistic $X_{(n)}$ as required. Hence the improved estimator is\n$$\nT'=\\frac{n+1}{n}\\,X_{(n)}.\n$$", "answer": "$$\\boxed{\\frac{n+1}{n}\\,X_{(n)}}$$", "id": "1922455"}]}