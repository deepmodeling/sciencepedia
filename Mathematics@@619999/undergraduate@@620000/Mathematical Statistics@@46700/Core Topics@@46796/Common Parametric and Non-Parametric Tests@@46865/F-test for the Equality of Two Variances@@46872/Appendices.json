{"hands_on_practices": [{"introduction": "Mastering any statistical test begins with understanding its core calculation. This first practice focuses on the fundamental step of computing the F-statistic to compare the variances of two independent samples. By working through a clear, practical scenario from agricultural science, you will apply the standard formula and convention for the F-test, building a solid foundation for more complex applications. [@problem_id:1916681]", "problem": "An agricultural scientist is studying the consistency of crop yield for two new fertilizer treatments, Treatment 1 and Treatment 2. It is assumed that the crop yield (in kilograms per plot) for both treatments is approximately normally distributed. The scientist conducts an experiment using two independent random samples of plots. For a sample of $n_1 = 18$ plots receiving Treatment 1, the sample variance of the yield was $s_1^2 = 45.5$. For a sample of $n_2 = 25$ plots receiving Treatment 2, the sample variance of the yield was $s_2^2 = 28.2$.\n\nThe scientist wants to test whether the variability in crop yield for Treatment 1 is significantly different from that of Treatment 2. The standard procedure for this involves calculating an F-test statistic. By convention, to simplify the use of statistical tables for a two-tailed test, the larger sample variance is placed in the numerator. Following this convention, calculate the value of the F-test statistic. Round your final answer to three significant figures.", "solution": "We are testing equality of variances for two independent normal populations using the F-test. The test statistic is defined, by convention placing the larger sample variance in the numerator, as\n$$\nF=\\frac{s_{\\text{large}}^{2}}{s_{\\text{small}}^{2}}.\n$$\nGiven $n_{1}=18$ with $s_{1}^{2}=45.5$ and $n_{2}=25$ with $s_{2}^{2}=28.2$, the larger sample variance is $s_{1}^{2}=45.5$. Therefore,\n$$\nF=\\frac{45.5}{28.2}=\\frac{455}{282}\\approx 1.613\\ldots\n$$\nThe associated degrees of freedom for reference are $n_{1}-1=17$ and $n_{2}-1=24$, but only the statistic value is requested. Rounding to three significant figures gives\n$$\nF\\approx 1.61.\n$$", "answer": "$$\\boxed{1.61}$$", "id": "1916681"}, {"introduction": "The F-test is more versatile than it first appears. This advanced exercise demonstrates how it can be adapted to test hypotheses that are not directly about variance. By examining data from log-normal distributions, you'll discover that testing for the equality of the coefficients of variation is equivalent to performing an F-test on log-transformed data. This problem highlights the power of data transformation and a deep understanding of probability distributions in statistical modeling. [@problem_id:1916949]", "problem": "Two rival suppliers, Alpha Components and Beta Devices, manufacture a specific type of high-endurance capacitor. The lifetime of these capacitors, in thousands of hours, is known to be well-approximated by a log-normal distribution. Let the lifetimes from Alpha Components be modeled by a random variable $X_1 \\sim LN(\\mu_1, \\sigma_1^2)$ and from Beta Devices by $X_2 \\sim LN(\\mu_2, \\sigma_2^2)$.\n\nA quality assurance team wants to compare the relative consistency in the lifetimes of the capacitors from the two suppliers. The primary metric for this comparison is the coefficient of variation (CV). The team aims to test the null hypothesis that the CVs of the two populations are equal, against the alternative that they are not equal.\n\nTo perform this test, the team collects small samples of lifetime data from each supplier.\nSample from Alpha Components ($n_1=6$):\n$$ \\{2.72, 4.48, 7.39, 12.18, 20.09, 33.12\\} $$\nSample from Beta Devices ($n_2=5$):\n$$ \\{9.03, 11.02, 13.46, 16.45, 20.09\\} $$\n\nTo carry out the hypothesis test, a specific data transformation is required, followed by the calculation of an F-statistic. Your task is to compute the value of this F-statistic. For the purpose of this problem, the F-statistic is defined as the ratio of the larger sample variance to the smaller sample variance, calculated from the appropriately transformed data.\n\nCalculate the value of the F-statistic. Round your final answer to three significant figures.", "solution": "For lognormal data, if $X \\sim LN(\\mu, \\sigma^{2})$ then $Y=\\ln X \\sim N(\\mu, \\sigma^{2})$ and the coefficient of variation is $CV=\\sqrt{\\exp(\\sigma^{2})-1}$. Equality of CVs is therefore equivalent to equality of the log-scale variances $\\sigma_{1}^{2}=\\sigma_{2}^{2}$. To test this, transform the data via $y=\\ln x$ and use the usual variance-ratio statistic on the transformed samples.\n\nDefine $y_{1i}=\\ln x_{1i}$ for Alpha and $y_{2j}=\\ln x_{2j}$ for Beta. The given data are values very close to $\\exp$ of equally spaced points, so the transformed samples are (to numerical rounding accuracy)\n$$\ny_{1} \\approx \\{1.0, 1.5, 2.0, 2.5, 3.0, 3.5\\}, \\quad\ny_{2} \\approx \\{2.2, 2.4, 2.6, 2.8, 3.0\\}.\n$$\nCompute sample variances on the log scale using\n$$\ns^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}.\n$$\nFor Alpha ($n_{1}=6$), $\\bar{y}_{1}=\\frac{1+3.5}{2}=2.25$. The deviations are $\\{-1.25,-0.75,-0.25,0.25,0.75,1.25\\}$ with squared values summing to $1.5625+0.5625+0.0625+0.0625+0.5625+1.5625=4.375$. Thus\n$$\ns_{1}^{2}=\\frac{4.375}{6-1}=0.875.\n$$\nFor Beta ($n_{2}=5$), $\\bar{y}_{2}=\\frac{2.2+3.0}{2}=2.6$. The deviations are $\\{-0.4,-0.2,0,0.2,0.4\\}$ with squared values summing to $0.16+0.04+0+0.04+0.16=0.40$. Thus\n$$\ns_{2}^{2}=\\frac{0.40}{5-1}=0.10.\n$$\nThe required F-statistic (larger variance divided by smaller) is\n$$\nF=\\frac{\\max\\{s_{1}^{2},s_{2}^{2}\\}}{\\min\\{s_{1}^{2},s_{2}^{2}\\}}=\\frac{0.875}{0.10}=8.75.\n$$\nUsing the exact natural logarithms of the reported rounded data changes the variances only in the fourth decimal place and yields the same F-statistic to three significant figures. Therefore, rounding to three significant figures gives $8.75$.", "answer": "$$\\boxed{8.75}$$", "id": "1916949"}, {"introduction": "Beyond simply performing a hypothesis test lies the crucial question of its effectiveness. This final practice shifts our focus from application to theory by asking you to derive the power function of the F-test. Understanding a test's power—its probability of correctly rejecting a false null hypothesis—is essential for interpreting its results and designing effective experiments. This derivation will connect the test statistic to its underlying distribution and reveal how its sensitivity depends on the true ratio of the population variances. [@problem_id:1916926]", "problem": "An industrial engineer is tasked with comparing the consistency of two independent manufacturing processes for a specific electronic component. The critical characteristic of the component is assumed to follow a normal distribution. Let $X_1, X_2, \\dots, X_{n_1}$ be a random sample of measurements from the first process, assumed to be drawn from a normal distribution $N(\\mu_1, \\sigma_1^2)$. Similarly, let $Y_1, Y_2, \\dots, Y_{n_2}$ be an independent random sample from the second process, assumed to be drawn from a normal distribution $N(\\mu_2, \\sigma_2^2)$.\n\nThe engineer sets up a hypothesis test to determine if the first process is less consistent (i.e., has a larger variance) than the second. The null and alternative hypotheses are:\n$$H_0: \\sigma_1^2 = \\sigma_2^2$$\n$$H_1: \\sigma_1^2 > \\sigma_2^2$$\nThe test is performed at a significance level $\\alpha$. The test statistic used is the ratio of the sample variances, $F_{stat} = \\frac{S_1^2}{S_2^2}$, where $S_1^2 = \\frac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i - \\bar{X})^2$ and $S_2^2 = \\frac{1}{n_2-1}\\sum_{j=1}^{n_2}(Y_j - \\bar{Y})^2$ are the unbiased sample variances.\n\nYour task is to derive the power function of this test, denoted by $\\pi(\\lambda)$. The power function gives the probability of correctly rejecting the null hypothesis for a given state of nature. This function should be expressed in terms of the true variance ratio $\\lambda = \\sigma_1^2 / \\sigma_2^2$, the sample sizes $n_1$ and $n_2$, and the significance level $\\alpha$.\n\nFor your final expression, use the notation $\\mathcal{F}_{d_1, d_2}(x)$ to represent the cumulative distribution function (CDF) of an F-distribution with $d_1$ and $d_2$ degrees of freedom, evaluated at $x$. Use the notation $F_{d_1, d_2, \\alpha}$ to represent the upper $\\alpha$-quantile of this same distribution (i.e., the value $c$ such that $P(F > c) = \\alpha$ for a random variable $F \\sim F_{d_1, d_2}$).", "solution": "We use the exact sampling distributions implied by normality and independence. For the first process,\n$$(n_{1}-1)\\frac{S_{1}^{2}}{\\sigma_{1}^{2}} \\sim \\chi^{2}_{n_{1}-1},$$\nand for the second process,\n$$(n_{2}-1)\\frac{S_{2}^{2}}{\\sigma_{2}^{2}} \\sim \\chi^{2}_{n_{2}-1},$$\nwith the two chi-square variables independent. Define\n$$U = (n_{1}-1)\\frac{S_{1}^{2}}{\\sigma_{1}^{2}}, \\quad V = (n_{2}-1)\\frac{S_{2}^{2}}{\\sigma_{2}^{2}}, \\quad \\lambda = \\frac{\\sigma_{1}^{2}}{\\sigma_{2}^{2}}.$$\nThen the test statistic satisfies\n$$F_{\\text{stat}}=\\frac{S_{1}^{2}}{S_{2}^{2}}=\\lambda \\cdot \\frac{U/(n_{1}-1)}{V/(n_{2}-1)}.$$\nSince $U/(n_{1}-1)$ and $V/(n_{2}-1)$ are independent and the ratio has an $F$-distribution, it follows that\n$$\\frac{U/(n_{1}-1)}{V/(n_{2}-1)} \\sim F_{n_{1}-1,\\,n_{2}-1}, \\quad \\text{and hence} \\quad F_{\\text{stat}} \\stackrel{d}{=} \\lambda \\cdot F,$$\nwhere $F \\sim F_{n_{1}-1,\\,n_{2}-1}$. For the right-tailed test at significance level $\\alpha$, choose the critical value $c_{\\alpha}$ under $H_{0}$ so that\n$$\\Pr_{H_{0}}\\!\\big(F_{\\text{stat}} > c_{\\alpha}\\big)=\\alpha.$$\nUnder $H_{0}$, $\\lambda=1$ and $F_{\\text{stat}}\\sim F_{n_{1}-1,\\,n_{2}-1}$, so by the given notation,\n$$c_{\\alpha}=F_{n_{1}-1,\\,n_{2}-1,\\,\\alpha}.$$\nFor a true variance ratio $\\lambda>0$, the power is\n$$\\pi(\\lambda)=\\Pr\\!\\big(F_{\\text{stat}} > c_{\\alpha}\\,;\\,\\lambda\\big)=\\Pr\\!\\big(\\lambda F > c_{\\alpha}\\big)=\\Pr\\!\\big(F > c_{\\alpha}/\\lambda\\big).$$\nExpressed via the $F$-distribution CDF $\\mathcal{F}_{n_{1}-1,\\,n_{2}-1}(\\cdot)$, we obtain\n$$\\pi(\\lambda)=1-\\mathcal{F}_{n_{1}-1,\\,n_{2}-1}\\!\\left(\\frac{c_{\\alpha}}{\\lambda}\\right)\n=1-\\mathcal{F}_{n_{1}-1,\\,n_{2}-1}\\!\\left(\\frac{F_{n_{1}-1,\\,n_{2}-1,\\,\\alpha}}{\\lambda}\\right).$$\nThis function satisfies $\\pi(1)=\\alpha$, is increasing in $\\lambda$, and tends to $1$ as $\\lambda \\to \\infty$, as expected for a right-tailed test targeting larger $\\sigma_{1}^{2}$.", "answer": "$$\\boxed{1 - \\mathcal{F}_{n_{1}-1,\\,n_{2}-1}\\!\\left(\\frac{F_{n_{1}-1,\\,n_{2}-1,\\,\\alpha}}{\\lambda}\\right)}$$", "id": "1916926"}]}