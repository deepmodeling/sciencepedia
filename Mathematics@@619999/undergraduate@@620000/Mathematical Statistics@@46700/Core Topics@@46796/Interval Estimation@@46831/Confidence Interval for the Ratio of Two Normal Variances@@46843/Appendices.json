{"hands_on_practices": [{"introduction": "This first exercise provides foundational practice in applying the core statistical method for comparing two variances. By working through this problem, you will compute a confidence interval for the ratio of variances using sample data and critical values from the F-distribution. Mastering this fundamental calculation is the crucial first step toward confidently using and interpreting this important statistical tool. [@problem_id:1908225]", "problem": "A team of analysts is evaluating the performance consistency of two different financial risk models, labeled Model A and Model B. They run a series of simulations on each model to estimate the value-at-risk of a portfolio, and the primary metric for consistency is the variance of these estimates. It is assumed that the estimates from both models are independent and are drawn from normal distributions.\n\nA random sample of $n_A = 16$ simulations is run using Model A, yielding a sample variance of $s_A^2 = 4.80$ (in units of squared millions of dollars). A second independent random sample of $n_B = 11$ simulations is run using Model B, resulting in a sample variance of $s_B^2 = 2.10$ (in the same units).\n\nThe team wants to construct a 95% confidence interval for the ratio of the true population variances, $\\frac{\\sigma_A^2}{\\sigma_B^2}$, to compare the models' precision.\n\nYou are given the following critical values from the F-distribution, where the notation $F_{\\alpha, \\nu_1, \\nu_2}$ represents the critical value with an upper tail probability of $\\alpha$, for $\\nu_1$ numerator degrees of freedom and $\\nu_2$ denominator degrees of freedom:\n- $F_{0.025, 15, 10} = 3.522$\n- $F_{0.05, 15, 10} = 2.848$\n- $F_{0.025, 16, 11} = 3.379$\n- $F_{0.025, 10, 15} = 3.060$\n- $F_{0.05, 10, 15} = 2.544$\n\nCalculate the lower and upper bounds of the 95% confidence interval for the ratio $\\frac{\\sigma_A^2}{\\sigma_B^2}$. Report the lower bound and the upper bound, in that order, with each value rounded to three significant figures.", "solution": "We are comparing the variances of two independent normal populations using the ratio $R = \\frac{\\sigma_{A}^{2}}{\\sigma_{B}^{2}}$. For independent normal samples with sizes $n_{A}$ and $n_{B}$, the statistics\n$$\nX = \\frac{(n_{A}-1)S_{A}^{2}}{\\sigma_{A}^{2}} \\sim \\chi^{2}_{n_{A}-1}, \\quad Y = \\frac{(n_{B}-1)S_{B}^{2}}{\\sigma_{B}^{2}} \\sim \\chi^{2}_{n_{B}-1},\n$$\nand\n$$\n\\frac{X/(n_{A}-1)}{Y/(n_{B}-1)} = \\frac{(S_{A}^{2}/\\sigma_{A}^{2})}{(S_{B}^{2}/\\sigma_{B}^{2})} \\sim F_{n_{A}-1,\\; n_{B}-1}.\n$$\nSet $Q = \\frac{(S_{A}^{2}/\\sigma_{A}^{2})}{(S_{B}^{2}/\\sigma_{B}^{2})}$. Then $Q \\sim F_{\\nu_{1},\\nu_{2}}$ with $\\nu_{1} = n_{A}-1 = 15$ and $\\nu_{2} = n_{B}-1 = 10$. For a two-sided confidence interval with confidence $1-\\alpha = 0.95$ (so $\\alpha = 0.05$ and $\\alpha/2 = 0.025$), the central probability statement is\n$$\nP\\!\\left(q_{L} \\leq Q \\leq q_{U}\\right) = 1 - \\alpha,\n$$\nwhere $q_{U}$ is the upper critical value with upper-tail probability $\\alpha/2$, and $q_{L}$ is the lower critical value with lower-tail probability $\\alpha/2$. Using the problemâ€™s notation $F_{\\alpha,\\nu_{1},\\nu_{2}}$ for the upper-tail critical (so $P(F > F_{\\alpha,\\nu_{1},\\nu_{2}}) = \\alpha$), we have\n$$\nq_{U} = F_{0.025,\\,15,\\,10}, \\quad q_{L} = \\frac{1}{F_{0.025,\\,10,\\,15}},\n$$\nby the reciprocity property $F_{1-p;\\,\\nu_{1},\\nu_{2}} = \\frac{1}{F_{p;\\,\\nu_{2},\\nu_{1}}}$. Since\n$$\nQ = \\frac{(S_{A}^{2}/S_{B}^{2})}{(\\sigma_{A}^{2}/\\sigma_{B}^{2})},\n$$\nthe inequality $q_{L} \\leq Q \\leq q_{U}$ is equivalent to\n$$\n\\frac{S_{A}^{2}/S_{B}^{2}}{q_{U}} \\leq \\frac{\\sigma_{A}^{2}}{\\sigma_{B}^{2}} \\leq \\frac{S_{A}^{2}/S_{B}^{2}}{q_{L}}.\n$$\nThus the $95$ percent confidence interval for $\\frac{\\sigma_{A}^{2}}{\\sigma_{B}^{2}}$ is\n$$\n\\left[\\,\\frac{s_{A}^{2}/s_{B}^{2}}{F_{0.025,\\,15,\\,10}},\\; \\left(s_{A}^{2}/s_{B}^{2}\\right)\\,F_{0.025,\\,10,\\,15}\\,\\right].\n$$\nInsert the given values $s_{A}^{2} = 4.80$, $s_{B}^{2} = 2.10$, $F_{0.025,\\,15,\\,10} = 3.522$, and $F_{0.025,\\,10,\\,15} = 3.060$:\n$$\n\\frac{s_{A}^{2}}{s_{B}^{2}} = \\frac{4.80}{2.10} = \\frac{16}{7} \\approx 2.285714,\n$$\n$$\n\\text{Lower bound} = \\frac{16/7}{3.522} \\approx 0.64898 \\approx 0.649 \\text{ (three significant figures)},\n$$\n$$\n\\text{Upper bound} = \\left(\\frac{16}{7}\\right)\\,3.060 \\approx 6.99429 \\approx 6.99 \\text{ (three significant figures)}.\n$$\nTherefore, the $95$ percent confidence interval for $\\frac{\\sigma_{A}^{2}}{\\sigma_{B}^{2}}$ is approximately $\\left(0.649,\\; 6.99\\right)$, reported as lower bound then upper bound, each to three significant figures.", "answer": "$$\\boxed{\\begin{pmatrix}0.649 & 6.99\\end{pmatrix}}$$", "id": "1908225"}, {"introduction": "The F-test for comparing variances rests on the critical assumption that the two samples are statistically independent. This theoretical exercise challenges you to investigate what happens when this assumption is violated, using the common scenario of paired data. By deriving the covariance between the two sample variances, you will mathematically demonstrate why the standard method is inappropriate for dependent samples and gain a deeper appreciation for the assumptions underlying statistical tests. [@problem_id:1908233]", "problem": "In many experimental designs, researchers collect paired data. For example, in a clinical study, measurements might be taken on the same subject before and after a treatment. Let the pair of measurements for the $i$-th subject be denoted by $(X_i, Y_i)$. A common assumption is that these pairs are independent and identically distributed draws from a bivariate normal distribution.\n\nLet a random sample of $n$ such pairs, $(X_1, Y_1), (X_2, Y_2), \\dots, (X_n, Y_n)$, be drawn from a bivariate normal distribution with mean vector $(\\mu_X, \\mu_Y)$, positive variances $\\sigma_X^2$ and $\\sigma_Y^2$, and correlation coefficient $\\rho$. The unbiased sample variances for the $X$ and $Y$ measurements are given by:\n$$S_X^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$$\n$$S_Y^2 = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\bar{Y})^2$$\nwhere $\\bar{X}$ and $\\bar{Y}$ are the respective sample means.\n\nThe standard procedure for comparing variances of two independent normal populations relies on the statistical independence of their sample variances. To investigate the validity of this assumption in the context of paired data, consider the simplest non-trivial case with a sample of size $n=2$. Derive a closed-form analytic expression for the covariance between the two sample variances, $\\text{Cov}(S_X^2, S_Y^2)$, in terms of the population parameters $\\sigma_X^2$, $\\sigma_Y^2$, and $\\rho$.", "solution": "The goal is to compute the covariance $\\text{Cov}(S_X^2, S_Y^2)$ for a sample of size $n=2$.\n\nFirst, let's simplify the expressions for the sample variances $S_X^2$ and $S_Y^2$ when $n=2$.\nThe sample mean for $X$ is $\\bar{X} = \\frac{X_1 + X_2}{2}$.\nThe sample variance $S_X^2$ is:\n$$S_X^2 = \\frac{1}{2-1} \\left[ \\left(X_1 - \\frac{X_1+X_2}{2}\\right)^2 + \\left(X_2 - \\frac{X_1+X_2}{2}\\right)^2 \\right]$$\n$$S_X^2 = \\left( \\frac{X_1-X_2}{2} \\right)^2 + \\left( \\frac{X_2-X_1}{2} \\right)^2 = 2 \\left( \\frac{X_1-X_2}{2} \\right)^2 = \\frac{1}{2}(X_1 - X_2)^2$$\nSimilarly, for the $Y$ measurements:\n$$S_Y^2 = \\frac{1}{2}(Y_1 - Y_2)^2$$\n\nTo find the covariance between $S_X^2$ and $S_Y^2$, we define two new random variables:\n$U = X_1 - X_2$\n$V = Y_1 - Y_2$\nWith these definitions, we have $S_X^2 = \\frac{1}{2}U^2$ and $S_Y^2 = \\frac{1}{2}V^2$.\nThe covariance can now be written as:\n$$\\text{Cov}(S_X^2, S_Y^2) = \\text{Cov}\\left(\\frac{1}{2}U^2, \\frac{1}{2}V^2\\right) = \\frac{1}{4}\\text{Cov}(U^2, V^2)$$\nBy definition, $\\text{Cov}(U^2, V^2) = E[U^2 V^2] - E[U^2] E[V^2]$.\n\nNext, we characterize the joint distribution of $(U, V)$. Since $X_i$ and $Y_i$ are from a bivariate normal distribution, any linear combination of them will also be normally distributed. Thus, $(U, V)$ has a bivariate normal distribution. Let's find its parameters.\nThe means are:\n$E[U] = E[X_1 - X_2] = E[X_1] - E[X_2] = \\mu_X - \\mu_X = 0$\n$E[V] = E[Y_1 - Y_2] = E[Y_1] - E[Y_2] = \\mu_Y - \\mu_Y = 0$\n\nThe variances are:\n$\\text{Var}(U) = \\text{Var}(X_1 - X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) - 2\\text{Cov}(X_1, X_2)$. Since $(X_1, Y_1)$ and $(X_2, Y_2)$ are independent pairs, $X_1$ and $X_2$ are independent, so $\\text{Cov}(X_1, X_2) = 0$.\n$\\text{Var}(U) = \\sigma_X^2 + \\sigma_X^2 = 2\\sigma_X^2$.\nSimilarly, $\\text{Var}(V) = \\text{Var}(Y_1 - Y_2) = \\sigma_Y^2 + \\sigma_Y^2 = 2\\sigma_Y^2$.\n\nThe covariance between $U$ and $V$ is:\n$\\text{Cov}(U, V) = \\text{Cov}(X_1 - X_2, Y_1 - Y_2) = \\text{Cov}(X_1, Y_1) - \\text{Cov}(X_1, Y_2) - \\text{Cov}(X_2, Y_1) + \\text{Cov}(X_2, Y_2)$.\nDue to the independence of the pairs $(X_1, Y_1)$ and $(X_2, Y_2)$, we have $\\text{Cov}(X_1, Y_2) = 0$ and $\\text{Cov}(X_2, Y_1) = 0$.\nThe definition of the correlation coefficient is $\\rho = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}$, so $\\text{Cov}(X_i, Y_i) = \\rho \\sigma_X \\sigma_Y$.\nTherefore, $\\text{Cov}(U, V) = \\rho \\sigma_X \\sigma_Y - 0 - 0 + \\rho \\sigma_X \\sigma_Y = 2\\rho \\sigma_X \\sigma_Y$.\n\nNow we can compute the terms for $\\text{Cov}(U^2, V^2)$.\n$E[U^2] = \\text{Var}(U) + (E[U])^2 = 2\\sigma_X^2 + 0^2 = 2\\sigma_X^2$.\n$E[V^2] = \\text{Var}(V) + (E[V])^2 = 2\\sigma_Y^2 + 0^2 = 2\\sigma_Y^2$.\n\nTo find the mixed fourth moment $E[U^2 V^2]$, we use Isserlis' Theorem for zero-mean multivariate normal random variables, which states for variables $Z_1, Z_2, Z_3, Z_4$:\n$E[Z_1 Z_2 Z_3 Z_4] = E[Z_1 Z_2]E[Z_3 Z_4] + E[Z_1 Z_3]E[Z_2 Z_4] + E[Z_1 Z_4]E[Z_2 Z_3]$.\nApplying this to $E[U^2 V^2] = E[U U V V]$:\n$E[U^2 V^2] = E[UU]E[VV] + E[UV]E[UV] + E[UV]E[UV]$\n$E[U^2 V^2] = E[U^2]E[V^2] + 2(E[UV])^2$.\nWe know $E[UV] = \\text{Cov}(U, V)$ since the means are zero.\n$E[U^2 V^2] = (2\\sigma_X^2)(2\\sigma_Y^2) + 2(2\\rho \\sigma_X \\sigma_Y)^2$\n$E[U^2 V^2] = 4\\sigma_X^2\\sigma_Y^2 + 2(4\\rho^2 \\sigma_X^2 \\sigma_Y^2) = 4\\sigma_X^2\\sigma_Y^2(1 + 2\\rho^2)$.\n\nNow, we can compute $\\text{Cov}(U^2, V^2)$:\n$\\text{Cov}(U^2, V^2) = E[U^2 V^2] - E[U^2]E[V^2]$\n$\\text{Cov}(U^2, V^2) = 4\\sigma_X^2\\sigma_Y^2(1 + 2\\rho^2) - (2\\sigma_X^2)(2\\sigma_Y^2)$\n$\\text{Cov}(U^2, V^2) = 4\\sigma_X^2\\sigma_Y^2 + 8\\rho^2\\sigma_X^2\\sigma_Y^2 - 4\\sigma_X^2\\sigma_Y^2 = 8\\rho^2\\sigma_X^2\\sigma_Y^2$.\n\nFinally, we substitute this back into the expression for $\\text{Cov}(S_X^2, S_Y^2)$:\n$$\\text{Cov}(S_X^2, S_Y^2) = \\frac{1}{4}\\text{Cov}(U^2, V^2) = \\frac{1}{4}(8\\rho^2\\sigma_X^2\\sigma_Y^2) = 2\\rho^2\\sigma_X^2\\sigma_Y^2$$\n\nThis result shows that if the correlation $\\rho$ between the paired measurements is non-zero, then the sample variances $S_X^2$ and $S_Y^2$ are not independent, as their covariance is non-zero. This violates a fundamental assumption required for using the standard F-distribution to construct a confidence interval for the ratio of the variances.", "answer": "$$\\boxed{2\\rho^{2}\\sigma_{X}^{2}\\sigma_{Y}^{2}}$$", "id": "1908233"}, {"introduction": "Beyond independence, the validity of the confidence interval for the ratio of variances also hinges on the assumption of normality. This practice uses a hypothetical simulation study to explore the robustness of the standard method when this assumption is not met. By analyzing the performance of the classical F-based interval against a non-parametric alternative on heavy-tailed data, you will understand the practical implications of this assumption and why it's so important to check. [@problem_id:1908224]", "problem": "A data scientist is evaluating the performance of two different methods for constructing a 95% confidence interval for the ratio of two population variances, $\\theta = \\sigma_1^2 / \\sigma_2^2$.\n\nMethod A is the classical method derived under the assumption that the two populations are normally distributed. The confidence interval is based on the F-distribution.\nMethod B is a non-parametric percentile bootstrap method, which does not assume a specific distribution for the populations.\n\nThe scientist suspects that the underlying populations for her data are not normal, but are symmetric with heavier tails. To investigate the robustness of the two methods, she conducts a simulation study. In the simulation, she draws samples from two independent populations whose data follows scaled Student's t-distributions, since they are known to have heavy tails.\n\nThe simulation protocol is as follows:\n1.  Data for the first sample is drawn from a population with a true variance of $\\sigma_1^2$.\n2.  Data for the second sample is drawn from an independent population with a true variance of $\\sigma_2^2$.\n3.  The sample sizes are $n_1 = 20$ and $n_2 = 30$.\n4.  The true value of the ratio of variances is set to $\\theta = 1.5$.\n5.  Steps 1-3 are repeated $N = 50,000$ times. For each repetition, a 95% confidence interval for $\\theta$ is constructed using both Method A and Method B.\n\nAfter running the simulation, the scientist records the number of times each type of interval successfully \"covered\" (i.e., contained) the true value of the ratio, $\\theta = 1.5$.\n\nThe results are:\n- For Method A (F-distribution): The interval covered the true value in 43,155 out of the 50,000 trials.\n- For Method B (percentile bootstrap): The interval covered the true value in 47,420 out of the 50,000 trials.\n\nThe *coverage error* for a method is defined as the absolute difference between the method's estimated coverage probability and the nominal confidence level (95%). Calculate the absolute difference between the coverage error of Method A and the coverage error of Method B. Express your final answer as a decimal rounded to three significant figures.", "solution": "Let the estimated coverage probabilities be $\\hat{p}_{A} = \\frac{43155}{50000}$ and $\\hat{p}_{B} = \\frac{47420}{50000}$, and let the nominal level be $p_{0} = 0.95 = \\frac{19}{20}$. The coverage errors are defined as $E_{A} = |\\hat{p}_{A} - p_{0}|$ and $E_{B} = |\\hat{p}_{B} - p_{0}|$. Compute each term exactly:\n$$\\hat{p}_{A} = \\frac{43155}{50000} = \\frac{8631}{10000}, \\quad \\hat{p}_{B} = \\frac{47420}{50000} = \\frac{2371}{2500}.$$\nThen\n$$E_{A} = \\left|\\frac{8631}{10000} - \\frac{19}{20}\\right| = \\left|\\frac{8631}{10000} - \\frac{9500}{10000}\\right| = \\frac{869}{10000},$$\n$$E_{B} = \\left|\\frac{2371}{2500} - \\frac{19}{20}\\right| = \\left|\\frac{2371}{2500} - \\frac{2375}{2500}\\right| = \\frac{4}{2500} = \\frac{16}{10000}.$$\nThe absolute difference between the coverage errors is\n$$|E_{A} - E_{B}| = \\left|\\frac{869}{10000} - \\frac{16}{10000}\\right| = \\frac{853}{10000} = 0.0853.$$\nRounded to three significant figures, this remains $0.0853$.", "answer": "$$\\boxed{0.0853}$$", "id": "1908224"}]}