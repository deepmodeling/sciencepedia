{"hands_on_practices": [{"introduction": "While a high coefficient of determination, $R^2$, seems to indicate a good model fit, it doesn't tell the whole story. A critical step in model diagnostics is visually inspecting the plot of residuals versus fitted values to check for systematic patterns. This practice [@problem_id:1936332] illustrates a classic scenario where a high $R^2$ value masks a fundamental misspecification of the model's functional form, emphasizing that statistical significance must be paired with diagnostic checking.", "problem": "A materials scientist is studying the relationship between the operating temperature of a new type of battery and its effective lifespan. They collect data on 50 batteries, recording the constant operating temperature ($x$, in degrees Celsius) and the lifespan ($y$, in thousands of hours).\n\nThe scientist performs a simple linear regression analysis and obtains the following fitted regression line:\n$$ \\hat{y} = 45.2 - 0.3x $$\nThe coefficient of determination for this model is calculated to be $R^2 = 0.85$.\n\nUpon examining the diagnostic plots, the scientist observes the plot of residuals versus the fitted values ($\\hat{y}$). The points on this residual plot are not scattered randomly around the horizontal line at zero. Instead, they form a clear, distinct, U-shaped (parabolic) pattern, with negative residuals for intermediate fitted values and positive residuals for low and high fitted values.\n\nBased on this information, which of the following is the most accurate conclusion?\n\nA. The high $R^2$ value of 0.85 indicates that the linear model is a very good fit for the data, and the pattern in the residuals can be ignored as it is likely due to a few outliers.\n\nB. The clear pattern in the residual plot indicates a fundamental problem with the model, so the high $R^2$ value must be the result of a calculation error.\n\nC. The model explains a large proportion of the variance in lifespan, but the U-shaped pattern in the residuals suggests that the underlying relationship between temperature and lifespan is non-linear, making the simple linear model inappropriate.\n\nD. The U-shaped pattern in the residual plot is a classic sign of multicollinearity, which means the predictor variable is correlated with other unobserved variables.\n\nE. The U-shaped pattern in the residual plot indicates that the variance of the errors is not constant (heteroscedasticity), and the data should be transformed using a logarithm.", "solution": "In a simple linear regression, the assumed model is $y=\\beta_{0}+\\beta_{1}x+\\varepsilon$ with the standard assumptions that $E[\\varepsilon \\mid x]=0$ and $\\operatorname{Var}(\\varepsilon \\mid x)=\\sigma^{2}$, and that the conditional mean $E[y \\mid x]$ is linear in $x$. The fitted values are $\\hat{y}_{i}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{i}$ and the residuals are $e_{i}=y_{i}-\\hat{y}_{i}$. If the model is correctly specified, a plot of $e_{i}$ versus $\\hat{y}_{i}$ should show no systematic pattern; specifically, we expect $E[e \\mid \\hat{y}]=0$ across the range of fitted values.\n\nA clear U-shaped (parabolic) pattern in the residuals versus fitted plot indicates that $E[e \\mid \\hat{y}]$ is not zero uniformly and varies systematically with $\\hat{y}$, which is diagnostic of a misspecified mean structure, typically a nonlinear relationship between $y$ and $x$. A common cause is that the true relationship includes curvature, for example $E[y \\mid x]=\\beta_{0}+\\beta_{1}x+\\beta_{2}x^{2}$ with $\\beta_{2}\\neq 0$. If one fits only the linear term, the omitted curvature manifests as a U-shaped pattern in residuals.\n\nThe coefficient of determination $R^{2}$ measures the proportion of variance explained by the fitted linear function and can be large even when the functional form is incorrect; therefore, a high $R^{2}$ does not validate the linearity assumption or the model adequacy. Hence, statement A is incorrect.\n\nThere is no reason to infer a calculation error for $R^{2}$ from the presence of curvature in residuals; both a high $R^{2}$ and a misspecified mean function can coexist. Hence, statement B is incorrect.\n\nMulticollinearity concerns correlation among multiple predictors and is not applicable in a simple linear regression with a single predictor. A U-shaped residual pattern is not a sign of multicollinearity. Hence, statement D is incorrect.\n\nHeteroscedasticity refers to $\\operatorname{Var}(\\varepsilon \\mid x)$ changing with $x$ and typically appears as a funnel or fan-shaped spread in residuals, not as a systematic sign change forming a U-shape. While transformations can sometimes address both nonlinearity and heteroscedasticity, the primary diagnostic here is nonlinearity in the mean, not non-constant variance. Hence, statement E is not the most accurate interpretation.\n\nTherefore, the correct conclusion is that the model explains a large proportion of variance but violates the linearity assumption; a nonlinear model (e.g., including $x^{2}$ or another appropriate transformation) is more appropriate.", "answer": "$$\\boxed{C}$$", "id": "1936332"}, {"introduction": "One of the core assumptions of linear regression is the independence of error terms, a condition often violated when working with time-ordered data. The Durbin-Watson test provides a formal way to detect such a violation, known as autocorrelation, where the error at one time point is correlated with errors at previous time points. This exercise [@problem_id:1936355] challenges you to interpret the Durbin-Watson statistic in a practical context and understand its implications for model validity.", "problem": "An operations analyst is tasked with modeling a company's monthly widget production volume. They propose a simple linear regression model to predict the current month's production volume, $P_t$, based on the previous month's raw material inventory, $I_{t-1}$. The model takes the form $P_t = \\beta_0 + \\beta_1 I_{t-1} + \\epsilon_t$, where $t$ indexes the month and $\\epsilon_t$ represents the error term. After fitting the model to 60 consecutive months of data, the analyst examines the properties of the residuals, $e_t$, to assess the model's validity. They compute the Durbin-Watson statistic for the sequence of residuals and obtain a value of $3.96$.\n\nBased on this result, which of the following is the most likely conclusion regarding the model's error terms?\n\nA. The error terms exhibit strong positive first-order autocorrelation.\n\nB. The error terms are not autocorrelated.\n\nC. The model suffers from significant multicollinearity between the predictors.\n\nD. The error terms exhibit strong negative first-order autocorrelation.\n\nE. The variance of the error terms is not constant (heteroscedasticity).", "solution": "We begin with the given linear regression model for monthly production:\n$$\nP_{t}=\\beta_{0}+\\beta_{1} I_{t-1}+\\epsilon_{t},\n$$\nwhere $t$ indexes time in months, and $\\epsilon_{t}$ are the error terms. Let $e_{t}$ denote the OLS residuals from fitting the model on $n=60$ months.\n\nTo assess first-order autocorrelation in the residuals, the Durbin-Watson statistic is computed as\n$$\nd=\\frac{\\sum_{t=2}^{n}\\left(e_{t}-e_{t-1}\\right)^{2}}{\\sum_{t=1}^{n} e_{t}^{2}}.\n$$\nIts range is $0 \\leq d \\leq 4$, with the following interpretations:\n- $d \\approx 2$ suggests no first-order autocorrelation.\n- $d < 2$ suggests positive first-order autocorrelation, with $d$ near $0$ indicating strong positive autocorrelation.\n- $d > 2$ suggests negative first-order autocorrelation, with $d$ near $4$ indicating strong negative autocorrelation.\n\nThere is also an approximate relationship between $d$ and the first-order autocorrelation coefficient $\\rho_{1}$ of the residuals:\n$$\nd \\approx 2\\left(1-\\rho_{1}\\right).\n$$\nSolving for $\\rho_{1}$ gives\n$$\n\\rho_{1} \\approx 1-\\frac{d}{2}.\n$$\nWith the reported value $d=3.96$,\n$$\n\\rho_{1} \\approx 1-\\frac{3.96}{2}=1-1.98=-0.98,\n$$\nwhich indicates very strong negative first-order autocorrelation.\n\nTherefore, the most likely conclusion is that the error terms exhibit strong negative first-order autocorrelation.\n\nTo rule out other options:\n- Strong positive autocorrelation would correspond to $d$ close to $0$, not $3.96$, so A is inconsistent.\n- No autocorrelation would correspond to $d$ near $2$, not $3.96$, so B is inconsistent.\n- The Durbin-Watson statistic does not test for multicollinearity among predictors, so C is irrelevant here.\n- Heteroscedasticity is not what the Durbin-Watson statistic detects; it targets serial correlation, so E is irrelevant.\n\nThus, the correct choice is D.", "answer": "$$\\boxed{D}$$", "id": "1936355"}, {"introduction": "Not all observations contribute equally to a regression model; some points can disproportionately affect the estimated coefficients and overall model fit. Cook's Distance is a powerful diagnostic that quantifies the influence of a single data point by combining its leverage (how far its predictor value is from others) and its residual size. In this hands-on calculation [@problem_id:1936315], you will compute Cook's Distance for a high-leverage point to see firsthand how statistical influence is formally measured.", "problem": "A materials scientist is studying a new chemical vapor deposition process to create ultra-thin semiconductor films. The thickness of the film, $Y$ (in nanometers, nm), is hypothesized to have a linear relationship with the deposition duration, $X$ (in seconds, s). A simple linear regression model of the form $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$ is fitted to the data from $n=30$ experimental runs. The model has $p=2$ estimated parameters (intercept $\\beta_0$ and slope $\\beta_1$).\n\nFrom the regression analysis of all 30 data points, the following summary statistics are obtained:\n- The mean deposition duration is $\\bar{x} = 55.0$ s.\n- The sum of squared deviations for the deposition duration is $\\sum_{j=1}^{30} (x_j - \\bar{x})^2 = 25000 \\text{ s}^2$.\n- The Mean Squared Error (MSE) of the model is $\\hat{\\sigma}^2 = 40.0 \\text{ nm}^2$.\n\nThe scientist is particularly interested in one specific experimental run, the $k$-th observation, which was conducted under unusual conditions. For this data point:\n- The deposition duration was $x_k = 155.0$ s.\n- The raw residual (the difference between the observed and predicted thickness) was $e_k = y_k - \\hat{y}_k = 5.0$ nm.\n\nTo assess the influence of this single observation on the overall regression model, you are asked to calculate its Cook's Distance, $D_k$. The necessary formulas are provided below:\n- The leverage of the $i$-th observation, $h_{ii}$, is given by $h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2}$.\n- The internally studentized residual of the $i$-th observation, $r_i$, is given by $r_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}$.\n- Cook's Distance for the $i$-th observation, $D_i$, is related to these quantities by $D_i = \\frac{r_i^2}{p} \\frac{h_{ii}}{1-h_{ii}}$.\n\nCalculate the value of Cook's Distance, $D_k$, for this specific experimental run. Round your final answer to three significant figures.", "solution": "We are given a simple linear regression with $n=30$ observations and $p=2$ parameters. For the $k$-th observation, we compute its leverage, internally studentized residual, and then Cook’s Distance using the provided formulas.\n\nFirst, compute the leverage $h_{kk}$:\n$$\nh_{kk} = \\frac{1}{n} + \\frac{(x_{k} - \\bar{x})^{2}}{\\sum_{j=1}^{n} (x_{j} - \\bar{x})^{2}} = \\frac{1}{30} + \\frac{(155.0 - 55.0)^{2}}{25000} = \\frac{1}{30} + \\frac{10000}{25000} = \\frac{1}{30} + \\frac{2}{5} = \\frac{13}{30}.\n$$\n\nNext, the Mean Squared Error is $\\hat{\\sigma}^{2} = 40.0$, so $\\hat{\\sigma} = \\sqrt{40}$. The internally studentized residual is\n$$\nr_{k} = \\frac{e_{k}}{\\hat{\\sigma}\\sqrt{1 - h_{kk}}} = \\frac{5.0}{\\sqrt{40}\\sqrt{1 - \\frac{13}{30}}} = \\frac{5.0}{\\sqrt{40}\\sqrt{\\frac{17}{30}}}.\n$$\nIt is convenient to compute $r_{k}^{2}$:\n$$\nr_{k}^{2} = \\frac{e_{k}^{2}}{\\hat{\\sigma}^{2}(1 - h_{kk})} = \\frac{25}{40 \\cdot \\frac{17}{30}} = \\frac{25 \\cdot 30}{40 \\cdot 17} = \\frac{75}{68}.\n$$\n\nNow compute the factor $\\frac{h_{kk}}{1 - h_{kk}}$:\n$$\n\\frac{h_{kk}}{1 - h_{kk}} = \\frac{\\frac{13}{30}}{\\frac{17}{30}} = \\frac{13}{17}.\n$$\n\nCook’s Distance for this observation is\n$$\nD_{k} = \\frac{r_{k}^{2}}{p}\\cdot \\frac{h_{kk}}{1 - h_{kk}} = \\frac{\\frac{75}{68}}{2} \\cdot \\frac{13}{17} = \\frac{75}{136} \\cdot \\frac{13}{17} = \\frac{975}{2312}.\n$$\nNumerically, $\\frac{975}{2312} \\approx 0.4217\\ldots$, which rounds to three significant figures as $0.422$.", "answer": "$$\\boxed{0.422}$$", "id": "1936315"}]}