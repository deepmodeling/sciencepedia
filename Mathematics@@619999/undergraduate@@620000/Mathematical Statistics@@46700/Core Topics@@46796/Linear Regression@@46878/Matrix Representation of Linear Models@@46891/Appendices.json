{"hands_on_practices": [{"introduction": "The journey into linear modeling often begins with understanding how to translate a set of observations into a concise matrix format. This fundamental practice focuses on constructing the design matrix $\\mathbf{X}$ and computing the matrix product $\\mathbf{X}^T \\mathbf{X}$, which is a cornerstone of the Ordinary Least Squares (OLS) estimation procedure. By working through this exercise [@problem_id:1933331], you will gain hands-on experience in setting up the core components of the normal equations, $(\\mathbf{X}^T \\mathbf{X})\\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{y}$, which form the basis for estimating model parameters.", "problem": "Consider a simple linear regression model used to analyze the relationship between a predictor variable $x$ and a response variable $y$. The model is given by $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$ for $i=1, 2, 3, 4$. An experiment is conducted, yielding four observations for the predictor variable: $x_1 = 1$, $x_2 = 2$, $x_3 = 3$, and $x_4 = 4$.\n\nThe model can be written in matrix form as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, where $\\mathbf{X}$ is the design matrix.\n\nDetermine the matrix product $\\mathbf{X}^T \\mathbf{X}$. Present your answer as a $1 \\times 4$ row matrix, listing the elements of the $2 \\times 2$ product matrix in row-major order (i.e., the first row's elements followed by the second row's elements).", "solution": "The simple linear regression with intercept has the design matrix with a column of ones and a column of predictor values. Thus\n$$\n\\mathbf{X}=\\begin{pmatrix}\n1 & x_{1}\\\\\n1 & x_{2}\\\\\n1 & x_{3}\\\\\n1 & x_{4}\n\\end{pmatrix}.\n$$\nWith the given values $x_{1}=1$, $x_{2}=2$, $x_{3}=3$, $x_{4}=4$, this becomes\n$$\n\\mathbf{X}=\\begin{pmatrix}\n1 & 1\\\\\n1 & 2\\\\\n1 & 3\\\\\n1 & 4\n\\end{pmatrix}.\n$$\nBy definition of matrix multiplication, the entries of $\\mathbf{X}^{T}\\mathbf{X}$ are\n$$\n(\\mathbf{X}^{T}\\mathbf{X})_{11}=\\sum_{i=1}^{4}1\\cdot 1=\\sum_{i=1}^{4}1,\\quad\n(\\mathbf{X}^{T}\\mathbf{X})_{12}=\\sum_{i=1}^{4}1\\cdot x_{i}=\\sum_{i=1}^{4}x_{i},\n$$\n$$\n(\\mathbf{X}^{T}\\mathbf{X})_{21}=\\sum_{i=1}^{4}x_{i}\\cdot 1=\\sum_{i=1}^{4}x_{i},\\quad\n(\\mathbf{X}^{T}\\mathbf{X})_{22}=\\sum_{i=1}^{4}x_{i}\\cdot x_{i}=\\sum_{i=1}^{4}x_{i}^{2}.\n$$\nCompute the sums:\n$$\n\\sum_{i=1}^{4}1=4,\\quad \\sum_{i=1}^{4}x_{i}=1+2+3+4=10,\\quad \\sum_{i=1}^{4}x_{i}^{2}=1^{2}+2^{2}+3^{2}+4^{2}=30.\n$$\nTherefore\n$$\n\\mathbf{X}^{T}\\mathbf{X}=\\begin{pmatrix}\n4 & 10\\\\\n10 & 30\n\\end{pmatrix},\n$$\nwhich in row-major order as a $1\\times 4$ row matrix is\n$$\n\\begin{pmatrix}\n4 & 10 & 10 & 30\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 4 & 10 & 10 & 30 \\end{pmatrix}}$$", "id": "1933331"}, {"introduction": "Once the normal equations are established, the next step is to solve for the coefficient vector $\\boldsymbol{\\hat{\\beta}}$. While the theoretical solution involves inverting the $\\mathbf{X}^T \\mathbf{X}$ matrix, this can be computationally intensive and numerically unstable. This practice [@problem_id:1933337] introduces a more elegant and robust method: QR decomposition. You will see how factoring the design matrix $\\mathbf{X}$ into an orthogonal matrix $\\mathbf{Q}$ and an upper triangular matrix $\\mathbf{R}$ transforms the problem into a simple system of equations that can be solved efficiently using back substitution, bridging the gap between statistical theory and practical numerical linear algebra.", "problem": "A materials scientist is investigating the performance of a newly developed thermoelectric generator. The electrical power output, $y$, is modeled as a linear function of two operational parameters: the temperature gradient across the device, $x_1$, and the thermal conductivity of a new composite material used in its construction, $x_2$.\n\nThe relationship is described by the multiple linear regression model:\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$$\nwhere $\\beta_0, \\beta_1, \\beta_2$ are the model coefficients to be estimated, and $\\epsilon$ is the random error term.\n\nFour experiments are conducted, yielding the following data for the response vector $\\mathbf{y}$ and the design matrix $\\mathbf{X}$:\n$$\n\\mathbf{y} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\\\ 6 \\end{pmatrix}, \\quad\n\\mathbf{X} = \\begin{pmatrix} 1 & 1 & 2 \\\\ 1 & 2 & 3 \\\\ 1 & 3 & 5 \\\\ 1 & 4 & 6 \\end{pmatrix}\n$$\nFor numerical stability and efficiency, the design matrix $\\mathbf{X}$ has been decomposed into a product $\\mathbf{X}=\\mathbf{Q}\\mathbf{R}$, where $\\mathbf{Q}$ is a matrix with orthonormal columns and $\\mathbf{R}$ is an upper triangular matrix. These matrices are given as:\n$$\n\\mathbf{Q} = \\begin{pmatrix}\n\\frac{1}{2} & -\\frac{3}{2\\sqrt{5}} & \\frac{1}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & -\\frac{1}{2\\sqrt{5}} & -\\frac{3}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & \\frac{1}{2\\sqrt{5}} & \\frac{3}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & \\frac{3}{2\\sqrt{5}} & -\\frac{1}{2\\sqrt{5}}\n\\end{pmatrix}, \\quad\n\\mathbf{R} = \\begin{pmatrix}\n2 & 5 & 8 \\\\\n0 & \\sqrt{5} & \\frac{7}{\\sqrt{5}} \\\\\n0 & 0 & \\frac{1}{\\sqrt{5}}\n\\end{pmatrix}\n$$\n\nDetermine the Ordinary Least Squares (OLS) estimate of the coefficient vector, $\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)^T$. Express each component of the vector $\\hat{\\boldsymbol{\\beta}}$ as an exact decimal number.", "solution": "We use the QR approach to ordinary least squares. With $\\mathbf{X}=\\mathbf{Q}\\mathbf{R}$ where $\\mathbf{Q}$ has orthonormal columns ($\\mathbf{Q}^{T}\\mathbf{Q}=\\mathbf{I}$) and $\\mathbf{R}$ is upper triangular, the OLS estimate $\\hat{\\boldsymbol{\\beta}}$ solves\n$$\n\\min_{\\boldsymbol{\\beta}}\\|\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\|_{2}=\\min_{\\boldsymbol{\\beta}}\\|\\mathbf{y}-\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\beta}\\|_{2}.\n$$\nLeft-multiplying by $\\mathbf{Q}^{T}$ and using $\\mathbf{Q}^{T}\\mathbf{Q}=\\mathbf{I}$ gives the triangular system\n$$\n\\mathbf{R}\\hat{\\boldsymbol{\\beta}}=\\mathbf{Q}^{T}\\mathbf{y}.\n$$\nThus compute $\\mathbf{z}=\\mathbf{Q}^{T}\\mathbf{y}$ and then solve $\\mathbf{R}\\hat{\\boldsymbol{\\beta}}=\\mathbf{z}$ by back substitution.\n\nGiven\n$$\n\\mathbf{Q}=\\begin{pmatrix}\n\\frac{1}{2} & -\\frac{3}{2\\sqrt{5}} & \\frac{1}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & -\\frac{1}{2\\sqrt{5}} & -\\frac{3}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & \\frac{1}{2\\sqrt{5}} & \\frac{3}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & \\frac{3}{2\\sqrt{5}} & -\\frac{1}{2\\sqrt{5}}\n\\end{pmatrix},\\quad\n\\mathbf{y}=\\begin{pmatrix}1\\\\1\\\\4\\\\6\\end{pmatrix},\n$$\nthe components of $\\mathbf{z}=\\mathbf{Q}^{T}\\mathbf{y}$ are\n$$\nz_{1}=\\mathbf{q}_{1}^{T}\\mathbf{y}=\\frac{1}{2}(1+1+4+6)=6,\n$$\n$$\nz_{2}=\\mathbf{q}_{2}^{T}\\mathbf{y}=\\frac{1}{2\\sqrt{5}}\\big((-3)\\cdot 1+(-1)\\cdot 1+1\\cdot 4+3\\cdot 6\\big)=\\frac{18}{2\\sqrt{5}}=\\frac{9}{\\sqrt{5}},\n$$\n$$\nz_{3}=\\mathbf{q}_{3}^{T}\\mathbf{y}=\\frac{1}{2\\sqrt{5}}\\big(1\\cdot 1+(-3)\\cdot 1+3\\cdot 4+(-1)\\cdot 6\\big)=\\frac{4}{2\\sqrt{5}}=\\frac{2}{\\sqrt{5}}.\n$$\nWith\n$$\n\\mathbf{R}=\\begin{pmatrix}\n2 & 5 & 8 \\\\\n0 & \\sqrt{5} & \\frac{7}{\\sqrt{5}} \\\\\n0 & 0 & \\frac{1}{\\sqrt{5}}\n\\end{pmatrix},\n$$\nwe solve $\\mathbf{R}\\hat{\\boldsymbol{\\beta}}=\\mathbf{z}$:\n- Third equation: $\\frac{1}{\\sqrt{5}}\\hat{\\beta}_{2}=\\frac{2}{\\sqrt{5}} \\implies \\hat{\\beta}_{2}=2$.\n- Second equation: $\\sqrt{5}\\hat{\\beta}_{1}+\\frac{7}{\\sqrt{5}}\\hat{\\beta}_{2}=\\frac{9}{\\sqrt{5}}$. Multiplying both sides by $\\sqrt{5}$ gives $5\\hat{\\beta}_{1}+7\\hat{\\beta}_{2}=9$. Substituting $\\hat{\\beta}_{2}=2$ yields $5\\hat{\\beta}_{1}+14=9 \\implies \\hat{\\beta}_{1}=-1$.\n- First equation: $2\\hat{\\beta}_{0}+5\\hat{\\beta}_{1}+8\\hat{\\beta}_{2}=6$. Substituting $\\hat{\\beta}_{1}=-1$ and $\\hat{\\beta}_{2}=2$ gives $2\\hat{\\beta}_{0}-5+16=6 \\implies 2\\hat{\\beta}_{0}+11=6 \\implies \\hat{\\beta}_{0}=-2.5$.\n\nTherefore,\n$$\n\\hat{\\boldsymbol{\\beta}}=\\begin{pmatrix}-2.5 \\\\ -1 \\\\ 2\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}-2.5 \\\\ -1 \\\\ 2\\end{pmatrix}}$$", "id": "1933337"}, {"introduction": "The matrix representation of linear models does more than just simplify computation; it elegantly reveals fundamental properties of the model's estimates. This exercise [@problem_id:1933375] guides you through a matrix-based proof to show that the sum of the residuals, $\\mathbf{e} = \\mathbf{y} - \\mathbf{\\hat{y}}$, is always zero for any linear model that includes an intercept term. Mastering this derivation deepens your understanding of the geometry of least squares and the crucial role that the normal equations play in defining the relationship between the data and the model's predictions.", "problem": "In the study of a simple linear regression model, we express the relationship between a response vector $\\mathbf{y}$ and a predictor vector $\\mathbf{x}$ using matrix notation. Consider a dataset with $n$ observations. The model is given by $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, where:\n- $\\mathbf{y}$ is the $n \\times 1$ vector of observed responses.\n- $\\mathbf{X}$ is the $n \\times 2$ design matrix. For a simple linear regression model with an intercept, $\\mathbf{X}$ is constructed as $[\\mathbf{1} \\quad \\mathbf{x}]$, where $\\mathbf{1}$ is an $n \\times 1$ column vector of ones and $\\mathbf{x}$ is the $n \\times 1$ column vector of predictor variable observations.\n- $\\boldsymbol{\\beta} = [\\beta_0 \\quad \\beta_1]^T$ is the $2 \\times 1$ vector of model coefficients (intercept and slope).\n- $\\boldsymbol{\\epsilon}$ is the $n \\times 1$ vector of random errors.\n\nThe coefficients are estimated using Ordinary Least Squares (OLS), which yields the estimator $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$. The vector of fitted values is $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$, and the vector of residuals is defined as $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}}$.\n\nUsing the definitions provided, determine the value of the scalar quantity $\\mathbf{1}^T \\mathbf{e}$.\n\nA. $0$\n\nB. $n$\n\nC. $\\mathbf{1}^T\\mathbf{y}$\n\nD. $\\det(\\mathbf{X}^T\\mathbf{X})$", "solution": "The problem asks for the value of the scalar product $\\mathbf{1}^T \\mathbf{e}$. We start from the definition of the Ordinary Least Squares (OLS) estimator $\\hat{\\boldsymbol{\\beta}}$. The OLS estimator is derived by minimizing the sum of squared residuals, $S(\\boldsymbol{\\beta}) = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$.\n\nTo find the minimum, we take the derivative of $S(\\boldsymbol{\\beta})$ with respect to the vector $\\boldsymbol{\\beta}$ and set it to zero.\n$$S(\\boldsymbol{\\beta}) = \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}$$\nSince $\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y}$ is a scalar, it is equal to its transpose, $(\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y})^T = \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta}$. Thus, we can write:\n$$S(\\boldsymbol{\\beta}) = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}$$\nThe derivative with respect to $\\boldsymbol{\\beta}$ is:\n$$\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}$$\nSetting the derivative to zero to find the estimator $\\hat{\\boldsymbol{\\beta}}$ gives:\n$$-2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}$$\n$$\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}$$\nThis set of equations is known as the normal equations. We can rearrange the normal equations as:\n$$\\mathbf{X}^T\\mathbf{y} - \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}$$\nFactoring out $\\mathbf{X}^T$ gives:\n$$\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$$\nBy definition, the vector of residuals is $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}}$. Since the vector of fitted values is $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$, we can write the residual vector as $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$.\nSubstituting this into the rearranged normal equations, we get:\n$$\\mathbf{X}^T\\mathbf{e} = \\mathbf{0}$$\nThe problem specifies that this is a simple linear regression with an intercept. The design matrix $\\mathbf{X}$ is given by $\\mathbf{X} = [\\mathbf{1} \\quad \\mathbf{x}]$, where $\\mathbf{1}$ is an $n \\times 1$ vector of ones and $\\mathbf{x}$ is an $n \\times 1$ vector of the predictor values.\n\nThe transpose of the design matrix is $\\mathbf{X}^T = \\begin{pmatrix} \\mathbf{1}^T \\\\ \\mathbf{x}^T \\end{pmatrix}$.\nNow, we can write out the equation $\\mathbf{X}^T\\mathbf{e} = \\mathbf{0}$ explicitly:\n$$\\begin{pmatrix} \\mathbf{1}^T \\\\ \\mathbf{x}^T \\end{pmatrix} \\mathbf{e} = \\begin{pmatrix} \\mathbf{1}^T\\mathbf{e} \\\\ \\mathbf{x}^T\\mathbf{e} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThis matrix equation represents a system of two separate equations:\n1. $\\mathbf{1}^T\\mathbf{e} = 0$\n2. $\\mathbf{x}^T\\mathbf{e} = 0$\n\nThe quantity we are asked to find is $\\mathbf{1}^T\\mathbf{e}$. From the first equation in the system, we see that $\\mathbf{1}^T\\mathbf{e} = 0$.\nThe expression $\\mathbf{1}^T\\mathbf{e}$ is the dot product of a vector of ones with the residual vector, which is equivalent to the sum of the elements of the residual vector: $\\mathbf{1}^T\\mathbf{e} = \\sum_{i=1}^n e_i$. Thus, we have shown that the sum of the residuals in a simple linear regression with an intercept is zero.\n\nComparing our result with the given choices:\nA. $0$\nB. $n$\nC. $\\mathbf{1}^T\\mathbf{y}$\nD. $\\det(\\mathbf{X}^T\\mathbf{X})$\n\nOur derived value is $0$. Therefore, option A is the correct answer.", "answer": "$$\\boxed{A}$$", "id": "1933375"}]}