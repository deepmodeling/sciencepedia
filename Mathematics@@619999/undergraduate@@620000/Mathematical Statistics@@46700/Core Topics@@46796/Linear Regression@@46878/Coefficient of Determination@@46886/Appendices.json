{"hands_on_practices": [{"introduction": "The coefficient of determination, $R^2$, is fundamentally a story of variance. It tells us how much of the total variation in our dependent variable can be accounted for by the model. This first exercise [@problem_id:1904808] strips the concept down to its essential components, giving you direct practice with the relationship between the total sum of squares ($SST$), the regression sum of squares ($SSR$), and $R^2$. Mastering this calculation is the first step toward intuitively understanding what your model's $R^2$ value truly represents.", "problem": "An urban planning student is investigating the relationship between the population density of a city district and the average daily ridership on its public transit system. After collecting data from various districts, the student performs a simple linear regression analysis to model this relationship. The analysis reveals that the total sum of squares (SST), which represents the total variation in the daily ridership data around its mean, is 240. The model's coefficient of determination, $R^2$, is found to be 0.25. The coefficient of determination measures the proportion of the variance in the dependent variable (ridership) that is predictable from the independent variable (population density).\n\nCalculate the regression sum of squares (SSR), which quantifies the amount of variation in the ridership data explained by the regression model.", "solution": "Let $SST$ denote the total sum of squares, $SSR$ the regression sum of squares, and $SSE$ the error sum of squares. The standard decomposition is\n$$\nSST = SSR + SSE.\n$$\nThe coefficient of determination is defined by\n$$\nR^{2} = \\frac{SSR}{SST}.\n$$\nSolving for $SSR$ gives\n$$\nSSR = R^{2} \\cdot SST.\n$$\nSubstituting the given values $R^{2} = 0.25$ and $SST = 240$,\n$$\nSSR = 0.25 \\cdot 240 = \\frac{1}{4} \\cdot 240 = 60.\n$$\nThus, the regression sum of squares is $60$.", "answer": "$$\\boxed{60}$$", "id": "1904808"}, {"introduction": "While $R^2$ quantifies the proportion of explained variance, it doesn't, on its own, reveal the direction of the relationship. For this, we turn to the Pearson correlation coefficient, $r$. In a simple linear regression, $R^2$ and $r$ are intimately linked. This exercise [@problem_id:1904864] challenges you to bridge the gap between these two key statistics, emphasizing that interpreting data often requires synthesizing multiple pieces of informationâ€”in this case, using the direction of the slope to determine the sign of the correlation.", "problem": "An agricultural scientist is investigating the relationship between the amount of a specific fertilizer applied to a crop and the subsequent crop yield. The scientist conducts an experiment, applying varying amounts of fertilizer (in kg/hectare) to different plots of land and measuring the yield (in tonnes/hectare) for each plot. A simple linear regression analysis is performed on the collected data, with fertilizer amount as the independent variable and crop yield as the dependent variable. The analysis reveals that the coefficient of determination, denoted as $R^2$, is 0.49. A key observation from the data is that higher amounts of fertilizer are associated with lower crop yields, suggesting a potential toxic effect at the concentrations used.\n\nBased on this information, what is the value of the sample correlation coefficient, $r$, between the amount of fertilizer applied and the crop yield? Provide your answer as a single numerical value.", "solution": "We are given a simple linear regression with a single predictor and an intercept. For such a model, the coefficient of determination and the sample Pearson correlation coefficient satisfy\n$$\nR^{2} = r^{2}.\n$$\nGiven $R^{2} = 0.49$, it follows that\n$$\n|r| = \\sqrt{R^{2}} = \\sqrt{0.49} = 0.7.\n$$\nThe sign of $r$ matches the sign of the slope of the regression line. The observation that higher fertilizer amounts are associated with lower yields implies a negative slope and hence a negative correlation. Therefore,\n$$\nr = -0.7.\n$$", "answer": "$$\\boxed{-0.7}$$", "id": "1904864"}, {"introduction": "A common misconception is that $R^2$ must always lie between 0 and 1. While this holds for standard linear regression models that include an intercept, the rule can break under different assumptions. This thought-provoking problem [@problem_id:1904857] explores a regression-through-the-origin model, a scenario where the standard $R^2$ definition can yield a negative value. Working through this example reveals that $R^2$ is fundamentally a comparison of your model's performance against the simple baseline of the sample mean, an insight crucial for advanced modeling.", "problem": "An electrical engineer is characterizing a novel resistive component. According to theoretical models based on Ohm's Law, the voltage $V$ across the component should be directly proportional to the current $I$ passing through it, implying a relationship of the form $V = \\beta I$, where $\\beta$ is the resistance. This corresponds to a linear regression model forced to pass through the origin.\n\nTo test this, the engineer performs a small experiment and collects the following three data points, where current $I$ is measured in amperes (A) and voltage $V$ is measured in microvolts ($\\mu$V):\n$(I_1, V_1) = (10, 1)$\n$(I_2, V_2) = (11, 2)$\n$(I_3, V_3) = (12, 0)$\n\nTo assess the goodness-of-fit for the regression-through-the-origin model $V = \\beta I$, the engineer decides to compute the coefficient of determination, $R^2$. The standard definition for $R^2$ is used:\n$$R^2 = 1 - \\frac{SSE}{SST}$$\nwhere:\n- The Sum of Squared Errors (SSE) is defined as $SSE = \\sum_{i=1}^{n} (V_i - \\hat{V}_i)^2$, with $\\hat{V}_i$ being the voltage predicted by the model for the current $I_i$.\n- The Total Sum of Squares (SST) is defined as $SST = \\sum_{i=1}^{n} (V_i - \\bar{V})^2$, with $\\bar{V}$ being the sample mean of the observed voltages.\n\nFor a regression-through-the-origin model, the least squares estimate of the slope is given by $\\hat{\\beta} = \\frac{\\sum I_i V_i}{\\sum I_i^2}$.\n\nCalculate the value of the coefficient of determination, $R^2$, for this model and dataset. Round your final answer to three significant figures.", "solution": "We use the regression-through-the-origin model $V=\\beta I$. The least squares estimator is given by $\\hat{\\beta}=\\frac{\\sum I_{i}V_{i}}{\\sum I_{i}^{2}}$. For the data $(I_{1},V_{1})=(10,1)$, $(I_{2},V_{2})=(11,2)$, $(I_{3},V_{3})=(12,0)$, compute the required sums:\n$$\\sum I_{i}V_{i}=10\\cdot 1+11\\cdot 2+12\\cdot 0=32$$\n$$\\sum I_{i}^{2}=10^{2}+11^{2}+12^{2}=100+121+144=365$$\nThus,\n$$\\hat{\\beta}=\\frac{32}{365}.$$\nPredicted values are $\\hat{V}_{i}=\\hat{\\beta}I_{i}$, so\n$$\\hat{V}_{1}=\\frac{32}{365}\\cdot 10=\\frac{320}{365}=\\frac{64}{73},\\quad \\hat{V}_{2}=\\frac{32}{365}\\cdot 11=\\frac{352}{365},\\quad \\hat{V}_{3}=\\frac{32}{365}\\cdot 12=\\frac{384}{365}.$$\nResiduals are\n$$r_{1}=V_{1}-\\hat{V}_{1}=1-\\frac{64}{73}=\\frac{9}{73},\\quad r_{2}=2-\\frac{352}{365}=\\frac{378}{365},\\quad r_{3}=0-\\frac{384}{365}=-\\frac{384}{365}.$$\nHence the sum of squared errors is\n$$SSE=\\left(\\frac{9}{73}\\right)^{2}+\\left(\\frac{378}{365}\\right)^{2}+\\left(\\frac{384}{365}\\right)^{2}=\\frac{801}{365}.$$\nAlternatively, using the identity for regression through the origin,\n$$SSE=\\sum V_{i}^{2}-\\frac{\\left(\\sum I_{i}V_{i}\\right)^{2}}{\\sum I_{i}^{2}}=5-\\frac{32^{2}}{365}=\\frac{801}{365}.$$\nThe sample mean of the observed voltages is\n$$\\bar{V}=\\frac{1+2+0}{3}=1,$$\nso the total sum of squares is\n$$SST=\\sum\\left(V_{i}-\\bar{V}\\right)^{2}=(1-1)^{2}+(2-1)^{2}+(0-1)^{2}=0+1+1=2.$$\nTherefore,\n$$R^{2}=1-\\frac{SSE}{SST}=1-\\frac{\\frac{801}{365}}{2}=1-\\frac{801}{730}=-\\frac{71}{730}\\approx -0.0973,$$\nrounded to three significant figures.", "answer": "$$\\boxed{-0.0973}$$", "id": "1904857"}]}