{"hands_on_practices": [{"introduction": "The core of the method of least squares is the minimization of the sum of squared errors between observed data and a model's predictions. This first exercise takes you back to basics by having you construct this very function, the sum of squared residuals, for a small set of data points [@problem_id:1935126]. By explicitly writing out the function $S(\\beta_0, \\beta_1)$, you will gain a concrete understanding of the mathematical object that we seek to minimize to find the line of best fit.", "problem": "In statistical modeling, a simple linear regression model is often used to describe the relationship between a predictor variable $x$ and a response variable $y$. The model is given by the equation $y = \\beta_0 + \\beta_1 x$, where $\\beta_0$ represents the y-intercept and $\\beta_1$ represents the slope. The method of least squares provides a way to estimate the parameters $\\beta_0$ and $\\beta_1$ by finding the line that minimizes the sum of the squared differences between the observed responses and the responses predicted by the linear model.\n\nSuppose an experiment yields the following three data points $(x, y)$:\n$$ \\{(0, 1), (1, 3), (2, 4)\\} $$\nTo find the least squares regression line, one must minimize a function $S(\\beta_0, \\beta_1)$ representing the sum of squared residuals for this dataset.\n\nDetermine the explicit form of the function $S(\\beta_0, \\beta_1)$ in terms of the parameters $\\beta_0$ and $\\beta_1$.", "solution": "The least squares criterion minimizes the sum of squared residuals. For the linear model $y=\\beta_{0}+\\beta_{1}x$ and data points $(x_{i},y_{i})$, the residual for point $i$ is $r_{i}=y_{i}-(\\beta_{0}+\\beta_{1}x_{i})$, and the sum of squared residuals is\n$$\nS(\\beta_{0},\\beta_{1})=\\sum_{i=1}^{n}\\left[y_{i}-(\\beta_{0}+\\beta_{1}x_{i})\\right]^{2}.\n$$\nFor the given three points $(0,1)$, $(1,3)$, and $(2,4)$, this becomes\n$$\nS(\\beta_{0},\\beta_{1})=\\left[1-(\\beta_{0}+\\beta_{1}\\cdot 0)\\right]^{2}+\\left[3-(\\beta_{0}+\\beta_{1}\\cdot 1)\\right]^{2}+\\left[4-(\\beta_{0}+\\beta_{1}\\cdot 2)\\right]^{2}.\n$$\nSimplifying the arguments of the squares yields\n$$\nS(\\beta_{0},\\beta_{1})=(1-\\beta_{0})^{2}+(3-\\beta_{0}-\\beta_{1})^{2}+(4-\\beta_{0}-2\\beta_{1})^{2}.\n$$\nExpanding each term:\n$$\n(1-\\beta_{0})^{2}=\\beta_{0}^{2}-2\\beta_{0}+1,\n$$\n$$\n(3-\\beta_{0}-\\beta_{1})^{2}=\\beta_{0}^{2}+2\\beta_{0}\\beta_{1}+\\beta_{1}^{2}-6\\beta_{0}-6\\beta_{1}+9,\n$$\n$$\n(4-\\beta_{0}-2\\beta_{1})^{2}=\\beta_{0}^{2}+4\\beta_{0}\\beta_{1}+4\\beta_{1}^{2}-8\\beta_{0}-16\\beta_{1}+16.\n$$\nSumming and collecting like terms gives\n$$\nS(\\beta_{0},\\beta_{1})=3\\beta_{0}^{2}+6\\beta_{0}\\beta_{1}+5\\beta_{1}^{2}-16\\beta_{0}-22\\beta_{1}+26.\n$$", "answer": "$$\\boxed{3 \\beta_{0}^{2} + 6 \\beta_{0} \\beta_{1} + 5 \\beta_{1}^{2} - 16 \\beta_{0} - 22 \\beta_{1} + 26}$$", "id": "1935126"}, {"introduction": "To build strong intuition, it's often helpful to consider ideal or special scenarios. This practice presents a case where the data points already lie perfectly on a straight line, representing the best possible outcome for a linear model [@problem_id:1935161]. By determining the least squares estimators and the sum of squared errors ($SSE$) in this perfect-fit situation, you'll confirm that the method correctly identifies the underlying true relationship with zero error.", "problem": "Consider a simple linear regression model given by $Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$, where $\\beta_0$ and $\\beta_1$ are the model parameters and $\\epsilon_i$ are independent and identically distributed random error terms. An experiment is conducted, and a dataset of $n$ data points $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$ is collected, where $n \\geq 2$ and the values of $x_i$ are not all identical.\n\nUpon analyzing the data, it is discovered that every single data point perfectly satisfies the linear relationship $y_i = 2x_i + 1$.\n\nLet $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ be the standard least squares estimators for $\\beta_0$ and $\\beta_1$, respectively. Let SSE denote the Sum of Squared Errors for the fitted regression line.\n\nCalculate the numerical value of the expression $3\\hat{\\beta}_0 + 7\\hat{\\beta}_1 + \\text{SSE}$.", "solution": "The goal is to find the values of the least squares estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, and the Sum of Squared Errors (SSE), given that the data points $(x_i, y_i)$ perfectly lie on the line $y = 2x + 1$.\n\nThe method of least squares seeks to find the values of $\\beta_0$ and $\\beta_1$ that minimize the sum of the squared differences between the observed values $y_i$ and the values predicted by the model, $\\hat{y}_i = \\beta_0 + \\beta_1 x_i$. This sum is the objective function, $S(\\beta_0, \\beta_1)$:\n$$S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2$$\nThe least squares estimators, $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, are the specific values of $\\beta_0$ and $\\beta_1$ that minimize this function. The Sum of Squared Errors, SSE, is the minimum value of this function, i.e., $\\text{SSE} = S(\\hat{\\beta}_0, \\hat{\\beta}_1)$.\n\nWe are given that for every data point, $y_i = 2x_i + 1$. Let's substitute this information into the objective function $S(\\beta_0, \\beta_1)$:\n$$S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} ((2x_i + 1) - (\\beta_0 + \\beta_1 x_i))^2$$\nThis can be rewritten as:\n$$S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} ((1 - \\beta_0) + (2 - \\beta_1)x_i)^2$$\nThe function $S(\\beta_0, \\beta_1)$ represents a sum of squared terms. A sum of squares is always non-negative. The minimum possible value for $S(\\beta_0, \\beta_1)$ is 0. This minimum is achieved if and only if every term in the summation is zero.\nSo, we need to find if there exist values for $\\beta_0$ and $\\beta_1$ such that for all $i=1, \\dots, n$:\n$$(1 - \\beta_0) + (2 - \\beta_1)x_i = 0$$\nLet's see if we can find such $\\beta_0$ and $\\beta_1$. If we choose $\\beta_0 = 1$ and $\\beta_1 = 2$, the expression becomes:\n$$(1 - 1) + (2 - 2)x_i = 0 + 0 \\cdot x_i = 0$$\nThis equality holds for all values of $x_i$ and for all $i=1, \\dots, n$.\n\nTherefore, by choosing $\\beta_0 = 1$ and $\\beta_1 = 2$, the objective function $S(\\beta_0, \\beta_1)$ achieves its absolute minimum value of 0.\n$$S(1, 2) = \\sum_{i=1}^{n} (0)^2 = 0$$\nBy definition, the least squares estimators are the values that minimize $S(\\beta_0, \\beta_1)$. We have found that these values are $\\beta_0=1$ and $\\beta_1=2$.\nThus, the least squares estimators are:\n$$\\hat{\\beta}_0 = 1$$\n$$\\hat{\\beta}_1 = 2$$\nThe Sum of Squared Errors (SSE) is the value of the objective function evaluated at these estimators:\n$$\\text{SSE} = S(\\hat{\\beta}_0, \\hat{\\beta}_1) = S(1, 2) = 0$$\nThis makes intuitive sense: if the data points all lie perfectly on a line, the \"line of best fit\" must be that exact line, and the errors (the distances from the points to the line) must all be zero.\n\nFinally, we need to calculate the value of the expression $3\\hat{\\beta}_0 + 7\\hat{\\beta}_1 + \\text{SSE}$. Substituting the values we found:\n$$3\\hat{\\beta}_0 + 7\\hat{\\beta}_1 + \\text{SSE} = 3(1) + 7(2) + 0$$\n$$= 3 + 14 + 0$$\n$$= 17$$", "answer": "$$\\boxed{17}$$", "id": "1935161"}, {"introduction": "The process of least squares fitting leaves behind residuals, which are not just random noise but possess unique and important mathematical properties. This final practice explores a key property by asking you to perform a second regression, this time using the residuals from an initial model as your new response variable [@problem_id:1935149]. The result of this exercise reveals the fundamental concept of orthogonality in linear regression, a cornerstone property that ensures the model has extracted all linear information from the predictors.", "problem": "In the context of statistical modeling, consider a simple linear regression performed on a dataset consisting of $n$ pairs of observations $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$, where we assume that not all $x_i$ values are identical. The model is given by $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$. The method of least squares is used to find the estimators for the intercept, $\\hat{\\beta}_0$, and the slope, $\\hat{\\beta}_1$.\n\nFrom this initial regression, the fitted values are calculated as $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$. The corresponding residuals are then computed for each observation as $e_i = y_i - \\hat{y}_i$.\n\nNow, suppose you perform a second, new simple linear regression. In this new regression, you treat the calculated residuals $e_i$ as the new response variable and the original $x_i$ values as the predictor variable. This new model is of the form $e_i = \\gamma_0 + \\gamma_1 x_i + \\delta_i$.\n\nDetermine the values of the least squares estimators for the intercept, $\\hat{\\gamma}_0$, and the slope, $\\hat{\\gamma}_1$, for this second regression. Present your final answer as a row matrix containing two elements, where the first element is the value of $\\hat{\\gamma}_0$ and the second element is the value of $\\hat{\\gamma}_1$.", "solution": "Consider the first least squares problem with model $y_i=\\beta_{0}+\\beta_{1}x_i+\\epsilon_i$ and residuals $e_i=y_i-\\hat{y}_i=y_i-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_i$. The least squares estimators $(\\hat{\\beta}_{0},\\hat{\\beta}_{1})$ minimize\n$$\nS(\\beta_{0},\\beta_{1})=\\sum_{i=1}^{n}\\left(y_i-\\beta_{0}-\\beta_{1}x_i\\right)^{2}.\n$$\nThe normal equations are obtained by setting the partial derivatives to zero:\n$$\n\\frac{\\partial S}{\\partial \\beta_{0}}=-2\\sum_{i=1}^{n}\\left(y_i-\\beta_{0}-\\beta_{1}x_i\\right)=0,\\quad\n\\frac{\\partial S}{\\partial \\beta_{1}}=-2\\sum_{i=1}^{n}x_i\\left(y_i-\\beta_{0}-\\beta_{1}x_i\\right)=0.\n$$\nEvaluated at $(\\hat{\\beta}_{0},\\hat{\\beta}_{1})$, these give\n$$\n\\sum_{i=1}^{n}e_i=0,\\qquad \\sum_{i=1}^{n}x_i e_i=0.\n$$\nHence the residuals from the first regression satisfy both zero-sum and orthogonality to $x$.\n\nNow consider the second regression with response $e_i$ and predictor $x_i$:\n$$\ne_i=\\gamma_{0}+\\gamma_{1}x_i+\\delta_i,\n$$\nwhose least squares estimators $(\\hat{\\gamma}_{0},\\hat{\\gamma}_{1})$ minimize\n$$\nT(\\gamma_{0},\\gamma_{1})=\\sum_{i=1}^{n}\\left(e_i-\\gamma_{0}-\\gamma_{1}x_i\\right)^{2}.\n$$\nThe normal equations are\n$$\n\\frac{\\partial T}{\\partial \\gamma_{0}}=-2\\sum_{i=1}^{n}\\left(e_i-\\gamma_{0}-\\gamma_{1}x_i\\right)=0,\\qquad\n\\frac{\\partial T}{\\partial \\gamma_{1}}=-2\\sum_{i=1}^{n}x_i\\left(e_i-\\gamma_{0}-\\gamma_{1}x_i\\right)=0.\n$$\nLet $\\bar{e}=\\frac{1}{n}\\sum_{i=1}^{n}e_i$ and $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_i$. Solving the normal equations in the standard way yields\n$$\n\\hat{\\gamma}_{1}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(e_i-\\bar{e})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^{2}},\\qquad\n\\hat{\\gamma}_{0}=\\bar{e}-\\hat{\\gamma}_{1}\\bar{x}.\n$$\nFrom the first regression we have $\\sum_{i=1}^{n}e_i=0$, hence $\\bar{e}=0$, and $\\sum_{i=1}^{n}x_i e_i=0$. Therefore\n$$\n\\sum_{i=1}^{n}(x_i-\\bar{x})(e_i-\\bar{e})=\\sum_{i=1}^{n}x_i e_i - n\\bar{x}\\bar{e}=0- n\\bar{x}\\cdot 0=0.\n$$\nSince not all $x_i$ are equal, the denominator $\\sum_{i=1}^{n}(x_i-\\bar{x})^{2}>0$. Thus\n$$\n\\hat{\\gamma}_{1}=0,\\qquad \\hat{\\gamma}_{0}=\\bar{e}-\\hat{\\gamma}_{1}\\bar{x}=0-0\\cdot \\bar{x}=0.\n$$\nBy uniqueness of the least squares solution when $\\sum_{i=1}^{n}(x_i-\\bar{x})^{2}>0$, these are the least squares estimators for the second regression.", "answer": "$$\\boxed{\\begin{pmatrix}0 & 0\\end{pmatrix}}$$", "id": "1935149"}]}