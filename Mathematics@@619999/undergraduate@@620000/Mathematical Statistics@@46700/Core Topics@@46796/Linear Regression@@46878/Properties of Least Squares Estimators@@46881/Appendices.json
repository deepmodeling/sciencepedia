{"hands_on_practices": [{"introduction": "To build a solid understanding of least squares, we begin with the simplest case: a linear relationship forced through the origin. This exercise guides you through deriving the least squares estimator from first principles by minimizing the sum of squared errors in a model without an intercept, such as $Y_i = \\beta x_i + \\epsilon_i$. By exploring the estimator's unbiasedness and variance, you will gain insight into the fundamental mechanics of the method and see how model assumptions directly impact the properties of the results [@problem_id:1948111].", "problem": "An engineer is characterizing a new resistive material. According to physical theory, the voltage $V$ across the material should be directly proportional to the current $I$ passing through it, following the model $V = \\beta I$, where $\\beta$ is the unknown true resistance. To estimate $\\beta$, the engineer performs a series of $n$ experiments, setting the current to known, non-random values $x_1, x_2, \\ldots, x_n$ and measuring the corresponding voltages $Y_1, Y_2, \\ldots, Y_n$.\n\nThe relationship is described by the simple linear regression model forced through the origin:\n$$Y_i = \\beta x_i + \\epsilon_i \\quad \\text{for } i=1, \\ldots, n$$\nHere, $Y_i$ is the measured voltage for the set current $x_i$, and $\\epsilon_i$ represents the random measurement error. The errors are assumed to be independent with $E[\\epsilon_i] = 0$ and $\\text{Var}(\\epsilon_i) = \\sigma^2$ for some unknown constant $\\sigma^2 > 0$. It is also assumed that at least one of the applied currents $x_i$ is non-zero.\n\nThe engineer uses the method of least squares to obtain an estimator for the resistance, denoted by $\\hat{\\beta}$. Consider the following statements about the properties of this estimator $\\hat{\\beta}$ and its corresponding residuals $e_i = Y_i - \\hat{\\beta} x_i$.\n\nA. The formula for the estimator is $\\hat{\\beta} = \\frac{1}{n} \\sum_{i=1}^n \\frac{Y_i}{x_i}$.\n\nB. The estimator $\\hat{\\beta}$ is an unbiased estimator of $\\beta$.\n\nC. The variance of the estimator is given by $\\text{Var}(\\hat{\\beta}) = \\frac{\\sigma^2 \\sum_{i=1}^n x_i^2}{(\\sum_{i=1}^n x_i)^2}$.\n\nD. The sum of the residuals, $\\sum_{i=1}^n e_i$, is not guaranteed to be zero.\n\nE. The variance of the estimator is given by $\\text{Var}(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n x_i^2}$.\n\nWhich of the above statements are correct?", "solution": "We begin by deriving the ordinary least squares estimator for the slope in the regression through the origin. The estimator $\\hat{\\beta}$ minimizes the sum of squared residuals\n$$\nS(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-\\beta x_{i}\\right)^{2}.\n$$\nDifferentiating with respect to $\\beta$ and setting the derivative to zero gives the normal equation\n$$\n\\frac{\\partial S(\\beta)}{\\partial \\beta}=-2\\sum_{i=1}^{n}x_{i}\\left(Y_{i}-\\beta x_{i}\\right)=0,\n$$\nwhich simplifies to\n$$\n\\sum_{i=1}^{n}x_{i}Y_{i}-\\beta\\sum_{i=1}^{n}x_{i}^{2}=0.\n$$\nAssuming $\\sum_{i=1}^{n}x_{i}^{2}>0$ (guaranteed by at least one $x_{i}\\neq 0$), the least squares estimator is\n$$\n\\hat{\\beta}=\\frac{\\sum_{i=1}^{n}x_{i}Y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}}.\n$$\nThis directly shows that statement A, $\\hat{\\beta}=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{Y_{i}}{x_{i}}$, is incorrect in general; moreover, it would be undefined if any $x_{i}=0$, while the correct least squares estimator remains well-defined as long as not all $x_{i}$ are zero.\n\nNext, we establish unbiasedness. Using the model $Y_{i}=\\beta x_{i}+\\epsilon_{i}$ with $E[\\epsilon_{i}]=0$ and treating $x_{i}$ as fixed,\n$$\nE[\\hat{\\beta}]=\\frac{\\sum_{i=1}^{n}x_{i}E[Y_{i}]}{\\sum_{i=1}^{n}x_{i}^{2}}=\\frac{\\sum_{i=1}^{n}x_{i}(\\beta x_{i})}{\\sum_{i=1}^{n}x_{i}^{2}}=\\beta,\n$$\nso statement B is correct.\n\nFor the variance, write\n$$\n\\hat{\\beta}=\\beta+\\frac{\\sum_{i=1}^{n}x_{i}\\epsilon_{i}}{\\sum_{i=1}^{n}x_{i}^{2}}.\n$$\nSince the $\\epsilon_{i}$ are independent with $\\text{Var}(\\epsilon_{i})=\\sigma^{2}$,\n$$\n\\text{Var}\\!\\left(\\sum_{i=1}^{n}x_{i}\\epsilon_{i}\\right)=\\sum_{i=1}^{n}x_{i}^{2}\\,\\text{Var}(\\epsilon_{i})=\\sigma^{2}\\sum_{i=1}^{n}x_{i}^{2}.\n$$\nTherefore,\n$$\n\\text{Var}(\\hat{\\beta})=\\frac{\\text{Var}\\!\\left(\\sum_{i=1}^{n}x_{i}\\epsilon_{i}\\right)}{\\left(\\sum_{i=1}^{n}x_{i}^{2}\\right)^{2}}=\\frac{\\sigma^{2}}{\\sum_{i=1}^{n}x_{i}^{2}}.\n$$\nThus statement E is correct, and statement C, which gives a different expression, is incorrect.\n\nFinally, consider the residuals $e_{i}=Y_{i}-\\hat{\\beta}x_{i}$. The normal equation implies\n$$\n\\sum_{i=1}^{n}x_{i}e_{i}=\\sum_{i=1}^{n}x_{i}(Y_{i}-\\hat{\\beta}x_{i})=0.\n$$\nHowever, there is no constraint in the through-the-origin model that forces $\\sum_{i=1}^{n}e_{i}=0$. In fact,\n$$\n\\sum_{i=1}^{n}e_{i}=\\sum_{i=1}^{n}Y_{i}-\\hat{\\beta}\\sum_{i=1}^{n}x_{i},\n$$\nwhich is not generally zero. Therefore, statement D is correct.\n\nCollecting the conclusions: B, D, and E are correct; A and C are incorrect.", "answer": "$$\\boxed{BDE}$$", "id": "1948111"}, {"introduction": "In practical data analysis, the units of measurement for our variables are often arbitrary. This exercise explores how the least squares estimators in a standard simple linear regression model ($Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$) behave when the predictor variable is rescaled by a constant factor. Understanding these scaling properties is essential for correctly interpreting regression coefficients and their standard errors, ensuring that your conclusions are robust to changes in units [@problem_id:1948136].", "problem": "Consider a simple linear regression model used to describe the relationship between a response variable $Y$ and a predictor variable $x$. The model for a set of $n$ observations $(x_i, Y_i)$ is given by\n$$Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i = 1, 2, \\ldots, n$$\nwhere $\\beta_0$ and $\\beta_1$ are the intercept and slope parameters, respectively. The error terms $\\epsilon_i$ are assumed to be independent and identically distributed random variables with mean $E[\\epsilon_i] = 0$ and variance $\\text{Var}(\\epsilon_i) = \\sigma^2$. The least squares estimators for the original model are denoted by $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$.\n\nA researcher decides to rescale the predictor variable by multiplying it by a non-zero constant $c$. A new predictor variable $x_i'$ is defined as $x_i' = c x_i$. A new regression is then performed using this scaled predictor, with the model given by\n$$Y_i = \\beta_0' + \\beta_1' x_i' + \\epsilon_i$$\nThe least squares estimators for this new model are denoted by $\\hat{\\beta}_0'$ and $\\hat{\\beta}_1'$.\n\nWhat are the relationships between the new estimators ($\\hat{\\beta}_0'$, $\\hat{\\beta}_1'$) and the original estimators ($\\hat{\\beta}_0$, $\\hat{\\beta}_1$), and what is the relationship between the variance of the new slope estimator, $\\text{Var}(\\hat{\\beta}_1')$, and the variance of the original slope estimator, $\\text{Var}(\\hat{\\beta}_1)$?\n\nA. $\\hat{\\beta}_0' = \\hat{\\beta}_0$, $\\hat{\\beta}_1' = \\frac{\\hat{\\beta}_1}{c}$, and $\\text{Var}(\\hat{\\beta}_1') = \\frac{\\text{Var}(\\hat{\\beta}_1)}{c^2}$\n\nB. $\\hat{\\beta}_0' = c\\hat{\\beta}_0$, $\\hat{\\beta}_1' = c\\hat{\\beta}_1$, and $\\text{Var}(\\hat{\\beta}_1') = c^2\\text{Var}(\\hat{\\beta}_1)$\n\nC. $\\hat{\\beta}_0' = \\hat{\\beta}_0$, $\\hat{\\beta}_1' = c\\hat{\\beta}_1$, and $\\text{Var}(\\hat{\\beta}_1') = c^2\\text{Var}(\\hat{\\beta}_1)$\n\nD. $\\hat{\\beta}_0' = \\frac{\\hat{\\beta}_0}{c}$, $\\hat{\\beta}_1' = \\frac{\\hat{\\beta}_1}{c}$, and $\\text{Var}(\\hat{\\beta}_1') = \\frac{\\text{Var}(\\hat{\\beta}_1)}{c^2}$\n\nE. $\\hat{\\beta}_0' = \\hat{\\beta}_0$, $\\hat{\\beta}_1' = \\frac{\\hat{\\beta}_1}{c^2}$, and $\\text{Var}(\\hat{\\beta}_1') = \\frac{\\text{Var}(\\hat{\\beta}_1)}{c^4}$", "solution": "We start from the original simple linear regression model\n$$\nY_i=\\beta_{0}+\\beta_{1}x_i+\\epsilon_i,\\quad i=1,2,\\ldots,n,\n$$\nwith $E[\\epsilon_i]=0$ and $\\text{Var}(\\epsilon_i)=\\sigma^{2}$. The ordinary least squares (OLS) slope and intercept estimators can be written as\n$$\n\\hat{\\beta}_{1}=\\frac{S_{xy}}{S_{xx}},\\quad \\hat{\\beta}_{0}=\\bar{Y}-\\hat{\\beta}_{1}\\bar{x},\n$$\nwhere\n$$\nS_{xy}=\\sum_{i=1}^{n}(x_i-\\bar{x})(Y_i-\\bar{Y}),\\quad S_{xx}=\\sum_{i=1}^{n}(x_i-\\bar{x})^{2},\n$$\nand $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_i$, $\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i$.\n\nNow define the rescaled predictor $x_i'=c\\,x_i$ with $c\\neq 0$, and fit the model\n$$\nY_i=\\beta_{0}'+\\beta_{1}'x_i'+\\epsilon_i.\n$$\nThe corresponding sample mean satisfies $\\bar{x}'=c\\,\\bar{x}$. For the transformed sums, note that\n$$\nx_i'-\\bar{x}'=c(x_i-\\bar{x}),\n$$\nso\n$$\nS_{x'y}=\\sum_{i=1}^{n}(x_i'-\\bar{x}')(Y_i-\\bar{Y})=\\sum_{i=1}^{n}c(x_i-\\bar{x})(Y_i-\\bar{Y})=c\\,S_{xy},\n$$\nand\n$$\nS_{x'x'}=\\sum_{i=1}^{n}(x_i'-\\bar{x}')^{2}=\\sum_{i=1}^{n}c^{2}(x_i-\\bar{x})^{2}=c^{2}S_{xx}.\n$$\nTherefore, the new OLS slope is\n$$\n\\hat{\\beta}_{1}'=\\frac{S_{x'y}}{S_{x'x'}}=\\frac{c\\,S_{xy}}{c^{2}S_{xx}}=\\frac{1}{c}\\hat{\\beta}_{1}.\n$$\nThe new intercept is\n$$\n\\hat{\\beta}_{0}'=\\bar{Y}-\\hat{\\beta}_{1}'\\bar{x}'=\\bar{Y}-\\left(\\frac{1}{c}\\hat{\\beta}_{1}\\right)(c\\,\\bar{x})=\\bar{Y}-\\hat{\\beta}_{1}\\bar{x}=\\hat{\\beta}_{0}.\n$$\n\nFor the variance of the slope estimator under the homoskedastic model, the standard formula gives\n$$\n\\text{Var}(\\hat{\\beta}_{1})=\\frac{\\sigma^{2}}{S_{xx}},\\quad \\text{and}\\quad \\text{Var}(\\hat{\\beta}_{1}')=\\frac{\\sigma^{2}}{S_{x'x'}}=\\frac{\\sigma^{2}}{c^{2}S_{xx}}=\\frac{1}{c^{2}}\\text{Var}(\\hat{\\beta}_{1}).\n$$\nEquivalently, since $\\hat{\\beta}_{1}'=(1/c)\\hat{\\beta}_{1}$ with $c$ deterministic, $\\text{Var}(\\hat{\\beta}_{1}')=(1/c^{2})\\text{Var}(\\hat{\\beta}_{1})$.\n\nThus, the relationships are $\\hat{\\beta}_{0}'=\\hat{\\beta}_{0}$, $\\hat{\\beta}_{1}'=\\hat{\\beta}_{1}/c$, and $\\text{Var}(\\hat{\\beta}_{1}')=\\text{Var}(\\hat{\\beta}_{1})/c^{2}$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1948136"}, {"introduction": "Moving from simple to multiple regression introduces the challenge of interpreting a coefficient in the presence of other predictors. This practice illuminates the Frisch-Waugh-Lovell theorem, a cornerstone concept that clarifies the meaning of each coefficient in a multiple regression model. You will see that an estimate like $\\hat{\\beta}_1$ represents the effect of its predictor, $X_1$, after the linear influences of all other variables in the model have been \"partialled out\" from both $X_1$ and the response variable $Y$ [@problem_id:1948175].", "problem": "A data scientist is modeling a server's Central Processing Unit (CPU) load. The model aims to predict the CPU load ($Y$, in percent) based on two predictor variables: the number of active user sessions ($X_1$) and the network bandwidth utilization ($X_2$, in Mbps). The multiple linear regression model is given by:\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\n$$\nwhere $\\epsilon_i$ are independent and identically distributed error terms with mean zero, and the subscript $i$ denotes the $i$-th observation from a sample of $n$ observations.\n\nThe scientist is particularly interested in quantifying the unique contribution of active user sessions ($X_1$) to the CPU load, after accounting for the influence of network bandwidth ($X_2$). To investigate this, they perform two preliminary Ordinary Least Squares (OLS) regressions.\n\nFirst, they regress the CPU load ($Y$) on an intercept and the network bandwidth ($X_2$). Let the residuals from this regression be denoted by the vector $r_Y$.\nSecond, they regress the number of active user sessions ($X_1$) on an intercept and the network bandwidth ($X_2$). Let the residuals from this second regression be denoted by the vector $r_{X_1}$.\n\nFrom these two sets of residuals, the following summary statistics are calculated:\n- The sum of squared residuals from the second regression: $\\sum_{i=1}^{n} (r_{X_1,i})^2 = 88.4$.\n- The sum of the products of the corresponding residuals from the two regressions: $\\sum_{i=1}^{n} r_{Y,i} \\, r_{X_1,i} = 153.7$.\n\nUsing only these summary statistics, determine the OLS estimate for the coefficient $\\beta_1$ in the full multiple regression model. Round your final answer to three significant figures.", "solution": "We aim to estimate $\\hat{\\beta}_{1}$ in the multiple regression of $Y$ on an intercept, $X_{1}$, and $X_{2}$. By the Frisch–Waugh–Lovell theorem, the OLS estimate $\\hat{\\beta}_{1}$ equals the slope from regressing the residuals of $Y$ on $[1, X_{2}]$ onto the residuals of $X_{1}$ on $[1, X_{2}]$. Denote $Z$ as the matrix with columns for the intercept and $X_{2}$, and define the residual-maker matrix $M_{Z} = I - Z(Z^{\\top}Z)^{-1}Z^{\\top}$. Then\n$$\nr_{Y} = M_{Z}Y, \\quad r_{X_{1}} = M_{Z}X_{1}.\n$$\nThe OLS coefficient from the regression of $r_{Y}$ on $r_{X_{1}}$ without an intercept minimizes\n$$\nS(\\alpha) = \\|r_{Y} - \\alpha r_{X_{1}}\\|^{2} = (r_{Y} - \\alpha r_{X_{1}})^{\\top}(r_{Y} - \\alpha r_{X_{1}}).\n$$\nDifferentiating and setting to zero gives\n$$\n\\frac{dS}{d\\alpha} = -2 r_{X_{1}}^{\\top} r_{Y} + 2 \\alpha \\, r_{X_{1}}^{\\top} r_{X_{1}} = 0 \\;\\;\\Rightarrow\\;\\; \\hat{\\alpha} = \\frac{r_{X_{1}}^{\\top} r_{Y}}{r_{X_{1}}^{\\top} r_{X_{1}}}.\n$$\nBy the Frisch–Waugh–Lovell theorem, $\\hat{\\beta}_{1} = \\hat{\\alpha}$. Using the provided summary statistics,\n$$\nr_{X_{1}}^{\\top} r_{X_{1}} = \\sum_{i=1}^{n} (r_{X_{1},i})^{2} = 88.4, \\quad r_{X_{1}}^{\\top} r_{Y} = \\sum_{i=1}^{n} r_{X_{1},i} r_{Y,i} = 153.7,\n$$\nso\n$$\n\\hat{\\beta}_{1} = \\frac{153.7}{88.4} \\approx 1.7386877828\\ldots\n$$\nRounded to three significant figures,\n$$\n\\hat{\\beta}_{1} \\approx 1.74.\n$$", "answer": "$$\\boxed{1.74}$$", "id": "1948175"}]}