{"hands_on_practices": [{"introduction": "This first exercise explores the foundation of optimal estimation in its most straightforward form. When we have multiple measurements of the same quantity, each with the same level of uncertainty, common intuition suggests that the best way to combine them is to take a simple average. This practice [@problem_id:1919555] will allow you to prove this intuition mathematically, by finding the linear combination of two measurements that results in the minimum possible variance. It serves as a crucial first step in understanding the \"Best\" in Best Linear Unbiased Estimator (BLUE).", "problem": "An experimentalist makes two independent measurements, $y_1$ and $y_2$, of a physical quantity whose true, unknown value is $\\mu$. Each measurement is subject to a random error. The statistical model for these measurements is given by $y_i = \\mu + \\epsilon_i$ for $i=1, 2$.\n\nThe random errors $\\epsilon_1$ and $\\epsilon_2$ are assumed to satisfy the following standard properties:\n1.  The expected value of each error is zero, i.e., $E[\\epsilon_i] = 0$.\n2.  The errors have a common, finite variance, denoted by $\\sigma^2  0$.\n3.  The errors are uncorrelated, meaning $\\text{Cov}(\\epsilon_1, \\epsilon_2) = 0$.\n\nTo combine the two measurements into a single, improved estimate for $\\mu$, a linear estimator of the form $\\tilde{\\mu} = w y_1 + (1-w) y_2$ is proposed, where $w$ is a real-valued weight. Your task is to find the specific numerical value of $w$ that minimizes the variance of this estimator, $\\text{Var}(\\tilde{\\mu})$.", "solution": "The goal is to find the value of the weight $w$ that minimizes the variance of the estimator $\\tilde{\\mu} = w y_1 + (1-w) y_2$. The variance of the estimator is denoted by $\\text{Var}(\\tilde{\\mu})$.\n\nFirst, we express the variance of $\\tilde{\\mu}$ using the properties of variance. For a linear combination of random variables, the variance is given by:\n$$\n\\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y)\n$$\nIn our case, the estimator is $\\tilde{\\mu} = w y_1 + (1-w) y_2$. The variables are $y_1$ and $y_2$, with coefficients $a=w$ and $b=1-w$.\n$$\n\\text{Var}(\\tilde{\\mu}) = \\text{Var}(w y_1 + (1-w) y_2) = w^2 \\text{Var}(y_1) + (1-w)^2 \\text{Var}(y_2) + 2w(1-w) \\text{Cov}(y_1, y_2)\n$$\nNext, we need to find the variance of each measurement $y_i$ and the covariance between them.\nThe variance of $y_i$ is:\n$$\n\\text{Var}(y_i) = \\text{Var}(\\mu + \\epsilon_i)\n$$\nSince $\\mu$ is a constant, its variance is zero. Thus, the variance of $y_i$ is simply the variance of the error term $\\epsilon_i$.\n$$\n\\text{Var}(y_i) = \\text{Var}(\\epsilon_i) = \\sigma^2\n$$\nThis holds for both $i=1$ and $i=2$.\n\nThe covariance between $y_1$ and $y_2$ is:\n$$\n\\text{Cov}(y_1, y_2) = \\text{Cov}(\\mu + \\epsilon_1, \\mu + \\epsilon_2)\n$$\nSince $\\mu$ is a constant, it does not affect the covariance.\n$$\n\\text{Cov}(y_1, y_2) = \\text{Cov}(\\epsilon_1, \\epsilon_2)\n$$\nThe problem states that the errors are uncorrelated, which means $\\text{Cov}(\\epsilon_1, \\epsilon_2) = 0$. Therefore, $\\text{Cov}(y_1, y_2) = 0$.\n\nNow, we substitute these back into the expression for $\\text{Var}(\\tilde{\\mu})$:\n$$\n\\text{Var}(\\tilde{\\mu}) = w^2 (\\sigma^2) + (1-w)^2 (\\sigma^2) + 2w(1-w)(0)\n$$\n$$\n\\text{Var}(\\tilde{\\mu}) = w^2 \\sigma^2 + (1-w)^2 \\sigma^2\n$$\nWe can factor out the constant variance $\\sigma^2$:\n$$\n\\text{Var}(\\tilde{\\mu}) = \\sigma^2 [w^2 + (1-w)^2]\n$$\nTo find the value of $w$ that minimizes this variance, we can define a function $f(w) = w^2 + (1-w)^2$ and find its minimum. Since $\\sigma^2  0$, minimizing $f(w)$ is equivalent to minimizing $\\text{Var}(\\tilde{\\mu})$.\n\nLet's expand the function $f(w)$:\n$$\nf(w) = w^2 + (1 - 2w + w^2) = 2w^2 - 2w + 1\n$$\nThis is a quadratic function of $w$, representing a parabola opening upwards. The minimum can be found using calculus by taking the first derivative with respect to $w$ and setting it to zero.\n$$\n\\frac{df}{dw} = \\frac{d}{dw}(2w^2 - 2w + 1) = 4w - 2\n$$\nSet the derivative to zero to find the critical point:\n$$\n4w - 2 = 0\n$$\n$$\n4w = 2\n$$\n$$\nw = \\frac{2}{4} = \\frac{1}{2}\n$$\nTo confirm this is a minimum, we can use the second derivative test. The second derivative is:\n$$\n\\frac{d^2f}{dw^2} = \\frac{d}{dw}(4w - 2) = 4\n$$\nSince the second derivative is positive ($4  0$), the critical point $w = 1/2$ corresponds to a local minimum. Because $f(w)$ is a parabola, this is a global minimum.\n\nThus, the variance of the estimator $\\tilde{\\mu}$ is minimized when $w = 1/2$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1919555"}, {"introduction": "Real-world data is rarely perfect; some measurements are inherently more precise than others. This exercise [@problem_id:1919578] moves beyond the idealized scenario of identical variances and challenges you to find the optimal way to combine three measurements of varying quality. By working through this problem, you will derive the principle of inverse-variance weighting from first principles, a powerful technique that forms the core of finding the BLUE in the presence of heteroscedasticity.", "problem": "A team of scientists is attempting to determine a precise value for a physical constant, denoted by $\\mu$. They perform three independent experiments, yielding measurements $y_1, y_2$, and $y_3$. Each measurement can be described by the statistical model $y_i = \\mu + \\epsilon_i$ for $i=1, 2, 3$, where $\\epsilon_i$ represents the random error for the $i$-th experiment. The errors are known to be unbiased, meaning their expected value is zero, $E[\\epsilon_i]=0$.\n\nDue to improving experimental techniques and calibration over time, the first measurement is the most precise, and subsequent measurements are progressively less so. The variances of the measurements reflect this, and are given as $\\text{Var}(y_1) = \\sigma^2$, $\\text{Var}(y_2) = 2\\sigma^2$, and $\\text{Var}(y_3) = 3\\sigma^2$, where $\\sigma^2$ is a positive but unknown constant.\n\nThe scientists wish to combine the three measurements into a single, more reliable estimate for $\\mu$ using a linear combination of the form $\\tilde{\\mu} = c_1 y_1 + c_2 y_2 + c_3 y_3$. For this estimator to be unbiased (i.e., $E[\\tilde{\\mu}] = \\mu$), it is required that the coefficients sum to one: $c_1 + c_2 + c_3 = 1$.\n\nTo achieve the highest possible precision, the scientists need to choose the coefficients $c_1, c_2, c_3$ such that the variance of $\\tilde{\\mu}$ is minimized, subject to the unbiasedness constraint. This optimal estimator is known as the Best Linear Unbiased Estimator (BLUE).\n\nDetermine the numerical value of the coefficient $c_1$ for this optimal estimator. Express your answer as a fraction in simplest form.", "solution": "We are given independent measurements $y_{i}=\\mu+\\epsilon_{i}$ with $E[\\epsilon_{i}]=0$ and $\\operatorname{Var}(y_{1})=\\sigma^{2}$, $\\operatorname{Var}(y_{2})=2\\sigma^{2}$, $\\operatorname{Var}(y_{3})=3\\sigma^{2}$. Consider the linear estimator $\\tilde{\\mu}=c_{1}y_{1}+c_{2}y_{2}+c_{3}y_{3}$ subject to the unbiasedness constraint $c_{1}+c_{2}+c_{3}=1$, since\n$$\nE[\\tilde{\\mu}]=c_{1}E[y_{1}]+c_{2}E[y_{2}]+c_{3}E[y_{3}]=(c_{1}+c_{2}+c_{3})\\mu,\n$$\nand $E[\\tilde{\\mu}]=\\mu$ requires $c_{1}+c_{2}+c_{3}=1$.\n\nBecause the measurements are independent, the variance of $\\tilde{\\mu}$ is\n$$\n\\operatorname{Var}(\\tilde{\\mu})=\\operatorname{Var}(c_{1}y_{1}+c_{2}y_{2}+c_{3}y_{3})=c_{1}^{2}\\operatorname{Var}(y_{1})+c_{2}^{2}\\operatorname{Var}(y_{2})+c_{3}^{2}\\operatorname{Var}(y_{3})\n=\\sigma^{2}c_{1}^{2}+2\\sigma^{2}c_{2}^{2}+3\\sigma^{2}c_{3}^{2}.\n$$\nWe minimize this subject to $c_{1}+c_{2}+c_{3}=1$ using a Lagrange multiplier $\\lambda$. Define\n$$\n\\mathcal{L}(c_{1},c_{2},c_{3},\\lambda)=\\sigma^{2}c_{1}^{2}+2\\sigma^{2}c_{2}^{2}+3\\sigma^{2}c_{3}^{2}-\\lambda(c_{1}+c_{2}+c_{3}-1).\n$$\nSet partial derivatives to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial c_{1}}=2\\sigma^{2}c_{1}-\\lambda=0,\\quad\n\\frac{\\partial \\mathcal{L}}{\\partial c_{2}}=4\\sigma^{2}c_{2}-\\lambda=0,\\quad\n\\frac{\\partial \\mathcal{L}}{\\partial c_{3}}=6\\sigma^{2}c_{3}-\\lambda=0,\\quad\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda}=-(c_{1}+c_{2}+c_{3}-1)=0.\n$$\nFrom the first three equations,\n$$\nc_{1}=\\frac{\\lambda}{2\\sigma^{2}},\\quad c_{2}=\\frac{\\lambda}{4\\sigma^{2}},\\quad c_{3}=\\frac{\\lambda}{6\\sigma^{2}}.\n$$\nImpose the constraint:\n$$\n\\frac{\\lambda}{2\\sigma^{2}}+\\frac{\\lambda}{4\\sigma^{2}}+\\frac{\\lambda}{6\\sigma^{2}}=1\n\\;\\;\\Longrightarrow\\;\\;\n\\frac{\\lambda}{\\sigma^{2}}\\left(\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{6}\\right)=1\n\\;\\;\\Longrightarrow\\;\\;\n\\frac{\\lambda}{\\sigma^{2}}\\cdot\\frac{11}{12}=1\n\\;\\;\\Longrightarrow\\;\\;\n\\lambda=\\frac{12\\sigma^{2}}{11}.\n$$\nTherefore,\n$$\nc_{1}=\\frac{\\lambda}{2\\sigma^{2}}=\\frac{12\\sigma^{2}/11}{2\\sigma^{2}}=\\frac{6}{11}.\n$$\nThis is the BLUE weight on $y_{1}$, consistent with inverse-variance weighting.", "answer": "$$\\boxed{\\frac{6}{11}}$$", "id": "1919578"}, {"introduction": "Having explored how to optimally estimate a single constant, we now apply these principles to the more general case of linear regression. The Gauss-Markov theorem famously states that the Ordinary Least Squares (OLS) estimator is a BLUE under a set of specific assumptions. This practice [@problem_id:1919549] provides a concrete opportunity to quantify the \"best\" by calculating the variance of the OLS slope estimator, demonstrating how the precision of our model's parameters depends on both the underlying error variance and the design of the experiment itself.", "problem": "In an introductory physics laboratory, a student investigates the relationship between the elongation of a spring and the mass attached to it. The student posits a simple linear model relating the dependent variable, $y_i$, to an independent variable, $x_i$. The model is given by $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$ for $i=1, 2, 3$. Here, $\\beta_0$ and $\\beta_1$ are the unknown intercept and slope parameters, respectively. The term $\\epsilon_i$ represents the random measurement error for the $i$-th observation.\n\nThe standard assumptions of the classical linear regression model hold:\n1. The expected value of the error term is zero: $E[\\epsilon_i] = 0$.\n2. The errors have a constant variance (homoscedasticity): $\\text{Var}(\\epsilon_i) = \\sigma^2$ for some positive constant $\\sigma^2$.\n3. The errors are uncorrelated with each other: $\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0$ for all $i \\neq j$.\n\nThe student performs three measurements, choosing to set the independent variable $x$ to the specific values $x_1 = -1$, $x_2 = 0$, and $x_3 = 1$. The parameters $\\beta_0$ and $\\beta_1$ are estimated using the method of Ordinary Least Squares (OLS). According to the Gauss-Markov theorem, the OLS estimator for the slope, denoted $\\hat{\\beta_1}$, has the minimum variance among all linear unbiased estimators.\n\nDetermine the variance of the OLS estimator $\\hat{\\beta_1}$. Express your answer as an analytic expression in terms of the error variance $\\sigma^2$.", "solution": "We model the data as $y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\epsilon_{i}$ with $E[\\epsilon_{i}]=0$, $\\text{Var}(\\epsilon_{i})=\\sigma^{2}$, and $\\text{Cov}(\\epsilon_{i},\\epsilon_{j})=0$ for $i\\neq j$. In matrix form, let $y=X\\beta+\\epsilon$ with\n$$\nX=\\begin{pmatrix}\n1  x_{1}\\\\\n1  x_{2}\\\\\n1  x_{3}\n\\end{pmatrix},\\quad\n\\beta=\\begin{pmatrix}\\beta_{0}\\\\ \\beta_{1}\\end{pmatrix},\\quad\n\\epsilon=\\begin{pmatrix}\\epsilon_{1}\\\\ \\epsilon_{2}\\\\ \\epsilon_{3}\\end{pmatrix}.\n$$\nThe OLS estimator is $\\hat{\\beta}=(X^{\\top}X)^{-1}X^{\\top}y$, and under the stated assumptions its covariance matrix is\n$$\n\\text{Var}(\\hat{\\beta})=\\sigma^{2}(X^{\\top}X)^{-1}.\n$$\nWith $x_{1}=-1$, $x_{2}=0$, $x_{3}=1$, we compute\n$$\nX^{\\top}X=\\begin{pmatrix}\n\\sum_{i=1}^{3}1  \\sum_{i=1}^{3}x_{i}\\\\\n\\sum_{i=1}^{3}x_{i}  \\sum_{i=1}^{3}x_{i}^{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n3  (-1)+0+1\\\\\n(-1)+0+1  (-1)^{2}+0^{2}+1^{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n3  0\\\\\n0  2\n\\end{pmatrix}.\n$$\nSince $X^{\\top}X$ is diagonal, its inverse is\n$$\n(X^{\\top}X)^{-1}=\\begin{pmatrix}\n\\frac{1}{3}  0\\\\\n0  \\frac{1}{2}\n\\end{pmatrix}.\n$$\nTherefore,\n$$\n\\text{Var}(\\hat{\\beta})=\\sigma^{2}\\begin{pmatrix}\n\\frac{1}{3}  0\\\\\n0  \\frac{1}{2}\n\\end{pmatrix},\n$$\nso the variance of the OLS slope estimator is the $(2,2)$ entry,\n$$\n\\text{Var}(\\hat{\\beta}_{1})=\\sigma^{2}\\cdot\\frac{1}{2}=\\frac{\\sigma^{2}}{2}.\n$$", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{2}}$$", "id": "1919549"}]}