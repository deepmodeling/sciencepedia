{"hands_on_practices": [{"introduction": "A common question when first encountering the error variance estimator $S^2$ is why the denominator is $n-2$ and not the more familiar $n-1$ used for a simple sample variance. This practice invites you to investigate this question directly by calculating the expected value of an estimator that uses $n-1$ [@problem_id:1915695]. By working through this, you will uncover the subtle but crucial reason for the adjustment: the residuals from a regression are not as \"free\" as the original data points, and understanding this bias is key to correct statistical inference.", "problem": "A junior data scientist is working with a Simple Linear Regression (SLR) model to analyze experimental data. The model is given by $Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ for $i=1, \\dots, n$, where the $x_i$ are fixed non-random predictors, not all equal, and the random errors $\\varepsilon_i$ are independent with $E[\\varepsilon_i] = 0$ and $V(\\varepsilon_i) = \\sigma^2$.\n\nThe parameters $\\beta_0$ and $\\beta_1$ are estimated using the method of ordinary least squares, yielding the estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$. The fitted values are $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$, and the residuals are $e_i = y_i - \\hat{y}_i$.\n\nThe standard unbiased estimator for the error variance $\\sigma^2$ is known to be $S^2 = \\frac{1}{n-2}\\sum_{i=1}^n e_i^2$. However, recalling the formula for sample variance from an introductory statistics course, the analyst considers using a different denominator and proposes an alternative estimator for the error variance:\n$$ \\hat{\\sigma}^2_{alt} = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$\n\nTo evaluate this alternative estimator, your task is to determine its expected value. Express $E[\\hat{\\sigma}^2_{alt}]$ as an expression in terms of the sample size $n$ and the true error variance $\\sigma^2$.\n\nFor your derivation, you may use the following standard results from the theory of simple linear regression without proof:\n1. The least squares estimator $\\hat{\\beta}_1$ is an unbiased estimator of $\\beta_1$.\n2. The variance of the slope estimator is $V(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2}$.\n3. The sum of squared residuals (also called sum of squared errors or SSE) can be decomposed as $\\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n(y_i - \\bar{y})^2 - \\hat{\\beta}_1^2 \\sum_{i=1}^n(x_i - \\bar{x})^2$.", "solution": "We are asked to compute the expectation of the alternative estimator\n$$\n\\hat{\\sigma}^{2}_{alt}=\\frac{1}{n-1}\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}=\\frac{1}{n-1}\\,\\mathrm{SSE},\n$$\nso\n$$\nE[\\hat{\\sigma}^{2}_{alt}]=\\frac{1}{n-1}\\,E[\\mathrm{SSE}].\n$$\nUsing the given decomposition,\n$$\n\\mathrm{SSE}=\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}-\\hat{\\beta}_{1}^{2}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2},\n$$\nlet $S_{xx}=\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}$. Then\n$$\nE[\\mathrm{SSE}]=E\\!\\left[\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}\\right]-E[\\hat{\\beta}_{1}^{2}]\\,S_{xx}.\n$$\n\nCompute $E\\!\\left[\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}\\right]$. Under $y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\varepsilon_{i}$ with $\\bar{y}=\\beta_{0}+\\beta_{1}\\bar{x}+\\bar{\\varepsilon}$ and $\\bar{\\varepsilon}=\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_{i}$, we have\n$$\ny_{i}-\\bar{y}=\\beta_{1}(x_{i}-\\bar{x})+(\\varepsilon_{i}-\\bar{\\varepsilon}).\n$$\nTherefore,\n$$\n\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}=\\beta_{1}^{2}S_{xx}+2\\beta_{1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(\\varepsilon_{i}-\\bar{\\varepsilon})+\\sum_{i=1}^{n}(\\varepsilon_{i}-\\bar{\\varepsilon})^{2}.\n$$\nTaking expectations and using $E[\\varepsilon_{i}]=0$, independence of errors, and fixed $x_{i}$ gives\n$$\nE\\!\\left[\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}\\right]=\\beta_{1}^{2}S_{xx}+E\\!\\left[\\sum_{i=1}^{n}(\\varepsilon_{i}-\\bar{\\varepsilon})^{2}\\right].\n$$\nNow\n$$\n\\sum_{i=1}^{n}(\\varepsilon_{i}-\\bar{\\varepsilon})^{2}=\\sum_{i=1}^{n}\\varepsilon_{i}^{2}-n\\bar{\\varepsilon}^{2},\n$$\nso\n$$\nE\\!\\left[\\sum_{i=1}^{n}(\\varepsilon_{i}-\\bar{\\varepsilon})^{2}\\right]=n\\sigma^{2}-n\\,\\mathrm{Var}(\\bar{\\varepsilon})=n\\sigma^{2}-n\\frac{\\sigma^{2}}{n}=(n-1)\\sigma^{2}.\n$$\nThus\n$$\nE\\!\\left[\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}\\right]=\\beta_{1}^{2}S_{xx}+(n-1)\\sigma^{2}.\n$$\n\nNext, by the given results,\n$$\nE[\\hat{\\beta}_{1}]=\\beta_{1},\\qquad \\mathrm{Var}(\\hat{\\beta}_{1})=\\frac{\\sigma^{2}}{S_{xx}},\n$$\nso\n$$\nE[\\hat{\\beta}_{1}^{2}]=\\mathrm{Var}(\\hat{\\beta}_{1})+(E[\\hat{\\beta}_{1}])^{2}=\\frac{\\sigma^{2}}{S_{xx}}+\\beta_{1}^{2},\n$$\nand therefore\n$$\nE[\\hat{\\beta}_{1}^{2}]\\,S_{xx}=\\sigma^{2}+\\beta_{1}^{2}S_{xx}.\n$$\n\nCombine the pieces:\n$$\nE[\\mathrm{SSE}]=\\bigl(\\beta_{1}^{2}S_{xx}+(n-1)\\sigma^{2}\\bigr)-\\bigl(\\sigma^{2}+\\beta_{1}^{2}S_{xx}\\bigr)=(n-2)\\sigma^{2}.\n$$\nHence,\n$$\nE[\\hat{\\sigma}^{2}_{alt}]=\\frac{1}{n-1}E[\\mathrm{SSE}]=\\frac{n-2}{n-1}\\,\\sigma^{2}.\n$$", "answer": "$$\\boxed{\\frac{n-2}{n-1}\\sigma^{2}}$$", "id": "1915695"}, {"introduction": "Having established why the denominator for a standard simple linear regression is $n-2$, let's explore a different scenario: a regression that is forced to pass through the origin [@problem_id:1915696]. This model, $Y_i = \\beta_1 x_i + \\epsilon_i$, is simpler because it only requires estimating one parameter. This practice challenges you to apply the concept of degrees of freedom to determine the correct unbiased estimator for $\\sigma^2$ in this new context, reinforcing the general principle that links the estimator's form to the model's complexity.", "problem": "A physics student is investigating Hooke's Law, which states that the force $F$ required to stretch or compress a spring by some distance $x$ from its equilibrium position is directly proportional to that distance. The theoretical model for this relationship is $F = \\beta_1 x$, where $\\beta_1$ is the spring constant.\n\nTo estimate the spring constant, the student collects $n$ pairs of measurements $(x_i, y_i)$, where $x_i$ is the displacement and $y_i$ is the measured force. The student proposes a regression-through-the-origin model to describe the data:\n$$Y_i = \\beta_1 x_i + \\epsilon_i \\quad \\text{for } i = 1, 2, \\dots, n$$\nHere, the explanatory variables $x_i$ are treated as fixed, non-random constants, and not all $x_i$ are zero. The error terms $\\epsilon_i$ are assumed to be independent and identically distributed random variables with a mean $E[\\epsilon_i] = 0$ and a constant variance $Var(\\epsilon_i) = \\sigma^2$. The parameter $\\sigma^2$ represents the variance of the measurement error in the force.\n\nThe least squares estimator for the spring constant $\\beta_1$ is given by:\n$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n x_i Y_i}{\\sum_{i=1}^n x_i^2}$$\n\nTo estimate the error variance $\\sigma^2$, the student considers an estimator of the form:\n$$S^2 = k \\sum_{i=1}^n (Y_i - \\hat{\\beta}_1 x_i)^2$$\nwhere $k$ is a constant that may depend on the sample size $n$.\n\nFind the value of $k$ that makes $S^2$ an unbiased estimator for $\\sigma^2$. Express your answer as a function of $n$.", "solution": "We write the model in matrix form. Let $Y=(Y_{1},\\dots,Y_{n})'$, $X=(x_{1},\\dots,x_{n})'$ (an $n\\times 1$ matrix), $\\beta=\\beta_{1}$, and $\\epsilon=(\\epsilon_{1},\\dots,\\epsilon_{n})'$. Then\n$$\nY=X\\beta+\\epsilon,\\quad E[\\epsilon]=0,\\quad \\operatorname{Var}(\\epsilon)=\\sigma^{2}I_{n},\n$$\nwith $X$ nonzero so that $X'X>0$. The least squares estimator is\n$$\n\\hat{\\beta}=(X'X)^{-1}X'Y,\n$$\nand the residual vector is\n$$\ne=Y-X\\hat{\\beta}=(I_{n}-H)Y,\\quad H=X(X'X)^{-1}X'.\n$$\nUsing $Y=X\\beta+\\epsilon$ and $(I_{n}-H)X=0$, we obtain\n$$\ne=(I_{n}-H)\\epsilon.\n$$\nThe residual sum of squares is\n$$\n\\operatorname{RSS}=e'e=\\epsilon'(I_{n}-H)\\epsilon.\n$$\nFor any constant symmetric matrix $A$, the identity\n$$\nE[\\epsilon'A\\epsilon]=\\operatorname{tr}\\!\\big(A\\,\\operatorname{Var}(\\epsilon)\\big)+E[\\epsilon]'A\\,E[\\epsilon]\n$$\nholds. With $A=I_{n}-H$, $E[\\epsilon]=0$, and $\\operatorname{Var}(\\epsilon)=\\sigma^{2}I_{n}$, we get\n$$\nE[\\operatorname{RSS}]=\\sigma^{2}\\operatorname{tr}(I_{n}-H)=\\sigma^{2}\\big(n-\\operatorname{tr}(H)\\big).\n$$\nThe hat matrix $H$ is an idempotent projection of rank $p=1$ because $X$ has rank $1$ (not all $x_{i}$ are zero). Hence $\\operatorname{tr}(H)=1$, so\n$$\nE[\\operatorname{RSS}]=(n-1)\\sigma^{2}.\n$$\nThe proposed estimator is $S^{2}=k\\,\\operatorname{RSS}$. Its expectation is\n$$\nE[S^{2}]=k\\,(n-1)\\sigma^{2}.\n$$\nUnbiasedness, $E[S^{2}]=\\sigma^{2}$, therefore requires\n$$\nk=\\frac{1}{n-1}.\n$$", "answer": "$$\\boxed{\\frac{1}{n-1}}$$", "id": "1915696"}, {"introduction": "To truly solidify your intuition about degrees of freedom, we now turn to a thought experiment that pushes the concept to its limit [@problem_id:1915683]. Consider the case of fitting a standard two-parameter regression line to just two data points. This exercise asks you to determine what happens to the sum of squared errors and the error variance estimate in this minimal scenario, providing a definitive and memorable illustration of why we need \"room\" in our data to estimate variability.", "problem": "An analyst is studying the relationship between two variables, $X$ and $Y$. They collect a small dataset consisting of only two data points: $(x_1, y_1) = (10, 25)$ and $(x_2, y_2) = (30, 35)$.\n\nThe analyst fits a simple linear regression model of the form $Y = \\beta_0 + \\beta_1 X + \\epsilon$ to this dataset, where $\\epsilon$ represents the random error term. The procedure involves finding estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ for the model parameters.\n\nLet $SSE$ denote the Sum of Squared Errors (also known as the residual sum of squares) for the fitted model, calculated as $SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$, where $\\hat{y}_i$ are the predicted values from the regression line and $n$ is the number of data points.\n\nLet $s^2$ be the standard unbiased estimator for the error variance, $\\sigma^2$. This estimator is typically calculated as $s^2 = \\frac{SSE}{n-k}$, where $k$ is the number of parameters estimated in the regression model.\n\nBased on fitting the simple linear regression model to the given two data points, determine the resulting values of $SSE$ and $s^2$.\n\nA. $SSE = 50$ and $s^2 = 50$\n\nB. $SSE > 0$ and $s^2 > 0$, but their exact values cannot be determined from the information given.\n\nC. $SSE = 0$ and $s^2 = 0$\n\nD. $SSE = 0$ and $s^2$ is undefined\n\nE. $SSE = 100$ and $s^2$ is undefined", "solution": "We fit the simple linear regression model $Y=\\beta_{0}+\\beta_{1}X+\\epsilon$ to the two points $(x_{1},y_{1})=(10,25)$ and $(x_{2},y_{2})=(30,35)$. The ordinary least squares estimates in simple linear regression are\n$$\n\\hat{\\beta}_{1}=\\frac{S_{xy}}{S_{xx}}, \\quad \\hat{\\beta}_{0}=\\bar{y}-\\hat{\\beta}_{1}\\bar{x},\n$$\nwhere\n$$\n\\bar{x}=\\frac{x_{1}+x_{2}}{2}, \\quad \\bar{y}=\\frac{y_{1}+y_{2}}{2}, \\quad S_{xx}=\\sum_{i=1}^{2}(x_{i}-\\bar{x})^{2}, \\quad S_{xy}=\\sum_{i=1}^{2}(x_{i}-\\bar{x})(y_{i}-\\bar{y}).\n$$\nCompute the sample means:\n$$\n\\bar{x}=\\frac{10+30}{2}=20, \\quad \\bar{y}=\\frac{25+35}{2}=30.\n$$\nCompute $S_{xx}$ and $S_{xy}$:\n$$\nS_{xx}=(10-20)^{2}+(30-20)^{2}=100+100=200,\n$$\n$$\nS_{xy}=(10-20)(25-30)+(30-20)(35-30)=(-10)(-5)+(10)(5)=50+50=100.\n$$\nThus,\n$$\n\\hat{\\beta}_{1}=\\frac{100}{200}=\\frac{1}{2}, \\quad \\hat{\\beta}_{0}=30-\\frac{1}{2}\\cdot 20=20.\n$$\nThe fitted line is $\\hat{y}=20+\\frac{1}{2}x$. Predicted values at the observed $x$ are\n$$\n\\hat{y}_{1}=20+\\tfrac{1}{2}\\cdot 10=25=y_{1}, \\quad \\hat{y}_{2}=20+\\tfrac{1}{2}\\cdot 30=35=y_{2}.\n$$\nTherefore, both residuals are zero, so the sum of squared errors is\n$$\nSSE=\\sum_{i=1}^{2}(y_{i}-\\hat{y}_{i})^{2}=0.\n$$\nThe unbiased error variance estimator is defined as\n$$\ns^{2}=\\frac{SSE}{n-k},\n$$\nwith $n=2$ data points and $k=2$ estimated parameters (intercept and slope), giving $n-k=0$. Hence,\n$$\ns^{2}=\\frac{0}{0},\n$$\nwhich is undefined due to zero degrees of freedom. Consequently, the correct choice is that $SSE=0$ and $s^{2}$ is undefined.", "answer": "$$\\boxed{D}$$", "id": "1915683"}]}