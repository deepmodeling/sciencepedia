{"hands_on_practices": [{"introduction": "This first practice tests your understanding of the core definition of consistency. Starting with the sample mean, which the Law of Large Numbers tells us is a consistent estimator for the population mean, you will investigate how simple modifications affect this crucial property. By analyzing several estimators in [@problem_id:1909315], you will sharpen your ability to determine whether an estimator converges to the true parameter as the sample size $n$ approaches infinity.", "problem": "Let $X_1, X_2, \\dots, X_n$ be an independent and identically distributed (i.i.d.) random sample from a probability distribution with a finite mean $\\mu$ and a finite, positive variance $\\sigma^2 > 0$. Let $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample mean.\n\nAn estimator for a parameter is said to be consistent if it converges in probability to the true value of the parameter as the sample size $n$ approaches infinity.\n\nConsider the following four estimators for the population mean $\\mu$.\n\nA. $\\hat{\\mu}_A = \\bar{X}_n + \\frac{1}{\\sqrt{n}}$\n\nB. $\\hat{\\mu}_B = \\frac{n}{n+2} \\bar{X}_n$\n\nC. $\\hat{\\mu}_C = \\frac{1}{2}(\\bar{X}_n + X_1)$\n\nD. $\\hat{\\mu}_D = \\bar{X}_n + \\frac{\\cos(n\\pi)}{n}$\n\nWhich of the above estimators are consistent for $\\mu$? Select all that apply.", "solution": "By the weak law of large numbers, $\\bar{X}_{n} \\xrightarrow{p} \\mu$ since the $X_{i}$ are i.i.d. with finite mean $\\mu$ and finite positive variance $\\sigma^{2}>0$. We analyze each estimator using this fact and Slutsky’s theorem.\n\nA. $\\hat{\\mu}_{A}=\\bar{X}_{n}+\\frac{1}{\\sqrt{n}}$. Write\n$$\n\\hat{\\mu}_{A}-\\mu=(\\bar{X}_{n}-\\mu)+\\frac{1}{\\sqrt{n}}.\n$$\nWe have $\\bar{X}_{n}-\\mu \\xrightarrow{p} 0$ and $\\frac{1}{\\sqrt{n}} \\to 0$ deterministically. By Slutsky’s theorem (sum of a sequence converging in probability to $0$ and a deterministic sequence converging to $0$), $\\hat{\\mu}_{A} \\xrightarrow{p} \\mu$. Hence $\\hat{\\mu}_{A}$ is consistent.\n\nB. $\\hat{\\mu}_{B}=\\frac{n}{n+2}\\bar{X}_{n}$. Note that $\\frac{n}{n+2} \\to 1$ deterministically. Then\n$$\n\\hat{\\mu}_{B}=\\frac{n}{n+2}\\bar{X}_{n} \\xrightarrow{p} 1 \\cdot \\mu=\\mu\n$$\nby Slutsky’s theorem (product of a sequence converging in probability and a deterministic sequence converging to a constant). More explicitly,\n$$\n\\hat{\\mu}_{B}-\\mu=\\frac{n}{n+2}(\\bar{X}_{n}-\\mu)+\\left(\\frac{n}{n+2}-1\\right)\\mu,\n$$\nwhere the first term converges in probability to $0$ and the second term converges deterministically to $0$. Thus $\\hat{\\mu}_{B}$ is consistent.\n\nC. $\\hat{\\mu}_{C}=\\frac{1}{2}(\\bar{X}_{n}+X_{1})$. Consider, for any $\\epsilon>0$,\n$$\n\\left|\\hat{\\mu}_{C}-\\mu\\right|=\\left|\\frac{1}{2}(\\bar{X}_{n}-\\mu)+\\frac{1}{2}(X_{1}-\\mu)\\right|\\ge \\frac{1}{2}|X_{1}-\\mu|-\\frac{1}{2}|\\bar{X}_{n}-\\mu|.\n$$\nHence, on the event $\\{|X_{1}-\\mu|>3\\epsilon,\\ |\\bar{X}_{n}-\\mu|<\\epsilon\\}$, we have $\\left|\\hat{\\mu}_{C}-\\mu\\right|>\\epsilon$. Therefore,\n$$\n\\mathbb{P}\\left(\\left|\\hat{\\mu}_{C}-\\mu\\right|>\\epsilon\\right)\\ge \\mathbb{P}\\left(|X_{1}-\\mu|>3\\epsilon,\\ |\\bar{X}_{n}-\\mu|<\\epsilon\\right)\\ge \\mathbb{P}\\left(|X_{1}-\\mu|>3\\epsilon\\right)-\\mathbb{P}\\left(|\\bar{X}_{n}-\\mu|\\ge \\epsilon\\right).\n$$\nBy the weak law, $\\mathbb{P}\\left(|\\bar{X}_{n}-\\mu|\\ge \\epsilon\\right)\\to 0$. Since $\\sigma^{2}>0$, the distribution of $X_{1}$ is non-degenerate, so there exists some $\\epsilon>0$ such that $\\mathbb{P}\\left(|X_{1}-\\mu|>3\\epsilon\\right)>0$. For such $\\epsilon$, the lower bound above stays bounded away from $0$, so $\\mathbb{P}\\left(\\left|\\hat{\\mu}_{C}-\\mu\\right|>\\epsilon\\right)\\not\\to 0$. Thus $\\hat{\\mu}_{C}$ is not consistent.\n\nD. $\\hat{\\mu}_{D}=\\bar{X}_{n}+\\frac{\\cos(n\\pi)}{n}$. Note that $\\cos(n\\pi)=(-1)^{n}$, so $\\frac{\\cos(n\\pi)}{n}\\to 0$ deterministically. Then\n$$\n\\hat{\\mu}_{D}-\\mu=(\\bar{X}_{n}-\\mu)+\\frac{\\cos(n\\pi)}{n},\n$$\nwith the first term converging in probability to $0$ and the second term converging deterministically to $0$. By Slutsky’s theorem, $\\hat{\\mu}_{D} \\xrightarrow{p} \\mu$. Hence $\\hat{\\mu}_{D}$ is consistent.\n\nTherefore, the consistent estimators are A, B, and D.", "answer": "$$\\boxed{ABD}$$", "id": "1909315"}, {"introduction": "Many important estimators, including maximum likelihood estimators, are functions of sample averages. This exercise [@problem_id:1909316] demonstrates how to prove consistency for such estimators using the Continuous Mapping Theorem, a vital tool in a statistician's toolkit. You will see how this theorem, combined with the Law of Large Numbers, provides a clear and rigorous argument for why the estimator for the failure rate of a fiber optic cable is consistent.", "problem": "A materials scientist is studying the lifetime of a new type of fiber optic cable. The time to failure, $X$, for a segment of this cable is modeled by an Exponential distribution with a constant failure rate $\\lambda > 0$. The probability density function (PDF) for the lifetime is given by $f(x; \\lambda) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$. To estimate the failure rate, the scientist tests a random sample of $n$ independent cable segments, measuring their lifetimes $X_1, X_2, \\ldots, X_n$.\n\nAn estimator for $\\lambda$ is proposed based on the sample mean lifetime, $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$. The proposed estimator is $\\hat{\\lambda}_n = \\frac{1}{\\bar{X}_n}$.\n\nThe scientist wants to establish that this is a good estimator in the sense that it becomes more accurate as the sample size $n$ increases. This property is known as consistency. An estimator $\\hat{\\theta}_n$ is said to be a consistent estimator for a parameter $\\theta$ if $\\hat{\\theta}_n$ converges in probability to $\\theta$ as $n \\to \\infty$.\n\nWhich of the following statements provides the correct reasoning to prove that $\\hat{\\lambda}_n = 1/\\bar{X}_n$ is a consistent estimator for $\\lambda$?\n\nA. The estimator is consistent because the Central Limit Theorem states that for large $n$, the sampling distribution of $\\hat{\\lambda}_n$ is approximately Normal with a mean of $\\lambda$.\n\nB. The estimator is consistent because the Law of Large Numbers states that the sample mean $\\bar{X}_n$ converges in probability to the true failure rate $\\lambda$, and therefore $\\hat{\\lambda}_n = 1/\\bar{X}_n$ must converge in probability to $1/\\lambda$.\n\nC. The estimator is consistent because it can be shown to be an unbiased estimator of $\\lambda$, meaning that its expected value $E[\\hat{\\lambda}_n]$ is exactly equal to $\\lambda$ for any sample size $n$.\n\nD. The estimator is consistent because its variance, $\\text{Var}(\\hat{\\lambda}_n)$, can be shown to approach zero as the sample size $n$ approaches infinity. While this fact is true, it is not, by itself, a complete justification for consistency.\n\nE. The estimator is consistent because the Law of Large Numbers ensures that the sample mean $\\bar{X}_n$ converges in probability to the true mean lifetime, which is $1/\\lambda$. Since the function $g(y) = 1/y$ is continuous for $y \\neq 0$, the Continuous Mapping Theorem then guarantees that $\\hat{\\lambda}_n = g(\\bar{X}_n)$ converges in probability to $g(1/\\lambda) = \\lambda$.", "solution": "To determine the correct justification for the consistency of the estimator $\\hat{\\lambda}_n = 1/\\bar{X}_n$, we must follow the formal definition of consistency and apply the relevant theorems from probability theory.\n\nAn estimator $\\hat{\\theta}_n$ is consistent for a parameter $\\theta$ if it converges in probability to $\\theta$ as the sample size $n$ approaches infinity. We write this as $\\hat{\\theta}_n \\xrightarrow{p} \\theta$. We need to show that $\\hat{\\lambda}_n \\xrightarrow{p} \\lambda$.\n\nFirst, let's identify the properties of the random sample $X_1, X_2, \\ldots, X_n$. The variables are independent and identically distributed (i.i.d.) from an Exponential distribution with rate $\\lambda$. The expected value (or mean) of an Exponentially distributed random variable with rate $\\lambda$ is $E[X] = 1/\\lambda$. The variance is $\\text{Var}(X) = 1/\\lambda^2$. Since the rate $\\lambda > 0$, the mean $1/\\lambda$ is finite.\n\nThe core of the argument for consistency often involves the Law of Large Numbers. The Weak Law of Large Numbers (WLLN) states that for a sequence of i.i.d. random variables with a finite mean $\\mu$, the sample mean $\\bar{X}_n$ converges in probability to $\\mu$.\nIn our case, $\\mu = E[X] = 1/\\lambda$. Therefore, by the WLLN:\n$$ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\xrightarrow{p} E[X] = \\frac{1}{\\lambda} \\quad \\text{as } n \\to \\infty $$\n\nOur estimator is not $\\bar{X}_n$, but a function of it: $\\hat{\\lambda}_n = 1/\\bar{X}_n$. To handle the convergence of a function of a random variable, we use the Continuous Mapping Theorem (CMT). The CMT states that if a sequence of random variables $Y_n$ converges in probability to a constant $c$ (i.e., $Y_n \\xrightarrow{p} c$), and if $g$ is a function that is continuous at the point $c$, then the sequence of transformed random variables $g(Y_n)$ converges in probability to $g(c)$ (i.e., $g(Y_n) \\xrightarrow{p} g(c)$).\n\nIn this problem, our sequence of random variables is $Y_n = \\bar{X}_n$, and it converges in probability to the constant $c = 1/\\lambda$. Our function is $g(y) = 1/y$. This function is continuous for all $y \\neq 0$. Since $\\lambda > 0$, the constant $c = 1/\\lambda$ is also greater than 0, so the function $g(y)$ is continuous at $c=1/\\lambda$.\n\nApplying the Continuous Mapping Theorem:\n$$ \\hat{\\lambda}_n = g(\\bar{X}_n) \\xrightarrow{p} g\\left(\\frac{1}{\\lambda}\\right) $$\nNow, we evaluate $g(1/\\lambda)$:\n$$ g\\left(\\frac{1}{\\lambda}\\right) = \\frac{1}{(1/\\lambda)} = \\lambda $$\nThus, we have shown that $\\hat{\\lambda}_n \\xrightarrow{p} \\lambda$, which is the definition of consistency.\n\nThis line of reasoning perfectly matches option E.\n\nLet's evaluate why the other options are incorrect:\n- **A:** The Central Limit Theorem describes convergence in *distribution* (to a Normal distribution), not convergence in *probability* (to a constant). Consistency is defined by convergence in probability. This reasoning is flawed.\n- **B:** This statement incorrectly claims that $\\bar{X}_n$ converges to $\\lambda$. The WLLN states that $\\bar{X}_n$ converges to the mean of the distribution, which is $E[X] = 1/\\lambda$, not $\\lambda$. This leads to the contradictory conclusion that $\\hat{\\lambda}_n$ converges to $1/\\lambda$.\n- **C:** This statement claims the estimator is unbiased. The property of being unbiased ($E[\\hat{\\theta}_n] = \\theta$) is distinct from consistency. While some unbiased estimators are consistent, this is not a general rule, and one property does not automatically imply the other. Furthermore, for the Exponential distribution, $E[\\hat{\\lambda}_n] = E[1/\\bar{X}_n] \\neq \\lambda$ (due to Jensen's inequality, since $1/y$ is a convex function, $E[1/\\bar{X}_n] > 1/E[\\bar{X}_n] = \\lambda$), so the premise of this statement is false.\n- **D:** This statement mentions that $\\text{Var}(\\hat{\\lambda}_n) \\to 0$. A sufficient condition for consistency is that both the bias and the variance of the estimator approach zero as $n \\to \\infty$. Stating that only the variance approaches zero is incomplete. One must also show that the bias, $B(\\hat{\\lambda}_n) = E[\\hat{\\lambda}_n] - \\lambda$, goes to zero. While it is true that both conditions hold for this estimator, the statement that the variance approaching zero is a *complete justification by itself* is false. The reasoning in option E is more direct and fundamental.\n\nTherefore, the only correct and complete justification among the choices is E.", "answer": "$$\\boxed{E}$$", "id": "1909316"}, {"introduction": "An estimator can be consistent, yet consistently wrong. This final practice [@problem_id:1909343] presents a cautionary tale, asking you to evaluate an intuitive but flawed estimator for the population variance. The exercise highlights a common pitfall: an estimator might converge in probability, but not to the parameter you are trying to estimate, teaching you to always verify the limit with care.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sample of independent and identically distributed (i.i.d.) random variables drawn from a population. The population distribution has a finite, non-zero mean $E[X_i] = \\mu$ and a finite, positive variance $Var(X_i) = \\sigma^2$.\n\nConsider the estimator $\\hat{\\theta}_n$ defined as the uncentered second sample moment:\n$$\n\\hat{\\theta}_n = \\frac{1}{n} \\sum_{i=1}^n X_i^2\n$$\n\nWhich of the following statements correctly describes the properties of $\\hat{\\theta}_n$ as an estimator for the population variance $\\sigma^2$?\n\nA. Yes, $\\hat{\\theta}_n$ is a consistent estimator for $\\sigma^2$ because its expected value converges to $\\sigma^2$ as $n \\to \\infty$.\n\nB. Yes, $\\hat{\\theta}_n$ is a consistent estimator for $\\sigma^2$ because it converges in probability to $\\sigma^2$.\n\nC. No, $\\hat{\\theta}_n$ is not a consistent estimator for $\\sigma^2$ because it does not converge in probability to $\\sigma^2$.\n\nD. No, $\\hat{\\theta}_n$ is not a consistent estimator for $\\sigma^2$ because the estimator is biased for any finite sample size $n$.\n\nE. The consistency of $\\hat{\\theta}_n$ for $\\sigma^2$ cannot be determined without knowing the full distribution of the $X_i$.", "solution": "We are given i.i.d. random variables $X_{1},\\dots,X_{n}$ with $E[X_{i}]=\\mu$ and $\\operatorname{Var}(X_{i})=\\sigma^{2}$, and the estimator $\\hat{\\theta}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}^{2}$. We evaluate its properties as an estimator for $\\sigma^{2}$.\n\nFirst, compute its expectation using the identity $E[X^{2}]=\\operatorname{Var}(X)+\\{E[X]\\}^{2}$:\n$$\nE[\\hat{\\theta}_{n}]=E\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}^{2}\\right]=E[X_{1}^{2}]=\\sigma^{2}+\\mu^{2}.\n$$\nThus, $E[\\hat{\\theta}_{n}]\\neq\\sigma^{2}$ unless $\\mu=0$. The expectation does not depend on $n$, so it does not converge to $\\sigma^{2}$ as $n\\to\\infty$ unless $\\mu=0$. Therefore statement A is false.\n\nNext, consider convergence in probability. Since $E[X_{1}^{2}]<\\infty$ (because $\\operatorname{Var}(X_{1})<\\infty$), the Weak Law of Large Numbers applies to the sequence $X_{i}^{2}$, yielding\n$$\n\\hat{\\theta}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}^{2}\\xrightarrow{p}E[X_{1}^{2}]=\\sigma^{2}+\\mu^{2}.\n$$\nTherefore $\\hat{\\theta}_{n}$ converges in probability to $\\sigma^{2}+\\mu^{2}$, not to $\\sigma^{2}$ (unless $\\mu=0$). Hence statement B is false and statement C is true.\n\nRegarding bias, $\\hat{\\theta}_{n}$ is biased for $\\sigma^{2}$ for any finite $n$ unless $\\mu=0$, but bias at finite $n$ does not by itself imply inconsistency in general. In this problem, inconsistency follows from the convergence-in-probability limit being $\\sigma^{2}+\\mu^{2}\\neq\\sigma^{2}$. Thus the rationale in D is not the correct reason for inconsistency, making D an incorrect description.\n\nFinally, the given moment conditions are sufficient to determine the limit $E[X^{2}]=\\sigma^{2}+\\mu^{2}$, so full knowledge of the distribution is unnecessary. Hence E is false.\n\nTherefore, the correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "1909343"}]}