{"hands_on_practices": [{"introduction": "How do we measure the \"goodness\" of an estimator? This practice introduces a fundamental benchmark for the performance of any unbiased estimator: the Cramér-Rao Lower Bound (CRLB). By calculating the CRLB for the rate parameter $\\lambda$ of an exponential distribution and comparing it to the variance of a proposed estimator, you will compute its efficiency [@problem_id:1914827]. This exercise provides a concrete method for quantifying how close an estimator comes to achieving the theoretical limit of precision.", "problem": "The lifetime of a certain type of electronic component is modeled by an exponential distribution with a probability density function (PDF) given by $f(x; \\lambda) = \\lambda \\exp(-\\lambda x)$ for $x > 0$. Here, $\\lambda > 0$ is the constant failure rate. A random sample of $n$ such components, denoted by $X_1, X_2, \\dots, X_n$, is tested, and their lifetimes are recorded.\n\nAn engineer proposes the following estimator for the failure rate $\\lambda$:\n$$\n\\hat{\\lambda} = \\frac{n-1}{\\sum_{i=1}^n X_i}\n$$\nIt is given that for a sample size $n > 1$, this estimator is unbiased for $\\lambda$.\n\nAssuming a sample size of $n > 2$, calculate the efficiency of this estimator $\\hat{\\lambda}$. The efficiency is defined as the ratio of the Cramér-Rao Lower Bound (CRLB) for an unbiased estimator of $\\lambda$ to the actual variance of the estimator $\\hat{\\lambda}$. Express your final answer as a function of $n$.", "solution": "Let $X_{1},\\dots,X_{n}$ be iid with density $f(x;\\lambda)=\\lambda\\exp(-\\lambda x)$ for $x>0$. The sum $S=\\sum_{i=1}^{n}X_{i}$ has a Gamma distribution with shape $n$ and rate $\\lambda$, i.e., with density\n$$\ng(s;n,\\lambda)=\\frac{\\lambda^{n}}{\\Gamma(n)}s^{n-1}\\exp(-\\lambda s),\\quad s>0.\n$$\nThe proposed estimator is $\\hat{\\lambda}=(n-1)/S$. For $n>2$, we compute $\\operatorname{Var}(\\hat{\\lambda})$ by using moments of $S^{-1}$. For $m<n$,\n$$\n\\mathbb{E}\\!\\left[S^{-m}\\right]=\\int_{0}^{\\infty}s^{-m}g(s;n,\\lambda)\\,ds=\\frac{\\lambda^{m}\\Gamma(n-m)}{\\Gamma(n)}.\n$$\nThus,\n$$\n\\mathbb{E}\\!\\left[\\frac{1}{S}\\right]=\\frac{\\lambda\\,\\Gamma(n-1)}{\\Gamma(n)}=\\frac{\\lambda}{n-1},\\qquad\n\\mathbb{E}\\!\\left[\\frac{1}{S^{2}}\\right]=\\frac{\\lambda^{2}\\Gamma(n-2)}{\\Gamma(n)}=\\frac{\\lambda^{2}}{(n-1)(n-2)}.\n$$\nTherefore,\n$$\n\\operatorname{Var}\\!\\left(\\frac{1}{S}\\right)=\\mathbb{E}\\!\\left[\\frac{1}{S^{2}}\\right]-\\left(\\mathbb{E}\\!\\left[\\frac{1}{S}\\right]\\right)^{2}\n=\\frac{\\lambda^{2}}{(n-1)(n-2)}-\\frac{\\lambda^{2}}{(n-1)^{2}}\n=\\frac{\\lambda^{2}}{(n-1)^{2}(n-2)}.\n$$\nIt follows that\n$$\n\\operatorname{Var}(\\hat{\\lambda})=\\operatorname{Var}\\!\\left(\\frac{n-1}{S}\\right)=(n-1)^{2}\\operatorname{Var}\\!\\left(\\frac{1}{S}\\right)=\\frac{\\lambda^{2}}{n-2}.\n$$\n\nNext, compute the Cramér-Rao Lower Bound for unbiased estimators of $\\lambda$. The log-likelihood for the sample is\n$$\n\\ell(\\lambda)=\\sum_{i=1}^{n}\\ln f(X_{i};\\lambda)=n\\ln\\lambda-\\lambda\\sum_{i=1}^{n}X_{i}.\n$$\nThen\n$$\n\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}=-\\frac{n}{\\lambda^{2}},\\quad\n\\mathcal{I}_{n}(\\lambda)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}\\right]=\\frac{n}{\\lambda^{2}}.\n$$\nHence the CRLB for any unbiased estimator of $\\lambda$ is\n$$\n\\operatorname{CRLB}=\\frac{1}{\\mathcal{I}_{n}(\\lambda)}=\\frac{\\lambda^{2}}{n}.\n$$\n\nThe efficiency, defined as $\\operatorname{CRLB}/\\operatorname{Var}(\\hat{\\lambda})$, is therefore\n$$\n\\text{efficiency}=\\frac{\\lambda^{2}/n}{\\lambda^{2}/(n-2)}=\\frac{n-2}{n}.\n$$\nThis depends only on $n$ and is valid for $n>2$.", "answer": "$$\\boxed{\\frac{n-2}{n}}$$", "id": "1914827"}, {"introduction": "While we often seek unbiased estimators, is an unbiased estimator always the best choice? This thought-provoking exercise explores the bias-variance tradeoff by comparing the standard sample proportion with a simple, constant guess [@problem_id:1914839]. You will determine the conditions under which the constant, biased estimator actually outperforms the unbiased one by having a lower Mean Squared Error ($MSE$), revealing that a small amount of bias can sometimes be a worthy price for a large reduction in variance.", "problem": "Consider a random sample of size $n$, denoted by $X_1, X_2, \\ldots, X_n$, drawn from a Bernoulli distribution with an unknown parameter $p \\in (0, 1)$. We wish to estimate $p$ and are comparing two different estimators:\n\n1.  The sample proportion, $\\hat{p}_S = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n2.  A constant estimator, $\\hat{p}_C = \\frac{1}{2}$.\n\nThe performance of an estimator $\\hat{p}$ is judged by its Mean Squared Error (MSE), defined as $\\text{MSE}(\\hat{p}) = E\\left[ (\\hat{p} - p)^2 \\right]$. While $\\hat{p}_S$ is a standard data-driven estimator, the constant estimator $\\hat{p}_C$ can be more efficient (i.e., have a lower MSE) if the true parameter $p$ is very close to $1/2$.\n\nThere exists an open interval of $p$ values, symmetric around $1/2$, where the constant estimator $\\hat{p}_C$ has a strictly smaller MSE than the sample proportion $\\hat{p}_S$. Your task is to determine the lower boundary $p_L$ and the upper boundary $p_U$ of this interval. Express your final answer as two analytical expressions in terms of the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\text{Bernoulli}(p)$ with $p\\in(0,1)$. Consider the two estimators:\n- The sample proportion $\\hat{p}_{S}=\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$.\n- The constant estimator $\\hat{p}_{C}=\\frac{1}{2}$.\n\nBy the bias-variance decomposition, for any estimator $\\hat{p}$,\n$$\n\\text{MSE}(\\hat{p})=\\operatorname{Var}(\\hat{p})+\\left(\\operatorname{Bias}(\\hat{p})\\right)^{2}.\n$$\nFor $\\hat{p}_{S}$, we have $E[\\hat{p}_{S}]=p$ and $\\operatorname{Var}(\\hat{p}_{S})=\\frac{p(1-p)}{n}$, so\n$$\n\\text{MSE}(\\hat{p}_{S})=\\frac{p(1-p)}{n}.\n$$\nFor $\\hat{p}_{C}=\\frac{1}{2}$, the variance is zero and the bias is $\\frac{1}{2}-p$, hence\n$$\n\\text{MSE}(\\hat{p}_{C})=\\left(\\frac{1}{2}-p\\right)^{2}.\n$$\nWe seek the open interval of $p$ such that $\\text{MSE}(\\hat{p}_{C})<\\text{MSE}(\\hat{p}_{S})$, i.e.,\n$$\n\\left(p-\\frac{1}{2}\\right)^{2}<\\frac{p(1-p)}{n}.\n$$\nThe boundaries occur where equality holds:\n$$\n\\left(p-\\frac{1}{2}\\right)^{2}=\\frac{p(1-p)}{n}.\n$$\nExpanding and solving,\n$$\np^{2}-p+\\frac{1}{4}=\\frac{p-p^{2}}{n},\n$$\n$$\nn p^{2}-n p+\\frac{n}{4}=p-p^{2},\n$$\n$$\n(n+1)p^{2}-(n+1)p+\\frac{n}{4}=0.\n$$\nDivide by $(n+1)$ to obtain\n$$\np^{2}-p+\\frac{n}{4(n+1)}=0.\n$$\nThe discriminant is\n$$\n\\Delta=1-\\frac{n}{n+1}=\\frac{1}{n+1},\n$$\nso the two roots are\n$$\np=\\frac{1\\pm\\sqrt{\\frac{1}{n+1}}}{2}=\\frac{1}{2}\\pm\\frac{1}{2\\sqrt{n+1}}.\n$$\nSince the inequality is strict and the quadratic comparison reverses outside the roots, the constant estimator has strictly smaller MSE for $p$ in the open interval between these roots. Therefore, the lower and upper boundaries are\n$$\np_{L}=\\frac{1}{2}-\\frac{1}{2\\sqrt{n+1}},\\quad p_{U}=\\frac{1}{2}+\\frac{1}{2\\sqrt{n+1}}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2}-\\frac{1}{2\\sqrt{n+1}} & \\frac{1}{2}+\\frac{1}{2\\sqrt{n+1}}\\end{pmatrix}}$$", "id": "1914839"}, {"introduction": "Building on the previous concepts, we now ask: how can we systematically find the best possible unbiased estimator? This practice introduces the powerful Lehmann-Scheffé theorem to construct the Uniformly Minimum-Variance Unbiased Estimator (UMVUE). By working with a sample from a Poisson distribution, you will derive the UMVUE for the probability of a zero-count event [@problem_id:1914858], demonstrating a constructive method to achieve optimal efficiency among all unbiased estimators.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n$ from a Poisson distribution with an unknown parameter $\\lambda > 0$. The probability mass function for a single observation $X$ is given by:\n$$P(X=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}, \\quad \\text{for } k = 0, 1, 2, \\ldots$$\nA key metric for a process following this distribution is the probability of observing a zero-count event. Let this probability be denoted by $\\theta = P(X=0)$.\n\nYour task is to find the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for $\\theta$. Express your final answer as a function of the total sum of the observations, $T = \\sum_{i=1}^{n} X_i$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\operatorname{Poisson}(\\lambda)$ and let $\\theta=P(X=0)=\\exp(-\\lambda)$. We seek the UMVUE for $\\theta$ as a function of $T=\\sum_{i=1}^{n}X_{i}$.\n\nFirst, identify a complete sufficient statistic. By the additivity of independent Poisson random variables, $T\\sim\\operatorname{Poisson}(n\\lambda)$. By the factorization theorem, $T$ is sufficient for $\\lambda$. To see completeness, suppose $g$ is any function with $\\mathbb{E}_{\\lambda}[g(T)]=0$ for all $\\lambda>0$. Then\n$$\n\\sum_{t=0}^{\\infty}g(t)\\,\\exp(-n\\lambda)\\frac{(n\\lambda)^{t}}{t!}=0\\quad\\text{for all }\\lambda>0.\n$$\nMultiplying by $\\exp(n\\lambda)$ gives\n$$\n\\sum_{t=0}^{\\infty}g(t)\\,\\frac{(n\\lambda)^{t}}{t!}=0\\quad\\text{for all }\\lambda>0,\n$$\nwhich is the zero power series in $\\lambda$, hence all coefficients vanish and $g(t)=0$ for all $t$. Thus $T$ is complete sufficient.\n\nNext, take an unbiased estimator of $\\theta$: the indicator $U=\\mathbf{1}\\{X_{1}=0\\}$ satisfies\n$$\n\\mathbb{E}_{\\lambda}[U]=P_{\\lambda}(X_{1}=0)=\\exp(-\\lambda)=\\theta.\n$$\nBy the Lehmann–Scheffé theorem, the UMVUE is $\\mathbb{E}[U\\mid T]$. Compute $\\mathbb{E}[U\\mid T=t]=P(X_{1}=0\\mid T=t)$. Conditional on $T=t$, the vector $(X_{1},\\ldots,X_{n})$ has the multinomial distribution with $t$ trials and equal cell probabilities $1/n$, so\n$$\nP(X_{1}=0\\mid T=t)=\\left(1-\\frac{1}{n}\\right)^{t}.\n$$\nTherefore, the candidate UMVUE is\n$$\n\\delta(T)=\\left(1-\\frac{1}{n}\\right)^{T}.\n$$\nVerify unbiasedness directly via the probability generating function of $T\\sim\\operatorname{Poisson}(n\\lambda)$: for $s\\in\\mathbb{R}$,\n$$\n\\mathbb{E}\\!\\left[s^{T}\\right]=\\exp\\!\\left(n\\lambda(s-1)\\right).\n$$\nWith $s=1-\\frac{1}{n}$,\n$$\n\\mathbb{E}\\!\\left[\\left(1-\\frac{1}{n}\\right)^{T}\\right]=\\exp\\!\\left(n\\lambda\\left(1-\\frac{1}{n}-1\\right)\\right)=\\exp(-\\lambda)=\\theta.\n$$\nSince $\\delta(T)$ is a function of the complete sufficient statistic $T$ and is unbiased for $\\theta$, it is the UMVUE.", "answer": "$$\\boxed{\\left(1-\\frac{1}{n}\\right)^{T}}$$", "id": "1914858"}]}