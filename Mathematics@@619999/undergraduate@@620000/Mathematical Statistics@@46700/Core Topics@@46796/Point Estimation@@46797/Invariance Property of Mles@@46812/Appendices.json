{"hands_on_practices": [{"introduction": "We begin with a foundational application of the invariance property in a very common statistical setting: the binomial distribution. This exercise [@problem_id:1925545] asks you to estimate the variance of a binomial process, a quantity that depends on the unknown success probability $p$. You will first find the maximum likelihood estimator (MLE) for $p$ and then apply the invariance principle to directly find the MLE for the variance, $\\text{Var}(X) = np(1-p)$. This practice solidifies the core concept in a clear and intuitive context.", "problem": "A factory produces microchips in large batches. A single batch of $n$ microchips is selected for quality control inspection. From this batch, $x$ microchips are found to be defective. The number of defective microchips in a batch of size $n$ is assumed to follow a binomial distribution, denoted as $B(n, p)$, where $p$ is the unknown probability that any given microchip is defective. The parameter $n$ is a known positive integer, and $x$ is the observed number of defects, where $0 \\le x \\le n$. The variance of this binomial distribution is given by the formula $\\text{Var}(X) = np(1-p)$. Your task is to find the maximum likelihood estimate for the variance of the number of defective microchips. Express your answer as a formula in terms of $n$ and $x$.", "solution": "Let $X \\sim B(n,p)$ denote the number of defective microchips in a batch of size $n$, with observed value $x$. The likelihood function for $p$ given $x$ is\n$$\nL(p \\mid x) = \\binom{n}{x} p^{x} (1-p)^{n-x}.\n$$\nThe log-likelihood is\n$$\n\\ell(p) = \\ln L(p \\mid x) = \\ln \\binom{n}{x} + x \\ln p + (n-x) \\ln(1-p).\n$$\nDifferentiating with respect to $p$ and setting to zero gives\n$$\n\\frac{d\\ell}{dp} = \\frac{x}{p} - \\frac{n-x}{1-p} = 0 \\quad \\Rightarrow \\quad x(1-p) = (n-x)p \\quad \\Rightarrow \\quad \\hat{p} = \\frac{x}{n}.\n$$\nThe second derivative,\n$$\n\\frac{d^{2}\\ell}{dp^{2}} = -\\frac{x}{p^{2}} - \\frac{n-x}{(1-p)^{2}} < 0 \\quad \\text{for } p \\in (0,1),\n$$\nconfirms a maximum, and the boundary cases $x=0$ or $x=n$ yield $\\hat{p}=0$ or $\\hat{p}=1$ respectively, consistent with $\\hat{p}=x/n$.\n\nThe variance of $X$ is $np(1-p)$. By the invariance property of maximum likelihood estimators, the MLE of the variance is obtained by substituting $\\hat{p}$:\n$$\n\\widehat{\\text{Var}}(X) = n \\hat{p} \\left(1 - \\hat{p}\\right) = n \\left(\\frac{x}{n}\\right) \\left(1 - \\frac{x}{n}\\right) = \\frac{x(n - x)}{n}.\n$$\nThus, the maximum likelihood estimate for the variance in terms of $n$ and $x$ is $\\frac{x(n - x)}{n}$.", "answer": "$$\\boxed{\\frac{x(n - x)}{n}}$$", "id": "1925545"}, {"introduction": "This next exercise [@problem_id:1925562] explores the invariance property with the continuous uniform distribution, presenting a scenario where the standard calculus-based approach to finding an MLE does not apply. Here, the MLE for the parameter $\\theta$ is found by directly inspecting the likelihood function. This practice demonstrates the robustness of the invariance property; it remains a valid tool for transforming estimators even when the initial MLE is derived through non-standard methods.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n$ drawn from a continuous uniform distribution on the interval $(0, \\theta)$, where $\\theta > 0$ is an unknown parameter. The probability density function for any single observation $X_i$ is given by $f(x | \\theta) = \\frac{1}{\\theta}$ for $0 < x < \\theta$, and $f(x|\\theta) = 0$ otherwise.\n\nLet $X_{(n)}$ denote the maximum value in the sample, i.e., $X_{(n)} = \\max(X_1, X_2, \\dots, X_n)$.\n\nDetermine the Maximum Likelihood Estimator (MLE) for the variance of the distribution. Express your answer in terms of $X_{(n)}$ and numerical constants.", "solution": "We begin by writing the joint likelihood of the sample. For independent observations from the Uniform distribution on $(0,\\theta)$,\n$$\nL(\\theta \\mid x_{1},\\dots,x_{n})=\\prod_{i=1}^{n} f(x_{i}\\mid \\theta)=\\prod_{i=1}^{n}\\frac{1}{\\theta}\\,\\mathbf{1}_{\\{0<x_{i}<\\theta\\}}=\\theta^{-n}\\,\\mathbf{1}_{\\{\\theta\\geq x_{(n)}\\}},\n$$\nwhere $x_{(n)}=\\max\\{x_{1},\\dots,x_{n}\\}$. For fixed data, $L(\\theta)$ is proportional to $\\theta^{-n}$ on the interval $[x_{(n)},\\infty)$ and zero otherwise. Since $\\theta^{-n}$ is strictly decreasing in $\\theta$ for $\\theta>0$, the likelihood is maximized at the smallest allowable value of $\\theta$, namely\n$$\n\\hat{\\theta}_{\\text{MLE}}=X_{(n)}.\n$$\n\nNext, compute the variance of a single Uniform$(0,\\theta)$ variable. Using the definitions,\n$$\n\\mathbb{E}[X]=\\int_{0}^{\\theta} x\\,\\frac{1}{\\theta}\\,dx=\\frac{1}{\\theta}\\cdot\\frac{\\theta^{2}}{2}=\\frac{\\theta}{2},\\quad\n\\mathbb{E}[X^{2}]=\\int_{0}^{\\theta} x^{2}\\,\\frac{1}{\\theta}\\,dx=\\frac{1}{\\theta}\\cdot\\frac{\\theta^{3}}{3}=\\frac{\\theta^{2}}{3}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}=\\frac{\\theta^{2}}{3}-\\left(\\frac{\\theta}{2}\\right)^{2}=\\frac{\\theta^{2}}{12}.\n$$\n\nBy the invariance property of maximum likelihood estimators, if $g(\\theta)$ is a function of the parameter, then the MLE of $g(\\theta)$ is $g(\\hat{\\theta}_{\\text{MLE}})$. Taking $g(\\theta)=\\theta^{2}/12$, the MLE for the variance is\n$$\n\\widehat{\\operatorname{Var}}_{\\text{MLE}}=\\frac{\\hat{\\theta}_{\\text{MLE}}^{2}}{12}=\\frac{X_{(n)}^{2}}{12}.\n$$", "answer": "$$\\boxed{\\frac{X_{(n)}^{2}}{12}}$$", "id": "1925562"}, {"introduction": "Our final practice illustrates the power of the invariance principle in the context of a more advanced, multi-parameter model. The Zero-Inflated Poisson (ZIP) distribution, which is crucial for modeling count data with excess zeros in fields like ecology and econometrics, involves two parameters, $\\pi$ and $\\lambda$. This problem [@problem_id:1925553] challenges you to find the MLE for the overall variance of the distribution, which is a function of both parameters. It showcases how the invariance property extends elegantly to functions of multiple parameters, making it an indispensable tool for complex statistical modeling.", "problem": "A random variable $Y$ is said to follow a Zero-Inflated Poisson (ZIP) distribution with parameters $\\pi \\in [0, 1)$ and $\\lambda > 0$ if its probability mass function (PMF) is given by:\n$$\nP(Y=y) = \n\\begin{cases}\n\\pi + (1-\\pi)\\exp(-\\lambda) & \\text{if } y=0 \\\\\n(1-\\pi) \\frac{\\lambda^y \\exp(-\\lambda)}{y!} & \\text{if } y \\in \\{1, 2, 3, \\ldots\\}\n\\end{cases}\n$$\nHere, $\\pi$ represents the probability of an excess zero count that is not from the Poisson process, and $\\lambda$ is the mean of the underlying Poisson distribution.\n\nConsider a random sample $y_1, y_2, \\ldots, y_n$ drawn from a ZIP distribution. Let $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$ be the sample mean. The parameters $\\pi$ and $\\lambda$ are unknown. Let $\\hat{\\pi}$ and $\\hat{\\lambda}$ denote their respective Maximum Likelihood Estimators (MLEs).\n\nUsing the invariance property of MLEs, find the MLE for the variance of the distribution, $\\sigma^2 = \\text{Var}(Y)$. Express your answer as a closed-form analytic expression in terms of the sample mean $\\bar{y}$ and the MLE $\\hat{\\lambda}$.", "solution": "For a Zero-Inflated Poisson (ZIP) random variable $Y$ with parameters $\\pi \\in [0,1)$ and $\\lambda > 0$, write $Y = Z X$ where $Z \\sim \\text{Bernoulli}(1-\\pi)$ and, conditional on $Z=1$, $X \\sim \\text{Poisson}(\\lambda)$; otherwise $Y=0$. Using this mixture representation:\n$$\n\\mathbb{E}[Y] = (1 - \\pi)\\lambda \\equiv \\mu,\n$$\nand since for $X \\sim \\text{Poisson}(\\lambda)$, $\\mathbb{E}[X^{2}] = \\lambda + \\lambda^{2}$, we have\n$$\n\\mathbb{E}[Y^{2}] = (1 - \\pi)\\mathbb{E}[X^{2}] = (1 - \\pi)(\\lambda + \\lambda^{2}).\n$$\nTherefore,\n$$\n\\operatorname{Var}(Y) = \\mathbb{E}[Y^{2}] - \\{\\mathbb{E}[Y]\\}^{2} = (1 - \\pi)\\lambda + (1 - \\pi)\\lambda^{2} - (1 - \\pi)^{2}\\lambda^{2} = (1 - \\pi)\\lambda\\{1 + \\pi\\lambda\\}.\n$$\nEquivalently, in terms of $\\mu = (1 - \\pi)\\lambda$, note that $\\pi\\lambda = \\lambda - \\mu$, so\n$$\n\\sigma^{2} = \\operatorname{Var}(Y) = \\mu\\{1 + \\lambda - \\mu\\}.\n$$\n\nBy the invariance property of MLEs, the MLE of $\\sigma^{2}$ is obtained by plugging the MLEs of the parameters into this function. To express the result in terms of the sample mean $\\bar{y}$ and $\\hat{\\lambda}$, we show that the MLE of $\\mu$ equals $\\bar{y}$.\n\nReparameterize by $\\mu = (1 - \\pi)\\lambda$ so that $\\pi = 1 - \\mu/\\lambda$. Let $n_{0} = \\sum_{i=1}^{n} \\mathbf{1}\\{y_{i}=0\\}$. The log-likelihood (up to terms not involving $\\mu$) is\n$$\n\\ell(\\mu,\\lambda) = n_{0}\\ln\\!\\left(1 - \\frac{\\mu}{\\lambda}\\{1 - \\exp(-\\lambda)\\}\\right) + (n - n_{0})\\ln \\mu + C(\\lambda,y).\n$$\nDifferentiating with respect to $\\mu$ and setting to zero gives\n$$\n\\frac{\\partial \\ell}{\\partial \\mu} = -\\,\\frac{n_{0}\\,\\{(1/\\lambda)(1 - \\exp(-\\lambda))\\}}{1 - \\frac{\\mu}{\\lambda}(1 - \\exp(-\\lambda))} + \\frac{n - n_{0}}{\\mu} = 0,\n$$\nwhich yields\n$$\n(n - n_{0})\\lambda = \\mu\\,n\\,(1 - \\exp(-\\lambda)).\n$$\nFrom the score equation for $\\lambda$, one obtains\n$$\n\\frac{n\\bar{y}}{\\lambda} = \\frac{n - n_{0}}{1 - \\exp(-\\lambda)}.\n$$\nCombining the two identities gives $\\mu = \\bar{y}$. Therefore, at the MLEs, $\\hat{\\mu} = (1 - \\hat{\\pi})\\hat{\\lambda} = \\bar{y}$, i.e.,\n$$\n\\hat{\\pi} = 1 - \\frac{\\bar{y}}{\\hat{\\lambda}}.\n$$\n\nApplying invariance to $\\sigma^{2} = \\mu(1 + \\lambda - \\mu)$ yields\n$$\n\\hat{\\sigma}^{2} = \\hat{\\mu}\\{1 + \\hat{\\lambda} - \\hat{\\mu}\\} = \\bar{y}\\{1 + \\hat{\\lambda} - \\bar{y}\\}.\n$$\nThis is a closed-form analytic expression in terms of $\\bar{y}$ and $\\hat{\\lambda}$.", "answer": "$$\\boxed{\\bar{y}\\left(1+\\hat{\\lambda}-\\bar{y}\\right)}$$", "id": "1925553"}]}