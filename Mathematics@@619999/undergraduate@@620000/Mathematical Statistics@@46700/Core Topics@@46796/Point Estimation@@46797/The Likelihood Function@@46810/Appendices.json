{"hands_on_practices": [{"introduction": "This first practice grounds our understanding of the likelihood function in a simple, tangible scenario involving a sequence of independent trials. By constructing the likelihood for a discrete Bernoulli process, you will directly see how the probability of observing a specific set of outcomes changes as a function of the underlying success probability, $p$. This exercise [@problem_id:1961909] is fundamental to building intuition about how likelihood pinpoints the most plausible parameter values that could have generated the data.", "problem": "An experimental process is modeled by a sequence of independent Bernoulli trials. For each trial, the outcome $Y$ can be either a success (represented by $Y=1$) or a failure (represented by $Y=0$). The probability of a success is given by a parameter $p$, where $p \\in [0, 1]$. The probability mass function for a single trial is thus $f(y|p) = p^y (1-p)^{1-y}$ for $y \\in \\{0, 1\\}$.\n\nA small-scale experiment is conducted, resulting in a sequence of three independent outcomes: the first trial is a success, the second is a failure, and the third is a success. That is, the observed data is the sequence $(y_1, y_2, y_3) = (1, 0, 1)$.\n\nThe likelihood function, $L(p)$, represents the probability of observing this specific data sequence as a function of the parameter $p$. For independent trials, it is the product of the individual probabilities: $L(p) = f(y_1|p) \\times f(y_2|p) \\times f(y_3|p)$.\n\nDetermine the value of the parameter $p$ that maximizes this likelihood function $L(p)$. Express your answer as a fraction in its simplest form.", "solution": "For a Bernoulli trial with parameter $p$, the probability mass function is $f(y \\mid p) = p^{y}(1-p)^{1-y}$ for $y \\in \\{0,1\\}$. For independent trials, the likelihood of observing $(y_{1},y_{2},y_{3})=(1,0,1)$ is the product\n$$\nL(p)=\\prod_{i=1}^{3} f(y_{i}\\mid p)\n= p^{y_{1}}(1-p)^{1-y_{1}} \\cdot p^{y_{2}}(1-p)^{1-y_{2}} \\cdot p^{y_{3}}(1-p)^{1-y_{3}}.\n$$\nSubstituting $y_{1}=1$, $y_{2}=0$, $y_{3}=1$ gives\n$$\nL(p)=p^{1}(1-p)^{0}\\cdot p^{0}(1-p)^{1}\\cdot p^{1}(1-p)^{0}=p^{2}(1-p).\n$$\nTo maximize $L(p)$ over $p \\in [0,1]$, it is convenient to maximize the log-likelihood\n$$\n\\ell(p)=\\ln L(p)=2\\ln p+\\ln(1-p).\n$$\nDifferentiate and set to zero:\n$$\n\\ell'(p)=\\frac{2}{p}-\\frac{1}{1-p}=0\n\\;\\;\\Longrightarrow\\;\\;\n\\frac{2}{p}=\\frac{1}{1-p}\n\\;\\;\\Longrightarrow\\;\\;\n2(1-p)=p\n\\;\\;\\Longrightarrow\\;\\;\n2-2p=p\n\\;\\;\\Longrightarrow\\;\\;\np=\\frac{2}{3}.\n$$\nVerify it is a maximum: the second derivative is\n$$\n\\ell''(p)=-\\frac{2}{p^{2}}-\\frac{1}{(1-p)^{2}}<0 \\quad \\text{for} \\quad p\\in(0,1),\n$$\nso the critical point is a strict local maximum. Checking the boundaries, $L(0)=0$ and $L(1)=0$, so the global maximum on $[0,1]$ occurs at the interior point $p=\\frac{2}{3}$.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "1961909"}, {"introduction": "Building on the basics, we now move to a continuous model often used in fields like reliability engineering to describe lifetimes or waiting times. This practice demonstrates the power and convenience of using the log-likelihood function, which often simplifies the mathematics of maximization without changing the final estimate. Your task [@problem_id:1961965] is to apply calculus to find the Maximum Likelihood Estimator (MLE) for a parameter of the Gamma distribution, a core skill in parametric statistics.", "problem": "An engineer is studying the reliability of a particular electronic component. The lifetime of a component, in hours, is modeled as a continuous random variable $X$ that follows a Gamma distribution with a known shape parameter $k > 0$ and an unknown scale parameter $\\theta > 0$. The probability density function (PDF) of this distribution is given by:\n$$f(x; k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k} x^{k-1} \\exp\\left(-\\frac{x}{\\theta}\\right), \\quad \\text{for } x > 0$$\nwhere $\\Gamma(k)$ is the Gamma function evaluated at $k$.\n\nTo estimate the unknown parameter $\\theta$, the engineer tests a random sample of $n$ components and records their lifetimes, denoted as $x_1, x_2, \\ldots, x_n$.\n\nYour task is to find the Maximum Likelihood Estimator (MLE) for the scale parameter $\\theta$. Express your final answer for the estimator, denoted $\\hat{\\theta}$, in terms of the known shape parameter $k$ and the sample mean $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$.", "solution": "We assume the observed lifetimes $x_{1},\\ldots,x_{n}$ are independent and identically distributed as $\\Gamma(k,\\theta)$ with known $k>0$ and unknown $\\theta>0$. The joint likelihood function is\n$$\nL(\\theta; x_{1},\\ldots,x_{n})=\\prod_{i=1}^{n}\\frac{1}{\\Gamma(k)\\theta^{k}}x_{i}^{k-1}\\exp\\left(-\\frac{x_{i}}{\\theta}\\right).\n$$\nTaking the natural logarithm of the likelihood gives the log-likelihood\n$$\n\\ell(\\theta)=\\ln L(\\theta; x_{1},\\ldots,x_{n})=\\sum_{i=1}^{n}\\left[-\\ln\\Gamma(k)-k\\ln\\theta+(k-1)\\ln x_{i}-\\frac{x_{i}}{\\theta}\\right].\n$$\nDifferentiating with respect to $\\theta$ and using $\\frac{\\mathrm{d}}{\\mathrm{d}\\theta}\\ln\\theta=\\frac{1}{\\theta}$ and $\\frac{\\mathrm{d}}{\\mathrm{d}\\theta}\\left(-\\frac{x_{i}}{\\theta}\\right)=\\frac{x_{i}}{\\theta^{2}}$, we obtain\n$$\n\\frac{\\partial \\ell}{\\partial \\theta}=\\sum_{i=1}^{n}\\left[-\\frac{k}{\\theta}+\\frac{x_{i}}{\\theta^{2}}\\right]=-\\,\\frac{nk}{\\theta}+\\frac{1}{\\theta^{2}}\\sum_{i=1}^{n}x_{i}.\n$$\nSetting the score equation to zero for the MLE,\n$$\n-\\,\\frac{nk}{\\theta}+\\frac{1}{\\theta^{2}}\\sum_{i=1}^{n}x_{i}=0,\n$$\nand multiplying both sides by $\\theta^{2}>0$ gives\n$$\n-nk\\,\\theta+\\sum_{i=1}^{n}x_{i}=0.\n$$\nSolving for $\\theta$ yields\n$$\n\\hat{\\theta}=\\frac{1}{nk}\\sum_{i=1}^{n}x_{i}=\\frac{\\bar{x}}{k},\n$$\nwhere $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$ is the sample mean. To verify it is a maximum, compute the second derivative\n$$\n\\frac{\\partial^{2}\\ell}{\\partial \\theta^{2}}=\\sum_{i=1}^{n}\\left[\\frac{k}{\\theta^{2}}-\\frac{2x_{i}}{\\theta^{3}}\\right]=\\frac{nk}{\\theta^{2}}-\\frac{2}{\\theta^{3}}\\sum_{i=1}^{n}x_{i}.\n$$\nEvaluating at $\\hat{\\theta}$ and using $\\sum_{i=1}^{n}x_{i}=nk\\,\\hat{\\theta}$,\n$$\n\\left.\\frac{\\partial^{2}\\ell}{\\partial \\theta^{2}}\\right|_{\\theta=\\hat{\\theta}}=\\frac{nk}{\\hat{\\theta}^{2}}-\\frac{2}{\\hat{\\theta}^{3}}(nk\\,\\hat{\\theta})=-\\frac{nk}{\\hat{\\theta}^{2}}<0,\n$$\nconfirming that $\\hat{\\theta}$ maximizes the log-likelihood. Therefore, the MLE is $\\bar{x}/k$.", "answer": "$$\\boxed{\\frac{\\bar{x}}{k}}$$", "id": "1961965"}, {"introduction": "Maximum likelihood estimation is not always a straightforward calculus problem that can be solved by setting a derivative to zero. This exercise explores a scenario with a \"shifted\" distribution, where the parameter of interest, $\\mu$, defines the lower bound for the observed data. Here, you will see that maximizing the likelihood [@problem_id:1961936] requires careful inspection of the function's structure, particularly how the support of the distribution depends on the parameter.", "problem": "A manufacturer is testing a new type of microchip. A batch of $n$ identical chips are subjected to a stress test. The test begins at an unknown but fixed time $\\mu > 0$. The time of failure for the $i$-th chip, measured from time zero, is recorded as $X_i$. The lifetime of a chip, defined as the duration from the start of the test to its failure, is known to follow an exponential distribution with a rate parameter of 1. Consequently, the probability density function for the failure time $X_i$ of a single chip is given by:\n$$f(x | \\mu) = \\begin{cases} \\exp(-(x-\\mu)) & \\text{if } x \\ge \\mu \\\\ 0 & \\text{if } x < \\mu \\end{cases}$$\nSuppose a random sample of $n$ failure times, $x_1, x_2, \\ldots, x_n$, is observed. Your task is to determine the likelihood function $L(\\mu | x_1, x_2, \\ldots, x_n)$ for the unknown start time $\\mu$. The answer should be a single expression valid for all real values of $\\mu$.", "solution": "Each failure time has the shifted-exponential density with rate parameter 1, which can be written using an indicator function as\n$$\nf(x_{i}\\mid \\mu)=\\exp\\!\\bigl(-(x_{i}-\\mu)\\bigr)\\,\\mathbf{1}\\{x_{i}\\ge \\mu\\}.\n$$\nAssuming independence of the $n$ observations, the likelihood function is the product of the individual densities:\n$$\nL(\\mu\\mid x_{1},\\ldots,x_{n})=\\prod_{i=1}^{n} f(x_{i}\\mid \\mu)\n=\\prod_{i=1}^{n}\\left[\\exp\\!\\bigl(-(x_{i}-\\mu)\\bigr)\\,\\mathbf{1}\\{x_{i}\\ge \\mu\\}\\right].\n$$\nCollecting the exponential terms and the indicators separately gives\n$$\nL(\\mu\\mid x_{1},\\ldots,x_{n})\n=\\exp\\!\\left(-\\sum_{i=1}^{n}x_{i}+n\\mu\\right)\\,\\prod_{i=1}^{n}\\mathbf{1}\\{x_{i}\\ge \\mu\\}.\n$$\nEquivalently, since $\\prod_{i=1}^{n}\\mathbf{1}\\{x_{i}\\ge \\mu\\}=\\mathbf{1}\\{\\mu\\le x_{(1)}\\}$ where $x_{(1)}=\\min_{1\\le i\\le n}x_{i}$, this expression is valid for all real $\\mu$.", "answer": "$$\\boxed{\\exp\\!\\left(-\\sum_{i=1}^{n}x_{i}+n\\mu\\right)\\prod_{i=1}^{n}\\mathbf{1}\\{x_{i}\\ge \\mu\\}}$$", "id": "1961936"}]}