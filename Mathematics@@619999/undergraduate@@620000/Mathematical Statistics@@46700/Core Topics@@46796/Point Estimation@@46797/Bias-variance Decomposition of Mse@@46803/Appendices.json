{"hands_on_practices": [{"introduction": "To build a strong foundation, we begin with a thought experiment that pushes the concepts of bias and variance to their extremes. This exercise asks you to analyze a \"constant estimator,\" which completely ignores all data. By evaluating this simple, albeit impractical, estimator, you can clearly isolate and understand the definitions of bias and variance and see how an estimator can have zero variance but still be a poor choice due to high bias. [@problem_id:1900788]", "problem": "Suppose we are interested in estimating a single, unknown real parameter $\\theta$. A particularly simple estimator is proposed, which completely ignores any sample data and always outputs the same value. Let this constant estimator be denoted by $\\hat{\\theta} = 10$.\n\nYour task is to analyze the performance of this estimator. Find expressions for the following three quantities in terms of the true parameter $\\theta$:\n1. The bias of the estimator, $\\operatorname{Bias}(\\hat{\\theta})$.\n2. The variance of the estimator, $\\operatorname{Var}(\\hat{\\theta})$.\n3. The Mean Squared Error (MSE) of the estimator, $\\operatorname{MSE}(\\hat{\\theta})$.\n\nPresent your final answer as three distinct expressions for the bias, variance, and MSE, in that order, using a row matrix.", "solution": "We are given a constant estimator that ignores any sample data and always returns the fixed value $\\hat{\\theta}=10$. Since the estimator is a constant function of the data, it is a degenerate random variable that takes the value $10$ with probability $1$ under any sampling distribution. The expectation and variance of such an estimator are computed with respect to the sampling distribution, while the parameter $\\theta$ is a fixed (nonrandom) constant.\n\nBy definition, the bias of an estimator is\n$$\n\\operatorname{Bias}(\\hat{\\theta})= \\mathbb{E}[\\hat{\\theta}] - \\theta.\n$$\nSince $\\hat{\\theta}=10$ with probability $1$, we have $\\mathbb{E}[\\hat{\\theta}] = 10$, hence\n$$\n\\operatorname{Bias}(\\hat{\\theta}) = 10 - \\theta.\n$$\n\nThe variance of the estimator is\n$$\n\\operatorname{Var}(\\hat{\\theta}) = \\mathbb{E}\\big[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^{2}\\big].\n$$\nUsing $\\hat{\\theta}=10$ and $\\mathbb{E}[\\hat{\\theta}]=10$, we obtain\n$$\n\\operatorname{Var}(\\hat{\\theta}) = \\mathbb{E}[(10-10)^{2}] = 0.\n$$\n\nThe Mean Squared Error (MSE) is defined by\n$$\n\\operatorname{MSE}(\\hat{\\theta}) = \\mathbb{E}\\big[(\\hat{\\theta} - \\theta)^{2}\\big].\n$$\nBecause $\\hat{\\theta}=10$ deterministically and $\\theta$ is a fixed constant, we have\n$$\n\\operatorname{MSE}(\\hat{\\theta}) = \\mathbb{E}[(10 - \\theta)^{2}] = (10 - \\theta)^{2}.\n$$\nEquivalently, using the identity $\\operatorname{MSE}(\\hat{\\theta}) = \\operatorname{Bias}(\\hat{\\theta})^{2} + \\operatorname{Var}(\\hat{\\theta})$, we get\n$$\n\\operatorname{MSE}(\\hat{\\theta}) = (10 - \\theta)^{2} + 0 = (10 - \\theta)^{2}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}10 - \\theta & 0 & (10 - \\theta)^{2}\\end{pmatrix}}$$", "id": "1900788"}, {"introduction": "Moving from a purely conceptual case, this next practice compares two different unbiased estimators for a population mean. While the standard sample mean uses all available data, a proposed alternative discards some information. By calculating the Mean Squared Error for both and finding their ratio, you will gain a concrete understanding of how variance acts as the sole determinant of MSE for unbiased estimators and quantify the statistical cost of ignoring data. [@problem_id:1900727]", "problem": "A researcher is analyzing a process where measurements are taken in triplets. Let $X_1, X_2, X_3$ represent an independent and identically distributed (i.i.d.) random sample of size $n=3$ drawn from a population with a true mean $\\mu$ and a finite, non-zero variance $\\sigma^2$.\n\nThe standard estimator for the mean is the sample mean, $\\bar{X} = \\frac{1}{3}(X_1 + X_2 + X_3)$. However, suspecting that the middle measurement might be less reliable, the researcher proposes an alternative estimator, $\\hat{\\mu}$, which is constructed by averaging only the first and third measurements:\n$$\n\\hat{\\mu} = \\frac{1}{2}(X_1 + X_3)\n$$\nTo evaluate the performance of this new estimator, you are asked to compare their Mean Squared Error (MSE). The MSE of an estimator is a measure of its average squared difference from the true parameter, combining both its bias and variance.\n\nCalculate the ratio of the MSE of the proposed estimator $\\hat{\\mu}$ to the MSE of the standard sample mean $\\bar{X}$. Your final answer should be a numerical value.", "solution": "Let $X_{1},X_{2},X_{3}$ be i.i.d. with $\\mathbb{E}[X_{i}]=\\mu$ and $\\operatorname{Var}(X_{i})=\\sigma^{2}$.\n\nFor any estimator $T$, the mean squared error is $\\operatorname{MSE}(T)=\\operatorname{Var}(T)+\\{\\mathbb{E}[T]-\\mu\\}^{2}$.\n\nFirst, consider $\\hat{\\mu}=\\frac{1}{2}(X_{1}+X_{3})$. Its expectation is\n$$\n\\mathbb{E}[\\hat{\\mu}]=\\frac{1}{2}\\left(\\mathbb{E}[X_{1}]+\\mathbb{E}[X_{3}]\\right)=\\frac{1}{2}(\\mu+\\mu)=\\mu,\n$$\nso the bias is zero and $\\operatorname{MSE}(\\hat{\\mu})=\\operatorname{Var}(\\hat{\\mu})$. Using independence and $\\operatorname{Var}(aY)=a^{2}\\operatorname{Var}(Y)$,\n$$\n\\operatorname{Var}(\\hat{\\mu})=\\operatorname{Var}\\!\\left(\\frac{1}{2}(X_{1}+X_{3})\\right)=\\frac{1}{4}\\left(\\operatorname{Var}(X_{1})+\\operatorname{Var}(X_{3})\\right)=\\frac{1}{4}( \\sigma^{2}+\\sigma^{2})=\\frac{\\sigma^{2}}{2}.\n$$\n\nNext, consider $\\bar{X}=\\frac{1}{3}(X_{1}+X_{2}+X_{3})$. Its expectation is $\\mathbb{E}[\\bar{X}]=\\mu$, so $\\operatorname{MSE}(\\bar{X})=\\operatorname{Var}(\\bar{X})$. Using independence,\n$$\n\\operatorname{Var}(\\bar{X})=\\operatorname{Var}\\!\\left(\\frac{1}{3}(X_{1}+X_{2}+X_{3})\\right)=\\frac{1}{9}\\left(\\operatorname{Var}(X_{1})+\\operatorname{Var}(X_{2})+\\operatorname{Var}(X_{3})\\right)=\\frac{1}{9}(3\\sigma^{2})=\\frac{\\sigma^{2}}{3}.\n$$\n\nTherefore, the required ratio is\n$$\n\\frac{\\operatorname{MSE}(\\hat{\\mu})}{\\operatorname{MSE}(\\bar{X})}=\\frac{\\sigma^{2}/2}{\\sigma^{2}/3}=\\frac{3}{2}.\n$$", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "1900727"}, {"introduction": "This final practice delves into the heart of the bias-variance trade-off. Here, you will analyze an estimator for the parameter of a Poisson distribution that has been intentionally modified with a constant, introducing a bias. Your task is to derive the MSE, which will be a function of both a bias term and a variance term, perfectly illustrating how a statistician might accept a small, controlled bias in exchange for a significant reduction in variance to achieve a lower overall error. [@problem_id:1900753]", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n$ drawn from a Poisson distribution with parameter $\\lambda > 0$. A statistician proposes an estimator $\\hat{\\lambda}$ for the parameter $\\lambda$, defined as:\n$$\n\\hat{\\lambda} = \\frac{\\left(\\sum_{i=1}^n X_i\\right) + \\alpha}{n}\n$$\nwhere $\\alpha$ is a known real constant.\n\nFind a simplified expression for the Mean Squared Error (MSE), defined as $\\operatorname{MSE}(\\hat{\\lambda}) = \\mathbb{E}\\left[(\\hat{\\lambda} - \\lambda)^2\\right]$, of this estimator. Express your answer in terms of $n$, $\\lambda$, and $\\alpha$.", "solution": "We use the bias-variance decomposition for mean squared error: for any estimator $\\hat{\\lambda}$ of $\\lambda$, the mean squared error is $\\operatorname{MSE}(\\hat{\\lambda}) = \\operatorname{Var}(\\hat{\\lambda}) + \\left(\\operatorname{Bias}(\\hat{\\lambda})\\right)^{2}$, where $\\operatorname{Bias}(\\hat{\\lambda}) = \\mathbb{E}[\\hat{\\lambda}] - \\lambda$.\n\nFirst compute the expectation of $\\hat{\\lambda}$. Since $X_{1},\\ldots,X_{n}$ are independent and identically distributed with $\\mathbb{E}[X_{i}] = \\lambda$, we have $\\mathbb{E}\\left[\\sum_{i=1}^{n} X_{i}\\right] = n\\lambda$. Therefore,\n$$\n\\mathbb{E}[\\hat{\\lambda}] = \\mathbb{E}\\left[\\frac{\\sum_{i=1}^{n} X_{i} + \\alpha}{n}\\right] = \\frac{\\mathbb{E}\\left[\\sum_{i=1}^{n} X_{i}\\right] + \\alpha}{n} = \\frac{n\\lambda + \\alpha}{n} = \\lambda + \\frac{\\alpha}{n}.\n$$\nThus the bias is\n$$\n\\operatorname{Bias}(\\hat{\\lambda}) = \\mathbb{E}[\\hat{\\lambda}] - \\lambda = \\frac{\\alpha}{n}.\n$$\n\nNext compute the variance of $\\hat{\\lambda}$. Using $\\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(X_{i})$ for independent variables and $\\operatorname{Var}(X_{i}) = \\lambda$ for Poisson$(\\lambda)$,\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) = n\\lambda.\n$$\nSince adding a constant does not change variance and scaling by a constant $c$ multiplies variance by $c^{2}$, we get\n$$\n\\operatorname{Var}(\\hat{\\lambda}) = \\operatorname{Var}\\left(\\frac{\\sum_{i=1}^{n} X_{i} + \\alpha}{n}\\right) = \\frac{1}{n^{2}} \\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) = \\frac{1}{n^{2}} \\cdot n\\lambda = \\frac{\\lambda}{n}.\n$$\n\nCombining these,\n$$\n\\operatorname{MSE}(\\hat{\\lambda}) = \\operatorname{Var}(\\hat{\\lambda}) + \\left(\\operatorname{Bias}(\\hat{\\lambda})\\right)^{2} = \\frac{\\lambda}{n} + \\left(\\frac{\\alpha}{n}\\right)^{2} = \\frac{\\lambda}{n} + \\frac{\\alpha^{2}}{n^{2}}.\n$$\n\nEquivalently, one can verify directly:\n$$\n\\operatorname{MSE}(\\hat{\\lambda}) = \\mathbb{E}\\left[\\left(\\frac{\\sum_{i=1}^{n} X_{i} + \\alpha}{n} - \\lambda\\right)^{2}\\right] = \\frac{1}{n^{2}} \\mathbb{E}\\left[\\left(\\sum_{i=1}^{n} X_{i} - n\\lambda + \\alpha\\right)^{2}\\right] = \\frac{1}{n^{2}}\\left(\\operatorname{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) + \\alpha^{2}\\right) = \\frac{1}{n^{2}}(n\\lambda + \\alpha^{2}) = \\frac{\\lambda}{n} + \\frac{\\alpha^{2}}{n^{2}}.\n$$", "answer": "$$\\boxed{\\frac{\\lambda}{n}+\\frac{\\alpha^{2}}{n^{2}}}$$", "id": "1900753"}]}