{"hands_on_practices": [{"introduction": "Let's begin by applying the workhorse technique for finding Maximum Likelihood Estimators. This problem uses the Rayleigh distribution, a model frequently seen in communications theory and physics, to illustrate the standard procedure. You will construct the log-likelihood function for a continuous parameter, use calculus to find the point of maximum likelihood, and derive the estimator from a sample of data [@problem_id:1933625].", "problem": "In communication theory, the envelope of a narrow-band Gaussian noise signal is often modeled using a Rayleigh distribution. An engineer is studying a set of $n$ independent measurements of signal amplitude, denoted by $x_1, x_2, \\dots, x_n$. These measurements are assumed to be a random sample from a Rayleigh distribution with the probability density function (PDF) given by:\n$$f(x; \\sigma) = \\frac{x}{\\sigma^2} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$$\nfor $x \\ge 0$ and $\\sigma > 0$. The parameter $\\sigma$ is an unknown scale parameter related to the noise power.\n\nDetermine the maximum likelihood estimator (MLE), denoted as $\\hat{\\sigma}$, for the parameter $\\sigma$ in terms of the sample data $x_1, x_2, \\dots, x_n$.", "solution": "The goal is to find the maximum likelihood estimator (MLE) for the parameter $\\sigma$ of a Rayleigh distribution. Let the random sample be $X_1, X_2, \\dots, X_n$, with observed values $x_1, x_2, \\dots, x_n$.\n\nFirst, we construct the likelihood function, $L(\\sigma)$, which is the joint probability density function of the sample. Since the observations are independent and identically distributed, the likelihood function is the product of the individual probability density functions:\n$$L(\\sigma | x_1, \\dots, x_n) = \\prod_{i=1}^{n} f(x_i; \\sigma) = \\prod_{i=1}^{n} \\frac{x_i}{\\sigma^2} \\exp\\left(-\\frac{x_i^2}{2\\sigma^2}\\right)$$\n\nTo simplify the process of maximization, we work with the natural logarithm of the likelihood function, known as the log-likelihood function, $\\ell(\\sigma) = \\ln(L(\\sigma))$. Taking the logarithm turns the product into a sum:\n$$\\ell(\\sigma) = \\ln\\left( \\prod_{i=1}^{n} \\frac{x_i}{\\sigma^2} \\exp\\left(-\\frac{x_i^2}{2\\sigma^2}\\right) \\right)$$\nUsing the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(a/b) = \\ln(a) - \\ln(b)$, we get:\n$$\\ell(\\sigma) = \\sum_{i=1}^{n} \\ln\\left( \\frac{x_i}{\\sigma^2} \\exp\\left(-\\frac{x_i^2}{2\\sigma^2}\\right) \\right)$$\n$$\\ell(\\sigma) = \\sum_{i=1}^{n} \\left[ \\ln(x_i) - \\ln(\\sigma^2) + \\ln\\left(\\exp\\left(-\\frac{x_i^2}{2\\sigma^2}\\right)\\right) \\right]$$\n$$\\ell(\\sigma) = \\sum_{i=1}^{n} \\left[ \\ln(x_i) - 2\\ln(\\sigma) - \\frac{x_i^2}{2\\sigma^2} \\right]$$\nWe can separate the sums:\n$$\\ell(\\sigma) = \\sum_{i=1}^{n} \\ln(x_i) - \\sum_{i=1}^{n} 2\\ln(\\sigma) - \\sum_{i=1}^{n} \\frac{x_i^2}{2\\sigma^2}$$\nThe second term does not depend on the index $i$, so it becomes $2n\\ln(\\sigma)$. The third term can be written with the sum inside:\n$$\\ell(\\sigma) = \\sum_{i=1}^{n} \\ln(x_i) - 2n\\ln(\\sigma) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} x_i^2$$\n\nTo find the value of $\\sigma$ that maximizes $\\ell(\\sigma)$, we take the derivative of $\\ell(\\sigma)$ with respect to $\\sigma$ and set it to zero.\n$$\\frac{d\\ell}{d\\sigma} = \\frac{d}{d\\sigma} \\left( \\sum_{i=1}^{n} \\ln(x_i) - 2n\\ln(\\sigma) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} x_i^2 \\right)$$\nThe first term is a constant with respect to $\\sigma$, so its derivative is zero. For the second term, $\\frac{d}{d\\sigma}(-2n\\ln(\\sigma)) = -\\frac{2n}{\\sigma}$. For the third term, we use the power rule, treating $\\sigma^{-2}$:\n$$\\frac{d}{d\\sigma} \\left(-\\frac{1}{2}\\sigma^{-2} \\sum_{i=1}^{n} x_i^2\\right) = -\\frac{1}{2} (-2\\sigma^{-3}) \\sum_{i=1}^{n} x_i^2 = \\frac{1}{\\sigma^3}\\sum_{i=1}^{n} x_i^2$$\nCombining these, we get the full derivative:\n$$\\frac{d\\ell}{d\\sigma} = -\\frac{2n}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{i=1}^{n} x_i^2$$\n\nNow, we set the derivative to zero and solve for $\\sigma$. The solution will be the MLE, which we denote by $\\hat{\\sigma}$.\n$$-\\frac{2n}{\\hat{\\sigma}} + \\frac{1}{\\hat{\\sigma}^3} \\sum_{i=1}^{n} x_i^2 = 0$$\n$$\\frac{1}{\\hat{\\sigma}^3} \\sum_{i=1}^{n} x_i^2 = \\frac{2n}{\\hat{\\sigma}}$$\nAssuming $\\hat{\\sigma} \\ne 0$, we can multiply both sides by $\\hat{\\sigma}^3$:\n$$\\sum_{i=1}^{n} x_i^2 = 2n\\hat{\\sigma}^2$$\nSolving for $\\hat{\\sigma}^2$:\n$$\\hat{\\sigma}^2 = \\frac{1}{2n} \\sum_{i=1}^{n} x_i^2$$\nSince the parameter $\\sigma$ must be positive, we take the positive square root:\n$$\\hat{\\sigma} = \\sqrt{\\frac{1}{2n} \\sum_{i=1}^{n} x_i^2}$$\nTo confirm this is a maximum, we could check the second derivative, which would be negative at this point, but for this standard problem, we can be confident this is the MLE. The expression for the MLE, $\\hat{\\sigma}$, is the final answer.", "answer": "$$\\boxed{\\sqrt{\\frac{1}{2n} \\sum_{i=1}^{n} x_i^2}}$$", "id": "1933625"}, {"introduction": "The power of Maximum Likelihood Estimation is its fundamental principle: find the parameter value that makes your observed data most probable. This next exercise strips away the calculus to focus on this core idea. By considering a scenario where the parameter $\\theta$ can only take one of a few discrete values, you will find the MLE by directly comparing the likelihood of the observed outcome under each hypothesis [@problem_id:1933649]. This practice builds strong intuition for what the likelihood function truly represents.", "problem": "A materials scientist is testing a new type of non-volatile memory cell for its data retention reliability over a one-year period. The process of storing a bit and reading it back after a year is modeled as a series of independent Bernoulli trials. A \"success\" is defined as the bit being read back correctly without corruption. The probability of success for any given trial is denoted by the parameter $\\theta$.\n\nBased on the quantum-tunneling model of data degradation for this specific cell design, the lead engineer hypothesizes that the true value of $\\theta$ is restricted to a discrete set of possibilities: $\\{0.1, 0.5, 0.9\\}$.\n\nTo test this hypothesis and estimate the parameter, an experiment is conducted on 10 identical memory cells. After a year, it is found that 8 of the cells retained their data correctly (i.e., 8 successes were observed).\n\nGiven this experimental outcome, what is the Maximum Likelihood Estimate (MLE) for the parameter $\\theta$?", "solution": "We model the outcome as a Binomial likelihood. For a single parameter value $\\theta$ with $n=10$ trials and $k=8$ successes, the likelihood is\n$$\nL(\\theta)\\propto \\theta^{k}(1-\\theta)^{n-k}=\\theta^{8}(1-\\theta)^{2},\n$$\nwhere the proportionality ignores the binomial coefficient $\\binom{10}{8}$ common to all $\\theta$.\n\nThe candidate set is $\\{0.1,0.5,0.9\\}$. Compute the unnormalized likelihoods:\n- For $\\theta=0.1$:\n$$\nL(0.1)\\propto (0.1)^{8}(0.9)^{2}=\\left(\\frac{1}{10}\\right)^{8}\\left(\\frac{9}{10}\\right)^{2}=\\frac{9^{2}}{10^{10}}.\n$$\n- For $\\theta=0.5$:\n$$\nL(0.5)\\propto (0.5)^{8}(0.5)^{2}=\\left(\\frac{1}{2}\\right)^{10}=\\frac{1}{2^{10}}.\n$$\n- For $\\theta=0.9$:\n$$\nL(0.9)\\propto (0.9)^{8}(0.1)^{2}=\\left(\\frac{9}{10}\\right)^{8}\\left(\\frac{1}{10}\\right)^{2}=\\frac{9^{8}}{10^{10}}.\n$$\n\nCompare $L(0.9)$ to $L(0.1)$:\n$$\n\\frac{L(0.9)}{L(0.1)}=\\frac{9^{8}/10^{10}}{9^{2}/10^{10}}=9^{6}>1,\n$$\nso $L(0.9)>L(0.1)$.\n\nCompare $L(0.9)$ to $L(0.5)$:\n$$\n\\frac{L(0.9)}{L(0.5)}=\\frac{9^{8}/10^{10}}{1/2^{10}}=\\frac{9^{8}2^{10}}{10^{10}}=\\frac{9^{8}}{5^{10}}=\\left(\\frac{9^{4}}{5^{5}}\\right)^{2}.\n$$\nSince $9^{4}=6561$ and $5^{5}=3125$, we have $9^{4}>5^{5}$, hence $\\left(\\frac{9^{4}}{5^{5}}\\right)^{2}>1$, so $L(0.9)>L(0.5)$.\n\nTherefore $L(0.9)$ exceeds the likelihood at all other candidates, and the Maximum Likelihood Estimate is $\\hat{\\theta}=0.9$.", "answer": "$$\\boxed{0.9}$$", "id": "1933649"}, {"introduction": "Not all likelihood functions can be maximized by setting their derivative to zero. This problem explores a crucial scenario where the parameter of interest, $N$, defines the boundary of the data's support [@problem_id:1933607]. Here, the likelihood function is maximized at its edge, a common situation when estimating range parameters. This hands-on practice will guide you to find such an MLE and introduces the important subsequent step of evaluating an estimator's properties, such as its bias.", "problem": "A technology company is testing a new fleet of autonomous delivery robots. Each robot is assigned a unique serial number from the set $\\{1, 2, \\dots, N\\}$, where $N$, the total number of robots in the fleet, is an unknown parameter. A quality control team performs a spot check by randomly selecting $n$ robots and recording their serial numbers. Let these observed serial numbers be $\\{X_1, X_2, \\dots, X_n\\}$, which can be considered an independent and identically distributed random sample from a discrete uniform distribution on $\\{1, 2, \\dots, N\\}$.\n\nThe team uses the Maximum Likelihood Estimator (MLE), denoted $\\hat{N}$, to estimate the total number of robots $N$. It is known that this estimator is biased. Determine an approximation for the bias of this estimator, defined as $B(\\hat{N}) = E[\\hat{N}] - N$. This approximation should be valid for cases where the total number of robots $N$ is much larger than the sample size $n$. Your final answer should be an expression in terms of $N$ and $n$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. from the discrete uniform distribution on $\\{1,2,\\dots,N\\}$. Let $M := X_{(n)} = \\max\\{X_{1},\\dots,X_{n}\\}$.\n\n1) Maximum likelihood estimator:\nThe likelihood for $N$ (treated as an integer parameter) given the sample is\n$$\nL(N) = \\prod_{i=1}^{n} \\frac{1}{N}\\,\\mathbf{1}\\{1 \\leq X_{i} \\leq N\\}\n= N^{-n}\\,\\mathbf{1}\\{M \\leq N\\}.\n$$\nAs a function of $N$, this is nonzero only for $N \\geq M$ and decreases in $N$ on that domain, so the maximizer is\n$$\n\\hat{N} = M.\n$$\n\n2) Distribution and expectation of $M$:\nFor $m \\in \\{1,\\dots,N\\}$,\n$$\n\\Pr(M \\leq m) = \\Pr(X_{1} \\leq m,\\dots,X_{n} \\leq m) = \\left(\\frac{m}{N}\\right)^{n}.\n$$\nHence, by the tail-sum formula for nonnegative integer-valued random variables,\n$$\nE[M] = \\sum_{m=1}^{N} \\Pr(M \\geq m)\n= \\sum_{m=1}^{N} \\left[1 - \\left(\\frac{m-1}{N}\\right)^{n}\\right]\n= N - \\sum_{k=0}^{N-1} \\left(\\frac{k}{N}\\right)^{n}.\n$$\n\n3) Large-$N$ approximation (with $n$ fixed and $N \\gg n$):\nThe sum is a Riemann sum for $\\int_{0}^{1} x^{n}\\,dx$, so\n$$\n\\sum_{k=0}^{N-1} \\left(\\frac{k}{N}\\right)^{n} \\approx N \\int_{0}^{1} x^{n}\\,dx = \\frac{N}{n+1}.\n$$\nThus,\n$$\nE[\\hat{N}] = E[M] \\approx N - \\frac{N}{n+1} = \\frac{n}{n+1}\\,N.\n$$\n\n4) Bias:\nBy definition $B(\\hat{N}) = E[\\hat{N}] - N$, so for $N \\gg n$,\n$$\nB(\\hat{N}) \\approx -\\frac{N}{n+1}.\n$$\nThis gives the desired approximation in terms of $N$ and $n$ for the case $N$ much larger than $n$.", "answer": "$$\\boxed{-\\frac{N}{n+1}}$$", "id": "1933607"}]}