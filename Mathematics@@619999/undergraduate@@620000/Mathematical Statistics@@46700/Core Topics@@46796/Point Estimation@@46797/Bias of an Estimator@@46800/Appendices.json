{"hands_on_practices": [{"introduction": "We begin our exploration of estimator bias with a foundational concept: the properties of linear estimators. The sample mean is a cornerstone of statistics, known for being an unbiased estimator of the population mean. This exercise [@problem_id:1900487] challenges you to analyze a slight variation of the sample mean, where the scaling factor is modified. It provides a crystal-clear illustration of how an estimator's construction directly leads to bias and allows you to quantify that deviation precisely.", "problem": "In statistical analysis, an estimator is a rule for calculating an estimate of a given quantity based on observed data. The bias of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated.\n\nConsider a random sample of size $n$, denoted by $X_1, X_2, \\ldots, X_n$. These are independent and identically distributed random variables drawn from a population with a true mean $\\mu$ and a finite, non-zero variance $\\sigma^2$.\n\nAn analyst proposes a new estimator, $\\hat{\\mu}$, for the population mean $\\mu$, defined as:\n$$\n\\hat{\\mu} = \\frac{1}{n+a} \\sum_{i=1}^{n} X_i\n$$\nwhere $a$ is a known positive constant.\n\nDetermine the bias of this estimator, $\\text{Bias}(\\hat{\\mu})$. Express your answer as an analytic expression in terms of $\\mu$, $n$, and $a$.", "solution": "The bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. In this problem, the parameter is the population mean $\\mu$, and the estimator is $\\hat{\\mu} = \\frac{1}{n+a} \\sum_{i=1}^{n} X_i$. Therefore, we need to calculate $\\text{Bias}(\\hat{\\mu}) = E[\\hat{\\mu}] - \\mu$.\n\nFirst, let's find the expected value of the estimator $\\hat{\\mu}$.\n$$\nE[\\hat{\\mu}] = E\\left[ \\frac{1}{n+a} \\sum_{i=1}^{n} X_i \\right]\n$$\nUsing the linearity property of expectation, we can pull the constant factor $\\frac{1}{n+a}$ out of the expectation, and the expectation of the sum is the sum of the expectations.\n$$\nE[\\hat{\\mu}] = \\frac{1}{n+a} E\\left[ \\sum_{i=1}^{n} X_i \\right] = \\frac{1}{n+a} \\sum_{i=1}^{n} E[X_i]\n$$\nWe are given that each observation $X_i$ is drawn from a population with a true mean $\\mu$. This means that for every $i$ from $1$ to $n$, the expected value of $X_i$ is $\\mu$.\n$$\nE[X_i] = \\mu \\quad \\text{for } i = 1, 2, \\ldots, n\n$$\nSubstituting this into our expression for $E[\\hat{\\mu}]$:\n$$\nE[\\hat{\\mu}] = \\frac{1}{n+a} \\sum_{i=1}^{n} \\mu\n$$\nThe sum consists of $n$ identical terms, each equal to $\\mu$. Therefore, the sum is $n\\mu$.\n$$\nE[\\hat{\\mu}] = \\frac{1}{n+a} (n\\mu) = \\frac{n\\mu}{n+a}\n$$\nNow we can compute the bias by subtracting the true parameter value $\\mu$ from the expected value of the estimator.\n$$\n\\text{Bias}(\\hat{\\mu}) = E[\\hat{\\mu}] - \\mu = \\frac{n\\mu}{n+a} - \\mu\n$$\nTo simplify this expression, we find a common denominator:\n$$\n\\text{Bias}(\\hat{\\mu}) = \\frac{n\\mu}{n+a} - \\frac{\\mu(n+a)}{n+a} = \\frac{n\\mu - (\\mu n + \\mu a)}{n+a}\n$$\n$$\n\\text{Bias}(\\hat{\\mu}) = \\frac{n\\mu - n\\mu - a\\mu}{n+a} = \\frac{-a\\mu}{n+a}\n$$\nThus, the bias of the estimator $\\hat{\\mu}$ is $-\\frac{a\\mu}{n+a}$. Note that the population variance $\\sigma^2$ was not needed for this calculation.", "answer": "$$\\boxed{-\\frac{a\\mu}{n+a}}$$", "id": "1900487"}, {"introduction": "While some biased estimators arise from simple miscalculations, bias can also be an inherent property of well-established estimation techniques. This practice [@problem_id:1900439] delves into the Method of Moments, a systematic procedure for creating estimators. By calculating the bias of an estimator for $\\theta^2$ in a uniform distribution, you will discover that principled methods do not always yield unbiased results and see how bias can depend on the sample size $n$.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n$ drawn from a continuous uniform distribution on the interval $[0, \\theta]$, where $\\theta > 0$ is an unknown parameter. The goal is to estimate the parameter $\\theta^2$.\n\nDetermine the bias of the method of moments estimator of $\\theta^2$. Express your answer as a function of $\\theta$ and the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\operatorname{Uniform}(0,\\theta)$. The method of moments equates the sample mean $\\bar{X}$ to the population mean. For $\\operatorname{Uniform}(0,\\theta)$,\n$$\n\\mathbb{E}[X]=\\frac{\\theta}{2}, \\quad \\operatorname{Var}(X)=\\frac{\\theta^{2}}{12}.\n$$\nEquating $\\bar{X}$ to $\\mathbb{E}[X]$ gives the method of moments estimator for $\\theta$:\n$$\n\\widehat{\\theta}_{\\text{MOM}}=2\\bar{X}.\n$$\nTo estimate $\\theta^{2}$, use the plug-in estimator\n$$\n\\widehat{\\theta^2}_{\\text{MOM}}=(\\widehat{\\theta}_{\\text{MOM}})^{2}=4\\bar{X}^{2}.\n$$\nThe bias of $\\widehat{\\theta^2}_{\\text{MOM}}$ is\n$$\n\\operatorname{Bias}(\\widehat{\\theta^2}_{\\text{MOM}})=\\mathbb{E}[4\\bar{X}^{2}]-\\theta^{2}.\n$$\nUsing $\\mathbb{E}[\\bar{X}]=\\frac{\\theta}{2}$ and $\\operatorname{Var}(\\bar{X})=\\frac{\\operatorname{Var}(X)}{n}=\\frac{\\theta^{2}}{12n}$, and the identity $\\mathbb{E}[\\bar{X}^{2}]=\\operatorname{Var}(\\bar{X})+(\\mathbb{E}[\\bar{X}])^{2}$, we obtain\n$$\n\\mathbb{E}[4\\bar{X}^{2}]=4\\left(\\frac{\\theta^{2}}{12n}+\\left(\\frac{\\theta}{2}\\right)^{2}\\right)=\\frac{\\theta^{2}}{3n}+\\theta^{2}.\n$$\nTherefore,\n$$\n\\operatorname{Bias}(\\widehat{\\theta^2}_{\\text{MOM}})=\\left(\\frac{\\theta^{2}}{3n}+\\theta^{2}\\right)-\\theta^{2}=\\frac{\\theta^{2}}{3n}.\n$$", "answer": "$$\\boxed{\\frac{\\theta^{2}}{3 n}}$$", "id": "1900439"}, {"introduction": "Estimating a function of a parameter, such as its square, presents unique challenges and is a common source of bias. This exercise [@problem_id:1900434] uses the context of a Poisson-distributed random variable—common in modeling discrete events—to explore this very issue. You will assess the intuitive choice of using $X^2$ to estimate $\\lambda^2$ and learn why applying a non-linear function to an estimator often introduces bias, a consequence related to Jensen's Inequality.", "problem": "In a particle physics experiment, the number of rare decay events, $X$, observed in a fixed time interval is modeled by a Poisson distribution with an unknown mean rate $\\lambda > 0$. An experimenter proposes an estimator for the quantity $\\lambda^2$, which is of theoretical interest. Based on a single observation $X$, the proposed estimator is $T = X^2$. The bias of an estimator for a parameter is defined as the difference between the estimator's expected value and the true value of the parameter. Determine the bias of the estimator $T$ for the parameter $\\lambda^2$. Express your answer as a function of $\\lambda$.", "solution": "Let $X \\sim \\text{Poisson}(\\lambda)$ with $\\lambda > 0$, and consider the estimator $T = X^{2}$ for the parameter $\\lambda^{2}$. The bias of $T$ for $\\lambda^{2}$ is defined as\n$$\n\\text{Bias}(T) = \\mathbb{E}[T] - \\lambda^{2} = \\mathbb{E}[X^{2}] - \\lambda^{2}.\n$$\nUsing the identity that for any random variable $X$, $\\mathbb{E}[X^{2}] = \\operatorname{Var}(X) + \\left(\\mathbb{E}[X]\\right)^{2}$, and the properties of the Poisson distribution $\\mathbb{E}[X] = \\lambda$ and $\\operatorname{Var}(X) = \\lambda$, we obtain\n$$\n\\mathbb{E}[X^{2}] = \\lambda + \\lambda^{2}.\n$$\nTherefore, the bias is\n$$\n\\text{Bias}(T) = (\\lambda + \\lambda^{2}) - \\lambda^{2} = \\lambda.\n$$\nSo the estimator $T = X^{2}$ has bias equal to $\\lambda$ for estimating $\\lambda^{2}$.", "answer": "$$\\boxed{\\lambda}$$", "id": "1900434"}]}