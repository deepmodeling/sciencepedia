{"hands_on_practices": [{"introduction": "A fundamental task in statistics is to devise a recipe for estimating unknown parameters from data. The Method of Moments provides a simple and intuitive starting point for this process, based on the principle of matching the theoretical moments of a distribution (like the mean) to their sample-based counterparts. This practice [@problem_id:1944335] will guide you through deriving such an estimator, providing a foundational skill for constructing estimators for a wide variety of models.", "problem": "In a materials science experiment, the normalized fracture toughness, $X$, of a newly developed ceramic composite is found to follow a specific probability distribution. The probability density function (PDF) for a single measurement $X$ is given by\n$$f(x;\\theta) = (\\theta+1)x^{\\theta}$$\nfor $x \\in (0,1)$, where $\\theta > -1$ is an unknown parameter related to the material's microstructure.\n\nTo estimate this parameter, a set of $n$ independent measurements, $X_1, X_2, \\dots, X_n$, is collected from a random sample of the composite. An estimator for $\\theta$, let's call it $\\hat{\\theta}$, is constructed by equating the theoretical mean of the distribution, $E[X]$, to the sample mean, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nBased on this procedure, derive the expression for the estimator $\\hat{\\theta}$ in terms of the sample mean $\\bar{X}$.", "solution": "We are given a random variable $X$ with probability density function $f(x;\\theta)=(\\theta+1)x^{\\theta}$ for $x\\in(0,1)$ and $\\theta>-1$. The method of moments estimator equates the theoretical mean $E[X]$ to the sample mean $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$.\n\nFirst, compute the mean:\n$$\nE[X]=\\int_{0}^{1}x f(x;\\theta)\\,dx=\\int_{0}^{1}x(\\theta+1)x^{\\theta}\\,dx=(\\theta+1)\\int_{0}^{1}x^{\\theta+1}\\,dx.\n$$\nSince $\\theta>-1$, the integral converges and\n$$\n\\int_{0}^{1}x^{\\theta+1}\\,dx=\\left[\\frac{x^{\\theta+2}}{\\theta+2}\\right]_{0}^{1}=\\frac{1}{\\theta+2},\n$$\nhence\n$$\nE[X]=\\frac{\\theta+1}{\\theta+2}.\n$$\n\nSet $E[X]=\\bar{X}$ and solve for $\\theta$:\n$$\n\\bar{X}=\\frac{\\theta+1}{\\theta+2}\\;\\Longrightarrow\\;\\bar{X}(\\theta+2)=\\theta+1,\n$$\n$$\n\\bar{X}\\theta+2\\bar{X}=\\theta+1,\n$$\n$$\n\\bar{X}\\theta-\\theta=1-2\\bar{X},\n$$\n$$\n\\theta(\\bar{X}-1)=1-2\\bar{X},\n$$\n$$\n\\theta=\\frac{1-2\\bar{X}}{\\bar{X}-1}=\\frac{2\\bar{X}-1}{1-\\bar{X}}.\n$$\n\nTherefore, the method of moments estimator is obtained by replacing $\\bar{X}$ with the sample mean:\n$$\n\\hat{\\theta}=\\frac{2\\bar{X}-1}{1-\\bar{X}}.\n$$", "answer": "$$\\boxed{\\frac{2\\bar{X}-1}{1-\\bar{X}}}$$", "id": "1944335"}, {"introduction": "Simply creating an estimator is not enough; we need a way to measure its quality. The Mean Squared Error (MSE) is a crucial metric for evaluating an estimator's performance, as it neatly combines both its accuracy (related to bias) and its precision (related to variance). In this exercise [@problem_id:1944364], you'll compare two plausible estimators for the same parameter by calculating their MSEs, learning a quantitative method for justifying the choice of one estimator over another.", "problem": "Two students, Alice and Bob, are tasked with estimating an unknown physical constant, denoted by $\\mu$. They use a measurement device that is known to produce readings that are normally distributed with mean $\\mu$ and a variance of 1. That is, a single measurement $X$ is a random variable with distribution $N(\\mu, 1)$. They decide to take two independent measurements, $X_1$ and $X_2$.\n\nAlice proposes an estimator for $\\mu$ which is the sample mean:\n$$ \\hat{\\mu}_A = \\frac{1}{2}X_1 + \\frac{1}{2}X_2 $$\n\nBob suggests a different weighted average, arguing that the second measurement might be more \"settled\":\n$$ \\hat{\\mu}_B = \\frac{1}{3}X_1 + \\frac{2}{3}X_2 $$\n\nTo compare the quality of these two estimators, they decide to use the Mean Squared Error (MSE), which for an estimator $\\hat{\\theta}$ of a parameter $\\theta$ is defined as $MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$.\n\nCalculate the ratio of the MSE of Bob's estimator to the MSE of Alice's estimator, $\\frac{MSE(\\hat{\\mu}_B)}{MSE(\\hat{\\mu}_A)}$.", "solution": "Let $X_{1}$ and $X_{2}$ be independent with $X_{i} \\sim N(\\mu,1)$. For any estimator $\\hat{\\theta}$ of $\\theta$, the Mean Squared Error is\n$$\nMSE(\\hat{\\theta})=E\\!\\left[(\\hat{\\theta}-\\theta)^{2}\\right]=\\operatorname{Var}(\\hat{\\theta})+\\left(\\operatorname{Bias}(\\hat{\\theta})\\right)^{2}.\n$$\nCompute the bias for each estimator using linearity of expectation. For Alice,\n$$\nE[\\hat{\\mu}_{A}]=E\\!\\left[\\tfrac{1}{2}X_{1}+\\tfrac{1}{2}X_{2}\\right]=\\tfrac{1}{2}E[X_{1}]+\\tfrac{1}{2}E[X_{2}]=\\tfrac{1}{2}\\mu+\\tfrac{1}{2}\\mu=\\mu,\n$$\nso $\\operatorname{Bias}(\\hat{\\mu}_{A})=0$. For Bob,\n$$\nE[\\hat{\\mu}_{B}]=E\\!\\left[\\tfrac{1}{3}X_{1}+\\tfrac{2}{3}X_{2}\\right]=\\tfrac{1}{3}\\mu+\\tfrac{2}{3}\\mu=\\mu,\n$$\nso $\\operatorname{Bias}(\\hat{\\mu}_{B})=0$. Therefore, for both estimators $MSE=\\operatorname{Var}$.\n\nUse independence to compute the variance of a linear combination: for independent $X_{1},X_{2}$,\n$$\n\\operatorname{Var}(aX_{1}+bX_{2})=a^{2}\\operatorname{Var}(X_{1})+b^{2}\\operatorname{Var}(X_{2}),\n$$\nsince $\\operatorname{Cov}(X_{1},X_{2})=0$. With $\\operatorname{Var}(X_{i})=1$, we get\n$$\n\\operatorname{Var}(\\hat{\\mu}_{A})=\\left(\\tfrac{1}{2}\\right)^{2}\\cdot 1+\\left(\\tfrac{1}{2}\\right)^{2}\\cdot 1=\\tfrac{1}{4}+\\tfrac{1}{4}=\\tfrac{1}{2},\n$$\n$$\n\\operatorname{Var}(\\hat{\\mu}_{B})=\\left(\\tfrac{1}{3}\\right)^{2}\\cdot 1+\\left(\\tfrac{2}{3}\\right)^{2}\\cdot 1=\\tfrac{1}{9}+\\tfrac{4}{9}=\\tfrac{5}{9}.\n$$\nHence,\n$$\n\\frac{MSE(\\hat{\\mu}_{B})}{MSE(\\hat{\\mu}_{A})}=\\frac{\\operatorname{Var}(\\hat{\\mu}_{B})}{\\operatorname{Var}(\\hat{\\mu}_{A})}=\\frac{\\tfrac{5}{9}}{\\tfrac{1}{2}}=\\frac{10}{9}.\n$$", "answer": "$$\\boxed{\\frac{10}{9}}$$", "id": "1944364"}, {"introduction": "After learning to construct and evaluate estimators, the natural next question is: can we find the *best* one? In the realm of unbiased estimation, the 'best' estimator is the one with the smallest possible variance across all values of the parameter, known as the Uniformly Minimum-Variance Unbiased Estimator (UMVUE). This advanced practice [@problem_id:1944380] challenges you to derive the UMVUE by leveraging the powerful concepts of sufficiency and completeness, representing a capstone exercise in the theory of point estimation.", "problem": "A novel type of quantum sensor is designed to measure the one-dimensional position of a particle. Due to inherent quantum uncertainties in its measurement process, when the particle's true position is $\\theta$, a single measurement $X$ from the sensor is a random variable drawn from a uniform probability distribution over the interval $[\\theta - 1/2, \\theta + 1/2]$. An experiment is conducted where $n$ independent measurements, $X_1, X_2, \\dots, X_n$, are taken for a particle held at a fixed but unknown position $\\theta$.\n\nThe objective is to find the best possible estimator for $\\theta$ based on this sample. Your task is to derive the Uniformly Minimum-Variance Unbiased Estimator (UMVUE) for the parameter $\\theta$.\n\nLet the order statistics of the sample be denoted by $X_{(1)} \\le X_{(2)} \\le \\dots \\le X_{(n)}$, where $X_{(1)} = \\min(X_1, \\dots, X_n)$ and $X_{(n)} = \\max(X_1, \\dots, X_n)$. Express your final answer in terms of $X_{(1)}$ and $X_{(n)}$.", "solution": "The observation model is $X_{i} \\sim \\text{Uniform}\\!\\left[\\theta - \\frac{1}{2}, \\theta + \\frac{1}{2}\\right]$, independently for $i=1,\\dots,n$. The density of a single $X_{i}$ is\n$$\nf(x_{i}\\,|\\,\\theta) = \\mathbf{1}\\!\\left\\{\\theta - \\frac{1}{2} \\le x_{i} \\le \\theta + \\frac{1}{2}\\right\\}.\n$$\nHence the joint density is\n$$\nf(x_{1},\\dots,x_{n}\\,|\\,\\theta) = \\prod_{i=1}^{n} \\mathbf{1}\\!\\left\\{\\theta - \\frac{1}{2} \\le x_{i} \\le \\theta + \\frac{1}{2}\\right\\}\n= \\mathbf{1}\\!\\left\\{\\max_{1\\le i\\le n} x_{i} \\le \\theta + \\frac{1}{2},\\ \\min_{1\\le i\\le n} x_{i} \\ge \\theta - \\frac{1}{2}\\right\\}.\n$$\nWriting $X_{(1)}=\\min_{i}X_{i}$ and $X_{(n)}=\\max_{i}X_{i}$, this becomes\n$$\nf(x_{1},\\dots,x_{n}\\,|\\,\\theta) = \\mathbf{1}\\!\\left\\{ \\theta \\in \\left[X_{(n)} - \\frac{1}{2},\\ X_{(1)} + \\frac{1}{2}\\right]\\right\\},\n$$\nwhich depends on the sample only through $(X_{(1)},X_{(n)})$. By the factorization criterion, $(X_{(1)},X_{(n)})$ is a sufficient statistic for $\\theta$.\n\nIntroduce $U_{i} = X_{i} - \\theta + \\frac{1}{2}$, so that $U_{i} \\sim \\text{Uniform}[0,1]$ i.i.d., and $X_{(1)} = \\theta - \\frac{1}{2} + U_{(1)}$, $X_{(n)} = \\theta - \\frac{1}{2} + U_{(n)}$. For $k=1,\\dots,n$, the $k$-th order statistic $U_{(k)}$ has $\\text{Beta}(k,n+1-k)$ distribution, hence\n$$\n\\mathbb{E}[U_{(k)}] = \\frac{k}{n+1}.\n$$\nTherefore\n$$\n\\mathbb{E}\\!\\left[\\frac{X_{(1)} + X_{(n)}}{2}\\right]\n= \\mathbb{E}\\!\\left[\\theta - \\frac{1}{2} + \\frac{U_{(1)} + U_{(n)}}{2}\\right]\n= \\theta - \\frac{1}{2} + \\frac{1}{2}\\left(\\frac{1}{n+1} + \\frac{n}{n+1}\\right)\n= \\theta.\n$$\nThus the midrange $\\frac{X_{(1)} + X_{(n)}}{2}$ is an unbiased estimator of $\\theta$.\n\nTo apply Rao–Blackwellization, start from the unbiased estimator $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ (since $\\mathbb{E}[\\bar{X}] = \\theta$). Given $X_{(1)}=a$ and $X_{(n)}=b$, the remaining $n-2$ observations are conditionally i.i.d. $\\text{Uniform}[a,b]$, so their conditional mean is $\\frac{a+b}{2}$. Therefore\n$$\n\\mathbb{E}[\\bar{X}\\,|\\,X_{(1)}=a, X_{(n)}=b]\n= \\frac{1}{n}\\left(a + b + (n-2)\\cdot \\frac{a+b}{2}\\right)\n= \\frac{a+b}{2}.\n$$\nHence the Rao–Blackwell improvement of $\\bar{X}$ with respect to the sufficient statistic $(X_{(1)},X_{(n)})$ is exactly the midrange $\\frac{X_{(1)} + X_{(n)}}{2}$, which is unbiased.\n\nFinally, $(X_{(1)},X_{(n)})$ is not only sufficient but also complete for this one-parameter uniform location family. A standard way to see this is to express $(X_{(1)},X_{(n)})$ as $(\\theta - \\frac{1}{2} + U_{(1)}, \\theta - \\frac{1}{2} + U_{(n)})$ with $(U_{(1)},U_{(n)})$ having joint density $n(n-1)(v-u)^{n-2}$ on $\\{0 \\le u \\le v \\le 1\\}$. If $\\mathbb{E}_{\\theta}[g(X_{(1)},X_{(n)})]=0$ for all $\\theta$, then for all $t \\in \\mathbb{R}$,\n$$\n\\int_{0}^{1}\\int_{u}^{1} g(t+u, t+v)\\, n(n-1)(v-u)^{n-2}\\, dv\\, du = 0,\n$$\nwhich, by the translation structure and the nondegenerate kernel on the compact support, implies $g=0$ almost everywhere. Thus $(X_{(1)},X_{(n)})$ is complete and sufficient. By the Lehmann–Scheffé theorem, the Rao–Blackwellized unbiased estimator is the UMVUE.\n\nTherefore, the UMVUE for $\\theta$ is\n$$\n\\frac{X_{(1)} + X_{(n)}}{2}.\n$$", "answer": "$$\\boxed{\\frac{X_{(1)}+X_{(n)}}{2}}$$", "id": "1944380"}]}