{"hands_on_practices": [{"introduction": "When estimating a quantity like a physical constant, is it always better to use all available data? This practice provides a quantitative answer by comparing the Mean Squared Error of two different estimators: one that uses all experimental measurements and another that discards some. By calculating the ratio of their MSEs [@problem_id:1934106], you will gain a concrete understanding of statistical efficiency and why the sample mean is a fundamental tool in data analysis.", "problem": "An experimental physicist is trying to determine a fundamental physical constant, $\\mu$. They perform an experiment three times, obtaining measurements $X_1, X_2,$ and $X_3$. These measurements can be modeled as a random sample of size $n=3$ from a probability distribution with an unknown mean $\\mu$ and a finite, positive variance $\\sigma^2$.\n\nAfter collecting the data, the physicist considers two different estimators for $\\mu$:\n1. The standard sample mean: $\\hat{\\mu}_1 = \\frac{X_1 + X_2 + X_3}{3}$\n2. An alternative estimator that discards the third observation: $\\hat{\\mu}_2 = \\frac{X_1 + X_2}{2}$\n\nTo compare the performance of these two estimators, the physicist decides to use the Mean Squared Error (MSE) as a criterion, where the MSE of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$.\n\nWhat is the value of the ratio $\\frac{\\text{MSE}(\\hat{\\mu}_1)}{\\text{MSE}(\\hat{\\mu}_2)}$?\n\nA. $\\frac{1}{3}$\n\nB. $\\frac{2}{3}$\n\nC. $1$\n\nD. $\\frac{4}{3}$\n\nE. $\\frac{3}{2}$\n\nF. The ratio cannot be determined without knowing the values of $\\mu$ and $\\sigma^2$.", "solution": "We are given a random sample $X_{1}, X_{2}, X_{3}$ from a distribution with unknown mean $\\mu$ and finite, positive variance $\\sigma^{2}$. Define the estimators\n$$\n\\hat{\\mu}_{1}=\\frac{X_{1}+X_{2}+X_{3}}{3}, \\quad \\hat{\\mu}_{2}=\\frac{X_{1}+X_{2}}{2}.\n$$\nThe Mean Squared Error (MSE) of an estimator $\\hat{\\theta}$ for parameter $\\theta$ is $\\text{MSE}(\\hat{\\theta})=E[(\\hat{\\theta}-\\theta)^{2}]$. Using the bias-variance decomposition,\n$$\n\\text{MSE}(\\hat{\\theta})=\\operatorname{Var}(\\hat{\\theta})+\\left(\\operatorname{Bias}(\\hat{\\theta})\\right)^{2}, \\quad \\operatorname{Bias}(\\hat{\\theta})=E[\\hat{\\theta}]-\\theta.\n$$\n\nFirst, compute the biases. Since $E[X_{i}]=\\mu$ for each $i$,\n$$\nE[\\hat{\\mu}_{1}]=E\\left[\\frac{X_{1}+X_{2}+X_{3}}{3}\\right]=\\frac{E[X_{1}]+E[X_{2}]+E[X_{3}]}{3}=\\frac{3\\mu}{3}=\\mu,\n$$\n$$\nE[\\hat{\\mu}_{2}]=E\\left[\\frac{X_{1}+X_{2}}{2}\\right]=\\frac{E[X_{1}]+E[X_{2}]}{2}=\\frac{2\\mu}{2}=\\mu.\n$$\nThus both estimators are unbiased, so for each,\n$$\n\\text{MSE}(\\hat{\\mu}_{j})=\\operatorname{Var}(\\hat{\\mu}_{j}), \\quad j\\in\\{1,2\\}.\n$$\n\nNext, compute the variances using independence (random sample) and $\\operatorname{Var}(aY)=a^{2}\\operatorname{Var}(Y)$:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{1})=\\operatorname{Var}\\left(\\frac{X_{1}+X_{2}+X_{3}}{3}\\right)=\\frac{1}{9}\\operatorname{Var}(X_{1}+X_{2}+X_{3})=\\frac{1}{9}\\left(\\sigma^{2}+\\sigma^{2}+\\sigma^{2}\\right)=\\frac{\\sigma^{2}}{3},\n$$\n$$\n\\operatorname{Var}(\\hat{\\mu}_{2})=\\operatorname{Var}\\left(\\frac{X_{1}+X_{2}}{2}\\right)=\\frac{1}{4}\\operatorname{Var}(X_{1}+X_{2})=\\frac{1}{4}\\left(\\sigma^{2}+\\sigma^{2}\\right)=\\frac{\\sigma^{2}}{2}.\n$$\n\nTherefore,\n$$\n\\frac{\\text{MSE}(\\hat{\\mu}_{1})}{\\text{MSE}(\\hat{\\mu}_{2})}=\\frac{\\sigma^{2}/3}{\\sigma^{2}/2}=\\frac{1/3}{1/2}=\\frac{2}{3}.\n$$\nThis corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1934106"}, {"introduction": "Building on the idea of estimator quality, this practice explores the crucial bias-variance trade-off. Here, you will compare a standard, unbiased estimator (the sample mean) against a simple, but biased, estimator (a fixed target value). This exercise demonstrates that an estimator's quality, as measured by MSE, depends on both its variance and its bias, challenging the intuition that an unbiased estimator is always superior [@problem_id:1934128].", "problem": "A quality control engineer in a semiconductor fabrication plant is monitoring a chemical vapor deposition process. The thickness of a deposited silicon nitride layer, $X$, is a critical parameter. From historical data, the thickness measurements for this process are known to be normally distributed with a mean $\\mu$ that can drift between production runs, but with a stable and known process variance of $\\sigma^2 = 16.0$ nm$^2$.\n\nFor a new production run, the engineer considers two different methods to estimate the true mean thickness $\\mu$:\n\n1.  **The Sample Mean Estimator ($\\hat{\\mu}_S$):** This involves taking a random sample of $n=50$ wafers from the run and calculating their average thickness, $\\bar{X}$. So, $\\hat{\\mu}_S = \\bar{X}$.\n2.  **The Biased Legacy Estimator ($\\hat{\\mu}_L$):** This involves using a fixed value of $152.0$ nm, which was the target thickness for a previous-generation product line. Thus, $\\hat{\\mu}_L = 152.0$.\n\nAfter the run is complete, a highly accurate, but slow, metrology tool determines that the true mean thickness for this specific run was actually $\\mu = 150.5$ nm.\n\nTo retroactively evaluate which estimation strategy was superior for this particular run, the engineer decides to compare their respective Mean Squared Error (MSE). Calculate the difference $MSE(\\hat{\\mu}_L) - MSE(\\hat{\\mu}_S)$. Express your answer in nm$^2$, rounded to three significant figures.", "solution": "We are asked to compare the mean squared errors of two estimators for the true mean thickness $\\mu$ when the process variance is known to be $\\sigma^2 = 16.0$ nm$^2$ and the true run mean is $\\mu=150.5$ nm. The MSE of an estimator $\\hat{\\theta}$ for parameter $\\theta$ is defined by\n$$\n\\operatorname{MSE}(\\hat{\\theta})=E\\big[(\\hat{\\theta}-\\theta)^{2}\\big]=\\operatorname{Var}(\\hat{\\theta})+\\big(\\operatorname{Bias}(\\hat{\\theta})\\big)^{2},\n$$\nwhere $\\operatorname{Bias}(\\hat{\\theta})=E[\\hat{\\theta}]-\\theta$.\n\nFor the sample mean estimator $\\hat{\\mu}_{S}=\\bar{X}$ based on $n=50$ i.i.d. observations with variance $\\sigma^{2}$, we use $E[\\bar{X}]=\\mu$ and $\\operatorname{Var}(\\bar{X})=\\frac{\\sigma^{2}}{n}$. Therefore, the bias is $0$ and\n$$\n\\operatorname{MSE}(\\hat{\\mu}_{S})=\\operatorname{Var}(\\bar{X})+\\big(0\\big)^{2}=\\frac{\\sigma^{2}}{n}=\\frac{16.0}{50}=0.32 \\text{ nm}^{2}.\n$$\n\nFor the legacy estimator $\\hat{\\mu}_{L}=152.0$ (a constant), its variance is $0$ and its bias relative to the true $\\mu=150.5$ is\n$$\n\\operatorname{Bias}(\\hat{\\mu}_{L})=E[\\hat{\\mu}_{L}]-\\mu=152.0-150.5=1.5 \\text{ nm},\n$$\nso\n$$\n\\operatorname{MSE}(\\hat{\\mu}_{L})=0+(1.5)^{2}=2.25 \\text{ nm}^{2}.\n$$\n\nThe requested difference is\n$$\n\\operatorname{MSE}(\\hat{\\mu}_{L})-\\operatorname{MSE}(\\hat{\\mu}_{S})=2.25-0.32=1.93 \\text{ nm}^{2},\n$$\nwhich to three significant figures is $1.93$ in nm$^{2}$.", "answer": "$$\\boxed{1.93}$$", "id": "1934128"}, {"introduction": "Having learned to evaluate and compare estimators, the next logical step is to design an optimal one. This exercise asks you to find the constant $c$ that makes the estimator $\\hat{\\theta} = cX$ as accurate as possible by minimizing its Mean Squared Error. By using calculus to solve this optimization problem [@problem_id:1934177], you will transition from analyzing existing estimators to actively constructing new ones with desirable properties.", "problem": "A materials scientist is investigating the properties of a novel photoelectric film. When this film is exposed to a brief pulse of light, it generates a voltage that decays over time. According to a simplified theoretical model, the time $T$ it takes for the voltage to decay to a certain threshold is a random variable that follows a uniform distribution on the interval $[0, \\theta]$. Here, $\\theta$ is an unknown, non-zero physical parameter representing the maximum possible decay time for this process.\n\nTo estimate the value of $\\theta$, an experiment is conducted which yields a single measurement of the decay time, denoted by the random variable $X$. Based on this single observation, an estimator for $\\theta$ is proposed in the form $\\hat{\\theta} = cX$, where $c$ is a real constant. The quality of this estimator is to be judged by its Mean Squared Error (MSE), which is defined as $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$.\n\nDetermine the numerical value of the constant $c$ that minimizes the MSE of the estimator $\\hat{\\theta}$.", "solution": "Let $X$ be uniformly distributed on $[0,\\theta]$, so its probability density function is $f_{X}(x)=\\frac{1}{\\theta}$ for $0 \\leq x \\leq \\theta$ and $0$ otherwise.\n\nCompute the first two moments of $X$:\n$$\nE[X]=\\int_{0}^{\\theta} x \\cdot \\frac{1}{\\theta} \\, dx=\\frac{1}{\\theta}\\left[\\frac{x^{2}}{2}\\right]_{0}^{\\theta}=\\frac{\\theta}{2},\n$$\n$$\nE[X^{2}]=\\int_{0}^{\\theta} x^{2} \\cdot \\frac{1}{\\theta} \\, dx=\\frac{1}{\\theta}\\left[\\frac{x^{3}}{3}\\right]_{0}^{\\theta}=\\frac{\\theta^{2}}{3},\n$$\n$$\n\\operatorname{Var}(X)=E[X^{2}]-(E[X])^{2}=\\frac{\\theta^{2}}{3}-\\left(\\frac{\\theta}{2}\\right)^{2}=\\frac{\\theta^{2}}{12}.\n$$\n\nConsider the estimator $\\hat{\\theta}=cX$, where $c \\in \\mathbb{R}$. Its expectation and variance are\n$$\nE[\\hat{\\theta}]=c\\,E[X]=c \\cdot \\frac{\\theta}{2}, \n\\qquad\n\\operatorname{Var}(\\hat{\\theta})=c^{2}\\operatorname{Var}(X)=c^{2}\\cdot \\frac{\\theta^{2}}{12}.\n$$\nThe bias is\n$$\n\\operatorname{Bias}(\\hat{\\theta})=E[\\hat{\\theta}]-\\theta=\\theta\\left(\\frac{c}{2}-1\\right),\n$$\nso the squared bias is\n$$\n\\operatorname{Bias}(\\hat{\\theta})^{2}=\\theta^{2}\\left(\\frac{c}{2}-1\\right)^{2}.\n$$\n\nBy the bias-variance decomposition,\n$$\n\\operatorname{MSE}(\\hat{\\theta})=\\operatorname{Var}(\\hat{\\theta})+\\operatorname{Bias}(\\hat{\\theta})^{2}\n=\\theta^{2}\\left(\\frac{c^{2}}{12}+\\left(\\frac{c}{2}-1\\right)^{2}\\right).\n$$\nSince $\\theta^{2}>0$, minimizing $\\operatorname{MSE}(\\hat{\\theta})$ over $c$ is equivalent to minimizing\n$$\nf(c)=\\frac{c^{2}}{12}+\\left(\\frac{c}{2}-1\\right)^{2}\n=\\frac{c^{2}}{12}+\\frac{c^{2}}{4}-c+1\n=\\frac{c^{2}}{3}-c+1.\n$$\nDifferentiate and set to zero:\n$$\nf'(c)=\\frac{2}{3}c-1=0 \\quad \\Longrightarrow \\quad c=\\frac{3}{2}.\n$$\nThe second derivative is $f''(c)=\\frac{2}{3}>0$, confirming a minimum. Therefore, the value of $c$ that minimizes the MSE is $\\frac{3}{2}$.", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "1934177"}]}