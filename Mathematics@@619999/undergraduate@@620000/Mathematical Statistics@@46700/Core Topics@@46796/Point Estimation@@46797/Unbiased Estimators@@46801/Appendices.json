{"hands_on_practices": [{"introduction": "A fundamental task in statistics is to evaluate the quality of an estimator. This first exercise [@problem_id:1965917] explores the concept of bias by examining a scenario where our intuition might mislead us. You will discover that even the familiar sample mean, $\\bar{X}$, can be a biased estimator and learn to precisely quantify this bias, a foundational skill for constructing better estimators.", "problem": "A machine in a factory is designed to produce metal rods. Due to a slight, constant calibration error, the length of any rod produced, $X$, is a random variable that follows a Uniform distribution on the interval $[\\theta, \\theta+1]$, where length is measured in centimeters and $\\theta$ is an unknown parameter representing the calibration offset. To estimate this offset, an engineer collects a random sample of $n$ rods with lengths $X_1, X_2, \\dots, X_n$. The engineer proposes using the sample mean, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$, as an estimator for $\\theta$.\n\nThe bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as the difference between the expected value of the estimator and the true value of the parameter: $\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. An estimator with non-zero bias is called biased.\n\nCalculate the bias of the sample mean $\\bar{X}$ when used as an estimator for the parameter $\\theta$. Present your answer as a single numerical value.", "solution": "Let $X_{1},\\dots,X_{n}$ be independent and identically distributed as $\\text{Uniform}[\\theta,\\theta+1]$. The sample mean is $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. The bias of $\\bar{X}$ as an estimator of $\\theta$ is defined as $\\text{Bias}(\\bar{X})=E[\\bar{X}]-\\theta$.\n\nFirst compute $E[X_{i}]$ for a single observation. Using the definition of expectation with the uniform density on $[\\theta,\\theta+1]$, which is $f(x)=1$ for $x\\in[\\theta,\\theta+1]$ and $0$ otherwise, we have\n$$\nE[X_{i}] = \\int_{\\theta}^{\\theta+1} x \\cdot 1 \\, dx = \\left.\\frac{x^{2}}{2}\\right|_{\\theta}^{\\theta+1} = \\frac{(\\theta+1)^{2}-\\theta^{2}}{2} = \\frac{2\\theta+1}{2} = \\theta + \\frac{1}{2}.\n$$\nBy linearity of expectation,\n$$\nE[\\bar{X}] = E\\!\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n}E[X_{i}] = \\frac{1}{n}\\cdot n\\left(\\theta+\\frac{1}{2}\\right) = \\theta + \\frac{1}{2}.\n$$\nTherefore, the bias of $\\bar{X}$ as an estimator for $\\theta$ is\n$$\n\\text{Bias}(\\bar{X}) = E[\\bar{X}] - \\theta = \\left(\\theta + \\frac{1}{2}\\right) - \\theta = \\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1965917"}, {"introduction": "Once we identify that an estimator is biased, the next logical step is to correct it. This practice [@problem_id:1965899] presents a common task: estimating a function of a parameter, specifically $\\mu^2$ for a normal distribution. You will see how a naive estimator is biased and then apply the properties of expectation and variance to construct a new, corrected estimator that is unbiased.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n$ drawn from a normal distribution with an unknown mean $\\mu$ and a known variance of 1. The probability density function for this distribution is given by $f(x; \\mu) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2}\\right)$. The parameter of interest is $\\theta = \\mu^2$.\n\nFind an unbiased estimator for $\\theta$. Express your answer as a closed-form analytic expression in terms of the sample mean, $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$, and the sample size $n$.", "solution": "We have independent and identically distributed random variables $X_{1},\\ldots,X_{n}$ with $X_{i}\\sim \\mathcal{N}(\\mu,1)$. The sample mean is $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. By linearity of expectation and independence,\n$$\n\\mathbb{E}[\\bar{X}]=\\mu,\\quad \\mathrm{Var}(\\bar{X})=\\mathrm{Var}\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right)=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\mathrm{Var}(X_{i})=\\frac{1}{n}.\n$$\nUsing the identity $\\mathbb{E}[Y^{2}]=\\mathrm{Var}(Y)+\\{\\mathbb{E}[Y]\\}^{2}$ for any random variable $Y$, applied to $Y=\\bar{X}$, we obtain\n$$\n\\mathbb{E}[\\bar{X}^{2}]=\\mathrm{Var}(\\bar{X})+\\{\\mathbb{E}[\\bar{X}]\\}^{2}=\\frac{1}{n}+\\mu^{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[\\bar{X}^{2}-\\frac{1}{n}\\right]=\\mu^{2}=\\theta,\n$$\nso an unbiased estimator of $\\theta$ is\n$$\n\\hat{\\theta}=\\bar{X}^{2}-\\frac{1}{n}.\n$$\nThis expression is in closed form in terms of $\\bar{X}$ and $n$.", "answer": "$$\\boxed{\\bar{X}^{2}-\\frac{1}{n}}$$", "id": "1965899"}, {"introduction": "The true power of statistical principles is revealed when we apply them to non-standard problems. This final practice [@problem_id:1965896] presents a creative thought experiment that develops problem-solving skills for such challenges, asking you to estimate a total length $L$ based only on observing its shorter, broken piece. This exercise will strengthen your ability to derive results from first principles, a crucial skill for tackling unique, real-world data problems.", "problem": "An engineer is tasked with estimating an unknown length parameter $L$ of a newly manufactured type of structural rod. The quality control process involves subjecting each rod to a stress test that causes it to break at a single, random point. It is known from the physics of the fracture process that the break point is distributed uniformly along the length of the rod. After the break, only one of the two pieces is recovered for analysis. Due to the collection mechanism, the piece that is recovered is always the shorter of the two. Let the random variable $Y$ represent the length of this shorter piece.\n\nBased on a single observation of $Y$, find an unbiased estimator, which we will call $\\hat{L}$, for the unknown total length $L$. Your answer should be an expression for $\\hat{L}$ in terms of $Y$.", "solution": "Let $X$ denote the break point measured from one end of the rod. By the problem statement, $X$ is uniformly distributed on $[0, L]$, i.e., $X \\sim \\text{Uniform}(0, L)$. The observed piece length is the shorter of the two fragments, so define $Y = \\min(X, L - X)$.\n\nFirst, derive the distribution of $Y$. For $y \\in [0, L/2]$,\n$$\n\\{Y > y\\} = \\{\\min(X, L - X) > y\\} = \\{X > y \\text{ and } L - X > y\\} = \\{y < X < L - y\\}.\n$$\nSince $X$ is uniform on $[0, L]$,\n$$\n\\mathbb{P}(Y > y) = \\frac{L - 2y}{L}, \\quad 0 \\leq y \\leq \\frac{L}{2}.\n$$\nTherefore the cumulative distribution function of $Y$ is\n$$\nF_{Y}(y) = \\mathbb{P}(Y \\leq y) = 1 - \\frac{L - 2y}{L} = \\frac{2y}{L}, \\quad 0 \\leq y \\leq \\frac{L}{2},\n$$\nwith $F_{Y}(y) = 0$ for $y < 0$ and $F_{Y}(y) = 1$ for $y > \\frac{L}{2}$. Differentiating, the probability density function is\n$$\nf_{Y}(y) = \\frac{2}{L}, \\quad 0 \\leq y \\leq \\frac{L}{2},\n$$\nso $Y \\sim \\text{Uniform}(0, L/2)$.\n\nCompute the expectation:\n$$\n\\mathbb{E}[Y] = \\int_{0}^{L/2} y \\cdot \\frac{2}{L} \\, dy = \\frac{2}{L} \\cdot \\left[\\frac{y^{2}}{2}\\right]_{0}^{L/2} = \\frac{1}{L} \\cdot \\frac{L^{2}}{4} = \\frac{L}{4}.\n$$\n\nTo construct an unbiased estimator $\\hat{L} = g(Y)$ based on a single observation of $Y$, consider the linear form $\\hat{L} = cY$. Its expectation is\n$$\n\\mathbb{E}[\\hat{L}] = \\mathbb{E}[cY] = c \\, \\mathbb{E}[Y] = c \\cdot \\frac{L}{4}.\n$$\nUnbiasedness requires $\\mathbb{E}[\\hat{L}] = L$ for all $L$, which is satisfied by choosing $c = 4$. Hence an unbiased estimator is\n$$\n\\hat{L} = 4Y,\n$$\nand indeed $\\mathbb{E}[\\hat{L}] = 4 \\cdot \\frac{L}{4} = L$.", "answer": "$$\\boxed{4Y}$$", "id": "1965896"}]}