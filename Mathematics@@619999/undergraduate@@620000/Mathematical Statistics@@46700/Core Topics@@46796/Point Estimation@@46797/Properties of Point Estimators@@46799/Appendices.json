{"hands_on_practices": [{"introduction": "The first step in evaluating an estimator is to check for systemic error, or bias. An estimator is considered unbiased if, on average, it hits the true value of the parameter we're trying to estimate. This practice provides a foundational exercise in calculating the expected value of an estimator to determine if it is biased, a crucial skill for validating statistical methods [@problem_id:1948686].", "problem": "In a materials science laboratory, a process is developed to create thin films whose thickness, $X$, is a random variable. The process is calibrated such that the thickness is uniformly distributed on the interval $[0, \\theta]$, where $\\theta$ is an unknown maximum possible thickness. To estimate this parameter $\\theta$, a researcher collects a random sample of $n$ films, with thicknesses denoted by $X_1, X_2, \\ldots, X_n$. An estimator for $\\theta$ is proposed as $\\hat{\\theta} = 2\\bar{X}$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ is the sample mean of the thicknesses.\n\nDetermine whether the proposed estimator $\\hat{\\theta}$ is biased or unbiased for the true parameter $\\theta$.\n\nA. Yes, the estimator is unbiased.\n\nB. No, the estimator has a positive bias.\n\nC. No, the estimator has a negative bias.\n\nD. The nature of the bias (positive, negative, or zero) depends on the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. with $X_{i}\\sim \\text{Uniform}(0,\\theta)$. The density is $f_{X}(x;\\theta)=\\frac{1}{\\theta}$ for $0\\leq x\\leq \\theta$, and $0$ otherwise. Compute the mean of a single observation:\n$$\n\\mathbb{E}[X_{i}]=\\int_{0}^{\\theta}x\\cdot \\frac{1}{\\theta}\\,dx=\\frac{1}{\\theta}\\left[\\frac{x^{2}}{2}\\right]_{0}^{\\theta}=\\frac{1}{\\theta}\\cdot \\frac{\\theta^{2}}{2}=\\frac{\\theta}{2}.\n$$\nBy linearity of expectation, the sample mean satisfies\n$$\n\\mathbb{E}[\\bar{X}]=\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[X_{i}]=\\frac{1}{n}\\cdot n\\cdot \\frac{\\theta}{2}=\\frac{\\theta}{2}.\n$$\nFor the proposed estimator $\\hat{\\theta}=2\\bar{X}$,\n$$\n\\mathbb{E}[\\hat{\\theta}]=\\mathbb{E}[2\\bar{X}]=2\\,\\mathbb{E}[\\bar{X}]=2\\cdot \\frac{\\theta}{2}=\\theta.\n$$\nThe bias is defined as $\\text{Bias}(\\hat{\\theta})=\\mathbb{E}[\\hat{\\theta}]-\\theta$, hence\n$$\n\\text{Bias}(\\hat{\\theta})=\\theta-\\theta=0,\n$$\nso the estimator is unbiased. Therefore, the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1948686"}, {"introduction": "While bias tells us about an estimator's average accuracy, it doesn't capture the full picture of its performance; we also care about its precision. The Mean Squared Error (MSE) is a comprehensive measure that combines an estimator's variance (precision) and its bias (accuracy) into a single value representing the overall quality. This exercise challenges you to compute the MSE for an estimator derived from the sample maximum, illustrating how to formally quantify an estimator's total expected error [@problem_id:1948677].", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n$ from a continuous uniform distribution on the interval $[0, \\theta]$, where $\\theta > 0$ is an unknown parameter. The statistic $Y = X_{(n)}$, defined as the maximum value in the sample, i.e., $Y = \\max\\{X_1, X_2, \\dots, X_n\\}$, is proposed as an estimator for $\\theta$.\n\nCalculate the Mean Squared Error (MSE) of the estimator $Y$. Express your final answer as a function of the sample size $n$ and the parameter $\\theta$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. $\\operatorname{Uniform}(0,\\theta)$ and $Y=X_{(n)}=\\max\\{X_{1},\\dots,X_{n}\\}$. For $0\\leq y\\leq \\theta$, the cumulative distribution function of $Y$ is\n$$\nF_{Y}(y)=\\Pr(Y\\leq y)=\\Pr(X_{1}\\leq y,\\dots,X_{n}\\leq y)=\\left(\\frac{y}{\\theta}\\right)^{n},\n$$\nso the probability density function is\n$$\nf_{Y}(y)=\\frac{\\mathrm{d}}{\\mathrm{d}y}F_{Y}(y)=\\frac{n\\,y^{n-1}}{\\theta^{n}},\\quad 0\\leq y\\leq \\theta.\n$$\n\nCompute the moments of $Y$. For any $k>-n$,\n$$\n\\mathbb{E}[Y^{k}]=\\int_{0}^{\\theta} y^{k} f_{Y}(y)\\,\\mathrm{d}y=\\frac{n}{\\theta^{n}}\\int_{0}^{\\theta} y^{n+k-1}\\,\\mathrm{d}y=\\frac{n}{\\theta^{n}}\\cdot \\frac{\\theta^{n+k}}{n+k}=\\frac{n\\,\\theta^{k}}{n+k}.\n$$\nThus,\n$$\n\\mathbb{E}[Y]=\\frac{n\\,\\theta}{n+1},\\qquad \\mathbb{E}[Y^{2}]=\\frac{n\\,\\theta^{2}}{n+2}.\n$$\nThe variance is\n$$\n\\operatorname{Var}(Y)=\\mathbb{E}[Y^{2}]-(\\mathbb{E}[Y])^{2}=\\frac{n\\,\\theta^{2}}{n+2}-\\frac{n^{2}\\theta^{2}}{(n+1)^{2}}=\\frac{n\\,\\theta^{2}}{(n+1)^{2}(n+2)}.\n$$\n\nThe bias of $Y$ as an estimator of $\\theta$ is\n$$\n\\operatorname{Bias}(Y)=\\mathbb{E}[Y]-\\theta=\\theta\\left(\\frac{n}{n+1}-1\\right)=-\\frac{\\theta}{n+1},\n$$\nso\n$$\n\\operatorname{Bias}(Y)^{2}=\\frac{\\theta^{2}}{(n+1)^{2}}.\n$$\nTherefore, the mean squared error is\n$$\n\\operatorname{MSE}(Y)=\\operatorname{Var}(Y)+\\operatorname{Bias}(Y)^{2}=\\frac{n\\,\\theta^{2}}{(n+1)^{2}(n+2)}+\\frac{\\theta^{2}}{(n+1)^{2}}=\\frac{2\\,\\theta^{2}}{(n+1)(n+2)}.\n$$", "answer": "$$\\boxed{\\frac{2\\theta^{2}}{(n+1)(n+2)}}$$", "id": "1948677"}, {"introduction": "In practice, we often have multiple plausible estimators for the same parameter. The concept of efficiency, assessed through the Mean Squared Error, provides a powerful criterion for choosing the best one. This problem presents a realistic scenario where you must compare two different unbiased estimators for the rate parameter of a Poisson distribution, demonstrating how MSE is used to make a principled choice and select the more precise tool for the job [@problem_id:1948719].", "problem": "Two competing research groups are analyzing data on the number of photons detected from a distant, faint star over a series of short, equal-duration observation windows. Based on theoretical models of photon emission, the number of photons, $X$, detected in any given window is assumed to follow a Poisson distribution with an unknown mean rate $\\lambda$. Both groups have access to the same dataset, which consists of photon counts $X_1, X_2, \\ldots, X_n$ from $n$ independent observation windows, where the sample size $n > 1$.\n\nThe first group proposes to estimate the rate $\\lambda$ using the sample mean, which they denote as $\\hat{\\lambda}_1$:\n$$\n\\hat{\\lambda}_1 = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\n$$\nThe second group, leveraging the property that the variance of a Poisson distribution is equal to its mean, proposes an alternative estimator using the sample variance, denoted as $\\hat{\\lambda}_2$:\n$$\n\\hat{\\lambda}_2 = S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\bar{X})^2\n$$\nTo determine which estimator is more precise for this particular distribution, you are tasked with comparing their performance using the Mean Squared Error (MSE).\n\nCalculate the ratio $R = \\frac{\\text{MSE}(\\hat{\\lambda}_2)}{\\text{MSE}(\\hat{\\lambda}_1)}$. Your final answer should be a closed-form analytic expression in terms of the rate $\\lambda$ and the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be independent with $X_{i}\\sim \\text{Poisson}(\\lambda)$. Denote the sample mean by $\\bar{X}$ and the sample variance by $S^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}$. For any estimator $\\hat{\\theta}$ of a parameter $\\theta$, the Mean Squared Error is\n$$\n\\text{MSE}(\\hat{\\theta})=\\mathbb{E}\\big[(\\hat{\\theta}-\\theta)^{2}\\big]=\\operatorname{Var}(\\hat{\\theta})+\\big(\\mathbb{E}[\\hat{\\theta}]-\\theta\\big)^{2}.\n$$\n\nFirst estimator $\\hat{\\lambda}_{1}=\\bar{X}$:\nSince $\\mathbb{E}[X_{i}]=\\lambda$ and $\\operatorname{Var}(X_{i})=\\lambda$, independence implies\n$$\n\\mathbb{E}[\\bar{X}]=\\lambda,\\qquad \\operatorname{Var}(\\bar{X})=\\frac{\\operatorname{Var}(X_{i})}{n}=\\frac{\\lambda}{n}.\n$$\nThus $\\hat{\\lambda}_{1}$ is unbiased and\n$$\n\\text{MSE}(\\hat{\\lambda}_{1})=\\operatorname{Var}(\\bar{X})=\\frac{\\lambda}{n}.\n$$\n\nSecond estimator $\\hat{\\lambda}_{2}=S^{2}$:\nFor any distribution with finite second moment, $S^{2}$ is an unbiased estimator of the population variance $\\sigma^{2}$, so $\\mathbb{E}[S^{2}]=\\sigma^{2}$. Here $\\sigma^{2}=\\operatorname{Var}(X_{i})=\\lambda$, hence $\\mathbb{E}[\\hat{\\lambda}_{2}]=\\lambda$, so $\\hat{\\lambda}_{2}$ is also unbiased and\n$$\n\\text{MSE}(\\hat{\\lambda}_{2})=\\operatorname{Var}(S^{2}).\n$$\nA standard formula for the variance of the unbiased sample variance in terms of the fourth central moment $\\mu_{4}=\\mathbb{E}[(X-\\mathbb{E}X)^{4}]$ and $\\sigma^{2}$ is\n$$\n\\operatorname{Var}(S^{2})=\\frac{1}{n}\\left(\\mu_{4}-\\frac{n-3}{n-1}\\sigma^{4}\\right).\n$$\nFor a Poisson$(\\lambda)$ distribution, all cumulants equal $\\lambda$, and the fourth central moment is $\\mu_{4}=\\kappa_{4}+3\\kappa_{2}^{2}=\\lambda+3\\lambda^{2}$, while $\\sigma^{2}=\\lambda$. Substituting,\n$$\n\\operatorname{Var}(S^{2})=\\frac{1}{n}\\left(\\lambda+3\\lambda^{2}-\\frac{n-3}{n-1}\\lambda^{2}\\right)\n=\\frac{1}{n}\\left(\\lambda+\\lambda^{2}\\left(3-\\frac{n-3}{n-1}\\right)\\right).\n$$\nCompute the bracketed term:\n$$\n3-\\frac{n-3}{n-1}=\\frac{3(n-1)-(n-3)}{n-1}=\\frac{2n}{n-1},\n$$\nso\n$$\n\\operatorname{Var}(S^{2})=\\frac{\\lambda}{n}+\\frac{2\\lambda^{2}}{n-1}.\n$$\nTherefore,\n$$\n\\text{MSE}(\\hat{\\lambda}_{2})=\\frac{\\lambda}{n}+\\frac{2\\lambda^{2}}{n-1},\\qquad \\text{MSE}(\\hat{\\lambda}_{1})=\\frac{\\lambda}{n}.\n$$\nThe required ratio is\n$$\nR=\\frac{\\text{MSE}(\\hat{\\lambda}_{2})}{\\text{MSE}(\\hat{\\lambda}_{1})}\n=\\frac{\\frac{\\lambda}{n}+\\frac{2\\lambda^{2}}{n-1}}{\\frac{\\lambda}{n}}\n=1+\\frac{2n\\lambda}{n-1}.\n$$", "answer": "$$\\boxed{1+\\frac{2 n \\lambda}{n-1}}$$", "id": "1948719"}]}