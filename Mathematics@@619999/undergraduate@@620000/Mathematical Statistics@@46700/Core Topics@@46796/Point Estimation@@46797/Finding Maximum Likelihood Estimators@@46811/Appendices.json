{"hands_on_practices": [{"introduction": "Our first practice exercise places us in the role of a physicist calibrating a sensitive instrument whose measurement errors are modeled by a Laplace distribution. This problem is a perfect starting point, as it allows us to apply the standard, calculus-based workflow of Maximum Likelihood Estimation: defining the likelihood, taking the logarithm, and finding the maximum via differentiation. By working through this example, you will solidify your understanding of how to derive an estimator for a parameter in a continuous probability distribution. [@problem_id:1917514]", "problem": "In the field of quantum optics, researchers are developing an advanced sensor to detect minute energy fluctuations in a vacuum. Theoretical models predict that the error in each measurement, represented by a random variable $X$, follows a Laplace distribution after the sensor is calibrated to have a mean error of zero. The distribution's spread, and thus the sensor's precision, is characterized by a single unknown scale parameter $b > 0$.\n\nThe probability density function (PDF) for a single measurement error $x$, given the scale parameter $b$, is:\n$$ f(x | b) = \\frac{1}{2b} \\exp\\left( -\\frac{|x|}{b} \\right), \\quad \\text{for } -\\infty < x < \\infty $$\n\nAn experiment is conducted, yielding a random sample of $n$ independent measurements: $x_1, x_2, \\ldots, x_n$. Your task is to determine the Maximum Likelihood Estimator (MLE) for the scale parameter $b$. Express your answer, denoted as $\\hat{b}$, as a closed-form analytic expression in terms of the sample size $n$ and the measurements $x_i$.", "solution": "We have independent observations $x_{1},\\ldots,x_{n}$ from the Laplace density with mean zero and scale $b>0$:\n$$\nf(x\\mid b)=\\frac{1}{2b}\\exp\\!\\left(-\\frac{|x|}{b}\\right).\n$$\nThe likelihood for $b$ given the sample is\n$$\nL(b)=\\prod_{i=1}^{n}\\frac{1}{2b}\\exp\\!\\left(-\\frac{|x_{i}|}{b}\\right)\n=\\left(\\frac{1}{2b}\\right)^{n}\\exp\\!\\left(-\\frac{1}{b}\\sum_{i=1}^{n}|x_{i}|\\right).\n$$\nThe log-likelihood is\n$$\n\\ell(b)=\\ln L(b)=-n\\ln 2 - n\\ln b - \\frac{1}{b}\\sum_{i=1}^{n}|x_{i}|.\n$$\nDifferentiate with respect to $b$ and set to zero:\n$$\n\\frac{d\\ell}{db}=-\\frac{n}{b}+\\frac{1}{b^{2}}\\sum_{i=1}^{n}|x_{i}|=0.\n$$\nLet $S=\\sum_{i=1}^{n}|x_{i}|$. Then the score equation becomes\n$$\n-\\frac{n}{b}+\\frac{S}{b^{2}}=0 \\quad\\Longrightarrow\\quad S=nb \\quad\\Longrightarrow\\quad \\hat{b}=\\frac{S}{n}=\\frac{1}{n}\\sum_{i=1}^{n}|x_{i}|.\n$$\nTo verify it is a maximum, compute the second derivative:\n$$\n\\frac{d^{2}\\ell}{db^{2}}=\\frac{n}{b^{2}}-\\frac{2S}{b^{3}}.\n$$\nAt $b=\\hat{b}=S/n$,\n$$\n\\frac{d^{2}\\ell}{db^{2}}\\bigg|_{b=\\hat{b}}=\\frac{n}{(S/n)^{2}}-\\frac{2S}{(S/n)^{3}}=\\frac{n^{3}}{S^{2}}-\\frac{2n^{3}}{S^{2}}=-\\frac{n^{3}}{S^{2}}<0\n$$\nwhenever $S>0$, confirming a maximum. In the degenerate case $S=0$, the likelihood is unbounded as $b\\to 0^{+}$ and the estimator takes the boundary value $0$, which is consistent with $\\hat{b}=(1/n)\\sum_{i=1}^{n}|x_{i}|$.\nTherefore, the MLE is the sample mean of the absolute values.", "answer": "$$\\boxed{\\frac{1}{n}\\sum_{i=1}^{n}|x_{i}|}$$", "id": "1917514"}, {"introduction": "Next, we shift from continuous measurements to discrete counts in a quality control setting, a common task in industrial statistics and operations research. We will estimate the rate of defects in a manufacturing process, modeled by a Poisson distribution, a cornerstone for analyzing count data. This exercise reinforces the MLE procedure and also demonstrates how the estimator for a parameter can be used to estimate other key properties of the distribution, such as its variance. [@problem_id:1917484]", "problem": "A textile manufacturer is implementing a new quality control protocol. The number of defects, such as snags or discoloration, found in a randomly selected one-square-meter area of fabric is modeled by a Poisson distribution with an unknown rate parameter $\\lambda$. This parameter $\\lambda$ represents the average number of defects per square meter.\n\nTo assess a new batch of fabric, a quality control engineer inspects $n=8$ independent one-square-meter patches and records the number of defects for each. The observed counts are:\n$$3, 5, 2, 4, 3, 4, 6, 3$$\n\nAssuming the observations are independent and identically distributed samples from this Poisson distribution, determine the Maximum Likelihood Estimate (MLE) for the variance of the number of defects per square meter for this batch of fabric.", "solution": "Let $X_{1},\\dots,X_{n}$ be independent and identically distributed as $\\operatorname{Poisson}(\\lambda)$, with pmf $P(X=x)=\\frac{\\lambda^{x}\\exp(-\\lambda)}{x!}$ for $x\\in\\{0,1,2,\\dots\\}$. The likelihood for observations $x_{1},\\dots,x_{n}$ is\n$$\nL(\\lambda)=\\prod_{i=1}^{n}\\frac{\\lambda^{x_{i}}\\exp(-\\lambda)}{x_{i}!}\n=\\frac{\\lambda^{\\sum_{i=1}^{n}x_{i}}\\exp(-n\\lambda)}{\\prod_{i=1}^{n}x_{i}!}.\n$$\nThe log-likelihood is\n$$\n\\ell(\\lambda)=\\ln L(\\lambda)=\\left(\\sum_{i=1}^{n}x_{i}\\right)\\ln\\lambda - n\\lambda - \\sum_{i=1}^{n}\\ln(x_{i}!).\n$$\nDifferentiate with respect to $\\lambda$ and set to zero:\n$$\n\\frac{\\partial \\ell}{\\partial \\lambda}=\\frac{\\sum_{i=1}^{n}x_{i}}{\\lambda}-n=0\n\\quad\\Longrightarrow\\quad\n\\hat{\\lambda}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}.\n$$\nThe second derivative is $\\frac{\\partial^{2}\\ell}{\\partial \\lambda^{2}}=-\\frac{\\sum_{i=1}^{n}x_{i}}{\\lambda^{2}}<0$ for $\\lambda>0$, confirming a maximum.\n\nFor a Poisson distribution, the variance equals the parameter: $\\operatorname{Var}(X)=\\lambda$. Therefore, the MLE for the variance is the MLE for $\\lambda$:\n$$\n\\widehat{\\operatorname{Var}}(X)=\\hat{\\lambda}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}.\n$$\nWith the given data $3,5,2,4,3,4,6,3$, we have\n$$\n\\sum_{i=1}^{8}x_{i}=3+5+2+4+3+4+6+3=30,\\quad n=8,\n$$\nso\n$$\n\\widehat{\\operatorname{Var}}(X)=\\hat{\\lambda}=\\frac{30}{8}=\\frac{15}{4}.\n$$", "answer": "$$\\boxed{\\frac{15}{4}}$$", "id": "1917484"}, {"introduction": "Our final practice exercise tackles a classic challenge in ecology: estimating the size of a wildlife population using a capture-recapture study. This scenario is particularly insightful because our unknown parameter, the total population size $N$, must be an integer. Here, the standard method of differentiation is not applicable, requiring us to use a more fundamental approach based on analyzing the ratio of successive likelihood values to find the integer that makes our observation most probable. [@problem_id:1917490]", "problem": "A wildlife biologist is conducting a study to estimate the population size of a specific fish species in an isolated lake. The experiment, known as a capture-recapture study, proceeds in two stages. In the first stage, the biologist captures, tags, and releases $k$ fish. After allowing sufficient time for the tagged fish to mix thoroughly with the general population, the second stage is conducted. In this stage, a sample of $n$ fish is captured from the lake. Upon inspection, it is found that $m$ of these fish are tagged from the first stage.\n\nWe assume the total fish population in the lake, denoted by $N$, remains constant between the two stages (i.e., no births, deaths, immigration, or emigration occurred). We also assume that every fish in the population has an equal chance of being captured in either sample, and the tags do not fall off or affect the fish's a priori probability of being recaptured.\n\nYour task is to find a formula for the value of $N$ that makes the observed outcome (finding $m$ tagged fish in the second sample of size $n$) most probable. This value is known as the maximum likelihood estimate for $N$. Express your answer as a single formula in terms of $k$, $n$, and $m$.", "solution": "The number of tagged “success” items in the second sample is hypergeometric with parameters total population size $N$, number of tagged items $k$, draw size $n$, and observed successes $m$. The likelihood, viewed as a function of $N$ with $k,n,m$ fixed, is\n$$\nL(N)=\\Pr(M=m\\mid N)=\\frac{\\binom{k}{m}\\binom{N-k}{\\,n-m\\,}}{\\binom{N}{n}},\n$$\nwith the feasibility domain $N\\in\\{N_{\\min},N_{\\min}+1,\\ldots\\}$ where $N_{\\min}=k+n-m$ (so that $0\\leq n-m\\leq N-k$). The factor $\\binom{k}{m}$ does not depend on $N$, so maximizing $L(N)$ is equivalent to maximizing\n$$\n\\tilde{L}(N)=\\frac{\\binom{N-k}{\\,n-m\\,}}{\\binom{N}{n}}.\n$$\n\nTo locate the maximizer over integers $N\\geq N_{\\min}$, consider the ratio of successive likelihoods:\n$$\nr(N)=\\frac{\\tilde{L}(N+1)}{\\tilde{L}(N)}\n=\\frac{\\binom{N+1-k}{\\,n-m\\,}}{\\binom{N-k}{\\,n-m\\,}}\\cdot\\frac{\\binom{N}{n}}{\\binom{N+1}{n}}.\n$$\nUsing the identities $\\frac{\\binom{a+1}{b}}{\\binom{a}{b}}=\\frac{a+1}{a+1-b}$ and $\\frac{\\binom{A}{B}}{\\binom{A+1}{B}}=\\frac{A+1-B}{A+1}$, with $a=N-k$, $b=n-m$, $A=N$, $B=n$, we obtain\n$$\nr(N)=\\frac{N+1-k}{N+1-k-(n-m)}\\cdot\\frac{N+1-n}{N+1}\n=\\frac{(N+1-k)(N+1-n)}{(N+1-k-n+m)(N+1)}.\n$$\nTo determine where the sequence $\\tilde{L}(N)$ increases or decreases, solve $r(N)\\geq 1$:\n$$\n\\frac{(N+1-k)(N+1-n)}{(N+1-k-n+m)(N+1)}\\geq 1\n\\;\\Longleftrightarrow\\;\n(N+1-k)(N+1-n)\\geq (N+1-k-n+m)(N+1).\n$$\nLet $X=N+1$. Expanding both sides,\n$$\n(X-k)(X-n)\\geq (X-k-n+m)X\n\\;\\Longleftrightarrow\\;\nX^{2}-X(k+n)+kn \\geq X^{2}-X(k+n-m).\n$$\nSubtracting $X^{2}$ from both sides and collecting terms yields\n$$\n-X(k+n)+kn \\geq -X(k+n-m)\n\\;\\Longleftrightarrow\\;\nkn - mX \\geq 0\n\\;\\Longleftrightarrow\\;\nX \\leq \\frac{kn}{m}.\n$$\nThus, for $m>0$, we have $r(N)\\geq 1$ exactly when $N+1\\leq \\frac{kn}{m}$. Therefore $\\tilde{L}(N)$ is nondecreasing as $N$ increases up to the largest $N$ with $N+1\\leq \\frac{kn}{m}$, and then nonincreasing thereafter. Consequently, any maximizer $N^{\\ast}$ must satisfy\n$$\nr(N^{\\ast}-1)\\geq 1\n\\quad\\text{and}\\quad\nr(N^{\\ast})\\leq 1,\n$$\nwhich is equivalent to\n$$\nN^{\\ast} \\leq \\frac{kn}{m}\n\\quad\\text{and}\\quad\nN^{\\ast}+1 \\geq \\frac{kn}{m}.\n$$\nHence, when $\\frac{kn}{m}$ is not an integer, the unique maximizing integer is\n$$\nN^{\\ast}=\\left\\lfloor \\frac{kn}{m}\\right\\rfloor.\n$$\nWhen $\\frac{kn}{m}$ is an integer, both $N^{\\ast}=\\frac{kn}{m}-1$ and $N^{\\ast}=\\frac{kn}{m}$ achieve the same maximal likelihood; choosing $N^{\\ast}=\\left\\lfloor \\frac{kn}{m}\\right\\rfloor$ selects one valid maximizer. The feasibility constraint is automatically respected, since\n$$\n\\frac{kn}{m}\\geq k+n-m\n\\quad\\text{because}\\quad\nkn - m(k+n-m)=(k-m)(n-m)\\geq 0,\n$$\nso $\\left\\lfloor \\frac{kn}{m}\\right\\rfloor \\geq k+n-m$.\n\nIf $m=0$, then $r(N)\\geq 1$ for all feasible $N$, so $\\tilde{L}(N)$ is nondecreasing without bound and there is no finite maximizer (the likelihood increases with $N$). For $m>0$, the maximum likelihood estimate is therefore given by the single formula below.", "answer": "$$\\boxed{\\left\\lfloor \\frac{k n}{m} \\right\\rfloor}$$", "id": "1917490"}]}