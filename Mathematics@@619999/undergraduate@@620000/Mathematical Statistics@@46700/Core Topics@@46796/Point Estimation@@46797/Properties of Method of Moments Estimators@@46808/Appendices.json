{"hands_on_practices": [{"introduction": "The method of moments provides a straightforward recipe for constructing estimators: equate sample moments to their theoretical counterparts and solve for the unknown parameters. This exercise provides fundamental practice in applying this method to a single-parameter distribution. Beyond just finding the estimator, you will also investigate its bias, a key property that tells us whether the estimator, on average, hits the true parameter value. [@problem_id:1948412]", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n$ from a distribution with a probability density function (PDF) given by\n$$\nf(x; \\theta) = \\begin{cases} \\exp(-(x-\\theta)) & \\text{if } x > \\theta \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nwhere $\\theta$ is an unknown real parameter. Let $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ denote the sample mean.\n\nYour task is to find the Method of Moments Estimator (MOME) for the parameter $\\theta$ and to determine if this estimator is unbiased.\n\nWhich of the following statements is correct?\n\nA. The MOME for $\\theta$ is $\\hat{\\theta} = \\bar{X} - 1$, and it is an unbiased estimator.\n\nB. The MOME for $\\theta$ is $\\hat{\\theta} = \\bar{X} - 1$, and it is a biased estimator.\n\nC. The MOME for $\\theta$ is $\\hat{\\theta} = \\bar{X}$, and it is an unbiased estimator.\n\nD. The MOME for $\\theta$ is $\\hat{\\theta} = \\bar{X}$, and it is a biased estimator.\n\nE. The MOME for $\\theta$ is $\\hat{\\theta} = \\bar{X} + 1$, and it is a biased estimator.", "solution": "The given density is $f(x;\\theta)=\\exp(-(x-\\theta))$ for $x>\\theta$ and $0$ otherwise. This is a location-shifted exponential distribution with rate $1$, so $Y=X-\\theta$ follows $\\operatorname{Exp}(1)$.\n\nCompute the population mean:\n$$\n\\mathbb{E}[X]=\\int_{\\theta}^{\\infty} x\\,\\exp\\bigl(-(x-\\theta)\\bigr)\\,dx.\n$$\nUse the substitution $y=x-\\theta$, so $x=y+\\theta$ and $dx=dy$, with limits $y\\in[0,\\infty)$:\n$$\n\\mathbb{E}[X]=\\int_{0}^{\\infty} (y+\\theta)\\,\\exp(-y)\\,dy=\\int_{0}^{\\infty} y\\,\\exp(-y)\\,dy+\\theta\\int_{0}^{\\infty}\\exp(-y)\\,dy=1+\\theta.\n$$\n\nBy the method of moments, equate the sample mean to the population mean:\n$$\n\\bar{X}=\\mathbb{E}[X]=\\theta+1 \\quad \\Rightarrow \\quad \\hat{\\theta}_{\\mathrm{MOM}}=\\bar{X}-1.\n$$\n\nTo assess unbiasedness, use linearity of expectation and $\\mathbb{E}[\\bar{X}]=\\mathbb{E}[X]=\\theta+1$:\n$$\n\\mathbb{E}[\\hat{\\theta}_{\\mathrm{MOM}}]=\\mathbb{E}[\\bar{X}-1]=\\mathbb{E}[\\bar{X}]-1=(\\theta+1)-1=\\theta.\n$$\nThus, $\\hat{\\theta}_{\\mathrm{MOM}}=\\bar{X}-1$ is unbiased.\n\nTherefore, the correct statement is that the MOME is $\\hat{\\theta}=\\bar{X}-1$ and it is an unbiased estimator.", "answer": "$$\\boxed{A}$$", "id": "1948412"}, {"introduction": "An estimator can be unbiased, yet still be a poor choice if its estimates vary wildly from one sample to the next; this brings us to the concept of efficiency, which we measure using variance. This hands-on practice challenges you to derive the Method of Moments Estimator (MOME) for a parameter and then compare its variance against another unbiased estimator. This comparison illustrates that not all estimators are created equal and provides a concrete example of why statisticians seek to minimize variance. [@problem_id:1948421]", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n > 1$ drawn from a Uniform distribution on the interval $(0, \\theta)$, where $\\theta > 0$ is an unknown parameter. Two different unbiased estimators for $\\theta$ are to be compared for their efficiency.\n\nThe first estimator, $\\tilde{\\theta}_1$, is the unbiased estimator for $\\theta$ derived using the Method of Moments Estimator (MOME).\n\nThe second estimator, $\\tilde{\\theta}_2$, is the unbiased estimator for $\\theta$ obtained by multiplying the sample maximum, $X_{(n)} = \\max(X_1, X_2, \\dots, X_n)$, by an appropriate constant.\n\nDetermine the ratio of the variance of the first estimator to the variance of the second estimator, $\\frac{\\text{Var}(\\tilde{\\theta}_1)}{\\text{Var}(\\tilde{\\theta}_2)}$. Express your final answer as a function of the sample size $n$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. from $U(0,\\theta)$ with $\\theta>0$. For this distribution,\n$$\n\\mathbb{E}[X_{i}]=\\frac{\\theta}{2},\\qquad \\operatorname{Var}(X_{i})=\\frac{\\theta^{2}}{12}.\n$$\nThe Method of Moments estimator equates $\\bar{X}$ to $\\mathbb{E}[X_{i}]$, giving $\\tilde{\\theta}_{1}=2\\bar{X}$. Its unbiasedness follows from $\\mathbb{E}[\\bar{X}]=\\theta/2$, so $\\mathbb{E}[\\tilde{\\theta}_{1}]=\\theta$. Its variance is\n$$\n\\operatorname{Var}(\\tilde{\\theta}_{1})=\\operatorname{Var}(2\\bar{X})=4\\operatorname{Var}(\\bar{X})=4\\frac{\\operatorname{Var}(X_{i})}{n}=4\\frac{\\theta^{2}}{12n}=\\frac{\\theta^{2}}{3n}.\n$$\n\nLet $X_{(n)}=\\max(X_{1},\\dots,X_{n})$. Its cumulative distribution function is $F_{X_{(n)}}(x)=(x/\\theta)^{n}$ for $0<x<\\theta$, and density $f_{X_{(n)}}(x)=n x^{n-1}/\\theta^{n}$. Then\n$$\n\\mathbb{E}[X_{(n)}]=\\int_{0}^{\\theta} x \\frac{n x^{n-1}}{\\theta^{n}}\\,dx=\\frac{n}{\\theta^{n}}\\cdot\\frac{\\theta^{n+1}}{n+1}=\\frac{n\\theta}{n+1}.\n$$\nTherefore the unbiased multiple of $X_{(n)}$ is $\\tilde{\\theta}_{2}=cX_{(n)}$ with $c=(n+1)/n$. Next compute\n$$\n\\mathbb{E}[X_{(n)}^{2}]=\\int_{0}^{\\theta} x^{2} \\frac{n x^{n-1}}{\\theta^{n}}\\,dx=\\frac{n}{\\theta^{n}}\\cdot\\frac{\\theta^{n+2}}{n+2}=\\frac{n\\theta^{2}}{n+2},\n$$\nso\n$$\n\\operatorname{Var}(X_{(n)})=\\mathbb{E}[X_{(n)}^{2}]-\\mathbb{E}[X_{(n)}]^{2}\n=\\theta^{2}\\left(\\frac{n}{n+2}-\\frac{n^{2}}{(n+1)^{2}}\\right)\n=\\theta^{2}\\frac{n}{(n+1)^{2}(n+2)}.\n$$\nThus\n$$\n\\operatorname{Var}(\\tilde{\\theta}_{2})=c^{2}\\operatorname{Var}(X_{(n)})=\\frac{(n+1)^{2}}{n^{2}}\\cdot \\theta^{2}\\frac{n}{(n+1)^{2}(n+2)}=\\frac{\\theta^{2}}{n(n+2)}.\n$$\n\nThe desired ratio is\n$$\n\\frac{\\operatorname{Var}(\\tilde{\\theta}_{1})}{\\operatorname{Var}(\\tilde{\\theta}_{2})}\n=\\frac{\\theta^{2}/(3n)}{\\theta^{2}/(n(n+2))}=\\frac{n+2}{3}.\n$$", "answer": "$$\\boxed{\\frac{n+2}{3}}$$", "id": "1948421"}, {"introduction": "Real-world statistical problems often involve comparing groups and handling multiple unknown parameters simultaneously. This advanced practice extends the method of moments to a two-sample scenario where you will estimate two means, $\\mu_1$ and $\\mu_2$, and a common variance, $\\sigma^2$. The ultimate goal is to construct an estimator for the standardized difference $(\\mu_1 - \\mu_2)/\\sigma$, showcasing how MOME can deliver estimators for quantities of direct scientific interest. [@problem_id:1948462]", "problem": "Consider two independent random samples. The first sample, $X_1, X_2, \\ldots, X_{n_1}$, is drawn from a normal distribution $N(\\mu_1, \\sigma^2)$. The second sample, $Y_1, Y_2, \\ldots, Y_{n_2}$, is drawn from another normal distribution $N(\\mu_2, \\sigma^2)$, which has a different mean but the same variance as the first. The parameters $\\mu_1, \\mu_2$, and $\\sigma > 0$ are all unknown.\n\nIn statistical analysis, particularly in meta-analysis and the social sciences, the quantity $\\delta = \\frac{\\mu_1 - \\mu_2}{\\sigma}$, often referred to as Cohen's d, serves as a standardized measure of the difference between two means, or \"effect size.\"\n\nYour task is to find the Method of Moments Estimator (MOME) for $\\delta$.\n\nExpress your final answer as a single closed-form analytic expression in terms of the sample sizes $n_1, n_2$, the sample means $\\bar{X} = \\frac{1}{n_1}\\sum_{i=1}^{n_1} X_i$ and $\\bar{Y} = \\frac{1}{n_2}\\sum_{j=1}^{n_2} Y_j$, and the individual sample observations $X_i$ and $Y_j$.", "solution": "We have independent samples $X_{1},\\ldots,X_{n_{1}} \\sim N(\\mu_{1},\\sigma^{2})$ and $Y_{1},\\ldots,Y_{n_{2}} \\sim N(\\mu_{2},\\sigma^{2})$ with unknown $\\mu_{1},\\mu_{2}$ and $\\sigma>0$. Define the effect size $\\delta=(\\mu_{1}-\\mu_{2})/\\sigma$. The method of moments sets sample moments equal to their population counterparts and solves for the parameters.\n\nFirst moments:\nFor the $X$-sample, $E[X]=\\mu_{1}$. The sample analog is $\\bar{X}=\\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}}X_{i}$. For the $Y$-sample, $E[Y]=\\mu_{2}$, with sample analog $\\bar{Y}=\\frac{1}{n_{2}}\\sum_{j=1}^{n_{2}}Y_{j}$. Equating moments gives the MOM estimators\n$$\n\\hat{\\mu}_{1,\\text{MOM}}=\\bar{X},\\qquad \\hat{\\mu}_{2,\\text{MOM}}=\\bar{Y}.\n$$\n\nSecond moments:\nUse the pooled second raw moment to obtain a single equation for $\\sigma^{2}$. The pooled population second raw moment equals\n$$\n\\frac{n_{1}E[X^{2}]+n_{2}E[Y^{2}]}{n_{1}+n_{2}}=\\frac{n_{1}(\\mu_{1}^{2}+\\sigma^{2})+n_{2}(\\mu_{2}^{2}+\\sigma^{2})}{n_{1}+n_{2}}=\\frac{n_{1}\\mu_{1}^{2}+n_{2}\\mu_{2}^{2}}{n_{1}+n_{2}}+\\sigma^{2}.\n$$\nIts sample analog is\n$$\n\\frac{1}{n_{1}+n_{2}}\\left(\\sum_{i=1}^{n_{1}}X_{i}^{2}+\\sum_{j=1}^{n_{2}}Y_{j}^{2}\\right).\n$$\nEquating these and solving for $\\sigma^{2}$ yields\n$$\n\\hat{\\sigma}^{2}_{\\text{MOM}}=\\frac{1}{n_{1}+n_{2}}\\left(\\sum_{i=1}^{n_{1}}X_{i}^{2}+\\sum_{j=1}^{n_{2}}Y_{j}^{2}-n_{1}\\hat{\\mu}_{1,\\text{MOM}}^{2}-n_{2}\\hat{\\mu}_{2,\\text{MOM}}^{2}\\right).\n$$\nSubstituting $\\hat{\\mu}_{1,\\text{MOM}}=\\bar{X}$ and $\\hat{\\mu}_{2,\\text{MOM}}=\\bar{Y}$ and using $\\sum_{i=1}^{n_{1}}(X_{i}-\\bar{X})^{2}=\\sum_{i=1}^{n_{1}}X_{i}^{2}-n_{1}\\bar{X}^{2}$ and $\\sum_{j=1}^{n_{2}}(Y_{j}-\\bar{Y})^{2}=\\sum_{j=1}^{n_{2}}Y_{j}^{2}-n_{2}\\bar{Y}^{2}$, we obtain the pooled MOM variance\n$$\n\\hat{\\sigma}^{2}_{\\text{MOM}}=\\frac{\\sum_{i=1}^{n_{1}}(X_{i}-\\bar{X})^{2}+\\sum_{j=1}^{n_{2}}(Y_{j}-\\bar{Y})^{2}}{n_{1}+n_{2}}.\n$$\n\nFinally, the MOM estimator of $\\delta$ is the plug-in estimator using $\\hat{\\mu}_{1,\\text{MOM}},\\hat{\\mu}_{2,\\text{MOM}}$ and $\\hat{\\sigma}_{\\text{MOM}}=\\sqrt{\\hat{\\sigma}^{2}_{\\text{MOM}}}$:\n$$\n\\hat{\\delta}_{\\text{MOM}}=\\frac{\\bar{X}-\\bar{Y}}{\\sqrt{\\frac{1}{n_{1}+n_{2}}\\left(\\sum_{i=1}^{n_{1}}(X_{i}-\\bar{X})^{2}+\\sum_{j=1}^{n_{2}}(Y_{j}-\\bar{Y})^{2}\\right)}}.\n$$\nThis is a single closed-form analytic expression in terms of $n_{1},n_{2},\\bar{X},\\bar{Y}$, and the sample observations $X_{i},Y_{j}$.", "answer": "$$\\boxed{\\frac{\\bar{X}-\\bar{Y}}{\\sqrt{\\frac{1}{n_{1}+n_{2}}\\left(\\sum_{i=1}^{n_{1}}(X_{i}-\\bar{X})^{2}+\\sum_{j=1}^{n_{2}}(Y_{j}-\\bar{Y})^{2}\\right)}}}$$", "id": "1948462"}]}