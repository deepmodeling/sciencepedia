{"hands_on_practices": [{"introduction": "In statistics, we often default to using the sample mean ($\\bar{X}$) to estimate the center of a distribution. However, is it always the most efficient choice? This exercise challenges that assumption by exploring a scenario where the underlying data follows a uniform distribution. You will compare the performance of an estimator based on the sample mean to one based on the sample maximum, demonstrating that the optimal choice of an estimator is fundamentally tied to the nature of the data's distribution [@problem_id:1951462]. This practice is key to developing intuition for when to look beyond common statistical tools.", "problem": "A manufacturer is testing a new type of solid-state battery. The lifetime of a battery, denoted by the random variable $X$, is assumed to follow a uniform distribution on the interval $[0, \\theta]$, where $\\theta$ is the unknown maximum possible lifetime. To estimate $\\theta$, a random sample of $n$ batteries, $X_1, X_2, \\dots, X_n$, is tested until failure.\n\nTwo different estimators for $\\theta$ are proposed:\n1. The first estimator, $T_1$, is defined as twice the sample mean: $T_1 = 2\\bar{X}$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n2. The second estimator, $T_2$, is constructed by scaling the sample maximum, $X_{(n)} = \\max(X_1, X_2, \\dots, X_n)$, in such a way that $T_2$ is an unbiased estimator for $\\theta$.\n\nCalculate the relative efficiency of $T_1$ with respect to $T_2$. Express your answer as a function of the sample size $n$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. from the $\\mathrm{Uniform}(0,\\theta)$ distribution. Then the mean and variance of a single observation are\n$$\n\\mathbb{E}[X_{i}]=\\frac{\\theta}{2},\\qquad \\mathrm{Var}(X_{i})=\\frac{\\theta^{2}}{12}.\n$$\nFor the sample mean $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$, by linearity of expectation and independence,\n$$\n\\mathbb{E}[\\bar{X}]=\\frac{\\theta}{2},\\qquad \\mathrm{Var}(\\bar{X})=\\frac{1}{n}\\mathrm{Var}(X_{i})=\\frac{\\theta^{2}}{12n}.\n$$\nThe first estimator is $T_{1}=2\\bar{X}$, so\n$$\n\\mathbb{E}[T_{1}]=2\\,\\mathbb{E}[\\bar{X}]=\\theta,\\qquad \\mathrm{Var}(T_{1})=4\\,\\mathrm{Var}(\\bar{X})=\\frac{\\theta^{2}}{3n}.\n$$\n\nLet $X_{(n)}=\\max(X_{1},\\dots,X_{n})$ denote the sample maximum. For $X_{i}\\sim \\mathrm{Uniform}(0,\\theta)$, the scaled maximum $X_{(n)}/\\theta$ has the $\\mathrm{Beta}(n,1)$ distribution. Hence\n$$\n\\mathbb{E}[X_{(n)}]=\\theta\\,\\frac{n}{n+1},\\qquad \\mathrm{Var}\\!\\left(X_{(n)}\\right)=\\theta^{2}\\,\\frac{n}{(n+1)^{2}(n+2)}.\n$$\nTo make $T_{2}$ unbiased, define $T_{2}=c\\,X_{(n)}$ with $c$ chosen so that $\\mathbb{E}[T_{2}]=\\theta$. Using $\\mathbb{E}[X_{(n)}]=\\theta\\,\\frac{n}{n+1}$,\n$$\nc\\,\\theta\\,\\frac{n}{n+1}=\\theta\\;\\;\\Rightarrow\\;\\; c=\\frac{n+1}{n},\n$$\nso\n$$\nT_{2}=\\frac{n+1}{n}\\,X_{(n)},\\qquad \\mathrm{Var}(T_{2})=\\left(\\frac{n+1}{n}\\right)^{2}\\mathrm{Var}\\!\\left(X_{(n)}\\right)=\\left(\\frac{n+1}{n}\\right)^{2}\\theta^{2}\\frac{n}{(n+1)^{2}(n+2)}=\\frac{\\theta^{2}}{n(n+2)}.\n$$\n\nFor two unbiased estimators, the relative efficiency of $T_{1}$ with respect to $T_{2}$ is defined as\n$$\n\\mathrm{RE}(T_{1}\\text{ w.r.t. }T_{2})=\\frac{\\mathrm{Var}(T_{2})}{\\mathrm{Var}(T_{1})}.\n$$\nSubstituting the variances derived above gives\n$$\n\\mathrm{RE}(T_{1}\\text{ w.r.t. }T_{2})=\\frac{\\theta^{2}/\\bigl(n(n+2)\\bigr)}{\\theta^{2}/(3n)}=\\frac{3}{n+2}.\n$$\nThis expresses the relative efficiency as a function of $n$.", "answer": "$$\\boxed{\\frac{3}{n+2}}$$", "id": "1951462"}, {"introduction": "Beyond simply comparing existing estimators, a more powerful task is to construct a new, superior estimator from the information we already have. Imagine you have two different instruments measuring the same quantity; how do you best combine their readings? This practice explores how to create an optimal linear estimator, $T_C = w T_1 + (1-w) T_2$, by finding the weight $w$ that minimizes its variance. You will see how to leverage not just the variances of the individual estimators but also their correlation to build a combined estimator that is more efficient than either one alone [@problem_id:1951437].", "problem": "A materials scientist is developing a new class of semiconductor wafers and needs to precisely estimate the average impurity concentration, $\\theta$. Two different measurement systems are available. System A produces an unbiased estimate $T_1$ of $\\theta$, and System B produces another unbiased estimate $T_2$ of $\\theta$. From historical data, the variances of these estimators are known to be $\\text{Var}(T_1) = \\sigma_1^2$ and $\\text{Var}(T_2) = \\sigma_2^2$. Due to shared environmental factors affecting both systems, the estimates are not independent, and their correlation coefficient is $\\text{Corr}(T_1, T_2) = \\rho$, where $|\\rho| < 1$.\n\nTo improve the estimation, a combined linear estimator is proposed: $T_C = w T_1 + (1-w) T_2$, where $w$ is a real-valued weight.\n\nYour task is to find the optimal configuration for this combined estimator. Determine the expression for the optimal weight, $w_{opt}$, that minimizes the variance of $T_C$. Then, find the expression for the relative efficiency of this optimal combined estimator, $T_{C,opt}$, with respect to the estimator $T_1$. The relative efficiency is defined as the ratio $\\text{Eff}(T_{C,opt}, T_1) = \\frac{\\text{Var}(T_1)}{\\text{Var}(T_{C,opt})}$.\n\nPresent your final answer as a row matrix containing two elements: the expression for $w_{opt}$ as the first element and the expression for the relative efficiency as the second element.", "solution": "The combined estimator is $T_{C}=w T_{1}+(1-w) T_{2}$. Since $T_{1}$ and $T_{2}$ are unbiased for $\\theta$, and the weights sum to one, $T_{C}$ is unbiased: $\\mathbb{E}[T_{C}]=w \\theta+(1-w)\\theta=\\theta$.\n\nThe variance of $T_{C}$ uses $\\text{Cov}(T_{1},T_{2})=\\rho \\sigma_{1}\\sigma_{2}$:\n$$\n\\text{Var}(T_{C})=\\text{Var}\\big(w T_{1}+(1-w) T_{2}\\big)\n=w^{2}\\sigma_{1}^{2}+(1-w)^{2}\\sigma_{2}^{2}+2 w(1-w)\\rho \\sigma_{1}\\sigma_{2}.\n$$\nDefine $V(w)=w^{2}\\sigma_{1}^{2}+(1-w)^{2}\\sigma_{2}^{2}+2 w(1-w)\\rho \\sigma_{1}\\sigma_{2}$. Differentiate and set to zero:\n$$\n\\frac{dV}{dw}=2 w \\sigma_{1}^{2}-2(1-w)\\sigma_{2}^{2}+2(1-2w)\\rho \\sigma_{1}\\sigma_{2}=0.\n$$\nThis simplifies to\n$$\nw\\big(\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}\\big)+\\big(\\rho \\sigma_{1}\\sigma_{2}-\\sigma_{2}^{2}\\big)=0,\n$$\nso the optimal weight is\n$$\nw_{\\text{opt}}=\\frac{\\sigma_{2}^{2}-\\rho \\sigma_{1}\\sigma_{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}}.\n$$\nThe second derivative is\n$$\n\\frac{d^{2}V}{dw^{2}}=2\\big(\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}\\big)=2\\,\\text{Var}(T_{1}-T_{2})>0,\n$$\nsince $|\\rho|<1$, ensuring a minimum.\n\nTo find the minimized variance, write $V(w)=a w^{2}+2 b w+c$ with\n$$\na=\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2},\\quad b=-(\\sigma_{2}^{2}-\\rho \\sigma_{1}\\sigma_{2}),\\quad c=\\sigma_{2}^{2}.\n$$\nThen\n$$\n\\text{Var}(T_{C,\\text{opt}})=V(w_{\\text{opt}})=c-\\frac{b^{2}}{a}\n=\\frac{\\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^{2})}{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}}.\n$$\nThe relative efficiency of $T_{C,\\text{opt}}$ with respect to $T_{1}$ is\n$$\n\\text{Eff}(T_{C,\\text{opt}},T_{1})=\\frac{\\text{Var}(T_{1})}{\\text{Var}(T_{C,\\text{opt}})}\n=\\frac{\\sigma_{1}^{2}}{\\dfrac{\\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^{2})}{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}}}\n=\\frac{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}}{\\sigma_{2}^{2}(1-\\rho^{2})}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\dfrac{\\sigma_{2}^{2}-\\rho \\sigma_{1}\\sigma_{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}} & \\dfrac{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}}{\\sigma_{2}^{2}(1-\\rho^{2})}\\end{pmatrix}}$$", "id": "1951437"}, {"introduction": "In many real-world problems, we must estimate a parameter of interest while other \"nuisance\" parameters of the model are also unknown. This uncertainty has a cost, reducing the precision of our estimates. This problem delves into the realm of asymptotic efficiency to quantify this cost precisely. Using the framework of the Fisher Information Matrix for a Gumbel distribution, you will calculate the loss of efficiency in estimating the location parameter $\\mu$ that occurs when the scale parameter $\\sigma$ is not known. This exercise [@problem_id:1951476] provides a deep insight into the theoretical limits of estimation and the practical consequences of incomplete information.", "problem": "In statistical modeling, the efficiency of an estimator can be significantly affected by the presence of nuisance parameters. Consider a random variable $Y$ that follows a Gumbel distribution, which is an asymmetric location-scale family of distributions. The probability density function is parameterized by a location parameter $\\mu$ and a positive scale parameter $\\sigma$.\n\nFor a single observation $Y$, the Fisher Information Matrix (FIM) for the parameter vector $\\boldsymbol{\\theta} = (\\mu, \\sigma)^T$ is given by\n$$\nI(\\mu, \\sigma) = \\frac{1}{\\sigma^2}\n\\begin{pmatrix}\n1 & 1-\\gamma \\\\\n1-\\gamma & \\frac{\\pi^2}{6} + (1-\\gamma)^2\n\\end{pmatrix}\n$$\nwhere $\\gamma$ is the Euler-Mascheroni constant ($\\gamma \\approx 0.5772$). The off-diagonal terms are non-zero due to the asymmetry of the Gumbel distribution.\n\nWe are interested in estimating the location parameter $\\mu$. Compare the following two scenarios based on a large sample of $n$ independent and identically distributed observations $Y_1, \\dots, Y_n$:\n1.  **Known Scale:** The scale parameter $\\sigma$ is known to be some value $\\sigma_0$, and we only need to estimate $\\mu$.\n2.  **Unknown Scale:** Both $\\mu$ and $\\sigma$ are unknown and must be estimated simultaneously.\n\nThe performance of an estimator is measured by its asymptotic variance. The Asymptotic Relative Efficiency (ARE) of an estimator $\\hat{\\theta}_A$ with respect to another estimator $\\hat{\\theta}_B$ is defined as the ratio of their asymptotic variances, $\\text{ARE}(\\hat{\\theta}_A, \\hat{\\theta}_B) = \\frac{\\text{Asymptotic Var}(\\hat{\\theta}_B)}{\\text{Asymptotic Var}(\\hat{\\theta}_A)}$.\n\nCalculate the ARE of the Maximum Likelihood Estimator (MLE) for $\\mu$ in the \"Unknown Scale\" scenario with respect to the MLE for $\\mu$ in the \"Known Scale\" scenario. Express your answer as a closed-form analytic expression in terms of $\\pi$ and $\\gamma$.", "solution": "For a single observation, the Fisher Information Matrix (FIM) for $\\boldsymbol{\\theta} = (\\mu,\\sigma)^{T}$ is\n$$\nI_{1}(\\mu,\\sigma) = \\frac{1}{\\sigma^{2}}\n\\begin{pmatrix}\n1 & 1-\\gamma \\\\\n1-\\gamma & \\frac{\\pi^{2}}{6} + (1-\\gamma)^{2}\n\\end{pmatrix},\n$$\nand for $n$ i.i.d. observations it is $I_{n}(\\mu,\\sigma) = n I_{1}(\\mu,\\sigma)$.\n\nAsymptotic normality of the MLE implies that, for large $n$, the asymptotic covariance matrix of the joint MLE is $(I_{n})^{-1} = \\frac{1}{n} I_{1}^{-1}$, so the asymptotic variance of $\\hat{\\mu}$ equals the $(1,1)$ entry of $\\frac{1}{n} I_{1}^{-1}$.\n\nKnown scale $\\sigma$:\nWhen $\\sigma$ is known, the relevant Fisher information for $\\mu$ is the scalar $I_{1,\\mu\\mu} = \\sigma^{-2}$. Therefore, for $n$ observations, the asymptotic variance of the MLE $\\hat{\\mu}$ is\n$$\n\\operatorname{Avar}_{\\text{known}}(\\hat{\\mu}) = \\frac{1}{n I_{1,\\mu\\mu}} = \\frac{\\sigma^{2}}{n}.\n$$\n\nUnknown scale $\\sigma$:\nWhen both $\\mu$ and $\\sigma$ are unknown, write the per-observation FIM as\n$$\nI_{1} = \\frac{1}{\\sigma^{2}} M,\\quad M =\n\\begin{pmatrix}\n1 & a \\\\\na & b\n\\end{pmatrix},\\quad a = 1-\\gamma,\\quad b = \\frac{\\pi^{2}}{6} + (1-\\gamma)^{2}.\n$$\nThen $I_{1}^{-1} = \\sigma^{2} M^{-1}$. For a $2\\times 2$ matrix, $M^{-1} = \\frac{1}{\\det(M)}\n\\begin{pmatrix}\nb & -a \\\\\n-a & 1\n\\end{pmatrix}$, so the $(1,1)$ entry is $(M^{-1})_{11} = b/\\det(M)$. The determinant is\n$$\n\\det(M) = 1\\cdot b - a^{2} = \\left(\\frac{\\pi^{2}}{6} + (1-\\gamma)^{2}\\right) - (1-\\gamma)^{2} = \\frac{\\pi^{2}}{6}.\n$$\nTherefore,\n$$\n(I_{1}^{-1})_{11} = \\sigma^{2} \\frac{b}{\\det(M)} = \\sigma^{2} \\frac{\\frac{\\pi^{2}}{6} + (1-\\gamma)^{2}}{\\frac{\\pi^{2}}{6}}\n= \\sigma^{2}\\left(1 + \\frac{6}{\\pi^{2}}(1-\\gamma)^{2}\\right).\n$$\nThus, for $n$ observations,\n$$\n\\operatorname{Avar}_{\\text{unknown}}(\\hat{\\mu}) = \\frac{1}{n} (I_{1}^{-1})_{11}\n= \\frac{\\sigma^{2}}{n}\\left(1 + \\frac{6}{\\pi^{2}}(1-\\gamma)^{2}\\right).\n$$\n\nAsymptotic Relative Efficiency:\nBy definition, for estimator $A$ (unknown scale) relative to estimator $B$ (known scale),\n$$\n\\text{ARE}(\\hat{\\mu}_{\\text{unknown}}, \\hat{\\mu}_{\\text{known}}) = \\frac{\\operatorname{Avar}_{\\text{known}}(\\hat{\\mu})}{\\operatorname{Avar}_{\\text{unknown}}(\\hat{\\mu})}\n= \\frac{\\sigma^{2}/n}{\\frac{\\sigma^{2}}{n}\\left(1 + \\frac{6}{\\pi^{2}}(1-\\gamma)^{2}\\right)}\n= \\frac{1}{1 + \\frac{6}{\\pi^{2}}(1-\\gamma)^{2}}\n= \\frac{\\pi^{2}}{\\pi^{2} + 6(1-\\gamma)^{2}}.\n$$\nThis gives the closed-form expression in terms of $\\pi$ and $\\gamma$.", "answer": "$$\\boxed{\\frac{\\pi^{2}}{\\pi^{2} + 6\\left(1-\\gamma\\right)^{2}}}$$", "id": "1951476"}]}