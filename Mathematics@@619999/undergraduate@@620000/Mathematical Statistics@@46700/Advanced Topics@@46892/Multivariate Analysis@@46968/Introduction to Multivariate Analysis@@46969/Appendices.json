{"hands_on_practices": [{"introduction": "The cornerstone of multivariate analysis is understanding the relationships between variables, which is captured by the sample covariance matrix. This matrix provides a compact summary of the variance within each variable and the covariance between each pair of variables. This first exercise [@problem_id:1924314] offers fundamental practice in computing this matrix directly from a small, hypothetical dataset, translating abstract observations into a tangible mathematical object ready for analysis.", "problem": "An agricultural scientist is studying a new species of plant. They collect data on the height (in centimeters) and the number of leaves for a small sample of three plants. The measurements are recorded as pairs (height, number of leaves). The three data points, treated as 2-dimensional column vectors, are:\n$$ \\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}, \\quad \\mathbf{x}_2 = \\begin{pmatrix} 2 \\\\ 7 \\end{pmatrix}, \\quad \\mathbf{x}_3 = \\begin{pmatrix} 6 \\\\ 1 \\end{pmatrix} $$\nAssuming these three data points represent a random sample from a larger population, calculate the unbiased sample covariance matrix, $S$, for this bivariate dataset. Express your answer as a 2x2 matrix.", "solution": "To compute the unbiased sample covariance matrix for a bivariate sample, use the definition\n$$S=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(\\mathbf{x}_{i}-\\bar{\\mathbf{x}}\\right)\\left(\\mathbf{x}_{i}-\\bar{\\mathbf{x}}\\right)^{T},$$\nwhere $n$ is the sample size and $\\bar{\\mathbf{x}}$ is the sample mean vector given by\n$$\\bar{\\mathbf{x}}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_{i}.$$\nWith $n=3$ and data $\\mathbf{x}_{1}=\\begin{pmatrix}1\\\\4\\end{pmatrix}$, $\\mathbf{x}_{2}=\\begin{pmatrix}2\\\\7\\end{pmatrix}$, $\\mathbf{x}_{3}=\\begin{pmatrix}6\\\\1\\end{pmatrix}$, the mean vector is\n$$\\bar{\\mathbf{x}}=\\frac{1}{3}\\begin{pmatrix}1+2+6\\\\4+7+1\\end{pmatrix}=\\begin{pmatrix}3\\\\4\\end{pmatrix}.$$\nCompute deviations:\n$$\\mathbf{d}_{1}=\\mathbf{x}_{1}-\\bar{\\mathbf{x}}=\\begin{pmatrix}-2\\\\0\\end{pmatrix},\\quad \\mathbf{d}_{2}=\\mathbf{x}_{2}-\\bar{\\mathbf{x}}=\\begin{pmatrix}-1\\\\3\\end{pmatrix},\\quad \\mathbf{d}_{3}=\\mathbf{x}_{3}-\\bar{\\mathbf{x}}=\\begin{pmatrix}3\\\\-3\\end{pmatrix}.$$\nCompute outer products:\n$$\\mathbf{d}_{1}\\mathbf{d}_{1}^{T}=\\begin{pmatrix}4&0\\\\0&0\\end{pmatrix},\\quad \\mathbf{d}_{2}\\mathbf{d}_{2}^{T}=\\begin{pmatrix}1&-3\\\\-3&9\\end{pmatrix},\\quad \\mathbf{d}_{3}\\mathbf{d}_{3}^{T}=\\begin{pmatrix}9&-9\\\\-9&9\\end{pmatrix}.$$\nSum them:\n$$\\sum_{i=1}^{3}\\mathbf{d}_{i}\\mathbf{d}_{i}^{T}=\\begin{pmatrix}4+1+9&0-3-9\\\\0-3-9&0+9+9\\end{pmatrix}=\\begin{pmatrix}14&-12\\\\-12&18\\end{pmatrix}.$$\nDivide by $n-1=2$ to obtain the unbiased sample covariance matrix:\n$$S=\\frac{1}{2}\\begin{pmatrix}14&-12\\\\-12&18\\end{pmatrix}=\\begin{pmatrix}7&-6\\\\-6&9\\end{pmatrix}.$$", "answer": "$$\\boxed{\\begin{pmatrix}7 & -6 \\\\ -6 & 9\\end{pmatrix}}$$", "id": "1924314"}, {"introduction": "A deep understanding of the covariance matrix comes from interpreting its properties, not just calculating it. This exercise [@problem_id:1924300] explores a special, hypothetical scenario where the determinant of the sample covariance matrix is zero. By investigating the implications of this result, you will uncover the crucial link between the matrix's algebraic properties and the geometric structure of the data, providing key insights into concepts like multicollinearity and data redundancy.", "problem": "An engineering student is analyzing the movement of a new robotic arm. The arm's end-effector is designed to move freely in three-dimensional space. To characterize its typical operational volume, the student collects a large sample of $n$ position measurements, where each measurement is a 3D coordinate vector $\\mathbf{x}_i = (x_i, y_i, z_i)$ for $i=1, ..., n$.\n\nThe student then computes the $3 \\times 3$ sample covariance matrix $S$ for this dataset. After performing the calculation, they are surprised to find that the determinant of the covariance matrix is exactly zero, i.e., $\\det(S) = 0$. Assuming the student's calculation is correct and the sample size $n$ is greater than 3, which of the following provides the most accurate and general interpretation of this result?\n\nA. All the collected data points lie perfectly on a single plane or a single line within the 3D space.\n\nB. The three position variables, $x$, $y$, and $z$, are mutually uncorrelated.\n\nC. All the collected data points are identical, meaning $\\mathbf{x}_1 = \\mathbf{x}_2 = \\dots = \\mathbf{x}_n$.\n\nD. The sample mean of the data, $(\\bar{x}, \\bar{y}, \\bar{z})$, must be the origin $(0, 0, 0)$.\n\nE. A calculation error must have occurred, as the determinant of a sample covariance matrix for real-world measurements cannot be exactly zero.", "solution": "Let the observations be arranged as rows of an $n \\times 3$ matrix $X$, and let $\\bar{\\mathbf{x}} \\in \\mathbb{R}^{3}$ be the sample mean. Define the centered data matrix $X_{c} = X - \\mathbf{1}\\bar{\\mathbf{x}}^{\\top}$, where $\\mathbf{1} \\in \\mathbb{R}^{n}$ is the vector of ones. The $3 \\times 3$ sample covariance matrix is\n$$\nS = \\frac{1}{n-1} X_{c}^{\\top} X_{c}.\n$$\nThis is a Gram matrix, hence symmetric positive semidefinite. The condition $\\det(S) = 0$ is equivalent to $S$ being singular, which is equivalent to $\\operatorname{rank}(S) < 3$.\n\nSince $S = \\frac{1}{n-1} X_{c}^{\\top} X_{c}$, the singularity of $S$ is equivalent to $\\operatorname{rank}(X_{c}^{\\top} X_{c}) < 3$, which in turn is equivalent to $\\operatorname{rank}(X_{c}) < 3$. Equivalently, there exists a nonzero vector $v \\in \\mathbb{R}^{3}$ such that\n$$\nS v = \\frac{1}{n-1} X_{c}^{\\top} X_{c} v = 0.\n$$\nSince $v^{\\top} S v = \\frac{1}{n-1} \\| X_{c} v \\|^{2}$, the equality $S v = 0$ implies $X_{c} v = 0$. Writing this row-wise, for each $i$,\n$$\n(\\mathbf{x}_{i} - \\bar{\\mathbf{x}})^{\\top} v = 0,\n$$\nso every centered observation lies in the plane through the origin orthogonal to $v$. Therefore, the original observations $\\mathbf{x}_{i}$ all lie in the affine plane $\\{\\bar{\\mathbf{x}} + w : v^{\\top} w = 0\\}$. If $\\operatorname{rank}(X_{c}) = 2$, this is a plane; if $\\operatorname{rank}(X_{c}) = 1$, the points lie on a line; if $\\operatorname{rank}(X_{c}) = 0$, all points are identical (a point). Thus, the most accurate and general interpretation is that all data lie exactly in a common plane or line (with the identical-points case included as a degenerate subcase).\n\nAssessment of the options:\n- A is correct: it states precisely that the data lie in a lower-dimensional affine subspace (a plane or a line), which is equivalent to $\\operatorname{rank}(X_{c}) < 3$ and thus $\\det(S)=0$.\n- B is incorrect: mutual uncorrelatedness would make $S$ diagonal, but $\\det(S)=0$ does not imply $S$ is diagonal; conversely, $S$ diagonal with positive variances gives $\\det(S) > 0$.\n- C is too strong: identical points imply $\\det(S)=0$, but the converse need not hold; data can vary along one or two directions (line or plane) without being identical.\n- D is false: the determinant of $S$ depends only on centered data and does not force the sample mean to be the origin.\n- E is false: exact singularity can occur whenever there is an exact linear dependence among coordinates, so no calculation error is necessary.\n\nTherefore, the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1924300"}, {"introduction": "One of the most powerful applications of the covariance matrix is in simplifying complex, correlated data through a technique called Principal Component Analysis (PCA). This practice problem [@problem_id:1924285] guides you through the core procedure of PCA: finding an orthogonal transformation to create new, uncorrelated variables. By calculating the eigenvectors of a given covariance matrix, you will effectively reorient the data along its axes of maximum variance, a fundamental skill in dimensionality reduction and feature extraction.", "problem": "An analyst is studying a simplified model of the daily percentage returns of two stocks. The returns are represented by a zero-mean random vector $\\mathbf{X} = \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}$, which has the covariance matrix\n$$\n\\Sigma = \\begin{pmatrix} 13 & -4 \\\\ -4 & 7 \\end{pmatrix}\n$$\nThe analyst wishes to construct a new pair of variables, called principal components, $\\mathbf{Y} = \\begin{pmatrix} Y_1 \\\\ Y_2 \\end{pmatrix}$, by applying a linear transformation $\\mathbf{Y} = \\mathbf{A} \\mathbf{X}$, where $\\mathbf{A}$ is a $2 \\times 2$ real matrix. The transformation must satisfy the following three conditions:\n1.  The new variables $Y_1$ and $Y_2$ must be uncorrelated.\n2.  The transformation matrix $\\mathbf{A}$ must be an orthogonal matrix.\n3.  To ensure a unique solution, the variance of $Y_1$ must be greater than or equal to the variance of $Y_2$, and the first entry of each row of matrix $\\mathbf{A}$ must be non-negative.\n\nDetermine the transformation matrix $\\mathbf{A}$.", "solution": "We seek a linear transformation $\\mathbf{Y}=\\mathbf{A}\\mathbf{X}$ with $\\mathbf{A}$ real $2\\times 2$, such that:\n- $Y_{1}$ and $Y_{2}$ are uncorrelated, i.e., $\\operatorname{Cov}(\\mathbf{Y})=\\mathbf{A}\\Sigma\\mathbf{A}^{\\top}$ is diagonal.\n- $\\mathbf{A}$ is orthogonal, i.e., $\\mathbf{A}\\mathbf{A}^{\\top}=\\mathbf{I}$.\n- $\\operatorname{Var}(Y_{1})\\geq\\operatorname{Var}(Y_{2})$, and the first entry of each row of $\\mathbf{A}$ is non-negative.\n\nFor a real symmetric covariance matrix $\\Sigma$, there exists an orthogonal matrix whose rows are orthonormal eigenvectors of $\\Sigma$, and this orthogonally diagonalizes $\\Sigma$. Therefore, we choose the rows of $\\mathbf{A}$ to be orthonormal eigenvectors of $\\Sigma$, ordered so that the first has the larger eigenvalue. Then $\\mathbf{A}\\Sigma\\mathbf{A}^{\\top}$ is diagonal with the eigenvalues on the diagonal.\n\nCompute eigenvalues of $\\Sigma=\\begin{pmatrix}13 & -4 \\\\ -4 & 7\\end{pmatrix}$ from the characteristic equation:\n$$\n\\det(\\Sigma-\\lambda \\mathbf{I})=\\det\\begin{pmatrix}13-\\lambda & -4 \\\\ -4 & 7-\\lambda\\end{pmatrix}\n=(13-\\lambda)(7-\\lambda)-16\n=\\lambda^{2}-20\\lambda+75=0.\n$$\nThus\n$$\n\\lambda_{1,2}=\\frac{20\\pm\\sqrt{400-300}}{2}=\\frac{20\\pm 10}{2},\n$$\nso the eigenvalues are $\\lambda_{1}=15$ and $\\lambda_{2}=5$, with $\\lambda_{1}>\\lambda_{2}$.\n\nFind a corresponding eigenvector for $\\lambda_{1}=15$ by solving $(\\Sigma-15\\mathbf{I})\\mathbf{v}=\\mathbf{0}$:\n$$\n\\begin{pmatrix}-2 & -4 \\\\ -4 & -8\\end{pmatrix}\\begin{pmatrix}x \\\\ y\\end{pmatrix}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\n\\implies -2x-4y=0 \\implies x=-2y.\n$$\nAn eigenvector is $\\begin{pmatrix}-2 \\\\ 1\\end{pmatrix}$. To satisfy the sign convention (first entry non-negative), multiply by $-1$ to get $\\begin{pmatrix}2 \\\\ -1\\end{pmatrix}$. Normalize to unit length:\n$$\n\\left\\|\\begin{pmatrix}2 \\\\ -1\\end{pmatrix}\\right\\|=\\sqrt{2^{2}+(-1)^{2}}=\\sqrt{5},\\quad\n\\mathbf{u}_{1}=\\begin{pmatrix}\\frac{2}{\\sqrt{5}} \\\\ -\\frac{1}{\\sqrt{5}}\\end{pmatrix}.\n$$\n\nFor $\\lambda_{2}=5$, solve $(\\Sigma-5\\mathbf{I})\\mathbf{v}=\\mathbf{0}$:\n$$\n\\begin{pmatrix}8 & -4 \\\\ -4 & 2\\end{pmatrix}\\begin{pmatrix}x \\\\ y\\end{pmatrix}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\n\\implies 8x-4y=0 \\implies y=2x.\n$$\nAn eigenvector is $\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$, already with a non-negative first entry. Normalize:\n$$\n\\left\\|\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}\\right\\|=\\sqrt{1^{2}+2^{2}}=\\sqrt{5},\\quad\n\\mathbf{u}_{2}=\\begin{pmatrix}\\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}}\\end{pmatrix}.\n$$\n\nChoose the rows of $\\mathbf{A}$ as these orthonormal eigenvectors, ordered so that the first row corresponds to the larger eigenvalue $\\lambda_{1}=15$, ensuring $\\operatorname{Var}(Y_{1})\\geq\\operatorname{Var}(Y_{2})$. Thus\n$$\n\\mathbf{A}=\\begin{pmatrix}\n\\frac{2}{\\sqrt{5}} & -\\frac{1}{\\sqrt{5}} \\\\\n\\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{5}}\n\\end{pmatrix}.\n$$\nThis $\\mathbf{A}$ is orthogonal (its rows are orthonormal), has non-negative first entries in each row, and yields\n$$\n\\operatorname{Cov}(\\mathbf{Y})=\\mathbf{A}\\Sigma\\mathbf{A}^{\\top}=\\begin{pmatrix}15 & 0 \\\\ 0 & 5\\end{pmatrix},\n$$\nso $Y_{1}$ and $Y_{2}$ are uncorrelated with $\\operatorname{Var}(Y_{1})>\\operatorname{Var}(Y_{2})$, as required.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{2}{\\sqrt{5}} & -\\frac{1}{\\sqrt{5}} \\\\ \\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{5}}\\end{pmatrix}}$$", "id": "1924285"}]}