{"hands_on_practices": [{"introduction": "The foundational principle of factor analysis is that the covariance between observed variables arises from their shared dependence on underlying, unobserved factors. This first exercise provides a direct application of this core concept for a simple one-factor model. By calculating the model-implied covariance from the factor loadings, you will see how the model quantitatively links latent constructs to observable relationships in the data [@problem_id:1917241].", "problem": "In psychometric research, a factor analysis model is often used to study latent traits that are not directly observable. A researcher is investigating the structure of analytical skills using a standard one-factor model. The single common factor, denoted by $f$, represents a latent \"Analytical Reasoning\" ability. This factor is assumed to be standardized, meaning it has a mean of zero and a variance of one.\n\nThree observed variables are measured:\n- $X_1$: Score on a logic puzzle test.\n- $X_2$: Score on a pattern recognition test.\n- $X_3$: Time taken to complete a set of complex calculations, measured in minutes.\n\nThe relationship between the observed variables and the latent factor is described by the model equation for each variable $i \\in \\{1, 2, 3\\}$:\n$$X_i = \\mu_i + \\lambda_i f + \\epsilon_i$$\nHere, $\\mu_i$ is the mean of variable $X_i$, $\\lambda_i$ is the factor loading, and $\\epsilon_i$ is the specific factor (or error term) for variable $X_i$. The error terms are mutually uncorrelated and also uncorrelated with the common factor $f$. Each error term $\\epsilon_i$ has a mean of zero.\n\nThe vector of factor loadings for the three variables is estimated to be $\\Lambda = \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\\\ \\lambda_3 \\end{pmatrix} = \\begin{pmatrix} 0.70 \\\\ 0.60 \\\\ -0.50 \\end{pmatrix}$.\n\nBased on this one-factor model, what is the model-implied covariance between the score on the logic puzzle test ($X_1$) and the score on the pattern recognition test ($X_2$)? Provide your answer as a real number.", "solution": "We are given the one-factor model for each observed variable $X_{i}$:\n$$\nX_{i} = \\mu_{i} + \\lambda_{i} f + \\epsilon_{i},\n$$\nwith $E[f]=0$, $\\operatorname{Var}(f)=1$, $E[\\epsilon_{i}]=0$, $\\operatorname{Cov}(\\epsilon_{i},\\epsilon_{j})=0$ for $i \\neq j$, and $\\operatorname{Cov}(f,\\epsilon_{i})=0$ for all $i$.\n\nThe covariance between $X_{1}$ and $X_{2}$ is\n$$\n\\operatorname{Cov}(X_{1},X_{2}) = \\operatorname{Cov}\\big( (\\mu_{1} + \\lambda_{1} f + \\epsilon_{1}),(\\mu_{2} + \\lambda_{2} f + \\epsilon_{2}) \\big).\n$$\nUsing linearity of covariance and the facts that constants do not contribute to covariance and that the specified uncorrelatedness holds, we obtain\n$$\n\\operatorname{Cov}(X_{1},X_{2}) = \\lambda_{1}\\lambda_{2}\\operatorname{Var}(f) + \\lambda_{1}\\operatorname{Cov}(f,\\epsilon_{2}) + \\lambda_{2}\\operatorname{Cov}(\\epsilon_{1},f) + \\operatorname{Cov}(\\epsilon_{1},\\epsilon_{2}).\n$$\nGiven $\\operatorname{Var}(f)=1$, $\\operatorname{Cov}(f,\\epsilon_{i})=0$, and $\\operatorname{Cov}(\\epsilon_{1},\\epsilon_{2})=0$, this simplifies to\n$$\n\\operatorname{Cov}(X_{1},X_{2}) = \\lambda_{1}\\lambda_{2}.\n$$\nSubstituting the given loadings $\\lambda_{1}=0.70$ and $\\lambda_{2}=0.60$,\n$$\n\\operatorname{Cov}(X_{1},X_{2}) = 0.70 \\times 0.60 = 0.42.\n$$\nTherefore, the model-implied covariance between $X_{1}$ and $X_{2}$ is $0.42$.", "answer": "$$\\boxed{0.42}$$", "id": "1917241"}, {"introduction": "After understanding how factors create covariance between variables, the next step is to examine how they explain the variance within a single variable. This practice introduces the concept of communality, which represents the proportion of a variable's variance that is accounted for by the common factors in the model. Calculating the communality from the given factor loadings will help you master this crucial aspect of variance decomposition in factor analysis [@problem_id:1917206].", "problem": "In a psychometric study, researchers are investigating the structure of cognitive abilities using a set of four standardized tests administered to a large sample of students. The observed variables are the scores on these four tests: $X_1$ (Verbal Comprehension), $X_2$ (Mathematical Fluency), $X_3$ (Visual-Spatial Reasoning), and $X_4$ (Logical Problem Solving). The researchers propose an orthogonal factor model with two common factors, $F_1$ and $F_2$, representing 'Quantitative Ability' and 'Verbal-Logical Ability', respectively.\n\nThe factor model is described by the equation $X = \\mu + \\Lambda F + \\epsilon$, where $X$ is the vector of observed variables, $\\mu$ is the vector of mean scores, $F$ is the vector of common factors, $\\Lambda$ is the matrix of factor loadings, and $\\epsilon$ is the vector of specific errors. After fitting the model to their data, the researchers obtained the following loading matrix $\\Lambda$:\n\n$$\n\\Lambda = \n\\begin{pmatrix}\n0.15 & 0.88 \\\\\n0.92 & 0.10 \\\\\n0.70 & 0.45 \\\\\n0.30 & 0.75\n\\end{pmatrix}\n$$\n\nwhere the entry $\\lambda_{ij}$ in row $i$ and column $j$ is the loading of the $i$-th test variable on the $j$-th factor.\n\nCalculate the communality of the third test variable, $X_3$ (Visual-Spatial Reasoning). The communality, denoted $h_i^2$ for variable $X_i$, represents the proportion of the variance in that variable that is accounted for by the common factors. Provide your answer as a dimensionless value rounded to three significant figures.", "solution": "The factor model is $X=\\mu+\\Lambda F+\\epsilon$ with two orthogonal common factors. For an orthogonal factor model with $\\operatorname{Cov}(F)=I$, the communality of variable $X_{i}$ is the variance explained by the common factors:\n$$\nh_{i}^{2}=\\operatorname{Var}(\\Lambda_{i\\cdot}F)=\\Lambda_{i\\cdot}\\,\\operatorname{Cov}(F)\\,\\Lambda_{i\\cdot}^{\\top}=\\sum_{j} \\lambda_{ij}^{2}.\n$$\nFor $X_{3}$, the loadings are $\\lambda_{31}=0.70$ and $\\lambda_{32}=0.45$, hence\n$$\nh_{3}^{2}=(0.70)^{2}+(0.45)^{2}=0.49+0.2025=0.6925.\n$$\nRounding to three significant figures gives\n$$\nh_{3}^{2}\\approx 0.693.\n$$", "answer": "$$\\boxed{0.693}$$", "id": "1917206"}, {"introduction": "A factor analysis model must not only fit the data but also be theoretically sound. The variance explained by the common factors for any given variable, its communality, cannot logically exceed the total variance of that variable. This exercise challenges you to apply this fundamental constraint to evaluate the validity of a proposed factor model, moving from pure calculation to the critical skill of identifying an inadmissible solution known as a \"Heywood case\" [@problem_id:1917247].", "problem": "A team of data scientists is analyzing a complex system with four observable metrics, denoted by the random variables $X_1, X_2, X_3,$ and $X_4$. They propose that the underlying structure of these metrics can be described by an orthogonal factor model with two unobserved common factors, $F_1$ and $F_2$. The model is given by the equation $X - \\mu = \\Lambda F + \\epsilon$, where $X = (X_1, X_2, X_3, X_4)^T$ is the vector of observed variables, $\\mu$ is the vector of mean values, $F = (F_1, F_2)^T$ is the vector of common factors, $\\Lambda$ is the $4 \\times 2$ matrix of factor loadings, and $\\epsilon$ is the vector of specific factors or errors.\n\nThe common factors are assumed to be standardized (mean zero, variance one) and uncorrelated with each other. The specific factors are assumed to be uncorrelated with each other and with the common factors.\n\nFrom a large dataset, the team has estimated the covariance matrix $\\Sigma$ for the observed variables. The variances of the four metrics (the diagonal elements of $\\Sigma$) are found to be:\n$\\text{Var}(X_1) = 15.0$\n$\\text{Var}(X_2) = 12.0$\n$\\text{Var}(X_3) = 20.0$\n$\\text{Var}(X_4) = 18.0$\n\nThe team considers four different candidate matrices for the factor loadings, $\\Lambda$. One of these matrices is fundamentally inconsistent with the theoretical assumptions of the orthogonal factor model, implying a physically impossible scenario for the variance structure. Which of the following loading matrices is theoretically invalid?\n\nA.\n$$ \\Lambda_A = \\begin{pmatrix} 3 & 2 \\\\ 1 & 3 \\\\ 4 & 1 \\\\ 2 & 2 \\end{pmatrix} $$\n\nB.\n$$ \\Lambda_B = \\begin{pmatrix} 2 & 1 \\\\ 3 & 1 \\\\ 2 & 4 \\\\ 4 & 1 \\end{pmatrix} $$\n\nC.\n$$ \\Lambda_C = \\begin{pmatrix} 1 & 3 \\\\ 3 & 2 \\\\ 4 & 2 \\\\ 2 & 3 \\end{pmatrix} $$\n\nD.\n$$ \\Lambda_D = \\begin{pmatrix} 3 & 1 \\\\ 2 & 2 \\\\ 1 & 4 \\\\ 3 & 2 \\end{pmatrix} $$", "solution": "In the orthogonal factor model $X - \\mu = \\Lambda F + \\epsilon$, with standardized and uncorrelated common factors, $\\mathrm{Cov}(F) = I_{2}$, and specific factors uncorrelated with each other and with $F$, $\\mathrm{Cov}(\\epsilon) = \\Psi = \\mathrm{diag}(\\psi_{1},\\psi_{2},\\psi_{3},\\psi_{4})$ with $\\psi_{i} \\ge 0$. Therefore the covariance matrix of $X$ is\n$$\n\\Sigma = \\Lambda \\Lambda^{\\top} + \\Psi.\n$$\nTaking diagonal elements gives, for each $i \\in \\{1,2,3,4\\}$,\n$$\n\\mathrm{Var}(X_{i}) = \\Sigma_{ii} = \\sum_{k=1}^{2} \\lambda_{ik}^{2} + \\psi_{i},\n$$\nso the communality $h_{i}^{2} = \\sum_{k=1}^{2} \\lambda_{ik}^{2}$ must satisfy\n$$\nh_{i}^{2} \\le \\mathrm{Var}(X_{i}),\n$$\nbecause $\\psi_{i} = \\mathrm{Var}(X_{i}) - h_{i}^{2} \\ge 0$ is required.\n\nGiven $\\mathrm{Var}(X_{1}) = 15$, $\\mathrm{Var}(X_{2}) = 12$, $\\mathrm{Var}(X_{3}) = 20$, and $\\mathrm{Var}(X_{4}) = 18$, compute $h_{i}^{2}$ row-by-row for each candidate loading matrix and check $h_{i}^{2} \\le \\mathrm{Var}(X_{i})$.\n\nFor $\\Lambda_{A}$, rows $(3,2)$, $(1,3)$, $(4,1)$, $(2,2)$:\n$$\nh_{1}^{2} = 3^{2} + 2^{2} = 9 + 4 = 13 \\le 15,\\quad\nh_{2}^{2} = 1^{2} + 3^{2} = 1 + 9 = 10 \\le 12,\n$$\n$$\nh_{3}^{2} = 4^{2} + 1^{2} = 16 + 1 = 17 \\le 20,\\quad\nh_{4}^{2} = 2^{2} + 2^{2} = 4 + 4 = 8 \\le 18.\n$$\nAll implied specific variances $\\psi_{i} = \\mathrm{Var}(X_{i}) - h_{i}^{2}$ are nonnegative, so $\\Lambda_{A}$ is admissible.\n\nFor $\\Lambda_{B}$, rows $(2,1)$, $(3,1)$, $(2,4)$, $(4,1)$:\n$$\nh_{1}^{2} = 2^{2} + 1^{2} = 4 + 1 = 5 \\le 15,\\quad\nh_{2}^{2} = 3^{2} + 1^{2} = 9 + 1 = 10 \\le 12,\n$$\n$$\nh_{3}^{2} = 2^{2} + 4^{2} = 4 + 16 = 20 \\le 20,\\quad\nh_{4}^{2} = 4^{2} + 1^{2} = 16 + 1 = 17 \\le 18.\n$$\nAll $\\psi_{i} \\ge 0$, so $\\Lambda_{B}$ is admissible.\n\nFor $\\Lambda_{C}$, rows $(1,3)$, $(3,2)$, $(4,2)$, $(2,3)$:\n$$\nh_{1}^{2} = 1^{2} + 3^{2} = 1 + 9 = 10 \\le 15,\\quad\nh_{2}^{2} = 3^{2} + 2^{2} = 9 + 4 = 13,\n$$\nbut $\\mathrm{Var}(X_{2}) = 12$, so $h_{2}^{2} = 13 > 12$, which would imply $\\psi_{2} = 12 - 13 = -1 < 0$, an impossible (negative) specific variance. Hence $\\Lambda_{C}$ violates the orthogonal factor model assumptions.\n\nFor $\\Lambda_{D}$, rows $(3,1)$, $(2,2)$, $(1,4)$, $(3,2)$:\n$$\nh_{1}^{2} = 3^{2} + 1^{2} = 9 + 1 = 10 \\le 15,\\quad\nh_{2}^{2} = 2^{2} + 2^{2} = 4 + 4 = 8 \\le 12,\n$$\n$$\nh_{3}^{2} = 1^{2} + 4^{2} = 1 + 16 = 17 \\le 20,\\quad\nh_{4}^{2} = 3^{2} + 2^{2} = 9 + 4 = 13 \\le 18.\n$$\nAll $\\psi_{i} \\ge 0$, so $\\Lambda_{D}$ is admissible.\n\nTherefore, the only theoretically invalid loading matrix is option C, because it requires a negative specific variance for $X_{2}$.", "answer": "$$\\boxed{C}$$", "id": "1917247"}]}