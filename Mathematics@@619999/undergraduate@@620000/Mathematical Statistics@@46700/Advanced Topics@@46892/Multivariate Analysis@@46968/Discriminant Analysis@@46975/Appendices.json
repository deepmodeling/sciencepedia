{"hands_on_practices": [{"introduction": "A key assumption of standard Linear Discriminant Analysis (LDA) is that the populations from which our groups are sampled share a common covariance matrix. This practice walks you through the foundational procedure of estimating this matrix by \"pooling\" the covariance information from individual samples [@problem_id:1914041]. Calculating the pooled sample covariance matrix, $S_p$, is a critical first step in building a robust LDA classifier.", "problem": "A data scientist at an e-commerce company is conducting a preliminary analysis for a potential Linear Discriminant Analysis (LDA) model to classify customers. The goal is to distinguish between two groups of customers: \"Premium Subscribers\" and \"Standard Users\". Two key metrics have been measured for small random samples from each group:\n- $X_1$: Average session duration in minutes.\n- $X_2$: Number of items purchased in the last month.\n\nThe data is collected as two-dimensional vectors $\\mathbf{x} = [X_1, X_2]^T$.\n\nThe sample for the \"Premium Subscribers\" group (Group 1) consists of $n_1=4$ customers:\n$$ \\mathbf{x}_1^{(1)} = \\begin{pmatrix} 15 \\\\ 8 \\end{pmatrix}, \\quad \\mathbf{x}_2^{(1)} = \\begin{pmatrix} 20 \\\\ 10 \\end{pmatrix}, \\quad \\mathbf{x}_3^{(1)} = \\begin{pmatrix} 18 \\\\ 6 \\end{pmatrix}, \\quad \\mathbf{x}_4^{(1)} = \\begin{pmatrix} 17 \\\\ 8 \\end{pmatrix} $$\n\nThe sample for the \"Standard Users\" group (Group 2) consists of $n_2=5$ customers:\n$$ \\mathbf{x}_1^{(2)} = \\begin{pmatrix} 8 \\\\ 2 \\end{pmatrix}, \\quad \\mathbf{x}_2^{(2)} = \\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix}, \\quad \\mathbf{x}_3^{(2)} = \\begin{pmatrix} 10 \\\\ 3 \\end{pmatrix}, \\quad \\mathbf{x}_4^{(2)} = \\begin{pmatrix} 6 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{x}_5^{(2)} = \\begin{pmatrix} 9 \\\\ 5 \\end{pmatrix} $$\n\nA key assumption for standard LDA is that the covariance matrices of the different groups are equal. To proceed, the data scientist needs to compute a single estimate for this common covariance matrix, known as the pooled sample covariance matrix, $S_p$.\n\nCalculate the pooled sample covariance matrix $S_p$ based on the provided data.", "solution": "We estimate the common covariance via the pooled sample covariance matrix\n$$\nS_{p}=\\frac{(n_{1}-1)S_{1}+(n_{2}-1)S_{2}}{n_{1}+n_{2}-2},\n$$\nwhere, for group $k\\in\\{1,2\\}$ with size $n_{k}$, the sample covariance is\n$$\nS_{k}=\\frac{1}{n_{k}-1}\\sum_{i=1}^{n_{k}}\\left(\\mathbf{x}_{i}^{(k)}-\\bar{\\mathbf{x}}^{(k)}\\right)\\left(\\mathbf{x}_{i}^{(k)}-\\bar{\\mathbf{x}}^{(k)}\\right)^{T},\n$$\nand $\\bar{\\mathbf{x}}^{(k)}=\\frac{1}{n_{k}}\\sum_{i=1}^{n_{k}}\\mathbf{x}_{i}^{(k)}$.\n\nFirst compute the sample means.\n\nFor Group 1 ($n_{1}=4$):\n$$\n\\bar{\\mathbf{x}}^{(1)}=\\frac{1}{4}\\begin{pmatrix}15+20+18+17\\\\ 8+10+6+8\\end{pmatrix}=\\begin{pmatrix}17.5\\\\ 8\\end{pmatrix}.\n$$\nFor Group 2 ($n_{2}=5$):\n$$\n\\bar{\\mathbf{x}}^{(2)}=\\frac{1}{5}\\begin{pmatrix}8+12+10+6+9\\\\ 2+4+3+1+5\\end{pmatrix}=\\begin{pmatrix}9\\\\ 3\\end{pmatrix}.\n$$\n\nNext compute $S_{1}$. Let $\\mathbf{d}_{i}^{(1)}=\\mathbf{x}_{i}^{(1)}-\\bar{\\mathbf{x}}^{(1)}$.\nThe deviations and outer products are:\n$$\n\\mathbf{d}_{1}^{(1)}=\\begin{pmatrix}-2.5\\\\ 0\\end{pmatrix},\\quad \\mathbf{d}_{1}^{(1)}\\mathbf{d}_{1}^{(1)T}=\\begin{pmatrix}6.25&0\\\\ 0&0\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{2}^{(1)}=\\begin{pmatrix}2.5\\\\ 2\\end{pmatrix},\\quad \\mathbf{d}_{2}^{(1)}\\mathbf{d}_{2}^{(1)T}=\\begin{pmatrix}6.25&5\\\\ 5&4\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{3}^{(1)}=\\begin{pmatrix}0.5\\\\ -2\\end{pmatrix},\\quad \\mathbf{d}_{3}^{(1)}\\mathbf{d}_{3}^{(1)T}=\\begin{pmatrix}0.25&-1\\\\ -1&4\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{4}^{(1)}=\\begin{pmatrix}-0.5\\\\ 0\\end{pmatrix},\\quad \\mathbf{d}_{4}^{(1)}\\mathbf{d}_{4}^{(1)T}=\\begin{pmatrix}0.25&0\\\\ 0&0\\end{pmatrix}.\n$$\nSumming gives\n$$\n\\sum_{i=1}^{4}\\mathbf{d}_{i}^{(1)}\\mathbf{d}_{i}^{(1)T}=\\begin{pmatrix}13&4\\\\ 4&8\\end{pmatrix},\n$$\nhence\n$$\nS_{1}=\\frac{1}{n_{1}-1}\\begin{pmatrix}13&4\\\\ 4&8\\end{pmatrix}=\\frac{1}{3}\\begin{pmatrix}13&4\\\\ 4&8\\end{pmatrix}.\n$$\n\nNow compute $S_{2}$. Let $\\mathbf{d}_{i}^{(2)}=\\mathbf{x}_{i}^{(2)}-\\bar{\\mathbf{x}}^{(2)}$.\nThe deviations and outer products are:\n$$\n\\mathbf{d}_{1}^{(2)}=\\begin{pmatrix}-1\\\\ -1\\end{pmatrix},\\ \\ \\mathbf{d}_{1}^{(2)}\\mathbf{d}_{1}^{(2)T}=\\begin{pmatrix}1&1\\\\ 1&1\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{2}^{(2)}=\\begin{pmatrix}3\\\\ 1\\end{pmatrix},\\ \\ \\mathbf{d}_{2}^{(2)}\\mathbf{d}_{2}^{(2)T}=\\begin{pmatrix}9&3\\\\ 3&1\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{3}^{(2)}=\\begin{pmatrix}1\\\\ 0\\end{pmatrix},\\ \\ \\mathbf{d}_{3}^{(2)}\\mathbf{d}_{3}^{(2)T}=\\begin{pmatrix}1&0\\\\ 0&0\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{4}^{(2)}=\\begin{pmatrix}-3\\\\ -2\\end{pmatrix},\\ \\ \\mathbf{d}_{4}^{(2)}\\mathbf{d}_{4}^{(2)T}=\\begin{pmatrix}9&6\\\\ 6&4\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{5}^{(2)}=\\begin{pmatrix}0\\\\ 2\\end{pmatrix},\\ \\ \\mathbf{d}_{5}^{(2)}\\mathbf{d}_{5}^{(2)T}=\\begin{pmatrix}0&0\\\\ 0&4\\end{pmatrix}.\n$$\nSumming gives\n$$\n\\sum_{i=1}^{5}\\mathbf{d}_{i}^{(2)}\\mathbf{d}_{i}^{(2)T}=\\begin{pmatrix}20&10\\\\ 10&10\\end{pmatrix},\n$$\nhence\n$$\nS_{2}=\\frac{1}{n_{2}-1}\\begin{pmatrix}20&10\\\\ 10&10\\end{pmatrix}=\\frac{1}{4}\\begin{pmatrix}20&10\\\\ 10&10\\end{pmatrix}.\n$$\n\nFinally, the pooled covariance is\n$$\nS_{p}=\\frac{(n_{1}-1)S_{1}+(n_{2}-1)S_{2}}{n_{1}+n_{2}-2}=\\frac{\\begin{pmatrix}13&4\\\\ 4&8\\end{pmatrix}+\\begin{pmatrix}20&10\\\\ 10&10\\end{pmatrix}}{7}=\\frac{1}{7}\\begin{pmatrix}33&14\\\\ 14&18\\end{pmatrix}=\\begin{pmatrix}\\frac{33}{7}&2\\\\ 2&\\frac{18}{7}\\end{pmatrix}.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{33}{7} & 2 \\\\ 2 & \\frac{18}{7}\\end{pmatrix}}$$", "id": "1914041"}, {"introduction": "With the group means and the pooled covariance matrix established, the next step is to apply them to classify a new, unknown observation. This exercise introduces the Mahalanobis distance, a powerful statistical measure that accounts for the data's variance and covariance when calculating how \"far\" a point is from a group's center [@problem_id:1914060]. By assigning a new observation to the group with the smallest Mahalanobis distance, you will perform the essential task of discriminant analysis.", "problem": "A team of astrophysicists is developing an automated classification algorithm to distinguish between two types of celestial objects based on two measured features. The features are the 'Atmospheric Methane Concentration Index' ($x_1$) and the 'Planetary Albedo Index' ($x_2$). The data is collected for two established groups: Group 1, consisting of 'Potentially Habitable Exoplanets', and Group 2, consisting of 'Non-Habitable Gas Giants'.\n\nBased on a large number of previous observations, the team has calculated the sample mean vectors for each group, $\\bar{\\mathbf{x}}_1$ and $\\bar{\\mathbf{x}}_2$, and a pooled sample covariance matrix, $\\mathbf{S}_p$. The feature vector for an object is given by $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\n\nThe calculated statistics are as follows:\n- Mean of Group 1: $\\bar{\\mathbf{x}}_1 = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}$\n- Mean of Group 2: $\\bar{\\mathbf{x}}_2 = \\begin{pmatrix} 6 \\\\ 5 \\end{pmatrix}$\n- Pooled covariance matrix: $\\mathbf{S}_p = \\begin{pmatrix} 3 & 1 \\\\ 1 & 1 \\end{pmatrix}$\n\nA new, unclassified object is detected with the feature vector $\\mathbf{x}_{new} = \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}$.\n\nYou are to classify this new object by assigning it to the group with the smaller squared Mahalanobis distance. This distance, from a point $\\mathbf{x}$ to the center of group $k$, is defined as $D_k^2(\\mathbf{x}) = (\\mathbf{x} - \\bar{\\mathbf{x}}_k)^T \\mathbf{S}_p^{-1} (\\mathbf{x} - \\bar{\\mathbf{x}}_k)$, where the superscript $T$ denotes the transpose. Assume that the prior probabilities of an object belonging to either group are equal. Based on this rule, which of the following conclusions is correct?\n\nA. The object should be classified into Group 1 (Potentially Habitable Exoplanets).\n\nB. The object should be classified into Group 2 (Non-Habitable Gas Giants).\n\nC. The object is equidistant from both group centers, so classification is ambiguous.\n\nD. The provided pooled covariance matrix is not valid for this type of calculation, making classification impossible.", "solution": "We must assign the new object to the group with the smaller squared Mahalanobis distance defined by $D_{k}^{2}(\\mathbf{x})=(\\mathbf{x}-\\bar{\\mathbf{x}}_{k})^{T}\\mathbf{S}_{p}^{-1}(\\mathbf{x}-\\bar{\\mathbf{x}}_{k})$.\n\nFirst compute the inverse of the pooled covariance matrix. For $\\mathbf{S}_{p}=\\begin{pmatrix}3 & 1 \\\\ 1 & 1\\end{pmatrix}$, the determinant is $\\det(\\mathbf{S}_{p})=3\\cdot 1-1\\cdot 1=2$. Using the $2\\times 2$ inverse formula,\n$$\n\\mathbf{S}_{p}^{-1}=\\frac{1}{\\det(\\mathbf{S}_{p})}\\begin{pmatrix}1 & -1 \\\\ -1 & 3\\end{pmatrix}=\\frac{1}{2}\\begin{pmatrix}1 & -1 \\\\ -1 & 3\\end{pmatrix}.\n$$\n\nCompute the centered vectors:\n$$\n\\mathbf{v}_{1}=\\mathbf{x}_{new}-\\bar{\\mathbf{x}}_{1}=\\begin{pmatrix}4 \\\\ 5\\end{pmatrix}-\\begin{pmatrix}2 \\\\ 3\\end{pmatrix}=\\begin{pmatrix}2 \\\\ 2\\end{pmatrix},\\quad\n\\mathbf{v}_{2}=\\mathbf{x}_{new}-\\bar{\\mathbf{x}}_{2}=\\begin{pmatrix}4 \\\\ 5\\end{pmatrix}-\\begin{pmatrix}6 \\\\ 5\\end{pmatrix}=\\begin{pmatrix}-2 \\\\ 0\\end{pmatrix}.\n$$\n\nEvaluate $D_{1}^{2}$:\n$$\n\\mathbf{S}_{p}^{-1}\\mathbf{v}_{1}=\\frac{1}{2}\\begin{pmatrix}1 & -1 \\\\ -1 & 3\\end{pmatrix}\\begin{pmatrix}2 \\\\ 2\\end{pmatrix}\n=\\frac{1}{2}\\begin{pmatrix}0 \\\\ 4\\end{pmatrix}\n=\\begin{pmatrix}0 \\\\ 2\\end{pmatrix},\n$$\n$$\nD_{1}^{2}=\\mathbf{v}_{1}^{T}\\mathbf{S}_{p}^{-1}\\mathbf{v}_{1}=\\begin{pmatrix}2 & 2\\end{pmatrix}\\begin{pmatrix}0 \\\\ 2\\end{pmatrix}=4.\n$$\n\nEvaluate $D_{2}^{2}$:\n$$\n\\mathbf{S}_{p}^{-1}\\mathbf{v}_{2}=\\frac{1}{2}\\begin{pmatrix}1 & -1 \\\\ -1 & 3\\end{pmatrix}\\begin{pmatrix}-2 \\\\ 0\\end{pmatrix}\n=\\frac{1}{2}\\begin{pmatrix}-2 \\\\ 2\\end{pmatrix}\n=\\begin{pmatrix}-1 \\\\ 1\\end{pmatrix},\n$$\n$$\nD_{2}^{2}=\\mathbf{v}_{2}^{T}\\mathbf{S}_{p}^{-1}\\mathbf{v}_{2}=\\begin{pmatrix}-2 & 0\\end{pmatrix}\\begin{pmatrix}-1 \\\\ 1\\end{pmatrix}=2.\n$$\n\nSince $D_{2}^{2}=2<D_{1}^{2}=4$, the object is closer (in the Mahalanobis sense) to Group 2. The pooled covariance matrix is valid because its leading principal minors are positive, so it is positive definite. With equal priors, we classify to Group 2.", "answer": "$$\\boxed{B}$$", "id": "1914060"}, {"introduction": "While classifying individual points is the primary goal of discriminant analysis, understanding the overall decision rule provides deeper insight. This practice demonstrates how the classification logic of LDA translates into a simple geometric boundary—a straight line in two-dimensional feature space [@problem_id:1914104]. Deriving the equation for this linear discriminant function makes the classification rule explicit and allows us to visualize precisely how the model separates the different classes.", "problem": "A semiconductor manufacturing plant uses an automated optical inspection system to classify silicon wafers. The system measures two key geometric properties of a wafer: its bow ($x_1$) and its warp ($x_2$), both measured in micrometers. Based on historical data, wafers are categorized into two groups: 'Prime' (high quality) and 'Test' (lower quality, requires further testing).\n\nA statistical model, based on Linear Discriminant Analysis (LDA), is used to create a decision boundary to separate these two groups. The analysis assumes that the feature vectors for both groups, $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, are drawn from bivariate normal distributions with a common covariance matrix. It is also assumed that the prior probabilities of a wafer being 'Prime' or 'Test' are equal.\n\nThe historical data provides the following statistical summaries:\n- The mean vector for 'Prime' wafers is $\\boldsymbol{\\mu}_P = \\begin{pmatrix} 10 \\\\ 5 \\end{pmatrix}$.\n- The mean vector for 'Test' wafers is $\\boldsymbol{\\mu}_T = \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix}$.\n- The pooled covariance matrix for both groups is $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}$.\n\nDetermine the linear discriminant function, $f(x_1, x_2)$, such that the decision boundary is given by the equation $f(x_1, x_2) = 0$. The function should be in the form $f(x_1, x_2) = a_1 x_1 + a_2 x_2 + a_0$, where the coefficients $a_1$, $a_2$, and $a_0$ are integers with no common factors, and $a_1$ is positive.", "solution": "For two classes with means $\\boldsymbol{\\mu}_{P}$ and $\\boldsymbol{\\mu}_{T}$, common covariance $\\boldsymbol{\\Sigma}$, and equal priors, Linear Discriminant Analysis yields class scores\n$$\n\\delta_{k}(\\mathbf{x})= \\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{k} - \\frac{1}{2}\\boldsymbol{\\mu}_{k}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{k} + \\ln \\pi_{k},\n$$\nwhere $\\pi_{k}$ is the prior. With $\\pi_{P}=\\pi_{T}$, the decision boundary is $\\delta_{P}(\\mathbf{x})=\\delta_{T}(\\mathbf{x})$, equivalently\n$$\n\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_{P}-\\boldsymbol{\\mu}_{T}) - \\frac{1}{2}\\left(\\boldsymbol{\\mu}_{P}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{P} - \\boldsymbol{\\mu}_{T}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{T}\\right)=0.\n$$\nThus the linear discriminant function can be written as\n$$\nf(x_{1},x_{2})= \\mathbf{w}^{\\top}\\mathbf{x} + a_{0},\n$$\nwith $\\mathbf{w}=\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_{P}-\\boldsymbol{\\mu}_{T})$ and $a_{0}= -\\frac{1}{2}\\left(\\boldsymbol{\\mu}_{P}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{P} - \\boldsymbol{\\mu}_{T}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{T}\\right)$.\n\nGiven\n$$\n\\boldsymbol{\\mu}_{P}=\\begin{pmatrix}10\\\\ 5\\end{pmatrix},\\quad\n\\boldsymbol{\\mu}_{T}=\\begin{pmatrix}6\\\\ 9\\end{pmatrix},\\quad\n\\boldsymbol{\\Sigma}=\\begin{pmatrix}2&1\\\\ 1&3\\end{pmatrix},\n$$\ncompute $\\boldsymbol{\\Sigma}^{-1}$. Its determinant is $\\det(\\boldsymbol{\\Sigma})=2\\cdot 3 - 1\\cdot 1 = 5$, so\n$$\n\\boldsymbol{\\Sigma}^{-1}=\\frac{1}{5}\\begin{pmatrix}3&-1\\\\ -1&2\\end{pmatrix}.\n$$\nCompute the weight vector:\n$$\n\\boldsymbol{\\mu}_{P}-\\boldsymbol{\\mu}_{T}=\\begin{pmatrix}4\\\\ -4\\end{pmatrix},\\quad\n\\mathbf{w}=\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_{P}-\\boldsymbol{\\mu}_{T})=\\frac{1}{5}\\begin{pmatrix}3&-1\\\\ -1&2\\end{pmatrix}\\begin{pmatrix}4\\\\ -4\\end{pmatrix}=\\frac{1}{5}\\begin{pmatrix}16\\\\ -12\\end{pmatrix}=\\begin{pmatrix}\\frac{16}{5}\\\\ -\\frac{12}{5}\\end{pmatrix}.\n$$\nCompute the constant term:\n$$\n\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{P}=\\frac{1}{5}\\begin{pmatrix}3&-1\\\\ -1&2\\end{pmatrix}\\begin{pmatrix}10\\\\ 5\\end{pmatrix}=\\begin{pmatrix}5\\\\ 0\\end{pmatrix}\\Rightarrow \\boldsymbol{\\mu}_{P}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{P}=50,\n$$\n$$\n\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{T}=\\frac{1}{5}\\begin{pmatrix}3&-1\\\\ -1&2\\end{pmatrix}\\begin{pmatrix}6\\\\ 9\\end{pmatrix}=\\begin{pmatrix}\\frac{9}{5}\\\\ \\frac{12}{5}\\end{pmatrix}\\Rightarrow \\boldsymbol{\\mu}_{T}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{T}=\\frac{162}{5}.\n$$\nTherefore,\n$$\na_{0}=-\\frac{1}{2}\\left(50-\\frac{162}{5}\\right)=-\\frac{1}{2}\\cdot \\frac{88}{5}=-\\frac{44}{5}.\n$$\nHence\n$$\nf(x_{1},x_{2})=\\frac{16}{5}x_{1}-\\frac{12}{5}x_{2}-\\frac{44}{5}.\n$$\nMultiplying by the positive factor $5$ and simplifying to coprime integer coefficients yields\n$$\nf(x_{1},x_{2})=16x_{1}-12x_{2}-44=4x_{1}-3x_{2}-11,\n$$\nwith $a_{1}=4>0$ and $\\gcd(4,3,11)=1$, as required.", "answer": "$$\\boxed{4 x_{1} - 3 x_{2} - 11}$$", "id": "1914104"}]}