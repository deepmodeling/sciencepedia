{"hands_on_practices": [{"introduction": "The Wishart distribution is fundamental to understanding sample covariance matrices. This first exercise provides a practical application, connecting the abstract theory to a tangible statistical object used in biostatistics. By calculating the expected total variation of a sample from multivariate normal data, you will apply one of the most basic and important properties of the Wishart distributionâ€”its expectation [@problem_id:1967888].", "problem": "A biostatistician is analyzing phenotypic data from a newly discovered species of orchid. Four key floral measurements are recorded for each specimen: sepal length, petal width, labellum length, and column height. These measurements are modeled as a 4-variate random vector, which is assumed to follow a multivariate normal distribution with an unknown mean vector $\\mu$ and an unknown population covariance matrix $\\Sigma$.\n\nA random sample of $n=25$ orchids is collected. Let the vector of measurements for the $i$-th orchid be denoted by $X_i \\in \\mathbb{R}^4$. The sample mean vector is calculated as $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$.\n\nThe sum-of-squares and cross-products (SSCP) matrix, $A$, is computed using the formula $A = \\sum_{i=1}^{n} (X_i - \\bar{X})(X_i - \\bar{X})^T$. In multivariate statistics, it is a standard result that the matrix $A$ follows a Wishart distribution, denoted by $A \\sim W_p(k, \\Sigma)$, where $p$ is the dimensionality of the data and $k$ is the degrees of freedom.\n\nFrom extensive prior genetic analysis on related species, the population variances for the four measurements (sepal length, petal width, labellum length, and column height) are reliably estimated to be $10 \\text{ mm}^2$, $3 \\text{ mm}^2$, $8 \\text{ mm}^2$, and $4 \\text{ mm}^2$, respectively.\n\nThe biostatistician is interested in the expected total variation captured by the centered data. This corresponds to the expected value of the trace of the SSCP matrix, $A$. Calculate the expected value of the trace of $A$. Express your answer in units of $\\text{mm}^2$.", "solution": "Let $X_{i} \\in \\mathbb{R}^{4}$ be i.i.d. $N_{4}(\\mu,\\Sigma)$ and define the SSCP matrix $A=\\sum_{i=1}^{n}(X_{i}-\\bar{X})(X_{i}-\\bar{X})^{T}$ with $n=25$. A standard result is that $A \\sim W_{p}(k, \\Sigma)$ with $p=4$ and $k=n-1=24$. For a Wishart random matrix $W \\sim W_{p}(k, \\Sigma)$, the expectation satisfies $\\mathbb{E}[W]=k\\Sigma$. Using linearity of trace and $\\operatorname{tr}(cM)=c\\,\\operatorname{tr}(M)$, we have\n$$\n\\mathbb{E}[\\operatorname{tr}(A)]=\\operatorname{tr}(\\mathbb{E}[A])=\\operatorname{tr}(k\\Sigma)=k\\,\\operatorname{tr}(\\Sigma).\n$$\nThe trace of $\\Sigma$ equals the sum of the population variances of the four measurements, which are given as $10$, $3$, $8$, and $4$ in $\\text{mm}^{2}$. Therefore,\n$$\n\\operatorname{tr}(\\Sigma)=10+3+8+4=25 \\text{ mm}^{2},\n$$\nand hence\n$$\n\\mathbb{E}[\\operatorname{tr}(A)]=24 \\times 25=600 \\text{ mm}^{2}.\n$$", "answer": "$$\\boxed{600}$$", "id": "1967888"}, {"introduction": "Building on the concept of expectation, we now delve deeper into the structure of a Wishart-distributed matrix by examining its second moments. This practice asks you to derive the variance of a diagonal element of a Wishart matrix, which corresponds to the variance of a sample variance. This exercise will strengthen your understanding of how the Wishart distribution is constructed from underlying normal vectors and how variability propagates from the population to the sample [@problem_id:1967831].", "problem": "In the field of multivariate statistics, the Wishart distribution serves as a generalization of the chi-squared distribution to multiple dimensions and is crucial for inferring properties of covariance matrices.\n\nLet $W$ be a $p \\times p$ random matrix following a Wishart distribution, denoted as $W \\sim W_p(n, \\Sigma)$, where $n$ is the degrees of freedom and $\\Sigma$ is a $p \\times p$ symmetric, positive-definite scale matrix. The elements of the random matrix $W$ are denoted by $W_{ij}$, and the elements of the constant scale matrix $\\Sigma$ are denoted by $\\Sigma_{ij}$.\n\nGiven these definitions, determine the variance of a diagonal element of the random matrix $W$, denoted as $Var(W_{ii})$, for any $i \\in \\{1, 2, ..., p\\}$. Your final answer should be a closed-form analytic expression in terms of the degrees of freedom $n$ and the corresponding diagonal element of the scale matrix, $\\Sigma_{ii}$.", "solution": "Start from the constructive definition of the Wishart distribution: if $X_{1},\\dots,X_{n}$ are independent and identically distributed as $N_{p}(0,\\Sigma)$, then\n$$\nW \\equiv \\sum_{t=1}^{n} X_{t} X_{t}^{\\top} \\sim W_{p}(n,\\Sigma).\n$$\nFor a fixed index $i \\in \\{1,\\dots,p\\}$, the $(i,i)$ element of $W$ is\n$$\nW_{ii}=\\sum_{t=1}^{n} X_{ti}^{2},\n$$\nwhere $X_{ti}$ denotes the $i$th component of $X_{t}$. Since the vectors $X_{t}$ are independent across $t$, the random variables $X_{ti}^{2}$ are also independent across $t$ for fixed $i$. Therefore,\n$$\n\\operatorname{Var}(W_{ii})=\\operatorname{Var}\\!\\left(\\sum_{t=1}^{n} X_{ti}^{2}\\right)=\\sum_{t=1}^{n} \\operatorname{Var}(X_{ti}^{2})=n\\,\\operatorname{Var}(X_{1i}^{2}).\n$$\nBecause $X_{1i}\\sim N(0,\\Sigma_{ii})$, its second and fourth moments are\n$$\n\\mathbb{E}[X_{1i}^{2}] = \\Sigma_{ii}, \\qquad \\mathbb{E}[X_{1i}^{4}] = 3\\,\\Sigma_{ii}^{2}.\n$$\nHence,\n$$\n\\operatorname{Var}(X_{1i}^{2})=\\mathbb{E}[X_{1i}^{4}] - \\left(\\mathbb{E}[X_{1i}^{2}]\\right)^{2} = 3\\,\\Sigma_{ii}^{2} - \\Sigma_{ii}^{2} = 2\\,\\Sigma_{ii}^{2}.\n$$\nSubstituting back gives\n$$\n\\operatorname{Var}(W_{ii}) = n \\cdot 2\\,\\Sigma_{ii}^{2} = 2 n \\Sigma_{ii}^{2}.\n$$\nThis result is also consistent with the general covariance formula for Wishart entries, $\\operatorname{Cov}(W_{ij},W_{kl})=n(\\Sigma_{ik}\\Sigma_{jl}+\\Sigma_{il}\\Sigma_{jk})$, which for $(i,j,k,l)=(i,i,i,i)$ reduces to $2 n \\Sigma_{ii}^{2}$.", "answer": "$$\\boxed{2 n \\Sigma_{ii}^{2}}$$", "id": "1967831"}, {"introduction": "Our final practice transitions towards a key application of the Wishart distribution in modern statistical modeling, particularly within the Bayesian framework. In many scenarios, the inverse of the covariance matrix, known as the precision matrix, is the parameter of interest. This problem guides you to compute the expected value of the inverse of a Wishart matrix, a result that forms the basis of the Inverse-Wishart distribution and is crucial for Bayesian inference on covariance structures [@problem_id:1967861].", "problem": "In Bayesian analysis of multivariate data, the inverse of the covariance matrix, known as the precision matrix, often plays a central role. A fundamental tool for modeling covariance structures is the Wishart distribution.\n\nConsider a set of $n$ independent and identically distributed random vectors $X_1, X_2, \\dots, X_n$, each drawn from a $p$-variate normal distribution with a mean vector of zero and a positive definite covariance matrix $\\Sigma$, denoted as $X_i \\sim N_p(0, \\Sigma)$. The scatter matrix, defined as $W = \\sum_{i=1}^{n} X_i X_i^T$, follows a Wishart distribution with $n$ degrees of freedom and scale matrix $\\Sigma$, denoted as $W \\sim W_p(n, \\Sigma)$.\n\nFor the Wishart matrix $W$ to be invertible with probability one, the degrees of freedom $n$ must be greater than or equal to the dimension $p$. For the expected value of its inverse to exist, a stricter condition, $n > p+1$, is required.\n\nAssuming that $n > p+1$, compute the expected value of the inverse Wishart matrix, $E[W^{-1}]$. Your answer should be a closed-form analytic expression in terms of $n$, $p$, and the scale matrix $\\Sigma$.", "solution": "Let $X_{i} \\sim N_{p}(0,\\Sigma)$ i.i.d. with $\\Sigma$ positive definite. The scatter matrix $W=\\sum_{i=1}^{n} X_{i}X_{i}^{T}$ has the Wishart distribution $W \\sim W_{p}(n,\\Sigma)$, whose density on the cone of $p \\times p$ symmetric positive definite matrices is\n$$\nf_{W}(W)=\\frac{1}{2^{np/2}|\\Sigma|^{n/2}\\Gamma_{p}\\!\\left(\\frac{n}{2}\\right)}\\,|W|^{(n-p-1)/2}\\exp\\!\\left(-\\frac{1}{2}\\operatorname{tr}(\\Sigma^{-1}W)\\right),\n$$\nwhere $\\Gamma_{p}(\\cdot)$ is the multivariate gamma function and $n \\geq p$ ensures invertibility with probability one.\n\nDefine the matrix inverse transformation $S=W^{-1}$. The Jacobian of matrix inversion on the symmetric cone is\n$$\ndW=|S|^{-(p+1)}\\,dS.\n$$\nSubstituting $W=S^{-1}$ into $f_{W}(W)$ yields the density of $S$:\n$$\nf_{S}(S)=\\frac{|\\Sigma|^{-n/2}}{2^{np/2}\\Gamma_{p}\\!\\left(\\frac{n}{2}\\right)}\\,|S^{-1}|^{(n-p-1)/2}\\exp\\!\\left(-\\frac{1}{2}\\operatorname{tr}(\\Sigma^{-1}S^{-1})\\right)\\,|S|^{-(p+1)}.\n$$\nUsing $|S^{-1}|=|S|^{-1}$, this simplifies to\n$$\nf_{S}(S)=\\frac{|\\Sigma|^{-n/2}}{2^{np/2}\\Gamma_{p}\\!\\left(\\frac{n}{2}\\right)}\\,|S|^{-(n+p+1)/2}\\exp\\!\\left(-\\frac{1}{2}\\operatorname{tr}(\\Sigma^{-1}S^{-1})\\right).\n$$\nThis is the inverse Wishart density with degrees of freedom $n$ and scale matrix $\\Sigma^{-1}$, denoted $S \\sim IW_{p}(n,\\Sigma^{-1})$.\n\nA standard moment formula for the inverse Wishart distribution states that if $S \\sim IW_{p}(\\nu,\\Psi)$ with $\\nu>p+1$, then\n$$\nE[S]=\\frac{\\Psi}{\\nu-p-1}.\n$$\nThis follows by evaluating the first moment integral using the normalization constant of the inverse Wishart and the matrix-variate gamma function, valid precisely under the stated condition $\\nu>p+1$ for finiteness of the expectation.\n\nApplying this with $\\nu=n$ and $\\Psi=\\Sigma^{-1}$ gives, for $n>p+1$,\n$$\nE[W^{-1}]=E[S]=\\frac{\\Sigma^{-1}}{n-p-1}.\n$$\nThis is the required closed-form expression in terms of $n$, $p$, and $\\Sigma$.", "answer": "$$\\boxed{\\frac{1}{n - p - 1}\\,\\Sigma^{-1}}$$", "id": "1967861"}]}