{"hands_on_practices": [{"introduction": "This first exercise provides a fundamental starting point for understanding risk functions. We will calculate the risk for a simple, yet non-standard, estimator of a Bernoulli proportion under the ubiquitous squared error loss. This practice [@problem_id:1952134] will reinforce the mechanics of risk calculation and is an excellent opportunity to apply the powerful bias-variance decomposition, a cornerstone of estimator evaluation.", "problem": "Consider two random variables, $X_1$ and $X_2$, which are independent and identically distributed (i.i.d.) according to a Bernoulli distribution with an unknown parameter $p$, where $p \\in (0, 1)$. The probability mass function is $P(X=k) = p^k(1-p)^{1-k}$ for $k \\in \\{0, 1\\}$.\n\nAn analyst proposes an estimator, $\\hat{p}$, for the parameter $p$ based on these two observations, defined as:\n$$\n\\hat{p} = \\frac{X_1 + 2X_2}{3}\n$$\nThe quality of this estimator is to be assessed using the squared error loss function, which is defined as $L(p, \\hat{p}) = (p - \\hat{p})^2$.\n\nYour task is to calculate the risk function, $R(p, \\hat{p})$, for this estimator. The risk function is defined as the expected value of the loss function. Express your answer as a function of the parameter $p$.", "solution": "We need the risk under squared error loss:\n$$\nR(p,\\hat{p})=\\mathbb{E}\\big[(\\hat{p}-p)^{2}\\big].\n$$\nFor squared error, the risk equals the mean squared error:\n$$\nR(p,\\hat{p})=\\operatorname{Var}(\\hat{p})+\\big(\\mathbb{E}[\\hat{p}]-p\\big)^{2}.\n$$\nGiven $\\hat{p}=(X_{1}+2X_{2})/3$ with $X_{1}$ and $X_{2}$ i.i.d. $\\operatorname{Bernoulli}(p)$, we use $\\mathbb{E}[X_{i}]=p$ and $\\operatorname{Var}(X_{i})=p(1-p)$ for $i\\in\\{1,2\\}$, and independence implies $\\operatorname{Cov}(X_{1},X_{2})=0$.\n\nFirst compute the expectation:\n$$\n\\mathbb{E}[\\hat{p}]=\\mathbb{E}\\left[\\frac{X_{1}+2X_{2}}{3}\\right]=\\frac{\\mathbb{E}[X_{1}]+2\\mathbb{E}[X_{2}]}{3}=\\frac{p+2p}{3}=p,\n$$\nso the bias is zero:\n$$\n\\big(\\mathbb{E}[\\hat{p}]-p\\big)^{2}=0.\n$$\n\nNext compute the variance:\n$$\n\\operatorname{Var}(\\hat{p})=\\operatorname{Var}\\left(\\frac{X_{1}+2X_{2}}{3}\\right)=\\frac{1}{9}\\operatorname{Var}(X_{1}+2X_{2}).\n$$\nUsing $\\operatorname{Var}(X_{1}+2X_{2})=\\operatorname{Var}(X_{1})+4\\operatorname{Var}(X_{2})+4\\operatorname{Cov}(X_{1},X_{2})$ and $\\operatorname{Cov}(X_{1},X_{2})=0$,\n$$\n\\operatorname{Var}(X_{1}+2X_{2})=\\operatorname{Var}(X_{1})+4\\operatorname{Var}(X_{2})=p(1-p)+4p(1-p)=5p(1-p).\n$$\nTherefore,\n$$\n\\operatorname{Var}(\\hat{p})=\\frac{1}{9}\\cdot 5p(1-p)=\\frac{5}{9}p(1-p).\n$$\n\nCombining these results,\n$$\nR(p,\\hat{p})=\\operatorname{Var}(\\hat{p})+\\big(\\mathbb{E}[\\hat{p}]-p\\big)^{2}=\\frac{5}{9}p(1-p).\n$$", "answer": "$$\\boxed{\\frac{5}{9}p(1-p)}$$", "id": "1952134"}, {"introduction": "Moving beyond simple calculation, this practice delves into the core purpose of risk functions: comparing different estimators. We stage a compelling contest between the intuitive sample mean and a surprisingly simple constant estimator. By finding the conditions under which the constant estimator is superior [@problem_id:1952180], you will uncover the critical insight that no single estimator is universally best, and that its performance is a function of the true parameter it seeks to estimate.", "problem": "A data scientist is tasked with estimating a physical constant $\\mu$. It is known that measurements of this constant follow a normal distribution with mean $\\mu$ and a known variance of 1. A new experiment is conducted, yielding a random sample of $n$ measurements, denoted by $\\mathbf{X} = (X_1, X_2, \\ldots, X_n)$.\n\nThe data scientist considers two different estimators for $\\mu$:\n1. The sample mean estimator, $\\delta_{\\text{mean}}(\\mathbf{X}) = \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$.\n2. A constant estimator, $\\delta_{\\text{const}}(\\mathbf{X}) = c$, where $c$ is a pre-determined constant value based on a theoretical prediction.\n\nThe performance of an estimator $\\delta$ is evaluated using its risk function under squared error loss, defined as $R(\\mu, \\delta) = \\mathbb{E}[(\\delta(\\mathbf{X}) - \\mu)^2]$, where the expectation is taken with respect to the distribution of the sample $\\mathbf{X}$.\n\nDetermine the range of values for the true parameter $\\mu$, expressed in terms of $c$ and $n$, for which the constant estimator $\\delta_{\\text{const}}$ has a strictly smaller risk than the sample mean estimator $\\delta_{\\text{mean}}$. The final answer should be a row matrix containing two elements: the lower bound and the upper bound of the open interval for $\\mu$, in that order.", "solution": "Let $X_{1},\\ldots,X_{n}$ be independent with $X_{i} \\sim \\mathcal{N}(\\mu,1)$. The risk under squared error loss is $R(\\mu,\\delta)=\\mathbb{E}\\big[(\\delta(\\mathbf{X})-\\mu)^{2}\\big]$. For any estimator, the bias-variance decomposition gives\n$$\nR(\\mu,\\delta)=\\operatorname{Var}(\\delta(\\mathbf{X}))+\\big(\\mathbb{E}[\\delta(\\mathbf{X})]-\\mu\\big)^{2}.\n$$\n\nFor the sample mean $\\delta_{\\text{mean}}(\\mathbf{X})=\\bar{X}$, we have $\\bar{X} \\sim \\mathcal{N}\\!\\left(\\mu,\\frac{1}{n}\\right)$, so\n$$\n\\mathbb{E}[\\bar{X}]=\\mu,\\qquad \\operatorname{Var}(\\bar{X})=\\frac{1}{n}.\n$$\nHence\n$$\nR(\\mu,\\delta_{\\text{mean}})=\\operatorname{Var}(\\bar{X})+\\big(\\mathbb{E}[\\bar{X}]-\\mu\\big)^{2}=\\frac{1}{n}+0=\\frac{1}{n}.\n$$\n\nFor the constant estimator $\\delta_{\\text{const}}(\\mathbf{X})=c$, we have $\\operatorname{Var}(c)=0$ and $\\mathbb{E}[c]=c$, so\n$$\nR(\\mu,\\delta_{\\text{const}})=0+(c-\\mu)^{2}=(c-\\mu)^{2}.\n$$\n\nThe constant estimator has strictly smaller risk than the sample mean when\n$$\n(c-\\mu)^{2}<\\frac{1}{n}.\n$$\nTaking square roots yields\n$$\n|c-\\mu|<\\frac{1}{\\sqrt{n}},\n$$\nwhich is equivalent to\n$$\nc-\\frac{1}{\\sqrt{n}}<\\mu<c+\\frac{1}{\\sqrt{n}}.\n$$\nThus, the desired open interval for $\\mu$ is $\\left(c-\\frac{1}{\\sqrt{n}},\\,c+\\frac{1}{\\sqrt{n}}\\right)$, whose lower and upper bounds are $c-\\frac{1}{\\sqrt{n}}$ and $c+\\frac{1}{\\sqrt{n}}$, respectively.", "answer": "$$\\boxed{\\begin{pmatrix} c-\\frac{1}{\\sqrt{n}} & c+\\frac{1}{\\sqrt{n}} \\end{pmatrix}}$$", "id": "1952180"}, {"introduction": "Our final practice advances into a more realistic and complex scenario, mirroring challenges in fields like industrial quality control or a medical trial. Here, we analyze an estimation problem where collecting data incurs a cost, which must be factored into our decision-making. This problem [@problem_id:1952186] requires you to derive the risk under a loss function that combines both estimation error and sampling cost, demonstrating how the risk framework provides a principled way to manage the trade-off between accuracy and resources.", "problem": "In the field of quality control for nanomaterials, a company produces quantum dots in a process where each dot is independently either functional (a \"success\") with a constant but unknown probability $p$, or defective (a \"failure\"). To estimate the process efficiency, a sequential testing protocol is implemented: dots are tested one by one until a predetermined number, $r$, of functional dots have been identified. The total number of dots tested in this procedure is denoted by the random variable $N$. It is a known result from probability theory that $N$ follows a negative binomial distribution, and for the specific parameterization where $N$ is the trial number of the $r$-th success, its expectation is $\\mathbb{E}[N] = r/p$ and its variance is $\\operatorname{Var}(N) = r(1-p)/p^2$.\n\nThe parameter of interest for the company is the expected number of tests required to find a single functional dot, defined as $\\theta = 1/p$. A natural estimator for this parameter is proposed: $\\hat{\\theta} = N/r$.\n\nThe company's decision-making is guided by a loss function that balances estimation accuracy with testing cost. The loss incurred for an estimate $\\hat{\\theta}$ when the true parameter is $\\theta$ is given by the sum of the squared estimation error and a cost proportional to the total number of tests conducted:\n$$L(\\theta, \\hat{\\theta}) = (\\hat{\\theta} - \\theta)^2 + cN$$\nwhere $c$ is a positive constant representing the cost per test.\n\nYour task is to derive the statistical risk of using the estimator $\\hat{\\theta}$ under this loss function. The risk, $R(p)$, is defined as the expectation of the loss function, $R(p) = \\mathbb{E}[L(\\theta, \\hat{\\theta})]$. Express your final answer as a function of the true success probability $p$, the required number of successes $r$, and the cost coefficient $c$.", "solution": "We are given $N \\sim \\operatorname{NegBin}(r,p)$ as the trial count to the $r$-th success, with $\\mathbb{E}[N] = \\frac{r}{p}$ and $\\operatorname{Var}(N) = \\frac{r(1-p)}{p^{2}}$. The parameter of interest is $\\theta = \\frac{1}{p}$ and the estimator is $\\hat{\\theta} = \\frac{N}{r}$. The loss is\n$$\nL(\\theta,\\hat{\\theta}) = \\left(\\frac{N}{r} - \\frac{1}{p}\\right)^{2} + cN.\n$$\nThe risk as a function of $p$ is\n$$\nR(p) = \\mathbb{E}\\left[\\left(\\frac{N}{r} - \\frac{1}{p}\\right)^{2}\\right] + c\\,\\mathbb{E}[N].\n$$\nUsing the identity $\\mathbb{E}[(X-a)^{2}] = \\operatorname{Var}(X) + (\\mathbb{E}[X]-a)^{2}$ with $X = \\frac{N}{r}$ and $a = \\frac{1}{p}$, we obtain\n$$\n\\mathbb{E}\\left[\\left(\\frac{N}{r} - \\frac{1}{p}\\right)^{2}\\right] = \\operatorname{Var}\\left(\\frac{N}{r}\\right) + \\left(\\mathbb{E}\\left[\\frac{N}{r}\\right] - \\frac{1}{p}\\right)^{2}.\n$$\nCompute the mean and variance terms:\n$$\n\\mathbb{E}\\left[\\frac{N}{r}\\right] = \\frac{1}{r}\\mathbb{E}[N] = \\frac{1}{r}\\cdot \\frac{r}{p} = \\frac{1}{p},\n$$\nso the estimator is unbiased and the squared bias term is zero. Also,\n$$\n\\operatorname{Var}\\left(\\frac{N}{r}\\right) = \\frac{1}{r^{2}}\\operatorname{Var}(N) = \\frac{1}{r^{2}}\\cdot \\frac{r(1-p)}{p^{2}} = \\frac{1-p}{r p^{2}}.\n$$\nTherefore,\n$$\nR(p) = \\frac{1-p}{r p^{2}} + c\\,\\mathbb{E}[N] = \\frac{1-p}{r p^{2}} + c\\,\\frac{r}{p}.\n$$\nThis expresses the risk in terms of $p$, $r$, and $c$.", "answer": "$$\\boxed{\\frac{1-p}{r p^{2}}+\\frac{c r}{p}}$$", "id": "1952186"}]}