{"hands_on_practices": [{"introduction": "Before we can interpret PACF plots to identify time series models, it is crucial to understand the mechanics of their calculation. This first exercise provides direct practice with the Durbin-Levinson recursion, the mathematical engine that connects the values of the Autocorrelation Function (ACF) to the Partial Autocorrelation Function (PACF). Mastering this fundamental calculation is the first step toward building a solid foundation in time series analysis [@problem_id:1943287].", "problem": "In the analysis of a stationary time series, the Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PACF) are fundamental tools for identifying the underlying structure of the data. For a given stationary process, let $\\rho(k)$ denote the theoretical ACF at lag $k$, which measures the correlation between observations $k$ time steps apart. Let $\\phi_{kk}$ denote the theoretical PACF at lag $k$, which measures the correlation between observations $k$ time steps apart after removing the linear effects of the intermediate observations.\n\nSuppose a stationary time series has theoretical ACF values given by $\\rho(1) = 0.8$ and $\\rho(2) = 0.5$. Calculate the value of its theoretical PACF at lag 2, denoted by $\\phi_{22}$. Round your final answer to four significant figures.", "solution": "We use the Durbin–Levinson recursion relating the theoretical ACF $\\rho(k)$ and the PACF $\\phi_{kk}$. For $k=1$,\n$$\n\\phi_{11}=\\rho(1).\n$$\nFor $k=2$, the recursion gives\n$$\n\\phi_{22}=\\frac{\\rho(2)-\\phi_{11}\\rho(1)}{1-\\phi_{11}\\rho(1)}.\n$$\nSubstituting $\\phi_{11}=\\rho(1)$ yields\n$$\n\\phi_{22}=\\frac{\\rho(2)-\\rho(1)^{2}}{1-\\rho(1)^{2}}.\n$$\nWith the given values $\\rho(1)=0.8$ and $\\rho(2)=0.5$,\n$$\n\\phi_{22}=\\frac{0.5-0.8^{2}}{1-0.8^{2}}=\\frac{0.5-0.64}{1-0.64}=\\frac{-0.14}{0.36}=-\\frac{7}{18}\\approx -0.388888\\ldots\n$$\nRounding to four significant figures gives $-0.3889$.", "answer": "$$\\boxed{-0.3889}$$", "id": "1943287"}, {"introduction": "One of the primary applications of the PACF is identifying the order of an Autoregressive (AR) process. A key theoretical result states that for an AR($p$) model, the PACF will be non-zero for lags up to $p$ and will abruptly cut off to zero for all lags greater than $p$. This practice problem demonstrates this signature property for an AR(2) model, showing how the PACF value at lag 2, $\\phi_{22}$, directly corresponds to the model's second coefficient [@problem_id:1943263].", "problem": "Consider a stationary Autoregressive (AR) model of order 2, denoted AR(2), which is defined by the following equation for a time series $\\{X_t\\}$:\n$$X_t = 0.5 X_{t-1} - 0.3 X_{t-2} + \\epsilon_t$$\nHere, $\\{\\epsilon_t\\}$ is a white noise process, meaning its elements are independent and identically distributed random variables with a mean of zero and a constant variance $\\sigma^2_\\epsilon$.\n\nDetermine the exact value of the theoretical Partial Autocorrelation Function (PACF) at lag 2 for this process. The PACF at lag $k$ is commonly denoted by $\\phi_{kk}$. You are asked to find the value of $\\phi_{22}$.", "solution": "We rewrite the given process in standard AR(2) form:\n$$\nX_{t}=\\phi_{1}X_{t-1}+\\phi_{2}X_{t-2}+\\epsilon_{t},\n$$\nwith $\\phi_{1}=0.5$ and $\\phi_{2}=-0.3$, and $\\{\\epsilon_{t}\\}$ white noise with variance $\\sigma_{\\epsilon}^{2}$.\n\nThe theoretical partial autocorrelation at lag $2$, denoted $\\phi_{22}$, is defined as the coefficient on $X_{t-2}$ in the best linear predictor of $X_{t}$ based on $X_{t-1}$ and $X_{t-2}$. That is, $\\phi_{22}$ equals the value $\\beta_{2}$ in the linear projection\n$$\nX_{t}=\\beta_{1}X_{t-1}+\\beta_{2}X_{t-2}+u_{t},\n$$\nthat minimizes $E[u_{t}^{2}]$. The normal equations for this projection are\n$$\nE\\big[X_{t-1}(X_{t}-\\beta_{1}X_{t-1}-\\beta_{2}X_{t-2})\\big]=0,\\quad E\\big[X_{t-2}(X_{t}-\\beta_{1}X_{t-1}-\\beta_{2}X_{t-2})\\big]=0,\n$$\nwhich, writing $\\gamma(h)=\\operatorname{Cov}(X_{t},X_{t-h})$, become\n$$\n\\gamma(1)=\\beta_{1}\\gamma(0)+\\beta_{2}\\gamma(1),\\qquad \\gamma(2)=\\beta_{1}\\gamma(1)+\\beta_{2}\\gamma(0).\n$$\nIn matrix form,\n$$\n\\begin{pmatrix}\n\\gamma(0) & \\gamma(1)\\\\\n\\gamma(1) & \\gamma(0)\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_{1}\\\\\n\\beta_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\gamma(1)\\\\\n\\gamma(2)\n\\end{pmatrix}.\n$$\n\nFor an AR(2), the Yule–Walker equations state for $h=1,2$:\n$$\n\\gamma(1)=\\phi_{1}\\gamma(0)+\\phi_{2}\\gamma(1),\\qquad \\gamma(2)=\\phi_{1}\\gamma(1)+\\phi_{2}\\gamma(0).\n$$\nThese are identical to the normal equations with $(\\beta_{1},\\beta_{2})$ replaced by $(\\phi_{1},\\phi_{2})$. The autocovariance matrix is positive definite under stationarity, hence invertible, so the solution to the normal equations is unique. Therefore,\n$$\n\\beta_{1}=\\phi_{1},\\qquad \\beta_{2}=\\phi_{2}.\n$$\nBy the definition of the PACF at lag $2$, $\\phi_{22}=\\beta_{2}=\\phi_{2}$. Substituting the given coefficient yields\n$$\n\\phi_{22}=-0.3.\n$$", "answer": "$$\\boxed{-0.3}$$", "id": "1943263"}, {"introduction": "In contrast to Autoregressive models, Moving Average (MA) models exhibit a different signature in their PACF. While the ACF of an MA($q$) process cuts off after lag $q$, its PACF typically decays or \"tails off\" toward zero. This exercise illustrates this dual relationship by guiding you through the derivation of the theoretical PACF for an MA(1) process, which helps in learning to distinguish between AR and MA structures when analyzing time series data [@problem_id:1943283].", "problem": "In a quality control analysis, the daily deviation from the target weight of a manufactured product is modeled as a stationary time series $\\{X_t\\}$. An analyst proposes that this series can be described by a first-order Moving Average, MA(1), model:\n$$ X_t = \\epsilon_t + \\theta \\epsilon_{t-1} $$\nwhere $\\{\\epsilon_t\\}$ is a white noise process (a sequence of uncorrelated random variables with zero mean and constant variance $\\sigma^2_\\epsilon$), and $\\theta$ is the model parameter with $|\\theta|<1$.\n\nThe Autocorrelation Function (ACF), $\\rho(k)$, and the Partial Autocorrelation Function (PACF), $\\phi_{kk}$, are key tools for identifying and validating time series models. For the proposed MA(1) process, it is known that the ACF at lag $k=1$ is $\\rho(1) = \\frac{\\theta}{1+\\theta^2}$, and the ACF at all subsequent lags is zero, i.e., $\\rho(k)=0$ for all integers $k \\ge 2$.\n\nBased on this information, determine the theoretical value of the PACF at lag 2, denoted $\\phi_{22}$, for this MA(1) process. Express your answer as a single symbolic expression in terms of the parameter $\\theta$.", "solution": "For a stationary process with autocorrelation function $\\rho(k)$, the partial autocorrelation coefficients $\\{\\phi_{kk}\\}$ can be obtained via the Durbin–Levinson recursion. Specifically,\n$$\n\\phi_{11}=\\rho(1), \\quad \\text{and for } k\\ge 2,\\quad \\phi_{kk}=\\frac{\\rho(k)-\\sum_{j=1}^{k-1}\\phi_{k-1,j}\\,\\rho(k-j)}{1-\\sum_{j=1}^{k-1}\\phi_{k-1,j}\\,\\rho(j)}.\n$$\nFor the given MA(1), $\\rho(1)=\\frac{\\theta}{1+\\theta^{2}}$ and $\\rho(k)=0$ for all integers $k\\ge 2$. Applying the recursion for $k=2$, we use $\\phi_{11}=\\rho(1)$ and obtain\n$$\n\\phi_{22}=\\frac{\\rho(2)-\\phi_{11}\\rho(1)}{1-\\phi_{11}\\rho(1)}=\\frac{0-\\rho(1)^{2}}{1-\\rho(1)^{2}}=-\\frac{\\rho(1)^{2}}{1-\\rho(1)^{2}}.\n$$\nSubstituting $\\rho(1)=\\frac{\\theta}{1+\\theta^{2}}$ gives\n$$\n\\rho(1)^{2}=\\frac{\\theta^{2}}{(1+\\theta^{2})^{2}}, \\quad 1-\\rho(1)^{2}=\\frac{(1+\\theta^{2})^{2}-\\theta^{2}}{(1+\\theta^{2})^{2}}=\\frac{1+\\theta^{2}+\\theta^{4}}{(1+\\theta^{2})^{2}}.\n$$\nTherefore,\n$$\n\\phi_{22}=-\\frac{\\dfrac{\\theta^{2}}{(1+\\theta^{2})^{2}}}{\\dfrac{1+\\theta^{2}+\\theta^{4}}{(1+\\theta^{2})^{2}}}=-\\frac{\\theta^{2}}{1+\\theta^{2}+\\theta^{4}}.\n$$", "answer": "$$\\boxed{-\\frac{\\theta^{2}}{1+\\theta^{2}+\\theta^{4}}}$$", "id": "1943283"}]}