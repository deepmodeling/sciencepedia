{"hands_on_practices": [{"introduction": "Before exploring the deeper implications of Bayes risk, it's crucial to master the fundamental calculation. This first practice problem provides a complete, step-by-step walkthrough in a common scenario: estimating the rate parameter of an exponential distribution. By working through this example [@problem_id:1898422], you will see how the prior belief (a Gamma distribution) and the observed data (from an Exponential distribution) combine to determine the expected uncertainty, which is precisely what the Bayes risk quantifies under squared error loss.", "problem": "In reliability engineering, the lifetime of an electronic component is often modeled by an Exponential distribution. Suppose the time to failure, $X$, of a new type of component follows an Exponential distribution with an unknown rate parameter $\\lambda > 0$. The probability density function of the lifetime is given by $f(x|\\lambda) = \\lambda \\exp(-\\lambda x)$ for $x > 0$.\n\nFrom past experience with similar components, we have prior beliefs about the failure rate $\\lambda$. We model this uncertainty using a Gamma prior distribution with a known shape parameter $\\alpha > 0$ and a known rate parameter $\\beta > 0$. The prior probability density function for $\\lambda$ is given by $\\pi(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda)$ for $\\lambda > 0$.\n\nWe will make a single observation of a component's lifetime, $X=x$, and use it to estimate $\\lambda$. The quality of our estimate, $\\hat{\\lambda}$, is evaluated using a squared error loss function, defined as $L(\\lambda, \\hat{\\lambda}) = (\\lambda - \\hat{\\lambda})^2$.\n\nYour task is to calculate the Bayes risk associated with estimating $\\lambda$ under these conditions. The Bayes risk is the minimum possible expected loss, averaged over both the data distribution and the prior distribution of the parameter.\n\nExpress your answer as a closed-form analytic expression in terms of the prior parameters $\\alpha$ and $\\beta$.", "solution": "We are given a single observation $X=x$ from an Exponential distribution with rate $\\lambda$, with prior $\\lambda \\sim \\text{Gamma}(\\alpha,\\beta)$ under the rate parameterization. The loss is squared error, $L(\\lambda,\\hat{\\lambda})=(\\lambda-\\hat{\\lambda})^{2}$. Under squared error loss, the Bayes estimator is the posterior mean, and the Bayes risk equals the expected posterior variance:\n$$\n\\mathcal{R}_{B}=\\mathbb{E}_{X}\\!\\left[\\operatorname{Var}(\\lambda\\mid X)\\right].\n$$\n\nFirst, compute the posterior. The likelihood is $f(x\\mid \\lambda)=\\lambda \\exp(-\\lambda x)$ for $x0$, and the prior is $\\pi(\\lambda)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\beta \\lambda)$ for $\\lambda0$. By conjugacy, the posterior is\n$$\n\\lambda \\mid x \\sim \\text{Gamma}(\\alpha+1,\\beta+x),\n$$\nso its variance is\n$$\n\\operatorname{Var}(\\lambda\\mid x)=\\frac{\\alpha+1}{(\\beta+x)^{2}}.\n$$\nHence,\n$$\n\\mathcal{R}_{B}=\\mathbb{E}_{X}\\!\\left[\\frac{\\alpha+1}{(\\beta+X)^{2}}\\right].\n$$\n\nNext, compute the prior predictive (marginal) density of $X$:\n$$\nf_{X}(x)=\\int_{0}^{\\infty} \\lambda \\exp(-\\lambda x)\\,\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\beta \\lambda)\\,d\\lambda\n=\\frac{\\beta^{\\alpha}\\Gamma(\\alpha+1)}{\\Gamma(\\alpha)}\\frac{1}{(\\beta+x)^{\\alpha+1}}\n=\\frac{\\alpha \\beta^{\\alpha}}{(\\beta+x)^{\\alpha+1}},\n$$\nfor $x0$. Therefore,\n$$\n\\mathcal{R}_{B}=(\\alpha+1)\\int_{0}^{\\infty}\\frac{1}{(\\beta+x)^{2}}\\cdot \\frac{\\alpha \\beta^{\\alpha}}{(\\beta+x)^{\\alpha+1}}\\,dx\n=\\alpha(\\alpha+1)\\beta^{\\alpha}\\int_{0}^{\\infty}(\\beta+x)^{-(\\alpha+3)}\\,dx.\n$$\nEvaluate the integral using $u=\\beta+x$:\n$$\n\\int_{0}^{\\infty}(\\beta+x)^{-(\\alpha+3)}\\,dx=\\int_{\\beta}^{\\infty}u^{-(\\alpha+3)}\\,du=\\frac{\\beta^{-(\\alpha+2)}}{\\alpha+2}.\n$$\nThus,\n$$\n\\mathcal{R}_{B}=\\alpha(\\alpha+1)\\beta^{\\alpha}\\cdot \\frac{\\beta^{-(\\alpha+2)}}{\\alpha+2}=\\frac{\\alpha(\\alpha+1)}{\\alpha+2}\\cdot \\frac{1}{\\beta^{2}}.\n$$\n\nTherefore, the Bayes risk under squared error loss with one Exponential observation and a $\\text{Gamma}(\\alpha,\\beta)$ prior is $\\frac{\\alpha(\\alpha+1)}{(\\alpha+2)\\beta^{2}}$.", "answer": "$$\\boxed{\\frac{\\alpha(\\alpha+1)}{(\\alpha+2)\\beta^{2}}}$$", "id": "1898422"}, {"introduction": "Having established the computational procedure, we can now explore the conceptual behavior of Bayes risk. A key question in Bayesian analysis is the influence of the prior distribution. This exercise [@problem_id:1898419] investigates a fascinating limiting case: What happens to our minimum expected loss when our prior beliefs become increasingly vague? By examining the Bayes risk for a Normal mean as the prior variance, $\\tau^2$, tends to infinity, you'll uncover a fundamental connection between Bayesian risk and the inherent variability of the data itself.", "problem": "Consider a statistical estimation problem where we want to estimate an unknown parameter $\\mu$. Our prior belief about $\\mu$ is modeled by a normal distribution with a mean of 0 and a variance of $\\tau^2$, where $\\tau^2  0$. We denote this as $\\mu \\sim N(0, \\tau^2)$. We then observe a single data point $X$ from a normal distribution whose mean is the true value of $\\mu$ and whose variance is 1, which is denoted as $X | \\mu \\sim N(\\mu, 1)$.\n\nThe quality of an estimator, denoted $\\hat{\\mu}(X)$, is measured by the squared error loss function, $L(\\mu, \\hat{\\mu}) = (\\mu - \\hat{\\mu})^2$. The Bayes risk, which we will denote as $r(\\tau^2)$, is defined as the minimum possible expected loss, where the expectation is taken over the joint distribution of both $\\mu$ and $X$.\n\nDetermine the limiting value of the Bayes risk as the prior variance becomes infinitely large. That is, calculate the value of $\\lim_{\\tau^2 \\to \\infty} r(\\tau^2)$. Provide the final answer as a single numerical value.", "solution": "We are given a normal-normal model with prior $\\mu \\sim N(0,\\tau^{2})$ and likelihood $X \\mid \\mu \\sim N(\\mu,1)$. Under squared error loss $L(\\mu,\\hat{\\mu})=(\\mu-\\hat{\\mu})^{2}$, the Bayes estimator is the posterior mean:\n$$\n\\hat{\\mu}(X)=\\mathbb{E}[\\mu \\mid X].\n$$\nBy conjugacy, the posterior distribution is normal with precision equal to the sum of prior and likelihood precisions. Hence\n$$\n\\operatorname{Var}(\\mu \\mid X)=\\left(\\frac{1}{\\tau^{2}}+1\\right)^{-1}=\\frac{\\tau^{2}}{1+\\tau^{2}},\n$$\nand\n$$\n\\mathbb{E}[\\mu \\mid X]=\\operatorname{Var}(\\mu \\mid X)\\left(\\frac{0}{\\tau^{2}}+X\\right)=\\frac{\\tau^{2}}{1+\\tau^{2}}\\,X.\n$$\nThe Bayes risk is the minimum expected loss over estimators, which for squared error loss and the Bayes rule equals the expected posterior variance:\n$$\nr(\\tau^{2})=\\mathbb{E}\\big[(\\mu-\\hat{\\mu}(X))^{2}\\big]=\\mathbb{E}\\big[\\operatorname{Var}(\\mu \\mid X)\\big]=\\frac{\\tau^{2}}{1+\\tau^{2}},\n$$\nwhere the expectation is trivial because $\\operatorname{Var}(\\mu \\mid X)$ does not depend on $X$.\n\nTherefore, the limiting Bayes risk as $\\tau^{2}\\to\\infty$ is\n$$\n\\lim_{\\tau^{2}\\to\\infty} r(\\tau^{2})=\\lim_{\\tau^{2}\\to\\infty} \\frac{\\tau^{2}}{1+\\tau^{2}}=1.\n$$", "answer": "$$\\boxed{1}$$", "id": "1898419"}, {"introduction": "The power of the Bayes risk framework lies not only in finding the best possible estimator but also in evaluating the performance of any other decision rule. In many practical applications, estimators are constrained by system hardware or other requirements. This problem [@problem_id:1898413] puts you in the shoes of an engineer who must choose an estimate from a limited set of options, comparing this constrained approach to the optimal Bayes estimator. By calculating the increase in risk, you gain a concrete understanding of the optimality of the Bayes rule and a method for quantifying the cost of imposing practical constraints.", "problem": "An engineer is testing a manufacturing process that produces a specific type of semiconductor. The quality of each semiconductor can be modeled as an independent Bernoulli trial, where the probability of a semiconductor being non-defective is $p$. The engineer's prior belief about the value of $p$ is modeled by a Beta distribution with parameters $\\alpha=1$ and $\\beta=1$, i.e., $p \\sim \\text{Beta}(1, 1)$. To update this belief, the engineer tests a batch of $n=4$ semiconductors and observes the total number of non-defective items, $S$.\n\nThe engineer considers two different decision rules (estimators) for the parameter $p$, based on the observed data $S$. The performance of an estimator $\\delta$ is evaluated using the squared error loss function, $L(p, \\delta) = (p - \\delta)^2$.\n\nThe first estimator, $\\delta_B$, is the standard Bayes estimator for this model, which minimizes the posterior expected loss.\n\nThe second estimator, $\\delta_C$, is a constrained estimator. Due to system limitations, this estimator must choose its estimate for $p$ from the discrete set $D = \\{0, 0.25, 0.5, 0.75, 1\\}$. For a given number of successes $S=k$, the rule $\\delta_C$ selects the value from the set $D$ that minimizes the posterior expected squared error loss.\n\nThe overall performance of any estimator $\\delta$ is measured by its Bayes risk, $r(\\delta)$, which is the expected value of the loss function over both the prior distribution of $p$ and the sampling distribution of the data $S$.\n\nCalculate the difference in the Bayes risks of the two estimators, $r(\\delta_C) - r(\\delta_B)$. Express your answer as an exact fraction in simplest form.", "solution": "Let $p \\sim \\text{Beta}(\\alpha,\\beta)$ with $\\alpha=\\beta=1$ and $S \\mid p \\sim \\text{Binomial}(n,p)$ with $n=4$. Under squared error loss, the Bayes estimator is the posterior mean. Given $S=k$, the posterior is $\\text{Beta}(\\alpha+k,\\beta+n-k)=\\text{Beta}(k+1,5-k)$, so the Bayes estimator is\n$$\n\\delta_{B}(k)=\\mathbb{E}[p \\mid S=k]=\\frac{k+\\alpha}{\\alpha+\\beta+n}=\\frac{k+1}{6}.\n$$\nDenote $\\mu_{k}=\\delta_{B}(k)=\\frac{k+1}{6}$.\n\nFor any estimator $\\delta(S)$, the posterior expected squared loss decomposes as\n$$\n\\mathbb{E}\\big[(p-\\delta(S))^{2}\\mid S\\big]=\\operatorname{Var}(p\\mid S)+\\big(\\mathbb{E}[p\\mid S]-\\delta(S)\\big)^{2}\n=\\operatorname{Var}(p\\mid S)+\\big(\\mu_{S}-\\delta(S)\\big)^{2}.\n$$\nTherefore,\n$$\nr(\\delta_{C})-r(\\delta_{B})=\\mathbb{E}\\big[(\\mu_{S}-\\delta_{C}(S))^{2}\\big],\n$$\nwhere the expectation is over the prior predictive distribution of $S$.\n\nThe constrained estimator $\\delta_{C}(k)$ must lie in $D=\\{0,\\tfrac{1}{4},\\tfrac{1}{2},\\tfrac{3}{4},1\\}$ and minimizes the posterior expected loss, which by the quadratic form above is equivalent to choosing the element of $D$ closest to $\\mu_{k}$. Compute $\\mu_{k}$ for $k=0,1,2,3,4$:\n$$\n\\mu_{0}=\\frac{1}{6},\\quad \\mu_{1}=\\frac{1}{3},\\quad \\mu_{2}=\\frac{1}{2},\\quad \\mu_{3}=\\frac{2}{3},\\quad \\mu_{4}=\\frac{5}{6}.\n$$\nThe nearest elements in $D$ are\n$$\n\\delta_{C}(0)=\\frac{1}{4},\\quad \\delta_{C}(1)=\\frac{1}{4},\\quad \\delta_{C}(2)=\\frac{1}{2},\\quad \\delta_{C}(3)=\\frac{3}{4},\\quad \\delta_{C}(4)=\\frac{3}{4}.\n$$\nHence the squared deviations are\n$$\n(\\mu_{0}-\\delta_{C}(0))^{2}=\\left(\\frac{1}{6}-\\frac{1}{4}\\right)^{2}=\\frac{1}{144},\\quad\n(\\mu_{1}-\\delta_{C}(1))^{2}=\\left(\\frac{1}{3}-\\frac{1}{4}\\right)^{2}=\\frac{1}{144},\n$$\n$$\n(\\mu_{2}-\\delta_{C}(2))^{2}=0,\\quad\n(\\mu_{3}-\\delta_{C}(3))^{2}=\\left(\\frac{2}{3}-\\frac{3}{4}\\right)^{2}=\\frac{1}{144},\\quad\n(\\mu_{4}-\\delta_{C}(4))^{2}=\\left(\\frac{5}{6}-\\frac{3}{4}\\right)^{2}=\\frac{1}{144}.\n$$\n\nThe prior predictive distribution of $S$ under $\\text{Beta}(1,1)$ is\n$$\n\\mathbb{P}(S=k)=\\int_{0}^{1}\\binom{4}{k}p^{k}(1-p)^{4-k}\\,dp=\\binom{4}{k}B(k+1,5-k)=\\frac{1}{5},\n$$\nfor $k=0,1,2,3,4$, since $B(k+1,5-k)=\\frac{k!(4-k)!}{5!}$ and $\\binom{4}{k}\\frac{k!(4-k)!}{5!}=\\frac{1}{5}$.\n\nTherefore,\n$$\nr(\\delta_{C})-r(\\delta_{B})=\\sum_{k=0}^{4}\\mathbb{P}(S=k)\\,(\\mu_{k}-\\delta_{C}(k))^{2}\n=\\frac{1}{5}\\left(\\frac{1}{144}+\\frac{1}{144}+0+\\frac{1}{144}+\\frac{1}{144}\\right)\n=\\frac{4}{5}\\cdot\\frac{1}{144}\n=\\frac{1}{180}.\n$$", "answer": "$$\\boxed{\\frac{1}{180}}$$", "id": "1898413"}]}