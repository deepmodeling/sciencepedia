{"hands_on_practices": [{"introduction": "To truly grasp time series analysis, we must begin with its cornerstone concept: stationarity. This first exercise [@problem_id:1312104] challenges you to apply the formal definition of weak stationarity to a non-trivial case. By analyzing a process composed of a time-invariant random component and white noise, you will verify the three conditions—constant mean, constant variance, and time-invariant autocovariance—and solidify your understanding of what makes a process stable over time.", "problem": "An engineer is analyzing the output of a new type of micro-electromechanical system (MEMS) gyroscope. It is known that due to manufacturing imperfections, each individual gyroscope has a unique, constant-in-time angular rate bias. For a randomly selected gyroscope from a large production batch, this bias can be modeled as a random variable $M$. In addition to this static bias, the gyroscope's electronic components introduce a random noise at each measurement instant.\n\nThe total measured angular rate at a discrete time $t$ is modeled by the stochastic process $X_t = M + Z_t$, for $t \\in \\mathbb{Z}$. The properties of the components are as follows:\n\n-   The random bias $M$ is a single random variable drawn for each gyroscope with a mean $E[M] = 0$ and a finite, non-zero variance $Var(M) = \\sigma_M^2$.\n-   The electronic noise, $\\{Z_t\\}$, is a white noise process. This means it has a mean of $E[Z_t] = 0$ for all $t$, a constant variance $Var(Z_t) = \\sigma_Z^2$ for all $t$, and its values are uncorrelated over time, i.e., $Cov(Z_t, Z_s) = 0$ for any $t \\neq s$.\n-   The random bias $M$ and the entire noise process $\\{Z_t\\}$ are statistically independent of each other.\n\nBased on this model, which of the following statements about the weak stationarity of the process $\\{X_t\\}$ is correct?\n\nA. The process is not weakly stationary because its mean, $E[X_t]$, depends on the specific value of the random bias $M$ for a given gyroscope.\n\nB. The process is not weakly stationary because the variance, $Var(X_t)$, is not constant over time.\n\nC. The process is not weakly stationary because the autocovariance, $Cov(X_t, X_s)$ for $t \\neq s$, is always non-zero, which violates the condition that correlations must decay to zero.\n\nD. The process is weakly stationary because its mean and variance are constant over time, and its autocovariance function depends only on the time lag between observations.\n\nE. The stationarity of the process cannot be determined without knowing the specific probability distributions of $M$ and $Z_t$.", "solution": "We analyze the process defined by $X_{t}=M+Z_{t}$, where $M$ is a random variable with $E[M]=0$ and $Var(M)=\\sigma_{M}^{2}$, and $\\{Z_{t}\\}$ is a white noise process with $E[Z_{t}]=0$ for all $t$, $Var(Z_{t})=\\sigma_{Z}^{2}$ for all $t$, and $Cov(Z_{t},Z_{s})=0$ for $t\\neq s$. The process $M$ and the entire sequence $\\{Z_{t}\\}$ are independent.\n\nWeak stationarity requires: (i) a constant mean $E[X_{t}]$ independent of $t$, (ii) a constant and finite variance $Var(X_{t})$ independent of $t$, and (iii) an autocovariance $Cov(X_{t},X_{s})$ that depends only on the lag $h=t-s$.\n\nFirst, compute the mean using linearity of expectation:\n$$\nE[X_{t}]=E[M]+E[Z_{t}]=0+0=0,\n$$\nwhich is constant in $t$.\n\nSecond, compute the variance using $Var(A+B)=Var(A)+Var(B)+2\\,Cov(A,B)$ and the independence of $M$ and $Z_{t}$, which implies $Cov(M,Z_{t})=0$:\n$$\nVar(X_{t})=Var(M+Z_{t})=Var(M)+Var(Z_{t})=\\sigma_{M}^{2}+\\sigma_{Z}^{2},\n$$\nwhich is constant in $t$ and finite.\n\nThird, compute the autocovariance for arbitrary $t$ and $s$ using bilinearity of covariance:\n$$\nCov(X_{t},X_{s})=Cov(M+Z_{t},M+Z_{s})=Cov(M,M)+Cov(M,Z_{s})+Cov(Z_{t},M)+Cov(Z_{t},Z_{s}).\n$$\nIndependence of $M$ and $\\{Z_{t}\\}$ gives $Cov(M,Z_{s})=Cov(Z_{t},M)=0$. For $t\\neq s$, whiteness gives $Cov(Z_{t},Z_{s})=0$, hence\n$$\nCov(X_{t},X_{s})=\\sigma_{M}^{2}\\quad\\text{for }t\\neq s.\n$$\nFor $t=s$, we have\n$$\nCov(X_{t},X_{t})=Var(X_{t})=\\sigma_{M}^{2}+\\sigma_{Z}^{2}.\n$$\nDefine the lag $h=t-s$. Then the autocovariance function depends only on $h$, specifically\n$$\n\\gamma(h)=\\begin{cases}\n\\sigma_{M}^{2}+\\sigma_{Z}^{2}, & h=0,\\\\\n\\sigma_{M}^{2}, & h\\neq 0,\n\\end{cases}\n$$\nwhich satisfies the weak stationarity requirement.\n\nTherefore, the process $\\{X_{t}\\}$ is weakly stationary. Statement D is correct. Statements A, B, and C are incorrect: A confuses a random but time-invariant mean across realizations with time variation in $E[X_{t}]$ (the ensemble mean is $0$ for all $t$), B contradicts the constant variance shown above, and C imposes a non-requirement for weak stationarity (nonzero autocovariance at nonzero lags does not violate weak stationarity). Statement E is false because only second-order moments are required, which are already specified.", "answer": "$$\\boxed{D}$$", "id": "1312104"}, {"introduction": "A key task in time series analysis is identifying the underlying structure of a process from its data, and the autocorrelation function (ACF) is an indispensable tool for this purpose. This practice [@problem_id:1925260] provides direct experience in deriving the theoretical ACF from a given model, in this case a simple Moving Average (MA) process. Calculating how a model's parameters directly shape its correlation 'signature' is a foundational skill for both model selection and interpretation.", "problem": "In a signal processing application, a sensor takes a series of measurements, denoted by $\\{X_t\\}$, at discrete integer time steps $t$. The measured value at any given time $t$ is modeled as a sum of a primary random signal and a delayed, attenuated echo of a past signal. Specifically, the process is described by the equation:\n$$X_t = W_t + \\alpha W_{t-2}$$\nHere, $\\{W_t\\}$ represents a white noise process, which is a sequence of uncorrelated random variables, each with an expected value of 0 and a constant, finite variance of $\\sigma_W^2$. The parameter $\\alpha$ is a real-valued constant that represents the attenuation factor of the echo. For this particular sensor, the attenuation factor is given as $\\alpha = 0.5$.\n\nCalculate the theoretical autocorrelation of the process $\\{X_t\\}$ at lag 2.", "solution": "We are given a white noise process $\\{W_t\\}$ with $\\mathbb{E}[W_t]=0$, $\\operatorname{Var}(W_t)=\\sigma_{W}^{2}$, and $\\mathbb{E}[W_t W_s]=0$ for $t\\neq s$. The observed process is $X_t=W_t+\\alpha W_{t-2}$ with $\\alpha=0.5$.\n\nThe theoretical autocorrelation at lag $h$ is defined as $\\rho_{X}(h)=\\frac{\\gamma_{X}(h)}{\\gamma_{X}(0)}$, where $\\gamma_{X}(h)=\\mathbb{E}[X_t X_{t-h}]$ since the mean is zero.\n\nFirst, compute the autocovariance at lag $2$:\n$$\n\\gamma_{X}(2)=\\mathbb{E}[X_t X_{t-2}]\n=\\mathbb{E}\\big[(W_t+\\alpha W_{t-2})(W_{t-2}+\\alpha W_{t-4})\\big].\n$$\nExpanding and using the white noise properties,\n$$\n\\gamma_{X}(2)=\\mathbb{E}[W_t W_{t-2}]+\\alpha \\mathbb{E}[W_t W_{t-4}]+\\alpha \\mathbb{E}[W_{t-2} W_{t-2}]+\\alpha^{2}\\mathbb{E}[W_{t-2} W_{t-4}]\n=\\alpha \\sigma_{W}^{2},\n$$\nsince only the term with matching time indices survives.\n\nNext, compute the variance $\\gamma_{X}(0)=\\operatorname{Var}(X_t)$:\n$$\n\\gamma_{X}(0)=\\operatorname{Var}(W_t+\\alpha W_{t-2})\n=\\operatorname{Var}(W_t)+\\alpha^{2}\\operatorname{Var}(W_{t-2})+2\\operatorname{Cov}(W_t,\\alpha W_{t-2})\n=(1+\\alpha^{2})\\sigma_{W}^{2},\n$$\nbecause $\\operatorname{Cov}(W_t,W_{t-2})=0$.\n\nTherefore, the theoretical autocorrelation at lag $2$ is\n$$\n\\rho_{X}(2)=\\frac{\\gamma_{X}(2)}{\\gamma_{X}(0)}=\\frac{\\alpha \\sigma_{W}^{2}}{(1+\\alpha^{2})\\sigma_{W}^{2}}=\\frac{\\alpha}{1+\\alpha^{2}}.\n$$\nSubstituting $\\alpha=0.5$ gives\n$$\n\\rho_{X}(2)=\\frac{0.5}{1+0.25}=\\frac{2}{5}.\n$$", "answer": "$$\\boxed{\\frac{2}{5}}$$", "id": "1925260"}, {"introduction": "More complex models are not always better; the principle of parsimony encourages us to seek the simplest model that adequately describes the data. This thought-provoking problem [@problem_id:1312141] explores this idea through an ARMA(1,1) process that, under a specific condition, simplifies dramatically. By working through the concept of 'common factors', you will see how a seemingly complex model can be observationally equivalent to a much simpler one, a crucial lesson in efficient and effective modeling.", "problem": "Consider a weakly stationary Autoregressive-Moving-Average, or ARMA(1,1), process $\\{X_t\\}$ defined by the equation:\n$$X_t - \\phi X_{t-1} = Z_t + \\theta Z_{t-1}$$\nwhere $\\{Z_t\\}$ is a white noise process with mean zero and constant variance $\\sigma_Z^2$, and $t$ is an integer index representing time. The parameters $\\phi$ and $\\theta$ are real constants.\n\nIn a particular case, the moving-average parameter is set to be the negative of the autoregressive parameter, so that $\\theta = -\\phi$. The process is thus described by:\n$$X_t - \\phi X_{t-1} = Z_t - \\phi Z_{t-1}$$\n\nGiven that $|\\phi| \\ne 1$, determine the autocovariance function, $\\gamma_X(h) = \\text{Cov}(X_t, X_{t-h})$, for this process. Express your answer as a piecewise function of the integer lag $h$, in terms of $\\sigma_Z^2$.", "solution": "Start from the given ARMA(1,1) model with $\\theta=-\\phi$:\n$$(1-\\phi B)X_t=(1-\\phi B)Z_t,$$\nwhere $B$ is the backshift operator defined by $BX_t=X_{t-1}$ and $\\{Z_t\\}$ is white noise with mean zero and variance $\\sigma_Z^{2}$.\n\nDefine $Y_t=X_t-Z_t$. Subtracting the right-hand side from the left-hand side gives the homogeneous equation\n$$(1-\\phi B)Y_t=0.$$\nEquivalently, in the time domain,\n$$Y_t=\\phi Y_{t-1}\\quad\\text{for all }t.$$\n\nAssuming weak stationarity of $\\{Y_t\\}$ and $|\\phi|\\ne 1$, take variances on both sides to get\n$$\\operatorname{Var}(Y_t)=\\operatorname{Var}(\\phi Y_{t-1})=\\phi^{2}\\operatorname{Var}(Y_{t-1}).$$\nStationarity implies $\\operatorname{Var}(Y_t)=\\operatorname{Var}(Y_{t-1})=:v$, hence\n$$(1-\\phi^{2})v=0.$$\nGiven $|\\phi|\\ne 1$, it follows that $v=0$, so $Y_t=0$ almost surely for all $t$. Therefore,\n$$X_t=Z_t\\quad\\text{for all }t.$$\n\nUsing the definition of the autocovariance function and the white-noise property,\n$$\\gamma_X(h)=\\operatorname{Cov}(X_t,X_{t-h})=\\operatorname{Cov}(Z_t,Z_{t-h})=\\begin{cases}\n\\sigma_Z^{2}, & h=0,\\\\\n0, & h\\ne 0,\n\\end{cases}$$\nwhich is the desired piecewise form.", "answer": "$$\\boxed{\\gamma_X(h)=\\begin{cases}\\sigma_Z^{2}, & h=0,\\\\ 0, & h\\neq 0.\\end{cases}}$$", "id": "1312141"}]}