{"hands_on_practices": [{"introduction": "We begin our practical exploration with the two most fundamental building blocks of statistics: the mean and the variance. Understanding how these estimators react to individual data points is crucial for appreciating the need for robust methods. This exercise guides you through deriving and comparing the influence functions for the sample mean and sample variance, revealing their differing sensitivities to outliers and laying the groundwork for more advanced estimators [@problem_id:1923527].", "problem": "In robust statistics, the influence function is a crucial tool for analyzing the effect of a single observation on a statistical estimator. For a statistical functional $T$ and a target distribution with cumulative distribution function (CDF) $F$, the influence function at a point $x$ is defined as:\n$$ IF(x; T, F) = \\lim_{\\epsilon \\to 0^+} \\frac{T((1-\\epsilon)F + \\epsilon \\delta_x) - T(F)}{\\epsilon} $$\nwhere $\\delta_x$ represents the CDF of a distribution with all its mass concentrated at the point $x$. This function measures the sensitivity of the functional $T$ to an infinitesimal contamination at point $x$.\n\nConsider two fundamental statistical estimators: the mean and the variance. Their corresponding functionals are the expected value, $T_{\\mu}(F) = \\int_{-\\infty}^{\\infty} y \\, dF(y)$, and the variance, $T_{\\sigma^2}(F) = \\int_{-\\infty}^{\\infty} (y - T_{\\mu}(F))^2 \\, dF(y)$.\n\nLet's analyze these functionals in the context of a standard normal distribution, denoted by $F=\\Phi$, which has a mean of 0 and a variance of 1. Let $IF_{\\mu}(x)$ denote the influence function for the mean and $IF_{\\sigma^2}(x)$ denote the influence function for the variance, both evaluated with respect to the standard normal distribution.\n\nWhich of the following statements accurately characterizes the properties of $IF_{\\mu}(x)$ and $IF_{\\sigma^2}(x)$?\n\nA. Both $IF_{\\mu}(x)$ and $IF_{\\sigma^2}(x)$ are bounded functions, indicating that both the mean and variance are robust estimators.\n\nB. $IF_{\\mu}(x)$ is an even function of $x$ (i.e., $IF_{\\mu}(-x) = IF_{\\mu}(x)$), while $IF_{\\sigma^2}(x)$ is an odd function of $x$ (i.e., $IF_{\\sigma^2}(-x) = -IF_{\\sigma^2}(x)$).\n\nC. An observation at the center of the distribution ($x=0$) has no influence on the estimate of the mean, but it tends to decrease the estimate of the variance.\n\nD. For large values of $|x|$, the influence of an outlier on the variance estimator grows at a slower rate than its influence on the mean estimator.\n\nE. Both influence functions are always non-negative for any real number $x$.", "solution": "We use the definition of the influence function. For any functional $T$ and distribution $F$, define $G_{\\epsilon}=(1-\\epsilon)F+\\epsilon \\delta_{x}$ and\n$$\nIF(x;T,F)=\\lim_{\\epsilon \\to 0}\\frac{T(G_{\\epsilon})-T(F)}{\\epsilon}.\n$$\n\nMean functional. Let $T_{\\mu}(F)=\\int y\\,dF(y)$ and denote $\\mu=T_{\\mu}(F)$. Then\n$$\nT_{\\mu}(G_{\\epsilon})=(1-\\epsilon)\\int y\\,dF(y)+\\epsilon\\int y\\,d\\delta_{x}(y)=(1-\\epsilon)\\mu+\\epsilon x=\\mu+\\epsilon(x-\\mu).\n$$\nTherefore,\n$$\nIF_{\\mu}(x;F)=x-\\mu.\n$$\nFor the standard normal $F=\\Phi$, we have $\\mu=0$, hence\n$$\nIF_{\\mu}(x)=x.\n$$\n\nVariance functional. Let $T_{\\sigma^{2}}(F)=\\int (y-\\mu)^{2}\\,dF(y)$ with $\\mu=T_{\\mu}(F)$ and denote $\\sigma^{2}=T_{\\sigma^{2}}(F)$. For $G_{\\epsilon}$, its mean is\n$$\n\\mu_{\\epsilon}=T_{\\mu}(G_{\\epsilon})=\\mu+\\epsilon(x-\\mu).\n$$\nThen\n$$\nT_{\\sigma^{2}}(G_{\\epsilon})=\\int (y-\\mu_{\\epsilon})^{2}\\,dG_{\\epsilon}(y)=(1-\\epsilon)\\int (y-\\mu_{\\epsilon})^{2}\\,dF(y)+\\epsilon(x-\\mu_{\\epsilon})^{2}.\n$$\nExpand to first order in $\\epsilon$. First,\n$$\n\\int (y-\\mu_{\\epsilon})^{2}\\,dF(y)=\\int \\big[(y-\\mu)-\\epsilon(x-\\mu)\\big]^{2}\\,dF(y)\n=\\int (y-\\mu)^{2}\\,dF(y)-2\\epsilon(x-\\mu)\\int (y-\\mu)\\,dF(y)+O(\\epsilon^{2})\n=\\sigma^{2}+O(\\epsilon^{2}),\n$$\nsince $\\int (y-\\mu)\\,dF(y)=0$. Next,\n$$\n(x-\\mu_{\\epsilon})^{2}=\\big[(x-\\mu)-\\epsilon(x-\\mu)\\big]^{2}=(x-\\mu)^{2}-2\\epsilon(x-\\mu)^{2}+O(\\epsilon^{2}).\n$$\nTherefore,\n$$\nT_{\\sigma^{2}}(G_{\\epsilon})=(1-\\epsilon)\\big[\\sigma^{2}+O(\\epsilon^{2})\\big]+\\epsilon\\big[(x-\\mu)^{2}-2\\epsilon(x-\\mu)^{2}+O(\\epsilon^{2})\\big]\n=\\sigma^{2}+\\epsilon\\big[(x-\\mu)^{2}-\\sigma^{2}\\big]+O(\\epsilon^{2}),\n$$\nand hence\n$$\nIF_{\\sigma^{2}}(x;F)=(x-\\mu)^{2}-\\sigma^{2}.\n$$\nFor the standard normal, $\\mu=0$ and $\\sigma^{2}=1$, so\n$$\nIF_{\\sigma^{2}}(x)=x^{2}-1.\n$$\n\nNow evaluate the statements using $IF_{\\mu}(x)=x$ and $IF_{\\sigma^{2}}(x)=x^{2}-1$:\n- Both are unbounded functions, so A is false.\n- $IF_{\\mu}$ is odd, not even; $IF_{\\sigma^{2}}$ is even, not odd; hence B is false.\n- At $x=0$, $IF_{\\mu}(0)=0$ while $IF_{\\sigma^{2}}(0)=-1<0$, so an observation at the center has no influence on the mean and tends to decrease the variance; C is true.\n- For large $|x|$, $|IF_{\\mu}(x)|\\sim |x|$ while $|IF_{\\sigma^{2}}(x)|\\sim x^{2}$, which grows faster, not slower; D is false.\n- Signs are not always non-negative (e.g., $x<0$ for the mean; $|x|<1$ for the variance), so E is false.\n\nTherefore, the correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "1923527"}, {"introduction": "Our first practice revealed that both the mean and variance possess unbounded influence functions, making them highly sensitive to extreme outliers. To address this, we now turn to a class of 'robust' estimators designed to be resistant to such contamination. This problem introduces a powerful redescending M-estimator, Tukey's biweight, whose influence on an estimate diminishes and eventually becomes zero for data points far from the center, providing a powerful mechanism for outlier rejection [@problem_id:1923553].", "problem": "In robust statistics, M-estimators are a broad class of estimators for location parameters that are resistant to outliers. A location M-estimator $T_n$ for a dataset $\\{x_1, \\dots, x_n\\}$ is defined as the value $t$ that minimizes the objective function $\\sum_{i=1}^n \\rho(x_i - t)$, where $\\rho$ is a symmetric, non-negative loss function. Equivalently, $T_n$ is a solution to the implicit equation $\\sum_{i=1}^n \\psi(x_i - t) = 0$, where $\\psi(x) = \\frac{d\\rho(x)}{dx}$ is the score function.\n\nThe influence function, $IF(x; T, F)$, of an estimator $T$ at a distribution $F$ measures the effect of an infinitesimal contamination at point $x$ on the estimate. For an M-estimator with score function $\\psi$, evaluated at a distribution $F$ that is symmetric about the true location parameter (which we can assume to be 0 without loss of generality), the influence function is directly proportional to the score function: $IF(x; T, F) \\propto \\psi(x)$.\n\nTo completely reject extreme outliers, \"redescending\" M-estimators are used, for which the influence function is zero for observations sufficiently far from the center. This implies that their score function $\\psi(x)$ must be zero for $|x|$ larger than some positive constant $k$. One of the most common redescending score functions is the Tukey's biweight (or bisquare) function, defined as:\n$$\n\\psi_k(x) = \\begin{cases}\nx \\left(1 - \\left(\\frac{x}{k}\\right)^2\\right)^2 & \\text{if } |x| \\leq k \\\\\n0 & \\text{if } |x| > k\n\\end{cases}\n$$\nThe corresponding loss function $\\rho_k(x)$ is obtained by integrating the score function, subject to the condition $\\rho_k(0) = 0$. For a redescending M-estimator, the loss function $\\rho_k(x)$ is bounded, which means it becomes constant for $|x| > k$.\n\nWe wish to normalize this estimator by setting the maximum possible value of its loss function, $\\sup_{x \\in \\mathbb{R}} \\rho_k(x)$, to exactly 1. Determine the value of the positive constant $k$ that achieves this normalization.", "solution": "The problem asks for the value of the positive constant $k$ for the Tukey's biweight M-estimator such that the maximum value of its loss function $\\rho_k(x)$ is equal to 1.\n\nThe score function is given by:\n$$\n\\psi_k(x) = \\begin{cases}\nx \\left(1 - \\left(\\frac{x}{k}\\right)^2\\right)^2 & \\text{if } |x| \\leq k \\\\\n0 & \\text{if } |x| > k\n\\end{cases}\n$$\nThe loss function $\\rho_k(x)$ is related to the score function $\\psi_k(x)$ by $\\psi_k(x) = \\frac{d\\rho_k(x)}{dx}$. We can find $\\rho_k(x)$ by integrating $\\psi_k(u)$ from 0 to $x$, using the condition $\\rho_k(0) = 0$.\n$$\n\\rho_k(x) = \\int_0^x \\psi_k(u) du\n$$\nWe first expand the expression for $\\psi_k(u)$ for $|u| \\leq k$:\n$$\n\\psi_k(u) = u \\left(1 - \\frac{2u^2}{k^2} + \\frac{u^4}{k^4}\\right) = u - \\frac{2u^3}{k^2} + \\frac{u^5}{k^4}\n$$\nNow, we integrate this polynomial to find $\\rho_k(x)$ for the interval $|x| \\leq k$.\n$$\n\\rho_k(x) = \\int_0^x \\left(u - \\frac{2u^3}{k^2} + \\frac{u^5}{k^4}\\right) du\n$$\n$$\n\\rho_k(x) = \\left[ \\frac{u^2}{2} - \\frac{2u^4}{4k^2} + \\frac{u^6}{6k^4} \\right]_0^x\n$$\n$$\n\\rho_k(x) = \\frac{x^2}{2} - \\frac{x^4}{2k^2} + \\frac{x^6}{6k^4} \\quad \\text{for } |x| \\leq k\n$$\nFor $|x| > k$, the score function $\\psi_k(x)$ is 0. This means the loss function $\\rho_k(x)$ ceases to change. The value of $\\rho_k(x)$ for $x > k$ is thus constant and equal to its value at $x=k$.\n$$\n\\rho_k(x) = \\int_0^x \\psi_k(u) du = \\int_0^k \\psi_k(u) du + \\int_k^x 0 \\, du = \\int_0^k \\psi_k(u) du = \\rho_k(k)\n$$\nSince $\\rho_k(x)$ is an even function, the same holds for $x < -k$, where $\\rho_k(x) = \\rho_k(-k) = \\rho_k(k)$.\nThe function $\\rho_k(x)$ is non-decreasing for $x \\ge 0$ because its derivative, $\\psi_k(x) = x(1-(x/k)^2)^2$, is non-negative for $x \\in [0, k]$. Therefore, the maximum value of $\\rho_k(x)$ occurs for any $|x| \\ge k$ and is equal to $\\rho_k(k)$.\n$$\n\\rho_{\\max} = \\sup_{x \\in \\mathbb{R}} \\rho_k(x) = \\rho_k(k)\n$$\nWe can find this maximum value by substituting $x=k$ into our expression for $\\rho_k(x)$:\n$$\n\\rho_{\\max} = \\frac{k^2}{2} - \\frac{k^4}{2k^2} + \\frac{k^6}{6k^4}\n$$\n$$\n\\rho_{\\max} = \\frac{k^2}{2} - \\frac{k^2}{2} + \\frac{k^2}{6}\n$$\n$$\n\\rho_{\\max} = \\frac{k^2}{6}\n$$\nThe problem requires this maximum value to be normalized to 1.\n$$\n\\rho_{\\max} = 1\n$$\n$$\n\\frac{k^2}{6} = 1\n$$\nSolving for $k$:\n$$\nk^2 = 6\n$$\nSince $k$ is defined as a positive constant, we take the positive square root.\n$$\nk = \\sqrt{6}\n$$\nThus, the value of the constant $k$ that normalizes the maximum loss to 1 is $\\sqrt{6}$.", "answer": "$$\\boxed{\\sqrt{6}}$$", "id": "1923553"}, {"introduction": "Influence functions are a versatile tool applicable beyond simple location or scale parameters. In this final practice, we extend our analysis to a multivariate context by examining the Pearson correlation coefficient, a key measure of linear association between two variables. By deriving its influence function, we can gain precise insight into how a single bivariate observation $(x, y)$ can pull the estimated correlation in different directions, depending on its location in the plane [@problem_id:1923549].", "problem": "In the field of robust statistics, the Influence Function (IF) is a crucial tool for assessing the effect of an infinitesimal contamination at a single point on a statistical functional. It measures the robustness of an estimator.\n\nLet the Pearson correlation coefficient be represented by the statistical functional $\\rho(F)$, defined for a bivariate distribution $F$ as:\n$$ \\rho(F) = \\frac{E_F[(X-E_F[X])(Y-E_F[Y])]}{\\sqrt{E_F[(X-E_F[X])^2] E_F[(Y-E_F[Y])^2]}} $$\nwhere $E_F[\\cdot]$ denotes the expectation with respect to the distribution $F$.\n\nThe Influence Function of the functional $\\rho$ at the distribution $F_0$ for a point $z = (x, y)$ is given by:\n$$ \\text{IF}(z; \\rho, F_0) = \\lim_{\\epsilon \\to 0^+} \\frac{\\rho((1-\\epsilon)F_0 + \\epsilon \\delta_z) - \\rho(F_0)}{\\epsilon} $$\nwhere $\\delta_z$ is the Dirac delta measure which places a probability mass of 1 at the point $z$.\n\nConsider a base distribution $F_0$ that is a standard bivariate normal distribution with zero means, unit variances, and zero correlation. That is, the mean vector is $\\boldsymbol{\\mu} = (0, 0)^T$ and the covariance matrix is the identity matrix $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n\nDetermine the influence function, $\\text{IF}((x, y); \\rho, F_0)$, for the Pearson correlation coefficient functional at this specific distribution $F_0$. Express your answer as a function of $x$ and $y$.", "solution": "We consider the contaminated distribution $F_{\\epsilon} = (1 - \\epsilon) F_{0} + \\epsilon \\delta_{(x,y)}$ and expand all relevant moments to first order in $\\epsilon$. By linearity of expectation under mixtures, for any integrable function $g$,\n$$\nE_{F_{\\epsilon}}[g(X,Y)] = (1 - \\epsilon) E_{F_{0}}[g(X,Y)] + \\epsilon g(x,y).\n$$\nAt $F_{0}$ we have $E_{F_{0}}[X] = 0$, $E_{F_{0}}[Y] = 0$, $E_{F_{0}}[X^{2}] = 1$, $E_{F_{0}}[Y^{2}] = 1$, and $E_{F_{0}}[XY] = 0$.\n\nFirst, the means under $F_{\\epsilon}$ are\n$$\nm_{X}(F_{\\epsilon}) = E_{F_{\\epsilon}}[X] = \\epsilon x, \\qquad m_{Y}(F_{\\epsilon}) = E_{F_{\\epsilon}}[Y] = \\epsilon y.\n$$\nNext, the second moments are\n$$\nE_{F_{\\epsilon}}[X^{2}] = 1 + \\epsilon (x^{2} - 1), \\qquad E_{F_{\\epsilon}}[Y^{2}] = 1 + \\epsilon (y^{2} - 1).\n$$\nHence the variances are\n$$\nv_{X}(F_{\\epsilon}) = E_{F_{\\epsilon}}[(X - m_{X})^{2}] = E_{F_{\\epsilon}}[X^{2}] - m_{X}^{2} = 1 + \\epsilon (x^{2} - 1) + o(\\epsilon),\n$$\n$$\nv_{Y}(F_{\\epsilon}) = E_{F_{\\epsilon}}[(Y - m_{Y})^{2}] = E_{F_{\\epsilon}}[Y^{2}] - m_{Y}^{2} = 1 + \\epsilon (y^{2} - 1) + o(\\epsilon).\n$$\nTherefore, using the Taylor expansion $\\sqrt{1 + t} = 1 + \\frac{1}{2} t + o(t)$ as $t \\to 0$,\n$$\ns_{X}(F_{\\epsilon}) = \\sqrt{v_{X}(F_{\\epsilon})} = 1 + \\frac{1}{2} \\epsilon (x^{2} - 1) + o(\\epsilon),\n$$\n$$\ns_{Y}(F_{\\epsilon}) = \\sqrt{v_{Y}(F_{\\epsilon})} = 1 + \\frac{1}{2} \\epsilon (y^{2} - 1) + o(\\epsilon).\n$$\nThe cross-moment and covariance are\n$$\nE_{F_{\\epsilon}}[XY] = \\epsilon x y, \\qquad c(F_{\\epsilon}) = \\operatorname{Cov}_{F_{\\epsilon}}(X,Y) = E_{F_{\\epsilon}}[XY] - m_{X} m_{Y} = \\epsilon x y + o(\\epsilon).\n$$\nThe correlation functional is\n$$\n\\rho(F_{\\epsilon}) = \\frac{c(F_{\\epsilon})}{s_{X}(F_{\\epsilon}) s_{Y}(F_{\\epsilon})}.\n$$\nUsing the product expansion\n$$\ns_{X}(F_{\\epsilon}) s_{Y}(F_{\\epsilon}) = \\left(1 + \\frac{1}{2} \\epsilon (x^{2} - 1)\\right) \\left(1 + \\frac{1}{2} \\epsilon (y^{2} - 1)\\right) + o(\\epsilon) = 1 + \\frac{1}{2} \\epsilon (x^{2} + y^{2} - 2) + o(\\epsilon),\n$$\nand the reciprocal expansion $\\frac{1}{1 + u} = 1 - u + o(u)$ as $u \\to 0$, we obtain\n$$\n\\rho(F_{\\epsilon}) = \\left[\\epsilon x y + o(\\epsilon)\\right] \\left[1 - \\frac{1}{2} \\epsilon (x^{2} + y^{2} - 2) + o(\\epsilon)\\right] = \\epsilon x y + o(\\epsilon).\n$$\nSince $\\rho(F_{0}) = 0$, the influence function is\n$$\n\\text{IF}((x,y); \\rho, F_{0}) = \\lim_{\\epsilon \\to 0^{+}} \\frac{\\rho(F_{\\epsilon}) - \\rho(F_{0})}{\\epsilon} = \\lim_{\\epsilon \\to 0^{+}} \\frac{\\epsilon x y + o(\\epsilon)}{\\epsilon} = x y.\n$$\nTherefore, at the standard bivariate normal with zero correlation, the influence function of the Pearson correlation coefficient is $x y$.", "answer": "$$\\boxed{x y}$$", "id": "1923549"}]}