{"hands_on_practices": [{"introduction": "In statistical estimation, we often seek estimators that are unbiased, meaning their long-run average value is the true parameter. This exercise challenges the common intuition that unbiasedness is a sufficient condition for a \"good\" estimator. By calculating and comparing the risk functions for two different estimators of a uniform distribution's parameter $\\theta$, you will directly investigate the concept of admissibility and see how a biased estimator can sometimes be superior.", "problem": "Consider a single measurement $X$ drawn from a uniform probability distribution over the interval $(0, \\theta)$, where the parameter $\\theta > 0$ is unknown. We wish to estimate $\\theta$ using the estimator $\\delta(X) = 2X$.\n\nThe performance of an estimator is evaluated using the squared error loss function, defined as $L(\\theta, a) = (a - \\theta)^2$. The risk of an estimator $\\delta$ is its expected loss, given by $R(\\theta, \\delta) = E[(\\delta(X) - \\theta)^2]$.\n\nAn estimator $\\delta$ is called *inadmissible* if there exists another estimator $\\delta'$ such that $R(\\theta, \\delta') \\le R(\\theta, \\delta)$ for all values of the parameter $\\theta$, and $R(\\theta_0, \\delta') < R(\\theta_0, \\delta)$ for at least one value $\\theta_0$. If no such estimator $\\delta'$ exists, $\\delta$ is called *admissible*.\n\nWhich of the following statements about the estimator $\\delta(X)=2X$ is correct?\n\nA. The estimator is admissible because it is unbiased.\n\nB. The estimator is admissible because its risk is the minimum possible risk among all estimators.\n\nC. The estimator is inadmissible because the estimator $\\delta'(X) = X$ has a smaller risk for all $\\theta > 0$.\n\nD. The estimator is inadmissible because the estimator $\\delta'(X) = \\frac{3}{2}X$ has a smaller risk for all $\\theta > 0$.\n\nE. The estimator is inadmissible because it is a biased estimator of $\\theta$.", "solution": "The problem asks us to determine if the estimator $\\delta(X) = 2X$ is admissible for the parameter $\\theta$ of a $U(0, \\theta)$ distribution, under the squared error loss function $L(\\theta, a) = (a - \\theta)^2$. To do this, we need to calculate the risk of $\\delta(X)$ and see if we can find another estimator $\\delta'(X)$ with a uniformly smaller risk.\n\nFirst, let's find the properties of the random variable $X \\sim U(0, \\theta)$. Its probability density function (PDF) is $f(x|\\theta) = \\frac{1}{\\theta}$ for $0 < x < \\theta$, and $0$ otherwise.\nWe will need the first two moments of $X$.\nThe expected value of $X$ is:\n$$E[X] = \\int_{0}^{\\theta} x f(x|\\theta) dx = \\int_{0}^{\\theta} x \\left(\\frac{1}{\\theta}\\right) dx = \\frac{1}{\\theta} \\left[\\frac{x^2}{2}\\right]_{0}^{\\theta} = \\frac{1}{\\theta} \\left(\\frac{\\theta^2}{2}\\right) = \\frac{\\theta}{2}$$\nThe expected value of $X^2$ is:\n$$E[X^2] = \\int_{0}^{\\theta} x^2 f(x|\\theta) dx = \\int_{0}^{\\theta} x^2 \\left(\\frac{1}{\\theta}\\right) dx = \\frac{1}{\\theta} \\left[\\frac{x^3}{3}\\right]_{0}^{\\theta} = \\frac{1}{\\theta} \\left(\\frac{\\theta^3}{3}\\right) = \\frac{\\theta^2}{3}$$\n\nNow, let's analyze the given estimator $\\delta(X) = 2X$.\nFirst, let's check if it is biased (related to option E). The bias of an estimator is $Bias(\\delta) = E[\\delta(X)] - \\theta$.\n$$E[\\delta(X)] = E[2X] = 2E[X] = 2\\left(\\frac{\\theta}{2}\\right) = \\theta$$\nSince $E[\\delta(X)] = \\theta$, the bias is zero. The estimator is unbiased. This immediately tells us that option E is incorrect as its premise is false. Option A suggests admissibility because of unbiasedness, which is a common misconception we must investigate.\n\nNext, we calculate the risk of $\\delta(X) = 2X$. The risk is the Mean Squared Error (MSE), $R(\\theta, \\delta) = E[(\\delta(X) - \\theta)^2]$.\n$$R(\\theta, \\delta) = E[(2X - \\theta)^2] = E[4X^2 - 4\\theta X + \\theta^2]$$\nUsing the linearity of expectation:\n$$R(\\theta, \\delta) = 4E[X^2] - 4\\theta E[X] + \\theta^2$$\nSubstituting the moments we calculated:\n$$R(\\theta, \\delta) = 4\\left(\\frac{\\theta^2}{3}\\right) - 4\\theta\\left(\\frac{\\theta}{2}\\right) + \\theta^2 = \\frac{4}{3}\\theta^2 - 2\\theta^2 + \\theta^2 = \\left(\\frac{4}{3} - 1\\right)\\theta^2 = \\frac{1}{3}\\theta^2$$\nSo, the risk of the estimator $\\delta(X)=2X$ is $R(\\theta, \\delta) = \\frac{\\theta^2}{3}$.\n\nTo check for admissibility, we examine the alternative estimators proposed in the options.\n\nLet's check option C, which proposes the estimator $\\delta_C(X) = X$. Let's compute its risk.\n$$R(\\theta, \\delta_C) = E[(X - \\theta)^2] = E[X^2 - 2\\theta X + \\theta^2]$$\n$$R(\\theta, \\delta_C) = E[X^2] - 2\\theta E[X] + \\theta^2 = \\frac{\\theta^2}{3} - 2\\theta\\left(\\frac{\\theta}{2}\\right) + \\theta^2 = \\frac{\\theta^2}{3} - \\theta^2 + \\theta^2 = \\frac{1}{3}\\theta^2$$\nThe risk of $\\delta_C(X)$ is $R(\\theta, \\delta_C) = \\frac{\\theta^2}{3}$, which is equal to the risk of $\\delta(X)$. For $\\delta(X)$ to be inadmissible due to $\\delta_C(X)$, the risk of $\\delta_C(X)$ would have to be less than or equal to the risk of $\\delta(X)$ for all $\\theta$ and strictly less for at least one $\\theta$. Since the risks are identical, $\\delta_C(X)$ does not dominate $\\delta(X)$. Thus, option C is incorrect.\n\nNow, let's check option D, which proposes the estimator $\\delta_D(X) = \\frac{3}{2}X$. Let's compute its risk.\n$$R(\\theta, \\delta_D) = E\\left[\\left(\\frac{3}{2}X - \\theta\\right)^2\\right] = E\\left[\\frac{9}{4}X^2 - 3\\theta X + \\theta^2\\right]$$\n$$R(\\theta, \\delta_D) = \\frac{9}{4}E[X^2] - 3\\theta E[X] + \\theta^2 = \\frac{9}{4}\\left(\\frac{\\theta^2}{3}\\right) - 3\\theta\\left(\\frac{\\theta}{2}\\right) + \\theta^2$$\n$$R(\\theta, \\delta_D) = \\frac{3}{4}\\theta^2 - \\frac{3}{2}\\theta^2 + \\theta^2 = \\left(\\frac{3}{4} - \\frac{6}{4} + \\frac{4}{4}\\right)\\theta^2 = \\frac{1}{4}\\theta^2$$\nThe risk of $\\delta_D(X)$ is $R(\\theta, \\delta_D) = \\frac{1}{4}\\theta^2$.\n\nLet's compare this with the risk of our original estimator, $R(\\theta, \\delta) = \\frac{1}{3}\\theta^2$.\nFor any $\\theta > 0$, we have $\\frac{1}{4} < \\frac{1}{3}$, which implies $\\frac{1}{4}\\theta^2 < \\frac{1}{3}\\theta^2$.\nSo, $R(\\theta, \\delta_D) < R(\\theta, \\delta)$ for all $\\theta > 0$.\nThis means that the estimator $\\delta_D(X) = \\frac{3}{2}X$ dominates the estimator $\\delta(X) = 2X$. According to the definition of admissibility, the existence of such a strictly better estimator makes $\\delta(X) = 2X$ inadmissible.\nTherefore, option D is the correct statement.\n\nThis finding also allows us to invalidate the remaining options.\n- Option A is false: We have shown $\\delta(X)$ is unbiased, but it is inadmissible. So, unbiasedness does not imply admissibility.\n- Option B is false: $\\delta(X)$ is not the minimum risk estimator, as $\\delta_D(X)$ has a smaller risk.\n- Option E is false: As shown earlier, $\\delta(X)$ is unbiased.\n\nThe correct choice is D.", "answer": "$$\\boxed{D}$$", "id": "1894912"}, {"introduction": "The properties of an estimator can be surprisingly sensitive to the underlying assumptions of a statistical model, including the set of possible values for the parameter. This thought-provoking problem presents a hypothetical scenario where the parameter $\\lambda$ of a Poisson distribution is restricted to be a positive integer. You will explore whether the standard estimator $\\delta(X) = X$, which is admissible when $\\lambda$ can be any positive real number, maintains its admissibility under this unusual parameter space, revealing a key subtlety of decision theory.", "problem": "A physicist is investigating a radioactive source. According to a speculative quantum model, the effective decay rate parameter $\\lambda$ of this source is quantized, meaning it can only take on positive integer values in its natural units. Thus, the parameter space for $\\lambda$ is the set of positive integers, $\\Theta = \\{1, 2, 3, \\ldots\\}$. The number of decays, $X$, observed in a unit time interval is known to follow a Poisson distribution with mean $\\lambda$, i.e., $X \\sim \\text{Poisson}(\\lambda)$, with probability mass function $P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$ for $k=0, 1, 2, \\ldots$.\n\nTo estimate the unknown value of $\\lambda$, the standard choice of estimator is the observed count itself, denoted as $\\delta_0(X) = X$. The performance of any estimator $\\delta$ is evaluated using the squared error loss function, $L(\\lambda, \\delta) = (\\delta - \\lambda)^2$, and its corresponding risk function, which is the mean squared error $R(\\lambda, \\delta) = E_{\\lambda}[(\\delta(X) - \\lambda)^2]$.\n\nAn estimator $\\delta_0$ is defined as inadmissible if there exists an alternative estimator, $\\delta_1$, that dominates it. Domination means that $R(\\lambda, \\delta_1) \\le R(\\lambda, \\delta_0)$ for all $\\lambda \\in \\Theta$, and for at least one value $\\lambda_0 \\in \\Theta$, the inequality is strict: $R(\\lambda_0, \\delta_1) < R(\\lambda_0, \\delta_0)$.\n\nA colleague suggests exploring a modified class of estimators, $\\delta_c(X)$, defined by:\n$$\n\\delta_c(X) = \\begin{cases} c, & \\text{if } X=0 \\\\ X, & \\text{if } X \\ge 1 \\end{cases}\n$$\nwhere $c$ is a real constant. Note that the standard estimator $\\delta_0(X)$ is a member of this class with $c=0$.\n\nBy analyzing whether an estimator of the form $\\delta_c(X)$ can dominate $\\delta_0(X)$, determine which of the following statements is correct.\n\nA. The estimator $\\delta_0(X)=X$ is admissible because it is an unbiased estimator whose risk cannot be uniformly improved.\n\nB. The estimator $\\delta_0(X)=X$ is inadmissible. The estimator $\\delta_c(X)$ with $c=1$ dominates $\\delta_0(X)$.\n\nC. The estimator $\\delta_0(X)=X$ is inadmissible. The estimator $\\delta_c(X)$ with $c=0$ dominates $\\delta_0(X)$.\n\nD. The estimator $\\delta_0(X)=X$ is inadmissible. The estimator $\\delta_c(X)$ with $c=-1$ dominates $\\delta_0(X)$.\n\nE. All estimators of the form $\\delta_c(X)$ for $c \\ne 0$ have a risk function that is strictly greater than the risk of $\\delta_0(X)$ for at least one value of $\\lambda$, so none of them can dominate $\\delta_0(X)$.", "solution": "To determine the correct statement, we need to analyze the admissibility of the standard estimator $\\delta_0(X) = X$ by comparing its risk function with that of the proposed estimators $\\delta_c(X)$.\n\nFirst, let's calculate the risk of the standard estimator, $\\delta_0(X) = X$. The risk is the mean squared error:\n$R(\\lambda, \\delta_0) = E_{\\lambda}[ (X - \\lambda)^2 ]$\nSince $E_{\\lambda}[X] = \\lambda$, this is precisely the variance of the Poisson distribution.\n$R(\\lambda, \\delta_0) = \\text{Var}_{\\lambda}(X) = \\lambda$.\n\nNext, let's calculate the risk of the modified estimator, $\\delta_c(X)$.\n$R(\\lambda, \\delta_c) = E_{\\lambda}[ (\\delta_c(X) - \\lambda)^2 ] = \\sum_{k=0}^{\\infty} (\\delta_c(k) - \\lambda)^2 P(X=k)$\nWe can split the sum based on the definition of $\\delta_c(X)$:\n$R(\\lambda, \\delta_c) = (\\delta_c(0) - \\lambda)^2 P(X=0) + \\sum_{k=1}^{\\infty} (\\delta_c(k) - \\lambda)^2 P(X=k)$\nSubstituting the definitions of $\\delta_c(k)$ and the Poisson probability mass function $P(X=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}$:\n$R(\\lambda, \\delta_c) = (c - \\lambda)^2 P(X=0) + \\sum_{k=1}^{\\infty} (k - \\lambda)^2 P(X=k)$\n\nTo facilitate comparison, let's express the risk of $\\delta_0(X)$ in a similar sum form:\n$R(\\lambda, \\delta_0) = \\sum_{k=0}^{\\infty} (k - \\lambda)^2 P(X=k) = (0 - \\lambda)^2 P(X=0) + \\sum_{k=1}^{\\infty} (k - \\lambda)^2 P(X=k)$\n$R(\\lambda, \\delta_0) = \\lambda^2 P(X=0) + \\sum_{k=1}^{\\infty} (k - \\lambda)^2 P(X=k)$\n\nNow, we can find the difference in risk functions, $\\Delta R(\\lambda) = R(\\lambda, \\delta_c) - R(\\lambda, \\delta_0)$:\n$\\Delta R(\\lambda) = \\left[ (c - \\lambda)^2 P(X=0) + \\sum_{k=1}^{\\infty} (k - \\lambda)^2 P(X=k) \\right] - \\left[ \\lambda^2 P(X=0) + \\sum_{k=1}^{\\infty} (k - \\lambda)^2 P(X=k) \\right]$\nThe summation terms cancel out, leaving:\n$\\Delta R(\\lambda) = ((c - \\lambda)^2 - \\lambda^2) P(X=0)$\nSubstituting $P(X=0) = e^{-\\lambda}$:\n$\\Delta R(\\lambda) = (c^2 - 2c\\lambda + \\lambda^2 - \\lambda^2) e^{-\\lambda} = (c^2 - 2c\\lambda) e^{-\\lambda}$\n\nFor $\\delta_c(X)$ to dominate $\\delta_0(X)$, two conditions must be met:\n1. $\\Delta R(\\lambda) \\le 0$ for all $\\lambda \\in \\Theta = \\{1, 2, 3, \\ldots\\}$.\n2. $\\Delta R(\\lambda_0) < 0$ for at least one $\\lambda_0 \\in \\Theta$.\n\nSince $e^{-\\lambda} > 0$ for all $\\lambda$, the first condition simplifies to:\n$c^2 - 2c\\lambda \\le 0 \\implies c(c - 2\\lambda) \\le 0$\nThis inequality must hold for all $\\lambda \\in \\{1, 2, 3, \\ldots\\}$.\n\nLet's examine the options:\n\n- **Statement C (c=0):**\nIf $c=0$, then $c(c - 2\\lambda) = 0$. So $\\Delta R(\\lambda) = 0$ for all $\\lambda$. The risk is identical to that of $\\delta_0(X)$. Therefore, $\\delta_0(X)$ does not dominate itself. This statement is incorrect.\n\n- **Statement D (c=-1):**\nIf $c=-1$, the condition is $(-1)(-1 - 2\\lambda) \\le 0$, which simplifies to $1 + 2\\lambda \\le 0$. Since the parameter space is $\\lambda \\ge 1$, $1 + 2\\lambda$ is always positive (e.g., for $\\lambda=1$, it is 3). The inequality is never satisfied. In fact, $\\Delta R(\\lambda) > 0$ for all $\\lambda \\in \\Theta$, meaning $\\delta_{-1}(X)$ is strictly worse than $\\delta_0(X)$. This statement is incorrect.\n\n- **Statement B (c=1):**\nIf $c=1$, the condition is $1(1 - 2\\lambda) \\le 0$. Since $\\lambda \\in \\{1, 2, 3, \\ldots\\}$, the smallest value for $\\lambda$ is 1.\nFor $\\lambda=1$, $1 - 2(1) = -1 \\le 0$.\nFor $\\lambda \\ge 1$, we have $2\\lambda \\ge 2$, so $1-2\\lambda \\le -1 < 0$.\nThe condition $c(c - 2\\lambda) \\le 0$ is satisfied for all $\\lambda \\in \\Theta$.\nFurthermore, the inequality is strict for all $\\lambda \\in \\Theta$. This means $\\Delta R(\\lambda) = (1 - 2\\lambda)e^{-\\lambda} < 0$ for all $\\lambda \\in \\{1, 2, 3, \\ldots\\}$.\nSo, $R(\\lambda, \\delta_1) < R(\\lambda, \\delta_0)$ for all values of $\\lambda$ in the parameter space. This is a case of strict dominance. Thus, $\\delta_1(X)$ dominates $\\delta_0(X)$, which proves that $\\delta_0(X)=X$ is an inadmissible estimator. Statement B is correct.\n\n- **Statement A:** This statement is false because we have found an estimator, $\\delta_1(X)$, that uniformly improves upon $\\delta_0(X)$. The fact that $\\delta_0(X)$ is unbiased is not sufficient to guarantee admissibility, especially with this unusual parameter space.\n\n- **Statement E:** This statement is false because we found that for $c=1$, the risk is uniformly smaller.\n\nTherefore, the only correct statement is B.", "answer": "$$\\boxed{B}$$", "id": "1894876"}, {"introduction": "Our choice of how to penalize estimation errors, captured by the loss function, is a critical part of defining a statistical problem. While squared-error loss is common, many real-world applications involve asymmetric costs. This practice introduces the Linex loss function, which penalizes over- and under-estimation differently, and asks you to determine if the standard estimator for a normal mean remains admissible, thereby generalizing your understanding of risk and optimality.", "problem": "Consider a single observation $X$ from a normal distribution with an unknown mean $\\theta \\in \\mathbb{R}$ and a known variance of 1, i.e., $X \\sim N(\\theta, 1)$. We wish to estimate the parameter $\\theta$. The quality of an estimator $\\delta(X)$ is evaluated using the Linex (Linear-Exponential) loss function, defined as:\n$$L(\\theta, a) = \\exp(c(a-\\theta)) - c(a-\\theta) - 1$$\nwhere $a$ is the estimated value and $c$ is a known, non-zero real constant.\n\nAn estimator $\\delta$ is said to be inadmissible if there exists another estimator $\\delta'$ that dominates it. Domination means that the risk function of $\\delta'$, $R(\\theta, \\delta') = E_{\\theta}[L(\\theta, \\delta'(X))]$, is less than or equal to the risk function of $\\delta$, $R(\\theta, \\delta) = E_{\\theta}[L(\\theta, \\delta(X))]$, for all $\\theta$, with strict inequality for at least one value of $\\theta$.\n\nConsider the standard estimator $\\delta_0(X) = X$. Now, let's also define two other potential estimators: $\\delta_1(X) = X-c$ and $\\delta_*(X) = X - \\frac{c}{2}$.\n\nWhich of the following statements regarding the admissibility of $\\delta_0(X)$ is correct?\n\nA. The estimator $\\delta_0(X)$ is admissible.\n\nB. The estimator $\\delta_0(X)$ is inadmissible because it is dominated by $\\delta_1(X)$, but it is not dominated by $\\delta_*(X)$.\n\nC. The estimator $\\delta_0(X)$ is inadmissible because it is dominated by $\\delta_*(X)$, but it is not dominated by $\\delta_1(X)$.\n\nD. The estimator $\\delta_0(X)$ is inadmissible because it is dominated by both $\\delta_1(X)$ and $\\delta_*(X)$.\n\nE. The estimator $\\delta_0(X)$ is inadmissible, but neither $\\delta_1(X)$ nor $\\delta_*(X)$ are dominating estimators.", "solution": "Let $X \\sim N(\\theta,1)$ and consider the Linex loss $L(\\theta,a)=\\exp(c(a-\\theta)) - c(a-\\theta) - 1$ with $c \\neq 0$. For shift estimators of the form $\\delta_{b}(X)=X-b$, write $Z=X-\\theta \\sim N(0,1)$, so $a-\\theta=\\delta_{b}(X)-\\theta=Z-b$. The risk is\n$$\nR(\\theta,\\delta_{b})=E_{\\theta}[L(\\theta,\\delta_{b}(X))]=E\\big[\\exp(c(Z-b)) - c(Z-b) - 1\\big].\n$$\nUsing linearity of expectation and the moment generating function of $Z \\sim N(0,1)$, $E[\\exp(tZ)]=\\exp(t^{2}/2)$, we obtain\n$$\nE[\\exp(c(Z-b))]=\\exp(-cb)\\,E[\\exp(cZ)]=\\exp\\!\\left(\\frac{c^{2}}{2}-cb\\right),\\quad E[c(Z-b)]=c(E[Z]-b)=-cb,\n$$\nhence\n$$\nR(\\theta,\\delta_{b})=\\exp\\!\\left(\\frac{c^{2}}{2}-cb\\right)+cb-1.\n$$\nThis risk does not depend on $\\theta$. To find the best shift $b$, differentiate with respect to $b$:\n$$\n\\frac{d}{db}R(\\theta,\\delta_{b})=-c\\,\\exp\\!\\left(\\frac{c^{2}}{2}-cb\\right)+c,\\quad \\frac{d^{2}}{db^{2}}R(\\theta,\\delta_{b})=c^{2}\\,\\exp\\!\\left(\\frac{c^{2}}{2}-cb\\right)>0.\n$$\nSetting the first derivative to zero yields\n$$\n\\exp\\!\\left(\\frac{c^{2}}{2}-cb\\right)=1 \\;\\;\\Longrightarrow\\;\\; \\frac{c^{2}}{2}-cb=0 \\;\\;\\Longrightarrow\\;\\; b=\\frac{c}{2},\n$$\nwhich, by the positive second derivative, is the unique minimizer. Therefore $\\delta_{*}(X)=X-\\frac{c}{2}$ minimizes the constant risk in this class, with\n$$\nR(\\theta,\\delta_{*})=\\exp\\!\\left(\\frac{c^{2}}{2}-c\\cdot\\frac{c}{2}\\right)+c\\cdot\\frac{c}{2}-1=\\exp(0)+\\frac{c^{2}}{2}-1=\\frac{c^{2}}{2}.\n$$\nCompute the risks of the proposed estimators:\n- For $\\delta_{0}(X)=X$ ($b=0$):\n$$\nR(\\theta,\\delta_{0})=\\exp\\!\\left(\\frac{c^{2}}{2}\\right)-1.\n$$\n- For $\\delta_{1}(X)=X-c$ ($b=c$):\n$$\nR(\\theta,\\delta_{1})=\\exp\\!\\left(\\frac{c^{2}}{2}-c^{2}\\right)+c^{2}-1=\\exp\\!\\left(-\\frac{c^{2}}{2}\\right)+c^{2}-1.\n$$\nCompare risks:\n1) $\\delta_{*}$ versus $\\delta_{0}$:\nLet $x=\\frac{c^{2}}{2}>0$. Then\n$$\nR(\\theta,\\delta_{0})-R(\\theta,\\delta_{*})=\\exp(x)-1 - x.\n$$\nSince $\\exp(x) \\ge 1+x$ with strict inequality for $x>0$, it follows that $R(\\theta,\\delta_{0})>R(\\theta,\\delta_{*})$ for all $c \\neq 0$.\n\n2) $\\delta_{1}$ versus $\\delta_{0}$:\nWith $x=\\frac{c^{2}}{2}>0$,\n$$\nR(\\theta,\\delta_{1})-R(\\theta,\\delta_{0})=\\exp(-x)+2x-1-\\exp(x).\n$$\nDefine $q(x)=\\exp(-x)-\\exp(x)+2x$. Then $q(0)=0$ and\n$$\nq'(x)=-\\exp(-x)-\\exp(x)+2=2-(\\exp(x)+\\exp(-x))<0 \\quad \\text{for } x>0,\n$$\nsince $\\exp(x)+\\exp(-x)>2$. Hence $q(x)<0$ for $x>0$, which implies $R(\\theta,\\delta_{1})<R(\\theta,\\delta_{0})$ for all $c \\neq 0$.\n\nTherefore, for every $\\theta$ and $c \\neq 0$, both $\\delta_{*}$ and $\\delta_{1}$ have strictly smaller (constant) risk than $\\delta_{0}$. Consequently, $\\delta_{0}$ is inadmissible and is dominated by both $\\delta_{1}$ and $\\delta_{*}$.\n\nThus, the correct statement is that $\\delta_{0}(X)$ is inadmissible because it is dominated by both $\\delta_{1}(X)$ and $\\delta_{*}(X)$.", "answer": "$$\\boxed{D}$$", "id": "1894881"}]}