{"hands_on_practices": [{"introduction": "Before we delve into theoretical models, let's start with the fundamentals: calculating autocorrelation from observed data. This exercise will walk you through computing the sample Autocorrelation Function (ACF) for a short sequence, a core skill for summarizing the dependency structure within a time series. Understanding this calculation provides a concrete foundation for interpreting the ACF plots generated by statistical software. [@problem_id:1897249]", "problem": "Consider a discrete time series representing four sequential observations, given by the sequence $\\{X_t\\}_{t=1}^4 = \\{2, 8, 4, 10\\}$. For the purposes of this calculation, you are to assume that the true mean of the process generating this data is zero, and therefore, the sample mean $\\bar{X}$ should be taken as 0.\n\nThe sample Autocorrelation Function (ACF) at a given lag $h$ for a time series $\\{X_t\\}_{t=1}^n$ with a sample mean $\\bar{X}$ is defined as:\n$$r_h = \\frac{\\sum_{t=1}^{n-h} (X_t - \\bar{X})(X_{t+h} - \\bar{X})}{\\sum_{t=1}^{n} (X_t - \\bar{X})^2}$$\nYour task is to compute the value of the sample ACF at lag $h=1$ for the provided time series.\n\nExpress your final answer as a fraction in its simplest form.", "solution": "We are given the time series $\\{X_{t}\\}_{t=1}^{4}=\\{2,8,4,10\\}$ and instructed to take $\\bar{X}=0$. The sample ACF at lag $h=1$ is defined by\n$$\nr_{1}=\\frac{\\sum_{t=1}^{n-1}\\left(X_{t}-\\bar{X}\\right)\\left(X_{t+1}-\\bar{X}\\right)}{\\sum_{t=1}^{n}\\left(X_{t}-\\bar{X}\\right)^{2}}.\n$$\nWith $\\bar{X}=0$ and $n=4$, the numerator is\n$$\n\\sum_{t=1}^{3}X_{t}X_{t+1}=X_{1}X_{2}+X_{2}X_{3}+X_{3}X_{4}=2\\cdot 8+8\\cdot 4+4\\cdot 10=16+32+40=88.\n$$\nThe denominator is\n$$\n\\sum_{t=1}^{4}X_{t}^{2}=2^{2}+8^{2}+4^{2}+10^{2}=4+64+16+100=184.\n$$\nTherefore,\n$$\nr_{1}=\\frac{88}{184}.\n$$\nSimplifying the fraction by dividing numerator and denominator by $8$ gives\n$$\nr_{1}=\\frac{11}{23}.\n$$", "answer": "$$\\boxed{\\frac{11}{23}}$$", "id": "1897249"}, {"introduction": "We now move from sample data to a theoretical model, the Moving Average (MA) process, which is useful for modeling events whose effects are not persistent. This practice explores the MA(1) model, where a value in the series is affected by the current and immediately preceding random shocks, but not by older ones. Calculating the theoretical ACF will reveal a defining characteristic of these models and is a key step in learning to identify them from data. [@problem_id:1897209]", "problem": "A financial analyst is modeling the daily change in a particular commodity's price using a simple time series model. The model for the price change on day $t$, denoted by $X_t$, is given by a moving average process of order 1, MA(1):\n$$X_t = \\epsilon_t + 0.8 \\epsilon_{t-1}$$\nHere, $\\epsilon_t$ represents a \"shock\" or new information arriving on day $t$. The sequence of shocks $\\{\\epsilon_t\\}$ is assumed to be a white noise process with the following properties:\n1. The expected value of any shock is zero, i.e., $E[\\epsilon_t] = 0$ for all $t$.\n2. The variance of any shock is a constant, finite value $\\sigma^2$, i.e., $Var(\\epsilon_t) = \\sigma^2$ for all $t$.\n3. Shocks at different times are uncorrelated, i.e., $Cov(\\epsilon_t, \\epsilon_s) = 0$ for all $t \\neq s$.\n\nThe theoretical Autocorrelation Function (ACF) at lag $k$, denoted $\\rho(k)$, measures the correlation between the time series and its value $k$ periods ago. It is defined as:\n$$\\rho(k) = \\frac{\\gamma(k)}{\\gamma(0)}$$\nwhere $\\gamma(k) = Cov(X_t, X_{t-k})$ is the autocovariance function at lag $k$, and $\\gamma(0) = Var(X_t)$ is the variance of the process.\n\nCalculate the theoretical autocorrelation at lag 1, $\\rho(1)$, for this model. Express your answer as an exact fraction in its simplest form.", "solution": "We are given the MA(1) process $X_{t}=\\epsilon_{t}+\\theta\\,\\epsilon_{t-1}$ with $\\theta=\\frac{4}{5}$ and $\\{\\epsilon_{t}\\}$ white noise with $E[\\epsilon_{t}]=0$, $Var(\\epsilon_{t})=\\sigma^{2}$, and $Cov(\\epsilon_{t},\\epsilon_{s})=0$ for $t\\neq s$. The autocorrelation at lag $k$ is $\\rho(k)=\\frac{\\gamma(k)}{\\gamma(0)}$, where $\\gamma(k)=Cov(X_{t},X_{t-k})$.\n\nFirst compute the variance $\\gamma(0)=Var(X_{t})$. Using $Var(aY+bZ)=a^{2}Var(Y)+b^{2}Var(Z)+2ab\\,Cov(Y,Z)$ and the white noise properties,\n$$\n\\gamma(0)=Var(\\epsilon_{t}+\\theta\\,\\epsilon_{t-1})=Var(\\epsilon_{t})+\\theta^{2}Var(\\epsilon_{t-1})+2\\theta\\,Cov(\\epsilon_{t},\\epsilon_{t-1}).\n$$\nSince $Cov(\\epsilon_{t},\\epsilon_{t-1})=0$, this simplifies to\n$$\n\\gamma(0)=(1+\\theta^{2})\\sigma^{2}=\\left(1+\\left(\\frac{4}{5}\\right)^{2}\\right)\\sigma^{2}=\\left(1+\\frac{16}{25}\\right)\\sigma^{2}=\\frac{41}{25}\\sigma^{2}.\n$$\n\nNext compute the lag-1 autocovariance $\\gamma(1)=Cov(X_{t},X_{t-1})$. Using $X_{t-1}=\\epsilon_{t-1}+\\theta\\,\\epsilon_{t-2}$,\n$$\n\\gamma(1)=Cov(\\epsilon_{t}+\\theta\\,\\epsilon_{t-1},\\,\\epsilon_{t-1}+\\theta\\,\\epsilon_{t-2}).\n$$\nExpanding covariance bilinearly,\n$$\n\\gamma(1)=Cov(\\epsilon_{t},\\epsilon_{t-1})+\\theta\\,Cov(\\epsilon_{t},\\epsilon_{t-2})+\\theta\\,Cov(\\epsilon_{t-1},\\epsilon_{t-1})+\\theta^{2}Cov(\\epsilon_{t-1},\\epsilon_{t-2}).\n$$\nBy the white noise properties, $Cov(\\epsilon_{t},\\epsilon_{t-1})=0$, $Cov(\\epsilon_{t},\\epsilon_{t-2})=0$, and $Cov(\\epsilon_{t-1},\\epsilon_{t-2})=0$, while $Cov(\\epsilon_{t-1},\\epsilon_{t-1})=Var(\\epsilon_{t-1})=\\sigma^{2}$. Hence\n$$\n\\gamma(1)=\\theta\\,\\sigma^{2}=\\frac{4}{5}\\sigma^{2}.\n$$\n\nTherefore, the lag-1 autocorrelation is\n$$\n\\rho(1)=\\frac{\\gamma(1)}{\\gamma(0)}=\\frac{\\frac{4}{5}\\sigma^{2}}{\\frac{41}{25}\\sigma^{2}}=\\frac{4}{5}\\cdot\\frac{25}{41}=\\frac{20}{41}.\n$$\nThis fraction is already in simplest form.", "answer": "$$\\boxed{\\frac{20}{41}}$$", "id": "1897209"}, {"introduction": "In contrast to the short memory of MA processes, Autoregressive (AR) models describe systems where the past has a lingering influence on the present. This hands-on practice focuses on the AR(1) model, where the current value is a function of the immediately preceding value plus a random shock. By calculating its ACF, you will discover the characteristic pattern of gradual decay that helps distinguish AR processes from other time series structures. [@problem_id:1897238]", "problem": "A climatologist is modeling the daily temperature anomaly in a city, which is defined as the deviation from the long-term average temperature for a given day. The anomaly on day $t$, denoted by $X_t$, is found to be well-described by a stationary Autoregressive model of order 1 (AR(1)). The model is given by the equation:\n$$\nX_t = \\phi X_{t-1} + \\epsilon_t\n$$\nwhere $X_{t-1}$ is the anomaly on the previous day, $\\phi$ is the autoregressive coefficient, and $\\epsilon_t$ is a white noise process with mean zero and constant variance $\\sigma_\\epsilon^2$. The white noise term $\\epsilon_t$ is uncorrelated with all past values of the process $X$.\n\nGiven that the autoregressive coefficient for this model is $\\phi = 0.5$, calculate the value of the autocorrelation function (ACF) of the process at lag 2, denoted as $\\rho(2)$.", "solution": "We are given a stationary AR(1) process defined by $X_{t}=\\phi X_{t-1}+\\epsilon_{t}$, where $\\epsilon_{t}$ is white noise with mean zero and variance $\\sigma_{\\epsilon}^{2}$, uncorrelated with all past values of $\\{X_{t}\\}$. For a stationary process, define the autocovariance function $\\gamma(h)=\\operatorname{Cov}(X_{t},X_{t-h})$ and the autocorrelation function $\\rho(h)=\\gamma(h)/\\gamma(0)$.\n\nUsing the model equation and stationarity, for $h\\geq 1$,\n$$\n\\gamma(h)=\\operatorname{E}[X_{t}X_{t-h}]=\\operatorname{E}[(\\phi X_{t-1}+\\epsilon_{t})X_{t-h}]\n=\\phi\\,\\operatorname{E}[X_{t-1}X_{t-h}]+\\operatorname{E}[\\epsilon_{t}X_{t-h}].\n$$\nSince $\\epsilon_{t}$ is uncorrelated with all past values of the process, in particular with $X_{t-h}$ for $h\\geq 1$, we have $\\operatorname{E}[\\epsilon_{t}X_{t-h}]=0$. Therefore,\n$$\n\\gamma(h)=\\phi\\,\\gamma(h-1).\n$$\nBy recursion, this yields\n$$\n\\gamma(h)=\\phi^{h}\\gamma(0).\n$$\nHence the autocorrelation function is\n$$\n\\rho(h)=\\frac{\\gamma(h)}{\\gamma(0)}=\\phi^{h}.\n$$\nWith $\\phi=0.5$ and $h=2$, we obtain\n$$\n\\rho(2)=\\phi^{2}=\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{4}.\n$$", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "1897238"}]}