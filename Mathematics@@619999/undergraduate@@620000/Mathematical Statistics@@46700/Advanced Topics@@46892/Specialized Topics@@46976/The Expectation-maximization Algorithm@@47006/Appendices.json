{"hands_on_practices": [{"introduction": "The Expectation-Maximization algorithm is an iterative process, and to truly understand its mechanics, there is no substitute for performing an iteration by hand. This first practice problem provides a concrete, numerical exercise using the classic Gaussian Mixture Model (GMM). By manually computing the \"responsibilities\" in the E-step and then using these soft assignments to re-estimate the model parameters in the M-step, you will gain a feel for the fundamental two-step procedure and how the algorithm progressively refines its estimates. [@problem_id:1960172]", "problem": "A researcher is analyzing a dataset of measurements, which is believed to be sampled from a mixture of two distinct populations, A and B. They decide to model the data using a two-component Gaussian Mixture Model (GMM). The probability density function (PDF) for a single data point $x$ is given by:\n$$p(x | \\theta) = \\pi_A \\mathcal{N}(x | \\mu_A, \\sigma_A^2) + \\pi_B \\mathcal{N}(x | \\mu_B, \\sigma_B^2)$$\nwhere $\\pi_A$ and $\\pi_B$ are the mixing proportions (with $\\pi_A + \\pi_B = 1$), and $\\mathcal{N}(x | \\mu, \\sigma^2)$ is the Gaussian PDF:\n$$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n\nThe dataset consists of the following five measurements: $\\{4.0, 4.5, 5.0, 8.0, 9.0\\}$.\nFor simplicity, the variances of the two components are assumed to be known and fixed at $\\sigma_A^2 = \\sigma_B^2 = 1.0$.\n\nThe researcher initializes the model parameters for the Expectation-Maximization (EM) algorithm as follows:\n- Initial means: $\\mu_A^{(0)} = 4.2$ and $\\mu_B^{(0)} = 8.8$.\n- Initial mixing proportions: $\\pi_A^{(0)} = 0.5$ and $\\pi_B^{(0)} = 0.5$.\n\nYour task is to perform a single, full iteration of the EM algorithm, which consists of one Expectation step (E-step) followed by one Maximization step (M-step), to compute the updated parameters.\n\nCalculate the updated value for the mean of the first component, $\\mu_A^{(1)}$. Report your answer as a real number, rounded to four significant figures.", "solution": "We apply one Expectation-Maximization (EM) iteration for the two-component Gaussian Mixture Model with fixed variances $\\sigma_{A}^{2}=\\sigma_{B}^{2}=1$ and initial parameters $\\mu_{A}^{(0)}=4.2$, $\\mu_{B}^{(0)}=8.8$, $\\pi_{A}^{(0)}=\\pi_{B}^{(0)}=0.5$.\n\nE-step (responsibilities):\nFor each data point $x_{i}$, the responsibility of component $A$ is\n$$\nr_{iA} \\equiv \\gamma_{iA} = \\frac{\\pi_{A}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{A}^{(0)},1)}{\\pi_{A}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{A}^{(0)},1) + \\pi_{B}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{B}^{(0)},1)}.\n$$\nWith $\\pi_{A}^{(0)}=\\pi_{B}^{(0)}$ and equal variances, the common factors cancel:\n$$\nr_{iA}=\\frac{\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}}{2}\\right)}{\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}}{2}\\right)+\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{B}^{(0)})^{2}}{2}\\right)}=\\frac{1}{1+\\exp\\!\\left(\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}-(x_{i}-\\mu_{B}^{(0)})^{2}}{2}\\right)}.\n$$\nFor the dataset $\\{4.0,4.5,5.0,8.0,9.0\\}$ and $(\\mu_{A}^{(0)},\\mu_{B}^{(0)})=(4.2,8.8)$:\n- For $x_{1}=4.0$: $(x_{1}-\\mu_{A}^{(0)})^{2}=0.04$, $(x_{1}-\\mu_{B}^{(0)})^{2}=23.04$, so\n$$\nr_{1A}=\\frac{1}{1+\\exp(-11.5)}\\approx 0.9999898625.\n$$\n- For $x_{2}=4.5$: $(x_{2}-\\mu_{A}^{(0)})^{2}=0.09$, $(x_{2}-\\mu_{B}^{(0)})^{2}=18.49$, so\n$$\nr_{2A}=\\frac{1}{1+\\exp(-9.2)}\\approx 0.9998989606.\n$$\n- For $x_{3}=5.0$: $(x_{3}-\\mu_{A}^{(0)})^{2}=0.64$, $(x_{3}-\\mu_{B}^{(0)})^{2}=14.44$, so\n$$\nr_{3A}=\\frac{1}{1+\\exp(-6.9)}\\approx 0.9989932309.\n$$\n- For $x_{4}=8.0$: $(x_{4}-\\mu_{A}^{(0)})^{2}=14.44$, $(x_{4}-\\mu_{B}^{(0)})^{2}=0.64$, so\n$$\nr_{4A}=\\frac{1}{1+\\exp(6.9)}=1-r_{3A}\\approx 0.0010067691.\n$$\n- For $x_{5}=9.0$: $(x_{5}-\\mu_{A}^{(0)})^{2}=23.04$, $(x_{5}-\\mu_{B}^{(0)})^{2}=0.04$, so\n$$\nr_{5A}=\\frac{1}{1+\\exp(11.5)}=1-r_{1A}\\approx 0.0000101375.\n$$\nNote $r_{1A}+r_{5A}=1$ and $r_{3A}+r_{4A}=1$ by symmetry.\n\nM-step (update mean of component A):\nWith fixed variance, the updated mean is the responsibility-weighted average:\n$$\n\\mu_{A}^{(1)}=\\frac{\\sum_{i=1}^{5} r_{iA} x_{i}}{\\sum_{i=1}^{5} r_{iA}}.\n$$\nCompute the denominator using symmetry:\n$$\n\\sum_{i=1}^{5} r_{iA}=(r_{1A}+r_{5A})+(r_{3A}+r_{4A})+r_{2A}=2+r_{2A}\\approx 2.9998989606.\n$$\nCompute the numerator; group symmetric pairs:\n$$\n\\sum_{i=1}^{5} r_{iA} x_{i}=(r_{1A}\\cdot 4.0 + r_{5A}\\cdot 9.0) + (r_{3A}\\cdot 5.0 + r_{4A}\\cdot 8.0) + r_{2A}\\cdot 4.5.\n$$\nUse $r_{5A}=1-r_{1A}$ and $r_{4A}=1-r_{3A}$:\n$$\nr_{1A}\\cdot 4.0 + r_{5A}\\cdot 9.0 = 9 - 5 r_{1A},\\quad\nr_{3A}\\cdot 5.0 + r_{4A}\\cdot 8.0 = 8 - 3 r_{3A}.\n$$\nThus\n$$\n\\sum_{i=1}^{5} r_{iA} x_{i} = 17 - 5 r_{1A} - 3 r_{3A} + 4.5 r_{2A} \\approx 13.5026163178.\n$$\nTherefore,\n$$\n\\mu_{A}^{(1)}=\\frac{13.5026163178}{2.9998989606}\\approx 4.501023693.\n$$\nRounded to four significant figures, the updated mean is $4.501$.", "answer": "$$\\boxed{4.501}$$", "id": "1960172"}, {"introduction": "While applying known formulas is useful, a deeper understanding comes from deriving them. This exercise challenges you to adapt the M-step for a more complex scenario: a Gaussian mixture where both components are constrained to share the same mean. By deriving the update rule for this shared parameter, you will learn how to correctly set up and solve the maximization problem within the M-step when parameters are not independent, a common feature in practical modeling tasks. [@problem_id:1960133]", "problem": "In a data analysis task, we model a set of one-dimensional observations $X = \\{x_1, x_2, \\dots, x_n\\}$ using a mixture of two normal distributions. A specific constraint is imposed on the model: both distributions must share the same mean $\\mu$. The two components of the mixture are therefore given by $N(\\mu, \\sigma_1^2)$ and $N(\\mu, \\sigma_2^2)$, with mixing proportions $1-\\pi$ and $\\pi$, respectively. The full set of parameters for this model is $\\theta = (\\pi, \\mu, \\sigma_1^2, \\sigma_2^2)$.\n\nThe Expectation-Maximization (EM) algorithm is an iterative method used to find the maximum likelihood estimates for the parameters of such a model. The algorithm alternates between an E-step (Expectation) and an M-step (Maximization). The M-step consists of maximizing the expected complete-data log-likelihood, $Q(\\theta | \\theta^{(t)})$, with respect to the parameters $\\theta$. This function is defined as:\n\n$$Q(\\theta|\\theta^{(t)}) = \\sum_{i=1}^{n} \\left[ \\gamma_{i1}^{(t)} \\ln\\left((1-\\pi) \\phi(x_i; \\mu, \\sigma_1^2)\\right) + \\gamma_{i2}^{(t)} \\ln\\left(\\pi \\phi(x_i; \\mu, \\sigma_2^2)\\right) \\right]$$\n\nwhere $\\phi(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$ is the probability density function of a normal distribution. The terms $\\gamma_{i1}^{(t)}$ and $\\gamma_{i2}^{(t)}$ are the \"responsibilities\" computed in the E-step of iteration $t$; they represent the posterior probability that observation $x_i$ belongs to component 1 or 2, respectively, given the parameter estimates from iteration $t$.\n\nYour task is to derive the update expression for the common mean parameter, $\\mu$. This updated estimate, denoted $\\mu^{(t+1)}$, is the value of $\\mu$ that maximizes the function $Q(\\theta|\\theta^{(t)})$. To simplify the optimization, when deriving the update for $\\mu$, you should treat the variance parameters $\\sigma_1^2$ and $\\sigma_2^2$ as fixed at their current estimates from iteration $t$, which are $(\\sigma_1^2)^{(t)}$ and $(\\sigma_2^2)^{(t)}$.\n\nPresent your final answer as a symbolic expression for $\\mu^{(t+1)}$.", "solution": "We must maximize $Q(\\theta\\mid\\theta^{(t)})$ with respect to the common mean $\\mu$ while holding $(\\sigma_{1}^{2})^{(t)}$ and $(\\sigma_{2}^{2})^{(t)}$ fixed and using the responsibilities $\\gamma_{i1}^{(t)}$ and $\\gamma_{i2}^{(t)}$ from the E-step. The terms involving $\\pi$ do not depend on $\\mu$, so they can be dropped when optimizing over $\\mu$. Using $\\ln\\phi(x;\\mu,\\sigma^{2})=-\\frac{1}{2}\\ln(2\\pi\\sigma^{2})-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}$, the $\\mu$-dependent part of $Q$ is\n$$\nQ_{\\mu}(\\mu)= -\\frac{1}{2}\\sum_{i=1}^{n}\\left[\\gamma_{i1}^{(t)}\\frac{(x_{i}-\\mu)^{2}}{(\\sigma_{1}^{2})^{(t)}}+\\gamma_{i2}^{(t)}\\frac{(x_{i}-\\mu)^{2}}{(\\sigma_{2}^{2})^{(t)}}\\right]+C,\n$$\nwhere $C$ is a constant independent of $\\mu$. Define the weights $w_{i1}=\\gamma_{i1}^{(t)}/(\\sigma_{1}^{2})^{(t)}$ and $w_{i2}=\\gamma_{i2}^{(t)}/(\\sigma_{2}^{2})^{(t)}$. Then\n$$\nQ_{\\mu}(\\mu)= -\\frac{1}{2}\\sum_{i=1}^{n}\\left[(w_{i1}+w_{i2})(x_{i}-\\mu)^{2}\\right]+C.\n$$\nDifferentiate with respect to $\\mu$ and set to zero:\n$$\n\\frac{\\partial Q_{\\mu}}{\\partial \\mu}=-\\sum_{i=1}^{n}(w_{i1}+w_{i2})(\\mu-x_{i})=0\n\\quad\\Longrightarrow\\quad\n\\mu^{(t+1)}=\\frac{\\sum_{i=1}^{n}(w_{i1}+w_{i2})x_{i}}{\\sum_{i=1}^{n}(w_{i1}+w_{i2})}.\n$$\nSubstituting back $w_{i1}$ and $w_{i2}$ gives\n$$\n\\mu^{(t+1)}=\\frac{\\sum_{i=1}^{n}\\left(\\frac{\\gamma_{i1}^{(t)}x_{i}}{(\\sigma_{1}^{2})^{(t)}}+\\frac{\\gamma_{i2}^{(t)}x_{i}}{(\\sigma_{2}^{2})^{(t)}}\\right)}{\\sum_{i=1}^{n}\\left(\\frac{\\gamma_{i1}^{(t)}}{(\\sigma_{1}^{2})^{(t)}}+\\frac{\\gamma_{i2}^{(t)}}{(\\sigma_{2}^{2})^{(t)}}\\right)}.\n$$\nThe second derivative is $\\frac{\\partial^{2} Q_{\\mu}}{\\partial \\mu^{2}}=-\\sum_{i=1}^{n}(w_{i1}+w_{i2})<0$, since variances are positive and $\\gamma_{ik}^{(t)}\\geq 0$, confirming that this critical point is a maximizer.", "answer": "$$\\boxed{\\mu^{(t+1)}=\\frac{\\sum_{i=1}^{n}\\left(\\frac{\\gamma_{i1}^{(t)}x_{i}}{(\\sigma_{1}^{2})^{(t)}}+\\frac{\\gamma_{i2}^{(t)}x_{i}}{(\\sigma_{2}^{2})^{(t)}}\\right)}{\\sum_{i=1}^{n}\\left(\\frac{\\gamma_{i1}^{(t)}}{(\\sigma_{1}^{2})^{(t)}}+\\frac{\\gamma_{i2}^{(t)}}{(\\sigma_{2}^{2})^{(t)}}\\right)}}$$", "id": "1960133"}, {"introduction": "The power of the EM algorithm lies in its flexibility, allowing it to be adapted for a wide range of statistical models. This final practice demonstrates how to extend the framework from Maximum Likelihood Estimation (MLE) to a Bayesian context for Maximum a Posteriori (MAP) estimation. You will work with a mixture of Poisson distributions and incorporate prior knowledge using Gamma distributions, illustrating how to modify the M-step to maximize the expected log-posterior and how prior beliefs can regularize the estimation process. [@problem_id:1960196]", "problem": "A biophysicist is analyzing data from a single-molecule fluorescence experiment. The observed data consists of a series of $n$ independent photon counts, $X_1, X_2, \\dots, X_n$, recorded over consecutive time intervals. The molecule is known to stochastically switch between two distinct conformational states, State 1 and State 2. When in State $k$, the photon counts are modeled by a Poisson distribution with an unknown rate parameter $\\lambda_k$. The probability that any given measurement is from State 1 is a known constant $\\pi$, and from State 2 is $1-\\pi$.\n\nTo estimate the unknown rates $\\lambda_1$ and $\\lambda_2$, an iterative algorithm is used. Since prior knowledge about the system is available, a Bayesian approach is adopted. The goal is to find the Maximum a Posteriori (MAP) estimates of the parameters, not just the maximum likelihood estimates. The prior distributions for the rate parameters are chosen to be Gamma distributions, as they are conjugate to the Poisson likelihood. Specifically, the prior for $\\lambda_k$ is $\\text{Gamma}(\\alpha_k, \\beta_k)$, with the probability density function given by $p(\\lambda_k) \\propto \\lambda_k^{\\alpha_k-1} \\exp(-\\beta_k \\lambda_k)$ for $\\lambda_k > 0$.\n\nThe estimation is performed using a modified version of the Expectation-Maximization (EM) algorithm. The standard M-step, which maximizes the expected complete-data log-likelihood, is replaced with a step that maximizes the expected complete-data log-posterior.\n\nLet $\\theta^{(t)} = (\\lambda_1^{(t)}, \\lambda_2^{(t)})$ be the parameter estimates at iteration $t$. In the Expectation step (E-step), one computes the \"responsibilities,\" which are the posterior probabilities that observation $X_i$ belongs to State $k$, given the data and the current parameter estimates. Let's denote the responsibility for State 1 as $\\gamma_{i,1}^{(t)} = P(\\text{State}=1 | X_i, \\theta^{(t)})$.\n\nYour task is to derive the update equation for the parameter $\\lambda_1$ during the modified Maximization step (M-step). Express the updated estimate $\\lambda_1^{(t+1)}$ in terms of the observed data $\\{X_i\\}_{i=1}^n$, the responsibilities $\\{\\gamma_{i,1}^{(t)}\\}_{i=1}^n$, and the hyperparameters of the prior distribution for $\\lambda_1$, which are $\\alpha_1$ and $\\beta_1$.", "solution": "Introduce latent indicators $z_{i,1} \\in \\{0,1\\}$ with $z_{i,1}=1$ if observation $X_{i}$ comes from State 1 and $z_{i,1}=0$ otherwise. The complete-data log-likelihood terms involving $\\lambda_{1}$ are\n$$\n\\sum_{i=1}^{n} z_{i,1} \\left( X_{i} \\ln \\lambda_{1} - \\lambda_{1} \\right) + \\text{const},\n$$\nsince for a Poisson model, $\\ln p(X_{i} \\mid \\lambda_{1}) = X_{i} \\ln \\lambda_{1} - \\lambda_{1} - \\ln X_{i}!$.\nThe log-prior for $\\lambda_{1} \\sim \\text{Gamma}(\\alpha_{1},\\beta_{1})$ (shape-rate) is\n$$\n(\\alpha_{1}-1)\\ln \\lambda_{1} - \\beta_{1} \\lambda_{1} + \\text{const}.\n$$\nIn the E-step, replace $z_{i,1}$ by its conditional expectation given current parameters, $\\gamma_{i,1}^{(t)} = P(\\text{State}=1 \\mid X_{i}, \\theta^{(t)})$. The expected complete-data log-posterior in $\\lambda_{1}$ is then\n$$\nQ(\\lambda_{1}) = \\left( \\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i} \\right) \\ln \\lambda_{1} - \\left( \\beta_{1} + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} \\right) \\lambda_{1} + \\text{const}.\n$$\nDifferentiate with respect to $\\lambda_{1}$, set to zero, and solve:\n$$\n\\frac{\\partial Q}{\\partial \\lambda_{1}} = \\frac{\\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i}}{\\lambda_{1}} - \\left( \\beta_{1} + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} \\right) = 0\n$$\nwhich yields\n$$\n\\lambda_{1}^{(t+1)} = \\frac{\\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i}}{\\beta_{1} + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)}}.\n$$\nThe second derivative is\n$$\n\\frac{\\partial^{2} Q}{\\partial \\lambda_{1}^{2}} = -\\frac{\\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i}}{\\lambda_{1}^{2}},\n$$\nwhich is negative whenever the numerator is positive, confirming a maximum under the usual conditions.", "answer": "$$\\boxed{\\frac{\\alpha_{1}-1+\\sum_{i=1}^{n}\\gamma_{i,1}^{(t)} X_{i}}{\\beta_{1}+\\sum_{i=1}^{n}\\gamma_{i,1}^{(t)}}}$$", "id": "1960196"}]}