{"hands_on_practices": [{"introduction": "The success of multiple imputation hinges on choosing an imputation model that respects the nature of your data. A common challenge arises with variables constrained to a specific range, such as proportions, which must lie between 0 and 1. This practice explores statistically sound strategies for imputing such bounded variables, highlighting why a naive approach like standard linear regression can lead to invalid results and introducing more appropriate alternatives [@problem_id:1938743].", "problem": "A social scientist is analyzing a household survey dataset to understand expenditure patterns. The dataset includes the following fully observed variables for each household:\n- `LogIncome`: The natural logarithm of the total monthly household income.\n- `HouseholdSize`: The number of individuals in the household.\n\nThe primary variable of interest is `FoodProportion`, which represents the proportion of the monthly income spent on food. This variable is continuous and theoretically constrained to the interval $[0, 1]$. However, due to non-responses in the survey, the `FoodProportion` variable has missing values. The researcher decides to use Multiple Imputation (MI) to handle the missing data before proceeding with their main analysis.\n\nSeveral imputation strategies are proposed for the `FoodProportion` variable, using `LogIncome` and `HouseholdSize` as predictors.\n\nA. Impute `FoodProportion` directly using a standard linear regression model.\n\nB. Use Predictive Mean Matching (PMM), where the underlying model is a linear regression of `FoodProportion` on the predictors.\n\nC. Model `FoodProportion` using Beta regression with a logit link function for the mean.\n\nD. Apply an arcsine square root transformation to `FoodProportion` (i.e., $Y' = \\arcsin(\\sqrt{\\text{FoodProportion}})$), impute the transformed variable $Y'$ using linear regression, and then back-transform the results.\n\nE. Apply a logit transformation to `FoodProportion` (i.e., $Y' = \\ln(\\frac{\\text{FoodProportion}}{1-\\text{FoodProportion}})$), impute the transformed variable $Y'$ using linear regression, and then back-transform the results.\n\nF. Discretize `FoodProportion` into three categories ('Low', 'Medium', 'High') and then impute the missing categories using multinomial logistic regression.\n\nAssuming there are no observed values of `FoodProportion` that are exactly 0 or 1, select all of the strategies from the list above that are statistically appropriate for this task.", "solution": "The target variable is a continuous proportion with support $[0,1]$, and the problem states that no observed values are exactly $0$ or $1$, so effectively the observed data lie in $(0,1)$. For multiple imputation to be statistically appropriate, the imputation model should (i) generate imputations that respect the variable’s support, (ii) be congenial with the data-generating structure for a fractional response, and (iii) avoid ad hoc truncation that can bias imputations and distort uncertainty.\n\nEvaluate each proposed strategy:\n\nA. Standard linear regression imputation on the raw proportion does not constrain imputations to $[0,1]$. Under a normal-error linear model, predicted values plus stochastic residual draws can fall outside $[0,1]$, violating the support. Post hoc truncation at $0$ and $1$ is ad hoc and can induce bias and improper uncertainty quantification. Therefore, this is not statistically appropriate.\n\nB. Predictive mean matching (PMM) uses a linear model to define neighborhoods but imputes actual observed donor values. Because all donors lie in $(0,1)$ by the problem’s assumption, PMM preserves the support and is robust to non-normality and heteroskedasticity. Under missing at random and correct specification of predictors, PMM is statistically appropriate for bounded continuous outcomes. Hence, B is appropriate.\n\nC. Beta regression with a logit link models a response supported on $(0,1)$ using the Beta distribution, which has density on $(0,1)$ and accommodates mean-variance relationships typical for proportions. With no zeros or ones observed, the Beta likelihood is well-defined, and proper Bayesian or approximate Bayesian draws within MI yield imputations in $(0,1)$. Therefore, C is appropriate.\n\nD. The arcsine square root transform $Y'=\\arcsin(\\sqrt{p})$ maps $p\\in(0,1)$ to $Y'\\in(0,\\frac{\\pi}{2})$. Linear imputation on $Y'$ can produce values of $Y'$ outside $(0,\\frac{\\pi}{2})$, but the back-transformation $p=\\sin^{2}(Y')$ maps any real $Y'$ into $[0,1]$. Despite preserving the bound after back-transform, this transform is a variance-stabilizing device for binomial proportions with known denominators, relying on variance proportional to $p(1-p)/n$. For general continuous proportions without known $n$, it lacks a principled distributional justification and can induce nonmonotone relationships after back-transform if the imputed $Y'$ strays outside the principal interval, leading to distortions. Given the availability of correctly specified bounded models (B, C, E), this approach is not considered statistically appropriate.\n\nE. The logit transform $Y'=\\ln\\!\\left(\\frac{p}{1-p}\\right)$ maps $(0,1)$ to $\\mathbb{R}$. Linear regression imputation on $Y'$ with stochastic residuals is well-defined on $\\mathbb{R}$, and the inverse-logit back-transformation $p=\\frac{\\exp(Y')}{1+\\exp(Y')}$ guarantees imputations in $(0,1)$. With no zeros or ones observed, this approach is standard and statistically appropriate for fractional responses. Hence, E is appropriate.\n\nF. Discretizing a continuous proportion into categories and imputing via multinomial logistic regression discards within-category information, induces measurement error, and creates incongeniality if the analysis uses the variable as continuous. This can bias parameter estimates and standard errors. Therefore, F is not statistically appropriate for imputing a continuous proportion.\n\nTherefore, the statistically appropriate strategies are B, C, and E.", "answer": "$$\\boxed{BCE}$$", "id": "1938743"}, {"introduction": "After generating imputations using an appropriate model, how do you verify that the results are plausible? This exercise demonstrates the importance of diagnostic checks, a crucial step for building confidence in your imputation procedure [@problem_id:1938796]. You will learn to identify the most effective graphical method for comparing the distribution of the original, observed data with the newly generated complete data, ensuring that your imputation model is behaving as expected.", "problem": "A clinical researcher is analyzing a dataset from a health study. One of the key continuous variables, \"Systolic Blood Pressure\" (SBP), has a number of missing values. The researcher assumes that the mechanism leading to the missing data is Missing At Random (MAR), meaning the probability of a value being missing may depend on other observed patient variables. To address this, the researcher employs Multiple Imputation by Chained Equations (MICE) to generate $m=5$ distinct, complete versions of the dataset.\n\nA critical step in this process is to perform a diagnostic check to ensure the imputed SBP values are plausible. The researcher's goal is to graphically compare the marginal distribution of the originally observed SBP values with the marginal distributions of the SBP variable in the five newly completed datasets.\n\nWhich of the following graphical methods is the most effective and comprehensive for this specific purpose?\n\nA. Creating five separate box plots, one for the SBP variable from each of the five completed datasets.\n\nB. Constructing a single scatter plot of the imputed SBP values versus the patient's age, using data pooled from all five imputations.\n\nC. Plotting a single histogram that combines all the observed SBP values with all the imputed SBP values from all five datasets into one large sample.\n\nD. Generating a single plot that overlays six different density curves: one solid line for the distribution of the original observed SBP values, and five semi-transparent lines, one for the distribution of the full SBP variable (observed plus imputed values) from each of the five completed datasets.\n\nE. Calculating the mean and variance of SBP for the observed data and for each of the five completed datasets and presenting them in a table.", "solution": "The core task is to find the best graphical method to compare the distribution of an observed variable with the distributions of the same variable after multiple imputation. An effective method should allow for a direct comparison of distributional shapes (center, spread, skewness, modality) and should also visualize the uncertainty introduced by the imputation process (i.e., the differences between the $m=5$ imputed datasets).\n\nLet's evaluate each option based on these criteria:\n\n*   **Option A**: This involves creating five separate box plots. Box plots provide a five-number summary of a distribution. While useful, they can obscure important distributional features like bimodality. Furthermore, having five separate plots makes a direct comparison to the distribution of the original observed data cumbersome. This method is less informative than one that shows the full distributional shape.\n\n*   **Option B**: This suggests a scatter plot of imputed SBP versus age. This type of plot is a crucial diagnostic for checking if the imputation model has preserved the *conditional relationship* (or bivariate distribution) between SBP and age. However, the problem explicitly asks for a method to check the *marginal distribution* of the SBP variable itself. Therefore, this plot does not address the primary goal.\n\n*   **Option C**: Combining all observed and imputed values into a single histogram is a poor diagnostic technique. The purpose of this check is to *compare* the distribution of observed data to the distribution of imputed data, not to merge them. Merging the data into one histogram makes it impossible to detect if, for example, the imputed values have a different mean or shape than the observed values. This method conceals potential problems with the imputation model.\n\n*   **Option D**: This describes overlaying multiple density plots. This is the standard and most effective practice for this diagnostic task. A density plot provides a smooth estimate of the entire probability distribution, revealing its shape, center, spread, and any modes. By plotting the density of the observed data as a solid reference line and overlaying the densities of the five completed datasets, the researcher can perform an immediate and detailed visual comparison. This approach accomplishes two key goals simultaneously:\n    1.  It allows a check of whether the distributions of the completed datasets are similar to the distribution of the observed data. Large discrepancies could indicate a problem with the imputation model.\n    2.  The spread among the five semi-transparent lines for the completed datasets provides a direct visualization of the imputation uncertainty. If the lines are very different, it indicates high uncertainty in the imputed values.\n\n*   **Option E**: This option suggests presenting numerical summaries (mean and variance) in a table. While this is a valuable quantitative check, the question specifically asks for a *graphical method*. This option fails to meet this requirement. Furthermore, summary statistics like mean and variance do not capture the full shape of the distribution, which is a key aspect of this diagnostic check.\n\nBased on this analysis, Option D is the most comprehensive and appropriate graphical method for the researcher's stated goal. It directly compares the full marginal distributions while also illustrating the variability across the multiple imputations.", "answer": "$$\\boxed{D}$$", "id": "1938796"}, {"introduction": "With several complete datasets in hand and having checked their plausibility, the final step is to combine their results into a single, unified inference. This is accomplished using a set of formulas developed by Donald Rubin, collectively known as Rubin's rules. This practice provides a concrete, hands-on example of applying these rules to pool regression coefficients and calculate a total variance that properly reflects the uncertainty due to missing data [@problem_id:1938746].", "problem": "A data analyst is investigating the relationship between a person's average weekly screen time (in hours) and their score on a standardized wellness questionnaire. Due to survey non-response, a portion of the screen time data is missing. To address this, the analyst employs multiple imputation, creating $m=5$ distinct, complete datasets.\n\nFor each of the $m=5$ datasets, a simple linear regression analysis is performed. The analysis yields an estimated regression coefficient, $\\hat{\\beta}_j$, representing the change in wellness score per additional hour of weekly screen time, and its corresponding estimated variance, $U_j$. The results from the five separate analyses are as follows:\n\n- Dataset 1: $\\hat{\\beta}_1 = -0.45$, $U_1 = 0.012$\n- Dataset 2: $\\hat{\\beta}_2 = -0.51$, $U_2 = 0.015$\n- Dataset 3: $\\hat{\\beta}_3 = -0.48$, $U_3 = 0.014$\n- Dataset 4: $\\hat{\\beta}_4 = -0.55$, $U_4 = 0.013$\n- Dataset 5: $\\hat{\\beta}_5 = -0.46$, $U_5 = 0.011$\n\nUsing Rubin's rules for combining estimates from multiple imputations, calculate the final pooled point estimate for the regression coefficient and its associated total variance. Provide your answer as two numerical values: the pooled coefficient first, followed by the total variance. Round both of your final answers to three significant figures.", "solution": "We use Rubin's rules for multiple imputation. Let $m$ be the number of imputations, $\\hat{\\beta}_{j}$ the point estimates, and $U_{j}$ their estimated variances. The pooled point estimate is\n$$\n\\bar{\\beta}=\\frac{1}{m}\\sum_{j=1}^{m}\\hat{\\beta}_{j},\n$$\nthe within-imputation variance is\n$$\n\\bar{U}=\\frac{1}{m}\\sum_{j=1}^{m}U_{j},\n$$\nthe between-imputation variance is\n$$\nB=\\frac{1}{m-1}\\sum_{j=1}^{m}\\left(\\hat{\\beta}_{j}-\\bar{\\beta}\\right)^{2},\n$$\nand the total variance is\n$$\nT=\\bar{U}+\\left(1+\\frac{1}{m}\\right)B.\n$$\n\nGiven $m=5$, $\\hat{\\beta}_{j}\\in\\{-0.45,-0.51,-0.48,-0.55,-0.46\\}$ and $U_{j}\\in\\{0.012,0.015,0.014,0.013,0.011\\}$:\n\n1) Pooled estimate:\n$$\n\\sum_{j=1}^{5}\\hat{\\beta}_{j}=-0.45-0.51-0.48-0.55-0.46=-2.45,\\quad\n\\bar{\\beta}=\\frac{-2.45}{5}=-0.49.\n$$\n\n2) Within-imputation variance:\n$$\n\\sum_{j=1}^{5}U_{j}=0.012+0.015+0.014+0.013+0.011=0.065,\\quad\n\\bar{U}=\\frac{0.065}{5}=0.013.\n$$\n\n3) Between-imputation variance. Deviations from $\\bar{\\beta}$:\n$$\n0.04,\\,-0.02,\\,0.01,\\,-0.06,\\,0.03,\n$$\nsquared and summed:\n$$\n0.04^{2}+(-0.02)^{2}+0.01^{2}+(-0.06)^{2}+0.03^{2}=0.0016+0.0004+0.0001+0.0036+0.0009=0.0066,\n$$\nthus\n$$\nB=\\frac{0.0066}{5-1}=0.00165.\n$$\n\n4) Total variance:\n$$\nT=\\bar{U}+\\left(1+\\frac{1}{5}\\right)B=0.013+1.2\\times 0.00165=0.013+0.00198=0.01498.\n$$\n\nRounding to three significant figures gives $\\bar{\\beta}=-0.490$ and $T=0.0150$.", "answer": "$$\\boxed{\\begin{pmatrix}-0.490 & 0.0150\\end{pmatrix}}$$", "id": "1938746"}]}