{"hands_on_practices": [{"introduction": "Before we can minimize the maximum risk, we must first be comfortable calculating an estimator's risk function. This exercise [@problem_id:1935839] provides a foundational example using the Poisson distribution. You will see how the risk of a simple, intuitive estimator depends directly on the unknown parameter, and more importantly, that this risk can grow without bound, highlighting the potential pitfalls of an estimator and motivating the search for strategies that control for this worst-case behavior.", "problem": "A data scientist is modeling the number of fraudulent transactions, $X$, detected by an automated system in a 24-hour period. Based on historical data, it is assumed that $X$ follows a Poisson distribution with an unknown mean parameter $\\lambda > 0$, where $\\lambda$ represents the true average number of fraudulent transactions per day.\n\nThe data scientist proposes a simple estimator, $\\delta(X)$, for the unknown mean $\\lambda$. The chosen estimator is the observed number of fraudulent transactions itself, so $\\delta(X) = X$.\n\nThe performance of this estimator is evaluated using a squared error loss function, defined as $L(\\lambda, \\delta(X)) = (\\delta(X) - \\lambda)^2$. The risk of the estimator, denoted by $R(\\lambda, \\delta)$, is the expected value of this loss function, calculated with respect to the distribution of $X$. That is, $R(\\lambda, \\delta) = E[L(\\lambda, \\delta(X))]$.\n\nWhich of the following statements correctly describes the risk function $R(\\lambda, \\delta)$ for the estimator $\\delta(X) = X$ over its domain $\\lambda > 0$?\n\nA. The risk is constant for all $\\lambda > 0$.\n\nB. The risk is a strictly decreasing function of $\\lambda$.\n\nC. The risk is a strictly increasing function of $\\lambda$ and is bounded above.\n\nD. The risk is a strictly increasing function of $\\lambda$ and is unbounded.\n\nE. The risk is a non-monotonic function of $\\lambda$ (i.e., it both increases and decreases over its domain).", "solution": "We are given $X \\sim \\text{Poisson}(\\lambda)$ with $\\lambda>0$ and the estimator $\\delta(X)=X$ under squared error loss $L(\\lambda,\\delta(X))=(\\delta(X)-\\lambda)^{2}$. The risk is\n$$\nR(\\lambda,\\delta)=\\mathbb{E}\\big[(\\delta(X)-\\lambda)^{2}\\big]=\\mathbb{E}\\big[(X-\\lambda)^{2}\\big].\n$$\nUsing the bias-variance decomposition,\n$$\n\\mathbb{E}\\big[(X-\\lambda)^{2}\\big]=\\operatorname{Var}(X)+\\big(\\mathbb{E}[X]-\\lambda\\big)^{2}.\n$$\nFor a Poisson distribution, $\\mathbb{E}[X]=\\lambda$ and $\\operatorname{Var}(X)=\\lambda$. Therefore,\n$$\nR(\\lambda,\\delta)=\\lambda+\\big(\\lambda-\\lambda\\big)^{2}=\\lambda.\n$$\nThus $R(\\lambda,\\delta)=\\lambda$ for $\\lambda>0$, which is strictly increasing since\n$$\n\\frac{d}{d\\lambda}R(\\lambda,\\delta)=1>0,\n$$\nand it is unbounded as $\\lambda \\to \\infty$. Hence, the correct description is that the risk is a strictly increasing function of $\\lambda$ and is unbounded.", "answer": "$$\\boxed{D}$$", "id": "1935839"}, {"introduction": "An elegant way to satisfy the minimax criterion is to find an estimator whose risk is constant across all possible values of the parameter; such an estimator has no \"worst-case\" scenario. This practice [@problem_id:1935795] explores a classic scenario where, by using a standardized loss function common in fields like population genetics, a standard estimator achieves exactly this desirable constant-risk property. This immediately establishes the estimator as minimax without needing to find and minimize a maximum risk.", "problem": "A population geneticist is investigating the frequency of a specific allele 'A' at a gene locus within a large, randomly mating population. The true, but unknown, frequency of allele 'A' is denoted by $p$, where $0 < p < 1$. To estimate $p$, the geneticist collects a random sample of $n$ alleles from the population and finds that $X$ of them are of type 'A'. The number of 'A' alleles, $X$, can be modeled as a random variable following a binomial distribution with parameters $n$ and $p$.\n\nThe geneticist uses the sample proportion, $\\hat{p} = X/n$, as an estimator for $p$. In this field, it is common to evaluate estimators using a standardized loss function that accounts for the intrinsic population variance. This loss function is given by:\n$$L(p, a) = \\frac{(a-p)^2}{p(1-p)}$$\nwhere $a$ is the estimated allele frequency.\n\nThe risk of the estimator $\\hat{p}$ is defined as the expected value of the loss function, $R(p, \\hat{p}) = E[L(p, \\hat{p})]$. Determine the risk function for the estimator $\\hat{p}$.\n\nA. $R(p, \\hat{p}) = \\frac{1}{n^2}$\n\nB. $R(p, \\hat{p}) = \\frac{p(1-p)}{n}$\n\nC. $R(p, \\hat{p}) = \\frac{1}{n}$\n\nD. $R(p, \\hat{p}) = 1$\n\nE. $R(p, \\hat{p}) = \\frac{1}{n \\cdot p(1-p)}$", "solution": "We are given a population allele frequency parameter $p$ with $0<p<1$, a sample of $n$ alleles, and $X \\sim \\text{Binomial}(n,p)$. The estimator is $\\hat{p} = X/n$. The loss function is\n$$\nL(p,a) = \\frac{(a-p)^{2}}{p(1-p)}.\n$$\nThe risk of $\\hat{p}$ is the expected loss with respect to the sampling distribution of $X$ (equivalently, of $\\hat{p}$):\n$$\nR(p,\\hat{p}) = \\mathbb{E}\\left[ L(p,\\hat{p}) \\right] = \\mathbb{E}\\left[ \\frac{(\\hat{p}-p)^{2}}{p(1-p)} \\right].\n$$\nSince the expectation is over the randomness in $X$ (and thus in $\\hat{p}$) while $p$ is a fixed parameter, the denominator $p(1-p)$ is a constant with respect to the expectation. Therefore,\n$$\nR(p,\\hat{p}) = \\frac{1}{p(1-p)} \\, \\mathbb{E}\\left[ (\\hat{p}-p)^{2} \\right].\n$$\nWe recognize $\\mathbb{E}\\left[ (\\hat{p}-p)^{2} \\right]$ as the mean squared error of $\\hat{p}$, which decomposes as\n$$\n\\mathbb{E}\\left[ (\\hat{p}-p)^{2} \\right] = \\operatorname{Var}(\\hat{p}) + \\left( \\mathbb{E}[\\hat{p}] - p \\right)^{2}.\n$$\nFor $X \\sim \\text{Binomial}(n,p)$, we have $\\mathbb{E}[X] = np$ and $\\operatorname{Var}(X) = np(1-p)$. Thus,\n$$\n\\mathbb{E}[\\hat{p}] = \\mathbb{E}\\left[ \\frac{X}{n} \\right] = \\frac{\\mathbb{E}[X]}{n} = p,\n$$\nso the bias is zero, and\n$$\n\\operatorname{Var}(\\hat{p}) = \\operatorname{Var}\\left( \\frac{X}{n} \\right) = \\frac{\\operatorname{Var}(X)}{n^{2}} = \\frac{np(1-p)}{n^{2}} = \\frac{p(1-p)}{n}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\left[ (\\hat{p}-p)^{2} \\right] = \\frac{p(1-p)}{n}.\n$$\nSubstituting into the risk,\n$$\nR(p,\\hat{p}) = \\frac{1}{p(1-p)} \\cdot \\frac{p(1-p)}{n} = \\frac{1}{n}.\n$$\nThis matches option C.", "answer": "$$\\boxed{C}$$", "id": "1935795"}, {"introduction": "Often, we must choose between several competing estimators, none of which have constant risk. The minimax principle guides our choice by directing us to select the estimator with the smallest maximum, or supremum, risk. This problem [@problem_id:1935772] provides a hands-on comparison of two estimators for the parameter of a uniform distribution, where you will directly calculate and compare their maximum risks and discover that a carefully modified (and biased) estimator can actually be superior from a minimax perspective.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample from a continuous uniform distribution on the interval $[0, \\theta]$, where $\\theta > 0$ is an unknown parameter. The performance of an estimator $\\delta$ for $\\theta$ is evaluated using the scaled squared error loss function, defined as $L(\\delta, \\theta) = \\left(\\frac{\\delta - \\theta}{\\theta}\\right)^2$. The risk of an estimator $\\delta$ is its expected loss, given by $R(\\theta, \\delta) = E[L(\\delta, \\theta)]$.\n\nLet $X_{(n)}$ denote the maximum value in the sample, i.e., $X_{(n)} = \\max\\{X_1, X_2, \\dots, X_n\\}$.\n\nConsider the following two estimators for $\\theta$:\n1.  $\\hat{\\theta}_1 = X_{(n)}$\n2.  $\\hat{\\theta}_2 = \\frac{n+2}{n+1} X_{(n)}$\n\nFor each estimator $\\hat{\\theta}_i$, its maximum risk is defined as $R_{\\text{max}, i} = \\sup_{\\theta > 0} R(\\theta, \\hat{\\theta}_i)$.\n\nCalculate the ratio $\\frac{R_{\\text{max}, 2}}{R_{\\text{max}, 1}}$. Your final answer should be a closed-form expression in terms of the sample size $n$.", "solution": "Let $X_{1},\\dots,X_{n}\\sim \\text{Uniform}(0,\\theta)$ i.i.d. and $X_{(n)}=\\max_{1\\leq i\\leq n}X_{i}$. For estimators of the form $\\delta=cX_{(n)}$, the scaled squared error loss is\n$$\nL(\\delta,\\theta)=\\left(\\frac{\\delta-\\theta}{\\theta}\\right)^{2}=\\left(c\\frac{X_{(n)}}{\\theta}-1\\right)^{2}.\n$$\nDefine $Y=X_{(n)}/\\theta$. The distribution of $Y$ does not depend on $\\theta$; indeed $Y$ has density $f_{Y}(y)=n y^{n-1}$ for $0\\leq y\\leq 1$ (i.e., $Y\\sim \\text{Beta}(n,1)$). Therefore the risk is\n$$\nR(\\theta,cX_{(n)})=E\\left[\\left(cY-1\\right)^{2}\\right]=c^{2}E[Y^{2}]-2cE[Y]+1,\n$$\nwhich is independent of $\\theta$, so the maximum over $\\theta>0$ equals this constant value.\n\nCompute the moments of $Y$:\n$$\nE[Y]=\\int_{0}^{1} y\\cdot n y^{n-1}\\,dy=n\\int_{0}^{1} y^{n}\\,dy=\\frac{n}{n+1},\n$$\n$$\nE[Y^{2}]=\\int_{0}^{1} y^{2}\\cdot n y^{n-1}\\,dy=n\\int_{0}^{1} y^{n+1}\\,dy=\\frac{n}{n+2}.\n$$\nHence, for general $c$,\n$$\nR(c)=c^{2}\\frac{n}{n+2}-2c\\frac{n}{n+1}+1.\n$$\n\nFor $\\hat{\\theta}_{1}=X_{(n)}$ we have $c=1$, so\n$$\nR_{\\text{max},1}=R(1)=\\frac{n}{n+2}-2\\frac{n}{n+1}+1=\\frac{2}{(n+1)(n+2)}.\n$$\nFor $\\hat{\\theta}_{2}=\\frac{n+2}{n+1}X_{(n)}$ we have $c=\\frac{n+2}{n+1}$, so\n$$\nR_{\\text{max},2}=R\\!\\left(\\frac{n+2}{n+1}\\right)=\\frac{n}{n+2}\\left(\\frac{n+2}{n+1}\\right)^{2}-2\\frac{n}{n+1}\\left(\\frac{n+2}{n+1}\\right)+1\n=1-\\frac{n(n+2)}{(n+1)^{2}}=\\frac{1}{(n+1)^{2}}.\n$$\n\nTherefore the desired ratio is\n$$\n\\frac{R_{\\text{max},2}}{R_{\\text{max},1}}=\\frac{\\frac{1}{(n+1)^{2}}}{\\frac{2}{(n+1)(n+2)}}=\\frac{n+2}{2(n+1)}.\n$$", "answer": "$$\\boxed{\\frac{n+2}{2(n+1)}}$$", "id": "1935772"}]}