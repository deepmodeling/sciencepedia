{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of the Stein Paradox, we first need to master the basic application of the James-Stein estimator. This initial practice problem [@problem_id:1956802] walks you through the fundamental calculation, applying the shrinkage formula to a vector of observations. By working through this example, you will build a concrete understanding of how the estimator modifies the raw data, pulling each component towards the origin based on the collective magnitude of all observations.", "problem": "A biostatistician is analyzing data from a high-throughput experiment that simultaneously measures the expression levels of several genes. The vector of measurements, $X = (X_1, X_2, \\dots, X_p)^T$, is modeled as a single draw from a $p$-variate normal distribution, $N_p(\\theta, \\sigma^2 I_p)$. In this model, $\\theta = (\\theta_1, \\theta_2, \\dots, \\theta_p)^T$ represents the vector of the true, unknown mean expression levels for the genes, and $I_p$ is the $p \\times p$ identity matrix. The measurement process is known to have a common variance of $\\sigma^2$ for each gene, and the measurements are statistically independent.\n\nFor a particular experiment with $p=5$ genes, the common variance is known to be $\\sigma^2 = 2.25$. The observed measurement vector is obtained as:\n$$\nx = \\begin{pmatrix} 4.1 \\\\ -2.3 \\\\ 0.8 \\\\ 5.0 \\\\ -1.5 \\end{pmatrix}\n$$\nTo estimate the true mean vector $\\theta$, the statistician decides to use the James-Stein estimator, $\\hat{\\theta}_{JS}$. For an observation $x$ from $N_p(\\theta, \\sigma^2 I_p)$, this estimator is defined as:\n$$\n\\hat{\\theta}_{JS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|x\\|^2}\\right) x\n$$\nwhere $\\|x\\|^2 = \\sum_{i=1}^{p} x_i^2$ is the squared Euclidean norm of the vector $x$. This estimator is notable for having a uniformly lower total mean squared error than the standard Maximum Likelihood Estimator (MLE), which is simply $x$, when $p \\ge 3$.\n\nCalculate the James-Stein estimate, $\\hat{\\theta}_{JS}$, for the true mean expression levels based on the given data. Express your final answer as a row vector, with each component rounded to three significant figures.", "solution": "We are given a single observation $x \\in \\mathbb{R}^{p}$ from $N_{p}(\\theta,\\sigma^{2}I_{p})$ and the James-Stein estimator\n$$\n\\hat{\\theta}_{JS}=\\left(1-\\frac{(p-2)\\sigma^{2}}{\\|x\\|^{2}}\\right)x,\n$$\nwhere $\\|x\\|^{2}=\\sum_{i=1}^{p}x_{i}^{2}$. For this problem, $p=5$, $\\sigma^{2}=2.25$, and\n$$\nx=\\begin{pmatrix}4.1\\\\-2.3\\\\0.8\\\\5.0\\\\-1.5\\end{pmatrix}.\n$$\nFirst compute the squared norm:\n$$\n\\|x\\|^{2}=4.1^{2}+(-2.3)^{2}+0.8^{2}+5.0^{2}+(-1.5)^{2}=16.81+5.29+0.64+25+2.25=49.99.\n$$\nCompute the shrinkage factor:\n$$\n1-\\frac{(p-2)\\sigma^{2}}{\\|x\\|^{2}}=1-\\frac{3\\cdot 2.25}{49.99}=1-\\frac{6.75}{49.99}=\\frac{4999-675}{4999}=\\frac{4324}{4999}\\approx 0.8649729946.\n$$\nTherefore,\n$$\n\\hat{\\theta}_{JS}=\\left(\\frac{4324}{4999}\\right)x.\n$$\nApply this scalar to each component:\n$$\n\\hat{\\theta}_{JS,1}=4.1\\cdot\\frac{4324}{4999}\\approx 3.5463893,\\quad\n\\hat{\\theta}_{JS,2}=-2.3\\cdot\\frac{4324}{4999}\\approx -1.9894379,\n$$\n$$\n\\hat{\\theta}_{JS,3}=0.8\\cdot\\frac{4324}{4999}\\approx 0.6919784,\\quad\n\\hat{\\theta}_{JS,4}=5.0\\cdot\\frac{4324}{4999}\\approx 4.324865,\n$$\n$$\n\\hat{\\theta}_{JS,5}=-1.5\\cdot\\frac{4324}{4999}=-\\frac{6486}{4999}\\approx -1.2974595.\n$$\nRounding each component to three significant figures gives\n$$\n\\begin{pmatrix}\n3.55 & -1.99 & 0.692 & 4.32 & -1.30\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 3.55 & -1.99 & 0.692 & 4.32 & -1.30 \\end{pmatrix}}$$", "id": "1956802"}, {"introduction": "Now that you are familiar with the basic computation, we can investigate some of the estimator's more peculiar behaviors. This exercise [@problem_id:1956809] presents a scenario where the observed data lies close to the origin, leading to a surprising outcome. You will discover the phenomenon of \"overshrinkage,\" where the shrinkage factor becomes negative, and see how this can cause the James-Stein estimate to point in the opposite direction of the observation, a key insight that highlights the limitations of the standard formula.", "problem": "An astrophysicist is measuring faint microwave anisotropies in five different, non-overlapping regions of the sky. The theoretical \"null model\" predicts a perfectly uniform cosmic microwave background, implying that the true mean anisotropy $\\theta_i$ in each region $i$ is zero. Thus, the true mean vector is hypothesized to be $\\theta = (0, 0, 0, 0, 0)$.\n\nThe experimental measurements, after normalization and subtraction of the global mean, are collected into a vector $X = (x_1, x_2, x_3, x_4, x_5)$. Due to instrumental noise and quantum fluctuations, the measurement process is modeled by a 5-dimensional multivariate normal distribution, $X \\sim N_5(\\theta, I_5)$, where $\\theta \\in \\mathbb{R}^5$ is the unknown mean vector and $I_5$ is the $5 \\times 5$ identity matrix.\n\nThe conventional estimate for $\\theta$ is the Maximum Likelihood Estimate (MLE), which is simply the observed vector $X$. An alternative is the James-Stein estimator, defined for $p \\ge 3$ dimensions as:\n$$ \\delta_{JS}(X) = \\left(1 - \\frac{p-2}{\\|X\\|^2}\\right)X $$\nwhere $p$ is the dimension of the vector $X$ and $\\|X\\|^2 = \\sum_{i=1}^p x_i^2$. This estimator is known to have a lower total mean squared error than the MLE when averaged over all possible values of $\\theta$.\n\nSuppose the experimental observation yields the vector $X = (1.10, 0.60, -0.40, 0.30, 0.20)$.\n\nCalculate the numerical values for the first component and the second component of the James-Stein estimate, $\\delta_{JS}(X)$, based on this observation. Present your answers as a pair, ordered from the first to the second component. Round your final answers for both components to three significant figures.", "solution": "We use the James-Stein estimator for dimension $p=5$:\n$$\n\\delta_{JS}(X)=\\left(1-\\frac{p-2}{\\|X\\|^{2}}\\right)X.\n$$\nGiven $X=(1.10,0.60,-0.40,0.30,0.20)$, first compute the squared norm:\n$$\n\\|X\\|^{2}=\\sum_{i=1}^{5}x_{i}^{2}=1.10^{2}+0.60^{2}+(-0.40)^{2}+0.30^{2}+0.20^{2}=1.21+0.36+0.16+0.09+0.04=1.86.\n$$\nWrite $1.86=\\frac{93}{50}$. With $p=5$, we have $p-2=3$, so the shrinkage factor is\n$$\n1-\\frac{p-2}{\\|X\\|^{2}}=1-\\frac{3}{93/50}=1-\\frac{150}{93}=1-\\frac{50}{31}=-\\frac{19}{31}.\n$$\nTherefore,\n$$\n\\delta_{JS}(X)=\\left(-\\frac{19}{31}\\right)X.\n$$\nThe first component is\n$$\n\\delta_{JS,1}=\\left(-\\frac{19}{31}\\right)(1.10)=\\left(-\\frac{19}{31}\\right)\\left(\\frac{11}{10}\\right)=-\\frac{209}{310}\\approx -0.674193548\\ \\Rightarrow\\ \\text{to three significant figures: }-0.674.\n$$\nThe second component is\n$$\n\\delta_{JS,2}=\\left(-\\frac{19}{31}\\right)(0.60)=\\left(-\\frac{19}{31}\\right)\\left(\\frac{3}{5}\\right)=-\\frac{57}{155}\\approx -0.367741935\\ \\Rightarrow\\ \\text{to three significant figures: }-0.368.\n$$\nThus, the requested pair, ordered from the first to the second component, is $(-0.674,-0.368)$.", "answer": "$$\\boxed{\\begin{pmatrix}-0.674 & -0.368\\end{pmatrix}}$$", "id": "1956809"}, {"introduction": "To conclude our practical exploration, we will connect the James-Stein estimator to another fundamental concept in statistics: hypothesis testing. This problem [@problem_id:1956810] reveals a profound and elegant relationship between the estimator's shrinkage factor and the F-statistic used to test if all mean effects are zero. By showing how estimation and testing are linked, this practice deepens your intuition for why Stein's method works: it automatically adjusts the degree of shrinkage based on the strength of the evidence in the data.", "problem": "In a high-throughput biological experiment, we are interested in estimating the true mean effects $\\theta = (\\theta_1, \\dots, \\theta_p)^T$ of a treatment on $p$ different gene expressions, where $p \\ge 3$. The observed mean effects are modeled as a random vector $Y = (Y_1, \\dots, Y_p)^T$ drawn from a $p$-variate normal distribution with mean vector $\\theta$ and covariance matrix $\\sigma^2 I_p$, where $I_p$ is the $p \\times p$ identity matrix. The variance $\\sigma^2$ is unknown. Fortunately, we have an independent estimate of the variance, $S^2$, such that $\\frac{\\nu S^2}{\\sigma^2}$ follows a chi-squared distribution with $\\nu$ degrees of freedom.\n\nTwo distinct statistical procedures are considered for a preliminary analysis:\n\n1.  **Estimation**: The James-Stein (JS) estimator is used to estimate the true mean vector $\\theta$. This estimator takes the form $\\hat{\\theta}_{JS} = (1-k)Y$, where the data-dependent shrinkage factor $k$ is given by $k = \\frac{c S^2}{\\|Y\\|^2}$, with $c$ being a constant and $\\|Y\\|^2 = \\sum_{i=1}^p Y_i^2$. For this problem, assume the specific form where the constant is $c=p-2$.\n\n2.  **Hypothesis Testing**: An F-test is conducted to test the null hypothesis $H_0: \\theta = 0$ against the alternative $H_1: \\theta \\ne 0$. The test statistic for this purpose is given by $F = \\frac{\\|Y\\|^2}{p S^2}$.\n\nTo understand the connection between these two procedures, express the shrinkage factor $k$ as a function of the F-statistic $F$ and the dimension $p$.", "solution": "We are given the Jamesâ€“Stein shrinkage factor\n$$\nk=\\frac{c S^{2}}{\\|Y\\|^{2}}\n$$\nwith $c=p-2$, hence\n$$\nk=\\frac{(p-2) S^{2}}{\\|Y\\|^{2}}.\n$$\nThe F-statistic for testing $H_{0}:\\theta=0$ is\n$$\nF=\\frac{\\|Y\\|^{2}}{p S^{2}}.\n$$\nRearranging this expression gives\n$$\n\\frac{\\|Y\\|^{2}}{S^{2}}=pF \\quad \\Longrightarrow \\quad \\frac{S^{2}}{\\|Y\\|^{2}}=\\frac{1}{pF}.\n$$\nSubstituting this into the expression for $k$ yields\n$$\nk=(p-2)\\cdot \\frac{S^{2}}{\\|Y\\|^{2}}=(p-2)\\cdot \\frac{1}{pF}=\\frac{p-2}{pF}.\n$$\nThus, the shrinkage factor expressed in terms of $F$ and $p$ is $\\frac{p-2}{pF}$.", "answer": "$$\\boxed{\\frac{p-2}{p F}}$$", "id": "1956810"}]}