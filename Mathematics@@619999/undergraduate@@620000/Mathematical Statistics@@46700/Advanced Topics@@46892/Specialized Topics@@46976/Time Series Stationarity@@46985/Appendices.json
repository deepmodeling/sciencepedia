{"hands_on_practices": [{"introduction": "Building a solid intuition for stationarity starts with understanding how basic operations affect a time series. This practice examines the simplest transformation—adding a constant—to a weakly stationary process. By methodically checking the three conditions, you will see precisely how a constant shift impacts the statistical properties of a series and whether it remains stationary. [@problem_id:1964359]", "problem": "A data scientist is analyzing a time series representing daily sales fluctuations of a product. Let the true, underlying sales fluctuation process be denoted by $\\{X_t\\}$, where $t$ is the time in days. The process $\\{X_t\\}$ is known to be **weakly stationary**. A time series $\\{Z_t\\}$ is defined as weakly stationary if it satisfies the following three conditions for all valid time indices $t$ and $s$:\n1. The mean is a finite constant: $\\mathbb{E}[Z_t] = \\mu$.\n2. The variance is a finite, non-zero constant: $\\text{Var}(Z_t) = \\sigma^2$.\n3. The autocovariance between any two points depends only on the time lag between them: $\\text{Cov}(Z_t, Z_s) = \\gamma(|t-s|)$.\n\nDue to a consistent seasonal promotional event, the daily sales figures are observed to have a constant positive shift. This new observed time series is modeled as $Y_t = X_t + c$, where $c$ is a non-zero real constant representing the average increase in sales from the promotion.\n\nBased on the information provided, which of the following statements correctly describes the process $\\{Y_t\\}$?\n\nA. $Y_t$ is weakly stationary.\n\nB. $Y_t$ is not stationary because its mean depends on time $t$.\n\nC. $Y_t$ is not stationary because its variance depends on time $t$.\n\nD. $Y_t$ is not stationary because its autocovariance function depends on the specific time points $t$ and $s$, not just their lag.\n\nE. $Y_t$ cannot be weakly stationary because the constant $c$ is non-zero.", "solution": "To determine if the process $Y_t = X_t + c$ is weakly stationary, we must check if it satisfies the three conditions for weak stationarity, given that $X_t$ is a weakly stationary process.\n\nThe properties of the weakly stationary process $X_t$ are:\n- $\\mathbb{E}[X_t] = \\mu_X$, where $\\mu_X$ is a constant.\n- $\\text{Var}(X_t) = \\sigma_X^2$, where $\\sigma_X^2$ is a finite constant.\n- $\\text{Cov}(X_t, X_s) = \\gamma_X(|t-s|)$, a function only of the lag $|t-s|$.\n\nNow, let's analyze the process $Y_t = X_t + c$.\n\n**Condition 1: Mean of $Y_t$**\nWe compute the expected value of $Y_t$ using the linearity of the expectation operator.\n$$ \\mathbb{E}[Y_t] = \\mathbb{E}[X_t + c] $$\n$$ \\mathbb{E}[Y_t] = \\mathbb{E}[X_t] + \\mathbb{E}[c] $$\nSince $c$ is a constant, its expected value is just $c$. We know $\\mathbb{E}[X_t] = \\mu_X$.\n$$ \\mathbb{E}[Y_t] = \\mu_X + c $$\nLet $\\mu_Y = \\mu_X + c$. Since both $\\mu_X$ and $c$ are constants, their sum $\\mu_Y$ is also a constant. The mean of $Y_t$ does not depend on time $t$. Therefore, the first condition for weak stationarity is satisfied. This eliminates option B.\n\n**Condition 2: Variance of $Y_t$**\nWe compute the variance of $Y_t$. A key property of variance is that adding a constant to a random variable does not change its variance, i.e., $\\text{Var}(Z+k) = \\text{Var}(Z)$ for any random variable $Z$ and constant $k$.\n$$ \\text{Var}(Y_t) = \\text{Var}(X_t + c) $$\n$$ \\text{Var}(Y_t) = \\text{Var}(X_t) $$\nSince $X_t$ is weakly stationary, its variance is a constant $\\sigma_X^2$.\n$$ \\text{Var}(Y_t) = \\sigma_X^2 $$\nThe variance of $Y_t$ is a finite constant and does not depend on time $t$. Therefore, the second condition is satisfied. This eliminates option C.\n\n**Condition 3: Autocovariance of $Y_t$**\nWe compute the autocovariance of $Y_t$ between time $t$ and time $s$. A key property of covariance is its invariance to constant shifts, i.e., $\\text{Cov}(A+a, B+b) = \\text{Cov}(A,B)$ for random variables $A, B$ and constants $a, b$.\n$$ \\text{Cov}(Y_t, Y_s) = \\text{Cov}(X_t + c, X_s + c) $$\n$$ \\text{Cov}(Y_t, Y_s) = \\text{Cov}(X_t, X_s) $$\nSince $X_t$ is weakly stationary, its autocovariance depends only on the lag $|t-s|$.\n$$ \\text{Cov}(Y_t, Y_s) = \\gamma_X(|t-s|) $$\nLet $\\gamma_Y(k) = \\gamma_X(k)$ where $k = |t-s|$. The autocovariance of $Y_t$ depends only on the time lag $k$, not on the specific times $t$ and $s$. Therefore, the third condition is satisfied. This eliminates option D.\n\n**Conclusion**\nThe process $Y_t$ satisfies all three conditions for weak stationarity: its mean is constant, its variance is constant, and its autocovariance function depends only on the time lag. Thus, $Y_t$ is a weakly stationary process. The fact that $c$ is non-zero simply shifts the mean of the process but does not affect its stationarity, so option E is false. The correct statement is that $Y_t$ is weakly stationary.", "answer": "$$\\boxed{A}$$", "id": "1964359"}, {"introduction": "A key skill in time series analysis is diagnosing non-stationarity by referring back to its formal definition. This problem presents a hypothetical process that behaves differently at even and odd time steps, challenging you to test each condition for weak stationarity. This exercise demonstrates that even if some properties (like variance) are constant, a failure in just one condition is enough to render a process non-stationary. [@problem_id:1964357]", "problem": "Consider a time series process $X_t$ defined for all integers $t$. The process is constructed based on an underlying white noise process $\\epsilon_t$. The white noise process is a sequence of independent and identically distributed random variables with mean $E[\\epsilon_t] = 0$ and a constant, finite variance $Var(\\epsilon_t) = \\sigma^2 > 0$.\n\nThe process $X_t$ is defined as follows:\n$$\nX_t = \\begin{cases}\n\\epsilon_t  \\text{if } t \\text{ is an even integer} \\\\\n10 + \\epsilon_t  \\text{if } t \\text{ is an odd integer}\n\\end{cases}\n$$\n\nA time series process is considered weakly stationary (or covariance stationary) if it satisfies the following three conditions:\n1.  The mean, $E[X_t]$, is a constant that does not depend on time $t$.\n2.  The variance, $Var(X_t)$, is a constant that does not depend on time $t$.\n3.  The autocovariance, $Cov(X_t, X_{t-h})$, for any lag $h \\neq 0$, depends only on the lag $h$ and not on the time $t$.\n\nWhich of the following statements accurately describes the stationarity of the process $X_t$?\n\nA. The process is weakly stationary.\n\nB. Only condition 1 is satisfied.\n\nC. Only condition 2 is satisfied.\n\nD. Only conditions 1 and 3 are satisfied.\n\nE. Only conditions 2 and 3 are satisfied.\n\nF. None of the three conditions are satisfied.", "solution": "We analyze the three weak stationarity conditions for $X_{t}$ defined by $X_{t}=\\epsilon_{t}$ for even $t$ and $X_{t}=10+\\epsilon_{t}$ for odd $t$, where $\\{\\epsilon_{t}\\}$ is white noise with $E[\\epsilon_{t}]=0$ and $\\operatorname{Var}(\\epsilon_{t})=\\sigma^{2}$, and $\\epsilon_{t}$ are independent across $t$.\n\n1) Mean: By linearity of expectation and since adding a constant shifts the mean, we have\n$$\nE[X_{t}] =\n\\begin{cases}\nE[\\epsilon_{t}]=0  \\text{if $t$ is even} \\\\\nE[10+\\epsilon_{t}]=10+E[\\epsilon_{t}]=10  \\text{if $t$ is odd}\n\\end{cases}\n$$\nThus $E[X_{t}]$ depends on $t$ (alternates between $0$ and $10$), so condition 1 fails.\n\n2) Variance: Using $\\operatorname{Var}(a+Y)=\\operatorname{Var}(Y)$ for any constant $a$, we obtain\n$$\n\\operatorname{Var}(X_{t}) =\n\\begin{cases}\n\\operatorname{Var}(\\epsilon_{t})=\\sigma^{2}  \\text{if $t$ is even} \\\\\n\\operatorname{Var}(10+\\epsilon_{t})=\\operatorname{Var}(\\epsilon_{t})=\\sigma^{2}  \\text{if $t$ is odd}\n\\end{cases}\n$$\nHence $\\operatorname{Var}(X_{t})=\\sigma^{2}$ for all $t$, so condition 2 holds.\n\n3) Autocovariance: For $h\\neq 0$, using bilinearity of covariance, $\\operatorname{Cov}(a,Y)=0$ for constants $a$, and independence of white noise across time, we have\n$$\n\\operatorname{Cov}(X_{t},X_{t-h})\n=\\operatorname{Cov}(\\mu_{t}+\\epsilon_{t},\\,\\mu_{t-h}+\\epsilon_{t-h})\n=\\operatorname{Cov}(\\epsilon_{t},\\epsilon_{t-h}),\n$$\nwhere $\\mu_{t}\\in\\{0,10\\}$ is the deterministic shift. Since $\\epsilon_{t}$ and $\\epsilon_{t-h}$ are independent for $h\\neq 0$, $\\operatorname{Cov}(\\epsilon_{t},\\epsilon_{t-h})=0$. Therefore $\\operatorname{Cov}(X_{t},X_{t-h})=0$ for all $t$ when $h\\neq 0$, which depends only on $h$ and not on $t$. Thus condition 3 holds. For completeness, at $h=0$ the autocovariance is $\\operatorname{Var}(X_{t})=\\sigma^{2}$, which is also independent of $t$ as shown in condition 2.\n\nConclusion: Only conditions 2 and 3 are satisfied.", "answer": "$$\\boxed{E}$$", "id": "1964357"}, {"introduction": "We now turn to differencing, a fundamental tool for handling non-stationary data in practice. This problem explores the theoretical underpinnings of this technique by asking if applying a first-difference filter, $Y_t = X_t - X_{t-1}$, to a stationary process $X_t$ results in another stationary process. Working through this proof will deepen your understanding of why differencing is so effective at stabilizing a time series. [@problem_id:1964376]", "problem": "In time series analysis, a process $X_t$ is defined as weakly stationary if it satisfies three conditions:\n1. The mean of the process is constant over time, i.e., $E[X_t] = \\mu$ for all integers $t$.\n2. The variance of the process is a finite constant over time, i.e., $\\text{Var}(X_t) = \\sigma^2  \\infty$.\n3. The autocovariance between $X_t$ and $X_{t-h}$ depends only on the time lag $h$ and not on the time $t$. This is denoted as $\\text{Cov}(X_t, X_{t-h}) = \\gamma_X(h)$.\n\nConsider a time series process $X_t$ that is known to be weakly stationary. A new process, $Y_t$, is constructed by taking the first difference of $X_t$, defined as:\n$$Y_t = X_t - X_{t-1}$$\nWhich of the following statements regarding the process $Y_t$ is correct?\n\nA. $Y_t$ is always weakly stationary.\n\nB. $Y_t$ is never weakly stationary.\n\nC. $Y_t$ is weakly stationary only if the mean of $X_t$ is zero.\n\nD. $Y_t$ is weakly stationary only if $X_t$ is a white noise process (i.e., $\\gamma_X(h) = 0$ for all $h \\neq 0$).", "solution": "We are given that $X_{t}$ is weakly stationary, so $E[X_{t}]=\\mu$ for all $t$ and $\\text{Cov}(X_{t},X_{t-h})=\\gamma_{X}(h)$ depends only on $h$. Define $Y_{t}=X_{t}-X_{t-1}$.\n\nFirst, compute the mean of $Y_{t}$:\n$$\nE[Y_{t}]=E[X_{t}]-E[X_{t-1}]=\\mu-\\mu=0,\n$$\nwhich is constant in $t$.\n\nNext, compute the autocovariance of $Y_{t}$ at lag $h$:\n$$\n\\gamma_{Y}(h)=\\text{Cov}(Y_{t},Y_{t-h})=\\text{Cov}(X_{t}-X_{t-1},\\,X_{t-h}-X_{t-h-1}).\n$$\nUsing bilinearity of covariance,\n$$\n\\gamma_{Y}(h)=\\text{Cov}(X_{t},X_{t-h})-\\text{Cov}(X_{t},X_{t-h-1})-\\text{Cov}(X_{t-1},X_{t-h})+\\text{Cov}(X_{t-1},X_{t-h-1}).\n$$\nBy weak stationarity of $X_{t}$, each term depends only on the lag:\n$$\n\\text{Cov}(X_{t},X_{t-h})=\\gamma_{X}(h),\\quad\n\\text{Cov}(X_{t},X_{t-h-1})=\\gamma_{X}(h+1),\n$$\n$$\n\\text{Cov}(X_{t-1},X_{t-h})=\\gamma_{X}(h-1),\\quad\n\\text{Cov}(X_{t-1},X_{t-h-1})=\\gamma_{X}(h).\n$$\nTherefore,\n$$\n\\gamma_{Y}(h)=\\gamma_{X}(h)-\\gamma_{X}(h+1)-\\gamma_{X}(h-1)+\\gamma_{X}(h)=2\\gamma_{X}(h)-\\gamma_{X}(h+1)-\\gamma_{X}(h-1),\n$$\nwhich depends only on $h$. Since the variance of $X_t$ is finite, the variance of $Y_t$, which is $\\gamma_Y(0) = 2\\gamma_X(0) - 2\\gamma_X(1)$, is also finite. Hence $Y_{t}$ satisfies all conditions for weak stationarity.\n\nThis holds for any weakly stationary $X_{t}$, without requiring $\\mu=0$ or white noise. Therefore, the correct statement is that $Y_{t}$ is always weakly stationary.", "answer": "$$\\boxed{A}$$", "id": "1964376"}]}