{"hands_on_practices": [{"introduction": "The foundation of statistical decision theory lies in quantitatively evaluating our choices. Before we can decide which estimator is \"best,\" we must first define a metric for its performance. This exercise introduces the concept of the risk function, which measures the expected loss of an estimator, providing a concrete way to assess its accuracy and precision. By calculating and comparing the risk functions for two different estimators of a Poisson mean, you will gain firsthand insight into the fundamental bias-variance tradeoff that is central to many problems in statistics and machine learning.", "problem": "A physicist is studying a rare particle decay process. The number of decays detected in a fixed time interval, $X$, is modeled by a Poisson distribution with an unknown mean rate $\\lambda > 0$. To estimate $\\lambda$, the experiment is repeated $n$ times, yielding an independent and identically distributed (i.i.d.) random sample $X_1, X_2, \\ldots, X_n$.\n\nTwo estimators for $\\lambda$ are being considered. The first is the standard sample mean, defined as $\\delta_1(\\mathbf{X}) = \\bar{X}$, where $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$. The second is a non-standard estimator proposed as $\\delta_2(\\mathbf{X}) = \\bar{X} + 1$.\n\nTo evaluate and compare these estimators, the physicist uses a squared error loss function, $L(\\lambda, a) = (\\lambda - a)^2$. The performance of an estimator $\\delta$ is quantified by its risk function, $R(\\lambda, \\delta)$, which is the expected value of the loss function.\n\nDetermine the ratio of the risk of the second estimator to the risk of the first estimator, $\\frac{R(\\lambda, \\delta_2)}{R(\\lambda, \\delta_1)}$. Your final answer should be a closed-form analytic expression in terms of the sample size $n$ and the true mean rate $\\lambda$.", "solution": "We model $X_{1},\\ldots,X_{n}$ as i.i.d. $\\text{Poisson}(\\lambda)$ with $\\lambda>0$. For any estimator $\\delta$, under squared error loss $L(\\lambda,a)=(\\lambda-a)^{2}$, the risk is the mean squared error:\n$$\nR(\\lambda,\\delta)=\\mathbb{E}_{\\lambda}\\big[(\\lambda-\\delta)^{2}\\big]=\\operatorname{Var}_{\\lambda}(\\delta)+\\big(\\mathbb{E}_{\\lambda}[\\delta]-\\lambda\\big)^{2}.\n$$\n\nProperties of the sample mean $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ when $X_{i}\\sim\\text{Poisson}(\\lambda)$:\n- $\\mathbb{E}_{\\lambda}[X_{i}]=\\lambda$ and $\\operatorname{Var}_{\\lambda}(X_{i})=\\lambda$.\n- By linearity and independence,\n$$\n\\mathbb{E}_{\\lambda}[\\bar{X}]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\lambda}[X_{i}]=\\lambda,\\quad\n\\operatorname{Var}_{\\lambda}(\\bar{X})=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\operatorname{Var}_{\\lambda}(X_{i})=\\frac{\\lambda}{n}.\n$$\n\nRisk of $\\delta_{1}(\\mathbf{X})=\\bar{X}$:\n- Bias is $\\mathbb{E}_{\\lambda}[\\bar{X}]-\\lambda=0$.\n- Hence\n$$\nR(\\lambda,\\delta_{1})=\\operatorname{Var}_{\\lambda}(\\bar{X})+0^{2}=\\frac{\\lambda}{n}.\n$$\n\nRisk of $\\delta_{2}(\\mathbf{X})=\\bar{X}+1$:\n- Expectation $\\mathbb{E}_{\\lambda}[\\delta_{2}]=\\mathbb{E}_{\\lambda}[\\bar{X}]+1=\\lambda+1$, so bias is $1$.\n- Variance $\\operatorname{Var}_{\\lambda}(\\delta_{2})=\\operatorname{Var}_{\\lambda}(\\bar{X})=\\frac{\\lambda}{n}$.\n- Therefore\n$$\nR(\\lambda,\\delta_{2})=\\operatorname{Var}_{\\lambda}(\\delta_{2})+\\big(\\mathbb{E}_{\\lambda}[\\delta_{2}]-\\lambda\\big)^{2}=\\frac{\\lambda}{n}+1.\n$$\n\nRatio of risks:\n$$\n\\frac{R(\\lambda,\\delta_{2})}{R(\\lambda,\\delta_{1})}=\\frac{\\frac{\\lambda}{n}+1}{\\frac{\\lambda}{n}}=\\frac{\\lambda+n}{\\lambda}=1+\\frac{n}{\\lambda}.\n$$", "answer": "$$\\boxed{1+\\frac{n}{\\lambda}}$$", "id": "1924850"}, {"introduction": "Often, the risk functions of two estimators will cross, meaning neither is uniformly better than the other for all possible values of the unknown parameter. This raises a crucial question: how do we make a rational choice? This practice introduces the minimax principle, a powerful criterion for decision-making under uncertainty. You will apply this principle to determine which of two decision rules to prefer by identifying the rule that minimizes the worst-case, or maximum, possible risk.", "problem": "In a statistical decision problem, an analyst must choose between two decision rules, $\\delta_1$ and $\\delta_2$, to make an inference about an unknown parameter $\\theta$. The parameter $\\theta$ is known to lie in the interval $[0, 1]$. The performance of any decision rule $\\delta$ is evaluated using a pre-defined loss function, which leads to a risk function $R(\\theta, \\delta)$ representing the expected loss for a given value of $\\theta$.\n\nThe risk functions for the two rules are given as:\n1. $R(\\theta, \\delta_1) = A \\theta (1 - \\theta)$\n2. $R(\\theta, \\delta_2) = c$\n\nHere, $A$ and $c$ are known positive constants.\n\nThe analyst subscribes to the minimax principle for selecting a decision rule. This principle dictates that one should choose the rule that minimizes the maximum possible risk over all possible values of the parameter $\\theta$.\n\nFor the rule $\\delta_2$ to be strictly preferred over the rule $\\delta_1$ under the minimax principle, the constant $c$ must be less than a certain threshold value that depends on $A$. Determine this threshold value.", "solution": "Under the minimax principle, for a decision rule $\\delta$ the criterion is to minimize the maximum risk $\\sup_{\\theta \\in [0,1]} R(\\theta,\\delta)$. For $\\delta_{2}$, the risk is constant, so\n$$\n\\sup_{\\theta \\in [0,1]} R(\\theta,\\delta_{2}) = \\sup_{\\theta \\in [0,1]} c = c.\n$$\nFor $\\delta_{1}$, compute the supremum of $A\\theta(1-\\theta)$ on $[0,1]$. Using completing the square,\n$$\n\\theta(1-\\theta) = -\\theta^{2} + \\theta = -\\left(\\theta - \\frac{1}{2}\\right)^{2} + \\frac{1}{4},\n$$\nwhich attains its maximum $\\frac{1}{4}$ at $\\theta = \\frac{1}{2}$. Therefore,\n$$\n\\sup_{\\theta \\in [0,1]} R(\\theta,\\delta_{1}) = A \\cdot \\frac{1}{4} = \\frac{A}{4}.\n$$\nBy the minimax principle, $\\delta_{2}$ is strictly preferred over $\\delta_{1}$ if its maximal risk is strictly smaller:\n$$\nc < \\frac{A}{4}.\n$$\nThus, the threshold value for $c$ is $\\frac{A}{4}$.", "answer": "$$\\boxed{\\frac{A}{4}}$$", "id": "1924864"}, {"introduction": "An alternative to the frequentist approach of minimax is the Bayesian paradigm, which formally combines prior knowledge with observed data to make decisions. In this framework, we choose the action that minimizes the expected loss with respect to our updated beliefs, encapsulated in the posterior distribution. This hands-on problem will guide you through the complete Bayesian workflow: specifying a prior, deriving the posterior distribution after an observation, and determining the optimal Bayes estimator under a given loss function.", "problem": "A manufacturing process for a specialized electronic component has an unknown probability, $\\theta$, of producing a defective unit. The parameter $\\theta$ is considered to be a continuous random variable that can take any value in the interval $[0, 1]$. Lacking specific prior information about the process reliability, a quality control engineer models the uncertainty in $\\theta$ using a uniform prior distribution over its domain.\n\nTo gain information, a single component is randomly selected and tested. Let the outcome of the test be represented by a random variable $X$, where $X=1$ if the component is defective and $X=0$ if it is non-defective. The statistical model for the test outcome, given the probability $\\theta$, is a Bernoulli distribution with parameter $\\theta$.\n\nThe goal is to find an optimal point estimate for $\\theta$ based on the single observation. The criterion for optimality is the minimization of the expected loss, where the loss incurred by estimating $\\theta$ with a value $a$ is given by the absolute error loss function: $L(\\theta, a) = |\\theta - a|$.\n\nDetermine the Bayes estimator, $\\delta(x)$, for the parameter $\\theta$ as a function of the observed outcome $x \\in \\{0, 1\\}$.", "solution": "We model the parameter $\\theta$ with a uniform prior on $[0,1]$, which is the $\\operatorname{Beta}(1,1)$ prior. Given a single Bernoulli observation $X \\in \\{0,1\\}$ with likelihood\n$$\np(x \\mid \\theta)=\\theta^{x}(1-\\theta)^{1-x},\n$$\nthe posterior distribution is proportional to prior times likelihood:\n$$\n\\pi(\\theta \\mid x) \\propto \\theta^{x}(1-\\theta)^{1-x}, \\quad 0 \\le \\theta \\le 1.\n$$\nRecognizing the Beta family, the posterior is\n$$\n\\theta \\mid x \\sim \\operatorname{Beta}(1+x,\\,2-x).\n$$\n\nUnder absolute error loss $L(\\theta,a)=|\\theta-a|$, the Bayes estimator is any posterior median. Since the posterior is continuous for both $x=0$ and $x=1$, the posterior median is unique and satisfies $F_{\\theta \\mid x}(m)=\\frac{1}{2}$.\n\nCase $x=1$: $\\theta \\mid x \\sim \\operatorname{Beta}(2,1)$ with density $f(t)=2t$ on $[0,1]$ and CDF\n$$\nF(t)=\\int_{0}^{t}2u\\,du=t^{2}.\n$$\nSetting $F(m)=\\frac{1}{2}$ gives $m^{2}=\\frac{1}{2}$, hence\n$$\n\\delta(1)=\\frac{1}{\\sqrt{2}}.\n$$\n\nCase $x=0$: $\\theta \\mid x \\sim \\operatorname{Beta}(1,2)$ with density $f(t)=2(1-t)$ on $[0,1]$ and CDF\n$$\nF(t)=\\int_{0}^{t}2(1-u)\\,du=2t-t^{2}.\n$$\nSetting $F(m)=\\frac{1}{2}$ gives $2m-m^{2}=\\frac{1}{2}$, i.e., $m^{2}-2m+\\frac{1}{2}=0$, whose solution in $[0,1]$ is\n$$\n\\delta(0)=1-\\frac{1}{\\sqrt{2}}.\n$$\n\nTherefore, the Bayes estimator as a function of $x \\in \\{0,1\\}$ can be written compactly as\n$$\n\\delta(x)=(1-x)\\left(1-\\frac{1}{\\sqrt{2}}\\right)+x\\left(\\frac{1}{\\sqrt{2}}\\right).\n$$", "answer": "$$\\boxed{(1-x)\\left(1-\\frac{1}{\\sqrt{2}}\\right)+x\\left(\\frac{1}{\\sqrt{2}}\\right)}$$", "id": "1924877"}]}