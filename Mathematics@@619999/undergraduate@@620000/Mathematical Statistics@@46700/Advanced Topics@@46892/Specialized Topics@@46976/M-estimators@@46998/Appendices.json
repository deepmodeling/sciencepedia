{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of M-estimators, let's connect this general framework to a concept you are likely already familiar with: the sample median. This first practice demonstrates that by choosing a specific objective function, $\\rho(u) = |u|$, the M-estimation procedure of minimizing $\\sum \\rho(x_i - \\theta)$ becomes equivalent to finding the median of a dataset. This exercise provides a concrete and intuitive anchor for understanding the broader class of M-estimators by grounding it in a familiar statistical measure [@problem_id:1932002].", "problem": "In robust statistics, M-estimators provide a general framework for defining estimators of location. For a given data sample $\\{x_1, x_2, \\dots, x_n\\}$ and a chosen convex function $\\rho$, the M-estimator of location, denoted as $\\hat{\\theta}$, is the value of $\\theta$ that minimizes the objective function $S(\\theta) = \\sum_{i=1}^n \\rho(x_i - \\theta)$.\n\nConsider an M-estimator defined by the absolute value function, $\\rho(u) = |u|$. Calculate the value of this M-estimator for the data sample $X = \\{2.5, 9.8, 15.1, 4.0, 21.3\\}$. Express your answer as a single number.", "solution": "We are given the objective function for the M-estimator with $\\rho(u)=|u|$ as\n$$\nS(\\theta)=\\sum_{i=1}^{n} |x_{i}-\\theta|.\n$$\nSince $|\\,\\cdot\\,|$ is convex, $S(\\theta)$ is convex in $\\theta$. A necessary and sufficient condition for $\\hat{\\theta}$ to minimize $S(\\theta)$ is the subgradient optimality condition\n$$\n0 \\in \\partial S(\\theta)=\\sum_{i=1}^{n} \\partial |x_{i}-\\theta|.\n$$\nFor each term, the subgradient with respect to $\\theta$ is\n$$\n\\partial |x_{i}-\\theta|=\n\\begin{cases}\n\\{-1\\},  \\theta  x_{i},\\\\\n[-1,1],  \\theta = x_{i},\\\\\n\\{1\\},  \\theta > x_{i}.\n\\end{cases}\n$$\nLet $L(\\theta)$ be the number of $x_{i}$ strictly less than $\\theta$, $G(\\theta)$ the number strictly greater than $\\theta$, and $E(\\theta)$ the number equal to $\\theta$. Then the subgradient sum is an interval\n$$\n\\partial S(\\theta)=\\left[\\, -L(\\theta)+G(\\theta)-E(\\theta),\\; -L(\\theta)+G(\\theta)+E(\\theta)\\,\\right].\n$$\nThe condition $0 \\in \\partial S(\\theta)$ is equivalent to\n$$\n-L(\\theta)+G(\\theta) \\in [-E(\\theta),\\,E(\\theta)],\n$$\nwhich in turn is equivalent to the pair of inequalities\n$$\nL(\\theta) \\leq \\frac{n}{2} \\quad \\text{and} \\quad G(\\theta) \\leq \\frac{n}{2}.\n$$\nThus any minimizer $\\hat{\\theta}$ must be a median of the sample. For an odd sample size $n$ with all distinct values, the unique minimizer is the sample median, i.e., the middle order statistic.\n\nFor the data $X=\\{2.5, 9.8, 15.1, 4.0, 21.3\\}$, sort the values to obtain\n$$\n2.5  4.0  9.8  15.1  21.3.\n$$\nWith $n=5$, the median is the third order statistic, which is $9.8$. Therefore, the M-estimator defined by $\\rho(u)=|u|$ is\n$$\n\\hat{\\theta}=9.8.\n$$", "answer": "$$\\boxed{9.8}$$", "id": "1932002"}, {"introduction": "Having established the connection to the median, we now turn to a more flexible and widely-used M-estimator developed by Peter Huber. Huber's estimator provides a compromise between the sensitivity of the mean and the robustness of the median by using an influence function, $\\psi(u)$, that down-weights—but does not entirely ignore—outliers. This exercise will guide you through the calculation of a Huber M-estimate, giving you first-hand experience in applying its 'clipping' mechanism to achieve robustness in the presence of suspect data points [@problem_id:1952425].", "problem": "In robust statistics, M-estimators are a broad class of estimators that are robust to outliers. An M-estimator of location, denoted by $\\hat{\\theta}$, for a dataset $\\{x_1, x_2, \\ldots, x_n\\}$ is defined as the solution to the equation:\n$$\n\\sum_{i=1}^{n} \\psi\\left(\\frac{x_i - \\theta}{S}\\right) = 0\n$$\nwhere $S$ is a robust estimate of scale and $\\psi$ is a chosen influence function.\n\nConsider Huber's $\\psi$-function, defined for a given tuning constant $k > 0$ as:\n$$\n\\psi_k(u) = \\begin{cases}\n-k  \\text{if } u  -k \\\\\nu  \\text{if } |u| \\leq k \\\\\nk  \\text{if } u > k\n\\end{cases}\n$$\nThis function limits the influence of large residuals by \"clipping\" them at $\\pm k$.\n\nAn engineer is analyzing a set of five independent measurements of a component's length. The measurements obtained are:\n$$\n\\{2.1, 2.5, 2.3, 2.6, 4.5\\}\n$$\nTo obtain a location estimate that is not overly sensitive to the single large value, the engineer decides to use an M-estimator. For the calculation, a fixed scale estimate of $S=1$ and a tuning constant of $k=1.5$ are to be used.\n\nCalculate the M-estimate of location, $\\hat{\\theta}$, for this dataset. Report your answer rounded to three significant figures.", "solution": "We seek $\\hat{\\theta}$ solving the M-estimating equation\n$$\n\\sum_{i=1}^{5} \\psi_{k}\\!\\left(\\frac{x_{i}-\\theta}{S}\\right)=0,\n$$\nwith $S=1$, $k=1.5$, and $x=\\{2.1,2.5,2.3,2.6,4.5\\}$. With $S=1$, this becomes\n$$\n\\sum_{i=1}^{5} \\psi_{k}(x_{i}-\\theta)=0,\n$$\nwhere Huber’s function is $\\psi_{k}(u)=u$ if $|u|\\leq k$ and $\\psi_{k}(u)=\\pm k$ if $|u|k$, with the sign of $u$.\n\nFor each $x_{i}$, the term is linear (i.e., unclipped) when $|x_{i}-\\theta|\\leq 1.5$, equivalently $\\theta\\in[x_{i}-1.5,\\,x_{i}+1.5]$. These intervals are:\n- $x_{1}=2.1$: $\\theta\\in[0.6,\\,3.6]$,\n- $x_{2}=2.5$: $\\theta\\in[1.0,\\,4.0]$,\n- $x_{3}=2.3$: $\\theta\\in[0.8,\\,3.8]$,\n- $x_{4}=2.6$: $\\theta\\in[1.1,\\,4.1]$,\n- $x_{5}=4.5$: $\\theta\\in[3.0,\\,6.0]$.\n\nConsider $\\theta\\in[1.1,3.0)$, where the first four terms are linear and the fifth is clipped at $+1.5$ because $4.5-\\theta1.5$. Then\n$$\n\\sum_{i=1}^{5} \\psi_{k}(x_{i}-\\theta)=\\sum_{i=1}^{4} (x_{i}-\\theta) + 1.5.\n$$\nUsing $\\sum_{i=1}^{4} x_{i}=2.1+2.5+2.3+2.6=9.5$, we get\n$$\n9.5 - 4\\theta + 1.5 = 0 \\;\\;\\Longrightarrow\\;\\; 11 - 4\\theta = 0 \\;\\;\\Longrightarrow\\;\\; \\hat{\\theta}=\\frac{11}{4}=2.75.\n$$\nCheck the regime consistency at $\\hat{\\theta}=2.75$:\n- For $x_{1},x_{2},x_{3},x_{4}$, $|x_{i}-\\hat{\\theta}|\\in\\{0.65,0.25,0.45,0.15\\}\\leq 1.5$, so these are correctly treated as linear.\n- For $x_{5}$, $|4.5-2.75|=1.751.5$, so it is correctly clipped to $+1.5$.\n\nThus the solution is valid and unique in this region. Rounding to three significant figures yields $2.75$.", "answer": "$$\\boxed{2.75}$$", "id": "1952425"}, {"introduction": "Our final practice explores a more advanced class of M-estimators with 'redescending' influence functions, which completely reject extreme outliers rather than just down-weighting them. A key feature of these estimators is that their defining equation, $\\sum \\psi(x_i - \\theta) = 0$, can have multiple solutions. This problem challenges you to find all potential solutions and then identify the true M-estimate by returning to the fundamental principle: finding the global minimum of the objective function $\\sum \\rho(x_i - \\theta)$. This exercise highlights a critical theoretical and practical aspect of working with sophisticated robust estimators [@problem_id:1931970].", "problem": "In robust statistics, M-estimators are a broad class of estimators for a location parameter $\\theta$ that are obtained by minimizing a sum of functions of the residuals, or equivalently, as the solution to an equation.\n\nConsider an M-estimator of location, $T_n$, for a dataset $\\{x_1, \\dots, x_n\\}$. It is defined as a solution for $\\theta$ to the equation:\n$$ \\sum_{i=1}^{n} \\psi(x_i - \\theta) = 0 $$\nwhere $\\psi$ is a chosen function.\n\nFor a particular application, a statistician designs a custom M-estimator using the following piecewise $\\psi$-function, which is designed to ignore observations that are far from the central tendency:\n$$\n\\psi(u) =\n\\begin{cases}\nu  \\text{for } |u| \\leq c \\\\\n0  \\text{for } |u| > c\n\\end{cases}\n$$\nThe constant $c$ is a tuning parameter that controls the degree of robustness.\n\nGiven the dataset $X = \\{10, 12, 13, 15, 19, 20, 40\\}$ and a tuning parameter of $c=4$, calculate the value of the M-estimator $T_n$. Round your final answer to four significant figures.", "solution": "We seek $T_{n}$ solving\n$$\n\\sum_{i=1}^{n} \\psi(x_{i}-\\theta)=0, \\quad \\text{with} \\quad \\psi(u)=\\begin{cases}u  \\text{if } |u|\\leq c \\\\ 0  \\text{if } |u|c \\end{cases}, \\quad c=4,\n$$\nfor the data $X=\\{10,12,13,15,19,20,40\\}$. Writing the indicator $I_{i}(\\theta)=\\mathbf{1}\\{|x_{i}-\\theta|\\leq 4\\}$, the estimating equation is\n$$\n\\sum_{i=1}^{n} (x_{i}-\\theta) I_{i}(\\theta)=0 \\quad \\Longleftrightarrow \\quad \\theta \\sum_{i=1}^{n} I_{i}(\\theta)=\\sum_{i=1}^{n} x_{i} I_{i}(\\theta).\n$$\nIf $m(\\theta)=\\sum_{i=1}^{n} I_{i}(\\theta)\\geq 1$, this is equivalent to the fixed-point equation\n$$\n\\theta=\\frac{\\sum_{i=1}^{n} x_{i} I_{i}(\\theta)}{\\sum_{i=1}^{n} I_{i}(\\theta)},\n$$\ni.e., $\\theta$ equals the mean of those $x_{i}$ within $4$ of $\\theta$. The inclusion condition $|x_{i}-\\theta|\\leq 4$ is equivalent to $\\theta\\in[x_{i}-4,x_{i}+4]$. For the given data, these intervals are\n$$\n\\begin{aligned}\n10:\\ [6,14],\\quad 12:\\ [8,16],\\quad 13:\\ [9,17],\\quad 15:\\ [11,19],\\\\\n19:\\ [15,23],\\quad 20:\\ [16,24],\\quad 40:\\ [36,44].\n\\end{aligned}\n$$\nBetween consecutive endpoints, the included set $S(\\theta)=\\{i: \\theta\\in[x_{i}-4,x_{i}+4]\\}$ is constant, and any solution within such an interval must equal the mean of the corresponding $x_{i}$ and also lie in that interval. Checking the intervals yields the following self-consistent solutions:\n- For $\\theta\\in(11,14)$, $S=\\{10,12,13,15\\}$, mean $=\\frac{10+12+13+15}{4}=12.5\\in(11,14)$, hence $\\theta=12.5$ is a solution.\n- For $\\theta\\in(16,17)$, $S=\\{13,15,19,20\\}$, mean $=\\frac{13+15+19+20}{4}=16.75\\in(16,17)$, hence $\\theta=16.75$ is a solution.\n- For $\\theta\\in(17,19)$, $S=\\{15,19,20\\}$, mean $=\\frac{15+19+20}{3}=18\\in(17,19)$, hence $\\theta=18$ is a solution.\n- For $\\theta\\in(19,23)$, $S=\\{19,20\\}$, mean $=\\frac{19+20}{2}=19.5\\in(19,23)$, hence $\\theta=19.5$ is a solution.\n- For $\\theta\\in(36,44)$, $S=\\{40\\}$, mean $=40\\in(36,44)$, hence $\\theta=40$ is a solution.\n\nBecause $\\psi$ redescends to zero, the estimating equation can have multiple roots. The corresponding M-estimator is defined as a minimizer of the objective with $\\rho$ such that $\\rho'(u)=\\psi(u)$. Integrating,\n$$\n\\rho(u)=\\int_{0}^{u} \\psi(t)\\,dt=\\begin{cases}\\frac{u^{2}}{2}  \\text{if } |u|\\leq c\\\\ \\frac{c^{2}}{2}  \\text{if } |u|c\\end{cases},\n$$\nand with $c=4$ this gives $\\rho(u)=\\frac{u^{2}}{2}$ if $|u|\\leq 4$ and $\\rho(u)=8$ if $|u|4$. We evaluate $\\sum_{i=1}^{n}\\rho(x_{i}-\\theta)$ at the candidate roots:\n\n- At $\\theta=12.5$: residuals $(x_{i}-\\theta)$ are $-2.5,-0.5,0.5,2.5,6.5,7.5,27.5$, so the sum is\n$$\n\\frac{(-2.5)^{2}}{2}+\\frac{(-0.5)^{2}}{2}+\\frac{0.5^{2}}{2}+\\frac{2.5^{2}}{2}+8+8+8=3.125+0.125+0.125+3.125+24=30.5.\n$$\n- At $\\theta=16.75$: sum $=8+8+\\frac{(-3.75)^{2}}{2}+\\frac{(-1.75)^{2}}{2}+\\frac{2.25^{2}}{2}+\\frac{3.25^{2}}{2}+8=40.375$.\n- At $\\theta=18$: sum $=8+8+8+\\frac{(-3)^{2}}{2}+\\frac{1^{2}}{2}+\\frac{2^{2}}{2}+8=39$.\n- At $\\theta=19.5$: sum $=8+8+8+8+\\frac{(-0.5)^{2}}{2}+\\frac{0.5^{2}}{2}+8=40.25$.\n- At $\\theta=40$: sum $=8+8+8+8+8+8+0=48$.\n\nThe minimum value occurs at $\\theta=12.5$. Therefore, the M-estimator $T_{n}$ is $12.5$, which rounded to four significant figures is $12.50$.", "answer": "$$\\boxed{12.50}$$", "id": "1931970"}]}