{"hands_on_practices": [{"introduction": "At the heart of the Metropolis-Hastings algorithm is the acceptance probability, which determines whether a proposed new state is accepted or the chain remains in its current position. This decision rule is what guides the sampler towards the target distribution over time. This foundational exercise [@problem_id:1371728] provides direct practice in calculating this probability for a simple yet illustrative target distribution, solidifying your understanding of the core mechanism that drives the simulation.", "problem": "A data scientist is implementing a Markov Chain Monte Carlo (MCMC) simulation to draw samples from a posterior probability distribution for a parameter $x$. The target distribution, $\\pi(x)$, is proportional to the exponential of the negative absolute value of the parameter, such that $\\pi(x) \\propto \\exp(-|x|)$.\n\nThe scientist uses the Metropolis algorithm with a symmetric proposal distribution $q(x'|x)$, where the probability of proposing a new state $x'$ given the current state $x$ is equal to the probability of proposing $x$ given $x'$ (i.e., $q(x'|x) = q(x|x')$).\n\nSuppose that at a certain step in the simulation, the current state of the chain is $x = 1.5$. The algorithm then proposes a move to a new candidate state $x' = 2.0$.\n\nCalculate the acceptance probability for this specific move. Your answer should be a dimensionless real number. Round your final answer to four significant figures.", "solution": "The Metropolis acceptance probability for a move from $x$ to $x'$ with a symmetric proposal $q(x'|x)=q(x|x')$ is\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)=\\min\\left(1,\\frac{\\pi(x')}{\\pi(x)}\\right).\n$$\nGiven the target distribution $\\pi(x)\\propto \\exp(-|x|)$, the ratio simplifies to\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\frac{\\exp(-|x'|)}{\\exp(-|x|)}=\\exp\\!\\left(-\\left(|x'|-|x|\\right)\\right).\n$$\nWith $x=1.5$ and $x'=2.0$, we have $|x|=1.5$ and $|x'|=2.0$, so\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\exp\\!\\left(-\\left(2.0-1.5\\right)\\right)=\\exp(-0.5).\n$$\nTherefore,\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\exp(-0.5)\\right)=\\exp(-0.5).\n$$\nNumerically, $\\exp(-0.5)\\approx 0.6065$ when rounded to four significant figures.", "answer": "$$\\boxed{0.6065}$$", "id": "1371728"}, {"introduction": "While the Metropolis algorithm can be applied universally, more specialized MCMC methods like the Gibbs sampler can be far more efficient when applicable. The crucial first step in building a Gibbs sampler is to derive the full conditional distribution for each variable in the model. This exercise [@problem_id:1932854] provides hands-on practice with this essential prerequisite, showing how to derive these conditionals from a simple, geometrically defined joint distribution.", "problem": "A Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for generating a sequence of observations that approximate a target multivariate probability distribution, which is especially useful when direct sampling is difficult. A crucial step in implementing a Gibbs sampler is to derive the full conditional distribution for each variable in the model.\n\nConsider a bivariate random vector $(X, Y)$ with a joint probability density function (PDF), $p(x, y)$, that is uniform over a triangular region $\\mathcal{R}$ in the $xy$-plane. The region $\\mathcal{R}$ is defined by the inequalities $x>0$, $y>0$, and $x+y < 1$. The joint PDF is therefore:\n$$\np(x, y) =\n\\begin{cases}\n2 & \\text{if } (x,y) \\in \\mathcal{R} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nTo construct a Gibbs sampler for this distribution, we need the full conditional distributions, $p(x|y)$ and $p(y|x)$.\n\nWhich of the following options correctly identifies these two conditional distributions? Note that $\\text{Uniform}(a,b)$ denotes the uniform distribution on the interval $(a,b)$.\n\nA: $p(x|y)$ is Uniform$(0, 1-y)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(0, 1-x)$ for $x \\in (0,1)$.\n\nB: $p(x|y)$ is Uniform$(0, 1)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(0, 1)$ for $x \\in (0,1)$.\n\nC: $p(x|y)$ is Uniform$(0, y)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(0, x)$ for $x \\in (0,1)$.\n\nD: The conditional distributions are not uniform.\n\nE: $p(x|y)$ is Uniform$(y, 1)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(x, 1)$ for $x \\in (0,1)$.", "solution": "We are given a joint density $p(x,y)$ that is uniform over the region $\\mathcal{R}=\\{(x,y): x>0,\\ y>0,\\ x+y<1\\}$ with $p(x,y)=2$ on $\\mathcal{R}$ and $0$ otherwise. To obtain the full conditional distributions, we use the definition of conditionals via the marginals.\n\nFirst, compute the marginal density of $Y$. For a fixed $y$, the admissible $x$ values are those satisfying $0<x<1-y$, provided $0<y<1$. Therefore,\n$$\np_{Y}(y)=\\int_{-\\infty}^{\\infty} p(x,y)\\,dx=\\int_{0}^{1-y} 2\\,dx=2(1-y), \\quad \\text{for } 0<y<1,\n$$\nand $p_{Y}(y)=0$ otherwise.\n\nThen the conditional density of $X$ given $Y=y$ is\n$$\np_{X\\mid Y}(x\\mid y)=\\frac{p(x,y)}{p_{Y}(y)}=\\frac{2}{2(1-y)}=\\frac{1}{1-y},\n$$\nvalid exactly on the support where $p(x,y)>0$, namely $0<x<1-y$ (and $0<y<1$). This is the density of a Uniform distribution on $(0,1-y)$:\n$$\nX\\mid Y=y \\sim \\text{Uniform}(0,1-y), \\quad \\text{for } 0<y<1.\n$$\n\nBy symmetry, compute the marginal of $X$:\n$$\np_{X}(x)=\\int_{-\\infty}^{\\infty} p(x,y)\\,dy=\\int_{0}^{1-x} 2\\,dy=2(1-x), \\quad \\text{for } 0<x<1,\n$$\nand $p_{X}(x)=0$ otherwise.\n\nThen the conditional density of $Y$ given $X=x$ is\n$$\np_{Y\\mid X}(y\\mid x)=\\frac{p(x,y)}{p_{X}(x)}=\\frac{2}{2(1-x)}=\\frac{1}{1-x},\n$$\nvalid on $0<y<1-x$ (and $0<x<1$). This is Uniform on $(0,1-x)$:\n$$\nY\\mid X=x \\sim \\text{Uniform}(0,1-x), \\quad \\text{for } 0<x<1.\n$$\n\nTherefore, the correct option is that both conditionals are uniform on the respective truncated intervals determined by the constraint $x+y<1$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1932854"}, {"introduction": "A correctly implemented MCMC sampler is not always an efficient one, and its performance often hinges on carefully chosen tuning parameters. For the Random Walk Metropolis algorithm, the standard deviation, or \"width,\" of the proposal distribution is a critical parameter that must be set by the user. This problem [@problem_id:1932810] is a thought experiment designed to build your intuition about the practical trade-offs involved in this choice, exploring how proposal width affects both the acceptance rate and the sampler's ability to efficiently explore the target distribution.", "problem": "An analyst is using a Markov Chain Monte Carlo (MCMC) method, specifically the Random Walk Metropolis algorithm, to generate samples from a continuous, unimodal target probability density function $\\pi(x)$ defined on the real line. The algorithm proceeds as follows: given the current state of the chain is $x^{(t)}$, a new candidate state $x'$ is proposed from a symmetric proposal distribution $q(x'|x^{(t)})$. For this specific implementation, the proposal is generated by adding a random perturbation: $x' = x^{(t)} + \\epsilon$, where $\\epsilon$ is drawn from a Normal distribution with mean 0 and standard deviation $\\sigma$, i.e., $\\epsilon \\sim N(0, \\sigma^2)$. The proposed state $x'$ is then accepted as the next state, $x^{(t+1)} = x'$, with probability $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. If the proposal is rejected, the chain remains at the current state, i.e., $x^{(t+1)} = x^{(t)}$.\n\nThe analyst is experimenting with two different settings for the proposal distribution's standard deviation:\n1.  A very narrow proposal distribution, where $\\sigma = \\sigma_{small}$ is a very small positive number.\n2.  A very wide proposal distribution, where $\\sigma = \\sigma_{large}$ is a very large positive number.\n\nAssume the chain has been initialized in a region of non-negligible probability (e.g., near the mode of $\\pi(x)$). Which of the following statements most accurately describes the expected behavior of the MCMC chain in these two scenarios?\n\nA. The narrow proposal ($\\sigma_{small}$) will lead to a low acceptance rate and slow exploration. The wide proposal ($\\sigma_{large}$) will lead to a high acceptance rate and fast exploration.\n\nB. Both proposal widths will lead to similar acceptance rates, but the chain using the wide proposal ($\\sigma_{large}$) will explore the state space much more quickly than the chain using the narrow proposal ($\\sigma_{small}$).\n\nC. The narrow proposal ($\\sigma_{small}$) will result in a very high acceptance rate, but the chain will explore the state space very slowly. The wide proposal ($\\sigma_{large}$) will result in a very low acceptance rate, causing the chain to remain stuck at the same state for many iterations.\n\nD. The narrow proposal ($\\sigma_{small}$) will lead to a high acceptance rate and fast exploration. The wide proposal ($\\sigma_{large}$) will lead to a low acceptance rate and slow exploration.\n\nE. The width of the proposal distribution has a negligible effect on the algorithm's performance; both the acceptance rate and the speed of exploration are primarily determined by the properties of the target distribution $\\pi(x)$.", "solution": "The problem asks us to analyze the behavior of the Random Walk Metropolis algorithm, concentrating on how the width (standard deviation $\\sigma$) of the proposal distribution $N(0, \\sigma^2)$ affects the chain's acceptance rate and its ability to explore the state space of a target distribution $\\pi(x)$.\n\nThe acceptance probability for a proposed move from $x^{(t)}$ to $x'$ is given by $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. Let's analyze the two cases separately.\n\n**Case 1: Narrow Proposal Distribution ($\\sigma = \\sigma_{small}$)**\n\nWhen the standard deviation $\\sigma_{small}$ is very small, the random perturbation $\\epsilon$ drawn from $N(0, \\sigma_{small}^2)$ will also be very small in magnitude. This means the proposed state, $x' = x^{(t)} + \\epsilon$, will be very close to the current state $x^{(t)}$.\n\nSince the target distribution $\\pi(x)$ is continuous, if $x'$ is very close to $x^{(t)}$, then the value of the probability density at these two points, $\\pi(x')$ and $\\pi(x^{(t)})$, will be very similar.\nConsequently, the ratio $\\frac{\\pi(x')}{\\pi(x^{(t)})}$ will be very close to 1.\n\nThe acceptance probability is $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. Since the ratio is close to 1, the acceptance probability $\\alpha$ will be very high (close to 1). This means that almost every proposed move will be accepted.\n\nHowever, since each accepted move is just a tiny step away from the previous position, the chain moves through the state space very slowly. This is an inefficient way to explore the full range of the target distribution $\\pi(x)$, especially the tails. The chain exhibits high autocorrelation, meaning successive samples are highly dependent, and it takes a very large number of iterations to obtain a representative set of independent samples. This constitutes very slow exploration.\n\n**Case 2: Wide Proposal Distribution ($\\sigma = \\sigma_{large}$)**\n\nWhen the standard deviation $\\sigma_{large}$ is very large, the random perturbation $\\epsilon$ will often be large in magnitude. This means the proposed state $x' = x^{(t)} + \\epsilon$ is likely to be very far from the current state $x^{(t)}$.\n\nWe are given that the target distribution $\\pi(x)$ is unimodal and the chain starts in a region of high probability (near the mode). When the chain is in such a region, $\\pi(x^{(t)})$ is relatively large. A large jump from $x^{(t)}$ is very likely to land in a region far out in the tails of the distribution, where the probability density is extremely low. Thus, it is highly probable that $\\pi(x') \\ll \\pi(x^{(t)})$.\n\nIn this scenario, the ratio $\\frac{\\pi(x')}{\\pi(x^{(t)})}$ will be very close to 0.\n\nThe acceptance probability is $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. Since the ratio is very small, the acceptance probability $\\alpha$ will also be very small. This means that the vast majority of proposed moves will be rejected.\n\nWhen a proposal is rejected, the chain does not move: $x^{(t+1)} = x^{(t)}$. Therefore, for a wide proposal distribution, the chain will remain stuck at the same state for many consecutive iterations, waiting for a rare proposal that happens to land in another high-probability region. This is also a very inefficient way to explore the state space.\n\n**Conclusion and Evaluation of Options:**\n\n-   A narrow proposal gives a **high acceptance rate** but **slow exploration**.\n-   A wide proposal gives a **low acceptance rate** and **slow exploration** (because the chain gets stuck).\n\nLet's evaluate the given options based on this analysis:\n\n-   **A:** Incorrect. It claims a wide proposal leads to a high acceptance rate.\n-   **B:** Incorrect. It claims both have similar acceptance rates.\n-   **C:** This statement correctly captures both behaviors. The narrow proposal leads to a high acceptance rate but slow exploration. The wide proposal leads to a very low acceptance rate, causing the chain to get stuck. This is the most accurate description.\n-   **D:** Incorrect. It claims a narrow proposal leads to fast exploration.\n-   **E:** Incorrect. The width of the proposal distribution is a critical tuning parameter that profoundly affects the algorithm's performance.\n\nTherefore, the most accurate statement is C.", "answer": "$$\\boxed{C}$$", "id": "1932810"}]}