{"hands_on_practices": [{"introduction": "In many real-world scenarios, from processing sensor readings to analyzing financial data, we need to summarize a set of observations with a single representative value. While the arithmetic mean is a common choice, its sensitivity to outliers can be a significant drawback. This practice explores an alternative by using the absolute error loss, also known as the $L_1$ loss, which penalizes deviations linearly. By working through this exercise [@problem_id:1931715], you will discover the foundational link between minimizing the sum of absolute errors and finding the median of a dataset, a key concept in robust statistics.", "problem": "In the field of statistical estimation, a common task is to determine a single representative value for a set of measurements. The choice of this value often depends on a \"loss function,\" which quantifies the penalty for an estimate being inaccurate.\n\nAn engineer is calibrating a novel proximity sensor. For a target placed at a fixed, unknown distance, the sensor reports three measurements: $x_1 = 1$, $x_2 = 5$, and $x_3 = 9$, all in millimeters (mm). The engineer decides to find a single constant estimate, $\\hat{\\theta}$, that best represents the true distance. The criterion for \"best\" is that the estimate must minimize the sum of the absolute errors. This loss function, $L(\\hat{\\theta})$, is defined as the sum of the absolute differences between the estimate $\\hat{\\theta}$ and each individual measurement $x_i$.\n\nDetermine the exact value of the estimate $\\hat{\\theta}$ that minimizes this loss function. Provide your answer as a numerical value in millimeters.", "solution": "We are to minimize the sum of absolute deviations from a constant estimate. Define the loss function for an estimate $\\hat{\\theta}$ as\n$$\nL(\\hat{\\theta})=\\sum_{i=1}^{3}|\\hat{\\theta}-x_{i}|=|\\hat{\\theta}-1|+|\\hat{\\theta}-5|+|\\hat{\\theta}-9|.\n$$\nA standard result in estimation theory is that any median of the data minimizes the sum of absolute deviations. To verify this directly, sort the measurements $1<5<9$ and consider the derivative of $L$ with respect to $\\hat{\\theta}$ away from the points $\\hat{\\theta}\\in\\{1,5,9\\}$. Using $\\frac{d}{d\\hat{\\theta}}|\\hat{\\theta}-a|=\\operatorname{sgn}(\\hat{\\theta}-a)$ for $\\hat{\\theta}\\neq a$, we have:\n- For $\\hat{\\theta}\\in(-\\infty,1)$: $\\operatorname{sgn}(\\hat{\\theta}-1)=-1$, $\\operatorname{sgn}(\\hat{\\theta}-5)=-1$, $\\operatorname{sgn}(\\hat{\\theta}-9)=-1$, hence $L'(\\hat{\\theta})=-3$.\n- For $\\hat{\\theta}\\in(1,5)$: signs are $+1,-1,-1$, hence $L'(\\hat{\\theta})=-1$.\n- For $\\hat{\\theta}\\in(5,9)$: signs are $+1,+1,-1$, hence $L'(\\hat{\\theta})=+1$.\n- For $\\hat{\\theta}\\in(9,\\infty)$: signs are $+1,+1,+1$, hence $L'(\\hat{\\theta})=+3$.\n\nThus $L'(\\hat{\\theta})<0$ for $\\hat{\\theta}<5$ and $L'(\\hat{\\theta})>0$ for $\\hat{\\theta}>5$. At $\\hat{\\theta}=5$, the left derivative is $-1$ and the right derivative is $+1$, so the subdifferential contains $0$, confirming a minimizer at $\\hat{\\theta}=5$. Therefore, the loss is minimized at the median of the data, which is $5$.\n\nFor completeness, evaluating at the candidate point yields $L(5)=|5-1|+|5-5|+|5-9|=4+0+4=8$, which is less than the values at $\\hat{\\theta}=1$ or $\\hat{\\theta}=9$ (both equal to $12$), consistent with minimality.\n\nHence, the estimate that minimizes the sum of absolute errors is $\\hat{\\theta}=5$ millimeters.", "answer": "$$\\boxed{5}$$", "id": "1931715"}, {"introduction": "How can we intelligently combine information from multiple, imperfect sources to arrive at a single, more reliable conclusion? This is a central question in fields ranging from sensor fusion to meta-analysis. This problem [@problem_id:1931719] tasks you with finding the optimal way to blend two independent, unbiased estimates, each with its own level of uncertainty. By applying the principle of minimizing the Mean Squared Error (MSE), a criterion based on the squared error ($L_2$) loss, you will derive the celebrated inverse-variance weighting rule, a cornerstone of statistical estimation that gives more weight to more precise measurements.", "problem": "In the field of sensor fusion, data from multiple sensors are combined to produce estimates that are more reliable than those obtained from a single sensor. Consider a scenario where two independent sensor systems are used to estimate an unknown physical parameter, $\\theta$.\n\nThe first sensor provides an estimate $\\hat{\\theta}_1$, and the second sensor provides an estimate $\\hat{\\theta}_2$. Both estimators are known to be unbiased, meaning their expected values are equal to the true parameter, i.e., $E[\\hat{\\theta}_1] = \\theta$ and $E[\\hat{\\theta}_2] = \\theta$. The precision of these sensors is characterized by their variances, which are given by $\\text{Var}(\\hat{\\theta}_1) = \\sigma_1^2$ and $\\text{Var}(\\hat{\\theta}_2) = \\sigma_2^2$, where $\\sigma_1^2$ and $\\sigma_2^2$ are known positive constants.\n\nA weighted average estimator, $\\hat{\\theta}_p$, is constructed as a linear combination of the two individual estimates:\n$$\n\\hat{\\theta}_p = \\alpha \\hat{\\theta}_1 + (1-\\alpha) \\hat{\\theta}_2\n$$\nwhere $\\alpha$ is a real-valued weighting constant. We want to find the optimal value of $\\alpha$ that produces the \"best\" possible combined estimate. The quality of an estimator is measured by its Mean Squared Error (MSE), defined as $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$.\n\nDetermine the value of $\\alpha$ that minimizes the Mean Squared Error of the pooled estimator, $\\hat{\\theta}_p$. Express your answer in terms of $\\sigma_1^2$ and $\\sigma_2^2$.", "solution": "We are given two unbiased estimators with $E[\\hat{\\theta}_{1}] = \\theta$ and $E[\\hat{\\theta}_{2}] = \\theta$, and variances $\\text{Var}(\\hat{\\theta}_{1}) = \\sigma_{1}^{2}$ and $\\text{Var}(\\hat{\\theta}_{2}) = \\sigma_{2}^{2}$. The pooled estimator is $\\hat{\\theta}_{p} = \\alpha \\hat{\\theta}_{1} + (1 - \\alpha) \\hat{\\theta}_{2}$. Since the estimators are unbiased, the pooled estimator is also unbiased:\n$$\nE[\\hat{\\theta}_{p}] = \\alpha E[\\hat{\\theta}_{1}] + (1 - \\alpha) E[\\hat{\\theta}_{2}] = \\alpha \\theta + (1 - \\alpha) \\theta = \\theta.\n$$\nTherefore, the Mean Squared Error of $\\hat{\\theta}_{p}$ equals its variance:\n$$\n\\text{MSE}(\\hat{\\theta}_{p}) = \\text{Var}(\\hat{\\theta}_{p}).\n$$\nUsing the variance of a linear combination and independence (so covariance is zero),\n$$\n\\text{Var}(\\hat{\\theta}_{p}) = \\alpha^{2} \\text{Var}(\\hat{\\theta}_{1}) + (1 - \\alpha)^{2} \\text{Var}(\\hat{\\theta}_{2}) + 2 \\alpha (1 - \\alpha) \\text{Cov}(\\hat{\\theta}_{1}, \\hat{\\theta}_{2}) = \\alpha^{2} \\sigma_{1}^{2} + (1 - \\alpha)^{2} \\sigma_{2}^{2}.\n$$\nDefine\n$$\nf(\\alpha) = \\alpha^{2} \\sigma_{1}^{2} + (1 - \\alpha)^{2} \\sigma_{2}^{2}.\n$$\nDifferentiate with respect to $\\alpha$ and set to zero for optimality:\n$$\n\\frac{d f}{d \\alpha} = 2 \\alpha \\sigma_{1}^{2} - 2 (1 - \\alpha) \\sigma_{2}^{2} = 0.\n$$\nRearrange to solve for $\\alpha$:\n$$\n2 \\alpha \\sigma_{1}^{2} = 2 (1 - \\alpha) \\sigma_{2}^{2} \\quad \\Longrightarrow \\quad \\alpha \\sigma_{1}^{2} = \\sigma_{2}^{2} - \\alpha \\sigma_{2}^{2},\n$$\n$$\n\\alpha (\\sigma_{1}^{2} + \\sigma_{2}^{2}) = \\sigma_{2}^{2} \\quad \\Longrightarrow \\quad \\alpha^{\\ast} = \\frac{\\sigma_{2}^{2}}{\\sigma_{1}^{2} + \\sigma_{2}^{2}}.\n$$\nThe second derivative is\n$$\n\\frac{d^{2} f}{d \\alpha^{2}} = 2 \\sigma_{1}^{2} + 2 \\sigma_{2}^{2} > 0,\n$$\nso this critical point is a global minimum. Hence, the MSE-minimizing weight on $\\hat{\\theta}_{1}$ is $\\alpha^{\\ast} = \\frac{\\sigma_{2}^{2}}{\\sigma_{1}^{2} + \\sigma_{2}^{2}}$, which corresponds to inverse-variance weighting.", "answer": "$$\\boxed{\\frac{\\sigma_{2}^{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}}$$", "id": "1931719"}, {"introduction": "After conducting a Bayesian analysis, we often have a full posterior probability distribution describing our beliefs about an unknown parameter. To communicate a single-number summary, we must choose a point estimate, but which one is \"best\"? This exercise [@problem_id:1931727] demonstrates that the answer depends critically on our choice of loss function. By calculating the optimal estimates for a given posterior distribution under three different loss functions—squared error, absolute error, and zero-one loss—you will see firsthand how they correspond directly to the distribution's mean, median, and mode, respectively. This provides a powerful synthesis, clarifying that the notion of an \"optimal\" estimate is not universal but is defined by how we penalize error.", "problem": "In Bayesian statistical inference, a researcher models an unknown parameter $\\theta$, where $\\theta$ represents a proportion and is thus known to lie in the interval $[0, 1]$. After observing some data, the researcher determines that the posterior probability density function for $\\theta$ is given by an asymmetric triangular distribution.\n\nSpecifically, the posterior probability density function, $p(\\theta|\\text{data})$, is defined as:\n$$\np(\\theta|\\text{data}) = \\begin{cases} \n6\\theta & \\text{for } 0 \\le \\theta < 1/3 \\\\\n3(1-\\theta) & \\text{for } 1/3 \\le \\theta \\le 1 \n\\end{cases}\n$$\nand $p(\\theta|\\text{data}) = 0$ otherwise.\n\nThe goal is to determine three distinct Bayes point estimates for $\\theta$ derived from this posterior distribution. These estimates correspond to minimizing the posterior expected value of three different loss functions:\n1.  The estimator $\\hat{\\theta}_{SE}$, which minimizes the squared error loss, $L(\\theta, \\hat{\\theta}) = (\\theta - \\hat{\\theta})^2$.\n2.  The estimator $\\hat{\\theta}_{AE}$, which minimizes the absolute error loss, $L(\\theta, \\hat{\\theta}) = |\\theta - \\hat{\\theta}|$.\n3.  The estimator $\\hat{\\theta}_{01}$, which is the limiting Bayes rule for the zero-one loss function $L(\\theta, \\hat{\\theta}) = \\mathbb{I}(|\\theta - \\hat{\\theta}| > \\epsilon)$ as $\\epsilon \\to 0$.\n\nDetermine the exact values for the three estimators $\\hat{\\theta}_{SE}$, $\\hat{\\theta}_{AE}$, and $\\hat{\\theta}_{01}$. Provide your answer as a triplet of values $(\\hat{\\theta}_{SE}, \\hat{\\theta}_{AE}, \\hat{\\theta}_{01})$.", "solution": "The problem asks for three different Bayes point estimators for a parameter $\\theta$ with a given posterior probability density function (pdf). Each estimator corresponds to the minimization of a specific loss function's expected value over the posterior distribution.\n\nFirst, let's identify the statistical quantity corresponding to each estimator:\n1.  The estimator $\\hat{\\theta}_{SE}$ that minimizes the expected squared error is the mean of the posterior distribution. $\\hat{\\theta}_{SE} = E[\\theta|\\text{data}]$.\n2.  The estimator $\\hat{\\theta}_{AE}$ that minimizes the expected absolute error is the median of the posterior distribution.\n3.  The estimator $\\hat{\\theta}_{01}$ that minimizes the zero-one loss (in the continuous case) is the mode of the posterior distribution.\n\nThe posterior pdf is given as:\n$$\np(\\theta|\\text{data}) = \\begin{cases} \n6\\theta & \\text{for } 0 \\le \\theta < 1/3 \\\\\n3(1-\\theta) & \\text{for } 1/3 \\le \\theta \\le 1 \n\\end{cases}\n$$\n\nLet's calculate each of these quantities.\n\n**1. Calculation of the Mode ($\\hat{\\theta}_{01}$)**\n\nThe mode is the value of $\\theta$ that maximizes the posterior pdf $p(\\theta|\\text{data})$. We can analyze the two pieces of the function.\n- For $0 \\le \\theta < 1/3$, the function is $f_1(\\theta) = 6\\theta$, which is strictly increasing.\n- For $1/3 \\le \\theta \\le 1$, the function is $f_2(\\theta) = 3(1-\\theta)$, which is strictly decreasing.\nThe function increases up to $\\theta = 1/3$ and then decreases. Therefore, the maximum value of the pdf occurs at the point where the two pieces meet, which is $\\theta = 1/3$.\nThus, the mode is $\\hat{\\theta}_{01} = 1/3$.\n\n**2. Calculation of the Mean ($\\hat{\\theta}_{SE}$)**\n\nThe mean is the expected value of $\\theta$, calculated by the integral $E[\\theta] = \\int_{-\\infty}^{\\infty} \\theta p(\\theta|\\text{data}) d\\theta$. Given our pdf, this becomes:\n$$\n\\hat{\\theta}_{SE} = \\int_{0}^{1} \\theta p(\\theta|\\text{data}) d\\theta = \\int_{0}^{1/3} \\theta(6\\theta) d\\theta + \\int_{1/3}^{1} \\theta(3(1-\\theta)) d\\theta\n$$\nWe evaluate each integral separately.\n$$\n\\int_{0}^{1/3} 6\\theta^2 d\\theta = \\left[ 2\\theta^3 \\right]_{0}^{1/3} = 2\\left(\\frac{1}{3}\\right)^3 - 0 = 2\\left(\\frac{1}{27}\\right) = \\frac{2}{27}\n$$\n$$\n\\int_{1/3}^{1} (3\\theta - 3\\theta^2) d\\theta = \\left[ \\frac{3\\theta^2}{2} - \\theta^3 \\right]_{1/3}^{1} = \\left(\\frac{3}{2} - 1\\right) - \\left(\\frac{3(1/3)^2}{2} - (1/3)^3\\right) = \\frac{1}{2} - \\left(\\frac{3/9}{2} - \\frac{1}{27}\\right) = \\frac{1}{2} - \\left(\\frac{1}{6} - \\frac{1}{27}\\right)\n$$\n$$\n= \\frac{1}{2} - \\frac{9-2}{54} = \\frac{1}{2} - \\frac{7}{54} = \\frac{27-7}{54} = \\frac{20}{54} = \\frac{10}{27}\n$$\nNow, we sum the results of the two integrals:\n$$\n\\hat{\\theta}_{SE} = \\frac{2}{27} + \\frac{10}{27} = \\frac{12}{27} = \\frac{4}{9}\n$$\nSo, the mean is $\\hat{\\theta}_{SE} = 4/9$.\n\n**3. Calculation of the Median ($\\hat{\\theta}_{AE}$)**\n\nThe median, let's call it $m$, is the value such that the probability of $\\theta$ being less than $m$ is $0.5$.\n$$\n\\int_{0}^{m} p(\\theta|\\text{data}) d\\theta = 0.5\n$$\nFirst, let's find the cumulative probability up to the mode $\\theta = 1/3$.\n$$\n\\int_{0}^{1/3} 6\\theta d\\theta = \\left[ 3\\theta^2 \\right]_{0}^{1/3} = 3\\left(\\frac{1}{3}\\right)^2 = 3\\left(\\frac{1}{9}\\right) = \\frac{1}{3}\n$$\nSince $1/3 \\approx 0.333$ is less than $0.5$, the median must lie in the second interval, i.e., $m > 1/3$.\nThe condition for the median $m$ is:\n$$\n\\int_{0}^{1/3} p(\\theta|\\text{data}) d\\theta + \\int_{1/3}^{m} p(\\theta|\\text{data}) d\\theta = 0.5\n$$\nUsing our previous calculation:\n$$\n\\frac{1}{3} + \\int_{1/3}^{m} 3(1-\\theta) d\\theta = 0.5\n$$\nThis implies:\n$$\n\\int_{1/3}^{m} 3(1-\\theta) d\\theta = 0.5 - \\frac{1}{3} = \\frac{1}{2} - \\frac{1}{3} = \\frac{1}{6}\n$$\nNow, we evaluate the integral:\n$$\n\\int_{1/3}^{m} 3(1-\\theta) d\\theta = \\left[ 3\\theta - \\frac{3\\theta^2}{2} \\right]_{1/3}^{m} = \\left(3m - \\frac{3m^2}{2}\\right) - \\left(3\\left(\\frac{1}{3}\\right) - \\frac{3(1/3)^2}{2}\\right)\n$$\n$$\n= \\left(3m - \\frac{3m^2}{2}\\right) - \\left(1 - \\frac{3/9}{2}\\right) = \\left(3m - \\frac{3m^2}{2}\\right) - \\left(1 - \\frac{1}{6}\\right) = 3m - \\frac{3m^2}{2} - \\frac{5}{6}\n$$\nSetting this equal to $1/6$:\n$$\n3m - \\frac{3m^2}{2} - \\frac{5}{6} = \\frac{1}{6}\n$$\n$$\n3m - \\frac{3m^2}{2} = \\frac{6}{6} = 1\n$$\nMultiplying by 2 to clear the fraction, we get a quadratic equation for $m$:\n$$\n6m - 3m^2 = 2 \\implies 3m^2 - 6m + 2 = 0\n$$\nWe use the quadratic formula to solve for $m$:\n$$\nm = \\frac{-(-6) \\pm \\sqrt{(-6)^2 - 4(3)(2)}}{2(3)} = \\frac{6 \\pm \\sqrt{36 - 24}}{6} = \\frac{6 \\pm \\sqrt{12}}{6} = \\frac{6 \\pm 2\\sqrt{3}}{6} = 1 \\pm \\frac{\\sqrt{3}}{3}\n$$\nWe have two possible solutions: $m_1 = 1 + \\frac{\\sqrt{3}}{3}$ and $m_2 = 1 - \\frac{\\sqrt{3}}{3}$.\nSince the domain of $\\theta$ is $[0, 1]$, the solution $m_1 \\approx 1 + 0.577 = 1.577$ is outside the valid range. The other solution is $m_2 = 1 - \\frac{\\sqrt{3}}{3} \\approx 1 - 0.577 = 0.423$. This value lies in the interval $[1/3, 1] \\approx [0.333, 1]$, so this is the correct median.\nThus, the median is $\\hat{\\theta}_{AE} = 1 - \\frac{\\sqrt{3}}{3}$.\n\n**Conclusion**\nThe three estimators are:\n- $\\hat{\\theta}_{SE}$ (mean) = $4/9$\n- $\\hat{\\theta}_{AE}$ (median) = $1 - \\frac{\\sqrt{3}}{3}$\n- $\\hat{\\theta}_{01}$ (mode) = $1/3$\n\nThe problem asks for the triplet $(\\hat{\\theta}_{SE}, \\hat{\\theta}_{AE}, \\hat{\\theta}_{01})$.\nThe final answer is $(\\frac{4}{9}, 1 - \\frac{\\sqrt{3}}{3}, \\frac{1}{3})$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{4}{9} & 1 - \\frac{\\sqrt{3}}{3} & \\frac{1}{3} \\end{pmatrix}}$$", "id": "1931727"}]}