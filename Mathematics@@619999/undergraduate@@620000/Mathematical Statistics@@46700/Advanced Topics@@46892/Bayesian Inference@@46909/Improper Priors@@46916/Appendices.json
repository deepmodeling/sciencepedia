{"hands_on_practices": [{"introduction": "This first practice problem explores a foundational concept in Bayesian analysis: the ability of observed data to transform an improper prior into a valid, or 'proper,' posterior distribution. By working with an Exponential model, a common choice for modeling lifetimes or waiting times, you will determine the minimum amount of data needed to ensure your statistical inferences are well-founded [@problem_id:1922090]. This exercise highlights the crucial interplay between the prior specification and the sample size in achieving a legitimate posterior distribution.", "problem": "A statistician is analyzing the lifetime of a certain type of electronic component. The lifetime $X$ of a single component is modeled by an Exponential distribution with an unknown rate parameter $\\lambda > 0$. The probability density function (PDF) for a single observation is given by $f(x|\\lambda) = \\lambda \\exp(-\\lambda x)$ for $x > 0$.\n\nTo perform a Bayesian analysis on a random sample of $n$ components, $X_1, X_2, \\ldots, X_n$, the statistician chooses an improper prior distribution for the parameter $\\lambda$. This prior is defined by the relationship $p(\\lambda) \\propto \\lambda^{-3}$ for $\\lambda > 0$. An improper prior is one that does not integrate to a finite value over its domain.\n\nFor a Bayesian inference to be valid, the resulting posterior distribution, $p(\\lambda | X_1, \\ldots, X_n)$, must be a proper distribution. A distribution is considered 'proper' if its integral over its entire domain is finite, which allows for normalization into a valid probability distribution.\n\nAssuming that for any sample taken (i.e., for $n \\geq 1$), the sum of the observed lifetimes is positive, determine the minimum integer sample size $n$ required to ensure that the posterior distribution of $\\lambda$ is proper.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\operatorname{Exp}(\\lambda)$ with density $f(x \\mid \\lambda)=\\lambda \\exp(-\\lambda x)$ for $x>0$. The joint likelihood is\n$$\nL(\\lambda \\mid x_{1},\\ldots,x_{n})=\\prod_{i=1}^{n}\\lambda \\exp(-\\lambda x_{i})=\\lambda^{n}\\exp\\!\\left(-\\lambda \\sum_{i=1}^{n}x_{i}\\right).\n$$\nLet $S=\\sum_{i=1}^{n}x_{i}$. The prior is improper with density kernel $p(\\lambda)\\propto \\lambda^{-3}$ for $\\lambda>0$. The posterior density kernel is then\n$$\np(\\lambda \\mid x_{1},\\ldots,x_{n}) \\propto L(\\lambda \\mid x_{1},\\ldots,x_{n})\\,p(\\lambda)\n= \\lambda^{n}\\exp(-\\lambda S)\\cdot \\lambda^{-3}\n= \\lambda^{n-3}\\exp(-\\lambda S), \\quad \\lambda>0.\n$$\nAssuming $S>0$, this is proportional to a Gamma density with shape parameter $\\alpha=n-2$ and rate parameter $\\beta=S$, since the Gamma kernel is $\\lambda^{\\alpha-1}\\exp(-\\beta \\lambda)$. The posterior is proper if and only if its integral over $(0,\\infty)$ is finite:\n$$\n\\int_{0}^{\\infty}\\lambda^{n-3}\\exp(-S\\lambda)\\,d\\lambda<\\infty.\n$$\nUsing the standard Gamma integral,\n$$\n\\int_{0}^{\\infty}\\lambda^{a-1}\\exp(-\\beta \\lambda)\\,d\\lambda=\\frac{\\Gamma(a)}{\\beta^{a}} \\quad \\text{for } a>0,\\ \\beta>0,\n$$\nwe identify $a=n-2$ and $\\beta=S$. Since $S>0$ by assumption, finiteness requires $a>0$, i.e.,\n$$\nn-2>0 \\quad \\Longleftrightarrow \\quad n>2.\n$$\nBecause $n$ is an integer, the minimum $n$ ensuring a proper posterior is $n=3$. For $n=2$, the integrand behaves like $\\lambda^{-1}$ near $0$, and the integral diverges; for $n=1$, it behaves like $\\lambda^{-2}$ near $0$, which also diverges.", "answer": "$$\\boxed{3}$$", "id": "1922090"}, {"introduction": "While data can often remedy an improper prior, this is not a universal guarantee. This exercise presents a critical counterexample using the well-known Haldane prior for a single Bernoulli trial, a simple yet fundamental model for success/failure outcomes [@problem_id:1922144]. This practice will sharpen your understanding that the combination of the likelihood and the specific improper prior determines propriety, emphasizing the need to verify this condition in every analysis.", "problem": "In Bayesian inference, a prior distribution for a parameter is called *improper* if it does not integrate to a finite value over its domain. An improper prior cannot represent a true probability distribution. However, in many cases, combining an improper prior with observed data can yield a *proper* posterior distribution, which is a valid probability distribution that integrates to one.\n\nConsider a single experiment that can result in either a success ($X=1$) or a failure ($X=0$). This is known as a Bernoulli trial. The probability of success is governed by an unknown parameter $p$, where $p$ is in the interval $(0, 1)$. The probability mass function (the likelihood) for a single observation $x$ is given by $f(x|p) = p^{x}(1-p)^{1-x}$.\n\nFor the parameter $p$, we adopt the Haldane prior, which is an improper prior distribution specified by the proportionality relation:\n$$\np(p) \\propto \\frac{1}{p(1-p)} \\quad \\text{for } 0 < p < 1\n$$\nSuppose the experiment is performed once and results in a success ($x=1$). The posterior distribution, $p(p|x=1)$, is proportional to the product of the likelihood and the prior.\n\nWhich of the following statements correctly describes the resulting posterior distribution for the parameter $p$?\n\nA. Yes, the posterior distribution for $p$ is proper.\n\nB. No, the posterior distribution for $p$ is improper.\n\nC. The posterior distribution for $p$ would be proper only if a failure ($x=0$) had been observed instead of a success.\n\nD. The propriety of the posterior distribution cannot be determined because the prior distribution is improper.", "solution": "The posterior density is proportional to the product of the likelihood and the prior. For a single Bernoulli trial with observation $x=1$, the likelihood is\n$$\nf(1 \\mid p)=p^{1}(1-p)^{0}=p,\n$$\nand the Haldane prior is\n$$\n\\pi(p)\\propto \\frac{1}{p(1-p)}, \\quad 0<p<1.\n$$\nTherefore, the unnormalized posterior is\n$$\n\\pi(p \\mid x=1)\\propto f(1 \\mid p)\\,\\pi(p)=p \\cdot \\frac{1}{p(1-p)}=\\frac{1}{1-p}, \\quad 0<p<1.\n$$\nTo determine propriety, we check whether this can be normalized over $(0,1)$. The normalizing constant would be\n$$\nC^{-1}=\\int_{0}^{1}\\frac{1}{1-p}\\,dp.\n$$\nWith the substitution $u=1-p$, $du=-dp$, the integral becomes\n$$\n\\int_{0}^{1}\\frac{1}{1-p}\\,dp=\\int_{1}^{0}\\frac{-1}{u}\\,du=\\int_{0}^{1}\\frac{1}{u}\\,du,\n$$\nwhich diverges to $+\\infty$. Hence, no finite normalizing constant exists, and the posterior is improper.\n\nFor completeness, if $x=0$ had been observed, the unnormalized posterior would be\n$$\n\\pi(p \\mid x=0)\\propto (1-p)\\cdot \\frac{1}{p(1-p)}=\\frac{1}{p},\n$$\nand\n$$\n\\int_{0}^{1}\\frac{1}{p}\\,dp=+\\infty,\n$$\nso that case is also improper. Therefore, the correct statement is that the posterior for $x=1$ is improper, and the propriety can be determined despite the prior being improper.", "answer": "$$\\boxed{B}$$", "id": "1922144"}, {"introduction": "Moving beyond the binary question of propriety, this final problem investigates the tangible impact of different improper priors on the resulting statistical inference. We compare a standard flat prior with a physically motivated, restricted prior for the mean of a Normal distribution, two choices that both yield proper posteriors [@problem_id:1922111]. This exercise demonstrates how the choice of an 'uninformative' prior directly influences the posterior estimate, illustrating that all priors, proper or not, embed assumptions that shape your conclusions.", "problem": "Consider a single observation $x$ drawn from a Normal distribution with an unknown mean $\\mu$ and a known variance of 1. The Probability Density Function (PDF) of this observation given the parameter $\\mu$ is $f(x|\\mu) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^2\\right)$.\n\nTwo different Bayesian models are proposed to conduct inference on the unknown parameter $\\mu$.\n\n- **Model 1**: This model uses an improper prior distribution for $\\mu$ defined by the density $p_1(\\mu) \\propto 1$ for all real numbers $\\mu$.\n- **Model 2**: This model uses a different improper prior distribution for $\\mu$ defined by the density $p_2(\\mu) \\propto I(\\mu > 0)$, where $I(\\cdot)$ is the indicator function which evaluates to 1 if its argument is true and 0 otherwise.\n\nLet $E_1[\\mu]$ be the expected value of $\\mu$ under the posterior distribution derived from Model 1, and let $E_2[\\mu]$ be the expected value of $\\mu$ under the posterior distribution derived from Model 2.\n\nDetermine the ratio $\\frac{E_2[\\mu]}{E_1[\\mu]}$ as a function of the observation $x$. The final expression may involve the standard normal PDF, $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$, and the standard normal Cumulative Distribution Function (CDF), $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt$.", "solution": "We have one observation $x$ from $X \\mid \\mu \\sim \\mathcal{N}(\\mu,1)$, so the likelihood as a function of $\\mu$ is proportional to $f(x \\mid \\mu) \\propto \\exp\\!\\left(-\\frac{1}{2}(x-\\mu)^{2}\\right)$, equivalently $f(x \\mid \\mu) \\propto \\phi(x-\\mu)$ where $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{z^{2}}{2}\\right)$.\n\nModel 1 has prior $p_{1}(\\mu) \\propto 1$ for $\\mu \\in \\mathbb{R}$. The posterior is\n$$\np_{1}(\\mu \\mid x) \\propto f(x \\mid \\mu) p_{1}(\\mu) \\propto \\phi(x-\\mu).\n$$\nSince $\\int_{-\\infty}^{\\infty} \\phi(x-\\mu)\\,d\\mu = 1$, the normalized posterior is $p_{1}(\\mu \\mid x) = \\phi(\\mu - x)$, i.e., $\\mu \\mid x \\sim \\mathcal{N}(x,1)$. Therefore,\n$$\nE_{1}[\\mu] \\;=\\; \\int_{-\\infty}^{\\infty} \\mu\\, \\phi(\\mu - x)\\, d\\mu.\n$$\nWith the change of variable $t = \\mu - x$ (so $\\mu = t + x$ and $d\\mu = dt$), we get\n$$\nE_{1}[\\mu] = \\int_{-\\infty}^{\\infty} (t + x)\\, \\phi(t)\\, dt \\;=\\; x \\int_{-\\infty}^{\\infty} \\phi(t)\\, dt \\;+\\; \\int_{-\\infty}^{\\infty} t\\, \\phi(t)\\, dt \\;=\\; x + 0 \\;=\\; x.\n$$\n\nModel 2 has prior $p_{2}(\\mu) \\propto I(\\mu > 0)$. The posterior is\n$$\np_{2}(\\mu \\mid x) \\propto \\phi(x-\\mu)\\, I(\\mu > 0).\n$$\nTo normalize, compute\n$$\n\\int_{0}^{\\infty} \\phi(x-\\mu)\\, d\\mu \\;=\\; \\int_{0}^{\\infty} \\phi(\\mu - x)\\, d\\mu \\;=\\; \\int_{-x}^{\\infty} \\phi(t)\\, dt \\;=\\; 1 - \\Phi(-x) \\;=\\; \\Phi(x),\n$$\nwhere $\\Phi$ is the standard normal CDF and we used $\\Phi(-x) = 1 - \\Phi(x)$. Hence the normalized posterior is\n$$\np_{2}(\\mu \\mid x) \\;=\\; \\frac{\\phi(\\mu - x)}{\\Phi(x)}\\, I(\\mu > 0).\n$$\nTherefore,\n$$\nE_{2}[\\mu] \\;=\\; \\int_{0}^{\\infty} \\mu\\, \\frac{\\phi(\\mu - x)}{\\Phi(x)}\\, d\\mu \\;=\\; \\frac{1}{\\Phi(x)} \\int_{0}^{\\infty} \\mu\\, \\phi(\\mu - x)\\, d\\mu.\n$$\nApply the change of variable $t = \\mu - x$ (so $\\mu = t + x$, $d\\mu = dt$, and the lower limit becomes $t = -x$):\n$$\nE_{2}[\\mu] \\;=\\; \\frac{1}{\\Phi(x)} \\int_{-x}^{\\infty} (t + x)\\, \\phi(t)\\, dt \\;=\\; \\frac{1}{\\Phi(x)} \\left[ x \\int_{-x}^{\\infty} \\phi(t)\\, dt \\;+\\; \\int_{-x}^{\\infty} t\\, \\phi(t)\\, dt \\right].\n$$\nEvaluate the integrals:\n- $\\int_{-x}^{\\infty} \\phi(t)\\, dt = 1 - \\Phi(-x) = \\Phi(x)$.\n- For the second integral, use that $\\frac{d}{dt}\\phi(t) = -t\\,\\phi(t)$, hence $\\int t\\, \\phi(t)\\, dt = -\\phi(t) + C$, giving\n$$\n\\int_{-x}^{\\infty} t\\, \\phi(t)\\, dt \\;=\\; \\left[-\\phi(t)\\right]_{-x}^{\\infty} \\;=\\; 0 - \\left(-\\phi(-x)\\right) \\;=\\; \\phi(-x) \\;=\\; \\phi(x).\n$$\nThus,\n$$\nE_{2}[\\mu] \\;=\\; \\frac{1}{\\Phi(x)} \\left[ x\\, \\Phi(x) + \\phi(x) \\right] \\;=\\; x \\;+\\; \\frac{\\phi(x)}{\\Phi(x)}.\n$$\n\nFinally, the ratio is\n$$\n\\frac{E_{2}[\\mu]}{E_{1}[\\mu]} \\;=\\; \\frac{x + \\frac{\\phi(x)}{\\Phi(x)}}{x} \\;=\\; 1 + \\frac{\\phi(x)}{x\\, \\Phi(x)},\n$$\nwhich is defined for $x \\neq 0$; at $x=0$ the ratio is undefined because $E_{1}[\\mu]=0$.", "answer": "$$\\boxed{1+\\frac{\\phi(x)}{x\\,\\Phi(x)}}$$", "id": "1922111"}]}