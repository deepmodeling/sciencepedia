{"hands_on_practices": [{"introduction": "Bayesian inference provides a powerful framework for updating our beliefs in light of new evidence. A fundamental aspect of this approach is the prior distribution, which quantifies our initial beliefs before observing any data. This exercise [@problem_id:1899686] demonstrates the profound impact of the prior by comparing how a 'skeptical' and a 'non-informative' starting point lead to different conclusions, even when based on the exact same observation. Understanding this sensitivity is crucial for interpreting Bayesian results.", "problem": "A materials science lab is developing a new type of semiconductor for a high-frequency application. A critical manufacturing step has a certain probability, $p$, of success. Due to resource constraints, only a single attempt to produce the semiconductor can be made. The attempt is successful.\n\nTwo statisticians, an optimist and a skeptic, are tasked with providing an estimate for $p$ based on this single successful outcome. They both agree to model the outcome of the attempt as a Bernoulli trial and to use a Bayesian framework. Their disagreement lies in the prior belief about the success probability $p$.\n\nThe optimist, having confidence in the lab's general capabilities, assumes a \"non-informative\" prior for $p$, which is a Beta distribution with parameters $\\alpha_O = 1$ and $\\beta_O = 1$. This prior represents a uniform belief over all possible values of $p$ between 0 and 1.\n\nThe skeptic, aware of the high failure rates common in developing novel materials, assumes a \"skeptical\" prior for $p$, which is a Beta distribution with parameters $\\alpha_S = 1$ and $\\beta_S = 5$. This prior expresses a belief that low values of $p$ are more likely.\n\nAfter observing the single successful trial, both statisticians update their beliefs and calculate their respective posterior expected values for the success probability $p$. What is the absolute difference between the skeptic's posterior expected value and the optimist's posterior expected value? Express your answer as an exact fraction in simplest form.", "solution": "Model the outcome $X$ as a Bernoulli trial with success probability $p$. With a Beta prior $\\operatorname{Beta}(\\alpha,\\beta)$ and observing $s$ successes and $f$ failures, conjugacy gives the posterior $\\operatorname{Beta}(\\alpha+s,\\beta+f)$. The posterior mean for $\\operatorname{Beta}(a,b)$ is $a/(a+b)$.\n\nThere is a single attempt and it is successful, so $s=1$ and $f=0$.\n\nFor the optimist: prior $\\operatorname{Beta}(\\alpha_{O},\\beta_{O})$ with $\\alpha_{O}=1$, $\\beta_{O}=1$. The posterior is\n$$\n\\operatorname{Beta}(\\alpha_{O}+s,\\beta_{O}+f)=\\operatorname{Beta}(2,1),\n$$\nso the posterior expected value is\n$$\n\\mathbb{E}_{O}[p]=\\frac{2}{2+1}=\\frac{2}{3}.\n$$\n\nFor the skeptic: prior $\\operatorname{Beta}(\\alpha_{S},\\beta_{S})$ with $\\alpha_{S}=1$, $\\beta_{S}=5$. The posterior is\n$$\n\\operatorname{Beta}(\\alpha_{S}+s,\\beta_{S}+f)=\\operatorname{Beta}(2,5),\n$$\nso the posterior expected value is\n$$\n\\mathbb{E}_{S}[p]=\\frac{2}{2+5}=\\frac{2}{7}.\n$$\n\nThe absolute difference between the skeptic's and optimist's posterior expected values is\n$$\n\\left|\\mathbb{E}_{S}[p]-\\mathbb{E}_{O}[p]\\right|=\\left|\\frac{2}{7}-\\frac{2}{3}\\right|=\\left|\\frac{6-14}{21}\\right|=\\frac{8}{21}.\n$$", "answer": "$$\\boxed{\\frac{8}{21}}$$", "id": "1899686"}, {"introduction": "A primary application of building a statistical model is to make predictions about future observations. In this problem [@problem_id:1899633], you will move beyond simply estimating a parameter by using your updated beliefs to forecast future outcomes. You will first determine the posterior distribution for an unknown probability $p$ and then use it to find the expected number of successes in a new set of trials, a key technique in Bayesian prediction.", "problem": "A materials science company is developing a new type of photovoltaic cell. The manufacturing process is currently unstable, and each cell produced has an unknown, but constant, probability $p$ of meeting a high-efficiency standard. The engineering team, lacking any preliminary data, assumes that any value of $p$ between 0 and 1 is equally likely.\n\nIn a pilot production run, a batch of $n$ cells is manufactured, and upon testing, $k$ of them are found to meet the high-efficiency standard. The company now plans to start a larger production run of $m$ new cells.\n\nUsing a Bayesian approach, what is the expected number of high-efficiency cells in this new batch of $m$ cells? Derive a symbolic expression for this quantity in terms of $n$, $k$, and $m$.", "solution": "Let $p$ be the unknown probability that a manufactured cell meets the high-efficiency standard. The problem states that the prior belief about $p$ is that it is equally likely to be any value between 0 and 1. This corresponds to a uniform prior distribution on the interval $[0, 1]$. The probability density function (PDF) for this prior is given by:\n$$ \\pi(p) = 1, \\quad \\text{for } 0 \\le p \\le 1 $$\nThis is a special case of the Beta distribution, specifically $\\text{Beta}(\\alpha=1, \\beta=1)$.\n\nIn the pilot run, $n$ cells are produced, which can be modeled as $n$ independent Bernoulli trials. Let $X$ be the random variable representing the number of high-efficiency cells observed. The observation is that $X=k$. The probability of observing $k$ successes in $n$ trials, given the probability $p$, follows a Binomial distribution. The likelihood function is:\n$$ L(p|X=k) = P(X=k|p) = \\binom{n}{k} p^k (1-p)^{n-k} $$\n\nTo update our belief about $p$ after observing the data, we use Bayes' theorem to find the posterior distribution of $p$. The posterior PDF, $\\pi(p|X=k)$, is proportional to the product of the prior and the likelihood:\n$$ \\pi(p|X=k) \\propto L(p|X=k) \\cdot \\pi(p) $$\n$$ \\pi(p|X=k) \\propto \\left( \\binom{n}{k} p^k (1-p)^{n-k} \\right) \\cdot 1 $$\nSince $\\binom{n}{k}$ is a constant with respect to $p$, we can write:\n$$ \\pi(p|X=k) \\propto p^k (1-p)^{n-k} $$\nThis expression is the kernel of a Beta distribution. A general Beta distribution, $\\text{Beta}(\\alpha, \\beta)$, has a PDF proportional to $p^{\\alpha-1}(1-p)^{\\beta-1}$. By comparing the exponents, we can identify the parameters of our posterior distribution:\n$$ \\alpha - 1 = k \\implies \\alpha = k+1 $$\n$$ \\beta - 1 = n-k \\implies \\beta = n-k+1 $$\nThus, the posterior distribution for $p$, given the data $X=k$, is a Beta distribution with parameters $\\alpha=k+1$ and $\\beta=n-k+1$:\n$$ p | (X=k) \\sim \\text{Beta}(k+1, n-k+1) $$\n\nThe problem asks for the expected number of high-efficiency cells in a future batch of $m$ trials. Let $Y$ be the random variable for the number of successes in these $m$ future trials. Given a value of $p$, $Y$ follows a Binomial distribution, $Y|p \\sim \\text{Binomial}(m, p)$. The expectation of $Y$ given $p$ is $E[Y|p] = mp$.\n\nWe need to find the Bayesian predictive expectation of $Y$, which is the expectation of $Y$ averaged over the posterior distribution of $p$. This is denoted as $E[Y|X=k]$. We use the law of total expectation:\n$$ E[Y|X=k] = E_{p|k}[E[Y|p]] $$\nwhere the outer expectation is taken with respect to the posterior distribution of $p$.\nSubstituting $E[Y|p] = mp$, we get:\n$$ E[Y|X=k] = E_{p|k}[mp] = m \\cdot E_{p|k}[p] $$\nThe term $E_{p|k}[p]$ is the mean of the posterior distribution of $p$. The mean of a $\\text{Beta}(\\alpha, \\beta)$ distribution is given by the formula $\\frac{\\alpha}{\\alpha+\\beta}$.\nFor our posterior distribution, $\\text{Beta}(k+1, n-k+1)$, the mean is:\n$$ E_{p|k}[p] = \\frac{k+1}{(k+1) + (n-k+1)} = \\frac{k+1}{n+2} $$\nThis is the updated estimate for the probability $p$.\n\nFinally, the expected number of successes in the next $m$ trials is:\n$$ E[Y|X=k] = m \\cdot \\frac{k+1}{n+2} $$\nThis expression represents the Bayes estimate for the number of high-efficiency cells under a squared error loss function.", "answer": "$$\\boxed{m \\frac{k+1}{n+2}}$$", "id": "1899633"}, {"introduction": "While the posterior mean provides a robust estimate under squared error loss, other estimators can offer different insights. This exercise [@problem_id:1899685] introduces the Maximum A Posteriori (MAP) estimate, which seeks the single most probable value of a parameter given the data. You will apply this method to a scenario with a discrete parameter space, learning how to identify the most plausible hypothesis among a finite set of options.", "problem": "A company specializing in tabletop games is testing a new manufacturing process for producing non-standard dice. The number of faces on any given die, denoted by the parameter $N$, is unknown. Based on the manufacturing specifications, it is known that $N$ can only take one of three possible values: 6, 8, or 12. Initially, with no experimental data, each of these possibilities is considered equally likely.\n\nTo determine the most probable number of faces for a particular die, a single experiment is conducted: the die is rolled once, and the outcome is observed to be a '5'. Assume the die is fair, meaning that for a given $N$, each of the faces numbered 1 to $N$ has an equal probability of being rolled.\n\nYour task is to calculate the Maximum A Posteriori (MAP) estimate for the number of faces, $N$, given this single observation.", "solution": "Let the hypothesis space be $\\mathcal{H}=\\{6,8,12\\}$. The prior over $N$ is uniform, so for each $N \\in \\mathcal{H}$,\n$$\nP(N)=\\frac{1}{3}.\n$$\nLet the single observation be $X=5$. Given a fair $N$-sided die, the likelihood is\n$$\nP(X=5 \\mid N)=\\begin{cases}\n\\frac{1}{N}, & \\text{if } 5 \\leq N,\\\\\n0, & \\text{if } 5 > N.\n\\end{cases}\n$$\nSince $5 \\leq 6,8,12$, we have $P(X=5 \\mid N)=\\frac{1}{N}$ for all $N \\in \\{6,8,12\\}$.\n\nBy Bayes' rule, for each $N \\in \\mathcal{H}$,\n$$\nP(N \\mid X=5)=\\frac{P(N)\\,P(X=5 \\mid N)}{\\sum_{n \\in \\mathcal{H}} P(n)\\,P(X=5 \\mid n)}=\\frac{\\frac{1}{3}\\cdot \\frac{1}{N}}{\\sum_{n \\in \\{6,8,12\\}} \\frac{1}{3}\\cdot \\frac{1}{n}}=\\frac{\\frac{1}{N}}{\\sum_{n \\in \\{6,8,12\\}} \\frac{1}{n}}.\n$$\nTherefore, the posterior is proportional to $\\frac{1}{N}$. The Maximum A Posteriori (MAP) estimate is the $N$ that maximizes $P(N \\mid X=5)$, equivalently maximizes $\\frac{1}{N}$, which is achieved by the smallest feasible $N$. Among $\\{6,8,12\\}$, the smallest is $6$.\n\nHence, the MAP estimate is $N=6$.", "answer": "$$\\boxed{6}$$", "id": "1899685"}]}