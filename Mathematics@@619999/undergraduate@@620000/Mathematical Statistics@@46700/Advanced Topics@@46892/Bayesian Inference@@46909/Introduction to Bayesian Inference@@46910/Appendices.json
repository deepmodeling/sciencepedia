{"hands_on_practices": [{"introduction": "The cornerstone of Bayesian inference is the process of updating our beliefs in light of new evidence. This first exercise provides a classic example of this process in action. We will explore how to combine prior knowledge about a parameter—in this case, the success probability $p$ of a process—with observed data to form an updated, more informed posterior belief [@problem_id:1923967]. By using a Beta distribution as a prior for a parameter of a Geometric likelihood, we'll see the elegance of conjugate priors, where the posterior distribution belongs to the same family as the prior, making the update intuitive and mathematically tractable.", "problem": "A quantum engineer is testing a new protocol for initializing a quantum bit (qubit). The probability of a successful initialization on any given attempt is an unknown constant $p$, where $0 < p < 1$. Each initialization attempt is an independent trial.\n\nThe engineer's prior belief about the probability $p$ is described by a Beta distribution with known hyperparameters $\\alpha_0 > 0$ and $\\beta_0 > 0$. The probability density function of a $\\text{Beta}(\\alpha, \\beta)$ distribution for a variable $x \\in [0, 1]$ is proportional to $x^{\\alpha-1}(1-x)^{\\beta-1}$.\n\nIn an experiment, the engineer performs initialization attempts repeatedly until the first successful initialization is achieved. It is observed that the first success occurs on the $k$-th attempt.\n\nGiven this observation, the engineer updates their belief about $p$. The posterior distribution for $p$ is also a Beta distribution. Which of the following represents the new parameters, $(\\alpha_{\\text{post}}, \\beta_{\\text{post}})$, of this posterior distribution?\n\nA. $(\\alpha_0 + k, \\beta_0 + 1)$\n\nB. $(\\alpha_0 + k - 1, \\beta_0 + 1)$\n\nC. $(\\alpha_0 + 1, \\beta_0 + k)$\n\nD. $(\\alpha_0 + 1, \\beta_0 + k - 1)$\n\nE. $(\\alpha_0, \\beta_0 + k - 1)$", "solution": "Let $p \\in (0,1)$ denote the success probability per attempt. The prior for $p$ is $\\text{Beta}(\\alpha_{0},\\beta_{0})$ with density (up to normalization)\n$$\nf(p)\\propto p^{\\alpha_{0}-1}(1-p)^{\\beta_{0}-1}.\n$$\nThe observation is that the first success occurs on the $k$-th attempt. Under independent Bernoulli trials with success probability $p$, this event has likelihood given by the geometric model:\n$$\n\\mathcal{L}(p;k)=P(K=k\\mid p)=(1-p)^{k-1}p,\n$$\nsince it consists of $k-1$ failures followed by one success.\n\nBy Bayes’ rule, the posterior density is\n$$\nf(p\\mid k)=\\frac{\\mathcal{L}(p;k)f(p)}{\\int_{0}^{1}\\mathcal{L}(p;k)f(p)\\,dp}\\propto \\mathcal{L}(p;k)f(p).\n$$\nSubstituting the prior kernel and the likelihood,\n$$\nf(p\\mid k)\\propto \\left[p^{\\alpha_{0}-1}(1-p)^{\\beta_{0}-1}\\right]\\left[(1-p)^{k-1}p\\right]\n= p^{\\alpha_{0}-1+1}(1-p)^{\\beta_{0}-1+k-1}\n= p^{\\alpha_{0}}(1-p)^{\\beta_{0}+k-2}.\n$$\nRecognizing the Beta kernel $p^{\\alpha_{\\text{post}}-1}(1-p)^{\\beta_{\\text{post}}-1}$, we identify\n$$\n\\alpha_{\\text{post}}-1=\\alpha_{0}\\quad\\Rightarrow\\quad \\alpha_{\\text{post}}=\\alpha_{0}+1,\\qquad\n\\beta_{\\text{post}}-1=\\beta_{0}+k-2\\quad\\Rightarrow\\quad \\beta_{\\text{post}}=\\beta_{0}+k-1.\n$$\nEquivalently, the observation “first success at $k$” encodes $1$ success and $k-1$ failures, yielding the standard Beta update $\\alpha_{0}\\mapsto \\alpha_{0}+1$ and $\\beta_{0}\\mapsto \\beta_{0}+k-1$. Therefore the correct choice is $(\\alpha_{0}+1,\\beta_{0}+k-1)$.", "answer": "$$\\boxed{D}$$", "id": "1923967"}, {"introduction": "Once we have a posterior distribution that encapsulates our updated beliefs, a common next step is to summarize it to make a concrete estimate. This problem challenges you to do just that by calculating the posterior median, a robust measure of central tendency. This exercise highlights a significant strength of the Bayesian framework: its ability to provide reasonable answers even with sparse or extreme data, such as observing zero successes in a series of trials [@problem_id:1923968]. By starting with a uniform prior, representing initial impartiality, you will see how Bayesian estimation gracefully handles situations where other methods might yield impractical results.", "problem": "A new, high-risk surgical procedure is being developed for a condition that was previously considered untreatable. The true probability of success for this procedure, denoted by $p$, is unknown. Before conducting any clinical trials, medical researchers hold no preconceived bias about this probability, and thus consider every possible value of $p$ in the interval $[0, 1]$ to be equally likely.\n\nThe procedure is then attempted on $n$ different patients. In every one of these $n$ independent trials, the procedure fails to achieve a successful outcome.\n\nBased on this observed data of zero successes in $n$ trials, an updated belief about the success probability $p$ is formed. A medical statistician decides to summarize this updated belief by calculating a central value, $p_{med}$. This value is defined such that, given the observed data, it is equally probable for the true success rate $p$ to be less than or equal to $p_{med}$ as it is for it to be greater than $p_{med}$.\n\nFind a closed-form analytic expression for $p_{med}$ in terms of $n$.", "solution": "Let $p$ denote the success probability. The prior belief is uniform on $[0,1]$, which is the $\\operatorname{Beta}(1,1)$ prior with density $\\pi(p)=1$ for $p\\in[0,1]$.\n\nWe observe $n$ independent Bernoulli trials with $k=0$ successes. The likelihood is\n$$\nL(p\\mid \\text{data}) \\propto p^{k}(1-p)^{n-k}=(1-p)^{n}.\n$$\nBy Bayes’ theorem, the posterior is proportional to prior times likelihood:\n$$\n\\pi(p\\mid \\text{data}) \\propto \\pi(p)L(p\\mid \\text{data}) \\propto (1-p)^{n}, \\quad 0\\leq p\\leq 1.\n$$\nThis is a $\\operatorname{Beta}(1,n+1)$ distribution. Normalizing explicitly,\n$$\n\\int_{0}^{1} (1-p)^{n}\\,dp=\\frac{1}{n+1},\n$$\nso the posterior density is\n$$\nf(p\\mid \\text{data})=(n+1)(1-p)^{n}, \\quad 0\\leq p\\leq 1.\n$$\nThe posterior cumulative distribution function is\n$$\nF(p)=\\int_{0}^{p} (n+1)(1-t)^{n}\\,dt.\n$$\nCompute the integral via $u=1-t$, $du=-dt$:\n$$\nF(p)=(n+1)\\int_{0}^{p} (1-t)^{n}\\,dt=(n+1)\\int_{1-p}^{1} u^{n}\\,du\n= (n+1)\\left[\\frac{u^{n+1}}{n+1}\\right]_{1-p}^{1}\n=1-(1-p)^{n+1}.\n$$\nThe posterior median $p_{med}$ satisfies $F(p_{med})=\\frac{1}{2}$, hence\n$$\n1-(1-p_{med})^{n+1}=\\frac{1}{2}\n\\;\\;\\Longrightarrow\\;\\;\n(1-p_{med})^{n+1}=\\frac{1}{2}.\n$$\nTaking the $(n+1)$-th root gives\n$$\n1-p_{med}=2^{-\\frac{1}{n+1}},\n$$\nso\n$$\np_{med}=1-2^{-\\frac{1}{n+1}}.\n$$\nThis expression is valid for all $n\\geq 0$ and, in particular, for $n\\geq 1$ as in the problem context.", "answer": "$$\\boxed{1-2^{-\\frac{1}{n+1}}}$$", "id": "1923968"}, {"introduction": "Beyond estimating parameters, a key purpose of statistical modeling is to make predictions about future events. This final practice moves us from inference to forecasting by introducing the posterior predictive distribution. This concept involves averaging the predictions of our model over the entire posterior distribution of the unknown parameter, $\\lambda$, thereby accounting for our uncertainty about its true value [@problem_id:1923970]. Working with the well-known Poisson-Gamma conjugate model, you will calculate the probability of a future outcome, demonstrating how to leverage your updated beliefs to look ahead.", "problem": "A software development company is running a closed beta test for its new application. They model the number of bugs reported per month, $X$, by a Poisson distribution with an unknown average rate $\\lambda$. The probability mass function is given by $P(X=k | \\lambda) = \\frac{\\exp(-\\lambda) \\lambda^{k}}{k!}$ for $k=0, 1, 2, \\dots$.\n\nBased on historical data from similar projects, the project manager's prior belief about the rate $\\lambda$ is described by a Gamma distribution with a shape parameter $\\alpha = 2$ and a rate parameter $\\beta = 1$. The probability density function of this prior is $f(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda)$ for $\\lambda > 0$.\n\nDuring the first month of beta testing, the testers report a total of 3 bugs.\n\nAssuming that the underlying bug rate $\\lambda$ remains constant, calculate the probability that zero bugs will be reported in the second month of testing. Round your final answer to four significant figures.", "solution": "We model $X \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$ with probability mass function $P(X=k \\mid \\lambda)=\\frac{\\exp(-\\lambda)\\lambda^{k}}{k!}$ and prior $\\lambda \\sim \\text{Gamma}(\\alpha,\\beta)$ with density $f(\\lambda)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\beta\\lambda)$, where $\\alpha=2$ and $\\beta=1$.\n\nGiven the first-month observation $X_{1}=3$, the likelihood is\n$$\nL(\\lambda \\mid X_{1}=3)\\propto \\exp(-\\lambda)\\lambda^{3}.\n$$\nCombining with the prior,\n$$\np(\\lambda \\mid X_{1}=3)\\propto \\lambda^{\\alpha-1}\\exp(-\\beta\\lambda)\\cdot \\exp(-\\lambda)\\lambda^{3}\n=\\lambda^{\\alpha+3-1}\\exp\\big(-( \\beta+1)\\lambda\\big).\n$$\nRecognizing the kernel of a Gamma density, the posterior is\n$$\n\\lambda \\mid X_{1}=3 \\sim \\text{Gamma}(\\alpha',\\beta'), \\quad \\alpha'=\\alpha+3,\\ \\beta'=\\beta+1,\n$$\nso here $\\alpha'=5$ and $\\beta'=2$.\n\nThe posterior predictive probability for zero bugs in the second month is\n$$\nP(X_{2}=0 \\mid X_{1}=3)=\\int_{0}^{\\infty}P(X_{2}=0 \\mid \\lambda)\\,p(\\lambda \\mid X_{1}=3)\\,d\\lambda\n=\\int_{0}^{\\infty}\\exp(-\\lambda)\\,\\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')}\\lambda^{\\alpha'-1}\\exp(-\\beta'\\lambda)\\,d\\lambda.\n$$\nEvaluate the integral using the Gamma integral identity $\\int_{0}^{\\infty}\\lambda^{\\alpha'-1}\\exp(-c\\lambda)\\,d\\lambda=\\frac{\\Gamma(\\alpha')}{c^{\\alpha'}}$ for $c>0$:\n$$\nP(X_{2}=0 \\mid X_{1}=3)=\\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')}\\cdot \\frac{\\Gamma(\\alpha')}{(\\beta'+1)^{\\alpha'}}=\\left(\\frac{\\beta'}{\\beta'+1}\\right)^{\\alpha'}.\n$$\nSubstituting $\\alpha'=5$ and $\\beta'=2$ gives\n$$\nP(X_{2}=0 \\mid X_{1}=3)=\\left(\\frac{2}{3}\\right)^{5}=\\frac{32}{243}\\approx 0.131687\\ldots\n$$\nRounded to four significant figures, this is $0.1317$.", "answer": "$$\\boxed{0.1317}$$", "id": "1923970"}]}