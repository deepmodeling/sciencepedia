{"hands_on_practices": [{"introduction": "This first exercise provides a foundational walkthrough of the Bayesian updating process. We will begin with a prior belief about a parameter—in this case, the rate parameter $\\lambda$ of an exponential distribution—and see how that belief is refined by a single data point. This problem is a classic example of using conjugate priors, where the posterior distribution belongs to the same family as the prior, which simplifies the mathematical steps and allows us to focus on the core logic of belief updating. [@problem_id:1946583]", "problem": "An engineering team is testing the lifetime of a new type of experimental Light Emitting Diode (LED). The lifetime $X$ of a single LED, measured in thousands of hours, is modeled by an Exponential distribution with an unknown rate parameter $\\lambda > 0$. The probability density function (PDF) for the lifetime $X$ given the parameter $\\lambda$ is:\n$$f(x|\\lambda) = \\lambda \\exp(-\\lambda x) \\quad \\text{for } x \\ge 0$$\nBased on previous experience with similar technologies, the team's prior belief about the rate parameter $\\lambda$ is described by a Gamma distribution with shape parameter $\\alpha = 2$ and rate parameter $\\beta = 3$. The PDF of this prior distribution is proportional to:\n$$p(\\lambda) \\propto \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda) \\quad \\text{for } \\lambda > 0$$\nA single LED is selected for testing and its lifetime is observed to be $x = 5$ thousand hours. Determine the mode of the posterior distribution of $\\lambda$ given this observation. Express your answer as an exact value.", "solution": "The problem asks for the mode of the posterior distribution of the parameter $\\lambda$. We can find the posterior distribution using Bayes' theorem for continuous parameters. The theorem states that the posterior probability density is proportional to the product of the likelihood and the prior probability density.\n\nThe posterior PDF is given by:\n$$p(\\lambda | x) \\propto f(x | \\lambda) p(\\lambda)$$\n\nWe are given the likelihood function, which is the PDF of the Exponential distribution for a single observation $x=5$:\n$$f(x=5 | \\lambda) = \\lambda \\exp(-5\\lambda)$$\n\nWe are also given that the prior distribution for $\\lambda$ is a Gamma distribution with shape parameter $\\alpha=2$ and rate parameter $\\beta=3$. The prior PDF is proportional to:\n$$p(\\lambda) \\propto \\lambda^{\\alpha-1} \\exp(-\\beta\\lambda) = \\lambda^{2-1} \\exp(-3\\lambda) = \\lambda^{1} \\exp(-3\\lambda)$$\n\nNow, we multiply the likelihood and the prior to find the unnormalized posterior distribution:\n$$p(\\lambda | x=5) \\propto \\left( \\lambda \\exp(-5\\lambda) \\right) \\cdot \\left( \\lambda^{1} \\exp(-3\\lambda) \\right)$$\nCombining the terms with $\\lambda$:\n$$p(\\lambda | x=5) \\propto \\lambda^{1+1} \\exp(-5\\lambda - 3\\lambda)$$\n$$p(\\lambda | x=5) \\propto \\lambda^{2} \\exp(-8\\lambda)$$\n\nThis expression is the kernel of a Gamma distribution. A Gamma distribution with shape parameter $\\alpha_{\\text{post}}$ and rate parameter $\\beta_{\\text{post}}$ has a PDF proportional to $\\lambda^{\\alpha_{\\text{post}}-1} \\exp(-\\beta_{\\text{post}}\\lambda)$. By comparing this form to our result, we can identify the parameters of the posterior distribution:\n$$\\alpha_{\\text{post}} - 1 = 2 \\implies \\alpha_{\\text{post}} = 3$$\n$$\\beta_{\\text{post}} = 8$$\nSo, the posterior distribution of $\\lambda$ is a Gamma distribution, $\\lambda | (x=5) \\sim \\text{Gamma}(3, 8)$.\n\nThe mode of a distribution is the value at which its PDF is maximized. To find the mode of the posterior distribution, we need to find the value of $\\lambda$ that maximizes $p(\\lambda|x=5)$. It is often easier to maximize the natural logarithm of the PDF. Let $L(\\lambda) = \\ln(p(\\lambda|x=5))$.\n$$L(\\lambda) = \\ln(C \\lambda^{2} \\exp(-8\\lambda)) = \\ln(C) + 2\\ln(\\lambda) - 8\\lambda$$\nwhere $C$ is the normalization constant.\n\nTo find the maximum, we take the derivative of $L(\\lambda)$ with respect to $\\lambda$ and set it to zero:\n$$\\frac{d L(\\lambda)}{d \\lambda} = \\frac{2}{\\lambda} - 8$$\nSetting the derivative to zero to find the critical point:\n$$\\frac{2}{\\lambda} - 8 = 0 \\implies \\frac{2}{\\lambda} = 8 \\implies \\lambda = \\frac{2}{8} = \\frac{1}{4}$$\n\nTo confirm this critical point is a maximum, we can check the second derivative:\n$$\\frac{d^2 L(\\lambda)}{d \\lambda^2} = -\\frac{2}{\\lambda^2}$$\nSince $\\lambda$ must be positive, the second derivative is always negative. This confirms that $\\lambda = 1/4$ is a local maximum. As it's the only critical point in the domain $\\lambda > 0$, it is the global maximum, and thus the mode of the posterior distribution.\n\nAlternatively, the mode of a Gamma($\\alpha, \\beta$) distribution is given by the formula $\\frac{\\alpha-1}{\\beta}$ for $\\alpha > 1$. Our posterior distribution is Gamma(3, 8). Since $\\alpha_{\\text{post}} = 3 > 1$, we can use this formula:\n$$\\text{Mode} = \\frac{\\alpha_{\\text{post}} - 1}{\\beta_{\\text{post}}} = \\frac{3-1}{8} = \\frac{2}{8} = \\frac{1}{4}$$\nBoth methods yield the same result. The mode of the posterior distribution of $\\lambda$ is $1/4$.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "1946583"}, {"introduction": "Building on the basics, this practice introduces a common and important scenario where the parameter you are trying to estimate directly defines the range of possible data values. Unlike the previous problem, the posterior distribution here doesn't fall into a simple, named family, requiring you to carefully construct it by considering the valid ranges for both the prior and the likelihood. Solving this will sharpen your skills in applying Bayes' theorem from first principles and correctly handling the support of a distribution. [@problem_id:1946605]", "problem": "An engineer is testing a new type of experimental microchip. The maximum operational lifespan of this chip, denoted by the parameter $\\theta$ (in hours), is unknown. Based on the manufacturing process, the engineer's prior belief is that $\\theta$ is equally likely to be any value in the interval $[0, 10]$. The lifetime of any individual chip, $X$, is known to follow a uniform distribution on the interval $[0, \\theta]$. A single chip is tested and is observed to fail at exactly $x=3$ hours. Given this observation, what is the updated expected value for the maximum operational lifespan $\\theta$?\n\nProvide your answer in hours, rounded to four significant figures.", "solution": "This problem requires the use of Bayesian inference to update our belief about the parameter $\\theta$ after observing data. We need to find the posterior distribution of $\\theta$ given the observation $x=3$, and then calculate its expected value.\n\nStep 1: Define the prior distribution.\nThe problem states that the prior belief for $\\theta$ is that it is equally likely to be any value in the interval $[0, 10]$. This corresponds to a uniform distribution. The Probability Density Function (PDF) of the prior distribution, $p(\\theta)$, is:\n$$\np(\\theta) = \n\\begin{cases} \n\\frac{1}{10 - 0} = \\frac{1}{10} & \\text{for } 0 \\le \\theta \\le 10 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nStep 2: Define the likelihood function.\nThe lifetime of a single chip, $X$, follows a uniform distribution on $[0, \\theta]$. The PDF for a single observation $x$ given the parameter $\\theta$ is the likelihood function, $p(x|\\theta)$:\n$$\np(x|\\theta) = \n\\begin{cases} \n\\frac{1}{\\theta} & \\text{for } 0 \\le x \\le \\theta \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nWe are given a single observation $x=3$. For this observation to be possible, the parameter $\\theta$ must be greater than or equal to 3. If $\\theta < 3$, the probability of observing $x=3$ is zero. Therefore, the likelihood function for our specific data $x=3$ is:\n$$\np(x=3|\\theta) = \n\\begin{cases} \n\\frac{1}{\\theta} & \\text{for } \\theta \\ge 3 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nStep 3: Determine the posterior distribution.\nAccording to Bayes' theorem, the posterior distribution, $p(\\theta|x)$, is proportional to the product of the likelihood and the prior distribution:\n$$\np(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)\n$$\nWe need to consider the ranges where both functions are non-zero.\nThe prior $p(\\theta)$ is non-zero for $0 \\le \\theta \\le 10$.\nThe likelihood $p(x=3|\\theta)$ is non-zero for $\\theta \\ge 3$.\nThe product is non-zero only on the intersection of these two ranges, which is $3 \\le \\theta \\le 10$.\n\nFor $\\theta$ in the interval $[3, 10]$, we have:\n$$\np(\\theta|x=3) \\propto \\left(\\frac{1}{\\theta}\\right) \\cdot \\left(\\frac{1}{10}\\right) \\propto \\frac{1}{\\theta}\n$$\nSo, the posterior PDF has the form $p(\\theta|x=3) = \\frac{k}{\\theta}$ for $3 \\le \\theta \\le 10$, and 0 otherwise, where $k$ is a normalization constant.\n\nStep 4: Calculate the normalization constant.\nTo be a valid PDF, the posterior distribution must integrate to 1 over its domain.\n$$\n\\int_{3}^{10} p(\\theta|x=3) d\\theta = 1\n$$\n$$\n\\int_{3}^{10} \\frac{k}{\\theta} d\\theta = 1\n$$\n$$\nk \\int_{3}^{10} \\frac{1}{\\theta} d\\theta = 1\n$$\n$$\nk [\\ln|\\theta|]_{3}^{10} = 1\n$$\n$$\nk (\\ln(10) - \\ln(3)) = 1\n$$\n$$\nk \\ln\\left(\\frac{10}{3}\\right) = 1\n$$\n$$\nk = \\frac{1}{\\ln(10/3)}\n$$\nThus, the full posterior PDF is:\n$$\np(\\theta|x=3) = \n\\begin{cases} \n\\frac{1}{\\theta \\ln(10/3)} & \\text{for } 3 \\le \\theta \\le 10 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nStep 5: Calculate the expected value of the posterior distribution.\nThe question asks for the updated expected value of $\\theta$, which is the mean of the posterior distribution, $E[\\theta|x=3]$.\n$$\nE[\\theta|x=3] = \\int_{-\\infty}^{\\infty} \\theta \\cdot p(\\theta|x=3) d\\theta\n$$\n$$\nE[\\theta|x=3] = \\int_{3}^{10} \\theta \\cdot \\frac{1}{\\theta \\ln(10/3)} d\\theta\n$$\n$$\nE[\\theta|x=3] = \\frac{1}{\\ln(10/3)} \\int_{3}^{10} 1 d\\theta\n$$\n$$\nE[\\theta|x=3] = \\frac{1}{\\ln(10/3)} [\\theta]_{3}^{10}\n$$\n$$\nE[\\theta|x=3] = \\frac{10 - 3}{\\ln(10/3)} = \\frac{7}{\\ln(10/3)}\n$$\n\nStep 6: Compute the numerical value.\nNow we calculate the numerical value and round it to four significant figures.\n$$\n\\ln(10/3) \\approx 1.203972804\n$$\n$$\nE[\\theta|x=3] \\approx \\frac{7}{1.203972804} \\approx 5.8140645\n$$\nRounding to four significant figures, we get 5.814. The unit is hours.", "answer": "$$\\boxed{5.814}$$", "id": "1946605"}, {"introduction": "A crucial aspect of Bayesian modeling is the selection of a prior distribution. This exercise directly contrasts the impact of two different types of priors—an informative prior that encodes specific beliefs, and a non-informative (or \"objective\") prior designed to have minimal influence on the posterior. By calculating and comparing the resulting estimates for a Poisson rate parameter $\\lambda$, you will gain a deeper appreciation for how the choice of prior acts as a regularizer and shapes the final inference. [@problem_id:1946616]", "problem": "A digital marketing analyst is modeling the number of users who click on a specific advertisement link per hour. The number of clicks, $X$, is assumed to follow a Poisson distribution with an unknown average rate $\\lambda$. To estimate this rate, the analyst collects data over $n$ independent and identically distributed (i.i.d.) one-hour intervals, recording the click counts $x_1, x_2, \\dots, x_n$.\n\nTwo different Bayesian models are proposed to infer the value of $\\lambda$. The estimate for $\\lambda$ in each model is taken to be the mode of the posterior distribution, also known as the Maximum a Posteriori (MAP) estimate.\n\n- **Model A** uses a standard exponential prior distribution for the rate: $p_A(\\lambda) = \\exp(-\\lambda)$, for $\\lambda > 0$. Let the resulting posterior mode be denoted by $\\hat{\\lambda}_A$.\n- **Model B** uses a common non-informative improper prior, Jeffreys' prior for the Poisson rate: $p_B(\\lambda) \\propto \\frac{1}{\\sqrt{\\lambda}}$, for $\\lambda > 0$. Let this posterior mode be denoted by $\\hat{\\lambda}_B$.\n\nYour task is to calculate the difference between the two estimators, $\\hat{\\lambda}_A - \\hat{\\lambda}_B$. Express your answer as an analytic expression in terms of the number of intervals, $n$, and the sample mean, $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. given $\\lambda$ with $X_{i}\\mid\\lambda\\sim\\text{Poisson}(\\lambda)$. Denote $S=\\sum_{i=1}^{n}x_{i}=n\\bar{x}$. The likelihood is\n$$\nL(\\lambda\\mid x_{1:n})=\\prod_{i=1}^{n}\\frac{\\exp(-\\lambda)\\lambda^{x_{i}}}{x_{i}!}\\propto \\exp(-n\\lambda)\\lambda^{S}.\n$$\nIts log, up to an additive constant, is\n$$\n\\ell(\\lambda)=-n\\lambda+S\\ln\\lambda.\n$$\n\nModel A uses $p_{A}(\\lambda)=\\exp(-\\lambda)$ for $\\lambda>0$. The posterior (up to a multiplicative constant) is\n$$\np(\\lambda\\mid x_{1:n},A)\\propto \\exp(-n\\lambda)\\lambda^{S}\\cdot \\exp(-\\lambda)=\\exp(-(n+1)\\lambda)\\lambda^{S},\n$$\nwith log-posterior\n$$\n\\ell_{A}(\\lambda)=-(n+1)\\lambda+S\\ln\\lambda.\n$$\nDifferentiate and set to zero:\n$$\n\\frac{d\\ell_{A}}{d\\lambda}=-(n+1)+\\frac{S}{\\lambda}=0\\quad\\Longrightarrow\\quad \\hat{\\lambda}_{A}=\\frac{S}{n+1}.\n$$\nFor $S>0$, the second derivative $-\\frac{S}{\\lambda^{2}}<0$ confirms a mode in the interior; for $S=0$ the mode is at the boundary $\\lambda=0$.\n\nModel B uses $p_{B}(\\lambda)\\propto \\lambda^{-1/2}$ for $\\lambda>0$. The posterior (up to a multiplicative constant) is\n$$\np(\\lambda\\mid x_{1:n},B)\\propto \\exp(-n\\lambda)\\lambda^{S}\\cdot \\lambda^{-1/2}=\\exp(-n\\lambda)\\lambda^{S-\\frac{1}{2}},\n$$\nwith log-posterior\n$$\n\\ell_{B}(\\lambda)=-n\\lambda+\\left(S-\\frac{1}{2}\\right)\\ln\\lambda.\n$$\nDifferentiate and set to zero:\n$$\n\\frac{d\\ell_{B}}{d\\lambda}=-n+\\frac{S-\\frac{1}{2}}{\\lambda}=0\\quad\\Longrightarrow\\quad \\hat{\\lambda}_{B}=\\frac{S-\\frac{1}{2}}{n}.\n$$\nFor $S\\geq 1$ this is an interior mode; for $S=0$ the mode is at the boundary $\\lambda=0$.\n\nFor $S\\geq 1$ (the typical case with at least one observed click), the difference is\n$$\n\\hat{\\lambda}_{A}-\\hat{\\lambda}_{B}=\\frac{S}{n+1}-\\frac{S-\\frac{1}{2}}{n}.\n$$\nSubstitute $S=n\\bar{x}$ and simplify:\n$$\n\\hat{\\lambda}_{A}-\\hat{\\lambda}_{B}=\\frac{n\\bar{x}}{n+1}-\\left(\\bar{x}-\\frac{1}{2n}\\right)\n=-\\frac{\\bar{x}}{n+1}+\\frac{1}{2n}.\n$$\nThus, expressed in terms of $n$ and $\\bar{x}$,\n$$\n\\hat{\\lambda}_{A}-\\hat{\\lambda}_{B}=\\frac{1}{2n}-\\frac{\\bar{x}}{n+1}.\n$$\n(If $S=0$, both modes are at $0$ and the difference is $0$; the expression above applies when $S\\geq 1$.)", "answer": "$$\\boxed{\\frac{1}{2n}-\\frac{\\bar{x}}{n+1}}$$", "id": "1946616"}]}