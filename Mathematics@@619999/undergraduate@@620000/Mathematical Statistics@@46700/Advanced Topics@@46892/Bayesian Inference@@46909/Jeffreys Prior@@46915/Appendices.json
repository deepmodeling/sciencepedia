{"hands_on_practices": [{"introduction": "We begin our practical exploration by deriving Jeffreys prior for a parameter in a discrete probability distribution. The negative binomial model is fundamental for count data, such as the number of failures before a specified number of successes. By working through this example [@problem_id:1925869], you will master the core mechanics of calculating the Fisher information and deriving the corresponding prior for a success probability parameter $p$.", "problem": "A statistician is modeling a series of independent Bernoulli trials, each with a success probability $p \\in (0, 1)$. The experiment continues until a fixed, predetermined number of successes, denoted by the positive integer $r$, has been observed. Let the random variable $X$ represent the number of failures that occur before observing the $r$-th success. The probability mass function for $X$ is given by the negative binomial distribution:\n\n$$P(X=k | p, r) = \\binom{k+r-1}{k} p^r (1-p)^k, \\quad \\text{for } k = 0, 1, 2, \\dots$$\n\nTo conduct a Bayesian analysis on the parameter $p$, the statistician decides to use a non-informative prior. Specifically, they choose to use Jeffreys' prior.\n\nDetermine a function that is proportional to the Jeffreys' prior density, $\\pi_J(p)$, for the success probability $p$. The parameter $r$ is considered a known constant.", "solution": "We are given a negative binomial model for the number of failures before the fixed, predetermined number of successes $r$, with parameter $p \\in (0,1)$ and pmf\n$$\nP(X=k \\mid p,r) = \\binom{k+r-1}{k} p^{r} (1-p)^{k}, \\quad k=0,1,2,\\dots\n$$\nTreating $r$ as known, we derive Jeffreys' prior for $p$ using the Fisher information:\n$$\n\\pi_{J}(p) \\propto \\sqrt{I(p)}.\n$$\nFor a single observation $X$, the log-likelihood (up to an additive constant independent of $p$) is\n$$\n\\ell(p;X) = r \\ln p + X \\ln(1-p).\n$$\nIts first and second derivatives with respect to $p$ are\n$$\n\\frac{\\partial \\ell}{\\partial p} = \\frac{r}{p} - \\frac{X}{1-p}, \\qquad \\frac{\\partial^{2} \\ell}{\\partial p^{2}} = -\\frac{r}{p^{2}} - \\frac{X}{(1-p)^{2}}.\n$$\nThe Fisher information is given by\n$$\nI(p) = - \\mathbb{E}\\!\\left[ \\frac{\\partial^{2} \\ell}{\\partial p^{2}} \\right] = \\frac{r}{p^{2}} + \\frac{\\mathbb{E}[X]}{(1-p)^{2}}.\n$$\nFor the negative binomial distribution with this parameterization (failures before $r$-th success), the mean is\n$$\n\\mathbb{E}[X] = \\frac{r(1-p)}{p}.\n$$\nSubstituting into the expression for $I(p)$ yields\n$$\nI(p) = \\frac{r}{p^{2}} + \\frac{r(1-p)}{p(1-p)^{2}} = \\frac{r}{p^{2}} + \\frac{r}{p(1-p)} = \\frac{r(1-p) + rp}{p^2(1-p)} = \\frac{r}{p^{2}(1-p)}.\n$$\nTherefore, Jeffreys' prior is proportional to the square root of this Fisher information:\n$$\n\\pi_{J}(p) \\propto \\sqrt{I(p)} = \\sqrt{\\frac{r}{p^2(1-p)}} = \\frac{\\sqrt{r}}{p\\,(1-p)^{\\frac{1}{2}}} \\propto p^{-1} (1-p)^{-\\frac{1}{2}}, \\quad 0<p<1.\n$$\nSince Jeffreys' prior is defined up to a multiplicative constant, we can drop the factor $\\sqrt{r}$ (with $r$ known), giving the proportional form.", "answer": "$$\\boxed{p^{-1}\\,(1-p)^{-\\frac{1}{2}}}$$", "id": "1925869"}, {"introduction": "Moving from discrete to continuous distributions, we now consider a model with a different type of parameter. The Rayleigh distribution is often used to model phenomena in communications and physics, and its scale parameter $\\sigma$ dictates the spread of the distribution. This practice [@problem_id:1925852] is particularly insightful as it reveals a common and important form for non-informative priors on scale parameters, a recurring theme in Bayesian analysis.", "problem": "In the field of wireless communications, the amplitude of a received signal that has undergone multipath scattering is often modeled by a Rayleigh distribution. A key task for a receiver is to estimate the characteristics of the noise and signal environment. One approach is to use Bayesian methods, which require a prior distribution for the model's parameters.\n\nConsider a single measurement $x$ drawn from a Rayleigh distribution with a probability density function (PDF) given by:\n$$f(x; \\sigma) = \\frac{x}{\\sigma^2} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$$\nfor $x \\geq 0$. The parameter $\\sigma > 0$ is the scale parameter of the distribution.\n\nTo begin a Bayesian analysis without strong prior beliefs, one might use a non-informative prior. Jeffreys' prior is a popular choice, derived from the structure of the likelihood function. For a parameter $\\theta$, Jeffreys' prior $\\pi(\\theta)$ is defined to be proportional to the square root of the Fisher information, $I(\\theta)$:\n$$\\pi(\\theta) \\propto \\sqrt{I(\\theta)}$$\nThe Fisher information for a single observation is given by the formula:\n$$I(\\theta) = -E\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\ln f(x; \\theta)\\right]$$\nwhere $E[\\cdot]$ denotes the expectation with respect to the distribution $f(x; \\theta)$.\n\nDetermine Jeffreys' prior for the scale parameter $\\sigma$ of the Rayleigh distribution. Your answer should be the function of $\\sigma$ to which the prior $\\pi(\\sigma)$ is proportional.", "solution": "We are given a single observation from a Rayleigh distribution with density\n$$\nf(x;\\sigma)=\\frac{x}{\\sigma^{2}}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right),\\quad x\\geq 0,\\ \\sigma>0.\n$$\nJeffreys' prior for a parameter is proportional to the square root of the Fisher information. For the scale parameter $\\sigma$, we compute the Fisher information using\n$$\nI(\\sigma)=-E\\!\\left[\\frac{\\partial^{2}}{\\partial \\sigma^{2}}\\ln f(x;\\sigma)\\right].\n$$\nFirst, compute the log-likelihood for one observation:\n$$\n\\ln f(x;\\sigma)=\\ln x-2\\ln \\sigma-\\frac{x^{2}}{2\\sigma^{2}}.\n$$\nDifferentiate with respect to $\\sigma$:\n$$\n\\frac{\\partial}{\\partial \\sigma}\\ln f(x;\\sigma)=-\\frac{2}{\\sigma}+\\frac{x^{2}}{\\sigma^{3}}.\n$$\nDifferentiate again:\n$$\n\\frac{\\partial^{2}}{\\partial \\sigma^{2}}\\ln f(x;\\sigma)=\\frac{2}{\\sigma^{2}}-\\frac{3x^{2}}{\\sigma^{4}}.\n$$\nTherefore,\n$$\nI(\\sigma)=-E\\!\\left[\\frac{2}{\\sigma^{2}}-\\frac{3x^{2}}{\\sigma^{4}}\\right]=-\\frac{2}{\\sigma^{2}}+\\frac{3\\,E[x^{2}]}{\\sigma^{4}}.\n$$\nWe now compute $E[x^{2}]$ under the Rayleigh$(\\sigma)$ distribution:\n$$\nE[x^{2}]=\\int_{0}^{\\infty}x^{2}\\,\\frac{x}{\\sigma^{2}}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)\\mathrm{d}x=\\frac{1}{\\sigma^{2}}\\int_{0}^{\\infty}x^{3}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)\\mathrm{d}x.\n$$\nUse the substitution $u=\\frac{x^{2}}{2\\sigma^{2}}$ so that $\\mathrm{d}u=\\frac{x\\,\\mathrm{d}x}{\\sigma^{2}}$ and $x^{2}=2\\sigma^2 u$. Then $x^3 \\mathrm{d}x = x^2 \\cdot x \\mathrm{d}x = (2\\sigma^2 u)(\\sigma^2 \\mathrm{d}u) = 2\\sigma^4 u \\mathrm{d}u$. Then\n$$\nE[x^{2}]=\\int_{0}^{\\infty} 2\\sigma^2 u e^{-u} \\mathrm{d}u = 2\\sigma^2 \\Gamma(2) = 2\\sigma^2.\n$$\nSubstitute $E[x^{2}]=2\\sigma^{2}$ into the Fisher information:\n$$\nI(\\sigma)=-\\frac{2}{\\sigma^{2}}+\\frac{3\\cdot 2\\sigma^{2}}{\\sigma^{4}}=\\frac{4}{\\sigma^{2}}.\n$$\nJeffreys' prior is proportional to $\\sqrt{I(\\sigma)}$, hence\n$$\n\\pi(\\sigma)\\propto \\sqrt{I(\\sigma)}=\\sqrt{\\frac{4}{\\sigma^{2}}}=\\frac{2}{\\sigma}\\propto \\frac{1}{\\sigma},\\quad \\sigma>0.\n$$", "answer": "$$\\boxed{\\frac{1}{\\sigma}}$$", "id": "1925852"}, {"introduction": "Our final practice expands our skillset to handle models with multiple parameters, a common scenario in real-world statistical modeling. The Gamma distribution is characterized by both a shape parameter $\\alpha$ and a rate parameter $\\beta$, requiring a multivariate approach. This exercise [@problem_id:1925872] demonstrates how to construct the Fisher Information Matrix and use its determinant to derive the joint Jeffreys prior, showcasing the method's power and generality.", "problem": "In Bayesian statistics, an uninformative prior can be constructed using a method proposed by Harold Jeffreys. For a model with a parameter vector $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\dots, \\theta_k)$ and likelihood function $L(\\boldsymbol{\\theta}|x)$ based on data $x$, the multivariate Jeffreys' prior is defined as being proportional to the square root of the determinant of the Fisher Information Matrix, $\\mathcal{I}(\\boldsymbol{\\theta})$. That is,\n$$ \\pi_J(\\boldsymbol{\\theta}) \\propto \\sqrt{\\det(\\mathcal{I}(\\boldsymbol{\\theta}))} $$\nThe $(i,j)$-th element of the Fisher Information Matrix is given by\n$$ \\mathcal{I}_{ij}(\\boldsymbol{\\theta}) = -E\\left[\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\ln L(\\boldsymbol{\\theta}|X)\\right] $$\nwhere the expectation is taken with respect to the random variable $X$.\n\nConsider a random variable $X$ that follows a Gamma distribution, which is frequently used to model waiting times or rainfall amounts. Its Probability Density Function (PDF) is given by:\n$$ f(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x) \\quad \\text{for } x > 0 $$\nwhere $\\alpha > 0$ is the shape parameter, $\\beta > 0$ is the rate parameter, and $\\Gamma(\\alpha)$ is the Gamma function.\n\nFor a single observation $x$ from this distribution, determine the bivariate Jeffreys' prior, $\\pi_J(\\alpha, \\beta)$. Your task is to find the function of $\\alpha$ and $\\beta$ to which this prior is proportional.\n\nThe following definitions may be useful:\n- The digamma function: $\\psi(\\alpha) = \\frac{d}{d\\alpha} \\ln \\Gamma(\\alpha)$\n- The trigamma function: $\\psi_1(\\alpha) = \\frac{d}{d\\alpha} \\psi(\\alpha) = \\frac{d^2}{d\\alpha^2} \\ln \\Gamma(\\alpha)$\n\nProvide the final expression in terms of $\\alpha$, $\\beta$, and the trigamma function $\\psi_1(\\alpha)$.", "solution": "The problem asks for the expression to which the bivariate Jeffreys' prior for the parameters $(\\alpha, \\beta)$ of a Gamma distribution is proportional. According to the definition provided, $\\pi_J(\\alpha, \\beta) \\propto \\sqrt{\\det(\\mathcal{I}(\\alpha, \\beta))}$. We must first compute the Fisher Information Matrix, $\\mathcal{I}(\\alpha, \\beta)$.\n\n**Step 1: Find the log-likelihood function.**\nThe likelihood function for a single observation $x$ from a Gamma$(\\alpha, \\beta)$ distribution is the PDF itself:\n$$ L(\\alpha, \\beta|x) = f(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x) $$\nThe log-likelihood, denoted $\\ell(\\alpha, \\beta|x)$, is:\n$$ \\ell(\\alpha, \\beta|x) = \\ln L(\\alpha, \\beta|x) = \\alpha \\ln \\beta - \\ln \\Gamma(\\alpha) + (\\alpha-1) \\ln x - \\beta x $$\n\n**Step 2: Calculate the second-order partial derivatives of the log-likelihood.**\nWe first compute the first-order partial derivatives with respect to $\\alpha$ and $\\beta$.\n$$ \\frac{\\partial \\ell}{\\partial \\alpha} = \\ln \\beta - \\frac{d}{d\\alpha}\\ln \\Gamma(\\alpha) + \\ln x = \\ln \\beta - \\psi(\\alpha) + \\ln x $$\n$$ \\frac{\\partial \\ell}{\\partial \\beta} = \\frac{\\alpha}{\\beta} - x $$\nNow, we compute the three unique second-order partial derivatives:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = \\frac{\\partial}{\\partial \\alpha} (\\ln \\beta - \\psi(\\alpha) + \\ln x) = -\\frac{d}{d\\alpha}\\psi(\\alpha) = -\\psi_1(\\alpha) $$\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta^2} = \\frac{\\partial}{\\partial \\beta} \\left(\\frac{\\alpha}{\\beta} - x\\right) = -\\frac{\\alpha}{\\beta^2} $$\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\alpha} = \\frac{\\partial}{\\partial \\beta} \\left(\\frac{\\partial \\ell}{\\partial \\alpha}\\right) = \\frac{\\partial}{\\partial \\beta} (\\ln \\beta - \\psi(\\alpha) + \\ln x) = \\frac{1}{\\beta} $$\nBy Clairaut's theorem, $\\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\beta} = \\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\alpha}$, so we have all the necessary components.\n\n**Step 3: Compute the elements of the Fisher Information Matrix.**\nThe elements of the Fisher Information Matrix, $\\mathcal{I}(\\alpha, \\beta)$, are given by $\\mathcal{I}_{ij} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta_i \\partial \\theta_j}\\right]$. In our case, the parameter vector is $\\boldsymbol{\\theta} = (\\alpha, \\beta)$.\n\n$$ \\mathcal{I}_{11} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\alpha^2}\\right] = -E[-\\psi_1(\\alpha)] $$\nSince $\\psi_1(\\alpha)$ is a function of $\\alpha$ only and does not depend on the random variable $X$, its expectation is itself.\n$$ \\mathcal{I}_{11} = \\psi_1(\\alpha) $$\nSimilarly for $\\mathcal{I}_{22}$:\n$$ \\mathcal{I}_{22} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\beta^2}\\right] = -E\\left[-\\frac{\\alpha}{\\beta^2}\\right] = \\frac{\\alpha}{\\beta^2} $$\nAnd for the off-diagonal elements $\\mathcal{I}_{12} = \\mathcal{I}_{21}$:\n$$ \\mathcal{I}_{12} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\alpha}\\right] = -E\\left[\\frac{1}{\\beta}\\right] = -\\frac{1}{\\beta} $$\n\n**Step 4: Construct the Fisher Information Matrix and compute its determinant.**\nThe Fisher Information Matrix is:\n$$ \\mathcal{I}(\\alpha, \\beta) = \\begin{pmatrix} \\psi_1(\\alpha) & -1/\\beta \\\\ -1/\\beta & \\alpha/\\beta^2 \\end{pmatrix} $$\nThe determinant of this matrix is:\n$$ \\det(\\mathcal{I}(\\alpha, \\beta)) = (\\psi_1(\\alpha))\\left(\\frac{\\alpha}{\\beta^2}\\right) - \\left(-\\frac{1}{\\beta}\\right)\\left(-\\frac{1}{\\beta}\\right) $$\n$$ \\det(\\mathcal{I}(\\alpha, \\beta)) = \\frac{\\alpha \\psi_1(\\alpha)}{\\beta^2} - \\frac{1}{\\beta^2} = \\frac{\\alpha \\psi_1(\\alpha) - 1}{\\beta^2} $$\n\n**Step 5: Determine the Jeffreys' prior.**\nThe Jeffreys' prior is proportional to the square root of the determinant of the Fisher Information Matrix.\n$$ \\pi_J(\\alpha, \\beta) \\propto \\sqrt{\\det(\\mathcal{I}(\\alpha, \\beta))} = \\sqrt{\\frac{\\alpha \\psi_1(\\alpha) - 1}{\\beta^2}} $$\nSince $\\beta > 0$, we can simplify this expression:\n$$ \\pi_J(\\alpha, \\beta) \\propto \\frac{\\sqrt{\\alpha \\psi_1(\\alpha) - 1}}{\\beta} $$\nThis is the final expression to which the bivariate Jeffreys' prior is proportional. (Note: For $\\alpha > 0$, a known property of the trigamma function is $\\alpha \\psi_1(\\alpha) > 1$, so the term under the square root is always positive).", "answer": "$$\\boxed{\\frac{\\sqrt{\\alpha \\psi_1(\\alpha) - 1}}{\\beta}}$$", "id": "1925872"}]}