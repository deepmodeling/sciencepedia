{"hands_on_practices": [{"introduction": "A cornerstone of Bayesian statistics is the prior distribution, which mathematically encodes our knowledge before observing data. This exercise provides hands-on practice in one of the most practical skills for a Bayesian statistician: prior elicitation. By translating an expert's subjective assessment of a proportion's likely range into the parameters of a Beta distribution, you will learn how to build a formal prior from qualitative information [@problem_id:1898866].", "problem": "A team of astrophysicists is trying to estimate the proportion, $\\theta$, of a certain class of exoplanets that could host atmospheric biosignatures. To establish a prior distribution for this unknown proportion, they consult a senior expert in planetary science. The expert states that their subjective belief is that the median value for $\\theta$ is 0.5, and that there is a 50% probability that the true proportion $\\theta$ lies somewhere between 0.42 and 0.58.\n\nThe team decides to model this prior belief using a Beta distribution, Beta($\\alpha, \\beta$), which has a probability density function proportional to $x^{\\alpha-1}(1-x)^{\\beta-1}$ for $x \\in [0, 1]$ and parameters $\\alpha, \\beta > 0$. For this task, you may assume that for Beta distributions that are not heavily skewed and have sufficiently large parameters, the central quantiles can be approximated using a normal distribution. Specifically, the 25th and 75th percentiles can be approximated by $\\mu - z^* \\sigma$ and $\\mu + z^* \\sigma$, respectively, where $\\mu$ is the mean, $\\sigma$ is the standard deviation, and the constant $z^*$ is approximately $0.6745$.\n\nAssuming the expert's subjective belief is adequately described by this model and approximation, determine the parameters $\\alpha$ and $\\beta$ of the Beta distribution. Report your values for $\\alpha$ and $\\beta$, in that order, rounded to three significant figures.", "solution": "We model the prior as $\\theta \\sim \\text{Beta}(\\alpha,\\beta)$. For a Beta distribution, the mean and variance are\n$$\\mu=\\frac{\\alpha}{\\alpha+\\beta}, \\qquad \\sigma^{2}=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^{2}(\\alpha+\\beta+1)}.$$\n\nThe expert states the median is $0.5$ and that the central $50$ percent of the distribution lies between $0.42$ and $0.58$. Under the given normal approximation for central quantiles, the first and third quartiles satisfy\n$$Q_{1}\\approx \\mu - z^{*}\\sigma, \\qquad Q_{3}\\approx \\mu + z^{*}\\sigma,$$\nwith $z^{*}\\approx 0.6745$. The interval $[0.42,\\,0.58]$ represents the interquartile range, so\n$$Q_{1}=0.42, \\qquad Q_{3}=0.58.$$\n\nAdding these and dividing by $2$ gives the mean:\n$$\\mu \\approx \\frac{Q_{1}+Q_{3}}{2}=\\frac{0.42+0.58}{2}=0.5.$$\nThus\n$$\\frac{\\alpha}{\\alpha+\\beta}=0.5 \\quad \\Longrightarrow \\quad \\alpha=\\beta.$$\nLet $\\alpha=\\beta=a$. Then the variance simplifies to\n$$\\sigma^{2}=\\frac{a^{2}}{(2a)^{2}(2a+1)}=\\frac{1}{4(2a+1)}, \\qquad \\sigma=\\frac{1}{2\\sqrt{2a+1}}.$$\n\nFrom $Q_{3}-\\mu\\approx z^{*}\\sigma$ we have\n$$0.58-0.50 = z^{*}\\sigma \\quad \\Longrightarrow \\quad \\sigma=\\frac{0.08}{z^{*}}.$$\nEquating this to the expression for $\\sigma$ in terms of $a$,\n$$\\frac{1}{2\\sqrt{2a+1}}=\\frac{0.08}{z^{*}} \\quad \\Longrightarrow \\quad \\sqrt{2a+1}=\\frac{z^{*}}{0.16} \\quad \\Longrightarrow \\quad 2a+1=\\left(\\frac{z^{*}}{0.16}\\right)^{2}.$$\nHence\n$$a=\\frac{1}{2}\\left[\\left(\\frac{z^{*}}{0.16}\\right)^{2}-1\\right].$$\n\nWith $z^{*}=0.6745$,\n$$\\frac{z^{*}}{0.16}=\\frac{0.6745}{0.16}=4.215625,\\quad \\left(\\frac{z^{*}}{0.16}\\right)^{2}=17.771494140625,$$\nso\n$$a=\\frac{17.771494140625-1}{2}=8.3857470703125.$$\n\nTherefore, $\\alpha=\\beta\\approx 8.3857470703125$, which rounded to three significant figures gives\n$$\\alpha\\approx 8.39,\\qquad \\beta\\approx 8.39.$$", "answer": "$$\\boxed{\\begin{pmatrix}8.39 & 8.39\\end{pmatrix}}$$", "id": "1898866"}, {"introduction": "Once a prior distribution is established, we use Bayes' theorem to update our beliefs in light of new evidence. This exercise demonstrates this fundamental process using a conjugate prior pair, a scenario where the posterior distribution conveniently belongs to the same family as the prior. You will work with a Beta prior for the success probability $p$ in a process modeled by a Geometric distribution, seeing firsthand how data systematically shifts our understanding of the parameter [@problem_id:1898877].", "problem": "A data scientist at an educational technology company is analyzing player performance on a new puzzle. The number of attempts, $K$, a player needs to achieve their first success is modeled by a Geometric distribution with probability mass function $P(K=k|p) = p(1-p)^{k-1}$ for $k=1, 2, 3, \\dots$. Here, $p$ is the unknown probability of solving the puzzle on any single attempt, which is assumed to be constant for a given player.\n\nThe scientist uses a Bayesian approach to update her beliefs about the parameter $p$. Her prior belief about $p$ is described by a Beta distribution with parameters $\\alpha_0 = 4$ and $\\beta_0 = 6$.\n\nA player from the target audience is recruited for testing and is observed to take $k_{obs} = 8$ attempts to solve the puzzle for the first time. Given this observation, calculate the scientist's updated estimate for the probability $p$. Specifically, compute the posterior mean of $p$.\n\nRound your final answer to four significant figures.", "solution": "We model the number of attempts until first success by the geometric likelihood\n$$\nP(K=k \\mid p)=p(1-p)^{k-1}, \\quad k=1,2,3,\\dots .\n$$\nThe prior for $p$ is $\\operatorname{Beta}(\\alpha_{0},\\beta_{0})$ with density proportional to\n$$\n\\pi(p)\\propto p^{\\alpha_{0}-1}(1-p)^{\\beta_{0}-1}.\n$$\nGiven a single observation $k_{\\text{obs}}$, the posterior by Bayesâ€™ rule is proportional to the product of likelihood and prior:\n$$\n\\pi(p \\mid k_{\\text{obs}})\\propto \\left[p(1-p)^{k_{\\text{obs}}-1}\\right]\\cdot p^{\\alpha_{0}-1}(1-p)^{\\beta_{0}-1}\n= p^{\\alpha_{0}}(1-p)^{\\beta_{0}+k_{\\text{obs}}-2}.\n$$\nRecognizing the Beta kernel, the posterior is\n$$\np \\mid k_{\\text{obs}} \\sim \\operatorname{Beta}\\big(\\alpha_{0}+1, \\beta_{0}+k_{\\text{obs}}-1\\big).\n$$\nFor a $\\operatorname{Beta}(\\alpha,\\beta)$ distribution, the posterior mean is\n$$\n\\mathbb{E}[p \\mid k_{\\text{obs}}]=\\frac{\\alpha}{\\alpha+\\beta}.\n$$\nSubstituting $\\alpha_{0}=4$, $\\beta_{0}=6$, and $k_{\\text{obs}}=8$, we obtain\n$$\n\\alpha=\\alpha_{0}+1=5,\\qquad \\beta=\\beta_{0}+k_{\\text{obs}}-1=13,\n$$\nso\n$$\n\\mathbb{E}[p \\mid k_{\\text{obs}}]=\\frac{5}{5+13}=\\frac{5}{18}.\n$$\nNumerically, $\\frac{5}{18}=0.277777\\ldots$, which rounded to four significant figures is $0.2778$.", "answer": "$$\\boxed{0.2778}$$", "id": "1898877"}, {"introduction": "The elegance of conjugate priors is not always available in applied analysis, where model choice is driven by the problem, not mathematical convenience. This exercise takes you beyond a textbook scenario by pairing an Exponential likelihood with a non-conjugate Log-Normal prior. Your goal is to derive the mathematical form of the resulting posterior density, a crucial first step that highlights why numerical approximation methods are essential tools in the modern Bayesian toolkit [@problem_id:1898879].", "problem": "An engineer is modeling the lifetime of a specific electronic component. The lifetime, $X$, is assumed to follow an exponential distribution with an unknown failure rate parameter $\\lambda > 0$. The Probability Density Function (PDF) for a single observation $x$ is given by $p(x|\\lambda) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$.\n\nThe engineer's prior belief about the parameter $\\lambda$ is captured by a log-normal distribution with parameters $\\mu \\in \\mathbb{R}$ and $\\sigma^2 > 0$. The PDF for this prior belief is given by:\n$$p(\\lambda) = \\frac{1}{\\lambda \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln \\lambda - \\mu)^2}{2\\sigma^2}\\right) \\quad \\text{for } \\lambda > 0$$\n\nAfter conducting an experiment, the engineer observes $n$ independent lifetimes of the component, denoted by the data set $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$. For convenience, let $S = \\sum_{i=1}^{n} x_i$ represent the sum of these observed lifetimes.\n\nYour task is to determine the mathematical form of the posterior PDF for $\\lambda$, which is denoted by $p(\\lambda|\\mathbf{x})$. Your final expression should be proportional to the true posterior density, meaning you may omit any normalization constants that do not depend on $\\lambda$. Express your answer in terms of $\\lambda$, $n$, $S$, $\\mu$, and $\\sigma$.", "solution": "We are given an exponential likelihood for a single observation $x$ conditional on $\\lambda>0$:\n$$\np(x \\mid \\lambda) = \\lambda \\exp(-\\lambda x), \\quad x \\ge 0.\n$$\nFor $n$ independent observations $\\mathbf{x}=(x_{1},\\ldots,x_{n})$ with $S=\\sum_{i=1}^{n} x_{i}$, the likelihood is the product of the individual densities:\n$$\nL(\\lambda \\mid \\mathbf{x}) = \\prod_{i=1}^{n} \\left[\\lambda \\exp(-\\lambda x_{i})\\right]\n= \\lambda^{n} \\exp\\!\\left(-\\lambda \\sum_{i=1}^{n} x_{i}\\right)\n= \\lambda^{n} \\exp(-\\lambda S).\n$$\n\nThe prior for $\\lambda$ is log-normal with parameters $\\mu \\in \\mathbb{R}$ and $\\sigma^{2}>0$:\n$$\np(\\lambda) = \\frac{1}{\\lambda \\sigma \\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(\\ln \\lambda - \\mu)^{2}}{2\\sigma^{2}}\\right), \\quad \\lambda>0.\n$$\n\nBy Bayes' rule, the posterior density is proportional to the product of the likelihood and the prior:\n$$\np(\\lambda \\mid \\mathbf{x}) \\propto L(\\lambda \\mid \\mathbf{x}) \\, p(\\lambda).\n$$\nSubstituting the expressions for $L(\\lambda \\mid \\mathbf{x})$ and $p(\\lambda)$, we obtain\n$$\np(\\lambda \\mid \\mathbf{x}) \\propto \\left[\\lambda^{n} \\exp(-\\lambda S)\\right]\n\\left[\\frac{1}{\\lambda \\sigma \\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(\\ln \\lambda - \\mu)^{2}}{2\\sigma^{2}}\\right)\\right].\n$$\nDrop the multiplicative factor $\\frac{1}{\\sigma \\sqrt{2\\pi}}$ that does not depend on $\\lambda$, and combine powers of $\\lambda$:\n$$\np(\\lambda \\mid \\mathbf{x}) \\propto \\lambda^{n-1} \\exp(-\\lambda S) \\exp\\!\\left(-\\frac{(\\ln \\lambda - \\mu)^{2}}{2\\sigma^{2}}\\right), \\quad \\lambda>0.\n$$\nThis is the posterior density up to a normalization constant, expressed in terms of $\\lambda$, $n$, $S$, $\\mu$, and $\\sigma$. It does not correspond to a standard named family but is fully specified up to proportionality.", "answer": "$$\\boxed{\\lambda^{n-1}\\exp(-S\\lambda)\\exp\\!\\left(-\\frac{(\\ln \\lambda - \\mu)^{2}}{2\\sigma^{2}}\\right)}$$", "id": "1898879"}]}