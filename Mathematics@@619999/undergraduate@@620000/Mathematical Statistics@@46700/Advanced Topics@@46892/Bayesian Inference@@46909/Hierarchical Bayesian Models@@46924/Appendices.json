{"hands_on_practices": [{"introduction": "We begin with a foundational exercise that revisits the core of Bayesian inference. This problem demonstrates how to update our belief about a single parameter—in this hypothetical scenario, a manufacturing defect rate—by combining prior knowledge with observed data. Mastering this Beta-Binomial conjugate model [@problem_id:1920799] is a crucial first step before layering on the additional complexity of hierarchical structures.", "problem": "A technology company, \"Quantum Circuits Inc.\", manufactures highly sensitive quantum processors. The quality control process involves multiple identical fabrication lines. Based on extensive historical data from all lines, the company has established a prior belief about the defect rate, $p$, of any given fabrication line. This belief is modeled by a Beta distribution with parameters $\\alpha = 3$ and $\\beta = 147$.\n\nA new fabrication line, \"Line Gamma,\" is brought online. To assess its performance, a test batch of $n = 850$ processors is produced and inspected. In this batch, $k = 22$ defective processors are found.\n\nAssuming the number of defective processors in a batch follows a Binomial distribution, calculate the updated Bayesian point estimate for the defect rate of Line Gamma, $p_{\\gamma}$. Use the mean of the posterior distribution as your point estimate.\n\nProvide your answer as a single numerical value, rounded to three significant figures.", "solution": "We model the defect rate $p$ with a Beta prior and a Binomial likelihood. The prior is $p \\sim \\mathrm{Beta}(\\alpha,\\beta)$ with $\\alpha=3$ and $\\beta=147$. The data consist of $k=22$ defectives out of $n=850$, modeled as $k \\mid p,n \\sim \\mathrm{Binomial}(n,p)$.\n\nBy Beta-Binomial conjugacy, the posterior for $p$ is\n$$\np \\mid k,n \\sim \\mathrm{Beta}(\\alpha+k,\\ \\beta+n-k).\n$$\nSubstituting the given values,\n$$\n\\alpha_{\\text{post}}=\\alpha+k=3+22=25,\\qquad \\beta_{\\text{post}}=\\beta+n-k=147+850-22=975.\n$$\nThe posterior mean, used as the Bayesian point estimate, is\n$$\n\\mathbb{E}[p \\mid k,n]=\\frac{\\alpha_{\\text{post}}}{\\alpha_{\\text{post}}+\\beta_{\\text{post}}}\n=\\frac{\\alpha+k}{\\alpha+\\beta+n}\n=\\frac{3+22}{3+147+850}\n=\\frac{25}{1000}\n=0.025.\n$$\nRounded to three significant figures, this is $0.0250$.", "answer": "$$\\boxed{0.0250}$$", "id": "1920799"}, {"introduction": "This practice gets to the heart of why hierarchical models are so powerful: the concept of 'borrowing strength' across related groups. We will explore a common scenario in clinical trials where we estimate a true, unobservable effect for a new patient. By developing an empirical Bayes estimator [@problem_id:1920805], you will see how an individual's estimate is systematically 'shrunk' toward a group average, leading to more stable and realistic predictions, especially with limited data.", "problem": "In a clinical trial for a new cancer therapy, researchers model the treatment response for each patient. The response is characterized by a patient-specific true log-growth rate, denoted by $k_i$ for patient $i$, where a negative value indicates tumor shrinkage. The true rate $k_i$ is not directly observable. Instead, a noisy log-growth rate, $d_i$, is measured for each patient.\n\nThe researchers adopt a hierarchical Bayesian model to combine information across patients. The model is structured as follows:\n\n1.  **Observation Model**: The measured log-growth rate $d_i$ for patient $i$ is assumed to be normally distributed around the true rate $k_i$ with a known, constant variance $s^2$, representing measurement error.\n    $$d_i | k_i, s^2 \\sim \\mathcal{N}(k_i, s^2)$$\n\n2.  **Population Model**: The patient-specific true rates $k_i$ are assumed to be drawn from a common population distribution, which is normal with an unknown mean $\\mu$ and an unknown variance $\\tau^2$. This distribution represents the overall efficacy and patient-to-patient variability of the therapy.\n    $$k_i | \\mu, \\tau^2 \\sim \\mathcal{N}(\\mu, \\tau^2)$$\n\nAn initial study provides data from $N$ patients, yielding measured log-growth rates $\\{d_1, d_2, \\ldots, d_N\\}$. The population hyperparameters $\\mu$ and $\\tau^2$ are estimated from this data using the method of moments, yielding \"plug-in\" estimates $\\hat{\\mu}$ and $\\hat{\\tau}^2$. These estimates are computed as:\n-   $\\hat{\\mu} = \\bar{d}$, where $\\bar{d} = \\frac{1}{N} \\sum_{i=1}^{N} d_i$.\n-   $\\hat{\\tau}^2 = S_d^2 - s^2$, where $S_d^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (d_i - \\bar{d})^2$ is the sample variance of the measurements. You may assume that the study data is such that $S_d^2 > s^2$, ensuring $\\hat{\\tau}^2 > 0$.\n\nNow, a new patient, indexed by $N+1$, enters the trial, and their measured log-growth rate is observed to be $d_{N+1}$. Using the previously estimated hyperparameters, we want to find the best estimate for this new patient's *true* log-growth rate, $k_{N+1}$.\n\nYour task is to derive an expression for the empirical Bayes estimate of $k_{N+1}$, which is defined as its posterior mean, $E[k_{N+1} | d_{N+1}, \\hat{\\mu}, \\hat{\\tau}^2]$. Express your final answer in terms of $\\bar{d}$, $S_d^2$, $s^2$, and $d_{N+1}$.", "solution": "We start with the hierarchical model: for a new patient,\n$$d_{N+1} \\mid k_{N+1}, s^{2} \\sim \\mathcal{N}(k_{N+1}, s^{2}), \\quad k_{N+1} \\mid \\mu, \\tau^{2} \\sim \\mathcal{N}(\\mu, \\tau^{2}).$$\nUsing Bayes’ rule with the conjugate normal-normal pair, the posterior of $k_{N+1}$ given $\\mu$, $\\tau^{2}$, $s^{2}$, and $d_{N+1}$ is normal with precision equal to the sum of prior and likelihood precisions. Completing the square in the exponent,\n$$\\ln p(k_{N+1} \\mid d_{N+1}, \\mu, \\tau^{2}, s^{2})=\\text{const}-\\frac{1}{2s^{2}}(d_{N+1}-k_{N+1})^{2}-\\frac{1}{2\\tau^{2}}(k_{N+1}-\\mu)^{2},$$\nwhich is quadratic in $k_{N+1}$ with coefficient of $k_{N+1}^{2}$ equal to $\\frac{1}{s^{2}}+\\frac{1}{\\tau^{2}}$ and linear term coefficient equal to $\\frac{d_{N+1}}{s^{2}}+\\frac{\\mu}{\\tau^{2}}$. Therefore, the posterior mean is\n$$E[k_{N+1} \\mid d_{N+1}, \\mu, \\tau^{2}]=\\frac{\\frac{d_{N+1}}{s^{2}}+\\frac{\\mu}{\\tau^{2}}}{\\frac{1}{s^{2}}+\\frac{1}{\\tau^{2}}}=\\frac{\\tau^{2} d_{N+1}+s^{2}\\mu}{\\tau^{2}+s^{2}}.$$\nIn the empirical Bayes approach we plug in the method-of-moments estimates $\\hat{\\mu}=\\bar{d}$ and $\\hat{\\tau}^{2}=S_{d}^{2}-s^{2}$, yielding\n$$E[k_{N+1} \\mid d_{N+1}, \\hat{\\mu}, \\hat{\\tau}^{2}]=\\frac{(S_{d}^{2}-s^{2})\\, d_{N+1}+s^{2}\\,\\bar{d}}{(S_{d}^{2}-s^{2})+s^{2}}=\\frac{(S_{d}^{2}-s^{2})\\, d_{N+1}+s^{2}\\,\\bar{d}}{S_{d}^{2}}.$$\nEquivalently, this can be written as a shrinkage estimator\n$$\\left(1-\\frac{s^{2}}{S_{d}^{2}}\\right)d_{N+1}+\\frac{s^{2}}{S_{d}^{2}}\\bar{d},$$\nwhich makes explicit the data-driven weight $1-\\frac{s^{2}}{S_{d}^{2}}$ on $d_{N+1}$ toward the overall mean $\\bar{d}$.", "answer": "$$\\boxed{\\frac{(S_{d}^{2}-s^{2})\\, d_{N+1}+s^{2}\\,\\bar{d}}{S_{d}^{2}}}$$", "id": "1920805"}, {"introduction": "Building on the previous concepts, this advanced problem tackles a more complex, realistic marketing analytics scenario. Here, you will work with multivariate data and compare two competing hypotheses about market structure using Bayesian model selection. This exercise [@problem_id:1920764] challenges you to derive the marginal likelihood for a Dirichlet-Multinomial model, a key technique for evaluating how well a hierarchical model with specific hyperparameter settings explains the observed data.", "problem": "A marketing analytics firm is studying consumer purchasing behavior for three competing product brands (Brand A, Brand B, Brand C) in two distinct major cities (City 1 and City 2). The firm employs a hierarchical Bayesian model to analyze the data, allowing for both city-specific preferences and shared market characteristics.\n\nThe model is specified as follows:\n- For each city $j \\in \\{1, 2\\}$, a survey of $N_j$ consumers is conducted, and the number of consumers preferring each brand is recorded as a vector $\\mathbf{x}_j = (x_{jA}, x_{jB}, x_{jC})$, where $N_j = x_{jA} + x_{jB} + x_{jC}$. The data generating process for the counts in each city is assumed to be a Multinomial distribution conditional on a city-specific market share vector $\\boldsymbol{\\theta}_j = (\\theta_{jA}, \\theta_{jB}, \\theta_{jC})$.\n\n- The market share vectors $\\boldsymbol{\\theta}_j$ for each city are themselves assumed to be drawn from a common Dirichlet prior distribution, $\\boldsymbol{\\theta}_j \\sim \\text{Dirichlet}(\\alpha \\mathbf{p})$. The base measure $\\mathbf{p} = (p_A, p_B, p_C)$ is fixed to represent a uniform prior belief on nationwide market shares, so $\\mathbf{p} = (1/3, 1/3, 1/3)$.\n\n- The scalar concentration parameter $\\alpha > 0$ determines the degree of similarity between cities. A large $\\alpha$ suggests that market shares in both cities are very close to the nationwide average $\\mathbf{p}$, indicating a \"homogeneous\" market. A small $\\alpha$ allows for greater variation between cities, indicating a \"heterogeneous\" market.\n\nThe firm is considering two hypotheses for the market structure, distinguished by the value of $\\alpha$:\n1.  Homogeneous Market ($H_{hom}$): The concentration parameter is $\\alpha = \\alpha_{hom}$.\n2.  Heterogeneous Market ($H_{het}$): The concentration parameter is $\\alpha = \\alpha_{het}$.\n\nThe firm's initial belief is that the market is homogeneous with probability $\\pi_0$, so $P(H_{hom}) = \\pi_0$.\n\nGiven the observed consumer counts $\\mathbf{x}_1 = (x_{1A}, x_{1B}, x_{1C})$ and $\\mathbf{x}_2 = (x_{2A}, x_{2B}, x_{2C})$ from the two cities, derive a closed-form analytic expression for the posterior probability of the homogeneous market hypothesis, $P(H_{hom} | \\mathbf{x}_1, \\mathbf{x}_2)$. Your final expression should be in terms of $\\pi_0, \\alpha_{hom}, \\alpha_{het}, \\mathbf{x}_1, \\mathbf{x}_2, N_1,$ and $N_2$.", "solution": "We are asked to compute the posterior probability of the homogeneous market hypothesis under a hierarchical Bayesian model with two competing hypotheses that differ only in the fixed value of the Dirichlet concentration parameter. The data consist of city-wise brand counts $\\mathbf{x}_{j}=(x_{jA},x_{jB},x_{jC})$ for $j \\in \\{1,2\\}$, with $N_{j}=\\sum_{c \\in \\{A,B,C\\}} x_{jc}$. The likelihood for each city is multinomial conditional on the city-specific market share vector $\\boldsymbol{\\theta}_{j}$, and the prior for $\\boldsymbol{\\theta}_{j}$ is Dirichlet with parameters $\\alpha \\mathbf{p}$, where $\\mathbf{p}=(1/3,1/3,1/3)$ is fixed.\n\nWe first compute the marginal likelihood of the data given a fixed $\\alpha$. For each city $j$, the conditional model is\n$$\n\\mathbf{x}_{j} \\mid \\boldsymbol{\\theta}_{j} \\sim \\text{Multinomial}\\left(N_{j},\\boldsymbol{\\theta}_{j}\\right), \\quad \\boldsymbol{\\theta}_{j} \\sim \\text{Dirichlet}\\left(\\alpha \\mathbf{p}\\right).\n$$\nIntegrating out $\\boldsymbol{\\theta}_{j}$ yields the Dirichlet-multinomial marginal likelihood:\n$$\nm(\\mathbf{x}_{j} \\mid \\alpha)=\\int \\left[\\frac{N_{j}!}{\\prod_{c \\in \\{A,B,C\\}} x_{jc}!} \\prod_{c \\in \\{A,B,C\\}} \\theta_{jc}^{x_{jc}} \\right] \\cdot \\left[\\frac{1}{B(\\alpha \\mathbf{p})} \\prod_{c \\in \\{A,B,C\\}} \\theta_{jc}^{\\alpha p_{c}-1} \\right] d\\boldsymbol{\\theta}_{j}.\n$$\nUsing the identity for the multivariate Beta function $B(\\mathbf{a})=\\frac{\\prod_{c} \\Gamma(a_{c})}{\\Gamma(\\sum_{c} a_{c})}$ and the conjugacy of the Dirichlet prior to the multinomial, we have\n$$\nm(\\mathbf{x}_{j} \\mid \\alpha)=\\frac{N_{j}!}{\\prod_{c \\in \\{A,B,C\\}} x_{jc}!} \\cdot \\frac{B(\\alpha \\mathbf{p}+\\mathbf{x}_{j})}{B(\\alpha \\mathbf{p})}.\n$$\nWith $\\mathbf{p}=(1/3,1/3,1/3)$, we have $\\alpha p_{c}=\\alpha/3$ for each $c \\in \\{A,B,C\\}$ and $\\sum_{c} \\alpha p_{c}=\\alpha$. Therefore,\n$$\n\\frac{B(\\alpha \\mathbf{p}+\\mathbf{x}_{j})}{B(\\alpha \\mathbf{p})}\n=\\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha+N_{j})}\\prod_{c \\in \\{A,B,C\\}} \\frac{\\Gamma\\!\\left(\\frac{\\alpha}{3}+x_{jc}\\right)}{\\Gamma\\!\\left(\\frac{\\alpha}{3}\\right)}.\n$$\nThus the marginal likelihood for city $j$ is\n$$\nm(\\mathbf{x}_{j} \\mid \\alpha)=\\frac{N_{j}!}{\\prod_{c \\in \\{A,B,C\\}} x_{jc}!} \\cdot \\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha+N_{j})}\\prod_{c \\in \\{A,B,C\\}} \\frac{\\Gamma\\!\\left(\\frac{\\alpha}{3}+x_{jc}\\right)}{\\Gamma\\!\\left(\\frac{\\alpha}{3}\\right)}.\n$$\nAssuming independence across cities conditional on $\\alpha$, the marginal likelihood of all data is the product over cities:\n$$\nm(\\mathbf{x}_{1},\\mathbf{x}_{2} \\mid \\alpha)=\\prod_{j=1}^{2} \\left[ \\frac{N_{j}!}{\\prod_{c \\in \\{A,B,C\\}} x_{jc}!} \\cdot \\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha+N_{j})}\\prod_{c \\in \\{A,B,C\\}} \\frac{\\Gamma\\!\\left(\\frac{\\alpha}{3}+x_{jc}\\right)}{\\Gamma\\!\\left(\\frac{\\alpha}{3}\\right)} \\right].\n$$\nWe now perform Bayesian model selection between $H_{hom}$ and $H_{het}$, which specify $\\alpha=\\alpha_{hom}$ and $\\alpha=\\alpha_{het}$ respectively. By Bayes' rule for hypothesis testing with prior $P(H_{hom})=\\pi_{0}$ and $P(H_{het})=1-\\pi_{0}$, the posterior probability of $H_{hom}$ is\n$$\nP(H_{hom} \\mid \\mathbf{x}_{1},\\mathbf{x}_{2})=\\frac{\\pi_{0}\\, m(\\mathbf{x}_{1},\\mathbf{x}_{2} \\mid \\alpha_{hom})}{\\pi_{0}\\, m(\\mathbf{x}_{1},\\mathbf{x}_{2} \\mid \\alpha_{hom})+(1-\\pi_{0})\\, m(\\mathbf{x}_{1},\\mathbf{x}_{2} \\mid \\alpha_{het})}.\n$$\nEquivalently, using the Bayes factor representation, this can be written as\n$$\nP(H_{hom} \\mid \\mathbf{x}_{1},\\mathbf{x}_{2})=\\left[1+\\frac{1-\\pi_{0}}{\\pi_{0}} \\cdot \\frac{m(\\mathbf{x}_{1},\\mathbf{x}_{2} \\mid \\alpha_{het})}{m(\\mathbf{x}_{1},\\mathbf{x}_{2} \\mid \\alpha_{hom})}\\right]^{-1}.\n$$\nSubstituting the product-form marginal likelihoods and simplifying by canceling the multinomial coefficients, we obtain\n$$\n\\frac{m(\\mathbf{x}_{1},\\mathbf{x}_{2} \\mid \\alpha_{het})}{m(\\mathbf{x}_{1},\\mathbf{x}_{2} \\mid \\alpha_{hom})}\n=\\prod_{j=1}^{2} \\left[ \\frac{\\Gamma(\\alpha_{het})}{\\Gamma(\\alpha_{hom})} \\cdot \\frac{\\Gamma(\\alpha_{hom}+N_{j})}{\\Gamma(\\alpha_{het}+N_{j})} \\cdot \\prod_{c \\in \\{A,B,C\\}} \\frac{\\Gamma\\!\\left(\\frac{\\alpha_{het}}{3}+x_{jc}\\right)}{\\Gamma\\!\\left(\\frac{\\alpha_{hom}}{3}+x_{jc}\\right)} \\cdot \\frac{\\Gamma\\!\\left(\\frac{\\alpha_{hom}}{3}\\right)}{\\Gamma\\!\\left(\\frac{\\alpha_{het}}{3}\\right)} \\right].\n$$\nTherefore, the closed-form analytic expression for the posterior probability of the homogeneous market hypothesis is\n$$\nP(H_{hom} \\mid \\mathbf{x}_{1},\\mathbf{x}_{2})=\\left[1+\\frac{1-\\pi_{0}}{\\pi_{0}} \\prod_{j=1}^{2} \\left\\{ \\frac{\\Gamma(\\alpha_{het})}{\\Gamma(\\alpha_{hom})} \\cdot \\frac{\\Gamma(\\alpha_{hom}+N_{j})}{\\Gamma(\\alpha_{het}+N_{j})} \\cdot \\prod_{c \\in \\{A,B,C\\}} \\frac{\\Gamma\\!\\left(\\frac{\\alpha_{het}}{3}+x_{jc}\\right)}{\\Gamma\\!\\left(\\frac{\\alpha_{hom}}{3}+x_{jc}\\right)} \\cdot \\frac{\\Gamma\\!\\left(\\frac{\\alpha_{hom}}{3}\\right)}{\\Gamma\\!\\left(\\frac{\\alpha_{het}}{3}\\right)} \\right\\} \\right]^{-1}.\n$$\nThis expression is in terms of $\\pi_{0}$, $\\alpha_{hom}$, $\\alpha_{het}$, $\\mathbf{x}_{1}$, $\\mathbf{x}_{2}$, $N_{1}$, and $N_{2}$, as required.", "answer": "$$\\boxed{\\left[1+\\frac{1-\\pi_{0}}{\\pi_{0}} \\prod_{j=1}^{2} \\left\\{ \\frac{\\Gamma(\\alpha_{het})}{\\Gamma(\\alpha_{hom})} \\cdot \\frac{\\Gamma(\\alpha_{hom}+N_{j})}{\\Gamma(\\alpha_{het}+N_{j})} \\cdot \\prod_{c \\in \\{A,B,C\\}} \\frac{\\Gamma\\!\\left(\\frac{\\alpha_{het}}{3}+x_{jc}\\right)}{\\Gamma\\!\\left(\\frac{\\alpha_{hom}}{3}+x_{jc}\\right)} \\cdot \\frac{\\Gamma\\!\\left(\\frac{\\alpha_{hom}}{3}\\right)}{\\Gamma\\!\\left(\\frac{\\alpha_{het}}{3}\\right)} \\right\\} \\right]^{-1}}$$", "id": "1920764"}]}