{"hands_on_practices": [{"introduction": "The analysis of event rates is a cornerstone of many scientific disciplines, from physics to finance. This practice introduces the foundational Gamma-Poisson model, an elegant Bayesian tool for this exact purpose. Here, we model the rate parameter $\\lambda$ of a Poisson process using a Gamma distribution as a prior. The beauty of this pairing lies in its conjugacy: the posterior distribution for $\\lambda$, after observing data, remains within the Gamma family, simplifying calculations immensely. This exercise [@problem_id:1909044] offers a concrete, hands-on application, guiding you through the mechanics of updating your beliefs about a cosmic ray detection rate, a fundamental skill for any aspiring data scientist or statistician.", "problem": "A physicist is studying the rate of cosmic ray detections using a new sensor. The number of detections, $k$, in a fixed time interval is modeled by a Poisson distribution with an unknown rate parameter $\\lambda$. The probability mass function of the Poisson distribution is given by $P(k|\\lambda) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$ for $k = 0, 1, 2, \\dots$.\n\nBefore conducting the experiment, the physicist's prior belief about the rate $\\lambda$ (in detections per hour) is described by a Gamma distribution. The probability density function of a Gamma distribution with shape parameter $\\alpha$ and rate parameter $\\beta$ is $p(\\lambda|\\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta\\lambda)$ for $\\lambda > 0$. The physicist's prior is specified by a shape parameter $\\alpha_{prior} = 10$ and a rate parameter $\\beta_{prior} = 2$.\n\nIn a single one-hour observation period, the physicist records 5 cosmic ray detections.\n\nGiven this observation, determine the shape and rate parameters of the posterior distribution for $\\lambda$. Present your answer as a pair of numbers: the posterior shape parameter followed by the posterior rate parameter.", "solution": "We model the count in one hour as $k \\sim \\text{Poisson}(\\lambda)$ with likelihood\n$$\nP(k \\mid \\lambda)=\\frac{\\lambda^{k}\\exp(-\\lambda)}{k!}.\n$$\nThe prior for $\\lambda$ is $\\lambda \\sim \\text{Gamma}(\\alpha_{\\text{prior}},\\beta_{\\text{prior}})$ with density\n$$\np(\\lambda \\mid \\alpha_{\\text{prior}},\\beta_{\\text{prior}})=\\frac{\\beta_{\\text{prior}}^{\\alpha_{\\text{prior}}}}{\\Gamma(\\alpha_{\\text{prior}})}\\lambda^{\\alpha_{\\text{prior}}-1}\\exp(-\\beta_{\\text{prior}}\\lambda), \\quad \\lambda>0.\n$$\nUsing Bayesâ€™ rule, the posterior is proportional to likelihood times prior:\n$$\np(\\lambda \\mid k)\\propto P(k\\mid \\lambda)\\,p(\\lambda \\mid \\alpha_{\\text{prior}},\\beta_{\\text{prior}})\n\\propto \\left[\\lambda^{k}\\exp(-\\lambda)\\right]\\left[\\lambda^{\\alpha_{\\text{prior}}-1}\\exp(-\\beta_{\\text{prior}}\\lambda)\\right].\n$$\nCombine powers of $\\lambda$ and exponential terms:\n$$\np(\\lambda \\mid k)\\propto \\lambda^{(\\alpha_{\\text{prior}}+k)-1}\\exp\\!\\left(-(\\beta_{\\text{prior}}+1)\\lambda\\right).\n$$\nThis is the kernel of a Gamma distribution with shape parameter $\\alpha_{\\text{post}}=\\alpha_{\\text{prior}}+k$ and rate parameter $\\beta_{\\text{post}}=\\beta_{\\text{prior}}+1$ (the $+1$ arises from the one-hour observation window).\n\nSubstituting the given values $\\alpha_{\\text{prior}}=10$, $\\beta_{\\text{prior}}=2$, and $k=5$:\n$$\n\\alpha_{\\text{post}}=10+5=15, \\quad \\beta_{\\text{post}}=2+1=3.\n$$\nThus, the posterior is $\\text{Gamma}(15,3)$, so the requested pair is the posterior shape and rate parameters $(15,3)$.", "answer": "$$\\boxed{\\begin{pmatrix}15 & 3\\end{pmatrix}}$$", "id": "1909044"}, {"introduction": "Moving beyond single-rate events, many real-world problems involve classifying outcomes into multiple categories. This calls for a more sophisticated model, and the Dirichlet-Multinomial framework is the classic Bayesian solution. The Dirichlet distribution acts as a conjugate prior for the parameters of a Multinomial distribution, allowing us to model and update our beliefs about the probabilities of several categories simultaneously. This practice [@problem_id:1909019] demonstrates how the principles of conjugacy scale from a single parameter to a vector of parameters. By working through the example of classifying celestial objects, you will see how elegantly prior knowledge and new observational data combine to refine our understanding across all categories.", "problem": "An astronomer is using a new automated classification algorithm for celestial objects observed by a survey telescope. The algorithm categorizes each detected object into one of three classes: Star, Galaxy, or Nebula. Let the unknown probabilities that any given object belongs to these classes be $p_S$, $p_G$, and $p_N$ respectively, where $p_S + p_G + p_N = 1$.\n\nThe astronomer's prior belief about the probability vector $\\mathbf{p} = (p_S, p_G, p_N)$ is modeled by a Dirichlet distribution with parameters $\\boldsymbol{\\alpha} = (\\alpha_S, \\alpha_G, \\alpha_N) = (3, 5, 2)$.\n\nAfter a trial run, the algorithm is applied to a new set of observations, yielding the following counts: 40 Stars, 50 Galaxies, and 10 Nebulae.\n\nWhat are the parameters of the posterior Dirichlet distribution for the probability vector $\\mathbf{p}$ after observing this new data? Present your answer as a row matrix of the form $\\begin{pmatrix} \\alpha'_S & \\alpha'_G & \\alpha'_N \\end{pmatrix}$.", "solution": "We model the class probabilities $\\mathbf{p}=(p_{S},p_{G},p_{N})$ with a Dirichlet prior with parameters $\\boldsymbol{\\alpha}=(\\alpha_{S},\\alpha_{G},\\alpha_{N})=(3,5,2)$, written as $\\mathbf{p}\\sim\\mathrm{Dir}(\\alpha_{S},\\alpha_{G},\\alpha_{N})$. The data consist of counts from a multinomial sampling: $n_{S}=40$, $n_{G}=50$, and $n_{N}=10$. By Dirichlet-multinomial conjugacy, the posterior is also Dirichlet with updated parameters given componentwise by\n$$\n\\alpha'_{k}=\\alpha_{k}+n_{k}\\quad\\text{for}\\quad k\\in\\{S,G,N\\}.\n$$\nApplying this update rule yields\n$$\n\\alpha'_{S}=3+40=43,\\quad \\alpha'_{G}=5+50=55,\\quad \\alpha'_{N}=2+10=12.\n$$\nTherefore, the posterior Dirichlet parameters are\n$$\n\\begin{pmatrix}43 & 55 & 12\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}43 & 55 & 12\\end{pmatrix}}$$", "id": "1909019"}, {"introduction": "This exercise explores the conjugate relationship between the Normal distribution as both a prior and a likelihood, a common scenario when estimating continuous parameters like a physical constant. We investigate a special thought experiment: what happens when our experimental data perfectly corroborates our prior expectation? This specific case, where the sample mean $\\bar{x}$ equals the prior mean $\\mu_0$, is designed to reveal a deeper insight into the nature of Bayesian learning. Through this practice [@problem_id:1909023], you will discover a profound principle: even data that merely confirms what you already believed serves to increase your certainty. You will see firsthand how the posterior variance decreases, quantifying how new evidence sharpens our knowledge, even when it doesn't change our central estimate.", "problem": "A researcher is attempting to determine a fundamental physical constant, denoted by $\\mu$. Based on existing theoretical work, the researcher's prior belief about $\\mu$ is modeled by a normal distribution with a mean $\\mu_0$ and a variance $\\sigma_0^2$. To refine this estimate, the researcher conducts an experiment consisting of $n$ independent measurements, which are assumed to be drawn from a normal distribution with mean $\\mu$ and a known, constant instrumental variance of $\\sigma^2$. After completing the experiment, the researcher calculates the sample mean of the measurements, $\\bar{x}$. By a remarkable coincidence, the calculated sample mean is identical to the prior mean, i.e., $\\bar{x} = \\mu_0$.\n\nWithin this Bayesian framework, the researcher updates their belief about $\\mu$ to a posterior distribution. Find the simplified expressions for the posterior mean and the posterior variance of $\\mu$ under this specific condition. Your answer should be a pair of expressions, the first for the posterior mean and the second for the posterior variance, in terms of some or all of the given parameters: $\\mu_0, \\sigma_0^2, n,$ and $\\sigma^2$.", "solution": "We model the prior and likelihood as follows. The prior for the parameter is Gaussian: $\\mu \\sim \\mathcal{N}(\\mu_{0},\\sigma_{0}^{2})$. The data consist of $n$ independent observations $x_{1},\\ldots,x_{n}$ with $x_{i} \\mid \\mu \\sim \\mathcal{N}(\\mu,\\sigma^{2})$, so the sample mean satisfies $\\bar{x} \\mid \\mu \\sim \\mathcal{N}\\!\\left(\\mu,\\frac{\\sigma^{2}}{n}\\right)$.\n\nBy conjugacy of the normal prior with the normal likelihood with known variance, the posterior for $\\mu$ is normal. Writing the posterior density up to proportionality and collecting terms in $\\mu$,\n$$\np(\\mu \\mid \\bar{x}) \\propto \\exp\\!\\left(-\\frac{1}{2\\sigma_{0}^{2}}(\\mu-\\mu_{0})^{2}\\right)\\exp\\!\\left(-\\frac{n}{2\\sigma^{2}}(\\bar{x}-\\mu)^{2}\\right).\n$$\nExpanding and completing the square in $\\mu$, the quadratic coefficient (posterior precision) is\n$$\na \\equiv \\frac{1}{\\sigma_{0}^{2}}+\\frac{n}{\\sigma^{2}},\n$$\nand the linear coefficient is\n$$\nb \\equiv \\frac{\\mu_{0}}{\\sigma_{0}^{2}}+\\frac{n\\,\\bar{x}}{\\sigma^{2}}.\n$$\nTherefore, the posterior is $\\mathcal{N}(m_{n},v_{n})$ with\n$$\nv_{n}=\\frac{1}{a}=\\left(\\frac{1}{\\sigma_{0}^{2}}+\\frac{n}{\\sigma^{2}}\\right)^{-1}=\\frac{\\sigma_{0}^{2}\\sigma^{2}}{\\sigma^{2}+n\\sigma_{0}^{2}},\n\\qquad\nm_{n}=\\frac{b}{a}=v_{n}\\left(\\frac{\\mu_{0}}{\\sigma_{0}^{2}}+\\frac{n\\,\\bar{x}}{\\sigma^{2}}\\right).\n$$\nUnder the specific condition $\\bar{x}=\\mu_{0}$, the posterior mean simplifies to\n$$\nm_{n}=v_{n}\\left(\\frac{\\mu_{0}}{\\sigma_{0}^{2}}+\\frac{n\\,\\mu_{0}}{\\sigma^{2}}\\right)=\\mu_{0}\\,v_{n}\\left(\\frac{1}{\\sigma_{0}^{2}}+\\frac{n}{\\sigma^{2}}\\right)=\\mu_{0},\n$$\nand the posterior variance remains\n$$\nv_{n}=\\frac{\\sigma_{0}^{2}\\sigma^{2}}{\\sigma^{2}+n\\sigma_{0}^{2}}.\n$$\nThus, the posterior mean equals the common value $\\mu_{0}$ and the posterior variance is the harmonic-mean combination of the prior and data precisions.", "answer": "$$\\boxed{\\begin{pmatrix}\\mu_{0} & \\dfrac{\\sigma_{0}^{2}\\sigma^{2}}{\\sigma^{2}+n\\sigma_{0}^{2}}\\end{pmatrix}}$$", "id": "1909023"}]}