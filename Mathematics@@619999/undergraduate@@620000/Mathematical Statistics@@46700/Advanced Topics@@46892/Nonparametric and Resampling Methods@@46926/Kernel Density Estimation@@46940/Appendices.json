{"hands_on_practices": [{"introduction": "The best way to understand Kernel Density Estimation is to compute one by hand. This exercise walks you through the fundamental process of building a density estimate from a small dataset using the popular Gaussian kernel. By calculating the contribution of each data point, you will see precisely how the KDE formula transforms discrete observations into a smooth, continuous probability density function [@problem_id:1927623].", "problem": "In statistical analysis, kernel density estimation is a non-parametric way to estimate the probability density function of a random variable. The kernel density estimate at a point $x$ for a given set of data points $D = \\{x_1, x_2, \\ldots, x_n\\}$ is defined by the formula:\n$$\n\\hat{f}_h(x) = \\frac{1}{nh} \\sum_{i=1}^{n} K\\left(\\frac{x - x_i}{h}\\right)\n$$\nwhere $n$ is the number of data points, $h > 0$ is a smoothing parameter called the bandwidth, and $K(u)$ is a non-negative function called the kernel that integrates to one.\n\nConsider a small dataset of three experimental measurements: $D = \\{-2, 0, 3\\}$. We wish to estimate the underlying probability density at the point $x=1$. For this estimation, we will use the standard normal kernel, whose probability density function is given by $K(u) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}u^2\\right)$. The bandwidth is chosen to be $h=2$.\n\nCalculate the value of the kernel density estimate $\\hat{f}_h(x)$ at $x=1$. Provide your answer as a single closed-form analytic expression.", "solution": "We use the kernel density estimator\n$$\n\\hat{f}_{h}(x)=\\frac{1}{nh}\\sum_{i=1}^{n}K\\!\\left(\\frac{x-x_{i}}{h}\\right),\n$$\nwith $n=3$, $h=2$, $x=1$, data $D=\\{-2,0,3\\}$, and the standard normal kernel $K(u)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{1}{2}u^{2}\\right)$.\nThus,\n$$\n\\hat{f}_{2}(1)=\\frac{1}{3\\cdot 2}\\sum_{x_{i}\\in D}\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{1-x_{i}}{2}\\right)^{2}\\right)\n=\\frac{1}{6\\sqrt{2\\pi}}\\sum_{x_{i}\\in D}\\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{1-x_{i}}{2}\\right)^{2}\\right).\n$$\nCompute the arguments:\n$$\n\\frac{1-(-2)}{2}=\\frac{3}{2},\\quad \\frac{1-0}{2}=\\frac{1}{2},\\quad \\frac{1-3}{2}=-1.\n$$\nTherefore,\n$$\n\\hat{f}_{2}(1)=\\frac{1}{6\\sqrt{2\\pi}}\\left[\\exp\\!\\left(-\\frac{1}{2}\\cdot\\left(\\frac{3}{2}\\right)^{2}\\right)+\\exp\\!\\left(-\\frac{1}{2}\\cdot\\left(\\frac{1}{2}\\right)^{2}\\right)+\\exp\\!\\left(-\\frac{1}{2}\\cdot 1^{2}\\right)\\right]\n=\\frac{1}{6\\sqrt{2\\pi}}\\left(\\exp\\!\\left(-\\frac{9}{8}\\right)+\\exp\\!\\left(-\\frac{1}{8}\\right)+\\exp\\!\\left(-\\frac{1}{2}\\right)\\right).\n$$\nThis gives the required closed-form analytic expression.", "answer": "$$\\boxed{\\frac{1}{6\\sqrt{2\\pi}}\\left(\\exp\\!\\left(-\\frac{9}{8}\\right)+\\exp\\!\\left(-\\frac{1}{8}\\right)+\\exp\\!\\left(-\\frac{1}{2}\\right)\\right)}$$", "id": "1927623"}, {"introduction": "Now that we have a grasp on the mechanics of KDE, letâ€™s explore its fundamental properties. This thought experiment investigates how the density estimate behaves when the entire dataset is shifted by a constant value, $c$. Understanding this transformation-equivariance is crucial for building intuition about how the KDE model relates to the location and scale of your data [@problem_id:1927606].", "problem": "A statistician is analyzing a dataset consisting of $n$ observations, $\\{X_1, X_2, \\ldots, X_n\\}$. To estimate the underlying probability density function from which the data were sampled, they compute a Kernel Density Estimate (KDE). The KDE, denoted as $\\hat{f}_h(x)$, is calculated using a kernel function $K(u)$ and a bandwidth parameter $h > 0$. The formula for the KDE is given by:\n$$\n\\hat{f}_h(x) = \\frac{1}{nh} \\sum_{i=1}^{n} K\\left(\\frac{x - X_i}{h}\\right)\n$$\nSubsequently, the statistician decides to apply a linear transformation to the data for recalibration purposes. A new dataset, $\\{Y_1, Y_2, \\ldots, Y_n\\}$, is created where each new data point is a shifted version of the original, defined by the relation $Y_i = X_i + c$ for some non-zero constant $c$.\n\nA new KDE, denoted by $\\hat{g}_h(x)$, is then computed for this transformed dataset, using the exact same kernel function $K(u)$ and bandwidth $h$.\n\nWhich of the following options correctly describes the relationship between the new estimate $\\hat{g}_h(x)$ and the original estimate $\\hat{f}_h(x)$?\n\nA. $\\hat{g}_h(x) = \\hat{f}_h(x) + \\frac{c}{nh}$\n\nB. $\\hat{g}_h(x) = \\hat{f}_h(x+c)$\n\nC. $\\hat{g}_h(x) = \\hat{f}_h(x)$\n\nD. $\\hat{g}_h(x) = \\hat{f}_h(x-c)$\n\nE. $\\hat{g}_h(x) = \\hat{f}_h(x) + c$", "solution": "By definition, the KDE based on $\\{X_{i}\\}_{i=1}^{n}$ with kernel $K$ and bandwidth $h>0$ is\n$$\n\\hat{f}_{h}(x)=\\frac{1}{nh}\\sum_{i=1}^{n}K\\!\\left(\\frac{x-X_{i}}{h}\\right).\n$$\nFor the transformed data $Y_{i}=X_{i}+c$, the KDE (using the same $K$ and $h$) is\n$$\n\\hat{g}_{h}(x)=\\frac{1}{nh}\\sum_{i=1}^{n}K\\!\\left(\\frac{x-Y_{i}}{h}\\right).\n$$\nSubstituting $Y_{i}=X_{i}+c$ gives\n$$\n\\hat{g}_{h}(x)=\\frac{1}{nh}\\sum_{i=1}^{n}K\\!\\left(\\frac{x-(X_{i}+c)}{h}\\right)=\\frac{1}{nh}\\sum_{i=1}^{n}K\\!\\left(\\frac{(x-c)-X_{i}}{h}\\right).\n$$\nBy the definition of $\\hat{f}_{h}$, the right-hand side equals $\\hat{f}_{h}(x-c)$, since\n$$\n\\hat{f}_{h}(x-c)=\\frac{1}{nh}\\sum_{i=1}^{n}K\\!\\left(\\frac{(x-c)-X_{i}}{h}\\right).\n$$\nTherefore,\n$$\n\\hat{g}_{h}(x)=\\hat{f}_{h}(x-c),\n$$\nwhich corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1927606"}, {"introduction": "The performance of a kernel density estimate is critically dependent on the choice of the bandwidth, $h$. This final practice introduces a principled and widely used technique for optimizing this parameter: Leave-One-Out Cross-Validation (LOOCV). By calculating the LOOCV score, you will engage with the core challenge of balancing the bias-variance trade-off to find a bandwidth that generalizes well to new data [@problem_id:1927653].", "problem": "A data scientist is performing a preliminary analysis on a small set of normalized error measurements from a newly calibrated instrument. The dataset is given by $X = \\{x_1, x_2, x_3\\} = \\{-1, 0, 1\\}$. To model the probability distribution of these errors, the scientist employs a Kernel Density Estimation (KDE), a non-parametric method to estimate the probability density function of a random variable.\n\nThe KDE is defined as:\n$$ \\hat{f}_h(x) = \\frac{1}{nh} \\sum_{i=1}^{n} K\\left(\\frac{x-x_i}{h}\\right) $$\nwhere $n$ is the number of data points, $h$ is a smoothing parameter called the bandwidth, and $K(u)$ is the kernel function. For this analysis, the standard Gaussian kernel is used: $K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right)$.\n\nTo evaluate the suitability of a particular bandwidth, the scientist calculates the Leave-One-Out Cross-Validation (LOOCV) score, an objective function designed to estimate the error of the KDE. This function, denoted by $J(h)$, is defined as:\n$$ J(h) = \\int_{-\\infty}^{\\infty} \\left(\\hat{f}_h(x)\\right)^2 dx - \\frac{2}{n} \\sum_{i=1}^{n} \\hat{f}_{h, -i}(x_i) $$\nHere, $\\hat{f}_{h, -i}(x_i)$ is the 'leave-one-out' density estimate at data point $x_i$, which is computed by constructing the KDE using all data points except for $x_i$ itself:\n$$ \\hat{f}_{h, -i}(x_i) = \\frac{1}{(n-1)h} \\sum_{j \\neq i} K\\left(\\frac{x_i - x_j}{h}\\right) $$\nYou may use the following standard convolution identity for the Gaussian kernel without proof:\n$$ \\int_{-\\infty}^{\\infty} K\\left(\\frac{x-a}{h}\\right) K\\left(\\frac{x-b}{h}\\right) dx = \\frac{h}{\\sqrt{4\\pi}}\\exp\\left(-\\frac{(a-b)^2}{4h^2}\\right) $$\nCalculate the exact value of the LOOCV score $J(h)$ for the given dataset $X=\\{-1, 0, 1\\}$ using a bandwidth of $h=1$. Express your answer as a single symbolic expression involving $\\pi$, $e$, and numerical constants.", "solution": "We have $n=3$, $h=1$, $X=\\{-1,0,1\\}$, and $K(u)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{u^{2}}{2}\\right)$. The KDE is $\\hat{f}_{1}(x)=\\frac{1}{3}\\sum_{i=1}^{3}K(x-x_{i})$.\n\nFirst, compute the integrated squared density:\n$$\n\\int_{-\\infty}^{\\infty}\\left(\\hat{f}_{1}(x)\\right)^{2}dx\n=\\frac{1}{9}\\sum_{i=1}^{3}\\sum_{j=1}^{3}\\int_{-\\infty}^{\\infty}K(x-x_{i})K(x-x_{j})\\,dx.\n$$\nUsing the provided Gaussian convolution identity with $h=1$ and $a=x_{i}$, $b=x_{j}$,\n$$\n\\int_{-\\infty}^{\\infty}K(x-x_{i})K(x-x_{j})\\,dx=\\frac{1}{\\sqrt{4\\pi}}\\exp\\left(-\\frac{(x_{i}-x_{j})^{2}}{4}\\right).\n$$\nThus,\n$$\n\\int_{-\\infty}^{\\infty}\\left(\\hat{f}_{1}(x)\\right)^{2}dx\n=\\frac{1}{9\\sqrt{4\\pi}}\\sum_{i=1}^{3}\\sum_{j=1}^{3}\\exp\\left(-\\frac{(x_{i}-x_{j})^{2}}{4}\\right).\n$$\nFor $X=\\{-1,0,1\\}$, the pairwise squared differences are $0$ (three times on the diagonal), $1$ (four ordered off-diagonal pairs between neighbors), and $4$ (two ordered off-diagonal pairs between $-1$ and $1$). Therefore,\n$$\n\\sum_{i=1}^{3}\\sum_{j=1}^{3}\\exp\\left(-\\frac{(x_{i}-x_{j})^{2}}{4}\\right)\n=3+4\\exp\\left(-\\frac{1}{4}\\right)+2\\exp\\left(-1\\right).\n$$\nHence,\n$$\n\\int_{-\\infty}^{\\infty}\\left(\\hat{f}_{1}(x)\\right)^{2}dx\n=\\frac{1}{9\\sqrt{4\\pi}}\\left(3+4\\exp\\left(-\\frac{1}{4}\\right)+2\\exp(-1)\\right).\n$$\n\nNext, compute the leave-one-out terms. For $h=1$,\n$$\n\\hat{f}_{1,-i}(x_{i})=\\frac{1}{2}\\sum_{j\\neq i}K(x_{i}-x_{j}).\n$$\nEvaluate for each $x_{i}$:\n- For $x_{1}=-1$: differences to others are $-1$ and $-2$, so\n$$\n\\hat{f}_{1,-1}(-1)=\\frac{1}{2}\\left(\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\right)+\\frac{1}{\\sqrt{2\\pi}}\\exp(-2)\\right)=\\frac{1}{2\\sqrt{2\\pi}}\\left(\\exp\\left(-\\frac{1}{2}\\right)+\\exp(-2)\\right).\n$$\n- For $x_{2}=0$: differences are $1$ and $-1$, so\n$$\n\\hat{f}_{1,-2}(0)=\\frac{1}{2}\\left(\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\right)+\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\right)\\right)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\right).\n$$\n- For $x_{3}=1$: differences are $2$ and $1$, so\n$$\n\\hat{f}_{1,-3}(1)=\\frac{1}{2\\sqrt{2\\pi}}\\left(\\exp\\left(-\\frac{1}{2}\\right)+\\exp(-2)\\right).\n$$\nSumming,\n$$\n\\sum_{i=1}^{3}\\hat{f}_{1,-i}(x_{i})=\\frac{1}{\\sqrt{2\\pi}}\\left(2\\exp\\left(-\\frac{1}{2}\\right)+\\exp(-2)\\right).\n$$\n\nTherefore, the LOOCV score is\n$$\nJ(1)=\\int_{-\\infty}^{\\infty}\\left(\\hat{f}_{1}(x)\\right)^{2}dx-\\frac{2}{3}\\sum_{i=1}^{3}\\hat{f}_{1,-i}(x_{i}),\n$$\nwhich gives\n$$\nJ(1)=\\frac{1}{9\\sqrt{4\\pi}}\\left(3+4\\exp\\left(-\\frac{1}{4}\\right)+2\\exp(-1)\\right)-\\frac{2}{3\\sqrt{2\\pi}}\\left(2\\exp\\left(-\\frac{1}{2}\\right)+\\exp(-2)\\right).\n$$\nThis is the exact symbolic expression in terms of $\\pi$, $\\exp(\\cdot)$, and numerical constants.", "answer": "$$\\boxed{\\frac{1}{9\\sqrt{4\\pi}}\\left(3+4\\exp\\left(-\\frac{1}{4}\\right)+2\\exp(-1)\\right)-\\frac{2}{3\\sqrt{2\\pi}}\\left(2\\exp\\left(-\\frac{1}{2}\\right)+\\exp(-2)\\right)}$$", "id": "1927653"}]}