{"hands_on_practices": [{"introduction": "Let's begin by applying the jackknife method to a practical problem in reliability engineering. We will estimate the bias of a commonly used estimator for the rate parameter of an exponential distribution, which is itself a non-linear function of the sample mean [@problem_id:1961125]. This hands-on calculation will help you master the step-by-step mechanics of the jackknife procedure and demonstrate how it provides a numerical estimate of bias using only the observed data.", "problem": "In reliability engineering, the lifetime of a certain electronic component is often modeled by an exponential distribution with a rate parameter $\\lambda$. The parameter $\\lambda$ represents the constant failure rate. A common estimator for this parameter, based on a sample of observed lifetimes $X_1, X_2, \\ldots, X_n$, is the inverse of the sample mean, given by $\\hat{\\lambda} = 1/\\bar{X}$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. While this estimator is the Maximum Likelihood Estimator, it is known to be biased.\n\nSuppose a small sample of $n=4$ component lifetimes has been collected, with the observed values in minutes being:\n$$ \\{2.3, 1.5, 3.1, 2.5\\} $$\nApply the jackknife resampling method to calculate a numerical estimate for the bias of the estimator $\\hat{\\lambda}$.\n\nExpress your answer in units of $\\text{min}^{-1}$, rounded to four significant figures.", "solution": "The problem asks for the jackknife estimate of the bias of the estimator $\\hat{\\lambda} = 1/\\bar{X}$ for a given dataset. The dataset is $X = \\{2.3, 1.5, 3.1, 2.5\\}$, and the sample size is $n=4$. The units of the data are minutes, so the units of $\\lambda$ and its bias will be $\\text{min}^{-1}$.\n\nFirst, we calculate the sample mean $\\bar{X}$ and the estimate $\\hat{\\lambda}$ from the full sample.\nThe sum of the observations is $\\sum_{i=1}^4 X_i = 2.3 + 1.5 + 3.1 + 2.5 = 9.4$.\nThe sample mean is $\\bar{X} = \\frac{9.4}{4} = 2.35 \\text{ min}$.\nThe estimate of $\\lambda$ based on the full sample is $\\hat{\\lambda} = \\frac{1}{\\bar{X}} = \\frac{1}{2.35} \\text{ min}^{-1}$.\n\nNext, we apply the jackknife procedure. This involves creating $n=4$ subsamples, where each subsample is formed by removing one observation at a time from the original sample. For each subsample, we calculate the corresponding estimate of $\\lambda$. Let $X_{(i)}$ denote the subsample with the $i$-th observation removed, and let $\\hat{\\lambda}_{(i)}$ be the estimate calculated from this subsample.\n\nThe mean of a subsample with observation $X_i$ removed, $\\bar{X}_{(i)}$, can be calculated as $\\bar{X}_{(i)} = \\frac{n\\bar{X} - X_i}{n-1}$. The corresponding estimate is $\\hat{\\lambda}_{(i)} = 1/\\bar{X}_{(i)}$.\n\nLet's calculate $\\hat{\\lambda}_{(i)}$ for each $i=1, 2, 3, 4$:\nFor $i=1$, we remove $X_1 = 2.3$.\n$\\bar{X}_{(1)} = \\frac{4 \\times 2.35 - 2.3}{4-1} = \\frac{9.4 - 2.3}{3} = \\frac{7.1}{3} \\text{ min}$.\n$\\hat{\\lambda}_{(1)} = \\frac{1}{\\bar{X}_{(1)}} = \\frac{3}{7.1} \\text{ min}^{-1}$.\n\nFor $i=2$, we remove $X_2 = 1.5$.\n$\\bar{X}_{(2)} = \\frac{4 \\times 2.35 - 1.5}{4-1} = \\frac{9.4 - 1.5}{3} = \\frac{7.9}{3} \\text{ min}$.\n$\\hat{\\lambda}_{(2)} = \\frac{1}{\\bar{X}_{(2)}} = \\frac{3}{7.9} \\text{ min}^{-1}$.\n\nFor $i=3$, we remove $X_3 = 3.1$.\n$\\bar{X}_{(3)} = \\frac{4 \\times 2.35 - 3.1}{4-1} = \\frac{9.4 - 3.1}{3} = \\frac{6.3}{3} = 2.1 \\text{ min}$.\n$\\hat{\\lambda}_{(3)} = \\frac{1}{\\bar{X}_{(3)}} = \\frac{1}{2.1} \\text{ min}^{-1}$.\n\nFor $i=4$, we remove $X_4 = 2.5$.\n$\\bar{X}_{(4)} = \\frac{4 \\times 2.35 - 2.5}{4-1} = \\frac{9.4 - 2.5}{3} = \\frac{6.9}{3} = 2.3 \\text{ min}$.\n$\\hat{\\lambda}_{(4)} = \\frac{1}{\\bar{X}_{(4)}} = \\frac{1}{2.3} \\text{ min}^{-1}$.\n\nThe jackknife estimate of bias is given by the formula:\n$$ \\widehat{\\text{Bias}}_{\\text{jack}}(\\hat{\\lambda}) = (n-1)(\\bar{\\hat{\\lambda}}_{(\\cdot)} - \\hat{\\lambda}) $$\nwhere $\\bar{\\hat{\\lambda}}_{(\\cdot)}$ is the average of the leave-one-out estimates:\n$$ \\bar{\\hat{\\lambda}}_{(\\cdot)} = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\lambda}_{(i)} $$\n\nFirst, we find the numerical values of the estimates, keeping sufficient precision for intermediate calculations.\n$\\hat{\\lambda} = \\frac{1}{2.35} \\approx 0.42553191 \\text{ min}^{-1}$\n$\\hat{\\lambda}_{(1)} = \\frac{3}{7.1} \\approx 0.42253521 \\text{ min}^{-1}$\n$\\hat{\\lambda}_{(2)} = \\frac{3}{7.9} \\approx 0.37974684 \\text{ min}^{-1}$\n$\\hat{\\lambda}_{(3)} = \\frac{1}{2.1} \\approx 0.47619048 \\text{ min}^{-1}$\n$\\hat{\\lambda}_{(4)} = \\frac{1}{2.3} \\approx 0.43478261 \\text{ min}^{-1}$\n\nNow, we calculate the average of these leave-one-out estimates:\n$$ \\bar{\\hat{\\lambda}}_{(\\cdot)} = \\frac{1}{4} (0.42253521 + 0.37974684 + 0.47619048 + 0.43478261) $$\n$$ \\bar{\\hat{\\lambda}}_{(\\cdot)} = \\frac{1}{4} (1.71325514) \\approx 0.42831379 \\text{ min}^{-1} $$\n\nFinally, we use the jackknife bias formula:\n$$ \\widehat{\\text{Bias}}_{\\text{jack}}(\\hat{\\lambda}) = (4-1) (0.42831379 - 0.42553191) $$\n$$ \\widehat{\\text{Bias}}_{\\text{jack}}(\\hat{\\lambda}) = 3 \\times (0.00278188) $$\n$$ \\widehat{\\text{Bias}}_{\\text{jack}}(\\hat{\\lambda}) \\approx 0.00834564 \\text{ min}^{-1} $$\n\nThe problem asks for the answer rounded to four significant figures.\n$$ \\widehat{\\text{Bias}}_{\\text{jack}}(\\hat{\\lambda}) \\approx 0.008346 \\text{ min}^{-1} $$", "answer": "$$\\boxed{0.008346}$$", "id": "1961125"}, {"introduction": "Now that we've walked through a numerical example, let's explore the jackknife's ability to yield analytical results. In this problem, we will derive a closed-form expression for the jackknife bias estimate of a common estimator for the variance of a Bernoulli distribution [@problem_id:1961131]. This practice moves from concrete calculation to symbolic manipulation, revealing the deeper theoretical power of the method and providing a general formula rather than a single numerical value.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be an independent and identically distributed random sample of size $n > 1$ drawn from a Bernoulli distribution with an unknown parameter $p$, where $0 < p < 1$. The sample mean is denoted by $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nConsider the estimator $\\hat{\\theta} = \\bar{X}(1-\\bar{X})$ for the population variance, $\\theta = p(1-p)$.\n\nUsing the jackknife resampling method, find an estimate for the bias of $\\hat{\\theta}$. Express your answer as a closed-form analytic expression in terms of $\\bar{X}$ and $n$.", "solution": "We use the jackknife bias estimator for a scalar parameter, defined as\n$$\n\\widehat{\\operatorname{bias}}_{\\text{jack}}(\\hat{\\theta})=(n-1)\\left(\\bar{\\theta}_{(\\cdot)}-\\hat{\\theta}\\right),\n$$\nwhere $\\hat{\\theta}=\\bar{X}(1-\\bar{X})$, $\\hat{\\theta}_{(i)}$ is the leave-one-out version computed with the $i$-th observation removed, and\n$$\n\\bar{\\theta}_{(\\cdot)}=\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\theta}_{(i)}.\n$$\n\nLet $\\bar{X}_{(i)}$ denote the leave-one-out sample mean:\n$$\n\\bar{X}_{(i)}=\\frac{1}{n-1}\\sum_{j\\neq i}X_{j}=\\frac{n\\bar{X}-X_{i}}{n-1}.\n$$\nThen\n$$\n\\hat{\\theta}_{(i)}=\\bar{X}_{(i)}\\left(1-\\bar{X}_{(i)}\\right)=\\bar{X}_{(i)}-\\bar{X}_{(i)}^{2}.\n$$\nTherefore,\n$$\n\\bar{\\theta}_{(\\cdot)}=\\frac{1}{n}\\sum_{i=1}^{n}\\left[\\bar{X}_{(i)}-\\bar{X}_{(i)}^{2}\\right]=\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\bar{X}_{(i)}\\right)-\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\bar{X}_{(i)}^{2}\\right).\n$$\nFirst compute the average of $\\bar{X}_{(i)}$:\n$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\bar{X}_{(i)}=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{n\\bar{X}-X_{i}}{n-1}=\\frac{1}{n-1}\\left(n\\bar{X}-\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right)=\\frac{1}{n-1}\\left(n\\bar{X}-\\bar{X}\\right)=\\bar{X}.\n$$\nNext compute $\\frac{1}{n}\\sum_{i=1}^{n}\\bar{X}_{(i)}^{2}$. Write $b_{i}=\\bar{X}_{(i)}=\\frac{n\\bar{X}-X_{i}}{n-1}$, then\n$$\n\\sum_{i=1}^{n}b_{i}^{2}=\\frac{1}{(n-1)^{2}}\\sum_{i=1}^{n}(n\\bar{X}-X_{i})^{2}\n=\\frac{1}{(n-1)^{2}}\\left(\\sum_{i=1}^{n}n^{2}\\bar{X}^{2}-2n\\bar{X}\\sum_{i=1}^{n}X_{i}+\\sum_{i=1}^{n}X_{i}^{2}\\right).\n$$\nSince $X_{i}\\in\\{0,1\\}$, we have $X_{i}^{2}=X_{i}$, hence $\\sum_{i=1}^{n}X_{i}^{2}=\\sum_{i=1}^{n}X_{i}=n\\bar{X}$. Therefore,\n$$\n\\sum_{i=1}^{n}b_{i}^{2}=\\frac{1}{(n-1)^{2}}\\left(n\\cdot n^{2}\\bar{X}^{2}-2n\\bar{X}\\cdot n\\bar{X}+n\\bar{X}\\right)\n=\\frac{1}{(n-1)^{2}}\\left(n^{3}\\bar{X}^{2}-2n^{2}\\bar{X}^{2}+n\\bar{X}\\right)\n=\\frac{n^{2}(n-2)\\bar{X}^{2}+n\\bar{X}}{(n-1)^{2}}.\n$$\nThus\n$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\bar{X}_{(i)}^{2}=\\frac{n(n-2)\\bar{X}^{2}+\\bar{X}}{(n-1)^{2}}.\n$$\nIt follows that\n$$\n\\bar{\\theta}_{(\\cdot)}=\\bar{X}-\\frac{n(n-2)\\bar{X}^{2}+\\bar{X}}{(n-1)^{2}}.\n$$\nNow compute\n$$\n\\bar{\\theta}_{(\\cdot)}-\\hat{\\theta}=\\left[\\bar{X}-\\frac{n(n-2)\\bar{X}^{2}+\\bar{X}}{(n-1)^{2}}\\right]-\\left(\\bar{X}-\\bar{X}^{2}\\right)\n=\\bar{X}^{2}-\\frac{n(n-2)\\bar{X}^{2}+\\bar{X}}{(n-1)^{2}}.\n$$\nCombine terms over $(n-1)^{2}$:\n$$\n\\bar{\\theta}_{(\\cdot)}-\\hat{\\theta}=\\frac{\\bar{X}^{2}(n-1)^{2}-n(n-2)\\bar{X}^{2}-\\bar{X}}{(n-1)^{2}}.\n$$\nSince $(n-1)^{2}-n(n-2)=1$, the numerator simplifies to $\\bar{X}^{2}-\\bar{X}=-\\bar{X}(1-\\bar{X})$, hence\n$$\n\\bar{\\theta}_{(\\cdot)}-\\hat{\\theta}=-\\frac{\\bar{X}(1-\\bar{X})}{(n-1)^{2}}.\n$$\nTherefore, the jackknife estimate of the bias is\n$$\n\\widehat{\\operatorname{bias}}_{\\text{jack}}(\\hat{\\theta})=(n-1)\\left(\\bar{\\theta}_{(\\cdot)}-\\hat{\\theta}\\right)\n=-(n-1)\\frac{\\bar{X}(1-\\bar{X})}{(n-1)^{2}}=-\\frac{\\bar{X}(1-\\bar{X})}{n-1}.\n$$\nThis expression is in closed form in terms of $\\bar{X}$ and $n$.", "answer": "$$\\boxed{-\\frac{\\bar{X}\\left(1-\\bar{X}\\right)}{n-1}}$$", "id": "1961131"}, {"introduction": "A powerful tool is only as good as the user's understanding of its limitations. This final exercise explores a scenario where the jackknife method's performance is challenged by a 'non-smooth' estimator, the sample maximum [@problem_id:1961143]. By working through this important example, you will gain a more nuanced and critical understanding of the conditions under which the jackknife is an appropriate and effective tool for bias estimation.", "problem": "A small random sample of size $n=5$ is drawn from a continuous uniform distribution on the interval $[0, \\theta]$, where $\\theta > 0$ is an unknown parameter. The observed sample values are $\\{6.3, 2.5, 9.1, 4.8, 1.7\\}$. An estimator for $\\theta$ is proposed as the sample maximum, which we denote as $T(X)$.\n\nUsing the jackknife method, calculate the estimated bias of this estimator. Express your answer as an exact decimal value.", "solution": "We are given an estimator $T(X)$ equal to the sample maximum for a sample of size $n=5$ from a $\\operatorname{Unif}(0,\\theta)$ distribution, with observed sample $\\{6.3, 2.5, 9.1, 4.8, 1.7\\}$. The jackknife estimate of the bias of $T$ is defined by\n$$\n\\widehat{\\operatorname{bias}}_{\\text{jack}}(T)\n=\n(n-1)\\left(\\bar{T}_{(.)}-T\\right),\n$$\nwhere $T$ is the full-sample estimate and $\\bar{T}_{(.)}=\\frac{1}{n}\\sum_{i=1}^{n}T_{(i)}$ is the average of the leave-one-out estimates $T_{(i)}$ computed by omitting the $i$-th observation.\n\nFirst compute the full-sample estimator:\n$$\nT=\\max\\{6.3,2.5,9.1,4.8,1.7\\}=9.1.\n$$\n\nNext compute the leave-one-out maxima:\n- Omitting $6.3$: $T_{(1)}=\\max\\{2.5,9.1,4.8,1.7\\}=9.1$.\n- Omitting $2.5$: $T_{(2)}=\\max\\{6.3,9.1,4.8,1.7\\}=9.1$.\n- Omitting $9.1$: $T_{(3)}=\\max\\{6.3,2.5,4.8,1.7\\}=6.3$.\n- Omitting $4.8$: $T_{(4)}=\\max\\{6.3,2.5,9.1,1.7\\}=9.1$.\n- Omitting $1.7$: $T_{(5)}=\\max\\{6.3,2.5,9.1,4.8\\}=9.1$.\n\nThus\n$$\n\\bar{T}_{(.)}\n=\n\\frac{1}{5}\\left(T_{(1)}+T_{(2)}+T_{(3)}+T_{(4)}+T_{(5)}\\right)\n=\n\\frac{1}{5}\\left(9.1+9.1+6.3+9.1+9.1\\right)\n=\n\\frac{42.7}{5}\n=\n8.54.\n$$\n\nApply the jackknife bias formula with $n=5$:\n$$\n\\widehat{\\operatorname{bias}}_{\\text{jack}}(T)\n=\n(5-1)\\left(8.54-9.1\\right)\n=\n4\\cdot(-0.56)\n=\n-2.24.\n$$\n\nTherefore, the jackknife estimated bias of the sample-maximum estimator for this dataset is $-2.24$.", "answer": "$$\\boxed{-2.24}$$", "id": "1961143"}]}