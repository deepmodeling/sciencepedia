{"hands_on_practices": [{"introduction": "This first practice serves as a fundamental building block for understanding the consistency of Maximum Likelihood Estimators (MLEs). We will explore the Laplace distribution and derive the MLE for its scale parameter. This exercise demonstrates the standard workflow for establishing consistency: showing that the MLE can be expressed as an average of random quantities, which allows us to invoke the powerful Law of Large Numbers. [@problem_id:1895934]", "problem": "Consider a random sample of size $n$, denoted by $X_1, X_2, \\ldots, X_n$, drawn from a Laplace distribution with a known location parameter of 0. The probability density function (PDF) for any observation $X_i$ from this distribution is given by:\n$$f(x; b) = \\frac{1}{2b} \\exp\\left(-\\frac{|x|}{b}\\right)$$\nwhere $x \\in (-\\infty, \\infty)$ and $b > 0$ is the unknown scale parameter.\n\nYour task is to perform two calculations related to the estimation of $b$.\nFirst, determine the Maximum Likelihood Estimator (MLE), which we will denote as $\\hat{b}_n$, for the scale parameter $b$.\nSecond, a crucial step in arguing for the consistency of an estimator involves using the Law of Large Numbers. As a part of this argument, calculate the expected value $E[|X|]$ for a single random variable $X$ that follows the given Laplace distribution.\n\nProvide your answer as a two-component result, where the first component is the symbolic expression for the MLE $\\hat{b}_n$ in terms of the sample observations, and the second component is the symbolic expression for the expected value $E[|X|]$.", "solution": "We are given independent observations $X_{1},\\ldots,X_{n}$ from a Laplace distribution with location parameter $0$ and scale $b>0$, having density $f(x;b)=\\frac{1}{2b}\\exp\\!\\left(-\\frac{|x|}{b}\\right)$.\n\nTo find the maximum likelihood estimator for $b$, write the likelihood for the observed sample $x_{1},\\ldots,x_{n}$:\n$$\nL(b)=\\prod_{i=1}^{n}\\frac{1}{2b}\\exp\\!\\left(-\\frac{|x_{i}|}{b}\\right)=\\left(\\frac{1}{2b}\\right)^{n}\\exp\\!\\left(-\\frac{1}{b}\\sum_{i=1}^{n}|x_{i}|\\right),\n$$\nwith log-likelihood\n$$\n\\ell(b)=\\ln L(b)=-n\\ln(2b)-\\frac{1}{b}\\sum_{i=1}^{n}|x_{i}|.\n$$\nDifferentiate with respect to $b$:\n$$\n\\ell'(b)=-\\frac{n}{b}+\\frac{1}{b^{2}}\\sum_{i=1}^{n}|x_{i}|,\n$$\nand set $\\ell'(b)=0$ to obtain\n$$\n-\\frac{n}{b}+\\frac{1}{b^{2}}\\sum_{i=1}^{n}|x_{i}|=0 \\;\\;\\Longrightarrow\\;\\; -n b+\\sum_{i=1}^{n}|x_{i}|=0 \\;\\;\\Longrightarrow\\;\\; \\hat{b}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}|x_{i}|.\n$$\nTo verify this is a maximum, compute the second derivative\n$$\n\\ell''(b)=\\frac{n}{b^{2}}-\\frac{2}{b^{3}}\\sum_{i=1}^{n}|x_{i}|,\n$$\nand evaluate at $b=\\hat{b}_{n}$:\n$$\n\\ell''(\\hat{b}_{n})=\\frac{n}{\\hat{b}_{n}^{2}}-\\frac{2}{\\hat{b}_{n}^{3}}\\sum_{i=1}^{n}|x_{i}|=\\frac{n}{\\hat{b}_{n}^{2}}-\\frac{2n}{\\hat{b}_{n}^{2}}=-\\frac{n}{\\hat{b}_{n}^{2}}<0,\n$$\nso $\\hat{b}_{n}$ maximizes the log-likelihood (for the degenerate case $\\sum|x_{i}|=0$, the likelihood is maximized as $b\\to 0^{+}$; this event has probability zero under the model).\n\nNext, compute $E[|X|]$ for a single $X\\sim\\text{Laplace}(0,b)$:\n$$\nE[|X|]=\\int_{-\\infty}^{\\infty}|x|\\frac{1}{2b}\\exp\\!\\left(-\\frac{|x|}{b}\\right)\\,dx.\n$$\nBy symmetry,\n$$\nE[|X|]=\\frac{1}{b}\\int_{0}^{\\infty}x\\exp\\!\\left(-\\frac{x}{b}\\right)\\,dx.\n$$\nUse the substitution $u=\\frac{x}{b}$, so $x=bu$ and $dx=b\\,du$, to get\n$$\nE[|X|]=\\frac{1}{b}\\int_{0}^{\\infty}b u\\,\\exp(-u)\\,b\\,du=b\\int_{0}^{\\infty}u\\exp(-u)\\,du=b\\,\\Gamma(2)=b.\n$$\nTherefore, the required two-component result is the MLE $\\hat{b}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}|x_{i}|$ and the expectation $E[|X|]=b$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{n}\\sum_{i=1}^{n}|x_{i}| & b\\end{pmatrix}}$$", "id": "1895934"}, {"introduction": "Statistical modeling often involves simplifying assumptions, but what happens when our assumptions are not perfectly correct? This next problem investigates such a scenario, a concept known as model misspecification. By examining an estimator for a normal distribution's mean where the variance is misspecified, we explore the robustness of the MLE and gain deeper insight into what the estimator converges to, even when the underlying model is not perfectly accurate. [@problem_id:1895917]", "problem": "A statistician is analyzing a dataset consisting of observations $X_1, X_2, \\dots, X_n$, which constitutes a random sample from a normal distribution. The true distribution has an unknown mean $\\mu$ and a known variance $\\sigma_0^2$.\n\nDue to a misunderstanding, the statistician incorrectly specifies the model. They assume that the data is sampled from a normal distribution with a mean $\\theta$ to be estimated and a variance of exactly 1. Based on this incorrect assumption, the statistician constructs the likelihood function for the parameter $\\theta$ and finds its maximum likelihood estimator (MLE), which we denote as $\\hat{\\theta}_n$.\n\nDetermine the value to which this estimator $\\hat{\\theta}_n$ converges in probability as the sample size $n$ approaches infinity. Express your answer as a symbolic expression in terms of the true parameters of the distribution.", "solution": "Let the true data-generating process be $X_{1},\\dots,X_{n}$ i.i.d. $\\sim \\mathcal{N}(\\mu,\\sigma_{0}^{2})$ with unknown $\\mu$ and known $\\sigma_{0}^{2}$. The statistician incorrectly assumes the model $X_{i} \\sim \\mathcal{N}(\\theta,1)$ and forms the likelihood for $\\theta$:\n$$\nL_{n}(\\theta)=\\prod_{i=1}^{n}\\left[(2\\pi)^{-\\frac{1}{2}}\\exp\\!\\left(-\\frac{1}{2}(x_{i}-\\theta)^{2}\\right)\\right].\n$$\nThe log-likelihood is\n$$\n\\ell_{n}(\\theta)=\\ln L_{n}(\\theta)=-\\frac{n}{2}\\ln(2\\pi)-\\frac{1}{2}\\sum_{i=1}^{n}(x_{i}-\\theta)^{2}.\n$$\nDifferentiate with respect to $\\theta$ and set to zero to obtain the MLE:\n$$\n\\frac{\\partial}{\\partial\\theta}\\ell_{n}(\\theta)=-\\frac{1}{2}\\sum_{i=1}^{n}2(\\theta-x_{i})=-\\sum_{i=1}^{n}(\\theta-x_{i})=-n\\theta+\\sum_{i=1}^{n}x_{i}=0,\n$$\nwhich yields\n$$\n\\hat{\\theta}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}=\\bar{X}_{n}.\n$$\nUnder the true model, by the Weak Law of Large Numbers,\n$$\n\\bar{X}_{n}\\xrightarrow{p}\\mathbb{E}[X_{1}]=\\mu \\quad \\text{as } n\\to\\infty.\n$$\nTherefore, the MLE computed under the misspecified variance converges in probability to the true mean $\\mu$. Equivalently, from the misspecified M-estimation perspective, the pseudo-true parameter maximizes the expected log-likelihood\n$$\n\\mathbb{E}\\left[\\ell_{1}(\\theta)\\right]=-\\frac{1}{2}\\mathbb{E}\\left[(X_{1}-\\theta)^{2}\\right]+\\text{constant}=-\\frac{1}{2}\\left(\\sigma_{0}^{2}+(\\mu-\\theta)^{2}\\right)+\\text{constant},\n$$\nwhich is uniquely maximized at $\\theta=\\mu$, confirming the same limit.", "answer": "$$\\boxed{\\mu}$$", "id": "1895917"}, {"introduction": "The general theorems guaranteeing MLE consistency rely on a set of 'regularity conditions.' This final practice addresses the important question of what to do when those conditions are not met. Using the shifted exponential distribution, where the support depends on the parameter itself, you will find that even though standard theorems may not apply, we can prove consistency directly from its fundamental definition, providing a powerful tool for analyzing non-standard problems. [@problem_id:1895868]", "problem": "In reliability engineering, the lifetime of certain electronic components is modeled using a shifted exponential distribution. This distribution accounts for an initial \"burn-in\" period or guaranteed lifetime, $\\theta$, during which failure is impossible. Let the lifetimes of $n$ such components, represented by the random variables $X_1, X_2, \\dots, X_n$, be independent and identically distributed. Their probability density function (PDF) is given by:\n$$f(x; \\theta) = \\exp(-(x - \\theta)) \\quad \\text{for } x \\ge \\theta$$\nand $f(x; \\theta) = 0$ for $x < \\theta$. The parameter $\\theta > 0$ is an unknown constant representing the guaranteed lifetime.\n\nAn analyst obtains a sample of $n$ component lifetimes and wishes to estimate $\\theta$. First, determine the Maximum Likelihood Estimator (MLE) for $\\theta$, which we denote as $\\hat{\\theta}_n$. An estimator is said to be consistent if it converges in probability to the true parameter value as the sample size $n$ increases. For a consistent estimator, the probability of the estimate being more than any small, fixed distance $\\epsilon > 0$ away from the true parameter should approach zero as $n \\to \\infty$.\n\nYour task is to quantify this convergence for the shifted exponential model. Find a closed-form analytic expression for the probability $P(|\\hat{\\theta}_n - \\theta| > \\epsilon)$ in terms of the sample size $n$ and the positive constant $\\epsilon$.", "solution": "The joint likelihood for an observed sample $x_{1},\\dots,x_{n}$ is\n$$\nL(\\theta;x_{1},\\dots,x_{n})=\\prod_{i=1}^{n}f(x_{i};\\theta)\n=\\prod_{i=1}^{n}\\exp\\!\\big(-(x_{i}-\\theta)\\big)\\cdot \\mathbf{1}\\{x_{i}\\ge \\theta\\}\n=\\exp\\!\\Big(-\\sum_{i=1}^{n}x_{i}+n\\theta\\Big)\\cdot \\mathbf{1}\\{\\theta\\le x_{(1)}\\},\n$$\nwhere $x_{(1)}=\\min_{1\\le i\\le n}x_{i}$. The log-likelihood is\n$$\n\\ell(\\theta)=-\\sum_{i=1}^{n}x_{i}+n\\theta \\quad \\text{subject to} \\quad \\theta\\le x_{(1)}.\n$$\nSince $\\ell(\\theta)$ is strictly increasing in $\\theta$, the maximum over the feasible set is attained at the boundary, giving the MLE\n$$\n\\hat{\\theta}_{n}=x_{(1)}=\\min_{1\\le i\\le n}X_{i}.\n$$\n\nWrite $X_{i}=\\theta+Y_{i}$ with $Y_{i}\\stackrel{\\text{i.i.d.}}{\\sim}\\text{Exp}(1)$, so $\\hat{\\theta}_{n}-\\theta=\\min_{1\\le i\\le n}Y_{i}=Y_{(1)}$. For $y\\ge 0$,\n$$\n\\Pr(Y_{(1)}>y)=\\Pr(Y_{1}>y,\\dots,Y_{n}>y)=\\prod_{i=1}^{n}\\Pr(Y_{i}>y)=\\big(\\exp(-y)\\big)^{n}=\\exp(-ny),\n$$\nso $Y_{(1)}\\sim \\text{Exp}(n)$. Because $\\hat{\\theta}_{n}\\ge \\theta$, for any $\\epsilon>0$,\n$$\n\\Pr(|\\hat{\\theta}_{n}-\\theta|>\\epsilon)=\\Pr(\\hat{\\theta}_{n}-\\theta>\\epsilon)=\\Pr\\big(Y_{(1)}>\\epsilon\\big)=\\exp(-n\\epsilon).\n$$\nThis gives the required closed-form expression and shows the estimator is consistent since $\\exp(-n\\epsilon)\\to 0$ as $n\\to\\infty$ for any fixed $\\epsilon>0$.", "answer": "$$\\boxed{\\exp(-n\\epsilon)}$$", "id": "1895868"}]}