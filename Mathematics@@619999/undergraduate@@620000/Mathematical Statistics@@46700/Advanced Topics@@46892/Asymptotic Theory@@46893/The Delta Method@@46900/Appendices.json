{"hands_on_practices": [{"introduction": "To begin our exploration of the Delta Method, we will start with a classic application from physics. This exercise demonstrates how to find the approximate variance of an estimator that is a non-linear function of a sample mean. By calculating the asymptotic variance of a kinetic energy estimator, which depends on the square of the mean velocity, you will practice the fundamental mechanics of the one-dimensional Delta Method in a familiar and intuitive context [@problem_id:1959819].", "problem": "In a plasma physics experiment, a set of $n$ identical, non-interacting ions are confined in an electromagnetic trap. A measurement is taken of the velocity component, $X_i$, for each ion $i$ along a specific axis. The measurements $\\{X_1, X_2, \\dots, X_n\\}$ can be modeled as independent and identically distributed random variables from a distribution with mean $E[X_i] = v$ and variance $\\text{Var}(X_i) = \\sigma^2$. The mass of each ion is known to be $m$.\n\nTo estimate the kinetic energy associated with the mean velocity component, a researcher proposes the estimator $\\hat{K} = \\frac{1}{2}m\\bar{X}_n^2$, where $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ is the sample mean of the velocity measurements.\n\nDetermine the asymptotic variance of the estimator $\\hat{K}$. Express your answer in terms of $m, v, \\sigma^2,$ and $n$.", "solution": "We are given independent and identically distributed random variables $\\{X_{i}\\}_{i=1}^{n}$ with mean $E[X_{i}]=v$ and variance $\\text{Var}(X_{i})=\\sigma^{2}$. Define the sample mean $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. The estimator of interest is $\\hat{K}=\\frac{1}{2}m\\bar{X}_{n}^{2}$.\n\nBy the central limit theorem, \n$$\n\\sqrt{n}\\left(\\bar{X}_{n}-v\\right)\\xrightarrow{d}\\mathcal{N}\\left(0,\\sigma^{2}\\right),\n$$\nso that, asymptotically, $\\bar{X}_{n}$ is approximately normal with mean $v$ and variance $\\sigma^{2}/n$.\n\nLet $g(x)=\\frac{1}{2}mx^{2}$. Then $g'(x)=mx$. By the delta method,\n$$\n\\sqrt{n}\\left(g(\\bar{X}_{n})-g(v)\\right)\\xrightarrow{d}\\mathcal{N}\\left(0,\\left(g'(v)\\right)^{2}\\sigma^{2}\\right).\n$$\nTherefore, the asymptotic variance of $g(\\bar{X}_{n})=\\hat{K}$ is\n$$\n\\text{avar}(\\hat{K})\\approx\\frac{\\left(g'(v)\\right)^{2}\\sigma^{2}}{n}=\\frac{m^{2}v^{2}\\sigma^{2}}{n}.\n$$\nEquivalently, $\\text{Var}(\\hat{K})= \\frac{m^{2}v^{2}\\sigma^{2}}{n}+o\\!\\left(\\frac{1}{n}\\right)$. Note that if $v=0$, the first-order delta method yields zero; in that special case the variance is of smaller order, specifically $O\\!\\left(n^{-2}\\right)$. For $v\\neq 0$, the leading asymptotic variance is $\\frac{m^{2}v^{2}\\sigma^{2}}{n}$.", "answer": "$$\\boxed{\\frac{m^{2}v^{2}\\sigma^{2}}{n}}$$", "id": "1959819"}, {"introduction": "Having mastered a basic application, we now turn to a problem central to statistical practice itself: analyzing the properties of an estimator. This exercise asks you to derive the asymptotic distribution for the estimated variance of a Bernoulli process, a quantity written as $g(\\hat{p}) = \\hat{p}(1-\\hat{p})$. This is a powerful demonstration of how the Delta Method helps us understand the behavior of common statistics, revealing how uncertainty in an initial estimate (the sample proportion $\\hat{p}$) propagates to a function of that estimate [@problem_id:1959831].", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a sequence of independent and identically distributed random variables from a Bernoulli distribution with parameter $p$, where $0 < p < 1$. The sample proportion is defined as $\\hat{p} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$. This sample proportion is an unbiased estimator for the true proportion $p$.\n\nConsider the estimator for the variance of a single Bernoulli trial, defined as a function of the sample proportion: $V = g(\\hat{p}) = \\hat{p}(1-\\hat{p})$. The Central Limit Theorem implies that the distribution of $\\sqrt{n}(\\hat{p} - p)$ converges to a normal distribution as the sample size $n$ approaches infinity. As a consequence, the distribution of the centered and scaled estimator $\\sqrt{n}(V - p(1-p))$ also converges to a normal distribution with a mean of 0.\n\nDetermine the variance of this limiting normal distribution for $\\sqrt{n}(V - p(1-p))$. Express your answer as a closed-form analytic expression in terms of the parameter $p$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. Bernoulli$(p)$, with $0<p<1$. The sample proportion is $\\hat{p}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$, so by the Central Limit Theorem,\n$$\n\\sqrt{n}\\,(\\hat{p}-p)\\;\\xrightarrow{d}\\;N\\bigl(0,\\;p(1-p)\\bigr).\n$$\nDefine $V=g(\\hat{p})$ with $g(x)=x(1-x)$. We seek the limiting variance of $\\sqrt{n}\\bigl(g(\\hat{p})-g(p)\\bigr)$.\n\nCompute derivatives of $g$:\n$$\ng'(x)=1-2x,\\qquad g''(x)=-2.\n$$\nApply a second-order Taylor expansion of $g$ at $p$:\n$$\ng(\\hat{p})-g(p)=g'(p)(\\hat{p}-p)+\\frac{1}{2}g''(\\tilde{p})\\,(\\hat{p}-p)^{2},\n$$\nfor some $\\tilde{p}$ between $\\hat{p}$ and $p$. Multiply by $\\sqrt{n}$:\n$$\n\\sqrt{n}\\bigl(g(\\hat{p})-g(p)\\bigr)=g'(p)\\sqrt{n}\\,(\\hat{p}-p)+\\frac{1}{2}g''(\\tilde{p})\\,\\sqrt{n}\\,(\\hat{p}-p)^{2}.\n$$\nSince $\\sqrt{n}(\\hat{p}-p)=O_{p}(1)$, we have $\\sqrt{n}(\\hat{p}-p)^{2}=\\frac{1}{\\sqrt{n}}\\bigl(\\sqrt{n}(\\hat{p}-p)\\bigr)^{2}=o_{p}(1)$, and $g''(\\tilde{p})$ is bounded. Hence the second term is $o_{p}(1)$. By Slutskyâ€™s theorem and the delta method,\n$$\n\\sqrt{n}\\bigl(g(\\hat{p})-g(p)\\bigr)\\;\\xrightarrow{d}\\;N\\Bigl(0,\\;\\bigl(g'(p)\\bigr)^{2}\\,p(1-p)\\Bigr).\n$$\nSubstituting $g'(p)=1-2p$ yields the asymptotic variance\n$$\n\\bigl(1-2p\\bigr)^{2}\\,p(1-p).\n$$\nNote that at $p=\\frac{1}{2}$ this variance equals $0$, corresponding to a degenerate limit under the $\\sqrt{n}$ scaling.", "answer": "$$\\boxed{(1-2p)^{2}p(1-p)}$$", "id": "1959831"}, {"introduction": "Many real-world estimation problems involve functions of multiple variables, requiring an extension of our tools to a multivariate setting. This practice introduces the multivariate Delta Method through the tangible problem of determining a beacon's distance from noisy coordinate measurements. By working with a function of two sample means, $D_n = \\sqrt{\\bar{X}_n^2 + \\bar{Y}_n^2}$, you will learn how to incorporate gradients and covariance matrices to find the asymptotic variance, a crucial skill for tackling more complex, multi-dimensional statistical models [@problem_id:1396672].", "problem": "A stationary beacon is located at true Cartesian coordinates $(x_0, y_0)$ in a 2D plane. A mobile robot takes $n$ measurements of the beacon's position. The $i$-th measurement is a random vector $(X_i, Y_i)$. These $n$ measurements are independent and identically distributed (i.i.d.) with the following statistical properties:\n- Mean: $E[X_i] = x_0$ and $E[Y_i] = y_0$.\n- Variance: $\\text{Var}(X_i) = \\sigma^2$ and $\\text{Var}(Y_i) = \\sigma^2$.\n- Covariance: $\\text{Cov}(X_i, Y_i) = \\rho \\sigma^2$, where $\\rho$ is the correlation coefficient between the measurement errors in the x and y coordinates.\n\nThe robot computes the sample means $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$ and $\\bar{Y}_n = \\frac{1}{n} \\sum_{i=1}^n Y_i$. It then estimates the distance of the beacon from the origin using the estimator $D_n = \\sqrt{\\bar{X}_n^2 + \\bar{Y}_n^2}$.\n\nFor large $n$, the distribution of $D_n$ can be approximated by a normal distribution. Find an expression for the variance of this approximate normal distribution. Your answer should be in terms of $x_0, y_0, \\sigma, \\rho$, and $n$. Assume that $(x_0, y_0) \\neq (0,0)$.", "solution": "We seek the large-sample (asymptotic) variance of the estimator $D_{n}=\\sqrt{\\bar{X}_{n}^{2}+\\bar{Y}_{n}^{2}}$, where $(\\bar{X}_{n},\\bar{Y}_{n})$ are the sample means of i.i.d. observations with mean vector $(x_{0},y_{0})$ and covariance matrix\n$$\n\\Sigma=\\sigma^{2}\\begin{pmatrix}1 & \\rho \\\\ \\rho & 1\\end{pmatrix}.\n$$\nBy the multivariate central limit theorem,\n$$\n\\sqrt{n}\\left(\\begin{pmatrix}\\bar{X}_{n} \\\\ \\bar{Y}_{n}\\end{pmatrix}-\\begin{pmatrix}x_{0} \\\\ y_{0}\\end{pmatrix}\\right)\\;\\xrightarrow{d}\\;\\mathcal{N}\\!\\left(\\begin{pmatrix}0 \\\\ 0\\end{pmatrix},\\Sigma\\right).\n$$\nDefine $g:\\mathbb{R}^{2}\\to\\mathbb{R}$ by $g(x,y)=\\sqrt{x^{2}+y^{2}}$. Since $(x_{0},y_{0})\\neq(0,0)$, $g$ is differentiable at $(x_{0},y_{0})$ with gradient\n$$\n\\nabla g(x_{0},y_{0})=\\begin{pmatrix}\\dfrac{x_{0}}{\\sqrt{x_{0}^{2}+y_{0}^{2}}} \\\\ \\dfrac{y_{0}}{\\sqrt{x_{0}^{2}+y_{0}^{2}}}\\end{pmatrix}.\n$$\nBy the delta method,\n$$\n\\sqrt{n}\\left(g(\\bar{X}_{n},\\bar{Y}_{n})-g(x_{0},y_{0})\\right)\\;\\xrightarrow{d}\\;\\mathcal{N}\\!\\left(0,\\nabla g(x_{0},y_{0})^{\\top}\\Sigma\\,\\nabla g(x_{0},y_{0})\\right).\n$$\nTherefore the asymptotic variance of $D_{n}=g(\\bar{X}_{n},\\bar{Y}_{n})$ is\n$$\n\\frac{1}{n}\\,\\nabla g(x_{0},y_{0})^{\\top}\\Sigma\\,\\nabla g(x_{0},y_{0}).\n$$\nLet $r_{0}=\\sqrt{x_{0}^{2}+y_{0}^{2}}$. Then\n$$\n\\nabla g(x_{0},y_{0})^{\\top}\\Sigma\\,\\nabla g(x_{0},y_{0})\n=\\frac{\\sigma^{2}}{r_{0}^{2}}\\begin{pmatrix}x_{0} & y_{0}\\end{pmatrix}\n\\begin{pmatrix}1 & \\rho \\\\ \\rho & 1\\end{pmatrix}\n\\begin{pmatrix}x_{0} \\\\ y_{0}\\end{pmatrix}\n=\\frac{\\sigma^{2}}{r_{0}^{2}}\\left(x_{0}^{2}+2\\rho x_{0}y_{0}+y_{0}^{2}\\right).\n$$\nSubstituting $r_{0}^{2}=x_{0}^{2}+y_{0}^{2}$ gives the asymptotic variance\n$$\n\\operatorname{Var}(D_{n})\\approx \\frac{\\sigma^{2}}{n}\\,\\frac{x_{0}^{2}+2\\rho x_{0}y_{0}+y_{0}^{2}}{x_{0}^{2}+y_{0}^{2}}.\n$$\nThis is the variance of the approximate normal distribution of $D_{n}$ for large $n$.", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{n}\\,\\frac{x_{0}^{2}+2\\rho x_{0}y_{0}+y_{0}^{2}}{x_{0}^{2}+y_{0}^{2}}}$$", "id": "1396672"}]}