{"hands_on_practices": [{"introduction": "This first exercise serves as a direct and fundamental application of Slutsky's theorem. It explores what happens when we combine a sequence of random variables that converges in distribution with another that converges in probability to a constant. This practice solidifies the core principle that a consistent estimator can often be treated as its true value in large-sample calculations, a simplifying yet powerful concept in asymptotic theory [@problem_id:1955717].", "problem": "In statistical inference, the properties of estimators and test statistics as the sample size $n$ grows to infinity are of fundamental interest. A sequence of estimators $\\{\\hat{\\theta}_n\\}_{n=1}^{\\infty}$ for a parameter $\\theta$ is defined as *consistent* if it converges in probability to $\\theta$. This is written as $\\hat{\\theta}_n \\to_p \\theta$ and formally means that for any arbitrary small positive number $\\epsilon$, the probability $P(|\\hat{\\theta}_n - \\theta| \\ge \\epsilon)$ approaches zero as $n \\to \\infty$.\n\nLet $\\{Z_n\\}_{n=1}^{\\infty}$ be a sequence of random variables that is known to converge in distribution to a standard normal random variable, $Z \\sim N(0, 1)$. This is denoted as $Z_n \\to_d Z$.\n\nNow, suppose you have a consistent estimator $\\hat{\\theta}_n$ for a parameter $\\theta$, where $\\theta$ is a fixed, non-zero real number. Consider a new sequence of random variables $\\{T_n\\}_{n=1}^{\\infty}$ constructed as the ratio:\n$$T_n = \\frac{Z_n}{\\hat{\\theta}_n}$$\nDetermine the limiting distribution of $T_n$ as $n \\to \\infty$. Express your answer using standard notation for statistical distributions, for example, $N(\\mu, \\sigma^2)$ for a normal distribution with mean $\\mu$ and variance $\\sigma^2$.", "solution": "We are given two convergences: $Z_{n} \\to_{d} Z$ where $Z \\sim N(0,1)$, and $\\hat{\\theta}_{n} \\to_{p} \\theta$ where $\\theta \\in \\mathbb{R}\\setminus\\{0\\}$. Define $T_{n} = Z_{n}/\\hat{\\theta}_{n}$. To find the limiting distribution of $T_{n}$, apply Slutsky’s theorem (or the continuous mapping theorem in its two-sequence form).\n\nFirst, since $\\hat{\\theta}_{n} \\to_{p} \\theta$ with $\\theta \\neq 0$, the function $f(x,y) = x/y$ is continuous at all points with $y \\neq 0$, in particular at $(x,\\theta)$ for any $x \\in \\mathbb{R}$. Slutsky’s theorem states that if $X_{n} \\to_{d} X$ and $Y_{n} \\to_{p} c$ for a constant $c$, then $X_{n}/Y_{n} \\to_{d} X/c$. Taking $X_{n} = Z_{n}$, $Y_{n} = \\hat{\\theta}_{n}$, and $c = \\theta$, we obtain\n$$\nT_{n} \\;=\\; \\frac{Z_{n}}{\\hat{\\theta}_{n}} \\;\\to_{d}\\; \\frac{Z}{\\theta}.\n$$\nSince $Z \\sim N(0,1)$ and for any constant $a$ we have $aZ \\sim N(0, a^{2})$, it follows that\n$$\n\\frac{Z}{\\theta} \\;\\sim\\; N\\!\\left(0,\\;\\frac{1}{\\theta^{2}}\\right).\n$$\nTherefore, the limiting distribution of $T_{n}$ is $N\\!\\left(0, \\theta^{-2}\\right)$.", "answer": "$$\\boxed{N\\!\\left(0,\\,\\theta^{-2}\\right)}$$", "id": "1955717"}, {"introduction": "Building on the foundational concept, this problem demonstrates how Slutsky’s theorem acts as a vital bridge between two other pillars of statistical theory: the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT). You will analyze a statistic formed by the product of two sequences, one converging to a constant via the LLN and the other converging to a normal distribution via the CLT. This pattern is exceptionally common in statistical inference and is key to understanding the behavior of many estimators and test statistics [@problem_id:1955718].", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed random variables from a Bernoulli distribution with parameter $p$, where $0 < p < 1$. The parameter $p$ represents the probability of success, i.e., $P(X_i = 1) = p$ and $P(X_i = 0) = 1-p$.\n\nLet $\\hat{p}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ be the sample proportion of successes in the first $n$ trials.\n\nConsider the new random variable $Y_n = (1-\\hat{p}_n)\\sqrt{n}(\\hat{p}_n - p)$. Which of the following describes the limiting distribution of $Y_n$ as $n \\to \\infty$?\n\nA. A Normal distribution with mean $0$ and variance $p(1-p)$.\n\nB. A Normal distribution with mean $0$ and variance $p^2(1-p)^2$.\n\nC. A Normal distribution with mean $0$ and variance $p(1-p)^2$.\n\nD. A Normal distribution with mean $0$ and variance $p(1-p)^3$.\n\nE. The sequence of random variables $Y_n$ does not converge in distribution to a Normal distribution.", "solution": "We have $X_{i} \\sim \\text{Bernoulli}(p)$ i.i.d., so $E[X_{i}] = p$ and $\\operatorname{Var}(X_{i}) = p(1-p)$. The sample proportion $\\hat{p}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ satisfies the law of large numbers:\n$$\n\\hat{p}_{n} \\xrightarrow{p} p \\quad \\text{as } n \\to \\infty,\n$$\nwhich implies\n$$\n1 - \\hat{p}_{n} \\xrightarrow{p} 1 - p.\n$$\n\nBy the central limit theorem for i.i.d. Bernoulli variables,\n$$\n\\sqrt{n}\\,(\\hat{p}_{n} - p) \\xrightarrow{d} Z \\quad \\text{with } Z \\sim \\mathcal{N}\\bigl(0,\\,p(1-p)\\bigr).\n$$\n\nDefine $Y_{n} = (1 - \\hat{p}_{n}) \\sqrt{n} (\\hat{p}_{n} - p)$. Since $(1 - \\hat{p}_{n}) \\xrightarrow{p} (1 - p)$ and $\\sqrt{n}(\\hat{p}_{n} - p) \\xrightarrow{d} Z$, by Slutsky's theorem,\n$$\nY_{n} \\xrightarrow{d} (1 - p)\\,Z.\n$$\nIf $Z \\sim \\mathcal{N}(0, \\sigma^{2})$, then for any constant $c$, $cZ \\sim \\mathcal{N}(0, c^{2}\\sigma^{2})$. Applying this with $c = 1 - p$ and $\\sigma^{2} = p(1-p)$, we conclude\n$$\n(1 - p)\\,Z \\sim \\mathcal{N}\\bigl(0,\\,(1 - p)^{2}\\,p(1-p)\\bigr) = \\mathcal{N}\\bigl(0,\\,p(1-p)^{3}\\bigr).\n$$\n\nTherefore, the limiting distribution of $Y_{n}$ is Normal with mean $0$ and variance $p(1-p)^{3}$, which corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1955718"}, {"introduction": "This final practice illustrates one of the most significant practical applications of Slutsky's theorem: the process of \"studentization.\" In real-world statistics, population variances are rarely known and must be estimated from data. This exercise shows why we can substitute a consistent estimator for the true variance in a test statistic and still obtain a standard, pivotal limiting distribution. Understanding this process is essential for justifying the use of many common hypothesis tests, such as the large-sample t-test, and for constructing confidence intervals [@problem_id:840199].", "problem": "Consider two independent sequences of independent and identically distributed (i.i.d.) random variables, $\\{X_i\\}_{i=1}^n$ and $\\{Y_i\\}_{i=1}^n$. The random variables $X_i$ have a population mean $E[X_i] = \\mu_X$ and a finite population variance $Var(X_i) = \\sigma_X^2$. The random variables $Y_i$ have a population mean $E[Y_i] = \\mu_Y \\neq 0$ and a finite population variance $Var(Y_i) = \\sigma_Y^2$.\n\nLet $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$ and $\\bar{Y}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$ be the respective sample means. Let $S_{X,n}^2$ and $S_{Y,n}^2$ be consistent estimators for the population variances $\\sigma_X^2$ and $\\sigma_Y^2$, respectively.\n\nWe are interested in the ratio of the sample means, $R_n = \\frac{\\bar{X}_n}{\\bar{Y}_n}$, which is an estimator for the true ratio of the means, $\\rho = \\frac{\\mu_X}{\\mu_Y}$. A test statistic for $\\rho$ can be constructed as follows:\n$$ T_n = \\frac{\\sqrt{n}(R_n - \\rho)}{\\sqrt{\\hat{V}_n}} $$\nwhere $\\hat{V}_n$ is a consistent estimator for the asymptotic variance of $\\sqrt{n}(R_n - \\rho)$, given by:\n$$ \\hat{V}_n = \\frac{S_{X,n}^2 \\bar{Y}_n^2 + S_{Y,n}^2 \\bar{X}_n^2}{\\bar{Y}_n^4} $$\n\nDerive the variance of the limiting distribution of the statistic $T_n$ as $n \\to \\infty$.", "solution": "1. By the multivariate central limit theorem,\n$$\n\\sqrt{n}\\Bigl(\\begin{pmatrix}\\bar X_n\\\\\\bar Y_n\\end{pmatrix}-\\begin{pmatrix}\\mu_X\\\\\\mu_Y\\end{pmatrix}\\Bigr)\n\\xrightarrow{d}N\\!\\Bigl(0,\\begin{pmatrix}\\sigma_X^2&0\\\\0&\\sigma_Y^2\\end{pmatrix}\\Bigr).\n$$\n\n2. Apply the delta method to $g(x,y)=x/y$.  The gradient at $(\\mu_X,\\mu_Y)$ is\n$$\n\\nabla g(\\mu_X,\\mu_Y)\n=\\begin{pmatrix}1/\\mu_Y\\\\-\\,\\mu_X/\\mu_Y^2\\end{pmatrix}.\n$$\nHence the asymptotic variance of $\\sqrt{n}(R_n-\\rho)$ is\n$$\nV\n=\\nabla g(\\mu_X,\\mu_Y)^T\n\\begin{pmatrix}\\sigma_X^2&0\\\\0&\\sigma_Y^2\\end{pmatrix}\n\\nabla g(\\mu_X,\\mu_Y)\n=\\frac{\\sigma_X^2}{\\mu_Y^2}+\\frac{\\sigma_Y^2\\mu_X^2}{\\mu_Y^4}\n=\\frac{\\sigma_X^2\\mu_Y^2+\\sigma_Y^2\\mu_X^2}{\\mu_Y^4}.\n$$\n\n3. Since $\\hat V_n\\to_p V$, by Slutsky’s theorem\n$$\nT_n=\\frac{\\sqrt{n}(R_n-\\rho)}{\\sqrt{\\hat V_n}}\\xrightarrow{d}N(0,1),\n$$\nso the limiting variance is $1$.", "answer": "$$\\boxed{1}$$", "id": "840199"}]}