{"hands_on_practices": [{"introduction": "To build a solid understanding of Fisher information, we begin with a foundational exercise. This problem asks you to calculate the expected information for a parameter $\\alpha$ in a simplified Beta distribution [@problem_id:1941172]. Mastering this calculation will help you internalize the standard procedure: finding the log-likelihood, taking its second derivative, and then computing its expectation.", "problem": "In statistical inference, the Fisher information quantifies the amount of information that an observable random variable carries about an unknown parameter of a distribution that models the random variable. Consider a single random sample $X$ drawn from a Beta distribution, whose probability density function (PDF) is given by\n$$ f(x; \\alpha, \\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)} $$\nfor $x \\in (0, 1)$, where $\\alpha > 0$ and $\\beta > 0$ are the shape parameters, and $B(\\alpha, \\beta)$ is the Beta function.\n\nIn this specific problem, we are investigating a simplified model where the parameter $\\beta$ is fixed at $\\beta=1$. The parameter of interest is $\\alpha$. You are given that the Beta function for $\\beta=1$ simplifies to $B(\\alpha, 1) = \\frac{1}{\\alpha}$.\n\nDetermine the Fisher information, $I(\\alpha)$, for the parameter $\\alpha$ based on this single observation $X$. Your answer should be a closed-form analytic expression in terms of $\\alpha$.", "solution": "For $\\beta=1$, the Beta density simplifies using $B(\\alpha,1)=\\frac{1}{\\alpha}$ to\n$$\nf(x;\\alpha)=\\frac{x^{\\alpha-1}}{B(\\alpha,1)}=\\alpha x^{\\alpha-1}, \\quad 0<x<1,\\ \\alpha>0.\n$$\nFor a single observation $X=x$, the log-likelihood is\n$$\n\\ell(\\alpha;x)=\\ln f(x;\\alpha)=\\ln \\alpha + (\\alpha-1)\\ln x.\n$$\nThe score function (first derivative with respect to $\\alpha$) is\n$$\ns(\\alpha;x)=\\frac{\\partial \\ell(\\alpha;x)}{\\partial \\alpha}=\\frac{1}{\\alpha}+\\ln x.\n$$\nThe second derivative is\n$$\n\\frac{\\partial^{2} \\ell(\\alpha;x)}{\\partial \\alpha^{2}}=-\\frac{1}{\\alpha^{2}}.\n$$\nThe Fisher information based on a single observation is defined by\n$$\nI(\\alpha)=-\\mathbb{E}_{\\alpha}\\!\\left[\\frac{\\partial^{2} \\ell(\\alpha;X)}{\\partial \\alpha^{2}}\\right].\n$$\nSince $\\frac{\\partial^{2} \\ell(\\alpha;X)}{\\partial \\alpha^{2}}=-\\frac{1}{\\alpha^{2}}$ does not depend on $X$, its expectation equals itself, hence\n$$\nI(\\alpha)=-\\left(-\\frac{1}{\\alpha^{2}}\\right)=\\frac{1}{\\alpha^{2}}.\n$$\nThis yields the closed-form Fisher information for $\\alpha$.", "answer": "$$\\boxed{\\frac{1}{\\alpha^{2}}}$$", "id": "1941172"}, {"introduction": "This next practice moves from rote calculation to deeper conceptual insight by presenting a hypothetical scenario where detailed data from a Poisson process is reduced to a simple binary outcome [@problem_id:1941213]. By calculating and comparing the Fisher information before and after this data simplification, you will directly quantify \"information loss.\" This provides an intuitive grasp of how data processing choices impact our ability to make precise inferences about a parameter like $\\lambda$.", "problem": "A sensitive particle detector is designed to monitor a rare physical process. The number of particles, $X$, detected in a fixed time interval is modeled by a Poisson distribution with an unknown mean parameter $\\lambda > 0$. The probability mass function is given by $P(X=k) = \\frac{\\exp(-\\lambda)\\lambda^k}{k!}$ for $k=0, 1, 2, \\dots$.\n\nDue to a technical limitation, the detector can only report a binary outcome. It reports $Y=0$ if no particles are detected ($X=0$) and $Y=1$ if at least one particle is detected ($X>0$). This transformation from the detailed count $X$ to the binary outcome $Y$ results in a potential loss of statistical information about the parameter $\\lambda$.\n\nLet $I_X(\\lambda)$ be the Fisher information for $\\lambda$ contained in a single direct observation of the random variable $X$. Similarly, let $I_Y(\\lambda)$ be the Fisher information for $\\lambda$ contained in a single observation of the binary random variable $Y$.\n\nCalculate the ratio $\\frac{I_Y(\\lambda)}{I_X(\\lambda)}$. This ratio quantifies the fraction of information about $\\lambda$ that is retained by the binary detector compared to the full-count detector. Express your answer as a closed-form analytic expression in terms of $\\lambda$.", "solution": "We begin with the definition of Fisher information for a single observation. For a parametric family with density or mass function $f(x;\\lambda)$ and log-likelihood $\\ell(\\lambda;x)=\\ln f(x;\\lambda)$, the Fisher information is\n$$\nI(\\lambda)=\\operatorname{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda;X)\\right)^{2}\\right]\n=-\\operatorname{E}\\!\\left[\\frac{\\partial^{2}}{\\partial \\lambda^{2}}\\ell(\\lambda;X)\\right].\n$$\n\nFirst compute $I_{X}(\\lambda)$ for a single Poisson observation $X\\sim \\operatorname{Poisson}(\\lambda)$ with probability mass function $P(X=k)=\\exp(-\\lambda)\\lambda^{k}/k!$. The log-likelihood for observing $X=k$ is\n$$\n\\ell_{X}(\\lambda;k)= -\\lambda + k\\ln \\lambda - \\ln(k!).\n$$\nThe score is\n$$\n\\frac{\\partial}{\\partial \\lambda}\\ell_{X}(\\lambda;k) = -1 + \\frac{k}{\\lambda},\n$$\nand the second derivative is\n$$\n\\frac{\\partial^{2}}{\\partial \\lambda^{2}}\\ell_{X}(\\lambda;k) = -\\frac{k}{\\lambda^{2}}.\n$$\nTherefore,\n$$\nI_{X}(\\lambda)=-\\operatorname{E}\\!\\left[\\frac{\\partial^{2}}{\\partial \\lambda^{2}}\\ell_{X}(\\lambda;X)\\right]\n=-\\operatorname{E}\\!\\left[-\\frac{X}{\\lambda^{2}}\\right]\n=\\frac{\\operatorname{E}[X]}{\\lambda^{2}}=\\frac{\\lambda}{\\lambda^{2}}=\\frac{1}{\\lambda}.\n$$\n\nNext compute $I_{Y}(\\lambda)$ for the binary observation $Y=\\mathbf{1}\\{X>0\\}$. Define $p(\\lambda)=P(Y=1)=1-\\exp(-\\lambda)$, so $Y\\sim \\operatorname{Bernoulli}(p(\\lambda))$. The log-likelihood for observing $Y=y$ is\n$$\n\\ell_{Y}(\\lambda;y)= y\\ln p(\\lambda) + (1-y)\\ln\\bigl(1-p(\\lambda)\\bigr).\n$$\nDifferentiate with respect to $\\lambda$:\n$$\n\\frac{\\partial}{\\partial \\lambda}\\ell_{Y}(\\lambda;y)\n= y\\frac{p'(\\lambda)}{p(\\lambda)} - (1-y)\\frac{p'(\\lambda)}{1-p(\\lambda)}\n= p'(\\lambda)\\,\\frac{y-p(\\lambda)}{p(\\lambda)\\bigl(1-p(\\lambda)\\bigr)}.\n$$\nTaking the expectation of its square yields the Fisher information\n$$\nI_{Y}(\\lambda)=\\operatorname{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\lambda}\\ell_{Y}(\\lambda;Y)\\right)^{2}\\right]\n=\\frac{\\bigl(p'(\\lambda)\\bigr)^{2}}{p(\\lambda)\\bigl(1-p(\\lambda)\\bigr)},\n$$\nsince $\\operatorname{E}\\bigl[(Y-p(\\lambda))^{2}\\bigr]=\\operatorname{Var}(Y)=p(\\lambda)\\bigl(1-p(\\lambda)\\bigr)$. For $p(\\lambda)=1-\\exp(-\\lambda)$ we have $p'(\\lambda)=\\exp(-\\lambda)$ and $1-p(\\lambda)=\\exp(-\\lambda)$, so\n$$\nI_{Y}(\\lambda)=\\frac{\\exp(-2\\lambda)}{\\bigl(1-\\exp(-\\lambda)\\bigr)\\exp(-\\lambda)}=\\frac{\\exp(-\\lambda)}{1-\\exp(-\\lambda)}=\\frac{1}{\\exp(\\lambda)-1}.\n$$\n\nFinally, the ratio of Fisher informations is\n$$\n\\frac{I_{Y}(\\lambda)}{I_{X}(\\lambda)}=\\frac{\\dfrac{1}{\\exp(\\lambda)-1}}{\\dfrac{1}{\\lambda}}=\\frac{\\lambda}{\\exp(\\lambda)-1}.\n$$\nThis is a closed-form expression in terms of $\\lambda$.", "answer": "$$\\boxed{\\frac{\\lambda}{\\exp(\\lambda)-1}}$$", "id": "1941213"}, {"introduction": "Our final practice extends the concept of Fisher information into the multivariate domain, tackling a common question in data analysis: how precisely can we estimate the relationship between two variables? This exercise challenges you to compute the information for the correlation coefficient $\\rho$ in a standard bivariate normal distribution [@problem_id:1941226]. Successfully solving it demonstrates how this powerful tool applies not just to parameters of a single variable, but to the very structure of dependencies within a model.", "problem": "A biostatistician is modeling the relationship between two standardized biological markers, represented by random variables $X$ and $Y$. The joint behavior of these markers is described by a standard bivariate normal distribution, where the means are $\\mu_X = \\mu_Y = 0$ and the variances are $\\sigma_X^2 = \\sigma_Y^2 = 1$. The only unknown parameter is the correlation coefficient $\\rho$, where $-1 < \\rho < 1$.\n\nThe Probability Density Function (PDF) for a single observation $(x, y)$ from this distribution is given by:\n$$f(x, y; \\rho) = \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\exp\\left( -\\frac{1}{2(1-\\rho^2)} (x^2 - 2\\rho xy + y^2) \\right)$$\n\nYour task is to determine the theoretical amount of information a single observation $(X, Y)$ carries about the correlation parameter $\\rho$. Specifically, calculate the Fisher information, $I(\\rho)$, for the parameter $\\rho$.", "solution": "We model $(X,Y)$ as zero-mean Gaussian with covariance matrix $\\Sigma(\\rho)$ given by\n$$\n\\Sigma(\\rho)=\\begin{pmatrix}1 & \\rho \\\\ \\rho & 1\\end{pmatrix}, \\qquad -1<\\rho<1,\n$$\nso the parameter $\\rho$ appears only in $\\Sigma(\\rho)$. For a zero-mean multivariate normal with covariance depending on a scalar parameter, the Fisher information for that parameter is\n$$\nI(\\rho)=\\frac{1}{2}\\operatorname{tr}\\!\\left(\\Sigma(\\rho)^{-1}\\frac{\\partial \\Sigma(\\rho)}{\\partial \\rho}\\,\\Sigma(\\rho)^{-1}\\frac{\\partial \\Sigma(\\rho)}{\\partial \\rho}\\right).\n$$\nWe compute the required matrices explicitly. First,\n$$\n\\frac{\\partial \\Sigma(\\rho)}{\\partial \\rho}=\\begin{pmatrix}0 & 1 \\\\ 1 & 0\\end{pmatrix}\\equiv J,\n$$\nand\n$$\n\\Sigma(\\rho)^{-1}=\\frac{1}{1-\\rho^{2}}\\begin{pmatrix}1 & -\\rho \\\\ -\\rho & 1\\end{pmatrix}.\n$$\nDefine $A=\\Sigma(\\rho)^{-1}J$. Then\n$$\nA=\\frac{1}{1-\\rho^{2}}\\begin{pmatrix}1 & -\\rho \\\\ -\\rho & 1\\end{pmatrix}\\begin{pmatrix}0 & 1 \\\\ 1 & 0\\end{pmatrix}\n=\\frac{1}{1-\\rho^{2}}\\begin{pmatrix}-\\rho & 1 \\\\ 1 & -\\rho\\end{pmatrix}.\n$$\nHence\n$$\nA^{2}=\\frac{1}{(1-\\rho^{2})^{2}}\\begin{pmatrix}-\\rho & 1 \\\\ 1 & -\\rho\\end{pmatrix}^{2}\n=\\frac{1}{(1-\\rho^{2})^{2}}\\begin{pmatrix}1+\\rho^{2} & -2\\rho \\\\ -2\\rho & 1+\\rho^{2}\\end{pmatrix}.\n$$\nTaking the trace and applying the Fisher information formula,\n$$\nI(\\rho)=\\frac{1}{2}\\operatorname{tr}(A^{2})\n=\\frac{1}{2}\\cdot \\frac{2(1+\\rho^{2})}{(1-\\rho^{2})^{2}}\n=\\frac{1+\\rho^{2}}{(1-\\rho^{2})^{2}}.\n$$\nThis is the Fisher information in one observation $(X,Y)$ about the correlation parameter $\\rho$.", "answer": "$$\\boxed{\\frac{1+\\rho^{2}}{(1-\\rho^{2})^{2}}}$$", "id": "1941226"}]}