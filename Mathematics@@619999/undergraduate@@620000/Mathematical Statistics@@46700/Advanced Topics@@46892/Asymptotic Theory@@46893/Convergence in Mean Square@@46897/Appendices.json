{"hands_on_practices": [{"introduction": "To truly grasp convergence in mean square, we begin with a foundational exercise. This problem requires a direct application of the definition, providing essential practice in calculating the mean squared error and evaluating its limit. By working through the bias-variance decomposition for a general sequence, you will build the core skills needed to analyze more complex scenarios [@problem_id:1910454].", "problem": "Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$. The statistical properties of the $n$-th variable in the sequence are defined by its mean and variance, given by the expressions:\n$$E[X_n] = \\mu + \\frac{(-1)^n \\alpha}{n}$$\n$$Var(X_n) = \\frac{\\sigma^2}{n^2}$$\nwhere $\\mu$, $\\alpha$, and $\\sigma$ are given positive real constants.\n\nDetermine if the sequence $X_n$ converges in mean square to a constant. If it converges, what is the value of this constant? Your answer should be an expression written in terms of the given parameters.", "solution": "Convergence in mean square to a constant $c$ means $\\lim_{n \\to \\infty} E[(X_{n}-c)^{2}] = 0$. For any random variable $X$ and constant $c$, the bias-variance decomposition gives\n$$\nE[(X-c)^{2}] = \\operatorname{Var}(X) + \\left(E[X] - c\\right)^{2}.\n$$\nApplying this to $X_{n}$ with the given moments yields\n$$\nE[(X_{n}-c)^{2}] = \\frac{\\sigma^{2}}{n^{2}} + \\left(\\mu + \\frac{(-1)^{n}\\alpha}{n} - c\\right)^{2}.\n$$\nExpanding the square,\n$$\n\\left(\\mu + \\frac{(-1)^{n}\\alpha}{n} - c\\right)^{2} = (\\mu - c)^{2} + 2(\\mu - c)\\frac{(-1)^{n}\\alpha}{n} + \\frac{\\alpha^{2}}{n^{2}}.\n$$\nTherefore,\n$$\nE[(X_{n}-c)^{2}] = (\\mu - c)^{2} + 2(\\mu - c)\\frac{(-1)^{n}\\alpha}{n} + \\frac{\\alpha^{2} + \\sigma^{2}}{n^{2}}.\n$$\nTaking the limit as $n \\to \\infty$,\n$$\n\\lim_{n \\to \\infty} E[(X_{n}-c)^{2}] = (\\mu - c)^{2},\n$$\nsince the terms proportional to $\\frac{1}{n}$ and $\\frac{1}{n^{2}}$ vanish. For mean square convergence to a constant, this limit must equal $0$, which requires $(\\mu - c)^{2} = 0$, hence $c = \\mu$. Substituting $c = \\mu$ back gives\n$$\nE[(X_{n}-\\mu)^{2}] = \\frac{\\sigma^{2}}{n^{2}} + \\left(\\frac{(-1)^n \\alpha}{n}\\right)^2 = \\frac{\\sigma^2 + \\alpha^2}{n^2}.\n$$\nThe limit as $n \\to \\infty$ of this expression is $0$, so $X_{n}$ converges in mean square to $\\mu$.", "answer": "$$\\boxed{\\mu}$$", "id": "1910454"}, {"introduction": "Our next practice explores a more subtle and counterintuitive aspect of mean square convergence. Can a sequence of random variables converge to zero if it has a chance, however small, of taking on an unboundedly large value? This thought-provoking problem [@problem_id:1910471] demonstrates that it can, teaching us that convergence depends on how quickly the probability of these large values diminishes.", "problem": "Consider a sequence of independent, discrete random variables $\\{X_n\\}_{n=1}^{\\infty}$. For each integer $n \\ge 1$, the probability mass function for $X_n$ is defined as follows:\n$$\nP(X_n = n^2) = \\frac{1}{n^5}\n$$\n$$\nP(X_n = 0) = 1 - \\frac{1}{n^5}\n$$\nWe wish to determine if the sequence $X_n$ converges in mean square to the constant random variable $X=0$. Which of the following statements is correct?\n\nA. Yes, the sequence converges in mean square to 0 because the probability $P(X_n \\neq 0)$ approaches 0 as $n \\to \\infty$.\n\nB. Yes, the sequence converges in mean square to 0 because the limit of the mean squared error, $\\lim_{n \\to \\infty} E[(X_n - 0)^2]$, is 0.\n\nC. No, the sequence does not converge in mean square to 0 because one of the possible values for $X_n$, namely $n^2$, grows infinitely large as $n \\to \\infty$.\n\nD. No, the sequence does not converge in mean square to 0 because the expected value $E[X_n]$ does not converge to 0.", "solution": "Mean-square convergence to a constant $X=0$ is defined by\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}\\big[(X_{n}-0)^{2}\\big]=0.\n$$\nUsing the law of the unconscious statistician for a discrete random variable,\n$$\n\\mathbb{E}[X_{n}^{2}]=\\sum_{x} x^{2}\\, \\mathbb{P}(X_{n}=x).\n$$\nGiven $\\mathbb{P}(X_{n}=n^{2})=\\frac{1}{n^{5}}$ and $\\mathbb{P}(X_{n}=0)=1-\\frac{1}{n^{5}}$, we compute\n$$\n\\mathbb{E}[X_{n}^{2}]=(n^{2})^{2}\\cdot \\frac{1}{n^{5}}+0^{2}\\cdot\\left(1-\\frac{1}{n^{5}}\\right)=\\frac{n^{4}}{n^{5}}=\\frac{1}{n}.\n$$\nTherefore,\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}[(X_{n}-0)^{2}]=\\lim_{n \\to \\infty} \\frac{1}{n}=0,\n$$\nso $X_{n}$ converges in mean square to $0$. This directly validates statement B.\n\nTo assess the other options:\n- A: $\\mathbb{P}(X_{n}\\neq 0)=\\frac{1}{n^{5}} \\to 0$ shows convergence in probability to $0$, which does not in general imply mean-square convergence; thus the reasoning for mean-square convergence is not correct.\n- C: The fact that a possible value $n^{2}$ grows without bound does not preclude mean-square convergence; what matters is the decay of $\\mathbb{E}[X_{n}^{2}]$.\n- D: $\\mathbb{E}[X_{n}]=n^{2}\\cdot \\frac{1}{n^{5}}=\\frac{1}{n^{3}} \\to 0$, so the stated reason is false.\n\nHence, the correct choice is B.", "answer": "$$\\boxed{B}$$", "id": "1910471"}, {"introduction": "Finally, we connect theory to practice by examining the concept of estimator consistency. This exercise asks you to consider a simple, yet flawed, estimator for a population mean. By analyzing its behavior using the framework of mean square convergence [@problem_id:1910475], you will see firsthand why this mode of convergence is a crucial benchmark for evaluating the quality of statistical estimators and what non-convergence looks like in a practical context.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables drawn from a population with a finite mean $\\mu$ and a finite, non-zero variance $\\sigma^2$.\n\nConsider a sequence of estimators for the population mean $\\mu$, defined as $T_n = X_1$ for all sample sizes $n \\ge 1$. This means that for any sample size $n$, our estimate for the mean is always the value of the first observation.\n\nDoes the sequence of estimators $T_n$ converge in mean square to $\\mu$?\n\nA. Yes, the sequence converges in mean square to $\\mu$.\n\nB. No, the sequence does not converge in mean square to $\\mu$.\n\nC. The sequence converges in mean square, but to a value different from $\\mu$.\n\nD. There is not enough information to determine convergence, as the specific probability distribution of the $X_i$ is not provided.", "solution": "We recall the definition: a sequence of random variables $T_{n}$ converges in mean square to a constant $\\mu$ if and only if\n$$\n\\lim_{n\\to\\infty} \\mathbb{E}\\!\\left[(T_{n}-\\mu)^{2}\\right]=0.\n$$\nHere, $T_{n}=X_{1}$ for all $n\\geq 1$. Therefore,\n$$\n\\mathbb{E}\\!\\left[(T_{n}-\\mu)^{2}\\right]=\\mathbb{E}\\!\\left[(X_{1}-\\mu)^{2}\\right]=\\operatorname{Var}(X_{1})=\\sigma^{2},\n$$\nusing the definition $\\operatorname{Var}(X_{1})=\\mathbb{E}[(X_{1}-\\mu)^{2}]$ and the given that the variance is finite and non-zero. Since this quantity is the constant $\\sigma^{2}>0$ for all $n$, it does not tend to $0$ as $n\\to\\infty$.\n\nHence, $T_{n}$ does not converge in mean square to $\\mu$. (Note: While $T_{n}$ trivially converges in mean square to $X_{1}$, the question asks specifically about convergence to $\\mu$, which fails because $\\sigma^{2}\\neq 0$.)", "answer": "$$\\boxed{B}$$", "id": "1910475"}]}