{"hands_on_practices": [{"introduction": "Let's begin with a foundational example that illustrates the core principle of Wilks' theorem in a clear, single-parameter context [@problem_id:1896229]. By examining a hypothesis test for the success probability $p$ in a Geometric distribution, we can directly apply the theorem to see how the Likelihood Ratio Test (LRT) statistic asymptotically behaves. This exercise provides a concrete foundation for understanding how the difference in the dimensionality of parameter spaces—in this case, a one-dimensional space versus a zero-dimensional space under the null—translates to the degrees of freedom of the limiting chi-squared distribution.", "problem": "An experiment is conducted by repeatedly flipping a biased coin until the first \"Heads\" is observed. The random variable $X$ represents the total number of flips required. This process is modeled by a Geometric distribution with success probability $p$, where $p$ is the probability of getting \"Heads\" on a single flip. The Probability Mass Function (PMF) for $X$ is given by $P(X=k) = (1-p)^{k-1}p$ for $k=1, 2, 3, \\ldots$ and $0 < p < 1$.\n\nSuppose we have a large sample of $n$ independent and identically distributed (i.i.d.) observations, $X_1, X_2, \\ldots, X_n$, from this Geometric distribution. We wish to test the simple null hypothesis $H_0: p = p_0$ against the composite alternative hypothesis $H_1: p \\neq p_0$, where $p_0$ is a fixed, known value in the interval $(0, 1)$.\n\nThe Likelihood Ratio Test (LRT) statistic for this test is defined as:\n$$\n\\Lambda_n = \\frac{\\sup_{p \\in \\{p_0\\}} L(p|\\mathbf{x})}{\\sup_{p \\in (0,1)} L(p|\\mathbf{x})}\n$$\nwhere $L(p|\\mathbf{x})$ is the likelihood function for the observed sample $\\mathbf{x} = (x_1, \\ldots, x_n)$.\n\nAccording to a fundamental theorem in mathematical statistics regarding likelihood ratio tests, the distribution of the transformed statistic $-2\\ln\\Lambda_n$ converges to a specific well-known distribution as the sample size $n \\to \\infty$, provided the null hypothesis $H_0$ is true.\n\nWhat is this limiting (asymptotic) distribution of $-2\\ln\\Lambda_n$?\n\nA. A Normal distribution with mean 0 and variance 1, $N(0, 1)$.\n\nB. A Chi-squared distribution with 1 degree of freedom, $\\chi^2(1)$.\n\nC. A Chi-squared distribution with $n-1$ degrees of freedom, $\\chi^2(n-1)$.\n\nD. An F-distribution with $(1, n-1)$ degrees of freedom, $F_{1, n-1}$.\n\nE. A t-distribution with $n-1$ degrees of freedom, $t_{n-1}$.", "solution": "We observe i.i.d. $X_{1},\\ldots,X_{n}$ from a Geometric distribution with PMF $P(X=k)=(1-p)^{k-1}p$ for $k=1,2,\\ldots$ and $p\\in(0,1)$. The likelihood for a sample $\\mathbf{x}=(x_{1},\\ldots,x_{n})$ is\n$$\nL(p\\mid\\mathbf{x})=\\prod_{i=1}^{n}p(1-p)^{x_{i}-1}=p^{n}(1-p)^{\\sum_{i=1}^{n}x_{i}-n}.\n$$\nLet $S=\\sum_{i=1}^{n}x_{i}$. Then the log-likelihood is\n$$\n\\ell(p)=\\ln L(p\\mid\\mathbf{x})=n\\ln p+(S-n)\\ln(1-p).\n$$\nThe unrestricted MLE $\\hat{p}$ solves $\\frac{\\partial}{\\partial p}\\ell(p)=0$:\n$$\n\\frac{\\partial}{\\partial p}\\ell(p)=\\frac{n}{p}-\\frac{S-n}{1-p}=0\n\\;\\;\\Longrightarrow\\;\\; n(1-p)-p(S-n)=0\n\\;\\;\\Longrightarrow\\;\\; n-pS=0\n\\;\\;\\Longrightarrow\\;\\; \\hat{p}=\\frac{n}{S}.\n$$\nThe likelihood ratio statistic is\n$$\n\\Lambda_{n}=\\frac{\\sup_{p\\in\\{p_{0}\\}}L(p\\mid\\mathbf{x})}{\\sup_{p\\in(0,1)}L(p\\mid\\mathbf{x})}\n=\\frac{L(p_{0}\\mid\\mathbf{x})}{L(\\hat{p}\\mid\\mathbf{x})}.\n$$\nUnder the null hypothesis $H_{0}:p=p_{0}$ with $p_{0}\\in(0,1)$, this is a regular one-parameter problem in an open parameter space, and the Geometric family is a regular exponential family. Therefore, by Wilks’ theorem, the distribution of $-2\\ln\\Lambda_{n}$ converges in distribution, as $n\\to\\infty$, to a chi-squared distribution with degrees of freedom equal to the difference in dimensionality between the unrestricted and restricted parameter spaces. Here, the unrestricted model has dimension $1$ and the null model has dimension $0$, so the difference is $1$. Hence,\n$$\n-2\\ln\\Lambda_{n}\\;\\xrightarrow{d}\\;\\chi^{2}(1)\\quad\\text{under }H_{0}.\n$$\nTherefore, the correct choice is the chi-squared distribution with $1$ degree of freedom.", "answer": "$$\\boxed{B}$$", "id": "1896229"}, {"introduction": "Next, we move from a single-parameter model to a more practical and complex scenario: multiple linear regression [@problem_id:1896221]. This problem challenges you to apply the LRT framework to test a linear constraint on regression coefficients, specifically the hypothesis $H_0: \\beta_1 = \\beta_2$. This practice is crucial for developing the skill of correctly identifying the number of free parameters in both the full and restricted models to determine the degrees of freedom of the resulting chi-squared distribution, a common and essential task in applied statistics.", "problem": "A data science team is analyzing the factors that influence user engagement on their platform. They propose a multiple linear regression model to predict the weekly hours a user spends on the service, $Y$. The model includes two explanatory variables: $x_1$, a score representing the user's exposure to a content-ranking algorithm named \"EngageMax\", and $x_2$, a score representing exposure to a different algorithm named \"ExploreNow\".\n\nThe model is given by:\n$$Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i$$\nfor users $i = 1, \\dots, n$. The error terms $\\epsilon_i$ are assumed to be independent and identically distributed (i.i.d.) as normal random variables with a mean of 0 and an unknown variance $\\sigma^2 > 0$. The covariates $x_{i1}$ and $x_{i2}$ are treated as fixed, non-random quantities.\n\nThe team wants to determine if the two algorithms have an identical impact on user engagement. To do this, they formulate the null hypothesis $H_0: \\beta_1 = \\beta_2$. They construct the Likelihood Ratio Test (LRT) statistic, $T_n$, defined as:\n$$T_n = -2 \\ln\\left(\\frac{L_0}{L_1}\\right)$$\nwhere $L_0$ represents the maximized likelihood of the data under the null hypothesis $H_0$, and $L_1$ represents the maximized likelihood under the full, unrestricted model.\n\nAs the sample size $n$ tends to infinity, the test statistic $T_n$ converges to a specific probability distribution. Which of the following correctly describes this limiting distribution?\n\nA. A Chi-squared distribution with 1 degree of freedom.\n\nB. A Chi-squared distribution with 2 degrees of freedom.\n\nC. A Chi-squared distribution with 3 degrees of freedom.\n\nD. An F-distribution with $(1, n-3)$ degrees of freedom.\n\nE. A standard Normal distribution, $N(0,1)$.", "solution": "We have the normal linear regression model with fixed regressors:\n$$Y_{i} = \\beta_{0} + \\beta_{1} x_{i1} + \\beta_{2} x_{i2} + \\epsilon_{i}, \\quad \\epsilon_{i} \\stackrel{\\text{i.i.d.}}{\\sim} N(0,\\sigma^{2}), \\quad i=1,\\dots,n.$$\nWe test the null hypothesis $H_{0}:\\beta_{1}=\\beta_{2}$ using the likelihood ratio test with statistic\n$$T_{n} = -2 \\ln\\left(\\frac{L_{0}}{L_{1}}\\right),$$\nwhere $L_{0}$ is the maximized likelihood under $H_{0}$ and $L_{1}$ is the maximized likelihood under the full model.\n\nBy Wilks' theorem, under standard regularity conditions (which hold here for the normal linear model with fixed regressors and identifiable parameters), the asymptotic distribution under $H_{0}$ of the likelihood ratio statistic is\n$$T_{n} \\xrightarrow{d} \\chi^{2}_{k},$$\nwhere $k$ is the difference in the dimensions of the parameter spaces between the unrestricted and restricted models.\n\nCompute $k$:\n- Unrestricted parameter vector is $(\\beta_{0},\\beta_{1},\\beta_{2},\\sigma^{2})$, which has dimension $4$.\n- Under $H_{0}:\\beta_{1}=\\beta_{2}$, we can reparameterize as $(\\beta_{0},\\beta,\\sigma^{2})$ with $\\beta=\\beta_{1}=\\beta_{2}$, giving dimension $3$.\n\nThus the number of independent restrictions is $k=4-3=1$, so\n$$T_{n} \\xrightarrow{d} \\chi^{2}_{1}.$$\nThis corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1896221"}, {"introduction": "Finally, we explore a fascinating and critical case where the standard assumptions for Wilks' theorem are not met [@problem_id:1896246]. For a uniform distribution whose support depends on the parameters being estimated, the \"regularity conditions\" required for the theorem break down, and consequently, the asymptotic distribution of the LRT statistic is not what the standard theory would predict. This challenging problem is invaluable for deepening your understanding by demonstrating the importance of verifying theoretical assumptions, a crucial step in rigorous statistical analysis.", "problem": "Let $X_1, X_2, \\dots, X_n$ be an independent and identically distributed random sample from a uniform distribution on the interval $[\\theta_1, \\theta_2]$, where $\\theta_1, \\theta_2 \\in \\mathbb{R}$ with $\\theta_1 < \\theta_2$ are unknown parameters. Consider the hypothesis test with the null hypothesis $H_0: \\theta_2 - \\theta_1 = c$ against the alternative hypothesis $H_1: \\theta_2 - \\theta_1 \\neq c$, where $c$ is a known positive constant.\n\nLet $\\Lambda_n$ denote the Likelihood Ratio Test (LRT) statistic for this test, defined as the ratio of the maximum likelihood under the null hypothesis to the maximum likelihood over the entire parameter space. Determine the limiting distribution of the statistic $T_n = -2 \\ln \\Lambda_n$ as the sample size $n$ approaches infinity, under the assumption that the null hypothesis $H_0$ is true.\n\nSelect the correct description of the limiting distribution from the options below:\n\nA. A chi-squared distribution with 1 degree of freedom.\n\nB. A chi-squared distribution with 2 degrees of freedom.\n\nC. A standard normal distribution.\n\nD. A chi-squared distribution with 4 degrees of freedom.\n\nE. An exponential distribution with a rate parameter of 1.\n\nF. A gamma distribution with a shape parameter of 2 and a rate parameter of 1.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. $\\operatorname{Uniform}[\\theta_{1},\\theta_{2}]$ with unknown $\\theta_{1}<\\theta_{2}$ and let $R_{n}=X_{(n)}-X_{(1)}$ denote the sample range.\n\nThe likelihood for $(\\theta_{1},\\theta_{2})$ is\n$$\nL(\\theta_{1},\\theta_{2})=(\\theta_{2}-\\theta_{1})^{-n}\\,\\mathbf{1}\\{\\theta_{1}\\leq X_{(1)},\\,\\theta_{2}\\geq X_{(n)}\\}.\n$$\nMaximizing over the full parameter space yields the unconstrained MLE $\\hat{\\theta}_{1}=X_{(1)}$, $\\hat{\\theta}_{2}=X_{(n)}$, with maximized likelihood\n$$\nL_{\\text{full}}=R_{n}^{-n}.\n$$\nUnder $H_{0}:\\theta_{2}-\\theta_{1}=c$, any interval of length $c$ that contains all observations has likelihood $c^{-n}$; such an interval exists if and only if $R_{n}\\leq c$. Under $H_{0}$ this holds with probability $1$, so the constrained maximum is\n$$\nL_{0}=c^{-n}.\n$$\nHence the likelihood ratio is\n$$\n\\Lambda_{n}=\\frac{L_{0}}{L_{\\text{full}}}=\\left(\\frac{R_{n}}{c}\\right)^{n},\n$$\nand therefore\n$$\nT_{n}=-2\\ln\\Lambda_{n}=2n\\ln\\left(\\frac{c}{R_{n}}\\right).\n$$\n\nTo obtain the limiting distribution of $T_{n}$ under $H_{0}$, reparameterize to $Y_{i}=(X_{i}-\\theta_{1})/c\\sim \\operatorname{Uniform}[0,1]$. Then\n$$\nR_{n}=c\\bigl(Y_{(n)}-Y_{(1)}\\bigr),\\qquad D_{n}=c-R_{n}=c\\bigl[(1-Y_{(n)})+Y_{(1)}\\bigr]\\geq 0.\n$$\nClassical extreme-value limits for the uniform distribution give\n$$\nnY_{(1)}\\xrightarrow{d}E_{1},\\qquad n(1-Y_{(n)})\\xrightarrow{d}E_{2},\n$$\nwhere $E_{1}$ and $E_{2}$ are independent $\\operatorname{Exp}(1)$ variables. Thus\n$$\n\\frac{nD_{n}}{c}=n\\bigl[(1-Y_{(n)})+Y_{(1)}\\bigr]\\xrightarrow{d}E_{1}+E_{2}\\equiv G,\n$$\nwith $G\\sim \\operatorname{Gamma}(2,1)$ (shape $2$, rate $1$).\n\nSince $D_{n}/c\\to 0$ in probability at rate $n^{-1}$, use the expansion\n$$\n\\ln\\left(\\frac{c}{R_{n}}\\right)=-\\ln\\left(1-\\frac{D_{n}}{c}\\right)=\\frac{D_{n}}{c}+o_{p}\\left(\\frac{D_{n}}{c}\\right).\n$$\nTherefore\n$$\nT_{n}=2n\\ln\\left(\\frac{c}{R_{n}}\\right)=2n\\frac{D_{n}}{c}+o_{p}(1)\\xrightarrow{d}2G.\n$$\nIf $G\\sim \\operatorname{Gamma}(2,1)$, then $2G\\sim \\operatorname{Gamma}\\bigl(2,\\tfrac{1}{2}\\bigr)$, which is a chi-squared distribution with $4$ degrees of freedom. Hence, under $H_{0}$,\n$$\nT_{n}\\xrightarrow{d}\\chi^{2}_{4}.\n$$\n\nThus the correct option is D.", "answer": "$$\\boxed{D}$$", "id": "1896246"}]}