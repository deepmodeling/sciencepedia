{"hands_on_practices": [{"introduction": "The precision of a Maximum Likelihood Estimator (MLE) is fundamentally linked to the amount of information the data provides about an unknown parameter. This concept is quantified by Fisher Information, which serves as the cornerstone for understanding the asymptotic behavior of MLEs. This first exercise [@problem_id:1896729] guides you through the essential calculation of Fisher Information for a single observation from the widely-used exponential distribution, establishing a crucial foundation for the concepts that follow.", "problem": "Consider a random variable $X$ that models the time until a specific event occurs, such as the decay of a radioactive particle or the failure of an electronic component. This time is assumed to follow an exponential distribution with a constant rate parameter $\\lambda > 0$. The probability density function (PDF) for $X$ is given by:\n$$f(x; \\lambda) = \\lambda \\exp(-\\lambda x) \\quad \\text{for } x \\ge 0$$\nThe Fisher information, denoted as $I(\\lambda)$, quantifies the amount of information that a single observation $X$ provides about the unknown parameter $\\lambda$. It is a fundamental concept in the theory of statistical estimation.\n\nCalculate the Fisher information $I(\\lambda)$ for a single observation from this exponential distribution.", "solution": "We are given a single observation from an exponential distribution with rate parameter $\\lambda>0$ and density $f(x;\\lambda)=\\lambda \\exp(-\\lambda x)$ for $x\\ge 0$. The Fisher information for a single observation is defined by either\n$$\nI(\\lambda)=\\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\lambda}\\ln f(X;\\lambda)\\right)^{2}\\right]\n$$\nor equivalently\n$$\nI(\\lambda)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2}}{\\partial \\lambda^{2}}\\ln f(X;\\lambda)\\right].\n$$\nWe start from the log-likelihood for one observation:\n$$\n\\ell(\\lambda;X)=\\ln f(X;\\lambda)=\\ln \\lambda-\\lambda X.\n$$\nDifferentiate with respect to $\\lambda$ to obtain the score:\n$$\n\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda;X)=\\frac{1}{\\lambda}-X.\n$$\nHence,\n$$\nI(\\lambda)=\\mathbb{E}\\!\\left[\\left(\\frac{1}{\\lambda}-X\\right)^{2}\\right]=\\mathbb{E}\\!\\left[\\frac{1}{\\lambda^{2}}-\\frac{2}{\\lambda}X+X^{2}\\right]=\\frac{1}{\\lambda^{2}}-\\frac{2}{\\lambda}\\mathbb{E}[X]+\\mathbb{E}[X^{2}].\n$$\nWe compute the necessary moments under $X\\sim \\text{Exp}(\\lambda)$. Using the integral identity $\\int_{0}^{\\infty} x^{n}\\exp(-\\lambda x)\\,dx=\\frac{n!}{\\lambda^{n+1}}$ (obtained from the substitution $y=\\lambda x$ and the definition of the gamma function), we have\n$$\n\\mathbb{E}[X]=\\int_{0}^{\\infty} x \\,\\lambda \\exp(-\\lambda x)\\,dx=\\lambda \\int_{0}^{\\infty} x \\exp(-\\lambda x)\\,dx=\\lambda \\cdot \\frac{1!}{\\lambda^{2}}=\\frac{1}{\\lambda},\n$$\nand\n$$\n\\mathbb{E}[X^{2}]=\\int_{0}^{\\infty} x^{2}\\,\\lambda \\exp(-\\lambda x)\\,dx=\\lambda \\int_{0}^{\\infty} x^{2}\\exp(-\\lambda x)\\,dx=\\lambda \\cdot \\frac{2!}{\\lambda^{3}}=\\frac{2}{\\lambda^{2}}.\n$$\nSubstituting these into the expression for $I(\\lambda)$ gives\n$$\nI(\\lambda)=\\frac{1}{\\lambda^{2}}-\\frac{2}{\\lambda}\\cdot \\frac{1}{\\lambda}+\\frac{2}{\\lambda^{2}}=\\frac{1}{\\lambda^{2}}.\n$$\nEquivalently, differentiating the log-likelihood twice,\n$$\n\\frac{\\partial^{2}}{\\partial \\lambda^{2}}\\ell(\\lambda;X)=-\\frac{1}{\\lambda^{2}},\n$$\nand thus\n$$\nI(\\lambda)=-\\mathbb{E}\\!\\left[-\\frac{1}{\\lambda^{2}}\\right]=\\frac{1}{\\lambda^{2}},\n$$\nwhich agrees with the previous result.", "answer": "$$\\boxed{\\frac{1}{\\lambda^{2}}}$$", "id": "1896729"}, {"introduction": "With the concept of Fisher Information established, we can now apply the central theorem of asymptotic normality for MLEs. For large samples, the distribution of an MLE converges to a normal distribution with a variance equal to the inverse of the Fisher Information. This practice problem [@problem_id:1896666] provides a concrete example using a discrete distribution, asking you to determine the asymptotic variance for the MLE of a parameter in a geometric distribution, a common model for count data.", "problem": "A statistician is modeling the number of attempts a student makes to solve a specific type of logic puzzle before their first success. Let the random variable $K$ be the number of attempts required. The statistician models $K$ using a geometric distribution, where the probability of success on any given attempt is $\\theta$. The probability mass function (PMF) for $K$ is given by:\n$$P(K=k | \\theta) = (1-\\theta)^{k-1}\\theta$$\nfor $k = 1, 2, 3, \\ldots$, where $\\theta \\in (0, 1)$ is the unknown parameter of interest.\n\nFrom a random sample of $n$ students, $K_1, K_2, \\ldots, K_n$, the statistician computes the Maximum Likelihood Estimator (MLE) for $\\theta$, which is denoted by $\\hat{\\theta}_{MLE}$. For large sample sizes, the central limit theorem for MLEs states that the distribution of the estimator can be approximated by a normal distribution. Specifically, the quantity $\\sqrt{n}(\\hat{\\theta}_{MLE} - \\theta)$ converges in distribution to a normal distribution with a mean of 0 and an asymptotic variance, $V$.\n\nYour task is to determine this asymptotic variance, $V$. Express your answer as a function of the parameter $\\theta$.", "solution": "The sample consists of independent observations with PMF $f(k|\\theta)=(1-\\theta)^{k-1}\\theta$, $k\\in\\{1,2,\\ldots\\}$, $\\theta\\in(0,1)$. For an i.i.d. sample $K_{1},\\ldots,K_{n}$, the log-likelihood is\n$$\n\\ell(\\theta)=\\sum_{i=1}^{n}\\left[\\ln\\theta+(K_{i}-1)\\ln(1-\\theta)\\right]\n= n\\ln\\theta+\\left(\\sum_{i=1}^{n}K_{i}-n\\right)\\ln(1-\\theta).\n$$\nThe score for a single observation is\n$$\nu(\\theta;K)=\\frac{\\partial}{\\partial\\theta}\\left[\\ln\\theta+(K-1)\\ln(1-\\theta)\\right]\n=\\frac{1}{\\theta}-\\frac{K-1}{1-\\theta}.\n$$\nThe Fisher information per observation is $I_{1}(\\theta)=-\\mathbb{E}\\left[\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\ln f(K|\\theta)\\right]$. Compute the second derivative:\n$$\n\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\ln f(K|\\theta)=-\\frac{1}{\\theta^{2}}-\\frac{K-1}{(1-\\theta)^{2}}.\n$$\nTaking expectation under $K\\sim\\text{Geom}(\\theta)$ (with support $\\{1,2,\\ldots\\}$) uses $\\mathbb{E}[K-1]=\\frac{1-\\theta}{\\theta}$, which follows from\n$$\n\\mathbb{E}[K-1]=\\sum_{k=1}^{\\infty}(k-1)(1-\\theta)^{k-1}\\theta=\\theta\\sum_{j=0}^{\\infty}j(1-\\theta)^{j}\n=\\theta\\cdot\\frac{1-\\theta}{\\theta^{2}}=\\frac{1-\\theta}{\\theta}.\n$$\nThus\n$$\n\\mathbb{E}\\!\\left[\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\ln f(K|\\theta)\\right]\n=-\\frac{1}{\\theta^{2}}-\\frac{\\mathbb{E}[K-1]}{(1-\\theta)^{2}}\n=-\\frac{1}{\\theta^{2}}-\\frac{1}{\\theta(1-\\theta)}.\n$$\nHence the Fisher information per observation is\n$$\nI_{1}(\\theta)=\\frac{1}{\\theta^{2}}+\\frac{1}{\\theta(1-\\theta)}=\\frac{1}{\\theta^{2}(1-\\theta)}.\n$$\nFor $n$ i.i.d. observations, $I_{n}(\\theta)=nI_{1}(\\theta)$. The asymptotic normality of the MLE states\n$$\n\\sqrt{n}\\left(\\hat{\\theta}_{\\text{MLE}}-\\theta\\right)\\xrightarrow{d}\\mathcal{N}\\!\\left(0,\\;I_{1}(\\theta)^{-1}\\right),\n$$\nso the asymptotic variance is\n$$\nV=I_{1}(\\theta)^{-1}=\\theta^{2}(1-\\theta).\n$$\nAs a consistency check via the Delta method: The MLE is $\\hat{\\theta}_{\\text{MLE}}=1/\\bar{K}$. Let $\\mu = \\mathbb{E}[K]=1/\\theta$. For the function $g(\\mu) = 1/\\mu$, the derivative is $g'(\\mu) = -1/\\mu^2 = -\\theta^2$. The asymptotic variance of $\\sqrt{n}(\\hat{\\theta}_{\\text{MLE}}-\\theta)$ is given by $[g'(\\mu)]^2$ times the asymptotic variance of $\\sqrt{n}(\\bar{K}-\\mu)$, which is $\\operatorname{Var}(K)$. This yields $V = (-\\theta^2)^2 \\operatorname{Var}(K) = \\theta^4 \\cdot \\frac{1-\\theta}{\\theta^2} = \\theta^2(1-\\theta)$, which agrees.", "answer": "$$\\boxed{\\theta^{2}(1-\\theta)}$$", "id": "1896666"}, {"introduction": "Often in statistical modeling, we are interested not just in a parameter $\\theta$ itself, but in a function of that parameter, such as its logarithm or a log-odds transformation. The Delta Method is a powerful tool that allows us to find the approximate distribution of these transformed estimators based on the known asymptotic distribution of the original MLE. This exercise [@problem_id:1896726] demonstrates this vital technique by deriving the asymptotic variance of the log-odds of a success probability, a transformation central to logistic regression and the analysis of proportions.", "problem": "In a large-scale clinical trial, the effectiveness of a new treatment is modeled as a Bernoulli process. Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n$ from a Bernoulli distribution with parameter $p$, representing the probability of a successful treatment. The parameter $p$ is unknown, with $0 < p < 1$.\n\nThe standard estimator for $p$ is the sample proportion, $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n X_i$, which is the Maximum Likelihood Estimator (MLE). For a large sample size $n$, it is a known result that $\\hat{p}$ is approximately normally distributed with a mean of $p$ and a variance of $\\frac{p(1-p)}{n}$.\n\nStatisticians are often interested in the log-odds of success, defined as $L(p) = \\ln\\left(\\frac{p}{1-p}\\right)$, where $\\ln$ is the natural logarithm. The corresponding estimator is the log-odds of the sample proportion, $L(\\hat{p})$. Based on large-sample theory, this estimator $L(\\hat{p})$ is also approximately normally distributed.\n\nDetermine the variance of the approximate normal distribution for $L(\\hat{p})$. Your answer should be a closed-form analytic expression in terms of $p$ and $n$.", "solution": "We are given that, for large $n$, the sample proportion $\\hat{p}$ is approximately normal with mean $p$ and variance $\\frac{p(1-p)}{n}$:\n$$\n\\hat{p}\\approx \\mathcal{N}\\!\\left(p,\\ \\frac{p(1-p)}{n}\\right).\n$$\nWe seek the large-sample variance of $L(\\hat{p})$, where $L(p)=\\ln\\!\\left(\\frac{p}{1-p}\\right)$. Let $g(p)=L(p)$. By the Delta method, if $g$ is differentiable at $p$, then\n$$\n\\operatorname{Var}\\big(g(\\hat{p})\\big)\\approx \\big(g'(p)\\big)^{2}\\,\\operatorname{Var}(\\hat{p}).\n$$\nCompute $g'(p)$:\n$$\ng(p)=\\ln(p)-\\ln(1-p),\\quad g'(p)=\\frac{1}{p}+\\frac{1}{1-p}=\\frac{1}{p(1-p)}.\n$$\nSubstituting into the Delta method formula with $\\operatorname{Var}(\\hat{p})=\\frac{p(1-p)}{n}$ gives\n$$\n\\operatorname{Var}\\big(L(\\hat{p})\\big)\\approx \\left(\\frac{1}{p(1-p)}\\right)^{2}\\cdot \\frac{p(1-p)}{n}\n= \\frac{1}{n\\,p(1-p)}.\n$$\nTherefore, the asymptotic variance of the approximate normal distribution for $L(\\hat{p})$ is $\\frac{1}{n\\,p(1-p)}$.", "answer": "$$\\boxed{\\frac{1}{n\\,p(1-p)}}$$", "id": "1896726"}]}