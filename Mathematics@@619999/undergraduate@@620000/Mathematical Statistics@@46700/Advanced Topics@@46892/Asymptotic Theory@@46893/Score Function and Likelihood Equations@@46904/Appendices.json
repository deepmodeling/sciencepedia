{"hands_on_practices": [{"introduction": "This first exercise is a fundamental building block for understanding maximum likelihood estimation. We will tackle a classic scenario: estimating the variance $\\sigma^2$ of a Normal distribution when its mean $\\mu$ is already known. By working through this problem [@problem_id:1953749], you will practice the essential steps of writing the log-likelihood function, finding its derivative (the score function), and solving the resulting likelihood equation to obtain the estimator.", "problem": "A manufacturer of high-precision scientific instruments is characterizing a new voltage source. The device is designed to output a constant, known voltage of $\\mu$. However, due to inherent thermal noise in its electronic components, repeated measurements of the output voltage exhibit random fluctuations. These fluctuations are modeled as a random sample of $n$ independent observations, $X_1, X_2, \\ldots, X_n$, drawn from a Normal distribution with the known mean $\\mu$ and an unknown variance $\\sigma^2 > 0$. The variance $\\sigma^2$ is a critical parameter that quantifies the stability and quality of the voltage source.\n\nYour task is to determine the Maximum Likelihood Estimator (MLE) for the variance, $\\sigma^2$. Provide the estimator as an expression in terms of the sample observations $X_i$ (for $i=1, \\ldots, n$), the sample size $n$, and the known mean $\\mu$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be independent with $X_{i} \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ where $\\mu$ is known and $\\sigma^{2}>0$ is unknown. The probability density of $X_{i}$ is\n$$\nf(x_{i}\\mid \\mu,\\sigma^{2})=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\!\\left(-\\frac{(x_{i}-\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nBy independence, the likelihood is\n$$\nL(\\sigma^{2})=\\prod_{i=1}^{n}f(x_{i}\\mid \\mu,\\sigma^{2})=(2\\pi\\sigma^{2})^{-n/2}\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}\\right).\n$$\nThe log-likelihood is\n$$\n\\ell(\\sigma^{2})=\\ln L(\\sigma^{2})=-\\frac{n}{2}\\ln(2\\pi)-\\frac{n}{2}\\ln(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}.\n$$\nLet $v=\\sigma^{2}$. Then\n$$\n\\ell(v)=-\\frac{n}{2}\\ln(2\\pi)-\\frac{n}{2}\\ln v-\\frac{1}{2v}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}.\n$$\nDifferentiate with respect to $v$ and set to zero:\n$$\n\\frac{\\partial \\ell}{\\partial v}=-\\frac{n}{2}\\frac{1}{v}+\\frac{1}{2}\\frac{\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}}{v^{2}}=0.\n$$\nMultiplying by $2v^{2}$ gives\n$$\n-nv+\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}=0 \\quad \\Longrightarrow \\quad \\hat{v}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}.\n$$\nThe second derivative is\n$$\n\\frac{\\partial^{2}\\ell}{\\partial v^{2}}=\\frac{n}{2}\\frac{1}{v^{2}}-\\frac{\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}}{v^{3}},\n$$\nwhich, evaluated at $\\hat{v}$, equals $-\\frac{n^{3}}{2}\\Big/\\left(\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}\\right)^{2}0$ when $\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}>0$, confirming a maximum. Therefore, the MLE for $\\sigma^{2}$ is\n$$\n\\hat{\\sigma}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right)^{2}}$$", "id": "1953749"}, {"introduction": "Real-world statistical modeling often requires estimating multiple parameters simultaneously. This practice moves beyond the single-parameter case by exploring the Gamma distribution, a flexible model widely used for positive data such as waiting times. Here, you will generalize the score function to a score vector and derive the system of likelihood equations needed to find the maximum likelihood estimates for both the shape parameter $\\alpha$ and rate parameter $\\beta$ [@problem_id:1953820]. This skill is essential for tackling more complex, multi-dimensional estimation problems.", "problem": "In an underground neutrino observatory, the waiting times between consecutive high-energy neutrino detection events are being modeled. A physicist proposes that these waiting times, $X_1, X_2, \\ldots, X_n$, can be described as an independent and identically distributed random sample from a Gamma distribution with an unknown shape parameter $\\alpha  0$ and an unknown rate parameter $\\beta  0$.\n\nThe Probability Density Function (PDF) for a Gamma-distributed random variable $X$ is given by:\n$$f(x; \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x) \\quad \\text{for } x  0$$\nwhere $\\Gamma(\\alpha)$ is the gamma function.\n\nTo find the Maximum Likelihood Estimates (MLEs) for the parameter vector $(\\alpha, \\beta)$, one must first derive the likelihood equations by taking the partial derivatives of the log-likelihood function with respect to each parameter and setting them to zero. This results in a system of two equations.\n\nLet $\\psi(\\alpha) = \\frac{d}{d\\alpha} \\ln \\Gamma(\\alpha)$ denote the digamma function. Given the random sample $x_1, x_2, \\ldots, x_n$, which of the following systems of equations represents the correct likelihood equations for estimating $\\alpha$ and $\\beta$?\n\nA.\n$\\begin{cases}\nn \\ln(\\beta) - n \\psi(\\alpha) + \\sum_{i=1}^{n} \\ln(x_i) = 0 \\\\\n\\frac{n\\alpha}{\\beta} - \\sum_{i=1}^{n} x_i = 0\n\\end{cases}$\n\nB.\n$\\begin{cases}\nn \\ln(\\beta) - n \\psi(\\alpha) + \\sum_{i=1}^{n} \\ln(x_i) = 0 \\\\\n\\frac{n\\alpha}{\\beta} + \\sum_{i=1}^{n} x_i = 0\n\\end{cases}$\n\nC.\n$\\begin{cases}\nn \\ln(\\beta) + \\sum_{i=1}^{n} \\ln(x_i) = 0 \\\\\n\\frac{n\\alpha}{\\beta} - \\sum_{i=1}^{n} x_i = 0\n\\end{cases}$\n\nD.\n$\\begin{cases}\nn \\ln(\\beta) - \\sum_{i=1}^{n} \\ln(x_i) = 0 \\\\\nn \\alpha \\beta - \\psi(\\alpha) - \\sum_{i=1}^{n} x_i = 0\n\\end{cases}$", "solution": "For an i.i.d. sample $x_{1},\\ldots,x_{n}$ from $\\operatorname{Gamma}(\\alpha,\\beta)$ with pdf $f(x;\\alpha,\\beta)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha-1}\\exp(-\\beta x)$ for $x0$, the likelihood is\n$$\nL(\\alpha,\\beta)=\\prod_{i=1}^{n}\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x_{i}^{\\alpha-1}\\exp(-\\beta x_{i})\n=\\left(\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\right)^{n}\\left(\\prod_{i=1}^{n}x_{i}^{\\alpha-1}\\right)\\exp\\!\\left(-\\beta\\sum_{i=1}^{n}x_{i}\\right).\n$$\nThe log-likelihood is\n$$\n\\ell(\\alpha,\\beta)=\\ln L(\\alpha,\\beta)\n=n\\alpha\\ln(\\beta)-n\\ln\\Gamma(\\alpha)+(\\alpha-1)\\sum_{i=1}^{n}\\ln(x_{i})-\\beta\\sum_{i=1}^{n}x_{i}.\n$$\nDifferentiate with respect to $\\alpha$. Using $\\psi(\\alpha)=\\frac{d}{d\\alpha}\\ln\\Gamma(\\alpha)$,\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha}=n\\ln(\\beta)-n\\psi(\\alpha)+\\sum_{i=1}^{n}\\ln(x_{i}).\n$$\nSetting this to zero gives\n$$\nn\\ln(\\beta)-n\\psi(\\alpha)+\\sum_{i=1}^{n}\\ln(x_{i})=0.\n$$\nDifferentiate with respect to $\\beta$:\n$$\n\\frac{\\partial \\ell}{\\partial \\beta}=\\frac{n\\alpha}{\\beta}-\\sum_{i=1}^{n}x_{i}.\n$$\nSetting this to zero gives\n$$\n\\frac{n\\alpha}{\\beta}-\\sum_{i=1}^{n}x_{i}=0.\n$$\nThis system matches option A.", "answer": "$$\\boxed{A}$$", "id": "1953820"}, {"introduction": "This advanced practice delves into a critical and insightful question: what does maximum likelihood estimation achieve if our chosen statistical model does not match the true data-generating process? We explore a scenario of model misspecification, where data from a Poisson process is modeled using a Geometric distribution. By solving for the parameter value that the likelihood procedure would converge to, you will be introduced to the concept of a 'pseudo-true' parameter [@problem_id:1953768], gaining a deeper understanding of what the score function reveals even when a model's assumptions are not perfectly met.", "problem": "A researcher is modeling count data, representing the number of occurrences of a rare event within specified time intervals. The true underlying data-generating process for the number of events, $X$, is a Poisson distribution with a mean rate $\\lambda_0$. The probability mass function (PMF) for this process is given by:\n$$P(X=k) = \\frac{\\exp(-\\lambda_0) \\lambda_0^k}{k!} \\quad \\text{for } k = 0, 1, 2, \\dots$$\n\nHowever, the researcher incorrectly assumes the data follows a Geometric distribution, which models the number of failures before the first success in a sequence of Bernoulli trials. The PMF for this assumed geometric model is:\n$$P(X=k; p) = p(1-p)^k \\quad \\text{for } k = 0, 1, 2, \\dots$$\nwhere $p$ is the probability of success.\n\nIn statistical theory, when a model is misspecified, the parameter value that the Maximum Likelihood Estimator converges to is often called the \"pseudo-true\" value. This value is found by solving the equation where the expected score function is zero. The score function is the derivative of the log-likelihood function with respect to the parameter. Critically, the expectation must be taken with respect to the *true* data-generating distribution.\n\nYour task is to find this pseudo-true value of $p$. Determine the value of $p$, as an analytic expression in terms of $\\lambda_0$, for which the expected value of the score function of the geometric model is zero, where the expectation is calculated under the true Poisson($\\lambda_0$) distribution.", "solution": "The assumed geometric model has log-likelihood for one observation $x$ given by\n$$\\ell(p;x)=\\ln p + x \\ln(1-p).$$\nThe score function is the derivative with respect to $p$:\n$$s(p;x)=\\frac{\\partial \\ell}{\\partial p}=\\frac{1}{p} + x \\cdot \\frac{\\partial}{\\partial p}\\ln(1-p)=\\frac{1}{p} - \\frac{x}{1-p}.$$\nUnder model misspecification, the pseudo-true parameter $p^{\\ast}$ solves the equation that the expected score under the true distribution equals zero:\n$$\\mathbb{E}_{\\text{Poisson}(\\lambda_{0})}\\left[s(p;X)\\right]=0.$$\nUsing the expression for the score and linearity of expectation,\n$$\\mathbb{E}\\left[\\frac{1}{p} - \\frac{X}{1-p}\\right]=\\frac{1}{p} - \\frac{\\mathbb{E}[X]}{1-p}=\\frac{1}{p} - \\frac{\\lambda_{0}}{1-p}=0,$$\nsince for $X \\sim \\text{Poisson}(\\lambda_{0})$ we have $\\mathbb{E}[X]=\\lambda_{0}$. Solving for $p$,\n$$\\frac{1}{p}=\\frac{\\lambda_{0}}{1-p} \\;\\;\\Longrightarrow\\;\\; 1-p=\\lambda_{0}p \\;\\;\\Longrightarrow\\;\\; 1=p(1+\\lambda_{0}) \\;\\;\\Longrightarrow\\;\\; p=\\frac{1}{1+\\lambda_{0}}.$$\nThis value lies in $(0,1)$ for all $\\lambda_{0}0$, so it is a valid probability and is the unique solution.", "answer": "$$\\boxed{\\frac{1}{1+\\lambda_{0}}}$$", "id": "1953768"}]}