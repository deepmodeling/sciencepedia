{"hands_on_practices": [{"introduction": "The Central Limit Theorem (CLT) is a cornerstone of modern statistics, providing a powerful result about the collective behavior of random variables. This first exercise [@problem_id:1910206] offers a direct, hands-on application of the CLT. You will see how, regardless of the initial distribution's specific shape—in this case, a polynomial probability density function—the standardized sample mean invariably converges to the universal standard normal distribution, a phenomenon that underpins much of statistical inference.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables, each with a probability density function (PDF) given by:\n$$\nf(x) = \n\\begin{cases} \n3x^2 & \\text{for } 0 \\le x \\le 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nLet $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$ denote the sample mean. Furthermore, let $\\mu$ and $\\sigma$ represent the true mean and standard deviation of an individual random variable $X_i$, respectively.\n\nConsider the transformed random variable $Z_n = \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{\\sigma}$. Which of the following describes the distribution of $Z_n$ in the limit as the sample size $n$ approaches infinity?\n\nA. The standard normal distribution, $N(0,1)$.\n\nB. A normal distribution with mean $3/4$ and variance $3/80$.\n\nC. The uniform distribution on the interval $[0,1]$.\n\nD. The chi-squared distribution with 1 degree of freedom.\n\nE. A distribution with the PDF $f(x)=3x^2$ on $[0,1]$.\n\nF. A degenerate distribution at the point $3/4$.", "solution": "We first verify that $f(x)$ is a valid probability density function by computing its integral over its support. Using the definition of a PDF,\n$$\n\\int_{-\\infty}^{\\infty} f(x)\\,dx=\\int_{0}^{1} 3x^{2}\\,dx=\\left[x^{3}\\right]_{0}^{1}=1.\n$$\nThe mean $\\mu=\\mathbb{E}[X]$ is computed using the definition of expectation for a continuous random variable:\n$$\n\\mu=\\int_{-\\infty}^{\\infty} x f(x)\\,dx=\\int_{0}^{1} x\\cdot 3x^{2}\\,dx=\\int_{0}^{1} 3x^{3}\\,dx=\\left[\\frac{3}{4}x^{4}\\right]_{0}^{1}=\\frac{3}{4}.\n$$\nThe second moment is\n$$\n\\mathbb{E}[X^{2}]=\\int_{0}^{1} x^{2}\\cdot 3x^{2}\\,dx=\\int_{0}^{1} 3x^{4}\\,dx=\\left[\\frac{3}{5}x^{5}\\right]_{0}^{1}=\\frac{3}{5}.\n$$\nTherefore, the variance is\n$$\n\\sigma^{2}=\\text{Var}(X)=\\mathbb{E}[X^{2}]-\\mu^{2}=\\frac{3}{5}-\\left(\\frac{3}{4}\\right)^{2}=\\frac{3}{5}-\\frac{9}{16}=\\frac{48}{80}-\\frac{45}{80}=\\frac{3}{80},\n$$\nand the standard deviation is $\\sigma=\\sqrt{\\frac{3}{80}}$.\n\nDefine the standardized statistic\n$$\nZ_{n}=\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma}.\n$$\nBy the Lindeberg–Levy Central Limit Theorem, if $X_{1},X_{2},\\dots,X_{n}$ are i.i.d. with finite mean $\\mu$ and finite, positive variance $\\sigma^{2}$, then as $n\\to\\infty$,\n$$\n\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma}\\ \\xrightarrow{d}\\ N(0,1),\n$$\nthat is, $Z_{n}$ converges in distribution to the standard normal distribution. The conditions are satisfied here because $\\mu=\\frac{3}{4}$ and $\\sigma^{2}=\\frac{3}{80}$ are finite and $\\sigma^{2}>0$. Therefore, the limiting distribution of $Z_{n}$ is $N(0,1)$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1910206"}, {"introduction": "Once we establish that a sequence of random variables converges, a natural question arises: what happens if we apply a function to each term in the sequence? The Continuous Mapping Theorem (CMT) provides the answer, acting as a vital extension to our convergence toolkit. This practice problem [@problem_id:1910213] demonstrates the power of the CMT by exploring the limiting distribution of the square of a sequence of t-distributed random variables, elegantly connecting concepts from different fundamental statistical distributions.", "problem": "Let $\\{T_n\\}_{n=1}^{\\infty}$ be a sequence of random variables where each $T_n$ follows a t-distribution with $n$ degrees of freedom. It is a known result that this sequence of random variables converges in distribution to a standard normal random variable, $Z$, which has a probability density function given by $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$. Symbolically, this is written as $T_n \\xrightarrow{d} Z$.\n\nConsider a new sequence of random variables $\\{Y_n\\}_{n=1}^{\\infty}$ defined by the transformation $Y_n = T_n^2$.\n\nIdentify the limiting distribution of the sequence $Y_n$ as $n$ approaches infinity from the options below.\n\nA. A standard normal distribution, $N(0, 1)$.\n\nB. A chi-squared distribution with 1 degree of freedom, $\\chi^2(1)$.\n\nC. A t-distribution with 1 degree of freedom (which is also known as the Cauchy distribution).\n\nD. An F-distribution with $(1, 1)$ degrees of freedom.\n\nE. An exponential distribution with a rate parameter $\\lambda=1/2$.", "solution": "We are given that $T_{n} \\xrightarrow{d} Z$ where $Z \\sim N(0,1)$. Define the transformation $g:\\mathbb{R} \\to \\mathbb{R}$ by $g(x)=x^{2}$. The function $g$ is continuous on $\\mathbb{R}$. By the Continuous Mapping Theorem, if $X_{n} \\xrightarrow{d} X$ and $g$ is continuous, then $g(X_{n}) \\xrightarrow{d} g(X)$. Applying this with $X_{n}=T_{n}$ and $X=Z$, we obtain\n$$\nY_{n}=T_{n}^{2} \\xrightarrow{d} Z^{2}.\n$$\nIt is a defining property of the chi-squared distribution that if $Z \\sim N(0,1)$, then $Z^{2} \\sim \\chi^{2}(1)$. Therefore, the limiting distribution of $Y_{n}$ is $\\chi^{2}(1)$, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1910213"}, {"introduction": "Beyond the Central Limit Theorem and direct analysis of distribution functions, another powerful technique for proving convergence in distribution involves using moment generating functions (MGFs). According to Lévy's Continuity Theorem, the convergence of MGFs implies the convergence in distribution of the random variables. This final exercise [@problem_id:1910212] guides you through this method, showing how to determine a limiting distribution by analyzing the pointwise limit of a sequence of MGFs, and revealing how this can lead to insightful results like convergence to a constant.", "problem": "Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$. The moment generating function (MGF) of the random variable $X_n$ is given by\n$$ M_{X_n}(t) = \\left( \\frac{\\lambda}{\\lambda - \\frac{t}{n^2}} \\right)^k $$\nfor all $t < n^2 \\lambda$, where $k$ is a fixed positive integer and $\\lambda$ is a fixed positive real constant.\n\nDetermine the limiting distribution of the sequence $X_n$ as $n \\to \\infty$.\n\nA. The limiting distribution is a Gamma distribution with shape parameter $k$ and rate parameter $\\lambda$.\n\nB. The limiting distribution is an Exponential distribution with rate parameter $\\lambda k$.\n\nC. The limiting distribution is a degenerate distribution at 0.\n\nD. The limiting distribution is a Normal distribution with a mean of 0 and a non-zero variance.\n\nE. The limit of the MGF does not exist, so there is no limiting distribution.\n\nF. The limiting distribution is a degenerate distribution at $k/\\lambda$.", "solution": "We start from the given moment generating function (MGF)\n$$\nM_{X_{n}}(t)=\\left(\\frac{\\lambda}{\\lambda-\\frac{t}{n^{2}}}\\right)^{k}, \\quad \\text{for } t<n^{2}\\lambda,\n$$\nwhere $k$ is a fixed positive integer and $\\lambda$ is a fixed positive real constant. Recall that the MGF of a Gamma distribution with shape parameter $k$ and rate parameter $\\lambda$ is\n$$\nM_{Z}(t)=\\left(\\frac{\\lambda}{\\lambda-t}\\right)^{k}, \\quad \\text{for } t<\\lambda.\n$$\nIf $Z\\sim\\text{Gamma}(k,\\lambda)$ and $Y=Z/n^{2}$, then by the scaling property of MGFs we have\n$$\nM_{Y}(t)=M_{Z}\\!\\left(\\frac{t}{n^{2}}\\right)=\\left(\\frac{\\lambda}{\\lambda-\\frac{t}{n^{2}}}\\right)^{k}.\n$$\nThus $X_{n}$ has the same distribution as $Z/n^{2}$ with $Z\\sim\\text{Gamma}(k,\\lambda)$.\n\nTo determine the limiting distribution of $X_{n}$ as $n\\to\\infty$, consider the pointwise limit of the MGFs at any fixed real $t$. For each fixed $t\\in\\mathbb{R}$, for all sufficiently large $n$ we have $t<n^{2}\\lambda$, so\n$$\n\\lim_{n\\to\\infty}M_{X_{n}}(t)=\\lim_{n\\to\\infty}\\left(\\frac{\\lambda}{\\lambda-\\frac{t}{n^{2}}}\\right)^{k}=\\left(\\frac{\\lambda}{\\lambda-0}\\right)^{k}=1.\n$$\nEquivalently, using the logarithm,\n$$\n\\ln M_{X_{n}}(t)=-k\\ln\\!\\left(1-\\frac{t}{\\lambda n^{2}}\\right)=k\\frac{t}{\\lambda n^{2}}+o\\!\\left(\\frac{1}{n^{2}}\\right)\\to 0,\n$$\nso $M_{X_{n}}(t)\\to \\exp(0)=1$. The pointwise limit $M(t)\\equiv 1$ for all $t\\in\\mathbb{R}$ is the MGF of the degenerate distribution at $0$.\n\nTherefore, $X_{n}\\xrightarrow{d}0$, i.e., the limiting distribution is degenerate at $0$. This corresponds to option C. As a consistency check, $\\mathbb{E}[X_{n}]=k/(\\lambda n^{2})\\to 0$ and $\\text{Var}(X_{n})=k/(\\lambda^{2}n^{4})\\to 0$, which also indicates convergence to a point mass at $0$.", "answer": "$$\\boxed{C}$$", "id": "1910212"}]}