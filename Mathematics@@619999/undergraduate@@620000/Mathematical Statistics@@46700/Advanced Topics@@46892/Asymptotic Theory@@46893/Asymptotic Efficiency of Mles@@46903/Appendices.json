{"hands_on_practices": [{"introduction": "The journey into asymptotic efficiency begins with understanding how to quantify the amount of information a sample provides about an unknown parameter. This is precisely the role of Fisher information. This first exercise [@problem_id:1896446] will guide you through the fundamental mechanics of calculating Fisher information for a parameter in a continuous distribution, a foundational skill for evaluating the quality of estimators.", "problem": "In reliability engineering, the lifetime of certain components is often modeled using the Weibull distribution. The probability density function (PDF) for a Weibull-distributed random variable $X$ is given by\n$$f(x; k, \\lambda) = \\frac{k}{\\lambda} \\left(\\frac{x}{\\lambda}\\right)^{k-1} \\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^k\\right)$$\nfor $x \\ge 0$, where $k > 0$ is the shape parameter and $\\lambda > 0$ is the scale parameter.\n\nConsider a specific type of solid-state device whose lifetime, measured in hours, is known to follow a Weibull distribution with a shape parameter $k=2$. The scale parameter $\\lambda$ is unknown and is a characteristic of a particular manufacturing batch. To estimate this parameter, a random sample of $n$ devices, $X_1, X_2, \\ldots, X_n$, is taken from the batch and their lifetimes are recorded.\n\nCalculate the Fisher Information, $I(\\lambda)$, that this random sample provides about the unknown scale parameter $\\lambda$. Present your answer as a closed-form analytic expression in terms of $n$ and $\\lambda$.", "solution": "The devices have lifetimes modeled by a Weibull distribution with known shape parameter $k=2$ and unknown scale parameter $\\lambda>0$. For one observation $X$, the probability density function is\n$$\nf(x;2,\\lambda)=\\frac{2}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{1}\\exp\\!\\left(-\\left(\\frac{x}{\\lambda}\\right)^{2}\\right), \\quad x\\ge 0.\n$$\nThe log-likelihood for a single observation is\n$$\n\\ell(\\lambda\\mid x)=\\ln f(x;2,\\lambda) = \\ln 2 + \\ln x - 2\\ln \\lambda - \\frac{x^{2}}{\\lambda^{2}}.\n$$\nDifferentiate with respect to $\\lambda$ to obtain the score for one observation:\n$$\n\\frac{\\partial \\ell}{\\partial \\lambda} = -\\frac{2}{\\lambda} + 2 x^{2} \\lambda^{-3}.\n$$\nDifferentiate again to obtain the observed information (negative Hessian) for one observation:\n$$\n\\frac{\\partial^{2} \\ell}{\\partial \\lambda^{2}} = \\frac{2}{\\lambda^{2}} - 6 x^{2} \\lambda^{-4}.\n$$\nThe Fisher information for one observation is the negative expected second derivative:\n$$\nI_{1}(\\lambda) = -\\mathbb{E}\\!\\left[\\frac{\\partial^{2} \\ell}{\\partial \\lambda^{2}}\\right]\n= -\\left(\\frac{2}{\\lambda^{2}} - 6\\,\\mathbb{E}[X^{2}]\\,\\lambda^{-4}\\right).\n$$\nFor a Weibull distribution with shape $k=2$ and scale $\\lambda$, the moment $\\mathbb{E}[X^{r}]$ is given by $\\mathbb{E}[X^{r}] = \\lambda^{r}\\,\\Gamma\\!\\left(1+\\frac{r}{2}\\right)$. Setting $r=2$ gives\n$$\n\\mathbb{E}[X^{2}] = \\lambda^{2}\\,\\Gamma(2) = \\lambda^{2}.\n$$\nSubstitute into the information expression:\n$$\nI_{1}(\\lambda) = -\\frac{2}{\\lambda^{2}} + 6 \\cdot \\lambda^{2} \\cdot \\lambda^{-4}\n= -\\frac{2}{\\lambda^{2}} + \\frac{6}{\\lambda^{2}} = \\frac{4}{\\lambda^{2}}.\n$$\nFor $n$ independent observations, Fisher information adds, so the total information is\n$$\nI(\\lambda) = n\\,I_{1}(\\lambda) = \\frac{4n}{\\lambda^{2}}.\n$$", "answer": "$$\\boxed{\\frac{4n}{\\lambda^{2}}}$$", "id": "1896446"}, {"introduction": "With the ability to calculate Fisher information, we can now establish the CramÃ©r-Rao Lower Bound (CRLB), which sets a theoretical 'gold standard' for the precision of any unbiased estimator. This practice [@problem_id:1896440] challenges you to derive the CRLB in the context of a zero-truncated distribution, a scenario that mirrors real-world data collection where certain outcomes are systematically unobserved. Mastering this demonstrates how to adapt core statistical principles to more complex, practical problems.", "problem": "In the study of online engagement, a researcher models the number of comments received by posts on a popular social media platform. The researcher's dataset only includes posts that received at least one comment, effectively truncating the data at zero.\n\nThe number of comments, $X$, for any such post is modeled by a zero-truncated Poisson distribution. A random variable $X$ follows a zero-truncated Poisson distribution with parameter $\\lambda > 0$ if its probability mass function (PMF) is given by:\n$$P(X=k) = \\frac{\\lambda^k}{k! (\\exp(\\lambda) - 1)}$$\nfor $k = 1, 2, 3, \\ldots$.\n\nSuppose the researcher collects a random sample of $n$ such posts, with comment counts $X_1, X_2, \\ldots, X_n$, each independently and identically drawn from this distribution.\n\nFind the Cramer-Rao Lower Bound (CRLB) for the variance of any unbiased estimator of the parameter $\\lambda$. Present your answer as an expression in terms of $n$ and $\\lambda$.", "solution": "For one observation with PMF $p(x;\\lambda)=\\lambda^{x}\\big/(x!\\,(\\exp(\\lambda)-1))$ for $x\\in\\{1,2,\\ldots\\}$, the log-likelihood is\n$$\n\\ell(\\lambda;x)=x\\ln\\lambda-\\ln x!-\\ln\\big(\\exp(\\lambda)-1\\big).\n$$\nDifferentiate to obtain the score:\n$$\n\\frac{\\partial \\ell}{\\partial \\lambda}=\\frac{x}{\\lambda}-\\frac{\\exp(\\lambda)}{\\exp(\\lambda)-1}.\n$$\nDifferentiate again to get the observed information:\n$$\n\\frac{\\partial^{2}\\ell}{\\partial \\lambda^{2}}=-\\frac{x}{\\lambda^{2}}+\\frac{\\exp(\\lambda)}{\\big(\\exp(\\lambda)-1\\big)^{2}}.\n$$\nThus the Fisher information for one observation is\n$$\nI_{1}(\\lambda)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2}\\ell}{\\partial \\lambda^{2}}\\right]\n=\\frac{\\mathbb{E}[X]}{\\lambda^{2}}-\\frac{\\exp(\\lambda)}{\\big(\\exp(\\lambda)-1\\big)^{2}}.\n$$\nFor the zero-truncated Poisson, using that $X^{m}\\mathbf{1}_{\\{X\\ge 1\\}}=X^{m}$ for $m\\ge 1$ and the underlying Poisson moments, we have\n$$\n\\mathbb{E}[X]=\\frac{\\lambda}{1-\\exp(-\\lambda)}.\n$$\nSubstitute into $I_{1}(\\lambda)$:\n$$\nI_{1}(\\lambda)=\\frac{1}{\\lambda\\big(1-\\exp(-\\lambda)\\big)}-\\frac{\\exp(\\lambda)}{\\big(\\exp(\\lambda)-1\\big)^{2}}.\n$$\nUsing $\\frac{\\exp(\\lambda)}{(\\exp(\\lambda)-1)^{2}}=\\frac{\\exp(-\\lambda)}{\\big(1-\\exp(-\\lambda)\\big)^{2}}$, bring to a common denominator to simplify:\n$$\nI_{1}(\\lambda)=\\frac{1-\\big(1+\\lambda\\big)\\exp(-\\lambda)}{\\lambda\\big(1-\\exp(-\\lambda)\\big)^{2}}.\n$$\nFor an i.i.d. sample of size $n$, the Fisher information is $I_{n}(\\lambda)=n\\,I_{1}(\\lambda)$. Therefore, the Cramer-Rao Lower Bound for any unbiased estimator of $\\lambda$ is\n$$\n\\operatorname{Var}(\\hat{\\lambda})\\ge \\frac{1}{I_{n}(\\lambda)}=\\frac{\\lambda\\big(1-\\exp(-\\lambda)\\big)^{2}}{n\\left[1-\\big(1+\\lambda\\big)\\exp(-\\lambda)\\right]}.\n$$", "answer": "$$\\boxed{\\frac{\\lambda\\left(1-\\exp(-\\lambda)\\right)^{2}}{n\\left[1-(1+\\lambda)\\exp(-\\lambda)\\right]}}$$", "id": "1896440"}, {"introduction": "Our analysis often extends beyond estimating a model's direct parameters, such as a rate $\\theta$, to functions of those parameters, like the median lifetime. The Delta Method is the essential tool for determining the asymptotic properties of these transformed estimators. In this final exercise [@problem_id:1896441], you will apply the Delta Method to find the variance of a median estimator, a powerful technique that significantly broadens the scope of our inferential capabilities.", "problem": "The lifetime of a certain electronic component is modeled by an exponential distribution with an unknown rate parameter $\\theta > 0$. The probability density function for the lifetime $T$ is given by $f(t; \\theta) = \\theta \\exp(-\\theta t)$ for $t \\geq 0$. A random sample of $n$ components is tested, and their lifetimes $T_1, T_2, \\dots, T_n$ are recorded.\n\nThe median lifetime is a key reliability metric. The estimator for the median, constructed from the Maximum Likelihood Estimator (MLE) of the rate parameter, $\\hat{\\theta}_{MLE}$, is given by $\\hat{M} = \\frac{\\ln(2)}{\\hat{\\theta}_{MLE}}$. Based on large sample theory, we know that for a large sample size $n$, the sampling distribution of $\\hat{\\theta}_{MLE}$ is approximately normal.\n\nDetermine the asymptotic variance of the median estimator, $\\hat{M}$. Express your answer as a symbolic expression in terms of the sample size $n$ and the true rate parameter $\\theta$.", "solution": "Let $T_{1},\\dots,T_{n}$ be i.i.d. with density $f(t;\\theta)=\\theta \\exp(-\\theta t)$ for $t\\geq 0$. The log-likelihood for one observation is $\\ell(\\theta)=\\ln\\theta-\\theta T$. Then\n$$\n\\frac{\\partial \\ell}{\\partial \\theta}=\\frac{1}{\\theta}-T,\\qquad \\frac{\\partial^{2}\\ell}{\\partial \\theta^{2}}=-\\frac{1}{\\theta^{2}}.\n$$\nThe Fisher information for one observation is\n$$\nI_{1}(\\theta)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2}\\ell}{\\partial \\theta^{2}}\\right]=\\frac{1}{\\theta^{2}},\n$$\nand for $n$ observations $I_{n}(\\theta)=n/\\theta^{2}$. By standard MLE asymptotics,\n$$\n\\sqrt{n}\\,(\\hat{\\theta}_{\\text{MLE}}-\\theta)\\xrightarrow{d}\\mathcal{N}\\!\\left(0,\\;I_{1}(\\theta)^{-1}\\right)=\\mathcal{N}\\!\\left(0,\\;\\theta^{2}\\right),\n$$\nso $\\operatorname{Var}_{\\text{asym}}(\\hat{\\theta}_{\\text{MLE}})=\\theta^{2}/n$.\n\nThe population median is $M(\\theta)=\\ln(2)/\\theta$. Define $g(\\theta)=\\ln(2)/\\theta$. Then\n$$\ng'(\\theta)=-\\frac{\\ln(2)}{\\theta^{2}}.\n$$\nBy the delta method,\n$$\n\\sqrt{n}\\,\\big(g(\\hat{\\theta}_{\\text{MLE}})-g(\\theta)\\big)\\xrightarrow{d}\\mathcal{N}\\!\\left(0,\\;\\big(g'(\\theta)\\big)^{2}\\,\\theta^{2}\\right),\n$$\nwhich implies the asymptotic variance\n$$\n\\operatorname{Var}_{\\text{asym}}(\\hat{M})=\\big(g'(\\theta)\\big)^{2}\\,\\frac{\\theta^{2}}{n}=\\frac{(\\ln 2)^{2}}{\\theta^{4}}\\cdot\\frac{\\theta^{2}}{n}=\\frac{(\\ln 2)^{2}}{n\\,\\theta^{2}}.\n$$", "answer": "$$\\boxed{\\frac{(\\ln 2)^{2}}{n\\,\\theta^{2}}}$$", "id": "1896441"}]}