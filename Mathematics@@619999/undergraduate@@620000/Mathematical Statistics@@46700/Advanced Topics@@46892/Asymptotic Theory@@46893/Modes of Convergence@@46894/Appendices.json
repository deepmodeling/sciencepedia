{"hands_on_practices": [{"introduction": "We begin our exploration with two fundamental concepts: convergence in probability and convergence in the mean ($L^1$). This exercise uses a simple sequence of Bernoulli random variables to provide a concrete, hands-on application of their definitions [@problem_id:1936898]. By working through this problem, you will build an intuitive understanding of what it means for a sequence of random variables to approach a limiting value.", "problem": "Let $\\{X_n\\}_{n=1}^{\\infty}$ be a sequence of independent random variables. For each integer $n \\ge 1$, the random variable $X_n$ follows a Bernoulli distribution with parameter $p_n = \\frac{1}{n}$. A random variable following a Bernoulli distribution with parameter $p$ takes the value 1 with probability $p$ and the value 0 with probability $1-p$.\n\nConsider the following two statements regarding the convergence of the sequence $\\{X_n\\}$ to the constant random variable $X=0$:\n\nI. The sequence $\\{X_n\\}$ converges to $0$ in probability. This mode of convergence holds if for any constant $\\epsilon  0$, the following condition is met: $\\lim_{n \\to \\infty} \\mathbb{P}(|X_n - 0|  \\epsilon) = 0$.\n\nII. The sequence $\\{X_n\\}$ converges to $0$ in $L^1$. This mode of convergence holds if the following condition is met: $\\lim_{n \\to \\infty} \\mathbb{E}[|X_n - 0|] = 0$.\n\nWhich of these statements is/are true?\n\nA. Neither I nor II.\n\nB. I only.\n\nC. II only.\n\nD. Both I and II.", "solution": "We are given independent random variables $X_{n} \\sim \\text{Bernoulli}(p_{n})$ with $p_{n} = \\frac{1}{n}$, and we consider convergence to the constant random variable $X=0$.\n\nTo analyze Statement I (convergence in probability), fix any $\\epsilon  0$. Since $X_{n} \\in \\{0,1\\}$, we have\n$$\n|X_{n} - 0|  \\epsilon \\iff\n\\begin{cases}\nX_{n} = 1,  \\text{if } 0  \\epsilon \\leq 1, \\\\\n\\text{never},  \\text{if } \\epsilon  1.\n\\end{cases}\n$$\nThus,\n$$\n\\mathbb{P}(|X_{n} - 0|  \\epsilon) =\n\\begin{cases}\n\\mathbb{P}(X_{n} = 1) = p_{n} = \\frac{1}{n},  \\text{if } 0  \\epsilon \\leq 1, \\\\\n0,  \\text{if } \\epsilon  1.\n\\end{cases}\n$$\nTaking limits yields, for any fixed $\\epsilon  0$,\n$$\n\\lim_{n \\to \\infty} \\mathbb{P}(|X_{n} - 0|  \\epsilon) =\n\\begin{cases}\n\\lim_{n \\to \\infty} \\frac{1}{n} = 0,  \\text{if } 0  \\epsilon \\leq 1, \\\\\n0,  \\text{if } \\epsilon  1,\n\\end{cases}\n$$\nso $X_{n} \\to 0$ in probability. Hence Statement I is true.\n\nTo analyze Statement II (convergence in $L^{1}$), note that $|X_{n} - 0| = X_{n}$ since $X_{n} \\geq 0$. For a $\\text{Bernoulli}(p_{n})$ variable, $\\mathbb{E}[X_{n}] = p_{n}$. Therefore,\n$$\n\\mathbb{E}[|X_{n} - 0|] = \\mathbb{E}[X_{n}] = p_{n} = \\frac{1}{n},\n$$\nand hence\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}[|X_{n} - 0|] = \\lim_{n \\to \\infty} \\frac{1}{n} = 0.\n$$\nThus $X_{n} \\to 0$ in $L^{1}$, so Statement II is also true.\n\nTherefore, both statements I and II are true.", "answer": "$$\\boxed{D}$$", "id": "1936898"}, {"introduction": "Building on our foundation, we now investigate stronger modes of convergence, including mean square ($L^2$) and almost sure convergence. This practice problem considers a sequence of normally distributed random variables and demonstrates a case where multiple convergence modes hold simultaneously [@problem_id:1319236]. It's a valuable exercise for understanding the hierarchical relationship between these modes, particularly how convergence in mean square implies convergence in probability.", "problem": "Let $\\{X_n\\}_{n=1}^{\\infty}$ be a sequence of independent random variables. For each positive integer $n$, the random variable $X_n$ is drawn from a Normal distribution with a mean of $\\frac{1}{n}$ and a variance of $\\frac{1}{n^2}$.\n\nConsider the following statements regarding the limiting behavior of this sequence as $n$ approaches infinity:\n\nI. The sequence converges in probability to 0.\nII. The sequence converges in mean square to 0.\nIII. The sequence converges almost surely to 0.\n\nWhich of the above statements is/are correct?\n\nA. I only\n\nB. II only\n\nC. I and II only\n\nD. I and III only\n\nE. II and III only\n\nF. I, II, and III\n\nG. None are correct", "solution": "Let $X_{n} \\sim \\mathcal{N}(\\mu_{n},\\sigma_{n}^{2})$ with $\\mu_{n}=\\frac{1}{n}$ and $\\sigma_{n}^{2}=\\frac{1}{n^{2}}$.\n\nTo check convergence in mean square to $0$, compute\n$$\n\\mathbb{E}\\big[(X_{n}-0)^{2}\\big]=\\operatorname{Var}(X_{n})+\\big(\\mathbb{E}[X_{n}]\\big)^{2}=\\frac{1}{n^{2}}+\\frac{1}{n^{2}}=\\frac{2}{n^{2}} \\xrightarrow[n\\to\\infty]{} 0.\n$$\nThus $X_{n} \\to 0$ in mean square, so Statement II is true.\n\nConvergence in mean square implies convergence in probability. Explicitly, for any $\\epsilon0$, by Chebyshev (or Markov) inequality,\n$$\n\\mathbb{P}(|X_{n}|\\epsilon)\\leq \\frac{\\mathbb{E}[X_{n}^{2}]}{\\epsilon^{2}}=\\frac{2}{\\epsilon^{2}n^{2}} \\xrightarrow[n\\to\\infty]{} 0,\n$$\nso $X_{n} \\to 0$ in probability, and Statement I is true.\n\nFor almost sure convergence, fix $\\epsilon0$ and define $Y_{n}=nX_{n}$. Then $Y_{n}\\sim \\mathcal{N}(1,1)$ for all $n$, and\n$$\n\\mathbb{P}(|X_{n}|\\epsilon)=\\mathbb{P}(|Y_{n}|n\\epsilon).\n$$\nLet $W\\sim \\mathcal{N}(0,1)$. For $t1$, $\\mathbb{P}(|Y_{n}|t)=\\mathbb{P}(|W+1|t)\\leq \\mathbb{P}(|W|t-1)\\leq 2\\exp\\big(-\\frac{(t-1)^{2}}{2}\\big)$. For $n$ large so that $n\\epsilon1$,\n$$\n\\mathbb{P}(|X_{n}|\\epsilon)\\leq 2\\exp\\big(-\\tfrac{(n\\epsilon-1)^{2}}{2}\\big).\n$$\nHence\n$$\n\\sum_{n=1}^{\\infty}\\mathbb{P}(|X_{n}|\\epsilon)\\infty\n$$\nby comparison with a Gaussian-decay series. By the first Borel-Cantelli lemma, $\\mathbb{P}(|X_{n}|\\epsilon \\text{ i.o.})=0$, so $X_{n}\\to 0$ almost surely. Therefore Statement III is true.\n\nAll three statements I, II, III are correct.", "answer": "$$\\boxed{F}$$", "id": "1319236"}, {"introduction": "A deep understanding of convergence requires knowing not just the rules, but also the exceptions. This final practice presents a classic counterexample to illustrate that convergence in probability does not necessarily imply convergence of moments, such as convergence in mean square ($L^2$) [@problem_id:798687]. Analyzing this hypothetical scenario is crucial for appreciating the subtle but important distinctions between the different modes of convergence and why stronger conditions are sometimes required in theoretical results.", "problem": "Consider a sequence of discrete random variables $\\{X_n\\}_{n=1}^\\infty$. For each $n \\in \\mathbb{N}$, the random variable $X_n$ has the following probability mass function:\n$$\nP(X_n = x) = \\begin{cases}\n\\frac{1}{n}  \\text{if } x = \\sqrt{n} \\\\\n1 - \\frac{1}{n}  \\text{if } x = 0 \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nIt is a known result that this sequence of random variables converges in probability to the constant random variable $X=0$. That is, $X_n \\xrightarrow{\\mathbb{P}} 0$.\n\nHowever, convergence in probability does not in general imply convergence of moments. To demonstrate this for the given sequence, calculate the limit of the second moment of $X_n$ as $n$ approaches infinity. That is, derive the value of $\\lim_{n \\to \\infty} \\mathbb{E}[X_n^2]$.", "solution": "The problem asks for the calculation of $\\lim_{n \\to \\infty} \\mathbb{E}[X_n^2]$ for a given sequence of random variables $\\{X_n\\}$.\n\nFirst, we determine the second moment, $\\mathbb{E}[X_n^2]$, for a fixed $n$. The expectation of a function of a discrete random variable, $g(X_n)$, is given by the formula $\\mathbb{E}[g(X_n)] = \\sum_x g(x) \\mathbb{P}(X_n = x)$. In this case, the function is $g(x) = x^2$.\n\nThe possible values for $X_n$ are $\\sqrt{n}$ and $0$. The corresponding values for $X_n^2$ are $(\\sqrt{n})^2 = n$ and $0^2=0$.\n\nUsing the definition of expectation, we compute $\\mathbb{E}[X_n^2]$:\n$$\n\\mathbb{E}[X_n^2] = (\\sqrt{n})^2 \\cdot \\mathbb{P}(X_n = \\sqrt{n}) + (0)^2 \\cdot \\mathbb{P}(X_n = 0)\n$$\n\nWe substitute the given probabilities from the probability mass function: $\\mathbb{P}(X_n = \\sqrt{n}) = \\frac{1}{n}$ and $\\mathbb{P}(X_n = 0) = 1 - \\frac{1}{n}$.\n$$\n\\mathbb{E}[X_n^2] = n \\cdot \\left(\\frac{1}{n}\\right) + 0 \\cdot \\left(1 - \\frac{1}{n}\\right)\n$$\n\nPerforming the multiplication gives:\n$$\n\\mathbb{E}[X_n^2] = 1 + 0\n$$\n$$\n\\mathbb{E}[X_n^2] = 1\n$$\nThis expression for the second moment, $\\mathbb{E}[X_n^2]$, is a constant value of 1 for all $n \\in \\mathbb{N}$.\n\nNext, we need to find the limit of this expression as $n$ approaches infinity.\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}[X_n^2] = \\lim_{n \\to \\infty} 1\n$$\nThe limit of a constant is the constant itself.\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}[X_n^2] = 1\n$$\n\nFor context, let's compare this to the second moment of the limit random variable. The problem states that $X_n$ converges in probability to the constant random variable $X=0$. The second moment of this limit random variable is:\n$$\n\\mathbb{E}[X^2] = \\mathbb{E}[0^2] = \\mathbb{E}[0] = 0\n$$\nSince $\\lim_{n \\to \\infty} \\mathbb{E}[X_n^2] = 1$ and $\\mathbb{E}[(\\lim_{n \\to \\infty} X_n)^2] = \\mathbb{E}[X^2] = 0$, we have:\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}[X_n^2] \\neq \\mathbb{E}[(\\lim_{n \\to \\infty} X_n)^2]\n$$\nThis confirms that for this sequence, convergence in probability does not imply convergence in the second moment ($L^2$ convergence). The value requested by the problem is the limit of the second moments.", "answer": "$$\n\\boxed{1}\n$$", "id": "798687"}]}