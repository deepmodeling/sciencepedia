{"hands_on_practices": [{"introduction": "The Law of Large Numbers tells us that the simple average of many independent trials converges to the expected value. But what happens if the average is weighted, giving more importance to later observations? This exercise [@problem_id:1910722] challenges you to extend your understanding beyond standard theorems. By calculating the mean and variance of a weighted average and applying Chebyshev's inequality, you will develop a deeper, more flexible grasp of why and how convergence in probability occurs.", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\dots, X_n, \\dots$. These variables share a common finite mean $E[X_i] = \\mu$ and a common finite, non-zero variance $\\text{Var}(X_i) = \\sigma^2$.\n\nA new sequence of random variables, $Y_n$, is constructed as a weighted average of the first $n$ variables from the sequence:\n$$Y_n = \\frac{\\sum_{i=1}^n i X_i}{\\sum_{i=1}^n i}$$\nThis weighted average gives linearly increasing importance to later observations in the sequence. The sequence $Y_n$ can be shown to converge in probability to a specific constant value as $n \\to \\infty$.\n\nDetermine this constant value. Your answer should be an analytic expression in terms of the given parameters.", "solution": "Define the normalizing sum $S_{n}=\\sum_{i=1}^{n} i=\\frac{n(n+1)}{2}$. Then the weighted average can be written as\n$$\nY_{n}=\\sum_{i=1}^{n}\\frac{i}{S_{n}}X_{i}.\n$$\nCompute its expectation using linearity of expectation and the common mean $E[X_{i}]=\\mu$:\n$$\nE[Y_{n}]=\\sum_{i=1}^{n}\\frac{i}{S_{n}}E[X_{i}]=\\mu\\sum_{i=1}^{n}\\frac{i}{S_{n}}=\\mu.\n$$\nCompute its variance using independence, $\\text{Var}(X_{i})=\\sigma^{2}$, and $\\text{Cov}(X_{i},X_{j})=0$ for $i\\neq j$:\n$$\n\\text{Var}(Y_{n})=\\sum_{i=1}^{n}\\left(\\frac{i}{S_{n}}\\right)^{2}\\text{Var}(X_{i})=\\sigma^{2}\\frac{\\sum_{i=1}^{n} i^{2}}{S_{n}^{2}}.\n$$\nUse the identity $\\sum_{i=1}^{n} i^{2}=\\frac{n(n+1)(2n+1)}{6}$ and $S_{n}^{2}=\\left(\\frac{n(n+1)}{2}\\right)^{2}=\\frac{n^{2}(n+1)^{2}}{4}$ to obtain\n$$\n\\text{Var}(Y_{n})=\\sigma^{2}\\cdot\\frac{\\frac{n(n+1)(2n+1)}{6}}{\\frac{n^{2}(n+1)^{2}}{4}}\n=\\sigma^{2}\\cdot\\frac{4(2n+1)}{6n(n+1)}\n=\\sigma^{2}\\cdot\\frac{2(2n+1)}{3n(n+1)}.\n$$\nSince $\\frac{2(2n+1)}{3n(n+1)}\\to 0$ as $n\\to\\infty$, it follows that $\\text{Var}(Y_{n})\\to 0$. By Chebyshevâ€™s inequality, for any $\\varepsilon>0$,\n$$\nP\\big(|Y_{n}-\\mu|>\\varepsilon\\big)=P\\big(|Y_{n}-E[Y_{n}]|>\\varepsilon\\big)\\leq \\frac{\\text{Var}(Y_{n})}{\\varepsilon^{2}}\\to 0.\n$$\nTherefore $Y_{n}\\to \\mu$ in probability. The constant limit is $\\mu$.", "answer": "$$\\boxed{\\mu}$$", "id": "1910722"}, {"introduction": "The Law of Large Numbers is a pillar of probability and statistics, but it rests on a crucial assumption: the existence of a finite expected value. This practice explores what happens when that foundation is removed, using the infamous Cauchy distribution [@problem_id:1353353]. By analyzing the sample mean of Cauchy-distributed variables, you will discover a fascinating counterexample where more data does not lead to convergence, underscoring the vital importance of verifying the conditions behind theoretical guarantees.", "problem": "In a data analysis scenario, a sequence of independent and identically distributed (i.i.d.) measurements $X_1, X_2, \\dots, X_n$ is recorded. Each measurement $X_i$ is a random variable drawn from a standard Cauchy distribution. The probability density function (PDF) for a standard Cauchy distribution is given by:\n$$f(x) = \\frac{1}{\\pi(1+x^2)}$$\nfor $-\\infty < x < \\infty$.\n\nThe sample mean of these measurements is calculated as $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$.\n\nConsider the behavior of the sample mean $\\bar{X}_n$ in the limit as the number of measurements $n$ approaches infinity. To what does the sequence of random variables $\\{\\bar{X}_n\\}$ converge in distribution?\n\nA. The constant 0.\n\nB. A random variable with a standard Cauchy distribution.\n\nC. A random variable with a standard Normal distribution.\n\nD. It does not converge in distribution to any random variable or constant.\n\nE. A random variable with a Student's t-distribution with $n-1$ degrees of freedom.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. standard Cauchy random variables with density $f(x)=\\frac{1}{\\pi(1+x^{2})}$. Denote $S_{n}=\\sum_{i=1}^{n}X_{i}$ and $\\bar{X}_{n}=\\frac{S_{n}}{n}$.\n\nA key tool is the characteristic function. The characteristic function of a standard Cauchy random variable $X$ is the known formula\n$$\n\\varphi_{X}(t)=\\exp(-|t|),\\quad t\\in\\mathbb{R}.\n$$\nBecause $X_{1},\\dots,X_{n}$ are independent and identically distributed, the characteristic function of their sum is the product of the individual characteristic functions:\n$$\n\\varphi_{S_{n}}(t)=\\prod_{i=1}^{n}\\varphi_{X_{i}}(t)=[\\varphi_{X}(t)]^{n}=\\exp(-n|t|).\n$$\nThe function $t\\mapsto \\exp(-\\gamma|t|)$ is the characteristic function of a Cauchy distribution with location $0$ and scale parameter $\\gamma$. Hence $S_{n}$ has a Cauchy distribution with scale parameter $n$.\n\nFor the sample mean $\\bar{X}_{n}=S_{n}/n$, the characteristic function is, by the scaling property of characteristic functions,\n$$\n\\varphi_{\\bar{X}_{n}}(t)=\\varphi_{S_{n}}(t/n)=\\exp\\!\\left(-n\\left|\\frac{t}{n}\\right|\\right)=\\exp(-|t|).\n$$\nThis is exactly the characteristic function of a standard Cauchy random variable. Therefore, for every $n$,\n$$\n\\bar{X}_{n}\\stackrel{d}{=}\\text{Cauchy}(0,1).\n$$\nConsequently, the sequence $\\{\\bar{X}_{n}\\}$ does not change its distribution with $n$ and thus converges in distribution (trivially) to a standard Cauchy random variable. This also shows that the classical central limit theorem does not apply here because the Cauchy distribution has no finite variance.\n\nTherefore, the correct option is B.", "answer": "$$\\boxed{B}$$", "id": "1353353"}, {"introduction": "Convergence in probability is a precise mathematical concept, and it's essential to distinguish it from other, stronger types of convergence. This problem introduces the classic \"typewriter\" sequence [@problem_id:1293189], a carefully designed thought experiment to illuminate the difference between convergence in probability and almost sure convergence. Working through this example will reveal that while the probability of being non-zero shrinks to nothing, for any given outcome, the sequence never actually settles down, providing a profound insight into the subtleties of stochastic convergence.", "problem": "Consider a probability space $(\\Omega, \\mathcal{F}, P)$ where $\\Omega = [0, 1]$, $\\mathcal{F}$ is the Borel sigma-algebra on $[0, 1]$, and $P$ is the uniform (Lebesgue) measure on $[0, 1]$. We define a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ as follows. For any integer $n \\ge 1$, there exists a unique pair of non-negative integers $(k, j)$ such that $n = 2^k + j$, where $0 \\le j < 2^k$. The random variable $X_n$ is defined by its value at $\\omega \\in \\Omega$:\n$$\nX_n(\\omega) =\n\\begin{cases}\n1 & \\text{if } \\omega \\in \\left[\\frac{j}{2^k}, \\frac{j+1}{2^k}\\right] \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThis sequence can be visualized as an indicator function of a \"sweeping\" interval that repeatedly passes over the entire space $[0, 1]$ with decreasing width. Let $X$ be the random variable that is identically zero, i.e., $X(\\omega)=0$ for all $\\omega \\in \\Omega$.\n\nWhich of the following statements correctly describes the convergence of the sequence $\\{X_n\\}$ to $X=0$?\n\nA. The sequence $\\{X_n\\}$ converges to 0 almost surely, but not in probability.\n\nB. The sequence $\\{X_n\\}$ converges to 0 in probability, but not almost surely.\n\nC. The sequence $\\{X_n\\}$ converges to 0 both almost surely and in probability.\n\nD. The sequence $\\{X_n\\}$ converges to 0 neither almost surely nor in probability.", "solution": "We recall the definitions. For each integer $n \\geq 1$, there is a unique pair $(k,j)$ with $k \\in \\mathbb{Z}_{\\ge 0}$ and $0 \\le j < 2^{k}$ such that $n = 2^{k} + j$. Then\n$$\nX_{n}(\\omega) = \\mathbf{1}_{\\left[\\frac{j}{2^{k}},\\, \\frac{j+1}{2^{k}}\\right]}(\\omega).\n$$\nLet $X \\equiv 0$. We analyze almost sure and in-probability convergence to $X$.\n\nAlmost sure convergence: By definition, $\\{X_{n}\\}$ converges almost surely to $0$ if\n$$\nP\\big(\\{\\omega \\in \\Omega : \\lim_{n \\to \\infty} X_{n}(\\omega) = 0\\}\\big) = 1.\n$$\nFix any $\\omega \\in [0,1]$. For each $k \\in \\mathbb{Z}_{\\ge 0}$, the family $\\left\\{\\left[\\frac{j}{2^{k}}, \\frac{j+1}{2^{k}}\\right] : j = 0,1,\\dots,2^{k}-1\\right\\}$ covers $[0,1]$, so there exists $j_{k} \\in \\{0,\\dots,2^{k}-1\\}$ such that\n$$\n\\omega \\in \\left[\\frac{j_{k}}{2^{k}}, \\frac{j_{k}+1}{2^{k}}\\right].\n$$\nDefine $n_{k} = 2^{k} + j_{k}$. By construction, $X_{n_{k}}(\\omega) = 1$ for every $k$. Hence, for every $\\omega$, there are infinitely many $n$ such that $X_{n}(\\omega) = 1$. Moreover, in each block $\\{2^{k},\\dots,2^{k+1}-1\\}$ there are at most two indices with $X_{n}(\\omega) = 1$ (exactly one if $\\omega$ is not a dyadic rational with denominator $2^{k}$), and the rest yield $X_{n}(\\omega) = 0$, so there are also infinitely many $n$ with $X_{n}(\\omega) = 0$. Therefore, for every $\\omega \\in [0,1]$, the sequence $\\{X_{n}(\\omega)\\}$ takes the values $1$ and $0$ infinitely often and does not converge (in particular, it does not converge to $0$). Consequently,\n$$\nP\\big(\\{\\omega : \\lim_{n \\to \\infty} X_{n}(\\omega) = 0\\}\\big) = 0 \\neq 1,\n$$\nso $X_{n} \\not\\to 0$ almost surely.\n\nConvergence in probability: By definition, $X_{n} \\to 0$ in probability if for every $\\epsilon > 0$,\n$$\n\\lim_{n \\to \\infty} P\\big(|X_{n} - 0| > \\epsilon\\big) = 0.\n$$\nSince $X_{n}$ takes only the values $0$ and $1$, for any $\\epsilon \\in (0,1]$,\n$$\n\\{|X_{n} - 0| > \\epsilon\\} = \\{X_{n} = 1\\},\n$$\nso\n$$\nP\\big(|X_{n}| > \\epsilon\\big) = P(X_{n} = 1).\n$$\nGiven $n = 2^{k} + j$, $X_{n}$ is the indicator of an interval of length $2^{-k}$, and $P$ is Lebesgue measure on $[0,1]$. Therefore\n$$\nP(X_{n} = 1) = 2^{-k}.\n$$\nLet $k(n)$ denote the $k$ associated with $n$. As $n \\to \\infty$, necessarily $k(n) \\to \\infty$ because $n \\in \\{2^{k},\\dots,2^{k+1}-1\\}$ and these blocks shift to larger $k$. Hence,\n$$\n\\lim_{n \\to \\infty} P\\big(|X_{n}| > \\epsilon\\big) = \\lim_{n \\to \\infty} 2^{-k(n)} = 0 \\quad \\text{for all } \\epsilon \\in (0,1].\n$$\nFor $\\epsilon > 1$, $P(|X_{n}| > \\epsilon) = 0$ for all $n$, so the limit is also $0$. Thus $X_{n} \\to 0$ in probability.\n\nCombining both parts, the correct statement is that $\\{X_{n}\\}$ converges to $0$ in probability but not almost surely.", "answer": "$$\\boxed{B}$$", "id": "1293189"}]}