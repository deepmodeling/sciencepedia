{"hands_on_practices": [{"introduction": "To truly grasp a new method, it's often helpful to connect it to concepts you already know. This first exercise explores the behavior of Lasso regression at a critical boundary: when the tuning parameter, $\\lambda$, is set to zero. By working through this problem, you will see how the Lasso objective function simplifies and becomes equivalent to Ordinary Least Squares (OLS), reinforcing that Lasso is a powerful extension of this fundamental regression technique. [@problem_id:1928607]", "problem": "In statistical modeling, the Least Absolute Shrinkage and Selection Operator (LASSO) is a regression analysis method that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model it produces. For a general linear model with $p$ predictors, the LASSO estimate for the coefficients $\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_p)$ is the value that minimizes the following objective function:\n$$\n\\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j| = \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j\\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n$$\nwhere $(x_{i1}, \\dots, x_{ip}, y_i)$ are the observed data for $i=1, \\dots, n$, $\\beta_0$ is the intercept, and $\\lambda \\ge 0$ is a tuning parameter.\n\nA data scientist is analyzing a dataset to fit a simple linear regression model of the form $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$. They decide to use the LASSO framework to determine the model coefficients. However, as an initial exploratory step, they set the tuning parameter $\\lambda$ to exactly zero. The dataset consists of the following four $(x_i, y_i)$ data points:\n$$\n(1, 2), (2, 3), (3, 5), (4, 6)\n$$\nCalculate the value of the coefficient estimate $\\hat{\\beta}_1$ obtained under this specific setting. Round your final answer to three significant figures.", "solution": "Setting $\\lambda=0$ reduces the LASSO objective to the residual sum of squares, so the minimizing estimator is the ordinary least squares (OLS) solution. For simple linear regression with an intercept, the OLS slope is\n$$\n\\hat{\\beta}_{1}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}},\n$$\nwhere $\\bar{x}$ and $\\bar{y}$ are the sample means. For the data $(1,2),(2,3),(3,5),(4,6)$ with $n=4$,\n$$\n\\bar{x}=\\frac{1+2+3+4}{4}=\\frac{10}{4}=\\frac{5}{2},\\quad \\bar{y}=\\frac{2+3+5+6}{4}=\\frac{16}{4}=4.\n$$\nCompute the numerator:\n$$\n\\sum_{i=1}^{4}(x_{i}-\\bar{x})(y_{i}-\\bar{y})=(1-\\tfrac{5}{2})(2-4)+(2-\\tfrac{5}{2})(3-4)+(3-\\tfrac{5}{2})(5-4)+(4-\\tfrac{5}{2})(6-4).\n$$\nWriting all fractions with $\\frac{\\cdot}{\\cdot}$,\n$$\n(1-\\tfrac{5}{2})=-\\frac{3}{2},\\ (2-4)=-2\\ \\Rightarrow\\ \\left(-\\frac{3}{2}\\right)(-2)=3,\\\\\n(2-\\tfrac{5}{2})=-\\frac{1}{2},\\ (3-4)=-1\\ \\Rightarrow\\ \\left(-\\frac{1}{2}\\right)(-1)=\\frac{1}{2},\\\\\n(3-\\tfrac{5}{2})=\\frac{1}{2},\\ (5-4)=1\\ \\Rightarrow\\ \\left(\\frac{1}{2}\\right)(1)=\\frac{1}{2},\\\\\n(4-\\tfrac{5}{2})=\\frac{3}{2},\\ (6-4)=2\\ \\Rightarrow\\ \\left(\\frac{3}{2}\\right)(2)=3.\n$$\nSumming gives\n$$\n\\sum_{i=1}^{4}(x_{i}-\\bar{x})(y_{i}-\\bar{y})=3+\\frac{1}{2}+\\frac{1}{2}+3=7.\n$$\nCompute the denominator:\n$$\n\\sum_{i=1}^{4}(x_{i}-\\bar{x})^{2}=\\left(-\\frac{3}{2}\\right)^{2}+\\left(-\\frac{1}{2}\\right)^{2}+\\left(\\frac{1}{2}\\right)^{2}+\\left(\\frac{3}{2}\\right)^{2}=\\frac{9}{4}+\\frac{1}{4}+\\frac{1}{4}+\\frac{9}{4}=\\frac{20}{4}=5.\n$$\nTherefore,\n$$\n\\hat{\\beta}_{1}=\\frac{7}{5}=1.4.\n$$\nRounding to three significant figures yields $1.40$.", "answer": "$$\\boxed{1.40}$$", "id": "1928607"}, {"introduction": "Having seen what happens when the Lasso penalty is non-existent, we now investigate the opposite extreme. What happens when the penalty term, controlled by $\\lambda$, becomes overwhelmingly large? This conceptual exercise will help you build intuition about the \"shrinkage\" aspect of Lasso and understand its capacity for aggressive model simplification, which is crucial for interpretability and avoiding overfitting. [@problem_id:1928648]", "problem": "In the context of a linear model $y = \\beta_0 + \\sum_{j=1}^{p} x_j \\beta_j + \\epsilon$, the Least Absolute Shrinkage and Selection Operator (LASSO) method is used to find estimates for the coefficients $(\\beta_0, \\beta_1, \\dots, \\beta_p)$. These estimates are the values that minimize the following objective function:\n$$ \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| $$\nHere, $(y_i, x_{i1}, \\dots, x_{ip})$ for $i=1, \\dots, n$ are the observed data points. The term $\\lambda \\ge 0$ is a non-negative tuning parameter that controls the strength of the penalty. Note that the intercept term, $\\beta_0$, is not included in the penalty term.\n\nConsider what happens to the estimated coefficients as the tuning parameter $\\lambda$ is increased towards a very large value (i.e., as $\\lambda \\to \\infty$). Which of the following statements correctly describes the limiting behavior of the LASSO coefficient estimates?\n\nA. All coefficient estimates, including the intercept $\\hat{\\beta}_0$, are forced to exactly zero.\n\nB. The non-intercept coefficient estimates ($\\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$) are forced to exactly zero, while the intercept estimate $\\hat{\\beta}_0$ converges to the sample mean of the response variable, $\\bar{y}$.\n\nC. All coefficient estimates ($\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$) converge to their Ordinary Least Squares (OLS) estimates.\n\nD. At least one non-intercept coefficient estimate remains non-zero, while all others are forced to zero.\n\nE. The non-intercept coefficient estimates ($\\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$) grow in magnitude proportionally to $\\lambda$.", "solution": "We consider the LASSO objective\n$$\nQ_{\\lambda}(\\beta_{0},\\beta_{1},\\dots,\\beta_{p})=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}x_{ij}\\beta_{j}\\right)^{2}+\\lambda\\sum_{j=1}^{p}|\\beta_{j}|,\n$$\nwith $\\lambda\\ge 0$, where the intercept $\\beta_{0}$ is not penalized.\n\nStep 1: Behavior of non-intercept coefficients as $\\lambda\\to\\infty$.\n- For any candidate $(\\beta_{0},\\beta_{1},\\dots,\\beta_{p})$ with at least one $\\beta_{j}\\neq 0$ for some $j\\ge 1$, the penalty term satisfies $\\sum_{j=1}^{p}|\\beta_{j}|>0$, hence $\\lambda\\sum_{j=1}^{p}|\\beta_{j}|\\to\\infty$ as $\\lambda\\to\\infty$.\n- Consider instead the alternative with all non-intercept coefficients set to zero: $\\tilde{\\beta}_{j}=0$ for $j\\ge 1$, and choose $\\tilde{\\beta}_{0}$ to minimize the residual sum of squares. For this alternative, the penalty is identically zero for all $\\lambda$, and the residual sum of squares is finite.\n- Therefore, for sufficiently large $\\lambda$, any solution with some $\\beta_{j}\\neq 0$ has strictly larger objective value than the solution with $\\beta_{j}=0$ for all $j\\ge 1$. Hence, in the limit $\\lambda\\to\\infty$, the minimizers must satisfy\n$$\n\\hat{\\beta}_{j}(\\lambda)\\to 0 \\quad \\text{for all } j=1,\\dots,p.\n$$\n\nStep 2: Limiting value of the intercept when $\\beta_{1}=\\dots=\\beta_{p}=0$.\n- With $\\beta_{j}=0$ for $j\\ge 1$, the objective reduces to minimizing the residual sum of squares in $\\beta_{0}$:\n$$\nR(\\beta_{0})=\\sum_{i=1}^{n}(y_{i}-\\beta_{0})^{2}.\n$$\n- Differentiate and set to zero to obtain the first-order condition:\n$$\n\\frac{dR}{d\\beta_{0}}=-2\\sum_{i=1}^{n}(y_{i}-\\beta_{0})=0 \\quad \\Longrightarrow \\quad n\\beta_{0}=\\sum_{i=1}^{n}y_{i}.\n$$\nThus,\n$$\n\\beta_{0}=\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}.\n$$\n- The second derivative is $\\frac{d^{2}R}{d\\beta_{0}^{2}}=2n>0$, confirming a minimum.\n\nConclusion: As $\\lambda\\to\\infty$, the non-intercept coefficients are driven to exactly zero, while the intercept converges to the sample mean $\\bar{y}$. This corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1928648"}, {"introduction": "The previous exercises explored the two endpoints of the Lasso spectrum. Now, we delve into the dynamic process itself: the \"Lasso path.\" This practice provides a quantitative look at how coefficients shrink as $\\lambda$ increases, revealing the precise moment a coefficient is forced to zero. By using a hypothetical scenario with orthonormal predictors—a common tool for simplifying the mathematics—you can isolate and understand the core mechanism of soft-thresholding that gives Lasso its celebrated feature selection ability. [@problem_id:1928602]", "problem": "Consider a linear regression model with $n$ observations. The model has a response vector $\\mathbf{y}$ and two predictor vectors, $\\mathbf{x}_1$ and $\\mathbf{x}_2$. All variables are assumed to have been centered to have a mean of zero, so no intercept term is needed in the model. The two predictor vectors have been prepared such that they are orthogonal and their squared norms are equal to the number of observations, $n$. Specifically, they satisfy the following conditions:\n$$ \\sum_{i=1}^n x_{i1}^2 = n $$\n$$ \\sum_{i=1}^n x_{i2}^2 = n $$\n$$ \\sum_{i=1}^n x_{i1} x_{i2} = 0 $$\nThe regression model is given by $y_i = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i$.\n\nThe coefficients $\\beta_1$ and $\\beta_2$ are estimated using the Least Absolute Shrinkage and Selection Operator (LASSO) method. The LASSO estimates, denoted $\\hat{\\beta}_1(\\lambda)$ and $\\hat{\\beta}_2(\\lambda)$, are the values that minimize the objective function for a given tuning parameter $\\lambda > 0$:\n$$ L(\\beta_1, \\beta_2) = \\frac{1}{2} \\sum_{i=1}^n (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2 + \\lambda (|\\beta_1| + |\\beta_2|) $$\nAs the tuning parameter $\\lambda$ is increased from 0, the magnitudes of the estimated coefficients are continuously shrunk towards zero. Let $S_1 = \\sum_{i=1}^n x_{i1} y_i$ and $S_2 = \\sum_{i=1}^n x_{i2} y_i$. Assume that we are in a scenario where $|S_1| > |S_2| > 0$, which implies that for small, non-zero values of $\\lambda$, both $\\hat{\\beta}_1(\\lambda)$ and $\\hat{\\beta}_2(\\lambda)$ are non-zero.\n\nDetermine the exact value of the tuning parameter $\\lambda$ at which the coefficient estimate $\\hat{\\beta}_2(\\lambda)$ first becomes equal to zero. Express your answer in terms of $S_1$ and/or $S_2$.", "solution": "Let $X=[\\mathbf{x}_{1}\\ \\mathbf{x}_{2}]$ and note that the given conditions imply $X^{\\top}X=n I_{2}$ and $X^{\\top}\\mathbf{y}=(S_{1},S_{2})^{\\top}$. The LASSO objective can be written as\n$$\nL(\\beta_{1},\\beta_{2})=\\frac{1}{2}\\|\\mathbf{y}-X\\beta\\|^{2}+\\lambda\\left(|\\beta_{1}|+|\\beta_{2}|\\right),\n$$\nwhich expands, using $X^{\\top}X=n I_{2}$ and $X^{\\top}\\mathbf{y}=(S_{1},S_{2})^{\\top}$, to\n$$\nL(\\beta_{1},\\beta_{2})=\\frac{1}{2}\\mathbf{y}^{\\top}\\mathbf{y}-S_{1}\\beta_{1}-S_{2}\\beta_{2}+\\frac{n}{2}\\left(\\beta_{1}^{2}+\\beta_{2}^{2}\\right)+\\lambda\\left(|\\beta_{1}|+|\\beta_{2}|\\right).\n$$\nThis is separable in $\\beta_{1}$ and $\\beta_{2}$. For $j\\in\\{1,2\\}$, define\n$$\nf_{j}(\\beta_{j})=\\frac{n}{2}\\beta_{j}^{2}-S_{j}\\beta_{j}+\\lambda|\\beta_{j}|.\n$$\nWe minimize $f_{2}(\\beta_{2})$. Using subgradient optimality:\n- If $\\beta_{2}\\neq 0$, letting $s_{2}=\\operatorname{sign}(\\beta_{2})$, the first-order condition is\n$$\nn\\beta_{2}-S_{2}+\\lambda s_{2}=0\\quad\\Longrightarrow\\quad \\beta_{2}=\\frac{S_{2}-\\lambda s_{2}}{n}.\n$$\nConsistency requires $s_{2}=\\operatorname{sign}(\\beta_{2})=\\operatorname{sign}(S_{2}-\\lambda s_{2})$, which holds exactly when $|S_{2}|>\\lambda$. In that case,\n$$\n\\hat{\\beta}_{2}(\\lambda)=\\frac{\\operatorname{sign}(S_{2})\\left(|S_{2}|-\\lambda\\right)}{n}.\n$$\n- If $\\beta_{2}=0$, the subgradient condition is $0\\in -S_{2}+\\lambda u$ for some $u\\in[-1,1]$, which is equivalent to $|S_{2}|\\le \\lambda$.\n\nTherefore, as $\\lambda$ increases from $0$, $\\hat{\\beta}_{2}(\\lambda)$ becomes zero exactly when $\\lambda$ reaches $|S_{2}|$. Under the assumption $|S_{1}|>|S_{2}|>0$, $\\hat{\\beta}_{2}$ is the first coefficient to hit zero, at $\\lambda=|S_{2}|$.", "answer": "$$\\boxed{|S_{2}|}$$", "id": "1928602"}]}