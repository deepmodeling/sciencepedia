{"hands_on_practices": [{"introduction": "This first practice grounds the concept of Ridge regression in a direct calculation. By working with a very small, hypothetical dataset, we can move beyond abstract matrix formulas and focus on the core mechanic: minimizing the penalized sum of squares. This hands-on exercise [@problem_id:1950377] will clarify exactly how the penalty term, controlled by $\\lambda$, pulls the coefficient estimate away from the standard least squares solution to prevent overfitting.", "problem": "A data scientist is analyzing a very small dataset containing just two observations, $(x_1, y_1) = (1, 2)$ and $(x_2, y_2) = (3, 4)$. They choose to model the relationship between the variables using a simple linear model that passes through the origin, described by the equation $y_i = \\beta x_i + \\epsilon_i$, where $\\beta$ is the single model parameter to be estimated and $\\epsilon_i$ represents the error term for the $i$-th observation.\n\nTo prevent overfitting, even with this minimal dataset, the scientist decides to use Ridge regression. The objective is to find the value of $\\beta$ that minimizes the penalized sum of squared residuals, given by the expression:\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\nGiven a regularization parameter (or penalty) of $\\lambda = 1$, calculate the Ridge regression estimate, denoted as $\\hat{\\beta}_{\\text{Ridge}}$. Express your answer as an exact fraction.", "solution": "We minimize the penalized sum of squares\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}.\n$$\nDifferentiating with respect to $\\beta$ and setting to zero gives the first-order condition\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}(y_{i}-\\beta x_{i})+2\\lambda \\beta=0.\n$$\nRearranging,\n$$\n\\sum_{i=1}^{n}x_{i}y_{i}-\\beta \\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta=0\n\\quad\\Longrightarrow\\quad\n\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=\\sum_{i=1}^{n}x_{i}y_{i}.\n$$\nThus the Ridge estimator (with no intercept) is\n$$\n\\hat{\\beta}_{\\text{Ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\nFor the given data $(x_{1},y_{1})=(1,2)$ and $(x_{2},y_{2})=(3,4)$ with $\\lambda=1$,\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 2+3\\cdot 4=14,\\qquad \\sum_{i=1}^{2}x_{i}^{2}=1^{2}+3^{2}=10,\n$$\nso\n$$\n\\hat{\\beta}_{\\text{Ridge}}=\\frac{14}{10+1}=\\frac{14}{11}.\n$$", "answer": "$$\\boxed{\\frac{14}{11}}$$", "id": "1950377"}, {"introduction": "Next, we explore the Least Absolute Shrinkage and Selection Operator (LASSO), a powerful alternative to Ridge. This exercise [@problem_id:1950382] demonstrates LASSO's most defining characteristicâ€”its ability to perform automatic feature selection by shrinking coefficients to exactly zero. You will calculate a LASSO estimate and also discover the precise amount of penalty needed to completely remove a predictor from the model.", "problem": "A data scientist is investigating the relationship between a single predictor variable $x$ and a response variable $y$. The proposed model is a simple linear regression: $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$. To prevent overfitting and perform feature selection, the data scientist decides to use the Least Absolute Shrinkage and Selection Operator (LASSO) method for parameter estimation.\n\nThe estimates for the intercept $\\beta_0$ and the slope $\\beta_1$ are obtained by minimizing the following objective function:\n$$Q(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 + \\lambda |\\beta_1|$$\nwhere $\\lambda \\geq 0$ is the regularization parameter. Note that the intercept term $\\beta_0$ is not penalized.\n\nA small dataset of $n=4$ observations has been collected:\nPredictor $x$: $(-2, -1, 1, 2)$\nResponse $y$: $(-1, 0, 2, 3)$\n\nYour tasks are:\n1. For a regularization parameter value of $\\lambda = 12$, what is the LASSO estimate for the slope coefficient, $\\hat{\\beta}_1$?\n2. What is the smallest value of the regularization parameter, which we can denote as $\\lambda_{crit}$, that results in a slope estimate $\\hat{\\beta}_1$ of exactly zero?\n\nProvide your answer as an ordered pair of exact numerical values $(\\hat{\\beta}_1, \\lambda_{crit})$.", "solution": "We minimize the LASSO objective\n$$Q(\\beta_{0},\\beta_{1})=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)^{2}+\\lambda|\\beta_{1}|,$$\nwith $\\beta_{0}$ unpenalized. For any fixed $\\beta_{1}$, the minimizing intercept is obtained by setting the derivative of the squared-error part with respect to $\\beta_{0}$ to zero, which yields\n$$\\hat{\\beta}_{0}(\\beta_{1})=\\bar{y}-\\beta_{1}\\bar{x}.$$\nSubstituting gives the profiled objective in terms of centered variables $x_{i}^{c}=x_{i}-\\bar{x}$ and $y_{i}^{c}=y_{i}-\\bar{y}$:\n$$Q(\\beta_{1})=\\sum_{i=1}^{n}\\left(y_{i}^{c}-\\beta_{1}x_{i}^{c}\\right)^{2}+\\lambda|\\beta_{1}|.$$\nDefine\n$$S_{xx}=\\sum_{i=1}^{n}\\left(x_{i}^{c}\\right)^{2},\\qquad S_{xy}=\\sum_{i=1}^{n}x_{i}^{c}y_{i}^{c}.$$\nFor $\\beta_{1}\\neq 0$, the derivative condition is\n$$\\frac{dQ}{d\\beta_{1}}=-2S_{xy}+2\\beta_{1}S_{xx}+\\lambda\\,\\mathrm{sign}(\\beta_{1})=0,$$\nwhich gives, provided the sign is consistent,\n$$\\beta_{1}=\\frac{S_{xy}-\\frac{\\lambda}{2}\\mathrm{sign}(\\beta_{1})}{S_{xx}}.$$\nThe subgradient condition at $\\beta_{1}=0$ is\n$$0\\in -2S_{xy}+[-\\lambda,\\lambda]\\quad\\Longleftrightarrow\\quad |2S_{xy}|\\leq \\lambda.$$\nEquivalently, the solution is the soft-thresholding form\n$$\\hat{\\beta}_{1}=\\frac{\\mathrm{sign}(S_{xy})\\max\\left(|S_{xy}|-\\frac{\\lambda}{2},\\,0\\right)}{S_{xx}}.$$\n\nFor the given data, compute the means $\\bar{x}$ and $\\bar{y}$:\n$$\\bar{x}=\\frac{-2-1+1+2}{4}=0,\\qquad \\bar{y}=\\frac{-1+0+2+3}{4}=1.$$\nHence the centered variables are $x^{c}=(-2,-1,1,2)$ and $y^{c}=(-2,-1,1,2)$. Therefore,\n$$S_{xx}=\\sum_{i=1}^{4}(x_{i}^{c})^{2}=4+1+1+4=10,\\qquad S_{xy}=\\sum_{i=1}^{4}x_{i}^{c}y_{i}^{c}=\\sum_{i=1}^{4}(x_{i}^{c})^{2}=10.$$\n\n1) For $\\lambda=12$, apply the soft-thresholding formula:\n$$\\hat{\\beta}_{1}=\\frac{\\max\\left(10-\\frac{12}{2},\\,0\\right)}{10}=\\frac{10-6}{10}=\\frac{2}{5}.$$\n\n2) The smallest $\\lambda$ such that $\\hat{\\beta}_{1}=0$ is the threshold where $|2S_{xy}|\\leq \\lambda$, i.e.,\n$$\\lambda_{\\text{crit}}=2|S_{xy}|=2\\cdot 10=20.$$\n\nThus the ordered pair is $\\left(\\frac{2}{5},\\,20\\right)$.", "answer": "$$\\boxed{(\\frac{2}{5}, 20)}$$", "id": "1950382"}, {"introduction": "A key reason to employ regularization is to manage multicollinearity, a common issue where predictor variables are highly correlated. This practice explores a scenario of perfect collinearity, a situation where Ordinary Least Squares (OLS) would fail to find a unique solution. Through this exercise [@problem_id:1950412], you will see how Ridge regression elegantly handles this problem by distributing the influence between the identical predictors.", "problem": "A data scientist is working with a dataset to build a predictive model. After performing initial data exploration, they discover a perfect linear relationship between two of the predictor variables, $x_1$ and $x_2$. Specifically, for every data point $i$ in the sample of size $n$, it is found that $x_{i1} = x_{i2}$. It is also assumed that all variables (predictors and the response $y$) have been centered, meaning they have a mean of zero.\n\nThe data scientist decides to fit a linear model without an intercept:\n$$y_i = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i$$\nTo address the perfect collinearity between $x_1$ and $x_2$, they employ Ridge regression. The Ridge coefficient estimates $(\\hat{\\beta}_1, \\hat{\\beta}_2)$ are the values of $(\\beta_1, \\beta_2)$ that minimize the following objective function:\n$$L(\\beta_1, \\beta_2) = \\sum_{i=1}^{n} (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^{2} + \\lambda (\\beta_1^{2} + \\beta_2^{2})$$\nHere, $\\lambda$ is a strictly positive regularization parameter ($\\lambda > 0$).\n\nFrom the data, the following summary statistics have been calculated:\n- $\\sum_{i=1}^{n} y_i x_{i1} = A$\n- $\\sum_{i=1}^{n} x_{i1}^{2} = B$\n\nDetermine the estimated coefficient $\\hat{\\beta}_1$ in terms of $A$, $B$, and $\\lambda$.", "solution": "We minimize the Ridge objective\n$$L(\\beta_{1},\\beta_{2})=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{1}x_{i1}-\\beta_{2}x_{i2}\\right)^{2}+\\lambda\\left(\\beta_{1}^{2}+\\beta_{2}^{2}\\right),$$\nwith $x_{i1}=x_{i2}$ for all $i$, and with the summary statistics $\\sum_{i=1}^{n}y_{i}x_{i1}=A$ and $\\sum_{i=1}^{n}x_{i1}^{2}=B$. Define also $C=\\sum_{i=1}^{n}x_{i1}x_{i2}$. Because $x_{i1}=x_{i2}$ pointwise, we have $C=B$, and similarly $\\sum_{i=1}^{n}y_{i}x_{i2}=A$.\n\nSet the partial derivatives to zero. For $\\beta_{1}$:\n$$\\frac{\\partial L}{\\partial \\beta_{1}}=2\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{1}x_{i1}-\\beta_{2}x_{i2}\\right)(-x_{i1})+2\\lambda\\beta_{1}=0,$$\nwhich expands to\n$$-2\\sum_{i=1}^{n}x_{i1}y_{i}+2\\beta_{1}\\sum_{i=1}^{n}x_{i1}^{2}+2\\beta_{2}\\sum_{i=1}^{n}x_{i1}x_{i2}+2\\lambda\\beta_{1}=0.$$\nDivide by $2$ and substitute $A$, $B$, and $C$ to get\n$$(B+\\lambda)\\beta_{1}+C\\beta_{2}=A.$$\nBy symmetry, the derivative with respect to $\\beta_{2}$ yields\n$$C\\beta_{1}+(B+\\lambda)\\beta_{2}=A.$$\n\nUsing $C=B$, the system becomes\n$$(B+\\lambda)\\beta_{1}+B\\beta_{2}=A,\\qquad B\\beta_{1}+(B+\\lambda)\\beta_{2}=A.$$\nSubtracting the two equations gives\n$$\\lambda(\\beta_{1}-\\beta_{2})=0.$$\nSince $\\lambda>0$, it follows that $\\beta_{1}=\\beta_{2}$. Substituting $\\beta_{2}=\\beta_{1}$ into either equation yields\n$$(2B+\\lambda)\\beta_{1}=A,$$\nso\n$$\\hat{\\beta}_{1}=\\frac{A}{2B+\\lambda}.$$", "answer": "$$\\boxed{\\frac{A}{2B+\\lambda}}$$", "id": "1950412"}]}