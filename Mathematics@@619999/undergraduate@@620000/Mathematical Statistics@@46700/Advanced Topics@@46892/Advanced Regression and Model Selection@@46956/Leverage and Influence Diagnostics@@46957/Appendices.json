{"hands_on_practices": [{"introduction": "Before we can assess a data point's influence, we must first quantify its potential to be influential. This potential is measured by its statistical leverage. This exercise provides fundamental practice in computing leverage values directly from the design matrix $X$, giving you a concrete understanding of how a point's position in the predictor space determines its leverage value, $h_{ii}$. [@problem_id:1930389]", "problem": "In the study of linear regression, diagnostic tools are crucial for identifying influential data points. One such tool is the concept of statistical leverage. Consider a simple linear regression model of the form $Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$, where $\\beta_0$ and $\\beta_1$ are the intercept and slope parameters, respectively, and $\\epsilon_i$ are independent error terms.\n\nAn experiment is designed with three observations, corresponding to the predictor variable values $x_1 = -2$, $x_2 = 1$, and $x_3 = 4$. The response values $y_i$ are not provided. The leverage of the $i$-th data point is defined as the $i$-th diagonal entry, $h_{ii}$, of the hat matrix $H$. The hat matrix is defined by the expression $H = X(X^T X)^{-1} X^T$, where $X$ is the model's design matrix. For this model, the design matrix $X$ is a $3 \\times 2$ matrix whose first column consists of all ones and whose second column contains the predictor values $x_i$.\n\nCalculate the leverage values $h_{11}$, $h_{22}$, and $h_{33}$ for the three given data points. Express your answer as a row vector containing these three values in order, using their exact fractional form.", "solution": "In simple linear regression with an intercept, the design matrix has rows $x_{i}^{T} = \\begin{pmatrix} 1 & x_{i} \\end{pmatrix}$. For the given $x$ values, the design matrix is\n$$\nX = \\begin{pmatrix}\n1 & -2 \\\\\n1 & 1 \\\\\n1 & 4\n\\end{pmatrix}.\n$$\nThe hat matrix is $H = X(X^{T}X)^{-1}X^{T}$, and the leverage values are the diagonal entries $h_{ii} = x_{i}^{T}(X^{T}X)^{-1}x_{i}$.\n\nFirst compute $X^{T}X$:\n$$\nX^{T}X = \\begin{pmatrix}\n\\sum_{i=1}^{3} 1 & \\sum_{i=1}^{3} x_{i} \\\\\n\\sum_{i=1}^{3} x_{i} & \\sum_{i=1}^{3} x_{i}^{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 & (-2)+1+4 \\\\\n(-2)+1+4 & (-2)^{2}+1^{2}+4^{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 & 3 \\\\\n3 & 21\n\\end{pmatrix}.\n$$\nThe inverse of a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is $\\frac{1}{ad-bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$. Thus\n$$\n(X^{T}X)^{-1} = \\frac{1}{3\\cdot 21 - 3\\cdot 3}\\begin{pmatrix} 21 & -3 \\\\ -3 & 3 \\end{pmatrix}\n= \\frac{1}{54}\\begin{pmatrix} 21 & -3 \\\\ -3 & 3 \\end{pmatrix}.\n$$\nNow compute $h_{ii} = x_{i}^{T}(X^{T}X)^{-1}x_{i}$ for each $i$.\n\nFor $i=1$ with $x_{1}=-2$, let $v_{1}=\\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$. Then\n$$\n(X^{T}X)^{-1}v_{1} = \\frac{1}{54}\\begin{pmatrix} 21 & -3 \\\\ -3 & 3 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n= \\frac{1}{54}\\begin{pmatrix} 21 + 6 \\\\ -3 - 6 \\end{pmatrix}\n= \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix},\n$$\nso\n$$\nh_{11} = v_{1}^{T}(X^{T}X)^{-1}v_{1} = \\begin{pmatrix} 1 & -2 \\end{pmatrix}\\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix}\n= \\frac{1}{2} + \\frac{1}{3} = \\frac{5}{6}.\n$$\n\nFor $i=2$ with $x_{2}=1$, let $v_{2}=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Then\n$$\n(X^{T}X)^{-1}v_{2} = \\frac{1}{54}\\begin{pmatrix} 21 & -3 \\\\ -3 & 3 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n= \\frac{1}{54}\\begin{pmatrix} 18 \\\\ 0 \\end{pmatrix}\n= \\begin{pmatrix} \\frac{1}{3} \\\\ 0 \\end{pmatrix},\n$$\nso\n$$\nh_{22} = v_{2}^{T}(X^{T}X)^{-1}v_{2} = \\begin{pmatrix} 1 & 1 \\end{pmatrix}\\begin{pmatrix} \\frac{1}{3} \\\\ 0 \\end{pmatrix}\n= \\frac{1}{3}.\n$$\n\nFor $i=3$ with $x_{3}=4$, let $v_{3}=\\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}$. Then\n$$\n(X^{T}X)^{-1}v_{3} = \\frac{1}{54}\\begin{pmatrix} 21 & -3 \\\\ -3 & 3 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}\n= \\frac{1}{54}\\begin{pmatrix} 9 \\\\ 9 \\end{pmatrix}\n= \\begin{pmatrix} \\frac{1}{6} \\\\ \\frac{1}{6} \\end{pmatrix},\n$$\nso\n$$\nh_{33} = v_{3}^{T}(X^{T}X)^{-1}v_{3} = \\begin{pmatrix} 1 & 4 \\end{pmatrix}\\begin{pmatrix} \\frac{1}{6} \\\\ \\frac{1}{6} \\end{pmatrix}\n= \\frac{1}{6} + \\frac{4}{6} = \\frac{5}{6}.\n$$\n\nAs a check, the sum of leverages equals the trace of $H$, which is the model rank $p=2$, and indeed $\\frac{5}{6}+\\frac{1}{3}+\\frac{5}{6}=\\frac{12}{6}=2$.\n\nTherefore, the leverage values in order are $\\frac{5}{6}, \\frac{1}{3}, \\frac{5}{6}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{5}{6} & \\frac{1}{3} & \\frac{5}{6}\\end{pmatrix}}$$", "id": "1930389"}, {"introduction": "A point with high leverage only becomes influential if it also \"pulls\" the regression line away from the bulk of the data. This practice demonstrates this effect in a tangible way by examining a point's impact on the coefficient of determination, $R^2$. By calculating the change in $R^2$ after removing a suspicious data point, you will quantify how a single observation can dramatically alter a model's perceived goodness-of-fit. [@problem_id:1930381]", "problem": "A materials science undergraduate is conducting a preliminary experiment to investigate a potential linear relationship between the curing time of a new polymer blend and its resulting tensile strength. Four samples are prepared with different curing times. The recorded data pairs (curing time in hours, tensile strength in gigapascals (GPa)) are as follows:\n(1, 2.0), (2, 2.5), (3, 1.5), and (10, 8.0).\n\nA simple linear regression model of the form $S = \\beta_0 + \\beta_1 t + \\epsilon$ is proposed to fit the data, where $S$ is the tensile strength and $t$ is the curing time.\n\nDue to its high leverage, the data point corresponding to the longest curing time is suspected of being an influential point that may be artificially inflating the model's goodness of fit. To investigate this, calculate the coefficient of determination, denoted as $R^2_{full}$, for the model using all four data points. Then, remove the data point with the longest curing time and calculate the new coefficient of determination, $R^2_{reduced}$, for the model based on the remaining three points.\n\nYour task is to compute the absolute decrease in the coefficient of determination, $\\Delta R^2 = R^2_{full} - R^2_{reduced}$.\n\nReport your final answer for $\\Delta R^2$ rounded to three significant figures.", "solution": "We model tensile strength $S$ versus curing time $t$ with an intercept using simple linear regression. For a dataset $\\{(t_{i},S_{i})\\}_{i=1}^{n}$ with an intercept, the least-squares slope and intercept are\n$$\n\\hat{\\beta}_{1}=\\frac{S_{tS}}{S_{tt}},\\qquad \\hat{\\beta}_{0}=\\bar{S}-\\hat{\\beta}_{1}\\bar{t},\n$$\nwhere\n$$\n\\bar{t}=\\frac{1}{n}\\sum_{i=1}^{n}t_{i},\\quad \\bar{S}=\\frac{1}{n}\\sum_{i=1}^{n}S_{i},\\quad S_{tt}=\\sum_{i=1}^{n}(t_{i}-\\bar{t})^{2},\\quad S_{tS}=\\sum_{i=1}^{n}(t_{i}-\\bar{t})(S_{i}-\\bar{S}).\n$$\nWith an intercept, the coefficient of determination is\n$$\nR^{2}=\\frac{\\text{SSR}}{\\text{SST}}=1-\\frac{\\text{SSE}}{\\text{SST}},\\qquad \\text{SSR}=\\frac{S_{tS}^{2}}{S_{tt}},\\qquad \\text{SST}=\\sum_{i=1}^{n}(S_{i}-\\bar{S})^{2}.\n$$\n\nFull model (all four points): $(t,S)\\in\\{(1,2.0),(2,2.5),(3,1.5),(10,8.0)\\}$.\nCompute means: $\\bar{t}=(1+2+3+10)/4=4$, $\\bar{S}=(2+2.5+1.5+8)/4=3.5$.\nCompute sums of squares:\n$$\nS_{tt}=(1-4)^{2}+(2-4)^{2}+(3-4)^{2}+(10-4)^{2}=9+4+1+36=50,\n$$\n$$\nS_{tS}=(1-4)(2-3.5)+(2-4)(2.5-3.5)+(3-4)(1.5-3.5)+(10-4)(8-3.5)=4.5+2+2+27=35.5,\n$$\n$$\n\\text{SST}=(2-3.5)^{2}+(2.5-3.5)^{2}+(1.5-3.5)^{2}+(8-3.5)^{2}=2.25+1+4+20.25=27.5.\n$$\nThen\n$$\n\\text{SSR}=\\frac{S_{tS}^{2}}{S_{tt}}=\\frac{(35.5)^{2}}{50}=\\frac{1260.25}{50}=25.205,\\qquad\nR^{2}_{\\text{full}}=\\frac{\\text{SSR}}{\\text{SST}}=\\frac{25.205}{27.5}=\\frac{5041}{5500}.\n$$\n\nReduced model (remove the point with $t=10$): $(t,S)\\in\\{(1,2.0),(2,2.5),(3,1.5)\\}$.\nCompute means: $\\bar{t}=(1+2+3)/3=2$, $\\bar{S}=(2+2.5+1.5)/3=2$.\nCompute sums of squares:\n$$\nS_{tt}=(1-2)^{2}+(2-2)^{2}+(3-2)^{2}=1+0+1=2,\n$$\n$$\nS_{tS}=(1-2)(2-2)+(2-2)(2.5-2)+(3-2)(1.5-2)=0+0+(-0.5)=-0.5,\n$$\n$$\n\\text{SST}=(2-2)^{2}+(2.5-2)^{2}+(1.5-2)^{2}=0+0.25+0.25=0.5.\n$$\nThen\n$$\n\\text{SSR}=\\frac{S_{tS}^{2}}{S_{tt}}=\\frac{(-0.5)^{2}}{2}=\\frac{0.25}{2}=0.125,\\qquad\nR^{2}_{\\text{reduced}}=\\frac{\\text{SSR}}{\\text{SST}}=\\frac{0.125}{0.5}=0.25.\n$$\n\nTherefore, the absolute decrease in $R^{2}$ is\n$$\n\\Delta R^{2}=R^{2}_{\\text{full}}-R^{2}_{\\text{reduced}}=\\frac{5041}{5500}-0.25=\\frac{5041-1375}{5500}=\\frac{3666}{5500} \\approx 0.666545\\ldots\n$$\nRounded to three significant figures, this is $0.667$.", "answer": "$$\\boxed{0.667}$$", "id": "1930381"}, {"introduction": "It is a common misconception that all high-leverage points are detrimental to a regression model. This exercise explores the crucial distinction between \"good\" and \"bad\" leverage. You will analyze how a high-leverage point that aligns with the data's underlying trend can actually be beneficial by increasing the precision of an estimate, whereas a misaligned point can introduce significant bias. [@problem_id:1930386]", "problem": "In regression analysis, a data point with a predictor value that is far from the mean of the predictors is known as a high-leverage point. Such points can have a disproportionate effect on the estimated regression model. Their influence can be either beneficial, by increasing the precision of coefficient estimates, or detrimental, by introducing significant bias.\n\nA researcher is studying the relationship between the operating temperature ($T$, in arbitrary units) and the electrical resistance ($R$, in arbitrary units) of a new material. A simple linear model of the form $R = \\beta_0 + \\beta_1 T + \\epsilon$ is proposed. An initial set of four measurements, which we'll call the base dataset $D_0$, is collected under standard operating conditions:\n$$ D_0 = \\{ (1, 2.1), (2, 2.9), (3, 4.2), (4, 4.8) \\} $$\n\nTo explore the material's properties at higher temperatures, two additional, independent experiments are conducted, each yielding a single high-leverage data point:\n- Point A: $(10, 11.1)$\n- Point B: $(10, 5.0)$\n\nYou are tasked with analyzing the impact of these two high-leverage points. Consider two separate datasets:\n1.  $D_A$, formed by adding point A to the base dataset $D_0$.\n2.  $D_B$, formed by adding point B to the base dataset $D_0$.\n\nFor each dataset ($D_A$ and $D_B$), a simple linear regression model is fitted. Let $se(\\hat{\\beta}_{1,A})$ be the standard error of the estimated slope coefficient for the model fitted to dataset $D_A$, and let $se(\\hat{\\beta}_{1,B})$ be the standard error for the model fitted to dataset $D_B$.\n\nCalculate the ratio $R = \\frac{se(\\hat{\\beta}_{1,A})}{se(\\hat{\\beta}_{1,B})}$. Round your final answer to three significant figures.", "solution": "We model $R$ on $T$ by $R=\\beta_{0}+\\beta_{1}T+\\epsilon$. For simple linear regression with intercept, define for a dataset $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ the centered sums\n$$\nS_{xx}=\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2},\\quad S_{yy}=\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2},\\quad S_{xy}=\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y}),\n$$\nwith $\\bar{x}=(\\sum x_{i})/n$ and $\\bar{y}=(\\sum y_{i})/n$. The residual sum of squares is\n$$\n\\mathrm{SSE}=S_{yy}-\\frac{S_{xy}^{2}}{S_{xx}},\n$$\nand the standard error of the slope is\n$$\nse(\\hat{\\beta}_{1})=\\sqrt{\\frac{\\hat{\\sigma}^{2}}{S_{xx}}}=\\sqrt{\\frac{\\mathrm{SSE}/(n-2)}{S_{xx}}}.\n$$\nHence, for two datasets with the same $n$ and $S_{xx}$, the ratio of slope standard errors reduces to\n$$\nR=\\frac{se(\\hat{\\beta}_{1,A})}{se(\\hat{\\beta}_{1,B})}=\\sqrt{\\frac{\\mathrm{SSE}_{A}}{\\mathrm{SSE}_{B}}}.\n$$\n\nCompute base sums for $D_{0}=\\{(1,2.1),(2,2.9),(3,4.2),(4,4.8)\\}$:\n$$\n\\sum x=10,\\ \\sum y=14.0,\\ \\sum x^{2}=30,\\ \\sum y^{2}=53.5,\\ \\sum xy=39.7, \n$$\n$$\n\\bar{x}_{0}=2.5,\\ \\bar{y}_{0}=3.5,\\ S_{xx,0}=5,\\ S_{yy,0}=4.5,\\ S_{xy,0}=4.7.\n$$\n\nDataset $D_{A}$ adds $(10,11.1)$, so $n_{A}=5$ and\n$$\n\\sum x=20,\\ \\sum y=25.1,\\ \\sum x^{2}=130,\\ \\sum y^{2}=176.71,\\ \\sum xy=150.7,\n$$\n$$\n\\bar{x}_{A}=4,\\ \\bar{y}_{A}=5.02.\n$$\nTherefore,\n$$\nS_{xx,A}=130-5\\cdot 4^{2}=50,\\quad S_{yy,A}=176.71-5\\cdot(5.02)^{2}=50.708,\n$$\n$$\nS_{xy,A}=150.7-5\\cdot 4\\cdot 5.02=50.3.\n$$\nThus\n$$\n\\mathrm{SSE}_{A}=S_{yy,A}-\\frac{S_{xy,A}^{2}}{S_{xx,A}}=50.708-\\frac{(50.3)^{2}}{50}=50.708-50.6018=0.1062.\n$$\n\nDataset $D_{B}$ adds $(10,5.0)$, so $n_{B}=5$ and\n$$\n\\sum x=20,\\ \\sum y=19,\\ \\sum x^{2}=130,\\ \\sum y^{2}=78.5,\\ \\sum xy=89.7,\n$$\n$$\n\\bar{x}_{B}=4,\\ \\bar{y}_{B}=3.8.\n$$\nTherefore,\n$$\nS_{xx,B}=130-5\\cdot 4^{2}=50,\\quad S_{yy,B}=78.5-5\\cdot(3.8)^{2}=6.3,\n$$\n$$\nS_{xy,B}=89.7-5\\cdot 4\\cdot 3.8=13.7.\n$$\nThus\n$$\n\\mathrm{SSE}_{B}=S_{yy,B}-\\frac{S_{xy,B}^{2}}{S_{xx,B}}=6.3-\\frac{(13.7)^{2}}{50}=6.3-3.7538=2.5462.\n$$\n\nWith $S_{xx,A}=S_{xx,B}$ and $n_{A}=n_{B}$, the ratio of standard errors is\n$$\nR=\\sqrt{\\frac{\\mathrm{SSE}_{A}}{\\mathrm{SSE}_{B}}}=\\sqrt{\\frac{0.1062}{2.5462}}\\approx 0.204228\\ldots\n$$\nRounded to three significant figures, $R=0.204$.", "answer": "$$\\boxed{0.204}$$", "id": "1930386"}]}