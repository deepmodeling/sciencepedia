{"hands_on_practices": [{"introduction": "A common misconception when first encountering sequences of functions is to assume that if the functions converge to zero at every point, their integrals must also converge to zero. This exercise provides a powerful counterexample that illustrates the difference between pointwise convergence and convergence in mean. By analyzing a sequence of triangular functions that grow taller as their base narrows, you will see firsthand why pointwise convergence alone is not enough to allow the interchange of limits and integrals, a foundational concept for understanding $L^1$ convergence [@problem_id:1412493].", "problem": "For each integer $n \\ge 2$, a sequence of functions $f_n: [0,1] \\to \\mathbb{R}$ is defined. The graph of $y=f_n(x)$ is constructed from two line segments that form a triangle with vertices at $(\\frac{1}{n+1}, 0)$, $(\\frac{1}{n}, \\pi n^2)$, and $(\\frac{1}{n-1}, 0)$. Outside of the base of this triangle, on the interval $[0,1]$, the function is defined to be zero, i.e., $f_n(x) = 0$ for $x \\in [0, 1/(n+1)] \\cup [1/(n-1), 1]$.\n\nCalculate the exact value of the following limit:\n$$ L = \\lim_{n \\to \\infty} \\int_0^1 f_n(x) dx $$", "solution": "For each integer $n\\geq 2$, the function $f_{n}$ is zero outside the interval $\\left[\\frac{1}{n+1},\\frac{1}{n-1}\\right]$ and, on this interval, is piecewise linear forming a triangle with base on the $x$-axis and apex at $\\left(\\frac{1}{n},\\pi n^{2}\\right)$. Since $f_{n}(x)\\geq 0$ and the graph over its support is exactly the boundary of this triangle with base on the $x$-axis, the integral equals the Euclidean area of that triangle:\n$$\n\\int_{0}^{1} f_{n}(x)\\,dx=\\frac{1}{2}\\times\\text{(base length)}\\times\\text{(height)}.\n$$\nThe base length is\n$$\n\\frac{1}{n-1}-\\frac{1}{n+1}=\\frac{(n+1)-(n-1)}{(n-1)(n+1)}=\\frac{2}{n^{2}-1},\n$$\nand the height is\n$$\nf_{n}\\!\\left(\\frac{1}{n}\\right)=\\pi n^{2}.\n$$\nTherefore,\n$$\n\\int_{0}^{1} f_{n}(x)\\,dx=\\frac{1}{2}\\cdot\\frac{2}{n^{2}-1}\\cdot\\pi n^{2}=\\pi\\cdot\\frac{n^{2}}{n^{2}-1}.\n$$\nTaking the limit,\n$$\nL=\\lim_{n\\to\\infty}\\int_{0}^{1} f_{n}(x)\\,dx=\\pi\\cdot\\lim_{n\\to\\infty}\\frac{n^{2}}{n^{2}-1}\n=\\pi\\cdot\\lim_{n\\to\\infty}\\frac{1}{1-\\frac{1}{n^{2}}}=\\pi.\n$$", "answer": "$$\\boxed{\\pi}$$", "id": "1412493"}, {"introduction": "Now that we've established that pointwise convergence does not imply convergence in mean, let's explore the conditions for $L^p$ convergence directly from its definition. This exercise presents a sequence of simple functions where a growing amplitude conflicts with a shrinking domain. Your task is to determine for which exponents $p$ the sequence converges, providing a hands-on feel for how the $L^p$ norm's power $p$ balances these competing factors to determine convergence [@problem_id:1412517].", "problem": "Consider the sequence of real-valued functions $\\{f_n\\}_{n=1}^{\\infty}$ defined on the real line $\\mathbb{R}$ by the expression:\n$$ f_n(x) = n^{\\alpha} \\chi_{[0, 1/n]}(x) $$\nwhere the constant $\\alpha = \\frac{1}{3}$, and $\\chi_A(x)$ denotes the indicator function of a set $A$ (i.e., $\\chi_A(x)=1$ if $x \\in A$ and $\\chi_A(x)=0$ if $x \\notin A$).\n\nThe goal is to determine for which values of the parameter $p$ the sequence $\\{f_n\\}$ converges in the $L^p(\\mathbb{R})$ space. A sequence of functions $\\{g_n\\}$ is said to converge in $L^p$ to a function $g$ if the limit of the $L^p$-norm of their difference is zero: $\\lim_{n\\to\\infty} \\|g_n - g\\|_p = 0$. The $L^p$-norm for a function $h$ is defined as $\\|h\\|_p = \\left( \\int_{-\\infty}^{\\infty} |h(x)|^p \\, dx \\right)^{1/p}$.\n\nIdentify the complete set of values for $p \\in [1, \\infty)$ for which the sequence $\\{f_n\\}$ converges in $L^p(\\mathbb{R})$. Express your answer in standard interval notation (e.g., $[a, b)$ or $(a, b]$).", "solution": "We analyze $f_{n}(x)=n^{\\alpha}\\chi_{[0,1/n]}(x)$ with $\\alpha=\\frac{1}{3}$. For any fixed $x\\neq 0$, there exists $N$ such that $n\\geq N$ implies $x>1/n$, hence $f_{n}(x)=0$. At $x=0$, $f_{n}(0)=n^{1/3}\\to\\infty$. Therefore $f_{n}\\to 0$ pointwise almost everywhere. Convergence in $L^{p}$ to $0$ is equivalent to $\\|f_{n}\\|_{p}\\to 0$.\n\nCompute the $L^{p}$ norm:\n$$\n\\|f_{n}\\|_{p}^{p}=\\int_{\\mathbb{R}}|f_{n}(x)|^{p}\\,dx=\\int_{0}^{1/n}n^{\\alpha p}\\,dx=n^{\\alpha p}\\cdot\\frac{1}{n}=n^{\\alpha p-1}.\n$$\nHence\n$$\n\\|f_{n}\\|_{p}=n^{\\alpha-\\frac{1}{p}}.\n$$\nThus:\n- If $\\alpha-\\frac{1}{p}<0$, equivalently $p<\\frac{1}{\\alpha}$, then $\\|f_{n}\\|_{p}\\to 0$, so $f_{n}\\to 0$ in $L^{p}$.\n- If $\\alpha-\\frac{1}{p}=0$, equivalently $p=\\frac{1}{\\alpha}$, then $\\|f_{n}\\|_{p}$ is constant and nonzero, so $f_{n}$ cannot converge to $0$. To see it does not converge to any $L^{p}$ function, check it is not Cauchy. For $p=\\frac{1}{\\alpha}=3$ and $n>m$,\n$$\n\\|f_{n}-f_{m}\\|_{3}^{3}=\\int_{0}^{1/n}|n^{1/3}-m^{1/3}|^{3}\\,dx+\\int_{1/n}^{1/m}|m^{1/3}|^{3}\\,dx=\\frac{|n^{1/3}-m^{1/3}|^{3}}{n}+1-\\frac{m}{n},\n$$\nwhich tends to $2$ as $n\\to\\infty$ for fixed $m$, hence $\\|f_{n}-f_{m}\\|_{3}$ stays bounded away from $0$. Therefore no $L^{3}$ convergence.\n- If $\\alpha-\\frac{1}{p}>0$, equivalently $p>\\frac{1}{\\alpha}$, then $\\|f_{n}\\|_{p}\\to\\infty$, so the sequence is not Cauchy and cannot converge in $L^{p}$.\n\nWith $\\alpha=\\frac{1}{3}$, we have $\\frac{1}{\\alpha}=3$. Therefore $\\{f_{n}\\}$ converges in $L^{p}(\\mathbb{R})$ exactly for $p\\in[1,3)$, and the limit is the zero function.", "answer": "$$\\boxed{[1,3)}$$", "id": "1412517"}, {"introduction": "The concepts of convergence are not merely abstract exercises; they form the bedrock of modern probability theory and statistics. This problem brings the concept of convergence in mean into the world of random variables, where it is known as \"convergence in mean square.\" You will investigate a sequence of random variables modeling rare, high-magnitude events, and discover a scenario where convergence in probability holds, but the stronger condition of convergence in mean square fails, highlighting the practical importance of these distinctions [@problem_id:1910442].", "problem": "Consider a sequence of discrete random variables $X_n$ for $n=1, 2, 3, \\ldots$ that models a system with rare, high-magnitude events. The probability mass function for each $X_n$ is given by:\n$$\nP(X_n = n^k) = \\frac{1}{n}\n$$\n$$\nP(X_n = 0) = 1 - \\frac{1}{n}\n$$\nwhere $k$ is a positive real constant. This model implies that as $n$ increases, the non-zero event becomes rarer but its magnitude increases.\n\nA sequence of random variables $X_n$ is said to converge in probability to 0 if for every $\\epsilon > 0$, the condition $\\lim_{n \\to \\infty} P(|X_n| > \\epsilon) = 0$ is satisfied. It can be shown that for any positive $k$, this sequence $X_n$ converges in probability to 0.\n\nA sequence of random variables $X_n$ is said to converge in mean square to 0 if $\\lim_{n \\to \\infty} E[X_n^2] = 0$. This condition is stricter and is not satisfied for all values of $k$.\n\nDetermine the smallest value of the exponent $k$ for which the sequence $X_n$ fails to converge to 0 in mean square.", "solution": "The problem asks for the smallest positive value of $k$ for which the sequence of random variables $X_n$ does not converge to 0 in mean square.\n\nBy definition, convergence in mean square to 0 occurs if and only if $\\lim_{n \\to \\infty} E[X_n^2] = 0$. Therefore, the sequence fails to converge in mean square to 0 if this limit is not equal to 0. Our goal is to find the condition on $k$ that causes this failure and then find the minimum such $k$.\n\nFirst, we must calculate the expected value of $X_n^2$, denoted as $E[X_n^2]$. For a discrete random variable, the expected value of a function of that variable, say $g(X_n)$, is given by the sum of $g(x)$ over all possible values $x$ that $X_n$ can take, weighted by their respective probabilities. In this case, the function is $g(x) = x^2$.\n\nThe random variable $X_n$ can take on two values: $n^k$ and $0$.\nThe corresponding probabilities are $P(X_n = n^k) = \\frac{1}{n}$ and $P(X_n = 0) = 1 - \\frac{1}{n}$.\n\nApplying the definition of expected value:\n$$\nE[X_n^2] = (n^k)^2 \\cdot P(X_n = n^k) + (0)^2 \\cdot P(X_n = 0)\n$$\nSubstituting the probabilities:\n$$\nE[X_n^2] = (n^{2k}) \\cdot \\left(\\frac{1}{n}\\right) + 0 \\cdot \\left(1 - \\frac{1}{n}\\right)\n$$\nSimplifying the expression:\n$$\nE[X_n^2] = \\frac{n^{2k}}{n} = n^{2k-1}\n$$\nNow, we need to evaluate the limit of $E[X_n^2]$ as $n \\to \\infty$:\n$$\n\\lim_{n \\to \\infty} E[X_n^2] = \\lim_{n \\to \\infty} n^{2k-1}\n$$\nThe behavior of this limit depends on the sign of the exponent $2k-1$. There are three cases:\n1.  If $2k-1 < 0$, which means $k < \\frac{1}{2}$, then $\\lim_{n \\to \\infty} n^{2k-1} = 0$. In this case, the sequence converges in mean square to 0.\n2.  If $2k-1 = 0$, which means $k = \\frac{1}{2}$, then $\\lim_{n \\to \\infty} n^{2k-1} = \\lim_{n \\to \\infty} n^0 = \\lim_{n \\to \\infty} 1 = 1$. Since the limit is not 0, the sequence fails to converge in mean square.\n3.  If $2k-1 > 0$, which means $k > \\frac{1}{2}$, then $\\lim_{n \\to \\infty} n^{2k-1} = \\infty$. Since the limit is not 0, the sequence fails to converge in mean square.\n\nCombining these cases, the sequence $X_n$ fails to converge in mean square to 0 if and only if $2k-1 \\ge 0$, which is equivalent to $k \\ge \\frac{1}{2}$.\n\nThe problem asks for the smallest value of $k$ for which this failure occurs. The set of values for $k$ for which convergence fails is the interval $[\\frac{1}{2}, \\infty)$. The smallest value in this set is $\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1910442"}]}