{"hands_on_practices": [{"introduction": "The Gram-Schmidt process is a cornerstone algorithm in linear algebra and functional analysis, providing a systematic method to transform a set of linearly independent vectors or functions into an orthonormal set. This exercise gives you direct, hands-on experience applying this procedure within the function space $L^2([0, 1])$. By working through the steps, you will solidify your understanding of how to construct a custom orthogonal \"coordinate system\" for a subspace of functions. [@problem_id:1434523]", "problem": "Consider the vector space $L^2([0, 1])$ consisting of all real-valued, square-integrable functions on the interval $[0, 1]$. The inner product for any two functions $f, g \\in L^2([0, 1])$ is defined as:\n$$\n\\langle f, g \\rangle = \\int_0^1 f(x)g(x)dx\n$$\nLet a two-dimensional subspace $S$ of $L^2([0, 1])$ be spanned by the set of linearly independent functions $\\{u_1, u_2\\}$, where $u_1(x) = \\sqrt{x}$ and $u_2(x) = x$.\n\nConstruct an orthonormal basis $\\{e_1(x), e_2(x)\\}$ for the subspace $S$, such that $e_1(x)$ is in the direction of $u_1(x)$ and $e_2(x)$ is a linear combination of $u_1(x)$ and $u_2(x)$ that is orthogonal to $e_1(x)$. Provide the closed-form analytic expression for the function $e_2(x)$.", "solution": "We apply the Gram–Schmidt process in the inner product space $L^{2}([0,1])$ with inner product $\\langle f,g\\rangle=\\int_{0}^{1}f(x)g(x)\\,dx$ to the spanning set $\\{u_{1},u_{2}\\}$, where $u_{1}(x)=\\sqrt{x}$ and $u_{2}(x)=x$.\n\nFirst, compute the norm of $u_{1}$:\n$$\n\\|u_{1}\\|^{2}=\\int_{0}^{1}x\\,dx=\\frac{1}{2},\\quad \\|u_{1}\\|=\\frac{1}{\\sqrt{2}}.\n$$\nThus\n$$\ne_{1}(x)=\\frac{u_{1}(x)}{\\|u_{1}\\|}=\\sqrt{2}\\,\\sqrt{x}.\n$$\n\nNext, form $v_{2}$ by removing from $u_{2}$ its projection onto $u_{1}$:\n$$\nv_{2}=u_{2}-\\frac{\\langle u_{2},u_{1}\\rangle}{\\|u_{1}\\|^{2}}\\,u_{1}.\n$$\nCompute the required inner products:\n$$\n\\langle u_{2},u_{1}\\rangle=\\int_{0}^{1}x\\sqrt{x}\\,dx=\\int_{0}^{1}x^{3/2}\\,dx=\\frac{2}{5},\\qquad \\|u_{1}\\|^{2}=\\frac{1}{2}.\n$$\nHence\n$$\n\\frac{\\langle u_{2},u_{1}\\rangle}{\\|u_{1}\\|^{2}}=\\frac{\\frac{2}{5}}{\\frac{1}{2}}=\\frac{4}{5},\\quad\nv_{2}(x)=x-\\frac{4}{5}\\sqrt{x}.\n$$\n\nNormalize $v_{2}$ to obtain $e_{2}$. Compute its norm:\n$$\n\\|v_{2}\\|^{2}=\\left\\langle x-\\frac{4}{5}\\sqrt{x},\\,x-\\frac{4}{5}\\sqrt{x}\\right\\rangle\n=\\int_{0}^{1}\\left(x^{2}-\\frac{8}{5}x^{3/2}+\\frac{16}{25}x\\right)dx\n=\\frac{1}{3}-\\frac{8}{5}\\cdot\\frac{2}{5}+\\frac{16}{25}\\cdot\\frac{1}{2}=\\frac{1}{75}.\n$$\nTherefore $\\|v_{2}\\|=1/\\sqrt{75}$, and\n$$\ne_{2}(x)=\\frac{v_{2}(x)}{\\|v_{2}\\|}=\\sqrt{75}\\left(x-\\frac{4}{5}\\sqrt{x}\\right)=\\sqrt{3}\\left(5x-4\\sqrt{x}\\right).\n$$\nThis $e_{2}$ is orthogonal to $e_{1}$ by construction and has unit norm.", "answer": "$$\\boxed{\\sqrt{3}\\left(5x-4\\sqrt{x}\\right)}$$", "id": "1434523"}, {"introduction": "Once we have an orthogonal set of functions, it is natural to ask how orthogonality interacts with other mathematical operations. This practice poses a fundamental question: if two functions are orthogonal, will their derivatives also be orthogonal? By investigating a specific pair of continuously differentiable functions, you will discover that properties in vector spaces do not always transfer in intuitive ways, thereby deepening your understanding of the subtleties of functional analysis. [@problem_id:1434472]", "problem": "In the Hilbert space $L^2([0,1])$ of square-integrable functions on the interval $[0,1]$, the inner product between two real-valued functions $f(x)$ and $g(x)$ is defined as $\\langle f, g \\rangle = \\int_0^1 f(x)g(x) \\,dx$. Two functions are said to be orthogonal if their inner product is zero.\n\nConsider the following two continuously differentiable functions:\n$$\nf(x) = x - \\frac{2}{\\pi}\n$$\n$$\ng(x) = \\sin\\left(\\frac{\\pi x}{2}\\right)\n$$\nThese two functions are orthogonal in $L^2([0,1])$. The question of whether the derivatives of orthogonal functions are themselves orthogonal is a fundamental query in functional analysis.\n\nCalculate the value of the inner product of the derivatives of $f(x)$ and $g(x)$, denoted as $\\langle f', g' \\rangle$. Provide the result as a single real number.", "solution": "We are asked to compute the inner product of the derivatives of $f$ and $g$ in $L^{2}([0,1])$, which is defined by\n$$\n\\langle f', g' \\rangle = \\int_{0}^{1} f'(x)\\,g'(x)\\,dx.\n$$\nGiven $f(x) = x - \\frac{2}{\\pi}$ and $g(x) = \\sin\\left(\\frac{\\pi x}{2}\\right)$, compute derivatives using standard differentiation and the chain rule:\n$$\nf'(x) = 1, \\quad g'(x) = \\frac{d}{dx}\\sin\\left(\\frac{\\pi x}{2}\\right) = \\cos\\left(\\frac{\\pi x}{2}\\right)\\cdot\\frac{\\pi}{2}.\n$$\nThus,\n$$\n\\langle f', g' \\rangle = \\int_{0}^{1} 1 \\cdot \\left(\\frac{\\pi}{2}\\cos\\left(\\frac{\\pi x}{2}\\right)\\right)\\,dx = \\frac{\\pi}{2}\\int_{0}^{1}\\cos\\left(\\frac{\\pi x}{2}\\right)\\,dx.\n$$\nUsing the antiderivative $\\int \\cos(ax)\\,dx = \\frac{1}{a}\\sin(ax)$ with $a=\\frac{\\pi}{2}$, we get\n$$\n\\int_{0}^{1}\\cos\\left(\\frac{\\pi x}{2}\\right)\\,dx = \\left.\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi x}{2}\\right)\\right|_{0}^{1} = \\frac{2}{\\pi}\\left(\\sin\\left(\\frac{\\pi}{2}\\right) - \\sin(0)\\right) = \\frac{2}{\\pi}.\n$$\nTherefore,\n$$\n\\langle f', g' \\rangle = \\frac{\\pi}{2}\\cdot \\frac{2}{\\pi} = 1.\n$$", "answer": "$$\\boxed{1}$$", "id": "1434472"}, {"introduction": "Orthonormal systems find one of their most profound applications in Fourier analysis, where functions are decomposed into a series of sines and cosines. This exercise explores the crucial distinction between convergence in the $L^2$ norm and pointwise convergence, which is not always guaranteed. By analyzing a Fourier series at a point of discontinuity, you will encounter a classic result that illuminates the sophisticated behavior of infinite series approximations of functions. [@problem_id:1434511]", "problem": "Consider a function $f: [-\\pi, \\pi] \\to \\mathbb{R}$ defined by\n$$\nf(x) = \\begin{cases}\n-A & \\text{if } -\\pi \\le x < 0 \\\\\nB & \\text{if } 0 \\le x \\le \\pi\n\\end{cases}\n$$\nwhere $A$ and $B$ are distinct positive real constants. The function $f$ belongs to the space $L^2([-\\pi, \\pi])$ of square-integrable functions on the interval $[-\\pi, \\pi]$. Its Fourier series is given by\n$$ S(x) = \\frac{a_0}{2} + \\sum_{n=1}^{\\infty} (a_n \\cos(nx) + b_n \\sin(nx)) $$\nwhere the Fourier coefficients are defined as $a_n = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x) \\cos(nx) dx$ and $b_n = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x) \\sin(nx) dx$.\n\nLet $S_N(x)$ denote the $N$-th partial sum of this series:\n$$ S_N(x) = \\frac{a_0}{2} + \\sum_{n=1}^{N} (a_n \\cos(nx) + b_n \\sin(nx)) $$\nA fundamental result in Fourier analysis guarantees that the sequence of partial sums $\\{S_N\\}$ converges to $f$ in the $L^2$ norm. This means that $\\lim_{N\\to\\infty} \\int_{-\\pi}^{\\pi} |S_N(x) - f(x)|^2 dx = 0$.\n\nThis problem explores the relationship between convergence in $L^2$ and pointwise convergence. Specifically, investigate the pointwise convergence of the series at the point of discontinuity, $x=0$. Compute the value of the limit $\\lim_{N\\to\\infty} S_N(0)$. Express your answer as a closed-form analytic expression in terms of $A$ and $B$.", "solution": "We compute the Fourier coefficients symbolically. First, the constant term is\n$$\na_{0}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\,dx=\\frac{1}{\\pi}\\left(\\int_{-\\pi}^{0}(-A)\\,dx+\\int_{0}^{\\pi}B\\,dx\\right)=\\frac{1}{\\pi}\\left(-A\\pi+B\\pi\\right)=B-A,\n$$\nhence $a_{0}/2=(B-A)/2$.\n\nFor $n\\geq 1$,\n$$\na_{n}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\cos(nx)\\,dx=\\frac{1}{\\pi}\\left((-A)\\int_{-\\pi}^{0}\\cos(nx)\\,dx+B\\int_{0}^{\\pi}\\cos(nx)\\,dx\\right).\n$$\nUsing $\\int \\cos(nx)\\,dx=\\frac{1}{n}\\sin(nx)$ and $\\sin(n\\pi)=0$ gives\n$$\n\\int_{-\\pi}^{0}\\cos(nx)\\,dx=\\frac{1}{n}\\left[\\sin(nx)\\right]_{-\\pi}^{0}=0,\\qquad \\int_{0}^{\\pi}\\cos(nx)\\,dx=\\frac{1}{n}\\left[\\sin(nx)\\right]_{0}^{\\pi}=0,\n$$\nso $a_{n}=0$ for all $n\\geq 1$.\n\nFor completeness, compute $b_{n}$:\n$$\nb_{n}=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\sin(nx)\\,dx=\\frac{1}{\\pi}\\left((-A)\\int_{-\\pi}^{0}\\sin(nx)\\,dx+B\\int_{0}^{\\pi}\\sin(nx)\\,dx\\right).\n$$\nUsing $\\int \\sin(nx)\\,dx=-\\frac{1}{n}\\cos(nx)$ and $\\cos(-n\\pi)=\\cos(n\\pi)=(-1)^{n}$,\n$$\n\\int_{-\\pi}^{0}\\sin(nx)\\,dx=-\\frac{1}{n}\\left[\\cos(nx)\\right]_{-\\pi}^{0}=-\\frac{1}{n}\\left(1-(-1)^{n}\\right),\\quad \\int_{0}^{\\pi}\\sin(nx)\\,dx=-\\frac{1}{n}\\left[ \\cos(nx)\\right]_{0}^{\\pi}=-\\frac{1}{n}\\left((-1)^{n}-1\\right),\n$$\nhence\n$$\nb_{n}=\\frac{1}{\\pi n}\\left(A\\left(1-(-1)^{n}\\right)+B\\left(1-(-1)^{n}\\right)\\right)=\\frac{A+B}{\\pi n}\\left(1-(-1)^{n}\\right).\n$$\nIn particular, $b_{n}=0$ for even $n$ and $b_{n}=\\frac{2(A+B)}{\\pi n}$ for odd $n$.\n\nNow evaluate the $N$-th partial sum at $x=0$. Since $\\cos(n\\cdot 0)=1$ and $\\sin(n\\cdot 0)=0$,\n$$\nS_{N}(0)=\\frac{a_{0}}{2}+\\sum_{n=1}^{N}\\left(a_{n}\\cos(0)+b_{n}\\sin(0)\\right)=\\frac{a_{0}}{2}+\\sum_{n=1}^{N}a_{n}.\n$$\nBecause $a_{n}=0$ for all $n\\geq 1$, it follows that\n$$\nS_{N}(0)=\\frac{a_{0}}{2}=\\frac{B-A}{2}\\quad \\text{for all }N,\n$$\nand therefore\n$$\n\\lim_{N\\to\\infty}S_{N}(0)=\\frac{B-A}{2}.\n$$\nThis equals $\\frac{f(0^{-})+f(0^{+})}{2}=\\frac{-A+B}{2}$, in agreement with the general Dirichlet–Jordan theorem for Fourier series at a jump discontinuity.", "answer": "$$\\boxed{\\frac{B-A}{2}}$$", "id": "1434511"}]}