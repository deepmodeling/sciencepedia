{"hands_on_practices": [{"introduction": "This first exercise grounds the abstract concept of conditional expectation in a familiar, concrete scenario: a sequence of Bernoulli trials. You will calculate the expected waiting time for the first success, given only the information from the very first trial. This practice directly illustrates how $E[X|\\mathcal{G}]$ acts as a random variable that takes specific, calculated values on each part of the partition defined by the information in $\\mathcal{G}$ [@problem_id:1410778].", "problem": "Consider a sequence of independent Bernoulli trials where the probability of success on any given trial is a constant $p$, with $0 < p < 1$. Let the random variable $X$ denote the trial number on which the first success occurs. Let $Y_1$ be a random variable representing the outcome of the first trial, where $Y_1=1$ if the first trial is a success and $Y_1=0$ if it is a failure. Furthermore, let $\\mathcal{G} = \\sigma(Y_1)$ be the sigma-algebra generated by the random variable $Y_1$.\n\nDetermine the conditional expectation $E[X|\\mathcal{G}]$. Your answer should be a single closed-form analytic expression in terms of $p$ and $Y_1$.", "solution": "Let $\\{Y_{n}\\}_{n\\geq 1}$ be i.i.d. Bernoulli trials with success probability $p$, and let $X$ be the index of the first success, so $X$ has the geometric distribution on $\\{1,2,\\dots\\}$ with parameter $p$. The sigma-algebra $\\mathcal{G}=\\sigma(Y_{1})$ has two atoms $\\{Y_{1}=1\\}$ and $\\{Y_{1}=0\\}$, and $E[X|\\mathcal{G}]$ must be a function of $Y_{1}$.\n\nBy the definition of conditional expectation on atoms,\n$$\nE[X|\\mathcal{G}] = E[X\\,|\\,Y_{1}=1]\\cdot \\mathbf{1}_{\\{Y_{1}=1\\}} + E[X\\,|\\,Y_{1}=0]\\cdot \\mathbf{1}_{\\{Y_{1}=0\\}}.\n$$\nOn $\\{Y_{1}=1\\}$, the first trial is a success, hence $X=1$ almost surely, so\n$$\nE[X\\,|\\,Y_{1}=1]=1.\n$$\nOn $\\{Y_{1}=0\\}$, the first trial is a failure, and by independence and stationarity of the trials, the waiting time from trial $2$ onward is $\\text{Geometric}(p)$ and independent of $Y_{1}$. Thus\n$$\nX \\,\\big|\\, \\{Y_{1}=0\\} \\stackrel{d}{=} 1 + Z,\n$$\nwhere $Z$ is $\\text{Geometric}(p)$ on $\\{1,2,\\dots\\}$. Therefore,\n$$\nE[X\\,|\\,Y_{1}=0]=1+E[Z].\n$$\nTo compute $E[Z]$, use the series representation\n$$\nE[Z]=\\sum_{k=1}^{\\infty} k\\,(1-p)^{k-1} p = p \\sum_{k=1}^{\\infty} k r^{k-1} \\quad \\text{with } r=1-p,\n$$\nand the identity $\\sum_{k=1}^{\\infty} k r^{k-1} = \\frac{1}{(1-r)^{2}}$ for $|r|<1$, which yields\n$$\nE[Z]= p \\cdot \\frac{1}{(1-(1-p))^{2}} = p \\cdot \\frac{1}{p^{2}} = \\frac{1}{p}.\n$$\nHence,\n$$\nE[X\\,|\\,Y_{1}=0]=1+\\frac{1}{p}.\n$$\n\nSince $Y_{1}\\in\\{0,1\\}$, we can write the conditional expectation as a single function of $Y_{1}$:\n$$\nE[X|\\mathcal{G}] = 1\\cdot Y_{1} + \\left(1+\\frac{1}{p}\\right)(1-Y_{1}) = 1 + \\frac{1 - Y_{1}}{p}.\n$$\nThis is measurable with respect to $\\sigma(Y_{1})$ and matches the conditional expectations on the atoms, hence it is the desired conditional expectation.", "answer": "$$\\boxed{1+\\frac{1-Y_{1}}{p}}$$", "id": "1410778"}, {"introduction": "Building on the previous example, this practice moves from conditioning on a simple binary event to conditioning on the outcome of a discrete random variable with multiple states. By calculating the expected maximum of two die rolls given the result of the first, you will construct the conditional expectation as an explicit function of the conditioning random variable. This exercise is key to solidifying the understanding that $E[Z|\\sigma(X)]$ is itself a random variable measurable with respect to $\\sigma(X)$ [@problem_id:1410798].", "problem": "Consider a probability space $(\\Omega, \\mathcal{F}, P)$ modeling the outcomes of two independent rolls of a fair six-sided die. Let the random variable $X: \\Omega \\to \\{1, 2, 3, 4, 5, 6\\}$ represent the outcome of the first roll, and $Y: \\Omega \\to \\{1, 2, 3, 4, 5, 6\\}$ represent the outcome of the second roll. The random variables $X$ and $Y$ are independent and identically distributed, with $P(X=k) = P(Y=k) = 1/6$ for each $k \\in \\{1, 2, 3, 4, 5, 6\\}$.\n\nLet $\\sigma(X)$ be the sigma-algebra generated by the random variable $X$. Determine the conditional expectation of the random variable $Z = \\max(X, Y)$ with respect to $\\sigma(X)$, denoted by $E[\\max(X,Y) | \\sigma(X)]$.\n\nExpress your answer as a closed-form analytic expression in terms of the random variable $X$.", "solution": "We seek the $\\sigma(X)$-conditional expectation of $Z=\\max(X,Y)$, where $X$ and $Y$ are independent and uniformly distributed on $\\{1,2,3,4,5,6\\}$. Because $\\sigma(X)$ is generated by $X$, the conditional expectation $E[\\max(X,Y)\\mid\\sigma(X)]$ must be an $X$-measurable function. By the definition of conditional expectation with respect to $\\sigma(X)$ and using independence, there exists a function $g$ such that\n$$\nE[\\max(X,Y)\\mid\\sigma(X)] = g(X), \\quad \\text{with } g(x) = E[\\max(x,Y)] \\text{ for each } x\\in\\{1,\\ldots,6\\}.\n$$\nFix $x\\in\\{1,\\ldots,6\\}$. Since $Y$ is uniform on $\\{1,\\ldots,6\\}$ and independent of $X$,\n$$\ng(x) = \\sum_{y=1}^{6} \\max(x,y) P(Y=y) = \\frac{1}{6}\\left(\\sum_{y=1}^{x} x \\;+\\; \\sum_{y=x+1}^{6} y\\right).\n$$\nThe first sum contributes $x$ repeated $x$ times, and the second sum runs over $y>x$. Thus,\n$$\ng(x) = \\frac{1}{6}\\left( x\\cdot x \\;+\\; \\sum_{y=1}^{6} y \\;-\\; \\sum_{y=1}^{x} y \\right).\n$$\nUsing the identity $\\sum_{y=1}^{n} y = \\frac{n(n+1)}{2}$, we have $\\sum_{y=1}^{6} y = \\frac{6\\cdot 7}{2} = 21$ and $\\sum_{y=1}^{x} y = \\frac{x(x+1)}{2}$. Therefore,\n$$\ng(x) = \\frac{1}{6}\\left( x^{2} + 21 - \\frac{x(x+1)}{2} \\right)\n= \\frac{1}{6}\\left(21 + \\frac{x^{2}-x}{2}\\right)\n= \\frac{x^{2} - x + 42}{12}.\n$$\nHence,\n$$\nE[\\max(X,Y)\\mid \\sigma(X)] = g(X) = \\frac{X^{2} - X + 42}{12}.\n$$\nThis is an $X$-measurable (hence $\\sigma(X)$-measurable) function, and by construction satisfies the defining property of the conditional expectation.", "answer": "$$\\boxed{\\frac{X^{2}-X+42}{12}}$$", "id": "1410798"}, {"introduction": "Our final practice shifts the focus from direct computation to the elegant application of theoretical properties. This problem explores how symmetry in a random variable's distribution interacts with the information contained in a $\\sigma$-algebra. By leveraging the principles of measurability and symmetry, you can deduce the conditional expectation with minimal calculation, revealing the analytical power and depth of the conditional expectation framework [@problem_id:1410799].", "problem": "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space. Let $X$ be a continuous random variable whose probability density function, $f_X(x)$, is an even function, meaning $f_X(x) = f_X(-x)$ for all real numbers $x$. Assume that the expectation of the absolute value of $X$, denoted $E[|X|]$, is finite. Let $a$ be a given non-zero real constant. Consider the sigma-algebra $\\mathcal{G} = \\sigma(X^2)$, which is the sigma-algebra generated by the random variable $X^2$. Your task is to determine the conditional expectation $E[X + aX^2 | \\mathcal{G}]$. Express your final answer as a symbolic expression in terms of the constant $a$ and the random variable $X$.", "solution": "We are given a continuous random variable $X$ with even density $f_{X}(x)=f_{X}(-x)$ and $\\mathbb{E}[|X|] < \\infty$. Let $\\mathcal{G}=\\sigma(X^{2})$. We compute $\\mathbb{E}[X+aX^{2}\\mid \\mathcal{G}]$ by determining $\\mathbb{E}[X\\mid \\mathcal{G}]$ and $\\mathbb{E}[X^{2}\\mid \\mathcal{G}]$.\n\nFirst, for any bounded Borel function $g$ on $[0,\\infty)$, the random variable $Z=g(X^{2})$ is $\\mathcal{G}$-measurable and bounded. Because $g$ is bounded and $\\mathbb{E}[|X|]<\\infty$, we have\n$$\n\\mathbb{E}\\!\\left[\\,|X|\\,|g(X^{2})|\\,\\right]\\leq \\|g\\|_{\\infty}\\,\\mathbb{E}[|X|] < \\infty,\n$$\nso we may compute\n$$\n\\mathbb{E}[X g(X^{2})]=\\int_{\\mathbb{R}} x\\,g(x^{2})\\,f_{X}(x)\\,dx.\n$$\nSince $f_{X}$ is even and $g(x^{2})$ is an even function of $x$, the integrand $x\\,g(x^{2})\\,f_{X}(x)$ is an odd function of $x$. Therefore,\n$$\n\\int_{\\mathbb{R}} x\\,g(x^{2})\\,f_{X}(x)\\,dx=0,\n$$\nwhich implies $\\mathbb{E}[X Z]=0$ for every bounded $\\mathcal{G}$-measurable $Z$. By the defining property of conditional expectation, this shows\n$$\n\\mathbb{E}[X\\mid \\mathcal{G}]=0 \\quad \\text{a.s.}\n$$\n\nNext, $X^{2}$ is $\\mathcal{G}$-measurable, so by the measurability property of conditional expectation,\n$$\n\\mathbb{E}[X^{2}\\mid \\mathcal{G}]=X^{2} \\quad \\text{a.s.}\n$$\n\nUsing linearity and the fact that constants pull out of conditional expectation, we obtain\n$$\n\\mathbb{E}[X+aX^{2}\\mid \\mathcal{G}]\n=\\mathbb{E}[X\\mid \\mathcal{G}]+a\\,\\mathbb{E}[X^{2}\\mid \\mathcal{G}]\n=0+aX^{2}\n=aX^{2}\n\\quad \\text{a.s.}\n$$\nThis expresses the conditional expectation as a function of $X$ and the parameter $a$, as required.", "answer": "$$\\boxed{aX^{2}}$$", "id": "1410799"}]}