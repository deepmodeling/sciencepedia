{"hands_on_practices": [{"introduction": "We begin by grounding the abstract concept of the Lebesgue integral in a familiar task: calculating the variance of a random variable. This exercise demonstrates how the formal definition, $E[g(X)] = \\int_{\\mathbb{R}} g(x) \\, dP(x)$, translates directly into a practical computation when a probability density function is known. By working through the calculation of the first and second moments for a simple triangular distribution, you will solidify your understanding of how expectation is defined and computed as an integral. [@problem_id:1418566]", "problem": "Let $(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}), P)$ be a probability space, where $\\mathcal{B}(\\mathbb{R})$ is the Borel sigma-algebra on the real line. The probability measure $P$ is absolutely continuous with respect to the Lebesgue measure $\\lambda$, and its Radon-Nikodym derivative (the probability density function) is given by:\n$$\nf(x) = \\frac{dP}{d\\lambda}(x) =\n\\begin{cases}\n2 - 2x & \\text{if } x \\in [0, 1] \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nConsider the random variable $X: (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})) \\to (\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$ defined by the identity function, $X(\\omega) = \\omega$. The expectation of a real-valued measurable function $g$ of the random variable $X$ is defined by the Lebesgue integral $E[g(X)] = \\int_{\\mathbb{R}} g(x) \\, dP(x)$.\n\nThe variance of the random variable $X$ is defined as $\\text{Var}(X) = E\\left[(X - E[X])^2\\right]$.\n\nCalculate the variance, $\\text{Var}(X)$, of this random variable. Your final answer should be a single numerical value expressed as an irreducible fraction.", "solution": "First verify that $f$ is a valid probability density on $\\mathbb{R}$. By definition, $f(x)=2-2x$ for $x\\in[0,1]$ and $f(x)=0$ otherwise. The total mass is\n$$\n\\int_{\\mathbb{R}} f(x)\\,dx=\\int_{0}^{1} (2-2x)\\,dx=\\left[2x - x^{2}\\right]_{0}^{1}=2-1=1,\n$$\nso $f$ is normalized.\n\nThe expectation of $X$ is given by\n$$\nE[X]=\\int_{\\mathbb{R}} x\\,dP(x)=\\int_{\\mathbb{R}} x f(x)\\,dx=\\int_{0}^{1} x(2-2x)\\,dx.\n$$\nCompute\n$$\n\\int_{0}^{1} x(2-2x)\\,dx=2\\int_{0}^{1} x\\,dx-2\\int_{0}^{1} x^{2}\\,dx=2\\left[\\frac{x^{2}}{2}\\right]_{0}^{1}-2\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}=1-\\frac{2}{3}=\\frac{1}{3}.\n$$\n\nNext compute the second moment:\n$$\nE[X^{2}]=\\int_{\\mathbb{R}} x^{2}\\,dP(x)=\\int_{0}^{1} x^{2}(2-2x)\\,dx=2\\int_{0}^{1} x^{2}\\,dx-2\\int_{0}^{1} x^{3}\\,dx.\n$$\nEvaluate\n$$\n2\\int_{0}^{1} x^{2}\\,dx-2\\int_{0}^{1} x^{3}\\,dx=2\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}-2\\left[\\frac{x^{4}}{4}\\right]_{0}^{1}=\\frac{2}{3}-\\frac{1}{2}=\\frac{1}{6}.\n$$\n\nUsing the identity $\\text{Var}(X)=E[X^{2}]-(E[X])^{2}$, which follows from expanding $E[(X-E[X])^{2}]=E[X^{2}]-2E[X]E[X]+(E[X])^{2}$ and linearity of expectation, we obtain\n$$\n\\text{Var}(X)=\\frac{1}{6}-\\left(\\frac{1}{3}\\right)^{2}=\\frac{1}{6}-\\frac{1}{9}=\\frac{3}{18}-\\frac{2}{18}=\\frac{1}{18}.\n$$\nThus the variance is the irreducible fraction $\\frac{1}{18}$.", "answer": "$$\\boxed{\\frac{1}{18}}$$", "id": "1418566"}, {"introduction": "Beyond direct computation, the Lebesgue theory offers elegant and powerful alternative methods for calculating moments. This practice introduces the \"layer-cake representation,\" a beautiful formula that relates the moments of a non-negative random variable to the integral of its survival function. You will apply this technique to find the variance of the minimum of two uniform random variables, a task that becomes remarkably straightforward with this tool. [@problem_id:744724]", "problem": "Let $U_1$ and $U_2$ be independent and identically distributed (i.i.d.) random variables, each following a continuous uniform distribution on the interval $[0, 1]$. Let the random variable $M$ be defined as the minimum of these two variables, i.e., $M = \\min(U_1, U_2)$.\n\nIn probability theory, the expectation of a non-negative random variable $X$ can be expressed as a Lebesgue integral over its value space. This leads to a useful formula for its moments, often called the \"layer-cake representation\". For a non-negative random variable $X$ with survival function $S_X(x) = P(X > x)$, its $k$-th moment is given by:\n$$\nE[X^k] = \\int_0^\\infty kx^{k-1} S_X(x) \\, dx\n$$\nFor the special case of the mean ($k=1$), this simplifies to $E[X] = \\int_0^\\infty S_X(x) \\, dx$.\n\nUsing this layer-cake representation, derive the variance of the random variable $M$, which is defined as $\\text{Var}(M) = E[M^2] - (E[M])^2$.", "solution": "1. Survival function of $M=\\min(U_1, U_2)$:  \n   For $0\\le x\\le1$,  \n   \n$$\n     S_M(x)=P(M>x)=P(U_1>x, U_2>x)=(1-x)^2.\n   $$\n\n2. First moment via layer-cake ($k=1$):  \n   \n$$\n     E[M]=\\int_0^\\infty S_M(x)\\,dx\n           =\\int_0^1(1-x)^2\\,dx\n           =\\biggl[-\\frac{(1-x)^3}{3}\\biggr]_0^1\n           =\\frac{1}{3}.\n   $$\n\n3. Second moment via layer-cake ($k=2$):  \n   \n$$\n     E[M^2]=\\int_0^\\infty2x\\,S_M(x)\\,dx\n           =\\int_0^1 2x(1-x)^2\\,dx.\n   $$\n\n   Expand and integrate:\n   \n$$\n     2x(1-x)^2=2x(1-2x+x^2)=2x-4x^2+2x^3,\n   $$\n\n   \n$$\n     \\int_0^1(2x-4x^2+2x^3)\\,dx\n     =\\Bigl[x^2-\\tfrac{4}{3}x^3+\\tfrac{1}{2}x^4\\Bigr]_0^1\n     =1-\\tfrac{4}{3}+\\tfrac{1}{2}\n     =\\tfrac{1}{6}.\n   $$\n\n4. Variance:\n   \n$$\n     \\text{Var}(M)=E[M^2]-(E[M])^2\n                    =\\tfrac{1}{6}-\\bigl(\\tfrac{1}{3}\\bigr)^2\n                    =\\tfrac{1}{6}-\\tfrac{1}{9}\n                    =\\tfrac{1}{18}.\n   $$", "answer": "$$\\boxed{\\frac{1}{18}}$$", "id": "744724"}, {"introduction": "One of the most profound questions in analysis is when we can exchange the order of limiting operations, such as a limit and an integral. This thought experiment explores a classic scenario where naively swapping the limit and the expectation leads to a contradiction. By analyzing this specific sequence of random variables, you will gain a deeper appreciation for the crucial role of the Dominated Convergence Theorem and understand why its conditions are not mere technicalities, but essential safeguards. [@problem_id:1360914]", "problem": "Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ defined on the probability space $(\\Omega, \\mathcal{F}, P)$, where the sample space is the unit interval $\\Omega = [0, 1]$, the sigma-algebra $\\mathcal{F}$ is the Borel sigma-algebra on $[0,1]$, and the probability measure $P$ is the Lebesgue measure on $[0,1]$ (corresponding to a continuous uniform distribution).\n\nEach random variable in the sequence is defined by the function $X_n(\\omega) = n \\cdot I_{(0, 1/n)}(\\omega)$, where $I_A(\\omega)$ is the indicator function for the set $A$, which equals 1 if $\\omega \\in A$ and 0 otherwise.\n\nThis sequence can be interpreted as a simplified model for a series of high-intensity, short-duration phenomena. As $n$ increases, the magnitude of the outcome, $n$, grows, while the interval over which it occurs, $(0, 1/n)$, shrinks.\n\nYour task is to analyze the limiting behavior of the expectations of this sequence.\n1.  First, calculate the limit of the expected values, denoted as $L_E = \\lim_{n \\to \\infty} E[X_n]$.\n2.  Second, determine the pointwise limit of the sequence of random variables, which we'll call $X(\\omega) = \\lim_{n \\to \\infty} X_n(\\omega)$ for each $\\omega \\in [0, 1]$. Then, calculate the expectation of this limiting random variable, denoted as $E_L = E[X]$.\n\nProvide the values of $L_E$ and $E_L$ as an ordered pair $(L_E, E_L)$.", "solution": "We are given $X_{n}(\\omega)=n\\,I_{(0,1/n)}(\\omega)$ on $\\Omega=[0,1]$ with Lebesgue measure $P$. By definition of expectation for a nonnegative measurable function,\n$$\nE[X_{n}]=\\int_{0}^{1}X_{n}(\\omega)\\,d\\omega=\\int_{0}^{1}n\\,I_{(0,1/n)}(\\omega)\\,d\\omega.\n$$\nUsing linearity of the integral and the property $\\int I_{A}\\,dP=P(A)$,\n$$\nE[X_{n}]=n\\,P\\big((0,1/n)\\big)=n\\cdot\\frac{1}{n}=1.\n$$\nTherefore, the limit of the expectations is\n$$\nL_{E}=\\lim_{n\\to\\infty}E[X_{n}]=\\lim_{n\\to\\infty}1=1.\n$$\n\nNext, fix $\\omega\\in[0,1]$ and analyze the pointwise limit $X(\\omega)=\\lim_{n\\to\\infty}X_{n}(\\omega)$. If $\\omega=0$, then $I_{(0,1/n)}(0)=0$ for all $n$, hence $X_{n}(0)=0$ for all $n$ and $\\lim_{n\\to\\infty}X_{n}(0)=0$. If $\\omega>0$, choose $N\\in\\mathbb{N}$ with $N>\\frac{1}{\\omega}$, which implies that for all $n\\geq N$ we have $\\frac{1}{n}<\\omega$, so $\\omega\\notin(0,1/n)$ and $I_{(0,1/n)}(\\omega)=0$. Thus $X_{n}(\\omega)=0$ for all sufficiently large $n$, and therefore $\\lim_{n\\to\\infty}X_{n}(\\omega)=0$. It follows that\n$$\nX(\\omega)=0\\quad\\text{for all }\\omega\\in[0,1].\n$$\nHence $X\\equiv 0$ almost everywhere, and its expectation is\n$$\nE_{L}=E[X]=\\int_{0}^{1}0\\,d\\omega=0.\n$$\n\nTherefore, the ordered pair is $(L_{E},E_{L})=(1,0)$.", "answer": "$$\\boxed{\\begin{pmatrix}1 & 0\\end{pmatrix}}$$", "id": "1360914"}]}