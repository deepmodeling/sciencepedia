{"hands_on_practices": [{"introduction": "To build a solid foundation, we first explore the direct logical consequences of the definition of independence. This exercise [@problem_id:1422232] challenges you to vet several fundamental claims, such as whether an event can be independent of itself or its complement. Working through these assertions is a crucial step to move from memorizing the formula for independence to truly understanding its implications.", "problem": "Within the framework of a given probability space $(\\Omega, \\mathcal{F}, P)$, an event $E \\in \\mathcal{F}$ is said to be independent of itself if the occurrence of $E$ provides no new information about the occurrence of $E$. Formally, this means the independence condition $P(A \\cap B) = P(A)P(B)$ holds when $A=B=E$, which simplifies to $P(E) = P(E)^2$.\n\nConsider the following four assertions regarding events within such a probability space. Your task is to identify all assertions that are necessarily true for any choice of events and probability measure $P$ that satisfy the given conditions.\n\nA. If an event $A$ is independent of itself, then $A$ must be either the empty set ($\\emptyset$) or the entire sample space ($\\Omega$).\n\nB. If an event $A$ has probability $P(A)=1$, it is independent of any other event $B \\in \\mathcal{F}$.\n\nC. If an event $A$ is independent of an event $B$, and event $B$ is independent of an event $C$, then event $A$ is necessarily independent of event $C$.\n\nD. An event $A$ for which its probability satisfies $0 < P(A) < 1$ can never be independent of its own complement, $A^c$.", "solution": "Independence of an event with itself requires $P(E\\cap E)=P(E)P(E)$, which simplifies to $P(E)=P(E)^{2}$. Writing $p=P(E)$, this gives the quadratic equation $p^{2}-p=0$, hence $p\\in\\{0,1\\}$.\n\nAssertion A: If $A$ is independent of itself, then $P(A)\\in\\{0,1\\}$. However, $P(A)=0$ does not imply $A=\\emptyset$, and $P(A)=1$ does not imply $A=\\Omega$. There exist nonempty null sets and proper co-null sets in standard probability spaces. Therefore, A is not necessarily true.\n\nAssertion B: If $P(A)=1$, then for any $B\\in\\mathcal{F}$,\n$$\nB=(A\\cap B)\\cup(A^{c}\\cap B),\\quad (A\\cap B)\\cap(A^{c}\\cap B)=\\emptyset.\n$$\nTaking probabilities and using additivity,\n$$\nP(B)=P(A\\cap B)+P(A^{c}\\cap B).\n$$\nSince $P(A^{c})=0$ and $A^{c}\\cap B\\subseteq A^{c}$ with $A^{c}\\cap B\\in\\mathcal{F}$, monotonicity yields $P(A^{c}\\cap B)=0$. Hence\n$$\nP(A\\cap B)=P(B)=P(A)P(B),\n$$\nso $A$ is independent of every $B$. Therefore, B is necessarily true.\n\nAssertion C: It is not necessarily true. For a counterexample, let $B=\\Omega$ so that $P(B)=1$. Then $A$ is independent of $B$ and $B$ is independent of $C$ for any events $A,C$, by the calculation in B. Choose $A$ and $C$ to be dependent (for instance, take any nontrivial dependent pair in a finite probability space). Then $A$ is not independent of $C$. Thus, C is not necessarily true.\n\nAssertion D: If $A$ were independent of $A^{c}$, then\n$$\nP(A\\cap A^{c})=P(A)P(A^{c}).\n$$\nThe left-hand side equals $0$, whereas the right-hand side equals $P(A)\\bigl(1-P(A)\\bigr)$. Hence\n$$\n0=P(A)\\bigl(1-P(A)\\bigr).\n$$\nThis equality holds only when $P(A)\\in\\{0,1\\}$. Therefore, if $0<P(A)<1$, $A$ cannot be independent of $A^{c}$. Thus, D is necessarily true.\n\nCollecting the conclusions: A is false, B is true, C is false, and D is true. The necessarily true assertions are B and D.", "answer": "$$\\boxed{BD}$$", "id": "1422232"}, {"introduction": "A common pitfall is to equate independence with having zero covariance. This practice [@problem_id:1422221] demonstrates why this is incorrect by presenting a classic counterexample involving a random variable $X$ and its square $X^2$. By analyzing this seemingly simple relationship, you will discover that variables can be highly dependent yet uncorrelated, a critical distinction for any practitioner of probability and statistics.", "problem": "Let $X$ be a random variable that follows a standard normal distribution, meaning its probability density function is given by $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$ for $x \\in (-\\infty, \\infty)$. Consider two new random variables, $Y$ and $Z$, defined by $Y = X$ and $Z = X^2$.\n\nWhich of the following statements accurately describes the relationship between the random variables $Y$ and $Z$?\n\nA. $Y$ and $Z$ are independent because the expectation of their product, $E[YZ]$, is equal to the product of their expectations, $E[Y]E[Z]$.\n\nB. $Y$ and $Z$ are dependent because knowledge of the value of $Z$ provides information that restricts the possible values of $Y$.\n\nC. $Y$ and $Z$ are independent because the probability distribution of $Y$ is symmetric about the origin.\n\nD. $Y$ and $Z$ are dependent because their covariance, $\\text{Cov}(Y, Z)$, is non-zero.\n\nE. The relationship between $Y$ and $Z$ cannot be determined from the given information alone.", "solution": "We are given a standard normal random variable $X$ with density $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)$ for $x \\in (-\\infty,\\infty)$, and define $Y = X$ and $Z = X^{2}$.\n\nFirst, observe that $Z$ is a measurable function of $Y$, specifically $Z = g(Y)$ with $g(y) = y^{2}$. A fundamental probability principle is that if $Z = g(Y)$ is non-constant almost surely, then $Y$ and $Z$ cannot be independent: knowledge of $Z$ restricts $Y$ to the set $\\{y : y^{2} = Z\\}$. Concretely, for any $z > 0$, knowing $Z = z$ restricts $Y$ to the two-point set $\\{-\\sqrt{z}, \\sqrt{z}\\}$, which shows dependence. This directly supports statement B.\n\nNow evaluate the claims one by one using standard definitions.\n\nRegarding A: Independence implies $E[YZ] = E[Y]E[Z]$, but the converse does not generally hold. Compute\n$$\nE[YZ] = E[X \\cdot X^{2}] = E[X^{3}] = \\int_{-\\infty}^{\\infty} x^{3} \\phi(x) \\, dx = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} x^{3} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\, dx = 0,\n$$\nsince the integrand is an odd function over a symmetric domain. Also,\n$$\nE[Y] = E[X] = 0 \\quad \\text{(by symmetry of the standard normal)},\n$$\nand\n$$\nE[Z] = E[X^{2}] = \\operatorname{Var}(X) + (E[X])^{2} = 1 + 0 = 1.\n$$\nThus $E[YZ] = 0 = E[Y]E[Z]$, but this equality alone does not imply independence. Therefore A is false.\n\nRegarding C: The symmetry of $Y$ about the origin does not imply independence between $Y$ and $Z$; independence requires factorizations of joint distributions (or expectations of products of a sufficiently rich class of functions), which does not follow from symmetry. Hence C is false.\n\nRegarding D: Compute the covariance using $\\operatorname{Cov}(Y,Z) = E[YZ] - E[Y]E[Z]$. From the computations above,\n$$\n\\operatorname{Cov}(Y,Z) = 0 - 0 \\cdot 1 = 0.\n$$\nTherefore the covariance is zero, not non-zero, so D is false. (Moreover, even zero covariance would not imply independence in general.)\n\nRegarding E: The relationship is determined from the given information because $Z$ is a non-constant function of $Y$, implying dependence. Hence E is false.\n\nTherefore, the correct statement is B: $Y$ and $Z$ are dependent because knowledge of $Z$ restricts the possible values of $Y$.", "answer": "$$\\boxed{B}$$", "id": "1422221"}, {"introduction": "After establishing that uncorrelatedness does not imply independence, we now investigate an important special case where it does. This problem [@problem_id:1422273] focuses on jointly normal random variables, which are foundational to many scientific models. You will derive the condition under which linear combinations of these variables are independent, highlighting the powerful simplification that arises when working with Gaussian distributions.", "problem": "Let $X$ and $Y$ be two independent random variables, both following a standard normal distribution, i.e., with a mean of 0 and a variance of 1. Consider two new random variables, $U$ and $V$, which are defined as linear combinations of $X$ and $Y$ with real constant coefficients $a, b, c, d$:\n$$U = aX + bY$$\n$$V = cX + dY$$\nWe assume that the coefficient pairs $(a, b)$ and $(c, d)$ are not both zero, meaning $a^2+b^2 \\neq 0$ and $c^2+d^2 \\neq 0$.\n\nWhat is the necessary and sufficient condition on the constants $a, b, c, d$ for the random variables $U$ and $V$ to be independent?\n\nA. $ab + cd = 0$\n\nB. $ac + bd = 0$\n\nC. $ad - bc = 0$\n\nD. $a^2+b^2 = c^2+d^2$\n\nE. $a+b = c+d$", "solution": "Let $(X,Y)$ be independent standard normal, so $E[X]=E[Y]=0$, $\\operatorname{Var}(X)=\\operatorname{Var}(Y)=1$, and $E[XY]=E[X]E[Y]=0$. Define $U=aX+bY$ and $V=cX+dY$. The vector $(U,V)$ is a linear transform of the jointly normal vector $(X,Y)$; hence $(U,V)$ is jointly normal. For jointly normal variables, independence is equivalent to zero covariance.\n\nCompute the covariance:\n$$\n\\operatorname{Cov}(U,V)=E[UV]-E[U]E[V].\n$$\nSince $E[U]=aE[X]+bE[Y]=0$ and $E[V]=cE[X]+dE[Y]=0$, we have\n$$\n\\operatorname{Cov}(U,V)=E[(aX+bY)(cX+dY)].\n$$\nExpanding and using linearity of expectation,\n$$\nE[(aX+bY)(cX+dY)]=ac\\,E[X^{2}]+bd\\,E[Y^{2}]+ad\\,E[XY]+bc\\,E[XY].\n$$\nUsing $E[X^{2}]=\\operatorname{Var}(X)=1$, $E[Y^{2}]=\\operatorname{Var}(Y)=1$, and $E[XY]=0$, we get\n$$\n\\operatorname{Cov}(U,V)=ac+bd.\n$$\nTherefore, $\\operatorname{Cov}(U,V)=0$ if and only if $ac+bd=0$. Since $(U,V)$ is jointly normal, this condition is necessary and sufficient for independence.\n\nThus the correct option is $ac+bd=0$, which corresponds to choice B.", "answer": "$$\\boxed{B}$$", "id": "1422273"}]}