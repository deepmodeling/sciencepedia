{"hands_on_practices": [{"introduction": "The Strong Law of Large Numbers (SLLN) provides a powerful guarantee about the long-term behavior of random processes. This first exercise demonstrates the law in its most direct form [@problem_id:1460774]. By considering a sequence of independent and identically distributed (i.i.d.) random variables from a given probability distribution, you will calculate the theoretical expectation and confirm that the SLLN ensures the sample average converges to this exact value almost surely.", "problem": "A research team is studying the output of a novel signal processor. Each time the processor runs, it generates a normalized value, which can be modeled as a random variable. A sequence of these values, $X_1, X_2, X_3, \\dots$, is recorded. These values are considered to be independent and identically distributed (i.i.d.) random variables. The theoretical model for the probability distribution of any single value $X_i$ is described by the probability density function (PDF):\n$$\nf(x) = \\begin{cases} 3x^2  \\text{for } 0 \\le x \\le 1 \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe long-term average of these measurements after $n$ trials is given by the sample mean $S_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. As the number of trials $n$ grows infinitely large, the value of $S_n$ will converge almost surely to a specific constant.\n\nDetermine the value of this constant. Express your answer as a fraction in simplest form.", "solution": "We are given independent and identically distributed random variables $X_{1},X_{2},\\dots$ with common density\n$$\nf(x)=\\begin{cases}\n3x^{2},  0\\le x\\le 1,\\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\nFirst, verify that $f$ is a valid probability density by checking normalization:\n$$\n\\int_{-\\infty}^{\\infty} f(x)\\,dx=\\int_{0}^{1}3x^{2}\\,dx=3\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}=1.\n$$\nBy the Strong Law of Large Numbers, since the $X_{i}$ are i.i.d. with finite mean $E[|X_{1}|]\\infty$ (which holds because $0\\le X_{1}\\le 1$ almost surely), the sample mean\n$$\nS_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\n$$\nconverges almost surely to $E[X_{1}]$ as $n\\to\\infty$.\n\nCompute the expectation:\n$$\nE[X_{1}]=\\int_{-\\infty}^{\\infty} x f(x)\\,dx=\\int_{0}^{1} x\\cdot 3x^{2}\\,dx=3\\int_{0}^{1} x^{3}\\,dx=3\\left[\\frac{x^{4}}{4}\\right]_{0}^{1}=\\frac{3}{4}.\n$$\nTherefore,\n$$\n\\lim_{n\\to\\infty} S_{n}=\\frac{3}{4}\\quad \\text{almost surely}.\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1460774"}, {"introduction": "While powerful, the conclusions of the SLLN are not unconditional, as its application relies on certain key assumptions. This practice explores the crucial requirement of a finite expected value, $E[|X|]  \\infty$ [@problem_id:1406772]. Using the heavy-tailed Pareto distribution, a model often seen in economics and insurance, you will determine the precise conditions under which an average stabilizes, providing critical insight into why this assumption is necessary for making reliable long-term predictions.", "problem": "An insurance company analyzes risk associated with catastrophic events. The financial size of a claim, denoted by a random variable $X$, is modeled using a Pareto distribution, which is suitable for phenomena with heavy tails. A sequence of claims $X_1, X_2, \\dots, X_n$ are assumed to be independent and identically distributed (i.i.d.).\n\nThe probability density function (PDF) for a single claim size $X$ is given by:\n$$f(x) = \\begin{cases} \\frac{\\alpha x_m^{\\alpha}}{x^{\\alpha+1}}  \\text{for } x \\ge x_m \\\\ 0  \\text{for } x  x_m \\end{cases}$$\nHere, $\\alpha  0$ is the shape parameter that determines the heaviness of the tail, and $x_m  0$ is the minimum possible claim size (the scale parameter).\n\nFor the company's long-term financial models to be stable, it is essential that the sample mean of the claim sizes, $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$, converges almost surely to a finite, non-zero constant as the number of claims $n$ approaches infinity.\n\nWhich of the following conditions on the shape parameter $\\alpha$ ensures this convergence?\n\nA. $\\alpha  0$\n\nB. $\\alpha  1$\n\nC. $\\alpha  2$\n\nD. $0  \\alpha \\le 1$\n\nE. $1  \\alpha \\le 2$", "solution": "We are given i.i.d. claim sizes $X_{1},X_{2},\\dots,X_{n}$ with common Pareto distribution having density\n$$\nf(x)=\\begin{cases}\n\\dfrac{\\alpha x_{m}^{\\alpha}}{x^{\\alpha+1}}  \\text{for } x\\geq x_{m},\\\\\n0  \\text{for } xx_{m},\n\\end{cases}\n$$\nwhere $\\alpha0$ and $x_{m}0$. The sample mean is $\\bar{X}_{n}=\\dfrac{1}{n}\\sum_{i=1}^{n}X_{i}$. By the strong law of large numbers (SLLN), if the $X_{i}$ are i.i.d. and $\\mathbb{E}[|X|]\\infty$, then\n$$\n\\bar{X}_{n}\\xrightarrow{\\text{a.s.}}\\mathbb{E}[X].\n$$\nTherefore, for $\\bar{X}_{n}$ to converge almost surely to a finite, non-zero constant, it is necessary and sufficient that $\\mathbb{E}[X]$ exist, be finite, and be positive.\n\nWe compute $\\mathbb{E}[X]$ for the Pareto distribution:\n$$\n\\mathbb{E}[X]=\\int_{x_{m}}^{\\infty}x\\,f(x)\\,dx=\\int_{x_{m}}^{\\infty}x\\cdot\\frac{\\alpha x_{m}^{\\alpha}}{x^{\\alpha+1}}\\,dx=\\alpha x_{m}^{\\alpha}\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx.\n$$\nFor $\\alpha\\neq 1$, we evaluate the integral\n$$\n\\int x^{-\\alpha}\\,dx=\\frac{x^{1-\\alpha}}{1-\\alpha},\n$$\nso\n$$\n\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx=\\lim_{b\\to\\infty}\\frac{b^{1-\\alpha}-x_{m}^{1-\\alpha}}{1-\\alpha}.\n$$\nThis converges if and only if $\\alpha1$. When $\\alpha1$, $1-\\alpha0$ and $b^{1-\\alpha}\\to 0$, giving\n$$\n\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx=\\frac{0-x_{m}^{1-\\alpha}}{1-\\alpha}=\\frac{x_{m}^{1-\\alpha}}{\\alpha-1}.\n$$\nHence, for $\\alpha1$,\n$$\n\\mathbb{E}[X]=\\alpha x_{m}^{\\alpha}\\cdot\\frac{x_{m}^{1-\\alpha}}{\\alpha-1}=\\frac{\\alpha x_{m}}{\\alpha-1},\n$$\nwhich is finite and strictly positive since $x_{m}0$.\n\nFor $\\alpha=1$,\n$$\n\\mathbb{E}[X]=\\alpha x_{m}^{\\alpha}\\int_{x_{m}}^{\\infty}x^{-1}\\,dx=x_{m}\\int_{x_{m}}^{\\infty}\\frac{dx}{x},\n$$\nwhich diverges to $+\\infty$. For $0\\alpha1$, the integral $\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx$ diverges because $1-\\alpha0$ implies $b^{1-\\alpha}\\to\\infty$. Thus $\\mathbb{E}[X]=\\infty$ for $\\alpha\\leq 1$.\n\nTherefore, the SLLN yields almost sure convergence of $\\bar{X}_{n}$ to the finite, non-zero constant $\\mathbb{E}[X]=\\dfrac{\\alpha x_{m}}{\\alpha-1}$ if and only if $\\alpha1$. Among the options, this corresponds to $\\alpha1$.\n\nOption C, $\\alpha2$, is sufficient but not necessary; the minimal and correct condition is $\\alpha1$, which is option B.", "answer": "$$\\boxed{B}$$", "id": "1406772"}, {"introduction": "The versatility of the SLLN extends far beyond simple averages, forming the bedrock of modern statistical estimation. This final practice demonstrates how the law can be cleverly applied to estimate a conditional expectation, a fundamental quantity in data analysis and machine learning [@problem_id:1957061]. You will see how the challenge of finding an average outcome, given that a specific condition is met, can be elegantly solved by constructing a ratio of two sample averages, both of which are guaranteed to converge by the SLLN.", "problem": "In a study of the performance of a distributed computing cluster, a pair of random variables $(X, Y)$ is measured repeatedly. Let $(X_i, Y_i)$, for $i = 1, 2, \\ldots, n$, be a sequence of independent and identically distributed (i.i.d.) random vectors representing these measurements. Here, $X_i$ is a normalized measure of the computational load, and $Y_i$ is the associated power consumption. The random vector $(X, Y)$ is described by the joint probability density function (PDF)\n$$\nf(x,y) = \\begin{cases} \\frac{1}{3}(x+y)  \\text{for } 0 \\le x \\le 2 \\text{ and } 0 \\le y \\le 1 \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nIt is given that the first absolute moment of $Y$, $E[|Y|]$, is finite.\n\nA systems analyst wants to estimate the average power consumption specifically during periods of low computational load. The \"low load\" condition is defined by the event $X \\in A$, where $A$ is the interval $[0, 1]$. To do this, the analyst computes the following estimator from the $n$ collected data points:\n$$\n\\hat{\\mu}_A = \\frac{\\sum_{i=1}^n Y_i I(X_i \\in A)}{\\sum_{i=1}^n I(X_i \\in A)}\n$$\nwhere $I(E)$ is the indicator function, which equals 1 if the event $E$ is true, and 0 otherwise.\n\nAssuming a very large number of measurements are taken, determine the value to which the estimator $\\hat{\\mu}_A$ converges almost surely as $n \\to \\infty$. Express your answer as an exact fraction.", "solution": "Define $A=[0,1]$ and write $N_{n}(A)=\\sum_{i=1}^{n} I(X_{i}\\in A)$ and $S_{n}(A)=\\sum_{i=1}^{n} Y_{i} I(X_{i}\\in A)$. The estimator is $\\hat{\\mu}_{A}=S_{n}(A)/N_{n}(A)$ when $N_{n}(A)0$.\n\nBy the strong law of large numbers and the assumption $E[|Y|]\\infty$, we have\n$$\n\\frac{N_{n}(A)}{n}\\to P(X\\in A)\\quad\\text{a.s.},\\qquad \\frac{S_{n}(A)}{n}\\to E\\!\\left[Y I(X\\in A)\\right]\\quad\\text{a.s.}\n$$\nSince $P(X\\in A)0$ (verified below), the continuous mapping theorem yields\n$$\n\\hat{\\mu}_{A}=\\frac{S_{n}(A)/n}{N_{n}(A)/n}\\to \\frac{E\\!\\left[Y I(X\\in A)\\right]}{P(X\\in A)}=E\\!\\left[Y\\mid X\\in A\\right]\\quad\\text{a.s.}\n$$\n\nWe compute $P(X\\in A)$ and $E\\!\\left[Y I(X\\in A)\\right]$ from the joint density $f(x,y)=\\frac{1}{3}(x+y)$ on $0\\le x\\le 2$, $0\\le y\\le 1$.\n\nFirst, the marginal density of $X$ for $0\\le x\\le 2$ is\n$$\nf_{X}(x)=\\int_{0}^{1}\\frac{1}{3}(x+y)\\,dy=\\frac{1}{3}\\left(x+\\frac{1}{2}\\right).\n$$\nThus\n$$\nP(X\\in A)=\\int_{0}^{1} f_{X}(x)\\,dx=\\int_{0}^{1}\\frac{1}{3}\\left(x+\\frac{1}{2}\\right)\\,dx=\\frac{1}{3}\\left(\\frac{1}{2}+\\frac{1}{2}\\right)=\\frac{1}{3}.\n$$\n\nNext,\n$$\nE\\!\\left[Y I(X\\in A)\\right]=\\int_{0}^{1}\\int_{0}^{1} y\\,\\frac{1}{3}(x+y)\\,dy\\,dx.\n$$\nCompute the inner integral:\n$$\n\\int_{0}^{1} y\\,\\frac{1}{3}(x+y)\\,dy=\\frac{1}{3}\\int_{0}^{1}(x y+y^{2})\\,dy=\\frac{1}{3}\\left(\\frac{x}{2}+\\frac{1}{3}\\right)=\\frac{x}{6}+\\frac{1}{9}.\n$$\nThen integrate over $x\\in[0,1]$:\n$$\n\\int_{0}^{1}\\left(\\frac{x}{6}+\\frac{1}{9}\\right)\\,dx=\\left.\\left(\\frac{x^{2}}{12}+\\frac{x}{9}\\right)\\right|_{0}^{1}=\\frac{1}{12}+\\frac{1}{9}=\\frac{7}{36}.\n$$\n\nTherefore,\n$$\nE\\!\\left[Y\\mid X\\in A\\right]=\\frac{E\\!\\left[Y I(X\\in A)\\right]}{P(X\\in A)}=\\frac{\\frac{7}{36}}{\\frac{1}{3}}=\\frac{7}{12}.\n$$\nHence $\\hat{\\mu}_{A}\\to \\frac{7}{12}$ almost surely as $n\\to\\infty$.", "answer": "$$\\boxed{\\frac{7}{12}}$$", "id": "1957061"}]}