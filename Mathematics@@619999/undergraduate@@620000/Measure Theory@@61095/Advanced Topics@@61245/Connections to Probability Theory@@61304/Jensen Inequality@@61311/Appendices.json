{"hands_on_practices": [{"introduction": "The first step in mastering any mathematical tool is to understand its fundamental definition. This exercise focuses directly on the core statement of Jensen's inequality for a convex function. By working with the function $f(x) = \\frac{x}{1-x}$, you will practice applying the inequality to correctly relate the average of the function's outputs to the function's output at the average input. This practice is essential for building a solid foundation and ensuring you can correctly orient the inequality, a common point of confusion for beginners. [@problem_id:2304592]", "problem": "Let $x_1, x_2, \\dots, x_n$ be a set of $n$ real numbers such that $x_i \\in (0, 1)$ for all $i=1, 2, \\dots, n$. The arithmetic mean of these numbers is defined as $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$. Consider the function $f(x) = \\frac{x}{1-x}$, which is known to be convex on the interval $(0, 1)$.\n\nBased on these definitions, which of the following inequalities correctly relates the arithmetic mean of the function values, $\\frac{1}{n} \\sum_{i=1}^{n} f(x_i)$, to the function of the arithmetic mean, $f(\\bar{x})$?\n\nA. $\\frac{1}{n} \\sum_{i=1}^{n} \\frac{x_i}{1-x_i} \\ge \\frac{\\bar{x}}{1-\\bar{x}}$\n\nB. $\\frac{1}{n} \\sum_{i=1}^{n} \\frac{x_i}{1-x_i} \\le \\frac{\\bar{x}}{1-\\bar{x}}$\n\nC. $\\frac{1}{n} \\sum_{i=1}^{n} \\frac{x_i}{1-x_i} = \\frac{\\bar{x}}{1-\\bar{x}}$\n\nD. $\\frac{1}{n} \\sum_{i=1}^{n} \\frac{x_i}{1-x_i} \\ge \\frac{1-\\bar{x}}{\\bar{x}}$\n\nE. $\\frac{1}{n} \\sum_{i=1}^{n} \\frac{x_i}{1-x_i} \\ge \\frac{n\\bar{x}}{1-n\\bar{x}}$", "solution": "We are given $x_{i} \\in (0,1)$ for $i=1,2,\\dots,n$, the arithmetic mean $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}$ with $\\bar{x} \\in (0,1)$, and the function $f(x) = \\frac{x}{1-x}$.\n\nFirst, we verify convexity of $f$ on $(0,1)$ by computing derivatives. The first derivative is\n$$\nf'(x) = \\frac{d}{dx}\\left(\\frac{x}{1-x}\\right) = \\frac{1}{(1-x)^{2}},\n$$\nand the second derivative is\n$$\nf''(x) = \\frac{d}{dx}\\left(\\frac{1}{(1-x)^{2}}\\right) = 2(1-x)^{-3} = \\frac{2}{(1-x)^{3}}.\n$$\nSince $x \\in (0,1)$ implies $(1-x) > 0$, we have $f''(x) > 0$ on $(0,1)$, hence $f$ is convex on $(0,1)$.\n\nBy Jensen's inequality for a convex function $f$, we have\n$$\nf\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n} x_{i}\\right) \\le \\frac{1}{n} \\sum_{i=1}^{n} f(x_{i}).\n$$\nSubstituting $f(x) = \\frac{x}{1-x}$ and $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}$ yields\n$$\n\\frac{\\bar{x}}{1-\\bar{x}} \\le \\frac{1}{n} \\sum_{i=1}^{n} \\frac{x_{i}}{1-x_{i}}.\n$$\nThis is exactly the inequality in option A. Equality holds if and only if all $x_{i}$ are equal, so option C is not generally valid. Options B, D, and E do not follow from convexity and are not generally true.", "answer": "$$\\boxed{A}$$", "id": "2304592"}, {"introduction": "Now, let's apply Jensen's inequality to a concept familiar from statistics and physics: the relationship between the arithmetic mean and the root-mean-square (RMS). This problem demonstrates that the inequality is not just an abstract theorem but a powerful tool for proving fundamental properties of descriptive statistics. By using the simple convex function $f(x) = x^2$, youâ€™ll derive a bedrock result that underpins the concept of variance and reveals the geometric intuition behind different types of averages. [@problem_id:2304611]", "problem": "In statistical analysis of physical measurements, different types of averages are used to characterize a dataset. Consider a set of $N$ non-negative experimental outcomes, denoted by $\\{x_1, x_2, \\ldots, x_N\\}$, where each $x_i \\ge 0$.\n\nTwo fundamental quantities describing this dataset are:\n1.  The arithmetic mean, $\\mu$, defined as $\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i$.\n2.  The Mean Square Value, $M_2$, defined as the square of the Root-Mean-Square (RMS) value: $M_2 = \\left(\\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} x_i^2}\\right)^2 = \\frac{1}{N} \\sum_{i=1}^{N} x_i^2$.\n\nDetermine the greatest possible lower bound for the Mean Square Value, $M_2$, that can be expressed solely in terms of the arithmetic mean, $\\mu$. This bound must be valid for any choice of $N$ and any set of non-negative values $\\{x_1, x_2, \\ldots, x_N\\}$.", "solution": "We are given $x_{i} \\ge 0$ for $i=1,\\ldots,N$, the arithmetic mean $\\mu = \\frac{1}{N}\\sum_{i=1}^{N} x_{i}$, and the mean square value $M_{2} = \\frac{1}{N}\\sum_{i=1}^{N} x_{i}^{2}$. We seek the greatest possible lower bound for $M_{2}$ in terms of $\\mu$, valid for any $N$ and any such dataset.\n\nConsider the variance identity derived from the definition of variance:\n$$\n\\frac{1}{N}\\sum_{i=1}^{N} (x_{i} - \\mu)^{2} \\ge 0.\n$$\nExpanding the square and summing term-by-term gives\n$$\n\\frac{1}{N}\\sum_{i=1}^{N} \\left(x_{i}^{2} - 2\\mu x_{i} + \\mu^{2}\\right)\n= \\frac{1}{N}\\sum_{i=1}^{N} x_{i}^{2} - \\frac{2\\mu}{N}\\sum_{i=1}^{N} x_{i} + \\frac{1}{N}\\sum_{i=1}^{N} \\mu^{2}.\n$$\nUsing $\\sum_{i=1}^{N} x_{i} = N\\mu$ and $\\sum_{i=1}^{N} \\mu^{2} = N\\mu^{2}$, we obtain\n$$\n\\frac{1}{N}\\sum_{i=1}^{N} (x_{i} - \\mu)^{2}\n= \\frac{1}{N}\\sum_{i=1}^{N} x_{i}^{2} - 2\\mu^{2} + \\mu^{2}\n= \\left(\\frac{1}{N}\\sum_{i=1}^{N} x_{i}^{2}\\right) - \\mu^{2}\n= M_{2} - \\mu^{2}.\n$$\nSince the left-hand side is nonnegative, it follows that\n$$\nM_{2} - \\mu^{2} \\ge 0 \\quad \\Longrightarrow \\quad M_{2} \\ge \\mu^{2}.\n$$\nEquality holds if and only if $\\frac{1}{N}\\sum_{i=1}^{N} (x_{i} - \\mu)^{2} = 0$, which occurs exactly when $x_{1} = x_{2} = \\cdots = x_{N} = \\mu$. Therefore, the lower bound $M_{2} \\ge \\mu^{2}$ is tight and thus is the greatest possible lower bound that depends only on $\\mu$ and holds for all $N$ and all nonnegative datasets.", "answer": "$$\\boxed{\\mu^{2}}$$", "id": "2304611"}, {"introduction": "Jensen's inequality is not limited to convex functions; it is equally powerful when applied to concave functions, where the direction of the inequality is reversed. This problem explores this dual aspect within the context of probability theory, specifically concerning the Beta distribution. By using the concave nature of the natural logarithm function, you will find an upper bound for the expected value $E[\\ln(X)]$, a quantity related to concepts like entropy in information theory, without performing any difficult integration. This exercise showcases the versatility of Jensen's inequality in deriving useful bounds in advanced statistical analysis. [@problem_id:1926102]", "problem": "In Bayesian statistics, the Beta distribution serves as a conjugate prior for the parameter of a Bernoulli process. Let a random variable $X$ follow a Beta distribution with positive shape parameters $\\alpha > 0$ and $\\beta > 0$. The support of this distribution is the open interval $(0, 1)$. The expected value, or mean, of this random variable is given by the expression $E[X] = \\frac{\\alpha}{\\alpha + \\beta}$.\n\nWithout performing any integration, find a simple upper bound for the expected value of the natural logarithm of the random variable, denoted as $E[\\ln(X)]$. Express your answer as a closed-form analytic expression in terms of $\\alpha$ and $\\beta$.", "solution": "We are asked for a simple upper bound on $E[\\ln(X)]$ when $X \\sim \\operatorname{Beta}(\\alpha,\\beta)$ with $\\alpha>0$ and $\\beta>0$, without performing integration.\n\nFirst, recall that the natural logarithm function $\\ln(x)$ is concave on $(0,\\infty)$. Since the support of $X$ is $(0,1)$, this concavity applies on the support of $X$. By Jensen's inequality for a concave function $f$, we have $E[f(X)] \\leq f(E[X])$ whenever the expectation exists. Applying this with $f(x)=\\ln(x)$ gives\n$$\nE[\\ln(X)] \\leq \\ln\\big(E[X]\\big).\n$$\nFor a Beta random variable with parameters $\\alpha$ and $\\beta$, the mean is known (without integration) to be\n$$\nE[X] = \\frac{\\alpha}{\\alpha+\\beta}.\n$$\nSubstituting this into the Jensen bound yields the desired simple closed-form upper bound:\n$$\nE[\\ln(X)] \\leq \\ln\\!\\left(\\frac{\\alpha}{\\alpha+\\beta}\\right).\n$$\nThis completes the derivation of a simple upper bound in terms of $\\alpha$ and $\\beta$.", "answer": "$$\\boxed{\\ln\\!\\left(\\frac{\\alpha}{\\alpha+\\beta}\\right)}$$", "id": "1926102"}]}