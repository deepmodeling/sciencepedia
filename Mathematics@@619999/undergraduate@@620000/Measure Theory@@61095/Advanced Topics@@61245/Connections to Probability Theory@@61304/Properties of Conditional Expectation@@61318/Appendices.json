{"hands_on_practices": [{"introduction": "We begin our hands-on journey with a straightforward, discrete scenario involving independent sensors. This exercise is designed to build intuition by showing how conditioning on new information—in this case, the total number of detections—can refine our estimate of a related event. You will discover how the conditional expectation can sometimes be expressed as a simple, deterministic function of the information we are given [@problem_id:1438518].", "problem": "Two independent sensors, Sensor 1 and Sensor 2, are deployed to detect a specific environmental signature. For any given observation period, each sensor reports a binary outcome: '1' if the signature is detected and '0' otherwise. Let $X_1$ and $X_2$ be the random variables representing the reports from Sensor 1 and Sensor 2, respectively. Both sensors operate independently and are statistically identical, with a probability $p$ of reporting '1'. Thus, $X_1$ and $X_2$ can be modeled as independent and identically distributed Bernoulli random variables. An analyst needs to estimate the event of joint detection, which is captured by the product $X_1 X_2$. However, the only data available to the analyst is the total number of detections, i.e., the sum $S = X_1 + X_2$. Determine the analyst's best estimate for the joint detection event given only the information about the sum $S$. In the language of probability theory, find the conditional expectation $E[X_1 X_2 | S]$. Express your answer as a function of $S$.", "solution": "Let $X_{1}, X_{2} \\sim \\text{Bernoulli}(p)$ be independent and let $S = X_{1} + X_{2}$. The quantity of interest is $X_{1}X_{2}$, which equals $1$ if and only if both sensors report $1$.\n\nList the possible values of $S$ and determine $X_{1}X_{2}$ in each case:\n- If $S=0$, then $(X_{1},X_{2})=(0,0)$, so $X_{1}X_{2}=0$.\n- If $S=1$, then one of $X_{1},X_{2}$ is $1$ and the other is $0$, so $X_{1}X_{2}=0$.\n- If $S=2$, then $(X_{1},X_{2})=(1,1)$, so $X_{1}X_{2}=1$.\n\nThus $X_{1}X_{2}$ is completely determined by $S$ and equals the indicator of the event $\\{S=2\\}$. Equivalently, as a function of $S \\in \\{0,1,2\\}$,\n$$\nX_{1}X_{2} = \\frac{S(S-1)}{2},\n$$\nsince this expression evaluates to $0$ for $S=0$ or $S=1$ and to $1$ for $S=2$.\n\nBecause $X_{1}X_{2}$ is a measurable function of $S$, by the defining property of conditional expectation,\n$$\nE[X_{1}X_{2} \\mid S] = X_{1}X_{2} = \\frac{S(S-1)}{2}.\n$$\nThis result does not depend on $p$.", "answer": "$$\\boxed{\\frac{S(S-1)}{2}}$$", "id": "1438518"}, {"introduction": "Moving from discrete to continuous random variables, this practice introduces a geometric flavor to conditional expectation. By considering a point chosen uniformly from a triangular region, you will learn the mechanics of deriving a conditional expectation by first finding the conditional probability density function. This problem [@problem_id:1327081] provides a visual and computational bridge to understanding how to handle conditioning in continuous spaces.", "problem": "A point with coordinates $(X,Y)$ is chosen uniformly at random from a two-dimensional region. This region is a triangle in the Cartesian plane with vertices located at the points $(0,0)$, $(L, H)$, and $(L, -H)$, where $L$ and $H$ are positive real constants. Determine the conditional expectation of the random variable $X$ given that the random variable $Y$ takes a specific value $y$, where $-H < y < H$. Express your answer as a function of $y$, $L$, and $H$.", "solution": "Let the triangle be denoted by $T$ with vertices $(0,0)$, $(L,H)$, and $(L,-H)$. The uniform density over $T$ is constant, so the joint density is $f_{X,Y}(x,y)=c$ for $(x,y)\\in T$ and zero otherwise, where $c$ is determined by the area of $T$.\n\nThe area of $T$ is\n$$\n\\text{Area}(T)=\\frac{1}{2}\\cdot (2H)\\cdot L=HL,\n$$\nso\n$$\nc=\\frac{1}{HL}.\n$$\n\nFor a fixed $y$ with $-H<y<H$, the horizontal slice of $T$ at height $y$ runs from the left boundary given by the lines $y=\\pm\\frac{H}{L}x$ to the right boundary $x=L$. Solving $y=\\pm\\frac{H}{L}x$ for $x$ gives\n$$\nx_{\\text{left}}(y)=\\frac{L}{H}|y|, \\quad x_{\\text{right}}(y)=L.\n$$\nThus, conditional on $Y=y$, the support of $X$ is the interval $\\left[\\frac{L}{H}|y|,\\,L\\right]$.\n\nThe marginal density of $Y$ is\n$$\nf_{Y}(y)=\\int_{x_{\\text{left}}(y)}^{L} c \\, dx\n=\\frac{1}{HL}\\left(L-\\frac{L}{H}|y|\\right)\n=\\frac{1}{H}\\left(1-\\frac{|y|}{H}\\right),\n\\quad -H<y<H.\n$$\nThe conditional density of $X$ given $Y=y$ for $x\\in\\left[\\frac{L}{H}|y|,\\,L\\right]$ is\n$$\nf_{X\\mid Y}(x\\mid y)=\\frac{f_{X,Y}(x,y)}{f_{Y}(y)}\n=\\frac{\\frac{1}{HL}}{\\frac{1}{H}\\left(1-\\frac{|y|}{H}\\right)}\n=\\frac{1}{L-\\frac{L}{H}|y|},\n$$\nwhich is the uniform density on $\\left[\\frac{L}{H}|y|,\\,L\\right]$.\n\nTherefore, the conditional expectation is the midpoint of this interval:\n$$\n\\mathbb{E}[X\\mid Y=y]=\\frac{\\frac{L}{H}|y|+L}{2}\n=\\frac{L}{2}\\left(1+\\frac{|y|}{H}\\right),\n\\quad -H<y<H.\n$$", "answer": "$$\\boxed{\\frac{L}{2}\\left(1+\\frac{|y|}{H}\\right)}$$", "id": "1327081"}, {"introduction": "This final practice elevates our understanding by engaging directly with the formal, measure-theoretic definition of conditional expectation. Instead of relying on pre-derived formulas for densities, you will compute the conditional expectation with respect to a sub-$\\sigma$-algebra generated by a simple partition. This exercise [@problem_id:1438480] not only reinforces the fundamental properties of conditional expectation but also reveals a powerful result about non-negative random variables.", "problem": "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space where $\\Omega = [0, 1]$, $\\mathcal{F}$ is the Borel $\\sigma$-algebra on $[0, 1]$, and $P$ is the Lebesgue measure. Consider a random variable $X$ defined by\n$$\nX(\\omega) = \\begin{cases}\n\\alpha & \\text{if } \\omega \\in [0, 1/3) \\\\\n\\beta & \\text{if } \\omega \\in [1/3, 1]\n\\end{cases}\n$$\nwhere $\\alpha$ and $\\beta$ are non-negative real constants. Let $\\mathcal{G}$ be the sub-$\\sigma$-algebra of $\\mathcal{F}$ generated by the partition $\\{[0, 1/2), [1/2, 1]\\}$. Suppose it is observed that the conditional expectation $E[X|\\mathcal{G}]$ is zero almost surely. Which of the following statements about the constants $\\alpha$ and $\\beta$ must be true?\n\nA. $\\alpha = \\beta = 0$\n\nB. $\\alpha = 0$ and $\\beta$ can be any non-negative value.\n\nC. $\\beta = 0$ and $\\alpha$ can be any non-negative value.\n\nD. $\\alpha = \\beta/2$\n\nE. The relationship cannot be determined from the given information.", "solution": "We work on $(\\Omega,\\mathcal{F},P)$ with $\\Omega=[0,1]$, $\\mathcal{F}$ the Borel $\\sigma$-algebra, and $P$ the Lebesgue measure. The random variable $X$ is defined by $X(\\omega)=\\alpha$ for $\\omega\\in[0,\\frac{1}{3})$ and $X(\\omega)=\\beta$ for $\\omega\\in[\\frac{1}{3},1]$, where $\\alpha,\\beta\\geq 0$. Let $\\mathcal{G}$ be the sub-$\\sigma$-algebra generated by the partition $\\{[0,\\frac{1}{2}),[\\frac{1}{2},1]\\}$.\n\nBy the definition of conditional expectation with respect to a finite partition, $E[X\\mid\\mathcal{G}]$ is $\\mathcal{G}$-measurable and hence constant on each atom. Write\n$$\nE[X\\mid\\mathcal{G}](\\omega)=\\begin{cases}\nc_{1} & \\text{for }\\omega\\in[0,\\frac{1}{2}),\\\\\nc_{2} & \\text{for }\\omega\\in[\\frac{1}{2},1].\n\\end{cases}\n$$\nBy the defining property of conditional expectation, for each $A\\in\\mathcal{G}$ we have\n$$\n\\int_{A}X\\,dP=\\int_{A}E[X\\mid\\mathcal{G}]\\,dP.\n$$\nTaking $A=[0,\\frac{1}{2})$, we compute the left-hand side using the piecewise definition of $X$:\n$$\n\\int_{[0,\\frac{1}{2})}X\\,dP=\\int_{0}^{\\frac{1}{3}}\\alpha\\,d\\omega+\\int_{\\frac{1}{3}}^{\\frac{1}{2}}\\beta\\,d\\omega=\\alpha\\Bigl(\\frac{1}{3}\\Bigr)+\\beta\\Bigl(\\frac{1}{2}-\\frac{1}{3}\\Bigr)=\\frac{\\alpha}{3}+\\frac{\\beta}{6}.\n$$\nThe right-hand side equals\n$$\n\\int_{[0,\\frac{1}{2})}c_{1}\\,dP=c_{1}\\,P\\bigl([0,\\tfrac{1}{2})\\bigr)=c_{1}\\cdot\\frac{1}{2}.\n$$\nEquating both sides gives\n$$\nc_{1}\\cdot\\frac{1}{2}=\\frac{\\alpha}{3}+\\frac{\\beta}{6}\\quad\\Longrightarrow\\quad c_{1}=\\frac{2\\alpha+\\beta}{3}.\n$$\n\nNext, take $A=[\\frac{1}{2},1]$. Since $[\\frac{1}{2},1]\\subset[\\frac{1}{3},1]$, we have $X=\\beta$ on this set, so\n$$\n\\int_{[\\frac{1}{2},1]}X\\,dP=\\int_{\\frac{1}{2}}^{1}\\beta\\,d\\omega=\\beta\\Bigl(1-\\frac{1}{2}\\Bigr)=\\frac{\\beta}{2}.\n$$\nOn the other hand,\n$$\n\\int_{[\\frac{1}{2},1]}E[X\\mid\\mathcal{G}]\\,dP=\\int_{[\\frac{1}{2},1]}c_{2}\\,dP=c_{2}\\cdot\\frac{1}{2}.\n$$\nEquating both sides yields\n$$\nc_{2}\\cdot\\frac{1}{2}=\\frac{\\beta}{2}\\quad\\Longrightarrow\\quad c_{2}=\\beta.\n$$\n\nThe condition that $E[X\\mid\\mathcal{G}]=0$ almost surely implies that both constants must be zero on their atoms (each atom has positive measure), hence\n$$\nc_{1}=0\\quad\\text{and}\\quad c_{2}=0.\n$$\nFrom $c_{2}=0$ we obtain $\\beta=0$, and from $c_{1}=0$ we obtain $\\frac{2\\alpha+\\beta}{3}=0$, hence $2\\alpha+\\beta=0$. With $\\beta=0$, this gives $\\alpha=0$. Therefore, the only possibility consistent with the given condition is $\\alpha=\\beta=0$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1438480"}]}