{"hands_on_practices": [{"introduction": "The Weak Law of Large Numbers (WLLN) provides the theoretical backbone for one of the most fundamental tasks in statistics: estimating unknown parameters from data. This first practice puts the WLLN to work in a classic estimation scenario. You will use the sample mean of data from a uniform distribution to derive the convergence point for an estimator built using the method of moments, a cornerstone technique in statistical inference [@problem_id:864068]. This exercise provides a concrete link between abstract probability theory and its practical application in estimating the properties of a population from a sample.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables drawn from a continuous uniform distribution on the interval $[0, \\theta]$, where $\\theta > 0$ is an unknown parameter. The probability density function of each $X_i$ is given by:\n$$\nf(x; \\theta) = \\begin{cases} \\frac{1}{\\theta} & \\text{if } 0 \\le x \\le \\theta \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe sample mean is defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$. A common estimator for the parameter $\\theta$, derived from the method of moments, is $\\hat{\\theta}_n = 2\\bar{X}_n$.\n\nWe say that a sequence of random variables $Y_n$ converges in probability to a constant $c$ (denoted $Y_n \\xrightarrow{p} c$) if for every $\\epsilon > 0$, the following condition holds:\n$$\n\\lim_{n \\to \\infty} P(|Y_n - c| > \\epsilon) = 0\n$$\nThe Weak Law of Large Numbers (WLLN) states that if $X_1, X_2, \\ldots$ are i.i.d. random variables with a finite expected value $E[X_i] = \\mu$, then their sample mean $\\bar{X}_n$ converges in probability to $\\mu$.\n\nUsing the WLLN and the properties of convergence in probability, determine the value $c$ such that the estimator $\\hat{\\theta}_n$ converges in probability to $c$.", "solution": "We seek the limit in probability of $\\hat\\theta_n=2\\bar X_n$ using the WLLN.  \n1. The expectation of each $X_i\\sim\\mathrm{Uniform}[0,\\theta]$ is  \n$$\nE[X_i]=\\int_{0}^{\\theta}x\\,\\frac{1}{\\theta}\\,dx\n=\\frac{1}{\\theta}\\,\\frac{\\theta^2}{2}\n=\\frac{\\theta}{2}.\n$$  \n2. By the WLLN,  \n$$\n\\bar X_n=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\xrightarrow{p}E[X_i]\n=\\frac{\\theta}{2}.\n$$  \n3. Hence  \n$$\n\\hat\\theta_n\n=2\\bar X_n\n\\xrightarrow{p}2\\cdot\\frac{\\theta}{2}\n=\\theta.\n$$  \nTherefore, $\\hat\\theta_n$ converges in probability to $\\theta\\,$.", "answer": "$$\\boxed{\\theta}$$", "id": "864068"}, {"introduction": "The power of the WLLN extends beyond just the sample mean. In many real-world analyses, we are interested in the long-term behavior of a more complex statistic that is a function of this sample mean. This practice delves into this by combining the WLLN with the Continuous Mapping Theorem [@problem_id:863858]. By determining the probabilistic limit of a rational function of the sample mean, you will see how convergence is reliably preserved under continuous transformations, a principle that is essential for understanding the asymptotic properties of a wide range of statistical estimators.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables, where each $X_i$ follows a continuous uniform distribution on the interval $(0, \\theta)$, with $\\theta > 0$ being a fixed parameter. The sample mean is defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$.\n\nConsider a new random variable $Y_n$, which is a rational function of the sample mean:\n$$Y_n = \\frac{(\\bar{X}_n)^2}{1 + \\bar{X}_n}$$\n\nUsing the Weak Law of Large Numbers and its associated properties for functions of random variables, derive the value $L$ to which $Y_n$ converges in probability as $n$ approaches infinity.", "solution": "1. For $X_i\\sim \\mathrm{Uniform}(0,\\theta)$, we have \n$$E[X_i]=\\frac{\\theta}{2},\\quad \\mathrm{Var}(X_i)=\\frac{\\theta^2}{12}.$$\nBy the Weak Law of Large Numbers,\n$$\\bar X_n=\\frac1n\\sum_{i=1}^nX_i\\;\\xrightarrow{p}\\;E[X_1]=\\frac{\\theta}{2}.$$\n2. Define the continuous function \n$$f(x)=\\frac{x^2}{1+x}.$$\nBy the Continuous Mapping Theorem,\n$$Y_n=f(\\bar X_n)\\;\\xrightarrow{p}\\;f\\bigl(E[X_1]\\bigr)\n=f\\Bigl(\\frac{\\theta}{2}\\Bigr).$$\n3. Compute the limit:\n$$f\\Bigl(\\frac{\\theta}{2}\\Bigr)\n=\\frac{\\bigl(\\frac{\\theta}{2}\\bigr)^2}{1+\\frac{\\theta}{2}}\n=\\frac{\\frac{\\theta^2}{4}}{\\frac{2+\\theta}{2}}\n=\\frac{\\theta^2}{2(\\theta+2)}.$$", "answer": "$$\\boxed{\\frac{\\theta^2}{2(\\theta+2)}}$$", "id": "863858"}, {"introduction": "The WLLN is often introduced with the strict assumption that observations are independent and identically distributed (i.i.d.). However, its true power is revealed in its application to more realistic models where these assumptions are relaxed, such as in econometrics and signal processing. This advanced practice challenges you to apply the law's principles to a moving-average (MA) time series, a process where each observation is correlated with the previous one [@problem_id:864090]. By solving this, you will discover that the sample mean can still converge to a predictable constant, demonstrating the robustness of the Law of Large Numbers in the presence of certain types of dependency.", "problem": "A stationary time series $\\{X_k\\}_{k\\in\\mathbb{Z}}$ is described by the moving-average model of order 1, denoted MA(1), which is given by the equation:\n$$\nX_k = Z_k + \\theta Z_{k-1}\n$$\nThe terms $\\{Z_k\\}_{k\\in\\mathbb{Z}}$ constitute a sequence of independent and identically distributed (i.i.d.) random variables, known as the innovation or noise process. These innovations have a finite mean $\\mathbb{E}[Z_k] = \\mu_Z$ and a finite variance $\\text{Var}(Z_k) = \\sigma_Z^2 < \\infty$. The parameter $\\theta$ is a fixed real constant.\n\nAs a foundational principle, recall the Weak Law of Large numbers (WLLN) for i.i.d. variables: if $\\{Y_i\\}$ is a sequence of i.i.d. random variables with finite mean $\\mathbb{E}[Y_i] = \\mu_Y$, their sample mean $\\bar{Y}_n = \\frac{1}{n} \\sum_{i=1}^n Y_i$ converges in probability to $\\mu_Y$. This is denoted as $\\bar{Y}_n \\xrightarrow{p} \\mu_Y$.\n\nThe sample mean of the process $\\{X_k\\}$ is defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{k=1}^n X_k$. While the variables $X_k$ are not independent, their sample mean still converges in probability to a constant value.\n\nDerive the value $L$ such that $\\bar{X}_n \\xrightarrow{p} L$ as $n \\to \\infty$. Express your answer in terms of $\\mu_Z$ and $\\theta$.", "solution": "1. Relevant equation: for the MA(1) process \n$$X_k = Z_k + \\theta Z_{k-1}.$$\n2. Compute the mean of $X_k$:\n$$\\mathbb{E}[X_k] = \\mathbb{E}[Z_k] + \\theta \\,\\mathbb{E}[Z_{k-1}]\n= \\mu_Z + \\theta\\,\\mu_Z = (1+\\theta)\\mu_Z.$$\n3. By stationarity and the Weak Law of Large Numbers for dependent but ergodic processes, the sample mean converges in probability to the true mean:\n$$\\bar{X}_n \\xrightarrow{p} \\mathbb{E}[X_k] = (1+\\theta)\\mu_Z.$$\nThus \n$$L = (1+\\theta)\\mu_Z.$$", "answer": "$$\\boxed{(1+\\theta)\\mu_Z}$$", "id": "864090"}]}