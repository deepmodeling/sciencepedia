{"hands_on_practices": [{"introduction": "Abstract inequalities are best understood through concrete examples. This exercise provides a hands-on opportunity to verify Jensen's inequality in a finite probability space using the convex function $\\phi(x) = \\exp(x)$. By explicitly computing both sides, you will gain a tangible intuition for how conditional expectation averages information and why the inequality holds [@problem_id:1425915].", "problem": "Consider a simple probabilistic model for a system with a finite number of states. The sample space is $\\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4\\}$. The probability measure $P$ on the power set of $\\Omega$ is defined by the probabilities of the elementary events: $P(\\{\\omega_1\\}) = \\frac{1}{8}$, $P(\\{\\omega_2\\}) = \\frac{1}{8}$, $P(\\{\\omega_3\\}) = \\frac{1}{4}$, and $P(\\{\\omega_4\\}) = \\frac{1}{2}$.\n\nA real-valued random variable $X$ is defined on this space as follows: $X(\\omega_1) = 1$, $X(\\omega_2) = 2$, $X(\\omega_3) = 3$, and $X(\\omega_4) = 4$.\n\nInformation about the system is coarse-grained, and we only observe which of two disjoint events has occurred: $A_1 = \\{\\omega_1, \\omega_2\\}$ or $A_2 = \\{\\omega_3, \\omega_4\\}$. This observation structure is captured by the sub-$\\sigma$-algebra $\\mathcal{G} = \\{\\emptyset, A_1, A_2, \\Omega\\}$, which is generated by the partition $\\{A_1, A_2\\}$.\n\nWe are interested in two derived random variables. The first is $Y_1 = E[e^X|\\mathcal{G}]$, which is the conditional expectation of $e^X$ given $\\mathcal{G}$. The second is $Y_2 = e^{E[X|\\mathcal{G}]}$, which is the exponential of the conditional expectation of $X$ given $\\mathcal{G}$.\n\nDetermine the value of the random variable $D = Y_1 - Y_2$ for any outcome in the event set $A_2$.", "solution": "We have a finite probability space $\\Omega=\\{\\omega_{1},\\omega_{2},\\omega_{3},\\omega_{4}\\}$ with\n$$\nP(\\{\\omega_{1}\\})=\\frac{1}{8},\\quad P(\\{\\omega_{2}\\})=\\frac{1}{8},\\quad P(\\{\\omega_{3}\\})=\\frac{1}{4},\\quad P(\\{\\omega_{4}\\})=\\frac{1}{2}.\n$$\nDefine $A_{1}=\\{\\omega_{1},\\omega_{2}\\}$ and $A_{2}=\\{\\omega_{3},\\omega_{4}\\}$, so the sub-$\\sigma$-algebra $\\mathcal{G}$ is generated by $\\{A_{1},A_{2}\\}$. The random variable $X$ satisfies\n$$\nX(\\omega_{1})=1,\\quad X(\\omega_{2})=2,\\quad X(\\omega_{3})=3,\\quad X(\\omega_{4})=4.\n$$\nFirst compute the probabilities of the partition elements:\n$$\nP(A_{1})=P(\\{\\omega_{1}\\})+P(\\{\\omega_{2}\\})=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4},\\qquad P(A_{2})=P(\\{\\omega_{3}\\})+P(\\{\\omega_{4}\\})=\\frac{1}{4}+\\frac{1}{2}=\\frac{3}{4}.\n$$\nBy definition of conditional expectation on a finite partition, for any $\\omega\\in A_{2}$,\n$$\nY_{1}(\\omega)=E[\\exp(X)\\mid\\mathcal{G}](\\omega)=E[\\exp(X)\\mid A_{2}]\n=\\frac{\\sum_{\\omega'\\in A_{2}}\\exp(X(\\omega'))\\,P(\\{\\omega'\\})}{P(A_{2})}.\n$$\nSubstitute the values on $A_{2}$:\n$$\n\\sum_{\\omega'\\in A_{2}}\\exp(X(\\omega'))\\,P(\\{\\omega'\\})\n=\\exp(3)\\cdot\\frac{1}{4}+\\exp(4)\\cdot\\frac{1}{2}=\\frac{\\exp(3)}{4}+\\frac{\\exp(4)}{2}.\n$$\nTherefore,\n$$\nY_{1}(\\omega)=\\frac{\\frac{\\exp(3)}{4}+\\frac{\\exp(4)}{2}}{\\frac{3}{4}}\n=\\left(\\frac{\\exp(3)+2\\exp(4)}{4}\\right)\\cdot\\frac{4}{3}\n=\\frac{\\exp(3)+2\\exp(4)}{3},\\qquad \\omega\\in A_{2}.\n$$\nNext, for any $\\omega\\in A_{2}$,\n$$\nY_{2}(\\omega)=\\exp\\!\\left(E[X\\mid\\mathcal{G}](\\omega)\\right)=\\exp\\!\\left(E[X\\mid A_{2}]\\right)\n=\\exp\\!\\left(\\frac{\\sum_{\\omega'\\in A_{2}}X(\\omega')\\,P(\\{\\omega'\\})}{P(A_{2})}\\right).\n$$\nCompute the conditional mean of $X$ on $A_{2}$:\n$$\n\\sum_{\\omega'\\in A_{2}}X(\\omega')\\,P(\\{\\omega'\\})\n=3\\cdot\\frac{1}{4}+4\\cdot\\frac{1}{2}=\\frac{3}{4}+2=\\frac{11}{4},\n$$\nhence\n$$\nE[X\\mid A_{2}]=\\frac{\\frac{11}{4}}{\\frac{3}{4}}=\\frac{11}{3},\n$$\nand thus\n$$\nY_{2}(\\omega)=\\exp\\!\\left(\\frac{11}{3}\\right),\\qquad \\omega\\in A_{2}.\n$$\nFinally, the random variable $D=Y_{1}-Y_{2}$ is constant on $A_{2}$ and equals\n$$\nD(\\omega)=\\frac{\\exp(3)+2\\exp(4)}{3}-\\exp\\!\\left(\\frac{11}{3}\\right),\\qquad \\omega\\in A_{2}.\n$$\nAs a consistency check, since $\\exp$ is convex, Jensen's inequality implies $Y_{1}\\geq Y_{2}$ on $A_{2}$, so $D(\\omega)\\geq 0$.", "answer": "$$\\boxed{\\frac{\\exp(3)+2\\exp(4)}{3}-\\exp\\!\\left(\\frac{11}{3}\\right)}$$", "id": "1425915"}, {"introduction": "We now generalize to a universally important application of the conditional Jensen's inequality using the convex function $\\phi(x) = x^2$. This problem guides you to establish the fundamental relationship $E[X^2|\\mathcal{G}] \\ge (E[X|\\mathcal{G}])^2$, which is the foundation for the concept of non-negative conditional variance. This principle is a cornerstone in statistics and stochastic processes [@problem_id:1425924].", "problem": "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space, and let $X$ be a random variable defined on this space such that its second moment is finite, i.e., $E[X^2] < \\infty$. Let $\\mathcal{G}$ be a sub-$\\sigma$-algebra of $\\mathcal{F}$. We are interested in the relationship between two random variables derived from $X$: the conditional second moment, $E[X^2|\\mathcal{G}]$, and the square of the conditional mean, $(E[X|\\mathcal{G}])^2$.\n\nWhich of the following statements correctly describes the relationship that holds almost surely between these two quantities for any such $X$ and $\\mathcal{G}$?\n\nA. $E[X^2|\\mathcal{G}] < (E[X|\\mathcal{G}])^2$\n\nB. $E[X^2|\\mathcal{G}] \\le (E[X|\\mathcal{G}])^2$\n\nC. $E[X^2|\\mathcal{G}] = (E[X|\\mathcal{G}])^2$\n\nD. $E[X^2|\\mathcal{G}] \\ge (E[X|\\mathcal{G}])^2$\n\nE. The relationship cannot be determined without more information about $X$ and $\\mathcal{G}$.", "solution": "We use that $E[X^{2}]<\\infty$ implies $X\\in L^{2}$, so $E[X\\mid\\mathcal{G}]$ exists and belongs to $L^{2}$, and conditional variance is well defined.\n\nConsider the conditional variance\n$$\n\\operatorname{Var}(X\\mid\\mathcal{G}) \\equiv E\\big[(X-E[X\\mid\\mathcal{G}])^{2}\\mid\\mathcal{G}\\big].\n$$\nBy definition and linearity of conditional expectation,\n$$\n\\operatorname{Var}(X\\mid\\mathcal{G})\n=E[X^{2}\\mid\\mathcal{G}]-2\\,E\\big[X\\,E[X\\mid\\mathcal{G}]\\mid\\mathcal{G}\\big]+E\\big[(E[X\\mid\\mathcal{G}])^{2}\\mid\\mathcal{G}\\big].\n$$\nUse the property that if $Y$ is $\\mathcal{G}$-measurable and integrable, then $E[XY\\mid\\mathcal{G}]=Y\\,E[X\\mid\\mathcal{G}]$. Taking $Y=E[X\\mid\\mathcal{G}]$, we obtain\n$$\nE\\big[X\\,E[X\\mid\\mathcal{G}]\\mid\\mathcal{G}\\big] = E[X\\mid\\mathcal{G}]\\,E[X\\mid\\mathcal{G}] = (E[X\\mid\\mathcal{G}])^{2}.\n$$\nSince $E[X\\mid\\mathcal{G}]$ is $\\mathcal{G}$-measurable, we also have\n$$\nE\\big[(E[X\\mid\\mathcal{G}])^{2}\\mid\\mathcal{G}\\big]=(E[X\\mid\\mathcal{G}])^{2}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(X\\mid\\mathcal{G})=E[X^{2}\\mid\\mathcal{G}]-(E[X\\mid\\mathcal{G}])^{2}.\n$$\nBy definition, $\\operatorname{Var}(X\\mid\\mathcal{G})\\ge 0$ almost surely. Hence,\n$$\nE[X^{2}\\mid\\mathcal{G}] \\ge (E[X\\mid\\mathcal{G}])^{2}\\quad\\text{almost surely}.\n$$\nEquality holds almost surely if and only if $\\operatorname{Var}(X\\mid\\mathcal{G})=0$ almost surely, i.e., when $X=E[X\\mid\\mathcal{G}]$ almost surely (for instance, if $X$ is $\\mathcal{G}$-measurable). This shows that a strict inequality need not hold, equality may occur, but the inequality direction is always $\\ge$. Equivalently, this is an instance of conditional Jensen's inequality for the convex function $t\\mapsto t^{2}$:\n$$\n(E[X\\mid\\mathcal{G}])^{2} \\le E[X^{2}\\mid\\mathcal{G}] \\quad\\text{almost surely}.\n$$\nThus the correct choice is D.", "answer": "$$\\boxed{D}$$", "id": "1425924"}, {"introduction": "A deep understanding of an inequality requires knowing the conditions under which it becomes an equality. This problem challenges you to characterize the random variables $X$ for which $E[\\phi(X) | \\mathcal{G}] = \\phi(E[X | \\mathcal{G}])$ holds for a strictly convex function $\\phi$. Solving this uncovers the crucial link between equality and $\\mathcal{G}$-measurability, revealing that Jensen's inequality quantifies the variability of $X$ that is unresolved by the information in $\\mathcal{G}$ [@problem_id:1425928].", "problem": "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space. Consider a countable partition of the sample space, $\\{A_n\\}_{n=1}^{\\infty}$, where each $A_n \\in \\mathcal{F}$ and $P(A_n) > 0$. Let $\\mathcal{G}$ be the sigma-algebra generated by this partition, i.e., $\\mathcal{G} = \\sigma(\\{A_n\\}_{n=1}^{\\infty})$.\n\nLet $X$ be an integrable real-valued random variable on this space. Let $\\phi: \\mathbb{R} \\to \\mathbb{R}$ be a strictly convex function, and assume that $\\phi(X)$ is also integrable.\n\nSuppose that the equality case of the conditional Jensen's inequality holds almost surely, which is to say:\n$$\nE[\\phi(X) | \\mathcal{G}] = \\phi(E[X | \\mathcal{G}]) \\quad P\\text{-almost surely.}\n$$\n\nWhich of the following statements provides the most accurate and general characterization of the random variable $X$?\n\nA. There exists a constant $c \\in \\mathbb{R}$ such that $P(X=c)=1$.\n\nB. $X$ is independent of the sigma-algebra $\\mathcal{G}$.\n\nC. There exists a $\\mathcal{G}$-measurable random variable $Y$ such that $P(X=Y)=1$.\n\nD. The random variable $X$ must be a discrete random variable, taking values only from a countable set.\n\nE. For each set $A_n$ in the partition, the expectation of $X$ conditional on $A_n$ is zero.", "solution": "The problem asks for the characterization of a random variable $X$ for which equality holds in the conditional Jensen's inequality, $E[\\phi(X) | \\mathcal{G}] = \\phi(E[X | \\mathcal{G}])$, where $\\phi$ is strictly convex and $\\mathcal{G}$ is a sigma-algebra generated by a countable partition $\\{A_n\\}_{n=1}^{\\infty}$ with $P(A_n)>0$.\n\nFirst, let's establish the form of the conditional expectation with respect to the sigma-algebra $\\mathcal{G} = \\sigma(\\{A_n\\}_{n=1}^{\\infty})$. For any integrable random variable $Z$, the conditional expectation $E[Z|\\mathcal{G}]$ is a $\\mathcal{G}$-measurable random variable. A random variable is $\\mathcal{G}$-measurable if and only if it is constant on each atom of $\\mathcal{G}$. The atoms of $\\mathcal{G}$ are the sets $A_n$. Therefore, $E[Z|\\mathcal{G}]$ must a simple function of the form $\\sum_{n=1}^{\\infty} c_n \\mathbb{1}_{A_n}$, where $\\mathbb{1}_{A_n}$ is the indicator function for the set $A_n$.\n\nTo find the constants $c_n$, we use the defining property of conditional expectation: for any $A \\in \\mathcal{G}$, $\\int_A E[Z|\\mathcal{G}] dP = \\int_A Z dP$. Let's choose $A = A_k$ for some $k \\in \\mathbb{N}$.\n$$\n\\int_{A_k} E[Z|\\mathcal{G}] dP = \\int_{A_k} \\left(\\sum_{n=1}^{\\infty} c_n \\mathbb{1}_{A_n}\\right) dP = \\int_{A_k} c_k \\mathbb{1}_{A_k} dP = c_k P(A_k)\n$$\nAnd\n$$\n\\int_{A_k} Z dP = E[Z \\mathbb{1}_{A_k}]\n$$\nEquating these two gives $c_k P(A_k) = E[Z \\mathbb{1}_{A_k}]$. Since $P(A_k) > 0$, we have $c_k = \\frac{E[Z \\mathbb{1}_{A_k}]}{P(A_k)}$, which is the definition of the conditional expectation of $Z$ given the event $A_k$, denoted $E[Z|A_k]$.\nThus, the conditional expectation can be written as:\n$$\nE[Z|\\mathcal{G}] = \\sum_{n=1}^{\\infty} E[Z|A_n] \\mathbb{1}_{A_n}\n$$\n\nNow, let's apply this to the given equality $E[\\phi(X) | \\mathcal{G}] = \\phi(E[X | \\mathcal{G}])$.\n\nThe left-hand side (LHS) is:\n$$\nE[\\phi(X)|\\mathcal{G}] = \\sum_{n=1}^{\\infty} E[\\phi(X)|A_n] \\mathbb{1}_{A_n}\n$$\n\nFor the right-hand side (RHS), we first find $E[X|\\mathcal{G}]$:\n$$\nE[X|\\mathcal{G}] = \\sum_{n=1}^{\\infty} E[X|A_n] \\mathbb{1}_{A_n}\n$$\nThen we apply the function $\\phi$ to it. Since the sets $A_n$ are disjoint, for any $\\omega \\in \\Omega$, $\\omega$ belongs to exactly one $A_k$. For such an $\\omega$, $E[X|\\mathcal{G}](\\omega) = E[X|A_k]$. Therefore, $\\phi(E[X|\\mathcal{G}])(\\omega) = \\phi(E[X|A_k])$. This means we can write the RHS as:\n$$\n\\phi(E[X|\\mathcal{G}]) = \\sum_{n=1}^{\\infty} \\phi(E[X|A_n]) \\mathbb{1}_{A_n}\n$$\n\nThe equality $E[\\phi(X) | \\mathcal{G}] = \\phi(E[X | \\mathcal{G}])$ holds almost surely if and only if the coefficients of the indicator functions $\\mathbb{1}_{A_n}$ are equal for all $n$. That is, for each $n \\in \\mathbb{N}$:\n$$\nE[\\phi(X)|A_n] = \\phi(E[X|A_n])\n$$\nLet's analyze this condition for a fixed $n$. Let $P_n(\\cdot) = P(\\cdot | A_n)$ be the conditional probability measure on $A_n$. Then the equation can be rewritten using expectations with respect to $P_n$:\n$$\nE_{P_n}[\\phi(X)] = \\phi(E_{P_n}[X])\n$$\nThis is the equality case of the standard (unconditional) Jensen's inequality for the random variable $X$ under the probability measure $P_n$. For a strictly convex function $\\phi$, equality holds if and only if the random variable is constant almost surely with respect to the governing probability measure.\nIn our context, this means that for each $n$, $X$ must be constant $P_n$-almost surely. That is, for each $n$, there exists a constant $c_n$ such that $P_n(X=c_n)=1$.\nBy definition of conditional probability, $P(X=c_n | A_n) = 1$. This is equivalent to $P(\\{ \\omega \\in A_n : X(\\omega) \\neq c_n \\}) = 0$.\n\nLet $Y = \\sum_{n=1}^{\\infty} c_n \\mathbb{1}_{A_n}$. By construction, $Y$ is a $\\mathcal{G}$-measurable random variable. We have shown that for each $n$, $X$ is equal to $c_n$ almost surely on $A_n$.\nThe set where $X$ and $Y$ differ is $S = \\{\\omega \\in \\Omega : X(\\omega) \\neq Y(\\omega)\\}$. We can write this set as a disjoint union:\n$$\nS = \\bigcup_{n=1}^{\\infty} \\{\\omega \\in A_n : X(\\omega) \\neq Y(\\omega)\\} = \\bigcup_{n=1}^{\\infty} \\{\\omega \\in A_n : X(\\omega) \\neq c_n\\}\n$$\nThe probability of this set is:\n$$\nP(S) = P\\left(\\bigcup_{n=1}^{\\infty} \\{\\omega \\in A_n : X(\\omega) \\neq c_n\\}\\right) = \\sum_{n=1}^{\\infty} P(\\{\\omega \\in A_n : X(\\omega) \\neq c_n\\})\n$$\nSince $P(\\{\\omega \\in A_n : X(\\omega) \\neq c_n\\}) = 0$ for each $n$, the sum is also 0. So, $P(S)=0$, which means $P(X=Y)=1$.\n\nThis shows that $X$ must be equal almost surely to a $\\mathcal{G}$-measurable random variable $Y$. This is precisely the statement in option C.\n\nLet's review the other options:\nA. $P(X=c)=1$ for some constant $c$. This is a special case of our conclusion where all $c_n$ are equal. It is not the most general characterization.\nB. $X$ is independent of $\\mathcal{G}$. This means $E[X|\\mathcal{G}] = E[X]$ (a constant). The equality would become $E[\\phi(X)|\\mathcal{G}] = \\phi(E[X])$. This would imply $X$ is $\\mathcal{G}$-measurable (from our derivation) and independent of $\\mathcal{G}$, which means $X$ must be constant almost surely. This is the same restrictive case as A.\nD. $X$ must be a discrete random variable. A $\\mathcal{G}$-measurable variable $Y = \\sum c_n \\mathbb{1}_{A_n}$ is discrete (it takes values from the countable set $\\{c_n\\}$). Since $X=Y$ almost surely, $X$ will also have a range that is countable up to a set of measure zero. However, option C is a more direct and fundamental characterization based on measurability. The property of being discrete is a consequence. More importantly, option C is the definition of being \"almost surely $\\mathcal{G}$-measurable\", which is the core concept being tested.\nE. $E[X|A_n] = 0$ for each $n$. This would imply $E[X|\\mathcal{G}]=0$ almost surely. The equality becomes $E[\\phi(X)|\\mathcal{G}] = \\phi(0)$. This is a very specific condition on the values of $X$ and is not a general characterization for the equality in Jensen's inequality.\n\nTherefore, the most accurate and general characterization is that $X$ is equal, almost surely, to a $\\mathcal{G}$-measurable random variable.", "answer": "$$\\boxed{C}$$", "id": "1425928"}]}